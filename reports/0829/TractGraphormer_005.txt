Namespace(inputDirectory='data', outputDirectory='0829', DisFile='data/dis_sort_roi2dis2000.npy', task='sex', type='binary', feature=['Nos', 'FA1', 'MD1'], CUDA_id='0', data_id='0', dataset='Classification', norm=True, epochs=120, tensorboard=True, net_architecture='TractGraphormer', batch_size=32, rate=1e-06, weight=0.0, sched_step=300, sched_gamma=0.1, printing_frequency=1, seed=1, alpha=0, remix_kappa=0, remix_tau=0, loss='CE', sigma=0, k=20, fl=64, nh=1)
Training the 'TractGraphormer' architecture

The following parameters are used:
Batch size:	32
Number of workers:	0
Learning rate:	1e-06
Weight decay:	0.0
Scheduler steps:	300
Scheduler gamma:	0.1
Number of epochs of training:	120
Number of input channels:	3

Data preparation
Reading data from:	./data
Training set size:	6538
Validation set size:	1869

Performing calculations on:	cuda:0

Pretraining:	Epoch 1/120
----------
training:	Epoch: [1][1/204]	Loss 0.7156 (0.7156)	
training:	Epoch: [1][2/204]	Loss 0.6939 (0.7047)	
training:	Epoch: [1][3/204]	Loss 0.7074 (0.7056)	
training:	Epoch: [1][4/204]	Loss 0.6913 (0.7020)	
training:	Epoch: [1][5/204]	Loss 0.7022 (0.7021)	
training:	Epoch: [1][6/204]	Loss 0.7016 (0.7020)	
training:	Epoch: [1][7/204]	Loss 0.7016 (0.7019)	
training:	Epoch: [1][8/204]	Loss 0.6861 (0.7000)	
training:	Epoch: [1][9/204]	Loss 0.7077 (0.7008)	
training:	Epoch: [1][10/204]	Loss 0.7031 (0.7010)	
training:	Epoch: [1][11/204]	Loss 0.7007 (0.7010)	
training:	Epoch: [1][12/204]	Loss 0.6990 (0.7008)	
training:	Epoch: [1][13/204]	Loss 0.6845 (0.6996)	
training:	Epoch: [1][14/204]	Loss 0.6925 (0.6991)	
training:	Epoch: [1][15/204]	Loss 0.6812 (0.6979)	
training:	Epoch: [1][16/204]	Loss 0.6988 (0.6979)	
training:	Epoch: [1][17/204]	Loss 0.6938 (0.6977)	
training:	Epoch: [1][18/204]	Loss 0.6786 (0.6966)	
training:	Epoch: [1][19/204]	Loss 0.7054 (0.6971)	
training:	Epoch: [1][20/204]	Loss 0.6908 (0.6968)	
training:	Epoch: [1][21/204]	Loss 0.6869 (0.6963)	
training:	Epoch: [1][22/204]	Loss 0.7003 (0.6965)	
training:	Epoch: [1][23/204]	Loss 0.6901 (0.6962)	
training:	Epoch: [1][24/204]	Loss 0.6862 (0.6958)	
training:	Epoch: [1][25/204]	Loss 0.7200 (0.6968)	
training:	Epoch: [1][26/204]	Loss 0.6991 (0.6969)	
training:	Epoch: [1][27/204]	Loss 0.6818 (0.6963)	
training:	Epoch: [1][28/204]	Loss 0.7107 (0.6968)	
training:	Epoch: [1][29/204]	Loss 0.7023 (0.6970)	
training:	Epoch: [1][30/204]	Loss 0.6988 (0.6971)	
training:	Epoch: [1][31/204]	Loss 0.6949 (0.6970)	
training:	Epoch: [1][32/204]	Loss 0.7091 (0.6974)	
training:	Epoch: [1][33/204]	Loss 0.7000 (0.6974)	
training:	Epoch: [1][34/204]	Loss 0.6847 (0.6971)	
training:	Epoch: [1][35/204]	Loss 0.6948 (0.6970)	
training:	Epoch: [1][36/204]	Loss 0.6821 (0.6966)	
training:	Epoch: [1][37/204]	Loss 0.7012 (0.6967)	
training:	Epoch: [1][38/204]	Loss 0.6860 (0.6964)	
training:	Epoch: [1][39/204]	Loss 0.6913 (0.6963)	
training:	Epoch: [1][40/204]	Loss 0.6905 (0.6962)	
training:	Epoch: [1][41/204]	Loss 0.6945 (0.6961)	
training:	Epoch: [1][42/204]	Loss 0.7254 (0.6968)	
training:	Epoch: [1][43/204]	Loss 0.7035 (0.6970)	
training:	Epoch: [1][44/204]	Loss 0.6984 (0.6970)	
training:	Epoch: [1][45/204]	Loss 0.6991 (0.6971)	
training:	Epoch: [1][46/204]	Loss 0.7025 (0.6972)	
training:	Epoch: [1][47/204]	Loss 0.7103 (0.6974)	
training:	Epoch: [1][48/204]	Loss 0.6977 (0.6975)	
training:	Epoch: [1][49/204]	Loss 0.6981 (0.6975)	
training:	Epoch: [1][50/204]	Loss 0.6717 (0.6970)	
training:	Epoch: [1][51/204]	Loss 0.7082 (0.6972)	
training:	Epoch: [1][52/204]	Loss 0.6864 (0.6970)	
training:	Epoch: [1][53/204]	Loss 0.7075 (0.6972)	
training:	Epoch: [1][54/204]	Loss 0.6948 (0.6971)	
training:	Epoch: [1][55/204]	Loss 0.6933 (0.6970)	
training:	Epoch: [1][56/204]	Loss 0.6983 (0.6971)	
training:	Epoch: [1][57/204]	Loss 0.6948 (0.6970)	
training:	Epoch: [1][58/204]	Loss 0.6924 (0.6970)	
training:	Epoch: [1][59/204]	Loss 0.6953 (0.6969)	
training:	Epoch: [1][60/204]	Loss 0.6849 (0.6967)	
training:	Epoch: [1][61/204]	Loss 0.6760 (0.6964)	
training:	Epoch: [1][62/204]	Loss 0.6812 (0.6961)	
training:	Epoch: [1][63/204]	Loss 0.6991 (0.6962)	
training:	Epoch: [1][64/204]	Loss 0.6929 (0.6961)	
training:	Epoch: [1][65/204]	Loss 0.6902 (0.6960)	
training:	Epoch: [1][66/204]	Loss 0.6867 (0.6959)	
training:	Epoch: [1][67/204]	Loss 0.6982 (0.6959)	
training:	Epoch: [1][68/204]	Loss 0.6962 (0.6959)	
training:	Epoch: [1][69/204]	Loss 0.6974 (0.6960)	
training:	Epoch: [1][70/204]	Loss 0.7018 (0.6960)	
training:	Epoch: [1][71/204]	Loss 0.6923 (0.6960)	
training:	Epoch: [1][72/204]	Loss 0.7045 (0.6961)	
training:	Epoch: [1][73/204]	Loss 0.7019 (0.6962)	
training:	Epoch: [1][74/204]	Loss 0.7153 (0.6964)	
training:	Epoch: [1][75/204]	Loss 0.6787 (0.6962)	
training:	Epoch: [1][76/204]	Loss 0.6686 (0.6958)	
training:	Epoch: [1][77/204]	Loss 0.6692 (0.6955)	
training:	Epoch: [1][78/204]	Loss 0.6891 (0.6954)	
training:	Epoch: [1][79/204]	Loss 0.6858 (0.6953)	
training:	Epoch: [1][80/204]	Loss 0.6850 (0.6952)	
training:	Epoch: [1][81/204]	Loss 0.6973 (0.6952)	
training:	Epoch: [1][82/204]	Loss 0.6986 (0.6952)	
training:	Epoch: [1][83/204]	Loss 0.6894 (0.6952)	
training:	Epoch: [1][84/204]	Loss 0.6882 (0.6951)	
training:	Epoch: [1][85/204]	Loss 0.6802 (0.6949)	
training:	Epoch: [1][86/204]	Loss 0.6861 (0.6948)	
training:	Epoch: [1][87/204]	Loss 0.6952 (0.6948)	
training:	Epoch: [1][88/204]	Loss 0.7087 (0.6950)	
training:	Epoch: [1][89/204]	Loss 0.7093 (0.6951)	
training:	Epoch: [1][90/204]	Loss 0.6918 (0.6951)	
training:	Epoch: [1][91/204]	Loss 0.6812 (0.6949)	
training:	Epoch: [1][92/204]	Loss 0.6794 (0.6948)	
training:	Epoch: [1][93/204]	Loss 0.6930 (0.6948)	
training:	Epoch: [1][94/204]	Loss 0.7063 (0.6949)	
training:	Epoch: [1][95/204]	Loss 0.6859 (0.6948)	
training:	Epoch: [1][96/204]	Loss 0.7004 (0.6948)	
training:	Epoch: [1][97/204]	Loss 0.7042 (0.6949)	
training:	Epoch: [1][98/204]	Loss 0.6938 (0.6949)	
training:	Epoch: [1][99/204]	Loss 0.7022 (0.6950)	
training:	Epoch: [1][100/204]	Loss 0.6723 (0.6948)	
training:	Epoch: [1][101/204]	Loss 0.6969 (0.6948)	
training:	Epoch: [1][102/204]	Loss 0.6757 (0.6946)	
training:	Epoch: [1][103/204]	Loss 0.6993 (0.6947)	
training:	Epoch: [1][104/204]	Loss 0.6845 (0.6946)	
training:	Epoch: [1][105/204]	Loss 0.6782 (0.6944)	
training:	Epoch: [1][106/204]	Loss 0.7026 (0.6945)	
training:	Epoch: [1][107/204]	Loss 0.7090 (0.6946)	
training:	Epoch: [1][108/204]	Loss 0.6930 (0.6946)	
training:	Epoch: [1][109/204]	Loss 0.6659 (0.6943)	
training:	Epoch: [1][110/204]	Loss 0.6846 (0.6942)	
training:	Epoch: [1][111/204]	Loss 0.6856 (0.6942)	
training:	Epoch: [1][112/204]	Loss 0.6607 (0.6939)	
training:	Epoch: [1][113/204]	Loss 0.6713 (0.6937)	
training:	Epoch: [1][114/204]	Loss 0.6888 (0.6936)	
training:	Epoch: [1][115/204]	Loss 0.6947 (0.6936)	
training:	Epoch: [1][116/204]	Loss 0.6943 (0.6936)	
training:	Epoch: [1][117/204]	Loss 0.7136 (0.6938)	
training:	Epoch: [1][118/204]	Loss 0.6918 (0.6938)	
training:	Epoch: [1][119/204]	Loss 0.6888 (0.6938)	
training:	Epoch: [1][120/204]	Loss 0.6897 (0.6937)	
training:	Epoch: [1][121/204]	Loss 0.7070 (0.6938)	
training:	Epoch: [1][122/204]	Loss 0.6896 (0.6938)	
training:	Epoch: [1][123/204]	Loss 0.6660 (0.6936)	
training:	Epoch: [1][124/204]	Loss 0.6664 (0.6933)	
training:	Epoch: [1][125/204]	Loss 0.6889 (0.6933)	
training:	Epoch: [1][126/204]	Loss 0.6875 (0.6933)	
training:	Epoch: [1][127/204]	Loss 0.6958 (0.6933)	
training:	Epoch: [1][128/204]	Loss 0.6854 (0.6932)	
training:	Epoch: [1][129/204]	Loss 0.6903 (0.6932)	
training:	Epoch: [1][130/204]	Loss 0.6907 (0.6932)	
training:	Epoch: [1][131/204]	Loss 0.6963 (0.6932)	
training:	Epoch: [1][132/204]	Loss 0.7008 (0.6933)	
training:	Epoch: [1][133/204]	Loss 0.6861 (0.6932)	
training:	Epoch: [1][134/204]	Loss 0.7236 (0.6934)	
training:	Epoch: [1][135/204]	Loss 0.7035 (0.6935)	
training:	Epoch: [1][136/204]	Loss 0.6842 (0.6934)	
training:	Epoch: [1][137/204]	Loss 0.7013 (0.6935)	
training:	Epoch: [1][138/204]	Loss 0.7118 (0.6936)	
training:	Epoch: [1][139/204]	Loss 0.6863 (0.6936)	
training:	Epoch: [1][140/204]	Loss 0.7022 (0.6936)	
training:	Epoch: [1][141/204]	Loss 0.6829 (0.6936)	
training:	Epoch: [1][142/204]	Loss 0.6854 (0.6935)	
training:	Epoch: [1][143/204]	Loss 0.6771 (0.6934)	
training:	Epoch: [1][144/204]	Loss 0.6899 (0.6934)	
training:	Epoch: [1][145/204]	Loss 0.6826 (0.6933)	
training:	Epoch: [1][146/204]	Loss 0.6832 (0.6932)	
training:	Epoch: [1][147/204]	Loss 0.6770 (0.6931)	
training:	Epoch: [1][148/204]	Loss 0.7129 (0.6933)	
training:	Epoch: [1][149/204]	Loss 0.6794 (0.6932)	
training:	Epoch: [1][150/204]	Loss 0.6769 (0.6930)	
training:	Epoch: [1][151/204]	Loss 0.6858 (0.6930)	
training:	Epoch: [1][152/204]	Loss 0.7005 (0.6931)	
training:	Epoch: [1][153/204]	Loss 0.6972 (0.6931)	
training:	Epoch: [1][154/204]	Loss 0.6910 (0.6931)	
training:	Epoch: [1][155/204]	Loss 0.6787 (0.6930)	
training:	Epoch: [1][156/204]	Loss 0.7071 (0.6931)	
training:	Epoch: [1][157/204]	Loss 0.6984 (0.6931)	
training:	Epoch: [1][158/204]	Loss 0.6903 (0.6931)	
training:	Epoch: [1][159/204]	Loss 0.7057 (0.6932)	
training:	Epoch: [1][160/204]	Loss 0.6926 (0.6932)	
training:	Epoch: [1][161/204]	Loss 0.6840 (0.6931)	
training:	Epoch: [1][162/204]	Loss 0.6998 (0.6931)	
training:	Epoch: [1][163/204]	Loss 0.7004 (0.6932)	
training:	Epoch: [1][164/204]	Loss 0.6957 (0.6932)	
training:	Epoch: [1][165/204]	Loss 0.6796 (0.6931)	
training:	Epoch: [1][166/204]	Loss 0.7005 (0.6932)	
training:	Epoch: [1][167/204]	Loss 0.6848 (0.6931)	
training:	Epoch: [1][168/204]	Loss 0.6926 (0.6931)	
training:	Epoch: [1][169/204]	Loss 0.6829 (0.6930)	
training:	Epoch: [1][170/204]	Loss 0.6792 (0.6930)	
training:	Epoch: [1][171/204]	Loss 0.7039 (0.6930)	
training:	Epoch: [1][172/204]	Loss 0.6981 (0.6931)	
training:	Epoch: [1][173/204]	Loss 0.6936 (0.6931)	
training:	Epoch: [1][174/204]	Loss 0.6877 (0.6930)	
training:	Epoch: [1][175/204]	Loss 0.7061 (0.6931)	
training:	Epoch: [1][176/204]	Loss 0.6983 (0.6931)	
training:	Epoch: [1][177/204]	Loss 0.7004 (0.6932)	
training:	Epoch: [1][178/204]	Loss 0.6966 (0.6932)	
training:	Epoch: [1][179/204]	Loss 0.6941 (0.6932)	
training:	Epoch: [1][180/204]	Loss 0.6749 (0.6931)	
training:	Epoch: [1][181/204]	Loss 0.6972 (0.6931)	
training:	Epoch: [1][182/204]	Loss 0.6777 (0.6930)	
training:	Epoch: [1][183/204]	Loss 0.6851 (0.6930)	
training:	Epoch: [1][184/204]	Loss 0.6952 (0.6930)	
training:	Epoch: [1][185/204]	Loss 0.6927 (0.6930)	
training:	Epoch: [1][186/204]	Loss 0.6843 (0.6930)	
training:	Epoch: [1][187/204]	Loss 0.6937 (0.6930)	
training:	Epoch: [1][188/204]	Loss 0.6955 (0.6930)	
training:	Epoch: [1][189/204]	Loss 0.6939 (0.6930)	
training:	Epoch: [1][190/204]	Loss 0.7038 (0.6930)	
training:	Epoch: [1][191/204]	Loss 0.6992 (0.6931)	
training:	Epoch: [1][192/204]	Loss 0.6834 (0.6930)	
training:	Epoch: [1][193/204]	Loss 0.6849 (0.6930)	
training:	Epoch: [1][194/204]	Loss 0.6945 (0.6930)	
training:	Epoch: [1][195/204]	Loss 0.6731 (0.6929)	
training:	Epoch: [1][196/204]	Loss 0.6755 (0.6928)	
training:	Epoch: [1][197/204]	Loss 0.6838 (0.6928)	
training:	Epoch: [1][198/204]	Loss 0.6841 (0.6927)	
training:	Epoch: [1][199/204]	Loss 0.6945 (0.6927)	
training:	Epoch: [1][200/204]	Loss 0.6936 (0.6927)	
training:	Epoch: [1][201/204]	Loss 0.7132 (0.6928)	
training:	Epoch: [1][202/204]	Loss 0.6857 (0.6928)	
training:	Epoch: [1][203/204]	Loss 0.6749 (0.6927)	
training:	Epoch: [1][204/204]	Loss 0.6859 (0.6927)	
Training:	 Loss: 0.6916

Training:	 ACC: 0.5381 0.5414 0.6214 0.4547
Validation:	 ACC: 0.5581 0.5618 0.6397 0.4765
Validation:	 Best_BACC: 0.5581 0.5618 0.6397 0.4765
Validation:	 Loss: 0.6888
Pretraining:	Epoch 2/120
----------
training:	Epoch: [2][1/204]	Loss 0.7107 (0.7107)	
training:	Epoch: [2][2/204]	Loss 0.6612 (0.6860)	
training:	Epoch: [2][3/204]	Loss 0.6895 (0.6871)	
training:	Epoch: [2][4/204]	Loss 0.6978 (0.6898)	
training:	Epoch: [2][5/204]	Loss 0.6903 (0.6899)	
training:	Epoch: [2][6/204]	Loss 0.6834 (0.6888)	
training:	Epoch: [2][7/204]	Loss 0.6899 (0.6890)	
training:	Epoch: [2][8/204]	Loss 0.6964 (0.6899)	
training:	Epoch: [2][9/204]	Loss 0.6879 (0.6897)	
training:	Epoch: [2][10/204]	Loss 0.7023 (0.6909)	
training:	Epoch: [2][11/204]	Loss 0.6991 (0.6917)	
training:	Epoch: [2][12/204]	Loss 0.6955 (0.6920)	
training:	Epoch: [2][13/204]	Loss 0.6834 (0.6913)	
training:	Epoch: [2][14/204]	Loss 0.6883 (0.6911)	
training:	Epoch: [2][15/204]	Loss 0.6845 (0.6907)	
training:	Epoch: [2][16/204]	Loss 0.7097 (0.6919)	
training:	Epoch: [2][17/204]	Loss 0.6693 (0.6905)	
training:	Epoch: [2][18/204]	Loss 0.7121 (0.6917)	
training:	Epoch: [2][19/204]	Loss 0.6904 (0.6917)	
training:	Epoch: [2][20/204]	Loss 0.6861 (0.6914)	
training:	Epoch: [2][21/204]	Loss 0.7005 (0.6918)	
training:	Epoch: [2][22/204]	Loss 0.6798 (0.6913)	
training:	Epoch: [2][23/204]	Loss 0.6854 (0.6910)	
training:	Epoch: [2][24/204]	Loss 0.6841 (0.6907)	
training:	Epoch: [2][25/204]	Loss 0.6842 (0.6905)	
training:	Epoch: [2][26/204]	Loss 0.6792 (0.6900)	
training:	Epoch: [2][27/204]	Loss 0.6890 (0.6900)	
training:	Epoch: [2][28/204]	Loss 0.6748 (0.6895)	
training:	Epoch: [2][29/204]	Loss 0.6767 (0.6890)	
training:	Epoch: [2][30/204]	Loss 0.6800 (0.6887)	
training:	Epoch: [2][31/204]	Loss 0.6691 (0.6881)	
training:	Epoch: [2][32/204]	Loss 0.7001 (0.6885)	
training:	Epoch: [2][33/204]	Loss 0.7225 (0.6895)	
training:	Epoch: [2][34/204]	Loss 0.6948 (0.6896)	
training:	Epoch: [2][35/204]	Loss 0.6991 (0.6899)	
training:	Epoch: [2][36/204]	Loss 0.6941 (0.6900)	
training:	Epoch: [2][37/204]	Loss 0.6913 (0.6901)	
training:	Epoch: [2][38/204]	Loss 0.6818 (0.6899)	
training:	Epoch: [2][39/204]	Loss 0.6867 (0.6898)	
training:	Epoch: [2][40/204]	Loss 0.7043 (0.6901)	
training:	Epoch: [2][41/204]	Loss 0.6885 (0.6901)	
training:	Epoch: [2][42/204]	Loss 0.6785 (0.6898)	
training:	Epoch: [2][43/204]	Loss 0.6974 (0.6900)	
training:	Epoch: [2][44/204]	Loss 0.6879 (0.6899)	
training:	Epoch: [2][45/204]	Loss 0.6892 (0.6899)	
training:	Epoch: [2][46/204]	Loss 0.6884 (0.6899)	
training:	Epoch: [2][47/204]	Loss 0.6800 (0.6897)	
training:	Epoch: [2][48/204]	Loss 0.7088 (0.6901)	
training:	Epoch: [2][49/204]	Loss 0.6930 (0.6901)	
training:	Epoch: [2][50/204]	Loss 0.6881 (0.6901)	
training:	Epoch: [2][51/204]	Loss 0.6864 (0.6900)	
training:	Epoch: [2][52/204]	Loss 0.6785 (0.6898)	
training:	Epoch: [2][53/204]	Loss 0.6702 (0.6894)	
training:	Epoch: [2][54/204]	Loss 0.6914 (0.6895)	
training:	Epoch: [2][55/204]	Loss 0.6866 (0.6894)	
training:	Epoch: [2][56/204]	Loss 0.7029 (0.6897)	
training:	Epoch: [2][57/204]	Loss 0.6938 (0.6897)	
training:	Epoch: [2][58/204]	Loss 0.7016 (0.6899)	
training:	Epoch: [2][59/204]	Loss 0.6775 (0.6897)	
training:	Epoch: [2][60/204]	Loss 0.6837 (0.6896)	
training:	Epoch: [2][61/204]	Loss 0.6844 (0.6895)	
training:	Epoch: [2][62/204]	Loss 0.7227 (0.6901)	
training:	Epoch: [2][63/204]	Loss 0.6801 (0.6899)	
training:	Epoch: [2][64/204]	Loss 0.7107 (0.6902)	
training:	Epoch: [2][65/204]	Loss 0.7079 (0.6905)	
training:	Epoch: [2][66/204]	Loss 0.7082 (0.6908)	
training:	Epoch: [2][67/204]	Loss 0.6833 (0.6907)	
training:	Epoch: [2][68/204]	Loss 0.6896 (0.6907)	
training:	Epoch: [2][69/204]	Loss 0.6884 (0.6906)	
training:	Epoch: [2][70/204]	Loss 0.7096 (0.6909)	
training:	Epoch: [2][71/204]	Loss 0.6906 (0.6909)	
training:	Epoch: [2][72/204]	Loss 0.6869 (0.6908)	
training:	Epoch: [2][73/204]	Loss 0.6799 (0.6907)	
training:	Epoch: [2][74/204]	Loss 0.6876 (0.6906)	
training:	Epoch: [2][75/204]	Loss 0.6846 (0.6906)	
training:	Epoch: [2][76/204]	Loss 0.6871 (0.6905)	
training:	Epoch: [2][77/204]	Loss 0.6891 (0.6905)	
training:	Epoch: [2][78/204]	Loss 0.7044 (0.6907)	
training:	Epoch: [2][79/204]	Loss 0.7110 (0.6909)	
training:	Epoch: [2][80/204]	Loss 0.6918 (0.6909)	
training:	Epoch: [2][81/204]	Loss 0.6839 (0.6909)	
training:	Epoch: [2][82/204]	Loss 0.6851 (0.6908)	
training:	Epoch: [2][83/204]	Loss 0.6946 (0.6908)	
training:	Epoch: [2][84/204]	Loss 0.6748 (0.6906)	
training:	Epoch: [2][85/204]	Loss 0.6853 (0.6906)	
training:	Epoch: [2][86/204]	Loss 0.6874 (0.6905)	
training:	Epoch: [2][87/204]	Loss 0.6900 (0.6905)	
training:	Epoch: [2][88/204]	Loss 0.7162 (0.6908)	
training:	Epoch: [2][89/204]	Loss 0.6857 (0.6908)	
training:	Epoch: [2][90/204]	Loss 0.7148 (0.6910)	
training:	Epoch: [2][91/204]	Loss 0.6716 (0.6908)	
training:	Epoch: [2][92/204]	Loss 0.7056 (0.6910)	
training:	Epoch: [2][93/204]	Loss 0.6895 (0.6910)	
training:	Epoch: [2][94/204]	Loss 0.6958 (0.6910)	
training:	Epoch: [2][95/204]	Loss 0.6977 (0.6911)	
training:	Epoch: [2][96/204]	Loss 0.7122 (0.6913)	
training:	Epoch: [2][97/204]	Loss 0.6760 (0.6911)	
training:	Epoch: [2][98/204]	Loss 0.6894 (0.6911)	
training:	Epoch: [2][99/204]	Loss 0.6669 (0.6909)	
training:	Epoch: [2][100/204]	Loss 0.6829 (0.6908)	
training:	Epoch: [2][101/204]	Loss 0.6862 (0.6908)	
training:	Epoch: [2][102/204]	Loss 0.6799 (0.6907)	
training:	Epoch: [2][103/204]	Loss 0.7001 (0.6907)	
training:	Epoch: [2][104/204]	Loss 0.6916 (0.6908)	
training:	Epoch: [2][105/204]	Loss 0.6705 (0.6906)	
training:	Epoch: [2][106/204]	Loss 0.6826 (0.6905)	
training:	Epoch: [2][107/204]	Loss 0.6838 (0.6904)	
training:	Epoch: [2][108/204]	Loss 0.7007 (0.6905)	
training:	Epoch: [2][109/204]	Loss 0.7096 (0.6907)	
training:	Epoch: [2][110/204]	Loss 0.6804 (0.6906)	
training:	Epoch: [2][111/204]	Loss 0.6812 (0.6905)	
training:	Epoch: [2][112/204]	Loss 0.7033 (0.6906)	
training:	Epoch: [2][113/204]	Loss 0.7078 (0.6908)	
training:	Epoch: [2][114/204]	Loss 0.6919 (0.6908)	
training:	Epoch: [2][115/204]	Loss 0.7135 (0.6910)	
training:	Epoch: [2][116/204]	Loss 0.6794 (0.6909)	
training:	Epoch: [2][117/204]	Loss 0.6909 (0.6909)	
training:	Epoch: [2][118/204]	Loss 0.6876 (0.6909)	
training:	Epoch: [2][119/204]	Loss 0.6833 (0.6908)	
training:	Epoch: [2][120/204]	Loss 0.6880 (0.6908)	
training:	Epoch: [2][121/204]	Loss 0.6856 (0.6907)	
training:	Epoch: [2][122/204]	Loss 0.6690 (0.6906)	
training:	Epoch: [2][123/204]	Loss 0.6856 (0.6905)	
training:	Epoch: [2][124/204]	Loss 0.6762 (0.6904)	
training:	Epoch: [2][125/204]	Loss 0.6981 (0.6905)	
training:	Epoch: [2][126/204]	Loss 0.6729 (0.6903)	
training:	Epoch: [2][127/204]	Loss 0.7094 (0.6905)	
training:	Epoch: [2][128/204]	Loss 0.6917 (0.6905)	
training:	Epoch: [2][129/204]	Loss 0.6940 (0.6905)	
training:	Epoch: [2][130/204]	Loss 0.7084 (0.6906)	
training:	Epoch: [2][131/204]	Loss 0.7046 (0.6907)	
training:	Epoch: [2][132/204]	Loss 0.7054 (0.6909)	
training:	Epoch: [2][133/204]	Loss 0.6912 (0.6909)	
training:	Epoch: [2][134/204]	Loss 0.6750 (0.6907)	
training:	Epoch: [2][135/204]	Loss 0.6815 (0.6907)	
training:	Epoch: [2][136/204]	Loss 0.6821 (0.6906)	
training:	Epoch: [2][137/204]	Loss 0.6927 (0.6906)	
training:	Epoch: [2][138/204]	Loss 0.7025 (0.6907)	
training:	Epoch: [2][139/204]	Loss 0.6866 (0.6907)	
training:	Epoch: [2][140/204]	Loss 0.6887 (0.6907)	
training:	Epoch: [2][141/204]	Loss 0.6886 (0.6907)	
training:	Epoch: [2][142/204]	Loss 0.6942 (0.6907)	
training:	Epoch: [2][143/204]	Loss 0.7123 (0.6908)	
training:	Epoch: [2][144/204]	Loss 0.6923 (0.6908)	
training:	Epoch: [2][145/204]	Loss 0.6910 (0.6908)	
training:	Epoch: [2][146/204]	Loss 0.6765 (0.6907)	
training:	Epoch: [2][147/204]	Loss 0.6761 (0.6906)	
training:	Epoch: [2][148/204]	Loss 0.6772 (0.6906)	
training:	Epoch: [2][149/204]	Loss 0.6881 (0.6905)	
training:	Epoch: [2][150/204]	Loss 0.6947 (0.6906)	
training:	Epoch: [2][151/204]	Loss 0.6774 (0.6905)	
training:	Epoch: [2][152/204]	Loss 0.7200 (0.6907)	
training:	Epoch: [2][153/204]	Loss 0.6856 (0.6906)	
training:	Epoch: [2][154/204]	Loss 0.6889 (0.6906)	
training:	Epoch: [2][155/204]	Loss 0.6783 (0.6905)	
training:	Epoch: [2][156/204]	Loss 0.6847 (0.6905)	
training:	Epoch: [2][157/204]	Loss 0.6848 (0.6905)	
training:	Epoch: [2][158/204]	Loss 0.6914 (0.6905)	
training:	Epoch: [2][159/204]	Loss 0.6841 (0.6904)	
training:	Epoch: [2][160/204]	Loss 0.6821 (0.6904)	
training:	Epoch: [2][161/204]	Loss 0.7034 (0.6905)	
training:	Epoch: [2][162/204]	Loss 0.6870 (0.6904)	
training:	Epoch: [2][163/204]	Loss 0.6990 (0.6905)	
training:	Epoch: [2][164/204]	Loss 0.6710 (0.6904)	
training:	Epoch: [2][165/204]	Loss 0.6802 (0.6903)	
training:	Epoch: [2][166/204]	Loss 0.6625 (0.6902)	
training:	Epoch: [2][167/204]	Loss 0.7039 (0.6902)	
training:	Epoch: [2][168/204]	Loss 0.6698 (0.6901)	
training:	Epoch: [2][169/204]	Loss 0.6696 (0.6900)	
training:	Epoch: [2][170/204]	Loss 0.6872 (0.6900)	
training:	Epoch: [2][171/204]	Loss 0.6994 (0.6900)	
training:	Epoch: [2][172/204]	Loss 0.6955 (0.6901)	
training:	Epoch: [2][173/204]	Loss 0.6948 (0.6901)	
training:	Epoch: [2][174/204]	Loss 0.6866 (0.6901)	
training:	Epoch: [2][175/204]	Loss 0.6806 (0.6900)	
training:	Epoch: [2][176/204]	Loss 0.7010 (0.6901)	
training:	Epoch: [2][177/204]	Loss 0.6896 (0.6901)	
training:	Epoch: [2][178/204]	Loss 0.6825 (0.6900)	
training:	Epoch: [2][179/204]	Loss 0.6896 (0.6900)	
training:	Epoch: [2][180/204]	Loss 0.6735 (0.6899)	
training:	Epoch: [2][181/204]	Loss 0.6987 (0.6900)	
training:	Epoch: [2][182/204]	Loss 0.6936 (0.6900)	
training:	Epoch: [2][183/204]	Loss 0.7030 (0.6901)	
training:	Epoch: [2][184/204]	Loss 0.7046 (0.6902)	
training:	Epoch: [2][185/204]	Loss 0.6886 (0.6901)	
training:	Epoch: [2][186/204]	Loss 0.6873 (0.6901)	
training:	Epoch: [2][187/204]	Loss 0.6717 (0.6900)	
training:	Epoch: [2][188/204]	Loss 0.6803 (0.6900)	
training:	Epoch: [2][189/204]	Loss 0.6977 (0.6900)	
training:	Epoch: [2][190/204]	Loss 0.6810 (0.6900)	
training:	Epoch: [2][191/204]	Loss 0.6964 (0.6900)	
training:	Epoch: [2][192/204]	Loss 0.6723 (0.6899)	
training:	Epoch: [2][193/204]	Loss 0.7034 (0.6900)	
training:	Epoch: [2][194/204]	Loss 0.6843 (0.6900)	
training:	Epoch: [2][195/204]	Loss 0.7017 (0.6900)	
training:	Epoch: [2][196/204]	Loss 0.6995 (0.6901)	
training:	Epoch: [2][197/204]	Loss 0.6750 (0.6900)	
training:	Epoch: [2][198/204]	Loss 0.7067 (0.6901)	
training:	Epoch: [2][199/204]	Loss 0.7129 (0.6902)	
training:	Epoch: [2][200/204]	Loss 0.6843 (0.6902)	
training:	Epoch: [2][201/204]	Loss 0.6942 (0.6902)	
training:	Epoch: [2][202/204]	Loss 0.6681 (0.6901)	
training:	Epoch: [2][203/204]	Loss 0.6995 (0.6901)	
training:	Epoch: [2][204/204]	Loss 0.6918 (0.6901)	
Training:	 Loss: 0.6891

Training:	 ACC: 0.5389 0.5480 0.7634 0.3144
Validation:	 ACC: 0.5458 0.5564 0.7810 0.3105
Validation:	 Best_BACC: 0.5581 0.5618 0.6397 0.4765
Validation:	 Loss: 0.6867
Pretraining:	Epoch 3/120
----------
training:	Epoch: [3][1/204]	Loss 0.7130 (0.7130)	
training:	Epoch: [3][2/204]	Loss 0.6750 (0.6940)	
training:	Epoch: [3][3/204]	Loss 0.7025 (0.6968)	
training:	Epoch: [3][4/204]	Loss 0.6911 (0.6954)	
training:	Epoch: [3][5/204]	Loss 0.7044 (0.6972)	
training:	Epoch: [3][6/204]	Loss 0.6940 (0.6967)	
training:	Epoch: [3][7/204]	Loss 0.6853 (0.6950)	
training:	Epoch: [3][8/204]	Loss 0.6859 (0.6939)	
training:	Epoch: [3][9/204]	Loss 0.6867 (0.6931)	
training:	Epoch: [3][10/204]	Loss 0.6723 (0.6910)	
training:	Epoch: [3][11/204]	Loss 0.7012 (0.6919)	
training:	Epoch: [3][12/204]	Loss 0.6798 (0.6909)	
training:	Epoch: [3][13/204]	Loss 0.6852 (0.6905)	
training:	Epoch: [3][14/204]	Loss 0.6928 (0.6907)	
training:	Epoch: [3][15/204]	Loss 0.6885 (0.6905)	
training:	Epoch: [3][16/204]	Loss 0.6835 (0.6901)	
training:	Epoch: [3][17/204]	Loss 0.6987 (0.6906)	
training:	Epoch: [3][18/204]	Loss 0.6897 (0.6905)	
training:	Epoch: [3][19/204]	Loss 0.6756 (0.6897)	
training:	Epoch: [3][20/204]	Loss 0.6799 (0.6893)	
training:	Epoch: [3][21/204]	Loss 0.6874 (0.6892)	
training:	Epoch: [3][22/204]	Loss 0.6998 (0.6896)	
training:	Epoch: [3][23/204]	Loss 0.6769 (0.6891)	
training:	Epoch: [3][24/204]	Loss 0.6595 (0.6879)	
training:	Epoch: [3][25/204]	Loss 0.6839 (0.6877)	
training:	Epoch: [3][26/204]	Loss 0.6910 (0.6878)	
training:	Epoch: [3][27/204]	Loss 0.6896 (0.6879)	
training:	Epoch: [3][28/204]	Loss 0.6982 (0.6883)	
training:	Epoch: [3][29/204]	Loss 0.6980 (0.6886)	
training:	Epoch: [3][30/204]	Loss 0.6843 (0.6885)	
training:	Epoch: [3][31/204]	Loss 0.6732 (0.6880)	
training:	Epoch: [3][32/204]	Loss 0.6816 (0.6878)	
training:	Epoch: [3][33/204]	Loss 0.7023 (0.6882)	
training:	Epoch: [3][34/204]	Loss 0.6741 (0.6878)	
training:	Epoch: [3][35/204]	Loss 0.6770 (0.6875)	
training:	Epoch: [3][36/204]	Loss 0.6676 (0.6869)	
training:	Epoch: [3][37/204]	Loss 0.6702 (0.6865)	
training:	Epoch: [3][38/204]	Loss 0.6778 (0.6863)	
training:	Epoch: [3][39/204]	Loss 0.7025 (0.6867)	
training:	Epoch: [3][40/204]	Loss 0.6948 (0.6869)	
training:	Epoch: [3][41/204]	Loss 0.6804 (0.6867)	
training:	Epoch: [3][42/204]	Loss 0.6811 (0.6866)	
training:	Epoch: [3][43/204]	Loss 0.7039 (0.6870)	
training:	Epoch: [3][44/204]	Loss 0.6798 (0.6868)	
training:	Epoch: [3][45/204]	Loss 0.6960 (0.6870)	
training:	Epoch: [3][46/204]	Loss 0.7030 (0.6874)	
training:	Epoch: [3][47/204]	Loss 0.7013 (0.6877)	
training:	Epoch: [3][48/204]	Loss 0.6877 (0.6877)	
training:	Epoch: [3][49/204]	Loss 0.6936 (0.6878)	
training:	Epoch: [3][50/204]	Loss 0.6847 (0.6877)	
training:	Epoch: [3][51/204]	Loss 0.6794 (0.6876)	
training:	Epoch: [3][52/204]	Loss 0.6954 (0.6877)	
training:	Epoch: [3][53/204]	Loss 0.6835 (0.6876)	
training:	Epoch: [3][54/204]	Loss 0.7159 (0.6882)	
training:	Epoch: [3][55/204]	Loss 0.7069 (0.6885)	
training:	Epoch: [3][56/204]	Loss 0.6808 (0.6884)	
training:	Epoch: [3][57/204]	Loss 0.6904 (0.6884)	
training:	Epoch: [3][58/204]	Loss 0.6959 (0.6885)	
training:	Epoch: [3][59/204]	Loss 0.6662 (0.6882)	
training:	Epoch: [3][60/204]	Loss 0.6955 (0.6883)	
training:	Epoch: [3][61/204]	Loss 0.6804 (0.6881)	
training:	Epoch: [3][62/204]	Loss 0.7073 (0.6885)	
training:	Epoch: [3][63/204]	Loss 0.7005 (0.6886)	
training:	Epoch: [3][64/204]	Loss 0.6845 (0.6886)	
training:	Epoch: [3][65/204]	Loss 0.6969 (0.6887)	
training:	Epoch: [3][66/204]	Loss 0.6886 (0.6887)	
training:	Epoch: [3][67/204]	Loss 0.6949 (0.6888)	
training:	Epoch: [3][68/204]	Loss 0.6762 (0.6886)	
training:	Epoch: [3][69/204]	Loss 0.6959 (0.6887)	
training:	Epoch: [3][70/204]	Loss 0.6709 (0.6885)	
training:	Epoch: [3][71/204]	Loss 0.6763 (0.6883)	
training:	Epoch: [3][72/204]	Loss 0.7017 (0.6885)	
training:	Epoch: [3][73/204]	Loss 0.6955 (0.6886)	
training:	Epoch: [3][74/204]	Loss 0.6920 (0.6886)	
training:	Epoch: [3][75/204]	Loss 0.6689 (0.6884)	
training:	Epoch: [3][76/204]	Loss 0.6794 (0.6882)	
training:	Epoch: [3][77/204]	Loss 0.6782 (0.6881)	
training:	Epoch: [3][78/204]	Loss 0.6986 (0.6882)	
training:	Epoch: [3][79/204]	Loss 0.7120 (0.6885)	
training:	Epoch: [3][80/204]	Loss 0.6943 (0.6886)	
training:	Epoch: [3][81/204]	Loss 0.7010 (0.6888)	
training:	Epoch: [3][82/204]	Loss 0.6863 (0.6887)	
training:	Epoch: [3][83/204]	Loss 0.6830 (0.6887)	
training:	Epoch: [3][84/204]	Loss 0.6855 (0.6886)	
training:	Epoch: [3][85/204]	Loss 0.6826 (0.6886)	
training:	Epoch: [3][86/204]	Loss 0.6941 (0.6886)	
training:	Epoch: [3][87/204]	Loss 0.6714 (0.6884)	
training:	Epoch: [3][88/204]	Loss 0.7146 (0.6887)	
training:	Epoch: [3][89/204]	Loss 0.6932 (0.6888)	
training:	Epoch: [3][90/204]	Loss 0.6795 (0.6887)	
training:	Epoch: [3][91/204]	Loss 0.6896 (0.6887)	
training:	Epoch: [3][92/204]	Loss 0.6925 (0.6887)	
training:	Epoch: [3][93/204]	Loss 0.7038 (0.6889)	
training:	Epoch: [3][94/204]	Loss 0.6731 (0.6887)	
training:	Epoch: [3][95/204]	Loss 0.6992 (0.6888)	
training:	Epoch: [3][96/204]	Loss 0.6898 (0.6888)	
training:	Epoch: [3][97/204]	Loss 0.6880 (0.6888)	
training:	Epoch: [3][98/204]	Loss 0.6819 (0.6888)	
training:	Epoch: [3][99/204]	Loss 0.6930 (0.6888)	
training:	Epoch: [3][100/204]	Loss 0.6917 (0.6888)	
training:	Epoch: [3][101/204]	Loss 0.6915 (0.6889)	
training:	Epoch: [3][102/204]	Loss 0.6678 (0.6887)	
training:	Epoch: [3][103/204]	Loss 0.6799 (0.6886)	
training:	Epoch: [3][104/204]	Loss 0.6971 (0.6886)	
training:	Epoch: [3][105/204]	Loss 0.6763 (0.6885)	
training:	Epoch: [3][106/204]	Loss 0.6747 (0.6884)	
training:	Epoch: [3][107/204]	Loss 0.6672 (0.6882)	
training:	Epoch: [3][108/204]	Loss 0.6681 (0.6880)	
training:	Epoch: [3][109/204]	Loss 0.6795 (0.6879)	
training:	Epoch: [3][110/204]	Loss 0.6913 (0.6880)	
training:	Epoch: [3][111/204]	Loss 0.7029 (0.6881)	
training:	Epoch: [3][112/204]	Loss 0.6689 (0.6879)	
training:	Epoch: [3][113/204]	Loss 0.6838 (0.6879)	
training:	Epoch: [3][114/204]	Loss 0.7022 (0.6880)	
training:	Epoch: [3][115/204]	Loss 0.7071 (0.6882)	
training:	Epoch: [3][116/204]	Loss 0.6822 (0.6881)	
training:	Epoch: [3][117/204]	Loss 0.6667 (0.6880)	
training:	Epoch: [3][118/204]	Loss 0.6715 (0.6878)	
training:	Epoch: [3][119/204]	Loss 0.6913 (0.6878)	
training:	Epoch: [3][120/204]	Loss 0.6785 (0.6878)	
training:	Epoch: [3][121/204]	Loss 0.6941 (0.6878)	
training:	Epoch: [3][122/204]	Loss 0.7006 (0.6879)	
training:	Epoch: [3][123/204]	Loss 0.6810 (0.6879)	
training:	Epoch: [3][124/204]	Loss 0.6734 (0.6877)	
training:	Epoch: [3][125/204]	Loss 0.6884 (0.6878)	
training:	Epoch: [3][126/204]	Loss 0.6821 (0.6877)	
training:	Epoch: [3][127/204]	Loss 0.6894 (0.6877)	
training:	Epoch: [3][128/204]	Loss 0.6870 (0.6877)	
training:	Epoch: [3][129/204]	Loss 0.6985 (0.6878)	
training:	Epoch: [3][130/204]	Loss 0.6868 (0.6878)	
training:	Epoch: [3][131/204]	Loss 0.6840 (0.6878)	
training:	Epoch: [3][132/204]	Loss 0.6742 (0.6877)	
training:	Epoch: [3][133/204]	Loss 0.6832 (0.6876)	
training:	Epoch: [3][134/204]	Loss 0.6941 (0.6877)	
training:	Epoch: [3][135/204]	Loss 0.6762 (0.6876)	
training:	Epoch: [3][136/204]	Loss 0.6718 (0.6875)	
training:	Epoch: [3][137/204]	Loss 0.6839 (0.6874)	
training:	Epoch: [3][138/204]	Loss 0.7050 (0.6876)	
training:	Epoch: [3][139/204]	Loss 0.6756 (0.6875)	
training:	Epoch: [3][140/204]	Loss 0.6934 (0.6875)	
training:	Epoch: [3][141/204]	Loss 0.6778 (0.6875)	
training:	Epoch: [3][142/204]	Loss 0.6622 (0.6873)	
training:	Epoch: [3][143/204]	Loss 0.7056 (0.6874)	
training:	Epoch: [3][144/204]	Loss 0.6863 (0.6874)	
training:	Epoch: [3][145/204]	Loss 0.7013 (0.6875)	
training:	Epoch: [3][146/204]	Loss 0.6912 (0.6875)	
training:	Epoch: [3][147/204]	Loss 0.6754 (0.6874)	
training:	Epoch: [3][148/204]	Loss 0.6852 (0.6874)	
training:	Epoch: [3][149/204]	Loss 0.6966 (0.6875)	
training:	Epoch: [3][150/204]	Loss 0.6931 (0.6875)	
training:	Epoch: [3][151/204]	Loss 0.6888 (0.6875)	
training:	Epoch: [3][152/204]	Loss 0.7037 (0.6876)	
training:	Epoch: [3][153/204]	Loss 0.6913 (0.6877)	
training:	Epoch: [3][154/204]	Loss 0.6911 (0.6877)	
training:	Epoch: [3][155/204]	Loss 0.6980 (0.6878)	
training:	Epoch: [3][156/204]	Loss 0.6764 (0.6877)	
training:	Epoch: [3][157/204]	Loss 0.7028 (0.6878)	
training:	Epoch: [3][158/204]	Loss 0.6959 (0.6878)	
training:	Epoch: [3][159/204]	Loss 0.6965 (0.6879)	
training:	Epoch: [3][160/204]	Loss 0.6760 (0.6878)	
training:	Epoch: [3][161/204]	Loss 0.6828 (0.6878)	
training:	Epoch: [3][162/204]	Loss 0.6939 (0.6878)	
training:	Epoch: [3][163/204]	Loss 0.7249 (0.6880)	
training:	Epoch: [3][164/204]	Loss 0.7014 (0.6881)	
training:	Epoch: [3][165/204]	Loss 0.6911 (0.6881)	
training:	Epoch: [3][166/204]	Loss 0.7049 (0.6882)	
training:	Epoch: [3][167/204]	Loss 0.6798 (0.6882)	
training:	Epoch: [3][168/204]	Loss 0.6874 (0.6882)	
training:	Epoch: [3][169/204]	Loss 0.6886 (0.6882)	
training:	Epoch: [3][170/204]	Loss 0.6820 (0.6882)	
training:	Epoch: [3][171/204]	Loss 0.6802 (0.6881)	
training:	Epoch: [3][172/204]	Loss 0.6886 (0.6881)	
training:	Epoch: [3][173/204]	Loss 0.6962 (0.6882)	
training:	Epoch: [3][174/204]	Loss 0.6785 (0.6881)	
training:	Epoch: [3][175/204]	Loss 0.6781 (0.6880)	
training:	Epoch: [3][176/204]	Loss 0.6758 (0.6880)	
training:	Epoch: [3][177/204]	Loss 0.6790 (0.6879)	
training:	Epoch: [3][178/204]	Loss 0.6695 (0.6878)	
training:	Epoch: [3][179/204]	Loss 0.6911 (0.6878)	
training:	Epoch: [3][180/204]	Loss 0.6809 (0.6878)	
training:	Epoch: [3][181/204]	Loss 0.6951 (0.6878)	
training:	Epoch: [3][182/204]	Loss 0.6988 (0.6879)	
training:	Epoch: [3][183/204]	Loss 0.6730 (0.6878)	
training:	Epoch: [3][184/204]	Loss 0.6939 (0.6879)	
training:	Epoch: [3][185/204]	Loss 0.6808 (0.6878)	
training:	Epoch: [3][186/204]	Loss 0.6854 (0.6878)	
training:	Epoch: [3][187/204]	Loss 0.6804 (0.6878)	
training:	Epoch: [3][188/204]	Loss 0.6829 (0.6877)	
training:	Epoch: [3][189/204]	Loss 0.6957 (0.6878)	
training:	Epoch: [3][190/204]	Loss 0.7084 (0.6879)	
training:	Epoch: [3][191/204]	Loss 0.6873 (0.6879)	
training:	Epoch: [3][192/204]	Loss 0.7013 (0.6880)	
training:	Epoch: [3][193/204]	Loss 0.6960 (0.6880)	
training:	Epoch: [3][194/204]	Loss 0.7035 (0.6881)	
training:	Epoch: [3][195/204]	Loss 0.6847 (0.6881)	
training:	Epoch: [3][196/204]	Loss 0.6835 (0.6880)	
training:	Epoch: [3][197/204]	Loss 0.6809 (0.6880)	
training:	Epoch: [3][198/204]	Loss 0.6970 (0.6880)	
training:	Epoch: [3][199/204]	Loss 0.6654 (0.6879)	
training:	Epoch: [3][200/204]	Loss 0.7150 (0.6881)	
training:	Epoch: [3][201/204]	Loss 0.6918 (0.6881)	
training:	Epoch: [3][202/204]	Loss 0.6891 (0.6881)	
training:	Epoch: [3][203/204]	Loss 0.6583 (0.6879)	
training:	Epoch: [3][204/204]	Loss 0.6833 (0.6879)	
Training:	 Loss: 0.6869

Training:	 ACC: 0.5514 0.5609 0.7848 0.3179
Validation:	 ACC: 0.5539 0.5650 0.7984 0.3094
Validation:	 Best_BACC: 0.5581 0.5618 0.6397 0.4765
Validation:	 Loss: 0.6849
Pretraining:	Epoch 4/120
----------
training:	Epoch: [4][1/204]	Loss 0.6733 (0.6733)	
training:	Epoch: [4][2/204]	Loss 0.6883 (0.6808)	
training:	Epoch: [4][3/204]	Loss 0.6973 (0.6863)	
training:	Epoch: [4][4/204]	Loss 0.7256 (0.6962)	
training:	Epoch: [4][5/204]	Loss 0.7020 (0.6973)	
training:	Epoch: [4][6/204]	Loss 0.7088 (0.6992)	
training:	Epoch: [4][7/204]	Loss 0.6817 (0.6967)	
training:	Epoch: [4][8/204]	Loss 0.6897 (0.6958)	
training:	Epoch: [4][9/204]	Loss 0.6835 (0.6945)	
training:	Epoch: [4][10/204]	Loss 0.6836 (0.6934)	
training:	Epoch: [4][11/204]	Loss 0.6727 (0.6915)	
training:	Epoch: [4][12/204]	Loss 0.6778 (0.6904)	
training:	Epoch: [4][13/204]	Loss 0.6867 (0.6901)	
training:	Epoch: [4][14/204]	Loss 0.6925 (0.6903)	
training:	Epoch: [4][15/204]	Loss 0.7067 (0.6914)	
training:	Epoch: [4][16/204]	Loss 0.6798 (0.6906)	
training:	Epoch: [4][17/204]	Loss 0.6770 (0.6898)	
training:	Epoch: [4][18/204]	Loss 0.6831 (0.6895)	
training:	Epoch: [4][19/204]	Loss 0.6848 (0.6892)	
training:	Epoch: [4][20/204]	Loss 0.6889 (0.6892)	
training:	Epoch: [4][21/204]	Loss 0.6778 (0.6886)	
training:	Epoch: [4][22/204]	Loss 0.6959 (0.6890)	
training:	Epoch: [4][23/204]	Loss 0.6751 (0.6884)	
training:	Epoch: [4][24/204]	Loss 0.6786 (0.6880)	
training:	Epoch: [4][25/204]	Loss 0.7053 (0.6887)	
training:	Epoch: [4][26/204]	Loss 0.6662 (0.6878)	
training:	Epoch: [4][27/204]	Loss 0.7001 (0.6882)	
training:	Epoch: [4][28/204]	Loss 0.7076 (0.6889)	
training:	Epoch: [4][29/204]	Loss 0.6802 (0.6886)	
training:	Epoch: [4][30/204]	Loss 0.6882 (0.6886)	
training:	Epoch: [4][31/204]	Loss 0.6636 (0.6878)	
training:	Epoch: [4][32/204]	Loss 0.6907 (0.6879)	
training:	Epoch: [4][33/204]	Loss 0.6870 (0.6879)	
training:	Epoch: [4][34/204]	Loss 0.6843 (0.6878)	
training:	Epoch: [4][35/204]	Loss 0.7071 (0.6883)	
training:	Epoch: [4][36/204]	Loss 0.7236 (0.6893)	
training:	Epoch: [4][37/204]	Loss 0.6662 (0.6887)	
training:	Epoch: [4][38/204]	Loss 0.7076 (0.6892)	
training:	Epoch: [4][39/204]	Loss 0.6973 (0.6894)	
training:	Epoch: [4][40/204]	Loss 0.6629 (0.6887)	
training:	Epoch: [4][41/204]	Loss 0.6909 (0.6888)	
training:	Epoch: [4][42/204]	Loss 0.6994 (0.6890)	
training:	Epoch: [4][43/204]	Loss 0.6985 (0.6892)	
training:	Epoch: [4][44/204]	Loss 0.6778 (0.6890)	
training:	Epoch: [4][45/204]	Loss 0.6748 (0.6887)	
training:	Epoch: [4][46/204]	Loss 0.6750 (0.6884)	
training:	Epoch: [4][47/204]	Loss 0.6951 (0.6885)	
training:	Epoch: [4][48/204]	Loss 0.6951 (0.6887)	
training:	Epoch: [4][49/204]	Loss 0.6923 (0.6887)	
training:	Epoch: [4][50/204]	Loss 0.7071 (0.6891)	
training:	Epoch: [4][51/204]	Loss 0.6873 (0.6891)	
training:	Epoch: [4][52/204]	Loss 0.6909 (0.6891)	
training:	Epoch: [4][53/204]	Loss 0.6797 (0.6889)	
training:	Epoch: [4][54/204]	Loss 0.6814 (0.6888)	
training:	Epoch: [4][55/204]	Loss 0.6988 (0.6890)	
training:	Epoch: [4][56/204]	Loss 0.6667 (0.6886)	
training:	Epoch: [4][57/204]	Loss 0.6910 (0.6886)	
training:	Epoch: [4][58/204]	Loss 0.6799 (0.6885)	
training:	Epoch: [4][59/204]	Loss 0.6944 (0.6886)	
training:	Epoch: [4][60/204]	Loss 0.6925 (0.6886)	
training:	Epoch: [4][61/204]	Loss 0.6696 (0.6883)	
training:	Epoch: [4][62/204]	Loss 0.6687 (0.6880)	
training:	Epoch: [4][63/204]	Loss 0.6692 (0.6877)	
training:	Epoch: [4][64/204]	Loss 0.6939 (0.6878)	
training:	Epoch: [4][65/204]	Loss 0.6843 (0.6877)	
training:	Epoch: [4][66/204]	Loss 0.6976 (0.6879)	
training:	Epoch: [4][67/204]	Loss 0.7229 (0.6884)	
training:	Epoch: [4][68/204]	Loss 0.6944 (0.6885)	
training:	Epoch: [4][69/204]	Loss 0.6858 (0.6885)	
training:	Epoch: [4][70/204]	Loss 0.6940 (0.6885)	
training:	Epoch: [4][71/204]	Loss 0.6866 (0.6885)	
training:	Epoch: [4][72/204]	Loss 0.6789 (0.6884)	
training:	Epoch: [4][73/204]	Loss 0.6587 (0.6880)	
training:	Epoch: [4][74/204]	Loss 0.6955 (0.6881)	
training:	Epoch: [4][75/204]	Loss 0.6907 (0.6881)	
training:	Epoch: [4][76/204]	Loss 0.6785 (0.6880)	
training:	Epoch: [4][77/204]	Loss 0.6708 (0.6878)	
training:	Epoch: [4][78/204]	Loss 0.6854 (0.6877)	
training:	Epoch: [4][79/204]	Loss 0.6945 (0.6878)	
training:	Epoch: [4][80/204]	Loss 0.6965 (0.6879)	
training:	Epoch: [4][81/204]	Loss 0.6901 (0.6880)	
training:	Epoch: [4][82/204]	Loss 0.6915 (0.6880)	
training:	Epoch: [4][83/204]	Loss 0.6735 (0.6878)	
training:	Epoch: [4][84/204]	Loss 0.6818 (0.6877)	
training:	Epoch: [4][85/204]	Loss 0.6769 (0.6876)	
training:	Epoch: [4][86/204]	Loss 0.6744 (0.6875)	
training:	Epoch: [4][87/204]	Loss 0.6895 (0.6875)	
training:	Epoch: [4][88/204]	Loss 0.6680 (0.6873)	
training:	Epoch: [4][89/204]	Loss 0.6990 (0.6874)	
training:	Epoch: [4][90/204]	Loss 0.6623 (0.6871)	
training:	Epoch: [4][91/204]	Loss 0.6997 (0.6873)	
training:	Epoch: [4][92/204]	Loss 0.6642 (0.6870)	
training:	Epoch: [4][93/204]	Loss 0.6799 (0.6869)	
training:	Epoch: [4][94/204]	Loss 0.6823 (0.6869)	
training:	Epoch: [4][95/204]	Loss 0.7045 (0.6871)	
training:	Epoch: [4][96/204]	Loss 0.6782 (0.6870)	
training:	Epoch: [4][97/204]	Loss 0.6950 (0.6871)	
training:	Epoch: [4][98/204]	Loss 0.6974 (0.6872)	
training:	Epoch: [4][99/204]	Loss 0.6846 (0.6871)	
training:	Epoch: [4][100/204]	Loss 0.6987 (0.6873)	
training:	Epoch: [4][101/204]	Loss 0.6760 (0.6871)	
training:	Epoch: [4][102/204]	Loss 0.6552 (0.6868)	
training:	Epoch: [4][103/204]	Loss 0.6808 (0.6868)	
training:	Epoch: [4][104/204]	Loss 0.6992 (0.6869)	
training:	Epoch: [4][105/204]	Loss 0.6679 (0.6867)	
training:	Epoch: [4][106/204]	Loss 0.6900 (0.6867)	
training:	Epoch: [4][107/204]	Loss 0.6948 (0.6868)	
training:	Epoch: [4][108/204]	Loss 0.6641 (0.6866)	
training:	Epoch: [4][109/204]	Loss 0.6932 (0.6867)	
training:	Epoch: [4][110/204]	Loss 0.6856 (0.6867)	
training:	Epoch: [4][111/204]	Loss 0.7002 (0.6868)	
training:	Epoch: [4][112/204]	Loss 0.6937 (0.6868)	
training:	Epoch: [4][113/204]	Loss 0.6811 (0.6868)	
training:	Epoch: [4][114/204]	Loss 0.6970 (0.6869)	
training:	Epoch: [4][115/204]	Loss 0.6667 (0.6867)	
training:	Epoch: [4][116/204]	Loss 0.6769 (0.6866)	
training:	Epoch: [4][117/204]	Loss 0.6816 (0.6866)	
training:	Epoch: [4][118/204]	Loss 0.6747 (0.6865)	
training:	Epoch: [4][119/204]	Loss 0.6893 (0.6865)	
training:	Epoch: [4][120/204]	Loss 0.6840 (0.6865)	
training:	Epoch: [4][121/204]	Loss 0.7072 (0.6866)	
training:	Epoch: [4][122/204]	Loss 0.6737 (0.6865)	
training:	Epoch: [4][123/204]	Loss 0.6963 (0.6866)	
training:	Epoch: [4][124/204]	Loss 0.6884 (0.6866)	
training:	Epoch: [4][125/204]	Loss 0.6563 (0.6864)	
training:	Epoch: [4][126/204]	Loss 0.6880 (0.6864)	
training:	Epoch: [4][127/204]	Loss 0.6627 (0.6862)	
training:	Epoch: [4][128/204]	Loss 0.6878 (0.6862)	
training:	Epoch: [4][129/204]	Loss 0.6847 (0.6862)	
training:	Epoch: [4][130/204]	Loss 0.6643 (0.6861)	
training:	Epoch: [4][131/204]	Loss 0.6727 (0.6859)	
training:	Epoch: [4][132/204]	Loss 0.6737 (0.6859)	
training:	Epoch: [4][133/204]	Loss 0.6873 (0.6859)	
training:	Epoch: [4][134/204]	Loss 0.6884 (0.6859)	
training:	Epoch: [4][135/204]	Loss 0.6691 (0.6858)	
training:	Epoch: [4][136/204]	Loss 0.6761 (0.6857)	
training:	Epoch: [4][137/204]	Loss 0.6852 (0.6857)	
training:	Epoch: [4][138/204]	Loss 0.6833 (0.6857)	
training:	Epoch: [4][139/204]	Loss 0.6792 (0.6856)	
training:	Epoch: [4][140/204]	Loss 0.6748 (0.6855)	
training:	Epoch: [4][141/204]	Loss 0.6877 (0.6856)	
training:	Epoch: [4][142/204]	Loss 0.6982 (0.6857)	
training:	Epoch: [4][143/204]	Loss 0.6735 (0.6856)	
training:	Epoch: [4][144/204]	Loss 0.6802 (0.6855)	
training:	Epoch: [4][145/204]	Loss 0.7034 (0.6857)	
training:	Epoch: [4][146/204]	Loss 0.7098 (0.6858)	
training:	Epoch: [4][147/204]	Loss 0.6982 (0.6859)	
training:	Epoch: [4][148/204]	Loss 0.7014 (0.6860)	
training:	Epoch: [4][149/204]	Loss 0.7025 (0.6861)	
training:	Epoch: [4][150/204]	Loss 0.6887 (0.6861)	
training:	Epoch: [4][151/204]	Loss 0.6851 (0.6861)	
training:	Epoch: [4][152/204]	Loss 0.7000 (0.6862)	
training:	Epoch: [4][153/204]	Loss 0.6922 (0.6863)	
training:	Epoch: [4][154/204]	Loss 0.6794 (0.6862)	
training:	Epoch: [4][155/204]	Loss 0.6879 (0.6862)	
training:	Epoch: [4][156/204]	Loss 0.6887 (0.6862)	
training:	Epoch: [4][157/204]	Loss 0.6814 (0.6862)	
training:	Epoch: [4][158/204]	Loss 0.6782 (0.6862)	
training:	Epoch: [4][159/204]	Loss 0.6969 (0.6862)	
training:	Epoch: [4][160/204]	Loss 0.7073 (0.6864)	
training:	Epoch: [4][161/204]	Loss 0.6908 (0.6864)	
training:	Epoch: [4][162/204]	Loss 0.7010 (0.6865)	
training:	Epoch: [4][163/204]	Loss 0.6973 (0.6865)	
training:	Epoch: [4][164/204]	Loss 0.6970 (0.6866)	
training:	Epoch: [4][165/204]	Loss 0.6836 (0.6866)	
training:	Epoch: [4][166/204]	Loss 0.6756 (0.6865)	
training:	Epoch: [4][167/204]	Loss 0.6693 (0.6864)	
training:	Epoch: [4][168/204]	Loss 0.7000 (0.6865)	
training:	Epoch: [4][169/204]	Loss 0.6718 (0.6864)	
training:	Epoch: [4][170/204]	Loss 0.6922 (0.6864)	
training:	Epoch: [4][171/204]	Loss 0.6845 (0.6864)	
training:	Epoch: [4][172/204]	Loss 0.6755 (0.6864)	
training:	Epoch: [4][173/204]	Loss 0.6839 (0.6864)	
training:	Epoch: [4][174/204]	Loss 0.6677 (0.6862)	
training:	Epoch: [4][175/204]	Loss 0.6885 (0.6863)	
training:	Epoch: [4][176/204]	Loss 0.6885 (0.6863)	
training:	Epoch: [4][177/204]	Loss 0.6634 (0.6861)	
training:	Epoch: [4][178/204]	Loss 0.6660 (0.6860)	
training:	Epoch: [4][179/204]	Loss 0.6934 (0.6861)	
training:	Epoch: [4][180/204]	Loss 0.6859 (0.6861)	
training:	Epoch: [4][181/204]	Loss 0.6837 (0.6861)	
training:	Epoch: [4][182/204]	Loss 0.6848 (0.6861)	
training:	Epoch: [4][183/204]	Loss 0.6983 (0.6861)	
training:	Epoch: [4][184/204]	Loss 0.6861 (0.6861)	
training:	Epoch: [4][185/204]	Loss 0.6571 (0.6860)	
training:	Epoch: [4][186/204]	Loss 0.6870 (0.6860)	
training:	Epoch: [4][187/204]	Loss 0.6741 (0.6859)	
training:	Epoch: [4][188/204]	Loss 0.6701 (0.6858)	
training:	Epoch: [4][189/204]	Loss 0.6900 (0.6858)	
training:	Epoch: [4][190/204]	Loss 0.6929 (0.6859)	
training:	Epoch: [4][191/204]	Loss 0.6972 (0.6859)	
training:	Epoch: [4][192/204]	Loss 0.6963 (0.6860)	
training:	Epoch: [4][193/204]	Loss 0.7053 (0.6861)	
training:	Epoch: [4][194/204]	Loss 0.6782 (0.6861)	
training:	Epoch: [4][195/204]	Loss 0.6846 (0.6860)	
training:	Epoch: [4][196/204]	Loss 0.6856 (0.6860)	
training:	Epoch: [4][197/204]	Loss 0.6747 (0.6860)	
training:	Epoch: [4][198/204]	Loss 0.6916 (0.6860)	
training:	Epoch: [4][199/204]	Loss 0.6723 (0.6859)	
training:	Epoch: [4][200/204]	Loss 0.7041 (0.6860)	
training:	Epoch: [4][201/204]	Loss 0.6949 (0.6861)	
training:	Epoch: [4][202/204]	Loss 0.6836 (0.6861)	
training:	Epoch: [4][203/204]	Loss 0.6732 (0.6860)	
training:	Epoch: [4][204/204]	Loss 0.6815 (0.6860)	
Training:	 Loss: 0.6849

Training:	 ACC: 0.5666 0.5754 0.7822 0.3511
Validation:	 ACC: 0.5693 0.5795 0.7932 0.3453
Validation:	 Best_BACC: 0.5693 0.5795 0.7932 0.3453
Validation:	 Loss: 0.6831
Pretraining:	Epoch 5/120
----------
training:	Epoch: [5][1/204]	Loss 0.6641 (0.6641)	
training:	Epoch: [5][2/204]	Loss 0.6853 (0.6747)	
training:	Epoch: [5][3/204]	Loss 0.6673 (0.6722)	
training:	Epoch: [5][4/204]	Loss 0.6639 (0.6701)	
training:	Epoch: [5][5/204]	Loss 0.6753 (0.6712)	
training:	Epoch: [5][6/204]	Loss 0.6895 (0.6742)	
training:	Epoch: [5][7/204]	Loss 0.6794 (0.6750)	
training:	Epoch: [5][8/204]	Loss 0.6783 (0.6754)	
training:	Epoch: [5][9/204]	Loss 0.7084 (0.6791)	
training:	Epoch: [5][10/204]	Loss 0.6919 (0.6803)	
training:	Epoch: [5][11/204]	Loss 0.6948 (0.6816)	
training:	Epoch: [5][12/204]	Loss 0.6821 (0.6817)	
training:	Epoch: [5][13/204]	Loss 0.6683 (0.6807)	
training:	Epoch: [5][14/204]	Loss 0.6820 (0.6808)	
training:	Epoch: [5][15/204]	Loss 0.6723 (0.6802)	
training:	Epoch: [5][16/204]	Loss 0.7000 (0.6814)	
training:	Epoch: [5][17/204]	Loss 0.6787 (0.6813)	
training:	Epoch: [5][18/204]	Loss 0.6802 (0.6812)	
training:	Epoch: [5][19/204]	Loss 0.6855 (0.6814)	
training:	Epoch: [5][20/204]	Loss 0.6724 (0.6810)	
training:	Epoch: [5][21/204]	Loss 0.7016 (0.6820)	
training:	Epoch: [5][22/204]	Loss 0.6896 (0.6823)	
training:	Epoch: [5][23/204]	Loss 0.6981 (0.6830)	
training:	Epoch: [5][24/204]	Loss 0.6938 (0.6834)	
training:	Epoch: [5][25/204]	Loss 0.6982 (0.6840)	
training:	Epoch: [5][26/204]	Loss 0.6665 (0.6834)	
training:	Epoch: [5][27/204]	Loss 0.7102 (0.6843)	
training:	Epoch: [5][28/204]	Loss 0.6867 (0.6844)	
training:	Epoch: [5][29/204]	Loss 0.6890 (0.6846)	
training:	Epoch: [5][30/204]	Loss 0.6966 (0.6850)	
training:	Epoch: [5][31/204]	Loss 0.6907 (0.6852)	
training:	Epoch: [5][32/204]	Loss 0.6784 (0.6850)	
training:	Epoch: [5][33/204]	Loss 0.6872 (0.6850)	
training:	Epoch: [5][34/204]	Loss 0.6975 (0.6854)	
training:	Epoch: [5][35/204]	Loss 0.6718 (0.6850)	
training:	Epoch: [5][36/204]	Loss 0.6927 (0.6852)	
training:	Epoch: [5][37/204]	Loss 0.6923 (0.6854)	
training:	Epoch: [5][38/204]	Loss 0.6874 (0.6855)	
training:	Epoch: [5][39/204]	Loss 0.6781 (0.6853)	
training:	Epoch: [5][40/204]	Loss 0.6821 (0.6852)	
training:	Epoch: [5][41/204]	Loss 0.6851 (0.6852)	
training:	Epoch: [5][42/204]	Loss 0.6877 (0.6853)	
training:	Epoch: [5][43/204]	Loss 0.7018 (0.6856)	
training:	Epoch: [5][44/204]	Loss 0.6627 (0.6851)	
training:	Epoch: [5][45/204]	Loss 0.6826 (0.6851)	
training:	Epoch: [5][46/204]	Loss 0.6842 (0.6850)	
training:	Epoch: [5][47/204]	Loss 0.6756 (0.6848)	
training:	Epoch: [5][48/204]	Loss 0.6809 (0.6848)	
training:	Epoch: [5][49/204]	Loss 0.6923 (0.6849)	
training:	Epoch: [5][50/204]	Loss 0.6878 (0.6850)	
training:	Epoch: [5][51/204]	Loss 0.6850 (0.6850)	
training:	Epoch: [5][52/204]	Loss 0.6886 (0.6850)	
training:	Epoch: [5][53/204]	Loss 0.6854 (0.6851)	
training:	Epoch: [5][54/204]	Loss 0.6768 (0.6849)	
training:	Epoch: [5][55/204]	Loss 0.6767 (0.6847)	
training:	Epoch: [5][56/204]	Loss 0.6829 (0.6847)	
training:	Epoch: [5][57/204]	Loss 0.6692 (0.6844)	
training:	Epoch: [5][58/204]	Loss 0.6889 (0.6845)	
training:	Epoch: [5][59/204]	Loss 0.6866 (0.6846)	
training:	Epoch: [5][60/204]	Loss 0.6784 (0.6845)	
training:	Epoch: [5][61/204]	Loss 0.6812 (0.6844)	
training:	Epoch: [5][62/204]	Loss 0.6730 (0.6842)	
training:	Epoch: [5][63/204]	Loss 0.6939 (0.6844)	
training:	Epoch: [5][64/204]	Loss 0.6733 (0.6842)	
training:	Epoch: [5][65/204]	Loss 0.6812 (0.6842)	
training:	Epoch: [5][66/204]	Loss 0.6904 (0.6842)	
training:	Epoch: [5][67/204]	Loss 0.6837 (0.6842)	
training:	Epoch: [5][68/204]	Loss 0.6898 (0.6843)	
training:	Epoch: [5][69/204]	Loss 0.6790 (0.6842)	
training:	Epoch: [5][70/204]	Loss 0.6801 (0.6842)	
training:	Epoch: [5][71/204]	Loss 0.7040 (0.6845)	
training:	Epoch: [5][72/204]	Loss 0.6633 (0.6842)	
training:	Epoch: [5][73/204]	Loss 0.6824 (0.6841)	
training:	Epoch: [5][74/204]	Loss 0.6701 (0.6840)	
training:	Epoch: [5][75/204]	Loss 0.6843 (0.6840)	
training:	Epoch: [5][76/204]	Loss 0.6782 (0.6839)	
training:	Epoch: [5][77/204]	Loss 0.6780 (0.6838)	
training:	Epoch: [5][78/204]	Loss 0.7044 (0.6841)	
training:	Epoch: [5][79/204]	Loss 0.6905 (0.6842)	
training:	Epoch: [5][80/204]	Loss 0.6739 (0.6840)	
training:	Epoch: [5][81/204]	Loss 0.6596 (0.6837)	
training:	Epoch: [5][82/204]	Loss 0.7016 (0.6839)	
training:	Epoch: [5][83/204]	Loss 0.6899 (0.6840)	
training:	Epoch: [5][84/204]	Loss 0.6772 (0.6839)	
training:	Epoch: [5][85/204]	Loss 0.6983 (0.6841)	
training:	Epoch: [5][86/204]	Loss 0.6729 (0.6840)	
training:	Epoch: [5][87/204]	Loss 0.6862 (0.6840)	
training:	Epoch: [5][88/204]	Loss 0.6871 (0.6840)	
training:	Epoch: [5][89/204]	Loss 0.7063 (0.6843)	
training:	Epoch: [5][90/204]	Loss 0.6858 (0.6843)	
training:	Epoch: [5][91/204]	Loss 0.6936 (0.6844)	
training:	Epoch: [5][92/204]	Loss 0.6862 (0.6844)	
training:	Epoch: [5][93/204]	Loss 0.6836 (0.6844)	
training:	Epoch: [5][94/204]	Loss 0.6775 (0.6843)	
training:	Epoch: [5][95/204]	Loss 0.6849 (0.6843)	
training:	Epoch: [5][96/204]	Loss 0.6754 (0.6842)	
training:	Epoch: [5][97/204]	Loss 0.6850 (0.6843)	
training:	Epoch: [5][98/204]	Loss 0.6886 (0.6843)	
training:	Epoch: [5][99/204]	Loss 0.6720 (0.6842)	
training:	Epoch: [5][100/204]	Loss 0.6894 (0.6842)	
training:	Epoch: [5][101/204]	Loss 0.6944 (0.6843)	
training:	Epoch: [5][102/204]	Loss 0.7092 (0.6846)	
training:	Epoch: [5][103/204]	Loss 0.6680 (0.6844)	
training:	Epoch: [5][104/204]	Loss 0.6764 (0.6843)	
training:	Epoch: [5][105/204]	Loss 0.6794 (0.6843)	
training:	Epoch: [5][106/204]	Loss 0.6770 (0.6842)	
training:	Epoch: [5][107/204]	Loss 0.6770 (0.6842)	
training:	Epoch: [5][108/204]	Loss 0.6842 (0.6842)	
training:	Epoch: [5][109/204]	Loss 0.6742 (0.6841)	
training:	Epoch: [5][110/204]	Loss 0.6957 (0.6842)	
training:	Epoch: [5][111/204]	Loss 0.6787 (0.6841)	
training:	Epoch: [5][112/204]	Loss 0.6906 (0.6842)	
training:	Epoch: [5][113/204]	Loss 0.6654 (0.6840)	
training:	Epoch: [5][114/204]	Loss 0.6563 (0.6838)	
training:	Epoch: [5][115/204]	Loss 0.6746 (0.6837)	
training:	Epoch: [5][116/204]	Loss 0.6729 (0.6836)	
training:	Epoch: [5][117/204]	Loss 0.7079 (0.6838)	
training:	Epoch: [5][118/204]	Loss 0.6696 (0.6837)	
training:	Epoch: [5][119/204]	Loss 0.6878 (0.6837)	
training:	Epoch: [5][120/204]	Loss 0.6937 (0.6838)	
training:	Epoch: [5][121/204]	Loss 0.6953 (0.6839)	
training:	Epoch: [5][122/204]	Loss 0.6809 (0.6839)	
training:	Epoch: [5][123/204]	Loss 0.6873 (0.6839)	
training:	Epoch: [5][124/204]	Loss 0.6686 (0.6838)	
training:	Epoch: [5][125/204]	Loss 0.6639 (0.6836)	
training:	Epoch: [5][126/204]	Loss 0.6889 (0.6837)	
training:	Epoch: [5][127/204]	Loss 0.6742 (0.6836)	
training:	Epoch: [5][128/204]	Loss 0.6785 (0.6835)	
training:	Epoch: [5][129/204]	Loss 0.6850 (0.6836)	
training:	Epoch: [5][130/204]	Loss 0.6769 (0.6835)	
training:	Epoch: [5][131/204]	Loss 0.7030 (0.6837)	
training:	Epoch: [5][132/204]	Loss 0.6893 (0.6837)	
training:	Epoch: [5][133/204]	Loss 0.7063 (0.6839)	
training:	Epoch: [5][134/204]	Loss 0.6761 (0.6838)	
training:	Epoch: [5][135/204]	Loss 0.6723 (0.6837)	
training:	Epoch: [5][136/204]	Loss 0.6874 (0.6838)	
training:	Epoch: [5][137/204]	Loss 0.6690 (0.6836)	
training:	Epoch: [5][138/204]	Loss 0.6795 (0.6836)	
training:	Epoch: [5][139/204]	Loss 0.6680 (0.6835)	
training:	Epoch: [5][140/204]	Loss 0.7087 (0.6837)	
training:	Epoch: [5][141/204]	Loss 0.6774 (0.6836)	
training:	Epoch: [5][142/204]	Loss 0.6724 (0.6836)	
training:	Epoch: [5][143/204]	Loss 0.6532 (0.6833)	
training:	Epoch: [5][144/204]	Loss 0.6616 (0.6832)	
training:	Epoch: [5][145/204]	Loss 0.6797 (0.6832)	
training:	Epoch: [5][146/204]	Loss 0.6628 (0.6830)	
training:	Epoch: [5][147/204]	Loss 0.6891 (0.6831)	
training:	Epoch: [5][148/204]	Loss 0.6911 (0.6831)	
training:	Epoch: [5][149/204]	Loss 0.6776 (0.6831)	
training:	Epoch: [5][150/204]	Loss 0.6653 (0.6830)	
training:	Epoch: [5][151/204]	Loss 0.6956 (0.6831)	
training:	Epoch: [5][152/204]	Loss 0.6941 (0.6831)	
training:	Epoch: [5][153/204]	Loss 0.6993 (0.6832)	
training:	Epoch: [5][154/204]	Loss 0.6741 (0.6832)	
training:	Epoch: [5][155/204]	Loss 0.6891 (0.6832)	
training:	Epoch: [5][156/204]	Loss 0.6845 (0.6832)	
training:	Epoch: [5][157/204]	Loss 0.6936 (0.6833)	
training:	Epoch: [5][158/204]	Loss 0.6602 (0.6831)	
training:	Epoch: [5][159/204]	Loss 0.6621 (0.6830)	
training:	Epoch: [5][160/204]	Loss 0.6702 (0.6829)	
training:	Epoch: [5][161/204]	Loss 0.6810 (0.6829)	
training:	Epoch: [5][162/204]	Loss 0.6776 (0.6829)	
training:	Epoch: [5][163/204]	Loss 0.6950 (0.6830)	
training:	Epoch: [5][164/204]	Loss 0.6820 (0.6829)	
training:	Epoch: [5][165/204]	Loss 0.6865 (0.6830)	
training:	Epoch: [5][166/204]	Loss 0.6661 (0.6829)	
training:	Epoch: [5][167/204]	Loss 0.6887 (0.6829)	
training:	Epoch: [5][168/204]	Loss 0.6986 (0.6830)	
training:	Epoch: [5][169/204]	Loss 0.6727 (0.6829)	
training:	Epoch: [5][170/204]	Loss 0.6815 (0.6829)	
training:	Epoch: [5][171/204]	Loss 0.6722 (0.6829)	
training:	Epoch: [5][172/204]	Loss 0.6919 (0.6829)	
training:	Epoch: [5][173/204]	Loss 0.6849 (0.6829)	
training:	Epoch: [5][174/204]	Loss 0.6650 (0.6828)	
training:	Epoch: [5][175/204]	Loss 0.6837 (0.6828)	
training:	Epoch: [5][176/204]	Loss 0.6897 (0.6829)	
training:	Epoch: [5][177/204]	Loss 0.6857 (0.6829)	
training:	Epoch: [5][178/204]	Loss 0.6836 (0.6829)	
training:	Epoch: [5][179/204]	Loss 0.6692 (0.6828)	
training:	Epoch: [5][180/204]	Loss 0.6763 (0.6828)	
training:	Epoch: [5][181/204]	Loss 0.6822 (0.6828)	
training:	Epoch: [5][182/204]	Loss 0.6794 (0.6828)	
training:	Epoch: [5][183/204]	Loss 0.6910 (0.6828)	
training:	Epoch: [5][184/204]	Loss 0.6798 (0.6828)	
training:	Epoch: [5][185/204]	Loss 0.6819 (0.6828)	
training:	Epoch: [5][186/204]	Loss 0.6883 (0.6828)	
training:	Epoch: [5][187/204]	Loss 0.6903 (0.6828)	
training:	Epoch: [5][188/204]	Loss 0.6911 (0.6829)	
training:	Epoch: [5][189/204]	Loss 0.6873 (0.6829)	
training:	Epoch: [5][190/204]	Loss 0.6698 (0.6828)	
training:	Epoch: [5][191/204]	Loss 0.6673 (0.6828)	
training:	Epoch: [5][192/204]	Loss 0.6823 (0.6828)	
training:	Epoch: [5][193/204]	Loss 0.6835 (0.6828)	
training:	Epoch: [5][194/204]	Loss 0.6563 (0.6826)	
training:	Epoch: [5][195/204]	Loss 0.6801 (0.6826)	
training:	Epoch: [5][196/204]	Loss 0.6741 (0.6826)	
training:	Epoch: [5][197/204]	Loss 0.6824 (0.6826)	
training:	Epoch: [5][198/204]	Loss 0.6826 (0.6826)	
training:	Epoch: [5][199/204]	Loss 0.6903 (0.6826)	
training:	Epoch: [5][200/204]	Loss 0.6901 (0.6826)	
training:	Epoch: [5][201/204]	Loss 0.6924 (0.6827)	
training:	Epoch: [5][202/204]	Loss 0.6735 (0.6827)	
training:	Epoch: [5][203/204]	Loss 0.6881 (0.6827)	
training:	Epoch: [5][204/204]	Loss 0.7228 (0.6829)	
Training:	 Loss: 0.6818

Training:	 ACC: 0.5845 0.5904 0.7299 0.4391
Validation:	 ACC: 0.5868 0.5939 0.7421 0.4316
Validation:	 Best_BACC: 0.5868 0.5939 0.7421 0.4316
Validation:	 Loss: 0.6807
Pretraining:	Epoch 6/120
----------
training:	Epoch: [6][1/204]	Loss 0.6934 (0.6934)	
training:	Epoch: [6][2/204]	Loss 0.6699 (0.6816)	
training:	Epoch: [6][3/204]	Loss 0.6827 (0.6820)	
training:	Epoch: [6][4/204]	Loss 0.6647 (0.6777)	
training:	Epoch: [6][5/204]	Loss 0.6751 (0.6771)	
training:	Epoch: [6][6/204]	Loss 0.6785 (0.6774)	
training:	Epoch: [6][7/204]	Loss 0.6599 (0.6749)	
training:	Epoch: [6][8/204]	Loss 0.6992 (0.6779)	
training:	Epoch: [6][9/204]	Loss 0.6996 (0.6803)	
training:	Epoch: [6][10/204]	Loss 0.6896 (0.6812)	
training:	Epoch: [6][11/204]	Loss 0.6899 (0.6820)	
training:	Epoch: [6][12/204]	Loss 0.6718 (0.6812)	
training:	Epoch: [6][13/204]	Loss 0.6858 (0.6815)	
training:	Epoch: [6][14/204]	Loss 0.6877 (0.6820)	
training:	Epoch: [6][15/204]	Loss 0.6796 (0.6818)	
training:	Epoch: [6][16/204]	Loss 0.6555 (0.6802)	
training:	Epoch: [6][17/204]	Loss 0.6907 (0.6808)	
training:	Epoch: [6][18/204]	Loss 0.6574 (0.6795)	
training:	Epoch: [6][19/204]	Loss 0.6518 (0.6780)	
training:	Epoch: [6][20/204]	Loss 0.7157 (0.6799)	
training:	Epoch: [6][21/204]	Loss 0.6666 (0.6793)	
training:	Epoch: [6][22/204]	Loss 0.6771 (0.6792)	
training:	Epoch: [6][23/204]	Loss 0.6793 (0.6792)	
training:	Epoch: [6][24/204]	Loss 0.6965 (0.6799)	
training:	Epoch: [6][25/204]	Loss 0.6978 (0.6806)	
training:	Epoch: [6][26/204]	Loss 0.6782 (0.6805)	
training:	Epoch: [6][27/204]	Loss 0.6749 (0.6803)	
training:	Epoch: [6][28/204]	Loss 0.6943 (0.6808)	
training:	Epoch: [6][29/204]	Loss 0.6732 (0.6806)	
training:	Epoch: [6][30/204]	Loss 0.6738 (0.6803)	
training:	Epoch: [6][31/204]	Loss 0.6592 (0.6797)	
training:	Epoch: [6][32/204]	Loss 0.6783 (0.6796)	
training:	Epoch: [6][33/204]	Loss 0.6590 (0.6790)	
training:	Epoch: [6][34/204]	Loss 0.6631 (0.6785)	
training:	Epoch: [6][35/204]	Loss 0.6948 (0.6790)	
training:	Epoch: [6][36/204]	Loss 0.6735 (0.6788)	
training:	Epoch: [6][37/204]	Loss 0.6819 (0.6789)	
training:	Epoch: [6][38/204]	Loss 0.6503 (0.6782)	
training:	Epoch: [6][39/204]	Loss 0.6679 (0.6779)	
training:	Epoch: [6][40/204]	Loss 0.6927 (0.6783)	
training:	Epoch: [6][41/204]	Loss 0.6638 (0.6779)	
training:	Epoch: [6][42/204]	Loss 0.6847 (0.6781)	
training:	Epoch: [6][43/204]	Loss 0.6877 (0.6783)	
training:	Epoch: [6][44/204]	Loss 0.7001 (0.6788)	
training:	Epoch: [6][45/204]	Loss 0.6824 (0.6789)	
training:	Epoch: [6][46/204]	Loss 0.6642 (0.6786)	
training:	Epoch: [6][47/204]	Loss 0.6829 (0.6787)	
training:	Epoch: [6][48/204]	Loss 0.6890 (0.6789)	
training:	Epoch: [6][49/204]	Loss 0.7136 (0.6796)	
training:	Epoch: [6][50/204]	Loss 0.6762 (0.6795)	
training:	Epoch: [6][51/204]	Loss 0.6925 (0.6798)	
training:	Epoch: [6][52/204]	Loss 0.6878 (0.6799)	
training:	Epoch: [6][53/204]	Loss 0.6754 (0.6798)	
training:	Epoch: [6][54/204]	Loss 0.6663 (0.6796)	
training:	Epoch: [6][55/204]	Loss 0.6961 (0.6799)	
training:	Epoch: [6][56/204]	Loss 0.6955 (0.6802)	
training:	Epoch: [6][57/204]	Loss 0.6880 (0.6803)	
training:	Epoch: [6][58/204]	Loss 0.6786 (0.6803)	
training:	Epoch: [6][59/204]	Loss 0.6802 (0.6803)	
training:	Epoch: [6][60/204]	Loss 0.6889 (0.6804)	
training:	Epoch: [6][61/204]	Loss 0.6611 (0.6801)	
training:	Epoch: [6][62/204]	Loss 0.6604 (0.6798)	
training:	Epoch: [6][63/204]	Loss 0.6873 (0.6799)	
training:	Epoch: [6][64/204]	Loss 0.6605 (0.6796)	
training:	Epoch: [6][65/204]	Loss 0.6855 (0.6797)	
training:	Epoch: [6][66/204]	Loss 0.6692 (0.6795)	
training:	Epoch: [6][67/204]	Loss 0.7053 (0.6799)	
training:	Epoch: [6][68/204]	Loss 0.6596 (0.6796)	
training:	Epoch: [6][69/204]	Loss 0.6756 (0.6796)	
training:	Epoch: [6][70/204]	Loss 0.6898 (0.6797)	
training:	Epoch: [6][71/204]	Loss 0.6763 (0.6797)	
training:	Epoch: [6][72/204]	Loss 0.6710 (0.6795)	
training:	Epoch: [6][73/204]	Loss 0.6793 (0.6795)	
training:	Epoch: [6][74/204]	Loss 0.6700 (0.6794)	
training:	Epoch: [6][75/204]	Loss 0.7025 (0.6797)	
training:	Epoch: [6][76/204]	Loss 0.6699 (0.6796)	
training:	Epoch: [6][77/204]	Loss 0.6942 (0.6798)	
training:	Epoch: [6][78/204]	Loss 0.6775 (0.6797)	
training:	Epoch: [6][79/204]	Loss 0.6866 (0.6798)	
training:	Epoch: [6][80/204]	Loss 0.6909 (0.6800)	
training:	Epoch: [6][81/204]	Loss 0.6646 (0.6798)	
training:	Epoch: [6][82/204]	Loss 0.6646 (0.6796)	
training:	Epoch: [6][83/204]	Loss 0.6811 (0.6796)	
training:	Epoch: [6][84/204]	Loss 0.6586 (0.6794)	
training:	Epoch: [6][85/204]	Loss 0.6931 (0.6795)	
training:	Epoch: [6][86/204]	Loss 0.6797 (0.6795)	
training:	Epoch: [6][87/204]	Loss 0.6766 (0.6795)	
training:	Epoch: [6][88/204]	Loss 0.6935 (0.6796)	
training:	Epoch: [6][89/204]	Loss 0.6769 (0.6796)	
training:	Epoch: [6][90/204]	Loss 0.7084 (0.6799)	
training:	Epoch: [6][91/204]	Loss 0.7030 (0.6802)	
training:	Epoch: [6][92/204]	Loss 0.7006 (0.6804)	
training:	Epoch: [6][93/204]	Loss 0.6495 (0.6801)	
training:	Epoch: [6][94/204]	Loss 0.6735 (0.6800)	
training:	Epoch: [6][95/204]	Loss 0.6810 (0.6800)	
training:	Epoch: [6][96/204]	Loss 0.6727 (0.6799)	
training:	Epoch: [6][97/204]	Loss 0.6849 (0.6800)	
training:	Epoch: [6][98/204]	Loss 0.6925 (0.6801)	
training:	Epoch: [6][99/204]	Loss 0.6754 (0.6801)	
training:	Epoch: [6][100/204]	Loss 0.6774 (0.6800)	
training:	Epoch: [6][101/204]	Loss 0.6621 (0.6799)	
training:	Epoch: [6][102/204]	Loss 0.6877 (0.6799)	
training:	Epoch: [6][103/204]	Loss 0.6755 (0.6799)	
training:	Epoch: [6][104/204]	Loss 0.6971 (0.6801)	
training:	Epoch: [6][105/204]	Loss 0.6988 (0.6802)	
training:	Epoch: [6][106/204]	Loss 0.6584 (0.6800)	
training:	Epoch: [6][107/204]	Loss 0.7072 (0.6803)	
training:	Epoch: [6][108/204]	Loss 0.6771 (0.6803)	
training:	Epoch: [6][109/204]	Loss 0.6844 (0.6803)	
training:	Epoch: [6][110/204]	Loss 0.7176 (0.6806)	
training:	Epoch: [6][111/204]	Loss 0.6797 (0.6806)	
training:	Epoch: [6][112/204]	Loss 0.6825 (0.6806)	
training:	Epoch: [6][113/204]	Loss 0.6727 (0.6806)	
training:	Epoch: [6][114/204]	Loss 0.6874 (0.6806)	
training:	Epoch: [6][115/204]	Loss 0.6724 (0.6806)	
training:	Epoch: [6][116/204]	Loss 0.6904 (0.6807)	
training:	Epoch: [6][117/204]	Loss 0.6812 (0.6807)	
training:	Epoch: [6][118/204]	Loss 0.6651 (0.6805)	
training:	Epoch: [6][119/204]	Loss 0.6776 (0.6805)	
training:	Epoch: [6][120/204]	Loss 0.6881 (0.6806)	
training:	Epoch: [6][121/204]	Loss 0.6775 (0.6805)	
training:	Epoch: [6][122/204]	Loss 0.6599 (0.6804)	
training:	Epoch: [6][123/204]	Loss 0.6889 (0.6804)	
training:	Epoch: [6][124/204]	Loss 0.6941 (0.6805)	
training:	Epoch: [6][125/204]	Loss 0.6793 (0.6805)	
training:	Epoch: [6][126/204]	Loss 0.6552 (0.6803)	
training:	Epoch: [6][127/204]	Loss 0.6901 (0.6804)	
training:	Epoch: [6][128/204]	Loss 0.6651 (0.6803)	
training:	Epoch: [6][129/204]	Loss 0.6759 (0.6803)	
training:	Epoch: [6][130/204]	Loss 0.6607 (0.6801)	
training:	Epoch: [6][131/204]	Loss 0.6740 (0.6801)	
training:	Epoch: [6][132/204]	Loss 0.6713 (0.6800)	
training:	Epoch: [6][133/204]	Loss 0.6693 (0.6799)	
training:	Epoch: [6][134/204]	Loss 0.6840 (0.6799)	
training:	Epoch: [6][135/204]	Loss 0.6744 (0.6799)	
training:	Epoch: [6][136/204]	Loss 0.6806 (0.6799)	
training:	Epoch: [6][137/204]	Loss 0.6891 (0.6800)	
training:	Epoch: [6][138/204]	Loss 0.6826 (0.6800)	
training:	Epoch: [6][139/204]	Loss 0.6556 (0.6798)	
training:	Epoch: [6][140/204]	Loss 0.6844 (0.6799)	
training:	Epoch: [6][141/204]	Loss 0.6718 (0.6798)	
training:	Epoch: [6][142/204]	Loss 0.6732 (0.6798)	
training:	Epoch: [6][143/204]	Loss 0.6973 (0.6799)	
training:	Epoch: [6][144/204]	Loss 0.6782 (0.6799)	
training:	Epoch: [6][145/204]	Loss 0.7017 (0.6800)	
training:	Epoch: [6][146/204]	Loss 0.6601 (0.6799)	
training:	Epoch: [6][147/204]	Loss 0.6774 (0.6799)	
training:	Epoch: [6][148/204]	Loss 0.7011 (0.6800)	
training:	Epoch: [6][149/204]	Loss 0.6878 (0.6801)	
training:	Epoch: [6][150/204]	Loss 0.6891 (0.6801)	
training:	Epoch: [6][151/204]	Loss 0.6931 (0.6802)	
training:	Epoch: [6][152/204]	Loss 0.6785 (0.6802)	
training:	Epoch: [6][153/204]	Loss 0.6838 (0.6802)	
training:	Epoch: [6][154/204]	Loss 0.6920 (0.6803)	
training:	Epoch: [6][155/204]	Loss 0.6909 (0.6804)	
training:	Epoch: [6][156/204]	Loss 0.6709 (0.6803)	
training:	Epoch: [6][157/204]	Loss 0.6935 (0.6804)	
training:	Epoch: [6][158/204]	Loss 0.6622 (0.6803)	
training:	Epoch: [6][159/204]	Loss 0.6717 (0.6802)	
training:	Epoch: [6][160/204]	Loss 0.6594 (0.6801)	
training:	Epoch: [6][161/204]	Loss 0.6933 (0.6802)	
training:	Epoch: [6][162/204]	Loss 0.6997 (0.6803)	
training:	Epoch: [6][163/204]	Loss 0.6838 (0.6803)	
training:	Epoch: [6][164/204]	Loss 0.7112 (0.6805)	
training:	Epoch: [6][165/204]	Loss 0.6947 (0.6806)	
training:	Epoch: [6][166/204]	Loss 0.6823 (0.6806)	
training:	Epoch: [6][167/204]	Loss 0.6656 (0.6805)	
training:	Epoch: [6][168/204]	Loss 0.6602 (0.6804)	
training:	Epoch: [6][169/204]	Loss 0.6940 (0.6805)	
training:	Epoch: [6][170/204]	Loss 0.6679 (0.6804)	
training:	Epoch: [6][171/204]	Loss 0.6960 (0.6805)	
training:	Epoch: [6][172/204]	Loss 0.6895 (0.6805)	
training:	Epoch: [6][173/204]	Loss 0.6801 (0.6805)	
training:	Epoch: [6][174/204]	Loss 0.7112 (0.6807)	
training:	Epoch: [6][175/204]	Loss 0.6510 (0.6805)	
training:	Epoch: [6][176/204]	Loss 0.6813 (0.6805)	
training:	Epoch: [6][177/204]	Loss 0.6691 (0.6805)	
training:	Epoch: [6][178/204]	Loss 0.6852 (0.6805)	
training:	Epoch: [6][179/204]	Loss 0.6918 (0.6806)	
training:	Epoch: [6][180/204]	Loss 0.6824 (0.6806)	
training:	Epoch: [6][181/204]	Loss 0.6845 (0.6806)	
training:	Epoch: [6][182/204]	Loss 0.6965 (0.6807)	
training:	Epoch: [6][183/204]	Loss 0.6764 (0.6807)	
training:	Epoch: [6][184/204]	Loss 0.7084 (0.6808)	
training:	Epoch: [6][185/204]	Loss 0.6812 (0.6808)	
training:	Epoch: [6][186/204]	Loss 0.6604 (0.6807)	
training:	Epoch: [6][187/204]	Loss 0.6838 (0.6807)	
training:	Epoch: [6][188/204]	Loss 0.6465 (0.6805)	
training:	Epoch: [6][189/204]	Loss 0.6722 (0.6805)	
training:	Epoch: [6][190/204]	Loss 0.6725 (0.6805)	
training:	Epoch: [6][191/204]	Loss 0.7143 (0.6806)	
training:	Epoch: [6][192/204]	Loss 0.6764 (0.6806)	
training:	Epoch: [6][193/204]	Loss 0.7111 (0.6808)	
training:	Epoch: [6][194/204]	Loss 0.6741 (0.6807)	
training:	Epoch: [6][195/204]	Loss 0.6928 (0.6808)	
training:	Epoch: [6][196/204]	Loss 0.6995 (0.6809)	
training:	Epoch: [6][197/204]	Loss 0.6742 (0.6809)	
training:	Epoch: [6][198/204]	Loss 0.6609 (0.6808)	
training:	Epoch: [6][199/204]	Loss 0.6696 (0.6807)	
training:	Epoch: [6][200/204]	Loss 0.6585 (0.6806)	
training:	Epoch: [6][201/204]	Loss 0.6691 (0.6805)	
training:	Epoch: [6][202/204]	Loss 0.6889 (0.6806)	
training:	Epoch: [6][203/204]	Loss 0.6632 (0.6805)	
training:	Epoch: [6][204/204]	Loss 0.6800 (0.6805)	
Training:	 Loss: 0.6794

Training:	 ACC: 0.5934 0.5999 0.7525 0.4343
Validation:	 ACC: 0.5912 0.5987 0.7574 0.4249
Validation:	 Best_BACC: 0.5912 0.5987 0.7574 0.4249
Validation:	 Loss: 0.6782
Pretraining:	Epoch 7/120
----------
training:	Epoch: [7][1/204]	Loss 0.6852 (0.6852)	
training:	Epoch: [7][2/204]	Loss 0.6661 (0.6756)	
training:	Epoch: [7][3/204]	Loss 0.6849 (0.6787)	
training:	Epoch: [7][4/204]	Loss 0.6635 (0.6749)	
training:	Epoch: [7][5/204]	Loss 0.6927 (0.6785)	
training:	Epoch: [7][6/204]	Loss 0.7158 (0.6847)	
training:	Epoch: [7][7/204]	Loss 0.6512 (0.6799)	
training:	Epoch: [7][8/204]	Loss 0.7153 (0.6843)	
training:	Epoch: [7][9/204]	Loss 0.6545 (0.6810)	
training:	Epoch: [7][10/204]	Loss 0.6602 (0.6789)	
training:	Epoch: [7][11/204]	Loss 0.6846 (0.6795)	
training:	Epoch: [7][12/204]	Loss 0.6834 (0.6798)	
training:	Epoch: [7][13/204]	Loss 0.6681 (0.6789)	
training:	Epoch: [7][14/204]	Loss 0.6898 (0.6797)	
training:	Epoch: [7][15/204]	Loss 0.6735 (0.6793)	
training:	Epoch: [7][16/204]	Loss 0.6740 (0.6789)	
training:	Epoch: [7][17/204]	Loss 0.6603 (0.6778)	
training:	Epoch: [7][18/204]	Loss 0.6721 (0.6775)	
training:	Epoch: [7][19/204]	Loss 0.6870 (0.6780)	
training:	Epoch: [7][20/204]	Loss 0.6868 (0.6784)	
training:	Epoch: [7][21/204]	Loss 0.6736 (0.6782)	
training:	Epoch: [7][22/204]	Loss 0.6608 (0.6774)	
training:	Epoch: [7][23/204]	Loss 0.6912 (0.6780)	
training:	Epoch: [7][24/204]	Loss 0.6683 (0.6776)	
training:	Epoch: [7][25/204]	Loss 0.6765 (0.6776)	
training:	Epoch: [7][26/204]	Loss 0.6922 (0.6781)	
training:	Epoch: [7][27/204]	Loss 0.6792 (0.6782)	
training:	Epoch: [7][28/204]	Loss 0.6723 (0.6780)	
training:	Epoch: [7][29/204]	Loss 0.6484 (0.6769)	
training:	Epoch: [7][30/204]	Loss 0.6744 (0.6769)	
training:	Epoch: [7][31/204]	Loss 0.6847 (0.6771)	
training:	Epoch: [7][32/204]	Loss 0.6689 (0.6769)	
training:	Epoch: [7][33/204]	Loss 0.6627 (0.6764)	
training:	Epoch: [7][34/204]	Loss 0.6869 (0.6767)	
training:	Epoch: [7][35/204]	Loss 0.6992 (0.6774)	
training:	Epoch: [7][36/204]	Loss 0.6906 (0.6777)	
training:	Epoch: [7][37/204]	Loss 0.6743 (0.6776)	
training:	Epoch: [7][38/204]	Loss 0.6853 (0.6779)	
training:	Epoch: [7][39/204]	Loss 0.6602 (0.6774)	
training:	Epoch: [7][40/204]	Loss 0.6716 (0.6773)	
training:	Epoch: [7][41/204]	Loss 0.6736 (0.6772)	
training:	Epoch: [7][42/204]	Loss 0.6630 (0.6768)	
training:	Epoch: [7][43/204]	Loss 0.6823 (0.6770)	
training:	Epoch: [7][44/204]	Loss 0.6785 (0.6770)	
training:	Epoch: [7][45/204]	Loss 0.6845 (0.6772)	
training:	Epoch: [7][46/204]	Loss 0.6880 (0.6774)	
training:	Epoch: [7][47/204]	Loss 0.7090 (0.6781)	
training:	Epoch: [7][48/204]	Loss 0.6848 (0.6782)	
training:	Epoch: [7][49/204]	Loss 0.6896 (0.6784)	
training:	Epoch: [7][50/204]	Loss 0.6787 (0.6784)	
training:	Epoch: [7][51/204]	Loss 0.6907 (0.6787)	
training:	Epoch: [7][52/204]	Loss 0.6778 (0.6787)	
training:	Epoch: [7][53/204]	Loss 0.6863 (0.6788)	
training:	Epoch: [7][54/204]	Loss 0.6656 (0.6786)	
training:	Epoch: [7][55/204]	Loss 0.6764 (0.6785)	
training:	Epoch: [7][56/204]	Loss 0.6568 (0.6781)	
training:	Epoch: [7][57/204]	Loss 0.6545 (0.6777)	
training:	Epoch: [7][58/204]	Loss 0.6840 (0.6778)	
training:	Epoch: [7][59/204]	Loss 0.7106 (0.6784)	
training:	Epoch: [7][60/204]	Loss 0.6827 (0.6785)	
training:	Epoch: [7][61/204]	Loss 0.6801 (0.6785)	
training:	Epoch: [7][62/204]	Loss 0.6964 (0.6788)	
training:	Epoch: [7][63/204]	Loss 0.6732 (0.6787)	
training:	Epoch: [7][64/204]	Loss 0.6807 (0.6787)	
training:	Epoch: [7][65/204]	Loss 0.6794 (0.6787)	
training:	Epoch: [7][66/204]	Loss 0.6657 (0.6785)	
training:	Epoch: [7][67/204]	Loss 0.7007 (0.6789)	
training:	Epoch: [7][68/204]	Loss 0.6942 (0.6791)	
training:	Epoch: [7][69/204]	Loss 0.6848 (0.6792)	
training:	Epoch: [7][70/204]	Loss 0.6475 (0.6787)	
training:	Epoch: [7][71/204]	Loss 0.6731 (0.6786)	
training:	Epoch: [7][72/204]	Loss 0.6505 (0.6782)	
training:	Epoch: [7][73/204]	Loss 0.7085 (0.6787)	
training:	Epoch: [7][74/204]	Loss 0.6526 (0.6783)	
training:	Epoch: [7][75/204]	Loss 0.6773 (0.6783)	
training:	Epoch: [7][76/204]	Loss 0.6950 (0.6785)	
training:	Epoch: [7][77/204]	Loss 0.6524 (0.6782)	
training:	Epoch: [7][78/204]	Loss 0.6742 (0.6781)	
training:	Epoch: [7][79/204]	Loss 0.6846 (0.6782)	
training:	Epoch: [7][80/204]	Loss 0.6667 (0.6781)	
training:	Epoch: [7][81/204]	Loss 0.6767 (0.6780)	
training:	Epoch: [7][82/204]	Loss 0.6793 (0.6781)	
training:	Epoch: [7][83/204]	Loss 0.6775 (0.6781)	
training:	Epoch: [7][84/204]	Loss 0.6795 (0.6781)	
training:	Epoch: [7][85/204]	Loss 0.6873 (0.6782)	
training:	Epoch: [7][86/204]	Loss 0.6877 (0.6783)	
training:	Epoch: [7][87/204]	Loss 0.6726 (0.6782)	
training:	Epoch: [7][88/204]	Loss 0.6931 (0.6784)	
training:	Epoch: [7][89/204]	Loss 0.6824 (0.6784)	
training:	Epoch: [7][90/204]	Loss 0.6877 (0.6785)	
training:	Epoch: [7][91/204]	Loss 0.6935 (0.6787)	
training:	Epoch: [7][92/204]	Loss 0.6572 (0.6785)	
training:	Epoch: [7][93/204]	Loss 0.6785 (0.6785)	
training:	Epoch: [7][94/204]	Loss 0.6633 (0.6783)	
training:	Epoch: [7][95/204]	Loss 0.6790 (0.6783)	
training:	Epoch: [7][96/204]	Loss 0.6766 (0.6783)	
training:	Epoch: [7][97/204]	Loss 0.7032 (0.6786)	
training:	Epoch: [7][98/204]	Loss 0.6600 (0.6784)	
training:	Epoch: [7][99/204]	Loss 0.6676 (0.6783)	
training:	Epoch: [7][100/204]	Loss 0.6695 (0.6782)	
training:	Epoch: [7][101/204]	Loss 0.6673 (0.6781)	
training:	Epoch: [7][102/204]	Loss 0.6811 (0.6781)	
training:	Epoch: [7][103/204]	Loss 0.6841 (0.6782)	
training:	Epoch: [7][104/204]	Loss 0.6651 (0.6780)	
training:	Epoch: [7][105/204]	Loss 0.6666 (0.6779)	
training:	Epoch: [7][106/204]	Loss 0.6767 (0.6779)	
training:	Epoch: [7][107/204]	Loss 0.6743 (0.6779)	
training:	Epoch: [7][108/204]	Loss 0.7018 (0.6781)	
training:	Epoch: [7][109/204]	Loss 0.7031 (0.6783)	
training:	Epoch: [7][110/204]	Loss 0.7000 (0.6785)	
training:	Epoch: [7][111/204]	Loss 0.6519 (0.6783)	
training:	Epoch: [7][112/204]	Loss 0.6576 (0.6781)	
training:	Epoch: [7][113/204]	Loss 0.6767 (0.6781)	
training:	Epoch: [7][114/204]	Loss 0.6689 (0.6780)	
training:	Epoch: [7][115/204]	Loss 0.7065 (0.6783)	
training:	Epoch: [7][116/204]	Loss 0.6608 (0.6781)	
training:	Epoch: [7][117/204]	Loss 0.6590 (0.6779)	
training:	Epoch: [7][118/204]	Loss 0.6743 (0.6779)	
training:	Epoch: [7][119/204]	Loss 0.6699 (0.6778)	
training:	Epoch: [7][120/204]	Loss 0.6817 (0.6779)	
training:	Epoch: [7][121/204]	Loss 0.6738 (0.6778)	
training:	Epoch: [7][122/204]	Loss 0.6669 (0.6777)	
training:	Epoch: [7][123/204]	Loss 0.6947 (0.6779)	
training:	Epoch: [7][124/204]	Loss 0.6869 (0.6780)	
training:	Epoch: [7][125/204]	Loss 0.7014 (0.6781)	
training:	Epoch: [7][126/204]	Loss 0.6916 (0.6783)	
training:	Epoch: [7][127/204]	Loss 0.6567 (0.6781)	
training:	Epoch: [7][128/204]	Loss 0.6949 (0.6782)	
training:	Epoch: [7][129/204]	Loss 0.6697 (0.6781)	
training:	Epoch: [7][130/204]	Loss 0.7006 (0.6783)	
training:	Epoch: [7][131/204]	Loss 0.6805 (0.6783)	
training:	Epoch: [7][132/204]	Loss 0.6763 (0.6783)	
training:	Epoch: [7][133/204]	Loss 0.6988 (0.6785)	
training:	Epoch: [7][134/204]	Loss 0.6878 (0.6785)	
training:	Epoch: [7][135/204]	Loss 0.6631 (0.6784)	
training:	Epoch: [7][136/204]	Loss 0.6612 (0.6783)	
training:	Epoch: [7][137/204]	Loss 0.6649 (0.6782)	
training:	Epoch: [7][138/204]	Loss 0.6848 (0.6783)	
training:	Epoch: [7][139/204]	Loss 0.6779 (0.6783)	
training:	Epoch: [7][140/204]	Loss 0.7018 (0.6784)	
training:	Epoch: [7][141/204]	Loss 0.6776 (0.6784)	
training:	Epoch: [7][142/204]	Loss 0.6542 (0.6782)	
training:	Epoch: [7][143/204]	Loss 0.6747 (0.6782)	
training:	Epoch: [7][144/204]	Loss 0.6760 (0.6782)	
training:	Epoch: [7][145/204]	Loss 0.7036 (0.6784)	
training:	Epoch: [7][146/204]	Loss 0.6807 (0.6784)	
training:	Epoch: [7][147/204]	Loss 0.6514 (0.6782)	
training:	Epoch: [7][148/204]	Loss 0.6823 (0.6782)	
training:	Epoch: [7][149/204]	Loss 0.6606 (0.6781)	
training:	Epoch: [7][150/204]	Loss 0.6960 (0.6782)	
training:	Epoch: [7][151/204]	Loss 0.6788 (0.6782)	
training:	Epoch: [7][152/204]	Loss 0.6630 (0.6781)	
training:	Epoch: [7][153/204]	Loss 0.6743 (0.6781)	
training:	Epoch: [7][154/204]	Loss 0.7047 (0.6783)	
training:	Epoch: [7][155/204]	Loss 0.6662 (0.6782)	
training:	Epoch: [7][156/204]	Loss 0.6858 (0.6783)	
training:	Epoch: [7][157/204]	Loss 0.6619 (0.6782)	
training:	Epoch: [7][158/204]	Loss 0.6703 (0.6781)	
training:	Epoch: [7][159/204]	Loss 0.6911 (0.6782)	
training:	Epoch: [7][160/204]	Loss 0.6717 (0.6782)	
training:	Epoch: [7][161/204]	Loss 0.6747 (0.6781)	
training:	Epoch: [7][162/204]	Loss 0.6465 (0.6779)	
training:	Epoch: [7][163/204]	Loss 0.6851 (0.6780)	
training:	Epoch: [7][164/204]	Loss 0.6677 (0.6779)	
training:	Epoch: [7][165/204]	Loss 0.6819 (0.6779)	
training:	Epoch: [7][166/204]	Loss 0.6664 (0.6779)	
training:	Epoch: [7][167/204]	Loss 0.6787 (0.6779)	
training:	Epoch: [7][168/204]	Loss 0.6691 (0.6778)	
training:	Epoch: [7][169/204]	Loss 0.6809 (0.6778)	
training:	Epoch: [7][170/204]	Loss 0.6841 (0.6779)	
training:	Epoch: [7][171/204]	Loss 0.6882 (0.6779)	
training:	Epoch: [7][172/204]	Loss 0.6558 (0.6778)	
training:	Epoch: [7][173/204]	Loss 0.6641 (0.6777)	
training:	Epoch: [7][174/204]	Loss 0.6569 (0.6776)	
training:	Epoch: [7][175/204]	Loss 0.6599 (0.6775)	
training:	Epoch: [7][176/204]	Loss 0.6779 (0.6775)	
training:	Epoch: [7][177/204]	Loss 0.6861 (0.6776)	
training:	Epoch: [7][178/204]	Loss 0.7026 (0.6777)	
training:	Epoch: [7][179/204]	Loss 0.6796 (0.6777)	
training:	Epoch: [7][180/204]	Loss 0.6744 (0.6777)	
training:	Epoch: [7][181/204]	Loss 0.6759 (0.6777)	
training:	Epoch: [7][182/204]	Loss 0.6514 (0.6775)	
training:	Epoch: [7][183/204]	Loss 0.6669 (0.6775)	
training:	Epoch: [7][184/204]	Loss 0.6806 (0.6775)	
training:	Epoch: [7][185/204]	Loss 0.6678 (0.6774)	
training:	Epoch: [7][186/204]	Loss 0.6616 (0.6774)	
training:	Epoch: [7][187/204]	Loss 0.6517 (0.6772)	
training:	Epoch: [7][188/204]	Loss 0.7092 (0.6774)	
training:	Epoch: [7][189/204]	Loss 0.7036 (0.6775)	
training:	Epoch: [7][190/204]	Loss 0.6899 (0.6776)	
training:	Epoch: [7][191/204]	Loss 0.6814 (0.6776)	
training:	Epoch: [7][192/204]	Loss 0.6617 (0.6775)	
training:	Epoch: [7][193/204]	Loss 0.6499 (0.6774)	
training:	Epoch: [7][194/204]	Loss 0.6496 (0.6772)	
training:	Epoch: [7][195/204]	Loss 0.6742 (0.6772)	
training:	Epoch: [7][196/204]	Loss 0.6640 (0.6772)	
training:	Epoch: [7][197/204]	Loss 0.6641 (0.6771)	
training:	Epoch: [7][198/204]	Loss 0.6720 (0.6771)	
training:	Epoch: [7][199/204]	Loss 0.6671 (0.6770)	
training:	Epoch: [7][200/204]	Loss 0.6651 (0.6770)	
training:	Epoch: [7][201/204]	Loss 0.6717 (0.6769)	
training:	Epoch: [7][202/204]	Loss 0.6956 (0.6770)	
training:	Epoch: [7][203/204]	Loss 0.7065 (0.6772)	
training:	Epoch: [7][204/204]	Loss 0.6540 (0.6771)	
Training:	 Loss: 0.6760

Training:	 ACC: 0.6126 0.6164 0.7052 0.5201
Validation:	 ACC: 0.6125 0.6174 0.7206 0.5045
Validation:	 Best_BACC: 0.6125 0.6174 0.7206 0.5045
Validation:	 Loss: 0.6751
Pretraining:	Epoch 8/120
----------
training:	Epoch: [8][1/204]	Loss 0.6890 (0.6890)	
training:	Epoch: [8][2/204]	Loss 0.6609 (0.6750)	
training:	Epoch: [8][3/204]	Loss 0.6682 (0.6727)	
training:	Epoch: [8][4/204]	Loss 0.6692 (0.6718)	
training:	Epoch: [8][5/204]	Loss 0.6638 (0.6702)	
training:	Epoch: [8][6/204]	Loss 0.6852 (0.6727)	
training:	Epoch: [8][7/204]	Loss 0.6531 (0.6699)	
training:	Epoch: [8][8/204]	Loss 0.6638 (0.6691)	
training:	Epoch: [8][9/204]	Loss 0.6620 (0.6683)	
training:	Epoch: [8][10/204]	Loss 0.6637 (0.6679)	
training:	Epoch: [8][11/204]	Loss 0.6935 (0.6702)	
training:	Epoch: [8][12/204]	Loss 0.6898 (0.6718)	
training:	Epoch: [8][13/204]	Loss 0.6771 (0.6722)	
training:	Epoch: [8][14/204]	Loss 0.6513 (0.6708)	
training:	Epoch: [8][15/204]	Loss 0.6957 (0.6724)	
training:	Epoch: [8][16/204]	Loss 0.6486 (0.6709)	
training:	Epoch: [8][17/204]	Loss 0.6771 (0.6713)	
training:	Epoch: [8][18/204]	Loss 0.6759 (0.6715)	
training:	Epoch: [8][19/204]	Loss 0.6936 (0.6727)	
training:	Epoch: [8][20/204]	Loss 0.7015 (0.6741)	
training:	Epoch: [8][21/204]	Loss 0.7018 (0.6755)	
training:	Epoch: [8][22/204]	Loss 0.6786 (0.6756)	
training:	Epoch: [8][23/204]	Loss 0.6558 (0.6747)	
training:	Epoch: [8][24/204]	Loss 0.6521 (0.6738)	
training:	Epoch: [8][25/204]	Loss 0.6783 (0.6740)	
training:	Epoch: [8][26/204]	Loss 0.6502 (0.6731)	
training:	Epoch: [8][27/204]	Loss 0.6385 (0.6718)	
training:	Epoch: [8][28/204]	Loss 0.6925 (0.6725)	
training:	Epoch: [8][29/204]	Loss 0.6491 (0.6717)	
training:	Epoch: [8][30/204]	Loss 0.7247 (0.6735)	
training:	Epoch: [8][31/204]	Loss 0.6685 (0.6733)	
training:	Epoch: [8][32/204]	Loss 0.6660 (0.6731)	
training:	Epoch: [8][33/204]	Loss 0.6545 (0.6725)	
training:	Epoch: [8][34/204]	Loss 0.6597 (0.6722)	
training:	Epoch: [8][35/204]	Loss 0.6424 (0.6713)	
training:	Epoch: [8][36/204]	Loss 0.6854 (0.6717)	
training:	Epoch: [8][37/204]	Loss 0.6853 (0.6721)	
training:	Epoch: [8][38/204]	Loss 0.6740 (0.6721)	
training:	Epoch: [8][39/204]	Loss 0.7073 (0.6730)	
training:	Epoch: [8][40/204]	Loss 0.6846 (0.6733)	
training:	Epoch: [8][41/204]	Loss 0.6604 (0.6730)	
training:	Epoch: [8][42/204]	Loss 0.6615 (0.6727)	
training:	Epoch: [8][43/204]	Loss 0.6906 (0.6731)	
training:	Epoch: [8][44/204]	Loss 0.6511 (0.6726)	
training:	Epoch: [8][45/204]	Loss 0.6663 (0.6725)	
training:	Epoch: [8][46/204]	Loss 0.6910 (0.6729)	
training:	Epoch: [8][47/204]	Loss 0.6798 (0.6730)	
training:	Epoch: [8][48/204]	Loss 0.6786 (0.6732)	
training:	Epoch: [8][49/204]	Loss 0.6638 (0.6730)	
training:	Epoch: [8][50/204]	Loss 0.6935 (0.6734)	
training:	Epoch: [8][51/204]	Loss 0.6654 (0.6732)	
training:	Epoch: [8][52/204]	Loss 0.6902 (0.6735)	
training:	Epoch: [8][53/204]	Loss 0.6687 (0.6735)	
training:	Epoch: [8][54/204]	Loss 0.6696 (0.6734)	
training:	Epoch: [8][55/204]	Loss 0.6714 (0.6733)	
training:	Epoch: [8][56/204]	Loss 0.6398 (0.6727)	
training:	Epoch: [8][57/204]	Loss 0.6712 (0.6727)	
training:	Epoch: [8][58/204]	Loss 0.6635 (0.6726)	
training:	Epoch: [8][59/204]	Loss 0.6734 (0.6726)	
training:	Epoch: [8][60/204]	Loss 0.6705 (0.6725)	
training:	Epoch: [8][61/204]	Loss 0.6639 (0.6724)	
training:	Epoch: [8][62/204]	Loss 0.6684 (0.6723)	
training:	Epoch: [8][63/204]	Loss 0.7069 (0.6729)	
training:	Epoch: [8][64/204]	Loss 0.6888 (0.6731)	
training:	Epoch: [8][65/204]	Loss 0.6613 (0.6729)	
training:	Epoch: [8][66/204]	Loss 0.6526 (0.6726)	
training:	Epoch: [8][67/204]	Loss 0.6720 (0.6726)	
training:	Epoch: [8][68/204]	Loss 0.6569 (0.6724)	
training:	Epoch: [8][69/204]	Loss 0.6482 (0.6720)	
training:	Epoch: [8][70/204]	Loss 0.7059 (0.6725)	
training:	Epoch: [8][71/204]	Loss 0.6782 (0.6726)	
training:	Epoch: [8][72/204]	Loss 0.6936 (0.6729)	
training:	Epoch: [8][73/204]	Loss 0.6493 (0.6726)	
training:	Epoch: [8][74/204]	Loss 0.6636 (0.6725)	
training:	Epoch: [8][75/204]	Loss 0.6564 (0.6722)	
training:	Epoch: [8][76/204]	Loss 0.6951 (0.6725)	
training:	Epoch: [8][77/204]	Loss 0.6532 (0.6723)	
training:	Epoch: [8][78/204]	Loss 0.6767 (0.6724)	
training:	Epoch: [8][79/204]	Loss 0.6608 (0.6722)	
training:	Epoch: [8][80/204]	Loss 0.6942 (0.6725)	
training:	Epoch: [8][81/204]	Loss 0.6675 (0.6724)	
training:	Epoch: [8][82/204]	Loss 0.6596 (0.6723)	
training:	Epoch: [8][83/204]	Loss 0.6749 (0.6723)	
training:	Epoch: [8][84/204]	Loss 0.6468 (0.6720)	
training:	Epoch: [8][85/204]	Loss 0.6730 (0.6720)	
training:	Epoch: [8][86/204]	Loss 0.6742 (0.6720)	
training:	Epoch: [8][87/204]	Loss 0.6842 (0.6722)	
training:	Epoch: [8][88/204]	Loss 0.6711 (0.6722)	
training:	Epoch: [8][89/204]	Loss 0.6814 (0.6723)	
training:	Epoch: [8][90/204]	Loss 0.6776 (0.6723)	
training:	Epoch: [8][91/204]	Loss 0.6981 (0.6726)	
training:	Epoch: [8][92/204]	Loss 0.6843 (0.6727)	
training:	Epoch: [8][93/204]	Loss 0.6651 (0.6726)	
training:	Epoch: [8][94/204]	Loss 0.6658 (0.6726)	
training:	Epoch: [8][95/204]	Loss 0.6693 (0.6725)	
training:	Epoch: [8][96/204]	Loss 0.6710 (0.6725)	
training:	Epoch: [8][97/204]	Loss 0.6921 (0.6727)	
training:	Epoch: [8][98/204]	Loss 0.6719 (0.6727)	
training:	Epoch: [8][99/204]	Loss 0.6799 (0.6728)	
training:	Epoch: [8][100/204]	Loss 0.6792 (0.6729)	
training:	Epoch: [8][101/204]	Loss 0.6723 (0.6728)	
training:	Epoch: [8][102/204]	Loss 0.6678 (0.6728)	
training:	Epoch: [8][103/204]	Loss 0.6509 (0.6726)	
training:	Epoch: [8][104/204]	Loss 0.6542 (0.6724)	
training:	Epoch: [8][105/204]	Loss 0.6640 (0.6723)	
training:	Epoch: [8][106/204]	Loss 0.6869 (0.6725)	
training:	Epoch: [8][107/204]	Loss 0.6916 (0.6726)	
training:	Epoch: [8][108/204]	Loss 0.6686 (0.6726)	
training:	Epoch: [8][109/204]	Loss 0.6789 (0.6727)	
training:	Epoch: [8][110/204]	Loss 0.6859 (0.6728)	
training:	Epoch: [8][111/204]	Loss 0.6840 (0.6729)	
training:	Epoch: [8][112/204]	Loss 0.6944 (0.6731)	
training:	Epoch: [8][113/204]	Loss 0.6630 (0.6730)	
training:	Epoch: [8][114/204]	Loss 0.6641 (0.6729)	
training:	Epoch: [8][115/204]	Loss 0.7049 (0.6732)	
training:	Epoch: [8][116/204]	Loss 0.6840 (0.6733)	
training:	Epoch: [8][117/204]	Loss 0.6461 (0.6730)	
training:	Epoch: [8][118/204]	Loss 0.6766 (0.6731)	
training:	Epoch: [8][119/204]	Loss 0.6861 (0.6732)	
training:	Epoch: [8][120/204]	Loss 0.6793 (0.6732)	
training:	Epoch: [8][121/204]	Loss 0.6795 (0.6733)	
training:	Epoch: [8][122/204]	Loss 0.6552 (0.6731)	
training:	Epoch: [8][123/204]	Loss 0.6648 (0.6731)	
training:	Epoch: [8][124/204]	Loss 0.6811 (0.6731)	
training:	Epoch: [8][125/204]	Loss 0.6876 (0.6733)	
training:	Epoch: [8][126/204]	Loss 0.6707 (0.6732)	
training:	Epoch: [8][127/204]	Loss 0.6677 (0.6732)	
training:	Epoch: [8][128/204]	Loss 0.6802 (0.6732)	
training:	Epoch: [8][129/204]	Loss 0.6517 (0.6731)	
training:	Epoch: [8][130/204]	Loss 0.6676 (0.6730)	
training:	Epoch: [8][131/204]	Loss 0.6585 (0.6729)	
training:	Epoch: [8][132/204]	Loss 0.6586 (0.6728)	
training:	Epoch: [8][133/204]	Loss 0.6659 (0.6728)	
training:	Epoch: [8][134/204]	Loss 0.6615 (0.6727)	
training:	Epoch: [8][135/204]	Loss 0.6738 (0.6727)	
training:	Epoch: [8][136/204]	Loss 0.6847 (0.6728)	
training:	Epoch: [8][137/204]	Loss 0.7008 (0.6730)	
training:	Epoch: [8][138/204]	Loss 0.6753 (0.6730)	
training:	Epoch: [8][139/204]	Loss 0.6579 (0.6729)	
training:	Epoch: [8][140/204]	Loss 0.6675 (0.6729)	
training:	Epoch: [8][141/204]	Loss 0.6669 (0.6728)	
training:	Epoch: [8][142/204]	Loss 0.6598 (0.6727)	
training:	Epoch: [8][143/204]	Loss 0.6824 (0.6728)	
training:	Epoch: [8][144/204]	Loss 0.6749 (0.6728)	
training:	Epoch: [8][145/204]	Loss 0.6709 (0.6728)	
training:	Epoch: [8][146/204]	Loss 0.7055 (0.6730)	
training:	Epoch: [8][147/204]	Loss 0.6620 (0.6729)	
training:	Epoch: [8][148/204]	Loss 0.6312 (0.6727)	
training:	Epoch: [8][149/204]	Loss 0.6714 (0.6726)	
training:	Epoch: [8][150/204]	Loss 0.6736 (0.6727)	
training:	Epoch: [8][151/204]	Loss 0.6643 (0.6726)	
training:	Epoch: [8][152/204]	Loss 0.7111 (0.6729)	
training:	Epoch: [8][153/204]	Loss 0.7105 (0.6731)	
training:	Epoch: [8][154/204]	Loss 0.6346 (0.6728)	
training:	Epoch: [8][155/204]	Loss 0.6633 (0.6728)	
training:	Epoch: [8][156/204]	Loss 0.6769 (0.6728)	
training:	Epoch: [8][157/204]	Loss 0.6392 (0.6726)	
training:	Epoch: [8][158/204]	Loss 0.7049 (0.6728)	
training:	Epoch: [8][159/204]	Loss 0.7004 (0.6730)	
training:	Epoch: [8][160/204]	Loss 0.6952 (0.6731)	
training:	Epoch: [8][161/204]	Loss 0.6723 (0.6731)	
training:	Epoch: [8][162/204]	Loss 0.6497 (0.6730)	
training:	Epoch: [8][163/204]	Loss 0.6469 (0.6728)	
training:	Epoch: [8][164/204]	Loss 0.6891 (0.6729)	
training:	Epoch: [8][165/204]	Loss 0.6907 (0.6730)	
training:	Epoch: [8][166/204]	Loss 0.6881 (0.6731)	
training:	Epoch: [8][167/204]	Loss 0.6624 (0.6730)	
training:	Epoch: [8][168/204]	Loss 0.6619 (0.6730)	
training:	Epoch: [8][169/204]	Loss 0.6272 (0.6727)	
training:	Epoch: [8][170/204]	Loss 0.6366 (0.6725)	
training:	Epoch: [8][171/204]	Loss 0.6837 (0.6726)	
training:	Epoch: [8][172/204]	Loss 0.6448 (0.6724)	
training:	Epoch: [8][173/204]	Loss 0.6658 (0.6724)	
training:	Epoch: [8][174/204]	Loss 0.6662 (0.6723)	
training:	Epoch: [8][175/204]	Loss 0.7016 (0.6725)	
training:	Epoch: [8][176/204]	Loss 0.6680 (0.6725)	
training:	Epoch: [8][177/204]	Loss 0.6624 (0.6724)	
training:	Epoch: [8][178/204]	Loss 0.6631 (0.6724)	
training:	Epoch: [8][179/204]	Loss 0.6642 (0.6723)	
training:	Epoch: [8][180/204]	Loss 0.6660 (0.6723)	
training:	Epoch: [8][181/204]	Loss 0.6716 (0.6723)	
training:	Epoch: [8][182/204]	Loss 0.6539 (0.6722)	
training:	Epoch: [8][183/204]	Loss 0.6601 (0.6721)	
training:	Epoch: [8][184/204]	Loss 0.6944 (0.6722)	
training:	Epoch: [8][185/204]	Loss 0.6979 (0.6724)	
training:	Epoch: [8][186/204]	Loss 0.6778 (0.6724)	
training:	Epoch: [8][187/204]	Loss 0.6788 (0.6724)	
training:	Epoch: [8][188/204]	Loss 0.6613 (0.6724)	
training:	Epoch: [8][189/204]	Loss 0.6530 (0.6723)	
training:	Epoch: [8][190/204]	Loss 0.6670 (0.6722)	
training:	Epoch: [8][191/204]	Loss 0.6595 (0.6722)	
training:	Epoch: [8][192/204]	Loss 0.6657 (0.6721)	
training:	Epoch: [8][193/204]	Loss 0.6722 (0.6721)	
training:	Epoch: [8][194/204]	Loss 0.6946 (0.6723)	
training:	Epoch: [8][195/204]	Loss 0.6835 (0.6723)	
training:	Epoch: [8][196/204]	Loss 0.6941 (0.6724)	
training:	Epoch: [8][197/204]	Loss 0.6608 (0.6724)	
training:	Epoch: [8][198/204]	Loss 0.6759 (0.6724)	
training:	Epoch: [8][199/204]	Loss 0.6655 (0.6723)	
training:	Epoch: [8][200/204]	Loss 0.6849 (0.6724)	
training:	Epoch: [8][201/204]	Loss 0.6758 (0.6724)	
training:	Epoch: [8][202/204]	Loss 0.6617 (0.6724)	
training:	Epoch: [8][203/204]	Loss 0.6451 (0.6722)	
training:	Epoch: [8][204/204]	Loss 0.7046 (0.6724)	
Training:	 Loss: 0.6714

Training:	 ACC: 0.6199 0.6247 0.7372 0.5026
Validation:	 ACC: 0.6159 0.6217 0.7431 0.4888
Validation:	 Best_BACC: 0.6159 0.6217 0.7431 0.4888
Validation:	 Loss: 0.6712
Pretraining:	Epoch 9/120
----------
training:	Epoch: [9][1/204]	Loss 0.6686 (0.6686)	
training:	Epoch: [9][2/204]	Loss 0.6623 (0.6655)	
training:	Epoch: [9][3/204]	Loss 0.6821 (0.6710)	
training:	Epoch: [9][4/204]	Loss 0.6746 (0.6719)	
training:	Epoch: [9][5/204]	Loss 0.6660 (0.6707)	
training:	Epoch: [9][6/204]	Loss 0.6734 (0.6712)	
training:	Epoch: [9][7/204]	Loss 0.6681 (0.6707)	
training:	Epoch: [9][8/204]	Loss 0.6987 (0.6742)	
training:	Epoch: [9][9/204]	Loss 0.6503 (0.6716)	
training:	Epoch: [9][10/204]	Loss 0.6607 (0.6705)	
training:	Epoch: [9][11/204]	Loss 0.6913 (0.6724)	
training:	Epoch: [9][12/204]	Loss 0.6330 (0.6691)	
training:	Epoch: [9][13/204]	Loss 0.6682 (0.6690)	
training:	Epoch: [9][14/204]	Loss 0.6840 (0.6701)	
training:	Epoch: [9][15/204]	Loss 0.6286 (0.6673)	
training:	Epoch: [9][16/204]	Loss 0.6835 (0.6683)	
training:	Epoch: [9][17/204]	Loss 0.6803 (0.6690)	
training:	Epoch: [9][18/204]	Loss 0.6580 (0.6684)	
training:	Epoch: [9][19/204]	Loss 0.6670 (0.6684)	
training:	Epoch: [9][20/204]	Loss 0.6461 (0.6672)	
training:	Epoch: [9][21/204]	Loss 0.6669 (0.6672)	
training:	Epoch: [9][22/204]	Loss 0.6936 (0.6684)	
training:	Epoch: [9][23/204]	Loss 0.6943 (0.6695)	
training:	Epoch: [9][24/204]	Loss 0.6589 (0.6691)	
training:	Epoch: [9][25/204]	Loss 0.6378 (0.6679)	
training:	Epoch: [9][26/204]	Loss 0.6672 (0.6678)	
training:	Epoch: [9][27/204]	Loss 0.6650 (0.6677)	
training:	Epoch: [9][28/204]	Loss 0.7013 (0.6689)	
training:	Epoch: [9][29/204]	Loss 0.6702 (0.6690)	
training:	Epoch: [9][30/204]	Loss 0.6578 (0.6686)	
training:	Epoch: [9][31/204]	Loss 0.6927 (0.6694)	
training:	Epoch: [9][32/204]	Loss 0.6610 (0.6691)	
training:	Epoch: [9][33/204]	Loss 0.6585 (0.6688)	
training:	Epoch: [9][34/204]	Loss 0.6698 (0.6688)	
training:	Epoch: [9][35/204]	Loss 0.6681 (0.6688)	
training:	Epoch: [9][36/204]	Loss 0.6603 (0.6686)	
training:	Epoch: [9][37/204]	Loss 0.6905 (0.6691)	
training:	Epoch: [9][38/204]	Loss 0.6847 (0.6696)	
training:	Epoch: [9][39/204]	Loss 0.6742 (0.6697)	
training:	Epoch: [9][40/204]	Loss 0.6493 (0.6692)	
training:	Epoch: [9][41/204]	Loss 0.6707 (0.6692)	
training:	Epoch: [9][42/204]	Loss 0.6517 (0.6688)	
training:	Epoch: [9][43/204]	Loss 0.6811 (0.6691)	
training:	Epoch: [9][44/204]	Loss 0.6522 (0.6687)	
training:	Epoch: [9][45/204]	Loss 0.6358 (0.6680)	
training:	Epoch: [9][46/204]	Loss 0.6673 (0.6679)	
training:	Epoch: [9][47/204]	Loss 0.6902 (0.6684)	
training:	Epoch: [9][48/204]	Loss 0.6899 (0.6689)	
training:	Epoch: [9][49/204]	Loss 0.6535 (0.6686)	
training:	Epoch: [9][50/204]	Loss 0.6455 (0.6681)	
training:	Epoch: [9][51/204]	Loss 0.6725 (0.6682)	
training:	Epoch: [9][52/204]	Loss 0.6759 (0.6683)	
training:	Epoch: [9][53/204]	Loss 0.6276 (0.6676)	
training:	Epoch: [9][54/204]	Loss 0.6892 (0.6680)	
training:	Epoch: [9][55/204]	Loss 0.7038 (0.6686)	
training:	Epoch: [9][56/204]	Loss 0.7111 (0.6694)	
training:	Epoch: [9][57/204]	Loss 0.6859 (0.6697)	
training:	Epoch: [9][58/204]	Loss 0.6453 (0.6692)	
training:	Epoch: [9][59/204]	Loss 0.6542 (0.6690)	
training:	Epoch: [9][60/204]	Loss 0.6800 (0.6692)	
training:	Epoch: [9][61/204]	Loss 0.6682 (0.6691)	
training:	Epoch: [9][62/204]	Loss 0.6672 (0.6691)	
training:	Epoch: [9][63/204]	Loss 0.6359 (0.6686)	
training:	Epoch: [9][64/204]	Loss 0.6806 (0.6688)	
training:	Epoch: [9][65/204]	Loss 0.6786 (0.6689)	
training:	Epoch: [9][66/204]	Loss 0.6672 (0.6689)	
training:	Epoch: [9][67/204]	Loss 0.6294 (0.6683)	
training:	Epoch: [9][68/204]	Loss 0.6742 (0.6684)	
training:	Epoch: [9][69/204]	Loss 0.6657 (0.6684)	
training:	Epoch: [9][70/204]	Loss 0.6643 (0.6683)	
training:	Epoch: [9][71/204]	Loss 0.6679 (0.6683)	
training:	Epoch: [9][72/204]	Loss 0.6518 (0.6681)	
training:	Epoch: [9][73/204]	Loss 0.6702 (0.6681)	
training:	Epoch: [9][74/204]	Loss 0.6440 (0.6678)	
training:	Epoch: [9][75/204]	Loss 0.6928 (0.6681)	
training:	Epoch: [9][76/204]	Loss 0.6555 (0.6679)	
training:	Epoch: [9][77/204]	Loss 0.6587 (0.6678)	
training:	Epoch: [9][78/204]	Loss 0.6563 (0.6677)	
training:	Epoch: [9][79/204]	Loss 0.6842 (0.6679)	
training:	Epoch: [9][80/204]	Loss 0.6872 (0.6681)	
training:	Epoch: [9][81/204]	Loss 0.6535 (0.6679)	
training:	Epoch: [9][82/204]	Loss 0.6485 (0.6677)	
training:	Epoch: [9][83/204]	Loss 0.6331 (0.6673)	
training:	Epoch: [9][84/204]	Loss 0.6656 (0.6673)	
training:	Epoch: [9][85/204]	Loss 0.6439 (0.6670)	
training:	Epoch: [9][86/204]	Loss 0.6951 (0.6673)	
training:	Epoch: [9][87/204]	Loss 0.6777 (0.6674)	
training:	Epoch: [9][88/204]	Loss 0.7018 (0.6678)	
training:	Epoch: [9][89/204]	Loss 0.6572 (0.6677)	
training:	Epoch: [9][90/204]	Loss 0.6841 (0.6679)	
training:	Epoch: [9][91/204]	Loss 0.6719 (0.6679)	
training:	Epoch: [9][92/204]	Loss 0.6858 (0.6681)	
training:	Epoch: [9][93/204]	Loss 0.6614 (0.6681)	
training:	Epoch: [9][94/204]	Loss 0.6675 (0.6681)	
training:	Epoch: [9][95/204]	Loss 0.6893 (0.6683)	
training:	Epoch: [9][96/204]	Loss 0.6654 (0.6682)	
training:	Epoch: [9][97/204]	Loss 0.6848 (0.6684)	
training:	Epoch: [9][98/204]	Loss 0.6898 (0.6686)	
training:	Epoch: [9][99/204]	Loss 0.6618 (0.6686)	
training:	Epoch: [9][100/204]	Loss 0.6915 (0.6688)	
training:	Epoch: [9][101/204]	Loss 0.6421 (0.6685)	
training:	Epoch: [9][102/204]	Loss 0.6569 (0.6684)	
training:	Epoch: [9][103/204]	Loss 0.6536 (0.6683)	
training:	Epoch: [9][104/204]	Loss 0.6768 (0.6684)	
training:	Epoch: [9][105/204]	Loss 0.6902 (0.6686)	
training:	Epoch: [9][106/204]	Loss 0.6862 (0.6687)	
training:	Epoch: [9][107/204]	Loss 0.6457 (0.6685)	
training:	Epoch: [9][108/204]	Loss 0.6905 (0.6687)	
training:	Epoch: [9][109/204]	Loss 0.6750 (0.6688)	
training:	Epoch: [9][110/204]	Loss 0.6948 (0.6690)	
training:	Epoch: [9][111/204]	Loss 0.6700 (0.6690)	
training:	Epoch: [9][112/204]	Loss 0.6772 (0.6691)	
training:	Epoch: [9][113/204]	Loss 0.6666 (0.6691)	
training:	Epoch: [9][114/204]	Loss 0.6669 (0.6691)	
training:	Epoch: [9][115/204]	Loss 0.6588 (0.6690)	
training:	Epoch: [9][116/204]	Loss 0.6636 (0.6689)	
training:	Epoch: [9][117/204]	Loss 0.6678 (0.6689)	
training:	Epoch: [9][118/204]	Loss 0.6785 (0.6690)	
training:	Epoch: [9][119/204]	Loss 0.6492 (0.6688)	
training:	Epoch: [9][120/204]	Loss 0.6700 (0.6688)	
training:	Epoch: [9][121/204]	Loss 0.6283 (0.6685)	
training:	Epoch: [9][122/204]	Loss 0.6840 (0.6686)	
training:	Epoch: [9][123/204]	Loss 0.6652 (0.6686)	
training:	Epoch: [9][124/204]	Loss 0.7131 (0.6690)	
training:	Epoch: [9][125/204]	Loss 0.6520 (0.6688)	
training:	Epoch: [9][126/204]	Loss 0.6421 (0.6686)	
training:	Epoch: [9][127/204]	Loss 0.7059 (0.6689)	
training:	Epoch: [9][128/204]	Loss 0.6708 (0.6689)	
training:	Epoch: [9][129/204]	Loss 0.6460 (0.6687)	
training:	Epoch: [9][130/204]	Loss 0.6926 (0.6689)	
training:	Epoch: [9][131/204]	Loss 0.6715 (0.6689)	
training:	Epoch: [9][132/204]	Loss 0.6768 (0.6690)	
training:	Epoch: [9][133/204]	Loss 0.6507 (0.6689)	
training:	Epoch: [9][134/204]	Loss 0.6833 (0.6690)	
training:	Epoch: [9][135/204]	Loss 0.6798 (0.6690)	
training:	Epoch: [9][136/204]	Loss 0.6512 (0.6689)	
training:	Epoch: [9][137/204]	Loss 0.6599 (0.6689)	
training:	Epoch: [9][138/204]	Loss 0.6879 (0.6690)	
training:	Epoch: [9][139/204]	Loss 0.6608 (0.6689)	
training:	Epoch: [9][140/204]	Loss 0.6674 (0.6689)	
training:	Epoch: [9][141/204]	Loss 0.7032 (0.6692)	
training:	Epoch: [9][142/204]	Loss 0.6497 (0.6690)	
training:	Epoch: [9][143/204]	Loss 0.7003 (0.6692)	
training:	Epoch: [9][144/204]	Loss 0.6700 (0.6692)	
training:	Epoch: [9][145/204]	Loss 0.6871 (0.6694)	
training:	Epoch: [9][146/204]	Loss 0.6803 (0.6694)	
training:	Epoch: [9][147/204]	Loss 0.6636 (0.6694)	
training:	Epoch: [9][148/204]	Loss 0.6695 (0.6694)	
training:	Epoch: [9][149/204]	Loss 0.6653 (0.6694)	
training:	Epoch: [9][150/204]	Loss 0.6597 (0.6693)	
training:	Epoch: [9][151/204]	Loss 0.6942 (0.6695)	
training:	Epoch: [9][152/204]	Loss 0.6586 (0.6694)	
training:	Epoch: [9][153/204]	Loss 0.6826 (0.6695)	
training:	Epoch: [9][154/204]	Loss 0.7043 (0.6697)	
training:	Epoch: [9][155/204]	Loss 0.6718 (0.6697)	
training:	Epoch: [9][156/204]	Loss 0.6699 (0.6697)	
training:	Epoch: [9][157/204]	Loss 0.6732 (0.6698)	
training:	Epoch: [9][158/204]	Loss 0.6968 (0.6699)	
training:	Epoch: [9][159/204]	Loss 0.6740 (0.6700)	
training:	Epoch: [9][160/204]	Loss 0.6599 (0.6699)	
training:	Epoch: [9][161/204]	Loss 0.6807 (0.6700)	
training:	Epoch: [9][162/204]	Loss 0.6817 (0.6700)	
training:	Epoch: [9][163/204]	Loss 0.6828 (0.6701)	
training:	Epoch: [9][164/204]	Loss 0.6442 (0.6700)	
training:	Epoch: [9][165/204]	Loss 0.6680 (0.6699)	
training:	Epoch: [9][166/204]	Loss 0.6747 (0.6700)	
training:	Epoch: [9][167/204]	Loss 0.6333 (0.6697)	
training:	Epoch: [9][168/204]	Loss 0.6672 (0.6697)	
training:	Epoch: [9][169/204]	Loss 0.6531 (0.6696)	
training:	Epoch: [9][170/204]	Loss 0.6352 (0.6694)	
training:	Epoch: [9][171/204]	Loss 0.6695 (0.6694)	
training:	Epoch: [9][172/204]	Loss 0.6778 (0.6695)	
training:	Epoch: [9][173/204]	Loss 0.6562 (0.6694)	
training:	Epoch: [9][174/204]	Loss 0.6676 (0.6694)	
training:	Epoch: [9][175/204]	Loss 0.6609 (0.6693)	
training:	Epoch: [9][176/204]	Loss 0.6544 (0.6693)	
training:	Epoch: [9][177/204]	Loss 0.6587 (0.6692)	
training:	Epoch: [9][178/204]	Loss 0.6873 (0.6693)	
training:	Epoch: [9][179/204]	Loss 0.6780 (0.6694)	
training:	Epoch: [9][180/204]	Loss 0.6553 (0.6693)	
training:	Epoch: [9][181/204]	Loss 0.6510 (0.6692)	
training:	Epoch: [9][182/204]	Loss 0.6523 (0.6691)	
training:	Epoch: [9][183/204]	Loss 0.6844 (0.6692)	
training:	Epoch: [9][184/204]	Loss 0.6466 (0.6690)	
training:	Epoch: [9][185/204]	Loss 0.6684 (0.6690)	
training:	Epoch: [9][186/204]	Loss 0.6701 (0.6690)	
training:	Epoch: [9][187/204]	Loss 0.6673 (0.6690)	
training:	Epoch: [9][188/204]	Loss 0.6555 (0.6690)	
training:	Epoch: [9][189/204]	Loss 0.6867 (0.6691)	
training:	Epoch: [9][190/204]	Loss 0.6704 (0.6691)	
training:	Epoch: [9][191/204]	Loss 0.6563 (0.6690)	
training:	Epoch: [9][192/204]	Loss 0.6934 (0.6691)	
training:	Epoch: [9][193/204]	Loss 0.6656 (0.6691)	
training:	Epoch: [9][194/204]	Loss 0.6667 (0.6691)	
training:	Epoch: [9][195/204]	Loss 0.6759 (0.6691)	
training:	Epoch: [9][196/204]	Loss 0.6407 (0.6690)	
training:	Epoch: [9][197/204]	Loss 0.6477 (0.6689)	
training:	Epoch: [9][198/204]	Loss 0.6721 (0.6689)	
training:	Epoch: [9][199/204]	Loss 0.6845 (0.6690)	
training:	Epoch: [9][200/204]	Loss 0.6591 (0.6689)	
training:	Epoch: [9][201/204]	Loss 0.6736 (0.6689)	
training:	Epoch: [9][202/204]	Loss 0.6574 (0.6689)	
training:	Epoch: [9][203/204]	Loss 0.6835 (0.6690)	
training:	Epoch: [9][204/204]	Loss 0.6792 (0.6690)	
Training:	 Loss: 0.6680

Training:	 ACC: 0.6335 0.6366 0.7105 0.5564
Validation:	 ACC: 0.6237 0.6281 0.7206 0.5269
Validation:	 Best_BACC: 0.6237 0.6281 0.7206 0.5269
Validation:	 Loss: 0.6671
Pretraining:	Epoch 10/120
----------
training:	Epoch: [10][1/204]	Loss 0.6471 (0.6471)	
training:	Epoch: [10][2/204]	Loss 0.6493 (0.6482)	
training:	Epoch: [10][3/204]	Loss 0.6661 (0.6542)	
training:	Epoch: [10][4/204]	Loss 0.7007 (0.6658)	
training:	Epoch: [10][5/204]	Loss 0.6932 (0.6713)	
training:	Epoch: [10][6/204]	Loss 0.6700 (0.6711)	
training:	Epoch: [10][7/204]	Loss 0.6439 (0.6672)	
training:	Epoch: [10][8/204]	Loss 0.6954 (0.6707)	
training:	Epoch: [10][9/204]	Loss 0.6515 (0.6686)	
training:	Epoch: [10][10/204]	Loss 0.7031 (0.6720)	
training:	Epoch: [10][11/204]	Loss 0.6535 (0.6703)	
training:	Epoch: [10][12/204]	Loss 0.6457 (0.6683)	
training:	Epoch: [10][13/204]	Loss 0.6730 (0.6687)	
training:	Epoch: [10][14/204]	Loss 0.6931 (0.6704)	
training:	Epoch: [10][15/204]	Loss 0.6513 (0.6691)	
training:	Epoch: [10][16/204]	Loss 0.6613 (0.6686)	
training:	Epoch: [10][17/204]	Loss 0.6734 (0.6689)	
training:	Epoch: [10][18/204]	Loss 0.6989 (0.6706)	
training:	Epoch: [10][19/204]	Loss 0.6482 (0.6694)	
training:	Epoch: [10][20/204]	Loss 0.6671 (0.6693)	
training:	Epoch: [10][21/204]	Loss 0.6793 (0.6698)	
training:	Epoch: [10][22/204]	Loss 0.6310 (0.6680)	
training:	Epoch: [10][23/204]	Loss 0.6832 (0.6687)	
training:	Epoch: [10][24/204]	Loss 0.6792 (0.6691)	
training:	Epoch: [10][25/204]	Loss 0.6622 (0.6688)	
training:	Epoch: [10][26/204]	Loss 0.6761 (0.6691)	
training:	Epoch: [10][27/204]	Loss 0.6518 (0.6685)	
training:	Epoch: [10][28/204]	Loss 0.6645 (0.6683)	
training:	Epoch: [10][29/204]	Loss 0.6561 (0.6679)	
training:	Epoch: [10][30/204]	Loss 0.6525 (0.6674)	
training:	Epoch: [10][31/204]	Loss 0.6609 (0.6672)	
training:	Epoch: [10][32/204]	Loss 0.6658 (0.6671)	
training:	Epoch: [10][33/204]	Loss 0.6689 (0.6672)	
training:	Epoch: [10][34/204]	Loss 0.6866 (0.6678)	
training:	Epoch: [10][35/204]	Loss 0.6721 (0.6679)	
training:	Epoch: [10][36/204]	Loss 0.6678 (0.6679)	
training:	Epoch: [10][37/204]	Loss 0.6527 (0.6675)	
training:	Epoch: [10][38/204]	Loss 0.6488 (0.6670)	
training:	Epoch: [10][39/204]	Loss 0.6311 (0.6661)	
training:	Epoch: [10][40/204]	Loss 0.6722 (0.6662)	
training:	Epoch: [10][41/204]	Loss 0.6275 (0.6653)	
training:	Epoch: [10][42/204]	Loss 0.6636 (0.6652)	
training:	Epoch: [10][43/204]	Loss 0.6573 (0.6650)	
training:	Epoch: [10][44/204]	Loss 0.6697 (0.6652)	
training:	Epoch: [10][45/204]	Loss 0.6390 (0.6646)	
training:	Epoch: [10][46/204]	Loss 0.6490 (0.6642)	
training:	Epoch: [10][47/204]	Loss 0.7047 (0.6651)	
training:	Epoch: [10][48/204]	Loss 0.6714 (0.6652)	
training:	Epoch: [10][49/204]	Loss 0.6441 (0.6648)	
training:	Epoch: [10][50/204]	Loss 0.6855 (0.6652)	
training:	Epoch: [10][51/204]	Loss 0.7074 (0.6660)	
training:	Epoch: [10][52/204]	Loss 0.6944 (0.6666)	
training:	Epoch: [10][53/204]	Loss 0.6805 (0.6668)	
training:	Epoch: [10][54/204]	Loss 0.6709 (0.6669)	
training:	Epoch: [10][55/204]	Loss 0.7002 (0.6675)	
training:	Epoch: [10][56/204]	Loss 0.6639 (0.6675)	
training:	Epoch: [10][57/204]	Loss 0.6552 (0.6672)	
training:	Epoch: [10][58/204]	Loss 0.6742 (0.6674)	
training:	Epoch: [10][59/204]	Loss 0.6642 (0.6673)	
training:	Epoch: [10][60/204]	Loss 0.6406 (0.6669)	
training:	Epoch: [10][61/204]	Loss 0.7081 (0.6675)	
training:	Epoch: [10][62/204]	Loss 0.6478 (0.6672)	
training:	Epoch: [10][63/204]	Loss 0.6611 (0.6671)	
training:	Epoch: [10][64/204]	Loss 0.6778 (0.6673)	
training:	Epoch: [10][65/204]	Loss 0.6637 (0.6672)	
training:	Epoch: [10][66/204]	Loss 0.6755 (0.6674)	
training:	Epoch: [10][67/204]	Loss 0.6736 (0.6675)	
training:	Epoch: [10][68/204]	Loss 0.6568 (0.6673)	
training:	Epoch: [10][69/204]	Loss 0.6687 (0.6673)	
training:	Epoch: [10][70/204]	Loss 0.6443 (0.6670)	
training:	Epoch: [10][71/204]	Loss 0.6572 (0.6669)	
training:	Epoch: [10][72/204]	Loss 0.6560 (0.6667)	
training:	Epoch: [10][73/204]	Loss 0.6489 (0.6665)	
training:	Epoch: [10][74/204]	Loss 0.6601 (0.6664)	
training:	Epoch: [10][75/204]	Loss 0.6667 (0.6664)	
training:	Epoch: [10][76/204]	Loss 0.6505 (0.6662)	
training:	Epoch: [10][77/204]	Loss 0.6709 (0.6662)	
training:	Epoch: [10][78/204]	Loss 0.6745 (0.6663)	
training:	Epoch: [10][79/204]	Loss 0.6873 (0.6666)	
training:	Epoch: [10][80/204]	Loss 0.6574 (0.6665)	
training:	Epoch: [10][81/204]	Loss 0.6475 (0.6663)	
training:	Epoch: [10][82/204]	Loss 0.6727 (0.6663)	
training:	Epoch: [10][83/204]	Loss 0.6549 (0.6662)	
training:	Epoch: [10][84/204]	Loss 0.6725 (0.6663)	
training:	Epoch: [10][85/204]	Loss 0.6877 (0.6665)	
training:	Epoch: [10][86/204]	Loss 0.6115 (0.6659)	
training:	Epoch: [10][87/204]	Loss 0.6421 (0.6656)	
training:	Epoch: [10][88/204]	Loss 0.6670 (0.6656)	
training:	Epoch: [10][89/204]	Loss 0.6784 (0.6658)	
training:	Epoch: [10][90/204]	Loss 0.6524 (0.6656)	
training:	Epoch: [10][91/204]	Loss 0.6511 (0.6655)	
training:	Epoch: [10][92/204]	Loss 0.6551 (0.6653)	
training:	Epoch: [10][93/204]	Loss 0.6701 (0.6654)	
training:	Epoch: [10][94/204]	Loss 0.6617 (0.6654)	
training:	Epoch: [10][95/204]	Loss 0.6386 (0.6651)	
training:	Epoch: [10][96/204]	Loss 0.6685 (0.6651)	
training:	Epoch: [10][97/204]	Loss 0.6456 (0.6649)	
training:	Epoch: [10][98/204]	Loss 0.6347 (0.6646)	
training:	Epoch: [10][99/204]	Loss 0.6684 (0.6646)	
training:	Epoch: [10][100/204]	Loss 0.6513 (0.6645)	
training:	Epoch: [10][101/204]	Loss 0.6525 (0.6644)	
training:	Epoch: [10][102/204]	Loss 0.6760 (0.6645)	
training:	Epoch: [10][103/204]	Loss 0.6541 (0.6644)	
training:	Epoch: [10][104/204]	Loss 0.6691 (0.6644)	
training:	Epoch: [10][105/204]	Loss 0.6944 (0.6647)	
training:	Epoch: [10][106/204]	Loss 0.6724 (0.6648)	
training:	Epoch: [10][107/204]	Loss 0.6437 (0.6646)	
training:	Epoch: [10][108/204]	Loss 0.6540 (0.6645)	
training:	Epoch: [10][109/204]	Loss 0.6725 (0.6646)	
training:	Epoch: [10][110/204]	Loss 0.6853 (0.6648)	
training:	Epoch: [10][111/204]	Loss 0.6682 (0.6648)	
training:	Epoch: [10][112/204]	Loss 0.6720 (0.6649)	
training:	Epoch: [10][113/204]	Loss 0.6273 (0.6645)	
training:	Epoch: [10][114/204]	Loss 0.6573 (0.6645)	
training:	Epoch: [10][115/204]	Loss 0.6847 (0.6646)	
training:	Epoch: [10][116/204]	Loss 0.7162 (0.6651)	
training:	Epoch: [10][117/204]	Loss 0.6483 (0.6649)	
training:	Epoch: [10][118/204]	Loss 0.6369 (0.6647)	
training:	Epoch: [10][119/204]	Loss 0.6593 (0.6647)	
training:	Epoch: [10][120/204]	Loss 0.6531 (0.6646)	
training:	Epoch: [10][121/204]	Loss 0.6436 (0.6644)	
training:	Epoch: [10][122/204]	Loss 0.6949 (0.6646)	
training:	Epoch: [10][123/204]	Loss 0.6742 (0.6647)	
training:	Epoch: [10][124/204]	Loss 0.6569 (0.6647)	
training:	Epoch: [10][125/204]	Loss 0.6628 (0.6646)	
training:	Epoch: [10][126/204]	Loss 0.6823 (0.6648)	
training:	Epoch: [10][127/204]	Loss 0.6530 (0.6647)	
training:	Epoch: [10][128/204]	Loss 0.6507 (0.6646)	
training:	Epoch: [10][129/204]	Loss 0.6747 (0.6647)	
training:	Epoch: [10][130/204]	Loss 0.6785 (0.6648)	
training:	Epoch: [10][131/204]	Loss 0.6364 (0.6646)	
training:	Epoch: [10][132/204]	Loss 0.6842 (0.6647)	
training:	Epoch: [10][133/204]	Loss 0.6763 (0.6648)	
training:	Epoch: [10][134/204]	Loss 0.6633 (0.6648)	
training:	Epoch: [10][135/204]	Loss 0.6779 (0.6649)	
training:	Epoch: [10][136/204]	Loss 0.6646 (0.6649)	
training:	Epoch: [10][137/204]	Loss 0.6389 (0.6647)	
training:	Epoch: [10][138/204]	Loss 0.6984 (0.6649)	
training:	Epoch: [10][139/204]	Loss 0.6378 (0.6647)	
training:	Epoch: [10][140/204]	Loss 0.6770 (0.6648)	
training:	Epoch: [10][141/204]	Loss 0.6638 (0.6648)	
training:	Epoch: [10][142/204]	Loss 0.6921 (0.6650)	
training:	Epoch: [10][143/204]	Loss 0.6670 (0.6650)	
training:	Epoch: [10][144/204]	Loss 0.6404 (0.6648)	
training:	Epoch: [10][145/204]	Loss 0.6461 (0.6647)	
training:	Epoch: [10][146/204]	Loss 0.6521 (0.6646)	
training:	Epoch: [10][147/204]	Loss 0.6912 (0.6648)	
training:	Epoch: [10][148/204]	Loss 0.6636 (0.6648)	
training:	Epoch: [10][149/204]	Loss 0.6421 (0.6647)	
training:	Epoch: [10][150/204]	Loss 0.6405 (0.6645)	
training:	Epoch: [10][151/204]	Loss 0.6631 (0.6645)	
training:	Epoch: [10][152/204]	Loss 0.6709 (0.6645)	
training:	Epoch: [10][153/204]	Loss 0.6566 (0.6645)	
training:	Epoch: [10][154/204]	Loss 0.6877 (0.6646)	
training:	Epoch: [10][155/204]	Loss 0.6707 (0.6647)	
training:	Epoch: [10][156/204]	Loss 0.6912 (0.6648)	
training:	Epoch: [10][157/204]	Loss 0.6889 (0.6650)	
training:	Epoch: [10][158/204]	Loss 0.7334 (0.6654)	
training:	Epoch: [10][159/204]	Loss 0.6531 (0.6653)	
training:	Epoch: [10][160/204]	Loss 0.6953 (0.6655)	
training:	Epoch: [10][161/204]	Loss 0.6935 (0.6657)	
training:	Epoch: [10][162/204]	Loss 0.7041 (0.6659)	
training:	Epoch: [10][163/204]	Loss 0.6600 (0.6659)	
training:	Epoch: [10][164/204]	Loss 0.6715 (0.6659)	
training:	Epoch: [10][165/204]	Loss 0.6620 (0.6659)	
training:	Epoch: [10][166/204]	Loss 0.6724 (0.6659)	
training:	Epoch: [10][167/204]	Loss 0.6787 (0.6660)	
training:	Epoch: [10][168/204]	Loss 0.6707 (0.6661)	
training:	Epoch: [10][169/204]	Loss 0.6786 (0.6661)	
training:	Epoch: [10][170/204]	Loss 0.6479 (0.6660)	
training:	Epoch: [10][171/204]	Loss 0.6454 (0.6659)	
training:	Epoch: [10][172/204]	Loss 0.6494 (0.6658)	
training:	Epoch: [10][173/204]	Loss 0.6684 (0.6658)	
training:	Epoch: [10][174/204]	Loss 0.6653 (0.6658)	
training:	Epoch: [10][175/204]	Loss 0.6498 (0.6657)	
training:	Epoch: [10][176/204]	Loss 0.6607 (0.6657)	
training:	Epoch: [10][177/204]	Loss 0.6592 (0.6657)	
training:	Epoch: [10][178/204]	Loss 0.6625 (0.6656)	
training:	Epoch: [10][179/204]	Loss 0.6658 (0.6656)	
training:	Epoch: [10][180/204]	Loss 0.6507 (0.6656)	
training:	Epoch: [10][181/204]	Loss 0.6585 (0.6655)	
training:	Epoch: [10][182/204]	Loss 0.6905 (0.6657)	
training:	Epoch: [10][183/204]	Loss 0.6760 (0.6657)	
training:	Epoch: [10][184/204]	Loss 0.6267 (0.6655)	
training:	Epoch: [10][185/204]	Loss 0.6228 (0.6653)	
training:	Epoch: [10][186/204]	Loss 0.6366 (0.6651)	
training:	Epoch: [10][187/204]	Loss 0.6715 (0.6652)	
training:	Epoch: [10][188/204]	Loss 0.6475 (0.6651)	
training:	Epoch: [10][189/204]	Loss 0.6543 (0.6650)	
training:	Epoch: [10][190/204]	Loss 0.6755 (0.6651)	
training:	Epoch: [10][191/204]	Loss 0.7017 (0.6652)	
training:	Epoch: [10][192/204]	Loss 0.6490 (0.6652)	
training:	Epoch: [10][193/204]	Loss 0.6406 (0.6650)	
training:	Epoch: [10][194/204]	Loss 0.6816 (0.6651)	
training:	Epoch: [10][195/204]	Loss 0.6444 (0.6650)	
training:	Epoch: [10][196/204]	Loss 0.6484 (0.6649)	
training:	Epoch: [10][197/204]	Loss 0.6413 (0.6648)	
training:	Epoch: [10][198/204]	Loss 0.6341 (0.6647)	
training:	Epoch: [10][199/204]	Loss 0.6877 (0.6648)	
training:	Epoch: [10][200/204]	Loss 0.6763 (0.6648)	
training:	Epoch: [10][201/204]	Loss 0.6733 (0.6649)	
training:	Epoch: [10][202/204]	Loss 0.6161 (0.6646)	
training:	Epoch: [10][203/204]	Loss 0.6478 (0.6645)	
training:	Epoch: [10][204/204]	Loss 0.6758 (0.6646)	
Training:	 Loss: 0.6636

Training:	 ACC: 0.6415 0.6433 0.6858 0.5973
Validation:	 ACC: 0.6232 0.6260 0.6847 0.5617
Validation:	 Best_BACC: 0.6237 0.6281 0.7206 0.5269
Validation:	 Loss: 0.6624
Pretraining:	Epoch 11/120
----------
training:	Epoch: [11][1/204]	Loss 0.6827 (0.6827)	
training:	Epoch: [11][2/204]	Loss 0.6803 (0.6815)	
training:	Epoch: [11][3/204]	Loss 0.6782 (0.6804)	
training:	Epoch: [11][4/204]	Loss 0.6311 (0.6681)	
training:	Epoch: [11][5/204]	Loss 0.6428 (0.6630)	
training:	Epoch: [11][6/204]	Loss 0.6665 (0.6636)	
training:	Epoch: [11][7/204]	Loss 0.6252 (0.6581)	
training:	Epoch: [11][8/204]	Loss 0.6727 (0.6599)	
training:	Epoch: [11][9/204]	Loss 0.6425 (0.6580)	
training:	Epoch: [11][10/204]	Loss 0.6798 (0.6602)	
training:	Epoch: [11][11/204]	Loss 0.6224 (0.6568)	
training:	Epoch: [11][12/204]	Loss 0.6480 (0.6560)	
training:	Epoch: [11][13/204]	Loss 0.6682 (0.6570)	
training:	Epoch: [11][14/204]	Loss 0.6411 (0.6558)	
training:	Epoch: [11][15/204]	Loss 0.6798 (0.6574)	
training:	Epoch: [11][16/204]	Loss 0.6515 (0.6571)	
training:	Epoch: [11][17/204]	Loss 0.6598 (0.6572)	
training:	Epoch: [11][18/204]	Loss 0.6440 (0.6565)	
training:	Epoch: [11][19/204]	Loss 0.6986 (0.6587)	
training:	Epoch: [11][20/204]	Loss 0.6813 (0.6598)	
training:	Epoch: [11][21/204]	Loss 0.6909 (0.6613)	
training:	Epoch: [11][22/204]	Loss 0.6652 (0.6615)	
training:	Epoch: [11][23/204]	Loss 0.6247 (0.6599)	
training:	Epoch: [11][24/204]	Loss 0.6531 (0.6596)	
training:	Epoch: [11][25/204]	Loss 0.6828 (0.6605)	
training:	Epoch: [11][26/204]	Loss 0.6708 (0.6609)	
training:	Epoch: [11][27/204]	Loss 0.6946 (0.6622)	
training:	Epoch: [11][28/204]	Loss 0.6582 (0.6620)	
training:	Epoch: [11][29/204]	Loss 0.6443 (0.6614)	
training:	Epoch: [11][30/204]	Loss 0.6644 (0.6615)	
training:	Epoch: [11][31/204]	Loss 0.6344 (0.6606)	
training:	Epoch: [11][32/204]	Loss 0.6397 (0.6600)	
training:	Epoch: [11][33/204]	Loss 0.6750 (0.6604)	
training:	Epoch: [11][34/204]	Loss 0.6612 (0.6605)	
training:	Epoch: [11][35/204]	Loss 0.6964 (0.6615)	
training:	Epoch: [11][36/204]	Loss 0.6497 (0.6612)	
training:	Epoch: [11][37/204]	Loss 0.6622 (0.6612)	
training:	Epoch: [11][38/204]	Loss 0.6522 (0.6610)	
training:	Epoch: [11][39/204]	Loss 0.6681 (0.6611)	
training:	Epoch: [11][40/204]	Loss 0.6651 (0.6612)	
training:	Epoch: [11][41/204]	Loss 0.6739 (0.6616)	
training:	Epoch: [11][42/204]	Loss 0.6387 (0.6610)	
training:	Epoch: [11][43/204]	Loss 0.6706 (0.6612)	
training:	Epoch: [11][44/204]	Loss 0.6807 (0.6617)	
training:	Epoch: [11][45/204]	Loss 0.6308 (0.6610)	
training:	Epoch: [11][46/204]	Loss 0.6469 (0.6607)	
training:	Epoch: [11][47/204]	Loss 0.6463 (0.6604)	
training:	Epoch: [11][48/204]	Loss 0.6412 (0.6600)	
training:	Epoch: [11][49/204]	Loss 0.6849 (0.6605)	
training:	Epoch: [11][50/204]	Loss 0.6305 (0.6599)	
training:	Epoch: [11][51/204]	Loss 0.6888 (0.6605)	
training:	Epoch: [11][52/204]	Loss 0.6748 (0.6607)	
training:	Epoch: [11][53/204]	Loss 0.6454 (0.6604)	
training:	Epoch: [11][54/204]	Loss 0.6542 (0.6603)	
training:	Epoch: [11][55/204]	Loss 0.6184 (0.6596)	
training:	Epoch: [11][56/204]	Loss 0.6554 (0.6595)	
training:	Epoch: [11][57/204]	Loss 0.6696 (0.6597)	
training:	Epoch: [11][58/204]	Loss 0.6492 (0.6595)	
training:	Epoch: [11][59/204]	Loss 0.6544 (0.6594)	
training:	Epoch: [11][60/204]	Loss 0.6416 (0.6591)	
training:	Epoch: [11][61/204]	Loss 0.6626 (0.6592)	
training:	Epoch: [11][62/204]	Loss 0.6744 (0.6594)	
training:	Epoch: [11][63/204]	Loss 0.6352 (0.6590)	
training:	Epoch: [11][64/204]	Loss 0.6645 (0.6591)	
training:	Epoch: [11][65/204]	Loss 0.6747 (0.6593)	
training:	Epoch: [11][66/204]	Loss 0.6735 (0.6596)	
training:	Epoch: [11][67/204]	Loss 0.6905 (0.6600)	
training:	Epoch: [11][68/204]	Loss 0.6785 (0.6603)	
training:	Epoch: [11][69/204]	Loss 0.6267 (0.6598)	
training:	Epoch: [11][70/204]	Loss 0.6403 (0.6595)	
training:	Epoch: [11][71/204]	Loss 0.6533 (0.6594)	
training:	Epoch: [11][72/204]	Loss 0.6596 (0.6594)	
training:	Epoch: [11][73/204]	Loss 0.6292 (0.6590)	
training:	Epoch: [11][74/204]	Loss 0.6579 (0.6590)	
training:	Epoch: [11][75/204]	Loss 0.6946 (0.6595)	
training:	Epoch: [11][76/204]	Loss 0.6328 (0.6591)	
training:	Epoch: [11][77/204]	Loss 0.6243 (0.6587)	
training:	Epoch: [11][78/204]	Loss 0.6700 (0.6588)	
training:	Epoch: [11][79/204]	Loss 0.6439 (0.6586)	
training:	Epoch: [11][80/204]	Loss 0.6220 (0.6582)	
training:	Epoch: [11][81/204]	Loss 0.7032 (0.6587)	
training:	Epoch: [11][82/204]	Loss 0.6758 (0.6589)	
training:	Epoch: [11][83/204]	Loss 0.6727 (0.6591)	
training:	Epoch: [11][84/204]	Loss 0.6470 (0.6590)	
training:	Epoch: [11][85/204]	Loss 0.6662 (0.6591)	
training:	Epoch: [11][86/204]	Loss 0.6771 (0.6593)	
training:	Epoch: [11][87/204]	Loss 0.7194 (0.6600)	
training:	Epoch: [11][88/204]	Loss 0.6706 (0.6601)	
training:	Epoch: [11][89/204]	Loss 0.6404 (0.6599)	
training:	Epoch: [11][90/204]	Loss 0.6659 (0.6599)	
training:	Epoch: [11][91/204]	Loss 0.6678 (0.6600)	
training:	Epoch: [11][92/204]	Loss 0.6590 (0.6600)	
training:	Epoch: [11][93/204]	Loss 0.6810 (0.6602)	
training:	Epoch: [11][94/204]	Loss 0.6216 (0.6598)	
training:	Epoch: [11][95/204]	Loss 0.6518 (0.6597)	
training:	Epoch: [11][96/204]	Loss 0.7128 (0.6603)	
training:	Epoch: [11][97/204]	Loss 0.6681 (0.6604)	
training:	Epoch: [11][98/204]	Loss 0.6371 (0.6601)	
training:	Epoch: [11][99/204]	Loss 0.6923 (0.6604)	
training:	Epoch: [11][100/204]	Loss 0.7323 (0.6612)	
training:	Epoch: [11][101/204]	Loss 0.6123 (0.6607)	
training:	Epoch: [11][102/204]	Loss 0.6837 (0.6609)	
training:	Epoch: [11][103/204]	Loss 0.6683 (0.6610)	
training:	Epoch: [11][104/204]	Loss 0.6587 (0.6610)	
training:	Epoch: [11][105/204]	Loss 0.6264 (0.6606)	
training:	Epoch: [11][106/204]	Loss 0.6933 (0.6609)	
training:	Epoch: [11][107/204]	Loss 0.6739 (0.6611)	
training:	Epoch: [11][108/204]	Loss 0.6216 (0.6607)	
training:	Epoch: [11][109/204]	Loss 0.6442 (0.6605)	
training:	Epoch: [11][110/204]	Loss 0.6978 (0.6609)	
training:	Epoch: [11][111/204]	Loss 0.6820 (0.6611)	
training:	Epoch: [11][112/204]	Loss 0.6543 (0.6610)	
training:	Epoch: [11][113/204]	Loss 0.6739 (0.6611)	
training:	Epoch: [11][114/204]	Loss 0.6529 (0.6611)	
training:	Epoch: [11][115/204]	Loss 0.6467 (0.6609)	
training:	Epoch: [11][116/204]	Loss 0.6784 (0.6611)	
training:	Epoch: [11][117/204]	Loss 0.6730 (0.6612)	
training:	Epoch: [11][118/204]	Loss 0.6657 (0.6612)	
training:	Epoch: [11][119/204]	Loss 0.6558 (0.6612)	
training:	Epoch: [11][120/204]	Loss 0.6595 (0.6612)	
training:	Epoch: [11][121/204]	Loss 0.6927 (0.6614)	
training:	Epoch: [11][122/204]	Loss 0.6430 (0.6613)	
training:	Epoch: [11][123/204]	Loss 0.7048 (0.6616)	
training:	Epoch: [11][124/204]	Loss 0.6487 (0.6615)	
training:	Epoch: [11][125/204]	Loss 0.6786 (0.6617)	
training:	Epoch: [11][126/204]	Loss 0.7113 (0.6620)	
training:	Epoch: [11][127/204]	Loss 0.6640 (0.6621)	
training:	Epoch: [11][128/204]	Loss 0.6441 (0.6619)	
training:	Epoch: [11][129/204]	Loss 0.6660 (0.6620)	
training:	Epoch: [11][130/204]	Loss 0.6091 (0.6615)	
training:	Epoch: [11][131/204]	Loss 0.6395 (0.6614)	
training:	Epoch: [11][132/204]	Loss 0.6849 (0.6616)	
training:	Epoch: [11][133/204]	Loss 0.6343 (0.6614)	
training:	Epoch: [11][134/204]	Loss 0.6070 (0.6609)	
training:	Epoch: [11][135/204]	Loss 0.6705 (0.6610)	
training:	Epoch: [11][136/204]	Loss 0.6867 (0.6612)	
training:	Epoch: [11][137/204]	Loss 0.6429 (0.6611)	
training:	Epoch: [11][138/204]	Loss 0.6390 (0.6609)	
training:	Epoch: [11][139/204]	Loss 0.6839 (0.6611)	
training:	Epoch: [11][140/204]	Loss 0.6505 (0.6610)	
training:	Epoch: [11][141/204]	Loss 0.6427 (0.6609)	
training:	Epoch: [11][142/204]	Loss 0.6301 (0.6607)	
training:	Epoch: [11][143/204]	Loss 0.7281 (0.6611)	
training:	Epoch: [11][144/204]	Loss 0.6802 (0.6613)	
training:	Epoch: [11][145/204]	Loss 0.6314 (0.6611)	
training:	Epoch: [11][146/204]	Loss 0.6315 (0.6609)	
training:	Epoch: [11][147/204]	Loss 0.6578 (0.6608)	
training:	Epoch: [11][148/204]	Loss 0.6757 (0.6609)	
training:	Epoch: [11][149/204]	Loss 0.6628 (0.6609)	
training:	Epoch: [11][150/204]	Loss 0.6497 (0.6609)	
training:	Epoch: [11][151/204]	Loss 0.6257 (0.6606)	
training:	Epoch: [11][152/204]	Loss 0.6391 (0.6605)	
training:	Epoch: [11][153/204]	Loss 0.6720 (0.6606)	
training:	Epoch: [11][154/204]	Loss 0.6500 (0.6605)	
training:	Epoch: [11][155/204]	Loss 0.6483 (0.6604)	
training:	Epoch: [11][156/204]	Loss 0.6867 (0.6606)	
training:	Epoch: [11][157/204]	Loss 0.6903 (0.6608)	
training:	Epoch: [11][158/204]	Loss 0.6367 (0.6606)	
training:	Epoch: [11][159/204]	Loss 0.6539 (0.6606)	
training:	Epoch: [11][160/204]	Loss 0.6580 (0.6606)	
training:	Epoch: [11][161/204]	Loss 0.6281 (0.6604)	
training:	Epoch: [11][162/204]	Loss 0.6661 (0.6604)	
training:	Epoch: [11][163/204]	Loss 0.6830 (0.6605)	
training:	Epoch: [11][164/204]	Loss 0.6663 (0.6606)	
training:	Epoch: [11][165/204]	Loss 0.6180 (0.6603)	
training:	Epoch: [11][166/204]	Loss 0.6569 (0.6603)	
training:	Epoch: [11][167/204]	Loss 0.6950 (0.6605)	
training:	Epoch: [11][168/204]	Loss 0.6339 (0.6603)	
training:	Epoch: [11][169/204]	Loss 0.6635 (0.6604)	
training:	Epoch: [11][170/204]	Loss 0.6570 (0.6603)	
training:	Epoch: [11][171/204]	Loss 0.5992 (0.6600)	
training:	Epoch: [11][172/204]	Loss 0.6556 (0.6600)	
training:	Epoch: [11][173/204]	Loss 0.6497 (0.6599)	
training:	Epoch: [11][174/204]	Loss 0.6488 (0.6598)	
training:	Epoch: [11][175/204]	Loss 0.6661 (0.6599)	
training:	Epoch: [11][176/204]	Loss 0.6346 (0.6597)	
training:	Epoch: [11][177/204]	Loss 0.6491 (0.6597)	
training:	Epoch: [11][178/204]	Loss 0.5969 (0.6593)	
training:	Epoch: [11][179/204]	Loss 0.6604 (0.6593)	
training:	Epoch: [11][180/204]	Loss 0.6353 (0.6592)	
training:	Epoch: [11][181/204]	Loss 0.6273 (0.6590)	
training:	Epoch: [11][182/204]	Loss 0.6628 (0.6590)	
training:	Epoch: [11][183/204]	Loss 0.6558 (0.6590)	
training:	Epoch: [11][184/204]	Loss 0.7244 (0.6594)	
training:	Epoch: [11][185/204]	Loss 0.6542 (0.6593)	
training:	Epoch: [11][186/204]	Loss 0.6765 (0.6594)	
training:	Epoch: [11][187/204]	Loss 0.6435 (0.6594)	
training:	Epoch: [11][188/204]	Loss 0.6759 (0.6594)	
training:	Epoch: [11][189/204]	Loss 0.6782 (0.6595)	
training:	Epoch: [11][190/204]	Loss 0.6933 (0.6597)	
training:	Epoch: [11][191/204]	Loss 0.6242 (0.6595)	
training:	Epoch: [11][192/204]	Loss 0.6642 (0.6596)	
training:	Epoch: [11][193/204]	Loss 0.6630 (0.6596)	
training:	Epoch: [11][194/204]	Loss 0.6949 (0.6598)	
training:	Epoch: [11][195/204]	Loss 0.6657 (0.6598)	
training:	Epoch: [11][196/204]	Loss 0.6498 (0.6597)	
training:	Epoch: [11][197/204]	Loss 0.6798 (0.6598)	
training:	Epoch: [11][198/204]	Loss 0.6254 (0.6597)	
training:	Epoch: [11][199/204]	Loss 0.6776 (0.6598)	
training:	Epoch: [11][200/204]	Loss 0.6718 (0.6598)	
training:	Epoch: [11][201/204]	Loss 0.6369 (0.6597)	
training:	Epoch: [11][202/204]	Loss 0.6342 (0.6596)	
training:	Epoch: [11][203/204]	Loss 0.6509 (0.6595)	
training:	Epoch: [11][204/204]	Loss 0.6796 (0.6596)	
Training:	 Loss: 0.6586

Training:	 ACC: 0.6465 0.6488 0.7043 0.5886
Validation:	 ACC: 0.6316 0.6351 0.7083 0.5549
Validation:	 Best_BACC: 0.6316 0.6351 0.7083 0.5549
Validation:	 Loss: 0.6574
Pretraining:	Epoch 12/120
----------
training:	Epoch: [12][1/204]	Loss 0.6835 (0.6835)	
training:	Epoch: [12][2/204]	Loss 0.6528 (0.6682)	
training:	Epoch: [12][3/204]	Loss 0.6573 (0.6645)	
training:	Epoch: [12][4/204]	Loss 0.6459 (0.6599)	
training:	Epoch: [12][5/204]	Loss 0.6754 (0.6630)	
training:	Epoch: [12][6/204]	Loss 0.6758 (0.6651)	
training:	Epoch: [12][7/204]	Loss 0.6574 (0.6640)	
training:	Epoch: [12][8/204]	Loss 0.6275 (0.6595)	
training:	Epoch: [12][9/204]	Loss 0.6269 (0.6558)	
training:	Epoch: [12][10/204]	Loss 0.6645 (0.6567)	
training:	Epoch: [12][11/204]	Loss 0.6500 (0.6561)	
training:	Epoch: [12][12/204]	Loss 0.7003 (0.6598)	
training:	Epoch: [12][13/204]	Loss 0.6648 (0.6602)	
training:	Epoch: [12][14/204]	Loss 0.6539 (0.6597)	
training:	Epoch: [12][15/204]	Loss 0.6432 (0.6586)	
training:	Epoch: [12][16/204]	Loss 0.6701 (0.6593)	
training:	Epoch: [12][17/204]	Loss 0.6285 (0.6575)	
training:	Epoch: [12][18/204]	Loss 0.6115 (0.6550)	
training:	Epoch: [12][19/204]	Loss 0.6282 (0.6536)	
training:	Epoch: [12][20/204]	Loss 0.6271 (0.6522)	
training:	Epoch: [12][21/204]	Loss 0.6437 (0.6518)	
training:	Epoch: [12][22/204]	Loss 0.6498 (0.6517)	
training:	Epoch: [12][23/204]	Loss 0.7007 (0.6539)	
training:	Epoch: [12][24/204]	Loss 0.6506 (0.6537)	
training:	Epoch: [12][25/204]	Loss 0.6707 (0.6544)	
training:	Epoch: [12][26/204]	Loss 0.6964 (0.6560)	
training:	Epoch: [12][27/204]	Loss 0.6384 (0.6554)	
training:	Epoch: [12][28/204]	Loss 0.6483 (0.6551)	
training:	Epoch: [12][29/204]	Loss 0.6534 (0.6551)	
training:	Epoch: [12][30/204]	Loss 0.6279 (0.6542)	
training:	Epoch: [12][31/204]	Loss 0.6647 (0.6545)	
training:	Epoch: [12][32/204]	Loss 0.6373 (0.6540)	
training:	Epoch: [12][33/204]	Loss 0.6145 (0.6528)	
training:	Epoch: [12][34/204]	Loss 0.6462 (0.6526)	
training:	Epoch: [12][35/204]	Loss 0.6313 (0.6520)	
training:	Epoch: [12][36/204]	Loss 0.6345 (0.6515)	
training:	Epoch: [12][37/204]	Loss 0.6940 (0.6526)	
training:	Epoch: [12][38/204]	Loss 0.6703 (0.6531)	
training:	Epoch: [12][39/204]	Loss 0.6541 (0.6531)	
training:	Epoch: [12][40/204]	Loss 0.6478 (0.6530)	
training:	Epoch: [12][41/204]	Loss 0.6493 (0.6529)	
training:	Epoch: [12][42/204]	Loss 0.6295 (0.6523)	
training:	Epoch: [12][43/204]	Loss 0.6846 (0.6531)	
training:	Epoch: [12][44/204]	Loss 0.6722 (0.6535)	
training:	Epoch: [12][45/204]	Loss 0.6318 (0.6530)	
training:	Epoch: [12][46/204]	Loss 0.6179 (0.6523)	
training:	Epoch: [12][47/204]	Loss 0.6481 (0.6522)	
training:	Epoch: [12][48/204]	Loss 0.7074 (0.6533)	
training:	Epoch: [12][49/204]	Loss 0.6754 (0.6538)	
training:	Epoch: [12][50/204]	Loss 0.6334 (0.6534)	
training:	Epoch: [12][51/204]	Loss 0.6264 (0.6528)	
training:	Epoch: [12][52/204]	Loss 0.6476 (0.6527)	
training:	Epoch: [12][53/204]	Loss 0.6624 (0.6529)	
training:	Epoch: [12][54/204]	Loss 0.6209 (0.6523)	
training:	Epoch: [12][55/204]	Loss 0.6709 (0.6527)	
training:	Epoch: [12][56/204]	Loss 0.6348 (0.6524)	
training:	Epoch: [12][57/204]	Loss 0.6318 (0.6520)	
training:	Epoch: [12][58/204]	Loss 0.6504 (0.6520)	
training:	Epoch: [12][59/204]	Loss 0.6694 (0.6523)	
training:	Epoch: [12][60/204]	Loss 0.7167 (0.6533)	
training:	Epoch: [12][61/204]	Loss 0.6408 (0.6531)	
training:	Epoch: [12][62/204]	Loss 0.6530 (0.6531)	
training:	Epoch: [12][63/204]	Loss 0.7032 (0.6539)	
training:	Epoch: [12][64/204]	Loss 0.6325 (0.6536)	
training:	Epoch: [12][65/204]	Loss 0.6325 (0.6533)	
training:	Epoch: [12][66/204]	Loss 0.6325 (0.6530)	
training:	Epoch: [12][67/204]	Loss 0.6755 (0.6533)	
training:	Epoch: [12][68/204]	Loss 0.6118 (0.6527)	
training:	Epoch: [12][69/204]	Loss 0.6621 (0.6528)	
training:	Epoch: [12][70/204]	Loss 0.6126 (0.6522)	
training:	Epoch: [12][71/204]	Loss 0.6521 (0.6522)	
training:	Epoch: [12][72/204]	Loss 0.6465 (0.6522)	
training:	Epoch: [12][73/204]	Loss 0.6747 (0.6525)	
training:	Epoch: [12][74/204]	Loss 0.6663 (0.6527)	
training:	Epoch: [12][75/204]	Loss 0.6411 (0.6525)	
training:	Epoch: [12][76/204]	Loss 0.7031 (0.6532)	
training:	Epoch: [12][77/204]	Loss 0.6703 (0.6534)	
training:	Epoch: [12][78/204]	Loss 0.6835 (0.6538)	
training:	Epoch: [12][79/204]	Loss 0.6454 (0.6537)	
training:	Epoch: [12][80/204]	Loss 0.6111 (0.6531)	
training:	Epoch: [12][81/204]	Loss 0.6529 (0.6531)	
training:	Epoch: [12][82/204]	Loss 0.6596 (0.6532)	
training:	Epoch: [12][83/204]	Loss 0.6815 (0.6536)	
training:	Epoch: [12][84/204]	Loss 0.6613 (0.6536)	
training:	Epoch: [12][85/204]	Loss 0.6244 (0.6533)	
training:	Epoch: [12][86/204]	Loss 0.6733 (0.6535)	
training:	Epoch: [12][87/204]	Loss 0.6428 (0.6534)	
training:	Epoch: [12][88/204]	Loss 0.6383 (0.6532)	
training:	Epoch: [12][89/204]	Loss 0.6243 (0.6529)	
training:	Epoch: [12][90/204]	Loss 0.6222 (0.6526)	
training:	Epoch: [12][91/204]	Loss 0.7027 (0.6531)	
training:	Epoch: [12][92/204]	Loss 0.6183 (0.6527)	
training:	Epoch: [12][93/204]	Loss 0.6511 (0.6527)	
training:	Epoch: [12][94/204]	Loss 0.6472 (0.6527)	
training:	Epoch: [12][95/204]	Loss 0.6755 (0.6529)	
training:	Epoch: [12][96/204]	Loss 0.6648 (0.6530)	
training:	Epoch: [12][97/204]	Loss 0.6705 (0.6532)	
training:	Epoch: [12][98/204]	Loss 0.6750 (0.6534)	
training:	Epoch: [12][99/204]	Loss 0.6155 (0.6531)	
training:	Epoch: [12][100/204]	Loss 0.6245 (0.6528)	
training:	Epoch: [12][101/204]	Loss 0.6161 (0.6524)	
training:	Epoch: [12][102/204]	Loss 0.6271 (0.6522)	
training:	Epoch: [12][103/204]	Loss 0.6177 (0.6518)	
training:	Epoch: [12][104/204]	Loss 0.6523 (0.6518)	
training:	Epoch: [12][105/204]	Loss 0.6501 (0.6518)	
training:	Epoch: [12][106/204]	Loss 0.6100 (0.6514)	
training:	Epoch: [12][107/204]	Loss 0.6986 (0.6519)	
training:	Epoch: [12][108/204]	Loss 0.6690 (0.6520)	
training:	Epoch: [12][109/204]	Loss 0.6723 (0.6522)	
training:	Epoch: [12][110/204]	Loss 0.6716 (0.6524)	
training:	Epoch: [12][111/204]	Loss 0.6729 (0.6526)	
training:	Epoch: [12][112/204]	Loss 0.6360 (0.6524)	
training:	Epoch: [12][113/204]	Loss 0.6629 (0.6525)	
training:	Epoch: [12][114/204]	Loss 0.7159 (0.6531)	
training:	Epoch: [12][115/204]	Loss 0.6625 (0.6531)	
training:	Epoch: [12][116/204]	Loss 0.6856 (0.6534)	
training:	Epoch: [12][117/204]	Loss 0.6449 (0.6534)	
training:	Epoch: [12][118/204]	Loss 0.6783 (0.6536)	
training:	Epoch: [12][119/204]	Loss 0.6854 (0.6538)	
training:	Epoch: [12][120/204]	Loss 0.6557 (0.6539)	
training:	Epoch: [12][121/204]	Loss 0.6433 (0.6538)	
training:	Epoch: [12][122/204]	Loss 0.6642 (0.6538)	
training:	Epoch: [12][123/204]	Loss 0.6573 (0.6539)	
training:	Epoch: [12][124/204]	Loss 0.6539 (0.6539)	
training:	Epoch: [12][125/204]	Loss 0.6795 (0.6541)	
training:	Epoch: [12][126/204]	Loss 0.6522 (0.6541)	
training:	Epoch: [12][127/204]	Loss 0.6914 (0.6544)	
training:	Epoch: [12][128/204]	Loss 0.6447 (0.6543)	
training:	Epoch: [12][129/204]	Loss 0.6463 (0.6542)	
training:	Epoch: [12][130/204]	Loss 0.6845 (0.6545)	
training:	Epoch: [12][131/204]	Loss 0.6507 (0.6544)	
training:	Epoch: [12][132/204]	Loss 0.6460 (0.6544)	
training:	Epoch: [12][133/204]	Loss 0.6609 (0.6544)	
training:	Epoch: [12][134/204]	Loss 0.6490 (0.6544)	
training:	Epoch: [12][135/204]	Loss 0.6884 (0.6546)	
training:	Epoch: [12][136/204]	Loss 0.6492 (0.6546)	
training:	Epoch: [12][137/204]	Loss 0.6567 (0.6546)	
training:	Epoch: [12][138/204]	Loss 0.6484 (0.6546)	
training:	Epoch: [12][139/204]	Loss 0.6506 (0.6545)	
training:	Epoch: [12][140/204]	Loss 0.6062 (0.6542)	
training:	Epoch: [12][141/204]	Loss 0.6724 (0.6543)	
training:	Epoch: [12][142/204]	Loss 0.6280 (0.6541)	
training:	Epoch: [12][143/204]	Loss 0.6291 (0.6539)	
training:	Epoch: [12][144/204]	Loss 0.6628 (0.6540)	
training:	Epoch: [12][145/204]	Loss 0.6623 (0.6541)	
training:	Epoch: [12][146/204]	Loss 0.6566 (0.6541)	
training:	Epoch: [12][147/204]	Loss 0.6732 (0.6542)	
training:	Epoch: [12][148/204]	Loss 0.6606 (0.6543)	
training:	Epoch: [12][149/204]	Loss 0.6287 (0.6541)	
training:	Epoch: [12][150/204]	Loss 0.6230 (0.6539)	
training:	Epoch: [12][151/204]	Loss 0.6490 (0.6538)	
training:	Epoch: [12][152/204]	Loss 0.6780 (0.6540)	
training:	Epoch: [12][153/204]	Loss 0.6593 (0.6540)	
training:	Epoch: [12][154/204]	Loss 0.6965 (0.6543)	
training:	Epoch: [12][155/204]	Loss 0.6782 (0.6545)	
training:	Epoch: [12][156/204]	Loss 0.6591 (0.6545)	
training:	Epoch: [12][157/204]	Loss 0.6388 (0.6544)	
training:	Epoch: [12][158/204]	Loss 0.6672 (0.6545)	
training:	Epoch: [12][159/204]	Loss 0.7005 (0.6548)	
training:	Epoch: [12][160/204]	Loss 0.6602 (0.6548)	
training:	Epoch: [12][161/204]	Loss 0.6496 (0.6548)	
training:	Epoch: [12][162/204]	Loss 0.6809 (0.6549)	
training:	Epoch: [12][163/204]	Loss 0.6556 (0.6549)	
training:	Epoch: [12][164/204]	Loss 0.6476 (0.6549)	
training:	Epoch: [12][165/204]	Loss 0.6201 (0.6547)	
training:	Epoch: [12][166/204]	Loss 0.6393 (0.6546)	
training:	Epoch: [12][167/204]	Loss 0.6479 (0.6545)	
training:	Epoch: [12][168/204]	Loss 0.6251 (0.6544)	
training:	Epoch: [12][169/204]	Loss 0.6693 (0.6545)	
training:	Epoch: [12][170/204]	Loss 0.5953 (0.6541)	
training:	Epoch: [12][171/204]	Loss 0.6555 (0.6541)	
training:	Epoch: [12][172/204]	Loss 0.6776 (0.6543)	
training:	Epoch: [12][173/204]	Loss 0.6659 (0.6543)	
training:	Epoch: [12][174/204]	Loss 0.6389 (0.6542)	
training:	Epoch: [12][175/204]	Loss 0.6842 (0.6544)	
training:	Epoch: [12][176/204]	Loss 0.6367 (0.6543)	
training:	Epoch: [12][177/204]	Loss 0.6367 (0.6542)	
training:	Epoch: [12][178/204]	Loss 0.6689 (0.6543)	
training:	Epoch: [12][179/204]	Loss 0.6478 (0.6543)	
training:	Epoch: [12][180/204]	Loss 0.6592 (0.6543)	
training:	Epoch: [12][181/204]	Loss 0.6382 (0.6542)	
training:	Epoch: [12][182/204]	Loss 0.6781 (0.6543)	
training:	Epoch: [12][183/204]	Loss 0.6379 (0.6542)	
training:	Epoch: [12][184/204]	Loss 0.6729 (0.6543)	
training:	Epoch: [12][185/204]	Loss 0.6184 (0.6541)	
training:	Epoch: [12][186/204]	Loss 0.6124 (0.6539)	
training:	Epoch: [12][187/204]	Loss 0.6334 (0.6538)	
training:	Epoch: [12][188/204]	Loss 0.6836 (0.6540)	
training:	Epoch: [12][189/204]	Loss 0.6189 (0.6538)	
training:	Epoch: [12][190/204]	Loss 0.6398 (0.6537)	
training:	Epoch: [12][191/204]	Loss 0.6810 (0.6539)	
training:	Epoch: [12][192/204]	Loss 0.6279 (0.6537)	
training:	Epoch: [12][193/204]	Loss 0.6516 (0.6537)	
training:	Epoch: [12][194/204]	Loss 0.6457 (0.6537)	
training:	Epoch: [12][195/204]	Loss 0.6813 (0.6538)	
training:	Epoch: [12][196/204]	Loss 0.6516 (0.6538)	
training:	Epoch: [12][197/204]	Loss 0.6635 (0.6538)	
training:	Epoch: [12][198/204]	Loss 0.7074 (0.6541)	
training:	Epoch: [12][199/204]	Loss 0.6706 (0.6542)	
training:	Epoch: [12][200/204]	Loss 0.6257 (0.6541)	
training:	Epoch: [12][201/204]	Loss 0.6727 (0.6541)	
training:	Epoch: [12][202/204]	Loss 0.6301 (0.6540)	
training:	Epoch: [12][203/204]	Loss 0.6829 (0.6542)	
training:	Epoch: [12][204/204]	Loss 0.6451 (0.6541)	
Training:	 Loss: 0.6531

Training:	 ACC: 0.6570 0.6582 0.6864 0.6276
Validation:	 ACC: 0.6425 0.6447 0.6909 0.5942
Validation:	 Best_BACC: 0.6425 0.6447 0.6909 0.5942
Validation:	 Loss: 0.6518
Pretraining:	Epoch 13/120
----------
training:	Epoch: [13][1/204]	Loss 0.6573 (0.6573)	
training:	Epoch: [13][2/204]	Loss 0.6474 (0.6523)	
training:	Epoch: [13][3/204]	Loss 0.6274 (0.6440)	
training:	Epoch: [13][4/204]	Loss 0.6679 (0.6500)	
training:	Epoch: [13][5/204]	Loss 0.6328 (0.6466)	
training:	Epoch: [13][6/204]	Loss 0.5927 (0.6376)	
training:	Epoch: [13][7/204]	Loss 0.6758 (0.6430)	
training:	Epoch: [13][8/204]	Loss 0.7212 (0.6528)	
training:	Epoch: [13][9/204]	Loss 0.6475 (0.6522)	
training:	Epoch: [13][10/204]	Loss 0.5876 (0.6458)	
training:	Epoch: [13][11/204]	Loss 0.6677 (0.6478)	
training:	Epoch: [13][12/204]	Loss 0.7013 (0.6522)	
training:	Epoch: [13][13/204]	Loss 0.6565 (0.6525)	
training:	Epoch: [13][14/204]	Loss 0.6532 (0.6526)	
training:	Epoch: [13][15/204]	Loss 0.6915 (0.6552)	
training:	Epoch: [13][16/204]	Loss 0.5790 (0.6504)	
training:	Epoch: [13][17/204]	Loss 0.6473 (0.6502)	
training:	Epoch: [13][18/204]	Loss 0.6450 (0.6499)	
training:	Epoch: [13][19/204]	Loss 0.6352 (0.6492)	
training:	Epoch: [13][20/204]	Loss 0.6818 (0.6508)	
training:	Epoch: [13][21/204]	Loss 0.6701 (0.6517)	
training:	Epoch: [13][22/204]	Loss 0.6643 (0.6523)	
training:	Epoch: [13][23/204]	Loss 0.6186 (0.6508)	
training:	Epoch: [13][24/204]	Loss 0.7061 (0.6531)	
training:	Epoch: [13][25/204]	Loss 0.6273 (0.6521)	
training:	Epoch: [13][26/204]	Loss 0.6295 (0.6512)	
training:	Epoch: [13][27/204]	Loss 0.6604 (0.6516)	
training:	Epoch: [13][28/204]	Loss 0.6432 (0.6513)	
training:	Epoch: [13][29/204]	Loss 0.6215 (0.6502)	
training:	Epoch: [13][30/204]	Loss 0.6352 (0.6497)	
training:	Epoch: [13][31/204]	Loss 0.7058 (0.6515)	
training:	Epoch: [13][32/204]	Loss 0.6295 (0.6509)	
training:	Epoch: [13][33/204]	Loss 0.6381 (0.6505)	
training:	Epoch: [13][34/204]	Loss 0.6191 (0.6495)	
training:	Epoch: [13][35/204]	Loss 0.6345 (0.6491)	
training:	Epoch: [13][36/204]	Loss 0.6763 (0.6499)	
training:	Epoch: [13][37/204]	Loss 0.6123 (0.6489)	
training:	Epoch: [13][38/204]	Loss 0.6685 (0.6494)	
training:	Epoch: [13][39/204]	Loss 0.6101 (0.6484)	
training:	Epoch: [13][40/204]	Loss 0.6656 (0.6488)	
training:	Epoch: [13][41/204]	Loss 0.6331 (0.6484)	
training:	Epoch: [13][42/204]	Loss 0.6743 (0.6490)	
training:	Epoch: [13][43/204]	Loss 0.6255 (0.6485)	
training:	Epoch: [13][44/204]	Loss 0.6224 (0.6479)	
training:	Epoch: [13][45/204]	Loss 0.6181 (0.6472)	
training:	Epoch: [13][46/204]	Loss 0.6576 (0.6475)	
training:	Epoch: [13][47/204]	Loss 0.6086 (0.6466)	
training:	Epoch: [13][48/204]	Loss 0.5935 (0.6455)	
training:	Epoch: [13][49/204]	Loss 0.6191 (0.6450)	
training:	Epoch: [13][50/204]	Loss 0.6755 (0.6456)	
training:	Epoch: [13][51/204]	Loss 0.6628 (0.6459)	
training:	Epoch: [13][52/204]	Loss 0.6244 (0.6455)	
training:	Epoch: [13][53/204]	Loss 0.6267 (0.6452)	
training:	Epoch: [13][54/204]	Loss 0.6550 (0.6453)	
training:	Epoch: [13][55/204]	Loss 0.6178 (0.6448)	
training:	Epoch: [13][56/204]	Loss 0.6796 (0.6455)	
training:	Epoch: [13][57/204]	Loss 0.6807 (0.6461)	
training:	Epoch: [13][58/204]	Loss 0.7307 (0.6475)	
training:	Epoch: [13][59/204]	Loss 0.6524 (0.6476)	
training:	Epoch: [13][60/204]	Loss 0.6317 (0.6474)	
training:	Epoch: [13][61/204]	Loss 0.6179 (0.6469)	
training:	Epoch: [13][62/204]	Loss 0.6618 (0.6471)	
training:	Epoch: [13][63/204]	Loss 0.6377 (0.6470)	
training:	Epoch: [13][64/204]	Loss 0.6249 (0.6466)	
training:	Epoch: [13][65/204]	Loss 0.6154 (0.6461)	
training:	Epoch: [13][66/204]	Loss 0.6557 (0.6463)	
training:	Epoch: [13][67/204]	Loss 0.6418 (0.6462)	
training:	Epoch: [13][68/204]	Loss 0.5876 (0.6454)	
training:	Epoch: [13][69/204]	Loss 0.6610 (0.6456)	
training:	Epoch: [13][70/204]	Loss 0.6818 (0.6461)	
training:	Epoch: [13][71/204]	Loss 0.6386 (0.6460)	
training:	Epoch: [13][72/204]	Loss 0.6904 (0.6466)	
training:	Epoch: [13][73/204]	Loss 0.6101 (0.6461)	
training:	Epoch: [13][74/204]	Loss 0.6614 (0.6463)	
training:	Epoch: [13][75/204]	Loss 0.6160 (0.6459)	
training:	Epoch: [13][76/204]	Loss 0.6696 (0.6462)	
training:	Epoch: [13][77/204]	Loss 0.6677 (0.6465)	
training:	Epoch: [13][78/204]	Loss 0.6290 (0.6463)	
training:	Epoch: [13][79/204]	Loss 0.6570 (0.6464)	
training:	Epoch: [13][80/204]	Loss 0.6769 (0.6468)	
training:	Epoch: [13][81/204]	Loss 0.6485 (0.6468)	
training:	Epoch: [13][82/204]	Loss 0.6823 (0.6472)	
training:	Epoch: [13][83/204]	Loss 0.6394 (0.6472)	
training:	Epoch: [13][84/204]	Loss 0.6585 (0.6473)	
training:	Epoch: [13][85/204]	Loss 0.6438 (0.6472)	
training:	Epoch: [13][86/204]	Loss 0.6593 (0.6474)	
training:	Epoch: [13][87/204]	Loss 0.6722 (0.6477)	
training:	Epoch: [13][88/204]	Loss 0.6556 (0.6478)	
training:	Epoch: [13][89/204]	Loss 0.6438 (0.6477)	
training:	Epoch: [13][90/204]	Loss 0.6812 (0.6481)	
training:	Epoch: [13][91/204]	Loss 0.6269 (0.6479)	
training:	Epoch: [13][92/204]	Loss 0.6577 (0.6480)	
training:	Epoch: [13][93/204]	Loss 0.6247 (0.6477)	
training:	Epoch: [13][94/204]	Loss 0.6687 (0.6479)	
training:	Epoch: [13][95/204]	Loss 0.6588 (0.6481)	
training:	Epoch: [13][96/204]	Loss 0.6428 (0.6480)	
training:	Epoch: [13][97/204]	Loss 0.6727 (0.6483)	
training:	Epoch: [13][98/204]	Loss 0.6397 (0.6482)	
training:	Epoch: [13][99/204]	Loss 0.6701 (0.6484)	
training:	Epoch: [13][100/204]	Loss 0.6479 (0.6484)	
training:	Epoch: [13][101/204]	Loss 0.6739 (0.6486)	
training:	Epoch: [13][102/204]	Loss 0.6208 (0.6484)	
training:	Epoch: [13][103/204]	Loss 0.6489 (0.6484)	
training:	Epoch: [13][104/204]	Loss 0.6434 (0.6483)	
training:	Epoch: [13][105/204]	Loss 0.6334 (0.6482)	
training:	Epoch: [13][106/204]	Loss 0.6437 (0.6481)	
training:	Epoch: [13][107/204]	Loss 0.6175 (0.6478)	
training:	Epoch: [13][108/204]	Loss 0.6443 (0.6478)	
training:	Epoch: [13][109/204]	Loss 0.6229 (0.6476)	
training:	Epoch: [13][110/204]	Loss 0.6488 (0.6476)	
training:	Epoch: [13][111/204]	Loss 0.6644 (0.6477)	
training:	Epoch: [13][112/204]	Loss 0.6989 (0.6482)	
training:	Epoch: [13][113/204]	Loss 0.6868 (0.6485)	
training:	Epoch: [13][114/204]	Loss 0.6654 (0.6487)	
training:	Epoch: [13][115/204]	Loss 0.6246 (0.6485)	
training:	Epoch: [13][116/204]	Loss 0.6304 (0.6483)	
training:	Epoch: [13][117/204]	Loss 0.6857 (0.6486)	
training:	Epoch: [13][118/204]	Loss 0.6128 (0.6483)	
training:	Epoch: [13][119/204]	Loss 0.6389 (0.6483)	
training:	Epoch: [13][120/204]	Loss 0.6575 (0.6483)	
training:	Epoch: [13][121/204]	Loss 0.6857 (0.6487)	
training:	Epoch: [13][122/204]	Loss 0.6597 (0.6487)	
training:	Epoch: [13][123/204]	Loss 0.6344 (0.6486)	
training:	Epoch: [13][124/204]	Loss 0.6823 (0.6489)	
training:	Epoch: [13][125/204]	Loss 0.5963 (0.6485)	
training:	Epoch: [13][126/204]	Loss 0.6546 (0.6485)	
training:	Epoch: [13][127/204]	Loss 0.6411 (0.6485)	
training:	Epoch: [13][128/204]	Loss 0.6583 (0.6485)	
training:	Epoch: [13][129/204]	Loss 0.6302 (0.6484)	
training:	Epoch: [13][130/204]	Loss 0.6687 (0.6486)	
training:	Epoch: [13][131/204]	Loss 0.6847 (0.6488)	
training:	Epoch: [13][132/204]	Loss 0.6422 (0.6488)	
training:	Epoch: [13][133/204]	Loss 0.6068 (0.6485)	
training:	Epoch: [13][134/204]	Loss 0.6568 (0.6485)	
training:	Epoch: [13][135/204]	Loss 0.6554 (0.6486)	
training:	Epoch: [13][136/204]	Loss 0.6096 (0.6483)	
training:	Epoch: [13][137/204]	Loss 0.6385 (0.6482)	
training:	Epoch: [13][138/204]	Loss 0.6776 (0.6484)	
training:	Epoch: [13][139/204]	Loss 0.6546 (0.6485)	
training:	Epoch: [13][140/204]	Loss 0.6250 (0.6483)	
training:	Epoch: [13][141/204]	Loss 0.6654 (0.6484)	
training:	Epoch: [13][142/204]	Loss 0.6416 (0.6484)	
training:	Epoch: [13][143/204]	Loss 0.6323 (0.6483)	
training:	Epoch: [13][144/204]	Loss 0.6211 (0.6481)	
training:	Epoch: [13][145/204]	Loss 0.6296 (0.6480)	
training:	Epoch: [13][146/204]	Loss 0.6346 (0.6479)	
training:	Epoch: [13][147/204]	Loss 0.6300 (0.6477)	
training:	Epoch: [13][148/204]	Loss 0.6716 (0.6479)	
training:	Epoch: [13][149/204]	Loss 0.6321 (0.6478)	
training:	Epoch: [13][150/204]	Loss 0.6827 (0.6480)	
training:	Epoch: [13][151/204]	Loss 0.6218 (0.6479)	
training:	Epoch: [13][152/204]	Loss 0.6561 (0.6479)	
training:	Epoch: [13][153/204]	Loss 0.6406 (0.6479)	
training:	Epoch: [13][154/204]	Loss 0.6187 (0.6477)	
training:	Epoch: [13][155/204]	Loss 0.6423 (0.6476)	
training:	Epoch: [13][156/204]	Loss 0.6355 (0.6476)	
training:	Epoch: [13][157/204]	Loss 0.6222 (0.6474)	
training:	Epoch: [13][158/204]	Loss 0.6676 (0.6475)	
training:	Epoch: [13][159/204]	Loss 0.6408 (0.6475)	
training:	Epoch: [13][160/204]	Loss 0.6568 (0.6475)	
training:	Epoch: [13][161/204]	Loss 0.6117 (0.6473)	
training:	Epoch: [13][162/204]	Loss 0.6460 (0.6473)	
training:	Epoch: [13][163/204]	Loss 0.6856 (0.6475)	
training:	Epoch: [13][164/204]	Loss 0.6423 (0.6475)	
training:	Epoch: [13][165/204]	Loss 0.6613 (0.6476)	
training:	Epoch: [13][166/204]	Loss 0.6457 (0.6476)	
training:	Epoch: [13][167/204]	Loss 0.6618 (0.6477)	
training:	Epoch: [13][168/204]	Loss 0.6534 (0.6477)	
training:	Epoch: [13][169/204]	Loss 0.6535 (0.6477)	
training:	Epoch: [13][170/204]	Loss 0.6579 (0.6478)	
training:	Epoch: [13][171/204]	Loss 0.6854 (0.6480)	
training:	Epoch: [13][172/204]	Loss 0.6370 (0.6480)	
training:	Epoch: [13][173/204]	Loss 0.6113 (0.6477)	
training:	Epoch: [13][174/204]	Loss 0.6329 (0.6477)	
training:	Epoch: [13][175/204]	Loss 0.6652 (0.6478)	
training:	Epoch: [13][176/204]	Loss 0.6342 (0.6477)	
training:	Epoch: [13][177/204]	Loss 0.6971 (0.6480)	
training:	Epoch: [13][178/204]	Loss 0.6267 (0.6478)	
training:	Epoch: [13][179/204]	Loss 0.6961 (0.6481)	
training:	Epoch: [13][180/204]	Loss 0.6296 (0.6480)	
training:	Epoch: [13][181/204]	Loss 0.6877 (0.6482)	
training:	Epoch: [13][182/204]	Loss 0.5959 (0.6479)	
training:	Epoch: [13][183/204]	Loss 0.7526 (0.6485)	
training:	Epoch: [13][184/204]	Loss 0.6210 (0.6484)	
training:	Epoch: [13][185/204]	Loss 0.6375 (0.6483)	
training:	Epoch: [13][186/204]	Loss 0.5990 (0.6480)	
training:	Epoch: [13][187/204]	Loss 0.6594 (0.6481)	
training:	Epoch: [13][188/204]	Loss 0.6600 (0.6482)	
training:	Epoch: [13][189/204]	Loss 0.6652 (0.6483)	
training:	Epoch: [13][190/204]	Loss 0.6301 (0.6482)	
training:	Epoch: [13][191/204]	Loss 0.6480 (0.6482)	
training:	Epoch: [13][192/204]	Loss 0.6348 (0.6481)	
training:	Epoch: [13][193/204]	Loss 0.6632 (0.6482)	
training:	Epoch: [13][194/204]	Loss 0.6508 (0.6482)	
training:	Epoch: [13][195/204]	Loss 0.6226 (0.6481)	
training:	Epoch: [13][196/204]	Loss 0.6340 (0.6480)	
training:	Epoch: [13][197/204]	Loss 0.6091 (0.6478)	
training:	Epoch: [13][198/204]	Loss 0.6606 (0.6478)	
training:	Epoch: [13][199/204]	Loss 0.5986 (0.6476)	
training:	Epoch: [13][200/204]	Loss 0.6040 (0.6474)	
training:	Epoch: [13][201/204]	Loss 0.6565 (0.6474)	
training:	Epoch: [13][202/204]	Loss 0.6909 (0.6476)	
training:	Epoch: [13][203/204]	Loss 0.6114 (0.6475)	
training:	Epoch: [13][204/204]	Loss 0.6909 (0.6477)	
Training:	 Loss: 0.6467

Training:	 ACC: 0.6559 0.6589 0.7302 0.5816
Validation:	 ACC: 0.6463 0.6506 0.7421 0.5504
Validation:	 Best_BACC: 0.6463 0.6506 0.7421 0.5504
Validation:	 Loss: 0.6460
Pretraining:	Epoch 14/120
----------
training:	Epoch: [14][1/204]	Loss 0.6678 (0.6678)	
training:	Epoch: [14][2/204]	Loss 0.6471 (0.6575)	
training:	Epoch: [14][3/204]	Loss 0.7023 (0.6724)	
training:	Epoch: [14][4/204]	Loss 0.6600 (0.6693)	
training:	Epoch: [14][5/204]	Loss 0.6580 (0.6670)	
training:	Epoch: [14][6/204]	Loss 0.6048 (0.6567)	
training:	Epoch: [14][7/204]	Loss 0.6406 (0.6544)	
training:	Epoch: [14][8/204]	Loss 0.5724 (0.6441)	
training:	Epoch: [14][9/204]	Loss 0.6163 (0.6410)	
training:	Epoch: [14][10/204]	Loss 0.7002 (0.6469)	
training:	Epoch: [14][11/204]	Loss 0.6340 (0.6458)	
training:	Epoch: [14][12/204]	Loss 0.6999 (0.6503)	
training:	Epoch: [14][13/204]	Loss 0.6494 (0.6502)	
training:	Epoch: [14][14/204]	Loss 0.6438 (0.6497)	
training:	Epoch: [14][15/204]	Loss 0.6463 (0.6495)	
training:	Epoch: [14][16/204]	Loss 0.6544 (0.6498)	
training:	Epoch: [14][17/204]	Loss 0.6438 (0.6495)	
training:	Epoch: [14][18/204]	Loss 0.6435 (0.6491)	
training:	Epoch: [14][19/204]	Loss 0.6342 (0.6483)	
training:	Epoch: [14][20/204]	Loss 0.6530 (0.6486)	
training:	Epoch: [14][21/204]	Loss 0.6196 (0.6472)	
training:	Epoch: [14][22/204]	Loss 0.6501 (0.6473)	
training:	Epoch: [14][23/204]	Loss 0.6325 (0.6467)	
training:	Epoch: [14][24/204]	Loss 0.6502 (0.6468)	
training:	Epoch: [14][25/204]	Loss 0.5918 (0.6446)	
training:	Epoch: [14][26/204]	Loss 0.6482 (0.6448)	
training:	Epoch: [14][27/204]	Loss 0.6265 (0.6441)	
training:	Epoch: [14][28/204]	Loss 0.5874 (0.6421)	
training:	Epoch: [14][29/204]	Loss 0.6573 (0.6426)	
training:	Epoch: [14][30/204]	Loss 0.6378 (0.6424)	
training:	Epoch: [14][31/204]	Loss 0.6878 (0.6439)	
training:	Epoch: [14][32/204]	Loss 0.6071 (0.6427)	
training:	Epoch: [14][33/204]	Loss 0.6237 (0.6422)	
training:	Epoch: [14][34/204]	Loss 0.6882 (0.6435)	
training:	Epoch: [14][35/204]	Loss 0.5916 (0.6420)	
training:	Epoch: [14][36/204]	Loss 0.6754 (0.6430)	
training:	Epoch: [14][37/204]	Loss 0.6497 (0.6431)	
training:	Epoch: [14][38/204]	Loss 0.5983 (0.6420)	
training:	Epoch: [14][39/204]	Loss 0.6300 (0.6417)	
training:	Epoch: [14][40/204]	Loss 0.6110 (0.6409)	
training:	Epoch: [14][41/204]	Loss 0.6509 (0.6411)	
training:	Epoch: [14][42/204]	Loss 0.6111 (0.6404)	
training:	Epoch: [14][43/204]	Loss 0.6341 (0.6403)	
training:	Epoch: [14][44/204]	Loss 0.6801 (0.6412)	
training:	Epoch: [14][45/204]	Loss 0.6505 (0.6414)	
training:	Epoch: [14][46/204]	Loss 0.6127 (0.6408)	
training:	Epoch: [14][47/204]	Loss 0.6306 (0.6405)	
training:	Epoch: [14][48/204]	Loss 0.6198 (0.6401)	
training:	Epoch: [14][49/204]	Loss 0.6036 (0.6394)	
training:	Epoch: [14][50/204]	Loss 0.6057 (0.6387)	
training:	Epoch: [14][51/204]	Loss 0.6495 (0.6389)	
training:	Epoch: [14][52/204]	Loss 0.6655 (0.6394)	
training:	Epoch: [14][53/204]	Loss 0.6654 (0.6399)	
training:	Epoch: [14][54/204]	Loss 0.6439 (0.6400)	
training:	Epoch: [14][55/204]	Loss 0.6905 (0.6409)	
training:	Epoch: [14][56/204]	Loss 0.6510 (0.6411)	
training:	Epoch: [14][57/204]	Loss 0.6584 (0.6414)	
training:	Epoch: [14][58/204]	Loss 0.5890 (0.6405)	
training:	Epoch: [14][59/204]	Loss 0.6149 (0.6400)	
training:	Epoch: [14][60/204]	Loss 0.6413 (0.6401)	
training:	Epoch: [14][61/204]	Loss 0.5993 (0.6394)	
training:	Epoch: [14][62/204]	Loss 0.6426 (0.6395)	
training:	Epoch: [14][63/204]	Loss 0.6225 (0.6392)	
training:	Epoch: [14][64/204]	Loss 0.6703 (0.6397)	
training:	Epoch: [14][65/204]	Loss 0.6283 (0.6395)	
training:	Epoch: [14][66/204]	Loss 0.6215 (0.6392)	
training:	Epoch: [14][67/204]	Loss 0.6331 (0.6391)	
training:	Epoch: [14][68/204]	Loss 0.6588 (0.6394)	
training:	Epoch: [14][69/204]	Loss 0.6463 (0.6395)	
training:	Epoch: [14][70/204]	Loss 0.6819 (0.6401)	
training:	Epoch: [14][71/204]	Loss 0.6793 (0.6407)	
training:	Epoch: [14][72/204]	Loss 0.6281 (0.6405)	
training:	Epoch: [14][73/204]	Loss 0.6413 (0.6405)	
training:	Epoch: [14][74/204]	Loss 0.6573 (0.6407)	
training:	Epoch: [14][75/204]	Loss 0.6546 (0.6409)	
training:	Epoch: [14][76/204]	Loss 0.6687 (0.6413)	
training:	Epoch: [14][77/204]	Loss 0.6348 (0.6412)	
training:	Epoch: [14][78/204]	Loss 0.6048 (0.6407)	
training:	Epoch: [14][79/204]	Loss 0.6122 (0.6404)	
training:	Epoch: [14][80/204]	Loss 0.5920 (0.6398)	
training:	Epoch: [14][81/204]	Loss 0.5899 (0.6392)	
training:	Epoch: [14][82/204]	Loss 0.6348 (0.6391)	
training:	Epoch: [14][83/204]	Loss 0.6297 (0.6390)	
training:	Epoch: [14][84/204]	Loss 0.6456 (0.6391)	
training:	Epoch: [14][85/204]	Loss 0.6096 (0.6387)	
training:	Epoch: [14][86/204]	Loss 0.6450 (0.6388)	
training:	Epoch: [14][87/204]	Loss 0.5971 (0.6383)	
training:	Epoch: [14][88/204]	Loss 0.6336 (0.6383)	
training:	Epoch: [14][89/204]	Loss 0.6224 (0.6381)	
training:	Epoch: [14][90/204]	Loss 0.6350 (0.6381)	
training:	Epoch: [14][91/204]	Loss 0.6965 (0.6387)	
training:	Epoch: [14][92/204]	Loss 0.6467 (0.6388)	
training:	Epoch: [14][93/204]	Loss 0.6668 (0.6391)	
training:	Epoch: [14][94/204]	Loss 0.6681 (0.6394)	
training:	Epoch: [14][95/204]	Loss 0.6779 (0.6398)	
training:	Epoch: [14][96/204]	Loss 0.6279 (0.6397)	
training:	Epoch: [14][97/204]	Loss 0.6741 (0.6400)	
training:	Epoch: [14][98/204]	Loss 0.7452 (0.6411)	
training:	Epoch: [14][99/204]	Loss 0.6714 (0.6414)	
training:	Epoch: [14][100/204]	Loss 0.6808 (0.6418)	
training:	Epoch: [14][101/204]	Loss 0.6693 (0.6421)	
training:	Epoch: [14][102/204]	Loss 0.6568 (0.6422)	
training:	Epoch: [14][103/204]	Loss 0.6192 (0.6420)	
training:	Epoch: [14][104/204]	Loss 0.6492 (0.6421)	
training:	Epoch: [14][105/204]	Loss 0.6162 (0.6418)	
training:	Epoch: [14][106/204]	Loss 0.6400 (0.6418)	
training:	Epoch: [14][107/204]	Loss 0.6094 (0.6415)	
training:	Epoch: [14][108/204]	Loss 0.5950 (0.6411)	
training:	Epoch: [14][109/204]	Loss 0.7069 (0.6417)	
training:	Epoch: [14][110/204]	Loss 0.6315 (0.6416)	
training:	Epoch: [14][111/204]	Loss 0.6122 (0.6413)	
training:	Epoch: [14][112/204]	Loss 0.6488 (0.6414)	
training:	Epoch: [14][113/204]	Loss 0.6145 (0.6411)	
training:	Epoch: [14][114/204]	Loss 0.6186 (0.6409)	
training:	Epoch: [14][115/204]	Loss 0.6478 (0.6410)	
training:	Epoch: [14][116/204]	Loss 0.6364 (0.6410)	
training:	Epoch: [14][117/204]	Loss 0.6002 (0.6406)	
training:	Epoch: [14][118/204]	Loss 0.6138 (0.6404)	
training:	Epoch: [14][119/204]	Loss 0.6000 (0.6400)	
training:	Epoch: [14][120/204]	Loss 0.6746 (0.6403)	
training:	Epoch: [14][121/204]	Loss 0.6477 (0.6404)	
training:	Epoch: [14][122/204]	Loss 0.6597 (0.6406)	
training:	Epoch: [14][123/204]	Loss 0.6357 (0.6405)	
training:	Epoch: [14][124/204]	Loss 0.6582 (0.6407)	
training:	Epoch: [14][125/204]	Loss 0.6141 (0.6404)	
training:	Epoch: [14][126/204]	Loss 0.6641 (0.6406)	
training:	Epoch: [14][127/204]	Loss 0.6569 (0.6408)	
training:	Epoch: [14][128/204]	Loss 0.6125 (0.6405)	
training:	Epoch: [14][129/204]	Loss 0.6087 (0.6403)	
training:	Epoch: [14][130/204]	Loss 0.6874 (0.6407)	
training:	Epoch: [14][131/204]	Loss 0.6135 (0.6404)	
training:	Epoch: [14][132/204]	Loss 0.6548 (0.6406)	
training:	Epoch: [14][133/204]	Loss 0.6591 (0.6407)	
training:	Epoch: [14][134/204]	Loss 0.6564 (0.6408)	
training:	Epoch: [14][135/204]	Loss 0.6689 (0.6410)	
training:	Epoch: [14][136/204]	Loss 0.6619 (0.6412)	
training:	Epoch: [14][137/204]	Loss 0.5964 (0.6408)	
training:	Epoch: [14][138/204]	Loss 0.6757 (0.6411)	
training:	Epoch: [14][139/204]	Loss 0.6290 (0.6410)	
training:	Epoch: [14][140/204]	Loss 0.6695 (0.6412)	
training:	Epoch: [14][141/204]	Loss 0.6802 (0.6415)	
training:	Epoch: [14][142/204]	Loss 0.6544 (0.6416)	
training:	Epoch: [14][143/204]	Loss 0.7280 (0.6422)	
training:	Epoch: [14][144/204]	Loss 0.6256 (0.6421)	
training:	Epoch: [14][145/204]	Loss 0.6972 (0.6425)	
training:	Epoch: [14][146/204]	Loss 0.6534 (0.6425)	
training:	Epoch: [14][147/204]	Loss 0.6737 (0.6427)	
training:	Epoch: [14][148/204]	Loss 0.6811 (0.6430)	
training:	Epoch: [14][149/204]	Loss 0.6392 (0.6430)	
training:	Epoch: [14][150/204]	Loss 0.6268 (0.6429)	
training:	Epoch: [14][151/204]	Loss 0.6710 (0.6431)	
training:	Epoch: [14][152/204]	Loss 0.6294 (0.6430)	
training:	Epoch: [14][153/204]	Loss 0.6328 (0.6429)	
training:	Epoch: [14][154/204]	Loss 0.6621 (0.6430)	
training:	Epoch: [14][155/204]	Loss 0.6161 (0.6428)	
training:	Epoch: [14][156/204]	Loss 0.6090 (0.6426)	
training:	Epoch: [14][157/204]	Loss 0.6552 (0.6427)	
training:	Epoch: [14][158/204]	Loss 0.6715 (0.6429)	
training:	Epoch: [14][159/204]	Loss 0.6276 (0.6428)	
training:	Epoch: [14][160/204]	Loss 0.6530 (0.6429)	
training:	Epoch: [14][161/204]	Loss 0.6633 (0.6430)	
training:	Epoch: [14][162/204]	Loss 0.6330 (0.6429)	
training:	Epoch: [14][163/204]	Loss 0.6663 (0.6431)	
training:	Epoch: [14][164/204]	Loss 0.6228 (0.6429)	
training:	Epoch: [14][165/204]	Loss 0.6559 (0.6430)	
training:	Epoch: [14][166/204]	Loss 0.6242 (0.6429)	
training:	Epoch: [14][167/204]	Loss 0.7115 (0.6433)	
training:	Epoch: [14][168/204]	Loss 0.6268 (0.6432)	
training:	Epoch: [14][169/204]	Loss 0.6154 (0.6431)	
training:	Epoch: [14][170/204]	Loss 0.6046 (0.6428)	
training:	Epoch: [14][171/204]	Loss 0.6377 (0.6428)	
training:	Epoch: [14][172/204]	Loss 0.6473 (0.6428)	
training:	Epoch: [14][173/204]	Loss 0.6006 (0.6426)	
training:	Epoch: [14][174/204]	Loss 0.6331 (0.6425)	
training:	Epoch: [14][175/204]	Loss 0.6185 (0.6424)	
training:	Epoch: [14][176/204]	Loss 0.6036 (0.6422)	
training:	Epoch: [14][177/204]	Loss 0.6271 (0.6421)	
training:	Epoch: [14][178/204]	Loss 0.6291 (0.6420)	
training:	Epoch: [14][179/204]	Loss 0.6618 (0.6421)	
training:	Epoch: [14][180/204]	Loss 0.5990 (0.6419)	
training:	Epoch: [14][181/204]	Loss 0.6420 (0.6419)	
training:	Epoch: [14][182/204]	Loss 0.6128 (0.6417)	
training:	Epoch: [14][183/204]	Loss 0.6199 (0.6416)	
training:	Epoch: [14][184/204]	Loss 0.6494 (0.6416)	
training:	Epoch: [14][185/204]	Loss 0.6336 (0.6416)	
training:	Epoch: [14][186/204]	Loss 0.6807 (0.6418)	
training:	Epoch: [14][187/204]	Loss 0.6368 (0.6418)	
training:	Epoch: [14][188/204]	Loss 0.5751 (0.6414)	
training:	Epoch: [14][189/204]	Loss 0.6294 (0.6414)	
training:	Epoch: [14][190/204]	Loss 0.6060 (0.6412)	
training:	Epoch: [14][191/204]	Loss 0.6742 (0.6414)	
training:	Epoch: [14][192/204]	Loss 0.6713 (0.6415)	
training:	Epoch: [14][193/204]	Loss 0.6483 (0.6415)	
training:	Epoch: [14][194/204]	Loss 0.6342 (0.6415)	
training:	Epoch: [14][195/204]	Loss 0.6420 (0.6415)	
training:	Epoch: [14][196/204]	Loss 0.6804 (0.6417)	
training:	Epoch: [14][197/204]	Loss 0.6497 (0.6418)	
training:	Epoch: [14][198/204]	Loss 0.6400 (0.6417)	
training:	Epoch: [14][199/204]	Loss 0.6114 (0.6416)	
training:	Epoch: [14][200/204]	Loss 0.6932 (0.6418)	
training:	Epoch: [14][201/204]	Loss 0.6340 (0.6418)	
training:	Epoch: [14][202/204]	Loss 0.6152 (0.6417)	
training:	Epoch: [14][203/204]	Loss 0.6575 (0.6418)	
training:	Epoch: [14][204/204]	Loss 0.6470 (0.6418)	
Training:	 Loss: 0.6408

Training:	 ACC: 0.6637 0.6661 0.7225 0.6049
Validation:	 ACC: 0.6565 0.6602 0.7380 0.5751
Validation:	 Best_BACC: 0.6565 0.6602 0.7380 0.5751
Validation:	 Loss: 0.6403
Pretraining:	Epoch 15/120
----------
training:	Epoch: [15][1/204]	Loss 0.6213 (0.6213)	
training:	Epoch: [15][2/204]	Loss 0.5645 (0.5929)	
training:	Epoch: [15][3/204]	Loss 0.6796 (0.6218)	
training:	Epoch: [15][4/204]	Loss 0.6701 (0.6339)	
training:	Epoch: [15][5/204]	Loss 0.6579 (0.6387)	
training:	Epoch: [15][6/204]	Loss 0.6386 (0.6387)	
training:	Epoch: [15][7/204]	Loss 0.6059 (0.6340)	
training:	Epoch: [15][8/204]	Loss 0.6333 (0.6339)	
training:	Epoch: [15][9/204]	Loss 0.6169 (0.6320)	
training:	Epoch: [15][10/204]	Loss 0.6356 (0.6324)	
training:	Epoch: [15][11/204]	Loss 0.6542 (0.6343)	
training:	Epoch: [15][12/204]	Loss 0.6261 (0.6337)	
training:	Epoch: [15][13/204]	Loss 0.6440 (0.6345)	
training:	Epoch: [15][14/204]	Loss 0.6369 (0.6346)	
training:	Epoch: [15][15/204]	Loss 0.6645 (0.6366)	
training:	Epoch: [15][16/204]	Loss 0.6558 (0.6378)	
training:	Epoch: [15][17/204]	Loss 0.6071 (0.6360)	
training:	Epoch: [15][18/204]	Loss 0.6480 (0.6367)	
training:	Epoch: [15][19/204]	Loss 0.6494 (0.6374)	
training:	Epoch: [15][20/204]	Loss 0.5913 (0.6350)	
training:	Epoch: [15][21/204]	Loss 0.6137 (0.6340)	
training:	Epoch: [15][22/204]	Loss 0.6572 (0.6351)	
training:	Epoch: [15][23/204]	Loss 0.6297 (0.6349)	
training:	Epoch: [15][24/204]	Loss 0.5991 (0.6334)	
training:	Epoch: [15][25/204]	Loss 0.5776 (0.6311)	
training:	Epoch: [15][26/204]	Loss 0.6369 (0.6314)	
training:	Epoch: [15][27/204]	Loss 0.6957 (0.6337)	
training:	Epoch: [15][28/204]	Loss 0.6782 (0.6353)	
training:	Epoch: [15][29/204]	Loss 0.6365 (0.6354)	
training:	Epoch: [15][30/204]	Loss 0.6001 (0.6342)	
training:	Epoch: [15][31/204]	Loss 0.6303 (0.6341)	
training:	Epoch: [15][32/204]	Loss 0.6047 (0.6331)	
training:	Epoch: [15][33/204]	Loss 0.6776 (0.6345)	
training:	Epoch: [15][34/204]	Loss 0.6418 (0.6347)	
training:	Epoch: [15][35/204]	Loss 0.6550 (0.6353)	
training:	Epoch: [15][36/204]	Loss 0.6537 (0.6358)	
training:	Epoch: [15][37/204]	Loss 0.6680 (0.6367)	
training:	Epoch: [15][38/204]	Loss 0.6283 (0.6365)	
training:	Epoch: [15][39/204]	Loss 0.6228 (0.6361)	
training:	Epoch: [15][40/204]	Loss 0.6280 (0.6359)	
training:	Epoch: [15][41/204]	Loss 0.6134 (0.6353)	
training:	Epoch: [15][42/204]	Loss 0.6194 (0.6350)	
training:	Epoch: [15][43/204]	Loss 0.6292 (0.6348)	
training:	Epoch: [15][44/204]	Loss 0.6282 (0.6347)	
training:	Epoch: [15][45/204]	Loss 0.6295 (0.6346)	
training:	Epoch: [15][46/204]	Loss 0.6474 (0.6348)	
training:	Epoch: [15][47/204]	Loss 0.6901 (0.6360)	
training:	Epoch: [15][48/204]	Loss 0.6328 (0.6360)	
training:	Epoch: [15][49/204]	Loss 0.6274 (0.6358)	
training:	Epoch: [15][50/204]	Loss 0.6481 (0.6360)	
training:	Epoch: [15][51/204]	Loss 0.6388 (0.6361)	
training:	Epoch: [15][52/204]	Loss 0.6373 (0.6361)	
training:	Epoch: [15][53/204]	Loss 0.6016 (0.6355)	
training:	Epoch: [15][54/204]	Loss 0.6779 (0.6362)	
training:	Epoch: [15][55/204]	Loss 0.5397 (0.6345)	
training:	Epoch: [15][56/204]	Loss 0.6298 (0.6344)	
training:	Epoch: [15][57/204]	Loss 0.5983 (0.6338)	
training:	Epoch: [15][58/204]	Loss 0.6438 (0.6339)	
training:	Epoch: [15][59/204]	Loss 0.5952 (0.6333)	
training:	Epoch: [15][60/204]	Loss 0.6417 (0.6334)	
training:	Epoch: [15][61/204]	Loss 0.6378 (0.6335)	
training:	Epoch: [15][62/204]	Loss 0.6725 (0.6341)	
training:	Epoch: [15][63/204]	Loss 0.6556 (0.6345)	
training:	Epoch: [15][64/204]	Loss 0.6529 (0.6348)	
training:	Epoch: [15][65/204]	Loss 0.6337 (0.6347)	
training:	Epoch: [15][66/204]	Loss 0.5979 (0.6342)	
training:	Epoch: [15][67/204]	Loss 0.6597 (0.6346)	
training:	Epoch: [15][68/204]	Loss 0.6020 (0.6341)	
training:	Epoch: [15][69/204]	Loss 0.6223 (0.6339)	
training:	Epoch: [15][70/204]	Loss 0.5787 (0.6331)	
training:	Epoch: [15][71/204]	Loss 0.6442 (0.6333)	
training:	Epoch: [15][72/204]	Loss 0.6090 (0.6329)	
training:	Epoch: [15][73/204]	Loss 0.6419 (0.6331)	
training:	Epoch: [15][74/204]	Loss 0.6792 (0.6337)	
training:	Epoch: [15][75/204]	Loss 0.6556 (0.6340)	
training:	Epoch: [15][76/204]	Loss 0.5932 (0.6334)	
training:	Epoch: [15][77/204]	Loss 0.6582 (0.6338)	
training:	Epoch: [15][78/204]	Loss 0.7180 (0.6348)	
training:	Epoch: [15][79/204]	Loss 0.6796 (0.6354)	
training:	Epoch: [15][80/204]	Loss 0.5787 (0.6347)	
training:	Epoch: [15][81/204]	Loss 0.6577 (0.6350)	
training:	Epoch: [15][82/204]	Loss 0.7046 (0.6358)	
training:	Epoch: [15][83/204]	Loss 0.6713 (0.6363)	
training:	Epoch: [15][84/204]	Loss 0.6661 (0.6366)	
training:	Epoch: [15][85/204]	Loss 0.6503 (0.6368)	
training:	Epoch: [15][86/204]	Loss 0.6288 (0.6367)	
training:	Epoch: [15][87/204]	Loss 0.6214 (0.6365)	
training:	Epoch: [15][88/204]	Loss 0.6427 (0.6366)	
training:	Epoch: [15][89/204]	Loss 0.5939 (0.6361)	
training:	Epoch: [15][90/204]	Loss 0.7050 (0.6369)	
training:	Epoch: [15][91/204]	Loss 0.6245 (0.6367)	
training:	Epoch: [15][92/204]	Loss 0.6202 (0.6365)	
training:	Epoch: [15][93/204]	Loss 0.6285 (0.6365)	
training:	Epoch: [15][94/204]	Loss 0.6646 (0.6368)	
training:	Epoch: [15][95/204]	Loss 0.6048 (0.6364)	
training:	Epoch: [15][96/204]	Loss 0.7110 (0.6372)	
training:	Epoch: [15][97/204]	Loss 0.6672 (0.6375)	
training:	Epoch: [15][98/204]	Loss 0.6106 (0.6372)	
training:	Epoch: [15][99/204]	Loss 0.6347 (0.6372)	
training:	Epoch: [15][100/204]	Loss 0.6636 (0.6375)	
training:	Epoch: [15][101/204]	Loss 0.6769 (0.6379)	
training:	Epoch: [15][102/204]	Loss 0.6352 (0.6378)	
training:	Epoch: [15][103/204]	Loss 0.5771 (0.6372)	
training:	Epoch: [15][104/204]	Loss 0.6254 (0.6371)	
training:	Epoch: [15][105/204]	Loss 0.6531 (0.6373)	
training:	Epoch: [15][106/204]	Loss 0.6310 (0.6372)	
training:	Epoch: [15][107/204]	Loss 0.6207 (0.6371)	
training:	Epoch: [15][108/204]	Loss 0.5895 (0.6366)	
training:	Epoch: [15][109/204]	Loss 0.6051 (0.6363)	
training:	Epoch: [15][110/204]	Loss 0.5631 (0.6357)	
training:	Epoch: [15][111/204]	Loss 0.6351 (0.6357)	
training:	Epoch: [15][112/204]	Loss 0.6669 (0.6360)	
training:	Epoch: [15][113/204]	Loss 0.6637 (0.6362)	
training:	Epoch: [15][114/204]	Loss 0.6229 (0.6361)	
training:	Epoch: [15][115/204]	Loss 0.6196 (0.6359)	
training:	Epoch: [15][116/204]	Loss 0.7108 (0.6366)	
training:	Epoch: [15][117/204]	Loss 0.6486 (0.6367)	
training:	Epoch: [15][118/204]	Loss 0.6434 (0.6367)	
training:	Epoch: [15][119/204]	Loss 0.6164 (0.6366)	
training:	Epoch: [15][120/204]	Loss 0.6348 (0.6366)	
training:	Epoch: [15][121/204]	Loss 0.6862 (0.6370)	
training:	Epoch: [15][122/204]	Loss 0.6444 (0.6370)	
training:	Epoch: [15][123/204]	Loss 0.6292 (0.6370)	
training:	Epoch: [15][124/204]	Loss 0.6709 (0.6372)	
training:	Epoch: [15][125/204]	Loss 0.5935 (0.6369)	
training:	Epoch: [15][126/204]	Loss 0.6259 (0.6368)	
training:	Epoch: [15][127/204]	Loss 0.6223 (0.6367)	
training:	Epoch: [15][128/204]	Loss 0.5930 (0.6363)	
training:	Epoch: [15][129/204]	Loss 0.6674 (0.6366)	
training:	Epoch: [15][130/204]	Loss 0.6709 (0.6369)	
training:	Epoch: [15][131/204]	Loss 0.6503 (0.6370)	
training:	Epoch: [15][132/204]	Loss 0.7016 (0.6374)	
training:	Epoch: [15][133/204]	Loss 0.6828 (0.6378)	
training:	Epoch: [15][134/204]	Loss 0.6220 (0.6377)	
training:	Epoch: [15][135/204]	Loss 0.6324 (0.6376)	
training:	Epoch: [15][136/204]	Loss 0.6333 (0.6376)	
training:	Epoch: [15][137/204]	Loss 0.6609 (0.6378)	
training:	Epoch: [15][138/204]	Loss 0.6606 (0.6379)	
training:	Epoch: [15][139/204]	Loss 0.6253 (0.6378)	
training:	Epoch: [15][140/204]	Loss 0.6346 (0.6378)	
training:	Epoch: [15][141/204]	Loss 0.6311 (0.6378)	
training:	Epoch: [15][142/204]	Loss 0.6296 (0.6377)	
training:	Epoch: [15][143/204]	Loss 0.6294 (0.6377)	
training:	Epoch: [15][144/204]	Loss 0.6084 (0.6375)	
training:	Epoch: [15][145/204]	Loss 0.6277 (0.6374)	
training:	Epoch: [15][146/204]	Loss 0.5821 (0.6370)	
training:	Epoch: [15][147/204]	Loss 0.5593 (0.6365)	
training:	Epoch: [15][148/204]	Loss 0.6000 (0.6362)	
training:	Epoch: [15][149/204]	Loss 0.5995 (0.6360)	
training:	Epoch: [15][150/204]	Loss 0.6315 (0.6360)	
training:	Epoch: [15][151/204]	Loss 0.6392 (0.6360)	
training:	Epoch: [15][152/204]	Loss 0.6223 (0.6359)	
training:	Epoch: [15][153/204]	Loss 0.6232 (0.6358)	
training:	Epoch: [15][154/204]	Loss 0.6127 (0.6357)	
training:	Epoch: [15][155/204]	Loss 0.6175 (0.6355)	
training:	Epoch: [15][156/204]	Loss 0.6491 (0.6356)	
training:	Epoch: [15][157/204]	Loss 0.5796 (0.6353)	
training:	Epoch: [15][158/204]	Loss 0.6898 (0.6356)	
training:	Epoch: [15][159/204]	Loss 0.6734 (0.6358)	
training:	Epoch: [15][160/204]	Loss 0.7014 (0.6363)	
training:	Epoch: [15][161/204]	Loss 0.5909 (0.6360)	
training:	Epoch: [15][162/204]	Loss 0.6536 (0.6361)	
training:	Epoch: [15][163/204]	Loss 0.6397 (0.6361)	
training:	Epoch: [15][164/204]	Loss 0.6347 (0.6361)	
training:	Epoch: [15][165/204]	Loss 0.6407 (0.6361)	
training:	Epoch: [15][166/204]	Loss 0.6281 (0.6361)	
training:	Epoch: [15][167/204]	Loss 0.6571 (0.6362)	
training:	Epoch: [15][168/204]	Loss 0.6673 (0.6364)	
training:	Epoch: [15][169/204]	Loss 0.6708 (0.6366)	
training:	Epoch: [15][170/204]	Loss 0.6405 (0.6366)	
training:	Epoch: [15][171/204]	Loss 0.6344 (0.6366)	
training:	Epoch: [15][172/204]	Loss 0.6455 (0.6367)	
training:	Epoch: [15][173/204]	Loss 0.6174 (0.6365)	
training:	Epoch: [15][174/204]	Loss 0.5989 (0.6363)	
training:	Epoch: [15][175/204]	Loss 0.5947 (0.6361)	
training:	Epoch: [15][176/204]	Loss 0.6261 (0.6360)	
training:	Epoch: [15][177/204]	Loss 0.6383 (0.6360)	
training:	Epoch: [15][178/204]	Loss 0.6813 (0.6363)	
training:	Epoch: [15][179/204]	Loss 0.6215 (0.6362)	
training:	Epoch: [15][180/204]	Loss 0.6363 (0.6362)	
training:	Epoch: [15][181/204]	Loss 0.6416 (0.6362)	
training:	Epoch: [15][182/204]	Loss 0.6362 (0.6362)	
training:	Epoch: [15][183/204]	Loss 0.6802 (0.6365)	
training:	Epoch: [15][184/204]	Loss 0.6855 (0.6368)	
training:	Epoch: [15][185/204]	Loss 0.6145 (0.6366)	
training:	Epoch: [15][186/204]	Loss 0.5881 (0.6364)	
training:	Epoch: [15][187/204]	Loss 0.6339 (0.6364)	
training:	Epoch: [15][188/204]	Loss 0.5755 (0.6360)	
training:	Epoch: [15][189/204]	Loss 0.6184 (0.6359)	
training:	Epoch: [15][190/204]	Loss 0.6131 (0.6358)	
training:	Epoch: [15][191/204]	Loss 0.6147 (0.6357)	
training:	Epoch: [15][192/204]	Loss 0.6729 (0.6359)	
training:	Epoch: [15][193/204]	Loss 0.6115 (0.6358)	
training:	Epoch: [15][194/204]	Loss 0.6259 (0.6357)	
training:	Epoch: [15][195/204]	Loss 0.6521 (0.6358)	
training:	Epoch: [15][196/204]	Loss 0.6092 (0.6357)	
training:	Epoch: [15][197/204]	Loss 0.6155 (0.6356)	
training:	Epoch: [15][198/204]	Loss 0.6634 (0.6357)	
training:	Epoch: [15][199/204]	Loss 0.6913 (0.6360)	
training:	Epoch: [15][200/204]	Loss 0.6977 (0.6363)	
training:	Epoch: [15][201/204]	Loss 0.5963 (0.6361)	
training:	Epoch: [15][202/204]	Loss 0.6959 (0.6364)	
training:	Epoch: [15][203/204]	Loss 0.5714 (0.6361)	
training:	Epoch: [15][204/204]	Loss 0.6211 (0.6360)	
Training:	 Loss: 0.6350

Training:	 ACC: 0.6688 0.6712 0.7257 0.6119
Validation:	 ACC: 0.6611 0.6651 0.7472 0.5751
Validation:	 Best_BACC: 0.6611 0.6651 0.7472 0.5751
Validation:	 Loss: 0.6344
Pretraining:	Epoch 16/120
----------
training:	Epoch: [16][1/204]	Loss 0.6675 (0.6675)	
training:	Epoch: [16][2/204]	Loss 0.6001 (0.6338)	
training:	Epoch: [16][3/204]	Loss 0.6325 (0.6334)	
training:	Epoch: [16][4/204]	Loss 0.6253 (0.6313)	
training:	Epoch: [16][5/204]	Loss 0.6181 (0.6287)	
training:	Epoch: [16][6/204]	Loss 0.6585 (0.6337)	
training:	Epoch: [16][7/204]	Loss 0.6402 (0.6346)	
training:	Epoch: [16][8/204]	Loss 0.5946 (0.6296)	
training:	Epoch: [16][9/204]	Loss 0.6281 (0.6294)	
training:	Epoch: [16][10/204]	Loss 0.6642 (0.6329)	
training:	Epoch: [16][11/204]	Loss 0.6231 (0.6320)	
training:	Epoch: [16][12/204]	Loss 0.5891 (0.6284)	
training:	Epoch: [16][13/204]	Loss 0.6383 (0.6292)	
training:	Epoch: [16][14/204]	Loss 0.5911 (0.6265)	
training:	Epoch: [16][15/204]	Loss 0.6689 (0.6293)	
training:	Epoch: [16][16/204]	Loss 0.6615 (0.6313)	
training:	Epoch: [16][17/204]	Loss 0.6061 (0.6298)	
training:	Epoch: [16][18/204]	Loss 0.6559 (0.6313)	
training:	Epoch: [16][19/204]	Loss 0.6435 (0.6319)	
training:	Epoch: [16][20/204]	Loss 0.6506 (0.6329)	
training:	Epoch: [16][21/204]	Loss 0.6836 (0.6353)	
training:	Epoch: [16][22/204]	Loss 0.6314 (0.6351)	
training:	Epoch: [16][23/204]	Loss 0.5929 (0.6333)	
training:	Epoch: [16][24/204]	Loss 0.6168 (0.6326)	
training:	Epoch: [16][25/204]	Loss 0.7470 (0.6371)	
training:	Epoch: [16][26/204]	Loss 0.6010 (0.6358)	
training:	Epoch: [16][27/204]	Loss 0.5816 (0.6338)	
training:	Epoch: [16][28/204]	Loss 0.6076 (0.6328)	
training:	Epoch: [16][29/204]	Loss 0.6146 (0.6322)	
training:	Epoch: [16][30/204]	Loss 0.6776 (0.6337)	
training:	Epoch: [16][31/204]	Loss 0.6415 (0.6340)	
training:	Epoch: [16][32/204]	Loss 0.6655 (0.6349)	
training:	Epoch: [16][33/204]	Loss 0.5711 (0.6330)	
training:	Epoch: [16][34/204]	Loss 0.6353 (0.6331)	
training:	Epoch: [16][35/204]	Loss 0.6195 (0.6327)	
training:	Epoch: [16][36/204]	Loss 0.6263 (0.6325)	
training:	Epoch: [16][37/204]	Loss 0.6763 (0.6337)	
training:	Epoch: [16][38/204]	Loss 0.6182 (0.6333)	
training:	Epoch: [16][39/204]	Loss 0.6698 (0.6342)	
training:	Epoch: [16][40/204]	Loss 0.6512 (0.6346)	
training:	Epoch: [16][41/204]	Loss 0.6263 (0.6344)	
training:	Epoch: [16][42/204]	Loss 0.6492 (0.6348)	
training:	Epoch: [16][43/204]	Loss 0.5868 (0.6337)	
training:	Epoch: [16][44/204]	Loss 0.5942 (0.6328)	
training:	Epoch: [16][45/204]	Loss 0.5927 (0.6319)	
training:	Epoch: [16][46/204]	Loss 0.6023 (0.6312)	
training:	Epoch: [16][47/204]	Loss 0.6376 (0.6314)	
training:	Epoch: [16][48/204]	Loss 0.5970 (0.6307)	
training:	Epoch: [16][49/204]	Loss 0.5761 (0.6295)	
training:	Epoch: [16][50/204]	Loss 0.5851 (0.6287)	
training:	Epoch: [16][51/204]	Loss 0.6329 (0.6287)	
training:	Epoch: [16][52/204]	Loss 0.5966 (0.6281)	
training:	Epoch: [16][53/204]	Loss 0.5911 (0.6274)	
training:	Epoch: [16][54/204]	Loss 0.6278 (0.6274)	
training:	Epoch: [16][55/204]	Loss 0.7221 (0.6292)	
training:	Epoch: [16][56/204]	Loss 0.6665 (0.6298)	
training:	Epoch: [16][57/204]	Loss 0.6215 (0.6297)	
training:	Epoch: [16][58/204]	Loss 0.6575 (0.6302)	
training:	Epoch: [16][59/204]	Loss 0.6544 (0.6306)	
training:	Epoch: [16][60/204]	Loss 0.6836 (0.6314)	
training:	Epoch: [16][61/204]	Loss 0.6438 (0.6317)	
training:	Epoch: [16][62/204]	Loss 0.6681 (0.6322)	
training:	Epoch: [16][63/204]	Loss 0.6761 (0.6329)	
training:	Epoch: [16][64/204]	Loss 0.6393 (0.6330)	
training:	Epoch: [16][65/204]	Loss 0.6561 (0.6334)	
training:	Epoch: [16][66/204]	Loss 0.5873 (0.6327)	
training:	Epoch: [16][67/204]	Loss 0.6427 (0.6328)	
training:	Epoch: [16][68/204]	Loss 0.6712 (0.6334)	
training:	Epoch: [16][69/204]	Loss 0.5388 (0.6320)	
training:	Epoch: [16][70/204]	Loss 0.6312 (0.6320)	
training:	Epoch: [16][71/204]	Loss 0.6656 (0.6325)	
training:	Epoch: [16][72/204]	Loss 0.6371 (0.6326)	
training:	Epoch: [16][73/204]	Loss 0.6617 (0.6330)	
training:	Epoch: [16][74/204]	Loss 0.6895 (0.6337)	
training:	Epoch: [16][75/204]	Loss 0.6474 (0.6339)	
training:	Epoch: [16][76/204]	Loss 0.6450 (0.6341)	
training:	Epoch: [16][77/204]	Loss 0.6007 (0.6336)	
training:	Epoch: [16][78/204]	Loss 0.5721 (0.6328)	
training:	Epoch: [16][79/204]	Loss 0.6329 (0.6328)	
training:	Epoch: [16][80/204]	Loss 0.6105 (0.6326)	
training:	Epoch: [16][81/204]	Loss 0.6501 (0.6328)	
training:	Epoch: [16][82/204]	Loss 0.6036 (0.6324)	
training:	Epoch: [16][83/204]	Loss 0.6269 (0.6323)	
training:	Epoch: [16][84/204]	Loss 0.6262 (0.6323)	
training:	Epoch: [16][85/204]	Loss 0.6910 (0.6330)	
training:	Epoch: [16][86/204]	Loss 0.5928 (0.6325)	
training:	Epoch: [16][87/204]	Loss 0.6872 (0.6331)	
training:	Epoch: [16][88/204]	Loss 0.6625 (0.6335)	
training:	Epoch: [16][89/204]	Loss 0.6105 (0.6332)	
training:	Epoch: [16][90/204]	Loss 0.6404 (0.6333)	
training:	Epoch: [16][91/204]	Loss 0.5980 (0.6329)	
training:	Epoch: [16][92/204]	Loss 0.5542 (0.6320)	
training:	Epoch: [16][93/204]	Loss 0.6388 (0.6321)	
training:	Epoch: [16][94/204]	Loss 0.6621 (0.6324)	
training:	Epoch: [16][95/204]	Loss 0.6236 (0.6323)	
training:	Epoch: [16][96/204]	Loss 0.6008 (0.6320)	
training:	Epoch: [16][97/204]	Loss 0.5264 (0.6309)	
training:	Epoch: [16][98/204]	Loss 0.5911 (0.6305)	
training:	Epoch: [16][99/204]	Loss 0.6294 (0.6305)	
training:	Epoch: [16][100/204]	Loss 0.6646 (0.6308)	
training:	Epoch: [16][101/204]	Loss 0.6275 (0.6308)	
training:	Epoch: [16][102/204]	Loss 0.6348 (0.6309)	
training:	Epoch: [16][103/204]	Loss 0.6450 (0.6310)	
training:	Epoch: [16][104/204]	Loss 0.5726 (0.6304)	
training:	Epoch: [16][105/204]	Loss 0.6023 (0.6302)	
training:	Epoch: [16][106/204]	Loss 0.5884 (0.6298)	
training:	Epoch: [16][107/204]	Loss 0.6375 (0.6298)	
training:	Epoch: [16][108/204]	Loss 0.5701 (0.6293)	
training:	Epoch: [16][109/204]	Loss 0.5974 (0.6290)	
training:	Epoch: [16][110/204]	Loss 0.6138 (0.6289)	
training:	Epoch: [16][111/204]	Loss 0.6092 (0.6287)	
training:	Epoch: [16][112/204]	Loss 0.6219 (0.6286)	
training:	Epoch: [16][113/204]	Loss 0.5937 (0.6283)	
training:	Epoch: [16][114/204]	Loss 0.6487 (0.6285)	
training:	Epoch: [16][115/204]	Loss 0.6397 (0.6286)	
training:	Epoch: [16][116/204]	Loss 0.7241 (0.6294)	
training:	Epoch: [16][117/204]	Loss 0.6278 (0.6294)	
training:	Epoch: [16][118/204]	Loss 0.5988 (0.6291)	
training:	Epoch: [16][119/204]	Loss 0.6068 (0.6289)	
training:	Epoch: [16][120/204]	Loss 0.6164 (0.6288)	
training:	Epoch: [16][121/204]	Loss 0.6398 (0.6289)	
training:	Epoch: [16][122/204]	Loss 0.6817 (0.6294)	
training:	Epoch: [16][123/204]	Loss 0.6145 (0.6292)	
training:	Epoch: [16][124/204]	Loss 0.6028 (0.6290)	
training:	Epoch: [16][125/204]	Loss 0.6078 (0.6289)	
training:	Epoch: [16][126/204]	Loss 0.7245 (0.6296)	
training:	Epoch: [16][127/204]	Loss 0.7380 (0.6305)	
training:	Epoch: [16][128/204]	Loss 0.7037 (0.6310)	
training:	Epoch: [16][129/204]	Loss 0.6922 (0.6315)	
training:	Epoch: [16][130/204]	Loss 0.6763 (0.6319)	
training:	Epoch: [16][131/204]	Loss 0.6319 (0.6319)	
training:	Epoch: [16][132/204]	Loss 0.6459 (0.6320)	
training:	Epoch: [16][133/204]	Loss 0.6182 (0.6319)	
training:	Epoch: [16][134/204]	Loss 0.6741 (0.6322)	
training:	Epoch: [16][135/204]	Loss 0.5987 (0.6319)	
training:	Epoch: [16][136/204]	Loss 0.6474 (0.6320)	
training:	Epoch: [16][137/204]	Loss 0.6210 (0.6320)	
training:	Epoch: [16][138/204]	Loss 0.6296 (0.6320)	
training:	Epoch: [16][139/204]	Loss 0.5796 (0.6316)	
training:	Epoch: [16][140/204]	Loss 0.6321 (0.6316)	
training:	Epoch: [16][141/204]	Loss 0.6547 (0.6317)	
training:	Epoch: [16][142/204]	Loss 0.5679 (0.6313)	
training:	Epoch: [16][143/204]	Loss 0.6329 (0.6313)	
training:	Epoch: [16][144/204]	Loss 0.6612 (0.6315)	
training:	Epoch: [16][145/204]	Loss 0.6356 (0.6315)	
training:	Epoch: [16][146/204]	Loss 0.6517 (0.6317)	
training:	Epoch: [16][147/204]	Loss 0.5410 (0.6311)	
training:	Epoch: [16][148/204]	Loss 0.5908 (0.6308)	
training:	Epoch: [16][149/204]	Loss 0.6980 (0.6312)	
training:	Epoch: [16][150/204]	Loss 0.6710 (0.6315)	
training:	Epoch: [16][151/204]	Loss 0.5844 (0.6312)	
training:	Epoch: [16][152/204]	Loss 0.6768 (0.6315)	
training:	Epoch: [16][153/204]	Loss 0.5952 (0.6313)	
training:	Epoch: [16][154/204]	Loss 0.5927 (0.6310)	
training:	Epoch: [16][155/204]	Loss 0.6406 (0.6311)	
training:	Epoch: [16][156/204]	Loss 0.6272 (0.6310)	
training:	Epoch: [16][157/204]	Loss 0.6424 (0.6311)	
training:	Epoch: [16][158/204]	Loss 0.6280 (0.6311)	
training:	Epoch: [16][159/204]	Loss 0.6087 (0.6310)	
training:	Epoch: [16][160/204]	Loss 0.6050 (0.6308)	
training:	Epoch: [16][161/204]	Loss 0.5572 (0.6303)	
training:	Epoch: [16][162/204]	Loss 0.6127 (0.6302)	
training:	Epoch: [16][163/204]	Loss 0.7003 (0.6307)	
training:	Epoch: [16][164/204]	Loss 0.6333 (0.6307)	
training:	Epoch: [16][165/204]	Loss 0.6171 (0.6306)	
training:	Epoch: [16][166/204]	Loss 0.6208 (0.6305)	
training:	Epoch: [16][167/204]	Loss 0.6287 (0.6305)	
training:	Epoch: [16][168/204]	Loss 0.5874 (0.6303)	
training:	Epoch: [16][169/204]	Loss 0.6321 (0.6303)	
training:	Epoch: [16][170/204]	Loss 0.6410 (0.6303)	
training:	Epoch: [16][171/204]	Loss 0.6419 (0.6304)	
training:	Epoch: [16][172/204]	Loss 0.5740 (0.6301)	
training:	Epoch: [16][173/204]	Loss 0.6687 (0.6303)	
training:	Epoch: [16][174/204]	Loss 0.6041 (0.6301)	
training:	Epoch: [16][175/204]	Loss 0.5893 (0.6299)	
training:	Epoch: [16][176/204]	Loss 0.6857 (0.6302)	
training:	Epoch: [16][177/204]	Loss 0.6562 (0.6304)	
training:	Epoch: [16][178/204]	Loss 0.6677 (0.6306)	
training:	Epoch: [16][179/204]	Loss 0.6228 (0.6305)	
training:	Epoch: [16][180/204]	Loss 0.6238 (0.6305)	
training:	Epoch: [16][181/204]	Loss 0.6486 (0.6306)	
training:	Epoch: [16][182/204]	Loss 0.6548 (0.6307)	
training:	Epoch: [16][183/204]	Loss 0.6796 (0.6310)	
training:	Epoch: [16][184/204]	Loss 0.6617 (0.6312)	
training:	Epoch: [16][185/204]	Loss 0.6365 (0.6312)	
training:	Epoch: [16][186/204]	Loss 0.6589 (0.6314)	
training:	Epoch: [16][187/204]	Loss 0.6331 (0.6314)	
training:	Epoch: [16][188/204]	Loss 0.6421 (0.6314)	
training:	Epoch: [16][189/204]	Loss 0.6042 (0.6313)	
training:	Epoch: [16][190/204]	Loss 0.5732 (0.6310)	
training:	Epoch: [16][191/204]	Loss 0.6353 (0.6310)	
training:	Epoch: [16][192/204]	Loss 0.6212 (0.6309)	
training:	Epoch: [16][193/204]	Loss 0.5808 (0.6307)	
training:	Epoch: [16][194/204]	Loss 0.5674 (0.6304)	
training:	Epoch: [16][195/204]	Loss 0.6935 (0.6307)	
training:	Epoch: [16][196/204]	Loss 0.6253 (0.6307)	
training:	Epoch: [16][197/204]	Loss 0.5782 (0.6304)	
training:	Epoch: [16][198/204]	Loss 0.6391 (0.6304)	
training:	Epoch: [16][199/204]	Loss 0.6000 (0.6303)	
training:	Epoch: [16][200/204]	Loss 0.6579 (0.6304)	
training:	Epoch: [16][201/204]	Loss 0.5963 (0.6302)	
training:	Epoch: [16][202/204]	Loss 0.5926 (0.6301)	
training:	Epoch: [16][203/204]	Loss 0.6527 (0.6302)	
training:	Epoch: [16][204/204]	Loss 0.6763 (0.6304)	
Training:	 Loss: 0.6294

Training:	 ACC: 0.6715 0.6742 0.7390 0.6040
Validation:	 ACC: 0.6646 0.6688 0.7574 0.5717
Validation:	 Best_BACC: 0.6646 0.6688 0.7574 0.5717
Validation:	 Loss: 0.6286
Pretraining:	Epoch 17/120
----------
training:	Epoch: [17][1/204]	Loss 0.5569 (0.5569)	
training:	Epoch: [17][2/204]	Loss 0.6719 (0.6144)	
training:	Epoch: [17][3/204]	Loss 0.5674 (0.5987)	
training:	Epoch: [17][4/204]	Loss 0.5875 (0.5959)	
training:	Epoch: [17][5/204]	Loss 0.5762 (0.5920)	
training:	Epoch: [17][6/204]	Loss 0.7037 (0.6106)	
training:	Epoch: [17][7/204]	Loss 0.5902 (0.6077)	
training:	Epoch: [17][8/204]	Loss 0.6441 (0.6122)	
training:	Epoch: [17][9/204]	Loss 0.5837 (0.6091)	
training:	Epoch: [17][10/204]	Loss 0.6393 (0.6121)	
training:	Epoch: [17][11/204]	Loss 0.6074 (0.6117)	
training:	Epoch: [17][12/204]	Loss 0.5238 (0.6043)	
training:	Epoch: [17][13/204]	Loss 0.5543 (0.6005)	
training:	Epoch: [17][14/204]	Loss 0.6005 (0.6005)	
training:	Epoch: [17][15/204]	Loss 0.6149 (0.6015)	
training:	Epoch: [17][16/204]	Loss 0.6209 (0.6027)	
training:	Epoch: [17][17/204]	Loss 0.5556 (0.5999)	
training:	Epoch: [17][18/204]	Loss 0.6444 (0.6024)	
training:	Epoch: [17][19/204]	Loss 0.5833 (0.6014)	
training:	Epoch: [17][20/204]	Loss 0.5952 (0.6011)	
training:	Epoch: [17][21/204]	Loss 0.6118 (0.6016)	
training:	Epoch: [17][22/204]	Loss 0.5994 (0.6015)	
training:	Epoch: [17][23/204]	Loss 0.6403 (0.6032)	
training:	Epoch: [17][24/204]	Loss 0.6159 (0.6037)	
training:	Epoch: [17][25/204]	Loss 0.6677 (0.6062)	
training:	Epoch: [17][26/204]	Loss 0.6165 (0.6066)	
training:	Epoch: [17][27/204]	Loss 0.5697 (0.6053)	
training:	Epoch: [17][28/204]	Loss 0.6261 (0.6060)	
training:	Epoch: [17][29/204]	Loss 0.5709 (0.6048)	
training:	Epoch: [17][30/204]	Loss 0.6631 (0.6067)	
training:	Epoch: [17][31/204]	Loss 0.6319 (0.6076)	
training:	Epoch: [17][32/204]	Loss 0.6404 (0.6086)	
training:	Epoch: [17][33/204]	Loss 0.6345 (0.6094)	
training:	Epoch: [17][34/204]	Loss 0.6349 (0.6101)	
training:	Epoch: [17][35/204]	Loss 0.6697 (0.6118)	
training:	Epoch: [17][36/204]	Loss 0.6915 (0.6140)	
training:	Epoch: [17][37/204]	Loss 0.6482 (0.6150)	
training:	Epoch: [17][38/204]	Loss 0.5416 (0.6130)	
training:	Epoch: [17][39/204]	Loss 0.5894 (0.6124)	
training:	Epoch: [17][40/204]	Loss 0.6411 (0.6131)	
training:	Epoch: [17][41/204]	Loss 0.7175 (0.6157)	
training:	Epoch: [17][42/204]	Loss 0.6155 (0.6157)	
training:	Epoch: [17][43/204]	Loss 0.6256 (0.6159)	
training:	Epoch: [17][44/204]	Loss 0.5578 (0.6146)	
training:	Epoch: [17][45/204]	Loss 0.5904 (0.6141)	
training:	Epoch: [17][46/204]	Loss 0.5930 (0.6136)	
training:	Epoch: [17][47/204]	Loss 0.6790 (0.6150)	
training:	Epoch: [17][48/204]	Loss 0.6508 (0.6157)	
training:	Epoch: [17][49/204]	Loss 0.6212 (0.6159)	
training:	Epoch: [17][50/204]	Loss 0.6084 (0.6157)	
training:	Epoch: [17][51/204]	Loss 0.6650 (0.6167)	
training:	Epoch: [17][52/204]	Loss 0.6417 (0.6172)	
training:	Epoch: [17][53/204]	Loss 0.6787 (0.6183)	
training:	Epoch: [17][54/204]	Loss 0.6712 (0.6193)	
training:	Epoch: [17][55/204]	Loss 0.6012 (0.6190)	
training:	Epoch: [17][56/204]	Loss 0.6406 (0.6193)	
training:	Epoch: [17][57/204]	Loss 0.6195 (0.6194)	
training:	Epoch: [17][58/204]	Loss 0.6391 (0.6197)	
training:	Epoch: [17][59/204]	Loss 0.6636 (0.6204)	
training:	Epoch: [17][60/204]	Loss 0.6643 (0.6212)	
training:	Epoch: [17][61/204]	Loss 0.6340 (0.6214)	
training:	Epoch: [17][62/204]	Loss 0.6364 (0.6216)	
training:	Epoch: [17][63/204]	Loss 0.5475 (0.6204)	
training:	Epoch: [17][64/204]	Loss 0.6646 (0.6211)	
training:	Epoch: [17][65/204]	Loss 0.5749 (0.6204)	
training:	Epoch: [17][66/204]	Loss 0.6059 (0.6202)	
training:	Epoch: [17][67/204]	Loss 0.5944 (0.6198)	
training:	Epoch: [17][68/204]	Loss 0.6556 (0.6203)	
training:	Epoch: [17][69/204]	Loss 0.6365 (0.6206)	
training:	Epoch: [17][70/204]	Loss 0.7020 (0.6217)	
training:	Epoch: [17][71/204]	Loss 0.6774 (0.6225)	
training:	Epoch: [17][72/204]	Loss 0.6262 (0.6226)	
training:	Epoch: [17][73/204]	Loss 0.7116 (0.6238)	
training:	Epoch: [17][74/204]	Loss 0.5909 (0.6233)	
training:	Epoch: [17][75/204]	Loss 0.6763 (0.6241)	
training:	Epoch: [17][76/204]	Loss 0.6375 (0.6242)	
training:	Epoch: [17][77/204]	Loss 0.5658 (0.6235)	
training:	Epoch: [17][78/204]	Loss 0.6051 (0.6232)	
training:	Epoch: [17][79/204]	Loss 0.6150 (0.6231)	
training:	Epoch: [17][80/204]	Loss 0.5536 (0.6223)	
training:	Epoch: [17][81/204]	Loss 0.6387 (0.6225)	
training:	Epoch: [17][82/204]	Loss 0.6330 (0.6226)	
training:	Epoch: [17][83/204]	Loss 0.6474 (0.6229)	
training:	Epoch: [17][84/204]	Loss 0.6574 (0.6233)	
training:	Epoch: [17][85/204]	Loss 0.6226 (0.6233)	
training:	Epoch: [17][86/204]	Loss 0.5823 (0.6228)	
training:	Epoch: [17][87/204]	Loss 0.6780 (0.6235)	
training:	Epoch: [17][88/204]	Loss 0.7428 (0.6248)	
training:	Epoch: [17][89/204]	Loss 0.6087 (0.6246)	
training:	Epoch: [17][90/204]	Loss 0.5865 (0.6242)	
training:	Epoch: [17][91/204]	Loss 0.6401 (0.6244)	
training:	Epoch: [17][92/204]	Loss 0.6371 (0.6245)	
training:	Epoch: [17][93/204]	Loss 0.5689 (0.6239)	
training:	Epoch: [17][94/204]	Loss 0.6038 (0.6237)	
training:	Epoch: [17][95/204]	Loss 0.5911 (0.6234)	
training:	Epoch: [17][96/204]	Loss 0.6210 (0.6233)	
training:	Epoch: [17][97/204]	Loss 0.6592 (0.6237)	
training:	Epoch: [17][98/204]	Loss 0.6793 (0.6243)	
training:	Epoch: [17][99/204]	Loss 0.5769 (0.6238)	
training:	Epoch: [17][100/204]	Loss 0.7192 (0.6248)	
training:	Epoch: [17][101/204]	Loss 0.5987 (0.6245)	
training:	Epoch: [17][102/204]	Loss 0.6709 (0.6249)	
training:	Epoch: [17][103/204]	Loss 0.6110 (0.6248)	
training:	Epoch: [17][104/204]	Loss 0.6207 (0.6248)	
training:	Epoch: [17][105/204]	Loss 0.6557 (0.6251)	
training:	Epoch: [17][106/204]	Loss 0.6356 (0.6252)	
training:	Epoch: [17][107/204]	Loss 0.6019 (0.6250)	
training:	Epoch: [17][108/204]	Loss 0.5932 (0.6247)	
training:	Epoch: [17][109/204]	Loss 0.6908 (0.6253)	
training:	Epoch: [17][110/204]	Loss 0.6233 (0.6252)	
training:	Epoch: [17][111/204]	Loss 0.6505 (0.6255)	
training:	Epoch: [17][112/204]	Loss 0.6421 (0.6256)	
training:	Epoch: [17][113/204]	Loss 0.5620 (0.6251)	
training:	Epoch: [17][114/204]	Loss 0.6025 (0.6249)	
training:	Epoch: [17][115/204]	Loss 0.5844 (0.6245)	
training:	Epoch: [17][116/204]	Loss 0.6732 (0.6249)	
training:	Epoch: [17][117/204]	Loss 0.5748 (0.6245)	
training:	Epoch: [17][118/204]	Loss 0.6234 (0.6245)	
training:	Epoch: [17][119/204]	Loss 0.6463 (0.6247)	
training:	Epoch: [17][120/204]	Loss 0.7039 (0.6253)	
training:	Epoch: [17][121/204]	Loss 0.6172 (0.6253)	
training:	Epoch: [17][122/204]	Loss 0.5725 (0.6248)	
training:	Epoch: [17][123/204]	Loss 0.5931 (0.6246)	
training:	Epoch: [17][124/204]	Loss 0.6493 (0.6248)	
training:	Epoch: [17][125/204]	Loss 0.6921 (0.6253)	
training:	Epoch: [17][126/204]	Loss 0.6050 (0.6252)	
training:	Epoch: [17][127/204]	Loss 0.6146 (0.6251)	
training:	Epoch: [17][128/204]	Loss 0.6406 (0.6252)	
training:	Epoch: [17][129/204]	Loss 0.6889 (0.6257)	
training:	Epoch: [17][130/204]	Loss 0.6055 (0.6255)	
training:	Epoch: [17][131/204]	Loss 0.6673 (0.6258)	
training:	Epoch: [17][132/204]	Loss 0.6166 (0.6258)	
training:	Epoch: [17][133/204]	Loss 0.6408 (0.6259)	
training:	Epoch: [17][134/204]	Loss 0.6348 (0.6260)	
training:	Epoch: [17][135/204]	Loss 0.6775 (0.6263)	
training:	Epoch: [17][136/204]	Loss 0.6361 (0.6264)	
training:	Epoch: [17][137/204]	Loss 0.6018 (0.6262)	
training:	Epoch: [17][138/204]	Loss 0.6904 (0.6267)	
training:	Epoch: [17][139/204]	Loss 0.5640 (0.6262)	
training:	Epoch: [17][140/204]	Loss 0.6472 (0.6264)	
training:	Epoch: [17][141/204]	Loss 0.6502 (0.6266)	
training:	Epoch: [17][142/204]	Loss 0.5979 (0.6264)	
training:	Epoch: [17][143/204]	Loss 0.6566 (0.6266)	
training:	Epoch: [17][144/204]	Loss 0.6742 (0.6269)	
training:	Epoch: [17][145/204]	Loss 0.5689 (0.6265)	
training:	Epoch: [17][146/204]	Loss 0.6795 (0.6269)	
training:	Epoch: [17][147/204]	Loss 0.6075 (0.6267)	
training:	Epoch: [17][148/204]	Loss 0.6603 (0.6270)	
training:	Epoch: [17][149/204]	Loss 0.5566 (0.6265)	
training:	Epoch: [17][150/204]	Loss 0.5521 (0.6260)	
training:	Epoch: [17][151/204]	Loss 0.6345 (0.6260)	
training:	Epoch: [17][152/204]	Loss 0.6097 (0.6259)	
training:	Epoch: [17][153/204]	Loss 0.5948 (0.6257)	
training:	Epoch: [17][154/204]	Loss 0.5628 (0.6253)	
training:	Epoch: [17][155/204]	Loss 0.6179 (0.6253)	
training:	Epoch: [17][156/204]	Loss 0.5838 (0.6250)	
training:	Epoch: [17][157/204]	Loss 0.5890 (0.6248)	
training:	Epoch: [17][158/204]	Loss 0.5863 (0.6245)	
training:	Epoch: [17][159/204]	Loss 0.6354 (0.6246)	
training:	Epoch: [17][160/204]	Loss 0.6368 (0.6247)	
training:	Epoch: [17][161/204]	Loss 0.6556 (0.6249)	
training:	Epoch: [17][162/204]	Loss 0.5783 (0.6246)	
training:	Epoch: [17][163/204]	Loss 0.6393 (0.6247)	
training:	Epoch: [17][164/204]	Loss 0.6539 (0.6249)	
training:	Epoch: [17][165/204]	Loss 0.6520 (0.6250)	
training:	Epoch: [17][166/204]	Loss 0.6971 (0.6255)	
training:	Epoch: [17][167/204]	Loss 0.6783 (0.6258)	
training:	Epoch: [17][168/204]	Loss 0.6404 (0.6259)	
training:	Epoch: [17][169/204]	Loss 0.6005 (0.6257)	
training:	Epoch: [17][170/204]	Loss 0.6025 (0.6256)	
training:	Epoch: [17][171/204]	Loss 0.6748 (0.6259)	
training:	Epoch: [17][172/204]	Loss 0.5878 (0.6256)	
training:	Epoch: [17][173/204]	Loss 0.6619 (0.6259)	
training:	Epoch: [17][174/204]	Loss 0.6227 (0.6258)	
training:	Epoch: [17][175/204]	Loss 0.5982 (0.6257)	
training:	Epoch: [17][176/204]	Loss 0.6626 (0.6259)	
training:	Epoch: [17][177/204]	Loss 0.6041 (0.6258)	
training:	Epoch: [17][178/204]	Loss 0.6926 (0.6261)	
training:	Epoch: [17][179/204]	Loss 0.6354 (0.6262)	
training:	Epoch: [17][180/204]	Loss 0.6384 (0.6263)	
training:	Epoch: [17][181/204]	Loss 0.5962 (0.6261)	
training:	Epoch: [17][182/204]	Loss 0.6574 (0.6263)	
training:	Epoch: [17][183/204]	Loss 0.6127 (0.6262)	
training:	Epoch: [17][184/204]	Loss 0.6647 (0.6264)	
training:	Epoch: [17][185/204]	Loss 0.5872 (0.6262)	
training:	Epoch: [17][186/204]	Loss 0.6110 (0.6261)	
training:	Epoch: [17][187/204]	Loss 0.6338 (0.6261)	
training:	Epoch: [17][188/204]	Loss 0.6235 (0.6261)	
training:	Epoch: [17][189/204]	Loss 0.6420 (0.6262)	
training:	Epoch: [17][190/204]	Loss 0.5771 (0.6260)	
training:	Epoch: [17][191/204]	Loss 0.6648 (0.6262)	
training:	Epoch: [17][192/204]	Loss 0.5544 (0.6258)	
training:	Epoch: [17][193/204]	Loss 0.6435 (0.6259)	
training:	Epoch: [17][194/204]	Loss 0.5926 (0.6257)	
training:	Epoch: [17][195/204]	Loss 0.6258 (0.6257)	
training:	Epoch: [17][196/204]	Loss 0.5827 (0.6255)	
training:	Epoch: [17][197/204]	Loss 0.6143 (0.6254)	
training:	Epoch: [17][198/204]	Loss 0.6437 (0.6255)	
training:	Epoch: [17][199/204]	Loss 0.6251 (0.6255)	
training:	Epoch: [17][200/204]	Loss 0.6487 (0.6256)	
training:	Epoch: [17][201/204]	Loss 0.5900 (0.6255)	
training:	Epoch: [17][202/204]	Loss 0.6398 (0.6255)	
training:	Epoch: [17][203/204]	Loss 0.6063 (0.6254)	
training:	Epoch: [17][204/204]	Loss 0.6865 (0.6257)	
Training:	 Loss: 0.6248

Training:	 ACC: 0.6775 0.6791 0.7175 0.6374
Validation:	 ACC: 0.6762 0.6790 0.7380 0.6143
Validation:	 Best_BACC: 0.6762 0.6790 0.7380 0.6143
Validation:	 Loss: 0.6228
Pretraining:	Epoch 18/120
----------
training:	Epoch: [18][1/204]	Loss 0.6053 (0.6053)	
training:	Epoch: [18][2/204]	Loss 0.5999 (0.6026)	
training:	Epoch: [18][3/204]	Loss 0.6368 (0.6140)	
training:	Epoch: [18][4/204]	Loss 0.6282 (0.6175)	
training:	Epoch: [18][5/204]	Loss 0.6911 (0.6322)	
training:	Epoch: [18][6/204]	Loss 0.6138 (0.6292)	
training:	Epoch: [18][7/204]	Loss 0.6455 (0.6315)	
training:	Epoch: [18][8/204]	Loss 0.6134 (0.6292)	
training:	Epoch: [18][9/204]	Loss 0.6501 (0.6316)	
training:	Epoch: [18][10/204]	Loss 0.6309 (0.6315)	
training:	Epoch: [18][11/204]	Loss 0.6340 (0.6317)	
training:	Epoch: [18][12/204]	Loss 0.6094 (0.6299)	
training:	Epoch: [18][13/204]	Loss 0.5756 (0.6257)	
training:	Epoch: [18][14/204]	Loss 0.6097 (0.6246)	
training:	Epoch: [18][15/204]	Loss 0.5684 (0.6208)	
training:	Epoch: [18][16/204]	Loss 0.6346 (0.6217)	
training:	Epoch: [18][17/204]	Loss 0.5981 (0.6203)	
training:	Epoch: [18][18/204]	Loss 0.6774 (0.6235)	
training:	Epoch: [18][19/204]	Loss 0.6474 (0.6247)	
training:	Epoch: [18][20/204]	Loss 0.6446 (0.6257)	
training:	Epoch: [18][21/204]	Loss 0.6449 (0.6266)	
training:	Epoch: [18][22/204]	Loss 0.5503 (0.6232)	
training:	Epoch: [18][23/204]	Loss 0.6234 (0.6232)	
training:	Epoch: [18][24/204]	Loss 0.6432 (0.6240)	
training:	Epoch: [18][25/204]	Loss 0.6266 (0.6241)	
training:	Epoch: [18][26/204]	Loss 0.5651 (0.6218)	
training:	Epoch: [18][27/204]	Loss 0.5727 (0.6200)	
training:	Epoch: [18][28/204]	Loss 0.6281 (0.6203)	
training:	Epoch: [18][29/204]	Loss 0.5641 (0.6184)	
training:	Epoch: [18][30/204]	Loss 0.6691 (0.6201)	
training:	Epoch: [18][31/204]	Loss 0.7145 (0.6231)	
training:	Epoch: [18][32/204]	Loss 0.5767 (0.6217)	
training:	Epoch: [18][33/204]	Loss 0.6210 (0.6216)	
training:	Epoch: [18][34/204]	Loss 0.5950 (0.6208)	
training:	Epoch: [18][35/204]	Loss 0.6182 (0.6208)	
training:	Epoch: [18][36/204]	Loss 0.5599 (0.6191)	
training:	Epoch: [18][37/204]	Loss 0.7128 (0.6216)	
training:	Epoch: [18][38/204]	Loss 0.6050 (0.6212)	
training:	Epoch: [18][39/204]	Loss 0.6028 (0.6207)	
training:	Epoch: [18][40/204]	Loss 0.6739 (0.6220)	
training:	Epoch: [18][41/204]	Loss 0.6237 (0.6221)	
training:	Epoch: [18][42/204]	Loss 0.6049 (0.6217)	
training:	Epoch: [18][43/204]	Loss 0.6981 (0.6234)	
training:	Epoch: [18][44/204]	Loss 0.5940 (0.6228)	
training:	Epoch: [18][45/204]	Loss 0.5721 (0.6216)	
training:	Epoch: [18][46/204]	Loss 0.6237 (0.6217)	
training:	Epoch: [18][47/204]	Loss 0.6033 (0.6213)	
training:	Epoch: [18][48/204]	Loss 0.5758 (0.6204)	
training:	Epoch: [18][49/204]	Loss 0.7157 (0.6223)	
training:	Epoch: [18][50/204]	Loss 0.6700 (0.6233)	
training:	Epoch: [18][51/204]	Loss 0.7019 (0.6248)	
training:	Epoch: [18][52/204]	Loss 0.6148 (0.6246)	
training:	Epoch: [18][53/204]	Loss 0.5695 (0.6236)	
training:	Epoch: [18][54/204]	Loss 0.5892 (0.6229)	
training:	Epoch: [18][55/204]	Loss 0.5671 (0.6219)	
training:	Epoch: [18][56/204]	Loss 0.5890 (0.6213)	
training:	Epoch: [18][57/204]	Loss 0.6096 (0.6211)	
training:	Epoch: [18][58/204]	Loss 0.6351 (0.6214)	
training:	Epoch: [18][59/204]	Loss 0.6023 (0.6210)	
training:	Epoch: [18][60/204]	Loss 0.5755 (0.6203)	
training:	Epoch: [18][61/204]	Loss 0.6194 (0.6203)	
training:	Epoch: [18][62/204]	Loss 0.6312 (0.6204)	
training:	Epoch: [18][63/204]	Loss 0.6460 (0.6208)	
training:	Epoch: [18][64/204]	Loss 0.5899 (0.6204)	
training:	Epoch: [18][65/204]	Loss 0.6457 (0.6208)	
training:	Epoch: [18][66/204]	Loss 0.5978 (0.6204)	
training:	Epoch: [18][67/204]	Loss 0.5811 (0.6198)	
training:	Epoch: [18][68/204]	Loss 0.6578 (0.6204)	
training:	Epoch: [18][69/204]	Loss 0.6233 (0.6204)	
training:	Epoch: [18][70/204]	Loss 0.5627 (0.6196)	
training:	Epoch: [18][71/204]	Loss 0.6040 (0.6194)	
training:	Epoch: [18][72/204]	Loss 0.6294 (0.6195)	
training:	Epoch: [18][73/204]	Loss 0.7141 (0.6208)	
training:	Epoch: [18][74/204]	Loss 0.5473 (0.6198)	
training:	Epoch: [18][75/204]	Loss 0.5903 (0.6194)	
training:	Epoch: [18][76/204]	Loss 0.6123 (0.6193)	
training:	Epoch: [18][77/204]	Loss 0.6552 (0.6198)	
training:	Epoch: [18][78/204]	Loss 0.6253 (0.6199)	
training:	Epoch: [18][79/204]	Loss 0.5579 (0.6191)	
training:	Epoch: [18][80/204]	Loss 0.6755 (0.6198)	
training:	Epoch: [18][81/204]	Loss 0.6211 (0.6198)	
training:	Epoch: [18][82/204]	Loss 0.5676 (0.6192)	
training:	Epoch: [18][83/204]	Loss 0.6301 (0.6193)	
training:	Epoch: [18][84/204]	Loss 0.6266 (0.6194)	
training:	Epoch: [18][85/204]	Loss 0.6759 (0.6201)	
training:	Epoch: [18][86/204]	Loss 0.6131 (0.6200)	
training:	Epoch: [18][87/204]	Loss 0.5918 (0.6196)	
training:	Epoch: [18][88/204]	Loss 0.5392 (0.6187)	
training:	Epoch: [18][89/204]	Loss 0.6348 (0.6189)	
training:	Epoch: [18][90/204]	Loss 0.6607 (0.6194)	
training:	Epoch: [18][91/204]	Loss 0.6186 (0.6194)	
training:	Epoch: [18][92/204]	Loss 0.6453 (0.6196)	
training:	Epoch: [18][93/204]	Loss 0.6066 (0.6195)	
training:	Epoch: [18][94/204]	Loss 0.6080 (0.6194)	
training:	Epoch: [18][95/204]	Loss 0.6361 (0.6196)	
training:	Epoch: [18][96/204]	Loss 0.5746 (0.6191)	
training:	Epoch: [18][97/204]	Loss 0.6365 (0.6193)	
training:	Epoch: [18][98/204]	Loss 0.6627 (0.6197)	
training:	Epoch: [18][99/204]	Loss 0.5942 (0.6195)	
training:	Epoch: [18][100/204]	Loss 0.5646 (0.6189)	
training:	Epoch: [18][101/204]	Loss 0.6675 (0.6194)	
training:	Epoch: [18][102/204]	Loss 0.6600 (0.6198)	
training:	Epoch: [18][103/204]	Loss 0.5931 (0.6195)	
training:	Epoch: [18][104/204]	Loss 0.6221 (0.6196)	
training:	Epoch: [18][105/204]	Loss 0.6226 (0.6196)	
training:	Epoch: [18][106/204]	Loss 0.6825 (0.6202)	
training:	Epoch: [18][107/204]	Loss 0.6225 (0.6202)	
training:	Epoch: [18][108/204]	Loss 0.6531 (0.6205)	
training:	Epoch: [18][109/204]	Loss 0.5968 (0.6203)	
training:	Epoch: [18][110/204]	Loss 0.6627 (0.6207)	
training:	Epoch: [18][111/204]	Loss 0.6670 (0.6211)	
training:	Epoch: [18][112/204]	Loss 0.6799 (0.6216)	
training:	Epoch: [18][113/204]	Loss 0.6290 (0.6217)	
training:	Epoch: [18][114/204]	Loss 0.6124 (0.6216)	
training:	Epoch: [18][115/204]	Loss 0.5625 (0.6211)	
training:	Epoch: [18][116/204]	Loss 0.5939 (0.6208)	
training:	Epoch: [18][117/204]	Loss 0.6329 (0.6210)	
training:	Epoch: [18][118/204]	Loss 0.5805 (0.6206)	
training:	Epoch: [18][119/204]	Loss 0.6385 (0.6208)	
training:	Epoch: [18][120/204]	Loss 0.6372 (0.6209)	
training:	Epoch: [18][121/204]	Loss 0.6537 (0.6212)	
training:	Epoch: [18][122/204]	Loss 0.6817 (0.6217)	
training:	Epoch: [18][123/204]	Loss 0.6065 (0.6215)	
training:	Epoch: [18][124/204]	Loss 0.5575 (0.6210)	
training:	Epoch: [18][125/204]	Loss 0.6720 (0.6214)	
training:	Epoch: [18][126/204]	Loss 0.5992 (0.6213)	
training:	Epoch: [18][127/204]	Loss 0.5842 (0.6210)	
training:	Epoch: [18][128/204]	Loss 0.5829 (0.6207)	
training:	Epoch: [18][129/204]	Loss 0.5977 (0.6205)	
training:	Epoch: [18][130/204]	Loss 0.6015 (0.6203)	
training:	Epoch: [18][131/204]	Loss 0.6411 (0.6205)	
training:	Epoch: [18][132/204]	Loss 0.6557 (0.6208)	
training:	Epoch: [18][133/204]	Loss 0.5621 (0.6203)	
training:	Epoch: [18][134/204]	Loss 0.6342 (0.6204)	
training:	Epoch: [18][135/204]	Loss 0.6893 (0.6209)	
training:	Epoch: [18][136/204]	Loss 0.5879 (0.6207)	
training:	Epoch: [18][137/204]	Loss 0.6836 (0.6212)	
training:	Epoch: [18][138/204]	Loss 0.5449 (0.6206)	
training:	Epoch: [18][139/204]	Loss 0.5483 (0.6201)	
training:	Epoch: [18][140/204]	Loss 0.6264 (0.6201)	
training:	Epoch: [18][141/204]	Loss 0.7026 (0.6207)	
training:	Epoch: [18][142/204]	Loss 0.6183 (0.6207)	
training:	Epoch: [18][143/204]	Loss 0.5994 (0.6205)	
training:	Epoch: [18][144/204]	Loss 0.6451 (0.6207)	
training:	Epoch: [18][145/204]	Loss 0.6267 (0.6208)	
training:	Epoch: [18][146/204]	Loss 0.5738 (0.6204)	
training:	Epoch: [18][147/204]	Loss 0.6240 (0.6205)	
training:	Epoch: [18][148/204]	Loss 0.6479 (0.6206)	
training:	Epoch: [18][149/204]	Loss 0.5984 (0.6205)	
training:	Epoch: [18][150/204]	Loss 0.5453 (0.6200)	
training:	Epoch: [18][151/204]	Loss 0.6857 (0.6204)	
training:	Epoch: [18][152/204]	Loss 0.5821 (0.6202)	
training:	Epoch: [18][153/204]	Loss 0.7096 (0.6208)	
training:	Epoch: [18][154/204]	Loss 0.5928 (0.6206)	
training:	Epoch: [18][155/204]	Loss 0.5971 (0.6204)	
training:	Epoch: [18][156/204]	Loss 0.5929 (0.6203)	
training:	Epoch: [18][157/204]	Loss 0.6954 (0.6207)	
training:	Epoch: [18][158/204]	Loss 0.5970 (0.6206)	
training:	Epoch: [18][159/204]	Loss 0.5963 (0.6204)	
training:	Epoch: [18][160/204]	Loss 0.5844 (0.6202)	
training:	Epoch: [18][161/204]	Loss 0.6376 (0.6203)	
training:	Epoch: [18][162/204]	Loss 0.6203 (0.6203)	
training:	Epoch: [18][163/204]	Loss 0.5885 (0.6201)	
training:	Epoch: [18][164/204]	Loss 0.5480 (0.6197)	
training:	Epoch: [18][165/204]	Loss 0.6210 (0.6197)	
training:	Epoch: [18][166/204]	Loss 0.7025 (0.6202)	
training:	Epoch: [18][167/204]	Loss 0.6093 (0.6201)	
training:	Epoch: [18][168/204]	Loss 0.5681 (0.6198)	
training:	Epoch: [18][169/204]	Loss 0.6432 (0.6199)	
training:	Epoch: [18][170/204]	Loss 0.6282 (0.6200)	
training:	Epoch: [18][171/204]	Loss 0.6627 (0.6202)	
training:	Epoch: [18][172/204]	Loss 0.5678 (0.6199)	
training:	Epoch: [18][173/204]	Loss 0.5310 (0.6194)	
training:	Epoch: [18][174/204]	Loss 0.6416 (0.6196)	
training:	Epoch: [18][175/204]	Loss 0.5990 (0.6194)	
training:	Epoch: [18][176/204]	Loss 0.6101 (0.6194)	
training:	Epoch: [18][177/204]	Loss 0.5983 (0.6193)	
training:	Epoch: [18][178/204]	Loss 0.6427 (0.6194)	
training:	Epoch: [18][179/204]	Loss 0.6374 (0.6195)	
training:	Epoch: [18][180/204]	Loss 0.6442 (0.6196)	
training:	Epoch: [18][181/204]	Loss 0.6234 (0.6197)	
training:	Epoch: [18][182/204]	Loss 0.6413 (0.6198)	
training:	Epoch: [18][183/204]	Loss 0.6445 (0.6199)	
training:	Epoch: [18][184/204]	Loss 0.6025 (0.6198)	
training:	Epoch: [18][185/204]	Loss 0.6164 (0.6198)	
training:	Epoch: [18][186/204]	Loss 0.6273 (0.6198)	
training:	Epoch: [18][187/204]	Loss 0.5412 (0.6194)	
training:	Epoch: [18][188/204]	Loss 0.5672 (0.6191)	
training:	Epoch: [18][189/204]	Loss 0.5807 (0.6189)	
training:	Epoch: [18][190/204]	Loss 0.6474 (0.6191)	
training:	Epoch: [18][191/204]	Loss 0.5910 (0.6189)	
training:	Epoch: [18][192/204]	Loss 0.5985 (0.6188)	
training:	Epoch: [18][193/204]	Loss 0.6578 (0.6190)	
training:	Epoch: [18][194/204]	Loss 0.6591 (0.6192)	
training:	Epoch: [18][195/204]	Loss 0.5865 (0.6191)	
training:	Epoch: [18][196/204]	Loss 0.6176 (0.6191)	
training:	Epoch: [18][197/204]	Loss 0.6918 (0.6194)	
training:	Epoch: [18][198/204]	Loss 0.5564 (0.6191)	
training:	Epoch: [18][199/204]	Loss 0.6165 (0.6191)	
training:	Epoch: [18][200/204]	Loss 0.6349 (0.6192)	
training:	Epoch: [18][201/204]	Loss 0.6077 (0.6191)	
training:	Epoch: [18][202/204]	Loss 0.5535 (0.6188)	
training:	Epoch: [18][203/204]	Loss 0.5856 (0.6186)	
training:	Epoch: [18][204/204]	Loss 0.5462 (0.6183)	
Training:	 Loss: 0.6173

Training:	 ACC: 0.6805 0.6822 0.7205 0.6406
Validation:	 ACC: 0.6799 0.6827 0.7421 0.6177
Validation:	 Best_BACC: 0.6799 0.6827 0.7421 0.6177
Validation:	 Loss: 0.6173
Pretraining:	Epoch 19/120
----------
training:	Epoch: [19][1/204]	Loss 0.6022 (0.6022)	
training:	Epoch: [19][2/204]	Loss 0.5662 (0.5842)	
training:	Epoch: [19][3/204]	Loss 0.6496 (0.6060)	
training:	Epoch: [19][4/204]	Loss 0.6018 (0.6050)	
training:	Epoch: [19][5/204]	Loss 0.5893 (0.6018)	
training:	Epoch: [19][6/204]	Loss 0.6384 (0.6079)	
training:	Epoch: [19][7/204]	Loss 0.6171 (0.6092)	
training:	Epoch: [19][8/204]	Loss 0.5763 (0.6051)	
training:	Epoch: [19][9/204]	Loss 0.6395 (0.6089)	
training:	Epoch: [19][10/204]	Loss 0.6640 (0.6145)	
training:	Epoch: [19][11/204]	Loss 0.6361 (0.6164)	
training:	Epoch: [19][12/204]	Loss 0.6567 (0.6198)	
training:	Epoch: [19][13/204]	Loss 0.6068 (0.6188)	
training:	Epoch: [19][14/204]	Loss 0.5639 (0.6149)	
training:	Epoch: [19][15/204]	Loss 0.5720 (0.6120)	
training:	Epoch: [19][16/204]	Loss 0.6184 (0.6124)	
training:	Epoch: [19][17/204]	Loss 0.5478 (0.6086)	
training:	Epoch: [19][18/204]	Loss 0.6563 (0.6113)	
training:	Epoch: [19][19/204]	Loss 0.6101 (0.6112)	
training:	Epoch: [19][20/204]	Loss 0.6293 (0.6121)	
training:	Epoch: [19][21/204]	Loss 0.7214 (0.6173)	
training:	Epoch: [19][22/204]	Loss 0.5933 (0.6162)	
training:	Epoch: [19][23/204]	Loss 0.6971 (0.6197)	
training:	Epoch: [19][24/204]	Loss 0.6041 (0.6191)	
training:	Epoch: [19][25/204]	Loss 0.6399 (0.6199)	
training:	Epoch: [19][26/204]	Loss 0.5783 (0.6183)	
training:	Epoch: [19][27/204]	Loss 0.6277 (0.6187)	
training:	Epoch: [19][28/204]	Loss 0.5697 (0.6169)	
training:	Epoch: [19][29/204]	Loss 0.6009 (0.6164)	
training:	Epoch: [19][30/204]	Loss 0.5589 (0.6144)	
training:	Epoch: [19][31/204]	Loss 0.6112 (0.6143)	
training:	Epoch: [19][32/204]	Loss 0.6267 (0.6147)	
training:	Epoch: [19][33/204]	Loss 0.6579 (0.6160)	
training:	Epoch: [19][34/204]	Loss 0.6179 (0.6161)	
training:	Epoch: [19][35/204]	Loss 0.5736 (0.6149)	
training:	Epoch: [19][36/204]	Loss 0.5838 (0.6140)	
training:	Epoch: [19][37/204]	Loss 0.6191 (0.6141)	
training:	Epoch: [19][38/204]	Loss 0.6098 (0.6140)	
training:	Epoch: [19][39/204]	Loss 0.6291 (0.6144)	
training:	Epoch: [19][40/204]	Loss 0.6208 (0.6146)	
training:	Epoch: [19][41/204]	Loss 0.6076 (0.6144)	
training:	Epoch: [19][42/204]	Loss 0.5835 (0.6137)	
training:	Epoch: [19][43/204]	Loss 0.6131 (0.6137)	
training:	Epoch: [19][44/204]	Loss 0.5920 (0.6132)	
training:	Epoch: [19][45/204]	Loss 0.6856 (0.6148)	
training:	Epoch: [19][46/204]	Loss 0.6199 (0.6149)	
training:	Epoch: [19][47/204]	Loss 0.5963 (0.6145)	
training:	Epoch: [19][48/204]	Loss 0.6514 (0.6153)	
training:	Epoch: [19][49/204]	Loss 0.6524 (0.6160)	
training:	Epoch: [19][50/204]	Loss 0.6552 (0.6168)	
training:	Epoch: [19][51/204]	Loss 0.6852 (0.6181)	
training:	Epoch: [19][52/204]	Loss 0.6104 (0.6180)	
training:	Epoch: [19][53/204]	Loss 0.5760 (0.6172)	
training:	Epoch: [19][54/204]	Loss 0.5506 (0.6160)	
training:	Epoch: [19][55/204]	Loss 0.5896 (0.6155)	
training:	Epoch: [19][56/204]	Loss 0.6177 (0.6155)	
training:	Epoch: [19][57/204]	Loss 0.5738 (0.6148)	
training:	Epoch: [19][58/204]	Loss 0.6082 (0.6147)	
training:	Epoch: [19][59/204]	Loss 0.6394 (0.6151)	
training:	Epoch: [19][60/204]	Loss 0.6476 (0.6156)	
training:	Epoch: [19][61/204]	Loss 0.6255 (0.6158)	
training:	Epoch: [19][62/204]	Loss 0.5843 (0.6153)	
training:	Epoch: [19][63/204]	Loss 0.6513 (0.6159)	
training:	Epoch: [19][64/204]	Loss 0.6298 (0.6161)	
training:	Epoch: [19][65/204]	Loss 0.6742 (0.6170)	
training:	Epoch: [19][66/204]	Loss 0.6328 (0.6172)	
training:	Epoch: [19][67/204]	Loss 0.6296 (0.6174)	
training:	Epoch: [19][68/204]	Loss 0.7053 (0.6187)	
training:	Epoch: [19][69/204]	Loss 0.6751 (0.6195)	
training:	Epoch: [19][70/204]	Loss 0.6857 (0.6205)	
training:	Epoch: [19][71/204]	Loss 0.6825 (0.6213)	
training:	Epoch: [19][72/204]	Loss 0.6181 (0.6213)	
training:	Epoch: [19][73/204]	Loss 0.6453 (0.6216)	
training:	Epoch: [19][74/204]	Loss 0.5237 (0.6203)	
training:	Epoch: [19][75/204]	Loss 0.6436 (0.6206)	
training:	Epoch: [19][76/204]	Loss 0.5881 (0.6202)	
training:	Epoch: [19][77/204]	Loss 0.5975 (0.6199)	
training:	Epoch: [19][78/204]	Loss 0.5472 (0.6189)	
training:	Epoch: [19][79/204]	Loss 0.6565 (0.6194)	
training:	Epoch: [19][80/204]	Loss 0.5781 (0.6189)	
training:	Epoch: [19][81/204]	Loss 0.6468 (0.6193)	
training:	Epoch: [19][82/204]	Loss 0.5818 (0.6188)	
training:	Epoch: [19][83/204]	Loss 0.6128 (0.6187)	
training:	Epoch: [19][84/204]	Loss 0.6258 (0.6188)	
training:	Epoch: [19][85/204]	Loss 0.6090 (0.6187)	
training:	Epoch: [19][86/204]	Loss 0.5614 (0.6180)	
training:	Epoch: [19][87/204]	Loss 0.6155 (0.6180)	
training:	Epoch: [19][88/204]	Loss 0.5632 (0.6174)	
training:	Epoch: [19][89/204]	Loss 0.6911 (0.6182)	
training:	Epoch: [19][90/204]	Loss 0.5812 (0.6178)	
training:	Epoch: [19][91/204]	Loss 0.6314 (0.6179)	
training:	Epoch: [19][92/204]	Loss 0.6260 (0.6180)	
training:	Epoch: [19][93/204]	Loss 0.5789 (0.6176)	
training:	Epoch: [19][94/204]	Loss 0.5568 (0.6170)	
training:	Epoch: [19][95/204]	Loss 0.6517 (0.6173)	
training:	Epoch: [19][96/204]	Loss 0.5443 (0.6166)	
training:	Epoch: [19][97/204]	Loss 0.5848 (0.6162)	
training:	Epoch: [19][98/204]	Loss 0.6115 (0.6162)	
training:	Epoch: [19][99/204]	Loss 0.6321 (0.6164)	
training:	Epoch: [19][100/204]	Loss 0.5945 (0.6161)	
training:	Epoch: [19][101/204]	Loss 0.5973 (0.6159)	
training:	Epoch: [19][102/204]	Loss 0.6270 (0.6161)	
training:	Epoch: [19][103/204]	Loss 0.6051 (0.6159)	
training:	Epoch: [19][104/204]	Loss 0.6174 (0.6160)	
training:	Epoch: [19][105/204]	Loss 0.5859 (0.6157)	
training:	Epoch: [19][106/204]	Loss 0.5349 (0.6149)	
training:	Epoch: [19][107/204]	Loss 0.6370 (0.6151)	
training:	Epoch: [19][108/204]	Loss 0.6011 (0.6150)	
training:	Epoch: [19][109/204]	Loss 0.5464 (0.6144)	
training:	Epoch: [19][110/204]	Loss 0.6853 (0.6150)	
training:	Epoch: [19][111/204]	Loss 0.7192 (0.6159)	
training:	Epoch: [19][112/204]	Loss 0.6678 (0.6164)	
training:	Epoch: [19][113/204]	Loss 0.5560 (0.6159)	
training:	Epoch: [19][114/204]	Loss 0.6010 (0.6157)	
training:	Epoch: [19][115/204]	Loss 0.5818 (0.6154)	
training:	Epoch: [19][116/204]	Loss 0.5974 (0.6153)	
training:	Epoch: [19][117/204]	Loss 0.5812 (0.6150)	
training:	Epoch: [19][118/204]	Loss 0.6151 (0.6150)	
training:	Epoch: [19][119/204]	Loss 0.5357 (0.6143)	
training:	Epoch: [19][120/204]	Loss 0.6217 (0.6144)	
training:	Epoch: [19][121/204]	Loss 0.6671 (0.6148)	
training:	Epoch: [19][122/204]	Loss 0.5493 (0.6143)	
training:	Epoch: [19][123/204]	Loss 0.6513 (0.6146)	
training:	Epoch: [19][124/204]	Loss 0.6259 (0.6147)	
training:	Epoch: [19][125/204]	Loss 0.6063 (0.6146)	
training:	Epoch: [19][126/204]	Loss 0.6463 (0.6149)	
training:	Epoch: [19][127/204]	Loss 0.6444 (0.6151)	
training:	Epoch: [19][128/204]	Loss 0.5823 (0.6148)	
training:	Epoch: [19][129/204]	Loss 0.6071 (0.6148)	
training:	Epoch: [19][130/204]	Loss 0.6011 (0.6147)	
training:	Epoch: [19][131/204]	Loss 0.6432 (0.6149)	
training:	Epoch: [19][132/204]	Loss 0.6586 (0.6152)	
training:	Epoch: [19][133/204]	Loss 0.6346 (0.6154)	
training:	Epoch: [19][134/204]	Loss 0.6732 (0.6158)	
training:	Epoch: [19][135/204]	Loss 0.5907 (0.6156)	
training:	Epoch: [19][136/204]	Loss 0.7524 (0.6166)	
training:	Epoch: [19][137/204]	Loss 0.6070 (0.6166)	
training:	Epoch: [19][138/204]	Loss 0.5899 (0.6164)	
training:	Epoch: [19][139/204]	Loss 0.5618 (0.6160)	
training:	Epoch: [19][140/204]	Loss 0.5587 (0.6156)	
training:	Epoch: [19][141/204]	Loss 0.6479 (0.6158)	
training:	Epoch: [19][142/204]	Loss 0.6098 (0.6158)	
training:	Epoch: [19][143/204]	Loss 0.7107 (0.6164)	
training:	Epoch: [19][144/204]	Loss 0.6328 (0.6165)	
training:	Epoch: [19][145/204]	Loss 0.5454 (0.6160)	
training:	Epoch: [19][146/204]	Loss 0.6947 (0.6166)	
training:	Epoch: [19][147/204]	Loss 0.5612 (0.6162)	
training:	Epoch: [19][148/204]	Loss 0.5123 (0.6155)	
training:	Epoch: [19][149/204]	Loss 0.6377 (0.6156)	
training:	Epoch: [19][150/204]	Loss 0.6191 (0.6157)	
training:	Epoch: [19][151/204]	Loss 0.5994 (0.6156)	
training:	Epoch: [19][152/204]	Loss 0.5972 (0.6154)	
training:	Epoch: [19][153/204]	Loss 0.6557 (0.6157)	
training:	Epoch: [19][154/204]	Loss 0.5565 (0.6153)	
training:	Epoch: [19][155/204]	Loss 0.6491 (0.6155)	
training:	Epoch: [19][156/204]	Loss 0.5574 (0.6152)	
training:	Epoch: [19][157/204]	Loss 0.6466 (0.6154)	
training:	Epoch: [19][158/204]	Loss 0.6104 (0.6153)	
training:	Epoch: [19][159/204]	Loss 0.6424 (0.6155)	
training:	Epoch: [19][160/204]	Loss 0.5762 (0.6153)	
training:	Epoch: [19][161/204]	Loss 0.6682 (0.6156)	
training:	Epoch: [19][162/204]	Loss 0.6301 (0.6157)	
training:	Epoch: [19][163/204]	Loss 0.5691 (0.6154)	
training:	Epoch: [19][164/204]	Loss 0.5332 (0.6149)	
training:	Epoch: [19][165/204]	Loss 0.6323 (0.6150)	
training:	Epoch: [19][166/204]	Loss 0.5463 (0.6146)	
training:	Epoch: [19][167/204]	Loss 0.5829 (0.6144)	
training:	Epoch: [19][168/204]	Loss 0.6031 (0.6143)	
training:	Epoch: [19][169/204]	Loss 0.6233 (0.6144)	
training:	Epoch: [19][170/204]	Loss 0.5632 (0.6141)	
training:	Epoch: [19][171/204]	Loss 0.5687 (0.6138)	
training:	Epoch: [19][172/204]	Loss 0.6123 (0.6138)	
training:	Epoch: [19][173/204]	Loss 0.6105 (0.6138)	
training:	Epoch: [19][174/204]	Loss 0.6073 (0.6137)	
training:	Epoch: [19][175/204]	Loss 0.5986 (0.6137)	
training:	Epoch: [19][176/204]	Loss 0.5805 (0.6135)	
training:	Epoch: [19][177/204]	Loss 0.6199 (0.6135)	
training:	Epoch: [19][178/204]	Loss 0.6449 (0.6137)	
training:	Epoch: [19][179/204]	Loss 0.6127 (0.6137)	
training:	Epoch: [19][180/204]	Loss 0.5595 (0.6134)	
training:	Epoch: [19][181/204]	Loss 0.5728 (0.6132)	
training:	Epoch: [19][182/204]	Loss 0.6220 (0.6132)	
training:	Epoch: [19][183/204]	Loss 0.6330 (0.6133)	
training:	Epoch: [19][184/204]	Loss 0.6227 (0.6134)	
training:	Epoch: [19][185/204]	Loss 0.5945 (0.6133)	
training:	Epoch: [19][186/204]	Loss 0.6260 (0.6133)	
training:	Epoch: [19][187/204]	Loss 0.6816 (0.6137)	
training:	Epoch: [19][188/204]	Loss 0.7679 (0.6145)	
training:	Epoch: [19][189/204]	Loss 0.6496 (0.6147)	
training:	Epoch: [19][190/204]	Loss 0.6162 (0.6147)	
training:	Epoch: [19][191/204]	Loss 0.5765 (0.6145)	
training:	Epoch: [19][192/204]	Loss 0.5228 (0.6140)	
training:	Epoch: [19][193/204]	Loss 0.6959 (0.6145)	
training:	Epoch: [19][194/204]	Loss 0.6409 (0.6146)	
training:	Epoch: [19][195/204]	Loss 0.6222 (0.6146)	
training:	Epoch: [19][196/204]	Loss 0.5129 (0.6141)	
training:	Epoch: [19][197/204]	Loss 0.6387 (0.6142)	
training:	Epoch: [19][198/204]	Loss 0.6402 (0.6144)	
training:	Epoch: [19][199/204]	Loss 0.6326 (0.6145)	
training:	Epoch: [19][200/204]	Loss 0.6683 (0.6147)	
training:	Epoch: [19][201/204]	Loss 0.6317 (0.6148)	
training:	Epoch: [19][202/204]	Loss 0.5878 (0.6147)	
training:	Epoch: [19][203/204]	Loss 0.5568 (0.6144)	
training:	Epoch: [19][204/204]	Loss 0.5624 (0.6141)	
Training:	 Loss: 0.6132

Training:	 ACC: 0.6828 0.6851 0.7390 0.6266
Validation:	 ACC: 0.6826 0.6859 0.7564 0.6087
Validation:	 Best_BACC: 0.6826 0.6859 0.7564 0.6087
Validation:	 Loss: 0.6117
Pretraining:	Epoch 20/120
----------
training:	Epoch: [20][1/204]	Loss 0.6059 (0.6059)	
training:	Epoch: [20][2/204]	Loss 0.6963 (0.6511)	
training:	Epoch: [20][3/204]	Loss 0.6070 (0.6364)	
training:	Epoch: [20][4/204]	Loss 0.6151 (0.6311)	
training:	Epoch: [20][5/204]	Loss 0.5755 (0.6199)	
training:	Epoch: [20][6/204]	Loss 0.6323 (0.6220)	
training:	Epoch: [20][7/204]	Loss 0.5448 (0.6110)	
training:	Epoch: [20][8/204]	Loss 0.5698 (0.6058)	
training:	Epoch: [20][9/204]	Loss 0.6175 (0.6071)	
training:	Epoch: [20][10/204]	Loss 0.6032 (0.6067)	
training:	Epoch: [20][11/204]	Loss 0.6131 (0.6073)	
training:	Epoch: [20][12/204]	Loss 0.6108 (0.6076)	
training:	Epoch: [20][13/204]	Loss 0.5608 (0.6040)	
training:	Epoch: [20][14/204]	Loss 0.6549 (0.6076)	
training:	Epoch: [20][15/204]	Loss 0.5519 (0.6039)	
training:	Epoch: [20][16/204]	Loss 0.6208 (0.6050)	
training:	Epoch: [20][17/204]	Loss 0.5309 (0.6006)	
training:	Epoch: [20][18/204]	Loss 0.5882 (0.5999)	
training:	Epoch: [20][19/204]	Loss 0.6163 (0.6008)	
training:	Epoch: [20][20/204]	Loss 0.6029 (0.6009)	
training:	Epoch: [20][21/204]	Loss 0.6381 (0.6027)	
training:	Epoch: [20][22/204]	Loss 0.6492 (0.6048)	
training:	Epoch: [20][23/204]	Loss 0.6465 (0.6066)	
training:	Epoch: [20][24/204]	Loss 0.6144 (0.6069)	
training:	Epoch: [20][25/204]	Loss 0.6010 (0.6067)	
training:	Epoch: [20][26/204]	Loss 0.6325 (0.6077)	
training:	Epoch: [20][27/204]	Loss 0.5312 (0.6048)	
training:	Epoch: [20][28/204]	Loss 0.6188 (0.6053)	
training:	Epoch: [20][29/204]	Loss 0.6510 (0.6069)	
training:	Epoch: [20][30/204]	Loss 0.5401 (0.6047)	
training:	Epoch: [20][31/204]	Loss 0.6555 (0.6063)	
training:	Epoch: [20][32/204]	Loss 0.6227 (0.6068)	
training:	Epoch: [20][33/204]	Loss 0.6114 (0.6070)	
training:	Epoch: [20][34/204]	Loss 0.5582 (0.6055)	
training:	Epoch: [20][35/204]	Loss 0.5862 (0.6050)	
training:	Epoch: [20][36/204]	Loss 0.6474 (0.6062)	
training:	Epoch: [20][37/204]	Loss 0.6129 (0.6063)	
training:	Epoch: [20][38/204]	Loss 0.6369 (0.6071)	
training:	Epoch: [20][39/204]	Loss 0.6277 (0.6077)	
training:	Epoch: [20][40/204]	Loss 0.5993 (0.6075)	
training:	Epoch: [20][41/204]	Loss 0.6205 (0.6078)	
training:	Epoch: [20][42/204]	Loss 0.6163 (0.6080)	
training:	Epoch: [20][43/204]	Loss 0.5847 (0.6074)	
training:	Epoch: [20][44/204]	Loss 0.6531 (0.6085)	
training:	Epoch: [20][45/204]	Loss 0.6425 (0.6092)	
training:	Epoch: [20][46/204]	Loss 0.5236 (0.6074)	
training:	Epoch: [20][47/204]	Loss 0.5880 (0.6070)	
training:	Epoch: [20][48/204]	Loss 0.6140 (0.6071)	
training:	Epoch: [20][49/204]	Loss 0.5584 (0.6061)	
training:	Epoch: [20][50/204]	Loss 0.6455 (0.6069)	
training:	Epoch: [20][51/204]	Loss 0.6012 (0.6068)	
training:	Epoch: [20][52/204]	Loss 0.5705 (0.6061)	
training:	Epoch: [20][53/204]	Loss 0.5947 (0.6059)	
training:	Epoch: [20][54/204]	Loss 0.5909 (0.6056)	
training:	Epoch: [20][55/204]	Loss 0.5895 (0.6053)	
training:	Epoch: [20][56/204]	Loss 0.5337 (0.6040)	
training:	Epoch: [20][57/204]	Loss 0.5887 (0.6038)	
training:	Epoch: [20][58/204]	Loss 0.6392 (0.6044)	
training:	Epoch: [20][59/204]	Loss 0.5908 (0.6041)	
training:	Epoch: [20][60/204]	Loss 0.6241 (0.6045)	
training:	Epoch: [20][61/204]	Loss 0.5035 (0.6028)	
training:	Epoch: [20][62/204]	Loss 0.5409 (0.6018)	
training:	Epoch: [20][63/204]	Loss 0.6099 (0.6020)	
training:	Epoch: [20][64/204]	Loss 0.5797 (0.6016)	
training:	Epoch: [20][65/204]	Loss 0.6406 (0.6022)	
training:	Epoch: [20][66/204]	Loss 0.5912 (0.6020)	
training:	Epoch: [20][67/204]	Loss 0.5308 (0.6010)	
training:	Epoch: [20][68/204]	Loss 0.5967 (0.6009)	
training:	Epoch: [20][69/204]	Loss 0.6201 (0.6012)	
training:	Epoch: [20][70/204]	Loss 0.6156 (0.6014)	
training:	Epoch: [20][71/204]	Loss 0.6503 (0.6021)	
training:	Epoch: [20][72/204]	Loss 0.5603 (0.6015)	
training:	Epoch: [20][73/204]	Loss 0.5981 (0.6015)	
training:	Epoch: [20][74/204]	Loss 0.6374 (0.6019)	
training:	Epoch: [20][75/204]	Loss 0.6025 (0.6019)	
training:	Epoch: [20][76/204]	Loss 0.5841 (0.6017)	
training:	Epoch: [20][77/204]	Loss 0.5857 (0.6015)	
training:	Epoch: [20][78/204]	Loss 0.5755 (0.6012)	
training:	Epoch: [20][79/204]	Loss 0.5790 (0.6009)	
training:	Epoch: [20][80/204]	Loss 0.6069 (0.6010)	
training:	Epoch: [20][81/204]	Loss 0.6101 (0.6011)	
training:	Epoch: [20][82/204]	Loss 0.6375 (0.6015)	
training:	Epoch: [20][83/204]	Loss 0.6444 (0.6020)	
training:	Epoch: [20][84/204]	Loss 0.6612 (0.6027)	
training:	Epoch: [20][85/204]	Loss 0.6092 (0.6028)	
training:	Epoch: [20][86/204]	Loss 0.6330 (0.6032)	
training:	Epoch: [20][87/204]	Loss 0.6580 (0.6038)	
training:	Epoch: [20][88/204]	Loss 0.6512 (0.6043)	
training:	Epoch: [20][89/204]	Loss 0.6689 (0.6051)	
training:	Epoch: [20][90/204]	Loss 0.5787 (0.6048)	
training:	Epoch: [20][91/204]	Loss 0.6206 (0.6049)	
training:	Epoch: [20][92/204]	Loss 0.6013 (0.6049)	
training:	Epoch: [20][93/204]	Loss 0.6701 (0.6056)	
training:	Epoch: [20][94/204]	Loss 0.6079 (0.6056)	
training:	Epoch: [20][95/204]	Loss 0.5920 (0.6055)	
training:	Epoch: [20][96/204]	Loss 0.6280 (0.6057)	
training:	Epoch: [20][97/204]	Loss 0.6474 (0.6062)	
training:	Epoch: [20][98/204]	Loss 0.6063 (0.6062)	
training:	Epoch: [20][99/204]	Loss 0.5119 (0.6052)	
training:	Epoch: [20][100/204]	Loss 0.6357 (0.6055)	
training:	Epoch: [20][101/204]	Loss 0.6272 (0.6057)	
training:	Epoch: [20][102/204]	Loss 0.5740 (0.6054)	
training:	Epoch: [20][103/204]	Loss 0.6182 (0.6055)	
training:	Epoch: [20][104/204]	Loss 0.6617 (0.6061)	
training:	Epoch: [20][105/204]	Loss 0.6247 (0.6063)	
training:	Epoch: [20][106/204]	Loss 0.6391 (0.6066)	
training:	Epoch: [20][107/204]	Loss 0.5938 (0.6064)	
training:	Epoch: [20][108/204]	Loss 0.5983 (0.6064)	
training:	Epoch: [20][109/204]	Loss 0.6388 (0.6067)	
training:	Epoch: [20][110/204]	Loss 0.6180 (0.6068)	
training:	Epoch: [20][111/204]	Loss 0.6461 (0.6071)	
training:	Epoch: [20][112/204]	Loss 0.6416 (0.6074)	
training:	Epoch: [20][113/204]	Loss 0.5679 (0.6071)	
training:	Epoch: [20][114/204]	Loss 0.5711 (0.6068)	
training:	Epoch: [20][115/204]	Loss 0.6436 (0.6071)	
training:	Epoch: [20][116/204]	Loss 0.5818 (0.6069)	
training:	Epoch: [20][117/204]	Loss 0.5741 (0.6066)	
training:	Epoch: [20][118/204]	Loss 0.6837 (0.6072)	
training:	Epoch: [20][119/204]	Loss 0.6161 (0.6073)	
training:	Epoch: [20][120/204]	Loss 0.6056 (0.6073)	
training:	Epoch: [20][121/204]	Loss 0.6218 (0.6074)	
training:	Epoch: [20][122/204]	Loss 0.6552 (0.6078)	
training:	Epoch: [20][123/204]	Loss 0.6547 (0.6082)	
training:	Epoch: [20][124/204]	Loss 0.6041 (0.6082)	
training:	Epoch: [20][125/204]	Loss 0.6157 (0.6082)	
training:	Epoch: [20][126/204]	Loss 0.6528 (0.6086)	
training:	Epoch: [20][127/204]	Loss 0.6110 (0.6086)	
training:	Epoch: [20][128/204]	Loss 0.5413 (0.6081)	
training:	Epoch: [20][129/204]	Loss 0.5813 (0.6079)	
training:	Epoch: [20][130/204]	Loss 0.6165 (0.6079)	
training:	Epoch: [20][131/204]	Loss 0.6210 (0.6080)	
training:	Epoch: [20][132/204]	Loss 0.6270 (0.6082)	
training:	Epoch: [20][133/204]	Loss 0.6032 (0.6081)	
training:	Epoch: [20][134/204]	Loss 0.6633 (0.6085)	
training:	Epoch: [20][135/204]	Loss 0.5378 (0.6080)	
training:	Epoch: [20][136/204]	Loss 0.6028 (0.6080)	
training:	Epoch: [20][137/204]	Loss 0.6100 (0.6080)	
training:	Epoch: [20][138/204]	Loss 0.6158 (0.6081)	
training:	Epoch: [20][139/204]	Loss 0.5817 (0.6079)	
training:	Epoch: [20][140/204]	Loss 0.6081 (0.6079)	
training:	Epoch: [20][141/204]	Loss 0.6105 (0.6079)	
training:	Epoch: [20][142/204]	Loss 0.6756 (0.6084)	
training:	Epoch: [20][143/204]	Loss 0.5910 (0.6082)	
training:	Epoch: [20][144/204]	Loss 0.6421 (0.6085)	
training:	Epoch: [20][145/204]	Loss 0.6355 (0.6087)	
training:	Epoch: [20][146/204]	Loss 0.5867 (0.6085)	
training:	Epoch: [20][147/204]	Loss 0.6399 (0.6087)	
training:	Epoch: [20][148/204]	Loss 0.5882 (0.6086)	
training:	Epoch: [20][149/204]	Loss 0.4944 (0.6078)	
training:	Epoch: [20][150/204]	Loss 0.6261 (0.6079)	
training:	Epoch: [20][151/204]	Loss 0.5936 (0.6078)	
training:	Epoch: [20][152/204]	Loss 0.6591 (0.6082)	
training:	Epoch: [20][153/204]	Loss 0.6009 (0.6081)	
training:	Epoch: [20][154/204]	Loss 0.6472 (0.6084)	
training:	Epoch: [20][155/204]	Loss 0.7481 (0.6093)	
training:	Epoch: [20][156/204]	Loss 0.6794 (0.6097)	
training:	Epoch: [20][157/204]	Loss 0.5599 (0.6094)	
training:	Epoch: [20][158/204]	Loss 0.5591 (0.6091)	
training:	Epoch: [20][159/204]	Loss 0.6676 (0.6095)	
training:	Epoch: [20][160/204]	Loss 0.6359 (0.6096)	
training:	Epoch: [20][161/204]	Loss 0.6074 (0.6096)	
training:	Epoch: [20][162/204]	Loss 0.5992 (0.6096)	
training:	Epoch: [20][163/204]	Loss 0.6213 (0.6096)	
training:	Epoch: [20][164/204]	Loss 0.5185 (0.6091)	
training:	Epoch: [20][165/204]	Loss 0.5795 (0.6089)	
training:	Epoch: [20][166/204]	Loss 0.5807 (0.6087)	
training:	Epoch: [20][167/204]	Loss 0.6036 (0.6087)	
training:	Epoch: [20][168/204]	Loss 0.6435 (0.6089)	
training:	Epoch: [20][169/204]	Loss 0.6164 (0.6089)	
training:	Epoch: [20][170/204]	Loss 0.6080 (0.6089)	
training:	Epoch: [20][171/204]	Loss 0.5276 (0.6085)	
training:	Epoch: [20][172/204]	Loss 0.6530 (0.6087)	
training:	Epoch: [20][173/204]	Loss 0.6639 (0.6090)	
training:	Epoch: [20][174/204]	Loss 0.6029 (0.6090)	
training:	Epoch: [20][175/204]	Loss 0.5354 (0.6086)	
training:	Epoch: [20][176/204]	Loss 0.6024 (0.6086)	
training:	Epoch: [20][177/204]	Loss 0.5506 (0.6082)	
training:	Epoch: [20][178/204]	Loss 0.7418 (0.6090)	
training:	Epoch: [20][179/204]	Loss 0.6288 (0.6091)	
training:	Epoch: [20][180/204]	Loss 0.6442 (0.6093)	
training:	Epoch: [20][181/204]	Loss 0.6433 (0.6095)	
training:	Epoch: [20][182/204]	Loss 0.6608 (0.6098)	
training:	Epoch: [20][183/204]	Loss 0.5828 (0.6096)	
training:	Epoch: [20][184/204]	Loss 0.5462 (0.6093)	
training:	Epoch: [20][185/204]	Loss 0.6801 (0.6096)	
training:	Epoch: [20][186/204]	Loss 0.7016 (0.6101)	
training:	Epoch: [20][187/204]	Loss 0.6234 (0.6102)	
training:	Epoch: [20][188/204]	Loss 0.5837 (0.6101)	
training:	Epoch: [20][189/204]	Loss 0.5576 (0.6098)	
training:	Epoch: [20][190/204]	Loss 0.6030 (0.6098)	
training:	Epoch: [20][191/204]	Loss 0.6484 (0.6100)	
training:	Epoch: [20][192/204]	Loss 0.6323 (0.6101)	
training:	Epoch: [20][193/204]	Loss 0.5943 (0.6100)	
training:	Epoch: [20][194/204]	Loss 0.5857 (0.6099)	
training:	Epoch: [20][195/204]	Loss 0.6174 (0.6099)	
training:	Epoch: [20][196/204]	Loss 0.6564 (0.6101)	
training:	Epoch: [20][197/204]	Loss 0.5781 (0.6100)	
training:	Epoch: [20][198/204]	Loss 0.6413 (0.6101)	
training:	Epoch: [20][199/204]	Loss 0.6270 (0.6102)	
training:	Epoch: [20][200/204]	Loss 0.5782 (0.6101)	
training:	Epoch: [20][201/204]	Loss 0.6243 (0.6101)	
training:	Epoch: [20][202/204]	Loss 0.6593 (0.6104)	
training:	Epoch: [20][203/204]	Loss 0.6097 (0.6104)	
training:	Epoch: [20][204/204]	Loss 0.6242 (0.6104)	
Training:	 Loss: 0.6095

Training:	 ACC: 0.6856 0.6875 0.7316 0.6397
Validation:	 ACC: 0.6842 0.6870 0.7451 0.6233
Validation:	 Best_BACC: 0.6842 0.6870 0.7451 0.6233
Validation:	 Loss: 0.6064
Pretraining:	Epoch 21/120
----------
training:	Epoch: [21][1/204]	Loss 0.5550 (0.5550)	
training:	Epoch: [21][2/204]	Loss 0.5789 (0.5670)	
training:	Epoch: [21][3/204]	Loss 0.6116 (0.5818)	
training:	Epoch: [21][4/204]	Loss 0.6284 (0.5935)	
training:	Epoch: [21][5/204]	Loss 0.5489 (0.5846)	
training:	Epoch: [21][6/204]	Loss 0.6106 (0.5889)	
training:	Epoch: [21][7/204]	Loss 0.5544 (0.5840)	
training:	Epoch: [21][8/204]	Loss 0.6710 (0.5949)	
training:	Epoch: [21][9/204]	Loss 0.5121 (0.5857)	
training:	Epoch: [21][10/204]	Loss 0.6691 (0.5940)	
training:	Epoch: [21][11/204]	Loss 0.6135 (0.5958)	
training:	Epoch: [21][12/204]	Loss 0.5701 (0.5936)	
training:	Epoch: [21][13/204]	Loss 0.5978 (0.5940)	
training:	Epoch: [21][14/204]	Loss 0.5870 (0.5935)	
training:	Epoch: [21][15/204]	Loss 0.5884 (0.5931)	
training:	Epoch: [21][16/204]	Loss 0.6466 (0.5965)	
training:	Epoch: [21][17/204]	Loss 0.6314 (0.5985)	
training:	Epoch: [21][18/204]	Loss 0.6167 (0.5995)	
training:	Epoch: [21][19/204]	Loss 0.6730 (0.6034)	
training:	Epoch: [21][20/204]	Loss 0.5785 (0.6022)	
training:	Epoch: [21][21/204]	Loss 0.5963 (0.6019)	
training:	Epoch: [21][22/204]	Loss 0.5884 (0.6013)	
training:	Epoch: [21][23/204]	Loss 0.6285 (0.6024)	
training:	Epoch: [21][24/204]	Loss 0.5909 (0.6020)	
training:	Epoch: [21][25/204]	Loss 0.6388 (0.6034)	
training:	Epoch: [21][26/204]	Loss 0.6388 (0.6048)	
training:	Epoch: [21][27/204]	Loss 0.6186 (0.6053)	
training:	Epoch: [21][28/204]	Loss 0.6210 (0.6059)	
training:	Epoch: [21][29/204]	Loss 0.5997 (0.6057)	
training:	Epoch: [21][30/204]	Loss 0.5707 (0.6045)	
training:	Epoch: [21][31/204]	Loss 0.5684 (0.6033)	
training:	Epoch: [21][32/204]	Loss 0.5678 (0.6022)	
training:	Epoch: [21][33/204]	Loss 0.5892 (0.6018)	
training:	Epoch: [21][34/204]	Loss 0.6518 (0.6033)	
training:	Epoch: [21][35/204]	Loss 0.6641 (0.6050)	
training:	Epoch: [21][36/204]	Loss 0.5977 (0.6048)	
training:	Epoch: [21][37/204]	Loss 0.5831 (0.6042)	
training:	Epoch: [21][38/204]	Loss 0.6603 (0.6057)	
training:	Epoch: [21][39/204]	Loss 0.5793 (0.6050)	
training:	Epoch: [21][40/204]	Loss 0.6265 (0.6056)	
training:	Epoch: [21][41/204]	Loss 0.6387 (0.6064)	
training:	Epoch: [21][42/204]	Loss 0.6716 (0.6079)	
training:	Epoch: [21][43/204]	Loss 0.5675 (0.6070)	
training:	Epoch: [21][44/204]	Loss 0.6925 (0.6089)	
training:	Epoch: [21][45/204]	Loss 0.6039 (0.6088)	
training:	Epoch: [21][46/204]	Loss 0.6099 (0.6089)	
training:	Epoch: [21][47/204]	Loss 0.6190 (0.6091)	
training:	Epoch: [21][48/204]	Loss 0.6123 (0.6091)	
training:	Epoch: [21][49/204]	Loss 0.6394 (0.6098)	
training:	Epoch: [21][50/204]	Loss 0.5263 (0.6081)	
training:	Epoch: [21][51/204]	Loss 0.6226 (0.6084)	
training:	Epoch: [21][52/204]	Loss 0.6050 (0.6083)	
training:	Epoch: [21][53/204]	Loss 0.5991 (0.6081)	
training:	Epoch: [21][54/204]	Loss 0.6081 (0.6081)	
training:	Epoch: [21][55/204]	Loss 0.5990 (0.6080)	
training:	Epoch: [21][56/204]	Loss 0.5434 (0.6068)	
training:	Epoch: [21][57/204]	Loss 0.5599 (0.6060)	
training:	Epoch: [21][58/204]	Loss 0.5739 (0.6054)	
training:	Epoch: [21][59/204]	Loss 0.6381 (0.6060)	
training:	Epoch: [21][60/204]	Loss 0.6857 (0.6073)	
training:	Epoch: [21][61/204]	Loss 0.5838 (0.6069)	
training:	Epoch: [21][62/204]	Loss 0.5078 (0.6053)	
training:	Epoch: [21][63/204]	Loss 0.6093 (0.6054)	
training:	Epoch: [21][64/204]	Loss 0.6568 (0.6062)	
training:	Epoch: [21][65/204]	Loss 0.5969 (0.6061)	
training:	Epoch: [21][66/204]	Loss 0.5464 (0.6051)	
training:	Epoch: [21][67/204]	Loss 0.5829 (0.6048)	
training:	Epoch: [21][68/204]	Loss 0.6962 (0.6062)	
training:	Epoch: [21][69/204]	Loss 0.6723 (0.6071)	
training:	Epoch: [21][70/204]	Loss 0.5622 (0.6065)	
training:	Epoch: [21][71/204]	Loss 0.4940 (0.6049)	
training:	Epoch: [21][72/204]	Loss 0.5931 (0.6047)	
training:	Epoch: [21][73/204]	Loss 0.6783 (0.6057)	
training:	Epoch: [21][74/204]	Loss 0.5653 (0.6052)	
training:	Epoch: [21][75/204]	Loss 0.6177 (0.6054)	
training:	Epoch: [21][76/204]	Loss 0.6185 (0.6055)	
training:	Epoch: [21][77/204]	Loss 0.5790 (0.6052)	
training:	Epoch: [21][78/204]	Loss 0.6464 (0.6057)	
training:	Epoch: [21][79/204]	Loss 0.6155 (0.6058)	
training:	Epoch: [21][80/204]	Loss 0.6878 (0.6069)	
training:	Epoch: [21][81/204]	Loss 0.5504 (0.6062)	
training:	Epoch: [21][82/204]	Loss 0.5355 (0.6053)	
training:	Epoch: [21][83/204]	Loss 0.4970 (0.6040)	
training:	Epoch: [21][84/204]	Loss 0.6104 (0.6041)	
training:	Epoch: [21][85/204]	Loss 0.6073 (0.6041)	
training:	Epoch: [21][86/204]	Loss 0.6528 (0.6047)	
training:	Epoch: [21][87/204]	Loss 0.5587 (0.6042)	
training:	Epoch: [21][88/204]	Loss 0.7295 (0.6056)	
training:	Epoch: [21][89/204]	Loss 0.6591 (0.6062)	
training:	Epoch: [21][90/204]	Loss 0.6594 (0.6068)	
training:	Epoch: [21][91/204]	Loss 0.6247 (0.6070)	
training:	Epoch: [21][92/204]	Loss 0.6183 (0.6071)	
training:	Epoch: [21][93/204]	Loss 0.6076 (0.6071)	
training:	Epoch: [21][94/204]	Loss 0.5766 (0.6068)	
training:	Epoch: [21][95/204]	Loss 0.6313 (0.6070)	
training:	Epoch: [21][96/204]	Loss 0.6496 (0.6075)	
training:	Epoch: [21][97/204]	Loss 0.6522 (0.6079)	
training:	Epoch: [21][98/204]	Loss 0.5982 (0.6078)	
training:	Epoch: [21][99/204]	Loss 0.5754 (0.6075)	
training:	Epoch: [21][100/204]	Loss 0.6261 (0.6077)	
training:	Epoch: [21][101/204]	Loss 0.6229 (0.6078)	
training:	Epoch: [21][102/204]	Loss 0.6754 (0.6085)	
training:	Epoch: [21][103/204]	Loss 0.5638 (0.6081)	
training:	Epoch: [21][104/204]	Loss 0.6642 (0.6086)	
training:	Epoch: [21][105/204]	Loss 0.6349 (0.6089)	
training:	Epoch: [21][106/204]	Loss 0.5400 (0.6082)	
training:	Epoch: [21][107/204]	Loss 0.5686 (0.6078)	
training:	Epoch: [21][108/204]	Loss 0.5465 (0.6073)	
training:	Epoch: [21][109/204]	Loss 0.6538 (0.6077)	
training:	Epoch: [21][110/204]	Loss 0.6240 (0.6078)	
training:	Epoch: [21][111/204]	Loss 0.5887 (0.6077)	
training:	Epoch: [21][112/204]	Loss 0.5510 (0.6072)	
training:	Epoch: [21][113/204]	Loss 0.5042 (0.6063)	
training:	Epoch: [21][114/204]	Loss 0.6228 (0.6064)	
training:	Epoch: [21][115/204]	Loss 0.5510 (0.6059)	
training:	Epoch: [21][116/204]	Loss 0.5614 (0.6055)	
training:	Epoch: [21][117/204]	Loss 0.5503 (0.6051)	
training:	Epoch: [21][118/204]	Loss 0.5281 (0.6044)	
training:	Epoch: [21][119/204]	Loss 0.6521 (0.6048)	
training:	Epoch: [21][120/204]	Loss 0.6010 (0.6048)	
training:	Epoch: [21][121/204]	Loss 0.6229 (0.6049)	
training:	Epoch: [21][122/204]	Loss 0.6290 (0.6051)	
training:	Epoch: [21][123/204]	Loss 0.5265 (0.6045)	
training:	Epoch: [21][124/204]	Loss 0.6249 (0.6047)	
training:	Epoch: [21][125/204]	Loss 0.6178 (0.6048)	
training:	Epoch: [21][126/204]	Loss 0.6674 (0.6053)	
training:	Epoch: [21][127/204]	Loss 0.6056 (0.6053)	
training:	Epoch: [21][128/204]	Loss 0.6011 (0.6052)	
training:	Epoch: [21][129/204]	Loss 0.5503 (0.6048)	
training:	Epoch: [21][130/204]	Loss 0.5522 (0.6044)	
training:	Epoch: [21][131/204]	Loss 0.5986 (0.6044)	
training:	Epoch: [21][132/204]	Loss 0.5129 (0.6037)	
training:	Epoch: [21][133/204]	Loss 0.7008 (0.6044)	
training:	Epoch: [21][134/204]	Loss 0.6263 (0.6046)	
training:	Epoch: [21][135/204]	Loss 0.6156 (0.6046)	
training:	Epoch: [21][136/204]	Loss 0.5683 (0.6044)	
training:	Epoch: [21][137/204]	Loss 0.6279 (0.6045)	
training:	Epoch: [21][138/204]	Loss 0.5441 (0.6041)	
training:	Epoch: [21][139/204]	Loss 0.6802 (0.6046)	
training:	Epoch: [21][140/204]	Loss 0.5900 (0.6045)	
training:	Epoch: [21][141/204]	Loss 0.6163 (0.6046)	
training:	Epoch: [21][142/204]	Loss 0.6393 (0.6049)	
training:	Epoch: [21][143/204]	Loss 0.5805 (0.6047)	
training:	Epoch: [21][144/204]	Loss 0.5917 (0.6046)	
training:	Epoch: [21][145/204]	Loss 0.5869 (0.6045)	
training:	Epoch: [21][146/204]	Loss 0.6649 (0.6049)	
training:	Epoch: [21][147/204]	Loss 0.6374 (0.6051)	
training:	Epoch: [21][148/204]	Loss 0.6200 (0.6052)	
training:	Epoch: [21][149/204]	Loss 0.5687 (0.6050)	
training:	Epoch: [21][150/204]	Loss 0.5787 (0.6048)	
training:	Epoch: [21][151/204]	Loss 0.6850 (0.6053)	
training:	Epoch: [21][152/204]	Loss 0.5362 (0.6049)	
training:	Epoch: [21][153/204]	Loss 0.5315 (0.6044)	
training:	Epoch: [21][154/204]	Loss 0.6229 (0.6045)	
training:	Epoch: [21][155/204]	Loss 0.6604 (0.6049)	
training:	Epoch: [21][156/204]	Loss 0.7063 (0.6055)	
training:	Epoch: [21][157/204]	Loss 0.5770 (0.6053)	
training:	Epoch: [21][158/204]	Loss 0.6912 (0.6059)	
training:	Epoch: [21][159/204]	Loss 0.5243 (0.6054)	
training:	Epoch: [21][160/204]	Loss 0.5927 (0.6053)	
training:	Epoch: [21][161/204]	Loss 0.5904 (0.6052)	
training:	Epoch: [21][162/204]	Loss 0.6152 (0.6053)	
training:	Epoch: [21][163/204]	Loss 0.5664 (0.6050)	
training:	Epoch: [21][164/204]	Loss 0.7139 (0.6057)	
training:	Epoch: [21][165/204]	Loss 0.4996 (0.6051)	
training:	Epoch: [21][166/204]	Loss 0.5929 (0.6050)	
training:	Epoch: [21][167/204]	Loss 0.5987 (0.6049)	
training:	Epoch: [21][168/204]	Loss 0.5882 (0.6048)	
training:	Epoch: [21][169/204]	Loss 0.6412 (0.6051)	
training:	Epoch: [21][170/204]	Loss 0.6201 (0.6051)	
training:	Epoch: [21][171/204]	Loss 0.6581 (0.6055)	
training:	Epoch: [21][172/204]	Loss 0.5877 (0.6054)	
training:	Epoch: [21][173/204]	Loss 0.6892 (0.6058)	
training:	Epoch: [21][174/204]	Loss 0.5967 (0.6058)	
training:	Epoch: [21][175/204]	Loss 0.5838 (0.6057)	
training:	Epoch: [21][176/204]	Loss 0.6023 (0.6056)	
training:	Epoch: [21][177/204]	Loss 0.5434 (0.6053)	
training:	Epoch: [21][178/204]	Loss 0.5360 (0.6049)	
training:	Epoch: [21][179/204]	Loss 0.6270 (0.6050)	
training:	Epoch: [21][180/204]	Loss 0.5434 (0.6047)	
training:	Epoch: [21][181/204]	Loss 0.6696 (0.6050)	
training:	Epoch: [21][182/204]	Loss 0.6339 (0.6052)	
training:	Epoch: [21][183/204]	Loss 0.5995 (0.6052)	
training:	Epoch: [21][184/204]	Loss 0.6968 (0.6057)	
training:	Epoch: [21][185/204]	Loss 0.5509 (0.6054)	
training:	Epoch: [21][186/204]	Loss 0.5892 (0.6053)	
training:	Epoch: [21][187/204]	Loss 0.5496 (0.6050)	
training:	Epoch: [21][188/204]	Loss 0.6402 (0.6052)	
training:	Epoch: [21][189/204]	Loss 0.6660 (0.6055)	
training:	Epoch: [21][190/204]	Loss 0.6646 (0.6058)	
training:	Epoch: [21][191/204]	Loss 0.5647 (0.6056)	
training:	Epoch: [21][192/204]	Loss 0.6892 (0.6060)	
training:	Epoch: [21][193/204]	Loss 0.6277 (0.6061)	
training:	Epoch: [21][194/204]	Loss 0.6108 (0.6062)	
training:	Epoch: [21][195/204]	Loss 0.6052 (0.6062)	
training:	Epoch: [21][196/204]	Loss 0.6253 (0.6063)	
training:	Epoch: [21][197/204]	Loss 0.5742 (0.6061)	
training:	Epoch: [21][198/204]	Loss 0.5540 (0.6058)	
training:	Epoch: [21][199/204]	Loss 0.6500 (0.6060)	
training:	Epoch: [21][200/204]	Loss 0.5472 (0.6058)	
training:	Epoch: [21][201/204]	Loss 0.7253 (0.6063)	
training:	Epoch: [21][202/204]	Loss 0.5895 (0.6063)	
training:	Epoch: [21][203/204]	Loss 0.5079 (0.6058)	
training:	Epoch: [21][204/204]	Loss 0.5851 (0.6057)	
Training:	 Loss: 0.6048

Training:	 ACC: 0.6890 0.6907 0.7313 0.6467
Validation:	 ACC: 0.6902 0.6929 0.7482 0.6323
Validation:	 Best_BACC: 0.6902 0.6929 0.7482 0.6323
Validation:	 Loss: 0.6019
Pretraining:	Epoch 22/120
----------
training:	Epoch: [22][1/204]	Loss 0.5270 (0.5270)	
training:	Epoch: [22][2/204]	Loss 0.6254 (0.5762)	
training:	Epoch: [22][3/204]	Loss 0.5724 (0.5749)	
training:	Epoch: [22][4/204]	Loss 0.6483 (0.5933)	
training:	Epoch: [22][5/204]	Loss 0.5377 (0.5821)	
training:	Epoch: [22][6/204]	Loss 0.5683 (0.5798)	
training:	Epoch: [22][7/204]	Loss 0.6229 (0.5860)	
training:	Epoch: [22][8/204]	Loss 0.5955 (0.5872)	
training:	Epoch: [22][9/204]	Loss 0.6104 (0.5898)	
training:	Epoch: [22][10/204]	Loss 0.5866 (0.5894)	
training:	Epoch: [22][11/204]	Loss 0.6313 (0.5932)	
training:	Epoch: [22][12/204]	Loss 0.6531 (0.5982)	
training:	Epoch: [22][13/204]	Loss 0.5242 (0.5925)	
training:	Epoch: [22][14/204]	Loss 0.5672 (0.5907)	
training:	Epoch: [22][15/204]	Loss 0.5357 (0.5871)	
training:	Epoch: [22][16/204]	Loss 0.6098 (0.5885)	
training:	Epoch: [22][17/204]	Loss 0.6052 (0.5895)	
training:	Epoch: [22][18/204]	Loss 0.5300 (0.5862)	
training:	Epoch: [22][19/204]	Loss 0.6085 (0.5873)	
training:	Epoch: [22][20/204]	Loss 0.5456 (0.5852)	
training:	Epoch: [22][21/204]	Loss 0.5868 (0.5853)	
training:	Epoch: [22][22/204]	Loss 0.5896 (0.5855)	
training:	Epoch: [22][23/204]	Loss 0.5265 (0.5829)	
training:	Epoch: [22][24/204]	Loss 0.5906 (0.5833)	
training:	Epoch: [22][25/204]	Loss 0.6207 (0.5848)	
training:	Epoch: [22][26/204]	Loss 0.5735 (0.5843)	
training:	Epoch: [22][27/204]	Loss 0.5983 (0.5848)	
training:	Epoch: [22][28/204]	Loss 0.5714 (0.5844)	
training:	Epoch: [22][29/204]	Loss 0.6412 (0.5863)	
training:	Epoch: [22][30/204]	Loss 0.5690 (0.5857)	
training:	Epoch: [22][31/204]	Loss 0.6157 (0.5867)	
training:	Epoch: [22][32/204]	Loss 0.5842 (0.5866)	
training:	Epoch: [22][33/204]	Loss 0.5922 (0.5868)	
training:	Epoch: [22][34/204]	Loss 0.5990 (0.5872)	
training:	Epoch: [22][35/204]	Loss 0.5393 (0.5858)	
training:	Epoch: [22][36/204]	Loss 0.6300 (0.5870)	
training:	Epoch: [22][37/204]	Loss 0.7010 (0.5901)	
training:	Epoch: [22][38/204]	Loss 0.6298 (0.5912)	
training:	Epoch: [22][39/204]	Loss 0.6085 (0.5916)	
training:	Epoch: [22][40/204]	Loss 0.6099 (0.5921)	
training:	Epoch: [22][41/204]	Loss 0.6935 (0.5945)	
training:	Epoch: [22][42/204]	Loss 0.6259 (0.5953)	
training:	Epoch: [22][43/204]	Loss 0.5722 (0.5947)	
training:	Epoch: [22][44/204]	Loss 0.5477 (0.5937)	
training:	Epoch: [22][45/204]	Loss 0.6737 (0.5954)	
training:	Epoch: [22][46/204]	Loss 0.6294 (0.5962)	
training:	Epoch: [22][47/204]	Loss 0.5596 (0.5954)	
training:	Epoch: [22][48/204]	Loss 0.6402 (0.5963)	
training:	Epoch: [22][49/204]	Loss 0.6008 (0.5964)	
training:	Epoch: [22][50/204]	Loss 0.6314 (0.5971)	
training:	Epoch: [22][51/204]	Loss 0.6229 (0.5976)	
training:	Epoch: [22][52/204]	Loss 0.6205 (0.5981)	
training:	Epoch: [22][53/204]	Loss 0.5765 (0.5977)	
training:	Epoch: [22][54/204]	Loss 0.6084 (0.5979)	
training:	Epoch: [22][55/204]	Loss 0.6365 (0.5986)	
training:	Epoch: [22][56/204]	Loss 0.5413 (0.5975)	
training:	Epoch: [22][57/204]	Loss 0.5391 (0.5965)	
training:	Epoch: [22][58/204]	Loss 0.5915 (0.5964)	
training:	Epoch: [22][59/204]	Loss 0.6059 (0.5966)	
training:	Epoch: [22][60/204]	Loss 0.5891 (0.5965)	
training:	Epoch: [22][61/204]	Loss 0.5897 (0.5964)	
training:	Epoch: [22][62/204]	Loss 0.5437 (0.5955)	
training:	Epoch: [22][63/204]	Loss 0.6187 (0.5959)	
training:	Epoch: [22][64/204]	Loss 0.5848 (0.5957)	
training:	Epoch: [22][65/204]	Loss 0.6364 (0.5963)	
training:	Epoch: [22][66/204]	Loss 0.6210 (0.5967)	
training:	Epoch: [22][67/204]	Loss 0.5881 (0.5966)	
training:	Epoch: [22][68/204]	Loss 0.6337 (0.5971)	
training:	Epoch: [22][69/204]	Loss 0.6508 (0.5979)	
training:	Epoch: [22][70/204]	Loss 0.6249 (0.5983)	
training:	Epoch: [22][71/204]	Loss 0.6279 (0.5987)	
training:	Epoch: [22][72/204]	Loss 0.6597 (0.5995)	
training:	Epoch: [22][73/204]	Loss 0.6266 (0.5999)	
training:	Epoch: [22][74/204]	Loss 0.5575 (0.5993)	
training:	Epoch: [22][75/204]	Loss 0.5796 (0.5991)	
training:	Epoch: [22][76/204]	Loss 0.6709 (0.6000)	
training:	Epoch: [22][77/204]	Loss 0.5532 (0.5994)	
training:	Epoch: [22][78/204]	Loss 0.6002 (0.5994)	
training:	Epoch: [22][79/204]	Loss 0.5943 (0.5994)	
training:	Epoch: [22][80/204]	Loss 0.5501 (0.5987)	
training:	Epoch: [22][81/204]	Loss 0.6510 (0.5994)	
training:	Epoch: [22][82/204]	Loss 0.6373 (0.5999)	
training:	Epoch: [22][83/204]	Loss 0.6436 (0.6004)	
training:	Epoch: [22][84/204]	Loss 0.5888 (0.6002)	
training:	Epoch: [22][85/204]	Loss 0.6175 (0.6004)	
training:	Epoch: [22][86/204]	Loss 0.5328 (0.5997)	
training:	Epoch: [22][87/204]	Loss 0.6402 (0.6001)	
training:	Epoch: [22][88/204]	Loss 0.5522 (0.5996)	
training:	Epoch: [22][89/204]	Loss 0.6345 (0.6000)	
training:	Epoch: [22][90/204]	Loss 0.6012 (0.6000)	
training:	Epoch: [22][91/204]	Loss 0.4887 (0.5988)	
training:	Epoch: [22][92/204]	Loss 0.6588 (0.5994)	
training:	Epoch: [22][93/204]	Loss 0.5481 (0.5989)	
training:	Epoch: [22][94/204]	Loss 0.6996 (0.5999)	
training:	Epoch: [22][95/204]	Loss 0.5697 (0.5996)	
training:	Epoch: [22][96/204]	Loss 0.5002 (0.5986)	
training:	Epoch: [22][97/204]	Loss 0.5572 (0.5982)	
training:	Epoch: [22][98/204]	Loss 0.5613 (0.5978)	
training:	Epoch: [22][99/204]	Loss 0.7111 (0.5989)	
training:	Epoch: [22][100/204]	Loss 0.5916 (0.5989)	
training:	Epoch: [22][101/204]	Loss 0.6127 (0.5990)	
training:	Epoch: [22][102/204]	Loss 0.6520 (0.5995)	
training:	Epoch: [22][103/204]	Loss 0.6362 (0.5999)	
training:	Epoch: [22][104/204]	Loss 0.6467 (0.6003)	
training:	Epoch: [22][105/204]	Loss 0.6039 (0.6003)	
training:	Epoch: [22][106/204]	Loss 0.6497 (0.6008)	
training:	Epoch: [22][107/204]	Loss 0.5984 (0.6008)	
training:	Epoch: [22][108/204]	Loss 0.5209 (0.6001)	
training:	Epoch: [22][109/204]	Loss 0.5319 (0.5994)	
training:	Epoch: [22][110/204]	Loss 0.5475 (0.5990)	
training:	Epoch: [22][111/204]	Loss 0.5627 (0.5986)	
training:	Epoch: [22][112/204]	Loss 0.6630 (0.5992)	
training:	Epoch: [22][113/204]	Loss 0.5883 (0.5991)	
training:	Epoch: [22][114/204]	Loss 0.5925 (0.5990)	
training:	Epoch: [22][115/204]	Loss 0.5813 (0.5989)	
training:	Epoch: [22][116/204]	Loss 0.5816 (0.5987)	
training:	Epoch: [22][117/204]	Loss 0.6472 (0.5992)	
training:	Epoch: [22][118/204]	Loss 0.6555 (0.5996)	
training:	Epoch: [22][119/204]	Loss 0.6803 (0.6003)	
training:	Epoch: [22][120/204]	Loss 0.5867 (0.6002)	
training:	Epoch: [22][121/204]	Loss 0.5073 (0.5994)	
training:	Epoch: [22][122/204]	Loss 0.6652 (0.6000)	
training:	Epoch: [22][123/204]	Loss 0.6130 (0.6001)	
training:	Epoch: [22][124/204]	Loss 0.6207 (0.6002)	
training:	Epoch: [22][125/204]	Loss 0.5223 (0.5996)	
training:	Epoch: [22][126/204]	Loss 0.5758 (0.5994)	
training:	Epoch: [22][127/204]	Loss 0.6154 (0.5996)	
training:	Epoch: [22][128/204]	Loss 0.6421 (0.5999)	
training:	Epoch: [22][129/204]	Loss 0.5702 (0.5997)	
training:	Epoch: [22][130/204]	Loss 0.5889 (0.5996)	
training:	Epoch: [22][131/204]	Loss 0.5530 (0.5992)	
training:	Epoch: [22][132/204]	Loss 0.5283 (0.5987)	
training:	Epoch: [22][133/204]	Loss 0.6079 (0.5988)	
training:	Epoch: [22][134/204]	Loss 0.6008 (0.5988)	
training:	Epoch: [22][135/204]	Loss 0.6095 (0.5988)	
training:	Epoch: [22][136/204]	Loss 0.6098 (0.5989)	
training:	Epoch: [22][137/204]	Loss 0.6892 (0.5996)	
training:	Epoch: [22][138/204]	Loss 0.5774 (0.5994)	
training:	Epoch: [22][139/204]	Loss 0.5485 (0.5991)	
training:	Epoch: [22][140/204]	Loss 0.6784 (0.5996)	
training:	Epoch: [22][141/204]	Loss 0.5365 (0.5992)	
training:	Epoch: [22][142/204]	Loss 0.5495 (0.5988)	
training:	Epoch: [22][143/204]	Loss 0.5579 (0.5985)	
training:	Epoch: [22][144/204]	Loss 0.6285 (0.5988)	
training:	Epoch: [22][145/204]	Loss 0.6505 (0.5991)	
training:	Epoch: [22][146/204]	Loss 0.7492 (0.6001)	
training:	Epoch: [22][147/204]	Loss 0.6169 (0.6002)	
training:	Epoch: [22][148/204]	Loss 0.6026 (0.6003)	
training:	Epoch: [22][149/204]	Loss 0.5807 (0.6001)	
training:	Epoch: [22][150/204]	Loss 0.5780 (0.6000)	
training:	Epoch: [22][151/204]	Loss 0.5698 (0.5998)	
training:	Epoch: [22][152/204]	Loss 0.5557 (0.5995)	
training:	Epoch: [22][153/204]	Loss 0.5672 (0.5993)	
training:	Epoch: [22][154/204]	Loss 0.6850 (0.5998)	
training:	Epoch: [22][155/204]	Loss 0.5627 (0.5996)	
training:	Epoch: [22][156/204]	Loss 0.6111 (0.5997)	
training:	Epoch: [22][157/204]	Loss 0.5812 (0.5996)	
training:	Epoch: [22][158/204]	Loss 0.6262 (0.5997)	
training:	Epoch: [22][159/204]	Loss 0.5443 (0.5994)	
training:	Epoch: [22][160/204]	Loss 0.5971 (0.5994)	
training:	Epoch: [22][161/204]	Loss 0.7214 (0.6001)	
training:	Epoch: [22][162/204]	Loss 0.5613 (0.5999)	
training:	Epoch: [22][163/204]	Loss 0.5791 (0.5998)	
training:	Epoch: [22][164/204]	Loss 0.6312 (0.5999)	
training:	Epoch: [22][165/204]	Loss 0.6369 (0.6002)	
training:	Epoch: [22][166/204]	Loss 0.5775 (0.6000)	
training:	Epoch: [22][167/204]	Loss 0.5811 (0.5999)	
training:	Epoch: [22][168/204]	Loss 0.5676 (0.5997)	
training:	Epoch: [22][169/204]	Loss 0.6405 (0.6000)	
training:	Epoch: [22][170/204]	Loss 0.5903 (0.5999)	
training:	Epoch: [22][171/204]	Loss 0.6556 (0.6002)	
training:	Epoch: [22][172/204]	Loss 0.6002 (0.6002)	
training:	Epoch: [22][173/204]	Loss 0.6226 (0.6004)	
training:	Epoch: [22][174/204]	Loss 0.5929 (0.6003)	
training:	Epoch: [22][175/204]	Loss 0.5760 (0.6002)	
training:	Epoch: [22][176/204]	Loss 0.6623 (0.6005)	
training:	Epoch: [22][177/204]	Loss 0.7646 (0.6015)	
training:	Epoch: [22][178/204]	Loss 0.6616 (0.6018)	
training:	Epoch: [22][179/204]	Loss 0.5913 (0.6017)	
training:	Epoch: [22][180/204]	Loss 0.5018 (0.6012)	
training:	Epoch: [22][181/204]	Loss 0.5915 (0.6011)	
training:	Epoch: [22][182/204]	Loss 0.6139 (0.6012)	
training:	Epoch: [22][183/204]	Loss 0.6354 (0.6014)	
training:	Epoch: [22][184/204]	Loss 0.5597 (0.6012)	
training:	Epoch: [22][185/204]	Loss 0.6214 (0.6013)	
training:	Epoch: [22][186/204]	Loss 0.5973 (0.6013)	
training:	Epoch: [22][187/204]	Loss 0.6462 (0.6015)	
training:	Epoch: [22][188/204]	Loss 0.5672 (0.6013)	
training:	Epoch: [22][189/204]	Loss 0.6049 (0.6013)	
training:	Epoch: [22][190/204]	Loss 0.6594 (0.6016)	
training:	Epoch: [22][191/204]	Loss 0.5783 (0.6015)	
training:	Epoch: [22][192/204]	Loss 0.6029 (0.6015)	
training:	Epoch: [22][193/204]	Loss 0.6356 (0.6017)	
training:	Epoch: [22][194/204]	Loss 0.5514 (0.6014)	
training:	Epoch: [22][195/204]	Loss 0.5665 (0.6013)	
training:	Epoch: [22][196/204]	Loss 0.5649 (0.6011)	
training:	Epoch: [22][197/204]	Loss 0.6236 (0.6012)	
training:	Epoch: [22][198/204]	Loss 0.6093 (0.6012)	
training:	Epoch: [22][199/204]	Loss 0.6252 (0.6014)	
training:	Epoch: [22][200/204]	Loss 0.6402 (0.6015)	
training:	Epoch: [22][201/204]	Loss 0.5428 (0.6013)	
training:	Epoch: [22][202/204]	Loss 0.5988 (0.6012)	
training:	Epoch: [22][203/204]	Loss 0.5848 (0.6012)	
training:	Epoch: [22][204/204]	Loss 0.5421 (0.6009)	
Training:	 Loss: 0.6000

Training:	 ACC: 0.6940 0.6955 0.7307 0.6572
Validation:	 ACC: 0.6927 0.6950 0.7431 0.6424
Validation:	 Best_BACC: 0.6927 0.6950 0.7431 0.6424
Validation:	 Loss: 0.5963
Pretraining:	Epoch 23/120
----------
training:	Epoch: [23][1/204]	Loss 0.7253 (0.7253)	
training:	Epoch: [23][2/204]	Loss 0.6326 (0.6790)	
training:	Epoch: [23][3/204]	Loss 0.5728 (0.6436)	
training:	Epoch: [23][4/204]	Loss 0.6434 (0.6435)	
training:	Epoch: [23][5/204]	Loss 0.5520 (0.6252)	
training:	Epoch: [23][6/204]	Loss 0.6722 (0.6330)	
training:	Epoch: [23][7/204]	Loss 0.5627 (0.6230)	
training:	Epoch: [23][8/204]	Loss 0.6752 (0.6295)	
training:	Epoch: [23][9/204]	Loss 0.6351 (0.6301)	
training:	Epoch: [23][10/204]	Loss 0.5103 (0.6182)	
training:	Epoch: [23][11/204]	Loss 0.5475 (0.6117)	
training:	Epoch: [23][12/204]	Loss 0.5850 (0.6095)	
training:	Epoch: [23][13/204]	Loss 0.5480 (0.6048)	
training:	Epoch: [23][14/204]	Loss 0.5431 (0.6004)	
training:	Epoch: [23][15/204]	Loss 0.5306 (0.5957)	
training:	Epoch: [23][16/204]	Loss 0.5563 (0.5933)	
training:	Epoch: [23][17/204]	Loss 0.6554 (0.5969)	
training:	Epoch: [23][18/204]	Loss 0.5499 (0.5943)	
training:	Epoch: [23][19/204]	Loss 0.5908 (0.5941)	
training:	Epoch: [23][20/204]	Loss 0.6118 (0.5950)	
training:	Epoch: [23][21/204]	Loss 0.5929 (0.5949)	
training:	Epoch: [23][22/204]	Loss 0.5675 (0.5937)	
training:	Epoch: [23][23/204]	Loss 0.5474 (0.5916)	
training:	Epoch: [23][24/204]	Loss 0.5794 (0.5911)	
training:	Epoch: [23][25/204]	Loss 0.5091 (0.5879)	
training:	Epoch: [23][26/204]	Loss 0.5339 (0.5858)	
training:	Epoch: [23][27/204]	Loss 0.6243 (0.5872)	
training:	Epoch: [23][28/204]	Loss 0.6720 (0.5902)	
training:	Epoch: [23][29/204]	Loss 0.5275 (0.5881)	
training:	Epoch: [23][30/204]	Loss 0.6031 (0.5886)	
training:	Epoch: [23][31/204]	Loss 0.5657 (0.5878)	
training:	Epoch: [23][32/204]	Loss 0.6601 (0.5901)	
training:	Epoch: [23][33/204]	Loss 0.6244 (0.5911)	
training:	Epoch: [23][34/204]	Loss 0.6049 (0.5915)	
training:	Epoch: [23][35/204]	Loss 0.5652 (0.5908)	
training:	Epoch: [23][36/204]	Loss 0.4842 (0.5878)	
training:	Epoch: [23][37/204]	Loss 0.5427 (0.5866)	
training:	Epoch: [23][38/204]	Loss 0.6323 (0.5878)	
training:	Epoch: [23][39/204]	Loss 0.6239 (0.5887)	
training:	Epoch: [23][40/204]	Loss 0.5534 (0.5878)	
training:	Epoch: [23][41/204]	Loss 0.5891 (0.5879)	
training:	Epoch: [23][42/204]	Loss 0.5615 (0.5872)	
training:	Epoch: [23][43/204]	Loss 0.5488 (0.5864)	
training:	Epoch: [23][44/204]	Loss 0.6684 (0.5882)	
training:	Epoch: [23][45/204]	Loss 0.6415 (0.5894)	
training:	Epoch: [23][46/204]	Loss 0.6755 (0.5913)	
training:	Epoch: [23][47/204]	Loss 0.5935 (0.5913)	
training:	Epoch: [23][48/204]	Loss 0.6960 (0.5935)	
training:	Epoch: [23][49/204]	Loss 0.5274 (0.5922)	
training:	Epoch: [23][50/204]	Loss 0.5683 (0.5917)	
training:	Epoch: [23][51/204]	Loss 0.6327 (0.5925)	
training:	Epoch: [23][52/204]	Loss 0.6644 (0.5939)	
training:	Epoch: [23][53/204]	Loss 0.5336 (0.5927)	
training:	Epoch: [23][54/204]	Loss 0.6592 (0.5940)	
training:	Epoch: [23][55/204]	Loss 0.6752 (0.5954)	
training:	Epoch: [23][56/204]	Loss 0.5648 (0.5949)	
training:	Epoch: [23][57/204]	Loss 0.5960 (0.5949)	
training:	Epoch: [23][58/204]	Loss 0.6497 (0.5959)	
training:	Epoch: [23][59/204]	Loss 0.6629 (0.5970)	
training:	Epoch: [23][60/204]	Loss 0.5601 (0.5964)	
training:	Epoch: [23][61/204]	Loss 0.6658 (0.5975)	
training:	Epoch: [23][62/204]	Loss 0.6495 (0.5983)	
training:	Epoch: [23][63/204]	Loss 0.5293 (0.5973)	
training:	Epoch: [23][64/204]	Loss 0.5059 (0.5958)	
training:	Epoch: [23][65/204]	Loss 0.6171 (0.5962)	
training:	Epoch: [23][66/204]	Loss 0.6128 (0.5964)	
training:	Epoch: [23][67/204]	Loss 0.4858 (0.5948)	
training:	Epoch: [23][68/204]	Loss 0.6040 (0.5949)	
training:	Epoch: [23][69/204]	Loss 0.5697 (0.5945)	
training:	Epoch: [23][70/204]	Loss 0.6762 (0.5957)	
training:	Epoch: [23][71/204]	Loss 0.6477 (0.5964)	
training:	Epoch: [23][72/204]	Loss 0.6555 (0.5972)	
training:	Epoch: [23][73/204]	Loss 0.5226 (0.5962)	
training:	Epoch: [23][74/204]	Loss 0.5828 (0.5960)	
training:	Epoch: [23][75/204]	Loss 0.5751 (0.5958)	
training:	Epoch: [23][76/204]	Loss 0.5755 (0.5955)	
training:	Epoch: [23][77/204]	Loss 0.5611 (0.5950)	
training:	Epoch: [23][78/204]	Loss 0.5859 (0.5949)	
training:	Epoch: [23][79/204]	Loss 0.5621 (0.5945)	
training:	Epoch: [23][80/204]	Loss 0.5889 (0.5944)	
training:	Epoch: [23][81/204]	Loss 0.6157 (0.5947)	
training:	Epoch: [23][82/204]	Loss 0.5680 (0.5944)	
training:	Epoch: [23][83/204]	Loss 0.6039 (0.5945)	
training:	Epoch: [23][84/204]	Loss 0.6152 (0.5947)	
training:	Epoch: [23][85/204]	Loss 0.6046 (0.5949)	
training:	Epoch: [23][86/204]	Loss 0.5195 (0.5940)	
training:	Epoch: [23][87/204]	Loss 0.6845 (0.5950)	
training:	Epoch: [23][88/204]	Loss 0.6526 (0.5957)	
training:	Epoch: [23][89/204]	Loss 0.5410 (0.5951)	
training:	Epoch: [23][90/204]	Loss 0.5658 (0.5947)	
training:	Epoch: [23][91/204]	Loss 0.5811 (0.5946)	
training:	Epoch: [23][92/204]	Loss 0.5731 (0.5944)	
training:	Epoch: [23][93/204]	Loss 0.6751 (0.5952)	
training:	Epoch: [23][94/204]	Loss 0.6097 (0.5954)	
training:	Epoch: [23][95/204]	Loss 0.6848 (0.5963)	
training:	Epoch: [23][96/204]	Loss 0.5538 (0.5959)	
training:	Epoch: [23][97/204]	Loss 0.7493 (0.5975)	
training:	Epoch: [23][98/204]	Loss 0.6083 (0.5976)	
training:	Epoch: [23][99/204]	Loss 0.5740 (0.5973)	
training:	Epoch: [23][100/204]	Loss 0.6181 (0.5975)	
training:	Epoch: [23][101/204]	Loss 0.6479 (0.5980)	
training:	Epoch: [23][102/204]	Loss 0.6640 (0.5987)	
training:	Epoch: [23][103/204]	Loss 0.5546 (0.5983)	
training:	Epoch: [23][104/204]	Loss 0.6333 (0.5986)	
training:	Epoch: [23][105/204]	Loss 0.6386 (0.5990)	
training:	Epoch: [23][106/204]	Loss 0.4733 (0.5978)	
training:	Epoch: [23][107/204]	Loss 0.6145 (0.5979)	
training:	Epoch: [23][108/204]	Loss 0.5525 (0.5975)	
training:	Epoch: [23][109/204]	Loss 0.6329 (0.5979)	
training:	Epoch: [23][110/204]	Loss 0.5827 (0.5977)	
training:	Epoch: [23][111/204]	Loss 0.6646 (0.5983)	
training:	Epoch: [23][112/204]	Loss 0.5349 (0.5977)	
training:	Epoch: [23][113/204]	Loss 0.6434 (0.5982)	
training:	Epoch: [23][114/204]	Loss 0.6248 (0.5984)	
training:	Epoch: [23][115/204]	Loss 0.6167 (0.5985)	
training:	Epoch: [23][116/204]	Loss 0.6122 (0.5987)	
training:	Epoch: [23][117/204]	Loss 0.6703 (0.5993)	
training:	Epoch: [23][118/204]	Loss 0.5285 (0.5987)	
training:	Epoch: [23][119/204]	Loss 0.6407 (0.5990)	
training:	Epoch: [23][120/204]	Loss 0.5747 (0.5988)	
training:	Epoch: [23][121/204]	Loss 0.6109 (0.5989)	
training:	Epoch: [23][122/204]	Loss 0.6489 (0.5993)	
training:	Epoch: [23][123/204]	Loss 0.6760 (0.6000)	
training:	Epoch: [23][124/204]	Loss 0.6687 (0.6005)	
training:	Epoch: [23][125/204]	Loss 0.5848 (0.6004)	
training:	Epoch: [23][126/204]	Loss 0.5699 (0.6001)	
training:	Epoch: [23][127/204]	Loss 0.5360 (0.5996)	
training:	Epoch: [23][128/204]	Loss 0.5769 (0.5995)	
training:	Epoch: [23][129/204]	Loss 0.5298 (0.5989)	
training:	Epoch: [23][130/204]	Loss 0.5821 (0.5988)	
training:	Epoch: [23][131/204]	Loss 0.5471 (0.5984)	
training:	Epoch: [23][132/204]	Loss 0.6051 (0.5984)	
training:	Epoch: [23][133/204]	Loss 0.4713 (0.5975)	
training:	Epoch: [23][134/204]	Loss 0.5687 (0.5973)	
training:	Epoch: [23][135/204]	Loss 0.5628 (0.5970)	
training:	Epoch: [23][136/204]	Loss 0.6449 (0.5974)	
training:	Epoch: [23][137/204]	Loss 0.5044 (0.5967)	
training:	Epoch: [23][138/204]	Loss 0.6001 (0.5967)	
training:	Epoch: [23][139/204]	Loss 0.6045 (0.5968)	
training:	Epoch: [23][140/204]	Loss 0.5105 (0.5962)	
training:	Epoch: [23][141/204]	Loss 0.6032 (0.5962)	
training:	Epoch: [23][142/204]	Loss 0.6801 (0.5968)	
training:	Epoch: [23][143/204]	Loss 0.5922 (0.5968)	
training:	Epoch: [23][144/204]	Loss 0.5456 (0.5964)	
training:	Epoch: [23][145/204]	Loss 0.4991 (0.5957)	
training:	Epoch: [23][146/204]	Loss 0.5847 (0.5957)	
training:	Epoch: [23][147/204]	Loss 0.5803 (0.5956)	
training:	Epoch: [23][148/204]	Loss 0.5089 (0.5950)	
training:	Epoch: [23][149/204]	Loss 0.6076 (0.5951)	
training:	Epoch: [23][150/204]	Loss 0.6997 (0.5958)	
training:	Epoch: [23][151/204]	Loss 0.5269 (0.5953)	
training:	Epoch: [23][152/204]	Loss 0.6515 (0.5957)	
training:	Epoch: [23][153/204]	Loss 0.5585 (0.5954)	
training:	Epoch: [23][154/204]	Loss 0.5404 (0.5951)	
training:	Epoch: [23][155/204]	Loss 0.6385 (0.5954)	
training:	Epoch: [23][156/204]	Loss 0.6755 (0.5959)	
training:	Epoch: [23][157/204]	Loss 0.5548 (0.5956)	
training:	Epoch: [23][158/204]	Loss 0.5033 (0.5950)	
training:	Epoch: [23][159/204]	Loss 0.5849 (0.5950)	
training:	Epoch: [23][160/204]	Loss 0.6197 (0.5951)	
training:	Epoch: [23][161/204]	Loss 0.5962 (0.5951)	
training:	Epoch: [23][162/204]	Loss 0.6631 (0.5955)	
training:	Epoch: [23][163/204]	Loss 0.5861 (0.5955)	
training:	Epoch: [23][164/204]	Loss 0.5401 (0.5951)	
training:	Epoch: [23][165/204]	Loss 0.6410 (0.5954)	
training:	Epoch: [23][166/204]	Loss 0.6315 (0.5956)	
training:	Epoch: [23][167/204]	Loss 0.6326 (0.5959)	
training:	Epoch: [23][168/204]	Loss 0.5272 (0.5954)	
training:	Epoch: [23][169/204]	Loss 0.6222 (0.5956)	
training:	Epoch: [23][170/204]	Loss 0.6349 (0.5958)	
training:	Epoch: [23][171/204]	Loss 0.5450 (0.5955)	
training:	Epoch: [23][172/204]	Loss 0.6047 (0.5956)	
training:	Epoch: [23][173/204]	Loss 0.5404 (0.5953)	
training:	Epoch: [23][174/204]	Loss 0.5464 (0.5950)	
training:	Epoch: [23][175/204]	Loss 0.5578 (0.5948)	
training:	Epoch: [23][176/204]	Loss 0.5936 (0.5948)	
training:	Epoch: [23][177/204]	Loss 0.6194 (0.5949)	
training:	Epoch: [23][178/204]	Loss 0.6385 (0.5952)	
training:	Epoch: [23][179/204]	Loss 0.5388 (0.5948)	
training:	Epoch: [23][180/204]	Loss 0.5219 (0.5944)	
training:	Epoch: [23][181/204]	Loss 0.6377 (0.5947)	
training:	Epoch: [23][182/204]	Loss 0.5978 (0.5947)	
training:	Epoch: [23][183/204]	Loss 0.6227 (0.5948)	
training:	Epoch: [23][184/204]	Loss 0.6351 (0.5951)	
training:	Epoch: [23][185/204]	Loss 0.5513 (0.5948)	
training:	Epoch: [23][186/204]	Loss 0.5854 (0.5948)	
training:	Epoch: [23][187/204]	Loss 0.6409 (0.5950)	
training:	Epoch: [23][188/204]	Loss 0.6372 (0.5953)	
training:	Epoch: [23][189/204]	Loss 0.6454 (0.5955)	
training:	Epoch: [23][190/204]	Loss 0.5883 (0.5955)	
training:	Epoch: [23][191/204]	Loss 0.5980 (0.5955)	
training:	Epoch: [23][192/204]	Loss 0.4898 (0.5949)	
training:	Epoch: [23][193/204]	Loss 0.5259 (0.5946)	
training:	Epoch: [23][194/204]	Loss 0.5695 (0.5945)	
training:	Epoch: [23][195/204]	Loss 0.6061 (0.5945)	
training:	Epoch: [23][196/204]	Loss 0.7259 (0.5952)	
training:	Epoch: [23][197/204]	Loss 0.5372 (0.5949)	
training:	Epoch: [23][198/204]	Loss 0.6093 (0.5950)	
training:	Epoch: [23][199/204]	Loss 0.6182 (0.5951)	
training:	Epoch: [23][200/204]	Loss 0.5372 (0.5948)	
training:	Epoch: [23][201/204]	Loss 0.5884 (0.5948)	
training:	Epoch: [23][202/204]	Loss 0.6127 (0.5948)	
training:	Epoch: [23][203/204]	Loss 0.6089 (0.5949)	
training:	Epoch: [23][204/204]	Loss 0.7034 (0.5954)	
Training:	 Loss: 0.5945

Training:	 ACC: 0.6982 0.6996 0.7325 0.6639
Validation:	 ACC: 0.6954 0.6977 0.7451 0.6457
Validation:	 Best_BACC: 0.6954 0.6977 0.7451 0.6457
Validation:	 Loss: 0.5918
Pretraining:	Epoch 24/120
----------
training:	Epoch: [24][1/204]	Loss 0.6771 (0.6771)	
training:	Epoch: [24][2/204]	Loss 0.5894 (0.6333)	
training:	Epoch: [24][3/204]	Loss 0.6398 (0.6355)	
training:	Epoch: [24][4/204]	Loss 0.5414 (0.6119)	
training:	Epoch: [24][5/204]	Loss 0.5257 (0.5947)	
training:	Epoch: [24][6/204]	Loss 0.6002 (0.5956)	
training:	Epoch: [24][7/204]	Loss 0.6107 (0.5978)	
training:	Epoch: [24][8/204]	Loss 0.6013 (0.5982)	
training:	Epoch: [24][9/204]	Loss 0.6595 (0.6050)	
training:	Epoch: [24][10/204]	Loss 0.6123 (0.6057)	
training:	Epoch: [24][11/204]	Loss 0.5875 (0.6041)	
training:	Epoch: [24][12/204]	Loss 0.5696 (0.6012)	
training:	Epoch: [24][13/204]	Loss 0.5507 (0.5973)	
training:	Epoch: [24][14/204]	Loss 0.6400 (0.6004)	
training:	Epoch: [24][15/204]	Loss 0.6501 (0.6037)	
training:	Epoch: [24][16/204]	Loss 0.5498 (0.6003)	
training:	Epoch: [24][17/204]	Loss 0.5869 (0.5995)	
training:	Epoch: [24][18/204]	Loss 0.5769 (0.5983)	
training:	Epoch: [24][19/204]	Loss 0.5972 (0.5982)	
training:	Epoch: [24][20/204]	Loss 0.6679 (0.6017)	
training:	Epoch: [24][21/204]	Loss 0.5663 (0.6000)	
training:	Epoch: [24][22/204]	Loss 0.7788 (0.6081)	
training:	Epoch: [24][23/204]	Loss 0.6060 (0.6080)	
training:	Epoch: [24][24/204]	Loss 0.4812 (0.6028)	
training:	Epoch: [24][25/204]	Loss 0.5964 (0.6025)	
training:	Epoch: [24][26/204]	Loss 0.6578 (0.6046)	
training:	Epoch: [24][27/204]	Loss 0.5678 (0.6033)	
training:	Epoch: [24][28/204]	Loss 0.6301 (0.6042)	
training:	Epoch: [24][29/204]	Loss 0.5333 (0.6018)	
training:	Epoch: [24][30/204]	Loss 0.5977 (0.6016)	
training:	Epoch: [24][31/204]	Loss 0.5576 (0.6002)	
training:	Epoch: [24][32/204]	Loss 0.5265 (0.5979)	
training:	Epoch: [24][33/204]	Loss 0.5038 (0.5951)	
training:	Epoch: [24][34/204]	Loss 0.7404 (0.5993)	
training:	Epoch: [24][35/204]	Loss 0.5206 (0.5971)	
training:	Epoch: [24][36/204]	Loss 0.5524 (0.5959)	
training:	Epoch: [24][37/204]	Loss 0.5840 (0.5955)	
training:	Epoch: [24][38/204]	Loss 0.5178 (0.5935)	
training:	Epoch: [24][39/204]	Loss 0.6555 (0.5951)	
training:	Epoch: [24][40/204]	Loss 0.7488 (0.5989)	
training:	Epoch: [24][41/204]	Loss 0.6176 (0.5994)	
training:	Epoch: [24][42/204]	Loss 0.5995 (0.5994)	
training:	Epoch: [24][43/204]	Loss 0.5144 (0.5974)	
training:	Epoch: [24][44/204]	Loss 0.6166 (0.5978)	
training:	Epoch: [24][45/204]	Loss 0.5418 (0.5966)	
training:	Epoch: [24][46/204]	Loss 0.6120 (0.5969)	
training:	Epoch: [24][47/204]	Loss 0.5483 (0.5959)	
training:	Epoch: [24][48/204]	Loss 0.6077 (0.5961)	
training:	Epoch: [24][49/204]	Loss 0.6354 (0.5969)	
training:	Epoch: [24][50/204]	Loss 0.5374 (0.5958)	
training:	Epoch: [24][51/204]	Loss 0.5386 (0.5946)	
training:	Epoch: [24][52/204]	Loss 0.5493 (0.5938)	
training:	Epoch: [24][53/204]	Loss 0.5112 (0.5922)	
training:	Epoch: [24][54/204]	Loss 0.5894 (0.5922)	
training:	Epoch: [24][55/204]	Loss 0.6179 (0.5926)	
training:	Epoch: [24][56/204]	Loss 0.6140 (0.5930)	
training:	Epoch: [24][57/204]	Loss 0.6965 (0.5948)	
training:	Epoch: [24][58/204]	Loss 0.6189 (0.5952)	
training:	Epoch: [24][59/204]	Loss 0.6462 (0.5961)	
training:	Epoch: [24][60/204]	Loss 0.5068 (0.5946)	
training:	Epoch: [24][61/204]	Loss 0.6069 (0.5948)	
training:	Epoch: [24][62/204]	Loss 0.5466 (0.5940)	
training:	Epoch: [24][63/204]	Loss 0.5502 (0.5933)	
training:	Epoch: [24][64/204]	Loss 0.5400 (0.5925)	
training:	Epoch: [24][65/204]	Loss 0.6805 (0.5939)	
training:	Epoch: [24][66/204]	Loss 0.7063 (0.5956)	
training:	Epoch: [24][67/204]	Loss 0.6113 (0.5958)	
training:	Epoch: [24][68/204]	Loss 0.6567 (0.5967)	
training:	Epoch: [24][69/204]	Loss 0.6656 (0.5977)	
training:	Epoch: [24][70/204]	Loss 0.4795 (0.5960)	
training:	Epoch: [24][71/204]	Loss 0.5482 (0.5953)	
training:	Epoch: [24][72/204]	Loss 0.6064 (0.5955)	
training:	Epoch: [24][73/204]	Loss 0.6110 (0.5957)	
training:	Epoch: [24][74/204]	Loss 0.5315 (0.5948)	
training:	Epoch: [24][75/204]	Loss 0.6666 (0.5958)	
training:	Epoch: [24][76/204]	Loss 0.5686 (0.5954)	
training:	Epoch: [24][77/204]	Loss 0.6170 (0.5957)	
training:	Epoch: [24][78/204]	Loss 0.5207 (0.5947)	
training:	Epoch: [24][79/204]	Loss 0.5716 (0.5945)	
training:	Epoch: [24][80/204]	Loss 0.5726 (0.5942)	
training:	Epoch: [24][81/204]	Loss 0.4763 (0.5927)	
training:	Epoch: [24][82/204]	Loss 0.5767 (0.5925)	
training:	Epoch: [24][83/204]	Loss 0.5792 (0.5924)	
training:	Epoch: [24][84/204]	Loss 0.6095 (0.5926)	
training:	Epoch: [24][85/204]	Loss 0.5392 (0.5919)	
training:	Epoch: [24][86/204]	Loss 0.6094 (0.5921)	
training:	Epoch: [24][87/204]	Loss 0.5740 (0.5919)	
training:	Epoch: [24][88/204]	Loss 0.5612 (0.5916)	
training:	Epoch: [24][89/204]	Loss 0.6064 (0.5918)	
training:	Epoch: [24][90/204]	Loss 0.5147 (0.5909)	
training:	Epoch: [24][91/204]	Loss 0.6803 (0.5919)	
training:	Epoch: [24][92/204]	Loss 0.6455 (0.5925)	
training:	Epoch: [24][93/204]	Loss 0.6015 (0.5926)	
training:	Epoch: [24][94/204]	Loss 0.5194 (0.5918)	
training:	Epoch: [24][95/204]	Loss 0.6312 (0.5922)	
training:	Epoch: [24][96/204]	Loss 0.5697 (0.5920)	
training:	Epoch: [24][97/204]	Loss 0.6230 (0.5923)	
training:	Epoch: [24][98/204]	Loss 0.5758 (0.5921)	
training:	Epoch: [24][99/204]	Loss 0.6237 (0.5924)	
training:	Epoch: [24][100/204]	Loss 0.5783 (0.5923)	
training:	Epoch: [24][101/204]	Loss 0.6007 (0.5924)	
training:	Epoch: [24][102/204]	Loss 0.5119 (0.5916)	
training:	Epoch: [24][103/204]	Loss 0.5549 (0.5912)	
training:	Epoch: [24][104/204]	Loss 0.5931 (0.5912)	
training:	Epoch: [24][105/204]	Loss 0.5869 (0.5912)	
training:	Epoch: [24][106/204]	Loss 0.6425 (0.5917)	
training:	Epoch: [24][107/204]	Loss 0.4979 (0.5908)	
training:	Epoch: [24][108/204]	Loss 0.5850 (0.5908)	
training:	Epoch: [24][109/204]	Loss 0.4973 (0.5899)	
training:	Epoch: [24][110/204]	Loss 0.5399 (0.5894)	
training:	Epoch: [24][111/204]	Loss 0.4909 (0.5886)	
training:	Epoch: [24][112/204]	Loss 0.6066 (0.5887)	
training:	Epoch: [24][113/204]	Loss 0.5517 (0.5884)	
training:	Epoch: [24][114/204]	Loss 0.5609 (0.5882)	
training:	Epoch: [24][115/204]	Loss 0.5880 (0.5882)	
training:	Epoch: [24][116/204]	Loss 0.6220 (0.5884)	
training:	Epoch: [24][117/204]	Loss 0.6044 (0.5886)	
training:	Epoch: [24][118/204]	Loss 0.5689 (0.5884)	
training:	Epoch: [24][119/204]	Loss 0.6521 (0.5889)	
training:	Epoch: [24][120/204]	Loss 0.5811 (0.5889)	
training:	Epoch: [24][121/204]	Loss 0.5847 (0.5888)	
training:	Epoch: [24][122/204]	Loss 0.4588 (0.5878)	
training:	Epoch: [24][123/204]	Loss 0.5466 (0.5874)	
training:	Epoch: [24][124/204]	Loss 0.6676 (0.5881)	
training:	Epoch: [24][125/204]	Loss 0.6303 (0.5884)	
training:	Epoch: [24][126/204]	Loss 0.5867 (0.5884)	
training:	Epoch: [24][127/204]	Loss 0.5431 (0.5881)	
training:	Epoch: [24][128/204]	Loss 0.6371 (0.5884)	
training:	Epoch: [24][129/204]	Loss 0.5863 (0.5884)	
training:	Epoch: [24][130/204]	Loss 0.6173 (0.5887)	
training:	Epoch: [24][131/204]	Loss 0.6025 (0.5888)	
training:	Epoch: [24][132/204]	Loss 0.5698 (0.5886)	
training:	Epoch: [24][133/204]	Loss 0.5995 (0.5887)	
training:	Epoch: [24][134/204]	Loss 0.5464 (0.5884)	
training:	Epoch: [24][135/204]	Loss 0.6090 (0.5885)	
training:	Epoch: [24][136/204]	Loss 0.6761 (0.5892)	
training:	Epoch: [24][137/204]	Loss 0.5770 (0.5891)	
training:	Epoch: [24][138/204]	Loss 0.6295 (0.5894)	
training:	Epoch: [24][139/204]	Loss 0.7361 (0.5904)	
training:	Epoch: [24][140/204]	Loss 0.5745 (0.5903)	
training:	Epoch: [24][141/204]	Loss 0.6158 (0.5905)	
training:	Epoch: [24][142/204]	Loss 0.5930 (0.5905)	
training:	Epoch: [24][143/204]	Loss 0.5415 (0.5902)	
training:	Epoch: [24][144/204]	Loss 0.5200 (0.5897)	
training:	Epoch: [24][145/204]	Loss 0.6290 (0.5900)	
training:	Epoch: [24][146/204]	Loss 0.6665 (0.5905)	
training:	Epoch: [24][147/204]	Loss 0.5029 (0.5899)	
training:	Epoch: [24][148/204]	Loss 0.6382 (0.5902)	
training:	Epoch: [24][149/204]	Loss 0.5390 (0.5899)	
training:	Epoch: [24][150/204]	Loss 0.5881 (0.5899)	
training:	Epoch: [24][151/204]	Loss 0.6163 (0.5900)	
training:	Epoch: [24][152/204]	Loss 0.5845 (0.5900)	
training:	Epoch: [24][153/204]	Loss 0.6679 (0.5905)	
training:	Epoch: [24][154/204]	Loss 0.4494 (0.5896)	
training:	Epoch: [24][155/204]	Loss 0.5882 (0.5896)	
training:	Epoch: [24][156/204]	Loss 0.5266 (0.5892)	
training:	Epoch: [24][157/204]	Loss 0.6176 (0.5894)	
training:	Epoch: [24][158/204]	Loss 0.6206 (0.5896)	
training:	Epoch: [24][159/204]	Loss 0.6709 (0.5901)	
training:	Epoch: [24][160/204]	Loss 0.6662 (0.5905)	
training:	Epoch: [24][161/204]	Loss 0.6527 (0.5909)	
training:	Epoch: [24][162/204]	Loss 0.5539 (0.5907)	
training:	Epoch: [24][163/204]	Loss 0.5378 (0.5904)	
training:	Epoch: [24][164/204]	Loss 0.6482 (0.5907)	
training:	Epoch: [24][165/204]	Loss 0.5541 (0.5905)	
training:	Epoch: [24][166/204]	Loss 0.6862 (0.5911)	
training:	Epoch: [24][167/204]	Loss 0.6517 (0.5914)	
training:	Epoch: [24][168/204]	Loss 0.6487 (0.5918)	
training:	Epoch: [24][169/204]	Loss 0.6579 (0.5922)	
training:	Epoch: [24][170/204]	Loss 0.6026 (0.5922)	
training:	Epoch: [24][171/204]	Loss 0.5456 (0.5920)	
training:	Epoch: [24][172/204]	Loss 0.6529 (0.5923)	
training:	Epoch: [24][173/204]	Loss 0.6443 (0.5926)	
training:	Epoch: [24][174/204]	Loss 0.6653 (0.5930)	
training:	Epoch: [24][175/204]	Loss 0.5510 (0.5928)	
training:	Epoch: [24][176/204]	Loss 0.5592 (0.5926)	
training:	Epoch: [24][177/204]	Loss 0.4916 (0.5920)	
training:	Epoch: [24][178/204]	Loss 0.5771 (0.5920)	
training:	Epoch: [24][179/204]	Loss 0.6706 (0.5924)	
training:	Epoch: [24][180/204]	Loss 0.5118 (0.5919)	
training:	Epoch: [24][181/204]	Loss 0.6664 (0.5924)	
training:	Epoch: [24][182/204]	Loss 0.6283 (0.5926)	
training:	Epoch: [24][183/204]	Loss 0.5797 (0.5925)	
training:	Epoch: [24][184/204]	Loss 0.5622 (0.5923)	
training:	Epoch: [24][185/204]	Loss 0.5747 (0.5922)	
training:	Epoch: [24][186/204]	Loss 0.5195 (0.5918)	
training:	Epoch: [24][187/204]	Loss 0.5563 (0.5916)	
training:	Epoch: [24][188/204]	Loss 0.6408 (0.5919)	
training:	Epoch: [24][189/204]	Loss 0.6063 (0.5920)	
training:	Epoch: [24][190/204]	Loss 0.5702 (0.5919)	
training:	Epoch: [24][191/204]	Loss 0.6148 (0.5920)	
training:	Epoch: [24][192/204]	Loss 0.6524 (0.5923)	
training:	Epoch: [24][193/204]	Loss 0.5165 (0.5919)	
training:	Epoch: [24][194/204]	Loss 0.5435 (0.5917)	
training:	Epoch: [24][195/204]	Loss 0.5927 (0.5917)	
training:	Epoch: [24][196/204]	Loss 0.5556 (0.5915)	
training:	Epoch: [24][197/204]	Loss 0.5295 (0.5912)	
training:	Epoch: [24][198/204]	Loss 0.5984 (0.5912)	
training:	Epoch: [24][199/204]	Loss 0.5305 (0.5909)	
training:	Epoch: [24][200/204]	Loss 0.5516 (0.5907)	
training:	Epoch: [24][201/204]	Loss 0.6218 (0.5909)	
training:	Epoch: [24][202/204]	Loss 0.5902 (0.5909)	
training:	Epoch: [24][203/204]	Loss 0.6273 (0.5910)	
training:	Epoch: [24][204/204]	Loss 0.5471 (0.5908)	
Training:	 Loss: 0.5899

Training:	 ACC: 0.6994 0.7013 0.7460 0.6527
Validation:	 ACC: 0.7009 0.7036 0.7605 0.6413
Validation:	 Best_BACC: 0.7009 0.7036 0.7605 0.6413
Validation:	 Loss: 0.5881
Pretraining:	Epoch 25/120
----------
training:	Epoch: [25][1/204]	Loss 0.6090 (0.6090)	
training:	Epoch: [25][2/204]	Loss 0.5633 (0.5862)	
training:	Epoch: [25][3/204]	Loss 0.5881 (0.5868)	
training:	Epoch: [25][4/204]	Loss 0.5892 (0.5874)	
training:	Epoch: [25][5/204]	Loss 0.5661 (0.5831)	
training:	Epoch: [25][6/204]	Loss 0.5757 (0.5819)	
training:	Epoch: [25][7/204]	Loss 0.5643 (0.5794)	
training:	Epoch: [25][8/204]	Loss 0.5988 (0.5818)	
training:	Epoch: [25][9/204]	Loss 0.5176 (0.5747)	
training:	Epoch: [25][10/204]	Loss 0.5218 (0.5694)	
training:	Epoch: [25][11/204]	Loss 0.5742 (0.5698)	
training:	Epoch: [25][12/204]	Loss 0.5440 (0.5677)	
training:	Epoch: [25][13/204]	Loss 0.4877 (0.5615)	
training:	Epoch: [25][14/204]	Loss 0.5046 (0.5575)	
training:	Epoch: [25][15/204]	Loss 0.5988 (0.5602)	
training:	Epoch: [25][16/204]	Loss 0.5688 (0.5608)	
training:	Epoch: [25][17/204]	Loss 0.5866 (0.5623)	
training:	Epoch: [25][18/204]	Loss 0.5853 (0.5636)	
training:	Epoch: [25][19/204]	Loss 0.6392 (0.5675)	
training:	Epoch: [25][20/204]	Loss 0.7518 (0.5768)	
training:	Epoch: [25][21/204]	Loss 0.5448 (0.5752)	
training:	Epoch: [25][22/204]	Loss 0.5762 (0.5753)	
training:	Epoch: [25][23/204]	Loss 0.5552 (0.5744)	
training:	Epoch: [25][24/204]	Loss 0.6070 (0.5758)	
training:	Epoch: [25][25/204]	Loss 0.5871 (0.5762)	
training:	Epoch: [25][26/204]	Loss 0.6444 (0.5788)	
training:	Epoch: [25][27/204]	Loss 0.6747 (0.5824)	
training:	Epoch: [25][28/204]	Loss 0.5906 (0.5827)	
training:	Epoch: [25][29/204]	Loss 0.6034 (0.5834)	
training:	Epoch: [25][30/204]	Loss 0.5578 (0.5825)	
training:	Epoch: [25][31/204]	Loss 0.5233 (0.5806)	
training:	Epoch: [25][32/204]	Loss 0.5972 (0.5812)	
training:	Epoch: [25][33/204]	Loss 0.5966 (0.5816)	
training:	Epoch: [25][34/204]	Loss 0.6140 (0.5826)	
training:	Epoch: [25][35/204]	Loss 0.5548 (0.5818)	
training:	Epoch: [25][36/204]	Loss 0.5847 (0.5819)	
training:	Epoch: [25][37/204]	Loss 0.6227 (0.5830)	
training:	Epoch: [25][38/204]	Loss 0.5765 (0.5828)	
training:	Epoch: [25][39/204]	Loss 0.6496 (0.5845)	
training:	Epoch: [25][40/204]	Loss 0.5884 (0.5846)	
training:	Epoch: [25][41/204]	Loss 0.6295 (0.5857)	
training:	Epoch: [25][42/204]	Loss 0.5242 (0.5842)	
training:	Epoch: [25][43/204]	Loss 0.6074 (0.5848)	
training:	Epoch: [25][44/204]	Loss 0.6444 (0.5861)	
training:	Epoch: [25][45/204]	Loss 0.5455 (0.5852)	
training:	Epoch: [25][46/204]	Loss 0.6314 (0.5862)	
training:	Epoch: [25][47/204]	Loss 0.5877 (0.5863)	
training:	Epoch: [25][48/204]	Loss 0.6043 (0.5866)	
training:	Epoch: [25][49/204]	Loss 0.5356 (0.5856)	
training:	Epoch: [25][50/204]	Loss 0.5957 (0.5858)	
training:	Epoch: [25][51/204]	Loss 0.6737 (0.5875)	
training:	Epoch: [25][52/204]	Loss 0.5852 (0.5875)	
training:	Epoch: [25][53/204]	Loss 0.6062 (0.5878)	
training:	Epoch: [25][54/204]	Loss 0.6245 (0.5885)	
training:	Epoch: [25][55/204]	Loss 0.5714 (0.5882)	
training:	Epoch: [25][56/204]	Loss 0.5825 (0.5881)	
training:	Epoch: [25][57/204]	Loss 0.6315 (0.5889)	
training:	Epoch: [25][58/204]	Loss 0.5663 (0.5885)	
training:	Epoch: [25][59/204]	Loss 0.5608 (0.5880)	
training:	Epoch: [25][60/204]	Loss 0.5982 (0.5882)	
training:	Epoch: [25][61/204]	Loss 0.5790 (0.5880)	
training:	Epoch: [25][62/204]	Loss 0.6315 (0.5887)	
training:	Epoch: [25][63/204]	Loss 0.6729 (0.5901)	
training:	Epoch: [25][64/204]	Loss 0.6736 (0.5914)	
training:	Epoch: [25][65/204]	Loss 0.6389 (0.5921)	
training:	Epoch: [25][66/204]	Loss 0.5795 (0.5919)	
training:	Epoch: [25][67/204]	Loss 0.6250 (0.5924)	
training:	Epoch: [25][68/204]	Loss 0.6405 (0.5931)	
training:	Epoch: [25][69/204]	Loss 0.5312 (0.5922)	
training:	Epoch: [25][70/204]	Loss 0.5958 (0.5923)	
training:	Epoch: [25][71/204]	Loss 0.5172 (0.5912)	
training:	Epoch: [25][72/204]	Loss 0.6397 (0.5919)	
training:	Epoch: [25][73/204]	Loss 0.6018 (0.5920)	
training:	Epoch: [25][74/204]	Loss 0.7073 (0.5936)	
training:	Epoch: [25][75/204]	Loss 0.5562 (0.5931)	
training:	Epoch: [25][76/204]	Loss 0.5561 (0.5926)	
training:	Epoch: [25][77/204]	Loss 0.5510 (0.5920)	
training:	Epoch: [25][78/204]	Loss 0.4863 (0.5907)	
training:	Epoch: [25][79/204]	Loss 0.5933 (0.5907)	
training:	Epoch: [25][80/204]	Loss 0.6134 (0.5910)	
training:	Epoch: [25][81/204]	Loss 0.5518 (0.5905)	
training:	Epoch: [25][82/204]	Loss 0.5365 (0.5899)	
training:	Epoch: [25][83/204]	Loss 0.5522 (0.5894)	
training:	Epoch: [25][84/204]	Loss 0.5213 (0.5886)	
training:	Epoch: [25][85/204]	Loss 0.5077 (0.5876)	
training:	Epoch: [25][86/204]	Loss 0.5694 (0.5874)	
training:	Epoch: [25][87/204]	Loss 0.5715 (0.5872)	
training:	Epoch: [25][88/204]	Loss 0.5823 (0.5872)	
training:	Epoch: [25][89/204]	Loss 0.6997 (0.5885)	
training:	Epoch: [25][90/204]	Loss 0.5969 (0.5886)	
training:	Epoch: [25][91/204]	Loss 0.6231 (0.5889)	
training:	Epoch: [25][92/204]	Loss 0.6385 (0.5895)	
training:	Epoch: [25][93/204]	Loss 0.6090 (0.5897)	
training:	Epoch: [25][94/204]	Loss 0.5434 (0.5892)	
training:	Epoch: [25][95/204]	Loss 0.6722 (0.5901)	
training:	Epoch: [25][96/204]	Loss 0.5970 (0.5901)	
training:	Epoch: [25][97/204]	Loss 0.5470 (0.5897)	
training:	Epoch: [25][98/204]	Loss 0.6119 (0.5899)	
training:	Epoch: [25][99/204]	Loss 0.5561 (0.5896)	
training:	Epoch: [25][100/204]	Loss 0.4425 (0.5881)	
training:	Epoch: [25][101/204]	Loss 0.5074 (0.5873)	
training:	Epoch: [25][102/204]	Loss 0.5839 (0.5873)	
training:	Epoch: [25][103/204]	Loss 0.5262 (0.5867)	
training:	Epoch: [25][104/204]	Loss 0.5397 (0.5862)	
training:	Epoch: [25][105/204]	Loss 0.6927 (0.5872)	
training:	Epoch: [25][106/204]	Loss 0.6638 (0.5880)	
training:	Epoch: [25][107/204]	Loss 0.6153 (0.5882)	
training:	Epoch: [25][108/204]	Loss 0.5109 (0.5875)	
training:	Epoch: [25][109/204]	Loss 0.6575 (0.5881)	
training:	Epoch: [25][110/204]	Loss 0.4930 (0.5873)	
training:	Epoch: [25][111/204]	Loss 0.4765 (0.5863)	
training:	Epoch: [25][112/204]	Loss 0.6374 (0.5867)	
training:	Epoch: [25][113/204]	Loss 0.5193 (0.5861)	
training:	Epoch: [25][114/204]	Loss 0.5695 (0.5860)	
training:	Epoch: [25][115/204]	Loss 0.5498 (0.5857)	
training:	Epoch: [25][116/204]	Loss 0.6931 (0.5866)	
training:	Epoch: [25][117/204]	Loss 0.6157 (0.5869)	
training:	Epoch: [25][118/204]	Loss 0.5218 (0.5863)	
training:	Epoch: [25][119/204]	Loss 0.5759 (0.5862)	
training:	Epoch: [25][120/204]	Loss 0.6278 (0.5866)	
training:	Epoch: [25][121/204]	Loss 0.6327 (0.5869)	
training:	Epoch: [25][122/204]	Loss 0.6555 (0.5875)	
training:	Epoch: [25][123/204]	Loss 0.5020 (0.5868)	
training:	Epoch: [25][124/204]	Loss 0.5993 (0.5869)	
training:	Epoch: [25][125/204]	Loss 0.6303 (0.5873)	
training:	Epoch: [25][126/204]	Loss 0.5146 (0.5867)	
training:	Epoch: [25][127/204]	Loss 0.6187 (0.5869)	
training:	Epoch: [25][128/204]	Loss 0.6098 (0.5871)	
training:	Epoch: [25][129/204]	Loss 0.6065 (0.5873)	
training:	Epoch: [25][130/204]	Loss 0.6143 (0.5875)	
training:	Epoch: [25][131/204]	Loss 0.6788 (0.5882)	
training:	Epoch: [25][132/204]	Loss 0.7180 (0.5892)	
training:	Epoch: [25][133/204]	Loss 0.5118 (0.5886)	
training:	Epoch: [25][134/204]	Loss 0.5667 (0.5884)	
training:	Epoch: [25][135/204]	Loss 0.5739 (0.5883)	
training:	Epoch: [25][136/204]	Loss 0.5072 (0.5877)	
training:	Epoch: [25][137/204]	Loss 0.5354 (0.5873)	
training:	Epoch: [25][138/204]	Loss 0.5251 (0.5869)	
training:	Epoch: [25][139/204]	Loss 0.5802 (0.5868)	
training:	Epoch: [25][140/204]	Loss 0.7341 (0.5879)	
training:	Epoch: [25][141/204]	Loss 0.5956 (0.5879)	
training:	Epoch: [25][142/204]	Loss 0.5718 (0.5878)	
training:	Epoch: [25][143/204]	Loss 0.6672 (0.5884)	
training:	Epoch: [25][144/204]	Loss 0.6492 (0.5888)	
training:	Epoch: [25][145/204]	Loss 0.5573 (0.5886)	
training:	Epoch: [25][146/204]	Loss 0.6172 (0.5888)	
training:	Epoch: [25][147/204]	Loss 0.5566 (0.5886)	
training:	Epoch: [25][148/204]	Loss 0.6564 (0.5890)	
training:	Epoch: [25][149/204]	Loss 0.5934 (0.5890)	
training:	Epoch: [25][150/204]	Loss 0.6265 (0.5893)	
training:	Epoch: [25][151/204]	Loss 0.5666 (0.5891)	
training:	Epoch: [25][152/204]	Loss 0.5612 (0.5890)	
training:	Epoch: [25][153/204]	Loss 0.6586 (0.5894)	
training:	Epoch: [25][154/204]	Loss 0.5929 (0.5894)	
training:	Epoch: [25][155/204]	Loss 0.4993 (0.5889)	
training:	Epoch: [25][156/204]	Loss 0.6252 (0.5891)	
training:	Epoch: [25][157/204]	Loss 0.5411 (0.5888)	
training:	Epoch: [25][158/204]	Loss 0.5447 (0.5885)	
training:	Epoch: [25][159/204]	Loss 0.5985 (0.5886)	
training:	Epoch: [25][160/204]	Loss 0.5798 (0.5885)	
training:	Epoch: [25][161/204]	Loss 0.5839 (0.5885)	
training:	Epoch: [25][162/204]	Loss 0.5555 (0.5883)	
training:	Epoch: [25][163/204]	Loss 0.5302 (0.5879)	
training:	Epoch: [25][164/204]	Loss 0.6398 (0.5882)	
training:	Epoch: [25][165/204]	Loss 0.6520 (0.5886)	
training:	Epoch: [25][166/204]	Loss 0.5215 (0.5882)	
training:	Epoch: [25][167/204]	Loss 0.5749 (0.5881)	
training:	Epoch: [25][168/204]	Loss 0.5778 (0.5881)	
training:	Epoch: [25][169/204]	Loss 0.5552 (0.5879)	
training:	Epoch: [25][170/204]	Loss 0.5664 (0.5878)	
training:	Epoch: [25][171/204]	Loss 0.6628 (0.5882)	
training:	Epoch: [25][172/204]	Loss 0.5918 (0.5882)	
training:	Epoch: [25][173/204]	Loss 0.5711 (0.5881)	
training:	Epoch: [25][174/204]	Loss 0.6359 (0.5884)	
training:	Epoch: [25][175/204]	Loss 0.5834 (0.5884)	
training:	Epoch: [25][176/204]	Loss 0.5518 (0.5882)	
training:	Epoch: [25][177/204]	Loss 0.5367 (0.5879)	
training:	Epoch: [25][178/204]	Loss 0.6483 (0.5882)	
training:	Epoch: [25][179/204]	Loss 0.6279 (0.5884)	
training:	Epoch: [25][180/204]	Loss 0.6611 (0.5888)	
training:	Epoch: [25][181/204]	Loss 0.6522 (0.5892)	
training:	Epoch: [25][182/204]	Loss 0.5116 (0.5888)	
training:	Epoch: [25][183/204]	Loss 0.5542 (0.5886)	
training:	Epoch: [25][184/204]	Loss 0.5985 (0.5886)	
training:	Epoch: [25][185/204]	Loss 0.6079 (0.5887)	
training:	Epoch: [25][186/204]	Loss 0.4907 (0.5882)	
training:	Epoch: [25][187/204]	Loss 0.6105 (0.5883)	
training:	Epoch: [25][188/204]	Loss 0.5855 (0.5883)	
training:	Epoch: [25][189/204]	Loss 0.5885 (0.5883)	
training:	Epoch: [25][190/204]	Loss 0.5265 (0.5880)	
training:	Epoch: [25][191/204]	Loss 0.5571 (0.5878)	
training:	Epoch: [25][192/204]	Loss 0.6214 (0.5880)	
training:	Epoch: [25][193/204]	Loss 0.5857 (0.5880)	
training:	Epoch: [25][194/204]	Loss 0.4689 (0.5874)	
training:	Epoch: [25][195/204]	Loss 0.6073 (0.5875)	
training:	Epoch: [25][196/204]	Loss 0.7180 (0.5881)	
training:	Epoch: [25][197/204]	Loss 0.6328 (0.5884)	
training:	Epoch: [25][198/204]	Loss 0.5257 (0.5880)	
training:	Epoch: [25][199/204]	Loss 0.6398 (0.5883)	
training:	Epoch: [25][200/204]	Loss 0.6746 (0.5887)	
training:	Epoch: [25][201/204]	Loss 0.5362 (0.5885)	
training:	Epoch: [25][202/204]	Loss 0.5700 (0.5884)	
training:	Epoch: [25][203/204]	Loss 0.5995 (0.5884)	
training:	Epoch: [25][204/204]	Loss 0.5992 (0.5885)	
Training:	 Loss: 0.5876

Training:	 ACC: 0.7069 0.7080 0.7352 0.6786
Validation:	 ACC: 0.7032 0.7052 0.7472 0.6592
Validation:	 Best_BACC: 0.7032 0.7052 0.7472 0.6592
Validation:	 Loss: 0.5827
Pretraining:	Epoch 26/120
----------
training:	Epoch: [26][1/204]	Loss 0.5079 (0.5079)	
training:	Epoch: [26][2/204]	Loss 0.6210 (0.5645)	
training:	Epoch: [26][3/204]	Loss 0.5393 (0.5561)	
training:	Epoch: [26][4/204]	Loss 0.5076 (0.5440)	
training:	Epoch: [26][5/204]	Loss 0.6901 (0.5732)	
training:	Epoch: [26][6/204]	Loss 0.6777 (0.5906)	
training:	Epoch: [26][7/204]	Loss 0.5489 (0.5847)	
training:	Epoch: [26][8/204]	Loss 0.5795 (0.5840)	
training:	Epoch: [26][9/204]	Loss 0.5706 (0.5825)	
training:	Epoch: [26][10/204]	Loss 0.6206 (0.5863)	
training:	Epoch: [26][11/204]	Loss 0.6566 (0.5927)	
training:	Epoch: [26][12/204]	Loss 0.5458 (0.5888)	
training:	Epoch: [26][13/204]	Loss 0.5468 (0.5856)	
training:	Epoch: [26][14/204]	Loss 0.6602 (0.5909)	
training:	Epoch: [26][15/204]	Loss 0.5588 (0.5888)	
training:	Epoch: [26][16/204]	Loss 0.5951 (0.5892)	
training:	Epoch: [26][17/204]	Loss 0.5825 (0.5888)	
training:	Epoch: [26][18/204]	Loss 0.5127 (0.5845)	
training:	Epoch: [26][19/204]	Loss 0.6895 (0.5901)	
training:	Epoch: [26][20/204]	Loss 0.5304 (0.5871)	
training:	Epoch: [26][21/204]	Loss 0.5101 (0.5834)	
training:	Epoch: [26][22/204]	Loss 0.6234 (0.5852)	
training:	Epoch: [26][23/204]	Loss 0.5485 (0.5836)	
training:	Epoch: [26][24/204]	Loss 0.6418 (0.5861)	
training:	Epoch: [26][25/204]	Loss 0.6205 (0.5874)	
training:	Epoch: [26][26/204]	Loss 0.5917 (0.5876)	
training:	Epoch: [26][27/204]	Loss 0.6033 (0.5882)	
training:	Epoch: [26][28/204]	Loss 0.5228 (0.5859)	
training:	Epoch: [26][29/204]	Loss 0.5116 (0.5833)	
training:	Epoch: [26][30/204]	Loss 0.6192 (0.5845)	
training:	Epoch: [26][31/204]	Loss 0.5586 (0.5837)	
training:	Epoch: [26][32/204]	Loss 0.5509 (0.5826)	
training:	Epoch: [26][33/204]	Loss 0.6092 (0.5834)	
training:	Epoch: [26][34/204]	Loss 0.6100 (0.5842)	
training:	Epoch: [26][35/204]	Loss 0.5701 (0.5838)	
training:	Epoch: [26][36/204]	Loss 0.7223 (0.5877)	
training:	Epoch: [26][37/204]	Loss 0.6338 (0.5889)	
training:	Epoch: [26][38/204]	Loss 0.6059 (0.5894)	
training:	Epoch: [26][39/204]	Loss 0.6353 (0.5905)	
training:	Epoch: [26][40/204]	Loss 0.5639 (0.5899)	
training:	Epoch: [26][41/204]	Loss 0.5408 (0.5887)	
training:	Epoch: [26][42/204]	Loss 0.6021 (0.5890)	
training:	Epoch: [26][43/204]	Loss 0.5890 (0.5890)	
training:	Epoch: [26][44/204]	Loss 0.5570 (0.5883)	
training:	Epoch: [26][45/204]	Loss 0.5376 (0.5871)	
training:	Epoch: [26][46/204]	Loss 0.6609 (0.5887)	
training:	Epoch: [26][47/204]	Loss 0.5326 (0.5875)	
training:	Epoch: [26][48/204]	Loss 0.6646 (0.5892)	
training:	Epoch: [26][49/204]	Loss 0.5596 (0.5885)	
training:	Epoch: [26][50/204]	Loss 0.5390 (0.5876)	
training:	Epoch: [26][51/204]	Loss 0.7363 (0.5905)	
training:	Epoch: [26][52/204]	Loss 0.6974 (0.5925)	
training:	Epoch: [26][53/204]	Loss 0.6798 (0.5942)	
training:	Epoch: [26][54/204]	Loss 0.6061 (0.5944)	
training:	Epoch: [26][55/204]	Loss 0.5530 (0.5936)	
training:	Epoch: [26][56/204]	Loss 0.6079 (0.5939)	
training:	Epoch: [26][57/204]	Loss 0.6277 (0.5945)	
training:	Epoch: [26][58/204]	Loss 0.6210 (0.5950)	
training:	Epoch: [26][59/204]	Loss 0.5950 (0.5950)	
training:	Epoch: [26][60/204]	Loss 0.5157 (0.5936)	
training:	Epoch: [26][61/204]	Loss 0.6796 (0.5950)	
training:	Epoch: [26][62/204]	Loss 0.5345 (0.5941)	
training:	Epoch: [26][63/204]	Loss 0.5429 (0.5933)	
training:	Epoch: [26][64/204]	Loss 0.7140 (0.5951)	
training:	Epoch: [26][65/204]	Loss 0.6015 (0.5952)	
training:	Epoch: [26][66/204]	Loss 0.5074 (0.5939)	
training:	Epoch: [26][67/204]	Loss 0.5018 (0.5925)	
training:	Epoch: [26][68/204]	Loss 0.5004 (0.5912)	
training:	Epoch: [26][69/204]	Loss 0.5660 (0.5908)	
training:	Epoch: [26][70/204]	Loss 0.6645 (0.5919)	
training:	Epoch: [26][71/204]	Loss 0.6720 (0.5930)	
training:	Epoch: [26][72/204]	Loss 0.6202 (0.5934)	
training:	Epoch: [26][73/204]	Loss 0.7043 (0.5949)	
training:	Epoch: [26][74/204]	Loss 0.5962 (0.5949)	
training:	Epoch: [26][75/204]	Loss 0.5977 (0.5949)	
training:	Epoch: [26][76/204]	Loss 0.5746 (0.5947)	
training:	Epoch: [26][77/204]	Loss 0.6196 (0.5950)	
training:	Epoch: [26][78/204]	Loss 0.5901 (0.5949)	
training:	Epoch: [26][79/204]	Loss 0.5325 (0.5941)	
training:	Epoch: [26][80/204]	Loss 0.5576 (0.5937)	
training:	Epoch: [26][81/204]	Loss 0.5341 (0.5930)	
training:	Epoch: [26][82/204]	Loss 0.6089 (0.5931)	
training:	Epoch: [26][83/204]	Loss 0.6357 (0.5937)	
training:	Epoch: [26][84/204]	Loss 0.5258 (0.5929)	
training:	Epoch: [26][85/204]	Loss 0.5971 (0.5929)	
training:	Epoch: [26][86/204]	Loss 0.4772 (0.5916)	
training:	Epoch: [26][87/204]	Loss 0.5261 (0.5908)	
training:	Epoch: [26][88/204]	Loss 0.5515 (0.5904)	
training:	Epoch: [26][89/204]	Loss 0.6624 (0.5912)	
training:	Epoch: [26][90/204]	Loss 0.7306 (0.5927)	
training:	Epoch: [26][91/204]	Loss 0.5847 (0.5926)	
training:	Epoch: [26][92/204]	Loss 0.5730 (0.5924)	
training:	Epoch: [26][93/204]	Loss 0.6341 (0.5929)	
training:	Epoch: [26][94/204]	Loss 0.6548 (0.5935)	
training:	Epoch: [26][95/204]	Loss 0.5237 (0.5928)	
training:	Epoch: [26][96/204]	Loss 0.6623 (0.5935)	
training:	Epoch: [26][97/204]	Loss 0.6687 (0.5943)	
training:	Epoch: [26][98/204]	Loss 0.5227 (0.5936)	
training:	Epoch: [26][99/204]	Loss 0.5584 (0.5932)	
training:	Epoch: [26][100/204]	Loss 0.5448 (0.5927)	
training:	Epoch: [26][101/204]	Loss 0.5795 (0.5926)	
training:	Epoch: [26][102/204]	Loss 0.5485 (0.5922)	
training:	Epoch: [26][103/204]	Loss 0.5412 (0.5917)	
training:	Epoch: [26][104/204]	Loss 0.4983 (0.5908)	
training:	Epoch: [26][105/204]	Loss 0.6629 (0.5915)	
training:	Epoch: [26][106/204]	Loss 0.5461 (0.5910)	
training:	Epoch: [26][107/204]	Loss 0.6672 (0.5917)	
training:	Epoch: [26][108/204]	Loss 0.5835 (0.5917)	
training:	Epoch: [26][109/204]	Loss 0.5434 (0.5912)	
training:	Epoch: [26][110/204]	Loss 0.5425 (0.5908)	
training:	Epoch: [26][111/204]	Loss 0.5736 (0.5906)	
training:	Epoch: [26][112/204]	Loss 0.6827 (0.5914)	
training:	Epoch: [26][113/204]	Loss 0.5656 (0.5912)	
training:	Epoch: [26][114/204]	Loss 0.5621 (0.5910)	
training:	Epoch: [26][115/204]	Loss 0.4981 (0.5902)	
training:	Epoch: [26][116/204]	Loss 0.5950 (0.5902)	
training:	Epoch: [26][117/204]	Loss 0.6286 (0.5905)	
training:	Epoch: [26][118/204]	Loss 0.5890 (0.5905)	
training:	Epoch: [26][119/204]	Loss 0.5044 (0.5898)	
training:	Epoch: [26][120/204]	Loss 0.5677 (0.5896)	
training:	Epoch: [26][121/204]	Loss 0.6795 (0.5903)	
training:	Epoch: [26][122/204]	Loss 0.5202 (0.5898)	
training:	Epoch: [26][123/204]	Loss 0.5557 (0.5895)	
training:	Epoch: [26][124/204]	Loss 0.5649 (0.5893)	
training:	Epoch: [26][125/204]	Loss 0.5924 (0.5893)	
training:	Epoch: [26][126/204]	Loss 0.6226 (0.5896)	
training:	Epoch: [26][127/204]	Loss 0.5504 (0.5893)	
training:	Epoch: [26][128/204]	Loss 0.5391 (0.5889)	
training:	Epoch: [26][129/204]	Loss 0.6278 (0.5892)	
training:	Epoch: [26][130/204]	Loss 0.6306 (0.5895)	
training:	Epoch: [26][131/204]	Loss 0.5157 (0.5889)	
training:	Epoch: [26][132/204]	Loss 0.4834 (0.5881)	
training:	Epoch: [26][133/204]	Loss 0.6330 (0.5885)	
training:	Epoch: [26][134/204]	Loss 0.5761 (0.5884)	
training:	Epoch: [26][135/204]	Loss 0.6265 (0.5887)	
training:	Epoch: [26][136/204]	Loss 0.5551 (0.5884)	
training:	Epoch: [26][137/204]	Loss 0.6034 (0.5885)	
training:	Epoch: [26][138/204]	Loss 0.5803 (0.5885)	
training:	Epoch: [26][139/204]	Loss 0.5015 (0.5878)	
training:	Epoch: [26][140/204]	Loss 0.5056 (0.5873)	
training:	Epoch: [26][141/204]	Loss 0.5743 (0.5872)	
training:	Epoch: [26][142/204]	Loss 0.4958 (0.5865)	
training:	Epoch: [26][143/204]	Loss 0.5406 (0.5862)	
training:	Epoch: [26][144/204]	Loss 0.5627 (0.5860)	
training:	Epoch: [26][145/204]	Loss 0.6925 (0.5868)	
training:	Epoch: [26][146/204]	Loss 0.6448 (0.5872)	
training:	Epoch: [26][147/204]	Loss 0.5494 (0.5869)	
training:	Epoch: [26][148/204]	Loss 0.6825 (0.5876)	
training:	Epoch: [26][149/204]	Loss 0.6420 (0.5879)	
training:	Epoch: [26][150/204]	Loss 0.5999 (0.5880)	
training:	Epoch: [26][151/204]	Loss 0.5122 (0.5875)	
training:	Epoch: [26][152/204]	Loss 0.5424 (0.5872)	
training:	Epoch: [26][153/204]	Loss 0.5866 (0.5872)	
training:	Epoch: [26][154/204]	Loss 0.5656 (0.5871)	
training:	Epoch: [26][155/204]	Loss 0.4867 (0.5864)	
training:	Epoch: [26][156/204]	Loss 0.4656 (0.5856)	
training:	Epoch: [26][157/204]	Loss 0.6385 (0.5860)	
training:	Epoch: [26][158/204]	Loss 0.5806 (0.5859)	
training:	Epoch: [26][159/204]	Loss 0.5686 (0.5858)	
training:	Epoch: [26][160/204]	Loss 0.5978 (0.5859)	
training:	Epoch: [26][161/204]	Loss 0.5199 (0.5855)	
training:	Epoch: [26][162/204]	Loss 0.5819 (0.5855)	
training:	Epoch: [26][163/204]	Loss 0.5063 (0.5850)	
training:	Epoch: [26][164/204]	Loss 0.6529 (0.5854)	
training:	Epoch: [26][165/204]	Loss 0.6341 (0.5857)	
training:	Epoch: [26][166/204]	Loss 0.4970 (0.5852)	
training:	Epoch: [26][167/204]	Loss 0.5243 (0.5848)	
training:	Epoch: [26][168/204]	Loss 0.6316 (0.5851)	
training:	Epoch: [26][169/204]	Loss 0.4644 (0.5844)	
training:	Epoch: [26][170/204]	Loss 0.5690 (0.5843)	
training:	Epoch: [26][171/204]	Loss 0.5505 (0.5841)	
training:	Epoch: [26][172/204]	Loss 0.5658 (0.5840)	
training:	Epoch: [26][173/204]	Loss 0.5936 (0.5840)	
training:	Epoch: [26][174/204]	Loss 0.5484 (0.5838)	
training:	Epoch: [26][175/204]	Loss 0.6465 (0.5842)	
training:	Epoch: [26][176/204]	Loss 0.5847 (0.5842)	
training:	Epoch: [26][177/204]	Loss 0.5186 (0.5838)	
training:	Epoch: [26][178/204]	Loss 0.5768 (0.5838)	
training:	Epoch: [26][179/204]	Loss 0.6338 (0.5841)	
training:	Epoch: [26][180/204]	Loss 0.6484 (0.5844)	
training:	Epoch: [26][181/204]	Loss 0.5871 (0.5844)	
training:	Epoch: [26][182/204]	Loss 0.5896 (0.5845)	
training:	Epoch: [26][183/204]	Loss 0.5670 (0.5844)	
training:	Epoch: [26][184/204]	Loss 0.6281 (0.5846)	
training:	Epoch: [26][185/204]	Loss 0.5901 (0.5846)	
training:	Epoch: [26][186/204]	Loss 0.6541 (0.5850)	
training:	Epoch: [26][187/204]	Loss 0.5878 (0.5850)	
training:	Epoch: [26][188/204]	Loss 0.4841 (0.5845)	
training:	Epoch: [26][189/204]	Loss 0.7453 (0.5853)	
training:	Epoch: [26][190/204]	Loss 0.5737 (0.5853)	
training:	Epoch: [26][191/204]	Loss 0.5550 (0.5851)	
training:	Epoch: [26][192/204]	Loss 0.6121 (0.5852)	
training:	Epoch: [26][193/204]	Loss 0.4918 (0.5848)	
training:	Epoch: [26][194/204]	Loss 0.6789 (0.5852)	
training:	Epoch: [26][195/204]	Loss 0.6089 (0.5854)	
training:	Epoch: [26][196/204]	Loss 0.5000 (0.5849)	
training:	Epoch: [26][197/204]	Loss 0.6347 (0.5852)	
training:	Epoch: [26][198/204]	Loss 0.5340 (0.5849)	
training:	Epoch: [26][199/204]	Loss 0.5733 (0.5849)	
training:	Epoch: [26][200/204]	Loss 0.5684 (0.5848)	
training:	Epoch: [26][201/204]	Loss 0.4917 (0.5843)	
training:	Epoch: [26][202/204]	Loss 0.5790 (0.5843)	
training:	Epoch: [26][203/204]	Loss 0.5654 (0.5842)	
training:	Epoch: [26][204/204]	Loss 0.5207 (0.5839)	
Training:	 Loss: 0.5830

Training:	 ACC: 0.7125 0.7134 0.7334 0.6916
Validation:	 ACC: 0.7069 0.7084 0.7400 0.6738
Validation:	 Best_BACC: 0.7069 0.7084 0.7400 0.6738
Validation:	 Loss: 0.5786
Pretraining:	Epoch 27/120
----------
training:	Epoch: [27][1/204]	Loss 0.6759 (0.6759)	
training:	Epoch: [27][2/204]	Loss 0.5494 (0.6127)	
training:	Epoch: [27][3/204]	Loss 0.5806 (0.6020)	
training:	Epoch: [27][4/204]	Loss 0.6578 (0.6159)	
training:	Epoch: [27][5/204]	Loss 0.5698 (0.6067)	
training:	Epoch: [27][6/204]	Loss 0.5513 (0.5975)	
training:	Epoch: [27][7/204]	Loss 0.5207 (0.5865)	
training:	Epoch: [27][8/204]	Loss 0.4629 (0.5711)	
training:	Epoch: [27][9/204]	Loss 0.5501 (0.5687)	
training:	Epoch: [27][10/204]	Loss 0.6573 (0.5776)	
training:	Epoch: [27][11/204]	Loss 0.5259 (0.5729)	
training:	Epoch: [27][12/204]	Loss 0.5451 (0.5706)	
training:	Epoch: [27][13/204]	Loss 0.6169 (0.5741)	
training:	Epoch: [27][14/204]	Loss 0.5278 (0.5708)	
training:	Epoch: [27][15/204]	Loss 0.5232 (0.5677)	
training:	Epoch: [27][16/204]	Loss 0.6188 (0.5708)	
training:	Epoch: [27][17/204]	Loss 0.5229 (0.5680)	
training:	Epoch: [27][18/204]	Loss 0.5623 (0.5677)	
training:	Epoch: [27][19/204]	Loss 0.6302 (0.5710)	
training:	Epoch: [27][20/204]	Loss 0.6332 (0.5741)	
training:	Epoch: [27][21/204]	Loss 0.6905 (0.5797)	
training:	Epoch: [27][22/204]	Loss 0.5813 (0.5797)	
training:	Epoch: [27][23/204]	Loss 0.6732 (0.5838)	
training:	Epoch: [27][24/204]	Loss 0.5619 (0.5829)	
training:	Epoch: [27][25/204]	Loss 0.5897 (0.5832)	
training:	Epoch: [27][26/204]	Loss 0.5658 (0.5825)	
training:	Epoch: [27][27/204]	Loss 0.4877 (0.5790)	
training:	Epoch: [27][28/204]	Loss 0.5792 (0.5790)	
training:	Epoch: [27][29/204]	Loss 0.5382 (0.5776)	
training:	Epoch: [27][30/204]	Loss 0.4840 (0.5745)	
training:	Epoch: [27][31/204]	Loss 0.5071 (0.5723)	
training:	Epoch: [27][32/204]	Loss 0.5706 (0.5722)	
training:	Epoch: [27][33/204]	Loss 0.6119 (0.5734)	
training:	Epoch: [27][34/204]	Loss 0.5805 (0.5736)	
training:	Epoch: [27][35/204]	Loss 0.5137 (0.5719)	
training:	Epoch: [27][36/204]	Loss 0.4892 (0.5696)	
training:	Epoch: [27][37/204]	Loss 0.5106 (0.5680)	
training:	Epoch: [27][38/204]	Loss 0.6245 (0.5695)	
training:	Epoch: [27][39/204]	Loss 0.6068 (0.5705)	
training:	Epoch: [27][40/204]	Loss 0.6040 (0.5713)	
training:	Epoch: [27][41/204]	Loss 0.6074 (0.5722)	
training:	Epoch: [27][42/204]	Loss 0.6569 (0.5742)	
training:	Epoch: [27][43/204]	Loss 0.5631 (0.5740)	
training:	Epoch: [27][44/204]	Loss 0.6640 (0.5760)	
training:	Epoch: [27][45/204]	Loss 0.5642 (0.5757)	
training:	Epoch: [27][46/204]	Loss 0.5676 (0.5756)	
training:	Epoch: [27][47/204]	Loss 0.5433 (0.5749)	
training:	Epoch: [27][48/204]	Loss 0.5969 (0.5753)	
training:	Epoch: [27][49/204]	Loss 0.6107 (0.5761)	
training:	Epoch: [27][50/204]	Loss 0.6689 (0.5779)	
training:	Epoch: [27][51/204]	Loss 0.5463 (0.5773)	
training:	Epoch: [27][52/204]	Loss 0.5835 (0.5774)	
training:	Epoch: [27][53/204]	Loss 0.5221 (0.5764)	
training:	Epoch: [27][54/204]	Loss 0.5009 (0.5750)	
training:	Epoch: [27][55/204]	Loss 0.5062 (0.5737)	
training:	Epoch: [27][56/204]	Loss 0.5364 (0.5731)	
training:	Epoch: [27][57/204]	Loss 0.5348 (0.5724)	
training:	Epoch: [27][58/204]	Loss 0.5576 (0.5721)	
training:	Epoch: [27][59/204]	Loss 0.5002 (0.5709)	
training:	Epoch: [27][60/204]	Loss 0.6405 (0.5721)	
training:	Epoch: [27][61/204]	Loss 0.5795 (0.5722)	
training:	Epoch: [27][62/204]	Loss 0.6557 (0.5735)	
training:	Epoch: [27][63/204]	Loss 0.5642 (0.5734)	
training:	Epoch: [27][64/204]	Loss 0.5228 (0.5726)	
training:	Epoch: [27][65/204]	Loss 0.6106 (0.5732)	
training:	Epoch: [27][66/204]	Loss 0.6568 (0.5744)	
training:	Epoch: [27][67/204]	Loss 0.5028 (0.5734)	
training:	Epoch: [27][68/204]	Loss 0.5709 (0.5733)	
training:	Epoch: [27][69/204]	Loss 0.5503 (0.5730)	
training:	Epoch: [27][70/204]	Loss 0.5333 (0.5724)	
training:	Epoch: [27][71/204]	Loss 0.6382 (0.5734)	
training:	Epoch: [27][72/204]	Loss 0.5325 (0.5728)	
training:	Epoch: [27][73/204]	Loss 0.5165 (0.5720)	
training:	Epoch: [27][74/204]	Loss 0.6100 (0.5725)	
training:	Epoch: [27][75/204]	Loss 0.5852 (0.5727)	
training:	Epoch: [27][76/204]	Loss 0.6580 (0.5738)	
training:	Epoch: [27][77/204]	Loss 0.5963 (0.5741)	
training:	Epoch: [27][78/204]	Loss 0.5221 (0.5735)	
training:	Epoch: [27][79/204]	Loss 0.6210 (0.5741)	
training:	Epoch: [27][80/204]	Loss 0.5924 (0.5743)	
training:	Epoch: [27][81/204]	Loss 0.5156 (0.5736)	
training:	Epoch: [27][82/204]	Loss 0.4745 (0.5724)	
training:	Epoch: [27][83/204]	Loss 0.5549 (0.5721)	
training:	Epoch: [27][84/204]	Loss 0.5679 (0.5721)	
training:	Epoch: [27][85/204]	Loss 0.6357 (0.5728)	
training:	Epoch: [27][86/204]	Loss 0.5377 (0.5724)	
training:	Epoch: [27][87/204]	Loss 0.5694 (0.5724)	
training:	Epoch: [27][88/204]	Loss 0.5215 (0.5718)	
training:	Epoch: [27][89/204]	Loss 0.5395 (0.5715)	
training:	Epoch: [27][90/204]	Loss 0.5734 (0.5715)	
training:	Epoch: [27][91/204]	Loss 0.4666 (0.5703)	
training:	Epoch: [27][92/204]	Loss 0.6366 (0.5710)	
training:	Epoch: [27][93/204]	Loss 0.5404 (0.5707)	
training:	Epoch: [27][94/204]	Loss 0.5856 (0.5709)	
training:	Epoch: [27][95/204]	Loss 0.6286 (0.5715)	
training:	Epoch: [27][96/204]	Loss 0.5265 (0.5710)	
training:	Epoch: [27][97/204]	Loss 0.5830 (0.5711)	
training:	Epoch: [27][98/204]	Loss 0.5238 (0.5707)	
training:	Epoch: [27][99/204]	Loss 0.4934 (0.5699)	
training:	Epoch: [27][100/204]	Loss 0.5765 (0.5699)	
training:	Epoch: [27][101/204]	Loss 0.5736 (0.5700)	
training:	Epoch: [27][102/204]	Loss 0.6224 (0.5705)	
training:	Epoch: [27][103/204]	Loss 0.6758 (0.5715)	
training:	Epoch: [27][104/204]	Loss 0.6436 (0.5722)	
training:	Epoch: [27][105/204]	Loss 0.6274 (0.5727)	
training:	Epoch: [27][106/204]	Loss 0.5969 (0.5730)	
training:	Epoch: [27][107/204]	Loss 0.6004 (0.5732)	
training:	Epoch: [27][108/204]	Loss 0.5593 (0.5731)	
training:	Epoch: [27][109/204]	Loss 0.5773 (0.5731)	
training:	Epoch: [27][110/204]	Loss 0.5505 (0.5729)	
training:	Epoch: [27][111/204]	Loss 0.5760 (0.5729)	
training:	Epoch: [27][112/204]	Loss 0.6986 (0.5741)	
training:	Epoch: [27][113/204]	Loss 0.5550 (0.5739)	
training:	Epoch: [27][114/204]	Loss 0.6056 (0.5742)	
training:	Epoch: [27][115/204]	Loss 0.4756 (0.5733)	
training:	Epoch: [27][116/204]	Loss 0.5713 (0.5733)	
training:	Epoch: [27][117/204]	Loss 0.5136 (0.5728)	
training:	Epoch: [27][118/204]	Loss 0.5602 (0.5727)	
training:	Epoch: [27][119/204]	Loss 0.5678 (0.5726)	
training:	Epoch: [27][120/204]	Loss 0.6151 (0.5730)	
training:	Epoch: [27][121/204]	Loss 0.6136 (0.5733)	
training:	Epoch: [27][122/204]	Loss 0.7152 (0.5745)	
training:	Epoch: [27][123/204]	Loss 0.4810 (0.5737)	
training:	Epoch: [27][124/204]	Loss 0.6166 (0.5741)	
training:	Epoch: [27][125/204]	Loss 0.5911 (0.5742)	
training:	Epoch: [27][126/204]	Loss 0.6645 (0.5749)	
training:	Epoch: [27][127/204]	Loss 0.5892 (0.5750)	
training:	Epoch: [27][128/204]	Loss 0.5291 (0.5747)	
training:	Epoch: [27][129/204]	Loss 0.6110 (0.5750)	
training:	Epoch: [27][130/204]	Loss 0.6726 (0.5757)	
training:	Epoch: [27][131/204]	Loss 0.7276 (0.5769)	
training:	Epoch: [27][132/204]	Loss 0.5276 (0.5765)	
training:	Epoch: [27][133/204]	Loss 0.6289 (0.5769)	
training:	Epoch: [27][134/204]	Loss 0.8068 (0.5786)	
training:	Epoch: [27][135/204]	Loss 0.5868 (0.5787)	
training:	Epoch: [27][136/204]	Loss 0.6416 (0.5791)	
training:	Epoch: [27][137/204]	Loss 0.5910 (0.5792)	
training:	Epoch: [27][138/204]	Loss 0.5254 (0.5788)	
training:	Epoch: [27][139/204]	Loss 0.5288 (0.5785)	
training:	Epoch: [27][140/204]	Loss 0.5010 (0.5779)	
training:	Epoch: [27][141/204]	Loss 0.5651 (0.5778)	
training:	Epoch: [27][142/204]	Loss 0.5088 (0.5773)	
training:	Epoch: [27][143/204]	Loss 0.5594 (0.5772)	
training:	Epoch: [27][144/204]	Loss 0.6405 (0.5777)	
training:	Epoch: [27][145/204]	Loss 0.5895 (0.5777)	
training:	Epoch: [27][146/204]	Loss 0.5092 (0.5773)	
training:	Epoch: [27][147/204]	Loss 0.6449 (0.5777)	
training:	Epoch: [27][148/204]	Loss 0.5762 (0.5777)	
training:	Epoch: [27][149/204]	Loss 0.4819 (0.5771)	
training:	Epoch: [27][150/204]	Loss 0.6539 (0.5776)	
training:	Epoch: [27][151/204]	Loss 0.4152 (0.5765)	
training:	Epoch: [27][152/204]	Loss 0.5472 (0.5763)	
training:	Epoch: [27][153/204]	Loss 0.6863 (0.5770)	
training:	Epoch: [27][154/204]	Loss 0.4836 (0.5764)	
training:	Epoch: [27][155/204]	Loss 0.5520 (0.5763)	
training:	Epoch: [27][156/204]	Loss 0.5492 (0.5761)	
training:	Epoch: [27][157/204]	Loss 0.5956 (0.5762)	
training:	Epoch: [27][158/204]	Loss 0.6367 (0.5766)	
training:	Epoch: [27][159/204]	Loss 0.5927 (0.5767)	
training:	Epoch: [27][160/204]	Loss 0.6837 (0.5774)	
training:	Epoch: [27][161/204]	Loss 0.6518 (0.5778)	
training:	Epoch: [27][162/204]	Loss 0.6340 (0.5782)	
training:	Epoch: [27][163/204]	Loss 0.6205 (0.5785)	
training:	Epoch: [27][164/204]	Loss 0.6378 (0.5788)	
training:	Epoch: [27][165/204]	Loss 0.6305 (0.5791)	
training:	Epoch: [27][166/204]	Loss 0.6137 (0.5793)	
training:	Epoch: [27][167/204]	Loss 0.5461 (0.5791)	
training:	Epoch: [27][168/204]	Loss 0.5916 (0.5792)	
training:	Epoch: [27][169/204]	Loss 0.5490 (0.5790)	
training:	Epoch: [27][170/204]	Loss 0.5387 (0.5788)	
training:	Epoch: [27][171/204]	Loss 0.6234 (0.5791)	
training:	Epoch: [27][172/204]	Loss 0.5908 (0.5791)	
training:	Epoch: [27][173/204]	Loss 0.5350 (0.5789)	
training:	Epoch: [27][174/204]	Loss 0.6498 (0.5793)	
training:	Epoch: [27][175/204]	Loss 0.5834 (0.5793)	
training:	Epoch: [27][176/204]	Loss 0.5768 (0.5793)	
training:	Epoch: [27][177/204]	Loss 0.4868 (0.5788)	
training:	Epoch: [27][178/204]	Loss 0.5420 (0.5786)	
training:	Epoch: [27][179/204]	Loss 0.4617 (0.5779)	
training:	Epoch: [27][180/204]	Loss 0.5200 (0.5776)	
training:	Epoch: [27][181/204]	Loss 0.5517 (0.5774)	
training:	Epoch: [27][182/204]	Loss 0.5845 (0.5775)	
training:	Epoch: [27][183/204]	Loss 0.4479 (0.5768)	
training:	Epoch: [27][184/204]	Loss 0.5760 (0.5768)	
training:	Epoch: [27][185/204]	Loss 0.5023 (0.5764)	
training:	Epoch: [27][186/204]	Loss 0.5478 (0.5762)	
training:	Epoch: [27][187/204]	Loss 0.5414 (0.5760)	
training:	Epoch: [27][188/204]	Loss 0.6270 (0.5763)	
training:	Epoch: [27][189/204]	Loss 0.6024 (0.5764)	
training:	Epoch: [27][190/204]	Loss 0.6047 (0.5766)	
training:	Epoch: [27][191/204]	Loss 0.7162 (0.5773)	
training:	Epoch: [27][192/204]	Loss 0.4941 (0.5769)	
training:	Epoch: [27][193/204]	Loss 0.6533 (0.5773)	
training:	Epoch: [27][194/204]	Loss 0.6335 (0.5776)	
training:	Epoch: [27][195/204]	Loss 0.6345 (0.5779)	
training:	Epoch: [27][196/204]	Loss 0.5183 (0.5776)	
training:	Epoch: [27][197/204]	Loss 0.6162 (0.5777)	
training:	Epoch: [27][198/204]	Loss 0.6353 (0.5780)	
training:	Epoch: [27][199/204]	Loss 0.5604 (0.5779)	
training:	Epoch: [27][200/204]	Loss 0.5714 (0.5779)	
training:	Epoch: [27][201/204]	Loss 0.7515 (0.5788)	
training:	Epoch: [27][202/204]	Loss 0.5293 (0.5785)	
training:	Epoch: [27][203/204]	Loss 0.5557 (0.5784)	
training:	Epoch: [27][204/204]	Loss 0.5591 (0.5783)	
Training:	 Loss: 0.5774

Training:	 ACC: 0.7140 0.7152 0.7434 0.6846
Validation:	 ACC: 0.7101 0.7121 0.7554 0.6648
Validation:	 Best_BACC: 0.7101 0.7121 0.7554 0.6648
Validation:	 Loss: 0.5738
Pretraining:	Epoch 28/120
----------
training:	Epoch: [28][1/204]	Loss 0.5666 (0.5666)	
training:	Epoch: [28][2/204]	Loss 0.5343 (0.5505)	
training:	Epoch: [28][3/204]	Loss 0.5132 (0.5381)	
training:	Epoch: [28][4/204]	Loss 0.6045 (0.5547)	
training:	Epoch: [28][5/204]	Loss 0.6264 (0.5690)	
training:	Epoch: [28][6/204]	Loss 0.6055 (0.5751)	
training:	Epoch: [28][7/204]	Loss 0.5474 (0.5711)	
training:	Epoch: [28][8/204]	Loss 0.6375 (0.5794)	
training:	Epoch: [28][9/204]	Loss 0.6179 (0.5837)	
training:	Epoch: [28][10/204]	Loss 0.5731 (0.5826)	
training:	Epoch: [28][11/204]	Loss 0.5596 (0.5806)	
training:	Epoch: [28][12/204]	Loss 0.5734 (0.5800)	
training:	Epoch: [28][13/204]	Loss 0.5642 (0.5787)	
training:	Epoch: [28][14/204]	Loss 0.5811 (0.5789)	
training:	Epoch: [28][15/204]	Loss 0.6220 (0.5818)	
training:	Epoch: [28][16/204]	Loss 0.5625 (0.5806)	
training:	Epoch: [28][17/204]	Loss 0.5016 (0.5759)	
training:	Epoch: [28][18/204]	Loss 0.6162 (0.5782)	
training:	Epoch: [28][19/204]	Loss 0.5385 (0.5761)	
training:	Epoch: [28][20/204]	Loss 0.5638 (0.5755)	
training:	Epoch: [28][21/204]	Loss 0.4942 (0.5716)	
training:	Epoch: [28][22/204]	Loss 0.5336 (0.5699)	
training:	Epoch: [28][23/204]	Loss 0.6293 (0.5725)	
training:	Epoch: [28][24/204]	Loss 0.5513 (0.5716)	
training:	Epoch: [28][25/204]	Loss 0.6384 (0.5742)	
training:	Epoch: [28][26/204]	Loss 0.5749 (0.5743)	
training:	Epoch: [28][27/204]	Loss 0.6184 (0.5759)	
training:	Epoch: [28][28/204]	Loss 0.5265 (0.5741)	
training:	Epoch: [28][29/204]	Loss 0.5448 (0.5731)	
training:	Epoch: [28][30/204]	Loss 0.5877 (0.5736)	
training:	Epoch: [28][31/204]	Loss 0.5936 (0.5743)	
training:	Epoch: [28][32/204]	Loss 0.6956 (0.5781)	
training:	Epoch: [28][33/204]	Loss 0.6385 (0.5799)	
training:	Epoch: [28][34/204]	Loss 0.5399 (0.5787)	
training:	Epoch: [28][35/204]	Loss 0.5534 (0.5780)	
training:	Epoch: [28][36/204]	Loss 0.6169 (0.5791)	
training:	Epoch: [28][37/204]	Loss 0.6086 (0.5799)	
training:	Epoch: [28][38/204]	Loss 0.5991 (0.5804)	
training:	Epoch: [28][39/204]	Loss 0.5635 (0.5799)	
training:	Epoch: [28][40/204]	Loss 0.5756 (0.5798)	
training:	Epoch: [28][41/204]	Loss 0.5146 (0.5782)	
training:	Epoch: [28][42/204]	Loss 0.5374 (0.5773)	
training:	Epoch: [28][43/204]	Loss 0.5191 (0.5759)	
training:	Epoch: [28][44/204]	Loss 0.5574 (0.5755)	
training:	Epoch: [28][45/204]	Loss 0.6620 (0.5774)	
training:	Epoch: [28][46/204]	Loss 0.5839 (0.5776)	
training:	Epoch: [28][47/204]	Loss 0.5657 (0.5773)	
training:	Epoch: [28][48/204]	Loss 0.5971 (0.5777)	
training:	Epoch: [28][49/204]	Loss 0.5268 (0.5767)	
training:	Epoch: [28][50/204]	Loss 0.5966 (0.5771)	
training:	Epoch: [28][51/204]	Loss 0.6332 (0.5782)	
training:	Epoch: [28][52/204]	Loss 0.6098 (0.5788)	
training:	Epoch: [28][53/204]	Loss 0.5115 (0.5775)	
training:	Epoch: [28][54/204]	Loss 0.6345 (0.5786)	
training:	Epoch: [28][55/204]	Loss 0.5908 (0.5788)	
training:	Epoch: [28][56/204]	Loss 0.5444 (0.5782)	
training:	Epoch: [28][57/204]	Loss 0.5313 (0.5774)	
training:	Epoch: [28][58/204]	Loss 0.4980 (0.5760)	
training:	Epoch: [28][59/204]	Loss 0.5312 (0.5752)	
training:	Epoch: [28][60/204]	Loss 0.6106 (0.5758)	
training:	Epoch: [28][61/204]	Loss 0.5948 (0.5761)	
training:	Epoch: [28][62/204]	Loss 0.6609 (0.5775)	
training:	Epoch: [28][63/204]	Loss 0.5845 (0.5776)	
training:	Epoch: [28][64/204]	Loss 0.5672 (0.5774)	
training:	Epoch: [28][65/204]	Loss 0.5957 (0.5777)	
training:	Epoch: [28][66/204]	Loss 0.5991 (0.5780)	
training:	Epoch: [28][67/204]	Loss 0.6469 (0.5791)	
training:	Epoch: [28][68/204]	Loss 0.5539 (0.5787)	
training:	Epoch: [28][69/204]	Loss 0.5809 (0.5787)	
training:	Epoch: [28][70/204]	Loss 0.5511 (0.5783)	
training:	Epoch: [28][71/204]	Loss 0.5104 (0.5774)	
training:	Epoch: [28][72/204]	Loss 0.5570 (0.5771)	
training:	Epoch: [28][73/204]	Loss 0.5466 (0.5767)	
training:	Epoch: [28][74/204]	Loss 0.6715 (0.5780)	
training:	Epoch: [28][75/204]	Loss 0.5285 (0.5773)	
training:	Epoch: [28][76/204]	Loss 0.6041 (0.5777)	
training:	Epoch: [28][77/204]	Loss 0.5612 (0.5774)	
training:	Epoch: [28][78/204]	Loss 0.5759 (0.5774)	
training:	Epoch: [28][79/204]	Loss 0.7147 (0.5792)	
training:	Epoch: [28][80/204]	Loss 0.5392 (0.5787)	
training:	Epoch: [28][81/204]	Loss 0.4538 (0.5771)	
training:	Epoch: [28][82/204]	Loss 0.5337 (0.5766)	
training:	Epoch: [28][83/204]	Loss 0.6273 (0.5772)	
training:	Epoch: [28][84/204]	Loss 0.5058 (0.5764)	
training:	Epoch: [28][85/204]	Loss 0.6271 (0.5770)	
training:	Epoch: [28][86/204]	Loss 0.5160 (0.5762)	
training:	Epoch: [28][87/204]	Loss 0.6190 (0.5767)	
training:	Epoch: [28][88/204]	Loss 0.5012 (0.5759)	
training:	Epoch: [28][89/204]	Loss 0.5526 (0.5756)	
training:	Epoch: [28][90/204]	Loss 0.6085 (0.5760)	
training:	Epoch: [28][91/204]	Loss 0.4572 (0.5747)	
training:	Epoch: [28][92/204]	Loss 0.5246 (0.5741)	
training:	Epoch: [28][93/204]	Loss 0.6126 (0.5745)	
training:	Epoch: [28][94/204]	Loss 0.5428 (0.5742)	
training:	Epoch: [28][95/204]	Loss 0.5889 (0.5744)	
training:	Epoch: [28][96/204]	Loss 0.5104 (0.5737)	
training:	Epoch: [28][97/204]	Loss 0.5267 (0.5732)	
training:	Epoch: [28][98/204]	Loss 0.6853 (0.5744)	
training:	Epoch: [28][99/204]	Loss 0.6213 (0.5748)	
training:	Epoch: [28][100/204]	Loss 0.4732 (0.5738)	
training:	Epoch: [28][101/204]	Loss 0.6557 (0.5746)	
training:	Epoch: [28][102/204]	Loss 0.5907 (0.5748)	
training:	Epoch: [28][103/204]	Loss 0.5621 (0.5747)	
training:	Epoch: [28][104/204]	Loss 0.6383 (0.5753)	
training:	Epoch: [28][105/204]	Loss 0.6713 (0.5762)	
training:	Epoch: [28][106/204]	Loss 0.5428 (0.5759)	
training:	Epoch: [28][107/204]	Loss 0.5246 (0.5754)	
training:	Epoch: [28][108/204]	Loss 0.5997 (0.5756)	
training:	Epoch: [28][109/204]	Loss 0.5328 (0.5752)	
training:	Epoch: [28][110/204]	Loss 0.4743 (0.5743)	
training:	Epoch: [28][111/204]	Loss 0.6764 (0.5752)	
training:	Epoch: [28][112/204]	Loss 0.5230 (0.5748)	
training:	Epoch: [28][113/204]	Loss 0.5481 (0.5745)	
training:	Epoch: [28][114/204]	Loss 0.6108 (0.5748)	
training:	Epoch: [28][115/204]	Loss 0.5791 (0.5749)	
training:	Epoch: [28][116/204]	Loss 0.6219 (0.5753)	
training:	Epoch: [28][117/204]	Loss 0.6046 (0.5755)	
training:	Epoch: [28][118/204]	Loss 0.6190 (0.5759)	
training:	Epoch: [28][119/204]	Loss 0.6124 (0.5762)	
training:	Epoch: [28][120/204]	Loss 0.5519 (0.5760)	
training:	Epoch: [28][121/204]	Loss 0.7138 (0.5771)	
training:	Epoch: [28][122/204]	Loss 0.6449 (0.5777)	
training:	Epoch: [28][123/204]	Loss 0.5320 (0.5773)	
training:	Epoch: [28][124/204]	Loss 0.6491 (0.5779)	
training:	Epoch: [28][125/204]	Loss 0.4411 (0.5768)	
training:	Epoch: [28][126/204]	Loss 0.5121 (0.5763)	
training:	Epoch: [28][127/204]	Loss 0.5943 (0.5764)	
training:	Epoch: [28][128/204]	Loss 0.5621 (0.5763)	
training:	Epoch: [28][129/204]	Loss 0.6847 (0.5772)	
training:	Epoch: [28][130/204]	Loss 0.5323 (0.5768)	
training:	Epoch: [28][131/204]	Loss 0.6428 (0.5773)	
training:	Epoch: [28][132/204]	Loss 0.5402 (0.5770)	
training:	Epoch: [28][133/204]	Loss 0.7290 (0.5782)	
training:	Epoch: [28][134/204]	Loss 0.5774 (0.5782)	
training:	Epoch: [28][135/204]	Loss 0.5887 (0.5783)	
training:	Epoch: [28][136/204]	Loss 0.6629 (0.5789)	
training:	Epoch: [28][137/204]	Loss 0.5268 (0.5785)	
training:	Epoch: [28][138/204]	Loss 0.5271 (0.5781)	
training:	Epoch: [28][139/204]	Loss 0.5773 (0.5781)	
training:	Epoch: [28][140/204]	Loss 0.5514 (0.5779)	
training:	Epoch: [28][141/204]	Loss 0.6801 (0.5787)	
training:	Epoch: [28][142/204]	Loss 0.5279 (0.5783)	
training:	Epoch: [28][143/204]	Loss 0.4698 (0.5775)	
training:	Epoch: [28][144/204]	Loss 0.4630 (0.5767)	
training:	Epoch: [28][145/204]	Loss 0.5891 (0.5768)	
training:	Epoch: [28][146/204]	Loss 0.5963 (0.5770)	
training:	Epoch: [28][147/204]	Loss 0.5649 (0.5769)	
training:	Epoch: [28][148/204]	Loss 0.5231 (0.5765)	
training:	Epoch: [28][149/204]	Loss 0.5322 (0.5762)	
training:	Epoch: [28][150/204]	Loss 0.5537 (0.5761)	
training:	Epoch: [28][151/204]	Loss 0.5112 (0.5756)	
training:	Epoch: [28][152/204]	Loss 0.5763 (0.5756)	
training:	Epoch: [28][153/204]	Loss 0.5221 (0.5753)	
training:	Epoch: [28][154/204]	Loss 0.6259 (0.5756)	
training:	Epoch: [28][155/204]	Loss 0.5412 (0.5754)	
training:	Epoch: [28][156/204]	Loss 0.6107 (0.5756)	
training:	Epoch: [28][157/204]	Loss 0.5173 (0.5753)	
training:	Epoch: [28][158/204]	Loss 0.6633 (0.5758)	
training:	Epoch: [28][159/204]	Loss 0.7365 (0.5768)	
training:	Epoch: [28][160/204]	Loss 0.5026 (0.5764)	
training:	Epoch: [28][161/204]	Loss 0.6723 (0.5770)	
training:	Epoch: [28][162/204]	Loss 0.5819 (0.5770)	
training:	Epoch: [28][163/204]	Loss 0.6100 (0.5772)	
training:	Epoch: [28][164/204]	Loss 0.5090 (0.5768)	
training:	Epoch: [28][165/204]	Loss 0.5272 (0.5765)	
training:	Epoch: [28][166/204]	Loss 0.5733 (0.5765)	
training:	Epoch: [28][167/204]	Loss 0.4404 (0.5756)	
training:	Epoch: [28][168/204]	Loss 0.5428 (0.5754)	
training:	Epoch: [28][169/204]	Loss 0.5841 (0.5755)	
training:	Epoch: [28][170/204]	Loss 0.6167 (0.5757)	
training:	Epoch: [28][171/204]	Loss 0.6689 (0.5763)	
training:	Epoch: [28][172/204]	Loss 0.5726 (0.5763)	
training:	Epoch: [28][173/204]	Loss 0.5763 (0.5763)	
training:	Epoch: [28][174/204]	Loss 0.6388 (0.5766)	
training:	Epoch: [28][175/204]	Loss 0.5423 (0.5764)	
training:	Epoch: [28][176/204]	Loss 0.5252 (0.5761)	
training:	Epoch: [28][177/204]	Loss 0.5806 (0.5762)	
training:	Epoch: [28][178/204]	Loss 0.5964 (0.5763)	
training:	Epoch: [28][179/204]	Loss 0.5819 (0.5763)	
training:	Epoch: [28][180/204]	Loss 0.4903 (0.5758)	
training:	Epoch: [28][181/204]	Loss 0.5113 (0.5755)	
training:	Epoch: [28][182/204]	Loss 0.6049 (0.5756)	
training:	Epoch: [28][183/204]	Loss 0.7243 (0.5764)	
training:	Epoch: [28][184/204]	Loss 0.5440 (0.5763)	
training:	Epoch: [28][185/204]	Loss 0.5258 (0.5760)	
training:	Epoch: [28][186/204]	Loss 0.5277 (0.5757)	
training:	Epoch: [28][187/204]	Loss 0.4829 (0.5752)	
training:	Epoch: [28][188/204]	Loss 0.5562 (0.5751)	
training:	Epoch: [28][189/204]	Loss 0.5744 (0.5751)	
training:	Epoch: [28][190/204]	Loss 0.5085 (0.5748)	
training:	Epoch: [28][191/204]	Loss 0.5876 (0.5748)	
training:	Epoch: [28][192/204]	Loss 0.5406 (0.5747)	
training:	Epoch: [28][193/204]	Loss 0.5789 (0.5747)	
training:	Epoch: [28][194/204]	Loss 0.5653 (0.5746)	
training:	Epoch: [28][195/204]	Loss 0.7020 (0.5753)	
training:	Epoch: [28][196/204]	Loss 0.4721 (0.5748)	
training:	Epoch: [28][197/204]	Loss 0.5610 (0.5747)	
training:	Epoch: [28][198/204]	Loss 0.5640 (0.5746)	
training:	Epoch: [28][199/204]	Loss 0.6200 (0.5749)	
training:	Epoch: [28][200/204]	Loss 0.5822 (0.5749)	
training:	Epoch: [28][201/204]	Loss 0.5441 (0.5748)	
training:	Epoch: [28][202/204]	Loss 0.5706 (0.5747)	
training:	Epoch: [28][203/204]	Loss 0.5236 (0.5745)	
training:	Epoch: [28][204/204]	Loss 0.5656 (0.5744)	
Training:	 Loss: 0.5736

Training:	 ACC: 0.7168 0.7180 0.7449 0.6888
Validation:	 ACC: 0.7140 0.7159 0.7554 0.6726
Validation:	 Best_BACC: 0.7140 0.7159 0.7554 0.6726
Validation:	 Loss: 0.5691
Pretraining:	Epoch 29/120
----------
training:	Epoch: [29][1/204]	Loss 0.5507 (0.5507)	
training:	Epoch: [29][2/204]	Loss 0.5072 (0.5289)	
training:	Epoch: [29][3/204]	Loss 0.5727 (0.5435)	
training:	Epoch: [29][4/204]	Loss 0.6275 (0.5645)	
training:	Epoch: [29][5/204]	Loss 0.4750 (0.5466)	
training:	Epoch: [29][6/204]	Loss 0.5280 (0.5435)	
training:	Epoch: [29][7/204]	Loss 0.5796 (0.5487)	
training:	Epoch: [29][8/204]	Loss 0.5362 (0.5471)	
training:	Epoch: [29][9/204]	Loss 0.5717 (0.5498)	
training:	Epoch: [29][10/204]	Loss 0.5378 (0.5486)	
training:	Epoch: [29][11/204]	Loss 0.5051 (0.5447)	
training:	Epoch: [29][12/204]	Loss 0.5006 (0.5410)	
training:	Epoch: [29][13/204]	Loss 0.7151 (0.5544)	
training:	Epoch: [29][14/204]	Loss 0.5359 (0.5531)	
training:	Epoch: [29][15/204]	Loss 0.4523 (0.5464)	
training:	Epoch: [29][16/204]	Loss 0.6299 (0.5516)	
training:	Epoch: [29][17/204]	Loss 0.5300 (0.5503)	
training:	Epoch: [29][18/204]	Loss 0.7305 (0.5603)	
training:	Epoch: [29][19/204]	Loss 0.5416 (0.5593)	
training:	Epoch: [29][20/204]	Loss 0.5905 (0.5609)	
training:	Epoch: [29][21/204]	Loss 0.5650 (0.5611)	
training:	Epoch: [29][22/204]	Loss 0.5428 (0.5603)	
training:	Epoch: [29][23/204]	Loss 0.5205 (0.5585)	
training:	Epoch: [29][24/204]	Loss 0.6632 (0.5629)	
training:	Epoch: [29][25/204]	Loss 0.6840 (0.5677)	
training:	Epoch: [29][26/204]	Loss 0.5058 (0.5653)	
training:	Epoch: [29][27/204]	Loss 0.5621 (0.5652)	
training:	Epoch: [29][28/204]	Loss 0.5287 (0.5639)	
training:	Epoch: [29][29/204]	Loss 0.5111 (0.5621)	
training:	Epoch: [29][30/204]	Loss 0.5128 (0.5605)	
training:	Epoch: [29][31/204]	Loss 0.5658 (0.5606)	
training:	Epoch: [29][32/204]	Loss 0.7090 (0.5653)	
training:	Epoch: [29][33/204]	Loss 0.4526 (0.5619)	
training:	Epoch: [29][34/204]	Loss 0.5962 (0.5629)	
training:	Epoch: [29][35/204]	Loss 0.5955 (0.5638)	
training:	Epoch: [29][36/204]	Loss 0.4647 (0.5610)	
training:	Epoch: [29][37/204]	Loss 0.5676 (0.5612)	
training:	Epoch: [29][38/204]	Loss 0.6565 (0.5637)	
training:	Epoch: [29][39/204]	Loss 0.5889 (0.5644)	
training:	Epoch: [29][40/204]	Loss 0.5407 (0.5638)	
training:	Epoch: [29][41/204]	Loss 0.6416 (0.5657)	
training:	Epoch: [29][42/204]	Loss 0.5548 (0.5654)	
training:	Epoch: [29][43/204]	Loss 0.4796 (0.5634)	
training:	Epoch: [29][44/204]	Loss 0.5662 (0.5635)	
training:	Epoch: [29][45/204]	Loss 0.6040 (0.5644)	
training:	Epoch: [29][46/204]	Loss 0.5855 (0.5648)	
training:	Epoch: [29][47/204]	Loss 0.6349 (0.5663)	
training:	Epoch: [29][48/204]	Loss 0.6631 (0.5684)	
training:	Epoch: [29][49/204]	Loss 0.4744 (0.5664)	
training:	Epoch: [29][50/204]	Loss 0.5060 (0.5652)	
training:	Epoch: [29][51/204]	Loss 0.5773 (0.5655)	
training:	Epoch: [29][52/204]	Loss 0.6114 (0.5663)	
training:	Epoch: [29][53/204]	Loss 0.5196 (0.5655)	
training:	Epoch: [29][54/204]	Loss 0.6091 (0.5663)	
training:	Epoch: [29][55/204]	Loss 0.6998 (0.5687)	
training:	Epoch: [29][56/204]	Loss 0.6073 (0.5694)	
training:	Epoch: [29][57/204]	Loss 0.4952 (0.5681)	
training:	Epoch: [29][58/204]	Loss 0.5425 (0.5676)	
training:	Epoch: [29][59/204]	Loss 0.5368 (0.5671)	
training:	Epoch: [29][60/204]	Loss 0.5636 (0.5671)	
training:	Epoch: [29][61/204]	Loss 0.6745 (0.5688)	
training:	Epoch: [29][62/204]	Loss 0.4986 (0.5677)	
training:	Epoch: [29][63/204]	Loss 0.4927 (0.5665)	
training:	Epoch: [29][64/204]	Loss 0.6087 (0.5672)	
training:	Epoch: [29][65/204]	Loss 0.5159 (0.5664)	
training:	Epoch: [29][66/204]	Loss 0.5286 (0.5658)	
training:	Epoch: [29][67/204]	Loss 0.6936 (0.5677)	
training:	Epoch: [29][68/204]	Loss 0.7412 (0.5703)	
training:	Epoch: [29][69/204]	Loss 0.5457 (0.5699)	
training:	Epoch: [29][70/204]	Loss 0.4935 (0.5688)	
training:	Epoch: [29][71/204]	Loss 0.5828 (0.5690)	
training:	Epoch: [29][72/204]	Loss 0.6124 (0.5696)	
training:	Epoch: [29][73/204]	Loss 0.5884 (0.5699)	
training:	Epoch: [29][74/204]	Loss 0.5820 (0.5700)	
training:	Epoch: [29][75/204]	Loss 0.6772 (0.5715)	
training:	Epoch: [29][76/204]	Loss 0.6335 (0.5723)	
training:	Epoch: [29][77/204]	Loss 0.6891 (0.5738)	
training:	Epoch: [29][78/204]	Loss 0.4496 (0.5722)	
training:	Epoch: [29][79/204]	Loss 0.7332 (0.5742)	
training:	Epoch: [29][80/204]	Loss 0.6015 (0.5746)	
training:	Epoch: [29][81/204]	Loss 0.5770 (0.5746)	
training:	Epoch: [29][82/204]	Loss 0.5906 (0.5748)	
training:	Epoch: [29][83/204]	Loss 0.5757 (0.5748)	
training:	Epoch: [29][84/204]	Loss 0.5313 (0.5743)	
training:	Epoch: [29][85/204]	Loss 0.6160 (0.5748)	
training:	Epoch: [29][86/204]	Loss 0.5673 (0.5747)	
training:	Epoch: [29][87/204]	Loss 0.4842 (0.5737)	
training:	Epoch: [29][88/204]	Loss 0.6227 (0.5742)	
training:	Epoch: [29][89/204]	Loss 0.6443 (0.5750)	
training:	Epoch: [29][90/204]	Loss 0.5654 (0.5749)	
training:	Epoch: [29][91/204]	Loss 0.5529 (0.5747)	
training:	Epoch: [29][92/204]	Loss 0.6122 (0.5751)	
training:	Epoch: [29][93/204]	Loss 0.5448 (0.5747)	
training:	Epoch: [29][94/204]	Loss 0.5444 (0.5744)	
training:	Epoch: [29][95/204]	Loss 0.5362 (0.5740)	
training:	Epoch: [29][96/204]	Loss 0.5278 (0.5735)	
training:	Epoch: [29][97/204]	Loss 0.5583 (0.5734)	
training:	Epoch: [29][98/204]	Loss 0.5561 (0.5732)	
training:	Epoch: [29][99/204]	Loss 0.5440 (0.5729)	
training:	Epoch: [29][100/204]	Loss 0.6134 (0.5733)	
training:	Epoch: [29][101/204]	Loss 0.5017 (0.5726)	
training:	Epoch: [29][102/204]	Loss 0.4941 (0.5718)	
training:	Epoch: [29][103/204]	Loss 0.6413 (0.5725)	
training:	Epoch: [29][104/204]	Loss 0.5642 (0.5724)	
training:	Epoch: [29][105/204]	Loss 0.5649 (0.5724)	
training:	Epoch: [29][106/204]	Loss 0.5548 (0.5722)	
training:	Epoch: [29][107/204]	Loss 0.5945 (0.5724)	
training:	Epoch: [29][108/204]	Loss 0.6133 (0.5728)	
training:	Epoch: [29][109/204]	Loss 0.5971 (0.5730)	
training:	Epoch: [29][110/204]	Loss 0.5448 (0.5727)	
training:	Epoch: [29][111/204]	Loss 0.5540 (0.5726)	
training:	Epoch: [29][112/204]	Loss 0.5210 (0.5721)	
training:	Epoch: [29][113/204]	Loss 0.4876 (0.5714)	
training:	Epoch: [29][114/204]	Loss 0.6901 (0.5724)	
training:	Epoch: [29][115/204]	Loss 0.6529 (0.5731)	
training:	Epoch: [29][116/204]	Loss 0.6415 (0.5737)	
training:	Epoch: [29][117/204]	Loss 0.4650 (0.5728)	
training:	Epoch: [29][118/204]	Loss 0.5988 (0.5730)	
training:	Epoch: [29][119/204]	Loss 0.5902 (0.5731)	
training:	Epoch: [29][120/204]	Loss 0.4798 (0.5724)	
training:	Epoch: [29][121/204]	Loss 0.5338 (0.5720)	
training:	Epoch: [29][122/204]	Loss 0.5715 (0.5720)	
training:	Epoch: [29][123/204]	Loss 0.5700 (0.5720)	
training:	Epoch: [29][124/204]	Loss 0.5832 (0.5721)	
training:	Epoch: [29][125/204]	Loss 0.6895 (0.5730)	
training:	Epoch: [29][126/204]	Loss 0.5097 (0.5725)	
training:	Epoch: [29][127/204]	Loss 0.5941 (0.5727)	
training:	Epoch: [29][128/204]	Loss 0.5415 (0.5725)	
training:	Epoch: [29][129/204]	Loss 0.5358 (0.5722)	
training:	Epoch: [29][130/204]	Loss 0.5540 (0.5720)	
training:	Epoch: [29][131/204]	Loss 0.6529 (0.5727)	
training:	Epoch: [29][132/204]	Loss 0.6418 (0.5732)	
training:	Epoch: [29][133/204]	Loss 0.5891 (0.5733)	
training:	Epoch: [29][134/204]	Loss 0.5324 (0.5730)	
training:	Epoch: [29][135/204]	Loss 0.6105 (0.5733)	
training:	Epoch: [29][136/204]	Loss 0.5212 (0.5729)	
training:	Epoch: [29][137/204]	Loss 0.6445 (0.5734)	
training:	Epoch: [29][138/204]	Loss 0.5021 (0.5729)	
training:	Epoch: [29][139/204]	Loss 0.6214 (0.5732)	
training:	Epoch: [29][140/204]	Loss 0.5314 (0.5730)	
training:	Epoch: [29][141/204]	Loss 0.5618 (0.5729)	
training:	Epoch: [29][142/204]	Loss 0.6120 (0.5731)	
training:	Epoch: [29][143/204]	Loss 0.5584 (0.5730)	
training:	Epoch: [29][144/204]	Loss 0.5545 (0.5729)	
training:	Epoch: [29][145/204]	Loss 0.6386 (0.5734)	
training:	Epoch: [29][146/204]	Loss 0.5485 (0.5732)	
training:	Epoch: [29][147/204]	Loss 0.5167 (0.5728)	
training:	Epoch: [29][148/204]	Loss 0.4899 (0.5723)	
training:	Epoch: [29][149/204]	Loss 0.6434 (0.5727)	
training:	Epoch: [29][150/204]	Loss 0.5393 (0.5725)	
training:	Epoch: [29][151/204]	Loss 0.7032 (0.5734)	
training:	Epoch: [29][152/204]	Loss 0.5054 (0.5729)	
training:	Epoch: [29][153/204]	Loss 0.6204 (0.5732)	
training:	Epoch: [29][154/204]	Loss 0.4551 (0.5725)	
training:	Epoch: [29][155/204]	Loss 0.5432 (0.5723)	
training:	Epoch: [29][156/204]	Loss 0.5378 (0.5721)	
training:	Epoch: [29][157/204]	Loss 0.5534 (0.5719)	
training:	Epoch: [29][158/204]	Loss 0.5698 (0.5719)	
training:	Epoch: [29][159/204]	Loss 0.5857 (0.5720)	
training:	Epoch: [29][160/204]	Loss 0.6484 (0.5725)	
training:	Epoch: [29][161/204]	Loss 0.6465 (0.5730)	
training:	Epoch: [29][162/204]	Loss 0.6241 (0.5733)	
training:	Epoch: [29][163/204]	Loss 0.5535 (0.5731)	
training:	Epoch: [29][164/204]	Loss 0.5062 (0.5727)	
training:	Epoch: [29][165/204]	Loss 0.5243 (0.5724)	
training:	Epoch: [29][166/204]	Loss 0.6348 (0.5728)	
training:	Epoch: [29][167/204]	Loss 0.5464 (0.5727)	
training:	Epoch: [29][168/204]	Loss 0.5518 (0.5725)	
training:	Epoch: [29][169/204]	Loss 0.5890 (0.5726)	
training:	Epoch: [29][170/204]	Loss 0.4916 (0.5722)	
training:	Epoch: [29][171/204]	Loss 0.6752 (0.5728)	
training:	Epoch: [29][172/204]	Loss 0.4649 (0.5721)	
training:	Epoch: [29][173/204]	Loss 0.6500 (0.5726)	
training:	Epoch: [29][174/204]	Loss 0.5931 (0.5727)	
training:	Epoch: [29][175/204]	Loss 0.5501 (0.5726)	
training:	Epoch: [29][176/204]	Loss 0.5325 (0.5723)	
training:	Epoch: [29][177/204]	Loss 0.6542 (0.5728)	
training:	Epoch: [29][178/204]	Loss 0.5314 (0.5726)	
training:	Epoch: [29][179/204]	Loss 0.5843 (0.5726)	
training:	Epoch: [29][180/204]	Loss 0.6077 (0.5728)	
training:	Epoch: [29][181/204]	Loss 0.5943 (0.5730)	
training:	Epoch: [29][182/204]	Loss 0.5063 (0.5726)	
training:	Epoch: [29][183/204]	Loss 0.6066 (0.5728)	
training:	Epoch: [29][184/204]	Loss 0.5749 (0.5728)	
training:	Epoch: [29][185/204]	Loss 0.5381 (0.5726)	
training:	Epoch: [29][186/204]	Loss 0.5053 (0.5722)	
training:	Epoch: [29][187/204]	Loss 0.5867 (0.5723)	
training:	Epoch: [29][188/204]	Loss 0.5744 (0.5723)	
training:	Epoch: [29][189/204]	Loss 0.5981 (0.5725)	
training:	Epoch: [29][190/204]	Loss 0.6003 (0.5726)	
training:	Epoch: [29][191/204]	Loss 0.5057 (0.5723)	
training:	Epoch: [29][192/204]	Loss 0.6726 (0.5728)	
training:	Epoch: [29][193/204]	Loss 0.5682 (0.5728)	
training:	Epoch: [29][194/204]	Loss 0.5190 (0.5725)	
training:	Epoch: [29][195/204]	Loss 0.4944 (0.5721)	
training:	Epoch: [29][196/204]	Loss 0.5498 (0.5720)	
training:	Epoch: [29][197/204]	Loss 0.6565 (0.5724)	
training:	Epoch: [29][198/204]	Loss 0.5671 (0.5724)	
training:	Epoch: [29][199/204]	Loss 0.5363 (0.5722)	
training:	Epoch: [29][200/204]	Loss 0.5637 (0.5721)	
training:	Epoch: [29][201/204]	Loss 0.5499 (0.5720)	
training:	Epoch: [29][202/204]	Loss 0.5852 (0.5721)	
training:	Epoch: [29][203/204]	Loss 0.5285 (0.5719)	
training:	Epoch: [29][204/204]	Loss 0.5849 (0.5719)	
Training:	 Loss: 0.5711

Training:	 ACC: 0.7206 0.7216 0.7451 0.6961
Validation:	 ACC: 0.7186 0.7202 0.7523 0.6850
Validation:	 Best_BACC: 0.7186 0.7202 0.7523 0.6850
Validation:	 Loss: 0.5653
Pretraining:	Epoch 30/120
----------
training:	Epoch: [30][1/204]	Loss 0.5675 (0.5675)	
training:	Epoch: [30][2/204]	Loss 0.5326 (0.5500)	
training:	Epoch: [30][3/204]	Loss 0.6550 (0.5850)	
training:	Epoch: [30][4/204]	Loss 0.7191 (0.6185)	
training:	Epoch: [30][5/204]	Loss 0.4448 (0.5838)	
training:	Epoch: [30][6/204]	Loss 0.5358 (0.5758)	
training:	Epoch: [30][7/204]	Loss 0.5728 (0.5754)	
training:	Epoch: [30][8/204]	Loss 0.6377 (0.5832)	
training:	Epoch: [30][9/204]	Loss 0.5572 (0.5803)	
training:	Epoch: [30][10/204]	Loss 0.5701 (0.5793)	
training:	Epoch: [30][11/204]	Loss 0.6360 (0.5844)	
training:	Epoch: [30][12/204]	Loss 0.6044 (0.5861)	
training:	Epoch: [30][13/204]	Loss 0.5567 (0.5838)	
training:	Epoch: [30][14/204]	Loss 0.5074 (0.5784)	
training:	Epoch: [30][15/204]	Loss 0.5973 (0.5796)	
training:	Epoch: [30][16/204]	Loss 0.4659 (0.5725)	
training:	Epoch: [30][17/204]	Loss 0.6301 (0.5759)	
training:	Epoch: [30][18/204]	Loss 0.5678 (0.5755)	
training:	Epoch: [30][19/204]	Loss 0.6040 (0.5770)	
training:	Epoch: [30][20/204]	Loss 0.6745 (0.5818)	
training:	Epoch: [30][21/204]	Loss 0.5666 (0.5811)	
training:	Epoch: [30][22/204]	Loss 0.6739 (0.5853)	
training:	Epoch: [30][23/204]	Loss 0.5262 (0.5828)	
training:	Epoch: [30][24/204]	Loss 0.5203 (0.5802)	
training:	Epoch: [30][25/204]	Loss 0.6225 (0.5819)	
training:	Epoch: [30][26/204]	Loss 0.5735 (0.5815)	
training:	Epoch: [30][27/204]	Loss 0.5440 (0.5801)	
training:	Epoch: [30][28/204]	Loss 0.6020 (0.5809)	
training:	Epoch: [30][29/204]	Loss 0.4991 (0.5781)	
training:	Epoch: [30][30/204]	Loss 0.5114 (0.5759)	
training:	Epoch: [30][31/204]	Loss 0.5842 (0.5761)	
training:	Epoch: [30][32/204]	Loss 0.5469 (0.5752)	
training:	Epoch: [30][33/204]	Loss 0.4585 (0.5717)	
training:	Epoch: [30][34/204]	Loss 0.5508 (0.5711)	
training:	Epoch: [30][35/204]	Loss 0.5773 (0.5713)	
training:	Epoch: [30][36/204]	Loss 0.5466 (0.5706)	
training:	Epoch: [30][37/204]	Loss 0.6854 (0.5737)	
training:	Epoch: [30][38/204]	Loss 0.5066 (0.5719)	
training:	Epoch: [30][39/204]	Loss 0.6522 (0.5740)	
training:	Epoch: [30][40/204]	Loss 0.5580 (0.5736)	
training:	Epoch: [30][41/204]	Loss 0.5647 (0.5734)	
training:	Epoch: [30][42/204]	Loss 0.5050 (0.5717)	
training:	Epoch: [30][43/204]	Loss 0.5256 (0.5707)	
training:	Epoch: [30][44/204]	Loss 0.6356 (0.5721)	
training:	Epoch: [30][45/204]	Loss 0.5252 (0.5711)	
training:	Epoch: [30][46/204]	Loss 0.6024 (0.5718)	
training:	Epoch: [30][47/204]	Loss 0.5398 (0.5711)	
training:	Epoch: [30][48/204]	Loss 0.5587 (0.5708)	
training:	Epoch: [30][49/204]	Loss 0.6094 (0.5716)	
training:	Epoch: [30][50/204]	Loss 0.5039 (0.5703)	
training:	Epoch: [30][51/204]	Loss 0.5859 (0.5706)	
training:	Epoch: [30][52/204]	Loss 0.5727 (0.5706)	
training:	Epoch: [30][53/204]	Loss 0.4777 (0.5689)	
training:	Epoch: [30][54/204]	Loss 0.5860 (0.5692)	
training:	Epoch: [30][55/204]	Loss 0.4760 (0.5675)	
training:	Epoch: [30][56/204]	Loss 0.5443 (0.5671)	
training:	Epoch: [30][57/204]	Loss 0.4200 (0.5645)	
training:	Epoch: [30][58/204]	Loss 0.5460 (0.5642)	
training:	Epoch: [30][59/204]	Loss 0.6404 (0.5655)	
training:	Epoch: [30][60/204]	Loss 0.5658 (0.5655)	
training:	Epoch: [30][61/204]	Loss 0.4782 (0.5640)	
training:	Epoch: [30][62/204]	Loss 0.5208 (0.5633)	
training:	Epoch: [30][63/204]	Loss 0.5722 (0.5635)	
training:	Epoch: [30][64/204]	Loss 0.5517 (0.5633)	
training:	Epoch: [30][65/204]	Loss 0.5344 (0.5628)	
training:	Epoch: [30][66/204]	Loss 0.6166 (0.5637)	
training:	Epoch: [30][67/204]	Loss 0.7444 (0.5664)	
training:	Epoch: [30][68/204]	Loss 0.6394 (0.5674)	
training:	Epoch: [30][69/204]	Loss 0.5729 (0.5675)	
training:	Epoch: [30][70/204]	Loss 0.5523 (0.5673)	
training:	Epoch: [30][71/204]	Loss 0.5807 (0.5675)	
training:	Epoch: [30][72/204]	Loss 0.5405 (0.5671)	
training:	Epoch: [30][73/204]	Loss 0.5057 (0.5663)	
training:	Epoch: [30][74/204]	Loss 0.6373 (0.5672)	
training:	Epoch: [30][75/204]	Loss 0.6346 (0.5681)	
training:	Epoch: [30][76/204]	Loss 0.4835 (0.5670)	
training:	Epoch: [30][77/204]	Loss 0.5105 (0.5663)	
training:	Epoch: [30][78/204]	Loss 0.7612 (0.5688)	
training:	Epoch: [30][79/204]	Loss 0.4492 (0.5673)	
training:	Epoch: [30][80/204]	Loss 0.4535 (0.5658)	
training:	Epoch: [30][81/204]	Loss 0.5456 (0.5656)	
training:	Epoch: [30][82/204]	Loss 0.5155 (0.5650)	
training:	Epoch: [30][83/204]	Loss 0.6018 (0.5654)	
training:	Epoch: [30][84/204]	Loss 0.6158 (0.5660)	
training:	Epoch: [30][85/204]	Loss 0.5923 (0.5663)	
training:	Epoch: [30][86/204]	Loss 0.5074 (0.5656)	
training:	Epoch: [30][87/204]	Loss 0.5593 (0.5656)	
training:	Epoch: [30][88/204]	Loss 0.5686 (0.5656)	
training:	Epoch: [30][89/204]	Loss 0.4754 (0.5646)	
training:	Epoch: [30][90/204]	Loss 0.5208 (0.5641)	
training:	Epoch: [30][91/204]	Loss 0.4959 (0.5634)	
training:	Epoch: [30][92/204]	Loss 0.5401 (0.5631)	
training:	Epoch: [30][93/204]	Loss 0.6502 (0.5640)	
training:	Epoch: [30][94/204]	Loss 0.6625 (0.5651)	
training:	Epoch: [30][95/204]	Loss 0.5336 (0.5648)	
training:	Epoch: [30][96/204]	Loss 0.6561 (0.5657)	
training:	Epoch: [30][97/204]	Loss 0.4715 (0.5647)	
training:	Epoch: [30][98/204]	Loss 0.5209 (0.5643)	
training:	Epoch: [30][99/204]	Loss 0.5088 (0.5637)	
training:	Epoch: [30][100/204]	Loss 0.5133 (0.5632)	
training:	Epoch: [30][101/204]	Loss 0.4887 (0.5625)	
training:	Epoch: [30][102/204]	Loss 0.6085 (0.5629)	
training:	Epoch: [30][103/204]	Loss 0.4649 (0.5620)	
training:	Epoch: [30][104/204]	Loss 0.5990 (0.5623)	
training:	Epoch: [30][105/204]	Loss 0.5459 (0.5622)	
training:	Epoch: [30][106/204]	Loss 0.4916 (0.5615)	
training:	Epoch: [30][107/204]	Loss 0.5692 (0.5616)	
training:	Epoch: [30][108/204]	Loss 0.5702 (0.5617)	
training:	Epoch: [30][109/204]	Loss 0.4948 (0.5611)	
training:	Epoch: [30][110/204]	Loss 0.4950 (0.5605)	
training:	Epoch: [30][111/204]	Loss 0.5173 (0.5601)	
training:	Epoch: [30][112/204]	Loss 0.5510 (0.5600)	
training:	Epoch: [30][113/204]	Loss 0.4703 (0.5592)	
training:	Epoch: [30][114/204]	Loss 0.5519 (0.5591)	
training:	Epoch: [30][115/204]	Loss 0.6198 (0.5597)	
training:	Epoch: [30][116/204]	Loss 0.5105 (0.5592)	
training:	Epoch: [30][117/204]	Loss 0.4710 (0.5585)	
training:	Epoch: [30][118/204]	Loss 0.5033 (0.5580)	
training:	Epoch: [30][119/204]	Loss 0.6020 (0.5584)	
training:	Epoch: [30][120/204]	Loss 0.5348 (0.5582)	
training:	Epoch: [30][121/204]	Loss 0.5832 (0.5584)	
training:	Epoch: [30][122/204]	Loss 0.5035 (0.5579)	
training:	Epoch: [30][123/204]	Loss 0.6262 (0.5585)	
training:	Epoch: [30][124/204]	Loss 0.6110 (0.5589)	
training:	Epoch: [30][125/204]	Loss 0.6120 (0.5593)	
training:	Epoch: [30][126/204]	Loss 0.5261 (0.5591)	
training:	Epoch: [30][127/204]	Loss 0.6605 (0.5599)	
training:	Epoch: [30][128/204]	Loss 0.6773 (0.5608)	
training:	Epoch: [30][129/204]	Loss 0.7195 (0.5620)	
training:	Epoch: [30][130/204]	Loss 0.5399 (0.5619)	
training:	Epoch: [30][131/204]	Loss 0.6176 (0.5623)	
training:	Epoch: [30][132/204]	Loss 0.5646 (0.5623)	
training:	Epoch: [30][133/204]	Loss 0.5209 (0.5620)	
training:	Epoch: [30][134/204]	Loss 0.6062 (0.5623)	
training:	Epoch: [30][135/204]	Loss 0.5123 (0.5620)	
training:	Epoch: [30][136/204]	Loss 0.6003 (0.5622)	
training:	Epoch: [30][137/204]	Loss 0.5583 (0.5622)	
training:	Epoch: [30][138/204]	Loss 0.6336 (0.5627)	
training:	Epoch: [30][139/204]	Loss 0.5930 (0.5629)	
training:	Epoch: [30][140/204]	Loss 0.5155 (0.5626)	
training:	Epoch: [30][141/204]	Loss 0.6578 (0.5633)	
training:	Epoch: [30][142/204]	Loss 0.6732 (0.5641)	
training:	Epoch: [30][143/204]	Loss 0.6020 (0.5643)	
training:	Epoch: [30][144/204]	Loss 0.6275 (0.5648)	
training:	Epoch: [30][145/204]	Loss 0.6344 (0.5652)	
training:	Epoch: [30][146/204]	Loss 0.5357 (0.5650)	
training:	Epoch: [30][147/204]	Loss 0.5377 (0.5648)	
training:	Epoch: [30][148/204]	Loss 0.4372 (0.5640)	
training:	Epoch: [30][149/204]	Loss 0.6792 (0.5648)	
training:	Epoch: [30][150/204]	Loss 0.4864 (0.5642)	
training:	Epoch: [30][151/204]	Loss 0.6157 (0.5646)	
training:	Epoch: [30][152/204]	Loss 0.6182 (0.5649)	
training:	Epoch: [30][153/204]	Loss 0.6668 (0.5656)	
training:	Epoch: [30][154/204]	Loss 0.5238 (0.5653)	
training:	Epoch: [30][155/204]	Loss 0.6336 (0.5658)	
training:	Epoch: [30][156/204]	Loss 0.5868 (0.5659)	
training:	Epoch: [30][157/204]	Loss 0.6199 (0.5662)	
training:	Epoch: [30][158/204]	Loss 0.5125 (0.5659)	
training:	Epoch: [30][159/204]	Loss 0.5518 (0.5658)	
training:	Epoch: [30][160/204]	Loss 0.6019 (0.5660)	
training:	Epoch: [30][161/204]	Loss 0.4539 (0.5653)	
training:	Epoch: [30][162/204]	Loss 0.5143 (0.5650)	
training:	Epoch: [30][163/204]	Loss 0.5166 (0.5647)	
training:	Epoch: [30][164/204]	Loss 0.5701 (0.5648)	
training:	Epoch: [30][165/204]	Loss 0.4818 (0.5643)	
training:	Epoch: [30][166/204]	Loss 0.5675 (0.5643)	
training:	Epoch: [30][167/204]	Loss 0.5766 (0.5644)	
training:	Epoch: [30][168/204]	Loss 0.5393 (0.5642)	
training:	Epoch: [30][169/204]	Loss 0.5839 (0.5643)	
training:	Epoch: [30][170/204]	Loss 0.5173 (0.5640)	
training:	Epoch: [30][171/204]	Loss 0.5701 (0.5641)	
training:	Epoch: [30][172/204]	Loss 0.4691 (0.5635)	
training:	Epoch: [30][173/204]	Loss 0.5657 (0.5635)	
training:	Epoch: [30][174/204]	Loss 0.7384 (0.5645)	
training:	Epoch: [30][175/204]	Loss 0.6838 (0.5652)	
training:	Epoch: [30][176/204]	Loss 0.5042 (0.5649)	
training:	Epoch: [30][177/204]	Loss 0.5151 (0.5646)	
training:	Epoch: [30][178/204]	Loss 0.4937 (0.5642)	
training:	Epoch: [30][179/204]	Loss 0.5457 (0.5641)	
training:	Epoch: [30][180/204]	Loss 0.6248 (0.5644)	
training:	Epoch: [30][181/204]	Loss 0.5511 (0.5644)	
training:	Epoch: [30][182/204]	Loss 0.6601 (0.5649)	
training:	Epoch: [30][183/204]	Loss 0.6075 (0.5651)	
training:	Epoch: [30][184/204]	Loss 0.4844 (0.5647)	
training:	Epoch: [30][185/204]	Loss 0.5605 (0.5647)	
training:	Epoch: [30][186/204]	Loss 0.4766 (0.5642)	
training:	Epoch: [30][187/204]	Loss 0.5938 (0.5643)	
training:	Epoch: [30][188/204]	Loss 0.5872 (0.5645)	
training:	Epoch: [30][189/204]	Loss 0.4979 (0.5641)	
training:	Epoch: [30][190/204]	Loss 0.5044 (0.5638)	
training:	Epoch: [30][191/204]	Loss 0.5745 (0.5639)	
training:	Epoch: [30][192/204]	Loss 0.6659 (0.5644)	
training:	Epoch: [30][193/204]	Loss 0.6001 (0.5646)	
training:	Epoch: [30][194/204]	Loss 0.5258 (0.5644)	
training:	Epoch: [30][195/204]	Loss 0.4504 (0.5638)	
training:	Epoch: [30][196/204]	Loss 0.5186 (0.5636)	
training:	Epoch: [30][197/204]	Loss 0.6539 (0.5640)	
training:	Epoch: [30][198/204]	Loss 0.4534 (0.5635)	
training:	Epoch: [30][199/204]	Loss 0.5582 (0.5634)	
training:	Epoch: [30][200/204]	Loss 0.5055 (0.5631)	
training:	Epoch: [30][201/204]	Loss 0.5473 (0.5631)	
training:	Epoch: [30][202/204]	Loss 0.5476 (0.5630)	
training:	Epoch: [30][203/204]	Loss 0.6568 (0.5634)	
training:	Epoch: [30][204/204]	Loss 0.5615 (0.5634)	
Training:	 Loss: 0.5626

Training:	 ACC: 0.7215 0.7229 0.7537 0.6894
Validation:	 ACC: 0.7202 0.7223 0.7666 0.6738
Validation:	 Best_BACC: 0.7202 0.7223 0.7666 0.6738
Validation:	 Loss: 0.5607
Pretraining:	Epoch 31/120
----------
training:	Epoch: [31][1/204]	Loss 0.6314 (0.6314)	
training:	Epoch: [31][2/204]	Loss 0.5133 (0.5723)	
training:	Epoch: [31][3/204]	Loss 0.7180 (0.6209)	
training:	Epoch: [31][4/204]	Loss 0.5505 (0.6033)	
training:	Epoch: [31][5/204]	Loss 0.5831 (0.5993)	
training:	Epoch: [31][6/204]	Loss 0.5561 (0.5921)	
training:	Epoch: [31][7/204]	Loss 0.5825 (0.5907)	
training:	Epoch: [31][8/204]	Loss 0.6589 (0.5992)	
training:	Epoch: [31][9/204]	Loss 0.6469 (0.6045)	
training:	Epoch: [31][10/204]	Loss 0.5292 (0.5970)	
training:	Epoch: [31][11/204]	Loss 0.4943 (0.5877)	
training:	Epoch: [31][12/204]	Loss 0.5781 (0.5869)	
training:	Epoch: [31][13/204]	Loss 0.5788 (0.5862)	
training:	Epoch: [31][14/204]	Loss 0.7541 (0.5982)	
training:	Epoch: [31][15/204]	Loss 0.5214 (0.5931)	
training:	Epoch: [31][16/204]	Loss 0.5684 (0.5916)	
training:	Epoch: [31][17/204]	Loss 0.6401 (0.5944)	
training:	Epoch: [31][18/204]	Loss 0.5462 (0.5917)	
training:	Epoch: [31][19/204]	Loss 0.6742 (0.5961)	
training:	Epoch: [31][20/204]	Loss 0.6105 (0.5968)	
training:	Epoch: [31][21/204]	Loss 0.3749 (0.5862)	
training:	Epoch: [31][22/204]	Loss 0.5865 (0.5862)	
training:	Epoch: [31][23/204]	Loss 0.5674 (0.5854)	
training:	Epoch: [31][24/204]	Loss 0.5209 (0.5827)	
training:	Epoch: [31][25/204]	Loss 0.4604 (0.5778)	
training:	Epoch: [31][26/204]	Loss 0.4317 (0.5722)	
training:	Epoch: [31][27/204]	Loss 0.5636 (0.5719)	
training:	Epoch: [31][28/204]	Loss 0.5063 (0.5696)	
training:	Epoch: [31][29/204]	Loss 0.5433 (0.5687)	
training:	Epoch: [31][30/204]	Loss 0.5640 (0.5685)	
training:	Epoch: [31][31/204]	Loss 0.6469 (0.5710)	
training:	Epoch: [31][32/204]	Loss 0.6496 (0.5735)	
training:	Epoch: [31][33/204]	Loss 0.6177 (0.5748)	
training:	Epoch: [31][34/204]	Loss 0.4890 (0.5723)	
training:	Epoch: [31][35/204]	Loss 0.5641 (0.5721)	
training:	Epoch: [31][36/204]	Loss 0.4758 (0.5694)	
training:	Epoch: [31][37/204]	Loss 0.4904 (0.5673)	
training:	Epoch: [31][38/204]	Loss 0.6162 (0.5685)	
training:	Epoch: [31][39/204]	Loss 0.6143 (0.5697)	
training:	Epoch: [31][40/204]	Loss 0.6119 (0.5708)	
training:	Epoch: [31][41/204]	Loss 0.5924 (0.5713)	
training:	Epoch: [31][42/204]	Loss 0.5158 (0.5700)	
training:	Epoch: [31][43/204]	Loss 0.4798 (0.5679)	
training:	Epoch: [31][44/204]	Loss 0.6245 (0.5692)	
training:	Epoch: [31][45/204]	Loss 0.5611 (0.5690)	
training:	Epoch: [31][46/204]	Loss 0.6410 (0.5706)	
training:	Epoch: [31][47/204]	Loss 0.4750 (0.5685)	
training:	Epoch: [31][48/204]	Loss 0.6486 (0.5702)	
training:	Epoch: [31][49/204]	Loss 0.5557 (0.5699)	
training:	Epoch: [31][50/204]	Loss 0.5440 (0.5694)	
training:	Epoch: [31][51/204]	Loss 0.5545 (0.5691)	
training:	Epoch: [31][52/204]	Loss 0.6476 (0.5706)	
training:	Epoch: [31][53/204]	Loss 0.5830 (0.5708)	
training:	Epoch: [31][54/204]	Loss 0.6566 (0.5724)	
training:	Epoch: [31][55/204]	Loss 0.6783 (0.5743)	
training:	Epoch: [31][56/204]	Loss 0.5523 (0.5739)	
training:	Epoch: [31][57/204]	Loss 0.5153 (0.5729)	
training:	Epoch: [31][58/204]	Loss 0.5606 (0.5727)	
training:	Epoch: [31][59/204]	Loss 0.6269 (0.5736)	
training:	Epoch: [31][60/204]	Loss 0.6299 (0.5746)	
training:	Epoch: [31][61/204]	Loss 0.5596 (0.5743)	
training:	Epoch: [31][62/204]	Loss 0.6114 (0.5749)	
training:	Epoch: [31][63/204]	Loss 0.6764 (0.5765)	
training:	Epoch: [31][64/204]	Loss 0.5271 (0.5758)	
training:	Epoch: [31][65/204]	Loss 0.5144 (0.5748)	
training:	Epoch: [31][66/204]	Loss 0.5338 (0.5742)	
training:	Epoch: [31][67/204]	Loss 0.5726 (0.5742)	
training:	Epoch: [31][68/204]	Loss 0.6543 (0.5753)	
training:	Epoch: [31][69/204]	Loss 0.6241 (0.5761)	
training:	Epoch: [31][70/204]	Loss 0.6421 (0.5770)	
training:	Epoch: [31][71/204]	Loss 0.5371 (0.5764)	
training:	Epoch: [31][72/204]	Loss 0.6310 (0.5772)	
training:	Epoch: [31][73/204]	Loss 0.5839 (0.5773)	
training:	Epoch: [31][74/204]	Loss 0.6023 (0.5776)	
training:	Epoch: [31][75/204]	Loss 0.5026 (0.5766)	
training:	Epoch: [31][76/204]	Loss 0.5312 (0.5760)	
training:	Epoch: [31][77/204]	Loss 0.6200 (0.5766)	
training:	Epoch: [31][78/204]	Loss 0.6387 (0.5774)	
training:	Epoch: [31][79/204]	Loss 0.6527 (0.5783)	
training:	Epoch: [31][80/204]	Loss 0.5079 (0.5775)	
training:	Epoch: [31][81/204]	Loss 0.6033 (0.5778)	
training:	Epoch: [31][82/204]	Loss 0.6262 (0.5784)	
training:	Epoch: [31][83/204]	Loss 0.5276 (0.5778)	
training:	Epoch: [31][84/204]	Loss 0.4833 (0.5766)	
training:	Epoch: [31][85/204]	Loss 0.6091 (0.5770)	
training:	Epoch: [31][86/204]	Loss 0.4540 (0.5756)	
training:	Epoch: [31][87/204]	Loss 0.5481 (0.5753)	
training:	Epoch: [31][88/204]	Loss 0.5097 (0.5745)	
training:	Epoch: [31][89/204]	Loss 0.4937 (0.5736)	
training:	Epoch: [31][90/204]	Loss 0.4569 (0.5723)	
training:	Epoch: [31][91/204]	Loss 0.5246 (0.5718)	
training:	Epoch: [31][92/204]	Loss 0.5223 (0.5713)	
training:	Epoch: [31][93/204]	Loss 0.6477 (0.5721)	
training:	Epoch: [31][94/204]	Loss 0.5379 (0.5717)	
training:	Epoch: [31][95/204]	Loss 0.4867 (0.5708)	
training:	Epoch: [31][96/204]	Loss 0.5324 (0.5704)	
training:	Epoch: [31][97/204]	Loss 0.4506 (0.5692)	
training:	Epoch: [31][98/204]	Loss 0.5481 (0.5690)	
training:	Epoch: [31][99/204]	Loss 0.5049 (0.5683)	
training:	Epoch: [31][100/204]	Loss 0.4422 (0.5671)	
training:	Epoch: [31][101/204]	Loss 0.7545 (0.5689)	
training:	Epoch: [31][102/204]	Loss 0.5940 (0.5692)	
training:	Epoch: [31][103/204]	Loss 0.4728 (0.5682)	
training:	Epoch: [31][104/204]	Loss 0.5241 (0.5678)	
training:	Epoch: [31][105/204]	Loss 0.5145 (0.5673)	
training:	Epoch: [31][106/204]	Loss 0.7162 (0.5687)	
training:	Epoch: [31][107/204]	Loss 0.5920 (0.5689)	
training:	Epoch: [31][108/204]	Loss 0.4791 (0.5681)	
training:	Epoch: [31][109/204]	Loss 0.5887 (0.5683)	
training:	Epoch: [31][110/204]	Loss 0.6424 (0.5690)	
training:	Epoch: [31][111/204]	Loss 0.6153 (0.5694)	
training:	Epoch: [31][112/204]	Loss 0.5662 (0.5693)	
training:	Epoch: [31][113/204]	Loss 0.4964 (0.5687)	
training:	Epoch: [31][114/204]	Loss 0.5336 (0.5684)	
training:	Epoch: [31][115/204]	Loss 0.6075 (0.5687)	
training:	Epoch: [31][116/204]	Loss 0.4706 (0.5679)	
training:	Epoch: [31][117/204]	Loss 0.5493 (0.5677)	
training:	Epoch: [31][118/204]	Loss 0.4905 (0.5671)	
training:	Epoch: [31][119/204]	Loss 0.4288 (0.5659)	
training:	Epoch: [31][120/204]	Loss 0.6420 (0.5665)	
training:	Epoch: [31][121/204]	Loss 0.6346 (0.5671)	
training:	Epoch: [31][122/204]	Loss 0.5874 (0.5673)	
training:	Epoch: [31][123/204]	Loss 0.5592 (0.5672)	
training:	Epoch: [31][124/204]	Loss 0.5566 (0.5671)	
training:	Epoch: [31][125/204]	Loss 0.6738 (0.5680)	
training:	Epoch: [31][126/204]	Loss 0.5268 (0.5676)	
training:	Epoch: [31][127/204]	Loss 0.4965 (0.5671)	
training:	Epoch: [31][128/204]	Loss 0.7263 (0.5683)	
training:	Epoch: [31][129/204]	Loss 0.5435 (0.5681)	
training:	Epoch: [31][130/204]	Loss 0.5691 (0.5681)	
training:	Epoch: [31][131/204]	Loss 0.3502 (0.5665)	
training:	Epoch: [31][132/204]	Loss 0.5010 (0.5660)	
training:	Epoch: [31][133/204]	Loss 0.6264 (0.5664)	
training:	Epoch: [31][134/204]	Loss 0.5458 (0.5663)	
training:	Epoch: [31][135/204]	Loss 0.5194 (0.5659)	
training:	Epoch: [31][136/204]	Loss 0.5806 (0.5660)	
training:	Epoch: [31][137/204]	Loss 0.5670 (0.5661)	
training:	Epoch: [31][138/204]	Loss 0.5604 (0.5660)	
training:	Epoch: [31][139/204]	Loss 0.6400 (0.5665)	
training:	Epoch: [31][140/204]	Loss 0.5086 (0.5661)	
training:	Epoch: [31][141/204]	Loss 0.5587 (0.5661)	
training:	Epoch: [31][142/204]	Loss 0.6008 (0.5663)	
training:	Epoch: [31][143/204]	Loss 0.5092 (0.5659)	
training:	Epoch: [31][144/204]	Loss 0.5706 (0.5660)	
training:	Epoch: [31][145/204]	Loss 0.5190 (0.5656)	
training:	Epoch: [31][146/204]	Loss 0.4417 (0.5648)	
training:	Epoch: [31][147/204]	Loss 0.5130 (0.5644)	
training:	Epoch: [31][148/204]	Loss 0.4772 (0.5638)	
training:	Epoch: [31][149/204]	Loss 0.4880 (0.5633)	
training:	Epoch: [31][150/204]	Loss 0.6021 (0.5636)	
training:	Epoch: [31][151/204]	Loss 0.5514 (0.5635)	
training:	Epoch: [31][152/204]	Loss 0.5907 (0.5637)	
training:	Epoch: [31][153/204]	Loss 0.5935 (0.5639)	
training:	Epoch: [31][154/204]	Loss 0.5670 (0.5639)	
training:	Epoch: [31][155/204]	Loss 0.5055 (0.5635)	
training:	Epoch: [31][156/204]	Loss 0.5972 (0.5637)	
training:	Epoch: [31][157/204]	Loss 0.5273 (0.5635)	
training:	Epoch: [31][158/204]	Loss 0.6664 (0.5642)	
training:	Epoch: [31][159/204]	Loss 0.7172 (0.5651)	
training:	Epoch: [31][160/204]	Loss 0.6291 (0.5655)	
training:	Epoch: [31][161/204]	Loss 0.5373 (0.5653)	
training:	Epoch: [31][162/204]	Loss 0.5132 (0.5650)	
training:	Epoch: [31][163/204]	Loss 0.5335 (0.5648)	
training:	Epoch: [31][164/204]	Loss 0.6354 (0.5653)	
training:	Epoch: [31][165/204]	Loss 0.5282 (0.5650)	
training:	Epoch: [31][166/204]	Loss 0.5927 (0.5652)	
training:	Epoch: [31][167/204]	Loss 0.5149 (0.5649)	
training:	Epoch: [31][168/204]	Loss 0.5497 (0.5648)	
training:	Epoch: [31][169/204]	Loss 0.5374 (0.5647)	
training:	Epoch: [31][170/204]	Loss 0.5342 (0.5645)	
training:	Epoch: [31][171/204]	Loss 0.5881 (0.5646)	
training:	Epoch: [31][172/204]	Loss 0.5154 (0.5643)	
training:	Epoch: [31][173/204]	Loss 0.5742 (0.5644)	
training:	Epoch: [31][174/204]	Loss 0.4543 (0.5637)	
training:	Epoch: [31][175/204]	Loss 0.5868 (0.5639)	
training:	Epoch: [31][176/204]	Loss 0.6448 (0.5643)	
training:	Epoch: [31][177/204]	Loss 0.5698 (0.5644)	
training:	Epoch: [31][178/204]	Loss 0.6657 (0.5649)	
training:	Epoch: [31][179/204]	Loss 0.6606 (0.5655)	
training:	Epoch: [31][180/204]	Loss 0.5034 (0.5651)	
training:	Epoch: [31][181/204]	Loss 0.5363 (0.5650)	
training:	Epoch: [31][182/204]	Loss 0.5113 (0.5647)	
training:	Epoch: [31][183/204]	Loss 0.4534 (0.5641)	
training:	Epoch: [31][184/204]	Loss 0.6141 (0.5643)	
training:	Epoch: [31][185/204]	Loss 0.5921 (0.5645)	
training:	Epoch: [31][186/204]	Loss 0.5683 (0.5645)	
training:	Epoch: [31][187/204]	Loss 0.5692 (0.5645)	
training:	Epoch: [31][188/204]	Loss 0.5274 (0.5643)	
training:	Epoch: [31][189/204]	Loss 0.5588 (0.5643)	
training:	Epoch: [31][190/204]	Loss 0.5014 (0.5640)	
training:	Epoch: [31][191/204]	Loss 0.4511 (0.5634)	
training:	Epoch: [31][192/204]	Loss 0.6089 (0.5636)	
training:	Epoch: [31][193/204]	Loss 0.6143 (0.5639)	
training:	Epoch: [31][194/204]	Loss 0.4810 (0.5635)	
training:	Epoch: [31][195/204]	Loss 0.5789 (0.5635)	
training:	Epoch: [31][196/204]	Loss 0.6226 (0.5638)	
training:	Epoch: [31][197/204]	Loss 0.6234 (0.5641)	
training:	Epoch: [31][198/204]	Loss 0.5428 (0.5640)	
training:	Epoch: [31][199/204]	Loss 0.5764 (0.5641)	
training:	Epoch: [31][200/204]	Loss 0.5762 (0.5642)	
training:	Epoch: [31][201/204]	Loss 0.6866 (0.5648)	
training:	Epoch: [31][202/204]	Loss 0.5861 (0.5649)	
training:	Epoch: [31][203/204]	Loss 0.4286 (0.5642)	
training:	Epoch: [31][204/204]	Loss 0.5367 (0.5641)	
Training:	 Loss: 0.5632

Training:	 ACC: 0.7272 0.7281 0.7472 0.7073
Validation:	 ACC: 0.7224 0.7239 0.7554 0.6895
Validation:	 Best_BACC: 0.7224 0.7239 0.7554 0.6895
Validation:	 Loss: 0.5567
Pretraining:	Epoch 32/120
----------
training:	Epoch: [32][1/204]	Loss 0.4769 (0.4769)	
training:	Epoch: [32][2/204]	Loss 0.6372 (0.5571)	
training:	Epoch: [32][3/204]	Loss 0.5216 (0.5452)	
training:	Epoch: [32][4/204]	Loss 0.5543 (0.5475)	
training:	Epoch: [32][5/204]	Loss 0.5642 (0.5508)	
training:	Epoch: [32][6/204]	Loss 0.3945 (0.5248)	
training:	Epoch: [32][7/204]	Loss 0.5916 (0.5343)	
training:	Epoch: [32][8/204]	Loss 0.5875 (0.5410)	
training:	Epoch: [32][9/204]	Loss 0.5677 (0.5439)	
training:	Epoch: [32][10/204]	Loss 0.6571 (0.5553)	
training:	Epoch: [32][11/204]	Loss 0.4423 (0.5450)	
training:	Epoch: [32][12/204]	Loss 0.5557 (0.5459)	
training:	Epoch: [32][13/204]	Loss 0.5908 (0.5493)	
training:	Epoch: [32][14/204]	Loss 0.6416 (0.5559)	
training:	Epoch: [32][15/204]	Loss 0.5381 (0.5547)	
training:	Epoch: [32][16/204]	Loss 0.4675 (0.5493)	
training:	Epoch: [32][17/204]	Loss 0.4974 (0.5462)	
training:	Epoch: [32][18/204]	Loss 0.5400 (0.5459)	
training:	Epoch: [32][19/204]	Loss 0.6096 (0.5492)	
training:	Epoch: [32][20/204]	Loss 0.6685 (0.5552)	
training:	Epoch: [32][21/204]	Loss 0.4762 (0.5514)	
training:	Epoch: [32][22/204]	Loss 0.6974 (0.5581)	
training:	Epoch: [32][23/204]	Loss 0.5369 (0.5572)	
training:	Epoch: [32][24/204]	Loss 0.5297 (0.5560)	
training:	Epoch: [32][25/204]	Loss 0.5126 (0.5543)	
training:	Epoch: [32][26/204]	Loss 0.6209 (0.5568)	
training:	Epoch: [32][27/204]	Loss 0.5978 (0.5584)	
training:	Epoch: [32][28/204]	Loss 0.5000 (0.5563)	
training:	Epoch: [32][29/204]	Loss 0.4932 (0.5541)	
training:	Epoch: [32][30/204]	Loss 0.5289 (0.5533)	
training:	Epoch: [32][31/204]	Loss 0.5858 (0.5543)	
training:	Epoch: [32][32/204]	Loss 0.5140 (0.5530)	
training:	Epoch: [32][33/204]	Loss 0.5331 (0.5524)	
training:	Epoch: [32][34/204]	Loss 0.6742 (0.5560)	
training:	Epoch: [32][35/204]	Loss 0.5521 (0.5559)	
training:	Epoch: [32][36/204]	Loss 0.6102 (0.5574)	
training:	Epoch: [32][37/204]	Loss 0.6029 (0.5586)	
training:	Epoch: [32][38/204]	Loss 0.5730 (0.5590)	
training:	Epoch: [32][39/204]	Loss 0.4835 (0.5571)	
training:	Epoch: [32][40/204]	Loss 0.7117 (0.5610)	
training:	Epoch: [32][41/204]	Loss 0.5278 (0.5601)	
training:	Epoch: [32][42/204]	Loss 0.4662 (0.5579)	
training:	Epoch: [32][43/204]	Loss 0.5825 (0.5585)	
training:	Epoch: [32][44/204]	Loss 0.5380 (0.5580)	
training:	Epoch: [32][45/204]	Loss 0.5834 (0.5586)	
training:	Epoch: [32][46/204]	Loss 0.5495 (0.5584)	
training:	Epoch: [32][47/204]	Loss 0.5573 (0.5584)	
training:	Epoch: [32][48/204]	Loss 0.5339 (0.5578)	
training:	Epoch: [32][49/204]	Loss 0.4915 (0.5565)	
training:	Epoch: [32][50/204]	Loss 0.5767 (0.5569)	
training:	Epoch: [32][51/204]	Loss 0.5980 (0.5577)	
training:	Epoch: [32][52/204]	Loss 0.5600 (0.5577)	
training:	Epoch: [32][53/204]	Loss 0.5658 (0.5579)	
training:	Epoch: [32][54/204]	Loss 0.5336 (0.5574)	
training:	Epoch: [32][55/204]	Loss 0.5059 (0.5565)	
training:	Epoch: [32][56/204]	Loss 0.5939 (0.5572)	
training:	Epoch: [32][57/204]	Loss 0.6447 (0.5587)	
training:	Epoch: [32][58/204]	Loss 0.5271 (0.5582)	
training:	Epoch: [32][59/204]	Loss 0.6596 (0.5599)	
training:	Epoch: [32][60/204]	Loss 0.4512 (0.5581)	
training:	Epoch: [32][61/204]	Loss 0.5583 (0.5581)	
training:	Epoch: [32][62/204]	Loss 0.5336 (0.5577)	
training:	Epoch: [32][63/204]	Loss 0.5226 (0.5571)	
training:	Epoch: [32][64/204]	Loss 0.5217 (0.5566)	
training:	Epoch: [32][65/204]	Loss 0.6642 (0.5582)	
training:	Epoch: [32][66/204]	Loss 0.5021 (0.5574)	
training:	Epoch: [32][67/204]	Loss 0.5409 (0.5571)	
training:	Epoch: [32][68/204]	Loss 0.5502 (0.5570)	
training:	Epoch: [32][69/204]	Loss 0.6026 (0.5577)	
training:	Epoch: [32][70/204]	Loss 0.4579 (0.5563)	
training:	Epoch: [32][71/204]	Loss 0.5359 (0.5560)	
training:	Epoch: [32][72/204]	Loss 0.5287 (0.5556)	
training:	Epoch: [32][73/204]	Loss 0.6413 (0.5568)	
training:	Epoch: [32][74/204]	Loss 0.5566 (0.5568)	
training:	Epoch: [32][75/204]	Loss 0.6502 (0.5580)	
training:	Epoch: [32][76/204]	Loss 0.5706 (0.5582)	
training:	Epoch: [32][77/204]	Loss 0.5519 (0.5581)	
training:	Epoch: [32][78/204]	Loss 0.6101 (0.5588)	
training:	Epoch: [32][79/204]	Loss 0.6128 (0.5595)	
training:	Epoch: [32][80/204]	Loss 0.6297 (0.5603)	
training:	Epoch: [32][81/204]	Loss 0.5096 (0.5597)	
training:	Epoch: [32][82/204]	Loss 0.4979 (0.5590)	
training:	Epoch: [32][83/204]	Loss 0.5597 (0.5590)	
training:	Epoch: [32][84/204]	Loss 0.6634 (0.5602)	
training:	Epoch: [32][85/204]	Loss 0.5955 (0.5606)	
training:	Epoch: [32][86/204]	Loss 0.6867 (0.5621)	
training:	Epoch: [32][87/204]	Loss 0.5153 (0.5615)	
training:	Epoch: [32][88/204]	Loss 0.5689 (0.5616)	
training:	Epoch: [32][89/204]	Loss 0.4912 (0.5608)	
training:	Epoch: [32][90/204]	Loss 0.5650 (0.5609)	
training:	Epoch: [32][91/204]	Loss 0.5147 (0.5604)	
training:	Epoch: [32][92/204]	Loss 0.5536 (0.5603)	
training:	Epoch: [32][93/204]	Loss 0.6042 (0.5608)	
training:	Epoch: [32][94/204]	Loss 0.5456 (0.5606)	
training:	Epoch: [32][95/204]	Loss 0.5310 (0.5603)	
training:	Epoch: [32][96/204]	Loss 0.6491 (0.5612)	
training:	Epoch: [32][97/204]	Loss 0.7769 (0.5635)	
training:	Epoch: [32][98/204]	Loss 0.5652 (0.5635)	
training:	Epoch: [32][99/204]	Loss 0.5525 (0.5634)	
training:	Epoch: [32][100/204]	Loss 0.5428 (0.5632)	
training:	Epoch: [32][101/204]	Loss 0.4889 (0.5624)	
training:	Epoch: [32][102/204]	Loss 0.6973 (0.5637)	
training:	Epoch: [32][103/204]	Loss 0.5112 (0.5632)	
training:	Epoch: [32][104/204]	Loss 0.5822 (0.5634)	
training:	Epoch: [32][105/204]	Loss 0.5936 (0.5637)	
training:	Epoch: [32][106/204]	Loss 0.5407 (0.5635)	
training:	Epoch: [32][107/204]	Loss 0.5554 (0.5634)	
training:	Epoch: [32][108/204]	Loss 0.6485 (0.5642)	
training:	Epoch: [32][109/204]	Loss 0.5387 (0.5640)	
training:	Epoch: [32][110/204]	Loss 0.4860 (0.5633)	
training:	Epoch: [32][111/204]	Loss 0.4552 (0.5623)	
training:	Epoch: [32][112/204]	Loss 0.5323 (0.5620)	
training:	Epoch: [32][113/204]	Loss 0.5464 (0.5619)	
training:	Epoch: [32][114/204]	Loss 0.5112 (0.5614)	
training:	Epoch: [32][115/204]	Loss 0.5598 (0.5614)	
training:	Epoch: [32][116/204]	Loss 0.5257 (0.5611)	
training:	Epoch: [32][117/204]	Loss 0.6403 (0.5618)	
training:	Epoch: [32][118/204]	Loss 0.6180 (0.5623)	
training:	Epoch: [32][119/204]	Loss 0.5256 (0.5620)	
training:	Epoch: [32][120/204]	Loss 0.4761 (0.5612)	
training:	Epoch: [32][121/204]	Loss 0.5686 (0.5613)	
training:	Epoch: [32][122/204]	Loss 0.4938 (0.5607)	
training:	Epoch: [32][123/204]	Loss 0.6906 (0.5618)	
training:	Epoch: [32][124/204]	Loss 0.5862 (0.5620)	
training:	Epoch: [32][125/204]	Loss 0.5512 (0.5619)	
training:	Epoch: [32][126/204]	Loss 0.5761 (0.5620)	
training:	Epoch: [32][127/204]	Loss 0.4981 (0.5615)	
training:	Epoch: [32][128/204]	Loss 0.6163 (0.5619)	
training:	Epoch: [32][129/204]	Loss 0.6131 (0.5623)	
training:	Epoch: [32][130/204]	Loss 0.5782 (0.5625)	
training:	Epoch: [32][131/204]	Loss 0.5510 (0.5624)	
training:	Epoch: [32][132/204]	Loss 0.4675 (0.5617)	
training:	Epoch: [32][133/204]	Loss 0.6154 (0.5621)	
training:	Epoch: [32][134/204]	Loss 0.5793 (0.5622)	
training:	Epoch: [32][135/204]	Loss 0.4778 (0.5616)	
training:	Epoch: [32][136/204]	Loss 0.5291 (0.5613)	
training:	Epoch: [32][137/204]	Loss 0.4904 (0.5608)	
training:	Epoch: [32][138/204]	Loss 0.5751 (0.5609)	
training:	Epoch: [32][139/204]	Loss 0.5501 (0.5608)	
training:	Epoch: [32][140/204]	Loss 0.5526 (0.5608)	
training:	Epoch: [32][141/204]	Loss 0.5355 (0.5606)	
training:	Epoch: [32][142/204]	Loss 0.7038 (0.5616)	
training:	Epoch: [32][143/204]	Loss 0.6605 (0.5623)	
training:	Epoch: [32][144/204]	Loss 0.5266 (0.5621)	
training:	Epoch: [32][145/204]	Loss 0.4825 (0.5615)	
training:	Epoch: [32][146/204]	Loss 0.5398 (0.5614)	
training:	Epoch: [32][147/204]	Loss 0.5636 (0.5614)	
training:	Epoch: [32][148/204]	Loss 0.4994 (0.5609)	
training:	Epoch: [32][149/204]	Loss 0.5620 (0.5610)	
training:	Epoch: [32][150/204]	Loss 0.4765 (0.5604)	
training:	Epoch: [32][151/204]	Loss 0.5537 (0.5603)	
training:	Epoch: [32][152/204]	Loss 0.5203 (0.5601)	
training:	Epoch: [32][153/204]	Loss 0.5869 (0.5603)	
training:	Epoch: [32][154/204]	Loss 0.5468 (0.5602)	
training:	Epoch: [32][155/204]	Loss 0.5226 (0.5599)	
training:	Epoch: [32][156/204]	Loss 0.5483 (0.5599)	
training:	Epoch: [32][157/204]	Loss 0.5685 (0.5599)	
training:	Epoch: [32][158/204]	Loss 0.6793 (0.5607)	
training:	Epoch: [32][159/204]	Loss 0.5259 (0.5604)	
training:	Epoch: [32][160/204]	Loss 0.5512 (0.5604)	
training:	Epoch: [32][161/204]	Loss 0.4856 (0.5599)	
training:	Epoch: [32][162/204]	Loss 0.5752 (0.5600)	
training:	Epoch: [32][163/204]	Loss 0.8096 (0.5616)	
training:	Epoch: [32][164/204]	Loss 0.5369 (0.5614)	
training:	Epoch: [32][165/204]	Loss 0.5402 (0.5613)	
training:	Epoch: [32][166/204]	Loss 0.5319 (0.5611)	
training:	Epoch: [32][167/204]	Loss 0.5638 (0.5611)	
training:	Epoch: [32][168/204]	Loss 0.5056 (0.5608)	
training:	Epoch: [32][169/204]	Loss 0.6208 (0.5611)	
training:	Epoch: [32][170/204]	Loss 0.5052 (0.5608)	
training:	Epoch: [32][171/204]	Loss 0.5602 (0.5608)	
training:	Epoch: [32][172/204]	Loss 0.5781 (0.5609)	
training:	Epoch: [32][173/204]	Loss 0.6584 (0.5615)	
training:	Epoch: [32][174/204]	Loss 0.5552 (0.5614)	
training:	Epoch: [32][175/204]	Loss 0.7247 (0.5624)	
training:	Epoch: [32][176/204]	Loss 0.5834 (0.5625)	
training:	Epoch: [32][177/204]	Loss 0.6122 (0.5628)	
training:	Epoch: [32][178/204]	Loss 0.5562 (0.5627)	
training:	Epoch: [32][179/204]	Loss 0.5936 (0.5629)	
training:	Epoch: [32][180/204]	Loss 0.5247 (0.5627)	
training:	Epoch: [32][181/204]	Loss 0.5102 (0.5624)	
training:	Epoch: [32][182/204]	Loss 0.5006 (0.5621)	
training:	Epoch: [32][183/204]	Loss 0.4197 (0.5613)	
training:	Epoch: [32][184/204]	Loss 0.5761 (0.5614)	
training:	Epoch: [32][185/204]	Loss 0.5978 (0.5616)	
training:	Epoch: [32][186/204]	Loss 0.5090 (0.5613)	
training:	Epoch: [32][187/204]	Loss 0.4663 (0.5608)	
training:	Epoch: [32][188/204]	Loss 0.6494 (0.5612)	
training:	Epoch: [32][189/204]	Loss 0.5574 (0.5612)	
training:	Epoch: [32][190/204]	Loss 0.5185 (0.5610)	
training:	Epoch: [32][191/204]	Loss 0.5126 (0.5607)	
training:	Epoch: [32][192/204]	Loss 0.3963 (0.5599)	
training:	Epoch: [32][193/204]	Loss 0.5893 (0.5600)	
training:	Epoch: [32][194/204]	Loss 0.5003 (0.5597)	
training:	Epoch: [32][195/204]	Loss 0.5553 (0.5597)	
training:	Epoch: [32][196/204]	Loss 0.5517 (0.5597)	
training:	Epoch: [32][197/204]	Loss 0.4855 (0.5593)	
training:	Epoch: [32][198/204]	Loss 0.4851 (0.5589)	
training:	Epoch: [32][199/204]	Loss 0.4961 (0.5586)	
training:	Epoch: [32][200/204]	Loss 0.6328 (0.5590)	
training:	Epoch: [32][201/204]	Loss 0.5704 (0.5590)	
training:	Epoch: [32][202/204]	Loss 0.5827 (0.5591)	
training:	Epoch: [32][203/204]	Loss 0.5084 (0.5589)	
training:	Epoch: [32][204/204]	Loss 0.5317 (0.5588)	
Training:	 Loss: 0.5579

Training:	 ACC: 0.7263 0.7276 0.7590 0.6936
Validation:	 ACC: 0.7299 0.7319 0.7758 0.6839
Validation:	 Best_BACC: 0.7299 0.7319 0.7758 0.6839
Validation:	 Loss: 0.5535
Pretraining:	Epoch 33/120
----------
training:	Epoch: [33][1/204]	Loss 0.5176 (0.5176)	
training:	Epoch: [33][2/204]	Loss 0.5236 (0.5206)	
training:	Epoch: [33][3/204]	Loss 0.6038 (0.5483)	
training:	Epoch: [33][4/204]	Loss 0.5384 (0.5458)	
training:	Epoch: [33][5/204]	Loss 0.5578 (0.5482)	
training:	Epoch: [33][6/204]	Loss 0.4778 (0.5365)	
training:	Epoch: [33][7/204]	Loss 0.5730 (0.5417)	
training:	Epoch: [33][8/204]	Loss 0.5874 (0.5474)	
training:	Epoch: [33][9/204]	Loss 0.6319 (0.5568)	
training:	Epoch: [33][10/204]	Loss 0.5707 (0.5582)	
training:	Epoch: [33][11/204]	Loss 0.6352 (0.5652)	
training:	Epoch: [33][12/204]	Loss 0.5058 (0.5602)	
training:	Epoch: [33][13/204]	Loss 0.5359 (0.5584)	
training:	Epoch: [33][14/204]	Loss 0.5894 (0.5606)	
training:	Epoch: [33][15/204]	Loss 0.6073 (0.5637)	
training:	Epoch: [33][16/204]	Loss 0.5632 (0.5637)	
training:	Epoch: [33][17/204]	Loss 0.6258 (0.5673)	
training:	Epoch: [33][18/204]	Loss 0.6213 (0.5703)	
training:	Epoch: [33][19/204]	Loss 0.6019 (0.5720)	
training:	Epoch: [33][20/204]	Loss 0.6616 (0.5765)	
training:	Epoch: [33][21/204]	Loss 0.5087 (0.5732)	
training:	Epoch: [33][22/204]	Loss 0.4812 (0.5691)	
training:	Epoch: [33][23/204]	Loss 0.5076 (0.5664)	
training:	Epoch: [33][24/204]	Loss 0.4638 (0.5621)	
training:	Epoch: [33][25/204]	Loss 0.5026 (0.5597)	
training:	Epoch: [33][26/204]	Loss 0.4522 (0.5556)	
training:	Epoch: [33][27/204]	Loss 0.5828 (0.5566)	
training:	Epoch: [33][28/204]	Loss 0.6705 (0.5607)	
training:	Epoch: [33][29/204]	Loss 0.5296 (0.5596)	
training:	Epoch: [33][30/204]	Loss 0.5690 (0.5599)	
training:	Epoch: [33][31/204]	Loss 0.5519 (0.5597)	
training:	Epoch: [33][32/204]	Loss 0.5680 (0.5599)	
training:	Epoch: [33][33/204]	Loss 0.6488 (0.5626)	
training:	Epoch: [33][34/204]	Loss 0.6529 (0.5653)	
training:	Epoch: [33][35/204]	Loss 0.5788 (0.5657)	
training:	Epoch: [33][36/204]	Loss 0.5329 (0.5647)	
training:	Epoch: [33][37/204]	Loss 0.4840 (0.5626)	
training:	Epoch: [33][38/204]	Loss 0.4269 (0.5590)	
training:	Epoch: [33][39/204]	Loss 0.4317 (0.5557)	
training:	Epoch: [33][40/204]	Loss 0.5951 (0.5567)	
training:	Epoch: [33][41/204]	Loss 0.6053 (0.5579)	
training:	Epoch: [33][42/204]	Loss 0.5003 (0.5565)	
training:	Epoch: [33][43/204]	Loss 0.5023 (0.5553)	
training:	Epoch: [33][44/204]	Loss 0.5175 (0.5544)	
training:	Epoch: [33][45/204]	Loss 0.4928 (0.5530)	
training:	Epoch: [33][46/204]	Loss 0.5699 (0.5534)	
training:	Epoch: [33][47/204]	Loss 0.5772 (0.5539)	
training:	Epoch: [33][48/204]	Loss 0.6163 (0.5552)	
training:	Epoch: [33][49/204]	Loss 0.5912 (0.5559)	
training:	Epoch: [33][50/204]	Loss 0.4237 (0.5533)	
training:	Epoch: [33][51/204]	Loss 0.4637 (0.5515)	
training:	Epoch: [33][52/204]	Loss 0.5986 (0.5524)	
training:	Epoch: [33][53/204]	Loss 0.4345 (0.5502)	
training:	Epoch: [33][54/204]	Loss 0.5557 (0.5503)	
training:	Epoch: [33][55/204]	Loss 0.6921 (0.5529)	
training:	Epoch: [33][56/204]	Loss 0.4922 (0.5518)	
training:	Epoch: [33][57/204]	Loss 0.5496 (0.5518)	
training:	Epoch: [33][58/204]	Loss 0.4220 (0.5495)	
training:	Epoch: [33][59/204]	Loss 0.5046 (0.5488)	
training:	Epoch: [33][60/204]	Loss 0.5571 (0.5489)	
training:	Epoch: [33][61/204]	Loss 0.6085 (0.5499)	
training:	Epoch: [33][62/204]	Loss 0.5369 (0.5497)	
training:	Epoch: [33][63/204]	Loss 0.4938 (0.5488)	
training:	Epoch: [33][64/204]	Loss 0.6190 (0.5499)	
training:	Epoch: [33][65/204]	Loss 0.5111 (0.5493)	
training:	Epoch: [33][66/204]	Loss 0.6958 (0.5515)	
training:	Epoch: [33][67/204]	Loss 0.5213 (0.5511)	
training:	Epoch: [33][68/204]	Loss 0.5263 (0.5507)	
training:	Epoch: [33][69/204]	Loss 0.5088 (0.5501)	
training:	Epoch: [33][70/204]	Loss 0.6240 (0.5511)	
training:	Epoch: [33][71/204]	Loss 0.4932 (0.5503)	
training:	Epoch: [33][72/204]	Loss 0.5957 (0.5510)	
training:	Epoch: [33][73/204]	Loss 0.4213 (0.5492)	
training:	Epoch: [33][74/204]	Loss 0.5675 (0.5494)	
training:	Epoch: [33][75/204]	Loss 0.5005 (0.5488)	
training:	Epoch: [33][76/204]	Loss 0.5392 (0.5487)	
training:	Epoch: [33][77/204]	Loss 0.6455 (0.5499)	
training:	Epoch: [33][78/204]	Loss 0.6070 (0.5506)	
training:	Epoch: [33][79/204]	Loss 0.4574 (0.5495)	
training:	Epoch: [33][80/204]	Loss 0.4858 (0.5487)	
training:	Epoch: [33][81/204]	Loss 0.4446 (0.5474)	
training:	Epoch: [33][82/204]	Loss 0.5771 (0.5477)	
training:	Epoch: [33][83/204]	Loss 0.5786 (0.5481)	
training:	Epoch: [33][84/204]	Loss 0.5916 (0.5486)	
training:	Epoch: [33][85/204]	Loss 0.5630 (0.5488)	
training:	Epoch: [33][86/204]	Loss 0.5503 (0.5488)	
training:	Epoch: [33][87/204]	Loss 0.5092 (0.5484)	
training:	Epoch: [33][88/204]	Loss 0.5908 (0.5488)	
training:	Epoch: [33][89/204]	Loss 0.4526 (0.5478)	
training:	Epoch: [33][90/204]	Loss 0.4782 (0.5470)	
training:	Epoch: [33][91/204]	Loss 0.6143 (0.5477)	
training:	Epoch: [33][92/204]	Loss 0.5505 (0.5478)	
training:	Epoch: [33][93/204]	Loss 0.6057 (0.5484)	
training:	Epoch: [33][94/204]	Loss 0.6103 (0.5490)	
training:	Epoch: [33][95/204]	Loss 0.5468 (0.5490)	
training:	Epoch: [33][96/204]	Loss 0.4769 (0.5483)	
training:	Epoch: [33][97/204]	Loss 0.6463 (0.5493)	
training:	Epoch: [33][98/204]	Loss 0.5365 (0.5492)	
training:	Epoch: [33][99/204]	Loss 0.5696 (0.5494)	
training:	Epoch: [33][100/204]	Loss 0.6030 (0.5499)	
training:	Epoch: [33][101/204]	Loss 0.4100 (0.5485)	
training:	Epoch: [33][102/204]	Loss 0.6200 (0.5492)	
training:	Epoch: [33][103/204]	Loss 0.5340 (0.5491)	
training:	Epoch: [33][104/204]	Loss 0.5695 (0.5493)	
training:	Epoch: [33][105/204]	Loss 0.5760 (0.5495)	
training:	Epoch: [33][106/204]	Loss 0.4442 (0.5485)	
training:	Epoch: [33][107/204]	Loss 0.5076 (0.5481)	
training:	Epoch: [33][108/204]	Loss 0.5003 (0.5477)	
training:	Epoch: [33][109/204]	Loss 0.5543 (0.5478)	
training:	Epoch: [33][110/204]	Loss 0.5538 (0.5478)	
training:	Epoch: [33][111/204]	Loss 0.5590 (0.5479)	
training:	Epoch: [33][112/204]	Loss 0.5182 (0.5476)	
training:	Epoch: [33][113/204]	Loss 0.4583 (0.5469)	
training:	Epoch: [33][114/204]	Loss 0.5219 (0.5466)	
training:	Epoch: [33][115/204]	Loss 0.5628 (0.5468)	
training:	Epoch: [33][116/204]	Loss 0.5399 (0.5467)	
training:	Epoch: [33][117/204]	Loss 0.5274 (0.5466)	
training:	Epoch: [33][118/204]	Loss 0.5806 (0.5468)	
training:	Epoch: [33][119/204]	Loss 0.6816 (0.5480)	
training:	Epoch: [33][120/204]	Loss 0.5768 (0.5482)	
training:	Epoch: [33][121/204]	Loss 0.4690 (0.5476)	
training:	Epoch: [33][122/204]	Loss 0.7060 (0.5489)	
training:	Epoch: [33][123/204]	Loss 0.6352 (0.5496)	
training:	Epoch: [33][124/204]	Loss 0.5177 (0.5493)	
training:	Epoch: [33][125/204]	Loss 0.5271 (0.5491)	
training:	Epoch: [33][126/204]	Loss 0.5741 (0.5493)	
training:	Epoch: [33][127/204]	Loss 0.4998 (0.5489)	
training:	Epoch: [33][128/204]	Loss 0.6078 (0.5494)	
training:	Epoch: [33][129/204]	Loss 0.5382 (0.5493)	
training:	Epoch: [33][130/204]	Loss 0.5468 (0.5493)	
training:	Epoch: [33][131/204]	Loss 0.6712 (0.5502)	
training:	Epoch: [33][132/204]	Loss 0.6243 (0.5508)	
training:	Epoch: [33][133/204]	Loss 0.6537 (0.5516)	
training:	Epoch: [33][134/204]	Loss 0.6214 (0.5521)	
training:	Epoch: [33][135/204]	Loss 0.5006 (0.5517)	
training:	Epoch: [33][136/204]	Loss 0.6068 (0.5521)	
training:	Epoch: [33][137/204]	Loss 0.6011 (0.5525)	
training:	Epoch: [33][138/204]	Loss 0.5553 (0.5525)	
training:	Epoch: [33][139/204]	Loss 0.6605 (0.5533)	
training:	Epoch: [33][140/204]	Loss 0.4993 (0.5529)	
training:	Epoch: [33][141/204]	Loss 0.4788 (0.5523)	
training:	Epoch: [33][142/204]	Loss 0.6363 (0.5529)	
training:	Epoch: [33][143/204]	Loss 0.5766 (0.5531)	
training:	Epoch: [33][144/204]	Loss 0.4927 (0.5527)	
training:	Epoch: [33][145/204]	Loss 0.4960 (0.5523)	
training:	Epoch: [33][146/204]	Loss 0.6086 (0.5527)	
training:	Epoch: [33][147/204]	Loss 0.4796 (0.5522)	
training:	Epoch: [33][148/204]	Loss 0.5812 (0.5524)	
training:	Epoch: [33][149/204]	Loss 0.5368 (0.5523)	
training:	Epoch: [33][150/204]	Loss 0.6737 (0.5531)	
training:	Epoch: [33][151/204]	Loss 0.5914 (0.5533)	
training:	Epoch: [33][152/204]	Loss 0.5407 (0.5532)	
training:	Epoch: [33][153/204]	Loss 0.4148 (0.5523)	
training:	Epoch: [33][154/204]	Loss 0.6206 (0.5528)	
training:	Epoch: [33][155/204]	Loss 0.5674 (0.5529)	
training:	Epoch: [33][156/204]	Loss 0.5242 (0.5527)	
training:	Epoch: [33][157/204]	Loss 0.5924 (0.5529)	
training:	Epoch: [33][158/204]	Loss 0.7047 (0.5539)	
training:	Epoch: [33][159/204]	Loss 0.6069 (0.5542)	
training:	Epoch: [33][160/204]	Loss 0.5406 (0.5542)	
training:	Epoch: [33][161/204]	Loss 0.5651 (0.5542)	
training:	Epoch: [33][162/204]	Loss 0.6936 (0.5551)	
training:	Epoch: [33][163/204]	Loss 0.5469 (0.5550)	
training:	Epoch: [33][164/204]	Loss 0.6717 (0.5557)	
training:	Epoch: [33][165/204]	Loss 0.3888 (0.5547)	
training:	Epoch: [33][166/204]	Loss 0.5413 (0.5547)	
training:	Epoch: [33][167/204]	Loss 0.5719 (0.5548)	
training:	Epoch: [33][168/204]	Loss 0.6457 (0.5553)	
training:	Epoch: [33][169/204]	Loss 0.4677 (0.5548)	
training:	Epoch: [33][170/204]	Loss 0.4978 (0.5544)	
training:	Epoch: [33][171/204]	Loss 0.6908 (0.5552)	
training:	Epoch: [33][172/204]	Loss 0.6268 (0.5557)	
training:	Epoch: [33][173/204]	Loss 0.4876 (0.5553)	
training:	Epoch: [33][174/204]	Loss 0.5217 (0.5551)	
training:	Epoch: [33][175/204]	Loss 0.6642 (0.5557)	
training:	Epoch: [33][176/204]	Loss 0.5136 (0.5555)	
training:	Epoch: [33][177/204]	Loss 0.7298 (0.5564)	
training:	Epoch: [33][178/204]	Loss 0.4950 (0.5561)	
training:	Epoch: [33][179/204]	Loss 0.6188 (0.5564)	
training:	Epoch: [33][180/204]	Loss 0.5765 (0.5566)	
training:	Epoch: [33][181/204]	Loss 0.5173 (0.5563)	
training:	Epoch: [33][182/204]	Loss 0.5924 (0.5565)	
training:	Epoch: [33][183/204]	Loss 0.6129 (0.5568)	
training:	Epoch: [33][184/204]	Loss 0.5615 (0.5569)	
training:	Epoch: [33][185/204]	Loss 0.5964 (0.5571)	
training:	Epoch: [33][186/204]	Loss 0.5797 (0.5572)	
training:	Epoch: [33][187/204]	Loss 0.5754 (0.5573)	
training:	Epoch: [33][188/204]	Loss 0.5765 (0.5574)	
training:	Epoch: [33][189/204]	Loss 0.5472 (0.5574)	
training:	Epoch: [33][190/204]	Loss 0.5934 (0.5575)	
training:	Epoch: [33][191/204]	Loss 0.5300 (0.5574)	
training:	Epoch: [33][192/204]	Loss 0.4781 (0.5570)	
training:	Epoch: [33][193/204]	Loss 0.5222 (0.5568)	
training:	Epoch: [33][194/204]	Loss 0.5659 (0.5569)	
training:	Epoch: [33][195/204]	Loss 0.7523 (0.5579)	
training:	Epoch: [33][196/204]	Loss 0.6210 (0.5582)	
training:	Epoch: [33][197/204]	Loss 0.5480 (0.5581)	
training:	Epoch: [33][198/204]	Loss 0.6442 (0.5586)	
training:	Epoch: [33][199/204]	Loss 0.6538 (0.5590)	
training:	Epoch: [33][200/204]	Loss 0.5042 (0.5588)	
training:	Epoch: [33][201/204]	Loss 0.5579 (0.5588)	
training:	Epoch: [33][202/204]	Loss 0.5425 (0.5587)	
training:	Epoch: [33][203/204]	Loss 0.6111 (0.5589)	
training:	Epoch: [33][204/204]	Loss 0.4405 (0.5584)	
Training:	 Loss: 0.5575

Training:	 ACC: 0.7301 0.7308 0.7484 0.7117
Validation:	 ACC: 0.7335 0.7346 0.7584 0.7085
Validation:	 Best_BACC: 0.7335 0.7346 0.7584 0.7085
Validation:	 Loss: 0.5494
Pretraining:	Epoch 34/120
----------
training:	Epoch: [34][1/204]	Loss 0.5038 (0.5038)	
training:	Epoch: [34][2/204]	Loss 0.5084 (0.5061)	
training:	Epoch: [34][3/204]	Loss 0.5544 (0.5222)	
training:	Epoch: [34][4/204]	Loss 0.7252 (0.5730)	
training:	Epoch: [34][5/204]	Loss 0.5995 (0.5783)	
training:	Epoch: [34][6/204]	Loss 0.4554 (0.5578)	
training:	Epoch: [34][7/204]	Loss 0.5182 (0.5521)	
training:	Epoch: [34][8/204]	Loss 0.6487 (0.5642)	
training:	Epoch: [34][9/204]	Loss 0.6541 (0.5742)	
training:	Epoch: [34][10/204]	Loss 0.4680 (0.5636)	
training:	Epoch: [34][11/204]	Loss 0.5907 (0.5660)	
training:	Epoch: [34][12/204]	Loss 0.5481 (0.5645)	
training:	Epoch: [34][13/204]	Loss 0.5269 (0.5617)	
training:	Epoch: [34][14/204]	Loss 0.4778 (0.5557)	
training:	Epoch: [34][15/204]	Loss 0.5943 (0.5582)	
training:	Epoch: [34][16/204]	Loss 0.5876 (0.5601)	
training:	Epoch: [34][17/204]	Loss 0.5304 (0.5583)	
training:	Epoch: [34][18/204]	Loss 0.5476 (0.5577)	
training:	Epoch: [34][19/204]	Loss 0.5100 (0.5552)	
training:	Epoch: [34][20/204]	Loss 0.5293 (0.5539)	
training:	Epoch: [34][21/204]	Loss 0.6727 (0.5596)	
training:	Epoch: [34][22/204]	Loss 0.5820 (0.5606)	
training:	Epoch: [34][23/204]	Loss 0.5617 (0.5606)	
training:	Epoch: [34][24/204]	Loss 0.6472 (0.5643)	
training:	Epoch: [34][25/204]	Loss 0.5260 (0.5627)	
training:	Epoch: [34][26/204]	Loss 0.6115 (0.5646)	
training:	Epoch: [34][27/204]	Loss 0.5005 (0.5622)	
training:	Epoch: [34][28/204]	Loss 0.4183 (0.5571)	
training:	Epoch: [34][29/204]	Loss 0.4353 (0.5529)	
training:	Epoch: [34][30/204]	Loss 0.6464 (0.5560)	
training:	Epoch: [34][31/204]	Loss 0.5657 (0.5563)	
training:	Epoch: [34][32/204]	Loss 0.4872 (0.5541)	
training:	Epoch: [34][33/204]	Loss 0.5481 (0.5540)	
training:	Epoch: [34][34/204]	Loss 0.6435 (0.5566)	
training:	Epoch: [34][35/204]	Loss 0.5324 (0.5559)	
training:	Epoch: [34][36/204]	Loss 0.5468 (0.5557)	
training:	Epoch: [34][37/204]	Loss 0.5408 (0.5553)	
training:	Epoch: [34][38/204]	Loss 0.5439 (0.5550)	
training:	Epoch: [34][39/204]	Loss 0.4944 (0.5534)	
training:	Epoch: [34][40/204]	Loss 0.5462 (0.5532)	
training:	Epoch: [34][41/204]	Loss 0.5373 (0.5528)	
training:	Epoch: [34][42/204]	Loss 0.5285 (0.5523)	
training:	Epoch: [34][43/204]	Loss 0.5594 (0.5524)	
training:	Epoch: [34][44/204]	Loss 0.6372 (0.5543)	
training:	Epoch: [34][45/204]	Loss 0.5849 (0.5550)	
training:	Epoch: [34][46/204]	Loss 0.4722 (0.5532)	
training:	Epoch: [34][47/204]	Loss 0.5057 (0.5522)	
training:	Epoch: [34][48/204]	Loss 0.5887 (0.5530)	
training:	Epoch: [34][49/204]	Loss 0.6840 (0.5557)	
training:	Epoch: [34][50/204]	Loss 0.6404 (0.5573)	
training:	Epoch: [34][51/204]	Loss 0.5346 (0.5569)	
training:	Epoch: [34][52/204]	Loss 0.6848 (0.5594)	
training:	Epoch: [34][53/204]	Loss 0.6048 (0.5602)	
training:	Epoch: [34][54/204]	Loss 0.4893 (0.5589)	
training:	Epoch: [34][55/204]	Loss 0.4378 (0.5567)	
training:	Epoch: [34][56/204]	Loss 0.6343 (0.5581)	
training:	Epoch: [34][57/204]	Loss 0.4782 (0.5567)	
training:	Epoch: [34][58/204]	Loss 0.5132 (0.5559)	
training:	Epoch: [34][59/204]	Loss 0.6816 (0.5581)	
training:	Epoch: [34][60/204]	Loss 0.5512 (0.5580)	
training:	Epoch: [34][61/204]	Loss 0.5357 (0.5576)	
training:	Epoch: [34][62/204]	Loss 0.5893 (0.5581)	
training:	Epoch: [34][63/204]	Loss 0.5479 (0.5579)	
training:	Epoch: [34][64/204]	Loss 0.5023 (0.5571)	
training:	Epoch: [34][65/204]	Loss 0.6158 (0.5580)	
training:	Epoch: [34][66/204]	Loss 0.6168 (0.5589)	
training:	Epoch: [34][67/204]	Loss 0.5762 (0.5591)	
training:	Epoch: [34][68/204]	Loss 0.5440 (0.5589)	
training:	Epoch: [34][69/204]	Loss 0.6014 (0.5595)	
training:	Epoch: [34][70/204]	Loss 0.5914 (0.5600)	
training:	Epoch: [34][71/204]	Loss 0.5648 (0.5600)	
training:	Epoch: [34][72/204]	Loss 0.5623 (0.5601)	
training:	Epoch: [34][73/204]	Loss 0.5293 (0.5596)	
training:	Epoch: [34][74/204]	Loss 0.6019 (0.5602)	
training:	Epoch: [34][75/204]	Loss 0.4973 (0.5594)	
training:	Epoch: [34][76/204]	Loss 0.4966 (0.5586)	
training:	Epoch: [34][77/204]	Loss 0.4961 (0.5577)	
training:	Epoch: [34][78/204]	Loss 0.6208 (0.5585)	
training:	Epoch: [34][79/204]	Loss 0.6111 (0.5592)	
training:	Epoch: [34][80/204]	Loss 0.5613 (0.5592)	
training:	Epoch: [34][81/204]	Loss 0.4336 (0.5577)	
training:	Epoch: [34][82/204]	Loss 0.4885 (0.5568)	
training:	Epoch: [34][83/204]	Loss 0.4941 (0.5561)	
training:	Epoch: [34][84/204]	Loss 0.5120 (0.5556)	
training:	Epoch: [34][85/204]	Loss 0.5438 (0.5554)	
training:	Epoch: [34][86/204]	Loss 0.6129 (0.5561)	
training:	Epoch: [34][87/204]	Loss 0.5411 (0.5559)	
training:	Epoch: [34][88/204]	Loss 0.5954 (0.5564)	
training:	Epoch: [34][89/204]	Loss 0.5002 (0.5557)	
training:	Epoch: [34][90/204]	Loss 0.5210 (0.5554)	
training:	Epoch: [34][91/204]	Loss 0.6138 (0.5560)	
training:	Epoch: [34][92/204]	Loss 0.5036 (0.5554)	
training:	Epoch: [34][93/204]	Loss 0.5952 (0.5559)	
training:	Epoch: [34][94/204]	Loss 0.6355 (0.5567)	
training:	Epoch: [34][95/204]	Loss 0.5163 (0.5563)	
training:	Epoch: [34][96/204]	Loss 0.4658 (0.5553)	
training:	Epoch: [34][97/204]	Loss 0.5457 (0.5552)	
training:	Epoch: [34][98/204]	Loss 0.6126 (0.5558)	
training:	Epoch: [34][99/204]	Loss 0.4637 (0.5549)	
training:	Epoch: [34][100/204]	Loss 0.4941 (0.5543)	
training:	Epoch: [34][101/204]	Loss 0.6130 (0.5549)	
training:	Epoch: [34][102/204]	Loss 0.7063 (0.5564)	
training:	Epoch: [34][103/204]	Loss 0.4776 (0.5556)	
training:	Epoch: [34][104/204]	Loss 0.5600 (0.5556)	
training:	Epoch: [34][105/204]	Loss 0.6049 (0.5561)	
training:	Epoch: [34][106/204]	Loss 0.5914 (0.5564)	
training:	Epoch: [34][107/204]	Loss 0.4780 (0.5557)	
training:	Epoch: [34][108/204]	Loss 0.5268 (0.5554)	
training:	Epoch: [34][109/204]	Loss 0.6423 (0.5562)	
training:	Epoch: [34][110/204]	Loss 0.6127 (0.5567)	
training:	Epoch: [34][111/204]	Loss 0.5840 (0.5570)	
training:	Epoch: [34][112/204]	Loss 0.5775 (0.5572)	
training:	Epoch: [34][113/204]	Loss 0.5958 (0.5575)	
training:	Epoch: [34][114/204]	Loss 0.4468 (0.5565)	
training:	Epoch: [34][115/204]	Loss 0.5376 (0.5564)	
training:	Epoch: [34][116/204]	Loss 0.5919 (0.5567)	
training:	Epoch: [34][117/204]	Loss 0.5086 (0.5563)	
training:	Epoch: [34][118/204]	Loss 0.5992 (0.5566)	
training:	Epoch: [34][119/204]	Loss 0.4434 (0.5557)	
training:	Epoch: [34][120/204]	Loss 0.4960 (0.5552)	
training:	Epoch: [34][121/204]	Loss 0.5594 (0.5552)	
training:	Epoch: [34][122/204]	Loss 0.5939 (0.5555)	
training:	Epoch: [34][123/204]	Loss 0.4648 (0.5548)	
training:	Epoch: [34][124/204]	Loss 0.5472 (0.5547)	
training:	Epoch: [34][125/204]	Loss 0.4171 (0.5536)	
training:	Epoch: [34][126/204]	Loss 0.4894 (0.5531)	
training:	Epoch: [34][127/204]	Loss 0.5319 (0.5530)	
training:	Epoch: [34][128/204]	Loss 0.5660 (0.5531)	
training:	Epoch: [34][129/204]	Loss 0.6747 (0.5540)	
training:	Epoch: [34][130/204]	Loss 0.4713 (0.5534)	
training:	Epoch: [34][131/204]	Loss 0.5084 (0.5530)	
training:	Epoch: [34][132/204]	Loss 0.5556 (0.5530)	
training:	Epoch: [34][133/204]	Loss 0.5938 (0.5534)	
training:	Epoch: [34][134/204]	Loss 0.4997 (0.5530)	
training:	Epoch: [34][135/204]	Loss 0.4010 (0.5518)	
training:	Epoch: [34][136/204]	Loss 0.5661 (0.5519)	
training:	Epoch: [34][137/204]	Loss 0.5780 (0.5521)	
training:	Epoch: [34][138/204]	Loss 0.5680 (0.5522)	
training:	Epoch: [34][139/204]	Loss 0.4858 (0.5518)	
training:	Epoch: [34][140/204]	Loss 0.6083 (0.5522)	
training:	Epoch: [34][141/204]	Loss 0.5515 (0.5522)	
training:	Epoch: [34][142/204]	Loss 0.6266 (0.5527)	
training:	Epoch: [34][143/204]	Loss 0.5794 (0.5529)	
training:	Epoch: [34][144/204]	Loss 0.5169 (0.5526)	
training:	Epoch: [34][145/204]	Loss 0.5644 (0.5527)	
training:	Epoch: [34][146/204]	Loss 0.4368 (0.5519)	
training:	Epoch: [34][147/204]	Loss 0.5570 (0.5519)	
training:	Epoch: [34][148/204]	Loss 0.4215 (0.5511)	
training:	Epoch: [34][149/204]	Loss 0.4816 (0.5506)	
training:	Epoch: [34][150/204]	Loss 0.5694 (0.5507)	
training:	Epoch: [34][151/204]	Loss 0.4598 (0.5501)	
training:	Epoch: [34][152/204]	Loss 0.5579 (0.5502)	
training:	Epoch: [34][153/204]	Loss 0.5157 (0.5499)	
training:	Epoch: [34][154/204]	Loss 0.5492 (0.5499)	
training:	Epoch: [34][155/204]	Loss 0.5658 (0.5500)	
training:	Epoch: [34][156/204]	Loss 0.5427 (0.5500)	
training:	Epoch: [34][157/204]	Loss 0.6051 (0.5503)	
training:	Epoch: [34][158/204]	Loss 0.5186 (0.5501)	
training:	Epoch: [34][159/204]	Loss 0.5258 (0.5500)	
training:	Epoch: [34][160/204]	Loss 0.5153 (0.5498)	
training:	Epoch: [34][161/204]	Loss 0.5143 (0.5496)	
training:	Epoch: [34][162/204]	Loss 0.6222 (0.5500)	
training:	Epoch: [34][163/204]	Loss 0.5832 (0.5502)	
training:	Epoch: [34][164/204]	Loss 0.5650 (0.5503)	
training:	Epoch: [34][165/204]	Loss 0.6818 (0.5511)	
training:	Epoch: [34][166/204]	Loss 0.5871 (0.5513)	
training:	Epoch: [34][167/204]	Loss 0.5697 (0.5514)	
training:	Epoch: [34][168/204]	Loss 0.5905 (0.5517)	
training:	Epoch: [34][169/204]	Loss 0.5598 (0.5517)	
training:	Epoch: [34][170/204]	Loss 0.6602 (0.5523)	
training:	Epoch: [34][171/204]	Loss 0.5624 (0.5524)	
training:	Epoch: [34][172/204]	Loss 0.6748 (0.5531)	
training:	Epoch: [34][173/204]	Loss 0.5814 (0.5533)	
training:	Epoch: [34][174/204]	Loss 0.5889 (0.5535)	
training:	Epoch: [34][175/204]	Loss 0.5182 (0.5533)	
training:	Epoch: [34][176/204]	Loss 0.4778 (0.5528)	
training:	Epoch: [34][177/204]	Loss 0.5865 (0.5530)	
training:	Epoch: [34][178/204]	Loss 0.4893 (0.5527)	
training:	Epoch: [34][179/204]	Loss 0.5960 (0.5529)	
training:	Epoch: [34][180/204]	Loss 0.5772 (0.5531)	
training:	Epoch: [34][181/204]	Loss 0.4645 (0.5526)	
training:	Epoch: [34][182/204]	Loss 0.5708 (0.5527)	
training:	Epoch: [34][183/204]	Loss 0.6097 (0.5530)	
training:	Epoch: [34][184/204]	Loss 0.5897 (0.5532)	
training:	Epoch: [34][185/204]	Loss 0.4357 (0.5525)	
training:	Epoch: [34][186/204]	Loss 0.4790 (0.5521)	
training:	Epoch: [34][187/204]	Loss 0.5949 (0.5524)	
training:	Epoch: [34][188/204]	Loss 0.5908 (0.5526)	
training:	Epoch: [34][189/204]	Loss 0.4911 (0.5523)	
training:	Epoch: [34][190/204]	Loss 0.5152 (0.5521)	
training:	Epoch: [34][191/204]	Loss 0.4890 (0.5517)	
training:	Epoch: [34][192/204]	Loss 0.5142 (0.5515)	
training:	Epoch: [34][193/204]	Loss 0.5833 (0.5517)	
training:	Epoch: [34][194/204]	Loss 0.6469 (0.5522)	
training:	Epoch: [34][195/204]	Loss 0.5527 (0.5522)	
training:	Epoch: [34][196/204]	Loss 0.6280 (0.5526)	
training:	Epoch: [34][197/204]	Loss 0.4093 (0.5519)	
training:	Epoch: [34][198/204]	Loss 0.3566 (0.5509)	
training:	Epoch: [34][199/204]	Loss 0.6625 (0.5514)	
training:	Epoch: [34][200/204]	Loss 0.5523 (0.5514)	
training:	Epoch: [34][201/204]	Loss 0.5580 (0.5515)	
training:	Epoch: [34][202/204]	Loss 0.5986 (0.5517)	
training:	Epoch: [34][203/204]	Loss 0.5645 (0.5518)	
training:	Epoch: [34][204/204]	Loss 0.5452 (0.5517)	
Training:	 Loss: 0.5509

Training:	 ACC: 0.7329 0.7339 0.7563 0.7095
Validation:	 ACC: 0.7347 0.7362 0.7687 0.7007
Validation:	 Best_BACC: 0.7347 0.7362 0.7687 0.7007
Validation:	 Loss: 0.5459
Pretraining:	Epoch 35/120
----------
training:	Epoch: [35][1/204]	Loss 0.5721 (0.5721)	
training:	Epoch: [35][2/204]	Loss 0.5366 (0.5544)	
training:	Epoch: [35][3/204]	Loss 0.5928 (0.5672)	
training:	Epoch: [35][4/204]	Loss 0.5962 (0.5744)	
training:	Epoch: [35][5/204]	Loss 0.6586 (0.5913)	
training:	Epoch: [35][6/204]	Loss 0.4425 (0.5665)	
training:	Epoch: [35][7/204]	Loss 0.6450 (0.5777)	
training:	Epoch: [35][8/204]	Loss 0.6030 (0.5809)	
training:	Epoch: [35][9/204]	Loss 0.5210 (0.5742)	
training:	Epoch: [35][10/204]	Loss 0.5784 (0.5746)	
training:	Epoch: [35][11/204]	Loss 0.5368 (0.5712)	
training:	Epoch: [35][12/204]	Loss 0.6214 (0.5754)	
training:	Epoch: [35][13/204]	Loss 0.3904 (0.5611)	
training:	Epoch: [35][14/204]	Loss 0.6774 (0.5694)	
training:	Epoch: [35][15/204]	Loss 0.5509 (0.5682)	
training:	Epoch: [35][16/204]	Loss 0.5668 (0.5681)	
training:	Epoch: [35][17/204]	Loss 0.5162 (0.5651)	
training:	Epoch: [35][18/204]	Loss 0.5240 (0.5628)	
training:	Epoch: [35][19/204]	Loss 0.5591 (0.5626)	
training:	Epoch: [35][20/204]	Loss 0.5285 (0.5609)	
training:	Epoch: [35][21/204]	Loss 0.5365 (0.5597)	
training:	Epoch: [35][22/204]	Loss 0.4717 (0.5557)	
training:	Epoch: [35][23/204]	Loss 0.5245 (0.5544)	
training:	Epoch: [35][24/204]	Loss 0.4694 (0.5508)	
training:	Epoch: [35][25/204]	Loss 0.5451 (0.5506)	
training:	Epoch: [35][26/204]	Loss 0.4785 (0.5478)	
training:	Epoch: [35][27/204]	Loss 0.6078 (0.5500)	
training:	Epoch: [35][28/204]	Loss 0.5290 (0.5493)	
training:	Epoch: [35][29/204]	Loss 0.5817 (0.5504)	
training:	Epoch: [35][30/204]	Loss 0.5178 (0.5493)	
training:	Epoch: [35][31/204]	Loss 0.4604 (0.5465)	
training:	Epoch: [35][32/204]	Loss 0.5524 (0.5466)	
training:	Epoch: [35][33/204]	Loss 0.5786 (0.5476)	
training:	Epoch: [35][34/204]	Loss 0.6607 (0.5509)	
training:	Epoch: [35][35/204]	Loss 0.5042 (0.5496)	
training:	Epoch: [35][36/204]	Loss 0.5018 (0.5483)	
training:	Epoch: [35][37/204]	Loss 0.6803 (0.5518)	
training:	Epoch: [35][38/204]	Loss 0.4385 (0.5489)	
training:	Epoch: [35][39/204]	Loss 0.4953 (0.5475)	
training:	Epoch: [35][40/204]	Loss 0.5171 (0.5467)	
training:	Epoch: [35][41/204]	Loss 0.4706 (0.5449)	
training:	Epoch: [35][42/204]	Loss 0.6003 (0.5462)	
training:	Epoch: [35][43/204]	Loss 0.6204 (0.5479)	
training:	Epoch: [35][44/204]	Loss 0.5234 (0.5474)	
training:	Epoch: [35][45/204]	Loss 0.5236 (0.5468)	
training:	Epoch: [35][46/204]	Loss 0.4041 (0.5437)	
training:	Epoch: [35][47/204]	Loss 0.5490 (0.5438)	
training:	Epoch: [35][48/204]	Loss 0.5502 (0.5440)	
training:	Epoch: [35][49/204]	Loss 0.6421 (0.5460)	
training:	Epoch: [35][50/204]	Loss 0.5046 (0.5451)	
training:	Epoch: [35][51/204]	Loss 0.4622 (0.5435)	
training:	Epoch: [35][52/204]	Loss 0.6581 (0.5457)	
training:	Epoch: [35][53/204]	Loss 0.6262 (0.5472)	
training:	Epoch: [35][54/204]	Loss 0.5228 (0.5468)	
training:	Epoch: [35][55/204]	Loss 0.4567 (0.5451)	
training:	Epoch: [35][56/204]	Loss 0.5586 (0.5454)	
training:	Epoch: [35][57/204]	Loss 0.5307 (0.5451)	
training:	Epoch: [35][58/204]	Loss 0.6193 (0.5464)	
training:	Epoch: [35][59/204]	Loss 0.4865 (0.5454)	
training:	Epoch: [35][60/204]	Loss 0.6231 (0.5467)	
training:	Epoch: [35][61/204]	Loss 0.5838 (0.5473)	
training:	Epoch: [35][62/204]	Loss 0.5010 (0.5465)	
training:	Epoch: [35][63/204]	Loss 0.7029 (0.5490)	
training:	Epoch: [35][64/204]	Loss 0.5922 (0.5497)	
training:	Epoch: [35][65/204]	Loss 0.5020 (0.5490)	
training:	Epoch: [35][66/204]	Loss 0.5679 (0.5493)	
training:	Epoch: [35][67/204]	Loss 0.6542 (0.5508)	
training:	Epoch: [35][68/204]	Loss 0.5868 (0.5514)	
training:	Epoch: [35][69/204]	Loss 0.6565 (0.5529)	
training:	Epoch: [35][70/204]	Loss 0.5036 (0.5522)	
training:	Epoch: [35][71/204]	Loss 0.5029 (0.5515)	
training:	Epoch: [35][72/204]	Loss 0.6588 (0.5530)	
training:	Epoch: [35][73/204]	Loss 0.5136 (0.5524)	
training:	Epoch: [35][74/204]	Loss 0.5363 (0.5522)	
training:	Epoch: [35][75/204]	Loss 0.4743 (0.5512)	
training:	Epoch: [35][76/204]	Loss 0.5427 (0.5511)	
training:	Epoch: [35][77/204]	Loss 0.4638 (0.5499)	
training:	Epoch: [35][78/204]	Loss 0.5440 (0.5498)	
training:	Epoch: [35][79/204]	Loss 0.6090 (0.5506)	
training:	Epoch: [35][80/204]	Loss 0.5282 (0.5503)	
training:	Epoch: [35][81/204]	Loss 0.5381 (0.5502)	
training:	Epoch: [35][82/204]	Loss 0.5739 (0.5505)	
training:	Epoch: [35][83/204]	Loss 0.6075 (0.5511)	
training:	Epoch: [35][84/204]	Loss 0.4491 (0.5499)	
training:	Epoch: [35][85/204]	Loss 0.5449 (0.5499)	
training:	Epoch: [35][86/204]	Loss 0.5812 (0.5502)	
training:	Epoch: [35][87/204]	Loss 0.5882 (0.5507)	
training:	Epoch: [35][88/204]	Loss 0.5237 (0.5504)	
training:	Epoch: [35][89/204]	Loss 0.4273 (0.5490)	
training:	Epoch: [35][90/204]	Loss 0.6444 (0.5500)	
training:	Epoch: [35][91/204]	Loss 0.5691 (0.5503)	
training:	Epoch: [35][92/204]	Loss 0.6754 (0.5516)	
training:	Epoch: [35][93/204]	Loss 0.4495 (0.5505)	
training:	Epoch: [35][94/204]	Loss 0.3552 (0.5484)	
training:	Epoch: [35][95/204]	Loss 0.5167 (0.5481)	
training:	Epoch: [35][96/204]	Loss 0.5482 (0.5481)	
training:	Epoch: [35][97/204]	Loss 0.5171 (0.5478)	
training:	Epoch: [35][98/204]	Loss 0.5900 (0.5482)	
training:	Epoch: [35][99/204]	Loss 0.7198 (0.5499)	
training:	Epoch: [35][100/204]	Loss 0.6382 (0.5508)	
training:	Epoch: [35][101/204]	Loss 0.5649 (0.5510)	
training:	Epoch: [35][102/204]	Loss 0.5387 (0.5509)	
training:	Epoch: [35][103/204]	Loss 0.5540 (0.5509)	
training:	Epoch: [35][104/204]	Loss 0.4610 (0.5500)	
training:	Epoch: [35][105/204]	Loss 0.5943 (0.5504)	
training:	Epoch: [35][106/204]	Loss 0.6690 (0.5516)	
training:	Epoch: [35][107/204]	Loss 0.5329 (0.5514)	
training:	Epoch: [35][108/204]	Loss 0.5037 (0.5509)	
training:	Epoch: [35][109/204]	Loss 0.5265 (0.5507)	
training:	Epoch: [35][110/204]	Loss 0.5900 (0.5511)	
training:	Epoch: [35][111/204]	Loss 0.6666 (0.5521)	
training:	Epoch: [35][112/204]	Loss 0.5644 (0.5522)	
training:	Epoch: [35][113/204]	Loss 0.6627 (0.5532)	
training:	Epoch: [35][114/204]	Loss 0.5014 (0.5527)	
training:	Epoch: [35][115/204]	Loss 0.4822 (0.5521)	
training:	Epoch: [35][116/204]	Loss 0.5273 (0.5519)	
training:	Epoch: [35][117/204]	Loss 0.4927 (0.5514)	
training:	Epoch: [35][118/204]	Loss 0.5938 (0.5518)	
training:	Epoch: [35][119/204]	Loss 0.6288 (0.5524)	
training:	Epoch: [35][120/204]	Loss 0.4457 (0.5515)	
training:	Epoch: [35][121/204]	Loss 0.5588 (0.5516)	
training:	Epoch: [35][122/204]	Loss 0.4086 (0.5504)	
training:	Epoch: [35][123/204]	Loss 0.5760 (0.5506)	
training:	Epoch: [35][124/204]	Loss 0.4808 (0.5501)	
training:	Epoch: [35][125/204]	Loss 0.5067 (0.5497)	
training:	Epoch: [35][126/204]	Loss 0.6855 (0.5508)	
training:	Epoch: [35][127/204]	Loss 0.5194 (0.5505)	
training:	Epoch: [35][128/204]	Loss 0.4993 (0.5501)	
training:	Epoch: [35][129/204]	Loss 0.5268 (0.5500)	
training:	Epoch: [35][130/204]	Loss 0.4795 (0.5494)	
training:	Epoch: [35][131/204]	Loss 0.5847 (0.5497)	
training:	Epoch: [35][132/204]	Loss 0.5907 (0.5500)	
training:	Epoch: [35][133/204]	Loss 0.5865 (0.5503)	
training:	Epoch: [35][134/204]	Loss 0.5048 (0.5499)	
training:	Epoch: [35][135/204]	Loss 0.5819 (0.5502)	
training:	Epoch: [35][136/204]	Loss 0.5410 (0.5501)	
training:	Epoch: [35][137/204]	Loss 0.5512 (0.5501)	
training:	Epoch: [35][138/204]	Loss 0.4523 (0.5494)	
training:	Epoch: [35][139/204]	Loss 0.5461 (0.5494)	
training:	Epoch: [35][140/204]	Loss 0.5767 (0.5496)	
training:	Epoch: [35][141/204]	Loss 0.5497 (0.5496)	
training:	Epoch: [35][142/204]	Loss 0.6607 (0.5504)	
training:	Epoch: [35][143/204]	Loss 0.6829 (0.5513)	
training:	Epoch: [35][144/204]	Loss 0.4867 (0.5508)	
training:	Epoch: [35][145/204]	Loss 0.4690 (0.5503)	
training:	Epoch: [35][146/204]	Loss 0.5677 (0.5504)	
training:	Epoch: [35][147/204]	Loss 0.5343 (0.5503)	
training:	Epoch: [35][148/204]	Loss 0.5659 (0.5504)	
training:	Epoch: [35][149/204]	Loss 0.5041 (0.5501)	
training:	Epoch: [35][150/204]	Loss 0.5016 (0.5498)	
training:	Epoch: [35][151/204]	Loss 0.5413 (0.5497)	
training:	Epoch: [35][152/204]	Loss 0.5349 (0.5496)	
training:	Epoch: [35][153/204]	Loss 0.5721 (0.5498)	
training:	Epoch: [35][154/204]	Loss 0.4521 (0.5491)	
training:	Epoch: [35][155/204]	Loss 0.5773 (0.5493)	
training:	Epoch: [35][156/204]	Loss 0.5556 (0.5493)	
training:	Epoch: [35][157/204]	Loss 0.4218 (0.5485)	
training:	Epoch: [35][158/204]	Loss 0.5241 (0.5484)	
training:	Epoch: [35][159/204]	Loss 0.5967 (0.5487)	
training:	Epoch: [35][160/204]	Loss 0.5546 (0.5487)	
training:	Epoch: [35][161/204]	Loss 0.6784 (0.5495)	
training:	Epoch: [35][162/204]	Loss 0.5081 (0.5493)	
training:	Epoch: [35][163/204]	Loss 0.6179 (0.5497)	
training:	Epoch: [35][164/204]	Loss 0.6306 (0.5502)	
training:	Epoch: [35][165/204]	Loss 0.5714 (0.5503)	
training:	Epoch: [35][166/204]	Loss 0.5107 (0.5501)	
training:	Epoch: [35][167/204]	Loss 0.5824 (0.5503)	
training:	Epoch: [35][168/204]	Loss 0.4320 (0.5496)	
training:	Epoch: [35][169/204]	Loss 0.5435 (0.5495)	
training:	Epoch: [35][170/204]	Loss 0.5022 (0.5492)	
training:	Epoch: [35][171/204]	Loss 0.5812 (0.5494)	
training:	Epoch: [35][172/204]	Loss 0.5619 (0.5495)	
training:	Epoch: [35][173/204]	Loss 0.5639 (0.5496)	
training:	Epoch: [35][174/204]	Loss 0.4872 (0.5492)	
training:	Epoch: [35][175/204]	Loss 0.5324 (0.5491)	
training:	Epoch: [35][176/204]	Loss 0.4533 (0.5486)	
training:	Epoch: [35][177/204]	Loss 0.5716 (0.5487)	
training:	Epoch: [35][178/204]	Loss 0.4563 (0.5482)	
training:	Epoch: [35][179/204]	Loss 0.5127 (0.5480)	
training:	Epoch: [35][180/204]	Loss 0.5666 (0.5481)	
training:	Epoch: [35][181/204]	Loss 0.5933 (0.5484)	
training:	Epoch: [35][182/204]	Loss 0.5377 (0.5483)	
training:	Epoch: [35][183/204]	Loss 0.5832 (0.5485)	
training:	Epoch: [35][184/204]	Loss 0.5928 (0.5487)	
training:	Epoch: [35][185/204]	Loss 0.5962 (0.5490)	
training:	Epoch: [35][186/204]	Loss 0.6623 (0.5496)	
training:	Epoch: [35][187/204]	Loss 0.5167 (0.5494)	
training:	Epoch: [35][188/204]	Loss 0.4972 (0.5491)	
training:	Epoch: [35][189/204]	Loss 0.4891 (0.5488)	
training:	Epoch: [35][190/204]	Loss 0.5195 (0.5487)	
training:	Epoch: [35][191/204]	Loss 0.4844 (0.5483)	
training:	Epoch: [35][192/204]	Loss 0.5499 (0.5483)	
training:	Epoch: [35][193/204]	Loss 0.5245 (0.5482)	
training:	Epoch: [35][194/204]	Loss 0.6715 (0.5488)	
training:	Epoch: [35][195/204]	Loss 0.5180 (0.5487)	
training:	Epoch: [35][196/204]	Loss 0.5545 (0.5487)	
training:	Epoch: [35][197/204]	Loss 0.4645 (0.5483)	
training:	Epoch: [35][198/204]	Loss 0.4735 (0.5479)	
training:	Epoch: [35][199/204]	Loss 0.4803 (0.5476)	
training:	Epoch: [35][200/204]	Loss 0.4380 (0.5470)	
training:	Epoch: [35][201/204]	Loss 0.6341 (0.5475)	
training:	Epoch: [35][202/204]	Loss 0.4749 (0.5471)	
training:	Epoch: [35][203/204]	Loss 0.4867 (0.5468)	
training:	Epoch: [35][204/204]	Loss 0.5180 (0.5467)	
Training:	 Loss: 0.5458

Training:	 ACC: 0.7362 0.7372 0.7622 0.7101
Validation:	 ACC: 0.7379 0.7394 0.7718 0.7040
Validation:	 Best_BACC: 0.7379 0.7394 0.7718 0.7040
Validation:	 Loss: 0.5428
Pretraining:	Epoch 36/120
----------
training:	Epoch: [36][1/204]	Loss 0.5616 (0.5616)	
training:	Epoch: [36][2/204]	Loss 0.5222 (0.5419)	
training:	Epoch: [36][3/204]	Loss 0.5838 (0.5558)	
training:	Epoch: [36][4/204]	Loss 0.5017 (0.5423)	
training:	Epoch: [36][5/204]	Loss 0.6176 (0.5574)	
training:	Epoch: [36][6/204]	Loss 0.4695 (0.5427)	
training:	Epoch: [36][7/204]	Loss 0.6178 (0.5534)	
training:	Epoch: [36][8/204]	Loss 0.5215 (0.5494)	
training:	Epoch: [36][9/204]	Loss 0.6269 (0.5581)	
training:	Epoch: [36][10/204]	Loss 0.5529 (0.5575)	
training:	Epoch: [36][11/204]	Loss 0.5182 (0.5540)	
training:	Epoch: [36][12/204]	Loss 0.4893 (0.5486)	
training:	Epoch: [36][13/204]	Loss 0.5997 (0.5525)	
training:	Epoch: [36][14/204]	Loss 0.6368 (0.5585)	
training:	Epoch: [36][15/204]	Loss 0.5454 (0.5577)	
training:	Epoch: [36][16/204]	Loss 0.5148 (0.5550)	
training:	Epoch: [36][17/204]	Loss 0.4617 (0.5495)	
training:	Epoch: [36][18/204]	Loss 0.5853 (0.5515)	
training:	Epoch: [36][19/204]	Loss 0.5035 (0.5490)	
training:	Epoch: [36][20/204]	Loss 0.6865 (0.5558)	
training:	Epoch: [36][21/204]	Loss 0.7140 (0.5634)	
training:	Epoch: [36][22/204]	Loss 0.4864 (0.5599)	
training:	Epoch: [36][23/204]	Loss 0.4764 (0.5562)	
training:	Epoch: [36][24/204]	Loss 0.5204 (0.5547)	
training:	Epoch: [36][25/204]	Loss 0.4606 (0.5510)	
training:	Epoch: [36][26/204]	Loss 0.4444 (0.5469)	
training:	Epoch: [36][27/204]	Loss 0.5156 (0.5457)	
training:	Epoch: [36][28/204]	Loss 0.4994 (0.5441)	
training:	Epoch: [36][29/204]	Loss 0.5804 (0.5453)	
training:	Epoch: [36][30/204]	Loss 0.4883 (0.5434)	
training:	Epoch: [36][31/204]	Loss 0.5041 (0.5422)	
training:	Epoch: [36][32/204]	Loss 0.5450 (0.5422)	
training:	Epoch: [36][33/204]	Loss 0.4576 (0.5397)	
training:	Epoch: [36][34/204]	Loss 0.5377 (0.5396)	
training:	Epoch: [36][35/204]	Loss 0.5691 (0.5405)	
training:	Epoch: [36][36/204]	Loss 0.5111 (0.5396)	
training:	Epoch: [36][37/204]	Loss 0.6371 (0.5423)	
training:	Epoch: [36][38/204]	Loss 0.6937 (0.5463)	
training:	Epoch: [36][39/204]	Loss 0.4307 (0.5433)	
training:	Epoch: [36][40/204]	Loss 0.5733 (0.5441)	
training:	Epoch: [36][41/204]	Loss 0.5405 (0.5440)	
training:	Epoch: [36][42/204]	Loss 0.5076 (0.5431)	
training:	Epoch: [36][43/204]	Loss 0.5494 (0.5432)	
training:	Epoch: [36][44/204]	Loss 0.6258 (0.5451)	
training:	Epoch: [36][45/204]	Loss 0.5354 (0.5449)	
training:	Epoch: [36][46/204]	Loss 0.5332 (0.5446)	
training:	Epoch: [36][47/204]	Loss 0.6220 (0.5463)	
training:	Epoch: [36][48/204]	Loss 0.6480 (0.5484)	
training:	Epoch: [36][49/204]	Loss 0.6895 (0.5513)	
training:	Epoch: [36][50/204]	Loss 0.5614 (0.5515)	
training:	Epoch: [36][51/204]	Loss 0.4001 (0.5485)	
training:	Epoch: [36][52/204]	Loss 0.4970 (0.5475)	
training:	Epoch: [36][53/204]	Loss 0.5034 (0.5467)	
training:	Epoch: [36][54/204]	Loss 0.6219 (0.5481)	
training:	Epoch: [36][55/204]	Loss 0.4099 (0.5456)	
training:	Epoch: [36][56/204]	Loss 0.6390 (0.5473)	
training:	Epoch: [36][57/204]	Loss 0.5700 (0.5477)	
training:	Epoch: [36][58/204]	Loss 0.4980 (0.5468)	
training:	Epoch: [36][59/204]	Loss 0.6335 (0.5483)	
training:	Epoch: [36][60/204]	Loss 0.5397 (0.5481)	
training:	Epoch: [36][61/204]	Loss 0.6209 (0.5493)	
training:	Epoch: [36][62/204]	Loss 0.6558 (0.5510)	
training:	Epoch: [36][63/204]	Loss 0.6009 (0.5518)	
training:	Epoch: [36][64/204]	Loss 0.4952 (0.5509)	
training:	Epoch: [36][65/204]	Loss 0.4756 (0.5498)	
training:	Epoch: [36][66/204]	Loss 0.4387 (0.5481)	
training:	Epoch: [36][67/204]	Loss 0.5334 (0.5479)	
training:	Epoch: [36][68/204]	Loss 0.4234 (0.5460)	
training:	Epoch: [36][69/204]	Loss 0.6613 (0.5477)	
training:	Epoch: [36][70/204]	Loss 0.4530 (0.5464)	
training:	Epoch: [36][71/204]	Loss 0.5100 (0.5459)	
training:	Epoch: [36][72/204]	Loss 0.5141 (0.5454)	
training:	Epoch: [36][73/204]	Loss 0.5078 (0.5449)	
training:	Epoch: [36][74/204]	Loss 0.5633 (0.5451)	
training:	Epoch: [36][75/204]	Loss 0.4287 (0.5436)	
training:	Epoch: [36][76/204]	Loss 0.4657 (0.5426)	
training:	Epoch: [36][77/204]	Loss 0.5042 (0.5421)	
training:	Epoch: [36][78/204]	Loss 0.5003 (0.5415)	
training:	Epoch: [36][79/204]	Loss 0.5506 (0.5416)	
training:	Epoch: [36][80/204]	Loss 0.5336 (0.5415)	
training:	Epoch: [36][81/204]	Loss 0.5837 (0.5421)	
training:	Epoch: [36][82/204]	Loss 0.5254 (0.5419)	
training:	Epoch: [36][83/204]	Loss 0.5219 (0.5416)	
training:	Epoch: [36][84/204]	Loss 0.6499 (0.5429)	
training:	Epoch: [36][85/204]	Loss 0.5775 (0.5433)	
training:	Epoch: [36][86/204]	Loss 0.4965 (0.5428)	
training:	Epoch: [36][87/204]	Loss 0.4824 (0.5421)	
training:	Epoch: [36][88/204]	Loss 0.5636 (0.5423)	
training:	Epoch: [36][89/204]	Loss 0.6073 (0.5431)	
training:	Epoch: [36][90/204]	Loss 0.4994 (0.5426)	
training:	Epoch: [36][91/204]	Loss 0.4193 (0.5412)	
training:	Epoch: [36][92/204]	Loss 0.5837 (0.5417)	
training:	Epoch: [36][93/204]	Loss 0.4906 (0.5411)	
training:	Epoch: [36][94/204]	Loss 0.4885 (0.5406)	
training:	Epoch: [36][95/204]	Loss 0.5167 (0.5403)	
training:	Epoch: [36][96/204]	Loss 0.5625 (0.5405)	
training:	Epoch: [36][97/204]	Loss 0.5176 (0.5403)	
training:	Epoch: [36][98/204]	Loss 0.5726 (0.5406)	
training:	Epoch: [36][99/204]	Loss 0.5070 (0.5403)	
training:	Epoch: [36][100/204]	Loss 0.4727 (0.5396)	
training:	Epoch: [36][101/204]	Loss 0.5524 (0.5398)	
training:	Epoch: [36][102/204]	Loss 0.4313 (0.5387)	
training:	Epoch: [36][103/204]	Loss 0.5353 (0.5387)	
training:	Epoch: [36][104/204]	Loss 0.5438 (0.5387)	
training:	Epoch: [36][105/204]	Loss 0.5727 (0.5390)	
training:	Epoch: [36][106/204]	Loss 0.6186 (0.5398)	
training:	Epoch: [36][107/204]	Loss 0.7087 (0.5414)	
training:	Epoch: [36][108/204]	Loss 0.5069 (0.5410)	
training:	Epoch: [36][109/204]	Loss 0.5685 (0.5413)	
training:	Epoch: [36][110/204]	Loss 0.5852 (0.5417)	
training:	Epoch: [36][111/204]	Loss 0.6448 (0.5426)	
training:	Epoch: [36][112/204]	Loss 0.4339 (0.5416)	
training:	Epoch: [36][113/204]	Loss 0.6160 (0.5423)	
training:	Epoch: [36][114/204]	Loss 0.5230 (0.5421)	
training:	Epoch: [36][115/204]	Loss 0.6474 (0.5431)	
training:	Epoch: [36][116/204]	Loss 0.5325 (0.5430)	
training:	Epoch: [36][117/204]	Loss 0.4858 (0.5425)	
training:	Epoch: [36][118/204]	Loss 0.5184 (0.5423)	
training:	Epoch: [36][119/204]	Loss 0.6459 (0.5431)	
training:	Epoch: [36][120/204]	Loss 0.5302 (0.5430)	
training:	Epoch: [36][121/204]	Loss 0.5354 (0.5430)	
training:	Epoch: [36][122/204]	Loss 0.5122 (0.5427)	
training:	Epoch: [36][123/204]	Loss 0.4744 (0.5422)	
training:	Epoch: [36][124/204]	Loss 0.5462 (0.5422)	
training:	Epoch: [36][125/204]	Loss 0.5189 (0.5420)	
training:	Epoch: [36][126/204]	Loss 0.4262 (0.5411)	
training:	Epoch: [36][127/204]	Loss 0.6580 (0.5420)	
training:	Epoch: [36][128/204]	Loss 0.4686 (0.5414)	
training:	Epoch: [36][129/204]	Loss 0.5937 (0.5418)	
training:	Epoch: [36][130/204]	Loss 0.6024 (0.5423)	
training:	Epoch: [36][131/204]	Loss 0.5561 (0.5424)	
training:	Epoch: [36][132/204]	Loss 0.6675 (0.5434)	
training:	Epoch: [36][133/204]	Loss 0.4753 (0.5428)	
training:	Epoch: [36][134/204]	Loss 0.6515 (0.5437)	
training:	Epoch: [36][135/204]	Loss 0.5236 (0.5435)	
training:	Epoch: [36][136/204]	Loss 0.6803 (0.5445)	
training:	Epoch: [36][137/204]	Loss 0.5138 (0.5443)	
training:	Epoch: [36][138/204]	Loss 0.4372 (0.5435)	
training:	Epoch: [36][139/204]	Loss 0.5734 (0.5437)	
training:	Epoch: [36][140/204]	Loss 0.6563 (0.5445)	
training:	Epoch: [36][141/204]	Loss 0.5507 (0.5446)	
training:	Epoch: [36][142/204]	Loss 0.4927 (0.5442)	
training:	Epoch: [36][143/204]	Loss 0.4873 (0.5438)	
training:	Epoch: [36][144/204]	Loss 0.5887 (0.5441)	
training:	Epoch: [36][145/204]	Loss 0.5001 (0.5438)	
training:	Epoch: [36][146/204]	Loss 0.4882 (0.5434)	
training:	Epoch: [36][147/204]	Loss 0.4623 (0.5429)	
training:	Epoch: [36][148/204]	Loss 0.6200 (0.5434)	
training:	Epoch: [36][149/204]	Loss 0.5729 (0.5436)	
training:	Epoch: [36][150/204]	Loss 0.5151 (0.5434)	
training:	Epoch: [36][151/204]	Loss 0.5088 (0.5432)	
training:	Epoch: [36][152/204]	Loss 0.5697 (0.5434)	
training:	Epoch: [36][153/204]	Loss 0.5859 (0.5436)	
training:	Epoch: [36][154/204]	Loss 0.4892 (0.5433)	
training:	Epoch: [36][155/204]	Loss 0.5679 (0.5434)	
training:	Epoch: [36][156/204]	Loss 0.5761 (0.5437)	
training:	Epoch: [36][157/204]	Loss 0.5767 (0.5439)	
training:	Epoch: [36][158/204]	Loss 0.5976 (0.5442)	
training:	Epoch: [36][159/204]	Loss 0.5245 (0.5441)	
training:	Epoch: [36][160/204]	Loss 0.6028 (0.5445)	
training:	Epoch: [36][161/204]	Loss 0.5616 (0.5446)	
training:	Epoch: [36][162/204]	Loss 0.5540 (0.5446)	
training:	Epoch: [36][163/204]	Loss 0.5047 (0.5444)	
training:	Epoch: [36][164/204]	Loss 0.6060 (0.5447)	
training:	Epoch: [36][165/204]	Loss 0.6334 (0.5453)	
training:	Epoch: [36][166/204]	Loss 0.5831 (0.5455)	
training:	Epoch: [36][167/204]	Loss 0.5066 (0.5453)	
training:	Epoch: [36][168/204]	Loss 0.5723 (0.5454)	
training:	Epoch: [36][169/204]	Loss 0.5269 (0.5453)	
training:	Epoch: [36][170/204]	Loss 0.5086 (0.5451)	
training:	Epoch: [36][171/204]	Loss 0.4948 (0.5448)	
training:	Epoch: [36][172/204]	Loss 0.4210 (0.5441)	
training:	Epoch: [36][173/204]	Loss 0.5181 (0.5439)	
training:	Epoch: [36][174/204]	Loss 0.6337 (0.5445)	
training:	Epoch: [36][175/204]	Loss 0.5752 (0.5446)	
training:	Epoch: [36][176/204]	Loss 0.4862 (0.5443)	
training:	Epoch: [36][177/204]	Loss 0.6845 (0.5451)	
training:	Epoch: [36][178/204]	Loss 0.6015 (0.5454)	
training:	Epoch: [36][179/204]	Loss 0.6474 (0.5460)	
training:	Epoch: [36][180/204]	Loss 0.6231 (0.5464)	
training:	Epoch: [36][181/204]	Loss 0.5626 (0.5465)	
training:	Epoch: [36][182/204]	Loss 0.4348 (0.5459)	
training:	Epoch: [36][183/204]	Loss 0.5895 (0.5461)	
training:	Epoch: [36][184/204]	Loss 0.5015 (0.5459)	
training:	Epoch: [36][185/204]	Loss 0.4140 (0.5452)	
training:	Epoch: [36][186/204]	Loss 0.5305 (0.5451)	
training:	Epoch: [36][187/204]	Loss 0.5929 (0.5453)	
training:	Epoch: [36][188/204]	Loss 0.5243 (0.5452)	
training:	Epoch: [36][189/204]	Loss 0.5185 (0.5451)	
training:	Epoch: [36][190/204]	Loss 0.5635 (0.5452)	
training:	Epoch: [36][191/204]	Loss 0.6264 (0.5456)	
training:	Epoch: [36][192/204]	Loss 0.6150 (0.5460)	
training:	Epoch: [36][193/204]	Loss 0.5814 (0.5462)	
training:	Epoch: [36][194/204]	Loss 0.5613 (0.5462)	
training:	Epoch: [36][195/204]	Loss 0.4775 (0.5459)	
training:	Epoch: [36][196/204]	Loss 0.7170 (0.5468)	
training:	Epoch: [36][197/204]	Loss 0.4672 (0.5464)	
training:	Epoch: [36][198/204]	Loss 0.5239 (0.5462)	
training:	Epoch: [36][199/204]	Loss 0.6354 (0.5467)	
training:	Epoch: [36][200/204]	Loss 0.5029 (0.5465)	
training:	Epoch: [36][201/204]	Loss 0.4955 (0.5462)	
training:	Epoch: [36][202/204]	Loss 0.5951 (0.5465)	
training:	Epoch: [36][203/204]	Loss 0.5885 (0.5467)	
training:	Epoch: [36][204/204]	Loss 0.6150 (0.5470)	
Training:	 Loss: 0.5462

Training:	 ACC: 0.7390 0.7400 0.7634 0.7146
Validation:	 ACC: 0.7423 0.7437 0.7738 0.7108
Validation:	 Best_BACC: 0.7423 0.7437 0.7738 0.7108
Validation:	 Loss: 0.5391
Pretraining:	Epoch 37/120
----------
training:	Epoch: [37][1/204]	Loss 0.5533 (0.5533)	
training:	Epoch: [37][2/204]	Loss 0.5474 (0.5504)	
training:	Epoch: [37][3/204]	Loss 0.6201 (0.5736)	
training:	Epoch: [37][4/204]	Loss 0.4201 (0.5352)	
training:	Epoch: [37][5/204]	Loss 0.4813 (0.5245)	
training:	Epoch: [37][6/204]	Loss 0.6319 (0.5424)	
training:	Epoch: [37][7/204]	Loss 0.5341 (0.5412)	
training:	Epoch: [37][8/204]	Loss 0.6257 (0.5518)	
training:	Epoch: [37][9/204]	Loss 0.6376 (0.5613)	
training:	Epoch: [37][10/204]	Loss 0.5367 (0.5588)	
training:	Epoch: [37][11/204]	Loss 0.5926 (0.5619)	
training:	Epoch: [37][12/204]	Loss 0.5453 (0.5605)	
training:	Epoch: [37][13/204]	Loss 0.5775 (0.5618)	
training:	Epoch: [37][14/204]	Loss 0.5458 (0.5607)	
training:	Epoch: [37][15/204]	Loss 0.6124 (0.5641)	
training:	Epoch: [37][16/204]	Loss 0.5041 (0.5604)	
training:	Epoch: [37][17/204]	Loss 0.6355 (0.5648)	
training:	Epoch: [37][18/204]	Loss 0.6231 (0.5680)	
training:	Epoch: [37][19/204]	Loss 0.6530 (0.5725)	
training:	Epoch: [37][20/204]	Loss 0.4813 (0.5679)	
training:	Epoch: [37][21/204]	Loss 0.4848 (0.5640)	
training:	Epoch: [37][22/204]	Loss 0.4791 (0.5601)	
training:	Epoch: [37][23/204]	Loss 0.6028 (0.5620)	
training:	Epoch: [37][24/204]	Loss 0.6059 (0.5638)	
training:	Epoch: [37][25/204]	Loss 0.4857 (0.5607)	
training:	Epoch: [37][26/204]	Loss 0.5303 (0.5595)	
training:	Epoch: [37][27/204]	Loss 0.4842 (0.5567)	
training:	Epoch: [37][28/204]	Loss 0.5775 (0.5575)	
training:	Epoch: [37][29/204]	Loss 0.5673 (0.5578)	
training:	Epoch: [37][30/204]	Loss 0.5870 (0.5588)	
training:	Epoch: [37][31/204]	Loss 0.5776 (0.5594)	
training:	Epoch: [37][32/204]	Loss 0.4750 (0.5568)	
training:	Epoch: [37][33/204]	Loss 0.5158 (0.5555)	
training:	Epoch: [37][34/204]	Loss 0.5284 (0.5547)	
training:	Epoch: [37][35/204]	Loss 0.5754 (0.5553)	
training:	Epoch: [37][36/204]	Loss 0.5268 (0.5545)	
training:	Epoch: [37][37/204]	Loss 0.5929 (0.5555)	
training:	Epoch: [37][38/204]	Loss 0.6445 (0.5579)	
training:	Epoch: [37][39/204]	Loss 0.5129 (0.5567)	
training:	Epoch: [37][40/204]	Loss 0.4880 (0.5550)	
training:	Epoch: [37][41/204]	Loss 0.4778 (0.5531)	
training:	Epoch: [37][42/204]	Loss 0.5467 (0.5530)	
training:	Epoch: [37][43/204]	Loss 0.4387 (0.5503)	
training:	Epoch: [37][44/204]	Loss 0.5528 (0.5504)	
training:	Epoch: [37][45/204]	Loss 0.5567 (0.5505)	
training:	Epoch: [37][46/204]	Loss 0.3896 (0.5470)	
training:	Epoch: [37][47/204]	Loss 0.6275 (0.5487)	
training:	Epoch: [37][48/204]	Loss 0.4408 (0.5465)	
training:	Epoch: [37][49/204]	Loss 0.6873 (0.5494)	
training:	Epoch: [37][50/204]	Loss 0.4991 (0.5484)	
training:	Epoch: [37][51/204]	Loss 0.4852 (0.5471)	
training:	Epoch: [37][52/204]	Loss 0.4989 (0.5462)	
training:	Epoch: [37][53/204]	Loss 0.6137 (0.5475)	
training:	Epoch: [37][54/204]	Loss 0.5614 (0.5477)	
training:	Epoch: [37][55/204]	Loss 0.5271 (0.5473)	
training:	Epoch: [37][56/204]	Loss 0.5210 (0.5469)	
training:	Epoch: [37][57/204]	Loss 0.4609 (0.5454)	
training:	Epoch: [37][58/204]	Loss 0.5538 (0.5455)	
training:	Epoch: [37][59/204]	Loss 0.5803 (0.5461)	
training:	Epoch: [37][60/204]	Loss 0.5387 (0.5460)	
training:	Epoch: [37][61/204]	Loss 0.5205 (0.5456)	
training:	Epoch: [37][62/204]	Loss 0.5975 (0.5464)	
training:	Epoch: [37][63/204]	Loss 0.4217 (0.5444)	
training:	Epoch: [37][64/204]	Loss 0.5814 (0.5450)	
training:	Epoch: [37][65/204]	Loss 0.6161 (0.5461)	
training:	Epoch: [37][66/204]	Loss 0.5283 (0.5458)	
training:	Epoch: [37][67/204]	Loss 0.4764 (0.5448)	
training:	Epoch: [37][68/204]	Loss 0.5290 (0.5446)	
training:	Epoch: [37][69/204]	Loss 0.4499 (0.5432)	
training:	Epoch: [37][70/204]	Loss 0.4249 (0.5415)	
training:	Epoch: [37][71/204]	Loss 0.7076 (0.5438)	
training:	Epoch: [37][72/204]	Loss 0.4707 (0.5428)	
training:	Epoch: [37][73/204]	Loss 0.5313 (0.5427)	
training:	Epoch: [37][74/204]	Loss 0.5065 (0.5422)	
training:	Epoch: [37][75/204]	Loss 0.4638 (0.5411)	
training:	Epoch: [37][76/204]	Loss 0.5153 (0.5408)	
training:	Epoch: [37][77/204]	Loss 0.6027 (0.5416)	
training:	Epoch: [37][78/204]	Loss 0.4736 (0.5407)	
training:	Epoch: [37][79/204]	Loss 0.5262 (0.5405)	
training:	Epoch: [37][80/204]	Loss 0.6484 (0.5419)	
training:	Epoch: [37][81/204]	Loss 0.7130 (0.5440)	
training:	Epoch: [37][82/204]	Loss 0.6080 (0.5448)	
training:	Epoch: [37][83/204]	Loss 0.5773 (0.5452)	
training:	Epoch: [37][84/204]	Loss 0.4763 (0.5443)	
training:	Epoch: [37][85/204]	Loss 0.6593 (0.5457)	
training:	Epoch: [37][86/204]	Loss 0.4743 (0.5449)	
training:	Epoch: [37][87/204]	Loss 0.6166 (0.5457)	
training:	Epoch: [37][88/204]	Loss 0.5873 (0.5462)	
training:	Epoch: [37][89/204]	Loss 0.5595 (0.5463)	
training:	Epoch: [37][90/204]	Loss 0.6791 (0.5478)	
training:	Epoch: [37][91/204]	Loss 0.5405 (0.5477)	
training:	Epoch: [37][92/204]	Loss 0.5369 (0.5476)	
training:	Epoch: [37][93/204]	Loss 0.5220 (0.5473)	
training:	Epoch: [37][94/204]	Loss 0.5633 (0.5475)	
training:	Epoch: [37][95/204]	Loss 0.5547 (0.5476)	
training:	Epoch: [37][96/204]	Loss 0.5621 (0.5477)	
training:	Epoch: [37][97/204]	Loss 0.4834 (0.5470)	
training:	Epoch: [37][98/204]	Loss 0.5471 (0.5471)	
training:	Epoch: [37][99/204]	Loss 0.5706 (0.5473)	
training:	Epoch: [37][100/204]	Loss 0.4151 (0.5460)	
training:	Epoch: [37][101/204]	Loss 0.6072 (0.5466)	
training:	Epoch: [37][102/204]	Loss 0.3909 (0.5450)	
training:	Epoch: [37][103/204]	Loss 0.5085 (0.5447)	
training:	Epoch: [37][104/204]	Loss 0.5887 (0.5451)	
training:	Epoch: [37][105/204]	Loss 0.6135 (0.5458)	
training:	Epoch: [37][106/204]	Loss 0.4934 (0.5453)	
training:	Epoch: [37][107/204]	Loss 0.5370 (0.5452)	
training:	Epoch: [37][108/204]	Loss 0.5394 (0.5451)	
training:	Epoch: [37][109/204]	Loss 0.5465 (0.5452)	
training:	Epoch: [37][110/204]	Loss 0.6534 (0.5461)	
training:	Epoch: [37][111/204]	Loss 0.5775 (0.5464)	
training:	Epoch: [37][112/204]	Loss 0.6230 (0.5471)	
training:	Epoch: [37][113/204]	Loss 0.4373 (0.5461)	
training:	Epoch: [37][114/204]	Loss 0.4678 (0.5454)	
training:	Epoch: [37][115/204]	Loss 0.5566 (0.5455)	
training:	Epoch: [37][116/204]	Loss 0.4333 (0.5446)	
training:	Epoch: [37][117/204]	Loss 0.4545 (0.5438)	
training:	Epoch: [37][118/204]	Loss 0.4983 (0.5434)	
training:	Epoch: [37][119/204]	Loss 0.5079 (0.5431)	
training:	Epoch: [37][120/204]	Loss 0.5198 (0.5429)	
training:	Epoch: [37][121/204]	Loss 0.6183 (0.5436)	
training:	Epoch: [37][122/204]	Loss 0.5230 (0.5434)	
training:	Epoch: [37][123/204]	Loss 0.5166 (0.5432)	
training:	Epoch: [37][124/204]	Loss 0.3908 (0.5419)	
training:	Epoch: [37][125/204]	Loss 0.6039 (0.5424)	
training:	Epoch: [37][126/204]	Loss 0.5247 (0.5423)	
training:	Epoch: [37][127/204]	Loss 0.5697 (0.5425)	
training:	Epoch: [37][128/204]	Loss 0.5479 (0.5425)	
training:	Epoch: [37][129/204]	Loss 0.5066 (0.5423)	
training:	Epoch: [37][130/204]	Loss 0.5283 (0.5422)	
training:	Epoch: [37][131/204]	Loss 0.4778 (0.5417)	
training:	Epoch: [37][132/204]	Loss 0.5897 (0.5420)	
training:	Epoch: [37][133/204]	Loss 0.6366 (0.5427)	
training:	Epoch: [37][134/204]	Loss 0.4265 (0.5419)	
training:	Epoch: [37][135/204]	Loss 0.4742 (0.5414)	
training:	Epoch: [37][136/204]	Loss 0.6040 (0.5418)	
training:	Epoch: [37][137/204]	Loss 0.5577 (0.5420)	
training:	Epoch: [37][138/204]	Loss 0.6161 (0.5425)	
training:	Epoch: [37][139/204]	Loss 0.5586 (0.5426)	
training:	Epoch: [37][140/204]	Loss 0.5690 (0.5428)	
training:	Epoch: [37][141/204]	Loss 0.4805 (0.5424)	
training:	Epoch: [37][142/204]	Loss 0.6541 (0.5431)	
training:	Epoch: [37][143/204]	Loss 0.6512 (0.5439)	
training:	Epoch: [37][144/204]	Loss 0.6092 (0.5443)	
training:	Epoch: [37][145/204]	Loss 0.5202 (0.5442)	
training:	Epoch: [37][146/204]	Loss 0.6022 (0.5446)	
training:	Epoch: [37][147/204]	Loss 0.5204 (0.5444)	
training:	Epoch: [37][148/204]	Loss 0.6706 (0.5453)	
training:	Epoch: [37][149/204]	Loss 0.4510 (0.5446)	
training:	Epoch: [37][150/204]	Loss 0.6726 (0.5455)	
training:	Epoch: [37][151/204]	Loss 0.5047 (0.5452)	
training:	Epoch: [37][152/204]	Loss 0.4455 (0.5446)	
training:	Epoch: [37][153/204]	Loss 0.5548 (0.5446)	
training:	Epoch: [37][154/204]	Loss 0.5716 (0.5448)	
training:	Epoch: [37][155/204]	Loss 0.5244 (0.5447)	
training:	Epoch: [37][156/204]	Loss 0.6460 (0.5453)	
training:	Epoch: [37][157/204]	Loss 0.5516 (0.5454)	
training:	Epoch: [37][158/204]	Loss 0.5366 (0.5453)	
training:	Epoch: [37][159/204]	Loss 0.6002 (0.5457)	
training:	Epoch: [37][160/204]	Loss 0.5315 (0.5456)	
training:	Epoch: [37][161/204]	Loss 0.4113 (0.5447)	
training:	Epoch: [37][162/204]	Loss 0.5130 (0.5445)	
training:	Epoch: [37][163/204]	Loss 0.5128 (0.5443)	
training:	Epoch: [37][164/204]	Loss 0.6108 (0.5447)	
training:	Epoch: [37][165/204]	Loss 0.6900 (0.5456)	
training:	Epoch: [37][166/204]	Loss 0.5042 (0.5454)	
training:	Epoch: [37][167/204]	Loss 0.5434 (0.5454)	
training:	Epoch: [37][168/204]	Loss 0.5484 (0.5454)	
training:	Epoch: [37][169/204]	Loss 0.5603 (0.5455)	
training:	Epoch: [37][170/204]	Loss 0.5776 (0.5457)	
training:	Epoch: [37][171/204]	Loss 0.4814 (0.5453)	
training:	Epoch: [37][172/204]	Loss 0.4939 (0.5450)	
training:	Epoch: [37][173/204]	Loss 0.4884 (0.5447)	
training:	Epoch: [37][174/204]	Loss 0.5655 (0.5448)	
training:	Epoch: [37][175/204]	Loss 0.4029 (0.5440)	
training:	Epoch: [37][176/204]	Loss 0.6887 (0.5448)	
training:	Epoch: [37][177/204]	Loss 0.6432 (0.5453)	
training:	Epoch: [37][178/204]	Loss 0.5871 (0.5456)	
training:	Epoch: [37][179/204]	Loss 0.5607 (0.5457)	
training:	Epoch: [37][180/204]	Loss 0.6212 (0.5461)	
training:	Epoch: [37][181/204]	Loss 0.5303 (0.5460)	
training:	Epoch: [37][182/204]	Loss 0.5577 (0.5461)	
training:	Epoch: [37][183/204]	Loss 0.4174 (0.5454)	
training:	Epoch: [37][184/204]	Loss 0.5805 (0.5455)	
training:	Epoch: [37][185/204]	Loss 0.5824 (0.5457)	
training:	Epoch: [37][186/204]	Loss 0.5045 (0.5455)	
training:	Epoch: [37][187/204]	Loss 0.5173 (0.5454)	
training:	Epoch: [37][188/204]	Loss 0.5694 (0.5455)	
training:	Epoch: [37][189/204]	Loss 0.5852 (0.5457)	
training:	Epoch: [37][190/204]	Loss 0.4159 (0.5450)	
training:	Epoch: [37][191/204]	Loss 0.5432 (0.5450)	
training:	Epoch: [37][192/204]	Loss 0.4390 (0.5445)	
training:	Epoch: [37][193/204]	Loss 0.4504 (0.5440)	
training:	Epoch: [37][194/204]	Loss 0.6262 (0.5444)	
training:	Epoch: [37][195/204]	Loss 0.4992 (0.5442)	
training:	Epoch: [37][196/204]	Loss 0.6047 (0.5445)	
training:	Epoch: [37][197/204]	Loss 0.4102 (0.5438)	
training:	Epoch: [37][198/204]	Loss 0.5291 (0.5437)	
training:	Epoch: [37][199/204]	Loss 0.5807 (0.5439)	
training:	Epoch: [37][200/204]	Loss 0.6114 (0.5442)	
training:	Epoch: [37][201/204]	Loss 0.4622 (0.5438)	
training:	Epoch: [37][202/204]	Loss 0.4595 (0.5434)	
training:	Epoch: [37][203/204]	Loss 0.4871 (0.5431)	
training:	Epoch: [37][204/204]	Loss 0.6032 (0.5434)	
Training:	 Loss: 0.5426

Training:	 ACC: 0.7422 0.7430 0.7619 0.7226
Validation:	 ACC: 0.7441 0.7453 0.7718 0.7164
Validation:	 Best_BACC: 0.7441 0.7453 0.7718 0.7164
Validation:	 Loss: 0.5357
Pretraining:	Epoch 38/120
----------
training:	Epoch: [38][1/204]	Loss 0.5661 (0.5661)	
training:	Epoch: [38][2/204]	Loss 0.5489 (0.5575)	
training:	Epoch: [38][3/204]	Loss 0.5512 (0.5554)	
training:	Epoch: [38][4/204]	Loss 0.5730 (0.5598)	
training:	Epoch: [38][5/204]	Loss 0.5964 (0.5671)	
training:	Epoch: [38][6/204]	Loss 0.5346 (0.5617)	
training:	Epoch: [38][7/204]	Loss 0.4781 (0.5497)	
training:	Epoch: [38][8/204]	Loss 0.4963 (0.5431)	
training:	Epoch: [38][9/204]	Loss 0.6299 (0.5527)	
training:	Epoch: [38][10/204]	Loss 0.5134 (0.5488)	
training:	Epoch: [38][11/204]	Loss 0.5048 (0.5448)	
training:	Epoch: [38][12/204]	Loss 0.6376 (0.5525)	
training:	Epoch: [38][13/204]	Loss 0.5261 (0.5505)	
training:	Epoch: [38][14/204]	Loss 0.6904 (0.5605)	
training:	Epoch: [38][15/204]	Loss 0.5142 (0.5574)	
training:	Epoch: [38][16/204]	Loss 0.5231 (0.5553)	
training:	Epoch: [38][17/204]	Loss 0.5056 (0.5523)	
training:	Epoch: [38][18/204]	Loss 0.5075 (0.5498)	
training:	Epoch: [38][19/204]	Loss 0.5823 (0.5516)	
training:	Epoch: [38][20/204]	Loss 0.6462 (0.5563)	
training:	Epoch: [38][21/204]	Loss 0.5848 (0.5576)	
training:	Epoch: [38][22/204]	Loss 0.5208 (0.5560)	
training:	Epoch: [38][23/204]	Loss 0.6370 (0.5595)	
training:	Epoch: [38][24/204]	Loss 0.5413 (0.5587)	
training:	Epoch: [38][25/204]	Loss 0.6355 (0.5618)	
training:	Epoch: [38][26/204]	Loss 0.5223 (0.5603)	
training:	Epoch: [38][27/204]	Loss 0.5085 (0.5584)	
training:	Epoch: [38][28/204]	Loss 0.5553 (0.5583)	
training:	Epoch: [38][29/204]	Loss 0.5802 (0.5590)	
training:	Epoch: [38][30/204]	Loss 0.5312 (0.5581)	
training:	Epoch: [38][31/204]	Loss 0.4347 (0.5541)	
training:	Epoch: [38][32/204]	Loss 0.4693 (0.5515)	
training:	Epoch: [38][33/204]	Loss 0.4464 (0.5483)	
training:	Epoch: [38][34/204]	Loss 0.5602 (0.5486)	
training:	Epoch: [38][35/204]	Loss 0.4828 (0.5467)	
training:	Epoch: [38][36/204]	Loss 0.5639 (0.5472)	
training:	Epoch: [38][37/204]	Loss 0.4661 (0.5450)	
training:	Epoch: [38][38/204]	Loss 0.5628 (0.5455)	
training:	Epoch: [38][39/204]	Loss 0.4498 (0.5430)	
training:	Epoch: [38][40/204]	Loss 0.4645 (0.5411)	
training:	Epoch: [38][41/204]	Loss 0.6660 (0.5441)	
training:	Epoch: [38][42/204]	Loss 0.4081 (0.5409)	
training:	Epoch: [38][43/204]	Loss 0.5253 (0.5405)	
training:	Epoch: [38][44/204]	Loss 0.5123 (0.5399)	
training:	Epoch: [38][45/204]	Loss 0.5401 (0.5399)	
training:	Epoch: [38][46/204]	Loss 0.5465 (0.5400)	
training:	Epoch: [38][47/204]	Loss 0.5256 (0.5397)	
training:	Epoch: [38][48/204]	Loss 0.5206 (0.5393)	
training:	Epoch: [38][49/204]	Loss 0.4674 (0.5379)	
training:	Epoch: [38][50/204]	Loss 0.5589 (0.5383)	
training:	Epoch: [38][51/204]	Loss 0.5265 (0.5380)	
training:	Epoch: [38][52/204]	Loss 0.6111 (0.5395)	
training:	Epoch: [38][53/204]	Loss 0.5651 (0.5399)	
training:	Epoch: [38][54/204]	Loss 0.5628 (0.5404)	
training:	Epoch: [38][55/204]	Loss 0.5631 (0.5408)	
training:	Epoch: [38][56/204]	Loss 0.5614 (0.5411)	
training:	Epoch: [38][57/204]	Loss 0.7344 (0.5445)	
training:	Epoch: [38][58/204]	Loss 0.4670 (0.5432)	
training:	Epoch: [38][59/204]	Loss 0.6734 (0.5454)	
training:	Epoch: [38][60/204]	Loss 0.4686 (0.5441)	
training:	Epoch: [38][61/204]	Loss 0.5935 (0.5449)	
training:	Epoch: [38][62/204]	Loss 0.5167 (0.5445)	
training:	Epoch: [38][63/204]	Loss 0.5000 (0.5438)	
training:	Epoch: [38][64/204]	Loss 0.4490 (0.5423)	
training:	Epoch: [38][65/204]	Loss 0.5531 (0.5425)	
training:	Epoch: [38][66/204]	Loss 0.5508 (0.5426)	
training:	Epoch: [38][67/204]	Loss 0.4345 (0.5410)	
training:	Epoch: [38][68/204]	Loss 0.4245 (0.5393)	
training:	Epoch: [38][69/204]	Loss 0.5108 (0.5388)	
training:	Epoch: [38][70/204]	Loss 0.4402 (0.5374)	
training:	Epoch: [38][71/204]	Loss 0.5466 (0.5376)	
training:	Epoch: [38][72/204]	Loss 0.7652 (0.5407)	
training:	Epoch: [38][73/204]	Loss 0.4624 (0.5397)	
training:	Epoch: [38][74/204]	Loss 0.4737 (0.5388)	
training:	Epoch: [38][75/204]	Loss 0.6079 (0.5397)	
training:	Epoch: [38][76/204]	Loss 0.4872 (0.5390)	
training:	Epoch: [38][77/204]	Loss 0.6323 (0.5402)	
training:	Epoch: [38][78/204]	Loss 0.4767 (0.5394)	
training:	Epoch: [38][79/204]	Loss 0.5058 (0.5390)	
training:	Epoch: [38][80/204]	Loss 0.3783 (0.5370)	
training:	Epoch: [38][81/204]	Loss 0.5111 (0.5366)	
training:	Epoch: [38][82/204]	Loss 0.6513 (0.5380)	
training:	Epoch: [38][83/204]	Loss 0.5427 (0.5381)	
training:	Epoch: [38][84/204]	Loss 0.4400 (0.5369)	
training:	Epoch: [38][85/204]	Loss 0.6081 (0.5378)	
training:	Epoch: [38][86/204]	Loss 0.5661 (0.5381)	
training:	Epoch: [38][87/204]	Loss 0.5181 (0.5379)	
training:	Epoch: [38][88/204]	Loss 0.6134 (0.5387)	
training:	Epoch: [38][89/204]	Loss 0.4904 (0.5382)	
training:	Epoch: [38][90/204]	Loss 0.4953 (0.5377)	
training:	Epoch: [38][91/204]	Loss 0.6143 (0.5385)	
training:	Epoch: [38][92/204]	Loss 0.5497 (0.5387)	
training:	Epoch: [38][93/204]	Loss 0.5826 (0.5391)	
training:	Epoch: [38][94/204]	Loss 0.5230 (0.5390)	
training:	Epoch: [38][95/204]	Loss 0.6352 (0.5400)	
training:	Epoch: [38][96/204]	Loss 0.5103 (0.5397)	
training:	Epoch: [38][97/204]	Loss 0.5789 (0.5401)	
training:	Epoch: [38][98/204]	Loss 0.6711 (0.5414)	
training:	Epoch: [38][99/204]	Loss 0.5280 (0.5413)	
training:	Epoch: [38][100/204]	Loss 0.4642 (0.5405)	
training:	Epoch: [38][101/204]	Loss 0.4433 (0.5395)	
training:	Epoch: [38][102/204]	Loss 0.5642 (0.5398)	
training:	Epoch: [38][103/204]	Loss 0.5916 (0.5403)	
training:	Epoch: [38][104/204]	Loss 0.5976 (0.5408)	
training:	Epoch: [38][105/204]	Loss 0.6157 (0.5415)	
training:	Epoch: [38][106/204]	Loss 0.6597 (0.5427)	
training:	Epoch: [38][107/204]	Loss 0.5509 (0.5427)	
training:	Epoch: [38][108/204]	Loss 0.5379 (0.5427)	
training:	Epoch: [38][109/204]	Loss 0.6025 (0.5432)	
training:	Epoch: [38][110/204]	Loss 0.5855 (0.5436)	
training:	Epoch: [38][111/204]	Loss 0.6015 (0.5441)	
training:	Epoch: [38][112/204]	Loss 0.5137 (0.5439)	
training:	Epoch: [38][113/204]	Loss 0.5254 (0.5437)	
training:	Epoch: [38][114/204]	Loss 0.7010 (0.5451)	
training:	Epoch: [38][115/204]	Loss 0.6962 (0.5464)	
training:	Epoch: [38][116/204]	Loss 0.5658 (0.5466)	
training:	Epoch: [38][117/204]	Loss 0.5989 (0.5470)	
training:	Epoch: [38][118/204]	Loss 0.5046 (0.5467)	
training:	Epoch: [38][119/204]	Loss 0.4735 (0.5460)	
training:	Epoch: [38][120/204]	Loss 0.5940 (0.5464)	
training:	Epoch: [38][121/204]	Loss 0.5103 (0.5461)	
training:	Epoch: [38][122/204]	Loss 0.4401 (0.5453)	
training:	Epoch: [38][123/204]	Loss 0.5436 (0.5453)	
training:	Epoch: [38][124/204]	Loss 0.5043 (0.5449)	
training:	Epoch: [38][125/204]	Loss 0.4796 (0.5444)	
training:	Epoch: [38][126/204]	Loss 0.5016 (0.5441)	
training:	Epoch: [38][127/204]	Loss 0.5600 (0.5442)	
training:	Epoch: [38][128/204]	Loss 0.5775 (0.5445)	
training:	Epoch: [38][129/204]	Loss 0.4735 (0.5439)	
training:	Epoch: [38][130/204]	Loss 0.4356 (0.5431)	
training:	Epoch: [38][131/204]	Loss 0.5846 (0.5434)	
training:	Epoch: [38][132/204]	Loss 0.5441 (0.5434)	
training:	Epoch: [38][133/204]	Loss 0.6508 (0.5442)	
training:	Epoch: [38][134/204]	Loss 0.5470 (0.5442)	
training:	Epoch: [38][135/204]	Loss 0.5727 (0.5444)	
training:	Epoch: [38][136/204]	Loss 0.5127 (0.5442)	
training:	Epoch: [38][137/204]	Loss 0.5115 (0.5440)	
training:	Epoch: [38][138/204]	Loss 0.4576 (0.5433)	
training:	Epoch: [38][139/204]	Loss 0.5646 (0.5435)	
training:	Epoch: [38][140/204]	Loss 0.4025 (0.5425)	
training:	Epoch: [38][141/204]	Loss 0.5061 (0.5422)	
training:	Epoch: [38][142/204]	Loss 0.4961 (0.5419)	
training:	Epoch: [38][143/204]	Loss 0.5626 (0.5420)	
training:	Epoch: [38][144/204]	Loss 0.3721 (0.5409)	
training:	Epoch: [38][145/204]	Loss 0.5941 (0.5412)	
training:	Epoch: [38][146/204]	Loss 0.4824 (0.5408)	
training:	Epoch: [38][147/204]	Loss 0.4224 (0.5400)	
training:	Epoch: [38][148/204]	Loss 0.5250 (0.5399)	
training:	Epoch: [38][149/204]	Loss 0.5901 (0.5403)	
training:	Epoch: [38][150/204]	Loss 0.6287 (0.5408)	
training:	Epoch: [38][151/204]	Loss 0.6095 (0.5413)	
training:	Epoch: [38][152/204]	Loss 0.5852 (0.5416)	
training:	Epoch: [38][153/204]	Loss 0.7782 (0.5431)	
training:	Epoch: [38][154/204]	Loss 0.4879 (0.5428)	
training:	Epoch: [38][155/204]	Loss 0.5355 (0.5427)	
training:	Epoch: [38][156/204]	Loss 0.4475 (0.5421)	
training:	Epoch: [38][157/204]	Loss 0.5760 (0.5423)	
training:	Epoch: [38][158/204]	Loss 0.6126 (0.5428)	
training:	Epoch: [38][159/204]	Loss 0.4136 (0.5420)	
training:	Epoch: [38][160/204]	Loss 0.5313 (0.5419)	
training:	Epoch: [38][161/204]	Loss 0.4131 (0.5411)	
training:	Epoch: [38][162/204]	Loss 0.4228 (0.5404)	
training:	Epoch: [38][163/204]	Loss 0.5146 (0.5402)	
training:	Epoch: [38][164/204]	Loss 0.5106 (0.5400)	
training:	Epoch: [38][165/204]	Loss 0.5566 (0.5401)	
training:	Epoch: [38][166/204]	Loss 0.6549 (0.5408)	
training:	Epoch: [38][167/204]	Loss 0.6677 (0.5416)	
training:	Epoch: [38][168/204]	Loss 0.5465 (0.5416)	
training:	Epoch: [38][169/204]	Loss 0.5015 (0.5414)	
training:	Epoch: [38][170/204]	Loss 0.5650 (0.5415)	
training:	Epoch: [38][171/204]	Loss 0.5600 (0.5416)	
training:	Epoch: [38][172/204]	Loss 0.5661 (0.5418)	
training:	Epoch: [38][173/204]	Loss 0.5359 (0.5417)	
training:	Epoch: [38][174/204]	Loss 0.4886 (0.5414)	
training:	Epoch: [38][175/204]	Loss 0.6455 (0.5420)	
training:	Epoch: [38][176/204]	Loss 0.5386 (0.5420)	
training:	Epoch: [38][177/204]	Loss 0.4303 (0.5414)	
training:	Epoch: [38][178/204]	Loss 0.5251 (0.5413)	
training:	Epoch: [38][179/204]	Loss 0.5434 (0.5413)	
training:	Epoch: [38][180/204]	Loss 0.6722 (0.5420)	
training:	Epoch: [38][181/204]	Loss 0.5431 (0.5420)	
training:	Epoch: [38][182/204]	Loss 0.5753 (0.5422)	
training:	Epoch: [38][183/204]	Loss 0.6334 (0.5427)	
training:	Epoch: [38][184/204]	Loss 0.5371 (0.5427)	
training:	Epoch: [38][185/204]	Loss 0.4983 (0.5424)	
training:	Epoch: [38][186/204]	Loss 0.5327 (0.5424)	
training:	Epoch: [38][187/204]	Loss 0.5204 (0.5423)	
training:	Epoch: [38][188/204]	Loss 0.4879 (0.5420)	
training:	Epoch: [38][189/204]	Loss 0.5563 (0.5421)	
training:	Epoch: [38][190/204]	Loss 0.4461 (0.5415)	
training:	Epoch: [38][191/204]	Loss 0.6109 (0.5419)	
training:	Epoch: [38][192/204]	Loss 0.5105 (0.5417)	
training:	Epoch: [38][193/204]	Loss 0.5401 (0.5417)	
training:	Epoch: [38][194/204]	Loss 0.4461 (0.5412)	
training:	Epoch: [38][195/204]	Loss 0.4333 (0.5407)	
training:	Epoch: [38][196/204]	Loss 0.5953 (0.5410)	
training:	Epoch: [38][197/204]	Loss 0.4805 (0.5407)	
training:	Epoch: [38][198/204]	Loss 0.4972 (0.5404)	
training:	Epoch: [38][199/204]	Loss 0.6720 (0.5411)	
training:	Epoch: [38][200/204]	Loss 0.4417 (0.5406)	
training:	Epoch: [38][201/204]	Loss 0.5329 (0.5406)	
training:	Epoch: [38][202/204]	Loss 0.5060 (0.5404)	
training:	Epoch: [38][203/204]	Loss 0.6613 (0.5410)	
training:	Epoch: [38][204/204]	Loss 0.4890 (0.5407)	
Training:	 Loss: 0.5399

Training:	 ACC: 0.7450 0.7456 0.7616 0.7283
Validation:	 ACC: 0.7458 0.7469 0.7707 0.7209
Validation:	 Best_BACC: 0.7458 0.7469 0.7707 0.7209
Validation:	 Loss: 0.5330
Pretraining:	Epoch 39/120
----------
training:	Epoch: [39][1/204]	Loss 0.6165 (0.6165)	
training:	Epoch: [39][2/204]	Loss 0.5391 (0.5778)	
training:	Epoch: [39][3/204]	Loss 0.4531 (0.5362)	
training:	Epoch: [39][4/204]	Loss 0.4345 (0.5108)	
training:	Epoch: [39][5/204]	Loss 0.5023 (0.5091)	
training:	Epoch: [39][6/204]	Loss 0.6238 (0.5282)	
training:	Epoch: [39][7/204]	Loss 0.5010 (0.5243)	
training:	Epoch: [39][8/204]	Loss 0.5475 (0.5272)	
training:	Epoch: [39][9/204]	Loss 0.4900 (0.5231)	
training:	Epoch: [39][10/204]	Loss 0.4864 (0.5194)	
training:	Epoch: [39][11/204]	Loss 0.4371 (0.5119)	
training:	Epoch: [39][12/204]	Loss 0.5807 (0.5177)	
training:	Epoch: [39][13/204]	Loss 0.5572 (0.5207)	
training:	Epoch: [39][14/204]	Loss 0.4722 (0.5172)	
training:	Epoch: [39][15/204]	Loss 0.5558 (0.5198)	
training:	Epoch: [39][16/204]	Loss 0.6092 (0.5254)	
training:	Epoch: [39][17/204]	Loss 0.5366 (0.5261)	
training:	Epoch: [39][18/204]	Loss 0.6527 (0.5331)	
training:	Epoch: [39][19/204]	Loss 0.5980 (0.5365)	
training:	Epoch: [39][20/204]	Loss 0.4724 (0.5333)	
training:	Epoch: [39][21/204]	Loss 0.5621 (0.5347)	
training:	Epoch: [39][22/204]	Loss 0.4460 (0.5307)	
training:	Epoch: [39][23/204]	Loss 0.5069 (0.5296)	
training:	Epoch: [39][24/204]	Loss 0.5143 (0.5290)	
training:	Epoch: [39][25/204]	Loss 0.5698 (0.5306)	
training:	Epoch: [39][26/204]	Loss 0.5807 (0.5325)	
training:	Epoch: [39][27/204]	Loss 0.4918 (0.5310)	
training:	Epoch: [39][28/204]	Loss 0.5544 (0.5319)	
training:	Epoch: [39][29/204]	Loss 0.4490 (0.5290)	
training:	Epoch: [39][30/204]	Loss 0.4138 (0.5252)	
training:	Epoch: [39][31/204]	Loss 0.6554 (0.5294)	
training:	Epoch: [39][32/204]	Loss 0.6235 (0.5323)	
training:	Epoch: [39][33/204]	Loss 0.5184 (0.5319)	
training:	Epoch: [39][34/204]	Loss 0.4231 (0.5287)	
training:	Epoch: [39][35/204]	Loss 0.5269 (0.5286)	
training:	Epoch: [39][36/204]	Loss 0.4795 (0.5273)	
training:	Epoch: [39][37/204]	Loss 0.4851 (0.5261)	
training:	Epoch: [39][38/204]	Loss 0.4284 (0.5236)	
training:	Epoch: [39][39/204]	Loss 0.5708 (0.5248)	
training:	Epoch: [39][40/204]	Loss 0.4470 (0.5228)	
training:	Epoch: [39][41/204]	Loss 0.5500 (0.5235)	
training:	Epoch: [39][42/204]	Loss 0.4738 (0.5223)	
training:	Epoch: [39][43/204]	Loss 0.5061 (0.5219)	
training:	Epoch: [39][44/204]	Loss 0.5543 (0.5227)	
training:	Epoch: [39][45/204]	Loss 0.5461 (0.5232)	
training:	Epoch: [39][46/204]	Loss 0.5751 (0.5243)	
training:	Epoch: [39][47/204]	Loss 0.5032 (0.5239)	
training:	Epoch: [39][48/204]	Loss 0.6443 (0.5264)	
training:	Epoch: [39][49/204]	Loss 0.4987 (0.5258)	
training:	Epoch: [39][50/204]	Loss 0.6416 (0.5281)	
training:	Epoch: [39][51/204]	Loss 0.4496 (0.5266)	
training:	Epoch: [39][52/204]	Loss 0.4998 (0.5261)	
training:	Epoch: [39][53/204]	Loss 0.4484 (0.5246)	
training:	Epoch: [39][54/204]	Loss 0.5013 (0.5242)	
training:	Epoch: [39][55/204]	Loss 0.5942 (0.5254)	
training:	Epoch: [39][56/204]	Loss 0.5284 (0.5255)	
training:	Epoch: [39][57/204]	Loss 0.5259 (0.5255)	
training:	Epoch: [39][58/204]	Loss 0.6151 (0.5271)	
training:	Epoch: [39][59/204]	Loss 0.5805 (0.5280)	
training:	Epoch: [39][60/204]	Loss 0.6059 (0.5293)	
training:	Epoch: [39][61/204]	Loss 0.6522 (0.5313)	
training:	Epoch: [39][62/204]	Loss 0.5278 (0.5312)	
training:	Epoch: [39][63/204]	Loss 0.5516 (0.5315)	
training:	Epoch: [39][64/204]	Loss 0.4849 (0.5308)	
training:	Epoch: [39][65/204]	Loss 0.5518 (0.5311)	
training:	Epoch: [39][66/204]	Loss 0.5621 (0.5316)	
training:	Epoch: [39][67/204]	Loss 0.5099 (0.5313)	
training:	Epoch: [39][68/204]	Loss 0.5598 (0.5317)	
training:	Epoch: [39][69/204]	Loss 0.5991 (0.5327)	
training:	Epoch: [39][70/204]	Loss 0.5242 (0.5326)	
training:	Epoch: [39][71/204]	Loss 0.5320 (0.5325)	
training:	Epoch: [39][72/204]	Loss 0.4977 (0.5321)	
training:	Epoch: [39][73/204]	Loss 0.5234 (0.5319)	
training:	Epoch: [39][74/204]	Loss 0.5504 (0.5322)	
training:	Epoch: [39][75/204]	Loss 0.4175 (0.5307)	
training:	Epoch: [39][76/204]	Loss 0.5998 (0.5316)	
training:	Epoch: [39][77/204]	Loss 0.4199 (0.5301)	
training:	Epoch: [39][78/204]	Loss 0.5300 (0.5301)	
training:	Epoch: [39][79/204]	Loss 0.4997 (0.5297)	
training:	Epoch: [39][80/204]	Loss 0.5164 (0.5296)	
training:	Epoch: [39][81/204]	Loss 0.7299 (0.5320)	
training:	Epoch: [39][82/204]	Loss 0.5413 (0.5322)	
training:	Epoch: [39][83/204]	Loss 0.5086 (0.5319)	
training:	Epoch: [39][84/204]	Loss 0.5899 (0.5326)	
training:	Epoch: [39][85/204]	Loss 0.5946 (0.5333)	
training:	Epoch: [39][86/204]	Loss 0.4188 (0.5320)	
training:	Epoch: [39][87/204]	Loss 0.6039 (0.5328)	
training:	Epoch: [39][88/204]	Loss 0.5743 (0.5333)	
training:	Epoch: [39][89/204]	Loss 0.5831 (0.5338)	
training:	Epoch: [39][90/204]	Loss 0.7686 (0.5364)	
training:	Epoch: [39][91/204]	Loss 0.5376 (0.5364)	
training:	Epoch: [39][92/204]	Loss 0.4821 (0.5359)	
training:	Epoch: [39][93/204]	Loss 0.4834 (0.5353)	
training:	Epoch: [39][94/204]	Loss 0.5309 (0.5352)	
training:	Epoch: [39][95/204]	Loss 0.5723 (0.5356)	
training:	Epoch: [39][96/204]	Loss 0.5848 (0.5361)	
training:	Epoch: [39][97/204]	Loss 0.5568 (0.5364)	
training:	Epoch: [39][98/204]	Loss 0.6325 (0.5373)	
training:	Epoch: [39][99/204]	Loss 0.5272 (0.5372)	
training:	Epoch: [39][100/204]	Loss 0.4244 (0.5361)	
training:	Epoch: [39][101/204]	Loss 0.5179 (0.5359)	
training:	Epoch: [39][102/204]	Loss 0.5390 (0.5360)	
training:	Epoch: [39][103/204]	Loss 0.5882 (0.5365)	
training:	Epoch: [39][104/204]	Loss 0.5928 (0.5370)	
training:	Epoch: [39][105/204]	Loss 0.5171 (0.5368)	
training:	Epoch: [39][106/204]	Loss 0.5174 (0.5366)	
training:	Epoch: [39][107/204]	Loss 0.4433 (0.5358)	
training:	Epoch: [39][108/204]	Loss 0.4488 (0.5350)	
training:	Epoch: [39][109/204]	Loss 0.6027 (0.5356)	
training:	Epoch: [39][110/204]	Loss 0.4682 (0.5350)	
training:	Epoch: [39][111/204]	Loss 0.4659 (0.5343)	
training:	Epoch: [39][112/204]	Loss 0.5392 (0.5344)	
training:	Epoch: [39][113/204]	Loss 0.6211 (0.5352)	
training:	Epoch: [39][114/204]	Loss 0.5353 (0.5352)	
training:	Epoch: [39][115/204]	Loss 0.5287 (0.5351)	
training:	Epoch: [39][116/204]	Loss 0.5527 (0.5352)	
training:	Epoch: [39][117/204]	Loss 0.5105 (0.5350)	
training:	Epoch: [39][118/204]	Loss 0.4913 (0.5347)	
training:	Epoch: [39][119/204]	Loss 0.5951 (0.5352)	
training:	Epoch: [39][120/204]	Loss 0.4687 (0.5346)	
training:	Epoch: [39][121/204]	Loss 0.5031 (0.5344)	
training:	Epoch: [39][122/204]	Loss 0.5901 (0.5348)	
training:	Epoch: [39][123/204]	Loss 0.5167 (0.5347)	
training:	Epoch: [39][124/204]	Loss 0.5886 (0.5351)	
training:	Epoch: [39][125/204]	Loss 0.5269 (0.5350)	
training:	Epoch: [39][126/204]	Loss 0.5642 (0.5353)	
training:	Epoch: [39][127/204]	Loss 0.5399 (0.5353)	
training:	Epoch: [39][128/204]	Loss 0.6058 (0.5359)	
training:	Epoch: [39][129/204]	Loss 0.4745 (0.5354)	
training:	Epoch: [39][130/204]	Loss 0.4947 (0.5351)	
training:	Epoch: [39][131/204]	Loss 0.5329 (0.5351)	
training:	Epoch: [39][132/204]	Loss 0.5411 (0.5351)	
training:	Epoch: [39][133/204]	Loss 0.4946 (0.5348)	
training:	Epoch: [39][134/204]	Loss 0.5760 (0.5351)	
training:	Epoch: [39][135/204]	Loss 0.5876 (0.5355)	
training:	Epoch: [39][136/204]	Loss 0.5526 (0.5356)	
training:	Epoch: [39][137/204]	Loss 0.5400 (0.5356)	
training:	Epoch: [39][138/204]	Loss 0.4470 (0.5350)	
training:	Epoch: [39][139/204]	Loss 0.5256 (0.5349)	
training:	Epoch: [39][140/204]	Loss 0.4074 (0.5340)	
training:	Epoch: [39][141/204]	Loss 0.5880 (0.5344)	
training:	Epoch: [39][142/204]	Loss 0.6102 (0.5349)	
training:	Epoch: [39][143/204]	Loss 0.6109 (0.5355)	
training:	Epoch: [39][144/204]	Loss 0.6091 (0.5360)	
training:	Epoch: [39][145/204]	Loss 0.4726 (0.5355)	
training:	Epoch: [39][146/204]	Loss 0.5275 (0.5355)	
training:	Epoch: [39][147/204]	Loss 0.5970 (0.5359)	
training:	Epoch: [39][148/204]	Loss 0.5023 (0.5357)	
training:	Epoch: [39][149/204]	Loss 0.3845 (0.5347)	
training:	Epoch: [39][150/204]	Loss 0.4863 (0.5343)	
training:	Epoch: [39][151/204]	Loss 0.5208 (0.5343)	
training:	Epoch: [39][152/204]	Loss 0.4627 (0.5338)	
training:	Epoch: [39][153/204]	Loss 0.4379 (0.5332)	
training:	Epoch: [39][154/204]	Loss 0.5405 (0.5332)	
training:	Epoch: [39][155/204]	Loss 0.5631 (0.5334)	
training:	Epoch: [39][156/204]	Loss 0.5386 (0.5334)	
training:	Epoch: [39][157/204]	Loss 0.4594 (0.5330)	
training:	Epoch: [39][158/204]	Loss 0.4681 (0.5326)	
training:	Epoch: [39][159/204]	Loss 0.5693 (0.5328)	
training:	Epoch: [39][160/204]	Loss 0.5620 (0.5330)	
training:	Epoch: [39][161/204]	Loss 0.4358 (0.5324)	
training:	Epoch: [39][162/204]	Loss 0.5622 (0.5325)	
training:	Epoch: [39][163/204]	Loss 0.4303 (0.5319)	
training:	Epoch: [39][164/204]	Loss 0.5574 (0.5321)	
training:	Epoch: [39][165/204]	Loss 0.6485 (0.5328)	
training:	Epoch: [39][166/204]	Loss 0.5546 (0.5329)	
training:	Epoch: [39][167/204]	Loss 0.5335 (0.5329)	
training:	Epoch: [39][168/204]	Loss 0.5355 (0.5329)	
training:	Epoch: [39][169/204]	Loss 0.5660 (0.5331)	
training:	Epoch: [39][170/204]	Loss 0.4243 (0.5325)	
training:	Epoch: [39][171/204]	Loss 0.5691 (0.5327)	
training:	Epoch: [39][172/204]	Loss 0.5148 (0.5326)	
training:	Epoch: [39][173/204]	Loss 0.6671 (0.5334)	
training:	Epoch: [39][174/204]	Loss 0.5931 (0.5337)	
training:	Epoch: [39][175/204]	Loss 0.5558 (0.5338)	
training:	Epoch: [39][176/204]	Loss 0.4830 (0.5336)	
training:	Epoch: [39][177/204]	Loss 0.5786 (0.5338)	
training:	Epoch: [39][178/204]	Loss 0.4997 (0.5336)	
training:	Epoch: [39][179/204]	Loss 0.5292 (0.5336)	
training:	Epoch: [39][180/204]	Loss 0.5729 (0.5338)	
training:	Epoch: [39][181/204]	Loss 0.4634 (0.5334)	
training:	Epoch: [39][182/204]	Loss 0.5259 (0.5334)	
training:	Epoch: [39][183/204]	Loss 0.6190 (0.5338)	
training:	Epoch: [39][184/204]	Loss 0.6025 (0.5342)	
training:	Epoch: [39][185/204]	Loss 0.4842 (0.5340)	
training:	Epoch: [39][186/204]	Loss 0.6023 (0.5343)	
training:	Epoch: [39][187/204]	Loss 0.4104 (0.5337)	
training:	Epoch: [39][188/204]	Loss 0.7624 (0.5349)	
training:	Epoch: [39][189/204]	Loss 0.5387 (0.5349)	
training:	Epoch: [39][190/204]	Loss 0.5756 (0.5351)	
training:	Epoch: [39][191/204]	Loss 0.6464 (0.5357)	
training:	Epoch: [39][192/204]	Loss 0.5361 (0.5357)	
training:	Epoch: [39][193/204]	Loss 0.5538 (0.5358)	
training:	Epoch: [39][194/204]	Loss 0.5997 (0.5361)	
training:	Epoch: [39][195/204]	Loss 0.4753 (0.5358)	
training:	Epoch: [39][196/204]	Loss 0.6717 (0.5365)	
training:	Epoch: [39][197/204]	Loss 0.4809 (0.5362)	
training:	Epoch: [39][198/204]	Loss 0.5505 (0.5363)	
training:	Epoch: [39][199/204]	Loss 0.5984 (0.5366)	
training:	Epoch: [39][200/204]	Loss 0.5916 (0.5369)	
training:	Epoch: [39][201/204]	Loss 0.4647 (0.5365)	
training:	Epoch: [39][202/204]	Loss 0.5899 (0.5368)	
training:	Epoch: [39][203/204]	Loss 0.5474 (0.5368)	
training:	Epoch: [39][204/204]	Loss 0.5288 (0.5368)	
Training:	 Loss: 0.5360

Training:	 ACC: 0.7459 0.7469 0.7690 0.7229
Validation:	 ACC: 0.7445 0.7459 0.7748 0.7141
Validation:	 Best_BACC: 0.7458 0.7469 0.7707 0.7209
Validation:	 Loss: 0.5302
Pretraining:	Epoch 40/120
----------
training:	Epoch: [40][1/204]	Loss 0.5521 (0.5521)	
training:	Epoch: [40][2/204]	Loss 0.5705 (0.5613)	
training:	Epoch: [40][3/204]	Loss 0.4932 (0.5386)	
training:	Epoch: [40][4/204]	Loss 0.5416 (0.5394)	
training:	Epoch: [40][5/204]	Loss 0.6553 (0.5626)	
training:	Epoch: [40][6/204]	Loss 0.6330 (0.5743)	
training:	Epoch: [40][7/204]	Loss 0.5143 (0.5657)	
training:	Epoch: [40][8/204]	Loss 0.4613 (0.5527)	
training:	Epoch: [40][9/204]	Loss 0.5155 (0.5486)	
training:	Epoch: [40][10/204]	Loss 0.4671 (0.5404)	
training:	Epoch: [40][11/204]	Loss 0.5042 (0.5371)	
training:	Epoch: [40][12/204]	Loss 0.5082 (0.5347)	
training:	Epoch: [40][13/204]	Loss 0.6805 (0.5459)	
training:	Epoch: [40][14/204]	Loss 0.5024 (0.5428)	
training:	Epoch: [40][15/204]	Loss 0.4702 (0.5380)	
training:	Epoch: [40][16/204]	Loss 0.5510 (0.5388)	
training:	Epoch: [40][17/204]	Loss 0.6021 (0.5425)	
training:	Epoch: [40][18/204]	Loss 0.5140 (0.5409)	
training:	Epoch: [40][19/204]	Loss 0.6026 (0.5442)	
training:	Epoch: [40][20/204]	Loss 0.3995 (0.5369)	
training:	Epoch: [40][21/204]	Loss 0.5677 (0.5384)	
training:	Epoch: [40][22/204]	Loss 0.4388 (0.5339)	
training:	Epoch: [40][23/204]	Loss 0.5149 (0.5331)	
training:	Epoch: [40][24/204]	Loss 0.4765 (0.5307)	
training:	Epoch: [40][25/204]	Loss 0.5971 (0.5334)	
training:	Epoch: [40][26/204]	Loss 0.5402 (0.5336)	
training:	Epoch: [40][27/204]	Loss 0.4772 (0.5315)	
training:	Epoch: [40][28/204]	Loss 0.5247 (0.5313)	
training:	Epoch: [40][29/204]	Loss 0.6015 (0.5337)	
training:	Epoch: [40][30/204]	Loss 0.4825 (0.5320)	
training:	Epoch: [40][31/204]	Loss 0.5887 (0.5338)	
training:	Epoch: [40][32/204]	Loss 0.5373 (0.5339)	
training:	Epoch: [40][33/204]	Loss 0.5282 (0.5338)	
training:	Epoch: [40][34/204]	Loss 0.3978 (0.5298)	
training:	Epoch: [40][35/204]	Loss 0.5079 (0.5291)	
training:	Epoch: [40][36/204]	Loss 0.6257 (0.5318)	
training:	Epoch: [40][37/204]	Loss 0.4851 (0.5306)	
training:	Epoch: [40][38/204]	Loss 0.6600 (0.5340)	
training:	Epoch: [40][39/204]	Loss 0.5325 (0.5339)	
training:	Epoch: [40][40/204]	Loss 0.5527 (0.5344)	
training:	Epoch: [40][41/204]	Loss 0.4581 (0.5325)	
training:	Epoch: [40][42/204]	Loss 0.5772 (0.5336)	
training:	Epoch: [40][43/204]	Loss 0.7081 (0.5377)	
training:	Epoch: [40][44/204]	Loss 0.5689 (0.5384)	
training:	Epoch: [40][45/204]	Loss 0.5621 (0.5389)	
training:	Epoch: [40][46/204]	Loss 0.4068 (0.5360)	
training:	Epoch: [40][47/204]	Loss 0.6740 (0.5390)	
training:	Epoch: [40][48/204]	Loss 0.5585 (0.5394)	
training:	Epoch: [40][49/204]	Loss 0.6006 (0.5406)	
training:	Epoch: [40][50/204]	Loss 0.6322 (0.5424)	
training:	Epoch: [40][51/204]	Loss 0.6080 (0.5437)	
training:	Epoch: [40][52/204]	Loss 0.5873 (0.5446)	
training:	Epoch: [40][53/204]	Loss 0.5765 (0.5452)	
training:	Epoch: [40][54/204]	Loss 0.6328 (0.5468)	
training:	Epoch: [40][55/204]	Loss 0.5381 (0.5466)	
training:	Epoch: [40][56/204]	Loss 0.7204 (0.5497)	
training:	Epoch: [40][57/204]	Loss 0.6034 (0.5507)	
training:	Epoch: [40][58/204]	Loss 0.5168 (0.5501)	
training:	Epoch: [40][59/204]	Loss 0.6660 (0.5521)	
training:	Epoch: [40][60/204]	Loss 0.5371 (0.5518)	
training:	Epoch: [40][61/204]	Loss 0.5085 (0.5511)	
training:	Epoch: [40][62/204]	Loss 0.4430 (0.5494)	
training:	Epoch: [40][63/204]	Loss 0.5168 (0.5488)	
training:	Epoch: [40][64/204]	Loss 0.4944 (0.5480)	
training:	Epoch: [40][65/204]	Loss 0.5189 (0.5475)	
training:	Epoch: [40][66/204]	Loss 0.5512 (0.5476)	
training:	Epoch: [40][67/204]	Loss 0.4550 (0.5462)	
training:	Epoch: [40][68/204]	Loss 0.4779 (0.5452)	
training:	Epoch: [40][69/204]	Loss 0.5078 (0.5447)	
training:	Epoch: [40][70/204]	Loss 0.5655 (0.5450)	
training:	Epoch: [40][71/204]	Loss 0.5331 (0.5448)	
training:	Epoch: [40][72/204]	Loss 0.5868 (0.5454)	
training:	Epoch: [40][73/204]	Loss 0.6028 (0.5462)	
training:	Epoch: [40][74/204]	Loss 0.5406 (0.5461)	
training:	Epoch: [40][75/204]	Loss 0.5282 (0.5459)	
training:	Epoch: [40][76/204]	Loss 0.5107 (0.5454)	
training:	Epoch: [40][77/204]	Loss 0.5485 (0.5454)	
training:	Epoch: [40][78/204]	Loss 0.5087 (0.5450)	
training:	Epoch: [40][79/204]	Loss 0.7049 (0.5470)	
training:	Epoch: [40][80/204]	Loss 0.4996 (0.5464)	
training:	Epoch: [40][81/204]	Loss 0.4918 (0.5457)	
training:	Epoch: [40][82/204]	Loss 0.5045 (0.5452)	
training:	Epoch: [40][83/204]	Loss 0.5133 (0.5448)	
training:	Epoch: [40][84/204]	Loss 0.5571 (0.5450)	
training:	Epoch: [40][85/204]	Loss 0.4309 (0.5436)	
training:	Epoch: [40][86/204]	Loss 0.5613 (0.5438)	
training:	Epoch: [40][87/204]	Loss 0.4822 (0.5431)	
training:	Epoch: [40][88/204]	Loss 0.5206 (0.5429)	
training:	Epoch: [40][89/204]	Loss 0.5132 (0.5425)	
training:	Epoch: [40][90/204]	Loss 0.5010 (0.5421)	
training:	Epoch: [40][91/204]	Loss 0.6031 (0.5428)	
training:	Epoch: [40][92/204]	Loss 0.5228 (0.5425)	
training:	Epoch: [40][93/204]	Loss 0.5406 (0.5425)	
training:	Epoch: [40][94/204]	Loss 0.5710 (0.5428)	
training:	Epoch: [40][95/204]	Loss 0.4506 (0.5418)	
training:	Epoch: [40][96/204]	Loss 0.5711 (0.5422)	
training:	Epoch: [40][97/204]	Loss 0.5599 (0.5423)	
training:	Epoch: [40][98/204]	Loss 0.5282 (0.5422)	
training:	Epoch: [40][99/204]	Loss 0.4960 (0.5417)	
training:	Epoch: [40][100/204]	Loss 0.5332 (0.5416)	
training:	Epoch: [40][101/204]	Loss 0.5507 (0.5417)	
training:	Epoch: [40][102/204]	Loss 0.5374 (0.5417)	
training:	Epoch: [40][103/204]	Loss 0.5420 (0.5417)	
training:	Epoch: [40][104/204]	Loss 0.5399 (0.5417)	
training:	Epoch: [40][105/204]	Loss 0.5439 (0.5417)	
training:	Epoch: [40][106/204]	Loss 0.6324 (0.5425)	
training:	Epoch: [40][107/204]	Loss 0.5395 (0.5425)	
training:	Epoch: [40][108/204]	Loss 0.5161 (0.5423)	
training:	Epoch: [40][109/204]	Loss 0.4705 (0.5416)	
training:	Epoch: [40][110/204]	Loss 0.5978 (0.5421)	
training:	Epoch: [40][111/204]	Loss 0.5703 (0.5424)	
training:	Epoch: [40][112/204]	Loss 0.5743 (0.5427)	
training:	Epoch: [40][113/204]	Loss 0.5682 (0.5429)	
training:	Epoch: [40][114/204]	Loss 0.5956 (0.5434)	
training:	Epoch: [40][115/204]	Loss 0.4397 (0.5425)	
training:	Epoch: [40][116/204]	Loss 0.5332 (0.5424)	
training:	Epoch: [40][117/204]	Loss 0.6322 (0.5431)	
training:	Epoch: [40][118/204]	Loss 0.5115 (0.5429)	
training:	Epoch: [40][119/204]	Loss 0.5544 (0.5430)	
training:	Epoch: [40][120/204]	Loss 0.5819 (0.5433)	
training:	Epoch: [40][121/204]	Loss 0.5603 (0.5434)	
training:	Epoch: [40][122/204]	Loss 0.5441 (0.5434)	
training:	Epoch: [40][123/204]	Loss 0.4389 (0.5426)	
training:	Epoch: [40][124/204]	Loss 0.5895 (0.5430)	
training:	Epoch: [40][125/204]	Loss 0.5579 (0.5431)	
training:	Epoch: [40][126/204]	Loss 0.4526 (0.5424)	
training:	Epoch: [40][127/204]	Loss 0.4913 (0.5420)	
training:	Epoch: [40][128/204]	Loss 0.5911 (0.5424)	
training:	Epoch: [40][129/204]	Loss 0.4642 (0.5417)	
training:	Epoch: [40][130/204]	Loss 0.3924 (0.5406)	
training:	Epoch: [40][131/204]	Loss 0.5092 (0.5404)	
training:	Epoch: [40][132/204]	Loss 0.4758 (0.5399)	
training:	Epoch: [40][133/204]	Loss 0.5201 (0.5397)	
training:	Epoch: [40][134/204]	Loss 0.6068 (0.5402)	
training:	Epoch: [40][135/204]	Loss 0.4929 (0.5399)	
training:	Epoch: [40][136/204]	Loss 0.4792 (0.5394)	
training:	Epoch: [40][137/204]	Loss 0.4234 (0.5386)	
training:	Epoch: [40][138/204]	Loss 0.6119 (0.5391)	
training:	Epoch: [40][139/204]	Loss 0.4556 (0.5385)	
training:	Epoch: [40][140/204]	Loss 0.5418 (0.5385)	
training:	Epoch: [40][141/204]	Loss 0.5009 (0.5383)	
training:	Epoch: [40][142/204]	Loss 0.4724 (0.5378)	
training:	Epoch: [40][143/204]	Loss 0.5126 (0.5376)	
training:	Epoch: [40][144/204]	Loss 0.4418 (0.5370)	
training:	Epoch: [40][145/204]	Loss 0.3497 (0.5357)	
training:	Epoch: [40][146/204]	Loss 0.4826 (0.5353)	
training:	Epoch: [40][147/204]	Loss 0.3328 (0.5339)	
training:	Epoch: [40][148/204]	Loss 0.5859 (0.5343)	
training:	Epoch: [40][149/204]	Loss 0.5815 (0.5346)	
training:	Epoch: [40][150/204]	Loss 0.5128 (0.5344)	
training:	Epoch: [40][151/204]	Loss 0.5384 (0.5345)	
training:	Epoch: [40][152/204]	Loss 0.4755 (0.5341)	
training:	Epoch: [40][153/204]	Loss 0.4965 (0.5338)	
training:	Epoch: [40][154/204]	Loss 0.3548 (0.5327)	
training:	Epoch: [40][155/204]	Loss 0.4012 (0.5318)	
training:	Epoch: [40][156/204]	Loss 0.4809 (0.5315)	
training:	Epoch: [40][157/204]	Loss 0.5074 (0.5314)	
training:	Epoch: [40][158/204]	Loss 0.4517 (0.5308)	
training:	Epoch: [40][159/204]	Loss 0.5580 (0.5310)	
training:	Epoch: [40][160/204]	Loss 0.6101 (0.5315)	
training:	Epoch: [40][161/204]	Loss 0.5303 (0.5315)	
training:	Epoch: [40][162/204]	Loss 0.5469 (0.5316)	
training:	Epoch: [40][163/204]	Loss 0.5412 (0.5317)	
training:	Epoch: [40][164/204]	Loss 0.4844 (0.5314)	
training:	Epoch: [40][165/204]	Loss 0.5473 (0.5315)	
training:	Epoch: [40][166/204]	Loss 0.4869 (0.5312)	
training:	Epoch: [40][167/204]	Loss 0.5019 (0.5310)	
training:	Epoch: [40][168/204]	Loss 0.5067 (0.5309)	
training:	Epoch: [40][169/204]	Loss 0.5163 (0.5308)	
training:	Epoch: [40][170/204]	Loss 0.5232 (0.5307)	
training:	Epoch: [40][171/204]	Loss 0.4599 (0.5303)	
training:	Epoch: [40][172/204]	Loss 0.5732 (0.5306)	
training:	Epoch: [40][173/204]	Loss 0.4127 (0.5299)	
training:	Epoch: [40][174/204]	Loss 0.5131 (0.5298)	
training:	Epoch: [40][175/204]	Loss 0.6287 (0.5304)	
training:	Epoch: [40][176/204]	Loss 0.5400 (0.5304)	
training:	Epoch: [40][177/204]	Loss 0.5956 (0.5308)	
training:	Epoch: [40][178/204]	Loss 0.5831 (0.5311)	
training:	Epoch: [40][179/204]	Loss 0.5377 (0.5311)	
training:	Epoch: [40][180/204]	Loss 0.3797 (0.5303)	
training:	Epoch: [40][181/204]	Loss 0.5841 (0.5306)	
training:	Epoch: [40][182/204]	Loss 0.4523 (0.5302)	
training:	Epoch: [40][183/204]	Loss 0.5506 (0.5303)	
training:	Epoch: [40][184/204]	Loss 0.5118 (0.5302)	
training:	Epoch: [40][185/204]	Loss 0.5458 (0.5302)	
training:	Epoch: [40][186/204]	Loss 0.5237 (0.5302)	
training:	Epoch: [40][187/204]	Loss 0.6199 (0.5307)	
training:	Epoch: [40][188/204]	Loss 0.5796 (0.5310)	
training:	Epoch: [40][189/204]	Loss 0.5736 (0.5312)	
training:	Epoch: [40][190/204]	Loss 0.4417 (0.5307)	
training:	Epoch: [40][191/204]	Loss 0.5583 (0.5308)	
training:	Epoch: [40][192/204]	Loss 0.5499 (0.5309)	
training:	Epoch: [40][193/204]	Loss 0.5820 (0.5312)	
training:	Epoch: [40][194/204]	Loss 0.5734 (0.5314)	
training:	Epoch: [40][195/204]	Loss 0.5350 (0.5314)	
training:	Epoch: [40][196/204]	Loss 0.5347 (0.5315)	
training:	Epoch: [40][197/204]	Loss 0.6598 (0.5321)	
training:	Epoch: [40][198/204]	Loss 0.5284 (0.5321)	
training:	Epoch: [40][199/204]	Loss 0.5696 (0.5323)	
training:	Epoch: [40][200/204]	Loss 0.5371 (0.5323)	
training:	Epoch: [40][201/204]	Loss 0.5215 (0.5323)	
training:	Epoch: [40][202/204]	Loss 0.3809 (0.5315)	
training:	Epoch: [40][203/204]	Loss 0.4320 (0.5310)	
training:	Epoch: [40][204/204]	Loss 0.4946 (0.5308)	
Training:	 Loss: 0.5300

Training:	 ACC: 0.7484 0.7495 0.7740 0.7229
Validation:	 ACC: 0.7476 0.7491 0.7799 0.7152
Validation:	 Best_BACC: 0.7476 0.7491 0.7799 0.7152
Validation:	 Loss: 0.5268
Pretraining:	Epoch 41/120
----------
training:	Epoch: [41][1/204]	Loss 0.5396 (0.5396)	
training:	Epoch: [41][2/204]	Loss 0.5714 (0.5555)	
training:	Epoch: [41][3/204]	Loss 0.4942 (0.5351)	
training:	Epoch: [41][4/204]	Loss 0.4397 (0.5112)	
training:	Epoch: [41][5/204]	Loss 0.5268 (0.5143)	
training:	Epoch: [41][6/204]	Loss 0.5241 (0.5160)	
training:	Epoch: [41][7/204]	Loss 0.5948 (0.5272)	
training:	Epoch: [41][8/204]	Loss 0.6112 (0.5377)	
training:	Epoch: [41][9/204]	Loss 0.5497 (0.5391)	
training:	Epoch: [41][10/204]	Loss 0.4762 (0.5328)	
training:	Epoch: [41][11/204]	Loss 0.4052 (0.5212)	
training:	Epoch: [41][12/204]	Loss 0.4983 (0.5193)	
training:	Epoch: [41][13/204]	Loss 0.4940 (0.5173)	
training:	Epoch: [41][14/204]	Loss 0.5474 (0.5195)	
training:	Epoch: [41][15/204]	Loss 0.5593 (0.5221)	
training:	Epoch: [41][16/204]	Loss 0.4579 (0.5181)	
training:	Epoch: [41][17/204]	Loss 0.5722 (0.5213)	
training:	Epoch: [41][18/204]	Loss 0.5055 (0.5204)	
training:	Epoch: [41][19/204]	Loss 0.4917 (0.5189)	
training:	Epoch: [41][20/204]	Loss 0.6521 (0.5256)	
training:	Epoch: [41][21/204]	Loss 0.5929 (0.5288)	
training:	Epoch: [41][22/204]	Loss 0.5789 (0.5310)	
training:	Epoch: [41][23/204]	Loss 0.4658 (0.5282)	
training:	Epoch: [41][24/204]	Loss 0.5365 (0.5286)	
training:	Epoch: [41][25/204]	Loss 0.5515 (0.5295)	
training:	Epoch: [41][26/204]	Loss 0.5263 (0.5294)	
training:	Epoch: [41][27/204]	Loss 0.5053 (0.5285)	
training:	Epoch: [41][28/204]	Loss 0.5056 (0.5276)	
training:	Epoch: [41][29/204]	Loss 0.4747 (0.5258)	
training:	Epoch: [41][30/204]	Loss 0.5271 (0.5259)	
training:	Epoch: [41][31/204]	Loss 0.4773 (0.5243)	
training:	Epoch: [41][32/204]	Loss 0.4974 (0.5235)	
training:	Epoch: [41][33/204]	Loss 0.4951 (0.5226)	
training:	Epoch: [41][34/204]	Loss 0.6608 (0.5267)	
training:	Epoch: [41][35/204]	Loss 0.5151 (0.5263)	
training:	Epoch: [41][36/204]	Loss 0.4656 (0.5246)	
training:	Epoch: [41][37/204]	Loss 0.5432 (0.5251)	
training:	Epoch: [41][38/204]	Loss 0.4817 (0.5240)	
training:	Epoch: [41][39/204]	Loss 0.5335 (0.5243)	
training:	Epoch: [41][40/204]	Loss 0.4927 (0.5235)	
training:	Epoch: [41][41/204]	Loss 0.6192 (0.5258)	
training:	Epoch: [41][42/204]	Loss 0.4276 (0.5235)	
training:	Epoch: [41][43/204]	Loss 0.5836 (0.5249)	
training:	Epoch: [41][44/204]	Loss 0.6139 (0.5269)	
training:	Epoch: [41][45/204]	Loss 0.5306 (0.5270)	
training:	Epoch: [41][46/204]	Loss 0.6808 (0.5303)	
training:	Epoch: [41][47/204]	Loss 0.3869 (0.5273)	
training:	Epoch: [41][48/204]	Loss 0.4860 (0.5264)	
training:	Epoch: [41][49/204]	Loss 0.4833 (0.5255)	
training:	Epoch: [41][50/204]	Loss 0.5149 (0.5253)	
training:	Epoch: [41][51/204]	Loss 0.4655 (0.5241)	
training:	Epoch: [41][52/204]	Loss 0.6591 (0.5267)	
training:	Epoch: [41][53/204]	Loss 0.5226 (0.5267)	
training:	Epoch: [41][54/204]	Loss 0.5077 (0.5263)	
training:	Epoch: [41][55/204]	Loss 0.4969 (0.5258)	
training:	Epoch: [41][56/204]	Loss 0.5825 (0.5268)	
training:	Epoch: [41][57/204]	Loss 0.5534 (0.5272)	
training:	Epoch: [41][58/204]	Loss 0.6665 (0.5296)	
training:	Epoch: [41][59/204]	Loss 0.4820 (0.5288)	
training:	Epoch: [41][60/204]	Loss 0.4610 (0.5277)	
training:	Epoch: [41][61/204]	Loss 0.4200 (0.5259)	
training:	Epoch: [41][62/204]	Loss 0.5556 (0.5264)	
training:	Epoch: [41][63/204]	Loss 0.5665 (0.5271)	
training:	Epoch: [41][64/204]	Loss 0.6956 (0.5297)	
training:	Epoch: [41][65/204]	Loss 0.4931 (0.5291)	
training:	Epoch: [41][66/204]	Loss 0.4378 (0.5277)	
training:	Epoch: [41][67/204]	Loss 0.5792 (0.5285)	
training:	Epoch: [41][68/204]	Loss 0.4367 (0.5272)	
training:	Epoch: [41][69/204]	Loss 0.6318 (0.5287)	
training:	Epoch: [41][70/204]	Loss 0.4262 (0.5272)	
training:	Epoch: [41][71/204]	Loss 0.5906 (0.5281)	
training:	Epoch: [41][72/204]	Loss 0.5631 (0.5286)	
training:	Epoch: [41][73/204]	Loss 0.4823 (0.5280)	
training:	Epoch: [41][74/204]	Loss 0.4931 (0.5275)	
training:	Epoch: [41][75/204]	Loss 0.5309 (0.5275)	
training:	Epoch: [41][76/204]	Loss 0.5241 (0.5275)	
training:	Epoch: [41][77/204]	Loss 0.4698 (0.5267)	
training:	Epoch: [41][78/204]	Loss 0.4766 (0.5261)	
training:	Epoch: [41][79/204]	Loss 0.5916 (0.5269)	
training:	Epoch: [41][80/204]	Loss 0.5363 (0.5270)	
training:	Epoch: [41][81/204]	Loss 0.5248 (0.5270)	
training:	Epoch: [41][82/204]	Loss 0.5447 (0.5272)	
training:	Epoch: [41][83/204]	Loss 0.5417 (0.5274)	
training:	Epoch: [41][84/204]	Loss 0.4811 (0.5269)	
training:	Epoch: [41][85/204]	Loss 0.5065 (0.5266)	
training:	Epoch: [41][86/204]	Loss 0.5172 (0.5265)	
training:	Epoch: [41][87/204]	Loss 0.5749 (0.5271)	
training:	Epoch: [41][88/204]	Loss 0.5800 (0.5277)	
training:	Epoch: [41][89/204]	Loss 0.4917 (0.5273)	
training:	Epoch: [41][90/204]	Loss 0.5118 (0.5271)	
training:	Epoch: [41][91/204]	Loss 0.4545 (0.5263)	
training:	Epoch: [41][92/204]	Loss 0.4967 (0.5260)	
training:	Epoch: [41][93/204]	Loss 0.5070 (0.5258)	
training:	Epoch: [41][94/204]	Loss 0.4750 (0.5252)	
training:	Epoch: [41][95/204]	Loss 0.4718 (0.5247)	
training:	Epoch: [41][96/204]	Loss 0.5670 (0.5251)	
training:	Epoch: [41][97/204]	Loss 0.4788 (0.5246)	
training:	Epoch: [41][98/204]	Loss 0.4221 (0.5236)	
training:	Epoch: [41][99/204]	Loss 0.5526 (0.5239)	
training:	Epoch: [41][100/204]	Loss 0.6232 (0.5249)	
training:	Epoch: [41][101/204]	Loss 0.5868 (0.5255)	
training:	Epoch: [41][102/204]	Loss 0.6438 (0.5266)	
training:	Epoch: [41][103/204]	Loss 0.5054 (0.5264)	
training:	Epoch: [41][104/204]	Loss 0.4653 (0.5258)	
training:	Epoch: [41][105/204]	Loss 0.4370 (0.5250)	
training:	Epoch: [41][106/204]	Loss 0.5002 (0.5248)	
training:	Epoch: [41][107/204]	Loss 0.5182 (0.5247)	
training:	Epoch: [41][108/204]	Loss 0.4368 (0.5239)	
training:	Epoch: [41][109/204]	Loss 0.4787 (0.5235)	
training:	Epoch: [41][110/204]	Loss 0.5480 (0.5237)	
training:	Epoch: [41][111/204]	Loss 0.5822 (0.5242)	
training:	Epoch: [41][112/204]	Loss 0.4680 (0.5237)	
training:	Epoch: [41][113/204]	Loss 0.4865 (0.5234)	
training:	Epoch: [41][114/204]	Loss 0.5611 (0.5237)	
training:	Epoch: [41][115/204]	Loss 0.5532 (0.5240)	
training:	Epoch: [41][116/204]	Loss 0.6389 (0.5250)	
training:	Epoch: [41][117/204]	Loss 0.5123 (0.5249)	
training:	Epoch: [41][118/204]	Loss 0.5414 (0.5250)	
training:	Epoch: [41][119/204]	Loss 0.5101 (0.5249)	
training:	Epoch: [41][120/204]	Loss 0.5980 (0.5255)	
training:	Epoch: [41][121/204]	Loss 0.5077 (0.5253)	
training:	Epoch: [41][122/204]	Loss 0.3970 (0.5243)	
training:	Epoch: [41][123/204]	Loss 0.4723 (0.5239)	
training:	Epoch: [41][124/204]	Loss 0.8021 (0.5261)	
training:	Epoch: [41][125/204]	Loss 0.5025 (0.5259)	
training:	Epoch: [41][126/204]	Loss 0.5240 (0.5259)	
training:	Epoch: [41][127/204]	Loss 0.3568 (0.5246)	
training:	Epoch: [41][128/204]	Loss 0.5126 (0.5245)	
training:	Epoch: [41][129/204]	Loss 0.5659 (0.5248)	
training:	Epoch: [41][130/204]	Loss 0.5809 (0.5252)	
training:	Epoch: [41][131/204]	Loss 0.5666 (0.5256)	
training:	Epoch: [41][132/204]	Loss 0.4890 (0.5253)	
training:	Epoch: [41][133/204]	Loss 0.5619 (0.5255)	
training:	Epoch: [41][134/204]	Loss 0.3839 (0.5245)	
training:	Epoch: [41][135/204]	Loss 0.6267 (0.5252)	
training:	Epoch: [41][136/204]	Loss 0.5555 (0.5255)	
training:	Epoch: [41][137/204]	Loss 0.5264 (0.5255)	
training:	Epoch: [41][138/204]	Loss 0.4852 (0.5252)	
training:	Epoch: [41][139/204]	Loss 0.5378 (0.5253)	
training:	Epoch: [41][140/204]	Loss 0.4621 (0.5248)	
training:	Epoch: [41][141/204]	Loss 0.5702 (0.5251)	
training:	Epoch: [41][142/204]	Loss 0.5597 (0.5254)	
training:	Epoch: [41][143/204]	Loss 0.5332 (0.5254)	
training:	Epoch: [41][144/204]	Loss 0.4997 (0.5253)	
training:	Epoch: [41][145/204]	Loss 0.5098 (0.5252)	
training:	Epoch: [41][146/204]	Loss 0.6126 (0.5258)	
training:	Epoch: [41][147/204]	Loss 0.7597 (0.5274)	
training:	Epoch: [41][148/204]	Loss 0.4544 (0.5269)	
training:	Epoch: [41][149/204]	Loss 0.5459 (0.5270)	
training:	Epoch: [41][150/204]	Loss 0.5525 (0.5272)	
training:	Epoch: [41][151/204]	Loss 0.5526 (0.5273)	
training:	Epoch: [41][152/204]	Loss 0.4515 (0.5268)	
training:	Epoch: [41][153/204]	Loss 0.4572 (0.5264)	
training:	Epoch: [41][154/204]	Loss 0.5654 (0.5266)	
training:	Epoch: [41][155/204]	Loss 0.6812 (0.5276)	
training:	Epoch: [41][156/204]	Loss 0.4420 (0.5271)	
training:	Epoch: [41][157/204]	Loss 0.6210 (0.5277)	
training:	Epoch: [41][158/204]	Loss 0.6085 (0.5282)	
training:	Epoch: [41][159/204]	Loss 0.6823 (0.5292)	
training:	Epoch: [41][160/204]	Loss 0.4237 (0.5285)	
training:	Epoch: [41][161/204]	Loss 0.5842 (0.5288)	
training:	Epoch: [41][162/204]	Loss 0.5638 (0.5291)	
training:	Epoch: [41][163/204]	Loss 0.6132 (0.5296)	
training:	Epoch: [41][164/204]	Loss 0.4616 (0.5292)	
training:	Epoch: [41][165/204]	Loss 0.5423 (0.5292)	
training:	Epoch: [41][166/204]	Loss 0.6054 (0.5297)	
training:	Epoch: [41][167/204]	Loss 0.5080 (0.5296)	
training:	Epoch: [41][168/204]	Loss 0.5570 (0.5297)	
training:	Epoch: [41][169/204]	Loss 0.4569 (0.5293)	
training:	Epoch: [41][170/204]	Loss 0.4628 (0.5289)	
training:	Epoch: [41][171/204]	Loss 0.5024 (0.5287)	
training:	Epoch: [41][172/204]	Loss 0.4638 (0.5284)	
training:	Epoch: [41][173/204]	Loss 0.6431 (0.5290)	
training:	Epoch: [41][174/204]	Loss 0.5167 (0.5290)	
training:	Epoch: [41][175/204]	Loss 0.5458 (0.5291)	
training:	Epoch: [41][176/204]	Loss 0.5797 (0.5293)	
training:	Epoch: [41][177/204]	Loss 0.5004 (0.5292)	
training:	Epoch: [41][178/204]	Loss 0.4897 (0.5290)	
training:	Epoch: [41][179/204]	Loss 0.6175 (0.5295)	
training:	Epoch: [41][180/204]	Loss 0.4198 (0.5288)	
training:	Epoch: [41][181/204]	Loss 0.4708 (0.5285)	
training:	Epoch: [41][182/204]	Loss 0.6841 (0.5294)	
training:	Epoch: [41][183/204]	Loss 0.4470 (0.5289)	
training:	Epoch: [41][184/204]	Loss 0.6537 (0.5296)	
training:	Epoch: [41][185/204]	Loss 0.4470 (0.5292)	
training:	Epoch: [41][186/204]	Loss 0.5951 (0.5295)	
training:	Epoch: [41][187/204]	Loss 0.5314 (0.5295)	
training:	Epoch: [41][188/204]	Loss 0.5048 (0.5294)	
training:	Epoch: [41][189/204]	Loss 0.3928 (0.5287)	
training:	Epoch: [41][190/204]	Loss 0.4414 (0.5282)	
training:	Epoch: [41][191/204]	Loss 0.5256 (0.5282)	
training:	Epoch: [41][192/204]	Loss 0.5495 (0.5283)	
training:	Epoch: [41][193/204]	Loss 0.6702 (0.5290)	
training:	Epoch: [41][194/204]	Loss 0.6228 (0.5295)	
training:	Epoch: [41][195/204]	Loss 0.5627 (0.5297)	
training:	Epoch: [41][196/204]	Loss 0.5104 (0.5296)	
training:	Epoch: [41][197/204]	Loss 0.5105 (0.5295)	
training:	Epoch: [41][198/204]	Loss 0.4771 (0.5292)	
training:	Epoch: [41][199/204]	Loss 0.7209 (0.5302)	
training:	Epoch: [41][200/204]	Loss 0.4384 (0.5297)	
training:	Epoch: [41][201/204]	Loss 0.4933 (0.5296)	
training:	Epoch: [41][202/204]	Loss 0.4551 (0.5292)	
training:	Epoch: [41][203/204]	Loss 0.4244 (0.5287)	
training:	Epoch: [41][204/204]	Loss 0.4938 (0.5285)	
Training:	 Loss: 0.5277

Training:	 ACC: 0.7499 0.7502 0.7581 0.7417
Validation:	 ACC: 0.7522 0.7528 0.7646 0.7399
Validation:	 Best_BACC: 0.7522 0.7528 0.7646 0.7399
Validation:	 Loss: 0.5240
Pretraining:	Epoch 42/120
----------
training:	Epoch: [42][1/204]	Loss 0.5996 (0.5996)	
training:	Epoch: [42][2/204]	Loss 0.4779 (0.5387)	
training:	Epoch: [42][3/204]	Loss 0.5878 (0.5551)	
training:	Epoch: [42][4/204]	Loss 0.5400 (0.5513)	
training:	Epoch: [42][5/204]	Loss 0.3975 (0.5206)	
training:	Epoch: [42][6/204]	Loss 0.5505 (0.5256)	
training:	Epoch: [42][7/204]	Loss 0.4643 (0.5168)	
training:	Epoch: [42][8/204]	Loss 0.4745 (0.5115)	
training:	Epoch: [42][9/204]	Loss 0.4365 (0.5032)	
training:	Epoch: [42][10/204]	Loss 0.5980 (0.5127)	
training:	Epoch: [42][11/204]	Loss 0.4580 (0.5077)	
training:	Epoch: [42][12/204]	Loss 0.4512 (0.5030)	
training:	Epoch: [42][13/204]	Loss 0.3985 (0.4949)	
training:	Epoch: [42][14/204]	Loss 0.6180 (0.5037)	
training:	Epoch: [42][15/204]	Loss 0.5751 (0.5085)	
training:	Epoch: [42][16/204]	Loss 0.6720 (0.5187)	
training:	Epoch: [42][17/204]	Loss 0.5021 (0.5177)	
training:	Epoch: [42][18/204]	Loss 0.6187 (0.5233)	
training:	Epoch: [42][19/204]	Loss 0.4526 (0.5196)	
training:	Epoch: [42][20/204]	Loss 0.5891 (0.5231)	
training:	Epoch: [42][21/204]	Loss 0.4875 (0.5214)	
training:	Epoch: [42][22/204]	Loss 0.4380 (0.5176)	
training:	Epoch: [42][23/204]	Loss 0.4928 (0.5165)	
training:	Epoch: [42][24/204]	Loss 0.4031 (0.5118)	
training:	Epoch: [42][25/204]	Loss 0.4381 (0.5089)	
training:	Epoch: [42][26/204]	Loss 0.4721 (0.5074)	
training:	Epoch: [42][27/204]	Loss 0.5728 (0.5099)	
training:	Epoch: [42][28/204]	Loss 0.6171 (0.5137)	
training:	Epoch: [42][29/204]	Loss 0.5741 (0.5158)	
training:	Epoch: [42][30/204]	Loss 0.5498 (0.5169)	
training:	Epoch: [42][31/204]	Loss 0.5156 (0.5169)	
training:	Epoch: [42][32/204]	Loss 0.5253 (0.5171)	
training:	Epoch: [42][33/204]	Loss 0.5048 (0.5168)	
training:	Epoch: [42][34/204]	Loss 0.5339 (0.5173)	
training:	Epoch: [42][35/204]	Loss 0.5566 (0.5184)	
training:	Epoch: [42][36/204]	Loss 0.6052 (0.5208)	
training:	Epoch: [42][37/204]	Loss 0.5974 (0.5229)	
training:	Epoch: [42][38/204]	Loss 0.4995 (0.5223)	
training:	Epoch: [42][39/204]	Loss 0.4433 (0.5202)	
training:	Epoch: [42][40/204]	Loss 0.5798 (0.5217)	
training:	Epoch: [42][41/204]	Loss 0.5489 (0.5224)	
training:	Epoch: [42][42/204]	Loss 0.4023 (0.5195)	
training:	Epoch: [42][43/204]	Loss 0.4900 (0.5188)	
training:	Epoch: [42][44/204]	Loss 0.5880 (0.5204)	
training:	Epoch: [42][45/204]	Loss 0.5343 (0.5207)	
training:	Epoch: [42][46/204]	Loss 0.5025 (0.5203)	
training:	Epoch: [42][47/204]	Loss 0.5173 (0.5203)	
training:	Epoch: [42][48/204]	Loss 0.5531 (0.5209)	
training:	Epoch: [42][49/204]	Loss 0.5663 (0.5219)	
training:	Epoch: [42][50/204]	Loss 0.6611 (0.5246)	
training:	Epoch: [42][51/204]	Loss 0.4685 (0.5235)	
training:	Epoch: [42][52/204]	Loss 0.6034 (0.5251)	
training:	Epoch: [42][53/204]	Loss 0.5731 (0.5260)	
training:	Epoch: [42][54/204]	Loss 0.4796 (0.5251)	
training:	Epoch: [42][55/204]	Loss 0.4662 (0.5241)	
training:	Epoch: [42][56/204]	Loss 0.5411 (0.5244)	
training:	Epoch: [42][57/204]	Loss 0.5770 (0.5253)	
training:	Epoch: [42][58/204]	Loss 0.5737 (0.5261)	
training:	Epoch: [42][59/204]	Loss 0.5190 (0.5260)	
training:	Epoch: [42][60/204]	Loss 0.5994 (0.5272)	
training:	Epoch: [42][61/204]	Loss 0.5609 (0.5278)	
training:	Epoch: [42][62/204]	Loss 0.5380 (0.5279)	
training:	Epoch: [42][63/204]	Loss 0.5053 (0.5276)	
training:	Epoch: [42][64/204]	Loss 0.5128 (0.5274)	
training:	Epoch: [42][65/204]	Loss 0.4968 (0.5269)	
training:	Epoch: [42][66/204]	Loss 0.5278 (0.5269)	
training:	Epoch: [42][67/204]	Loss 0.5244 (0.5269)	
training:	Epoch: [42][68/204]	Loss 0.5031 (0.5265)	
training:	Epoch: [42][69/204]	Loss 0.4725 (0.5257)	
training:	Epoch: [42][70/204]	Loss 0.5541 (0.5261)	
training:	Epoch: [42][71/204]	Loss 0.5865 (0.5270)	
training:	Epoch: [42][72/204]	Loss 0.4556 (0.5260)	
training:	Epoch: [42][73/204]	Loss 0.4702 (0.5252)	
training:	Epoch: [42][74/204]	Loss 0.4562 (0.5243)	
training:	Epoch: [42][75/204]	Loss 0.5710 (0.5249)	
training:	Epoch: [42][76/204]	Loss 0.4214 (0.5236)	
training:	Epoch: [42][77/204]	Loss 0.3941 (0.5219)	
training:	Epoch: [42][78/204]	Loss 0.4376 (0.5208)	
training:	Epoch: [42][79/204]	Loss 0.5512 (0.5212)	
training:	Epoch: [42][80/204]	Loss 0.5335 (0.5213)	
training:	Epoch: [42][81/204]	Loss 0.6478 (0.5229)	
training:	Epoch: [42][82/204]	Loss 0.5625 (0.5234)	
training:	Epoch: [42][83/204]	Loss 0.5221 (0.5234)	
training:	Epoch: [42][84/204]	Loss 0.5664 (0.5239)	
training:	Epoch: [42][85/204]	Loss 0.5047 (0.5236)	
training:	Epoch: [42][86/204]	Loss 0.5039 (0.5234)	
training:	Epoch: [42][87/204]	Loss 0.5323 (0.5235)	
training:	Epoch: [42][88/204]	Loss 0.5381 (0.5237)	
training:	Epoch: [42][89/204]	Loss 0.6167 (0.5247)	
training:	Epoch: [42][90/204]	Loss 0.5966 (0.5255)	
training:	Epoch: [42][91/204]	Loss 0.6238 (0.5266)	
training:	Epoch: [42][92/204]	Loss 0.4897 (0.5262)	
training:	Epoch: [42][93/204]	Loss 0.3819 (0.5247)	
training:	Epoch: [42][94/204]	Loss 0.5831 (0.5253)	
training:	Epoch: [42][95/204]	Loss 0.5546 (0.5256)	
training:	Epoch: [42][96/204]	Loss 0.4516 (0.5248)	
training:	Epoch: [42][97/204]	Loss 0.5929 (0.5255)	
training:	Epoch: [42][98/204]	Loss 0.4065 (0.5243)	
training:	Epoch: [42][99/204]	Loss 0.5347 (0.5244)	
training:	Epoch: [42][100/204]	Loss 0.6335 (0.5255)	
training:	Epoch: [42][101/204]	Loss 0.5204 (0.5254)	
training:	Epoch: [42][102/204]	Loss 0.4682 (0.5249)	
training:	Epoch: [42][103/204]	Loss 0.6428 (0.5260)	
training:	Epoch: [42][104/204]	Loss 0.5940 (0.5267)	
training:	Epoch: [42][105/204]	Loss 0.4930 (0.5264)	
training:	Epoch: [42][106/204]	Loss 0.4690 (0.5258)	
training:	Epoch: [42][107/204]	Loss 0.5349 (0.5259)	
training:	Epoch: [42][108/204]	Loss 0.4749 (0.5254)	
training:	Epoch: [42][109/204]	Loss 0.5471 (0.5256)	
training:	Epoch: [42][110/204]	Loss 0.5193 (0.5256)	
training:	Epoch: [42][111/204]	Loss 0.5624 (0.5259)	
training:	Epoch: [42][112/204]	Loss 0.4441 (0.5252)	
training:	Epoch: [42][113/204]	Loss 0.5989 (0.5258)	
training:	Epoch: [42][114/204]	Loss 0.5207 (0.5258)	
training:	Epoch: [42][115/204]	Loss 0.5554 (0.5260)	
training:	Epoch: [42][116/204]	Loss 0.3827 (0.5248)	
training:	Epoch: [42][117/204]	Loss 0.5323 (0.5249)	
training:	Epoch: [42][118/204]	Loss 0.5418 (0.5250)	
training:	Epoch: [42][119/204]	Loss 0.5158 (0.5249)	
training:	Epoch: [42][120/204]	Loss 0.5151 (0.5249)	
training:	Epoch: [42][121/204]	Loss 0.5513 (0.5251)	
training:	Epoch: [42][122/204]	Loss 0.4814 (0.5247)	
training:	Epoch: [42][123/204]	Loss 0.5050 (0.5246)	
training:	Epoch: [42][124/204]	Loss 0.6195 (0.5253)	
training:	Epoch: [42][125/204]	Loss 0.5545 (0.5256)	
training:	Epoch: [42][126/204]	Loss 0.5803 (0.5260)	
training:	Epoch: [42][127/204]	Loss 0.4939 (0.5257)	
training:	Epoch: [42][128/204]	Loss 0.4837 (0.5254)	
training:	Epoch: [42][129/204]	Loss 0.4654 (0.5249)	
training:	Epoch: [42][130/204]	Loss 0.6423 (0.5258)	
training:	Epoch: [42][131/204]	Loss 0.5481 (0.5260)	
training:	Epoch: [42][132/204]	Loss 0.5425 (0.5261)	
training:	Epoch: [42][133/204]	Loss 0.6218 (0.5269)	
training:	Epoch: [42][134/204]	Loss 0.6136 (0.5275)	
training:	Epoch: [42][135/204]	Loss 0.5524 (0.5277)	
training:	Epoch: [42][136/204]	Loss 0.5620 (0.5279)	
training:	Epoch: [42][137/204]	Loss 0.4421 (0.5273)	
training:	Epoch: [42][138/204]	Loss 0.4867 (0.5270)	
training:	Epoch: [42][139/204]	Loss 0.5377 (0.5271)	
training:	Epoch: [42][140/204]	Loss 0.5986 (0.5276)	
training:	Epoch: [42][141/204]	Loss 0.5520 (0.5278)	
training:	Epoch: [42][142/204]	Loss 0.5848 (0.5282)	
training:	Epoch: [42][143/204]	Loss 0.6234 (0.5289)	
training:	Epoch: [42][144/204]	Loss 0.5207 (0.5288)	
training:	Epoch: [42][145/204]	Loss 0.5670 (0.5291)	
training:	Epoch: [42][146/204]	Loss 0.5585 (0.5293)	
training:	Epoch: [42][147/204]	Loss 0.5355 (0.5293)	
training:	Epoch: [42][148/204]	Loss 0.4944 (0.5291)	
training:	Epoch: [42][149/204]	Loss 0.5148 (0.5290)	
training:	Epoch: [42][150/204]	Loss 0.4171 (0.5282)	
training:	Epoch: [42][151/204]	Loss 0.5589 (0.5284)	
training:	Epoch: [42][152/204]	Loss 0.5809 (0.5288)	
training:	Epoch: [42][153/204]	Loss 0.5120 (0.5287)	
training:	Epoch: [42][154/204]	Loss 0.4007 (0.5278)	
training:	Epoch: [42][155/204]	Loss 0.5341 (0.5279)	
training:	Epoch: [42][156/204]	Loss 0.5276 (0.5279)	
training:	Epoch: [42][157/204]	Loss 0.4903 (0.5276)	
training:	Epoch: [42][158/204]	Loss 0.5785 (0.5280)	
training:	Epoch: [42][159/204]	Loss 0.5385 (0.5280)	
training:	Epoch: [42][160/204]	Loss 0.5957 (0.5284)	
training:	Epoch: [42][161/204]	Loss 0.4766 (0.5281)	
training:	Epoch: [42][162/204]	Loss 0.4698 (0.5278)	
training:	Epoch: [42][163/204]	Loss 0.6302 (0.5284)	
training:	Epoch: [42][164/204]	Loss 0.4674 (0.5280)	
training:	Epoch: [42][165/204]	Loss 0.3244 (0.5268)	
training:	Epoch: [42][166/204]	Loss 0.4524 (0.5263)	
training:	Epoch: [42][167/204]	Loss 0.5128 (0.5263)	
training:	Epoch: [42][168/204]	Loss 0.5746 (0.5265)	
training:	Epoch: [42][169/204]	Loss 0.4331 (0.5260)	
training:	Epoch: [42][170/204]	Loss 0.4130 (0.5253)	
training:	Epoch: [42][171/204]	Loss 0.5161 (0.5253)	
training:	Epoch: [42][172/204]	Loss 0.4961 (0.5251)	
training:	Epoch: [42][173/204]	Loss 0.5988 (0.5255)	
training:	Epoch: [42][174/204]	Loss 0.6302 (0.5261)	
training:	Epoch: [42][175/204]	Loss 0.4989 (0.5260)	
training:	Epoch: [42][176/204]	Loss 0.5184 (0.5259)	
training:	Epoch: [42][177/204]	Loss 0.4727 (0.5256)	
training:	Epoch: [42][178/204]	Loss 0.5464 (0.5257)	
training:	Epoch: [42][179/204]	Loss 0.4989 (0.5256)	
training:	Epoch: [42][180/204]	Loss 0.4826 (0.5254)	
training:	Epoch: [42][181/204]	Loss 0.4422 (0.5249)	
training:	Epoch: [42][182/204]	Loss 0.4521 (0.5245)	
training:	Epoch: [42][183/204]	Loss 0.4589 (0.5241)	
training:	Epoch: [42][184/204]	Loss 0.5042 (0.5240)	
training:	Epoch: [42][185/204]	Loss 0.5557 (0.5242)	
training:	Epoch: [42][186/204]	Loss 0.5208 (0.5242)	
training:	Epoch: [42][187/204]	Loss 0.4315 (0.5237)	
training:	Epoch: [42][188/204]	Loss 0.4248 (0.5232)	
training:	Epoch: [42][189/204]	Loss 0.4486 (0.5228)	
training:	Epoch: [42][190/204]	Loss 0.6805 (0.5236)	
training:	Epoch: [42][191/204]	Loss 0.5765 (0.5239)	
training:	Epoch: [42][192/204]	Loss 0.6634 (0.5246)	
training:	Epoch: [42][193/204]	Loss 0.5038 (0.5245)	
training:	Epoch: [42][194/204]	Loss 0.6094 (0.5249)	
training:	Epoch: [42][195/204]	Loss 0.5236 (0.5249)	
training:	Epoch: [42][196/204]	Loss 0.7444 (0.5260)	
training:	Epoch: [42][197/204]	Loss 0.5223 (0.5260)	
training:	Epoch: [42][198/204]	Loss 0.5306 (0.5260)	
training:	Epoch: [42][199/204]	Loss 0.3868 (0.5253)	
training:	Epoch: [42][200/204]	Loss 0.5435 (0.5254)	
training:	Epoch: [42][201/204]	Loss 0.6348 (0.5260)	
training:	Epoch: [42][202/204]	Loss 0.5972 (0.5263)	
training:	Epoch: [42][203/204]	Loss 0.3863 (0.5256)	
training:	Epoch: [42][204/204]	Loss 0.4396 (0.5252)	
Training:	 Loss: 0.5244

Training:	 ACC: 0.7534 0.7547 0.7834 0.7235
Validation:	 ACC: 0.7490 0.7507 0.7861 0.7119
Validation:	 Best_BACC: 0.7522 0.7528 0.7646 0.7399
Validation:	 Loss: 0.5212
Pretraining:	Epoch 43/120
----------
training:	Epoch: [43][1/204]	Loss 0.6403 (0.6403)	
training:	Epoch: [43][2/204]	Loss 0.5869 (0.6136)	
training:	Epoch: [43][3/204]	Loss 0.6398 (0.6223)	
training:	Epoch: [43][4/204]	Loss 0.5896 (0.6142)	
training:	Epoch: [43][5/204]	Loss 0.4416 (0.5796)	
training:	Epoch: [43][6/204]	Loss 0.5812 (0.5799)	
training:	Epoch: [43][7/204]	Loss 0.4358 (0.5593)	
training:	Epoch: [43][8/204]	Loss 0.5598 (0.5594)	
training:	Epoch: [43][9/204]	Loss 0.4679 (0.5492)	
training:	Epoch: [43][10/204]	Loss 0.5001 (0.5443)	
training:	Epoch: [43][11/204]	Loss 0.5548 (0.5453)	
training:	Epoch: [43][12/204]	Loss 0.5057 (0.5420)	
training:	Epoch: [43][13/204]	Loss 0.5915 (0.5458)	
training:	Epoch: [43][14/204]	Loss 0.5263 (0.5444)	
training:	Epoch: [43][15/204]	Loss 0.4806 (0.5401)	
training:	Epoch: [43][16/204]	Loss 0.5605 (0.5414)	
training:	Epoch: [43][17/204]	Loss 0.4468 (0.5358)	
training:	Epoch: [43][18/204]	Loss 0.5489 (0.5366)	
training:	Epoch: [43][19/204]	Loss 0.5762 (0.5386)	
training:	Epoch: [43][20/204]	Loss 0.6087 (0.5421)	
training:	Epoch: [43][21/204]	Loss 0.4495 (0.5377)	
training:	Epoch: [43][22/204]	Loss 0.5523 (0.5384)	
training:	Epoch: [43][23/204]	Loss 0.5285 (0.5380)	
training:	Epoch: [43][24/204]	Loss 0.4404 (0.5339)	
training:	Epoch: [43][25/204]	Loss 0.5022 (0.5326)	
training:	Epoch: [43][26/204]	Loss 0.5564 (0.5336)	
training:	Epoch: [43][27/204]	Loss 0.4607 (0.5309)	
training:	Epoch: [43][28/204]	Loss 0.5238 (0.5306)	
training:	Epoch: [43][29/204]	Loss 0.5553 (0.5315)	
training:	Epoch: [43][30/204]	Loss 0.5519 (0.5321)	
training:	Epoch: [43][31/204]	Loss 0.5984 (0.5343)	
training:	Epoch: [43][32/204]	Loss 0.7135 (0.5399)	
training:	Epoch: [43][33/204]	Loss 0.5149 (0.5391)	
training:	Epoch: [43][34/204]	Loss 0.5238 (0.5387)	
training:	Epoch: [43][35/204]	Loss 0.6028 (0.5405)	
training:	Epoch: [43][36/204]	Loss 0.7034 (0.5450)	
training:	Epoch: [43][37/204]	Loss 0.4957 (0.5437)	
training:	Epoch: [43][38/204]	Loss 0.4445 (0.5411)	
training:	Epoch: [43][39/204]	Loss 0.5073 (0.5402)	
training:	Epoch: [43][40/204]	Loss 0.5317 (0.5400)	
training:	Epoch: [43][41/204]	Loss 0.4723 (0.5383)	
training:	Epoch: [43][42/204]	Loss 0.5779 (0.5393)	
training:	Epoch: [43][43/204]	Loss 0.6092 (0.5409)	
training:	Epoch: [43][44/204]	Loss 0.4874 (0.5397)	
training:	Epoch: [43][45/204]	Loss 0.4299 (0.5373)	
training:	Epoch: [43][46/204]	Loss 0.6529 (0.5398)	
training:	Epoch: [43][47/204]	Loss 0.6366 (0.5418)	
training:	Epoch: [43][48/204]	Loss 0.4083 (0.5391)	
training:	Epoch: [43][49/204]	Loss 0.4364 (0.5370)	
training:	Epoch: [43][50/204]	Loss 0.5680 (0.5376)	
training:	Epoch: [43][51/204]	Loss 0.5480 (0.5378)	
training:	Epoch: [43][52/204]	Loss 0.5452 (0.5379)	
training:	Epoch: [43][53/204]	Loss 0.5851 (0.5388)	
training:	Epoch: [43][54/204]	Loss 0.4571 (0.5373)	
training:	Epoch: [43][55/204]	Loss 0.5152 (0.5369)	
training:	Epoch: [43][56/204]	Loss 0.5781 (0.5376)	
training:	Epoch: [43][57/204]	Loss 0.4866 (0.5367)	
training:	Epoch: [43][58/204]	Loss 0.5702 (0.5373)	
training:	Epoch: [43][59/204]	Loss 0.5503 (0.5375)	
training:	Epoch: [43][60/204]	Loss 0.4601 (0.5362)	
training:	Epoch: [43][61/204]	Loss 0.5054 (0.5357)	
training:	Epoch: [43][62/204]	Loss 0.4557 (0.5345)	
training:	Epoch: [43][63/204]	Loss 0.4887 (0.5337)	
training:	Epoch: [43][64/204]	Loss 0.5012 (0.5332)	
training:	Epoch: [43][65/204]	Loss 0.5595 (0.5336)	
training:	Epoch: [43][66/204]	Loss 0.4678 (0.5326)	
training:	Epoch: [43][67/204]	Loss 0.5084 (0.5323)	
training:	Epoch: [43][68/204]	Loss 0.5841 (0.5330)	
training:	Epoch: [43][69/204]	Loss 0.5331 (0.5330)	
training:	Epoch: [43][70/204]	Loss 0.5437 (0.5332)	
training:	Epoch: [43][71/204]	Loss 0.4636 (0.5322)	
training:	Epoch: [43][72/204]	Loss 0.5182 (0.5320)	
training:	Epoch: [43][73/204]	Loss 0.4489 (0.5309)	
training:	Epoch: [43][74/204]	Loss 0.4776 (0.5301)	
training:	Epoch: [43][75/204]	Loss 0.5010 (0.5298)	
training:	Epoch: [43][76/204]	Loss 0.5173 (0.5296)	
training:	Epoch: [43][77/204]	Loss 0.3854 (0.5277)	
training:	Epoch: [43][78/204]	Loss 0.6081 (0.5288)	
training:	Epoch: [43][79/204]	Loss 0.5262 (0.5287)	
training:	Epoch: [43][80/204]	Loss 0.5450 (0.5289)	
training:	Epoch: [43][81/204]	Loss 0.5334 (0.5290)	
training:	Epoch: [43][82/204]	Loss 0.5305 (0.5290)	
training:	Epoch: [43][83/204]	Loss 0.4898 (0.5285)	
training:	Epoch: [43][84/204]	Loss 0.5427 (0.5287)	
training:	Epoch: [43][85/204]	Loss 0.4719 (0.5280)	
training:	Epoch: [43][86/204]	Loss 0.5251 (0.5280)	
training:	Epoch: [43][87/204]	Loss 0.5350 (0.5281)	
training:	Epoch: [43][88/204]	Loss 0.5094 (0.5279)	
training:	Epoch: [43][89/204]	Loss 0.4353 (0.5268)	
training:	Epoch: [43][90/204]	Loss 0.4718 (0.5262)	
training:	Epoch: [43][91/204]	Loss 0.4951 (0.5259)	
training:	Epoch: [43][92/204]	Loss 0.7111 (0.5279)	
training:	Epoch: [43][93/204]	Loss 0.5812 (0.5285)	
training:	Epoch: [43][94/204]	Loss 0.5755 (0.5290)	
training:	Epoch: [43][95/204]	Loss 0.5065 (0.5287)	
training:	Epoch: [43][96/204]	Loss 0.4500 (0.5279)	
training:	Epoch: [43][97/204]	Loss 0.4534 (0.5271)	
training:	Epoch: [43][98/204]	Loss 0.4940 (0.5268)	
training:	Epoch: [43][99/204]	Loss 0.5689 (0.5272)	
training:	Epoch: [43][100/204]	Loss 0.4178 (0.5261)	
training:	Epoch: [43][101/204]	Loss 0.5178 (0.5260)	
training:	Epoch: [43][102/204]	Loss 0.5200 (0.5260)	
training:	Epoch: [43][103/204]	Loss 0.4483 (0.5252)	
training:	Epoch: [43][104/204]	Loss 0.5595 (0.5256)	
training:	Epoch: [43][105/204]	Loss 0.5119 (0.5254)	
training:	Epoch: [43][106/204]	Loss 0.6217 (0.5263)	
training:	Epoch: [43][107/204]	Loss 0.5855 (0.5269)	
training:	Epoch: [43][108/204]	Loss 0.5173 (0.5268)	
training:	Epoch: [43][109/204]	Loss 0.5946 (0.5274)	
training:	Epoch: [43][110/204]	Loss 0.4786 (0.5270)	
training:	Epoch: [43][111/204]	Loss 0.5979 (0.5276)	
training:	Epoch: [43][112/204]	Loss 0.4055 (0.5265)	
training:	Epoch: [43][113/204]	Loss 0.5178 (0.5264)	
training:	Epoch: [43][114/204]	Loss 0.5611 (0.5268)	
training:	Epoch: [43][115/204]	Loss 0.6940 (0.5282)	
training:	Epoch: [43][116/204]	Loss 0.5765 (0.5286)	
training:	Epoch: [43][117/204]	Loss 0.5567 (0.5289)	
training:	Epoch: [43][118/204]	Loss 0.4824 (0.5285)	
training:	Epoch: [43][119/204]	Loss 0.4360 (0.5277)	
training:	Epoch: [43][120/204]	Loss 0.4786 (0.5273)	
training:	Epoch: [43][121/204]	Loss 0.5814 (0.5277)	
training:	Epoch: [43][122/204]	Loss 0.4024 (0.5267)	
training:	Epoch: [43][123/204]	Loss 0.5483 (0.5269)	
training:	Epoch: [43][124/204]	Loss 0.5141 (0.5268)	
training:	Epoch: [43][125/204]	Loss 0.4035 (0.5258)	
training:	Epoch: [43][126/204]	Loss 0.5097 (0.5257)	
training:	Epoch: [43][127/204]	Loss 0.5531 (0.5259)	
training:	Epoch: [43][128/204]	Loss 0.4406 (0.5252)	
training:	Epoch: [43][129/204]	Loss 0.5017 (0.5250)	
training:	Epoch: [43][130/204]	Loss 0.4288 (0.5243)	
training:	Epoch: [43][131/204]	Loss 0.5687 (0.5246)	
training:	Epoch: [43][132/204]	Loss 0.4822 (0.5243)	
training:	Epoch: [43][133/204]	Loss 0.5194 (0.5243)	
training:	Epoch: [43][134/204]	Loss 0.5142 (0.5242)	
training:	Epoch: [43][135/204]	Loss 0.5023 (0.5240)	
training:	Epoch: [43][136/204]	Loss 0.4931 (0.5238)	
training:	Epoch: [43][137/204]	Loss 0.5272 (0.5238)	
training:	Epoch: [43][138/204]	Loss 0.4116 (0.5230)	
training:	Epoch: [43][139/204]	Loss 0.6782 (0.5241)	
training:	Epoch: [43][140/204]	Loss 0.5095 (0.5240)	
training:	Epoch: [43][141/204]	Loss 0.6168 (0.5247)	
training:	Epoch: [43][142/204]	Loss 0.5729 (0.5250)	
training:	Epoch: [43][143/204]	Loss 0.6057 (0.5256)	
training:	Epoch: [43][144/204]	Loss 0.6023 (0.5261)	
training:	Epoch: [43][145/204]	Loss 0.6080 (0.5267)	
training:	Epoch: [43][146/204]	Loss 0.5594 (0.5269)	
training:	Epoch: [43][147/204]	Loss 0.4744 (0.5266)	
training:	Epoch: [43][148/204]	Loss 0.5640 (0.5268)	
training:	Epoch: [43][149/204]	Loss 0.5474 (0.5269)	
training:	Epoch: [43][150/204]	Loss 0.5553 (0.5271)	
training:	Epoch: [43][151/204]	Loss 0.4925 (0.5269)	
training:	Epoch: [43][152/204]	Loss 0.4890 (0.5267)	
training:	Epoch: [43][153/204]	Loss 0.5677 (0.5269)	
training:	Epoch: [43][154/204]	Loss 0.4985 (0.5267)	
training:	Epoch: [43][155/204]	Loss 0.5971 (0.5272)	
training:	Epoch: [43][156/204]	Loss 0.4581 (0.5267)	
training:	Epoch: [43][157/204]	Loss 0.4157 (0.5260)	
training:	Epoch: [43][158/204]	Loss 0.4635 (0.5256)	
training:	Epoch: [43][159/204]	Loss 0.5486 (0.5258)	
training:	Epoch: [43][160/204]	Loss 0.5025 (0.5256)	
training:	Epoch: [43][161/204]	Loss 0.5366 (0.5257)	
training:	Epoch: [43][162/204]	Loss 0.5024 (0.5256)	
training:	Epoch: [43][163/204]	Loss 0.5753 (0.5259)	
training:	Epoch: [43][164/204]	Loss 0.6566 (0.5267)	
training:	Epoch: [43][165/204]	Loss 0.5971 (0.5271)	
training:	Epoch: [43][166/204]	Loss 0.5438 (0.5272)	
training:	Epoch: [43][167/204]	Loss 0.5794 (0.5275)	
training:	Epoch: [43][168/204]	Loss 0.5398 (0.5276)	
training:	Epoch: [43][169/204]	Loss 0.5590 (0.5278)	
training:	Epoch: [43][170/204]	Loss 0.4973 (0.5276)	
training:	Epoch: [43][171/204]	Loss 0.5285 (0.5276)	
training:	Epoch: [43][172/204]	Loss 0.5736 (0.5279)	
training:	Epoch: [43][173/204]	Loss 0.4485 (0.5274)	
training:	Epoch: [43][174/204]	Loss 0.5547 (0.5276)	
training:	Epoch: [43][175/204]	Loss 0.4505 (0.5271)	
training:	Epoch: [43][176/204]	Loss 0.5495 (0.5272)	
training:	Epoch: [43][177/204]	Loss 0.4119 (0.5266)	
training:	Epoch: [43][178/204]	Loss 0.6048 (0.5270)	
training:	Epoch: [43][179/204]	Loss 0.4990 (0.5269)	
training:	Epoch: [43][180/204]	Loss 0.4317 (0.5264)	
training:	Epoch: [43][181/204]	Loss 0.6686 (0.5271)	
training:	Epoch: [43][182/204]	Loss 0.6453 (0.5278)	
training:	Epoch: [43][183/204]	Loss 0.6230 (0.5283)	
training:	Epoch: [43][184/204]	Loss 0.4808 (0.5280)	
training:	Epoch: [43][185/204]	Loss 0.6034 (0.5285)	
training:	Epoch: [43][186/204]	Loss 0.4217 (0.5279)	
training:	Epoch: [43][187/204]	Loss 0.5186 (0.5278)	
training:	Epoch: [43][188/204]	Loss 0.5493 (0.5279)	
training:	Epoch: [43][189/204]	Loss 0.4656 (0.5276)	
training:	Epoch: [43][190/204]	Loss 0.3900 (0.5269)	
training:	Epoch: [43][191/204]	Loss 0.5435 (0.5270)	
training:	Epoch: [43][192/204]	Loss 0.6392 (0.5276)	
training:	Epoch: [43][193/204]	Loss 0.5250 (0.5275)	
training:	Epoch: [43][194/204]	Loss 0.3815 (0.5268)	
training:	Epoch: [43][195/204]	Loss 0.4897 (0.5266)	
training:	Epoch: [43][196/204]	Loss 0.4133 (0.5260)	
training:	Epoch: [43][197/204]	Loss 0.4622 (0.5257)	
training:	Epoch: [43][198/204]	Loss 0.6302 (0.5262)	
training:	Epoch: [43][199/204]	Loss 0.5178 (0.5262)	
training:	Epoch: [43][200/204]	Loss 0.4314 (0.5257)	
training:	Epoch: [43][201/204]	Loss 0.3789 (0.5250)	
training:	Epoch: [43][202/204]	Loss 0.4772 (0.5247)	
training:	Epoch: [43][203/204]	Loss 0.6546 (0.5254)	
training:	Epoch: [43][204/204]	Loss 0.5866 (0.5257)	
Training:	 Loss: 0.5249

Training:	 ACC: 0.7541 0.7545 0.7648 0.7433
Validation:	 ACC: 0.7563 0.7571 0.7728 0.7399
Validation:	 Best_BACC: 0.7563 0.7571 0.7728 0.7399
Validation:	 Loss: 0.5185
Pretraining:	Epoch 44/120
----------
training:	Epoch: [44][1/204]	Loss 0.3797 (0.3797)	
training:	Epoch: [44][2/204]	Loss 0.7208 (0.5503)	
training:	Epoch: [44][3/204]	Loss 0.4340 (0.5115)	
training:	Epoch: [44][4/204]	Loss 0.5138 (0.5121)	
training:	Epoch: [44][5/204]	Loss 0.5963 (0.5289)	
training:	Epoch: [44][6/204]	Loss 0.5710 (0.5359)	
training:	Epoch: [44][7/204]	Loss 0.4485 (0.5234)	
training:	Epoch: [44][8/204]	Loss 0.5523 (0.5271)	
training:	Epoch: [44][9/204]	Loss 0.4215 (0.5153)	
training:	Epoch: [44][10/204]	Loss 0.3684 (0.5006)	
training:	Epoch: [44][11/204]	Loss 0.5706 (0.5070)	
training:	Epoch: [44][12/204]	Loss 0.6290 (0.5172)	
training:	Epoch: [44][13/204]	Loss 0.4681 (0.5134)	
training:	Epoch: [44][14/204]	Loss 0.5730 (0.5177)	
training:	Epoch: [44][15/204]	Loss 0.5080 (0.5170)	
training:	Epoch: [44][16/204]	Loss 0.5409 (0.5185)	
training:	Epoch: [44][17/204]	Loss 0.5786 (0.5220)	
training:	Epoch: [44][18/204]	Loss 0.5108 (0.5214)	
training:	Epoch: [44][19/204]	Loss 0.5819 (0.5246)	
training:	Epoch: [44][20/204]	Loss 0.5321 (0.5250)	
training:	Epoch: [44][21/204]	Loss 0.4385 (0.5209)	
training:	Epoch: [44][22/204]	Loss 0.5367 (0.5216)	
training:	Epoch: [44][23/204]	Loss 0.5098 (0.5211)	
training:	Epoch: [44][24/204]	Loss 0.5380 (0.5218)	
training:	Epoch: [44][25/204]	Loss 0.4679 (0.5196)	
training:	Epoch: [44][26/204]	Loss 0.3986 (0.5150)	
training:	Epoch: [44][27/204]	Loss 0.4794 (0.5136)	
training:	Epoch: [44][28/204]	Loss 0.4846 (0.5126)	
training:	Epoch: [44][29/204]	Loss 0.5847 (0.5151)	
training:	Epoch: [44][30/204]	Loss 0.6199 (0.5186)	
training:	Epoch: [44][31/204]	Loss 0.5845 (0.5207)	
training:	Epoch: [44][32/204]	Loss 0.5784 (0.5225)	
training:	Epoch: [44][33/204]	Loss 0.4327 (0.5198)	
training:	Epoch: [44][34/204]	Loss 0.4675 (0.5183)	
training:	Epoch: [44][35/204]	Loss 0.5748 (0.5199)	
training:	Epoch: [44][36/204]	Loss 0.5488 (0.5207)	
training:	Epoch: [44][37/204]	Loss 0.4780 (0.5195)	
training:	Epoch: [44][38/204]	Loss 0.5928 (0.5215)	
training:	Epoch: [44][39/204]	Loss 0.4439 (0.5195)	
training:	Epoch: [44][40/204]	Loss 0.5098 (0.5192)	
training:	Epoch: [44][41/204]	Loss 0.4293 (0.5170)	
training:	Epoch: [44][42/204]	Loss 0.4383 (0.5152)	
training:	Epoch: [44][43/204]	Loss 0.5812 (0.5167)	
training:	Epoch: [44][44/204]	Loss 0.4540 (0.5153)	
training:	Epoch: [44][45/204]	Loss 0.5393 (0.5158)	
training:	Epoch: [44][46/204]	Loss 0.5003 (0.5155)	
training:	Epoch: [44][47/204]	Loss 0.5135 (0.5154)	
training:	Epoch: [44][48/204]	Loss 0.5715 (0.5166)	
training:	Epoch: [44][49/204]	Loss 0.4134 (0.5145)	
training:	Epoch: [44][50/204]	Loss 0.5860 (0.5159)	
training:	Epoch: [44][51/204]	Loss 0.3712 (0.5131)	
training:	Epoch: [44][52/204]	Loss 0.5060 (0.5129)	
training:	Epoch: [44][53/204]	Loss 0.6382 (0.5153)	
training:	Epoch: [44][54/204]	Loss 0.4621 (0.5143)	
training:	Epoch: [44][55/204]	Loss 0.5772 (0.5155)	
training:	Epoch: [44][56/204]	Loss 0.4744 (0.5147)	
training:	Epoch: [44][57/204]	Loss 0.5075 (0.5146)	
training:	Epoch: [44][58/204]	Loss 0.5396 (0.5150)	
training:	Epoch: [44][59/204]	Loss 0.5895 (0.5163)	
training:	Epoch: [44][60/204]	Loss 0.4424 (0.5151)	
training:	Epoch: [44][61/204]	Loss 0.4904 (0.5147)	
training:	Epoch: [44][62/204]	Loss 0.5934 (0.5159)	
training:	Epoch: [44][63/204]	Loss 0.4481 (0.5149)	
training:	Epoch: [44][64/204]	Loss 0.5363 (0.5152)	
training:	Epoch: [44][65/204]	Loss 0.6108 (0.5167)	
training:	Epoch: [44][66/204]	Loss 0.5345 (0.5169)	
training:	Epoch: [44][67/204]	Loss 0.5236 (0.5170)	
training:	Epoch: [44][68/204]	Loss 0.5317 (0.5172)	
training:	Epoch: [44][69/204]	Loss 0.4401 (0.5161)	
training:	Epoch: [44][70/204]	Loss 0.6085 (0.5174)	
training:	Epoch: [44][71/204]	Loss 0.5167 (0.5174)	
training:	Epoch: [44][72/204]	Loss 0.5222 (0.5175)	
training:	Epoch: [44][73/204]	Loss 0.4993 (0.5173)	
training:	Epoch: [44][74/204]	Loss 0.5527 (0.5177)	
training:	Epoch: [44][75/204]	Loss 0.6083 (0.5189)	
training:	Epoch: [44][76/204]	Loss 0.5351 (0.5192)	
training:	Epoch: [44][77/204]	Loss 0.6220 (0.5205)	
training:	Epoch: [44][78/204]	Loss 0.4858 (0.5200)	
training:	Epoch: [44][79/204]	Loss 0.5656 (0.5206)	
training:	Epoch: [44][80/204]	Loss 0.5507 (0.5210)	
training:	Epoch: [44][81/204]	Loss 0.8172 (0.5247)	
training:	Epoch: [44][82/204]	Loss 0.4877 (0.5242)	
training:	Epoch: [44][83/204]	Loss 0.5800 (0.5249)	
training:	Epoch: [44][84/204]	Loss 0.4458 (0.5239)	
training:	Epoch: [44][85/204]	Loss 0.4516 (0.5231)	
training:	Epoch: [44][86/204]	Loss 0.5244 (0.5231)	
training:	Epoch: [44][87/204]	Loss 0.4750 (0.5225)	
training:	Epoch: [44][88/204]	Loss 0.4613 (0.5218)	
training:	Epoch: [44][89/204]	Loss 0.5901 (0.5226)	
training:	Epoch: [44][90/204]	Loss 0.7576 (0.5252)	
training:	Epoch: [44][91/204]	Loss 0.4601 (0.5245)	
training:	Epoch: [44][92/204]	Loss 0.5339 (0.5246)	
training:	Epoch: [44][93/204]	Loss 0.6190 (0.5256)	
training:	Epoch: [44][94/204]	Loss 0.5849 (0.5263)	
training:	Epoch: [44][95/204]	Loss 0.4666 (0.5256)	
training:	Epoch: [44][96/204]	Loss 0.3637 (0.5239)	
training:	Epoch: [44][97/204]	Loss 0.5780 (0.5245)	
training:	Epoch: [44][98/204]	Loss 0.4109 (0.5233)	
training:	Epoch: [44][99/204]	Loss 0.5149 (0.5233)	
training:	Epoch: [44][100/204]	Loss 0.5294 (0.5233)	
training:	Epoch: [44][101/204]	Loss 0.6552 (0.5246)	
training:	Epoch: [44][102/204]	Loss 0.5526 (0.5249)	
training:	Epoch: [44][103/204]	Loss 0.3906 (0.5236)	
training:	Epoch: [44][104/204]	Loss 0.4985 (0.5234)	
training:	Epoch: [44][105/204]	Loss 0.5665 (0.5238)	
training:	Epoch: [44][106/204]	Loss 0.4637 (0.5232)	
training:	Epoch: [44][107/204]	Loss 0.4939 (0.5229)	
training:	Epoch: [44][108/204]	Loss 0.4639 (0.5224)	
training:	Epoch: [44][109/204]	Loss 0.5457 (0.5226)	
training:	Epoch: [44][110/204]	Loss 0.5028 (0.5224)	
training:	Epoch: [44][111/204]	Loss 0.3986 (0.5213)	
training:	Epoch: [44][112/204]	Loss 0.5871 (0.5219)	
training:	Epoch: [44][113/204]	Loss 0.4832 (0.5215)	
training:	Epoch: [44][114/204]	Loss 0.5645 (0.5219)	
training:	Epoch: [44][115/204]	Loss 0.4906 (0.5216)	
training:	Epoch: [44][116/204]	Loss 0.4718 (0.5212)	
training:	Epoch: [44][117/204]	Loss 0.5038 (0.5211)	
training:	Epoch: [44][118/204]	Loss 0.5892 (0.5216)	
training:	Epoch: [44][119/204]	Loss 0.6770 (0.5230)	
training:	Epoch: [44][120/204]	Loss 0.6672 (0.5242)	
training:	Epoch: [44][121/204]	Loss 0.7160 (0.5257)	
training:	Epoch: [44][122/204]	Loss 0.4302 (0.5250)	
training:	Epoch: [44][123/204]	Loss 0.6248 (0.5258)	
training:	Epoch: [44][124/204]	Loss 0.3673 (0.5245)	
training:	Epoch: [44][125/204]	Loss 0.4814 (0.5241)	
training:	Epoch: [44][126/204]	Loss 0.4502 (0.5236)	
training:	Epoch: [44][127/204]	Loss 0.4436 (0.5229)	
training:	Epoch: [44][128/204]	Loss 0.4779 (0.5226)	
training:	Epoch: [44][129/204]	Loss 0.4753 (0.5222)	
training:	Epoch: [44][130/204]	Loss 0.4067 (0.5213)	
training:	Epoch: [44][131/204]	Loss 0.5311 (0.5214)	
training:	Epoch: [44][132/204]	Loss 0.4743 (0.5210)	
training:	Epoch: [44][133/204]	Loss 0.4443 (0.5205)	
training:	Epoch: [44][134/204]	Loss 0.5224 (0.5205)	
training:	Epoch: [44][135/204]	Loss 0.4547 (0.5200)	
training:	Epoch: [44][136/204]	Loss 0.4878 (0.5198)	
training:	Epoch: [44][137/204]	Loss 0.5452 (0.5199)	
training:	Epoch: [44][138/204]	Loss 0.5287 (0.5200)	
training:	Epoch: [44][139/204]	Loss 0.5798 (0.5204)	
training:	Epoch: [44][140/204]	Loss 0.5555 (0.5207)	
training:	Epoch: [44][141/204]	Loss 0.5367 (0.5208)	
training:	Epoch: [44][142/204]	Loss 0.4951 (0.5206)	
training:	Epoch: [44][143/204]	Loss 0.5129 (0.5206)	
training:	Epoch: [44][144/204]	Loss 0.4825 (0.5203)	
training:	Epoch: [44][145/204]	Loss 0.4077 (0.5195)	
training:	Epoch: [44][146/204]	Loss 0.4914 (0.5193)	
training:	Epoch: [44][147/204]	Loss 0.5088 (0.5193)	
training:	Epoch: [44][148/204]	Loss 0.5934 (0.5198)	
training:	Epoch: [44][149/204]	Loss 0.4531 (0.5193)	
training:	Epoch: [44][150/204]	Loss 0.4720 (0.5190)	
training:	Epoch: [44][151/204]	Loss 0.4796 (0.5187)	
training:	Epoch: [44][152/204]	Loss 0.4411 (0.5182)	
training:	Epoch: [44][153/204]	Loss 0.7146 (0.5195)	
training:	Epoch: [44][154/204]	Loss 0.6030 (0.5200)	
training:	Epoch: [44][155/204]	Loss 0.4662 (0.5197)	
training:	Epoch: [44][156/204]	Loss 0.4330 (0.5191)	
training:	Epoch: [44][157/204]	Loss 0.4461 (0.5187)	
training:	Epoch: [44][158/204]	Loss 0.6014 (0.5192)	
training:	Epoch: [44][159/204]	Loss 0.5087 (0.5191)	
training:	Epoch: [44][160/204]	Loss 0.5711 (0.5195)	
training:	Epoch: [44][161/204]	Loss 0.5587 (0.5197)	
training:	Epoch: [44][162/204]	Loss 0.5671 (0.5200)	
training:	Epoch: [44][163/204]	Loss 0.6111 (0.5206)	
training:	Epoch: [44][164/204]	Loss 0.6127 (0.5211)	
training:	Epoch: [44][165/204]	Loss 0.4721 (0.5208)	
training:	Epoch: [44][166/204]	Loss 0.5007 (0.5207)	
training:	Epoch: [44][167/204]	Loss 0.5053 (0.5206)	
training:	Epoch: [44][168/204]	Loss 0.5624 (0.5209)	
training:	Epoch: [44][169/204]	Loss 0.4280 (0.5203)	
training:	Epoch: [44][170/204]	Loss 0.4365 (0.5198)	
training:	Epoch: [44][171/204]	Loss 0.4735 (0.5195)	
training:	Epoch: [44][172/204]	Loss 0.5959 (0.5200)	
training:	Epoch: [44][173/204]	Loss 0.5078 (0.5199)	
training:	Epoch: [44][174/204]	Loss 0.4874 (0.5197)	
training:	Epoch: [44][175/204]	Loss 0.6183 (0.5203)	
training:	Epoch: [44][176/204]	Loss 0.4685 (0.5200)	
training:	Epoch: [44][177/204]	Loss 0.4918 (0.5198)	
training:	Epoch: [44][178/204]	Loss 0.4856 (0.5196)	
training:	Epoch: [44][179/204]	Loss 0.5173 (0.5196)	
training:	Epoch: [44][180/204]	Loss 0.5987 (0.5201)	
training:	Epoch: [44][181/204]	Loss 0.6081 (0.5206)	
training:	Epoch: [44][182/204]	Loss 0.6086 (0.5210)	
training:	Epoch: [44][183/204]	Loss 0.6499 (0.5217)	
training:	Epoch: [44][184/204]	Loss 0.4322 (0.5213)	
training:	Epoch: [44][185/204]	Loss 0.4323 (0.5208)	
training:	Epoch: [44][186/204]	Loss 0.5607 (0.5210)	
training:	Epoch: [44][187/204]	Loss 0.4534 (0.5206)	
training:	Epoch: [44][188/204]	Loss 0.4952 (0.5205)	
training:	Epoch: [44][189/204]	Loss 0.5598 (0.5207)	
training:	Epoch: [44][190/204]	Loss 0.7121 (0.5217)	
training:	Epoch: [44][191/204]	Loss 0.4897 (0.5215)	
training:	Epoch: [44][192/204]	Loss 0.6406 (0.5222)	
training:	Epoch: [44][193/204]	Loss 0.6094 (0.5226)	
training:	Epoch: [44][194/204]	Loss 0.4845 (0.5224)	
training:	Epoch: [44][195/204]	Loss 0.5126 (0.5224)	
training:	Epoch: [44][196/204]	Loss 0.4916 (0.5222)	
training:	Epoch: [44][197/204]	Loss 0.5615 (0.5224)	
training:	Epoch: [44][198/204]	Loss 0.4330 (0.5220)	
training:	Epoch: [44][199/204]	Loss 0.5265 (0.5220)	
training:	Epoch: [44][200/204]	Loss 0.4108 (0.5214)	
training:	Epoch: [44][201/204]	Loss 0.4926 (0.5213)	
training:	Epoch: [44][202/204]	Loss 0.5273 (0.5213)	
training:	Epoch: [44][203/204]	Loss 0.5339 (0.5214)	
training:	Epoch: [44][204/204]	Loss 0.6478 (0.5220)	
Training:	 Loss: 0.5212

Training:	 ACC: 0.7574 0.7583 0.7807 0.7341
Validation:	 ACC: 0.7503 0.7517 0.7810 0.7197
Validation:	 Best_BACC: 0.7563 0.7571 0.7728 0.7399
Validation:	 Loss: 0.5159
Pretraining:	Epoch 45/120
----------
training:	Epoch: [45][1/204]	Loss 0.4287 (0.4287)	
training:	Epoch: [45][2/204]	Loss 0.7144 (0.5716)	
training:	Epoch: [45][3/204]	Loss 0.5425 (0.5619)	
training:	Epoch: [45][4/204]	Loss 0.4154 (0.5252)	
training:	Epoch: [45][5/204]	Loss 0.4635 (0.5129)	
training:	Epoch: [45][6/204]	Loss 0.4658 (0.5050)	
training:	Epoch: [45][7/204]	Loss 0.4835 (0.5020)	
training:	Epoch: [45][8/204]	Loss 0.5354 (0.5061)	
training:	Epoch: [45][9/204]	Loss 0.4887 (0.5042)	
training:	Epoch: [45][10/204]	Loss 0.6478 (0.5186)	
training:	Epoch: [45][11/204]	Loss 0.5432 (0.5208)	
training:	Epoch: [45][12/204]	Loss 0.6229 (0.5293)	
training:	Epoch: [45][13/204]	Loss 0.5588 (0.5316)	
training:	Epoch: [45][14/204]	Loss 0.6089 (0.5371)	
training:	Epoch: [45][15/204]	Loss 0.6106 (0.5420)	
training:	Epoch: [45][16/204]	Loss 0.4037 (0.5334)	
training:	Epoch: [45][17/204]	Loss 0.5097 (0.5320)	
training:	Epoch: [45][18/204]	Loss 0.6473 (0.5384)	
training:	Epoch: [45][19/204]	Loss 0.4105 (0.5316)	
training:	Epoch: [45][20/204]	Loss 0.5002 (0.5301)	
training:	Epoch: [45][21/204]	Loss 0.5013 (0.5287)	
training:	Epoch: [45][22/204]	Loss 0.5354 (0.5290)	
training:	Epoch: [45][23/204]	Loss 0.4844 (0.5271)	
training:	Epoch: [45][24/204]	Loss 0.5565 (0.5283)	
training:	Epoch: [45][25/204]	Loss 0.6458 (0.5330)	
training:	Epoch: [45][26/204]	Loss 0.4550 (0.5300)	
training:	Epoch: [45][27/204]	Loss 0.5843 (0.5320)	
training:	Epoch: [45][28/204]	Loss 0.3943 (0.5271)	
training:	Epoch: [45][29/204]	Loss 0.5628 (0.5283)	
training:	Epoch: [45][30/204]	Loss 0.5260 (0.5282)	
training:	Epoch: [45][31/204]	Loss 0.4884 (0.5270)	
training:	Epoch: [45][32/204]	Loss 0.5134 (0.5265)	
training:	Epoch: [45][33/204]	Loss 0.4576 (0.5244)	
training:	Epoch: [45][34/204]	Loss 0.4279 (0.5216)	
training:	Epoch: [45][35/204]	Loss 0.4773 (0.5203)	
training:	Epoch: [45][36/204]	Loss 0.5079 (0.5200)	
training:	Epoch: [45][37/204]	Loss 0.6055 (0.5223)	
training:	Epoch: [45][38/204]	Loss 0.6228 (0.5249)	
training:	Epoch: [45][39/204]	Loss 0.5425 (0.5254)	
training:	Epoch: [45][40/204]	Loss 0.4836 (0.5244)	
training:	Epoch: [45][41/204]	Loss 0.4976 (0.5237)	
training:	Epoch: [45][42/204]	Loss 0.3980 (0.5207)	
training:	Epoch: [45][43/204]	Loss 0.5636 (0.5217)	
training:	Epoch: [45][44/204]	Loss 0.5044 (0.5213)	
training:	Epoch: [45][45/204]	Loss 0.4345 (0.5194)	
training:	Epoch: [45][46/204]	Loss 0.4749 (0.5184)	
training:	Epoch: [45][47/204]	Loss 0.5912 (0.5200)	
training:	Epoch: [45][48/204]	Loss 0.4505 (0.5185)	
training:	Epoch: [45][49/204]	Loss 0.4793 (0.5177)	
training:	Epoch: [45][50/204]	Loss 0.5473 (0.5183)	
training:	Epoch: [45][51/204]	Loss 0.6455 (0.5208)	
training:	Epoch: [45][52/204]	Loss 0.4732 (0.5199)	
training:	Epoch: [45][53/204]	Loss 0.5854 (0.5211)	
training:	Epoch: [45][54/204]	Loss 0.5519 (0.5217)	
training:	Epoch: [45][55/204]	Loss 0.4957 (0.5212)	
training:	Epoch: [45][56/204]	Loss 0.5418 (0.5216)	
training:	Epoch: [45][57/204]	Loss 0.6156 (0.5232)	
training:	Epoch: [45][58/204]	Loss 0.4679 (0.5223)	
training:	Epoch: [45][59/204]	Loss 0.5574 (0.5229)	
training:	Epoch: [45][60/204]	Loss 0.5468 (0.5233)	
training:	Epoch: [45][61/204]	Loss 0.5082 (0.5230)	
training:	Epoch: [45][62/204]	Loss 0.4822 (0.5224)	
training:	Epoch: [45][63/204]	Loss 0.4608 (0.5214)	
training:	Epoch: [45][64/204]	Loss 0.4338 (0.5200)	
training:	Epoch: [45][65/204]	Loss 0.4926 (0.5196)	
training:	Epoch: [45][66/204]	Loss 0.5419 (0.5199)	
training:	Epoch: [45][67/204]	Loss 0.5092 (0.5198)	
training:	Epoch: [45][68/204]	Loss 0.6850 (0.5222)	
training:	Epoch: [45][69/204]	Loss 0.4017 (0.5205)	
training:	Epoch: [45][70/204]	Loss 0.6382 (0.5221)	
training:	Epoch: [45][71/204]	Loss 0.5139 (0.5220)	
training:	Epoch: [45][72/204]	Loss 0.4909 (0.5216)	
training:	Epoch: [45][73/204]	Loss 0.5731 (0.5223)	
training:	Epoch: [45][74/204]	Loss 0.5110 (0.5221)	
training:	Epoch: [45][75/204]	Loss 0.4940 (0.5218)	
training:	Epoch: [45][76/204]	Loss 0.4478 (0.5208)	
training:	Epoch: [45][77/204]	Loss 0.4778 (0.5202)	
training:	Epoch: [45][78/204]	Loss 0.4536 (0.5194)	
training:	Epoch: [45][79/204]	Loss 0.4287 (0.5182)	
training:	Epoch: [45][80/204]	Loss 0.5229 (0.5183)	
training:	Epoch: [45][81/204]	Loss 0.4545 (0.5175)	
training:	Epoch: [45][82/204]	Loss 0.4573 (0.5168)	
training:	Epoch: [45][83/204]	Loss 0.6572 (0.5185)	
training:	Epoch: [45][84/204]	Loss 0.4210 (0.5173)	
training:	Epoch: [45][85/204]	Loss 0.3908 (0.5158)	
training:	Epoch: [45][86/204]	Loss 0.5543 (0.5163)	
training:	Epoch: [45][87/204]	Loss 0.6076 (0.5173)	
training:	Epoch: [45][88/204]	Loss 0.6522 (0.5188)	
training:	Epoch: [45][89/204]	Loss 0.5547 (0.5192)	
training:	Epoch: [45][90/204]	Loss 0.5284 (0.5194)	
training:	Epoch: [45][91/204]	Loss 0.4498 (0.5186)	
training:	Epoch: [45][92/204]	Loss 0.5019 (0.5184)	
training:	Epoch: [45][93/204]	Loss 0.5949 (0.5192)	
training:	Epoch: [45][94/204]	Loss 0.5076 (0.5191)	
training:	Epoch: [45][95/204]	Loss 0.4682 (0.5186)	
training:	Epoch: [45][96/204]	Loss 0.7293 (0.5208)	
training:	Epoch: [45][97/204]	Loss 0.4745 (0.5203)	
training:	Epoch: [45][98/204]	Loss 0.4780 (0.5199)	
training:	Epoch: [45][99/204]	Loss 0.6495 (0.5212)	
training:	Epoch: [45][100/204]	Loss 0.5388 (0.5213)	
training:	Epoch: [45][101/204]	Loss 0.5545 (0.5217)	
training:	Epoch: [45][102/204]	Loss 0.4643 (0.5211)	
training:	Epoch: [45][103/204]	Loss 0.4305 (0.5202)	
training:	Epoch: [45][104/204]	Loss 0.4026 (0.5191)	
training:	Epoch: [45][105/204]	Loss 0.4842 (0.5188)	
training:	Epoch: [45][106/204]	Loss 0.4485 (0.5181)	
training:	Epoch: [45][107/204]	Loss 0.4868 (0.5178)	
training:	Epoch: [45][108/204]	Loss 0.4165 (0.5169)	
training:	Epoch: [45][109/204]	Loss 0.4895 (0.5166)	
training:	Epoch: [45][110/204]	Loss 0.5017 (0.5165)	
training:	Epoch: [45][111/204]	Loss 0.7158 (0.5183)	
training:	Epoch: [45][112/204]	Loss 0.5584 (0.5186)	
training:	Epoch: [45][113/204]	Loss 0.5926 (0.5193)	
training:	Epoch: [45][114/204]	Loss 0.5075 (0.5192)	
training:	Epoch: [45][115/204]	Loss 0.6279 (0.5201)	
training:	Epoch: [45][116/204]	Loss 0.4872 (0.5198)	
training:	Epoch: [45][117/204]	Loss 0.6578 (0.5210)	
training:	Epoch: [45][118/204]	Loss 0.4846 (0.5207)	
training:	Epoch: [45][119/204]	Loss 0.6614 (0.5219)	
training:	Epoch: [45][120/204]	Loss 0.4332 (0.5212)	
training:	Epoch: [45][121/204]	Loss 0.5721 (0.5216)	
training:	Epoch: [45][122/204]	Loss 0.6027 (0.5222)	
training:	Epoch: [45][123/204]	Loss 0.4998 (0.5221)	
training:	Epoch: [45][124/204]	Loss 0.4079 (0.5211)	
training:	Epoch: [45][125/204]	Loss 0.6242 (0.5220)	
training:	Epoch: [45][126/204]	Loss 0.4091 (0.5211)	
training:	Epoch: [45][127/204]	Loss 0.3203 (0.5195)	
training:	Epoch: [45][128/204]	Loss 0.4591 (0.5190)	
training:	Epoch: [45][129/204]	Loss 0.5299 (0.5191)	
training:	Epoch: [45][130/204]	Loss 0.6605 (0.5202)	
training:	Epoch: [45][131/204]	Loss 0.5094 (0.5201)	
training:	Epoch: [45][132/204]	Loss 0.5290 (0.5202)	
training:	Epoch: [45][133/204]	Loss 0.4427 (0.5196)	
training:	Epoch: [45][134/204]	Loss 0.5065 (0.5195)	
training:	Epoch: [45][135/204]	Loss 0.4848 (0.5192)	
training:	Epoch: [45][136/204]	Loss 0.5707 (0.5196)	
training:	Epoch: [45][137/204]	Loss 0.7211 (0.5211)	
training:	Epoch: [45][138/204]	Loss 0.4519 (0.5206)	
training:	Epoch: [45][139/204]	Loss 0.4683 (0.5202)	
training:	Epoch: [45][140/204]	Loss 0.5276 (0.5203)	
training:	Epoch: [45][141/204]	Loss 0.5763 (0.5207)	
training:	Epoch: [45][142/204]	Loss 0.4105 (0.5199)	
training:	Epoch: [45][143/204]	Loss 0.5544 (0.5201)	
training:	Epoch: [45][144/204]	Loss 0.4557 (0.5197)	
training:	Epoch: [45][145/204]	Loss 0.4613 (0.5193)	
training:	Epoch: [45][146/204]	Loss 0.4626 (0.5189)	
training:	Epoch: [45][147/204]	Loss 0.5877 (0.5194)	
training:	Epoch: [45][148/204]	Loss 0.5608 (0.5196)	
training:	Epoch: [45][149/204]	Loss 0.4567 (0.5192)	
training:	Epoch: [45][150/204]	Loss 0.5259 (0.5193)	
training:	Epoch: [45][151/204]	Loss 0.3607 (0.5182)	
training:	Epoch: [45][152/204]	Loss 0.6588 (0.5191)	
training:	Epoch: [45][153/204]	Loss 0.5509 (0.5193)	
training:	Epoch: [45][154/204]	Loss 0.4795 (0.5191)	
training:	Epoch: [45][155/204]	Loss 0.3980 (0.5183)	
training:	Epoch: [45][156/204]	Loss 0.6407 (0.5191)	
training:	Epoch: [45][157/204]	Loss 0.5996 (0.5196)	
training:	Epoch: [45][158/204]	Loss 0.4536 (0.5192)	
training:	Epoch: [45][159/204]	Loss 0.4527 (0.5188)	
training:	Epoch: [45][160/204]	Loss 0.5105 (0.5187)	
training:	Epoch: [45][161/204]	Loss 0.5334 (0.5188)	
training:	Epoch: [45][162/204]	Loss 0.4717 (0.5185)	
training:	Epoch: [45][163/204]	Loss 0.6621 (0.5194)	
training:	Epoch: [45][164/204]	Loss 0.4823 (0.5192)	
training:	Epoch: [45][165/204]	Loss 0.4641 (0.5188)	
training:	Epoch: [45][166/204]	Loss 0.4631 (0.5185)	
training:	Epoch: [45][167/204]	Loss 0.5517 (0.5187)	
training:	Epoch: [45][168/204]	Loss 0.4553 (0.5183)	
training:	Epoch: [45][169/204]	Loss 0.4370 (0.5178)	
training:	Epoch: [45][170/204]	Loss 0.4682 (0.5175)	
training:	Epoch: [45][171/204]	Loss 0.5408 (0.5177)	
training:	Epoch: [45][172/204]	Loss 0.4127 (0.5171)	
training:	Epoch: [45][173/204]	Loss 0.4821 (0.5169)	
training:	Epoch: [45][174/204]	Loss 0.4230 (0.5163)	
training:	Epoch: [45][175/204]	Loss 0.4460 (0.5159)	
training:	Epoch: [45][176/204]	Loss 0.6092 (0.5165)	
training:	Epoch: [45][177/204]	Loss 0.4142 (0.5159)	
training:	Epoch: [45][178/204]	Loss 0.5368 (0.5160)	
training:	Epoch: [45][179/204]	Loss 0.5573 (0.5162)	
training:	Epoch: [45][180/204]	Loss 0.5140 (0.5162)	
training:	Epoch: [45][181/204]	Loss 0.6225 (0.5168)	
training:	Epoch: [45][182/204]	Loss 0.5044 (0.5167)	
training:	Epoch: [45][183/204]	Loss 0.4877 (0.5166)	
training:	Epoch: [45][184/204]	Loss 0.5643 (0.5168)	
training:	Epoch: [45][185/204]	Loss 0.4874 (0.5167)	
training:	Epoch: [45][186/204]	Loss 0.4679 (0.5164)	
training:	Epoch: [45][187/204]	Loss 0.5172 (0.5164)	
training:	Epoch: [45][188/204]	Loss 0.4580 (0.5161)	
training:	Epoch: [45][189/204]	Loss 0.5276 (0.5162)	
training:	Epoch: [45][190/204]	Loss 0.4397 (0.5158)	
training:	Epoch: [45][191/204]	Loss 0.5651 (0.5160)	
training:	Epoch: [45][192/204]	Loss 0.5595 (0.5163)	
training:	Epoch: [45][193/204]	Loss 0.6385 (0.5169)	
training:	Epoch: [45][194/204]	Loss 0.4703 (0.5166)	
training:	Epoch: [45][195/204]	Loss 0.5634 (0.5169)	
training:	Epoch: [45][196/204]	Loss 0.6192 (0.5174)	
training:	Epoch: [45][197/204]	Loss 0.7340 (0.5185)	
training:	Epoch: [45][198/204]	Loss 0.3462 (0.5176)	
training:	Epoch: [45][199/204]	Loss 0.4796 (0.5174)	
training:	Epoch: [45][200/204]	Loss 0.4146 (0.5169)	
training:	Epoch: [45][201/204]	Loss 0.5883 (0.5173)	
training:	Epoch: [45][202/204]	Loss 0.4370 (0.5169)	
training:	Epoch: [45][203/204]	Loss 0.6418 (0.5175)	
training:	Epoch: [45][204/204]	Loss 0.6433 (0.5181)	
Training:	 Loss: 0.5173

Training:	 ACC: 0.7587 0.7597 0.7834 0.7341
Validation:	 ACC: 0.7508 0.7523 0.7840 0.7175
Validation:	 Best_BACC: 0.7563 0.7571 0.7728 0.7399
Validation:	 Loss: 0.5136
Pretraining:	Epoch 46/120
----------
training:	Epoch: [46][1/204]	Loss 0.5677 (0.5677)	
training:	Epoch: [46][2/204]	Loss 0.5615 (0.5646)	
training:	Epoch: [46][3/204]	Loss 0.6192 (0.5828)	
training:	Epoch: [46][4/204]	Loss 0.4519 (0.5500)	
training:	Epoch: [46][5/204]	Loss 0.5927 (0.5586)	
training:	Epoch: [46][6/204]	Loss 0.5049 (0.5496)	
training:	Epoch: [46][7/204]	Loss 0.6822 (0.5686)	
training:	Epoch: [46][8/204]	Loss 0.4396 (0.5524)	
training:	Epoch: [46][9/204]	Loss 0.4995 (0.5466)	
training:	Epoch: [46][10/204]	Loss 0.5233 (0.5442)	
training:	Epoch: [46][11/204]	Loss 0.6490 (0.5538)	
training:	Epoch: [46][12/204]	Loss 0.5040 (0.5496)	
training:	Epoch: [46][13/204]	Loss 0.4708 (0.5436)	
training:	Epoch: [46][14/204]	Loss 0.5714 (0.5455)	
training:	Epoch: [46][15/204]	Loss 0.5146 (0.5435)	
training:	Epoch: [46][16/204]	Loss 0.5022 (0.5409)	
training:	Epoch: [46][17/204]	Loss 0.4867 (0.5377)	
training:	Epoch: [46][18/204]	Loss 0.5511 (0.5385)	
training:	Epoch: [46][19/204]	Loss 0.4840 (0.5356)	
training:	Epoch: [46][20/204]	Loss 0.4225 (0.5299)	
training:	Epoch: [46][21/204]	Loss 0.4760 (0.5274)	
training:	Epoch: [46][22/204]	Loss 0.4507 (0.5239)	
training:	Epoch: [46][23/204]	Loss 0.5293 (0.5241)	
training:	Epoch: [46][24/204]	Loss 0.5356 (0.5246)	
training:	Epoch: [46][25/204]	Loss 0.5888 (0.5272)	
training:	Epoch: [46][26/204]	Loss 0.5846 (0.5294)	
training:	Epoch: [46][27/204]	Loss 0.5994 (0.5320)	
training:	Epoch: [46][28/204]	Loss 0.6470 (0.5361)	
training:	Epoch: [46][29/204]	Loss 0.3405 (0.5293)	
training:	Epoch: [46][30/204]	Loss 0.4450 (0.5265)	
training:	Epoch: [46][31/204]	Loss 0.6034 (0.5290)	
training:	Epoch: [46][32/204]	Loss 0.3945 (0.5248)	
training:	Epoch: [46][33/204]	Loss 0.4325 (0.5220)	
training:	Epoch: [46][34/204]	Loss 0.5797 (0.5237)	
training:	Epoch: [46][35/204]	Loss 0.5411 (0.5242)	
training:	Epoch: [46][36/204]	Loss 0.5256 (0.5242)	
training:	Epoch: [46][37/204]	Loss 0.4830 (0.5231)	
training:	Epoch: [46][38/204]	Loss 0.4976 (0.5224)	
training:	Epoch: [46][39/204]	Loss 0.5282 (0.5226)	
training:	Epoch: [46][40/204]	Loss 0.5883 (0.5242)	
training:	Epoch: [46][41/204]	Loss 0.4907 (0.5234)	
training:	Epoch: [46][42/204]	Loss 0.4376 (0.5214)	
training:	Epoch: [46][43/204]	Loss 0.5299 (0.5216)	
training:	Epoch: [46][44/204]	Loss 0.6216 (0.5238)	
training:	Epoch: [46][45/204]	Loss 0.5352 (0.5241)	
training:	Epoch: [46][46/204]	Loss 0.5042 (0.5237)	
training:	Epoch: [46][47/204]	Loss 0.5196 (0.5236)	
training:	Epoch: [46][48/204]	Loss 0.4922 (0.5229)	
training:	Epoch: [46][49/204]	Loss 0.4255 (0.5209)	
training:	Epoch: [46][50/204]	Loss 0.6676 (0.5239)	
training:	Epoch: [46][51/204]	Loss 0.4279 (0.5220)	
training:	Epoch: [46][52/204]	Loss 0.6313 (0.5241)	
training:	Epoch: [46][53/204]	Loss 0.5537 (0.5246)	
training:	Epoch: [46][54/204]	Loss 0.4429 (0.5231)	
training:	Epoch: [46][55/204]	Loss 0.4214 (0.5213)	
training:	Epoch: [46][56/204]	Loss 0.4715 (0.5204)	
training:	Epoch: [46][57/204]	Loss 0.5039 (0.5201)	
training:	Epoch: [46][58/204]	Loss 0.3826 (0.5177)	
training:	Epoch: [46][59/204]	Loss 0.5511 (0.5183)	
training:	Epoch: [46][60/204]	Loss 0.5155 (0.5183)	
training:	Epoch: [46][61/204]	Loss 0.5540 (0.5188)	
training:	Epoch: [46][62/204]	Loss 0.4178 (0.5172)	
training:	Epoch: [46][63/204]	Loss 0.5528 (0.5178)	
training:	Epoch: [46][64/204]	Loss 0.5572 (0.5184)	
training:	Epoch: [46][65/204]	Loss 0.4796 (0.5178)	
training:	Epoch: [46][66/204]	Loss 0.4584 (0.5169)	
training:	Epoch: [46][67/204]	Loss 0.5597 (0.5175)	
training:	Epoch: [46][68/204]	Loss 0.5665 (0.5183)	
training:	Epoch: [46][69/204]	Loss 0.5268 (0.5184)	
training:	Epoch: [46][70/204]	Loss 0.4384 (0.5172)	
training:	Epoch: [46][71/204]	Loss 0.4338 (0.5161)	
training:	Epoch: [46][72/204]	Loss 0.5570 (0.5166)	
training:	Epoch: [46][73/204]	Loss 0.4149 (0.5152)	
training:	Epoch: [46][74/204]	Loss 0.5341 (0.5155)	
training:	Epoch: [46][75/204]	Loss 0.6561 (0.5174)	
training:	Epoch: [46][76/204]	Loss 0.5545 (0.5179)	
training:	Epoch: [46][77/204]	Loss 0.4814 (0.5174)	
training:	Epoch: [46][78/204]	Loss 0.5602 (0.5179)	
training:	Epoch: [46][79/204]	Loss 0.4388 (0.5169)	
training:	Epoch: [46][80/204]	Loss 0.5157 (0.5169)	
training:	Epoch: [46][81/204]	Loss 0.5042 (0.5168)	
training:	Epoch: [46][82/204]	Loss 0.5769 (0.5175)	
training:	Epoch: [46][83/204]	Loss 0.5588 (0.5180)	
training:	Epoch: [46][84/204]	Loss 0.5037 (0.5178)	
training:	Epoch: [46][85/204]	Loss 0.4852 (0.5174)	
training:	Epoch: [46][86/204]	Loss 0.5387 (0.5177)	
training:	Epoch: [46][87/204]	Loss 0.5432 (0.5180)	
training:	Epoch: [46][88/204]	Loss 0.5189 (0.5180)	
training:	Epoch: [46][89/204]	Loss 0.5427 (0.5183)	
training:	Epoch: [46][90/204]	Loss 0.5068 (0.5181)	
training:	Epoch: [46][91/204]	Loss 0.4432 (0.5173)	
training:	Epoch: [46][92/204]	Loss 0.4250 (0.5163)	
training:	Epoch: [46][93/204]	Loss 0.5200 (0.5163)	
training:	Epoch: [46][94/204]	Loss 0.5323 (0.5165)	
training:	Epoch: [46][95/204]	Loss 0.5463 (0.5168)	
training:	Epoch: [46][96/204]	Loss 0.5692 (0.5174)	
training:	Epoch: [46][97/204]	Loss 0.5822 (0.5180)	
training:	Epoch: [46][98/204]	Loss 0.5011 (0.5179)	
training:	Epoch: [46][99/204]	Loss 0.5271 (0.5180)	
training:	Epoch: [46][100/204]	Loss 0.5531 (0.5183)	
training:	Epoch: [46][101/204]	Loss 0.5220 (0.5183)	
training:	Epoch: [46][102/204]	Loss 0.3556 (0.5168)	
training:	Epoch: [46][103/204]	Loss 0.5465 (0.5170)	
training:	Epoch: [46][104/204]	Loss 0.5605 (0.5175)	
training:	Epoch: [46][105/204]	Loss 0.4147 (0.5165)	
training:	Epoch: [46][106/204]	Loss 0.4544 (0.5159)	
training:	Epoch: [46][107/204]	Loss 0.4820 (0.5156)	
training:	Epoch: [46][108/204]	Loss 0.6534 (0.5169)	
training:	Epoch: [46][109/204]	Loss 0.4188 (0.5160)	
training:	Epoch: [46][110/204]	Loss 0.5119 (0.5159)	
training:	Epoch: [46][111/204]	Loss 0.4463 (0.5153)	
training:	Epoch: [46][112/204]	Loss 0.3618 (0.5139)	
training:	Epoch: [46][113/204]	Loss 0.4792 (0.5136)	
training:	Epoch: [46][114/204]	Loss 0.7558 (0.5157)	
training:	Epoch: [46][115/204]	Loss 0.4972 (0.5156)	
training:	Epoch: [46][116/204]	Loss 0.4631 (0.5151)	
training:	Epoch: [46][117/204]	Loss 0.5467 (0.5154)	
training:	Epoch: [46][118/204]	Loss 0.5199 (0.5154)	
training:	Epoch: [46][119/204]	Loss 0.4822 (0.5152)	
training:	Epoch: [46][120/204]	Loss 0.4826 (0.5149)	
training:	Epoch: [46][121/204]	Loss 0.5299 (0.5150)	
training:	Epoch: [46][122/204]	Loss 0.5099 (0.5150)	
training:	Epoch: [46][123/204]	Loss 0.4092 (0.5141)	
training:	Epoch: [46][124/204]	Loss 0.5274 (0.5142)	
training:	Epoch: [46][125/204]	Loss 0.5860 (0.5148)	
training:	Epoch: [46][126/204]	Loss 0.4632 (0.5144)	
training:	Epoch: [46][127/204]	Loss 0.5047 (0.5143)	
training:	Epoch: [46][128/204]	Loss 0.5951 (0.5149)	
training:	Epoch: [46][129/204]	Loss 0.4282 (0.5143)	
training:	Epoch: [46][130/204]	Loss 0.5208 (0.5143)	
training:	Epoch: [46][131/204]	Loss 0.4249 (0.5136)	
training:	Epoch: [46][132/204]	Loss 0.4740 (0.5133)	
training:	Epoch: [46][133/204]	Loss 0.6012 (0.5140)	
training:	Epoch: [46][134/204]	Loss 0.5669 (0.5144)	
training:	Epoch: [46][135/204]	Loss 0.5083 (0.5143)	
training:	Epoch: [46][136/204]	Loss 0.4255 (0.5137)	
training:	Epoch: [46][137/204]	Loss 0.4868 (0.5135)	
training:	Epoch: [46][138/204]	Loss 0.5474 (0.5137)	
training:	Epoch: [46][139/204]	Loss 0.4883 (0.5135)	
training:	Epoch: [46][140/204]	Loss 0.5220 (0.5136)	
training:	Epoch: [46][141/204]	Loss 0.6440 (0.5145)	
training:	Epoch: [46][142/204]	Loss 0.4234 (0.5139)	
training:	Epoch: [46][143/204]	Loss 0.6713 (0.5150)	
training:	Epoch: [46][144/204]	Loss 0.5256 (0.5151)	
training:	Epoch: [46][145/204]	Loss 0.5544 (0.5153)	
training:	Epoch: [46][146/204]	Loss 0.5116 (0.5153)	
training:	Epoch: [46][147/204]	Loss 0.7740 (0.5171)	
training:	Epoch: [46][148/204]	Loss 0.4662 (0.5167)	
training:	Epoch: [46][149/204]	Loss 0.5207 (0.5168)	
training:	Epoch: [46][150/204]	Loss 0.5057 (0.5167)	
training:	Epoch: [46][151/204]	Loss 0.5186 (0.5167)	
training:	Epoch: [46][152/204]	Loss 0.3634 (0.5157)	
training:	Epoch: [46][153/204]	Loss 0.4126 (0.5150)	
training:	Epoch: [46][154/204]	Loss 0.4567 (0.5146)	
training:	Epoch: [46][155/204]	Loss 0.6216 (0.5153)	
training:	Epoch: [46][156/204]	Loss 0.5411 (0.5155)	
training:	Epoch: [46][157/204]	Loss 0.4018 (0.5148)	
training:	Epoch: [46][158/204]	Loss 0.4616 (0.5144)	
training:	Epoch: [46][159/204]	Loss 0.3946 (0.5137)	
training:	Epoch: [46][160/204]	Loss 0.5229 (0.5137)	
training:	Epoch: [46][161/204]	Loss 0.5748 (0.5141)	
training:	Epoch: [46][162/204]	Loss 0.5039 (0.5140)	
training:	Epoch: [46][163/204]	Loss 0.6542 (0.5149)	
training:	Epoch: [46][164/204]	Loss 0.5869 (0.5153)	
training:	Epoch: [46][165/204]	Loss 0.4381 (0.5149)	
training:	Epoch: [46][166/204]	Loss 0.6951 (0.5160)	
training:	Epoch: [46][167/204]	Loss 0.6094 (0.5165)	
training:	Epoch: [46][168/204]	Loss 0.4645 (0.5162)	
training:	Epoch: [46][169/204]	Loss 0.5251 (0.5163)	
training:	Epoch: [46][170/204]	Loss 0.4583 (0.5159)	
training:	Epoch: [46][171/204]	Loss 0.6174 (0.5165)	
training:	Epoch: [46][172/204]	Loss 0.4709 (0.5163)	
training:	Epoch: [46][173/204]	Loss 0.5227 (0.5163)	
training:	Epoch: [46][174/204]	Loss 0.5297 (0.5164)	
training:	Epoch: [46][175/204]	Loss 0.7234 (0.5176)	
training:	Epoch: [46][176/204]	Loss 0.7397 (0.5188)	
training:	Epoch: [46][177/204]	Loss 0.5176 (0.5188)	
training:	Epoch: [46][178/204]	Loss 0.4228 (0.5183)	
training:	Epoch: [46][179/204]	Loss 0.5031 (0.5182)	
training:	Epoch: [46][180/204]	Loss 0.4945 (0.5181)	
training:	Epoch: [46][181/204]	Loss 0.6233 (0.5186)	
training:	Epoch: [46][182/204]	Loss 0.4367 (0.5182)	
training:	Epoch: [46][183/204]	Loss 0.4246 (0.5177)	
training:	Epoch: [46][184/204]	Loss 0.5113 (0.5176)	
training:	Epoch: [46][185/204]	Loss 0.6284 (0.5182)	
training:	Epoch: [46][186/204]	Loss 0.4478 (0.5179)	
training:	Epoch: [46][187/204]	Loss 0.4920 (0.5177)	
training:	Epoch: [46][188/204]	Loss 0.4300 (0.5173)	
training:	Epoch: [46][189/204]	Loss 0.5736 (0.5176)	
training:	Epoch: [46][190/204]	Loss 0.5344 (0.5176)	
training:	Epoch: [46][191/204]	Loss 0.4720 (0.5174)	
training:	Epoch: [46][192/204]	Loss 0.5355 (0.5175)	
training:	Epoch: [46][193/204]	Loss 0.5168 (0.5175)	
training:	Epoch: [46][194/204]	Loss 0.4331 (0.5171)	
training:	Epoch: [46][195/204]	Loss 0.5710 (0.5173)	
training:	Epoch: [46][196/204]	Loss 0.3685 (0.5166)	
training:	Epoch: [46][197/204]	Loss 0.3340 (0.5156)	
training:	Epoch: [46][198/204]	Loss 0.4501 (0.5153)	
training:	Epoch: [46][199/204]	Loss 0.5943 (0.5157)	
training:	Epoch: [46][200/204]	Loss 0.4272 (0.5153)	
training:	Epoch: [46][201/204]	Loss 0.4933 (0.5152)	
training:	Epoch: [46][202/204]	Loss 0.4867 (0.5150)	
training:	Epoch: [46][203/204]	Loss 0.4586 (0.5147)	
training:	Epoch: [46][204/204]	Loss 0.5125 (0.5147)	
Training:	 Loss: 0.5139

Training:	 ACC: 0.7615 0.7623 0.7807 0.7423
Validation:	 ACC: 0.7580 0.7592 0.7840 0.7321
Validation:	 Best_BACC: 0.7580 0.7592 0.7840 0.7321
Validation:	 Loss: 0.5108
Pretraining:	Epoch 47/120
----------
training:	Epoch: [47][1/204]	Loss 0.5221 (0.5221)	
training:	Epoch: [47][2/204]	Loss 0.5516 (0.5369)	
training:	Epoch: [47][3/204]	Loss 0.4006 (0.4914)	
training:	Epoch: [47][4/204]	Loss 0.4185 (0.4732)	
training:	Epoch: [47][5/204]	Loss 0.5493 (0.4884)	
training:	Epoch: [47][6/204]	Loss 0.6322 (0.5124)	
training:	Epoch: [47][7/204]	Loss 0.4765 (0.5073)	
training:	Epoch: [47][8/204]	Loss 0.5510 (0.5127)	
training:	Epoch: [47][9/204]	Loss 0.6111 (0.5237)	
training:	Epoch: [47][10/204]	Loss 0.4180 (0.5131)	
training:	Epoch: [47][11/204]	Loss 0.4612 (0.5084)	
training:	Epoch: [47][12/204]	Loss 0.5988 (0.5159)	
training:	Epoch: [47][13/204]	Loss 0.4598 (0.5116)	
training:	Epoch: [47][14/204]	Loss 0.5162 (0.5119)	
training:	Epoch: [47][15/204]	Loss 0.5955 (0.5175)	
training:	Epoch: [47][16/204]	Loss 0.5030 (0.5166)	
training:	Epoch: [47][17/204]	Loss 0.4912 (0.5151)	
training:	Epoch: [47][18/204]	Loss 0.4839 (0.5134)	
training:	Epoch: [47][19/204]	Loss 0.4232 (0.5086)	
training:	Epoch: [47][20/204]	Loss 0.5090 (0.5086)	
training:	Epoch: [47][21/204]	Loss 0.5671 (0.5114)	
training:	Epoch: [47][22/204]	Loss 0.5548 (0.5134)	
training:	Epoch: [47][23/204]	Loss 0.5271 (0.5140)	
training:	Epoch: [47][24/204]	Loss 0.7472 (0.5237)	
training:	Epoch: [47][25/204]	Loss 0.4175 (0.5195)	
training:	Epoch: [47][26/204]	Loss 0.5935 (0.5223)	
training:	Epoch: [47][27/204]	Loss 0.5053 (0.5217)	
training:	Epoch: [47][28/204]	Loss 0.6089 (0.5248)	
training:	Epoch: [47][29/204]	Loss 0.5233 (0.5247)	
training:	Epoch: [47][30/204]	Loss 0.4481 (0.5222)	
training:	Epoch: [47][31/204]	Loss 0.5706 (0.5237)	
training:	Epoch: [47][32/204]	Loss 0.3968 (0.5198)	
training:	Epoch: [47][33/204]	Loss 0.4538 (0.5178)	
training:	Epoch: [47][34/204]	Loss 0.5102 (0.5176)	
training:	Epoch: [47][35/204]	Loss 0.6325 (0.5208)	
training:	Epoch: [47][36/204]	Loss 0.4537 (0.5190)	
training:	Epoch: [47][37/204]	Loss 0.5292 (0.5192)	
training:	Epoch: [47][38/204]	Loss 0.5772 (0.5208)	
training:	Epoch: [47][39/204]	Loss 0.5559 (0.5217)	
training:	Epoch: [47][40/204]	Loss 0.5433 (0.5222)	
training:	Epoch: [47][41/204]	Loss 0.5715 (0.5234)	
training:	Epoch: [47][42/204]	Loss 0.5142 (0.5232)	
training:	Epoch: [47][43/204]	Loss 0.5684 (0.5243)	
training:	Epoch: [47][44/204]	Loss 0.4809 (0.5233)	
training:	Epoch: [47][45/204]	Loss 0.3992 (0.5205)	
training:	Epoch: [47][46/204]	Loss 0.4385 (0.5187)	
training:	Epoch: [47][47/204]	Loss 0.5545 (0.5195)	
training:	Epoch: [47][48/204]	Loss 0.5037 (0.5192)	
training:	Epoch: [47][49/204]	Loss 0.6354 (0.5215)	
training:	Epoch: [47][50/204]	Loss 0.5699 (0.5225)	
training:	Epoch: [47][51/204]	Loss 0.4738 (0.5215)	
training:	Epoch: [47][52/204]	Loss 0.5385 (0.5219)	
training:	Epoch: [47][53/204]	Loss 0.6308 (0.5239)	
training:	Epoch: [47][54/204]	Loss 0.3966 (0.5216)	
training:	Epoch: [47][55/204]	Loss 0.5082 (0.5213)	
training:	Epoch: [47][56/204]	Loss 0.4594 (0.5202)	
training:	Epoch: [47][57/204]	Loss 0.4608 (0.5192)	
training:	Epoch: [47][58/204]	Loss 0.5225 (0.5192)	
training:	Epoch: [47][59/204]	Loss 0.3811 (0.5169)	
training:	Epoch: [47][60/204]	Loss 0.4711 (0.5161)	
training:	Epoch: [47][61/204]	Loss 0.4860 (0.5156)	
training:	Epoch: [47][62/204]	Loss 0.4335 (0.5143)	
training:	Epoch: [47][63/204]	Loss 0.4775 (0.5137)	
training:	Epoch: [47][64/204]	Loss 0.5533 (0.5143)	
training:	Epoch: [47][65/204]	Loss 0.6183 (0.5159)	
training:	Epoch: [47][66/204]	Loss 0.3563 (0.5135)	
training:	Epoch: [47][67/204]	Loss 0.4130 (0.5120)	
training:	Epoch: [47][68/204]	Loss 0.6008 (0.5133)	
training:	Epoch: [47][69/204]	Loss 0.7385 (0.5166)	
training:	Epoch: [47][70/204]	Loss 0.4225 (0.5152)	
training:	Epoch: [47][71/204]	Loss 0.3752 (0.5133)	
training:	Epoch: [47][72/204]	Loss 0.4648 (0.5126)	
training:	Epoch: [47][73/204]	Loss 0.5604 (0.5133)	
training:	Epoch: [47][74/204]	Loss 0.6011 (0.5144)	
training:	Epoch: [47][75/204]	Loss 0.4864 (0.5141)	
training:	Epoch: [47][76/204]	Loss 0.4455 (0.5132)	
training:	Epoch: [47][77/204]	Loss 0.5158 (0.5132)	
training:	Epoch: [47][78/204]	Loss 0.6502 (0.5150)	
training:	Epoch: [47][79/204]	Loss 0.5636 (0.5156)	
training:	Epoch: [47][80/204]	Loss 0.4792 (0.5151)	
training:	Epoch: [47][81/204]	Loss 0.4423 (0.5142)	
training:	Epoch: [47][82/204]	Loss 0.5890 (0.5151)	
training:	Epoch: [47][83/204]	Loss 0.6134 (0.5163)	
training:	Epoch: [47][84/204]	Loss 0.4678 (0.5157)	
training:	Epoch: [47][85/204]	Loss 0.5462 (0.5161)	
training:	Epoch: [47][86/204]	Loss 0.5274 (0.5162)	
training:	Epoch: [47][87/204]	Loss 0.5247 (0.5163)	
training:	Epoch: [47][88/204]	Loss 0.5150 (0.5163)	
training:	Epoch: [47][89/204]	Loss 0.5153 (0.5163)	
training:	Epoch: [47][90/204]	Loss 0.4865 (0.5160)	
training:	Epoch: [47][91/204]	Loss 0.6449 (0.5174)	
training:	Epoch: [47][92/204]	Loss 0.5870 (0.5181)	
training:	Epoch: [47][93/204]	Loss 0.5919 (0.5189)	
training:	Epoch: [47][94/204]	Loss 0.5252 (0.5190)	
training:	Epoch: [47][95/204]	Loss 0.4818 (0.5186)	
training:	Epoch: [47][96/204]	Loss 0.3970 (0.5173)	
training:	Epoch: [47][97/204]	Loss 0.3830 (0.5160)	
training:	Epoch: [47][98/204]	Loss 0.5955 (0.5168)	
training:	Epoch: [47][99/204]	Loss 0.6780 (0.5184)	
training:	Epoch: [47][100/204]	Loss 0.4579 (0.5178)	
training:	Epoch: [47][101/204]	Loss 0.5779 (0.5184)	
training:	Epoch: [47][102/204]	Loss 0.4669 (0.5179)	
training:	Epoch: [47][103/204]	Loss 0.5430 (0.5181)	
training:	Epoch: [47][104/204]	Loss 0.4887 (0.5178)	
training:	Epoch: [47][105/204]	Loss 0.4634 (0.5173)	
training:	Epoch: [47][106/204]	Loss 0.4682 (0.5169)	
training:	Epoch: [47][107/204]	Loss 0.5071 (0.5168)	
training:	Epoch: [47][108/204]	Loss 0.5298 (0.5169)	
training:	Epoch: [47][109/204]	Loss 0.5209 (0.5169)	
training:	Epoch: [47][110/204]	Loss 0.5758 (0.5175)	
training:	Epoch: [47][111/204]	Loss 0.4816 (0.5171)	
training:	Epoch: [47][112/204]	Loss 0.5857 (0.5178)	
training:	Epoch: [47][113/204]	Loss 0.4740 (0.5174)	
training:	Epoch: [47][114/204]	Loss 0.5487 (0.5176)	
training:	Epoch: [47][115/204]	Loss 0.5483 (0.5179)	
training:	Epoch: [47][116/204]	Loss 0.5358 (0.5181)	
training:	Epoch: [47][117/204]	Loss 0.5398 (0.5182)	
training:	Epoch: [47][118/204]	Loss 0.3573 (0.5169)	
training:	Epoch: [47][119/204]	Loss 0.4396 (0.5162)	
training:	Epoch: [47][120/204]	Loss 0.6729 (0.5175)	
training:	Epoch: [47][121/204]	Loss 0.6882 (0.5190)	
training:	Epoch: [47][122/204]	Loss 0.5527 (0.5192)	
training:	Epoch: [47][123/204]	Loss 0.5423 (0.5194)	
training:	Epoch: [47][124/204]	Loss 0.4531 (0.5189)	
training:	Epoch: [47][125/204]	Loss 0.4238 (0.5181)	
training:	Epoch: [47][126/204]	Loss 0.4695 (0.5177)	
training:	Epoch: [47][127/204]	Loss 0.4982 (0.5176)	
training:	Epoch: [47][128/204]	Loss 0.4930 (0.5174)	
training:	Epoch: [47][129/204]	Loss 0.6157 (0.5182)	
training:	Epoch: [47][130/204]	Loss 0.4729 (0.5178)	
training:	Epoch: [47][131/204]	Loss 0.4101 (0.5170)	
training:	Epoch: [47][132/204]	Loss 0.6327 (0.5179)	
training:	Epoch: [47][133/204]	Loss 0.5368 (0.5180)	
training:	Epoch: [47][134/204]	Loss 0.3571 (0.5168)	
training:	Epoch: [47][135/204]	Loss 0.6180 (0.5176)	
training:	Epoch: [47][136/204]	Loss 0.5104 (0.5175)	
training:	Epoch: [47][137/204]	Loss 0.5193 (0.5175)	
training:	Epoch: [47][138/204]	Loss 0.4414 (0.5170)	
training:	Epoch: [47][139/204]	Loss 0.6315 (0.5178)	
training:	Epoch: [47][140/204]	Loss 0.5447 (0.5180)	
training:	Epoch: [47][141/204]	Loss 0.4563 (0.5175)	
training:	Epoch: [47][142/204]	Loss 0.5048 (0.5174)	
training:	Epoch: [47][143/204]	Loss 0.4785 (0.5172)	
training:	Epoch: [47][144/204]	Loss 0.4013 (0.5164)	
training:	Epoch: [47][145/204]	Loss 0.5566 (0.5166)	
training:	Epoch: [47][146/204]	Loss 0.4657 (0.5163)	
training:	Epoch: [47][147/204]	Loss 0.5994 (0.5169)	
training:	Epoch: [47][148/204]	Loss 0.4305 (0.5163)	
training:	Epoch: [47][149/204]	Loss 0.4408 (0.5158)	
training:	Epoch: [47][150/204]	Loss 0.4350 (0.5152)	
training:	Epoch: [47][151/204]	Loss 0.5371 (0.5154)	
training:	Epoch: [47][152/204]	Loss 0.6680 (0.5164)	
training:	Epoch: [47][153/204]	Loss 0.4223 (0.5158)	
training:	Epoch: [47][154/204]	Loss 0.5540 (0.5160)	
training:	Epoch: [47][155/204]	Loss 0.4834 (0.5158)	
training:	Epoch: [47][156/204]	Loss 0.5002 (0.5157)	
training:	Epoch: [47][157/204]	Loss 0.4112 (0.5150)	
training:	Epoch: [47][158/204]	Loss 0.6099 (0.5156)	
training:	Epoch: [47][159/204]	Loss 0.3974 (0.5149)	
training:	Epoch: [47][160/204]	Loss 0.5658 (0.5152)	
training:	Epoch: [47][161/204]	Loss 0.4422 (0.5148)	
training:	Epoch: [47][162/204]	Loss 0.5702 (0.5151)	
training:	Epoch: [47][163/204]	Loss 0.5371 (0.5152)	
training:	Epoch: [47][164/204]	Loss 0.6397 (0.5160)	
training:	Epoch: [47][165/204]	Loss 0.5878 (0.5164)	
training:	Epoch: [47][166/204]	Loss 0.4442 (0.5160)	
training:	Epoch: [47][167/204]	Loss 0.3492 (0.5150)	
training:	Epoch: [47][168/204]	Loss 0.3988 (0.5143)	
training:	Epoch: [47][169/204]	Loss 0.5422 (0.5145)	
training:	Epoch: [47][170/204]	Loss 0.5861 (0.5149)	
training:	Epoch: [47][171/204]	Loss 0.6459 (0.5157)	
training:	Epoch: [47][172/204]	Loss 0.6474 (0.5164)	
training:	Epoch: [47][173/204]	Loss 0.5091 (0.5164)	
training:	Epoch: [47][174/204]	Loss 0.4971 (0.5163)	
training:	Epoch: [47][175/204]	Loss 0.4858 (0.5161)	
training:	Epoch: [47][176/204]	Loss 0.3895 (0.5154)	
training:	Epoch: [47][177/204]	Loss 0.5158 (0.5154)	
training:	Epoch: [47][178/204]	Loss 0.4198 (0.5148)	
training:	Epoch: [47][179/204]	Loss 0.6472 (0.5156)	
training:	Epoch: [47][180/204]	Loss 0.4466 (0.5152)	
training:	Epoch: [47][181/204]	Loss 0.5135 (0.5152)	
training:	Epoch: [47][182/204]	Loss 0.4765 (0.5150)	
training:	Epoch: [47][183/204]	Loss 0.5572 (0.5152)	
training:	Epoch: [47][184/204]	Loss 0.5135 (0.5152)	
training:	Epoch: [47][185/204]	Loss 0.4769 (0.5150)	
training:	Epoch: [47][186/204]	Loss 0.4390 (0.5146)	
training:	Epoch: [47][187/204]	Loss 0.5103 (0.5146)	
training:	Epoch: [47][188/204]	Loss 0.4152 (0.5140)	
training:	Epoch: [47][189/204]	Loss 0.5109 (0.5140)	
training:	Epoch: [47][190/204]	Loss 0.4785 (0.5138)	
training:	Epoch: [47][191/204]	Loss 0.4688 (0.5136)	
training:	Epoch: [47][192/204]	Loss 0.5200 (0.5136)	
training:	Epoch: [47][193/204]	Loss 0.4552 (0.5133)	
training:	Epoch: [47][194/204]	Loss 0.5622 (0.5136)	
training:	Epoch: [47][195/204]	Loss 0.4522 (0.5133)	
training:	Epoch: [47][196/204]	Loss 0.6066 (0.5137)	
training:	Epoch: [47][197/204]	Loss 0.4869 (0.5136)	
training:	Epoch: [47][198/204]	Loss 0.6090 (0.5141)	
training:	Epoch: [47][199/204]	Loss 0.5442 (0.5142)	
training:	Epoch: [47][200/204]	Loss 0.4927 (0.5141)	
training:	Epoch: [47][201/204]	Loss 0.5745 (0.5144)	
training:	Epoch: [47][202/204]	Loss 0.5126 (0.5144)	
training:	Epoch: [47][203/204]	Loss 0.5999 (0.5148)	
training:	Epoch: [47][204/204]	Loss 0.4678 (0.5146)	
Training:	 Loss: 0.5138

Training:	 ACC: 0.7608 0.7622 0.7934 0.7283
Validation:	 ACC: 0.7564 0.7582 0.7953 0.7175
Validation:	 Best_BACC: 0.7580 0.7592 0.7840 0.7321
Validation:	 Loss: 0.5089
Pretraining:	Epoch 48/120
----------
training:	Epoch: [48][1/204]	Loss 0.5576 (0.5576)	
training:	Epoch: [48][2/204]	Loss 0.5736 (0.5656)	
training:	Epoch: [48][3/204]	Loss 0.5421 (0.5578)	
training:	Epoch: [48][4/204]	Loss 0.5779 (0.5628)	
training:	Epoch: [48][5/204]	Loss 0.5717 (0.5646)	
training:	Epoch: [48][6/204]	Loss 0.5080 (0.5552)	
training:	Epoch: [48][7/204]	Loss 0.5636 (0.5564)	
training:	Epoch: [48][8/204]	Loss 0.5400 (0.5543)	
training:	Epoch: [48][9/204]	Loss 0.5863 (0.5579)	
training:	Epoch: [48][10/204]	Loss 0.4843 (0.5505)	
training:	Epoch: [48][11/204]	Loss 0.4869 (0.5447)	
training:	Epoch: [48][12/204]	Loss 0.4816 (0.5395)	
training:	Epoch: [48][13/204]	Loss 0.5798 (0.5426)	
training:	Epoch: [48][14/204]	Loss 0.4744 (0.5377)	
training:	Epoch: [48][15/204]	Loss 0.4659 (0.5329)	
training:	Epoch: [48][16/204]	Loss 0.4560 (0.5281)	
training:	Epoch: [48][17/204]	Loss 0.4669 (0.5245)	
training:	Epoch: [48][18/204]	Loss 0.5636 (0.5267)	
training:	Epoch: [48][19/204]	Loss 0.5444 (0.5276)	
training:	Epoch: [48][20/204]	Loss 0.6201 (0.5322)	
training:	Epoch: [48][21/204]	Loss 0.5192 (0.5316)	
training:	Epoch: [48][22/204]	Loss 0.5052 (0.5304)	
training:	Epoch: [48][23/204]	Loss 0.4632 (0.5275)	
training:	Epoch: [48][24/204]	Loss 0.6216 (0.5314)	
training:	Epoch: [48][25/204]	Loss 0.4696 (0.5289)	
training:	Epoch: [48][26/204]	Loss 0.3968 (0.5239)	
training:	Epoch: [48][27/204]	Loss 0.4216 (0.5201)	
training:	Epoch: [48][28/204]	Loss 0.4190 (0.5165)	
training:	Epoch: [48][29/204]	Loss 0.6201 (0.5200)	
training:	Epoch: [48][30/204]	Loss 0.4516 (0.5178)	
training:	Epoch: [48][31/204]	Loss 0.5379 (0.5184)	
training:	Epoch: [48][32/204]	Loss 0.3893 (0.5144)	
training:	Epoch: [48][33/204]	Loss 0.4174 (0.5114)	
training:	Epoch: [48][34/204]	Loss 0.5141 (0.5115)	
training:	Epoch: [48][35/204]	Loss 0.4029 (0.5084)	
training:	Epoch: [48][36/204]	Loss 0.5535 (0.5097)	
training:	Epoch: [48][37/204]	Loss 0.3576 (0.5055)	
training:	Epoch: [48][38/204]	Loss 0.4734 (0.5047)	
training:	Epoch: [48][39/204]	Loss 0.4240 (0.5026)	
training:	Epoch: [48][40/204]	Loss 0.5727 (0.5044)	
training:	Epoch: [48][41/204]	Loss 0.4819 (0.5038)	
training:	Epoch: [48][42/204]	Loss 0.4680 (0.5030)	
training:	Epoch: [48][43/204]	Loss 0.3696 (0.4999)	
training:	Epoch: [48][44/204]	Loss 0.6174 (0.5026)	
training:	Epoch: [48][45/204]	Loss 0.5042 (0.5026)	
training:	Epoch: [48][46/204]	Loss 0.4726 (0.5019)	
training:	Epoch: [48][47/204]	Loss 0.6204 (0.5045)	
training:	Epoch: [48][48/204]	Loss 0.6239 (0.5069)	
training:	Epoch: [48][49/204]	Loss 0.5926 (0.5087)	
training:	Epoch: [48][50/204]	Loss 0.5866 (0.5103)	
training:	Epoch: [48][51/204]	Loss 0.6361 (0.5127)	
training:	Epoch: [48][52/204]	Loss 0.4439 (0.5114)	
training:	Epoch: [48][53/204]	Loss 0.5803 (0.5127)	
training:	Epoch: [48][54/204]	Loss 0.5357 (0.5131)	
training:	Epoch: [48][55/204]	Loss 0.4395 (0.5118)	
training:	Epoch: [48][56/204]	Loss 0.7582 (0.5162)	
training:	Epoch: [48][57/204]	Loss 0.5065 (0.5160)	
training:	Epoch: [48][58/204]	Loss 0.3312 (0.5128)	
training:	Epoch: [48][59/204]	Loss 0.4621 (0.5120)	
training:	Epoch: [48][60/204]	Loss 0.4534 (0.5110)	
training:	Epoch: [48][61/204]	Loss 0.3919 (0.5090)	
training:	Epoch: [48][62/204]	Loss 0.5479 (0.5097)	
training:	Epoch: [48][63/204]	Loss 0.3892 (0.5078)	
training:	Epoch: [48][64/204]	Loss 0.5516 (0.5084)	
training:	Epoch: [48][65/204]	Loss 0.4869 (0.5081)	
training:	Epoch: [48][66/204]	Loss 0.5405 (0.5086)	
training:	Epoch: [48][67/204]	Loss 0.5768 (0.5096)	
training:	Epoch: [48][68/204]	Loss 0.5652 (0.5104)	
training:	Epoch: [48][69/204]	Loss 0.5633 (0.5112)	
training:	Epoch: [48][70/204]	Loss 0.4182 (0.5099)	
training:	Epoch: [48][71/204]	Loss 0.6047 (0.5112)	
training:	Epoch: [48][72/204]	Loss 0.4458 (0.5103)	
training:	Epoch: [48][73/204]	Loss 0.5624 (0.5110)	
training:	Epoch: [48][74/204]	Loss 0.4850 (0.5107)	
training:	Epoch: [48][75/204]	Loss 0.3997 (0.5092)	
training:	Epoch: [48][76/204]	Loss 0.5687 (0.5100)	
training:	Epoch: [48][77/204]	Loss 0.4004 (0.5085)	
training:	Epoch: [48][78/204]	Loss 0.4013 (0.5072)	
training:	Epoch: [48][79/204]	Loss 0.5082 (0.5072)	
training:	Epoch: [48][80/204]	Loss 0.4354 (0.5063)	
training:	Epoch: [48][81/204]	Loss 0.4520 (0.5056)	
training:	Epoch: [48][82/204]	Loss 0.4907 (0.5054)	
training:	Epoch: [48][83/204]	Loss 0.6151 (0.5068)	
training:	Epoch: [48][84/204]	Loss 0.7047 (0.5091)	
training:	Epoch: [48][85/204]	Loss 0.5406 (0.5095)	
training:	Epoch: [48][86/204]	Loss 0.5045 (0.5094)	
training:	Epoch: [48][87/204]	Loss 0.3803 (0.5079)	
training:	Epoch: [48][88/204]	Loss 0.5378 (0.5083)	
training:	Epoch: [48][89/204]	Loss 0.4449 (0.5076)	
training:	Epoch: [48][90/204]	Loss 0.4706 (0.5072)	
training:	Epoch: [48][91/204]	Loss 0.4477 (0.5065)	
training:	Epoch: [48][92/204]	Loss 0.4439 (0.5058)	
training:	Epoch: [48][93/204]	Loss 0.5446 (0.5062)	
training:	Epoch: [48][94/204]	Loss 0.5589 (0.5068)	
training:	Epoch: [48][95/204]	Loss 0.6676 (0.5085)	
training:	Epoch: [48][96/204]	Loss 0.4580 (0.5080)	
training:	Epoch: [48][97/204]	Loss 0.4783 (0.5077)	
training:	Epoch: [48][98/204]	Loss 0.6153 (0.5088)	
training:	Epoch: [48][99/204]	Loss 0.4940 (0.5086)	
training:	Epoch: [48][100/204]	Loss 0.5456 (0.5090)	
training:	Epoch: [48][101/204]	Loss 0.4384 (0.5083)	
training:	Epoch: [48][102/204]	Loss 0.5164 (0.5084)	
training:	Epoch: [48][103/204]	Loss 0.5176 (0.5084)	
training:	Epoch: [48][104/204]	Loss 0.5918 (0.5093)	
training:	Epoch: [48][105/204]	Loss 0.4291 (0.5085)	
training:	Epoch: [48][106/204]	Loss 0.3740 (0.5072)	
training:	Epoch: [48][107/204]	Loss 0.4549 (0.5067)	
training:	Epoch: [48][108/204]	Loss 0.6308 (0.5079)	
training:	Epoch: [48][109/204]	Loss 0.4720 (0.5075)	
training:	Epoch: [48][110/204]	Loss 0.4685 (0.5072)	
training:	Epoch: [48][111/204]	Loss 0.5407 (0.5075)	
training:	Epoch: [48][112/204]	Loss 0.5267 (0.5077)	
training:	Epoch: [48][113/204]	Loss 0.3670 (0.5064)	
training:	Epoch: [48][114/204]	Loss 0.5763 (0.5070)	
training:	Epoch: [48][115/204]	Loss 0.6240 (0.5081)	
training:	Epoch: [48][116/204]	Loss 0.5189 (0.5081)	
training:	Epoch: [48][117/204]	Loss 0.4735 (0.5078)	
training:	Epoch: [48][118/204]	Loss 0.5624 (0.5083)	
training:	Epoch: [48][119/204]	Loss 0.4921 (0.5082)	
training:	Epoch: [48][120/204]	Loss 0.4583 (0.5078)	
training:	Epoch: [48][121/204]	Loss 0.4401 (0.5072)	
training:	Epoch: [48][122/204]	Loss 0.5328 (0.5074)	
training:	Epoch: [48][123/204]	Loss 0.6861 (0.5089)	
training:	Epoch: [48][124/204]	Loss 0.3775 (0.5078)	
training:	Epoch: [48][125/204]	Loss 0.6632 (0.5090)	
training:	Epoch: [48][126/204]	Loss 0.5543 (0.5094)	
training:	Epoch: [48][127/204]	Loss 0.4617 (0.5090)	
training:	Epoch: [48][128/204]	Loss 0.5123 (0.5091)	
training:	Epoch: [48][129/204]	Loss 0.5827 (0.5096)	
training:	Epoch: [48][130/204]	Loss 0.6598 (0.5108)	
training:	Epoch: [48][131/204]	Loss 0.5513 (0.5111)	
training:	Epoch: [48][132/204]	Loss 0.5348 (0.5113)	
training:	Epoch: [48][133/204]	Loss 0.4261 (0.5106)	
training:	Epoch: [48][134/204]	Loss 0.5160 (0.5107)	
training:	Epoch: [48][135/204]	Loss 0.3926 (0.5098)	
training:	Epoch: [48][136/204]	Loss 0.3987 (0.5090)	
training:	Epoch: [48][137/204]	Loss 0.4076 (0.5082)	
training:	Epoch: [48][138/204]	Loss 0.6343 (0.5092)	
training:	Epoch: [48][139/204]	Loss 0.7158 (0.5106)	
training:	Epoch: [48][140/204]	Loss 0.6451 (0.5116)	
training:	Epoch: [48][141/204]	Loss 0.5051 (0.5116)	
training:	Epoch: [48][142/204]	Loss 0.6659 (0.5126)	
training:	Epoch: [48][143/204]	Loss 0.6484 (0.5136)	
training:	Epoch: [48][144/204]	Loss 0.6603 (0.5146)	
training:	Epoch: [48][145/204]	Loss 0.3763 (0.5137)	
training:	Epoch: [48][146/204]	Loss 0.3980 (0.5129)	
training:	Epoch: [48][147/204]	Loss 0.5484 (0.5131)	
training:	Epoch: [48][148/204]	Loss 0.5924 (0.5136)	
training:	Epoch: [48][149/204]	Loss 0.4431 (0.5132)	
training:	Epoch: [48][150/204]	Loss 0.5601 (0.5135)	
training:	Epoch: [48][151/204]	Loss 0.3911 (0.5127)	
training:	Epoch: [48][152/204]	Loss 0.5167 (0.5127)	
training:	Epoch: [48][153/204]	Loss 0.6090 (0.5133)	
training:	Epoch: [48][154/204]	Loss 0.4637 (0.5130)	
training:	Epoch: [48][155/204]	Loss 0.5598 (0.5133)	
training:	Epoch: [48][156/204]	Loss 0.5391 (0.5135)	
training:	Epoch: [48][157/204]	Loss 0.4576 (0.5131)	
training:	Epoch: [48][158/204]	Loss 0.3198 (0.5119)	
training:	Epoch: [48][159/204]	Loss 0.4153 (0.5113)	
training:	Epoch: [48][160/204]	Loss 0.4152 (0.5107)	
training:	Epoch: [48][161/204]	Loss 0.4083 (0.5100)	
training:	Epoch: [48][162/204]	Loss 0.5173 (0.5101)	
training:	Epoch: [48][163/204]	Loss 0.5295 (0.5102)	
training:	Epoch: [48][164/204]	Loss 0.4483 (0.5098)	
training:	Epoch: [48][165/204]	Loss 0.4721 (0.5096)	
training:	Epoch: [48][166/204]	Loss 0.5996 (0.5101)	
training:	Epoch: [48][167/204]	Loss 0.5326 (0.5103)	
training:	Epoch: [48][168/204]	Loss 0.5388 (0.5105)	
training:	Epoch: [48][169/204]	Loss 0.3866 (0.5097)	
training:	Epoch: [48][170/204]	Loss 0.5152 (0.5098)	
training:	Epoch: [48][171/204]	Loss 0.6182 (0.5104)	
training:	Epoch: [48][172/204]	Loss 0.4535 (0.5101)	
training:	Epoch: [48][173/204]	Loss 0.6851 (0.5111)	
training:	Epoch: [48][174/204]	Loss 0.5021 (0.5110)	
training:	Epoch: [48][175/204]	Loss 0.5051 (0.5110)	
training:	Epoch: [48][176/204]	Loss 0.4265 (0.5105)	
training:	Epoch: [48][177/204]	Loss 0.4412 (0.5101)	
training:	Epoch: [48][178/204]	Loss 0.5576 (0.5104)	
training:	Epoch: [48][179/204]	Loss 0.4826 (0.5102)	
training:	Epoch: [48][180/204]	Loss 0.4800 (0.5101)	
training:	Epoch: [48][181/204]	Loss 0.6996 (0.5111)	
training:	Epoch: [48][182/204]	Loss 0.4794 (0.5109)	
training:	Epoch: [48][183/204]	Loss 0.5114 (0.5109)	
training:	Epoch: [48][184/204]	Loss 0.6033 (0.5114)	
training:	Epoch: [48][185/204]	Loss 0.5279 (0.5115)	
training:	Epoch: [48][186/204]	Loss 0.4457 (0.5112)	
training:	Epoch: [48][187/204]	Loss 0.4874 (0.5110)	
training:	Epoch: [48][188/204]	Loss 0.5018 (0.5110)	
training:	Epoch: [48][189/204]	Loss 0.4941 (0.5109)	
training:	Epoch: [48][190/204]	Loss 0.4153 (0.5104)	
training:	Epoch: [48][191/204]	Loss 0.4609 (0.5101)	
training:	Epoch: [48][192/204]	Loss 0.5375 (0.5103)	
training:	Epoch: [48][193/204]	Loss 0.4079 (0.5098)	
training:	Epoch: [48][194/204]	Loss 0.4469 (0.5094)	
training:	Epoch: [48][195/204]	Loss 0.4710 (0.5092)	
training:	Epoch: [48][196/204]	Loss 0.5194 (0.5093)	
training:	Epoch: [48][197/204]	Loss 0.4038 (0.5087)	
training:	Epoch: [48][198/204]	Loss 0.5092 (0.5087)	
training:	Epoch: [48][199/204]	Loss 0.6134 (0.5093)	
training:	Epoch: [48][200/204]	Loss 0.5371 (0.5094)	
training:	Epoch: [48][201/204]	Loss 0.5030 (0.5094)	
training:	Epoch: [48][202/204]	Loss 0.6031 (0.5098)	
training:	Epoch: [48][203/204]	Loss 0.5247 (0.5099)	
training:	Epoch: [48][204/204]	Loss 0.5289 (0.5100)	
Training:	 Loss: 0.5092

Training:	 ACC: 0.7655 0.7661 0.7810 0.7500
Validation:	 ACC: 0.7609 0.7619 0.7840 0.7377
Validation:	 Best_BACC: 0.7609 0.7619 0.7840 0.7377
Validation:	 Loss: 0.5067
Pretraining:	Epoch 49/120
----------
training:	Epoch: [49][1/204]	Loss 0.5062 (0.5062)	
training:	Epoch: [49][2/204]	Loss 0.3892 (0.4477)	
training:	Epoch: [49][3/204]	Loss 0.3388 (0.4114)	
training:	Epoch: [49][4/204]	Loss 0.5002 (0.4336)	
training:	Epoch: [49][5/204]	Loss 0.4418 (0.4352)	
training:	Epoch: [49][6/204]	Loss 0.4788 (0.4425)	
training:	Epoch: [49][7/204]	Loss 0.6227 (0.4682)	
training:	Epoch: [49][8/204]	Loss 0.4856 (0.4704)	
training:	Epoch: [49][9/204]	Loss 0.5350 (0.4776)	
training:	Epoch: [49][10/204]	Loss 0.5252 (0.4823)	
training:	Epoch: [49][11/204]	Loss 0.6036 (0.4934)	
training:	Epoch: [49][12/204]	Loss 0.5340 (0.4967)	
training:	Epoch: [49][13/204]	Loss 0.6324 (0.5072)	
training:	Epoch: [49][14/204]	Loss 0.4613 (0.5039)	
training:	Epoch: [49][15/204]	Loss 0.4563 (0.5007)	
training:	Epoch: [49][16/204]	Loss 0.6502 (0.5101)	
training:	Epoch: [49][17/204]	Loss 0.7434 (0.5238)	
training:	Epoch: [49][18/204]	Loss 0.6058 (0.5283)	
training:	Epoch: [49][19/204]	Loss 0.4172 (0.5225)	
training:	Epoch: [49][20/204]	Loss 0.5363 (0.5232)	
training:	Epoch: [49][21/204]	Loss 0.5192 (0.5230)	
training:	Epoch: [49][22/204]	Loss 0.6304 (0.5279)	
training:	Epoch: [49][23/204]	Loss 0.5211 (0.5276)	
training:	Epoch: [49][24/204]	Loss 0.4808 (0.5256)	
training:	Epoch: [49][25/204]	Loss 0.5023 (0.5247)	
training:	Epoch: [49][26/204]	Loss 0.3484 (0.5179)	
training:	Epoch: [49][27/204]	Loss 0.4026 (0.5136)	
training:	Epoch: [49][28/204]	Loss 0.7151 (0.5208)	
training:	Epoch: [49][29/204]	Loss 0.4840 (0.5196)	
training:	Epoch: [49][30/204]	Loss 0.4377 (0.5168)	
training:	Epoch: [49][31/204]	Loss 0.6440 (0.5209)	
training:	Epoch: [49][32/204]	Loss 0.4459 (0.5186)	
training:	Epoch: [49][33/204]	Loss 0.4174 (0.5155)	
training:	Epoch: [49][34/204]	Loss 0.5709 (0.5172)	
training:	Epoch: [49][35/204]	Loss 0.6231 (0.5202)	
training:	Epoch: [49][36/204]	Loss 0.5399 (0.5207)	
training:	Epoch: [49][37/204]	Loss 0.5061 (0.5203)	
training:	Epoch: [49][38/204]	Loss 0.4444 (0.5183)	
training:	Epoch: [49][39/204]	Loss 0.4727 (0.5172)	
training:	Epoch: [49][40/204]	Loss 0.4879 (0.5164)	
training:	Epoch: [49][41/204]	Loss 0.5922 (0.5183)	
training:	Epoch: [49][42/204]	Loss 0.5106 (0.5181)	
training:	Epoch: [49][43/204]	Loss 0.4926 (0.5175)	
training:	Epoch: [49][44/204]	Loss 0.3728 (0.5142)	
training:	Epoch: [49][45/204]	Loss 0.7074 (0.5185)	
training:	Epoch: [49][46/204]	Loss 0.4585 (0.5172)	
training:	Epoch: [49][47/204]	Loss 0.5608 (0.5181)	
training:	Epoch: [49][48/204]	Loss 0.5757 (0.5193)	
training:	Epoch: [49][49/204]	Loss 0.5421 (0.5198)	
training:	Epoch: [49][50/204]	Loss 0.6174 (0.5218)	
training:	Epoch: [49][51/204]	Loss 0.5174 (0.5217)	
training:	Epoch: [49][52/204]	Loss 0.5583 (0.5224)	
training:	Epoch: [49][53/204]	Loss 0.4872 (0.5217)	
training:	Epoch: [49][54/204]	Loss 0.4314 (0.5200)	
training:	Epoch: [49][55/204]	Loss 0.5647 (0.5209)	
training:	Epoch: [49][56/204]	Loss 0.4210 (0.5191)	
training:	Epoch: [49][57/204]	Loss 0.4086 (0.5171)	
training:	Epoch: [49][58/204]	Loss 0.4114 (0.5153)	
training:	Epoch: [49][59/204]	Loss 0.6228 (0.5171)	
training:	Epoch: [49][60/204]	Loss 0.4761 (0.5164)	
training:	Epoch: [49][61/204]	Loss 0.3661 (0.5140)	
training:	Epoch: [49][62/204]	Loss 0.5400 (0.5144)	
training:	Epoch: [49][63/204]	Loss 0.4478 (0.5133)	
training:	Epoch: [49][64/204]	Loss 0.4163 (0.5118)	
training:	Epoch: [49][65/204]	Loss 0.4217 (0.5104)	
training:	Epoch: [49][66/204]	Loss 0.5479 (0.5110)	
training:	Epoch: [49][67/204]	Loss 0.6205 (0.5126)	
training:	Epoch: [49][68/204]	Loss 0.3802 (0.5107)	
training:	Epoch: [49][69/204]	Loss 0.4670 (0.5101)	
training:	Epoch: [49][70/204]	Loss 0.4986 (0.5099)	
training:	Epoch: [49][71/204]	Loss 0.5730 (0.5108)	
training:	Epoch: [49][72/204]	Loss 0.5416 (0.5112)	
training:	Epoch: [49][73/204]	Loss 0.3859 (0.5095)	
training:	Epoch: [49][74/204]	Loss 0.5784 (0.5104)	
training:	Epoch: [49][75/204]	Loss 0.4563 (0.5097)	
training:	Epoch: [49][76/204]	Loss 0.4205 (0.5085)	
training:	Epoch: [49][77/204]	Loss 0.5045 (0.5085)	
training:	Epoch: [49][78/204]	Loss 0.5010 (0.5084)	
training:	Epoch: [49][79/204]	Loss 0.4813 (0.5080)	
training:	Epoch: [49][80/204]	Loss 0.5847 (0.5090)	
training:	Epoch: [49][81/204]	Loss 0.5520 (0.5095)	
training:	Epoch: [49][82/204]	Loss 0.4297 (0.5086)	
training:	Epoch: [49][83/204]	Loss 0.5452 (0.5090)	
training:	Epoch: [49][84/204]	Loss 0.4601 (0.5084)	
training:	Epoch: [49][85/204]	Loss 0.4769 (0.5080)	
training:	Epoch: [49][86/204]	Loss 0.4557 (0.5074)	
training:	Epoch: [49][87/204]	Loss 0.6210 (0.5087)	
training:	Epoch: [49][88/204]	Loss 0.6345 (0.5102)	
training:	Epoch: [49][89/204]	Loss 0.4877 (0.5099)	
training:	Epoch: [49][90/204]	Loss 0.5417 (0.5103)	
training:	Epoch: [49][91/204]	Loss 0.5552 (0.5108)	
training:	Epoch: [49][92/204]	Loss 0.4031 (0.5096)	
training:	Epoch: [49][93/204]	Loss 0.6359 (0.5110)	
training:	Epoch: [49][94/204]	Loss 0.4534 (0.5103)	
training:	Epoch: [49][95/204]	Loss 0.4106 (0.5093)	
training:	Epoch: [49][96/204]	Loss 0.5074 (0.5093)	
training:	Epoch: [49][97/204]	Loss 0.5156 (0.5093)	
training:	Epoch: [49][98/204]	Loss 0.4926 (0.5092)	
training:	Epoch: [49][99/204]	Loss 0.5229 (0.5093)	
training:	Epoch: [49][100/204]	Loss 0.4987 (0.5092)	
training:	Epoch: [49][101/204]	Loss 0.5163 (0.5093)	
training:	Epoch: [49][102/204]	Loss 0.4425 (0.5086)	
training:	Epoch: [49][103/204]	Loss 0.5357 (0.5089)	
training:	Epoch: [49][104/204]	Loss 0.4401 (0.5082)	
training:	Epoch: [49][105/204]	Loss 0.4546 (0.5077)	
training:	Epoch: [49][106/204]	Loss 0.5405 (0.5080)	
training:	Epoch: [49][107/204]	Loss 0.4339 (0.5073)	
training:	Epoch: [49][108/204]	Loss 0.4785 (0.5071)	
training:	Epoch: [49][109/204]	Loss 0.4127 (0.5062)	
training:	Epoch: [49][110/204]	Loss 0.4619 (0.5058)	
training:	Epoch: [49][111/204]	Loss 0.4459 (0.5052)	
training:	Epoch: [49][112/204]	Loss 0.4314 (0.5046)	
training:	Epoch: [49][113/204]	Loss 0.5917 (0.5054)	
training:	Epoch: [49][114/204]	Loss 0.6388 (0.5065)	
training:	Epoch: [49][115/204]	Loss 0.4532 (0.5061)	
training:	Epoch: [49][116/204]	Loss 0.4256 (0.5054)	
training:	Epoch: [49][117/204]	Loss 0.5504 (0.5058)	
training:	Epoch: [49][118/204]	Loss 0.5320 (0.5060)	
training:	Epoch: [49][119/204]	Loss 0.5814 (0.5066)	
training:	Epoch: [49][120/204]	Loss 0.4104 (0.5058)	
training:	Epoch: [49][121/204]	Loss 0.4425 (0.5053)	
training:	Epoch: [49][122/204]	Loss 0.5667 (0.5058)	
training:	Epoch: [49][123/204]	Loss 0.3224 (0.5043)	
training:	Epoch: [49][124/204]	Loss 0.4678 (0.5040)	
training:	Epoch: [49][125/204]	Loss 0.3423 (0.5027)	
training:	Epoch: [49][126/204]	Loss 0.5160 (0.5028)	
training:	Epoch: [49][127/204]	Loss 0.5162 (0.5029)	
training:	Epoch: [49][128/204]	Loss 0.4553 (0.5025)	
training:	Epoch: [49][129/204]	Loss 0.5894 (0.5032)	
training:	Epoch: [49][130/204]	Loss 0.5383 (0.5035)	
training:	Epoch: [49][131/204]	Loss 0.6378 (0.5045)	
training:	Epoch: [49][132/204]	Loss 0.4565 (0.5042)	
training:	Epoch: [49][133/204]	Loss 0.5948 (0.5048)	
training:	Epoch: [49][134/204]	Loss 0.5061 (0.5048)	
training:	Epoch: [49][135/204]	Loss 0.4890 (0.5047)	
training:	Epoch: [49][136/204]	Loss 0.5745 (0.5052)	
training:	Epoch: [49][137/204]	Loss 0.4530 (0.5049)	
training:	Epoch: [49][138/204]	Loss 0.4736 (0.5046)	
training:	Epoch: [49][139/204]	Loss 0.5688 (0.5051)	
training:	Epoch: [49][140/204]	Loss 0.4041 (0.5044)	
training:	Epoch: [49][141/204]	Loss 0.5120 (0.5044)	
training:	Epoch: [49][142/204]	Loss 0.4543 (0.5041)	
training:	Epoch: [49][143/204]	Loss 0.4567 (0.5037)	
training:	Epoch: [49][144/204]	Loss 0.4814 (0.5036)	
training:	Epoch: [49][145/204]	Loss 0.5951 (0.5042)	
training:	Epoch: [49][146/204]	Loss 0.5061 (0.5042)	
training:	Epoch: [49][147/204]	Loss 0.5794 (0.5047)	
training:	Epoch: [49][148/204]	Loss 0.3706 (0.5038)	
training:	Epoch: [49][149/204]	Loss 0.5054 (0.5038)	
training:	Epoch: [49][150/204]	Loss 0.4114 (0.5032)	
training:	Epoch: [49][151/204]	Loss 0.4138 (0.5026)	
training:	Epoch: [49][152/204]	Loss 0.5702 (0.5031)	
training:	Epoch: [49][153/204]	Loss 0.4119 (0.5025)	
training:	Epoch: [49][154/204]	Loss 0.6130 (0.5032)	
training:	Epoch: [49][155/204]	Loss 0.4656 (0.5030)	
training:	Epoch: [49][156/204]	Loss 0.4787 (0.5028)	
training:	Epoch: [49][157/204]	Loss 0.5713 (0.5032)	
training:	Epoch: [49][158/204]	Loss 0.5155 (0.5033)	
training:	Epoch: [49][159/204]	Loss 0.6176 (0.5040)	
training:	Epoch: [49][160/204]	Loss 0.4871 (0.5039)	
training:	Epoch: [49][161/204]	Loss 0.4015 (0.5033)	
training:	Epoch: [49][162/204]	Loss 0.4619 (0.5030)	
training:	Epoch: [49][163/204]	Loss 0.5524 (0.5033)	
training:	Epoch: [49][164/204]	Loss 0.4797 (0.5032)	
training:	Epoch: [49][165/204]	Loss 0.4976 (0.5032)	
training:	Epoch: [49][166/204]	Loss 0.6259 (0.5039)	
training:	Epoch: [49][167/204]	Loss 0.6054 (0.5045)	
training:	Epoch: [49][168/204]	Loss 0.4498 (0.5042)	
training:	Epoch: [49][169/204]	Loss 0.4767 (0.5040)	
training:	Epoch: [49][170/204]	Loss 0.4234 (0.5036)	
training:	Epoch: [49][171/204]	Loss 0.4473 (0.5032)	
training:	Epoch: [49][172/204]	Loss 0.6538 (0.5041)	
training:	Epoch: [49][173/204]	Loss 0.6413 (0.5049)	
training:	Epoch: [49][174/204]	Loss 0.5481 (0.5051)	
training:	Epoch: [49][175/204]	Loss 0.5512 (0.5054)	
training:	Epoch: [49][176/204]	Loss 0.5953 (0.5059)	
training:	Epoch: [49][177/204]	Loss 0.4131 (0.5054)	
training:	Epoch: [49][178/204]	Loss 0.4872 (0.5053)	
training:	Epoch: [49][179/204]	Loss 0.5034 (0.5053)	
training:	Epoch: [49][180/204]	Loss 0.5375 (0.5055)	
training:	Epoch: [49][181/204]	Loss 0.5939 (0.5059)	
training:	Epoch: [49][182/204]	Loss 0.4308 (0.5055)	
training:	Epoch: [49][183/204]	Loss 0.4331 (0.5051)	
training:	Epoch: [49][184/204]	Loss 0.4993 (0.5051)	
training:	Epoch: [49][185/204]	Loss 0.4773 (0.5050)	
training:	Epoch: [49][186/204]	Loss 0.5726 (0.5053)	
training:	Epoch: [49][187/204]	Loss 0.4151 (0.5048)	
training:	Epoch: [49][188/204]	Loss 0.4998 (0.5048)	
training:	Epoch: [49][189/204]	Loss 0.6200 (0.5054)	
training:	Epoch: [49][190/204]	Loss 0.6177 (0.5060)	
training:	Epoch: [49][191/204]	Loss 0.4049 (0.5055)	
training:	Epoch: [49][192/204]	Loss 0.6071 (0.5060)	
training:	Epoch: [49][193/204]	Loss 0.4551 (0.5057)	
training:	Epoch: [49][194/204]	Loss 0.4302 (0.5054)	
training:	Epoch: [49][195/204]	Loss 0.6150 (0.5059)	
training:	Epoch: [49][196/204]	Loss 0.4530 (0.5056)	
training:	Epoch: [49][197/204]	Loss 0.5633 (0.5059)	
training:	Epoch: [49][198/204]	Loss 0.5098 (0.5060)	
training:	Epoch: [49][199/204]	Loss 0.4564 (0.5057)	
training:	Epoch: [49][200/204]	Loss 0.6154 (0.5063)	
training:	Epoch: [49][201/204]	Loss 0.5476 (0.5065)	
training:	Epoch: [49][202/204]	Loss 0.5265 (0.5066)	
training:	Epoch: [49][203/204]	Loss 0.5047 (0.5066)	
training:	Epoch: [49][204/204]	Loss 0.5490 (0.5068)	
Training:	 Loss: 0.5060

Training:	 ACC: 0.7656 0.7663 0.7819 0.7494
Validation:	 ACC: 0.7624 0.7635 0.7871 0.7377
Validation:	 Best_BACC: 0.7624 0.7635 0.7871 0.7377
Validation:	 Loss: 0.5042
Pretraining:	Epoch 50/120
----------
training:	Epoch: [50][1/204]	Loss 0.4821 (0.4821)	
training:	Epoch: [50][2/204]	Loss 0.3912 (0.4366)	
training:	Epoch: [50][3/204]	Loss 0.5682 (0.4805)	
training:	Epoch: [50][4/204]	Loss 0.4407 (0.4705)	
training:	Epoch: [50][5/204]	Loss 0.4659 (0.4696)	
training:	Epoch: [50][6/204]	Loss 0.4370 (0.4642)	
training:	Epoch: [50][7/204]	Loss 0.5161 (0.4716)	
training:	Epoch: [50][8/204]	Loss 0.5537 (0.4818)	
training:	Epoch: [50][9/204]	Loss 0.6316 (0.4985)	
training:	Epoch: [50][10/204]	Loss 0.5153 (0.5002)	
training:	Epoch: [50][11/204]	Loss 0.3437 (0.4859)	
training:	Epoch: [50][12/204]	Loss 0.5457 (0.4909)	
training:	Epoch: [50][13/204]	Loss 0.5831 (0.4980)	
training:	Epoch: [50][14/204]	Loss 0.5372 (0.5008)	
training:	Epoch: [50][15/204]	Loss 0.3827 (0.4929)	
training:	Epoch: [50][16/204]	Loss 0.4860 (0.4925)	
training:	Epoch: [50][17/204]	Loss 0.4996 (0.4929)	
training:	Epoch: [50][18/204]	Loss 0.7208 (0.5056)	
training:	Epoch: [50][19/204]	Loss 0.5691 (0.5089)	
training:	Epoch: [50][20/204]	Loss 0.5587 (0.5114)	
training:	Epoch: [50][21/204]	Loss 0.4694 (0.5094)	
training:	Epoch: [50][22/204]	Loss 0.3965 (0.5043)	
training:	Epoch: [50][23/204]	Loss 0.5005 (0.5041)	
training:	Epoch: [50][24/204]	Loss 0.4194 (0.5006)	
training:	Epoch: [50][25/204]	Loss 0.5828 (0.5039)	
training:	Epoch: [50][26/204]	Loss 0.5389 (0.5052)	
training:	Epoch: [50][27/204]	Loss 0.4904 (0.5047)	
training:	Epoch: [50][28/204]	Loss 0.5528 (0.5064)	
training:	Epoch: [50][29/204]	Loss 0.4648 (0.5050)	
training:	Epoch: [50][30/204]	Loss 0.4662 (0.5037)	
training:	Epoch: [50][31/204]	Loss 0.5141 (0.5040)	
training:	Epoch: [50][32/204]	Loss 0.5240 (0.5046)	
training:	Epoch: [50][33/204]	Loss 0.5605 (0.5063)	
training:	Epoch: [50][34/204]	Loss 0.4445 (0.5045)	
training:	Epoch: [50][35/204]	Loss 0.4831 (0.5039)	
training:	Epoch: [50][36/204]	Loss 0.5667 (0.5056)	
training:	Epoch: [50][37/204]	Loss 0.5496 (0.5068)	
training:	Epoch: [50][38/204]	Loss 0.4752 (0.5060)	
training:	Epoch: [50][39/204]	Loss 0.4899 (0.5056)	
training:	Epoch: [50][40/204]	Loss 0.5275 (0.5061)	
training:	Epoch: [50][41/204]	Loss 0.5298 (0.5067)	
training:	Epoch: [50][42/204]	Loss 0.5029 (0.5066)	
training:	Epoch: [50][43/204]	Loss 0.5965 (0.5087)	
training:	Epoch: [50][44/204]	Loss 0.5568 (0.5098)	
training:	Epoch: [50][45/204]	Loss 0.3435 (0.5061)	
training:	Epoch: [50][46/204]	Loss 0.4005 (0.5038)	
training:	Epoch: [50][47/204]	Loss 0.4083 (0.5018)	
training:	Epoch: [50][48/204]	Loss 0.5364 (0.5025)	
training:	Epoch: [50][49/204]	Loss 0.5854 (0.5042)	
training:	Epoch: [50][50/204]	Loss 0.4395 (0.5029)	
training:	Epoch: [50][51/204]	Loss 0.5347 (0.5035)	
training:	Epoch: [50][52/204]	Loss 0.4973 (0.5034)	
training:	Epoch: [50][53/204]	Loss 0.4987 (0.5033)	
training:	Epoch: [50][54/204]	Loss 0.6274 (0.5056)	
training:	Epoch: [50][55/204]	Loss 0.4381 (0.5044)	
training:	Epoch: [50][56/204]	Loss 0.4072 (0.5026)	
training:	Epoch: [50][57/204]	Loss 0.4590 (0.5019)	
training:	Epoch: [50][58/204]	Loss 0.5872 (0.5034)	
training:	Epoch: [50][59/204]	Loss 0.3756 (0.5012)	
training:	Epoch: [50][60/204]	Loss 0.5590 (0.5022)	
training:	Epoch: [50][61/204]	Loss 0.6321 (0.5043)	
training:	Epoch: [50][62/204]	Loss 0.5327 (0.5047)	
training:	Epoch: [50][63/204]	Loss 0.4901 (0.5045)	
training:	Epoch: [50][64/204]	Loss 0.5975 (0.5060)	
training:	Epoch: [50][65/204]	Loss 0.4669 (0.5054)	
training:	Epoch: [50][66/204]	Loss 0.3238 (0.5026)	
training:	Epoch: [50][67/204]	Loss 0.3656 (0.5006)	
training:	Epoch: [50][68/204]	Loss 0.5076 (0.5007)	
training:	Epoch: [50][69/204]	Loss 0.4704 (0.5002)	
training:	Epoch: [50][70/204]	Loss 0.6712 (0.5027)	
training:	Epoch: [50][71/204]	Loss 0.5125 (0.5028)	
training:	Epoch: [50][72/204]	Loss 0.4833 (0.5025)	
training:	Epoch: [50][73/204]	Loss 0.5052 (0.5026)	
training:	Epoch: [50][74/204]	Loss 0.4420 (0.5018)	
training:	Epoch: [50][75/204]	Loss 0.4294 (0.5008)	
training:	Epoch: [50][76/204]	Loss 0.5667 (0.5017)	
training:	Epoch: [50][77/204]	Loss 0.5043 (0.5017)	
training:	Epoch: [50][78/204]	Loss 0.5298 (0.5021)	
training:	Epoch: [50][79/204]	Loss 0.4312 (0.5012)	
training:	Epoch: [50][80/204]	Loss 0.4498 (0.5005)	
training:	Epoch: [50][81/204]	Loss 0.5766 (0.5015)	
training:	Epoch: [50][82/204]	Loss 0.4944 (0.5014)	
training:	Epoch: [50][83/204]	Loss 0.4623 (0.5009)	
training:	Epoch: [50][84/204]	Loss 0.5475 (0.5015)	
training:	Epoch: [50][85/204]	Loss 0.5275 (0.5018)	
training:	Epoch: [50][86/204]	Loss 0.5031 (0.5018)	
training:	Epoch: [50][87/204]	Loss 0.4778 (0.5015)	
training:	Epoch: [50][88/204]	Loss 0.5690 (0.5023)	
training:	Epoch: [50][89/204]	Loss 0.6062 (0.5034)	
training:	Epoch: [50][90/204]	Loss 0.5228 (0.5037)	
training:	Epoch: [50][91/204]	Loss 0.5709 (0.5044)	
training:	Epoch: [50][92/204]	Loss 0.6020 (0.5054)	
training:	Epoch: [50][93/204]	Loss 0.4945 (0.5053)	
training:	Epoch: [50][94/204]	Loss 0.5631 (0.5059)	
training:	Epoch: [50][95/204]	Loss 0.4581 (0.5054)	
training:	Epoch: [50][96/204]	Loss 0.5636 (0.5060)	
training:	Epoch: [50][97/204]	Loss 0.6020 (0.5070)	
training:	Epoch: [50][98/204]	Loss 0.4928 (0.5069)	
training:	Epoch: [50][99/204]	Loss 0.4352 (0.5062)	
training:	Epoch: [50][100/204]	Loss 0.6590 (0.5077)	
training:	Epoch: [50][101/204]	Loss 0.5712 (0.5083)	
training:	Epoch: [50][102/204]	Loss 0.5108 (0.5083)	
training:	Epoch: [50][103/204]	Loss 0.4144 (0.5074)	
training:	Epoch: [50][104/204]	Loss 0.4054 (0.5065)	
training:	Epoch: [50][105/204]	Loss 0.4856 (0.5063)	
training:	Epoch: [50][106/204]	Loss 0.5060 (0.5063)	
training:	Epoch: [50][107/204]	Loss 0.5349 (0.5065)	
training:	Epoch: [50][108/204]	Loss 0.3648 (0.5052)	
training:	Epoch: [50][109/204]	Loss 0.5526 (0.5056)	
training:	Epoch: [50][110/204]	Loss 0.4173 (0.5048)	
training:	Epoch: [50][111/204]	Loss 0.5211 (0.5050)	
training:	Epoch: [50][112/204]	Loss 0.5069 (0.5050)	
training:	Epoch: [50][113/204]	Loss 0.4455 (0.5045)	
training:	Epoch: [50][114/204]	Loss 0.4671 (0.5042)	
training:	Epoch: [50][115/204]	Loss 0.6165 (0.5051)	
training:	Epoch: [50][116/204]	Loss 0.5226 (0.5053)	
training:	Epoch: [50][117/204]	Loss 0.5451 (0.5056)	
training:	Epoch: [50][118/204]	Loss 0.5545 (0.5060)	
training:	Epoch: [50][119/204]	Loss 0.4506 (0.5056)	
training:	Epoch: [50][120/204]	Loss 0.6080 (0.5064)	
training:	Epoch: [50][121/204]	Loss 0.6282 (0.5074)	
training:	Epoch: [50][122/204]	Loss 0.5280 (0.5076)	
training:	Epoch: [50][123/204]	Loss 0.5434 (0.5079)	
training:	Epoch: [50][124/204]	Loss 0.5459 (0.5082)	
training:	Epoch: [50][125/204]	Loss 0.5328 (0.5084)	
training:	Epoch: [50][126/204]	Loss 0.4286 (0.5078)	
training:	Epoch: [50][127/204]	Loss 0.6488 (0.5089)	
training:	Epoch: [50][128/204]	Loss 0.4223 (0.5082)	
training:	Epoch: [50][129/204]	Loss 0.4087 (0.5074)	
training:	Epoch: [50][130/204]	Loss 0.4096 (0.5067)	
training:	Epoch: [50][131/204]	Loss 0.4191 (0.5060)	
training:	Epoch: [50][132/204]	Loss 0.5636 (0.5064)	
training:	Epoch: [50][133/204]	Loss 0.6223 (0.5073)	
training:	Epoch: [50][134/204]	Loss 0.4858 (0.5071)	
training:	Epoch: [50][135/204]	Loss 0.4373 (0.5066)	
training:	Epoch: [50][136/204]	Loss 0.4429 (0.5062)	
training:	Epoch: [50][137/204]	Loss 0.4152 (0.5055)	
training:	Epoch: [50][138/204]	Loss 0.4490 (0.5051)	
training:	Epoch: [50][139/204]	Loss 0.4474 (0.5047)	
training:	Epoch: [50][140/204]	Loss 0.5016 (0.5047)	
training:	Epoch: [50][141/204]	Loss 0.4815 (0.5045)	
training:	Epoch: [50][142/204]	Loss 0.6127 (0.5052)	
training:	Epoch: [50][143/204]	Loss 0.5086 (0.5053)	
training:	Epoch: [50][144/204]	Loss 0.4370 (0.5048)	
training:	Epoch: [50][145/204]	Loss 0.4316 (0.5043)	
training:	Epoch: [50][146/204]	Loss 0.4800 (0.5041)	
training:	Epoch: [50][147/204]	Loss 0.3998 (0.5034)	
training:	Epoch: [50][148/204]	Loss 0.5441 (0.5037)	
training:	Epoch: [50][149/204]	Loss 0.4234 (0.5032)	
training:	Epoch: [50][150/204]	Loss 0.4298 (0.5027)	
training:	Epoch: [50][151/204]	Loss 0.3998 (0.5020)	
training:	Epoch: [50][152/204]	Loss 0.5785 (0.5025)	
training:	Epoch: [50][153/204]	Loss 0.4728 (0.5023)	
training:	Epoch: [50][154/204]	Loss 0.5953 (0.5029)	
training:	Epoch: [50][155/204]	Loss 0.5206 (0.5030)	
training:	Epoch: [50][156/204]	Loss 0.4674 (0.5028)	
training:	Epoch: [50][157/204]	Loss 0.5780 (0.5033)	
training:	Epoch: [50][158/204]	Loss 0.3955 (0.5026)	
training:	Epoch: [50][159/204]	Loss 0.6034 (0.5032)	
training:	Epoch: [50][160/204]	Loss 0.5106 (0.5033)	
training:	Epoch: [50][161/204]	Loss 0.4742 (0.5031)	
training:	Epoch: [50][162/204]	Loss 0.3958 (0.5024)	
training:	Epoch: [50][163/204]	Loss 0.4582 (0.5021)	
training:	Epoch: [50][164/204]	Loss 0.5990 (0.5027)	
training:	Epoch: [50][165/204]	Loss 0.4232 (0.5023)	
training:	Epoch: [50][166/204]	Loss 0.4640 (0.5020)	
training:	Epoch: [50][167/204]	Loss 0.5621 (0.5024)	
training:	Epoch: [50][168/204]	Loss 0.4420 (0.5020)	
training:	Epoch: [50][169/204]	Loss 0.7016 (0.5032)	
training:	Epoch: [50][170/204]	Loss 0.4321 (0.5028)	
training:	Epoch: [50][171/204]	Loss 0.4969 (0.5028)	
training:	Epoch: [50][172/204]	Loss 0.5785 (0.5032)	
training:	Epoch: [50][173/204]	Loss 0.4670 (0.5030)	
training:	Epoch: [50][174/204]	Loss 0.4989 (0.5030)	
training:	Epoch: [50][175/204]	Loss 0.7465 (0.5044)	
training:	Epoch: [50][176/204]	Loss 0.5152 (0.5044)	
training:	Epoch: [50][177/204]	Loss 0.5249 (0.5045)	
training:	Epoch: [50][178/204]	Loss 0.3854 (0.5039)	
training:	Epoch: [50][179/204]	Loss 0.4616 (0.5036)	
training:	Epoch: [50][180/204]	Loss 0.5188 (0.5037)	
training:	Epoch: [50][181/204]	Loss 0.4851 (0.5036)	
training:	Epoch: [50][182/204]	Loss 0.4576 (0.5034)	
training:	Epoch: [50][183/204]	Loss 0.5512 (0.5036)	
training:	Epoch: [50][184/204]	Loss 0.4605 (0.5034)	
training:	Epoch: [50][185/204]	Loss 0.4563 (0.5031)	
training:	Epoch: [50][186/204]	Loss 0.3752 (0.5024)	
training:	Epoch: [50][187/204]	Loss 0.4787 (0.5023)	
training:	Epoch: [50][188/204]	Loss 0.4436 (0.5020)	
training:	Epoch: [50][189/204]	Loss 0.4539 (0.5017)	
training:	Epoch: [50][190/204]	Loss 0.4311 (0.5014)	
training:	Epoch: [50][191/204]	Loss 0.3786 (0.5007)	
training:	Epoch: [50][192/204]	Loss 0.6068 (0.5013)	
training:	Epoch: [50][193/204]	Loss 0.5438 (0.5015)	
training:	Epoch: [50][194/204]	Loss 0.4720 (0.5013)	
training:	Epoch: [50][195/204]	Loss 0.8361 (0.5031)	
training:	Epoch: [50][196/204]	Loss 0.7385 (0.5043)	
training:	Epoch: [50][197/204]	Loss 0.4387 (0.5039)	
training:	Epoch: [50][198/204]	Loss 0.5988 (0.5044)	
training:	Epoch: [50][199/204]	Loss 0.5469 (0.5046)	
training:	Epoch: [50][200/204]	Loss 0.5187 (0.5047)	
training:	Epoch: [50][201/204]	Loss 0.5623 (0.5050)	
training:	Epoch: [50][202/204]	Loss 0.5790 (0.5053)	
training:	Epoch: [50][203/204]	Loss 0.5529 (0.5056)	
training:	Epoch: [50][204/204]	Loss 0.3976 (0.5051)	
Training:	 Loss: 0.5043

Training:	 ACC: 0.7663 0.7675 0.7969 0.7357
Validation:	 ACC: 0.7639 0.7657 0.8025 0.7253
Validation:	 Best_BACC: 0.7639 0.7657 0.8025 0.7253
Validation:	 Loss: 0.5026
Pretraining:	Epoch 51/120
----------
training:	Epoch: [51][1/204]	Loss 0.5572 (0.5572)	
training:	Epoch: [51][2/204]	Loss 0.5855 (0.5713)	
training:	Epoch: [51][3/204]	Loss 0.5244 (0.5557)	
training:	Epoch: [51][4/204]	Loss 0.3175 (0.4961)	
training:	Epoch: [51][5/204]	Loss 0.5550 (0.5079)	
training:	Epoch: [51][6/204]	Loss 0.6381 (0.5296)	
training:	Epoch: [51][7/204]	Loss 0.5420 (0.5314)	
training:	Epoch: [51][8/204]	Loss 0.4816 (0.5252)	
training:	Epoch: [51][9/204]	Loss 0.5658 (0.5297)	
training:	Epoch: [51][10/204]	Loss 0.6367 (0.5404)	
training:	Epoch: [51][11/204]	Loss 0.4357 (0.5309)	
training:	Epoch: [51][12/204]	Loss 0.4194 (0.5216)	
training:	Epoch: [51][13/204]	Loss 0.5617 (0.5247)	
training:	Epoch: [51][14/204]	Loss 0.5211 (0.5244)	
training:	Epoch: [51][15/204]	Loss 0.5800 (0.5281)	
training:	Epoch: [51][16/204]	Loss 0.4095 (0.5207)	
training:	Epoch: [51][17/204]	Loss 0.5403 (0.5218)	
training:	Epoch: [51][18/204]	Loss 0.4510 (0.5179)	
training:	Epoch: [51][19/204]	Loss 0.5702 (0.5207)	
training:	Epoch: [51][20/204]	Loss 0.5602 (0.5226)	
training:	Epoch: [51][21/204]	Loss 0.4199 (0.5177)	
training:	Epoch: [51][22/204]	Loss 0.5993 (0.5215)	
training:	Epoch: [51][23/204]	Loss 0.5942 (0.5246)	
training:	Epoch: [51][24/204]	Loss 0.3998 (0.5194)	
training:	Epoch: [51][25/204]	Loss 0.4859 (0.5181)	
training:	Epoch: [51][26/204]	Loss 0.5036 (0.5175)	
training:	Epoch: [51][27/204]	Loss 0.5343 (0.5181)	
training:	Epoch: [51][28/204]	Loss 0.5359 (0.5188)	
training:	Epoch: [51][29/204]	Loss 0.5019 (0.5182)	
training:	Epoch: [51][30/204]	Loss 0.3270 (0.5118)	
training:	Epoch: [51][31/204]	Loss 0.4535 (0.5099)	
training:	Epoch: [51][32/204]	Loss 0.4044 (0.5066)	
training:	Epoch: [51][33/204]	Loss 0.5203 (0.5071)	
training:	Epoch: [51][34/204]	Loss 0.5983 (0.5097)	
training:	Epoch: [51][35/204]	Loss 0.4675 (0.5085)	
training:	Epoch: [51][36/204]	Loss 0.5029 (0.5084)	
training:	Epoch: [51][37/204]	Loss 0.3438 (0.5039)	
training:	Epoch: [51][38/204]	Loss 0.5453 (0.5050)	
training:	Epoch: [51][39/204]	Loss 0.6035 (0.5075)	
training:	Epoch: [51][40/204]	Loss 0.5673 (0.5090)	
training:	Epoch: [51][41/204]	Loss 0.5386 (0.5098)	
training:	Epoch: [51][42/204]	Loss 0.4554 (0.5085)	
training:	Epoch: [51][43/204]	Loss 0.5841 (0.5102)	
training:	Epoch: [51][44/204]	Loss 0.3944 (0.5076)	
training:	Epoch: [51][45/204]	Loss 0.4070 (0.5054)	
training:	Epoch: [51][46/204]	Loss 0.7156 (0.5099)	
training:	Epoch: [51][47/204]	Loss 0.5407 (0.5106)	
training:	Epoch: [51][48/204]	Loss 0.4500 (0.5093)	
training:	Epoch: [51][49/204]	Loss 0.5875 (0.5109)	
training:	Epoch: [51][50/204]	Loss 0.4490 (0.5097)	
training:	Epoch: [51][51/204]	Loss 0.4877 (0.5092)	
training:	Epoch: [51][52/204]	Loss 0.4864 (0.5088)	
training:	Epoch: [51][53/204]	Loss 0.4990 (0.5086)	
training:	Epoch: [51][54/204]	Loss 0.4138 (0.5069)	
training:	Epoch: [51][55/204]	Loss 0.4418 (0.5057)	
training:	Epoch: [51][56/204]	Loss 0.5611 (0.5067)	
training:	Epoch: [51][57/204]	Loss 0.5106 (0.5067)	
training:	Epoch: [51][58/204]	Loss 0.5632 (0.5077)	
training:	Epoch: [51][59/204]	Loss 0.4399 (0.5066)	
training:	Epoch: [51][60/204]	Loss 0.5321 (0.5070)	
training:	Epoch: [51][61/204]	Loss 0.3744 (0.5048)	
training:	Epoch: [51][62/204]	Loss 0.5661 (0.5058)	
training:	Epoch: [51][63/204]	Loss 0.4827 (0.5054)	
training:	Epoch: [51][64/204]	Loss 0.4157 (0.5040)	
training:	Epoch: [51][65/204]	Loss 0.4707 (0.5035)	
training:	Epoch: [51][66/204]	Loss 0.4124 (0.5021)	
training:	Epoch: [51][67/204]	Loss 0.4807 (0.5018)	
training:	Epoch: [51][68/204]	Loss 0.5247 (0.5022)	
training:	Epoch: [51][69/204]	Loss 0.3737 (0.5003)	
training:	Epoch: [51][70/204]	Loss 0.4419 (0.4995)	
training:	Epoch: [51][71/204]	Loss 0.4889 (0.4993)	
training:	Epoch: [51][72/204]	Loss 0.5594 (0.5002)	
training:	Epoch: [51][73/204]	Loss 0.6273 (0.5019)	
training:	Epoch: [51][74/204]	Loss 0.4741 (0.5015)	
training:	Epoch: [51][75/204]	Loss 0.4592 (0.5010)	
training:	Epoch: [51][76/204]	Loss 0.4367 (0.5001)	
training:	Epoch: [51][77/204]	Loss 0.3761 (0.4985)	
training:	Epoch: [51][78/204]	Loss 0.5132 (0.4987)	
training:	Epoch: [51][79/204]	Loss 0.7262 (0.5016)	
training:	Epoch: [51][80/204]	Loss 0.4011 (0.5003)	
training:	Epoch: [51][81/204]	Loss 0.4494 (0.4997)	
training:	Epoch: [51][82/204]	Loss 0.5581 (0.5004)	
training:	Epoch: [51][83/204]	Loss 0.4305 (0.4996)	
training:	Epoch: [51][84/204]	Loss 0.4486 (0.4989)	
training:	Epoch: [51][85/204]	Loss 0.5781 (0.4999)	
training:	Epoch: [51][86/204]	Loss 0.6104 (0.5012)	
training:	Epoch: [51][87/204]	Loss 0.5205 (0.5014)	
training:	Epoch: [51][88/204]	Loss 0.4263 (0.5005)	
training:	Epoch: [51][89/204]	Loss 0.5490 (0.5011)	
training:	Epoch: [51][90/204]	Loss 0.4324 (0.5003)	
training:	Epoch: [51][91/204]	Loss 0.4695 (0.5000)	
training:	Epoch: [51][92/204]	Loss 0.5322 (0.5003)	
training:	Epoch: [51][93/204]	Loss 0.4965 (0.5003)	
training:	Epoch: [51][94/204]	Loss 0.3516 (0.4987)	
training:	Epoch: [51][95/204]	Loss 0.4450 (0.4981)	
training:	Epoch: [51][96/204]	Loss 0.3696 (0.4968)	
training:	Epoch: [51][97/204]	Loss 0.4961 (0.4968)	
training:	Epoch: [51][98/204]	Loss 0.5543 (0.4974)	
training:	Epoch: [51][99/204]	Loss 0.3702 (0.4961)	
training:	Epoch: [51][100/204]	Loss 0.5882 (0.4970)	
training:	Epoch: [51][101/204]	Loss 0.3982 (0.4960)	
training:	Epoch: [51][102/204]	Loss 0.7298 (0.4983)	
training:	Epoch: [51][103/204]	Loss 0.4581 (0.4979)	
training:	Epoch: [51][104/204]	Loss 0.4303 (0.4973)	
training:	Epoch: [51][105/204]	Loss 0.4636 (0.4970)	
training:	Epoch: [51][106/204]	Loss 0.4927 (0.4969)	
training:	Epoch: [51][107/204]	Loss 0.4272 (0.4963)	
training:	Epoch: [51][108/204]	Loss 0.5262 (0.4965)	
training:	Epoch: [51][109/204]	Loss 0.4202 (0.4958)	
training:	Epoch: [51][110/204]	Loss 0.4538 (0.4955)	
training:	Epoch: [51][111/204]	Loss 0.4803 (0.4953)	
training:	Epoch: [51][112/204]	Loss 0.5260 (0.4956)	
training:	Epoch: [51][113/204]	Loss 0.5574 (0.4962)	
training:	Epoch: [51][114/204]	Loss 0.6384 (0.4974)	
training:	Epoch: [51][115/204]	Loss 0.4780 (0.4972)	
training:	Epoch: [51][116/204]	Loss 0.4946 (0.4972)	
training:	Epoch: [51][117/204]	Loss 0.4764 (0.4970)	
training:	Epoch: [51][118/204]	Loss 0.4424 (0.4966)	
training:	Epoch: [51][119/204]	Loss 0.4692 (0.4963)	
training:	Epoch: [51][120/204]	Loss 0.3896 (0.4954)	
training:	Epoch: [51][121/204]	Loss 0.3691 (0.4944)	
training:	Epoch: [51][122/204]	Loss 0.2936 (0.4928)	
training:	Epoch: [51][123/204]	Loss 0.5297 (0.4931)	
training:	Epoch: [51][124/204]	Loss 0.4821 (0.4930)	
training:	Epoch: [51][125/204]	Loss 0.5359 (0.4933)	
training:	Epoch: [51][126/204]	Loss 0.4032 (0.4926)	
training:	Epoch: [51][127/204]	Loss 0.5759 (0.4933)	
training:	Epoch: [51][128/204]	Loss 0.4525 (0.4929)	
training:	Epoch: [51][129/204]	Loss 0.5660 (0.4935)	
training:	Epoch: [51][130/204]	Loss 0.4322 (0.4930)	
training:	Epoch: [51][131/204]	Loss 0.6096 (0.4939)	
training:	Epoch: [51][132/204]	Loss 0.5421 (0.4943)	
training:	Epoch: [51][133/204]	Loss 0.5976 (0.4951)	
training:	Epoch: [51][134/204]	Loss 0.6888 (0.4965)	
training:	Epoch: [51][135/204]	Loss 0.7528 (0.4984)	
training:	Epoch: [51][136/204]	Loss 0.3903 (0.4976)	
training:	Epoch: [51][137/204]	Loss 0.5491 (0.4980)	
training:	Epoch: [51][138/204]	Loss 0.3532 (0.4969)	
training:	Epoch: [51][139/204]	Loss 0.4991 (0.4970)	
training:	Epoch: [51][140/204]	Loss 0.5933 (0.4976)	
training:	Epoch: [51][141/204]	Loss 0.5182 (0.4978)	
training:	Epoch: [51][142/204]	Loss 0.5569 (0.4982)	
training:	Epoch: [51][143/204]	Loss 0.6515 (0.4993)	
training:	Epoch: [51][144/204]	Loss 0.6979 (0.5007)	
training:	Epoch: [51][145/204]	Loss 0.4020 (0.5000)	
training:	Epoch: [51][146/204]	Loss 0.3900 (0.4992)	
training:	Epoch: [51][147/204]	Loss 0.6051 (0.4999)	
training:	Epoch: [51][148/204]	Loss 0.7129 (0.5014)	
training:	Epoch: [51][149/204]	Loss 0.5471 (0.5017)	
training:	Epoch: [51][150/204]	Loss 0.4174 (0.5011)	
training:	Epoch: [51][151/204]	Loss 0.4783 (0.5010)	
training:	Epoch: [51][152/204]	Loss 0.6007 (0.5016)	
training:	Epoch: [51][153/204]	Loss 0.5860 (0.5022)	
training:	Epoch: [51][154/204]	Loss 0.4823 (0.5020)	
training:	Epoch: [51][155/204]	Loss 0.4611 (0.5018)	
training:	Epoch: [51][156/204]	Loss 0.5032 (0.5018)	
training:	Epoch: [51][157/204]	Loss 0.5154 (0.5019)	
training:	Epoch: [51][158/204]	Loss 0.4675 (0.5017)	
training:	Epoch: [51][159/204]	Loss 0.4926 (0.5016)	
training:	Epoch: [51][160/204]	Loss 0.4496 (0.5013)	
training:	Epoch: [51][161/204]	Loss 0.5478 (0.5016)	
training:	Epoch: [51][162/204]	Loss 0.4256 (0.5011)	
training:	Epoch: [51][163/204]	Loss 0.4628 (0.5009)	
training:	Epoch: [51][164/204]	Loss 0.4708 (0.5007)	
training:	Epoch: [51][165/204]	Loss 0.5203 (0.5008)	
training:	Epoch: [51][166/204]	Loss 0.5008 (0.5008)	
training:	Epoch: [51][167/204]	Loss 0.3093 (0.4997)	
training:	Epoch: [51][168/204]	Loss 0.6198 (0.5004)	
training:	Epoch: [51][169/204]	Loss 0.6133 (0.5010)	
training:	Epoch: [51][170/204]	Loss 0.3574 (0.5002)	
training:	Epoch: [51][171/204]	Loss 0.4253 (0.4998)	
training:	Epoch: [51][172/204]	Loss 0.4696 (0.4996)	
training:	Epoch: [51][173/204]	Loss 0.6054 (0.5002)	
training:	Epoch: [51][174/204]	Loss 0.4852 (0.5001)	
training:	Epoch: [51][175/204]	Loss 0.5331 (0.5003)	
training:	Epoch: [51][176/204]	Loss 0.5934 (0.5008)	
training:	Epoch: [51][177/204]	Loss 0.6718 (0.5018)	
training:	Epoch: [51][178/204]	Loss 0.4669 (0.5016)	
training:	Epoch: [51][179/204]	Loss 0.5118 (0.5017)	
training:	Epoch: [51][180/204]	Loss 0.4391 (0.5013)	
training:	Epoch: [51][181/204]	Loss 0.4609 (0.5011)	
training:	Epoch: [51][182/204]	Loss 0.4850 (0.5010)	
training:	Epoch: [51][183/204]	Loss 0.3755 (0.5003)	
training:	Epoch: [51][184/204]	Loss 0.5584 (0.5006)	
training:	Epoch: [51][185/204]	Loss 0.5170 (0.5007)	
training:	Epoch: [51][186/204]	Loss 0.5816 (0.5011)	
training:	Epoch: [51][187/204]	Loss 0.4456 (0.5008)	
training:	Epoch: [51][188/204]	Loss 0.5507 (0.5011)	
training:	Epoch: [51][189/204]	Loss 0.5075 (0.5011)	
training:	Epoch: [51][190/204]	Loss 0.5958 (0.5016)	
training:	Epoch: [51][191/204]	Loss 0.5737 (0.5020)	
training:	Epoch: [51][192/204]	Loss 0.5866 (0.5025)	
training:	Epoch: [51][193/204]	Loss 0.5042 (0.5025)	
training:	Epoch: [51][194/204]	Loss 0.4978 (0.5024)	
training:	Epoch: [51][195/204]	Loss 0.5906 (0.5029)	
training:	Epoch: [51][196/204]	Loss 0.4280 (0.5025)	
training:	Epoch: [51][197/204]	Loss 0.5398 (0.5027)	
training:	Epoch: [51][198/204]	Loss 0.4522 (0.5025)	
training:	Epoch: [51][199/204]	Loss 0.5271 (0.5026)	
training:	Epoch: [51][200/204]	Loss 0.5472 (0.5028)	
training:	Epoch: [51][201/204]	Loss 0.6249 (0.5034)	
training:	Epoch: [51][202/204]	Loss 0.4528 (0.5032)	
training:	Epoch: [51][203/204]	Loss 0.5107 (0.5032)	
training:	Epoch: [51][204/204]	Loss 0.4752 (0.5031)	
Training:	 Loss: 0.5023

Training:	 ACC: 0.7698 0.7706 0.7895 0.7500
Validation:	 ACC: 0.7633 0.7646 0.7922 0.7343
Validation:	 Best_BACC: 0.7639 0.7657 0.8025 0.7253
Validation:	 Loss: 0.4999
Pretraining:	Epoch 52/120
----------
training:	Epoch: [52][1/204]	Loss 0.4801 (0.4801)	
training:	Epoch: [52][2/204]	Loss 0.4175 (0.4488)	
training:	Epoch: [52][3/204]	Loss 0.5739 (0.4905)	
training:	Epoch: [52][4/204]	Loss 0.4865 (0.4895)	
training:	Epoch: [52][5/204]	Loss 0.5122 (0.4940)	
training:	Epoch: [52][6/204]	Loss 0.5947 (0.5108)	
training:	Epoch: [52][7/204]	Loss 0.5729 (0.5197)	
training:	Epoch: [52][8/204]	Loss 0.4702 (0.5135)	
training:	Epoch: [52][9/204]	Loss 0.5049 (0.5125)	
training:	Epoch: [52][10/204]	Loss 0.3719 (0.4985)	
training:	Epoch: [52][11/204]	Loss 0.4290 (0.4922)	
training:	Epoch: [52][12/204]	Loss 0.5739 (0.4990)	
training:	Epoch: [52][13/204]	Loss 0.4619 (0.4961)	
training:	Epoch: [52][14/204]	Loss 0.6997 (0.5107)	
training:	Epoch: [52][15/204]	Loss 0.6422 (0.5194)	
training:	Epoch: [52][16/204]	Loss 0.4812 (0.5170)	
training:	Epoch: [52][17/204]	Loss 0.4253 (0.5116)	
training:	Epoch: [52][18/204]	Loss 0.4065 (0.5058)	
training:	Epoch: [52][19/204]	Loss 0.5238 (0.5068)	
training:	Epoch: [52][20/204]	Loss 0.4217 (0.5025)	
training:	Epoch: [52][21/204]	Loss 0.5314 (0.5039)	
training:	Epoch: [52][22/204]	Loss 0.4483 (0.5014)	
training:	Epoch: [52][23/204]	Loss 0.5053 (0.5015)	
training:	Epoch: [52][24/204]	Loss 0.5103 (0.5019)	
training:	Epoch: [52][25/204]	Loss 0.5521 (0.5039)	
training:	Epoch: [52][26/204]	Loss 0.6345 (0.5089)	
training:	Epoch: [52][27/204]	Loss 0.5184 (0.5093)	
training:	Epoch: [52][28/204]	Loss 0.5114 (0.5093)	
training:	Epoch: [52][29/204]	Loss 0.3969 (0.5055)	
training:	Epoch: [52][30/204]	Loss 0.4209 (0.5026)	
training:	Epoch: [52][31/204]	Loss 0.4373 (0.5005)	
training:	Epoch: [52][32/204]	Loss 0.6210 (0.5043)	
training:	Epoch: [52][33/204]	Loss 0.5588 (0.5060)	
training:	Epoch: [52][34/204]	Loss 0.8225 (0.5153)	
training:	Epoch: [52][35/204]	Loss 0.3870 (0.5116)	
training:	Epoch: [52][36/204]	Loss 0.4179 (0.5090)	
training:	Epoch: [52][37/204]	Loss 0.4233 (0.5067)	
training:	Epoch: [52][38/204]	Loss 0.4333 (0.5047)	
training:	Epoch: [52][39/204]	Loss 0.4531 (0.5034)	
training:	Epoch: [52][40/204]	Loss 0.5548 (0.5047)	
training:	Epoch: [52][41/204]	Loss 0.4426 (0.5032)	
training:	Epoch: [52][42/204]	Loss 0.4850 (0.5028)	
training:	Epoch: [52][43/204]	Loss 0.4203 (0.5008)	
training:	Epoch: [52][44/204]	Loss 0.4215 (0.4990)	
training:	Epoch: [52][45/204]	Loss 0.5007 (0.4991)	
training:	Epoch: [52][46/204]	Loss 0.4815 (0.4987)	
training:	Epoch: [52][47/204]	Loss 0.4845 (0.4984)	
training:	Epoch: [52][48/204]	Loss 0.4977 (0.4984)	
training:	Epoch: [52][49/204]	Loss 0.4869 (0.4981)	
training:	Epoch: [52][50/204]	Loss 0.3818 (0.4958)	
training:	Epoch: [52][51/204]	Loss 0.3662 (0.4933)	
training:	Epoch: [52][52/204]	Loss 0.5547 (0.4945)	
training:	Epoch: [52][53/204]	Loss 0.4620 (0.4938)	
training:	Epoch: [52][54/204]	Loss 0.4548 (0.4931)	
training:	Epoch: [52][55/204]	Loss 0.5522 (0.4942)	
training:	Epoch: [52][56/204]	Loss 0.4118 (0.4927)	
training:	Epoch: [52][57/204]	Loss 0.4399 (0.4918)	
training:	Epoch: [52][58/204]	Loss 0.5343 (0.4925)	
training:	Epoch: [52][59/204]	Loss 0.6938 (0.4959)	
training:	Epoch: [52][60/204]	Loss 0.5070 (0.4961)	
training:	Epoch: [52][61/204]	Loss 0.4745 (0.4958)	
training:	Epoch: [52][62/204]	Loss 0.4526 (0.4951)	
training:	Epoch: [52][63/204]	Loss 0.4249 (0.4940)	
training:	Epoch: [52][64/204]	Loss 0.5841 (0.4954)	
training:	Epoch: [52][65/204]	Loss 0.4405 (0.4945)	
training:	Epoch: [52][66/204]	Loss 0.4444 (0.4938)	
training:	Epoch: [52][67/204]	Loss 0.4389 (0.4930)	
training:	Epoch: [52][68/204]	Loss 0.5878 (0.4943)	
training:	Epoch: [52][69/204]	Loss 0.4337 (0.4935)	
training:	Epoch: [52][70/204]	Loss 0.6293 (0.4954)	
training:	Epoch: [52][71/204]	Loss 0.5646 (0.4964)	
training:	Epoch: [52][72/204]	Loss 0.5791 (0.4975)	
training:	Epoch: [52][73/204]	Loss 0.4908 (0.4974)	
training:	Epoch: [52][74/204]	Loss 0.4300 (0.4965)	
training:	Epoch: [52][75/204]	Loss 0.4926 (0.4965)	
training:	Epoch: [52][76/204]	Loss 0.5755 (0.4975)	
training:	Epoch: [52][77/204]	Loss 0.5951 (0.4988)	
training:	Epoch: [52][78/204]	Loss 0.4824 (0.4986)	
training:	Epoch: [52][79/204]	Loss 0.4782 (0.4983)	
training:	Epoch: [52][80/204]	Loss 0.4153 (0.4973)	
training:	Epoch: [52][81/204]	Loss 0.5674 (0.4981)	
training:	Epoch: [52][82/204]	Loss 0.4349 (0.4974)	
training:	Epoch: [52][83/204]	Loss 0.4155 (0.4964)	
training:	Epoch: [52][84/204]	Loss 0.4170 (0.4954)	
training:	Epoch: [52][85/204]	Loss 0.4626 (0.4951)	
training:	Epoch: [52][86/204]	Loss 0.4851 (0.4949)	
training:	Epoch: [52][87/204]	Loss 0.5598 (0.4957)	
training:	Epoch: [52][88/204]	Loss 0.6114 (0.4970)	
training:	Epoch: [52][89/204]	Loss 0.5963 (0.4981)	
training:	Epoch: [52][90/204]	Loss 0.4179 (0.4972)	
training:	Epoch: [52][91/204]	Loss 0.4973 (0.4972)	
training:	Epoch: [52][92/204]	Loss 0.5582 (0.4979)	
training:	Epoch: [52][93/204]	Loss 0.4472 (0.4973)	
training:	Epoch: [52][94/204]	Loss 0.5440 (0.4978)	
training:	Epoch: [52][95/204]	Loss 0.6344 (0.4993)	
training:	Epoch: [52][96/204]	Loss 0.4615 (0.4989)	
training:	Epoch: [52][97/204]	Loss 0.4731 (0.4986)	
training:	Epoch: [52][98/204]	Loss 0.4605 (0.4982)	
training:	Epoch: [52][99/204]	Loss 0.6685 (0.4999)	
training:	Epoch: [52][100/204]	Loss 0.5652 (0.5006)	
training:	Epoch: [52][101/204]	Loss 0.4545 (0.5001)	
training:	Epoch: [52][102/204]	Loss 0.6759 (0.5019)	
training:	Epoch: [52][103/204]	Loss 0.4870 (0.5017)	
training:	Epoch: [52][104/204]	Loss 0.4680 (0.5014)	
training:	Epoch: [52][105/204]	Loss 0.5025 (0.5014)	
training:	Epoch: [52][106/204]	Loss 0.4932 (0.5013)	
training:	Epoch: [52][107/204]	Loss 0.5691 (0.5020)	
training:	Epoch: [52][108/204]	Loss 0.6683 (0.5035)	
training:	Epoch: [52][109/204]	Loss 0.3506 (0.5021)	
training:	Epoch: [52][110/204]	Loss 0.5850 (0.5029)	
training:	Epoch: [52][111/204]	Loss 0.4501 (0.5024)	
training:	Epoch: [52][112/204]	Loss 0.5144 (0.5025)	
training:	Epoch: [52][113/204]	Loss 0.5063 (0.5025)	
training:	Epoch: [52][114/204]	Loss 0.4177 (0.5018)	
training:	Epoch: [52][115/204]	Loss 0.5573 (0.5023)	
training:	Epoch: [52][116/204]	Loss 0.4251 (0.5016)	
training:	Epoch: [52][117/204]	Loss 0.5015 (0.5016)	
training:	Epoch: [52][118/204]	Loss 0.5208 (0.5018)	
training:	Epoch: [52][119/204]	Loss 0.4832 (0.5016)	
training:	Epoch: [52][120/204]	Loss 0.7253 (0.5035)	
training:	Epoch: [52][121/204]	Loss 0.6002 (0.5043)	
training:	Epoch: [52][122/204]	Loss 0.4471 (0.5038)	
training:	Epoch: [52][123/204]	Loss 0.5763 (0.5044)	
training:	Epoch: [52][124/204]	Loss 0.5032 (0.5044)	
training:	Epoch: [52][125/204]	Loss 0.5168 (0.5045)	
training:	Epoch: [52][126/204]	Loss 0.5224 (0.5046)	
training:	Epoch: [52][127/204]	Loss 0.4839 (0.5045)	
training:	Epoch: [52][128/204]	Loss 0.4415 (0.5040)	
training:	Epoch: [52][129/204]	Loss 0.4312 (0.5034)	
training:	Epoch: [52][130/204]	Loss 0.5531 (0.5038)	
training:	Epoch: [52][131/204]	Loss 0.4781 (0.5036)	
training:	Epoch: [52][132/204]	Loss 0.4864 (0.5035)	
training:	Epoch: [52][133/204]	Loss 0.5756 (0.5040)	
training:	Epoch: [52][134/204]	Loss 0.4878 (0.5039)	
training:	Epoch: [52][135/204]	Loss 0.4349 (0.5034)	
training:	Epoch: [52][136/204]	Loss 0.4633 (0.5031)	
training:	Epoch: [52][137/204]	Loss 0.6802 (0.5044)	
training:	Epoch: [52][138/204]	Loss 0.5547 (0.5047)	
training:	Epoch: [52][139/204]	Loss 0.5121 (0.5048)	
training:	Epoch: [52][140/204]	Loss 0.7160 (0.5063)	
training:	Epoch: [52][141/204]	Loss 0.4771 (0.5061)	
training:	Epoch: [52][142/204]	Loss 0.6121 (0.5068)	
training:	Epoch: [52][143/204]	Loss 0.5648 (0.5072)	
training:	Epoch: [52][144/204]	Loss 0.5771 (0.5077)	
training:	Epoch: [52][145/204]	Loss 0.5252 (0.5078)	
training:	Epoch: [52][146/204]	Loss 0.4859 (0.5077)	
training:	Epoch: [52][147/204]	Loss 0.5757 (0.5082)	
training:	Epoch: [52][148/204]	Loss 0.4243 (0.5076)	
training:	Epoch: [52][149/204]	Loss 0.5652 (0.5080)	
training:	Epoch: [52][150/204]	Loss 0.5765 (0.5084)	
training:	Epoch: [52][151/204]	Loss 0.3732 (0.5075)	
training:	Epoch: [52][152/204]	Loss 0.4390 (0.5071)	
training:	Epoch: [52][153/204]	Loss 0.6763 (0.5082)	
training:	Epoch: [52][154/204]	Loss 0.4571 (0.5079)	
training:	Epoch: [52][155/204]	Loss 0.4383 (0.5074)	
training:	Epoch: [52][156/204]	Loss 0.4918 (0.5073)	
training:	Epoch: [52][157/204]	Loss 0.5580 (0.5076)	
training:	Epoch: [52][158/204]	Loss 0.5258 (0.5077)	
training:	Epoch: [52][159/204]	Loss 0.3985 (0.5071)	
training:	Epoch: [52][160/204]	Loss 0.5299 (0.5072)	
training:	Epoch: [52][161/204]	Loss 0.5503 (0.5075)	
training:	Epoch: [52][162/204]	Loss 0.4531 (0.5071)	
training:	Epoch: [52][163/204]	Loss 0.5372 (0.5073)	
training:	Epoch: [52][164/204]	Loss 0.4868 (0.5072)	
training:	Epoch: [52][165/204]	Loss 0.4996 (0.5071)	
training:	Epoch: [52][166/204]	Loss 0.4905 (0.5070)	
training:	Epoch: [52][167/204]	Loss 0.5546 (0.5073)	
training:	Epoch: [52][168/204]	Loss 0.6306 (0.5081)	
training:	Epoch: [52][169/204]	Loss 0.5943 (0.5086)	
training:	Epoch: [52][170/204]	Loss 0.5350 (0.5087)	
training:	Epoch: [52][171/204]	Loss 0.4393 (0.5083)	
training:	Epoch: [52][172/204]	Loss 0.4195 (0.5078)	
training:	Epoch: [52][173/204]	Loss 0.4273 (0.5073)	
training:	Epoch: [52][174/204]	Loss 0.4993 (0.5073)	
training:	Epoch: [52][175/204]	Loss 0.6552 (0.5081)	
training:	Epoch: [52][176/204]	Loss 0.3490 (0.5072)	
training:	Epoch: [52][177/204]	Loss 0.5066 (0.5072)	
training:	Epoch: [52][178/204]	Loss 0.4005 (0.5066)	
training:	Epoch: [52][179/204]	Loss 0.4864 (0.5065)	
training:	Epoch: [52][180/204]	Loss 0.4256 (0.5061)	
training:	Epoch: [52][181/204]	Loss 0.4560 (0.5058)	
training:	Epoch: [52][182/204]	Loss 0.4384 (0.5054)	
training:	Epoch: [52][183/204]	Loss 0.4609 (0.5052)	
training:	Epoch: [52][184/204]	Loss 0.4163 (0.5047)	
training:	Epoch: [52][185/204]	Loss 0.4500 (0.5044)	
training:	Epoch: [52][186/204]	Loss 0.3708 (0.5037)	
training:	Epoch: [52][187/204]	Loss 0.4101 (0.5032)	
training:	Epoch: [52][188/204]	Loss 0.5457 (0.5034)	
training:	Epoch: [52][189/204]	Loss 0.3804 (0.5028)	
training:	Epoch: [52][190/204]	Loss 0.6350 (0.5035)	
training:	Epoch: [52][191/204]	Loss 0.4300 (0.5031)	
training:	Epoch: [52][192/204]	Loss 0.4933 (0.5030)	
training:	Epoch: [52][193/204]	Loss 0.3580 (0.5023)	
training:	Epoch: [52][194/204]	Loss 0.4237 (0.5019)	
training:	Epoch: [52][195/204]	Loss 0.4941 (0.5018)	
training:	Epoch: [52][196/204]	Loss 0.5193 (0.5019)	
training:	Epoch: [52][197/204]	Loss 0.3471 (0.5011)	
training:	Epoch: [52][198/204]	Loss 0.5827 (0.5015)	
training:	Epoch: [52][199/204]	Loss 0.6263 (0.5022)	
training:	Epoch: [52][200/204]	Loss 0.3535 (0.5014)	
training:	Epoch: [52][201/204]	Loss 0.4531 (0.5012)	
training:	Epoch: [52][202/204]	Loss 0.3990 (0.5007)	
training:	Epoch: [52][203/204]	Loss 0.5139 (0.5007)	
training:	Epoch: [52][204/204]	Loss 0.5126 (0.5008)	
Training:	 Loss: 0.5000

Training:	 ACC: 0.7717 0.7727 0.7960 0.7474
Validation:	 ACC: 0.7674 0.7689 0.8004 0.7343
Validation:	 Best_BACC: 0.7674 0.7689 0.8004 0.7343
Validation:	 Loss: 0.4981
Pretraining:	Epoch 53/120
----------
training:	Epoch: [53][1/204]	Loss 0.5457 (0.5457)	
training:	Epoch: [53][2/204]	Loss 0.4869 (0.5163)	
training:	Epoch: [53][3/204]	Loss 0.4942 (0.5089)	
training:	Epoch: [53][4/204]	Loss 0.5265 (0.5133)	
training:	Epoch: [53][5/204]	Loss 0.4019 (0.4910)	
training:	Epoch: [53][6/204]	Loss 0.5046 (0.4933)	
training:	Epoch: [53][7/204]	Loss 0.4636 (0.4890)	
training:	Epoch: [53][8/204]	Loss 0.4639 (0.4859)	
training:	Epoch: [53][9/204]	Loss 0.5192 (0.4896)	
training:	Epoch: [53][10/204]	Loss 0.6014 (0.5008)	
training:	Epoch: [53][11/204]	Loss 0.4180 (0.4933)	
training:	Epoch: [53][12/204]	Loss 0.4486 (0.4895)	
training:	Epoch: [53][13/204]	Loss 0.4700 (0.4880)	
training:	Epoch: [53][14/204]	Loss 0.5214 (0.4904)	
training:	Epoch: [53][15/204]	Loss 0.5435 (0.4940)	
training:	Epoch: [53][16/204]	Loss 0.5109 (0.4950)	
training:	Epoch: [53][17/204]	Loss 0.3462 (0.4863)	
training:	Epoch: [53][18/204]	Loss 0.3865 (0.4807)	
training:	Epoch: [53][19/204]	Loss 0.3816 (0.4755)	
training:	Epoch: [53][20/204]	Loss 0.5507 (0.4793)	
training:	Epoch: [53][21/204]	Loss 0.5189 (0.4811)	
training:	Epoch: [53][22/204]	Loss 0.8629 (0.4985)	
training:	Epoch: [53][23/204]	Loss 0.4184 (0.4950)	
training:	Epoch: [53][24/204]	Loss 0.4988 (0.4952)	
training:	Epoch: [53][25/204]	Loss 0.4265 (0.4924)	
training:	Epoch: [53][26/204]	Loss 0.5166 (0.4934)	
training:	Epoch: [53][27/204]	Loss 0.6876 (0.5005)	
training:	Epoch: [53][28/204]	Loss 0.5398 (0.5019)	
training:	Epoch: [53][29/204]	Loss 0.3969 (0.4983)	
training:	Epoch: [53][30/204]	Loss 0.4850 (0.4979)	
training:	Epoch: [53][31/204]	Loss 0.5142 (0.4984)	
training:	Epoch: [53][32/204]	Loss 0.5865 (0.5012)	
training:	Epoch: [53][33/204]	Loss 0.5253 (0.5019)	
training:	Epoch: [53][34/204]	Loss 0.4107 (0.4992)	
training:	Epoch: [53][35/204]	Loss 0.4880 (0.4989)	
training:	Epoch: [53][36/204]	Loss 0.4144 (0.4965)	
training:	Epoch: [53][37/204]	Loss 0.4501 (0.4953)	
training:	Epoch: [53][38/204]	Loss 0.5305 (0.4962)	
training:	Epoch: [53][39/204]	Loss 0.5508 (0.4976)	
training:	Epoch: [53][40/204]	Loss 0.4017 (0.4952)	
training:	Epoch: [53][41/204]	Loss 0.4654 (0.4945)	
training:	Epoch: [53][42/204]	Loss 0.6076 (0.4972)	
training:	Epoch: [53][43/204]	Loss 0.5934 (0.4994)	
training:	Epoch: [53][44/204]	Loss 0.6143 (0.5020)	
training:	Epoch: [53][45/204]	Loss 0.5792 (0.5037)	
training:	Epoch: [53][46/204]	Loss 0.4912 (0.5035)	
training:	Epoch: [53][47/204]	Loss 0.5046 (0.5035)	
training:	Epoch: [53][48/204]	Loss 0.4323 (0.5020)	
training:	Epoch: [53][49/204]	Loss 0.5703 (0.5034)	
training:	Epoch: [53][50/204]	Loss 0.5674 (0.5047)	
training:	Epoch: [53][51/204]	Loss 0.5864 (0.5063)	
training:	Epoch: [53][52/204]	Loss 0.4387 (0.5050)	
training:	Epoch: [53][53/204]	Loss 0.4606 (0.5042)	
training:	Epoch: [53][54/204]	Loss 0.4153 (0.5025)	
training:	Epoch: [53][55/204]	Loss 0.4366 (0.5013)	
training:	Epoch: [53][56/204]	Loss 0.4993 (0.5013)	
training:	Epoch: [53][57/204]	Loss 0.4486 (0.5003)	
training:	Epoch: [53][58/204]	Loss 0.5019 (0.5004)	
training:	Epoch: [53][59/204]	Loss 0.5671 (0.5015)	
training:	Epoch: [53][60/204]	Loss 0.5472 (0.5023)	
training:	Epoch: [53][61/204]	Loss 0.4199 (0.5009)	
training:	Epoch: [53][62/204]	Loss 0.5959 (0.5024)	
training:	Epoch: [53][63/204]	Loss 0.5278 (0.5029)	
training:	Epoch: [53][64/204]	Loss 0.4876 (0.5026)	
training:	Epoch: [53][65/204]	Loss 0.4447 (0.5017)	
training:	Epoch: [53][66/204]	Loss 0.4762 (0.5013)	
training:	Epoch: [53][67/204]	Loss 0.4699 (0.5009)	
training:	Epoch: [53][68/204]	Loss 0.4932 (0.5008)	
training:	Epoch: [53][69/204]	Loss 0.3896 (0.4991)	
training:	Epoch: [53][70/204]	Loss 0.6163 (0.5008)	
training:	Epoch: [53][71/204]	Loss 0.3604 (0.4988)	
training:	Epoch: [53][72/204]	Loss 0.5627 (0.4997)	
training:	Epoch: [53][73/204]	Loss 0.4624 (0.4992)	
training:	Epoch: [53][74/204]	Loss 0.5009 (0.4992)	
training:	Epoch: [53][75/204]	Loss 0.5267 (0.4996)	
training:	Epoch: [53][76/204]	Loss 0.5758 (0.5006)	
training:	Epoch: [53][77/204]	Loss 0.4534 (0.5000)	
training:	Epoch: [53][78/204]	Loss 0.4882 (0.4998)	
training:	Epoch: [53][79/204]	Loss 0.5339 (0.5003)	
training:	Epoch: [53][80/204]	Loss 0.4481 (0.4996)	
training:	Epoch: [53][81/204]	Loss 0.6196 (0.5011)	
training:	Epoch: [53][82/204]	Loss 0.3976 (0.4998)	
training:	Epoch: [53][83/204]	Loss 0.4559 (0.4993)	
training:	Epoch: [53][84/204]	Loss 0.5350 (0.4997)	
training:	Epoch: [53][85/204]	Loss 0.3949 (0.4985)	
training:	Epoch: [53][86/204]	Loss 0.4880 (0.4984)	
training:	Epoch: [53][87/204]	Loss 0.3380 (0.4965)	
training:	Epoch: [53][88/204]	Loss 0.6141 (0.4979)	
training:	Epoch: [53][89/204]	Loss 0.4268 (0.4971)	
training:	Epoch: [53][90/204]	Loss 0.4879 (0.4970)	
training:	Epoch: [53][91/204]	Loss 0.3912 (0.4958)	
training:	Epoch: [53][92/204]	Loss 0.6484 (0.4975)	
training:	Epoch: [53][93/204]	Loss 0.5102 (0.4976)	
training:	Epoch: [53][94/204]	Loss 0.5741 (0.4984)	
training:	Epoch: [53][95/204]	Loss 0.5553 (0.4990)	
training:	Epoch: [53][96/204]	Loss 0.3618 (0.4976)	
training:	Epoch: [53][97/204]	Loss 0.4766 (0.4974)	
training:	Epoch: [53][98/204]	Loss 0.6305 (0.4987)	
training:	Epoch: [53][99/204]	Loss 0.3863 (0.4976)	
training:	Epoch: [53][100/204]	Loss 0.4809 (0.4974)	
training:	Epoch: [53][101/204]	Loss 0.5558 (0.4980)	
training:	Epoch: [53][102/204]	Loss 0.4577 (0.4976)	
training:	Epoch: [53][103/204]	Loss 0.5012 (0.4976)	
training:	Epoch: [53][104/204]	Loss 0.3774 (0.4965)	
training:	Epoch: [53][105/204]	Loss 0.4766 (0.4963)	
training:	Epoch: [53][106/204]	Loss 0.3785 (0.4952)	
training:	Epoch: [53][107/204]	Loss 0.4975 (0.4952)	
training:	Epoch: [53][108/204]	Loss 0.5678 (0.4959)	
training:	Epoch: [53][109/204]	Loss 0.6486 (0.4973)	
training:	Epoch: [53][110/204]	Loss 0.5913 (0.4981)	
training:	Epoch: [53][111/204]	Loss 0.6239 (0.4993)	
training:	Epoch: [53][112/204]	Loss 0.6452 (0.5006)	
training:	Epoch: [53][113/204]	Loss 0.4086 (0.4998)	
training:	Epoch: [53][114/204]	Loss 0.6016 (0.5007)	
training:	Epoch: [53][115/204]	Loss 0.5300 (0.5009)	
training:	Epoch: [53][116/204]	Loss 0.3887 (0.4999)	
training:	Epoch: [53][117/204]	Loss 0.4892 (0.4998)	
training:	Epoch: [53][118/204]	Loss 0.5869 (0.5006)	
training:	Epoch: [53][119/204]	Loss 0.5519 (0.5010)	
training:	Epoch: [53][120/204]	Loss 0.4259 (0.5004)	
training:	Epoch: [53][121/204]	Loss 0.4452 (0.4999)	
training:	Epoch: [53][122/204]	Loss 0.4834 (0.4998)	
training:	Epoch: [53][123/204]	Loss 0.4193 (0.4991)	
training:	Epoch: [53][124/204]	Loss 0.4620 (0.4988)	
training:	Epoch: [53][125/204]	Loss 0.5735 (0.4994)	
training:	Epoch: [53][126/204]	Loss 0.5035 (0.4995)	
training:	Epoch: [53][127/204]	Loss 0.5070 (0.4995)	
training:	Epoch: [53][128/204]	Loss 0.4299 (0.4990)	
training:	Epoch: [53][129/204]	Loss 0.6107 (0.4999)	
training:	Epoch: [53][130/204]	Loss 0.4322 (0.4993)	
training:	Epoch: [53][131/204]	Loss 0.5047 (0.4994)	
training:	Epoch: [53][132/204]	Loss 0.4545 (0.4990)	
training:	Epoch: [53][133/204]	Loss 0.4337 (0.4985)	
training:	Epoch: [53][134/204]	Loss 0.5997 (0.4993)	
training:	Epoch: [53][135/204]	Loss 0.5008 (0.4993)	
training:	Epoch: [53][136/204]	Loss 0.5175 (0.4994)	
training:	Epoch: [53][137/204]	Loss 0.4490 (0.4991)	
training:	Epoch: [53][138/204]	Loss 0.3986 (0.4983)	
training:	Epoch: [53][139/204]	Loss 0.4154 (0.4977)	
training:	Epoch: [53][140/204]	Loss 0.5441 (0.4981)	
training:	Epoch: [53][141/204]	Loss 0.4881 (0.4980)	
training:	Epoch: [53][142/204]	Loss 0.4039 (0.4973)	
training:	Epoch: [53][143/204]	Loss 0.4727 (0.4972)	
training:	Epoch: [53][144/204]	Loss 0.5314 (0.4974)	
training:	Epoch: [53][145/204]	Loss 0.4836 (0.4973)	
training:	Epoch: [53][146/204]	Loss 0.3550 (0.4963)	
training:	Epoch: [53][147/204]	Loss 0.4805 (0.4962)	
training:	Epoch: [53][148/204]	Loss 0.4832 (0.4961)	
training:	Epoch: [53][149/204]	Loss 0.3636 (0.4953)	
training:	Epoch: [53][150/204]	Loss 0.3873 (0.4945)	
training:	Epoch: [53][151/204]	Loss 0.4756 (0.4944)	
training:	Epoch: [53][152/204]	Loss 0.3888 (0.4937)	
training:	Epoch: [53][153/204]	Loss 0.4470 (0.4934)	
training:	Epoch: [53][154/204]	Loss 0.5819 (0.4940)	
training:	Epoch: [53][155/204]	Loss 0.7584 (0.4957)	
training:	Epoch: [53][156/204]	Loss 0.4508 (0.4954)	
training:	Epoch: [53][157/204]	Loss 0.4507 (0.4951)	
training:	Epoch: [53][158/204]	Loss 0.4293 (0.4947)	
training:	Epoch: [53][159/204]	Loss 0.4844 (0.4946)	
training:	Epoch: [53][160/204]	Loss 0.4086 (0.4941)	
training:	Epoch: [53][161/204]	Loss 0.4477 (0.4938)	
training:	Epoch: [53][162/204]	Loss 0.5744 (0.4943)	
training:	Epoch: [53][163/204]	Loss 0.4243 (0.4939)	
training:	Epoch: [53][164/204]	Loss 0.5558 (0.4943)	
training:	Epoch: [53][165/204]	Loss 0.5192 (0.4944)	
training:	Epoch: [53][166/204]	Loss 0.5764 (0.4949)	
training:	Epoch: [53][167/204]	Loss 0.5230 (0.4951)	
training:	Epoch: [53][168/204]	Loss 0.4506 (0.4948)	
training:	Epoch: [53][169/204]	Loss 0.4756 (0.4947)	
training:	Epoch: [53][170/204]	Loss 0.5089 (0.4948)	
training:	Epoch: [53][171/204]	Loss 0.4567 (0.4946)	
training:	Epoch: [53][172/204]	Loss 0.6391 (0.4954)	
training:	Epoch: [53][173/204]	Loss 0.4099 (0.4949)	
training:	Epoch: [53][174/204]	Loss 0.3556 (0.4941)	
training:	Epoch: [53][175/204]	Loss 0.5155 (0.4942)	
training:	Epoch: [53][176/204]	Loss 0.5238 (0.4944)	
training:	Epoch: [53][177/204]	Loss 0.5645 (0.4948)	
training:	Epoch: [53][178/204]	Loss 0.3983 (0.4942)	
training:	Epoch: [53][179/204]	Loss 0.3842 (0.4936)	
training:	Epoch: [53][180/204]	Loss 0.5145 (0.4937)	
training:	Epoch: [53][181/204]	Loss 0.5190 (0.4939)	
training:	Epoch: [53][182/204]	Loss 0.5607 (0.4943)	
training:	Epoch: [53][183/204]	Loss 0.5739 (0.4947)	
training:	Epoch: [53][184/204]	Loss 0.4983 (0.4947)	
training:	Epoch: [53][185/204]	Loss 0.6507 (0.4956)	
training:	Epoch: [53][186/204]	Loss 0.4805 (0.4955)	
training:	Epoch: [53][187/204]	Loss 0.5893 (0.4960)	
training:	Epoch: [53][188/204]	Loss 0.6423 (0.4967)	
training:	Epoch: [53][189/204]	Loss 0.5061 (0.4968)	
training:	Epoch: [53][190/204]	Loss 0.5868 (0.4973)	
training:	Epoch: [53][191/204]	Loss 0.5939 (0.4978)	
training:	Epoch: [53][192/204]	Loss 0.3843 (0.4972)	
training:	Epoch: [53][193/204]	Loss 0.5991 (0.4977)	
training:	Epoch: [53][194/204]	Loss 0.6132 (0.4983)	
training:	Epoch: [53][195/204]	Loss 0.5161 (0.4984)	
training:	Epoch: [53][196/204]	Loss 0.4090 (0.4979)	
training:	Epoch: [53][197/204]	Loss 0.4116 (0.4975)	
training:	Epoch: [53][198/204]	Loss 0.4546 (0.4973)	
training:	Epoch: [53][199/204]	Loss 0.5052 (0.4973)	
training:	Epoch: [53][200/204]	Loss 0.5693 (0.4977)	
training:	Epoch: [53][201/204]	Loss 0.3732 (0.4971)	
training:	Epoch: [53][202/204]	Loss 0.5943 (0.4976)	
training:	Epoch: [53][203/204]	Loss 0.6006 (0.4981)	
training:	Epoch: [53][204/204]	Loss 0.3827 (0.4975)	
Training:	 Loss: 0.4967

Training:	 ACC: 0.7729 0.7736 0.7907 0.7551
Validation:	 ACC: 0.7680 0.7694 0.7984 0.7377
Validation:	 Best_BACC: 0.7680 0.7694 0.7984 0.7377
Validation:	 Loss: 0.4960
Pretraining:	Epoch 54/120
----------
training:	Epoch: [54][1/204]	Loss 0.5409 (0.5409)	
training:	Epoch: [54][2/204]	Loss 0.5017 (0.5213)	
training:	Epoch: [54][3/204]	Loss 0.6072 (0.5499)	
training:	Epoch: [54][4/204]	Loss 0.5276 (0.5444)	
training:	Epoch: [54][5/204]	Loss 0.5360 (0.5427)	
training:	Epoch: [54][6/204]	Loss 0.5447 (0.5430)	
training:	Epoch: [54][7/204]	Loss 0.5120 (0.5386)	
training:	Epoch: [54][8/204]	Loss 0.5377 (0.5385)	
training:	Epoch: [54][9/204]	Loss 0.5472 (0.5394)	
training:	Epoch: [54][10/204]	Loss 0.5136 (0.5369)	
training:	Epoch: [54][11/204]	Loss 0.3487 (0.5198)	
training:	Epoch: [54][12/204]	Loss 0.4503 (0.5140)	
training:	Epoch: [54][13/204]	Loss 0.5520 (0.5169)	
training:	Epoch: [54][14/204]	Loss 0.4064 (0.5090)	
training:	Epoch: [54][15/204]	Loss 0.4902 (0.5077)	
training:	Epoch: [54][16/204]	Loss 0.5500 (0.5104)	
training:	Epoch: [54][17/204]	Loss 0.5381 (0.5120)	
training:	Epoch: [54][18/204]	Loss 0.5967 (0.5167)	
training:	Epoch: [54][19/204]	Loss 0.6699 (0.5248)	
training:	Epoch: [54][20/204]	Loss 0.4079 (0.5189)	
training:	Epoch: [54][21/204]	Loss 0.6002 (0.5228)	
training:	Epoch: [54][22/204]	Loss 0.5057 (0.5220)	
training:	Epoch: [54][23/204]	Loss 0.4465 (0.5188)	
training:	Epoch: [54][24/204]	Loss 0.4746 (0.5169)	
training:	Epoch: [54][25/204]	Loss 0.3996 (0.5122)	
training:	Epoch: [54][26/204]	Loss 0.4671 (0.5105)	
training:	Epoch: [54][27/204]	Loss 0.5618 (0.5124)	
training:	Epoch: [54][28/204]	Loss 0.4701 (0.5109)	
training:	Epoch: [54][29/204]	Loss 0.6674 (0.5163)	
training:	Epoch: [54][30/204]	Loss 0.6194 (0.5197)	
training:	Epoch: [54][31/204]	Loss 0.4855 (0.5186)	
training:	Epoch: [54][32/204]	Loss 0.3612 (0.5137)	
training:	Epoch: [54][33/204]	Loss 0.3380 (0.5084)	
training:	Epoch: [54][34/204]	Loss 0.4995 (0.5081)	
training:	Epoch: [54][35/204]	Loss 0.3937 (0.5048)	
training:	Epoch: [54][36/204]	Loss 0.3484 (0.5005)	
training:	Epoch: [54][37/204]	Loss 0.6052 (0.5033)	
training:	Epoch: [54][38/204]	Loss 0.5234 (0.5038)	
training:	Epoch: [54][39/204]	Loss 0.5205 (0.5043)	
training:	Epoch: [54][40/204]	Loss 0.4442 (0.5028)	
training:	Epoch: [54][41/204]	Loss 0.4528 (0.5016)	
training:	Epoch: [54][42/204]	Loss 0.5373 (0.5024)	
training:	Epoch: [54][43/204]	Loss 0.5240 (0.5029)	
training:	Epoch: [54][44/204]	Loss 0.5136 (0.5031)	
training:	Epoch: [54][45/204]	Loss 0.5418 (0.5040)	
training:	Epoch: [54][46/204]	Loss 0.4834 (0.5036)	
training:	Epoch: [54][47/204]	Loss 0.5328 (0.5042)	
training:	Epoch: [54][48/204]	Loss 0.4415 (0.5029)	
training:	Epoch: [54][49/204]	Loss 0.6469 (0.5058)	
training:	Epoch: [54][50/204]	Loss 0.6077 (0.5079)	
training:	Epoch: [54][51/204]	Loss 0.5239 (0.5082)	
training:	Epoch: [54][52/204]	Loss 0.6325 (0.5106)	
training:	Epoch: [54][53/204]	Loss 0.4840 (0.5101)	
training:	Epoch: [54][54/204]	Loss 0.4552 (0.5090)	
training:	Epoch: [54][55/204]	Loss 0.4045 (0.5071)	
training:	Epoch: [54][56/204]	Loss 0.4529 (0.5062)	
training:	Epoch: [54][57/204]	Loss 0.5288 (0.5066)	
training:	Epoch: [54][58/204]	Loss 0.4641 (0.5058)	
training:	Epoch: [54][59/204]	Loss 0.4704 (0.5052)	
training:	Epoch: [54][60/204]	Loss 0.4712 (0.5047)	
training:	Epoch: [54][61/204]	Loss 0.4721 (0.5041)	
training:	Epoch: [54][62/204]	Loss 0.5487 (0.5049)	
training:	Epoch: [54][63/204]	Loss 0.5374 (0.5054)	
training:	Epoch: [54][64/204]	Loss 0.4185 (0.5040)	
training:	Epoch: [54][65/204]	Loss 0.4315 (0.5029)	
training:	Epoch: [54][66/204]	Loss 0.5361 (0.5034)	
training:	Epoch: [54][67/204]	Loss 0.4799 (0.5031)	
training:	Epoch: [54][68/204]	Loss 0.4685 (0.5025)	
training:	Epoch: [54][69/204]	Loss 0.5216 (0.5028)	
training:	Epoch: [54][70/204]	Loss 0.4270 (0.5017)	
training:	Epoch: [54][71/204]	Loss 0.5979 (0.5031)	
training:	Epoch: [54][72/204]	Loss 0.4530 (0.5024)	
training:	Epoch: [54][73/204]	Loss 0.4152 (0.5012)	
training:	Epoch: [54][74/204]	Loss 0.4612 (0.5007)	
training:	Epoch: [54][75/204]	Loss 0.3491 (0.4986)	
training:	Epoch: [54][76/204]	Loss 0.5393 (0.4992)	
training:	Epoch: [54][77/204]	Loss 0.4910 (0.4991)	
training:	Epoch: [54][78/204]	Loss 0.4873 (0.4989)	
training:	Epoch: [54][79/204]	Loss 0.4358 (0.4981)	
training:	Epoch: [54][80/204]	Loss 0.4449 (0.4975)	
training:	Epoch: [54][81/204]	Loss 0.4786 (0.4972)	
training:	Epoch: [54][82/204]	Loss 0.6136 (0.4986)	
training:	Epoch: [54][83/204]	Loss 0.7087 (0.5012)	
training:	Epoch: [54][84/204]	Loss 0.5705 (0.5020)	
training:	Epoch: [54][85/204]	Loss 0.3828 (0.5006)	
training:	Epoch: [54][86/204]	Loss 0.3596 (0.4990)	
training:	Epoch: [54][87/204]	Loss 0.5161 (0.4992)	
training:	Epoch: [54][88/204]	Loss 0.4860 (0.4990)	
training:	Epoch: [54][89/204]	Loss 0.5456 (0.4995)	
training:	Epoch: [54][90/204]	Loss 0.4734 (0.4992)	
training:	Epoch: [54][91/204]	Loss 0.5637 (0.4999)	
training:	Epoch: [54][92/204]	Loss 0.4653 (0.4996)	
training:	Epoch: [54][93/204]	Loss 0.4266 (0.4988)	
training:	Epoch: [54][94/204]	Loss 0.5281 (0.4991)	
training:	Epoch: [54][95/204]	Loss 0.5394 (0.4995)	
training:	Epoch: [54][96/204]	Loss 0.5335 (0.4999)	
training:	Epoch: [54][97/204]	Loss 0.3894 (0.4987)	
training:	Epoch: [54][98/204]	Loss 0.5254 (0.4990)	
training:	Epoch: [54][99/204]	Loss 0.5358 (0.4994)	
training:	Epoch: [54][100/204]	Loss 0.5468 (0.4999)	
training:	Epoch: [54][101/204]	Loss 0.5780 (0.5006)	
training:	Epoch: [54][102/204]	Loss 0.5045 (0.5007)	
training:	Epoch: [54][103/204]	Loss 0.4310 (0.5000)	
training:	Epoch: [54][104/204]	Loss 0.5467 (0.5004)	
training:	Epoch: [54][105/204]	Loss 0.5344 (0.5008)	
training:	Epoch: [54][106/204]	Loss 0.5202 (0.5009)	
training:	Epoch: [54][107/204]	Loss 0.6006 (0.5019)	
training:	Epoch: [54][108/204]	Loss 0.4308 (0.5012)	
training:	Epoch: [54][109/204]	Loss 0.3958 (0.5003)	
training:	Epoch: [54][110/204]	Loss 0.4211 (0.4995)	
training:	Epoch: [54][111/204]	Loss 0.5101 (0.4996)	
training:	Epoch: [54][112/204]	Loss 0.4123 (0.4988)	
training:	Epoch: [54][113/204]	Loss 0.5715 (0.4995)	
training:	Epoch: [54][114/204]	Loss 0.6368 (0.5007)	
training:	Epoch: [54][115/204]	Loss 0.4361 (0.5001)	
training:	Epoch: [54][116/204]	Loss 0.4747 (0.4999)	
training:	Epoch: [54][117/204]	Loss 0.6283 (0.5010)	
training:	Epoch: [54][118/204]	Loss 0.4647 (0.5007)	
training:	Epoch: [54][119/204]	Loss 0.4184 (0.5000)	
training:	Epoch: [54][120/204]	Loss 0.4466 (0.4996)	
training:	Epoch: [54][121/204]	Loss 0.4568 (0.4992)	
training:	Epoch: [54][122/204]	Loss 0.5332 (0.4995)	
training:	Epoch: [54][123/204]	Loss 0.5078 (0.4996)	
training:	Epoch: [54][124/204]	Loss 0.5064 (0.4996)	
training:	Epoch: [54][125/204]	Loss 0.4843 (0.4995)	
training:	Epoch: [54][126/204]	Loss 0.4513 (0.4991)	
training:	Epoch: [54][127/204]	Loss 0.5717 (0.4997)	
training:	Epoch: [54][128/204]	Loss 0.4686 (0.4994)	
training:	Epoch: [54][129/204]	Loss 0.5147 (0.4996)	
training:	Epoch: [54][130/204]	Loss 0.4391 (0.4991)	
training:	Epoch: [54][131/204]	Loss 0.4546 (0.4988)	
training:	Epoch: [54][132/204]	Loss 0.3890 (0.4979)	
training:	Epoch: [54][133/204]	Loss 0.5359 (0.4982)	
training:	Epoch: [54][134/204]	Loss 0.5154 (0.4983)	
training:	Epoch: [54][135/204]	Loss 0.4540 (0.4980)	
training:	Epoch: [54][136/204]	Loss 0.5977 (0.4987)	
training:	Epoch: [54][137/204]	Loss 0.3134 (0.4974)	
training:	Epoch: [54][138/204]	Loss 0.5523 (0.4978)	
training:	Epoch: [54][139/204]	Loss 0.4777 (0.4976)	
training:	Epoch: [54][140/204]	Loss 0.4988 (0.4976)	
training:	Epoch: [54][141/204]	Loss 0.5520 (0.4980)	
training:	Epoch: [54][142/204]	Loss 0.4121 (0.4974)	
training:	Epoch: [54][143/204]	Loss 0.5111 (0.4975)	
training:	Epoch: [54][144/204]	Loss 0.5483 (0.4979)	
training:	Epoch: [54][145/204]	Loss 0.4632 (0.4976)	
training:	Epoch: [54][146/204]	Loss 0.4343 (0.4972)	
training:	Epoch: [54][147/204]	Loss 0.4277 (0.4967)	
training:	Epoch: [54][148/204]	Loss 0.6110 (0.4975)	
training:	Epoch: [54][149/204]	Loss 0.4298 (0.4970)	
training:	Epoch: [54][150/204]	Loss 0.4161 (0.4965)	
training:	Epoch: [54][151/204]	Loss 0.6173 (0.4973)	
training:	Epoch: [54][152/204]	Loss 0.5522 (0.4977)	
training:	Epoch: [54][153/204]	Loss 0.5656 (0.4981)	
training:	Epoch: [54][154/204]	Loss 0.4413 (0.4977)	
training:	Epoch: [54][155/204]	Loss 0.4826 (0.4976)	
training:	Epoch: [54][156/204]	Loss 0.3412 (0.4966)	
training:	Epoch: [54][157/204]	Loss 0.5220 (0.4968)	
training:	Epoch: [54][158/204]	Loss 0.5101 (0.4969)	
training:	Epoch: [54][159/204]	Loss 0.5496 (0.4972)	
training:	Epoch: [54][160/204]	Loss 0.5188 (0.4974)	
training:	Epoch: [54][161/204]	Loss 0.5374 (0.4976)	
training:	Epoch: [54][162/204]	Loss 0.3608 (0.4968)	
training:	Epoch: [54][163/204]	Loss 0.4814 (0.4967)	
training:	Epoch: [54][164/204]	Loss 0.4930 (0.4966)	
training:	Epoch: [54][165/204]	Loss 0.5107 (0.4967)	
training:	Epoch: [54][166/204]	Loss 0.5770 (0.4972)	
training:	Epoch: [54][167/204]	Loss 0.6013 (0.4978)	
training:	Epoch: [54][168/204]	Loss 0.4661 (0.4976)	
training:	Epoch: [54][169/204]	Loss 0.4519 (0.4974)	
training:	Epoch: [54][170/204]	Loss 0.4622 (0.4972)	
training:	Epoch: [54][171/204]	Loss 0.4256 (0.4968)	
training:	Epoch: [54][172/204]	Loss 0.5396 (0.4970)	
training:	Epoch: [54][173/204]	Loss 0.4522 (0.4967)	
training:	Epoch: [54][174/204]	Loss 0.5338 (0.4970)	
training:	Epoch: [54][175/204]	Loss 0.4381 (0.4966)	
training:	Epoch: [54][176/204]	Loss 0.4640 (0.4964)	
training:	Epoch: [54][177/204]	Loss 0.4100 (0.4959)	
training:	Epoch: [54][178/204]	Loss 0.5436 (0.4962)	
training:	Epoch: [54][179/204]	Loss 0.4257 (0.4958)	
training:	Epoch: [54][180/204]	Loss 0.5575 (0.4962)	
training:	Epoch: [54][181/204]	Loss 0.5150 (0.4963)	
training:	Epoch: [54][182/204]	Loss 0.5918 (0.4968)	
training:	Epoch: [54][183/204]	Loss 0.4751 (0.4967)	
training:	Epoch: [54][184/204]	Loss 0.5123 (0.4968)	
training:	Epoch: [54][185/204]	Loss 0.7174 (0.4980)	
training:	Epoch: [54][186/204]	Loss 0.5708 (0.4983)	
training:	Epoch: [54][187/204]	Loss 0.7130 (0.4995)	
training:	Epoch: [54][188/204]	Loss 0.6385 (0.5002)	
training:	Epoch: [54][189/204]	Loss 0.4112 (0.4998)	
training:	Epoch: [54][190/204]	Loss 0.4493 (0.4995)	
training:	Epoch: [54][191/204]	Loss 0.4747 (0.4994)	
training:	Epoch: [54][192/204]	Loss 0.4462 (0.4991)	
training:	Epoch: [54][193/204]	Loss 0.6072 (0.4996)	
training:	Epoch: [54][194/204]	Loss 0.4244 (0.4993)	
training:	Epoch: [54][195/204]	Loss 0.4345 (0.4989)	
training:	Epoch: [54][196/204]	Loss 0.5202 (0.4990)	
training:	Epoch: [54][197/204]	Loss 0.4648 (0.4989)	
training:	Epoch: [54][198/204]	Loss 0.4235 (0.4985)	
training:	Epoch: [54][199/204]	Loss 0.4404 (0.4982)	
training:	Epoch: [54][200/204]	Loss 0.4191 (0.4978)	
training:	Epoch: [54][201/204]	Loss 0.3932 (0.4973)	
training:	Epoch: [54][202/204]	Loss 0.4347 (0.4970)	
training:	Epoch: [54][203/204]	Loss 0.4685 (0.4968)	
training:	Epoch: [54][204/204]	Loss 0.4403 (0.4965)	
Training:	 Loss: 0.4958

Training:	 ACC: 0.7741 0.7747 0.7898 0.7583
Validation:	 ACC: 0.7697 0.7710 0.7973 0.7422
Validation:	 Best_BACC: 0.7697 0.7710 0.7973 0.7422
Validation:	 Loss: 0.4941
Pretraining:	Epoch 55/120
----------
training:	Epoch: [55][1/204]	Loss 0.4147 (0.4147)	
training:	Epoch: [55][2/204]	Loss 0.4721 (0.4434)	
training:	Epoch: [55][3/204]	Loss 0.5823 (0.4897)	
training:	Epoch: [55][4/204]	Loss 0.4997 (0.4922)	
training:	Epoch: [55][5/204]	Loss 0.4899 (0.4918)	
training:	Epoch: [55][6/204]	Loss 0.4398 (0.4831)	
training:	Epoch: [55][7/204]	Loss 0.4085 (0.4724)	
training:	Epoch: [55][8/204]	Loss 0.5611 (0.4835)	
training:	Epoch: [55][9/204]	Loss 0.5474 (0.4906)	
training:	Epoch: [55][10/204]	Loss 0.4645 (0.4880)	
training:	Epoch: [55][11/204]	Loss 0.5472 (0.4934)	
training:	Epoch: [55][12/204]	Loss 0.5216 (0.4957)	
training:	Epoch: [55][13/204]	Loss 0.4023 (0.4885)	
training:	Epoch: [55][14/204]	Loss 0.4448 (0.4854)	
training:	Epoch: [55][15/204]	Loss 0.5154 (0.4874)	
training:	Epoch: [55][16/204]	Loss 0.5316 (0.4902)	
training:	Epoch: [55][17/204]	Loss 0.4221 (0.4862)	
training:	Epoch: [55][18/204]	Loss 0.5698 (0.4908)	
training:	Epoch: [55][19/204]	Loss 0.6214 (0.4977)	
training:	Epoch: [55][20/204]	Loss 0.5456 (0.5001)	
training:	Epoch: [55][21/204]	Loss 0.4385 (0.4972)	
training:	Epoch: [55][22/204]	Loss 0.3828 (0.4920)	
training:	Epoch: [55][23/204]	Loss 0.4546 (0.4903)	
training:	Epoch: [55][24/204]	Loss 0.5408 (0.4924)	
training:	Epoch: [55][25/204]	Loss 0.3468 (0.4866)	
training:	Epoch: [55][26/204]	Loss 0.4169 (0.4839)	
training:	Epoch: [55][27/204]	Loss 0.4744 (0.4836)	
training:	Epoch: [55][28/204]	Loss 0.4587 (0.4827)	
training:	Epoch: [55][29/204]	Loss 0.4795 (0.4826)	
training:	Epoch: [55][30/204]	Loss 0.4833 (0.4826)	
training:	Epoch: [55][31/204]	Loss 0.5575 (0.4850)	
training:	Epoch: [55][32/204]	Loss 0.5303 (0.4864)	
training:	Epoch: [55][33/204]	Loss 0.3272 (0.4816)	
training:	Epoch: [55][34/204]	Loss 0.4345 (0.4802)	
training:	Epoch: [55][35/204]	Loss 0.5938 (0.4835)	
training:	Epoch: [55][36/204]	Loss 0.4249 (0.4818)	
training:	Epoch: [55][37/204]	Loss 0.5341 (0.4833)	
training:	Epoch: [55][38/204]	Loss 0.6152 (0.4867)	
training:	Epoch: [55][39/204]	Loss 0.4781 (0.4865)	
training:	Epoch: [55][40/204]	Loss 0.4084 (0.4846)	
training:	Epoch: [55][41/204]	Loss 0.4984 (0.4849)	
training:	Epoch: [55][42/204]	Loss 0.5031 (0.4853)	
training:	Epoch: [55][43/204]	Loss 0.4389 (0.4842)	
training:	Epoch: [55][44/204]	Loss 0.3977 (0.4823)	
training:	Epoch: [55][45/204]	Loss 0.5535 (0.4839)	
training:	Epoch: [55][46/204]	Loss 0.4702 (0.4836)	
training:	Epoch: [55][47/204]	Loss 0.3587 (0.4809)	
training:	Epoch: [55][48/204]	Loss 0.4558 (0.4804)	
training:	Epoch: [55][49/204]	Loss 0.5663 (0.4821)	
training:	Epoch: [55][50/204]	Loss 0.6177 (0.4848)	
training:	Epoch: [55][51/204]	Loss 0.6590 (0.4883)	
training:	Epoch: [55][52/204]	Loss 0.3386 (0.4854)	
training:	Epoch: [55][53/204]	Loss 0.5270 (0.4862)	
training:	Epoch: [55][54/204]	Loss 0.4608 (0.4857)	
training:	Epoch: [55][55/204]	Loss 0.6367 (0.4884)	
training:	Epoch: [55][56/204]	Loss 0.4337 (0.4875)	
training:	Epoch: [55][57/204]	Loss 0.5357 (0.4883)	
training:	Epoch: [55][58/204]	Loss 0.5940 (0.4901)	
training:	Epoch: [55][59/204]	Loss 0.3736 (0.4882)	
training:	Epoch: [55][60/204]	Loss 0.4842 (0.4881)	
training:	Epoch: [55][61/204]	Loss 0.4662 (0.4877)	
training:	Epoch: [55][62/204]	Loss 0.5192 (0.4882)	
training:	Epoch: [55][63/204]	Loss 0.4535 (0.4877)	
training:	Epoch: [55][64/204]	Loss 0.5639 (0.4889)	
training:	Epoch: [55][65/204]	Loss 0.3458 (0.4867)	
training:	Epoch: [55][66/204]	Loss 0.4131 (0.4856)	
training:	Epoch: [55][67/204]	Loss 0.5713 (0.4868)	
training:	Epoch: [55][68/204]	Loss 0.4838 (0.4868)	
training:	Epoch: [55][69/204]	Loss 0.5032 (0.4870)	
training:	Epoch: [55][70/204]	Loss 0.5833 (0.4884)	
training:	Epoch: [55][71/204]	Loss 0.4134 (0.4874)	
training:	Epoch: [55][72/204]	Loss 0.3985 (0.4861)	
training:	Epoch: [55][73/204]	Loss 0.4514 (0.4856)	
training:	Epoch: [55][74/204]	Loss 0.5392 (0.4864)	
training:	Epoch: [55][75/204]	Loss 0.5348 (0.4870)	
training:	Epoch: [55][76/204]	Loss 0.4283 (0.4862)	
training:	Epoch: [55][77/204]	Loss 0.4362 (0.4856)	
training:	Epoch: [55][78/204]	Loss 0.5265 (0.4861)	
training:	Epoch: [55][79/204]	Loss 0.3359 (0.4842)	
training:	Epoch: [55][80/204]	Loss 0.4276 (0.4835)	
training:	Epoch: [55][81/204]	Loss 0.4407 (0.4830)	
training:	Epoch: [55][82/204]	Loss 0.4010 (0.4820)	
training:	Epoch: [55][83/204]	Loss 0.3978 (0.4810)	
training:	Epoch: [55][84/204]	Loss 0.4672 (0.4808)	
training:	Epoch: [55][85/204]	Loss 0.4602 (0.4806)	
training:	Epoch: [55][86/204]	Loss 0.6648 (0.4827)	
training:	Epoch: [55][87/204]	Loss 0.4541 (0.4824)	
training:	Epoch: [55][88/204]	Loss 0.6328 (0.4841)	
training:	Epoch: [55][89/204]	Loss 0.3939 (0.4831)	
training:	Epoch: [55][90/204]	Loss 0.4597 (0.4828)	
training:	Epoch: [55][91/204]	Loss 0.6215 (0.4843)	
training:	Epoch: [55][92/204]	Loss 0.4854 (0.4843)	
training:	Epoch: [55][93/204]	Loss 0.5137 (0.4847)	
training:	Epoch: [55][94/204]	Loss 0.5998 (0.4859)	
training:	Epoch: [55][95/204]	Loss 0.5896 (0.4870)	
training:	Epoch: [55][96/204]	Loss 0.4785 (0.4869)	
training:	Epoch: [55][97/204]	Loss 0.4716 (0.4867)	
training:	Epoch: [55][98/204]	Loss 0.6276 (0.4882)	
training:	Epoch: [55][99/204]	Loss 0.4395 (0.4877)	
training:	Epoch: [55][100/204]	Loss 0.5048 (0.4878)	
training:	Epoch: [55][101/204]	Loss 0.5945 (0.4889)	
training:	Epoch: [55][102/204]	Loss 0.5343 (0.4894)	
training:	Epoch: [55][103/204]	Loss 0.4850 (0.4893)	
training:	Epoch: [55][104/204]	Loss 0.5365 (0.4898)	
training:	Epoch: [55][105/204]	Loss 0.5252 (0.4901)	
training:	Epoch: [55][106/204]	Loss 0.4979 (0.4902)	
training:	Epoch: [55][107/204]	Loss 0.5802 (0.4910)	
training:	Epoch: [55][108/204]	Loss 0.3736 (0.4899)	
training:	Epoch: [55][109/204]	Loss 0.5954 (0.4909)	
training:	Epoch: [55][110/204]	Loss 0.5548 (0.4915)	
training:	Epoch: [55][111/204]	Loss 0.5444 (0.4920)	
training:	Epoch: [55][112/204]	Loss 0.4605 (0.4917)	
training:	Epoch: [55][113/204]	Loss 0.5165 (0.4919)	
training:	Epoch: [55][114/204]	Loss 0.5853 (0.4927)	
training:	Epoch: [55][115/204]	Loss 0.6960 (0.4945)	
training:	Epoch: [55][116/204]	Loss 0.4093 (0.4937)	
training:	Epoch: [55][117/204]	Loss 0.4778 (0.4936)	
training:	Epoch: [55][118/204]	Loss 0.6392 (0.4948)	
training:	Epoch: [55][119/204]	Loss 0.5283 (0.4951)	
training:	Epoch: [55][120/204]	Loss 0.3623 (0.4940)	
training:	Epoch: [55][121/204]	Loss 0.5144 (0.4942)	
training:	Epoch: [55][122/204]	Loss 0.4753 (0.4940)	
training:	Epoch: [55][123/204]	Loss 0.4599 (0.4938)	
training:	Epoch: [55][124/204]	Loss 0.4825 (0.4937)	
training:	Epoch: [55][125/204]	Loss 0.4445 (0.4933)	
training:	Epoch: [55][126/204]	Loss 0.5962 (0.4941)	
training:	Epoch: [55][127/204]	Loss 0.3360 (0.4928)	
training:	Epoch: [55][128/204]	Loss 0.5613 (0.4934)	
training:	Epoch: [55][129/204]	Loss 0.5728 (0.4940)	
training:	Epoch: [55][130/204]	Loss 0.5328 (0.4943)	
training:	Epoch: [55][131/204]	Loss 0.5097 (0.4944)	
training:	Epoch: [55][132/204]	Loss 0.5087 (0.4945)	
training:	Epoch: [55][133/204]	Loss 0.3274 (0.4933)	
training:	Epoch: [55][134/204]	Loss 0.4938 (0.4933)	
training:	Epoch: [55][135/204]	Loss 0.5583 (0.4937)	
training:	Epoch: [55][136/204]	Loss 0.6160 (0.4946)	
training:	Epoch: [55][137/204]	Loss 0.5228 (0.4949)	
training:	Epoch: [55][138/204]	Loss 0.6379 (0.4959)	
training:	Epoch: [55][139/204]	Loss 0.5867 (0.4965)	
training:	Epoch: [55][140/204]	Loss 0.3768 (0.4957)	
training:	Epoch: [55][141/204]	Loss 0.4461 (0.4953)	
training:	Epoch: [55][142/204]	Loss 0.4088 (0.4947)	
training:	Epoch: [55][143/204]	Loss 0.5304 (0.4950)	
training:	Epoch: [55][144/204]	Loss 0.5880 (0.4956)	
training:	Epoch: [55][145/204]	Loss 0.3746 (0.4948)	
training:	Epoch: [55][146/204]	Loss 0.4837 (0.4947)	
training:	Epoch: [55][147/204]	Loss 0.4164 (0.4942)	
training:	Epoch: [55][148/204]	Loss 0.3860 (0.4934)	
training:	Epoch: [55][149/204]	Loss 0.3275 (0.4923)	
training:	Epoch: [55][150/204]	Loss 0.4816 (0.4923)	
training:	Epoch: [55][151/204]	Loss 0.6509 (0.4933)	
training:	Epoch: [55][152/204]	Loss 0.4533 (0.4930)	
training:	Epoch: [55][153/204]	Loss 0.5618 (0.4935)	
training:	Epoch: [55][154/204]	Loss 0.5136 (0.4936)	
training:	Epoch: [55][155/204]	Loss 0.4150 (0.4931)	
training:	Epoch: [55][156/204]	Loss 0.4765 (0.4930)	
training:	Epoch: [55][157/204]	Loss 0.5807 (0.4936)	
training:	Epoch: [55][158/204]	Loss 0.3742 (0.4928)	
training:	Epoch: [55][159/204]	Loss 0.4748 (0.4927)	
training:	Epoch: [55][160/204]	Loss 0.4906 (0.4927)	
training:	Epoch: [55][161/204]	Loss 0.3682 (0.4919)	
training:	Epoch: [55][162/204]	Loss 0.4080 (0.4914)	
training:	Epoch: [55][163/204]	Loss 0.5632 (0.4918)	
training:	Epoch: [55][164/204]	Loss 0.5928 (0.4925)	
training:	Epoch: [55][165/204]	Loss 0.3899 (0.4918)	
training:	Epoch: [55][166/204]	Loss 0.3857 (0.4912)	
training:	Epoch: [55][167/204]	Loss 0.4833 (0.4911)	
training:	Epoch: [55][168/204]	Loss 0.5692 (0.4916)	
training:	Epoch: [55][169/204]	Loss 0.6274 (0.4924)	
training:	Epoch: [55][170/204]	Loss 0.4771 (0.4923)	
training:	Epoch: [55][171/204]	Loss 0.3532 (0.4915)	
training:	Epoch: [55][172/204]	Loss 0.5588 (0.4919)	
training:	Epoch: [55][173/204]	Loss 0.5314 (0.4921)	
training:	Epoch: [55][174/204]	Loss 0.3963 (0.4916)	
training:	Epoch: [55][175/204]	Loss 0.4709 (0.4915)	
training:	Epoch: [55][176/204]	Loss 0.5472 (0.4918)	
training:	Epoch: [55][177/204]	Loss 0.6647 (0.4928)	
training:	Epoch: [55][178/204]	Loss 0.6324 (0.4935)	
training:	Epoch: [55][179/204]	Loss 0.4551 (0.4933)	
training:	Epoch: [55][180/204]	Loss 0.5119 (0.4934)	
training:	Epoch: [55][181/204]	Loss 0.4139 (0.4930)	
training:	Epoch: [55][182/204]	Loss 0.5007 (0.4930)	
training:	Epoch: [55][183/204]	Loss 0.4931 (0.4930)	
training:	Epoch: [55][184/204]	Loss 0.5982 (0.4936)	
training:	Epoch: [55][185/204]	Loss 0.4772 (0.4935)	
training:	Epoch: [55][186/204]	Loss 0.4765 (0.4934)	
training:	Epoch: [55][187/204]	Loss 0.6969 (0.4945)	
training:	Epoch: [55][188/204]	Loss 0.4568 (0.4943)	
training:	Epoch: [55][189/204]	Loss 0.4248 (0.4939)	
training:	Epoch: [55][190/204]	Loss 0.4467 (0.4937)	
training:	Epoch: [55][191/204]	Loss 0.4040 (0.4932)	
training:	Epoch: [55][192/204]	Loss 0.6161 (0.4939)	
training:	Epoch: [55][193/204]	Loss 0.5931 (0.4944)	
training:	Epoch: [55][194/204]	Loss 0.7255 (0.4956)	
training:	Epoch: [55][195/204]	Loss 0.4939 (0.4956)	
training:	Epoch: [55][196/204]	Loss 0.5359 (0.4958)	
training:	Epoch: [55][197/204]	Loss 0.3338 (0.4949)	
training:	Epoch: [55][198/204]	Loss 0.5678 (0.4953)	
training:	Epoch: [55][199/204]	Loss 0.4976 (0.4953)	
training:	Epoch: [55][200/204]	Loss 0.5042 (0.4954)	
training:	Epoch: [55][201/204]	Loss 0.3435 (0.4946)	
training:	Epoch: [55][202/204]	Loss 0.4427 (0.4944)	
training:	Epoch: [55][203/204]	Loss 0.7140 (0.4954)	
training:	Epoch: [55][204/204]	Loss 0.6017 (0.4960)	
Training:	 Loss: 0.4952

Training:	 ACC: 0.7760 0.7767 0.7925 0.7596
Validation:	 ACC: 0.7719 0.7731 0.7994 0.7444
Validation:	 Best_BACC: 0.7719 0.7731 0.7994 0.7444
Validation:	 Loss: 0.4921
Pretraining:	Epoch 56/120
----------
training:	Epoch: [56][1/204]	Loss 0.5578 (0.5578)	
training:	Epoch: [56][2/204]	Loss 0.5436 (0.5507)	
training:	Epoch: [56][3/204]	Loss 0.4474 (0.5162)	
training:	Epoch: [56][4/204]	Loss 0.6270 (0.5439)	
training:	Epoch: [56][5/204]	Loss 0.6197 (0.5591)	
training:	Epoch: [56][6/204]	Loss 0.3964 (0.5320)	
training:	Epoch: [56][7/204]	Loss 0.5184 (0.5300)	
training:	Epoch: [56][8/204]	Loss 0.3442 (0.5068)	
training:	Epoch: [56][9/204]	Loss 0.4793 (0.5037)	
training:	Epoch: [56][10/204]	Loss 0.4357 (0.4969)	
training:	Epoch: [56][11/204]	Loss 0.4758 (0.4950)	
training:	Epoch: [56][12/204]	Loss 0.4578 (0.4919)	
training:	Epoch: [56][13/204]	Loss 0.4957 (0.4922)	
training:	Epoch: [56][14/204]	Loss 0.4663 (0.4904)	
training:	Epoch: [56][15/204]	Loss 0.5862 (0.4968)	
training:	Epoch: [56][16/204]	Loss 0.5510 (0.5002)	
training:	Epoch: [56][17/204]	Loss 0.4591 (0.4977)	
training:	Epoch: [56][18/204]	Loss 0.5456 (0.5004)	
training:	Epoch: [56][19/204]	Loss 0.4103 (0.4957)	
training:	Epoch: [56][20/204]	Loss 0.4374 (0.4927)	
training:	Epoch: [56][21/204]	Loss 0.3740 (0.4871)	
training:	Epoch: [56][22/204]	Loss 0.4996 (0.4877)	
training:	Epoch: [56][23/204]	Loss 0.4964 (0.4880)	
training:	Epoch: [56][24/204]	Loss 0.5221 (0.4895)	
training:	Epoch: [56][25/204]	Loss 0.3199 (0.4827)	
training:	Epoch: [56][26/204]	Loss 0.5708 (0.4861)	
training:	Epoch: [56][27/204]	Loss 0.5673 (0.4891)	
training:	Epoch: [56][28/204]	Loss 0.5551 (0.4914)	
training:	Epoch: [56][29/204]	Loss 0.5428 (0.4932)	
training:	Epoch: [56][30/204]	Loss 0.5078 (0.4937)	
training:	Epoch: [56][31/204]	Loss 0.5339 (0.4950)	
training:	Epoch: [56][32/204]	Loss 0.5039 (0.4953)	
training:	Epoch: [56][33/204]	Loss 0.4387 (0.4935)	
training:	Epoch: [56][34/204]	Loss 0.4419 (0.4920)	
training:	Epoch: [56][35/204]	Loss 0.5170 (0.4927)	
training:	Epoch: [56][36/204]	Loss 0.4921 (0.4927)	
training:	Epoch: [56][37/204]	Loss 0.6825 (0.4979)	
training:	Epoch: [56][38/204]	Loss 0.5717 (0.4998)	
training:	Epoch: [56][39/204]	Loss 0.3517 (0.4960)	
training:	Epoch: [56][40/204]	Loss 0.3933 (0.4934)	
training:	Epoch: [56][41/204]	Loss 0.4129 (0.4915)	
training:	Epoch: [56][42/204]	Loss 0.4359 (0.4901)	
training:	Epoch: [56][43/204]	Loss 0.3267 (0.4863)	
training:	Epoch: [56][44/204]	Loss 0.3957 (0.4843)	
training:	Epoch: [56][45/204]	Loss 0.5770 (0.4863)	
training:	Epoch: [56][46/204]	Loss 0.3450 (0.4833)	
training:	Epoch: [56][47/204]	Loss 0.4219 (0.4820)	
training:	Epoch: [56][48/204]	Loss 0.5336 (0.4830)	
training:	Epoch: [56][49/204]	Loss 0.4900 (0.4832)	
training:	Epoch: [56][50/204]	Loss 0.4497 (0.4825)	
training:	Epoch: [56][51/204]	Loss 0.3780 (0.4805)	
training:	Epoch: [56][52/204]	Loss 0.6054 (0.4829)	
training:	Epoch: [56][53/204]	Loss 0.5849 (0.4848)	
training:	Epoch: [56][54/204]	Loss 0.7364 (0.4895)	
training:	Epoch: [56][55/204]	Loss 0.5008 (0.4897)	
training:	Epoch: [56][56/204]	Loss 0.4898 (0.4897)	
training:	Epoch: [56][57/204]	Loss 0.3961 (0.4880)	
training:	Epoch: [56][58/204]	Loss 0.4871 (0.4880)	
training:	Epoch: [56][59/204]	Loss 0.5292 (0.4887)	
training:	Epoch: [56][60/204]	Loss 0.5726 (0.4901)	
training:	Epoch: [56][61/204]	Loss 0.4641 (0.4897)	
training:	Epoch: [56][62/204]	Loss 0.5532 (0.4907)	
training:	Epoch: [56][63/204]	Loss 0.5132 (0.4911)	
training:	Epoch: [56][64/204]	Loss 0.4742 (0.4908)	
training:	Epoch: [56][65/204]	Loss 0.3962 (0.4893)	
training:	Epoch: [56][66/204]	Loss 0.5181 (0.4898)	
training:	Epoch: [56][67/204]	Loss 0.5435 (0.4906)	
training:	Epoch: [56][68/204]	Loss 0.3941 (0.4892)	
training:	Epoch: [56][69/204]	Loss 0.5689 (0.4903)	
training:	Epoch: [56][70/204]	Loss 0.4068 (0.4891)	
training:	Epoch: [56][71/204]	Loss 0.4779 (0.4890)	
training:	Epoch: [56][72/204]	Loss 0.4021 (0.4878)	
training:	Epoch: [56][73/204]	Loss 0.5612 (0.4888)	
training:	Epoch: [56][74/204]	Loss 0.4274 (0.4879)	
training:	Epoch: [56][75/204]	Loss 0.4073 (0.4869)	
training:	Epoch: [56][76/204]	Loss 0.4828 (0.4868)	
training:	Epoch: [56][77/204]	Loss 0.6735 (0.4892)	
training:	Epoch: [56][78/204]	Loss 0.4097 (0.4882)	
training:	Epoch: [56][79/204]	Loss 0.4130 (0.4873)	
training:	Epoch: [56][80/204]	Loss 0.5144 (0.4876)	
training:	Epoch: [56][81/204]	Loss 0.3809 (0.4863)	
training:	Epoch: [56][82/204]	Loss 0.4372 (0.4857)	
training:	Epoch: [56][83/204]	Loss 0.4522 (0.4853)	
training:	Epoch: [56][84/204]	Loss 0.5136 (0.4856)	
training:	Epoch: [56][85/204]	Loss 0.4325 (0.4850)	
training:	Epoch: [56][86/204]	Loss 0.4623 (0.4847)	
training:	Epoch: [56][87/204]	Loss 0.3386 (0.4830)	
training:	Epoch: [56][88/204]	Loss 0.4608 (0.4828)	
training:	Epoch: [56][89/204]	Loss 0.5136 (0.4831)	
training:	Epoch: [56][90/204]	Loss 0.5965 (0.4844)	
training:	Epoch: [56][91/204]	Loss 0.4634 (0.4842)	
training:	Epoch: [56][92/204]	Loss 0.4361 (0.4836)	
training:	Epoch: [56][93/204]	Loss 0.4552 (0.4833)	
training:	Epoch: [56][94/204]	Loss 0.4809 (0.4833)	
training:	Epoch: [56][95/204]	Loss 0.6621 (0.4852)	
training:	Epoch: [56][96/204]	Loss 0.4153 (0.4845)	
training:	Epoch: [56][97/204]	Loss 0.5481 (0.4851)	
training:	Epoch: [56][98/204]	Loss 0.5034 (0.4853)	
training:	Epoch: [56][99/204]	Loss 0.5964 (0.4864)	
training:	Epoch: [56][100/204]	Loss 0.4142 (0.4857)	
training:	Epoch: [56][101/204]	Loss 0.4198 (0.4851)	
training:	Epoch: [56][102/204]	Loss 0.4904 (0.4851)	
training:	Epoch: [56][103/204]	Loss 0.5011 (0.4853)	
training:	Epoch: [56][104/204]	Loss 0.6684 (0.4870)	
training:	Epoch: [56][105/204]	Loss 0.4339 (0.4865)	
training:	Epoch: [56][106/204]	Loss 0.4345 (0.4860)	
training:	Epoch: [56][107/204]	Loss 0.4359 (0.4856)	
training:	Epoch: [56][108/204]	Loss 0.4533 (0.4853)	
training:	Epoch: [56][109/204]	Loss 0.4247 (0.4847)	
training:	Epoch: [56][110/204]	Loss 0.3708 (0.4837)	
training:	Epoch: [56][111/204]	Loss 0.5543 (0.4843)	
training:	Epoch: [56][112/204]	Loss 0.4504 (0.4840)	
training:	Epoch: [56][113/204]	Loss 0.6608 (0.4856)	
training:	Epoch: [56][114/204]	Loss 0.4405 (0.4852)	
training:	Epoch: [56][115/204]	Loss 0.4518 (0.4849)	
training:	Epoch: [56][116/204]	Loss 0.5043 (0.4850)	
training:	Epoch: [56][117/204]	Loss 0.5064 (0.4852)	
training:	Epoch: [56][118/204]	Loss 0.5756 (0.4860)	
training:	Epoch: [56][119/204]	Loss 0.4546 (0.4857)	
training:	Epoch: [56][120/204]	Loss 0.5167 (0.4860)	
training:	Epoch: [56][121/204]	Loss 0.6019 (0.4869)	
training:	Epoch: [56][122/204]	Loss 0.4025 (0.4863)	
training:	Epoch: [56][123/204]	Loss 0.5256 (0.4866)	
training:	Epoch: [56][124/204]	Loss 0.5191 (0.4868)	
training:	Epoch: [56][125/204]	Loss 0.3865 (0.4860)	
training:	Epoch: [56][126/204]	Loss 0.4412 (0.4857)	
training:	Epoch: [56][127/204]	Loss 0.4127 (0.4851)	
training:	Epoch: [56][128/204]	Loss 0.4984 (0.4852)	
training:	Epoch: [56][129/204]	Loss 0.4899 (0.4852)	
training:	Epoch: [56][130/204]	Loss 0.4903 (0.4853)	
training:	Epoch: [56][131/204]	Loss 0.5325 (0.4856)	
training:	Epoch: [56][132/204]	Loss 0.4384 (0.4853)	
training:	Epoch: [56][133/204]	Loss 0.4470 (0.4850)	
training:	Epoch: [56][134/204]	Loss 0.5042 (0.4851)	
training:	Epoch: [56][135/204]	Loss 0.4943 (0.4852)	
training:	Epoch: [56][136/204]	Loss 0.4815 (0.4852)	
training:	Epoch: [56][137/204]	Loss 0.5138 (0.4854)	
training:	Epoch: [56][138/204]	Loss 0.5232 (0.4857)	
training:	Epoch: [56][139/204]	Loss 0.4388 (0.4853)	
training:	Epoch: [56][140/204]	Loss 0.5273 (0.4856)	
training:	Epoch: [56][141/204]	Loss 0.4217 (0.4852)	
training:	Epoch: [56][142/204]	Loss 0.4555 (0.4850)	
training:	Epoch: [56][143/204]	Loss 0.3410 (0.4840)	
training:	Epoch: [56][144/204]	Loss 0.6513 (0.4851)	
training:	Epoch: [56][145/204]	Loss 0.6417 (0.4862)	
training:	Epoch: [56][146/204]	Loss 0.3660 (0.4854)	
training:	Epoch: [56][147/204]	Loss 0.5076 (0.4855)	
training:	Epoch: [56][148/204]	Loss 0.6785 (0.4868)	
training:	Epoch: [56][149/204]	Loss 0.4591 (0.4866)	
training:	Epoch: [56][150/204]	Loss 0.4004 (0.4861)	
training:	Epoch: [56][151/204]	Loss 0.5080 (0.4862)	
training:	Epoch: [56][152/204]	Loss 0.5009 (0.4863)	
training:	Epoch: [56][153/204]	Loss 0.3357 (0.4853)	
training:	Epoch: [56][154/204]	Loss 0.4227 (0.4849)	
training:	Epoch: [56][155/204]	Loss 0.5379 (0.4853)	
training:	Epoch: [56][156/204]	Loss 0.4994 (0.4854)	
training:	Epoch: [56][157/204]	Loss 0.6074 (0.4861)	
training:	Epoch: [56][158/204]	Loss 0.5876 (0.4868)	
training:	Epoch: [56][159/204]	Loss 0.4925 (0.4868)	
training:	Epoch: [56][160/204]	Loss 0.3672 (0.4861)	
training:	Epoch: [56][161/204]	Loss 0.3710 (0.4853)	
training:	Epoch: [56][162/204]	Loss 0.4960 (0.4854)	
training:	Epoch: [56][163/204]	Loss 0.3618 (0.4847)	
training:	Epoch: [56][164/204]	Loss 0.5522 (0.4851)	
training:	Epoch: [56][165/204]	Loss 0.5589 (0.4855)	
training:	Epoch: [56][166/204]	Loss 0.4754 (0.4855)	
training:	Epoch: [56][167/204]	Loss 0.5465 (0.4858)	
training:	Epoch: [56][168/204]	Loss 0.5294 (0.4861)	
training:	Epoch: [56][169/204]	Loss 0.5373 (0.4864)	
training:	Epoch: [56][170/204]	Loss 0.4293 (0.4860)	
training:	Epoch: [56][171/204]	Loss 0.4056 (0.4856)	
training:	Epoch: [56][172/204]	Loss 0.4622 (0.4854)	
training:	Epoch: [56][173/204]	Loss 0.4428 (0.4852)	
training:	Epoch: [56][174/204]	Loss 0.4642 (0.4851)	
training:	Epoch: [56][175/204]	Loss 0.5393 (0.4854)	
training:	Epoch: [56][176/204]	Loss 0.7704 (0.4870)	
training:	Epoch: [56][177/204]	Loss 0.5438 (0.4873)	
training:	Epoch: [56][178/204]	Loss 0.4792 (0.4873)	
training:	Epoch: [56][179/204]	Loss 0.4335 (0.4870)	
training:	Epoch: [56][180/204]	Loss 0.4439 (0.4867)	
training:	Epoch: [56][181/204]	Loss 0.4191 (0.4864)	
training:	Epoch: [56][182/204]	Loss 0.6208 (0.4871)	
training:	Epoch: [56][183/204]	Loss 0.4604 (0.4870)	
training:	Epoch: [56][184/204]	Loss 0.3121 (0.4860)	
training:	Epoch: [56][185/204]	Loss 0.3999 (0.4855)	
training:	Epoch: [56][186/204]	Loss 0.6040 (0.4862)	
training:	Epoch: [56][187/204]	Loss 0.4300 (0.4859)	
training:	Epoch: [56][188/204]	Loss 0.5759 (0.4864)	
training:	Epoch: [56][189/204]	Loss 0.6498 (0.4872)	
training:	Epoch: [56][190/204]	Loss 0.5181 (0.4874)	
training:	Epoch: [56][191/204]	Loss 0.5293 (0.4876)	
training:	Epoch: [56][192/204]	Loss 0.5322 (0.4878)	
training:	Epoch: [56][193/204]	Loss 0.5468 (0.4881)	
training:	Epoch: [56][194/204]	Loss 0.5553 (0.4885)	
training:	Epoch: [56][195/204]	Loss 0.5380 (0.4887)	
training:	Epoch: [56][196/204]	Loss 0.3894 (0.4882)	
training:	Epoch: [56][197/204]	Loss 0.4595 (0.4881)	
training:	Epoch: [56][198/204]	Loss 0.5631 (0.4885)	
training:	Epoch: [56][199/204]	Loss 0.4337 (0.4882)	
training:	Epoch: [56][200/204]	Loss 0.4968 (0.4882)	
training:	Epoch: [56][201/204]	Loss 0.3997 (0.4878)	
training:	Epoch: [56][202/204]	Loss 0.4798 (0.4878)	
training:	Epoch: [56][203/204]	Loss 0.5642 (0.4881)	
training:	Epoch: [56][204/204]	Loss 0.6964 (0.4892)	
Training:	 Loss: 0.4884

Training:	 ACC: 0.7776 0.7782 0.7937 0.7615
Validation:	 ACC: 0.7709 0.7721 0.7973 0.7444
Validation:	 Best_BACC: 0.7719 0.7731 0.7994 0.7444
Validation:	 Loss: 0.4904
Pretraining:	Epoch 57/120
----------
training:	Epoch: [57][1/204]	Loss 0.4275 (0.4275)	
training:	Epoch: [57][2/204]	Loss 0.3864 (0.4069)	
training:	Epoch: [57][3/204]	Loss 0.4140 (0.4093)	
training:	Epoch: [57][4/204]	Loss 0.4553 (0.4208)	
training:	Epoch: [57][5/204]	Loss 0.4659 (0.4298)	
training:	Epoch: [57][6/204]	Loss 0.5031 (0.4420)	
training:	Epoch: [57][7/204]	Loss 0.4435 (0.4422)	
training:	Epoch: [57][8/204]	Loss 0.4479 (0.4429)	
training:	Epoch: [57][9/204]	Loss 0.5474 (0.4545)	
training:	Epoch: [57][10/204]	Loss 0.6966 (0.4787)	
training:	Epoch: [57][11/204]	Loss 0.4515 (0.4763)	
training:	Epoch: [57][12/204]	Loss 0.3425 (0.4651)	
training:	Epoch: [57][13/204]	Loss 0.5200 (0.4693)	
training:	Epoch: [57][14/204]	Loss 0.5191 (0.4729)	
training:	Epoch: [57][15/204]	Loss 0.4235 (0.4696)	
training:	Epoch: [57][16/204]	Loss 0.5206 (0.4728)	
training:	Epoch: [57][17/204]	Loss 0.7068 (0.4866)	
training:	Epoch: [57][18/204]	Loss 0.4731 (0.4858)	
training:	Epoch: [57][19/204]	Loss 0.6024 (0.4919)	
training:	Epoch: [57][20/204]	Loss 0.3628 (0.4855)	
training:	Epoch: [57][21/204]	Loss 0.3956 (0.4812)	
training:	Epoch: [57][22/204]	Loss 0.3995 (0.4775)	
training:	Epoch: [57][23/204]	Loss 0.4449 (0.4761)	
training:	Epoch: [57][24/204]	Loss 0.4247 (0.4739)	
training:	Epoch: [57][25/204]	Loss 0.5518 (0.4771)	
training:	Epoch: [57][26/204]	Loss 0.4554 (0.4762)	
training:	Epoch: [57][27/204]	Loss 0.5898 (0.4804)	
training:	Epoch: [57][28/204]	Loss 0.6360 (0.4860)	
training:	Epoch: [57][29/204]	Loss 0.4526 (0.4848)	
training:	Epoch: [57][30/204]	Loss 0.3633 (0.4808)	
training:	Epoch: [57][31/204]	Loss 0.4122 (0.4786)	
training:	Epoch: [57][32/204]	Loss 0.4184 (0.4767)	
training:	Epoch: [57][33/204]	Loss 0.4923 (0.4772)	
training:	Epoch: [57][34/204]	Loss 0.5617 (0.4796)	
training:	Epoch: [57][35/204]	Loss 0.4710 (0.4794)	
training:	Epoch: [57][36/204]	Loss 0.5244 (0.4807)	
training:	Epoch: [57][37/204]	Loss 0.5080 (0.4814)	
training:	Epoch: [57][38/204]	Loss 0.4845 (0.4815)	
training:	Epoch: [57][39/204]	Loss 0.3728 (0.4787)	
training:	Epoch: [57][40/204]	Loss 0.5050 (0.4793)	
training:	Epoch: [57][41/204]	Loss 0.4741 (0.4792)	
training:	Epoch: [57][42/204]	Loss 0.6060 (0.4822)	
training:	Epoch: [57][43/204]	Loss 0.5390 (0.4836)	
training:	Epoch: [57][44/204]	Loss 0.3854 (0.4813)	
training:	Epoch: [57][45/204]	Loss 0.4537 (0.4807)	
training:	Epoch: [57][46/204]	Loss 0.4588 (0.4802)	
training:	Epoch: [57][47/204]	Loss 0.4268 (0.4791)	
training:	Epoch: [57][48/204]	Loss 0.4865 (0.4792)	
training:	Epoch: [57][49/204]	Loss 0.4888 (0.4794)	
training:	Epoch: [57][50/204]	Loss 0.4905 (0.4797)	
training:	Epoch: [57][51/204]	Loss 0.5011 (0.4801)	
training:	Epoch: [57][52/204]	Loss 0.4768 (0.4800)	
training:	Epoch: [57][53/204]	Loss 0.5679 (0.4817)	
training:	Epoch: [57][54/204]	Loss 0.4022 (0.4802)	
training:	Epoch: [57][55/204]	Loss 0.4020 (0.4788)	
training:	Epoch: [57][56/204]	Loss 0.5102 (0.4793)	
training:	Epoch: [57][57/204]	Loss 0.3311 (0.4767)	
training:	Epoch: [57][58/204]	Loss 0.5520 (0.4780)	
training:	Epoch: [57][59/204]	Loss 0.5483 (0.4792)	
training:	Epoch: [57][60/204]	Loss 0.3046 (0.4763)	
training:	Epoch: [57][61/204]	Loss 0.4416 (0.4758)	
training:	Epoch: [57][62/204]	Loss 0.5036 (0.4762)	
training:	Epoch: [57][63/204]	Loss 0.5462 (0.4773)	
training:	Epoch: [57][64/204]	Loss 0.5477 (0.4784)	
training:	Epoch: [57][65/204]	Loss 0.3730 (0.4768)	
training:	Epoch: [57][66/204]	Loss 0.4856 (0.4769)	
training:	Epoch: [57][67/204]	Loss 0.6063 (0.4789)	
training:	Epoch: [57][68/204]	Loss 0.3925 (0.4776)	
training:	Epoch: [57][69/204]	Loss 0.5947 (0.4793)	
training:	Epoch: [57][70/204]	Loss 0.4322 (0.4786)	
training:	Epoch: [57][71/204]	Loss 0.5153 (0.4791)	
training:	Epoch: [57][72/204]	Loss 0.5181 (0.4797)	
training:	Epoch: [57][73/204]	Loss 0.3684 (0.4781)	
training:	Epoch: [57][74/204]	Loss 0.5642 (0.4793)	
training:	Epoch: [57][75/204]	Loss 0.4815 (0.4793)	
training:	Epoch: [57][76/204]	Loss 0.4844 (0.4794)	
training:	Epoch: [57][77/204]	Loss 0.5286 (0.4800)	
training:	Epoch: [57][78/204]	Loss 0.3024 (0.4778)	
training:	Epoch: [57][79/204]	Loss 0.5796 (0.4791)	
training:	Epoch: [57][80/204]	Loss 0.5716 (0.4802)	
training:	Epoch: [57][81/204]	Loss 0.5796 (0.4814)	
training:	Epoch: [57][82/204]	Loss 0.6043 (0.4829)	
training:	Epoch: [57][83/204]	Loss 0.5674 (0.4840)	
training:	Epoch: [57][84/204]	Loss 0.4788 (0.4839)	
training:	Epoch: [57][85/204]	Loss 0.4182 (0.4831)	
training:	Epoch: [57][86/204]	Loss 0.2804 (0.4808)	
training:	Epoch: [57][87/204]	Loss 0.3484 (0.4792)	
training:	Epoch: [57][88/204]	Loss 0.6206 (0.4808)	
training:	Epoch: [57][89/204]	Loss 0.6209 (0.4824)	
training:	Epoch: [57][90/204]	Loss 0.4328 (0.4819)	
training:	Epoch: [57][91/204]	Loss 0.4276 (0.4813)	
training:	Epoch: [57][92/204]	Loss 0.3699 (0.4801)	
training:	Epoch: [57][93/204]	Loss 0.3346 (0.4785)	
training:	Epoch: [57][94/204]	Loss 0.4520 (0.4782)	
training:	Epoch: [57][95/204]	Loss 0.4576 (0.4780)	
training:	Epoch: [57][96/204]	Loss 0.5991 (0.4793)	
training:	Epoch: [57][97/204]	Loss 0.4874 (0.4793)	
training:	Epoch: [57][98/204]	Loss 0.4164 (0.4787)	
training:	Epoch: [57][99/204]	Loss 0.5673 (0.4796)	
training:	Epoch: [57][100/204]	Loss 0.6410 (0.4812)	
training:	Epoch: [57][101/204]	Loss 0.5896 (0.4823)	
training:	Epoch: [57][102/204]	Loss 0.5118 (0.4826)	
training:	Epoch: [57][103/204]	Loss 0.5395 (0.4831)	
training:	Epoch: [57][104/204]	Loss 0.5230 (0.4835)	
training:	Epoch: [57][105/204]	Loss 0.5677 (0.4843)	
training:	Epoch: [57][106/204]	Loss 0.4992 (0.4845)	
training:	Epoch: [57][107/204]	Loss 0.5461 (0.4850)	
training:	Epoch: [57][108/204]	Loss 0.3789 (0.4840)	
training:	Epoch: [57][109/204]	Loss 0.5077 (0.4843)	
training:	Epoch: [57][110/204]	Loss 0.4934 (0.4843)	
training:	Epoch: [57][111/204]	Loss 0.6557 (0.4859)	
training:	Epoch: [57][112/204]	Loss 0.4662 (0.4857)	
training:	Epoch: [57][113/204]	Loss 0.4305 (0.4852)	
training:	Epoch: [57][114/204]	Loss 0.3696 (0.4842)	
training:	Epoch: [57][115/204]	Loss 0.3491 (0.4830)	
training:	Epoch: [57][116/204]	Loss 0.5467 (0.4836)	
training:	Epoch: [57][117/204]	Loss 0.4880 (0.4836)	
training:	Epoch: [57][118/204]	Loss 0.4736 (0.4835)	
training:	Epoch: [57][119/204]	Loss 0.6751 (0.4851)	
training:	Epoch: [57][120/204]	Loss 0.5776 (0.4859)	
training:	Epoch: [57][121/204]	Loss 0.5502 (0.4865)	
training:	Epoch: [57][122/204]	Loss 0.4877 (0.4865)	
training:	Epoch: [57][123/204]	Loss 0.2906 (0.4849)	
training:	Epoch: [57][124/204]	Loss 0.3941 (0.4841)	
training:	Epoch: [57][125/204]	Loss 0.4802 (0.4841)	
training:	Epoch: [57][126/204]	Loss 0.4366 (0.4837)	
training:	Epoch: [57][127/204]	Loss 0.4519 (0.4835)	
training:	Epoch: [57][128/204]	Loss 0.4890 (0.4835)	
training:	Epoch: [57][129/204]	Loss 0.5477 (0.4840)	
training:	Epoch: [57][130/204]	Loss 0.5277 (0.4844)	
training:	Epoch: [57][131/204]	Loss 0.6762 (0.4858)	
training:	Epoch: [57][132/204]	Loss 0.4444 (0.4855)	
training:	Epoch: [57][133/204]	Loss 0.4705 (0.4854)	
training:	Epoch: [57][134/204]	Loss 0.3627 (0.4845)	
training:	Epoch: [57][135/204]	Loss 0.5744 (0.4851)	
training:	Epoch: [57][136/204]	Loss 0.5227 (0.4854)	
training:	Epoch: [57][137/204]	Loss 0.4964 (0.4855)	
training:	Epoch: [57][138/204]	Loss 0.3620 (0.4846)	
training:	Epoch: [57][139/204]	Loss 0.5333 (0.4850)	
training:	Epoch: [57][140/204]	Loss 0.4574 (0.4848)	
training:	Epoch: [57][141/204]	Loss 0.5139 (0.4850)	
training:	Epoch: [57][142/204]	Loss 0.4838 (0.4850)	
training:	Epoch: [57][143/204]	Loss 0.6513 (0.4861)	
training:	Epoch: [57][144/204]	Loss 0.5122 (0.4863)	
training:	Epoch: [57][145/204]	Loss 0.4843 (0.4863)	
training:	Epoch: [57][146/204]	Loss 0.3961 (0.4857)	
training:	Epoch: [57][147/204]	Loss 0.4345 (0.4853)	
training:	Epoch: [57][148/204]	Loss 0.4858 (0.4853)	
training:	Epoch: [57][149/204]	Loss 0.3709 (0.4846)	
training:	Epoch: [57][150/204]	Loss 0.6442 (0.4856)	
training:	Epoch: [57][151/204]	Loss 0.4450 (0.4854)	
training:	Epoch: [57][152/204]	Loss 0.5601 (0.4858)	
training:	Epoch: [57][153/204]	Loss 0.3904 (0.4852)	
training:	Epoch: [57][154/204]	Loss 0.3915 (0.4846)	
training:	Epoch: [57][155/204]	Loss 0.5133 (0.4848)	
training:	Epoch: [57][156/204]	Loss 0.5090 (0.4849)	
training:	Epoch: [57][157/204]	Loss 0.5349 (0.4853)	
training:	Epoch: [57][158/204]	Loss 0.5151 (0.4855)	
training:	Epoch: [57][159/204]	Loss 0.4642 (0.4853)	
training:	Epoch: [57][160/204]	Loss 0.6130 (0.4861)	
training:	Epoch: [57][161/204]	Loss 0.5117 (0.4863)	
training:	Epoch: [57][162/204]	Loss 0.4648 (0.4861)	
training:	Epoch: [57][163/204]	Loss 0.4358 (0.4858)	
training:	Epoch: [57][164/204]	Loss 0.4568 (0.4857)	
training:	Epoch: [57][165/204]	Loss 0.3912 (0.4851)	
training:	Epoch: [57][166/204]	Loss 0.5009 (0.4852)	
training:	Epoch: [57][167/204]	Loss 0.3844 (0.4846)	
training:	Epoch: [57][168/204]	Loss 0.6039 (0.4853)	
training:	Epoch: [57][169/204]	Loss 0.5057 (0.4854)	
training:	Epoch: [57][170/204]	Loss 0.5271 (0.4857)	
training:	Epoch: [57][171/204]	Loss 0.5557 (0.4861)	
training:	Epoch: [57][172/204]	Loss 0.3800 (0.4854)	
training:	Epoch: [57][173/204]	Loss 0.4863 (0.4855)	
training:	Epoch: [57][174/204]	Loss 0.4995 (0.4855)	
training:	Epoch: [57][175/204]	Loss 0.5671 (0.4860)	
training:	Epoch: [57][176/204]	Loss 0.5529 (0.4864)	
training:	Epoch: [57][177/204]	Loss 0.5255 (0.4866)	
training:	Epoch: [57][178/204]	Loss 0.4421 (0.4864)	
training:	Epoch: [57][179/204]	Loss 0.6017 (0.4870)	
training:	Epoch: [57][180/204]	Loss 0.4871 (0.4870)	
training:	Epoch: [57][181/204]	Loss 0.4673 (0.4869)	
training:	Epoch: [57][182/204]	Loss 0.5206 (0.4871)	
training:	Epoch: [57][183/204]	Loss 0.3611 (0.4864)	
training:	Epoch: [57][184/204]	Loss 0.3983 (0.4859)	
training:	Epoch: [57][185/204]	Loss 0.4866 (0.4859)	
training:	Epoch: [57][186/204]	Loss 0.6195 (0.4866)	
training:	Epoch: [57][187/204]	Loss 0.4771 (0.4866)	
training:	Epoch: [57][188/204]	Loss 0.3619 (0.4859)	
training:	Epoch: [57][189/204]	Loss 0.6063 (0.4866)	
training:	Epoch: [57][190/204]	Loss 0.4005 (0.4861)	
training:	Epoch: [57][191/204]	Loss 0.4272 (0.4858)	
training:	Epoch: [57][192/204]	Loss 0.5283 (0.4860)	
training:	Epoch: [57][193/204]	Loss 0.4166 (0.4857)	
training:	Epoch: [57][194/204]	Loss 0.4031 (0.4852)	
training:	Epoch: [57][195/204]	Loss 0.5600 (0.4856)	
training:	Epoch: [57][196/204]	Loss 0.6739 (0.4866)	
training:	Epoch: [57][197/204]	Loss 0.4586 (0.4864)	
training:	Epoch: [57][198/204]	Loss 0.5075 (0.4865)	
training:	Epoch: [57][199/204]	Loss 0.5753 (0.4870)	
training:	Epoch: [57][200/204]	Loss 0.7180 (0.4881)	
training:	Epoch: [57][201/204]	Loss 0.3740 (0.4876)	
training:	Epoch: [57][202/204]	Loss 0.5491 (0.4879)	
training:	Epoch: [57][203/204]	Loss 0.5068 (0.4880)	
training:	Epoch: [57][204/204]	Loss 0.3590 (0.4873)	
Training:	 Loss: 0.4866

Training:	 ACC: 0.7791 0.7794 0.7881 0.7701
Validation:	 ACC: 0.7734 0.7742 0.7912 0.7556
Validation:	 Best_BACC: 0.7734 0.7742 0.7912 0.7556
Validation:	 Loss: 0.4885
Pretraining:	Epoch 58/120
----------
training:	Epoch: [58][1/204]	Loss 0.5250 (0.5250)	
training:	Epoch: [58][2/204]	Loss 0.4563 (0.4907)	
training:	Epoch: [58][3/204]	Loss 0.4236 (0.4683)	
training:	Epoch: [58][4/204]	Loss 0.7414 (0.5366)	
training:	Epoch: [58][5/204]	Loss 0.6255 (0.5544)	
training:	Epoch: [58][6/204]	Loss 0.3367 (0.5181)	
training:	Epoch: [58][7/204]	Loss 0.4817 (0.5129)	
training:	Epoch: [58][8/204]	Loss 0.3495 (0.4925)	
training:	Epoch: [58][9/204]	Loss 0.5171 (0.4952)	
training:	Epoch: [58][10/204]	Loss 0.5743 (0.5031)	
training:	Epoch: [58][11/204]	Loss 0.6463 (0.5161)	
training:	Epoch: [58][12/204]	Loss 0.5239 (0.5168)	
training:	Epoch: [58][13/204]	Loss 0.5252 (0.5174)	
training:	Epoch: [58][14/204]	Loss 0.4428 (0.5121)	
training:	Epoch: [58][15/204]	Loss 0.4151 (0.5056)	
training:	Epoch: [58][16/204]	Loss 0.4362 (0.5013)	
training:	Epoch: [58][17/204]	Loss 0.4544 (0.4985)	
training:	Epoch: [58][18/204]	Loss 0.5047 (0.4989)	
training:	Epoch: [58][19/204]	Loss 0.4512 (0.4964)	
training:	Epoch: [58][20/204]	Loss 0.3936 (0.4912)	
training:	Epoch: [58][21/204]	Loss 0.4997 (0.4916)	
training:	Epoch: [58][22/204]	Loss 0.4613 (0.4903)	
training:	Epoch: [58][23/204]	Loss 0.4640 (0.4891)	
training:	Epoch: [58][24/204]	Loss 0.3909 (0.4850)	
training:	Epoch: [58][25/204]	Loss 0.4138 (0.4822)	
training:	Epoch: [58][26/204]	Loss 0.7901 (0.4940)	
training:	Epoch: [58][27/204]	Loss 0.6225 (0.4988)	
training:	Epoch: [58][28/204]	Loss 0.4319 (0.4964)	
training:	Epoch: [58][29/204]	Loss 0.4433 (0.4946)	
training:	Epoch: [58][30/204]	Loss 0.5032 (0.4948)	
training:	Epoch: [58][31/204]	Loss 0.4032 (0.4919)	
training:	Epoch: [58][32/204]	Loss 0.6755 (0.4976)	
training:	Epoch: [58][33/204]	Loss 0.3770 (0.4940)	
training:	Epoch: [58][34/204]	Loss 0.4632 (0.4931)	
training:	Epoch: [58][35/204]	Loss 0.3631 (0.4894)	
training:	Epoch: [58][36/204]	Loss 0.5619 (0.4914)	
training:	Epoch: [58][37/204]	Loss 0.4953 (0.4915)	
training:	Epoch: [58][38/204]	Loss 0.4244 (0.4897)	
training:	Epoch: [58][39/204]	Loss 0.5764 (0.4919)	
training:	Epoch: [58][40/204]	Loss 0.5017 (0.4922)	
training:	Epoch: [58][41/204]	Loss 0.4124 (0.4902)	
training:	Epoch: [58][42/204]	Loss 0.5000 (0.4905)	
training:	Epoch: [58][43/204]	Loss 0.4489 (0.4895)	
training:	Epoch: [58][44/204]	Loss 0.4033 (0.4875)	
training:	Epoch: [58][45/204]	Loss 0.3872 (0.4853)	
training:	Epoch: [58][46/204]	Loss 0.3569 (0.4825)	
training:	Epoch: [58][47/204]	Loss 0.6183 (0.4854)	
training:	Epoch: [58][48/204]	Loss 0.4319 (0.4843)	
training:	Epoch: [58][49/204]	Loss 0.6085 (0.4868)	
training:	Epoch: [58][50/204]	Loss 0.4529 (0.4861)	
training:	Epoch: [58][51/204]	Loss 0.5940 (0.4883)	
training:	Epoch: [58][52/204]	Loss 0.4821 (0.4881)	
training:	Epoch: [58][53/204]	Loss 0.4092 (0.4867)	
training:	Epoch: [58][54/204]	Loss 0.5015 (0.4869)	
training:	Epoch: [58][55/204]	Loss 0.6949 (0.4907)	
training:	Epoch: [58][56/204]	Loss 0.3300 (0.4878)	
training:	Epoch: [58][57/204]	Loss 0.5009 (0.4881)	
training:	Epoch: [58][58/204]	Loss 0.4802 (0.4879)	
training:	Epoch: [58][59/204]	Loss 0.5272 (0.4886)	
training:	Epoch: [58][60/204]	Loss 0.4905 (0.4886)	
training:	Epoch: [58][61/204]	Loss 0.4929 (0.4887)	
training:	Epoch: [58][62/204]	Loss 0.5733 (0.4901)	
training:	Epoch: [58][63/204]	Loss 0.4335 (0.4892)	
training:	Epoch: [58][64/204]	Loss 0.5892 (0.4907)	
training:	Epoch: [58][65/204]	Loss 0.4900 (0.4907)	
training:	Epoch: [58][66/204]	Loss 0.5427 (0.4915)	
training:	Epoch: [58][67/204]	Loss 0.4554 (0.4910)	
training:	Epoch: [58][68/204]	Loss 0.3813 (0.4894)	
training:	Epoch: [58][69/204]	Loss 0.4741 (0.4891)	
training:	Epoch: [58][70/204]	Loss 0.5944 (0.4906)	
training:	Epoch: [58][71/204]	Loss 0.4001 (0.4894)	
training:	Epoch: [58][72/204]	Loss 0.5814 (0.4906)	
training:	Epoch: [58][73/204]	Loss 0.4753 (0.4904)	
training:	Epoch: [58][74/204]	Loss 0.4742 (0.4902)	
training:	Epoch: [58][75/204]	Loss 0.3622 (0.4885)	
training:	Epoch: [58][76/204]	Loss 0.5113 (0.4888)	
training:	Epoch: [58][77/204]	Loss 0.4374 (0.4881)	
training:	Epoch: [58][78/204]	Loss 0.5223 (0.4886)	
training:	Epoch: [58][79/204]	Loss 0.5428 (0.4893)	
training:	Epoch: [58][80/204]	Loss 0.5849 (0.4905)	
training:	Epoch: [58][81/204]	Loss 0.4749 (0.4903)	
training:	Epoch: [58][82/204]	Loss 0.5104 (0.4905)	
training:	Epoch: [58][83/204]	Loss 0.5527 (0.4913)	
training:	Epoch: [58][84/204]	Loss 0.5249 (0.4917)	
training:	Epoch: [58][85/204]	Loss 0.4356 (0.4910)	
training:	Epoch: [58][86/204]	Loss 0.4441 (0.4905)	
training:	Epoch: [58][87/204]	Loss 0.4410 (0.4899)	
training:	Epoch: [58][88/204]	Loss 0.4725 (0.4897)	
training:	Epoch: [58][89/204]	Loss 0.6882 (0.4919)	
training:	Epoch: [58][90/204]	Loss 0.6124 (0.4933)	
training:	Epoch: [58][91/204]	Loss 0.5668 (0.4941)	
training:	Epoch: [58][92/204]	Loss 0.3693 (0.4927)	
training:	Epoch: [58][93/204]	Loss 0.4213 (0.4919)	
training:	Epoch: [58][94/204]	Loss 0.4115 (0.4911)	
training:	Epoch: [58][95/204]	Loss 0.4555 (0.4907)	
training:	Epoch: [58][96/204]	Loss 0.5019 (0.4908)	
training:	Epoch: [58][97/204]	Loss 0.5315 (0.4912)	
training:	Epoch: [58][98/204]	Loss 0.6102 (0.4925)	
training:	Epoch: [58][99/204]	Loss 0.4708 (0.4922)	
training:	Epoch: [58][100/204]	Loss 0.4568 (0.4919)	
training:	Epoch: [58][101/204]	Loss 0.4401 (0.4914)	
training:	Epoch: [58][102/204]	Loss 0.3499 (0.4900)	
training:	Epoch: [58][103/204]	Loss 0.4564 (0.4897)	
training:	Epoch: [58][104/204]	Loss 0.4171 (0.4890)	
training:	Epoch: [58][105/204]	Loss 0.6082 (0.4901)	
training:	Epoch: [58][106/204]	Loss 0.5295 (0.4905)	
training:	Epoch: [58][107/204]	Loss 0.5085 (0.4906)	
training:	Epoch: [58][108/204]	Loss 0.4782 (0.4905)	
training:	Epoch: [58][109/204]	Loss 0.5803 (0.4913)	
training:	Epoch: [58][110/204]	Loss 0.4637 (0.4911)	
training:	Epoch: [58][111/204]	Loss 0.4975 (0.4912)	
training:	Epoch: [58][112/204]	Loss 0.4766 (0.4910)	
training:	Epoch: [58][113/204]	Loss 0.6004 (0.4920)	
training:	Epoch: [58][114/204]	Loss 0.4479 (0.4916)	
training:	Epoch: [58][115/204]	Loss 0.4375 (0.4911)	
training:	Epoch: [58][116/204]	Loss 0.5397 (0.4916)	
training:	Epoch: [58][117/204]	Loss 0.3785 (0.4906)	
training:	Epoch: [58][118/204]	Loss 0.4499 (0.4902)	
training:	Epoch: [58][119/204]	Loss 0.6116 (0.4913)	
training:	Epoch: [58][120/204]	Loss 0.4193 (0.4907)	
training:	Epoch: [58][121/204]	Loss 0.4622 (0.4904)	
training:	Epoch: [58][122/204]	Loss 0.7359 (0.4924)	
training:	Epoch: [58][123/204]	Loss 0.3997 (0.4917)	
training:	Epoch: [58][124/204]	Loss 0.5139 (0.4919)	
training:	Epoch: [58][125/204]	Loss 0.4594 (0.4916)	
training:	Epoch: [58][126/204]	Loss 0.5222 (0.4919)	
training:	Epoch: [58][127/204]	Loss 0.3731 (0.4909)	
training:	Epoch: [58][128/204]	Loss 0.4693 (0.4907)	
training:	Epoch: [58][129/204]	Loss 0.5618 (0.4913)	
training:	Epoch: [58][130/204]	Loss 0.4464 (0.4910)	
training:	Epoch: [58][131/204]	Loss 0.4114 (0.4903)	
training:	Epoch: [58][132/204]	Loss 0.4881 (0.4903)	
training:	Epoch: [58][133/204]	Loss 0.4845 (0.4903)	
training:	Epoch: [58][134/204]	Loss 0.3599 (0.4893)	
training:	Epoch: [58][135/204]	Loss 0.4017 (0.4887)	
training:	Epoch: [58][136/204]	Loss 0.4270 (0.4882)	
training:	Epoch: [58][137/204]	Loss 0.4053 (0.4876)	
training:	Epoch: [58][138/204]	Loss 0.4966 (0.4877)	
training:	Epoch: [58][139/204]	Loss 0.4653 (0.4875)	
training:	Epoch: [58][140/204]	Loss 0.6683 (0.4888)	
training:	Epoch: [58][141/204]	Loss 0.5438 (0.4892)	
training:	Epoch: [58][142/204]	Loss 0.4969 (0.4892)	
training:	Epoch: [58][143/204]	Loss 0.4430 (0.4889)	
training:	Epoch: [58][144/204]	Loss 0.4648 (0.4888)	
training:	Epoch: [58][145/204]	Loss 0.4920 (0.4888)	
training:	Epoch: [58][146/204]	Loss 0.5708 (0.4893)	
training:	Epoch: [58][147/204]	Loss 0.5206 (0.4895)	
training:	Epoch: [58][148/204]	Loss 0.5701 (0.4901)	
training:	Epoch: [58][149/204]	Loss 0.5442 (0.4905)	
training:	Epoch: [58][150/204]	Loss 0.4117 (0.4899)	
training:	Epoch: [58][151/204]	Loss 0.4331 (0.4896)	
training:	Epoch: [58][152/204]	Loss 0.5507 (0.4900)	
training:	Epoch: [58][153/204]	Loss 0.5386 (0.4903)	
training:	Epoch: [58][154/204]	Loss 0.4204 (0.4898)	
training:	Epoch: [58][155/204]	Loss 0.6070 (0.4906)	
training:	Epoch: [58][156/204]	Loss 0.5531 (0.4910)	
training:	Epoch: [58][157/204]	Loss 0.5119 (0.4911)	
training:	Epoch: [58][158/204]	Loss 0.3857 (0.4904)	
training:	Epoch: [58][159/204]	Loss 0.5769 (0.4910)	
training:	Epoch: [58][160/204]	Loss 0.5851 (0.4916)	
training:	Epoch: [58][161/204]	Loss 0.4648 (0.4914)	
training:	Epoch: [58][162/204]	Loss 0.6286 (0.4923)	
training:	Epoch: [58][163/204]	Loss 0.3592 (0.4914)	
training:	Epoch: [58][164/204]	Loss 0.3569 (0.4906)	
training:	Epoch: [58][165/204]	Loss 0.4653 (0.4905)	
training:	Epoch: [58][166/204]	Loss 0.4752 (0.4904)	
training:	Epoch: [58][167/204]	Loss 0.4988 (0.4904)	
training:	Epoch: [58][168/204]	Loss 0.4108 (0.4900)	
training:	Epoch: [58][169/204]	Loss 0.4518 (0.4897)	
training:	Epoch: [58][170/204]	Loss 0.3871 (0.4891)	
training:	Epoch: [58][171/204]	Loss 0.5296 (0.4894)	
training:	Epoch: [58][172/204]	Loss 0.4353 (0.4890)	
training:	Epoch: [58][173/204]	Loss 0.6166 (0.4898)	
training:	Epoch: [58][174/204]	Loss 0.6359 (0.4906)	
training:	Epoch: [58][175/204]	Loss 0.5260 (0.4908)	
training:	Epoch: [58][176/204]	Loss 0.4193 (0.4904)	
training:	Epoch: [58][177/204]	Loss 0.4850 (0.4904)	
training:	Epoch: [58][178/204]	Loss 0.4268 (0.4900)	
training:	Epoch: [58][179/204]	Loss 0.5067 (0.4901)	
training:	Epoch: [58][180/204]	Loss 0.4024 (0.4896)	
training:	Epoch: [58][181/204]	Loss 0.4078 (0.4892)	
training:	Epoch: [58][182/204]	Loss 0.4817 (0.4891)	
training:	Epoch: [58][183/204]	Loss 0.4486 (0.4889)	
training:	Epoch: [58][184/204]	Loss 0.5126 (0.4890)	
training:	Epoch: [58][185/204]	Loss 0.4266 (0.4887)	
training:	Epoch: [58][186/204]	Loss 0.4744 (0.4886)	
training:	Epoch: [58][187/204]	Loss 0.5695 (0.4891)	
training:	Epoch: [58][188/204]	Loss 0.4612 (0.4889)	
training:	Epoch: [58][189/204]	Loss 0.5886 (0.4894)	
training:	Epoch: [58][190/204]	Loss 0.4836 (0.4894)	
training:	Epoch: [58][191/204]	Loss 0.6277 (0.4901)	
training:	Epoch: [58][192/204]	Loss 0.4009 (0.4897)	
training:	Epoch: [58][193/204]	Loss 0.5248 (0.4899)	
training:	Epoch: [58][194/204]	Loss 0.4981 (0.4899)	
training:	Epoch: [58][195/204]	Loss 0.4575 (0.4897)	
training:	Epoch: [58][196/204]	Loss 0.4183 (0.4894)	
training:	Epoch: [58][197/204]	Loss 0.4559 (0.4892)	
training:	Epoch: [58][198/204]	Loss 0.4936 (0.4892)	
training:	Epoch: [58][199/204]	Loss 0.4078 (0.4888)	
training:	Epoch: [58][200/204]	Loss 0.4439 (0.4886)	
training:	Epoch: [58][201/204]	Loss 0.4014 (0.4882)	
training:	Epoch: [58][202/204]	Loss 0.3453 (0.4874)	
training:	Epoch: [58][203/204]	Loss 0.3917 (0.4870)	
training:	Epoch: [58][204/204]	Loss 0.4613 (0.4868)	
Training:	 Loss: 0.4861

Training:	 ACC: 0.7812 0.7817 0.7934 0.7691
Validation:	 ACC: 0.7733 0.7742 0.7932 0.7534
Validation:	 Best_BACC: 0.7734 0.7742 0.7912 0.7556
Validation:	 Loss: 0.4866
Pretraining:	Epoch 59/120
----------
training:	Epoch: [59][1/204]	Loss 0.4886 (0.4886)	
training:	Epoch: [59][2/204]	Loss 0.5034 (0.4960)	
training:	Epoch: [59][3/204]	Loss 0.6096 (0.5339)	
training:	Epoch: [59][4/204]	Loss 0.6958 (0.5744)	
training:	Epoch: [59][5/204]	Loss 0.3650 (0.5325)	
training:	Epoch: [59][6/204]	Loss 0.3912 (0.5089)	
training:	Epoch: [59][7/204]	Loss 0.3685 (0.4889)	
training:	Epoch: [59][8/204]	Loss 0.6869 (0.5136)	
training:	Epoch: [59][9/204]	Loss 0.5492 (0.5176)	
training:	Epoch: [59][10/204]	Loss 0.4901 (0.5148)	
training:	Epoch: [59][11/204]	Loss 0.4829 (0.5119)	
training:	Epoch: [59][12/204]	Loss 0.5483 (0.5150)	
training:	Epoch: [59][13/204]	Loss 0.5411 (0.5170)	
training:	Epoch: [59][14/204]	Loss 0.5916 (0.5223)	
training:	Epoch: [59][15/204]	Loss 0.6387 (0.5301)	
training:	Epoch: [59][16/204]	Loss 0.3778 (0.5205)	
training:	Epoch: [59][17/204]	Loss 0.4699 (0.5176)	
training:	Epoch: [59][18/204]	Loss 0.3845 (0.5102)	
training:	Epoch: [59][19/204]	Loss 0.4789 (0.5085)	
training:	Epoch: [59][20/204]	Loss 0.4184 (0.5040)	
training:	Epoch: [59][21/204]	Loss 0.5635 (0.5068)	
training:	Epoch: [59][22/204]	Loss 0.4376 (0.5037)	
training:	Epoch: [59][23/204]	Loss 0.4441 (0.5011)	
training:	Epoch: [59][24/204]	Loss 0.4423 (0.4987)	
training:	Epoch: [59][25/204]	Loss 0.5390 (0.5003)	
training:	Epoch: [59][26/204]	Loss 0.4028 (0.4965)	
training:	Epoch: [59][27/204]	Loss 0.3936 (0.4927)	
training:	Epoch: [59][28/204]	Loss 0.5295 (0.4940)	
training:	Epoch: [59][29/204]	Loss 0.4739 (0.4933)	
training:	Epoch: [59][30/204]	Loss 0.4483 (0.4918)	
training:	Epoch: [59][31/204]	Loss 0.6095 (0.4956)	
training:	Epoch: [59][32/204]	Loss 0.4525 (0.4943)	
training:	Epoch: [59][33/204]	Loss 0.4257 (0.4922)	
training:	Epoch: [59][34/204]	Loss 0.4500 (0.4910)	
training:	Epoch: [59][35/204]	Loss 0.4706 (0.4904)	
training:	Epoch: [59][36/204]	Loss 0.5128 (0.4910)	
training:	Epoch: [59][37/204]	Loss 0.5319 (0.4921)	
training:	Epoch: [59][38/204]	Loss 0.4533 (0.4911)	
training:	Epoch: [59][39/204]	Loss 0.4589 (0.4903)	
training:	Epoch: [59][40/204]	Loss 0.4709 (0.4898)	
training:	Epoch: [59][41/204]	Loss 0.4867 (0.4897)	
training:	Epoch: [59][42/204]	Loss 0.4324 (0.4883)	
training:	Epoch: [59][43/204]	Loss 0.3454 (0.4850)	
training:	Epoch: [59][44/204]	Loss 0.4429 (0.4841)	
training:	Epoch: [59][45/204]	Loss 0.4259 (0.4828)	
training:	Epoch: [59][46/204]	Loss 0.5937 (0.4852)	
training:	Epoch: [59][47/204]	Loss 0.4614 (0.4847)	
training:	Epoch: [59][48/204]	Loss 0.4224 (0.4834)	
training:	Epoch: [59][49/204]	Loss 0.3819 (0.4813)	
training:	Epoch: [59][50/204]	Loss 0.3296 (0.4783)	
training:	Epoch: [59][51/204]	Loss 0.4916 (0.4785)	
training:	Epoch: [59][52/204]	Loss 0.3818 (0.4767)	
training:	Epoch: [59][53/204]	Loss 0.6543 (0.4800)	
training:	Epoch: [59][54/204]	Loss 0.4664 (0.4798)	
training:	Epoch: [59][55/204]	Loss 0.3885 (0.4781)	
training:	Epoch: [59][56/204]	Loss 0.3999 (0.4767)	
training:	Epoch: [59][57/204]	Loss 0.6383 (0.4795)	
training:	Epoch: [59][58/204]	Loss 0.4588 (0.4792)	
training:	Epoch: [59][59/204]	Loss 0.6893 (0.4827)	
training:	Epoch: [59][60/204]	Loss 0.3438 (0.4804)	
training:	Epoch: [59][61/204]	Loss 0.4778 (0.4804)	
training:	Epoch: [59][62/204]	Loss 0.3830 (0.4788)	
training:	Epoch: [59][63/204]	Loss 0.5884 (0.4806)	
training:	Epoch: [59][64/204]	Loss 0.4730 (0.4804)	
training:	Epoch: [59][65/204]	Loss 0.4239 (0.4796)	
training:	Epoch: [59][66/204]	Loss 0.5264 (0.4803)	
training:	Epoch: [59][67/204]	Loss 0.4483 (0.4798)	
training:	Epoch: [59][68/204]	Loss 0.4809 (0.4798)	
training:	Epoch: [59][69/204]	Loss 0.5483 (0.4808)	
training:	Epoch: [59][70/204]	Loss 0.4814 (0.4808)	
training:	Epoch: [59][71/204]	Loss 0.4169 (0.4799)	
training:	Epoch: [59][72/204]	Loss 0.4901 (0.4801)	
training:	Epoch: [59][73/204]	Loss 0.6892 (0.4829)	
training:	Epoch: [59][74/204]	Loss 0.3815 (0.4816)	
training:	Epoch: [59][75/204]	Loss 0.5362 (0.4823)	
training:	Epoch: [59][76/204]	Loss 0.4860 (0.4823)	
training:	Epoch: [59][77/204]	Loss 0.4967 (0.4825)	
training:	Epoch: [59][78/204]	Loss 0.3587 (0.4809)	
training:	Epoch: [59][79/204]	Loss 0.4747 (0.4809)	
training:	Epoch: [59][80/204]	Loss 0.4761 (0.4808)	
training:	Epoch: [59][81/204]	Loss 0.5340 (0.4815)	
training:	Epoch: [59][82/204]	Loss 0.4782 (0.4814)	
training:	Epoch: [59][83/204]	Loss 0.5457 (0.4822)	
training:	Epoch: [59][84/204]	Loss 0.4981 (0.4824)	
training:	Epoch: [59][85/204]	Loss 0.5766 (0.4835)	
training:	Epoch: [59][86/204]	Loss 0.4944 (0.4836)	
training:	Epoch: [59][87/204]	Loss 0.5256 (0.4841)	
training:	Epoch: [59][88/204]	Loss 0.5610 (0.4850)	
training:	Epoch: [59][89/204]	Loss 0.4183 (0.4842)	
training:	Epoch: [59][90/204]	Loss 0.5386 (0.4848)	
training:	Epoch: [59][91/204]	Loss 0.4189 (0.4841)	
training:	Epoch: [59][92/204]	Loss 0.4218 (0.4834)	
training:	Epoch: [59][93/204]	Loss 0.3532 (0.4820)	
training:	Epoch: [59][94/204]	Loss 0.4814 (0.4820)	
training:	Epoch: [59][95/204]	Loss 0.5789 (0.4830)	
training:	Epoch: [59][96/204]	Loss 0.4001 (0.4822)	
training:	Epoch: [59][97/204]	Loss 0.3906 (0.4812)	
training:	Epoch: [59][98/204]	Loss 0.3434 (0.4798)	
training:	Epoch: [59][99/204]	Loss 0.3924 (0.4789)	
training:	Epoch: [59][100/204]	Loss 0.4533 (0.4787)	
training:	Epoch: [59][101/204]	Loss 0.3897 (0.4778)	
training:	Epoch: [59][102/204]	Loss 0.5757 (0.4788)	
training:	Epoch: [59][103/204]	Loss 0.4818 (0.4788)	
training:	Epoch: [59][104/204]	Loss 0.4574 (0.4786)	
training:	Epoch: [59][105/204]	Loss 0.4847 (0.4786)	
training:	Epoch: [59][106/204]	Loss 0.5672 (0.4795)	
training:	Epoch: [59][107/204]	Loss 0.7012 (0.4816)	
training:	Epoch: [59][108/204]	Loss 0.5625 (0.4823)	
training:	Epoch: [59][109/204]	Loss 0.3733 (0.4813)	
training:	Epoch: [59][110/204]	Loss 0.4139 (0.4807)	
training:	Epoch: [59][111/204]	Loss 0.4941 (0.4808)	
training:	Epoch: [59][112/204]	Loss 0.5227 (0.4812)	
training:	Epoch: [59][113/204]	Loss 0.5868 (0.4821)	
training:	Epoch: [59][114/204]	Loss 0.4630 (0.4820)	
training:	Epoch: [59][115/204]	Loss 0.5871 (0.4829)	
training:	Epoch: [59][116/204]	Loss 0.4577 (0.4826)	
training:	Epoch: [59][117/204]	Loss 0.4561 (0.4824)	
training:	Epoch: [59][118/204]	Loss 0.5207 (0.4827)	
training:	Epoch: [59][119/204]	Loss 0.4672 (0.4826)	
training:	Epoch: [59][120/204]	Loss 0.4220 (0.4821)	
training:	Epoch: [59][121/204]	Loss 0.5642 (0.4828)	
training:	Epoch: [59][122/204]	Loss 0.5478 (0.4833)	
training:	Epoch: [59][123/204]	Loss 0.5752 (0.4841)	
training:	Epoch: [59][124/204]	Loss 0.4999 (0.4842)	
training:	Epoch: [59][125/204]	Loss 0.5524 (0.4847)	
training:	Epoch: [59][126/204]	Loss 0.3356 (0.4836)	
training:	Epoch: [59][127/204]	Loss 0.4786 (0.4835)	
training:	Epoch: [59][128/204]	Loss 0.4927 (0.4836)	
training:	Epoch: [59][129/204]	Loss 0.5282 (0.4839)	
training:	Epoch: [59][130/204]	Loss 0.4268 (0.4835)	
training:	Epoch: [59][131/204]	Loss 0.4884 (0.4835)	
training:	Epoch: [59][132/204]	Loss 0.4551 (0.4833)	
training:	Epoch: [59][133/204]	Loss 0.5349 (0.4837)	
training:	Epoch: [59][134/204]	Loss 0.6550 (0.4850)	
training:	Epoch: [59][135/204]	Loss 0.5089 (0.4852)	
training:	Epoch: [59][136/204]	Loss 0.3773 (0.4844)	
training:	Epoch: [59][137/204]	Loss 0.5644 (0.4850)	
training:	Epoch: [59][138/204]	Loss 0.4925 (0.4850)	
training:	Epoch: [59][139/204]	Loss 0.6134 (0.4859)	
training:	Epoch: [59][140/204]	Loss 0.5932 (0.4867)	
training:	Epoch: [59][141/204]	Loss 0.3740 (0.4859)	
training:	Epoch: [59][142/204]	Loss 0.5475 (0.4863)	
training:	Epoch: [59][143/204]	Loss 0.6328 (0.4874)	
training:	Epoch: [59][144/204]	Loss 0.5614 (0.4879)	
training:	Epoch: [59][145/204]	Loss 0.5374 (0.4882)	
training:	Epoch: [59][146/204]	Loss 0.4915 (0.4882)	
training:	Epoch: [59][147/204]	Loss 0.6223 (0.4891)	
training:	Epoch: [59][148/204]	Loss 0.5462 (0.4895)	
training:	Epoch: [59][149/204]	Loss 0.4258 (0.4891)	
training:	Epoch: [59][150/204]	Loss 0.4832 (0.4891)	
training:	Epoch: [59][151/204]	Loss 0.2561 (0.4875)	
training:	Epoch: [59][152/204]	Loss 0.5063 (0.4876)	
training:	Epoch: [59][153/204]	Loss 0.4121 (0.4872)	
training:	Epoch: [59][154/204]	Loss 0.4990 (0.4872)	
training:	Epoch: [59][155/204]	Loss 0.4039 (0.4867)	
training:	Epoch: [59][156/204]	Loss 0.5568 (0.4871)	
training:	Epoch: [59][157/204]	Loss 0.5136 (0.4873)	
training:	Epoch: [59][158/204]	Loss 0.4590 (0.4871)	
training:	Epoch: [59][159/204]	Loss 0.5719 (0.4877)	
training:	Epoch: [59][160/204]	Loss 0.4918 (0.4877)	
training:	Epoch: [59][161/204]	Loss 0.5139 (0.4879)	
training:	Epoch: [59][162/204]	Loss 0.5068 (0.4880)	
training:	Epoch: [59][163/204]	Loss 0.4009 (0.4874)	
training:	Epoch: [59][164/204]	Loss 0.4567 (0.4872)	
training:	Epoch: [59][165/204]	Loss 0.4509 (0.4870)	
training:	Epoch: [59][166/204]	Loss 0.3424 (0.4862)	
training:	Epoch: [59][167/204]	Loss 0.5589 (0.4866)	
training:	Epoch: [59][168/204]	Loss 0.4848 (0.4866)	
training:	Epoch: [59][169/204]	Loss 0.6104 (0.4873)	
training:	Epoch: [59][170/204]	Loss 0.5042 (0.4874)	
training:	Epoch: [59][171/204]	Loss 0.4865 (0.4874)	
training:	Epoch: [59][172/204]	Loss 0.4377 (0.4871)	
training:	Epoch: [59][173/204]	Loss 0.3547 (0.4864)	
training:	Epoch: [59][174/204]	Loss 0.6067 (0.4870)	
training:	Epoch: [59][175/204]	Loss 0.5205 (0.4872)	
training:	Epoch: [59][176/204]	Loss 0.3631 (0.4865)	
training:	Epoch: [59][177/204]	Loss 0.4890 (0.4865)	
training:	Epoch: [59][178/204]	Loss 0.3850 (0.4860)	
training:	Epoch: [59][179/204]	Loss 0.3893 (0.4854)	
training:	Epoch: [59][180/204]	Loss 0.4812 (0.4854)	
training:	Epoch: [59][181/204]	Loss 0.3641 (0.4847)	
training:	Epoch: [59][182/204]	Loss 0.5255 (0.4850)	
training:	Epoch: [59][183/204]	Loss 0.5535 (0.4853)	
training:	Epoch: [59][184/204]	Loss 0.5327 (0.4856)	
training:	Epoch: [59][185/204]	Loss 0.4885 (0.4856)	
training:	Epoch: [59][186/204]	Loss 0.3438 (0.4848)	
training:	Epoch: [59][187/204]	Loss 0.5434 (0.4852)	
training:	Epoch: [59][188/204]	Loss 0.4851 (0.4852)	
training:	Epoch: [59][189/204]	Loss 0.5901 (0.4857)	
training:	Epoch: [59][190/204]	Loss 0.5765 (0.4862)	
training:	Epoch: [59][191/204]	Loss 0.5162 (0.4864)	
training:	Epoch: [59][192/204]	Loss 0.4734 (0.4863)	
training:	Epoch: [59][193/204]	Loss 0.5272 (0.4865)	
training:	Epoch: [59][194/204]	Loss 0.4878 (0.4865)	
training:	Epoch: [59][195/204]	Loss 0.4163 (0.4861)	
training:	Epoch: [59][196/204]	Loss 0.4301 (0.4859)	
training:	Epoch: [59][197/204]	Loss 0.4020 (0.4854)	
training:	Epoch: [59][198/204]	Loss 0.3242 (0.4846)	
training:	Epoch: [59][199/204]	Loss 0.3966 (0.4842)	
training:	Epoch: [59][200/204]	Loss 0.3882 (0.4837)	
training:	Epoch: [59][201/204]	Loss 0.3738 (0.4831)	
training:	Epoch: [59][202/204]	Loss 0.4049 (0.4828)	
training:	Epoch: [59][203/204]	Loss 0.4992 (0.4828)	
training:	Epoch: [59][204/204]	Loss 0.3676 (0.4823)	
Training:	 Loss: 0.4815

Training:	 ACC: 0.7824 0.7831 0.8001 0.7647
Validation:	 ACC: 0.7752 0.7764 0.8004 0.7500
Validation:	 Best_BACC: 0.7752 0.7764 0.8004 0.7500
Validation:	 Loss: 0.4849
Pretraining:	Epoch 60/120
----------
training:	Epoch: [60][1/204]	Loss 0.5866 (0.5866)	
training:	Epoch: [60][2/204]	Loss 0.6247 (0.6057)	
training:	Epoch: [60][3/204]	Loss 0.5550 (0.5888)	
training:	Epoch: [60][4/204]	Loss 0.3987 (0.5413)	
training:	Epoch: [60][5/204]	Loss 0.4590 (0.5248)	
training:	Epoch: [60][6/204]	Loss 0.2969 (0.4868)	
training:	Epoch: [60][7/204]	Loss 0.4505 (0.4817)	
training:	Epoch: [60][8/204]	Loss 0.4067 (0.4723)	
training:	Epoch: [60][9/204]	Loss 0.4344 (0.4681)	
training:	Epoch: [60][10/204]	Loss 0.5378 (0.4751)	
training:	Epoch: [60][11/204]	Loss 0.6865 (0.4943)	
training:	Epoch: [60][12/204]	Loss 0.5723 (0.5008)	
training:	Epoch: [60][13/204]	Loss 0.4405 (0.4961)	
training:	Epoch: [60][14/204]	Loss 0.5503 (0.5000)	
training:	Epoch: [60][15/204]	Loss 0.4220 (0.4948)	
training:	Epoch: [60][16/204]	Loss 0.4636 (0.4929)	
training:	Epoch: [60][17/204]	Loss 0.3884 (0.4867)	
training:	Epoch: [60][18/204]	Loss 0.4109 (0.4825)	
training:	Epoch: [60][19/204]	Loss 0.5143 (0.4842)	
training:	Epoch: [60][20/204]	Loss 0.5132 (0.4856)	
training:	Epoch: [60][21/204]	Loss 0.2647 (0.4751)	
training:	Epoch: [60][22/204]	Loss 0.5088 (0.4766)	
training:	Epoch: [60][23/204]	Loss 0.5172 (0.4784)	
training:	Epoch: [60][24/204]	Loss 0.5397 (0.4809)	
training:	Epoch: [60][25/204]	Loss 0.5218 (0.4826)	
training:	Epoch: [60][26/204]	Loss 0.4625 (0.4818)	
training:	Epoch: [60][27/204]	Loss 0.4238 (0.4797)	
training:	Epoch: [60][28/204]	Loss 0.4852 (0.4799)	
training:	Epoch: [60][29/204]	Loss 0.5627 (0.4827)	
training:	Epoch: [60][30/204]	Loss 0.3534 (0.4784)	
training:	Epoch: [60][31/204]	Loss 0.4411 (0.4772)	
training:	Epoch: [60][32/204]	Loss 0.4873 (0.4775)	
training:	Epoch: [60][33/204]	Loss 0.4593 (0.4770)	
training:	Epoch: [60][34/204]	Loss 0.4547 (0.4763)	
training:	Epoch: [60][35/204]	Loss 0.5478 (0.4784)	
training:	Epoch: [60][36/204]	Loss 0.5272 (0.4797)	
training:	Epoch: [60][37/204]	Loss 0.5097 (0.4805)	
training:	Epoch: [60][38/204]	Loss 0.4399 (0.4794)	
training:	Epoch: [60][39/204]	Loss 0.4950 (0.4798)	
training:	Epoch: [60][40/204]	Loss 0.4741 (0.4797)	
training:	Epoch: [60][41/204]	Loss 0.4859 (0.4799)	
training:	Epoch: [60][42/204]	Loss 0.4367 (0.4788)	
training:	Epoch: [60][43/204]	Loss 0.5673 (0.4809)	
training:	Epoch: [60][44/204]	Loss 0.4434 (0.4800)	
training:	Epoch: [60][45/204]	Loss 0.4962 (0.4804)	
training:	Epoch: [60][46/204]	Loss 0.5501 (0.4819)	
training:	Epoch: [60][47/204]	Loss 0.4626 (0.4815)	
training:	Epoch: [60][48/204]	Loss 0.4266 (0.4804)	
training:	Epoch: [60][49/204]	Loss 0.4959 (0.4807)	
training:	Epoch: [60][50/204]	Loss 0.4242 (0.4795)	
training:	Epoch: [60][51/204]	Loss 0.4665 (0.4793)	
training:	Epoch: [60][52/204]	Loss 0.5221 (0.4801)	
training:	Epoch: [60][53/204]	Loss 0.5079 (0.4806)	
training:	Epoch: [60][54/204]	Loss 0.6171 (0.4832)	
training:	Epoch: [60][55/204]	Loss 0.4789 (0.4831)	
training:	Epoch: [60][56/204]	Loss 0.4413 (0.4823)	
training:	Epoch: [60][57/204]	Loss 0.6013 (0.4844)	
training:	Epoch: [60][58/204]	Loss 0.4711 (0.4842)	
training:	Epoch: [60][59/204]	Loss 0.4663 (0.4839)	
training:	Epoch: [60][60/204]	Loss 0.5305 (0.4847)	
training:	Epoch: [60][61/204]	Loss 0.3869 (0.4831)	
training:	Epoch: [60][62/204]	Loss 0.5510 (0.4842)	
training:	Epoch: [60][63/204]	Loss 0.5149 (0.4846)	
training:	Epoch: [60][64/204]	Loss 0.4487 (0.4841)	
training:	Epoch: [60][65/204]	Loss 0.5977 (0.4858)	
training:	Epoch: [60][66/204]	Loss 0.4427 (0.4852)	
training:	Epoch: [60][67/204]	Loss 0.4244 (0.4843)	
training:	Epoch: [60][68/204]	Loss 0.5798 (0.4857)	
training:	Epoch: [60][69/204]	Loss 0.4280 (0.4848)	
training:	Epoch: [60][70/204]	Loss 0.4491 (0.4843)	
training:	Epoch: [60][71/204]	Loss 0.5391 (0.4851)	
training:	Epoch: [60][72/204]	Loss 0.5293 (0.4857)	
training:	Epoch: [60][73/204]	Loss 0.5559 (0.4867)	
training:	Epoch: [60][74/204]	Loss 0.4528 (0.4862)	
training:	Epoch: [60][75/204]	Loss 0.4715 (0.4860)	
training:	Epoch: [60][76/204]	Loss 0.4449 (0.4855)	
training:	Epoch: [60][77/204]	Loss 0.5023 (0.4857)	
training:	Epoch: [60][78/204]	Loss 0.4198 (0.4849)	
training:	Epoch: [60][79/204]	Loss 0.4756 (0.4847)	
training:	Epoch: [60][80/204]	Loss 0.5071 (0.4850)	
training:	Epoch: [60][81/204]	Loss 0.4387 (0.4844)	
training:	Epoch: [60][82/204]	Loss 0.3503 (0.4828)	
training:	Epoch: [60][83/204]	Loss 0.4962 (0.4830)	
training:	Epoch: [60][84/204]	Loss 0.4194 (0.4822)	
training:	Epoch: [60][85/204]	Loss 0.4952 (0.4824)	
training:	Epoch: [60][86/204]	Loss 0.4636 (0.4822)	
training:	Epoch: [60][87/204]	Loss 0.3294 (0.4804)	
training:	Epoch: [60][88/204]	Loss 0.5687 (0.4814)	
training:	Epoch: [60][89/204]	Loss 0.4565 (0.4811)	
training:	Epoch: [60][90/204]	Loss 0.4339 (0.4806)	
training:	Epoch: [60][91/204]	Loss 0.4180 (0.4799)	
training:	Epoch: [60][92/204]	Loss 0.5122 (0.4803)	
training:	Epoch: [60][93/204]	Loss 0.4249 (0.4797)	
training:	Epoch: [60][94/204]	Loss 0.4148 (0.4790)	
training:	Epoch: [60][95/204]	Loss 0.4645 (0.4788)	
training:	Epoch: [60][96/204]	Loss 0.4002 (0.4780)	
training:	Epoch: [60][97/204]	Loss 0.5419 (0.4787)	
training:	Epoch: [60][98/204]	Loss 0.5126 (0.4790)	
training:	Epoch: [60][99/204]	Loss 0.5616 (0.4798)	
training:	Epoch: [60][100/204]	Loss 0.4817 (0.4799)	
training:	Epoch: [60][101/204]	Loss 0.4257 (0.4793)	
training:	Epoch: [60][102/204]	Loss 0.5044 (0.4796)	
training:	Epoch: [60][103/204]	Loss 0.4128 (0.4789)	
training:	Epoch: [60][104/204]	Loss 0.4463 (0.4786)	
training:	Epoch: [60][105/204]	Loss 0.4827 (0.4786)	
training:	Epoch: [60][106/204]	Loss 0.3726 (0.4776)	
training:	Epoch: [60][107/204]	Loss 0.4447 (0.4773)	
training:	Epoch: [60][108/204]	Loss 0.6308 (0.4788)	
training:	Epoch: [60][109/204]	Loss 0.5122 (0.4791)	
training:	Epoch: [60][110/204]	Loss 0.5136 (0.4794)	
training:	Epoch: [60][111/204]	Loss 0.5056 (0.4796)	
training:	Epoch: [60][112/204]	Loss 0.4127 (0.4790)	
training:	Epoch: [60][113/204]	Loss 0.4511 (0.4788)	
training:	Epoch: [60][114/204]	Loss 0.3679 (0.4778)	
training:	Epoch: [60][115/204]	Loss 0.6038 (0.4789)	
training:	Epoch: [60][116/204]	Loss 0.4935 (0.4790)	
training:	Epoch: [60][117/204]	Loss 0.5048 (0.4792)	
training:	Epoch: [60][118/204]	Loss 0.6067 (0.4803)	
training:	Epoch: [60][119/204]	Loss 0.4465 (0.4800)	
training:	Epoch: [60][120/204]	Loss 0.5345 (0.4805)	
training:	Epoch: [60][121/204]	Loss 0.3186 (0.4792)	
training:	Epoch: [60][122/204]	Loss 0.5597 (0.4798)	
training:	Epoch: [60][123/204]	Loss 0.3919 (0.4791)	
training:	Epoch: [60][124/204]	Loss 0.6475 (0.4805)	
training:	Epoch: [60][125/204]	Loss 0.4830 (0.4805)	
training:	Epoch: [60][126/204]	Loss 0.3386 (0.4794)	
training:	Epoch: [60][127/204]	Loss 0.6327 (0.4806)	
training:	Epoch: [60][128/204]	Loss 0.4036 (0.4800)	
training:	Epoch: [60][129/204]	Loss 0.4102 (0.4794)	
training:	Epoch: [60][130/204]	Loss 0.4570 (0.4792)	
training:	Epoch: [60][131/204]	Loss 0.4833 (0.4793)	
training:	Epoch: [60][132/204]	Loss 0.4915 (0.4794)	
training:	Epoch: [60][133/204]	Loss 0.4332 (0.4790)	
training:	Epoch: [60][134/204]	Loss 0.3737 (0.4782)	
training:	Epoch: [60][135/204]	Loss 0.4307 (0.4779)	
training:	Epoch: [60][136/204]	Loss 0.4919 (0.4780)	
training:	Epoch: [60][137/204]	Loss 0.3102 (0.4768)	
training:	Epoch: [60][138/204]	Loss 0.5385 (0.4772)	
training:	Epoch: [60][139/204]	Loss 0.4971 (0.4774)	
training:	Epoch: [60][140/204]	Loss 0.4335 (0.4770)	
training:	Epoch: [60][141/204]	Loss 0.4760 (0.4770)	
training:	Epoch: [60][142/204]	Loss 0.4141 (0.4766)	
training:	Epoch: [60][143/204]	Loss 0.4041 (0.4761)	
training:	Epoch: [60][144/204]	Loss 0.6276 (0.4771)	
training:	Epoch: [60][145/204]	Loss 0.5655 (0.4777)	
training:	Epoch: [60][146/204]	Loss 0.5816 (0.4785)	
training:	Epoch: [60][147/204]	Loss 0.4654 (0.4784)	
training:	Epoch: [60][148/204]	Loss 0.4323 (0.4781)	
training:	Epoch: [60][149/204]	Loss 0.4413 (0.4778)	
training:	Epoch: [60][150/204]	Loss 0.4612 (0.4777)	
training:	Epoch: [60][151/204]	Loss 0.6752 (0.4790)	
training:	Epoch: [60][152/204]	Loss 0.5784 (0.4797)	
training:	Epoch: [60][153/204]	Loss 0.5047 (0.4798)	
training:	Epoch: [60][154/204]	Loss 0.4847 (0.4799)	
training:	Epoch: [60][155/204]	Loss 0.5356 (0.4802)	
training:	Epoch: [60][156/204]	Loss 0.5233 (0.4805)	
training:	Epoch: [60][157/204]	Loss 0.4271 (0.4802)	
training:	Epoch: [60][158/204]	Loss 0.5379 (0.4805)	
training:	Epoch: [60][159/204]	Loss 0.4899 (0.4806)	
training:	Epoch: [60][160/204]	Loss 0.3759 (0.4799)	
training:	Epoch: [60][161/204]	Loss 0.5310 (0.4802)	
training:	Epoch: [60][162/204]	Loss 0.4858 (0.4803)	
training:	Epoch: [60][163/204]	Loss 0.5348 (0.4806)	
training:	Epoch: [60][164/204]	Loss 0.5357 (0.4809)	
training:	Epoch: [60][165/204]	Loss 0.4100 (0.4805)	
training:	Epoch: [60][166/204]	Loss 0.5708 (0.4811)	
training:	Epoch: [60][167/204]	Loss 0.4470 (0.4809)	
training:	Epoch: [60][168/204]	Loss 0.5445 (0.4812)	
training:	Epoch: [60][169/204]	Loss 0.4559 (0.4811)	
training:	Epoch: [60][170/204]	Loss 0.6678 (0.4822)	
training:	Epoch: [60][171/204]	Loss 0.4656 (0.4821)	
training:	Epoch: [60][172/204]	Loss 0.7258 (0.4835)	
training:	Epoch: [60][173/204]	Loss 0.6597 (0.4845)	
training:	Epoch: [60][174/204]	Loss 0.4230 (0.4842)	
training:	Epoch: [60][175/204]	Loss 0.4156 (0.4838)	
training:	Epoch: [60][176/204]	Loss 0.4441 (0.4835)	
training:	Epoch: [60][177/204]	Loss 0.4922 (0.4836)	
training:	Epoch: [60][178/204]	Loss 0.5498 (0.4840)	
training:	Epoch: [60][179/204]	Loss 0.4628 (0.4839)	
training:	Epoch: [60][180/204]	Loss 0.5149 (0.4840)	
training:	Epoch: [60][181/204]	Loss 0.3460 (0.4833)	
training:	Epoch: [60][182/204]	Loss 0.5208 (0.4835)	
training:	Epoch: [60][183/204]	Loss 0.4008 (0.4830)	
training:	Epoch: [60][184/204]	Loss 0.3667 (0.4824)	
training:	Epoch: [60][185/204]	Loss 0.6016 (0.4830)	
training:	Epoch: [60][186/204]	Loss 0.5069 (0.4832)	
training:	Epoch: [60][187/204]	Loss 0.3893 (0.4827)	
training:	Epoch: [60][188/204]	Loss 0.4282 (0.4824)	
training:	Epoch: [60][189/204]	Loss 0.5146 (0.4825)	
training:	Epoch: [60][190/204]	Loss 0.4868 (0.4826)	
training:	Epoch: [60][191/204]	Loss 0.5766 (0.4830)	
training:	Epoch: [60][192/204]	Loss 0.4625 (0.4829)	
training:	Epoch: [60][193/204]	Loss 0.4925 (0.4830)	
training:	Epoch: [60][194/204]	Loss 0.5380 (0.4833)	
training:	Epoch: [60][195/204]	Loss 0.4785 (0.4833)	
training:	Epoch: [60][196/204]	Loss 0.5004 (0.4833)	
training:	Epoch: [60][197/204]	Loss 0.4329 (0.4831)	
training:	Epoch: [60][198/204]	Loss 0.5078 (0.4832)	
training:	Epoch: [60][199/204]	Loss 0.4512 (0.4830)	
training:	Epoch: [60][200/204]	Loss 0.4114 (0.4827)	
training:	Epoch: [60][201/204]	Loss 0.4910 (0.4827)	
training:	Epoch: [60][202/204]	Loss 0.4928 (0.4828)	
training:	Epoch: [60][203/204]	Loss 0.5852 (0.4833)	
training:	Epoch: [60][204/204]	Loss 0.3992 (0.4829)	
Training:	 Loss: 0.4821

Training:	 ACC: 0.7845 0.7851 0.8004 0.7685
Validation:	 ACC: 0.7753 0.7764 0.7984 0.7522
Validation:	 Best_BACC: 0.7753 0.7764 0.7984 0.7522
Validation:	 Loss: 0.4836
Pretraining:	Epoch 61/120
----------
training:	Epoch: [61][1/204]	Loss 0.6329 (0.6329)	
training:	Epoch: [61][2/204]	Loss 0.5448 (0.5888)	
training:	Epoch: [61][3/204]	Loss 0.4854 (0.5544)	
training:	Epoch: [61][4/204]	Loss 0.4210 (0.5210)	
training:	Epoch: [61][5/204]	Loss 0.4298 (0.5028)	
training:	Epoch: [61][6/204]	Loss 0.6008 (0.5191)	
training:	Epoch: [61][7/204]	Loss 0.4369 (0.5074)	
training:	Epoch: [61][8/204]	Loss 0.4628 (0.5018)	
training:	Epoch: [61][9/204]	Loss 0.7117 (0.5251)	
training:	Epoch: [61][10/204]	Loss 0.5034 (0.5229)	
training:	Epoch: [61][11/204]	Loss 0.4391 (0.5153)	
training:	Epoch: [61][12/204]	Loss 0.4933 (0.5135)	
training:	Epoch: [61][13/204]	Loss 0.4592 (0.5093)	
training:	Epoch: [61][14/204]	Loss 0.5610 (0.5130)	
training:	Epoch: [61][15/204]	Loss 0.4094 (0.5061)	
training:	Epoch: [61][16/204]	Loss 0.5489 (0.5088)	
training:	Epoch: [61][17/204]	Loss 0.5867 (0.5134)	
training:	Epoch: [61][18/204]	Loss 0.4293 (0.5087)	
training:	Epoch: [61][19/204]	Loss 0.4334 (0.5047)	
training:	Epoch: [61][20/204]	Loss 0.4382 (0.5014)	
training:	Epoch: [61][21/204]	Loss 0.5021 (0.5014)	
training:	Epoch: [61][22/204]	Loss 0.5640 (0.5043)	
training:	Epoch: [61][23/204]	Loss 0.3069 (0.4957)	
training:	Epoch: [61][24/204]	Loss 0.4546 (0.4940)	
training:	Epoch: [61][25/204]	Loss 0.3526 (0.4883)	
training:	Epoch: [61][26/204]	Loss 0.4310 (0.4861)	
training:	Epoch: [61][27/204]	Loss 0.5507 (0.4885)	
training:	Epoch: [61][28/204]	Loss 0.5595 (0.4911)	
training:	Epoch: [61][29/204]	Loss 0.5208 (0.4921)	
training:	Epoch: [61][30/204]	Loss 0.4110 (0.4894)	
training:	Epoch: [61][31/204]	Loss 0.4618 (0.4885)	
training:	Epoch: [61][32/204]	Loss 0.3865 (0.4853)	
training:	Epoch: [61][33/204]	Loss 0.6168 (0.4893)	
training:	Epoch: [61][34/204]	Loss 0.5389 (0.4907)	
training:	Epoch: [61][35/204]	Loss 0.3378 (0.4864)	
training:	Epoch: [61][36/204]	Loss 0.4162 (0.4844)	
training:	Epoch: [61][37/204]	Loss 0.3858 (0.4818)	
training:	Epoch: [61][38/204]	Loss 0.4629 (0.4813)	
training:	Epoch: [61][39/204]	Loss 0.4031 (0.4793)	
training:	Epoch: [61][40/204]	Loss 0.6513 (0.4836)	
training:	Epoch: [61][41/204]	Loss 0.4385 (0.4825)	
training:	Epoch: [61][42/204]	Loss 0.4047 (0.4806)	
training:	Epoch: [61][43/204]	Loss 0.4164 (0.4791)	
training:	Epoch: [61][44/204]	Loss 0.4923 (0.4794)	
training:	Epoch: [61][45/204]	Loss 0.5256 (0.4804)	
training:	Epoch: [61][46/204]	Loss 0.5177 (0.4812)	
training:	Epoch: [61][47/204]	Loss 0.4017 (0.4796)	
training:	Epoch: [61][48/204]	Loss 0.5008 (0.4800)	
training:	Epoch: [61][49/204]	Loss 0.4929 (0.4803)	
training:	Epoch: [61][50/204]	Loss 0.5787 (0.4822)	
training:	Epoch: [61][51/204]	Loss 0.6626 (0.4858)	
training:	Epoch: [61][52/204]	Loss 0.4097 (0.4843)	
training:	Epoch: [61][53/204]	Loss 0.4653 (0.4839)	
training:	Epoch: [61][54/204]	Loss 0.4784 (0.4838)	
training:	Epoch: [61][55/204]	Loss 0.3605 (0.4816)	
training:	Epoch: [61][56/204]	Loss 0.3857 (0.4799)	
training:	Epoch: [61][57/204]	Loss 0.5161 (0.4805)	
training:	Epoch: [61][58/204]	Loss 0.4317 (0.4797)	
training:	Epoch: [61][59/204]	Loss 0.5738 (0.4813)	
training:	Epoch: [61][60/204]	Loss 0.6630 (0.4843)	
training:	Epoch: [61][61/204]	Loss 0.5045 (0.4846)	
training:	Epoch: [61][62/204]	Loss 0.3539 (0.4825)	
training:	Epoch: [61][63/204]	Loss 0.4393 (0.4818)	
training:	Epoch: [61][64/204]	Loss 0.4421 (0.4812)	
training:	Epoch: [61][65/204]	Loss 0.4045 (0.4800)	
training:	Epoch: [61][66/204]	Loss 0.4018 (0.4789)	
training:	Epoch: [61][67/204]	Loss 0.5664 (0.4802)	
training:	Epoch: [61][68/204]	Loss 0.4714 (0.4800)	
training:	Epoch: [61][69/204]	Loss 0.2873 (0.4772)	
training:	Epoch: [61][70/204]	Loss 0.4727 (0.4772)	
training:	Epoch: [61][71/204]	Loss 0.6020 (0.4789)	
training:	Epoch: [61][72/204]	Loss 0.3991 (0.4778)	
training:	Epoch: [61][73/204]	Loss 0.4451 (0.4774)	
training:	Epoch: [61][74/204]	Loss 0.4089 (0.4765)	
training:	Epoch: [61][75/204]	Loss 0.3573 (0.4749)	
training:	Epoch: [61][76/204]	Loss 0.4343 (0.4743)	
training:	Epoch: [61][77/204]	Loss 0.5347 (0.4751)	
training:	Epoch: [61][78/204]	Loss 0.5741 (0.4764)	
training:	Epoch: [61][79/204]	Loss 0.7157 (0.4794)	
training:	Epoch: [61][80/204]	Loss 0.6279 (0.4813)	
training:	Epoch: [61][81/204]	Loss 0.5335 (0.4819)	
training:	Epoch: [61][82/204]	Loss 0.3993 (0.4809)	
training:	Epoch: [61][83/204]	Loss 0.5339 (0.4815)	
training:	Epoch: [61][84/204]	Loss 0.4458 (0.4811)	
training:	Epoch: [61][85/204]	Loss 0.5138 (0.4815)	
training:	Epoch: [61][86/204]	Loss 0.3383 (0.4798)	
training:	Epoch: [61][87/204]	Loss 0.3945 (0.4789)	
training:	Epoch: [61][88/204]	Loss 0.5311 (0.4795)	
training:	Epoch: [61][89/204]	Loss 0.3505 (0.4780)	
training:	Epoch: [61][90/204]	Loss 0.5376 (0.4787)	
training:	Epoch: [61][91/204]	Loss 0.4619 (0.4785)	
training:	Epoch: [61][92/204]	Loss 0.4734 (0.4784)	
training:	Epoch: [61][93/204]	Loss 0.4929 (0.4786)	
training:	Epoch: [61][94/204]	Loss 0.3339 (0.4770)	
training:	Epoch: [61][95/204]	Loss 0.5988 (0.4783)	
training:	Epoch: [61][96/204]	Loss 0.5199 (0.4788)	
training:	Epoch: [61][97/204]	Loss 0.4110 (0.4781)	
training:	Epoch: [61][98/204]	Loss 0.5414 (0.4787)	
training:	Epoch: [61][99/204]	Loss 0.4215 (0.4781)	
training:	Epoch: [61][100/204]	Loss 0.3985 (0.4773)	
training:	Epoch: [61][101/204]	Loss 0.5638 (0.4782)	
training:	Epoch: [61][102/204]	Loss 0.4542 (0.4779)	
training:	Epoch: [61][103/204]	Loss 0.3953 (0.4771)	
training:	Epoch: [61][104/204]	Loss 0.5246 (0.4776)	
training:	Epoch: [61][105/204]	Loss 0.5280 (0.4781)	
training:	Epoch: [61][106/204]	Loss 0.5237 (0.4785)	
training:	Epoch: [61][107/204]	Loss 0.5907 (0.4796)	
training:	Epoch: [61][108/204]	Loss 0.6075 (0.4807)	
training:	Epoch: [61][109/204]	Loss 0.5519 (0.4814)	
training:	Epoch: [61][110/204]	Loss 0.5713 (0.4822)	
training:	Epoch: [61][111/204]	Loss 0.3723 (0.4812)	
training:	Epoch: [61][112/204]	Loss 0.4647 (0.4811)	
training:	Epoch: [61][113/204]	Loss 0.5043 (0.4813)	
training:	Epoch: [61][114/204]	Loss 0.3917 (0.4805)	
training:	Epoch: [61][115/204]	Loss 0.4412 (0.4802)	
training:	Epoch: [61][116/204]	Loss 0.6821 (0.4819)	
training:	Epoch: [61][117/204]	Loss 0.6140 (0.4830)	
training:	Epoch: [61][118/204]	Loss 0.5548 (0.4836)	
training:	Epoch: [61][119/204]	Loss 0.4440 (0.4833)	
training:	Epoch: [61][120/204]	Loss 0.5730 (0.4840)	
training:	Epoch: [61][121/204]	Loss 0.5230 (0.4844)	
training:	Epoch: [61][122/204]	Loss 0.6482 (0.4857)	
training:	Epoch: [61][123/204]	Loss 0.3999 (0.4850)	
training:	Epoch: [61][124/204]	Loss 0.4292 (0.4846)	
training:	Epoch: [61][125/204]	Loss 0.3644 (0.4836)	
training:	Epoch: [61][126/204]	Loss 0.4129 (0.4830)	
training:	Epoch: [61][127/204]	Loss 0.4638 (0.4829)	
training:	Epoch: [61][128/204]	Loss 0.5436 (0.4834)	
training:	Epoch: [61][129/204]	Loss 0.3625 (0.4824)	
training:	Epoch: [61][130/204]	Loss 0.5208 (0.4827)	
training:	Epoch: [61][131/204]	Loss 0.5291 (0.4831)	
training:	Epoch: [61][132/204]	Loss 0.5438 (0.4835)	
training:	Epoch: [61][133/204]	Loss 0.4586 (0.4833)	
training:	Epoch: [61][134/204]	Loss 0.5324 (0.4837)	
training:	Epoch: [61][135/204]	Loss 0.5157 (0.4840)	
training:	Epoch: [61][136/204]	Loss 0.4959 (0.4840)	
training:	Epoch: [61][137/204]	Loss 0.4863 (0.4841)	
training:	Epoch: [61][138/204]	Loss 0.4707 (0.4840)	
training:	Epoch: [61][139/204]	Loss 0.4921 (0.4840)	
training:	Epoch: [61][140/204]	Loss 0.5128 (0.4842)	
training:	Epoch: [61][141/204]	Loss 0.3656 (0.4834)	
training:	Epoch: [61][142/204]	Loss 0.5003 (0.4835)	
training:	Epoch: [61][143/204]	Loss 0.4485 (0.4833)	
training:	Epoch: [61][144/204]	Loss 0.4782 (0.4832)	
training:	Epoch: [61][145/204]	Loss 0.3476 (0.4823)	
training:	Epoch: [61][146/204]	Loss 0.5635 (0.4828)	
training:	Epoch: [61][147/204]	Loss 0.4421 (0.4826)	
training:	Epoch: [61][148/204]	Loss 0.6085 (0.4834)	
training:	Epoch: [61][149/204]	Loss 0.4679 (0.4833)	
training:	Epoch: [61][150/204]	Loss 0.6046 (0.4841)	
training:	Epoch: [61][151/204]	Loss 0.5610 (0.4846)	
training:	Epoch: [61][152/204]	Loss 0.3422 (0.4837)	
training:	Epoch: [61][153/204]	Loss 0.4700 (0.4836)	
training:	Epoch: [61][154/204]	Loss 0.5185 (0.4838)	
training:	Epoch: [61][155/204]	Loss 0.4268 (0.4835)	
training:	Epoch: [61][156/204]	Loss 0.4792 (0.4834)	
training:	Epoch: [61][157/204]	Loss 0.3804 (0.4828)	
training:	Epoch: [61][158/204]	Loss 0.4756 (0.4827)	
training:	Epoch: [61][159/204]	Loss 0.5250 (0.4830)	
training:	Epoch: [61][160/204]	Loss 0.3510 (0.4822)	
training:	Epoch: [61][161/204]	Loss 0.4135 (0.4817)	
training:	Epoch: [61][162/204]	Loss 0.4718 (0.4817)	
training:	Epoch: [61][163/204]	Loss 0.6287 (0.4826)	
training:	Epoch: [61][164/204]	Loss 0.4243 (0.4822)	
training:	Epoch: [61][165/204]	Loss 0.4131 (0.4818)	
training:	Epoch: [61][166/204]	Loss 0.5456 (0.4822)	
training:	Epoch: [61][167/204]	Loss 0.4234 (0.4818)	
training:	Epoch: [61][168/204]	Loss 0.4428 (0.4816)	
training:	Epoch: [61][169/204]	Loss 0.5407 (0.4820)	
training:	Epoch: [61][170/204]	Loss 0.4098 (0.4815)	
training:	Epoch: [61][171/204]	Loss 0.5560 (0.4820)	
training:	Epoch: [61][172/204]	Loss 0.3825 (0.4814)	
training:	Epoch: [61][173/204]	Loss 0.3792 (0.4808)	
training:	Epoch: [61][174/204]	Loss 0.3371 (0.4800)	
training:	Epoch: [61][175/204]	Loss 0.5407 (0.4803)	
training:	Epoch: [61][176/204]	Loss 0.5327 (0.4806)	
training:	Epoch: [61][177/204]	Loss 0.5215 (0.4809)	
training:	Epoch: [61][178/204]	Loss 0.5210 (0.4811)	
training:	Epoch: [61][179/204]	Loss 0.4185 (0.4807)	
training:	Epoch: [61][180/204]	Loss 0.4634 (0.4806)	
training:	Epoch: [61][181/204]	Loss 0.4566 (0.4805)	
training:	Epoch: [61][182/204]	Loss 0.4715 (0.4805)	
training:	Epoch: [61][183/204]	Loss 0.5274 (0.4807)	
training:	Epoch: [61][184/204]	Loss 0.3699 (0.4801)	
training:	Epoch: [61][185/204]	Loss 0.5044 (0.4802)	
training:	Epoch: [61][186/204]	Loss 0.3615 (0.4796)	
training:	Epoch: [61][187/204]	Loss 0.4855 (0.4796)	
training:	Epoch: [61][188/204]	Loss 0.4276 (0.4794)	
training:	Epoch: [61][189/204]	Loss 0.4974 (0.4794)	
training:	Epoch: [61][190/204]	Loss 0.4530 (0.4793)	
training:	Epoch: [61][191/204]	Loss 0.5336 (0.4796)	
training:	Epoch: [61][192/204]	Loss 0.5255 (0.4798)	
training:	Epoch: [61][193/204]	Loss 0.5516 (0.4802)	
training:	Epoch: [61][194/204]	Loss 0.3904 (0.4797)	
training:	Epoch: [61][195/204]	Loss 0.4073 (0.4794)	
training:	Epoch: [61][196/204]	Loss 0.3556 (0.4787)	
training:	Epoch: [61][197/204]	Loss 0.4897 (0.4788)	
training:	Epoch: [61][198/204]	Loss 0.3840 (0.4783)	
training:	Epoch: [61][199/204]	Loss 0.4176 (0.4780)	
training:	Epoch: [61][200/204]	Loss 0.4705 (0.4780)	
training:	Epoch: [61][201/204]	Loss 0.4875 (0.4780)	
training:	Epoch: [61][202/204]	Loss 0.3824 (0.4775)	
training:	Epoch: [61][203/204]	Loss 0.5436 (0.4779)	
training:	Epoch: [61][204/204]	Loss 0.5921 (0.4784)	
Training:	 Loss: 0.4777

Training:	 ACC: 0.7868 0.7872 0.7981 0.7755
Validation:	 ACC: 0.7782 0.7790 0.7973 0.7590
Validation:	 Best_BACC: 0.7782 0.7790 0.7973 0.7590
Validation:	 Loss: 0.4818
Pretraining:	Epoch 62/120
----------
training:	Epoch: [62][1/204]	Loss 0.4902 (0.4902)	
training:	Epoch: [62][2/204]	Loss 0.3468 (0.4185)	
training:	Epoch: [62][3/204]	Loss 0.3954 (0.4108)	
training:	Epoch: [62][4/204]	Loss 0.5226 (0.4387)	
training:	Epoch: [62][5/204]	Loss 0.5665 (0.4643)	
training:	Epoch: [62][6/204]	Loss 0.4871 (0.4681)	
training:	Epoch: [62][7/204]	Loss 0.4777 (0.4695)	
training:	Epoch: [62][8/204]	Loss 0.3530 (0.4549)	
training:	Epoch: [62][9/204]	Loss 0.5117 (0.4612)	
training:	Epoch: [62][10/204]	Loss 0.5528 (0.4704)	
training:	Epoch: [62][11/204]	Loss 0.5292 (0.4757)	
training:	Epoch: [62][12/204]	Loss 0.6087 (0.4868)	
training:	Epoch: [62][13/204]	Loss 0.4979 (0.4877)	
training:	Epoch: [62][14/204]	Loss 0.4310 (0.4836)	
training:	Epoch: [62][15/204]	Loss 0.6068 (0.4918)	
training:	Epoch: [62][16/204]	Loss 0.3383 (0.4822)	
training:	Epoch: [62][17/204]	Loss 0.5452 (0.4859)	
training:	Epoch: [62][18/204]	Loss 0.5756 (0.4909)	
training:	Epoch: [62][19/204]	Loss 0.3372 (0.4828)	
training:	Epoch: [62][20/204]	Loss 0.3293 (0.4751)	
training:	Epoch: [62][21/204]	Loss 0.5275 (0.4776)	
training:	Epoch: [62][22/204]	Loss 0.4486 (0.4763)	
training:	Epoch: [62][23/204]	Loss 0.5304 (0.4787)	
training:	Epoch: [62][24/204]	Loss 0.4374 (0.4769)	
training:	Epoch: [62][25/204]	Loss 0.5158 (0.4785)	
training:	Epoch: [62][26/204]	Loss 0.5194 (0.4801)	
training:	Epoch: [62][27/204]	Loss 0.4880 (0.4804)	
training:	Epoch: [62][28/204]	Loss 0.5129 (0.4815)	
training:	Epoch: [62][29/204]	Loss 0.4848 (0.4816)	
training:	Epoch: [62][30/204]	Loss 0.5402 (0.4836)	
training:	Epoch: [62][31/204]	Loss 0.5697 (0.4864)	
training:	Epoch: [62][32/204]	Loss 0.5739 (0.4891)	
training:	Epoch: [62][33/204]	Loss 0.4264 (0.4872)	
training:	Epoch: [62][34/204]	Loss 0.4796 (0.4870)	
training:	Epoch: [62][35/204]	Loss 0.4362 (0.4855)	
training:	Epoch: [62][36/204]	Loss 0.3292 (0.4812)	
training:	Epoch: [62][37/204]	Loss 0.4734 (0.4810)	
training:	Epoch: [62][38/204]	Loss 0.4068 (0.4790)	
training:	Epoch: [62][39/204]	Loss 0.4528 (0.4783)	
training:	Epoch: [62][40/204]	Loss 0.4861 (0.4785)	
training:	Epoch: [62][41/204]	Loss 0.4742 (0.4784)	
training:	Epoch: [62][42/204]	Loss 0.4528 (0.4778)	
training:	Epoch: [62][43/204]	Loss 0.4989 (0.4783)	
training:	Epoch: [62][44/204]	Loss 0.4919 (0.4786)	
training:	Epoch: [62][45/204]	Loss 0.5964 (0.4812)	
training:	Epoch: [62][46/204]	Loss 0.4762 (0.4811)	
training:	Epoch: [62][47/204]	Loss 0.4358 (0.4802)	
training:	Epoch: [62][48/204]	Loss 0.3950 (0.4784)	
training:	Epoch: [62][49/204]	Loss 0.4811 (0.4784)	
training:	Epoch: [62][50/204]	Loss 0.5328 (0.4795)	
training:	Epoch: [62][51/204]	Loss 0.3550 (0.4771)	
training:	Epoch: [62][52/204]	Loss 0.4883 (0.4773)	
training:	Epoch: [62][53/204]	Loss 0.4191 (0.4762)	
training:	Epoch: [62][54/204]	Loss 0.4683 (0.4761)	
training:	Epoch: [62][55/204]	Loss 0.4811 (0.4762)	
training:	Epoch: [62][56/204]	Loss 0.3956 (0.4747)	
training:	Epoch: [62][57/204]	Loss 0.6424 (0.4777)	
training:	Epoch: [62][58/204]	Loss 0.4602 (0.4774)	
training:	Epoch: [62][59/204]	Loss 0.4861 (0.4775)	
training:	Epoch: [62][60/204]	Loss 0.6057 (0.4796)	
training:	Epoch: [62][61/204]	Loss 0.7046 (0.4833)	
training:	Epoch: [62][62/204]	Loss 0.5095 (0.4838)	
training:	Epoch: [62][63/204]	Loss 0.4799 (0.4837)	
training:	Epoch: [62][64/204]	Loss 0.4949 (0.4839)	
training:	Epoch: [62][65/204]	Loss 0.4647 (0.4836)	
training:	Epoch: [62][66/204]	Loss 0.4676 (0.4833)	
training:	Epoch: [62][67/204]	Loss 0.4980 (0.4836)	
training:	Epoch: [62][68/204]	Loss 0.4881 (0.4836)	
training:	Epoch: [62][69/204]	Loss 0.4401 (0.4830)	
training:	Epoch: [62][70/204]	Loss 0.5303 (0.4837)	
training:	Epoch: [62][71/204]	Loss 0.6398 (0.4859)	
training:	Epoch: [62][72/204]	Loss 0.5548 (0.4868)	
training:	Epoch: [62][73/204]	Loss 0.4810 (0.4867)	
training:	Epoch: [62][74/204]	Loss 0.4185 (0.4858)	
training:	Epoch: [62][75/204]	Loss 0.4591 (0.4855)	
training:	Epoch: [62][76/204]	Loss 0.4769 (0.4854)	
training:	Epoch: [62][77/204]	Loss 0.5068 (0.4856)	
training:	Epoch: [62][78/204]	Loss 0.4845 (0.4856)	
training:	Epoch: [62][79/204]	Loss 0.4647 (0.4854)	
training:	Epoch: [62][80/204]	Loss 0.4556 (0.4850)	
training:	Epoch: [62][81/204]	Loss 0.4704 (0.4848)	
training:	Epoch: [62][82/204]	Loss 0.4366 (0.4842)	
training:	Epoch: [62][83/204]	Loss 0.6869 (0.4867)	
training:	Epoch: [62][84/204]	Loss 0.4842 (0.4866)	
training:	Epoch: [62][85/204]	Loss 0.6329 (0.4883)	
training:	Epoch: [62][86/204]	Loss 0.4149 (0.4875)	
training:	Epoch: [62][87/204]	Loss 0.5294 (0.4880)	
training:	Epoch: [62][88/204]	Loss 0.4863 (0.4880)	
training:	Epoch: [62][89/204]	Loss 0.4812 (0.4879)	
training:	Epoch: [62][90/204]	Loss 0.6870 (0.4901)	
training:	Epoch: [62][91/204]	Loss 0.3813 (0.4889)	
training:	Epoch: [62][92/204]	Loss 0.4621 (0.4886)	
training:	Epoch: [62][93/204]	Loss 0.5587 (0.4894)	
training:	Epoch: [62][94/204]	Loss 0.5746 (0.4903)	
training:	Epoch: [62][95/204]	Loss 0.4355 (0.4897)	
training:	Epoch: [62][96/204]	Loss 0.5755 (0.4906)	
training:	Epoch: [62][97/204]	Loss 0.5132 (0.4908)	
training:	Epoch: [62][98/204]	Loss 0.4006 (0.4899)	
training:	Epoch: [62][99/204]	Loss 0.4404 (0.4894)	
training:	Epoch: [62][100/204]	Loss 0.5119 (0.4896)	
training:	Epoch: [62][101/204]	Loss 0.3909 (0.4886)	
training:	Epoch: [62][102/204]	Loss 0.6215 (0.4899)	
training:	Epoch: [62][103/204]	Loss 0.4784 (0.4898)	
training:	Epoch: [62][104/204]	Loss 0.4099 (0.4891)	
training:	Epoch: [62][105/204]	Loss 0.4639 (0.4888)	
training:	Epoch: [62][106/204]	Loss 0.3382 (0.4874)	
training:	Epoch: [62][107/204]	Loss 0.5332 (0.4878)	
training:	Epoch: [62][108/204]	Loss 0.4628 (0.4876)	
training:	Epoch: [62][109/204]	Loss 0.5324 (0.4880)	
training:	Epoch: [62][110/204]	Loss 0.4624 (0.4878)	
training:	Epoch: [62][111/204]	Loss 0.4565 (0.4875)	
training:	Epoch: [62][112/204]	Loss 0.5567 (0.4881)	
training:	Epoch: [62][113/204]	Loss 0.5015 (0.4882)	
training:	Epoch: [62][114/204]	Loss 0.4936 (0.4883)	
training:	Epoch: [62][115/204]	Loss 0.4998 (0.4884)	
training:	Epoch: [62][116/204]	Loss 0.5268 (0.4887)	
training:	Epoch: [62][117/204]	Loss 0.3925 (0.4879)	
training:	Epoch: [62][118/204]	Loss 0.6907 (0.4896)	
training:	Epoch: [62][119/204]	Loss 0.5110 (0.4898)	
training:	Epoch: [62][120/204]	Loss 0.4750 (0.4897)	
training:	Epoch: [62][121/204]	Loss 0.4870 (0.4896)	
training:	Epoch: [62][122/204]	Loss 0.3743 (0.4887)	
training:	Epoch: [62][123/204]	Loss 0.4542 (0.4884)	
training:	Epoch: [62][124/204]	Loss 0.5610 (0.4890)	
training:	Epoch: [62][125/204]	Loss 0.5075 (0.4892)	
training:	Epoch: [62][126/204]	Loss 0.4923 (0.4892)	
training:	Epoch: [62][127/204]	Loss 0.4790 (0.4891)	
training:	Epoch: [62][128/204]	Loss 0.4163 (0.4885)	
training:	Epoch: [62][129/204]	Loss 0.5063 (0.4887)	
training:	Epoch: [62][130/204]	Loss 0.3551 (0.4876)	
training:	Epoch: [62][131/204]	Loss 0.3314 (0.4864)	
training:	Epoch: [62][132/204]	Loss 0.5259 (0.4867)	
training:	Epoch: [62][133/204]	Loss 0.4641 (0.4866)	
training:	Epoch: [62][134/204]	Loss 0.4433 (0.4863)	
training:	Epoch: [62][135/204]	Loss 0.5065 (0.4864)	
training:	Epoch: [62][136/204]	Loss 0.6038 (0.4873)	
training:	Epoch: [62][137/204]	Loss 0.6834 (0.4887)	
training:	Epoch: [62][138/204]	Loss 0.5582 (0.4892)	
training:	Epoch: [62][139/204]	Loss 0.4605 (0.4890)	
training:	Epoch: [62][140/204]	Loss 0.3668 (0.4881)	
training:	Epoch: [62][141/204]	Loss 0.5095 (0.4883)	
training:	Epoch: [62][142/204]	Loss 0.4684 (0.4881)	
training:	Epoch: [62][143/204]	Loss 0.4402 (0.4878)	
training:	Epoch: [62][144/204]	Loss 0.4716 (0.4877)	
training:	Epoch: [62][145/204]	Loss 0.5403 (0.4880)	
training:	Epoch: [62][146/204]	Loss 0.5779 (0.4887)	
training:	Epoch: [62][147/204]	Loss 0.3708 (0.4879)	
training:	Epoch: [62][148/204]	Loss 0.5164 (0.4881)	
training:	Epoch: [62][149/204]	Loss 0.4543 (0.4878)	
training:	Epoch: [62][150/204]	Loss 0.4162 (0.4874)	
training:	Epoch: [62][151/204]	Loss 0.3886 (0.4867)	
training:	Epoch: [62][152/204]	Loss 0.4963 (0.4868)	
training:	Epoch: [62][153/204]	Loss 0.4050 (0.4862)	
training:	Epoch: [62][154/204]	Loss 0.4949 (0.4863)	
training:	Epoch: [62][155/204]	Loss 0.5496 (0.4867)	
training:	Epoch: [62][156/204]	Loss 0.4446 (0.4864)	
training:	Epoch: [62][157/204]	Loss 0.4261 (0.4860)	
training:	Epoch: [62][158/204]	Loss 0.4718 (0.4859)	
training:	Epoch: [62][159/204]	Loss 0.5921 (0.4866)	
training:	Epoch: [62][160/204]	Loss 0.4423 (0.4863)	
training:	Epoch: [62][161/204]	Loss 0.3131 (0.4853)	
training:	Epoch: [62][162/204]	Loss 0.4965 (0.4853)	
training:	Epoch: [62][163/204]	Loss 0.3570 (0.4845)	
training:	Epoch: [62][164/204]	Loss 0.5963 (0.4852)	
training:	Epoch: [62][165/204]	Loss 0.3699 (0.4845)	
training:	Epoch: [62][166/204]	Loss 0.5254 (0.4848)	
training:	Epoch: [62][167/204]	Loss 0.4971 (0.4848)	
training:	Epoch: [62][168/204]	Loss 0.4444 (0.4846)	
training:	Epoch: [62][169/204]	Loss 0.2946 (0.4835)	
training:	Epoch: [62][170/204]	Loss 0.4520 (0.4833)	
training:	Epoch: [62][171/204]	Loss 0.3459 (0.4825)	
training:	Epoch: [62][172/204]	Loss 0.5004 (0.4826)	
training:	Epoch: [62][173/204]	Loss 0.4205 (0.4822)	
training:	Epoch: [62][174/204]	Loss 0.4285 (0.4819)	
training:	Epoch: [62][175/204]	Loss 0.4806 (0.4819)	
training:	Epoch: [62][176/204]	Loss 0.3457 (0.4811)	
training:	Epoch: [62][177/204]	Loss 0.4980 (0.4812)	
training:	Epoch: [62][178/204]	Loss 0.4305 (0.4810)	
training:	Epoch: [62][179/204]	Loss 0.4113 (0.4806)	
training:	Epoch: [62][180/204]	Loss 0.3969 (0.4801)	
training:	Epoch: [62][181/204]	Loss 0.4817 (0.4801)	
training:	Epoch: [62][182/204]	Loss 0.4492 (0.4799)	
training:	Epoch: [62][183/204]	Loss 0.7183 (0.4812)	
training:	Epoch: [62][184/204]	Loss 0.4061 (0.4808)	
training:	Epoch: [62][185/204]	Loss 0.5413 (0.4812)	
training:	Epoch: [62][186/204]	Loss 0.6272 (0.4819)	
training:	Epoch: [62][187/204]	Loss 0.4479 (0.4818)	
training:	Epoch: [62][188/204]	Loss 0.5294 (0.4820)	
training:	Epoch: [62][189/204]	Loss 0.5756 (0.4825)	
training:	Epoch: [62][190/204]	Loss 0.3678 (0.4819)	
training:	Epoch: [62][191/204]	Loss 0.2534 (0.4807)	
training:	Epoch: [62][192/204]	Loss 0.4862 (0.4807)	
training:	Epoch: [62][193/204]	Loss 0.4453 (0.4806)	
training:	Epoch: [62][194/204]	Loss 0.4724 (0.4805)	
training:	Epoch: [62][195/204]	Loss 0.5807 (0.4810)	
training:	Epoch: [62][196/204]	Loss 0.3758 (0.4805)	
training:	Epoch: [62][197/204]	Loss 0.3203 (0.4797)	
training:	Epoch: [62][198/204]	Loss 0.5330 (0.4799)	
training:	Epoch: [62][199/204]	Loss 0.3451 (0.4793)	
training:	Epoch: [62][200/204]	Loss 0.4004 (0.4789)	
training:	Epoch: [62][201/204]	Loss 0.3586 (0.4783)	
training:	Epoch: [62][202/204]	Loss 0.3552 (0.4777)	
training:	Epoch: [62][203/204]	Loss 0.5335 (0.4779)	
training:	Epoch: [62][204/204]	Loss 0.3748 (0.4774)	
Training:	 Loss: 0.4767

Training:	 ACC: 0.7871 0.7877 0.8019 0.7723
Validation:	 ACC: 0.7802 0.7812 0.8025 0.7578
Validation:	 Best_BACC: 0.7802 0.7812 0.8025 0.7578
Validation:	 Loss: 0.4803
Pretraining:	Epoch 63/120
----------
training:	Epoch: [63][1/204]	Loss 0.4957 (0.4957)	
training:	Epoch: [63][2/204]	Loss 0.3253 (0.4105)	
training:	Epoch: [63][3/204]	Loss 0.5061 (0.4424)	
training:	Epoch: [63][4/204]	Loss 0.4484 (0.4439)	
training:	Epoch: [63][5/204]	Loss 0.4325 (0.4416)	
training:	Epoch: [63][6/204]	Loss 0.5392 (0.4579)	
training:	Epoch: [63][7/204]	Loss 0.3792 (0.4466)	
training:	Epoch: [63][8/204]	Loss 0.3957 (0.4403)	
training:	Epoch: [63][9/204]	Loss 0.5488 (0.4523)	
training:	Epoch: [63][10/204]	Loss 0.2645 (0.4336)	
training:	Epoch: [63][11/204]	Loss 0.4489 (0.4349)	
training:	Epoch: [63][12/204]	Loss 0.4909 (0.4396)	
training:	Epoch: [63][13/204]	Loss 0.4054 (0.4370)	
training:	Epoch: [63][14/204]	Loss 0.3555 (0.4312)	
training:	Epoch: [63][15/204]	Loss 0.4577 (0.4329)	
training:	Epoch: [63][16/204]	Loss 0.3655 (0.4287)	
training:	Epoch: [63][17/204]	Loss 0.5427 (0.4354)	
training:	Epoch: [63][18/204]	Loss 0.5673 (0.4427)	
training:	Epoch: [63][19/204]	Loss 0.4011 (0.4406)	
training:	Epoch: [63][20/204]	Loss 0.4345 (0.4403)	
training:	Epoch: [63][21/204]	Loss 0.3619 (0.4365)	
training:	Epoch: [63][22/204]	Loss 0.4020 (0.4350)	
training:	Epoch: [63][23/204]	Loss 0.5732 (0.4410)	
training:	Epoch: [63][24/204]	Loss 0.4847 (0.4428)	
training:	Epoch: [63][25/204]	Loss 0.4641 (0.4436)	
training:	Epoch: [63][26/204]	Loss 0.4494 (0.4439)	
training:	Epoch: [63][27/204]	Loss 0.5755 (0.4487)	
training:	Epoch: [63][28/204]	Loss 0.5103 (0.4509)	
training:	Epoch: [63][29/204]	Loss 0.4624 (0.4513)	
training:	Epoch: [63][30/204]	Loss 0.4815 (0.4523)	
training:	Epoch: [63][31/204]	Loss 0.5530 (0.4556)	
training:	Epoch: [63][32/204]	Loss 0.4538 (0.4555)	
training:	Epoch: [63][33/204]	Loss 0.4081 (0.4541)	
training:	Epoch: [63][34/204]	Loss 0.4946 (0.4553)	
training:	Epoch: [63][35/204]	Loss 0.4747 (0.4558)	
training:	Epoch: [63][36/204]	Loss 0.4468 (0.4556)	
training:	Epoch: [63][37/204]	Loss 0.5292 (0.4576)	
training:	Epoch: [63][38/204]	Loss 0.4102 (0.4563)	
training:	Epoch: [63][39/204]	Loss 0.5183 (0.4579)	
training:	Epoch: [63][40/204]	Loss 0.4776 (0.4584)	
training:	Epoch: [63][41/204]	Loss 0.5417 (0.4604)	
training:	Epoch: [63][42/204]	Loss 0.4534 (0.4603)	
training:	Epoch: [63][43/204]	Loss 0.4208 (0.4594)	
training:	Epoch: [63][44/204]	Loss 0.4168 (0.4584)	
training:	Epoch: [63][45/204]	Loss 0.4364 (0.4579)	
training:	Epoch: [63][46/204]	Loss 0.4182 (0.4570)	
training:	Epoch: [63][47/204]	Loss 0.4113 (0.4561)	
training:	Epoch: [63][48/204]	Loss 0.5461 (0.4579)	
training:	Epoch: [63][49/204]	Loss 0.4776 (0.4583)	
training:	Epoch: [63][50/204]	Loss 0.5247 (0.4597)	
training:	Epoch: [63][51/204]	Loss 0.4170 (0.4588)	
training:	Epoch: [63][52/204]	Loss 0.4796 (0.4592)	
training:	Epoch: [63][53/204]	Loss 0.4614 (0.4593)	
training:	Epoch: [63][54/204]	Loss 0.5778 (0.4615)	
training:	Epoch: [63][55/204]	Loss 0.4688 (0.4616)	
training:	Epoch: [63][56/204]	Loss 0.7516 (0.4668)	
training:	Epoch: [63][57/204]	Loss 0.6319 (0.4697)	
training:	Epoch: [63][58/204]	Loss 0.4829 (0.4699)	
training:	Epoch: [63][59/204]	Loss 0.4885 (0.4702)	
training:	Epoch: [63][60/204]	Loss 0.4283 (0.4695)	
training:	Epoch: [63][61/204]	Loss 0.4410 (0.4690)	
training:	Epoch: [63][62/204]	Loss 0.5473 (0.4703)	
training:	Epoch: [63][63/204]	Loss 0.5109 (0.4710)	
training:	Epoch: [63][64/204]	Loss 0.6119 (0.4732)	
training:	Epoch: [63][65/204]	Loss 0.4338 (0.4726)	
training:	Epoch: [63][66/204]	Loss 0.6972 (0.4760)	
training:	Epoch: [63][67/204]	Loss 0.5962 (0.4777)	
training:	Epoch: [63][68/204]	Loss 0.3869 (0.4764)	
training:	Epoch: [63][69/204]	Loss 0.3638 (0.4748)	
training:	Epoch: [63][70/204]	Loss 0.4419 (0.4743)	
training:	Epoch: [63][71/204]	Loss 0.3268 (0.4722)	
training:	Epoch: [63][72/204]	Loss 0.4399 (0.4718)	
training:	Epoch: [63][73/204]	Loss 0.4103 (0.4709)	
training:	Epoch: [63][74/204]	Loss 0.4302 (0.4704)	
training:	Epoch: [63][75/204]	Loss 0.4739 (0.4704)	
training:	Epoch: [63][76/204]	Loss 0.5875 (0.4720)	
training:	Epoch: [63][77/204]	Loss 0.5062 (0.4724)	
training:	Epoch: [63][78/204]	Loss 0.4768 (0.4725)	
training:	Epoch: [63][79/204]	Loss 0.4037 (0.4716)	
training:	Epoch: [63][80/204]	Loss 0.4261 (0.4710)	
training:	Epoch: [63][81/204]	Loss 0.4974 (0.4714)	
training:	Epoch: [63][82/204]	Loss 0.4539 (0.4712)	
training:	Epoch: [63][83/204]	Loss 0.5514 (0.4721)	
training:	Epoch: [63][84/204]	Loss 0.6341 (0.4740)	
training:	Epoch: [63][85/204]	Loss 0.4690 (0.4740)	
training:	Epoch: [63][86/204]	Loss 0.4726 (0.4740)	
training:	Epoch: [63][87/204]	Loss 0.4244 (0.4734)	
training:	Epoch: [63][88/204]	Loss 0.4230 (0.4728)	
training:	Epoch: [63][89/204]	Loss 0.4405 (0.4725)	
training:	Epoch: [63][90/204]	Loss 0.4364 (0.4721)	
training:	Epoch: [63][91/204]	Loss 0.5134 (0.4725)	
training:	Epoch: [63][92/204]	Loss 0.4005 (0.4717)	
training:	Epoch: [63][93/204]	Loss 0.3941 (0.4709)	
training:	Epoch: [63][94/204]	Loss 0.4988 (0.4712)	
training:	Epoch: [63][95/204]	Loss 0.5334 (0.4719)	
training:	Epoch: [63][96/204]	Loss 0.3484 (0.4706)	
training:	Epoch: [63][97/204]	Loss 0.4958 (0.4708)	
training:	Epoch: [63][98/204]	Loss 0.4719 (0.4708)	
training:	Epoch: [63][99/204]	Loss 0.4331 (0.4705)	
training:	Epoch: [63][100/204]	Loss 0.4450 (0.4702)	
training:	Epoch: [63][101/204]	Loss 0.4433 (0.4699)	
training:	Epoch: [63][102/204]	Loss 0.6653 (0.4719)	
training:	Epoch: [63][103/204]	Loss 0.4274 (0.4714)	
training:	Epoch: [63][104/204]	Loss 0.6956 (0.4736)	
training:	Epoch: [63][105/204]	Loss 0.3961 (0.4728)	
training:	Epoch: [63][106/204]	Loss 0.4294 (0.4724)	
training:	Epoch: [63][107/204]	Loss 0.4329 (0.4721)	
training:	Epoch: [63][108/204]	Loss 0.4396 (0.4718)	
training:	Epoch: [63][109/204]	Loss 0.5457 (0.4724)	
training:	Epoch: [63][110/204]	Loss 0.4826 (0.4725)	
training:	Epoch: [63][111/204]	Loss 0.3710 (0.4716)	
training:	Epoch: [63][112/204]	Loss 0.4975 (0.4718)	
training:	Epoch: [63][113/204]	Loss 0.5510 (0.4725)	
training:	Epoch: [63][114/204]	Loss 0.3369 (0.4714)	
training:	Epoch: [63][115/204]	Loss 0.4370 (0.4711)	
training:	Epoch: [63][116/204]	Loss 0.5496 (0.4717)	
training:	Epoch: [63][117/204]	Loss 0.5963 (0.4728)	
training:	Epoch: [63][118/204]	Loss 0.4556 (0.4727)	
training:	Epoch: [63][119/204]	Loss 0.5152 (0.4730)	
training:	Epoch: [63][120/204]	Loss 0.3538 (0.4720)	
training:	Epoch: [63][121/204]	Loss 0.5249 (0.4725)	
training:	Epoch: [63][122/204]	Loss 0.4517 (0.4723)	
training:	Epoch: [63][123/204]	Loss 0.4806 (0.4724)	
training:	Epoch: [63][124/204]	Loss 0.4718 (0.4723)	
training:	Epoch: [63][125/204]	Loss 0.4470 (0.4721)	
training:	Epoch: [63][126/204]	Loss 0.5383 (0.4727)	
training:	Epoch: [63][127/204]	Loss 0.3685 (0.4719)	
training:	Epoch: [63][128/204]	Loss 0.4547 (0.4717)	
training:	Epoch: [63][129/204]	Loss 0.4254 (0.4714)	
training:	Epoch: [63][130/204]	Loss 0.3835 (0.4707)	
training:	Epoch: [63][131/204]	Loss 0.3198 (0.4695)	
training:	Epoch: [63][132/204]	Loss 0.2969 (0.4682)	
training:	Epoch: [63][133/204]	Loss 0.3506 (0.4673)	
training:	Epoch: [63][134/204]	Loss 0.4944 (0.4675)	
training:	Epoch: [63][135/204]	Loss 0.3361 (0.4666)	
training:	Epoch: [63][136/204]	Loss 0.5390 (0.4671)	
training:	Epoch: [63][137/204]	Loss 0.4903 (0.4673)	
training:	Epoch: [63][138/204]	Loss 0.4242 (0.4670)	
training:	Epoch: [63][139/204]	Loss 0.4786 (0.4670)	
training:	Epoch: [63][140/204]	Loss 0.3541 (0.4662)	
training:	Epoch: [63][141/204]	Loss 0.5613 (0.4669)	
training:	Epoch: [63][142/204]	Loss 0.5661 (0.4676)	
training:	Epoch: [63][143/204]	Loss 0.6094 (0.4686)	
training:	Epoch: [63][144/204]	Loss 0.3427 (0.4677)	
training:	Epoch: [63][145/204]	Loss 0.4435 (0.4676)	
training:	Epoch: [63][146/204]	Loss 0.6001 (0.4685)	
training:	Epoch: [63][147/204]	Loss 0.3182 (0.4674)	
training:	Epoch: [63][148/204]	Loss 0.6395 (0.4686)	
training:	Epoch: [63][149/204]	Loss 0.3341 (0.4677)	
training:	Epoch: [63][150/204]	Loss 0.3784 (0.4671)	
training:	Epoch: [63][151/204]	Loss 0.5525 (0.4677)	
training:	Epoch: [63][152/204]	Loss 0.4125 (0.4673)	
training:	Epoch: [63][153/204]	Loss 0.5307 (0.4677)	
training:	Epoch: [63][154/204]	Loss 0.4434 (0.4676)	
training:	Epoch: [63][155/204]	Loss 0.5138 (0.4679)	
training:	Epoch: [63][156/204]	Loss 0.3729 (0.4673)	
training:	Epoch: [63][157/204]	Loss 0.3519 (0.4665)	
training:	Epoch: [63][158/204]	Loss 0.4020 (0.4661)	
training:	Epoch: [63][159/204]	Loss 0.4556 (0.4660)	
training:	Epoch: [63][160/204]	Loss 0.4955 (0.4662)	
training:	Epoch: [63][161/204]	Loss 0.6295 (0.4672)	
training:	Epoch: [63][162/204]	Loss 0.3928 (0.4668)	
training:	Epoch: [63][163/204]	Loss 0.5259 (0.4671)	
training:	Epoch: [63][164/204]	Loss 0.4700 (0.4672)	
training:	Epoch: [63][165/204]	Loss 0.5776 (0.4678)	
training:	Epoch: [63][166/204]	Loss 0.4050 (0.4675)	
training:	Epoch: [63][167/204]	Loss 0.4974 (0.4676)	
training:	Epoch: [63][168/204]	Loss 0.4694 (0.4676)	
training:	Epoch: [63][169/204]	Loss 0.3987 (0.4672)	
training:	Epoch: [63][170/204]	Loss 0.6327 (0.4682)	
training:	Epoch: [63][171/204]	Loss 0.4251 (0.4680)	
training:	Epoch: [63][172/204]	Loss 0.7608 (0.4697)	
training:	Epoch: [63][173/204]	Loss 0.3987 (0.4692)	
training:	Epoch: [63][174/204]	Loss 0.3678 (0.4687)	
training:	Epoch: [63][175/204]	Loss 0.6014 (0.4694)	
training:	Epoch: [63][176/204]	Loss 0.5128 (0.4697)	
training:	Epoch: [63][177/204]	Loss 0.4569 (0.4696)	
training:	Epoch: [63][178/204]	Loss 0.3621 (0.4690)	
training:	Epoch: [63][179/204]	Loss 0.5560 (0.4695)	
training:	Epoch: [63][180/204]	Loss 0.5576 (0.4700)	
training:	Epoch: [63][181/204]	Loss 0.4242 (0.4697)	
training:	Epoch: [63][182/204]	Loss 0.5903 (0.4704)	
training:	Epoch: [63][183/204]	Loss 0.6555 (0.4714)	
training:	Epoch: [63][184/204]	Loss 0.5816 (0.4720)	
training:	Epoch: [63][185/204]	Loss 0.3447 (0.4713)	
training:	Epoch: [63][186/204]	Loss 0.5349 (0.4716)	
training:	Epoch: [63][187/204]	Loss 0.5291 (0.4720)	
training:	Epoch: [63][188/204]	Loss 0.4380 (0.4718)	
training:	Epoch: [63][189/204]	Loss 0.4933 (0.4719)	
training:	Epoch: [63][190/204]	Loss 0.5314 (0.4722)	
training:	Epoch: [63][191/204]	Loss 0.6599 (0.4732)	
training:	Epoch: [63][192/204]	Loss 0.5431 (0.4735)	
training:	Epoch: [63][193/204]	Loss 0.6308 (0.4744)	
training:	Epoch: [63][194/204]	Loss 0.5171 (0.4746)	
training:	Epoch: [63][195/204]	Loss 0.4614 (0.4745)	
training:	Epoch: [63][196/204]	Loss 0.4895 (0.4746)	
training:	Epoch: [63][197/204]	Loss 0.4471 (0.4745)	
training:	Epoch: [63][198/204]	Loss 0.4728 (0.4744)	
training:	Epoch: [63][199/204]	Loss 0.4657 (0.4744)	
training:	Epoch: [63][200/204]	Loss 0.4581 (0.4743)	
training:	Epoch: [63][201/204]	Loss 0.4309 (0.4741)	
training:	Epoch: [63][202/204]	Loss 0.5885 (0.4747)	
training:	Epoch: [63][203/204]	Loss 0.5444 (0.4750)	
training:	Epoch: [63][204/204]	Loss 0.3845 (0.4746)	
Training:	 Loss: 0.4738

Training:	 ACC: 0.7885 0.7889 0.7981 0.7790
Validation:	 ACC: 0.7830 0.7838 0.8014 0.7646
Validation:	 Best_BACC: 0.7830 0.7838 0.8014 0.7646
Validation:	 Loss: 0.4783
Pretraining:	Epoch 64/120
----------
training:	Epoch: [64][1/204]	Loss 0.4292 (0.4292)	
training:	Epoch: [64][2/204]	Loss 0.3479 (0.3886)	
training:	Epoch: [64][3/204]	Loss 0.5515 (0.4429)	
training:	Epoch: [64][4/204]	Loss 0.3643 (0.4232)	
training:	Epoch: [64][5/204]	Loss 0.5686 (0.4523)	
training:	Epoch: [64][6/204]	Loss 0.4955 (0.4595)	
training:	Epoch: [64][7/204]	Loss 0.5025 (0.4656)	
training:	Epoch: [64][8/204]	Loss 0.5479 (0.4759)	
training:	Epoch: [64][9/204]	Loss 0.5353 (0.4825)	
training:	Epoch: [64][10/204]	Loss 0.3666 (0.4709)	
training:	Epoch: [64][11/204]	Loss 0.5077 (0.4743)	
training:	Epoch: [64][12/204]	Loss 0.4005 (0.4681)	
training:	Epoch: [64][13/204]	Loss 0.5252 (0.4725)	
training:	Epoch: [64][14/204]	Loss 0.4769 (0.4728)	
training:	Epoch: [64][15/204]	Loss 0.4812 (0.4734)	
training:	Epoch: [64][16/204]	Loss 0.6605 (0.4851)	
training:	Epoch: [64][17/204]	Loss 0.3168 (0.4752)	
training:	Epoch: [64][18/204]	Loss 0.5053 (0.4769)	
training:	Epoch: [64][19/204]	Loss 0.4061 (0.4731)	
training:	Epoch: [64][20/204]	Loss 0.6231 (0.4806)	
training:	Epoch: [64][21/204]	Loss 0.4264 (0.4780)	
training:	Epoch: [64][22/204]	Loss 0.4931 (0.4787)	
training:	Epoch: [64][23/204]	Loss 0.3804 (0.4745)	
training:	Epoch: [64][24/204]	Loss 0.4945 (0.4753)	
training:	Epoch: [64][25/204]	Loss 0.4065 (0.4725)	
training:	Epoch: [64][26/204]	Loss 0.4314 (0.4710)	
training:	Epoch: [64][27/204]	Loss 0.4417 (0.4699)	
training:	Epoch: [64][28/204]	Loss 0.4367 (0.4687)	
training:	Epoch: [64][29/204]	Loss 0.5513 (0.4715)	
training:	Epoch: [64][30/204]	Loss 0.4141 (0.4696)	
training:	Epoch: [64][31/204]	Loss 0.3650 (0.4662)	
training:	Epoch: [64][32/204]	Loss 0.4878 (0.4669)	
training:	Epoch: [64][33/204]	Loss 0.3442 (0.4632)	
training:	Epoch: [64][34/204]	Loss 0.4654 (0.4633)	
training:	Epoch: [64][35/204]	Loss 0.4339 (0.4624)	
training:	Epoch: [64][36/204]	Loss 0.4728 (0.4627)	
training:	Epoch: [64][37/204]	Loss 0.3628 (0.4600)	
training:	Epoch: [64][38/204]	Loss 0.5393 (0.4621)	
training:	Epoch: [64][39/204]	Loss 0.4673 (0.4622)	
training:	Epoch: [64][40/204]	Loss 0.7525 (0.4695)	
training:	Epoch: [64][41/204]	Loss 0.3214 (0.4659)	
training:	Epoch: [64][42/204]	Loss 0.4260 (0.4649)	
training:	Epoch: [64][43/204]	Loss 0.5896 (0.4678)	
training:	Epoch: [64][44/204]	Loss 0.5003 (0.4686)	
training:	Epoch: [64][45/204]	Loss 0.3920 (0.4669)	
training:	Epoch: [64][46/204]	Loss 0.6282 (0.4704)	
training:	Epoch: [64][47/204]	Loss 0.3955 (0.4688)	
training:	Epoch: [64][48/204]	Loss 0.6825 (0.4732)	
training:	Epoch: [64][49/204]	Loss 0.4437 (0.4726)	
training:	Epoch: [64][50/204]	Loss 0.4477 (0.4721)	
training:	Epoch: [64][51/204]	Loss 0.5927 (0.4745)	
training:	Epoch: [64][52/204]	Loss 0.4244 (0.4735)	
training:	Epoch: [64][53/204]	Loss 0.3711 (0.4716)	
training:	Epoch: [64][54/204]	Loss 0.4211 (0.4707)	
training:	Epoch: [64][55/204]	Loss 0.5133 (0.4714)	
training:	Epoch: [64][56/204]	Loss 0.4333 (0.4708)	
training:	Epoch: [64][57/204]	Loss 0.5827 (0.4727)	
training:	Epoch: [64][58/204]	Loss 0.5081 (0.4733)	
training:	Epoch: [64][59/204]	Loss 0.5581 (0.4748)	
training:	Epoch: [64][60/204]	Loss 0.5228 (0.4756)	
training:	Epoch: [64][61/204]	Loss 0.3316 (0.4732)	
training:	Epoch: [64][62/204]	Loss 0.4318 (0.4725)	
training:	Epoch: [64][63/204]	Loss 0.3034 (0.4699)	
training:	Epoch: [64][64/204]	Loss 0.3895 (0.4686)	
training:	Epoch: [64][65/204]	Loss 0.3335 (0.4665)	
training:	Epoch: [64][66/204]	Loss 0.5865 (0.4683)	
training:	Epoch: [64][67/204]	Loss 0.4349 (0.4678)	
training:	Epoch: [64][68/204]	Loss 0.4845 (0.4681)	
training:	Epoch: [64][69/204]	Loss 0.4307 (0.4675)	
training:	Epoch: [64][70/204]	Loss 0.4418 (0.4672)	
training:	Epoch: [64][71/204]	Loss 0.5541 (0.4684)	
training:	Epoch: [64][72/204]	Loss 0.4076 (0.4676)	
training:	Epoch: [64][73/204]	Loss 0.3996 (0.4666)	
training:	Epoch: [64][74/204]	Loss 0.4507 (0.4664)	
training:	Epoch: [64][75/204]	Loss 0.4689 (0.4664)	
training:	Epoch: [64][76/204]	Loss 0.5829 (0.4680)	
training:	Epoch: [64][77/204]	Loss 0.6433 (0.4703)	
training:	Epoch: [64][78/204]	Loss 0.4270 (0.4697)	
training:	Epoch: [64][79/204]	Loss 0.3675 (0.4684)	
training:	Epoch: [64][80/204]	Loss 0.4976 (0.4688)	
training:	Epoch: [64][81/204]	Loss 0.4935 (0.4691)	
training:	Epoch: [64][82/204]	Loss 0.5812 (0.4704)	
training:	Epoch: [64][83/204]	Loss 0.4146 (0.4698)	
training:	Epoch: [64][84/204]	Loss 0.3831 (0.4687)	
training:	Epoch: [64][85/204]	Loss 0.4070 (0.4680)	
training:	Epoch: [64][86/204]	Loss 0.5407 (0.4689)	
training:	Epoch: [64][87/204]	Loss 0.4450 (0.4686)	
training:	Epoch: [64][88/204]	Loss 0.4612 (0.4685)	
training:	Epoch: [64][89/204]	Loss 0.4312 (0.4681)	
training:	Epoch: [64][90/204]	Loss 0.4901 (0.4683)	
training:	Epoch: [64][91/204]	Loss 0.5642 (0.4694)	
training:	Epoch: [64][92/204]	Loss 0.3788 (0.4684)	
training:	Epoch: [64][93/204]	Loss 0.5805 (0.4696)	
training:	Epoch: [64][94/204]	Loss 0.5291 (0.4702)	
training:	Epoch: [64][95/204]	Loss 0.4317 (0.4698)	
training:	Epoch: [64][96/204]	Loss 0.4258 (0.4694)	
training:	Epoch: [64][97/204]	Loss 0.4116 (0.4688)	
training:	Epoch: [64][98/204]	Loss 0.6863 (0.4710)	
training:	Epoch: [64][99/204]	Loss 0.4479 (0.4708)	
training:	Epoch: [64][100/204]	Loss 0.3232 (0.4693)	
training:	Epoch: [64][101/204]	Loss 0.4219 (0.4688)	
training:	Epoch: [64][102/204]	Loss 0.5238 (0.4693)	
training:	Epoch: [64][103/204]	Loss 0.5020 (0.4697)	
training:	Epoch: [64][104/204]	Loss 0.5710 (0.4706)	
training:	Epoch: [64][105/204]	Loss 0.5047 (0.4710)	
training:	Epoch: [64][106/204]	Loss 0.4235 (0.4705)	
training:	Epoch: [64][107/204]	Loss 0.3994 (0.4699)	
training:	Epoch: [64][108/204]	Loss 0.4187 (0.4694)	
training:	Epoch: [64][109/204]	Loss 0.4937 (0.4696)	
training:	Epoch: [64][110/204]	Loss 0.3540 (0.4686)	
training:	Epoch: [64][111/204]	Loss 0.4972 (0.4688)	
training:	Epoch: [64][112/204]	Loss 0.3995 (0.4682)	
training:	Epoch: [64][113/204]	Loss 0.4976 (0.4685)	
training:	Epoch: [64][114/204]	Loss 0.4979 (0.4687)	
training:	Epoch: [64][115/204]	Loss 0.4791 (0.4688)	
training:	Epoch: [64][116/204]	Loss 0.5438 (0.4694)	
training:	Epoch: [64][117/204]	Loss 0.5478 (0.4701)	
training:	Epoch: [64][118/204]	Loss 0.7435 (0.4724)	
training:	Epoch: [64][119/204]	Loss 0.3022 (0.4710)	
training:	Epoch: [64][120/204]	Loss 0.4192 (0.4706)	
training:	Epoch: [64][121/204]	Loss 0.3581 (0.4696)	
training:	Epoch: [64][122/204]	Loss 0.6214 (0.4709)	
training:	Epoch: [64][123/204]	Loss 0.5532 (0.4716)	
training:	Epoch: [64][124/204]	Loss 0.5578 (0.4722)	
training:	Epoch: [64][125/204]	Loss 0.4559 (0.4721)	
training:	Epoch: [64][126/204]	Loss 0.6047 (0.4732)	
training:	Epoch: [64][127/204]	Loss 0.3364 (0.4721)	
training:	Epoch: [64][128/204]	Loss 0.3736 (0.4713)	
training:	Epoch: [64][129/204]	Loss 0.4593 (0.4712)	
training:	Epoch: [64][130/204]	Loss 0.4458 (0.4710)	
training:	Epoch: [64][131/204]	Loss 0.6146 (0.4721)	
training:	Epoch: [64][132/204]	Loss 0.3963 (0.4716)	
training:	Epoch: [64][133/204]	Loss 0.5342 (0.4720)	
training:	Epoch: [64][134/204]	Loss 0.5793 (0.4728)	
training:	Epoch: [64][135/204]	Loss 0.4499 (0.4727)	
training:	Epoch: [64][136/204]	Loss 0.3903 (0.4721)	
training:	Epoch: [64][137/204]	Loss 0.3765 (0.4714)	
training:	Epoch: [64][138/204]	Loss 0.5141 (0.4717)	
training:	Epoch: [64][139/204]	Loss 0.5581 (0.4723)	
training:	Epoch: [64][140/204]	Loss 0.6361 (0.4735)	
training:	Epoch: [64][141/204]	Loss 0.3355 (0.4725)	
training:	Epoch: [64][142/204]	Loss 0.5503 (0.4730)	
training:	Epoch: [64][143/204]	Loss 0.6004 (0.4739)	
training:	Epoch: [64][144/204]	Loss 0.3913 (0.4733)	
training:	Epoch: [64][145/204]	Loss 0.5644 (0.4740)	
training:	Epoch: [64][146/204]	Loss 0.5840 (0.4747)	
training:	Epoch: [64][147/204]	Loss 0.3729 (0.4740)	
training:	Epoch: [64][148/204]	Loss 0.5610 (0.4746)	
training:	Epoch: [64][149/204]	Loss 0.4488 (0.4744)	
training:	Epoch: [64][150/204]	Loss 0.4938 (0.4746)	
training:	Epoch: [64][151/204]	Loss 0.4057 (0.4741)	
training:	Epoch: [64][152/204]	Loss 0.3521 (0.4733)	
training:	Epoch: [64][153/204]	Loss 0.4572 (0.4732)	
training:	Epoch: [64][154/204]	Loss 0.3778 (0.4726)	
training:	Epoch: [64][155/204]	Loss 0.5295 (0.4730)	
training:	Epoch: [64][156/204]	Loss 0.4801 (0.4730)	
training:	Epoch: [64][157/204]	Loss 0.3858 (0.4724)	
training:	Epoch: [64][158/204]	Loss 0.5269 (0.4728)	
training:	Epoch: [64][159/204]	Loss 0.4914 (0.4729)	
training:	Epoch: [64][160/204]	Loss 0.3574 (0.4722)	
training:	Epoch: [64][161/204]	Loss 0.3599 (0.4715)	
training:	Epoch: [64][162/204]	Loss 0.3861 (0.4710)	
training:	Epoch: [64][163/204]	Loss 0.5038 (0.4712)	
training:	Epoch: [64][164/204]	Loss 0.4795 (0.4712)	
training:	Epoch: [64][165/204]	Loss 0.5872 (0.4719)	
training:	Epoch: [64][166/204]	Loss 0.3728 (0.4713)	
training:	Epoch: [64][167/204]	Loss 0.5614 (0.4719)	
training:	Epoch: [64][168/204]	Loss 0.5456 (0.4723)	
training:	Epoch: [64][169/204]	Loss 0.3581 (0.4716)	
training:	Epoch: [64][170/204]	Loss 0.4479 (0.4715)	
training:	Epoch: [64][171/204]	Loss 0.4503 (0.4714)	
training:	Epoch: [64][172/204]	Loss 0.2972 (0.4703)	
training:	Epoch: [64][173/204]	Loss 0.4867 (0.4704)	
training:	Epoch: [64][174/204]	Loss 0.4329 (0.4702)	
training:	Epoch: [64][175/204]	Loss 0.5473 (0.4707)	
training:	Epoch: [64][176/204]	Loss 0.4688 (0.4707)	
training:	Epoch: [64][177/204]	Loss 0.7478 (0.4722)	
training:	Epoch: [64][178/204]	Loss 0.3997 (0.4718)	
training:	Epoch: [64][179/204]	Loss 0.4863 (0.4719)	
training:	Epoch: [64][180/204]	Loss 0.3772 (0.4714)	
training:	Epoch: [64][181/204]	Loss 0.7304 (0.4728)	
training:	Epoch: [64][182/204]	Loss 0.4413 (0.4726)	
training:	Epoch: [64][183/204]	Loss 0.4431 (0.4725)	
training:	Epoch: [64][184/204]	Loss 0.4302 (0.4722)	
training:	Epoch: [64][185/204]	Loss 0.5266 (0.4725)	
training:	Epoch: [64][186/204]	Loss 0.5152 (0.4728)	
training:	Epoch: [64][187/204]	Loss 0.4796 (0.4728)	
training:	Epoch: [64][188/204]	Loss 0.4853 (0.4729)	
training:	Epoch: [64][189/204]	Loss 0.5033 (0.4730)	
training:	Epoch: [64][190/204]	Loss 0.5017 (0.4732)	
training:	Epoch: [64][191/204]	Loss 0.3409 (0.4725)	
training:	Epoch: [64][192/204]	Loss 0.4850 (0.4725)	
training:	Epoch: [64][193/204]	Loss 0.3868 (0.4721)	
training:	Epoch: [64][194/204]	Loss 0.4136 (0.4718)	
training:	Epoch: [64][195/204]	Loss 0.3235 (0.4710)	
training:	Epoch: [64][196/204]	Loss 0.4835 (0.4711)	
training:	Epoch: [64][197/204]	Loss 0.4568 (0.4710)	
training:	Epoch: [64][198/204]	Loss 0.5540 (0.4715)	
training:	Epoch: [64][199/204]	Loss 0.5494 (0.4718)	
training:	Epoch: [64][200/204]	Loss 0.5456 (0.4722)	
training:	Epoch: [64][201/204]	Loss 0.6414 (0.4731)	
training:	Epoch: [64][202/204]	Loss 0.7192 (0.4743)	
training:	Epoch: [64][203/204]	Loss 0.4223 (0.4740)	
training:	Epoch: [64][204/204]	Loss 0.5673 (0.4745)	
Training:	 Loss: 0.4737

Training:	 ACC: 0.7905 0.7909 0.8010 0.7800
Validation:	 ACC: 0.7814 0.7822 0.8004 0.7623
Validation:	 Best_BACC: 0.7830 0.7838 0.8014 0.7646
Validation:	 Loss: 0.4771
Pretraining:	Epoch 65/120
----------
training:	Epoch: [65][1/204]	Loss 0.4728 (0.4728)	
training:	Epoch: [65][2/204]	Loss 0.6509 (0.5619)	
training:	Epoch: [65][3/204]	Loss 0.5615 (0.5617)	
training:	Epoch: [65][4/204]	Loss 0.6274 (0.5782)	
training:	Epoch: [65][5/204]	Loss 0.3873 (0.5400)	
training:	Epoch: [65][6/204]	Loss 0.6808 (0.5635)	
training:	Epoch: [65][7/204]	Loss 0.4405 (0.5459)	
training:	Epoch: [65][8/204]	Loss 0.6314 (0.5566)	
training:	Epoch: [65][9/204]	Loss 0.5978 (0.5612)	
training:	Epoch: [65][10/204]	Loss 0.4777 (0.5528)	
training:	Epoch: [65][11/204]	Loss 0.3957 (0.5385)	
training:	Epoch: [65][12/204]	Loss 0.4931 (0.5347)	
training:	Epoch: [65][13/204]	Loss 0.4510 (0.5283)	
training:	Epoch: [65][14/204]	Loss 0.4560 (0.5231)	
training:	Epoch: [65][15/204]	Loss 0.6253 (0.5300)	
training:	Epoch: [65][16/204]	Loss 0.5752 (0.5328)	
training:	Epoch: [65][17/204]	Loss 0.4055 (0.5253)	
training:	Epoch: [65][18/204]	Loss 0.6709 (0.5334)	
training:	Epoch: [65][19/204]	Loss 0.5256 (0.5330)	
training:	Epoch: [65][20/204]	Loss 0.3879 (0.5257)	
training:	Epoch: [65][21/204]	Loss 0.4893 (0.5240)	
training:	Epoch: [65][22/204]	Loss 0.4751 (0.5218)	
training:	Epoch: [65][23/204]	Loss 0.3123 (0.5127)	
training:	Epoch: [65][24/204]	Loss 0.5754 (0.5153)	
training:	Epoch: [65][25/204]	Loss 0.5915 (0.5183)	
training:	Epoch: [65][26/204]	Loss 0.4293 (0.5149)	
training:	Epoch: [65][27/204]	Loss 0.4329 (0.5119)	
training:	Epoch: [65][28/204]	Loss 0.5467 (0.5131)	
training:	Epoch: [65][29/204]	Loss 0.4892 (0.5123)	
training:	Epoch: [65][30/204]	Loss 0.4882 (0.5115)	
training:	Epoch: [65][31/204]	Loss 0.5491 (0.5127)	
training:	Epoch: [65][32/204]	Loss 0.3532 (0.5077)	
training:	Epoch: [65][33/204]	Loss 0.4186 (0.5050)	
training:	Epoch: [65][34/204]	Loss 0.3804 (0.5013)	
training:	Epoch: [65][35/204]	Loss 0.3519 (0.4971)	
training:	Epoch: [65][36/204]	Loss 0.5602 (0.4988)	
training:	Epoch: [65][37/204]	Loss 0.5564 (0.5004)	
training:	Epoch: [65][38/204]	Loss 0.4270 (0.4985)	
training:	Epoch: [65][39/204]	Loss 0.4146 (0.4963)	
training:	Epoch: [65][40/204]	Loss 0.4705 (0.4957)	
training:	Epoch: [65][41/204]	Loss 0.4173 (0.4937)	
training:	Epoch: [65][42/204]	Loss 0.5358 (0.4947)	
training:	Epoch: [65][43/204]	Loss 0.5218 (0.4954)	
training:	Epoch: [65][44/204]	Loss 0.3418 (0.4919)	
training:	Epoch: [65][45/204]	Loss 0.3942 (0.4897)	
training:	Epoch: [65][46/204]	Loss 0.4573 (0.4890)	
training:	Epoch: [65][47/204]	Loss 0.5660 (0.4906)	
training:	Epoch: [65][48/204]	Loss 0.5489 (0.4919)	
training:	Epoch: [65][49/204]	Loss 0.3716 (0.4894)	
training:	Epoch: [65][50/204]	Loss 0.4917 (0.4895)	
training:	Epoch: [65][51/204]	Loss 0.3852 (0.4874)	
training:	Epoch: [65][52/204]	Loss 0.3566 (0.4849)	
training:	Epoch: [65][53/204]	Loss 0.4090 (0.4835)	
training:	Epoch: [65][54/204]	Loss 0.3951 (0.4818)	
training:	Epoch: [65][55/204]	Loss 0.3484 (0.4794)	
training:	Epoch: [65][56/204]	Loss 0.6651 (0.4827)	
training:	Epoch: [65][57/204]	Loss 0.5351 (0.4836)	
training:	Epoch: [65][58/204]	Loss 0.4925 (0.4838)	
training:	Epoch: [65][59/204]	Loss 0.5196 (0.4844)	
training:	Epoch: [65][60/204]	Loss 0.5404 (0.4853)	
training:	Epoch: [65][61/204]	Loss 0.3171 (0.4826)	
training:	Epoch: [65][62/204]	Loss 0.4325 (0.4818)	
training:	Epoch: [65][63/204]	Loss 0.4159 (0.4807)	
training:	Epoch: [65][64/204]	Loss 0.4410 (0.4801)	
training:	Epoch: [65][65/204]	Loss 0.3350 (0.4779)	
training:	Epoch: [65][66/204]	Loss 0.4250 (0.4771)	
training:	Epoch: [65][67/204]	Loss 0.3098 (0.4746)	
training:	Epoch: [65][68/204]	Loss 0.4449 (0.4741)	
training:	Epoch: [65][69/204]	Loss 0.4514 (0.4738)	
training:	Epoch: [65][70/204]	Loss 0.4211 (0.4730)	
training:	Epoch: [65][71/204]	Loss 0.5578 (0.4742)	
training:	Epoch: [65][72/204]	Loss 0.4236 (0.4735)	
training:	Epoch: [65][73/204]	Loss 0.4349 (0.4730)	
training:	Epoch: [65][74/204]	Loss 0.6177 (0.4750)	
training:	Epoch: [65][75/204]	Loss 0.6225 (0.4769)	
training:	Epoch: [65][76/204]	Loss 0.3342 (0.4751)	
training:	Epoch: [65][77/204]	Loss 0.5095 (0.4755)	
training:	Epoch: [65][78/204]	Loss 0.3687 (0.4741)	
training:	Epoch: [65][79/204]	Loss 0.6335 (0.4761)	
training:	Epoch: [65][80/204]	Loss 0.5352 (0.4769)	
training:	Epoch: [65][81/204]	Loss 0.5324 (0.4776)	
training:	Epoch: [65][82/204]	Loss 0.5187 (0.4781)	
training:	Epoch: [65][83/204]	Loss 0.4631 (0.4779)	
training:	Epoch: [65][84/204]	Loss 0.4286 (0.4773)	
training:	Epoch: [65][85/204]	Loss 0.3916 (0.4763)	
training:	Epoch: [65][86/204]	Loss 0.5991 (0.4777)	
training:	Epoch: [65][87/204]	Loss 0.5082 (0.4781)	
training:	Epoch: [65][88/204]	Loss 0.4351 (0.4776)	
training:	Epoch: [65][89/204]	Loss 0.5318 (0.4782)	
training:	Epoch: [65][90/204]	Loss 0.4802 (0.4782)	
training:	Epoch: [65][91/204]	Loss 0.4019 (0.4774)	
training:	Epoch: [65][92/204]	Loss 0.3561 (0.4761)	
training:	Epoch: [65][93/204]	Loss 0.4760 (0.4761)	
training:	Epoch: [65][94/204]	Loss 0.3933 (0.4752)	
training:	Epoch: [65][95/204]	Loss 0.5005 (0.4754)	
training:	Epoch: [65][96/204]	Loss 0.4769 (0.4755)	
training:	Epoch: [65][97/204]	Loss 0.2821 (0.4735)	
training:	Epoch: [65][98/204]	Loss 0.4947 (0.4737)	
training:	Epoch: [65][99/204]	Loss 0.4771 (0.4737)	
training:	Epoch: [65][100/204]	Loss 0.4478 (0.4735)	
training:	Epoch: [65][101/204]	Loss 0.5667 (0.4744)	
training:	Epoch: [65][102/204]	Loss 0.4013 (0.4737)	
training:	Epoch: [65][103/204]	Loss 0.3598 (0.4726)	
training:	Epoch: [65][104/204]	Loss 0.3966 (0.4718)	
training:	Epoch: [65][105/204]	Loss 0.4240 (0.4714)	
training:	Epoch: [65][106/204]	Loss 0.3728 (0.4704)	
training:	Epoch: [65][107/204]	Loss 0.2570 (0.4685)	
training:	Epoch: [65][108/204]	Loss 0.4359 (0.4681)	
training:	Epoch: [65][109/204]	Loss 0.4666 (0.4681)	
training:	Epoch: [65][110/204]	Loss 0.5306 (0.4687)	
training:	Epoch: [65][111/204]	Loss 0.3393 (0.4675)	
training:	Epoch: [65][112/204]	Loss 0.2732 (0.4658)	
training:	Epoch: [65][113/204]	Loss 0.5477 (0.4665)	
training:	Epoch: [65][114/204]	Loss 0.5388 (0.4672)	
training:	Epoch: [65][115/204]	Loss 0.4954 (0.4674)	
training:	Epoch: [65][116/204]	Loss 0.6145 (0.4687)	
training:	Epoch: [65][117/204]	Loss 0.4800 (0.4688)	
training:	Epoch: [65][118/204]	Loss 0.5833 (0.4697)	
training:	Epoch: [65][119/204]	Loss 0.6247 (0.4710)	
training:	Epoch: [65][120/204]	Loss 0.5404 (0.4716)	
training:	Epoch: [65][121/204]	Loss 0.4781 (0.4717)	
training:	Epoch: [65][122/204]	Loss 0.4401 (0.4714)	
training:	Epoch: [65][123/204]	Loss 0.5314 (0.4719)	
training:	Epoch: [65][124/204]	Loss 0.5212 (0.4723)	
training:	Epoch: [65][125/204]	Loss 0.3574 (0.4714)	
training:	Epoch: [65][126/204]	Loss 0.4069 (0.4709)	
training:	Epoch: [65][127/204]	Loss 0.6386 (0.4722)	
training:	Epoch: [65][128/204]	Loss 0.4685 (0.4722)	
training:	Epoch: [65][129/204]	Loss 0.4100 (0.4717)	
training:	Epoch: [65][130/204]	Loss 0.4860 (0.4718)	
training:	Epoch: [65][131/204]	Loss 0.4720 (0.4718)	
training:	Epoch: [65][132/204]	Loss 0.4970 (0.4720)	
training:	Epoch: [65][133/204]	Loss 0.4626 (0.4719)	
training:	Epoch: [65][134/204]	Loss 0.5384 (0.4724)	
training:	Epoch: [65][135/204]	Loss 0.4740 (0.4724)	
training:	Epoch: [65][136/204]	Loss 0.4611 (0.4723)	
training:	Epoch: [65][137/204]	Loss 0.4034 (0.4718)	
training:	Epoch: [65][138/204]	Loss 0.3730 (0.4711)	
training:	Epoch: [65][139/204]	Loss 0.3808 (0.4705)	
training:	Epoch: [65][140/204]	Loss 0.4725 (0.4705)	
training:	Epoch: [65][141/204]	Loss 0.4778 (0.4705)	
training:	Epoch: [65][142/204]	Loss 0.3915 (0.4700)	
training:	Epoch: [65][143/204]	Loss 0.3730 (0.4693)	
training:	Epoch: [65][144/204]	Loss 0.4655 (0.4693)	
training:	Epoch: [65][145/204]	Loss 0.5708 (0.4700)	
training:	Epoch: [65][146/204]	Loss 0.5079 (0.4702)	
training:	Epoch: [65][147/204]	Loss 0.6086 (0.4712)	
training:	Epoch: [65][148/204]	Loss 0.4431 (0.4710)	
training:	Epoch: [65][149/204]	Loss 0.3837 (0.4704)	
training:	Epoch: [65][150/204]	Loss 0.5752 (0.4711)	
training:	Epoch: [65][151/204]	Loss 0.4953 (0.4713)	
training:	Epoch: [65][152/204]	Loss 0.3623 (0.4705)	
training:	Epoch: [65][153/204]	Loss 0.4191 (0.4702)	
training:	Epoch: [65][154/204]	Loss 0.2941 (0.4691)	
training:	Epoch: [65][155/204]	Loss 0.4422 (0.4689)	
training:	Epoch: [65][156/204]	Loss 0.5821 (0.4696)	
training:	Epoch: [65][157/204]	Loss 0.6196 (0.4706)	
training:	Epoch: [65][158/204]	Loss 0.5738 (0.4712)	
training:	Epoch: [65][159/204]	Loss 0.4352 (0.4710)	
training:	Epoch: [65][160/204]	Loss 0.5614 (0.4716)	
training:	Epoch: [65][161/204]	Loss 0.5164 (0.4718)	
training:	Epoch: [65][162/204]	Loss 0.3748 (0.4712)	
training:	Epoch: [65][163/204]	Loss 0.3961 (0.4708)	
training:	Epoch: [65][164/204]	Loss 0.4813 (0.4708)	
training:	Epoch: [65][165/204]	Loss 0.3712 (0.4702)	
training:	Epoch: [65][166/204]	Loss 0.4132 (0.4699)	
training:	Epoch: [65][167/204]	Loss 0.4420 (0.4697)	
training:	Epoch: [65][168/204]	Loss 0.5881 (0.4704)	
training:	Epoch: [65][169/204]	Loss 0.3818 (0.4699)	
training:	Epoch: [65][170/204]	Loss 0.4963 (0.4701)	
training:	Epoch: [65][171/204]	Loss 0.4984 (0.4702)	
training:	Epoch: [65][172/204]	Loss 0.4861 (0.4703)	
training:	Epoch: [65][173/204]	Loss 0.2901 (0.4693)	
training:	Epoch: [65][174/204]	Loss 0.4765 (0.4693)	
training:	Epoch: [65][175/204]	Loss 0.3695 (0.4688)	
training:	Epoch: [65][176/204]	Loss 0.5084 (0.4690)	
training:	Epoch: [65][177/204]	Loss 0.5690 (0.4695)	
training:	Epoch: [65][178/204]	Loss 0.4201 (0.4693)	
training:	Epoch: [65][179/204]	Loss 0.4459 (0.4691)	
training:	Epoch: [65][180/204]	Loss 0.5480 (0.4696)	
training:	Epoch: [65][181/204]	Loss 0.4833 (0.4696)	
training:	Epoch: [65][182/204]	Loss 0.4341 (0.4695)	
training:	Epoch: [65][183/204]	Loss 0.4208 (0.4692)	
training:	Epoch: [65][184/204]	Loss 0.4582 (0.4691)	
training:	Epoch: [65][185/204]	Loss 0.5598 (0.4696)	
training:	Epoch: [65][186/204]	Loss 0.3234 (0.4688)	
training:	Epoch: [65][187/204]	Loss 0.4213 (0.4686)	
training:	Epoch: [65][188/204]	Loss 0.4499 (0.4685)	
training:	Epoch: [65][189/204]	Loss 0.4336 (0.4683)	
training:	Epoch: [65][190/204]	Loss 0.3960 (0.4679)	
training:	Epoch: [65][191/204]	Loss 0.5223 (0.4682)	
training:	Epoch: [65][192/204]	Loss 0.5169 (0.4684)	
training:	Epoch: [65][193/204]	Loss 0.3905 (0.4680)	
training:	Epoch: [65][194/204]	Loss 0.4172 (0.4678)	
training:	Epoch: [65][195/204]	Loss 0.5225 (0.4681)	
training:	Epoch: [65][196/204]	Loss 0.4762 (0.4681)	
training:	Epoch: [65][197/204]	Loss 0.3949 (0.4677)	
training:	Epoch: [65][198/204]	Loss 0.5122 (0.4680)	
training:	Epoch: [65][199/204]	Loss 0.4617 (0.4679)	
training:	Epoch: [65][200/204]	Loss 0.5841 (0.4685)	
training:	Epoch: [65][201/204]	Loss 0.4211 (0.4683)	
training:	Epoch: [65][202/204]	Loss 0.4921 (0.4684)	
training:	Epoch: [65][203/204]	Loss 0.4800 (0.4684)	
training:	Epoch: [65][204/204]	Loss 0.5516 (0.4689)	
Training:	 Loss: 0.4681

Training:	 ACC: 0.7919 0.7931 0.8210 0.7628
Validation:	 ACC: 0.7787 0.7801 0.8096 0.7478
Validation:	 Best_BACC: 0.7830 0.7838 0.8014 0.7646
Validation:	 Loss: 0.4765
Pretraining:	Epoch 66/120
----------
training:	Epoch: [66][1/204]	Loss 0.4287 (0.4287)	
training:	Epoch: [66][2/204]	Loss 0.5132 (0.4709)	
training:	Epoch: [66][3/204]	Loss 0.3408 (0.4276)	
training:	Epoch: [66][4/204]	Loss 0.5155 (0.4496)	
training:	Epoch: [66][5/204]	Loss 0.4100 (0.4416)	
training:	Epoch: [66][6/204]	Loss 0.4688 (0.4462)	
training:	Epoch: [66][7/204]	Loss 0.4455 (0.4461)	
training:	Epoch: [66][8/204]	Loss 0.4731 (0.4495)	
training:	Epoch: [66][9/204]	Loss 0.4084 (0.4449)	
training:	Epoch: [66][10/204]	Loss 0.5174 (0.4521)	
training:	Epoch: [66][11/204]	Loss 0.3682 (0.4445)	
training:	Epoch: [66][12/204]	Loss 0.4492 (0.4449)	
training:	Epoch: [66][13/204]	Loss 0.4504 (0.4453)	
training:	Epoch: [66][14/204]	Loss 0.4291 (0.4442)	
training:	Epoch: [66][15/204]	Loss 0.5098 (0.4485)	
training:	Epoch: [66][16/204]	Loss 0.4167 (0.4465)	
training:	Epoch: [66][17/204]	Loss 0.4994 (0.4497)	
training:	Epoch: [66][18/204]	Loss 0.5312 (0.4542)	
training:	Epoch: [66][19/204]	Loss 0.3840 (0.4505)	
training:	Epoch: [66][20/204]	Loss 0.4663 (0.4513)	
training:	Epoch: [66][21/204]	Loss 0.4461 (0.4510)	
training:	Epoch: [66][22/204]	Loss 0.4989 (0.4532)	
training:	Epoch: [66][23/204]	Loss 0.4767 (0.4542)	
training:	Epoch: [66][24/204]	Loss 0.5360 (0.4576)	
training:	Epoch: [66][25/204]	Loss 0.5098 (0.4597)	
training:	Epoch: [66][26/204]	Loss 0.4452 (0.4592)	
training:	Epoch: [66][27/204]	Loss 0.5137 (0.4612)	
training:	Epoch: [66][28/204]	Loss 0.5345 (0.4638)	
training:	Epoch: [66][29/204]	Loss 0.5200 (0.4657)	
training:	Epoch: [66][30/204]	Loss 0.4666 (0.4658)	
training:	Epoch: [66][31/204]	Loss 0.3815 (0.4631)	
training:	Epoch: [66][32/204]	Loss 0.4405 (0.4623)	
training:	Epoch: [66][33/204]	Loss 0.6259 (0.4673)	
training:	Epoch: [66][34/204]	Loss 0.3553 (0.4640)	
training:	Epoch: [66][35/204]	Loss 0.3874 (0.4618)	
training:	Epoch: [66][36/204]	Loss 0.5527 (0.4643)	
training:	Epoch: [66][37/204]	Loss 0.5044 (0.4654)	
training:	Epoch: [66][38/204]	Loss 0.3403 (0.4621)	
training:	Epoch: [66][39/204]	Loss 0.3516 (0.4593)	
training:	Epoch: [66][40/204]	Loss 0.5373 (0.4613)	
training:	Epoch: [66][41/204]	Loss 0.4176 (0.4602)	
training:	Epoch: [66][42/204]	Loss 0.4637 (0.4603)	
training:	Epoch: [66][43/204]	Loss 0.3504 (0.4577)	
training:	Epoch: [66][44/204]	Loss 0.3931 (0.4563)	
training:	Epoch: [66][45/204]	Loss 0.4371 (0.4558)	
training:	Epoch: [66][46/204]	Loss 0.4436 (0.4556)	
training:	Epoch: [66][47/204]	Loss 0.5811 (0.4582)	
training:	Epoch: [66][48/204]	Loss 0.4702 (0.4585)	
training:	Epoch: [66][49/204]	Loss 0.5136 (0.4596)	
training:	Epoch: [66][50/204]	Loss 0.5594 (0.4616)	
training:	Epoch: [66][51/204]	Loss 0.2469 (0.4574)	
training:	Epoch: [66][52/204]	Loss 0.4620 (0.4575)	
training:	Epoch: [66][53/204]	Loss 0.5191 (0.4586)	
training:	Epoch: [66][54/204]	Loss 0.6593 (0.4624)	
training:	Epoch: [66][55/204]	Loss 0.4683 (0.4625)	
training:	Epoch: [66][56/204]	Loss 0.4202 (0.4617)	
training:	Epoch: [66][57/204]	Loss 0.4988 (0.4624)	
training:	Epoch: [66][58/204]	Loss 0.4324 (0.4618)	
training:	Epoch: [66][59/204]	Loss 0.5213 (0.4629)	
training:	Epoch: [66][60/204]	Loss 0.4179 (0.4621)	
training:	Epoch: [66][61/204]	Loss 0.4531 (0.4620)	
training:	Epoch: [66][62/204]	Loss 0.5394 (0.4632)	
training:	Epoch: [66][63/204]	Loss 0.5018 (0.4638)	
training:	Epoch: [66][64/204]	Loss 0.4508 (0.4636)	
training:	Epoch: [66][65/204]	Loss 0.4330 (0.4631)	
training:	Epoch: [66][66/204]	Loss 0.5757 (0.4648)	
training:	Epoch: [66][67/204]	Loss 0.4384 (0.4645)	
training:	Epoch: [66][68/204]	Loss 0.4694 (0.4645)	
training:	Epoch: [66][69/204]	Loss 0.4651 (0.4645)	
training:	Epoch: [66][70/204]	Loss 0.6558 (0.4673)	
training:	Epoch: [66][71/204]	Loss 0.4204 (0.4666)	
training:	Epoch: [66][72/204]	Loss 0.2830 (0.4641)	
training:	Epoch: [66][73/204]	Loss 0.5456 (0.4652)	
training:	Epoch: [66][74/204]	Loss 0.3784 (0.4640)	
training:	Epoch: [66][75/204]	Loss 0.4204 (0.4634)	
training:	Epoch: [66][76/204]	Loss 0.5783 (0.4649)	
training:	Epoch: [66][77/204]	Loss 0.6413 (0.4672)	
training:	Epoch: [66][78/204]	Loss 0.4549 (0.4671)	
training:	Epoch: [66][79/204]	Loss 0.4452 (0.4668)	
training:	Epoch: [66][80/204]	Loss 0.3197 (0.4650)	
training:	Epoch: [66][81/204]	Loss 0.5460 (0.4660)	
training:	Epoch: [66][82/204]	Loss 0.6186 (0.4678)	
training:	Epoch: [66][83/204]	Loss 0.3606 (0.4665)	
training:	Epoch: [66][84/204]	Loss 0.4197 (0.4660)	
training:	Epoch: [66][85/204]	Loss 0.5614 (0.4671)	
training:	Epoch: [66][86/204]	Loss 0.4748 (0.4672)	
training:	Epoch: [66][87/204]	Loss 0.3370 (0.4657)	
training:	Epoch: [66][88/204]	Loss 0.3961 (0.4649)	
training:	Epoch: [66][89/204]	Loss 0.3245 (0.4633)	
training:	Epoch: [66][90/204]	Loss 0.4842 (0.4635)	
training:	Epoch: [66][91/204]	Loss 0.4576 (0.4635)	
training:	Epoch: [66][92/204]	Loss 0.5363 (0.4643)	
training:	Epoch: [66][93/204]	Loss 0.5242 (0.4649)	
training:	Epoch: [66][94/204]	Loss 0.5847 (0.4662)	
training:	Epoch: [66][95/204]	Loss 0.5333 (0.4669)	
training:	Epoch: [66][96/204]	Loss 0.4840 (0.4671)	
training:	Epoch: [66][97/204]	Loss 0.4398 (0.4668)	
training:	Epoch: [66][98/204]	Loss 0.4181 (0.4663)	
training:	Epoch: [66][99/204]	Loss 0.4211 (0.4658)	
training:	Epoch: [66][100/204]	Loss 0.4285 (0.4655)	
training:	Epoch: [66][101/204]	Loss 0.3997 (0.4648)	
training:	Epoch: [66][102/204]	Loss 0.4559 (0.4647)	
training:	Epoch: [66][103/204]	Loss 0.5713 (0.4658)	
training:	Epoch: [66][104/204]	Loss 0.5213 (0.4663)	
training:	Epoch: [66][105/204]	Loss 0.5130 (0.4667)	
training:	Epoch: [66][106/204]	Loss 0.4665 (0.4667)	
training:	Epoch: [66][107/204]	Loss 0.5437 (0.4675)	
training:	Epoch: [66][108/204]	Loss 0.6534 (0.4692)	
training:	Epoch: [66][109/204]	Loss 0.4886 (0.4694)	
training:	Epoch: [66][110/204]	Loss 0.3266 (0.4681)	
training:	Epoch: [66][111/204]	Loss 0.5588 (0.4689)	
training:	Epoch: [66][112/204]	Loss 0.5312 (0.4694)	
training:	Epoch: [66][113/204]	Loss 0.4274 (0.4691)	
training:	Epoch: [66][114/204]	Loss 0.4731 (0.4691)	
training:	Epoch: [66][115/204]	Loss 0.3313 (0.4679)	
training:	Epoch: [66][116/204]	Loss 0.5031 (0.4682)	
training:	Epoch: [66][117/204]	Loss 0.4573 (0.4681)	
training:	Epoch: [66][118/204]	Loss 0.3423 (0.4670)	
training:	Epoch: [66][119/204]	Loss 0.4066 (0.4665)	
training:	Epoch: [66][120/204]	Loss 0.5185 (0.4670)	
training:	Epoch: [66][121/204]	Loss 0.5743 (0.4679)	
training:	Epoch: [66][122/204]	Loss 0.5077 (0.4682)	
training:	Epoch: [66][123/204]	Loss 0.4446 (0.4680)	
training:	Epoch: [66][124/204]	Loss 0.3774 (0.4673)	
training:	Epoch: [66][125/204]	Loss 0.4346 (0.4670)	
training:	Epoch: [66][126/204]	Loss 0.4013 (0.4665)	
training:	Epoch: [66][127/204]	Loss 0.4145 (0.4661)	
training:	Epoch: [66][128/204]	Loss 0.3352 (0.4650)	
training:	Epoch: [66][129/204]	Loss 0.6039 (0.4661)	
training:	Epoch: [66][130/204]	Loss 0.4825 (0.4662)	
training:	Epoch: [66][131/204]	Loss 0.5527 (0.4669)	
training:	Epoch: [66][132/204]	Loss 0.3913 (0.4663)	
training:	Epoch: [66][133/204]	Loss 0.3033 (0.4651)	
training:	Epoch: [66][134/204]	Loss 0.5198 (0.4655)	
training:	Epoch: [66][135/204]	Loss 0.4791 (0.4656)	
training:	Epoch: [66][136/204]	Loss 0.4966 (0.4658)	
training:	Epoch: [66][137/204]	Loss 0.3230 (0.4648)	
training:	Epoch: [66][138/204]	Loss 0.3425 (0.4639)	
training:	Epoch: [66][139/204]	Loss 0.5058 (0.4642)	
training:	Epoch: [66][140/204]	Loss 0.3685 (0.4635)	
training:	Epoch: [66][141/204]	Loss 0.6288 (0.4647)	
training:	Epoch: [66][142/204]	Loss 0.6601 (0.4661)	
training:	Epoch: [66][143/204]	Loss 0.5228 (0.4665)	
training:	Epoch: [66][144/204]	Loss 0.3834 (0.4659)	
training:	Epoch: [66][145/204]	Loss 0.5701 (0.4666)	
training:	Epoch: [66][146/204]	Loss 0.5071 (0.4669)	
training:	Epoch: [66][147/204]	Loss 0.4922 (0.4671)	
training:	Epoch: [66][148/204]	Loss 0.4924 (0.4672)	
training:	Epoch: [66][149/204]	Loss 0.4294 (0.4670)	
training:	Epoch: [66][150/204]	Loss 0.3962 (0.4665)	
training:	Epoch: [66][151/204]	Loss 0.4389 (0.4663)	
training:	Epoch: [66][152/204]	Loss 0.5449 (0.4668)	
training:	Epoch: [66][153/204]	Loss 0.4193 (0.4665)	
training:	Epoch: [66][154/204]	Loss 0.3310 (0.4657)	
training:	Epoch: [66][155/204]	Loss 0.6338 (0.4667)	
training:	Epoch: [66][156/204]	Loss 0.5853 (0.4675)	
training:	Epoch: [66][157/204]	Loss 0.5096 (0.4678)	
training:	Epoch: [66][158/204]	Loss 0.4967 (0.4680)	
training:	Epoch: [66][159/204]	Loss 0.7286 (0.4696)	
training:	Epoch: [66][160/204]	Loss 0.5030 (0.4698)	
training:	Epoch: [66][161/204]	Loss 0.4675 (0.4698)	
training:	Epoch: [66][162/204]	Loss 0.4841 (0.4699)	
training:	Epoch: [66][163/204]	Loss 0.4700 (0.4699)	
training:	Epoch: [66][164/204]	Loss 0.5882 (0.4706)	
training:	Epoch: [66][165/204]	Loss 0.4572 (0.4705)	
training:	Epoch: [66][166/204]	Loss 0.5835 (0.4712)	
training:	Epoch: [66][167/204]	Loss 0.4811 (0.4713)	
training:	Epoch: [66][168/204]	Loss 0.3882 (0.4708)	
training:	Epoch: [66][169/204]	Loss 0.4262 (0.4705)	
training:	Epoch: [66][170/204]	Loss 0.3562 (0.4698)	
training:	Epoch: [66][171/204]	Loss 0.6579 (0.4709)	
training:	Epoch: [66][172/204]	Loss 0.3988 (0.4705)	
training:	Epoch: [66][173/204]	Loss 0.3040 (0.4695)	
training:	Epoch: [66][174/204]	Loss 0.3528 (0.4689)	
training:	Epoch: [66][175/204]	Loss 0.4476 (0.4688)	
training:	Epoch: [66][176/204]	Loss 0.3843 (0.4683)	
training:	Epoch: [66][177/204]	Loss 0.4193 (0.4680)	
training:	Epoch: [66][178/204]	Loss 0.3890 (0.4676)	
training:	Epoch: [66][179/204]	Loss 0.5212 (0.4679)	
training:	Epoch: [66][180/204]	Loss 0.4938 (0.4680)	
training:	Epoch: [66][181/204]	Loss 0.5038 (0.4682)	
training:	Epoch: [66][182/204]	Loss 0.5531 (0.4687)	
training:	Epoch: [66][183/204]	Loss 0.4193 (0.4684)	
training:	Epoch: [66][184/204]	Loss 0.5857 (0.4690)	
training:	Epoch: [66][185/204]	Loss 0.4872 (0.4691)	
training:	Epoch: [66][186/204]	Loss 0.6074 (0.4699)	
training:	Epoch: [66][187/204]	Loss 0.4599 (0.4698)	
training:	Epoch: [66][188/204]	Loss 0.6287 (0.4707)	
training:	Epoch: [66][189/204]	Loss 0.5371 (0.4710)	
training:	Epoch: [66][190/204]	Loss 0.3444 (0.4703)	
training:	Epoch: [66][191/204]	Loss 0.5192 (0.4706)	
training:	Epoch: [66][192/204]	Loss 0.3277 (0.4699)	
training:	Epoch: [66][193/204]	Loss 0.2910 (0.4689)	
training:	Epoch: [66][194/204]	Loss 0.4204 (0.4687)	
training:	Epoch: [66][195/204]	Loss 0.5862 (0.4693)	
training:	Epoch: [66][196/204]	Loss 0.4134 (0.4690)	
training:	Epoch: [66][197/204]	Loss 0.3815 (0.4686)	
training:	Epoch: [66][198/204]	Loss 0.5626 (0.4690)	
training:	Epoch: [66][199/204]	Loss 0.6472 (0.4699)	
training:	Epoch: [66][200/204]	Loss 0.3584 (0.4694)	
training:	Epoch: [66][201/204]	Loss 0.5330 (0.4697)	
training:	Epoch: [66][202/204]	Loss 0.4528 (0.4696)	
training:	Epoch: [66][203/204]	Loss 0.5043 (0.4698)	
training:	Epoch: [66][204/204]	Loss 0.4193 (0.4695)	
Training:	 Loss: 0.4688

Training:	 ACC: 0.7937 0.7941 0.8039 0.7835
Validation:	 ACC: 0.7814 0.7822 0.7994 0.7635
Validation:	 Best_BACC: 0.7830 0.7838 0.8014 0.7646
Validation:	 Loss: 0.4738
Pretraining:	Epoch 67/120
----------
training:	Epoch: [67][1/204]	Loss 0.4811 (0.4811)	
training:	Epoch: [67][2/204]	Loss 0.5190 (0.5001)	
training:	Epoch: [67][3/204]	Loss 0.3685 (0.4562)	
training:	Epoch: [67][4/204]	Loss 0.6171 (0.4964)	
training:	Epoch: [67][5/204]	Loss 0.4500 (0.4872)	
training:	Epoch: [67][6/204]	Loss 0.4660 (0.4836)	
training:	Epoch: [67][7/204]	Loss 0.3484 (0.4643)	
training:	Epoch: [67][8/204]	Loss 0.5371 (0.4734)	
training:	Epoch: [67][9/204]	Loss 0.4771 (0.4738)	
training:	Epoch: [67][10/204]	Loss 0.4765 (0.4741)	
training:	Epoch: [67][11/204]	Loss 0.4729 (0.4740)	
training:	Epoch: [67][12/204]	Loss 0.5391 (0.4794)	
training:	Epoch: [67][13/204]	Loss 0.6048 (0.4891)	
training:	Epoch: [67][14/204]	Loss 0.5125 (0.4907)	
training:	Epoch: [67][15/204]	Loss 0.5507 (0.4947)	
training:	Epoch: [67][16/204]	Loss 0.5275 (0.4968)	
training:	Epoch: [67][17/204]	Loss 0.5013 (0.4970)	
training:	Epoch: [67][18/204]	Loss 0.5482 (0.4999)	
training:	Epoch: [67][19/204]	Loss 0.3823 (0.4937)	
training:	Epoch: [67][20/204]	Loss 0.4062 (0.4893)	
training:	Epoch: [67][21/204]	Loss 0.3590 (0.4831)	
training:	Epoch: [67][22/204]	Loss 0.4003 (0.4793)	
training:	Epoch: [67][23/204]	Loss 0.4723 (0.4790)	
training:	Epoch: [67][24/204]	Loss 0.4167 (0.4764)	
training:	Epoch: [67][25/204]	Loss 0.5546 (0.4796)	
training:	Epoch: [67][26/204]	Loss 0.4735 (0.4793)	
training:	Epoch: [67][27/204]	Loss 0.4467 (0.4781)	
training:	Epoch: [67][28/204]	Loss 0.5168 (0.4795)	
training:	Epoch: [67][29/204]	Loss 0.4281 (0.4777)	
training:	Epoch: [67][30/204]	Loss 0.4812 (0.4779)	
training:	Epoch: [67][31/204]	Loss 0.4152 (0.4758)	
training:	Epoch: [67][32/204]	Loss 0.4248 (0.4742)	
training:	Epoch: [67][33/204]	Loss 0.4732 (0.4742)	
training:	Epoch: [67][34/204]	Loss 0.5071 (0.4752)	
training:	Epoch: [67][35/204]	Loss 0.4591 (0.4747)	
training:	Epoch: [67][36/204]	Loss 0.4796 (0.4749)	
training:	Epoch: [67][37/204]	Loss 0.5062 (0.4757)	
training:	Epoch: [67][38/204]	Loss 0.4633 (0.4754)	
training:	Epoch: [67][39/204]	Loss 0.4679 (0.4752)	
training:	Epoch: [67][40/204]	Loss 0.5219 (0.4763)	
training:	Epoch: [67][41/204]	Loss 0.6460 (0.4805)	
training:	Epoch: [67][42/204]	Loss 0.4405 (0.4795)	
training:	Epoch: [67][43/204]	Loss 0.4997 (0.4800)	
training:	Epoch: [67][44/204]	Loss 0.5786 (0.4822)	
training:	Epoch: [67][45/204]	Loss 0.3728 (0.4798)	
training:	Epoch: [67][46/204]	Loss 0.5721 (0.4818)	
training:	Epoch: [67][47/204]	Loss 0.4397 (0.4809)	
training:	Epoch: [67][48/204]	Loss 0.2973 (0.4771)	
training:	Epoch: [67][49/204]	Loss 0.4203 (0.4759)	
training:	Epoch: [67][50/204]	Loss 0.4168 (0.4748)	
training:	Epoch: [67][51/204]	Loss 0.5680 (0.4766)	
training:	Epoch: [67][52/204]	Loss 0.4592 (0.4762)	
training:	Epoch: [67][53/204]	Loss 0.3189 (0.4733)	
training:	Epoch: [67][54/204]	Loss 0.4391 (0.4726)	
training:	Epoch: [67][55/204]	Loss 0.4691 (0.4726)	
training:	Epoch: [67][56/204]	Loss 0.4580 (0.4723)	
training:	Epoch: [67][57/204]	Loss 0.6206 (0.4749)	
training:	Epoch: [67][58/204]	Loss 0.4162 (0.4739)	
training:	Epoch: [67][59/204]	Loss 0.5602 (0.4754)	
training:	Epoch: [67][60/204]	Loss 0.4538 (0.4750)	
training:	Epoch: [67][61/204]	Loss 0.6268 (0.4775)	
training:	Epoch: [67][62/204]	Loss 0.4451 (0.4770)	
training:	Epoch: [67][63/204]	Loss 0.4318 (0.4763)	
training:	Epoch: [67][64/204]	Loss 0.4286 (0.4755)	
training:	Epoch: [67][65/204]	Loss 0.4305 (0.4748)	
training:	Epoch: [67][66/204]	Loss 0.4777 (0.4749)	
training:	Epoch: [67][67/204]	Loss 0.2570 (0.4716)	
training:	Epoch: [67][68/204]	Loss 0.5472 (0.4727)	
training:	Epoch: [67][69/204]	Loss 0.3453 (0.4709)	
training:	Epoch: [67][70/204]	Loss 0.3943 (0.4698)	
training:	Epoch: [67][71/204]	Loss 0.4770 (0.4699)	
training:	Epoch: [67][72/204]	Loss 0.4333 (0.4694)	
training:	Epoch: [67][73/204]	Loss 0.3696 (0.4680)	
training:	Epoch: [67][74/204]	Loss 0.3657 (0.4666)	
training:	Epoch: [67][75/204]	Loss 0.6306 (0.4688)	
training:	Epoch: [67][76/204]	Loss 0.5513 (0.4699)	
training:	Epoch: [67][77/204]	Loss 0.5756 (0.4713)	
training:	Epoch: [67][78/204]	Loss 0.3342 (0.4695)	
training:	Epoch: [67][79/204]	Loss 0.5089 (0.4700)	
training:	Epoch: [67][80/204]	Loss 0.4384 (0.4696)	
training:	Epoch: [67][81/204]	Loss 0.4160 (0.4690)	
training:	Epoch: [67][82/204]	Loss 0.5364 (0.4698)	
training:	Epoch: [67][83/204]	Loss 0.5531 (0.4708)	
training:	Epoch: [67][84/204]	Loss 0.4990 (0.4711)	
training:	Epoch: [67][85/204]	Loss 0.4588 (0.4710)	
training:	Epoch: [67][86/204]	Loss 0.5577 (0.4720)	
training:	Epoch: [67][87/204]	Loss 0.3549 (0.4706)	
training:	Epoch: [67][88/204]	Loss 0.5222 (0.4712)	
training:	Epoch: [67][89/204]	Loss 0.4705 (0.4712)	
training:	Epoch: [67][90/204]	Loss 0.5894 (0.4725)	
training:	Epoch: [67][91/204]	Loss 0.3721 (0.4714)	
training:	Epoch: [67][92/204]	Loss 0.3830 (0.4705)	
training:	Epoch: [67][93/204]	Loss 0.4399 (0.4701)	
training:	Epoch: [67][94/204]	Loss 0.4740 (0.4702)	
training:	Epoch: [67][95/204]	Loss 0.5745 (0.4713)	
training:	Epoch: [67][96/204]	Loss 0.4408 (0.4710)	
training:	Epoch: [67][97/204]	Loss 0.5644 (0.4719)	
training:	Epoch: [67][98/204]	Loss 0.4273 (0.4715)	
training:	Epoch: [67][99/204]	Loss 0.6155 (0.4729)	
training:	Epoch: [67][100/204]	Loss 0.4099 (0.4723)	
training:	Epoch: [67][101/204]	Loss 0.5022 (0.4726)	
training:	Epoch: [67][102/204]	Loss 0.3452 (0.4713)	
training:	Epoch: [67][103/204]	Loss 0.5922 (0.4725)	
training:	Epoch: [67][104/204]	Loss 0.4612 (0.4724)	
training:	Epoch: [67][105/204]	Loss 0.4937 (0.4726)	
training:	Epoch: [67][106/204]	Loss 0.4355 (0.4723)	
training:	Epoch: [67][107/204]	Loss 0.4847 (0.4724)	
training:	Epoch: [67][108/204]	Loss 0.4664 (0.4723)	
training:	Epoch: [67][109/204]	Loss 0.4426 (0.4720)	
training:	Epoch: [67][110/204]	Loss 0.5402 (0.4727)	
training:	Epoch: [67][111/204]	Loss 0.4503 (0.4725)	
training:	Epoch: [67][112/204]	Loss 0.3990 (0.4718)	
training:	Epoch: [67][113/204]	Loss 0.5623 (0.4726)	
training:	Epoch: [67][114/204]	Loss 0.4887 (0.4728)	
training:	Epoch: [67][115/204]	Loss 0.4915 (0.4729)	
training:	Epoch: [67][116/204]	Loss 0.4579 (0.4728)	
training:	Epoch: [67][117/204]	Loss 0.3625 (0.4718)	
training:	Epoch: [67][118/204]	Loss 0.4183 (0.4714)	
training:	Epoch: [67][119/204]	Loss 0.5760 (0.4723)	
training:	Epoch: [67][120/204]	Loss 0.3381 (0.4712)	
training:	Epoch: [67][121/204]	Loss 0.4699 (0.4711)	
training:	Epoch: [67][122/204]	Loss 0.5063 (0.4714)	
training:	Epoch: [67][123/204]	Loss 0.3756 (0.4706)	
training:	Epoch: [67][124/204]	Loss 0.3903 (0.4700)	
training:	Epoch: [67][125/204]	Loss 0.5127 (0.4703)	
training:	Epoch: [67][126/204]	Loss 0.3425 (0.4693)	
training:	Epoch: [67][127/204]	Loss 0.4037 (0.4688)	
training:	Epoch: [67][128/204]	Loss 0.5583 (0.4695)	
training:	Epoch: [67][129/204]	Loss 0.4251 (0.4692)	
training:	Epoch: [67][130/204]	Loss 0.5472 (0.4698)	
training:	Epoch: [67][131/204]	Loss 0.4929 (0.4699)	
training:	Epoch: [67][132/204]	Loss 0.5265 (0.4704)	
training:	Epoch: [67][133/204]	Loss 0.5084 (0.4707)	
training:	Epoch: [67][134/204]	Loss 0.5263 (0.4711)	
training:	Epoch: [67][135/204]	Loss 0.5432 (0.4716)	
training:	Epoch: [67][136/204]	Loss 0.5193 (0.4720)	
training:	Epoch: [67][137/204]	Loss 0.4179 (0.4716)	
training:	Epoch: [67][138/204]	Loss 0.3249 (0.4705)	
training:	Epoch: [67][139/204]	Loss 0.4884 (0.4706)	
training:	Epoch: [67][140/204]	Loss 0.3224 (0.4696)	
training:	Epoch: [67][141/204]	Loss 0.5027 (0.4698)	
training:	Epoch: [67][142/204]	Loss 0.4507 (0.4697)	
training:	Epoch: [67][143/204]	Loss 0.5825 (0.4705)	
training:	Epoch: [67][144/204]	Loss 0.3734 (0.4698)	
training:	Epoch: [67][145/204]	Loss 0.4510 (0.4697)	
training:	Epoch: [67][146/204]	Loss 0.3920 (0.4691)	
training:	Epoch: [67][147/204]	Loss 0.4599 (0.4691)	
training:	Epoch: [67][148/204]	Loss 0.5169 (0.4694)	
training:	Epoch: [67][149/204]	Loss 0.3792 (0.4688)	
training:	Epoch: [67][150/204]	Loss 0.3805 (0.4682)	
training:	Epoch: [67][151/204]	Loss 0.3320 (0.4673)	
training:	Epoch: [67][152/204]	Loss 0.4132 (0.4669)	
training:	Epoch: [67][153/204]	Loss 0.3823 (0.4664)	
training:	Epoch: [67][154/204]	Loss 0.4930 (0.4666)	
training:	Epoch: [67][155/204]	Loss 0.4013 (0.4661)	
training:	Epoch: [67][156/204]	Loss 0.5085 (0.4664)	
training:	Epoch: [67][157/204]	Loss 0.3825 (0.4659)	
training:	Epoch: [67][158/204]	Loss 0.4412 (0.4657)	
training:	Epoch: [67][159/204]	Loss 0.4216 (0.4654)	
training:	Epoch: [67][160/204]	Loss 0.3894 (0.4650)	
training:	Epoch: [67][161/204]	Loss 0.4964 (0.4652)	
training:	Epoch: [67][162/204]	Loss 0.4002 (0.4648)	
training:	Epoch: [67][163/204]	Loss 0.4890 (0.4649)	
training:	Epoch: [67][164/204]	Loss 0.4616 (0.4649)	
training:	Epoch: [67][165/204]	Loss 0.5902 (0.4656)	
training:	Epoch: [67][166/204]	Loss 0.5020 (0.4659)	
training:	Epoch: [67][167/204]	Loss 0.6053 (0.4667)	
training:	Epoch: [67][168/204]	Loss 0.6363 (0.4677)	
training:	Epoch: [67][169/204]	Loss 0.3461 (0.4670)	
training:	Epoch: [67][170/204]	Loss 0.3692 (0.4664)	
training:	Epoch: [67][171/204]	Loss 0.3120 (0.4655)	
training:	Epoch: [67][172/204]	Loss 0.4140 (0.4652)	
training:	Epoch: [67][173/204]	Loss 0.2994 (0.4642)	
training:	Epoch: [67][174/204]	Loss 0.4950 (0.4644)	
training:	Epoch: [67][175/204]	Loss 0.5243 (0.4648)	
training:	Epoch: [67][176/204]	Loss 0.5422 (0.4652)	
training:	Epoch: [67][177/204]	Loss 0.4517 (0.4651)	
training:	Epoch: [67][178/204]	Loss 0.3891 (0.4647)	
training:	Epoch: [67][179/204]	Loss 0.4197 (0.4645)	
training:	Epoch: [67][180/204]	Loss 0.6131 (0.4653)	
training:	Epoch: [67][181/204]	Loss 0.5101 (0.4655)	
training:	Epoch: [67][182/204]	Loss 0.4989 (0.4657)	
training:	Epoch: [67][183/204]	Loss 0.4126 (0.4654)	
training:	Epoch: [67][184/204]	Loss 0.5191 (0.4657)	
training:	Epoch: [67][185/204]	Loss 0.3613 (0.4651)	
training:	Epoch: [67][186/204]	Loss 0.3734 (0.4647)	
training:	Epoch: [67][187/204]	Loss 0.4705 (0.4647)	
training:	Epoch: [67][188/204]	Loss 0.4032 (0.4644)	
training:	Epoch: [67][189/204]	Loss 0.6116 (0.4651)	
training:	Epoch: [67][190/204]	Loss 0.4295 (0.4650)	
training:	Epoch: [67][191/204]	Loss 0.4828 (0.4650)	
training:	Epoch: [67][192/204]	Loss 0.6097 (0.4658)	
training:	Epoch: [67][193/204]	Loss 0.3921 (0.4654)	
training:	Epoch: [67][194/204]	Loss 0.4611 (0.4654)	
training:	Epoch: [67][195/204]	Loss 0.5967 (0.4661)	
training:	Epoch: [67][196/204]	Loss 0.5364 (0.4664)	
training:	Epoch: [67][197/204]	Loss 0.5241 (0.4667)	
training:	Epoch: [67][198/204]	Loss 0.2976 (0.4659)	
training:	Epoch: [67][199/204]	Loss 0.6014 (0.4665)	
training:	Epoch: [67][200/204]	Loss 0.4063 (0.4662)	
training:	Epoch: [67][201/204]	Loss 0.3596 (0.4657)	
training:	Epoch: [67][202/204]	Loss 0.3775 (0.4653)	
training:	Epoch: [67][203/204]	Loss 0.4911 (0.4654)	
training:	Epoch: [67][204/204]	Loss 0.3118 (0.4647)	
Training:	 Loss: 0.4639

Training:	 ACC: 0.7938 0.7940 0.7992 0.7883
Validation:	 ACC: 0.7833 0.7838 0.7953 0.7713
Validation:	 Best_BACC: 0.7833 0.7838 0.7953 0.7713
Validation:	 Loss: 0.4728
Pretraining:	Epoch 68/120
----------
training:	Epoch: [68][1/204]	Loss 0.3349 (0.3349)	
training:	Epoch: [68][2/204]	Loss 0.4032 (0.3690)	
training:	Epoch: [68][3/204]	Loss 0.5590 (0.4323)	
training:	Epoch: [68][4/204]	Loss 0.4426 (0.4349)	
training:	Epoch: [68][5/204]	Loss 0.3748 (0.4229)	
training:	Epoch: [68][6/204]	Loss 0.4436 (0.4264)	
training:	Epoch: [68][7/204]	Loss 0.3771 (0.4193)	
training:	Epoch: [68][8/204]	Loss 0.5425 (0.4347)	
training:	Epoch: [68][9/204]	Loss 0.4701 (0.4386)	
training:	Epoch: [68][10/204]	Loss 0.4797 (0.4427)	
training:	Epoch: [68][11/204]	Loss 0.4461 (0.4431)	
training:	Epoch: [68][12/204]	Loss 0.4435 (0.4431)	
training:	Epoch: [68][13/204]	Loss 0.4118 (0.4407)	
training:	Epoch: [68][14/204]	Loss 0.5248 (0.4467)	
training:	Epoch: [68][15/204]	Loss 0.4373 (0.4461)	
training:	Epoch: [68][16/204]	Loss 0.3615 (0.4408)	
training:	Epoch: [68][17/204]	Loss 0.4878 (0.4435)	
training:	Epoch: [68][18/204]	Loss 0.4687 (0.4449)	
training:	Epoch: [68][19/204]	Loss 0.4622 (0.4459)	
training:	Epoch: [68][20/204]	Loss 0.4092 (0.4440)	
training:	Epoch: [68][21/204]	Loss 0.4956 (0.4465)	
training:	Epoch: [68][22/204]	Loss 0.4955 (0.4487)	
training:	Epoch: [68][23/204]	Loss 0.4906 (0.4505)	
training:	Epoch: [68][24/204]	Loss 0.4164 (0.4491)	
training:	Epoch: [68][25/204]	Loss 0.4300 (0.4483)	
training:	Epoch: [68][26/204]	Loss 0.5065 (0.4506)	
training:	Epoch: [68][27/204]	Loss 0.4816 (0.4517)	
training:	Epoch: [68][28/204]	Loss 0.5097 (0.4538)	
training:	Epoch: [68][29/204]	Loss 0.4624 (0.4541)	
training:	Epoch: [68][30/204]	Loss 0.5548 (0.4575)	
training:	Epoch: [68][31/204]	Loss 0.4571 (0.4574)	
training:	Epoch: [68][32/204]	Loss 0.4726 (0.4579)	
training:	Epoch: [68][33/204]	Loss 0.4138 (0.4566)	
training:	Epoch: [68][34/204]	Loss 0.6113 (0.4611)	
training:	Epoch: [68][35/204]	Loss 0.5887 (0.4648)	
training:	Epoch: [68][36/204]	Loss 0.3501 (0.4616)	
training:	Epoch: [68][37/204]	Loss 0.4570 (0.4615)	
training:	Epoch: [68][38/204]	Loss 0.5153 (0.4629)	
training:	Epoch: [68][39/204]	Loss 0.3564 (0.4602)	
training:	Epoch: [68][40/204]	Loss 0.5432 (0.4622)	
training:	Epoch: [68][41/204]	Loss 0.3579 (0.4597)	
training:	Epoch: [68][42/204]	Loss 0.4406 (0.4592)	
training:	Epoch: [68][43/204]	Loss 0.5087 (0.4604)	
training:	Epoch: [68][44/204]	Loss 0.4842 (0.4609)	
training:	Epoch: [68][45/204]	Loss 0.4876 (0.4615)	
training:	Epoch: [68][46/204]	Loss 0.4065 (0.4603)	
training:	Epoch: [68][47/204]	Loss 0.4488 (0.4601)	
training:	Epoch: [68][48/204]	Loss 0.3665 (0.4581)	
training:	Epoch: [68][49/204]	Loss 0.4608 (0.4582)	
training:	Epoch: [68][50/204]	Loss 0.6766 (0.4625)	
training:	Epoch: [68][51/204]	Loss 0.3902 (0.4611)	
training:	Epoch: [68][52/204]	Loss 0.4550 (0.4610)	
training:	Epoch: [68][53/204]	Loss 0.5267 (0.4622)	
training:	Epoch: [68][54/204]	Loss 0.4284 (0.4616)	
training:	Epoch: [68][55/204]	Loss 0.4909 (0.4622)	
training:	Epoch: [68][56/204]	Loss 0.4189 (0.4614)	
training:	Epoch: [68][57/204]	Loss 0.2974 (0.4585)	
training:	Epoch: [68][58/204]	Loss 0.4143 (0.4577)	
training:	Epoch: [68][59/204]	Loss 0.4159 (0.4570)	
training:	Epoch: [68][60/204]	Loss 0.4173 (0.4564)	
training:	Epoch: [68][61/204]	Loss 0.4545 (0.4563)	
training:	Epoch: [68][62/204]	Loss 0.4862 (0.4568)	
training:	Epoch: [68][63/204]	Loss 0.3351 (0.4549)	
training:	Epoch: [68][64/204]	Loss 0.6180 (0.4574)	
training:	Epoch: [68][65/204]	Loss 0.5550 (0.4589)	
training:	Epoch: [68][66/204]	Loss 0.5028 (0.4596)	
training:	Epoch: [68][67/204]	Loss 0.6508 (0.4625)	
training:	Epoch: [68][68/204]	Loss 0.5112 (0.4632)	
training:	Epoch: [68][69/204]	Loss 0.3812 (0.4620)	
training:	Epoch: [68][70/204]	Loss 0.4913 (0.4624)	
training:	Epoch: [68][71/204]	Loss 0.7488 (0.4664)	
training:	Epoch: [68][72/204]	Loss 0.4800 (0.4666)	
training:	Epoch: [68][73/204]	Loss 0.5562 (0.4679)	
training:	Epoch: [68][74/204]	Loss 0.3386 (0.4661)	
training:	Epoch: [68][75/204]	Loss 0.6323 (0.4683)	
training:	Epoch: [68][76/204]	Loss 0.4691 (0.4683)	
training:	Epoch: [68][77/204]	Loss 0.2782 (0.4659)	
training:	Epoch: [68][78/204]	Loss 0.6765 (0.4686)	
training:	Epoch: [68][79/204]	Loss 0.4955 (0.4689)	
training:	Epoch: [68][80/204]	Loss 0.4114 (0.4682)	
training:	Epoch: [68][81/204]	Loss 0.4493 (0.4680)	
training:	Epoch: [68][82/204]	Loss 0.3653 (0.4667)	
training:	Epoch: [68][83/204]	Loss 0.4519 (0.4665)	
training:	Epoch: [68][84/204]	Loss 0.5398 (0.4674)	
training:	Epoch: [68][85/204]	Loss 0.5142 (0.4679)	
training:	Epoch: [68][86/204]	Loss 0.3642 (0.4667)	
training:	Epoch: [68][87/204]	Loss 0.4116 (0.4661)	
training:	Epoch: [68][88/204]	Loss 0.4524 (0.4659)	
training:	Epoch: [68][89/204]	Loss 0.4623 (0.4659)	
training:	Epoch: [68][90/204]	Loss 0.4950 (0.4662)	
training:	Epoch: [68][91/204]	Loss 0.5719 (0.4674)	
training:	Epoch: [68][92/204]	Loss 0.5574 (0.4684)	
training:	Epoch: [68][93/204]	Loss 0.4760 (0.4684)	
training:	Epoch: [68][94/204]	Loss 0.4765 (0.4685)	
training:	Epoch: [68][95/204]	Loss 0.3009 (0.4668)	
training:	Epoch: [68][96/204]	Loss 0.4846 (0.4670)	
training:	Epoch: [68][97/204]	Loss 0.3173 (0.4654)	
training:	Epoch: [68][98/204]	Loss 0.4255 (0.4650)	
training:	Epoch: [68][99/204]	Loss 0.3674 (0.4640)	
training:	Epoch: [68][100/204]	Loss 0.5416 (0.4648)	
training:	Epoch: [68][101/204]	Loss 0.4419 (0.4646)	
training:	Epoch: [68][102/204]	Loss 0.5336 (0.4652)	
training:	Epoch: [68][103/204]	Loss 0.3760 (0.4644)	
training:	Epoch: [68][104/204]	Loss 0.4409 (0.4642)	
training:	Epoch: [68][105/204]	Loss 0.4641 (0.4642)	
training:	Epoch: [68][106/204]	Loss 0.6991 (0.4664)	
training:	Epoch: [68][107/204]	Loss 0.5737 (0.4674)	
training:	Epoch: [68][108/204]	Loss 0.3204 (0.4660)	
training:	Epoch: [68][109/204]	Loss 0.3910 (0.4653)	
training:	Epoch: [68][110/204]	Loss 0.5635 (0.4662)	
training:	Epoch: [68][111/204]	Loss 0.3613 (0.4653)	
training:	Epoch: [68][112/204]	Loss 0.4549 (0.4652)	
training:	Epoch: [68][113/204]	Loss 0.4804 (0.4653)	
training:	Epoch: [68][114/204]	Loss 0.5842 (0.4664)	
training:	Epoch: [68][115/204]	Loss 0.4272 (0.4660)	
training:	Epoch: [68][116/204]	Loss 0.4796 (0.4661)	
training:	Epoch: [68][117/204]	Loss 0.5199 (0.4666)	
training:	Epoch: [68][118/204]	Loss 0.4471 (0.4664)	
training:	Epoch: [68][119/204]	Loss 0.3996 (0.4659)	
training:	Epoch: [68][120/204]	Loss 0.3495 (0.4649)	
training:	Epoch: [68][121/204]	Loss 0.3256 (0.4637)	
training:	Epoch: [68][122/204]	Loss 0.4057 (0.4633)	
training:	Epoch: [68][123/204]	Loss 0.4586 (0.4632)	
training:	Epoch: [68][124/204]	Loss 0.3719 (0.4625)	
training:	Epoch: [68][125/204]	Loss 0.5233 (0.4630)	
training:	Epoch: [68][126/204]	Loss 0.3849 (0.4624)	
training:	Epoch: [68][127/204]	Loss 0.3979 (0.4619)	
training:	Epoch: [68][128/204]	Loss 0.4965 (0.4621)	
training:	Epoch: [68][129/204]	Loss 0.5278 (0.4626)	
training:	Epoch: [68][130/204]	Loss 0.4900 (0.4628)	
training:	Epoch: [68][131/204]	Loss 0.5157 (0.4632)	
training:	Epoch: [68][132/204]	Loss 0.5720 (0.4641)	
training:	Epoch: [68][133/204]	Loss 0.3813 (0.4635)	
training:	Epoch: [68][134/204]	Loss 0.4790 (0.4636)	
training:	Epoch: [68][135/204]	Loss 0.3419 (0.4627)	
training:	Epoch: [68][136/204]	Loss 0.4160 (0.4623)	
training:	Epoch: [68][137/204]	Loss 0.5699 (0.4631)	
training:	Epoch: [68][138/204]	Loss 0.4767 (0.4632)	
training:	Epoch: [68][139/204]	Loss 0.5065 (0.4635)	
training:	Epoch: [68][140/204]	Loss 0.4968 (0.4638)	
training:	Epoch: [68][141/204]	Loss 0.4908 (0.4639)	
training:	Epoch: [68][142/204]	Loss 0.3683 (0.4633)	
training:	Epoch: [68][143/204]	Loss 0.4327 (0.4631)	
training:	Epoch: [68][144/204]	Loss 0.4087 (0.4627)	
training:	Epoch: [68][145/204]	Loss 0.4768 (0.4628)	
training:	Epoch: [68][146/204]	Loss 0.5347 (0.4633)	
training:	Epoch: [68][147/204]	Loss 0.2953 (0.4621)	
training:	Epoch: [68][148/204]	Loss 0.6666 (0.4635)	
training:	Epoch: [68][149/204]	Loss 0.4422 (0.4634)	
training:	Epoch: [68][150/204]	Loss 0.6312 (0.4645)	
training:	Epoch: [68][151/204]	Loss 0.4485 (0.4644)	
training:	Epoch: [68][152/204]	Loss 0.4075 (0.4640)	
training:	Epoch: [68][153/204]	Loss 0.2966 (0.4629)	
training:	Epoch: [68][154/204]	Loss 0.4475 (0.4628)	
training:	Epoch: [68][155/204]	Loss 0.3908 (0.4623)	
training:	Epoch: [68][156/204]	Loss 0.4893 (0.4625)	
training:	Epoch: [68][157/204]	Loss 0.6075 (0.4634)	
training:	Epoch: [68][158/204]	Loss 0.4985 (0.4637)	
training:	Epoch: [68][159/204]	Loss 0.4354 (0.4635)	
training:	Epoch: [68][160/204]	Loss 0.3056 (0.4625)	
training:	Epoch: [68][161/204]	Loss 0.5703 (0.4632)	
training:	Epoch: [68][162/204]	Loss 0.4343 (0.4630)	
training:	Epoch: [68][163/204]	Loss 0.3953 (0.4626)	
training:	Epoch: [68][164/204]	Loss 0.5444 (0.4631)	
training:	Epoch: [68][165/204]	Loss 0.4750 (0.4632)	
training:	Epoch: [68][166/204]	Loss 0.4344 (0.4630)	
training:	Epoch: [68][167/204]	Loss 0.4269 (0.4628)	
training:	Epoch: [68][168/204]	Loss 0.3829 (0.4623)	
training:	Epoch: [68][169/204]	Loss 0.6176 (0.4632)	
training:	Epoch: [68][170/204]	Loss 0.4417 (0.4631)	
training:	Epoch: [68][171/204]	Loss 0.3285 (0.4623)	
training:	Epoch: [68][172/204]	Loss 0.3973 (0.4619)	
training:	Epoch: [68][173/204]	Loss 0.3812 (0.4614)	
training:	Epoch: [68][174/204]	Loss 0.4464 (0.4614)	
training:	Epoch: [68][175/204]	Loss 0.6591 (0.4625)	
training:	Epoch: [68][176/204]	Loss 0.4968 (0.4627)	
training:	Epoch: [68][177/204]	Loss 0.5472 (0.4632)	
training:	Epoch: [68][178/204]	Loss 0.4513 (0.4631)	
training:	Epoch: [68][179/204]	Loss 0.4636 (0.4631)	
training:	Epoch: [68][180/204]	Loss 0.4617 (0.4631)	
training:	Epoch: [68][181/204]	Loss 0.5863 (0.4638)	
training:	Epoch: [68][182/204]	Loss 0.4198 (0.4635)	
training:	Epoch: [68][183/204]	Loss 0.4443 (0.4634)	
training:	Epoch: [68][184/204]	Loss 0.4680 (0.4635)	
training:	Epoch: [68][185/204]	Loss 0.4987 (0.4636)	
training:	Epoch: [68][186/204]	Loss 0.4617 (0.4636)	
training:	Epoch: [68][187/204]	Loss 0.4307 (0.4635)	
training:	Epoch: [68][188/204]	Loss 0.4139 (0.4632)	
training:	Epoch: [68][189/204]	Loss 0.4262 (0.4630)	
training:	Epoch: [68][190/204]	Loss 0.5216 (0.4633)	
training:	Epoch: [68][191/204]	Loss 0.5368 (0.4637)	
training:	Epoch: [68][192/204]	Loss 0.4293 (0.4635)	
training:	Epoch: [68][193/204]	Loss 0.4189 (0.4633)	
training:	Epoch: [68][194/204]	Loss 0.5358 (0.4637)	
training:	Epoch: [68][195/204]	Loss 0.4290 (0.4635)	
training:	Epoch: [68][196/204]	Loss 0.4289 (0.4633)	
training:	Epoch: [68][197/204]	Loss 0.4468 (0.4632)	
training:	Epoch: [68][198/204]	Loss 0.5622 (0.4637)	
training:	Epoch: [68][199/204]	Loss 0.5268 (0.4640)	
training:	Epoch: [68][200/204]	Loss 0.5393 (0.4644)	
training:	Epoch: [68][201/204]	Loss 0.5312 (0.4647)	
training:	Epoch: [68][202/204]	Loss 0.3455 (0.4641)	
training:	Epoch: [68][203/204]	Loss 0.3978 (0.4638)	
training:	Epoch: [68][204/204]	Loss 0.5245 (0.4641)	
Training:	 Loss: 0.4634

Training:	 ACC: 0.7959 0.7966 0.8122 0.7797
Validation:	 ACC: 0.7834 0.7844 0.8045 0.7623
Validation:	 Best_BACC: 0.7834 0.7844 0.8045 0.7623
Validation:	 Loss: 0.4717
Pretraining:	Epoch 69/120
----------
training:	Epoch: [69][1/204]	Loss 0.3161 (0.3161)	
training:	Epoch: [69][2/204]	Loss 0.4009 (0.3585)	
training:	Epoch: [69][3/204]	Loss 0.3022 (0.3397)	
training:	Epoch: [69][4/204]	Loss 0.4194 (0.3597)	
training:	Epoch: [69][5/204]	Loss 0.3561 (0.3589)	
training:	Epoch: [69][6/204]	Loss 0.5130 (0.3846)	
training:	Epoch: [69][7/204]	Loss 0.4858 (0.3991)	
training:	Epoch: [69][8/204]	Loss 0.4051 (0.3998)	
training:	Epoch: [69][9/204]	Loss 0.5747 (0.4193)	
training:	Epoch: [69][10/204]	Loss 0.4480 (0.4221)	
training:	Epoch: [69][11/204]	Loss 0.4608 (0.4256)	
training:	Epoch: [69][12/204]	Loss 0.4188 (0.4251)	
training:	Epoch: [69][13/204]	Loss 0.4019 (0.4233)	
training:	Epoch: [69][14/204]	Loss 0.6563 (0.4399)	
training:	Epoch: [69][15/204]	Loss 0.5094 (0.4446)	
training:	Epoch: [69][16/204]	Loss 0.5331 (0.4501)	
training:	Epoch: [69][17/204]	Loss 0.3753 (0.4457)	
training:	Epoch: [69][18/204]	Loss 0.4944 (0.4484)	
training:	Epoch: [69][19/204]	Loss 0.5479 (0.4536)	
training:	Epoch: [69][20/204]	Loss 0.3359 (0.4478)	
training:	Epoch: [69][21/204]	Loss 0.4216 (0.4465)	
training:	Epoch: [69][22/204]	Loss 0.4979 (0.4488)	
training:	Epoch: [69][23/204]	Loss 0.4937 (0.4508)	
training:	Epoch: [69][24/204]	Loss 0.4549 (0.4510)	
training:	Epoch: [69][25/204]	Loss 0.4865 (0.4524)	
training:	Epoch: [69][26/204]	Loss 0.4119 (0.4508)	
training:	Epoch: [69][27/204]	Loss 0.4349 (0.4502)	
training:	Epoch: [69][28/204]	Loss 0.3385 (0.4463)	
training:	Epoch: [69][29/204]	Loss 0.5054 (0.4483)	
training:	Epoch: [69][30/204]	Loss 0.4475 (0.4483)	
training:	Epoch: [69][31/204]	Loss 0.3904 (0.4464)	
training:	Epoch: [69][32/204]	Loss 0.5852 (0.4507)	
training:	Epoch: [69][33/204]	Loss 0.4946 (0.4521)	
training:	Epoch: [69][34/204]	Loss 0.4670 (0.4525)	
training:	Epoch: [69][35/204]	Loss 0.5248 (0.4546)	
training:	Epoch: [69][36/204]	Loss 0.5414 (0.4570)	
training:	Epoch: [69][37/204]	Loss 0.4419 (0.4566)	
training:	Epoch: [69][38/204]	Loss 0.4193 (0.4556)	
training:	Epoch: [69][39/204]	Loss 0.3177 (0.4521)	
training:	Epoch: [69][40/204]	Loss 0.4625 (0.4523)	
training:	Epoch: [69][41/204]	Loss 0.6295 (0.4566)	
training:	Epoch: [69][42/204]	Loss 0.4305 (0.4560)	
training:	Epoch: [69][43/204]	Loss 0.5385 (0.4579)	
training:	Epoch: [69][44/204]	Loss 0.3826 (0.4562)	
training:	Epoch: [69][45/204]	Loss 0.4750 (0.4566)	
training:	Epoch: [69][46/204]	Loss 0.3860 (0.4551)	
training:	Epoch: [69][47/204]	Loss 0.4468 (0.4549)	
training:	Epoch: [69][48/204]	Loss 0.4319 (0.4544)	
training:	Epoch: [69][49/204]	Loss 0.5270 (0.4559)	
training:	Epoch: [69][50/204]	Loss 0.4697 (0.4562)	
training:	Epoch: [69][51/204]	Loss 0.5594 (0.4582)	
training:	Epoch: [69][52/204]	Loss 0.5399 (0.4598)	
training:	Epoch: [69][53/204]	Loss 0.4648 (0.4599)	
training:	Epoch: [69][54/204]	Loss 0.3758 (0.4583)	
training:	Epoch: [69][55/204]	Loss 0.6020 (0.4609)	
training:	Epoch: [69][56/204]	Loss 0.5069 (0.4618)	
training:	Epoch: [69][57/204]	Loss 0.3717 (0.4602)	
training:	Epoch: [69][58/204]	Loss 0.5458 (0.4617)	
training:	Epoch: [69][59/204]	Loss 0.4645 (0.4617)	
training:	Epoch: [69][60/204]	Loss 0.4026 (0.4607)	
training:	Epoch: [69][61/204]	Loss 0.4621 (0.4607)	
training:	Epoch: [69][62/204]	Loss 0.3670 (0.4592)	
training:	Epoch: [69][63/204]	Loss 0.4958 (0.4598)	
training:	Epoch: [69][64/204]	Loss 0.5426 (0.4611)	
training:	Epoch: [69][65/204]	Loss 0.4334 (0.4607)	
training:	Epoch: [69][66/204]	Loss 0.4983 (0.4613)	
training:	Epoch: [69][67/204]	Loss 0.4488 (0.4611)	
training:	Epoch: [69][68/204]	Loss 0.4180 (0.4604)	
training:	Epoch: [69][69/204]	Loss 0.3973 (0.4595)	
training:	Epoch: [69][70/204]	Loss 0.4953 (0.4600)	
training:	Epoch: [69][71/204]	Loss 0.4855 (0.4604)	
training:	Epoch: [69][72/204]	Loss 0.4761 (0.4606)	
training:	Epoch: [69][73/204]	Loss 0.3873 (0.4596)	
training:	Epoch: [69][74/204]	Loss 0.4376 (0.4593)	
training:	Epoch: [69][75/204]	Loss 0.4265 (0.4589)	
training:	Epoch: [69][76/204]	Loss 0.2892 (0.4566)	
training:	Epoch: [69][77/204]	Loss 0.6286 (0.4589)	
training:	Epoch: [69][78/204]	Loss 0.5786 (0.4604)	
training:	Epoch: [69][79/204]	Loss 0.3525 (0.4590)	
training:	Epoch: [69][80/204]	Loss 0.4528 (0.4590)	
training:	Epoch: [69][81/204]	Loss 0.5115 (0.4596)	
training:	Epoch: [69][82/204]	Loss 0.4248 (0.4592)	
training:	Epoch: [69][83/204]	Loss 0.4590 (0.4592)	
training:	Epoch: [69][84/204]	Loss 0.4733 (0.4594)	
training:	Epoch: [69][85/204]	Loss 0.3364 (0.4579)	
training:	Epoch: [69][86/204]	Loss 0.5759 (0.4593)	
training:	Epoch: [69][87/204]	Loss 0.4959 (0.4597)	
training:	Epoch: [69][88/204]	Loss 0.3114 (0.4580)	
training:	Epoch: [69][89/204]	Loss 0.4215 (0.4576)	
training:	Epoch: [69][90/204]	Loss 0.6322 (0.4595)	
training:	Epoch: [69][91/204]	Loss 0.4519 (0.4595)	
training:	Epoch: [69][92/204]	Loss 0.4589 (0.4595)	
training:	Epoch: [69][93/204]	Loss 0.5131 (0.4600)	
training:	Epoch: [69][94/204]	Loss 0.4755 (0.4602)	
training:	Epoch: [69][95/204]	Loss 0.4053 (0.4596)	
training:	Epoch: [69][96/204]	Loss 0.4504 (0.4595)	
training:	Epoch: [69][97/204]	Loss 0.3810 (0.4587)	
training:	Epoch: [69][98/204]	Loss 0.4592 (0.4587)	
training:	Epoch: [69][99/204]	Loss 0.4990 (0.4591)	
training:	Epoch: [69][100/204]	Loss 0.3735 (0.4583)	
training:	Epoch: [69][101/204]	Loss 0.5154 (0.4588)	
training:	Epoch: [69][102/204]	Loss 0.3300 (0.4576)	
training:	Epoch: [69][103/204]	Loss 0.3854 (0.4569)	
training:	Epoch: [69][104/204]	Loss 0.3542 (0.4559)	
training:	Epoch: [69][105/204]	Loss 0.4476 (0.4558)	
training:	Epoch: [69][106/204]	Loss 0.4072 (0.4553)	
training:	Epoch: [69][107/204]	Loss 0.5585 (0.4563)	
training:	Epoch: [69][108/204]	Loss 0.3041 (0.4549)	
training:	Epoch: [69][109/204]	Loss 0.5617 (0.4559)	
training:	Epoch: [69][110/204]	Loss 0.4272 (0.4556)	
training:	Epoch: [69][111/204]	Loss 0.4472 (0.4555)	
training:	Epoch: [69][112/204]	Loss 0.3788 (0.4549)	
training:	Epoch: [69][113/204]	Loss 0.4015 (0.4544)	
training:	Epoch: [69][114/204]	Loss 0.4730 (0.4545)	
training:	Epoch: [69][115/204]	Loss 0.3786 (0.4539)	
training:	Epoch: [69][116/204]	Loss 0.4782 (0.4541)	
training:	Epoch: [69][117/204]	Loss 0.4966 (0.4545)	
training:	Epoch: [69][118/204]	Loss 0.5662 (0.4554)	
training:	Epoch: [69][119/204]	Loss 0.4755 (0.4556)	
training:	Epoch: [69][120/204]	Loss 0.3416 (0.4546)	
training:	Epoch: [69][121/204]	Loss 0.6821 (0.4565)	
training:	Epoch: [69][122/204]	Loss 0.6175 (0.4578)	
training:	Epoch: [69][123/204]	Loss 0.3544 (0.4570)	
training:	Epoch: [69][124/204]	Loss 0.4161 (0.4567)	
training:	Epoch: [69][125/204]	Loss 0.5451 (0.4574)	
training:	Epoch: [69][126/204]	Loss 0.5492 (0.4581)	
training:	Epoch: [69][127/204]	Loss 0.3830 (0.4575)	
training:	Epoch: [69][128/204]	Loss 0.5150 (0.4579)	
training:	Epoch: [69][129/204]	Loss 0.5133 (0.4584)	
training:	Epoch: [69][130/204]	Loss 0.6222 (0.4596)	
training:	Epoch: [69][131/204]	Loss 0.3819 (0.4590)	
training:	Epoch: [69][132/204]	Loss 0.4231 (0.4588)	
training:	Epoch: [69][133/204]	Loss 0.6275 (0.4600)	
training:	Epoch: [69][134/204]	Loss 0.3308 (0.4591)	
training:	Epoch: [69][135/204]	Loss 0.3698 (0.4584)	
training:	Epoch: [69][136/204]	Loss 0.3736 (0.4578)	
training:	Epoch: [69][137/204]	Loss 0.5217 (0.4583)	
training:	Epoch: [69][138/204]	Loss 0.3896 (0.4578)	
training:	Epoch: [69][139/204]	Loss 0.4456 (0.4577)	
training:	Epoch: [69][140/204]	Loss 0.4007 (0.4573)	
training:	Epoch: [69][141/204]	Loss 0.5337 (0.4578)	
training:	Epoch: [69][142/204]	Loss 0.4995 (0.4581)	
training:	Epoch: [69][143/204]	Loss 0.4064 (0.4577)	
training:	Epoch: [69][144/204]	Loss 0.4870 (0.4579)	
training:	Epoch: [69][145/204]	Loss 0.6118 (0.4590)	
training:	Epoch: [69][146/204]	Loss 0.5191 (0.4594)	
training:	Epoch: [69][147/204]	Loss 0.5826 (0.4603)	
training:	Epoch: [69][148/204]	Loss 0.5378 (0.4608)	
training:	Epoch: [69][149/204]	Loss 0.3703 (0.4602)	
training:	Epoch: [69][150/204]	Loss 0.4689 (0.4602)	
training:	Epoch: [69][151/204]	Loss 0.3163 (0.4593)	
training:	Epoch: [69][152/204]	Loss 0.5989 (0.4602)	
training:	Epoch: [69][153/204]	Loss 0.3632 (0.4596)	
training:	Epoch: [69][154/204]	Loss 0.5496 (0.4601)	
training:	Epoch: [69][155/204]	Loss 0.3609 (0.4595)	
training:	Epoch: [69][156/204]	Loss 0.4022 (0.4591)	
training:	Epoch: [69][157/204]	Loss 0.4893 (0.4593)	
training:	Epoch: [69][158/204]	Loss 0.3953 (0.4589)	
training:	Epoch: [69][159/204]	Loss 0.3655 (0.4583)	
training:	Epoch: [69][160/204]	Loss 0.3647 (0.4578)	
training:	Epoch: [69][161/204]	Loss 0.4976 (0.4580)	
training:	Epoch: [69][162/204]	Loss 0.3448 (0.4573)	
training:	Epoch: [69][163/204]	Loss 0.3580 (0.4567)	
training:	Epoch: [69][164/204]	Loss 0.5092 (0.4570)	
training:	Epoch: [69][165/204]	Loss 0.4762 (0.4571)	
training:	Epoch: [69][166/204]	Loss 0.4665 (0.4572)	
training:	Epoch: [69][167/204]	Loss 0.5346 (0.4576)	
training:	Epoch: [69][168/204]	Loss 0.3518 (0.4570)	
training:	Epoch: [69][169/204]	Loss 0.3762 (0.4565)	
training:	Epoch: [69][170/204]	Loss 0.8008 (0.4586)	
training:	Epoch: [69][171/204]	Loss 0.5353 (0.4590)	
training:	Epoch: [69][172/204]	Loss 0.3908 (0.4586)	
training:	Epoch: [69][173/204]	Loss 0.4936 (0.4588)	
training:	Epoch: [69][174/204]	Loss 0.4176 (0.4586)	
training:	Epoch: [69][175/204]	Loss 0.5669 (0.4592)	
training:	Epoch: [69][176/204]	Loss 0.4984 (0.4594)	
training:	Epoch: [69][177/204]	Loss 0.5584 (0.4600)	
training:	Epoch: [69][178/204]	Loss 0.7248 (0.4615)	
training:	Epoch: [69][179/204]	Loss 0.4559 (0.4614)	
training:	Epoch: [69][180/204]	Loss 0.4886 (0.4616)	
training:	Epoch: [69][181/204]	Loss 0.3711 (0.4611)	
training:	Epoch: [69][182/204]	Loss 0.4255 (0.4609)	
training:	Epoch: [69][183/204]	Loss 0.4538 (0.4609)	
training:	Epoch: [69][184/204]	Loss 0.4310 (0.4607)	
training:	Epoch: [69][185/204]	Loss 0.6091 (0.4615)	
training:	Epoch: [69][186/204]	Loss 0.3884 (0.4611)	
training:	Epoch: [69][187/204]	Loss 0.3399 (0.4605)	
training:	Epoch: [69][188/204]	Loss 0.5415 (0.4609)	
training:	Epoch: [69][189/204]	Loss 0.3926 (0.4605)	
training:	Epoch: [69][190/204]	Loss 0.4318 (0.4604)	
training:	Epoch: [69][191/204]	Loss 0.3917 (0.4600)	
training:	Epoch: [69][192/204]	Loss 0.4379 (0.4599)	
training:	Epoch: [69][193/204]	Loss 0.3467 (0.4593)	
training:	Epoch: [69][194/204]	Loss 0.4201 (0.4591)	
training:	Epoch: [69][195/204]	Loss 0.5576 (0.4596)	
training:	Epoch: [69][196/204]	Loss 0.5242 (0.4599)	
training:	Epoch: [69][197/204]	Loss 0.5582 (0.4604)	
training:	Epoch: [69][198/204]	Loss 0.4297 (0.4603)	
training:	Epoch: [69][199/204]	Loss 0.3728 (0.4598)	
training:	Epoch: [69][200/204]	Loss 0.5107 (0.4601)	
training:	Epoch: [69][201/204]	Loss 0.3099 (0.4594)	
training:	Epoch: [69][202/204]	Loss 0.5140 (0.4596)	
training:	Epoch: [69][203/204]	Loss 0.4735 (0.4597)	
training:	Epoch: [69][204/204]	Loss 0.5336 (0.4601)	
Training:	 Loss: 0.4594

Training:	 ACC: 0.7971 0.7976 0.8095 0.7848
Validation:	 ACC: 0.7858 0.7865 0.8025 0.7691
Validation:	 Best_BACC: 0.7858 0.7865 0.8025 0.7691
Validation:	 Loss: 0.4702
Pretraining:	Epoch 70/120
----------
training:	Epoch: [70][1/204]	Loss 0.4904 (0.4904)	
training:	Epoch: [70][2/204]	Loss 0.4346 (0.4625)	
training:	Epoch: [70][3/204]	Loss 0.5786 (0.5012)	
training:	Epoch: [70][4/204]	Loss 0.5040 (0.5019)	
training:	Epoch: [70][5/204]	Loss 0.3510 (0.4717)	
training:	Epoch: [70][6/204]	Loss 0.6652 (0.5040)	
training:	Epoch: [70][7/204]	Loss 0.5443 (0.5097)	
training:	Epoch: [70][8/204]	Loss 0.3910 (0.4949)	
training:	Epoch: [70][9/204]	Loss 0.5523 (0.5013)	
training:	Epoch: [70][10/204]	Loss 0.3478 (0.4859)	
training:	Epoch: [70][11/204]	Loss 0.4425 (0.4820)	
training:	Epoch: [70][12/204]	Loss 0.4022 (0.4753)	
training:	Epoch: [70][13/204]	Loss 0.2855 (0.4607)	
training:	Epoch: [70][14/204]	Loss 0.6483 (0.4741)	
training:	Epoch: [70][15/204]	Loss 0.3836 (0.4681)	
training:	Epoch: [70][16/204]	Loss 0.4138 (0.4647)	
training:	Epoch: [70][17/204]	Loss 0.4801 (0.4656)	
training:	Epoch: [70][18/204]	Loss 0.5333 (0.4694)	
training:	Epoch: [70][19/204]	Loss 0.4295 (0.4673)	
training:	Epoch: [70][20/204]	Loss 0.4657 (0.4672)	
training:	Epoch: [70][21/204]	Loss 0.2680 (0.4577)	
training:	Epoch: [70][22/204]	Loss 0.4945 (0.4594)	
training:	Epoch: [70][23/204]	Loss 0.4143 (0.4574)	
training:	Epoch: [70][24/204]	Loss 0.4364 (0.4565)	
training:	Epoch: [70][25/204]	Loss 0.4223 (0.4552)	
training:	Epoch: [70][26/204]	Loss 0.5886 (0.4603)	
training:	Epoch: [70][27/204]	Loss 0.3843 (0.4575)	
training:	Epoch: [70][28/204]	Loss 0.4270 (0.4564)	
training:	Epoch: [70][29/204]	Loss 0.3876 (0.4540)	
training:	Epoch: [70][30/204]	Loss 0.6247 (0.4597)	
training:	Epoch: [70][31/204]	Loss 0.5336 (0.4621)	
training:	Epoch: [70][32/204]	Loss 0.5563 (0.4650)	
training:	Epoch: [70][33/204]	Loss 0.4593 (0.4649)	
training:	Epoch: [70][34/204]	Loss 0.3509 (0.4615)	
training:	Epoch: [70][35/204]	Loss 0.3207 (0.4575)	
training:	Epoch: [70][36/204]	Loss 0.5521 (0.4601)	
training:	Epoch: [70][37/204]	Loss 0.3861 (0.4581)	
training:	Epoch: [70][38/204]	Loss 0.3013 (0.4540)	
training:	Epoch: [70][39/204]	Loss 0.5892 (0.4575)	
training:	Epoch: [70][40/204]	Loss 0.6886 (0.4632)	
training:	Epoch: [70][41/204]	Loss 0.4963 (0.4641)	
training:	Epoch: [70][42/204]	Loss 0.5525 (0.4662)	
training:	Epoch: [70][43/204]	Loss 0.3971 (0.4646)	
training:	Epoch: [70][44/204]	Loss 0.3743 (0.4625)	
training:	Epoch: [70][45/204]	Loss 0.5392 (0.4642)	
training:	Epoch: [70][46/204]	Loss 0.5033 (0.4651)	
training:	Epoch: [70][47/204]	Loss 0.5041 (0.4659)	
training:	Epoch: [70][48/204]	Loss 0.3580 (0.4636)	
training:	Epoch: [70][49/204]	Loss 0.5196 (0.4648)	
training:	Epoch: [70][50/204]	Loss 0.4749 (0.4650)	
training:	Epoch: [70][51/204]	Loss 0.5495 (0.4666)	
training:	Epoch: [70][52/204]	Loss 0.2746 (0.4629)	
training:	Epoch: [70][53/204]	Loss 0.3886 (0.4615)	
training:	Epoch: [70][54/204]	Loss 0.3766 (0.4600)	
training:	Epoch: [70][55/204]	Loss 0.5166 (0.4610)	
training:	Epoch: [70][56/204]	Loss 0.5058 (0.4618)	
training:	Epoch: [70][57/204]	Loss 0.5974 (0.4642)	
training:	Epoch: [70][58/204]	Loss 0.5881 (0.4663)	
training:	Epoch: [70][59/204]	Loss 0.4829 (0.4666)	
training:	Epoch: [70][60/204]	Loss 0.5574 (0.4681)	
training:	Epoch: [70][61/204]	Loss 0.5749 (0.4699)	
training:	Epoch: [70][62/204]	Loss 0.5310 (0.4708)	
training:	Epoch: [70][63/204]	Loss 0.4498 (0.4705)	
training:	Epoch: [70][64/204]	Loss 0.5153 (0.4712)	
training:	Epoch: [70][65/204]	Loss 0.4831 (0.4714)	
training:	Epoch: [70][66/204]	Loss 0.3403 (0.4694)	
training:	Epoch: [70][67/204]	Loss 0.3973 (0.4683)	
training:	Epoch: [70][68/204]	Loss 0.4264 (0.4677)	
training:	Epoch: [70][69/204]	Loss 0.4427 (0.4674)	
training:	Epoch: [70][70/204]	Loss 0.4860 (0.4676)	
training:	Epoch: [70][71/204]	Loss 0.5226 (0.4684)	
training:	Epoch: [70][72/204]	Loss 0.4101 (0.4676)	
training:	Epoch: [70][73/204]	Loss 0.6247 (0.4697)	
training:	Epoch: [70][74/204]	Loss 0.4984 (0.4701)	
training:	Epoch: [70][75/204]	Loss 0.4728 (0.4702)	
training:	Epoch: [70][76/204]	Loss 0.6026 (0.4719)	
training:	Epoch: [70][77/204]	Loss 0.4898 (0.4721)	
training:	Epoch: [70][78/204]	Loss 0.3094 (0.4700)	
training:	Epoch: [70][79/204]	Loss 0.4230 (0.4695)	
training:	Epoch: [70][80/204]	Loss 0.3943 (0.4685)	
training:	Epoch: [70][81/204]	Loss 0.4361 (0.4681)	
training:	Epoch: [70][82/204]	Loss 0.4371 (0.4677)	
training:	Epoch: [70][83/204]	Loss 0.4285 (0.4673)	
training:	Epoch: [70][84/204]	Loss 0.6585 (0.4695)	
training:	Epoch: [70][85/204]	Loss 0.3727 (0.4684)	
training:	Epoch: [70][86/204]	Loss 0.3160 (0.4666)	
training:	Epoch: [70][87/204]	Loss 0.5579 (0.4677)	
training:	Epoch: [70][88/204]	Loss 0.4912 (0.4679)	
training:	Epoch: [70][89/204]	Loss 0.4531 (0.4678)	
training:	Epoch: [70][90/204]	Loss 0.4633 (0.4677)	
training:	Epoch: [70][91/204]	Loss 0.4541 (0.4676)	
training:	Epoch: [70][92/204]	Loss 0.4335 (0.4672)	
training:	Epoch: [70][93/204]	Loss 0.5097 (0.4677)	
training:	Epoch: [70][94/204]	Loss 0.3734 (0.4667)	
training:	Epoch: [70][95/204]	Loss 0.3225 (0.4651)	
training:	Epoch: [70][96/204]	Loss 0.4363 (0.4648)	
training:	Epoch: [70][97/204]	Loss 0.5608 (0.4658)	
training:	Epoch: [70][98/204]	Loss 0.4931 (0.4661)	
training:	Epoch: [70][99/204]	Loss 0.4296 (0.4657)	
training:	Epoch: [70][100/204]	Loss 0.7008 (0.4681)	
training:	Epoch: [70][101/204]	Loss 0.4575 (0.4680)	
training:	Epoch: [70][102/204]	Loss 0.5528 (0.4688)	
training:	Epoch: [70][103/204]	Loss 0.4583 (0.4687)	
training:	Epoch: [70][104/204]	Loss 0.4568 (0.4686)	
training:	Epoch: [70][105/204]	Loss 0.4647 (0.4686)	
training:	Epoch: [70][106/204]	Loss 0.5625 (0.4694)	
training:	Epoch: [70][107/204]	Loss 0.4307 (0.4691)	
training:	Epoch: [70][108/204]	Loss 0.4402 (0.4688)	
training:	Epoch: [70][109/204]	Loss 0.4235 (0.4684)	
training:	Epoch: [70][110/204]	Loss 0.5190 (0.4689)	
training:	Epoch: [70][111/204]	Loss 0.5460 (0.4696)	
training:	Epoch: [70][112/204]	Loss 0.4807 (0.4697)	
training:	Epoch: [70][113/204]	Loss 0.4646 (0.4696)	
training:	Epoch: [70][114/204]	Loss 0.3771 (0.4688)	
training:	Epoch: [70][115/204]	Loss 0.3577 (0.4678)	
training:	Epoch: [70][116/204]	Loss 0.3826 (0.4671)	
training:	Epoch: [70][117/204]	Loss 0.3791 (0.4663)	
training:	Epoch: [70][118/204]	Loss 0.4131 (0.4659)	
training:	Epoch: [70][119/204]	Loss 0.6689 (0.4676)	
training:	Epoch: [70][120/204]	Loss 0.5105 (0.4680)	
training:	Epoch: [70][121/204]	Loss 0.5362 (0.4685)	
training:	Epoch: [70][122/204]	Loss 0.3707 (0.4677)	
training:	Epoch: [70][123/204]	Loss 0.5412 (0.4683)	
training:	Epoch: [70][124/204]	Loss 0.4846 (0.4685)	
training:	Epoch: [70][125/204]	Loss 0.5359 (0.4690)	
training:	Epoch: [70][126/204]	Loss 0.3494 (0.4680)	
training:	Epoch: [70][127/204]	Loss 0.5302 (0.4685)	
training:	Epoch: [70][128/204]	Loss 0.4991 (0.4688)	
training:	Epoch: [70][129/204]	Loss 0.3916 (0.4682)	
training:	Epoch: [70][130/204]	Loss 0.4068 (0.4677)	
training:	Epoch: [70][131/204]	Loss 0.3900 (0.4671)	
training:	Epoch: [70][132/204]	Loss 0.6844 (0.4688)	
training:	Epoch: [70][133/204]	Loss 0.3683 (0.4680)	
training:	Epoch: [70][134/204]	Loss 0.4524 (0.4679)	
training:	Epoch: [70][135/204]	Loss 0.3266 (0.4668)	
training:	Epoch: [70][136/204]	Loss 0.4442 (0.4667)	
training:	Epoch: [70][137/204]	Loss 0.5154 (0.4670)	
training:	Epoch: [70][138/204]	Loss 0.3964 (0.4665)	
training:	Epoch: [70][139/204]	Loss 0.3714 (0.4658)	
training:	Epoch: [70][140/204]	Loss 0.4923 (0.4660)	
training:	Epoch: [70][141/204]	Loss 0.3335 (0.4651)	
training:	Epoch: [70][142/204]	Loss 0.3625 (0.4644)	
training:	Epoch: [70][143/204]	Loss 0.5453 (0.4649)	
training:	Epoch: [70][144/204]	Loss 0.4764 (0.4650)	
training:	Epoch: [70][145/204]	Loss 0.3602 (0.4643)	
training:	Epoch: [70][146/204]	Loss 0.4710 (0.4643)	
training:	Epoch: [70][147/204]	Loss 0.4444 (0.4642)	
training:	Epoch: [70][148/204]	Loss 0.4917 (0.4644)	
training:	Epoch: [70][149/204]	Loss 0.3291 (0.4635)	
training:	Epoch: [70][150/204]	Loss 0.5054 (0.4637)	
training:	Epoch: [70][151/204]	Loss 0.4115 (0.4634)	
training:	Epoch: [70][152/204]	Loss 0.7688 (0.4654)	
training:	Epoch: [70][153/204]	Loss 0.4017 (0.4650)	
training:	Epoch: [70][154/204]	Loss 0.4259 (0.4647)	
training:	Epoch: [70][155/204]	Loss 0.3228 (0.4638)	
training:	Epoch: [70][156/204]	Loss 0.5128 (0.4641)	
training:	Epoch: [70][157/204]	Loss 0.4388 (0.4640)	
training:	Epoch: [70][158/204]	Loss 0.3184 (0.4631)	
training:	Epoch: [70][159/204]	Loss 0.4025 (0.4627)	
training:	Epoch: [70][160/204]	Loss 0.3625 (0.4620)	
training:	Epoch: [70][161/204]	Loss 0.3404 (0.4613)	
training:	Epoch: [70][162/204]	Loss 0.4772 (0.4614)	
training:	Epoch: [70][163/204]	Loss 0.5112 (0.4617)	
training:	Epoch: [70][164/204]	Loss 0.3782 (0.4612)	
training:	Epoch: [70][165/204]	Loss 0.3812 (0.4607)	
training:	Epoch: [70][166/204]	Loss 0.4126 (0.4604)	
training:	Epoch: [70][167/204]	Loss 0.3041 (0.4595)	
training:	Epoch: [70][168/204]	Loss 0.4738 (0.4596)	
training:	Epoch: [70][169/204]	Loss 0.4893 (0.4597)	
training:	Epoch: [70][170/204]	Loss 0.6257 (0.4607)	
training:	Epoch: [70][171/204]	Loss 0.4387 (0.4606)	
training:	Epoch: [70][172/204]	Loss 0.5313 (0.4610)	
training:	Epoch: [70][173/204]	Loss 0.7068 (0.4624)	
training:	Epoch: [70][174/204]	Loss 0.4486 (0.4623)	
training:	Epoch: [70][175/204]	Loss 0.3914 (0.4619)	
training:	Epoch: [70][176/204]	Loss 0.5529 (0.4624)	
training:	Epoch: [70][177/204]	Loss 0.3964 (0.4621)	
training:	Epoch: [70][178/204]	Loss 0.4705 (0.4621)	
training:	Epoch: [70][179/204]	Loss 0.3440 (0.4615)	
training:	Epoch: [70][180/204]	Loss 0.3146 (0.4606)	
training:	Epoch: [70][181/204]	Loss 0.4110 (0.4604)	
training:	Epoch: [70][182/204]	Loss 0.4721 (0.4604)	
training:	Epoch: [70][183/204]	Loss 0.4621 (0.4604)	
training:	Epoch: [70][184/204]	Loss 0.4510 (0.4604)	
training:	Epoch: [70][185/204]	Loss 0.3967 (0.4600)	
training:	Epoch: [70][186/204]	Loss 0.4935 (0.4602)	
training:	Epoch: [70][187/204]	Loss 0.3300 (0.4595)	
training:	Epoch: [70][188/204]	Loss 0.2971 (0.4587)	
training:	Epoch: [70][189/204]	Loss 0.5547 (0.4592)	
training:	Epoch: [70][190/204]	Loss 0.5415 (0.4596)	
training:	Epoch: [70][191/204]	Loss 0.5559 (0.4601)	
training:	Epoch: [70][192/204]	Loss 0.4776 (0.4602)	
training:	Epoch: [70][193/204]	Loss 0.5559 (0.4607)	
training:	Epoch: [70][194/204]	Loss 0.3078 (0.4599)	
training:	Epoch: [70][195/204]	Loss 0.5131 (0.4602)	
training:	Epoch: [70][196/204]	Loss 0.5308 (0.4605)	
training:	Epoch: [70][197/204]	Loss 0.5015 (0.4608)	
training:	Epoch: [70][198/204]	Loss 0.5431 (0.4612)	
training:	Epoch: [70][199/204]	Loss 0.3943 (0.4608)	
training:	Epoch: [70][200/204]	Loss 0.6184 (0.4616)	
training:	Epoch: [70][201/204]	Loss 0.4462 (0.4615)	
training:	Epoch: [70][202/204]	Loss 0.6155 (0.4623)	
training:	Epoch: [70][203/204]	Loss 0.4857 (0.4624)	
training:	Epoch: [70][204/204]	Loss 0.4811 (0.4625)	
Training:	 Loss: 0.4618

Training:	 ACC: 0.7988 0.7993 0.8119 0.7857
Validation:	 ACC: 0.7852 0.7860 0.8025 0.7679
Validation:	 Best_BACC: 0.7858 0.7865 0.8025 0.7691
Validation:	 Loss: 0.4687
Pretraining:	Epoch 71/120
----------
training:	Epoch: [71][1/204]	Loss 0.4532 (0.4532)	
training:	Epoch: [71][2/204]	Loss 0.5394 (0.4963)	
training:	Epoch: [71][3/204]	Loss 0.4365 (0.4763)	
training:	Epoch: [71][4/204]	Loss 0.4038 (0.4582)	
training:	Epoch: [71][5/204]	Loss 0.6129 (0.4891)	
training:	Epoch: [71][6/204]	Loss 0.3220 (0.4613)	
training:	Epoch: [71][7/204]	Loss 0.5078 (0.4679)	
training:	Epoch: [71][8/204]	Loss 0.3925 (0.4585)	
training:	Epoch: [71][9/204]	Loss 0.5582 (0.4696)	
training:	Epoch: [71][10/204]	Loss 0.4612 (0.4687)	
training:	Epoch: [71][11/204]	Loss 0.4408 (0.4662)	
training:	Epoch: [71][12/204]	Loss 0.3790 (0.4589)	
training:	Epoch: [71][13/204]	Loss 0.4570 (0.4588)	
training:	Epoch: [71][14/204]	Loss 0.3432 (0.4505)	
training:	Epoch: [71][15/204]	Loss 0.5517 (0.4573)	
training:	Epoch: [71][16/204]	Loss 0.5141 (0.4608)	
training:	Epoch: [71][17/204]	Loss 0.5243 (0.4645)	
training:	Epoch: [71][18/204]	Loss 0.6039 (0.4723)	
training:	Epoch: [71][19/204]	Loss 0.5327 (0.4755)	
training:	Epoch: [71][20/204]	Loss 0.4165 (0.4725)	
training:	Epoch: [71][21/204]	Loss 0.4494 (0.4714)	
training:	Epoch: [71][22/204]	Loss 0.5578 (0.4754)	
training:	Epoch: [71][23/204]	Loss 0.4312 (0.4734)	
training:	Epoch: [71][24/204]	Loss 0.3620 (0.4688)	
training:	Epoch: [71][25/204]	Loss 0.4544 (0.4682)	
training:	Epoch: [71][26/204]	Loss 0.4618 (0.4680)	
training:	Epoch: [71][27/204]	Loss 0.4832 (0.4685)	
training:	Epoch: [71][28/204]	Loss 0.5054 (0.4698)	
training:	Epoch: [71][29/204]	Loss 0.3974 (0.4673)	
training:	Epoch: [71][30/204]	Loss 0.3758 (0.4643)	
training:	Epoch: [71][31/204]	Loss 0.5731 (0.4678)	
training:	Epoch: [71][32/204]	Loss 0.3320 (0.4636)	
training:	Epoch: [71][33/204]	Loss 0.5271 (0.4655)	
training:	Epoch: [71][34/204]	Loss 0.4125 (0.4639)	
training:	Epoch: [71][35/204]	Loss 0.4294 (0.4629)	
training:	Epoch: [71][36/204]	Loss 0.2914 (0.4582)	
training:	Epoch: [71][37/204]	Loss 0.4469 (0.4579)	
training:	Epoch: [71][38/204]	Loss 0.4862 (0.4586)	
training:	Epoch: [71][39/204]	Loss 0.5121 (0.4600)	
training:	Epoch: [71][40/204]	Loss 0.5127 (0.4613)	
training:	Epoch: [71][41/204]	Loss 0.5156 (0.4626)	
training:	Epoch: [71][42/204]	Loss 0.5469 (0.4646)	
training:	Epoch: [71][43/204]	Loss 0.4103 (0.4634)	
training:	Epoch: [71][44/204]	Loss 0.3781 (0.4614)	
training:	Epoch: [71][45/204]	Loss 0.5380 (0.4631)	
training:	Epoch: [71][46/204]	Loss 0.3731 (0.4612)	
training:	Epoch: [71][47/204]	Loss 0.3541 (0.4589)	
training:	Epoch: [71][48/204]	Loss 0.4390 (0.4585)	
training:	Epoch: [71][49/204]	Loss 0.5020 (0.4594)	
training:	Epoch: [71][50/204]	Loss 0.4990 (0.4602)	
training:	Epoch: [71][51/204]	Loss 0.4322 (0.4596)	
training:	Epoch: [71][52/204]	Loss 0.3590 (0.4577)	
training:	Epoch: [71][53/204]	Loss 0.5701 (0.4598)	
training:	Epoch: [71][54/204]	Loss 0.3494 (0.4578)	
training:	Epoch: [71][55/204]	Loss 0.5006 (0.4585)	
training:	Epoch: [71][56/204]	Loss 0.3210 (0.4561)	
training:	Epoch: [71][57/204]	Loss 0.4663 (0.4563)	
training:	Epoch: [71][58/204]	Loss 0.4505 (0.4562)	
training:	Epoch: [71][59/204]	Loss 0.4767 (0.4565)	
training:	Epoch: [71][60/204]	Loss 0.3711 (0.4551)	
training:	Epoch: [71][61/204]	Loss 0.4788 (0.4555)	
training:	Epoch: [71][62/204]	Loss 0.5590 (0.4572)	
training:	Epoch: [71][63/204]	Loss 0.3233 (0.4550)	
training:	Epoch: [71][64/204]	Loss 0.3211 (0.4529)	
training:	Epoch: [71][65/204]	Loss 0.5063 (0.4538)	
training:	Epoch: [71][66/204]	Loss 0.6390 (0.4566)	
training:	Epoch: [71][67/204]	Loss 0.3263 (0.4546)	
training:	Epoch: [71][68/204]	Loss 0.5618 (0.4562)	
training:	Epoch: [71][69/204]	Loss 0.3853 (0.4552)	
training:	Epoch: [71][70/204]	Loss 0.3328 (0.4534)	
training:	Epoch: [71][71/204]	Loss 0.4277 (0.4531)	
training:	Epoch: [71][72/204]	Loss 0.2987 (0.4509)	
training:	Epoch: [71][73/204]	Loss 0.4832 (0.4514)	
training:	Epoch: [71][74/204]	Loss 0.4371 (0.4512)	
training:	Epoch: [71][75/204]	Loss 0.5399 (0.4523)	
training:	Epoch: [71][76/204]	Loss 0.5250 (0.4533)	
training:	Epoch: [71][77/204]	Loss 0.4858 (0.4537)	
training:	Epoch: [71][78/204]	Loss 0.4805 (0.4541)	
training:	Epoch: [71][79/204]	Loss 0.5674 (0.4555)	
training:	Epoch: [71][80/204]	Loss 0.4577 (0.4555)	
training:	Epoch: [71][81/204]	Loss 0.5557 (0.4568)	
training:	Epoch: [71][82/204]	Loss 0.5469 (0.4579)	
training:	Epoch: [71][83/204]	Loss 0.4451 (0.4577)	
training:	Epoch: [71][84/204]	Loss 0.4623 (0.4578)	
training:	Epoch: [71][85/204]	Loss 0.4106 (0.4572)	
training:	Epoch: [71][86/204]	Loss 0.4600 (0.4572)	
training:	Epoch: [71][87/204]	Loss 0.4833 (0.4575)	
training:	Epoch: [71][88/204]	Loss 0.3578 (0.4564)	
training:	Epoch: [71][89/204]	Loss 0.4053 (0.4558)	
training:	Epoch: [71][90/204]	Loss 0.3447 (0.4546)	
training:	Epoch: [71][91/204]	Loss 0.4653 (0.4547)	
training:	Epoch: [71][92/204]	Loss 0.4551 (0.4547)	
training:	Epoch: [71][93/204]	Loss 0.4900 (0.4551)	
training:	Epoch: [71][94/204]	Loss 0.4650 (0.4552)	
training:	Epoch: [71][95/204]	Loss 0.4018 (0.4546)	
training:	Epoch: [71][96/204]	Loss 0.3652 (0.4537)	
training:	Epoch: [71][97/204]	Loss 0.4429 (0.4536)	
training:	Epoch: [71][98/204]	Loss 0.5150 (0.4542)	
training:	Epoch: [71][99/204]	Loss 0.4706 (0.4544)	
training:	Epoch: [71][100/204]	Loss 0.4720 (0.4546)	
training:	Epoch: [71][101/204]	Loss 0.6298 (0.4563)	
training:	Epoch: [71][102/204]	Loss 0.5045 (0.4568)	
training:	Epoch: [71][103/204]	Loss 0.5725 (0.4579)	
training:	Epoch: [71][104/204]	Loss 0.4049 (0.4574)	
training:	Epoch: [71][105/204]	Loss 0.3859 (0.4567)	
training:	Epoch: [71][106/204]	Loss 0.6191 (0.4582)	
training:	Epoch: [71][107/204]	Loss 0.4099 (0.4578)	
training:	Epoch: [71][108/204]	Loss 0.5187 (0.4584)	
training:	Epoch: [71][109/204]	Loss 0.4453 (0.4582)	
training:	Epoch: [71][110/204]	Loss 0.5866 (0.4594)	
training:	Epoch: [71][111/204]	Loss 0.4718 (0.4595)	
training:	Epoch: [71][112/204]	Loss 0.4140 (0.4591)	
training:	Epoch: [71][113/204]	Loss 0.6231 (0.4606)	
training:	Epoch: [71][114/204]	Loss 0.4146 (0.4602)	
training:	Epoch: [71][115/204]	Loss 0.3724 (0.4594)	
training:	Epoch: [71][116/204]	Loss 0.4976 (0.4597)	
training:	Epoch: [71][117/204]	Loss 0.4705 (0.4598)	
training:	Epoch: [71][118/204]	Loss 0.4325 (0.4596)	
training:	Epoch: [71][119/204]	Loss 0.4143 (0.4592)	
training:	Epoch: [71][120/204]	Loss 0.4514 (0.4591)	
training:	Epoch: [71][121/204]	Loss 0.5371 (0.4598)	
training:	Epoch: [71][122/204]	Loss 0.4729 (0.4599)	
training:	Epoch: [71][123/204]	Loss 0.4833 (0.4601)	
training:	Epoch: [71][124/204]	Loss 0.4541 (0.4600)	
training:	Epoch: [71][125/204]	Loss 0.4403 (0.4599)	
training:	Epoch: [71][126/204]	Loss 0.5606 (0.4607)	
training:	Epoch: [71][127/204]	Loss 0.4455 (0.4606)	
training:	Epoch: [71][128/204]	Loss 0.4337 (0.4603)	
training:	Epoch: [71][129/204]	Loss 0.4366 (0.4602)	
training:	Epoch: [71][130/204]	Loss 0.3302 (0.4592)	
training:	Epoch: [71][131/204]	Loss 0.5131 (0.4596)	
training:	Epoch: [71][132/204]	Loss 0.3750 (0.4589)	
training:	Epoch: [71][133/204]	Loss 0.3522 (0.4581)	
training:	Epoch: [71][134/204]	Loss 0.3976 (0.4577)	
training:	Epoch: [71][135/204]	Loss 0.4299 (0.4575)	
training:	Epoch: [71][136/204]	Loss 0.5069 (0.4578)	
training:	Epoch: [71][137/204]	Loss 0.5538 (0.4585)	
training:	Epoch: [71][138/204]	Loss 0.3464 (0.4577)	
training:	Epoch: [71][139/204]	Loss 0.4429 (0.4576)	
training:	Epoch: [71][140/204]	Loss 0.3122 (0.4566)	
training:	Epoch: [71][141/204]	Loss 0.4870 (0.4568)	
training:	Epoch: [71][142/204]	Loss 0.3835 (0.4563)	
training:	Epoch: [71][143/204]	Loss 0.7121 (0.4581)	
training:	Epoch: [71][144/204]	Loss 0.5723 (0.4589)	
training:	Epoch: [71][145/204]	Loss 0.3003 (0.4578)	
training:	Epoch: [71][146/204]	Loss 0.3447 (0.4570)	
training:	Epoch: [71][147/204]	Loss 0.4432 (0.4569)	
training:	Epoch: [71][148/204]	Loss 0.5143 (0.4573)	
training:	Epoch: [71][149/204]	Loss 0.5019 (0.4576)	
training:	Epoch: [71][150/204]	Loss 0.5230 (0.4580)	
training:	Epoch: [71][151/204]	Loss 0.4398 (0.4579)	
training:	Epoch: [71][152/204]	Loss 0.4319 (0.4577)	
training:	Epoch: [71][153/204]	Loss 0.5283 (0.4582)	
training:	Epoch: [71][154/204]	Loss 0.4387 (0.4581)	
training:	Epoch: [71][155/204]	Loss 0.3031 (0.4571)	
training:	Epoch: [71][156/204]	Loss 0.4541 (0.4570)	
training:	Epoch: [71][157/204]	Loss 0.4916 (0.4573)	
training:	Epoch: [71][158/204]	Loss 0.3349 (0.4565)	
training:	Epoch: [71][159/204]	Loss 0.3248 (0.4557)	
training:	Epoch: [71][160/204]	Loss 0.4650 (0.4557)	
training:	Epoch: [71][161/204]	Loss 0.4131 (0.4555)	
training:	Epoch: [71][162/204]	Loss 0.4493 (0.4554)	
training:	Epoch: [71][163/204]	Loss 0.4879 (0.4556)	
training:	Epoch: [71][164/204]	Loss 0.5228 (0.4560)	
training:	Epoch: [71][165/204]	Loss 0.5374 (0.4565)	
training:	Epoch: [71][166/204]	Loss 0.5933 (0.4573)	
training:	Epoch: [71][167/204]	Loss 0.3730 (0.4568)	
training:	Epoch: [71][168/204]	Loss 0.5578 (0.4574)	
training:	Epoch: [71][169/204]	Loss 0.5791 (0.4582)	
training:	Epoch: [71][170/204]	Loss 0.5566 (0.4587)	
training:	Epoch: [71][171/204]	Loss 0.3214 (0.4579)	
training:	Epoch: [71][172/204]	Loss 0.4561 (0.4579)	
training:	Epoch: [71][173/204]	Loss 0.5123 (0.4582)	
training:	Epoch: [71][174/204]	Loss 0.4081 (0.4580)	
training:	Epoch: [71][175/204]	Loss 0.4402 (0.4578)	
training:	Epoch: [71][176/204]	Loss 0.4737 (0.4579)	
training:	Epoch: [71][177/204]	Loss 0.4685 (0.4580)	
training:	Epoch: [71][178/204]	Loss 0.5704 (0.4586)	
training:	Epoch: [71][179/204]	Loss 0.5382 (0.4591)	
training:	Epoch: [71][180/204]	Loss 0.4895 (0.4592)	
training:	Epoch: [71][181/204]	Loss 0.5245 (0.4596)	
training:	Epoch: [71][182/204]	Loss 0.4800 (0.4597)	
training:	Epoch: [71][183/204]	Loss 0.4885 (0.4599)	
training:	Epoch: [71][184/204]	Loss 0.3310 (0.4592)	
training:	Epoch: [71][185/204]	Loss 0.4714 (0.4592)	
training:	Epoch: [71][186/204]	Loss 0.5244 (0.4596)	
training:	Epoch: [71][187/204]	Loss 0.3654 (0.4591)	
training:	Epoch: [71][188/204]	Loss 0.4459 (0.4590)	
training:	Epoch: [71][189/204]	Loss 0.2642 (0.4580)	
training:	Epoch: [71][190/204]	Loss 0.4201 (0.4578)	
training:	Epoch: [71][191/204]	Loss 0.4177 (0.4576)	
training:	Epoch: [71][192/204]	Loss 0.4541 (0.4576)	
training:	Epoch: [71][193/204]	Loss 0.4825 (0.4577)	
training:	Epoch: [71][194/204]	Loss 0.4500 (0.4576)	
training:	Epoch: [71][195/204]	Loss 0.7038 (0.4589)	
training:	Epoch: [71][196/204]	Loss 0.5460 (0.4594)	
training:	Epoch: [71][197/204]	Loss 0.4847 (0.4595)	
training:	Epoch: [71][198/204]	Loss 0.3718 (0.4590)	
training:	Epoch: [71][199/204]	Loss 0.6311 (0.4599)	
training:	Epoch: [71][200/204]	Loss 0.3086 (0.4591)	
training:	Epoch: [71][201/204]	Loss 0.4856 (0.4593)	
training:	Epoch: [71][202/204]	Loss 0.4199 (0.4591)	
training:	Epoch: [71][203/204]	Loss 0.3422 (0.4585)	
training:	Epoch: [71][204/204]	Loss 0.4722 (0.4586)	
Training:	 Loss: 0.4579

Training:	 ACC: 0.7995 0.7998 0.8069 0.7921
Validation:	 ACC: 0.7864 0.7871 0.8004 0.7724
Validation:	 Best_BACC: 0.7864 0.7871 0.8004 0.7724
Validation:	 Loss: 0.4677
Pretraining:	Epoch 72/120
----------
training:	Epoch: [72][1/204]	Loss 0.4401 (0.4401)	
training:	Epoch: [72][2/204]	Loss 0.4577 (0.4489)	
training:	Epoch: [72][3/204]	Loss 0.5333 (0.4770)	
training:	Epoch: [72][4/204]	Loss 0.5407 (0.4930)	
training:	Epoch: [72][5/204]	Loss 0.5113 (0.4966)	
training:	Epoch: [72][6/204]	Loss 0.4507 (0.4890)	
training:	Epoch: [72][7/204]	Loss 0.3294 (0.4662)	
training:	Epoch: [72][8/204]	Loss 0.4653 (0.4661)	
training:	Epoch: [72][9/204]	Loss 0.4196 (0.4609)	
training:	Epoch: [72][10/204]	Loss 0.5400 (0.4688)	
training:	Epoch: [72][11/204]	Loss 0.5294 (0.4743)	
training:	Epoch: [72][12/204]	Loss 0.3404 (0.4632)	
training:	Epoch: [72][13/204]	Loss 0.4074 (0.4589)	
training:	Epoch: [72][14/204]	Loss 0.3624 (0.4520)	
training:	Epoch: [72][15/204]	Loss 0.4344 (0.4508)	
training:	Epoch: [72][16/204]	Loss 0.4740 (0.4523)	
training:	Epoch: [72][17/204]	Loss 0.3767 (0.4478)	
training:	Epoch: [72][18/204]	Loss 0.3719 (0.4436)	
training:	Epoch: [72][19/204]	Loss 0.4297 (0.4429)	
training:	Epoch: [72][20/204]	Loss 0.4935 (0.4454)	
training:	Epoch: [72][21/204]	Loss 0.4123 (0.4438)	
training:	Epoch: [72][22/204]	Loss 0.4768 (0.4453)	
training:	Epoch: [72][23/204]	Loss 0.4311 (0.4447)	
training:	Epoch: [72][24/204]	Loss 0.4608 (0.4454)	
training:	Epoch: [72][25/204]	Loss 0.5726 (0.4505)	
training:	Epoch: [72][26/204]	Loss 0.5115 (0.4528)	
training:	Epoch: [72][27/204]	Loss 0.4769 (0.4537)	
training:	Epoch: [72][28/204]	Loss 0.4410 (0.4532)	
training:	Epoch: [72][29/204]	Loss 0.4025 (0.4515)	
training:	Epoch: [72][30/204]	Loss 0.4094 (0.4501)	
training:	Epoch: [72][31/204]	Loss 0.5123 (0.4521)	
training:	Epoch: [72][32/204]	Loss 0.4940 (0.4534)	
training:	Epoch: [72][33/204]	Loss 0.5463 (0.4562)	
training:	Epoch: [72][34/204]	Loss 0.3769 (0.4539)	
training:	Epoch: [72][35/204]	Loss 0.5398 (0.4563)	
training:	Epoch: [72][36/204]	Loss 0.4424 (0.4560)	
training:	Epoch: [72][37/204]	Loss 0.3753 (0.4538)	
training:	Epoch: [72][38/204]	Loss 0.5629 (0.4566)	
training:	Epoch: [72][39/204]	Loss 0.4478 (0.4564)	
training:	Epoch: [72][40/204]	Loss 0.4406 (0.4560)	
training:	Epoch: [72][41/204]	Loss 0.4985 (0.4571)	
training:	Epoch: [72][42/204]	Loss 0.3541 (0.4546)	
training:	Epoch: [72][43/204]	Loss 0.5177 (0.4561)	
training:	Epoch: [72][44/204]	Loss 0.4737 (0.4565)	
training:	Epoch: [72][45/204]	Loss 0.4433 (0.4562)	
training:	Epoch: [72][46/204]	Loss 0.3701 (0.4543)	
training:	Epoch: [72][47/204]	Loss 0.5631 (0.4566)	
training:	Epoch: [72][48/204]	Loss 0.4931 (0.4574)	
training:	Epoch: [72][49/204]	Loss 0.3452 (0.4551)	
training:	Epoch: [72][50/204]	Loss 0.3944 (0.4539)	
training:	Epoch: [72][51/204]	Loss 0.5841 (0.4564)	
training:	Epoch: [72][52/204]	Loss 0.5159 (0.4576)	
training:	Epoch: [72][53/204]	Loss 0.4137 (0.4568)	
training:	Epoch: [72][54/204]	Loss 0.2936 (0.4537)	
training:	Epoch: [72][55/204]	Loss 0.3699 (0.4522)	
training:	Epoch: [72][56/204]	Loss 0.5187 (0.4534)	
training:	Epoch: [72][57/204]	Loss 0.4323 (0.4530)	
training:	Epoch: [72][58/204]	Loss 0.3699 (0.4516)	
training:	Epoch: [72][59/204]	Loss 0.5662 (0.4535)	
training:	Epoch: [72][60/204]	Loss 0.6825 (0.4574)	
training:	Epoch: [72][61/204]	Loss 0.3512 (0.4556)	
training:	Epoch: [72][62/204]	Loss 0.4263 (0.4551)	
training:	Epoch: [72][63/204]	Loss 0.5710 (0.4570)	
training:	Epoch: [72][64/204]	Loss 0.5153 (0.4579)	
training:	Epoch: [72][65/204]	Loss 0.5193 (0.4588)	
training:	Epoch: [72][66/204]	Loss 0.6659 (0.4620)	
training:	Epoch: [72][67/204]	Loss 0.6585 (0.4649)	
training:	Epoch: [72][68/204]	Loss 0.3934 (0.4639)	
training:	Epoch: [72][69/204]	Loss 0.4833 (0.4641)	
training:	Epoch: [72][70/204]	Loss 0.4369 (0.4637)	
training:	Epoch: [72][71/204]	Loss 0.5608 (0.4651)	
training:	Epoch: [72][72/204]	Loss 0.4632 (0.4651)	
training:	Epoch: [72][73/204]	Loss 0.3552 (0.4636)	
training:	Epoch: [72][74/204]	Loss 0.5225 (0.4644)	
training:	Epoch: [72][75/204]	Loss 0.3668 (0.4631)	
training:	Epoch: [72][76/204]	Loss 0.3300 (0.4613)	
training:	Epoch: [72][77/204]	Loss 0.3291 (0.4596)	
training:	Epoch: [72][78/204]	Loss 0.4924 (0.4600)	
training:	Epoch: [72][79/204]	Loss 0.4214 (0.4595)	
training:	Epoch: [72][80/204]	Loss 0.5290 (0.4604)	
training:	Epoch: [72][81/204]	Loss 0.5181 (0.4611)	
training:	Epoch: [72][82/204]	Loss 0.4553 (0.4610)	
training:	Epoch: [72][83/204]	Loss 0.4391 (0.4608)	
training:	Epoch: [72][84/204]	Loss 0.4420 (0.4606)	
training:	Epoch: [72][85/204]	Loss 0.6075 (0.4623)	
training:	Epoch: [72][86/204]	Loss 0.4567 (0.4622)	
training:	Epoch: [72][87/204]	Loss 0.4074 (0.4616)	
training:	Epoch: [72][88/204]	Loss 0.4748 (0.4617)	
training:	Epoch: [72][89/204]	Loss 0.5390 (0.4626)	
training:	Epoch: [72][90/204]	Loss 0.4955 (0.4630)	
training:	Epoch: [72][91/204]	Loss 0.4106 (0.4624)	
training:	Epoch: [72][92/204]	Loss 0.4624 (0.4624)	
training:	Epoch: [72][93/204]	Loss 0.4875 (0.4627)	
training:	Epoch: [72][94/204]	Loss 0.6810 (0.4650)	
training:	Epoch: [72][95/204]	Loss 0.4315 (0.4646)	
training:	Epoch: [72][96/204]	Loss 0.4391 (0.4644)	
training:	Epoch: [72][97/204]	Loss 0.5412 (0.4652)	
training:	Epoch: [72][98/204]	Loss 0.5332 (0.4659)	
training:	Epoch: [72][99/204]	Loss 0.3356 (0.4645)	
training:	Epoch: [72][100/204]	Loss 0.4232 (0.4641)	
training:	Epoch: [72][101/204]	Loss 0.2947 (0.4625)	
training:	Epoch: [72][102/204]	Loss 0.4083 (0.4619)	
training:	Epoch: [72][103/204]	Loss 0.4352 (0.4617)	
training:	Epoch: [72][104/204]	Loss 0.3949 (0.4610)	
training:	Epoch: [72][105/204]	Loss 0.4884 (0.4613)	
training:	Epoch: [72][106/204]	Loss 0.4813 (0.4615)	
training:	Epoch: [72][107/204]	Loss 0.4701 (0.4616)	
training:	Epoch: [72][108/204]	Loss 0.3508 (0.4605)	
training:	Epoch: [72][109/204]	Loss 0.3837 (0.4598)	
training:	Epoch: [72][110/204]	Loss 0.4302 (0.4596)	
training:	Epoch: [72][111/204]	Loss 0.6359 (0.4611)	
training:	Epoch: [72][112/204]	Loss 0.5275 (0.4617)	
training:	Epoch: [72][113/204]	Loss 0.4623 (0.4617)	
training:	Epoch: [72][114/204]	Loss 0.3439 (0.4607)	
training:	Epoch: [72][115/204]	Loss 0.5907 (0.4618)	
training:	Epoch: [72][116/204]	Loss 0.4886 (0.4621)	
training:	Epoch: [72][117/204]	Loss 0.3925 (0.4615)	
training:	Epoch: [72][118/204]	Loss 0.4469 (0.4613)	
training:	Epoch: [72][119/204]	Loss 0.4250 (0.4610)	
training:	Epoch: [72][120/204]	Loss 0.4557 (0.4610)	
training:	Epoch: [72][121/204]	Loss 0.4419 (0.4608)	
training:	Epoch: [72][122/204]	Loss 0.4735 (0.4609)	
training:	Epoch: [72][123/204]	Loss 0.4511 (0.4609)	
training:	Epoch: [72][124/204]	Loss 0.5228 (0.4614)	
training:	Epoch: [72][125/204]	Loss 0.4051 (0.4609)	
training:	Epoch: [72][126/204]	Loss 0.5320 (0.4615)	
training:	Epoch: [72][127/204]	Loss 0.4571 (0.4614)	
training:	Epoch: [72][128/204]	Loss 0.4886 (0.4617)	
training:	Epoch: [72][129/204]	Loss 0.5980 (0.4627)	
training:	Epoch: [72][130/204]	Loss 0.3865 (0.4621)	
training:	Epoch: [72][131/204]	Loss 0.4430 (0.4620)	
training:	Epoch: [72][132/204]	Loss 0.4352 (0.4618)	
training:	Epoch: [72][133/204]	Loss 0.3124 (0.4607)	
training:	Epoch: [72][134/204]	Loss 0.3788 (0.4600)	
training:	Epoch: [72][135/204]	Loss 0.4556 (0.4600)	
training:	Epoch: [72][136/204]	Loss 0.2693 (0.4586)	
training:	Epoch: [72][137/204]	Loss 0.4504 (0.4585)	
training:	Epoch: [72][138/204]	Loss 0.2800 (0.4573)	
training:	Epoch: [72][139/204]	Loss 0.3906 (0.4568)	
training:	Epoch: [72][140/204]	Loss 0.4032 (0.4564)	
training:	Epoch: [72][141/204]	Loss 0.4103 (0.4561)	
training:	Epoch: [72][142/204]	Loss 0.5916 (0.4570)	
training:	Epoch: [72][143/204]	Loss 0.4403 (0.4569)	
training:	Epoch: [72][144/204]	Loss 0.4548 (0.4569)	
training:	Epoch: [72][145/204]	Loss 0.4223 (0.4567)	
training:	Epoch: [72][146/204]	Loss 0.4155 (0.4564)	
training:	Epoch: [72][147/204]	Loss 0.4957 (0.4566)	
training:	Epoch: [72][148/204]	Loss 0.2792 (0.4554)	
training:	Epoch: [72][149/204]	Loss 0.5771 (0.4563)	
training:	Epoch: [72][150/204]	Loss 0.3673 (0.4557)	
training:	Epoch: [72][151/204]	Loss 0.4974 (0.4559)	
training:	Epoch: [72][152/204]	Loss 0.3560 (0.4553)	
training:	Epoch: [72][153/204]	Loss 0.4948 (0.4555)	
training:	Epoch: [72][154/204]	Loss 0.5578 (0.4562)	
training:	Epoch: [72][155/204]	Loss 0.4047 (0.4559)	
training:	Epoch: [72][156/204]	Loss 0.5906 (0.4567)	
training:	Epoch: [72][157/204]	Loss 0.5623 (0.4574)	
training:	Epoch: [72][158/204]	Loss 0.3962 (0.4570)	
training:	Epoch: [72][159/204]	Loss 0.4943 (0.4573)	
training:	Epoch: [72][160/204]	Loss 0.6722 (0.4586)	
training:	Epoch: [72][161/204]	Loss 0.4470 (0.4585)	
training:	Epoch: [72][162/204]	Loss 0.4077 (0.4582)	
training:	Epoch: [72][163/204]	Loss 0.5081 (0.4585)	
training:	Epoch: [72][164/204]	Loss 0.3176 (0.4577)	
training:	Epoch: [72][165/204]	Loss 0.5139 (0.4580)	
training:	Epoch: [72][166/204]	Loss 0.3294 (0.4572)	
training:	Epoch: [72][167/204]	Loss 0.4956 (0.4575)	
training:	Epoch: [72][168/204]	Loss 0.5125 (0.4578)	
training:	Epoch: [72][169/204]	Loss 0.4604 (0.4578)	
training:	Epoch: [72][170/204]	Loss 0.5504 (0.4583)	
training:	Epoch: [72][171/204]	Loss 0.4165 (0.4581)	
training:	Epoch: [72][172/204]	Loss 0.4396 (0.4580)	
training:	Epoch: [72][173/204]	Loss 0.3045 (0.4571)	
training:	Epoch: [72][174/204]	Loss 0.3583 (0.4565)	
training:	Epoch: [72][175/204]	Loss 0.5260 (0.4569)	
training:	Epoch: [72][176/204]	Loss 0.5217 (0.4573)	
training:	Epoch: [72][177/204]	Loss 0.4483 (0.4572)	
training:	Epoch: [72][178/204]	Loss 0.5700 (0.4579)	
training:	Epoch: [72][179/204]	Loss 0.3222 (0.4571)	
training:	Epoch: [72][180/204]	Loss 0.5540 (0.4577)	
training:	Epoch: [72][181/204]	Loss 0.4023 (0.4574)	
training:	Epoch: [72][182/204]	Loss 0.5456 (0.4578)	
training:	Epoch: [72][183/204]	Loss 0.3329 (0.4572)	
training:	Epoch: [72][184/204]	Loss 0.6422 (0.4582)	
training:	Epoch: [72][185/204]	Loss 0.5573 (0.4587)	
training:	Epoch: [72][186/204]	Loss 0.5627 (0.4593)	
training:	Epoch: [72][187/204]	Loss 0.5685 (0.4598)	
training:	Epoch: [72][188/204]	Loss 0.2993 (0.4590)	
training:	Epoch: [72][189/204]	Loss 0.5304 (0.4594)	
training:	Epoch: [72][190/204]	Loss 0.3756 (0.4589)	
training:	Epoch: [72][191/204]	Loss 0.5939 (0.4596)	
training:	Epoch: [72][192/204]	Loss 0.3886 (0.4593)	
training:	Epoch: [72][193/204]	Loss 0.5193 (0.4596)	
training:	Epoch: [72][194/204]	Loss 0.3634 (0.4591)	
training:	Epoch: [72][195/204]	Loss 0.4873 (0.4592)	
training:	Epoch: [72][196/204]	Loss 0.5027 (0.4594)	
training:	Epoch: [72][197/204]	Loss 0.4715 (0.4595)	
training:	Epoch: [72][198/204]	Loss 0.5620 (0.4600)	
training:	Epoch: [72][199/204]	Loss 0.3912 (0.4597)	
training:	Epoch: [72][200/204]	Loss 0.4683 (0.4597)	
training:	Epoch: [72][201/204]	Loss 0.4268 (0.4596)	
training:	Epoch: [72][202/204]	Loss 0.4289 (0.4594)	
training:	Epoch: [72][203/204]	Loss 0.5538 (0.4599)	
training:	Epoch: [72][204/204]	Loss 0.3339 (0.4593)	
Training:	 Loss: 0.4586

Training:	 ACC: 0.8003 0.8012 0.8222 0.7784
Validation:	 ACC: 0.7859 0.7871 0.8106 0.7612
Validation:	 Best_BACC: 0.7864 0.7871 0.8004 0.7724
Validation:	 Loss: 0.4666
Pretraining:	Epoch 73/120
----------
training:	Epoch: [73][1/204]	Loss 0.3641 (0.3641)	
training:	Epoch: [73][2/204]	Loss 0.3773 (0.3707)	
training:	Epoch: [73][3/204]	Loss 0.4041 (0.3818)	
training:	Epoch: [73][4/204]	Loss 0.5688 (0.4286)	
training:	Epoch: [73][5/204]	Loss 0.4400 (0.4308)	
training:	Epoch: [73][6/204]	Loss 0.5696 (0.4540)	
training:	Epoch: [73][7/204]	Loss 0.5129 (0.4624)	
training:	Epoch: [73][8/204]	Loss 0.3771 (0.4517)	
training:	Epoch: [73][9/204]	Loss 0.5557 (0.4633)	
training:	Epoch: [73][10/204]	Loss 0.5634 (0.4733)	
training:	Epoch: [73][11/204]	Loss 0.3939 (0.4661)	
training:	Epoch: [73][12/204]	Loss 0.3781 (0.4587)	
training:	Epoch: [73][13/204]	Loss 0.4091 (0.4549)	
training:	Epoch: [73][14/204]	Loss 0.5249 (0.4599)	
training:	Epoch: [73][15/204]	Loss 0.3210 (0.4507)	
training:	Epoch: [73][16/204]	Loss 0.4769 (0.4523)	
training:	Epoch: [73][17/204]	Loss 0.4538 (0.4524)	
training:	Epoch: [73][18/204]	Loss 0.4956 (0.4548)	
training:	Epoch: [73][19/204]	Loss 0.4717 (0.4557)	
training:	Epoch: [73][20/204]	Loss 0.4745 (0.4566)	
training:	Epoch: [73][21/204]	Loss 0.4668 (0.4571)	
training:	Epoch: [73][22/204]	Loss 0.2956 (0.4498)	
training:	Epoch: [73][23/204]	Loss 0.4389 (0.4493)	
training:	Epoch: [73][24/204]	Loss 0.3797 (0.4464)	
training:	Epoch: [73][25/204]	Loss 0.3584 (0.4429)	
training:	Epoch: [73][26/204]	Loss 0.5236 (0.4460)	
training:	Epoch: [73][27/204]	Loss 0.4922 (0.4477)	
training:	Epoch: [73][28/204]	Loss 0.5150 (0.4501)	
training:	Epoch: [73][29/204]	Loss 0.4186 (0.4490)	
training:	Epoch: [73][30/204]	Loss 0.3783 (0.4467)	
training:	Epoch: [73][31/204]	Loss 0.4682 (0.4473)	
training:	Epoch: [73][32/204]	Loss 0.3903 (0.4456)	
training:	Epoch: [73][33/204]	Loss 0.4537 (0.4458)	
training:	Epoch: [73][34/204]	Loss 0.3435 (0.4428)	
training:	Epoch: [73][35/204]	Loss 0.2427 (0.4371)	
training:	Epoch: [73][36/204]	Loss 0.4396 (0.4372)	
training:	Epoch: [73][37/204]	Loss 0.4032 (0.4362)	
training:	Epoch: [73][38/204]	Loss 0.4141 (0.4357)	
training:	Epoch: [73][39/204]	Loss 0.4584 (0.4362)	
training:	Epoch: [73][40/204]	Loss 0.5150 (0.4382)	
training:	Epoch: [73][41/204]	Loss 0.5106 (0.4400)	
training:	Epoch: [73][42/204]	Loss 0.3034 (0.4367)	
training:	Epoch: [73][43/204]	Loss 0.7189 (0.4433)	
training:	Epoch: [73][44/204]	Loss 0.4408 (0.4432)	
training:	Epoch: [73][45/204]	Loss 0.5021 (0.4445)	
training:	Epoch: [73][46/204]	Loss 0.4550 (0.4448)	
training:	Epoch: [73][47/204]	Loss 0.6245 (0.4486)	
training:	Epoch: [73][48/204]	Loss 0.4450 (0.4485)	
training:	Epoch: [73][49/204]	Loss 0.4140 (0.4478)	
training:	Epoch: [73][50/204]	Loss 0.4498 (0.4478)	
training:	Epoch: [73][51/204]	Loss 0.4324 (0.4475)	
training:	Epoch: [73][52/204]	Loss 0.5232 (0.4490)	
training:	Epoch: [73][53/204]	Loss 0.4357 (0.4487)	
training:	Epoch: [73][54/204]	Loss 0.4610 (0.4490)	
training:	Epoch: [73][55/204]	Loss 0.3606 (0.4474)	
training:	Epoch: [73][56/204]	Loss 0.3918 (0.4464)	
training:	Epoch: [73][57/204]	Loss 0.4772 (0.4469)	
training:	Epoch: [73][58/204]	Loss 0.3854 (0.4459)	
training:	Epoch: [73][59/204]	Loss 0.4389 (0.4457)	
training:	Epoch: [73][60/204]	Loss 0.4166 (0.4453)	
training:	Epoch: [73][61/204]	Loss 0.4348 (0.4451)	
training:	Epoch: [73][62/204]	Loss 0.4159 (0.4446)	
training:	Epoch: [73][63/204]	Loss 0.4659 (0.4449)	
training:	Epoch: [73][64/204]	Loss 0.5614 (0.4468)	
training:	Epoch: [73][65/204]	Loss 0.4853 (0.4474)	
training:	Epoch: [73][66/204]	Loss 0.4625 (0.4476)	
training:	Epoch: [73][67/204]	Loss 0.4254 (0.4473)	
training:	Epoch: [73][68/204]	Loss 0.6025 (0.4495)	
training:	Epoch: [73][69/204]	Loss 0.4548 (0.4496)	
training:	Epoch: [73][70/204]	Loss 0.5061 (0.4504)	
training:	Epoch: [73][71/204]	Loss 0.3644 (0.4492)	
training:	Epoch: [73][72/204]	Loss 0.3201 (0.4474)	
training:	Epoch: [73][73/204]	Loss 0.5233 (0.4485)	
training:	Epoch: [73][74/204]	Loss 0.3867 (0.4476)	
training:	Epoch: [73][75/204]	Loss 0.4070 (0.4471)	
training:	Epoch: [73][76/204]	Loss 0.3982 (0.4464)	
training:	Epoch: [73][77/204]	Loss 0.5138 (0.4473)	
training:	Epoch: [73][78/204]	Loss 0.4656 (0.4476)	
training:	Epoch: [73][79/204]	Loss 0.4302 (0.4473)	
training:	Epoch: [73][80/204]	Loss 0.3342 (0.4459)	
training:	Epoch: [73][81/204]	Loss 0.4477 (0.4459)	
training:	Epoch: [73][82/204]	Loss 0.4901 (0.4465)	
training:	Epoch: [73][83/204]	Loss 0.5536 (0.4478)	
training:	Epoch: [73][84/204]	Loss 0.3967 (0.4472)	
training:	Epoch: [73][85/204]	Loss 0.3979 (0.4466)	
training:	Epoch: [73][86/204]	Loss 0.5622 (0.4479)	
training:	Epoch: [73][87/204]	Loss 0.5377 (0.4490)	
training:	Epoch: [73][88/204]	Loss 0.4173 (0.4486)	
training:	Epoch: [73][89/204]	Loss 0.5101 (0.4493)	
training:	Epoch: [73][90/204]	Loss 0.4842 (0.4497)	
training:	Epoch: [73][91/204]	Loss 0.4972 (0.4502)	
training:	Epoch: [73][92/204]	Loss 0.5006 (0.4507)	
training:	Epoch: [73][93/204]	Loss 0.3492 (0.4497)	
training:	Epoch: [73][94/204]	Loss 0.4548 (0.4497)	
training:	Epoch: [73][95/204]	Loss 0.4535 (0.4497)	
training:	Epoch: [73][96/204]	Loss 0.5572 (0.4509)	
training:	Epoch: [73][97/204]	Loss 0.5967 (0.4524)	
training:	Epoch: [73][98/204]	Loss 0.3722 (0.4516)	
training:	Epoch: [73][99/204]	Loss 0.5325 (0.4524)	
training:	Epoch: [73][100/204]	Loss 0.4708 (0.4526)	
training:	Epoch: [73][101/204]	Loss 0.4156 (0.4522)	
training:	Epoch: [73][102/204]	Loss 0.4944 (0.4526)	
training:	Epoch: [73][103/204]	Loss 0.3808 (0.4519)	
training:	Epoch: [73][104/204]	Loss 0.3831 (0.4512)	
training:	Epoch: [73][105/204]	Loss 0.5348 (0.4520)	
training:	Epoch: [73][106/204]	Loss 0.5698 (0.4532)	
training:	Epoch: [73][107/204]	Loss 0.3400 (0.4521)	
training:	Epoch: [73][108/204]	Loss 0.2806 (0.4505)	
training:	Epoch: [73][109/204]	Loss 0.4611 (0.4506)	
training:	Epoch: [73][110/204]	Loss 0.4196 (0.4503)	
training:	Epoch: [73][111/204]	Loss 0.5083 (0.4508)	
training:	Epoch: [73][112/204]	Loss 0.3756 (0.4502)	
training:	Epoch: [73][113/204]	Loss 0.6824 (0.4522)	
training:	Epoch: [73][114/204]	Loss 0.3969 (0.4517)	
training:	Epoch: [73][115/204]	Loss 0.5637 (0.4527)	
training:	Epoch: [73][116/204]	Loss 0.5371 (0.4534)	
training:	Epoch: [73][117/204]	Loss 0.4848 (0.4537)	
training:	Epoch: [73][118/204]	Loss 0.4621 (0.4538)	
training:	Epoch: [73][119/204]	Loss 0.4574 (0.4538)	
training:	Epoch: [73][120/204]	Loss 0.3052 (0.4526)	
training:	Epoch: [73][121/204]	Loss 0.4084 (0.4522)	
training:	Epoch: [73][122/204]	Loss 0.4528 (0.4522)	
training:	Epoch: [73][123/204]	Loss 0.3977 (0.4518)	
training:	Epoch: [73][124/204]	Loss 0.5205 (0.4523)	
training:	Epoch: [73][125/204]	Loss 0.5020 (0.4527)	
training:	Epoch: [73][126/204]	Loss 0.5160 (0.4532)	
training:	Epoch: [73][127/204]	Loss 0.4711 (0.4534)	
training:	Epoch: [73][128/204]	Loss 0.5847 (0.4544)	
training:	Epoch: [73][129/204]	Loss 0.4428 (0.4543)	
training:	Epoch: [73][130/204]	Loss 0.4804 (0.4545)	
training:	Epoch: [73][131/204]	Loss 0.4051 (0.4541)	
training:	Epoch: [73][132/204]	Loss 0.4326 (0.4540)	
training:	Epoch: [73][133/204]	Loss 0.4354 (0.4538)	
training:	Epoch: [73][134/204]	Loss 0.3347 (0.4529)	
training:	Epoch: [73][135/204]	Loss 0.4351 (0.4528)	
training:	Epoch: [73][136/204]	Loss 0.4536 (0.4528)	
training:	Epoch: [73][137/204]	Loss 0.5362 (0.4534)	
training:	Epoch: [73][138/204]	Loss 0.3767 (0.4529)	
training:	Epoch: [73][139/204]	Loss 0.3304 (0.4520)	
training:	Epoch: [73][140/204]	Loss 0.4227 (0.4518)	
training:	Epoch: [73][141/204]	Loss 0.3455 (0.4510)	
training:	Epoch: [73][142/204]	Loss 0.5059 (0.4514)	
training:	Epoch: [73][143/204]	Loss 0.3918 (0.4510)	
training:	Epoch: [73][144/204]	Loss 0.3206 (0.4501)	
training:	Epoch: [73][145/204]	Loss 0.4851 (0.4503)	
training:	Epoch: [73][146/204]	Loss 0.4182 (0.4501)	
training:	Epoch: [73][147/204]	Loss 0.6590 (0.4515)	
training:	Epoch: [73][148/204]	Loss 0.3988 (0.4512)	
training:	Epoch: [73][149/204]	Loss 0.4612 (0.4512)	
training:	Epoch: [73][150/204]	Loss 0.3869 (0.4508)	
training:	Epoch: [73][151/204]	Loss 0.4735 (0.4510)	
training:	Epoch: [73][152/204]	Loss 0.3591 (0.4504)	
training:	Epoch: [73][153/204]	Loss 0.4473 (0.4503)	
training:	Epoch: [73][154/204]	Loss 0.5284 (0.4508)	
training:	Epoch: [73][155/204]	Loss 0.5637 (0.4516)	
training:	Epoch: [73][156/204]	Loss 0.5521 (0.4522)	
training:	Epoch: [73][157/204]	Loss 0.5233 (0.4527)	
training:	Epoch: [73][158/204]	Loss 0.4514 (0.4527)	
training:	Epoch: [73][159/204]	Loss 0.4648 (0.4527)	
training:	Epoch: [73][160/204]	Loss 0.5681 (0.4535)	
training:	Epoch: [73][161/204]	Loss 0.5438 (0.4540)	
training:	Epoch: [73][162/204]	Loss 0.5431 (0.4546)	
training:	Epoch: [73][163/204]	Loss 0.4131 (0.4543)	
training:	Epoch: [73][164/204]	Loss 0.4702 (0.4544)	
training:	Epoch: [73][165/204]	Loss 0.4248 (0.4542)	
training:	Epoch: [73][166/204]	Loss 0.4150 (0.4540)	
training:	Epoch: [73][167/204]	Loss 0.4129 (0.4537)	
training:	Epoch: [73][168/204]	Loss 0.5257 (0.4542)	
training:	Epoch: [73][169/204]	Loss 0.4083 (0.4539)	
training:	Epoch: [73][170/204]	Loss 0.3987 (0.4536)	
training:	Epoch: [73][171/204]	Loss 0.4247 (0.4534)	
training:	Epoch: [73][172/204]	Loss 0.5479 (0.4540)	
training:	Epoch: [73][173/204]	Loss 0.4471 (0.4539)	
training:	Epoch: [73][174/204]	Loss 0.5110 (0.4542)	
training:	Epoch: [73][175/204]	Loss 0.6065 (0.4551)	
training:	Epoch: [73][176/204]	Loss 0.4454 (0.4551)	
training:	Epoch: [73][177/204]	Loss 0.6053 (0.4559)	
training:	Epoch: [73][178/204]	Loss 0.5394 (0.4564)	
training:	Epoch: [73][179/204]	Loss 0.4340 (0.4563)	
training:	Epoch: [73][180/204]	Loss 0.6197 (0.4572)	
training:	Epoch: [73][181/204]	Loss 0.3970 (0.4568)	
training:	Epoch: [73][182/204]	Loss 0.4400 (0.4567)	
training:	Epoch: [73][183/204]	Loss 0.5552 (0.4573)	
training:	Epoch: [73][184/204]	Loss 0.3853 (0.4569)	
training:	Epoch: [73][185/204]	Loss 0.4299 (0.4567)	
training:	Epoch: [73][186/204]	Loss 0.3929 (0.4564)	
training:	Epoch: [73][187/204]	Loss 0.4045 (0.4561)	
training:	Epoch: [73][188/204]	Loss 0.5308 (0.4565)	
training:	Epoch: [73][189/204]	Loss 0.4900 (0.4567)	
training:	Epoch: [73][190/204]	Loss 0.4566 (0.4567)	
training:	Epoch: [73][191/204]	Loss 0.4334 (0.4566)	
training:	Epoch: [73][192/204]	Loss 0.6382 (0.4575)	
training:	Epoch: [73][193/204]	Loss 0.4269 (0.4574)	
training:	Epoch: [73][194/204]	Loss 0.3666 (0.4569)	
training:	Epoch: [73][195/204]	Loss 0.4191 (0.4567)	
training:	Epoch: [73][196/204]	Loss 0.5957 (0.4574)	
training:	Epoch: [73][197/204]	Loss 0.4122 (0.4572)	
training:	Epoch: [73][198/204]	Loss 0.3248 (0.4565)	
training:	Epoch: [73][199/204]	Loss 0.3582 (0.4560)	
training:	Epoch: [73][200/204]	Loss 0.5229 (0.4563)	
training:	Epoch: [73][201/204]	Loss 0.3683 (0.4559)	
training:	Epoch: [73][202/204]	Loss 0.4535 (0.4559)	
training:	Epoch: [73][203/204]	Loss 0.4601 (0.4559)	
training:	Epoch: [73][204/204]	Loss 0.4682 (0.4560)	
Training:	 Loss: 0.4553

Training:	 ACC: 0.8019 0.8025 0.8169 0.7870
Validation:	 ACC: 0.7856 0.7865 0.8055 0.7657
Validation:	 Best_BACC: 0.7864 0.7871 0.8004 0.7724
Validation:	 Loss: 0.4650
Pretraining:	Epoch 74/120
----------
training:	Epoch: [74][1/204]	Loss 0.5519 (0.5519)	
training:	Epoch: [74][2/204]	Loss 0.4261 (0.4890)	
training:	Epoch: [74][3/204]	Loss 0.5652 (0.5144)	
training:	Epoch: [74][4/204]	Loss 0.4714 (0.5036)	
training:	Epoch: [74][5/204]	Loss 0.4994 (0.5028)	
training:	Epoch: [74][6/204]	Loss 0.4723 (0.4977)	
training:	Epoch: [74][7/204]	Loss 0.5313 (0.5025)	
training:	Epoch: [74][8/204]	Loss 0.4300 (0.4934)	
training:	Epoch: [74][9/204]	Loss 0.5528 (0.5000)	
training:	Epoch: [74][10/204]	Loss 0.4288 (0.4929)	
training:	Epoch: [74][11/204]	Loss 0.3786 (0.4825)	
training:	Epoch: [74][12/204]	Loss 0.2792 (0.4656)	
training:	Epoch: [74][13/204]	Loss 0.5191 (0.4697)	
training:	Epoch: [74][14/204]	Loss 0.3297 (0.4597)	
training:	Epoch: [74][15/204]	Loss 0.3194 (0.4503)	
training:	Epoch: [74][16/204]	Loss 0.6348 (0.4619)	
training:	Epoch: [74][17/204]	Loss 0.4558 (0.4615)	
training:	Epoch: [74][18/204]	Loss 0.2376 (0.4491)	
training:	Epoch: [74][19/204]	Loss 0.5068 (0.4521)	
training:	Epoch: [74][20/204]	Loss 0.5670 (0.4579)	
training:	Epoch: [74][21/204]	Loss 0.4516 (0.4576)	
training:	Epoch: [74][22/204]	Loss 0.4807 (0.4586)	
training:	Epoch: [74][23/204]	Loss 0.6113 (0.4652)	
training:	Epoch: [74][24/204]	Loss 0.4493 (0.4646)	
training:	Epoch: [74][25/204]	Loss 0.4446 (0.4638)	
training:	Epoch: [74][26/204]	Loss 0.5411 (0.4668)	
training:	Epoch: [74][27/204]	Loss 0.5796 (0.4709)	
training:	Epoch: [74][28/204]	Loss 0.6246 (0.4764)	
training:	Epoch: [74][29/204]	Loss 0.4417 (0.4752)	
training:	Epoch: [74][30/204]	Loss 0.4300 (0.4737)	
training:	Epoch: [74][31/204]	Loss 0.3270 (0.4690)	
training:	Epoch: [74][32/204]	Loss 0.4031 (0.4669)	
training:	Epoch: [74][33/204]	Loss 0.2835 (0.4614)	
training:	Epoch: [74][34/204]	Loss 0.4306 (0.4605)	
training:	Epoch: [74][35/204]	Loss 0.4312 (0.4596)	
training:	Epoch: [74][36/204]	Loss 0.5162 (0.4612)	
training:	Epoch: [74][37/204]	Loss 0.5053 (0.4624)	
training:	Epoch: [74][38/204]	Loss 0.3684 (0.4599)	
training:	Epoch: [74][39/204]	Loss 0.3392 (0.4568)	
training:	Epoch: [74][40/204]	Loss 0.5769 (0.4598)	
training:	Epoch: [74][41/204]	Loss 0.3677 (0.4576)	
training:	Epoch: [74][42/204]	Loss 0.3906 (0.4560)	
training:	Epoch: [74][43/204]	Loss 0.2971 (0.4523)	
training:	Epoch: [74][44/204]	Loss 0.4832 (0.4530)	
training:	Epoch: [74][45/204]	Loss 0.5703 (0.4556)	
training:	Epoch: [74][46/204]	Loss 0.6425 (0.4597)	
training:	Epoch: [74][47/204]	Loss 0.4771 (0.4600)	
training:	Epoch: [74][48/204]	Loss 0.3796 (0.4584)	
training:	Epoch: [74][49/204]	Loss 0.3768 (0.4567)	
training:	Epoch: [74][50/204]	Loss 0.4239 (0.4560)	
training:	Epoch: [74][51/204]	Loss 0.6706 (0.4602)	
training:	Epoch: [74][52/204]	Loss 0.5465 (0.4619)	
training:	Epoch: [74][53/204]	Loss 0.4389 (0.4615)	
training:	Epoch: [74][54/204]	Loss 0.4146 (0.4606)	
training:	Epoch: [74][55/204]	Loss 0.3324 (0.4583)	
training:	Epoch: [74][56/204]	Loss 0.5293 (0.4595)	
training:	Epoch: [74][57/204]	Loss 0.3308 (0.4573)	
training:	Epoch: [74][58/204]	Loss 0.4063 (0.4564)	
training:	Epoch: [74][59/204]	Loss 0.4180 (0.4558)	
training:	Epoch: [74][60/204]	Loss 0.4930 (0.4564)	
training:	Epoch: [74][61/204]	Loss 0.3792 (0.4551)	
training:	Epoch: [74][62/204]	Loss 0.4481 (0.4550)	
training:	Epoch: [74][63/204]	Loss 0.4215 (0.4545)	
training:	Epoch: [74][64/204]	Loss 0.4864 (0.4550)	
training:	Epoch: [74][65/204]	Loss 0.2505 (0.4518)	
training:	Epoch: [74][66/204]	Loss 0.4434 (0.4517)	
training:	Epoch: [74][67/204]	Loss 0.3150 (0.4496)	
training:	Epoch: [74][68/204]	Loss 0.4726 (0.4500)	
training:	Epoch: [74][69/204]	Loss 0.3783 (0.4489)	
training:	Epoch: [74][70/204]	Loss 0.3772 (0.4479)	
training:	Epoch: [74][71/204]	Loss 0.4756 (0.4483)	
training:	Epoch: [74][72/204]	Loss 0.4742 (0.4487)	
training:	Epoch: [74][73/204]	Loss 0.5622 (0.4502)	
training:	Epoch: [74][74/204]	Loss 0.3594 (0.4490)	
training:	Epoch: [74][75/204]	Loss 0.5193 (0.4499)	
training:	Epoch: [74][76/204]	Loss 0.4912 (0.4505)	
training:	Epoch: [74][77/204]	Loss 0.5639 (0.4520)	
training:	Epoch: [74][78/204]	Loss 0.3466 (0.4506)	
training:	Epoch: [74][79/204]	Loss 0.4436 (0.4505)	
training:	Epoch: [74][80/204]	Loss 0.3192 (0.4489)	
training:	Epoch: [74][81/204]	Loss 0.3360 (0.4475)	
training:	Epoch: [74][82/204]	Loss 0.4797 (0.4479)	
training:	Epoch: [74][83/204]	Loss 0.3589 (0.4468)	
training:	Epoch: [74][84/204]	Loss 0.3587 (0.4457)	
training:	Epoch: [74][85/204]	Loss 0.5738 (0.4473)	
training:	Epoch: [74][86/204]	Loss 0.5176 (0.4481)	
training:	Epoch: [74][87/204]	Loss 0.5201 (0.4489)	
training:	Epoch: [74][88/204]	Loss 0.3838 (0.4482)	
training:	Epoch: [74][89/204]	Loss 0.4505 (0.4482)	
training:	Epoch: [74][90/204]	Loss 0.4469 (0.4482)	
training:	Epoch: [74][91/204]	Loss 0.4173 (0.4478)	
training:	Epoch: [74][92/204]	Loss 0.5013 (0.4484)	
training:	Epoch: [74][93/204]	Loss 0.4233 (0.4481)	
training:	Epoch: [74][94/204]	Loss 0.5043 (0.4487)	
training:	Epoch: [74][95/204]	Loss 0.2926 (0.4471)	
training:	Epoch: [74][96/204]	Loss 0.4574 (0.4472)	
training:	Epoch: [74][97/204]	Loss 0.4718 (0.4475)	
training:	Epoch: [74][98/204]	Loss 0.5510 (0.4485)	
training:	Epoch: [74][99/204]	Loss 0.4835 (0.4489)	
training:	Epoch: [74][100/204]	Loss 0.5188 (0.4496)	
training:	Epoch: [74][101/204]	Loss 0.5465 (0.4505)	
training:	Epoch: [74][102/204]	Loss 0.5132 (0.4511)	
training:	Epoch: [74][103/204]	Loss 0.4249 (0.4509)	
training:	Epoch: [74][104/204]	Loss 0.4641 (0.4510)	
training:	Epoch: [74][105/204]	Loss 0.3621 (0.4502)	
training:	Epoch: [74][106/204]	Loss 0.3717 (0.4494)	
training:	Epoch: [74][107/204]	Loss 0.4132 (0.4491)	
training:	Epoch: [74][108/204]	Loss 0.4580 (0.4492)	
training:	Epoch: [74][109/204]	Loss 0.5709 (0.4503)	
training:	Epoch: [74][110/204]	Loss 0.4195 (0.4500)	
training:	Epoch: [74][111/204]	Loss 0.4450 (0.4500)	
training:	Epoch: [74][112/204]	Loss 0.4539 (0.4500)	
training:	Epoch: [74][113/204]	Loss 0.4957 (0.4504)	
training:	Epoch: [74][114/204]	Loss 0.4706 (0.4506)	
training:	Epoch: [74][115/204]	Loss 0.4280 (0.4504)	
training:	Epoch: [74][116/204]	Loss 0.5948 (0.4516)	
training:	Epoch: [74][117/204]	Loss 0.6795 (0.4536)	
training:	Epoch: [74][118/204]	Loss 0.3329 (0.4526)	
training:	Epoch: [74][119/204]	Loss 0.4890 (0.4529)	
training:	Epoch: [74][120/204]	Loss 0.4519 (0.4529)	
training:	Epoch: [74][121/204]	Loss 0.4924 (0.4532)	
training:	Epoch: [74][122/204]	Loss 0.4493 (0.4531)	
training:	Epoch: [74][123/204]	Loss 0.4266 (0.4529)	
training:	Epoch: [74][124/204]	Loss 0.3906 (0.4524)	
training:	Epoch: [74][125/204]	Loss 0.4926 (0.4528)	
training:	Epoch: [74][126/204]	Loss 0.6503 (0.4543)	
training:	Epoch: [74][127/204]	Loss 0.3585 (0.4536)	
training:	Epoch: [74][128/204]	Loss 0.4465 (0.4535)	
training:	Epoch: [74][129/204]	Loss 0.5096 (0.4539)	
training:	Epoch: [74][130/204]	Loss 0.5006 (0.4543)	
training:	Epoch: [74][131/204]	Loss 0.5259 (0.4548)	
training:	Epoch: [74][132/204]	Loss 0.3622 (0.4541)	
training:	Epoch: [74][133/204]	Loss 0.3740 (0.4535)	
training:	Epoch: [74][134/204]	Loss 0.4825 (0.4538)	
training:	Epoch: [74][135/204]	Loss 0.5271 (0.4543)	
training:	Epoch: [74][136/204]	Loss 0.2942 (0.4531)	
training:	Epoch: [74][137/204]	Loss 0.4032 (0.4528)	
training:	Epoch: [74][138/204]	Loss 0.3990 (0.4524)	
training:	Epoch: [74][139/204]	Loss 0.3893 (0.4519)	
training:	Epoch: [74][140/204]	Loss 0.4262 (0.4517)	
training:	Epoch: [74][141/204]	Loss 0.4957 (0.4520)	
training:	Epoch: [74][142/204]	Loss 0.5300 (0.4526)	
training:	Epoch: [74][143/204]	Loss 0.3863 (0.4521)	
training:	Epoch: [74][144/204]	Loss 0.3514 (0.4514)	
training:	Epoch: [74][145/204]	Loss 0.3755 (0.4509)	
training:	Epoch: [74][146/204]	Loss 0.5766 (0.4518)	
training:	Epoch: [74][147/204]	Loss 0.5307 (0.4523)	
training:	Epoch: [74][148/204]	Loss 0.3525 (0.4516)	
training:	Epoch: [74][149/204]	Loss 0.5284 (0.4521)	
training:	Epoch: [74][150/204]	Loss 0.5193 (0.4526)	
training:	Epoch: [74][151/204]	Loss 0.4697 (0.4527)	
training:	Epoch: [74][152/204]	Loss 0.4423 (0.4526)	
training:	Epoch: [74][153/204]	Loss 0.4020 (0.4523)	
training:	Epoch: [74][154/204]	Loss 0.6064 (0.4533)	
training:	Epoch: [74][155/204]	Loss 0.5194 (0.4537)	
training:	Epoch: [74][156/204]	Loss 0.3790 (0.4533)	
training:	Epoch: [74][157/204]	Loss 0.3354 (0.4525)	
training:	Epoch: [74][158/204]	Loss 0.4994 (0.4528)	
training:	Epoch: [74][159/204]	Loss 0.3948 (0.4524)	
training:	Epoch: [74][160/204]	Loss 0.4312 (0.4523)	
training:	Epoch: [74][161/204]	Loss 0.4099 (0.4520)	
training:	Epoch: [74][162/204]	Loss 0.4593 (0.4521)	
training:	Epoch: [74][163/204]	Loss 0.4795 (0.4523)	
training:	Epoch: [74][164/204]	Loss 0.3775 (0.4518)	
training:	Epoch: [74][165/204]	Loss 0.4122 (0.4516)	
training:	Epoch: [74][166/204]	Loss 0.4988 (0.4518)	
training:	Epoch: [74][167/204]	Loss 0.4373 (0.4518)	
training:	Epoch: [74][168/204]	Loss 0.5834 (0.4525)	
training:	Epoch: [74][169/204]	Loss 0.4587 (0.4526)	
training:	Epoch: [74][170/204]	Loss 0.3770 (0.4521)	
training:	Epoch: [74][171/204]	Loss 0.4185 (0.4519)	
training:	Epoch: [74][172/204]	Loss 0.4236 (0.4518)	
training:	Epoch: [74][173/204]	Loss 0.4846 (0.4520)	
training:	Epoch: [74][174/204]	Loss 0.4551 (0.4520)	
training:	Epoch: [74][175/204]	Loss 0.5134 (0.4523)	
training:	Epoch: [74][176/204]	Loss 0.3556 (0.4518)	
training:	Epoch: [74][177/204]	Loss 0.3710 (0.4513)	
training:	Epoch: [74][178/204]	Loss 0.5105 (0.4517)	
training:	Epoch: [74][179/204]	Loss 0.4572 (0.4517)	
training:	Epoch: [74][180/204]	Loss 0.3896 (0.4513)	
training:	Epoch: [74][181/204]	Loss 0.6411 (0.4524)	
training:	Epoch: [74][182/204]	Loss 0.5280 (0.4528)	
training:	Epoch: [74][183/204]	Loss 0.4723 (0.4529)	
training:	Epoch: [74][184/204]	Loss 0.4717 (0.4530)	
training:	Epoch: [74][185/204]	Loss 0.5252 (0.4534)	
training:	Epoch: [74][186/204]	Loss 0.3462 (0.4528)	
training:	Epoch: [74][187/204]	Loss 0.3270 (0.4522)	
training:	Epoch: [74][188/204]	Loss 0.4210 (0.4520)	
training:	Epoch: [74][189/204]	Loss 0.4761 (0.4521)	
training:	Epoch: [74][190/204]	Loss 0.5128 (0.4524)	
training:	Epoch: [74][191/204]	Loss 0.5049 (0.4527)	
training:	Epoch: [74][192/204]	Loss 0.2957 (0.4519)	
training:	Epoch: [74][193/204]	Loss 0.4699 (0.4520)	
training:	Epoch: [74][194/204]	Loss 0.4858 (0.4522)	
training:	Epoch: [74][195/204]	Loss 0.6232 (0.4530)	
training:	Epoch: [74][196/204]	Loss 0.3887 (0.4527)	
training:	Epoch: [74][197/204]	Loss 0.6421 (0.4537)	
training:	Epoch: [74][198/204]	Loss 0.3231 (0.4530)	
training:	Epoch: [74][199/204]	Loss 0.3363 (0.4524)	
training:	Epoch: [74][200/204]	Loss 0.5108 (0.4527)	
training:	Epoch: [74][201/204]	Loss 0.4893 (0.4529)	
training:	Epoch: [74][202/204]	Loss 0.4699 (0.4530)	
training:	Epoch: [74][203/204]	Loss 0.5110 (0.4533)	
training:	Epoch: [74][204/204]	Loss 0.5235 (0.4536)	
Training:	 Loss: 0.4529

Training:	 ACC: 0.8038 0.8047 0.8245 0.7832
Validation:	 ACC: 0.7865 0.7876 0.8106 0.7623
Validation:	 Best_BACC: 0.7865 0.7876 0.8106 0.7623
Validation:	 Loss: 0.4642
Pretraining:	Epoch 75/120
----------
training:	Epoch: [75][1/204]	Loss 0.3604 (0.3604)	
training:	Epoch: [75][2/204]	Loss 0.5463 (0.4533)	
training:	Epoch: [75][3/204]	Loss 0.3950 (0.4339)	
training:	Epoch: [75][4/204]	Loss 0.4448 (0.4366)	
training:	Epoch: [75][5/204]	Loss 0.4410 (0.4375)	
training:	Epoch: [75][6/204]	Loss 0.6118 (0.4666)	
training:	Epoch: [75][7/204]	Loss 0.3764 (0.4537)	
training:	Epoch: [75][8/204]	Loss 0.3515 (0.4409)	
training:	Epoch: [75][9/204]	Loss 0.3956 (0.4359)	
training:	Epoch: [75][10/204]	Loss 0.6760 (0.4599)	
training:	Epoch: [75][11/204]	Loss 0.5308 (0.4663)	
training:	Epoch: [75][12/204]	Loss 0.5008 (0.4692)	
training:	Epoch: [75][13/204]	Loss 0.2805 (0.4547)	
training:	Epoch: [75][14/204]	Loss 0.6730 (0.4703)	
training:	Epoch: [75][15/204]	Loss 0.4271 (0.4674)	
training:	Epoch: [75][16/204]	Loss 0.5261 (0.4711)	
training:	Epoch: [75][17/204]	Loss 0.5437 (0.4754)	
training:	Epoch: [75][18/204]	Loss 0.5281 (0.4783)	
training:	Epoch: [75][19/204]	Loss 0.5710 (0.4832)	
training:	Epoch: [75][20/204]	Loss 0.3688 (0.4774)	
training:	Epoch: [75][21/204]	Loss 0.4681 (0.4770)	
training:	Epoch: [75][22/204]	Loss 0.5096 (0.4785)	
training:	Epoch: [75][23/204]	Loss 0.3216 (0.4717)	
training:	Epoch: [75][24/204]	Loss 0.5334 (0.4742)	
training:	Epoch: [75][25/204]	Loss 0.6351 (0.4807)	
training:	Epoch: [75][26/204]	Loss 0.5691 (0.4841)	
training:	Epoch: [75][27/204]	Loss 0.4062 (0.4812)	
training:	Epoch: [75][28/204]	Loss 0.5189 (0.4825)	
training:	Epoch: [75][29/204]	Loss 0.5795 (0.4859)	
training:	Epoch: [75][30/204]	Loss 0.4868 (0.4859)	
training:	Epoch: [75][31/204]	Loss 0.5661 (0.4885)	
training:	Epoch: [75][32/204]	Loss 0.4541 (0.4874)	
training:	Epoch: [75][33/204]	Loss 0.4834 (0.4873)	
training:	Epoch: [75][34/204]	Loss 0.5382 (0.4888)	
training:	Epoch: [75][35/204]	Loss 0.4277 (0.4871)	
training:	Epoch: [75][36/204]	Loss 0.4230 (0.4853)	
training:	Epoch: [75][37/204]	Loss 0.4477 (0.4843)	
training:	Epoch: [75][38/204]	Loss 0.6289 (0.4881)	
training:	Epoch: [75][39/204]	Loss 0.4127 (0.4861)	
training:	Epoch: [75][40/204]	Loss 0.5091 (0.4867)	
training:	Epoch: [75][41/204]	Loss 0.5899 (0.4892)	
training:	Epoch: [75][42/204]	Loss 0.5115 (0.4898)	
training:	Epoch: [75][43/204]	Loss 0.2322 (0.4838)	
training:	Epoch: [75][44/204]	Loss 0.6328 (0.4872)	
training:	Epoch: [75][45/204]	Loss 0.4050 (0.4853)	
training:	Epoch: [75][46/204]	Loss 0.4685 (0.4850)	
training:	Epoch: [75][47/204]	Loss 0.4841 (0.4849)	
training:	Epoch: [75][48/204]	Loss 0.4767 (0.4848)	
training:	Epoch: [75][49/204]	Loss 0.6561 (0.4883)	
training:	Epoch: [75][50/204]	Loss 0.5365 (0.4892)	
training:	Epoch: [75][51/204]	Loss 0.4159 (0.4878)	
training:	Epoch: [75][52/204]	Loss 0.5525 (0.4890)	
training:	Epoch: [75][53/204]	Loss 0.4792 (0.4889)	
training:	Epoch: [75][54/204]	Loss 0.3618 (0.4865)	
training:	Epoch: [75][55/204]	Loss 0.5024 (0.4868)	
training:	Epoch: [75][56/204]	Loss 0.3305 (0.4840)	
training:	Epoch: [75][57/204]	Loss 0.4364 (0.4832)	
training:	Epoch: [75][58/204]	Loss 0.4165 (0.4820)	
training:	Epoch: [75][59/204]	Loss 0.3640 (0.4800)	
training:	Epoch: [75][60/204]	Loss 0.6047 (0.4821)	
training:	Epoch: [75][61/204]	Loss 0.2853 (0.4789)	
training:	Epoch: [75][62/204]	Loss 0.5941 (0.4807)	
training:	Epoch: [75][63/204]	Loss 0.4682 (0.4805)	
training:	Epoch: [75][64/204]	Loss 0.4336 (0.4798)	
training:	Epoch: [75][65/204]	Loss 0.3197 (0.4773)	
training:	Epoch: [75][66/204]	Loss 0.3990 (0.4761)	
training:	Epoch: [75][67/204]	Loss 0.4612 (0.4759)	
training:	Epoch: [75][68/204]	Loss 0.4053 (0.4749)	
training:	Epoch: [75][69/204]	Loss 0.4209 (0.4741)	
training:	Epoch: [75][70/204]	Loss 0.2896 (0.4715)	
training:	Epoch: [75][71/204]	Loss 0.4465 (0.4711)	
training:	Epoch: [75][72/204]	Loss 0.4298 (0.4705)	
training:	Epoch: [75][73/204]	Loss 0.4819 (0.4707)	
training:	Epoch: [75][74/204]	Loss 0.5255 (0.4714)	
training:	Epoch: [75][75/204]	Loss 0.3612 (0.4700)	
training:	Epoch: [75][76/204]	Loss 0.4290 (0.4694)	
training:	Epoch: [75][77/204]	Loss 0.3283 (0.4676)	
training:	Epoch: [75][78/204]	Loss 0.5131 (0.4682)	
training:	Epoch: [75][79/204]	Loss 0.6642 (0.4707)	
training:	Epoch: [75][80/204]	Loss 0.4759 (0.4707)	
training:	Epoch: [75][81/204]	Loss 0.3787 (0.4696)	
training:	Epoch: [75][82/204]	Loss 0.4624 (0.4695)	
training:	Epoch: [75][83/204]	Loss 0.5104 (0.4700)	
training:	Epoch: [75][84/204]	Loss 0.5695 (0.4712)	
training:	Epoch: [75][85/204]	Loss 0.4099 (0.4705)	
training:	Epoch: [75][86/204]	Loss 0.4056 (0.4697)	
training:	Epoch: [75][87/204]	Loss 0.3823 (0.4687)	
training:	Epoch: [75][88/204]	Loss 0.5210 (0.4693)	
training:	Epoch: [75][89/204]	Loss 0.5775 (0.4705)	
training:	Epoch: [75][90/204]	Loss 0.4438 (0.4702)	
training:	Epoch: [75][91/204]	Loss 0.4304 (0.4698)	
training:	Epoch: [75][92/204]	Loss 0.3888 (0.4689)	
training:	Epoch: [75][93/204]	Loss 0.4857 (0.4691)	
training:	Epoch: [75][94/204]	Loss 0.3790 (0.4681)	
training:	Epoch: [75][95/204]	Loss 0.6139 (0.4697)	
training:	Epoch: [75][96/204]	Loss 0.4319 (0.4693)	
training:	Epoch: [75][97/204]	Loss 0.4765 (0.4693)	
training:	Epoch: [75][98/204]	Loss 0.3679 (0.4683)	
training:	Epoch: [75][99/204]	Loss 0.6531 (0.4702)	
training:	Epoch: [75][100/204]	Loss 0.3421 (0.4689)	
training:	Epoch: [75][101/204]	Loss 0.5145 (0.4693)	
training:	Epoch: [75][102/204]	Loss 0.4242 (0.4689)	
training:	Epoch: [75][103/204]	Loss 0.2914 (0.4672)	
training:	Epoch: [75][104/204]	Loss 0.4733 (0.4672)	
training:	Epoch: [75][105/204]	Loss 0.4249 (0.4668)	
training:	Epoch: [75][106/204]	Loss 0.3879 (0.4661)	
training:	Epoch: [75][107/204]	Loss 0.3569 (0.4651)	
training:	Epoch: [75][108/204]	Loss 0.5104 (0.4655)	
training:	Epoch: [75][109/204]	Loss 0.3543 (0.4645)	
training:	Epoch: [75][110/204]	Loss 0.4597 (0.4644)	
training:	Epoch: [75][111/204]	Loss 0.3779 (0.4636)	
training:	Epoch: [75][112/204]	Loss 0.4643 (0.4636)	
training:	Epoch: [75][113/204]	Loss 0.3854 (0.4630)	
training:	Epoch: [75][114/204]	Loss 0.3777 (0.4622)	
training:	Epoch: [75][115/204]	Loss 0.5406 (0.4629)	
training:	Epoch: [75][116/204]	Loss 0.2955 (0.4614)	
training:	Epoch: [75][117/204]	Loss 0.5400 (0.4621)	
training:	Epoch: [75][118/204]	Loss 0.4580 (0.4621)	
training:	Epoch: [75][119/204]	Loss 0.3284 (0.4610)	
training:	Epoch: [75][120/204]	Loss 0.3996 (0.4604)	
training:	Epoch: [75][121/204]	Loss 0.2769 (0.4589)	
training:	Epoch: [75][122/204]	Loss 0.4941 (0.4592)	
training:	Epoch: [75][123/204]	Loss 0.4054 (0.4588)	
training:	Epoch: [75][124/204]	Loss 0.3079 (0.4576)	
training:	Epoch: [75][125/204]	Loss 0.3628 (0.4568)	
training:	Epoch: [75][126/204]	Loss 0.3432 (0.4559)	
training:	Epoch: [75][127/204]	Loss 0.4627 (0.4560)	
training:	Epoch: [75][128/204]	Loss 0.3957 (0.4555)	
training:	Epoch: [75][129/204]	Loss 0.6670 (0.4571)	
training:	Epoch: [75][130/204]	Loss 0.3269 (0.4561)	
training:	Epoch: [75][131/204]	Loss 0.4651 (0.4562)	
training:	Epoch: [75][132/204]	Loss 0.4050 (0.4558)	
training:	Epoch: [75][133/204]	Loss 0.6255 (0.4571)	
training:	Epoch: [75][134/204]	Loss 0.5095 (0.4575)	
training:	Epoch: [75][135/204]	Loss 0.6366 (0.4588)	
training:	Epoch: [75][136/204]	Loss 0.3184 (0.4578)	
training:	Epoch: [75][137/204]	Loss 0.3765 (0.4572)	
training:	Epoch: [75][138/204]	Loss 0.3830 (0.4566)	
training:	Epoch: [75][139/204]	Loss 0.5718 (0.4575)	
training:	Epoch: [75][140/204]	Loss 0.5079 (0.4578)	
training:	Epoch: [75][141/204]	Loss 0.3958 (0.4574)	
training:	Epoch: [75][142/204]	Loss 0.3927 (0.4569)	
training:	Epoch: [75][143/204]	Loss 0.6386 (0.4582)	
training:	Epoch: [75][144/204]	Loss 0.3773 (0.4576)	
training:	Epoch: [75][145/204]	Loss 0.4330 (0.4575)	
training:	Epoch: [75][146/204]	Loss 0.4530 (0.4574)	
training:	Epoch: [75][147/204]	Loss 0.3813 (0.4569)	
training:	Epoch: [75][148/204]	Loss 0.5280 (0.4574)	
training:	Epoch: [75][149/204]	Loss 0.4451 (0.4573)	
training:	Epoch: [75][150/204]	Loss 0.5719 (0.4581)	
training:	Epoch: [75][151/204]	Loss 0.4857 (0.4583)	
training:	Epoch: [75][152/204]	Loss 0.4110 (0.4580)	
training:	Epoch: [75][153/204]	Loss 0.5941 (0.4588)	
training:	Epoch: [75][154/204]	Loss 0.5295 (0.4593)	
training:	Epoch: [75][155/204]	Loss 0.5310 (0.4598)	
training:	Epoch: [75][156/204]	Loss 0.6011 (0.4607)	
training:	Epoch: [75][157/204]	Loss 0.4802 (0.4608)	
training:	Epoch: [75][158/204]	Loss 0.4740 (0.4609)	
training:	Epoch: [75][159/204]	Loss 0.4695 (0.4609)	
training:	Epoch: [75][160/204]	Loss 0.5258 (0.4613)	
training:	Epoch: [75][161/204]	Loss 0.4358 (0.4612)	
training:	Epoch: [75][162/204]	Loss 0.5603 (0.4618)	
training:	Epoch: [75][163/204]	Loss 0.4996 (0.4620)	
training:	Epoch: [75][164/204]	Loss 0.3353 (0.4613)	
training:	Epoch: [75][165/204]	Loss 0.4706 (0.4613)	
training:	Epoch: [75][166/204]	Loss 0.4038 (0.4610)	
training:	Epoch: [75][167/204]	Loss 0.5899 (0.4617)	
training:	Epoch: [75][168/204]	Loss 0.3975 (0.4614)	
training:	Epoch: [75][169/204]	Loss 0.4198 (0.4611)	
training:	Epoch: [75][170/204]	Loss 0.3660 (0.4605)	
training:	Epoch: [75][171/204]	Loss 0.4510 (0.4605)	
training:	Epoch: [75][172/204]	Loss 0.4785 (0.4606)	
training:	Epoch: [75][173/204]	Loss 0.4977 (0.4608)	
training:	Epoch: [75][174/204]	Loss 0.3448 (0.4601)	
training:	Epoch: [75][175/204]	Loss 0.3373 (0.4594)	
training:	Epoch: [75][176/204]	Loss 0.8275 (0.4615)	
training:	Epoch: [75][177/204]	Loss 0.3932 (0.4611)	
training:	Epoch: [75][178/204]	Loss 0.3686 (0.4606)	
training:	Epoch: [75][179/204]	Loss 0.3032 (0.4597)	
training:	Epoch: [75][180/204]	Loss 0.4586 (0.4597)	
training:	Epoch: [75][181/204]	Loss 0.3275 (0.4590)	
training:	Epoch: [75][182/204]	Loss 0.3507 (0.4584)	
training:	Epoch: [75][183/204]	Loss 0.3726 (0.4579)	
training:	Epoch: [75][184/204]	Loss 0.4404 (0.4578)	
training:	Epoch: [75][185/204]	Loss 0.2457 (0.4567)	
training:	Epoch: [75][186/204]	Loss 0.4442 (0.4566)	
training:	Epoch: [75][187/204]	Loss 0.5690 (0.4572)	
training:	Epoch: [75][188/204]	Loss 0.3398 (0.4566)	
training:	Epoch: [75][189/204]	Loss 0.5324 (0.4570)	
training:	Epoch: [75][190/204]	Loss 0.4110 (0.4568)	
training:	Epoch: [75][191/204]	Loss 0.2988 (0.4559)	
training:	Epoch: [75][192/204]	Loss 0.4900 (0.4561)	
training:	Epoch: [75][193/204]	Loss 0.3567 (0.4556)	
training:	Epoch: [75][194/204]	Loss 0.4468 (0.4556)	
training:	Epoch: [75][195/204]	Loss 0.3730 (0.4551)	
training:	Epoch: [75][196/204]	Loss 0.3264 (0.4545)	
training:	Epoch: [75][197/204]	Loss 0.5288 (0.4549)	
training:	Epoch: [75][198/204]	Loss 0.4982 (0.4551)	
training:	Epoch: [75][199/204]	Loss 0.3240 (0.4544)	
training:	Epoch: [75][200/204]	Loss 0.4421 (0.4544)	
training:	Epoch: [75][201/204]	Loss 0.4391 (0.4543)	
training:	Epoch: [75][202/204]	Loss 0.4706 (0.4544)	
training:	Epoch: [75][203/204]	Loss 0.3609 (0.4539)	
training:	Epoch: [75][204/204]	Loss 0.3020 (0.4532)	
Training:	 Loss: 0.4525

Training:	 ACC: 0.8042 0.8045 0.8113 0.7972
Validation:	 ACC: 0.7848 0.7854 0.7984 0.7713
Validation:	 Best_BACC: 0.7865 0.7876 0.8106 0.7623
Validation:	 Loss: 0.4628
Pretraining:	Epoch 76/120
----------
training:	Epoch: [76][1/204]	Loss 0.4853 (0.4853)	
training:	Epoch: [76][2/204]	Loss 0.5601 (0.5227)	
training:	Epoch: [76][3/204]	Loss 0.5251 (0.5235)	
training:	Epoch: [76][4/204]	Loss 0.4230 (0.4984)	
training:	Epoch: [76][5/204]	Loss 0.4297 (0.4846)	
training:	Epoch: [76][6/204]	Loss 0.7016 (0.5208)	
training:	Epoch: [76][7/204]	Loss 0.4776 (0.5146)	
training:	Epoch: [76][8/204]	Loss 0.5741 (0.5221)	
training:	Epoch: [76][9/204]	Loss 0.3521 (0.5032)	
training:	Epoch: [76][10/204]	Loss 0.4870 (0.5016)	
training:	Epoch: [76][11/204]	Loss 0.4257 (0.4947)	
training:	Epoch: [76][12/204]	Loss 0.5634 (0.5004)	
training:	Epoch: [76][13/204]	Loss 0.4108 (0.4935)	
training:	Epoch: [76][14/204]	Loss 0.5406 (0.4969)	
training:	Epoch: [76][15/204]	Loss 0.3118 (0.4845)	
training:	Epoch: [76][16/204]	Loss 0.3947 (0.4789)	
training:	Epoch: [76][17/204]	Loss 0.4795 (0.4790)	
training:	Epoch: [76][18/204]	Loss 0.2923 (0.4686)	
training:	Epoch: [76][19/204]	Loss 0.2839 (0.4589)	
training:	Epoch: [76][20/204]	Loss 0.4258 (0.4572)	
training:	Epoch: [76][21/204]	Loss 0.3110 (0.4502)	
training:	Epoch: [76][22/204]	Loss 0.4970 (0.4524)	
training:	Epoch: [76][23/204]	Loss 0.5166 (0.4552)	
training:	Epoch: [76][24/204]	Loss 0.2479 (0.4465)	
training:	Epoch: [76][25/204]	Loss 0.4189 (0.4454)	
training:	Epoch: [76][26/204]	Loss 0.3984 (0.4436)	
training:	Epoch: [76][27/204]	Loss 0.3620 (0.4406)	
training:	Epoch: [76][28/204]	Loss 0.3581 (0.4376)	
training:	Epoch: [76][29/204]	Loss 0.4201 (0.4370)	
training:	Epoch: [76][30/204]	Loss 0.5501 (0.4408)	
training:	Epoch: [76][31/204]	Loss 0.5263 (0.4436)	
training:	Epoch: [76][32/204]	Loss 0.4793 (0.4447)	
training:	Epoch: [76][33/204]	Loss 0.2910 (0.4400)	
training:	Epoch: [76][34/204]	Loss 0.4429 (0.4401)	
training:	Epoch: [76][35/204]	Loss 0.3467 (0.4374)	
training:	Epoch: [76][36/204]	Loss 0.4648 (0.4382)	
training:	Epoch: [76][37/204]	Loss 0.4326 (0.4381)	
training:	Epoch: [76][38/204]	Loss 0.4686 (0.4389)	
training:	Epoch: [76][39/204]	Loss 0.3991 (0.4378)	
training:	Epoch: [76][40/204]	Loss 0.5634 (0.4410)	
training:	Epoch: [76][41/204]	Loss 0.4165 (0.4404)	
training:	Epoch: [76][42/204]	Loss 0.4597 (0.4408)	
training:	Epoch: [76][43/204]	Loss 0.4389 (0.4408)	
training:	Epoch: [76][44/204]	Loss 0.4129 (0.4402)	
training:	Epoch: [76][45/204]	Loss 0.4640 (0.4407)	
training:	Epoch: [76][46/204]	Loss 0.4358 (0.4406)	
training:	Epoch: [76][47/204]	Loss 0.4175 (0.4401)	
training:	Epoch: [76][48/204]	Loss 0.5014 (0.4414)	
training:	Epoch: [76][49/204]	Loss 0.4167 (0.4409)	
training:	Epoch: [76][50/204]	Loss 0.3542 (0.4391)	
training:	Epoch: [76][51/204]	Loss 0.4559 (0.4395)	
training:	Epoch: [76][52/204]	Loss 0.3441 (0.4376)	
training:	Epoch: [76][53/204]	Loss 0.5220 (0.4392)	
training:	Epoch: [76][54/204]	Loss 0.4619 (0.4396)	
training:	Epoch: [76][55/204]	Loss 0.3840 (0.4386)	
training:	Epoch: [76][56/204]	Loss 0.5273 (0.4402)	
training:	Epoch: [76][57/204]	Loss 0.5140 (0.4415)	
training:	Epoch: [76][58/204]	Loss 0.3860 (0.4405)	
training:	Epoch: [76][59/204]	Loss 0.6832 (0.4447)	
training:	Epoch: [76][60/204]	Loss 0.4091 (0.4441)	
training:	Epoch: [76][61/204]	Loss 0.4682 (0.4445)	
training:	Epoch: [76][62/204]	Loss 0.3930 (0.4436)	
training:	Epoch: [76][63/204]	Loss 0.4430 (0.4436)	
training:	Epoch: [76][64/204]	Loss 0.5181 (0.4448)	
training:	Epoch: [76][65/204]	Loss 0.5018 (0.4457)	
training:	Epoch: [76][66/204]	Loss 0.2964 (0.4434)	
training:	Epoch: [76][67/204]	Loss 0.3815 (0.4425)	
training:	Epoch: [76][68/204]	Loss 0.2542 (0.4397)	
training:	Epoch: [76][69/204]	Loss 0.3654 (0.4386)	
training:	Epoch: [76][70/204]	Loss 0.4531 (0.4388)	
training:	Epoch: [76][71/204]	Loss 0.3095 (0.4370)	
training:	Epoch: [76][72/204]	Loss 0.3812 (0.4362)	
training:	Epoch: [76][73/204]	Loss 0.5074 (0.4372)	
training:	Epoch: [76][74/204]	Loss 0.4380 (0.4372)	
training:	Epoch: [76][75/204]	Loss 0.4069 (0.4368)	
training:	Epoch: [76][76/204]	Loss 0.3755 (0.4360)	
training:	Epoch: [76][77/204]	Loss 0.4284 (0.4359)	
training:	Epoch: [76][78/204]	Loss 0.4350 (0.4359)	
training:	Epoch: [76][79/204]	Loss 0.4439 (0.4360)	
training:	Epoch: [76][80/204]	Loss 0.4148 (0.4357)	
training:	Epoch: [76][81/204]	Loss 0.5377 (0.4370)	
training:	Epoch: [76][82/204]	Loss 0.6425 (0.4395)	
training:	Epoch: [76][83/204]	Loss 0.3487 (0.4384)	
training:	Epoch: [76][84/204]	Loss 0.4152 (0.4381)	
training:	Epoch: [76][85/204]	Loss 0.5734 (0.4397)	
training:	Epoch: [76][86/204]	Loss 0.4638 (0.4400)	
training:	Epoch: [76][87/204]	Loss 0.4298 (0.4399)	
training:	Epoch: [76][88/204]	Loss 0.4212 (0.4397)	
training:	Epoch: [76][89/204]	Loss 0.4472 (0.4398)	
training:	Epoch: [76][90/204]	Loss 0.4347 (0.4397)	
training:	Epoch: [76][91/204]	Loss 0.4734 (0.4401)	
training:	Epoch: [76][92/204]	Loss 0.5514 (0.4413)	
training:	Epoch: [76][93/204]	Loss 0.5327 (0.4423)	
training:	Epoch: [76][94/204]	Loss 0.5217 (0.4431)	
training:	Epoch: [76][95/204]	Loss 0.3896 (0.4426)	
training:	Epoch: [76][96/204]	Loss 0.5031 (0.4432)	
training:	Epoch: [76][97/204]	Loss 0.4992 (0.4438)	
training:	Epoch: [76][98/204]	Loss 0.3331 (0.4426)	
training:	Epoch: [76][99/204]	Loss 0.3407 (0.4416)	
training:	Epoch: [76][100/204]	Loss 0.4956 (0.4421)	
training:	Epoch: [76][101/204]	Loss 0.3976 (0.4417)	
training:	Epoch: [76][102/204]	Loss 0.3845 (0.4411)	
training:	Epoch: [76][103/204]	Loss 0.4633 (0.4414)	
training:	Epoch: [76][104/204]	Loss 0.4623 (0.4416)	
training:	Epoch: [76][105/204]	Loss 0.3640 (0.4408)	
training:	Epoch: [76][106/204]	Loss 0.4825 (0.4412)	
training:	Epoch: [76][107/204]	Loss 0.4361 (0.4412)	
training:	Epoch: [76][108/204]	Loss 0.4489 (0.4412)	
training:	Epoch: [76][109/204]	Loss 0.5535 (0.4423)	
training:	Epoch: [76][110/204]	Loss 0.4168 (0.4420)	
training:	Epoch: [76][111/204]	Loss 0.5049 (0.4426)	
training:	Epoch: [76][112/204]	Loss 0.3761 (0.4420)	
training:	Epoch: [76][113/204]	Loss 0.6724 (0.4440)	
training:	Epoch: [76][114/204]	Loss 0.3257 (0.4430)	
training:	Epoch: [76][115/204]	Loss 0.5176 (0.4437)	
training:	Epoch: [76][116/204]	Loss 0.4669 (0.4439)	
training:	Epoch: [76][117/204]	Loss 0.4549 (0.4439)	
training:	Epoch: [76][118/204]	Loss 0.3739 (0.4434)	
training:	Epoch: [76][119/204]	Loss 0.2669 (0.4419)	
training:	Epoch: [76][120/204]	Loss 0.5136 (0.4425)	
training:	Epoch: [76][121/204]	Loss 0.3596 (0.4418)	
training:	Epoch: [76][122/204]	Loss 0.4347 (0.4417)	
training:	Epoch: [76][123/204]	Loss 0.3329 (0.4408)	
training:	Epoch: [76][124/204]	Loss 0.4180 (0.4407)	
training:	Epoch: [76][125/204]	Loss 0.3961 (0.4403)	
training:	Epoch: [76][126/204]	Loss 0.4347 (0.4403)	
training:	Epoch: [76][127/204]	Loss 0.4847 (0.4406)	
training:	Epoch: [76][128/204]	Loss 0.6447 (0.4422)	
training:	Epoch: [76][129/204]	Loss 0.5699 (0.4432)	
training:	Epoch: [76][130/204]	Loss 0.4545 (0.4433)	
training:	Epoch: [76][131/204]	Loss 0.4212 (0.4431)	
training:	Epoch: [76][132/204]	Loss 0.3249 (0.4422)	
training:	Epoch: [76][133/204]	Loss 0.3983 (0.4419)	
training:	Epoch: [76][134/204]	Loss 0.5994 (0.4431)	
training:	Epoch: [76][135/204]	Loss 0.4201 (0.4429)	
training:	Epoch: [76][136/204]	Loss 0.4805 (0.4432)	
training:	Epoch: [76][137/204]	Loss 0.3442 (0.4424)	
training:	Epoch: [76][138/204]	Loss 0.4984 (0.4428)	
training:	Epoch: [76][139/204]	Loss 0.4418 (0.4428)	
training:	Epoch: [76][140/204]	Loss 0.5692 (0.4437)	
training:	Epoch: [76][141/204]	Loss 0.4399 (0.4437)	
training:	Epoch: [76][142/204]	Loss 0.3989 (0.4434)	
training:	Epoch: [76][143/204]	Loss 0.4664 (0.4436)	
training:	Epoch: [76][144/204]	Loss 0.4683 (0.4437)	
training:	Epoch: [76][145/204]	Loss 0.4295 (0.4436)	
training:	Epoch: [76][146/204]	Loss 0.4642 (0.4438)	
training:	Epoch: [76][147/204]	Loss 0.3299 (0.4430)	
training:	Epoch: [76][148/204]	Loss 0.4232 (0.4429)	
training:	Epoch: [76][149/204]	Loss 0.4458 (0.4429)	
training:	Epoch: [76][150/204]	Loss 0.6179 (0.4441)	
training:	Epoch: [76][151/204]	Loss 0.4049 (0.4438)	
training:	Epoch: [76][152/204]	Loss 0.4681 (0.4440)	
training:	Epoch: [76][153/204]	Loss 0.5188 (0.4444)	
training:	Epoch: [76][154/204]	Loss 0.5802 (0.4453)	
training:	Epoch: [76][155/204]	Loss 0.5432 (0.4460)	
training:	Epoch: [76][156/204]	Loss 0.3304 (0.4452)	
training:	Epoch: [76][157/204]	Loss 0.3348 (0.4445)	
training:	Epoch: [76][158/204]	Loss 0.4180 (0.4443)	
training:	Epoch: [76][159/204]	Loss 0.4082 (0.4441)	
training:	Epoch: [76][160/204]	Loss 0.4544 (0.4442)	
training:	Epoch: [76][161/204]	Loss 0.5199 (0.4447)	
training:	Epoch: [76][162/204]	Loss 0.4787 (0.4449)	
training:	Epoch: [76][163/204]	Loss 0.3598 (0.4443)	
training:	Epoch: [76][164/204]	Loss 0.3925 (0.4440)	
training:	Epoch: [76][165/204]	Loss 0.3691 (0.4436)	
training:	Epoch: [76][166/204]	Loss 0.5434 (0.4442)	
training:	Epoch: [76][167/204]	Loss 0.6702 (0.4455)	
training:	Epoch: [76][168/204]	Loss 0.4000 (0.4453)	
training:	Epoch: [76][169/204]	Loss 0.3990 (0.4450)	
training:	Epoch: [76][170/204]	Loss 0.4606 (0.4451)	
training:	Epoch: [76][171/204]	Loss 0.4256 (0.4450)	
training:	Epoch: [76][172/204]	Loss 0.5318 (0.4455)	
training:	Epoch: [76][173/204]	Loss 0.4747 (0.4456)	
training:	Epoch: [76][174/204]	Loss 0.4706 (0.4458)	
training:	Epoch: [76][175/204]	Loss 0.4675 (0.4459)	
training:	Epoch: [76][176/204]	Loss 0.3452 (0.4453)	
training:	Epoch: [76][177/204]	Loss 0.4786 (0.4455)	
training:	Epoch: [76][178/204]	Loss 0.4826 (0.4457)	
training:	Epoch: [76][179/204]	Loss 0.4156 (0.4456)	
training:	Epoch: [76][180/204]	Loss 0.4396 (0.4455)	
training:	Epoch: [76][181/204]	Loss 0.5880 (0.4463)	
training:	Epoch: [76][182/204]	Loss 0.5414 (0.4468)	
training:	Epoch: [76][183/204]	Loss 0.4709 (0.4470)	
training:	Epoch: [76][184/204]	Loss 0.3511 (0.4464)	
training:	Epoch: [76][185/204]	Loss 0.6017 (0.4473)	
training:	Epoch: [76][186/204]	Loss 0.4921 (0.4475)	
training:	Epoch: [76][187/204]	Loss 0.3869 (0.4472)	
training:	Epoch: [76][188/204]	Loss 0.6343 (0.4482)	
training:	Epoch: [76][189/204]	Loss 0.4710 (0.4483)	
training:	Epoch: [76][190/204]	Loss 0.4714 (0.4484)	
training:	Epoch: [76][191/204]	Loss 0.4615 (0.4485)	
training:	Epoch: [76][192/204]	Loss 0.6832 (0.4497)	
training:	Epoch: [76][193/204]	Loss 0.4524 (0.4497)	
training:	Epoch: [76][194/204]	Loss 0.4558 (0.4498)	
training:	Epoch: [76][195/204]	Loss 0.4909 (0.4500)	
training:	Epoch: [76][196/204]	Loss 0.5410 (0.4505)	
training:	Epoch: [76][197/204]	Loss 0.3753 (0.4501)	
training:	Epoch: [76][198/204]	Loss 0.3816 (0.4497)	
training:	Epoch: [76][199/204]	Loss 0.4308 (0.4496)	
training:	Epoch: [76][200/204]	Loss 0.5880 (0.4503)	
training:	Epoch: [76][201/204]	Loss 0.5054 (0.4506)	
training:	Epoch: [76][202/204]	Loss 0.5079 (0.4509)	
training:	Epoch: [76][203/204]	Loss 0.3640 (0.4504)	
training:	Epoch: [76][204/204]	Loss 0.4789 (0.4506)	
Training:	 Loss: 0.4499

Training:	 ACC: 0.8050 0.8056 0.8195 0.7905
Validation:	 ACC: 0.7879 0.7887 0.8055 0.7702
Validation:	 Best_BACC: 0.7879 0.7887 0.8055 0.7702
Validation:	 Loss: 0.4619
Pretraining:	Epoch 77/120
----------
training:	Epoch: [77][1/204]	Loss 0.5166 (0.5166)	
training:	Epoch: [77][2/204]	Loss 0.3563 (0.4365)	
training:	Epoch: [77][3/204]	Loss 0.3505 (0.4078)	
training:	Epoch: [77][4/204]	Loss 0.4726 (0.4240)	
training:	Epoch: [77][5/204]	Loss 0.4848 (0.4362)	
training:	Epoch: [77][6/204]	Loss 0.3021 (0.4138)	
training:	Epoch: [77][7/204]	Loss 0.3441 (0.4039)	
training:	Epoch: [77][8/204]	Loss 0.5274 (0.4193)	
training:	Epoch: [77][9/204]	Loss 0.5679 (0.4358)	
training:	Epoch: [77][10/204]	Loss 0.3581 (0.4280)	
training:	Epoch: [77][11/204]	Loss 0.4460 (0.4297)	
training:	Epoch: [77][12/204]	Loss 0.3655 (0.4243)	
training:	Epoch: [77][13/204]	Loss 0.5777 (0.4361)	
training:	Epoch: [77][14/204]	Loss 0.4556 (0.4375)	
training:	Epoch: [77][15/204]	Loss 0.4054 (0.4354)	
training:	Epoch: [77][16/204]	Loss 0.3388 (0.4293)	
training:	Epoch: [77][17/204]	Loss 0.5551 (0.4367)	
training:	Epoch: [77][18/204]	Loss 0.4692 (0.4385)	
training:	Epoch: [77][19/204]	Loss 0.5271 (0.4432)	
training:	Epoch: [77][20/204]	Loss 0.4802 (0.4451)	
training:	Epoch: [77][21/204]	Loss 0.3934 (0.4426)	
training:	Epoch: [77][22/204]	Loss 0.4438 (0.4426)	
training:	Epoch: [77][23/204]	Loss 0.4736 (0.4440)	
training:	Epoch: [77][24/204]	Loss 0.4158 (0.4428)	
training:	Epoch: [77][25/204]	Loss 0.3992 (0.4411)	
training:	Epoch: [77][26/204]	Loss 0.4472 (0.4413)	
training:	Epoch: [77][27/204]	Loss 0.4395 (0.4412)	
training:	Epoch: [77][28/204]	Loss 0.4077 (0.4400)	
training:	Epoch: [77][29/204]	Loss 0.5017 (0.4422)	
training:	Epoch: [77][30/204]	Loss 0.4212 (0.4415)	
training:	Epoch: [77][31/204]	Loss 0.3061 (0.4371)	
training:	Epoch: [77][32/204]	Loss 0.3105 (0.4332)	
training:	Epoch: [77][33/204]	Loss 0.5512 (0.4367)	
training:	Epoch: [77][34/204]	Loss 0.4235 (0.4363)	
training:	Epoch: [77][35/204]	Loss 0.5128 (0.4385)	
training:	Epoch: [77][36/204]	Loss 0.4339 (0.4384)	
training:	Epoch: [77][37/204]	Loss 0.4692 (0.4392)	
training:	Epoch: [77][38/204]	Loss 0.5132 (0.4412)	
training:	Epoch: [77][39/204]	Loss 0.3855 (0.4397)	
training:	Epoch: [77][40/204]	Loss 0.5944 (0.4436)	
training:	Epoch: [77][41/204]	Loss 0.4530 (0.4438)	
training:	Epoch: [77][42/204]	Loss 0.2549 (0.4393)	
training:	Epoch: [77][43/204]	Loss 0.5445 (0.4418)	
training:	Epoch: [77][44/204]	Loss 0.3416 (0.4395)	
training:	Epoch: [77][45/204]	Loss 0.3202 (0.4369)	
training:	Epoch: [77][46/204]	Loss 0.4294 (0.4367)	
training:	Epoch: [77][47/204]	Loss 0.4666 (0.4373)	
training:	Epoch: [77][48/204]	Loss 0.3596 (0.4357)	
training:	Epoch: [77][49/204]	Loss 0.5600 (0.4383)	
training:	Epoch: [77][50/204]	Loss 0.3719 (0.4369)	
training:	Epoch: [77][51/204]	Loss 0.4724 (0.4376)	
training:	Epoch: [77][52/204]	Loss 0.3260 (0.4355)	
training:	Epoch: [77][53/204]	Loss 0.5226 (0.4371)	
training:	Epoch: [77][54/204]	Loss 0.5441 (0.4391)	
training:	Epoch: [77][55/204]	Loss 0.4086 (0.4385)	
training:	Epoch: [77][56/204]	Loss 0.4246 (0.4383)	
training:	Epoch: [77][57/204]	Loss 0.3418 (0.4366)	
training:	Epoch: [77][58/204]	Loss 0.5042 (0.4378)	
training:	Epoch: [77][59/204]	Loss 0.2453 (0.4345)	
training:	Epoch: [77][60/204]	Loss 0.4720 (0.4351)	
training:	Epoch: [77][61/204]	Loss 0.4576 (0.4355)	
training:	Epoch: [77][62/204]	Loss 0.3656 (0.4344)	
training:	Epoch: [77][63/204]	Loss 0.3487 (0.4330)	
training:	Epoch: [77][64/204]	Loss 0.3222 (0.4313)	
training:	Epoch: [77][65/204]	Loss 0.3943 (0.4307)	
training:	Epoch: [77][66/204]	Loss 0.5460 (0.4325)	
training:	Epoch: [77][67/204]	Loss 0.4544 (0.4328)	
training:	Epoch: [77][68/204]	Loss 0.3970 (0.4323)	
training:	Epoch: [77][69/204]	Loss 0.5274 (0.4336)	
training:	Epoch: [77][70/204]	Loss 0.4861 (0.4344)	
training:	Epoch: [77][71/204]	Loss 0.5237 (0.4356)	
training:	Epoch: [77][72/204]	Loss 0.4954 (0.4365)	
training:	Epoch: [77][73/204]	Loss 0.4369 (0.4365)	
training:	Epoch: [77][74/204]	Loss 0.5269 (0.4377)	
training:	Epoch: [77][75/204]	Loss 0.4892 (0.4384)	
training:	Epoch: [77][76/204]	Loss 0.4859 (0.4390)	
training:	Epoch: [77][77/204]	Loss 0.6324 (0.4415)	
training:	Epoch: [77][78/204]	Loss 0.3798 (0.4407)	
training:	Epoch: [77][79/204]	Loss 0.3561 (0.4397)	
training:	Epoch: [77][80/204]	Loss 0.3951 (0.4391)	
training:	Epoch: [77][81/204]	Loss 0.3621 (0.4382)	
training:	Epoch: [77][82/204]	Loss 0.4261 (0.4380)	
training:	Epoch: [77][83/204]	Loss 0.2312 (0.4355)	
training:	Epoch: [77][84/204]	Loss 0.4356 (0.4355)	
training:	Epoch: [77][85/204]	Loss 0.4002 (0.4351)	
training:	Epoch: [77][86/204]	Loss 0.3592 (0.4342)	
training:	Epoch: [77][87/204]	Loss 0.5256 (0.4353)	
training:	Epoch: [77][88/204]	Loss 0.4217 (0.4351)	
training:	Epoch: [77][89/204]	Loss 0.4399 (0.4352)	
training:	Epoch: [77][90/204]	Loss 0.6975 (0.4381)	
training:	Epoch: [77][91/204]	Loss 0.4821 (0.4386)	
training:	Epoch: [77][92/204]	Loss 0.4025 (0.4382)	
training:	Epoch: [77][93/204]	Loss 0.4136 (0.4379)	
training:	Epoch: [77][94/204]	Loss 0.3893 (0.4374)	
training:	Epoch: [77][95/204]	Loss 0.4284 (0.4373)	
training:	Epoch: [77][96/204]	Loss 0.3832 (0.4367)	
training:	Epoch: [77][97/204]	Loss 0.3950 (0.4363)	
training:	Epoch: [77][98/204]	Loss 0.4460 (0.4364)	
training:	Epoch: [77][99/204]	Loss 0.5840 (0.4379)	
training:	Epoch: [77][100/204]	Loss 0.4441 (0.4380)	
training:	Epoch: [77][101/204]	Loss 0.4673 (0.4383)	
training:	Epoch: [77][102/204]	Loss 0.4728 (0.4386)	
training:	Epoch: [77][103/204]	Loss 0.3732 (0.4380)	
training:	Epoch: [77][104/204]	Loss 0.6329 (0.4398)	
training:	Epoch: [77][105/204]	Loss 0.4853 (0.4403)	
training:	Epoch: [77][106/204]	Loss 0.3717 (0.4396)	
training:	Epoch: [77][107/204]	Loss 0.5287 (0.4404)	
training:	Epoch: [77][108/204]	Loss 0.4354 (0.4404)	
training:	Epoch: [77][109/204]	Loss 0.3329 (0.4394)	
training:	Epoch: [77][110/204]	Loss 0.4903 (0.4399)	
training:	Epoch: [77][111/204]	Loss 0.3059 (0.4387)	
training:	Epoch: [77][112/204]	Loss 0.3677 (0.4380)	
training:	Epoch: [77][113/204]	Loss 0.6119 (0.4396)	
training:	Epoch: [77][114/204]	Loss 0.4938 (0.4401)	
training:	Epoch: [77][115/204]	Loss 0.5901 (0.4414)	
training:	Epoch: [77][116/204]	Loss 0.5050 (0.4419)	
training:	Epoch: [77][117/204]	Loss 0.3918 (0.4415)	
training:	Epoch: [77][118/204]	Loss 0.3586 (0.4408)	
training:	Epoch: [77][119/204]	Loss 0.5584 (0.4418)	
training:	Epoch: [77][120/204]	Loss 0.5609 (0.4428)	
training:	Epoch: [77][121/204]	Loss 0.3862 (0.4423)	
training:	Epoch: [77][122/204]	Loss 0.4216 (0.4421)	
training:	Epoch: [77][123/204]	Loss 0.3568 (0.4414)	
training:	Epoch: [77][124/204]	Loss 0.4524 (0.4415)	
training:	Epoch: [77][125/204]	Loss 0.6761 (0.4434)	
training:	Epoch: [77][126/204]	Loss 0.5454 (0.4442)	
training:	Epoch: [77][127/204]	Loss 0.5256 (0.4448)	
training:	Epoch: [77][128/204]	Loss 0.4776 (0.4451)	
training:	Epoch: [77][129/204]	Loss 0.5890 (0.4462)	
training:	Epoch: [77][130/204]	Loss 0.3821 (0.4457)	
training:	Epoch: [77][131/204]	Loss 0.4295 (0.4456)	
training:	Epoch: [77][132/204]	Loss 0.4767 (0.4458)	
training:	Epoch: [77][133/204]	Loss 0.5182 (0.4464)	
training:	Epoch: [77][134/204]	Loss 0.5006 (0.4468)	
training:	Epoch: [77][135/204]	Loss 0.5825 (0.4478)	
training:	Epoch: [77][136/204]	Loss 0.4784 (0.4480)	
training:	Epoch: [77][137/204]	Loss 0.4106 (0.4477)	
training:	Epoch: [77][138/204]	Loss 0.4648 (0.4479)	
training:	Epoch: [77][139/204]	Loss 0.7548 (0.4501)	
training:	Epoch: [77][140/204]	Loss 0.6224 (0.4513)	
training:	Epoch: [77][141/204]	Loss 0.3789 (0.4508)	
training:	Epoch: [77][142/204]	Loss 0.2787 (0.4496)	
training:	Epoch: [77][143/204]	Loss 0.4945 (0.4499)	
training:	Epoch: [77][144/204]	Loss 0.4456 (0.4499)	
training:	Epoch: [77][145/204]	Loss 0.4293 (0.4497)	
training:	Epoch: [77][146/204]	Loss 0.4618 (0.4498)	
training:	Epoch: [77][147/204]	Loss 0.4173 (0.4496)	
training:	Epoch: [77][148/204]	Loss 0.4811 (0.4498)	
training:	Epoch: [77][149/204]	Loss 0.2658 (0.4486)	
training:	Epoch: [77][150/204]	Loss 0.2905 (0.4475)	
training:	Epoch: [77][151/204]	Loss 0.4292 (0.4474)	
training:	Epoch: [77][152/204]	Loss 0.4884 (0.4477)	
training:	Epoch: [77][153/204]	Loss 0.4479 (0.4477)	
training:	Epoch: [77][154/204]	Loss 0.6186 (0.4488)	
training:	Epoch: [77][155/204]	Loss 0.4121 (0.4485)	
training:	Epoch: [77][156/204]	Loss 0.3563 (0.4479)	
training:	Epoch: [77][157/204]	Loss 0.4678 (0.4481)	
training:	Epoch: [77][158/204]	Loss 0.5591 (0.4488)	
training:	Epoch: [77][159/204]	Loss 0.5079 (0.4491)	
training:	Epoch: [77][160/204]	Loss 0.4260 (0.4490)	
training:	Epoch: [77][161/204]	Loss 0.4969 (0.4493)	
training:	Epoch: [77][162/204]	Loss 0.4366 (0.4492)	
training:	Epoch: [77][163/204]	Loss 0.4942 (0.4495)	
training:	Epoch: [77][164/204]	Loss 0.3841 (0.4491)	
training:	Epoch: [77][165/204]	Loss 0.3386 (0.4484)	
training:	Epoch: [77][166/204]	Loss 0.5866 (0.4493)	
training:	Epoch: [77][167/204]	Loss 0.4940 (0.4495)	
training:	Epoch: [77][168/204]	Loss 0.3251 (0.4488)	
training:	Epoch: [77][169/204]	Loss 0.3989 (0.4485)	
training:	Epoch: [77][170/204]	Loss 0.4665 (0.4486)	
training:	Epoch: [77][171/204]	Loss 0.5312 (0.4491)	
training:	Epoch: [77][172/204]	Loss 0.3695 (0.4486)	
training:	Epoch: [77][173/204]	Loss 0.5665 (0.4493)	
training:	Epoch: [77][174/204]	Loss 0.4436 (0.4493)	
training:	Epoch: [77][175/204]	Loss 0.3088 (0.4485)	
training:	Epoch: [77][176/204]	Loss 0.4039 (0.4482)	
training:	Epoch: [77][177/204]	Loss 0.6224 (0.4492)	
training:	Epoch: [77][178/204]	Loss 0.4580 (0.4492)	
training:	Epoch: [77][179/204]	Loss 0.3348 (0.4486)	
training:	Epoch: [77][180/204]	Loss 0.5221 (0.4490)	
training:	Epoch: [77][181/204]	Loss 0.4526 (0.4490)	
training:	Epoch: [77][182/204]	Loss 0.4425 (0.4490)	
training:	Epoch: [77][183/204]	Loss 0.3015 (0.4482)	
training:	Epoch: [77][184/204]	Loss 0.3780 (0.4478)	
training:	Epoch: [77][185/204]	Loss 0.5324 (0.4483)	
training:	Epoch: [77][186/204]	Loss 0.3468 (0.4477)	
training:	Epoch: [77][187/204]	Loss 0.4792 (0.4479)	
training:	Epoch: [77][188/204]	Loss 0.5301 (0.4483)	
training:	Epoch: [77][189/204]	Loss 0.4218 (0.4482)	
training:	Epoch: [77][190/204]	Loss 0.5027 (0.4485)	
training:	Epoch: [77][191/204]	Loss 0.5324 (0.4489)	
training:	Epoch: [77][192/204]	Loss 0.4145 (0.4487)	
training:	Epoch: [77][193/204]	Loss 0.4653 (0.4488)	
training:	Epoch: [77][194/204]	Loss 0.4688 (0.4489)	
training:	Epoch: [77][195/204]	Loss 0.5308 (0.4493)	
training:	Epoch: [77][196/204]	Loss 0.4195 (0.4492)	
training:	Epoch: [77][197/204]	Loss 0.4603 (0.4492)	
training:	Epoch: [77][198/204]	Loss 0.3757 (0.4489)	
training:	Epoch: [77][199/204]	Loss 0.3773 (0.4485)	
training:	Epoch: [77][200/204]	Loss 0.4513 (0.4485)	
training:	Epoch: [77][201/204]	Loss 0.4246 (0.4484)	
training:	Epoch: [77][202/204]	Loss 0.4764 (0.4485)	
training:	Epoch: [77][203/204]	Loss 0.3869 (0.4482)	
training:	Epoch: [77][204/204]	Loss 0.5365 (0.4487)	
Training:	 Loss: 0.4480

Training:	 ACC: 0.8048 0.8051 0.8122 0.7975
Validation:	 ACC: 0.7865 0.7871 0.7984 0.7747
Validation:	 Best_BACC: 0.7879 0.7887 0.8055 0.7702
Validation:	 Loss: 0.4607
Pretraining:	Epoch 78/120
----------
training:	Epoch: [78][1/204]	Loss 0.2929 (0.2929)	
training:	Epoch: [78][2/204]	Loss 0.5837 (0.4383)	
training:	Epoch: [78][3/204]	Loss 0.5770 (0.4845)	
training:	Epoch: [78][4/204]	Loss 0.4191 (0.4682)	
training:	Epoch: [78][5/204]	Loss 0.4847 (0.4715)	
training:	Epoch: [78][6/204]	Loss 0.3763 (0.4556)	
training:	Epoch: [78][7/204]	Loss 0.3752 (0.4441)	
training:	Epoch: [78][8/204]	Loss 0.6265 (0.4669)	
training:	Epoch: [78][9/204]	Loss 0.5492 (0.4761)	
training:	Epoch: [78][10/204]	Loss 0.6334 (0.4918)	
training:	Epoch: [78][11/204]	Loss 0.4153 (0.4848)	
training:	Epoch: [78][12/204]	Loss 0.5354 (0.4891)	
training:	Epoch: [78][13/204]	Loss 0.5188 (0.4913)	
training:	Epoch: [78][14/204]	Loss 0.5425 (0.4950)	
training:	Epoch: [78][15/204]	Loss 0.4583 (0.4925)	
training:	Epoch: [78][16/204]	Loss 0.3844 (0.4858)	
training:	Epoch: [78][17/204]	Loss 0.4792 (0.4854)	
training:	Epoch: [78][18/204]	Loss 0.3497 (0.4779)	
training:	Epoch: [78][19/204]	Loss 0.3670 (0.4720)	
training:	Epoch: [78][20/204]	Loss 0.3894 (0.4679)	
training:	Epoch: [78][21/204]	Loss 0.4220 (0.4657)	
training:	Epoch: [78][22/204]	Loss 0.3798 (0.4618)	
training:	Epoch: [78][23/204]	Loss 0.6183 (0.4686)	
training:	Epoch: [78][24/204]	Loss 0.5170 (0.4706)	
training:	Epoch: [78][25/204]	Loss 0.4707 (0.4706)	
training:	Epoch: [78][26/204]	Loss 0.3864 (0.4674)	
training:	Epoch: [78][27/204]	Loss 0.4131 (0.4654)	
training:	Epoch: [78][28/204]	Loss 0.4346 (0.4643)	
training:	Epoch: [78][29/204]	Loss 0.3906 (0.4617)	
training:	Epoch: [78][30/204]	Loss 0.3609 (0.4584)	
training:	Epoch: [78][31/204]	Loss 0.3592 (0.4552)	
training:	Epoch: [78][32/204]	Loss 0.4835 (0.4561)	
training:	Epoch: [78][33/204]	Loss 0.3397 (0.4525)	
training:	Epoch: [78][34/204]	Loss 0.6827 (0.4593)	
training:	Epoch: [78][35/204]	Loss 0.5847 (0.4629)	
training:	Epoch: [78][36/204]	Loss 0.4415 (0.4623)	
training:	Epoch: [78][37/204]	Loss 0.3603 (0.4595)	
training:	Epoch: [78][38/204]	Loss 0.4672 (0.4597)	
training:	Epoch: [78][39/204]	Loss 0.4381 (0.4592)	
training:	Epoch: [78][40/204]	Loss 0.5987 (0.4627)	
training:	Epoch: [78][41/204]	Loss 0.4299 (0.4619)	
training:	Epoch: [78][42/204]	Loss 0.4934 (0.4626)	
training:	Epoch: [78][43/204]	Loss 0.2899 (0.4586)	
training:	Epoch: [78][44/204]	Loss 0.4793 (0.4591)	
training:	Epoch: [78][45/204]	Loss 0.4289 (0.4584)	
training:	Epoch: [78][46/204]	Loss 0.4487 (0.4582)	
training:	Epoch: [78][47/204]	Loss 0.5824 (0.4608)	
training:	Epoch: [78][48/204]	Loss 0.4985 (0.4616)	
training:	Epoch: [78][49/204]	Loss 0.4872 (0.4621)	
training:	Epoch: [78][50/204]	Loss 0.3961 (0.4608)	
training:	Epoch: [78][51/204]	Loss 0.4286 (0.4602)	
training:	Epoch: [78][52/204]	Loss 0.5262 (0.4615)	
training:	Epoch: [78][53/204]	Loss 0.4626 (0.4615)	
training:	Epoch: [78][54/204]	Loss 0.4949 (0.4621)	
training:	Epoch: [78][55/204]	Loss 0.5156 (0.4631)	
training:	Epoch: [78][56/204]	Loss 0.3310 (0.4607)	
training:	Epoch: [78][57/204]	Loss 0.2575 (0.4571)	
training:	Epoch: [78][58/204]	Loss 0.3147 (0.4547)	
training:	Epoch: [78][59/204]	Loss 0.6288 (0.4576)	
training:	Epoch: [78][60/204]	Loss 0.5124 (0.4586)	
training:	Epoch: [78][61/204]	Loss 0.4826 (0.4590)	
training:	Epoch: [78][62/204]	Loss 0.3477 (0.4572)	
training:	Epoch: [78][63/204]	Loss 0.5157 (0.4581)	
training:	Epoch: [78][64/204]	Loss 0.4004 (0.4572)	
training:	Epoch: [78][65/204]	Loss 0.4117 (0.4565)	
training:	Epoch: [78][66/204]	Loss 0.4401 (0.4562)	
training:	Epoch: [78][67/204]	Loss 0.4785 (0.4566)	
training:	Epoch: [78][68/204]	Loss 0.3837 (0.4555)	
training:	Epoch: [78][69/204]	Loss 0.4971 (0.4561)	
training:	Epoch: [78][70/204]	Loss 0.3246 (0.4542)	
training:	Epoch: [78][71/204]	Loss 0.4096 (0.4536)	
training:	Epoch: [78][72/204]	Loss 0.4754 (0.4539)	
training:	Epoch: [78][73/204]	Loss 0.5311 (0.4550)	
training:	Epoch: [78][74/204]	Loss 0.3887 (0.4541)	
training:	Epoch: [78][75/204]	Loss 0.5255 (0.4550)	
training:	Epoch: [78][76/204]	Loss 0.4953 (0.4555)	
training:	Epoch: [78][77/204]	Loss 0.3296 (0.4539)	
training:	Epoch: [78][78/204]	Loss 0.3922 (0.4531)	
training:	Epoch: [78][79/204]	Loss 0.2976 (0.4511)	
training:	Epoch: [78][80/204]	Loss 0.4133 (0.4507)	
training:	Epoch: [78][81/204]	Loss 0.4749 (0.4510)	
training:	Epoch: [78][82/204]	Loss 0.3557 (0.4498)	
training:	Epoch: [78][83/204]	Loss 0.4185 (0.4494)	
training:	Epoch: [78][84/204]	Loss 0.4532 (0.4495)	
training:	Epoch: [78][85/204]	Loss 0.4947 (0.4500)	
training:	Epoch: [78][86/204]	Loss 0.3968 (0.4494)	
training:	Epoch: [78][87/204]	Loss 0.4698 (0.4496)	
training:	Epoch: [78][88/204]	Loss 0.3893 (0.4489)	
training:	Epoch: [78][89/204]	Loss 0.3982 (0.4484)	
training:	Epoch: [78][90/204]	Loss 0.4148 (0.4480)	
training:	Epoch: [78][91/204]	Loss 0.3567 (0.4470)	
training:	Epoch: [78][92/204]	Loss 0.4052 (0.4465)	
training:	Epoch: [78][93/204]	Loss 0.3817 (0.4458)	
training:	Epoch: [78][94/204]	Loss 0.3973 (0.4453)	
training:	Epoch: [78][95/204]	Loss 0.4031 (0.4449)	
training:	Epoch: [78][96/204]	Loss 0.3599 (0.4440)	
training:	Epoch: [78][97/204]	Loss 0.3548 (0.4431)	
training:	Epoch: [78][98/204]	Loss 0.4968 (0.4436)	
training:	Epoch: [78][99/204]	Loss 0.4317 (0.4435)	
training:	Epoch: [78][100/204]	Loss 0.4413 (0.4435)	
training:	Epoch: [78][101/204]	Loss 0.3414 (0.4425)	
training:	Epoch: [78][102/204]	Loss 0.4491 (0.4425)	
training:	Epoch: [78][103/204]	Loss 0.4913 (0.4430)	
training:	Epoch: [78][104/204]	Loss 0.4060 (0.4427)	
training:	Epoch: [78][105/204]	Loss 0.4598 (0.4428)	
training:	Epoch: [78][106/204]	Loss 0.3263 (0.4417)	
training:	Epoch: [78][107/204]	Loss 0.3011 (0.4404)	
training:	Epoch: [78][108/204]	Loss 0.6282 (0.4421)	
training:	Epoch: [78][109/204]	Loss 0.5434 (0.4431)	
training:	Epoch: [78][110/204]	Loss 0.3677 (0.4424)	
training:	Epoch: [78][111/204]	Loss 0.5309 (0.4432)	
training:	Epoch: [78][112/204]	Loss 0.3620 (0.4425)	
training:	Epoch: [78][113/204]	Loss 0.3503 (0.4416)	
training:	Epoch: [78][114/204]	Loss 0.5829 (0.4429)	
training:	Epoch: [78][115/204]	Loss 0.6668 (0.4448)	
training:	Epoch: [78][116/204]	Loss 0.5010 (0.4453)	
training:	Epoch: [78][117/204]	Loss 0.5569 (0.4463)	
training:	Epoch: [78][118/204]	Loss 0.5270 (0.4470)	
training:	Epoch: [78][119/204]	Loss 0.4409 (0.4469)	
training:	Epoch: [78][120/204]	Loss 0.4400 (0.4468)	
training:	Epoch: [78][121/204]	Loss 0.4691 (0.4470)	
training:	Epoch: [78][122/204]	Loss 0.3687 (0.4464)	
training:	Epoch: [78][123/204]	Loss 0.3328 (0.4455)	
training:	Epoch: [78][124/204]	Loss 0.5420 (0.4462)	
training:	Epoch: [78][125/204]	Loss 0.4118 (0.4460)	
training:	Epoch: [78][126/204]	Loss 0.3860 (0.4455)	
training:	Epoch: [78][127/204]	Loss 0.5835 (0.4466)	
training:	Epoch: [78][128/204]	Loss 0.4669 (0.4467)	
training:	Epoch: [78][129/204]	Loss 0.5503 (0.4475)	
training:	Epoch: [78][130/204]	Loss 0.2930 (0.4464)	
training:	Epoch: [78][131/204]	Loss 0.6278 (0.4477)	
training:	Epoch: [78][132/204]	Loss 0.5475 (0.4485)	
training:	Epoch: [78][133/204]	Loss 0.4836 (0.4488)	
training:	Epoch: [78][134/204]	Loss 0.4907 (0.4491)	
training:	Epoch: [78][135/204]	Loss 0.5324 (0.4497)	
training:	Epoch: [78][136/204]	Loss 0.5426 (0.4504)	
training:	Epoch: [78][137/204]	Loss 0.2949 (0.4492)	
training:	Epoch: [78][138/204]	Loss 0.4119 (0.4490)	
training:	Epoch: [78][139/204]	Loss 0.3899 (0.4485)	
training:	Epoch: [78][140/204]	Loss 0.5133 (0.4490)	
training:	Epoch: [78][141/204]	Loss 0.4749 (0.4492)	
training:	Epoch: [78][142/204]	Loss 0.3636 (0.4486)	
training:	Epoch: [78][143/204]	Loss 0.2680 (0.4473)	
training:	Epoch: [78][144/204]	Loss 0.3522 (0.4467)	
training:	Epoch: [78][145/204]	Loss 0.3593 (0.4461)	
training:	Epoch: [78][146/204]	Loss 0.5534 (0.4468)	
training:	Epoch: [78][147/204]	Loss 0.4889 (0.4471)	
training:	Epoch: [78][148/204]	Loss 0.4108 (0.4468)	
training:	Epoch: [78][149/204]	Loss 0.5986 (0.4479)	
training:	Epoch: [78][150/204]	Loss 0.4053 (0.4476)	
training:	Epoch: [78][151/204]	Loss 0.5050 (0.4479)	
training:	Epoch: [78][152/204]	Loss 0.5238 (0.4484)	
training:	Epoch: [78][153/204]	Loss 0.5228 (0.4489)	
training:	Epoch: [78][154/204]	Loss 0.4886 (0.4492)	
training:	Epoch: [78][155/204]	Loss 0.3770 (0.4487)	
training:	Epoch: [78][156/204]	Loss 0.2793 (0.4476)	
training:	Epoch: [78][157/204]	Loss 0.3949 (0.4473)	
training:	Epoch: [78][158/204]	Loss 0.4984 (0.4476)	
training:	Epoch: [78][159/204]	Loss 0.4826 (0.4478)	
training:	Epoch: [78][160/204]	Loss 0.5529 (0.4485)	
training:	Epoch: [78][161/204]	Loss 0.4092 (0.4483)	
training:	Epoch: [78][162/204]	Loss 0.4607 (0.4483)	
training:	Epoch: [78][163/204]	Loss 0.3815 (0.4479)	
training:	Epoch: [78][164/204]	Loss 0.4981 (0.4482)	
training:	Epoch: [78][165/204]	Loss 0.3098 (0.4474)	
training:	Epoch: [78][166/204]	Loss 0.5269 (0.4479)	
training:	Epoch: [78][167/204]	Loss 0.4400 (0.4478)	
training:	Epoch: [78][168/204]	Loss 0.4439 (0.4478)	
training:	Epoch: [78][169/204]	Loss 0.4685 (0.4479)	
training:	Epoch: [78][170/204]	Loss 0.3723 (0.4475)	
training:	Epoch: [78][171/204]	Loss 0.4477 (0.4475)	
training:	Epoch: [78][172/204]	Loss 0.5166 (0.4479)	
training:	Epoch: [78][173/204]	Loss 0.4695 (0.4480)	
training:	Epoch: [78][174/204]	Loss 0.4720 (0.4481)	
training:	Epoch: [78][175/204]	Loss 0.4392 (0.4481)	
training:	Epoch: [78][176/204]	Loss 0.4707 (0.4482)	
training:	Epoch: [78][177/204]	Loss 0.4140 (0.4480)	
training:	Epoch: [78][178/204]	Loss 0.4565 (0.4481)	
training:	Epoch: [78][179/204]	Loss 0.4358 (0.4480)	
training:	Epoch: [78][180/204]	Loss 0.4413 (0.4480)	
training:	Epoch: [78][181/204]	Loss 0.5082 (0.4483)	
training:	Epoch: [78][182/204]	Loss 0.4334 (0.4482)	
training:	Epoch: [78][183/204]	Loss 0.4076 (0.4480)	
training:	Epoch: [78][184/204]	Loss 0.5365 (0.4485)	
training:	Epoch: [78][185/204]	Loss 0.3796 (0.4481)	
training:	Epoch: [78][186/204]	Loss 0.4753 (0.4483)	
training:	Epoch: [78][187/204]	Loss 0.7145 (0.4497)	
training:	Epoch: [78][188/204]	Loss 0.4683 (0.4498)	
training:	Epoch: [78][189/204]	Loss 0.3028 (0.4490)	
training:	Epoch: [78][190/204]	Loss 0.3954 (0.4487)	
training:	Epoch: [78][191/204]	Loss 0.3839 (0.4484)	
training:	Epoch: [78][192/204]	Loss 0.4969 (0.4486)	
training:	Epoch: [78][193/204]	Loss 0.4912 (0.4489)	
training:	Epoch: [78][194/204]	Loss 0.3207 (0.4482)	
training:	Epoch: [78][195/204]	Loss 0.4987 (0.4484)	
training:	Epoch: [78][196/204]	Loss 0.4912 (0.4487)	
training:	Epoch: [78][197/204]	Loss 0.4852 (0.4489)	
training:	Epoch: [78][198/204]	Loss 0.2996 (0.4481)	
training:	Epoch: [78][199/204]	Loss 0.3780 (0.4477)	
training:	Epoch: [78][200/204]	Loss 0.5868 (0.4484)	
training:	Epoch: [78][201/204]	Loss 0.5085 (0.4487)	
training:	Epoch: [78][202/204]	Loss 0.4075 (0.4485)	
training:	Epoch: [78][203/204]	Loss 0.5571 (0.4491)	
training:	Epoch: [78][204/204]	Loss 0.5350 (0.4495)	
Training:	 Loss: 0.4488

Training:	 ACC: 0.8075 0.8079 0.8180 0.7969
Validation:	 ACC: 0.7875 0.7881 0.8014 0.7735
Validation:	 Best_BACC: 0.7879 0.7887 0.8055 0.7702
Validation:	 Loss: 0.4595
Pretraining:	Epoch 79/120
----------
training:	Epoch: [79][1/204]	Loss 0.3197 (0.3197)	
training:	Epoch: [79][2/204]	Loss 0.4695 (0.3946)	
training:	Epoch: [79][3/204]	Loss 0.4715 (0.4202)	
training:	Epoch: [79][4/204]	Loss 0.4589 (0.4299)	
training:	Epoch: [79][5/204]	Loss 0.4404 (0.4320)	
training:	Epoch: [79][6/204]	Loss 0.5582 (0.4530)	
training:	Epoch: [79][7/204]	Loss 0.5612 (0.4685)	
training:	Epoch: [79][8/204]	Loss 0.4025 (0.4602)	
training:	Epoch: [79][9/204]	Loss 0.3231 (0.4450)	
training:	Epoch: [79][10/204]	Loss 0.4791 (0.4484)	
training:	Epoch: [79][11/204]	Loss 0.3958 (0.4436)	
training:	Epoch: [79][12/204]	Loss 0.4125 (0.4410)	
training:	Epoch: [79][13/204]	Loss 0.4918 (0.4449)	
training:	Epoch: [79][14/204]	Loss 0.3508 (0.4382)	
training:	Epoch: [79][15/204]	Loss 0.4531 (0.4392)	
training:	Epoch: [79][16/204]	Loss 0.2744 (0.4289)	
training:	Epoch: [79][17/204]	Loss 0.5032 (0.4333)	
training:	Epoch: [79][18/204]	Loss 0.5242 (0.4383)	
training:	Epoch: [79][19/204]	Loss 0.4142 (0.4371)	
training:	Epoch: [79][20/204]	Loss 0.4724 (0.4388)	
training:	Epoch: [79][21/204]	Loss 0.3959 (0.4368)	
training:	Epoch: [79][22/204]	Loss 0.4370 (0.4368)	
training:	Epoch: [79][23/204]	Loss 0.4436 (0.4371)	
training:	Epoch: [79][24/204]	Loss 0.4825 (0.4390)	
training:	Epoch: [79][25/204]	Loss 0.4857 (0.4408)	
training:	Epoch: [79][26/204]	Loss 0.4023 (0.4394)	
training:	Epoch: [79][27/204]	Loss 0.5236 (0.4425)	
training:	Epoch: [79][28/204]	Loss 0.4573 (0.4430)	
training:	Epoch: [79][29/204]	Loss 0.3146 (0.4386)	
training:	Epoch: [79][30/204]	Loss 0.3579 (0.4359)	
training:	Epoch: [79][31/204]	Loss 0.4924 (0.4377)	
training:	Epoch: [79][32/204]	Loss 0.4298 (0.4375)	
training:	Epoch: [79][33/204]	Loss 0.4385 (0.4375)	
training:	Epoch: [79][34/204]	Loss 0.4042 (0.4365)	
training:	Epoch: [79][35/204]	Loss 0.3696 (0.4346)	
training:	Epoch: [79][36/204]	Loss 0.4710 (0.4356)	
training:	Epoch: [79][37/204]	Loss 0.5013 (0.4374)	
training:	Epoch: [79][38/204]	Loss 0.2926 (0.4336)	
training:	Epoch: [79][39/204]	Loss 0.6828 (0.4400)	
training:	Epoch: [79][40/204]	Loss 0.4407 (0.4400)	
training:	Epoch: [79][41/204]	Loss 0.4435 (0.4401)	
training:	Epoch: [79][42/204]	Loss 0.5901 (0.4436)	
training:	Epoch: [79][43/204]	Loss 0.5904 (0.4471)	
training:	Epoch: [79][44/204]	Loss 0.3571 (0.4450)	
training:	Epoch: [79][45/204]	Loss 0.4813 (0.4458)	
training:	Epoch: [79][46/204]	Loss 0.3814 (0.4444)	
training:	Epoch: [79][47/204]	Loss 0.3553 (0.4425)	
training:	Epoch: [79][48/204]	Loss 0.4018 (0.4417)	
training:	Epoch: [79][49/204]	Loss 0.5457 (0.4438)	
training:	Epoch: [79][50/204]	Loss 0.5417 (0.4458)	
training:	Epoch: [79][51/204]	Loss 0.4921 (0.4467)	
training:	Epoch: [79][52/204]	Loss 0.4629 (0.4470)	
training:	Epoch: [79][53/204]	Loss 0.3734 (0.4456)	
training:	Epoch: [79][54/204]	Loss 0.5326 (0.4472)	
training:	Epoch: [79][55/204]	Loss 0.3662 (0.4457)	
training:	Epoch: [79][56/204]	Loss 0.5657 (0.4479)	
training:	Epoch: [79][57/204]	Loss 0.4248 (0.4475)	
training:	Epoch: [79][58/204]	Loss 0.5150 (0.4486)	
training:	Epoch: [79][59/204]	Loss 0.4724 (0.4490)	
training:	Epoch: [79][60/204]	Loss 0.4992 (0.4499)	
training:	Epoch: [79][61/204]	Loss 0.3443 (0.4481)	
training:	Epoch: [79][62/204]	Loss 0.5512 (0.4498)	
training:	Epoch: [79][63/204]	Loss 0.4480 (0.4498)	
training:	Epoch: [79][64/204]	Loss 0.5655 (0.4516)	
training:	Epoch: [79][65/204]	Loss 0.5314 (0.4528)	
training:	Epoch: [79][66/204]	Loss 0.2878 (0.4503)	
training:	Epoch: [79][67/204]	Loss 0.5021 (0.4511)	
training:	Epoch: [79][68/204]	Loss 0.5462 (0.4525)	
training:	Epoch: [79][69/204]	Loss 0.3994 (0.4517)	
training:	Epoch: [79][70/204]	Loss 0.5282 (0.4528)	
training:	Epoch: [79][71/204]	Loss 0.5518 (0.4542)	
training:	Epoch: [79][72/204]	Loss 0.3060 (0.4521)	
training:	Epoch: [79][73/204]	Loss 0.3566 (0.4508)	
training:	Epoch: [79][74/204]	Loss 0.4174 (0.4504)	
training:	Epoch: [79][75/204]	Loss 0.3836 (0.4495)	
training:	Epoch: [79][76/204]	Loss 0.5319 (0.4506)	
training:	Epoch: [79][77/204]	Loss 0.3012 (0.4486)	
training:	Epoch: [79][78/204]	Loss 0.4122 (0.4482)	
training:	Epoch: [79][79/204]	Loss 0.4766 (0.4485)	
training:	Epoch: [79][80/204]	Loss 0.3805 (0.4477)	
training:	Epoch: [79][81/204]	Loss 0.4689 (0.4479)	
training:	Epoch: [79][82/204]	Loss 0.3887 (0.4472)	
training:	Epoch: [79][83/204]	Loss 0.4160 (0.4468)	
training:	Epoch: [79][84/204]	Loss 0.4676 (0.4471)	
training:	Epoch: [79][85/204]	Loss 0.5461 (0.4483)	
training:	Epoch: [79][86/204]	Loss 0.2702 (0.4462)	
training:	Epoch: [79][87/204]	Loss 0.3931 (0.4456)	
training:	Epoch: [79][88/204]	Loss 0.4925 (0.4461)	
training:	Epoch: [79][89/204]	Loss 0.6336 (0.4482)	
training:	Epoch: [79][90/204]	Loss 0.4335 (0.4481)	
training:	Epoch: [79][91/204]	Loss 0.2468 (0.4458)	
training:	Epoch: [79][92/204]	Loss 0.3701 (0.4450)	
training:	Epoch: [79][93/204]	Loss 0.3211 (0.4437)	
training:	Epoch: [79][94/204]	Loss 0.5615 (0.4449)	
training:	Epoch: [79][95/204]	Loss 0.5683 (0.4462)	
training:	Epoch: [79][96/204]	Loss 0.4779 (0.4466)	
training:	Epoch: [79][97/204]	Loss 0.4096 (0.4462)	
training:	Epoch: [79][98/204]	Loss 0.4447 (0.4462)	
training:	Epoch: [79][99/204]	Loss 0.4469 (0.4462)	
training:	Epoch: [79][100/204]	Loss 0.6158 (0.4479)	
training:	Epoch: [79][101/204]	Loss 0.3238 (0.4466)	
training:	Epoch: [79][102/204]	Loss 0.4178 (0.4464)	
training:	Epoch: [79][103/204]	Loss 0.3368 (0.4453)	
training:	Epoch: [79][104/204]	Loss 0.5407 (0.4462)	
training:	Epoch: [79][105/204]	Loss 0.3908 (0.4457)	
training:	Epoch: [79][106/204]	Loss 0.4624 (0.4458)	
training:	Epoch: [79][107/204]	Loss 0.4274 (0.4457)	
training:	Epoch: [79][108/204]	Loss 0.4839 (0.4460)	
training:	Epoch: [79][109/204]	Loss 0.5270 (0.4468)	
training:	Epoch: [79][110/204]	Loss 0.5462 (0.4477)	
training:	Epoch: [79][111/204]	Loss 0.4949 (0.4481)	
training:	Epoch: [79][112/204]	Loss 0.6433 (0.4498)	
training:	Epoch: [79][113/204]	Loss 0.5199 (0.4505)	
training:	Epoch: [79][114/204]	Loss 0.4246 (0.4502)	
training:	Epoch: [79][115/204]	Loss 0.5481 (0.4511)	
training:	Epoch: [79][116/204]	Loss 0.4246 (0.4509)	
training:	Epoch: [79][117/204]	Loss 0.4109 (0.4505)	
training:	Epoch: [79][118/204]	Loss 0.4437 (0.4505)	
training:	Epoch: [79][119/204]	Loss 0.4741 (0.4507)	
training:	Epoch: [79][120/204]	Loss 0.3962 (0.4502)	
training:	Epoch: [79][121/204]	Loss 0.4712 (0.4504)	
training:	Epoch: [79][122/204]	Loss 0.4542 (0.4504)	
training:	Epoch: [79][123/204]	Loss 0.4871 (0.4507)	
training:	Epoch: [79][124/204]	Loss 0.3827 (0.4502)	
training:	Epoch: [79][125/204]	Loss 0.2751 (0.4488)	
training:	Epoch: [79][126/204]	Loss 0.2694 (0.4473)	
training:	Epoch: [79][127/204]	Loss 0.4028 (0.4470)	
training:	Epoch: [79][128/204]	Loss 0.4409 (0.4469)	
training:	Epoch: [79][129/204]	Loss 0.5379 (0.4476)	
training:	Epoch: [79][130/204]	Loss 0.4813 (0.4479)	
training:	Epoch: [79][131/204]	Loss 0.5461 (0.4486)	
training:	Epoch: [79][132/204]	Loss 0.5920 (0.4497)	
training:	Epoch: [79][133/204]	Loss 0.6292 (0.4511)	
training:	Epoch: [79][134/204]	Loss 0.3571 (0.4504)	
training:	Epoch: [79][135/204]	Loss 0.3933 (0.4500)	
training:	Epoch: [79][136/204]	Loss 0.5083 (0.4504)	
training:	Epoch: [79][137/204]	Loss 0.4487 (0.4504)	
training:	Epoch: [79][138/204]	Loss 0.3377 (0.4496)	
training:	Epoch: [79][139/204]	Loss 0.4431 (0.4495)	
training:	Epoch: [79][140/204]	Loss 0.3791 (0.4490)	
training:	Epoch: [79][141/204]	Loss 0.3480 (0.4483)	
training:	Epoch: [79][142/204]	Loss 0.3708 (0.4478)	
training:	Epoch: [79][143/204]	Loss 0.6300 (0.4490)	
training:	Epoch: [79][144/204]	Loss 0.3922 (0.4486)	
training:	Epoch: [79][145/204]	Loss 0.3534 (0.4480)	
training:	Epoch: [79][146/204]	Loss 0.3537 (0.4473)	
training:	Epoch: [79][147/204]	Loss 0.3004 (0.4463)	
training:	Epoch: [79][148/204]	Loss 0.4663 (0.4465)	
training:	Epoch: [79][149/204]	Loss 0.3154 (0.4456)	
training:	Epoch: [79][150/204]	Loss 0.3807 (0.4452)	
training:	Epoch: [79][151/204]	Loss 0.4367 (0.4451)	
training:	Epoch: [79][152/204]	Loss 0.5968 (0.4461)	
training:	Epoch: [79][153/204]	Loss 0.4134 (0.4459)	
training:	Epoch: [79][154/204]	Loss 0.6086 (0.4469)	
training:	Epoch: [79][155/204]	Loss 0.2794 (0.4459)	
training:	Epoch: [79][156/204]	Loss 0.3416 (0.4452)	
training:	Epoch: [79][157/204]	Loss 0.2919 (0.4442)	
training:	Epoch: [79][158/204]	Loss 0.5705 (0.4450)	
training:	Epoch: [79][159/204]	Loss 0.5792 (0.4459)	
training:	Epoch: [79][160/204]	Loss 0.4339 (0.4458)	
training:	Epoch: [79][161/204]	Loss 0.5359 (0.4463)	
training:	Epoch: [79][162/204]	Loss 0.4999 (0.4467)	
training:	Epoch: [79][163/204]	Loss 0.3660 (0.4462)	
training:	Epoch: [79][164/204]	Loss 0.5339 (0.4467)	
training:	Epoch: [79][165/204]	Loss 0.3588 (0.4462)	
training:	Epoch: [79][166/204]	Loss 0.6838 (0.4476)	
training:	Epoch: [79][167/204]	Loss 0.5330 (0.4481)	
training:	Epoch: [79][168/204]	Loss 0.3277 (0.4474)	
training:	Epoch: [79][169/204]	Loss 0.3589 (0.4469)	
training:	Epoch: [79][170/204]	Loss 0.3589 (0.4464)	
training:	Epoch: [79][171/204]	Loss 0.5002 (0.4467)	
training:	Epoch: [79][172/204]	Loss 0.6119 (0.4476)	
training:	Epoch: [79][173/204]	Loss 0.3531 (0.4471)	
training:	Epoch: [79][174/204]	Loss 0.4272 (0.4470)	
training:	Epoch: [79][175/204]	Loss 0.4950 (0.4473)	
training:	Epoch: [79][176/204]	Loss 0.4002 (0.4470)	
training:	Epoch: [79][177/204]	Loss 0.4271 (0.4469)	
training:	Epoch: [79][178/204]	Loss 0.3939 (0.4466)	
training:	Epoch: [79][179/204]	Loss 0.3241 (0.4459)	
training:	Epoch: [79][180/204]	Loss 0.5082 (0.4462)	
training:	Epoch: [79][181/204]	Loss 0.5211 (0.4466)	
training:	Epoch: [79][182/204]	Loss 0.3811 (0.4463)	
training:	Epoch: [79][183/204]	Loss 0.3066 (0.4455)	
training:	Epoch: [79][184/204]	Loss 0.4822 (0.4457)	
training:	Epoch: [79][185/204]	Loss 0.4115 (0.4455)	
training:	Epoch: [79][186/204]	Loss 0.3376 (0.4450)	
training:	Epoch: [79][187/204]	Loss 0.5587 (0.4456)	
training:	Epoch: [79][188/204]	Loss 0.4156 (0.4454)	
training:	Epoch: [79][189/204]	Loss 0.3846 (0.4451)	
training:	Epoch: [79][190/204]	Loss 0.3821 (0.4448)	
training:	Epoch: [79][191/204]	Loss 0.4286 (0.4447)	
training:	Epoch: [79][192/204]	Loss 0.5174 (0.4450)	
training:	Epoch: [79][193/204]	Loss 0.5075 (0.4454)	
training:	Epoch: [79][194/204]	Loss 0.4044 (0.4452)	
training:	Epoch: [79][195/204]	Loss 0.4815 (0.4453)	
training:	Epoch: [79][196/204]	Loss 0.3518 (0.4449)	
training:	Epoch: [79][197/204]	Loss 0.4394 (0.4448)	
training:	Epoch: [79][198/204]	Loss 0.4794 (0.4450)	
training:	Epoch: [79][199/204]	Loss 0.4957 (0.4453)	
training:	Epoch: [79][200/204]	Loss 0.3438 (0.4448)	
training:	Epoch: [79][201/204]	Loss 0.6748 (0.4459)	
training:	Epoch: [79][202/204]	Loss 0.6269 (0.4468)	
training:	Epoch: [79][203/204]	Loss 0.3456 (0.4463)	
training:	Epoch: [79][204/204]	Loss 0.5311 (0.4467)	
Training:	 Loss: 0.4460

Training:	 ACC: 0.8091 0.8097 0.8251 0.7930
Validation:	 ACC: 0.7893 0.7903 0.8096 0.7691
Validation:	 Best_BACC: 0.7893 0.7903 0.8096 0.7691
Validation:	 Loss: 0.4583
Pretraining:	Epoch 80/120
----------
training:	Epoch: [80][1/204]	Loss 0.4151 (0.4151)	
training:	Epoch: [80][2/204]	Loss 0.5341 (0.4746)	
training:	Epoch: [80][3/204]	Loss 0.3133 (0.4208)	
training:	Epoch: [80][4/204]	Loss 0.4593 (0.4304)	
training:	Epoch: [80][5/204]	Loss 0.5635 (0.4571)	
training:	Epoch: [80][6/204]	Loss 0.4226 (0.4513)	
training:	Epoch: [80][7/204]	Loss 0.4379 (0.4494)	
training:	Epoch: [80][8/204]	Loss 0.3848 (0.4413)	
training:	Epoch: [80][9/204]	Loss 0.4164 (0.4386)	
training:	Epoch: [80][10/204]	Loss 0.5709 (0.4518)	
training:	Epoch: [80][11/204]	Loss 0.6349 (0.4684)	
training:	Epoch: [80][12/204]	Loss 0.4097 (0.4635)	
training:	Epoch: [80][13/204]	Loss 0.3799 (0.4571)	
training:	Epoch: [80][14/204]	Loss 0.6126 (0.4682)	
training:	Epoch: [80][15/204]	Loss 0.5301 (0.4723)	
training:	Epoch: [80][16/204]	Loss 0.6069 (0.4807)	
training:	Epoch: [80][17/204]	Loss 0.6527 (0.4909)	
training:	Epoch: [80][18/204]	Loss 0.3941 (0.4855)	
training:	Epoch: [80][19/204]	Loss 0.3593 (0.4788)	
training:	Epoch: [80][20/204]	Loss 0.3954 (0.4747)	
training:	Epoch: [80][21/204]	Loss 0.5226 (0.4770)	
training:	Epoch: [80][22/204]	Loss 0.3701 (0.4721)	
training:	Epoch: [80][23/204]	Loss 0.3983 (0.4689)	
training:	Epoch: [80][24/204]	Loss 0.3242 (0.4629)	
training:	Epoch: [80][25/204]	Loss 0.3549 (0.4585)	
training:	Epoch: [80][26/204]	Loss 0.3954 (0.4561)	
training:	Epoch: [80][27/204]	Loss 0.5234 (0.4586)	
training:	Epoch: [80][28/204]	Loss 0.2632 (0.4516)	
training:	Epoch: [80][29/204]	Loss 0.4635 (0.4520)	
training:	Epoch: [80][30/204]	Loss 0.4117 (0.4507)	
training:	Epoch: [80][31/204]	Loss 0.5778 (0.4548)	
training:	Epoch: [80][32/204]	Loss 0.5200 (0.4568)	
training:	Epoch: [80][33/204]	Loss 0.4915 (0.4579)	
training:	Epoch: [80][34/204]	Loss 0.3230 (0.4539)	
training:	Epoch: [80][35/204]	Loss 0.4792 (0.4546)	
training:	Epoch: [80][36/204]	Loss 0.4190 (0.4536)	
training:	Epoch: [80][37/204]	Loss 0.3463 (0.4507)	
training:	Epoch: [80][38/204]	Loss 0.3721 (0.4487)	
training:	Epoch: [80][39/204]	Loss 0.4596 (0.4490)	
training:	Epoch: [80][40/204]	Loss 0.5080 (0.4504)	
training:	Epoch: [80][41/204]	Loss 0.5207 (0.4521)	
training:	Epoch: [80][42/204]	Loss 0.4646 (0.4524)	
training:	Epoch: [80][43/204]	Loss 0.3711 (0.4506)	
training:	Epoch: [80][44/204]	Loss 0.6613 (0.4553)	
training:	Epoch: [80][45/204]	Loss 0.4914 (0.4561)	
training:	Epoch: [80][46/204]	Loss 0.3896 (0.4547)	
training:	Epoch: [80][47/204]	Loss 0.6725 (0.4593)	
training:	Epoch: [80][48/204]	Loss 0.4105 (0.4583)	
training:	Epoch: [80][49/204]	Loss 0.3550 (0.4562)	
training:	Epoch: [80][50/204]	Loss 0.4458 (0.4560)	
training:	Epoch: [80][51/204]	Loss 0.4963 (0.4568)	
training:	Epoch: [80][52/204]	Loss 0.3083 (0.4539)	
training:	Epoch: [80][53/204]	Loss 0.5185 (0.4552)	
training:	Epoch: [80][54/204]	Loss 0.4018 (0.4542)	
training:	Epoch: [80][55/204]	Loss 0.6063 (0.4569)	
training:	Epoch: [80][56/204]	Loss 0.3083 (0.4543)	
training:	Epoch: [80][57/204]	Loss 0.3673 (0.4527)	
training:	Epoch: [80][58/204]	Loss 0.3670 (0.4513)	
training:	Epoch: [80][59/204]	Loss 0.5170 (0.4524)	
training:	Epoch: [80][60/204]	Loss 0.5689 (0.4543)	
training:	Epoch: [80][61/204]	Loss 0.3133 (0.4520)	
training:	Epoch: [80][62/204]	Loss 0.5331 (0.4533)	
training:	Epoch: [80][63/204]	Loss 0.3374 (0.4515)	
training:	Epoch: [80][64/204]	Loss 0.3196 (0.4494)	
training:	Epoch: [80][65/204]	Loss 0.4254 (0.4491)	
training:	Epoch: [80][66/204]	Loss 0.6867 (0.4527)	
training:	Epoch: [80][67/204]	Loss 0.5577 (0.4542)	
training:	Epoch: [80][68/204]	Loss 0.4992 (0.4549)	
training:	Epoch: [80][69/204]	Loss 0.4934 (0.4554)	
training:	Epoch: [80][70/204]	Loss 0.3496 (0.4539)	
training:	Epoch: [80][71/204]	Loss 0.5085 (0.4547)	
training:	Epoch: [80][72/204]	Loss 0.4568 (0.4547)	
training:	Epoch: [80][73/204]	Loss 0.6968 (0.4580)	
training:	Epoch: [80][74/204]	Loss 0.4971 (0.4586)	
training:	Epoch: [80][75/204]	Loss 0.5576 (0.4599)	
training:	Epoch: [80][76/204]	Loss 0.4230 (0.4594)	
training:	Epoch: [80][77/204]	Loss 0.4226 (0.4589)	
training:	Epoch: [80][78/204]	Loss 0.3931 (0.4581)	
training:	Epoch: [80][79/204]	Loss 0.3116 (0.4562)	
training:	Epoch: [80][80/204]	Loss 0.3461 (0.4549)	
training:	Epoch: [80][81/204]	Loss 0.4951 (0.4554)	
training:	Epoch: [80][82/204]	Loss 0.5263 (0.4562)	
training:	Epoch: [80][83/204]	Loss 0.5470 (0.4573)	
training:	Epoch: [80][84/204]	Loss 0.5183 (0.4580)	
training:	Epoch: [80][85/204]	Loss 0.4591 (0.4580)	
training:	Epoch: [80][86/204]	Loss 0.5517 (0.4591)	
training:	Epoch: [80][87/204]	Loss 0.4681 (0.4592)	
training:	Epoch: [80][88/204]	Loss 0.5310 (0.4601)	
training:	Epoch: [80][89/204]	Loss 0.4507 (0.4600)	
training:	Epoch: [80][90/204]	Loss 0.3170 (0.4584)	
training:	Epoch: [80][91/204]	Loss 0.3435 (0.4571)	
training:	Epoch: [80][92/204]	Loss 0.5496 (0.4581)	
training:	Epoch: [80][93/204]	Loss 0.3482 (0.4569)	
training:	Epoch: [80][94/204]	Loss 0.4541 (0.4569)	
training:	Epoch: [80][95/204]	Loss 0.3231 (0.4555)	
training:	Epoch: [80][96/204]	Loss 0.4654 (0.4556)	
training:	Epoch: [80][97/204]	Loss 0.4990 (0.4560)	
training:	Epoch: [80][98/204]	Loss 0.4990 (0.4565)	
training:	Epoch: [80][99/204]	Loss 0.4747 (0.4567)	
training:	Epoch: [80][100/204]	Loss 0.2683 (0.4548)	
training:	Epoch: [80][101/204]	Loss 0.5677 (0.4559)	
training:	Epoch: [80][102/204]	Loss 0.5423 (0.4567)	
training:	Epoch: [80][103/204]	Loss 0.4073 (0.4563)	
training:	Epoch: [80][104/204]	Loss 0.4224 (0.4559)	
training:	Epoch: [80][105/204]	Loss 0.4939 (0.4563)	
training:	Epoch: [80][106/204]	Loss 0.3798 (0.4556)	
training:	Epoch: [80][107/204]	Loss 0.3173 (0.4543)	
training:	Epoch: [80][108/204]	Loss 0.2872 (0.4527)	
training:	Epoch: [80][109/204]	Loss 0.3927 (0.4522)	
training:	Epoch: [80][110/204]	Loss 0.4500 (0.4522)	
training:	Epoch: [80][111/204]	Loss 0.4521 (0.4522)	
training:	Epoch: [80][112/204]	Loss 0.4085 (0.4518)	
training:	Epoch: [80][113/204]	Loss 0.3713 (0.4511)	
training:	Epoch: [80][114/204]	Loss 0.6608 (0.4529)	
training:	Epoch: [80][115/204]	Loss 0.5040 (0.4533)	
training:	Epoch: [80][116/204]	Loss 0.5318 (0.4540)	
training:	Epoch: [80][117/204]	Loss 0.4780 (0.4542)	
training:	Epoch: [80][118/204]	Loss 0.6748 (0.4561)	
training:	Epoch: [80][119/204]	Loss 0.4457 (0.4560)	
training:	Epoch: [80][120/204]	Loss 0.4539 (0.4560)	
training:	Epoch: [80][121/204]	Loss 0.5127 (0.4565)	
training:	Epoch: [80][122/204]	Loss 0.3365 (0.4555)	
training:	Epoch: [80][123/204]	Loss 0.5679 (0.4564)	
training:	Epoch: [80][124/204]	Loss 0.5073 (0.4568)	
training:	Epoch: [80][125/204]	Loss 0.3985 (0.4563)	
training:	Epoch: [80][126/204]	Loss 0.3256 (0.4553)	
training:	Epoch: [80][127/204]	Loss 0.4936 (0.4556)	
training:	Epoch: [80][128/204]	Loss 0.4646 (0.4557)	
training:	Epoch: [80][129/204]	Loss 0.4236 (0.4554)	
training:	Epoch: [80][130/204]	Loss 0.4410 (0.4553)	
training:	Epoch: [80][131/204]	Loss 0.3714 (0.4547)	
training:	Epoch: [80][132/204]	Loss 0.3940 (0.4542)	
training:	Epoch: [80][133/204]	Loss 0.5535 (0.4550)	
training:	Epoch: [80][134/204]	Loss 0.5640 (0.4558)	
training:	Epoch: [80][135/204]	Loss 0.3797 (0.4552)	
training:	Epoch: [80][136/204]	Loss 0.5019 (0.4556)	
training:	Epoch: [80][137/204]	Loss 0.4706 (0.4557)	
training:	Epoch: [80][138/204]	Loss 0.3515 (0.4549)	
training:	Epoch: [80][139/204]	Loss 0.4706 (0.4550)	
training:	Epoch: [80][140/204]	Loss 0.4668 (0.4551)	
training:	Epoch: [80][141/204]	Loss 0.4111 (0.4548)	
training:	Epoch: [80][142/204]	Loss 0.5093 (0.4552)	
training:	Epoch: [80][143/204]	Loss 0.4893 (0.4554)	
training:	Epoch: [80][144/204]	Loss 0.3508 (0.4547)	
training:	Epoch: [80][145/204]	Loss 0.4897 (0.4549)	
training:	Epoch: [80][146/204]	Loss 0.3693 (0.4543)	
training:	Epoch: [80][147/204]	Loss 0.6281 (0.4555)	
training:	Epoch: [80][148/204]	Loss 0.4072 (0.4552)	
training:	Epoch: [80][149/204]	Loss 0.4145 (0.4549)	
training:	Epoch: [80][150/204]	Loss 0.3697 (0.4544)	
training:	Epoch: [80][151/204]	Loss 0.5878 (0.4552)	
training:	Epoch: [80][152/204]	Loss 0.4411 (0.4551)	
training:	Epoch: [80][153/204]	Loss 0.5408 (0.4557)	
training:	Epoch: [80][154/204]	Loss 0.4717 (0.4558)	
training:	Epoch: [80][155/204]	Loss 0.2509 (0.4545)	
training:	Epoch: [80][156/204]	Loss 0.3888 (0.4541)	
training:	Epoch: [80][157/204]	Loss 0.4221 (0.4539)	
training:	Epoch: [80][158/204]	Loss 0.4501 (0.4538)	
training:	Epoch: [80][159/204]	Loss 0.4204 (0.4536)	
training:	Epoch: [80][160/204]	Loss 0.3443 (0.4529)	
training:	Epoch: [80][161/204]	Loss 0.5181 (0.4534)	
training:	Epoch: [80][162/204]	Loss 0.4119 (0.4531)	
training:	Epoch: [80][163/204]	Loss 0.4036 (0.4528)	
training:	Epoch: [80][164/204]	Loss 0.5554 (0.4534)	
training:	Epoch: [80][165/204]	Loss 0.4983 (0.4537)	
training:	Epoch: [80][166/204]	Loss 0.4269 (0.4535)	
training:	Epoch: [80][167/204]	Loss 0.5239 (0.4540)	
training:	Epoch: [80][168/204]	Loss 0.4939 (0.4542)	
training:	Epoch: [80][169/204]	Loss 0.4196 (0.4540)	
training:	Epoch: [80][170/204]	Loss 0.4071 (0.4537)	
training:	Epoch: [80][171/204]	Loss 0.3798 (0.4533)	
training:	Epoch: [80][172/204]	Loss 0.5584 (0.4539)	
training:	Epoch: [80][173/204]	Loss 0.3877 (0.4535)	
training:	Epoch: [80][174/204]	Loss 0.4283 (0.4534)	
training:	Epoch: [80][175/204]	Loss 0.4221 (0.4532)	
training:	Epoch: [80][176/204]	Loss 0.4941 (0.4534)	
training:	Epoch: [80][177/204]	Loss 0.3719 (0.4530)	
training:	Epoch: [80][178/204]	Loss 0.5332 (0.4534)	
training:	Epoch: [80][179/204]	Loss 0.3253 (0.4527)	
training:	Epoch: [80][180/204]	Loss 0.3343 (0.4520)	
training:	Epoch: [80][181/204]	Loss 0.5721 (0.4527)	
training:	Epoch: [80][182/204]	Loss 0.5587 (0.4533)	
training:	Epoch: [80][183/204]	Loss 0.3425 (0.4527)	
training:	Epoch: [80][184/204]	Loss 0.5294 (0.4531)	
training:	Epoch: [80][185/204]	Loss 0.3993 (0.4528)	
training:	Epoch: [80][186/204]	Loss 0.4879 (0.4530)	
training:	Epoch: [80][187/204]	Loss 0.3770 (0.4526)	
training:	Epoch: [80][188/204]	Loss 0.4109 (0.4524)	
training:	Epoch: [80][189/204]	Loss 0.3638 (0.4519)	
training:	Epoch: [80][190/204]	Loss 0.3525 (0.4514)	
training:	Epoch: [80][191/204]	Loss 0.2984 (0.4506)	
training:	Epoch: [80][192/204]	Loss 0.3699 (0.4501)	
training:	Epoch: [80][193/204]	Loss 0.4156 (0.4500)	
training:	Epoch: [80][194/204]	Loss 0.3245 (0.4493)	
training:	Epoch: [80][195/204]	Loss 0.4019 (0.4491)	
training:	Epoch: [80][196/204]	Loss 0.3997 (0.4488)	
training:	Epoch: [80][197/204]	Loss 0.4256 (0.4487)	
training:	Epoch: [80][198/204]	Loss 0.3685 (0.4483)	
training:	Epoch: [80][199/204]	Loss 0.3883 (0.4480)	
training:	Epoch: [80][200/204]	Loss 0.4470 (0.4480)	
training:	Epoch: [80][201/204]	Loss 0.4907 (0.4482)	
training:	Epoch: [80][202/204]	Loss 0.3537 (0.4477)	
training:	Epoch: [80][203/204]	Loss 0.3307 (0.4472)	
training:	Epoch: [80][204/204]	Loss 0.2997 (0.4464)	
Training:	 Loss: 0.4458

Training:	 ACC: 0.8099 0.8102 0.8180 0.8017
Validation:	 ACC: 0.7882 0.7887 0.7984 0.7780
Validation:	 Best_BACC: 0.7893 0.7903 0.8096 0.7691
Validation:	 Loss: 0.4573
Pretraining:	Epoch 81/120
----------
training:	Epoch: [81][1/204]	Loss 0.5047 (0.5047)	
training:	Epoch: [81][2/204]	Loss 0.4279 (0.4663)	
training:	Epoch: [81][3/204]	Loss 0.4571 (0.4632)	
training:	Epoch: [81][4/204]	Loss 0.4151 (0.4512)	
training:	Epoch: [81][5/204]	Loss 0.3269 (0.4264)	
training:	Epoch: [81][6/204]	Loss 0.4065 (0.4230)	
training:	Epoch: [81][7/204]	Loss 0.4630 (0.4287)	
training:	Epoch: [81][8/204]	Loss 0.4583 (0.4324)	
training:	Epoch: [81][9/204]	Loss 0.5798 (0.4488)	
training:	Epoch: [81][10/204]	Loss 0.3450 (0.4384)	
training:	Epoch: [81][11/204]	Loss 0.4399 (0.4386)	
training:	Epoch: [81][12/204]	Loss 0.5194 (0.4453)	
training:	Epoch: [81][13/204]	Loss 0.4110 (0.4427)	
training:	Epoch: [81][14/204]	Loss 0.4163 (0.4408)	
training:	Epoch: [81][15/204]	Loss 0.3831 (0.4369)	
training:	Epoch: [81][16/204]	Loss 0.3859 (0.4337)	
training:	Epoch: [81][17/204]	Loss 0.2947 (0.4256)	
training:	Epoch: [81][18/204]	Loss 0.4174 (0.4251)	
training:	Epoch: [81][19/204]	Loss 0.3188 (0.4195)	
training:	Epoch: [81][20/204]	Loss 0.4533 (0.4212)	
training:	Epoch: [81][21/204]	Loss 0.3556 (0.4181)	
training:	Epoch: [81][22/204]	Loss 0.3986 (0.4172)	
training:	Epoch: [81][23/204]	Loss 0.3539 (0.4144)	
training:	Epoch: [81][24/204]	Loss 0.4766 (0.4170)	
training:	Epoch: [81][25/204]	Loss 0.3481 (0.4143)	
training:	Epoch: [81][26/204]	Loss 0.4479 (0.4156)	
training:	Epoch: [81][27/204]	Loss 0.5203 (0.4195)	
training:	Epoch: [81][28/204]	Loss 0.4144 (0.4193)	
training:	Epoch: [81][29/204]	Loss 0.4745 (0.4212)	
training:	Epoch: [81][30/204]	Loss 0.4200 (0.4211)	
training:	Epoch: [81][31/204]	Loss 0.5134 (0.4241)	
training:	Epoch: [81][32/204]	Loss 0.4560 (0.4251)	
training:	Epoch: [81][33/204]	Loss 0.5012 (0.4274)	
training:	Epoch: [81][34/204]	Loss 0.4304 (0.4275)	
training:	Epoch: [81][35/204]	Loss 0.3814 (0.4262)	
training:	Epoch: [81][36/204]	Loss 0.4255 (0.4262)	
training:	Epoch: [81][37/204]	Loss 0.3782 (0.4249)	
training:	Epoch: [81][38/204]	Loss 0.4447 (0.4254)	
training:	Epoch: [81][39/204]	Loss 0.3542 (0.4236)	
training:	Epoch: [81][40/204]	Loss 0.5519 (0.4268)	
training:	Epoch: [81][41/204]	Loss 0.5106 (0.4288)	
training:	Epoch: [81][42/204]	Loss 0.3760 (0.4276)	
training:	Epoch: [81][43/204]	Loss 0.4258 (0.4275)	
training:	Epoch: [81][44/204]	Loss 0.5098 (0.4294)	
training:	Epoch: [81][45/204]	Loss 0.3990 (0.4287)	
training:	Epoch: [81][46/204]	Loss 0.4411 (0.4290)	
training:	Epoch: [81][47/204]	Loss 0.5721 (0.4320)	
training:	Epoch: [81][48/204]	Loss 0.4686 (0.4328)	
training:	Epoch: [81][49/204]	Loss 0.5752 (0.4357)	
training:	Epoch: [81][50/204]	Loss 0.4776 (0.4365)	
training:	Epoch: [81][51/204]	Loss 0.3458 (0.4348)	
training:	Epoch: [81][52/204]	Loss 0.3554 (0.4332)	
training:	Epoch: [81][53/204]	Loss 0.4519 (0.4336)	
training:	Epoch: [81][54/204]	Loss 0.4489 (0.4339)	
training:	Epoch: [81][55/204]	Loss 0.3391 (0.4321)	
training:	Epoch: [81][56/204]	Loss 0.5893 (0.4349)	
training:	Epoch: [81][57/204]	Loss 0.3358 (0.4332)	
training:	Epoch: [81][58/204]	Loss 0.4923 (0.4342)	
training:	Epoch: [81][59/204]	Loss 0.4206 (0.4340)	
training:	Epoch: [81][60/204]	Loss 0.5287 (0.4356)	
training:	Epoch: [81][61/204]	Loss 0.3102 (0.4335)	
training:	Epoch: [81][62/204]	Loss 0.4094 (0.4331)	
training:	Epoch: [81][63/204]	Loss 0.5074 (0.4343)	
training:	Epoch: [81][64/204]	Loss 0.3730 (0.4334)	
training:	Epoch: [81][65/204]	Loss 0.4922 (0.4343)	
training:	Epoch: [81][66/204]	Loss 0.3957 (0.4337)	
training:	Epoch: [81][67/204]	Loss 0.3507 (0.4324)	
training:	Epoch: [81][68/204]	Loss 0.4900 (0.4333)	
training:	Epoch: [81][69/204]	Loss 0.4299 (0.4332)	
training:	Epoch: [81][70/204]	Loss 0.3465 (0.4320)	
training:	Epoch: [81][71/204]	Loss 0.5590 (0.4338)	
training:	Epoch: [81][72/204]	Loss 0.4168 (0.4335)	
training:	Epoch: [81][73/204]	Loss 0.3809 (0.4328)	
training:	Epoch: [81][74/204]	Loss 0.4020 (0.4324)	
training:	Epoch: [81][75/204]	Loss 0.4126 (0.4321)	
training:	Epoch: [81][76/204]	Loss 0.5105 (0.4332)	
training:	Epoch: [81][77/204]	Loss 0.4870 (0.4339)	
training:	Epoch: [81][78/204]	Loss 0.4905 (0.4346)	
training:	Epoch: [81][79/204]	Loss 0.3163 (0.4331)	
training:	Epoch: [81][80/204]	Loss 0.4346 (0.4331)	
training:	Epoch: [81][81/204]	Loss 0.5423 (0.4345)	
training:	Epoch: [81][82/204]	Loss 0.4335 (0.4345)	
training:	Epoch: [81][83/204]	Loss 0.4679 (0.4349)	
training:	Epoch: [81][84/204]	Loss 0.4594 (0.4352)	
training:	Epoch: [81][85/204]	Loss 0.3677 (0.4344)	
training:	Epoch: [81][86/204]	Loss 0.4615 (0.4347)	
training:	Epoch: [81][87/204]	Loss 0.4220 (0.4345)	
training:	Epoch: [81][88/204]	Loss 0.4639 (0.4349)	
training:	Epoch: [81][89/204]	Loss 0.4033 (0.4345)	
training:	Epoch: [81][90/204]	Loss 0.5145 (0.4354)	
training:	Epoch: [81][91/204]	Loss 0.3796 (0.4348)	
training:	Epoch: [81][92/204]	Loss 0.4197 (0.4346)	
training:	Epoch: [81][93/204]	Loss 0.4946 (0.4353)	
training:	Epoch: [81][94/204]	Loss 0.4204 (0.4351)	
training:	Epoch: [81][95/204]	Loss 0.4427 (0.4352)	
training:	Epoch: [81][96/204]	Loss 0.5097 (0.4360)	
training:	Epoch: [81][97/204]	Loss 0.4731 (0.4363)	
training:	Epoch: [81][98/204]	Loss 0.5097 (0.4371)	
training:	Epoch: [81][99/204]	Loss 0.6078 (0.4388)	
training:	Epoch: [81][100/204]	Loss 0.4877 (0.4393)	
training:	Epoch: [81][101/204]	Loss 0.4545 (0.4395)	
training:	Epoch: [81][102/204]	Loss 0.5933 (0.4410)	
training:	Epoch: [81][103/204]	Loss 0.4365 (0.4409)	
training:	Epoch: [81][104/204]	Loss 0.5644 (0.4421)	
training:	Epoch: [81][105/204]	Loss 0.4941 (0.4426)	
training:	Epoch: [81][106/204]	Loss 0.5117 (0.4433)	
training:	Epoch: [81][107/204]	Loss 0.3231 (0.4421)	
training:	Epoch: [81][108/204]	Loss 0.6212 (0.4438)	
training:	Epoch: [81][109/204]	Loss 0.5126 (0.4444)	
training:	Epoch: [81][110/204]	Loss 0.5578 (0.4455)	
training:	Epoch: [81][111/204]	Loss 0.4115 (0.4451)	
training:	Epoch: [81][112/204]	Loss 0.4505 (0.4452)	
training:	Epoch: [81][113/204]	Loss 0.4464 (0.4452)	
training:	Epoch: [81][114/204]	Loss 0.5339 (0.4460)	
training:	Epoch: [81][115/204]	Loss 0.4552 (0.4461)	
training:	Epoch: [81][116/204]	Loss 0.3815 (0.4455)	
training:	Epoch: [81][117/204]	Loss 0.3213 (0.4444)	
training:	Epoch: [81][118/204]	Loss 0.4658 (0.4446)	
training:	Epoch: [81][119/204]	Loss 0.6081 (0.4460)	
training:	Epoch: [81][120/204]	Loss 0.3927 (0.4456)	
training:	Epoch: [81][121/204]	Loss 0.5574 (0.4465)	
training:	Epoch: [81][122/204]	Loss 0.4225 (0.4463)	
training:	Epoch: [81][123/204]	Loss 0.4070 (0.4460)	
training:	Epoch: [81][124/204]	Loss 0.4248 (0.4458)	
training:	Epoch: [81][125/204]	Loss 0.3861 (0.4453)	
training:	Epoch: [81][126/204]	Loss 0.5113 (0.4458)	
training:	Epoch: [81][127/204]	Loss 0.6313 (0.4473)	
training:	Epoch: [81][128/204]	Loss 0.4776 (0.4475)	
training:	Epoch: [81][129/204]	Loss 0.2417 (0.4459)	
training:	Epoch: [81][130/204]	Loss 0.4819 (0.4462)	
training:	Epoch: [81][131/204]	Loss 0.5389 (0.4469)	
training:	Epoch: [81][132/204]	Loss 0.3898 (0.4465)	
training:	Epoch: [81][133/204]	Loss 0.4467 (0.4465)	
training:	Epoch: [81][134/204]	Loss 0.3998 (0.4461)	
training:	Epoch: [81][135/204]	Loss 0.4805 (0.4464)	
training:	Epoch: [81][136/204]	Loss 0.4807 (0.4467)	
training:	Epoch: [81][137/204]	Loss 0.5279 (0.4472)	
training:	Epoch: [81][138/204]	Loss 0.3700 (0.4467)	
training:	Epoch: [81][139/204]	Loss 0.2840 (0.4455)	
training:	Epoch: [81][140/204]	Loss 0.3356 (0.4447)	
training:	Epoch: [81][141/204]	Loss 0.3600 (0.4441)	
training:	Epoch: [81][142/204]	Loss 0.4320 (0.4440)	
training:	Epoch: [81][143/204]	Loss 0.3507 (0.4434)	
training:	Epoch: [81][144/204]	Loss 0.5133 (0.4439)	
training:	Epoch: [81][145/204]	Loss 0.3957 (0.4435)	
training:	Epoch: [81][146/204]	Loss 0.4149 (0.4433)	
training:	Epoch: [81][147/204]	Loss 0.4108 (0.4431)	
training:	Epoch: [81][148/204]	Loss 0.4936 (0.4435)	
training:	Epoch: [81][149/204]	Loss 0.4716 (0.4437)	
training:	Epoch: [81][150/204]	Loss 0.4546 (0.4437)	
training:	Epoch: [81][151/204]	Loss 0.5240 (0.4443)	
training:	Epoch: [81][152/204]	Loss 0.3892 (0.4439)	
training:	Epoch: [81][153/204]	Loss 0.4318 (0.4438)	
training:	Epoch: [81][154/204]	Loss 0.4596 (0.4439)	
training:	Epoch: [81][155/204]	Loss 0.3737 (0.4435)	
training:	Epoch: [81][156/204]	Loss 0.5182 (0.4439)	
training:	Epoch: [81][157/204]	Loss 0.6795 (0.4454)	
training:	Epoch: [81][158/204]	Loss 0.5021 (0.4458)	
training:	Epoch: [81][159/204]	Loss 0.4915 (0.4461)	
training:	Epoch: [81][160/204]	Loss 0.4831 (0.4463)	
training:	Epoch: [81][161/204]	Loss 0.5840 (0.4472)	
training:	Epoch: [81][162/204]	Loss 0.4123 (0.4470)	
training:	Epoch: [81][163/204]	Loss 0.5807 (0.4478)	
training:	Epoch: [81][164/204]	Loss 0.4735 (0.4479)	
training:	Epoch: [81][165/204]	Loss 0.4399 (0.4479)	
training:	Epoch: [81][166/204]	Loss 0.3787 (0.4475)	
training:	Epoch: [81][167/204]	Loss 0.3315 (0.4468)	
training:	Epoch: [81][168/204]	Loss 0.4496 (0.4468)	
training:	Epoch: [81][169/204]	Loss 0.4989 (0.4471)	
training:	Epoch: [81][170/204]	Loss 0.4282 (0.4470)	
training:	Epoch: [81][171/204]	Loss 0.2985 (0.4461)	
training:	Epoch: [81][172/204]	Loss 0.3889 (0.4458)	
training:	Epoch: [81][173/204]	Loss 0.3242 (0.4451)	
training:	Epoch: [81][174/204]	Loss 0.4699 (0.4452)	
training:	Epoch: [81][175/204]	Loss 0.4010 (0.4450)	
training:	Epoch: [81][176/204]	Loss 0.4745 (0.4451)	
training:	Epoch: [81][177/204]	Loss 0.2871 (0.4443)	
training:	Epoch: [81][178/204]	Loss 0.4075 (0.4440)	
training:	Epoch: [81][179/204]	Loss 0.4418 (0.4440)	
training:	Epoch: [81][180/204]	Loss 0.6072 (0.4449)	
training:	Epoch: [81][181/204]	Loss 0.5321 (0.4454)	
training:	Epoch: [81][182/204]	Loss 0.3906 (0.4451)	
training:	Epoch: [81][183/204]	Loss 0.4220 (0.4450)	
training:	Epoch: [81][184/204]	Loss 0.5138 (0.4454)	
training:	Epoch: [81][185/204]	Loss 0.4221 (0.4452)	
training:	Epoch: [81][186/204]	Loss 0.6083 (0.4461)	
training:	Epoch: [81][187/204]	Loss 0.5250 (0.4465)	
training:	Epoch: [81][188/204]	Loss 0.5071 (0.4469)	
training:	Epoch: [81][189/204]	Loss 0.5641 (0.4475)	
training:	Epoch: [81][190/204]	Loss 0.5000 (0.4478)	
training:	Epoch: [81][191/204]	Loss 0.3731 (0.4474)	
training:	Epoch: [81][192/204]	Loss 0.4779 (0.4475)	
training:	Epoch: [81][193/204]	Loss 0.3980 (0.4473)	
training:	Epoch: [81][194/204]	Loss 0.4116 (0.4471)	
training:	Epoch: [81][195/204]	Loss 0.4218 (0.4470)	
training:	Epoch: [81][196/204]	Loss 0.4319 (0.4469)	
training:	Epoch: [81][197/204]	Loss 0.4267 (0.4468)	
training:	Epoch: [81][198/204]	Loss 0.3906 (0.4465)	
training:	Epoch: [81][199/204]	Loss 0.4206 (0.4464)	
training:	Epoch: [81][200/204]	Loss 0.5023 (0.4466)	
training:	Epoch: [81][201/204]	Loss 0.4936 (0.4469)	
training:	Epoch: [81][202/204]	Loss 0.3197 (0.4462)	
training:	Epoch: [81][203/204]	Loss 0.5278 (0.4467)	
training:	Epoch: [81][204/204]	Loss 0.3603 (0.4462)	
Training:	 Loss: 0.4455

Training:	 ACC: 0.8105 0.8106 0.8151 0.8058
Validation:	 ACC: 0.7878 0.7881 0.7943 0.7814
Validation:	 Best_BACC: 0.7893 0.7903 0.8096 0.7691
Validation:	 Loss: 0.4565
Pretraining:	Epoch 82/120
----------
training:	Epoch: [82][1/204]	Loss 0.4113 (0.4113)	
training:	Epoch: [82][2/204]	Loss 0.4281 (0.4197)	
training:	Epoch: [82][3/204]	Loss 0.4711 (0.4368)	
training:	Epoch: [82][4/204]	Loss 0.4797 (0.4476)	
training:	Epoch: [82][5/204]	Loss 0.4171 (0.4415)	
training:	Epoch: [82][6/204]	Loss 0.4402 (0.4412)	
training:	Epoch: [82][7/204]	Loss 0.4406 (0.4412)	
training:	Epoch: [82][8/204]	Loss 0.4747 (0.4454)	
training:	Epoch: [82][9/204]	Loss 0.4481 (0.4457)	
training:	Epoch: [82][10/204]	Loss 0.4382 (0.4449)	
training:	Epoch: [82][11/204]	Loss 0.4316 (0.4437)	
training:	Epoch: [82][12/204]	Loss 0.4636 (0.4454)	
training:	Epoch: [82][13/204]	Loss 0.5441 (0.4530)	
training:	Epoch: [82][14/204]	Loss 0.4878 (0.4554)	
training:	Epoch: [82][15/204]	Loss 0.5660 (0.4628)	
training:	Epoch: [82][16/204]	Loss 0.6364 (0.4737)	
training:	Epoch: [82][17/204]	Loss 0.4335 (0.4713)	
training:	Epoch: [82][18/204]	Loss 0.3668 (0.4655)	
training:	Epoch: [82][19/204]	Loss 0.3464 (0.4592)	
training:	Epoch: [82][20/204]	Loss 0.4770 (0.4601)	
training:	Epoch: [82][21/204]	Loss 0.3879 (0.4567)	
training:	Epoch: [82][22/204]	Loss 0.5303 (0.4600)	
training:	Epoch: [82][23/204]	Loss 0.3963 (0.4573)	
training:	Epoch: [82][24/204]	Loss 0.5497 (0.4611)	
training:	Epoch: [82][25/204]	Loss 0.5591 (0.4650)	
training:	Epoch: [82][26/204]	Loss 0.3801 (0.4618)	
training:	Epoch: [82][27/204]	Loss 0.3200 (0.4565)	
training:	Epoch: [82][28/204]	Loss 0.3405 (0.4524)	
training:	Epoch: [82][29/204]	Loss 0.3241 (0.4479)	
training:	Epoch: [82][30/204]	Loss 0.5614 (0.4517)	
training:	Epoch: [82][31/204]	Loss 0.3386 (0.4481)	
training:	Epoch: [82][32/204]	Loss 0.3370 (0.4446)	
training:	Epoch: [82][33/204]	Loss 0.4213 (0.4439)	
training:	Epoch: [82][34/204]	Loss 0.3628 (0.4415)	
training:	Epoch: [82][35/204]	Loss 0.3475 (0.4388)	
training:	Epoch: [82][36/204]	Loss 0.4268 (0.4385)	
training:	Epoch: [82][37/204]	Loss 0.4159 (0.4379)	
training:	Epoch: [82][38/204]	Loss 0.3960 (0.4368)	
training:	Epoch: [82][39/204]	Loss 0.4090 (0.4361)	
training:	Epoch: [82][40/204]	Loss 0.3774 (0.4346)	
training:	Epoch: [82][41/204]	Loss 0.4812 (0.4357)	
training:	Epoch: [82][42/204]	Loss 0.2815 (0.4321)	
training:	Epoch: [82][43/204]	Loss 0.4550 (0.4326)	
training:	Epoch: [82][44/204]	Loss 0.4421 (0.4328)	
training:	Epoch: [82][45/204]	Loss 0.4080 (0.4323)	
training:	Epoch: [82][46/204]	Loss 0.4787 (0.4333)	
training:	Epoch: [82][47/204]	Loss 0.5588 (0.4359)	
training:	Epoch: [82][48/204]	Loss 0.4676 (0.4366)	
training:	Epoch: [82][49/204]	Loss 0.5051 (0.4380)	
training:	Epoch: [82][50/204]	Loss 0.5470 (0.4402)	
training:	Epoch: [82][51/204]	Loss 0.4538 (0.4404)	
training:	Epoch: [82][52/204]	Loss 0.5146 (0.4419)	
training:	Epoch: [82][53/204]	Loss 0.4467 (0.4420)	
training:	Epoch: [82][54/204]	Loss 0.3950 (0.4411)	
training:	Epoch: [82][55/204]	Loss 0.4211 (0.4407)	
training:	Epoch: [82][56/204]	Loss 0.3826 (0.4397)	
training:	Epoch: [82][57/204]	Loss 0.3671 (0.4384)	
training:	Epoch: [82][58/204]	Loss 0.4260 (0.4382)	
training:	Epoch: [82][59/204]	Loss 0.2890 (0.4357)	
training:	Epoch: [82][60/204]	Loss 0.5627 (0.4378)	
training:	Epoch: [82][61/204]	Loss 0.3387 (0.4362)	
training:	Epoch: [82][62/204]	Loss 0.4977 (0.4372)	
training:	Epoch: [82][63/204]	Loss 0.3215 (0.4353)	
training:	Epoch: [82][64/204]	Loss 0.3450 (0.4339)	
training:	Epoch: [82][65/204]	Loss 0.5785 (0.4361)	
training:	Epoch: [82][66/204]	Loss 0.3061 (0.4342)	
training:	Epoch: [82][67/204]	Loss 0.4616 (0.4346)	
training:	Epoch: [82][68/204]	Loss 0.6324 (0.4375)	
training:	Epoch: [82][69/204]	Loss 0.5908 (0.4397)	
training:	Epoch: [82][70/204]	Loss 0.3221 (0.4380)	
training:	Epoch: [82][71/204]	Loss 0.3066 (0.4362)	
training:	Epoch: [82][72/204]	Loss 0.5364 (0.4376)	
training:	Epoch: [82][73/204]	Loss 0.3647 (0.4366)	
training:	Epoch: [82][74/204]	Loss 0.4237 (0.4364)	
training:	Epoch: [82][75/204]	Loss 0.5050 (0.4373)	
training:	Epoch: [82][76/204]	Loss 0.4037 (0.4369)	
training:	Epoch: [82][77/204]	Loss 0.4247 (0.4367)	
training:	Epoch: [82][78/204]	Loss 0.3538 (0.4356)	
training:	Epoch: [82][79/204]	Loss 0.3091 (0.4340)	
training:	Epoch: [82][80/204]	Loss 0.5557 (0.4356)	
training:	Epoch: [82][81/204]	Loss 0.3211 (0.4342)	
training:	Epoch: [82][82/204]	Loss 0.3460 (0.4331)	
training:	Epoch: [82][83/204]	Loss 0.4884 (0.4337)	
training:	Epoch: [82][84/204]	Loss 0.3739 (0.4330)	
training:	Epoch: [82][85/204]	Loss 0.4274 (0.4330)	
training:	Epoch: [82][86/204]	Loss 0.4256 (0.4329)	
training:	Epoch: [82][87/204]	Loss 0.4110 (0.4326)	
training:	Epoch: [82][88/204]	Loss 0.4394 (0.4327)	
training:	Epoch: [82][89/204]	Loss 0.5284 (0.4338)	
training:	Epoch: [82][90/204]	Loss 0.5223 (0.4348)	
training:	Epoch: [82][91/204]	Loss 0.5128 (0.4356)	
training:	Epoch: [82][92/204]	Loss 0.3500 (0.4347)	
training:	Epoch: [82][93/204]	Loss 0.5694 (0.4361)	
training:	Epoch: [82][94/204]	Loss 0.3748 (0.4355)	
training:	Epoch: [82][95/204]	Loss 0.3845 (0.4350)	
training:	Epoch: [82][96/204]	Loss 0.5743 (0.4364)	
training:	Epoch: [82][97/204]	Loss 0.6068 (0.4382)	
training:	Epoch: [82][98/204]	Loss 0.5646 (0.4394)	
training:	Epoch: [82][99/204]	Loss 0.4774 (0.4398)	
training:	Epoch: [82][100/204]	Loss 0.4220 (0.4397)	
training:	Epoch: [82][101/204]	Loss 0.3664 (0.4389)	
training:	Epoch: [82][102/204]	Loss 0.4938 (0.4395)	
training:	Epoch: [82][103/204]	Loss 0.5290 (0.4403)	
training:	Epoch: [82][104/204]	Loss 0.4543 (0.4405)	
training:	Epoch: [82][105/204]	Loss 0.4654 (0.4407)	
training:	Epoch: [82][106/204]	Loss 0.4266 (0.4406)	
training:	Epoch: [82][107/204]	Loss 0.4257 (0.4404)	
training:	Epoch: [82][108/204]	Loss 0.4743 (0.4407)	
training:	Epoch: [82][109/204]	Loss 0.4704 (0.4410)	
training:	Epoch: [82][110/204]	Loss 0.5625 (0.4421)	
training:	Epoch: [82][111/204]	Loss 0.3987 (0.4417)	
training:	Epoch: [82][112/204]	Loss 0.4625 (0.4419)	
training:	Epoch: [82][113/204]	Loss 0.2988 (0.4407)	
training:	Epoch: [82][114/204]	Loss 0.4078 (0.4404)	
training:	Epoch: [82][115/204]	Loss 0.5704 (0.4415)	
training:	Epoch: [82][116/204]	Loss 0.4415 (0.4415)	
training:	Epoch: [82][117/204]	Loss 0.4008 (0.4411)	
training:	Epoch: [82][118/204]	Loss 0.5516 (0.4421)	
training:	Epoch: [82][119/204]	Loss 0.5752 (0.4432)	
training:	Epoch: [82][120/204]	Loss 0.3935 (0.4428)	
training:	Epoch: [82][121/204]	Loss 0.4406 (0.4428)	
training:	Epoch: [82][122/204]	Loss 0.4021 (0.4424)	
training:	Epoch: [82][123/204]	Loss 0.4640 (0.4426)	
training:	Epoch: [82][124/204]	Loss 0.4252 (0.4425)	
training:	Epoch: [82][125/204]	Loss 0.5334 (0.4432)	
training:	Epoch: [82][126/204]	Loss 0.4457 (0.4432)	
training:	Epoch: [82][127/204]	Loss 0.3214 (0.4423)	
training:	Epoch: [82][128/204]	Loss 0.4067 (0.4420)	
training:	Epoch: [82][129/204]	Loss 0.4244 (0.4418)	
training:	Epoch: [82][130/204]	Loss 0.4811 (0.4421)	
training:	Epoch: [82][131/204]	Loss 0.4289 (0.4420)	
training:	Epoch: [82][132/204]	Loss 0.4363 (0.4420)	
training:	Epoch: [82][133/204]	Loss 0.4917 (0.4424)	
training:	Epoch: [82][134/204]	Loss 0.4275 (0.4423)	
training:	Epoch: [82][135/204]	Loss 0.3783 (0.4418)	
training:	Epoch: [82][136/204]	Loss 0.4086 (0.4415)	
training:	Epoch: [82][137/204]	Loss 0.3018 (0.4405)	
training:	Epoch: [82][138/204]	Loss 0.4361 (0.4405)	
training:	Epoch: [82][139/204]	Loss 0.5505 (0.4413)	
training:	Epoch: [82][140/204]	Loss 0.4924 (0.4417)	
training:	Epoch: [82][141/204]	Loss 0.4184 (0.4415)	
training:	Epoch: [82][142/204]	Loss 0.3782 (0.4410)	
training:	Epoch: [82][143/204]	Loss 0.3775 (0.4406)	
training:	Epoch: [82][144/204]	Loss 0.4431 (0.4406)	
training:	Epoch: [82][145/204]	Loss 0.3861 (0.4402)	
training:	Epoch: [82][146/204]	Loss 0.4577 (0.4404)	
training:	Epoch: [82][147/204]	Loss 0.3145 (0.4395)	
training:	Epoch: [82][148/204]	Loss 0.3622 (0.4390)	
training:	Epoch: [82][149/204]	Loss 0.4537 (0.4391)	
training:	Epoch: [82][150/204]	Loss 0.3556 (0.4385)	
training:	Epoch: [82][151/204]	Loss 0.4022 (0.4383)	
training:	Epoch: [82][152/204]	Loss 0.4568 (0.4384)	
training:	Epoch: [82][153/204]	Loss 0.3696 (0.4380)	
training:	Epoch: [82][154/204]	Loss 0.3826 (0.4376)	
training:	Epoch: [82][155/204]	Loss 0.6968 (0.4393)	
training:	Epoch: [82][156/204]	Loss 0.4337 (0.4392)	
training:	Epoch: [82][157/204]	Loss 0.3440 (0.4386)	
training:	Epoch: [82][158/204]	Loss 0.4574 (0.4387)	
training:	Epoch: [82][159/204]	Loss 0.4283 (0.4387)	
training:	Epoch: [82][160/204]	Loss 0.4897 (0.4390)	
training:	Epoch: [82][161/204]	Loss 0.2952 (0.4381)	
training:	Epoch: [82][162/204]	Loss 0.4921 (0.4384)	
training:	Epoch: [82][163/204]	Loss 0.3397 (0.4378)	
training:	Epoch: [82][164/204]	Loss 0.4958 (0.4382)	
training:	Epoch: [82][165/204]	Loss 0.3498 (0.4376)	
training:	Epoch: [82][166/204]	Loss 0.4596 (0.4378)	
training:	Epoch: [82][167/204]	Loss 0.4310 (0.4377)	
training:	Epoch: [82][168/204]	Loss 0.6906 (0.4392)	
training:	Epoch: [82][169/204]	Loss 0.3253 (0.4386)	
training:	Epoch: [82][170/204]	Loss 0.4229 (0.4385)	
training:	Epoch: [82][171/204]	Loss 0.2938 (0.4376)	
training:	Epoch: [82][172/204]	Loss 0.3811 (0.4373)	
training:	Epoch: [82][173/204]	Loss 0.2488 (0.4362)	
training:	Epoch: [82][174/204]	Loss 0.5738 (0.4370)	
training:	Epoch: [82][175/204]	Loss 0.4291 (0.4370)	
training:	Epoch: [82][176/204]	Loss 0.3785 (0.4366)	
training:	Epoch: [82][177/204]	Loss 0.4343 (0.4366)	
training:	Epoch: [82][178/204]	Loss 0.4212 (0.4365)	
training:	Epoch: [82][179/204]	Loss 0.3629 (0.4361)	
training:	Epoch: [82][180/204]	Loss 0.4613 (0.4363)	
training:	Epoch: [82][181/204]	Loss 0.3252 (0.4356)	
training:	Epoch: [82][182/204]	Loss 0.5951 (0.4365)	
training:	Epoch: [82][183/204]	Loss 0.3994 (0.4363)	
training:	Epoch: [82][184/204]	Loss 0.5887 (0.4371)	
training:	Epoch: [82][185/204]	Loss 0.5209 (0.4376)	
training:	Epoch: [82][186/204]	Loss 0.3232 (0.4370)	
training:	Epoch: [82][187/204]	Loss 0.2948 (0.4362)	
training:	Epoch: [82][188/204]	Loss 0.3439 (0.4357)	
training:	Epoch: [82][189/204]	Loss 0.3220 (0.4351)	
training:	Epoch: [82][190/204]	Loss 0.6098 (0.4360)	
training:	Epoch: [82][191/204]	Loss 0.4278 (0.4360)	
training:	Epoch: [82][192/204]	Loss 0.4241 (0.4359)	
training:	Epoch: [82][193/204]	Loss 0.3925 (0.4357)	
training:	Epoch: [82][194/204]	Loss 0.3678 (0.4354)	
training:	Epoch: [82][195/204]	Loss 0.3189 (0.4348)	
training:	Epoch: [82][196/204]	Loss 0.3758 (0.4345)	
training:	Epoch: [82][197/204]	Loss 0.4101 (0.4343)	
training:	Epoch: [82][198/204]	Loss 0.5234 (0.4348)	
training:	Epoch: [82][199/204]	Loss 0.4838 (0.4350)	
training:	Epoch: [82][200/204]	Loss 0.4757 (0.4352)	
training:	Epoch: [82][201/204]	Loss 0.7364 (0.4367)	
training:	Epoch: [82][202/204]	Loss 0.5956 (0.4375)	
training:	Epoch: [82][203/204]	Loss 0.5739 (0.4382)	
training:	Epoch: [82][204/204]	Loss 0.5032 (0.4385)	
Training:	 Loss: 0.4379

Training:	 ACC: 0.8131 0.8137 0.8289 0.7972
Validation:	 ACC: 0.7915 0.7924 0.8106 0.7724
Validation:	 Best_BACC: 0.7915 0.7924 0.8106 0.7724
Validation:	 Loss: 0.4554
Pretraining:	Epoch 83/120
----------
training:	Epoch: [83][1/204]	Loss 0.3832 (0.3832)	
training:	Epoch: [83][2/204]	Loss 0.3149 (0.3491)	
training:	Epoch: [83][3/204]	Loss 0.4513 (0.3832)	
training:	Epoch: [83][4/204]	Loss 0.4140 (0.3909)	
training:	Epoch: [83][5/204]	Loss 0.3718 (0.3871)	
training:	Epoch: [83][6/204]	Loss 0.4659 (0.4002)	
training:	Epoch: [83][7/204]	Loss 0.4696 (0.4101)	
training:	Epoch: [83][8/204]	Loss 0.5190 (0.4237)	
training:	Epoch: [83][9/204]	Loss 0.5668 (0.4396)	
training:	Epoch: [83][10/204]	Loss 0.3678 (0.4324)	
training:	Epoch: [83][11/204]	Loss 0.4155 (0.4309)	
training:	Epoch: [83][12/204]	Loss 0.2496 (0.4158)	
training:	Epoch: [83][13/204]	Loss 0.4386 (0.4175)	
training:	Epoch: [83][14/204]	Loss 0.2725 (0.4072)	
training:	Epoch: [83][15/204]	Loss 0.5412 (0.4161)	
training:	Epoch: [83][16/204]	Loss 0.5161 (0.4224)	
training:	Epoch: [83][17/204]	Loss 0.4213 (0.4223)	
training:	Epoch: [83][18/204]	Loss 0.5026 (0.4268)	
training:	Epoch: [83][19/204]	Loss 0.5150 (0.4314)	
training:	Epoch: [83][20/204]	Loss 0.4909 (0.4344)	
training:	Epoch: [83][21/204]	Loss 0.4821 (0.4367)	
training:	Epoch: [83][22/204]	Loss 0.4017 (0.4351)	
training:	Epoch: [83][23/204]	Loss 0.5464 (0.4399)	
training:	Epoch: [83][24/204]	Loss 0.6601 (0.4491)	
training:	Epoch: [83][25/204]	Loss 0.4283 (0.4482)	
training:	Epoch: [83][26/204]	Loss 0.4280 (0.4475)	
training:	Epoch: [83][27/204]	Loss 0.5402 (0.4509)	
training:	Epoch: [83][28/204]	Loss 0.3680 (0.4479)	
training:	Epoch: [83][29/204]	Loss 0.3939 (0.4461)	
training:	Epoch: [83][30/204]	Loss 0.3456 (0.4427)	
training:	Epoch: [83][31/204]	Loss 0.4415 (0.4427)	
training:	Epoch: [83][32/204]	Loss 0.5074 (0.4447)	
training:	Epoch: [83][33/204]	Loss 0.3390 (0.4415)	
training:	Epoch: [83][34/204]	Loss 0.4593 (0.4420)	
training:	Epoch: [83][35/204]	Loss 0.2925 (0.4378)	
training:	Epoch: [83][36/204]	Loss 0.4526 (0.4382)	
training:	Epoch: [83][37/204]	Loss 0.4279 (0.4379)	
training:	Epoch: [83][38/204]	Loss 0.3259 (0.4349)	
training:	Epoch: [83][39/204]	Loss 0.4686 (0.4358)	
training:	Epoch: [83][40/204]	Loss 0.4234 (0.4355)	
training:	Epoch: [83][41/204]	Loss 0.3931 (0.4345)	
training:	Epoch: [83][42/204]	Loss 0.3655 (0.4328)	
training:	Epoch: [83][43/204]	Loss 0.5374 (0.4353)	
training:	Epoch: [83][44/204]	Loss 0.5044 (0.4368)	
training:	Epoch: [83][45/204]	Loss 0.5346 (0.4390)	
training:	Epoch: [83][46/204]	Loss 0.4761 (0.4398)	
training:	Epoch: [83][47/204]	Loss 0.5788 (0.4428)	
training:	Epoch: [83][48/204]	Loss 0.3871 (0.4416)	
training:	Epoch: [83][49/204]	Loss 0.6103 (0.4450)	
training:	Epoch: [83][50/204]	Loss 0.4787 (0.4457)	
training:	Epoch: [83][51/204]	Loss 0.4795 (0.4464)	
training:	Epoch: [83][52/204]	Loss 0.5014 (0.4474)	
training:	Epoch: [83][53/204]	Loss 0.3555 (0.4457)	
training:	Epoch: [83][54/204]	Loss 0.3324 (0.4436)	
training:	Epoch: [83][55/204]	Loss 0.3069 (0.4411)	
training:	Epoch: [83][56/204]	Loss 0.3965 (0.4403)	
training:	Epoch: [83][57/204]	Loss 0.4506 (0.4405)	
training:	Epoch: [83][58/204]	Loss 0.4621 (0.4409)	
training:	Epoch: [83][59/204]	Loss 0.4604 (0.4412)	
training:	Epoch: [83][60/204]	Loss 0.2479 (0.4380)	
training:	Epoch: [83][61/204]	Loss 0.5005 (0.4390)	
training:	Epoch: [83][62/204]	Loss 0.4924 (0.4399)	
training:	Epoch: [83][63/204]	Loss 0.5518 (0.4416)	
training:	Epoch: [83][64/204]	Loss 0.4675 (0.4421)	
training:	Epoch: [83][65/204]	Loss 0.4191 (0.4417)	
training:	Epoch: [83][66/204]	Loss 0.3941 (0.4410)	
training:	Epoch: [83][67/204]	Loss 0.3364 (0.4394)	
training:	Epoch: [83][68/204]	Loss 0.3556 (0.4382)	
training:	Epoch: [83][69/204]	Loss 0.3715 (0.4372)	
training:	Epoch: [83][70/204]	Loss 0.3697 (0.4363)	
training:	Epoch: [83][71/204]	Loss 0.4822 (0.4369)	
training:	Epoch: [83][72/204]	Loss 0.4174 (0.4366)	
training:	Epoch: [83][73/204]	Loss 0.3468 (0.4354)	
training:	Epoch: [83][74/204]	Loss 0.4065 (0.4350)	
training:	Epoch: [83][75/204]	Loss 0.5128 (0.4360)	
training:	Epoch: [83][76/204]	Loss 0.5062 (0.4370)	
training:	Epoch: [83][77/204]	Loss 0.5351 (0.4382)	
training:	Epoch: [83][78/204]	Loss 0.4029 (0.4378)	
training:	Epoch: [83][79/204]	Loss 0.4772 (0.4383)	
training:	Epoch: [83][80/204]	Loss 0.4791 (0.4388)	
training:	Epoch: [83][81/204]	Loss 0.3263 (0.4374)	
training:	Epoch: [83][82/204]	Loss 0.4764 (0.4379)	
training:	Epoch: [83][83/204]	Loss 0.4134 (0.4376)	
training:	Epoch: [83][84/204]	Loss 0.5359 (0.4388)	
training:	Epoch: [83][85/204]	Loss 0.5608 (0.4402)	
training:	Epoch: [83][86/204]	Loss 0.5806 (0.4418)	
training:	Epoch: [83][87/204]	Loss 0.3537 (0.4408)	
training:	Epoch: [83][88/204]	Loss 0.4566 (0.4410)	
training:	Epoch: [83][89/204]	Loss 0.3437 (0.4399)	
training:	Epoch: [83][90/204]	Loss 0.4265 (0.4398)	
training:	Epoch: [83][91/204]	Loss 0.5783 (0.4413)	
training:	Epoch: [83][92/204]	Loss 0.6181 (0.4432)	
training:	Epoch: [83][93/204]	Loss 0.4640 (0.4434)	
training:	Epoch: [83][94/204]	Loss 0.3393 (0.4423)	
training:	Epoch: [83][95/204]	Loss 0.4364 (0.4423)	
training:	Epoch: [83][96/204]	Loss 0.4434 (0.4423)	
training:	Epoch: [83][97/204]	Loss 0.4239 (0.4421)	
training:	Epoch: [83][98/204]	Loss 0.4216 (0.4419)	
training:	Epoch: [83][99/204]	Loss 0.3185 (0.4406)	
training:	Epoch: [83][100/204]	Loss 0.5604 (0.4418)	
training:	Epoch: [83][101/204]	Loss 0.4256 (0.4417)	
training:	Epoch: [83][102/204]	Loss 0.5634 (0.4429)	
training:	Epoch: [83][103/204]	Loss 0.5005 (0.4434)	
training:	Epoch: [83][104/204]	Loss 0.4637 (0.4436)	
training:	Epoch: [83][105/204]	Loss 0.2319 (0.4416)	
training:	Epoch: [83][106/204]	Loss 0.4019 (0.4412)	
training:	Epoch: [83][107/204]	Loss 0.3768 (0.4406)	
training:	Epoch: [83][108/204]	Loss 0.5767 (0.4419)	
training:	Epoch: [83][109/204]	Loss 0.4178 (0.4417)	
training:	Epoch: [83][110/204]	Loss 0.3458 (0.4408)	
training:	Epoch: [83][111/204]	Loss 0.4682 (0.4410)	
training:	Epoch: [83][112/204]	Loss 0.4371 (0.4410)	
training:	Epoch: [83][113/204]	Loss 0.4800 (0.4413)	
training:	Epoch: [83][114/204]	Loss 0.4130 (0.4411)	
training:	Epoch: [83][115/204]	Loss 0.4082 (0.4408)	
training:	Epoch: [83][116/204]	Loss 0.5927 (0.4421)	
training:	Epoch: [83][117/204]	Loss 0.4236 (0.4420)	
training:	Epoch: [83][118/204]	Loss 0.3302 (0.4410)	
training:	Epoch: [83][119/204]	Loss 0.4681 (0.4412)	
training:	Epoch: [83][120/204]	Loss 0.5172 (0.4419)	
training:	Epoch: [83][121/204]	Loss 0.4837 (0.4422)	
training:	Epoch: [83][122/204]	Loss 0.7152 (0.4445)	
training:	Epoch: [83][123/204]	Loss 0.3411 (0.4436)	
training:	Epoch: [83][124/204]	Loss 0.5921 (0.4448)	
training:	Epoch: [83][125/204]	Loss 0.3455 (0.4440)	
training:	Epoch: [83][126/204]	Loss 0.5623 (0.4450)	
training:	Epoch: [83][127/204]	Loss 0.4176 (0.4447)	
training:	Epoch: [83][128/204]	Loss 0.5250 (0.4454)	
training:	Epoch: [83][129/204]	Loss 0.5011 (0.4458)	
training:	Epoch: [83][130/204]	Loss 0.6151 (0.4471)	
training:	Epoch: [83][131/204]	Loss 0.4737 (0.4473)	
training:	Epoch: [83][132/204]	Loss 0.3792 (0.4468)	
training:	Epoch: [83][133/204]	Loss 0.4310 (0.4467)	
training:	Epoch: [83][134/204]	Loss 0.5181 (0.4472)	
training:	Epoch: [83][135/204]	Loss 0.4396 (0.4471)	
training:	Epoch: [83][136/204]	Loss 0.4697 (0.4473)	
training:	Epoch: [83][137/204]	Loss 0.4300 (0.4472)	
training:	Epoch: [83][138/204]	Loss 0.3574 (0.4465)	
training:	Epoch: [83][139/204]	Loss 0.3669 (0.4460)	
training:	Epoch: [83][140/204]	Loss 0.4800 (0.4462)	
training:	Epoch: [83][141/204]	Loss 0.3250 (0.4453)	
training:	Epoch: [83][142/204]	Loss 0.5389 (0.4460)	
training:	Epoch: [83][143/204]	Loss 0.3836 (0.4456)	
training:	Epoch: [83][144/204]	Loss 0.3222 (0.4447)	
training:	Epoch: [83][145/204]	Loss 0.4636 (0.4448)	
training:	Epoch: [83][146/204]	Loss 0.4153 (0.4446)	
training:	Epoch: [83][147/204]	Loss 0.4459 (0.4446)	
training:	Epoch: [83][148/204]	Loss 0.4807 (0.4449)	
training:	Epoch: [83][149/204]	Loss 0.5840 (0.4458)	
training:	Epoch: [83][150/204]	Loss 0.4612 (0.4459)	
training:	Epoch: [83][151/204]	Loss 0.3850 (0.4455)	
training:	Epoch: [83][152/204]	Loss 0.5158 (0.4460)	
training:	Epoch: [83][153/204]	Loss 0.4669 (0.4461)	
training:	Epoch: [83][154/204]	Loss 0.4243 (0.4460)	
training:	Epoch: [83][155/204]	Loss 0.4151 (0.4458)	
training:	Epoch: [83][156/204]	Loss 0.5163 (0.4462)	
training:	Epoch: [83][157/204]	Loss 0.4317 (0.4461)	
training:	Epoch: [83][158/204]	Loss 0.4182 (0.4460)	
training:	Epoch: [83][159/204]	Loss 0.4599 (0.4461)	
training:	Epoch: [83][160/204]	Loss 0.4029 (0.4458)	
training:	Epoch: [83][161/204]	Loss 0.3989 (0.4455)	
training:	Epoch: [83][162/204]	Loss 0.2994 (0.4446)	
training:	Epoch: [83][163/204]	Loss 0.3345 (0.4439)	
training:	Epoch: [83][164/204]	Loss 0.5377 (0.4445)	
training:	Epoch: [83][165/204]	Loss 0.5956 (0.4454)	
training:	Epoch: [83][166/204]	Loss 0.3516 (0.4448)	
training:	Epoch: [83][167/204]	Loss 0.5652 (0.4456)	
training:	Epoch: [83][168/204]	Loss 0.3740 (0.4451)	
training:	Epoch: [83][169/204]	Loss 0.4861 (0.4454)	
training:	Epoch: [83][170/204]	Loss 0.4254 (0.4453)	
training:	Epoch: [83][171/204]	Loss 0.4991 (0.4456)	
training:	Epoch: [83][172/204]	Loss 0.3154 (0.4448)	
training:	Epoch: [83][173/204]	Loss 0.4314 (0.4447)	
training:	Epoch: [83][174/204]	Loss 0.3998 (0.4445)	
training:	Epoch: [83][175/204]	Loss 0.6648 (0.4457)	
training:	Epoch: [83][176/204]	Loss 0.4414 (0.4457)	
training:	Epoch: [83][177/204]	Loss 0.5334 (0.4462)	
training:	Epoch: [83][178/204]	Loss 0.4076 (0.4460)	
training:	Epoch: [83][179/204]	Loss 0.3892 (0.4457)	
training:	Epoch: [83][180/204]	Loss 0.5099 (0.4460)	
training:	Epoch: [83][181/204]	Loss 0.5095 (0.4464)	
training:	Epoch: [83][182/204]	Loss 0.3983 (0.4461)	
training:	Epoch: [83][183/204]	Loss 0.4210 (0.4460)	
training:	Epoch: [83][184/204]	Loss 0.2324 (0.4448)	
training:	Epoch: [83][185/204]	Loss 0.3736 (0.4444)	
training:	Epoch: [83][186/204]	Loss 0.4150 (0.4443)	
training:	Epoch: [83][187/204]	Loss 0.4873 (0.4445)	
training:	Epoch: [83][188/204]	Loss 0.3718 (0.4441)	
training:	Epoch: [83][189/204]	Loss 0.4859 (0.4443)	
training:	Epoch: [83][190/204]	Loss 0.5124 (0.4447)	
training:	Epoch: [83][191/204]	Loss 0.5716 (0.4454)	
training:	Epoch: [83][192/204]	Loss 0.1942 (0.4441)	
training:	Epoch: [83][193/204]	Loss 0.4226 (0.4439)	
training:	Epoch: [83][194/204]	Loss 0.5446 (0.4445)	
training:	Epoch: [83][195/204]	Loss 0.5409 (0.4450)	
training:	Epoch: [83][196/204]	Loss 0.4249 (0.4449)	
training:	Epoch: [83][197/204]	Loss 0.3859 (0.4446)	
training:	Epoch: [83][198/204]	Loss 0.6197 (0.4454)	
training:	Epoch: [83][199/204]	Loss 0.3764 (0.4451)	
training:	Epoch: [83][200/204]	Loss 0.4123 (0.4449)	
training:	Epoch: [83][201/204]	Loss 0.3645 (0.4445)	
training:	Epoch: [83][202/204]	Loss 0.4064 (0.4443)	
training:	Epoch: [83][203/204]	Loss 0.4034 (0.4441)	
training:	Epoch: [83][204/204]	Loss 0.3903 (0.4439)	
Training:	 Loss: 0.4432

Training:	 ACC: 0.8141 0.8149 0.8336 0.7946
Validation:	 ACC: 0.7925 0.7935 0.8137 0.7713
Validation:	 Best_BACC: 0.7925 0.7935 0.8137 0.7713
Validation:	 Loss: 0.4545
Pretraining:	Epoch 84/120
----------
training:	Epoch: [84][1/204]	Loss 0.4039 (0.4039)	
training:	Epoch: [84][2/204]	Loss 0.5301 (0.4670)	
training:	Epoch: [84][3/204]	Loss 0.4480 (0.4607)	
training:	Epoch: [84][4/204]	Loss 0.3628 (0.4362)	
training:	Epoch: [84][5/204]	Loss 0.3785 (0.4246)	
training:	Epoch: [84][6/204]	Loss 0.4217 (0.4242)	
training:	Epoch: [84][7/204]	Loss 0.3480 (0.4133)	
training:	Epoch: [84][8/204]	Loss 0.5110 (0.4255)	
training:	Epoch: [84][9/204]	Loss 0.3359 (0.4155)	
training:	Epoch: [84][10/204]	Loss 0.4297 (0.4170)	
training:	Epoch: [84][11/204]	Loss 0.3445 (0.4104)	
training:	Epoch: [84][12/204]	Loss 0.3253 (0.4033)	
training:	Epoch: [84][13/204]	Loss 0.6369 (0.4212)	
training:	Epoch: [84][14/204]	Loss 0.2089 (0.4061)	
training:	Epoch: [84][15/204]	Loss 0.4411 (0.4084)	
training:	Epoch: [84][16/204]	Loss 0.3270 (0.4033)	
training:	Epoch: [84][17/204]	Loss 0.5853 (0.4140)	
training:	Epoch: [84][18/204]	Loss 0.3496 (0.4105)	
training:	Epoch: [84][19/204]	Loss 0.4866 (0.4145)	
training:	Epoch: [84][20/204]	Loss 0.4583 (0.4167)	
training:	Epoch: [84][21/204]	Loss 0.4468 (0.4181)	
training:	Epoch: [84][22/204]	Loss 0.5588 (0.4245)	
training:	Epoch: [84][23/204]	Loss 0.5360 (0.4293)	
training:	Epoch: [84][24/204]	Loss 0.5038 (0.4324)	
training:	Epoch: [84][25/204]	Loss 0.3977 (0.4310)	
training:	Epoch: [84][26/204]	Loss 0.4372 (0.4313)	
training:	Epoch: [84][27/204]	Loss 0.3506 (0.4283)	
training:	Epoch: [84][28/204]	Loss 0.3322 (0.4249)	
training:	Epoch: [84][29/204]	Loss 0.4422 (0.4255)	
training:	Epoch: [84][30/204]	Loss 0.4419 (0.4260)	
training:	Epoch: [84][31/204]	Loss 0.6890 (0.4345)	
training:	Epoch: [84][32/204]	Loss 0.5023 (0.4366)	
training:	Epoch: [84][33/204]	Loss 0.4750 (0.4378)	
training:	Epoch: [84][34/204]	Loss 0.3408 (0.4349)	
training:	Epoch: [84][35/204]	Loss 0.4172 (0.4344)	
training:	Epoch: [84][36/204]	Loss 0.3802 (0.4329)	
training:	Epoch: [84][37/204]	Loss 0.4369 (0.4330)	
training:	Epoch: [84][38/204]	Loss 0.3439 (0.4307)	
training:	Epoch: [84][39/204]	Loss 0.4681 (0.4316)	
training:	Epoch: [84][40/204]	Loss 0.4725 (0.4327)	
training:	Epoch: [84][41/204]	Loss 0.3873 (0.4315)	
training:	Epoch: [84][42/204]	Loss 0.4245 (0.4314)	
training:	Epoch: [84][43/204]	Loss 0.2960 (0.4282)	
training:	Epoch: [84][44/204]	Loss 0.3759 (0.4270)	
training:	Epoch: [84][45/204]	Loss 0.3958 (0.4264)	
training:	Epoch: [84][46/204]	Loss 0.3544 (0.4248)	
training:	Epoch: [84][47/204]	Loss 0.5452 (0.4273)	
training:	Epoch: [84][48/204]	Loss 0.3393 (0.4255)	
training:	Epoch: [84][49/204]	Loss 0.4838 (0.4267)	
training:	Epoch: [84][50/204]	Loss 0.4522 (0.4272)	
training:	Epoch: [84][51/204]	Loss 0.4539 (0.4277)	
training:	Epoch: [84][52/204]	Loss 0.2363 (0.4241)	
training:	Epoch: [84][53/204]	Loss 0.4694 (0.4249)	
training:	Epoch: [84][54/204]	Loss 0.3908 (0.4243)	
training:	Epoch: [84][55/204]	Loss 0.3244 (0.4225)	
training:	Epoch: [84][56/204]	Loss 0.2791 (0.4199)	
training:	Epoch: [84][57/204]	Loss 0.3806 (0.4192)	
training:	Epoch: [84][58/204]	Loss 0.5058 (0.4207)	
training:	Epoch: [84][59/204]	Loss 0.3505 (0.4195)	
training:	Epoch: [84][60/204]	Loss 0.4942 (0.4208)	
training:	Epoch: [84][61/204]	Loss 0.4864 (0.4218)	
training:	Epoch: [84][62/204]	Loss 0.5326 (0.4236)	
training:	Epoch: [84][63/204]	Loss 0.6555 (0.4273)	
training:	Epoch: [84][64/204]	Loss 0.5586 (0.4294)	
training:	Epoch: [84][65/204]	Loss 0.4018 (0.4289)	
training:	Epoch: [84][66/204]	Loss 0.4236 (0.4288)	
training:	Epoch: [84][67/204]	Loss 0.3502 (0.4277)	
training:	Epoch: [84][68/204]	Loss 0.3145 (0.4260)	
training:	Epoch: [84][69/204]	Loss 0.3768 (0.4253)	
training:	Epoch: [84][70/204]	Loss 0.3871 (0.4248)	
training:	Epoch: [84][71/204]	Loss 0.3661 (0.4239)	
training:	Epoch: [84][72/204]	Loss 0.5125 (0.4252)	
training:	Epoch: [84][73/204]	Loss 0.5355 (0.4267)	
training:	Epoch: [84][74/204]	Loss 0.3409 (0.4255)	
training:	Epoch: [84][75/204]	Loss 0.5142 (0.4267)	
training:	Epoch: [84][76/204]	Loss 0.3631 (0.4259)	
training:	Epoch: [84][77/204]	Loss 0.4783 (0.4265)	
training:	Epoch: [84][78/204]	Loss 0.4817 (0.4272)	
training:	Epoch: [84][79/204]	Loss 0.3996 (0.4269)	
training:	Epoch: [84][80/204]	Loss 0.3577 (0.4260)	
training:	Epoch: [84][81/204]	Loss 0.3705 (0.4253)	
training:	Epoch: [84][82/204]	Loss 0.6516 (0.4281)	
training:	Epoch: [84][83/204]	Loss 0.5872 (0.4300)	
training:	Epoch: [84][84/204]	Loss 0.5030 (0.4309)	
training:	Epoch: [84][85/204]	Loss 0.3859 (0.4304)	
training:	Epoch: [84][86/204]	Loss 0.4312 (0.4304)	
training:	Epoch: [84][87/204]	Loss 0.4605 (0.4307)	
training:	Epoch: [84][88/204]	Loss 0.5030 (0.4315)	
training:	Epoch: [84][89/204]	Loss 0.4037 (0.4312)	
training:	Epoch: [84][90/204]	Loss 0.3315 (0.4301)	
training:	Epoch: [84][91/204]	Loss 0.4048 (0.4298)	
training:	Epoch: [84][92/204]	Loss 0.4057 (0.4296)	
training:	Epoch: [84][93/204]	Loss 0.3913 (0.4292)	
training:	Epoch: [84][94/204]	Loss 0.4677 (0.4296)	
training:	Epoch: [84][95/204]	Loss 0.3773 (0.4290)	
training:	Epoch: [84][96/204]	Loss 0.4661 (0.4294)	
training:	Epoch: [84][97/204]	Loss 0.3930 (0.4290)	
training:	Epoch: [84][98/204]	Loss 0.3968 (0.4287)	
training:	Epoch: [84][99/204]	Loss 0.5574 (0.4300)	
training:	Epoch: [84][100/204]	Loss 0.4911 (0.4306)	
training:	Epoch: [84][101/204]	Loss 0.3933 (0.4302)	
training:	Epoch: [84][102/204]	Loss 0.3573 (0.4295)	
training:	Epoch: [84][103/204]	Loss 0.3784 (0.4290)	
training:	Epoch: [84][104/204]	Loss 0.4508 (0.4292)	
training:	Epoch: [84][105/204]	Loss 0.4075 (0.4290)	
training:	Epoch: [84][106/204]	Loss 0.3375 (0.4282)	
training:	Epoch: [84][107/204]	Loss 0.6376 (0.4301)	
training:	Epoch: [84][108/204]	Loss 0.4263 (0.4301)	
training:	Epoch: [84][109/204]	Loss 0.3580 (0.4294)	
training:	Epoch: [84][110/204]	Loss 0.3689 (0.4289)	
training:	Epoch: [84][111/204]	Loss 0.4031 (0.4287)	
training:	Epoch: [84][112/204]	Loss 0.5777 (0.4300)	
training:	Epoch: [84][113/204]	Loss 0.4621 (0.4303)	
training:	Epoch: [84][114/204]	Loss 0.4789 (0.4307)	
training:	Epoch: [84][115/204]	Loss 0.3271 (0.4298)	
training:	Epoch: [84][116/204]	Loss 0.3773 (0.4293)	
training:	Epoch: [84][117/204]	Loss 0.4561 (0.4296)	
training:	Epoch: [84][118/204]	Loss 0.3250 (0.4287)	
training:	Epoch: [84][119/204]	Loss 0.4684 (0.4290)	
training:	Epoch: [84][120/204]	Loss 0.4239 (0.4290)	
training:	Epoch: [84][121/204]	Loss 0.4141 (0.4289)	
training:	Epoch: [84][122/204]	Loss 0.5669 (0.4300)	
training:	Epoch: [84][123/204]	Loss 0.5047 (0.4306)	
training:	Epoch: [84][124/204]	Loss 0.3706 (0.4301)	
training:	Epoch: [84][125/204]	Loss 0.3344 (0.4293)	
training:	Epoch: [84][126/204]	Loss 0.3367 (0.4286)	
training:	Epoch: [84][127/204]	Loss 0.5282 (0.4294)	
training:	Epoch: [84][128/204]	Loss 0.3046 (0.4284)	
training:	Epoch: [84][129/204]	Loss 0.5090 (0.4290)	
training:	Epoch: [84][130/204]	Loss 0.4669 (0.4293)	
training:	Epoch: [84][131/204]	Loss 0.5408 (0.4302)	
training:	Epoch: [84][132/204]	Loss 0.3658 (0.4297)	
training:	Epoch: [84][133/204]	Loss 0.4357 (0.4297)	
training:	Epoch: [84][134/204]	Loss 0.4256 (0.4297)	
training:	Epoch: [84][135/204]	Loss 0.3659 (0.4292)	
training:	Epoch: [84][136/204]	Loss 0.5756 (0.4303)	
training:	Epoch: [84][137/204]	Loss 0.4585 (0.4305)	
training:	Epoch: [84][138/204]	Loss 0.4475 (0.4306)	
training:	Epoch: [84][139/204]	Loss 0.5076 (0.4312)	
training:	Epoch: [84][140/204]	Loss 0.3781 (0.4308)	
training:	Epoch: [84][141/204]	Loss 0.2712 (0.4297)	
training:	Epoch: [84][142/204]	Loss 0.3972 (0.4295)	
training:	Epoch: [84][143/204]	Loss 0.4700 (0.4297)	
training:	Epoch: [84][144/204]	Loss 0.2581 (0.4285)	
training:	Epoch: [84][145/204]	Loss 0.5355 (0.4293)	
training:	Epoch: [84][146/204]	Loss 0.3770 (0.4289)	
training:	Epoch: [84][147/204]	Loss 0.3863 (0.4286)	
training:	Epoch: [84][148/204]	Loss 0.3495 (0.4281)	
training:	Epoch: [84][149/204]	Loss 0.3517 (0.4276)	
training:	Epoch: [84][150/204]	Loss 0.3036 (0.4268)	
training:	Epoch: [84][151/204]	Loss 0.4874 (0.4272)	
training:	Epoch: [84][152/204]	Loss 0.4679 (0.4274)	
training:	Epoch: [84][153/204]	Loss 0.3578 (0.4270)	
training:	Epoch: [84][154/204]	Loss 0.5017 (0.4275)	
training:	Epoch: [84][155/204]	Loss 0.5768 (0.4284)	
training:	Epoch: [84][156/204]	Loss 0.4889 (0.4288)	
training:	Epoch: [84][157/204]	Loss 0.4175 (0.4287)	
training:	Epoch: [84][158/204]	Loss 0.4531 (0.4289)	
training:	Epoch: [84][159/204]	Loss 0.4831 (0.4292)	
training:	Epoch: [84][160/204]	Loss 0.5184 (0.4298)	
training:	Epoch: [84][161/204]	Loss 0.5796 (0.4307)	
training:	Epoch: [84][162/204]	Loss 0.4695 (0.4310)	
training:	Epoch: [84][163/204]	Loss 0.3777 (0.4306)	
training:	Epoch: [84][164/204]	Loss 0.4991 (0.4311)	
training:	Epoch: [84][165/204]	Loss 0.4372 (0.4311)	
training:	Epoch: [84][166/204]	Loss 0.4767 (0.4314)	
training:	Epoch: [84][167/204]	Loss 0.5798 (0.4323)	
training:	Epoch: [84][168/204]	Loss 0.3770 (0.4319)	
training:	Epoch: [84][169/204]	Loss 0.3751 (0.4316)	
training:	Epoch: [84][170/204]	Loss 0.3742 (0.4312)	
training:	Epoch: [84][171/204]	Loss 0.4039 (0.4311)	
training:	Epoch: [84][172/204]	Loss 0.4881 (0.4314)	
training:	Epoch: [84][173/204]	Loss 0.5889 (0.4323)	
training:	Epoch: [84][174/204]	Loss 0.5409 (0.4330)	
training:	Epoch: [84][175/204]	Loss 0.5857 (0.4338)	
training:	Epoch: [84][176/204]	Loss 0.3617 (0.4334)	
training:	Epoch: [84][177/204]	Loss 0.4181 (0.4333)	
training:	Epoch: [84][178/204]	Loss 0.6492 (0.4345)	
training:	Epoch: [84][179/204]	Loss 0.2806 (0.4337)	
training:	Epoch: [84][180/204]	Loss 0.4165 (0.4336)	
training:	Epoch: [84][181/204]	Loss 0.3913 (0.4334)	
training:	Epoch: [84][182/204]	Loss 0.4634 (0.4335)	
training:	Epoch: [84][183/204]	Loss 0.5650 (0.4342)	
training:	Epoch: [84][184/204]	Loss 0.4967 (0.4346)	
training:	Epoch: [84][185/204]	Loss 0.3974 (0.4344)	
training:	Epoch: [84][186/204]	Loss 0.4136 (0.4343)	
training:	Epoch: [84][187/204]	Loss 0.5307 (0.4348)	
training:	Epoch: [84][188/204]	Loss 0.2684 (0.4339)	
training:	Epoch: [84][189/204]	Loss 0.2965 (0.4332)	
training:	Epoch: [84][190/204]	Loss 0.4677 (0.4334)	
training:	Epoch: [84][191/204]	Loss 0.3321 (0.4328)	
training:	Epoch: [84][192/204]	Loss 0.4444 (0.4329)	
training:	Epoch: [84][193/204]	Loss 0.3929 (0.4327)	
training:	Epoch: [84][194/204]	Loss 0.4662 (0.4328)	
training:	Epoch: [84][195/204]	Loss 0.5275 (0.4333)	
training:	Epoch: [84][196/204]	Loss 0.4507 (0.4334)	
training:	Epoch: [84][197/204]	Loss 0.4253 (0.4334)	
training:	Epoch: [84][198/204]	Loss 0.3856 (0.4331)	
training:	Epoch: [84][199/204]	Loss 0.3208 (0.4326)	
training:	Epoch: [84][200/204]	Loss 0.5277 (0.4330)	
training:	Epoch: [84][201/204]	Loss 0.5375 (0.4336)	
training:	Epoch: [84][202/204]	Loss 0.6059 (0.4344)	
training:	Epoch: [84][203/204]	Loss 0.5296 (0.4349)	
training:	Epoch: [84][204/204]	Loss 0.4751 (0.4351)	
Training:	 Loss: 0.4344

Training:	 ACC: 0.8144 0.8146 0.8207 0.8080
Validation:	 ACC: 0.7898 0.7903 0.7994 0.7803
Validation:	 Best_BACC: 0.7925 0.7935 0.8137 0.7713
Validation:	 Loss: 0.4535
Pretraining:	Epoch 85/120
----------
training:	Epoch: [85][1/204]	Loss 0.3157 (0.3157)	
training:	Epoch: [85][2/204]	Loss 0.4048 (0.3603)	
training:	Epoch: [85][3/204]	Loss 0.3827 (0.3677)	
training:	Epoch: [85][4/204]	Loss 0.3106 (0.3535)	
training:	Epoch: [85][5/204]	Loss 0.2830 (0.3394)	
training:	Epoch: [85][6/204]	Loss 0.4357 (0.3554)	
training:	Epoch: [85][7/204]	Loss 0.4491 (0.3688)	
training:	Epoch: [85][8/204]	Loss 0.4097 (0.3739)	
training:	Epoch: [85][9/204]	Loss 0.3968 (0.3765)	
training:	Epoch: [85][10/204]	Loss 0.5199 (0.3908)	
training:	Epoch: [85][11/204]	Loss 0.4848 (0.3993)	
training:	Epoch: [85][12/204]	Loss 0.3908 (0.3986)	
training:	Epoch: [85][13/204]	Loss 0.3708 (0.3965)	
training:	Epoch: [85][14/204]	Loss 0.4574 (0.4008)	
training:	Epoch: [85][15/204]	Loss 0.4204 (0.4021)	
training:	Epoch: [85][16/204]	Loss 0.8055 (0.4274)	
training:	Epoch: [85][17/204]	Loss 0.7305 (0.4452)	
training:	Epoch: [85][18/204]	Loss 0.5088 (0.4487)	
training:	Epoch: [85][19/204]	Loss 0.4777 (0.4502)	
training:	Epoch: [85][20/204]	Loss 0.4955 (0.4525)	
training:	Epoch: [85][21/204]	Loss 0.3385 (0.4471)	
training:	Epoch: [85][22/204]	Loss 0.4791 (0.4485)	
training:	Epoch: [85][23/204]	Loss 0.4068 (0.4467)	
training:	Epoch: [85][24/204]	Loss 0.3881 (0.4443)	
training:	Epoch: [85][25/204]	Loss 0.5633 (0.4490)	
training:	Epoch: [85][26/204]	Loss 0.3911 (0.4468)	
training:	Epoch: [85][27/204]	Loss 0.3972 (0.4450)	
training:	Epoch: [85][28/204]	Loss 0.4198 (0.4441)	
training:	Epoch: [85][29/204]	Loss 0.3806 (0.4419)	
training:	Epoch: [85][30/204]	Loss 0.4514 (0.4422)	
training:	Epoch: [85][31/204]	Loss 0.4318 (0.4419)	
training:	Epoch: [85][32/204]	Loss 0.5674 (0.4458)	
training:	Epoch: [85][33/204]	Loss 0.3130 (0.4418)	
training:	Epoch: [85][34/204]	Loss 0.5766 (0.4457)	
training:	Epoch: [85][35/204]	Loss 0.4766 (0.4466)	
training:	Epoch: [85][36/204]	Loss 0.4148 (0.4457)	
training:	Epoch: [85][37/204]	Loss 0.6579 (0.4515)	
training:	Epoch: [85][38/204]	Loss 0.3605 (0.4491)	
training:	Epoch: [85][39/204]	Loss 0.4162 (0.4482)	
training:	Epoch: [85][40/204]	Loss 0.3680 (0.4462)	
training:	Epoch: [85][41/204]	Loss 0.3592 (0.4441)	
training:	Epoch: [85][42/204]	Loss 0.6254 (0.4484)	
training:	Epoch: [85][43/204]	Loss 0.5425 (0.4506)	
training:	Epoch: [85][44/204]	Loss 0.6424 (0.4550)	
training:	Epoch: [85][45/204]	Loss 0.3389 (0.4524)	
training:	Epoch: [85][46/204]	Loss 0.4288 (0.4519)	
training:	Epoch: [85][47/204]	Loss 0.4585 (0.4520)	
training:	Epoch: [85][48/204]	Loss 0.4073 (0.4511)	
training:	Epoch: [85][49/204]	Loss 0.4583 (0.4512)	
training:	Epoch: [85][50/204]	Loss 0.4361 (0.4509)	
training:	Epoch: [85][51/204]	Loss 0.2757 (0.4475)	
training:	Epoch: [85][52/204]	Loss 0.4888 (0.4483)	
training:	Epoch: [85][53/204]	Loss 0.4723 (0.4487)	
training:	Epoch: [85][54/204]	Loss 0.3160 (0.4463)	
training:	Epoch: [85][55/204]	Loss 0.5318 (0.4478)	
training:	Epoch: [85][56/204]	Loss 0.4495 (0.4479)	
training:	Epoch: [85][57/204]	Loss 0.3220 (0.4457)	
training:	Epoch: [85][58/204]	Loss 0.4743 (0.4461)	
training:	Epoch: [85][59/204]	Loss 0.4598 (0.4464)	
training:	Epoch: [85][60/204]	Loss 0.5329 (0.4478)	
training:	Epoch: [85][61/204]	Loss 0.5594 (0.4496)	
training:	Epoch: [85][62/204]	Loss 0.4358 (0.4494)	
training:	Epoch: [85][63/204]	Loss 0.3739 (0.4482)	
training:	Epoch: [85][64/204]	Loss 0.4956 (0.4490)	
training:	Epoch: [85][65/204]	Loss 0.3542 (0.4475)	
training:	Epoch: [85][66/204]	Loss 0.2627 (0.4447)	
training:	Epoch: [85][67/204]	Loss 0.3017 (0.4426)	
training:	Epoch: [85][68/204]	Loss 0.3940 (0.4419)	
training:	Epoch: [85][69/204]	Loss 0.3865 (0.4411)	
training:	Epoch: [85][70/204]	Loss 0.5704 (0.4429)	
training:	Epoch: [85][71/204]	Loss 0.3759 (0.4420)	
training:	Epoch: [85][72/204]	Loss 0.4051 (0.4414)	
training:	Epoch: [85][73/204]	Loss 0.6430 (0.4442)	
training:	Epoch: [85][74/204]	Loss 0.4342 (0.4441)	
training:	Epoch: [85][75/204]	Loss 0.5482 (0.4455)	
training:	Epoch: [85][76/204]	Loss 0.4466 (0.4455)	
training:	Epoch: [85][77/204]	Loss 0.5310 (0.4466)	
training:	Epoch: [85][78/204]	Loss 0.3953 (0.4459)	
training:	Epoch: [85][79/204]	Loss 0.5189 (0.4469)	
training:	Epoch: [85][80/204]	Loss 0.5700 (0.4484)	
training:	Epoch: [85][81/204]	Loss 0.4951 (0.4490)	
training:	Epoch: [85][82/204]	Loss 0.2937 (0.4471)	
training:	Epoch: [85][83/204]	Loss 0.2926 (0.4452)	
training:	Epoch: [85][84/204]	Loss 0.3391 (0.4440)	
training:	Epoch: [85][85/204]	Loss 0.5214 (0.4449)	
training:	Epoch: [85][86/204]	Loss 0.4943 (0.4454)	
training:	Epoch: [85][87/204]	Loss 0.5300 (0.4464)	
training:	Epoch: [85][88/204]	Loss 0.4164 (0.4461)	
training:	Epoch: [85][89/204]	Loss 0.3588 (0.4451)	
training:	Epoch: [85][90/204]	Loss 0.3227 (0.4437)	
training:	Epoch: [85][91/204]	Loss 0.4444 (0.4437)	
training:	Epoch: [85][92/204]	Loss 0.4991 (0.4443)	
training:	Epoch: [85][93/204]	Loss 0.3374 (0.4432)	
training:	Epoch: [85][94/204]	Loss 0.5754 (0.4446)	
training:	Epoch: [85][95/204]	Loss 0.3698 (0.4438)	
training:	Epoch: [85][96/204]	Loss 0.3811 (0.4432)	
training:	Epoch: [85][97/204]	Loss 0.3980 (0.4427)	
training:	Epoch: [85][98/204]	Loss 0.3653 (0.4419)	
training:	Epoch: [85][99/204]	Loss 0.4800 (0.4423)	
training:	Epoch: [85][100/204]	Loss 0.4443 (0.4423)	
training:	Epoch: [85][101/204]	Loss 0.4754 (0.4426)	
training:	Epoch: [85][102/204]	Loss 0.4297 (0.4425)	
training:	Epoch: [85][103/204]	Loss 0.4123 (0.4422)	
training:	Epoch: [85][104/204]	Loss 0.4214 (0.4420)	
training:	Epoch: [85][105/204]	Loss 0.3589 (0.4412)	
training:	Epoch: [85][106/204]	Loss 0.3584 (0.4404)	
training:	Epoch: [85][107/204]	Loss 0.3611 (0.4397)	
training:	Epoch: [85][108/204]	Loss 0.3969 (0.4393)	
training:	Epoch: [85][109/204]	Loss 0.4233 (0.4392)	
training:	Epoch: [85][110/204]	Loss 0.3095 (0.4380)	
training:	Epoch: [85][111/204]	Loss 0.4146 (0.4378)	
training:	Epoch: [85][112/204]	Loss 0.3404 (0.4369)	
training:	Epoch: [85][113/204]	Loss 0.3403 (0.4360)	
training:	Epoch: [85][114/204]	Loss 0.3951 (0.4357)	
training:	Epoch: [85][115/204]	Loss 0.3925 (0.4353)	
training:	Epoch: [85][116/204]	Loss 0.6090 (0.4368)	
training:	Epoch: [85][117/204]	Loss 0.3246 (0.4358)	
training:	Epoch: [85][118/204]	Loss 0.5362 (0.4367)	
training:	Epoch: [85][119/204]	Loss 0.4681 (0.4370)	
training:	Epoch: [85][120/204]	Loss 0.5073 (0.4375)	
training:	Epoch: [85][121/204]	Loss 0.4704 (0.4378)	
training:	Epoch: [85][122/204]	Loss 0.3544 (0.4371)	
training:	Epoch: [85][123/204]	Loss 0.4167 (0.4370)	
training:	Epoch: [85][124/204]	Loss 0.4835 (0.4373)	
training:	Epoch: [85][125/204]	Loss 0.3780 (0.4369)	
training:	Epoch: [85][126/204]	Loss 0.3032 (0.4358)	
training:	Epoch: [85][127/204]	Loss 0.6871 (0.4378)	
training:	Epoch: [85][128/204]	Loss 0.5609 (0.4387)	
training:	Epoch: [85][129/204]	Loss 0.3534 (0.4381)	
training:	Epoch: [85][130/204]	Loss 0.6472 (0.4397)	
training:	Epoch: [85][131/204]	Loss 0.3939 (0.4393)	
training:	Epoch: [85][132/204]	Loss 0.4918 (0.4397)	
training:	Epoch: [85][133/204]	Loss 0.5514 (0.4406)	
training:	Epoch: [85][134/204]	Loss 0.4767 (0.4409)	
training:	Epoch: [85][135/204]	Loss 0.4992 (0.4413)	
training:	Epoch: [85][136/204]	Loss 0.3862 (0.4409)	
training:	Epoch: [85][137/204]	Loss 0.4862 (0.4412)	
training:	Epoch: [85][138/204]	Loss 0.5747 (0.4422)	
training:	Epoch: [85][139/204]	Loss 0.5550 (0.4430)	
training:	Epoch: [85][140/204]	Loss 0.3785 (0.4425)	
training:	Epoch: [85][141/204]	Loss 0.3466 (0.4418)	
training:	Epoch: [85][142/204]	Loss 0.5432 (0.4426)	
training:	Epoch: [85][143/204]	Loss 0.4940 (0.4429)	
training:	Epoch: [85][144/204]	Loss 0.3799 (0.4425)	
training:	Epoch: [85][145/204]	Loss 0.4235 (0.4424)	
training:	Epoch: [85][146/204]	Loss 0.4644 (0.4425)	
training:	Epoch: [85][147/204]	Loss 0.3875 (0.4421)	
training:	Epoch: [85][148/204]	Loss 0.4175 (0.4420)	
training:	Epoch: [85][149/204]	Loss 0.4559 (0.4421)	
training:	Epoch: [85][150/204]	Loss 0.3196 (0.4412)	
training:	Epoch: [85][151/204]	Loss 0.2929 (0.4403)	
training:	Epoch: [85][152/204]	Loss 0.3120 (0.4394)	
training:	Epoch: [85][153/204]	Loss 0.3784 (0.4390)	
training:	Epoch: [85][154/204]	Loss 0.3976 (0.4387)	
training:	Epoch: [85][155/204]	Loss 0.2971 (0.4378)	
training:	Epoch: [85][156/204]	Loss 0.4722 (0.4381)	
training:	Epoch: [85][157/204]	Loss 0.6011 (0.4391)	
training:	Epoch: [85][158/204]	Loss 0.3687 (0.4386)	
training:	Epoch: [85][159/204]	Loss 0.3649 (0.4382)	
training:	Epoch: [85][160/204]	Loss 0.3439 (0.4376)	
training:	Epoch: [85][161/204]	Loss 0.4946 (0.4379)	
training:	Epoch: [85][162/204]	Loss 0.4739 (0.4382)	
training:	Epoch: [85][163/204]	Loss 0.5365 (0.4388)	
training:	Epoch: [85][164/204]	Loss 0.3564 (0.4383)	
training:	Epoch: [85][165/204]	Loss 0.4810 (0.4385)	
training:	Epoch: [85][166/204]	Loss 0.4087 (0.4383)	
training:	Epoch: [85][167/204]	Loss 0.4192 (0.4382)	
training:	Epoch: [85][168/204]	Loss 0.3049 (0.4374)	
training:	Epoch: [85][169/204]	Loss 0.3584 (0.4370)	
training:	Epoch: [85][170/204]	Loss 0.3914 (0.4367)	
training:	Epoch: [85][171/204]	Loss 0.3423 (0.4362)	
training:	Epoch: [85][172/204]	Loss 0.4213 (0.4361)	
training:	Epoch: [85][173/204]	Loss 0.2832 (0.4352)	
training:	Epoch: [85][174/204]	Loss 0.3131 (0.4345)	
training:	Epoch: [85][175/204]	Loss 0.4928 (0.4348)	
training:	Epoch: [85][176/204]	Loss 0.4003 (0.4346)	
training:	Epoch: [85][177/204]	Loss 0.2442 (0.4335)	
training:	Epoch: [85][178/204]	Loss 0.2911 (0.4327)	
training:	Epoch: [85][179/204]	Loss 0.3059 (0.4320)	
training:	Epoch: [85][180/204]	Loss 0.4102 (0.4319)	
training:	Epoch: [85][181/204]	Loss 0.4148 (0.4318)	
training:	Epoch: [85][182/204]	Loss 0.4135 (0.4317)	
training:	Epoch: [85][183/204]	Loss 0.3772 (0.4314)	
training:	Epoch: [85][184/204]	Loss 0.4925 (0.4318)	
training:	Epoch: [85][185/204]	Loss 0.4974 (0.4321)	
training:	Epoch: [85][186/204]	Loss 0.3582 (0.4317)	
training:	Epoch: [85][187/204]	Loss 0.5464 (0.4323)	
training:	Epoch: [85][188/204]	Loss 0.4797 (0.4326)	
training:	Epoch: [85][189/204]	Loss 0.5634 (0.4333)	
training:	Epoch: [85][190/204]	Loss 0.5827 (0.4341)	
training:	Epoch: [85][191/204]	Loss 0.5226 (0.4345)	
training:	Epoch: [85][192/204]	Loss 0.6601 (0.4357)	
training:	Epoch: [85][193/204]	Loss 0.3141 (0.4351)	
training:	Epoch: [85][194/204]	Loss 0.3195 (0.4345)	
training:	Epoch: [85][195/204]	Loss 0.4400 (0.4345)	
training:	Epoch: [85][196/204]	Loss 0.4916 (0.4348)	
training:	Epoch: [85][197/204]	Loss 0.3267 (0.4342)	
training:	Epoch: [85][198/204]	Loss 0.3619 (0.4339)	
training:	Epoch: [85][199/204]	Loss 0.6709 (0.4351)	
training:	Epoch: [85][200/204]	Loss 0.4851 (0.4353)	
training:	Epoch: [85][201/204]	Loss 0.4105 (0.4352)	
training:	Epoch: [85][202/204]	Loss 0.5953 (0.4360)	
training:	Epoch: [85][203/204]	Loss 0.4916 (0.4363)	
training:	Epoch: [85][204/204]	Loss 0.4841 (0.4365)	
Training:	 Loss: 0.4358

Training:	 ACC: 0.8166 0.8171 0.8292 0.8039
Validation:	 ACC: 0.7913 0.7919 0.8045 0.7780
Validation:	 Best_BACC: 0.7925 0.7935 0.8137 0.7713
Validation:	 Loss: 0.4525
Pretraining:	Epoch 86/120
----------
training:	Epoch: [86][1/204]	Loss 0.5006 (0.5006)	
training:	Epoch: [86][2/204]	Loss 0.5440 (0.5223)	
training:	Epoch: [86][3/204]	Loss 0.6806 (0.5751)	
training:	Epoch: [86][4/204]	Loss 0.4496 (0.5437)	
training:	Epoch: [86][5/204]	Loss 0.3829 (0.5116)	
training:	Epoch: [86][6/204]	Loss 0.5423 (0.5167)	
training:	Epoch: [86][7/204]	Loss 0.6077 (0.5297)	
training:	Epoch: [86][8/204]	Loss 0.6358 (0.5430)	
training:	Epoch: [86][9/204]	Loss 0.7403 (0.5649)	
training:	Epoch: [86][10/204]	Loss 0.4170 (0.5501)	
training:	Epoch: [86][11/204]	Loss 0.3642 (0.5332)	
training:	Epoch: [86][12/204]	Loss 0.3469 (0.5177)	
training:	Epoch: [86][13/204]	Loss 0.4117 (0.5095)	
training:	Epoch: [86][14/204]	Loss 0.4415 (0.5047)	
training:	Epoch: [86][15/204]	Loss 0.4453 (0.5007)	
training:	Epoch: [86][16/204]	Loss 0.4536 (0.4978)	
training:	Epoch: [86][17/204]	Loss 0.5602 (0.5014)	
training:	Epoch: [86][18/204]	Loss 0.4153 (0.4966)	
training:	Epoch: [86][19/204]	Loss 0.4942 (0.4965)	
training:	Epoch: [86][20/204]	Loss 0.5006 (0.4967)	
training:	Epoch: [86][21/204]	Loss 0.4483 (0.4944)	
training:	Epoch: [86][22/204]	Loss 0.3037 (0.4857)	
training:	Epoch: [86][23/204]	Loss 0.3652 (0.4805)	
training:	Epoch: [86][24/204]	Loss 0.4870 (0.4808)	
training:	Epoch: [86][25/204]	Loss 0.5270 (0.4826)	
training:	Epoch: [86][26/204]	Loss 0.2761 (0.4747)	
training:	Epoch: [86][27/204]	Loss 0.3157 (0.4688)	
training:	Epoch: [86][28/204]	Loss 0.4351 (0.4676)	
training:	Epoch: [86][29/204]	Loss 0.5075 (0.4690)	
training:	Epoch: [86][30/204]	Loss 0.5486 (0.4716)	
training:	Epoch: [86][31/204]	Loss 0.4519 (0.4710)	
training:	Epoch: [86][32/204]	Loss 0.3179 (0.4662)	
training:	Epoch: [86][33/204]	Loss 0.5473 (0.4687)	
training:	Epoch: [86][34/204]	Loss 0.4586 (0.4684)	
training:	Epoch: [86][35/204]	Loss 0.4656 (0.4683)	
training:	Epoch: [86][36/204]	Loss 0.3733 (0.4656)	
training:	Epoch: [86][37/204]	Loss 0.3562 (0.4627)	
training:	Epoch: [86][38/204]	Loss 0.3611 (0.4600)	
training:	Epoch: [86][39/204]	Loss 0.5403 (0.4621)	
training:	Epoch: [86][40/204]	Loss 0.5931 (0.4654)	
training:	Epoch: [86][41/204]	Loss 0.5439 (0.4673)	
training:	Epoch: [86][42/204]	Loss 0.3517 (0.4645)	
training:	Epoch: [86][43/204]	Loss 0.3704 (0.4623)	
training:	Epoch: [86][44/204]	Loss 0.5282 (0.4638)	
training:	Epoch: [86][45/204]	Loss 0.3336 (0.4609)	
training:	Epoch: [86][46/204]	Loss 0.3850 (0.4593)	
training:	Epoch: [86][47/204]	Loss 0.2862 (0.4556)	
training:	Epoch: [86][48/204]	Loss 0.4475 (0.4554)	
training:	Epoch: [86][49/204]	Loss 0.4808 (0.4560)	
training:	Epoch: [86][50/204]	Loss 0.5347 (0.4575)	
training:	Epoch: [86][51/204]	Loss 0.3975 (0.4564)	
training:	Epoch: [86][52/204]	Loss 0.3104 (0.4535)	
training:	Epoch: [86][53/204]	Loss 0.5635 (0.4556)	
training:	Epoch: [86][54/204]	Loss 0.3730 (0.4541)	
training:	Epoch: [86][55/204]	Loss 0.4102 (0.4533)	
training:	Epoch: [86][56/204]	Loss 0.4265 (0.4528)	
training:	Epoch: [86][57/204]	Loss 0.2855 (0.4499)	
training:	Epoch: [86][58/204]	Loss 0.4562 (0.4500)	
training:	Epoch: [86][59/204]	Loss 0.6042 (0.4526)	
training:	Epoch: [86][60/204]	Loss 0.4751 (0.4530)	
training:	Epoch: [86][61/204]	Loss 0.3088 (0.4506)	
training:	Epoch: [86][62/204]	Loss 0.4705 (0.4509)	
training:	Epoch: [86][63/204]	Loss 0.5599 (0.4527)	
training:	Epoch: [86][64/204]	Loss 0.3637 (0.4513)	
training:	Epoch: [86][65/204]	Loss 0.6345 (0.4541)	
training:	Epoch: [86][66/204]	Loss 0.4589 (0.4542)	
training:	Epoch: [86][67/204]	Loss 0.4608 (0.4543)	
training:	Epoch: [86][68/204]	Loss 0.3262 (0.4524)	
training:	Epoch: [86][69/204]	Loss 0.3284 (0.4506)	
training:	Epoch: [86][70/204]	Loss 0.4648 (0.4508)	
training:	Epoch: [86][71/204]	Loss 0.5167 (0.4517)	
training:	Epoch: [86][72/204]	Loss 0.3367 (0.4501)	
training:	Epoch: [86][73/204]	Loss 0.5157 (0.4510)	
training:	Epoch: [86][74/204]	Loss 0.4083 (0.4504)	
training:	Epoch: [86][75/204]	Loss 0.2805 (0.4482)	
training:	Epoch: [86][76/204]	Loss 0.4017 (0.4476)	
training:	Epoch: [86][77/204]	Loss 0.5482 (0.4489)	
training:	Epoch: [86][78/204]	Loss 0.4984 (0.4495)	
training:	Epoch: [86][79/204]	Loss 0.6042 (0.4515)	
training:	Epoch: [86][80/204]	Loss 0.4091 (0.4509)	
training:	Epoch: [86][81/204]	Loss 0.4716 (0.4512)	
training:	Epoch: [86][82/204]	Loss 0.4704 (0.4514)	
training:	Epoch: [86][83/204]	Loss 0.4373 (0.4513)	
training:	Epoch: [86][84/204]	Loss 0.4200 (0.4509)	
training:	Epoch: [86][85/204]	Loss 0.3629 (0.4498)	
training:	Epoch: [86][86/204]	Loss 0.5143 (0.4506)	
training:	Epoch: [86][87/204]	Loss 0.3815 (0.4498)	
training:	Epoch: [86][88/204]	Loss 0.4634 (0.4500)	
training:	Epoch: [86][89/204]	Loss 0.4806 (0.4503)	
training:	Epoch: [86][90/204]	Loss 0.5324 (0.4512)	
training:	Epoch: [86][91/204]	Loss 0.3795 (0.4504)	
training:	Epoch: [86][92/204]	Loss 0.3909 (0.4498)	
training:	Epoch: [86][93/204]	Loss 0.3814 (0.4490)	
training:	Epoch: [86][94/204]	Loss 0.3719 (0.4482)	
training:	Epoch: [86][95/204]	Loss 0.5476 (0.4493)	
training:	Epoch: [86][96/204]	Loss 0.4665 (0.4494)	
training:	Epoch: [86][97/204]	Loss 0.3123 (0.4480)	
training:	Epoch: [86][98/204]	Loss 0.4274 (0.4478)	
training:	Epoch: [86][99/204]	Loss 0.4264 (0.4476)	
training:	Epoch: [86][100/204]	Loss 0.4817 (0.4479)	
training:	Epoch: [86][101/204]	Loss 0.2744 (0.4462)	
training:	Epoch: [86][102/204]	Loss 0.3460 (0.4452)	
training:	Epoch: [86][103/204]	Loss 0.4670 (0.4455)	
training:	Epoch: [86][104/204]	Loss 0.5402 (0.4464)	
training:	Epoch: [86][105/204]	Loss 0.2514 (0.4445)	
training:	Epoch: [86][106/204]	Loss 0.3298 (0.4434)	
training:	Epoch: [86][107/204]	Loss 0.3585 (0.4426)	
training:	Epoch: [86][108/204]	Loss 0.3228 (0.4415)	
training:	Epoch: [86][109/204]	Loss 0.4518 (0.4416)	
training:	Epoch: [86][110/204]	Loss 0.4735 (0.4419)	
training:	Epoch: [86][111/204]	Loss 0.3084 (0.4407)	
training:	Epoch: [86][112/204]	Loss 0.4228 (0.4405)	
training:	Epoch: [86][113/204]	Loss 0.3258 (0.4395)	
training:	Epoch: [86][114/204]	Loss 0.2876 (0.4382)	
training:	Epoch: [86][115/204]	Loss 0.2859 (0.4369)	
training:	Epoch: [86][116/204]	Loss 0.4565 (0.4370)	
training:	Epoch: [86][117/204]	Loss 0.5112 (0.4377)	
training:	Epoch: [86][118/204]	Loss 0.6027 (0.4391)	
training:	Epoch: [86][119/204]	Loss 0.3173 (0.4381)	
training:	Epoch: [86][120/204]	Loss 0.3571 (0.4374)	
training:	Epoch: [86][121/204]	Loss 0.6352 (0.4390)	
training:	Epoch: [86][122/204]	Loss 0.5987 (0.4403)	
training:	Epoch: [86][123/204]	Loss 0.4709 (0.4406)	
training:	Epoch: [86][124/204]	Loss 0.5128 (0.4412)	
training:	Epoch: [86][125/204]	Loss 0.3883 (0.4407)	
training:	Epoch: [86][126/204]	Loss 0.4694 (0.4410)	
training:	Epoch: [86][127/204]	Loss 0.4625 (0.4411)	
training:	Epoch: [86][128/204]	Loss 0.5020 (0.4416)	
training:	Epoch: [86][129/204]	Loss 0.5239 (0.4422)	
training:	Epoch: [86][130/204]	Loss 0.3796 (0.4418)	
training:	Epoch: [86][131/204]	Loss 0.4945 (0.4422)	
training:	Epoch: [86][132/204]	Loss 0.3118 (0.4412)	
training:	Epoch: [86][133/204]	Loss 0.3555 (0.4405)	
training:	Epoch: [86][134/204]	Loss 0.3929 (0.4402)	
training:	Epoch: [86][135/204]	Loss 0.4309 (0.4401)	
training:	Epoch: [86][136/204]	Loss 0.4049 (0.4398)	
training:	Epoch: [86][137/204]	Loss 0.3512 (0.4392)	
training:	Epoch: [86][138/204]	Loss 0.4185 (0.4390)	
training:	Epoch: [86][139/204]	Loss 0.4788 (0.4393)	
training:	Epoch: [86][140/204]	Loss 0.2945 (0.4383)	
training:	Epoch: [86][141/204]	Loss 0.2431 (0.4369)	
training:	Epoch: [86][142/204]	Loss 0.5071 (0.4374)	
training:	Epoch: [86][143/204]	Loss 0.4802 (0.4377)	
training:	Epoch: [86][144/204]	Loss 0.2813 (0.4366)	
training:	Epoch: [86][145/204]	Loss 0.3381 (0.4359)	
training:	Epoch: [86][146/204]	Loss 0.4416 (0.4360)	
training:	Epoch: [86][147/204]	Loss 0.3670 (0.4355)	
training:	Epoch: [86][148/204]	Loss 0.4004 (0.4353)	
training:	Epoch: [86][149/204]	Loss 0.4525 (0.4354)	
training:	Epoch: [86][150/204]	Loss 0.4394 (0.4354)	
training:	Epoch: [86][151/204]	Loss 0.4891 (0.4358)	
training:	Epoch: [86][152/204]	Loss 0.3040 (0.4349)	
training:	Epoch: [86][153/204]	Loss 0.5094 (0.4354)	
training:	Epoch: [86][154/204]	Loss 0.3850 (0.4351)	
training:	Epoch: [86][155/204]	Loss 0.3989 (0.4348)	
training:	Epoch: [86][156/204]	Loss 0.4980 (0.4352)	
training:	Epoch: [86][157/204]	Loss 0.5167 (0.4358)	
training:	Epoch: [86][158/204]	Loss 0.5120 (0.4362)	
training:	Epoch: [86][159/204]	Loss 0.3763 (0.4359)	
training:	Epoch: [86][160/204]	Loss 0.3945 (0.4356)	
training:	Epoch: [86][161/204]	Loss 0.4019 (0.4354)	
training:	Epoch: [86][162/204]	Loss 0.4096 (0.4352)	
training:	Epoch: [86][163/204]	Loss 0.3975 (0.4350)	
training:	Epoch: [86][164/204]	Loss 0.3464 (0.4345)	
training:	Epoch: [86][165/204]	Loss 0.3823 (0.4341)	
training:	Epoch: [86][166/204]	Loss 0.4543 (0.4343)	
training:	Epoch: [86][167/204]	Loss 0.4777 (0.4345)	
training:	Epoch: [86][168/204]	Loss 0.4669 (0.4347)	
training:	Epoch: [86][169/204]	Loss 0.3730 (0.4344)	
training:	Epoch: [86][170/204]	Loss 0.5063 (0.4348)	
training:	Epoch: [86][171/204]	Loss 0.4742 (0.4350)	
training:	Epoch: [86][172/204]	Loss 0.2894 (0.4342)	
training:	Epoch: [86][173/204]	Loss 0.4999 (0.4345)	
training:	Epoch: [86][174/204]	Loss 0.5375 (0.4351)	
training:	Epoch: [86][175/204]	Loss 0.3824 (0.4348)	
training:	Epoch: [86][176/204]	Loss 0.4125 (0.4347)	
training:	Epoch: [86][177/204]	Loss 0.4372 (0.4347)	
training:	Epoch: [86][178/204]	Loss 0.4560 (0.4348)	
training:	Epoch: [86][179/204]	Loss 0.3882 (0.4346)	
training:	Epoch: [86][180/204]	Loss 0.5780 (0.4354)	
training:	Epoch: [86][181/204]	Loss 0.4754 (0.4356)	
training:	Epoch: [86][182/204]	Loss 0.3131 (0.4349)	
training:	Epoch: [86][183/204]	Loss 0.4461 (0.4350)	
training:	Epoch: [86][184/204]	Loss 0.8022 (0.4370)	
training:	Epoch: [86][185/204]	Loss 0.3556 (0.4365)	
training:	Epoch: [86][186/204]	Loss 0.3322 (0.4360)	
training:	Epoch: [86][187/204]	Loss 0.5771 (0.4367)	
training:	Epoch: [86][188/204]	Loss 0.3715 (0.4364)	
training:	Epoch: [86][189/204]	Loss 0.5682 (0.4371)	
training:	Epoch: [86][190/204]	Loss 0.5048 (0.4374)	
training:	Epoch: [86][191/204]	Loss 0.3969 (0.4372)	
training:	Epoch: [86][192/204]	Loss 0.3854 (0.4370)	
training:	Epoch: [86][193/204]	Loss 0.5393 (0.4375)	
training:	Epoch: [86][194/204]	Loss 0.3399 (0.4370)	
training:	Epoch: [86][195/204]	Loss 0.4818 (0.4372)	
training:	Epoch: [86][196/204]	Loss 0.4382 (0.4372)	
training:	Epoch: [86][197/204]	Loss 0.3789 (0.4369)	
training:	Epoch: [86][198/204]	Loss 0.6735 (0.4381)	
training:	Epoch: [86][199/204]	Loss 0.2501 (0.4372)	
training:	Epoch: [86][200/204]	Loss 0.3846 (0.4369)	
training:	Epoch: [86][201/204]	Loss 0.2933 (0.4362)	
training:	Epoch: [86][202/204]	Loss 0.3999 (0.4360)	
training:	Epoch: [86][203/204]	Loss 0.5033 (0.4363)	
training:	Epoch: [86][204/204]	Loss 0.2351 (0.4354)	
Training:	 Loss: 0.4347

Training:	 ACC: 0.8161 0.8172 0.8436 0.7886
Validation:	 ACC: 0.7941 0.7956 0.8280 0.7601
Validation:	 Best_BACC: 0.7941 0.7956 0.8280 0.7601
Validation:	 Loss: 0.4523
Pretraining:	Epoch 87/120
----------
training:	Epoch: [87][1/204]	Loss 0.5564 (0.5564)	
training:	Epoch: [87][2/204]	Loss 0.4605 (0.5085)	
training:	Epoch: [87][3/204]	Loss 0.5620 (0.5263)	
training:	Epoch: [87][4/204]	Loss 0.4164 (0.4988)	
training:	Epoch: [87][5/204]	Loss 0.3587 (0.4708)	
training:	Epoch: [87][6/204]	Loss 0.2682 (0.4370)	
training:	Epoch: [87][7/204]	Loss 0.4634 (0.4408)	
training:	Epoch: [87][8/204]	Loss 0.4108 (0.4371)	
training:	Epoch: [87][9/204]	Loss 0.4761 (0.4414)	
training:	Epoch: [87][10/204]	Loss 0.5648 (0.4537)	
training:	Epoch: [87][11/204]	Loss 0.3926 (0.4482)	
training:	Epoch: [87][12/204]	Loss 0.5550 (0.4571)	
training:	Epoch: [87][13/204]	Loss 0.3288 (0.4472)	
training:	Epoch: [87][14/204]	Loss 0.4987 (0.4509)	
training:	Epoch: [87][15/204]	Loss 0.3875 (0.4467)	
training:	Epoch: [87][16/204]	Loss 0.5065 (0.4504)	
training:	Epoch: [87][17/204]	Loss 0.3980 (0.4473)	
training:	Epoch: [87][18/204]	Loss 0.5973 (0.4556)	
training:	Epoch: [87][19/204]	Loss 0.3422 (0.4497)	
training:	Epoch: [87][20/204]	Loss 0.3262 (0.4435)	
training:	Epoch: [87][21/204]	Loss 0.2192 (0.4328)	
training:	Epoch: [87][22/204]	Loss 0.4451 (0.4334)	
training:	Epoch: [87][23/204]	Loss 0.6315 (0.4420)	
training:	Epoch: [87][24/204]	Loss 0.4373 (0.4418)	
training:	Epoch: [87][25/204]	Loss 0.3894 (0.4397)	
training:	Epoch: [87][26/204]	Loss 0.4819 (0.4413)	
training:	Epoch: [87][27/204]	Loss 0.3376 (0.4375)	
training:	Epoch: [87][28/204]	Loss 0.4643 (0.4384)	
training:	Epoch: [87][29/204]	Loss 0.4839 (0.4400)	
training:	Epoch: [87][30/204]	Loss 0.4815 (0.4414)	
training:	Epoch: [87][31/204]	Loss 0.4243 (0.4408)	
training:	Epoch: [87][32/204]	Loss 0.5025 (0.4428)	
training:	Epoch: [87][33/204]	Loss 0.4133 (0.4419)	
training:	Epoch: [87][34/204]	Loss 0.3428 (0.4390)	
training:	Epoch: [87][35/204]	Loss 0.4746 (0.4400)	
training:	Epoch: [87][36/204]	Loss 0.4360 (0.4399)	
training:	Epoch: [87][37/204]	Loss 0.4424 (0.4399)	
training:	Epoch: [87][38/204]	Loss 0.6059 (0.4443)	
training:	Epoch: [87][39/204]	Loss 0.4364 (0.4441)	
training:	Epoch: [87][40/204]	Loss 0.4417 (0.4440)	
training:	Epoch: [87][41/204]	Loss 0.3699 (0.4422)	
training:	Epoch: [87][42/204]	Loss 0.4249 (0.4418)	
training:	Epoch: [87][43/204]	Loss 0.3407 (0.4395)	
training:	Epoch: [87][44/204]	Loss 0.5925 (0.4429)	
training:	Epoch: [87][45/204]	Loss 0.3703 (0.4413)	
training:	Epoch: [87][46/204]	Loss 0.5008 (0.4426)	
training:	Epoch: [87][47/204]	Loss 0.5958 (0.4459)	
training:	Epoch: [87][48/204]	Loss 0.4883 (0.4468)	
training:	Epoch: [87][49/204]	Loss 0.3620 (0.4450)	
training:	Epoch: [87][50/204]	Loss 0.4440 (0.4450)	
training:	Epoch: [87][51/204]	Loss 0.4517 (0.4451)	
training:	Epoch: [87][52/204]	Loss 0.5592 (0.4473)	
training:	Epoch: [87][53/204]	Loss 0.3445 (0.4454)	
training:	Epoch: [87][54/204]	Loss 0.3911 (0.4444)	
training:	Epoch: [87][55/204]	Loss 0.5654 (0.4466)	
training:	Epoch: [87][56/204]	Loss 0.3778 (0.4454)	
training:	Epoch: [87][57/204]	Loss 0.3842 (0.4443)	
training:	Epoch: [87][58/204]	Loss 0.4543 (0.4445)	
training:	Epoch: [87][59/204]	Loss 0.4662 (0.4448)	
training:	Epoch: [87][60/204]	Loss 0.3824 (0.4438)	
training:	Epoch: [87][61/204]	Loss 0.2513 (0.4406)	
training:	Epoch: [87][62/204]	Loss 0.4798 (0.4413)	
training:	Epoch: [87][63/204]	Loss 0.4746 (0.4418)	
training:	Epoch: [87][64/204]	Loss 0.3752 (0.4408)	
training:	Epoch: [87][65/204]	Loss 0.4444 (0.4408)	
training:	Epoch: [87][66/204]	Loss 0.4671 (0.4412)	
training:	Epoch: [87][67/204]	Loss 0.4801 (0.4418)	
training:	Epoch: [87][68/204]	Loss 0.3679 (0.4407)	
training:	Epoch: [87][69/204]	Loss 0.4403 (0.4407)	
training:	Epoch: [87][70/204]	Loss 0.5373 (0.4421)	
training:	Epoch: [87][71/204]	Loss 0.4722 (0.4425)	
training:	Epoch: [87][72/204]	Loss 0.5481 (0.4440)	
training:	Epoch: [87][73/204]	Loss 0.4642 (0.4442)	
training:	Epoch: [87][74/204]	Loss 0.3806 (0.4434)	
training:	Epoch: [87][75/204]	Loss 0.5197 (0.4444)	
training:	Epoch: [87][76/204]	Loss 0.4559 (0.4446)	
training:	Epoch: [87][77/204]	Loss 0.5608 (0.4461)	
training:	Epoch: [87][78/204]	Loss 0.3068 (0.4443)	
training:	Epoch: [87][79/204]	Loss 0.4770 (0.4447)	
training:	Epoch: [87][80/204]	Loss 0.4325 (0.4445)	
training:	Epoch: [87][81/204]	Loss 0.5532 (0.4459)	
training:	Epoch: [87][82/204]	Loss 0.3850 (0.4451)	
training:	Epoch: [87][83/204]	Loss 0.4848 (0.4456)	
training:	Epoch: [87][84/204]	Loss 0.5725 (0.4471)	
training:	Epoch: [87][85/204]	Loss 0.4848 (0.4476)	
training:	Epoch: [87][86/204]	Loss 0.3942 (0.4469)	
training:	Epoch: [87][87/204]	Loss 0.4183 (0.4466)	
training:	Epoch: [87][88/204]	Loss 0.4800 (0.4470)	
training:	Epoch: [87][89/204]	Loss 0.4655 (0.4472)	
training:	Epoch: [87][90/204]	Loss 0.4629 (0.4474)	
training:	Epoch: [87][91/204]	Loss 0.2899 (0.4457)	
training:	Epoch: [87][92/204]	Loss 0.3887 (0.4450)	
training:	Epoch: [87][93/204]	Loss 0.3996 (0.4445)	
training:	Epoch: [87][94/204]	Loss 0.4509 (0.4446)	
training:	Epoch: [87][95/204]	Loss 0.4898 (0.4451)	
training:	Epoch: [87][96/204]	Loss 0.5772 (0.4465)	
training:	Epoch: [87][97/204]	Loss 0.4515 (0.4465)	
training:	Epoch: [87][98/204]	Loss 0.5767 (0.4478)	
training:	Epoch: [87][99/204]	Loss 0.3604 (0.4470)	
training:	Epoch: [87][100/204]	Loss 0.3927 (0.4464)	
training:	Epoch: [87][101/204]	Loss 0.4096 (0.4461)	
training:	Epoch: [87][102/204]	Loss 0.5074 (0.4467)	
training:	Epoch: [87][103/204]	Loss 0.3110 (0.4453)	
training:	Epoch: [87][104/204]	Loss 0.5243 (0.4461)	
training:	Epoch: [87][105/204]	Loss 0.3518 (0.4452)	
training:	Epoch: [87][106/204]	Loss 0.5044 (0.4458)	
training:	Epoch: [87][107/204]	Loss 0.2484 (0.4439)	
training:	Epoch: [87][108/204]	Loss 0.3202 (0.4428)	
training:	Epoch: [87][109/204]	Loss 0.4189 (0.4425)	
training:	Epoch: [87][110/204]	Loss 0.4475 (0.4426)	
training:	Epoch: [87][111/204]	Loss 0.4291 (0.4425)	
training:	Epoch: [87][112/204]	Loss 0.3824 (0.4419)	
training:	Epoch: [87][113/204]	Loss 0.2796 (0.4405)	
training:	Epoch: [87][114/204]	Loss 0.4086 (0.4402)	
training:	Epoch: [87][115/204]	Loss 0.3281 (0.4392)	
training:	Epoch: [87][116/204]	Loss 0.5535 (0.4402)	
training:	Epoch: [87][117/204]	Loss 0.4097 (0.4400)	
training:	Epoch: [87][118/204]	Loss 0.4121 (0.4397)	
training:	Epoch: [87][119/204]	Loss 0.3302 (0.4388)	
training:	Epoch: [87][120/204]	Loss 0.4145 (0.4386)	
training:	Epoch: [87][121/204]	Loss 0.4481 (0.4387)	
training:	Epoch: [87][122/204]	Loss 0.2281 (0.4370)	
training:	Epoch: [87][123/204]	Loss 0.4192 (0.4368)	
training:	Epoch: [87][124/204]	Loss 0.4740 (0.4371)	
training:	Epoch: [87][125/204]	Loss 0.4078 (0.4369)	
training:	Epoch: [87][126/204]	Loss 0.4373 (0.4369)	
training:	Epoch: [87][127/204]	Loss 0.4341 (0.4369)	
training:	Epoch: [87][128/204]	Loss 0.3473 (0.4362)	
training:	Epoch: [87][129/204]	Loss 0.5957 (0.4374)	
training:	Epoch: [87][130/204]	Loss 0.4123 (0.4372)	
training:	Epoch: [87][131/204]	Loss 0.2785 (0.4360)	
training:	Epoch: [87][132/204]	Loss 0.3416 (0.4353)	
training:	Epoch: [87][133/204]	Loss 0.2723 (0.4341)	
training:	Epoch: [87][134/204]	Loss 0.4979 (0.4345)	
training:	Epoch: [87][135/204]	Loss 0.4099 (0.4344)	
training:	Epoch: [87][136/204]	Loss 0.6702 (0.4361)	
training:	Epoch: [87][137/204]	Loss 0.3913 (0.4358)	
training:	Epoch: [87][138/204]	Loss 0.6002 (0.4369)	
training:	Epoch: [87][139/204]	Loss 0.4464 (0.4370)	
training:	Epoch: [87][140/204]	Loss 0.5479 (0.4378)	
training:	Epoch: [87][141/204]	Loss 0.5191 (0.4384)	
training:	Epoch: [87][142/204]	Loss 0.3670 (0.4379)	
training:	Epoch: [87][143/204]	Loss 0.3851 (0.4375)	
training:	Epoch: [87][144/204]	Loss 0.4951 (0.4379)	
training:	Epoch: [87][145/204]	Loss 0.3723 (0.4375)	
training:	Epoch: [87][146/204]	Loss 0.4502 (0.4375)	
training:	Epoch: [87][147/204]	Loss 0.5279 (0.4382)	
training:	Epoch: [87][148/204]	Loss 0.4916 (0.4385)	
training:	Epoch: [87][149/204]	Loss 0.5328 (0.4392)	
training:	Epoch: [87][150/204]	Loss 0.5131 (0.4397)	
training:	Epoch: [87][151/204]	Loss 0.3169 (0.4388)	
training:	Epoch: [87][152/204]	Loss 0.4491 (0.4389)	
training:	Epoch: [87][153/204]	Loss 0.5748 (0.4398)	
training:	Epoch: [87][154/204]	Loss 0.3414 (0.4392)	
training:	Epoch: [87][155/204]	Loss 0.5148 (0.4396)	
training:	Epoch: [87][156/204]	Loss 0.2655 (0.4385)	
training:	Epoch: [87][157/204]	Loss 0.4032 (0.4383)	
training:	Epoch: [87][158/204]	Loss 0.4274 (0.4382)	
training:	Epoch: [87][159/204]	Loss 0.4388 (0.4382)	
training:	Epoch: [87][160/204]	Loss 0.3591 (0.4377)	
training:	Epoch: [87][161/204]	Loss 0.3269 (0.4371)	
training:	Epoch: [87][162/204]	Loss 0.3810 (0.4367)	
training:	Epoch: [87][163/204]	Loss 0.4124 (0.4366)	
training:	Epoch: [87][164/204]	Loss 0.3948 (0.4363)	
training:	Epoch: [87][165/204]	Loss 0.3638 (0.4359)	
training:	Epoch: [87][166/204]	Loss 0.3971 (0.4356)	
training:	Epoch: [87][167/204]	Loss 0.4343 (0.4356)	
training:	Epoch: [87][168/204]	Loss 0.3155 (0.4349)	
training:	Epoch: [87][169/204]	Loss 0.4688 (0.4351)	
training:	Epoch: [87][170/204]	Loss 0.3996 (0.4349)	
training:	Epoch: [87][171/204]	Loss 0.4785 (0.4352)	
training:	Epoch: [87][172/204]	Loss 0.4745 (0.4354)	
training:	Epoch: [87][173/204]	Loss 0.4010 (0.4352)	
training:	Epoch: [87][174/204]	Loss 0.4289 (0.4351)	
training:	Epoch: [87][175/204]	Loss 0.2948 (0.4343)	
training:	Epoch: [87][176/204]	Loss 0.4870 (0.4346)	
training:	Epoch: [87][177/204]	Loss 0.6166 (0.4357)	
training:	Epoch: [87][178/204]	Loss 0.4690 (0.4359)	
training:	Epoch: [87][179/204]	Loss 0.4952 (0.4362)	
training:	Epoch: [87][180/204]	Loss 0.4434 (0.4362)	
training:	Epoch: [87][181/204]	Loss 0.3257 (0.4356)	
training:	Epoch: [87][182/204]	Loss 0.3966 (0.4354)	
training:	Epoch: [87][183/204]	Loss 0.4609 (0.4355)	
training:	Epoch: [87][184/204]	Loss 0.5496 (0.4362)	
training:	Epoch: [87][185/204]	Loss 0.3883 (0.4359)	
training:	Epoch: [87][186/204]	Loss 0.5073 (0.4363)	
training:	Epoch: [87][187/204]	Loss 0.6122 (0.4372)	
training:	Epoch: [87][188/204]	Loss 0.5786 (0.4380)	
training:	Epoch: [87][189/204]	Loss 0.3051 (0.4373)	
training:	Epoch: [87][190/204]	Loss 0.4382 (0.4373)	
training:	Epoch: [87][191/204]	Loss 0.4323 (0.4373)	
training:	Epoch: [87][192/204]	Loss 0.4225 (0.4372)	
training:	Epoch: [87][193/204]	Loss 0.3128 (0.4365)	
training:	Epoch: [87][194/204]	Loss 0.4030 (0.4364)	
training:	Epoch: [87][195/204]	Loss 0.2994 (0.4357)	
training:	Epoch: [87][196/204]	Loss 0.3602 (0.4353)	
training:	Epoch: [87][197/204]	Loss 0.5213 (0.4357)	
training:	Epoch: [87][198/204]	Loss 0.4512 (0.4358)	
training:	Epoch: [87][199/204]	Loss 0.4916 (0.4361)	
training:	Epoch: [87][200/204]	Loss 0.6160 (0.4370)	
training:	Epoch: [87][201/204]	Loss 0.4854 (0.4372)	
training:	Epoch: [87][202/204]	Loss 0.3580 (0.4368)	
training:	Epoch: [87][203/204]	Loss 0.4011 (0.4366)	
training:	Epoch: [87][204/204]	Loss 0.4626 (0.4368)	
Training:	 Loss: 0.4361

Training:	 ACC: 0.8176 0.8183 0.8354 0.7997
Validation:	 ACC: 0.7943 0.7951 0.8117 0.7769
Validation:	 Best_BACC: 0.7943 0.7951 0.8117 0.7769
Validation:	 Loss: 0.4508
Pretraining:	Epoch 88/120
----------
training:	Epoch: [88][1/204]	Loss 0.4899 (0.4899)	
training:	Epoch: [88][2/204]	Loss 0.5127 (0.5013)	
training:	Epoch: [88][3/204]	Loss 0.2256 (0.4094)	
training:	Epoch: [88][4/204]	Loss 0.4397 (0.4170)	
training:	Epoch: [88][5/204]	Loss 0.4817 (0.4299)	
training:	Epoch: [88][6/204]	Loss 0.4470 (0.4328)	
training:	Epoch: [88][7/204]	Loss 0.5151 (0.4445)	
training:	Epoch: [88][8/204]	Loss 0.3405 (0.4315)	
training:	Epoch: [88][9/204]	Loss 0.5384 (0.4434)	
training:	Epoch: [88][10/204]	Loss 0.3531 (0.4344)	
training:	Epoch: [88][11/204]	Loss 0.5467 (0.4446)	
training:	Epoch: [88][12/204]	Loss 0.3703 (0.4384)	
training:	Epoch: [88][13/204]	Loss 0.3486 (0.4315)	
training:	Epoch: [88][14/204]	Loss 0.4134 (0.4302)	
training:	Epoch: [88][15/204]	Loss 0.5536 (0.4384)	
training:	Epoch: [88][16/204]	Loss 0.3594 (0.4335)	
training:	Epoch: [88][17/204]	Loss 0.4891 (0.4368)	
training:	Epoch: [88][18/204]	Loss 0.2966 (0.4290)	
training:	Epoch: [88][19/204]	Loss 0.5371 (0.4347)	
training:	Epoch: [88][20/204]	Loss 0.5318 (0.4395)	
training:	Epoch: [88][21/204]	Loss 0.5433 (0.4445)	
training:	Epoch: [88][22/204]	Loss 0.5803 (0.4506)	
training:	Epoch: [88][23/204]	Loss 0.4630 (0.4512)	
training:	Epoch: [88][24/204]	Loss 0.4818 (0.4524)	
training:	Epoch: [88][25/204]	Loss 0.4904 (0.4540)	
training:	Epoch: [88][26/204]	Loss 0.4355 (0.4533)	
training:	Epoch: [88][27/204]	Loss 0.3467 (0.4493)	
training:	Epoch: [88][28/204]	Loss 0.3506 (0.4458)	
training:	Epoch: [88][29/204]	Loss 0.2395 (0.4387)	
training:	Epoch: [88][30/204]	Loss 0.4594 (0.4394)	
training:	Epoch: [88][31/204]	Loss 0.4561 (0.4399)	
training:	Epoch: [88][32/204]	Loss 0.3056 (0.4357)	
training:	Epoch: [88][33/204]	Loss 0.4745 (0.4369)	
training:	Epoch: [88][34/204]	Loss 0.2636 (0.4318)	
training:	Epoch: [88][35/204]	Loss 0.4223 (0.4315)	
training:	Epoch: [88][36/204]	Loss 0.4008 (0.4307)	
training:	Epoch: [88][37/204]	Loss 0.4979 (0.4325)	
training:	Epoch: [88][38/204]	Loss 0.2301 (0.4272)	
training:	Epoch: [88][39/204]	Loss 0.4942 (0.4289)	
training:	Epoch: [88][40/204]	Loss 0.5695 (0.4324)	
training:	Epoch: [88][41/204]	Loss 0.4138 (0.4319)	
training:	Epoch: [88][42/204]	Loss 0.4052 (0.4313)	
training:	Epoch: [88][43/204]	Loss 0.3399 (0.4292)	
training:	Epoch: [88][44/204]	Loss 0.6404 (0.4340)	
training:	Epoch: [88][45/204]	Loss 0.4509 (0.4344)	
training:	Epoch: [88][46/204]	Loss 0.3393 (0.4323)	
training:	Epoch: [88][47/204]	Loss 0.4117 (0.4319)	
training:	Epoch: [88][48/204]	Loss 0.4002 (0.4312)	
training:	Epoch: [88][49/204]	Loss 0.3743 (0.4300)	
training:	Epoch: [88][50/204]	Loss 0.5085 (0.4316)	
training:	Epoch: [88][51/204]	Loss 0.2498 (0.4280)	
training:	Epoch: [88][52/204]	Loss 0.5621 (0.4306)	
training:	Epoch: [88][53/204]	Loss 0.5665 (0.4332)	
training:	Epoch: [88][54/204]	Loss 0.4495 (0.4335)	
training:	Epoch: [88][55/204]	Loss 0.5162 (0.4350)	
training:	Epoch: [88][56/204]	Loss 0.5131 (0.4364)	
training:	Epoch: [88][57/204]	Loss 0.3553 (0.4350)	
training:	Epoch: [88][58/204]	Loss 0.5884 (0.4376)	
training:	Epoch: [88][59/204]	Loss 0.3495 (0.4361)	
training:	Epoch: [88][60/204]	Loss 0.4213 (0.4359)	
training:	Epoch: [88][61/204]	Loss 0.6164 (0.4388)	
training:	Epoch: [88][62/204]	Loss 0.4380 (0.4388)	
training:	Epoch: [88][63/204]	Loss 0.3588 (0.4375)	
training:	Epoch: [88][64/204]	Loss 0.4409 (0.4376)	
training:	Epoch: [88][65/204]	Loss 0.3039 (0.4355)	
training:	Epoch: [88][66/204]	Loss 0.4217 (0.4353)	
training:	Epoch: [88][67/204]	Loss 0.4789 (0.4360)	
training:	Epoch: [88][68/204]	Loss 0.3458 (0.4346)	
training:	Epoch: [88][69/204]	Loss 0.4603 (0.4350)	
training:	Epoch: [88][70/204]	Loss 0.4066 (0.4346)	
training:	Epoch: [88][71/204]	Loss 0.4491 (0.4348)	
training:	Epoch: [88][72/204]	Loss 0.4665 (0.4353)	
training:	Epoch: [88][73/204]	Loss 0.5371 (0.4367)	
training:	Epoch: [88][74/204]	Loss 0.4140 (0.4363)	
training:	Epoch: [88][75/204]	Loss 0.4906 (0.4371)	
training:	Epoch: [88][76/204]	Loss 0.4054 (0.4367)	
training:	Epoch: [88][77/204]	Loss 0.5717 (0.4384)	
training:	Epoch: [88][78/204]	Loss 0.4287 (0.4383)	
training:	Epoch: [88][79/204]	Loss 0.4997 (0.4391)	
training:	Epoch: [88][80/204]	Loss 0.3985 (0.4386)	
training:	Epoch: [88][81/204]	Loss 0.3417 (0.4374)	
training:	Epoch: [88][82/204]	Loss 0.5629 (0.4389)	
training:	Epoch: [88][83/204]	Loss 0.5015 (0.4396)	
training:	Epoch: [88][84/204]	Loss 0.4765 (0.4401)	
training:	Epoch: [88][85/204]	Loss 0.4074 (0.4397)	
training:	Epoch: [88][86/204]	Loss 0.3653 (0.4388)	
training:	Epoch: [88][87/204]	Loss 0.3214 (0.4375)	
training:	Epoch: [88][88/204]	Loss 0.4116 (0.4372)	
training:	Epoch: [88][89/204]	Loss 0.4318 (0.4371)	
training:	Epoch: [88][90/204]	Loss 0.4307 (0.4371)	
training:	Epoch: [88][91/204]	Loss 0.4511 (0.4372)	
training:	Epoch: [88][92/204]	Loss 0.4076 (0.4369)	
training:	Epoch: [88][93/204]	Loss 0.5046 (0.4376)	
training:	Epoch: [88][94/204]	Loss 0.3889 (0.4371)	
training:	Epoch: [88][95/204]	Loss 0.2561 (0.4352)	
training:	Epoch: [88][96/204]	Loss 0.2986 (0.4338)	
training:	Epoch: [88][97/204]	Loss 0.5841 (0.4353)	
training:	Epoch: [88][98/204]	Loss 0.5956 (0.4370)	
training:	Epoch: [88][99/204]	Loss 0.3552 (0.4361)	
training:	Epoch: [88][100/204]	Loss 0.3188 (0.4350)	
training:	Epoch: [88][101/204]	Loss 0.3270 (0.4339)	
training:	Epoch: [88][102/204]	Loss 0.4584 (0.4341)	
training:	Epoch: [88][103/204]	Loss 0.3567 (0.4334)	
training:	Epoch: [88][104/204]	Loss 0.2445 (0.4316)	
training:	Epoch: [88][105/204]	Loss 0.6312 (0.4335)	
training:	Epoch: [88][106/204]	Loss 0.3396 (0.4326)	
training:	Epoch: [88][107/204]	Loss 0.5351 (0.4335)	
training:	Epoch: [88][108/204]	Loss 0.4989 (0.4341)	
training:	Epoch: [88][109/204]	Loss 0.4856 (0.4346)	
training:	Epoch: [88][110/204]	Loss 0.5391 (0.4356)	
training:	Epoch: [88][111/204]	Loss 0.3063 (0.4344)	
training:	Epoch: [88][112/204]	Loss 0.4643 (0.4347)	
training:	Epoch: [88][113/204]	Loss 0.5869 (0.4360)	
training:	Epoch: [88][114/204]	Loss 0.5078 (0.4366)	
training:	Epoch: [88][115/204]	Loss 0.4997 (0.4372)	
training:	Epoch: [88][116/204]	Loss 0.5099 (0.4378)	
training:	Epoch: [88][117/204]	Loss 0.6386 (0.4395)	
training:	Epoch: [88][118/204]	Loss 0.3700 (0.4389)	
training:	Epoch: [88][119/204]	Loss 0.5188 (0.4396)	
training:	Epoch: [88][120/204]	Loss 0.3838 (0.4391)	
training:	Epoch: [88][121/204]	Loss 0.4482 (0.4392)	
training:	Epoch: [88][122/204]	Loss 0.3659 (0.4386)	
training:	Epoch: [88][123/204]	Loss 0.5464 (0.4395)	
training:	Epoch: [88][124/204]	Loss 0.4976 (0.4400)	
training:	Epoch: [88][125/204]	Loss 0.3465 (0.4392)	
training:	Epoch: [88][126/204]	Loss 0.3096 (0.4382)	
training:	Epoch: [88][127/204]	Loss 0.4577 (0.4383)	
training:	Epoch: [88][128/204]	Loss 0.3696 (0.4378)	
training:	Epoch: [88][129/204]	Loss 0.4338 (0.4378)	
training:	Epoch: [88][130/204]	Loss 0.4670 (0.4380)	
training:	Epoch: [88][131/204]	Loss 0.3818 (0.4376)	
training:	Epoch: [88][132/204]	Loss 0.3404 (0.4368)	
training:	Epoch: [88][133/204]	Loss 0.4330 (0.4368)	
training:	Epoch: [88][134/204]	Loss 0.3352 (0.4360)	
training:	Epoch: [88][135/204]	Loss 0.3473 (0.4354)	
training:	Epoch: [88][136/204]	Loss 0.4204 (0.4353)	
training:	Epoch: [88][137/204]	Loss 0.2701 (0.4341)	
training:	Epoch: [88][138/204]	Loss 0.3988 (0.4338)	
training:	Epoch: [88][139/204]	Loss 0.4019 (0.4336)	
training:	Epoch: [88][140/204]	Loss 0.4226 (0.4335)	
training:	Epoch: [88][141/204]	Loss 0.4908 (0.4339)	
training:	Epoch: [88][142/204]	Loss 0.5495 (0.4347)	
training:	Epoch: [88][143/204]	Loss 0.4427 (0.4348)	
training:	Epoch: [88][144/204]	Loss 0.4564 (0.4349)	
training:	Epoch: [88][145/204]	Loss 0.2813 (0.4339)	
training:	Epoch: [88][146/204]	Loss 0.4642 (0.4341)	
training:	Epoch: [88][147/204]	Loss 0.5233 (0.4347)	
training:	Epoch: [88][148/204]	Loss 0.5198 (0.4353)	
training:	Epoch: [88][149/204]	Loss 0.4341 (0.4353)	
training:	Epoch: [88][150/204]	Loss 0.3973 (0.4350)	
training:	Epoch: [88][151/204]	Loss 0.5130 (0.4355)	
training:	Epoch: [88][152/204]	Loss 0.3371 (0.4349)	
training:	Epoch: [88][153/204]	Loss 0.2895 (0.4339)	
training:	Epoch: [88][154/204]	Loss 0.3759 (0.4335)	
training:	Epoch: [88][155/204]	Loss 0.3673 (0.4331)	
training:	Epoch: [88][156/204]	Loss 0.3636 (0.4327)	
training:	Epoch: [88][157/204]	Loss 0.3913 (0.4324)	
training:	Epoch: [88][158/204]	Loss 0.3261 (0.4317)	
training:	Epoch: [88][159/204]	Loss 0.4253 (0.4317)	
training:	Epoch: [88][160/204]	Loss 0.5108 (0.4322)	
training:	Epoch: [88][161/204]	Loss 0.4009 (0.4320)	
training:	Epoch: [88][162/204]	Loss 0.5106 (0.4325)	
training:	Epoch: [88][163/204]	Loss 0.4722 (0.4327)	
training:	Epoch: [88][164/204]	Loss 0.4782 (0.4330)	
training:	Epoch: [88][165/204]	Loss 0.5737 (0.4339)	
training:	Epoch: [88][166/204]	Loss 0.3444 (0.4333)	
training:	Epoch: [88][167/204]	Loss 0.4048 (0.4331)	
training:	Epoch: [88][168/204]	Loss 0.3396 (0.4326)	
training:	Epoch: [88][169/204]	Loss 0.5151 (0.4331)	
training:	Epoch: [88][170/204]	Loss 0.3330 (0.4325)	
training:	Epoch: [88][171/204]	Loss 0.4561 (0.4326)	
training:	Epoch: [88][172/204]	Loss 0.2953 (0.4318)	
training:	Epoch: [88][173/204]	Loss 0.3049 (0.4311)	
training:	Epoch: [88][174/204]	Loss 0.4146 (0.4310)	
training:	Epoch: [88][175/204]	Loss 0.6076 (0.4320)	
training:	Epoch: [88][176/204]	Loss 0.3948 (0.4318)	
training:	Epoch: [88][177/204]	Loss 0.3113 (0.4311)	
training:	Epoch: [88][178/204]	Loss 0.3959 (0.4309)	
training:	Epoch: [88][179/204]	Loss 0.2359 (0.4298)	
training:	Epoch: [88][180/204]	Loss 0.3612 (0.4294)	
training:	Epoch: [88][181/204]	Loss 0.2188 (0.4283)	
training:	Epoch: [88][182/204]	Loss 0.3472 (0.4278)	
training:	Epoch: [88][183/204]	Loss 0.4792 (0.4281)	
training:	Epoch: [88][184/204]	Loss 0.4912 (0.4285)	
training:	Epoch: [88][185/204]	Loss 0.3720 (0.4282)	
training:	Epoch: [88][186/204]	Loss 0.4747 (0.4284)	
training:	Epoch: [88][187/204]	Loss 0.4723 (0.4286)	
training:	Epoch: [88][188/204]	Loss 0.4581 (0.4288)	
training:	Epoch: [88][189/204]	Loss 0.4901 (0.4291)	
training:	Epoch: [88][190/204]	Loss 0.5385 (0.4297)	
training:	Epoch: [88][191/204]	Loss 0.4141 (0.4296)	
training:	Epoch: [88][192/204]	Loss 0.3997 (0.4295)	
training:	Epoch: [88][193/204]	Loss 0.3794 (0.4292)	
training:	Epoch: [88][194/204]	Loss 0.5372 (0.4298)	
training:	Epoch: [88][195/204]	Loss 0.3956 (0.4296)	
training:	Epoch: [88][196/204]	Loss 0.4480 (0.4297)	
training:	Epoch: [88][197/204]	Loss 0.3383 (0.4292)	
training:	Epoch: [88][198/204]	Loss 0.4478 (0.4293)	
training:	Epoch: [88][199/204]	Loss 0.2841 (0.4286)	
training:	Epoch: [88][200/204]	Loss 0.4145 (0.4285)	
training:	Epoch: [88][201/204]	Loss 0.4128 (0.4284)	
training:	Epoch: [88][202/204]	Loss 0.5262 (0.4289)	
training:	Epoch: [88][203/204]	Loss 0.4936 (0.4292)	
training:	Epoch: [88][204/204]	Loss 0.5860 (0.4300)	
Training:	 Loss: 0.4293

Training:	 ACC: 0.8208 0.8214 0.8333 0.8084
Validation:	 ACC: 0.7934 0.7940 0.8076 0.7791
Validation:	 Best_BACC: 0.7943 0.7951 0.8117 0.7769
Validation:	 Loss: 0.4500
Pretraining:	Epoch 89/120
----------
training:	Epoch: [89][1/204]	Loss 0.4347 (0.4347)	
training:	Epoch: [89][2/204]	Loss 0.4215 (0.4281)	
training:	Epoch: [89][3/204]	Loss 0.3760 (0.4107)	
training:	Epoch: [89][4/204]	Loss 0.4120 (0.4110)	
training:	Epoch: [89][5/204]	Loss 0.4154 (0.4119)	
training:	Epoch: [89][6/204]	Loss 0.4896 (0.4249)	
training:	Epoch: [89][7/204]	Loss 0.2943 (0.4062)	
training:	Epoch: [89][8/204]	Loss 0.4228 (0.4083)	
training:	Epoch: [89][9/204]	Loss 0.4244 (0.4101)	
training:	Epoch: [89][10/204]	Loss 0.3596 (0.4050)	
training:	Epoch: [89][11/204]	Loss 0.3328 (0.3985)	
training:	Epoch: [89][12/204]	Loss 0.4839 (0.4056)	
training:	Epoch: [89][13/204]	Loss 0.4381 (0.4081)	
training:	Epoch: [89][14/204]	Loss 0.3644 (0.4050)	
training:	Epoch: [89][15/204]	Loss 0.4136 (0.4055)	
training:	Epoch: [89][16/204]	Loss 0.4526 (0.4085)	
training:	Epoch: [89][17/204]	Loss 0.5081 (0.4143)	
training:	Epoch: [89][18/204]	Loss 0.4185 (0.4146)	
training:	Epoch: [89][19/204]	Loss 0.3516 (0.4113)	
training:	Epoch: [89][20/204]	Loss 0.4561 (0.4135)	
training:	Epoch: [89][21/204]	Loss 0.5216 (0.4186)	
training:	Epoch: [89][22/204]	Loss 0.4311 (0.4192)	
training:	Epoch: [89][23/204]	Loss 0.3776 (0.4174)	
training:	Epoch: [89][24/204]	Loss 0.4729 (0.4197)	
training:	Epoch: [89][25/204]	Loss 0.4194 (0.4197)	
training:	Epoch: [89][26/204]	Loss 0.3741 (0.4180)	
training:	Epoch: [89][27/204]	Loss 0.3673 (0.4161)	
training:	Epoch: [89][28/204]	Loss 0.4191 (0.4162)	
training:	Epoch: [89][29/204]	Loss 0.2690 (0.4111)	
training:	Epoch: [89][30/204]	Loss 0.4451 (0.4122)	
training:	Epoch: [89][31/204]	Loss 0.4616 (0.4138)	
training:	Epoch: [89][32/204]	Loss 0.6035 (0.4198)	
training:	Epoch: [89][33/204]	Loss 0.3392 (0.4173)	
training:	Epoch: [89][34/204]	Loss 0.4099 (0.4171)	
training:	Epoch: [89][35/204]	Loss 0.6158 (0.4228)	
training:	Epoch: [89][36/204]	Loss 0.2951 (0.4192)	
training:	Epoch: [89][37/204]	Loss 0.3926 (0.4185)	
training:	Epoch: [89][38/204]	Loss 0.3840 (0.4176)	
training:	Epoch: [89][39/204]	Loss 0.3947 (0.4170)	
training:	Epoch: [89][40/204]	Loss 0.5507 (0.4204)	
training:	Epoch: [89][41/204]	Loss 0.4402 (0.4208)	
training:	Epoch: [89][42/204]	Loss 0.5025 (0.4228)	
training:	Epoch: [89][43/204]	Loss 0.5929 (0.4267)	
training:	Epoch: [89][44/204]	Loss 0.6257 (0.4313)	
training:	Epoch: [89][45/204]	Loss 0.4466 (0.4316)	
training:	Epoch: [89][46/204]	Loss 0.3257 (0.4293)	
training:	Epoch: [89][47/204]	Loss 0.4860 (0.4305)	
training:	Epoch: [89][48/204]	Loss 0.3860 (0.4296)	
training:	Epoch: [89][49/204]	Loss 0.4407 (0.4298)	
training:	Epoch: [89][50/204]	Loss 0.3964 (0.4291)	
training:	Epoch: [89][51/204]	Loss 0.5198 (0.4309)	
training:	Epoch: [89][52/204]	Loss 0.3369 (0.4291)	
training:	Epoch: [89][53/204]	Loss 0.5608 (0.4316)	
training:	Epoch: [89][54/204]	Loss 0.3214 (0.4296)	
training:	Epoch: [89][55/204]	Loss 0.3685 (0.4284)	
training:	Epoch: [89][56/204]	Loss 0.5065 (0.4298)	
training:	Epoch: [89][57/204]	Loss 0.4425 (0.4301)	
training:	Epoch: [89][58/204]	Loss 0.4818 (0.4310)	
training:	Epoch: [89][59/204]	Loss 0.4249 (0.4308)	
training:	Epoch: [89][60/204]	Loss 0.4912 (0.4319)	
training:	Epoch: [89][61/204]	Loss 0.4625 (0.4324)	
training:	Epoch: [89][62/204]	Loss 0.4739 (0.4330)	
training:	Epoch: [89][63/204]	Loss 0.5991 (0.4357)	
training:	Epoch: [89][64/204]	Loss 0.3744 (0.4347)	
training:	Epoch: [89][65/204]	Loss 0.3712 (0.4337)	
training:	Epoch: [89][66/204]	Loss 0.3340 (0.4322)	
training:	Epoch: [89][67/204]	Loss 0.3639 (0.4312)	
training:	Epoch: [89][68/204]	Loss 0.4649 (0.4317)	
training:	Epoch: [89][69/204]	Loss 0.4665 (0.4322)	
training:	Epoch: [89][70/204]	Loss 0.4123 (0.4319)	
training:	Epoch: [89][71/204]	Loss 0.5208 (0.4332)	
training:	Epoch: [89][72/204]	Loss 0.4463 (0.4333)	
training:	Epoch: [89][73/204]	Loss 0.3287 (0.4319)	
training:	Epoch: [89][74/204]	Loss 0.5174 (0.4331)	
training:	Epoch: [89][75/204]	Loss 0.4641 (0.4335)	
training:	Epoch: [89][76/204]	Loss 0.3508 (0.4324)	
training:	Epoch: [89][77/204]	Loss 0.3570 (0.4314)	
training:	Epoch: [89][78/204]	Loss 0.4901 (0.4322)	
training:	Epoch: [89][79/204]	Loss 0.4086 (0.4319)	
training:	Epoch: [89][80/204]	Loss 0.4785 (0.4325)	
training:	Epoch: [89][81/204]	Loss 0.4070 (0.4321)	
training:	Epoch: [89][82/204]	Loss 0.3417 (0.4310)	
training:	Epoch: [89][83/204]	Loss 0.4733 (0.4315)	
training:	Epoch: [89][84/204]	Loss 0.4132 (0.4313)	
training:	Epoch: [89][85/204]	Loss 0.4252 (0.4313)	
training:	Epoch: [89][86/204]	Loss 0.5626 (0.4328)	
training:	Epoch: [89][87/204]	Loss 0.6167 (0.4349)	
training:	Epoch: [89][88/204]	Loss 0.3483 (0.4339)	
training:	Epoch: [89][89/204]	Loss 0.5417 (0.4351)	
training:	Epoch: [89][90/204]	Loss 0.3074 (0.4337)	
training:	Epoch: [89][91/204]	Loss 0.3725 (0.4330)	
training:	Epoch: [89][92/204]	Loss 0.2724 (0.4313)	
training:	Epoch: [89][93/204]	Loss 0.4179 (0.4311)	
training:	Epoch: [89][94/204]	Loss 0.2713 (0.4294)	
training:	Epoch: [89][95/204]	Loss 0.3489 (0.4286)	
training:	Epoch: [89][96/204]	Loss 0.5102 (0.4294)	
training:	Epoch: [89][97/204]	Loss 0.3845 (0.4290)	
training:	Epoch: [89][98/204]	Loss 0.4346 (0.4290)	
training:	Epoch: [89][99/204]	Loss 0.3927 (0.4287)	
training:	Epoch: [89][100/204]	Loss 0.4940 (0.4293)	
training:	Epoch: [89][101/204]	Loss 0.3638 (0.4287)	
training:	Epoch: [89][102/204]	Loss 0.5876 (0.4302)	
training:	Epoch: [89][103/204]	Loss 0.4699 (0.4306)	
training:	Epoch: [89][104/204]	Loss 0.5161 (0.4314)	
training:	Epoch: [89][105/204]	Loss 0.4268 (0.4314)	
training:	Epoch: [89][106/204]	Loss 0.3746 (0.4309)	
training:	Epoch: [89][107/204]	Loss 0.3489 (0.4301)	
training:	Epoch: [89][108/204]	Loss 0.3626 (0.4295)	
training:	Epoch: [89][109/204]	Loss 0.4860 (0.4300)	
training:	Epoch: [89][110/204]	Loss 0.5237 (0.4308)	
training:	Epoch: [89][111/204]	Loss 0.3246 (0.4299)	
training:	Epoch: [89][112/204]	Loss 0.4044 (0.4297)	
training:	Epoch: [89][113/204]	Loss 0.4765 (0.4301)	
training:	Epoch: [89][114/204]	Loss 0.3718 (0.4296)	
training:	Epoch: [89][115/204]	Loss 0.3388 (0.4288)	
training:	Epoch: [89][116/204]	Loss 0.3055 (0.4277)	
training:	Epoch: [89][117/204]	Loss 0.3796 (0.4273)	
training:	Epoch: [89][118/204]	Loss 0.5051 (0.4280)	
training:	Epoch: [89][119/204]	Loss 0.5218 (0.4287)	
training:	Epoch: [89][120/204]	Loss 0.5290 (0.4296)	
training:	Epoch: [89][121/204]	Loss 0.3412 (0.4288)	
training:	Epoch: [89][122/204]	Loss 0.5149 (0.4296)	
training:	Epoch: [89][123/204]	Loss 0.2375 (0.4280)	
training:	Epoch: [89][124/204]	Loss 0.5737 (0.4292)	
training:	Epoch: [89][125/204]	Loss 0.3784 (0.4288)	
training:	Epoch: [89][126/204]	Loss 0.5133 (0.4294)	
training:	Epoch: [89][127/204]	Loss 0.3536 (0.4288)	
training:	Epoch: [89][128/204]	Loss 0.4296 (0.4288)	
training:	Epoch: [89][129/204]	Loss 0.5568 (0.4298)	
training:	Epoch: [89][130/204]	Loss 0.3230 (0.4290)	
training:	Epoch: [89][131/204]	Loss 0.3509 (0.4284)	
training:	Epoch: [89][132/204]	Loss 0.4518 (0.4286)	
training:	Epoch: [89][133/204]	Loss 0.2488 (0.4272)	
training:	Epoch: [89][134/204]	Loss 0.3821 (0.4269)	
training:	Epoch: [89][135/204]	Loss 0.4242 (0.4269)	
training:	Epoch: [89][136/204]	Loss 0.3957 (0.4267)	
training:	Epoch: [89][137/204]	Loss 0.4354 (0.4267)	
training:	Epoch: [89][138/204]	Loss 0.5265 (0.4274)	
training:	Epoch: [89][139/204]	Loss 0.3855 (0.4271)	
training:	Epoch: [89][140/204]	Loss 0.5724 (0.4282)	
training:	Epoch: [89][141/204]	Loss 0.4772 (0.4285)	
training:	Epoch: [89][142/204]	Loss 0.3955 (0.4283)	
training:	Epoch: [89][143/204]	Loss 0.2167 (0.4268)	
training:	Epoch: [89][144/204]	Loss 0.3603 (0.4264)	
training:	Epoch: [89][145/204]	Loss 0.4636 (0.4266)	
training:	Epoch: [89][146/204]	Loss 0.5534 (0.4275)	
training:	Epoch: [89][147/204]	Loss 0.4839 (0.4279)	
training:	Epoch: [89][148/204]	Loss 0.3511 (0.4273)	
training:	Epoch: [89][149/204]	Loss 0.5103 (0.4279)	
training:	Epoch: [89][150/204]	Loss 0.4789 (0.4282)	
training:	Epoch: [89][151/204]	Loss 0.4498 (0.4284)	
training:	Epoch: [89][152/204]	Loss 0.4639 (0.4286)	
training:	Epoch: [89][153/204]	Loss 0.5178 (0.4292)	
training:	Epoch: [89][154/204]	Loss 0.4633 (0.4294)	
training:	Epoch: [89][155/204]	Loss 0.5440 (0.4302)	
training:	Epoch: [89][156/204]	Loss 0.3994 (0.4300)	
training:	Epoch: [89][157/204]	Loss 0.6227 (0.4312)	
training:	Epoch: [89][158/204]	Loss 0.4056 (0.4310)	
training:	Epoch: [89][159/204]	Loss 0.3003 (0.4302)	
training:	Epoch: [89][160/204]	Loss 0.4853 (0.4305)	
training:	Epoch: [89][161/204]	Loss 0.5962 (0.4316)	
training:	Epoch: [89][162/204]	Loss 0.4670 (0.4318)	
training:	Epoch: [89][163/204]	Loss 0.4498 (0.4319)	
training:	Epoch: [89][164/204]	Loss 0.3479 (0.4314)	
training:	Epoch: [89][165/204]	Loss 0.3740 (0.4310)	
training:	Epoch: [89][166/204]	Loss 0.4000 (0.4309)	
training:	Epoch: [89][167/204]	Loss 0.3546 (0.4304)	
training:	Epoch: [89][168/204]	Loss 0.4918 (0.4308)	
training:	Epoch: [89][169/204]	Loss 0.5114 (0.4312)	
training:	Epoch: [89][170/204]	Loss 0.4542 (0.4314)	
training:	Epoch: [89][171/204]	Loss 0.4757 (0.4316)	
training:	Epoch: [89][172/204]	Loss 0.3585 (0.4312)	
training:	Epoch: [89][173/204]	Loss 0.3348 (0.4307)	
training:	Epoch: [89][174/204]	Loss 0.2940 (0.4299)	
training:	Epoch: [89][175/204]	Loss 0.5458 (0.4305)	
training:	Epoch: [89][176/204]	Loss 0.3098 (0.4298)	
training:	Epoch: [89][177/204]	Loss 0.4324 (0.4299)	
training:	Epoch: [89][178/204]	Loss 0.3380 (0.4293)	
training:	Epoch: [89][179/204]	Loss 0.5695 (0.4301)	
training:	Epoch: [89][180/204]	Loss 0.4697 (0.4303)	
training:	Epoch: [89][181/204]	Loss 0.3055 (0.4297)	
training:	Epoch: [89][182/204]	Loss 0.4580 (0.4298)	
training:	Epoch: [89][183/204]	Loss 0.3946 (0.4296)	
training:	Epoch: [89][184/204]	Loss 0.3609 (0.4292)	
training:	Epoch: [89][185/204]	Loss 0.5424 (0.4299)	
training:	Epoch: [89][186/204]	Loss 0.4269 (0.4298)	
training:	Epoch: [89][187/204]	Loss 0.3439 (0.4294)	
training:	Epoch: [89][188/204]	Loss 0.5212 (0.4299)	
training:	Epoch: [89][189/204]	Loss 0.3873 (0.4296)	
training:	Epoch: [89][190/204]	Loss 0.4367 (0.4297)	
training:	Epoch: [89][191/204]	Loss 0.2913 (0.4290)	
training:	Epoch: [89][192/204]	Loss 0.4111 (0.4289)	
training:	Epoch: [89][193/204]	Loss 0.5926 (0.4297)	
training:	Epoch: [89][194/204]	Loss 0.3962 (0.4295)	
training:	Epoch: [89][195/204]	Loss 0.6231 (0.4305)	
training:	Epoch: [89][196/204]	Loss 0.4325 (0.4305)	
training:	Epoch: [89][197/204]	Loss 0.3554 (0.4302)	
training:	Epoch: [89][198/204]	Loss 0.5282 (0.4307)	
training:	Epoch: [89][199/204]	Loss 0.4148 (0.4306)	
training:	Epoch: [89][200/204]	Loss 0.5938 (0.4314)	
training:	Epoch: [89][201/204]	Loss 0.4381 (0.4314)	
training:	Epoch: [89][202/204]	Loss 0.4312 (0.4314)	
training:	Epoch: [89][203/204]	Loss 0.4208 (0.4314)	
training:	Epoch: [89][204/204]	Loss 0.4842 (0.4316)	
Training:	 Loss: 0.4310

Training:	 ACC: 0.8204 0.8210 0.8363 0.8045
Validation:	 ACC: 0.7942 0.7951 0.8137 0.7747
Validation:	 Best_BACC: 0.7943 0.7951 0.8117 0.7769
Validation:	 Loss: 0.4491
Pretraining:	Epoch 90/120
----------
training:	Epoch: [90][1/204]	Loss 0.4909 (0.4909)	
training:	Epoch: [90][2/204]	Loss 0.5729 (0.5319)	
training:	Epoch: [90][3/204]	Loss 0.3724 (0.4787)	
training:	Epoch: [90][4/204]	Loss 0.4112 (0.4618)	
training:	Epoch: [90][5/204]	Loss 0.3458 (0.4386)	
training:	Epoch: [90][6/204]	Loss 0.2950 (0.4147)	
training:	Epoch: [90][7/204]	Loss 0.3412 (0.4042)	
training:	Epoch: [90][8/204]	Loss 0.5875 (0.4271)	
training:	Epoch: [90][9/204]	Loss 0.4083 (0.4250)	
training:	Epoch: [90][10/204]	Loss 0.3713 (0.4196)	
training:	Epoch: [90][11/204]	Loss 0.3637 (0.4146)	
training:	Epoch: [90][12/204]	Loss 0.2854 (0.4038)	
training:	Epoch: [90][13/204]	Loss 0.3604 (0.4005)	
training:	Epoch: [90][14/204]	Loss 0.4092 (0.4011)	
training:	Epoch: [90][15/204]	Loss 0.5069 (0.4081)	
training:	Epoch: [90][16/204]	Loss 0.2491 (0.3982)	
training:	Epoch: [90][17/204]	Loss 0.5079 (0.4047)	
training:	Epoch: [90][18/204]	Loss 0.4108 (0.4050)	
training:	Epoch: [90][19/204]	Loss 0.5733 (0.4139)	
training:	Epoch: [90][20/204]	Loss 0.4160 (0.4140)	
training:	Epoch: [90][21/204]	Loss 0.3238 (0.4097)	
training:	Epoch: [90][22/204]	Loss 0.5058 (0.4140)	
training:	Epoch: [90][23/204]	Loss 0.2618 (0.4074)	
training:	Epoch: [90][24/204]	Loss 0.3101 (0.4034)	
training:	Epoch: [90][25/204]	Loss 0.5630 (0.4097)	
training:	Epoch: [90][26/204]	Loss 0.4327 (0.4106)	
training:	Epoch: [90][27/204]	Loss 0.5250 (0.4149)	
training:	Epoch: [90][28/204]	Loss 0.4749 (0.4170)	
training:	Epoch: [90][29/204]	Loss 0.3704 (0.4154)	
training:	Epoch: [90][30/204]	Loss 0.2980 (0.4115)	
training:	Epoch: [90][31/204]	Loss 0.4992 (0.4143)	
training:	Epoch: [90][32/204]	Loss 0.4402 (0.4151)	
training:	Epoch: [90][33/204]	Loss 0.5593 (0.4195)	
training:	Epoch: [90][34/204]	Loss 0.6815 (0.4272)	
training:	Epoch: [90][35/204]	Loss 0.4625 (0.4282)	
training:	Epoch: [90][36/204]	Loss 0.2733 (0.4239)	
training:	Epoch: [90][37/204]	Loss 0.2962 (0.4204)	
training:	Epoch: [90][38/204]	Loss 0.3961 (0.4198)	
training:	Epoch: [90][39/204]	Loss 0.6013 (0.4245)	
training:	Epoch: [90][40/204]	Loss 0.3154 (0.4217)	
training:	Epoch: [90][41/204]	Loss 0.4477 (0.4224)	
training:	Epoch: [90][42/204]	Loss 0.5710 (0.4259)	
training:	Epoch: [90][43/204]	Loss 0.3558 (0.4243)	
training:	Epoch: [90][44/204]	Loss 0.3482 (0.4225)	
training:	Epoch: [90][45/204]	Loss 0.5887 (0.4262)	
training:	Epoch: [90][46/204]	Loss 0.4303 (0.4263)	
training:	Epoch: [90][47/204]	Loss 0.4255 (0.4263)	
training:	Epoch: [90][48/204]	Loss 0.5043 (0.4279)	
training:	Epoch: [90][49/204]	Loss 0.4386 (0.4282)	
training:	Epoch: [90][50/204]	Loss 0.4809 (0.4292)	
training:	Epoch: [90][51/204]	Loss 0.4222 (0.4291)	
training:	Epoch: [90][52/204]	Loss 0.2394 (0.4254)	
training:	Epoch: [90][53/204]	Loss 0.4698 (0.4263)	
training:	Epoch: [90][54/204]	Loss 0.2905 (0.4237)	
training:	Epoch: [90][55/204]	Loss 0.2400 (0.4204)	
training:	Epoch: [90][56/204]	Loss 0.3532 (0.4192)	
training:	Epoch: [90][57/204]	Loss 0.4012 (0.4189)	
training:	Epoch: [90][58/204]	Loss 0.5898 (0.4218)	
training:	Epoch: [90][59/204]	Loss 0.4551 (0.4224)	
training:	Epoch: [90][60/204]	Loss 0.4974 (0.4236)	
training:	Epoch: [90][61/204]	Loss 0.6829 (0.4279)	
training:	Epoch: [90][62/204]	Loss 0.4226 (0.4278)	
training:	Epoch: [90][63/204]	Loss 0.2971 (0.4257)	
training:	Epoch: [90][64/204]	Loss 0.3972 (0.4253)	
training:	Epoch: [90][65/204]	Loss 0.3841 (0.4247)	
training:	Epoch: [90][66/204]	Loss 0.4246 (0.4247)	
training:	Epoch: [90][67/204]	Loss 0.4128 (0.4245)	
training:	Epoch: [90][68/204]	Loss 0.4701 (0.4252)	
training:	Epoch: [90][69/204]	Loss 0.4300 (0.4252)	
training:	Epoch: [90][70/204]	Loss 0.4182 (0.4251)	
training:	Epoch: [90][71/204]	Loss 0.4152 (0.4250)	
training:	Epoch: [90][72/204]	Loss 0.3072 (0.4233)	
training:	Epoch: [90][73/204]	Loss 0.3976 (0.4230)	
training:	Epoch: [90][74/204]	Loss 0.3650 (0.4222)	
training:	Epoch: [90][75/204]	Loss 0.6397 (0.4251)	
training:	Epoch: [90][76/204]	Loss 0.3835 (0.4246)	
training:	Epoch: [90][77/204]	Loss 0.3677 (0.4238)	
training:	Epoch: [90][78/204]	Loss 0.4279 (0.4239)	
training:	Epoch: [90][79/204]	Loss 0.4139 (0.4238)	
training:	Epoch: [90][80/204]	Loss 0.3019 (0.4222)	
training:	Epoch: [90][81/204]	Loss 0.4466 (0.4225)	
training:	Epoch: [90][82/204]	Loss 0.5995 (0.4247)	
training:	Epoch: [90][83/204]	Loss 0.3960 (0.4243)	
training:	Epoch: [90][84/204]	Loss 0.4978 (0.4252)	
training:	Epoch: [90][85/204]	Loss 0.3634 (0.4245)	
training:	Epoch: [90][86/204]	Loss 0.6348 (0.4269)	
training:	Epoch: [90][87/204]	Loss 0.4057 (0.4267)	
training:	Epoch: [90][88/204]	Loss 0.4206 (0.4266)	
training:	Epoch: [90][89/204]	Loss 0.5311 (0.4278)	
training:	Epoch: [90][90/204]	Loss 0.3418 (0.4268)	
training:	Epoch: [90][91/204]	Loss 0.4474 (0.4271)	
training:	Epoch: [90][92/204]	Loss 0.4686 (0.4275)	
training:	Epoch: [90][93/204]	Loss 0.3702 (0.4269)	
training:	Epoch: [90][94/204]	Loss 0.3855 (0.4265)	
training:	Epoch: [90][95/204]	Loss 0.5018 (0.4273)	
training:	Epoch: [90][96/204]	Loss 0.4174 (0.4271)	
training:	Epoch: [90][97/204]	Loss 0.4210 (0.4271)	
training:	Epoch: [90][98/204]	Loss 0.4169 (0.4270)	
training:	Epoch: [90][99/204]	Loss 0.4442 (0.4272)	
training:	Epoch: [90][100/204]	Loss 0.3832 (0.4267)	
training:	Epoch: [90][101/204]	Loss 0.3729 (0.4262)	
training:	Epoch: [90][102/204]	Loss 0.3141 (0.4251)	
training:	Epoch: [90][103/204]	Loss 0.5610 (0.4264)	
training:	Epoch: [90][104/204]	Loss 0.6045 (0.4281)	
training:	Epoch: [90][105/204]	Loss 0.4604 (0.4284)	
training:	Epoch: [90][106/204]	Loss 0.4298 (0.4284)	
training:	Epoch: [90][107/204]	Loss 0.2809 (0.4271)	
training:	Epoch: [90][108/204]	Loss 0.4327 (0.4271)	
training:	Epoch: [90][109/204]	Loss 0.4381 (0.4272)	
training:	Epoch: [90][110/204]	Loss 0.4974 (0.4278)	
training:	Epoch: [90][111/204]	Loss 0.4757 (0.4283)	
training:	Epoch: [90][112/204]	Loss 0.3388 (0.4275)	
training:	Epoch: [90][113/204]	Loss 0.3316 (0.4266)	
training:	Epoch: [90][114/204]	Loss 0.3853 (0.4263)	
training:	Epoch: [90][115/204]	Loss 0.4345 (0.4263)	
training:	Epoch: [90][116/204]	Loss 0.3217 (0.4254)	
training:	Epoch: [90][117/204]	Loss 0.4202 (0.4254)	
training:	Epoch: [90][118/204]	Loss 0.3904 (0.4251)	
training:	Epoch: [90][119/204]	Loss 0.3139 (0.4242)	
training:	Epoch: [90][120/204]	Loss 0.4754 (0.4246)	
training:	Epoch: [90][121/204]	Loss 0.4591 (0.4249)	
training:	Epoch: [90][122/204]	Loss 0.3862 (0.4246)	
training:	Epoch: [90][123/204]	Loss 0.4438 (0.4247)	
training:	Epoch: [90][124/204]	Loss 0.4736 (0.4251)	
training:	Epoch: [90][125/204]	Loss 0.5507 (0.4261)	
training:	Epoch: [90][126/204]	Loss 0.4039 (0.4259)	
training:	Epoch: [90][127/204]	Loss 0.3667 (0.4255)	
training:	Epoch: [90][128/204]	Loss 0.4777 (0.4259)	
training:	Epoch: [90][129/204]	Loss 0.3966 (0.4257)	
training:	Epoch: [90][130/204]	Loss 0.4210 (0.4256)	
training:	Epoch: [90][131/204]	Loss 0.5461 (0.4265)	
training:	Epoch: [90][132/204]	Loss 0.5135 (0.4272)	
training:	Epoch: [90][133/204]	Loss 0.4212 (0.4272)	
training:	Epoch: [90][134/204]	Loss 0.5756 (0.4283)	
training:	Epoch: [90][135/204]	Loss 0.4266 (0.4282)	
training:	Epoch: [90][136/204]	Loss 0.4806 (0.4286)	
training:	Epoch: [90][137/204]	Loss 0.4810 (0.4290)	
training:	Epoch: [90][138/204]	Loss 0.3773 (0.4286)	
training:	Epoch: [90][139/204]	Loss 0.2841 (0.4276)	
training:	Epoch: [90][140/204]	Loss 0.4162 (0.4275)	
training:	Epoch: [90][141/204]	Loss 0.4766 (0.4279)	
training:	Epoch: [90][142/204]	Loss 0.4602 (0.4281)	
training:	Epoch: [90][143/204]	Loss 0.3974 (0.4279)	
training:	Epoch: [90][144/204]	Loss 0.6434 (0.4294)	
training:	Epoch: [90][145/204]	Loss 0.3082 (0.4285)	
training:	Epoch: [90][146/204]	Loss 0.3342 (0.4279)	
training:	Epoch: [90][147/204]	Loss 0.3882 (0.4276)	
training:	Epoch: [90][148/204]	Loss 0.3743 (0.4273)	
training:	Epoch: [90][149/204]	Loss 0.3605 (0.4268)	
training:	Epoch: [90][150/204]	Loss 0.4393 (0.4269)	
training:	Epoch: [90][151/204]	Loss 0.3458 (0.4264)	
training:	Epoch: [90][152/204]	Loss 0.4770 (0.4267)	
training:	Epoch: [90][153/204]	Loss 0.4366 (0.4268)	
training:	Epoch: [90][154/204]	Loss 0.4684 (0.4270)	
training:	Epoch: [90][155/204]	Loss 0.5988 (0.4281)	
training:	Epoch: [90][156/204]	Loss 0.4498 (0.4283)	
training:	Epoch: [90][157/204]	Loss 0.3087 (0.4275)	
training:	Epoch: [90][158/204]	Loss 0.4626 (0.4277)	
training:	Epoch: [90][159/204]	Loss 0.5350 (0.4284)	
training:	Epoch: [90][160/204]	Loss 0.4561 (0.4286)	
training:	Epoch: [90][161/204]	Loss 0.3736 (0.4282)	
training:	Epoch: [90][162/204]	Loss 0.4023 (0.4281)	
training:	Epoch: [90][163/204]	Loss 0.3086 (0.4274)	
training:	Epoch: [90][164/204]	Loss 0.4726 (0.4276)	
training:	Epoch: [90][165/204]	Loss 0.4471 (0.4277)	
training:	Epoch: [90][166/204]	Loss 0.5084 (0.4282)	
training:	Epoch: [90][167/204]	Loss 0.6569 (0.4296)	
training:	Epoch: [90][168/204]	Loss 0.3436 (0.4291)	
training:	Epoch: [90][169/204]	Loss 0.4745 (0.4294)	
training:	Epoch: [90][170/204]	Loss 0.3749 (0.4290)	
training:	Epoch: [90][171/204]	Loss 0.4865 (0.4294)	
training:	Epoch: [90][172/204]	Loss 0.5781 (0.4302)	
training:	Epoch: [90][173/204]	Loss 0.3620 (0.4298)	
training:	Epoch: [90][174/204]	Loss 0.3462 (0.4294)	
training:	Epoch: [90][175/204]	Loss 0.4951 (0.4297)	
training:	Epoch: [90][176/204]	Loss 0.3972 (0.4296)	
training:	Epoch: [90][177/204]	Loss 0.5242 (0.4301)	
training:	Epoch: [90][178/204]	Loss 0.5367 (0.4307)	
training:	Epoch: [90][179/204]	Loss 0.3746 (0.4304)	
training:	Epoch: [90][180/204]	Loss 0.4412 (0.4304)	
training:	Epoch: [90][181/204]	Loss 0.3112 (0.4298)	
training:	Epoch: [90][182/204]	Loss 0.4635 (0.4300)	
training:	Epoch: [90][183/204]	Loss 0.6376 (0.4311)	
training:	Epoch: [90][184/204]	Loss 0.2827 (0.4303)	
training:	Epoch: [90][185/204]	Loss 0.4441 (0.4304)	
training:	Epoch: [90][186/204]	Loss 0.5140 (0.4308)	
training:	Epoch: [90][187/204]	Loss 0.5073 (0.4312)	
training:	Epoch: [90][188/204]	Loss 0.3638 (0.4309)	
training:	Epoch: [90][189/204]	Loss 0.2970 (0.4302)	
training:	Epoch: [90][190/204]	Loss 0.4005 (0.4300)	
training:	Epoch: [90][191/204]	Loss 0.2846 (0.4292)	
training:	Epoch: [90][192/204]	Loss 0.3545 (0.4288)	
training:	Epoch: [90][193/204]	Loss 0.5081 (0.4293)	
training:	Epoch: [90][194/204]	Loss 0.2911 (0.4285)	
training:	Epoch: [90][195/204]	Loss 0.4379 (0.4286)	
training:	Epoch: [90][196/204]	Loss 0.4062 (0.4285)	
training:	Epoch: [90][197/204]	Loss 0.5579 (0.4291)	
training:	Epoch: [90][198/204]	Loss 0.3021 (0.4285)	
training:	Epoch: [90][199/204]	Loss 0.3768 (0.4282)	
training:	Epoch: [90][200/204]	Loss 0.3346 (0.4278)	
training:	Epoch: [90][201/204]	Loss 0.4638 (0.4279)	
training:	Epoch: [90][202/204]	Loss 0.3941 (0.4278)	
training:	Epoch: [90][203/204]	Loss 0.4026 (0.4277)	
training:	Epoch: [90][204/204]	Loss 0.3952 (0.4275)	
Training:	 Loss: 0.4268

Training:	 ACC: 0.8216 0.8218 0.8266 0.8166
Validation:	 ACC: 0.7930 0.7935 0.8035 0.7825
Validation:	 Best_BACC: 0.7943 0.7951 0.8117 0.7769
Validation:	 Loss: 0.4482
Pretraining:	Epoch 91/120
----------
training:	Epoch: [91][1/204]	Loss 0.4995 (0.4995)	
training:	Epoch: [91][2/204]	Loss 0.4793 (0.4894)	
training:	Epoch: [91][3/204]	Loss 0.3874 (0.4554)	
training:	Epoch: [91][4/204]	Loss 0.3796 (0.4365)	
training:	Epoch: [91][5/204]	Loss 0.3009 (0.4093)	
training:	Epoch: [91][6/204]	Loss 0.5189 (0.4276)	
training:	Epoch: [91][7/204]	Loss 0.4626 (0.4326)	
training:	Epoch: [91][8/204]	Loss 0.4093 (0.4297)	
training:	Epoch: [91][9/204]	Loss 0.3342 (0.4191)	
training:	Epoch: [91][10/204]	Loss 0.4797 (0.4251)	
training:	Epoch: [91][11/204]	Loss 0.4401 (0.4265)	
training:	Epoch: [91][12/204]	Loss 0.4082 (0.4250)	
training:	Epoch: [91][13/204]	Loss 0.4184 (0.4245)	
training:	Epoch: [91][14/204]	Loss 0.5512 (0.4335)	
training:	Epoch: [91][15/204]	Loss 0.3063 (0.4250)	
training:	Epoch: [91][16/204]	Loss 0.3322 (0.4192)	
training:	Epoch: [91][17/204]	Loss 0.4407 (0.4205)	
training:	Epoch: [91][18/204]	Loss 0.2408 (0.4105)	
training:	Epoch: [91][19/204]	Loss 0.4938 (0.4149)	
training:	Epoch: [91][20/204]	Loss 0.3032 (0.4093)	
training:	Epoch: [91][21/204]	Loss 0.3454 (0.4063)	
training:	Epoch: [91][22/204]	Loss 0.4852 (0.4099)	
training:	Epoch: [91][23/204]	Loss 0.5601 (0.4164)	
training:	Epoch: [91][24/204]	Loss 0.3882 (0.4152)	
training:	Epoch: [91][25/204]	Loss 0.3810 (0.4138)	
training:	Epoch: [91][26/204]	Loss 0.4303 (0.4145)	
training:	Epoch: [91][27/204]	Loss 0.4783 (0.4168)	
training:	Epoch: [91][28/204]	Loss 0.3213 (0.4134)	
training:	Epoch: [91][29/204]	Loss 0.3464 (0.4111)	
training:	Epoch: [91][30/204]	Loss 0.3634 (0.4095)	
training:	Epoch: [91][31/204]	Loss 0.4723 (0.4116)	
training:	Epoch: [91][32/204]	Loss 0.4822 (0.4138)	
training:	Epoch: [91][33/204]	Loss 0.6404 (0.4206)	
training:	Epoch: [91][34/204]	Loss 0.4805 (0.4224)	
training:	Epoch: [91][35/204]	Loss 0.5220 (0.4252)	
training:	Epoch: [91][36/204]	Loss 0.3925 (0.4243)	
training:	Epoch: [91][37/204]	Loss 0.5310 (0.4272)	
training:	Epoch: [91][38/204]	Loss 0.3916 (0.4263)	
training:	Epoch: [91][39/204]	Loss 0.3579 (0.4245)	
training:	Epoch: [91][40/204]	Loss 0.3210 (0.4219)	
training:	Epoch: [91][41/204]	Loss 0.5590 (0.4253)	
training:	Epoch: [91][42/204]	Loss 0.4677 (0.4263)	
training:	Epoch: [91][43/204]	Loss 0.4149 (0.4260)	
training:	Epoch: [91][44/204]	Loss 0.5910 (0.4298)	
training:	Epoch: [91][45/204]	Loss 0.3945 (0.4290)	
training:	Epoch: [91][46/204]	Loss 0.3605 (0.4275)	
training:	Epoch: [91][47/204]	Loss 0.3862 (0.4266)	
training:	Epoch: [91][48/204]	Loss 0.5955 (0.4301)	
training:	Epoch: [91][49/204]	Loss 0.5571 (0.4327)	
training:	Epoch: [91][50/204]	Loss 0.3639 (0.4313)	
training:	Epoch: [91][51/204]	Loss 0.7486 (0.4376)	
training:	Epoch: [91][52/204]	Loss 0.3858 (0.4366)	
training:	Epoch: [91][53/204]	Loss 0.3270 (0.4345)	
training:	Epoch: [91][54/204]	Loss 0.4614 (0.4350)	
training:	Epoch: [91][55/204]	Loss 0.4696 (0.4356)	
training:	Epoch: [91][56/204]	Loss 0.4423 (0.4358)	
training:	Epoch: [91][57/204]	Loss 0.5662 (0.4380)	
training:	Epoch: [91][58/204]	Loss 0.5198 (0.4394)	
training:	Epoch: [91][59/204]	Loss 0.4702 (0.4400)	
training:	Epoch: [91][60/204]	Loss 0.4575 (0.4403)	
training:	Epoch: [91][61/204]	Loss 0.2609 (0.4373)	
training:	Epoch: [91][62/204]	Loss 0.5104 (0.4385)	
training:	Epoch: [91][63/204]	Loss 0.3199 (0.4366)	
training:	Epoch: [91][64/204]	Loss 0.2434 (0.4336)	
training:	Epoch: [91][65/204]	Loss 0.4596 (0.4340)	
training:	Epoch: [91][66/204]	Loss 0.5227 (0.4353)	
training:	Epoch: [91][67/204]	Loss 0.2913 (0.4332)	
training:	Epoch: [91][68/204]	Loss 0.2845 (0.4310)	
training:	Epoch: [91][69/204]	Loss 0.4767 (0.4317)	
training:	Epoch: [91][70/204]	Loss 0.4091 (0.4313)	
training:	Epoch: [91][71/204]	Loss 0.4055 (0.4310)	
training:	Epoch: [91][72/204]	Loss 0.5369 (0.4325)	
training:	Epoch: [91][73/204]	Loss 0.3705 (0.4316)	
training:	Epoch: [91][74/204]	Loss 0.3323 (0.4303)	
training:	Epoch: [91][75/204]	Loss 0.5494 (0.4319)	
training:	Epoch: [91][76/204]	Loss 0.4336 (0.4319)	
training:	Epoch: [91][77/204]	Loss 0.3602 (0.4309)	
training:	Epoch: [91][78/204]	Loss 0.3106 (0.4294)	
training:	Epoch: [91][79/204]	Loss 0.4413 (0.4296)	
training:	Epoch: [91][80/204]	Loss 0.3821 (0.4290)	
training:	Epoch: [91][81/204]	Loss 0.5464 (0.4304)	
training:	Epoch: [91][82/204]	Loss 0.3245 (0.4291)	
training:	Epoch: [91][83/204]	Loss 0.4174 (0.4290)	
training:	Epoch: [91][84/204]	Loss 0.4252 (0.4289)	
training:	Epoch: [91][85/204]	Loss 0.4701 (0.4294)	
training:	Epoch: [91][86/204]	Loss 0.3943 (0.4290)	
training:	Epoch: [91][87/204]	Loss 0.5675 (0.4306)	
training:	Epoch: [91][88/204]	Loss 0.4699 (0.4310)	
training:	Epoch: [91][89/204]	Loss 0.5365 (0.4322)	
training:	Epoch: [91][90/204]	Loss 0.2416 (0.4301)	
training:	Epoch: [91][91/204]	Loss 0.3620 (0.4294)	
training:	Epoch: [91][92/204]	Loss 0.5677 (0.4309)	
training:	Epoch: [91][93/204]	Loss 0.5153 (0.4318)	
training:	Epoch: [91][94/204]	Loss 0.2983 (0.4304)	
training:	Epoch: [91][95/204]	Loss 0.3649 (0.4297)	
training:	Epoch: [91][96/204]	Loss 0.3986 (0.4293)	
training:	Epoch: [91][97/204]	Loss 0.3650 (0.4287)	
training:	Epoch: [91][98/204]	Loss 0.4033 (0.4284)	
training:	Epoch: [91][99/204]	Loss 0.4255 (0.4284)	
training:	Epoch: [91][100/204]	Loss 0.5756 (0.4299)	
training:	Epoch: [91][101/204]	Loss 0.3745 (0.4293)	
training:	Epoch: [91][102/204]	Loss 0.2413 (0.4275)	
training:	Epoch: [91][103/204]	Loss 0.4939 (0.4281)	
training:	Epoch: [91][104/204]	Loss 0.3101 (0.4270)	
training:	Epoch: [91][105/204]	Loss 0.4941 (0.4276)	
training:	Epoch: [91][106/204]	Loss 0.3393 (0.4268)	
training:	Epoch: [91][107/204]	Loss 0.5183 (0.4276)	
training:	Epoch: [91][108/204]	Loss 0.4254 (0.4276)	
training:	Epoch: [91][109/204]	Loss 0.3715 (0.4271)	
training:	Epoch: [91][110/204]	Loss 0.4485 (0.4273)	
training:	Epoch: [91][111/204]	Loss 0.4069 (0.4271)	
training:	Epoch: [91][112/204]	Loss 0.3304 (0.4263)	
training:	Epoch: [91][113/204]	Loss 0.4213 (0.4262)	
training:	Epoch: [91][114/204]	Loss 0.4622 (0.4265)	
training:	Epoch: [91][115/204]	Loss 0.3968 (0.4263)	
training:	Epoch: [91][116/204]	Loss 0.3752 (0.4258)	
training:	Epoch: [91][117/204]	Loss 0.4030 (0.4256)	
training:	Epoch: [91][118/204]	Loss 0.4474 (0.4258)	
training:	Epoch: [91][119/204]	Loss 0.2195 (0.4241)	
training:	Epoch: [91][120/204]	Loss 0.5291 (0.4250)	
training:	Epoch: [91][121/204]	Loss 0.3724 (0.4245)	
training:	Epoch: [91][122/204]	Loss 0.4426 (0.4247)	
training:	Epoch: [91][123/204]	Loss 0.3703 (0.4242)	
training:	Epoch: [91][124/204]	Loss 0.5040 (0.4249)	
training:	Epoch: [91][125/204]	Loss 0.3062 (0.4239)	
training:	Epoch: [91][126/204]	Loss 0.3384 (0.4232)	
training:	Epoch: [91][127/204]	Loss 0.4908 (0.4238)	
training:	Epoch: [91][128/204]	Loss 0.4411 (0.4239)	
training:	Epoch: [91][129/204]	Loss 0.3761 (0.4235)	
training:	Epoch: [91][130/204]	Loss 0.4985 (0.4241)	
training:	Epoch: [91][131/204]	Loss 0.4169 (0.4241)	
training:	Epoch: [91][132/204]	Loss 0.4186 (0.4240)	
training:	Epoch: [91][133/204]	Loss 0.5095 (0.4247)	
training:	Epoch: [91][134/204]	Loss 0.4507 (0.4249)	
training:	Epoch: [91][135/204]	Loss 0.3957 (0.4246)	
training:	Epoch: [91][136/204]	Loss 0.5049 (0.4252)	
training:	Epoch: [91][137/204]	Loss 0.3684 (0.4248)	
training:	Epoch: [91][138/204]	Loss 0.3897 (0.4246)	
training:	Epoch: [91][139/204]	Loss 0.3926 (0.4243)	
training:	Epoch: [91][140/204]	Loss 0.3931 (0.4241)	
training:	Epoch: [91][141/204]	Loss 0.4565 (0.4243)	
training:	Epoch: [91][142/204]	Loss 0.4132 (0.4243)	
training:	Epoch: [91][143/204]	Loss 0.2820 (0.4233)	
training:	Epoch: [91][144/204]	Loss 0.4709 (0.4236)	
training:	Epoch: [91][145/204]	Loss 0.5011 (0.4241)	
training:	Epoch: [91][146/204]	Loss 0.3940 (0.4239)	
training:	Epoch: [91][147/204]	Loss 0.5095 (0.4245)	
training:	Epoch: [91][148/204]	Loss 0.3812 (0.4242)	
training:	Epoch: [91][149/204]	Loss 0.5051 (0.4248)	
training:	Epoch: [91][150/204]	Loss 0.5402 (0.4255)	
training:	Epoch: [91][151/204]	Loss 0.3935 (0.4253)	
training:	Epoch: [91][152/204]	Loss 0.3530 (0.4248)	
training:	Epoch: [91][153/204]	Loss 0.3989 (0.4247)	
training:	Epoch: [91][154/204]	Loss 0.6111 (0.4259)	
training:	Epoch: [91][155/204]	Loss 0.5377 (0.4266)	
training:	Epoch: [91][156/204]	Loss 0.5367 (0.4273)	
training:	Epoch: [91][157/204]	Loss 0.3226 (0.4266)	
training:	Epoch: [91][158/204]	Loss 0.4114 (0.4265)	
training:	Epoch: [91][159/204]	Loss 0.4546 (0.4267)	
training:	Epoch: [91][160/204]	Loss 0.4641 (0.4270)	
training:	Epoch: [91][161/204]	Loss 0.3470 (0.4265)	
training:	Epoch: [91][162/204]	Loss 0.4089 (0.4264)	
training:	Epoch: [91][163/204]	Loss 0.3097 (0.4256)	
training:	Epoch: [91][164/204]	Loss 0.5595 (0.4265)	
training:	Epoch: [91][165/204]	Loss 0.1870 (0.4250)	
training:	Epoch: [91][166/204]	Loss 0.4067 (0.4249)	
training:	Epoch: [91][167/204]	Loss 0.4717 (0.4252)	
training:	Epoch: [91][168/204]	Loss 0.3670 (0.4248)	
training:	Epoch: [91][169/204]	Loss 0.5073 (0.4253)	
training:	Epoch: [91][170/204]	Loss 0.4516 (0.4255)	
training:	Epoch: [91][171/204]	Loss 0.3853 (0.4252)	
training:	Epoch: [91][172/204]	Loss 0.3254 (0.4247)	
training:	Epoch: [91][173/204]	Loss 0.4748 (0.4249)	
training:	Epoch: [91][174/204]	Loss 0.3281 (0.4244)	
training:	Epoch: [91][175/204]	Loss 0.3300 (0.4238)	
training:	Epoch: [91][176/204]	Loss 0.5025 (0.4243)	
training:	Epoch: [91][177/204]	Loss 0.5159 (0.4248)	
training:	Epoch: [91][178/204]	Loss 0.5741 (0.4256)	
training:	Epoch: [91][179/204]	Loss 0.3530 (0.4252)	
training:	Epoch: [91][180/204]	Loss 0.2919 (0.4245)	
training:	Epoch: [91][181/204]	Loss 0.3875 (0.4243)	
training:	Epoch: [91][182/204]	Loss 0.4717 (0.4246)	
training:	Epoch: [91][183/204]	Loss 0.3787 (0.4243)	
training:	Epoch: [91][184/204]	Loss 0.3244 (0.4238)	
training:	Epoch: [91][185/204]	Loss 0.5349 (0.4244)	
training:	Epoch: [91][186/204]	Loss 0.4231 (0.4244)	
training:	Epoch: [91][187/204]	Loss 0.4931 (0.4247)	
training:	Epoch: [91][188/204]	Loss 0.4457 (0.4248)	
training:	Epoch: [91][189/204]	Loss 0.4641 (0.4250)	
training:	Epoch: [91][190/204]	Loss 0.4295 (0.4251)	
training:	Epoch: [91][191/204]	Loss 0.4825 (0.4254)	
training:	Epoch: [91][192/204]	Loss 0.3012 (0.4247)	
training:	Epoch: [91][193/204]	Loss 0.7042 (0.4262)	
training:	Epoch: [91][194/204]	Loss 0.3343 (0.4257)	
training:	Epoch: [91][195/204]	Loss 0.4344 (0.4257)	
training:	Epoch: [91][196/204]	Loss 0.4603 (0.4259)	
training:	Epoch: [91][197/204]	Loss 0.3928 (0.4258)	
training:	Epoch: [91][198/204]	Loss 0.4239 (0.4257)	
training:	Epoch: [91][199/204]	Loss 0.3621 (0.4254)	
training:	Epoch: [91][200/204]	Loss 0.5399 (0.4260)	
training:	Epoch: [91][201/204]	Loss 0.5037 (0.4264)	
training:	Epoch: [91][202/204]	Loss 0.3669 (0.4261)	
training:	Epoch: [91][203/204]	Loss 0.4041 (0.4260)	
training:	Epoch: [91][204/204]	Loss 0.2809 (0.4253)	
Training:	 Loss: 0.4246

Training:	 ACC: 0.8221 0.8227 0.8372 0.8071
Validation:	 ACC: 0.7957 0.7967 0.8168 0.7747
Validation:	 Best_BACC: 0.7957 0.7967 0.8168 0.7747
Validation:	 Loss: 0.4474
Pretraining:	Epoch 92/120
----------
training:	Epoch: [92][1/204]	Loss 0.4777 (0.4777)	
training:	Epoch: [92][2/204]	Loss 0.6255 (0.5516)	
training:	Epoch: [92][3/204]	Loss 0.2714 (0.4582)	
training:	Epoch: [92][4/204]	Loss 0.4452 (0.4549)	
training:	Epoch: [92][5/204]	Loss 0.4945 (0.4628)	
training:	Epoch: [92][6/204]	Loss 0.2594 (0.4289)	
training:	Epoch: [92][7/204]	Loss 0.3694 (0.4204)	
training:	Epoch: [92][8/204]	Loss 0.3799 (0.4154)	
training:	Epoch: [92][9/204]	Loss 0.3027 (0.4028)	
training:	Epoch: [92][10/204]	Loss 0.4644 (0.4090)	
training:	Epoch: [92][11/204]	Loss 0.3684 (0.4053)	
training:	Epoch: [92][12/204]	Loss 0.4510 (0.4091)	
training:	Epoch: [92][13/204]	Loss 0.4892 (0.4153)	
training:	Epoch: [92][14/204]	Loss 0.4445 (0.4174)	
training:	Epoch: [92][15/204]	Loss 0.3260 (0.4113)	
training:	Epoch: [92][16/204]	Loss 0.3271 (0.4060)	
training:	Epoch: [92][17/204]	Loss 0.2817 (0.3987)	
training:	Epoch: [92][18/204]	Loss 0.3319 (0.3950)	
training:	Epoch: [92][19/204]	Loss 0.3732 (0.3938)	
training:	Epoch: [92][20/204]	Loss 0.5395 (0.4011)	
training:	Epoch: [92][21/204]	Loss 0.3804 (0.4001)	
training:	Epoch: [92][22/204]	Loss 0.3618 (0.3984)	
training:	Epoch: [92][23/204]	Loss 0.3155 (0.3948)	
training:	Epoch: [92][24/204]	Loss 0.4298 (0.3963)	
training:	Epoch: [92][25/204]	Loss 0.5901 (0.4040)	
training:	Epoch: [92][26/204]	Loss 0.4303 (0.4050)	
training:	Epoch: [92][27/204]	Loss 0.7569 (0.4180)	
training:	Epoch: [92][28/204]	Loss 0.3427 (0.4154)	
training:	Epoch: [92][29/204]	Loss 0.6254 (0.4226)	
training:	Epoch: [92][30/204]	Loss 0.3789 (0.4211)	
training:	Epoch: [92][31/204]	Loss 0.4499 (0.4221)	
training:	Epoch: [92][32/204]	Loss 0.4099 (0.4217)	
training:	Epoch: [92][33/204]	Loss 0.3412 (0.4193)	
training:	Epoch: [92][34/204]	Loss 0.3758 (0.4180)	
training:	Epoch: [92][35/204]	Loss 0.3761 (0.4168)	
training:	Epoch: [92][36/204]	Loss 0.4361 (0.4173)	
training:	Epoch: [92][37/204]	Loss 0.2638 (0.4132)	
training:	Epoch: [92][38/204]	Loss 0.2817 (0.4097)	
training:	Epoch: [92][39/204]	Loss 0.4710 (0.4113)	
training:	Epoch: [92][40/204]	Loss 0.4639 (0.4126)	
training:	Epoch: [92][41/204]	Loss 0.2990 (0.4098)	
training:	Epoch: [92][42/204]	Loss 0.4409 (0.4106)	
training:	Epoch: [92][43/204]	Loss 0.4423 (0.4113)	
training:	Epoch: [92][44/204]	Loss 0.5139 (0.4136)	
training:	Epoch: [92][45/204]	Loss 0.4067 (0.4135)	
training:	Epoch: [92][46/204]	Loss 0.5428 (0.4163)	
training:	Epoch: [92][47/204]	Loss 0.4323 (0.4166)	
training:	Epoch: [92][48/204]	Loss 0.6618 (0.4217)	
training:	Epoch: [92][49/204]	Loss 0.2326 (0.4179)	
training:	Epoch: [92][50/204]	Loss 0.4198 (0.4179)	
training:	Epoch: [92][51/204]	Loss 0.4116 (0.4178)	
training:	Epoch: [92][52/204]	Loss 0.2593 (0.4147)	
training:	Epoch: [92][53/204]	Loss 0.4264 (0.4150)	
training:	Epoch: [92][54/204]	Loss 0.3497 (0.4138)	
training:	Epoch: [92][55/204]	Loss 0.5195 (0.4157)	
training:	Epoch: [92][56/204]	Loss 0.5147 (0.4174)	
training:	Epoch: [92][57/204]	Loss 0.4691 (0.4184)	
training:	Epoch: [92][58/204]	Loss 0.4553 (0.4190)	
training:	Epoch: [92][59/204]	Loss 0.3855 (0.4184)	
training:	Epoch: [92][60/204]	Loss 0.4961 (0.4197)	
training:	Epoch: [92][61/204]	Loss 0.2933 (0.4176)	
training:	Epoch: [92][62/204]	Loss 0.3623 (0.4168)	
training:	Epoch: [92][63/204]	Loss 0.4190 (0.4168)	
training:	Epoch: [92][64/204]	Loss 0.5742 (0.4192)	
training:	Epoch: [92][65/204]	Loss 0.4844 (0.4202)	
training:	Epoch: [92][66/204]	Loss 0.4091 (0.4201)	
training:	Epoch: [92][67/204]	Loss 0.4268 (0.4202)	
training:	Epoch: [92][68/204]	Loss 0.3869 (0.4197)	
training:	Epoch: [92][69/204]	Loss 0.3307 (0.4184)	
training:	Epoch: [92][70/204]	Loss 0.4466 (0.4188)	
training:	Epoch: [92][71/204]	Loss 0.3066 (0.4172)	
training:	Epoch: [92][72/204]	Loss 0.5535 (0.4191)	
training:	Epoch: [92][73/204]	Loss 0.3776 (0.4185)	
training:	Epoch: [92][74/204]	Loss 0.3473 (0.4176)	
training:	Epoch: [92][75/204]	Loss 0.5557 (0.4194)	
training:	Epoch: [92][76/204]	Loss 0.3557 (0.4186)	
training:	Epoch: [92][77/204]	Loss 0.5450 (0.4202)	
training:	Epoch: [92][78/204]	Loss 0.3238 (0.4190)	
training:	Epoch: [92][79/204]	Loss 0.2993 (0.4175)	
training:	Epoch: [92][80/204]	Loss 0.5016 (0.4185)	
training:	Epoch: [92][81/204]	Loss 0.4902 (0.4194)	
training:	Epoch: [92][82/204]	Loss 0.4927 (0.4203)	
training:	Epoch: [92][83/204]	Loss 0.4107 (0.4202)	
training:	Epoch: [92][84/204]	Loss 0.7151 (0.4237)	
training:	Epoch: [92][85/204]	Loss 0.2890 (0.4221)	
training:	Epoch: [92][86/204]	Loss 0.3318 (0.4211)	
training:	Epoch: [92][87/204]	Loss 0.2560 (0.4192)	
training:	Epoch: [92][88/204]	Loss 0.4836 (0.4199)	
training:	Epoch: [92][89/204]	Loss 0.3617 (0.4192)	
training:	Epoch: [92][90/204]	Loss 0.3758 (0.4188)	
training:	Epoch: [92][91/204]	Loss 0.5643 (0.4204)	
training:	Epoch: [92][92/204]	Loss 0.4402 (0.4206)	
training:	Epoch: [92][93/204]	Loss 0.3526 (0.4198)	
training:	Epoch: [92][94/204]	Loss 0.3908 (0.4195)	
training:	Epoch: [92][95/204]	Loss 0.4884 (0.4203)	
training:	Epoch: [92][96/204]	Loss 0.3443 (0.4195)	
training:	Epoch: [92][97/204]	Loss 0.3815 (0.4191)	
training:	Epoch: [92][98/204]	Loss 0.3450 (0.4183)	
training:	Epoch: [92][99/204]	Loss 0.3853 (0.4180)	
training:	Epoch: [92][100/204]	Loss 0.4642 (0.4185)	
training:	Epoch: [92][101/204]	Loss 0.4588 (0.4189)	
training:	Epoch: [92][102/204]	Loss 0.3046 (0.4177)	
training:	Epoch: [92][103/204]	Loss 0.3586 (0.4172)	
training:	Epoch: [92][104/204]	Loss 0.3538 (0.4166)	
training:	Epoch: [92][105/204]	Loss 0.4893 (0.4172)	
training:	Epoch: [92][106/204]	Loss 0.2531 (0.4157)	
training:	Epoch: [92][107/204]	Loss 0.3453 (0.4150)	
training:	Epoch: [92][108/204]	Loss 0.5853 (0.4166)	
training:	Epoch: [92][109/204]	Loss 0.4659 (0.4171)	
training:	Epoch: [92][110/204]	Loss 0.3036 (0.4160)	
training:	Epoch: [92][111/204]	Loss 0.3446 (0.4154)	
training:	Epoch: [92][112/204]	Loss 0.3678 (0.4150)	
training:	Epoch: [92][113/204]	Loss 0.3938 (0.4148)	
training:	Epoch: [92][114/204]	Loss 0.3331 (0.4141)	
training:	Epoch: [92][115/204]	Loss 0.2968 (0.4130)	
training:	Epoch: [92][116/204]	Loss 0.3542 (0.4125)	
training:	Epoch: [92][117/204]	Loss 0.4694 (0.4130)	
training:	Epoch: [92][118/204]	Loss 0.4190 (0.4131)	
training:	Epoch: [92][119/204]	Loss 0.6876 (0.4154)	
training:	Epoch: [92][120/204]	Loss 0.3244 (0.4146)	
training:	Epoch: [92][121/204]	Loss 0.5094 (0.4154)	
training:	Epoch: [92][122/204]	Loss 0.4096 (0.4154)	
training:	Epoch: [92][123/204]	Loss 0.5002 (0.4160)	
training:	Epoch: [92][124/204]	Loss 0.4239 (0.4161)	
training:	Epoch: [92][125/204]	Loss 0.3886 (0.4159)	
training:	Epoch: [92][126/204]	Loss 0.4268 (0.4160)	
training:	Epoch: [92][127/204]	Loss 0.3603 (0.4155)	
training:	Epoch: [92][128/204]	Loss 0.3863 (0.4153)	
training:	Epoch: [92][129/204]	Loss 0.4016 (0.4152)	
training:	Epoch: [92][130/204]	Loss 0.4756 (0.4157)	
training:	Epoch: [92][131/204]	Loss 0.4803 (0.4162)	
training:	Epoch: [92][132/204]	Loss 0.3452 (0.4156)	
training:	Epoch: [92][133/204]	Loss 0.5093 (0.4163)	
training:	Epoch: [92][134/204]	Loss 0.4368 (0.4165)	
training:	Epoch: [92][135/204]	Loss 0.5178 (0.4172)	
training:	Epoch: [92][136/204]	Loss 0.3884 (0.4170)	
training:	Epoch: [92][137/204]	Loss 0.6429 (0.4187)	
training:	Epoch: [92][138/204]	Loss 0.3987 (0.4185)	
training:	Epoch: [92][139/204]	Loss 0.4259 (0.4186)	
training:	Epoch: [92][140/204]	Loss 0.2280 (0.4172)	
training:	Epoch: [92][141/204]	Loss 0.3774 (0.4169)	
training:	Epoch: [92][142/204]	Loss 0.4539 (0.4172)	
training:	Epoch: [92][143/204]	Loss 0.4341 (0.4173)	
training:	Epoch: [92][144/204]	Loss 0.6560 (0.4190)	
training:	Epoch: [92][145/204]	Loss 0.3503 (0.4185)	
training:	Epoch: [92][146/204]	Loss 0.5200 (0.4192)	
training:	Epoch: [92][147/204]	Loss 0.3253 (0.4186)	
training:	Epoch: [92][148/204]	Loss 0.4179 (0.4185)	
training:	Epoch: [92][149/204]	Loss 0.4674 (0.4189)	
training:	Epoch: [92][150/204]	Loss 0.4470 (0.4191)	
training:	Epoch: [92][151/204]	Loss 0.4482 (0.4193)	
training:	Epoch: [92][152/204]	Loss 0.2817 (0.4183)	
training:	Epoch: [92][153/204]	Loss 0.4854 (0.4188)	
training:	Epoch: [92][154/204]	Loss 0.4648 (0.4191)	
training:	Epoch: [92][155/204]	Loss 0.4738 (0.4194)	
training:	Epoch: [92][156/204]	Loss 0.4874 (0.4199)	
training:	Epoch: [92][157/204]	Loss 0.3976 (0.4197)	
training:	Epoch: [92][158/204]	Loss 0.4363 (0.4198)	
training:	Epoch: [92][159/204]	Loss 0.5528 (0.4207)	
training:	Epoch: [92][160/204]	Loss 0.3220 (0.4201)	
training:	Epoch: [92][161/204]	Loss 0.4749 (0.4204)	
training:	Epoch: [92][162/204]	Loss 0.3478 (0.4200)	
training:	Epoch: [92][163/204]	Loss 0.4966 (0.4204)	
training:	Epoch: [92][164/204]	Loss 0.3735 (0.4201)	
training:	Epoch: [92][165/204]	Loss 0.3861 (0.4199)	
training:	Epoch: [92][166/204]	Loss 0.4223 (0.4199)	
training:	Epoch: [92][167/204]	Loss 0.3920 (0.4198)	
training:	Epoch: [92][168/204]	Loss 0.4857 (0.4202)	
training:	Epoch: [92][169/204]	Loss 0.4462 (0.4203)	
training:	Epoch: [92][170/204]	Loss 0.3169 (0.4197)	
training:	Epoch: [92][171/204]	Loss 0.4630 (0.4200)	
training:	Epoch: [92][172/204]	Loss 0.4747 (0.4203)	
training:	Epoch: [92][173/204]	Loss 0.4823 (0.4206)	
training:	Epoch: [92][174/204]	Loss 0.4941 (0.4211)	
training:	Epoch: [92][175/204]	Loss 0.5755 (0.4219)	
training:	Epoch: [92][176/204]	Loss 0.4075 (0.4219)	
training:	Epoch: [92][177/204]	Loss 0.4274 (0.4219)	
training:	Epoch: [92][178/204]	Loss 0.3195 (0.4213)	
training:	Epoch: [92][179/204]	Loss 0.4058 (0.4212)	
training:	Epoch: [92][180/204]	Loss 0.3786 (0.4210)	
training:	Epoch: [92][181/204]	Loss 0.3948 (0.4209)	
training:	Epoch: [92][182/204]	Loss 0.3817 (0.4206)	
training:	Epoch: [92][183/204]	Loss 0.4975 (0.4211)	
training:	Epoch: [92][184/204]	Loss 0.4075 (0.4210)	
training:	Epoch: [92][185/204]	Loss 0.2993 (0.4203)	
training:	Epoch: [92][186/204]	Loss 0.6146 (0.4214)	
training:	Epoch: [92][187/204]	Loss 0.4647 (0.4216)	
training:	Epoch: [92][188/204]	Loss 0.3056 (0.4210)	
training:	Epoch: [92][189/204]	Loss 0.3072 (0.4204)	
training:	Epoch: [92][190/204]	Loss 0.4294 (0.4204)	
training:	Epoch: [92][191/204]	Loss 0.4047 (0.4203)	
training:	Epoch: [92][192/204]	Loss 0.5872 (0.4212)	
training:	Epoch: [92][193/204]	Loss 0.4410 (0.4213)	
training:	Epoch: [92][194/204]	Loss 0.4311 (0.4214)	
training:	Epoch: [92][195/204]	Loss 0.5027 (0.4218)	
training:	Epoch: [92][196/204]	Loss 0.4269 (0.4218)	
training:	Epoch: [92][197/204]	Loss 0.3601 (0.4215)	
training:	Epoch: [92][198/204]	Loss 0.3487 (0.4211)	
training:	Epoch: [92][199/204]	Loss 0.5128 (0.4216)	
training:	Epoch: [92][200/204]	Loss 0.5138 (0.4221)	
training:	Epoch: [92][201/204]	Loss 0.4550 (0.4222)	
training:	Epoch: [92][202/204]	Loss 0.3251 (0.4217)	
training:	Epoch: [92][203/204]	Loss 0.4241 (0.4217)	
training:	Epoch: [92][204/204]	Loss 0.5920 (0.4226)	
Training:	 Loss: 0.4219

Training:	 ACC: 0.8240 0.8243 0.8304 0.8176
Validation:	 ACC: 0.7935 0.7940 0.8045 0.7825
Validation:	 Best_BACC: 0.7957 0.7967 0.8168 0.7747
Validation:	 Loss: 0.4464
Pretraining:	Epoch 93/120
----------
training:	Epoch: [93][1/204]	Loss 0.5058 (0.5058)	
training:	Epoch: [93][2/204]	Loss 0.3416 (0.4237)	
training:	Epoch: [93][3/204]	Loss 0.4441 (0.4305)	
training:	Epoch: [93][4/204]	Loss 0.3326 (0.4060)	
training:	Epoch: [93][5/204]	Loss 0.3024 (0.3853)	
training:	Epoch: [93][6/204]	Loss 0.4992 (0.4043)	
training:	Epoch: [93][7/204]	Loss 0.5797 (0.4293)	
training:	Epoch: [93][8/204]	Loss 0.5521 (0.4447)	
training:	Epoch: [93][9/204]	Loss 0.4315 (0.4432)	
training:	Epoch: [93][10/204]	Loss 0.3344 (0.4323)	
training:	Epoch: [93][11/204]	Loss 0.4905 (0.4376)	
training:	Epoch: [93][12/204]	Loss 0.2365 (0.4209)	
training:	Epoch: [93][13/204]	Loss 0.2815 (0.4101)	
training:	Epoch: [93][14/204]	Loss 0.4522 (0.4131)	
training:	Epoch: [93][15/204]	Loss 0.3810 (0.4110)	
training:	Epoch: [93][16/204]	Loss 0.4387 (0.4127)	
training:	Epoch: [93][17/204]	Loss 0.5056 (0.4182)	
training:	Epoch: [93][18/204]	Loss 0.4391 (0.4194)	
training:	Epoch: [93][19/204]	Loss 0.4496 (0.4209)	
training:	Epoch: [93][20/204]	Loss 0.5569 (0.4277)	
training:	Epoch: [93][21/204]	Loss 0.4895 (0.4307)	
training:	Epoch: [93][22/204]	Loss 0.3810 (0.4284)	
training:	Epoch: [93][23/204]	Loss 0.3786 (0.4263)	
training:	Epoch: [93][24/204]	Loss 0.3998 (0.4252)	
training:	Epoch: [93][25/204]	Loss 0.4993 (0.4281)	
training:	Epoch: [93][26/204]	Loss 0.2958 (0.4230)	
training:	Epoch: [93][27/204]	Loss 0.3556 (0.4205)	
training:	Epoch: [93][28/204]	Loss 0.4820 (0.4227)	
training:	Epoch: [93][29/204]	Loss 0.4625 (0.4241)	
training:	Epoch: [93][30/204]	Loss 0.4566 (0.4252)	
training:	Epoch: [93][31/204]	Loss 0.3765 (0.4236)	
training:	Epoch: [93][32/204]	Loss 0.4443 (0.4243)	
training:	Epoch: [93][33/204]	Loss 0.3642 (0.4224)	
training:	Epoch: [93][34/204]	Loss 0.3735 (0.4210)	
training:	Epoch: [93][35/204]	Loss 0.3561 (0.4191)	
training:	Epoch: [93][36/204]	Loss 0.4375 (0.4197)	
training:	Epoch: [93][37/204]	Loss 0.3874 (0.4188)	
training:	Epoch: [93][38/204]	Loss 0.3448 (0.4168)	
training:	Epoch: [93][39/204]	Loss 0.3610 (0.4154)	
training:	Epoch: [93][40/204]	Loss 0.4073 (0.4152)	
training:	Epoch: [93][41/204]	Loss 0.3682 (0.4141)	
training:	Epoch: [93][42/204]	Loss 0.2976 (0.4113)	
training:	Epoch: [93][43/204]	Loss 0.3607 (0.4101)	
training:	Epoch: [93][44/204]	Loss 0.4147 (0.4102)	
training:	Epoch: [93][45/204]	Loss 0.6263 (0.4150)	
training:	Epoch: [93][46/204]	Loss 0.4620 (0.4160)	
training:	Epoch: [93][47/204]	Loss 0.3667 (0.4150)	
training:	Epoch: [93][48/204]	Loss 0.5172 (0.4171)	
training:	Epoch: [93][49/204]	Loss 0.4279 (0.4173)	
training:	Epoch: [93][50/204]	Loss 0.3498 (0.4160)	
training:	Epoch: [93][51/204]	Loss 0.4395 (0.4164)	
training:	Epoch: [93][52/204]	Loss 0.3111 (0.4144)	
training:	Epoch: [93][53/204]	Loss 0.2802 (0.4119)	
training:	Epoch: [93][54/204]	Loss 0.4408 (0.4124)	
training:	Epoch: [93][55/204]	Loss 0.4034 (0.4123)	
training:	Epoch: [93][56/204]	Loss 0.3632 (0.4114)	
training:	Epoch: [93][57/204]	Loss 0.4709 (0.4124)	
training:	Epoch: [93][58/204]	Loss 0.4279 (0.4127)	
training:	Epoch: [93][59/204]	Loss 0.4097 (0.4126)	
training:	Epoch: [93][60/204]	Loss 0.4709 (0.4136)	
training:	Epoch: [93][61/204]	Loss 0.3049 (0.4118)	
training:	Epoch: [93][62/204]	Loss 0.3230 (0.4104)	
training:	Epoch: [93][63/204]	Loss 0.4288 (0.4107)	
training:	Epoch: [93][64/204]	Loss 0.5097 (0.4122)	
training:	Epoch: [93][65/204]	Loss 0.3255 (0.4109)	
training:	Epoch: [93][66/204]	Loss 0.3962 (0.4107)	
training:	Epoch: [93][67/204]	Loss 0.4011 (0.4105)	
training:	Epoch: [93][68/204]	Loss 0.5360 (0.4124)	
training:	Epoch: [93][69/204]	Loss 0.3730 (0.4118)	
training:	Epoch: [93][70/204]	Loss 0.5308 (0.4135)	
training:	Epoch: [93][71/204]	Loss 0.3325 (0.4124)	
training:	Epoch: [93][72/204]	Loss 0.4767 (0.4133)	
training:	Epoch: [93][73/204]	Loss 0.5716 (0.4154)	
training:	Epoch: [93][74/204]	Loss 0.4125 (0.4154)	
training:	Epoch: [93][75/204]	Loss 0.4164 (0.4154)	
training:	Epoch: [93][76/204]	Loss 0.3452 (0.4145)	
training:	Epoch: [93][77/204]	Loss 0.3896 (0.4142)	
training:	Epoch: [93][78/204]	Loss 0.4104 (0.4141)	
training:	Epoch: [93][79/204]	Loss 0.3518 (0.4133)	
training:	Epoch: [93][80/204]	Loss 0.5342 (0.4148)	
training:	Epoch: [93][81/204]	Loss 0.6121 (0.4173)	
training:	Epoch: [93][82/204]	Loss 0.4931 (0.4182)	
training:	Epoch: [93][83/204]	Loss 0.2766 (0.4165)	
training:	Epoch: [93][84/204]	Loss 0.3605 (0.4158)	
training:	Epoch: [93][85/204]	Loss 0.3492 (0.4150)	
training:	Epoch: [93][86/204]	Loss 0.3844 (0.4147)	
training:	Epoch: [93][87/204]	Loss 0.3392 (0.4138)	
training:	Epoch: [93][88/204]	Loss 0.4730 (0.4145)	
training:	Epoch: [93][89/204]	Loss 0.5066 (0.4155)	
training:	Epoch: [93][90/204]	Loss 0.5172 (0.4167)	
training:	Epoch: [93][91/204]	Loss 0.2941 (0.4153)	
training:	Epoch: [93][92/204]	Loss 0.4353 (0.4155)	
training:	Epoch: [93][93/204]	Loss 0.4786 (0.4162)	
training:	Epoch: [93][94/204]	Loss 0.4204 (0.4162)	
training:	Epoch: [93][95/204]	Loss 0.4060 (0.4161)	
training:	Epoch: [93][96/204]	Loss 0.6827 (0.4189)	
training:	Epoch: [93][97/204]	Loss 0.5021 (0.4198)	
training:	Epoch: [93][98/204]	Loss 0.4493 (0.4201)	
training:	Epoch: [93][99/204]	Loss 0.3580 (0.4194)	
training:	Epoch: [93][100/204]	Loss 0.3340 (0.4186)	
training:	Epoch: [93][101/204]	Loss 0.4020 (0.4184)	
training:	Epoch: [93][102/204]	Loss 0.4846 (0.4191)	
training:	Epoch: [93][103/204]	Loss 0.3889 (0.4188)	
training:	Epoch: [93][104/204]	Loss 0.4121 (0.4187)	
training:	Epoch: [93][105/204]	Loss 0.3902 (0.4184)	
training:	Epoch: [93][106/204]	Loss 0.3687 (0.4180)	
training:	Epoch: [93][107/204]	Loss 0.4295 (0.4181)	
training:	Epoch: [93][108/204]	Loss 0.4934 (0.4188)	
training:	Epoch: [93][109/204]	Loss 0.4131 (0.4187)	
training:	Epoch: [93][110/204]	Loss 0.3443 (0.4181)	
training:	Epoch: [93][111/204]	Loss 0.4108 (0.4180)	
training:	Epoch: [93][112/204]	Loss 0.3989 (0.4178)	
training:	Epoch: [93][113/204]	Loss 0.3553 (0.4173)	
training:	Epoch: [93][114/204]	Loss 0.2790 (0.4161)	
training:	Epoch: [93][115/204]	Loss 0.4466 (0.4163)	
training:	Epoch: [93][116/204]	Loss 0.4134 (0.4163)	
training:	Epoch: [93][117/204]	Loss 0.3724 (0.4159)	
training:	Epoch: [93][118/204]	Loss 0.6235 (0.4177)	
training:	Epoch: [93][119/204]	Loss 0.4759 (0.4182)	
training:	Epoch: [93][120/204]	Loss 0.3592 (0.4177)	
training:	Epoch: [93][121/204]	Loss 0.5498 (0.4188)	
training:	Epoch: [93][122/204]	Loss 0.4222 (0.4188)	
training:	Epoch: [93][123/204]	Loss 0.3121 (0.4179)	
training:	Epoch: [93][124/204]	Loss 0.4577 (0.4182)	
training:	Epoch: [93][125/204]	Loss 0.4363 (0.4184)	
training:	Epoch: [93][126/204]	Loss 0.3641 (0.4180)	
training:	Epoch: [93][127/204]	Loss 0.5504 (0.4190)	
training:	Epoch: [93][128/204]	Loss 0.4612 (0.4193)	
training:	Epoch: [93][129/204]	Loss 0.3613 (0.4189)	
training:	Epoch: [93][130/204]	Loss 0.5603 (0.4200)	
training:	Epoch: [93][131/204]	Loss 0.4229 (0.4200)	
training:	Epoch: [93][132/204]	Loss 0.4193 (0.4200)	
training:	Epoch: [93][133/204]	Loss 0.3890 (0.4198)	
training:	Epoch: [93][134/204]	Loss 0.4258 (0.4198)	
training:	Epoch: [93][135/204]	Loss 0.5132 (0.4205)	
training:	Epoch: [93][136/204]	Loss 0.6119 (0.4219)	
training:	Epoch: [93][137/204]	Loss 0.3963 (0.4217)	
training:	Epoch: [93][138/204]	Loss 0.5294 (0.4225)	
training:	Epoch: [93][139/204]	Loss 0.4836 (0.4229)	
training:	Epoch: [93][140/204]	Loss 0.4971 (0.4235)	
training:	Epoch: [93][141/204]	Loss 0.6210 (0.4249)	
training:	Epoch: [93][142/204]	Loss 0.4645 (0.4251)	
training:	Epoch: [93][143/204]	Loss 0.4684 (0.4254)	
training:	Epoch: [93][144/204]	Loss 0.4728 (0.4258)	
training:	Epoch: [93][145/204]	Loss 0.5057 (0.4263)	
training:	Epoch: [93][146/204]	Loss 0.5063 (0.4269)	
training:	Epoch: [93][147/204]	Loss 0.4339 (0.4269)	
training:	Epoch: [93][148/204]	Loss 0.3552 (0.4264)	
training:	Epoch: [93][149/204]	Loss 0.4970 (0.4269)	
training:	Epoch: [93][150/204]	Loss 0.5199 (0.4275)	
training:	Epoch: [93][151/204]	Loss 0.5727 (0.4285)	
training:	Epoch: [93][152/204]	Loss 0.3714 (0.4281)	
training:	Epoch: [93][153/204]	Loss 0.4074 (0.4280)	
training:	Epoch: [93][154/204]	Loss 0.3577 (0.4275)	
training:	Epoch: [93][155/204]	Loss 0.4505 (0.4277)	
training:	Epoch: [93][156/204]	Loss 0.4197 (0.4276)	
training:	Epoch: [93][157/204]	Loss 0.3448 (0.4271)	
training:	Epoch: [93][158/204]	Loss 0.5258 (0.4277)	
training:	Epoch: [93][159/204]	Loss 0.3667 (0.4273)	
training:	Epoch: [93][160/204]	Loss 0.4881 (0.4277)	
training:	Epoch: [93][161/204]	Loss 0.3919 (0.4275)	
training:	Epoch: [93][162/204]	Loss 0.4936 (0.4279)	
training:	Epoch: [93][163/204]	Loss 0.3709 (0.4276)	
training:	Epoch: [93][164/204]	Loss 0.3581 (0.4271)	
training:	Epoch: [93][165/204]	Loss 0.3955 (0.4269)	
training:	Epoch: [93][166/204]	Loss 0.3287 (0.4263)	
training:	Epoch: [93][167/204]	Loss 0.3628 (0.4260)	
training:	Epoch: [93][168/204]	Loss 0.4704 (0.4262)	
training:	Epoch: [93][169/204]	Loss 0.3437 (0.4257)	
training:	Epoch: [93][170/204]	Loss 0.3910 (0.4255)	
training:	Epoch: [93][171/204]	Loss 0.6072 (0.4266)	
training:	Epoch: [93][172/204]	Loss 0.4286 (0.4266)	
training:	Epoch: [93][173/204]	Loss 0.5119 (0.4271)	
training:	Epoch: [93][174/204]	Loss 0.4770 (0.4274)	
training:	Epoch: [93][175/204]	Loss 0.4450 (0.4275)	
training:	Epoch: [93][176/204]	Loss 0.4480 (0.4276)	
training:	Epoch: [93][177/204]	Loss 0.4130 (0.4275)	
training:	Epoch: [93][178/204]	Loss 0.2940 (0.4268)	
training:	Epoch: [93][179/204]	Loss 0.3615 (0.4264)	
training:	Epoch: [93][180/204]	Loss 0.3482 (0.4260)	
training:	Epoch: [93][181/204]	Loss 0.3942 (0.4258)	
training:	Epoch: [93][182/204]	Loss 0.4797 (0.4261)	
training:	Epoch: [93][183/204]	Loss 0.3766 (0.4258)	
training:	Epoch: [93][184/204]	Loss 0.3527 (0.4254)	
training:	Epoch: [93][185/204]	Loss 0.4259 (0.4254)	
training:	Epoch: [93][186/204]	Loss 0.4326 (0.4255)	
training:	Epoch: [93][187/204]	Loss 0.4494 (0.4256)	
training:	Epoch: [93][188/204]	Loss 0.2881 (0.4249)	
training:	Epoch: [93][189/204]	Loss 0.3035 (0.4242)	
training:	Epoch: [93][190/204]	Loss 0.3374 (0.4238)	
training:	Epoch: [93][191/204]	Loss 0.5465 (0.4244)	
training:	Epoch: [93][192/204]	Loss 0.4494 (0.4245)	
training:	Epoch: [93][193/204]	Loss 0.5719 (0.4253)	
training:	Epoch: [93][194/204]	Loss 0.2065 (0.4242)	
training:	Epoch: [93][195/204]	Loss 0.3633 (0.4239)	
training:	Epoch: [93][196/204]	Loss 0.4969 (0.4242)	
training:	Epoch: [93][197/204]	Loss 0.3942 (0.4241)	
training:	Epoch: [93][198/204]	Loss 0.3359 (0.4236)	
training:	Epoch: [93][199/204]	Loss 0.3951 (0.4235)	
training:	Epoch: [93][200/204]	Loss 0.3358 (0.4231)	
training:	Epoch: [93][201/204]	Loss 0.4730 (0.4233)	
training:	Epoch: [93][202/204]	Loss 0.5340 (0.4239)	
training:	Epoch: [93][203/204]	Loss 0.3392 (0.4234)	
training:	Epoch: [93][204/204]	Loss 0.4025 (0.4233)	
Training:	 Loss: 0.4227

Training:	 ACC: 0.8236 0.8236 0.8248 0.8224
Validation:	 ACC: 0.7931 0.7935 0.8004 0.7859
Validation:	 Best_BACC: 0.7957 0.7967 0.8168 0.7747
Validation:	 Loss: 0.4459
Pretraining:	Epoch 94/120
----------
training:	Epoch: [94][1/204]	Loss 0.3577 (0.3577)	
training:	Epoch: [94][2/204]	Loss 0.4029 (0.3803)	
training:	Epoch: [94][3/204]	Loss 0.2954 (0.3520)	
training:	Epoch: [94][4/204]	Loss 0.3874 (0.3608)	
training:	Epoch: [94][5/204]	Loss 0.4918 (0.3870)	
training:	Epoch: [94][6/204]	Loss 0.3937 (0.3881)	
training:	Epoch: [94][7/204]	Loss 0.5343 (0.4090)	
training:	Epoch: [94][8/204]	Loss 0.5615 (0.4281)	
training:	Epoch: [94][9/204]	Loss 0.4488 (0.4304)	
training:	Epoch: [94][10/204]	Loss 0.4448 (0.4318)	
training:	Epoch: [94][11/204]	Loss 0.5556 (0.4431)	
training:	Epoch: [94][12/204]	Loss 0.5280 (0.4501)	
training:	Epoch: [94][13/204]	Loss 0.2812 (0.4371)	
training:	Epoch: [94][14/204]	Loss 0.5511 (0.4453)	
training:	Epoch: [94][15/204]	Loss 0.3017 (0.4357)	
training:	Epoch: [94][16/204]	Loss 0.4385 (0.4359)	
training:	Epoch: [94][17/204]	Loss 0.4578 (0.4372)	
training:	Epoch: [94][18/204]	Loss 0.3184 (0.4306)	
training:	Epoch: [94][19/204]	Loss 0.3546 (0.4266)	
training:	Epoch: [94][20/204]	Loss 0.4834 (0.4294)	
training:	Epoch: [94][21/204]	Loss 0.4378 (0.4298)	
training:	Epoch: [94][22/204]	Loss 0.3963 (0.4283)	
training:	Epoch: [94][23/204]	Loss 0.3054 (0.4230)	
training:	Epoch: [94][24/204]	Loss 0.5778 (0.4294)	
training:	Epoch: [94][25/204]	Loss 0.4594 (0.4306)	
training:	Epoch: [94][26/204]	Loss 0.3804 (0.4287)	
training:	Epoch: [94][27/204]	Loss 0.4508 (0.4295)	
training:	Epoch: [94][28/204]	Loss 0.4413 (0.4299)	
training:	Epoch: [94][29/204]	Loss 0.3936 (0.4287)	
training:	Epoch: [94][30/204]	Loss 0.5273 (0.4319)	
training:	Epoch: [94][31/204]	Loss 0.5065 (0.4344)	
training:	Epoch: [94][32/204]	Loss 0.3536 (0.4318)	
training:	Epoch: [94][33/204]	Loss 0.3765 (0.4302)	
training:	Epoch: [94][34/204]	Loss 0.2911 (0.4261)	
training:	Epoch: [94][35/204]	Loss 0.2767 (0.4218)	
training:	Epoch: [94][36/204]	Loss 0.3865 (0.4208)	
training:	Epoch: [94][37/204]	Loss 0.4995 (0.4229)	
training:	Epoch: [94][38/204]	Loss 0.4340 (0.4232)	
training:	Epoch: [94][39/204]	Loss 0.2981 (0.4200)	
training:	Epoch: [94][40/204]	Loss 0.4657 (0.4212)	
training:	Epoch: [94][41/204]	Loss 0.3957 (0.4205)	
training:	Epoch: [94][42/204]	Loss 0.5187 (0.4229)	
training:	Epoch: [94][43/204]	Loss 0.3222 (0.4205)	
training:	Epoch: [94][44/204]	Loss 0.5270 (0.4230)	
training:	Epoch: [94][45/204]	Loss 0.3516 (0.4214)	
training:	Epoch: [94][46/204]	Loss 0.4611 (0.4222)	
training:	Epoch: [94][47/204]	Loss 0.4161 (0.4221)	
training:	Epoch: [94][48/204]	Loss 0.5027 (0.4238)	
training:	Epoch: [94][49/204]	Loss 0.3382 (0.4220)	
training:	Epoch: [94][50/204]	Loss 0.4219 (0.4220)	
training:	Epoch: [94][51/204]	Loss 0.4704 (0.4230)	
training:	Epoch: [94][52/204]	Loss 0.4963 (0.4244)	
training:	Epoch: [94][53/204]	Loss 0.3295 (0.4226)	
training:	Epoch: [94][54/204]	Loss 0.4692 (0.4235)	
training:	Epoch: [94][55/204]	Loss 0.3939 (0.4229)	
training:	Epoch: [94][56/204]	Loss 0.5855 (0.4258)	
training:	Epoch: [94][57/204]	Loss 0.4554 (0.4264)	
training:	Epoch: [94][58/204]	Loss 0.2084 (0.4226)	
training:	Epoch: [94][59/204]	Loss 0.4145 (0.4225)	
training:	Epoch: [94][60/204]	Loss 0.6348 (0.4260)	
training:	Epoch: [94][61/204]	Loss 0.3392 (0.4246)	
training:	Epoch: [94][62/204]	Loss 0.3894 (0.4240)	
training:	Epoch: [94][63/204]	Loss 0.3754 (0.4232)	
training:	Epoch: [94][64/204]	Loss 0.3674 (0.4224)	
training:	Epoch: [94][65/204]	Loss 0.3167 (0.4207)	
training:	Epoch: [94][66/204]	Loss 0.3854 (0.4202)	
training:	Epoch: [94][67/204]	Loss 0.4280 (0.4203)	
training:	Epoch: [94][68/204]	Loss 0.3800 (0.4197)	
training:	Epoch: [94][69/204]	Loss 0.3857 (0.4192)	
training:	Epoch: [94][70/204]	Loss 0.3957 (0.4189)	
training:	Epoch: [94][71/204]	Loss 0.5990 (0.4214)	
training:	Epoch: [94][72/204]	Loss 0.3451 (0.4204)	
training:	Epoch: [94][73/204]	Loss 0.2792 (0.4184)	
training:	Epoch: [94][74/204]	Loss 0.4999 (0.4195)	
training:	Epoch: [94][75/204]	Loss 0.4116 (0.4194)	
training:	Epoch: [94][76/204]	Loss 0.4275 (0.4195)	
training:	Epoch: [94][77/204]	Loss 0.5465 (0.4212)	
training:	Epoch: [94][78/204]	Loss 0.4023 (0.4209)	
training:	Epoch: [94][79/204]	Loss 0.3910 (0.4206)	
training:	Epoch: [94][80/204]	Loss 0.3014 (0.4191)	
training:	Epoch: [94][81/204]	Loss 0.3273 (0.4179)	
training:	Epoch: [94][82/204]	Loss 0.6225 (0.4204)	
training:	Epoch: [94][83/204]	Loss 0.3361 (0.4194)	
training:	Epoch: [94][84/204]	Loss 0.4898 (0.4203)	
training:	Epoch: [94][85/204]	Loss 0.4899 (0.4211)	
training:	Epoch: [94][86/204]	Loss 0.4083 (0.4209)	
training:	Epoch: [94][87/204]	Loss 0.4179 (0.4209)	
training:	Epoch: [94][88/204]	Loss 0.3583 (0.4202)	
training:	Epoch: [94][89/204]	Loss 0.4795 (0.4209)	
training:	Epoch: [94][90/204]	Loss 0.3449 (0.4200)	
training:	Epoch: [94][91/204]	Loss 0.3174 (0.4189)	
training:	Epoch: [94][92/204]	Loss 0.5255 (0.4200)	
training:	Epoch: [94][93/204]	Loss 0.4844 (0.4207)	
training:	Epoch: [94][94/204]	Loss 0.3728 (0.4202)	
training:	Epoch: [94][95/204]	Loss 0.3733 (0.4197)	
training:	Epoch: [94][96/204]	Loss 0.4716 (0.4203)	
training:	Epoch: [94][97/204]	Loss 0.4243 (0.4203)	
training:	Epoch: [94][98/204]	Loss 0.4912 (0.4210)	
training:	Epoch: [94][99/204]	Loss 0.4526 (0.4214)	
training:	Epoch: [94][100/204]	Loss 0.4138 (0.4213)	
training:	Epoch: [94][101/204]	Loss 0.4854 (0.4219)	
training:	Epoch: [94][102/204]	Loss 0.4259 (0.4219)	
training:	Epoch: [94][103/204]	Loss 0.2448 (0.4202)	
training:	Epoch: [94][104/204]	Loss 0.6531 (0.4225)	
training:	Epoch: [94][105/204]	Loss 0.3279 (0.4216)	
training:	Epoch: [94][106/204]	Loss 0.4811 (0.4221)	
training:	Epoch: [94][107/204]	Loss 0.3786 (0.4217)	
training:	Epoch: [94][108/204]	Loss 0.5267 (0.4227)	
training:	Epoch: [94][109/204]	Loss 0.4841 (0.4233)	
training:	Epoch: [94][110/204]	Loss 0.3863 (0.4229)	
training:	Epoch: [94][111/204]	Loss 0.4491 (0.4232)	
training:	Epoch: [94][112/204]	Loss 0.5109 (0.4239)	
training:	Epoch: [94][113/204]	Loss 0.4277 (0.4240)	
training:	Epoch: [94][114/204]	Loss 0.4760 (0.4244)	
training:	Epoch: [94][115/204]	Loss 0.5028 (0.4251)	
training:	Epoch: [94][116/204]	Loss 0.5289 (0.4260)	
training:	Epoch: [94][117/204]	Loss 0.3888 (0.4257)	
training:	Epoch: [94][118/204]	Loss 0.5404 (0.4267)	
training:	Epoch: [94][119/204]	Loss 0.2909 (0.4255)	
training:	Epoch: [94][120/204]	Loss 0.3490 (0.4249)	
training:	Epoch: [94][121/204]	Loss 0.3644 (0.4244)	
training:	Epoch: [94][122/204]	Loss 0.4761 (0.4248)	
training:	Epoch: [94][123/204]	Loss 0.4889 (0.4253)	
training:	Epoch: [94][124/204]	Loss 0.3622 (0.4248)	
training:	Epoch: [94][125/204]	Loss 0.4458 (0.4250)	
training:	Epoch: [94][126/204]	Loss 0.4561 (0.4252)	
training:	Epoch: [94][127/204]	Loss 0.4258 (0.4252)	
training:	Epoch: [94][128/204]	Loss 0.2767 (0.4241)	
training:	Epoch: [94][129/204]	Loss 0.5357 (0.4249)	
training:	Epoch: [94][130/204]	Loss 0.3732 (0.4245)	
training:	Epoch: [94][131/204]	Loss 0.6262 (0.4261)	
training:	Epoch: [94][132/204]	Loss 0.2582 (0.4248)	
training:	Epoch: [94][133/204]	Loss 0.4460 (0.4250)	
training:	Epoch: [94][134/204]	Loss 0.3560 (0.4245)	
training:	Epoch: [94][135/204]	Loss 0.3069 (0.4236)	
training:	Epoch: [94][136/204]	Loss 0.4428 (0.4237)	
training:	Epoch: [94][137/204]	Loss 0.7676 (0.4262)	
training:	Epoch: [94][138/204]	Loss 0.4567 (0.4265)	
training:	Epoch: [94][139/204]	Loss 0.2743 (0.4254)	
training:	Epoch: [94][140/204]	Loss 0.4421 (0.4255)	
training:	Epoch: [94][141/204]	Loss 0.4595 (0.4257)	
training:	Epoch: [94][142/204]	Loss 0.3635 (0.4253)	
training:	Epoch: [94][143/204]	Loss 0.3955 (0.4251)	
training:	Epoch: [94][144/204]	Loss 0.5519 (0.4260)	
training:	Epoch: [94][145/204]	Loss 0.4787 (0.4263)	
training:	Epoch: [94][146/204]	Loss 0.3233 (0.4256)	
training:	Epoch: [94][147/204]	Loss 0.5393 (0.4264)	
training:	Epoch: [94][148/204]	Loss 0.4337 (0.4264)	
training:	Epoch: [94][149/204]	Loss 0.3463 (0.4259)	
training:	Epoch: [94][150/204]	Loss 0.4448 (0.4260)	
training:	Epoch: [94][151/204]	Loss 0.3572 (0.4256)	
training:	Epoch: [94][152/204]	Loss 0.5298 (0.4263)	
training:	Epoch: [94][153/204]	Loss 0.3115 (0.4255)	
training:	Epoch: [94][154/204]	Loss 0.4350 (0.4256)	
training:	Epoch: [94][155/204]	Loss 0.3468 (0.4251)	
training:	Epoch: [94][156/204]	Loss 0.3976 (0.4249)	
training:	Epoch: [94][157/204]	Loss 0.4278 (0.4249)	
training:	Epoch: [94][158/204]	Loss 0.5092 (0.4254)	
training:	Epoch: [94][159/204]	Loss 0.3471 (0.4249)	
training:	Epoch: [94][160/204]	Loss 0.4245 (0.4249)	
training:	Epoch: [94][161/204]	Loss 0.3786 (0.4247)	
training:	Epoch: [94][162/204]	Loss 0.5581 (0.4255)	
training:	Epoch: [94][163/204]	Loss 0.4266 (0.4255)	
training:	Epoch: [94][164/204]	Loss 0.3861 (0.4252)	
training:	Epoch: [94][165/204]	Loss 0.3608 (0.4249)	
training:	Epoch: [94][166/204]	Loss 0.5310 (0.4255)	
training:	Epoch: [94][167/204]	Loss 0.4397 (0.4256)	
training:	Epoch: [94][168/204]	Loss 0.3068 (0.4249)	
training:	Epoch: [94][169/204]	Loss 0.5864 (0.4258)	
training:	Epoch: [94][170/204]	Loss 0.3901 (0.4256)	
training:	Epoch: [94][171/204]	Loss 0.2697 (0.4247)	
training:	Epoch: [94][172/204]	Loss 0.4168 (0.4247)	
training:	Epoch: [94][173/204]	Loss 0.4623 (0.4249)	
training:	Epoch: [94][174/204]	Loss 0.4678 (0.4251)	
training:	Epoch: [94][175/204]	Loss 0.3700 (0.4248)	
training:	Epoch: [94][176/204]	Loss 0.4430 (0.4249)	
training:	Epoch: [94][177/204]	Loss 0.4078 (0.4248)	
training:	Epoch: [94][178/204]	Loss 0.2573 (0.4239)	
training:	Epoch: [94][179/204]	Loss 0.2989 (0.4232)	
training:	Epoch: [94][180/204]	Loss 0.3217 (0.4226)	
training:	Epoch: [94][181/204]	Loss 0.2855 (0.4219)	
training:	Epoch: [94][182/204]	Loss 0.3389 (0.4214)	
training:	Epoch: [94][183/204]	Loss 0.6183 (0.4225)	
training:	Epoch: [94][184/204]	Loss 0.6043 (0.4235)	
training:	Epoch: [94][185/204]	Loss 0.5346 (0.4241)	
training:	Epoch: [94][186/204]	Loss 0.5078 (0.4245)	
training:	Epoch: [94][187/204]	Loss 0.3554 (0.4241)	
training:	Epoch: [94][188/204]	Loss 0.3664 (0.4238)	
training:	Epoch: [94][189/204]	Loss 0.3629 (0.4235)	
training:	Epoch: [94][190/204]	Loss 0.4366 (0.4236)	
training:	Epoch: [94][191/204]	Loss 0.4876 (0.4239)	
training:	Epoch: [94][192/204]	Loss 0.5866 (0.4248)	
training:	Epoch: [94][193/204]	Loss 0.5447 (0.4254)	
training:	Epoch: [94][194/204]	Loss 0.4736 (0.4256)	
training:	Epoch: [94][195/204]	Loss 0.5285 (0.4262)	
training:	Epoch: [94][196/204]	Loss 0.5415 (0.4268)	
training:	Epoch: [94][197/204]	Loss 0.2376 (0.4258)	
training:	Epoch: [94][198/204]	Loss 0.3618 (0.4255)	
training:	Epoch: [94][199/204]	Loss 0.3573 (0.4251)	
training:	Epoch: [94][200/204]	Loss 0.3422 (0.4247)	
training:	Epoch: [94][201/204]	Loss 0.5604 (0.4254)	
training:	Epoch: [94][202/204]	Loss 0.3571 (0.4250)	
training:	Epoch: [94][203/204]	Loss 0.5277 (0.4256)	
training:	Epoch: [94][204/204]	Loss 0.5752 (0.4263)	
Training:	 Loss: 0.4256

Training:	 ACC: 0.8248 0.8250 0.8298 0.8198
Validation:	 ACC: 0.7924 0.7929 0.8045 0.7803
Validation:	 Best_BACC: 0.7957 0.7967 0.8168 0.7747
Validation:	 Loss: 0.4451
Pretraining:	Epoch 95/120
----------
training:	Epoch: [95][1/204]	Loss 0.4537 (0.4537)	
training:	Epoch: [95][2/204]	Loss 0.3606 (0.4071)	
training:	Epoch: [95][3/204]	Loss 0.3831 (0.3991)	
training:	Epoch: [95][4/204]	Loss 0.3983 (0.3989)	
training:	Epoch: [95][5/204]	Loss 0.4674 (0.4126)	
training:	Epoch: [95][6/204]	Loss 0.4492 (0.4187)	
training:	Epoch: [95][7/204]	Loss 0.5139 (0.4323)	
training:	Epoch: [95][8/204]	Loss 0.4240 (0.4313)	
training:	Epoch: [95][9/204]	Loss 0.4755 (0.4362)	
training:	Epoch: [95][10/204]	Loss 0.5685 (0.4494)	
training:	Epoch: [95][11/204]	Loss 0.5104 (0.4550)	
training:	Epoch: [95][12/204]	Loss 0.3576 (0.4469)	
training:	Epoch: [95][13/204]	Loss 0.4613 (0.4480)	
training:	Epoch: [95][14/204]	Loss 0.4319 (0.4468)	
training:	Epoch: [95][15/204]	Loss 0.4379 (0.4462)	
training:	Epoch: [95][16/204]	Loss 0.3865 (0.4425)	
training:	Epoch: [95][17/204]	Loss 0.3656 (0.4380)	
training:	Epoch: [95][18/204]	Loss 0.4196 (0.4369)	
training:	Epoch: [95][19/204]	Loss 0.3213 (0.4309)	
training:	Epoch: [95][20/204]	Loss 0.4863 (0.4336)	
training:	Epoch: [95][21/204]	Loss 0.5098 (0.4373)	
training:	Epoch: [95][22/204]	Loss 0.3227 (0.4321)	
training:	Epoch: [95][23/204]	Loss 0.4349 (0.4322)	
training:	Epoch: [95][24/204]	Loss 0.4319 (0.4322)	
training:	Epoch: [95][25/204]	Loss 0.4807 (0.4341)	
training:	Epoch: [95][26/204]	Loss 0.3698 (0.4316)	
training:	Epoch: [95][27/204]	Loss 0.3906 (0.4301)	
training:	Epoch: [95][28/204]	Loss 0.4780 (0.4318)	
training:	Epoch: [95][29/204]	Loss 0.4174 (0.4313)	
training:	Epoch: [95][30/204]	Loss 0.3676 (0.4292)	
training:	Epoch: [95][31/204]	Loss 0.3614 (0.4270)	
training:	Epoch: [95][32/204]	Loss 0.5408 (0.4306)	
training:	Epoch: [95][33/204]	Loss 0.4485 (0.4311)	
training:	Epoch: [95][34/204]	Loss 0.4708 (0.4323)	
training:	Epoch: [95][35/204]	Loss 0.4621 (0.4331)	
training:	Epoch: [95][36/204]	Loss 0.3330 (0.4304)	
training:	Epoch: [95][37/204]	Loss 0.3652 (0.4286)	
training:	Epoch: [95][38/204]	Loss 0.2093 (0.4228)	
training:	Epoch: [95][39/204]	Loss 0.6008 (0.4274)	
training:	Epoch: [95][40/204]	Loss 0.6294 (0.4324)	
training:	Epoch: [95][41/204]	Loss 0.4606 (0.4331)	
training:	Epoch: [95][42/204]	Loss 0.4359 (0.4332)	
training:	Epoch: [95][43/204]	Loss 0.3744 (0.4318)	
training:	Epoch: [95][44/204]	Loss 0.5581 (0.4347)	
training:	Epoch: [95][45/204]	Loss 0.4957 (0.4361)	
training:	Epoch: [95][46/204]	Loss 0.5366 (0.4382)	
training:	Epoch: [95][47/204]	Loss 0.3440 (0.4362)	
training:	Epoch: [95][48/204]	Loss 0.4346 (0.4362)	
training:	Epoch: [95][49/204]	Loss 0.4071 (0.4356)	
training:	Epoch: [95][50/204]	Loss 0.3049 (0.4330)	
training:	Epoch: [95][51/204]	Loss 0.5160 (0.4346)	
training:	Epoch: [95][52/204]	Loss 0.4586 (0.4351)	
training:	Epoch: [95][53/204]	Loss 0.3917 (0.4343)	
training:	Epoch: [95][54/204]	Loss 0.3490 (0.4327)	
training:	Epoch: [95][55/204]	Loss 0.5555 (0.4349)	
training:	Epoch: [95][56/204]	Loss 0.4272 (0.4348)	
training:	Epoch: [95][57/204]	Loss 0.2940 (0.4323)	
training:	Epoch: [95][58/204]	Loss 0.3233 (0.4304)	
training:	Epoch: [95][59/204]	Loss 0.4088 (0.4301)	
training:	Epoch: [95][60/204]	Loss 0.4108 (0.4297)	
training:	Epoch: [95][61/204]	Loss 0.3500 (0.4284)	
training:	Epoch: [95][62/204]	Loss 0.4890 (0.4294)	
training:	Epoch: [95][63/204]	Loss 0.6130 (0.4323)	
training:	Epoch: [95][64/204]	Loss 0.4408 (0.4325)	
training:	Epoch: [95][65/204]	Loss 0.6480 (0.4358)	
training:	Epoch: [95][66/204]	Loss 0.3317 (0.4342)	
training:	Epoch: [95][67/204]	Loss 0.6338 (0.4372)	
training:	Epoch: [95][68/204]	Loss 0.4356 (0.4372)	
training:	Epoch: [95][69/204]	Loss 0.4105 (0.4368)	
training:	Epoch: [95][70/204]	Loss 0.3950 (0.4362)	
training:	Epoch: [95][71/204]	Loss 0.4252 (0.4360)	
training:	Epoch: [95][72/204]	Loss 0.3939 (0.4354)	
training:	Epoch: [95][73/204]	Loss 0.3325 (0.4340)	
training:	Epoch: [95][74/204]	Loss 0.2915 (0.4321)	
training:	Epoch: [95][75/204]	Loss 0.3003 (0.4303)	
training:	Epoch: [95][76/204]	Loss 0.5281 (0.4316)	
training:	Epoch: [95][77/204]	Loss 0.4829 (0.4323)	
training:	Epoch: [95][78/204]	Loss 0.4460 (0.4325)	
training:	Epoch: [95][79/204]	Loss 0.4798 (0.4331)	
training:	Epoch: [95][80/204]	Loss 0.3913 (0.4325)	
training:	Epoch: [95][81/204]	Loss 0.4182 (0.4324)	
training:	Epoch: [95][82/204]	Loss 0.2540 (0.4302)	
training:	Epoch: [95][83/204]	Loss 0.3616 (0.4294)	
training:	Epoch: [95][84/204]	Loss 0.3621 (0.4286)	
training:	Epoch: [95][85/204]	Loss 0.4001 (0.4282)	
training:	Epoch: [95][86/204]	Loss 0.4553 (0.4285)	
training:	Epoch: [95][87/204]	Loss 0.3264 (0.4274)	
training:	Epoch: [95][88/204]	Loss 0.6246 (0.4296)	
training:	Epoch: [95][89/204]	Loss 0.4502 (0.4298)	
training:	Epoch: [95][90/204]	Loss 0.4010 (0.4295)	
training:	Epoch: [95][91/204]	Loss 0.4723 (0.4300)	
training:	Epoch: [95][92/204]	Loss 0.6935 (0.4329)	
training:	Epoch: [95][93/204]	Loss 0.2621 (0.4310)	
training:	Epoch: [95][94/204]	Loss 0.4179 (0.4309)	
training:	Epoch: [95][95/204]	Loss 0.3934 (0.4305)	
training:	Epoch: [95][96/204]	Loss 0.3990 (0.4302)	
training:	Epoch: [95][97/204]	Loss 0.3623 (0.4295)	
training:	Epoch: [95][98/204]	Loss 0.3838 (0.4290)	
training:	Epoch: [95][99/204]	Loss 0.2799 (0.4275)	
training:	Epoch: [95][100/204]	Loss 0.3697 (0.4269)	
training:	Epoch: [95][101/204]	Loss 0.4829 (0.4275)	
training:	Epoch: [95][102/204]	Loss 0.3996 (0.4272)	
training:	Epoch: [95][103/204]	Loss 0.3833 (0.4268)	
training:	Epoch: [95][104/204]	Loss 0.3955 (0.4265)	
training:	Epoch: [95][105/204]	Loss 0.3948 (0.4262)	
training:	Epoch: [95][106/204]	Loss 0.4346 (0.4262)	
training:	Epoch: [95][107/204]	Loss 0.3405 (0.4254)	
training:	Epoch: [95][108/204]	Loss 0.3086 (0.4244)	
training:	Epoch: [95][109/204]	Loss 0.4151 (0.4243)	
training:	Epoch: [95][110/204]	Loss 0.4045 (0.4241)	
training:	Epoch: [95][111/204]	Loss 0.2379 (0.4224)	
training:	Epoch: [95][112/204]	Loss 0.3978 (0.4222)	
training:	Epoch: [95][113/204]	Loss 0.3476 (0.4215)	
training:	Epoch: [95][114/204]	Loss 0.3419 (0.4208)	
training:	Epoch: [95][115/204]	Loss 0.4436 (0.4210)	
training:	Epoch: [95][116/204]	Loss 0.3787 (0.4207)	
training:	Epoch: [95][117/204]	Loss 0.4977 (0.4213)	
training:	Epoch: [95][118/204]	Loss 0.4084 (0.4212)	
training:	Epoch: [95][119/204]	Loss 0.4394 (0.4214)	
training:	Epoch: [95][120/204]	Loss 0.5235 (0.4222)	
training:	Epoch: [95][121/204]	Loss 0.4336 (0.4223)	
training:	Epoch: [95][122/204]	Loss 0.2844 (0.4212)	
training:	Epoch: [95][123/204]	Loss 0.3427 (0.4205)	
training:	Epoch: [95][124/204]	Loss 0.4323 (0.4206)	
training:	Epoch: [95][125/204]	Loss 0.4036 (0.4205)	
training:	Epoch: [95][126/204]	Loss 0.4296 (0.4206)	
training:	Epoch: [95][127/204]	Loss 0.6410 (0.4223)	
training:	Epoch: [95][128/204]	Loss 0.4862 (0.4228)	
training:	Epoch: [95][129/204]	Loss 0.2944 (0.4218)	
training:	Epoch: [95][130/204]	Loss 0.3718 (0.4214)	
training:	Epoch: [95][131/204]	Loss 0.4331 (0.4215)	
training:	Epoch: [95][132/204]	Loss 0.4833 (0.4220)	
training:	Epoch: [95][133/204]	Loss 0.3608 (0.4215)	
training:	Epoch: [95][134/204]	Loss 0.3120 (0.4207)	
training:	Epoch: [95][135/204]	Loss 0.5927 (0.4220)	
training:	Epoch: [95][136/204]	Loss 0.2632 (0.4208)	
training:	Epoch: [95][137/204]	Loss 0.4438 (0.4210)	
training:	Epoch: [95][138/204]	Loss 0.4028 (0.4209)	
training:	Epoch: [95][139/204]	Loss 0.4193 (0.4208)	
training:	Epoch: [95][140/204]	Loss 0.5294 (0.4216)	
training:	Epoch: [95][141/204]	Loss 0.5641 (0.4226)	
training:	Epoch: [95][142/204]	Loss 0.5267 (0.4234)	
training:	Epoch: [95][143/204]	Loss 0.4027 (0.4232)	
training:	Epoch: [95][144/204]	Loss 0.5411 (0.4240)	
training:	Epoch: [95][145/204]	Loss 0.4437 (0.4242)	
training:	Epoch: [95][146/204]	Loss 0.2391 (0.4229)	
training:	Epoch: [95][147/204]	Loss 0.3880 (0.4227)	
training:	Epoch: [95][148/204]	Loss 0.5210 (0.4233)	
training:	Epoch: [95][149/204]	Loss 0.3976 (0.4232)	
training:	Epoch: [95][150/204]	Loss 0.6645 (0.4248)	
training:	Epoch: [95][151/204]	Loss 0.2868 (0.4239)	
training:	Epoch: [95][152/204]	Loss 0.3143 (0.4231)	
training:	Epoch: [95][153/204]	Loss 0.3100 (0.4224)	
training:	Epoch: [95][154/204]	Loss 0.4602 (0.4226)	
training:	Epoch: [95][155/204]	Loss 0.4002 (0.4225)	
training:	Epoch: [95][156/204]	Loss 0.4419 (0.4226)	
training:	Epoch: [95][157/204]	Loss 0.2800 (0.4217)	
training:	Epoch: [95][158/204]	Loss 0.4102 (0.4216)	
training:	Epoch: [95][159/204]	Loss 0.5349 (0.4224)	
training:	Epoch: [95][160/204]	Loss 0.5640 (0.4232)	
training:	Epoch: [95][161/204]	Loss 0.4036 (0.4231)	
training:	Epoch: [95][162/204]	Loss 0.3723 (0.4228)	
training:	Epoch: [95][163/204]	Loss 0.4094 (0.4227)	
training:	Epoch: [95][164/204]	Loss 0.4792 (0.4231)	
training:	Epoch: [95][165/204]	Loss 0.5685 (0.4239)	
training:	Epoch: [95][166/204]	Loss 0.4074 (0.4238)	
training:	Epoch: [95][167/204]	Loss 0.3823 (0.4236)	
training:	Epoch: [95][168/204]	Loss 0.3804 (0.4233)	
training:	Epoch: [95][169/204]	Loss 0.6434 (0.4246)	
training:	Epoch: [95][170/204]	Loss 0.4692 (0.4249)	
training:	Epoch: [95][171/204]	Loss 0.3627 (0.4245)	
training:	Epoch: [95][172/204]	Loss 0.5154 (0.4251)	
training:	Epoch: [95][173/204]	Loss 0.3143 (0.4244)	
training:	Epoch: [95][174/204]	Loss 0.3297 (0.4239)	
training:	Epoch: [95][175/204]	Loss 0.5283 (0.4245)	
training:	Epoch: [95][176/204]	Loss 0.6347 (0.4257)	
training:	Epoch: [95][177/204]	Loss 0.3198 (0.4251)	
training:	Epoch: [95][178/204]	Loss 0.3584 (0.4247)	
training:	Epoch: [95][179/204]	Loss 0.4778 (0.4250)	
training:	Epoch: [95][180/204]	Loss 0.2959 (0.4243)	
training:	Epoch: [95][181/204]	Loss 0.2155 (0.4231)	
training:	Epoch: [95][182/204]	Loss 0.3900 (0.4229)	
training:	Epoch: [95][183/204]	Loss 0.4366 (0.4230)	
training:	Epoch: [95][184/204]	Loss 0.3465 (0.4226)	
training:	Epoch: [95][185/204]	Loss 0.5191 (0.4231)	
training:	Epoch: [95][186/204]	Loss 0.4947 (0.4235)	
training:	Epoch: [95][187/204]	Loss 0.6598 (0.4248)	
training:	Epoch: [95][188/204]	Loss 0.4389 (0.4249)	
training:	Epoch: [95][189/204]	Loss 0.3871 (0.4247)	
training:	Epoch: [95][190/204]	Loss 0.3647 (0.4243)	
training:	Epoch: [95][191/204]	Loss 0.3419 (0.4239)	
training:	Epoch: [95][192/204]	Loss 0.4517 (0.4240)	
training:	Epoch: [95][193/204]	Loss 0.5830 (0.4249)	
training:	Epoch: [95][194/204]	Loss 0.4464 (0.4250)	
training:	Epoch: [95][195/204]	Loss 0.4772 (0.4253)	
training:	Epoch: [95][196/204]	Loss 0.3384 (0.4248)	
training:	Epoch: [95][197/204]	Loss 0.4169 (0.4248)	
training:	Epoch: [95][198/204]	Loss 0.3999 (0.4246)	
training:	Epoch: [95][199/204]	Loss 0.4561 (0.4248)	
training:	Epoch: [95][200/204]	Loss 0.4673 (0.4250)	
training:	Epoch: [95][201/204]	Loss 0.3557 (0.4247)	
training:	Epoch: [95][202/204]	Loss 0.4301 (0.4247)	
training:	Epoch: [95][203/204]	Loss 0.3518 (0.4243)	
training:	Epoch: [95][204/204]	Loss 0.4620 (0.4245)	
Training:	 Loss: 0.4239

Training:	 ACC: 0.8252 0.8253 0.8275 0.8230
Validation:	 ACC: 0.7947 0.7951 0.8035 0.7859
Validation:	 Best_BACC: 0.7957 0.7967 0.8168 0.7747
Validation:	 Loss: 0.4442
Pretraining:	Epoch 96/120
----------
training:	Epoch: [96][1/204]	Loss 0.4037 (0.4037)	
training:	Epoch: [96][2/204]	Loss 0.3349 (0.3693)	
training:	Epoch: [96][3/204]	Loss 0.4164 (0.3850)	
training:	Epoch: [96][4/204]	Loss 0.4446 (0.3999)	
training:	Epoch: [96][5/204]	Loss 0.5765 (0.4352)	
training:	Epoch: [96][6/204]	Loss 0.5018 (0.4463)	
training:	Epoch: [96][7/204]	Loss 0.4088 (0.4409)	
training:	Epoch: [96][8/204]	Loss 0.3377 (0.4280)	
training:	Epoch: [96][9/204]	Loss 0.2796 (0.4115)	
training:	Epoch: [96][10/204]	Loss 0.3754 (0.4079)	
training:	Epoch: [96][11/204]	Loss 0.5207 (0.4182)	
training:	Epoch: [96][12/204]	Loss 0.3531 (0.4128)	
training:	Epoch: [96][13/204]	Loss 0.3482 (0.4078)	
training:	Epoch: [96][14/204]	Loss 0.4544 (0.4111)	
training:	Epoch: [96][15/204]	Loss 0.3134 (0.4046)	
training:	Epoch: [96][16/204]	Loss 0.3595 (0.4018)	
training:	Epoch: [96][17/204]	Loss 0.6017 (0.4135)	
training:	Epoch: [96][18/204]	Loss 0.5651 (0.4220)	
training:	Epoch: [96][19/204]	Loss 0.4956 (0.4258)	
training:	Epoch: [96][20/204]	Loss 0.2685 (0.4180)	
training:	Epoch: [96][21/204]	Loss 0.4628 (0.4201)	
training:	Epoch: [96][22/204]	Loss 0.4089 (0.4196)	
training:	Epoch: [96][23/204]	Loss 0.3020 (0.4145)	
training:	Epoch: [96][24/204]	Loss 0.4758 (0.4170)	
training:	Epoch: [96][25/204]	Loss 0.2551 (0.4106)	
training:	Epoch: [96][26/204]	Loss 0.4565 (0.4123)	
training:	Epoch: [96][27/204]	Loss 0.3928 (0.4116)	
training:	Epoch: [96][28/204]	Loss 0.3788 (0.4104)	
training:	Epoch: [96][29/204]	Loss 0.3344 (0.4078)	
training:	Epoch: [96][30/204]	Loss 0.3595 (0.4062)	
training:	Epoch: [96][31/204]	Loss 0.5340 (0.4103)	
training:	Epoch: [96][32/204]	Loss 0.4750 (0.4123)	
training:	Epoch: [96][33/204]	Loss 0.5248 (0.4157)	
training:	Epoch: [96][34/204]	Loss 0.3543 (0.4139)	
training:	Epoch: [96][35/204]	Loss 0.4760 (0.4157)	
training:	Epoch: [96][36/204]	Loss 0.2816 (0.4120)	
training:	Epoch: [96][37/204]	Loss 0.5209 (0.4149)	
training:	Epoch: [96][38/204]	Loss 0.5604 (0.4188)	
training:	Epoch: [96][39/204]	Loss 0.4175 (0.4187)	
training:	Epoch: [96][40/204]	Loss 0.4692 (0.4200)	
training:	Epoch: [96][41/204]	Loss 0.4939 (0.4218)	
training:	Epoch: [96][42/204]	Loss 0.3462 (0.4200)	
training:	Epoch: [96][43/204]	Loss 0.2845 (0.4168)	
training:	Epoch: [96][44/204]	Loss 0.3766 (0.4159)	
training:	Epoch: [96][45/204]	Loss 0.4138 (0.4159)	
training:	Epoch: [96][46/204]	Loss 0.4710 (0.4171)	
training:	Epoch: [96][47/204]	Loss 0.4774 (0.4184)	
training:	Epoch: [96][48/204]	Loss 0.3771 (0.4175)	
training:	Epoch: [96][49/204]	Loss 0.2898 (0.4149)	
training:	Epoch: [96][50/204]	Loss 0.4215 (0.4150)	
training:	Epoch: [96][51/204]	Loss 0.2975 (0.4127)	
training:	Epoch: [96][52/204]	Loss 0.8016 (0.4202)	
training:	Epoch: [96][53/204]	Loss 0.5202 (0.4221)	
training:	Epoch: [96][54/204]	Loss 0.4794 (0.4232)	
training:	Epoch: [96][55/204]	Loss 0.3342 (0.4215)	
training:	Epoch: [96][56/204]	Loss 0.4855 (0.4227)	
training:	Epoch: [96][57/204]	Loss 0.3893 (0.4221)	
training:	Epoch: [96][58/204]	Loss 0.3795 (0.4214)	
training:	Epoch: [96][59/204]	Loss 0.2797 (0.4190)	
training:	Epoch: [96][60/204]	Loss 0.3400 (0.4176)	
training:	Epoch: [96][61/204]	Loss 0.4623 (0.4184)	
training:	Epoch: [96][62/204]	Loss 0.2193 (0.4152)	
training:	Epoch: [96][63/204]	Loss 0.3189 (0.4136)	
training:	Epoch: [96][64/204]	Loss 0.5806 (0.4162)	
training:	Epoch: [96][65/204]	Loss 0.4103 (0.4161)	
training:	Epoch: [96][66/204]	Loss 0.3330 (0.4149)	
training:	Epoch: [96][67/204]	Loss 0.3208 (0.4135)	
training:	Epoch: [96][68/204]	Loss 0.4021 (0.4133)	
training:	Epoch: [96][69/204]	Loss 0.4518 (0.4139)	
training:	Epoch: [96][70/204]	Loss 0.4858 (0.4149)	
training:	Epoch: [96][71/204]	Loss 0.4916 (0.4160)	
training:	Epoch: [96][72/204]	Loss 0.6052 (0.4186)	
training:	Epoch: [96][73/204]	Loss 0.5377 (0.4202)	
training:	Epoch: [96][74/204]	Loss 0.4110 (0.4201)	
training:	Epoch: [96][75/204]	Loss 0.3523 (0.4192)	
training:	Epoch: [96][76/204]	Loss 0.3078 (0.4177)	
training:	Epoch: [96][77/204]	Loss 0.3769 (0.4172)	
training:	Epoch: [96][78/204]	Loss 0.5273 (0.4186)	
training:	Epoch: [96][79/204]	Loss 0.6007 (0.4209)	
training:	Epoch: [96][80/204]	Loss 0.3818 (0.4204)	
training:	Epoch: [96][81/204]	Loss 0.3666 (0.4198)	
training:	Epoch: [96][82/204]	Loss 0.4256 (0.4199)	
training:	Epoch: [96][83/204]	Loss 0.3146 (0.4186)	
training:	Epoch: [96][84/204]	Loss 0.3255 (0.4175)	
training:	Epoch: [96][85/204]	Loss 0.3749 (0.4170)	
training:	Epoch: [96][86/204]	Loss 0.4784 (0.4177)	
training:	Epoch: [96][87/204]	Loss 0.3994 (0.4175)	
training:	Epoch: [96][88/204]	Loss 0.5140 (0.4186)	
training:	Epoch: [96][89/204]	Loss 0.4929 (0.4194)	
training:	Epoch: [96][90/204]	Loss 0.3817 (0.4190)	
training:	Epoch: [96][91/204]	Loss 0.4932 (0.4198)	
training:	Epoch: [96][92/204]	Loss 0.3297 (0.4188)	
training:	Epoch: [96][93/204]	Loss 0.3633 (0.4182)	
training:	Epoch: [96][94/204]	Loss 0.2620 (0.4166)	
training:	Epoch: [96][95/204]	Loss 0.3492 (0.4159)	
training:	Epoch: [96][96/204]	Loss 0.5389 (0.4171)	
training:	Epoch: [96][97/204]	Loss 0.4574 (0.4176)	
training:	Epoch: [96][98/204]	Loss 0.2400 (0.4157)	
training:	Epoch: [96][99/204]	Loss 0.4755 (0.4163)	
training:	Epoch: [96][100/204]	Loss 0.3509 (0.4157)	
training:	Epoch: [96][101/204]	Loss 0.4742 (0.4163)	
training:	Epoch: [96][102/204]	Loss 0.5379 (0.4175)	
training:	Epoch: [96][103/204]	Loss 0.3784 (0.4171)	
training:	Epoch: [96][104/204]	Loss 0.4307 (0.4172)	
training:	Epoch: [96][105/204]	Loss 0.3455 (0.4165)	
training:	Epoch: [96][106/204]	Loss 0.3165 (0.4156)	
training:	Epoch: [96][107/204]	Loss 0.3455 (0.4149)	
training:	Epoch: [96][108/204]	Loss 0.4454 (0.4152)	
training:	Epoch: [96][109/204]	Loss 0.3721 (0.4148)	
training:	Epoch: [96][110/204]	Loss 0.4567 (0.4152)	
training:	Epoch: [96][111/204]	Loss 0.5319 (0.4163)	
training:	Epoch: [96][112/204]	Loss 0.3655 (0.4158)	
training:	Epoch: [96][113/204]	Loss 0.5209 (0.4167)	
training:	Epoch: [96][114/204]	Loss 0.4602 (0.4171)	
training:	Epoch: [96][115/204]	Loss 0.3621 (0.4166)	
training:	Epoch: [96][116/204]	Loss 0.4044 (0.4165)	
training:	Epoch: [96][117/204]	Loss 0.6778 (0.4188)	
training:	Epoch: [96][118/204]	Loss 0.4340 (0.4189)	
training:	Epoch: [96][119/204]	Loss 0.3375 (0.4182)	
training:	Epoch: [96][120/204]	Loss 0.3541 (0.4177)	
training:	Epoch: [96][121/204]	Loss 0.4814 (0.4182)	
training:	Epoch: [96][122/204]	Loss 0.4641 (0.4186)	
training:	Epoch: [96][123/204]	Loss 0.5061 (0.4193)	
training:	Epoch: [96][124/204]	Loss 0.4528 (0.4196)	
training:	Epoch: [96][125/204]	Loss 0.2824 (0.4185)	
training:	Epoch: [96][126/204]	Loss 0.4080 (0.4184)	
training:	Epoch: [96][127/204]	Loss 0.4192 (0.4184)	
training:	Epoch: [96][128/204]	Loss 0.5149 (0.4191)	
training:	Epoch: [96][129/204]	Loss 0.7356 (0.4216)	
training:	Epoch: [96][130/204]	Loss 0.6380 (0.4233)	
training:	Epoch: [96][131/204]	Loss 0.4242 (0.4233)	
training:	Epoch: [96][132/204]	Loss 0.2866 (0.4222)	
training:	Epoch: [96][133/204]	Loss 0.4339 (0.4223)	
training:	Epoch: [96][134/204]	Loss 0.4699 (0.4227)	
training:	Epoch: [96][135/204]	Loss 0.3846 (0.4224)	
training:	Epoch: [96][136/204]	Loss 0.4180 (0.4224)	
training:	Epoch: [96][137/204]	Loss 0.4082 (0.4223)	
training:	Epoch: [96][138/204]	Loss 0.4012 (0.4221)	
training:	Epoch: [96][139/204]	Loss 0.4677 (0.4224)	
training:	Epoch: [96][140/204]	Loss 0.3959 (0.4222)	
training:	Epoch: [96][141/204]	Loss 0.3548 (0.4218)	
training:	Epoch: [96][142/204]	Loss 0.3779 (0.4215)	
training:	Epoch: [96][143/204]	Loss 0.5021 (0.4220)	
training:	Epoch: [96][144/204]	Loss 0.4611 (0.4223)	
training:	Epoch: [96][145/204]	Loss 0.4405 (0.4224)	
training:	Epoch: [96][146/204]	Loss 0.5540 (0.4233)	
training:	Epoch: [96][147/204]	Loss 0.3704 (0.4230)	
training:	Epoch: [96][148/204]	Loss 0.3848 (0.4227)	
training:	Epoch: [96][149/204]	Loss 0.4866 (0.4231)	
training:	Epoch: [96][150/204]	Loss 0.3879 (0.4229)	
training:	Epoch: [96][151/204]	Loss 0.4399 (0.4230)	
training:	Epoch: [96][152/204]	Loss 0.5397 (0.4238)	
training:	Epoch: [96][153/204]	Loss 0.2464 (0.4226)	
training:	Epoch: [96][154/204]	Loss 0.4873 (0.4230)	
training:	Epoch: [96][155/204]	Loss 0.4327 (0.4231)	
training:	Epoch: [96][156/204]	Loss 0.5074 (0.4236)	
training:	Epoch: [96][157/204]	Loss 0.4272 (0.4237)	
training:	Epoch: [96][158/204]	Loss 0.4834 (0.4240)	
training:	Epoch: [96][159/204]	Loss 0.5171 (0.4246)	
training:	Epoch: [96][160/204]	Loss 0.3102 (0.4239)	
training:	Epoch: [96][161/204]	Loss 0.4037 (0.4238)	
training:	Epoch: [96][162/204]	Loss 0.4094 (0.4237)	
training:	Epoch: [96][163/204]	Loss 0.5604 (0.4245)	
training:	Epoch: [96][164/204]	Loss 0.3692 (0.4242)	
training:	Epoch: [96][165/204]	Loss 0.4392 (0.4243)	
training:	Epoch: [96][166/204]	Loss 0.3292 (0.4237)	
training:	Epoch: [96][167/204]	Loss 0.3275 (0.4231)	
training:	Epoch: [96][168/204]	Loss 0.6067 (0.4242)	
training:	Epoch: [96][169/204]	Loss 0.4948 (0.4246)	
training:	Epoch: [96][170/204]	Loss 0.3543 (0.4242)	
training:	Epoch: [96][171/204]	Loss 0.6903 (0.4258)	
training:	Epoch: [96][172/204]	Loss 0.4932 (0.4262)	
training:	Epoch: [96][173/204]	Loss 0.5173 (0.4267)	
training:	Epoch: [96][174/204]	Loss 0.3359 (0.4262)	
training:	Epoch: [96][175/204]	Loss 0.4123 (0.4261)	
training:	Epoch: [96][176/204]	Loss 0.2983 (0.4254)	
training:	Epoch: [96][177/204]	Loss 0.4136 (0.4253)	
training:	Epoch: [96][178/204]	Loss 0.4682 (0.4256)	
training:	Epoch: [96][179/204]	Loss 0.4612 (0.4258)	
training:	Epoch: [96][180/204]	Loss 0.3613 (0.4254)	
training:	Epoch: [96][181/204]	Loss 0.2589 (0.4245)	
training:	Epoch: [96][182/204]	Loss 0.3468 (0.4240)	
training:	Epoch: [96][183/204]	Loss 0.3965 (0.4239)	
training:	Epoch: [96][184/204]	Loss 0.2720 (0.4231)	
training:	Epoch: [96][185/204]	Loss 0.2033 (0.4219)	
training:	Epoch: [96][186/204]	Loss 0.3192 (0.4213)	
training:	Epoch: [96][187/204]	Loss 0.4216 (0.4213)	
training:	Epoch: [96][188/204]	Loss 0.4314 (0.4214)	
training:	Epoch: [96][189/204]	Loss 0.2447 (0.4205)	
training:	Epoch: [96][190/204]	Loss 0.4558 (0.4206)	
training:	Epoch: [96][191/204]	Loss 0.5501 (0.4213)	
training:	Epoch: [96][192/204]	Loss 0.6189 (0.4223)	
training:	Epoch: [96][193/204]	Loss 0.4319 (0.4224)	
training:	Epoch: [96][194/204]	Loss 0.4767 (0.4227)	
training:	Epoch: [96][195/204]	Loss 0.3642 (0.4224)	
training:	Epoch: [96][196/204]	Loss 0.4168 (0.4223)	
training:	Epoch: [96][197/204]	Loss 0.4229 (0.4223)	
training:	Epoch: [96][198/204]	Loss 0.3555 (0.4220)	
training:	Epoch: [96][199/204]	Loss 0.3803 (0.4218)	
training:	Epoch: [96][200/204]	Loss 0.5502 (0.4224)	
training:	Epoch: [96][201/204]	Loss 0.4450 (0.4226)	
training:	Epoch: [96][202/204]	Loss 0.5256 (0.4231)	
training:	Epoch: [96][203/204]	Loss 0.3545 (0.4227)	
training:	Epoch: [96][204/204]	Loss 0.3468 (0.4224)	
Training:	 Loss: 0.4217

Training:	 ACC: 0.8260 0.8266 0.8401 0.8119
Validation:	 ACC: 0.7973 0.7983 0.8199 0.7747
Validation:	 Best_BACC: 0.7973 0.7983 0.8199 0.7747
Validation:	 Loss: 0.4435
Pretraining:	Epoch 97/120
----------
training:	Epoch: [97][1/204]	Loss 0.4330 (0.4330)	
training:	Epoch: [97][2/204]	Loss 0.3039 (0.3684)	
training:	Epoch: [97][3/204]	Loss 0.3640 (0.3670)	
training:	Epoch: [97][4/204]	Loss 0.3799 (0.3702)	
training:	Epoch: [97][5/204]	Loss 0.4459 (0.3853)	
training:	Epoch: [97][6/204]	Loss 0.5168 (0.4073)	
training:	Epoch: [97][7/204]	Loss 0.4740 (0.4168)	
training:	Epoch: [97][8/204]	Loss 0.3952 (0.4141)	
training:	Epoch: [97][9/204]	Loss 0.5020 (0.4239)	
training:	Epoch: [97][10/204]	Loss 0.4033 (0.4218)	
training:	Epoch: [97][11/204]	Loss 0.4258 (0.4222)	
training:	Epoch: [97][12/204]	Loss 0.3951 (0.4199)	
training:	Epoch: [97][13/204]	Loss 0.3755 (0.4165)	
training:	Epoch: [97][14/204]	Loss 0.4912 (0.4218)	
training:	Epoch: [97][15/204]	Loss 0.3952 (0.4200)	
training:	Epoch: [97][16/204]	Loss 0.3774 (0.4174)	
training:	Epoch: [97][17/204]	Loss 0.6752 (0.4325)	
training:	Epoch: [97][18/204]	Loss 0.5358 (0.4383)	
training:	Epoch: [97][19/204]	Loss 0.3709 (0.4347)	
training:	Epoch: [97][20/204]	Loss 0.4800 (0.4370)	
training:	Epoch: [97][21/204]	Loss 0.3943 (0.4350)	
training:	Epoch: [97][22/204]	Loss 0.2716 (0.4275)	
training:	Epoch: [97][23/204]	Loss 0.3819 (0.4256)	
training:	Epoch: [97][24/204]	Loss 0.4801 (0.4278)	
training:	Epoch: [97][25/204]	Loss 0.4754 (0.4297)	
training:	Epoch: [97][26/204]	Loss 0.2288 (0.4220)	
training:	Epoch: [97][27/204]	Loss 0.3503 (0.4194)	
training:	Epoch: [97][28/204]	Loss 0.2991 (0.4151)	
training:	Epoch: [97][29/204]	Loss 0.4868 (0.4175)	
training:	Epoch: [97][30/204]	Loss 0.6609 (0.4256)	
training:	Epoch: [97][31/204]	Loss 0.5396 (0.4293)	
training:	Epoch: [97][32/204]	Loss 0.3958 (0.4283)	
training:	Epoch: [97][33/204]	Loss 0.5316 (0.4314)	
training:	Epoch: [97][34/204]	Loss 0.4469 (0.4319)	
training:	Epoch: [97][35/204]	Loss 0.4365 (0.4320)	
training:	Epoch: [97][36/204]	Loss 0.2833 (0.4279)	
training:	Epoch: [97][37/204]	Loss 0.4040 (0.4272)	
training:	Epoch: [97][38/204]	Loss 0.4523 (0.4279)	
training:	Epoch: [97][39/204]	Loss 0.3777 (0.4266)	
training:	Epoch: [97][40/204]	Loss 0.4653 (0.4276)	
training:	Epoch: [97][41/204]	Loss 0.3881 (0.4266)	
training:	Epoch: [97][42/204]	Loss 0.4958 (0.4282)	
training:	Epoch: [97][43/204]	Loss 0.4869 (0.4296)	
training:	Epoch: [97][44/204]	Loss 0.2584 (0.4257)	
training:	Epoch: [97][45/204]	Loss 0.3591 (0.4242)	
training:	Epoch: [97][46/204]	Loss 0.3125 (0.4218)	
training:	Epoch: [97][47/204]	Loss 0.5368 (0.4242)	
training:	Epoch: [97][48/204]	Loss 0.2572 (0.4208)	
training:	Epoch: [97][49/204]	Loss 0.3522 (0.4194)	
training:	Epoch: [97][50/204]	Loss 0.4789 (0.4206)	
training:	Epoch: [97][51/204]	Loss 0.4977 (0.4221)	
training:	Epoch: [97][52/204]	Loss 0.2680 (0.4191)	
training:	Epoch: [97][53/204]	Loss 0.4623 (0.4199)	
training:	Epoch: [97][54/204]	Loss 0.5060 (0.4215)	
training:	Epoch: [97][55/204]	Loss 0.3773 (0.4207)	
training:	Epoch: [97][56/204]	Loss 0.4132 (0.4206)	
training:	Epoch: [97][57/204]	Loss 0.4272 (0.4207)	
training:	Epoch: [97][58/204]	Loss 0.2907 (0.4185)	
training:	Epoch: [97][59/204]	Loss 0.5169 (0.4201)	
training:	Epoch: [97][60/204]	Loss 0.5848 (0.4229)	
training:	Epoch: [97][61/204]	Loss 0.5064 (0.4242)	
training:	Epoch: [97][62/204]	Loss 0.2412 (0.4213)	
training:	Epoch: [97][63/204]	Loss 0.4027 (0.4210)	
training:	Epoch: [97][64/204]	Loss 0.5555 (0.4231)	
training:	Epoch: [97][65/204]	Loss 0.4665 (0.4238)	
training:	Epoch: [97][66/204]	Loss 0.2743 (0.4215)	
training:	Epoch: [97][67/204]	Loss 0.6107 (0.4243)	
training:	Epoch: [97][68/204]	Loss 0.4673 (0.4250)	
training:	Epoch: [97][69/204]	Loss 0.3335 (0.4236)	
training:	Epoch: [97][70/204]	Loss 0.4937 (0.4246)	
training:	Epoch: [97][71/204]	Loss 0.5061 (0.4258)	
training:	Epoch: [97][72/204]	Loss 0.5968 (0.4282)	
training:	Epoch: [97][73/204]	Loss 0.4219 (0.4281)	
training:	Epoch: [97][74/204]	Loss 0.5849 (0.4302)	
training:	Epoch: [97][75/204]	Loss 0.3846 (0.4296)	
training:	Epoch: [97][76/204]	Loss 0.3115 (0.4280)	
training:	Epoch: [97][77/204]	Loss 0.3210 (0.4266)	
training:	Epoch: [97][78/204]	Loss 0.3380 (0.4255)	
training:	Epoch: [97][79/204]	Loss 0.3563 (0.4246)	
training:	Epoch: [97][80/204]	Loss 0.3772 (0.4240)	
training:	Epoch: [97][81/204]	Loss 0.4246 (0.4240)	
training:	Epoch: [97][82/204]	Loss 0.1736 (0.4210)	
training:	Epoch: [97][83/204]	Loss 0.3097 (0.4196)	
training:	Epoch: [97][84/204]	Loss 0.3230 (0.4185)	
training:	Epoch: [97][85/204]	Loss 0.3736 (0.4180)	
training:	Epoch: [97][86/204]	Loss 0.6150 (0.4203)	
training:	Epoch: [97][87/204]	Loss 0.3376 (0.4193)	
training:	Epoch: [97][88/204]	Loss 0.4026 (0.4191)	
training:	Epoch: [97][89/204]	Loss 0.4656 (0.4196)	
training:	Epoch: [97][90/204]	Loss 0.2965 (0.4183)	
training:	Epoch: [97][91/204]	Loss 0.3205 (0.4172)	
training:	Epoch: [97][92/204]	Loss 0.3244 (0.4162)	
training:	Epoch: [97][93/204]	Loss 0.4889 (0.4170)	
training:	Epoch: [97][94/204]	Loss 0.3824 (0.4166)	
training:	Epoch: [97][95/204]	Loss 0.3358 (0.4157)	
training:	Epoch: [97][96/204]	Loss 0.4669 (0.4163)	
training:	Epoch: [97][97/204]	Loss 0.5213 (0.4174)	
training:	Epoch: [97][98/204]	Loss 0.4280 (0.4175)	
training:	Epoch: [97][99/204]	Loss 0.2710 (0.4160)	
training:	Epoch: [97][100/204]	Loss 0.5119 (0.4170)	
training:	Epoch: [97][101/204]	Loss 0.3611 (0.4164)	
training:	Epoch: [97][102/204]	Loss 0.3343 (0.4156)	
training:	Epoch: [97][103/204]	Loss 0.4550 (0.4160)	
training:	Epoch: [97][104/204]	Loss 0.4510 (0.4163)	
training:	Epoch: [97][105/204]	Loss 0.5640 (0.4177)	
training:	Epoch: [97][106/204]	Loss 0.4575 (0.4181)	
training:	Epoch: [97][107/204]	Loss 0.5530 (0.4194)	
training:	Epoch: [97][108/204]	Loss 0.4261 (0.4194)	
training:	Epoch: [97][109/204]	Loss 0.3451 (0.4187)	
training:	Epoch: [97][110/204]	Loss 0.4111 (0.4187)	
training:	Epoch: [97][111/204]	Loss 0.3871 (0.4184)	
training:	Epoch: [97][112/204]	Loss 0.4012 (0.4182)	
training:	Epoch: [97][113/204]	Loss 0.3507 (0.4176)	
training:	Epoch: [97][114/204]	Loss 0.4728 (0.4181)	
training:	Epoch: [97][115/204]	Loss 0.3864 (0.4178)	
training:	Epoch: [97][116/204]	Loss 0.4006 (0.4177)	
training:	Epoch: [97][117/204]	Loss 0.4599 (0.4181)	
training:	Epoch: [97][118/204]	Loss 0.4819 (0.4186)	
training:	Epoch: [97][119/204]	Loss 0.5848 (0.4200)	
training:	Epoch: [97][120/204]	Loss 0.5143 (0.4208)	
training:	Epoch: [97][121/204]	Loss 0.4017 (0.4206)	
training:	Epoch: [97][122/204]	Loss 0.4064 (0.4205)	
training:	Epoch: [97][123/204]	Loss 0.3934 (0.4203)	
training:	Epoch: [97][124/204]	Loss 0.4456 (0.4205)	
training:	Epoch: [97][125/204]	Loss 0.5180 (0.4213)	
training:	Epoch: [97][126/204]	Loss 0.3736 (0.4209)	
training:	Epoch: [97][127/204]	Loss 0.3841 (0.4206)	
training:	Epoch: [97][128/204]	Loss 0.4616 (0.4209)	
training:	Epoch: [97][129/204]	Loss 0.3649 (0.4205)	
training:	Epoch: [97][130/204]	Loss 0.4023 (0.4203)	
training:	Epoch: [97][131/204]	Loss 0.4155 (0.4203)	
training:	Epoch: [97][132/204]	Loss 0.2829 (0.4193)	
training:	Epoch: [97][133/204]	Loss 0.4053 (0.4192)	
training:	Epoch: [97][134/204]	Loss 0.3925 (0.4190)	
training:	Epoch: [97][135/204]	Loss 0.4891 (0.4195)	
training:	Epoch: [97][136/204]	Loss 0.3182 (0.4187)	
training:	Epoch: [97][137/204]	Loss 0.3166 (0.4180)	
training:	Epoch: [97][138/204]	Loss 0.4719 (0.4184)	
training:	Epoch: [97][139/204]	Loss 0.4786 (0.4188)	
training:	Epoch: [97][140/204]	Loss 0.4375 (0.4189)	
training:	Epoch: [97][141/204]	Loss 0.5127 (0.4196)	
training:	Epoch: [97][142/204]	Loss 0.5831 (0.4208)	
training:	Epoch: [97][143/204]	Loss 0.3594 (0.4203)	
training:	Epoch: [97][144/204]	Loss 0.3660 (0.4200)	
training:	Epoch: [97][145/204]	Loss 0.7247 (0.4221)	
training:	Epoch: [97][146/204]	Loss 0.2395 (0.4208)	
training:	Epoch: [97][147/204]	Loss 0.4150 (0.4208)	
training:	Epoch: [97][148/204]	Loss 0.4114 (0.4207)	
training:	Epoch: [97][149/204]	Loss 0.3670 (0.4203)	
training:	Epoch: [97][150/204]	Loss 0.6672 (0.4220)	
training:	Epoch: [97][151/204]	Loss 0.4560 (0.4222)	
training:	Epoch: [97][152/204]	Loss 0.2770 (0.4213)	
training:	Epoch: [97][153/204]	Loss 0.4705 (0.4216)	
training:	Epoch: [97][154/204]	Loss 0.6914 (0.4233)	
training:	Epoch: [97][155/204]	Loss 0.4209 (0.4233)	
training:	Epoch: [97][156/204]	Loss 0.3547 (0.4229)	
training:	Epoch: [97][157/204]	Loss 0.3347 (0.4223)	
training:	Epoch: [97][158/204]	Loss 0.5480 (0.4231)	
training:	Epoch: [97][159/204]	Loss 0.4889 (0.4235)	
training:	Epoch: [97][160/204]	Loss 0.4487 (0.4237)	
training:	Epoch: [97][161/204]	Loss 0.2808 (0.4228)	
training:	Epoch: [97][162/204]	Loss 0.3409 (0.4223)	
training:	Epoch: [97][163/204]	Loss 0.4665 (0.4226)	
training:	Epoch: [97][164/204]	Loss 0.3630 (0.4222)	
training:	Epoch: [97][165/204]	Loss 0.4674 (0.4225)	
training:	Epoch: [97][166/204]	Loss 0.3028 (0.4218)	
training:	Epoch: [97][167/204]	Loss 0.4331 (0.4218)	
training:	Epoch: [97][168/204]	Loss 0.3847 (0.4216)	
training:	Epoch: [97][169/204]	Loss 0.4128 (0.4215)	
training:	Epoch: [97][170/204]	Loss 0.5035 (0.4220)	
training:	Epoch: [97][171/204]	Loss 0.4173 (0.4220)	
training:	Epoch: [97][172/204]	Loss 0.3290 (0.4215)	
training:	Epoch: [97][173/204]	Loss 0.3920 (0.4213)	
training:	Epoch: [97][174/204]	Loss 0.4059 (0.4212)	
training:	Epoch: [97][175/204]	Loss 0.2619 (0.4203)	
training:	Epoch: [97][176/204]	Loss 0.3924 (0.4201)	
training:	Epoch: [97][177/204]	Loss 0.3757 (0.4199)	
training:	Epoch: [97][178/204]	Loss 0.4604 (0.4201)	
training:	Epoch: [97][179/204]	Loss 0.5890 (0.4211)	
training:	Epoch: [97][180/204]	Loss 0.4757 (0.4214)	
training:	Epoch: [97][181/204]	Loss 0.4387 (0.4215)	
training:	Epoch: [97][182/204]	Loss 0.2608 (0.4206)	
training:	Epoch: [97][183/204]	Loss 0.4935 (0.4210)	
training:	Epoch: [97][184/204]	Loss 0.4992 (0.4214)	
training:	Epoch: [97][185/204]	Loss 0.3767 (0.4212)	
training:	Epoch: [97][186/204]	Loss 0.4577 (0.4213)	
training:	Epoch: [97][187/204]	Loss 0.5392 (0.4220)	
training:	Epoch: [97][188/204]	Loss 0.4260 (0.4220)	
training:	Epoch: [97][189/204]	Loss 0.2932 (0.4213)	
training:	Epoch: [97][190/204]	Loss 0.6379 (0.4225)	
training:	Epoch: [97][191/204]	Loss 0.4933 (0.4228)	
training:	Epoch: [97][192/204]	Loss 0.3558 (0.4225)	
training:	Epoch: [97][193/204]	Loss 0.3344 (0.4220)	
training:	Epoch: [97][194/204]	Loss 0.4965 (0.4224)	
training:	Epoch: [97][195/204]	Loss 0.4416 (0.4225)	
training:	Epoch: [97][196/204]	Loss 0.4567 (0.4227)	
training:	Epoch: [97][197/204]	Loss 0.4171 (0.4227)	
training:	Epoch: [97][198/204]	Loss 0.3135 (0.4221)	
training:	Epoch: [97][199/204]	Loss 0.4061 (0.4220)	
training:	Epoch: [97][200/204]	Loss 0.3535 (0.4217)	
training:	Epoch: [97][201/204]	Loss 0.3825 (0.4215)	
training:	Epoch: [97][202/204]	Loss 0.2726 (0.4207)	
training:	Epoch: [97][203/204]	Loss 0.4164 (0.4207)	
training:	Epoch: [97][204/204]	Loss 0.2097 (0.4197)	
Training:	 Loss: 0.4190

Training:	 ACC: 0.8271 0.8273 0.8327 0.8214
Validation:	 ACC: 0.7956 0.7961 0.8076 0.7836
Validation:	 Best_BACC: 0.7973 0.7983 0.8199 0.7747
Validation:	 Loss: 0.4426
Pretraining:	Epoch 98/120
----------
training:	Epoch: [98][1/204]	Loss 0.4537 (0.4537)	
training:	Epoch: [98][2/204]	Loss 0.5597 (0.5067)	
training:	Epoch: [98][3/204]	Loss 0.5412 (0.5182)	
training:	Epoch: [98][4/204]	Loss 0.3350 (0.4724)	
training:	Epoch: [98][5/204]	Loss 0.3946 (0.4568)	
training:	Epoch: [98][6/204]	Loss 0.4311 (0.4525)	
training:	Epoch: [98][7/204]	Loss 0.5109 (0.4609)	
training:	Epoch: [98][8/204]	Loss 0.3599 (0.4483)	
training:	Epoch: [98][9/204]	Loss 0.3847 (0.4412)	
training:	Epoch: [98][10/204]	Loss 0.2777 (0.4249)	
training:	Epoch: [98][11/204]	Loss 0.5368 (0.4350)	
training:	Epoch: [98][12/204]	Loss 0.5498 (0.4446)	
training:	Epoch: [98][13/204]	Loss 0.5100 (0.4496)	
training:	Epoch: [98][14/204]	Loss 0.3834 (0.4449)	
training:	Epoch: [98][15/204]	Loss 0.2881 (0.4344)	
training:	Epoch: [98][16/204]	Loss 0.4644 (0.4363)	
training:	Epoch: [98][17/204]	Loss 0.4721 (0.4384)	
training:	Epoch: [98][18/204]	Loss 0.6559 (0.4505)	
training:	Epoch: [98][19/204]	Loss 0.4355 (0.4497)	
training:	Epoch: [98][20/204]	Loss 0.3510 (0.4448)	
training:	Epoch: [98][21/204]	Loss 0.4971 (0.4473)	
training:	Epoch: [98][22/204]	Loss 0.5330 (0.4512)	
training:	Epoch: [98][23/204]	Loss 0.5680 (0.4562)	
training:	Epoch: [98][24/204]	Loss 0.5086 (0.4584)	
training:	Epoch: [98][25/204]	Loss 0.6216 (0.4649)	
training:	Epoch: [98][26/204]	Loss 0.3250 (0.4596)	
training:	Epoch: [98][27/204]	Loss 0.3136 (0.4542)	
training:	Epoch: [98][28/204]	Loss 0.3096 (0.4490)	
training:	Epoch: [98][29/204]	Loss 0.4435 (0.4488)	
training:	Epoch: [98][30/204]	Loss 0.3438 (0.4453)	
training:	Epoch: [98][31/204]	Loss 0.4264 (0.4447)	
training:	Epoch: [98][32/204]	Loss 0.5288 (0.4473)	
training:	Epoch: [98][33/204]	Loss 0.4345 (0.4469)	
training:	Epoch: [98][34/204]	Loss 0.4490 (0.4470)	
training:	Epoch: [98][35/204]	Loss 0.4579 (0.4473)	
training:	Epoch: [98][36/204]	Loss 0.2944 (0.4431)	
training:	Epoch: [98][37/204]	Loss 0.2473 (0.4378)	
training:	Epoch: [98][38/204]	Loss 0.5502 (0.4407)	
training:	Epoch: [98][39/204]	Loss 0.2988 (0.4371)	
training:	Epoch: [98][40/204]	Loss 0.5575 (0.4401)	
training:	Epoch: [98][41/204]	Loss 0.3259 (0.4373)	
training:	Epoch: [98][42/204]	Loss 0.2190 (0.4321)	
training:	Epoch: [98][43/204]	Loss 0.4009 (0.4314)	
training:	Epoch: [98][44/204]	Loss 0.4467 (0.4317)	
training:	Epoch: [98][45/204]	Loss 0.4236 (0.4316)	
training:	Epoch: [98][46/204]	Loss 0.3987 (0.4308)	
training:	Epoch: [98][47/204]	Loss 0.2650 (0.4273)	
training:	Epoch: [98][48/204]	Loss 0.2829 (0.4243)	
training:	Epoch: [98][49/204]	Loss 0.3961 (0.4237)	
training:	Epoch: [98][50/204]	Loss 0.3487 (0.4222)	
training:	Epoch: [98][51/204]	Loss 0.3738 (0.4213)	
training:	Epoch: [98][52/204]	Loss 0.5060 (0.4229)	
training:	Epoch: [98][53/204]	Loss 0.5098 (0.4245)	
training:	Epoch: [98][54/204]	Loss 0.3478 (0.4231)	
training:	Epoch: [98][55/204]	Loss 0.4335 (0.4233)	
training:	Epoch: [98][56/204]	Loss 0.4381 (0.4236)	
training:	Epoch: [98][57/204]	Loss 0.2755 (0.4210)	
training:	Epoch: [98][58/204]	Loss 0.4384 (0.4213)	
training:	Epoch: [98][59/204]	Loss 0.4393 (0.4216)	
training:	Epoch: [98][60/204]	Loss 0.3605 (0.4206)	
training:	Epoch: [98][61/204]	Loss 0.4645 (0.4213)	
training:	Epoch: [98][62/204]	Loss 0.4755 (0.4222)	
training:	Epoch: [98][63/204]	Loss 0.3638 (0.4212)	
training:	Epoch: [98][64/204]	Loss 0.5378 (0.4231)	
training:	Epoch: [98][65/204]	Loss 0.3324 (0.4217)	
training:	Epoch: [98][66/204]	Loss 0.4746 (0.4225)	
training:	Epoch: [98][67/204]	Loss 0.3990 (0.4221)	
training:	Epoch: [98][68/204]	Loss 0.3624 (0.4212)	
training:	Epoch: [98][69/204]	Loss 0.3737 (0.4205)	
training:	Epoch: [98][70/204]	Loss 0.3709 (0.4198)	
training:	Epoch: [98][71/204]	Loss 0.4088 (0.4197)	
training:	Epoch: [98][72/204]	Loss 0.3780 (0.4191)	
training:	Epoch: [98][73/204]	Loss 0.3951 (0.4188)	
training:	Epoch: [98][74/204]	Loss 0.3874 (0.4184)	
training:	Epoch: [98][75/204]	Loss 0.3698 (0.4177)	
training:	Epoch: [98][76/204]	Loss 0.4426 (0.4180)	
training:	Epoch: [98][77/204]	Loss 0.2671 (0.4161)	
training:	Epoch: [98][78/204]	Loss 0.4169 (0.4161)	
training:	Epoch: [98][79/204]	Loss 0.5499 (0.4178)	
training:	Epoch: [98][80/204]	Loss 0.4615 (0.4183)	
training:	Epoch: [98][81/204]	Loss 0.4633 (0.4189)	
training:	Epoch: [98][82/204]	Loss 0.3186 (0.4177)	
training:	Epoch: [98][83/204]	Loss 0.3806 (0.4172)	
training:	Epoch: [98][84/204]	Loss 0.3326 (0.4162)	
training:	Epoch: [98][85/204]	Loss 0.5641 (0.4179)	
training:	Epoch: [98][86/204]	Loss 0.4581 (0.4184)	
training:	Epoch: [98][87/204]	Loss 0.4064 (0.4183)	
training:	Epoch: [98][88/204]	Loss 0.4428 (0.4185)	
training:	Epoch: [98][89/204]	Loss 0.3928 (0.4183)	
training:	Epoch: [98][90/204]	Loss 0.4666 (0.4188)	
training:	Epoch: [98][91/204]	Loss 0.4527 (0.4192)	
training:	Epoch: [98][92/204]	Loss 0.4479 (0.4195)	
training:	Epoch: [98][93/204]	Loss 0.4209 (0.4195)	
training:	Epoch: [98][94/204]	Loss 0.4214 (0.4195)	
training:	Epoch: [98][95/204]	Loss 0.4374 (0.4197)	
training:	Epoch: [98][96/204]	Loss 0.3358 (0.4188)	
training:	Epoch: [98][97/204]	Loss 0.6359 (0.4211)	
training:	Epoch: [98][98/204]	Loss 0.4329 (0.4212)	
training:	Epoch: [98][99/204]	Loss 0.6257 (0.4233)	
training:	Epoch: [98][100/204]	Loss 0.3731 (0.4228)	
training:	Epoch: [98][101/204]	Loss 0.4007 (0.4225)	
training:	Epoch: [98][102/204]	Loss 0.3624 (0.4219)	
training:	Epoch: [98][103/204]	Loss 0.6622 (0.4243)	
training:	Epoch: [98][104/204]	Loss 0.3666 (0.4237)	
training:	Epoch: [98][105/204]	Loss 0.3079 (0.4226)	
training:	Epoch: [98][106/204]	Loss 0.3124 (0.4216)	
training:	Epoch: [98][107/204]	Loss 0.3365 (0.4208)	
training:	Epoch: [98][108/204]	Loss 0.4209 (0.4208)	
training:	Epoch: [98][109/204]	Loss 0.5347 (0.4218)	
training:	Epoch: [98][110/204]	Loss 0.4009 (0.4216)	
training:	Epoch: [98][111/204]	Loss 0.3801 (0.4213)	
training:	Epoch: [98][112/204]	Loss 0.4172 (0.4212)	
training:	Epoch: [98][113/204]	Loss 0.4936 (0.4219)	
training:	Epoch: [98][114/204]	Loss 0.2757 (0.4206)	
training:	Epoch: [98][115/204]	Loss 0.4238 (0.4206)	
training:	Epoch: [98][116/204]	Loss 0.4578 (0.4209)	
training:	Epoch: [98][117/204]	Loss 0.2245 (0.4193)	
training:	Epoch: [98][118/204]	Loss 0.5103 (0.4200)	
training:	Epoch: [98][119/204]	Loss 0.3484 (0.4194)	
training:	Epoch: [98][120/204]	Loss 0.3931 (0.4192)	
training:	Epoch: [98][121/204]	Loss 0.2488 (0.4178)	
training:	Epoch: [98][122/204]	Loss 0.3566 (0.4173)	
training:	Epoch: [98][123/204]	Loss 0.5723 (0.4186)	
training:	Epoch: [98][124/204]	Loss 0.3645 (0.4181)	
training:	Epoch: [98][125/204]	Loss 0.3104 (0.4173)	
training:	Epoch: [98][126/204]	Loss 0.2888 (0.4162)	
training:	Epoch: [98][127/204]	Loss 0.3686 (0.4159)	
training:	Epoch: [98][128/204]	Loss 0.3521 (0.4154)	
training:	Epoch: [98][129/204]	Loss 0.4817 (0.4159)	
training:	Epoch: [98][130/204]	Loss 0.4165 (0.4159)	
training:	Epoch: [98][131/204]	Loss 0.4467 (0.4161)	
training:	Epoch: [98][132/204]	Loss 0.4895 (0.4167)	
training:	Epoch: [98][133/204]	Loss 0.4176 (0.4167)	
training:	Epoch: [98][134/204]	Loss 0.2333 (0.4153)	
training:	Epoch: [98][135/204]	Loss 0.3533 (0.4149)	
training:	Epoch: [98][136/204]	Loss 0.2728 (0.4138)	
training:	Epoch: [98][137/204]	Loss 0.4050 (0.4137)	
training:	Epoch: [98][138/204]	Loss 0.3940 (0.4136)	
training:	Epoch: [98][139/204]	Loss 0.4063 (0.4136)	
training:	Epoch: [98][140/204]	Loss 0.4915 (0.4141)	
training:	Epoch: [98][141/204]	Loss 0.4148 (0.4141)	
training:	Epoch: [98][142/204]	Loss 0.4529 (0.4144)	
training:	Epoch: [98][143/204]	Loss 0.3722 (0.4141)	
training:	Epoch: [98][144/204]	Loss 0.5204 (0.4148)	
training:	Epoch: [98][145/204]	Loss 0.5220 (0.4156)	
training:	Epoch: [98][146/204]	Loss 0.3768 (0.4153)	
training:	Epoch: [98][147/204]	Loss 0.3919 (0.4151)	
training:	Epoch: [98][148/204]	Loss 0.4228 (0.4152)	
training:	Epoch: [98][149/204]	Loss 0.4462 (0.4154)	
training:	Epoch: [98][150/204]	Loss 0.6348 (0.4169)	
training:	Epoch: [98][151/204]	Loss 0.4517 (0.4171)	
training:	Epoch: [98][152/204]	Loss 0.5319 (0.4179)	
training:	Epoch: [98][153/204]	Loss 0.4373 (0.4180)	
training:	Epoch: [98][154/204]	Loss 0.4976 (0.4185)	
training:	Epoch: [98][155/204]	Loss 0.4686 (0.4188)	
training:	Epoch: [98][156/204]	Loss 0.4352 (0.4189)	
training:	Epoch: [98][157/204]	Loss 0.3525 (0.4185)	
training:	Epoch: [98][158/204]	Loss 0.3887 (0.4183)	
training:	Epoch: [98][159/204]	Loss 0.5309 (0.4190)	
training:	Epoch: [98][160/204]	Loss 0.3084 (0.4183)	
training:	Epoch: [98][161/204]	Loss 0.4626 (0.4186)	
training:	Epoch: [98][162/204]	Loss 0.4161 (0.4186)	
training:	Epoch: [98][163/204]	Loss 0.5445 (0.4194)	
training:	Epoch: [98][164/204]	Loss 0.3297 (0.4188)	
training:	Epoch: [98][165/204]	Loss 0.3906 (0.4186)	
training:	Epoch: [98][166/204]	Loss 0.5518 (0.4194)	
training:	Epoch: [98][167/204]	Loss 0.3711 (0.4192)	
training:	Epoch: [98][168/204]	Loss 0.3708 (0.4189)	
training:	Epoch: [98][169/204]	Loss 0.3936 (0.4187)	
training:	Epoch: [98][170/204]	Loss 0.4859 (0.4191)	
training:	Epoch: [98][171/204]	Loss 0.5041 (0.4196)	
training:	Epoch: [98][172/204]	Loss 0.4437 (0.4198)	
training:	Epoch: [98][173/204]	Loss 0.3444 (0.4193)	
training:	Epoch: [98][174/204]	Loss 0.6289 (0.4205)	
training:	Epoch: [98][175/204]	Loss 0.4675 (0.4208)	
training:	Epoch: [98][176/204]	Loss 0.3898 (0.4206)	
training:	Epoch: [98][177/204]	Loss 0.3920 (0.4205)	
training:	Epoch: [98][178/204]	Loss 0.4288 (0.4205)	
training:	Epoch: [98][179/204]	Loss 0.3856 (0.4203)	
training:	Epoch: [98][180/204]	Loss 0.3880 (0.4201)	
training:	Epoch: [98][181/204]	Loss 0.3712 (0.4199)	
training:	Epoch: [98][182/204]	Loss 0.3526 (0.4195)	
training:	Epoch: [98][183/204]	Loss 0.4328 (0.4196)	
training:	Epoch: [98][184/204]	Loss 0.2576 (0.4187)	
training:	Epoch: [98][185/204]	Loss 0.5249 (0.4193)	
training:	Epoch: [98][186/204]	Loss 0.5967 (0.4202)	
training:	Epoch: [98][187/204]	Loss 0.4262 (0.4202)	
training:	Epoch: [98][188/204]	Loss 0.4330 (0.4203)	
training:	Epoch: [98][189/204]	Loss 0.3099 (0.4197)	
training:	Epoch: [98][190/204]	Loss 0.4641 (0.4200)	
training:	Epoch: [98][191/204]	Loss 0.4218 (0.4200)	
training:	Epoch: [98][192/204]	Loss 0.4449 (0.4201)	
training:	Epoch: [98][193/204]	Loss 0.3825 (0.4199)	
training:	Epoch: [98][194/204]	Loss 0.4488 (0.4201)	
training:	Epoch: [98][195/204]	Loss 0.3789 (0.4198)	
training:	Epoch: [98][196/204]	Loss 0.5093 (0.4203)	
training:	Epoch: [98][197/204]	Loss 0.4535 (0.4205)	
training:	Epoch: [98][198/204]	Loss 0.4378 (0.4206)	
training:	Epoch: [98][199/204]	Loss 0.3056 (0.4200)	
training:	Epoch: [98][200/204]	Loss 0.4734 (0.4202)	
training:	Epoch: [98][201/204]	Loss 0.3446 (0.4199)	
training:	Epoch: [98][202/204]	Loss 0.3567 (0.4196)	
training:	Epoch: [98][203/204]	Loss 0.4448 (0.4197)	
training:	Epoch: [98][204/204]	Loss 0.3348 (0.4193)	
Training:	 Loss: 0.4186

Training:	 ACC: 0.8275 0.8282 0.8457 0.8093
Validation:	 ACC: 0.7965 0.7978 0.8240 0.7691
Validation:	 Best_BACC: 0.7973 0.7983 0.8199 0.7747
Validation:	 Loss: 0.4425
Pretraining:	Epoch 99/120
----------
training:	Epoch: [99][1/204]	Loss 0.4613 (0.4613)	
training:	Epoch: [99][2/204]	Loss 0.4199 (0.4406)	
training:	Epoch: [99][3/204]	Loss 0.3405 (0.4073)	
training:	Epoch: [99][4/204]	Loss 0.3948 (0.4041)	
training:	Epoch: [99][5/204]	Loss 0.3689 (0.3971)	
training:	Epoch: [99][6/204]	Loss 0.2879 (0.3789)	
training:	Epoch: [99][7/204]	Loss 0.5241 (0.3996)	
training:	Epoch: [99][8/204]	Loss 0.4694 (0.4084)	
training:	Epoch: [99][9/204]	Loss 0.3450 (0.4013)	
training:	Epoch: [99][10/204]	Loss 0.2755 (0.3887)	
training:	Epoch: [99][11/204]	Loss 0.3646 (0.3865)	
training:	Epoch: [99][12/204]	Loss 0.4198 (0.3893)	
training:	Epoch: [99][13/204]	Loss 0.3887 (0.3893)	
training:	Epoch: [99][14/204]	Loss 0.5496 (0.4007)	
training:	Epoch: [99][15/204]	Loss 0.4602 (0.4047)	
training:	Epoch: [99][16/204]	Loss 0.4584 (0.4080)	
training:	Epoch: [99][17/204]	Loss 0.4010 (0.4076)	
training:	Epoch: [99][18/204]	Loss 0.4737 (0.4113)	
training:	Epoch: [99][19/204]	Loss 0.3376 (0.4074)	
training:	Epoch: [99][20/204]	Loss 0.3577 (0.4049)	
training:	Epoch: [99][21/204]	Loss 0.4869 (0.4088)	
training:	Epoch: [99][22/204]	Loss 0.4159 (0.4092)	
training:	Epoch: [99][23/204]	Loss 0.5521 (0.4154)	
training:	Epoch: [99][24/204]	Loss 0.3099 (0.4110)	
training:	Epoch: [99][25/204]	Loss 0.5448 (0.4163)	
training:	Epoch: [99][26/204]	Loss 0.3781 (0.4149)	
training:	Epoch: [99][27/204]	Loss 0.3593 (0.4128)	
training:	Epoch: [99][28/204]	Loss 0.5096 (0.4163)	
training:	Epoch: [99][29/204]	Loss 0.3925 (0.4154)	
training:	Epoch: [99][30/204]	Loss 0.4911 (0.4180)	
training:	Epoch: [99][31/204]	Loss 0.5293 (0.4216)	
training:	Epoch: [99][32/204]	Loss 0.3316 (0.4187)	
training:	Epoch: [99][33/204]	Loss 0.4530 (0.4198)	
training:	Epoch: [99][34/204]	Loss 0.2880 (0.4159)	
training:	Epoch: [99][35/204]	Loss 0.3713 (0.4146)	
training:	Epoch: [99][36/204]	Loss 0.2757 (0.4108)	
training:	Epoch: [99][37/204]	Loss 0.3881 (0.4102)	
training:	Epoch: [99][38/204]	Loss 0.3656 (0.4090)	
training:	Epoch: [99][39/204]	Loss 0.6386 (0.4149)	
training:	Epoch: [99][40/204]	Loss 0.2329 (0.4103)	
training:	Epoch: [99][41/204]	Loss 0.2374 (0.4061)	
training:	Epoch: [99][42/204]	Loss 0.3325 (0.4044)	
training:	Epoch: [99][43/204]	Loss 0.3829 (0.4039)	
training:	Epoch: [99][44/204]	Loss 0.3187 (0.4019)	
training:	Epoch: [99][45/204]	Loss 0.5860 (0.4060)	
training:	Epoch: [99][46/204]	Loss 0.5856 (0.4099)	
training:	Epoch: [99][47/204]	Loss 0.4648 (0.4111)	
training:	Epoch: [99][48/204]	Loss 0.3728 (0.4103)	
training:	Epoch: [99][49/204]	Loss 0.6226 (0.4146)	
training:	Epoch: [99][50/204]	Loss 0.5574 (0.4175)	
training:	Epoch: [99][51/204]	Loss 0.6188 (0.4214)	
training:	Epoch: [99][52/204]	Loss 0.5627 (0.4241)	
training:	Epoch: [99][53/204]	Loss 0.5069 (0.4257)	
training:	Epoch: [99][54/204]	Loss 0.3973 (0.4252)	
training:	Epoch: [99][55/204]	Loss 0.5589 (0.4276)	
training:	Epoch: [99][56/204]	Loss 0.3979 (0.4271)	
training:	Epoch: [99][57/204]	Loss 0.4350 (0.4272)	
training:	Epoch: [99][58/204]	Loss 0.4770 (0.4281)	
training:	Epoch: [99][59/204]	Loss 0.2415 (0.4249)	
training:	Epoch: [99][60/204]	Loss 0.4818 (0.4259)	
training:	Epoch: [99][61/204]	Loss 0.4173 (0.4257)	
training:	Epoch: [99][62/204]	Loss 0.4096 (0.4255)	
training:	Epoch: [99][63/204]	Loss 0.2452 (0.4226)	
training:	Epoch: [99][64/204]	Loss 0.5348 (0.4244)	
training:	Epoch: [99][65/204]	Loss 0.4131 (0.4242)	
training:	Epoch: [99][66/204]	Loss 0.4048 (0.4239)	
training:	Epoch: [99][67/204]	Loss 0.5401 (0.4256)	
training:	Epoch: [99][68/204]	Loss 0.3389 (0.4243)	
training:	Epoch: [99][69/204]	Loss 0.2608 (0.4220)	
training:	Epoch: [99][70/204]	Loss 0.3248 (0.4206)	
training:	Epoch: [99][71/204]	Loss 0.4284 (0.4207)	
training:	Epoch: [99][72/204]	Loss 0.3811 (0.4201)	
training:	Epoch: [99][73/204]	Loss 0.3930 (0.4198)	
training:	Epoch: [99][74/204]	Loss 0.3427 (0.4187)	
training:	Epoch: [99][75/204]	Loss 0.4341 (0.4189)	
training:	Epoch: [99][76/204]	Loss 0.4857 (0.4198)	
training:	Epoch: [99][77/204]	Loss 0.4821 (0.4206)	
training:	Epoch: [99][78/204]	Loss 0.3720 (0.4200)	
training:	Epoch: [99][79/204]	Loss 0.2622 (0.4180)	
training:	Epoch: [99][80/204]	Loss 0.3331 (0.4169)	
training:	Epoch: [99][81/204]	Loss 0.5748 (0.4189)	
training:	Epoch: [99][82/204]	Loss 0.4076 (0.4188)	
training:	Epoch: [99][83/204]	Loss 0.4741 (0.4194)	
training:	Epoch: [99][84/204]	Loss 0.5670 (0.4212)	
training:	Epoch: [99][85/204]	Loss 0.4430 (0.4214)	
training:	Epoch: [99][86/204]	Loss 0.2937 (0.4200)	
training:	Epoch: [99][87/204]	Loss 0.5461 (0.4214)	
training:	Epoch: [99][88/204]	Loss 0.2586 (0.4196)	
training:	Epoch: [99][89/204]	Loss 0.4891 (0.4203)	
training:	Epoch: [99][90/204]	Loss 0.3864 (0.4200)	
training:	Epoch: [99][91/204]	Loss 0.4496 (0.4203)	
training:	Epoch: [99][92/204]	Loss 0.3860 (0.4199)	
training:	Epoch: [99][93/204]	Loss 0.2221 (0.4178)	
training:	Epoch: [99][94/204]	Loss 0.3260 (0.4168)	
training:	Epoch: [99][95/204]	Loss 0.4868 (0.4175)	
training:	Epoch: [99][96/204]	Loss 0.4234 (0.4176)	
training:	Epoch: [99][97/204]	Loss 0.5935 (0.4194)	
training:	Epoch: [99][98/204]	Loss 0.2954 (0.4182)	
training:	Epoch: [99][99/204]	Loss 0.4153 (0.4181)	
training:	Epoch: [99][100/204]	Loss 0.4462 (0.4184)	
training:	Epoch: [99][101/204]	Loss 0.5651 (0.4199)	
training:	Epoch: [99][102/204]	Loss 0.2863 (0.4185)	
training:	Epoch: [99][103/204]	Loss 0.4348 (0.4187)	
training:	Epoch: [99][104/204]	Loss 0.3494 (0.4180)	
training:	Epoch: [99][105/204]	Loss 0.3463 (0.4174)	
training:	Epoch: [99][106/204]	Loss 0.3705 (0.4169)	
training:	Epoch: [99][107/204]	Loss 0.4542 (0.4173)	
training:	Epoch: [99][108/204]	Loss 0.3451 (0.4166)	
training:	Epoch: [99][109/204]	Loss 0.3113 (0.4156)	
training:	Epoch: [99][110/204]	Loss 0.4484 (0.4159)	
training:	Epoch: [99][111/204]	Loss 0.4096 (0.4159)	
training:	Epoch: [99][112/204]	Loss 0.6512 (0.4180)	
training:	Epoch: [99][113/204]	Loss 0.3115 (0.4170)	
training:	Epoch: [99][114/204]	Loss 0.4245 (0.4171)	
training:	Epoch: [99][115/204]	Loss 0.4223 (0.4171)	
training:	Epoch: [99][116/204]	Loss 0.4741 (0.4176)	
training:	Epoch: [99][117/204]	Loss 0.5258 (0.4186)	
training:	Epoch: [99][118/204]	Loss 0.3162 (0.4177)	
training:	Epoch: [99][119/204]	Loss 0.4129 (0.4176)	
training:	Epoch: [99][120/204]	Loss 0.3940 (0.4174)	
training:	Epoch: [99][121/204]	Loss 0.3380 (0.4168)	
training:	Epoch: [99][122/204]	Loss 0.4132 (0.4168)	
training:	Epoch: [99][123/204]	Loss 0.4868 (0.4173)	
training:	Epoch: [99][124/204]	Loss 0.3059 (0.4164)	
training:	Epoch: [99][125/204]	Loss 0.4832 (0.4170)	
training:	Epoch: [99][126/204]	Loss 0.5270 (0.4178)	
training:	Epoch: [99][127/204]	Loss 0.4285 (0.4179)	
training:	Epoch: [99][128/204]	Loss 0.5855 (0.4192)	
training:	Epoch: [99][129/204]	Loss 0.4272 (0.4193)	
training:	Epoch: [99][130/204]	Loss 0.5664 (0.4204)	
training:	Epoch: [99][131/204]	Loss 0.3601 (0.4200)	
training:	Epoch: [99][132/204]	Loss 0.3379 (0.4193)	
training:	Epoch: [99][133/204]	Loss 0.3715 (0.4190)	
training:	Epoch: [99][134/204]	Loss 0.3777 (0.4187)	
training:	Epoch: [99][135/204]	Loss 0.5659 (0.4198)	
training:	Epoch: [99][136/204]	Loss 0.5041 (0.4204)	
training:	Epoch: [99][137/204]	Loss 0.2168 (0.4189)	
training:	Epoch: [99][138/204]	Loss 0.3888 (0.4187)	
training:	Epoch: [99][139/204]	Loss 0.5809 (0.4198)	
training:	Epoch: [99][140/204]	Loss 0.3419 (0.4193)	
training:	Epoch: [99][141/204]	Loss 0.3806 (0.4190)	
training:	Epoch: [99][142/204]	Loss 0.3957 (0.4189)	
training:	Epoch: [99][143/204]	Loss 0.3645 (0.4185)	
training:	Epoch: [99][144/204]	Loss 0.4991 (0.4190)	
training:	Epoch: [99][145/204]	Loss 0.3748 (0.4187)	
training:	Epoch: [99][146/204]	Loss 0.4291 (0.4188)	
training:	Epoch: [99][147/204]	Loss 0.4447 (0.4190)	
training:	Epoch: [99][148/204]	Loss 0.4807 (0.4194)	
training:	Epoch: [99][149/204]	Loss 0.5435 (0.4202)	
training:	Epoch: [99][150/204]	Loss 0.2680 (0.4192)	
training:	Epoch: [99][151/204]	Loss 0.3127 (0.4185)	
training:	Epoch: [99][152/204]	Loss 0.3894 (0.4183)	
training:	Epoch: [99][153/204]	Loss 0.3702 (0.4180)	
training:	Epoch: [99][154/204]	Loss 0.3303 (0.4174)	
training:	Epoch: [99][155/204]	Loss 0.4862 (0.4179)	
training:	Epoch: [99][156/204]	Loss 0.4912 (0.4183)	
training:	Epoch: [99][157/204]	Loss 0.2750 (0.4174)	
training:	Epoch: [99][158/204]	Loss 0.4358 (0.4175)	
training:	Epoch: [99][159/204]	Loss 0.2396 (0.4164)	
training:	Epoch: [99][160/204]	Loss 0.4260 (0.4165)	
training:	Epoch: [99][161/204]	Loss 0.3732 (0.4162)	
training:	Epoch: [99][162/204]	Loss 0.4197 (0.4162)	
training:	Epoch: [99][163/204]	Loss 0.5014 (0.4168)	
training:	Epoch: [99][164/204]	Loss 0.3626 (0.4164)	
training:	Epoch: [99][165/204]	Loss 0.3707 (0.4162)	
training:	Epoch: [99][166/204]	Loss 0.3871 (0.4160)	
training:	Epoch: [99][167/204]	Loss 0.4598 (0.4162)	
training:	Epoch: [99][168/204]	Loss 0.5907 (0.4173)	
training:	Epoch: [99][169/204]	Loss 0.3243 (0.4167)	
training:	Epoch: [99][170/204]	Loss 0.5314 (0.4174)	
training:	Epoch: [99][171/204]	Loss 0.3089 (0.4168)	
training:	Epoch: [99][172/204]	Loss 0.2656 (0.4159)	
training:	Epoch: [99][173/204]	Loss 0.6554 (0.4173)	
training:	Epoch: [99][174/204]	Loss 0.4119 (0.4172)	
training:	Epoch: [99][175/204]	Loss 0.3354 (0.4168)	
training:	Epoch: [99][176/204]	Loss 0.4166 (0.4168)	
training:	Epoch: [99][177/204]	Loss 0.3538 (0.4164)	
training:	Epoch: [99][178/204]	Loss 0.4314 (0.4165)	
training:	Epoch: [99][179/204]	Loss 0.3465 (0.4161)	
training:	Epoch: [99][180/204]	Loss 0.5326 (0.4168)	
training:	Epoch: [99][181/204]	Loss 0.4019 (0.4167)	
training:	Epoch: [99][182/204]	Loss 0.5131 (0.4172)	
training:	Epoch: [99][183/204]	Loss 0.4414 (0.4173)	
training:	Epoch: [99][184/204]	Loss 0.5351 (0.4180)	
training:	Epoch: [99][185/204]	Loss 0.3474 (0.4176)	
training:	Epoch: [99][186/204]	Loss 0.2435 (0.4167)	
training:	Epoch: [99][187/204]	Loss 0.4436 (0.4168)	
training:	Epoch: [99][188/204]	Loss 0.2847 (0.4161)	
training:	Epoch: [99][189/204]	Loss 0.3157 (0.4156)	
training:	Epoch: [99][190/204]	Loss 0.5455 (0.4163)	
training:	Epoch: [99][191/204]	Loss 0.4561 (0.4165)	
training:	Epoch: [99][192/204]	Loss 0.2741 (0.4157)	
training:	Epoch: [99][193/204]	Loss 0.6462 (0.4169)	
training:	Epoch: [99][194/204]	Loss 0.4748 (0.4172)	
training:	Epoch: [99][195/204]	Loss 0.5237 (0.4178)	
training:	Epoch: [99][196/204]	Loss 0.3224 (0.4173)	
training:	Epoch: [99][197/204]	Loss 0.2132 (0.4162)	
training:	Epoch: [99][198/204]	Loss 0.2261 (0.4153)	
training:	Epoch: [99][199/204]	Loss 0.4736 (0.4156)	
training:	Epoch: [99][200/204]	Loss 0.4903 (0.4159)	
training:	Epoch: [99][201/204]	Loss 0.4179 (0.4160)	
training:	Epoch: [99][202/204]	Loss 0.3858 (0.4158)	
training:	Epoch: [99][203/204]	Loss 0.4214 (0.4158)	
training:	Epoch: [99][204/204]	Loss 0.2965 (0.4152)	
Training:	 Loss: 0.4146

Training:	 ACC: 0.8287 0.8290 0.8357 0.8217
Validation:	 ACC: 0.7966 0.7972 0.8106 0.7825
Validation:	 Best_BACC: 0.7973 0.7983 0.8199 0.7747
Validation:	 Loss: 0.4415
Pretraining:	Epoch 100/120
----------
training:	Epoch: [100][1/204]	Loss 0.4610 (0.4610)	
training:	Epoch: [100][2/204]	Loss 0.4173 (0.4391)	
training:	Epoch: [100][3/204]	Loss 0.2865 (0.3883)	
training:	Epoch: [100][4/204]	Loss 0.3719 (0.3842)	
training:	Epoch: [100][5/204]	Loss 0.4047 (0.3883)	
training:	Epoch: [100][6/204]	Loss 0.5024 (0.4073)	
training:	Epoch: [100][7/204]	Loss 0.5252 (0.4241)	
training:	Epoch: [100][8/204]	Loss 0.4172 (0.4233)	
training:	Epoch: [100][9/204]	Loss 0.3847 (0.4190)	
training:	Epoch: [100][10/204]	Loss 0.3197 (0.4090)	
training:	Epoch: [100][11/204]	Loss 0.4268 (0.4107)	
training:	Epoch: [100][12/204]	Loss 0.3198 (0.4031)	
training:	Epoch: [100][13/204]	Loss 0.5332 (0.4131)	
training:	Epoch: [100][14/204]	Loss 0.3150 (0.4061)	
training:	Epoch: [100][15/204]	Loss 0.4657 (0.4101)	
training:	Epoch: [100][16/204]	Loss 0.3689 (0.4075)	
training:	Epoch: [100][17/204]	Loss 0.3895 (0.4064)	
training:	Epoch: [100][18/204]	Loss 0.4803 (0.4105)	
training:	Epoch: [100][19/204]	Loss 0.4493 (0.4126)	
training:	Epoch: [100][20/204]	Loss 0.6107 (0.4225)	
training:	Epoch: [100][21/204]	Loss 0.2600 (0.4147)	
training:	Epoch: [100][22/204]	Loss 0.3539 (0.4120)	
training:	Epoch: [100][23/204]	Loss 0.4053 (0.4117)	
training:	Epoch: [100][24/204]	Loss 0.3108 (0.4075)	
training:	Epoch: [100][25/204]	Loss 0.5016 (0.4112)	
training:	Epoch: [100][26/204]	Loss 0.6496 (0.4204)	
training:	Epoch: [100][27/204]	Loss 0.3201 (0.4167)	
training:	Epoch: [100][28/204]	Loss 0.4587 (0.4182)	
training:	Epoch: [100][29/204]	Loss 0.3871 (0.4171)	
training:	Epoch: [100][30/204]	Loss 0.5722 (0.4223)	
training:	Epoch: [100][31/204]	Loss 0.3837 (0.4210)	
training:	Epoch: [100][32/204]	Loss 0.4079 (0.4206)	
training:	Epoch: [100][33/204]	Loss 0.4300 (0.4209)	
training:	Epoch: [100][34/204]	Loss 0.4439 (0.4216)	
training:	Epoch: [100][35/204]	Loss 0.6000 (0.4267)	
training:	Epoch: [100][36/204]	Loss 0.4408 (0.4271)	
training:	Epoch: [100][37/204]	Loss 0.3420 (0.4248)	
training:	Epoch: [100][38/204]	Loss 0.3736 (0.4234)	
training:	Epoch: [100][39/204]	Loss 0.2223 (0.4183)	
training:	Epoch: [100][40/204]	Loss 0.4274 (0.4185)	
training:	Epoch: [100][41/204]	Loss 0.5107 (0.4208)	
training:	Epoch: [100][42/204]	Loss 0.4806 (0.4222)	
training:	Epoch: [100][43/204]	Loss 0.3764 (0.4211)	
training:	Epoch: [100][44/204]	Loss 0.3595 (0.4197)	
training:	Epoch: [100][45/204]	Loss 0.3683 (0.4186)	
training:	Epoch: [100][46/204]	Loss 0.4792 (0.4199)	
training:	Epoch: [100][47/204]	Loss 0.3566 (0.4186)	
training:	Epoch: [100][48/204]	Loss 0.2477 (0.4150)	
training:	Epoch: [100][49/204]	Loss 0.3513 (0.4137)	
training:	Epoch: [100][50/204]	Loss 0.4203 (0.4138)	
training:	Epoch: [100][51/204]	Loss 0.3187 (0.4120)	
training:	Epoch: [100][52/204]	Loss 0.5038 (0.4137)	
training:	Epoch: [100][53/204]	Loss 0.2665 (0.4110)	
training:	Epoch: [100][54/204]	Loss 0.5216 (0.4130)	
training:	Epoch: [100][55/204]	Loss 0.4441 (0.4136)	
training:	Epoch: [100][56/204]	Loss 0.4251 (0.4138)	
training:	Epoch: [100][57/204]	Loss 0.4024 (0.4136)	
training:	Epoch: [100][58/204]	Loss 0.3605 (0.4127)	
training:	Epoch: [100][59/204]	Loss 0.4444 (0.4132)	
training:	Epoch: [100][60/204]	Loss 0.3300 (0.4118)	
training:	Epoch: [100][61/204]	Loss 0.4381 (0.4122)	
training:	Epoch: [100][62/204]	Loss 0.3538 (0.4113)	
training:	Epoch: [100][63/204]	Loss 0.5281 (0.4132)	
training:	Epoch: [100][64/204]	Loss 0.3153 (0.4116)	
training:	Epoch: [100][65/204]	Loss 0.3090 (0.4100)	
training:	Epoch: [100][66/204]	Loss 0.4815 (0.4111)	
training:	Epoch: [100][67/204]	Loss 0.4464 (0.4117)	
training:	Epoch: [100][68/204]	Loss 0.5709 (0.4140)	
training:	Epoch: [100][69/204]	Loss 0.5195 (0.4155)	
training:	Epoch: [100][70/204]	Loss 0.5497 (0.4174)	
training:	Epoch: [100][71/204]	Loss 0.4141 (0.4174)	
training:	Epoch: [100][72/204]	Loss 0.5027 (0.4186)	
training:	Epoch: [100][73/204]	Loss 0.3759 (0.4180)	
training:	Epoch: [100][74/204]	Loss 0.4258 (0.4181)	
training:	Epoch: [100][75/204]	Loss 0.3516 (0.4172)	
training:	Epoch: [100][76/204]	Loss 0.4535 (0.4177)	
training:	Epoch: [100][77/204]	Loss 0.3059 (0.4162)	
training:	Epoch: [100][78/204]	Loss 0.3642 (0.4156)	
training:	Epoch: [100][79/204]	Loss 0.3928 (0.4153)	
training:	Epoch: [100][80/204]	Loss 0.5086 (0.4165)	
training:	Epoch: [100][81/204]	Loss 0.3241 (0.4153)	
training:	Epoch: [100][82/204]	Loss 0.4953 (0.4163)	
training:	Epoch: [100][83/204]	Loss 0.4524 (0.4167)	
training:	Epoch: [100][84/204]	Loss 0.3678 (0.4161)	
training:	Epoch: [100][85/204]	Loss 0.4613 (0.4167)	
training:	Epoch: [100][86/204]	Loss 0.2419 (0.4146)	
training:	Epoch: [100][87/204]	Loss 0.3960 (0.4144)	
training:	Epoch: [100][88/204]	Loss 0.5615 (0.4161)	
training:	Epoch: [100][89/204]	Loss 0.4506 (0.4165)	
training:	Epoch: [100][90/204]	Loss 0.4104 (0.4164)	
training:	Epoch: [100][91/204]	Loss 0.4158 (0.4164)	
training:	Epoch: [100][92/204]	Loss 0.3208 (0.4154)	
training:	Epoch: [100][93/204]	Loss 0.3081 (0.4142)	
training:	Epoch: [100][94/204]	Loss 0.3803 (0.4139)	
training:	Epoch: [100][95/204]	Loss 0.3841 (0.4135)	
training:	Epoch: [100][96/204]	Loss 0.3282 (0.4127)	
training:	Epoch: [100][97/204]	Loss 0.4573 (0.4131)	
training:	Epoch: [100][98/204]	Loss 0.4427 (0.4134)	
training:	Epoch: [100][99/204]	Loss 0.4433 (0.4137)	
training:	Epoch: [100][100/204]	Loss 0.4318 (0.4139)	
training:	Epoch: [100][101/204]	Loss 0.4772 (0.4145)	
training:	Epoch: [100][102/204]	Loss 0.2704 (0.4131)	
training:	Epoch: [100][103/204]	Loss 0.2415 (0.4114)	
training:	Epoch: [100][104/204]	Loss 0.3912 (0.4112)	
training:	Epoch: [100][105/204]	Loss 0.1793 (0.4090)	
training:	Epoch: [100][106/204]	Loss 0.4254 (0.4092)	
training:	Epoch: [100][107/204]	Loss 0.3957 (0.4091)	
training:	Epoch: [100][108/204]	Loss 0.6259 (0.4111)	
training:	Epoch: [100][109/204]	Loss 0.4360 (0.4113)	
training:	Epoch: [100][110/204]	Loss 0.5606 (0.4127)	
training:	Epoch: [100][111/204]	Loss 0.4946 (0.4134)	
training:	Epoch: [100][112/204]	Loss 0.5931 (0.4150)	
training:	Epoch: [100][113/204]	Loss 0.4053 (0.4149)	
training:	Epoch: [100][114/204]	Loss 0.1815 (0.4129)	
training:	Epoch: [100][115/204]	Loss 0.2916 (0.4118)	
training:	Epoch: [100][116/204]	Loss 0.5228 (0.4128)	
training:	Epoch: [100][117/204]	Loss 0.3491 (0.4122)	
training:	Epoch: [100][118/204]	Loss 0.3941 (0.4121)	
training:	Epoch: [100][119/204]	Loss 0.5297 (0.4131)	
training:	Epoch: [100][120/204]	Loss 0.4327 (0.4132)	
training:	Epoch: [100][121/204]	Loss 0.3787 (0.4129)	
training:	Epoch: [100][122/204]	Loss 0.4424 (0.4132)	
training:	Epoch: [100][123/204]	Loss 0.4602 (0.4136)	
training:	Epoch: [100][124/204]	Loss 0.3343 (0.4129)	
training:	Epoch: [100][125/204]	Loss 0.4521 (0.4132)	
training:	Epoch: [100][126/204]	Loss 0.6778 (0.4153)	
training:	Epoch: [100][127/204]	Loss 0.5402 (0.4163)	
training:	Epoch: [100][128/204]	Loss 0.4094 (0.4163)	
training:	Epoch: [100][129/204]	Loss 0.5079 (0.4170)	
training:	Epoch: [100][130/204]	Loss 0.3699 (0.4166)	
training:	Epoch: [100][131/204]	Loss 0.4723 (0.4170)	
training:	Epoch: [100][132/204]	Loss 0.2729 (0.4159)	
training:	Epoch: [100][133/204]	Loss 0.3018 (0.4151)	
training:	Epoch: [100][134/204]	Loss 0.5749 (0.4163)	
training:	Epoch: [100][135/204]	Loss 0.2944 (0.4154)	
training:	Epoch: [100][136/204]	Loss 0.4194 (0.4154)	
training:	Epoch: [100][137/204]	Loss 0.3641 (0.4150)	
training:	Epoch: [100][138/204]	Loss 0.4703 (0.4154)	
training:	Epoch: [100][139/204]	Loss 0.3999 (0.4153)	
training:	Epoch: [100][140/204]	Loss 0.3797 (0.4151)	
training:	Epoch: [100][141/204]	Loss 0.4514 (0.4153)	
training:	Epoch: [100][142/204]	Loss 0.3322 (0.4147)	
training:	Epoch: [100][143/204]	Loss 0.5084 (0.4154)	
training:	Epoch: [100][144/204]	Loss 0.4095 (0.4154)	
training:	Epoch: [100][145/204]	Loss 0.4101 (0.4153)	
training:	Epoch: [100][146/204]	Loss 0.4430 (0.4155)	
training:	Epoch: [100][147/204]	Loss 0.3240 (0.4149)	
training:	Epoch: [100][148/204]	Loss 0.4481 (0.4151)	
training:	Epoch: [100][149/204]	Loss 0.5744 (0.4162)	
training:	Epoch: [100][150/204]	Loss 0.2835 (0.4153)	
training:	Epoch: [100][151/204]	Loss 0.5051 (0.4159)	
training:	Epoch: [100][152/204]	Loss 0.3976 (0.4158)	
training:	Epoch: [100][153/204]	Loss 0.4123 (0.4157)	
training:	Epoch: [100][154/204]	Loss 0.4175 (0.4158)	
training:	Epoch: [100][155/204]	Loss 0.5687 (0.4167)	
training:	Epoch: [100][156/204]	Loss 0.4265 (0.4168)	
training:	Epoch: [100][157/204]	Loss 0.5345 (0.4176)	
training:	Epoch: [100][158/204]	Loss 0.3942 (0.4174)	
training:	Epoch: [100][159/204]	Loss 0.4579 (0.4177)	
training:	Epoch: [100][160/204]	Loss 0.5130 (0.4183)	
training:	Epoch: [100][161/204]	Loss 0.6015 (0.4194)	
training:	Epoch: [100][162/204]	Loss 0.3370 (0.4189)	
training:	Epoch: [100][163/204]	Loss 0.4456 (0.4191)	
training:	Epoch: [100][164/204]	Loss 0.3209 (0.4185)	
training:	Epoch: [100][165/204]	Loss 0.4818 (0.4188)	
training:	Epoch: [100][166/204]	Loss 0.5363 (0.4195)	
training:	Epoch: [100][167/204]	Loss 0.4453 (0.4197)	
training:	Epoch: [100][168/204]	Loss 0.2768 (0.4188)	
training:	Epoch: [100][169/204]	Loss 0.3528 (0.4185)	
training:	Epoch: [100][170/204]	Loss 0.3696 (0.4182)	
training:	Epoch: [100][171/204]	Loss 0.5128 (0.4187)	
training:	Epoch: [100][172/204]	Loss 0.4250 (0.4188)	
training:	Epoch: [100][173/204]	Loss 0.5339 (0.4194)	
training:	Epoch: [100][174/204]	Loss 0.6778 (0.4209)	
training:	Epoch: [100][175/204]	Loss 0.2739 (0.4201)	
training:	Epoch: [100][176/204]	Loss 0.3570 (0.4197)	
training:	Epoch: [100][177/204]	Loss 0.3017 (0.4190)	
training:	Epoch: [100][178/204]	Loss 0.3636 (0.4187)	
training:	Epoch: [100][179/204]	Loss 0.3654 (0.4184)	
training:	Epoch: [100][180/204]	Loss 0.3748 (0.4182)	
training:	Epoch: [100][181/204]	Loss 0.4381 (0.4183)	
training:	Epoch: [100][182/204]	Loss 0.3485 (0.4179)	
training:	Epoch: [100][183/204]	Loss 0.3330 (0.4175)	
training:	Epoch: [100][184/204]	Loss 0.2724 (0.4167)	
training:	Epoch: [100][185/204]	Loss 0.4672 (0.4169)	
training:	Epoch: [100][186/204]	Loss 0.5040 (0.4174)	
training:	Epoch: [100][187/204]	Loss 0.4305 (0.4175)	
training:	Epoch: [100][188/204]	Loss 0.5737 (0.4183)	
training:	Epoch: [100][189/204]	Loss 0.2919 (0.4176)	
training:	Epoch: [100][190/204]	Loss 0.4797 (0.4180)	
training:	Epoch: [100][191/204]	Loss 0.6245 (0.4190)	
training:	Epoch: [100][192/204]	Loss 0.4152 (0.4190)	
training:	Epoch: [100][193/204]	Loss 0.4871 (0.4194)	
training:	Epoch: [100][194/204]	Loss 0.2316 (0.4184)	
training:	Epoch: [100][195/204]	Loss 0.3062 (0.4178)	
training:	Epoch: [100][196/204]	Loss 0.3329 (0.4174)	
training:	Epoch: [100][197/204]	Loss 0.4821 (0.4177)	
training:	Epoch: [100][198/204]	Loss 0.4234 (0.4178)	
training:	Epoch: [100][199/204]	Loss 0.5277 (0.4183)	
training:	Epoch: [100][200/204]	Loss 0.2936 (0.4177)	
training:	Epoch: [100][201/204]	Loss 0.5769 (0.4185)	
training:	Epoch: [100][202/204]	Loss 0.3763 (0.4183)	
training:	Epoch: [100][203/204]	Loss 0.2727 (0.4176)	
training:	Epoch: [100][204/204]	Loss 0.3516 (0.4172)	
Training:	 Loss: 0.4166

Training:	 ACC: 0.8288 0.8295 0.8448 0.8128
Validation:	 ACC: 0.7966 0.7978 0.8219 0.7713
Validation:	 Best_BACC: 0.7973 0.7983 0.8199 0.7747
Validation:	 Loss: 0.4409
Pretraining:	Epoch 101/120
----------
training:	Epoch: [101][1/204]	Loss 0.3983 (0.3983)	
training:	Epoch: [101][2/204]	Loss 0.2990 (0.3486)	
training:	Epoch: [101][3/204]	Loss 0.4815 (0.3929)	
training:	Epoch: [101][4/204]	Loss 0.4132 (0.3980)	
training:	Epoch: [101][5/204]	Loss 0.4022 (0.3988)	
training:	Epoch: [101][6/204]	Loss 0.3493 (0.3906)	
training:	Epoch: [101][7/204]	Loss 0.3939 (0.3910)	
training:	Epoch: [101][8/204]	Loss 0.3246 (0.3827)	
training:	Epoch: [101][9/204]	Loss 0.3317 (0.3771)	
training:	Epoch: [101][10/204]	Loss 0.4095 (0.3803)	
training:	Epoch: [101][11/204]	Loss 0.4444 (0.3861)	
training:	Epoch: [101][12/204]	Loss 0.3594 (0.3839)	
training:	Epoch: [101][13/204]	Loss 0.5459 (0.3964)	
training:	Epoch: [101][14/204]	Loss 0.3034 (0.3897)	
training:	Epoch: [101][15/204]	Loss 0.4097 (0.3911)	
training:	Epoch: [101][16/204]	Loss 0.3464 (0.3883)	
training:	Epoch: [101][17/204]	Loss 0.2423 (0.3797)	
training:	Epoch: [101][18/204]	Loss 0.5429 (0.3887)	
training:	Epoch: [101][19/204]	Loss 0.4376 (0.3913)	
training:	Epoch: [101][20/204]	Loss 0.4797 (0.3957)	
training:	Epoch: [101][21/204]	Loss 0.4179 (0.3968)	
training:	Epoch: [101][22/204]	Loss 0.4006 (0.3970)	
training:	Epoch: [101][23/204]	Loss 0.4132 (0.3977)	
training:	Epoch: [101][24/204]	Loss 0.5466 (0.4039)	
training:	Epoch: [101][25/204]	Loss 0.5628 (0.4102)	
training:	Epoch: [101][26/204]	Loss 0.3999 (0.4098)	
training:	Epoch: [101][27/204]	Loss 0.2696 (0.4046)	
training:	Epoch: [101][28/204]	Loss 0.2866 (0.4004)	
training:	Epoch: [101][29/204]	Loss 0.4050 (0.4006)	
training:	Epoch: [101][30/204]	Loss 0.4087 (0.4009)	
training:	Epoch: [101][31/204]	Loss 0.3377 (0.3988)	
training:	Epoch: [101][32/204]	Loss 0.2676 (0.3947)	
training:	Epoch: [101][33/204]	Loss 0.4762 (0.3972)	
training:	Epoch: [101][34/204]	Loss 0.4788 (0.3996)	
training:	Epoch: [101][35/204]	Loss 0.5244 (0.4032)	
training:	Epoch: [101][36/204]	Loss 0.5620 (0.4076)	
training:	Epoch: [101][37/204]	Loss 0.5810 (0.4123)	
training:	Epoch: [101][38/204]	Loss 0.3822 (0.4115)	
training:	Epoch: [101][39/204]	Loss 0.3890 (0.4109)	
training:	Epoch: [101][40/204]	Loss 0.4377 (0.4116)	
training:	Epoch: [101][41/204]	Loss 0.3284 (0.4095)	
training:	Epoch: [101][42/204]	Loss 0.3947 (0.4092)	
training:	Epoch: [101][43/204]	Loss 0.2555 (0.4056)	
training:	Epoch: [101][44/204]	Loss 0.3840 (0.4051)	
training:	Epoch: [101][45/204]	Loss 0.3634 (0.4042)	
training:	Epoch: [101][46/204]	Loss 0.4693 (0.4056)	
training:	Epoch: [101][47/204]	Loss 0.5211 (0.4081)	
training:	Epoch: [101][48/204]	Loss 0.2833 (0.4055)	
training:	Epoch: [101][49/204]	Loss 0.3417 (0.4042)	
training:	Epoch: [101][50/204]	Loss 0.3177 (0.4024)	
training:	Epoch: [101][51/204]	Loss 0.4250 (0.4029)	
training:	Epoch: [101][52/204]	Loss 0.3785 (0.4024)	
training:	Epoch: [101][53/204]	Loss 0.5115 (0.4045)	
training:	Epoch: [101][54/204]	Loss 0.3413 (0.4033)	
training:	Epoch: [101][55/204]	Loss 0.6471 (0.4077)	
training:	Epoch: [101][56/204]	Loss 0.3519 (0.4067)	
training:	Epoch: [101][57/204]	Loss 0.4522 (0.4075)	
training:	Epoch: [101][58/204]	Loss 0.4186 (0.4077)	
training:	Epoch: [101][59/204]	Loss 0.4412 (0.4083)	
training:	Epoch: [101][60/204]	Loss 0.4405 (0.4088)	
training:	Epoch: [101][61/204]	Loss 0.2589 (0.4064)	
training:	Epoch: [101][62/204]	Loss 0.2491 (0.4038)	
training:	Epoch: [101][63/204]	Loss 0.3771 (0.4034)	
training:	Epoch: [101][64/204]	Loss 0.4259 (0.4038)	
training:	Epoch: [101][65/204]	Loss 0.5039 (0.4053)	
training:	Epoch: [101][66/204]	Loss 0.4626 (0.4062)	
training:	Epoch: [101][67/204]	Loss 0.4192 (0.4064)	
training:	Epoch: [101][68/204]	Loss 0.4034 (0.4063)	
training:	Epoch: [101][69/204]	Loss 0.2054 (0.4034)	
training:	Epoch: [101][70/204]	Loss 0.4148 (0.4036)	
training:	Epoch: [101][71/204]	Loss 0.4284 (0.4039)	
training:	Epoch: [101][72/204]	Loss 0.3494 (0.4032)	
training:	Epoch: [101][73/204]	Loss 0.3714 (0.4027)	
training:	Epoch: [101][74/204]	Loss 0.4764 (0.4037)	
training:	Epoch: [101][75/204]	Loss 0.2129 (0.4012)	
training:	Epoch: [101][76/204]	Loss 0.5335 (0.4029)	
training:	Epoch: [101][77/204]	Loss 0.6413 (0.4060)	
training:	Epoch: [101][78/204]	Loss 0.4066 (0.4060)	
training:	Epoch: [101][79/204]	Loss 0.3640 (0.4055)	
training:	Epoch: [101][80/204]	Loss 0.3422 (0.4047)	
training:	Epoch: [101][81/204]	Loss 0.3721 (0.4043)	
training:	Epoch: [101][82/204]	Loss 0.4520 (0.4049)	
training:	Epoch: [101][83/204]	Loss 0.2474 (0.4030)	
training:	Epoch: [101][84/204]	Loss 0.4983 (0.4041)	
training:	Epoch: [101][85/204]	Loss 0.4870 (0.4051)	
training:	Epoch: [101][86/204]	Loss 0.5161 (0.4064)	
training:	Epoch: [101][87/204]	Loss 0.4363 (0.4067)	
training:	Epoch: [101][88/204]	Loss 0.4918 (0.4077)	
training:	Epoch: [101][89/204]	Loss 0.4542 (0.4082)	
training:	Epoch: [101][90/204]	Loss 0.5132 (0.4094)	
training:	Epoch: [101][91/204]	Loss 0.4551 (0.4099)	
training:	Epoch: [101][92/204]	Loss 0.3994 (0.4098)	
training:	Epoch: [101][93/204]	Loss 0.4567 (0.4103)	
training:	Epoch: [101][94/204]	Loss 0.3001 (0.4091)	
training:	Epoch: [101][95/204]	Loss 0.3183 (0.4081)	
training:	Epoch: [101][96/204]	Loss 0.5259 (0.4094)	
training:	Epoch: [101][97/204]	Loss 0.5428 (0.4107)	
training:	Epoch: [101][98/204]	Loss 0.3350 (0.4100)	
training:	Epoch: [101][99/204]	Loss 0.4558 (0.4104)	
training:	Epoch: [101][100/204]	Loss 0.3459 (0.4098)	
training:	Epoch: [101][101/204]	Loss 0.4295 (0.4100)	
training:	Epoch: [101][102/204]	Loss 0.5032 (0.4109)	
training:	Epoch: [101][103/204]	Loss 0.3084 (0.4099)	
training:	Epoch: [101][104/204]	Loss 0.5043 (0.4108)	
training:	Epoch: [101][105/204]	Loss 0.4100 (0.4108)	
training:	Epoch: [101][106/204]	Loss 0.3724 (0.4104)	
training:	Epoch: [101][107/204]	Loss 0.3602 (0.4100)	
training:	Epoch: [101][108/204]	Loss 0.4220 (0.4101)	
training:	Epoch: [101][109/204]	Loss 0.5468 (0.4113)	
training:	Epoch: [101][110/204]	Loss 0.4492 (0.4117)	
training:	Epoch: [101][111/204]	Loss 0.6258 (0.4136)	
training:	Epoch: [101][112/204]	Loss 0.5724 (0.4150)	
training:	Epoch: [101][113/204]	Loss 0.5092 (0.4159)	
training:	Epoch: [101][114/204]	Loss 0.3009 (0.4149)	
training:	Epoch: [101][115/204]	Loss 0.3733 (0.4145)	
training:	Epoch: [101][116/204]	Loss 0.4402 (0.4147)	
training:	Epoch: [101][117/204]	Loss 0.5973 (0.4163)	
training:	Epoch: [101][118/204]	Loss 0.5043 (0.4170)	
training:	Epoch: [101][119/204]	Loss 0.4032 (0.4169)	
training:	Epoch: [101][120/204]	Loss 0.5963 (0.4184)	
training:	Epoch: [101][121/204]	Loss 0.2990 (0.4174)	
training:	Epoch: [101][122/204]	Loss 0.5328 (0.4184)	
training:	Epoch: [101][123/204]	Loss 0.4780 (0.4188)	
training:	Epoch: [101][124/204]	Loss 0.3077 (0.4179)	
training:	Epoch: [101][125/204]	Loss 0.3094 (0.4171)	
training:	Epoch: [101][126/204]	Loss 0.4177 (0.4171)	
training:	Epoch: [101][127/204]	Loss 0.4693 (0.4175)	
training:	Epoch: [101][128/204]	Loss 0.3706 (0.4171)	
training:	Epoch: [101][129/204]	Loss 0.3661 (0.4167)	
training:	Epoch: [101][130/204]	Loss 0.3264 (0.4160)	
training:	Epoch: [101][131/204]	Loss 0.4191 (0.4161)	
training:	Epoch: [101][132/204]	Loss 0.4597 (0.4164)	
training:	Epoch: [101][133/204]	Loss 0.3577 (0.4160)	
training:	Epoch: [101][134/204]	Loss 0.4185 (0.4160)	
training:	Epoch: [101][135/204]	Loss 0.2958 (0.4151)	
training:	Epoch: [101][136/204]	Loss 0.5342 (0.4160)	
training:	Epoch: [101][137/204]	Loss 0.4245 (0.4160)	
training:	Epoch: [101][138/204]	Loss 0.4349 (0.4162)	
training:	Epoch: [101][139/204]	Loss 0.4202 (0.4162)	
training:	Epoch: [101][140/204]	Loss 0.3744 (0.4159)	
training:	Epoch: [101][141/204]	Loss 0.5366 (0.4167)	
training:	Epoch: [101][142/204]	Loss 0.4021 (0.4166)	
training:	Epoch: [101][143/204]	Loss 0.3034 (0.4158)	
training:	Epoch: [101][144/204]	Loss 0.5799 (0.4170)	
training:	Epoch: [101][145/204]	Loss 0.3777 (0.4167)	
training:	Epoch: [101][146/204]	Loss 0.3516 (0.4163)	
training:	Epoch: [101][147/204]	Loss 0.4113 (0.4162)	
training:	Epoch: [101][148/204]	Loss 0.3511 (0.4158)	
training:	Epoch: [101][149/204]	Loss 0.3148 (0.4151)	
training:	Epoch: [101][150/204]	Loss 0.5268 (0.4159)	
training:	Epoch: [101][151/204]	Loss 0.4502 (0.4161)	
training:	Epoch: [101][152/204]	Loss 0.5089 (0.4167)	
training:	Epoch: [101][153/204]	Loss 0.3009 (0.4159)	
training:	Epoch: [101][154/204]	Loss 0.4004 (0.4158)	
training:	Epoch: [101][155/204]	Loss 0.6517 (0.4174)	
training:	Epoch: [101][156/204]	Loss 0.3039 (0.4166)	
training:	Epoch: [101][157/204]	Loss 0.3559 (0.4163)	
training:	Epoch: [101][158/204]	Loss 0.3756 (0.4160)	
training:	Epoch: [101][159/204]	Loss 0.4717 (0.4163)	
training:	Epoch: [101][160/204]	Loss 0.4800 (0.4167)	
training:	Epoch: [101][161/204]	Loss 0.3367 (0.4162)	
training:	Epoch: [101][162/204]	Loss 0.3865 (0.4161)	
training:	Epoch: [101][163/204]	Loss 0.4958 (0.4166)	
training:	Epoch: [101][164/204]	Loss 0.2641 (0.4156)	
training:	Epoch: [101][165/204]	Loss 0.3756 (0.4154)	
training:	Epoch: [101][166/204]	Loss 0.3629 (0.4151)	
training:	Epoch: [101][167/204]	Loss 0.4158 (0.4151)	
training:	Epoch: [101][168/204]	Loss 0.3655 (0.4148)	
training:	Epoch: [101][169/204]	Loss 0.4989 (0.4153)	
training:	Epoch: [101][170/204]	Loss 0.3329 (0.4148)	
training:	Epoch: [101][171/204]	Loss 0.4028 (0.4147)	
training:	Epoch: [101][172/204]	Loss 0.5455 (0.4155)	
training:	Epoch: [101][173/204]	Loss 0.3611 (0.4152)	
training:	Epoch: [101][174/204]	Loss 0.2520 (0.4142)	
training:	Epoch: [101][175/204]	Loss 0.3746 (0.4140)	
training:	Epoch: [101][176/204]	Loss 0.3639 (0.4137)	
training:	Epoch: [101][177/204]	Loss 0.3553 (0.4134)	
training:	Epoch: [101][178/204]	Loss 0.4648 (0.4137)	
training:	Epoch: [101][179/204]	Loss 0.3261 (0.4132)	
training:	Epoch: [101][180/204]	Loss 0.3368 (0.4128)	
training:	Epoch: [101][181/204]	Loss 0.3978 (0.4127)	
training:	Epoch: [101][182/204]	Loss 0.3474 (0.4123)	
training:	Epoch: [101][183/204]	Loss 0.5628 (0.4131)	
training:	Epoch: [101][184/204]	Loss 0.5318 (0.4138)	
training:	Epoch: [101][185/204]	Loss 0.4140 (0.4138)	
training:	Epoch: [101][186/204]	Loss 0.3358 (0.4134)	
training:	Epoch: [101][187/204]	Loss 0.3654 (0.4131)	
training:	Epoch: [101][188/204]	Loss 0.5676 (0.4139)	
training:	Epoch: [101][189/204]	Loss 0.3036 (0.4133)	
training:	Epoch: [101][190/204]	Loss 0.3805 (0.4132)	
training:	Epoch: [101][191/204]	Loss 0.4475 (0.4134)	
training:	Epoch: [101][192/204]	Loss 0.4930 (0.4138)	
training:	Epoch: [101][193/204]	Loss 0.4874 (0.4141)	
training:	Epoch: [101][194/204]	Loss 0.3651 (0.4139)	
training:	Epoch: [101][195/204]	Loss 0.4541 (0.4141)	
training:	Epoch: [101][196/204]	Loss 0.4796 (0.4144)	
training:	Epoch: [101][197/204]	Loss 0.2688 (0.4137)	
training:	Epoch: [101][198/204]	Loss 0.4069 (0.4137)	
training:	Epoch: [101][199/204]	Loss 0.5358 (0.4143)	
training:	Epoch: [101][200/204]	Loss 0.3209 (0.4138)	
training:	Epoch: [101][201/204]	Loss 0.4514 (0.4140)	
training:	Epoch: [101][202/204]	Loss 0.4995 (0.4144)	
training:	Epoch: [101][203/204]	Loss 0.3501 (0.4141)	
training:	Epoch: [101][204/204]	Loss 0.5242 (0.4146)	
Training:	 Loss: 0.4140

Training:	 ACC: 0.8286 0.8293 0.8469 0.8103
Validation:	 ACC: 0.7949 0.7961 0.8219 0.7679
Validation:	 Best_BACC: 0.7973 0.7983 0.8199 0.7747
Validation:	 Loss: 0.4404
Pretraining:	Epoch 102/120
----------
training:	Epoch: [102][1/204]	Loss 0.2901 (0.2901)	
training:	Epoch: [102][2/204]	Loss 0.5869 (0.4385)	
training:	Epoch: [102][3/204]	Loss 0.3029 (0.3933)	
training:	Epoch: [102][4/204]	Loss 0.4551 (0.4088)	
training:	Epoch: [102][5/204]	Loss 0.4544 (0.4179)	
training:	Epoch: [102][6/204]	Loss 0.3637 (0.4088)	
training:	Epoch: [102][7/204]	Loss 0.2829 (0.3909)	
training:	Epoch: [102][8/204]	Loss 0.4133 (0.3937)	
training:	Epoch: [102][9/204]	Loss 0.4387 (0.3987)	
training:	Epoch: [102][10/204]	Loss 0.5984 (0.4186)	
training:	Epoch: [102][11/204]	Loss 0.3773 (0.4149)	
training:	Epoch: [102][12/204]	Loss 0.4806 (0.4204)	
training:	Epoch: [102][13/204]	Loss 0.3122 (0.4120)	
training:	Epoch: [102][14/204]	Loss 0.3508 (0.4077)	
training:	Epoch: [102][15/204]	Loss 0.5688 (0.4184)	
training:	Epoch: [102][16/204]	Loss 0.2644 (0.4088)	
training:	Epoch: [102][17/204]	Loss 0.3731 (0.4067)	
training:	Epoch: [102][18/204]	Loss 0.2843 (0.3999)	
training:	Epoch: [102][19/204]	Loss 0.4026 (0.4000)	
training:	Epoch: [102][20/204]	Loss 0.4051 (0.4003)	
training:	Epoch: [102][21/204]	Loss 0.3775 (0.3992)	
training:	Epoch: [102][22/204]	Loss 0.5163 (0.4045)	
training:	Epoch: [102][23/204]	Loss 0.4083 (0.4047)	
training:	Epoch: [102][24/204]	Loss 0.3534 (0.4026)	
training:	Epoch: [102][25/204]	Loss 0.3940 (0.4022)	
training:	Epoch: [102][26/204]	Loss 0.3729 (0.4011)	
training:	Epoch: [102][27/204]	Loss 0.3780 (0.4002)	
training:	Epoch: [102][28/204]	Loss 0.4190 (0.4009)	
training:	Epoch: [102][29/204]	Loss 0.4884 (0.4039)	
training:	Epoch: [102][30/204]	Loss 0.3890 (0.4034)	
training:	Epoch: [102][31/204]	Loss 0.4415 (0.4046)	
training:	Epoch: [102][32/204]	Loss 0.3956 (0.4044)	
training:	Epoch: [102][33/204]	Loss 0.2519 (0.3997)	
training:	Epoch: [102][34/204]	Loss 0.4307 (0.4007)	
training:	Epoch: [102][35/204]	Loss 0.4525 (0.4021)	
training:	Epoch: [102][36/204]	Loss 0.4738 (0.4041)	
training:	Epoch: [102][37/204]	Loss 0.5813 (0.4089)	
training:	Epoch: [102][38/204]	Loss 0.4720 (0.4106)	
training:	Epoch: [102][39/204]	Loss 0.2633 (0.4068)	
training:	Epoch: [102][40/204]	Loss 0.4979 (0.4091)	
training:	Epoch: [102][41/204]	Loss 0.4412 (0.4099)	
training:	Epoch: [102][42/204]	Loss 0.4065 (0.4098)	
training:	Epoch: [102][43/204]	Loss 0.5393 (0.4128)	
training:	Epoch: [102][44/204]	Loss 0.2389 (0.4088)	
training:	Epoch: [102][45/204]	Loss 0.3577 (0.4077)	
training:	Epoch: [102][46/204]	Loss 0.5182 (0.4101)	
training:	Epoch: [102][47/204]	Loss 0.3169 (0.4081)	
training:	Epoch: [102][48/204]	Loss 0.4274 (0.4085)	
training:	Epoch: [102][49/204]	Loss 0.3993 (0.4083)	
training:	Epoch: [102][50/204]	Loss 0.6664 (0.4135)	
training:	Epoch: [102][51/204]	Loss 0.4354 (0.4139)	
training:	Epoch: [102][52/204]	Loss 0.4395 (0.4144)	
training:	Epoch: [102][53/204]	Loss 0.5081 (0.4162)	
training:	Epoch: [102][54/204]	Loss 0.3835 (0.4156)	
training:	Epoch: [102][55/204]	Loss 0.4256 (0.4158)	
training:	Epoch: [102][56/204]	Loss 0.5616 (0.4184)	
training:	Epoch: [102][57/204]	Loss 0.3762 (0.4176)	
training:	Epoch: [102][58/204]	Loss 0.3776 (0.4169)	
training:	Epoch: [102][59/204]	Loss 0.4462 (0.4174)	
training:	Epoch: [102][60/204]	Loss 0.3489 (0.4163)	
training:	Epoch: [102][61/204]	Loss 0.3754 (0.4156)	
training:	Epoch: [102][62/204]	Loss 0.4336 (0.4159)	
training:	Epoch: [102][63/204]	Loss 0.2446 (0.4132)	
training:	Epoch: [102][64/204]	Loss 0.2903 (0.4113)	
training:	Epoch: [102][65/204]	Loss 0.3872 (0.4109)	
training:	Epoch: [102][66/204]	Loss 0.3259 (0.4096)	
training:	Epoch: [102][67/204]	Loss 0.4045 (0.4095)	
training:	Epoch: [102][68/204]	Loss 0.2826 (0.4077)	
training:	Epoch: [102][69/204]	Loss 0.5926 (0.4103)	
training:	Epoch: [102][70/204]	Loss 0.2779 (0.4085)	
training:	Epoch: [102][71/204]	Loss 0.4676 (0.4093)	
training:	Epoch: [102][72/204]	Loss 0.5260 (0.4109)	
training:	Epoch: [102][73/204]	Loss 0.6432 (0.4141)	
training:	Epoch: [102][74/204]	Loss 0.3952 (0.4138)	
training:	Epoch: [102][75/204]	Loss 0.3015 (0.4123)	
training:	Epoch: [102][76/204]	Loss 0.5432 (0.4141)	
training:	Epoch: [102][77/204]	Loss 0.2944 (0.4125)	
training:	Epoch: [102][78/204]	Loss 0.3992 (0.4123)	
training:	Epoch: [102][79/204]	Loss 0.2469 (0.4102)	
training:	Epoch: [102][80/204]	Loss 0.3602 (0.4096)	
training:	Epoch: [102][81/204]	Loss 0.3233 (0.4086)	
training:	Epoch: [102][82/204]	Loss 0.4051 (0.4085)	
training:	Epoch: [102][83/204]	Loss 0.5184 (0.4098)	
training:	Epoch: [102][84/204]	Loss 0.3943 (0.4096)	
training:	Epoch: [102][85/204]	Loss 0.4683 (0.4103)	
training:	Epoch: [102][86/204]	Loss 0.3339 (0.4094)	
training:	Epoch: [102][87/204]	Loss 0.4332 (0.4097)	
training:	Epoch: [102][88/204]	Loss 0.2934 (0.4084)	
training:	Epoch: [102][89/204]	Loss 0.2695 (0.4068)	
training:	Epoch: [102][90/204]	Loss 0.6428 (0.4095)	
training:	Epoch: [102][91/204]	Loss 0.4183 (0.4096)	
training:	Epoch: [102][92/204]	Loss 0.3910 (0.4094)	
training:	Epoch: [102][93/204]	Loss 0.4704 (0.4100)	
training:	Epoch: [102][94/204]	Loss 0.2263 (0.4081)	
training:	Epoch: [102][95/204]	Loss 0.3824 (0.4078)	
training:	Epoch: [102][96/204]	Loss 0.3215 (0.4069)	
training:	Epoch: [102][97/204]	Loss 0.3603 (0.4064)	
training:	Epoch: [102][98/204]	Loss 0.3298 (0.4056)	
training:	Epoch: [102][99/204]	Loss 0.5485 (0.4071)	
training:	Epoch: [102][100/204]	Loss 0.5625 (0.4086)	
training:	Epoch: [102][101/204]	Loss 0.5081 (0.4096)	
training:	Epoch: [102][102/204]	Loss 0.3462 (0.4090)	
training:	Epoch: [102][103/204]	Loss 0.3530 (0.4084)	
training:	Epoch: [102][104/204]	Loss 0.3127 (0.4075)	
training:	Epoch: [102][105/204]	Loss 0.3541 (0.4070)	
training:	Epoch: [102][106/204]	Loss 0.4187 (0.4071)	
training:	Epoch: [102][107/204]	Loss 0.2238 (0.4054)	
training:	Epoch: [102][108/204]	Loss 0.3643 (0.4050)	
training:	Epoch: [102][109/204]	Loss 0.4691 (0.4056)	
training:	Epoch: [102][110/204]	Loss 0.4158 (0.4057)	
training:	Epoch: [102][111/204]	Loss 0.5222 (0.4068)	
training:	Epoch: [102][112/204]	Loss 0.3591 (0.4063)	
training:	Epoch: [102][113/204]	Loss 0.4096 (0.4064)	
training:	Epoch: [102][114/204]	Loss 0.6713 (0.4087)	
training:	Epoch: [102][115/204]	Loss 0.4557 (0.4091)	
training:	Epoch: [102][116/204]	Loss 0.4870 (0.4098)	
training:	Epoch: [102][117/204]	Loss 0.4510 (0.4101)	
training:	Epoch: [102][118/204]	Loss 0.4231 (0.4102)	
training:	Epoch: [102][119/204]	Loss 0.4164 (0.4103)	
training:	Epoch: [102][120/204]	Loss 0.4511 (0.4106)	
training:	Epoch: [102][121/204]	Loss 0.4335 (0.4108)	
training:	Epoch: [102][122/204]	Loss 0.4542 (0.4112)	
training:	Epoch: [102][123/204]	Loss 0.5611 (0.4124)	
training:	Epoch: [102][124/204]	Loss 0.4155 (0.4124)	
training:	Epoch: [102][125/204]	Loss 0.3274 (0.4117)	
training:	Epoch: [102][126/204]	Loss 0.3769 (0.4115)	
training:	Epoch: [102][127/204]	Loss 0.3942 (0.4113)	
training:	Epoch: [102][128/204]	Loss 0.3849 (0.4111)	
training:	Epoch: [102][129/204]	Loss 0.5356 (0.4121)	
training:	Epoch: [102][130/204]	Loss 0.4668 (0.4125)	
training:	Epoch: [102][131/204]	Loss 0.2972 (0.4116)	
training:	Epoch: [102][132/204]	Loss 0.5078 (0.4123)	
training:	Epoch: [102][133/204]	Loss 0.5198 (0.4132)	
training:	Epoch: [102][134/204]	Loss 0.3555 (0.4127)	
training:	Epoch: [102][135/204]	Loss 0.4019 (0.4126)	
training:	Epoch: [102][136/204]	Loss 0.3838 (0.4124)	
training:	Epoch: [102][137/204]	Loss 0.4603 (0.4128)	
training:	Epoch: [102][138/204]	Loss 0.3565 (0.4124)	
training:	Epoch: [102][139/204]	Loss 0.3521 (0.4119)	
training:	Epoch: [102][140/204]	Loss 0.3798 (0.4117)	
training:	Epoch: [102][141/204]	Loss 0.3118 (0.4110)	
training:	Epoch: [102][142/204]	Loss 0.3809 (0.4108)	
training:	Epoch: [102][143/204]	Loss 0.3350 (0.4103)	
training:	Epoch: [102][144/204]	Loss 0.5176 (0.4110)	
training:	Epoch: [102][145/204]	Loss 0.3522 (0.4106)	
training:	Epoch: [102][146/204]	Loss 0.3486 (0.4102)	
training:	Epoch: [102][147/204]	Loss 0.5008 (0.4108)	
training:	Epoch: [102][148/204]	Loss 0.5227 (0.4115)	
training:	Epoch: [102][149/204]	Loss 0.3190 (0.4109)	
training:	Epoch: [102][150/204]	Loss 0.5269 (0.4117)	
training:	Epoch: [102][151/204]	Loss 0.2313 (0.4105)	
training:	Epoch: [102][152/204]	Loss 0.3260 (0.4100)	
training:	Epoch: [102][153/204]	Loss 0.5020 (0.4106)	
training:	Epoch: [102][154/204]	Loss 0.5292 (0.4113)	
training:	Epoch: [102][155/204]	Loss 0.5268 (0.4121)	
training:	Epoch: [102][156/204]	Loss 0.5178 (0.4127)	
training:	Epoch: [102][157/204]	Loss 0.4733 (0.4131)	
training:	Epoch: [102][158/204]	Loss 0.3443 (0.4127)	
training:	Epoch: [102][159/204]	Loss 0.4083 (0.4127)	
training:	Epoch: [102][160/204]	Loss 0.4522 (0.4129)	
training:	Epoch: [102][161/204]	Loss 0.4006 (0.4128)	
training:	Epoch: [102][162/204]	Loss 0.3737 (0.4126)	
training:	Epoch: [102][163/204]	Loss 0.3790 (0.4124)	
training:	Epoch: [102][164/204]	Loss 0.2502 (0.4114)	
training:	Epoch: [102][165/204]	Loss 0.2613 (0.4105)	
training:	Epoch: [102][166/204]	Loss 0.3974 (0.4104)	
training:	Epoch: [102][167/204]	Loss 0.3101 (0.4098)	
training:	Epoch: [102][168/204]	Loss 0.4506 (0.4101)	
training:	Epoch: [102][169/204]	Loss 0.5503 (0.4109)	
training:	Epoch: [102][170/204]	Loss 0.3583 (0.4106)	
training:	Epoch: [102][171/204]	Loss 0.5035 (0.4111)	
training:	Epoch: [102][172/204]	Loss 0.3562 (0.4108)	
training:	Epoch: [102][173/204]	Loss 0.4847 (0.4112)	
training:	Epoch: [102][174/204]	Loss 0.3725 (0.4110)	
training:	Epoch: [102][175/204]	Loss 0.2653 (0.4102)	
training:	Epoch: [102][176/204]	Loss 0.3372 (0.4098)	
training:	Epoch: [102][177/204]	Loss 0.3200 (0.4093)	
training:	Epoch: [102][178/204]	Loss 0.4266 (0.4093)	
training:	Epoch: [102][179/204]	Loss 0.5176 (0.4100)	
training:	Epoch: [102][180/204]	Loss 0.5528 (0.4107)	
training:	Epoch: [102][181/204]	Loss 0.4014 (0.4107)	
training:	Epoch: [102][182/204]	Loss 0.3164 (0.4102)	
training:	Epoch: [102][183/204]	Loss 0.4820 (0.4106)	
training:	Epoch: [102][184/204]	Loss 0.3100 (0.4100)	
training:	Epoch: [102][185/204]	Loss 0.3604 (0.4098)	
training:	Epoch: [102][186/204]	Loss 0.5087 (0.4103)	
training:	Epoch: [102][187/204]	Loss 0.3995 (0.4102)	
training:	Epoch: [102][188/204]	Loss 0.5424 (0.4109)	
training:	Epoch: [102][189/204]	Loss 0.4340 (0.4111)	
training:	Epoch: [102][190/204]	Loss 0.6733 (0.4124)	
training:	Epoch: [102][191/204]	Loss 0.5078 (0.4129)	
training:	Epoch: [102][192/204]	Loss 0.3605 (0.4127)	
training:	Epoch: [102][193/204]	Loss 0.6802 (0.4140)	
training:	Epoch: [102][194/204]	Loss 0.3140 (0.4135)	
training:	Epoch: [102][195/204]	Loss 0.3938 (0.4134)	
training:	Epoch: [102][196/204]	Loss 0.6266 (0.4145)	
training:	Epoch: [102][197/204]	Loss 0.2990 (0.4139)	
training:	Epoch: [102][198/204]	Loss 0.4474 (0.4141)	
training:	Epoch: [102][199/204]	Loss 0.3318 (0.4137)	
training:	Epoch: [102][200/204]	Loss 0.3714 (0.4135)	
training:	Epoch: [102][201/204]	Loss 0.4513 (0.4137)	
training:	Epoch: [102][202/204]	Loss 0.6761 (0.4150)	
training:	Epoch: [102][203/204]	Loss 0.3866 (0.4148)	
training:	Epoch: [102][204/204]	Loss 0.5278 (0.4154)	
Training:	 Loss: 0.4147

Training:	 ACC: 0.8300 0.8304 0.8404 0.8195
Validation:	 ACC: 0.7958 0.7967 0.8147 0.7769
Validation:	 Best_BACC: 0.7973 0.7983 0.8199 0.7747
Validation:	 Loss: 0.4396
Pretraining:	Epoch 103/120
----------
training:	Epoch: [103][1/204]	Loss 0.3002 (0.3002)	
training:	Epoch: [103][2/204]	Loss 0.3527 (0.3265)	
training:	Epoch: [103][3/204]	Loss 0.3663 (0.3398)	
training:	Epoch: [103][4/204]	Loss 0.3832 (0.3506)	
training:	Epoch: [103][5/204]	Loss 0.3447 (0.3494)	
training:	Epoch: [103][6/204]	Loss 0.3979 (0.3575)	
training:	Epoch: [103][7/204]	Loss 0.3300 (0.3536)	
training:	Epoch: [103][8/204]	Loss 0.6686 (0.3930)	
training:	Epoch: [103][9/204]	Loss 0.5099 (0.4059)	
training:	Epoch: [103][10/204]	Loss 0.4422 (0.4096)	
training:	Epoch: [103][11/204]	Loss 0.4286 (0.4113)	
training:	Epoch: [103][12/204]	Loss 0.3078 (0.4027)	
training:	Epoch: [103][13/204]	Loss 0.4938 (0.4097)	
training:	Epoch: [103][14/204]	Loss 0.5258 (0.4180)	
training:	Epoch: [103][15/204]	Loss 0.5280 (0.4253)	
training:	Epoch: [103][16/204]	Loss 0.3707 (0.4219)	
training:	Epoch: [103][17/204]	Loss 0.3188 (0.4158)	
training:	Epoch: [103][18/204]	Loss 0.5684 (0.4243)	
training:	Epoch: [103][19/204]	Loss 0.3284 (0.4193)	
training:	Epoch: [103][20/204]	Loss 0.3336 (0.4150)	
training:	Epoch: [103][21/204]	Loss 0.3008 (0.4095)	
training:	Epoch: [103][22/204]	Loss 0.4112 (0.4096)	
training:	Epoch: [103][23/204]	Loss 0.2879 (0.4043)	
training:	Epoch: [103][24/204]	Loss 0.3612 (0.4025)	
training:	Epoch: [103][25/204]	Loss 0.3674 (0.4011)	
training:	Epoch: [103][26/204]	Loss 0.3366 (0.3986)	
training:	Epoch: [103][27/204]	Loss 0.5214 (0.4032)	
training:	Epoch: [103][28/204]	Loss 0.4713 (0.4056)	
training:	Epoch: [103][29/204]	Loss 0.3319 (0.4031)	
training:	Epoch: [103][30/204]	Loss 0.4597 (0.4050)	
training:	Epoch: [103][31/204]	Loss 0.2894 (0.4012)	
training:	Epoch: [103][32/204]	Loss 0.3858 (0.4008)	
training:	Epoch: [103][33/204]	Loss 0.3508 (0.3992)	
training:	Epoch: [103][34/204]	Loss 0.3832 (0.3988)	
training:	Epoch: [103][35/204]	Loss 0.4350 (0.3998)	
training:	Epoch: [103][36/204]	Loss 0.4058 (0.4000)	
training:	Epoch: [103][37/204]	Loss 0.3553 (0.3988)	
training:	Epoch: [103][38/204]	Loss 0.4041 (0.3989)	
training:	Epoch: [103][39/204]	Loss 0.3153 (0.3968)	
training:	Epoch: [103][40/204]	Loss 0.2592 (0.3933)	
training:	Epoch: [103][41/204]	Loss 0.4471 (0.3946)	
training:	Epoch: [103][42/204]	Loss 0.5017 (0.3972)	
training:	Epoch: [103][43/204]	Loss 0.3058 (0.3951)	
training:	Epoch: [103][44/204]	Loss 0.3781 (0.3947)	
training:	Epoch: [103][45/204]	Loss 0.5321 (0.3977)	
training:	Epoch: [103][46/204]	Loss 0.4581 (0.3990)	
training:	Epoch: [103][47/204]	Loss 0.6615 (0.4046)	
training:	Epoch: [103][48/204]	Loss 0.4055 (0.4046)	
training:	Epoch: [103][49/204]	Loss 0.4080 (0.4047)	
training:	Epoch: [103][50/204]	Loss 0.3208 (0.4030)	
training:	Epoch: [103][51/204]	Loss 0.4054 (0.4031)	
training:	Epoch: [103][52/204]	Loss 0.4791 (0.4045)	
training:	Epoch: [103][53/204]	Loss 0.5284 (0.4069)	
training:	Epoch: [103][54/204]	Loss 0.5456 (0.4094)	
training:	Epoch: [103][55/204]	Loss 0.3333 (0.4081)	
training:	Epoch: [103][56/204]	Loss 0.2912 (0.4060)	
training:	Epoch: [103][57/204]	Loss 0.2911 (0.4040)	
training:	Epoch: [103][58/204]	Loss 0.5127 (0.4058)	
training:	Epoch: [103][59/204]	Loss 0.4326 (0.4063)	
training:	Epoch: [103][60/204]	Loss 0.4121 (0.4064)	
training:	Epoch: [103][61/204]	Loss 0.3445 (0.4054)	
training:	Epoch: [103][62/204]	Loss 0.4051 (0.4054)	
training:	Epoch: [103][63/204]	Loss 0.4571 (0.4062)	
training:	Epoch: [103][64/204]	Loss 0.5157 (0.4079)	
training:	Epoch: [103][65/204]	Loss 0.6055 (0.4109)	
training:	Epoch: [103][66/204]	Loss 0.3811 (0.4105)	
training:	Epoch: [103][67/204]	Loss 0.4783 (0.4115)	
training:	Epoch: [103][68/204]	Loss 0.4674 (0.4123)	
training:	Epoch: [103][69/204]	Loss 0.4788 (0.4133)	
training:	Epoch: [103][70/204]	Loss 0.3790 (0.4128)	
training:	Epoch: [103][71/204]	Loss 0.4021 (0.4126)	
training:	Epoch: [103][72/204]	Loss 0.5357 (0.4143)	
training:	Epoch: [103][73/204]	Loss 0.2620 (0.4123)	
training:	Epoch: [103][74/204]	Loss 0.4242 (0.4124)	
training:	Epoch: [103][75/204]	Loss 0.3176 (0.4112)	
training:	Epoch: [103][76/204]	Loss 0.4448 (0.4116)	
training:	Epoch: [103][77/204]	Loss 0.3555 (0.4109)	
training:	Epoch: [103][78/204]	Loss 0.6133 (0.4135)	
training:	Epoch: [103][79/204]	Loss 0.4460 (0.4139)	
training:	Epoch: [103][80/204]	Loss 0.6837 (0.4173)	
training:	Epoch: [103][81/204]	Loss 0.2529 (0.4152)	
training:	Epoch: [103][82/204]	Loss 0.4938 (0.4162)	
training:	Epoch: [103][83/204]	Loss 0.3141 (0.4150)	
training:	Epoch: [103][84/204]	Loss 0.4692 (0.4156)	
training:	Epoch: [103][85/204]	Loss 0.3370 (0.4147)	
training:	Epoch: [103][86/204]	Loss 0.3783 (0.4143)	
training:	Epoch: [103][87/204]	Loss 0.4328 (0.4145)	
training:	Epoch: [103][88/204]	Loss 0.5167 (0.4156)	
training:	Epoch: [103][89/204]	Loss 0.3273 (0.4146)	
training:	Epoch: [103][90/204]	Loss 0.3248 (0.4136)	
training:	Epoch: [103][91/204]	Loss 0.4877 (0.4145)	
training:	Epoch: [103][92/204]	Loss 0.4024 (0.4143)	
training:	Epoch: [103][93/204]	Loss 0.6544 (0.4169)	
training:	Epoch: [103][94/204]	Loss 0.3617 (0.4163)	
training:	Epoch: [103][95/204]	Loss 0.3653 (0.4158)	
training:	Epoch: [103][96/204]	Loss 0.3999 (0.4156)	
training:	Epoch: [103][97/204]	Loss 0.3081 (0.4145)	
training:	Epoch: [103][98/204]	Loss 0.3421 (0.4138)	
training:	Epoch: [103][99/204]	Loss 0.3704 (0.4133)	
training:	Epoch: [103][100/204]	Loss 0.4071 (0.4133)	
training:	Epoch: [103][101/204]	Loss 0.6599 (0.4157)	
training:	Epoch: [103][102/204]	Loss 0.5292 (0.4168)	
training:	Epoch: [103][103/204]	Loss 0.4049 (0.4167)	
training:	Epoch: [103][104/204]	Loss 0.4230 (0.4168)	
training:	Epoch: [103][105/204]	Loss 0.4245 (0.4168)	
training:	Epoch: [103][106/204]	Loss 0.4377 (0.4170)	
training:	Epoch: [103][107/204]	Loss 0.2909 (0.4159)	
training:	Epoch: [103][108/204]	Loss 0.2214 (0.4141)	
training:	Epoch: [103][109/204]	Loss 0.5003 (0.4148)	
training:	Epoch: [103][110/204]	Loss 0.3330 (0.4141)	
training:	Epoch: [103][111/204]	Loss 0.6190 (0.4159)	
training:	Epoch: [103][112/204]	Loss 0.3410 (0.4153)	
training:	Epoch: [103][113/204]	Loss 0.4625 (0.4157)	
training:	Epoch: [103][114/204]	Loss 0.4295 (0.4158)	
training:	Epoch: [103][115/204]	Loss 0.4617 (0.4162)	
training:	Epoch: [103][116/204]	Loss 0.5108 (0.4170)	
training:	Epoch: [103][117/204]	Loss 0.2923 (0.4160)	
training:	Epoch: [103][118/204]	Loss 0.3356 (0.4153)	
training:	Epoch: [103][119/204]	Loss 0.3029 (0.4143)	
training:	Epoch: [103][120/204]	Loss 0.5521 (0.4155)	
training:	Epoch: [103][121/204]	Loss 0.3771 (0.4152)	
training:	Epoch: [103][122/204]	Loss 0.6112 (0.4168)	
training:	Epoch: [103][123/204]	Loss 0.3730 (0.4164)	
training:	Epoch: [103][124/204]	Loss 0.3908 (0.4162)	
training:	Epoch: [103][125/204]	Loss 0.2756 (0.4151)	
training:	Epoch: [103][126/204]	Loss 0.5234 (0.4159)	
training:	Epoch: [103][127/204]	Loss 0.4029 (0.4158)	
training:	Epoch: [103][128/204]	Loss 0.3490 (0.4153)	
training:	Epoch: [103][129/204]	Loss 0.3790 (0.4150)	
training:	Epoch: [103][130/204]	Loss 0.4747 (0.4155)	
training:	Epoch: [103][131/204]	Loss 0.4388 (0.4157)	
training:	Epoch: [103][132/204]	Loss 0.3188 (0.4149)	
training:	Epoch: [103][133/204]	Loss 0.4064 (0.4149)	
training:	Epoch: [103][134/204]	Loss 0.5183 (0.4157)	
training:	Epoch: [103][135/204]	Loss 0.2820 (0.4147)	
training:	Epoch: [103][136/204]	Loss 0.3340 (0.4141)	
training:	Epoch: [103][137/204]	Loss 0.5069 (0.4147)	
training:	Epoch: [103][138/204]	Loss 0.3801 (0.4145)	
training:	Epoch: [103][139/204]	Loss 0.4706 (0.4149)	
training:	Epoch: [103][140/204]	Loss 0.5505 (0.4159)	
training:	Epoch: [103][141/204]	Loss 0.3268 (0.4152)	
training:	Epoch: [103][142/204]	Loss 0.3392 (0.4147)	
training:	Epoch: [103][143/204]	Loss 0.4740 (0.4151)	
training:	Epoch: [103][144/204]	Loss 0.4432 (0.4153)	
training:	Epoch: [103][145/204]	Loss 0.4684 (0.4157)	
training:	Epoch: [103][146/204]	Loss 0.4103 (0.4156)	
training:	Epoch: [103][147/204]	Loss 0.5707 (0.4167)	
training:	Epoch: [103][148/204]	Loss 0.4510 (0.4169)	
training:	Epoch: [103][149/204]	Loss 0.4462 (0.4171)	
training:	Epoch: [103][150/204]	Loss 0.3571 (0.4167)	
training:	Epoch: [103][151/204]	Loss 0.3505 (0.4163)	
training:	Epoch: [103][152/204]	Loss 0.3907 (0.4161)	
training:	Epoch: [103][153/204]	Loss 0.3312 (0.4156)	
training:	Epoch: [103][154/204]	Loss 0.2726 (0.4146)	
training:	Epoch: [103][155/204]	Loss 0.5078 (0.4152)	
training:	Epoch: [103][156/204]	Loss 0.3179 (0.4146)	
training:	Epoch: [103][157/204]	Loss 0.3526 (0.4142)	
training:	Epoch: [103][158/204]	Loss 0.3453 (0.4138)	
training:	Epoch: [103][159/204]	Loss 0.4835 (0.4142)	
training:	Epoch: [103][160/204]	Loss 0.3436 (0.4138)	
training:	Epoch: [103][161/204]	Loss 0.2834 (0.4130)	
training:	Epoch: [103][162/204]	Loss 0.4361 (0.4131)	
training:	Epoch: [103][163/204]	Loss 0.3338 (0.4126)	
training:	Epoch: [103][164/204]	Loss 0.5695 (0.4136)	
training:	Epoch: [103][165/204]	Loss 0.2879 (0.4128)	
training:	Epoch: [103][166/204]	Loss 0.4684 (0.4132)	
training:	Epoch: [103][167/204]	Loss 0.4671 (0.4135)	
training:	Epoch: [103][168/204]	Loss 0.4273 (0.4136)	
training:	Epoch: [103][169/204]	Loss 0.4781 (0.4139)	
training:	Epoch: [103][170/204]	Loss 0.2766 (0.4131)	
training:	Epoch: [103][171/204]	Loss 0.2365 (0.4121)	
training:	Epoch: [103][172/204]	Loss 0.4570 (0.4124)	
training:	Epoch: [103][173/204]	Loss 0.4421 (0.4125)	
training:	Epoch: [103][174/204]	Loss 0.5494 (0.4133)	
training:	Epoch: [103][175/204]	Loss 0.3150 (0.4128)	
training:	Epoch: [103][176/204]	Loss 0.6777 (0.4143)	
training:	Epoch: [103][177/204]	Loss 0.3388 (0.4138)	
training:	Epoch: [103][178/204]	Loss 0.3200 (0.4133)	
training:	Epoch: [103][179/204]	Loss 0.4582 (0.4136)	
training:	Epoch: [103][180/204]	Loss 0.4417 (0.4137)	
training:	Epoch: [103][181/204]	Loss 0.4586 (0.4140)	
training:	Epoch: [103][182/204]	Loss 0.4031 (0.4139)	
training:	Epoch: [103][183/204]	Loss 0.4396 (0.4140)	
training:	Epoch: [103][184/204]	Loss 0.4640 (0.4143)	
training:	Epoch: [103][185/204]	Loss 0.3506 (0.4140)	
training:	Epoch: [103][186/204]	Loss 0.4803 (0.4143)	
training:	Epoch: [103][187/204]	Loss 0.3117 (0.4138)	
training:	Epoch: [103][188/204]	Loss 0.3674 (0.4135)	
training:	Epoch: [103][189/204]	Loss 0.2928 (0.4129)	
training:	Epoch: [103][190/204]	Loss 0.4582 (0.4131)	
training:	Epoch: [103][191/204]	Loss 0.4916 (0.4135)	
training:	Epoch: [103][192/204]	Loss 0.4039 (0.4135)	
training:	Epoch: [103][193/204]	Loss 0.4404 (0.4136)	
training:	Epoch: [103][194/204]	Loss 0.4440 (0.4138)	
training:	Epoch: [103][195/204]	Loss 0.3118 (0.4133)	
training:	Epoch: [103][196/204]	Loss 0.3384 (0.4129)	
training:	Epoch: [103][197/204]	Loss 0.4312 (0.4130)	
training:	Epoch: [103][198/204]	Loss 0.5782 (0.4138)	
training:	Epoch: [103][199/204]	Loss 0.3114 (0.4133)	
training:	Epoch: [103][200/204]	Loss 0.2758 (0.4126)	
training:	Epoch: [103][201/204]	Loss 0.5037 (0.4131)	
training:	Epoch: [103][202/204]	Loss 0.4433 (0.4132)	
training:	Epoch: [103][203/204]	Loss 0.4267 (0.4133)	
training:	Epoch: [103][204/204]	Loss 0.2721 (0.4126)	
Training:	 Loss: 0.4120

Training:	 ACC: 0.8304 0.8310 0.8445 0.8163
Validation:	 ACC: 0.7946 0.7956 0.8168 0.7724
Validation:	 Best_BACC: 0.7973 0.7983 0.8199 0.7747
Validation:	 Loss: 0.4389
Pretraining:	Epoch 104/120
----------
training:	Epoch: [104][1/204]	Loss 0.4908 (0.4908)	
training:	Epoch: [104][2/204]	Loss 0.4267 (0.4588)	
training:	Epoch: [104][3/204]	Loss 0.4197 (0.4457)	
training:	Epoch: [104][4/204]	Loss 0.4802 (0.4543)	
training:	Epoch: [104][5/204]	Loss 0.4491 (0.4533)	
training:	Epoch: [104][6/204]	Loss 0.4687 (0.4559)	
training:	Epoch: [104][7/204]	Loss 0.4254 (0.4515)	
training:	Epoch: [104][8/204]	Loss 0.3713 (0.4415)	
training:	Epoch: [104][9/204]	Loss 0.3189 (0.4279)	
training:	Epoch: [104][10/204]	Loss 0.4488 (0.4300)	
training:	Epoch: [104][11/204]	Loss 0.4396 (0.4308)	
training:	Epoch: [104][12/204]	Loss 0.2378 (0.4147)	
training:	Epoch: [104][13/204]	Loss 0.4545 (0.4178)	
training:	Epoch: [104][14/204]	Loss 0.4283 (0.4185)	
training:	Epoch: [104][15/204]	Loss 0.3102 (0.4113)	
training:	Epoch: [104][16/204]	Loss 0.2622 (0.4020)	
training:	Epoch: [104][17/204]	Loss 0.3706 (0.4002)	
training:	Epoch: [104][18/204]	Loss 0.3975 (0.4000)	
training:	Epoch: [104][19/204]	Loss 0.4628 (0.4033)	
training:	Epoch: [104][20/204]	Loss 0.5259 (0.4094)	
training:	Epoch: [104][21/204]	Loss 0.4564 (0.4117)	
training:	Epoch: [104][22/204]	Loss 0.4545 (0.4136)	
training:	Epoch: [104][23/204]	Loss 0.2883 (0.4082)	
training:	Epoch: [104][24/204]	Loss 0.2643 (0.4022)	
training:	Epoch: [104][25/204]	Loss 0.4592 (0.4045)	
training:	Epoch: [104][26/204]	Loss 0.4222 (0.4051)	
training:	Epoch: [104][27/204]	Loss 0.3347 (0.4025)	
training:	Epoch: [104][28/204]	Loss 0.2888 (0.3985)	
training:	Epoch: [104][29/204]	Loss 0.4009 (0.3986)	
training:	Epoch: [104][30/204]	Loss 0.3936 (0.3984)	
training:	Epoch: [104][31/204]	Loss 0.4796 (0.4010)	
training:	Epoch: [104][32/204]	Loss 0.5384 (0.4053)	
training:	Epoch: [104][33/204]	Loss 0.4570 (0.4069)	
training:	Epoch: [104][34/204]	Loss 0.5059 (0.4098)	
training:	Epoch: [104][35/204]	Loss 0.7279 (0.4189)	
training:	Epoch: [104][36/204]	Loss 0.4470 (0.4197)	
training:	Epoch: [104][37/204]	Loss 0.4502 (0.4205)	
training:	Epoch: [104][38/204]	Loss 0.4318 (0.4208)	
training:	Epoch: [104][39/204]	Loss 0.5045 (0.4229)	
training:	Epoch: [104][40/204]	Loss 0.4569 (0.4238)	
training:	Epoch: [104][41/204]	Loss 0.5645 (0.4272)	
training:	Epoch: [104][42/204]	Loss 0.3188 (0.4246)	
training:	Epoch: [104][43/204]	Loss 0.5402 (0.4273)	
training:	Epoch: [104][44/204]	Loss 0.4141 (0.4270)	
training:	Epoch: [104][45/204]	Loss 0.3789 (0.4259)	
training:	Epoch: [104][46/204]	Loss 0.2939 (0.4231)	
training:	Epoch: [104][47/204]	Loss 0.3736 (0.4220)	
training:	Epoch: [104][48/204]	Loss 0.4167 (0.4219)	
training:	Epoch: [104][49/204]	Loss 0.5122 (0.4238)	
training:	Epoch: [104][50/204]	Loss 0.4960 (0.4252)	
training:	Epoch: [104][51/204]	Loss 0.2898 (0.4225)	
training:	Epoch: [104][52/204]	Loss 0.5459 (0.4249)	
training:	Epoch: [104][53/204]	Loss 0.3837 (0.4241)	
training:	Epoch: [104][54/204]	Loss 0.3602 (0.4230)	
training:	Epoch: [104][55/204]	Loss 0.3546 (0.4217)	
training:	Epoch: [104][56/204]	Loss 0.4587 (0.4224)	
training:	Epoch: [104][57/204]	Loss 0.3416 (0.4210)	
training:	Epoch: [104][58/204]	Loss 0.3669 (0.4200)	
training:	Epoch: [104][59/204]	Loss 0.3643 (0.4191)	
training:	Epoch: [104][60/204]	Loss 0.4139 (0.4190)	
training:	Epoch: [104][61/204]	Loss 0.4644 (0.4197)	
training:	Epoch: [104][62/204]	Loss 0.3199 (0.4181)	
training:	Epoch: [104][63/204]	Loss 0.5680 (0.4205)	
training:	Epoch: [104][64/204]	Loss 0.3748 (0.4198)	
training:	Epoch: [104][65/204]	Loss 0.4503 (0.4203)	
training:	Epoch: [104][66/204]	Loss 0.3452 (0.4191)	
training:	Epoch: [104][67/204]	Loss 0.4599 (0.4197)	
training:	Epoch: [104][68/204]	Loss 0.3769 (0.4191)	
training:	Epoch: [104][69/204]	Loss 0.5014 (0.4203)	
training:	Epoch: [104][70/204]	Loss 0.3918 (0.4199)	
training:	Epoch: [104][71/204]	Loss 0.5891 (0.4223)	
training:	Epoch: [104][72/204]	Loss 0.4370 (0.4225)	
training:	Epoch: [104][73/204]	Loss 0.4253 (0.4225)	
training:	Epoch: [104][74/204]	Loss 0.4485 (0.4229)	
training:	Epoch: [104][75/204]	Loss 0.4673 (0.4235)	
training:	Epoch: [104][76/204]	Loss 0.4397 (0.4237)	
training:	Epoch: [104][77/204]	Loss 0.4230 (0.4237)	
training:	Epoch: [104][78/204]	Loss 0.5037 (0.4247)	
training:	Epoch: [104][79/204]	Loss 0.2957 (0.4231)	
training:	Epoch: [104][80/204]	Loss 0.6206 (0.4255)	
training:	Epoch: [104][81/204]	Loss 0.3824 (0.4250)	
training:	Epoch: [104][82/204]	Loss 0.3820 (0.4245)	
training:	Epoch: [104][83/204]	Loss 0.3052 (0.4230)	
training:	Epoch: [104][84/204]	Loss 0.2816 (0.4213)	
training:	Epoch: [104][85/204]	Loss 0.4491 (0.4217)	
training:	Epoch: [104][86/204]	Loss 0.2575 (0.4198)	
training:	Epoch: [104][87/204]	Loss 0.3898 (0.4194)	
training:	Epoch: [104][88/204]	Loss 0.6300 (0.4218)	
training:	Epoch: [104][89/204]	Loss 0.4870 (0.4225)	
training:	Epoch: [104][90/204]	Loss 0.3486 (0.4217)	
training:	Epoch: [104][91/204]	Loss 0.3746 (0.4212)	
training:	Epoch: [104][92/204]	Loss 0.3493 (0.4204)	
training:	Epoch: [104][93/204]	Loss 0.2812 (0.4189)	
training:	Epoch: [104][94/204]	Loss 0.6408 (0.4213)	
training:	Epoch: [104][95/204]	Loss 0.3706 (0.4208)	
training:	Epoch: [104][96/204]	Loss 0.3408 (0.4199)	
training:	Epoch: [104][97/204]	Loss 0.3621 (0.4193)	
training:	Epoch: [104][98/204]	Loss 0.4381 (0.4195)	
training:	Epoch: [104][99/204]	Loss 0.4133 (0.4195)	
training:	Epoch: [104][100/204]	Loss 0.3752 (0.4190)	
training:	Epoch: [104][101/204]	Loss 0.3532 (0.4184)	
training:	Epoch: [104][102/204]	Loss 0.4493 (0.4187)	
training:	Epoch: [104][103/204]	Loss 0.4871 (0.4193)	
training:	Epoch: [104][104/204]	Loss 0.4050 (0.4192)	
training:	Epoch: [104][105/204]	Loss 0.4173 (0.4192)	
training:	Epoch: [104][106/204]	Loss 0.3706 (0.4187)	
training:	Epoch: [104][107/204]	Loss 0.2656 (0.4173)	
training:	Epoch: [104][108/204]	Loss 0.3362 (0.4165)	
training:	Epoch: [104][109/204]	Loss 0.3454 (0.4159)	
training:	Epoch: [104][110/204]	Loss 0.4153 (0.4159)	
training:	Epoch: [104][111/204]	Loss 0.4241 (0.4159)	
training:	Epoch: [104][112/204]	Loss 0.6081 (0.4177)	
training:	Epoch: [104][113/204]	Loss 0.3729 (0.4173)	
training:	Epoch: [104][114/204]	Loss 0.3352 (0.4165)	
training:	Epoch: [104][115/204]	Loss 0.4240 (0.4166)	
training:	Epoch: [104][116/204]	Loss 0.3577 (0.4161)	
training:	Epoch: [104][117/204]	Loss 0.4270 (0.4162)	
training:	Epoch: [104][118/204]	Loss 0.4270 (0.4163)	
training:	Epoch: [104][119/204]	Loss 0.3611 (0.4158)	
training:	Epoch: [104][120/204]	Loss 0.4822 (0.4164)	
training:	Epoch: [104][121/204]	Loss 0.4358 (0.4165)	
training:	Epoch: [104][122/204]	Loss 0.4666 (0.4169)	
training:	Epoch: [104][123/204]	Loss 0.5371 (0.4179)	
training:	Epoch: [104][124/204]	Loss 0.5000 (0.4186)	
training:	Epoch: [104][125/204]	Loss 0.5072 (0.4193)	
training:	Epoch: [104][126/204]	Loss 0.3424 (0.4187)	
training:	Epoch: [104][127/204]	Loss 0.4371 (0.4188)	
training:	Epoch: [104][128/204]	Loss 0.3401 (0.4182)	
training:	Epoch: [104][129/204]	Loss 0.4103 (0.4182)	
training:	Epoch: [104][130/204]	Loss 0.6248 (0.4197)	
training:	Epoch: [104][131/204]	Loss 0.3701 (0.4194)	
training:	Epoch: [104][132/204]	Loss 0.5635 (0.4205)	
training:	Epoch: [104][133/204]	Loss 0.3098 (0.4196)	
training:	Epoch: [104][134/204]	Loss 0.3140 (0.4188)	
training:	Epoch: [104][135/204]	Loss 0.4893 (0.4194)	
training:	Epoch: [104][136/204]	Loss 0.2660 (0.4182)	
training:	Epoch: [104][137/204]	Loss 0.3615 (0.4178)	
training:	Epoch: [104][138/204]	Loss 0.3074 (0.4170)	
training:	Epoch: [104][139/204]	Loss 0.3178 (0.4163)	
training:	Epoch: [104][140/204]	Loss 0.4043 (0.4162)	
training:	Epoch: [104][141/204]	Loss 0.5264 (0.4170)	
training:	Epoch: [104][142/204]	Loss 0.4683 (0.4174)	
training:	Epoch: [104][143/204]	Loss 0.3694 (0.4170)	
training:	Epoch: [104][144/204]	Loss 0.3427 (0.4165)	
training:	Epoch: [104][145/204]	Loss 0.3278 (0.4159)	
training:	Epoch: [104][146/204]	Loss 0.3930 (0.4157)	
training:	Epoch: [104][147/204]	Loss 0.4038 (0.4157)	
training:	Epoch: [104][148/204]	Loss 0.3715 (0.4154)	
training:	Epoch: [104][149/204]	Loss 0.4287 (0.4155)	
training:	Epoch: [104][150/204]	Loss 0.4730 (0.4158)	
training:	Epoch: [104][151/204]	Loss 0.4192 (0.4159)	
training:	Epoch: [104][152/204]	Loss 0.5955 (0.4170)	
training:	Epoch: [104][153/204]	Loss 0.4663 (0.4174)	
training:	Epoch: [104][154/204]	Loss 0.2166 (0.4161)	
training:	Epoch: [104][155/204]	Loss 0.4629 (0.4164)	
training:	Epoch: [104][156/204]	Loss 0.4123 (0.4163)	
training:	Epoch: [104][157/204]	Loss 0.4124 (0.4163)	
training:	Epoch: [104][158/204]	Loss 0.2380 (0.4152)	
training:	Epoch: [104][159/204]	Loss 0.3361 (0.4147)	
training:	Epoch: [104][160/204]	Loss 0.3971 (0.4146)	
training:	Epoch: [104][161/204]	Loss 0.3076 (0.4139)	
training:	Epoch: [104][162/204]	Loss 0.4344 (0.4140)	
training:	Epoch: [104][163/204]	Loss 0.3756 (0.4138)	
training:	Epoch: [104][164/204]	Loss 0.5472 (0.4146)	
training:	Epoch: [104][165/204]	Loss 0.4001 (0.4145)	
training:	Epoch: [104][166/204]	Loss 0.3956 (0.4144)	
training:	Epoch: [104][167/204]	Loss 0.4028 (0.4143)	
training:	Epoch: [104][168/204]	Loss 0.2275 (0.4132)	
training:	Epoch: [104][169/204]	Loss 0.5160 (0.4138)	
training:	Epoch: [104][170/204]	Loss 0.4333 (0.4139)	
training:	Epoch: [104][171/204]	Loss 0.3448 (0.4135)	
training:	Epoch: [104][172/204]	Loss 0.2808 (0.4128)	
training:	Epoch: [104][173/204]	Loss 0.4107 (0.4128)	
training:	Epoch: [104][174/204]	Loss 0.3297 (0.4123)	
training:	Epoch: [104][175/204]	Loss 0.5338 (0.4130)	
training:	Epoch: [104][176/204]	Loss 0.4624 (0.4133)	
training:	Epoch: [104][177/204]	Loss 0.4954 (0.4137)	
training:	Epoch: [104][178/204]	Loss 0.3815 (0.4135)	
training:	Epoch: [104][179/204]	Loss 0.4615 (0.4138)	
training:	Epoch: [104][180/204]	Loss 0.2852 (0.4131)	
training:	Epoch: [104][181/204]	Loss 0.4123 (0.4131)	
training:	Epoch: [104][182/204]	Loss 0.3206 (0.4126)	
training:	Epoch: [104][183/204]	Loss 0.7318 (0.4143)	
training:	Epoch: [104][184/204]	Loss 0.4060 (0.4143)	
training:	Epoch: [104][185/204]	Loss 0.3544 (0.4140)	
training:	Epoch: [104][186/204]	Loss 0.3746 (0.4137)	
training:	Epoch: [104][187/204]	Loss 0.4212 (0.4138)	
training:	Epoch: [104][188/204]	Loss 0.4508 (0.4140)	
training:	Epoch: [104][189/204]	Loss 0.5703 (0.4148)	
training:	Epoch: [104][190/204]	Loss 0.4266 (0.4149)	
training:	Epoch: [104][191/204]	Loss 0.3313 (0.4144)	
training:	Epoch: [104][192/204]	Loss 0.3177 (0.4139)	
training:	Epoch: [104][193/204]	Loss 0.2606 (0.4131)	
training:	Epoch: [104][194/204]	Loss 0.3870 (0.4130)	
training:	Epoch: [104][195/204]	Loss 0.5216 (0.4136)	
training:	Epoch: [104][196/204]	Loss 0.4660 (0.4138)	
training:	Epoch: [104][197/204]	Loss 0.3241 (0.4134)	
training:	Epoch: [104][198/204]	Loss 0.5078 (0.4138)	
training:	Epoch: [104][199/204]	Loss 0.4039 (0.4138)	
training:	Epoch: [104][200/204]	Loss 0.4278 (0.4139)	
training:	Epoch: [104][201/204]	Loss 0.2905 (0.4133)	
training:	Epoch: [104][202/204]	Loss 0.2669 (0.4125)	
training:	Epoch: [104][203/204]	Loss 0.5464 (0.4132)	
training:	Epoch: [104][204/204]	Loss 0.3554 (0.4129)	
Training:	 Loss: 0.4123

Training:	 ACC: 0.8309 0.8311 0.8363 0.8256
Validation:	 ACC: 0.7939 0.7945 0.8076 0.7803
Validation:	 Best_BACC: 0.7973 0.7983 0.8199 0.7747
Validation:	 Loss: 0.4381
Pretraining:	Epoch 105/120
----------
training:	Epoch: [105][1/204]	Loss 0.3619 (0.3619)	
training:	Epoch: [105][2/204]	Loss 0.3929 (0.3774)	
training:	Epoch: [105][3/204]	Loss 0.3697 (0.3748)	
training:	Epoch: [105][4/204]	Loss 0.3588 (0.3708)	
training:	Epoch: [105][5/204]	Loss 0.2887 (0.3544)	
training:	Epoch: [105][6/204]	Loss 0.4481 (0.3700)	
training:	Epoch: [105][7/204]	Loss 0.5100 (0.3900)	
training:	Epoch: [105][8/204]	Loss 0.4363 (0.3958)	
training:	Epoch: [105][9/204]	Loss 0.2942 (0.3845)	
training:	Epoch: [105][10/204]	Loss 0.3630 (0.3824)	
training:	Epoch: [105][11/204]	Loss 0.3657 (0.3808)	
training:	Epoch: [105][12/204]	Loss 0.4110 (0.3834)	
training:	Epoch: [105][13/204]	Loss 0.4692 (0.3900)	
training:	Epoch: [105][14/204]	Loss 0.4686 (0.3956)	
training:	Epoch: [105][15/204]	Loss 0.3950 (0.3955)	
training:	Epoch: [105][16/204]	Loss 0.4119 (0.3966)	
training:	Epoch: [105][17/204]	Loss 0.4074 (0.3972)	
training:	Epoch: [105][18/204]	Loss 0.2938 (0.3915)	
training:	Epoch: [105][19/204]	Loss 0.4386 (0.3939)	
training:	Epoch: [105][20/204]	Loss 0.5360 (0.4010)	
training:	Epoch: [105][21/204]	Loss 0.3486 (0.3985)	
training:	Epoch: [105][22/204]	Loss 0.4316 (0.4000)	
training:	Epoch: [105][23/204]	Loss 0.5205 (0.4053)	
training:	Epoch: [105][24/204]	Loss 0.4621 (0.4077)	
training:	Epoch: [105][25/204]	Loss 0.3387 (0.4049)	
training:	Epoch: [105][26/204]	Loss 0.3409 (0.4024)	
training:	Epoch: [105][27/204]	Loss 0.5581 (0.4082)	
training:	Epoch: [105][28/204]	Loss 0.2603 (0.4029)	
training:	Epoch: [105][29/204]	Loss 0.4431 (0.4043)	
training:	Epoch: [105][30/204]	Loss 0.4566 (0.4060)	
training:	Epoch: [105][31/204]	Loss 0.3793 (0.4052)	
training:	Epoch: [105][32/204]	Loss 0.3375 (0.4031)	
training:	Epoch: [105][33/204]	Loss 0.4462 (0.4044)	
training:	Epoch: [105][34/204]	Loss 0.7173 (0.4136)	
training:	Epoch: [105][35/204]	Loss 0.2469 (0.4088)	
training:	Epoch: [105][36/204]	Loss 0.3001 (0.4058)	
training:	Epoch: [105][37/204]	Loss 0.2642 (0.4020)	
training:	Epoch: [105][38/204]	Loss 0.3507 (0.4006)	
training:	Epoch: [105][39/204]	Loss 0.3314 (0.3988)	
training:	Epoch: [105][40/204]	Loss 0.4812 (0.4009)	
training:	Epoch: [105][41/204]	Loss 0.3358 (0.3993)	
training:	Epoch: [105][42/204]	Loss 0.4277 (0.4000)	
training:	Epoch: [105][43/204]	Loss 0.4851 (0.4020)	
training:	Epoch: [105][44/204]	Loss 0.5453 (0.4052)	
training:	Epoch: [105][45/204]	Loss 0.4097 (0.4053)	
training:	Epoch: [105][46/204]	Loss 0.3686 (0.4045)	
training:	Epoch: [105][47/204]	Loss 0.4058 (0.4046)	
training:	Epoch: [105][48/204]	Loss 0.5633 (0.4079)	
training:	Epoch: [105][49/204]	Loss 0.4013 (0.4077)	
training:	Epoch: [105][50/204]	Loss 0.4301 (0.4082)	
training:	Epoch: [105][51/204]	Loss 0.4964 (0.4099)	
training:	Epoch: [105][52/204]	Loss 0.5989 (0.4135)	
training:	Epoch: [105][53/204]	Loss 0.4373 (0.4140)	
training:	Epoch: [105][54/204]	Loss 0.5039 (0.4157)	
training:	Epoch: [105][55/204]	Loss 0.3665 (0.4148)	
training:	Epoch: [105][56/204]	Loss 0.3397 (0.4134)	
training:	Epoch: [105][57/204]	Loss 0.4330 (0.4138)	
training:	Epoch: [105][58/204]	Loss 0.3375 (0.4124)	
training:	Epoch: [105][59/204]	Loss 0.2876 (0.4103)	
training:	Epoch: [105][60/204]	Loss 0.2891 (0.4083)	
training:	Epoch: [105][61/204]	Loss 0.4931 (0.4097)	
training:	Epoch: [105][62/204]	Loss 0.3667 (0.4090)	
training:	Epoch: [105][63/204]	Loss 0.2662 (0.4067)	
training:	Epoch: [105][64/204]	Loss 0.4306 (0.4071)	
training:	Epoch: [105][65/204]	Loss 0.5203 (0.4089)	
training:	Epoch: [105][66/204]	Loss 0.5295 (0.4107)	
training:	Epoch: [105][67/204]	Loss 0.2801 (0.4087)	
training:	Epoch: [105][68/204]	Loss 0.2259 (0.4060)	
training:	Epoch: [105][69/204]	Loss 0.4875 (0.4072)	
training:	Epoch: [105][70/204]	Loss 0.3475 (0.4064)	
training:	Epoch: [105][71/204]	Loss 0.5171 (0.4079)	
training:	Epoch: [105][72/204]	Loss 0.3980 (0.4078)	
training:	Epoch: [105][73/204]	Loss 0.5065 (0.4091)	
training:	Epoch: [105][74/204]	Loss 0.4795 (0.4101)	
training:	Epoch: [105][75/204]	Loss 0.3960 (0.4099)	
training:	Epoch: [105][76/204]	Loss 0.3272 (0.4088)	
training:	Epoch: [105][77/204]	Loss 0.6287 (0.4117)	
training:	Epoch: [105][78/204]	Loss 0.3068 (0.4103)	
training:	Epoch: [105][79/204]	Loss 0.3194 (0.4092)	
training:	Epoch: [105][80/204]	Loss 0.3315 (0.4082)	
training:	Epoch: [105][81/204]	Loss 0.3765 (0.4078)	
training:	Epoch: [105][82/204]	Loss 0.3963 (0.4077)	
training:	Epoch: [105][83/204]	Loss 0.4304 (0.4080)	
training:	Epoch: [105][84/204]	Loss 0.5553 (0.4097)	
training:	Epoch: [105][85/204]	Loss 0.4261 (0.4099)	
training:	Epoch: [105][86/204]	Loss 0.3183 (0.4088)	
training:	Epoch: [105][87/204]	Loss 0.5118 (0.4100)	
training:	Epoch: [105][88/204]	Loss 0.3378 (0.4092)	
training:	Epoch: [105][89/204]	Loss 0.5590 (0.4109)	
training:	Epoch: [105][90/204]	Loss 0.6619 (0.4137)	
training:	Epoch: [105][91/204]	Loss 0.3878 (0.4134)	
training:	Epoch: [105][92/204]	Loss 0.5615 (0.4150)	
training:	Epoch: [105][93/204]	Loss 0.5155 (0.4161)	
training:	Epoch: [105][94/204]	Loss 0.4545 (0.4165)	
training:	Epoch: [105][95/204]	Loss 0.3281 (0.4156)	
training:	Epoch: [105][96/204]	Loss 0.2520 (0.4139)	
training:	Epoch: [105][97/204]	Loss 0.4735 (0.4145)	
training:	Epoch: [105][98/204]	Loss 0.3317 (0.4136)	
training:	Epoch: [105][99/204]	Loss 0.3886 (0.4134)	
training:	Epoch: [105][100/204]	Loss 0.4916 (0.4142)	
training:	Epoch: [105][101/204]	Loss 0.4899 (0.4149)	
training:	Epoch: [105][102/204]	Loss 0.4005 (0.4148)	
training:	Epoch: [105][103/204]	Loss 0.4847 (0.4154)	
training:	Epoch: [105][104/204]	Loss 0.5077 (0.4163)	
training:	Epoch: [105][105/204]	Loss 0.4410 (0.4166)	
training:	Epoch: [105][106/204]	Loss 0.2701 (0.4152)	
training:	Epoch: [105][107/204]	Loss 0.2672 (0.4138)	
training:	Epoch: [105][108/204]	Loss 0.5648 (0.4152)	
training:	Epoch: [105][109/204]	Loss 0.2837 (0.4140)	
training:	Epoch: [105][110/204]	Loss 0.3712 (0.4136)	
training:	Epoch: [105][111/204]	Loss 0.3884 (0.4134)	
training:	Epoch: [105][112/204]	Loss 0.4864 (0.4140)	
training:	Epoch: [105][113/204]	Loss 0.3149 (0.4131)	
training:	Epoch: [105][114/204]	Loss 0.3124 (0.4123)	
training:	Epoch: [105][115/204]	Loss 0.3485 (0.4117)	
training:	Epoch: [105][116/204]	Loss 0.2626 (0.4104)	
training:	Epoch: [105][117/204]	Loss 0.3354 (0.4098)	
training:	Epoch: [105][118/204]	Loss 0.4328 (0.4100)	
training:	Epoch: [105][119/204]	Loss 0.2390 (0.4085)	
training:	Epoch: [105][120/204]	Loss 0.3679 (0.4082)	
training:	Epoch: [105][121/204]	Loss 0.4968 (0.4089)	
training:	Epoch: [105][122/204]	Loss 0.3829 (0.4087)	
training:	Epoch: [105][123/204]	Loss 0.5079 (0.4095)	
training:	Epoch: [105][124/204]	Loss 0.3730 (0.4092)	
training:	Epoch: [105][125/204]	Loss 0.3095 (0.4084)	
training:	Epoch: [105][126/204]	Loss 0.4311 (0.4086)	
training:	Epoch: [105][127/204]	Loss 0.4326 (0.4088)	
training:	Epoch: [105][128/204]	Loss 0.3733 (0.4085)	
training:	Epoch: [105][129/204]	Loss 0.3946 (0.4084)	
training:	Epoch: [105][130/204]	Loss 0.5695 (0.4097)	
training:	Epoch: [105][131/204]	Loss 0.2976 (0.4088)	
training:	Epoch: [105][132/204]	Loss 0.4188 (0.4089)	
training:	Epoch: [105][133/204]	Loss 0.5787 (0.4102)	
training:	Epoch: [105][134/204]	Loss 0.4496 (0.4104)	
training:	Epoch: [105][135/204]	Loss 0.3635 (0.4101)	
training:	Epoch: [105][136/204]	Loss 0.4372 (0.4103)	
training:	Epoch: [105][137/204]	Loss 0.3308 (0.4097)	
training:	Epoch: [105][138/204]	Loss 0.5981 (0.4111)	
training:	Epoch: [105][139/204]	Loss 0.4536 (0.4114)	
training:	Epoch: [105][140/204]	Loss 0.4551 (0.4117)	
training:	Epoch: [105][141/204]	Loss 0.4358 (0.4119)	
training:	Epoch: [105][142/204]	Loss 0.4529 (0.4122)	
training:	Epoch: [105][143/204]	Loss 0.3680 (0.4119)	
training:	Epoch: [105][144/204]	Loss 0.3761 (0.4116)	
training:	Epoch: [105][145/204]	Loss 0.5323 (0.4124)	
training:	Epoch: [105][146/204]	Loss 0.3797 (0.4122)	
training:	Epoch: [105][147/204]	Loss 0.4866 (0.4127)	
training:	Epoch: [105][148/204]	Loss 0.2962 (0.4119)	
training:	Epoch: [105][149/204]	Loss 0.5039 (0.4125)	
training:	Epoch: [105][150/204]	Loss 0.3385 (0.4121)	
training:	Epoch: [105][151/204]	Loss 0.2454 (0.4110)	
training:	Epoch: [105][152/204]	Loss 0.3735 (0.4107)	
training:	Epoch: [105][153/204]	Loss 0.4601 (0.4110)	
training:	Epoch: [105][154/204]	Loss 0.4449 (0.4112)	
training:	Epoch: [105][155/204]	Loss 0.3667 (0.4110)	
training:	Epoch: [105][156/204]	Loss 0.5011 (0.4115)	
training:	Epoch: [105][157/204]	Loss 0.3319 (0.4110)	
training:	Epoch: [105][158/204]	Loss 0.3378 (0.4106)	
training:	Epoch: [105][159/204]	Loss 0.5645 (0.4115)	
training:	Epoch: [105][160/204]	Loss 0.5321 (0.4123)	
training:	Epoch: [105][161/204]	Loss 0.4771 (0.4127)	
training:	Epoch: [105][162/204]	Loss 0.4197 (0.4127)	
training:	Epoch: [105][163/204]	Loss 0.5287 (0.4134)	
training:	Epoch: [105][164/204]	Loss 0.4211 (0.4135)	
training:	Epoch: [105][165/204]	Loss 0.5935 (0.4146)	
training:	Epoch: [105][166/204]	Loss 0.3114 (0.4140)	
training:	Epoch: [105][167/204]	Loss 0.2669 (0.4131)	
training:	Epoch: [105][168/204]	Loss 0.2713 (0.4122)	
training:	Epoch: [105][169/204]	Loss 0.4240 (0.4123)	
training:	Epoch: [105][170/204]	Loss 0.3555 (0.4120)	
training:	Epoch: [105][171/204]	Loss 0.3649 (0.4117)	
training:	Epoch: [105][172/204]	Loss 0.5136 (0.4123)	
training:	Epoch: [105][173/204]	Loss 0.6423 (0.4136)	
training:	Epoch: [105][174/204]	Loss 0.5727 (0.4145)	
training:	Epoch: [105][175/204]	Loss 0.4662 (0.4148)	
training:	Epoch: [105][176/204]	Loss 0.1789 (0.4135)	
training:	Epoch: [105][177/204]	Loss 0.4226 (0.4135)	
training:	Epoch: [105][178/204]	Loss 0.5331 (0.4142)	
training:	Epoch: [105][179/204]	Loss 0.4491 (0.4144)	
training:	Epoch: [105][180/204]	Loss 0.3867 (0.4143)	
training:	Epoch: [105][181/204]	Loss 0.5052 (0.4148)	
training:	Epoch: [105][182/204]	Loss 0.3939 (0.4146)	
training:	Epoch: [105][183/204]	Loss 0.4403 (0.4148)	
training:	Epoch: [105][184/204]	Loss 0.3421 (0.4144)	
training:	Epoch: [105][185/204]	Loss 0.3765 (0.4142)	
training:	Epoch: [105][186/204]	Loss 0.6720 (0.4156)	
training:	Epoch: [105][187/204]	Loss 0.4329 (0.4157)	
training:	Epoch: [105][188/204]	Loss 0.3440 (0.4153)	
training:	Epoch: [105][189/204]	Loss 0.6734 (0.4166)	
training:	Epoch: [105][190/204]	Loss 0.3805 (0.4165)	
training:	Epoch: [105][191/204]	Loss 0.4623 (0.4167)	
training:	Epoch: [105][192/204]	Loss 0.6341 (0.4178)	
training:	Epoch: [105][193/204]	Loss 0.3366 (0.4174)	
training:	Epoch: [105][194/204]	Loss 0.4042 (0.4173)	
training:	Epoch: [105][195/204]	Loss 0.3933 (0.4172)	
training:	Epoch: [105][196/204]	Loss 0.2894 (0.4166)	
training:	Epoch: [105][197/204]	Loss 0.1796 (0.4154)	
training:	Epoch: [105][198/204]	Loss 0.3697 (0.4151)	
training:	Epoch: [105][199/204]	Loss 0.3336 (0.4147)	
training:	Epoch: [105][200/204]	Loss 0.3485 (0.4144)	
training:	Epoch: [105][201/204]	Loss 0.3879 (0.4143)	
training:	Epoch: [105][202/204]	Loss 0.3901 (0.4141)	
training:	Epoch: [105][203/204]	Loss 0.3603 (0.4139)	
training:	Epoch: [105][204/204]	Loss 0.4759 (0.4142)	
Training:	 Loss: 0.4135

Training:	 ACC: 0.8323 0.8327 0.8413 0.8233
Validation:	 ACC: 0.7944 0.7951 0.8096 0.7791
Validation:	 Best_BACC: 0.7973 0.7983 0.8199 0.7747
Validation:	 Loss: 0.4376
Pretraining:	Epoch 106/120
----------
training:	Epoch: [106][1/204]	Loss 0.2877 (0.2877)	
training:	Epoch: [106][2/204]	Loss 0.3199 (0.3038)	
training:	Epoch: [106][3/204]	Loss 0.3694 (0.3257)	
training:	Epoch: [106][4/204]	Loss 0.2543 (0.3078)	
training:	Epoch: [106][5/204]	Loss 0.2981 (0.3059)	
training:	Epoch: [106][6/204]	Loss 0.4237 (0.3255)	
training:	Epoch: [106][7/204]	Loss 0.4139 (0.3381)	
training:	Epoch: [106][8/204]	Loss 0.4633 (0.3538)	
training:	Epoch: [106][9/204]	Loss 0.4020 (0.3592)	
training:	Epoch: [106][10/204]	Loss 0.4820 (0.3714)	
training:	Epoch: [106][11/204]	Loss 0.3462 (0.3691)	
training:	Epoch: [106][12/204]	Loss 0.4512 (0.3760)	
training:	Epoch: [106][13/204]	Loss 0.5614 (0.3902)	
training:	Epoch: [106][14/204]	Loss 0.4472 (0.3943)	
training:	Epoch: [106][15/204]	Loss 0.2912 (0.3874)	
training:	Epoch: [106][16/204]	Loss 0.4305 (0.3901)	
training:	Epoch: [106][17/204]	Loss 0.6232 (0.4038)	
training:	Epoch: [106][18/204]	Loss 0.3251 (0.3995)	
training:	Epoch: [106][19/204]	Loss 0.4864 (0.4040)	
training:	Epoch: [106][20/204]	Loss 0.5994 (0.4138)	
training:	Epoch: [106][21/204]	Loss 0.3764 (0.4120)	
training:	Epoch: [106][22/204]	Loss 0.4861 (0.4154)	
training:	Epoch: [106][23/204]	Loss 0.2877 (0.4098)	
training:	Epoch: [106][24/204]	Loss 0.3387 (0.4069)	
training:	Epoch: [106][25/204]	Loss 0.4537 (0.4087)	
training:	Epoch: [106][26/204]	Loss 0.1968 (0.4006)	
training:	Epoch: [106][27/204]	Loss 0.3895 (0.4002)	
training:	Epoch: [106][28/204]	Loss 0.5488 (0.4055)	
training:	Epoch: [106][29/204]	Loss 0.3962 (0.4052)	
training:	Epoch: [106][30/204]	Loss 0.3013 (0.4017)	
training:	Epoch: [106][31/204]	Loss 0.4160 (0.4022)	
training:	Epoch: [106][32/204]	Loss 0.3441 (0.4004)	
training:	Epoch: [106][33/204]	Loss 0.4664 (0.4024)	
training:	Epoch: [106][34/204]	Loss 0.4306 (0.4032)	
training:	Epoch: [106][35/204]	Loss 0.3727 (0.4023)	
training:	Epoch: [106][36/204]	Loss 0.4606 (0.4039)	
training:	Epoch: [106][37/204]	Loss 0.3061 (0.4013)	
training:	Epoch: [106][38/204]	Loss 0.4129 (0.4016)	
training:	Epoch: [106][39/204]	Loss 0.4178 (0.4020)	
training:	Epoch: [106][40/204]	Loss 0.3289 (0.4002)	
training:	Epoch: [106][41/204]	Loss 0.3941 (0.4000)	
training:	Epoch: [106][42/204]	Loss 0.3812 (0.3996)	
training:	Epoch: [106][43/204]	Loss 0.3108 (0.3975)	
training:	Epoch: [106][44/204]	Loss 0.2904 (0.3951)	
training:	Epoch: [106][45/204]	Loss 0.4189 (0.3956)	
training:	Epoch: [106][46/204]	Loss 0.3455 (0.3945)	
training:	Epoch: [106][47/204]	Loss 0.5151 (0.3971)	
training:	Epoch: [106][48/204]	Loss 0.3686 (0.3965)	
training:	Epoch: [106][49/204]	Loss 0.3855 (0.3963)	
training:	Epoch: [106][50/204]	Loss 0.3782 (0.3959)	
training:	Epoch: [106][51/204]	Loss 0.4444 (0.3969)	
training:	Epoch: [106][52/204]	Loss 0.4534 (0.3979)	
training:	Epoch: [106][53/204]	Loss 0.5505 (0.4008)	
training:	Epoch: [106][54/204]	Loss 0.4099 (0.4010)	
training:	Epoch: [106][55/204]	Loss 0.4664 (0.4022)	
training:	Epoch: [106][56/204]	Loss 0.4948 (0.4038)	
training:	Epoch: [106][57/204]	Loss 0.3963 (0.4037)	
training:	Epoch: [106][58/204]	Loss 0.3777 (0.4033)	
training:	Epoch: [106][59/204]	Loss 0.4874 (0.4047)	
training:	Epoch: [106][60/204]	Loss 0.3333 (0.4035)	
training:	Epoch: [106][61/204]	Loss 0.5190 (0.4054)	
training:	Epoch: [106][62/204]	Loss 0.3186 (0.4040)	
training:	Epoch: [106][63/204]	Loss 0.5305 (0.4060)	
training:	Epoch: [106][64/204]	Loss 0.4915 (0.4073)	
training:	Epoch: [106][65/204]	Loss 0.4511 (0.4080)	
training:	Epoch: [106][66/204]	Loss 0.4879 (0.4092)	
training:	Epoch: [106][67/204]	Loss 0.4079 (0.4092)	
training:	Epoch: [106][68/204]	Loss 0.5328 (0.4110)	
training:	Epoch: [106][69/204]	Loss 0.2816 (0.4091)	
training:	Epoch: [106][70/204]	Loss 0.3916 (0.4089)	
training:	Epoch: [106][71/204]	Loss 0.3761 (0.4084)	
training:	Epoch: [106][72/204]	Loss 0.4199 (0.4086)	
training:	Epoch: [106][73/204]	Loss 0.4132 (0.4087)	
training:	Epoch: [106][74/204]	Loss 0.4207 (0.4088)	
training:	Epoch: [106][75/204]	Loss 0.3902 (0.4086)	
training:	Epoch: [106][76/204]	Loss 0.5147 (0.4100)	
training:	Epoch: [106][77/204]	Loss 0.3098 (0.4087)	
training:	Epoch: [106][78/204]	Loss 0.4614 (0.4093)	
training:	Epoch: [106][79/204]	Loss 0.3957 (0.4092)	
training:	Epoch: [106][80/204]	Loss 0.4877 (0.4101)	
training:	Epoch: [106][81/204]	Loss 0.5911 (0.4124)	
training:	Epoch: [106][82/204]	Loss 0.3146 (0.4112)	
training:	Epoch: [106][83/204]	Loss 0.4148 (0.4112)	
training:	Epoch: [106][84/204]	Loss 0.4181 (0.4113)	
training:	Epoch: [106][85/204]	Loss 0.3747 (0.4109)	
training:	Epoch: [106][86/204]	Loss 0.3727 (0.4104)	
training:	Epoch: [106][87/204]	Loss 0.4436 (0.4108)	
training:	Epoch: [106][88/204]	Loss 0.4680 (0.4115)	
training:	Epoch: [106][89/204]	Loss 0.3506 (0.4108)	
training:	Epoch: [106][90/204]	Loss 0.2876 (0.4094)	
training:	Epoch: [106][91/204]	Loss 0.5790 (0.4113)	
training:	Epoch: [106][92/204]	Loss 0.4920 (0.4122)	
training:	Epoch: [106][93/204]	Loss 0.6081 (0.4143)	
training:	Epoch: [106][94/204]	Loss 0.4420 (0.4146)	
training:	Epoch: [106][95/204]	Loss 0.3819 (0.4142)	
training:	Epoch: [106][96/204]	Loss 0.2727 (0.4127)	
training:	Epoch: [106][97/204]	Loss 0.4049 (0.4127)	
training:	Epoch: [106][98/204]	Loss 0.3558 (0.4121)	
training:	Epoch: [106][99/204]	Loss 0.6439 (0.4144)	
training:	Epoch: [106][100/204]	Loss 0.3478 (0.4138)	
training:	Epoch: [106][101/204]	Loss 0.3859 (0.4135)	
training:	Epoch: [106][102/204]	Loss 0.4051 (0.4134)	
training:	Epoch: [106][103/204]	Loss 0.2968 (0.4123)	
training:	Epoch: [106][104/204]	Loss 0.2507 (0.4107)	
training:	Epoch: [106][105/204]	Loss 0.4964 (0.4115)	
training:	Epoch: [106][106/204]	Loss 0.5167 (0.4125)	
training:	Epoch: [106][107/204]	Loss 0.3831 (0.4122)	
training:	Epoch: [106][108/204]	Loss 0.3232 (0.4114)	
training:	Epoch: [106][109/204]	Loss 0.3732 (0.4111)	
training:	Epoch: [106][110/204]	Loss 0.2880 (0.4100)	
training:	Epoch: [106][111/204]	Loss 0.5369 (0.4111)	
training:	Epoch: [106][112/204]	Loss 0.5691 (0.4125)	
training:	Epoch: [106][113/204]	Loss 0.4290 (0.4127)	
training:	Epoch: [106][114/204]	Loss 0.6203 (0.4145)	
training:	Epoch: [106][115/204]	Loss 0.3783 (0.4142)	
training:	Epoch: [106][116/204]	Loss 0.4936 (0.4148)	
training:	Epoch: [106][117/204]	Loss 0.3426 (0.4142)	
training:	Epoch: [106][118/204]	Loss 0.3847 (0.4140)	
training:	Epoch: [106][119/204]	Loss 0.2231 (0.4124)	
training:	Epoch: [106][120/204]	Loss 0.5039 (0.4131)	
training:	Epoch: [106][121/204]	Loss 0.2952 (0.4122)	
training:	Epoch: [106][122/204]	Loss 0.3992 (0.4121)	
training:	Epoch: [106][123/204]	Loss 0.3920 (0.4119)	
training:	Epoch: [106][124/204]	Loss 0.2825 (0.4108)	
training:	Epoch: [106][125/204]	Loss 0.6223 (0.4125)	
training:	Epoch: [106][126/204]	Loss 0.5060 (0.4133)	
training:	Epoch: [106][127/204]	Loss 0.4277 (0.4134)	
training:	Epoch: [106][128/204]	Loss 0.4562 (0.4137)	
training:	Epoch: [106][129/204]	Loss 0.4293 (0.4138)	
training:	Epoch: [106][130/204]	Loss 0.3752 (0.4136)	
training:	Epoch: [106][131/204]	Loss 0.3782 (0.4133)	
training:	Epoch: [106][132/204]	Loss 0.4050 (0.4132)	
training:	Epoch: [106][133/204]	Loss 0.4065 (0.4132)	
training:	Epoch: [106][134/204]	Loss 0.2422 (0.4119)	
training:	Epoch: [106][135/204]	Loss 0.3334 (0.4113)	
training:	Epoch: [106][136/204]	Loss 0.5012 (0.4120)	
training:	Epoch: [106][137/204]	Loss 0.4157 (0.4120)	
training:	Epoch: [106][138/204]	Loss 0.5012 (0.4126)	
training:	Epoch: [106][139/204]	Loss 0.3539 (0.4122)	
training:	Epoch: [106][140/204]	Loss 0.4805 (0.4127)	
training:	Epoch: [106][141/204]	Loss 0.4097 (0.4127)	
training:	Epoch: [106][142/204]	Loss 0.4378 (0.4129)	
training:	Epoch: [106][143/204]	Loss 0.2554 (0.4118)	
training:	Epoch: [106][144/204]	Loss 0.3281 (0.4112)	
training:	Epoch: [106][145/204]	Loss 0.3921 (0.4111)	
training:	Epoch: [106][146/204]	Loss 0.3727 (0.4108)	
training:	Epoch: [106][147/204]	Loss 0.4964 (0.4114)	
training:	Epoch: [106][148/204]	Loss 0.3583 (0.4110)	
training:	Epoch: [106][149/204]	Loss 0.4741 (0.4114)	
training:	Epoch: [106][150/204]	Loss 0.4667 (0.4118)	
training:	Epoch: [106][151/204]	Loss 0.3085 (0.4111)	
training:	Epoch: [106][152/204]	Loss 0.6227 (0.4125)	
training:	Epoch: [106][153/204]	Loss 0.5575 (0.4135)	
training:	Epoch: [106][154/204]	Loss 0.6496 (0.4150)	
training:	Epoch: [106][155/204]	Loss 0.4252 (0.4151)	
training:	Epoch: [106][156/204]	Loss 0.4182 (0.4151)	
training:	Epoch: [106][157/204]	Loss 0.4145 (0.4151)	
training:	Epoch: [106][158/204]	Loss 0.3170 (0.4145)	
training:	Epoch: [106][159/204]	Loss 0.4606 (0.4147)	
training:	Epoch: [106][160/204]	Loss 0.3665 (0.4144)	
training:	Epoch: [106][161/204]	Loss 0.4267 (0.4145)	
training:	Epoch: [106][162/204]	Loss 0.4284 (0.4146)	
training:	Epoch: [106][163/204]	Loss 0.3867 (0.4144)	
training:	Epoch: [106][164/204]	Loss 0.3974 (0.4143)	
training:	Epoch: [106][165/204]	Loss 0.7206 (0.4162)	
training:	Epoch: [106][166/204]	Loss 0.2873 (0.4154)	
training:	Epoch: [106][167/204]	Loss 0.3310 (0.4149)	
training:	Epoch: [106][168/204]	Loss 0.3729 (0.4147)	
training:	Epoch: [106][169/204]	Loss 0.2944 (0.4139)	
training:	Epoch: [106][170/204]	Loss 0.4118 (0.4139)	
training:	Epoch: [106][171/204]	Loss 0.4227 (0.4140)	
training:	Epoch: [106][172/204]	Loss 0.3887 (0.4138)	
training:	Epoch: [106][173/204]	Loss 0.3049 (0.4132)	
training:	Epoch: [106][174/204]	Loss 0.4600 (0.4135)	
training:	Epoch: [106][175/204]	Loss 0.2904 (0.4128)	
training:	Epoch: [106][176/204]	Loss 0.4465 (0.4130)	
training:	Epoch: [106][177/204]	Loss 0.5118 (0.4135)	
training:	Epoch: [106][178/204]	Loss 0.2428 (0.4126)	
training:	Epoch: [106][179/204]	Loss 0.3838 (0.4124)	
training:	Epoch: [106][180/204]	Loss 0.3967 (0.4123)	
training:	Epoch: [106][181/204]	Loss 0.4435 (0.4125)	
training:	Epoch: [106][182/204]	Loss 0.4858 (0.4129)	
training:	Epoch: [106][183/204]	Loss 0.3864 (0.4127)	
training:	Epoch: [106][184/204]	Loss 0.5176 (0.4133)	
training:	Epoch: [106][185/204]	Loss 0.4973 (0.4138)	
training:	Epoch: [106][186/204]	Loss 0.3354 (0.4133)	
training:	Epoch: [106][187/204]	Loss 0.3216 (0.4129)	
training:	Epoch: [106][188/204]	Loss 0.4238 (0.4129)	
training:	Epoch: [106][189/204]	Loss 0.3835 (0.4128)	
training:	Epoch: [106][190/204]	Loss 0.4081 (0.4127)	
training:	Epoch: [106][191/204]	Loss 0.2362 (0.4118)	
training:	Epoch: [106][192/204]	Loss 0.4453 (0.4120)	
training:	Epoch: [106][193/204]	Loss 0.3802 (0.4118)	
training:	Epoch: [106][194/204]	Loss 0.3874 (0.4117)	
training:	Epoch: [106][195/204]	Loss 0.3714 (0.4115)	
training:	Epoch: [106][196/204]	Loss 0.4386 (0.4116)	
training:	Epoch: [106][197/204]	Loss 0.3401 (0.4113)	
training:	Epoch: [106][198/204]	Loss 0.4429 (0.4114)	
training:	Epoch: [106][199/204]	Loss 0.3893 (0.4113)	
training:	Epoch: [106][200/204]	Loss 0.2429 (0.4105)	
training:	Epoch: [106][201/204]	Loss 0.5400 (0.4111)	
training:	Epoch: [106][202/204]	Loss 0.4033 (0.4111)	
training:	Epoch: [106][203/204]	Loss 0.2492 (0.4103)	
training:	Epoch: [106][204/204]	Loss 0.3023 (0.4097)	
Training:	 Loss: 0.4091

Training:	 ACC: 0.8313 0.8321 0.8498 0.8128
Validation:	 ACC: 0.7975 0.7988 0.8270 0.7679
Validation:	 Best_BACC: 0.7975 0.7988 0.8270 0.7679
Validation:	 Loss: 0.4374
Pretraining:	Epoch 107/120
----------
training:	Epoch: [107][1/204]	Loss 0.2657 (0.2657)	
training:	Epoch: [107][2/204]	Loss 0.2994 (0.2826)	
training:	Epoch: [107][3/204]	Loss 0.4698 (0.3450)	
training:	Epoch: [107][4/204]	Loss 0.6148 (0.4124)	
training:	Epoch: [107][5/204]	Loss 0.3510 (0.4002)	
training:	Epoch: [107][6/204]	Loss 0.3109 (0.3853)	
training:	Epoch: [107][7/204]	Loss 0.4076 (0.3885)	
training:	Epoch: [107][8/204]	Loss 0.5442 (0.4079)	
training:	Epoch: [107][9/204]	Loss 0.4353 (0.4110)	
training:	Epoch: [107][10/204]	Loss 0.4115 (0.4110)	
training:	Epoch: [107][11/204]	Loss 0.3139 (0.4022)	
training:	Epoch: [107][12/204]	Loss 0.5151 (0.4116)	
training:	Epoch: [107][13/204]	Loss 0.5334 (0.4210)	
training:	Epoch: [107][14/204]	Loss 0.2952 (0.4120)	
training:	Epoch: [107][15/204]	Loss 0.4430 (0.4141)	
training:	Epoch: [107][16/204]	Loss 0.4079 (0.4137)	
training:	Epoch: [107][17/204]	Loss 0.3413 (0.4094)	
training:	Epoch: [107][18/204]	Loss 0.4344 (0.4108)	
training:	Epoch: [107][19/204]	Loss 0.5370 (0.4174)	
training:	Epoch: [107][20/204]	Loss 0.5507 (0.4241)	
training:	Epoch: [107][21/204]	Loss 0.5165 (0.4285)	
training:	Epoch: [107][22/204]	Loss 0.4550 (0.4297)	
training:	Epoch: [107][23/204]	Loss 0.3859 (0.4278)	
training:	Epoch: [107][24/204]	Loss 0.4307 (0.4279)	
training:	Epoch: [107][25/204]	Loss 0.4083 (0.4271)	
training:	Epoch: [107][26/204]	Loss 0.4191 (0.4268)	
training:	Epoch: [107][27/204]	Loss 0.3001 (0.4221)	
training:	Epoch: [107][28/204]	Loss 0.1942 (0.4140)	
training:	Epoch: [107][29/204]	Loss 0.4884 (0.4166)	
training:	Epoch: [107][30/204]	Loss 0.3682 (0.4150)	
training:	Epoch: [107][31/204]	Loss 0.4841 (0.4172)	
training:	Epoch: [107][32/204]	Loss 0.3415 (0.4148)	
training:	Epoch: [107][33/204]	Loss 0.4666 (0.4164)	
training:	Epoch: [107][34/204]	Loss 0.2640 (0.4119)	
training:	Epoch: [107][35/204]	Loss 0.4231 (0.4122)	
training:	Epoch: [107][36/204]	Loss 0.3891 (0.4116)	
training:	Epoch: [107][37/204]	Loss 0.3334 (0.4095)	
training:	Epoch: [107][38/204]	Loss 0.3794 (0.4087)	
training:	Epoch: [107][39/204]	Loss 0.3719 (0.4077)	
training:	Epoch: [107][40/204]	Loss 0.5555 (0.4114)	
training:	Epoch: [107][41/204]	Loss 0.4800 (0.4131)	
training:	Epoch: [107][42/204]	Loss 0.4246 (0.4134)	
training:	Epoch: [107][43/204]	Loss 0.4143 (0.4134)	
training:	Epoch: [107][44/204]	Loss 0.2762 (0.4103)	
training:	Epoch: [107][45/204]	Loss 0.5405 (0.4132)	
training:	Epoch: [107][46/204]	Loss 0.2825 (0.4103)	
training:	Epoch: [107][47/204]	Loss 0.3975 (0.4101)	
training:	Epoch: [107][48/204]	Loss 0.3178 (0.4081)	
training:	Epoch: [107][49/204]	Loss 0.3848 (0.4077)	
training:	Epoch: [107][50/204]	Loss 0.3491 (0.4065)	
training:	Epoch: [107][51/204]	Loss 0.4735 (0.4078)	
training:	Epoch: [107][52/204]	Loss 0.4182 (0.4080)	
training:	Epoch: [107][53/204]	Loss 0.2293 (0.4046)	
training:	Epoch: [107][54/204]	Loss 0.4363 (0.4052)	
training:	Epoch: [107][55/204]	Loss 0.5747 (0.4083)	
training:	Epoch: [107][56/204]	Loss 0.4679 (0.4094)	
training:	Epoch: [107][57/204]	Loss 0.4818 (0.4106)	
training:	Epoch: [107][58/204]	Loss 0.4099 (0.4106)	
training:	Epoch: [107][59/204]	Loss 0.4605 (0.4115)	
training:	Epoch: [107][60/204]	Loss 0.3141 (0.4098)	
training:	Epoch: [107][61/204]	Loss 0.5129 (0.4115)	
training:	Epoch: [107][62/204]	Loss 0.4627 (0.4124)	
training:	Epoch: [107][63/204]	Loss 0.3526 (0.4114)	
training:	Epoch: [107][64/204]	Loss 0.2576 (0.4090)	
training:	Epoch: [107][65/204]	Loss 0.4765 (0.4101)	
training:	Epoch: [107][66/204]	Loss 0.3264 (0.4088)	
training:	Epoch: [107][67/204]	Loss 0.4371 (0.4092)	
training:	Epoch: [107][68/204]	Loss 0.5000 (0.4105)	
training:	Epoch: [107][69/204]	Loss 0.2749 (0.4086)	
training:	Epoch: [107][70/204]	Loss 0.6339 (0.4118)	
training:	Epoch: [107][71/204]	Loss 0.2548 (0.4096)	
training:	Epoch: [107][72/204]	Loss 0.3886 (0.4093)	
training:	Epoch: [107][73/204]	Loss 0.3240 (0.4081)	
training:	Epoch: [107][74/204]	Loss 0.2272 (0.4057)	
training:	Epoch: [107][75/204]	Loss 0.4908 (0.4068)	
training:	Epoch: [107][76/204]	Loss 0.5467 (0.4087)	
training:	Epoch: [107][77/204]	Loss 0.4458 (0.4091)	
training:	Epoch: [107][78/204]	Loss 0.3854 (0.4088)	
training:	Epoch: [107][79/204]	Loss 0.3305 (0.4078)	
training:	Epoch: [107][80/204]	Loss 0.3310 (0.4069)	
training:	Epoch: [107][81/204]	Loss 0.3482 (0.4062)	
training:	Epoch: [107][82/204]	Loss 0.3147 (0.4050)	
training:	Epoch: [107][83/204]	Loss 0.5465 (0.4067)	
training:	Epoch: [107][84/204]	Loss 0.3395 (0.4059)	
training:	Epoch: [107][85/204]	Loss 0.4303 (0.4062)	
training:	Epoch: [107][86/204]	Loss 0.3646 (0.4057)	
training:	Epoch: [107][87/204]	Loss 0.4651 (0.4064)	
training:	Epoch: [107][88/204]	Loss 0.5782 (0.4084)	
training:	Epoch: [107][89/204]	Loss 0.5053 (0.4095)	
training:	Epoch: [107][90/204]	Loss 0.3216 (0.4085)	
training:	Epoch: [107][91/204]	Loss 0.5062 (0.4096)	
training:	Epoch: [107][92/204]	Loss 0.3319 (0.4087)	
training:	Epoch: [107][93/204]	Loss 0.4825 (0.4095)	
training:	Epoch: [107][94/204]	Loss 0.3859 (0.4093)	
training:	Epoch: [107][95/204]	Loss 0.3643 (0.4088)	
training:	Epoch: [107][96/204]	Loss 0.4360 (0.4091)	
training:	Epoch: [107][97/204]	Loss 0.2664 (0.4076)	
training:	Epoch: [107][98/204]	Loss 0.3764 (0.4073)	
training:	Epoch: [107][99/204]	Loss 0.3718 (0.4069)	
training:	Epoch: [107][100/204]	Loss 0.3589 (0.4065)	
training:	Epoch: [107][101/204]	Loss 0.6435 (0.4088)	
training:	Epoch: [107][102/204]	Loss 0.2614 (0.4074)	
training:	Epoch: [107][103/204]	Loss 0.5436 (0.4087)	
training:	Epoch: [107][104/204]	Loss 0.4123 (0.4087)	
training:	Epoch: [107][105/204]	Loss 0.4486 (0.4091)	
training:	Epoch: [107][106/204]	Loss 0.4319 (0.4093)	
training:	Epoch: [107][107/204]	Loss 0.4097 (0.4093)	
training:	Epoch: [107][108/204]	Loss 0.4351 (0.4095)	
training:	Epoch: [107][109/204]	Loss 0.4295 (0.4097)	
training:	Epoch: [107][110/204]	Loss 0.4041 (0.4097)	
training:	Epoch: [107][111/204]	Loss 0.3263 (0.4089)	
training:	Epoch: [107][112/204]	Loss 0.4784 (0.4095)	
training:	Epoch: [107][113/204]	Loss 0.2651 (0.4083)	
training:	Epoch: [107][114/204]	Loss 0.5023 (0.4091)	
training:	Epoch: [107][115/204]	Loss 0.3594 (0.4087)	
training:	Epoch: [107][116/204]	Loss 0.4978 (0.4094)	
training:	Epoch: [107][117/204]	Loss 0.3878 (0.4092)	
training:	Epoch: [107][118/204]	Loss 0.3764 (0.4090)	
training:	Epoch: [107][119/204]	Loss 0.4893 (0.4096)	
training:	Epoch: [107][120/204]	Loss 0.4112 (0.4097)	
training:	Epoch: [107][121/204]	Loss 0.2632 (0.4084)	
training:	Epoch: [107][122/204]	Loss 0.3655 (0.4081)	
training:	Epoch: [107][123/204]	Loss 0.3641 (0.4077)	
training:	Epoch: [107][124/204]	Loss 0.5220 (0.4087)	
training:	Epoch: [107][125/204]	Loss 0.2489 (0.4074)	
training:	Epoch: [107][126/204]	Loss 0.3219 (0.4067)	
training:	Epoch: [107][127/204]	Loss 0.4197 (0.4068)	
training:	Epoch: [107][128/204]	Loss 0.3288 (0.4062)	
training:	Epoch: [107][129/204]	Loss 0.4414 (0.4065)	
training:	Epoch: [107][130/204]	Loss 0.3563 (0.4061)	
training:	Epoch: [107][131/204]	Loss 0.4153 (0.4062)	
training:	Epoch: [107][132/204]	Loss 0.5658 (0.4074)	
training:	Epoch: [107][133/204]	Loss 0.2710 (0.4063)	
training:	Epoch: [107][134/204]	Loss 0.2016 (0.4048)	
training:	Epoch: [107][135/204]	Loss 0.6039 (0.4063)	
training:	Epoch: [107][136/204]	Loss 0.4171 (0.4064)	
training:	Epoch: [107][137/204]	Loss 0.3567 (0.4060)	
training:	Epoch: [107][138/204]	Loss 0.2939 (0.4052)	
training:	Epoch: [107][139/204]	Loss 0.2849 (0.4043)	
training:	Epoch: [107][140/204]	Loss 0.3590 (0.4040)	
training:	Epoch: [107][141/204]	Loss 0.3920 (0.4039)	
training:	Epoch: [107][142/204]	Loss 0.5039 (0.4046)	
training:	Epoch: [107][143/204]	Loss 0.2526 (0.4036)	
training:	Epoch: [107][144/204]	Loss 0.5779 (0.4048)	
training:	Epoch: [107][145/204]	Loss 0.3915 (0.4047)	
training:	Epoch: [107][146/204]	Loss 0.4923 (0.4053)	
training:	Epoch: [107][147/204]	Loss 0.4249 (0.4054)	
training:	Epoch: [107][148/204]	Loss 0.5068 (0.4061)	
training:	Epoch: [107][149/204]	Loss 0.3784 (0.4059)	
training:	Epoch: [107][150/204]	Loss 0.3762 (0.4057)	
training:	Epoch: [107][151/204]	Loss 0.3244 (0.4052)	
training:	Epoch: [107][152/204]	Loss 0.3536 (0.4048)	
training:	Epoch: [107][153/204]	Loss 0.2804 (0.4040)	
training:	Epoch: [107][154/204]	Loss 0.4484 (0.4043)	
training:	Epoch: [107][155/204]	Loss 0.3260 (0.4038)	
training:	Epoch: [107][156/204]	Loss 0.4479 (0.4041)	
training:	Epoch: [107][157/204]	Loss 0.4606 (0.4044)	
training:	Epoch: [107][158/204]	Loss 0.3865 (0.4043)	
training:	Epoch: [107][159/204]	Loss 0.2976 (0.4037)	
training:	Epoch: [107][160/204]	Loss 0.3186 (0.4031)	
training:	Epoch: [107][161/204]	Loss 0.3409 (0.4027)	
training:	Epoch: [107][162/204]	Loss 0.5562 (0.4037)	
training:	Epoch: [107][163/204]	Loss 0.4251 (0.4038)	
training:	Epoch: [107][164/204]	Loss 0.6486 (0.4053)	
training:	Epoch: [107][165/204]	Loss 0.3072 (0.4047)	
training:	Epoch: [107][166/204]	Loss 0.4730 (0.4051)	
training:	Epoch: [107][167/204]	Loss 0.3808 (0.4050)	
training:	Epoch: [107][168/204]	Loss 0.2451 (0.4040)	
training:	Epoch: [107][169/204]	Loss 0.5800 (0.4051)	
training:	Epoch: [107][170/204]	Loss 0.3783 (0.4049)	
training:	Epoch: [107][171/204]	Loss 0.3789 (0.4048)	
training:	Epoch: [107][172/204]	Loss 0.4595 (0.4051)	
training:	Epoch: [107][173/204]	Loss 0.5730 (0.4061)	
training:	Epoch: [107][174/204]	Loss 0.3468 (0.4057)	
training:	Epoch: [107][175/204]	Loss 0.2945 (0.4051)	
training:	Epoch: [107][176/204]	Loss 0.2768 (0.4043)	
training:	Epoch: [107][177/204]	Loss 0.5729 (0.4053)	
training:	Epoch: [107][178/204]	Loss 0.6872 (0.4069)	
training:	Epoch: [107][179/204]	Loss 0.4165 (0.4069)	
training:	Epoch: [107][180/204]	Loss 0.4755 (0.4073)	
training:	Epoch: [107][181/204]	Loss 0.3799 (0.4072)	
training:	Epoch: [107][182/204]	Loss 0.2642 (0.4064)	
training:	Epoch: [107][183/204]	Loss 0.1652 (0.4051)	
training:	Epoch: [107][184/204]	Loss 0.5160 (0.4057)	
training:	Epoch: [107][185/204]	Loss 0.3380 (0.4053)	
training:	Epoch: [107][186/204]	Loss 0.4072 (0.4053)	
training:	Epoch: [107][187/204]	Loss 0.6302 (0.4065)	
training:	Epoch: [107][188/204]	Loss 0.4420 (0.4067)	
training:	Epoch: [107][189/204]	Loss 0.5275 (0.4073)	
training:	Epoch: [107][190/204]	Loss 0.4831 (0.4077)	
training:	Epoch: [107][191/204]	Loss 0.3563 (0.4075)	
training:	Epoch: [107][192/204]	Loss 0.5624 (0.4083)	
training:	Epoch: [107][193/204]	Loss 0.2978 (0.4077)	
training:	Epoch: [107][194/204]	Loss 0.4327 (0.4078)	
training:	Epoch: [107][195/204]	Loss 0.3592 (0.4076)	
training:	Epoch: [107][196/204]	Loss 0.4151 (0.4076)	
training:	Epoch: [107][197/204]	Loss 0.2949 (0.4070)	
training:	Epoch: [107][198/204]	Loss 0.6577 (0.4083)	
training:	Epoch: [107][199/204]	Loss 0.5128 (0.4088)	
training:	Epoch: [107][200/204]	Loss 0.3886 (0.4087)	
training:	Epoch: [107][201/204]	Loss 0.5194 (0.4093)	
training:	Epoch: [107][202/204]	Loss 0.3509 (0.4090)	
training:	Epoch: [107][203/204]	Loss 0.5092 (0.4095)	
training:	Epoch: [107][204/204]	Loss 0.2189 (0.4086)	
Training:	 Loss: 0.4079

Training:	 ACC: 0.8319 0.8324 0.8439 0.8198
Validation:	 ACC: 0.7958 0.7967 0.8147 0.7769
Validation:	 Best_BACC: 0.7975 0.7988 0.8270 0.7679
Validation:	 Loss: 0.4364
Pretraining:	Epoch 108/120
----------
training:	Epoch: [108][1/204]	Loss 0.3590 (0.3590)	
training:	Epoch: [108][2/204]	Loss 0.4215 (0.3903)	
training:	Epoch: [108][3/204]	Loss 0.4307 (0.4037)	
training:	Epoch: [108][4/204]	Loss 0.4780 (0.4223)	
training:	Epoch: [108][5/204]	Loss 0.4970 (0.4372)	
training:	Epoch: [108][6/204]	Loss 0.4894 (0.4459)	
training:	Epoch: [108][7/204]	Loss 0.4325 (0.4440)	
training:	Epoch: [108][8/204]	Loss 0.4719 (0.4475)	
training:	Epoch: [108][9/204]	Loss 0.3791 (0.4399)	
training:	Epoch: [108][10/204]	Loss 0.3171 (0.4276)	
training:	Epoch: [108][11/204]	Loss 0.4728 (0.4317)	
training:	Epoch: [108][12/204]	Loss 0.4201 (0.4308)	
training:	Epoch: [108][13/204]	Loss 0.3624 (0.4255)	
training:	Epoch: [108][14/204]	Loss 0.3565 (0.4206)	
training:	Epoch: [108][15/204]	Loss 0.4618 (0.4233)	
training:	Epoch: [108][16/204]	Loss 0.4663 (0.4260)	
training:	Epoch: [108][17/204]	Loss 0.5037 (0.4306)	
training:	Epoch: [108][18/204]	Loss 0.5402 (0.4367)	
training:	Epoch: [108][19/204]	Loss 0.4258 (0.4361)	
training:	Epoch: [108][20/204]	Loss 0.2857 (0.4286)	
training:	Epoch: [108][21/204]	Loss 0.3769 (0.4261)	
training:	Epoch: [108][22/204]	Loss 0.5214 (0.4304)	
training:	Epoch: [108][23/204]	Loss 0.3816 (0.4283)	
training:	Epoch: [108][24/204]	Loss 0.3321 (0.4243)	
training:	Epoch: [108][25/204]	Loss 0.2682 (0.4181)	
training:	Epoch: [108][26/204]	Loss 0.4844 (0.4206)	
training:	Epoch: [108][27/204]	Loss 0.3221 (0.4170)	
training:	Epoch: [108][28/204]	Loss 0.3809 (0.4157)	
training:	Epoch: [108][29/204]	Loss 0.5575 (0.4206)	
training:	Epoch: [108][30/204]	Loss 0.3244 (0.4174)	
training:	Epoch: [108][31/204]	Loss 0.4690 (0.4190)	
training:	Epoch: [108][32/204]	Loss 0.2848 (0.4148)	
training:	Epoch: [108][33/204]	Loss 0.4882 (0.4171)	
training:	Epoch: [108][34/204]	Loss 0.4294 (0.4174)	
training:	Epoch: [108][35/204]	Loss 0.4400 (0.4181)	
training:	Epoch: [108][36/204]	Loss 0.3419 (0.4160)	
training:	Epoch: [108][37/204]	Loss 0.4927 (0.4180)	
training:	Epoch: [108][38/204]	Loss 0.3342 (0.4158)	
training:	Epoch: [108][39/204]	Loss 0.4193 (0.4159)	
training:	Epoch: [108][40/204]	Loss 0.4741 (0.4174)	
training:	Epoch: [108][41/204]	Loss 0.5282 (0.4201)	
training:	Epoch: [108][42/204]	Loss 0.4016 (0.4196)	
training:	Epoch: [108][43/204]	Loss 0.3832 (0.4188)	
training:	Epoch: [108][44/204]	Loss 0.3348 (0.4169)	
training:	Epoch: [108][45/204]	Loss 0.3113 (0.4145)	
training:	Epoch: [108][46/204]	Loss 0.3458 (0.4130)	
training:	Epoch: [108][47/204]	Loss 0.3113 (0.4109)	
training:	Epoch: [108][48/204]	Loss 0.3657 (0.4099)	
training:	Epoch: [108][49/204]	Loss 0.4911 (0.4116)	
training:	Epoch: [108][50/204]	Loss 0.4482 (0.4123)	
training:	Epoch: [108][51/204]	Loss 0.3594 (0.4113)	
training:	Epoch: [108][52/204]	Loss 0.5922 (0.4148)	
training:	Epoch: [108][53/204]	Loss 0.4746 (0.4159)	
training:	Epoch: [108][54/204]	Loss 0.4566 (0.4166)	
training:	Epoch: [108][55/204]	Loss 0.2872 (0.4143)	
training:	Epoch: [108][56/204]	Loss 0.3895 (0.4138)	
training:	Epoch: [108][57/204]	Loss 0.4157 (0.4139)	
training:	Epoch: [108][58/204]	Loss 0.2962 (0.4118)	
training:	Epoch: [108][59/204]	Loss 0.2955 (0.4099)	
training:	Epoch: [108][60/204]	Loss 0.3138 (0.4083)	
training:	Epoch: [108][61/204]	Loss 0.4664 (0.4092)	
training:	Epoch: [108][62/204]	Loss 0.3545 (0.4083)	
training:	Epoch: [108][63/204]	Loss 0.4530 (0.4091)	
training:	Epoch: [108][64/204]	Loss 0.4753 (0.4101)	
training:	Epoch: [108][65/204]	Loss 0.4806 (0.4112)	
training:	Epoch: [108][66/204]	Loss 0.3979 (0.4110)	
training:	Epoch: [108][67/204]	Loss 0.3408 (0.4099)	
training:	Epoch: [108][68/204]	Loss 0.3080 (0.4084)	
training:	Epoch: [108][69/204]	Loss 0.3959 (0.4082)	
training:	Epoch: [108][70/204]	Loss 0.2890 (0.4065)	
training:	Epoch: [108][71/204]	Loss 0.4232 (0.4068)	
training:	Epoch: [108][72/204]	Loss 0.3533 (0.4060)	
training:	Epoch: [108][73/204]	Loss 0.3581 (0.4054)	
training:	Epoch: [108][74/204]	Loss 0.3961 (0.4052)	
training:	Epoch: [108][75/204]	Loss 0.5201 (0.4068)	
training:	Epoch: [108][76/204]	Loss 0.3236 (0.4057)	
training:	Epoch: [108][77/204]	Loss 0.4480 (0.4062)	
training:	Epoch: [108][78/204]	Loss 0.3355 (0.4053)	
training:	Epoch: [108][79/204]	Loss 0.3187 (0.4042)	
training:	Epoch: [108][80/204]	Loss 0.3961 (0.4041)	
training:	Epoch: [108][81/204]	Loss 0.5017 (0.4053)	
training:	Epoch: [108][82/204]	Loss 0.4045 (0.4053)	
training:	Epoch: [108][83/204]	Loss 0.5512 (0.4071)	
training:	Epoch: [108][84/204]	Loss 0.3525 (0.4064)	
training:	Epoch: [108][85/204]	Loss 0.4044 (0.4064)	
training:	Epoch: [108][86/204]	Loss 0.6423 (0.4092)	
training:	Epoch: [108][87/204]	Loss 0.3453 (0.4084)	
training:	Epoch: [108][88/204]	Loss 0.3571 (0.4078)	
training:	Epoch: [108][89/204]	Loss 0.5269 (0.4092)	
training:	Epoch: [108][90/204]	Loss 0.4214 (0.4093)	
training:	Epoch: [108][91/204]	Loss 0.5314 (0.4106)	
training:	Epoch: [108][92/204]	Loss 0.4259 (0.4108)	
training:	Epoch: [108][93/204]	Loss 0.5064 (0.4118)	
training:	Epoch: [108][94/204]	Loss 0.5854 (0.4137)	
training:	Epoch: [108][95/204]	Loss 0.5324 (0.4149)	
training:	Epoch: [108][96/204]	Loss 0.3587 (0.4144)	
training:	Epoch: [108][97/204]	Loss 0.5159 (0.4154)	
training:	Epoch: [108][98/204]	Loss 0.3671 (0.4149)	
training:	Epoch: [108][99/204]	Loss 0.3279 (0.4140)	
training:	Epoch: [108][100/204]	Loss 0.4636 (0.4145)	
training:	Epoch: [108][101/204]	Loss 0.4486 (0.4149)	
training:	Epoch: [108][102/204]	Loss 0.4961 (0.4157)	
training:	Epoch: [108][103/204]	Loss 0.3879 (0.4154)	
training:	Epoch: [108][104/204]	Loss 0.4113 (0.4153)	
training:	Epoch: [108][105/204]	Loss 0.4878 (0.4160)	
training:	Epoch: [108][106/204]	Loss 0.4403 (0.4163)	
training:	Epoch: [108][107/204]	Loss 0.3775 (0.4159)	
training:	Epoch: [108][108/204]	Loss 0.5643 (0.4173)	
training:	Epoch: [108][109/204]	Loss 0.3847 (0.4170)	
training:	Epoch: [108][110/204]	Loss 0.4552 (0.4173)	
training:	Epoch: [108][111/204]	Loss 0.5934 (0.4189)	
training:	Epoch: [108][112/204]	Loss 0.3474 (0.4183)	
training:	Epoch: [108][113/204]	Loss 0.3076 (0.4173)	
training:	Epoch: [108][114/204]	Loss 0.3023 (0.4163)	
training:	Epoch: [108][115/204]	Loss 0.4648 (0.4167)	
training:	Epoch: [108][116/204]	Loss 0.3875 (0.4165)	
training:	Epoch: [108][117/204]	Loss 0.4122 (0.4164)	
training:	Epoch: [108][118/204]	Loss 0.3621 (0.4160)	
training:	Epoch: [108][119/204]	Loss 0.3377 (0.4153)	
training:	Epoch: [108][120/204]	Loss 0.2638 (0.4140)	
training:	Epoch: [108][121/204]	Loss 0.2572 (0.4127)	
training:	Epoch: [108][122/204]	Loss 0.4041 (0.4127)	
training:	Epoch: [108][123/204]	Loss 0.3164 (0.4119)	
training:	Epoch: [108][124/204]	Loss 0.2867 (0.4109)	
training:	Epoch: [108][125/204]	Loss 0.3874 (0.4107)	
training:	Epoch: [108][126/204]	Loss 0.4258 (0.4108)	
training:	Epoch: [108][127/204]	Loss 0.4567 (0.4112)	
training:	Epoch: [108][128/204]	Loss 0.3184 (0.4104)	
training:	Epoch: [108][129/204]	Loss 0.3239 (0.4098)	
training:	Epoch: [108][130/204]	Loss 0.3937 (0.4097)	
training:	Epoch: [108][131/204]	Loss 0.3724 (0.4094)	
training:	Epoch: [108][132/204]	Loss 0.3523 (0.4089)	
training:	Epoch: [108][133/204]	Loss 0.3928 (0.4088)	
training:	Epoch: [108][134/204]	Loss 0.3855 (0.4086)	
training:	Epoch: [108][135/204]	Loss 0.4906 (0.4093)	
training:	Epoch: [108][136/204]	Loss 0.3852 (0.4091)	
training:	Epoch: [108][137/204]	Loss 0.3010 (0.4083)	
training:	Epoch: [108][138/204]	Loss 0.3814 (0.4081)	
training:	Epoch: [108][139/204]	Loss 0.4450 (0.4084)	
training:	Epoch: [108][140/204]	Loss 0.3102 (0.4077)	
training:	Epoch: [108][141/204]	Loss 0.2684 (0.4067)	
training:	Epoch: [108][142/204]	Loss 0.4933 (0.4073)	
training:	Epoch: [108][143/204]	Loss 0.6050 (0.4087)	
training:	Epoch: [108][144/204]	Loss 0.4486 (0.4089)	
training:	Epoch: [108][145/204]	Loss 0.3826 (0.4088)	
training:	Epoch: [108][146/204]	Loss 0.4349 (0.4089)	
training:	Epoch: [108][147/204]	Loss 0.3489 (0.4085)	
training:	Epoch: [108][148/204]	Loss 0.4064 (0.4085)	
training:	Epoch: [108][149/204]	Loss 0.3447 (0.4081)	
training:	Epoch: [108][150/204]	Loss 0.4263 (0.4082)	
training:	Epoch: [108][151/204]	Loss 0.4784 (0.4087)	
training:	Epoch: [108][152/204]	Loss 0.5213 (0.4094)	
training:	Epoch: [108][153/204]	Loss 0.4408 (0.4096)	
training:	Epoch: [108][154/204]	Loss 0.2432 (0.4085)	
training:	Epoch: [108][155/204]	Loss 0.5448 (0.4094)	
training:	Epoch: [108][156/204]	Loss 0.3888 (0.4093)	
training:	Epoch: [108][157/204]	Loss 0.4289 (0.4094)	
training:	Epoch: [108][158/204]	Loss 0.5044 (0.4100)	
training:	Epoch: [108][159/204]	Loss 0.3350 (0.4095)	
training:	Epoch: [108][160/204]	Loss 0.4763 (0.4100)	
training:	Epoch: [108][161/204]	Loss 0.5863 (0.4110)	
training:	Epoch: [108][162/204]	Loss 0.4094 (0.4110)	
training:	Epoch: [108][163/204]	Loss 0.5245 (0.4117)	
training:	Epoch: [108][164/204]	Loss 0.2950 (0.4110)	
training:	Epoch: [108][165/204]	Loss 0.3280 (0.4105)	
training:	Epoch: [108][166/204]	Loss 0.4407 (0.4107)	
training:	Epoch: [108][167/204]	Loss 0.4300 (0.4108)	
training:	Epoch: [108][168/204]	Loss 0.5557 (0.4117)	
training:	Epoch: [108][169/204]	Loss 0.4886 (0.4121)	
training:	Epoch: [108][170/204]	Loss 0.3401 (0.4117)	
training:	Epoch: [108][171/204]	Loss 0.3885 (0.4116)	
training:	Epoch: [108][172/204]	Loss 0.3532 (0.4112)	
training:	Epoch: [108][173/204]	Loss 0.4280 (0.4113)	
training:	Epoch: [108][174/204]	Loss 0.3017 (0.4107)	
training:	Epoch: [108][175/204]	Loss 0.4253 (0.4108)	
training:	Epoch: [108][176/204]	Loss 0.4888 (0.4112)	
training:	Epoch: [108][177/204]	Loss 0.4354 (0.4114)	
training:	Epoch: [108][178/204]	Loss 0.5024 (0.4119)	
training:	Epoch: [108][179/204]	Loss 0.2745 (0.4111)	
training:	Epoch: [108][180/204]	Loss 0.3814 (0.4109)	
training:	Epoch: [108][181/204]	Loss 0.3519 (0.4106)	
training:	Epoch: [108][182/204]	Loss 0.6211 (0.4118)	
training:	Epoch: [108][183/204]	Loss 0.3555 (0.4115)	
training:	Epoch: [108][184/204]	Loss 0.2874 (0.4108)	
training:	Epoch: [108][185/204]	Loss 0.3605 (0.4105)	
training:	Epoch: [108][186/204]	Loss 0.3510 (0.4102)	
training:	Epoch: [108][187/204]	Loss 0.4187 (0.4102)	
training:	Epoch: [108][188/204]	Loss 0.3871 (0.4101)	
training:	Epoch: [108][189/204]	Loss 0.4149 (0.4101)	
training:	Epoch: [108][190/204]	Loss 0.3321 (0.4097)	
training:	Epoch: [108][191/204]	Loss 0.4459 (0.4099)	
training:	Epoch: [108][192/204]	Loss 0.3094 (0.4094)	
training:	Epoch: [108][193/204]	Loss 0.3560 (0.4091)	
training:	Epoch: [108][194/204]	Loss 0.4407 (0.4093)	
training:	Epoch: [108][195/204]	Loss 0.3957 (0.4092)	
training:	Epoch: [108][196/204]	Loss 0.2379 (0.4083)	
training:	Epoch: [108][197/204]	Loss 0.2972 (0.4078)	
training:	Epoch: [108][198/204]	Loss 0.4669 (0.4081)	
training:	Epoch: [108][199/204]	Loss 0.4408 (0.4082)	
training:	Epoch: [108][200/204]	Loss 0.4552 (0.4085)	
training:	Epoch: [108][201/204]	Loss 0.4987 (0.4089)	
training:	Epoch: [108][202/204]	Loss 0.3242 (0.4085)	
training:	Epoch: [108][203/204]	Loss 0.4034 (0.4085)	
training:	Epoch: [108][204/204]	Loss 0.2231 (0.4076)	
Training:	 Loss: 0.4070

Training:	 ACC: 0.8328 0.8331 0.8419 0.8237
Validation:	 ACC: 0.7949 0.7956 0.8106 0.7791
Validation:	 Best_BACC: 0.7975 0.7988 0.8270 0.7679
Validation:	 Loss: 0.4358
Pretraining:	Epoch 109/120
----------
training:	Epoch: [109][1/204]	Loss 0.5836 (0.5836)	
training:	Epoch: [109][2/204]	Loss 0.3109 (0.4472)	
training:	Epoch: [109][3/204]	Loss 0.4351 (0.4432)	
training:	Epoch: [109][4/204]	Loss 0.4330 (0.4406)	
training:	Epoch: [109][5/204]	Loss 0.1685 (0.3862)	
training:	Epoch: [109][6/204]	Loss 0.3014 (0.3721)	
training:	Epoch: [109][7/204]	Loss 0.4979 (0.3901)	
training:	Epoch: [109][8/204]	Loss 0.2750 (0.3757)	
training:	Epoch: [109][9/204]	Loss 0.4030 (0.3787)	
training:	Epoch: [109][10/204]	Loss 0.5331 (0.3941)	
training:	Epoch: [109][11/204]	Loss 0.4090 (0.3955)	
training:	Epoch: [109][12/204]	Loss 0.4383 (0.3991)	
training:	Epoch: [109][13/204]	Loss 0.5295 (0.4091)	
training:	Epoch: [109][14/204]	Loss 0.4149 (0.4095)	
training:	Epoch: [109][15/204]	Loss 0.4083 (0.4094)	
training:	Epoch: [109][16/204]	Loss 0.5190 (0.4163)	
training:	Epoch: [109][17/204]	Loss 0.5656 (0.4251)	
training:	Epoch: [109][18/204]	Loss 0.4909 (0.4287)	
training:	Epoch: [109][19/204]	Loss 0.4068 (0.4276)	
training:	Epoch: [109][20/204]	Loss 0.3429 (0.4233)	
training:	Epoch: [109][21/204]	Loss 0.4322 (0.4238)	
training:	Epoch: [109][22/204]	Loss 0.3887 (0.4222)	
training:	Epoch: [109][23/204]	Loss 0.4191 (0.4220)	
training:	Epoch: [109][24/204]	Loss 0.4053 (0.4213)	
training:	Epoch: [109][25/204]	Loss 0.3982 (0.4204)	
training:	Epoch: [109][26/204]	Loss 0.4497 (0.4215)	
training:	Epoch: [109][27/204]	Loss 0.4380 (0.4221)	
training:	Epoch: [109][28/204]	Loss 0.5328 (0.4261)	
training:	Epoch: [109][29/204]	Loss 0.3316 (0.4228)	
training:	Epoch: [109][30/204]	Loss 0.6198 (0.4294)	
training:	Epoch: [109][31/204]	Loss 0.5147 (0.4322)	
training:	Epoch: [109][32/204]	Loss 0.5278 (0.4351)	
training:	Epoch: [109][33/204]	Loss 0.4074 (0.4343)	
training:	Epoch: [109][34/204]	Loss 0.2973 (0.4303)	
training:	Epoch: [109][35/204]	Loss 0.3347 (0.4275)	
training:	Epoch: [109][36/204]	Loss 0.4186 (0.4273)	
training:	Epoch: [109][37/204]	Loss 0.3090 (0.4241)	
training:	Epoch: [109][38/204]	Loss 0.4541 (0.4249)	
training:	Epoch: [109][39/204]	Loss 0.3011 (0.4217)	
training:	Epoch: [109][40/204]	Loss 0.3484 (0.4199)	
training:	Epoch: [109][41/204]	Loss 0.5910 (0.4241)	
training:	Epoch: [109][42/204]	Loss 0.2524 (0.4200)	
training:	Epoch: [109][43/204]	Loss 0.6807 (0.4260)	
training:	Epoch: [109][44/204]	Loss 0.4750 (0.4271)	
training:	Epoch: [109][45/204]	Loss 0.2700 (0.4237)	
training:	Epoch: [109][46/204]	Loss 0.3644 (0.4224)	
training:	Epoch: [109][47/204]	Loss 0.3996 (0.4219)	
training:	Epoch: [109][48/204]	Loss 0.3761 (0.4209)	
training:	Epoch: [109][49/204]	Loss 0.3344 (0.4192)	
training:	Epoch: [109][50/204]	Loss 0.4460 (0.4197)	
training:	Epoch: [109][51/204]	Loss 0.3719 (0.4188)	
training:	Epoch: [109][52/204]	Loss 0.4035 (0.4185)	
training:	Epoch: [109][53/204]	Loss 0.3969 (0.4181)	
training:	Epoch: [109][54/204]	Loss 0.1927 (0.4139)	
training:	Epoch: [109][55/204]	Loss 0.3689 (0.4131)	
training:	Epoch: [109][56/204]	Loss 0.3489 (0.4119)	
training:	Epoch: [109][57/204]	Loss 0.3201 (0.4103)	
training:	Epoch: [109][58/204]	Loss 0.4958 (0.4118)	
training:	Epoch: [109][59/204]	Loss 0.4115 (0.4118)	
training:	Epoch: [109][60/204]	Loss 0.3300 (0.4104)	
training:	Epoch: [109][61/204]	Loss 0.3251 (0.4090)	
training:	Epoch: [109][62/204]	Loss 0.3074 (0.4074)	
training:	Epoch: [109][63/204]	Loss 0.6634 (0.4114)	
training:	Epoch: [109][64/204]	Loss 0.2557 (0.4090)	
training:	Epoch: [109][65/204]	Loss 0.4549 (0.4097)	
training:	Epoch: [109][66/204]	Loss 0.3399 (0.4087)	
training:	Epoch: [109][67/204]	Loss 0.5274 (0.4104)	
training:	Epoch: [109][68/204]	Loss 0.4839 (0.4115)	
training:	Epoch: [109][69/204]	Loss 0.2362 (0.4090)	
training:	Epoch: [109][70/204]	Loss 0.2961 (0.4074)	
training:	Epoch: [109][71/204]	Loss 0.3432 (0.4065)	
training:	Epoch: [109][72/204]	Loss 0.3365 (0.4055)	
training:	Epoch: [109][73/204]	Loss 0.4713 (0.4064)	
training:	Epoch: [109][74/204]	Loss 0.5794 (0.4087)	
training:	Epoch: [109][75/204]	Loss 0.2705 (0.4069)	
training:	Epoch: [109][76/204]	Loss 0.3520 (0.4062)	
training:	Epoch: [109][77/204]	Loss 0.5453 (0.4080)	
training:	Epoch: [109][78/204]	Loss 0.3675 (0.4074)	
training:	Epoch: [109][79/204]	Loss 0.4142 (0.4075)	
training:	Epoch: [109][80/204]	Loss 0.3434 (0.4067)	
training:	Epoch: [109][81/204]	Loss 0.4344 (0.4071)	
training:	Epoch: [109][82/204]	Loss 0.4104 (0.4071)	
training:	Epoch: [109][83/204]	Loss 0.3534 (0.4065)	
training:	Epoch: [109][84/204]	Loss 0.5145 (0.4077)	
training:	Epoch: [109][85/204]	Loss 0.5066 (0.4089)	
training:	Epoch: [109][86/204]	Loss 0.5616 (0.4107)	
training:	Epoch: [109][87/204]	Loss 0.4340 (0.4110)	
training:	Epoch: [109][88/204]	Loss 0.5265 (0.4123)	
training:	Epoch: [109][89/204]	Loss 0.3863 (0.4120)	
training:	Epoch: [109][90/204]	Loss 0.2743 (0.4104)	
training:	Epoch: [109][91/204]	Loss 0.4352 (0.4107)	
training:	Epoch: [109][92/204]	Loss 0.2723 (0.4092)	
training:	Epoch: [109][93/204]	Loss 0.3150 (0.4082)	
training:	Epoch: [109][94/204]	Loss 0.5240 (0.4094)	
training:	Epoch: [109][95/204]	Loss 0.7842 (0.4134)	
training:	Epoch: [109][96/204]	Loss 0.4441 (0.4137)	
training:	Epoch: [109][97/204]	Loss 0.5939 (0.4156)	
training:	Epoch: [109][98/204]	Loss 0.2734 (0.4141)	
training:	Epoch: [109][99/204]	Loss 0.3118 (0.4131)	
training:	Epoch: [109][100/204]	Loss 0.3371 (0.4123)	
training:	Epoch: [109][101/204]	Loss 0.2661 (0.4109)	
training:	Epoch: [109][102/204]	Loss 0.3381 (0.4102)	
training:	Epoch: [109][103/204]	Loss 0.2916 (0.4090)	
training:	Epoch: [109][104/204]	Loss 0.4276 (0.4092)	
training:	Epoch: [109][105/204]	Loss 0.3539 (0.4087)	
training:	Epoch: [109][106/204]	Loss 0.3056 (0.4077)	
training:	Epoch: [109][107/204]	Loss 0.3990 (0.4076)	
training:	Epoch: [109][108/204]	Loss 0.3588 (0.4072)	
training:	Epoch: [109][109/204]	Loss 0.4384 (0.4074)	
training:	Epoch: [109][110/204]	Loss 0.3849 (0.4072)	
training:	Epoch: [109][111/204]	Loss 0.3256 (0.4065)	
training:	Epoch: [109][112/204]	Loss 0.3061 (0.4056)	
training:	Epoch: [109][113/204]	Loss 0.3166 (0.4048)	
training:	Epoch: [109][114/204]	Loss 0.4820 (0.4055)	
training:	Epoch: [109][115/204]	Loss 0.4375 (0.4058)	
training:	Epoch: [109][116/204]	Loss 0.4348 (0.4060)	
training:	Epoch: [109][117/204]	Loss 0.4330 (0.4062)	
training:	Epoch: [109][118/204]	Loss 0.3871 (0.4061)	
training:	Epoch: [109][119/204]	Loss 0.2454 (0.4047)	
training:	Epoch: [109][120/204]	Loss 0.5117 (0.4056)	
training:	Epoch: [109][121/204]	Loss 0.3459 (0.4051)	
training:	Epoch: [109][122/204]	Loss 0.3431 (0.4046)	
training:	Epoch: [109][123/204]	Loss 0.4326 (0.4049)	
training:	Epoch: [109][124/204]	Loss 0.3992 (0.4048)	
training:	Epoch: [109][125/204]	Loss 0.4057 (0.4048)	
training:	Epoch: [109][126/204]	Loss 0.4322 (0.4050)	
training:	Epoch: [109][127/204]	Loss 0.3267 (0.4044)	
training:	Epoch: [109][128/204]	Loss 0.4532 (0.4048)	
training:	Epoch: [109][129/204]	Loss 0.3560 (0.4044)	
training:	Epoch: [109][130/204]	Loss 0.4074 (0.4044)	
training:	Epoch: [109][131/204]	Loss 0.4757 (0.4050)	
training:	Epoch: [109][132/204]	Loss 0.3448 (0.4045)	
training:	Epoch: [109][133/204]	Loss 0.3781 (0.4043)	
training:	Epoch: [109][134/204]	Loss 0.5127 (0.4051)	
training:	Epoch: [109][135/204]	Loss 0.5036 (0.4059)	
training:	Epoch: [109][136/204]	Loss 0.4249 (0.4060)	
training:	Epoch: [109][137/204]	Loss 0.4469 (0.4063)	
training:	Epoch: [109][138/204]	Loss 0.3159 (0.4057)	
training:	Epoch: [109][139/204]	Loss 0.4193 (0.4058)	
training:	Epoch: [109][140/204]	Loss 0.5157 (0.4065)	
training:	Epoch: [109][141/204]	Loss 0.4444 (0.4068)	
training:	Epoch: [109][142/204]	Loss 0.4811 (0.4073)	
training:	Epoch: [109][143/204]	Loss 0.4005 (0.4073)	
training:	Epoch: [109][144/204]	Loss 0.4275 (0.4074)	
training:	Epoch: [109][145/204]	Loss 0.5513 (0.4084)	
training:	Epoch: [109][146/204]	Loss 0.4982 (0.4090)	
training:	Epoch: [109][147/204]	Loss 0.3590 (0.4087)	
training:	Epoch: [109][148/204]	Loss 0.3435 (0.4082)	
training:	Epoch: [109][149/204]	Loss 0.3638 (0.4079)	
training:	Epoch: [109][150/204]	Loss 0.4508 (0.4082)	
training:	Epoch: [109][151/204]	Loss 0.5200 (0.4090)	
training:	Epoch: [109][152/204]	Loss 0.3702 (0.4087)	
training:	Epoch: [109][153/204]	Loss 0.3612 (0.4084)	
training:	Epoch: [109][154/204]	Loss 0.2898 (0.4076)	
training:	Epoch: [109][155/204]	Loss 0.3503 (0.4073)	
training:	Epoch: [109][156/204]	Loss 0.4210 (0.4074)	
training:	Epoch: [109][157/204]	Loss 0.3962 (0.4073)	
training:	Epoch: [109][158/204]	Loss 0.4124 (0.4073)	
training:	Epoch: [109][159/204]	Loss 0.5263 (0.4081)	
training:	Epoch: [109][160/204]	Loss 0.2686 (0.4072)	
training:	Epoch: [109][161/204]	Loss 0.3391 (0.4068)	
training:	Epoch: [109][162/204]	Loss 0.5335 (0.4076)	
training:	Epoch: [109][163/204]	Loss 0.4237 (0.4077)	
training:	Epoch: [109][164/204]	Loss 0.4266 (0.4078)	
training:	Epoch: [109][165/204]	Loss 0.5085 (0.4084)	
training:	Epoch: [109][166/204]	Loss 0.4457 (0.4086)	
training:	Epoch: [109][167/204]	Loss 0.3451 (0.4082)	
training:	Epoch: [109][168/204]	Loss 0.4370 (0.4084)	
training:	Epoch: [109][169/204]	Loss 0.2166 (0.4073)	
training:	Epoch: [109][170/204]	Loss 0.3130 (0.4067)	
training:	Epoch: [109][171/204]	Loss 0.3660 (0.4065)	
training:	Epoch: [109][172/204]	Loss 0.3180 (0.4060)	
training:	Epoch: [109][173/204]	Loss 0.4458 (0.4062)	
training:	Epoch: [109][174/204]	Loss 0.6390 (0.4075)	
training:	Epoch: [109][175/204]	Loss 0.5009 (0.4081)	
training:	Epoch: [109][176/204]	Loss 0.3495 (0.4077)	
training:	Epoch: [109][177/204]	Loss 0.3225 (0.4072)	
training:	Epoch: [109][178/204]	Loss 0.3274 (0.4068)	
training:	Epoch: [109][179/204]	Loss 0.4123 (0.4068)	
training:	Epoch: [109][180/204]	Loss 0.4489 (0.4071)	
training:	Epoch: [109][181/204]	Loss 0.4994 (0.4076)	
training:	Epoch: [109][182/204]	Loss 0.3143 (0.4071)	
training:	Epoch: [109][183/204]	Loss 0.4309 (0.4072)	
training:	Epoch: [109][184/204]	Loss 0.5459 (0.4079)	
training:	Epoch: [109][185/204]	Loss 0.3595 (0.4077)	
training:	Epoch: [109][186/204]	Loss 0.4307 (0.4078)	
training:	Epoch: [109][187/204]	Loss 0.4346 (0.4079)	
training:	Epoch: [109][188/204]	Loss 0.5422 (0.4087)	
training:	Epoch: [109][189/204]	Loss 0.4373 (0.4088)	
training:	Epoch: [109][190/204]	Loss 0.3306 (0.4084)	
training:	Epoch: [109][191/204]	Loss 0.3150 (0.4079)	
training:	Epoch: [109][192/204]	Loss 0.2886 (0.4073)	
training:	Epoch: [109][193/204]	Loss 0.4535 (0.4075)	
training:	Epoch: [109][194/204]	Loss 0.3767 (0.4074)	
training:	Epoch: [109][195/204]	Loss 0.4791 (0.4077)	
training:	Epoch: [109][196/204]	Loss 0.3883 (0.4076)	
training:	Epoch: [109][197/204]	Loss 0.4948 (0.4081)	
training:	Epoch: [109][198/204]	Loss 0.4776 (0.4084)	
training:	Epoch: [109][199/204]	Loss 0.4704 (0.4087)	
training:	Epoch: [109][200/204]	Loss 0.2534 (0.4080)	
training:	Epoch: [109][201/204]	Loss 0.3178 (0.4075)	
training:	Epoch: [109][202/204]	Loss 0.3934 (0.4074)	
training:	Epoch: [109][203/204]	Loss 0.5527 (0.4082)	
training:	Epoch: [109][204/204]	Loss 0.3182 (0.4077)	
Training:	 Loss: 0.4071

Training:	 ACC: 0.8327 0.8333 0.8463 0.8192
Validation:	 ACC: 0.7939 0.7951 0.8188 0.7691
Validation:	 Best_BACC: 0.7975 0.7988 0.8270 0.7679
Validation:	 Loss: 0.4354
Pretraining:	Epoch 110/120
----------
training:	Epoch: [110][1/204]	Loss 0.3929 (0.3929)	
training:	Epoch: [110][2/204]	Loss 0.3603 (0.3766)	
training:	Epoch: [110][3/204]	Loss 0.4035 (0.3856)	
training:	Epoch: [110][4/204]	Loss 0.4352 (0.3980)	
training:	Epoch: [110][5/204]	Loss 0.3735 (0.3931)	
training:	Epoch: [110][6/204]	Loss 0.3295 (0.3825)	
training:	Epoch: [110][7/204]	Loss 0.4224 (0.3882)	
training:	Epoch: [110][8/204]	Loss 0.4803 (0.3997)	
training:	Epoch: [110][9/204]	Loss 0.5662 (0.4182)	
training:	Epoch: [110][10/204]	Loss 0.3427 (0.4106)	
training:	Epoch: [110][11/204]	Loss 0.4437 (0.4136)	
training:	Epoch: [110][12/204]	Loss 0.2744 (0.4020)	
training:	Epoch: [110][13/204]	Loss 0.3873 (0.4009)	
training:	Epoch: [110][14/204]	Loss 0.4134 (0.4018)	
training:	Epoch: [110][15/204]	Loss 0.3567 (0.3988)	
training:	Epoch: [110][16/204]	Loss 0.5476 (0.4081)	
training:	Epoch: [110][17/204]	Loss 0.3948 (0.4073)	
training:	Epoch: [110][18/204]	Loss 0.3044 (0.4016)	
training:	Epoch: [110][19/204]	Loss 0.3103 (0.3968)	
training:	Epoch: [110][20/204]	Loss 0.4621 (0.4001)	
training:	Epoch: [110][21/204]	Loss 0.4561 (0.4027)	
training:	Epoch: [110][22/204]	Loss 0.3753 (0.4015)	
training:	Epoch: [110][23/204]	Loss 0.4942 (0.4055)	
training:	Epoch: [110][24/204]	Loss 0.4409 (0.4070)	
training:	Epoch: [110][25/204]	Loss 0.3081 (0.4030)	
training:	Epoch: [110][26/204]	Loss 0.5841 (0.4100)	
training:	Epoch: [110][27/204]	Loss 0.3904 (0.4093)	
training:	Epoch: [110][28/204]	Loss 0.4715 (0.4115)	
training:	Epoch: [110][29/204]	Loss 0.6443 (0.4195)	
training:	Epoch: [110][30/204]	Loss 0.3766 (0.4181)	
training:	Epoch: [110][31/204]	Loss 0.2605 (0.4130)	
training:	Epoch: [110][32/204]	Loss 0.5933 (0.4186)	
training:	Epoch: [110][33/204]	Loss 0.2490 (0.4135)	
training:	Epoch: [110][34/204]	Loss 0.4788 (0.4154)	
training:	Epoch: [110][35/204]	Loss 0.2828 (0.4116)	
training:	Epoch: [110][36/204]	Loss 0.4080 (0.4115)	
training:	Epoch: [110][37/204]	Loss 0.3291 (0.4093)	
training:	Epoch: [110][38/204]	Loss 0.4724 (0.4110)	
training:	Epoch: [110][39/204]	Loss 0.4891 (0.4130)	
training:	Epoch: [110][40/204]	Loss 0.2837 (0.4097)	
training:	Epoch: [110][41/204]	Loss 0.2638 (0.4062)	
training:	Epoch: [110][42/204]	Loss 0.3965 (0.4059)	
training:	Epoch: [110][43/204]	Loss 0.3788 (0.4053)	
training:	Epoch: [110][44/204]	Loss 0.3449 (0.4039)	
training:	Epoch: [110][45/204]	Loss 0.4186 (0.4043)	
training:	Epoch: [110][46/204]	Loss 0.4862 (0.4060)	
training:	Epoch: [110][47/204]	Loss 0.3945 (0.4058)	
training:	Epoch: [110][48/204]	Loss 0.4481 (0.4067)	
training:	Epoch: [110][49/204]	Loss 0.4304 (0.4072)	
training:	Epoch: [110][50/204]	Loss 0.5378 (0.4098)	
training:	Epoch: [110][51/204]	Loss 0.3864 (0.4093)	
training:	Epoch: [110][52/204]	Loss 0.4372 (0.4099)	
training:	Epoch: [110][53/204]	Loss 0.5274 (0.4121)	
training:	Epoch: [110][54/204]	Loss 0.5270 (0.4142)	
training:	Epoch: [110][55/204]	Loss 0.3134 (0.4124)	
training:	Epoch: [110][56/204]	Loss 0.4805 (0.4136)	
training:	Epoch: [110][57/204]	Loss 0.5926 (0.4167)	
training:	Epoch: [110][58/204]	Loss 0.5123 (0.4184)	
training:	Epoch: [110][59/204]	Loss 0.6715 (0.4227)	
training:	Epoch: [110][60/204]	Loss 0.4393 (0.4229)	
training:	Epoch: [110][61/204]	Loss 0.5127 (0.4244)	
training:	Epoch: [110][62/204]	Loss 0.3641 (0.4234)	
training:	Epoch: [110][63/204]	Loss 0.3754 (0.4227)	
training:	Epoch: [110][64/204]	Loss 0.3421 (0.4214)	
training:	Epoch: [110][65/204]	Loss 0.3956 (0.4210)	
training:	Epoch: [110][66/204]	Loss 0.4976 (0.4222)	
training:	Epoch: [110][67/204]	Loss 0.6020 (0.4249)	
training:	Epoch: [110][68/204]	Loss 0.4001 (0.4245)	
training:	Epoch: [110][69/204]	Loss 0.4368 (0.4247)	
training:	Epoch: [110][70/204]	Loss 0.2792 (0.4226)	
training:	Epoch: [110][71/204]	Loss 0.3165 (0.4211)	
training:	Epoch: [110][72/204]	Loss 0.4992 (0.4222)	
training:	Epoch: [110][73/204]	Loss 0.4876 (0.4231)	
training:	Epoch: [110][74/204]	Loss 0.2381 (0.4206)	
training:	Epoch: [110][75/204]	Loss 0.4729 (0.4213)	
training:	Epoch: [110][76/204]	Loss 0.4384 (0.4215)	
training:	Epoch: [110][77/204]	Loss 0.3957 (0.4212)	
training:	Epoch: [110][78/204]	Loss 0.3825 (0.4207)	
training:	Epoch: [110][79/204]	Loss 0.5052 (0.4217)	
training:	Epoch: [110][80/204]	Loss 0.3966 (0.4214)	
training:	Epoch: [110][81/204]	Loss 0.3177 (0.4202)	
training:	Epoch: [110][82/204]	Loss 0.2684 (0.4183)	
training:	Epoch: [110][83/204]	Loss 0.4323 (0.4185)	
training:	Epoch: [110][84/204]	Loss 0.2759 (0.4168)	
training:	Epoch: [110][85/204]	Loss 0.3200 (0.4156)	
training:	Epoch: [110][86/204]	Loss 0.3891 (0.4153)	
training:	Epoch: [110][87/204]	Loss 0.3662 (0.4148)	
training:	Epoch: [110][88/204]	Loss 0.3988 (0.4146)	
training:	Epoch: [110][89/204]	Loss 0.4243 (0.4147)	
training:	Epoch: [110][90/204]	Loss 0.3834 (0.4143)	
training:	Epoch: [110][91/204]	Loss 0.3516 (0.4137)	
training:	Epoch: [110][92/204]	Loss 0.4322 (0.4139)	
training:	Epoch: [110][93/204]	Loss 0.5017 (0.4148)	
training:	Epoch: [110][94/204]	Loss 0.3608 (0.4142)	
training:	Epoch: [110][95/204]	Loss 0.3467 (0.4135)	
training:	Epoch: [110][96/204]	Loss 0.3797 (0.4132)	
training:	Epoch: [110][97/204]	Loss 0.4222 (0.4133)	
training:	Epoch: [110][98/204]	Loss 0.3539 (0.4127)	
training:	Epoch: [110][99/204]	Loss 0.4891 (0.4134)	
training:	Epoch: [110][100/204]	Loss 0.3132 (0.4124)	
training:	Epoch: [110][101/204]	Loss 0.3629 (0.4119)	
training:	Epoch: [110][102/204]	Loss 0.3850 (0.4117)	
training:	Epoch: [110][103/204]	Loss 0.4181 (0.4117)	
training:	Epoch: [110][104/204]	Loss 0.4117 (0.4117)	
training:	Epoch: [110][105/204]	Loss 0.4423 (0.4120)	
training:	Epoch: [110][106/204]	Loss 0.2951 (0.4109)	
training:	Epoch: [110][107/204]	Loss 0.5795 (0.4125)	
training:	Epoch: [110][108/204]	Loss 0.4015 (0.4124)	
training:	Epoch: [110][109/204]	Loss 0.3660 (0.4120)	
training:	Epoch: [110][110/204]	Loss 0.3686 (0.4116)	
training:	Epoch: [110][111/204]	Loss 0.5281 (0.4126)	
training:	Epoch: [110][112/204]	Loss 0.4650 (0.4131)	
training:	Epoch: [110][113/204]	Loss 0.3777 (0.4128)	
training:	Epoch: [110][114/204]	Loss 0.3727 (0.4124)	
training:	Epoch: [110][115/204]	Loss 0.4476 (0.4127)	
training:	Epoch: [110][116/204]	Loss 0.2592 (0.4114)	
training:	Epoch: [110][117/204]	Loss 0.3744 (0.4111)	
training:	Epoch: [110][118/204]	Loss 0.3789 (0.4108)	
training:	Epoch: [110][119/204]	Loss 0.4477 (0.4111)	
training:	Epoch: [110][120/204]	Loss 0.4432 (0.4114)	
training:	Epoch: [110][121/204]	Loss 0.5229 (0.4123)	
training:	Epoch: [110][122/204]	Loss 0.4078 (0.4123)	
training:	Epoch: [110][123/204]	Loss 0.1888 (0.4105)	
training:	Epoch: [110][124/204]	Loss 0.4179 (0.4105)	
training:	Epoch: [110][125/204]	Loss 0.4947 (0.4112)	
training:	Epoch: [110][126/204]	Loss 0.2859 (0.4102)	
training:	Epoch: [110][127/204]	Loss 0.4619 (0.4106)	
training:	Epoch: [110][128/204]	Loss 0.5229 (0.4115)	
training:	Epoch: [110][129/204]	Loss 0.4349 (0.4117)	
training:	Epoch: [110][130/204]	Loss 0.5114 (0.4124)	
training:	Epoch: [110][131/204]	Loss 0.4407 (0.4126)	
training:	Epoch: [110][132/204]	Loss 0.2941 (0.4117)	
training:	Epoch: [110][133/204]	Loss 0.3341 (0.4112)	
training:	Epoch: [110][134/204]	Loss 0.4062 (0.4111)	
training:	Epoch: [110][135/204]	Loss 0.3465 (0.4107)	
training:	Epoch: [110][136/204]	Loss 0.5248 (0.4115)	
training:	Epoch: [110][137/204]	Loss 0.3527 (0.4111)	
training:	Epoch: [110][138/204]	Loss 0.2927 (0.4102)	
training:	Epoch: [110][139/204]	Loss 0.3401 (0.4097)	
training:	Epoch: [110][140/204]	Loss 0.3600 (0.4093)	
training:	Epoch: [110][141/204]	Loss 0.4023 (0.4093)	
training:	Epoch: [110][142/204]	Loss 0.4735 (0.4097)	
training:	Epoch: [110][143/204]	Loss 0.3581 (0.4094)	
training:	Epoch: [110][144/204]	Loss 0.5173 (0.4101)	
training:	Epoch: [110][145/204]	Loss 0.3515 (0.4097)	
training:	Epoch: [110][146/204]	Loss 0.5517 (0.4107)	
training:	Epoch: [110][147/204]	Loss 0.4653 (0.4111)	
training:	Epoch: [110][148/204]	Loss 0.3262 (0.4105)	
training:	Epoch: [110][149/204]	Loss 0.4983 (0.4111)	
training:	Epoch: [110][150/204]	Loss 0.3715 (0.4108)	
training:	Epoch: [110][151/204]	Loss 0.2090 (0.4095)	
training:	Epoch: [110][152/204]	Loss 0.3980 (0.4094)	
training:	Epoch: [110][153/204]	Loss 0.4827 (0.4099)	
training:	Epoch: [110][154/204]	Loss 0.3128 (0.4093)	
training:	Epoch: [110][155/204]	Loss 0.3630 (0.4090)	
training:	Epoch: [110][156/204]	Loss 0.3522 (0.4086)	
training:	Epoch: [110][157/204]	Loss 0.2464 (0.4076)	
training:	Epoch: [110][158/204]	Loss 0.3884 (0.4074)	
training:	Epoch: [110][159/204]	Loss 0.4654 (0.4078)	
training:	Epoch: [110][160/204]	Loss 0.3603 (0.4075)	
training:	Epoch: [110][161/204]	Loss 0.3584 (0.4072)	
training:	Epoch: [110][162/204]	Loss 0.3474 (0.4068)	
training:	Epoch: [110][163/204]	Loss 0.3557 (0.4065)	
training:	Epoch: [110][164/204]	Loss 0.3925 (0.4064)	
training:	Epoch: [110][165/204]	Loss 0.4830 (0.4069)	
training:	Epoch: [110][166/204]	Loss 0.3556 (0.4066)	
training:	Epoch: [110][167/204]	Loss 0.3778 (0.4064)	
training:	Epoch: [110][168/204]	Loss 0.4216 (0.4065)	
training:	Epoch: [110][169/204]	Loss 0.3461 (0.4062)	
training:	Epoch: [110][170/204]	Loss 0.5522 (0.4070)	
training:	Epoch: [110][171/204]	Loss 0.2659 (0.4062)	
training:	Epoch: [110][172/204]	Loss 0.3578 (0.4059)	
training:	Epoch: [110][173/204]	Loss 0.2353 (0.4049)	
training:	Epoch: [110][174/204]	Loss 0.4671 (0.4053)	
training:	Epoch: [110][175/204]	Loss 0.3251 (0.4048)	
training:	Epoch: [110][176/204]	Loss 0.6299 (0.4061)	
training:	Epoch: [110][177/204]	Loss 0.4151 (0.4062)	
training:	Epoch: [110][178/204]	Loss 0.4472 (0.4064)	
training:	Epoch: [110][179/204]	Loss 0.3717 (0.4062)	
training:	Epoch: [110][180/204]	Loss 0.3289 (0.4058)	
training:	Epoch: [110][181/204]	Loss 0.3923 (0.4057)	
training:	Epoch: [110][182/204]	Loss 0.4187 (0.4058)	
training:	Epoch: [110][183/204]	Loss 0.3932 (0.4057)	
training:	Epoch: [110][184/204]	Loss 0.4295 (0.4058)	
training:	Epoch: [110][185/204]	Loss 0.5100 (0.4064)	
training:	Epoch: [110][186/204]	Loss 0.5214 (0.4070)	
training:	Epoch: [110][187/204]	Loss 0.3584 (0.4067)	
training:	Epoch: [110][188/204]	Loss 0.3535 (0.4065)	
training:	Epoch: [110][189/204]	Loss 0.3133 (0.4060)	
training:	Epoch: [110][190/204]	Loss 0.3136 (0.4055)	
training:	Epoch: [110][191/204]	Loss 0.3999 (0.4054)	
training:	Epoch: [110][192/204]	Loss 0.4726 (0.4058)	
training:	Epoch: [110][193/204]	Loss 0.4521 (0.4060)	
training:	Epoch: [110][194/204]	Loss 0.4254 (0.4061)	
training:	Epoch: [110][195/204]	Loss 0.2029 (0.4051)	
training:	Epoch: [110][196/204]	Loss 0.4221 (0.4052)	
training:	Epoch: [110][197/204]	Loss 0.4272 (0.4053)	
training:	Epoch: [110][198/204]	Loss 0.5213 (0.4059)	
training:	Epoch: [110][199/204]	Loss 0.3718 (0.4057)	
training:	Epoch: [110][200/204]	Loss 0.4358 (0.4059)	
training:	Epoch: [110][201/204]	Loss 0.3980 (0.4058)	
training:	Epoch: [110][202/204]	Loss 0.3676 (0.4056)	
training:	Epoch: [110][203/204]	Loss 0.3838 (0.4055)	
training:	Epoch: [110][204/204]	Loss 0.3903 (0.4054)	
Training:	 Loss: 0.4048

Training:	 ACC: 0.8329 0.8334 0.8454 0.8205
Validation:	 ACC: 0.7967 0.7978 0.8188 0.7747
Validation:	 Best_BACC: 0.7975 0.7988 0.8270 0.7679
Validation:	 Loss: 0.4349
Pretraining:	Epoch 111/120
----------
training:	Epoch: [111][1/204]	Loss 0.5005 (0.5005)	
training:	Epoch: [111][2/204]	Loss 0.4441 (0.4723)	
training:	Epoch: [111][3/204]	Loss 0.4590 (0.4678)	
training:	Epoch: [111][4/204]	Loss 0.4252 (0.4572)	
training:	Epoch: [111][5/204]	Loss 0.4029 (0.4463)	
training:	Epoch: [111][6/204]	Loss 0.4713 (0.4505)	
training:	Epoch: [111][7/204]	Loss 0.4680 (0.4530)	
training:	Epoch: [111][8/204]	Loss 0.4539 (0.4531)	
training:	Epoch: [111][9/204]	Loss 0.3496 (0.4416)	
training:	Epoch: [111][10/204]	Loss 0.4776 (0.4452)	
training:	Epoch: [111][11/204]	Loss 0.3234 (0.4341)	
training:	Epoch: [111][12/204]	Loss 0.5001 (0.4396)	
training:	Epoch: [111][13/204]	Loss 0.3056 (0.4293)	
training:	Epoch: [111][14/204]	Loss 0.2117 (0.4138)	
training:	Epoch: [111][15/204]	Loss 0.3034 (0.4064)	
training:	Epoch: [111][16/204]	Loss 0.5083 (0.4128)	
training:	Epoch: [111][17/204]	Loss 0.5732 (0.4222)	
training:	Epoch: [111][18/204]	Loss 0.5622 (0.4300)	
training:	Epoch: [111][19/204]	Loss 0.5291 (0.4352)	
training:	Epoch: [111][20/204]	Loss 0.3184 (0.4294)	
training:	Epoch: [111][21/204]	Loss 0.5613 (0.4357)	
training:	Epoch: [111][22/204]	Loss 0.3390 (0.4313)	
training:	Epoch: [111][23/204]	Loss 0.4925 (0.4339)	
training:	Epoch: [111][24/204]	Loss 0.3413 (0.4301)	
training:	Epoch: [111][25/204]	Loss 0.4010 (0.4289)	
training:	Epoch: [111][26/204]	Loss 0.3371 (0.4254)	
training:	Epoch: [111][27/204]	Loss 0.6182 (0.4325)	
training:	Epoch: [111][28/204]	Loss 0.3247 (0.4287)	
training:	Epoch: [111][29/204]	Loss 0.5592 (0.4332)	
training:	Epoch: [111][30/204]	Loss 0.4210 (0.4328)	
training:	Epoch: [111][31/204]	Loss 0.3554 (0.4303)	
training:	Epoch: [111][32/204]	Loss 0.2916 (0.4259)	
training:	Epoch: [111][33/204]	Loss 0.2776 (0.4214)	
training:	Epoch: [111][34/204]	Loss 0.4670 (0.4228)	
training:	Epoch: [111][35/204]	Loss 0.4271 (0.4229)	
training:	Epoch: [111][36/204]	Loss 0.3413 (0.4206)	
training:	Epoch: [111][37/204]	Loss 0.3877 (0.4197)	
training:	Epoch: [111][38/204]	Loss 0.3923 (0.4190)	
training:	Epoch: [111][39/204]	Loss 0.3434 (0.4171)	
training:	Epoch: [111][40/204]	Loss 0.4963 (0.4191)	
training:	Epoch: [111][41/204]	Loss 0.4408 (0.4196)	
training:	Epoch: [111][42/204]	Loss 0.5270 (0.4221)	
training:	Epoch: [111][43/204]	Loss 0.4205 (0.4221)	
training:	Epoch: [111][44/204]	Loss 0.3634 (0.4208)	
training:	Epoch: [111][45/204]	Loss 0.4233 (0.4208)	
training:	Epoch: [111][46/204]	Loss 0.4241 (0.4209)	
training:	Epoch: [111][47/204]	Loss 0.4244 (0.4210)	
training:	Epoch: [111][48/204]	Loss 0.4080 (0.4207)	
training:	Epoch: [111][49/204]	Loss 0.4575 (0.4215)	
training:	Epoch: [111][50/204]	Loss 0.4034 (0.4211)	
training:	Epoch: [111][51/204]	Loss 0.3185 (0.4191)	
training:	Epoch: [111][52/204]	Loss 0.4900 (0.4204)	
training:	Epoch: [111][53/204]	Loss 0.4509 (0.4210)	
training:	Epoch: [111][54/204]	Loss 0.5840 (0.4240)	
training:	Epoch: [111][55/204]	Loss 0.2780 (0.4214)	
training:	Epoch: [111][56/204]	Loss 0.3669 (0.4204)	
training:	Epoch: [111][57/204]	Loss 0.3781 (0.4197)	
training:	Epoch: [111][58/204]	Loss 0.4906 (0.4209)	
training:	Epoch: [111][59/204]	Loss 0.3580 (0.4198)	
training:	Epoch: [111][60/204]	Loss 0.5408 (0.4218)	
training:	Epoch: [111][61/204]	Loss 0.3757 (0.4211)	
training:	Epoch: [111][62/204]	Loss 0.1942 (0.4174)	
training:	Epoch: [111][63/204]	Loss 0.3624 (0.4166)	
training:	Epoch: [111][64/204]	Loss 0.3701 (0.4158)	
training:	Epoch: [111][65/204]	Loss 0.4860 (0.4169)	
training:	Epoch: [111][66/204]	Loss 0.3252 (0.4155)	
training:	Epoch: [111][67/204]	Loss 0.3378 (0.4144)	
training:	Epoch: [111][68/204]	Loss 0.4404 (0.4147)	
training:	Epoch: [111][69/204]	Loss 0.5432 (0.4166)	
training:	Epoch: [111][70/204]	Loss 0.4437 (0.4170)	
training:	Epoch: [111][71/204]	Loss 0.3411 (0.4159)	
training:	Epoch: [111][72/204]	Loss 0.3015 (0.4143)	
training:	Epoch: [111][73/204]	Loss 0.3443 (0.4134)	
training:	Epoch: [111][74/204]	Loss 0.5119 (0.4147)	
training:	Epoch: [111][75/204]	Loss 0.2490 (0.4125)	
training:	Epoch: [111][76/204]	Loss 0.5023 (0.4137)	
training:	Epoch: [111][77/204]	Loss 0.4768 (0.4145)	
training:	Epoch: [111][78/204]	Loss 0.2325 (0.4122)	
training:	Epoch: [111][79/204]	Loss 0.3768 (0.4117)	
training:	Epoch: [111][80/204]	Loss 0.4594 (0.4123)	
training:	Epoch: [111][81/204]	Loss 0.3247 (0.4112)	
training:	Epoch: [111][82/204]	Loss 0.4952 (0.4123)	
training:	Epoch: [111][83/204]	Loss 0.3418 (0.4114)	
training:	Epoch: [111][84/204]	Loss 0.4711 (0.4121)	
training:	Epoch: [111][85/204]	Loss 0.5222 (0.4134)	
training:	Epoch: [111][86/204]	Loss 0.3346 (0.4125)	
training:	Epoch: [111][87/204]	Loss 0.4248 (0.4126)	
training:	Epoch: [111][88/204]	Loss 0.4110 (0.4126)	
training:	Epoch: [111][89/204]	Loss 0.5029 (0.4136)	
training:	Epoch: [111][90/204]	Loss 0.4162 (0.4137)	
training:	Epoch: [111][91/204]	Loss 0.2931 (0.4123)	
training:	Epoch: [111][92/204]	Loss 0.3814 (0.4120)	
training:	Epoch: [111][93/204]	Loss 0.4165 (0.4120)	
training:	Epoch: [111][94/204]	Loss 0.5397 (0.4134)	
training:	Epoch: [111][95/204]	Loss 0.5171 (0.4145)	
training:	Epoch: [111][96/204]	Loss 0.4536 (0.4149)	
training:	Epoch: [111][97/204]	Loss 0.3214 (0.4139)	
training:	Epoch: [111][98/204]	Loss 0.3193 (0.4130)	
training:	Epoch: [111][99/204]	Loss 0.4126 (0.4130)	
training:	Epoch: [111][100/204]	Loss 0.4113 (0.4130)	
training:	Epoch: [111][101/204]	Loss 0.2191 (0.4110)	
training:	Epoch: [111][102/204]	Loss 0.2892 (0.4098)	
training:	Epoch: [111][103/204]	Loss 0.3784 (0.4095)	
training:	Epoch: [111][104/204]	Loss 0.5422 (0.4108)	
training:	Epoch: [111][105/204]	Loss 0.5034 (0.4117)	
training:	Epoch: [111][106/204]	Loss 0.4509 (0.4121)	
training:	Epoch: [111][107/204]	Loss 0.3970 (0.4119)	
training:	Epoch: [111][108/204]	Loss 0.4705 (0.4125)	
training:	Epoch: [111][109/204]	Loss 0.4398 (0.4127)	
training:	Epoch: [111][110/204]	Loss 0.3597 (0.4122)	
training:	Epoch: [111][111/204]	Loss 0.3248 (0.4114)	
training:	Epoch: [111][112/204]	Loss 0.4208 (0.4115)	
training:	Epoch: [111][113/204]	Loss 0.4234 (0.4116)	
training:	Epoch: [111][114/204]	Loss 0.4524 (0.4120)	
training:	Epoch: [111][115/204]	Loss 0.4090 (0.4120)	
training:	Epoch: [111][116/204]	Loss 0.3620 (0.4115)	
training:	Epoch: [111][117/204]	Loss 0.3735 (0.4112)	
training:	Epoch: [111][118/204]	Loss 0.4441 (0.4115)	
training:	Epoch: [111][119/204]	Loss 0.4416 (0.4117)	
training:	Epoch: [111][120/204]	Loss 0.4563 (0.4121)	
training:	Epoch: [111][121/204]	Loss 0.2467 (0.4107)	
training:	Epoch: [111][122/204]	Loss 0.4167 (0.4108)	
training:	Epoch: [111][123/204]	Loss 0.3893 (0.4106)	
training:	Epoch: [111][124/204]	Loss 0.4315 (0.4108)	
training:	Epoch: [111][125/204]	Loss 0.3033 (0.4099)	
training:	Epoch: [111][126/204]	Loss 0.4311 (0.4101)	
training:	Epoch: [111][127/204]	Loss 0.5083 (0.4109)	
training:	Epoch: [111][128/204]	Loss 0.4899 (0.4115)	
training:	Epoch: [111][129/204]	Loss 0.2854 (0.4105)	
training:	Epoch: [111][130/204]	Loss 0.4732 (0.4110)	
training:	Epoch: [111][131/204]	Loss 0.3280 (0.4104)	
training:	Epoch: [111][132/204]	Loss 0.3686 (0.4100)	
training:	Epoch: [111][133/204]	Loss 0.4193 (0.4101)	
training:	Epoch: [111][134/204]	Loss 0.2725 (0.4091)	
training:	Epoch: [111][135/204]	Loss 0.3576 (0.4087)	
training:	Epoch: [111][136/204]	Loss 0.2191 (0.4073)	
training:	Epoch: [111][137/204]	Loss 0.4697 (0.4078)	
training:	Epoch: [111][138/204]	Loss 0.4677 (0.4082)	
training:	Epoch: [111][139/204]	Loss 0.3821 (0.4080)	
training:	Epoch: [111][140/204]	Loss 0.3735 (0.4078)	
training:	Epoch: [111][141/204]	Loss 0.2669 (0.4068)	
training:	Epoch: [111][142/204]	Loss 0.5902 (0.4081)	
training:	Epoch: [111][143/204]	Loss 0.4663 (0.4085)	
training:	Epoch: [111][144/204]	Loss 0.4409 (0.4087)	
training:	Epoch: [111][145/204]	Loss 0.3637 (0.4084)	
training:	Epoch: [111][146/204]	Loss 0.2602 (0.4074)	
training:	Epoch: [111][147/204]	Loss 0.4356 (0.4076)	
training:	Epoch: [111][148/204]	Loss 0.3966 (0.4075)	
training:	Epoch: [111][149/204]	Loss 0.3293 (0.4070)	
training:	Epoch: [111][150/204]	Loss 0.3786 (0.4068)	
training:	Epoch: [111][151/204]	Loss 0.4098 (0.4068)	
training:	Epoch: [111][152/204]	Loss 0.3795 (0.4066)	
training:	Epoch: [111][153/204]	Loss 0.4337 (0.4068)	
training:	Epoch: [111][154/204]	Loss 0.4551 (0.4071)	
training:	Epoch: [111][155/204]	Loss 0.3985 (0.4070)	
training:	Epoch: [111][156/204]	Loss 0.3798 (0.4069)	
training:	Epoch: [111][157/204]	Loss 0.3283 (0.4064)	
training:	Epoch: [111][158/204]	Loss 0.4583 (0.4067)	
training:	Epoch: [111][159/204]	Loss 0.4416 (0.4069)	
training:	Epoch: [111][160/204]	Loss 0.3800 (0.4068)	
training:	Epoch: [111][161/204]	Loss 0.2333 (0.4057)	
training:	Epoch: [111][162/204]	Loss 0.3384 (0.4053)	
training:	Epoch: [111][163/204]	Loss 0.2821 (0.4045)	
training:	Epoch: [111][164/204]	Loss 0.3517 (0.4042)	
training:	Epoch: [111][165/204]	Loss 0.3205 (0.4037)	
training:	Epoch: [111][166/204]	Loss 0.4437 (0.4039)	
training:	Epoch: [111][167/204]	Loss 0.2679 (0.4031)	
training:	Epoch: [111][168/204]	Loss 0.3335 (0.4027)	
training:	Epoch: [111][169/204]	Loss 0.3146 (0.4022)	
training:	Epoch: [111][170/204]	Loss 0.4225 (0.4023)	
training:	Epoch: [111][171/204]	Loss 0.3973 (0.4023)	
training:	Epoch: [111][172/204]	Loss 0.4010 (0.4022)	
training:	Epoch: [111][173/204]	Loss 0.4535 (0.4025)	
training:	Epoch: [111][174/204]	Loss 0.5068 (0.4031)	
training:	Epoch: [111][175/204]	Loss 0.4250 (0.4033)	
training:	Epoch: [111][176/204]	Loss 0.3805 (0.4031)	
training:	Epoch: [111][177/204]	Loss 0.3350 (0.4028)	
training:	Epoch: [111][178/204]	Loss 0.4696 (0.4031)	
training:	Epoch: [111][179/204]	Loss 0.3000 (0.4026)	
training:	Epoch: [111][180/204]	Loss 0.4365 (0.4027)	
training:	Epoch: [111][181/204]	Loss 0.4115 (0.4028)	
training:	Epoch: [111][182/204]	Loss 0.4498 (0.4030)	
training:	Epoch: [111][183/204]	Loss 0.3943 (0.4030)	
training:	Epoch: [111][184/204]	Loss 0.3827 (0.4029)	
training:	Epoch: [111][185/204]	Loss 0.4661 (0.4032)	
training:	Epoch: [111][186/204]	Loss 0.2890 (0.4026)	
training:	Epoch: [111][187/204]	Loss 0.3530 (0.4024)	
training:	Epoch: [111][188/204]	Loss 0.4876 (0.4028)	
training:	Epoch: [111][189/204]	Loss 0.3555 (0.4026)	
training:	Epoch: [111][190/204]	Loss 0.3789 (0.4024)	
training:	Epoch: [111][191/204]	Loss 0.6084 (0.4035)	
training:	Epoch: [111][192/204]	Loss 0.3926 (0.4035)	
training:	Epoch: [111][193/204]	Loss 0.3164 (0.4030)	
training:	Epoch: [111][194/204]	Loss 0.3193 (0.4026)	
training:	Epoch: [111][195/204]	Loss 0.3998 (0.4026)	
training:	Epoch: [111][196/204]	Loss 0.4486 (0.4028)	
training:	Epoch: [111][197/204]	Loss 0.3486 (0.4025)	
training:	Epoch: [111][198/204]	Loss 0.4783 (0.4029)	
training:	Epoch: [111][199/204]	Loss 0.4591 (0.4032)	
training:	Epoch: [111][200/204]	Loss 0.4073 (0.4032)	
training:	Epoch: [111][201/204]	Loss 0.3282 (0.4028)	
training:	Epoch: [111][202/204]	Loss 0.4791 (0.4032)	
training:	Epoch: [111][203/204]	Loss 0.3381 (0.4029)	
training:	Epoch: [111][204/204]	Loss 0.3865 (0.4028)	
Training:	 Loss: 0.4022

Training:	 ACC: 0.8342 0.8347 0.8448 0.8237
Validation:	 ACC: 0.7974 0.7983 0.8178 0.7769
Validation:	 Best_BACC: 0.7975 0.7988 0.8270 0.7679
Validation:	 Loss: 0.4341
Pretraining:	Epoch 112/120
----------
training:	Epoch: [112][1/204]	Loss 0.3272 (0.3272)	
training:	Epoch: [112][2/204]	Loss 0.3956 (0.3614)	
training:	Epoch: [112][3/204]	Loss 0.2846 (0.3358)	
training:	Epoch: [112][4/204]	Loss 0.2865 (0.3235)	
training:	Epoch: [112][5/204]	Loss 0.3840 (0.3356)	
training:	Epoch: [112][6/204]	Loss 0.4894 (0.3612)	
training:	Epoch: [112][7/204]	Loss 0.4386 (0.3723)	
training:	Epoch: [112][8/204]	Loss 0.3338 (0.3675)	
training:	Epoch: [112][9/204]	Loss 0.3955 (0.3706)	
training:	Epoch: [112][10/204]	Loss 0.3596 (0.3695)	
training:	Epoch: [112][11/204]	Loss 0.6669 (0.3965)	
training:	Epoch: [112][12/204]	Loss 0.5015 (0.4053)	
training:	Epoch: [112][13/204]	Loss 0.3373 (0.4000)	
training:	Epoch: [112][14/204]	Loss 0.3807 (0.3987)	
training:	Epoch: [112][15/204]	Loss 0.6977 (0.4186)	
training:	Epoch: [112][16/204]	Loss 0.3775 (0.4160)	
training:	Epoch: [112][17/204]	Loss 0.3928 (0.4147)	
training:	Epoch: [112][18/204]	Loss 0.3017 (0.4084)	
training:	Epoch: [112][19/204]	Loss 0.5089 (0.4137)	
training:	Epoch: [112][20/204]	Loss 0.2758 (0.4068)	
training:	Epoch: [112][21/204]	Loss 0.3937 (0.4062)	
training:	Epoch: [112][22/204]	Loss 0.4461 (0.4080)	
training:	Epoch: [112][23/204]	Loss 0.4722 (0.4108)	
training:	Epoch: [112][24/204]	Loss 0.4390 (0.4119)	
training:	Epoch: [112][25/204]	Loss 0.4340 (0.4128)	
training:	Epoch: [112][26/204]	Loss 0.5393 (0.4177)	
training:	Epoch: [112][27/204]	Loss 0.4017 (0.4171)	
training:	Epoch: [112][28/204]	Loss 0.2890 (0.4125)	
training:	Epoch: [112][29/204]	Loss 0.4218 (0.4128)	
training:	Epoch: [112][30/204]	Loss 0.4515 (0.4141)	
training:	Epoch: [112][31/204]	Loss 0.4065 (0.4139)	
training:	Epoch: [112][32/204]	Loss 0.5613 (0.4185)	
training:	Epoch: [112][33/204]	Loss 0.3592 (0.4167)	
training:	Epoch: [112][34/204]	Loss 0.3719 (0.4154)	
training:	Epoch: [112][35/204]	Loss 0.3440 (0.4133)	
training:	Epoch: [112][36/204]	Loss 0.4196 (0.4135)	
training:	Epoch: [112][37/204]	Loss 0.2772 (0.4098)	
training:	Epoch: [112][38/204]	Loss 0.2559 (0.4058)	
training:	Epoch: [112][39/204]	Loss 0.4017 (0.4057)	
training:	Epoch: [112][40/204]	Loss 0.4322 (0.4063)	
training:	Epoch: [112][41/204]	Loss 0.4152 (0.4066)	
training:	Epoch: [112][42/204]	Loss 0.2814 (0.4036)	
training:	Epoch: [112][43/204]	Loss 0.4716 (0.4052)	
training:	Epoch: [112][44/204]	Loss 0.4544 (0.4063)	
training:	Epoch: [112][45/204]	Loss 0.4234 (0.4067)	
training:	Epoch: [112][46/204]	Loss 0.3951 (0.4064)	
training:	Epoch: [112][47/204]	Loss 0.4174 (0.4066)	
training:	Epoch: [112][48/204]	Loss 0.3868 (0.4062)	
training:	Epoch: [112][49/204]	Loss 0.4609 (0.4073)	
training:	Epoch: [112][50/204]	Loss 0.4337 (0.4079)	
training:	Epoch: [112][51/204]	Loss 0.4281 (0.4083)	
training:	Epoch: [112][52/204]	Loss 0.2673 (0.4056)	
training:	Epoch: [112][53/204]	Loss 0.3531 (0.4046)	
training:	Epoch: [112][54/204]	Loss 0.4024 (0.4045)	
training:	Epoch: [112][55/204]	Loss 0.3871 (0.4042)	
training:	Epoch: [112][56/204]	Loss 0.3285 (0.4029)	
training:	Epoch: [112][57/204]	Loss 0.3231 (0.4015)	
training:	Epoch: [112][58/204]	Loss 0.4523 (0.4023)	
training:	Epoch: [112][59/204]	Loss 0.3382 (0.4012)	
training:	Epoch: [112][60/204]	Loss 0.5301 (0.4034)	
training:	Epoch: [112][61/204]	Loss 0.3450 (0.4024)	
training:	Epoch: [112][62/204]	Loss 0.3755 (0.4020)	
training:	Epoch: [112][63/204]	Loss 0.5477 (0.4043)	
training:	Epoch: [112][64/204]	Loss 0.3613 (0.4036)	
training:	Epoch: [112][65/204]	Loss 0.4109 (0.4038)	
training:	Epoch: [112][66/204]	Loss 0.3080 (0.4023)	
training:	Epoch: [112][67/204]	Loss 0.3552 (0.4016)	
training:	Epoch: [112][68/204]	Loss 0.4894 (0.4029)	
training:	Epoch: [112][69/204]	Loss 0.3632 (0.4023)	
training:	Epoch: [112][70/204]	Loss 0.5583 (0.4045)	
training:	Epoch: [112][71/204]	Loss 0.5144 (0.4061)	
training:	Epoch: [112][72/204]	Loss 0.4513 (0.4067)	
training:	Epoch: [112][73/204]	Loss 0.4011 (0.4066)	
training:	Epoch: [112][74/204]	Loss 0.3210 (0.4055)	
training:	Epoch: [112][75/204]	Loss 0.4438 (0.4060)	
training:	Epoch: [112][76/204]	Loss 0.3763 (0.4056)	
training:	Epoch: [112][77/204]	Loss 0.5264 (0.4072)	
training:	Epoch: [112][78/204]	Loss 0.4934 (0.4083)	
training:	Epoch: [112][79/204]	Loss 0.6092 (0.4108)	
training:	Epoch: [112][80/204]	Loss 0.2580 (0.4089)	
training:	Epoch: [112][81/204]	Loss 0.2491 (0.4069)	
training:	Epoch: [112][82/204]	Loss 0.3322 (0.4060)	
training:	Epoch: [112][83/204]	Loss 0.5238 (0.4074)	
training:	Epoch: [112][84/204]	Loss 0.4803 (0.4083)	
training:	Epoch: [112][85/204]	Loss 0.4255 (0.4085)	
training:	Epoch: [112][86/204]	Loss 0.3952 (0.4084)	
training:	Epoch: [112][87/204]	Loss 0.4858 (0.4093)	
training:	Epoch: [112][88/204]	Loss 0.3614 (0.4087)	
training:	Epoch: [112][89/204]	Loss 0.3720 (0.4083)	
training:	Epoch: [112][90/204]	Loss 0.3749 (0.4079)	
training:	Epoch: [112][91/204]	Loss 0.3275 (0.4070)	
training:	Epoch: [112][92/204]	Loss 0.4853 (0.4079)	
training:	Epoch: [112][93/204]	Loss 0.3164 (0.4069)	
training:	Epoch: [112][94/204]	Loss 0.4631 (0.4075)	
training:	Epoch: [112][95/204]	Loss 0.3877 (0.4073)	
training:	Epoch: [112][96/204]	Loss 0.3477 (0.4067)	
training:	Epoch: [112][97/204]	Loss 0.3290 (0.4059)	
training:	Epoch: [112][98/204]	Loss 0.4974 (0.4068)	
training:	Epoch: [112][99/204]	Loss 0.3680 (0.4064)	
training:	Epoch: [112][100/204]	Loss 0.4828 (0.4072)	
training:	Epoch: [112][101/204]	Loss 0.5847 (0.4089)	
training:	Epoch: [112][102/204]	Loss 0.4593 (0.4094)	
training:	Epoch: [112][103/204]	Loss 0.3790 (0.4091)	
training:	Epoch: [112][104/204]	Loss 0.3323 (0.4084)	
training:	Epoch: [112][105/204]	Loss 0.4358 (0.4087)	
training:	Epoch: [112][106/204]	Loss 0.4179 (0.4087)	
training:	Epoch: [112][107/204]	Loss 0.4344 (0.4090)	
training:	Epoch: [112][108/204]	Loss 0.5469 (0.4103)	
training:	Epoch: [112][109/204]	Loss 0.5289 (0.4114)	
training:	Epoch: [112][110/204]	Loss 0.2871 (0.4102)	
training:	Epoch: [112][111/204]	Loss 0.4446 (0.4105)	
training:	Epoch: [112][112/204]	Loss 0.5387 (0.4117)	
training:	Epoch: [112][113/204]	Loss 0.3136 (0.4108)	
training:	Epoch: [112][114/204]	Loss 0.4317 (0.4110)	
training:	Epoch: [112][115/204]	Loss 0.3321 (0.4103)	
training:	Epoch: [112][116/204]	Loss 0.2252 (0.4087)	
training:	Epoch: [112][117/204]	Loss 0.4777 (0.4093)	
training:	Epoch: [112][118/204]	Loss 0.4229 (0.4094)	
training:	Epoch: [112][119/204]	Loss 0.3091 (0.4086)	
training:	Epoch: [112][120/204]	Loss 0.2759 (0.4075)	
training:	Epoch: [112][121/204]	Loss 0.2847 (0.4065)	
training:	Epoch: [112][122/204]	Loss 0.3802 (0.4062)	
training:	Epoch: [112][123/204]	Loss 0.3039 (0.4054)	
training:	Epoch: [112][124/204]	Loss 0.2671 (0.4043)	
training:	Epoch: [112][125/204]	Loss 0.2959 (0.4034)	
training:	Epoch: [112][126/204]	Loss 0.3993 (0.4034)	
training:	Epoch: [112][127/204]	Loss 0.5058 (0.4042)	
training:	Epoch: [112][128/204]	Loss 0.2761 (0.4032)	
training:	Epoch: [112][129/204]	Loss 0.4109 (0.4033)	
training:	Epoch: [112][130/204]	Loss 0.5447 (0.4043)	
training:	Epoch: [112][131/204]	Loss 0.6028 (0.4059)	
training:	Epoch: [112][132/204]	Loss 0.2754 (0.4049)	
training:	Epoch: [112][133/204]	Loss 0.4886 (0.4055)	
training:	Epoch: [112][134/204]	Loss 0.3747 (0.4053)	
training:	Epoch: [112][135/204]	Loss 0.3326 (0.4047)	
training:	Epoch: [112][136/204]	Loss 0.3805 (0.4046)	
training:	Epoch: [112][137/204]	Loss 0.3499 (0.4042)	
training:	Epoch: [112][138/204]	Loss 0.3156 (0.4035)	
training:	Epoch: [112][139/204]	Loss 0.4427 (0.4038)	
training:	Epoch: [112][140/204]	Loss 0.3161 (0.4032)	
training:	Epoch: [112][141/204]	Loss 0.4799 (0.4037)	
training:	Epoch: [112][142/204]	Loss 0.3873 (0.4036)	
training:	Epoch: [112][143/204]	Loss 0.4235 (0.4037)	
training:	Epoch: [112][144/204]	Loss 0.3951 (0.4037)	
training:	Epoch: [112][145/204]	Loss 0.4804 (0.4042)	
training:	Epoch: [112][146/204]	Loss 0.5367 (0.4051)	
training:	Epoch: [112][147/204]	Loss 0.4196 (0.4052)	
training:	Epoch: [112][148/204]	Loss 0.2669 (0.4043)	
training:	Epoch: [112][149/204]	Loss 0.3447 (0.4039)	
training:	Epoch: [112][150/204]	Loss 0.3947 (0.4038)	
training:	Epoch: [112][151/204]	Loss 0.5563 (0.4048)	
training:	Epoch: [112][152/204]	Loss 0.5651 (0.4059)	
training:	Epoch: [112][153/204]	Loss 0.3206 (0.4053)	
training:	Epoch: [112][154/204]	Loss 0.4133 (0.4054)	
training:	Epoch: [112][155/204]	Loss 0.5938 (0.4066)	
training:	Epoch: [112][156/204]	Loss 0.2850 (0.4058)	
training:	Epoch: [112][157/204]	Loss 0.3444 (0.4054)	
training:	Epoch: [112][158/204]	Loss 0.4344 (0.4056)	
training:	Epoch: [112][159/204]	Loss 0.3547 (0.4053)	
training:	Epoch: [112][160/204]	Loss 0.5992 (0.4065)	
training:	Epoch: [112][161/204]	Loss 0.5502 (0.4074)	
training:	Epoch: [112][162/204]	Loss 0.3869 (0.4073)	
training:	Epoch: [112][163/204]	Loss 0.5296 (0.4080)	
training:	Epoch: [112][164/204]	Loss 0.4739 (0.4084)	
training:	Epoch: [112][165/204]	Loss 0.4713 (0.4088)	
training:	Epoch: [112][166/204]	Loss 0.4602 (0.4091)	
training:	Epoch: [112][167/204]	Loss 0.3733 (0.4089)	
training:	Epoch: [112][168/204]	Loss 0.3076 (0.4083)	
training:	Epoch: [112][169/204]	Loss 0.2574 (0.4074)	
training:	Epoch: [112][170/204]	Loss 0.5350 (0.4081)	
training:	Epoch: [112][171/204]	Loss 0.3329 (0.4077)	
training:	Epoch: [112][172/204]	Loss 0.3635 (0.4074)	
training:	Epoch: [112][173/204]	Loss 0.5065 (0.4080)	
training:	Epoch: [112][174/204]	Loss 0.3268 (0.4076)	
training:	Epoch: [112][175/204]	Loss 0.3874 (0.4074)	
training:	Epoch: [112][176/204]	Loss 0.4381 (0.4076)	
training:	Epoch: [112][177/204]	Loss 0.2766 (0.4069)	
training:	Epoch: [112][178/204]	Loss 0.3071 (0.4063)	
training:	Epoch: [112][179/204]	Loss 0.4425 (0.4065)	
training:	Epoch: [112][180/204]	Loss 0.3281 (0.4061)	
training:	Epoch: [112][181/204]	Loss 0.2901 (0.4054)	
training:	Epoch: [112][182/204]	Loss 0.4628 (0.4058)	
training:	Epoch: [112][183/204]	Loss 0.3757 (0.4056)	
training:	Epoch: [112][184/204]	Loss 0.2916 (0.4050)	
training:	Epoch: [112][185/204]	Loss 0.4473 (0.4052)	
training:	Epoch: [112][186/204]	Loss 0.3236 (0.4048)	
training:	Epoch: [112][187/204]	Loss 0.2926 (0.4042)	
training:	Epoch: [112][188/204]	Loss 0.4048 (0.4042)	
training:	Epoch: [112][189/204]	Loss 0.5112 (0.4047)	
training:	Epoch: [112][190/204]	Loss 0.3712 (0.4046)	
training:	Epoch: [112][191/204]	Loss 0.2975 (0.4040)	
training:	Epoch: [112][192/204]	Loss 0.3413 (0.4037)	
training:	Epoch: [112][193/204]	Loss 0.2864 (0.4031)	
training:	Epoch: [112][194/204]	Loss 0.4168 (0.4031)	
training:	Epoch: [112][195/204]	Loss 0.2718 (0.4025)	
training:	Epoch: [112][196/204]	Loss 0.5995 (0.4035)	
training:	Epoch: [112][197/204]	Loss 0.2630 (0.4027)	
training:	Epoch: [112][198/204]	Loss 0.2687 (0.4021)	
training:	Epoch: [112][199/204]	Loss 0.4247 (0.4022)	
training:	Epoch: [112][200/204]	Loss 0.4414 (0.4024)	
training:	Epoch: [112][201/204]	Loss 0.4064 (0.4024)	
training:	Epoch: [112][202/204]	Loss 0.3715 (0.4022)	
training:	Epoch: [112][203/204]	Loss 0.4567 (0.4025)	
training:	Epoch: [112][204/204]	Loss 0.4222 (0.4026)	
Training:	 Loss: 0.4020

Training:	 ACC: 0.8343 0.8347 0.8442 0.8243
Validation:	 ACC: 0.7974 0.7983 0.8168 0.7780
Validation:	 Best_BACC: 0.7975 0.7988 0.8270 0.7679
Validation:	 Loss: 0.4336
Pretraining:	Epoch 113/120
----------
training:	Epoch: [113][1/204]	Loss 0.3983 (0.3983)	
training:	Epoch: [113][2/204]	Loss 0.7012 (0.5497)	
training:	Epoch: [113][3/204]	Loss 0.6473 (0.5822)	
training:	Epoch: [113][4/204]	Loss 0.5162 (0.5657)	
training:	Epoch: [113][5/204]	Loss 0.3490 (0.5224)	
training:	Epoch: [113][6/204]	Loss 0.3678 (0.4966)	
training:	Epoch: [113][7/204]	Loss 0.3771 (0.4795)	
training:	Epoch: [113][8/204]	Loss 0.2535 (0.4513)	
training:	Epoch: [113][9/204]	Loss 0.3773 (0.4431)	
training:	Epoch: [113][10/204]	Loss 0.4858 (0.4473)	
training:	Epoch: [113][11/204]	Loss 0.4058 (0.4436)	
training:	Epoch: [113][12/204]	Loss 0.3472 (0.4355)	
training:	Epoch: [113][13/204]	Loss 0.5631 (0.4453)	
training:	Epoch: [113][14/204]	Loss 0.4509 (0.4457)	
training:	Epoch: [113][15/204]	Loss 0.3219 (0.4375)	
training:	Epoch: [113][16/204]	Loss 0.4075 (0.4356)	
training:	Epoch: [113][17/204]	Loss 0.2659 (0.4256)	
training:	Epoch: [113][18/204]	Loss 0.5139 (0.4305)	
training:	Epoch: [113][19/204]	Loss 0.3320 (0.4253)	
training:	Epoch: [113][20/204]	Loss 0.4477 (0.4265)	
training:	Epoch: [113][21/204]	Loss 0.5052 (0.4302)	
training:	Epoch: [113][22/204]	Loss 0.4432 (0.4308)	
training:	Epoch: [113][23/204]	Loss 0.2806 (0.4243)	
training:	Epoch: [113][24/204]	Loss 0.5396 (0.4291)	
training:	Epoch: [113][25/204]	Loss 0.4449 (0.4297)	
training:	Epoch: [113][26/204]	Loss 0.5155 (0.4330)	
training:	Epoch: [113][27/204]	Loss 0.4940 (0.4353)	
training:	Epoch: [113][28/204]	Loss 0.3112 (0.4308)	
training:	Epoch: [113][29/204]	Loss 0.3958 (0.4296)	
training:	Epoch: [113][30/204]	Loss 0.2633 (0.4241)	
training:	Epoch: [113][31/204]	Loss 0.3855 (0.4228)	
training:	Epoch: [113][32/204]	Loss 0.4671 (0.4242)	
training:	Epoch: [113][33/204]	Loss 0.4089 (0.4238)	
training:	Epoch: [113][34/204]	Loss 0.5140 (0.4264)	
training:	Epoch: [113][35/204]	Loss 0.5266 (0.4293)	
training:	Epoch: [113][36/204]	Loss 0.2275 (0.4237)	
training:	Epoch: [113][37/204]	Loss 0.2955 (0.4202)	
training:	Epoch: [113][38/204]	Loss 0.3661 (0.4188)	
training:	Epoch: [113][39/204]	Loss 0.4388 (0.4193)	
training:	Epoch: [113][40/204]	Loss 0.3480 (0.4175)	
training:	Epoch: [113][41/204]	Loss 0.2811 (0.4142)	
training:	Epoch: [113][42/204]	Loss 0.4449 (0.4149)	
training:	Epoch: [113][43/204]	Loss 0.3901 (0.4143)	
training:	Epoch: [113][44/204]	Loss 0.3189 (0.4122)	
training:	Epoch: [113][45/204]	Loss 0.2173 (0.4078)	
training:	Epoch: [113][46/204]	Loss 0.5235 (0.4104)	
training:	Epoch: [113][47/204]	Loss 0.4900 (0.4120)	
training:	Epoch: [113][48/204]	Loss 0.4864 (0.4136)	
training:	Epoch: [113][49/204]	Loss 0.2866 (0.4110)	
training:	Epoch: [113][50/204]	Loss 0.3859 (0.4105)	
training:	Epoch: [113][51/204]	Loss 0.4924 (0.4121)	
training:	Epoch: [113][52/204]	Loss 0.2257 (0.4085)	
training:	Epoch: [113][53/204]	Loss 0.3169 (0.4068)	
training:	Epoch: [113][54/204]	Loss 0.3907 (0.4065)	
training:	Epoch: [113][55/204]	Loss 0.5632 (0.4093)	
training:	Epoch: [113][56/204]	Loss 0.3387 (0.4081)	
training:	Epoch: [113][57/204]	Loss 0.2989 (0.4062)	
training:	Epoch: [113][58/204]	Loss 0.2988 (0.4043)	
training:	Epoch: [113][59/204]	Loss 0.2637 (0.4019)	
training:	Epoch: [113][60/204]	Loss 0.3813 (0.4016)	
training:	Epoch: [113][61/204]	Loss 0.4738 (0.4028)	
training:	Epoch: [113][62/204]	Loss 0.2212 (0.3998)	
training:	Epoch: [113][63/204]	Loss 0.6749 (0.4042)	
training:	Epoch: [113][64/204]	Loss 0.2156 (0.4013)	
training:	Epoch: [113][65/204]	Loss 0.3469 (0.4004)	
training:	Epoch: [113][66/204]	Loss 0.3268 (0.3993)	
training:	Epoch: [113][67/204]	Loss 0.4093 (0.3995)	
training:	Epoch: [113][68/204]	Loss 0.2898 (0.3979)	
training:	Epoch: [113][69/204]	Loss 0.4299 (0.3983)	
training:	Epoch: [113][70/204]	Loss 0.3399 (0.3975)	
training:	Epoch: [113][71/204]	Loss 0.4998 (0.3989)	
training:	Epoch: [113][72/204]	Loss 0.5331 (0.4008)	
training:	Epoch: [113][73/204]	Loss 0.4405 (0.4013)	
training:	Epoch: [113][74/204]	Loss 0.3372 (0.4005)	
training:	Epoch: [113][75/204]	Loss 0.4822 (0.4016)	
training:	Epoch: [113][76/204]	Loss 0.4694 (0.4024)	
training:	Epoch: [113][77/204]	Loss 0.2346 (0.4003)	
training:	Epoch: [113][78/204]	Loss 0.3701 (0.3999)	
training:	Epoch: [113][79/204]	Loss 0.4825 (0.4009)	
training:	Epoch: [113][80/204]	Loss 0.2680 (0.3993)	
training:	Epoch: [113][81/204]	Loss 0.4174 (0.3995)	
training:	Epoch: [113][82/204]	Loss 0.3746 (0.3992)	
training:	Epoch: [113][83/204]	Loss 0.3468 (0.3986)	
training:	Epoch: [113][84/204]	Loss 0.3562 (0.3981)	
training:	Epoch: [113][85/204]	Loss 0.4197 (0.3983)	
training:	Epoch: [113][86/204]	Loss 0.2743 (0.3969)	
training:	Epoch: [113][87/204]	Loss 0.3941 (0.3968)	
training:	Epoch: [113][88/204]	Loss 0.4316 (0.3972)	
training:	Epoch: [113][89/204]	Loss 0.3164 (0.3963)	
training:	Epoch: [113][90/204]	Loss 0.3847 (0.3962)	
training:	Epoch: [113][91/204]	Loss 0.4850 (0.3972)	
training:	Epoch: [113][92/204]	Loss 0.4142 (0.3974)	
training:	Epoch: [113][93/204]	Loss 0.3365 (0.3967)	
training:	Epoch: [113][94/204]	Loss 0.3340 (0.3960)	
training:	Epoch: [113][95/204]	Loss 0.3288 (0.3953)	
training:	Epoch: [113][96/204]	Loss 0.3177 (0.3945)	
training:	Epoch: [113][97/204]	Loss 0.5163 (0.3958)	
training:	Epoch: [113][98/204]	Loss 0.4344 (0.3962)	
training:	Epoch: [113][99/204]	Loss 0.3357 (0.3956)	
training:	Epoch: [113][100/204]	Loss 0.4978 (0.3966)	
training:	Epoch: [113][101/204]	Loss 0.2936 (0.3956)	
training:	Epoch: [113][102/204]	Loss 0.2218 (0.3939)	
training:	Epoch: [113][103/204]	Loss 0.2807 (0.3928)	
training:	Epoch: [113][104/204]	Loss 0.3726 (0.3926)	
training:	Epoch: [113][105/204]	Loss 0.3583 (0.3922)	
training:	Epoch: [113][106/204]	Loss 0.5044 (0.3933)	
training:	Epoch: [113][107/204]	Loss 0.4031 (0.3934)	
training:	Epoch: [113][108/204]	Loss 0.3726 (0.3932)	
training:	Epoch: [113][109/204]	Loss 0.3940 (0.3932)	
training:	Epoch: [113][110/204]	Loss 0.4959 (0.3941)	
training:	Epoch: [113][111/204]	Loss 0.5063 (0.3951)	
training:	Epoch: [113][112/204]	Loss 0.2901 (0.3942)	
training:	Epoch: [113][113/204]	Loss 0.4064 (0.3943)	
training:	Epoch: [113][114/204]	Loss 0.6446 (0.3965)	
training:	Epoch: [113][115/204]	Loss 0.3519 (0.3961)	
training:	Epoch: [113][116/204]	Loss 0.5472 (0.3974)	
training:	Epoch: [113][117/204]	Loss 0.4238 (0.3976)	
training:	Epoch: [113][118/204]	Loss 0.3599 (0.3973)	
training:	Epoch: [113][119/204]	Loss 0.4362 (0.3977)	
training:	Epoch: [113][120/204]	Loss 0.2511 (0.3964)	
training:	Epoch: [113][121/204]	Loss 0.3289 (0.3959)	
training:	Epoch: [113][122/204]	Loss 0.3530 (0.3955)	
training:	Epoch: [113][123/204]	Loss 0.3506 (0.3952)	
training:	Epoch: [113][124/204]	Loss 0.4567 (0.3957)	
training:	Epoch: [113][125/204]	Loss 0.4941 (0.3964)	
training:	Epoch: [113][126/204]	Loss 0.3428 (0.3960)	
training:	Epoch: [113][127/204]	Loss 0.4052 (0.3961)	
training:	Epoch: [113][128/204]	Loss 0.4679 (0.3967)	
training:	Epoch: [113][129/204]	Loss 0.4308 (0.3969)	
training:	Epoch: [113][130/204]	Loss 0.4716 (0.3975)	
training:	Epoch: [113][131/204]	Loss 0.1850 (0.3959)	
training:	Epoch: [113][132/204]	Loss 0.4078 (0.3960)	
training:	Epoch: [113][133/204]	Loss 0.6116 (0.3976)	
training:	Epoch: [113][134/204]	Loss 0.2447 (0.3964)	
training:	Epoch: [113][135/204]	Loss 0.5394 (0.3975)	
training:	Epoch: [113][136/204]	Loss 0.4939 (0.3982)	
training:	Epoch: [113][137/204]	Loss 0.5692 (0.3995)	
training:	Epoch: [113][138/204]	Loss 0.3791 (0.3993)	
training:	Epoch: [113][139/204]	Loss 0.3510 (0.3990)	
training:	Epoch: [113][140/204]	Loss 0.4078 (0.3990)	
training:	Epoch: [113][141/204]	Loss 0.3045 (0.3984)	
training:	Epoch: [113][142/204]	Loss 0.3139 (0.3978)	
training:	Epoch: [113][143/204]	Loss 0.4845 (0.3984)	
training:	Epoch: [113][144/204]	Loss 0.3867 (0.3983)	
training:	Epoch: [113][145/204]	Loss 0.5336 (0.3992)	
training:	Epoch: [113][146/204]	Loss 0.3520 (0.3989)	
training:	Epoch: [113][147/204]	Loss 0.5812 (0.4001)	
training:	Epoch: [113][148/204]	Loss 0.3381 (0.3997)	
training:	Epoch: [113][149/204]	Loss 0.6286 (0.4013)	
training:	Epoch: [113][150/204]	Loss 0.4297 (0.4014)	
training:	Epoch: [113][151/204]	Loss 0.4246 (0.4016)	
training:	Epoch: [113][152/204]	Loss 0.5004 (0.4022)	
training:	Epoch: [113][153/204]	Loss 0.3637 (0.4020)	
training:	Epoch: [113][154/204]	Loss 0.3163 (0.4014)	
training:	Epoch: [113][155/204]	Loss 0.5997 (0.4027)	
training:	Epoch: [113][156/204]	Loss 0.2342 (0.4016)	
training:	Epoch: [113][157/204]	Loss 0.4899 (0.4022)	
training:	Epoch: [113][158/204]	Loss 0.4003 (0.4022)	
training:	Epoch: [113][159/204]	Loss 0.5562 (0.4032)	
training:	Epoch: [113][160/204]	Loss 0.4370 (0.4034)	
training:	Epoch: [113][161/204]	Loss 0.3752 (0.4032)	
training:	Epoch: [113][162/204]	Loss 0.3207 (0.4027)	
training:	Epoch: [113][163/204]	Loss 0.3592 (0.4024)	
training:	Epoch: [113][164/204]	Loss 0.2860 (0.4017)	
training:	Epoch: [113][165/204]	Loss 0.3471 (0.4014)	
training:	Epoch: [113][166/204]	Loss 0.2670 (0.4006)	
training:	Epoch: [113][167/204]	Loss 0.4927 (0.4011)	
training:	Epoch: [113][168/204]	Loss 0.3952 (0.4011)	
training:	Epoch: [113][169/204]	Loss 0.3716 (0.4009)	
training:	Epoch: [113][170/204]	Loss 0.2531 (0.4000)	
training:	Epoch: [113][171/204]	Loss 0.4034 (0.4001)	
training:	Epoch: [113][172/204]	Loss 0.5637 (0.4010)	
training:	Epoch: [113][173/204]	Loss 0.5184 (0.4017)	
training:	Epoch: [113][174/204]	Loss 0.5028 (0.4023)	
training:	Epoch: [113][175/204]	Loss 0.4943 (0.4028)	
training:	Epoch: [113][176/204]	Loss 0.4767 (0.4032)	
training:	Epoch: [113][177/204]	Loss 0.3874 (0.4031)	
training:	Epoch: [113][178/204]	Loss 0.3466 (0.4028)	
training:	Epoch: [113][179/204]	Loss 0.3621 (0.4026)	
training:	Epoch: [113][180/204]	Loss 0.2460 (0.4017)	
training:	Epoch: [113][181/204]	Loss 0.3792 (0.4016)	
training:	Epoch: [113][182/204]	Loss 0.3527 (0.4013)	
training:	Epoch: [113][183/204]	Loss 0.4023 (0.4013)	
training:	Epoch: [113][184/204]	Loss 0.3818 (0.4012)	
training:	Epoch: [113][185/204]	Loss 0.4172 (0.4013)	
training:	Epoch: [113][186/204]	Loss 0.3745 (0.4012)	
training:	Epoch: [113][187/204]	Loss 0.3175 (0.4007)	
training:	Epoch: [113][188/204]	Loss 0.3098 (0.4002)	
training:	Epoch: [113][189/204]	Loss 0.2596 (0.3995)	
training:	Epoch: [113][190/204]	Loss 0.4084 (0.3995)	
training:	Epoch: [113][191/204]	Loss 0.2604 (0.3988)	
training:	Epoch: [113][192/204]	Loss 0.3125 (0.3984)	
training:	Epoch: [113][193/204]	Loss 0.2220 (0.3974)	
training:	Epoch: [113][194/204]	Loss 0.4249 (0.3976)	
training:	Epoch: [113][195/204]	Loss 0.3537 (0.3974)	
training:	Epoch: [113][196/204]	Loss 0.2388 (0.3965)	
training:	Epoch: [113][197/204]	Loss 0.3357 (0.3962)	
training:	Epoch: [113][198/204]	Loss 0.5748 (0.3971)	
training:	Epoch: [113][199/204]	Loss 0.3096 (0.3967)	
training:	Epoch: [113][200/204]	Loss 0.5344 (0.3974)	
training:	Epoch: [113][201/204]	Loss 0.4278 (0.3975)	
training:	Epoch: [113][202/204]	Loss 0.2277 (0.3967)	
training:	Epoch: [113][203/204]	Loss 0.3916 (0.3967)	
training:	Epoch: [113][204/204]	Loss 0.4548 (0.3970)	
Training:	 Loss: 0.3963

Training:	 ACC: 0.8340 0.8342 0.8386 0.8294
Validation:	 ACC: 0.7934 0.7940 0.8066 0.7803
Validation:	 Best_BACC: 0.7975 0.7988 0.8270 0.7679
Validation:	 Loss: 0.4328
Pretraining:	Epoch 114/120
----------
training:	Epoch: [114][1/204]	Loss 0.2546 (0.2546)	
training:	Epoch: [114][2/204]	Loss 0.3825 (0.3186)	
training:	Epoch: [114][3/204]	Loss 0.3716 (0.3363)	
training:	Epoch: [114][4/204]	Loss 0.5171 (0.3815)	
training:	Epoch: [114][5/204]	Loss 0.4469 (0.3946)	
training:	Epoch: [114][6/204]	Loss 0.2928 (0.3776)	
training:	Epoch: [114][7/204]	Loss 0.4682 (0.3906)	
training:	Epoch: [114][8/204]	Loss 0.4798 (0.4017)	
training:	Epoch: [114][9/204]	Loss 0.3099 (0.3915)	
training:	Epoch: [114][10/204]	Loss 0.4027 (0.3926)	
training:	Epoch: [114][11/204]	Loss 0.5268 (0.4048)	
training:	Epoch: [114][12/204]	Loss 0.4745 (0.4106)	
training:	Epoch: [114][13/204]	Loss 0.7276 (0.4350)	
training:	Epoch: [114][14/204]	Loss 0.6088 (0.4474)	
training:	Epoch: [114][15/204]	Loss 0.3949 (0.4439)	
training:	Epoch: [114][16/204]	Loss 0.5357 (0.4497)	
training:	Epoch: [114][17/204]	Loss 0.4889 (0.4520)	
training:	Epoch: [114][18/204]	Loss 0.5109 (0.4552)	
training:	Epoch: [114][19/204]	Loss 0.6088 (0.4633)	
training:	Epoch: [114][20/204]	Loss 0.5224 (0.4663)	
training:	Epoch: [114][21/204]	Loss 0.2862 (0.4577)	
training:	Epoch: [114][22/204]	Loss 0.3264 (0.4517)	
training:	Epoch: [114][23/204]	Loss 0.3421 (0.4470)	
training:	Epoch: [114][24/204]	Loss 0.4298 (0.4463)	
training:	Epoch: [114][25/204]	Loss 0.4012 (0.4445)	
training:	Epoch: [114][26/204]	Loss 0.3236 (0.4398)	
training:	Epoch: [114][27/204]	Loss 0.2729 (0.4336)	
training:	Epoch: [114][28/204]	Loss 0.4859 (0.4355)	
training:	Epoch: [114][29/204]	Loss 0.4979 (0.4376)	
training:	Epoch: [114][30/204]	Loss 0.3258 (0.4339)	
training:	Epoch: [114][31/204]	Loss 0.2768 (0.4289)	
training:	Epoch: [114][32/204]	Loss 0.4101 (0.4283)	
training:	Epoch: [114][33/204]	Loss 0.3239 (0.4251)	
training:	Epoch: [114][34/204]	Loss 0.3176 (0.4219)	
training:	Epoch: [114][35/204]	Loss 0.5103 (0.4245)	
training:	Epoch: [114][36/204]	Loss 0.4297 (0.4246)	
training:	Epoch: [114][37/204]	Loss 0.3571 (0.4228)	
training:	Epoch: [114][38/204]	Loss 0.3936 (0.4220)	
training:	Epoch: [114][39/204]	Loss 0.4068 (0.4216)	
training:	Epoch: [114][40/204]	Loss 0.6469 (0.4273)	
training:	Epoch: [114][41/204]	Loss 0.4112 (0.4269)	
training:	Epoch: [114][42/204]	Loss 0.4106 (0.4265)	
training:	Epoch: [114][43/204]	Loss 0.3877 (0.4256)	
training:	Epoch: [114][44/204]	Loss 0.3655 (0.4242)	
training:	Epoch: [114][45/204]	Loss 0.4918 (0.4257)	
training:	Epoch: [114][46/204]	Loss 0.3873 (0.4249)	
training:	Epoch: [114][47/204]	Loss 0.4373 (0.4251)	
training:	Epoch: [114][48/204]	Loss 0.6054 (0.4289)	
training:	Epoch: [114][49/204]	Loss 0.2119 (0.4245)	
training:	Epoch: [114][50/204]	Loss 0.2323 (0.4206)	
training:	Epoch: [114][51/204]	Loss 0.2881 (0.4180)	
training:	Epoch: [114][52/204]	Loss 0.4407 (0.4185)	
training:	Epoch: [114][53/204]	Loss 0.4485 (0.4190)	
training:	Epoch: [114][54/204]	Loss 0.3598 (0.4179)	
training:	Epoch: [114][55/204]	Loss 0.3097 (0.4160)	
training:	Epoch: [114][56/204]	Loss 0.4715 (0.4170)	
training:	Epoch: [114][57/204]	Loss 0.3102 (0.4151)	
training:	Epoch: [114][58/204]	Loss 0.6283 (0.4188)	
training:	Epoch: [114][59/204]	Loss 0.4866 (0.4199)	
training:	Epoch: [114][60/204]	Loss 0.3758 (0.4192)	
training:	Epoch: [114][61/204]	Loss 0.4660 (0.4199)	
training:	Epoch: [114][62/204]	Loss 0.4419 (0.4203)	
training:	Epoch: [114][63/204]	Loss 0.3967 (0.4199)	
training:	Epoch: [114][64/204]	Loss 0.4226 (0.4200)	
training:	Epoch: [114][65/204]	Loss 0.4409 (0.4203)	
training:	Epoch: [114][66/204]	Loss 0.5624 (0.4224)	
training:	Epoch: [114][67/204]	Loss 0.4595 (0.4230)	
training:	Epoch: [114][68/204]	Loss 0.4715 (0.4237)	
training:	Epoch: [114][69/204]	Loss 0.3157 (0.4221)	
training:	Epoch: [114][70/204]	Loss 0.6138 (0.4249)	
training:	Epoch: [114][71/204]	Loss 0.2770 (0.4228)	
training:	Epoch: [114][72/204]	Loss 0.3078 (0.4212)	
training:	Epoch: [114][73/204]	Loss 0.3673 (0.4205)	
training:	Epoch: [114][74/204]	Loss 0.5607 (0.4224)	
training:	Epoch: [114][75/204]	Loss 0.3979 (0.4220)	
training:	Epoch: [114][76/204]	Loss 0.3335 (0.4209)	
training:	Epoch: [114][77/204]	Loss 0.4990 (0.4219)	
training:	Epoch: [114][78/204]	Loss 0.4190 (0.4218)	
training:	Epoch: [114][79/204]	Loss 0.2300 (0.4194)	
training:	Epoch: [114][80/204]	Loss 0.3595 (0.4187)	
training:	Epoch: [114][81/204]	Loss 0.5249 (0.4200)	
training:	Epoch: [114][82/204]	Loss 0.3019 (0.4185)	
training:	Epoch: [114][83/204]	Loss 0.3604 (0.4178)	
training:	Epoch: [114][84/204]	Loss 0.4527 (0.4183)	
training:	Epoch: [114][85/204]	Loss 0.3603 (0.4176)	
training:	Epoch: [114][86/204]	Loss 0.2727 (0.4159)	
training:	Epoch: [114][87/204]	Loss 0.3142 (0.4147)	
training:	Epoch: [114][88/204]	Loss 0.2073 (0.4124)	
training:	Epoch: [114][89/204]	Loss 0.4915 (0.4133)	
training:	Epoch: [114][90/204]	Loss 0.3628 (0.4127)	
training:	Epoch: [114][91/204]	Loss 0.3169 (0.4116)	
training:	Epoch: [114][92/204]	Loss 0.4400 (0.4120)	
training:	Epoch: [114][93/204]	Loss 0.3247 (0.4110)	
training:	Epoch: [114][94/204]	Loss 0.2765 (0.4096)	
training:	Epoch: [114][95/204]	Loss 0.3589 (0.4090)	
training:	Epoch: [114][96/204]	Loss 0.5322 (0.4103)	
training:	Epoch: [114][97/204]	Loss 0.3528 (0.4097)	
training:	Epoch: [114][98/204]	Loss 0.2988 (0.4086)	
training:	Epoch: [114][99/204]	Loss 0.2907 (0.4074)	
training:	Epoch: [114][100/204]	Loss 0.2561 (0.4059)	
training:	Epoch: [114][101/204]	Loss 0.5563 (0.4074)	
training:	Epoch: [114][102/204]	Loss 0.4609 (0.4079)	
training:	Epoch: [114][103/204]	Loss 0.2858 (0.4067)	
training:	Epoch: [114][104/204]	Loss 0.4062 (0.4067)	
training:	Epoch: [114][105/204]	Loss 0.2951 (0.4057)	
training:	Epoch: [114][106/204]	Loss 0.3874 (0.4055)	
training:	Epoch: [114][107/204]	Loss 0.4725 (0.4061)	
training:	Epoch: [114][108/204]	Loss 0.3014 (0.4051)	
training:	Epoch: [114][109/204]	Loss 0.3463 (0.4046)	
training:	Epoch: [114][110/204]	Loss 0.3913 (0.4045)	
training:	Epoch: [114][111/204]	Loss 0.2602 (0.4032)	
training:	Epoch: [114][112/204]	Loss 0.3329 (0.4026)	
training:	Epoch: [114][113/204]	Loss 0.3153 (0.4018)	
training:	Epoch: [114][114/204]	Loss 0.4395 (0.4021)	
training:	Epoch: [114][115/204]	Loss 0.4588 (0.4026)	
training:	Epoch: [114][116/204]	Loss 0.3713 (0.4023)	
training:	Epoch: [114][117/204]	Loss 0.3410 (0.4018)	
training:	Epoch: [114][118/204]	Loss 0.3800 (0.4016)	
training:	Epoch: [114][119/204]	Loss 0.2883 (0.4007)	
training:	Epoch: [114][120/204]	Loss 0.6386 (0.4027)	
training:	Epoch: [114][121/204]	Loss 0.3320 (0.4021)	
training:	Epoch: [114][122/204]	Loss 0.4379 (0.4024)	
training:	Epoch: [114][123/204]	Loss 0.3002 (0.4015)	
training:	Epoch: [114][124/204]	Loss 0.4332 (0.4018)	
training:	Epoch: [114][125/204]	Loss 0.4978 (0.4026)	
training:	Epoch: [114][126/204]	Loss 0.4157 (0.4027)	
training:	Epoch: [114][127/204]	Loss 0.4278 (0.4029)	
training:	Epoch: [114][128/204]	Loss 0.4758 (0.4034)	
training:	Epoch: [114][129/204]	Loss 0.2590 (0.4023)	
training:	Epoch: [114][130/204]	Loss 0.3012 (0.4015)	
training:	Epoch: [114][131/204]	Loss 0.3235 (0.4009)	
training:	Epoch: [114][132/204]	Loss 0.3875 (0.4008)	
training:	Epoch: [114][133/204]	Loss 0.3718 (0.4006)	
training:	Epoch: [114][134/204]	Loss 0.3557 (0.4003)	
training:	Epoch: [114][135/204]	Loss 0.6302 (0.4020)	
training:	Epoch: [114][136/204]	Loss 0.3077 (0.4013)	
training:	Epoch: [114][137/204]	Loss 0.4880 (0.4019)	
training:	Epoch: [114][138/204]	Loss 0.3569 (0.4016)	
training:	Epoch: [114][139/204]	Loss 0.3011 (0.4009)	
training:	Epoch: [114][140/204]	Loss 0.5743 (0.4021)	
training:	Epoch: [114][141/204]	Loss 0.3806 (0.4020)	
training:	Epoch: [114][142/204]	Loss 0.4996 (0.4027)	
training:	Epoch: [114][143/204]	Loss 0.4599 (0.4031)	
training:	Epoch: [114][144/204]	Loss 0.4834 (0.4036)	
training:	Epoch: [114][145/204]	Loss 0.4535 (0.4040)	
training:	Epoch: [114][146/204]	Loss 0.4746 (0.4044)	
training:	Epoch: [114][147/204]	Loss 0.4950 (0.4051)	
training:	Epoch: [114][148/204]	Loss 0.4660 (0.4055)	
training:	Epoch: [114][149/204]	Loss 0.3528 (0.4051)	
training:	Epoch: [114][150/204]	Loss 0.3925 (0.4050)	
training:	Epoch: [114][151/204]	Loss 0.2620 (0.4041)	
training:	Epoch: [114][152/204]	Loss 0.4109 (0.4041)	
training:	Epoch: [114][153/204]	Loss 0.4540 (0.4045)	
training:	Epoch: [114][154/204]	Loss 0.4047 (0.4045)	
training:	Epoch: [114][155/204]	Loss 0.3990 (0.4044)	
training:	Epoch: [114][156/204]	Loss 0.3056 (0.4038)	
training:	Epoch: [114][157/204]	Loss 0.5540 (0.4047)	
training:	Epoch: [114][158/204]	Loss 0.4053 (0.4047)	
training:	Epoch: [114][159/204]	Loss 0.5022 (0.4054)	
training:	Epoch: [114][160/204]	Loss 0.5622 (0.4063)	
training:	Epoch: [114][161/204]	Loss 0.3695 (0.4061)	
training:	Epoch: [114][162/204]	Loss 0.3560 (0.4058)	
training:	Epoch: [114][163/204]	Loss 0.5243 (0.4065)	
training:	Epoch: [114][164/204]	Loss 0.3766 (0.4063)	
training:	Epoch: [114][165/204]	Loss 0.4356 (0.4065)	
training:	Epoch: [114][166/204]	Loss 0.4689 (0.4069)	
training:	Epoch: [114][167/204]	Loss 0.3003 (0.4063)	
training:	Epoch: [114][168/204]	Loss 0.5146 (0.4069)	
training:	Epoch: [114][169/204]	Loss 0.4938 (0.4074)	
training:	Epoch: [114][170/204]	Loss 0.3765 (0.4072)	
training:	Epoch: [114][171/204]	Loss 0.2179 (0.4061)	
training:	Epoch: [114][172/204]	Loss 0.2844 (0.4054)	
training:	Epoch: [114][173/204]	Loss 0.4300 (0.4056)	
training:	Epoch: [114][174/204]	Loss 0.3983 (0.4055)	
training:	Epoch: [114][175/204]	Loss 0.5115 (0.4061)	
training:	Epoch: [114][176/204]	Loss 0.4732 (0.4065)	
training:	Epoch: [114][177/204]	Loss 0.2640 (0.4057)	
training:	Epoch: [114][178/204]	Loss 0.4945 (0.4062)	
training:	Epoch: [114][179/204]	Loss 0.2393 (0.4053)	
training:	Epoch: [114][180/204]	Loss 0.3744 (0.4051)	
training:	Epoch: [114][181/204]	Loss 0.3519 (0.4048)	
training:	Epoch: [114][182/204]	Loss 0.3474 (0.4045)	
training:	Epoch: [114][183/204]	Loss 0.3875 (0.4044)	
training:	Epoch: [114][184/204]	Loss 0.4919 (0.4049)	
training:	Epoch: [114][185/204]	Loss 0.5276 (0.4055)	
training:	Epoch: [114][186/204]	Loss 0.3691 (0.4053)	
training:	Epoch: [114][187/204]	Loss 0.4459 (0.4056)	
training:	Epoch: [114][188/204]	Loss 0.4007 (0.4055)	
training:	Epoch: [114][189/204]	Loss 0.2967 (0.4050)	
training:	Epoch: [114][190/204]	Loss 0.3730 (0.4048)	
training:	Epoch: [114][191/204]	Loss 0.3936 (0.4047)	
training:	Epoch: [114][192/204]	Loss 0.3287 (0.4043)	
training:	Epoch: [114][193/204]	Loss 0.3007 (0.4038)	
training:	Epoch: [114][194/204]	Loss 0.3055 (0.4033)	
training:	Epoch: [114][195/204]	Loss 0.2883 (0.4027)	
training:	Epoch: [114][196/204]	Loss 0.2430 (0.4019)	
training:	Epoch: [114][197/204]	Loss 0.4486 (0.4021)	
training:	Epoch: [114][198/204]	Loss 0.4375 (0.4023)	
training:	Epoch: [114][199/204]	Loss 0.3378 (0.4020)	
training:	Epoch: [114][200/204]	Loss 0.2609 (0.4013)	
training:	Epoch: [114][201/204]	Loss 0.4283 (0.4014)	
training:	Epoch: [114][202/204]	Loss 0.4220 (0.4015)	
training:	Epoch: [114][203/204]	Loss 0.3789 (0.4014)	
training:	Epoch: [114][204/204]	Loss 0.3920 (0.4014)	
Training:	 Loss: 0.4007

Training:	 ACC: 0.8351 0.8356 0.8469 0.8233
Validation:	 ACC: 0.7988 0.7999 0.8229 0.7747
Validation:	 Best_BACC: 0.7988 0.7999 0.8229 0.7747
Validation:	 Loss: 0.4324
Pretraining:	Epoch 115/120
----------
training:	Epoch: [115][1/204]	Loss 0.3308 (0.3308)	
training:	Epoch: [115][2/204]	Loss 0.4293 (0.3801)	
training:	Epoch: [115][3/204]	Loss 0.4448 (0.4017)	
training:	Epoch: [115][4/204]	Loss 0.3746 (0.3949)	
training:	Epoch: [115][5/204]	Loss 0.4637 (0.4087)	
training:	Epoch: [115][6/204]	Loss 0.4565 (0.4166)	
training:	Epoch: [115][7/204]	Loss 0.4144 (0.4163)	
training:	Epoch: [115][8/204]	Loss 0.5230 (0.4297)	
training:	Epoch: [115][9/204]	Loss 0.4080 (0.4273)	
training:	Epoch: [115][10/204]	Loss 0.3448 (0.4190)	
training:	Epoch: [115][11/204]	Loss 0.5747 (0.4332)	
training:	Epoch: [115][12/204]	Loss 0.3183 (0.4236)	
training:	Epoch: [115][13/204]	Loss 0.3035 (0.4144)	
training:	Epoch: [115][14/204]	Loss 0.5255 (0.4223)	
training:	Epoch: [115][15/204]	Loss 0.5413 (0.4302)	
training:	Epoch: [115][16/204]	Loss 0.2465 (0.4187)	
training:	Epoch: [115][17/204]	Loss 0.4539 (0.4208)	
training:	Epoch: [115][18/204]	Loss 0.3646 (0.4177)	
training:	Epoch: [115][19/204]	Loss 0.3924 (0.4164)	
training:	Epoch: [115][20/204]	Loss 0.5022 (0.4207)	
training:	Epoch: [115][21/204]	Loss 0.3834 (0.4189)	
training:	Epoch: [115][22/204]	Loss 0.3955 (0.4178)	
training:	Epoch: [115][23/204]	Loss 0.4311 (0.4184)	
training:	Epoch: [115][24/204]	Loss 0.4041 (0.4178)	
training:	Epoch: [115][25/204]	Loss 0.3989 (0.4170)	
training:	Epoch: [115][26/204]	Loss 0.2790 (0.4117)	
training:	Epoch: [115][27/204]	Loss 0.3322 (0.4088)	
training:	Epoch: [115][28/204]	Loss 0.3741 (0.4076)	
training:	Epoch: [115][29/204]	Loss 0.4448 (0.4088)	
training:	Epoch: [115][30/204]	Loss 0.2620 (0.4039)	
training:	Epoch: [115][31/204]	Loss 0.2740 (0.3997)	
training:	Epoch: [115][32/204]	Loss 0.5768 (0.4053)	
training:	Epoch: [115][33/204]	Loss 0.4981 (0.4081)	
training:	Epoch: [115][34/204]	Loss 0.3825 (0.4073)	
training:	Epoch: [115][35/204]	Loss 0.3802 (0.4066)	
training:	Epoch: [115][36/204]	Loss 0.3554 (0.4051)	
training:	Epoch: [115][37/204]	Loss 0.4653 (0.4068)	
training:	Epoch: [115][38/204]	Loss 0.4816 (0.4087)	
training:	Epoch: [115][39/204]	Loss 0.3409 (0.4070)	
training:	Epoch: [115][40/204]	Loss 0.3436 (0.4054)	
training:	Epoch: [115][41/204]	Loss 0.3814 (0.4048)	
training:	Epoch: [115][42/204]	Loss 0.2218 (0.4005)	
training:	Epoch: [115][43/204]	Loss 0.4268 (0.4011)	
training:	Epoch: [115][44/204]	Loss 0.2772 (0.3983)	
training:	Epoch: [115][45/204]	Loss 0.4273 (0.3989)	
training:	Epoch: [115][46/204]	Loss 0.3024 (0.3968)	
training:	Epoch: [115][47/204]	Loss 0.4364 (0.3977)	
training:	Epoch: [115][48/204]	Loss 0.2741 (0.3951)	
training:	Epoch: [115][49/204]	Loss 0.3581 (0.3943)	
training:	Epoch: [115][50/204]	Loss 0.4260 (0.3950)	
training:	Epoch: [115][51/204]	Loss 0.3858 (0.3948)	
training:	Epoch: [115][52/204]	Loss 0.3684 (0.3943)	
training:	Epoch: [115][53/204]	Loss 0.4025 (0.3944)	
training:	Epoch: [115][54/204]	Loss 0.3214 (0.3931)	
training:	Epoch: [115][55/204]	Loss 0.3140 (0.3916)	
training:	Epoch: [115][56/204]	Loss 0.4693 (0.3930)	
training:	Epoch: [115][57/204]	Loss 0.3772 (0.3928)	
training:	Epoch: [115][58/204]	Loss 0.4119 (0.3931)	
training:	Epoch: [115][59/204]	Loss 0.3648 (0.3926)	
training:	Epoch: [115][60/204]	Loss 0.2609 (0.3904)	
training:	Epoch: [115][61/204]	Loss 0.2828 (0.3886)	
training:	Epoch: [115][62/204]	Loss 0.3871 (0.3886)	
training:	Epoch: [115][63/204]	Loss 0.3208 (0.3875)	
training:	Epoch: [115][64/204]	Loss 0.2698 (0.3857)	
training:	Epoch: [115][65/204]	Loss 0.2861 (0.3842)	
training:	Epoch: [115][66/204]	Loss 0.4568 (0.3853)	
training:	Epoch: [115][67/204]	Loss 0.4138 (0.3857)	
training:	Epoch: [115][68/204]	Loss 0.1655 (0.3825)	
training:	Epoch: [115][69/204]	Loss 0.4765 (0.3838)	
training:	Epoch: [115][70/204]	Loss 0.3564 (0.3834)	
training:	Epoch: [115][71/204]	Loss 0.2935 (0.3822)	
training:	Epoch: [115][72/204]	Loss 0.4459 (0.3830)	
training:	Epoch: [115][73/204]	Loss 0.6613 (0.3869)	
training:	Epoch: [115][74/204]	Loss 0.2888 (0.3855)	
training:	Epoch: [115][75/204]	Loss 0.4711 (0.3867)	
training:	Epoch: [115][76/204]	Loss 0.4533 (0.3876)	
training:	Epoch: [115][77/204]	Loss 0.4460 (0.3883)	
training:	Epoch: [115][78/204]	Loss 0.3903 (0.3883)	
training:	Epoch: [115][79/204]	Loss 0.2740 (0.3869)	
training:	Epoch: [115][80/204]	Loss 0.4381 (0.3875)	
training:	Epoch: [115][81/204]	Loss 0.4612 (0.3884)	
training:	Epoch: [115][82/204]	Loss 0.5395 (0.3903)	
training:	Epoch: [115][83/204]	Loss 0.5027 (0.3916)	
training:	Epoch: [115][84/204]	Loss 0.2864 (0.3904)	
training:	Epoch: [115][85/204]	Loss 0.2384 (0.3886)	
training:	Epoch: [115][86/204]	Loss 0.3179 (0.3878)	
training:	Epoch: [115][87/204]	Loss 0.4279 (0.3882)	
training:	Epoch: [115][88/204]	Loss 0.5669 (0.3903)	
training:	Epoch: [115][89/204]	Loss 0.4084 (0.3905)	
training:	Epoch: [115][90/204]	Loss 0.5862 (0.3926)	
training:	Epoch: [115][91/204]	Loss 0.5067 (0.3939)	
training:	Epoch: [115][92/204]	Loss 0.6793 (0.3970)	
training:	Epoch: [115][93/204]	Loss 0.2718 (0.3957)	
training:	Epoch: [115][94/204]	Loss 0.5218 (0.3970)	
training:	Epoch: [115][95/204]	Loss 0.5519 (0.3986)	
training:	Epoch: [115][96/204]	Loss 0.3620 (0.3982)	
training:	Epoch: [115][97/204]	Loss 0.3100 (0.3973)	
training:	Epoch: [115][98/204]	Loss 0.3213 (0.3966)	
training:	Epoch: [115][99/204]	Loss 0.4954 (0.3976)	
training:	Epoch: [115][100/204]	Loss 0.4546 (0.3981)	
training:	Epoch: [115][101/204]	Loss 0.3704 (0.3979)	
training:	Epoch: [115][102/204]	Loss 0.3190 (0.3971)	
training:	Epoch: [115][103/204]	Loss 0.4176 (0.3973)	
training:	Epoch: [115][104/204]	Loss 0.3970 (0.3973)	
training:	Epoch: [115][105/204]	Loss 0.2980 (0.3963)	
training:	Epoch: [115][106/204]	Loss 0.4848 (0.3972)	
training:	Epoch: [115][107/204]	Loss 0.4491 (0.3977)	
training:	Epoch: [115][108/204]	Loss 0.3674 (0.3974)	
training:	Epoch: [115][109/204]	Loss 0.4449 (0.3978)	
training:	Epoch: [115][110/204]	Loss 0.3047 (0.3970)	
training:	Epoch: [115][111/204]	Loss 0.3259 (0.3963)	
training:	Epoch: [115][112/204]	Loss 0.3691 (0.3961)	
training:	Epoch: [115][113/204]	Loss 0.4165 (0.3963)	
training:	Epoch: [115][114/204]	Loss 0.4453 (0.3967)	
training:	Epoch: [115][115/204]	Loss 0.5691 (0.3982)	
training:	Epoch: [115][116/204]	Loss 0.3141 (0.3975)	
training:	Epoch: [115][117/204]	Loss 0.4769 (0.3981)	
training:	Epoch: [115][118/204]	Loss 0.4447 (0.3985)	
training:	Epoch: [115][119/204]	Loss 0.3094 (0.3978)	
training:	Epoch: [115][120/204]	Loss 0.4420 (0.3982)	
training:	Epoch: [115][121/204]	Loss 0.6282 (0.4001)	
training:	Epoch: [115][122/204]	Loss 0.4919 (0.4008)	
training:	Epoch: [115][123/204]	Loss 0.2784 (0.3998)	
training:	Epoch: [115][124/204]	Loss 0.2880 (0.3989)	
training:	Epoch: [115][125/204]	Loss 0.4168 (0.3991)	
training:	Epoch: [115][126/204]	Loss 0.2385 (0.3978)	
training:	Epoch: [115][127/204]	Loss 0.3781 (0.3976)	
training:	Epoch: [115][128/204]	Loss 0.3254 (0.3971)	
training:	Epoch: [115][129/204]	Loss 0.1849 (0.3954)	
training:	Epoch: [115][130/204]	Loss 0.3080 (0.3947)	
training:	Epoch: [115][131/204]	Loss 0.4516 (0.3952)	
training:	Epoch: [115][132/204]	Loss 0.4003 (0.3952)	
training:	Epoch: [115][133/204]	Loss 0.3959 (0.3952)	
training:	Epoch: [115][134/204]	Loss 0.4897 (0.3959)	
training:	Epoch: [115][135/204]	Loss 0.4004 (0.3960)	
training:	Epoch: [115][136/204]	Loss 0.3206 (0.3954)	
training:	Epoch: [115][137/204]	Loss 0.2259 (0.3942)	
training:	Epoch: [115][138/204]	Loss 0.3570 (0.3939)	
training:	Epoch: [115][139/204]	Loss 0.4276 (0.3941)	
training:	Epoch: [115][140/204]	Loss 0.3064 (0.3935)	
training:	Epoch: [115][141/204]	Loss 0.4107 (0.3936)	
training:	Epoch: [115][142/204]	Loss 0.3641 (0.3934)	
training:	Epoch: [115][143/204]	Loss 0.3176 (0.3929)	
training:	Epoch: [115][144/204]	Loss 0.3443 (0.3926)	
training:	Epoch: [115][145/204]	Loss 0.2892 (0.3918)	
training:	Epoch: [115][146/204]	Loss 0.4973 (0.3926)	
training:	Epoch: [115][147/204]	Loss 0.3290 (0.3921)	
training:	Epoch: [115][148/204]	Loss 0.4390 (0.3925)	
training:	Epoch: [115][149/204]	Loss 0.3261 (0.3920)	
training:	Epoch: [115][150/204]	Loss 0.3794 (0.3919)	
training:	Epoch: [115][151/204]	Loss 0.2793 (0.3912)	
training:	Epoch: [115][152/204]	Loss 0.3220 (0.3907)	
training:	Epoch: [115][153/204]	Loss 0.5124 (0.3915)	
training:	Epoch: [115][154/204]	Loss 0.5971 (0.3929)	
training:	Epoch: [115][155/204]	Loss 0.4134 (0.3930)	
training:	Epoch: [115][156/204]	Loss 0.3446 (0.3927)	
training:	Epoch: [115][157/204]	Loss 0.6546 (0.3943)	
training:	Epoch: [115][158/204]	Loss 0.3654 (0.3942)	
training:	Epoch: [115][159/204]	Loss 0.3083 (0.3936)	
training:	Epoch: [115][160/204]	Loss 0.6927 (0.3955)	
training:	Epoch: [115][161/204]	Loss 0.2664 (0.3947)	
training:	Epoch: [115][162/204]	Loss 0.3642 (0.3945)	
training:	Epoch: [115][163/204]	Loss 0.3269 (0.3941)	
training:	Epoch: [115][164/204]	Loss 0.5167 (0.3948)	
training:	Epoch: [115][165/204]	Loss 0.4125 (0.3949)	
training:	Epoch: [115][166/204]	Loss 0.3843 (0.3949)	
training:	Epoch: [115][167/204]	Loss 0.4052 (0.3949)	
training:	Epoch: [115][168/204]	Loss 0.4972 (0.3955)	
training:	Epoch: [115][169/204]	Loss 0.4090 (0.3956)	
training:	Epoch: [115][170/204]	Loss 0.2720 (0.3949)	
training:	Epoch: [115][171/204]	Loss 0.5353 (0.3957)	
training:	Epoch: [115][172/204]	Loss 0.4611 (0.3961)	
training:	Epoch: [115][173/204]	Loss 0.5761 (0.3971)	
training:	Epoch: [115][174/204]	Loss 0.2350 (0.3962)	
training:	Epoch: [115][175/204]	Loss 0.4326 (0.3964)	
training:	Epoch: [115][176/204]	Loss 0.4013 (0.3964)	
training:	Epoch: [115][177/204]	Loss 0.6131 (0.3977)	
training:	Epoch: [115][178/204]	Loss 0.3498 (0.3974)	
training:	Epoch: [115][179/204]	Loss 0.4277 (0.3976)	
training:	Epoch: [115][180/204]	Loss 0.5565 (0.3985)	
training:	Epoch: [115][181/204]	Loss 0.4612 (0.3988)	
training:	Epoch: [115][182/204]	Loss 0.3077 (0.3983)	
training:	Epoch: [115][183/204]	Loss 0.4004 (0.3983)	
training:	Epoch: [115][184/204]	Loss 0.3684 (0.3981)	
training:	Epoch: [115][185/204]	Loss 0.5791 (0.3991)	
training:	Epoch: [115][186/204]	Loss 0.3700 (0.3990)	
training:	Epoch: [115][187/204]	Loss 0.5089 (0.3996)	
training:	Epoch: [115][188/204]	Loss 0.6460 (0.4009)	
training:	Epoch: [115][189/204]	Loss 0.2739 (0.4002)	
training:	Epoch: [115][190/204]	Loss 0.4535 (0.4005)	
training:	Epoch: [115][191/204]	Loss 0.4951 (0.4010)	
training:	Epoch: [115][192/204]	Loss 0.4090 (0.4010)	
training:	Epoch: [115][193/204]	Loss 0.3316 (0.4007)	
training:	Epoch: [115][194/204]	Loss 0.3852 (0.4006)	
training:	Epoch: [115][195/204]	Loss 0.3383 (0.4003)	
training:	Epoch: [115][196/204]	Loss 0.4876 (0.4007)	
training:	Epoch: [115][197/204]	Loss 0.3357 (0.4004)	
training:	Epoch: [115][198/204]	Loss 0.3557 (0.4001)	
training:	Epoch: [115][199/204]	Loss 0.3668 (0.4000)	
training:	Epoch: [115][200/204]	Loss 0.4500 (0.4002)	
training:	Epoch: [115][201/204]	Loss 0.2628 (0.3995)	
training:	Epoch: [115][202/204]	Loss 0.4007 (0.3996)	
training:	Epoch: [115][203/204]	Loss 0.5301 (0.4002)	
training:	Epoch: [115][204/204]	Loss 0.5130 (0.4007)	
Training:	 Loss: 0.4001

Training:	 ACC: 0.8358 0.8362 0.8457 0.8259
Validation:	 ACC: 0.7978 0.7988 0.8199 0.7758
Validation:	 Best_BACC: 0.7988 0.7999 0.8229 0.7747
Validation:	 Loss: 0.4318
Pretraining:	Epoch 116/120
----------
training:	Epoch: [116][1/204]	Loss 0.2820 (0.2820)	
training:	Epoch: [116][2/204]	Loss 0.2865 (0.2843)	
training:	Epoch: [116][3/204]	Loss 0.5094 (0.3593)	
training:	Epoch: [116][4/204]	Loss 0.3428 (0.3552)	
training:	Epoch: [116][5/204]	Loss 0.5993 (0.4040)	
training:	Epoch: [116][6/204]	Loss 0.3257 (0.3910)	
training:	Epoch: [116][7/204]	Loss 0.5045 (0.4072)	
training:	Epoch: [116][8/204]	Loss 0.3452 (0.3994)	
training:	Epoch: [116][9/204]	Loss 0.4429 (0.4043)	
training:	Epoch: [116][10/204]	Loss 0.3365 (0.3975)	
training:	Epoch: [116][11/204]	Loss 0.3619 (0.3943)	
training:	Epoch: [116][12/204]	Loss 0.4808 (0.4015)	
training:	Epoch: [116][13/204]	Loss 0.4545 (0.4055)	
training:	Epoch: [116][14/204]	Loss 0.4311 (0.4074)	
training:	Epoch: [116][15/204]	Loss 0.2705 (0.3982)	
training:	Epoch: [116][16/204]	Loss 0.3348 (0.3943)	
training:	Epoch: [116][17/204]	Loss 0.4694 (0.3987)	
training:	Epoch: [116][18/204]	Loss 0.2292 (0.3893)	
training:	Epoch: [116][19/204]	Loss 0.2511 (0.3820)	
training:	Epoch: [116][20/204]	Loss 0.5613 (0.3910)	
training:	Epoch: [116][21/204]	Loss 0.3351 (0.3883)	
training:	Epoch: [116][22/204]	Loss 0.4349 (0.3904)	
training:	Epoch: [116][23/204]	Loss 0.2029 (0.3823)	
training:	Epoch: [116][24/204]	Loss 0.3712 (0.3818)	
training:	Epoch: [116][25/204]	Loss 0.4347 (0.3839)	
training:	Epoch: [116][26/204]	Loss 0.3445 (0.3824)	
training:	Epoch: [116][27/204]	Loss 0.4289 (0.3841)	
training:	Epoch: [116][28/204]	Loss 0.6200 (0.3926)	
training:	Epoch: [116][29/204]	Loss 0.3868 (0.3924)	
training:	Epoch: [116][30/204]	Loss 0.3728 (0.3917)	
training:	Epoch: [116][31/204]	Loss 0.4290 (0.3929)	
training:	Epoch: [116][32/204]	Loss 0.3445 (0.3914)	
training:	Epoch: [116][33/204]	Loss 0.7170 (0.4013)	
training:	Epoch: [116][34/204]	Loss 0.4916 (0.4039)	
training:	Epoch: [116][35/204]	Loss 0.5502 (0.4081)	
training:	Epoch: [116][36/204]	Loss 0.5664 (0.4125)	
training:	Epoch: [116][37/204]	Loss 0.3338 (0.4104)	
training:	Epoch: [116][38/204]	Loss 0.2711 (0.4067)	
training:	Epoch: [116][39/204]	Loss 0.5059 (0.4093)	
training:	Epoch: [116][40/204]	Loss 0.5121 (0.4118)	
training:	Epoch: [116][41/204]	Loss 0.4099 (0.4118)	
training:	Epoch: [116][42/204]	Loss 0.4302 (0.4122)	
training:	Epoch: [116][43/204]	Loss 0.3263 (0.4102)	
training:	Epoch: [116][44/204]	Loss 0.6496 (0.4157)	
training:	Epoch: [116][45/204]	Loss 0.4309 (0.4160)	
training:	Epoch: [116][46/204]	Loss 0.5731 (0.4194)	
training:	Epoch: [116][47/204]	Loss 0.3550 (0.4180)	
training:	Epoch: [116][48/204]	Loss 0.4117 (0.4179)	
training:	Epoch: [116][49/204]	Loss 0.4180 (0.4179)	
training:	Epoch: [116][50/204]	Loss 0.2919 (0.4154)	
training:	Epoch: [116][51/204]	Loss 0.4319 (0.4157)	
training:	Epoch: [116][52/204]	Loss 0.4747 (0.4169)	
training:	Epoch: [116][53/204]	Loss 0.2818 (0.4143)	
training:	Epoch: [116][54/204]	Loss 0.4456 (0.4149)	
training:	Epoch: [116][55/204]	Loss 0.3829 (0.4143)	
training:	Epoch: [116][56/204]	Loss 0.2988 (0.4122)	
training:	Epoch: [116][57/204]	Loss 0.5658 (0.4149)	
training:	Epoch: [116][58/204]	Loss 0.2437 (0.4120)	
training:	Epoch: [116][59/204]	Loss 0.5297 (0.4140)	
training:	Epoch: [116][60/204]	Loss 0.4832 (0.4151)	
training:	Epoch: [116][61/204]	Loss 0.4264 (0.4153)	
training:	Epoch: [116][62/204]	Loss 0.3510 (0.4143)	
training:	Epoch: [116][63/204]	Loss 0.5978 (0.4172)	
training:	Epoch: [116][64/204]	Loss 0.3504 (0.4161)	
training:	Epoch: [116][65/204]	Loss 0.3247 (0.4147)	
training:	Epoch: [116][66/204]	Loss 0.4089 (0.4147)	
training:	Epoch: [116][67/204]	Loss 0.2498 (0.4122)	
training:	Epoch: [116][68/204]	Loss 0.3384 (0.4111)	
training:	Epoch: [116][69/204]	Loss 0.2833 (0.4093)	
training:	Epoch: [116][70/204]	Loss 0.4479 (0.4098)	
training:	Epoch: [116][71/204]	Loss 0.4407 (0.4102)	
training:	Epoch: [116][72/204]	Loss 0.3584 (0.4095)	
training:	Epoch: [116][73/204]	Loss 0.2846 (0.4078)	
training:	Epoch: [116][74/204]	Loss 0.4053 (0.4078)	
training:	Epoch: [116][75/204]	Loss 0.4273 (0.4080)	
training:	Epoch: [116][76/204]	Loss 0.5461 (0.4099)	
training:	Epoch: [116][77/204]	Loss 0.4114 (0.4099)	
training:	Epoch: [116][78/204]	Loss 0.4789 (0.4108)	
training:	Epoch: [116][79/204]	Loss 0.3584 (0.4101)	
training:	Epoch: [116][80/204]	Loss 0.3713 (0.4096)	
training:	Epoch: [116][81/204]	Loss 0.4307 (0.4099)	
training:	Epoch: [116][82/204]	Loss 0.4112 (0.4099)	
training:	Epoch: [116][83/204]	Loss 0.4320 (0.4102)	
training:	Epoch: [116][84/204]	Loss 0.2806 (0.4086)	
training:	Epoch: [116][85/204]	Loss 0.3752 (0.4082)	
training:	Epoch: [116][86/204]	Loss 0.3937 (0.4080)	
training:	Epoch: [116][87/204]	Loss 0.4556 (0.4086)	
training:	Epoch: [116][88/204]	Loss 0.3297 (0.4077)	
training:	Epoch: [116][89/204]	Loss 0.4904 (0.4086)	
training:	Epoch: [116][90/204]	Loss 0.4046 (0.4086)	
training:	Epoch: [116][91/204]	Loss 0.4243 (0.4088)	
training:	Epoch: [116][92/204]	Loss 0.3531 (0.4082)	
training:	Epoch: [116][93/204]	Loss 0.4104 (0.4082)	
training:	Epoch: [116][94/204]	Loss 0.3826 (0.4079)	
training:	Epoch: [116][95/204]	Loss 0.2669 (0.4064)	
training:	Epoch: [116][96/204]	Loss 0.4849 (0.4072)	
training:	Epoch: [116][97/204]	Loss 0.4407 (0.4076)	
training:	Epoch: [116][98/204]	Loss 0.2833 (0.4063)	
training:	Epoch: [116][99/204]	Loss 0.4798 (0.4071)	
training:	Epoch: [116][100/204]	Loss 0.3608 (0.4066)	
training:	Epoch: [116][101/204]	Loss 0.3770 (0.4063)	
training:	Epoch: [116][102/204]	Loss 0.2191 (0.4045)	
training:	Epoch: [116][103/204]	Loss 0.5209 (0.4056)	
training:	Epoch: [116][104/204]	Loss 0.3966 (0.4055)	
training:	Epoch: [116][105/204]	Loss 0.4685 (0.4061)	
training:	Epoch: [116][106/204]	Loss 0.4570 (0.4066)	
training:	Epoch: [116][107/204]	Loss 0.4525 (0.4070)	
training:	Epoch: [116][108/204]	Loss 0.4105 (0.4071)	
training:	Epoch: [116][109/204]	Loss 0.3887 (0.4069)	
training:	Epoch: [116][110/204]	Loss 0.3766 (0.4066)	
training:	Epoch: [116][111/204]	Loss 0.3560 (0.4062)	
training:	Epoch: [116][112/204]	Loss 0.2741 (0.4050)	
training:	Epoch: [116][113/204]	Loss 0.2921 (0.4040)	
training:	Epoch: [116][114/204]	Loss 0.4339 (0.4042)	
training:	Epoch: [116][115/204]	Loss 0.3997 (0.4042)	
training:	Epoch: [116][116/204]	Loss 0.3093 (0.4034)	
training:	Epoch: [116][117/204]	Loss 0.3352 (0.4028)	
training:	Epoch: [116][118/204]	Loss 0.3807 (0.4026)	
training:	Epoch: [116][119/204]	Loss 0.4821 (0.4033)	
training:	Epoch: [116][120/204]	Loss 0.2559 (0.4021)	
training:	Epoch: [116][121/204]	Loss 0.3772 (0.4018)	
training:	Epoch: [116][122/204]	Loss 0.3879 (0.4017)	
training:	Epoch: [116][123/204]	Loss 0.2988 (0.4009)	
training:	Epoch: [116][124/204]	Loss 0.3935 (0.4008)	
training:	Epoch: [116][125/204]	Loss 0.5285 (0.4019)	
training:	Epoch: [116][126/204]	Loss 0.3215 (0.4012)	
training:	Epoch: [116][127/204]	Loss 0.2624 (0.4001)	
training:	Epoch: [116][128/204]	Loss 0.3013 (0.3994)	
training:	Epoch: [116][129/204]	Loss 0.2896 (0.3985)	
training:	Epoch: [116][130/204]	Loss 0.5724 (0.3998)	
training:	Epoch: [116][131/204]	Loss 0.3939 (0.3998)	
training:	Epoch: [116][132/204]	Loss 0.2775 (0.3989)	
training:	Epoch: [116][133/204]	Loss 0.4663 (0.3994)	
training:	Epoch: [116][134/204]	Loss 0.3803 (0.3992)	
training:	Epoch: [116][135/204]	Loss 0.2087 (0.3978)	
training:	Epoch: [116][136/204]	Loss 0.3346 (0.3974)	
training:	Epoch: [116][137/204]	Loss 0.4480 (0.3977)	
training:	Epoch: [116][138/204]	Loss 0.5768 (0.3990)	
training:	Epoch: [116][139/204]	Loss 0.3722 (0.3988)	
training:	Epoch: [116][140/204]	Loss 0.3386 (0.3984)	
training:	Epoch: [116][141/204]	Loss 0.5191 (0.3993)	
training:	Epoch: [116][142/204]	Loss 0.3728 (0.3991)	
training:	Epoch: [116][143/204]	Loss 0.3249 (0.3986)	
training:	Epoch: [116][144/204]	Loss 0.3749 (0.3984)	
training:	Epoch: [116][145/204]	Loss 0.4090 (0.3985)	
training:	Epoch: [116][146/204]	Loss 0.4227 (0.3986)	
training:	Epoch: [116][147/204]	Loss 0.3741 (0.3985)	
training:	Epoch: [116][148/204]	Loss 0.4495 (0.3988)	
training:	Epoch: [116][149/204]	Loss 0.4332 (0.3990)	
training:	Epoch: [116][150/204]	Loss 0.4750 (0.3995)	
training:	Epoch: [116][151/204]	Loss 0.5322 (0.4004)	
training:	Epoch: [116][152/204]	Loss 0.3950 (0.4004)	
training:	Epoch: [116][153/204]	Loss 0.4334 (0.4006)	
training:	Epoch: [116][154/204]	Loss 0.4363 (0.4008)	
training:	Epoch: [116][155/204]	Loss 0.3762 (0.4007)	
training:	Epoch: [116][156/204]	Loss 0.4560 (0.4010)	
training:	Epoch: [116][157/204]	Loss 0.4585 (0.4014)	
training:	Epoch: [116][158/204]	Loss 0.2566 (0.4005)	
training:	Epoch: [116][159/204]	Loss 0.4020 (0.4005)	
training:	Epoch: [116][160/204]	Loss 0.4674 (0.4009)	
training:	Epoch: [116][161/204]	Loss 0.3016 (0.4003)	
training:	Epoch: [116][162/204]	Loss 0.4311 (0.4005)	
training:	Epoch: [116][163/204]	Loss 0.2076 (0.3993)	
training:	Epoch: [116][164/204]	Loss 0.4985 (0.3999)	
training:	Epoch: [116][165/204]	Loss 0.3910 (0.3998)	
training:	Epoch: [116][166/204]	Loss 0.2742 (0.3991)	
training:	Epoch: [116][167/204]	Loss 0.2852 (0.3984)	
training:	Epoch: [116][168/204]	Loss 0.4499 (0.3987)	
training:	Epoch: [116][169/204]	Loss 0.2603 (0.3979)	
training:	Epoch: [116][170/204]	Loss 0.5885 (0.3990)	
training:	Epoch: [116][171/204]	Loss 0.3122 (0.3985)	
training:	Epoch: [116][172/204]	Loss 0.3875 (0.3984)	
training:	Epoch: [116][173/204]	Loss 0.4419 (0.3987)	
training:	Epoch: [116][174/204]	Loss 0.4330 (0.3989)	
training:	Epoch: [116][175/204]	Loss 0.3742 (0.3988)	
training:	Epoch: [116][176/204]	Loss 0.6922 (0.4004)	
training:	Epoch: [116][177/204]	Loss 0.3126 (0.3999)	
training:	Epoch: [116][178/204]	Loss 0.4324 (0.4001)	
training:	Epoch: [116][179/204]	Loss 0.3978 (0.4001)	
training:	Epoch: [116][180/204]	Loss 0.3517 (0.3998)	
training:	Epoch: [116][181/204]	Loss 0.5491 (0.4006)	
training:	Epoch: [116][182/204]	Loss 0.3531 (0.4004)	
training:	Epoch: [116][183/204]	Loss 0.3424 (0.4001)	
training:	Epoch: [116][184/204]	Loss 0.3294 (0.3997)	
training:	Epoch: [116][185/204]	Loss 0.6113 (0.4008)	
training:	Epoch: [116][186/204]	Loss 0.2914 (0.4002)	
training:	Epoch: [116][187/204]	Loss 0.3328 (0.3999)	
training:	Epoch: [116][188/204]	Loss 0.4361 (0.4001)	
training:	Epoch: [116][189/204]	Loss 0.3794 (0.4000)	
training:	Epoch: [116][190/204]	Loss 0.2381 (0.3991)	
training:	Epoch: [116][191/204]	Loss 0.3680 (0.3989)	
training:	Epoch: [116][192/204]	Loss 0.4305 (0.3991)	
training:	Epoch: [116][193/204]	Loss 0.4149 (0.3992)	
training:	Epoch: [116][194/204]	Loss 0.3674 (0.3990)	
training:	Epoch: [116][195/204]	Loss 0.4847 (0.3995)	
training:	Epoch: [116][196/204]	Loss 0.5440 (0.4002)	
training:	Epoch: [116][197/204]	Loss 0.3001 (0.3997)	
training:	Epoch: [116][198/204]	Loss 0.2997 (0.3992)	
training:	Epoch: [116][199/204]	Loss 0.4793 (0.3996)	
training:	Epoch: [116][200/204]	Loss 0.3274 (0.3992)	
training:	Epoch: [116][201/204]	Loss 0.4064 (0.3993)	
training:	Epoch: [116][202/204]	Loss 0.3745 (0.3991)	
training:	Epoch: [116][203/204]	Loss 0.3543 (0.3989)	
training:	Epoch: [116][204/204]	Loss 0.4486 (0.3992)	
Training:	 Loss: 0.3986

Training:	 ACC: 0.8355 0.8356 0.8377 0.8332
Validation:	 ACC: 0.7935 0.7940 0.8055 0.7814
Validation:	 Best_BACC: 0.7988 0.7999 0.8229 0.7747
Validation:	 Loss: 0.4313
Pretraining:	Epoch 117/120
----------
training:	Epoch: [117][1/204]	Loss 0.5263 (0.5263)	
training:	Epoch: [117][2/204]	Loss 0.3611 (0.4437)	
training:	Epoch: [117][3/204]	Loss 0.5060 (0.4644)	
training:	Epoch: [117][4/204]	Loss 0.5403 (0.4834)	
training:	Epoch: [117][5/204]	Loss 0.4379 (0.4743)	
training:	Epoch: [117][6/204]	Loss 0.3685 (0.4567)	
training:	Epoch: [117][7/204]	Loss 0.3020 (0.4346)	
training:	Epoch: [117][8/204]	Loss 0.3815 (0.4279)	
training:	Epoch: [117][9/204]	Loss 0.7000 (0.4582)	
training:	Epoch: [117][10/204]	Loss 0.3943 (0.4518)	
training:	Epoch: [117][11/204]	Loss 0.3376 (0.4414)	
training:	Epoch: [117][12/204]	Loss 0.4065 (0.4385)	
training:	Epoch: [117][13/204]	Loss 0.5323 (0.4457)	
training:	Epoch: [117][14/204]	Loss 0.3816 (0.4411)	
training:	Epoch: [117][15/204]	Loss 0.5155 (0.4461)	
training:	Epoch: [117][16/204]	Loss 0.4478 (0.4462)	
training:	Epoch: [117][17/204]	Loss 0.4601 (0.4470)	
training:	Epoch: [117][18/204]	Loss 0.3406 (0.4411)	
training:	Epoch: [117][19/204]	Loss 0.2746 (0.4323)	
training:	Epoch: [117][20/204]	Loss 0.2988 (0.4257)	
training:	Epoch: [117][21/204]	Loss 0.4095 (0.4249)	
training:	Epoch: [117][22/204]	Loss 0.2759 (0.4181)	
training:	Epoch: [117][23/204]	Loss 0.4218 (0.4183)	
training:	Epoch: [117][24/204]	Loss 0.4447 (0.4194)	
training:	Epoch: [117][25/204]	Loss 0.3229 (0.4155)	
training:	Epoch: [117][26/204]	Loss 0.3972 (0.4148)	
training:	Epoch: [117][27/204]	Loss 0.4567 (0.4164)	
training:	Epoch: [117][28/204]	Loss 0.3132 (0.4127)	
training:	Epoch: [117][29/204]	Loss 0.4166 (0.4128)	
training:	Epoch: [117][30/204]	Loss 0.3220 (0.4098)	
training:	Epoch: [117][31/204]	Loss 0.4983 (0.4127)	
training:	Epoch: [117][32/204]	Loss 0.2812 (0.4085)	
training:	Epoch: [117][33/204]	Loss 0.3662 (0.4073)	
training:	Epoch: [117][34/204]	Loss 0.3200 (0.4047)	
training:	Epoch: [117][35/204]	Loss 0.4540 (0.4061)	
training:	Epoch: [117][36/204]	Loss 0.4299 (0.4068)	
training:	Epoch: [117][37/204]	Loss 0.3471 (0.4052)	
training:	Epoch: [117][38/204]	Loss 0.2636 (0.4014)	
training:	Epoch: [117][39/204]	Loss 0.4371 (0.4023)	
training:	Epoch: [117][40/204]	Loss 0.3083 (0.4000)	
training:	Epoch: [117][41/204]	Loss 0.2461 (0.3962)	
training:	Epoch: [117][42/204]	Loss 0.3624 (0.3954)	
training:	Epoch: [117][43/204]	Loss 0.5246 (0.3984)	
training:	Epoch: [117][44/204]	Loss 0.2773 (0.3957)	
training:	Epoch: [117][45/204]	Loss 0.3892 (0.3955)	
training:	Epoch: [117][46/204]	Loss 0.4206 (0.3961)	
training:	Epoch: [117][47/204]	Loss 0.3897 (0.3959)	
training:	Epoch: [117][48/204]	Loss 0.2627 (0.3932)	
training:	Epoch: [117][49/204]	Loss 0.3970 (0.3933)	
training:	Epoch: [117][50/204]	Loss 0.3230 (0.3918)	
training:	Epoch: [117][51/204]	Loss 0.3041 (0.3901)	
training:	Epoch: [117][52/204]	Loss 0.6079 (0.3943)	
training:	Epoch: [117][53/204]	Loss 0.3054 (0.3926)	
training:	Epoch: [117][54/204]	Loss 0.4819 (0.3943)	
training:	Epoch: [117][55/204]	Loss 0.4114 (0.3946)	
training:	Epoch: [117][56/204]	Loss 0.3156 (0.3932)	
training:	Epoch: [117][57/204]	Loss 0.4380 (0.3940)	
training:	Epoch: [117][58/204]	Loss 0.2525 (0.3915)	
training:	Epoch: [117][59/204]	Loss 0.3758 (0.3913)	
training:	Epoch: [117][60/204]	Loss 0.3137 (0.3900)	
training:	Epoch: [117][61/204]	Loss 0.3743 (0.3897)	
training:	Epoch: [117][62/204]	Loss 0.5047 (0.3916)	
training:	Epoch: [117][63/204]	Loss 0.4229 (0.3921)	
training:	Epoch: [117][64/204]	Loss 0.6057 (0.3954)	
training:	Epoch: [117][65/204]	Loss 0.4443 (0.3962)	
training:	Epoch: [117][66/204]	Loss 0.5172 (0.3980)	
training:	Epoch: [117][67/204]	Loss 0.2895 (0.3964)	
training:	Epoch: [117][68/204]	Loss 0.2871 (0.3948)	
training:	Epoch: [117][69/204]	Loss 0.3415 (0.3940)	
training:	Epoch: [117][70/204]	Loss 0.5735 (0.3966)	
training:	Epoch: [117][71/204]	Loss 0.5552 (0.3988)	
training:	Epoch: [117][72/204]	Loss 0.2623 (0.3969)	
training:	Epoch: [117][73/204]	Loss 0.2850 (0.3954)	
training:	Epoch: [117][74/204]	Loss 0.4951 (0.3967)	
training:	Epoch: [117][75/204]	Loss 0.2991 (0.3954)	
training:	Epoch: [117][76/204]	Loss 0.3571 (0.3949)	
training:	Epoch: [117][77/204]	Loss 0.3438 (0.3942)	
training:	Epoch: [117][78/204]	Loss 0.3291 (0.3934)	
training:	Epoch: [117][79/204]	Loss 0.4422 (0.3940)	
training:	Epoch: [117][80/204]	Loss 0.3340 (0.3933)	
training:	Epoch: [117][81/204]	Loss 0.4677 (0.3942)	
training:	Epoch: [117][82/204]	Loss 0.4803 (0.3952)	
training:	Epoch: [117][83/204]	Loss 0.4918 (0.3964)	
training:	Epoch: [117][84/204]	Loss 0.5541 (0.3983)	
training:	Epoch: [117][85/204]	Loss 0.5617 (0.4002)	
training:	Epoch: [117][86/204]	Loss 0.4585 (0.4009)	
training:	Epoch: [117][87/204]	Loss 0.3711 (0.4005)	
training:	Epoch: [117][88/204]	Loss 0.3301 (0.3997)	
training:	Epoch: [117][89/204]	Loss 0.3979 (0.3997)	
training:	Epoch: [117][90/204]	Loss 0.3283 (0.3989)	
training:	Epoch: [117][91/204]	Loss 0.3258 (0.3981)	
training:	Epoch: [117][92/204]	Loss 0.3094 (0.3972)	
training:	Epoch: [117][93/204]	Loss 0.2774 (0.3959)	
training:	Epoch: [117][94/204]	Loss 0.3740 (0.3956)	
training:	Epoch: [117][95/204]	Loss 0.5431 (0.3972)	
training:	Epoch: [117][96/204]	Loss 0.2977 (0.3962)	
training:	Epoch: [117][97/204]	Loss 0.6060 (0.3983)	
training:	Epoch: [117][98/204]	Loss 0.3231 (0.3976)	
training:	Epoch: [117][99/204]	Loss 0.4165 (0.3977)	
training:	Epoch: [117][100/204]	Loss 0.3116 (0.3969)	
training:	Epoch: [117][101/204]	Loss 0.4700 (0.3976)	
training:	Epoch: [117][102/204]	Loss 0.6273 (0.3999)	
training:	Epoch: [117][103/204]	Loss 0.7274 (0.4030)	
training:	Epoch: [117][104/204]	Loss 0.4852 (0.4038)	
training:	Epoch: [117][105/204]	Loss 0.4464 (0.4042)	
training:	Epoch: [117][106/204]	Loss 0.1914 (0.4022)	
training:	Epoch: [117][107/204]	Loss 0.3769 (0.4020)	
training:	Epoch: [117][108/204]	Loss 0.3121 (0.4012)	
training:	Epoch: [117][109/204]	Loss 0.4543 (0.4016)	
training:	Epoch: [117][110/204]	Loss 0.2620 (0.4004)	
training:	Epoch: [117][111/204]	Loss 0.4892 (0.4012)	
training:	Epoch: [117][112/204]	Loss 0.2918 (0.4002)	
training:	Epoch: [117][113/204]	Loss 0.3817 (0.4000)	
training:	Epoch: [117][114/204]	Loss 0.4283 (0.4003)	
training:	Epoch: [117][115/204]	Loss 0.3447 (0.3998)	
training:	Epoch: [117][116/204]	Loss 0.2893 (0.3988)	
training:	Epoch: [117][117/204]	Loss 0.2986 (0.3980)	
training:	Epoch: [117][118/204]	Loss 0.3950 (0.3980)	
training:	Epoch: [117][119/204]	Loss 0.4761 (0.3986)	
training:	Epoch: [117][120/204]	Loss 0.4479 (0.3990)	
training:	Epoch: [117][121/204]	Loss 0.2918 (0.3981)	
training:	Epoch: [117][122/204]	Loss 0.5008 (0.3990)	
training:	Epoch: [117][123/204]	Loss 0.3272 (0.3984)	
training:	Epoch: [117][124/204]	Loss 0.4025 (0.3984)	
training:	Epoch: [117][125/204]	Loss 0.5473 (0.3996)	
training:	Epoch: [117][126/204]	Loss 0.3318 (0.3991)	
training:	Epoch: [117][127/204]	Loss 0.3932 (0.3990)	
training:	Epoch: [117][128/204]	Loss 0.5640 (0.4003)	
training:	Epoch: [117][129/204]	Loss 0.4831 (0.4010)	
training:	Epoch: [117][130/204]	Loss 0.2887 (0.4001)	
training:	Epoch: [117][131/204]	Loss 0.3333 (0.3996)	
training:	Epoch: [117][132/204]	Loss 0.3768 (0.3994)	
training:	Epoch: [117][133/204]	Loss 0.3338 (0.3989)	
training:	Epoch: [117][134/204]	Loss 0.3410 (0.3985)	
training:	Epoch: [117][135/204]	Loss 0.4145 (0.3986)	
training:	Epoch: [117][136/204]	Loss 0.4289 (0.3988)	
training:	Epoch: [117][137/204]	Loss 0.3523 (0.3985)	
training:	Epoch: [117][138/204]	Loss 0.3460 (0.3981)	
training:	Epoch: [117][139/204]	Loss 0.2954 (0.3974)	
training:	Epoch: [117][140/204]	Loss 0.3632 (0.3971)	
training:	Epoch: [117][141/204]	Loss 0.5064 (0.3979)	
training:	Epoch: [117][142/204]	Loss 0.4213 (0.3981)	
training:	Epoch: [117][143/204]	Loss 0.5077 (0.3988)	
training:	Epoch: [117][144/204]	Loss 0.2375 (0.3977)	
training:	Epoch: [117][145/204]	Loss 0.4395 (0.3980)	
training:	Epoch: [117][146/204]	Loss 0.2647 (0.3971)	
training:	Epoch: [117][147/204]	Loss 0.6175 (0.3986)	
training:	Epoch: [117][148/204]	Loss 0.3103 (0.3980)	
training:	Epoch: [117][149/204]	Loss 0.3786 (0.3979)	
training:	Epoch: [117][150/204]	Loss 0.3236 (0.3974)	
training:	Epoch: [117][151/204]	Loss 0.2890 (0.3967)	
training:	Epoch: [117][152/204]	Loss 0.5002 (0.3973)	
training:	Epoch: [117][153/204]	Loss 0.4284 (0.3975)	
training:	Epoch: [117][154/204]	Loss 0.5609 (0.3986)	
training:	Epoch: [117][155/204]	Loss 0.4892 (0.3992)	
training:	Epoch: [117][156/204]	Loss 0.6618 (0.4009)	
training:	Epoch: [117][157/204]	Loss 0.3511 (0.4006)	
training:	Epoch: [117][158/204]	Loss 0.3574 (0.4003)	
training:	Epoch: [117][159/204]	Loss 0.2672 (0.3994)	
training:	Epoch: [117][160/204]	Loss 0.5258 (0.4002)	
training:	Epoch: [117][161/204]	Loss 0.3416 (0.3999)	
training:	Epoch: [117][162/204]	Loss 0.3521 (0.3996)	
training:	Epoch: [117][163/204]	Loss 0.3588 (0.3993)	
training:	Epoch: [117][164/204]	Loss 0.3278 (0.3989)	
training:	Epoch: [117][165/204]	Loss 0.3520 (0.3986)	
training:	Epoch: [117][166/204]	Loss 0.5293 (0.3994)	
training:	Epoch: [117][167/204]	Loss 0.4286 (0.3996)	
training:	Epoch: [117][168/204]	Loss 0.3997 (0.3996)	
training:	Epoch: [117][169/204]	Loss 0.3472 (0.3993)	
training:	Epoch: [117][170/204]	Loss 0.2556 (0.3984)	
training:	Epoch: [117][171/204]	Loss 0.4892 (0.3989)	
training:	Epoch: [117][172/204]	Loss 0.5024 (0.3995)	
training:	Epoch: [117][173/204]	Loss 0.3692 (0.3994)	
training:	Epoch: [117][174/204]	Loss 0.4684 (0.3998)	
training:	Epoch: [117][175/204]	Loss 0.5582 (0.4007)	
training:	Epoch: [117][176/204]	Loss 0.4446 (0.4009)	
training:	Epoch: [117][177/204]	Loss 0.3864 (0.4008)	
training:	Epoch: [117][178/204]	Loss 0.4300 (0.4010)	
training:	Epoch: [117][179/204]	Loss 0.3927 (0.4010)	
training:	Epoch: [117][180/204]	Loss 0.5129 (0.4016)	
training:	Epoch: [117][181/204]	Loss 0.2922 (0.4010)	
training:	Epoch: [117][182/204]	Loss 0.4098 (0.4010)	
training:	Epoch: [117][183/204]	Loss 0.3746 (0.4009)	
training:	Epoch: [117][184/204]	Loss 0.4927 (0.4014)	
training:	Epoch: [117][185/204]	Loss 0.3342 (0.4010)	
training:	Epoch: [117][186/204]	Loss 0.3332 (0.4006)	
training:	Epoch: [117][187/204]	Loss 0.3991 (0.4006)	
training:	Epoch: [117][188/204]	Loss 0.4871 (0.4011)	
training:	Epoch: [117][189/204]	Loss 0.5225 (0.4017)	
training:	Epoch: [117][190/204]	Loss 0.3802 (0.4016)	
training:	Epoch: [117][191/204]	Loss 0.2914 (0.4011)	
training:	Epoch: [117][192/204]	Loss 0.3266 (0.4007)	
training:	Epoch: [117][193/204]	Loss 0.5524 (0.4014)	
training:	Epoch: [117][194/204]	Loss 0.4575 (0.4017)	
training:	Epoch: [117][195/204]	Loss 0.3331 (0.4014)	
training:	Epoch: [117][196/204]	Loss 0.3562 (0.4012)	
training:	Epoch: [117][197/204]	Loss 0.4872 (0.4016)	
training:	Epoch: [117][198/204]	Loss 0.3994 (0.4016)	
training:	Epoch: [117][199/204]	Loss 0.3442 (0.4013)	
training:	Epoch: [117][200/204]	Loss 0.6101 (0.4023)	
training:	Epoch: [117][201/204]	Loss 0.4414 (0.4025)	
training:	Epoch: [117][202/204]	Loss 0.3331 (0.4022)	
training:	Epoch: [117][203/204]	Loss 0.5178 (0.4028)	
training:	Epoch: [117][204/204]	Loss 0.2711 (0.4021)	
Training:	 Loss: 0.4015

Training:	 ACC: 0.8370 0.8373 0.8433 0.8307
Validation:	 ACC: 0.7959 0.7967 0.8127 0.7791
Validation:	 Best_BACC: 0.7988 0.7999 0.8229 0.7747
Validation:	 Loss: 0.4309
Pretraining:	Epoch 118/120
----------
training:	Epoch: [118][1/204]	Loss 0.4100 (0.4100)	
training:	Epoch: [118][2/204]	Loss 0.4439 (0.4269)	
training:	Epoch: [118][3/204]	Loss 0.2940 (0.3826)	
training:	Epoch: [118][4/204]	Loss 0.4137 (0.3904)	
training:	Epoch: [118][5/204]	Loss 0.2543 (0.3632)	
training:	Epoch: [118][6/204]	Loss 0.4809 (0.3828)	
training:	Epoch: [118][7/204]	Loss 0.2466 (0.3633)	
training:	Epoch: [118][8/204]	Loss 0.3981 (0.3677)	
training:	Epoch: [118][9/204]	Loss 0.4060 (0.3719)	
training:	Epoch: [118][10/204]	Loss 0.5603 (0.3908)	
training:	Epoch: [118][11/204]	Loss 0.2757 (0.3803)	
training:	Epoch: [118][12/204]	Loss 0.3535 (0.3781)	
training:	Epoch: [118][13/204]	Loss 0.2915 (0.3714)	
training:	Epoch: [118][14/204]	Loss 0.2711 (0.3642)	
training:	Epoch: [118][15/204]	Loss 0.5393 (0.3759)	
training:	Epoch: [118][16/204]	Loss 0.3615 (0.3750)	
training:	Epoch: [118][17/204]	Loss 0.4715 (0.3807)	
training:	Epoch: [118][18/204]	Loss 0.3095 (0.3767)	
training:	Epoch: [118][19/204]	Loss 0.3643 (0.3761)	
training:	Epoch: [118][20/204]	Loss 0.3259 (0.3736)	
training:	Epoch: [118][21/204]	Loss 0.3771 (0.3737)	
training:	Epoch: [118][22/204]	Loss 0.2460 (0.3679)	
training:	Epoch: [118][23/204]	Loss 0.3425 (0.3668)	
training:	Epoch: [118][24/204]	Loss 0.3756 (0.3672)	
training:	Epoch: [118][25/204]	Loss 0.4625 (0.3710)	
training:	Epoch: [118][26/204]	Loss 0.4391 (0.3736)	
training:	Epoch: [118][27/204]	Loss 0.2833 (0.3703)	
training:	Epoch: [118][28/204]	Loss 0.2123 (0.3646)	
training:	Epoch: [118][29/204]	Loss 0.2822 (0.3618)	
training:	Epoch: [118][30/204]	Loss 0.6683 (0.3720)	
training:	Epoch: [118][31/204]	Loss 0.4111 (0.3733)	
training:	Epoch: [118][32/204]	Loss 0.3945 (0.3739)	
training:	Epoch: [118][33/204]	Loss 0.4419 (0.3760)	
training:	Epoch: [118][34/204]	Loss 0.4852 (0.3792)	
training:	Epoch: [118][35/204]	Loss 0.3186 (0.3775)	
training:	Epoch: [118][36/204]	Loss 0.2097 (0.3728)	
training:	Epoch: [118][37/204]	Loss 0.3360 (0.3718)	
training:	Epoch: [118][38/204]	Loss 0.4815 (0.3747)	
training:	Epoch: [118][39/204]	Loss 0.4440 (0.3765)	
training:	Epoch: [118][40/204]	Loss 0.4750 (0.3790)	
training:	Epoch: [118][41/204]	Loss 0.3529 (0.3783)	
training:	Epoch: [118][42/204]	Loss 0.4662 (0.3804)	
training:	Epoch: [118][43/204]	Loss 0.3883 (0.3806)	
training:	Epoch: [118][44/204]	Loss 0.6098 (0.3858)	
training:	Epoch: [118][45/204]	Loss 0.2933 (0.3837)	
training:	Epoch: [118][46/204]	Loss 0.5901 (0.3882)	
training:	Epoch: [118][47/204]	Loss 0.4847 (0.3903)	
training:	Epoch: [118][48/204]	Loss 0.4717 (0.3920)	
training:	Epoch: [118][49/204]	Loss 0.2928 (0.3900)	
training:	Epoch: [118][50/204]	Loss 0.2348 (0.3869)	
training:	Epoch: [118][51/204]	Loss 0.4811 (0.3887)	
training:	Epoch: [118][52/204]	Loss 0.4742 (0.3903)	
training:	Epoch: [118][53/204]	Loss 0.4555 (0.3916)	
training:	Epoch: [118][54/204]	Loss 0.4747 (0.3931)	
training:	Epoch: [118][55/204]	Loss 0.4246 (0.3937)	
training:	Epoch: [118][56/204]	Loss 0.3258 (0.3925)	
training:	Epoch: [118][57/204]	Loss 0.2555 (0.3901)	
training:	Epoch: [118][58/204]	Loss 0.3713 (0.3897)	
training:	Epoch: [118][59/204]	Loss 0.2384 (0.3872)	
training:	Epoch: [118][60/204]	Loss 0.4093 (0.3876)	
training:	Epoch: [118][61/204]	Loss 0.4591 (0.3887)	
training:	Epoch: [118][62/204]	Loss 0.2418 (0.3864)	
training:	Epoch: [118][63/204]	Loss 0.3944 (0.3865)	
training:	Epoch: [118][64/204]	Loss 0.4075 (0.3868)	
training:	Epoch: [118][65/204]	Loss 0.5294 (0.3890)	
training:	Epoch: [118][66/204]	Loss 0.3757 (0.3888)	
training:	Epoch: [118][67/204]	Loss 0.6061 (0.3920)	
training:	Epoch: [118][68/204]	Loss 0.4844 (0.3934)	
training:	Epoch: [118][69/204]	Loss 0.4078 (0.3936)	
training:	Epoch: [118][70/204]	Loss 0.4600 (0.3946)	
training:	Epoch: [118][71/204]	Loss 0.3217 (0.3935)	
training:	Epoch: [118][72/204]	Loss 0.5221 (0.3953)	
training:	Epoch: [118][73/204]	Loss 0.2902 (0.3939)	
training:	Epoch: [118][74/204]	Loss 0.4955 (0.3953)	
training:	Epoch: [118][75/204]	Loss 0.4027 (0.3954)	
training:	Epoch: [118][76/204]	Loss 0.3131 (0.3943)	
training:	Epoch: [118][77/204]	Loss 0.4117 (0.3945)	
training:	Epoch: [118][78/204]	Loss 0.3273 (0.3936)	
training:	Epoch: [118][79/204]	Loss 0.4616 (0.3945)	
training:	Epoch: [118][80/204]	Loss 0.4228 (0.3948)	
training:	Epoch: [118][81/204]	Loss 0.4398 (0.3954)	
training:	Epoch: [118][82/204]	Loss 0.3687 (0.3951)	
training:	Epoch: [118][83/204]	Loss 0.3031 (0.3940)	
training:	Epoch: [118][84/204]	Loss 0.2873 (0.3927)	
training:	Epoch: [118][85/204]	Loss 0.3076 (0.3917)	
training:	Epoch: [118][86/204]	Loss 0.4272 (0.3921)	
training:	Epoch: [118][87/204]	Loss 0.3817 (0.3920)	
training:	Epoch: [118][88/204]	Loss 0.4732 (0.3929)	
training:	Epoch: [118][89/204]	Loss 0.3337 (0.3923)	
training:	Epoch: [118][90/204]	Loss 0.2989 (0.3912)	
training:	Epoch: [118][91/204]	Loss 0.3024 (0.3902)	
training:	Epoch: [118][92/204]	Loss 0.3559 (0.3899)	
training:	Epoch: [118][93/204]	Loss 0.3395 (0.3893)	
training:	Epoch: [118][94/204]	Loss 0.4511 (0.3900)	
training:	Epoch: [118][95/204]	Loss 0.2372 (0.3884)	
training:	Epoch: [118][96/204]	Loss 0.3394 (0.3879)	
training:	Epoch: [118][97/204]	Loss 0.2439 (0.3864)	
training:	Epoch: [118][98/204]	Loss 0.3597 (0.3861)	
training:	Epoch: [118][99/204]	Loss 0.4778 (0.3870)	
training:	Epoch: [118][100/204]	Loss 0.3634 (0.3868)	
training:	Epoch: [118][101/204]	Loss 0.3577 (0.3865)	
training:	Epoch: [118][102/204]	Loss 0.4538 (0.3872)	
training:	Epoch: [118][103/204]	Loss 0.5961 (0.3892)	
training:	Epoch: [118][104/204]	Loss 0.3296 (0.3886)	
training:	Epoch: [118][105/204]	Loss 0.3701 (0.3884)	
training:	Epoch: [118][106/204]	Loss 0.4301 (0.3888)	
training:	Epoch: [118][107/204]	Loss 0.3543 (0.3885)	
training:	Epoch: [118][108/204]	Loss 0.5562 (0.3901)	
training:	Epoch: [118][109/204]	Loss 0.3327 (0.3895)	
training:	Epoch: [118][110/204]	Loss 0.3366 (0.3891)	
training:	Epoch: [118][111/204]	Loss 0.4162 (0.3893)	
training:	Epoch: [118][112/204]	Loss 0.3300 (0.3888)	
training:	Epoch: [118][113/204]	Loss 0.4177 (0.3890)	
training:	Epoch: [118][114/204]	Loss 0.4937 (0.3899)	
training:	Epoch: [118][115/204]	Loss 0.6070 (0.3918)	
training:	Epoch: [118][116/204]	Loss 0.2484 (0.3906)	
training:	Epoch: [118][117/204]	Loss 0.3013 (0.3898)	
training:	Epoch: [118][118/204]	Loss 0.2849 (0.3889)	
training:	Epoch: [118][119/204]	Loss 0.4818 (0.3897)	
training:	Epoch: [118][120/204]	Loss 0.2003 (0.3881)	
training:	Epoch: [118][121/204]	Loss 0.3330 (0.3877)	
training:	Epoch: [118][122/204]	Loss 0.5983 (0.3894)	
training:	Epoch: [118][123/204]	Loss 0.5132 (0.3904)	
training:	Epoch: [118][124/204]	Loss 0.2763 (0.3895)	
training:	Epoch: [118][125/204]	Loss 0.2960 (0.3888)	
training:	Epoch: [118][126/204]	Loss 0.4560 (0.3893)	
training:	Epoch: [118][127/204]	Loss 0.3849 (0.3893)	
training:	Epoch: [118][128/204]	Loss 0.3400 (0.3889)	
training:	Epoch: [118][129/204]	Loss 0.4494 (0.3893)	
training:	Epoch: [118][130/204]	Loss 0.4504 (0.3898)	
training:	Epoch: [118][131/204]	Loss 0.4334 (0.3901)	
training:	Epoch: [118][132/204]	Loss 0.4108 (0.3903)	
training:	Epoch: [118][133/204]	Loss 0.3990 (0.3904)	
training:	Epoch: [118][134/204]	Loss 0.3990 (0.3904)	
training:	Epoch: [118][135/204]	Loss 0.4503 (0.3909)	
training:	Epoch: [118][136/204]	Loss 0.3084 (0.3903)	
training:	Epoch: [118][137/204]	Loss 0.4576 (0.3908)	
training:	Epoch: [118][138/204]	Loss 0.3283 (0.3903)	
training:	Epoch: [118][139/204]	Loss 0.4557 (0.3908)	
training:	Epoch: [118][140/204]	Loss 0.3190 (0.3903)	
training:	Epoch: [118][141/204]	Loss 0.6030 (0.3918)	
training:	Epoch: [118][142/204]	Loss 0.3731 (0.3916)	
training:	Epoch: [118][143/204]	Loss 0.3069 (0.3910)	
training:	Epoch: [118][144/204]	Loss 0.3176 (0.3905)	
training:	Epoch: [118][145/204]	Loss 0.2791 (0.3898)	
training:	Epoch: [118][146/204]	Loss 0.5413 (0.3908)	
training:	Epoch: [118][147/204]	Loss 0.3050 (0.3902)	
training:	Epoch: [118][148/204]	Loss 0.3396 (0.3899)	
training:	Epoch: [118][149/204]	Loss 0.4598 (0.3904)	
training:	Epoch: [118][150/204]	Loss 0.5175 (0.3912)	
training:	Epoch: [118][151/204]	Loss 0.3951 (0.3912)	
training:	Epoch: [118][152/204]	Loss 0.4340 (0.3915)	
training:	Epoch: [118][153/204]	Loss 0.5740 (0.3927)	
training:	Epoch: [118][154/204]	Loss 0.4367 (0.3930)	
training:	Epoch: [118][155/204]	Loss 0.2805 (0.3923)	
training:	Epoch: [118][156/204]	Loss 0.4726 (0.3928)	
training:	Epoch: [118][157/204]	Loss 0.2552 (0.3919)	
training:	Epoch: [118][158/204]	Loss 0.5144 (0.3927)	
training:	Epoch: [118][159/204]	Loss 0.5558 (0.3937)	
training:	Epoch: [118][160/204]	Loss 0.3975 (0.3937)	
training:	Epoch: [118][161/204]	Loss 0.3022 (0.3932)	
training:	Epoch: [118][162/204]	Loss 0.4839 (0.3937)	
training:	Epoch: [118][163/204]	Loss 0.3511 (0.3935)	
training:	Epoch: [118][164/204]	Loss 0.2773 (0.3927)	
training:	Epoch: [118][165/204]	Loss 0.5139 (0.3935)	
training:	Epoch: [118][166/204]	Loss 0.6201 (0.3948)	
training:	Epoch: [118][167/204]	Loss 0.2910 (0.3942)	
training:	Epoch: [118][168/204]	Loss 0.3661 (0.3941)	
training:	Epoch: [118][169/204]	Loss 0.3841 (0.3940)	
training:	Epoch: [118][170/204]	Loss 0.4814 (0.3945)	
training:	Epoch: [118][171/204]	Loss 0.3725 (0.3944)	
training:	Epoch: [118][172/204]	Loss 0.4114 (0.3945)	
training:	Epoch: [118][173/204]	Loss 0.2726 (0.3938)	
training:	Epoch: [118][174/204]	Loss 0.6000 (0.3950)	
training:	Epoch: [118][175/204]	Loss 0.3246 (0.3946)	
training:	Epoch: [118][176/204]	Loss 0.3561 (0.3943)	
training:	Epoch: [118][177/204]	Loss 0.4204 (0.3945)	
training:	Epoch: [118][178/204]	Loss 0.2048 (0.3934)	
training:	Epoch: [118][179/204]	Loss 0.2724 (0.3927)	
training:	Epoch: [118][180/204]	Loss 0.5310 (0.3935)	
training:	Epoch: [118][181/204]	Loss 0.5908 (0.3946)	
training:	Epoch: [118][182/204]	Loss 0.3868 (0.3946)	
training:	Epoch: [118][183/204]	Loss 0.3395 (0.3943)	
training:	Epoch: [118][184/204]	Loss 0.4391 (0.3945)	
training:	Epoch: [118][185/204]	Loss 0.4552 (0.3948)	
training:	Epoch: [118][186/204]	Loss 0.3931 (0.3948)	
training:	Epoch: [118][187/204]	Loss 0.4569 (0.3952)	
training:	Epoch: [118][188/204]	Loss 0.4032 (0.3952)	
training:	Epoch: [118][189/204]	Loss 0.2911 (0.3946)	
training:	Epoch: [118][190/204]	Loss 0.3974 (0.3947)	
training:	Epoch: [118][191/204]	Loss 0.2426 (0.3939)	
training:	Epoch: [118][192/204]	Loss 0.4195 (0.3940)	
training:	Epoch: [118][193/204]	Loss 0.4620 (0.3944)	
training:	Epoch: [118][194/204]	Loss 0.3754 (0.3943)	
training:	Epoch: [118][195/204]	Loss 0.3793 (0.3942)	
training:	Epoch: [118][196/204]	Loss 0.3983 (0.3942)	
training:	Epoch: [118][197/204]	Loss 0.4552 (0.3945)	
training:	Epoch: [118][198/204]	Loss 0.2724 (0.3939)	
training:	Epoch: [118][199/204]	Loss 0.4765 (0.3943)	
training:	Epoch: [118][200/204]	Loss 0.4698 (0.3947)	
training:	Epoch: [118][201/204]	Loss 0.2782 (0.3941)	
training:	Epoch: [118][202/204]	Loss 0.5814 (0.3950)	
training:	Epoch: [118][203/204]	Loss 0.4772 (0.3954)	
training:	Epoch: [118][204/204]	Loss 0.4529 (0.3957)	
Training:	 Loss: 0.3951

Training:	 ACC: 0.8368 0.8370 0.8395 0.8342
Validation:	 ACC: 0.7944 0.7951 0.8086 0.7803
Validation:	 Best_BACC: 0.7988 0.7999 0.8229 0.7747
Validation:	 Loss: 0.4304
Pretraining:	Epoch 119/120
----------
training:	Epoch: [119][1/204]	Loss 0.3995 (0.3995)	
training:	Epoch: [119][2/204]	Loss 0.2880 (0.3437)	
training:	Epoch: [119][3/204]	Loss 0.3196 (0.3357)	
training:	Epoch: [119][4/204]	Loss 0.5890 (0.3990)	
training:	Epoch: [119][5/204]	Loss 0.5010 (0.4194)	
training:	Epoch: [119][6/204]	Loss 0.2805 (0.3963)	
training:	Epoch: [119][7/204]	Loss 0.4212 (0.3998)	
training:	Epoch: [119][8/204]	Loss 0.2485 (0.3809)	
training:	Epoch: [119][9/204]	Loss 0.4532 (0.3890)	
training:	Epoch: [119][10/204]	Loss 0.3781 (0.3879)	
training:	Epoch: [119][11/204]	Loss 0.3835 (0.3875)	
training:	Epoch: [119][12/204]	Loss 0.5831 (0.4038)	
training:	Epoch: [119][13/204]	Loss 0.3733 (0.4014)	
training:	Epoch: [119][14/204]	Loss 0.2799 (0.3928)	
training:	Epoch: [119][15/204]	Loss 0.3645 (0.3909)	
training:	Epoch: [119][16/204]	Loss 0.4832 (0.3966)	
training:	Epoch: [119][17/204]	Loss 0.4692 (0.4009)	
training:	Epoch: [119][18/204]	Loss 0.3917 (0.4004)	
training:	Epoch: [119][19/204]	Loss 0.3779 (0.3992)	
training:	Epoch: [119][20/204]	Loss 0.3303 (0.3958)	
training:	Epoch: [119][21/204]	Loss 0.5983 (0.4054)	
training:	Epoch: [119][22/204]	Loss 0.3987 (0.4051)	
training:	Epoch: [119][23/204]	Loss 0.3962 (0.4047)	
training:	Epoch: [119][24/204]	Loss 0.3783 (0.4036)	
training:	Epoch: [119][25/204]	Loss 0.2028 (0.3956)	
training:	Epoch: [119][26/204]	Loss 0.4249 (0.3967)	
training:	Epoch: [119][27/204]	Loss 0.3590 (0.3953)	
training:	Epoch: [119][28/204]	Loss 0.2618 (0.3905)	
training:	Epoch: [119][29/204]	Loss 0.2477 (0.3856)	
training:	Epoch: [119][30/204]	Loss 0.4861 (0.3890)	
training:	Epoch: [119][31/204]	Loss 0.3118 (0.3865)	
training:	Epoch: [119][32/204]	Loss 0.4709 (0.3891)	
training:	Epoch: [119][33/204]	Loss 0.3766 (0.3887)	
training:	Epoch: [119][34/204]	Loss 0.3712 (0.3882)	
training:	Epoch: [119][35/204]	Loss 0.3247 (0.3864)	
training:	Epoch: [119][36/204]	Loss 0.3221 (0.3846)	
training:	Epoch: [119][37/204]	Loss 0.5928 (0.3902)	
training:	Epoch: [119][38/204]	Loss 0.4211 (0.3911)	
training:	Epoch: [119][39/204]	Loss 0.3566 (0.3902)	
training:	Epoch: [119][40/204]	Loss 0.4592 (0.3919)	
training:	Epoch: [119][41/204]	Loss 0.3814 (0.3916)	
training:	Epoch: [119][42/204]	Loss 0.4385 (0.3928)	
training:	Epoch: [119][43/204]	Loss 0.3884 (0.3927)	
training:	Epoch: [119][44/204]	Loss 0.6746 (0.3991)	
training:	Epoch: [119][45/204]	Loss 0.4444 (0.4001)	
training:	Epoch: [119][46/204]	Loss 0.3422 (0.3988)	
training:	Epoch: [119][47/204]	Loss 0.2311 (0.3952)	
training:	Epoch: [119][48/204]	Loss 0.4094 (0.3955)	
training:	Epoch: [119][49/204]	Loss 0.3740 (0.3951)	
training:	Epoch: [119][50/204]	Loss 0.4682 (0.3966)	
training:	Epoch: [119][51/204]	Loss 0.3602 (0.3959)	
training:	Epoch: [119][52/204]	Loss 0.3553 (0.3951)	
training:	Epoch: [119][53/204]	Loss 0.4505 (0.3961)	
training:	Epoch: [119][54/204]	Loss 0.4465 (0.3971)	
training:	Epoch: [119][55/204]	Loss 0.3402 (0.3960)	
training:	Epoch: [119][56/204]	Loss 0.3385 (0.3950)	
training:	Epoch: [119][57/204]	Loss 0.3389 (0.3940)	
training:	Epoch: [119][58/204]	Loss 0.3487 (0.3932)	
training:	Epoch: [119][59/204]	Loss 0.4055 (0.3934)	
training:	Epoch: [119][60/204]	Loss 0.4738 (0.3948)	
training:	Epoch: [119][61/204]	Loss 0.5612 (0.3975)	
training:	Epoch: [119][62/204]	Loss 0.4151 (0.3978)	
training:	Epoch: [119][63/204]	Loss 0.4922 (0.3993)	
training:	Epoch: [119][64/204]	Loss 0.3840 (0.3990)	
training:	Epoch: [119][65/204]	Loss 0.4836 (0.4003)	
training:	Epoch: [119][66/204]	Loss 0.2699 (0.3984)	
training:	Epoch: [119][67/204]	Loss 0.2891 (0.3967)	
training:	Epoch: [119][68/204]	Loss 0.2973 (0.3953)	
training:	Epoch: [119][69/204]	Loss 0.4241 (0.3957)	
training:	Epoch: [119][70/204]	Loss 0.2848 (0.3941)	
training:	Epoch: [119][71/204]	Loss 0.5236 (0.3959)	
training:	Epoch: [119][72/204]	Loss 0.3144 (0.3948)	
training:	Epoch: [119][73/204]	Loss 0.2607 (0.3930)	
training:	Epoch: [119][74/204]	Loss 0.3864 (0.3929)	
training:	Epoch: [119][75/204]	Loss 0.2904 (0.3915)	
training:	Epoch: [119][76/204]	Loss 0.3559 (0.3910)	
training:	Epoch: [119][77/204]	Loss 0.3980 (0.3911)	
training:	Epoch: [119][78/204]	Loss 0.4382 (0.3917)	
training:	Epoch: [119][79/204]	Loss 0.4259 (0.3922)	
training:	Epoch: [119][80/204]	Loss 0.3917 (0.3922)	
training:	Epoch: [119][81/204]	Loss 0.5084 (0.3936)	
training:	Epoch: [119][82/204]	Loss 0.3294 (0.3928)	
training:	Epoch: [119][83/204]	Loss 0.3752 (0.3926)	
training:	Epoch: [119][84/204]	Loss 0.4124 (0.3928)	
training:	Epoch: [119][85/204]	Loss 0.2390 (0.3910)	
training:	Epoch: [119][86/204]	Loss 0.3532 (0.3906)	
training:	Epoch: [119][87/204]	Loss 0.4500 (0.3913)	
training:	Epoch: [119][88/204]	Loss 0.5965 (0.3936)	
training:	Epoch: [119][89/204]	Loss 0.5883 (0.3958)	
training:	Epoch: [119][90/204]	Loss 0.5043 (0.3970)	
training:	Epoch: [119][91/204]	Loss 0.2434 (0.3953)	
training:	Epoch: [119][92/204]	Loss 0.4426 (0.3958)	
training:	Epoch: [119][93/204]	Loss 0.3498 (0.3953)	
training:	Epoch: [119][94/204]	Loss 0.3356 (0.3947)	
training:	Epoch: [119][95/204]	Loss 0.4290 (0.3950)	
training:	Epoch: [119][96/204]	Loss 0.3877 (0.3950)	
training:	Epoch: [119][97/204]	Loss 0.4136 (0.3952)	
training:	Epoch: [119][98/204]	Loss 0.7432 (0.3987)	
training:	Epoch: [119][99/204]	Loss 0.3813 (0.3985)	
training:	Epoch: [119][100/204]	Loss 0.3597 (0.3982)	
training:	Epoch: [119][101/204]	Loss 0.3642 (0.3978)	
training:	Epoch: [119][102/204]	Loss 0.4642 (0.3985)	
training:	Epoch: [119][103/204]	Loss 0.4312 (0.3988)	
training:	Epoch: [119][104/204]	Loss 0.4055 (0.3988)	
training:	Epoch: [119][105/204]	Loss 0.3770 (0.3986)	
training:	Epoch: [119][106/204]	Loss 0.5658 (0.4002)	
training:	Epoch: [119][107/204]	Loss 0.3047 (0.3993)	
training:	Epoch: [119][108/204]	Loss 0.2349 (0.3978)	
training:	Epoch: [119][109/204]	Loss 0.6419 (0.4000)	
training:	Epoch: [119][110/204]	Loss 0.3611 (0.3997)	
training:	Epoch: [119][111/204]	Loss 0.4860 (0.4005)	
training:	Epoch: [119][112/204]	Loss 0.4084 (0.4005)	
training:	Epoch: [119][113/204]	Loss 0.4887 (0.4013)	
training:	Epoch: [119][114/204]	Loss 0.5059 (0.4022)	
training:	Epoch: [119][115/204]	Loss 0.6028 (0.4040)	
training:	Epoch: [119][116/204]	Loss 0.1976 (0.4022)	
training:	Epoch: [119][117/204]	Loss 0.4584 (0.4027)	
training:	Epoch: [119][118/204]	Loss 0.2855 (0.4017)	
training:	Epoch: [119][119/204]	Loss 0.3023 (0.4009)	
training:	Epoch: [119][120/204]	Loss 0.3505 (0.4004)	
training:	Epoch: [119][121/204]	Loss 0.4862 (0.4011)	
training:	Epoch: [119][122/204]	Loss 0.5022 (0.4020)	
training:	Epoch: [119][123/204]	Loss 0.4851 (0.4026)	
training:	Epoch: [119][124/204]	Loss 0.3340 (0.4021)	
training:	Epoch: [119][125/204]	Loss 0.3337 (0.4015)	
training:	Epoch: [119][126/204]	Loss 0.4425 (0.4019)	
training:	Epoch: [119][127/204]	Loss 0.4139 (0.4020)	
training:	Epoch: [119][128/204]	Loss 0.3537 (0.4016)	
training:	Epoch: [119][129/204]	Loss 0.5073 (0.4024)	
training:	Epoch: [119][130/204]	Loss 0.4142 (0.4025)	
training:	Epoch: [119][131/204]	Loss 0.4304 (0.4027)	
training:	Epoch: [119][132/204]	Loss 0.4698 (0.4032)	
training:	Epoch: [119][133/204]	Loss 0.2905 (0.4024)	
training:	Epoch: [119][134/204]	Loss 0.2572 (0.4013)	
training:	Epoch: [119][135/204]	Loss 0.2884 (0.4005)	
training:	Epoch: [119][136/204]	Loss 0.3850 (0.4003)	
training:	Epoch: [119][137/204]	Loss 0.2624 (0.3993)	
training:	Epoch: [119][138/204]	Loss 0.3538 (0.3990)	
training:	Epoch: [119][139/204]	Loss 0.3877 (0.3989)	
training:	Epoch: [119][140/204]	Loss 0.3220 (0.3984)	
training:	Epoch: [119][141/204]	Loss 0.3245 (0.3978)	
training:	Epoch: [119][142/204]	Loss 0.2702 (0.3969)	
training:	Epoch: [119][143/204]	Loss 0.5259 (0.3979)	
training:	Epoch: [119][144/204]	Loss 0.2501 (0.3968)	
training:	Epoch: [119][145/204]	Loss 0.3138 (0.3963)	
training:	Epoch: [119][146/204]	Loss 0.3799 (0.3961)	
training:	Epoch: [119][147/204]	Loss 0.3909 (0.3961)	
training:	Epoch: [119][148/204]	Loss 0.4349 (0.3964)	
training:	Epoch: [119][149/204]	Loss 0.4629 (0.3968)	
training:	Epoch: [119][150/204]	Loss 0.3659 (0.3966)	
training:	Epoch: [119][151/204]	Loss 0.4463 (0.3969)	
training:	Epoch: [119][152/204]	Loss 0.2439 (0.3959)	
training:	Epoch: [119][153/204]	Loss 0.4689 (0.3964)	
training:	Epoch: [119][154/204]	Loss 0.3423 (0.3961)	
training:	Epoch: [119][155/204]	Loss 0.2435 (0.3951)	
training:	Epoch: [119][156/204]	Loss 0.4155 (0.3952)	
training:	Epoch: [119][157/204]	Loss 0.4705 (0.3957)	
training:	Epoch: [119][158/204]	Loss 0.2671 (0.3949)	
training:	Epoch: [119][159/204]	Loss 0.4443 (0.3952)	
training:	Epoch: [119][160/204]	Loss 0.3286 (0.3948)	
training:	Epoch: [119][161/204]	Loss 0.4430 (0.3951)	
training:	Epoch: [119][162/204]	Loss 0.3877 (0.3950)	
training:	Epoch: [119][163/204]	Loss 0.4287 (0.3952)	
training:	Epoch: [119][164/204]	Loss 0.4106 (0.3953)	
training:	Epoch: [119][165/204]	Loss 0.2150 (0.3942)	
training:	Epoch: [119][166/204]	Loss 0.3578 (0.3940)	
training:	Epoch: [119][167/204]	Loss 0.3866 (0.3940)	
training:	Epoch: [119][168/204]	Loss 0.3392 (0.3936)	
training:	Epoch: [119][169/204]	Loss 0.4360 (0.3939)	
training:	Epoch: [119][170/204]	Loss 0.3654 (0.3937)	
training:	Epoch: [119][171/204]	Loss 0.6076 (0.3950)	
training:	Epoch: [119][172/204]	Loss 0.3913 (0.3949)	
training:	Epoch: [119][173/204]	Loss 0.4064 (0.3950)	
training:	Epoch: [119][174/204]	Loss 0.3485 (0.3947)	
training:	Epoch: [119][175/204]	Loss 0.5034 (0.3954)	
training:	Epoch: [119][176/204]	Loss 0.4212 (0.3955)	
training:	Epoch: [119][177/204]	Loss 0.5595 (0.3964)	
training:	Epoch: [119][178/204]	Loss 0.4512 (0.3967)	
training:	Epoch: [119][179/204]	Loss 0.5027 (0.3973)	
training:	Epoch: [119][180/204]	Loss 0.3142 (0.3969)	
training:	Epoch: [119][181/204]	Loss 0.4338 (0.3971)	
training:	Epoch: [119][182/204]	Loss 0.4086 (0.3971)	
training:	Epoch: [119][183/204]	Loss 0.3824 (0.3971)	
training:	Epoch: [119][184/204]	Loss 0.3027 (0.3966)	
training:	Epoch: [119][185/204]	Loss 0.3097 (0.3961)	
training:	Epoch: [119][186/204]	Loss 0.3744 (0.3960)	
training:	Epoch: [119][187/204]	Loss 0.5033 (0.3965)	
training:	Epoch: [119][188/204]	Loss 0.3436 (0.3963)	
training:	Epoch: [119][189/204]	Loss 0.3425 (0.3960)	
training:	Epoch: [119][190/204]	Loss 0.3287 (0.3956)	
training:	Epoch: [119][191/204]	Loss 0.4897 (0.3961)	
training:	Epoch: [119][192/204]	Loss 0.4344 (0.3963)	
training:	Epoch: [119][193/204]	Loss 0.2091 (0.3953)	
training:	Epoch: [119][194/204]	Loss 0.3939 (0.3953)	
training:	Epoch: [119][195/204]	Loss 0.4693 (0.3957)	
training:	Epoch: [119][196/204]	Loss 0.4814 (0.3962)	
training:	Epoch: [119][197/204]	Loss 0.2958 (0.3956)	
training:	Epoch: [119][198/204]	Loss 0.4990 (0.3962)	
training:	Epoch: [119][199/204]	Loss 0.3737 (0.3961)	
training:	Epoch: [119][200/204]	Loss 0.4303 (0.3962)	
training:	Epoch: [119][201/204]	Loss 0.4196 (0.3963)	
training:	Epoch: [119][202/204]	Loss 0.3922 (0.3963)	
training:	Epoch: [119][203/204]	Loss 0.3712 (0.3962)	
training:	Epoch: [119][204/204]	Loss 0.3591 (0.3960)	
Training:	 Loss: 0.3954

Training:	 ACC: 0.8385 0.8386 0.8407 0.8364
Validation:	 ACC: 0.7950 0.7956 0.8086 0.7814
Validation:	 Best_BACC: 0.7988 0.7999 0.8229 0.7747
Validation:	 Loss: 0.4297
Pretraining:	Epoch 120/120
----------
training:	Epoch: [120][1/204]	Loss 0.5327 (0.5327)	
training:	Epoch: [120][2/204]	Loss 0.4379 (0.4853)	
training:	Epoch: [120][3/204]	Loss 0.3412 (0.4373)	
training:	Epoch: [120][4/204]	Loss 0.2914 (0.4008)	
training:	Epoch: [120][5/204]	Loss 0.5448 (0.4296)	
training:	Epoch: [120][6/204]	Loss 0.4097 (0.4263)	
training:	Epoch: [120][7/204]	Loss 0.5205 (0.4398)	
training:	Epoch: [120][8/204]	Loss 0.3030 (0.4227)	
training:	Epoch: [120][9/204]	Loss 0.2471 (0.4032)	
training:	Epoch: [120][10/204]	Loss 0.5232 (0.4152)	
training:	Epoch: [120][11/204]	Loss 0.3551 (0.4097)	
training:	Epoch: [120][12/204]	Loss 0.3140 (0.4017)	
training:	Epoch: [120][13/204]	Loss 0.3080 (0.3945)	
training:	Epoch: [120][14/204]	Loss 0.4394 (0.3977)	
training:	Epoch: [120][15/204]	Loss 0.2702 (0.3892)	
training:	Epoch: [120][16/204]	Loss 0.5061 (0.3965)	
training:	Epoch: [120][17/204]	Loss 0.2700 (0.3891)	
training:	Epoch: [120][18/204]	Loss 0.3024 (0.3843)	
training:	Epoch: [120][19/204]	Loss 0.6425 (0.3979)	
training:	Epoch: [120][20/204]	Loss 0.3626 (0.3961)	
training:	Epoch: [120][21/204]	Loss 0.3237 (0.3926)	
training:	Epoch: [120][22/204]	Loss 0.3069 (0.3888)	
training:	Epoch: [120][23/204]	Loss 0.6859 (0.4017)	
training:	Epoch: [120][24/204]	Loss 0.3555 (0.3997)	
training:	Epoch: [120][25/204]	Loss 0.3465 (0.3976)	
training:	Epoch: [120][26/204]	Loss 0.4669 (0.4003)	
training:	Epoch: [120][27/204]	Loss 0.4152 (0.4008)	
training:	Epoch: [120][28/204]	Loss 0.4135 (0.4013)	
training:	Epoch: [120][29/204]	Loss 0.3714 (0.4003)	
training:	Epoch: [120][30/204]	Loss 0.3643 (0.3991)	
training:	Epoch: [120][31/204]	Loss 0.4015 (0.3991)	
training:	Epoch: [120][32/204]	Loss 0.3028 (0.3961)	
training:	Epoch: [120][33/204]	Loss 0.2410 (0.3914)	
training:	Epoch: [120][34/204]	Loss 0.4083 (0.3919)	
training:	Epoch: [120][35/204]	Loss 0.5126 (0.3954)	
training:	Epoch: [120][36/204]	Loss 0.3133 (0.3931)	
training:	Epoch: [120][37/204]	Loss 0.4334 (0.3942)	
training:	Epoch: [120][38/204]	Loss 0.5196 (0.3975)	
training:	Epoch: [120][39/204]	Loss 0.3196 (0.3955)	
training:	Epoch: [120][40/204]	Loss 0.3113 (0.3934)	
training:	Epoch: [120][41/204]	Loss 0.4335 (0.3944)	
training:	Epoch: [120][42/204]	Loss 0.4892 (0.3966)	
training:	Epoch: [120][43/204]	Loss 0.3842 (0.3963)	
training:	Epoch: [120][44/204]	Loss 0.4119 (0.3967)	
training:	Epoch: [120][45/204]	Loss 0.3266 (0.3951)	
training:	Epoch: [120][46/204]	Loss 0.4536 (0.3964)	
training:	Epoch: [120][47/204]	Loss 0.4531 (0.3976)	
training:	Epoch: [120][48/204]	Loss 0.3367 (0.3963)	
training:	Epoch: [120][49/204]	Loss 0.4585 (0.3976)	
training:	Epoch: [120][50/204]	Loss 0.4281 (0.3982)	
training:	Epoch: [120][51/204]	Loss 0.2866 (0.3960)	
training:	Epoch: [120][52/204]	Loss 0.5218 (0.3984)	
training:	Epoch: [120][53/204]	Loss 0.4447 (0.3993)	
training:	Epoch: [120][54/204]	Loss 0.3808 (0.3990)	
training:	Epoch: [120][55/204]	Loss 0.5291 (0.4013)	
training:	Epoch: [120][56/204]	Loss 0.4801 (0.4027)	
training:	Epoch: [120][57/204]	Loss 0.2744 (0.4005)	
training:	Epoch: [120][58/204]	Loss 0.2911 (0.3986)	
training:	Epoch: [120][59/204]	Loss 0.5335 (0.4009)	
training:	Epoch: [120][60/204]	Loss 0.3392 (0.3999)	
training:	Epoch: [120][61/204]	Loss 0.2533 (0.3975)	
training:	Epoch: [120][62/204]	Loss 0.4315 (0.3980)	
training:	Epoch: [120][63/204]	Loss 0.4073 (0.3982)	
training:	Epoch: [120][64/204]	Loss 0.4773 (0.3994)	
training:	Epoch: [120][65/204]	Loss 0.3519 (0.3987)	
training:	Epoch: [120][66/204]	Loss 0.3771 (0.3983)	
training:	Epoch: [120][67/204]	Loss 0.2942 (0.3968)	
training:	Epoch: [120][68/204]	Loss 0.2937 (0.3953)	
training:	Epoch: [120][69/204]	Loss 0.5371 (0.3973)	
training:	Epoch: [120][70/204]	Loss 0.4111 (0.3975)	
training:	Epoch: [120][71/204]	Loss 0.4433 (0.3982)	
training:	Epoch: [120][72/204]	Loss 0.6007 (0.4010)	
training:	Epoch: [120][73/204]	Loss 0.4033 (0.4010)	
training:	Epoch: [120][74/204]	Loss 0.2667 (0.3992)	
training:	Epoch: [120][75/204]	Loss 0.4708 (0.4001)	
training:	Epoch: [120][76/204]	Loss 0.4694 (0.4011)	
training:	Epoch: [120][77/204]	Loss 0.4311 (0.4014)	
training:	Epoch: [120][78/204]	Loss 0.4035 (0.4015)	
training:	Epoch: [120][79/204]	Loss 0.7441 (0.4058)	
training:	Epoch: [120][80/204]	Loss 0.6227 (0.4085)	
training:	Epoch: [120][81/204]	Loss 0.4908 (0.4095)	
training:	Epoch: [120][82/204]	Loss 0.5190 (0.4109)	
training:	Epoch: [120][83/204]	Loss 0.3928 (0.4107)	
training:	Epoch: [120][84/204]	Loss 0.3423 (0.4098)	
training:	Epoch: [120][85/204]	Loss 0.4467 (0.4103)	
training:	Epoch: [120][86/204]	Loss 0.3281 (0.4093)	
training:	Epoch: [120][87/204]	Loss 0.4906 (0.4103)	
training:	Epoch: [120][88/204]	Loss 0.3822 (0.4099)	
training:	Epoch: [120][89/204]	Loss 0.3247 (0.4090)	
training:	Epoch: [120][90/204]	Loss 0.3913 (0.4088)	
training:	Epoch: [120][91/204]	Loss 0.3946 (0.4086)	
training:	Epoch: [120][92/204]	Loss 0.3861 (0.4084)	
training:	Epoch: [120][93/204]	Loss 0.3649 (0.4079)	
training:	Epoch: [120][94/204]	Loss 0.5318 (0.4092)	
training:	Epoch: [120][95/204]	Loss 0.2668 (0.4077)	
training:	Epoch: [120][96/204]	Loss 0.3561 (0.4072)	
training:	Epoch: [120][97/204]	Loss 0.4049 (0.4072)	
training:	Epoch: [120][98/204]	Loss 0.2731 (0.4058)	
training:	Epoch: [120][99/204]	Loss 0.3481 (0.4052)	
training:	Epoch: [120][100/204]	Loss 0.3751 (0.4049)	
training:	Epoch: [120][101/204]	Loss 0.6267 (0.4071)	
training:	Epoch: [120][102/204]	Loss 0.4691 (0.4077)	
training:	Epoch: [120][103/204]	Loss 0.3233 (0.4069)	
training:	Epoch: [120][104/204]	Loss 0.3493 (0.4064)	
training:	Epoch: [120][105/204]	Loss 0.4388 (0.4067)	
training:	Epoch: [120][106/204]	Loss 0.2980 (0.4056)	
training:	Epoch: [120][107/204]	Loss 0.4003 (0.4056)	
training:	Epoch: [120][108/204]	Loss 0.3240 (0.4048)	
training:	Epoch: [120][109/204]	Loss 0.3256 (0.4041)	
training:	Epoch: [120][110/204]	Loss 0.3351 (0.4035)	
training:	Epoch: [120][111/204]	Loss 0.4137 (0.4036)	
training:	Epoch: [120][112/204]	Loss 0.3451 (0.4030)	
training:	Epoch: [120][113/204]	Loss 0.4724 (0.4037)	
training:	Epoch: [120][114/204]	Loss 0.4557 (0.4041)	
training:	Epoch: [120][115/204]	Loss 0.2892 (0.4031)	
training:	Epoch: [120][116/204]	Loss 0.3158 (0.4024)	
training:	Epoch: [120][117/204]	Loss 0.3966 (0.4023)	
training:	Epoch: [120][118/204]	Loss 0.4476 (0.4027)	
training:	Epoch: [120][119/204]	Loss 0.4542 (0.4031)	
training:	Epoch: [120][120/204]	Loss 0.4444 (0.4035)	
training:	Epoch: [120][121/204]	Loss 0.2857 (0.4025)	
training:	Epoch: [120][122/204]	Loss 0.3113 (0.4018)	
training:	Epoch: [120][123/204]	Loss 0.3450 (0.4013)	
training:	Epoch: [120][124/204]	Loss 0.4049 (0.4013)	
training:	Epoch: [120][125/204]	Loss 0.2670 (0.4002)	
training:	Epoch: [120][126/204]	Loss 0.4352 (0.4005)	
training:	Epoch: [120][127/204]	Loss 0.3379 (0.4000)	
training:	Epoch: [120][128/204]	Loss 0.5167 (0.4009)	
training:	Epoch: [120][129/204]	Loss 0.5293 (0.4019)	
training:	Epoch: [120][130/204]	Loss 0.4126 (0.4020)	
training:	Epoch: [120][131/204]	Loss 0.4348 (0.4023)	
training:	Epoch: [120][132/204]	Loss 0.2949 (0.4015)	
training:	Epoch: [120][133/204]	Loss 0.4562 (0.4019)	
training:	Epoch: [120][134/204]	Loss 0.4189 (0.4020)	
training:	Epoch: [120][135/204]	Loss 0.3785 (0.4018)	
training:	Epoch: [120][136/204]	Loss 0.3842 (0.4017)	
training:	Epoch: [120][137/204]	Loss 0.2709 (0.4007)	
training:	Epoch: [120][138/204]	Loss 0.3492 (0.4004)	
training:	Epoch: [120][139/204]	Loss 0.3729 (0.4002)	
training:	Epoch: [120][140/204]	Loss 0.3927 (0.4001)	
training:	Epoch: [120][141/204]	Loss 0.3425 (0.3997)	
training:	Epoch: [120][142/204]	Loss 0.4732 (0.4002)	
training:	Epoch: [120][143/204]	Loss 0.3339 (0.3998)	
training:	Epoch: [120][144/204]	Loss 0.4910 (0.4004)	
training:	Epoch: [120][145/204]	Loss 0.3502 (0.4000)	
training:	Epoch: [120][146/204]	Loss 0.3329 (0.3996)	
training:	Epoch: [120][147/204]	Loss 0.2377 (0.3985)	
training:	Epoch: [120][148/204]	Loss 0.4770 (0.3990)	
training:	Epoch: [120][149/204]	Loss 0.2538 (0.3980)	
training:	Epoch: [120][150/204]	Loss 0.4399 (0.3983)	
training:	Epoch: [120][151/204]	Loss 0.2747 (0.3975)	
training:	Epoch: [120][152/204]	Loss 0.3877 (0.3974)	
training:	Epoch: [120][153/204]	Loss 0.3520 (0.3971)	
training:	Epoch: [120][154/204]	Loss 0.4300 (0.3974)	
training:	Epoch: [120][155/204]	Loss 0.3923 (0.3973)	
training:	Epoch: [120][156/204]	Loss 0.3556 (0.3971)	
training:	Epoch: [120][157/204]	Loss 0.5519 (0.3980)	
training:	Epoch: [120][158/204]	Loss 0.5149 (0.3988)	
training:	Epoch: [120][159/204]	Loss 0.5163 (0.3995)	
training:	Epoch: [120][160/204]	Loss 0.3069 (0.3989)	
training:	Epoch: [120][161/204]	Loss 0.6660 (0.4006)	
training:	Epoch: [120][162/204]	Loss 0.3607 (0.4004)	
training:	Epoch: [120][163/204]	Loss 0.2807 (0.3996)	
training:	Epoch: [120][164/204]	Loss 0.4726 (0.4001)	
training:	Epoch: [120][165/204]	Loss 0.3043 (0.3995)	
training:	Epoch: [120][166/204]	Loss 0.3865 (0.3994)	
training:	Epoch: [120][167/204]	Loss 0.4995 (0.4000)	
training:	Epoch: [120][168/204]	Loss 0.4480 (0.4003)	
training:	Epoch: [120][169/204]	Loss 0.3852 (0.4002)	
training:	Epoch: [120][170/204]	Loss 0.4757 (0.4006)	
training:	Epoch: [120][171/204]	Loss 0.3910 (0.4006)	
training:	Epoch: [120][172/204]	Loss 0.4294 (0.4008)	
training:	Epoch: [120][173/204]	Loss 0.3222 (0.4003)	
training:	Epoch: [120][174/204]	Loss 0.3592 (0.4001)	
training:	Epoch: [120][175/204]	Loss 0.2820 (0.3994)	
training:	Epoch: [120][176/204]	Loss 0.3078 (0.3989)	
training:	Epoch: [120][177/204]	Loss 0.3762 (0.3987)	
training:	Epoch: [120][178/204]	Loss 0.3047 (0.3982)	
training:	Epoch: [120][179/204]	Loss 0.3581 (0.3980)	
training:	Epoch: [120][180/204]	Loss 0.4289 (0.3982)	
training:	Epoch: [120][181/204]	Loss 0.5065 (0.3988)	
training:	Epoch: [120][182/204]	Loss 0.3258 (0.3984)	
training:	Epoch: [120][183/204]	Loss 0.3773 (0.3982)	
training:	Epoch: [120][184/204]	Loss 0.4925 (0.3988)	
training:	Epoch: [120][185/204]	Loss 0.2980 (0.3982)	
training:	Epoch: [120][186/204]	Loss 0.5485 (0.3990)	
training:	Epoch: [120][187/204]	Loss 0.3717 (0.3989)	
training:	Epoch: [120][188/204]	Loss 0.3054 (0.3984)	
training:	Epoch: [120][189/204]	Loss 0.3784 (0.3983)	
training:	Epoch: [120][190/204]	Loss 0.3433 (0.3980)	
training:	Epoch: [120][191/204]	Loss 0.4541 (0.3983)	
training:	Epoch: [120][192/204]	Loss 0.3079 (0.3978)	
training:	Epoch: [120][193/204]	Loss 0.2531 (0.3971)	
training:	Epoch: [120][194/204]	Loss 0.4306 (0.3972)	
training:	Epoch: [120][195/204]	Loss 0.3210 (0.3968)	
training:	Epoch: [120][196/204]	Loss 0.5003 (0.3974)	
training:	Epoch: [120][197/204]	Loss 0.5099 (0.3979)	
training:	Epoch: [120][198/204]	Loss 0.3267 (0.3976)	
training:	Epoch: [120][199/204]	Loss 0.2800 (0.3970)	
training:	Epoch: [120][200/204]	Loss 0.4213 (0.3971)	
training:	Epoch: [120][201/204]	Loss 0.3262 (0.3968)	
training:	Epoch: [120][202/204]	Loss 0.3808 (0.3967)	
training:	Epoch: [120][203/204]	Loss 0.5760 (0.3976)	
training:	Epoch: [120][204/204]	Loss 0.3770 (0.3975)	
Training:	 Loss: 0.3968

Training:	 ACC: 0.8388 0.8388 0.8392 0.8383
Validation:	 ACC: 0.7950 0.7956 0.8076 0.7825
Validation:	 Best_BACC: 0.7988 0.7999 0.8229 0.7747
Validation:	 Loss: 0.4299
Training complete in 20m 28s
