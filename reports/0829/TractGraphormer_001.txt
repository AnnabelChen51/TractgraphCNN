Namespace(inputDirectory='data', outputDirectory='0829', DisFile='data/dis_sort_roi2dis2000.npy', task='sex', type='binary', feature=['Nos', 'FA1'], CUDA_id='0', data_id='0', dataset='Classification', norm=True, epochs=200, tensorboard=True, net_architecture='TractGraphormer', batch_size=32, rate=3e-05, weight=0.0, sched_step=300, sched_gamma=0.1, printing_frequency=1, seed=1, alpha=0, remix_kappa=0, remix_tau=0, loss='CE', sigma=0, k=20, fl=64, nh=1)
Training the 'TractGraphormer' architecture

The following parameters are used:
Batch size:	32
Number of workers:	0
Learning rate:	3e-05
Weight decay:	0.0
Scheduler steps:	300
Scheduler gamma:	0.1
Number of epochs of training:	200
Number of input channels:	2

Data preparation
Reading data from:	./data
Training set size:	6538
Validation set size:	1869

Performing calculations on:	cuda:0

Pretraining:	Epoch 1/200
----------
training:	Epoch: [1][1/204]	Loss 0.7372 (0.7372)	
training:	Epoch: [1][2/204]	Loss 0.7263 (0.7318)	
training:	Epoch: [1][3/204]	Loss 0.7231 (0.7289)	
training:	Epoch: [1][4/204]	Loss 0.6873 (0.7185)	
training:	Epoch: [1][5/204]	Loss 0.6699 (0.7088)	
training:	Epoch: [1][6/204]	Loss 0.6876 (0.7052)	
training:	Epoch: [1][7/204]	Loss 0.6996 (0.7044)	
training:	Epoch: [1][8/204]	Loss 0.6672 (0.6998)	
training:	Epoch: [1][9/204]	Loss 0.7008 (0.6999)	
training:	Epoch: [1][10/204]	Loss 0.6960 (0.6995)	
training:	Epoch: [1][11/204]	Loss 0.6933 (0.6989)	
training:	Epoch: [1][12/204]	Loss 0.7099 (0.6999)	
training:	Epoch: [1][13/204]	Loss 0.6868 (0.6989)	
training:	Epoch: [1][14/204]	Loss 0.6967 (0.6987)	
training:	Epoch: [1][15/204]	Loss 0.6601 (0.6961)	
training:	Epoch: [1][16/204]	Loss 0.7126 (0.6972)	
training:	Epoch: [1][17/204]	Loss 0.6899 (0.6967)	
training:	Epoch: [1][18/204]	Loss 0.6915 (0.6964)	
training:	Epoch: [1][19/204]	Loss 0.6842 (0.6958)	
training:	Epoch: [1][20/204]	Loss 0.6918 (0.6956)	
training:	Epoch: [1][21/204]	Loss 0.6612 (0.6940)	
training:	Epoch: [1][22/204]	Loss 0.7079 (0.6946)	
training:	Epoch: [1][23/204]	Loss 0.6825 (0.6941)	
training:	Epoch: [1][24/204]	Loss 0.6974 (0.6942)	
training:	Epoch: [1][25/204]	Loss 0.6916 (0.6941)	
training:	Epoch: [1][26/204]	Loss 0.7179 (0.6950)	
training:	Epoch: [1][27/204]	Loss 0.6800 (0.6945)	
training:	Epoch: [1][28/204]	Loss 0.6808 (0.6940)	
training:	Epoch: [1][29/204]	Loss 0.7031 (0.6943)	
training:	Epoch: [1][30/204]	Loss 0.7070 (0.6947)	
training:	Epoch: [1][31/204]	Loss 0.6844 (0.6944)	
training:	Epoch: [1][32/204]	Loss 0.7131 (0.6950)	
training:	Epoch: [1][33/204]	Loss 0.7099 (0.6954)	
training:	Epoch: [1][34/204]	Loss 0.6575 (0.6943)	
training:	Epoch: [1][35/204]	Loss 0.7157 (0.6949)	
training:	Epoch: [1][36/204]	Loss 0.6908 (0.6948)	
training:	Epoch: [1][37/204]	Loss 0.6605 (0.6939)	
training:	Epoch: [1][38/204]	Loss 0.6612 (0.6930)	
training:	Epoch: [1][39/204]	Loss 0.6647 (0.6923)	
training:	Epoch: [1][40/204]	Loss 0.6768 (0.6919)	
training:	Epoch: [1][41/204]	Loss 0.6925 (0.6919)	
training:	Epoch: [1][42/204]	Loss 0.6877 (0.6918)	
training:	Epoch: [1][43/204]	Loss 0.6661 (0.6912)	
training:	Epoch: [1][44/204]	Loss 0.6848 (0.6911)	
training:	Epoch: [1][45/204]	Loss 0.6720 (0.6906)	
training:	Epoch: [1][46/204]	Loss 0.7072 (0.6910)	
training:	Epoch: [1][47/204]	Loss 0.6877 (0.6909)	
training:	Epoch: [1][48/204]	Loss 0.6391 (0.6899)	
training:	Epoch: [1][49/204]	Loss 0.7146 (0.6904)	
training:	Epoch: [1][50/204]	Loss 0.6895 (0.6903)	
training:	Epoch: [1][51/204]	Loss 0.6531 (0.6896)	
training:	Epoch: [1][52/204]	Loss 0.6891 (0.6896)	
training:	Epoch: [1][53/204]	Loss 0.6736 (0.6893)	
training:	Epoch: [1][54/204]	Loss 0.6580 (0.6887)	
training:	Epoch: [1][55/204]	Loss 0.6647 (0.6883)	
training:	Epoch: [1][56/204]	Loss 0.6743 (0.6880)	
training:	Epoch: [1][57/204]	Loss 0.7043 (0.6883)	
training:	Epoch: [1][58/204]	Loss 0.6503 (0.6877)	
training:	Epoch: [1][59/204]	Loss 0.6972 (0.6878)	
training:	Epoch: [1][60/204]	Loss 0.6230 (0.6867)	
training:	Epoch: [1][61/204]	Loss 0.6573 (0.6863)	
training:	Epoch: [1][62/204]	Loss 0.6713 (0.6860)	
training:	Epoch: [1][63/204]	Loss 0.7339 (0.6868)	
training:	Epoch: [1][64/204]	Loss 0.6933 (0.6869)	
training:	Epoch: [1][65/204]	Loss 0.6687 (0.6866)	
training:	Epoch: [1][66/204]	Loss 0.7271 (0.6872)	
training:	Epoch: [1][67/204]	Loss 0.6697 (0.6870)	
training:	Epoch: [1][68/204]	Loss 0.6353 (0.6862)	
training:	Epoch: [1][69/204]	Loss 0.6561 (0.6858)	
training:	Epoch: [1][70/204]	Loss 0.6999 (0.6860)	
training:	Epoch: [1][71/204]	Loss 0.7114 (0.6863)	
training:	Epoch: [1][72/204]	Loss 0.6883 (0.6863)	
training:	Epoch: [1][73/204]	Loss 0.6837 (0.6863)	
training:	Epoch: [1][74/204]	Loss 0.7093 (0.6866)	
training:	Epoch: [1][75/204]	Loss 0.6915 (0.6867)	
training:	Epoch: [1][76/204]	Loss 0.6784 (0.6866)	
training:	Epoch: [1][77/204]	Loss 0.6712 (0.6864)	
training:	Epoch: [1][78/204]	Loss 0.6728 (0.6862)	
training:	Epoch: [1][79/204]	Loss 0.7030 (0.6864)	
training:	Epoch: [1][80/204]	Loss 0.6739 (0.6863)	
training:	Epoch: [1][81/204]	Loss 0.6880 (0.6863)	
training:	Epoch: [1][82/204]	Loss 0.6799 (0.6862)	
training:	Epoch: [1][83/204]	Loss 0.7054 (0.6864)	
training:	Epoch: [1][84/204]	Loss 0.6914 (0.6865)	
training:	Epoch: [1][85/204]	Loss 0.6789 (0.6864)	
training:	Epoch: [1][86/204]	Loss 0.7018 (0.6866)	
training:	Epoch: [1][87/204]	Loss 0.6960 (0.6867)	
training:	Epoch: [1][88/204]	Loss 0.6835 (0.6867)	
training:	Epoch: [1][89/204]	Loss 0.7165 (0.6870)	
training:	Epoch: [1][90/204]	Loss 0.7176 (0.6873)	
training:	Epoch: [1][91/204]	Loss 0.6527 (0.6870)	
training:	Epoch: [1][92/204]	Loss 0.6629 (0.6867)	
training:	Epoch: [1][93/204]	Loss 0.6572 (0.6864)	
training:	Epoch: [1][94/204]	Loss 0.7209 (0.6867)	
training:	Epoch: [1][95/204]	Loss 0.6890 (0.6868)	
training:	Epoch: [1][96/204]	Loss 0.6362 (0.6862)	
training:	Epoch: [1][97/204]	Loss 0.6699 (0.6861)	
training:	Epoch: [1][98/204]	Loss 0.6790 (0.6860)	
training:	Epoch: [1][99/204]	Loss 0.6637 (0.6858)	
training:	Epoch: [1][100/204]	Loss 0.6659 (0.6856)	
training:	Epoch: [1][101/204]	Loss 0.6694 (0.6854)	
training:	Epoch: [1][102/204]	Loss 0.7012 (0.6856)	
training:	Epoch: [1][103/204]	Loss 0.7170 (0.6859)	
training:	Epoch: [1][104/204]	Loss 0.6540 (0.6856)	
training:	Epoch: [1][105/204]	Loss 0.6579 (0.6853)	
training:	Epoch: [1][106/204]	Loss 0.7222 (0.6857)	
training:	Epoch: [1][107/204]	Loss 0.6769 (0.6856)	
training:	Epoch: [1][108/204]	Loss 0.6981 (0.6857)	
training:	Epoch: [1][109/204]	Loss 0.6687 (0.6855)	
training:	Epoch: [1][110/204]	Loss 0.7057 (0.6857)	
training:	Epoch: [1][111/204]	Loss 0.6756 (0.6856)	
training:	Epoch: [1][112/204]	Loss 0.6964 (0.6857)	
training:	Epoch: [1][113/204]	Loss 0.6744 (0.6856)	
training:	Epoch: [1][114/204]	Loss 0.6610 (0.6854)	
training:	Epoch: [1][115/204]	Loss 0.6500 (0.6851)	
training:	Epoch: [1][116/204]	Loss 0.6650 (0.6849)	
training:	Epoch: [1][117/204]	Loss 0.6875 (0.6849)	
training:	Epoch: [1][118/204]	Loss 0.7193 (0.6852)	
training:	Epoch: [1][119/204]	Loss 0.6697 (0.6851)	
training:	Epoch: [1][120/204]	Loss 0.6327 (0.6847)	
training:	Epoch: [1][121/204]	Loss 0.6620 (0.6845)	
training:	Epoch: [1][122/204]	Loss 0.6791 (0.6844)	
training:	Epoch: [1][123/204]	Loss 0.7030 (0.6846)	
training:	Epoch: [1][124/204]	Loss 0.6482 (0.6843)	
training:	Epoch: [1][125/204]	Loss 0.6620 (0.6841)	
training:	Epoch: [1][126/204]	Loss 0.6723 (0.6840)	
training:	Epoch: [1][127/204]	Loss 0.6529 (0.6838)	
training:	Epoch: [1][128/204]	Loss 0.6165 (0.6832)	
training:	Epoch: [1][129/204]	Loss 0.6613 (0.6831)	
training:	Epoch: [1][130/204]	Loss 0.6868 (0.6831)	
training:	Epoch: [1][131/204]	Loss 0.6867 (0.6831)	
training:	Epoch: [1][132/204]	Loss 0.6300 (0.6827)	
training:	Epoch: [1][133/204]	Loss 0.6534 (0.6825)	
training:	Epoch: [1][134/204]	Loss 0.6025 (0.6819)	
training:	Epoch: [1][135/204]	Loss 0.6751 (0.6819)	
training:	Epoch: [1][136/204]	Loss 0.6150 (0.6814)	
training:	Epoch: [1][137/204]	Loss 0.6442 (0.6811)	
training:	Epoch: [1][138/204]	Loss 0.6616 (0.6810)	
training:	Epoch: [1][139/204]	Loss 0.6756 (0.6809)	
training:	Epoch: [1][140/204]	Loss 0.6536 (0.6807)	
training:	Epoch: [1][141/204]	Loss 0.6122 (0.6802)	
training:	Epoch: [1][142/204]	Loss 0.6529 (0.6800)	
training:	Epoch: [1][143/204]	Loss 0.6481 (0.6798)	
training:	Epoch: [1][144/204]	Loss 0.6591 (0.6797)	
training:	Epoch: [1][145/204]	Loss 0.6670 (0.6796)	
training:	Epoch: [1][146/204]	Loss 0.6466 (0.6794)	
training:	Epoch: [1][147/204]	Loss 0.6350 (0.6791)	
training:	Epoch: [1][148/204]	Loss 0.6089 (0.6786)	
training:	Epoch: [1][149/204]	Loss 0.6170 (0.6782)	
training:	Epoch: [1][150/204]	Loss 0.6442 (0.6780)	
training:	Epoch: [1][151/204]	Loss 0.6679 (0.6779)	
training:	Epoch: [1][152/204]	Loss 0.6038 (0.6774)	
training:	Epoch: [1][153/204]	Loss 0.7344 (0.6778)	
training:	Epoch: [1][154/204]	Loss 0.6152 (0.6774)	
training:	Epoch: [1][155/204]	Loss 0.6517 (0.6772)	
training:	Epoch: [1][156/204]	Loss 0.6240 (0.6769)	
training:	Epoch: [1][157/204]	Loss 0.6283 (0.6765)	
training:	Epoch: [1][158/204]	Loss 0.6512 (0.6764)	
training:	Epoch: [1][159/204]	Loss 0.6735 (0.6764)	
training:	Epoch: [1][160/204]	Loss 0.6525 (0.6762)	
training:	Epoch: [1][161/204]	Loss 0.6406 (0.6760)	
training:	Epoch: [1][162/204]	Loss 0.6693 (0.6760)	
training:	Epoch: [1][163/204]	Loss 0.6152 (0.6756)	
training:	Epoch: [1][164/204]	Loss 0.6641 (0.6755)	
training:	Epoch: [1][165/204]	Loss 0.5945 (0.6750)	
training:	Epoch: [1][166/204]	Loss 0.6421 (0.6748)	
training:	Epoch: [1][167/204]	Loss 0.6218 (0.6745)	
training:	Epoch: [1][168/204]	Loss 0.5829 (0.6740)	
training:	Epoch: [1][169/204]	Loss 0.5802 (0.6734)	
training:	Epoch: [1][170/204]	Loss 0.6946 (0.6735)	
training:	Epoch: [1][171/204]	Loss 0.6505 (0.6734)	
training:	Epoch: [1][172/204]	Loss 0.6292 (0.6731)	
training:	Epoch: [1][173/204]	Loss 0.5982 (0.6727)	
training:	Epoch: [1][174/204]	Loss 0.6534 (0.6726)	
training:	Epoch: [1][175/204]	Loss 0.6349 (0.6724)	
training:	Epoch: [1][176/204]	Loss 0.6165 (0.6721)	
training:	Epoch: [1][177/204]	Loss 0.6094 (0.6717)	
training:	Epoch: [1][178/204]	Loss 0.6056 (0.6713)	
training:	Epoch: [1][179/204]	Loss 0.6422 (0.6712)	
training:	Epoch: [1][180/204]	Loss 0.6548 (0.6711)	
training:	Epoch: [1][181/204]	Loss 0.5153 (0.6702)	
training:	Epoch: [1][182/204]	Loss 0.6834 (0.6703)	
training:	Epoch: [1][183/204]	Loss 0.6094 (0.6700)	
training:	Epoch: [1][184/204]	Loss 0.6100 (0.6696)	
training:	Epoch: [1][185/204]	Loss 0.6404 (0.6695)	
training:	Epoch: [1][186/204]	Loss 0.6190 (0.6692)	
training:	Epoch: [1][187/204]	Loss 0.5671 (0.6687)	
training:	Epoch: [1][188/204]	Loss 0.6337 (0.6685)	
training:	Epoch: [1][189/204]	Loss 0.6346 (0.6683)	
training:	Epoch: [1][190/204]	Loss 0.6792 (0.6684)	
training:	Epoch: [1][191/204]	Loss 0.6537 (0.6683)	
training:	Epoch: [1][192/204]	Loss 0.6129 (0.6680)	
training:	Epoch: [1][193/204]	Loss 0.6076 (0.6677)	
training:	Epoch: [1][194/204]	Loss 0.6930 (0.6678)	
training:	Epoch: [1][195/204]	Loss 0.6754 (0.6678)	
training:	Epoch: [1][196/204]	Loss 0.7222 (0.6681)	
training:	Epoch: [1][197/204]	Loss 0.6307 (0.6679)	
training:	Epoch: [1][198/204]	Loss 0.6824 (0.6680)	
training:	Epoch: [1][199/204]	Loss 0.6079 (0.6677)	
training:	Epoch: [1][200/204]	Loss 0.5711 (0.6672)	
training:	Epoch: [1][201/204]	Loss 0.6475 (0.6671)	
training:	Epoch: [1][202/204]	Loss 0.6545 (0.6671)	
training:	Epoch: [1][203/204]	Loss 0.6015 (0.6667)	
training:	Epoch: [1][204/204]	Loss 0.5700 (0.6663)	
Training:	 Loss: 0.6652

Training:	 ACC: 0.6759 0.6733 0.6120 0.7398
Validation:	 ACC: 0.6780 0.6752 0.6172 0.7388
Validation:	 Best_BACC: 0.6780 0.6752 0.6172 0.7388
Validation:	 Loss: 0.6108
Pretraining:	Epoch 2/200
----------
training:	Epoch: [2][1/204]	Loss 0.5576 (0.5576)	
training:	Epoch: [2][2/204]	Loss 0.6257 (0.5916)	
training:	Epoch: [2][3/204]	Loss 0.6197 (0.6010)	
training:	Epoch: [2][4/204]	Loss 0.6334 (0.6091)	
training:	Epoch: [2][5/204]	Loss 0.5610 (0.5995)	
training:	Epoch: [2][6/204]	Loss 0.6018 (0.5999)	
training:	Epoch: [2][7/204]	Loss 0.5678 (0.5953)	
training:	Epoch: [2][8/204]	Loss 0.5787 (0.5932)	
training:	Epoch: [2][9/204]	Loss 0.5335 (0.5866)	
training:	Epoch: [2][10/204]	Loss 0.5682 (0.5847)	
training:	Epoch: [2][11/204]	Loss 0.5644 (0.5829)	
training:	Epoch: [2][12/204]	Loss 0.6501 (0.5885)	
training:	Epoch: [2][13/204]	Loss 0.4859 (0.5806)	
training:	Epoch: [2][14/204]	Loss 0.5717 (0.5800)	
training:	Epoch: [2][15/204]	Loss 0.5848 (0.5803)	
training:	Epoch: [2][16/204]	Loss 0.6368 (0.5838)	
training:	Epoch: [2][17/204]	Loss 0.4889 (0.5782)	
training:	Epoch: [2][18/204]	Loss 0.6822 (0.5840)	
training:	Epoch: [2][19/204]	Loss 0.6309 (0.5865)	
training:	Epoch: [2][20/204]	Loss 0.6104 (0.5877)	
training:	Epoch: [2][21/204]	Loss 0.6440 (0.5903)	
training:	Epoch: [2][22/204]	Loss 0.6234 (0.5918)	
training:	Epoch: [2][23/204]	Loss 0.6007 (0.5922)	
training:	Epoch: [2][24/204]	Loss 0.6067 (0.5928)	
training:	Epoch: [2][25/204]	Loss 0.6542 (0.5953)	
training:	Epoch: [2][26/204]	Loss 0.4935 (0.5914)	
training:	Epoch: [2][27/204]	Loss 0.5490 (0.5898)	
training:	Epoch: [2][28/204]	Loss 0.6625 (0.5924)	
training:	Epoch: [2][29/204]	Loss 0.6274 (0.5936)	
training:	Epoch: [2][30/204]	Loss 0.6169 (0.5944)	
training:	Epoch: [2][31/204]	Loss 0.6809 (0.5972)	
training:	Epoch: [2][32/204]	Loss 0.4695 (0.5932)	
training:	Epoch: [2][33/204]	Loss 0.7406 (0.5977)	
training:	Epoch: [2][34/204]	Loss 0.5178 (0.5953)	
training:	Epoch: [2][35/204]	Loss 0.6358 (0.5965)	
training:	Epoch: [2][36/204]	Loss 0.5422 (0.5950)	
training:	Epoch: [2][37/204]	Loss 0.7022 (0.5979)	
training:	Epoch: [2][38/204]	Loss 0.6513 (0.5993)	
training:	Epoch: [2][39/204]	Loss 0.5556 (0.5981)	
training:	Epoch: [2][40/204]	Loss 0.6447 (0.5993)	
training:	Epoch: [2][41/204]	Loss 0.6314 (0.6001)	
training:	Epoch: [2][42/204]	Loss 0.6092 (0.6003)	
training:	Epoch: [2][43/204]	Loss 0.5684 (0.5996)	
training:	Epoch: [2][44/204]	Loss 0.6149 (0.5999)	
training:	Epoch: [2][45/204]	Loss 0.6212 (0.6004)	
training:	Epoch: [2][46/204]	Loss 0.6796 (0.6021)	
training:	Epoch: [2][47/204]	Loss 0.7139 (0.6045)	
training:	Epoch: [2][48/204]	Loss 0.7795 (0.6081)	
training:	Epoch: [2][49/204]	Loss 0.6238 (0.6084)	
training:	Epoch: [2][50/204]	Loss 0.5885 (0.6081)	
training:	Epoch: [2][51/204]	Loss 0.5475 (0.6069)	
training:	Epoch: [2][52/204]	Loss 0.4880 (0.6046)	
training:	Epoch: [2][53/204]	Loss 0.6323 (0.6051)	
training:	Epoch: [2][54/204]	Loss 0.6230 (0.6054)	
training:	Epoch: [2][55/204]	Loss 0.4919 (0.6034)	
training:	Epoch: [2][56/204]	Loss 0.6673 (0.6045)	
training:	Epoch: [2][57/204]	Loss 0.5640 (0.6038)	
training:	Epoch: [2][58/204]	Loss 0.6416 (0.6045)	
training:	Epoch: [2][59/204]	Loss 0.6288 (0.6049)	
training:	Epoch: [2][60/204]	Loss 0.4966 (0.6031)	
training:	Epoch: [2][61/204]	Loss 0.6154 (0.6033)	
training:	Epoch: [2][62/204]	Loss 0.7359 (0.6054)	
training:	Epoch: [2][63/204]	Loss 0.5956 (0.6052)	
training:	Epoch: [2][64/204]	Loss 0.5780 (0.6048)	
training:	Epoch: [2][65/204]	Loss 0.6057 (0.6048)	
training:	Epoch: [2][66/204]	Loss 0.5975 (0.6047)	
training:	Epoch: [2][67/204]	Loss 0.5937 (0.6046)	
training:	Epoch: [2][68/204]	Loss 0.6835 (0.6057)	
training:	Epoch: [2][69/204]	Loss 0.4791 (0.6039)	
training:	Epoch: [2][70/204]	Loss 0.6272 (0.6042)	
training:	Epoch: [2][71/204]	Loss 0.5710 (0.6038)	
training:	Epoch: [2][72/204]	Loss 0.7099 (0.6052)	
training:	Epoch: [2][73/204]	Loss 0.6293 (0.6056)	
training:	Epoch: [2][74/204]	Loss 0.5823 (0.6052)	
training:	Epoch: [2][75/204]	Loss 0.4949 (0.6038)	
training:	Epoch: [2][76/204]	Loss 0.5732 (0.6034)	
training:	Epoch: [2][77/204]	Loss 0.6416 (0.6039)	
training:	Epoch: [2][78/204]	Loss 0.5474 (0.6031)	
training:	Epoch: [2][79/204]	Loss 0.5783 (0.6028)	
training:	Epoch: [2][80/204]	Loss 0.4607 (0.6010)	
training:	Epoch: [2][81/204]	Loss 0.6225 (0.6013)	
training:	Epoch: [2][82/204]	Loss 0.5819 (0.6011)	
training:	Epoch: [2][83/204]	Loss 0.6586 (0.6018)	
training:	Epoch: [2][84/204]	Loss 0.5937 (0.6017)	
training:	Epoch: [2][85/204]	Loss 0.5541 (0.6011)	
training:	Epoch: [2][86/204]	Loss 0.6083 (0.6012)	
training:	Epoch: [2][87/204]	Loss 0.6392 (0.6016)	
training:	Epoch: [2][88/204]	Loss 0.5412 (0.6009)	
training:	Epoch: [2][89/204]	Loss 0.6284 (0.6013)	
training:	Epoch: [2][90/204]	Loss 0.6125 (0.6014)	
training:	Epoch: [2][91/204]	Loss 0.6679 (0.6021)	
training:	Epoch: [2][92/204]	Loss 0.5952 (0.6020)	
training:	Epoch: [2][93/204]	Loss 0.5427 (0.6014)	
training:	Epoch: [2][94/204]	Loss 0.5315 (0.6007)	
training:	Epoch: [2][95/204]	Loss 0.6203 (0.6009)	
training:	Epoch: [2][96/204]	Loss 0.6111 (0.6010)	
training:	Epoch: [2][97/204]	Loss 0.6469 (0.6014)	
training:	Epoch: [2][98/204]	Loss 0.4619 (0.6000)	
training:	Epoch: [2][99/204]	Loss 0.6052 (0.6001)	
training:	Epoch: [2][100/204]	Loss 0.5933 (0.6000)	
training:	Epoch: [2][101/204]	Loss 0.5059 (0.5991)	
training:	Epoch: [2][102/204]	Loss 0.6129 (0.5992)	
training:	Epoch: [2][103/204]	Loss 0.5222 (0.5985)	
training:	Epoch: [2][104/204]	Loss 0.5511 (0.5980)	
training:	Epoch: [2][105/204]	Loss 0.5636 (0.5977)	
training:	Epoch: [2][106/204]	Loss 0.5971 (0.5977)	
training:	Epoch: [2][107/204]	Loss 0.5645 (0.5974)	
training:	Epoch: [2][108/204]	Loss 0.5424 (0.5969)	
training:	Epoch: [2][109/204]	Loss 0.4048 (0.5951)	
training:	Epoch: [2][110/204]	Loss 0.5046 (0.5943)	
training:	Epoch: [2][111/204]	Loss 0.5314 (0.5937)	
training:	Epoch: [2][112/204]	Loss 0.6654 (0.5943)	
training:	Epoch: [2][113/204]	Loss 0.5231 (0.5937)	
training:	Epoch: [2][114/204]	Loss 0.5555 (0.5934)	
training:	Epoch: [2][115/204]	Loss 0.6979 (0.5943)	
training:	Epoch: [2][116/204]	Loss 0.7187 (0.5954)	
training:	Epoch: [2][117/204]	Loss 0.6198 (0.5956)	
training:	Epoch: [2][118/204]	Loss 0.6689 (0.5962)	
training:	Epoch: [2][119/204]	Loss 0.4714 (0.5951)	
training:	Epoch: [2][120/204]	Loss 0.6455 (0.5956)	
training:	Epoch: [2][121/204]	Loss 0.6708 (0.5962)	
training:	Epoch: [2][122/204]	Loss 0.4878 (0.5953)	
training:	Epoch: [2][123/204]	Loss 0.7030 (0.5962)	
training:	Epoch: [2][124/204]	Loss 0.6435 (0.5965)	
training:	Epoch: [2][125/204]	Loss 0.5439 (0.5961)	
training:	Epoch: [2][126/204]	Loss 0.5123 (0.5955)	
training:	Epoch: [2][127/204]	Loss 0.4154 (0.5940)	
training:	Epoch: [2][128/204]	Loss 0.6272 (0.5943)	
training:	Epoch: [2][129/204]	Loss 0.5549 (0.5940)	
training:	Epoch: [2][130/204]	Loss 0.5542 (0.5937)	
training:	Epoch: [2][131/204]	Loss 0.5489 (0.5933)	
training:	Epoch: [2][132/204]	Loss 0.5539 (0.5930)	
training:	Epoch: [2][133/204]	Loss 0.5726 (0.5929)	
training:	Epoch: [2][134/204]	Loss 0.5121 (0.5923)	
training:	Epoch: [2][135/204]	Loss 0.6010 (0.5924)	
training:	Epoch: [2][136/204]	Loss 0.4920 (0.5916)	
training:	Epoch: [2][137/204]	Loss 0.5182 (0.5911)	
training:	Epoch: [2][138/204]	Loss 0.5517 (0.5908)	
training:	Epoch: [2][139/204]	Loss 0.6255 (0.5910)	
training:	Epoch: [2][140/204]	Loss 0.6253 (0.5913)	
training:	Epoch: [2][141/204]	Loss 0.5863 (0.5913)	
training:	Epoch: [2][142/204]	Loss 0.5766 (0.5912)	
training:	Epoch: [2][143/204]	Loss 0.5508 (0.5909)	
training:	Epoch: [2][144/204]	Loss 0.4619 (0.5900)	
training:	Epoch: [2][145/204]	Loss 0.5514 (0.5897)	
training:	Epoch: [2][146/204]	Loss 0.5131 (0.5892)	
training:	Epoch: [2][147/204]	Loss 0.5272 (0.5888)	
training:	Epoch: [2][148/204]	Loss 0.5635 (0.5886)	
training:	Epoch: [2][149/204]	Loss 0.7345 (0.5896)	
training:	Epoch: [2][150/204]	Loss 0.6133 (0.5897)	
training:	Epoch: [2][151/204]	Loss 0.6016 (0.5898)	
training:	Epoch: [2][152/204]	Loss 0.6374 (0.5901)	
training:	Epoch: [2][153/204]	Loss 0.5645 (0.5900)	
training:	Epoch: [2][154/204]	Loss 0.6040 (0.5900)	
training:	Epoch: [2][155/204]	Loss 0.4937 (0.5894)	
training:	Epoch: [2][156/204]	Loss 0.4829 (0.5887)	
training:	Epoch: [2][157/204]	Loss 0.5149 (0.5883)	
training:	Epoch: [2][158/204]	Loss 0.5480 (0.5880)	
training:	Epoch: [2][159/204]	Loss 0.5751 (0.5879)	
training:	Epoch: [2][160/204]	Loss 0.5148 (0.5875)	
training:	Epoch: [2][161/204]	Loss 0.4777 (0.5868)	
training:	Epoch: [2][162/204]	Loss 0.6029 (0.5869)	
training:	Epoch: [2][163/204]	Loss 0.5754 (0.5868)	
training:	Epoch: [2][164/204]	Loss 0.5379 (0.5865)	
training:	Epoch: [2][165/204]	Loss 0.5376 (0.5862)	
training:	Epoch: [2][166/204]	Loss 0.5234 (0.5859)	
training:	Epoch: [2][167/204]	Loss 0.4653 (0.5851)	
training:	Epoch: [2][168/204]	Loss 0.6044 (0.5852)	
training:	Epoch: [2][169/204]	Loss 0.6184 (0.5854)	
training:	Epoch: [2][170/204]	Loss 0.4985 (0.5849)	
training:	Epoch: [2][171/204]	Loss 0.4719 (0.5843)	
training:	Epoch: [2][172/204]	Loss 0.4644 (0.5836)	
training:	Epoch: [2][173/204]	Loss 0.6023 (0.5837)	
training:	Epoch: [2][174/204]	Loss 0.4332 (0.5828)	
training:	Epoch: [2][175/204]	Loss 0.5475 (0.5826)	
training:	Epoch: [2][176/204]	Loss 0.6831 (0.5832)	
training:	Epoch: [2][177/204]	Loss 0.6577 (0.5836)	
training:	Epoch: [2][178/204]	Loss 0.4497 (0.5829)	
training:	Epoch: [2][179/204]	Loss 0.5110 (0.5824)	
training:	Epoch: [2][180/204]	Loss 0.6915 (0.5831)	
training:	Epoch: [2][181/204]	Loss 0.5896 (0.5831)	
training:	Epoch: [2][182/204]	Loss 0.5929 (0.5831)	
training:	Epoch: [2][183/204]	Loss 0.5982 (0.5832)	
training:	Epoch: [2][184/204]	Loss 0.4939 (0.5827)	
training:	Epoch: [2][185/204]	Loss 0.5562 (0.5826)	
training:	Epoch: [2][186/204]	Loss 0.5517 (0.5824)	
training:	Epoch: [2][187/204]	Loss 0.4792 (0.5819)	
training:	Epoch: [2][188/204]	Loss 0.5746 (0.5818)	
training:	Epoch: [2][189/204]	Loss 0.5523 (0.5817)	
training:	Epoch: [2][190/204]	Loss 0.6377 (0.5820)	
training:	Epoch: [2][191/204]	Loss 0.4905 (0.5815)	
training:	Epoch: [2][192/204]	Loss 0.4293 (0.5807)	
training:	Epoch: [2][193/204]	Loss 0.6370 (0.5810)	
training:	Epoch: [2][194/204]	Loss 0.5170 (0.5807)	
training:	Epoch: [2][195/204]	Loss 0.5071 (0.5803)	
training:	Epoch: [2][196/204]	Loss 0.5435 (0.5801)	
training:	Epoch: [2][197/204]	Loss 0.4776 (0.5796)	
training:	Epoch: [2][198/204]	Loss 0.6096 (0.5797)	
training:	Epoch: [2][199/204]	Loss 0.5182 (0.5794)	
training:	Epoch: [2][200/204]	Loss 0.5613 (0.5793)	
training:	Epoch: [2][201/204]	Loss 0.6365 (0.5796)	
training:	Epoch: [2][202/204]	Loss 0.5056 (0.5793)	
training:	Epoch: [2][203/204]	Loss 0.4685 (0.5787)	
training:	Epoch: [2][204/204]	Loss 0.6045 (0.5788)	
Training:	 Loss: 0.5780

Training:	 ACC: 0.7498 0.7470 0.6814 0.8182
Validation:	 ACC: 0.7514 0.7485 0.6888 0.8139
Validation:	 Best_BACC: 0.7514 0.7485 0.6888 0.8139
Validation:	 Loss: 0.5294
Pretraining:	Epoch 3/200
----------
training:	Epoch: [3][1/204]	Loss 0.5311 (0.5311)	
training:	Epoch: [3][2/204]	Loss 0.4789 (0.5050)	
training:	Epoch: [3][3/204]	Loss 0.5414 (0.5171)	
training:	Epoch: [3][4/204]	Loss 0.6431 (0.5486)	
training:	Epoch: [3][5/204]	Loss 0.5528 (0.5495)	
training:	Epoch: [3][6/204]	Loss 0.4383 (0.5309)	
training:	Epoch: [3][7/204]	Loss 0.5127 (0.5283)	
training:	Epoch: [3][8/204]	Loss 0.5780 (0.5346)	
training:	Epoch: [3][9/204]	Loss 0.5335 (0.5344)	
training:	Epoch: [3][10/204]	Loss 0.4830 (0.5293)	
training:	Epoch: [3][11/204]	Loss 0.5972 (0.5355)	
training:	Epoch: [3][12/204]	Loss 0.6794 (0.5475)	
training:	Epoch: [3][13/204]	Loss 0.4096 (0.5369)	
training:	Epoch: [3][14/204]	Loss 0.4895 (0.5335)	
training:	Epoch: [3][15/204]	Loss 0.4384 (0.5271)	
training:	Epoch: [3][16/204]	Loss 0.5820 (0.5306)	
training:	Epoch: [3][17/204]	Loss 0.5840 (0.5337)	
training:	Epoch: [3][18/204]	Loss 0.4987 (0.5318)	
training:	Epoch: [3][19/204]	Loss 0.5989 (0.5353)	
training:	Epoch: [3][20/204]	Loss 0.5248 (0.5348)	
training:	Epoch: [3][21/204]	Loss 0.4541 (0.5309)	
training:	Epoch: [3][22/204]	Loss 0.4815 (0.5287)	
training:	Epoch: [3][23/204]	Loss 0.5220 (0.5284)	
training:	Epoch: [3][24/204]	Loss 0.6096 (0.5318)	
training:	Epoch: [3][25/204]	Loss 0.4844 (0.5299)	
training:	Epoch: [3][26/204]	Loss 0.6105 (0.5330)	
training:	Epoch: [3][27/204]	Loss 0.4235 (0.5289)	
training:	Epoch: [3][28/204]	Loss 0.4302 (0.5254)	
training:	Epoch: [3][29/204]	Loss 0.6533 (0.5298)	
training:	Epoch: [3][30/204]	Loss 0.6443 (0.5336)	
training:	Epoch: [3][31/204]	Loss 0.3992 (0.5293)	
training:	Epoch: [3][32/204]	Loss 0.5600 (0.5303)	
training:	Epoch: [3][33/204]	Loss 0.4646 (0.5283)	
training:	Epoch: [3][34/204]	Loss 0.5506 (0.5289)	
training:	Epoch: [3][35/204]	Loss 0.6990 (0.5338)	
training:	Epoch: [3][36/204]	Loss 0.5393 (0.5339)	
training:	Epoch: [3][37/204]	Loss 0.5706 (0.5349)	
training:	Epoch: [3][38/204]	Loss 0.5822 (0.5362)	
training:	Epoch: [3][39/204]	Loss 0.5051 (0.5354)	
training:	Epoch: [3][40/204]	Loss 0.4924 (0.5343)	
training:	Epoch: [3][41/204]	Loss 0.4641 (0.5326)	
training:	Epoch: [3][42/204]	Loss 0.5319 (0.5326)	
training:	Epoch: [3][43/204]	Loss 0.5056 (0.5319)	
training:	Epoch: [3][44/204]	Loss 0.7123 (0.5360)	
training:	Epoch: [3][45/204]	Loss 0.3862 (0.5327)	
training:	Epoch: [3][46/204]	Loss 0.5183 (0.5324)	
training:	Epoch: [3][47/204]	Loss 0.4738 (0.5312)	
training:	Epoch: [3][48/204]	Loss 0.4841 (0.5302)	
training:	Epoch: [3][49/204]	Loss 0.3843 (0.5272)	
training:	Epoch: [3][50/204]	Loss 0.5468 (0.5276)	
training:	Epoch: [3][51/204]	Loss 0.4992 (0.5270)	
training:	Epoch: [3][52/204]	Loss 0.5084 (0.5267)	
training:	Epoch: [3][53/204]	Loss 0.4019 (0.5243)	
training:	Epoch: [3][54/204]	Loss 0.5634 (0.5250)	
training:	Epoch: [3][55/204]	Loss 0.5092 (0.5248)	
training:	Epoch: [3][56/204]	Loss 0.5183 (0.5246)	
training:	Epoch: [3][57/204]	Loss 0.5076 (0.5243)	
training:	Epoch: [3][58/204]	Loss 0.4156 (0.5225)	
training:	Epoch: [3][59/204]	Loss 0.4640 (0.5215)	
training:	Epoch: [3][60/204]	Loss 0.5821 (0.5225)	
training:	Epoch: [3][61/204]	Loss 0.4662 (0.5216)	
training:	Epoch: [3][62/204]	Loss 0.4532 (0.5205)	
training:	Epoch: [3][63/204]	Loss 0.5821 (0.5214)	
training:	Epoch: [3][64/204]	Loss 0.5195 (0.5214)	
training:	Epoch: [3][65/204]	Loss 0.4466 (0.5203)	
training:	Epoch: [3][66/204]	Loss 0.5291 (0.5204)	
training:	Epoch: [3][67/204]	Loss 0.4667 (0.5196)	
training:	Epoch: [3][68/204]	Loss 0.4498 (0.5186)	
training:	Epoch: [3][69/204]	Loss 0.5068 (0.5184)	
training:	Epoch: [3][70/204]	Loss 0.4405 (0.5173)	
training:	Epoch: [3][71/204]	Loss 0.6221 (0.5188)	
training:	Epoch: [3][72/204]	Loss 0.5773 (0.5196)	
training:	Epoch: [3][73/204]	Loss 0.5338 (0.5198)	
training:	Epoch: [3][74/204]	Loss 0.4519 (0.5189)	
training:	Epoch: [3][75/204]	Loss 0.5493 (0.5193)	
training:	Epoch: [3][76/204]	Loss 0.4942 (0.5189)	
training:	Epoch: [3][77/204]	Loss 0.3860 (0.5172)	
training:	Epoch: [3][78/204]	Loss 0.6569 (0.5190)	
training:	Epoch: [3][79/204]	Loss 0.7279 (0.5216)	
training:	Epoch: [3][80/204]	Loss 0.4552 (0.5208)	
training:	Epoch: [3][81/204]	Loss 0.5447 (0.5211)	
training:	Epoch: [3][82/204]	Loss 0.6004 (0.5221)	
training:	Epoch: [3][83/204]	Loss 0.6460 (0.5236)	
training:	Epoch: [3][84/204]	Loss 0.3365 (0.5213)	
training:	Epoch: [3][85/204]	Loss 0.5631 (0.5218)	
training:	Epoch: [3][86/204]	Loss 0.5569 (0.5222)	
training:	Epoch: [3][87/204]	Loss 0.6228 (0.5234)	
training:	Epoch: [3][88/204]	Loss 0.3777 (0.5217)	
training:	Epoch: [3][89/204]	Loss 0.5934 (0.5225)	
training:	Epoch: [3][90/204]	Loss 0.4395 (0.5216)	
training:	Epoch: [3][91/204]	Loss 0.5512 (0.5219)	
training:	Epoch: [3][92/204]	Loss 0.4372 (0.5210)	
training:	Epoch: [3][93/204]	Loss 0.3914 (0.5196)	
training:	Epoch: [3][94/204]	Loss 0.5910 (0.5204)	
training:	Epoch: [3][95/204]	Loss 0.4463 (0.5196)	
training:	Epoch: [3][96/204]	Loss 0.4808 (0.5192)	
training:	Epoch: [3][97/204]	Loss 0.4517 (0.5185)	
training:	Epoch: [3][98/204]	Loss 0.6106 (0.5194)	
training:	Epoch: [3][99/204]	Loss 0.4949 (0.5192)	
training:	Epoch: [3][100/204]	Loss 0.5799 (0.5198)	
training:	Epoch: [3][101/204]	Loss 0.6276 (0.5209)	
training:	Epoch: [3][102/204]	Loss 0.5688 (0.5213)	
training:	Epoch: [3][103/204]	Loss 0.3762 (0.5199)	
training:	Epoch: [3][104/204]	Loss 0.5398 (0.5201)	
training:	Epoch: [3][105/204]	Loss 0.3249 (0.5183)	
training:	Epoch: [3][106/204]	Loss 0.4208 (0.5173)	
training:	Epoch: [3][107/204]	Loss 0.5203 (0.5174)	
training:	Epoch: [3][108/204]	Loss 0.5081 (0.5173)	
training:	Epoch: [3][109/204]	Loss 0.5922 (0.5180)	
training:	Epoch: [3][110/204]	Loss 0.3323 (0.5163)	
training:	Epoch: [3][111/204]	Loss 0.4326 (0.5155)	
training:	Epoch: [3][112/204]	Loss 0.5794 (0.5161)	
training:	Epoch: [3][113/204]	Loss 0.4789 (0.5158)	
training:	Epoch: [3][114/204]	Loss 0.6345 (0.5168)	
training:	Epoch: [3][115/204]	Loss 0.5387 (0.5170)	
training:	Epoch: [3][116/204]	Loss 0.4941 (0.5168)	
training:	Epoch: [3][117/204]	Loss 0.4391 (0.5161)	
training:	Epoch: [3][118/204]	Loss 0.6584 (0.5174)	
training:	Epoch: [3][119/204]	Loss 0.5232 (0.5174)	
training:	Epoch: [3][120/204]	Loss 0.3299 (0.5158)	
training:	Epoch: [3][121/204]	Loss 0.7288 (0.5176)	
training:	Epoch: [3][122/204]	Loss 0.5677 (0.5180)	
training:	Epoch: [3][123/204]	Loss 0.4678 (0.5176)	
training:	Epoch: [3][124/204]	Loss 0.5500 (0.5179)	
training:	Epoch: [3][125/204]	Loss 0.3392 (0.5164)	
training:	Epoch: [3][126/204]	Loss 0.5732 (0.5169)	
training:	Epoch: [3][127/204]	Loss 0.5056 (0.5168)	
training:	Epoch: [3][128/204]	Loss 0.5798 (0.5173)	
training:	Epoch: [3][129/204]	Loss 0.4842 (0.5170)	
training:	Epoch: [3][130/204]	Loss 0.4940 (0.5169)	
training:	Epoch: [3][131/204]	Loss 0.4418 (0.5163)	
training:	Epoch: [3][132/204]	Loss 0.3998 (0.5154)	
training:	Epoch: [3][133/204]	Loss 0.4531 (0.5149)	
training:	Epoch: [3][134/204]	Loss 0.4967 (0.5148)	
training:	Epoch: [3][135/204]	Loss 0.4879 (0.5146)	
training:	Epoch: [3][136/204]	Loss 0.3967 (0.5137)	
training:	Epoch: [3][137/204]	Loss 0.5012 (0.5136)	
training:	Epoch: [3][138/204]	Loss 0.6360 (0.5145)	
training:	Epoch: [3][139/204]	Loss 0.5188 (0.5146)	
training:	Epoch: [3][140/204]	Loss 0.4997 (0.5144)	
training:	Epoch: [3][141/204]	Loss 0.4704 (0.5141)	
training:	Epoch: [3][142/204]	Loss 0.4891 (0.5140)	
training:	Epoch: [3][143/204]	Loss 0.6124 (0.5146)	
training:	Epoch: [3][144/204]	Loss 0.4224 (0.5140)	
training:	Epoch: [3][145/204]	Loss 0.4673 (0.5137)	
training:	Epoch: [3][146/204]	Loss 0.4542 (0.5133)	
training:	Epoch: [3][147/204]	Loss 0.3308 (0.5120)	
training:	Epoch: [3][148/204]	Loss 0.4078 (0.5113)	
training:	Epoch: [3][149/204]	Loss 0.4210 (0.5107)	
training:	Epoch: [3][150/204]	Loss 0.6343 (0.5115)	
training:	Epoch: [3][151/204]	Loss 0.5288 (0.5117)	
training:	Epoch: [3][152/204]	Loss 0.4938 (0.5115)	
training:	Epoch: [3][153/204]	Loss 0.4131 (0.5109)	
training:	Epoch: [3][154/204]	Loss 0.4348 (0.5104)	
training:	Epoch: [3][155/204]	Loss 0.3846 (0.5096)	
training:	Epoch: [3][156/204]	Loss 0.3511 (0.5086)	
training:	Epoch: [3][157/204]	Loss 0.4578 (0.5083)	
training:	Epoch: [3][158/204]	Loss 0.4480 (0.5079)	
training:	Epoch: [3][159/204]	Loss 0.4661 (0.5076)	
training:	Epoch: [3][160/204]	Loss 0.3684 (0.5067)	
training:	Epoch: [3][161/204]	Loss 0.3588 (0.5058)	
training:	Epoch: [3][162/204]	Loss 0.4360 (0.5054)	
training:	Epoch: [3][163/204]	Loss 0.4971 (0.5053)	
training:	Epoch: [3][164/204]	Loss 0.5002 (0.5053)	
training:	Epoch: [3][165/204]	Loss 0.5550 (0.5056)	
training:	Epoch: [3][166/204]	Loss 0.4851 (0.5055)	
training:	Epoch: [3][167/204]	Loss 0.4187 (0.5050)	
training:	Epoch: [3][168/204]	Loss 0.3702 (0.5042)	
training:	Epoch: [3][169/204]	Loss 0.7189 (0.5054)	
training:	Epoch: [3][170/204]	Loss 0.5078 (0.5054)	
training:	Epoch: [3][171/204]	Loss 0.4675 (0.5052)	
training:	Epoch: [3][172/204]	Loss 0.5508 (0.5055)	
training:	Epoch: [3][173/204]	Loss 0.4080 (0.5049)	
training:	Epoch: [3][174/204]	Loss 0.3853 (0.5042)	
training:	Epoch: [3][175/204]	Loss 0.5480 (0.5045)	
training:	Epoch: [3][176/204]	Loss 0.5695 (0.5049)	
training:	Epoch: [3][177/204]	Loss 0.4361 (0.5045)	
training:	Epoch: [3][178/204]	Loss 0.4652 (0.5043)	
training:	Epoch: [3][179/204]	Loss 0.4320 (0.5038)	
training:	Epoch: [3][180/204]	Loss 0.4787 (0.5037)	
training:	Epoch: [3][181/204]	Loss 0.5039 (0.5037)	
training:	Epoch: [3][182/204]	Loss 0.3910 (0.5031)	
training:	Epoch: [3][183/204]	Loss 0.5413 (0.5033)	
training:	Epoch: [3][184/204]	Loss 0.5452 (0.5035)	
training:	Epoch: [3][185/204]	Loss 0.4017 (0.5030)	
training:	Epoch: [3][186/204]	Loss 0.4010 (0.5024)	
training:	Epoch: [3][187/204]	Loss 0.4594 (0.5022)	
training:	Epoch: [3][188/204]	Loss 0.4264 (0.5018)	
training:	Epoch: [3][189/204]	Loss 0.4458 (0.5015)	
training:	Epoch: [3][190/204]	Loss 0.4575 (0.5013)	
training:	Epoch: [3][191/204]	Loss 0.4321 (0.5009)	
training:	Epoch: [3][192/204]	Loss 0.6325 (0.5016)	
training:	Epoch: [3][193/204]	Loss 0.4356 (0.5012)	
training:	Epoch: [3][194/204]	Loss 0.5725 (0.5016)	
training:	Epoch: [3][195/204]	Loss 0.3592 (0.5009)	
training:	Epoch: [3][196/204]	Loss 0.3204 (0.5000)	
training:	Epoch: [3][197/204]	Loss 0.4241 (0.4996)	
training:	Epoch: [3][198/204]	Loss 0.5399 (0.4998)	
training:	Epoch: [3][199/204]	Loss 0.5508 (0.5000)	
training:	Epoch: [3][200/204]	Loss 0.5046 (0.5001)	
training:	Epoch: [3][201/204]	Loss 0.5276 (0.5002)	
training:	Epoch: [3][202/204]	Loss 0.4824 (0.5001)	
training:	Epoch: [3][203/204]	Loss 0.5262 (0.5002)	
training:	Epoch: [3][204/204]	Loss 0.5039 (0.5003)	
Training:	 Loss: 0.4995

Training:	 ACC: 0.8066 0.8079 0.8386 0.7746
Validation:	 ACC: 0.7844 0.7865 0.8301 0.7388
Validation:	 Best_BACC: 0.7844 0.7865 0.8301 0.7388
Validation:	 Loss: 0.4696
Pretraining:	Epoch 4/200
----------
training:	Epoch: [4][1/204]	Loss 0.3442 (0.3442)	
training:	Epoch: [4][2/204]	Loss 0.4951 (0.4196)	
training:	Epoch: [4][3/204]	Loss 0.5478 (0.4624)	
training:	Epoch: [4][4/204]	Loss 0.4819 (0.4672)	
training:	Epoch: [4][5/204]	Loss 0.3496 (0.4437)	
training:	Epoch: [4][6/204]	Loss 0.4906 (0.4515)	
training:	Epoch: [4][7/204]	Loss 0.3198 (0.4327)	
training:	Epoch: [4][8/204]	Loss 0.5176 (0.4433)	
training:	Epoch: [4][9/204]	Loss 0.4876 (0.4482)	
training:	Epoch: [4][10/204]	Loss 0.5209 (0.4555)	
training:	Epoch: [4][11/204]	Loss 0.6339 (0.4717)	
training:	Epoch: [4][12/204]	Loss 0.4416 (0.4692)	
training:	Epoch: [4][13/204]	Loss 0.3334 (0.4588)	
training:	Epoch: [4][14/204]	Loss 0.4843 (0.4606)	
training:	Epoch: [4][15/204]	Loss 0.4794 (0.4618)	
training:	Epoch: [4][16/204]	Loss 0.3858 (0.4571)	
training:	Epoch: [4][17/204]	Loss 0.3042 (0.4481)	
training:	Epoch: [4][18/204]	Loss 0.5562 (0.4541)	
training:	Epoch: [4][19/204]	Loss 0.4890 (0.4559)	
training:	Epoch: [4][20/204]	Loss 0.5987 (0.4631)	
training:	Epoch: [4][21/204]	Loss 0.4781 (0.4638)	
training:	Epoch: [4][22/204]	Loss 0.4018 (0.4610)	
training:	Epoch: [4][23/204]	Loss 0.4054 (0.4586)	
training:	Epoch: [4][24/204]	Loss 0.4814 (0.4595)	
training:	Epoch: [4][25/204]	Loss 0.3845 (0.4565)	
training:	Epoch: [4][26/204]	Loss 0.5595 (0.4605)	
training:	Epoch: [4][27/204]	Loss 0.4360 (0.4596)	
training:	Epoch: [4][28/204]	Loss 0.4201 (0.4582)	
training:	Epoch: [4][29/204]	Loss 0.4376 (0.4574)	
training:	Epoch: [4][30/204]	Loss 0.5091 (0.4592)	
training:	Epoch: [4][31/204]	Loss 0.4946 (0.4603)	
training:	Epoch: [4][32/204]	Loss 0.4098 (0.4587)	
training:	Epoch: [4][33/204]	Loss 0.4961 (0.4599)	
training:	Epoch: [4][34/204]	Loss 0.5652 (0.4630)	
training:	Epoch: [4][35/204]	Loss 0.4928 (0.4638)	
training:	Epoch: [4][36/204]	Loss 0.5842 (0.4672)	
training:	Epoch: [4][37/204]	Loss 0.5625 (0.4697)	
training:	Epoch: [4][38/204]	Loss 0.4932 (0.4704)	
training:	Epoch: [4][39/204]	Loss 0.5833 (0.4732)	
training:	Epoch: [4][40/204]	Loss 0.4723 (0.4732)	
training:	Epoch: [4][41/204]	Loss 0.4043 (0.4715)	
training:	Epoch: [4][42/204]	Loss 0.5358 (0.4731)	
training:	Epoch: [4][43/204]	Loss 0.3872 (0.4711)	
training:	Epoch: [4][44/204]	Loss 0.5261 (0.4723)	
training:	Epoch: [4][45/204]	Loss 0.5180 (0.4733)	
training:	Epoch: [4][46/204]	Loss 0.3363 (0.4704)	
training:	Epoch: [4][47/204]	Loss 0.4199 (0.4693)	
training:	Epoch: [4][48/204]	Loss 0.3437 (0.4667)	
training:	Epoch: [4][49/204]	Loss 0.4088 (0.4655)	
training:	Epoch: [4][50/204]	Loss 0.4223 (0.4646)	
training:	Epoch: [4][51/204]	Loss 0.5045 (0.4654)	
training:	Epoch: [4][52/204]	Loss 0.5455 (0.4669)	
training:	Epoch: [4][53/204]	Loss 0.3440 (0.4646)	
training:	Epoch: [4][54/204]	Loss 0.4686 (0.4647)	
training:	Epoch: [4][55/204]	Loss 0.4346 (0.4642)	
training:	Epoch: [4][56/204]	Loss 0.5237 (0.4652)	
training:	Epoch: [4][57/204]	Loss 0.4273 (0.4646)	
training:	Epoch: [4][58/204]	Loss 0.5410 (0.4659)	
training:	Epoch: [4][59/204]	Loss 0.3766 (0.4644)	
training:	Epoch: [4][60/204]	Loss 0.5129 (0.4652)	
training:	Epoch: [4][61/204]	Loss 0.3947 (0.4640)	
training:	Epoch: [4][62/204]	Loss 0.6311 (0.4667)	
training:	Epoch: [4][63/204]	Loss 0.4874 (0.4670)	
training:	Epoch: [4][64/204]	Loss 0.4159 (0.4662)	
training:	Epoch: [4][65/204]	Loss 0.4576 (0.4661)	
training:	Epoch: [4][66/204]	Loss 0.4802 (0.4663)	
training:	Epoch: [4][67/204]	Loss 0.4596 (0.4662)	
training:	Epoch: [4][68/204]	Loss 0.4150 (0.4655)	
training:	Epoch: [4][69/204]	Loss 0.3959 (0.4645)	
training:	Epoch: [4][70/204]	Loss 0.4397 (0.4641)	
training:	Epoch: [4][71/204]	Loss 0.4771 (0.4643)	
training:	Epoch: [4][72/204]	Loss 0.5284 (0.4652)	
training:	Epoch: [4][73/204]	Loss 0.4881 (0.4655)	
training:	Epoch: [4][74/204]	Loss 0.4806 (0.4657)	
training:	Epoch: [4][75/204]	Loss 0.4352 (0.4653)	
training:	Epoch: [4][76/204]	Loss 0.5756 (0.4667)	
training:	Epoch: [4][77/204]	Loss 0.4650 (0.4667)	
training:	Epoch: [4][78/204]	Loss 0.3044 (0.4646)	
training:	Epoch: [4][79/204]	Loss 0.4956 (0.4650)	
training:	Epoch: [4][80/204]	Loss 0.3432 (0.4635)	
training:	Epoch: [4][81/204]	Loss 0.4954 (0.4639)	
training:	Epoch: [4][82/204]	Loss 0.5754 (0.4653)	
training:	Epoch: [4][83/204]	Loss 0.4077 (0.4646)	
training:	Epoch: [4][84/204]	Loss 0.4157 (0.4640)	
training:	Epoch: [4][85/204]	Loss 0.3671 (0.4628)	
training:	Epoch: [4][86/204]	Loss 0.5178 (0.4635)	
training:	Epoch: [4][87/204]	Loss 0.4391 (0.4632)	
training:	Epoch: [4][88/204]	Loss 0.4126 (0.4626)	
training:	Epoch: [4][89/204]	Loss 0.5361 (0.4635)	
training:	Epoch: [4][90/204]	Loss 0.3939 (0.4627)	
training:	Epoch: [4][91/204]	Loss 0.3376 (0.4613)	
training:	Epoch: [4][92/204]	Loss 0.5180 (0.4619)	
training:	Epoch: [4][93/204]	Loss 0.3742 (0.4610)	
training:	Epoch: [4][94/204]	Loss 0.3692 (0.4600)	
training:	Epoch: [4][95/204]	Loss 0.5426 (0.4609)	
training:	Epoch: [4][96/204]	Loss 0.5412 (0.4617)	
training:	Epoch: [4][97/204]	Loss 0.3972 (0.4610)	
training:	Epoch: [4][98/204]	Loss 0.4602 (0.4610)	
training:	Epoch: [4][99/204]	Loss 0.4049 (0.4605)	
training:	Epoch: [4][100/204]	Loss 0.3708 (0.4596)	
training:	Epoch: [4][101/204]	Loss 0.3137 (0.4581)	
training:	Epoch: [4][102/204]	Loss 0.4583 (0.4581)	
training:	Epoch: [4][103/204]	Loss 0.3045 (0.4566)	
training:	Epoch: [4][104/204]	Loss 0.5083 (0.4571)	
training:	Epoch: [4][105/204]	Loss 0.3488 (0.4561)	
training:	Epoch: [4][106/204]	Loss 0.4320 (0.4559)	
training:	Epoch: [4][107/204]	Loss 0.5674 (0.4569)	
training:	Epoch: [4][108/204]	Loss 0.4922 (0.4572)	
training:	Epoch: [4][109/204]	Loss 0.4635 (0.4573)	
training:	Epoch: [4][110/204]	Loss 0.3962 (0.4567)	
training:	Epoch: [4][111/204]	Loss 0.4832 (0.4570)	
training:	Epoch: [4][112/204]	Loss 0.3309 (0.4559)	
training:	Epoch: [4][113/204]	Loss 0.4177 (0.4555)	
training:	Epoch: [4][114/204]	Loss 0.5005 (0.4559)	
training:	Epoch: [4][115/204]	Loss 0.4424 (0.4558)	
training:	Epoch: [4][116/204]	Loss 0.2257 (0.4538)	
training:	Epoch: [4][117/204]	Loss 0.4112 (0.4535)	
training:	Epoch: [4][118/204]	Loss 0.6245 (0.4549)	
training:	Epoch: [4][119/204]	Loss 0.7245 (0.4572)	
training:	Epoch: [4][120/204]	Loss 0.7079 (0.4593)	
training:	Epoch: [4][121/204]	Loss 0.3434 (0.4583)	
training:	Epoch: [4][122/204]	Loss 0.5706 (0.4592)	
training:	Epoch: [4][123/204]	Loss 0.4599 (0.4592)	
training:	Epoch: [4][124/204]	Loss 0.4154 (0.4589)	
training:	Epoch: [4][125/204]	Loss 0.3893 (0.4583)	
training:	Epoch: [4][126/204]	Loss 0.3211 (0.4572)	
training:	Epoch: [4][127/204]	Loss 0.6559 (0.4588)	
training:	Epoch: [4][128/204]	Loss 0.3719 (0.4581)	
training:	Epoch: [4][129/204]	Loss 0.5165 (0.4586)	
training:	Epoch: [4][130/204]	Loss 0.3742 (0.4579)	
training:	Epoch: [4][131/204]	Loss 0.4156 (0.4576)	
training:	Epoch: [4][132/204]	Loss 0.4776 (0.4577)	
training:	Epoch: [4][133/204]	Loss 0.4507 (0.4577)	
training:	Epoch: [4][134/204]	Loss 0.4628 (0.4577)	
training:	Epoch: [4][135/204]	Loss 0.4065 (0.4573)	
training:	Epoch: [4][136/204]	Loss 0.5349 (0.4579)	
training:	Epoch: [4][137/204]	Loss 0.4439 (0.4578)	
training:	Epoch: [4][138/204]	Loss 0.4007 (0.4574)	
training:	Epoch: [4][139/204]	Loss 0.5627 (0.4582)	
training:	Epoch: [4][140/204]	Loss 0.3983 (0.4577)	
training:	Epoch: [4][141/204]	Loss 0.2749 (0.4564)	
training:	Epoch: [4][142/204]	Loss 0.4741 (0.4566)	
training:	Epoch: [4][143/204]	Loss 0.3275 (0.4557)	
training:	Epoch: [4][144/204]	Loss 0.4865 (0.4559)	
training:	Epoch: [4][145/204]	Loss 0.4276 (0.4557)	
training:	Epoch: [4][146/204]	Loss 0.4889 (0.4559)	
training:	Epoch: [4][147/204]	Loss 0.4844 (0.4561)	
training:	Epoch: [4][148/204]	Loss 0.4139 (0.4558)	
training:	Epoch: [4][149/204]	Loss 0.4697 (0.4559)	
training:	Epoch: [4][150/204]	Loss 0.4182 (0.4557)	
training:	Epoch: [4][151/204]	Loss 0.4049 (0.4553)	
training:	Epoch: [4][152/204]	Loss 0.4685 (0.4554)	
training:	Epoch: [4][153/204]	Loss 0.4791 (0.4556)	
training:	Epoch: [4][154/204]	Loss 0.6121 (0.4566)	
training:	Epoch: [4][155/204]	Loss 0.3676 (0.4560)	
training:	Epoch: [4][156/204]	Loss 0.4766 (0.4561)	
training:	Epoch: [4][157/204]	Loss 0.4482 (0.4561)	
training:	Epoch: [4][158/204]	Loss 0.4192 (0.4559)	
training:	Epoch: [4][159/204]	Loss 0.2879 (0.4548)	
training:	Epoch: [4][160/204]	Loss 0.3043 (0.4539)	
training:	Epoch: [4][161/204]	Loss 0.3462 (0.4532)	
training:	Epoch: [4][162/204]	Loss 0.2990 (0.4522)	
training:	Epoch: [4][163/204]	Loss 0.4228 (0.4521)	
training:	Epoch: [4][164/204]	Loss 0.2921 (0.4511)	
training:	Epoch: [4][165/204]	Loss 0.2532 (0.4499)	
training:	Epoch: [4][166/204]	Loss 0.4917 (0.4501)	
training:	Epoch: [4][167/204]	Loss 0.4262 (0.4500)	
training:	Epoch: [4][168/204]	Loss 0.4008 (0.4497)	
training:	Epoch: [4][169/204]	Loss 0.4132 (0.4495)	
training:	Epoch: [4][170/204]	Loss 0.4571 (0.4495)	
training:	Epoch: [4][171/204]	Loss 0.4722 (0.4497)	
training:	Epoch: [4][172/204]	Loss 0.4190 (0.4495)	
training:	Epoch: [4][173/204]	Loss 0.1810 (0.4479)	
training:	Epoch: [4][174/204]	Loss 0.3666 (0.4475)	
training:	Epoch: [4][175/204]	Loss 0.4177 (0.4473)	
training:	Epoch: [4][176/204]	Loss 0.4455 (0.4473)	
training:	Epoch: [4][177/204]	Loss 0.3278 (0.4466)	
training:	Epoch: [4][178/204]	Loss 0.4238 (0.4465)	
training:	Epoch: [4][179/204]	Loss 0.3329 (0.4458)	
training:	Epoch: [4][180/204]	Loss 0.5777 (0.4466)	
training:	Epoch: [4][181/204]	Loss 0.4172 (0.4464)	
training:	Epoch: [4][182/204]	Loss 0.4951 (0.4467)	
training:	Epoch: [4][183/204]	Loss 0.4086 (0.4465)	
training:	Epoch: [4][184/204]	Loss 0.4670 (0.4466)	
training:	Epoch: [4][185/204]	Loss 0.4675 (0.4467)	
training:	Epoch: [4][186/204]	Loss 0.4212 (0.4466)	
training:	Epoch: [4][187/204]	Loss 0.3980 (0.4463)	
training:	Epoch: [4][188/204]	Loss 0.6323 (0.4473)	
training:	Epoch: [4][189/204]	Loss 0.2542 (0.4463)	
training:	Epoch: [4][190/204]	Loss 0.4919 (0.4465)	
training:	Epoch: [4][191/204]	Loss 0.4514 (0.4465)	
training:	Epoch: [4][192/204]	Loss 0.4019 (0.4463)	
training:	Epoch: [4][193/204]	Loss 0.4244 (0.4462)	
training:	Epoch: [4][194/204]	Loss 0.5364 (0.4467)	
training:	Epoch: [4][195/204]	Loss 0.4796 (0.4468)	
training:	Epoch: [4][196/204]	Loss 0.5152 (0.4472)	
training:	Epoch: [4][197/204]	Loss 0.3227 (0.4465)	
training:	Epoch: [4][198/204]	Loss 0.5387 (0.4470)	
training:	Epoch: [4][199/204]	Loss 0.5209 (0.4474)	
training:	Epoch: [4][200/204]	Loss 0.4355 (0.4473)	
training:	Epoch: [4][201/204]	Loss 0.2810 (0.4465)	
training:	Epoch: [4][202/204]	Loss 0.3057 (0.4458)	
training:	Epoch: [4][203/204]	Loss 0.3907 (0.4455)	
training:	Epoch: [4][204/204]	Loss 0.4905 (0.4457)	
Training:	 Loss: 0.4451

Training:	 ACC: 0.8358 0.8370 0.8633 0.8084
Validation:	 ACC: 0.8048 0.8063 0.8383 0.7713
Validation:	 Best_BACC: 0.8048 0.8063 0.8383 0.7713
Validation:	 Loss: 0.4386
Pretraining:	Epoch 5/200
----------
training:	Epoch: [5][1/204]	Loss 0.4450 (0.4450)	
training:	Epoch: [5][2/204]	Loss 0.5942 (0.5196)	
training:	Epoch: [5][3/204]	Loss 0.4089 (0.4827)	
training:	Epoch: [5][4/204]	Loss 0.4061 (0.4635)	
training:	Epoch: [5][5/204]	Loss 0.4509 (0.4610)	
training:	Epoch: [5][6/204]	Loss 0.3900 (0.4492)	
training:	Epoch: [5][7/204]	Loss 0.4200 (0.4450)	
training:	Epoch: [5][8/204]	Loss 0.3069 (0.4277)	
training:	Epoch: [5][9/204]	Loss 0.4235 (0.4273)	
training:	Epoch: [5][10/204]	Loss 0.4046 (0.4250)	
training:	Epoch: [5][11/204]	Loss 0.5618 (0.4374)	
training:	Epoch: [5][12/204]	Loss 0.4606 (0.4394)	
training:	Epoch: [5][13/204]	Loss 0.4347 (0.4390)	
training:	Epoch: [5][14/204]	Loss 0.2466 (0.4253)	
training:	Epoch: [5][15/204]	Loss 0.5023 (0.4304)	
training:	Epoch: [5][16/204]	Loss 0.3304 (0.4241)	
training:	Epoch: [5][17/204]	Loss 0.5355 (0.4307)	
training:	Epoch: [5][18/204]	Loss 0.4492 (0.4317)	
training:	Epoch: [5][19/204]	Loss 0.2991 (0.4247)	
training:	Epoch: [5][20/204]	Loss 0.3594 (0.4215)	
training:	Epoch: [5][21/204]	Loss 0.2606 (0.4138)	
training:	Epoch: [5][22/204]	Loss 0.5330 (0.4192)	
training:	Epoch: [5][23/204]	Loss 0.4912 (0.4224)	
training:	Epoch: [5][24/204]	Loss 0.6623 (0.4324)	
training:	Epoch: [5][25/204]	Loss 0.5490 (0.4370)	
training:	Epoch: [5][26/204]	Loss 0.4057 (0.4358)	
training:	Epoch: [5][27/204]	Loss 0.2592 (0.4293)	
training:	Epoch: [5][28/204]	Loss 0.4353 (0.4295)	
training:	Epoch: [5][29/204]	Loss 0.3510 (0.4268)	
training:	Epoch: [5][30/204]	Loss 0.4365 (0.4271)	
training:	Epoch: [5][31/204]	Loss 0.3561 (0.4248)	
training:	Epoch: [5][32/204]	Loss 0.2350 (0.4189)	
training:	Epoch: [5][33/204]	Loss 0.3142 (0.4157)	
training:	Epoch: [5][34/204]	Loss 0.2808 (0.4117)	
training:	Epoch: [5][35/204]	Loss 0.5023 (0.4143)	
training:	Epoch: [5][36/204]	Loss 0.3430 (0.4123)	
training:	Epoch: [5][37/204]	Loss 0.4494 (0.4133)	
training:	Epoch: [5][38/204]	Loss 0.5430 (0.4168)	
training:	Epoch: [5][39/204]	Loss 0.4209 (0.4169)	
training:	Epoch: [5][40/204]	Loss 0.2670 (0.4131)	
training:	Epoch: [5][41/204]	Loss 0.5530 (0.4165)	
training:	Epoch: [5][42/204]	Loss 0.3348 (0.4146)	
training:	Epoch: [5][43/204]	Loss 0.4595 (0.4156)	
training:	Epoch: [5][44/204]	Loss 0.3726 (0.4147)	
training:	Epoch: [5][45/204]	Loss 0.4570 (0.4156)	
training:	Epoch: [5][46/204]	Loss 0.4807 (0.4170)	
training:	Epoch: [5][47/204]	Loss 0.3727 (0.4161)	
training:	Epoch: [5][48/204]	Loss 0.3884 (0.4155)	
training:	Epoch: [5][49/204]	Loss 0.2897 (0.4129)	
training:	Epoch: [5][50/204]	Loss 0.5423 (0.4155)	
training:	Epoch: [5][51/204]	Loss 0.4614 (0.4164)	
training:	Epoch: [5][52/204]	Loss 0.5194 (0.4184)	
training:	Epoch: [5][53/204]	Loss 0.3958 (0.4180)	
training:	Epoch: [5][54/204]	Loss 0.5116 (0.4197)	
training:	Epoch: [5][55/204]	Loss 0.4989 (0.4211)	
training:	Epoch: [5][56/204]	Loss 0.2562 (0.4182)	
training:	Epoch: [5][57/204]	Loss 0.2631 (0.4155)	
training:	Epoch: [5][58/204]	Loss 0.3987 (0.4152)	
training:	Epoch: [5][59/204]	Loss 0.5082 (0.4168)	
training:	Epoch: [5][60/204]	Loss 0.3916 (0.4163)	
training:	Epoch: [5][61/204]	Loss 0.3767 (0.4157)	
training:	Epoch: [5][62/204]	Loss 0.4179 (0.4157)	
training:	Epoch: [5][63/204]	Loss 0.3987 (0.4155)	
training:	Epoch: [5][64/204]	Loss 0.4723 (0.4163)	
training:	Epoch: [5][65/204]	Loss 0.3861 (0.4159)	
training:	Epoch: [5][66/204]	Loss 0.3597 (0.4150)	
training:	Epoch: [5][67/204]	Loss 0.3487 (0.4140)	
training:	Epoch: [5][68/204]	Loss 0.3515 (0.4131)	
training:	Epoch: [5][69/204]	Loss 0.3755 (0.4126)	
training:	Epoch: [5][70/204]	Loss 0.2360 (0.4100)	
training:	Epoch: [5][71/204]	Loss 0.4215 (0.4102)	
training:	Epoch: [5][72/204]	Loss 0.3618 (0.4095)	
training:	Epoch: [5][73/204]	Loss 0.3570 (0.4088)	
training:	Epoch: [5][74/204]	Loss 0.3355 (0.4078)	
training:	Epoch: [5][75/204]	Loss 0.7381 (0.4122)	
training:	Epoch: [5][76/204]	Loss 0.4572 (0.4128)	
training:	Epoch: [5][77/204]	Loss 0.3000 (0.4114)	
training:	Epoch: [5][78/204]	Loss 0.3278 (0.4103)	
training:	Epoch: [5][79/204]	Loss 0.3764 (0.4099)	
training:	Epoch: [5][80/204]	Loss 0.3396 (0.4090)	
training:	Epoch: [5][81/204]	Loss 0.2755 (0.4073)	
training:	Epoch: [5][82/204]	Loss 0.3693 (0.4069)	
training:	Epoch: [5][83/204]	Loss 0.3435 (0.4061)	
training:	Epoch: [5][84/204]	Loss 0.1970 (0.4036)	
training:	Epoch: [5][85/204]	Loss 0.4413 (0.4041)	
training:	Epoch: [5][86/204]	Loss 0.4970 (0.4051)	
training:	Epoch: [5][87/204]	Loss 0.6273 (0.4077)	
training:	Epoch: [5][88/204]	Loss 0.3325 (0.4068)	
training:	Epoch: [5][89/204]	Loss 0.4987 (0.4079)	
training:	Epoch: [5][90/204]	Loss 0.6246 (0.4103)	
training:	Epoch: [5][91/204]	Loss 0.3720 (0.4099)	
training:	Epoch: [5][92/204]	Loss 0.2358 (0.4080)	
training:	Epoch: [5][93/204]	Loss 0.3434 (0.4073)	
training:	Epoch: [5][94/204]	Loss 0.3747 (0.4069)	
training:	Epoch: [5][95/204]	Loss 0.2572 (0.4053)	
training:	Epoch: [5][96/204]	Loss 0.3527 (0.4048)	
training:	Epoch: [5][97/204]	Loss 0.2458 (0.4032)	
training:	Epoch: [5][98/204]	Loss 0.4556 (0.4037)	
training:	Epoch: [5][99/204]	Loss 0.4202 (0.4039)	
training:	Epoch: [5][100/204]	Loss 0.3911 (0.4037)	
training:	Epoch: [5][101/204]	Loss 0.4431 (0.4041)	
training:	Epoch: [5][102/204]	Loss 0.2877 (0.4030)	
training:	Epoch: [5][103/204]	Loss 0.3137 (0.4021)	
training:	Epoch: [5][104/204]	Loss 0.3595 (0.4017)	
training:	Epoch: [5][105/204]	Loss 0.3644 (0.4014)	
training:	Epoch: [5][106/204]	Loss 0.3150 (0.4005)	
training:	Epoch: [5][107/204]	Loss 0.4152 (0.4007)	
training:	Epoch: [5][108/204]	Loss 0.4422 (0.4011)	
training:	Epoch: [5][109/204]	Loss 0.4248 (0.4013)	
training:	Epoch: [5][110/204]	Loss 0.5568 (0.4027)	
training:	Epoch: [5][111/204]	Loss 0.5462 (0.4040)	
training:	Epoch: [5][112/204]	Loss 0.4208 (0.4041)	
training:	Epoch: [5][113/204]	Loss 0.4533 (0.4046)	
training:	Epoch: [5][114/204]	Loss 0.2901 (0.4036)	
training:	Epoch: [5][115/204]	Loss 0.3152 (0.4028)	
training:	Epoch: [5][116/204]	Loss 0.4871 (0.4035)	
training:	Epoch: [5][117/204]	Loss 0.3319 (0.4029)	
training:	Epoch: [5][118/204]	Loss 0.4348 (0.4032)	
training:	Epoch: [5][119/204]	Loss 0.3047 (0.4024)	
training:	Epoch: [5][120/204]	Loss 0.4203 (0.4025)	
training:	Epoch: [5][121/204]	Loss 0.3470 (0.4020)	
training:	Epoch: [5][122/204]	Loss 0.4789 (0.4027)	
training:	Epoch: [5][123/204]	Loss 0.3394 (0.4022)	
training:	Epoch: [5][124/204]	Loss 0.2825 (0.4012)	
training:	Epoch: [5][125/204]	Loss 0.5258 (0.4022)	
training:	Epoch: [5][126/204]	Loss 0.3924 (0.4021)	
training:	Epoch: [5][127/204]	Loss 0.5421 (0.4032)	
training:	Epoch: [5][128/204]	Loss 0.4040 (0.4032)	
training:	Epoch: [5][129/204]	Loss 0.5607 (0.4044)	
training:	Epoch: [5][130/204]	Loss 0.5147 (0.4053)	
training:	Epoch: [5][131/204]	Loss 0.3466 (0.4048)	
training:	Epoch: [5][132/204]	Loss 0.4112 (0.4049)	
training:	Epoch: [5][133/204]	Loss 0.4485 (0.4052)	
training:	Epoch: [5][134/204]	Loss 0.4899 (0.4059)	
training:	Epoch: [5][135/204]	Loss 0.6292 (0.4075)	
training:	Epoch: [5][136/204]	Loss 0.3861 (0.4074)	
training:	Epoch: [5][137/204]	Loss 0.4964 (0.4080)	
training:	Epoch: [5][138/204]	Loss 0.3567 (0.4076)	
training:	Epoch: [5][139/204]	Loss 0.4034 (0.4076)	
training:	Epoch: [5][140/204]	Loss 0.3802 (0.4074)	
training:	Epoch: [5][141/204]	Loss 0.3940 (0.4073)	
training:	Epoch: [5][142/204]	Loss 0.4158 (0.4074)	
training:	Epoch: [5][143/204]	Loss 0.3662 (0.4071)	
training:	Epoch: [5][144/204]	Loss 0.4473 (0.4074)	
training:	Epoch: [5][145/204]	Loss 0.1901 (0.4059)	
training:	Epoch: [5][146/204]	Loss 0.3609 (0.4056)	
training:	Epoch: [5][147/204]	Loss 0.4843 (0.4061)	
training:	Epoch: [5][148/204]	Loss 0.4264 (0.4062)	
training:	Epoch: [5][149/204]	Loss 0.5205 (0.4070)	
training:	Epoch: [5][150/204]	Loss 0.4474 (0.4073)	
training:	Epoch: [5][151/204]	Loss 0.3615 (0.4070)	
training:	Epoch: [5][152/204]	Loss 0.3233 (0.4064)	
training:	Epoch: [5][153/204]	Loss 0.6210 (0.4078)	
training:	Epoch: [5][154/204]	Loss 0.3498 (0.4074)	
training:	Epoch: [5][155/204]	Loss 0.6001 (0.4087)	
training:	Epoch: [5][156/204]	Loss 0.2824 (0.4079)	
training:	Epoch: [5][157/204]	Loss 0.4032 (0.4078)	
training:	Epoch: [5][158/204]	Loss 0.3196 (0.4073)	
training:	Epoch: [5][159/204]	Loss 0.4152 (0.4073)	
training:	Epoch: [5][160/204]	Loss 0.5010 (0.4079)	
training:	Epoch: [5][161/204]	Loss 0.3185 (0.4074)	
training:	Epoch: [5][162/204]	Loss 0.3563 (0.4070)	
training:	Epoch: [5][163/204]	Loss 0.4469 (0.4073)	
training:	Epoch: [5][164/204]	Loss 0.2366 (0.4062)	
training:	Epoch: [5][165/204]	Loss 0.6535 (0.4077)	
training:	Epoch: [5][166/204]	Loss 0.3000 (0.4071)	
training:	Epoch: [5][167/204]	Loss 0.4324 (0.4072)	
training:	Epoch: [5][168/204]	Loss 0.3231 (0.4067)	
training:	Epoch: [5][169/204]	Loss 0.3762 (0.4066)	
training:	Epoch: [5][170/204]	Loss 0.3615 (0.4063)	
training:	Epoch: [5][171/204]	Loss 0.2823 (0.4056)	
training:	Epoch: [5][172/204]	Loss 0.3850 (0.4055)	
training:	Epoch: [5][173/204]	Loss 0.3999 (0.4054)	
training:	Epoch: [5][174/204]	Loss 0.3321 (0.4050)	
training:	Epoch: [5][175/204]	Loss 0.3411 (0.4046)	
training:	Epoch: [5][176/204]	Loss 0.6321 (0.4059)	
training:	Epoch: [5][177/204]	Loss 0.5729 (0.4069)	
training:	Epoch: [5][178/204]	Loss 0.4159 (0.4069)	
training:	Epoch: [5][179/204]	Loss 0.3299 (0.4065)	
training:	Epoch: [5][180/204]	Loss 0.4378 (0.4067)	
training:	Epoch: [5][181/204]	Loss 0.4899 (0.4071)	
training:	Epoch: [5][182/204]	Loss 0.4298 (0.4073)	
training:	Epoch: [5][183/204]	Loss 0.4013 (0.4072)	
training:	Epoch: [5][184/204]	Loss 0.5777 (0.4081)	
training:	Epoch: [5][185/204]	Loss 0.4811 (0.4085)	
training:	Epoch: [5][186/204]	Loss 0.3035 (0.4080)	
training:	Epoch: [5][187/204]	Loss 0.4600 (0.4083)	
training:	Epoch: [5][188/204]	Loss 0.3588 (0.4080)	
training:	Epoch: [5][189/204]	Loss 0.2732 (0.4073)	
training:	Epoch: [5][190/204]	Loss 0.3281 (0.4069)	
training:	Epoch: [5][191/204]	Loss 0.4231 (0.4069)	
training:	Epoch: [5][192/204]	Loss 0.4142 (0.4070)	
training:	Epoch: [5][193/204]	Loss 0.2520 (0.4062)	
training:	Epoch: [5][194/204]	Loss 0.4362 (0.4063)	
training:	Epoch: [5][195/204]	Loss 0.3795 (0.4062)	
training:	Epoch: [5][196/204]	Loss 0.3081 (0.4057)	
training:	Epoch: [5][197/204]	Loss 0.3248 (0.4053)	
training:	Epoch: [5][198/204]	Loss 0.4116 (0.4053)	
training:	Epoch: [5][199/204]	Loss 0.2828 (0.4047)	
training:	Epoch: [5][200/204]	Loss 0.3789 (0.4046)	
training:	Epoch: [5][201/204]	Loss 0.5004 (0.4051)	
training:	Epoch: [5][202/204]	Loss 0.3426 (0.4047)	
training:	Epoch: [5][203/204]	Loss 0.3023 (0.4042)	
training:	Epoch: [5][204/204]	Loss 0.4064 (0.4042)	
Training:	 Loss: 0.4036

Training:	 ACC: 0.8532 0.8532 0.8521 0.8543
Validation:	 ACC: 0.8201 0.8202 0.8229 0.8173
Validation:	 Best_BACC: 0.8201 0.8202 0.8229 0.8173
Validation:	 Loss: 0.4222
Pretraining:	Epoch 6/200
----------
training:	Epoch: [6][1/204]	Loss 0.3881 (0.3881)	
training:	Epoch: [6][2/204]	Loss 0.5981 (0.4931)	
training:	Epoch: [6][3/204]	Loss 0.4925 (0.4929)	
training:	Epoch: [6][4/204]	Loss 0.4072 (0.4715)	
training:	Epoch: [6][5/204]	Loss 0.3383 (0.4449)	
training:	Epoch: [6][6/204]	Loss 0.3864 (0.4351)	
training:	Epoch: [6][7/204]	Loss 0.3325 (0.4205)	
training:	Epoch: [6][8/204]	Loss 0.2018 (0.3931)	
training:	Epoch: [6][9/204]	Loss 0.3896 (0.3927)	
training:	Epoch: [6][10/204]	Loss 0.2767 (0.3811)	
training:	Epoch: [6][11/204]	Loss 0.6171 (0.4026)	
training:	Epoch: [6][12/204]	Loss 0.4160 (0.4037)	
training:	Epoch: [6][13/204]	Loss 0.4265 (0.4055)	
training:	Epoch: [6][14/204]	Loss 0.2972 (0.3977)	
training:	Epoch: [6][15/204]	Loss 0.4728 (0.4027)	
training:	Epoch: [6][16/204]	Loss 0.3100 (0.3969)	
training:	Epoch: [6][17/204]	Loss 0.3743 (0.3956)	
training:	Epoch: [6][18/204]	Loss 0.4442 (0.3983)	
training:	Epoch: [6][19/204]	Loss 0.3575 (0.3961)	
training:	Epoch: [6][20/204]	Loss 0.4576 (0.3992)	
training:	Epoch: [6][21/204]	Loss 0.2291 (0.3911)	
training:	Epoch: [6][22/204]	Loss 0.4853 (0.3954)	
training:	Epoch: [6][23/204]	Loss 0.3695 (0.3943)	
training:	Epoch: [6][24/204]	Loss 0.3527 (0.3925)	
training:	Epoch: [6][25/204]	Loss 0.3529 (0.3910)	
training:	Epoch: [6][26/204]	Loss 0.3385 (0.3889)	
training:	Epoch: [6][27/204]	Loss 0.3945 (0.3891)	
training:	Epoch: [6][28/204]	Loss 0.4906 (0.3928)	
training:	Epoch: [6][29/204]	Loss 0.3253 (0.3904)	
training:	Epoch: [6][30/204]	Loss 0.3536 (0.3892)	
training:	Epoch: [6][31/204]	Loss 0.3314 (0.3873)	
training:	Epoch: [6][32/204]	Loss 0.2364 (0.3826)	
training:	Epoch: [6][33/204]	Loss 0.4634 (0.3851)	
training:	Epoch: [6][34/204]	Loss 0.4586 (0.3872)	
training:	Epoch: [6][35/204]	Loss 0.5199 (0.3910)	
training:	Epoch: [6][36/204]	Loss 0.3854 (0.3909)	
training:	Epoch: [6][37/204]	Loss 0.5254 (0.3945)	
training:	Epoch: [6][38/204]	Loss 0.5430 (0.3984)	
training:	Epoch: [6][39/204]	Loss 0.4509 (0.3998)	
training:	Epoch: [6][40/204]	Loss 0.4491 (0.4010)	
training:	Epoch: [6][41/204]	Loss 0.4033 (0.4011)	
training:	Epoch: [6][42/204]	Loss 0.2378 (0.3972)	
training:	Epoch: [6][43/204]	Loss 0.2438 (0.3936)	
training:	Epoch: [6][44/204]	Loss 0.2895 (0.3912)	
training:	Epoch: [6][45/204]	Loss 0.3138 (0.3895)	
training:	Epoch: [6][46/204]	Loss 0.2899 (0.3873)	
training:	Epoch: [6][47/204]	Loss 0.3916 (0.3874)	
training:	Epoch: [6][48/204]	Loss 0.4385 (0.3885)	
training:	Epoch: [6][49/204]	Loss 0.3699 (0.3881)	
training:	Epoch: [6][50/204]	Loss 0.3552 (0.3875)	
training:	Epoch: [6][51/204]	Loss 0.2966 (0.3857)	
training:	Epoch: [6][52/204]	Loss 0.3998 (0.3860)	
training:	Epoch: [6][53/204]	Loss 0.5012 (0.3881)	
training:	Epoch: [6][54/204]	Loss 0.3162 (0.3868)	
training:	Epoch: [6][55/204]	Loss 0.4026 (0.3871)	
training:	Epoch: [6][56/204]	Loss 0.4136 (0.3876)	
training:	Epoch: [6][57/204]	Loss 0.3007 (0.3860)	
training:	Epoch: [6][58/204]	Loss 0.4475 (0.3871)	
training:	Epoch: [6][59/204]	Loss 0.3230 (0.3860)	
training:	Epoch: [6][60/204]	Loss 0.4243 (0.3866)	
training:	Epoch: [6][61/204]	Loss 0.2917 (0.3851)	
training:	Epoch: [6][62/204]	Loss 0.3801 (0.3850)	
training:	Epoch: [6][63/204]	Loss 0.4300 (0.3857)	
training:	Epoch: [6][64/204]	Loss 0.3327 (0.3849)	
training:	Epoch: [6][65/204]	Loss 0.4168 (0.3854)	
training:	Epoch: [6][66/204]	Loss 0.2523 (0.3834)	
training:	Epoch: [6][67/204]	Loss 0.4123 (0.3838)	
training:	Epoch: [6][68/204]	Loss 0.5499 (0.3862)	
training:	Epoch: [6][69/204]	Loss 0.2247 (0.3839)	
training:	Epoch: [6][70/204]	Loss 0.4148 (0.3843)	
training:	Epoch: [6][71/204]	Loss 0.2310 (0.3822)	
training:	Epoch: [6][72/204]	Loss 0.3576 (0.3818)	
training:	Epoch: [6][73/204]	Loss 0.3910 (0.3820)	
training:	Epoch: [6][74/204]	Loss 0.4387 (0.3827)	
training:	Epoch: [6][75/204]	Loss 0.2170 (0.3805)	
training:	Epoch: [6][76/204]	Loss 0.4606 (0.3816)	
training:	Epoch: [6][77/204]	Loss 0.3325 (0.3809)	
training:	Epoch: [6][78/204]	Loss 0.3190 (0.3801)	
training:	Epoch: [6][79/204]	Loss 0.6594 (0.3837)	
training:	Epoch: [6][80/204]	Loss 0.2982 (0.3826)	
training:	Epoch: [6][81/204]	Loss 0.3874 (0.3827)	
training:	Epoch: [6][82/204]	Loss 0.3886 (0.3827)	
training:	Epoch: [6][83/204]	Loss 0.3207 (0.3820)	
training:	Epoch: [6][84/204]	Loss 0.3614 (0.3818)	
training:	Epoch: [6][85/204]	Loss 0.3506 (0.3814)	
training:	Epoch: [6][86/204]	Loss 0.4240 (0.3819)	
training:	Epoch: [6][87/204]	Loss 0.4078 (0.3822)	
training:	Epoch: [6][88/204]	Loss 0.2722 (0.3809)	
training:	Epoch: [6][89/204]	Loss 0.3611 (0.3807)	
training:	Epoch: [6][90/204]	Loss 0.3468 (0.3803)	
training:	Epoch: [6][91/204]	Loss 0.5542 (0.3822)	
training:	Epoch: [6][92/204]	Loss 0.3633 (0.3820)	
training:	Epoch: [6][93/204]	Loss 0.3676 (0.3819)	
training:	Epoch: [6][94/204]	Loss 0.2272 (0.3802)	
training:	Epoch: [6][95/204]	Loss 0.4880 (0.3814)	
training:	Epoch: [6][96/204]	Loss 0.4849 (0.3824)	
training:	Epoch: [6][97/204]	Loss 0.3347 (0.3820)	
training:	Epoch: [6][98/204]	Loss 0.5602 (0.3838)	
training:	Epoch: [6][99/204]	Loss 0.2743 (0.3827)	
training:	Epoch: [6][100/204]	Loss 0.3087 (0.3819)	
training:	Epoch: [6][101/204]	Loss 0.2724 (0.3808)	
training:	Epoch: [6][102/204]	Loss 0.2453 (0.3795)	
training:	Epoch: [6][103/204]	Loss 0.4926 (0.3806)	
training:	Epoch: [6][104/204]	Loss 0.3226 (0.3801)	
training:	Epoch: [6][105/204]	Loss 0.4170 (0.3804)	
training:	Epoch: [6][106/204]	Loss 0.3689 (0.3803)	
training:	Epoch: [6][107/204]	Loss 0.3449 (0.3800)	
training:	Epoch: [6][108/204]	Loss 0.2936 (0.3792)	
training:	Epoch: [6][109/204]	Loss 0.2916 (0.3784)	
training:	Epoch: [6][110/204]	Loss 0.3790 (0.3784)	
training:	Epoch: [6][111/204]	Loss 0.4017 (0.3786)	
training:	Epoch: [6][112/204]	Loss 0.3959 (0.3787)	
training:	Epoch: [6][113/204]	Loss 0.2935 (0.3780)	
training:	Epoch: [6][114/204]	Loss 0.3109 (0.3774)	
training:	Epoch: [6][115/204]	Loss 0.3908 (0.3775)	
training:	Epoch: [6][116/204]	Loss 0.2822 (0.3767)	
training:	Epoch: [6][117/204]	Loss 0.2939 (0.3760)	
training:	Epoch: [6][118/204]	Loss 0.3451 (0.3757)	
training:	Epoch: [6][119/204]	Loss 0.3019 (0.3751)	
training:	Epoch: [6][120/204]	Loss 0.4728 (0.3759)	
training:	Epoch: [6][121/204]	Loss 0.4209 (0.3763)	
training:	Epoch: [6][122/204]	Loss 0.6029 (0.3781)	
training:	Epoch: [6][123/204]	Loss 0.4728 (0.3789)	
training:	Epoch: [6][124/204]	Loss 0.3015 (0.3783)	
training:	Epoch: [6][125/204]	Loss 0.3719 (0.3782)	
training:	Epoch: [6][126/204]	Loss 0.4076 (0.3785)	
training:	Epoch: [6][127/204]	Loss 0.3342 (0.3781)	
training:	Epoch: [6][128/204]	Loss 0.4498 (0.3787)	
training:	Epoch: [6][129/204]	Loss 0.3167 (0.3782)	
training:	Epoch: [6][130/204]	Loss 0.4149 (0.3785)	
training:	Epoch: [6][131/204]	Loss 0.4147 (0.3788)	
training:	Epoch: [6][132/204]	Loss 0.1638 (0.3771)	
training:	Epoch: [6][133/204]	Loss 0.3098 (0.3766)	
training:	Epoch: [6][134/204]	Loss 0.4241 (0.3770)	
training:	Epoch: [6][135/204]	Loss 0.4723 (0.3777)	
training:	Epoch: [6][136/204]	Loss 0.2284 (0.3766)	
training:	Epoch: [6][137/204]	Loss 0.3058 (0.3761)	
training:	Epoch: [6][138/204]	Loss 0.3999 (0.3762)	
training:	Epoch: [6][139/204]	Loss 0.4813 (0.3770)	
training:	Epoch: [6][140/204]	Loss 0.3198 (0.3766)	
training:	Epoch: [6][141/204]	Loss 0.4735 (0.3773)	
training:	Epoch: [6][142/204]	Loss 0.4623 (0.3779)	
training:	Epoch: [6][143/204]	Loss 0.4778 (0.3786)	
training:	Epoch: [6][144/204]	Loss 0.5071 (0.3795)	
training:	Epoch: [6][145/204]	Loss 0.3472 (0.3792)	
training:	Epoch: [6][146/204]	Loss 0.2309 (0.3782)	
training:	Epoch: [6][147/204]	Loss 0.5371 (0.3793)	
training:	Epoch: [6][148/204]	Loss 0.5884 (0.3807)	
training:	Epoch: [6][149/204]	Loss 0.2559 (0.3799)	
training:	Epoch: [6][150/204]	Loss 0.3585 (0.3797)	
training:	Epoch: [6][151/204]	Loss 0.3112 (0.3793)	
training:	Epoch: [6][152/204]	Loss 0.4354 (0.3797)	
training:	Epoch: [6][153/204]	Loss 0.4500 (0.3801)	
training:	Epoch: [6][154/204]	Loss 0.4487 (0.3806)	
training:	Epoch: [6][155/204]	Loss 0.4367 (0.3809)	
training:	Epoch: [6][156/204]	Loss 0.4185 (0.3812)	
training:	Epoch: [6][157/204]	Loss 0.4650 (0.3817)	
training:	Epoch: [6][158/204]	Loss 0.2561 (0.3809)	
training:	Epoch: [6][159/204]	Loss 0.4725 (0.3815)	
training:	Epoch: [6][160/204]	Loss 0.2144 (0.3804)	
training:	Epoch: [6][161/204]	Loss 0.6031 (0.3818)	
training:	Epoch: [6][162/204]	Loss 0.2343 (0.3809)	
training:	Epoch: [6][163/204]	Loss 0.6172 (0.3824)	
training:	Epoch: [6][164/204]	Loss 0.2729 (0.3817)	
training:	Epoch: [6][165/204]	Loss 0.5690 (0.3828)	
training:	Epoch: [6][166/204]	Loss 0.2785 (0.3822)	
training:	Epoch: [6][167/204]	Loss 0.2870 (0.3816)	
training:	Epoch: [6][168/204]	Loss 0.3778 (0.3816)	
training:	Epoch: [6][169/204]	Loss 0.3691 (0.3815)	
training:	Epoch: [6][170/204]	Loss 0.2528 (0.3808)	
training:	Epoch: [6][171/204]	Loss 0.4925 (0.3814)	
training:	Epoch: [6][172/204]	Loss 0.2851 (0.3809)	
training:	Epoch: [6][173/204]	Loss 0.3956 (0.3810)	
training:	Epoch: [6][174/204]	Loss 0.4666 (0.3814)	
training:	Epoch: [6][175/204]	Loss 0.4338 (0.3817)	
training:	Epoch: [6][176/204]	Loss 0.4319 (0.3820)	
training:	Epoch: [6][177/204]	Loss 0.3276 (0.3817)	
training:	Epoch: [6][178/204]	Loss 0.2643 (0.3811)	
training:	Epoch: [6][179/204]	Loss 0.4817 (0.3816)	
training:	Epoch: [6][180/204]	Loss 0.4343 (0.3819)	
training:	Epoch: [6][181/204]	Loss 0.1639 (0.3807)	
training:	Epoch: [6][182/204]	Loss 0.3174 (0.3804)	
training:	Epoch: [6][183/204]	Loss 0.4955 (0.3810)	
training:	Epoch: [6][184/204]	Loss 0.3786 (0.3810)	
training:	Epoch: [6][185/204]	Loss 0.5054 (0.3817)	
training:	Epoch: [6][186/204]	Loss 0.2604 (0.3810)	
training:	Epoch: [6][187/204]	Loss 0.3454 (0.3808)	
training:	Epoch: [6][188/204]	Loss 0.3326 (0.3806)	
training:	Epoch: [6][189/204]	Loss 0.2686 (0.3800)	
training:	Epoch: [6][190/204]	Loss 0.2948 (0.3795)	
training:	Epoch: [6][191/204]	Loss 0.2955 (0.3791)	
training:	Epoch: [6][192/204]	Loss 0.4145 (0.3793)	
training:	Epoch: [6][193/204]	Loss 0.4201 (0.3795)	
training:	Epoch: [6][194/204]	Loss 0.2586 (0.3788)	
training:	Epoch: [6][195/204]	Loss 0.3214 (0.3786)	
training:	Epoch: [6][196/204]	Loss 0.2697 (0.3780)	
training:	Epoch: [6][197/204]	Loss 0.2505 (0.3773)	
training:	Epoch: [6][198/204]	Loss 0.5441 (0.3782)	
training:	Epoch: [6][199/204]	Loss 0.3695 (0.3781)	
training:	Epoch: [6][200/204]	Loss 0.3825 (0.3782)	
training:	Epoch: [6][201/204]	Loss 0.4077 (0.3783)	
training:	Epoch: [6][202/204]	Loss 0.2422 (0.3776)	
training:	Epoch: [6][203/204]	Loss 0.2308 (0.3769)	
training:	Epoch: [6][204/204]	Loss 0.5238 (0.3776)	
Training:	 Loss: 0.3771

Training:	 ACC: 0.8645 0.8643 0.8610 0.8680
Validation:	 ACC: 0.8224 0.8224 0.8209 0.8240
Validation:	 Best_BACC: 0.8224 0.8224 0.8209 0.8240
Validation:	 Loss: 0.4123
Pretraining:	Epoch 7/200
----------
training:	Epoch: [7][1/204]	Loss 0.3752 (0.3752)	
training:	Epoch: [7][2/204]	Loss 0.3754 (0.3753)	
training:	Epoch: [7][3/204]	Loss 0.3728 (0.3744)	
training:	Epoch: [7][4/204]	Loss 0.3488 (0.3680)	
training:	Epoch: [7][5/204]	Loss 0.3575 (0.3659)	
training:	Epoch: [7][6/204]	Loss 0.1948 (0.3374)	
training:	Epoch: [7][7/204]	Loss 0.4208 (0.3493)	
training:	Epoch: [7][8/204]	Loss 0.3344 (0.3475)	
training:	Epoch: [7][9/204]	Loss 0.3682 (0.3498)	
training:	Epoch: [7][10/204]	Loss 0.2911 (0.3439)	
training:	Epoch: [7][11/204]	Loss 0.4394 (0.3526)	
training:	Epoch: [7][12/204]	Loss 0.2290 (0.3423)	
training:	Epoch: [7][13/204]	Loss 0.2177 (0.3327)	
training:	Epoch: [7][14/204]	Loss 0.4102 (0.3382)	
training:	Epoch: [7][15/204]	Loss 0.2259 (0.3307)	
training:	Epoch: [7][16/204]	Loss 0.4182 (0.3362)	
training:	Epoch: [7][17/204]	Loss 0.4282 (0.3416)	
training:	Epoch: [7][18/204]	Loss 0.3124 (0.3400)	
training:	Epoch: [7][19/204]	Loss 0.1909 (0.3321)	
training:	Epoch: [7][20/204]	Loss 0.1962 (0.3254)	
training:	Epoch: [7][21/204]	Loss 0.2767 (0.3230)	
training:	Epoch: [7][22/204]	Loss 0.3720 (0.3253)	
training:	Epoch: [7][23/204]	Loss 0.4078 (0.3288)	
training:	Epoch: [7][24/204]	Loss 0.3611 (0.3302)	
training:	Epoch: [7][25/204]	Loss 0.2699 (0.3278)	
training:	Epoch: [7][26/204]	Loss 0.3144 (0.3273)	
training:	Epoch: [7][27/204]	Loss 0.3331 (0.3275)	
training:	Epoch: [7][28/204]	Loss 0.1785 (0.3222)	
training:	Epoch: [7][29/204]	Loss 0.3517 (0.3232)	
training:	Epoch: [7][30/204]	Loss 0.2874 (0.3220)	
training:	Epoch: [7][31/204]	Loss 0.3586 (0.3232)	
training:	Epoch: [7][32/204]	Loss 0.3718 (0.3247)	
training:	Epoch: [7][33/204]	Loss 0.3716 (0.3261)	
training:	Epoch: [7][34/204]	Loss 0.5182 (0.3318)	
training:	Epoch: [7][35/204]	Loss 0.2416 (0.3292)	
training:	Epoch: [7][36/204]	Loss 0.5170 (0.3344)	
training:	Epoch: [7][37/204]	Loss 0.3258 (0.3342)	
training:	Epoch: [7][38/204]	Loss 0.3019 (0.3333)	
training:	Epoch: [7][39/204]	Loss 0.3069 (0.3326)	
training:	Epoch: [7][40/204]	Loss 0.6411 (0.3404)	
training:	Epoch: [7][41/204]	Loss 0.4148 (0.3422)	
training:	Epoch: [7][42/204]	Loss 0.3294 (0.3419)	
training:	Epoch: [7][43/204]	Loss 0.4467 (0.3443)	
training:	Epoch: [7][44/204]	Loss 0.4746 (0.3473)	
training:	Epoch: [7][45/204]	Loss 0.2675 (0.3455)	
training:	Epoch: [7][46/204]	Loss 0.5903 (0.3508)	
training:	Epoch: [7][47/204]	Loss 0.3014 (0.3498)	
training:	Epoch: [7][48/204]	Loss 0.5697 (0.3543)	
training:	Epoch: [7][49/204]	Loss 0.3899 (0.3551)	
training:	Epoch: [7][50/204]	Loss 0.3519 (0.3550)	
training:	Epoch: [7][51/204]	Loss 0.2601 (0.3531)	
training:	Epoch: [7][52/204]	Loss 0.4358 (0.3547)	
training:	Epoch: [7][53/204]	Loss 0.4694 (0.3569)	
training:	Epoch: [7][54/204]	Loss 0.2565 (0.3550)	
training:	Epoch: [7][55/204]	Loss 0.3502 (0.3550)	
training:	Epoch: [7][56/204]	Loss 0.2522 (0.3531)	
training:	Epoch: [7][57/204]	Loss 0.4348 (0.3546)	
training:	Epoch: [7][58/204]	Loss 0.5273 (0.3575)	
training:	Epoch: [7][59/204]	Loss 0.5022 (0.3600)	
training:	Epoch: [7][60/204]	Loss 0.4521 (0.3615)	
training:	Epoch: [7][61/204]	Loss 0.3886 (0.3620)	
training:	Epoch: [7][62/204]	Loss 0.2268 (0.3598)	
training:	Epoch: [7][63/204]	Loss 0.4666 (0.3615)	
training:	Epoch: [7][64/204]	Loss 0.2682 (0.3600)	
training:	Epoch: [7][65/204]	Loss 0.4969 (0.3621)	
training:	Epoch: [7][66/204]	Loss 0.3592 (0.3621)	
training:	Epoch: [7][67/204]	Loss 0.3759 (0.3623)	
training:	Epoch: [7][68/204]	Loss 0.3586 (0.3622)	
training:	Epoch: [7][69/204]	Loss 0.4403 (0.3634)	
training:	Epoch: [7][70/204]	Loss 0.3686 (0.3634)	
training:	Epoch: [7][71/204]	Loss 0.4299 (0.3644)	
training:	Epoch: [7][72/204]	Loss 0.2319 (0.3625)	
training:	Epoch: [7][73/204]	Loss 0.2220 (0.3606)	
training:	Epoch: [7][74/204]	Loss 0.3760 (0.3608)	
training:	Epoch: [7][75/204]	Loss 0.4504 (0.3620)	
training:	Epoch: [7][76/204]	Loss 0.1808 (0.3596)	
training:	Epoch: [7][77/204]	Loss 0.3932 (0.3601)	
training:	Epoch: [7][78/204]	Loss 0.4035 (0.3606)	
training:	Epoch: [7][79/204]	Loss 0.4744 (0.3621)	
training:	Epoch: [7][80/204]	Loss 0.2154 (0.3602)	
training:	Epoch: [7][81/204]	Loss 0.5049 (0.3620)	
training:	Epoch: [7][82/204]	Loss 0.1907 (0.3599)	
training:	Epoch: [7][83/204]	Loss 0.3344 (0.3596)	
training:	Epoch: [7][84/204]	Loss 0.2822 (0.3587)	
training:	Epoch: [7][85/204]	Loss 0.4628 (0.3599)	
training:	Epoch: [7][86/204]	Loss 0.2391 (0.3585)	
training:	Epoch: [7][87/204]	Loss 0.2311 (0.3571)	
training:	Epoch: [7][88/204]	Loss 0.2916 (0.3563)	
training:	Epoch: [7][89/204]	Loss 0.3967 (0.3568)	
training:	Epoch: [7][90/204]	Loss 0.3369 (0.3565)	
training:	Epoch: [7][91/204]	Loss 0.6525 (0.3598)	
training:	Epoch: [7][92/204]	Loss 0.5138 (0.3615)	
training:	Epoch: [7][93/204]	Loss 0.5113 (0.3631)	
training:	Epoch: [7][94/204]	Loss 0.3013 (0.3624)	
training:	Epoch: [7][95/204]	Loss 0.4472 (0.3633)	
training:	Epoch: [7][96/204]	Loss 0.3157 (0.3628)	
training:	Epoch: [7][97/204]	Loss 0.2035 (0.3612)	
training:	Epoch: [7][98/204]	Loss 0.2715 (0.3603)	
training:	Epoch: [7][99/204]	Loss 0.4032 (0.3607)	
training:	Epoch: [7][100/204]	Loss 0.3233 (0.3603)	
training:	Epoch: [7][101/204]	Loss 0.4236 (0.3609)	
training:	Epoch: [7][102/204]	Loss 0.2950 (0.3603)	
training:	Epoch: [7][103/204]	Loss 0.2932 (0.3596)	
training:	Epoch: [7][104/204]	Loss 0.4053 (0.3601)	
training:	Epoch: [7][105/204]	Loss 0.2975 (0.3595)	
training:	Epoch: [7][106/204]	Loss 0.3191 (0.3591)	
training:	Epoch: [7][107/204]	Loss 0.4703 (0.3602)	
training:	Epoch: [7][108/204]	Loss 0.2040 (0.3587)	
training:	Epoch: [7][109/204]	Loss 0.4288 (0.3593)	
training:	Epoch: [7][110/204]	Loss 0.3412 (0.3592)	
training:	Epoch: [7][111/204]	Loss 0.4801 (0.3603)	
training:	Epoch: [7][112/204]	Loss 0.3492 (0.3602)	
training:	Epoch: [7][113/204]	Loss 0.2844 (0.3595)	
training:	Epoch: [7][114/204]	Loss 0.3730 (0.3596)	
training:	Epoch: [7][115/204]	Loss 0.3343 (0.3594)	
training:	Epoch: [7][116/204]	Loss 0.4385 (0.3601)	
training:	Epoch: [7][117/204]	Loss 0.2741 (0.3593)	
training:	Epoch: [7][118/204]	Loss 0.6132 (0.3615)	
training:	Epoch: [7][119/204]	Loss 0.3885 (0.3617)	
training:	Epoch: [7][120/204]	Loss 0.4048 (0.3621)	
training:	Epoch: [7][121/204]	Loss 0.3972 (0.3624)	
training:	Epoch: [7][122/204]	Loss 0.3201 (0.3620)	
training:	Epoch: [7][123/204]	Loss 0.3060 (0.3616)	
training:	Epoch: [7][124/204]	Loss 0.2456 (0.3606)	
training:	Epoch: [7][125/204]	Loss 0.3635 (0.3607)	
training:	Epoch: [7][126/204]	Loss 0.4473 (0.3613)	
training:	Epoch: [7][127/204]	Loss 0.3302 (0.3611)	
training:	Epoch: [7][128/204]	Loss 0.4384 (0.3617)	
training:	Epoch: [7][129/204]	Loss 0.3330 (0.3615)	
training:	Epoch: [7][130/204]	Loss 0.4177 (0.3619)	
training:	Epoch: [7][131/204]	Loss 0.3131 (0.3615)	
training:	Epoch: [7][132/204]	Loss 0.4927 (0.3625)	
training:	Epoch: [7][133/204]	Loss 0.2385 (0.3616)	
training:	Epoch: [7][134/204]	Loss 0.3691 (0.3617)	
training:	Epoch: [7][135/204]	Loss 0.4813 (0.3625)	
training:	Epoch: [7][136/204]	Loss 0.2543 (0.3618)	
training:	Epoch: [7][137/204]	Loss 0.3394 (0.3616)	
training:	Epoch: [7][138/204]	Loss 0.3487 (0.3615)	
training:	Epoch: [7][139/204]	Loss 0.4026 (0.3618)	
training:	Epoch: [7][140/204]	Loss 0.5851 (0.3634)	
training:	Epoch: [7][141/204]	Loss 0.4047 (0.3637)	
training:	Epoch: [7][142/204]	Loss 0.3686 (0.3637)	
training:	Epoch: [7][143/204]	Loss 0.5287 (0.3649)	
training:	Epoch: [7][144/204]	Loss 0.2053 (0.3638)	
training:	Epoch: [7][145/204]	Loss 0.4899 (0.3646)	
training:	Epoch: [7][146/204]	Loss 0.3759 (0.3647)	
training:	Epoch: [7][147/204]	Loss 0.3072 (0.3643)	
training:	Epoch: [7][148/204]	Loss 0.4592 (0.3650)	
training:	Epoch: [7][149/204]	Loss 0.3139 (0.3646)	
training:	Epoch: [7][150/204]	Loss 0.4776 (0.3654)	
training:	Epoch: [7][151/204]	Loss 0.5019 (0.3663)	
training:	Epoch: [7][152/204]	Loss 0.3924 (0.3664)	
training:	Epoch: [7][153/204]	Loss 0.4377 (0.3669)	
training:	Epoch: [7][154/204]	Loss 0.1766 (0.3657)	
training:	Epoch: [7][155/204]	Loss 0.3177 (0.3654)	
training:	Epoch: [7][156/204]	Loss 0.3539 (0.3653)	
training:	Epoch: [7][157/204]	Loss 0.3196 (0.3650)	
training:	Epoch: [7][158/204]	Loss 0.2654 (0.3644)	
training:	Epoch: [7][159/204]	Loss 0.3750 (0.3644)	
training:	Epoch: [7][160/204]	Loss 0.3794 (0.3645)	
training:	Epoch: [7][161/204]	Loss 0.4210 (0.3649)	
training:	Epoch: [7][162/204]	Loss 0.3901 (0.3650)	
training:	Epoch: [7][163/204]	Loss 0.2972 (0.3646)	
training:	Epoch: [7][164/204]	Loss 0.2352 (0.3638)	
training:	Epoch: [7][165/204]	Loss 0.2778 (0.3633)	
training:	Epoch: [7][166/204]	Loss 0.3051 (0.3630)	
training:	Epoch: [7][167/204]	Loss 0.3072 (0.3626)	
training:	Epoch: [7][168/204]	Loss 0.4241 (0.3630)	
training:	Epoch: [7][169/204]	Loss 0.6114 (0.3645)	
training:	Epoch: [7][170/204]	Loss 0.3066 (0.3641)	
training:	Epoch: [7][171/204]	Loss 0.3787 (0.3642)	
training:	Epoch: [7][172/204]	Loss 0.3474 (0.3641)	
training:	Epoch: [7][173/204]	Loss 0.2886 (0.3637)	
training:	Epoch: [7][174/204]	Loss 0.2910 (0.3633)	
training:	Epoch: [7][175/204]	Loss 0.3436 (0.3631)	
training:	Epoch: [7][176/204]	Loss 0.3171 (0.3629)	
training:	Epoch: [7][177/204]	Loss 0.4175 (0.3632)	
training:	Epoch: [7][178/204]	Loss 0.4771 (0.3638)	
training:	Epoch: [7][179/204]	Loss 0.4692 (0.3644)	
training:	Epoch: [7][180/204]	Loss 0.4299 (0.3648)	
training:	Epoch: [7][181/204]	Loss 0.2927 (0.3644)	
training:	Epoch: [7][182/204]	Loss 0.2584 (0.3638)	
training:	Epoch: [7][183/204]	Loss 0.4402 (0.3642)	
training:	Epoch: [7][184/204]	Loss 0.2693 (0.3637)	
training:	Epoch: [7][185/204]	Loss 0.4303 (0.3641)	
training:	Epoch: [7][186/204]	Loss 0.5157 (0.3649)	
training:	Epoch: [7][187/204]	Loss 0.2716 (0.3644)	
training:	Epoch: [7][188/204]	Loss 0.2614 (0.3638)	
training:	Epoch: [7][189/204]	Loss 0.1271 (0.3626)	
training:	Epoch: [7][190/204]	Loss 0.3288 (0.3624)	
training:	Epoch: [7][191/204]	Loss 0.3066 (0.3621)	
training:	Epoch: [7][192/204]	Loss 0.3421 (0.3620)	
training:	Epoch: [7][193/204]	Loss 0.2782 (0.3616)	
training:	Epoch: [7][194/204]	Loss 0.2514 (0.3610)	
training:	Epoch: [7][195/204]	Loss 0.4207 (0.3613)	
training:	Epoch: [7][196/204]	Loss 0.4378 (0.3617)	
training:	Epoch: [7][197/204]	Loss 0.5696 (0.3628)	
training:	Epoch: [7][198/204]	Loss 0.5003 (0.3634)	
training:	Epoch: [7][199/204]	Loss 0.1500 (0.3624)	
training:	Epoch: [7][200/204]	Loss 0.2976 (0.3621)	
training:	Epoch: [7][201/204]	Loss 0.4539 (0.3625)	
training:	Epoch: [7][202/204]	Loss 0.4021 (0.3627)	
training:	Epoch: [7][203/204]	Loss 0.3481 (0.3626)	
training:	Epoch: [7][204/204]	Loss 0.3196 (0.3624)	
Training:	 Loss: 0.3619

Training:	 ACC: 0.8717 0.8717 0.8704 0.8731
Validation:	 ACC: 0.8214 0.8218 0.8301 0.8128
Validation:	 Best_BACC: 0.8224 0.8224 0.8209 0.8240
Validation:	 Loss: 0.4095
Pretraining:	Epoch 8/200
----------
training:	Epoch: [8][1/204]	Loss 0.2663 (0.2663)	
training:	Epoch: [8][2/204]	Loss 0.2580 (0.2622)	
training:	Epoch: [8][3/204]	Loss 0.4651 (0.3298)	
training:	Epoch: [8][4/204]	Loss 0.3643 (0.3384)	
training:	Epoch: [8][5/204]	Loss 0.1235 (0.2955)	
training:	Epoch: [8][6/204]	Loss 0.4245 (0.3170)	
training:	Epoch: [8][7/204]	Loss 0.2791 (0.3116)	
training:	Epoch: [8][8/204]	Loss 0.3026 (0.3104)	
training:	Epoch: [8][9/204]	Loss 0.4181 (0.3224)	
training:	Epoch: [8][10/204]	Loss 0.3085 (0.3210)	
training:	Epoch: [8][11/204]	Loss 0.2672 (0.3161)	
training:	Epoch: [8][12/204]	Loss 0.2996 (0.3147)	
training:	Epoch: [8][13/204]	Loss 0.3039 (0.3139)	
training:	Epoch: [8][14/204]	Loss 0.3031 (0.3131)	
training:	Epoch: [8][15/204]	Loss 0.2757 (0.3106)	
training:	Epoch: [8][16/204]	Loss 0.4802 (0.3212)	
training:	Epoch: [8][17/204]	Loss 0.3171 (0.3210)	
training:	Epoch: [8][18/204]	Loss 0.2315 (0.3160)	
training:	Epoch: [8][19/204]	Loss 0.4910 (0.3252)	
training:	Epoch: [8][20/204]	Loss 0.3712 (0.3275)	
training:	Epoch: [8][21/204]	Loss 0.4252 (0.3322)	
training:	Epoch: [8][22/204]	Loss 0.3891 (0.3348)	
training:	Epoch: [8][23/204]	Loss 0.3168 (0.3340)	
training:	Epoch: [8][24/204]	Loss 0.2239 (0.3294)	
training:	Epoch: [8][25/204]	Loss 0.2023 (0.3243)	
training:	Epoch: [8][26/204]	Loss 0.4309 (0.3284)	
training:	Epoch: [8][27/204]	Loss 0.2869 (0.3269)	
training:	Epoch: [8][28/204]	Loss 0.1544 (0.3207)	
training:	Epoch: [8][29/204]	Loss 0.2440 (0.3181)	
training:	Epoch: [8][30/204]	Loss 0.2191 (0.3148)	
training:	Epoch: [8][31/204]	Loss 0.3837 (0.3170)	
training:	Epoch: [8][32/204]	Loss 0.3451 (0.3179)	
training:	Epoch: [8][33/204]	Loss 0.6306 (0.3273)	
training:	Epoch: [8][34/204]	Loss 0.3248 (0.3273)	
training:	Epoch: [8][35/204]	Loss 0.2561 (0.3252)	
training:	Epoch: [8][36/204]	Loss 0.2807 (0.3240)	
training:	Epoch: [8][37/204]	Loss 0.4303 (0.3269)	
training:	Epoch: [8][38/204]	Loss 0.3139 (0.3265)	
training:	Epoch: [8][39/204]	Loss 0.3660 (0.3275)	
training:	Epoch: [8][40/204]	Loss 0.3245 (0.3275)	
training:	Epoch: [8][41/204]	Loss 0.3776 (0.3287)	
training:	Epoch: [8][42/204]	Loss 0.3904 (0.3302)	
training:	Epoch: [8][43/204]	Loss 0.3562 (0.3308)	
training:	Epoch: [8][44/204]	Loss 0.4942 (0.3345)	
training:	Epoch: [8][45/204]	Loss 0.3954 (0.3358)	
training:	Epoch: [8][46/204]	Loss 0.4176 (0.3376)	
training:	Epoch: [8][47/204]	Loss 0.2669 (0.3361)	
training:	Epoch: [8][48/204]	Loss 0.2084 (0.3334)	
training:	Epoch: [8][49/204]	Loss 0.1910 (0.3305)	
training:	Epoch: [8][50/204]	Loss 0.5195 (0.3343)	
training:	Epoch: [8][51/204]	Loss 0.2765 (0.3332)	
training:	Epoch: [8][52/204]	Loss 0.5376 (0.3371)	
training:	Epoch: [8][53/204]	Loss 0.3527 (0.3374)	
training:	Epoch: [8][54/204]	Loss 0.2482 (0.3358)	
training:	Epoch: [8][55/204]	Loss 0.1386 (0.3322)	
training:	Epoch: [8][56/204]	Loss 0.3975 (0.3333)	
training:	Epoch: [8][57/204]	Loss 0.1823 (0.3307)	
training:	Epoch: [8][58/204]	Loss 0.2866 (0.3299)	
training:	Epoch: [8][59/204]	Loss 0.2108 (0.3279)	
training:	Epoch: [8][60/204]	Loss 0.4147 (0.3294)	
training:	Epoch: [8][61/204]	Loss 0.4414 (0.3312)	
training:	Epoch: [8][62/204]	Loss 0.2835 (0.3304)	
training:	Epoch: [8][63/204]	Loss 0.2662 (0.3294)	
training:	Epoch: [8][64/204]	Loss 0.4538 (0.3313)	
training:	Epoch: [8][65/204]	Loss 0.2255 (0.3297)	
training:	Epoch: [8][66/204]	Loss 0.4471 (0.3315)	
training:	Epoch: [8][67/204]	Loss 0.4410 (0.3331)	
training:	Epoch: [8][68/204]	Loss 0.2624 (0.3321)	
training:	Epoch: [8][69/204]	Loss 0.3697 (0.3326)	
training:	Epoch: [8][70/204]	Loss 0.2986 (0.3322)	
training:	Epoch: [8][71/204]	Loss 0.5046 (0.3346)	
training:	Epoch: [8][72/204]	Loss 0.3689 (0.3351)	
training:	Epoch: [8][73/204]	Loss 0.3011 (0.3346)	
training:	Epoch: [8][74/204]	Loss 0.2885 (0.3340)	
training:	Epoch: [8][75/204]	Loss 0.3806 (0.3346)	
training:	Epoch: [8][76/204]	Loss 0.2669 (0.3337)	
training:	Epoch: [8][77/204]	Loss 0.3737 (0.3342)	
training:	Epoch: [8][78/204]	Loss 0.1978 (0.3325)	
training:	Epoch: [8][79/204]	Loss 0.4121 (0.3335)	
training:	Epoch: [8][80/204]	Loss 0.3083 (0.3332)	
training:	Epoch: [8][81/204]	Loss 0.3939 (0.3339)	
training:	Epoch: [8][82/204]	Loss 0.2543 (0.3329)	
training:	Epoch: [8][83/204]	Loss 0.5396 (0.3354)	
training:	Epoch: [8][84/204]	Loss 0.3100 (0.3351)	
training:	Epoch: [8][85/204]	Loss 0.5477 (0.3376)	
training:	Epoch: [8][86/204]	Loss 0.3300 (0.3375)	
training:	Epoch: [8][87/204]	Loss 0.5106 (0.3395)	
training:	Epoch: [8][88/204]	Loss 0.3943 (0.3402)	
training:	Epoch: [8][89/204]	Loss 0.3036 (0.3397)	
training:	Epoch: [8][90/204]	Loss 0.3820 (0.3402)	
training:	Epoch: [8][91/204]	Loss 0.2734 (0.3395)	
training:	Epoch: [8][92/204]	Loss 0.5747 (0.3420)	
training:	Epoch: [8][93/204]	Loss 0.4293 (0.3430)	
training:	Epoch: [8][94/204]	Loss 0.4959 (0.3446)	
training:	Epoch: [8][95/204]	Loss 0.3424 (0.3446)	
training:	Epoch: [8][96/204]	Loss 0.4147 (0.3453)	
training:	Epoch: [8][97/204]	Loss 0.2521 (0.3443)	
training:	Epoch: [8][98/204]	Loss 0.3367 (0.3443)	
training:	Epoch: [8][99/204]	Loss 0.2785 (0.3436)	
training:	Epoch: [8][100/204]	Loss 0.2895 (0.3431)	
training:	Epoch: [8][101/204]	Loss 0.4002 (0.3436)	
training:	Epoch: [8][102/204]	Loss 0.1902 (0.3421)	
training:	Epoch: [8][103/204]	Loss 0.2968 (0.3417)	
training:	Epoch: [8][104/204]	Loss 0.3691 (0.3419)	
training:	Epoch: [8][105/204]	Loss 0.2265 (0.3408)	
training:	Epoch: [8][106/204]	Loss 0.5855 (0.3432)	
training:	Epoch: [8][107/204]	Loss 0.5123 (0.3447)	
training:	Epoch: [8][108/204]	Loss 0.2356 (0.3437)	
training:	Epoch: [8][109/204]	Loss 0.4069 (0.3443)	
training:	Epoch: [8][110/204]	Loss 0.2866 (0.3438)	
training:	Epoch: [8][111/204]	Loss 0.3496 (0.3438)	
training:	Epoch: [8][112/204]	Loss 0.4876 (0.3451)	
training:	Epoch: [8][113/204]	Loss 0.4697 (0.3462)	
training:	Epoch: [8][114/204]	Loss 0.3870 (0.3466)	
training:	Epoch: [8][115/204]	Loss 0.4824 (0.3478)	
training:	Epoch: [8][116/204]	Loss 0.2610 (0.3470)	
training:	Epoch: [8][117/204]	Loss 0.3650 (0.3472)	
training:	Epoch: [8][118/204]	Loss 0.4581 (0.3481)	
training:	Epoch: [8][119/204]	Loss 0.3676 (0.3483)	
training:	Epoch: [8][120/204]	Loss 0.3015 (0.3479)	
training:	Epoch: [8][121/204]	Loss 0.2269 (0.3469)	
training:	Epoch: [8][122/204]	Loss 0.1613 (0.3454)	
training:	Epoch: [8][123/204]	Loss 0.3746 (0.3456)	
training:	Epoch: [8][124/204]	Loss 0.2761 (0.3450)	
training:	Epoch: [8][125/204]	Loss 0.3921 (0.3454)	
training:	Epoch: [8][126/204]	Loss 0.3590 (0.3455)	
training:	Epoch: [8][127/204]	Loss 0.4121 (0.3460)	
training:	Epoch: [8][128/204]	Loss 0.3056 (0.3457)	
training:	Epoch: [8][129/204]	Loss 0.4060 (0.3462)	
training:	Epoch: [8][130/204]	Loss 0.4403 (0.3469)	
training:	Epoch: [8][131/204]	Loss 0.2440 (0.3461)	
training:	Epoch: [8][132/204]	Loss 0.4258 (0.3467)	
training:	Epoch: [8][133/204]	Loss 0.3551 (0.3468)	
training:	Epoch: [8][134/204]	Loss 0.3852 (0.3471)	
training:	Epoch: [8][135/204]	Loss 0.3264 (0.3469)	
training:	Epoch: [8][136/204]	Loss 0.2656 (0.3463)	
training:	Epoch: [8][137/204]	Loss 0.2799 (0.3458)	
training:	Epoch: [8][138/204]	Loss 0.4101 (0.3463)	
training:	Epoch: [8][139/204]	Loss 0.4150 (0.3468)	
training:	Epoch: [8][140/204]	Loss 0.3235 (0.3466)	
training:	Epoch: [8][141/204]	Loss 0.4496 (0.3474)	
training:	Epoch: [8][142/204]	Loss 0.2910 (0.3470)	
training:	Epoch: [8][143/204]	Loss 0.2789 (0.3465)	
training:	Epoch: [8][144/204]	Loss 0.2406 (0.3458)	
training:	Epoch: [8][145/204]	Loss 0.4463 (0.3465)	
training:	Epoch: [8][146/204]	Loss 0.2859 (0.3460)	
training:	Epoch: [8][147/204]	Loss 0.2415 (0.3453)	
training:	Epoch: [8][148/204]	Loss 0.4070 (0.3457)	
training:	Epoch: [8][149/204]	Loss 0.3000 (0.3454)	
training:	Epoch: [8][150/204]	Loss 0.4495 (0.3461)	
training:	Epoch: [8][151/204]	Loss 0.4304 (0.3467)	
training:	Epoch: [8][152/204]	Loss 0.4924 (0.3477)	
training:	Epoch: [8][153/204]	Loss 0.2879 (0.3473)	
training:	Epoch: [8][154/204]	Loss 0.4956 (0.3482)	
training:	Epoch: [8][155/204]	Loss 0.2406 (0.3475)	
training:	Epoch: [8][156/204]	Loss 0.2888 (0.3472)	
training:	Epoch: [8][157/204]	Loss 0.2485 (0.3465)	
training:	Epoch: [8][158/204]	Loss 0.2014 (0.3456)	
training:	Epoch: [8][159/204]	Loss 0.1656 (0.3445)	
training:	Epoch: [8][160/204]	Loss 0.3683 (0.3446)	
training:	Epoch: [8][161/204]	Loss 0.4061 (0.3450)	
training:	Epoch: [8][162/204]	Loss 0.3851 (0.3453)	
training:	Epoch: [8][163/204]	Loss 0.4402 (0.3458)	
training:	Epoch: [8][164/204]	Loss 0.3501 (0.3459)	
training:	Epoch: [8][165/204]	Loss 0.3411 (0.3458)	
training:	Epoch: [8][166/204]	Loss 0.4111 (0.3462)	
training:	Epoch: [8][167/204]	Loss 0.2926 (0.3459)	
training:	Epoch: [8][168/204]	Loss 0.3940 (0.3462)	
training:	Epoch: [8][169/204]	Loss 0.2662 (0.3457)	
training:	Epoch: [8][170/204]	Loss 0.2757 (0.3453)	
training:	Epoch: [8][171/204]	Loss 0.3916 (0.3456)	
training:	Epoch: [8][172/204]	Loss 0.3350 (0.3455)	
training:	Epoch: [8][173/204]	Loss 0.2969 (0.3452)	
training:	Epoch: [8][174/204]	Loss 0.2475 (0.3447)	
training:	Epoch: [8][175/204]	Loss 0.4676 (0.3454)	
training:	Epoch: [8][176/204]	Loss 0.2432 (0.3448)	
training:	Epoch: [8][177/204]	Loss 0.4014 (0.3451)	
training:	Epoch: [8][178/204]	Loss 0.3883 (0.3454)	
training:	Epoch: [8][179/204]	Loss 0.3018 (0.3451)	
training:	Epoch: [8][180/204]	Loss 0.3716 (0.3453)	
training:	Epoch: [8][181/204]	Loss 0.5315 (0.3463)	
training:	Epoch: [8][182/204]	Loss 0.4042 (0.3466)	
training:	Epoch: [8][183/204]	Loss 0.3062 (0.3464)	
training:	Epoch: [8][184/204]	Loss 0.4146 (0.3468)	
training:	Epoch: [8][185/204]	Loss 0.3019 (0.3465)	
training:	Epoch: [8][186/204]	Loss 0.3086 (0.3463)	
training:	Epoch: [8][187/204]	Loss 0.4923 (0.3471)	
training:	Epoch: [8][188/204]	Loss 0.3360 (0.3470)	
training:	Epoch: [8][189/204]	Loss 0.2828 (0.3467)	
training:	Epoch: [8][190/204]	Loss 0.2836 (0.3464)	
training:	Epoch: [8][191/204]	Loss 0.4710 (0.3470)	
training:	Epoch: [8][192/204]	Loss 0.4647 (0.3476)	
training:	Epoch: [8][193/204]	Loss 0.3003 (0.3474)	
training:	Epoch: [8][194/204]	Loss 0.1660 (0.3464)	
training:	Epoch: [8][195/204]	Loss 0.3604 (0.3465)	
training:	Epoch: [8][196/204]	Loss 0.2076 (0.3458)	
training:	Epoch: [8][197/204]	Loss 0.3041 (0.3456)	
training:	Epoch: [8][198/204]	Loss 0.3834 (0.3458)	
training:	Epoch: [8][199/204]	Loss 0.3033 (0.3456)	
training:	Epoch: [8][200/204]	Loss 0.3732 (0.3457)	
training:	Epoch: [8][201/204]	Loss 0.2789 (0.3454)	
training:	Epoch: [8][202/204]	Loss 0.4720 (0.3460)	
training:	Epoch: [8][203/204]	Loss 0.3561 (0.3461)	
training:	Epoch: [8][204/204]	Loss 0.1863 (0.3453)	
Training:	 Loss: 0.3447

Training:	 ACC: 0.8784 0.8783 0.8754 0.8814
Validation:	 ACC: 0.8326 0.8331 0.8424 0.8229
Validation:	 Best_BACC: 0.8326 0.8331 0.8424 0.8229
Validation:	 Loss: 0.3996
Pretraining:	Epoch 9/200
----------
training:	Epoch: [9][1/204]	Loss 0.4026 (0.4026)	
training:	Epoch: [9][2/204]	Loss 0.2276 (0.3151)	
training:	Epoch: [9][3/204]	Loss 0.6647 (0.4316)	
training:	Epoch: [9][4/204]	Loss 0.2629 (0.3894)	
training:	Epoch: [9][5/204]	Loss 0.2390 (0.3593)	
training:	Epoch: [9][6/204]	Loss 0.5684 (0.3942)	
training:	Epoch: [9][7/204]	Loss 0.2486 (0.3734)	
training:	Epoch: [9][8/204]	Loss 0.3833 (0.3746)	
training:	Epoch: [9][9/204]	Loss 0.3513 (0.3720)	
training:	Epoch: [9][10/204]	Loss 0.3365 (0.3685)	
training:	Epoch: [9][11/204]	Loss 0.2812 (0.3605)	
training:	Epoch: [9][12/204]	Loss 0.4165 (0.3652)	
training:	Epoch: [9][13/204]	Loss 0.2973 (0.3600)	
training:	Epoch: [9][14/204]	Loss 0.3044 (0.3560)	
training:	Epoch: [9][15/204]	Loss 0.4457 (0.3620)	
training:	Epoch: [9][16/204]	Loss 0.4232 (0.3658)	
training:	Epoch: [9][17/204]	Loss 0.3295 (0.3637)	
training:	Epoch: [9][18/204]	Loss 0.3339 (0.3620)	
training:	Epoch: [9][19/204]	Loss 0.3352 (0.3606)	
training:	Epoch: [9][20/204]	Loss 0.1662 (0.3509)	
training:	Epoch: [9][21/204]	Loss 0.2715 (0.3471)	
training:	Epoch: [9][22/204]	Loss 0.2299 (0.3418)	
training:	Epoch: [9][23/204]	Loss 0.3456 (0.3420)	
training:	Epoch: [9][24/204]	Loss 0.2370 (0.3376)	
training:	Epoch: [9][25/204]	Loss 0.2376 (0.3336)	
training:	Epoch: [9][26/204]	Loss 0.1691 (0.3273)	
training:	Epoch: [9][27/204]	Loss 0.4957 (0.3335)	
training:	Epoch: [9][28/204]	Loss 0.2108 (0.3291)	
training:	Epoch: [9][29/204]	Loss 0.3821 (0.3309)	
training:	Epoch: [9][30/204]	Loss 0.2091 (0.3269)	
training:	Epoch: [9][31/204]	Loss 0.1790 (0.3221)	
training:	Epoch: [9][32/204]	Loss 0.2231 (0.3190)	
training:	Epoch: [9][33/204]	Loss 0.3397 (0.3196)	
training:	Epoch: [9][34/204]	Loss 0.5908 (0.3276)	
training:	Epoch: [9][35/204]	Loss 0.2747 (0.3261)	
training:	Epoch: [9][36/204]	Loss 0.2167 (0.3231)	
training:	Epoch: [9][37/204]	Loss 0.3446 (0.3236)	
training:	Epoch: [9][38/204]	Loss 0.3859 (0.3253)	
training:	Epoch: [9][39/204]	Loss 0.2996 (0.3246)	
training:	Epoch: [9][40/204]	Loss 0.4327 (0.3273)	
training:	Epoch: [9][41/204]	Loss 0.3974 (0.3290)	
training:	Epoch: [9][42/204]	Loss 0.5214 (0.3336)	
training:	Epoch: [9][43/204]	Loss 0.4762 (0.3369)	
training:	Epoch: [9][44/204]	Loss 0.2246 (0.3344)	
training:	Epoch: [9][45/204]	Loss 0.2253 (0.3320)	
training:	Epoch: [9][46/204]	Loss 0.1470 (0.3279)	
training:	Epoch: [9][47/204]	Loss 0.3153 (0.3277)	
training:	Epoch: [9][48/204]	Loss 0.2801 (0.3267)	
training:	Epoch: [9][49/204]	Loss 0.3405 (0.3270)	
training:	Epoch: [9][50/204]	Loss 0.4493 (0.3294)	
training:	Epoch: [9][51/204]	Loss 0.4409 (0.3316)	
training:	Epoch: [9][52/204]	Loss 0.2676 (0.3304)	
training:	Epoch: [9][53/204]	Loss 0.4298 (0.3322)	
training:	Epoch: [9][54/204]	Loss 0.2855 (0.3314)	
training:	Epoch: [9][55/204]	Loss 0.3151 (0.3311)	
training:	Epoch: [9][56/204]	Loss 0.3729 (0.3318)	
training:	Epoch: [9][57/204]	Loss 0.4233 (0.3334)	
training:	Epoch: [9][58/204]	Loss 0.3450 (0.3336)	
training:	Epoch: [9][59/204]	Loss 0.4210 (0.3351)	
training:	Epoch: [9][60/204]	Loss 0.5426 (0.3386)	
training:	Epoch: [9][61/204]	Loss 0.1565 (0.3356)	
training:	Epoch: [9][62/204]	Loss 0.2764 (0.3346)	
training:	Epoch: [9][63/204]	Loss 0.3122 (0.3343)	
training:	Epoch: [9][64/204]	Loss 0.4031 (0.3353)	
training:	Epoch: [9][65/204]	Loss 0.4596 (0.3373)	
training:	Epoch: [9][66/204]	Loss 0.3609 (0.3376)	
training:	Epoch: [9][67/204]	Loss 0.3029 (0.3371)	
training:	Epoch: [9][68/204]	Loss 0.3629 (0.3375)	
training:	Epoch: [9][69/204]	Loss 0.3668 (0.3379)	
training:	Epoch: [9][70/204]	Loss 0.2316 (0.3364)	
training:	Epoch: [9][71/204]	Loss 0.2941 (0.3358)	
training:	Epoch: [9][72/204]	Loss 0.3570 (0.3361)	
training:	Epoch: [9][73/204]	Loss 0.3298 (0.3360)	
training:	Epoch: [9][74/204]	Loss 0.3120 (0.3357)	
training:	Epoch: [9][75/204]	Loss 0.2854 (0.3350)	
training:	Epoch: [9][76/204]	Loss 0.2856 (0.3344)	
training:	Epoch: [9][77/204]	Loss 0.2540 (0.3333)	
training:	Epoch: [9][78/204]	Loss 0.3924 (0.3341)	
training:	Epoch: [9][79/204]	Loss 0.1881 (0.3322)	
training:	Epoch: [9][80/204]	Loss 0.3583 (0.3325)	
training:	Epoch: [9][81/204]	Loss 0.2477 (0.3315)	
training:	Epoch: [9][82/204]	Loss 0.2951 (0.3311)	
training:	Epoch: [9][83/204]	Loss 0.3331 (0.3311)	
training:	Epoch: [9][84/204]	Loss 0.4844 (0.3329)	
training:	Epoch: [9][85/204]	Loss 0.3199 (0.3327)	
training:	Epoch: [9][86/204]	Loss 0.2799 (0.3321)	
training:	Epoch: [9][87/204]	Loss 0.3161 (0.3319)	
training:	Epoch: [9][88/204]	Loss 0.7233 (0.3364)	
training:	Epoch: [9][89/204]	Loss 0.5302 (0.3386)	
training:	Epoch: [9][90/204]	Loss 0.1944 (0.3370)	
training:	Epoch: [9][91/204]	Loss 0.4033 (0.3377)	
training:	Epoch: [9][92/204]	Loss 0.2651 (0.3369)	
training:	Epoch: [9][93/204]	Loss 0.3480 (0.3370)	
training:	Epoch: [9][94/204]	Loss 0.3273 (0.3369)	
training:	Epoch: [9][95/204]	Loss 0.2379 (0.3359)	
training:	Epoch: [9][96/204]	Loss 0.3645 (0.3362)	
training:	Epoch: [9][97/204]	Loss 0.2320 (0.3351)	
training:	Epoch: [9][98/204]	Loss 0.3498 (0.3353)	
training:	Epoch: [9][99/204]	Loss 0.3575 (0.3355)	
training:	Epoch: [9][100/204]	Loss 0.2079 (0.3342)	
training:	Epoch: [9][101/204]	Loss 0.3712 (0.3346)	
training:	Epoch: [9][102/204]	Loss 0.2791 (0.3340)	
training:	Epoch: [9][103/204]	Loss 0.5381 (0.3360)	
training:	Epoch: [9][104/204]	Loss 0.3365 (0.3360)	
training:	Epoch: [9][105/204]	Loss 0.2888 (0.3356)	
training:	Epoch: [9][106/204]	Loss 0.2432 (0.3347)	
training:	Epoch: [9][107/204]	Loss 0.2889 (0.3343)	
training:	Epoch: [9][108/204]	Loss 0.3723 (0.3346)	
training:	Epoch: [9][109/204]	Loss 0.3475 (0.3347)	
training:	Epoch: [9][110/204]	Loss 0.2498 (0.3340)	
training:	Epoch: [9][111/204]	Loss 0.3940 (0.3345)	
training:	Epoch: [9][112/204]	Loss 0.3896 (0.3350)	
training:	Epoch: [9][113/204]	Loss 0.3450 (0.3351)	
training:	Epoch: [9][114/204]	Loss 0.1909 (0.3338)	
training:	Epoch: [9][115/204]	Loss 0.1890 (0.3326)	
training:	Epoch: [9][116/204]	Loss 0.4074 (0.3332)	
training:	Epoch: [9][117/204]	Loss 0.3702 (0.3335)	
training:	Epoch: [9][118/204]	Loss 0.3707 (0.3338)	
training:	Epoch: [9][119/204]	Loss 0.2513 (0.3331)	
training:	Epoch: [9][120/204]	Loss 0.4240 (0.3339)	
training:	Epoch: [9][121/204]	Loss 0.2305 (0.3330)	
training:	Epoch: [9][122/204]	Loss 0.4274 (0.3338)	
training:	Epoch: [9][123/204]	Loss 0.4152 (0.3345)	
training:	Epoch: [9][124/204]	Loss 0.2262 (0.3336)	
training:	Epoch: [9][125/204]	Loss 0.2814 (0.3332)	
training:	Epoch: [9][126/204]	Loss 0.3921 (0.3337)	
training:	Epoch: [9][127/204]	Loss 0.3437 (0.3337)	
training:	Epoch: [9][128/204]	Loss 0.2379 (0.3330)	
training:	Epoch: [9][129/204]	Loss 0.4234 (0.3337)	
training:	Epoch: [9][130/204]	Loss 0.4496 (0.3346)	
training:	Epoch: [9][131/204]	Loss 0.2644 (0.3340)	
training:	Epoch: [9][132/204]	Loss 0.3647 (0.3343)	
training:	Epoch: [9][133/204]	Loss 0.3397 (0.3343)	
training:	Epoch: [9][134/204]	Loss 0.3181 (0.3342)	
training:	Epoch: [9][135/204]	Loss 0.4645 (0.3352)	
training:	Epoch: [9][136/204]	Loss 0.4285 (0.3358)	
training:	Epoch: [9][137/204]	Loss 0.2322 (0.3351)	
training:	Epoch: [9][138/204]	Loss 0.3247 (0.3350)	
training:	Epoch: [9][139/204]	Loss 0.3625 (0.3352)	
training:	Epoch: [9][140/204]	Loss 0.3786 (0.3355)	
training:	Epoch: [9][141/204]	Loss 0.1979 (0.3345)	
training:	Epoch: [9][142/204]	Loss 0.3050 (0.3343)	
training:	Epoch: [9][143/204]	Loss 0.3735 (0.3346)	
training:	Epoch: [9][144/204]	Loss 0.4651 (0.3355)	
training:	Epoch: [9][145/204]	Loss 0.1822 (0.3345)	
training:	Epoch: [9][146/204]	Loss 0.1716 (0.3333)	
training:	Epoch: [9][147/204]	Loss 0.3698 (0.3336)	
training:	Epoch: [9][148/204]	Loss 0.4165 (0.3342)	
training:	Epoch: [9][149/204]	Loss 0.2263 (0.3334)	
training:	Epoch: [9][150/204]	Loss 0.3864 (0.3338)	
training:	Epoch: [9][151/204]	Loss 0.5207 (0.3350)	
training:	Epoch: [9][152/204]	Loss 0.3207 (0.3349)	
training:	Epoch: [9][153/204]	Loss 0.2335 (0.3343)	
training:	Epoch: [9][154/204]	Loss 0.1708 (0.3332)	
training:	Epoch: [9][155/204]	Loss 0.4172 (0.3337)	
training:	Epoch: [9][156/204]	Loss 0.2144 (0.3330)	
training:	Epoch: [9][157/204]	Loss 0.1326 (0.3317)	
training:	Epoch: [9][158/204]	Loss 0.2232 (0.3310)	
training:	Epoch: [9][159/204]	Loss 0.2520 (0.3305)	
training:	Epoch: [9][160/204]	Loss 0.3830 (0.3308)	
training:	Epoch: [9][161/204]	Loss 0.3568 (0.3310)	
training:	Epoch: [9][162/204]	Loss 0.3045 (0.3308)	
training:	Epoch: [9][163/204]	Loss 0.4859 (0.3318)	
training:	Epoch: [9][164/204]	Loss 0.5974 (0.3334)	
training:	Epoch: [9][165/204]	Loss 0.5276 (0.3346)	
training:	Epoch: [9][166/204]	Loss 0.2291 (0.3340)	
training:	Epoch: [9][167/204]	Loss 0.2858 (0.3337)	
training:	Epoch: [9][168/204]	Loss 0.2120 (0.3329)	
training:	Epoch: [9][169/204]	Loss 0.2768 (0.3326)	
training:	Epoch: [9][170/204]	Loss 0.3764 (0.3329)	
training:	Epoch: [9][171/204]	Loss 0.2715 (0.3325)	
training:	Epoch: [9][172/204]	Loss 0.2764 (0.3322)	
training:	Epoch: [9][173/204]	Loss 0.4335 (0.3328)	
training:	Epoch: [9][174/204]	Loss 0.3514 (0.3329)	
training:	Epoch: [9][175/204]	Loss 0.2818 (0.3326)	
training:	Epoch: [9][176/204]	Loss 0.3181 (0.3325)	
training:	Epoch: [9][177/204]	Loss 0.2831 (0.3322)	
training:	Epoch: [9][178/204]	Loss 0.3113 (0.3321)	
training:	Epoch: [9][179/204]	Loss 0.4014 (0.3325)	
training:	Epoch: [9][180/204]	Loss 0.4695 (0.3333)	
training:	Epoch: [9][181/204]	Loss 0.5109 (0.3342)	
training:	Epoch: [9][182/204]	Loss 0.3975 (0.3346)	
training:	Epoch: [9][183/204]	Loss 0.2307 (0.3340)	
training:	Epoch: [9][184/204]	Loss 0.3177 (0.3339)	
training:	Epoch: [9][185/204]	Loss 0.4017 (0.3343)	
training:	Epoch: [9][186/204]	Loss 0.3101 (0.3342)	
training:	Epoch: [9][187/204]	Loss 0.4750 (0.3349)	
training:	Epoch: [9][188/204]	Loss 0.4602 (0.3356)	
training:	Epoch: [9][189/204]	Loss 0.3477 (0.3356)	
training:	Epoch: [9][190/204]	Loss 0.3334 (0.3356)	
training:	Epoch: [9][191/204]	Loss 0.4430 (0.3362)	
training:	Epoch: [9][192/204]	Loss 0.3304 (0.3362)	
training:	Epoch: [9][193/204]	Loss 0.2897 (0.3359)	
training:	Epoch: [9][194/204]	Loss 0.2497 (0.3355)	
training:	Epoch: [9][195/204]	Loss 0.3290 (0.3355)	
training:	Epoch: [9][196/204]	Loss 0.2088 (0.3348)	
training:	Epoch: [9][197/204]	Loss 0.2670 (0.3345)	
training:	Epoch: [9][198/204]	Loss 0.2052 (0.3338)	
training:	Epoch: [9][199/204]	Loss 0.2666 (0.3335)	
training:	Epoch: [9][200/204]	Loss 0.3537 (0.3336)	
training:	Epoch: [9][201/204]	Loss 0.2531 (0.3332)	
training:	Epoch: [9][202/204]	Loss 0.4413 (0.3337)	
training:	Epoch: [9][203/204]	Loss 0.3311 (0.3337)	
training:	Epoch: [9][204/204]	Loss 0.1707 (0.3329)	
Training:	 Loss: 0.3324

Training:	 ACC: 0.8767 0.8776 0.9006 0.8527
Validation:	 ACC: 0.8238 0.8256 0.8628 0.7848
Validation:	 Best_BACC: 0.8326 0.8331 0.8424 0.8229
Validation:	 Loss: 0.4055
Pretraining:	Epoch 10/200
----------
training:	Epoch: [10][1/204]	Loss 0.3844 (0.3844)	
training:	Epoch: [10][2/204]	Loss 0.3868 (0.3856)	
training:	Epoch: [10][3/204]	Loss 0.2279 (0.3330)	
training:	Epoch: [10][4/204]	Loss 0.3018 (0.3252)	
training:	Epoch: [10][5/204]	Loss 0.3491 (0.3300)	
training:	Epoch: [10][6/204]	Loss 0.2098 (0.3100)	
training:	Epoch: [10][7/204]	Loss 0.2824 (0.3060)	
training:	Epoch: [10][8/204]	Loss 0.2094 (0.2939)	
training:	Epoch: [10][9/204]	Loss 0.2814 (0.2926)	
training:	Epoch: [10][10/204]	Loss 0.3045 (0.2938)	
training:	Epoch: [10][11/204]	Loss 0.3404 (0.2980)	
training:	Epoch: [10][12/204]	Loss 0.3150 (0.2994)	
training:	Epoch: [10][13/204]	Loss 0.2121 (0.2927)	
training:	Epoch: [10][14/204]	Loss 0.3926 (0.2998)	
training:	Epoch: [10][15/204]	Loss 0.3599 (0.3038)	
training:	Epoch: [10][16/204]	Loss 0.2240 (0.2988)	
training:	Epoch: [10][17/204]	Loss 0.4455 (0.3075)	
training:	Epoch: [10][18/204]	Loss 0.3909 (0.3121)	
training:	Epoch: [10][19/204]	Loss 0.1836 (0.3053)	
training:	Epoch: [10][20/204]	Loss 0.3569 (0.3079)	
training:	Epoch: [10][21/204]	Loss 0.1913 (0.3024)	
training:	Epoch: [10][22/204]	Loss 0.3047 (0.3025)	
training:	Epoch: [10][23/204]	Loss 0.3575 (0.3049)	
training:	Epoch: [10][24/204]	Loss 0.4359 (0.3103)	
training:	Epoch: [10][25/204]	Loss 0.4808 (0.3171)	
training:	Epoch: [10][26/204]	Loss 0.2251 (0.3136)	
training:	Epoch: [10][27/204]	Loss 0.3432 (0.3147)	
training:	Epoch: [10][28/204]	Loss 0.2776 (0.3134)	
training:	Epoch: [10][29/204]	Loss 0.4655 (0.3186)	
training:	Epoch: [10][30/204]	Loss 0.1879 (0.3143)	
training:	Epoch: [10][31/204]	Loss 0.6122 (0.3239)	
training:	Epoch: [10][32/204]	Loss 0.4053 (0.3264)	
training:	Epoch: [10][33/204]	Loss 0.6537 (0.3363)	
training:	Epoch: [10][34/204]	Loss 0.3411 (0.3365)	
training:	Epoch: [10][35/204]	Loss 0.3665 (0.3373)	
training:	Epoch: [10][36/204]	Loss 0.4158 (0.3395)	
training:	Epoch: [10][37/204]	Loss 0.4501 (0.3425)	
training:	Epoch: [10][38/204]	Loss 0.2274 (0.3395)	
training:	Epoch: [10][39/204]	Loss 0.3906 (0.3408)	
training:	Epoch: [10][40/204]	Loss 0.1894 (0.3370)	
training:	Epoch: [10][41/204]	Loss 0.3161 (0.3365)	
training:	Epoch: [10][42/204]	Loss 0.3452 (0.3367)	
training:	Epoch: [10][43/204]	Loss 0.5290 (0.3412)	
training:	Epoch: [10][44/204]	Loss 0.3124 (0.3405)	
training:	Epoch: [10][45/204]	Loss 0.5719 (0.3457)	
training:	Epoch: [10][46/204]	Loss 0.1776 (0.3420)	
training:	Epoch: [10][47/204]	Loss 0.3164 (0.3415)	
training:	Epoch: [10][48/204]	Loss 0.3936 (0.3425)	
training:	Epoch: [10][49/204]	Loss 0.3096 (0.3419)	
training:	Epoch: [10][50/204]	Loss 0.4871 (0.3448)	
training:	Epoch: [10][51/204]	Loss 0.3181 (0.3443)	
training:	Epoch: [10][52/204]	Loss 0.3081 (0.3436)	
training:	Epoch: [10][53/204]	Loss 0.5613 (0.3477)	
training:	Epoch: [10][54/204]	Loss 0.2660 (0.3462)	
training:	Epoch: [10][55/204]	Loss 0.3922 (0.3470)	
training:	Epoch: [10][56/204]	Loss 0.6162 (0.3518)	
training:	Epoch: [10][57/204]	Loss 0.1470 (0.3482)	
training:	Epoch: [10][58/204]	Loss 0.2039 (0.3457)	
training:	Epoch: [10][59/204]	Loss 0.4569 (0.3476)	
training:	Epoch: [10][60/204]	Loss 0.2681 (0.3463)	
training:	Epoch: [10][61/204]	Loss 0.1928 (0.3438)	
training:	Epoch: [10][62/204]	Loss 0.2156 (0.3417)	
training:	Epoch: [10][63/204]	Loss 0.3540 (0.3419)	
training:	Epoch: [10][64/204]	Loss 0.3544 (0.3421)	
training:	Epoch: [10][65/204]	Loss 0.4872 (0.3443)	
training:	Epoch: [10][66/204]	Loss 0.3670 (0.3447)	
training:	Epoch: [10][67/204]	Loss 0.4128 (0.3457)	
training:	Epoch: [10][68/204]	Loss 0.2728 (0.3446)	
training:	Epoch: [10][69/204]	Loss 0.1628 (0.3420)	
training:	Epoch: [10][70/204]	Loss 0.2982 (0.3413)	
training:	Epoch: [10][71/204]	Loss 0.1759 (0.3390)	
training:	Epoch: [10][72/204]	Loss 0.1318 (0.3361)	
training:	Epoch: [10][73/204]	Loss 0.2497 (0.3350)	
training:	Epoch: [10][74/204]	Loss 0.2291 (0.3335)	
training:	Epoch: [10][75/204]	Loss 0.5220 (0.3360)	
training:	Epoch: [10][76/204]	Loss 0.4084 (0.3370)	
training:	Epoch: [10][77/204]	Loss 0.4257 (0.3381)	
training:	Epoch: [10][78/204]	Loss 0.3752 (0.3386)	
training:	Epoch: [10][79/204]	Loss 0.2467 (0.3375)	
training:	Epoch: [10][80/204]	Loss 0.3500 (0.3376)	
training:	Epoch: [10][81/204]	Loss 0.2230 (0.3362)	
training:	Epoch: [10][82/204]	Loss 0.4953 (0.3381)	
training:	Epoch: [10][83/204]	Loss 0.2305 (0.3368)	
training:	Epoch: [10][84/204]	Loss 0.4300 (0.3379)	
training:	Epoch: [10][85/204]	Loss 0.3162 (0.3377)	
training:	Epoch: [10][86/204]	Loss 0.3019 (0.3373)	
training:	Epoch: [10][87/204]	Loss 0.2099 (0.3358)	
training:	Epoch: [10][88/204]	Loss 0.2283 (0.3346)	
training:	Epoch: [10][89/204]	Loss 0.3508 (0.3348)	
training:	Epoch: [10][90/204]	Loss 0.3063 (0.3345)	
training:	Epoch: [10][91/204]	Loss 0.4391 (0.3356)	
training:	Epoch: [10][92/204]	Loss 0.3203 (0.3354)	
training:	Epoch: [10][93/204]	Loss 0.2834 (0.3349)	
training:	Epoch: [10][94/204]	Loss 0.3893 (0.3355)	
training:	Epoch: [10][95/204]	Loss 0.1807 (0.3338)	
training:	Epoch: [10][96/204]	Loss 0.2600 (0.3331)	
training:	Epoch: [10][97/204]	Loss 0.3302 (0.3330)	
training:	Epoch: [10][98/204]	Loss 0.3855 (0.3336)	
training:	Epoch: [10][99/204]	Loss 0.3228 (0.3335)	
training:	Epoch: [10][100/204]	Loss 0.4504 (0.3346)	
training:	Epoch: [10][101/204]	Loss 0.2494 (0.3338)	
training:	Epoch: [10][102/204]	Loss 0.3004 (0.3335)	
training:	Epoch: [10][103/204]	Loss 0.2846 (0.3330)	
training:	Epoch: [10][104/204]	Loss 0.3917 (0.3335)	
training:	Epoch: [10][105/204]	Loss 0.3474 (0.3337)	
training:	Epoch: [10][106/204]	Loss 0.2697 (0.3331)	
training:	Epoch: [10][107/204]	Loss 0.2341 (0.3321)	
training:	Epoch: [10][108/204]	Loss 0.2881 (0.3317)	
training:	Epoch: [10][109/204]	Loss 0.2420 (0.3309)	
training:	Epoch: [10][110/204]	Loss 0.2951 (0.3306)	
training:	Epoch: [10][111/204]	Loss 0.4269 (0.3315)	
training:	Epoch: [10][112/204]	Loss 0.2473 (0.3307)	
training:	Epoch: [10][113/204]	Loss 0.2812 (0.3303)	
training:	Epoch: [10][114/204]	Loss 0.2938 (0.3299)	
training:	Epoch: [10][115/204]	Loss 0.2315 (0.3291)	
training:	Epoch: [10][116/204]	Loss 0.1650 (0.3277)	
training:	Epoch: [10][117/204]	Loss 0.4492 (0.3287)	
training:	Epoch: [10][118/204]	Loss 0.5000 (0.3302)	
training:	Epoch: [10][119/204]	Loss 0.2946 (0.3299)	
training:	Epoch: [10][120/204]	Loss 0.2821 (0.3295)	
training:	Epoch: [10][121/204]	Loss 0.2125 (0.3285)	
training:	Epoch: [10][122/204]	Loss 0.5169 (0.3300)	
training:	Epoch: [10][123/204]	Loss 0.4457 (0.3310)	
training:	Epoch: [10][124/204]	Loss 0.2330 (0.3302)	
training:	Epoch: [10][125/204]	Loss 0.2856 (0.3298)	
training:	Epoch: [10][126/204]	Loss 0.4508 (0.3308)	
training:	Epoch: [10][127/204]	Loss 0.2579 (0.3302)	
training:	Epoch: [10][128/204]	Loss 0.2392 (0.3295)	
training:	Epoch: [10][129/204]	Loss 0.2280 (0.3287)	
training:	Epoch: [10][130/204]	Loss 0.2984 (0.3285)	
training:	Epoch: [10][131/204]	Loss 0.3956 (0.3290)	
training:	Epoch: [10][132/204]	Loss 0.2324 (0.3283)	
training:	Epoch: [10][133/204]	Loss 0.3082 (0.3281)	
training:	Epoch: [10][134/204]	Loss 0.4478 (0.3290)	
training:	Epoch: [10][135/204]	Loss 0.4990 (0.3303)	
training:	Epoch: [10][136/204]	Loss 0.3634 (0.3305)	
training:	Epoch: [10][137/204]	Loss 0.2592 (0.3300)	
training:	Epoch: [10][138/204]	Loss 0.3879 (0.3304)	
training:	Epoch: [10][139/204]	Loss 0.3872 (0.3308)	
training:	Epoch: [10][140/204]	Loss 0.4763 (0.3319)	
training:	Epoch: [10][141/204]	Loss 0.2978 (0.3316)	
training:	Epoch: [10][142/204]	Loss 0.3553 (0.3318)	
training:	Epoch: [10][143/204]	Loss 0.2884 (0.3315)	
training:	Epoch: [10][144/204]	Loss 0.4681 (0.3324)	
training:	Epoch: [10][145/204]	Loss 0.3377 (0.3325)	
training:	Epoch: [10][146/204]	Loss 0.1881 (0.3315)	
training:	Epoch: [10][147/204]	Loss 0.3423 (0.3316)	
training:	Epoch: [10][148/204]	Loss 0.2400 (0.3309)	
training:	Epoch: [10][149/204]	Loss 0.1105 (0.3295)	
training:	Epoch: [10][150/204]	Loss 0.2453 (0.3289)	
training:	Epoch: [10][151/204]	Loss 0.1919 (0.3280)	
training:	Epoch: [10][152/204]	Loss 0.3179 (0.3279)	
training:	Epoch: [10][153/204]	Loss 0.2851 (0.3276)	
training:	Epoch: [10][154/204]	Loss 0.4506 (0.3284)	
training:	Epoch: [10][155/204]	Loss 0.3629 (0.3287)	
training:	Epoch: [10][156/204]	Loss 0.2260 (0.3280)	
training:	Epoch: [10][157/204]	Loss 0.2104 (0.3273)	
training:	Epoch: [10][158/204]	Loss 0.1458 (0.3261)	
training:	Epoch: [10][159/204]	Loss 0.3045 (0.3260)	
training:	Epoch: [10][160/204]	Loss 0.3970 (0.3264)	
training:	Epoch: [10][161/204]	Loss 0.2731 (0.3261)	
training:	Epoch: [10][162/204]	Loss 0.3636 (0.3263)	
training:	Epoch: [10][163/204]	Loss 0.3432 (0.3264)	
training:	Epoch: [10][164/204]	Loss 0.2284 (0.3258)	
training:	Epoch: [10][165/204]	Loss 0.3009 (0.3257)	
training:	Epoch: [10][166/204]	Loss 0.3345 (0.3257)	
training:	Epoch: [10][167/204]	Loss 0.2234 (0.3251)	
training:	Epoch: [10][168/204]	Loss 0.2419 (0.3246)	
training:	Epoch: [10][169/204]	Loss 0.2684 (0.3243)	
training:	Epoch: [10][170/204]	Loss 0.3787 (0.3246)	
training:	Epoch: [10][171/204]	Loss 0.1917 (0.3238)	
training:	Epoch: [10][172/204]	Loss 0.3156 (0.3238)	
training:	Epoch: [10][173/204]	Loss 0.2789 (0.3235)	
training:	Epoch: [10][174/204]	Loss 0.2243 (0.3229)	
training:	Epoch: [10][175/204]	Loss 0.3811 (0.3233)	
training:	Epoch: [10][176/204]	Loss 0.3861 (0.3236)	
training:	Epoch: [10][177/204]	Loss 0.1918 (0.3229)	
training:	Epoch: [10][178/204]	Loss 0.2647 (0.3226)	
training:	Epoch: [10][179/204]	Loss 0.4377 (0.3232)	
training:	Epoch: [10][180/204]	Loss 0.1979 (0.3225)	
training:	Epoch: [10][181/204]	Loss 0.4390 (0.3232)	
training:	Epoch: [10][182/204]	Loss 0.2388 (0.3227)	
training:	Epoch: [10][183/204]	Loss 0.4046 (0.3231)	
training:	Epoch: [10][184/204]	Loss 0.2398 (0.3227)	
training:	Epoch: [10][185/204]	Loss 0.1708 (0.3219)	
training:	Epoch: [10][186/204]	Loss 0.2128 (0.3213)	
training:	Epoch: [10][187/204]	Loss 0.2692 (0.3210)	
training:	Epoch: [10][188/204]	Loss 0.1628 (0.3202)	
training:	Epoch: [10][189/204]	Loss 0.3873 (0.3205)	
training:	Epoch: [10][190/204]	Loss 0.3332 (0.3206)	
training:	Epoch: [10][191/204]	Loss 0.4234 (0.3211)	
training:	Epoch: [10][192/204]	Loss 0.5868 (0.3225)	
training:	Epoch: [10][193/204]	Loss 0.3205 (0.3225)	
training:	Epoch: [10][194/204]	Loss 0.3587 (0.3227)	
training:	Epoch: [10][195/204]	Loss 0.5091 (0.3236)	
training:	Epoch: [10][196/204]	Loss 0.2744 (0.3234)	
training:	Epoch: [10][197/204]	Loss 0.4909 (0.3242)	
training:	Epoch: [10][198/204]	Loss 0.4437 (0.3248)	
training:	Epoch: [10][199/204]	Loss 0.3454 (0.3249)	
training:	Epoch: [10][200/204]	Loss 0.2897 (0.3248)	
training:	Epoch: [10][201/204]	Loss 0.3715 (0.3250)	
training:	Epoch: [10][202/204]	Loss 0.2616 (0.3247)	
training:	Epoch: [10][203/204]	Loss 0.4298 (0.3252)	
training:	Epoch: [10][204/204]	Loss 0.3896 (0.3255)	
Training:	 Loss: 0.3250

Training:	 ACC: 0.8733 0.8753 0.9239 0.8227
Validation:	 ACC: 0.8131 0.8159 0.8751 0.7511
Validation:	 Best_BACC: 0.8326 0.8331 0.8424 0.8229
Validation:	 Loss: 0.4197
Pretraining:	Epoch 11/200
----------
training:	Epoch: [11][1/204]	Loss 0.1827 (0.1827)	
training:	Epoch: [11][2/204]	Loss 0.3253 (0.2540)	
training:	Epoch: [11][3/204]	Loss 0.3065 (0.2715)	
training:	Epoch: [11][4/204]	Loss 0.3247 (0.2848)	
training:	Epoch: [11][5/204]	Loss 0.0988 (0.2476)	
training:	Epoch: [11][6/204]	Loss 0.3656 (0.2673)	
training:	Epoch: [11][7/204]	Loss 0.5244 (0.3040)	
training:	Epoch: [11][8/204]	Loss 0.1544 (0.2853)	
training:	Epoch: [11][9/204]	Loss 0.2986 (0.2868)	
training:	Epoch: [11][10/204]	Loss 0.0963 (0.2677)	
training:	Epoch: [11][11/204]	Loss 0.2810 (0.2689)	
training:	Epoch: [11][12/204]	Loss 0.2096 (0.2640)	
training:	Epoch: [11][13/204]	Loss 0.2959 (0.2664)	
training:	Epoch: [11][14/204]	Loss 0.3527 (0.2726)	
training:	Epoch: [11][15/204]	Loss 0.2834 (0.2733)	
training:	Epoch: [11][16/204]	Loss 0.4028 (0.2814)	
training:	Epoch: [11][17/204]	Loss 0.2580 (0.2800)	
training:	Epoch: [11][18/204]	Loss 0.3439 (0.2836)	
training:	Epoch: [11][19/204]	Loss 0.2402 (0.2813)	
training:	Epoch: [11][20/204]	Loss 0.1939 (0.2769)	
training:	Epoch: [11][21/204]	Loss 0.4210 (0.2838)	
training:	Epoch: [11][22/204]	Loss 0.1743 (0.2788)	
training:	Epoch: [11][23/204]	Loss 0.1306 (0.2724)	
training:	Epoch: [11][24/204]	Loss 0.2317 (0.2707)	
training:	Epoch: [11][25/204]	Loss 0.4026 (0.2760)	
training:	Epoch: [11][26/204]	Loss 0.3672 (0.2795)	
training:	Epoch: [11][27/204]	Loss 0.1710 (0.2754)	
training:	Epoch: [11][28/204]	Loss 0.3264 (0.2773)	
training:	Epoch: [11][29/204]	Loss 0.3439 (0.2796)	
training:	Epoch: [11][30/204]	Loss 0.2299 (0.2779)	
training:	Epoch: [11][31/204]	Loss 0.2630 (0.2774)	
training:	Epoch: [11][32/204]	Loss 0.2180 (0.2756)	
training:	Epoch: [11][33/204]	Loss 0.2510 (0.2748)	
training:	Epoch: [11][34/204]	Loss 0.2595 (0.2744)	
training:	Epoch: [11][35/204]	Loss 0.3888 (0.2776)	
training:	Epoch: [11][36/204]	Loss 0.3170 (0.2787)	
training:	Epoch: [11][37/204]	Loss 0.1947 (0.2765)	
training:	Epoch: [11][38/204]	Loss 0.1702 (0.2737)	
training:	Epoch: [11][39/204]	Loss 0.2621 (0.2734)	
training:	Epoch: [11][40/204]	Loss 0.4504 (0.2778)	
training:	Epoch: [11][41/204]	Loss 0.3573 (0.2797)	
training:	Epoch: [11][42/204]	Loss 0.2846 (0.2799)	
training:	Epoch: [11][43/204]	Loss 0.2845 (0.2800)	
training:	Epoch: [11][44/204]	Loss 0.4533 (0.2839)	
training:	Epoch: [11][45/204]	Loss 0.5585 (0.2900)	
training:	Epoch: [11][46/204]	Loss 0.2462 (0.2891)	
training:	Epoch: [11][47/204]	Loss 0.5255 (0.2941)	
training:	Epoch: [11][48/204]	Loss 0.5347 (0.2991)	
training:	Epoch: [11][49/204]	Loss 0.2749 (0.2986)	
training:	Epoch: [11][50/204]	Loss 0.2790 (0.2982)	
training:	Epoch: [11][51/204]	Loss 0.5485 (0.3031)	
training:	Epoch: [11][52/204]	Loss 0.2938 (0.3029)	
training:	Epoch: [11][53/204]	Loss 0.1839 (0.3007)	
training:	Epoch: [11][54/204]	Loss 0.4278 (0.3030)	
training:	Epoch: [11][55/204]	Loss 0.4302 (0.3054)	
training:	Epoch: [11][56/204]	Loss 0.2515 (0.3044)	
training:	Epoch: [11][57/204]	Loss 0.3191 (0.3047)	
training:	Epoch: [11][58/204]	Loss 0.2344 (0.3034)	
training:	Epoch: [11][59/204]	Loss 0.2062 (0.3018)	
training:	Epoch: [11][60/204]	Loss 0.4510 (0.3043)	
training:	Epoch: [11][61/204]	Loss 0.2536 (0.3035)	
training:	Epoch: [11][62/204]	Loss 0.3407 (0.3041)	
training:	Epoch: [11][63/204]	Loss 0.2116 (0.3026)	
training:	Epoch: [11][64/204]	Loss 0.4555 (0.3050)	
training:	Epoch: [11][65/204]	Loss 0.3753 (0.3061)	
training:	Epoch: [11][66/204]	Loss 0.1982 (0.3044)	
training:	Epoch: [11][67/204]	Loss 0.2856 (0.3041)	
training:	Epoch: [11][68/204]	Loss 0.2324 (0.3031)	
training:	Epoch: [11][69/204]	Loss 0.3602 (0.3039)	
training:	Epoch: [11][70/204]	Loss 0.4006 (0.3053)	
training:	Epoch: [11][71/204]	Loss 0.3029 (0.3053)	
training:	Epoch: [11][72/204]	Loss 0.2655 (0.3047)	
training:	Epoch: [11][73/204]	Loss 0.2574 (0.3041)	
training:	Epoch: [11][74/204]	Loss 0.1360 (0.3018)	
training:	Epoch: [11][75/204]	Loss 0.2353 (0.3009)	
training:	Epoch: [11][76/204]	Loss 0.1591 (0.2990)	
training:	Epoch: [11][77/204]	Loss 0.2686 (0.2986)	
training:	Epoch: [11][78/204]	Loss 0.3097 (0.2988)	
training:	Epoch: [11][79/204]	Loss 0.3880 (0.2999)	
training:	Epoch: [11][80/204]	Loss 0.4267 (0.3015)	
training:	Epoch: [11][81/204]	Loss 0.3378 (0.3019)	
training:	Epoch: [11][82/204]	Loss 0.2738 (0.3016)	
training:	Epoch: [11][83/204]	Loss 0.2494 (0.3010)	
training:	Epoch: [11][84/204]	Loss 0.1797 (0.2995)	
training:	Epoch: [11][85/204]	Loss 0.3424 (0.3000)	
training:	Epoch: [11][86/204]	Loss 0.4852 (0.3022)	
training:	Epoch: [11][87/204]	Loss 0.3605 (0.3029)	
training:	Epoch: [11][88/204]	Loss 0.3375 (0.3033)	
training:	Epoch: [11][89/204]	Loss 0.4570 (0.3050)	
training:	Epoch: [11][90/204]	Loss 0.2016 (0.3038)	
training:	Epoch: [11][91/204]	Loss 0.2269 (0.3030)	
training:	Epoch: [11][92/204]	Loss 0.1613 (0.3014)	
training:	Epoch: [11][93/204]	Loss 0.2333 (0.3007)	
training:	Epoch: [11][94/204]	Loss 0.1481 (0.2991)	
training:	Epoch: [11][95/204]	Loss 0.3379 (0.2995)	
training:	Epoch: [11][96/204]	Loss 0.3485 (0.3000)	
training:	Epoch: [11][97/204]	Loss 0.3640 (0.3007)	
training:	Epoch: [11][98/204]	Loss 0.4363 (0.3021)	
training:	Epoch: [11][99/204]	Loss 0.3491 (0.3025)	
training:	Epoch: [11][100/204]	Loss 0.2670 (0.3022)	
training:	Epoch: [11][101/204]	Loss 0.4841 (0.3040)	
training:	Epoch: [11][102/204]	Loss 0.3462 (0.3044)	
training:	Epoch: [11][103/204]	Loss 0.2377 (0.3037)	
training:	Epoch: [11][104/204]	Loss 0.2639 (0.3034)	
training:	Epoch: [11][105/204]	Loss 0.3712 (0.3040)	
training:	Epoch: [11][106/204]	Loss 0.5097 (0.3059)	
training:	Epoch: [11][107/204]	Loss 0.3918 (0.3067)	
training:	Epoch: [11][108/204]	Loss 0.4707 (0.3083)	
training:	Epoch: [11][109/204]	Loss 0.2850 (0.3081)	
training:	Epoch: [11][110/204]	Loss 0.2103 (0.3072)	
training:	Epoch: [11][111/204]	Loss 0.2106 (0.3063)	
training:	Epoch: [11][112/204]	Loss 0.3236 (0.3064)	
training:	Epoch: [11][113/204]	Loss 0.4429 (0.3077)	
training:	Epoch: [11][114/204]	Loss 0.2563 (0.3072)	
training:	Epoch: [11][115/204]	Loss 0.2373 (0.3066)	
training:	Epoch: [11][116/204]	Loss 0.3855 (0.3073)	
training:	Epoch: [11][117/204]	Loss 0.2654 (0.3069)	
training:	Epoch: [11][118/204]	Loss 0.3194 (0.3070)	
training:	Epoch: [11][119/204]	Loss 0.3404 (0.3073)	
training:	Epoch: [11][120/204]	Loss 0.4454 (0.3085)	
training:	Epoch: [11][121/204]	Loss 0.1567 (0.3072)	
training:	Epoch: [11][122/204]	Loss 0.2784 (0.3070)	
training:	Epoch: [11][123/204]	Loss 0.3138 (0.3070)	
training:	Epoch: [11][124/204]	Loss 0.3137 (0.3071)	
training:	Epoch: [11][125/204]	Loss 0.3406 (0.3073)	
training:	Epoch: [11][126/204]	Loss 0.2739 (0.3071)	
training:	Epoch: [11][127/204]	Loss 0.5224 (0.3088)	
training:	Epoch: [11][128/204]	Loss 0.3525 (0.3091)	
training:	Epoch: [11][129/204]	Loss 0.3350 (0.3093)	
training:	Epoch: [11][130/204]	Loss 0.2538 (0.3089)	
training:	Epoch: [11][131/204]	Loss 0.2635 (0.3085)	
training:	Epoch: [11][132/204]	Loss 0.3896 (0.3092)	
training:	Epoch: [11][133/204]	Loss 0.1605 (0.3080)	
training:	Epoch: [11][134/204]	Loss 0.2905 (0.3079)	
training:	Epoch: [11][135/204]	Loss 0.5080 (0.3094)	
training:	Epoch: [11][136/204]	Loss 0.3562 (0.3097)	
training:	Epoch: [11][137/204]	Loss 0.3713 (0.3102)	
training:	Epoch: [11][138/204]	Loss 0.2040 (0.3094)	
training:	Epoch: [11][139/204]	Loss 0.3630 (0.3098)	
training:	Epoch: [11][140/204]	Loss 0.2913 (0.3097)	
training:	Epoch: [11][141/204]	Loss 0.2694 (0.3094)	
training:	Epoch: [11][142/204]	Loss 0.3099 (0.3094)	
training:	Epoch: [11][143/204]	Loss 0.2879 (0.3092)	
training:	Epoch: [11][144/204]	Loss 0.4710 (0.3104)	
training:	Epoch: [11][145/204]	Loss 0.4826 (0.3115)	
training:	Epoch: [11][146/204]	Loss 0.2932 (0.3114)	
training:	Epoch: [11][147/204]	Loss 0.1951 (0.3106)	
training:	Epoch: [11][148/204]	Loss 0.4744 (0.3117)	
training:	Epoch: [11][149/204]	Loss 0.4108 (0.3124)	
training:	Epoch: [11][150/204]	Loss 0.1536 (0.3113)	
training:	Epoch: [11][151/204]	Loss 0.3890 (0.3119)	
training:	Epoch: [11][152/204]	Loss 0.2120 (0.3112)	
training:	Epoch: [11][153/204]	Loss 0.2341 (0.3107)	
training:	Epoch: [11][154/204]	Loss 0.3805 (0.3111)	
training:	Epoch: [11][155/204]	Loss 0.3269 (0.3113)	
training:	Epoch: [11][156/204]	Loss 0.3935 (0.3118)	
training:	Epoch: [11][157/204]	Loss 0.4390 (0.3126)	
training:	Epoch: [11][158/204]	Loss 0.3499 (0.3128)	
training:	Epoch: [11][159/204]	Loss 0.3058 (0.3128)	
training:	Epoch: [11][160/204]	Loss 0.2704 (0.3125)	
training:	Epoch: [11][161/204]	Loss 0.2473 (0.3121)	
training:	Epoch: [11][162/204]	Loss 0.2251 (0.3116)	
training:	Epoch: [11][163/204]	Loss 0.3339 (0.3117)	
training:	Epoch: [11][164/204]	Loss 0.3149 (0.3117)	
training:	Epoch: [11][165/204]	Loss 0.2854 (0.3116)	
training:	Epoch: [11][166/204]	Loss 0.6066 (0.3133)	
training:	Epoch: [11][167/204]	Loss 0.2062 (0.3127)	
training:	Epoch: [11][168/204]	Loss 0.2865 (0.3126)	
training:	Epoch: [11][169/204]	Loss 0.4469 (0.3133)	
training:	Epoch: [11][170/204]	Loss 0.3043 (0.3133)	
training:	Epoch: [11][171/204]	Loss 0.3473 (0.3135)	
training:	Epoch: [11][172/204]	Loss 0.3203 (0.3135)	
training:	Epoch: [11][173/204]	Loss 0.5229 (0.3147)	
training:	Epoch: [11][174/204]	Loss 0.1780 (0.3140)	
training:	Epoch: [11][175/204]	Loss 0.2973 (0.3139)	
training:	Epoch: [11][176/204]	Loss 0.3623 (0.3141)	
training:	Epoch: [11][177/204]	Loss 0.2973 (0.3140)	
training:	Epoch: [11][178/204]	Loss 0.2619 (0.3137)	
training:	Epoch: [11][179/204]	Loss 0.2040 (0.3131)	
training:	Epoch: [11][180/204]	Loss 0.2539 (0.3128)	
training:	Epoch: [11][181/204]	Loss 0.3465 (0.3130)	
training:	Epoch: [11][182/204]	Loss 0.2801 (0.3128)	
training:	Epoch: [11][183/204]	Loss 0.4035 (0.3133)	
training:	Epoch: [11][184/204]	Loss 0.4013 (0.3138)	
training:	Epoch: [11][185/204]	Loss 0.3018 (0.3137)	
training:	Epoch: [11][186/204]	Loss 0.2146 (0.3132)	
training:	Epoch: [11][187/204]	Loss 0.2433 (0.3128)	
training:	Epoch: [11][188/204]	Loss 0.3367 (0.3129)	
training:	Epoch: [11][189/204]	Loss 0.3379 (0.3131)	
training:	Epoch: [11][190/204]	Loss 0.4118 (0.3136)	
training:	Epoch: [11][191/204]	Loss 0.2422 (0.3132)	
training:	Epoch: [11][192/204]	Loss 0.2701 (0.3130)	
training:	Epoch: [11][193/204]	Loss 0.2222 (0.3125)	
training:	Epoch: [11][194/204]	Loss 0.3055 (0.3125)	
training:	Epoch: [11][195/204]	Loss 0.3968 (0.3129)	
training:	Epoch: [11][196/204]	Loss 0.3554 (0.3131)	
training:	Epoch: [11][197/204]	Loss 0.5336 (0.3143)	
training:	Epoch: [11][198/204]	Loss 0.2467 (0.3139)	
training:	Epoch: [11][199/204]	Loss 0.3142 (0.3139)	
training:	Epoch: [11][200/204]	Loss 0.2367 (0.3135)	
training:	Epoch: [11][201/204]	Loss 0.1225 (0.3126)	
training:	Epoch: [11][202/204]	Loss 0.3357 (0.3127)	
training:	Epoch: [11][203/204]	Loss 0.2837 (0.3125)	
training:	Epoch: [11][204/204]	Loss 0.1308 (0.3117)	
Training:	 Loss: 0.3112

Training:	 ACC: 0.8850 0.8862 0.9148 0.8552
Validation:	 ACC: 0.8238 0.8256 0.8639 0.7836
Validation:	 Best_BACC: 0.8326 0.8331 0.8424 0.8229
Validation:	 Loss: 0.4100
Pretraining:	Epoch 12/200
----------
training:	Epoch: [12][1/204]	Loss 0.5397 (0.5397)	
training:	Epoch: [12][2/204]	Loss 0.3177 (0.4287)	
training:	Epoch: [12][3/204]	Loss 0.3052 (0.3875)	
training:	Epoch: [12][4/204]	Loss 0.2945 (0.3643)	
training:	Epoch: [12][5/204]	Loss 0.1996 (0.3314)	
training:	Epoch: [12][6/204]	Loss 0.3523 (0.3348)	
training:	Epoch: [12][7/204]	Loss 0.2836 (0.3275)	
training:	Epoch: [12][8/204]	Loss 0.2123 (0.3131)	
training:	Epoch: [12][9/204]	Loss 0.1424 (0.2942)	
training:	Epoch: [12][10/204]	Loss 0.4119 (0.3059)	
training:	Epoch: [12][11/204]	Loss 0.1915 (0.2955)	
training:	Epoch: [12][12/204]	Loss 0.2565 (0.2923)	
training:	Epoch: [12][13/204]	Loss 0.2437 (0.2885)	
training:	Epoch: [12][14/204]	Loss 0.3584 (0.2935)	
training:	Epoch: [12][15/204]	Loss 0.2182 (0.2885)	
training:	Epoch: [12][16/204]	Loss 0.3838 (0.2945)	
training:	Epoch: [12][17/204]	Loss 0.2816 (0.2937)	
training:	Epoch: [12][18/204]	Loss 0.2770 (0.2928)	
training:	Epoch: [12][19/204]	Loss 0.2177 (0.2888)	
training:	Epoch: [12][20/204]	Loss 0.2886 (0.2888)	
training:	Epoch: [12][21/204]	Loss 0.1703 (0.2832)	
training:	Epoch: [12][22/204]	Loss 0.3310 (0.2853)	
training:	Epoch: [12][23/204]	Loss 0.1713 (0.2804)	
training:	Epoch: [12][24/204]	Loss 0.3456 (0.2831)	
training:	Epoch: [12][25/204]	Loss 0.2155 (0.2804)	
training:	Epoch: [12][26/204]	Loss 0.3564 (0.2833)	
training:	Epoch: [12][27/204]	Loss 0.4726 (0.2903)	
training:	Epoch: [12][28/204]	Loss 0.3291 (0.2917)	
training:	Epoch: [12][29/204]	Loss 0.3198 (0.2927)	
training:	Epoch: [12][30/204]	Loss 0.2180 (0.2902)	
training:	Epoch: [12][31/204]	Loss 0.4078 (0.2940)	
training:	Epoch: [12][32/204]	Loss 0.1893 (0.2907)	
training:	Epoch: [12][33/204]	Loss 0.3828 (0.2935)	
training:	Epoch: [12][34/204]	Loss 0.1406 (0.2890)	
training:	Epoch: [12][35/204]	Loss 0.3638 (0.2911)	
training:	Epoch: [12][36/204]	Loss 0.3734 (0.2934)	
training:	Epoch: [12][37/204]	Loss 0.3282 (0.2944)	
training:	Epoch: [12][38/204]	Loss 0.3414 (0.2956)	
training:	Epoch: [12][39/204]	Loss 0.3076 (0.2959)	
training:	Epoch: [12][40/204]	Loss 0.3435 (0.2971)	
training:	Epoch: [12][41/204]	Loss 0.2866 (0.2969)	
training:	Epoch: [12][42/204]	Loss 0.3602 (0.2984)	
training:	Epoch: [12][43/204]	Loss 0.3096 (0.2986)	
training:	Epoch: [12][44/204]	Loss 0.2622 (0.2978)	
training:	Epoch: [12][45/204]	Loss 0.3764 (0.2995)	
training:	Epoch: [12][46/204]	Loss 0.3279 (0.3002)	
training:	Epoch: [12][47/204]	Loss 0.3348 (0.3009)	
training:	Epoch: [12][48/204]	Loss 0.2696 (0.3002)	
training:	Epoch: [12][49/204]	Loss 0.3077 (0.3004)	
training:	Epoch: [12][50/204]	Loss 0.3109 (0.3006)	
training:	Epoch: [12][51/204]	Loss 0.2621 (0.2998)	
training:	Epoch: [12][52/204]	Loss 0.3333 (0.3005)	
training:	Epoch: [12][53/204]	Loss 0.4630 (0.3036)	
training:	Epoch: [12][54/204]	Loss 0.2455 (0.3025)	
training:	Epoch: [12][55/204]	Loss 0.2881 (0.3022)	
training:	Epoch: [12][56/204]	Loss 0.3472 (0.3030)	
training:	Epoch: [12][57/204]	Loss 0.4148 (0.3050)	
training:	Epoch: [12][58/204]	Loss 0.3356 (0.3055)	
training:	Epoch: [12][59/204]	Loss 0.4330 (0.3077)	
training:	Epoch: [12][60/204]	Loss 0.4760 (0.3105)	
training:	Epoch: [12][61/204]	Loss 0.2914 (0.3102)	
training:	Epoch: [12][62/204]	Loss 0.1224 (0.3071)	
training:	Epoch: [12][63/204]	Loss 0.4249 (0.3090)	
training:	Epoch: [12][64/204]	Loss 0.2393 (0.3079)	
training:	Epoch: [12][65/204]	Loss 0.3530 (0.3086)	
training:	Epoch: [12][66/204]	Loss 0.2030 (0.3070)	
training:	Epoch: [12][67/204]	Loss 0.2308 (0.3059)	
training:	Epoch: [12][68/204]	Loss 0.2828 (0.3055)	
training:	Epoch: [12][69/204]	Loss 0.3262 (0.3058)	
training:	Epoch: [12][70/204]	Loss 0.3756 (0.3068)	
training:	Epoch: [12][71/204]	Loss 0.2067 (0.3054)	
training:	Epoch: [12][72/204]	Loss 0.2948 (0.3053)	
training:	Epoch: [12][73/204]	Loss 0.2966 (0.3052)	
training:	Epoch: [12][74/204]	Loss 0.2876 (0.3049)	
training:	Epoch: [12][75/204]	Loss 0.1840 (0.3033)	
training:	Epoch: [12][76/204]	Loss 0.4749 (0.3056)	
training:	Epoch: [12][77/204]	Loss 0.1648 (0.3037)	
training:	Epoch: [12][78/204]	Loss 0.4588 (0.3057)	
training:	Epoch: [12][79/204]	Loss 0.2431 (0.3049)	
training:	Epoch: [12][80/204]	Loss 0.2208 (0.3039)	
training:	Epoch: [12][81/204]	Loss 0.3355 (0.3043)	
training:	Epoch: [12][82/204]	Loss 0.3488 (0.3048)	
training:	Epoch: [12][83/204]	Loss 0.3411 (0.3052)	
training:	Epoch: [12][84/204]	Loss 0.3532 (0.3058)	
training:	Epoch: [12][85/204]	Loss 0.2215 (0.3048)	
training:	Epoch: [12][86/204]	Loss 0.3129 (0.3049)	
training:	Epoch: [12][87/204]	Loss 0.2118 (0.3039)	
training:	Epoch: [12][88/204]	Loss 0.3966 (0.3049)	
training:	Epoch: [12][89/204]	Loss 0.3967 (0.3059)	
training:	Epoch: [12][90/204]	Loss 0.2240 (0.3050)	
training:	Epoch: [12][91/204]	Loss 0.3844 (0.3059)	
training:	Epoch: [12][92/204]	Loss 0.2710 (0.3055)	
training:	Epoch: [12][93/204]	Loss 0.2897 (0.3054)	
training:	Epoch: [12][94/204]	Loss 0.5508 (0.3080)	
training:	Epoch: [12][95/204]	Loss 0.1663 (0.3065)	
training:	Epoch: [12][96/204]	Loss 0.3217 (0.3066)	
training:	Epoch: [12][97/204]	Loss 0.3465 (0.3070)	
training:	Epoch: [12][98/204]	Loss 0.2162 (0.3061)	
training:	Epoch: [12][99/204]	Loss 0.2930 (0.3060)	
training:	Epoch: [12][100/204]	Loss 0.4995 (0.3079)	
training:	Epoch: [12][101/204]	Loss 0.3568 (0.3084)	
training:	Epoch: [12][102/204]	Loss 0.2743 (0.3081)	
training:	Epoch: [12][103/204]	Loss 0.2251 (0.3073)	
training:	Epoch: [12][104/204]	Loss 0.3148 (0.3073)	
training:	Epoch: [12][105/204]	Loss 0.3487 (0.3077)	
training:	Epoch: [12][106/204]	Loss 0.1516 (0.3063)	
training:	Epoch: [12][107/204]	Loss 0.2275 (0.3055)	
training:	Epoch: [12][108/204]	Loss 0.2142 (0.3047)	
training:	Epoch: [12][109/204]	Loss 0.3061 (0.3047)	
training:	Epoch: [12][110/204]	Loss 0.2490 (0.3042)	
training:	Epoch: [12][111/204]	Loss 0.2367 (0.3036)	
training:	Epoch: [12][112/204]	Loss 0.3767 (0.3042)	
training:	Epoch: [12][113/204]	Loss 0.5666 (0.3065)	
training:	Epoch: [12][114/204]	Loss 0.2049 (0.3057)	
training:	Epoch: [12][115/204]	Loss 0.3754 (0.3063)	
training:	Epoch: [12][116/204]	Loss 0.2091 (0.3054)	
training:	Epoch: [12][117/204]	Loss 0.3556 (0.3059)	
training:	Epoch: [12][118/204]	Loss 0.3032 (0.3058)	
training:	Epoch: [12][119/204]	Loss 0.2513 (0.3054)	
training:	Epoch: [12][120/204]	Loss 0.3684 (0.3059)	
training:	Epoch: [12][121/204]	Loss 0.2341 (0.3053)	
training:	Epoch: [12][122/204]	Loss 0.2238 (0.3046)	
training:	Epoch: [12][123/204]	Loss 0.2354 (0.3041)	
training:	Epoch: [12][124/204]	Loss 0.4020 (0.3049)	
training:	Epoch: [12][125/204]	Loss 0.4102 (0.3057)	
training:	Epoch: [12][126/204]	Loss 0.2928 (0.3056)	
training:	Epoch: [12][127/204]	Loss 0.2071 (0.3048)	
training:	Epoch: [12][128/204]	Loss 0.4208 (0.3057)	
training:	Epoch: [12][129/204]	Loss 0.2431 (0.3052)	
training:	Epoch: [12][130/204]	Loss 0.1548 (0.3041)	
training:	Epoch: [12][131/204]	Loss 0.3515 (0.3045)	
training:	Epoch: [12][132/204]	Loss 0.2518 (0.3041)	
training:	Epoch: [12][133/204]	Loss 0.3603 (0.3045)	
training:	Epoch: [12][134/204]	Loss 0.4830 (0.3058)	
training:	Epoch: [12][135/204]	Loss 0.3354 (0.3060)	
training:	Epoch: [12][136/204]	Loss 0.1900 (0.3052)	
training:	Epoch: [12][137/204]	Loss 0.3603 (0.3056)	
training:	Epoch: [12][138/204]	Loss 0.4227 (0.3064)	
training:	Epoch: [12][139/204]	Loss 0.3006 (0.3064)	
training:	Epoch: [12][140/204]	Loss 0.4660 (0.3075)	
training:	Epoch: [12][141/204]	Loss 0.1662 (0.3065)	
training:	Epoch: [12][142/204]	Loss 0.3176 (0.3066)	
training:	Epoch: [12][143/204]	Loss 0.2896 (0.3065)	
training:	Epoch: [12][144/204]	Loss 0.3039 (0.3065)	
training:	Epoch: [12][145/204]	Loss 0.4208 (0.3073)	
training:	Epoch: [12][146/204]	Loss 0.2114 (0.3066)	
training:	Epoch: [12][147/204]	Loss 0.2621 (0.3063)	
training:	Epoch: [12][148/204]	Loss 0.2934 (0.3062)	
training:	Epoch: [12][149/204]	Loss 0.3227 (0.3063)	
training:	Epoch: [12][150/204]	Loss 0.2307 (0.3058)	
training:	Epoch: [12][151/204]	Loss 0.3391 (0.3060)	
training:	Epoch: [12][152/204]	Loss 0.2202 (0.3055)	
training:	Epoch: [12][153/204]	Loss 0.3199 (0.3056)	
training:	Epoch: [12][154/204]	Loss 0.2242 (0.3050)	
training:	Epoch: [12][155/204]	Loss 0.4198 (0.3058)	
training:	Epoch: [12][156/204]	Loss 0.3481 (0.3060)	
training:	Epoch: [12][157/204]	Loss 0.2091 (0.3054)	
training:	Epoch: [12][158/204]	Loss 0.5412 (0.3069)	
training:	Epoch: [12][159/204]	Loss 0.4173 (0.3076)	
training:	Epoch: [12][160/204]	Loss 0.3051 (0.3076)	
training:	Epoch: [12][161/204]	Loss 0.3694 (0.3080)	
training:	Epoch: [12][162/204]	Loss 0.2540 (0.3076)	
training:	Epoch: [12][163/204]	Loss 0.3690 (0.3080)	
training:	Epoch: [12][164/204]	Loss 0.3250 (0.3081)	
training:	Epoch: [12][165/204]	Loss 0.2028 (0.3075)	
training:	Epoch: [12][166/204]	Loss 0.1233 (0.3064)	
training:	Epoch: [12][167/204]	Loss 0.2981 (0.3063)	
training:	Epoch: [12][168/204]	Loss 0.3545 (0.3066)	
training:	Epoch: [12][169/204]	Loss 0.3540 (0.3069)	
training:	Epoch: [12][170/204]	Loss 0.2442 (0.3065)	
training:	Epoch: [12][171/204]	Loss 0.3969 (0.3071)	
training:	Epoch: [12][172/204]	Loss 0.4628 (0.3080)	
training:	Epoch: [12][173/204]	Loss 0.1742 (0.3072)	
training:	Epoch: [12][174/204]	Loss 0.2679 (0.3070)	
training:	Epoch: [12][175/204]	Loss 0.4503 (0.3078)	
training:	Epoch: [12][176/204]	Loss 0.1495 (0.3069)	
training:	Epoch: [12][177/204]	Loss 0.2184 (0.3064)	
training:	Epoch: [12][178/204]	Loss 0.2152 (0.3059)	
training:	Epoch: [12][179/204]	Loss 0.2963 (0.3058)	
training:	Epoch: [12][180/204]	Loss 0.2652 (0.3056)	
training:	Epoch: [12][181/204]	Loss 0.2814 (0.3055)	
training:	Epoch: [12][182/204]	Loss 0.1328 (0.3045)	
training:	Epoch: [12][183/204]	Loss 0.3101 (0.3045)	
training:	Epoch: [12][184/204]	Loss 0.3146 (0.3046)	
training:	Epoch: [12][185/204]	Loss 0.1965 (0.3040)	
training:	Epoch: [12][186/204]	Loss 0.3154 (0.3041)	
training:	Epoch: [12][187/204]	Loss 0.3110 (0.3041)	
training:	Epoch: [12][188/204]	Loss 0.3004 (0.3041)	
training:	Epoch: [12][189/204]	Loss 0.2298 (0.3037)	
training:	Epoch: [12][190/204]	Loss 0.1654 (0.3030)	
training:	Epoch: [12][191/204]	Loss 0.1672 (0.3023)	
training:	Epoch: [12][192/204]	Loss 0.2947 (0.3022)	
training:	Epoch: [12][193/204]	Loss 0.4043 (0.3027)	
training:	Epoch: [12][194/204]	Loss 0.4181 (0.3033)	
training:	Epoch: [12][195/204]	Loss 0.2878 (0.3033)	
training:	Epoch: [12][196/204]	Loss 0.4868 (0.3042)	
training:	Epoch: [12][197/204]	Loss 0.1933 (0.3036)	
training:	Epoch: [12][198/204]	Loss 0.2336 (0.3033)	
training:	Epoch: [12][199/204]	Loss 0.2300 (0.3029)	
training:	Epoch: [12][200/204]	Loss 0.1788 (0.3023)	
training:	Epoch: [12][201/204]	Loss 0.4149 (0.3029)	
training:	Epoch: [12][202/204]	Loss 0.1867 (0.3023)	
training:	Epoch: [12][203/204]	Loss 0.5533 (0.3035)	
training:	Epoch: [12][204/204]	Loss 0.3760 (0.3039)	
Training:	 Loss: 0.3034

Training:	 ACC: 0.9000 0.8991 0.8760 0.9241
Validation:	 ACC: 0.8278 0.8272 0.8137 0.8419
Validation:	 Best_BACC: 0.8326 0.8331 0.8424 0.8229
Validation:	 Loss: 0.4060
Pretraining:	Epoch 13/200
----------
training:	Epoch: [13][1/204]	Loss 0.4192 (0.4192)	
training:	Epoch: [13][2/204]	Loss 0.2602 (0.3397)	
training:	Epoch: [13][3/204]	Loss 0.3773 (0.3522)	
training:	Epoch: [13][4/204]	Loss 0.2596 (0.3291)	
training:	Epoch: [13][5/204]	Loss 0.2151 (0.3063)	
training:	Epoch: [13][6/204]	Loss 0.1717 (0.2838)	
training:	Epoch: [13][7/204]	Loss 0.2016 (0.2721)	
training:	Epoch: [13][8/204]	Loss 0.2882 (0.2741)	
training:	Epoch: [13][9/204]	Loss 0.1990 (0.2658)	
training:	Epoch: [13][10/204]	Loss 0.3261 (0.2718)	
training:	Epoch: [13][11/204]	Loss 0.1052 (0.2567)	
training:	Epoch: [13][12/204]	Loss 0.2089 (0.2527)	
training:	Epoch: [13][13/204]	Loss 0.3676 (0.2615)	
training:	Epoch: [13][14/204]	Loss 0.2617 (0.2615)	
training:	Epoch: [13][15/204]	Loss 0.2878 (0.2633)	
training:	Epoch: [13][16/204]	Loss 0.2655 (0.2634)	
training:	Epoch: [13][17/204]	Loss 0.4027 (0.2716)	
training:	Epoch: [13][18/204]	Loss 0.2654 (0.2713)	
training:	Epoch: [13][19/204]	Loss 0.3121 (0.2734)	
training:	Epoch: [13][20/204]	Loss 0.3010 (0.2748)	
training:	Epoch: [13][21/204]	Loss 0.2742 (0.2748)	
training:	Epoch: [13][22/204]	Loss 0.2382 (0.2731)	
training:	Epoch: [13][23/204]	Loss 0.3120 (0.2748)	
training:	Epoch: [13][24/204]	Loss 0.3426 (0.2776)	
training:	Epoch: [13][25/204]	Loss 0.2875 (0.2780)	
training:	Epoch: [13][26/204]	Loss 0.2657 (0.2775)	
training:	Epoch: [13][27/204]	Loss 0.2640 (0.2770)	
training:	Epoch: [13][28/204]	Loss 0.1699 (0.2732)	
training:	Epoch: [13][29/204]	Loss 0.3308 (0.2752)	
training:	Epoch: [13][30/204]	Loss 0.3626 (0.2781)	
training:	Epoch: [13][31/204]	Loss 0.1311 (0.2734)	
training:	Epoch: [13][32/204]	Loss 0.3050 (0.2744)	
training:	Epoch: [13][33/204]	Loss 0.3993 (0.2781)	
training:	Epoch: [13][34/204]	Loss 0.3932 (0.2815)	
training:	Epoch: [13][35/204]	Loss 0.2767 (0.2814)	
training:	Epoch: [13][36/204]	Loss 0.1746 (0.2784)	
training:	Epoch: [13][37/204]	Loss 0.2871 (0.2787)	
training:	Epoch: [13][38/204]	Loss 0.2397 (0.2776)	
training:	Epoch: [13][39/204]	Loss 0.2651 (0.2773)	
training:	Epoch: [13][40/204]	Loss 0.1198 (0.2734)	
training:	Epoch: [13][41/204]	Loss 0.3362 (0.2749)	
training:	Epoch: [13][42/204]	Loss 0.3692 (0.2771)	
training:	Epoch: [13][43/204]	Loss 0.2174 (0.2758)	
training:	Epoch: [13][44/204]	Loss 0.2759 (0.2758)	
training:	Epoch: [13][45/204]	Loss 0.3735 (0.2779)	
training:	Epoch: [13][46/204]	Loss 0.3567 (0.2796)	
training:	Epoch: [13][47/204]	Loss 0.2083 (0.2781)	
training:	Epoch: [13][48/204]	Loss 0.1494 (0.2754)	
training:	Epoch: [13][49/204]	Loss 0.3842 (0.2777)	
training:	Epoch: [13][50/204]	Loss 0.4715 (0.2815)	
training:	Epoch: [13][51/204]	Loss 0.3114 (0.2821)	
training:	Epoch: [13][52/204]	Loss 0.3115 (0.2827)	
training:	Epoch: [13][53/204]	Loss 0.3140 (0.2833)	
training:	Epoch: [13][54/204]	Loss 0.2475 (0.2826)	
training:	Epoch: [13][55/204]	Loss 0.2413 (0.2819)	
training:	Epoch: [13][56/204]	Loss 0.4377 (0.2847)	
training:	Epoch: [13][57/204]	Loss 0.2591 (0.2842)	
training:	Epoch: [13][58/204]	Loss 0.2059 (0.2829)	
training:	Epoch: [13][59/204]	Loss 0.3336 (0.2837)	
training:	Epoch: [13][60/204]	Loss 0.2608 (0.2833)	
training:	Epoch: [13][61/204]	Loss 0.4283 (0.2857)	
training:	Epoch: [13][62/204]	Loss 0.5845 (0.2905)	
training:	Epoch: [13][63/204]	Loss 0.0913 (0.2874)	
training:	Epoch: [13][64/204]	Loss 0.3602 (0.2885)	
training:	Epoch: [13][65/204]	Loss 0.3043 (0.2887)	
training:	Epoch: [13][66/204]	Loss 0.2907 (0.2888)	
training:	Epoch: [13][67/204]	Loss 0.2789 (0.2886)	
training:	Epoch: [13][68/204]	Loss 0.3043 (0.2889)	
training:	Epoch: [13][69/204]	Loss 0.2673 (0.2885)	
training:	Epoch: [13][70/204]	Loss 0.2415 (0.2879)	
training:	Epoch: [13][71/204]	Loss 0.1064 (0.2853)	
training:	Epoch: [13][72/204]	Loss 0.5385 (0.2888)	
training:	Epoch: [13][73/204]	Loss 0.3029 (0.2890)	
training:	Epoch: [13][74/204]	Loss 0.2856 (0.2890)	
training:	Epoch: [13][75/204]	Loss 0.1322 (0.2869)	
training:	Epoch: [13][76/204]	Loss 0.2756 (0.2867)	
training:	Epoch: [13][77/204]	Loss 0.2940 (0.2868)	
training:	Epoch: [13][78/204]	Loss 0.4449 (0.2889)	
training:	Epoch: [13][79/204]	Loss 0.3078 (0.2891)	
training:	Epoch: [13][80/204]	Loss 0.5016 (0.2918)	
training:	Epoch: [13][81/204]	Loss 0.3105 (0.2920)	
training:	Epoch: [13][82/204]	Loss 0.2715 (0.2917)	
training:	Epoch: [13][83/204]	Loss 0.2681 (0.2915)	
training:	Epoch: [13][84/204]	Loss 0.3481 (0.2921)	
training:	Epoch: [13][85/204]	Loss 0.2677 (0.2918)	
training:	Epoch: [13][86/204]	Loss 0.2721 (0.2916)	
training:	Epoch: [13][87/204]	Loss 0.2828 (0.2915)	
training:	Epoch: [13][88/204]	Loss 0.1253 (0.2896)	
training:	Epoch: [13][89/204]	Loss 0.1980 (0.2886)	
training:	Epoch: [13][90/204]	Loss 0.2312 (0.2880)	
training:	Epoch: [13][91/204]	Loss 0.3161 (0.2883)	
training:	Epoch: [13][92/204]	Loss 0.2725 (0.2881)	
training:	Epoch: [13][93/204]	Loss 0.2655 (0.2878)	
training:	Epoch: [13][94/204]	Loss 0.3751 (0.2888)	
training:	Epoch: [13][95/204]	Loss 0.1234 (0.2870)	
training:	Epoch: [13][96/204]	Loss 0.2282 (0.2864)	
training:	Epoch: [13][97/204]	Loss 0.2706 (0.2863)	
training:	Epoch: [13][98/204]	Loss 0.4396 (0.2878)	
training:	Epoch: [13][99/204]	Loss 0.3966 (0.2889)	
training:	Epoch: [13][100/204]	Loss 0.2509 (0.2885)	
training:	Epoch: [13][101/204]	Loss 0.2149 (0.2878)	
training:	Epoch: [13][102/204]	Loss 0.2345 (0.2873)	
training:	Epoch: [13][103/204]	Loss 0.3476 (0.2879)	
training:	Epoch: [13][104/204]	Loss 0.3008 (0.2880)	
training:	Epoch: [13][105/204]	Loss 0.4680 (0.2897)	
training:	Epoch: [13][106/204]	Loss 0.2752 (0.2896)	
training:	Epoch: [13][107/204]	Loss 0.3802 (0.2904)	
training:	Epoch: [13][108/204]	Loss 0.1087 (0.2887)	
training:	Epoch: [13][109/204]	Loss 0.2734 (0.2886)	
training:	Epoch: [13][110/204]	Loss 0.3409 (0.2891)	
training:	Epoch: [13][111/204]	Loss 0.3762 (0.2899)	
training:	Epoch: [13][112/204]	Loss 0.3665 (0.2905)	
training:	Epoch: [13][113/204]	Loss 0.4863 (0.2923)	
training:	Epoch: [13][114/204]	Loss 0.2444 (0.2919)	
training:	Epoch: [13][115/204]	Loss 0.3111 (0.2920)	
training:	Epoch: [13][116/204]	Loss 0.3587 (0.2926)	
training:	Epoch: [13][117/204]	Loss 0.1494 (0.2914)	
training:	Epoch: [13][118/204]	Loss 0.2862 (0.2913)	
training:	Epoch: [13][119/204]	Loss 0.3300 (0.2917)	
training:	Epoch: [13][120/204]	Loss 0.3389 (0.2921)	
training:	Epoch: [13][121/204]	Loss 0.2491 (0.2917)	
training:	Epoch: [13][122/204]	Loss 0.2035 (0.2910)	
training:	Epoch: [13][123/204]	Loss 0.3124 (0.2911)	
training:	Epoch: [13][124/204]	Loss 0.3637 (0.2917)	
training:	Epoch: [13][125/204]	Loss 0.2953 (0.2918)	
training:	Epoch: [13][126/204]	Loss 0.1607 (0.2907)	
training:	Epoch: [13][127/204]	Loss 0.3235 (0.2910)	
training:	Epoch: [13][128/204]	Loss 0.2248 (0.2905)	
training:	Epoch: [13][129/204]	Loss 0.2689 (0.2903)	
training:	Epoch: [13][130/204]	Loss 0.2152 (0.2897)	
training:	Epoch: [13][131/204]	Loss 0.4436 (0.2909)	
training:	Epoch: [13][132/204]	Loss 0.2110 (0.2903)	
training:	Epoch: [13][133/204]	Loss 0.2841 (0.2902)	
training:	Epoch: [13][134/204]	Loss 0.2636 (0.2900)	
training:	Epoch: [13][135/204]	Loss 0.4417 (0.2912)	
training:	Epoch: [13][136/204]	Loss 0.2177 (0.2906)	
training:	Epoch: [13][137/204]	Loss 0.2122 (0.2901)	
training:	Epoch: [13][138/204]	Loss 0.3965 (0.2908)	
training:	Epoch: [13][139/204]	Loss 0.2894 (0.2908)	
training:	Epoch: [13][140/204]	Loss 0.3428 (0.2912)	
training:	Epoch: [13][141/204]	Loss 0.4571 (0.2924)	
training:	Epoch: [13][142/204]	Loss 0.3641 (0.2929)	
training:	Epoch: [13][143/204]	Loss 0.2507 (0.2926)	
training:	Epoch: [13][144/204]	Loss 0.1905 (0.2919)	
training:	Epoch: [13][145/204]	Loss 0.3462 (0.2922)	
training:	Epoch: [13][146/204]	Loss 0.3840 (0.2929)	
training:	Epoch: [13][147/204]	Loss 0.2886 (0.2928)	
training:	Epoch: [13][148/204]	Loss 0.2964 (0.2929)	
training:	Epoch: [13][149/204]	Loss 0.1677 (0.2920)	
training:	Epoch: [13][150/204]	Loss 0.2650 (0.2918)	
training:	Epoch: [13][151/204]	Loss 0.4961 (0.2932)	
training:	Epoch: [13][152/204]	Loss 0.2202 (0.2927)	
training:	Epoch: [13][153/204]	Loss 0.1333 (0.2917)	
training:	Epoch: [13][154/204]	Loss 0.1439 (0.2907)	
training:	Epoch: [13][155/204]	Loss 0.2227 (0.2903)	
training:	Epoch: [13][156/204]	Loss 0.3058 (0.2904)	
training:	Epoch: [13][157/204]	Loss 0.2861 (0.2903)	
training:	Epoch: [13][158/204]	Loss 0.3088 (0.2905)	
training:	Epoch: [13][159/204]	Loss 0.1915 (0.2898)	
training:	Epoch: [13][160/204]	Loss 0.2038 (0.2893)	
training:	Epoch: [13][161/204]	Loss 0.2956 (0.2893)	
training:	Epoch: [13][162/204]	Loss 0.3889 (0.2900)	
training:	Epoch: [13][163/204]	Loss 0.3120 (0.2901)	
training:	Epoch: [13][164/204]	Loss 0.3351 (0.2904)	
training:	Epoch: [13][165/204]	Loss 0.3724 (0.2909)	
training:	Epoch: [13][166/204]	Loss 0.2408 (0.2906)	
training:	Epoch: [13][167/204]	Loss 0.5640 (0.2922)	
training:	Epoch: [13][168/204]	Loss 0.1863 (0.2916)	
training:	Epoch: [13][169/204]	Loss 0.3053 (0.2916)	
training:	Epoch: [13][170/204]	Loss 0.2363 (0.2913)	
training:	Epoch: [13][171/204]	Loss 0.4156 (0.2921)	
training:	Epoch: [13][172/204]	Loss 0.2385 (0.2917)	
training:	Epoch: [13][173/204]	Loss 0.2507 (0.2915)	
training:	Epoch: [13][174/204]	Loss 0.1824 (0.2909)	
training:	Epoch: [13][175/204]	Loss 0.3531 (0.2912)	
training:	Epoch: [13][176/204]	Loss 0.2971 (0.2913)	
training:	Epoch: [13][177/204]	Loss 0.3289 (0.2915)	
training:	Epoch: [13][178/204]	Loss 0.2990 (0.2915)	
training:	Epoch: [13][179/204]	Loss 0.1984 (0.2910)	
training:	Epoch: [13][180/204]	Loss 0.2982 (0.2910)	
training:	Epoch: [13][181/204]	Loss 0.5354 (0.2924)	
training:	Epoch: [13][182/204]	Loss 0.3398 (0.2926)	
training:	Epoch: [13][183/204]	Loss 0.1937 (0.2921)	
training:	Epoch: [13][184/204]	Loss 0.1740 (0.2915)	
training:	Epoch: [13][185/204]	Loss 0.2192 (0.2911)	
training:	Epoch: [13][186/204]	Loss 0.1732 (0.2904)	
training:	Epoch: [13][187/204]	Loss 0.2207 (0.2901)	
training:	Epoch: [13][188/204]	Loss 0.3154 (0.2902)	
training:	Epoch: [13][189/204]	Loss 0.2777 (0.2901)	
training:	Epoch: [13][190/204]	Loss 0.3144 (0.2903)	
training:	Epoch: [13][191/204]	Loss 0.3707 (0.2907)	
training:	Epoch: [13][192/204]	Loss 0.2834 (0.2906)	
training:	Epoch: [13][193/204]	Loss 0.3307 (0.2909)	
training:	Epoch: [13][194/204]	Loss 0.1724 (0.2902)	
training:	Epoch: [13][195/204]	Loss 0.2967 (0.2903)	
training:	Epoch: [13][196/204]	Loss 0.3440 (0.2906)	
training:	Epoch: [13][197/204]	Loss 0.3964 (0.2911)	
training:	Epoch: [13][198/204]	Loss 0.1716 (0.2905)	
training:	Epoch: [13][199/204]	Loss 0.3401 (0.2907)	
training:	Epoch: [13][200/204]	Loss 0.2591 (0.2906)	
training:	Epoch: [13][201/204]	Loss 0.3265 (0.2908)	
training:	Epoch: [13][202/204]	Loss 0.2064 (0.2903)	
training:	Epoch: [13][203/204]	Loss 0.4094 (0.2909)	
training:	Epoch: [13][204/204]	Loss 0.2460 (0.2907)	
Training:	 Loss: 0.2903

Training:	 ACC: 0.9040 0.9029 0.8771 0.9308
Validation:	 ACC: 0.8227 0.8218 0.8045 0.8408
Validation:	 Best_BACC: 0.8326 0.8331 0.8424 0.8229
Validation:	 Loss: 0.4096
Pretraining:	Epoch 14/200
----------
training:	Epoch: [14][1/204]	Loss 0.1774 (0.1774)	
training:	Epoch: [14][2/204]	Loss 0.1824 (0.1799)	
training:	Epoch: [14][3/204]	Loss 0.2180 (0.1926)	
training:	Epoch: [14][4/204]	Loss 0.1885 (0.1916)	
training:	Epoch: [14][5/204]	Loss 0.2955 (0.2124)	
training:	Epoch: [14][6/204]	Loss 0.1279 (0.1983)	
training:	Epoch: [14][7/204]	Loss 0.3238 (0.2162)	
training:	Epoch: [14][8/204]	Loss 0.3652 (0.2348)	
training:	Epoch: [14][9/204]	Loss 0.1564 (0.2261)	
training:	Epoch: [14][10/204]	Loss 0.2778 (0.2313)	
training:	Epoch: [14][11/204]	Loss 0.3125 (0.2387)	
training:	Epoch: [14][12/204]	Loss 0.2281 (0.2378)	
training:	Epoch: [14][13/204]	Loss 0.2046 (0.2352)	
training:	Epoch: [14][14/204]	Loss 0.3399 (0.2427)	
training:	Epoch: [14][15/204]	Loss 0.2770 (0.2450)	
training:	Epoch: [14][16/204]	Loss 0.3015 (0.2485)	
training:	Epoch: [14][17/204]	Loss 0.1552 (0.2430)	
training:	Epoch: [14][18/204]	Loss 0.2898 (0.2456)	
training:	Epoch: [14][19/204]	Loss 0.2787 (0.2474)	
training:	Epoch: [14][20/204]	Loss 0.0961 (0.2398)	
training:	Epoch: [14][21/204]	Loss 0.2025 (0.2380)	
training:	Epoch: [14][22/204]	Loss 0.3176 (0.2417)	
training:	Epoch: [14][23/204]	Loss 0.3181 (0.2450)	
training:	Epoch: [14][24/204]	Loss 0.2138 (0.2437)	
training:	Epoch: [14][25/204]	Loss 0.3409 (0.2476)	
training:	Epoch: [14][26/204]	Loss 0.3136 (0.2501)	
training:	Epoch: [14][27/204]	Loss 0.3160 (0.2525)	
training:	Epoch: [14][28/204]	Loss 0.3318 (0.2554)	
training:	Epoch: [14][29/204]	Loss 0.3534 (0.2588)	
training:	Epoch: [14][30/204]	Loss 0.1518 (0.2552)	
training:	Epoch: [14][31/204]	Loss 0.1041 (0.2503)	
training:	Epoch: [14][32/204]	Loss 0.2885 (0.2515)	
training:	Epoch: [14][33/204]	Loss 0.2926 (0.2528)	
training:	Epoch: [14][34/204]	Loss 0.2817 (0.2536)	
training:	Epoch: [14][35/204]	Loss 0.3962 (0.2577)	
training:	Epoch: [14][36/204]	Loss 0.1870 (0.2557)	
training:	Epoch: [14][37/204]	Loss 0.3487 (0.2582)	
training:	Epoch: [14][38/204]	Loss 0.1696 (0.2559)	
training:	Epoch: [14][39/204]	Loss 0.1776 (0.2539)	
training:	Epoch: [14][40/204]	Loss 0.2682 (0.2542)	
training:	Epoch: [14][41/204]	Loss 0.3638 (0.2569)	
training:	Epoch: [14][42/204]	Loss 0.3934 (0.2602)	
training:	Epoch: [14][43/204]	Loss 0.5603 (0.2671)	
training:	Epoch: [14][44/204]	Loss 0.2574 (0.2669)	
training:	Epoch: [14][45/204]	Loss 0.3368 (0.2685)	
training:	Epoch: [14][46/204]	Loss 0.4268 (0.2719)	
training:	Epoch: [14][47/204]	Loss 0.3471 (0.2735)	
training:	Epoch: [14][48/204]	Loss 0.4422 (0.2770)	
training:	Epoch: [14][49/204]	Loss 0.3058 (0.2776)	
training:	Epoch: [14][50/204]	Loss 0.1846 (0.2758)	
training:	Epoch: [14][51/204]	Loss 0.2516 (0.2753)	
training:	Epoch: [14][52/204]	Loss 0.1489 (0.2729)	
training:	Epoch: [14][53/204]	Loss 0.2447 (0.2723)	
training:	Epoch: [14][54/204]	Loss 0.3577 (0.2739)	
training:	Epoch: [14][55/204]	Loss 0.4416 (0.2770)	
training:	Epoch: [14][56/204]	Loss 0.1432 (0.2746)	
training:	Epoch: [14][57/204]	Loss 0.3772 (0.2764)	
training:	Epoch: [14][58/204]	Loss 0.1103 (0.2735)	
training:	Epoch: [14][59/204]	Loss 0.3367 (0.2746)	
training:	Epoch: [14][60/204]	Loss 0.3384 (0.2756)	
training:	Epoch: [14][61/204]	Loss 0.3115 (0.2762)	
training:	Epoch: [14][62/204]	Loss 0.2942 (0.2765)	
training:	Epoch: [14][63/204]	Loss 0.2132 (0.2755)	
training:	Epoch: [14][64/204]	Loss 0.3036 (0.2760)	
training:	Epoch: [14][65/204]	Loss 0.4467 (0.2786)	
training:	Epoch: [14][66/204]	Loss 0.1443 (0.2765)	
training:	Epoch: [14][67/204]	Loss 0.2062 (0.2755)	
training:	Epoch: [14][68/204]	Loss 0.3739 (0.2769)	
training:	Epoch: [14][69/204]	Loss 0.3422 (0.2779)	
training:	Epoch: [14][70/204]	Loss 0.1498 (0.2761)	
training:	Epoch: [14][71/204]	Loss 0.3071 (0.2765)	
training:	Epoch: [14][72/204]	Loss 0.3694 (0.2778)	
training:	Epoch: [14][73/204]	Loss 0.1594 (0.2762)	
training:	Epoch: [14][74/204]	Loss 0.3161 (0.2767)	
training:	Epoch: [14][75/204]	Loss 0.2687 (0.2766)	
training:	Epoch: [14][76/204]	Loss 0.3992 (0.2782)	
training:	Epoch: [14][77/204]	Loss 0.2923 (0.2784)	
training:	Epoch: [14][78/204]	Loss 0.1828 (0.2772)	
training:	Epoch: [14][79/204]	Loss 0.4250 (0.2790)	
training:	Epoch: [14][80/204]	Loss 0.1907 (0.2779)	
training:	Epoch: [14][81/204]	Loss 0.1604 (0.2765)	
training:	Epoch: [14][82/204]	Loss 0.3979 (0.2780)	
training:	Epoch: [14][83/204]	Loss 0.1723 (0.2767)	
training:	Epoch: [14][84/204]	Loss 0.1941 (0.2757)	
training:	Epoch: [14][85/204]	Loss 0.2583 (0.2755)	
training:	Epoch: [14][86/204]	Loss 0.1606 (0.2742)	
training:	Epoch: [14][87/204]	Loss 0.2639 (0.2741)	
training:	Epoch: [14][88/204]	Loss 0.2916 (0.2743)	
training:	Epoch: [14][89/204]	Loss 0.3041 (0.2746)	
training:	Epoch: [14][90/204]	Loss 0.3004 (0.2749)	
training:	Epoch: [14][91/204]	Loss 0.1903 (0.2739)	
training:	Epoch: [14][92/204]	Loss 0.2751 (0.2740)	
training:	Epoch: [14][93/204]	Loss 0.1873 (0.2730)	
training:	Epoch: [14][94/204]	Loss 0.2214 (0.2725)	
training:	Epoch: [14][95/204]	Loss 0.3448 (0.2732)	
training:	Epoch: [14][96/204]	Loss 0.2575 (0.2731)	
training:	Epoch: [14][97/204]	Loss 0.1381 (0.2717)	
training:	Epoch: [14][98/204]	Loss 0.1553 (0.2705)	
training:	Epoch: [14][99/204]	Loss 0.2142 (0.2699)	
training:	Epoch: [14][100/204]	Loss 0.3111 (0.2703)	
training:	Epoch: [14][101/204]	Loss 0.2108 (0.2697)	
training:	Epoch: [14][102/204]	Loss 0.2037 (0.2691)	
training:	Epoch: [14][103/204]	Loss 0.3662 (0.2700)	
training:	Epoch: [14][104/204]	Loss 0.2495 (0.2698)	
training:	Epoch: [14][105/204]	Loss 0.2146 (0.2693)	
training:	Epoch: [14][106/204]	Loss 0.1291 (0.2680)	
training:	Epoch: [14][107/204]	Loss 0.1912 (0.2673)	
training:	Epoch: [14][108/204]	Loss 0.1692 (0.2664)	
training:	Epoch: [14][109/204]	Loss 0.5802 (0.2692)	
training:	Epoch: [14][110/204]	Loss 0.3613 (0.2701)	
training:	Epoch: [14][111/204]	Loss 0.2448 (0.2699)	
training:	Epoch: [14][112/204]	Loss 0.3034 (0.2702)	
training:	Epoch: [14][113/204]	Loss 0.3196 (0.2706)	
training:	Epoch: [14][114/204]	Loss 0.1908 (0.2699)	
training:	Epoch: [14][115/204]	Loss 0.2865 (0.2700)	
training:	Epoch: [14][116/204]	Loss 0.2481 (0.2699)	
training:	Epoch: [14][117/204]	Loss 0.2041 (0.2693)	
training:	Epoch: [14][118/204]	Loss 0.2302 (0.2690)	
training:	Epoch: [14][119/204]	Loss 0.2006 (0.2684)	
training:	Epoch: [14][120/204]	Loss 0.2389 (0.2681)	
training:	Epoch: [14][121/204]	Loss 0.2128 (0.2677)	
training:	Epoch: [14][122/204]	Loss 0.3247 (0.2681)	
training:	Epoch: [14][123/204]	Loss 0.3220 (0.2686)	
training:	Epoch: [14][124/204]	Loss 0.2739 (0.2686)	
training:	Epoch: [14][125/204]	Loss 0.2249 (0.2683)	
training:	Epoch: [14][126/204]	Loss 0.3650 (0.2690)	
training:	Epoch: [14][127/204]	Loss 0.3265 (0.2695)	
training:	Epoch: [14][128/204]	Loss 0.2464 (0.2693)	
training:	Epoch: [14][129/204]	Loss 0.4701 (0.2709)	
training:	Epoch: [14][130/204]	Loss 0.6290 (0.2736)	
training:	Epoch: [14][131/204]	Loss 0.5288 (0.2756)	
training:	Epoch: [14][132/204]	Loss 0.2565 (0.2754)	
training:	Epoch: [14][133/204]	Loss 0.3253 (0.2758)	
training:	Epoch: [14][134/204]	Loss 0.2980 (0.2760)	
training:	Epoch: [14][135/204]	Loss 0.2253 (0.2756)	
training:	Epoch: [14][136/204]	Loss 0.1908 (0.2750)	
training:	Epoch: [14][137/204]	Loss 0.4158 (0.2760)	
training:	Epoch: [14][138/204]	Loss 0.2668 (0.2759)	
training:	Epoch: [14][139/204]	Loss 0.2579 (0.2758)	
training:	Epoch: [14][140/204]	Loss 0.1953 (0.2752)	
training:	Epoch: [14][141/204]	Loss 0.3848 (0.2760)	
training:	Epoch: [14][142/204]	Loss 0.2365 (0.2757)	
training:	Epoch: [14][143/204]	Loss 0.1431 (0.2748)	
training:	Epoch: [14][144/204]	Loss 0.2758 (0.2748)	
training:	Epoch: [14][145/204]	Loss 0.2256 (0.2745)	
training:	Epoch: [14][146/204]	Loss 0.2740 (0.2745)	
training:	Epoch: [14][147/204]	Loss 0.2736 (0.2745)	
training:	Epoch: [14][148/204]	Loss 0.2494 (0.2743)	
training:	Epoch: [14][149/204]	Loss 0.1582 (0.2735)	
training:	Epoch: [14][150/204]	Loss 0.3066 (0.2737)	
training:	Epoch: [14][151/204]	Loss 0.2523 (0.2736)	
training:	Epoch: [14][152/204]	Loss 0.3436 (0.2741)	
training:	Epoch: [14][153/204]	Loss 0.3851 (0.2748)	
training:	Epoch: [14][154/204]	Loss 0.3875 (0.2755)	
training:	Epoch: [14][155/204]	Loss 0.3140 (0.2758)	
training:	Epoch: [14][156/204]	Loss 0.3453 (0.2762)	
training:	Epoch: [14][157/204]	Loss 0.3645 (0.2768)	
training:	Epoch: [14][158/204]	Loss 0.3277 (0.2771)	
training:	Epoch: [14][159/204]	Loss 0.3747 (0.2777)	
training:	Epoch: [14][160/204]	Loss 0.1564 (0.2769)	
training:	Epoch: [14][161/204]	Loss 0.2475 (0.2768)	
training:	Epoch: [14][162/204]	Loss 0.2742 (0.2767)	
training:	Epoch: [14][163/204]	Loss 0.2505 (0.2766)	
training:	Epoch: [14][164/204]	Loss 0.4284 (0.2775)	
training:	Epoch: [14][165/204]	Loss 0.2640 (0.2774)	
training:	Epoch: [14][166/204]	Loss 0.2088 (0.2770)	
training:	Epoch: [14][167/204]	Loss 0.2628 (0.2769)	
training:	Epoch: [14][168/204]	Loss 0.2370 (0.2767)	
training:	Epoch: [14][169/204]	Loss 0.4283 (0.2776)	
training:	Epoch: [14][170/204]	Loss 0.1667 (0.2769)	
training:	Epoch: [14][171/204]	Loss 0.5202 (0.2784)	
training:	Epoch: [14][172/204]	Loss 0.3516 (0.2788)	
training:	Epoch: [14][173/204]	Loss 0.3865 (0.2794)	
training:	Epoch: [14][174/204]	Loss 0.3677 (0.2799)	
training:	Epoch: [14][175/204]	Loss 0.2597 (0.2798)	
training:	Epoch: [14][176/204]	Loss 0.2081 (0.2794)	
training:	Epoch: [14][177/204]	Loss 0.2960 (0.2795)	
training:	Epoch: [14][178/204]	Loss 0.5020 (0.2807)	
training:	Epoch: [14][179/204]	Loss 0.3593 (0.2812)	
training:	Epoch: [14][180/204]	Loss 0.2231 (0.2809)	
training:	Epoch: [14][181/204]	Loss 0.4281 (0.2817)	
training:	Epoch: [14][182/204]	Loss 0.4437 (0.2826)	
training:	Epoch: [14][183/204]	Loss 0.2355 (0.2823)	
training:	Epoch: [14][184/204]	Loss 0.4213 (0.2831)	
training:	Epoch: [14][185/204]	Loss 0.3734 (0.2835)	
training:	Epoch: [14][186/204]	Loss 0.2598 (0.2834)	
training:	Epoch: [14][187/204]	Loss 0.3658 (0.2839)	
training:	Epoch: [14][188/204]	Loss 0.1761 (0.2833)	
training:	Epoch: [14][189/204]	Loss 0.1447 (0.2825)	
training:	Epoch: [14][190/204]	Loss 0.3298 (0.2828)	
training:	Epoch: [14][191/204]	Loss 0.4594 (0.2837)	
training:	Epoch: [14][192/204]	Loss 0.4443 (0.2846)	
training:	Epoch: [14][193/204]	Loss 0.3343 (0.2848)	
training:	Epoch: [14][194/204]	Loss 0.1975 (0.2844)	
training:	Epoch: [14][195/204]	Loss 0.1959 (0.2839)	
training:	Epoch: [14][196/204]	Loss 0.1728 (0.2833)	
training:	Epoch: [14][197/204]	Loss 0.2724 (0.2833)	
training:	Epoch: [14][198/204]	Loss 0.1939 (0.2828)	
training:	Epoch: [14][199/204]	Loss 0.4143 (0.2835)	
training:	Epoch: [14][200/204]	Loss 0.3440 (0.2838)	
training:	Epoch: [14][201/204]	Loss 0.2742 (0.2838)	
training:	Epoch: [14][202/204]	Loss 0.3234 (0.2840)	
training:	Epoch: [14][203/204]	Loss 0.2913 (0.2840)	
training:	Epoch: [14][204/204]	Loss 0.2496 (0.2838)	
Training:	 Loss: 0.2834

Training:	 ACC: 0.9064 0.9073 0.9277 0.8852
Validation:	 ACC: 0.8291 0.8309 0.8700 0.7881
Validation:	 Best_BACC: 0.8326 0.8331 0.8424 0.8229
Validation:	 Loss: 0.4078
Pretraining:	Epoch 15/200
----------
training:	Epoch: [15][1/204]	Loss 0.4356 (0.4356)	
training:	Epoch: [15][2/204]	Loss 0.2454 (0.3405)	
training:	Epoch: [15][3/204]	Loss 0.2479 (0.3096)	
training:	Epoch: [15][4/204]	Loss 0.3723 (0.3253)	
training:	Epoch: [15][5/204]	Loss 0.2181 (0.3039)	
training:	Epoch: [15][6/204]	Loss 0.3681 (0.3146)	
training:	Epoch: [15][7/204]	Loss 0.2817 (0.3099)	
training:	Epoch: [15][8/204]	Loss 0.3373 (0.3133)	
training:	Epoch: [15][9/204]	Loss 0.2465 (0.3059)	
training:	Epoch: [15][10/204]	Loss 0.1438 (0.2897)	
training:	Epoch: [15][11/204]	Loss 0.1254 (0.2747)	
training:	Epoch: [15][12/204]	Loss 0.2648 (0.2739)	
training:	Epoch: [15][13/204]	Loss 0.2286 (0.2704)	
training:	Epoch: [15][14/204]	Loss 0.2743 (0.2707)	
training:	Epoch: [15][15/204]	Loss 0.2462 (0.2691)	
training:	Epoch: [15][16/204]	Loss 0.3264 (0.2726)	
training:	Epoch: [15][17/204]	Loss 0.2536 (0.2715)	
training:	Epoch: [15][18/204]	Loss 0.2918 (0.2727)	
training:	Epoch: [15][19/204]	Loss 0.2508 (0.2715)	
training:	Epoch: [15][20/204]	Loss 0.3943 (0.2776)	
training:	Epoch: [15][21/204]	Loss 0.1653 (0.2723)	
training:	Epoch: [15][22/204]	Loss 0.4801 (0.2817)	
training:	Epoch: [15][23/204]	Loss 0.2134 (0.2788)	
training:	Epoch: [15][24/204]	Loss 0.1987 (0.2754)	
training:	Epoch: [15][25/204]	Loss 0.3759 (0.2794)	
training:	Epoch: [15][26/204]	Loss 0.3655 (0.2828)	
training:	Epoch: [15][27/204]	Loss 0.2159 (0.2803)	
training:	Epoch: [15][28/204]	Loss 0.1718 (0.2764)	
training:	Epoch: [15][29/204]	Loss 0.4503 (0.2824)	
training:	Epoch: [15][30/204]	Loss 0.1653 (0.2785)	
training:	Epoch: [15][31/204]	Loss 0.2418 (0.2773)	
training:	Epoch: [15][32/204]	Loss 0.1911 (0.2746)	
training:	Epoch: [15][33/204]	Loss 0.3898 (0.2781)	
training:	Epoch: [15][34/204]	Loss 0.2533 (0.2774)	
training:	Epoch: [15][35/204]	Loss 0.2175 (0.2757)	
training:	Epoch: [15][36/204]	Loss 0.1367 (0.2718)	
training:	Epoch: [15][37/204]	Loss 0.3259 (0.2733)	
training:	Epoch: [15][38/204]	Loss 0.3556 (0.2754)	
training:	Epoch: [15][39/204]	Loss 0.4494 (0.2799)	
training:	Epoch: [15][40/204]	Loss 0.2549 (0.2793)	
training:	Epoch: [15][41/204]	Loss 0.1864 (0.2770)	
training:	Epoch: [15][42/204]	Loss 0.2438 (0.2762)	
training:	Epoch: [15][43/204]	Loss 0.1911 (0.2742)	
training:	Epoch: [15][44/204]	Loss 0.2840 (0.2745)	
training:	Epoch: [15][45/204]	Loss 0.2242 (0.2733)	
training:	Epoch: [15][46/204]	Loss 0.1160 (0.2699)	
training:	Epoch: [15][47/204]	Loss 0.3055 (0.2707)	
training:	Epoch: [15][48/204]	Loss 0.4446 (0.2743)	
training:	Epoch: [15][49/204]	Loss 0.2416 (0.2736)	
training:	Epoch: [15][50/204]	Loss 0.2058 (0.2723)	
training:	Epoch: [15][51/204]	Loss 0.2499 (0.2718)	
training:	Epoch: [15][52/204]	Loss 0.3164 (0.2727)	
training:	Epoch: [15][53/204]	Loss 0.1702 (0.2708)	
training:	Epoch: [15][54/204]	Loss 0.0933 (0.2675)	
training:	Epoch: [15][55/204]	Loss 0.1887 (0.2660)	
training:	Epoch: [15][56/204]	Loss 0.2524 (0.2658)	
training:	Epoch: [15][57/204]	Loss 0.1475 (0.2637)	
training:	Epoch: [15][58/204]	Loss 0.2298 (0.2631)	
training:	Epoch: [15][59/204]	Loss 0.4028 (0.2655)	
training:	Epoch: [15][60/204]	Loss 0.4211 (0.2681)	
training:	Epoch: [15][61/204]	Loss 0.1450 (0.2661)	
training:	Epoch: [15][62/204]	Loss 0.2555 (0.2659)	
training:	Epoch: [15][63/204]	Loss 0.3560 (0.2673)	
training:	Epoch: [15][64/204]	Loss 0.1476 (0.2655)	
training:	Epoch: [15][65/204]	Loss 0.1928 (0.2643)	
training:	Epoch: [15][66/204]	Loss 0.2493 (0.2641)	
training:	Epoch: [15][67/204]	Loss 0.2918 (0.2645)	
training:	Epoch: [15][68/204]	Loss 0.1603 (0.2630)	
training:	Epoch: [15][69/204]	Loss 0.2032 (0.2621)	
training:	Epoch: [15][70/204]	Loss 0.2504 (0.2620)	
training:	Epoch: [15][71/204]	Loss 0.3014 (0.2625)	
training:	Epoch: [15][72/204]	Loss 0.1500 (0.2610)	
training:	Epoch: [15][73/204]	Loss 0.1373 (0.2593)	
training:	Epoch: [15][74/204]	Loss 0.2411 (0.2590)	
training:	Epoch: [15][75/204]	Loss 0.2665 (0.2591)	
training:	Epoch: [15][76/204]	Loss 0.1498 (0.2577)	
training:	Epoch: [15][77/204]	Loss 0.4035 (0.2596)	
training:	Epoch: [15][78/204]	Loss 0.1503 (0.2582)	
training:	Epoch: [15][79/204]	Loss 0.4479 (0.2606)	
training:	Epoch: [15][80/204]	Loss 0.4726 (0.2632)	
training:	Epoch: [15][81/204]	Loss 0.2415 (0.2630)	
training:	Epoch: [15][82/204]	Loss 0.3196 (0.2636)	
training:	Epoch: [15][83/204]	Loss 0.1807 (0.2626)	
training:	Epoch: [15][84/204]	Loss 0.3310 (0.2635)	
training:	Epoch: [15][85/204]	Loss 0.1399 (0.2620)	
training:	Epoch: [15][86/204]	Loss 0.2235 (0.2616)	
training:	Epoch: [15][87/204]	Loss 0.4420 (0.2636)	
training:	Epoch: [15][88/204]	Loss 0.4074 (0.2653)	
training:	Epoch: [15][89/204]	Loss 0.3235 (0.2659)	
training:	Epoch: [15][90/204]	Loss 0.4118 (0.2675)	
training:	Epoch: [15][91/204]	Loss 0.4439 (0.2695)	
training:	Epoch: [15][92/204]	Loss 0.2949 (0.2698)	
training:	Epoch: [15][93/204]	Loss 0.2643 (0.2697)	
training:	Epoch: [15][94/204]	Loss 0.1458 (0.2684)	
training:	Epoch: [15][95/204]	Loss 0.3014 (0.2687)	
training:	Epoch: [15][96/204]	Loss 0.3329 (0.2694)	
training:	Epoch: [15][97/204]	Loss 0.3066 (0.2698)	
training:	Epoch: [15][98/204]	Loss 0.2813 (0.2699)	
training:	Epoch: [15][99/204]	Loss 0.1534 (0.2687)	
training:	Epoch: [15][100/204]	Loss 0.5379 (0.2714)	
training:	Epoch: [15][101/204]	Loss 0.2510 (0.2712)	
training:	Epoch: [15][102/204]	Loss 0.3253 (0.2717)	
training:	Epoch: [15][103/204]	Loss 0.1855 (0.2709)	
training:	Epoch: [15][104/204]	Loss 0.2222 (0.2704)	
training:	Epoch: [15][105/204]	Loss 0.2169 (0.2699)	
training:	Epoch: [15][106/204]	Loss 0.2063 (0.2693)	
training:	Epoch: [15][107/204]	Loss 0.1876 (0.2686)	
training:	Epoch: [15][108/204]	Loss 0.4428 (0.2702)	
training:	Epoch: [15][109/204]	Loss 0.3608 (0.2710)	
training:	Epoch: [15][110/204]	Loss 0.2339 (0.2707)	
training:	Epoch: [15][111/204]	Loss 0.1745 (0.2698)	
training:	Epoch: [15][112/204]	Loss 0.4156 (0.2711)	
training:	Epoch: [15][113/204]	Loss 0.3519 (0.2718)	
training:	Epoch: [15][114/204]	Loss 0.1208 (0.2705)	
training:	Epoch: [15][115/204]	Loss 0.2788 (0.2706)	
training:	Epoch: [15][116/204]	Loss 0.5852 (0.2733)	
training:	Epoch: [15][117/204]	Loss 0.3411 (0.2739)	
training:	Epoch: [15][118/204]	Loss 0.2717 (0.2738)	
training:	Epoch: [15][119/204]	Loss 0.3355 (0.2744)	
training:	Epoch: [15][120/204]	Loss 0.2586 (0.2742)	
training:	Epoch: [15][121/204]	Loss 0.1453 (0.2732)	
training:	Epoch: [15][122/204]	Loss 0.2399 (0.2729)	
training:	Epoch: [15][123/204]	Loss 0.1830 (0.2722)	
training:	Epoch: [15][124/204]	Loss 0.3015 (0.2724)	
training:	Epoch: [15][125/204]	Loss 0.2576 (0.2723)	
training:	Epoch: [15][126/204]	Loss 0.2278 (0.2719)	
training:	Epoch: [15][127/204]	Loss 0.2326 (0.2716)	
training:	Epoch: [15][128/204]	Loss 0.4179 (0.2728)	
training:	Epoch: [15][129/204]	Loss 0.2295 (0.2724)	
training:	Epoch: [15][130/204]	Loss 0.2725 (0.2724)	
training:	Epoch: [15][131/204]	Loss 0.4482 (0.2738)	
training:	Epoch: [15][132/204]	Loss 0.1993 (0.2732)	
training:	Epoch: [15][133/204]	Loss 0.4432 (0.2745)	
training:	Epoch: [15][134/204]	Loss 0.1529 (0.2736)	
training:	Epoch: [15][135/204]	Loss 0.3883 (0.2744)	
training:	Epoch: [15][136/204]	Loss 0.1682 (0.2736)	
training:	Epoch: [15][137/204]	Loss 0.4663 (0.2750)	
training:	Epoch: [15][138/204]	Loss 0.2084 (0.2746)	
training:	Epoch: [15][139/204]	Loss 0.2398 (0.2743)	
training:	Epoch: [15][140/204]	Loss 0.5169 (0.2760)	
training:	Epoch: [15][141/204]	Loss 0.2452 (0.2758)	
training:	Epoch: [15][142/204]	Loss 0.2699 (0.2758)	
training:	Epoch: [15][143/204]	Loss 0.3758 (0.2765)	
training:	Epoch: [15][144/204]	Loss 0.2538 (0.2763)	
training:	Epoch: [15][145/204]	Loss 0.2977 (0.2765)	
training:	Epoch: [15][146/204]	Loss 0.3842 (0.2772)	
training:	Epoch: [15][147/204]	Loss 0.2474 (0.2770)	
training:	Epoch: [15][148/204]	Loss 0.1823 (0.2764)	
training:	Epoch: [15][149/204]	Loss 0.1533 (0.2755)	
training:	Epoch: [15][150/204]	Loss 0.1670 (0.2748)	
training:	Epoch: [15][151/204]	Loss 0.1411 (0.2739)	
training:	Epoch: [15][152/204]	Loss 0.4522 (0.2751)	
training:	Epoch: [15][153/204]	Loss 0.2379 (0.2749)	
training:	Epoch: [15][154/204]	Loss 0.2120 (0.2745)	
training:	Epoch: [15][155/204]	Loss 0.2429 (0.2743)	
training:	Epoch: [15][156/204]	Loss 0.3368 (0.2747)	
training:	Epoch: [15][157/204]	Loss 0.3656 (0.2752)	
training:	Epoch: [15][158/204]	Loss 0.1316 (0.2743)	
training:	Epoch: [15][159/204]	Loss 0.3404 (0.2747)	
training:	Epoch: [15][160/204]	Loss 0.1814 (0.2742)	
training:	Epoch: [15][161/204]	Loss 0.2340 (0.2739)	
training:	Epoch: [15][162/204]	Loss 0.3097 (0.2741)	
training:	Epoch: [15][163/204]	Loss 0.3512 (0.2746)	
training:	Epoch: [15][164/204]	Loss 0.4518 (0.2757)	
training:	Epoch: [15][165/204]	Loss 0.3614 (0.2762)	
training:	Epoch: [15][166/204]	Loss 0.2219 (0.2759)	
training:	Epoch: [15][167/204]	Loss 0.1750 (0.2753)	
training:	Epoch: [15][168/204]	Loss 0.4308 (0.2762)	
training:	Epoch: [15][169/204]	Loss 0.2617 (0.2761)	
training:	Epoch: [15][170/204]	Loss 0.2717 (0.2761)	
training:	Epoch: [15][171/204]	Loss 0.3909 (0.2768)	
training:	Epoch: [15][172/204]	Loss 0.2829 (0.2768)	
training:	Epoch: [15][173/204]	Loss 0.3141 (0.2770)	
training:	Epoch: [15][174/204]	Loss 0.1667 (0.2764)	
training:	Epoch: [15][175/204]	Loss 0.2648 (0.2763)	
training:	Epoch: [15][176/204]	Loss 0.2001 (0.2759)	
training:	Epoch: [15][177/204]	Loss 0.1326 (0.2751)	
training:	Epoch: [15][178/204]	Loss 0.3010 (0.2752)	
training:	Epoch: [15][179/204]	Loss 0.3129 (0.2754)	
training:	Epoch: [15][180/204]	Loss 0.3350 (0.2758)	
training:	Epoch: [15][181/204]	Loss 0.4198 (0.2765)	
training:	Epoch: [15][182/204]	Loss 0.2362 (0.2763)	
training:	Epoch: [15][183/204]	Loss 0.1988 (0.2759)	
training:	Epoch: [15][184/204]	Loss 0.0978 (0.2749)	
training:	Epoch: [15][185/204]	Loss 0.0758 (0.2739)	
training:	Epoch: [15][186/204]	Loss 0.1887 (0.2734)	
training:	Epoch: [15][187/204]	Loss 0.2805 (0.2734)	
training:	Epoch: [15][188/204]	Loss 0.3390 (0.2738)	
training:	Epoch: [15][189/204]	Loss 0.4926 (0.2749)	
training:	Epoch: [15][190/204]	Loss 0.1857 (0.2745)	
training:	Epoch: [15][191/204]	Loss 0.2913 (0.2746)	
training:	Epoch: [15][192/204]	Loss 0.3255 (0.2748)	
training:	Epoch: [15][193/204]	Loss 0.2306 (0.2746)	
training:	Epoch: [15][194/204]	Loss 0.3507 (0.2750)	
training:	Epoch: [15][195/204]	Loss 0.3029 (0.2751)	
training:	Epoch: [15][196/204]	Loss 0.5548 (0.2766)	
training:	Epoch: [15][197/204]	Loss 0.2971 (0.2767)	
training:	Epoch: [15][198/204]	Loss 0.3011 (0.2768)	
training:	Epoch: [15][199/204]	Loss 0.3056 (0.2769)	
training:	Epoch: [15][200/204]	Loss 0.2186 (0.2766)	
training:	Epoch: [15][201/204]	Loss 0.1538 (0.2760)	
training:	Epoch: [15][202/204]	Loss 0.3414 (0.2764)	
training:	Epoch: [15][203/204]	Loss 0.1469 (0.2757)	
training:	Epoch: [15][204/204]	Loss 0.1237 (0.2750)	
Training:	 Loss: 0.2746

Training:	 ACC: 0.9164 0.9163 0.9153 0.9174
Validation:	 ACC: 0.8265 0.8272 0.8414 0.8117
Validation:	 Best_BACC: 0.8326 0.8331 0.8424 0.8229
Validation:	 Loss: 0.4157
Pretraining:	Epoch 16/200
----------
training:	Epoch: [16][1/204]	Loss 0.3181 (0.3181)	
training:	Epoch: [16][2/204]	Loss 0.3230 (0.3205)	
training:	Epoch: [16][3/204]	Loss 0.2219 (0.2877)	
training:	Epoch: [16][4/204]	Loss 0.3749 (0.3095)	
training:	Epoch: [16][5/204]	Loss 0.3364 (0.3148)	
training:	Epoch: [16][6/204]	Loss 0.2077 (0.2970)	
training:	Epoch: [16][7/204]	Loss 0.1605 (0.2775)	
training:	Epoch: [16][8/204]	Loss 0.3697 (0.2890)	
training:	Epoch: [16][9/204]	Loss 0.4059 (0.3020)	
training:	Epoch: [16][10/204]	Loss 0.2213 (0.2939)	
training:	Epoch: [16][11/204]	Loss 0.2537 (0.2903)	
training:	Epoch: [16][12/204]	Loss 0.2844 (0.2898)	
training:	Epoch: [16][13/204]	Loss 0.2480 (0.2866)	
training:	Epoch: [16][14/204]	Loss 0.0993 (0.2732)	
training:	Epoch: [16][15/204]	Loss 0.1677 (0.2662)	
training:	Epoch: [16][16/204]	Loss 0.1545 (0.2592)	
training:	Epoch: [16][17/204]	Loss 0.0873 (0.2491)	
training:	Epoch: [16][18/204]	Loss 0.3245 (0.2533)	
training:	Epoch: [16][19/204]	Loss 0.3137 (0.2564)	
training:	Epoch: [16][20/204]	Loss 0.2338 (0.2553)	
training:	Epoch: [16][21/204]	Loss 0.2459 (0.2549)	
training:	Epoch: [16][22/204]	Loss 0.4354 (0.2631)	
training:	Epoch: [16][23/204]	Loss 0.3410 (0.2665)	
training:	Epoch: [16][24/204]	Loss 0.1562 (0.2619)	
training:	Epoch: [16][25/204]	Loss 0.2592 (0.2618)	
training:	Epoch: [16][26/204]	Loss 0.3642 (0.2657)	
training:	Epoch: [16][27/204]	Loss 0.2094 (0.2636)	
training:	Epoch: [16][28/204]	Loss 0.5324 (0.2732)	
training:	Epoch: [16][29/204]	Loss 0.2689 (0.2731)	
training:	Epoch: [16][30/204]	Loss 0.1427 (0.2687)	
training:	Epoch: [16][31/204]	Loss 0.1821 (0.2659)	
training:	Epoch: [16][32/204]	Loss 0.2295 (0.2648)	
training:	Epoch: [16][33/204]	Loss 0.0814 (0.2592)	
training:	Epoch: [16][34/204]	Loss 0.2165 (0.2580)	
training:	Epoch: [16][35/204]	Loss 0.2949 (0.2590)	
training:	Epoch: [16][36/204]	Loss 0.4062 (0.2631)	
training:	Epoch: [16][37/204]	Loss 0.1703 (0.2606)	
training:	Epoch: [16][38/204]	Loss 0.3034 (0.2617)	
training:	Epoch: [16][39/204]	Loss 0.1670 (0.2593)	
training:	Epoch: [16][40/204]	Loss 0.2191 (0.2583)	
training:	Epoch: [16][41/204]	Loss 0.4322 (0.2625)	
training:	Epoch: [16][42/204]	Loss 0.5038 (0.2683)	
training:	Epoch: [16][43/204]	Loss 0.2940 (0.2689)	
training:	Epoch: [16][44/204]	Loss 0.0752 (0.2645)	
training:	Epoch: [16][45/204]	Loss 0.4381 (0.2683)	
training:	Epoch: [16][46/204]	Loss 0.3975 (0.2711)	
training:	Epoch: [16][47/204]	Loss 0.3047 (0.2719)	
training:	Epoch: [16][48/204]	Loss 0.3393 (0.2733)	
training:	Epoch: [16][49/204]	Loss 0.2518 (0.2728)	
training:	Epoch: [16][50/204]	Loss 0.2386 (0.2721)	
training:	Epoch: [16][51/204]	Loss 0.0672 (0.2681)	
training:	Epoch: [16][52/204]	Loss 0.2349 (0.2675)	
training:	Epoch: [16][53/204]	Loss 0.4467 (0.2709)	
training:	Epoch: [16][54/204]	Loss 0.5650 (0.2763)	
training:	Epoch: [16][55/204]	Loss 0.2378 (0.2756)	
training:	Epoch: [16][56/204]	Loss 0.1785 (0.2739)	
training:	Epoch: [16][57/204]	Loss 0.3736 (0.2756)	
training:	Epoch: [16][58/204]	Loss 0.1755 (0.2739)	
training:	Epoch: [16][59/204]	Loss 0.2772 (0.2740)	
training:	Epoch: [16][60/204]	Loss 0.1297 (0.2716)	
training:	Epoch: [16][61/204]	Loss 0.3124 (0.2722)	
training:	Epoch: [16][62/204]	Loss 0.3434 (0.2734)	
training:	Epoch: [16][63/204]	Loss 0.2515 (0.2730)	
training:	Epoch: [16][64/204]	Loss 0.3137 (0.2737)	
training:	Epoch: [16][65/204]	Loss 0.2058 (0.2726)	
training:	Epoch: [16][66/204]	Loss 0.1905 (0.2714)	
training:	Epoch: [16][67/204]	Loss 0.1852 (0.2701)	
training:	Epoch: [16][68/204]	Loss 0.2651 (0.2700)	
training:	Epoch: [16][69/204]	Loss 0.4168 (0.2721)	
training:	Epoch: [16][70/204]	Loss 0.3017 (0.2726)	
training:	Epoch: [16][71/204]	Loss 0.2055 (0.2716)	
training:	Epoch: [16][72/204]	Loss 0.2496 (0.2713)	
training:	Epoch: [16][73/204]	Loss 0.3338 (0.2722)	
training:	Epoch: [16][74/204]	Loss 0.2226 (0.2715)	
training:	Epoch: [16][75/204]	Loss 0.2261 (0.2709)	
training:	Epoch: [16][76/204]	Loss 0.1078 (0.2687)	
training:	Epoch: [16][77/204]	Loss 0.1551 (0.2673)	
training:	Epoch: [16][78/204]	Loss 0.1811 (0.2662)	
training:	Epoch: [16][79/204]	Loss 0.2900 (0.2665)	
training:	Epoch: [16][80/204]	Loss 0.1255 (0.2647)	
training:	Epoch: [16][81/204]	Loss 0.1584 (0.2634)	
training:	Epoch: [16][82/204]	Loss 0.2705 (0.2635)	
training:	Epoch: [16][83/204]	Loss 0.1482 (0.2621)	
training:	Epoch: [16][84/204]	Loss 0.3136 (0.2627)	
training:	Epoch: [16][85/204]	Loss 0.3482 (0.2637)	
training:	Epoch: [16][86/204]	Loss 0.1974 (0.2629)	
training:	Epoch: [16][87/204]	Loss 0.1370 (0.2615)	
training:	Epoch: [16][88/204]	Loss 0.1316 (0.2600)	
training:	Epoch: [16][89/204]	Loss 0.2797 (0.2602)	
training:	Epoch: [16][90/204]	Loss 0.1614 (0.2591)	
training:	Epoch: [16][91/204]	Loss 0.3009 (0.2596)	
training:	Epoch: [16][92/204]	Loss 0.2589 (0.2596)	
training:	Epoch: [16][93/204]	Loss 0.2796 (0.2598)	
training:	Epoch: [16][94/204]	Loss 0.2739 (0.2600)	
training:	Epoch: [16][95/204]	Loss 0.1827 (0.2591)	
training:	Epoch: [16][96/204]	Loss 0.1142 (0.2576)	
training:	Epoch: [16][97/204]	Loss 0.1058 (0.2561)	
training:	Epoch: [16][98/204]	Loss 0.2044 (0.2555)	
training:	Epoch: [16][99/204]	Loss 0.2012 (0.2550)	
training:	Epoch: [16][100/204]	Loss 0.1861 (0.2543)	
training:	Epoch: [16][101/204]	Loss 0.2839 (0.2546)	
training:	Epoch: [16][102/204]	Loss 0.1435 (0.2535)	
training:	Epoch: [16][103/204]	Loss 0.3358 (0.2543)	
training:	Epoch: [16][104/204]	Loss 0.2110 (0.2539)	
training:	Epoch: [16][105/204]	Loss 0.5874 (0.2571)	
training:	Epoch: [16][106/204]	Loss 0.1602 (0.2561)	
training:	Epoch: [16][107/204]	Loss 0.5302 (0.2587)	
training:	Epoch: [16][108/204]	Loss 0.1274 (0.2575)	
training:	Epoch: [16][109/204]	Loss 0.1234 (0.2563)	
training:	Epoch: [16][110/204]	Loss 0.1092 (0.2549)	
training:	Epoch: [16][111/204]	Loss 0.1044 (0.2536)	
training:	Epoch: [16][112/204]	Loss 0.5155 (0.2559)	
training:	Epoch: [16][113/204]	Loss 0.2010 (0.2554)	
training:	Epoch: [16][114/204]	Loss 0.3364 (0.2561)	
training:	Epoch: [16][115/204]	Loss 0.3551 (0.2570)	
training:	Epoch: [16][116/204]	Loss 0.2126 (0.2566)	
training:	Epoch: [16][117/204]	Loss 0.2786 (0.2568)	
training:	Epoch: [16][118/204]	Loss 0.4060 (0.2581)	
training:	Epoch: [16][119/204]	Loss 0.2317 (0.2578)	
training:	Epoch: [16][120/204]	Loss 0.3408 (0.2585)	
training:	Epoch: [16][121/204]	Loss 0.3473 (0.2593)	
training:	Epoch: [16][122/204]	Loss 0.3290 (0.2598)	
training:	Epoch: [16][123/204]	Loss 0.1278 (0.2588)	
training:	Epoch: [16][124/204]	Loss 0.1608 (0.2580)	
training:	Epoch: [16][125/204]	Loss 0.2027 (0.2575)	
training:	Epoch: [16][126/204]	Loss 0.3155 (0.2580)	
training:	Epoch: [16][127/204]	Loss 0.3532 (0.2587)	
training:	Epoch: [16][128/204]	Loss 0.2521 (0.2587)	
training:	Epoch: [16][129/204]	Loss 0.3000 (0.2590)	
training:	Epoch: [16][130/204]	Loss 0.3298 (0.2596)	
training:	Epoch: [16][131/204]	Loss 0.1734 (0.2589)	
training:	Epoch: [16][132/204]	Loss 0.3928 (0.2599)	
training:	Epoch: [16][133/204]	Loss 0.3419 (0.2605)	
training:	Epoch: [16][134/204]	Loss 0.2453 (0.2604)	
training:	Epoch: [16][135/204]	Loss 0.1228 (0.2594)	
training:	Epoch: [16][136/204]	Loss 0.2836 (0.2596)	
training:	Epoch: [16][137/204]	Loss 0.0742 (0.2582)	
training:	Epoch: [16][138/204]	Loss 0.3671 (0.2590)	
training:	Epoch: [16][139/204]	Loss 0.3824 (0.2599)	
training:	Epoch: [16][140/204]	Loss 0.2357 (0.2597)	
training:	Epoch: [16][141/204]	Loss 0.1916 (0.2592)	
training:	Epoch: [16][142/204]	Loss 0.1762 (0.2587)	
training:	Epoch: [16][143/204]	Loss 0.2191 (0.2584)	
training:	Epoch: [16][144/204]	Loss 0.1493 (0.2576)	
training:	Epoch: [16][145/204]	Loss 0.2163 (0.2573)	
training:	Epoch: [16][146/204]	Loss 0.3306 (0.2578)	
training:	Epoch: [16][147/204]	Loss 0.1921 (0.2574)	
training:	Epoch: [16][148/204]	Loss 0.3339 (0.2579)	
training:	Epoch: [16][149/204]	Loss 0.2239 (0.2577)	
training:	Epoch: [16][150/204]	Loss 0.2067 (0.2573)	
training:	Epoch: [16][151/204]	Loss 0.1449 (0.2566)	
training:	Epoch: [16][152/204]	Loss 0.5452 (0.2585)	
training:	Epoch: [16][153/204]	Loss 0.2996 (0.2588)	
training:	Epoch: [16][154/204]	Loss 0.2589 (0.2588)	
training:	Epoch: [16][155/204]	Loss 0.3825 (0.2596)	
training:	Epoch: [16][156/204]	Loss 0.1270 (0.2587)	
training:	Epoch: [16][157/204]	Loss 0.1703 (0.2581)	
training:	Epoch: [16][158/204]	Loss 0.2134 (0.2579)	
training:	Epoch: [16][159/204]	Loss 0.3137 (0.2582)	
training:	Epoch: [16][160/204]	Loss 0.3141 (0.2586)	
training:	Epoch: [16][161/204]	Loss 0.4946 (0.2600)	
training:	Epoch: [16][162/204]	Loss 0.1131 (0.2591)	
training:	Epoch: [16][163/204]	Loss 0.2035 (0.2588)	
training:	Epoch: [16][164/204]	Loss 0.3818 (0.2595)	
training:	Epoch: [16][165/204]	Loss 0.1520 (0.2589)	
training:	Epoch: [16][166/204]	Loss 0.3479 (0.2594)	
training:	Epoch: [16][167/204]	Loss 0.1121 (0.2585)	
training:	Epoch: [16][168/204]	Loss 0.3476 (0.2591)	
training:	Epoch: [16][169/204]	Loss 0.3884 (0.2598)	
training:	Epoch: [16][170/204]	Loss 0.2799 (0.2600)	
training:	Epoch: [16][171/204]	Loss 0.2090 (0.2597)	
training:	Epoch: [16][172/204]	Loss 0.2633 (0.2597)	
training:	Epoch: [16][173/204]	Loss 0.3493 (0.2602)	
training:	Epoch: [16][174/204]	Loss 0.1946 (0.2598)	
training:	Epoch: [16][175/204]	Loss 0.1537 (0.2592)	
training:	Epoch: [16][176/204]	Loss 0.1974 (0.2589)	
training:	Epoch: [16][177/204]	Loss 0.2202 (0.2586)	
training:	Epoch: [16][178/204]	Loss 0.3968 (0.2594)	
training:	Epoch: [16][179/204]	Loss 0.3559 (0.2600)	
training:	Epoch: [16][180/204]	Loss 0.3663 (0.2605)	
training:	Epoch: [16][181/204]	Loss 0.3180 (0.2609)	
training:	Epoch: [16][182/204]	Loss 0.4820 (0.2621)	
training:	Epoch: [16][183/204]	Loss 0.3180 (0.2624)	
training:	Epoch: [16][184/204]	Loss 0.3783 (0.2630)	
training:	Epoch: [16][185/204]	Loss 0.2680 (0.2630)	
training:	Epoch: [16][186/204]	Loss 0.1499 (0.2624)	
training:	Epoch: [16][187/204]	Loss 0.3771 (0.2630)	
training:	Epoch: [16][188/204]	Loss 0.1228 (0.2623)	
training:	Epoch: [16][189/204]	Loss 0.2533 (0.2623)	
training:	Epoch: [16][190/204]	Loss 0.1808 (0.2618)	
training:	Epoch: [16][191/204]	Loss 0.2517 (0.2618)	
training:	Epoch: [16][192/204]	Loss 0.2200 (0.2616)	
training:	Epoch: [16][193/204]	Loss 0.3311 (0.2619)	
training:	Epoch: [16][194/204]	Loss 0.2979 (0.2621)	
training:	Epoch: [16][195/204]	Loss 0.1762 (0.2617)	
training:	Epoch: [16][196/204]	Loss 0.3078 (0.2619)	
training:	Epoch: [16][197/204]	Loss 0.2729 (0.2619)	
training:	Epoch: [16][198/204]	Loss 0.2222 (0.2617)	
training:	Epoch: [16][199/204]	Loss 0.1915 (0.2614)	
training:	Epoch: [16][200/204]	Loss 0.4732 (0.2625)	
training:	Epoch: [16][201/204]	Loss 0.2733 (0.2625)	
training:	Epoch: [16][202/204]	Loss 0.4102 (0.2632)	
training:	Epoch: [16][203/204]	Loss 0.1384 (0.2626)	
training:	Epoch: [16][204/204]	Loss 0.3552 (0.2631)	
Training:	 Loss: 0.2627

Training:	 ACC: 0.9261 0.9260 0.9230 0.9292
Validation:	 ACC: 0.8243 0.8245 0.8291 0.8195
Validation:	 Best_BACC: 0.8326 0.8331 0.8424 0.8229
Validation:	 Loss: 0.4180
Pretraining:	Epoch 17/200
----------
training:	Epoch: [17][1/204]	Loss 0.3539 (0.3539)	
training:	Epoch: [17][2/204]	Loss 0.1630 (0.2584)	
training:	Epoch: [17][3/204]	Loss 0.2846 (0.2672)	
training:	Epoch: [17][4/204]	Loss 0.1266 (0.2320)	
training:	Epoch: [17][5/204]	Loss 0.3824 (0.2621)	
training:	Epoch: [17][6/204]	Loss 0.2546 (0.2608)	
training:	Epoch: [17][7/204]	Loss 0.1639 (0.2470)	
training:	Epoch: [17][8/204]	Loss 0.1144 (0.2304)	
training:	Epoch: [17][9/204]	Loss 0.1757 (0.2243)	
training:	Epoch: [17][10/204]	Loss 0.3712 (0.2390)	
training:	Epoch: [17][11/204]	Loss 0.1920 (0.2348)	
training:	Epoch: [17][12/204]	Loss 0.2564 (0.2366)	
training:	Epoch: [17][13/204]	Loss 0.1661 (0.2311)	
training:	Epoch: [17][14/204]	Loss 0.1367 (0.2244)	
training:	Epoch: [17][15/204]	Loss 0.3612 (0.2335)	
training:	Epoch: [17][16/204]	Loss 0.3178 (0.2388)	
training:	Epoch: [17][17/204]	Loss 0.4126 (0.2490)	
training:	Epoch: [17][18/204]	Loss 0.3597 (0.2551)	
training:	Epoch: [17][19/204]	Loss 0.1641 (0.2504)	
training:	Epoch: [17][20/204]	Loss 0.1750 (0.2466)	
training:	Epoch: [17][21/204]	Loss 0.1866 (0.2437)	
training:	Epoch: [17][22/204]	Loss 0.1094 (0.2376)	
training:	Epoch: [17][23/204]	Loss 0.1496 (0.2338)	
training:	Epoch: [17][24/204]	Loss 0.1573 (0.2306)	
training:	Epoch: [17][25/204]	Loss 0.2266 (0.2305)	
training:	Epoch: [17][26/204]	Loss 0.1261 (0.2264)	
training:	Epoch: [17][27/204]	Loss 0.2536 (0.2274)	
training:	Epoch: [17][28/204]	Loss 0.1380 (0.2242)	
training:	Epoch: [17][29/204]	Loss 0.1900 (0.2231)	
training:	Epoch: [17][30/204]	Loss 0.1007 (0.2190)	
training:	Epoch: [17][31/204]	Loss 0.2801 (0.2210)	
training:	Epoch: [17][32/204]	Loss 0.2677 (0.2224)	
training:	Epoch: [17][33/204]	Loss 0.1655 (0.2207)	
training:	Epoch: [17][34/204]	Loss 0.1528 (0.2187)	
training:	Epoch: [17][35/204]	Loss 0.1739 (0.2174)	
training:	Epoch: [17][36/204]	Loss 0.3844 (0.2221)	
training:	Epoch: [17][37/204]	Loss 0.2621 (0.2231)	
training:	Epoch: [17][38/204]	Loss 0.0887 (0.2196)	
training:	Epoch: [17][39/204]	Loss 0.4147 (0.2246)	
training:	Epoch: [17][40/204]	Loss 0.2385 (0.2250)	
training:	Epoch: [17][41/204]	Loss 0.2219 (0.2249)	
training:	Epoch: [17][42/204]	Loss 0.1169 (0.2223)	
training:	Epoch: [17][43/204]	Loss 0.2984 (0.2241)	
training:	Epoch: [17][44/204]	Loss 0.0668 (0.2205)	
training:	Epoch: [17][45/204]	Loss 0.1891 (0.2198)	
training:	Epoch: [17][46/204]	Loss 0.3249 (0.2221)	
training:	Epoch: [17][47/204]	Loss 0.1541 (0.2206)	
training:	Epoch: [17][48/204]	Loss 0.3742 (0.2238)	
training:	Epoch: [17][49/204]	Loss 0.0960 (0.2212)	
training:	Epoch: [17][50/204]	Loss 0.3207 (0.2232)	
training:	Epoch: [17][51/204]	Loss 0.2268 (0.2233)	
training:	Epoch: [17][52/204]	Loss 0.3258 (0.2253)	
training:	Epoch: [17][53/204]	Loss 0.0941 (0.2228)	
training:	Epoch: [17][54/204]	Loss 0.1842 (0.2221)	
training:	Epoch: [17][55/204]	Loss 0.5247 (0.2276)	
training:	Epoch: [17][56/204]	Loss 0.1407 (0.2260)	
training:	Epoch: [17][57/204]	Loss 0.1579 (0.2248)	
training:	Epoch: [17][58/204]	Loss 0.2357 (0.2250)	
training:	Epoch: [17][59/204]	Loss 0.1480 (0.2237)	
training:	Epoch: [17][60/204]	Loss 0.1318 (0.2222)	
training:	Epoch: [17][61/204]	Loss 0.0693 (0.2197)	
training:	Epoch: [17][62/204]	Loss 0.2698 (0.2205)	
training:	Epoch: [17][63/204]	Loss 0.4599 (0.2243)	
training:	Epoch: [17][64/204]	Loss 0.3829 (0.2268)	
training:	Epoch: [17][65/204]	Loss 0.3769 (0.2291)	
training:	Epoch: [17][66/204]	Loss 0.1900 (0.2285)	
training:	Epoch: [17][67/204]	Loss 0.4160 (0.2313)	
training:	Epoch: [17][68/204]	Loss 0.1522 (0.2301)	
training:	Epoch: [17][69/204]	Loss 0.4731 (0.2336)	
training:	Epoch: [17][70/204]	Loss 0.2897 (0.2344)	
training:	Epoch: [17][71/204]	Loss 0.2742 (0.2350)	
training:	Epoch: [17][72/204]	Loss 0.1376 (0.2336)	
training:	Epoch: [17][73/204]	Loss 0.2566 (0.2340)	
training:	Epoch: [17][74/204]	Loss 0.1549 (0.2329)	
training:	Epoch: [17][75/204]	Loss 0.3392 (0.2343)	
training:	Epoch: [17][76/204]	Loss 0.1955 (0.2338)	
training:	Epoch: [17][77/204]	Loss 0.2848 (0.2345)	
training:	Epoch: [17][78/204]	Loss 0.1796 (0.2338)	
training:	Epoch: [17][79/204]	Loss 0.1349 (0.2325)	
training:	Epoch: [17][80/204]	Loss 0.2747 (0.2330)	
training:	Epoch: [17][81/204]	Loss 0.2684 (0.2335)	
training:	Epoch: [17][82/204]	Loss 0.5062 (0.2368)	
training:	Epoch: [17][83/204]	Loss 0.1698 (0.2360)	
training:	Epoch: [17][84/204]	Loss 0.1966 (0.2355)	
training:	Epoch: [17][85/204]	Loss 0.1502 (0.2345)	
training:	Epoch: [17][86/204]	Loss 0.3334 (0.2357)	
training:	Epoch: [17][87/204]	Loss 0.3355 (0.2368)	
training:	Epoch: [17][88/204]	Loss 0.3222 (0.2378)	
training:	Epoch: [17][89/204]	Loss 0.1823 (0.2372)	
training:	Epoch: [17][90/204]	Loss 0.1833 (0.2366)	
training:	Epoch: [17][91/204]	Loss 0.0629 (0.2347)	
training:	Epoch: [17][92/204]	Loss 0.2963 (0.2353)	
training:	Epoch: [17][93/204]	Loss 0.3216 (0.2363)	
training:	Epoch: [17][94/204]	Loss 0.2205 (0.2361)	
training:	Epoch: [17][95/204]	Loss 0.1669 (0.2354)	
training:	Epoch: [17][96/204]	Loss 0.1748 (0.2347)	
training:	Epoch: [17][97/204]	Loss 0.2444 (0.2348)	
training:	Epoch: [17][98/204]	Loss 0.3463 (0.2360)	
training:	Epoch: [17][99/204]	Loss 0.1690 (0.2353)	
training:	Epoch: [17][100/204]	Loss 0.3557 (0.2365)	
training:	Epoch: [17][101/204]	Loss 0.3044 (0.2372)	
training:	Epoch: [17][102/204]	Loss 0.2213 (0.2370)	
training:	Epoch: [17][103/204]	Loss 0.2370 (0.2370)	
training:	Epoch: [17][104/204]	Loss 0.2368 (0.2370)	
training:	Epoch: [17][105/204]	Loss 0.2159 (0.2368)	
training:	Epoch: [17][106/204]	Loss 0.2809 (0.2372)	
training:	Epoch: [17][107/204]	Loss 0.1071 (0.2360)	
training:	Epoch: [17][108/204]	Loss 0.2911 (0.2365)	
training:	Epoch: [17][109/204]	Loss 0.3220 (0.2373)	
training:	Epoch: [17][110/204]	Loss 0.4037 (0.2388)	
training:	Epoch: [17][111/204]	Loss 0.2220 (0.2387)	
training:	Epoch: [17][112/204]	Loss 0.1083 (0.2375)	
training:	Epoch: [17][113/204]	Loss 0.1255 (0.2365)	
training:	Epoch: [17][114/204]	Loss 0.2185 (0.2363)	
training:	Epoch: [17][115/204]	Loss 0.3377 (0.2372)	
training:	Epoch: [17][116/204]	Loss 0.2842 (0.2376)	
training:	Epoch: [17][117/204]	Loss 0.3696 (0.2388)	
training:	Epoch: [17][118/204]	Loss 0.1311 (0.2378)	
training:	Epoch: [17][119/204]	Loss 0.1789 (0.2374)	
training:	Epoch: [17][120/204]	Loss 0.2548 (0.2375)	
training:	Epoch: [17][121/204]	Loss 0.1858 (0.2371)	
training:	Epoch: [17][122/204]	Loss 0.1251 (0.2362)	
training:	Epoch: [17][123/204]	Loss 0.3580 (0.2371)	
training:	Epoch: [17][124/204]	Loss 0.1442 (0.2364)	
training:	Epoch: [17][125/204]	Loss 0.5503 (0.2389)	
training:	Epoch: [17][126/204]	Loss 0.1689 (0.2383)	
training:	Epoch: [17][127/204]	Loss 0.2714 (0.2386)	
training:	Epoch: [17][128/204]	Loss 0.2218 (0.2385)	
training:	Epoch: [17][129/204]	Loss 0.1149 (0.2375)	
training:	Epoch: [17][130/204]	Loss 0.4151 (0.2389)	
training:	Epoch: [17][131/204]	Loss 0.3755 (0.2399)	
training:	Epoch: [17][132/204]	Loss 0.1582 (0.2393)	
training:	Epoch: [17][133/204]	Loss 0.1686 (0.2388)	
training:	Epoch: [17][134/204]	Loss 0.3882 (0.2399)	
training:	Epoch: [17][135/204]	Loss 0.2771 (0.2402)	
training:	Epoch: [17][136/204]	Loss 0.1679 (0.2396)	
training:	Epoch: [17][137/204]	Loss 0.1870 (0.2393)	
training:	Epoch: [17][138/204]	Loss 0.2300 (0.2392)	
training:	Epoch: [17][139/204]	Loss 0.2203 (0.2390)	
training:	Epoch: [17][140/204]	Loss 0.2607 (0.2392)	
training:	Epoch: [17][141/204]	Loss 0.4149 (0.2405)	
training:	Epoch: [17][142/204]	Loss 0.1927 (0.2401)	
training:	Epoch: [17][143/204]	Loss 0.2470 (0.2402)	
training:	Epoch: [17][144/204]	Loss 0.2705 (0.2404)	
training:	Epoch: [17][145/204]	Loss 0.2981 (0.2408)	
training:	Epoch: [17][146/204]	Loss 0.0794 (0.2397)	
training:	Epoch: [17][147/204]	Loss 0.1500 (0.2391)	
training:	Epoch: [17][148/204]	Loss 0.1417 (0.2384)	
training:	Epoch: [17][149/204]	Loss 0.3248 (0.2390)	
training:	Epoch: [17][150/204]	Loss 0.2550 (0.2391)	
training:	Epoch: [17][151/204]	Loss 0.1246 (0.2383)	
training:	Epoch: [17][152/204]	Loss 0.3746 (0.2392)	
training:	Epoch: [17][153/204]	Loss 0.4324 (0.2405)	
training:	Epoch: [17][154/204]	Loss 0.3172 (0.2410)	
training:	Epoch: [17][155/204]	Loss 0.2606 (0.2411)	
training:	Epoch: [17][156/204]	Loss 0.2754 (0.2413)	
training:	Epoch: [17][157/204]	Loss 0.4106 (0.2424)	
training:	Epoch: [17][158/204]	Loss 0.3507 (0.2431)	
training:	Epoch: [17][159/204]	Loss 0.3443 (0.2437)	
training:	Epoch: [17][160/204]	Loss 0.1195 (0.2430)	
training:	Epoch: [17][161/204]	Loss 0.3489 (0.2436)	
training:	Epoch: [17][162/204]	Loss 0.1884 (0.2433)	
training:	Epoch: [17][163/204]	Loss 0.3206 (0.2437)	
training:	Epoch: [17][164/204]	Loss 0.2516 (0.2438)	
training:	Epoch: [17][165/204]	Loss 0.0974 (0.2429)	
training:	Epoch: [17][166/204]	Loss 0.2422 (0.2429)	
training:	Epoch: [17][167/204]	Loss 0.1619 (0.2424)	
training:	Epoch: [17][168/204]	Loss 0.1201 (0.2417)	
training:	Epoch: [17][169/204]	Loss 0.2511 (0.2417)	
training:	Epoch: [17][170/204]	Loss 0.1338 (0.2411)	
training:	Epoch: [17][171/204]	Loss 0.2483 (0.2412)	
training:	Epoch: [17][172/204]	Loss 0.2557 (0.2412)	
training:	Epoch: [17][173/204]	Loss 0.2400 (0.2412)	
training:	Epoch: [17][174/204]	Loss 0.3283 (0.2417)	
training:	Epoch: [17][175/204]	Loss 0.4348 (0.2428)	
training:	Epoch: [17][176/204]	Loss 0.1884 (0.2425)	
training:	Epoch: [17][177/204]	Loss 0.2653 (0.2427)	
training:	Epoch: [17][178/204]	Loss 0.2689 (0.2428)	
training:	Epoch: [17][179/204]	Loss 0.1596 (0.2423)	
training:	Epoch: [17][180/204]	Loss 0.1748 (0.2420)	
training:	Epoch: [17][181/204]	Loss 0.1585 (0.2415)	
training:	Epoch: [17][182/204]	Loss 0.2504 (0.2415)	
training:	Epoch: [17][183/204]	Loss 0.1579 (0.2411)	
training:	Epoch: [17][184/204]	Loss 0.3184 (0.2415)	
training:	Epoch: [17][185/204]	Loss 0.2556 (0.2416)	
training:	Epoch: [17][186/204]	Loss 0.3742 (0.2423)	
training:	Epoch: [17][187/204]	Loss 0.2006 (0.2421)	
training:	Epoch: [17][188/204]	Loss 0.1797 (0.2417)	
training:	Epoch: [17][189/204]	Loss 0.2736 (0.2419)	
training:	Epoch: [17][190/204]	Loss 0.2451 (0.2419)	
training:	Epoch: [17][191/204]	Loss 0.2549 (0.2420)	
training:	Epoch: [17][192/204]	Loss 0.1672 (0.2416)	
training:	Epoch: [17][193/204]	Loss 0.3289 (0.2421)	
training:	Epoch: [17][194/204]	Loss 0.3204 (0.2425)	
training:	Epoch: [17][195/204]	Loss 0.3498 (0.2430)	
training:	Epoch: [17][196/204]	Loss 0.1636 (0.2426)	
training:	Epoch: [17][197/204]	Loss 0.1144 (0.2420)	
training:	Epoch: [17][198/204]	Loss 0.0766 (0.2411)	
training:	Epoch: [17][199/204]	Loss 0.2958 (0.2414)	
training:	Epoch: [17][200/204]	Loss 0.3358 (0.2419)	
training:	Epoch: [17][201/204]	Loss 0.1831 (0.2416)	
training:	Epoch: [17][202/204]	Loss 0.1944 (0.2413)	
training:	Epoch: [17][203/204]	Loss 0.2227 (0.2413)	
training:	Epoch: [17][204/204]	Loss 0.1759 (0.2409)	
Training:	 Loss: 0.2406

Training:	 ACC: 0.9375 0.9373 0.9312 0.9439
Validation:	 ACC: 0.8222 0.8224 0.8260 0.8184
Validation:	 Best_BACC: 0.8326 0.8331 0.8424 0.8229
Validation:	 Loss: 0.4353
Pretraining:	Epoch 18/200
----------
training:	Epoch: [18][1/204]	Loss 0.1497 (0.1497)	
training:	Epoch: [18][2/204]	Loss 0.3254 (0.2376)	
training:	Epoch: [18][3/204]	Loss 0.1307 (0.2019)	
training:	Epoch: [18][4/204]	Loss 0.3067 (0.2281)	
training:	Epoch: [18][5/204]	Loss 0.1602 (0.2145)	
training:	Epoch: [18][6/204]	Loss 0.2987 (0.2286)	
training:	Epoch: [18][7/204]	Loss 0.1404 (0.2160)	
training:	Epoch: [18][8/204]	Loss 0.1234 (0.2044)	
training:	Epoch: [18][9/204]	Loss 0.1146 (0.1944)	
training:	Epoch: [18][10/204]	Loss 0.0609 (0.1811)	
training:	Epoch: [18][11/204]	Loss 0.1158 (0.1751)	
training:	Epoch: [18][12/204]	Loss 0.1952 (0.1768)	
training:	Epoch: [18][13/204]	Loss 0.4104 (0.1948)	
training:	Epoch: [18][14/204]	Loss 0.1574 (0.1921)	
training:	Epoch: [18][15/204]	Loss 0.3251 (0.2010)	
training:	Epoch: [18][16/204]	Loss 0.2011 (0.2010)	
training:	Epoch: [18][17/204]	Loss 0.1302 (0.1968)	
training:	Epoch: [18][18/204]	Loss 0.3802 (0.2070)	
training:	Epoch: [18][19/204]	Loss 0.2242 (0.2079)	
training:	Epoch: [18][20/204]	Loss 0.1323 (0.2041)	
training:	Epoch: [18][21/204]	Loss 0.2337 (0.2055)	
training:	Epoch: [18][22/204]	Loss 0.1707 (0.2040)	
training:	Epoch: [18][23/204]	Loss 0.3929 (0.2122)	
training:	Epoch: [18][24/204]	Loss 0.3194 (0.2166)	
training:	Epoch: [18][25/204]	Loss 0.3056 (0.2202)	
training:	Epoch: [18][26/204]	Loss 0.1681 (0.2182)	
training:	Epoch: [18][27/204]	Loss 0.1924 (0.2172)	
training:	Epoch: [18][28/204]	Loss 0.1903 (0.2163)	
training:	Epoch: [18][29/204]	Loss 0.1171 (0.2129)	
training:	Epoch: [18][30/204]	Loss 0.1808 (0.2118)	
training:	Epoch: [18][31/204]	Loss 0.3358 (0.2158)	
training:	Epoch: [18][32/204]	Loss 0.2012 (0.2153)	
training:	Epoch: [18][33/204]	Loss 0.3288 (0.2188)	
training:	Epoch: [18][34/204]	Loss 0.1522 (0.2168)	
training:	Epoch: [18][35/204]	Loss 0.1803 (0.2158)	
training:	Epoch: [18][36/204]	Loss 0.2101 (0.2156)	
training:	Epoch: [18][37/204]	Loss 0.1202 (0.2130)	
training:	Epoch: [18][38/204]	Loss 0.1399 (0.2111)	
training:	Epoch: [18][39/204]	Loss 0.4576 (0.2174)	
training:	Epoch: [18][40/204]	Loss 0.2807 (0.2190)	
training:	Epoch: [18][41/204]	Loss 0.2970 (0.2209)	
training:	Epoch: [18][42/204]	Loss 0.1564 (0.2194)	
training:	Epoch: [18][43/204]	Loss 0.1474 (0.2177)	
training:	Epoch: [18][44/204]	Loss 0.3270 (0.2202)	
training:	Epoch: [18][45/204]	Loss 0.1524 (0.2187)	
training:	Epoch: [18][46/204]	Loss 0.1457 (0.2171)	
training:	Epoch: [18][47/204]	Loss 0.4240 (0.2215)	
training:	Epoch: [18][48/204]	Loss 0.1072 (0.2191)	
training:	Epoch: [18][49/204]	Loss 0.1691 (0.2181)	
training:	Epoch: [18][50/204]	Loss 0.3105 (0.2199)	
training:	Epoch: [18][51/204]	Loss 0.2274 (0.2201)	
training:	Epoch: [18][52/204]	Loss 0.0945 (0.2177)	
training:	Epoch: [18][53/204]	Loss 0.0868 (0.2152)	
training:	Epoch: [18][54/204]	Loss 0.1355 (0.2137)	
training:	Epoch: [18][55/204]	Loss 0.1713 (0.2130)	
training:	Epoch: [18][56/204]	Loss 0.1674 (0.2121)	
training:	Epoch: [18][57/204]	Loss 0.0857 (0.2099)	
training:	Epoch: [18][58/204]	Loss 0.2207 (0.2101)	
training:	Epoch: [18][59/204]	Loss 0.1847 (0.2097)	
training:	Epoch: [18][60/204]	Loss 0.2158 (0.2098)	
training:	Epoch: [18][61/204]	Loss 0.1625 (0.2090)	
training:	Epoch: [18][62/204]	Loss 0.2694 (0.2100)	
training:	Epoch: [18][63/204]	Loss 0.2047 (0.2099)	
training:	Epoch: [18][64/204]	Loss 0.1692 (0.2093)	
training:	Epoch: [18][65/204]	Loss 0.1827 (0.2089)	
training:	Epoch: [18][66/204]	Loss 0.1005 (0.2072)	
training:	Epoch: [18][67/204]	Loss 0.2177 (0.2074)	
training:	Epoch: [18][68/204]	Loss 0.3186 (0.2090)	
training:	Epoch: [18][69/204]	Loss 0.1233 (0.2078)	
training:	Epoch: [18][70/204]	Loss 0.2040 (0.2077)	
training:	Epoch: [18][71/204]	Loss 0.1460 (0.2068)	
training:	Epoch: [18][72/204]	Loss 0.1710 (0.2063)	
training:	Epoch: [18][73/204]	Loss 0.2631 (0.2071)	
training:	Epoch: [18][74/204]	Loss 0.2400 (0.2076)	
training:	Epoch: [18][75/204]	Loss 0.1374 (0.2066)	
training:	Epoch: [18][76/204]	Loss 0.3411 (0.2084)	
training:	Epoch: [18][77/204]	Loss 0.2408 (0.2088)	
training:	Epoch: [18][78/204]	Loss 0.1533 (0.2081)	
training:	Epoch: [18][79/204]	Loss 0.1832 (0.2078)	
training:	Epoch: [18][80/204]	Loss 0.1953 (0.2076)	
training:	Epoch: [18][81/204]	Loss 0.1876 (0.2074)	
training:	Epoch: [18][82/204]	Loss 0.1093 (0.2062)	
training:	Epoch: [18][83/204]	Loss 0.1384 (0.2054)	
training:	Epoch: [18][84/204]	Loss 0.2875 (0.2063)	
training:	Epoch: [18][85/204]	Loss 0.2873 (0.2073)	
training:	Epoch: [18][86/204]	Loss 0.3754 (0.2093)	
training:	Epoch: [18][87/204]	Loss 0.2375 (0.2096)	
training:	Epoch: [18][88/204]	Loss 0.3396 (0.2111)	
training:	Epoch: [18][89/204]	Loss 0.1485 (0.2104)	
training:	Epoch: [18][90/204]	Loss 0.2166 (0.2104)	
training:	Epoch: [18][91/204]	Loss 0.2182 (0.2105)	
training:	Epoch: [18][92/204]	Loss 0.2215 (0.2106)	
training:	Epoch: [18][93/204]	Loss 0.1505 (0.2100)	
training:	Epoch: [18][94/204]	Loss 0.3629 (0.2116)	
training:	Epoch: [18][95/204]	Loss 0.3028 (0.2126)	
training:	Epoch: [18][96/204]	Loss 0.0992 (0.2114)	
training:	Epoch: [18][97/204]	Loss 0.2976 (0.2123)	
training:	Epoch: [18][98/204]	Loss 0.0836 (0.2110)	
training:	Epoch: [18][99/204]	Loss 0.1599 (0.2104)	
training:	Epoch: [18][100/204]	Loss 0.1309 (0.2097)	
training:	Epoch: [18][101/204]	Loss 0.2474 (0.2100)	
training:	Epoch: [18][102/204]	Loss 0.3130 (0.2110)	
training:	Epoch: [18][103/204]	Loss 0.4704 (0.2136)	
training:	Epoch: [18][104/204]	Loss 0.2299 (0.2137)	
training:	Epoch: [18][105/204]	Loss 0.2098 (0.2137)	
training:	Epoch: [18][106/204]	Loss 0.1572 (0.2131)	
training:	Epoch: [18][107/204]	Loss 0.2037 (0.2131)	
training:	Epoch: [18][108/204]	Loss 0.2036 (0.2130)	
training:	Epoch: [18][109/204]	Loss 0.1167 (0.2121)	
training:	Epoch: [18][110/204]	Loss 0.1017 (0.2111)	
training:	Epoch: [18][111/204]	Loss 0.1654 (0.2107)	
training:	Epoch: [18][112/204]	Loss 0.2561 (0.2111)	
training:	Epoch: [18][113/204]	Loss 0.2678 (0.2116)	
training:	Epoch: [18][114/204]	Loss 0.0608 (0.2103)	
training:	Epoch: [18][115/204]	Loss 0.1679 (0.2099)	
training:	Epoch: [18][116/204]	Loss 0.2598 (0.2103)	
training:	Epoch: [18][117/204]	Loss 0.1803 (0.2101)	
training:	Epoch: [18][118/204]	Loss 0.3318 (0.2111)	
training:	Epoch: [18][119/204]	Loss 0.1707 (0.2108)	
training:	Epoch: [18][120/204]	Loss 0.4397 (0.2127)	
training:	Epoch: [18][121/204]	Loss 0.1565 (0.2122)	
training:	Epoch: [18][122/204]	Loss 0.2720 (0.2127)	
training:	Epoch: [18][123/204]	Loss 0.1829 (0.2124)	
training:	Epoch: [18][124/204]	Loss 0.2720 (0.2129)	
training:	Epoch: [18][125/204]	Loss 0.4597 (0.2149)	
training:	Epoch: [18][126/204]	Loss 0.1452 (0.2143)	
training:	Epoch: [18][127/204]	Loss 0.1984 (0.2142)	
training:	Epoch: [18][128/204]	Loss 0.0692 (0.2131)	
training:	Epoch: [18][129/204]	Loss 0.3452 (0.2141)	
training:	Epoch: [18][130/204]	Loss 0.1436 (0.2136)	
training:	Epoch: [18][131/204]	Loss 0.3224 (0.2144)	
training:	Epoch: [18][132/204]	Loss 0.1793 (0.2141)	
training:	Epoch: [18][133/204]	Loss 0.3167 (0.2149)	
training:	Epoch: [18][134/204]	Loss 0.4035 (0.2163)	
training:	Epoch: [18][135/204]	Loss 0.2086 (0.2163)	
training:	Epoch: [18][136/204]	Loss 0.3028 (0.2169)	
training:	Epoch: [18][137/204]	Loss 0.2580 (0.2172)	
training:	Epoch: [18][138/204]	Loss 0.1017 (0.2164)	
training:	Epoch: [18][139/204]	Loss 0.0836 (0.2154)	
training:	Epoch: [18][140/204]	Loss 0.1733 (0.2151)	
training:	Epoch: [18][141/204]	Loss 0.1474 (0.2146)	
training:	Epoch: [18][142/204]	Loss 0.1154 (0.2139)	
training:	Epoch: [18][143/204]	Loss 0.2563 (0.2142)	
training:	Epoch: [18][144/204]	Loss 0.2513 (0.2145)	
training:	Epoch: [18][145/204]	Loss 0.1483 (0.2140)	
training:	Epoch: [18][146/204]	Loss 0.2878 (0.2145)	
training:	Epoch: [18][147/204]	Loss 0.3167 (0.2152)	
training:	Epoch: [18][148/204]	Loss 0.1974 (0.2151)	
training:	Epoch: [18][149/204]	Loss 0.2060 (0.2150)	
training:	Epoch: [18][150/204]	Loss 0.2072 (0.2150)	
training:	Epoch: [18][151/204]	Loss 0.2430 (0.2152)	
training:	Epoch: [18][152/204]	Loss 0.2681 (0.2155)	
training:	Epoch: [18][153/204]	Loss 0.2016 (0.2154)	
training:	Epoch: [18][154/204]	Loss 0.2951 (0.2159)	
training:	Epoch: [18][155/204]	Loss 0.2108 (0.2159)	
training:	Epoch: [18][156/204]	Loss 0.0693 (0.2150)	
training:	Epoch: [18][157/204]	Loss 0.2707 (0.2153)	
training:	Epoch: [18][158/204]	Loss 0.3676 (0.2163)	
training:	Epoch: [18][159/204]	Loss 0.4295 (0.2176)	
training:	Epoch: [18][160/204]	Loss 0.2061 (0.2176)	
training:	Epoch: [18][161/204]	Loss 0.2178 (0.2176)	
training:	Epoch: [18][162/204]	Loss 0.5189 (0.2194)	
training:	Epoch: [18][163/204]	Loss 0.1944 (0.2193)	
training:	Epoch: [18][164/204]	Loss 0.2216 (0.2193)	
training:	Epoch: [18][165/204]	Loss 0.1056 (0.2186)	
training:	Epoch: [18][166/204]	Loss 0.3465 (0.2194)	
training:	Epoch: [18][167/204]	Loss 0.3905 (0.2204)	
training:	Epoch: [18][168/204]	Loss 0.3295 (0.2210)	
training:	Epoch: [18][169/204]	Loss 0.0896 (0.2203)	
training:	Epoch: [18][170/204]	Loss 0.1941 (0.2201)	
training:	Epoch: [18][171/204]	Loss 0.3420 (0.2208)	
training:	Epoch: [18][172/204]	Loss 0.2753 (0.2211)	
training:	Epoch: [18][173/204]	Loss 0.3554 (0.2219)	
training:	Epoch: [18][174/204]	Loss 0.1267 (0.2214)	
training:	Epoch: [18][175/204]	Loss 0.1018 (0.2207)	
training:	Epoch: [18][176/204]	Loss 0.2775 (0.2210)	
training:	Epoch: [18][177/204]	Loss 0.1318 (0.2205)	
training:	Epoch: [18][178/204]	Loss 0.1133 (0.2199)	
training:	Epoch: [18][179/204]	Loss 0.1748 (0.2196)	
training:	Epoch: [18][180/204]	Loss 0.2443 (0.2198)	
training:	Epoch: [18][181/204]	Loss 0.1525 (0.2194)	
training:	Epoch: [18][182/204]	Loss 0.3041 (0.2199)	
training:	Epoch: [18][183/204]	Loss 0.3748 (0.2207)	
training:	Epoch: [18][184/204]	Loss 0.3777 (0.2216)	
training:	Epoch: [18][185/204]	Loss 0.2826 (0.2219)	
training:	Epoch: [18][186/204]	Loss 0.3692 (0.2227)	
training:	Epoch: [18][187/204]	Loss 0.2063 (0.2226)	
training:	Epoch: [18][188/204]	Loss 0.2976 (0.2230)	
training:	Epoch: [18][189/204]	Loss 0.1998 (0.2229)	
training:	Epoch: [18][190/204]	Loss 0.3109 (0.2233)	
training:	Epoch: [18][191/204]	Loss 0.1795 (0.2231)	
training:	Epoch: [18][192/204]	Loss 0.2521 (0.2233)	
training:	Epoch: [18][193/204]	Loss 0.1851 (0.2231)	
training:	Epoch: [18][194/204]	Loss 0.2843 (0.2234)	
training:	Epoch: [18][195/204]	Loss 0.1562 (0.2230)	
training:	Epoch: [18][196/204]	Loss 0.3018 (0.2234)	
training:	Epoch: [18][197/204]	Loss 0.2218 (0.2234)	
training:	Epoch: [18][198/204]	Loss 0.3460 (0.2241)	
training:	Epoch: [18][199/204]	Loss 0.2817 (0.2243)	
training:	Epoch: [18][200/204]	Loss 0.3271 (0.2249)	
training:	Epoch: [18][201/204]	Loss 0.2951 (0.2252)	
training:	Epoch: [18][202/204]	Loss 0.2755 (0.2255)	
training:	Epoch: [18][203/204]	Loss 0.3181 (0.2259)	
training:	Epoch: [18][204/204]	Loss 0.1073 (0.2253)	
Training:	 Loss: 0.2250

Training:	 ACC: 0.9425 0.9414 0.9153 0.9697
Validation:	 ACC: 0.8246 0.8229 0.7881 0.8610
Validation:	 Best_BACC: 0.8326 0.8331 0.8424 0.8229
Validation:	 Loss: 0.4519
Pretraining:	Epoch 19/200
----------
training:	Epoch: [19][1/204]	Loss 0.2338 (0.2338)	
training:	Epoch: [19][2/204]	Loss 0.2391 (0.2364)	
training:	Epoch: [19][3/204]	Loss 0.1001 (0.1910)	
training:	Epoch: [19][4/204]	Loss 0.2685 (0.2104)	
training:	Epoch: [19][5/204]	Loss 0.2399 (0.2163)	
training:	Epoch: [19][6/204]	Loss 0.0724 (0.1923)	
training:	Epoch: [19][7/204]	Loss 0.1750 (0.1898)	
training:	Epoch: [19][8/204]	Loss 0.0746 (0.1754)	
training:	Epoch: [19][9/204]	Loss 0.1681 (0.1746)	
training:	Epoch: [19][10/204]	Loss 0.1672 (0.1739)	
training:	Epoch: [19][11/204]	Loss 0.0822 (0.1655)	
training:	Epoch: [19][12/204]	Loss 0.2296 (0.1709)	
training:	Epoch: [19][13/204]	Loss 0.1292 (0.1677)	
training:	Epoch: [19][14/204]	Loss 0.0968 (0.1626)	
training:	Epoch: [19][15/204]	Loss 0.2523 (0.1686)	
training:	Epoch: [19][16/204]	Loss 0.1070 (0.1647)	
training:	Epoch: [19][17/204]	Loss 0.1941 (0.1665)	
training:	Epoch: [19][18/204]	Loss 0.1935 (0.1680)	
training:	Epoch: [19][19/204]	Loss 0.2703 (0.1734)	
training:	Epoch: [19][20/204]	Loss 0.1457 (0.1720)	
training:	Epoch: [19][21/204]	Loss 0.1488 (0.1709)	
training:	Epoch: [19][22/204]	Loss 0.0938 (0.1674)	
training:	Epoch: [19][23/204]	Loss 0.2424 (0.1706)	
training:	Epoch: [19][24/204]	Loss 0.2454 (0.1737)	
training:	Epoch: [19][25/204]	Loss 0.2167 (0.1755)	
training:	Epoch: [19][26/204]	Loss 0.2448 (0.1781)	
training:	Epoch: [19][27/204]	Loss 0.3520 (0.1846)	
training:	Epoch: [19][28/204]	Loss 0.0901 (0.1812)	
training:	Epoch: [19][29/204]	Loss 0.3418 (0.1867)	
training:	Epoch: [19][30/204]	Loss 0.0802 (0.1832)	
training:	Epoch: [19][31/204]	Loss 0.1083 (0.1808)	
training:	Epoch: [19][32/204]	Loss 0.2359 (0.1825)	
training:	Epoch: [19][33/204]	Loss 0.1296 (0.1809)	
training:	Epoch: [19][34/204]	Loss 0.2319 (0.1824)	
training:	Epoch: [19][35/204]	Loss 0.2376 (0.1840)	
training:	Epoch: [19][36/204]	Loss 0.0878 (0.1813)	
training:	Epoch: [19][37/204]	Loss 0.1454 (0.1803)	
training:	Epoch: [19][38/204]	Loss 0.3488 (0.1848)	
training:	Epoch: [19][39/204]	Loss 0.0992 (0.1826)	
training:	Epoch: [19][40/204]	Loss 0.1602 (0.1820)	
training:	Epoch: [19][41/204]	Loss 0.2199 (0.1829)	
training:	Epoch: [19][42/204]	Loss 0.1250 (0.1816)	
training:	Epoch: [19][43/204]	Loss 0.2284 (0.1826)	
training:	Epoch: [19][44/204]	Loss 0.1295 (0.1814)	
training:	Epoch: [19][45/204]	Loss 0.2893 (0.1838)	
training:	Epoch: [19][46/204]	Loss 0.0828 (0.1816)	
training:	Epoch: [19][47/204]	Loss 0.1156 (0.1802)	
training:	Epoch: [19][48/204]	Loss 0.3231 (0.1832)	
training:	Epoch: [19][49/204]	Loss 0.1089 (0.1817)	
training:	Epoch: [19][50/204]	Loss 0.1588 (0.1812)	
training:	Epoch: [19][51/204]	Loss 0.2255 (0.1821)	
training:	Epoch: [19][52/204]	Loss 0.3249 (0.1848)	
training:	Epoch: [19][53/204]	Loss 0.2385 (0.1859)	
training:	Epoch: [19][54/204]	Loss 0.1525 (0.1852)	
training:	Epoch: [19][55/204]	Loss 0.2296 (0.1860)	
training:	Epoch: [19][56/204]	Loss 0.1320 (0.1851)	
training:	Epoch: [19][57/204]	Loss 0.1864 (0.1851)	
training:	Epoch: [19][58/204]	Loss 0.2097 (0.1855)	
training:	Epoch: [19][59/204]	Loss 0.1126 (0.1843)	
training:	Epoch: [19][60/204]	Loss 0.4252 (0.1883)	
training:	Epoch: [19][61/204]	Loss 0.4328 (0.1923)	
training:	Epoch: [19][62/204]	Loss 0.1446 (0.1915)	
training:	Epoch: [19][63/204]	Loss 0.1637 (0.1911)	
training:	Epoch: [19][64/204]	Loss 0.1207 (0.1900)	
training:	Epoch: [19][65/204]	Loss 0.3787 (0.1929)	
training:	Epoch: [19][66/204]	Loss 0.1527 (0.1923)	
training:	Epoch: [19][67/204]	Loss 0.2805 (0.1936)	
training:	Epoch: [19][68/204]	Loss 0.1495 (0.1930)	
training:	Epoch: [19][69/204]	Loss 0.3247 (0.1949)	
training:	Epoch: [19][70/204]	Loss 0.2432 (0.1956)	
training:	Epoch: [19][71/204]	Loss 0.1314 (0.1947)	
training:	Epoch: [19][72/204]	Loss 0.0696 (0.1929)	
training:	Epoch: [19][73/204]	Loss 0.3210 (0.1947)	
training:	Epoch: [19][74/204]	Loss 0.1884 (0.1946)	
training:	Epoch: [19][75/204]	Loss 0.2850 (0.1958)	
training:	Epoch: [19][76/204]	Loss 0.0614 (0.1940)	
training:	Epoch: [19][77/204]	Loss 0.2057 (0.1942)	
training:	Epoch: [19][78/204]	Loss 0.1604 (0.1937)	
training:	Epoch: [19][79/204]	Loss 0.3222 (0.1954)	
training:	Epoch: [19][80/204]	Loss 0.1601 (0.1949)	
training:	Epoch: [19][81/204]	Loss 0.2486 (0.1956)	
training:	Epoch: [19][82/204]	Loss 0.3323 (0.1973)	
training:	Epoch: [19][83/204]	Loss 0.2821 (0.1983)	
training:	Epoch: [19][84/204]	Loss 0.2269 (0.1986)	
training:	Epoch: [19][85/204]	Loss 0.0518 (0.1969)	
training:	Epoch: [19][86/204]	Loss 0.2894 (0.1980)	
training:	Epoch: [19][87/204]	Loss 0.2097 (0.1981)	
training:	Epoch: [19][88/204]	Loss 0.1318 (0.1974)	
training:	Epoch: [19][89/204]	Loss 0.2786 (0.1983)	
training:	Epoch: [19][90/204]	Loss 0.2428 (0.1988)	
training:	Epoch: [19][91/204]	Loss 0.2562 (0.1994)	
training:	Epoch: [19][92/204]	Loss 0.2918 (0.2004)	
training:	Epoch: [19][93/204]	Loss 0.2764 (0.2012)	
training:	Epoch: [19][94/204]	Loss 0.1563 (0.2007)	
training:	Epoch: [19][95/204]	Loss 0.1583 (0.2003)	
training:	Epoch: [19][96/204]	Loss 0.2654 (0.2010)	
training:	Epoch: [19][97/204]	Loss 0.3338 (0.2023)	
training:	Epoch: [19][98/204]	Loss 0.1534 (0.2018)	
training:	Epoch: [19][99/204]	Loss 0.1351 (0.2012)	
training:	Epoch: [19][100/204]	Loss 0.1601 (0.2008)	
training:	Epoch: [19][101/204]	Loss 0.1661 (0.2004)	
training:	Epoch: [19][102/204]	Loss 0.1985 (0.2004)	
training:	Epoch: [19][103/204]	Loss 0.2492 (0.2009)	
training:	Epoch: [19][104/204]	Loss 0.0946 (0.1998)	
training:	Epoch: [19][105/204]	Loss 0.0959 (0.1989)	
training:	Epoch: [19][106/204]	Loss 0.1064 (0.1980)	
training:	Epoch: [19][107/204]	Loss 0.3821 (0.1997)	
training:	Epoch: [19][108/204]	Loss 0.2107 (0.1998)	
training:	Epoch: [19][109/204]	Loss 0.1243 (0.1991)	
training:	Epoch: [19][110/204]	Loss 0.4818 (0.2017)	
training:	Epoch: [19][111/204]	Loss 0.1949 (0.2016)	
training:	Epoch: [19][112/204]	Loss 0.2232 (0.2018)	
training:	Epoch: [19][113/204]	Loss 0.3236 (0.2029)	
training:	Epoch: [19][114/204]	Loss 0.1816 (0.2027)	
training:	Epoch: [19][115/204]	Loss 0.2504 (0.2031)	
training:	Epoch: [19][116/204]	Loss 0.2120 (0.2032)	
training:	Epoch: [19][117/204]	Loss 0.2141 (0.2033)	
training:	Epoch: [19][118/204]	Loss 0.2661 (0.2038)	
training:	Epoch: [19][119/204]	Loss 0.2863 (0.2045)	
training:	Epoch: [19][120/204]	Loss 0.1718 (0.2042)	
training:	Epoch: [19][121/204]	Loss 0.1894 (0.2041)	
training:	Epoch: [19][122/204]	Loss 0.1356 (0.2036)	
training:	Epoch: [19][123/204]	Loss 0.1213 (0.2029)	
training:	Epoch: [19][124/204]	Loss 0.1934 (0.2028)	
training:	Epoch: [19][125/204]	Loss 0.2756 (0.2034)	
training:	Epoch: [19][126/204]	Loss 0.0756 (0.2024)	
training:	Epoch: [19][127/204]	Loss 0.4693 (0.2045)	
training:	Epoch: [19][128/204]	Loss 0.2927 (0.2052)	
training:	Epoch: [19][129/204]	Loss 0.0881 (0.2043)	
training:	Epoch: [19][130/204]	Loss 0.1499 (0.2038)	
training:	Epoch: [19][131/204]	Loss 0.1697 (0.2036)	
training:	Epoch: [19][132/204]	Loss 0.0640 (0.2025)	
training:	Epoch: [19][133/204]	Loss 0.1199 (0.2019)	
training:	Epoch: [19][134/204]	Loss 0.2397 (0.2022)	
training:	Epoch: [19][135/204]	Loss 0.1829 (0.2020)	
training:	Epoch: [19][136/204]	Loss 0.1785 (0.2019)	
training:	Epoch: [19][137/204]	Loss 0.2386 (0.2021)	
training:	Epoch: [19][138/204]	Loss 0.2338 (0.2024)	
training:	Epoch: [19][139/204]	Loss 0.1556 (0.2020)	
training:	Epoch: [19][140/204]	Loss 0.1686 (0.2018)	
training:	Epoch: [19][141/204]	Loss 0.2518 (0.2022)	
training:	Epoch: [19][142/204]	Loss 0.1719 (0.2019)	
training:	Epoch: [19][143/204]	Loss 0.1758 (0.2018)	
training:	Epoch: [19][144/204]	Loss 0.2223 (0.2019)	
training:	Epoch: [19][145/204]	Loss 0.1561 (0.2016)	
training:	Epoch: [19][146/204]	Loss 0.1070 (0.2009)	
training:	Epoch: [19][147/204]	Loss 0.2173 (0.2010)	
training:	Epoch: [19][148/204]	Loss 0.1397 (0.2006)	
training:	Epoch: [19][149/204]	Loss 0.3757 (0.2018)	
training:	Epoch: [19][150/204]	Loss 0.1501 (0.2015)	
training:	Epoch: [19][151/204]	Loss 0.1161 (0.2009)	
training:	Epoch: [19][152/204]	Loss 0.1341 (0.2005)	
training:	Epoch: [19][153/204]	Loss 0.2673 (0.2009)	
training:	Epoch: [19][154/204]	Loss 0.1369 (0.2005)	
training:	Epoch: [19][155/204]	Loss 0.1053 (0.1999)	
training:	Epoch: [19][156/204]	Loss 0.0956 (0.1992)	
training:	Epoch: [19][157/204]	Loss 0.3086 (0.1999)	
training:	Epoch: [19][158/204]	Loss 0.1852 (0.1998)	
training:	Epoch: [19][159/204]	Loss 0.2741 (0.2003)	
training:	Epoch: [19][160/204]	Loss 0.1184 (0.1998)	
training:	Epoch: [19][161/204]	Loss 0.2415 (0.2000)	
training:	Epoch: [19][162/204]	Loss 0.0955 (0.1994)	
training:	Epoch: [19][163/204]	Loss 0.1488 (0.1991)	
training:	Epoch: [19][164/204]	Loss 0.2549 (0.1994)	
training:	Epoch: [19][165/204]	Loss 0.1847 (0.1993)	
training:	Epoch: [19][166/204]	Loss 0.2334 (0.1995)	
training:	Epoch: [19][167/204]	Loss 0.1920 (0.1995)	
training:	Epoch: [19][168/204]	Loss 0.2568 (0.1998)	
training:	Epoch: [19][169/204]	Loss 0.1537 (0.1995)	
training:	Epoch: [19][170/204]	Loss 0.0976 (0.1989)	
training:	Epoch: [19][171/204]	Loss 0.1161 (0.1985)	
training:	Epoch: [19][172/204]	Loss 0.1106 (0.1979)	
training:	Epoch: [19][173/204]	Loss 0.2082 (0.1980)	
training:	Epoch: [19][174/204]	Loss 0.5442 (0.2000)	
training:	Epoch: [19][175/204]	Loss 0.0980 (0.1994)	
training:	Epoch: [19][176/204]	Loss 0.2530 (0.1997)	
training:	Epoch: [19][177/204]	Loss 0.4807 (0.2013)	
training:	Epoch: [19][178/204]	Loss 0.2246 (0.2014)	
training:	Epoch: [19][179/204]	Loss 0.2974 (0.2020)	
training:	Epoch: [19][180/204]	Loss 0.3655 (0.2029)	
training:	Epoch: [19][181/204]	Loss 0.1158 (0.2024)	
training:	Epoch: [19][182/204]	Loss 0.2725 (0.2028)	
training:	Epoch: [19][183/204]	Loss 0.4851 (0.2043)	
training:	Epoch: [19][184/204]	Loss 0.0860 (0.2037)	
training:	Epoch: [19][185/204]	Loss 0.1406 (0.2033)	
training:	Epoch: [19][186/204]	Loss 0.1563 (0.2031)	
training:	Epoch: [19][187/204]	Loss 0.2280 (0.2032)	
training:	Epoch: [19][188/204]	Loss 0.1184 (0.2028)	
training:	Epoch: [19][189/204]	Loss 0.1945 (0.2027)	
training:	Epoch: [19][190/204]	Loss 0.2899 (0.2032)	
training:	Epoch: [19][191/204]	Loss 0.3467 (0.2039)	
training:	Epoch: [19][192/204]	Loss 0.1770 (0.2038)	
training:	Epoch: [19][193/204]	Loss 0.1816 (0.2037)	
training:	Epoch: [19][194/204]	Loss 0.0646 (0.2030)	
training:	Epoch: [19][195/204]	Loss 0.4375 (0.2042)	
training:	Epoch: [19][196/204]	Loss 0.1631 (0.2040)	
training:	Epoch: [19][197/204]	Loss 0.2015 (0.2039)	
training:	Epoch: [19][198/204]	Loss 0.2561 (0.2042)	
training:	Epoch: [19][199/204]	Loss 0.2756 (0.2046)	
training:	Epoch: [19][200/204]	Loss 0.2016 (0.2046)	
training:	Epoch: [19][201/204]	Loss 0.1222 (0.2041)	
training:	Epoch: [19][202/204]	Loss 0.1872 (0.2041)	
training:	Epoch: [19][203/204]	Loss 0.1569 (0.2038)	
training:	Epoch: [19][204/204]	Loss 0.2273 (0.2039)	
Training:	 Loss: 0.2036

Training:	 ACC: 0.9600 0.9599 0.9591 0.9608
Validation:	 ACC: 0.8172 0.8175 0.8250 0.8094
Validation:	 Best_BACC: 0.8326 0.8331 0.8424 0.8229
Validation:	 Loss: 0.4628
Pretraining:	Epoch 20/200
----------
training:	Epoch: [20][1/204]	Loss 0.1068 (0.1068)	
training:	Epoch: [20][2/204]	Loss 0.0916 (0.0992)	
training:	Epoch: [20][3/204]	Loss 0.1929 (0.1304)	
training:	Epoch: [20][4/204]	Loss 0.1921 (0.1458)	
training:	Epoch: [20][5/204]	Loss 0.1684 (0.1504)	
training:	Epoch: [20][6/204]	Loss 0.1453 (0.1495)	
training:	Epoch: [20][7/204]	Loss 0.1321 (0.1470)	
training:	Epoch: [20][8/204]	Loss 0.1770 (0.1508)	
training:	Epoch: [20][9/204]	Loss 0.1403 (0.1496)	
training:	Epoch: [20][10/204]	Loss 0.2103 (0.1557)	
training:	Epoch: [20][11/204]	Loss 0.0706 (0.1479)	
training:	Epoch: [20][12/204]	Loss 0.1553 (0.1486)	
training:	Epoch: [20][13/204]	Loss 0.1137 (0.1459)	
training:	Epoch: [20][14/204]	Loss 0.1811 (0.1484)	
training:	Epoch: [20][15/204]	Loss 0.1233 (0.1467)	
training:	Epoch: [20][16/204]	Loss 0.1252 (0.1454)	
training:	Epoch: [20][17/204]	Loss 0.3436 (0.1570)	
training:	Epoch: [20][18/204]	Loss 0.1306 (0.1556)	
training:	Epoch: [20][19/204]	Loss 0.1261 (0.1540)	
training:	Epoch: [20][20/204]	Loss 0.0938 (0.1510)	
training:	Epoch: [20][21/204]	Loss 0.1614 (0.1515)	
training:	Epoch: [20][22/204]	Loss 0.2106 (0.1542)	
training:	Epoch: [20][23/204]	Loss 0.3440 (0.1624)	
training:	Epoch: [20][24/204]	Loss 0.2691 (0.1669)	
training:	Epoch: [20][25/204]	Loss 0.0773 (0.1633)	
training:	Epoch: [20][26/204]	Loss 0.1137 (0.1614)	
training:	Epoch: [20][27/204]	Loss 0.0686 (0.1580)	
training:	Epoch: [20][28/204]	Loss 0.2322 (0.1606)	
training:	Epoch: [20][29/204]	Loss 0.1267 (0.1594)	
training:	Epoch: [20][30/204]	Loss 0.2640 (0.1629)	
training:	Epoch: [20][31/204]	Loss 0.3074 (0.1676)	
training:	Epoch: [20][32/204]	Loss 0.1271 (0.1663)	
training:	Epoch: [20][33/204]	Loss 0.0787 (0.1637)	
training:	Epoch: [20][34/204]	Loss 0.2352 (0.1658)	
training:	Epoch: [20][35/204]	Loss 0.1492 (0.1653)	
training:	Epoch: [20][36/204]	Loss 0.0769 (0.1628)	
training:	Epoch: [20][37/204]	Loss 0.0423 (0.1596)	
training:	Epoch: [20][38/204]	Loss 0.2968 (0.1632)	
training:	Epoch: [20][39/204]	Loss 0.0815 (0.1611)	
training:	Epoch: [20][40/204]	Loss 0.3809 (0.1666)	
training:	Epoch: [20][41/204]	Loss 0.2621 (0.1689)	
training:	Epoch: [20][42/204]	Loss 0.2217 (0.1702)	
training:	Epoch: [20][43/204]	Loss 0.2943 (0.1731)	
training:	Epoch: [20][44/204]	Loss 0.1023 (0.1715)	
training:	Epoch: [20][45/204]	Loss 0.0792 (0.1694)	
training:	Epoch: [20][46/204]	Loss 0.1534 (0.1691)	
training:	Epoch: [20][47/204]	Loss 0.2751 (0.1713)	
training:	Epoch: [20][48/204]	Loss 0.1252 (0.1703)	
training:	Epoch: [20][49/204]	Loss 0.0958 (0.1688)	
training:	Epoch: [20][50/204]	Loss 0.1388 (0.1682)	
training:	Epoch: [20][51/204]	Loss 0.1794 (0.1684)	
training:	Epoch: [20][52/204]	Loss 0.3822 (0.1726)	
training:	Epoch: [20][53/204]	Loss 0.2238 (0.1735)	
training:	Epoch: [20][54/204]	Loss 0.0640 (0.1715)	
training:	Epoch: [20][55/204]	Loss 0.2764 (0.1734)	
training:	Epoch: [20][56/204]	Loss 0.0836 (0.1718)	
training:	Epoch: [20][57/204]	Loss 0.4200 (0.1762)	
training:	Epoch: [20][58/204]	Loss 0.1591 (0.1759)	
training:	Epoch: [20][59/204]	Loss 0.1094 (0.1747)	
training:	Epoch: [20][60/204]	Loss 0.2422 (0.1759)	
training:	Epoch: [20][61/204]	Loss 0.2337 (0.1768)	
training:	Epoch: [20][62/204]	Loss 0.1198 (0.1759)	
training:	Epoch: [20][63/204]	Loss 0.1886 (0.1761)	
training:	Epoch: [20][64/204]	Loss 0.0786 (0.1746)	
training:	Epoch: [20][65/204]	Loss 0.0806 (0.1731)	
training:	Epoch: [20][66/204]	Loss 0.3016 (0.1751)	
training:	Epoch: [20][67/204]	Loss 0.2300 (0.1759)	
training:	Epoch: [20][68/204]	Loss 0.0915 (0.1746)	
training:	Epoch: [20][69/204]	Loss 0.1321 (0.1740)	
training:	Epoch: [20][70/204]	Loss 0.1158 (0.1732)	
training:	Epoch: [20][71/204]	Loss 0.3147 (0.1752)	
training:	Epoch: [20][72/204]	Loss 0.2152 (0.1757)	
training:	Epoch: [20][73/204]	Loss 0.1534 (0.1754)	
training:	Epoch: [20][74/204]	Loss 0.2248 (0.1761)	
training:	Epoch: [20][75/204]	Loss 0.2172 (0.1767)	
training:	Epoch: [20][76/204]	Loss 0.0679 (0.1752)	
training:	Epoch: [20][77/204]	Loss 0.3054 (0.1769)	
training:	Epoch: [20][78/204]	Loss 0.2056 (0.1773)	
training:	Epoch: [20][79/204]	Loss 0.2994 (0.1788)	
training:	Epoch: [20][80/204]	Loss 0.0933 (0.1778)	
training:	Epoch: [20][81/204]	Loss 0.3653 (0.1801)	
training:	Epoch: [20][82/204]	Loss 0.0995 (0.1791)	
training:	Epoch: [20][83/204]	Loss 0.1662 (0.1789)	
training:	Epoch: [20][84/204]	Loss 0.0601 (0.1775)	
training:	Epoch: [20][85/204]	Loss 0.1836 (0.1776)	
training:	Epoch: [20][86/204]	Loss 0.2741 (0.1787)	
training:	Epoch: [20][87/204]	Loss 0.1871 (0.1788)	
training:	Epoch: [20][88/204]	Loss 0.1339 (0.1783)	
training:	Epoch: [20][89/204]	Loss 0.1289 (0.1777)	
training:	Epoch: [20][90/204]	Loss 0.3394 (0.1795)	
training:	Epoch: [20][91/204]	Loss 0.2387 (0.1802)	
training:	Epoch: [20][92/204]	Loss 0.1442 (0.1798)	
training:	Epoch: [20][93/204]	Loss 0.0848 (0.1788)	
training:	Epoch: [20][94/204]	Loss 0.0819 (0.1777)	
training:	Epoch: [20][95/204]	Loss 0.0646 (0.1766)	
training:	Epoch: [20][96/204]	Loss 0.2089 (0.1769)	
training:	Epoch: [20][97/204]	Loss 0.0924 (0.1760)	
training:	Epoch: [20][98/204]	Loss 0.0901 (0.1751)	
training:	Epoch: [20][99/204]	Loss 0.1204 (0.1746)	
training:	Epoch: [20][100/204]	Loss 0.2337 (0.1752)	
training:	Epoch: [20][101/204]	Loss 0.2641 (0.1761)	
training:	Epoch: [20][102/204]	Loss 0.1168 (0.1755)	
training:	Epoch: [20][103/204]	Loss 0.3219 (0.1769)	
training:	Epoch: [20][104/204]	Loss 0.1280 (0.1764)	
training:	Epoch: [20][105/204]	Loss 0.2439 (0.1771)	
training:	Epoch: [20][106/204]	Loss 0.1988 (0.1773)	
training:	Epoch: [20][107/204]	Loss 0.1451 (0.1770)	
training:	Epoch: [20][108/204]	Loss 0.0528 (0.1758)	
training:	Epoch: [20][109/204]	Loss 0.0716 (0.1749)	
training:	Epoch: [20][110/204]	Loss 0.2360 (0.1754)	
training:	Epoch: [20][111/204]	Loss 0.1953 (0.1756)	
training:	Epoch: [20][112/204]	Loss 0.0910 (0.1749)	
training:	Epoch: [20][113/204]	Loss 0.0678 (0.1739)	
training:	Epoch: [20][114/204]	Loss 0.2364 (0.1745)	
training:	Epoch: [20][115/204]	Loss 0.1560 (0.1743)	
training:	Epoch: [20][116/204]	Loss 0.1132 (0.1738)	
training:	Epoch: [20][117/204]	Loss 0.0490 (0.1727)	
training:	Epoch: [20][118/204]	Loss 0.1365 (0.1724)	
training:	Epoch: [20][119/204]	Loss 0.2261 (0.1728)	
training:	Epoch: [20][120/204]	Loss 0.2131 (0.1732)	
training:	Epoch: [20][121/204]	Loss 0.1832 (0.1733)	
training:	Epoch: [20][122/204]	Loss 0.1791 (0.1733)	
training:	Epoch: [20][123/204]	Loss 0.3036 (0.1744)	
training:	Epoch: [20][124/204]	Loss 0.0808 (0.1736)	
training:	Epoch: [20][125/204]	Loss 0.2226 (0.1740)	
training:	Epoch: [20][126/204]	Loss 0.1793 (0.1741)	
training:	Epoch: [20][127/204]	Loss 0.0711 (0.1732)	
training:	Epoch: [20][128/204]	Loss 0.2938 (0.1742)	
training:	Epoch: [20][129/204]	Loss 0.1941 (0.1743)	
training:	Epoch: [20][130/204]	Loss 0.0802 (0.1736)	
training:	Epoch: [20][131/204]	Loss 0.1744 (0.1736)	
training:	Epoch: [20][132/204]	Loss 0.4387 (0.1756)	
training:	Epoch: [20][133/204]	Loss 0.1229 (0.1752)	
training:	Epoch: [20][134/204]	Loss 0.0724 (0.1745)	
training:	Epoch: [20][135/204]	Loss 0.0381 (0.1735)	
training:	Epoch: [20][136/204]	Loss 0.0863 (0.1728)	
training:	Epoch: [20][137/204]	Loss 0.0875 (0.1722)	
training:	Epoch: [20][138/204]	Loss 0.1850 (0.1723)	
training:	Epoch: [20][139/204]	Loss 0.3128 (0.1733)	
training:	Epoch: [20][140/204]	Loss 0.1345 (0.1730)	
training:	Epoch: [20][141/204]	Loss 0.2734 (0.1737)	
training:	Epoch: [20][142/204]	Loss 0.1292 (0.1734)	
training:	Epoch: [20][143/204]	Loss 0.0871 (0.1728)	
training:	Epoch: [20][144/204]	Loss 0.1746 (0.1728)	
training:	Epoch: [20][145/204]	Loss 0.0917 (0.1723)	
training:	Epoch: [20][146/204]	Loss 0.1047 (0.1718)	
training:	Epoch: [20][147/204]	Loss 0.1786 (0.1718)	
training:	Epoch: [20][148/204]	Loss 0.1280 (0.1716)	
training:	Epoch: [20][149/204]	Loss 0.1583 (0.1715)	
training:	Epoch: [20][150/204]	Loss 0.1615 (0.1714)	
training:	Epoch: [20][151/204]	Loss 0.2319 (0.1718)	
training:	Epoch: [20][152/204]	Loss 0.1616 (0.1717)	
training:	Epoch: [20][153/204]	Loss 0.1028 (0.1713)	
training:	Epoch: [20][154/204]	Loss 0.2333 (0.1717)	
training:	Epoch: [20][155/204]	Loss 0.3519 (0.1728)	
training:	Epoch: [20][156/204]	Loss 0.1714 (0.1728)	
training:	Epoch: [20][157/204]	Loss 0.1623 (0.1728)	
training:	Epoch: [20][158/204]	Loss 0.3557 (0.1739)	
training:	Epoch: [20][159/204]	Loss 0.2756 (0.1746)	
training:	Epoch: [20][160/204]	Loss 0.1250 (0.1743)	
training:	Epoch: [20][161/204]	Loss 0.1780 (0.1743)	
training:	Epoch: [20][162/204]	Loss 0.2332 (0.1746)	
training:	Epoch: [20][163/204]	Loss 0.3425 (0.1757)	
training:	Epoch: [20][164/204]	Loss 0.3510 (0.1767)	
training:	Epoch: [20][165/204]	Loss 0.2238 (0.1770)	
training:	Epoch: [20][166/204]	Loss 0.3031 (0.1778)	
training:	Epoch: [20][167/204]	Loss 0.1168 (0.1774)	
training:	Epoch: [20][168/204]	Loss 0.1443 (0.1772)	
training:	Epoch: [20][169/204]	Loss 0.2429 (0.1776)	
training:	Epoch: [20][170/204]	Loss 0.2965 (0.1783)	
training:	Epoch: [20][171/204]	Loss 0.1190 (0.1780)	
training:	Epoch: [20][172/204]	Loss 0.2534 (0.1784)	
training:	Epoch: [20][173/204]	Loss 0.1004 (0.1780)	
training:	Epoch: [20][174/204]	Loss 0.2548 (0.1784)	
training:	Epoch: [20][175/204]	Loss 0.0844 (0.1779)	
training:	Epoch: [20][176/204]	Loss 0.2687 (0.1784)	
training:	Epoch: [20][177/204]	Loss 0.2525 (0.1788)	
training:	Epoch: [20][178/204]	Loss 0.0774 (0.1782)	
training:	Epoch: [20][179/204]	Loss 0.2347 (0.1785)	
training:	Epoch: [20][180/204]	Loss 0.1765 (0.1785)	
training:	Epoch: [20][181/204]	Loss 0.1386 (0.1783)	
training:	Epoch: [20][182/204]	Loss 0.3534 (0.1793)	
training:	Epoch: [20][183/204]	Loss 0.1955 (0.1794)	
training:	Epoch: [20][184/204]	Loss 0.0597 (0.1787)	
training:	Epoch: [20][185/204]	Loss 0.0755 (0.1781)	
training:	Epoch: [20][186/204]	Loss 0.2130 (0.1783)	
training:	Epoch: [20][187/204]	Loss 0.3084 (0.1790)	
training:	Epoch: [20][188/204]	Loss 0.0818 (0.1785)	
training:	Epoch: [20][189/204]	Loss 0.1719 (0.1785)	
training:	Epoch: [20][190/204]	Loss 0.1384 (0.1783)	
training:	Epoch: [20][191/204]	Loss 0.1300 (0.1780)	
training:	Epoch: [20][192/204]	Loss 0.2916 (0.1786)	
training:	Epoch: [20][193/204]	Loss 0.1250 (0.1783)	
training:	Epoch: [20][194/204]	Loss 0.1638 (0.1783)	
training:	Epoch: [20][195/204]	Loss 0.2994 (0.1789)	
training:	Epoch: [20][196/204]	Loss 0.0911 (0.1784)	
training:	Epoch: [20][197/204]	Loss 0.1365 (0.1782)	
training:	Epoch: [20][198/204]	Loss 0.2178 (0.1784)	
training:	Epoch: [20][199/204]	Loss 0.0731 (0.1779)	
training:	Epoch: [20][200/204]	Loss 0.1269 (0.1776)	
training:	Epoch: [20][201/204]	Loss 0.1981 (0.1777)	
training:	Epoch: [20][202/204]	Loss 0.0586 (0.1771)	
training:	Epoch: [20][203/204]	Loss 0.0563 (0.1765)	
training:	Epoch: [20][204/204]	Loss 0.1425 (0.1764)	
Training:	 Loss: 0.1761

Training:	 ACC: 0.9586 0.9593 0.9768 0.9404
Validation:	 ACC: 0.8143 0.8165 0.8628 0.7657
Validation:	 Best_BACC: 0.8326 0.8331 0.8424 0.8229
Validation:	 Loss: 0.5058
Pretraining:	Epoch 21/200
----------
training:	Epoch: [21][1/204]	Loss 0.1134 (0.1134)	
training:	Epoch: [21][2/204]	Loss 0.0334 (0.0734)	
training:	Epoch: [21][3/204]	Loss 0.2296 (0.1255)	
training:	Epoch: [21][4/204]	Loss 0.2722 (0.1622)	
training:	Epoch: [21][5/204]	Loss 0.2130 (0.1723)	
training:	Epoch: [21][6/204]	Loss 0.0843 (0.1577)	
training:	Epoch: [21][7/204]	Loss 0.1719 (0.1597)	
training:	Epoch: [21][8/204]	Loss 0.0746 (0.1491)	
training:	Epoch: [21][9/204]	Loss 0.1469 (0.1488)	
training:	Epoch: [21][10/204]	Loss 0.2079 (0.1547)	
training:	Epoch: [21][11/204]	Loss 0.4042 (0.1774)	
training:	Epoch: [21][12/204]	Loss 0.0421 (0.1661)	
training:	Epoch: [21][13/204]	Loss 0.2202 (0.1703)	
training:	Epoch: [21][14/204]	Loss 0.1295 (0.1674)	
training:	Epoch: [21][15/204]	Loss 0.2545 (0.1732)	
training:	Epoch: [21][16/204]	Loss 0.1098 (0.1692)	
training:	Epoch: [21][17/204]	Loss 0.1884 (0.1704)	
training:	Epoch: [21][18/204]	Loss 0.2371 (0.1741)	
training:	Epoch: [21][19/204]	Loss 0.1616 (0.1734)	
training:	Epoch: [21][20/204]	Loss 0.2476 (0.1771)	
training:	Epoch: [21][21/204]	Loss 0.2286 (0.1796)	
training:	Epoch: [21][22/204]	Loss 0.1247 (0.1771)	
training:	Epoch: [21][23/204]	Loss 0.2229 (0.1791)	
training:	Epoch: [21][24/204]	Loss 0.2642 (0.1826)	
training:	Epoch: [21][25/204]	Loss 0.0415 (0.1770)	
training:	Epoch: [21][26/204]	Loss 0.1845 (0.1773)	
training:	Epoch: [21][27/204]	Loss 0.1955 (0.1779)	
training:	Epoch: [21][28/204]	Loss 0.1524 (0.1770)	
training:	Epoch: [21][29/204]	Loss 0.1769 (0.1770)	
training:	Epoch: [21][30/204]	Loss 0.0521 (0.1729)	
training:	Epoch: [21][31/204]	Loss 0.1191 (0.1711)	
training:	Epoch: [21][32/204]	Loss 0.0491 (0.1673)	
training:	Epoch: [21][33/204]	Loss 0.0789 (0.1646)	
training:	Epoch: [21][34/204]	Loss 0.2396 (0.1668)	
training:	Epoch: [21][35/204]	Loss 0.1945 (0.1676)	
training:	Epoch: [21][36/204]	Loss 0.0928 (0.1655)	
training:	Epoch: [21][37/204]	Loss 0.1097 (0.1640)	
training:	Epoch: [21][38/204]	Loss 0.2566 (0.1665)	
training:	Epoch: [21][39/204]	Loss 0.1465 (0.1660)	
training:	Epoch: [21][40/204]	Loss 0.0724 (0.1636)	
training:	Epoch: [21][41/204]	Loss 0.1511 (0.1633)	
training:	Epoch: [21][42/204]	Loss 0.0855 (0.1615)	
training:	Epoch: [21][43/204]	Loss 0.1631 (0.1615)	
training:	Epoch: [21][44/204]	Loss 0.0464 (0.1589)	
training:	Epoch: [21][45/204]	Loss 0.0283 (0.1560)	
training:	Epoch: [21][46/204]	Loss 0.0551 (0.1538)	
training:	Epoch: [21][47/204]	Loss 0.0495 (0.1516)	
training:	Epoch: [21][48/204]	Loss 0.2118 (0.1528)	
training:	Epoch: [21][49/204]	Loss 0.0704 (0.1511)	
training:	Epoch: [21][50/204]	Loss 0.1988 (0.1521)	
training:	Epoch: [21][51/204]	Loss 0.1234 (0.1515)	
training:	Epoch: [21][52/204]	Loss 0.0742 (0.1500)	
training:	Epoch: [21][53/204]	Loss 0.1114 (0.1493)	
training:	Epoch: [21][54/204]	Loss 0.0910 (0.1482)	
training:	Epoch: [21][55/204]	Loss 0.1203 (0.1477)	
training:	Epoch: [21][56/204]	Loss 0.1745 (0.1482)	
training:	Epoch: [21][57/204]	Loss 0.2245 (0.1495)	
training:	Epoch: [21][58/204]	Loss 0.0766 (0.1483)	
training:	Epoch: [21][59/204]	Loss 0.2619 (0.1502)	
training:	Epoch: [21][60/204]	Loss 0.0706 (0.1489)	
training:	Epoch: [21][61/204]	Loss 0.3119 (0.1516)	
training:	Epoch: [21][62/204]	Loss 0.0535 (0.1500)	
training:	Epoch: [21][63/204]	Loss 0.2850 (0.1521)	
training:	Epoch: [21][64/204]	Loss 0.0636 (0.1507)	
training:	Epoch: [21][65/204]	Loss 0.1846 (0.1513)	
training:	Epoch: [21][66/204]	Loss 0.1650 (0.1515)	
training:	Epoch: [21][67/204]	Loss 0.3804 (0.1549)	
training:	Epoch: [21][68/204]	Loss 0.2978 (0.1570)	
training:	Epoch: [21][69/204]	Loss 0.1706 (0.1572)	
training:	Epoch: [21][70/204]	Loss 0.0695 (0.1559)	
training:	Epoch: [21][71/204]	Loss 0.2200 (0.1568)	
training:	Epoch: [21][72/204]	Loss 0.0317 (0.1551)	
training:	Epoch: [21][73/204]	Loss 0.1052 (0.1544)	
training:	Epoch: [21][74/204]	Loss 0.1993 (0.1550)	
training:	Epoch: [21][75/204]	Loss 0.1346 (0.1547)	
training:	Epoch: [21][76/204]	Loss 0.2978 (0.1566)	
training:	Epoch: [21][77/204]	Loss 0.0664 (0.1555)	
training:	Epoch: [21][78/204]	Loss 0.0691 (0.1543)	
training:	Epoch: [21][79/204]	Loss 0.2055 (0.1550)	
training:	Epoch: [21][80/204]	Loss 0.0577 (0.1538)	
training:	Epoch: [21][81/204]	Loss 0.1290 (0.1535)	
training:	Epoch: [21][82/204]	Loss 0.2879 (0.1551)	
training:	Epoch: [21][83/204]	Loss 0.0913 (0.1543)	
training:	Epoch: [21][84/204]	Loss 0.1152 (0.1539)	
training:	Epoch: [21][85/204]	Loss 0.2894 (0.1555)	
training:	Epoch: [21][86/204]	Loss 0.0508 (0.1543)	
training:	Epoch: [21][87/204]	Loss 0.0322 (0.1529)	
training:	Epoch: [21][88/204]	Loss 0.1171 (0.1524)	
training:	Epoch: [21][89/204]	Loss 0.2073 (0.1531)	
training:	Epoch: [21][90/204]	Loss 0.1600 (0.1531)	
training:	Epoch: [21][91/204]	Loss 0.0310 (0.1518)	
training:	Epoch: [21][92/204]	Loss 0.1648 (0.1519)	
training:	Epoch: [21][93/204]	Loss 0.1465 (0.1519)	
training:	Epoch: [21][94/204]	Loss 0.2320 (0.1527)	
training:	Epoch: [21][95/204]	Loss 0.2008 (0.1532)	
training:	Epoch: [21][96/204]	Loss 0.1083 (0.1528)	
training:	Epoch: [21][97/204]	Loss 0.1569 (0.1528)	
training:	Epoch: [21][98/204]	Loss 0.1014 (0.1523)	
training:	Epoch: [21][99/204]	Loss 0.1269 (0.1520)	
training:	Epoch: [21][100/204]	Loss 0.1884 (0.1524)	
training:	Epoch: [21][101/204]	Loss 0.1356 (0.1522)	
training:	Epoch: [21][102/204]	Loss 0.0694 (0.1514)	
training:	Epoch: [21][103/204]	Loss 0.1248 (0.1512)	
training:	Epoch: [21][104/204]	Loss 0.0417 (0.1501)	
training:	Epoch: [21][105/204]	Loss 0.3635 (0.1521)	
training:	Epoch: [21][106/204]	Loss 0.1742 (0.1523)	
training:	Epoch: [21][107/204]	Loss 0.0780 (0.1517)	
training:	Epoch: [21][108/204]	Loss 0.1865 (0.1520)	
training:	Epoch: [21][109/204]	Loss 0.1498 (0.1520)	
training:	Epoch: [21][110/204]	Loss 0.1070 (0.1515)	
training:	Epoch: [21][111/204]	Loss 0.0543 (0.1507)	
training:	Epoch: [21][112/204]	Loss 0.0504 (0.1498)	
training:	Epoch: [21][113/204]	Loss 0.0506 (0.1489)	
training:	Epoch: [21][114/204]	Loss 0.1239 (0.1487)	
training:	Epoch: [21][115/204]	Loss 0.2969 (0.1500)	
training:	Epoch: [21][116/204]	Loss 0.1264 (0.1498)	
training:	Epoch: [21][117/204]	Loss 0.1658 (0.1499)	
training:	Epoch: [21][118/204]	Loss 0.1695 (0.1501)	
training:	Epoch: [21][119/204]	Loss 0.1869 (0.1504)	
training:	Epoch: [21][120/204]	Loss 0.2508 (0.1512)	
training:	Epoch: [21][121/204]	Loss 0.2705 (0.1522)	
training:	Epoch: [21][122/204]	Loss 0.0420 (0.1513)	
training:	Epoch: [21][123/204]	Loss 0.1771 (0.1515)	
training:	Epoch: [21][124/204]	Loss 0.1751 (0.1517)	
training:	Epoch: [21][125/204]	Loss 0.1909 (0.1520)	
training:	Epoch: [21][126/204]	Loss 0.1641 (0.1521)	
training:	Epoch: [21][127/204]	Loss 0.1147 (0.1518)	
training:	Epoch: [21][128/204]	Loss 0.1785 (0.1520)	
training:	Epoch: [21][129/204]	Loss 0.1152 (0.1517)	
training:	Epoch: [21][130/204]	Loss 0.0411 (0.1509)	
training:	Epoch: [21][131/204]	Loss 0.0771 (0.1503)	
training:	Epoch: [21][132/204]	Loss 0.0534 (0.1496)	
training:	Epoch: [21][133/204]	Loss 0.0331 (0.1487)	
training:	Epoch: [21][134/204]	Loss 0.0836 (0.1482)	
training:	Epoch: [21][135/204]	Loss 0.3651 (0.1498)	
training:	Epoch: [21][136/204]	Loss 0.1317 (0.1497)	
training:	Epoch: [21][137/204]	Loss 0.1092 (0.1494)	
training:	Epoch: [21][138/204]	Loss 0.2071 (0.1498)	
training:	Epoch: [21][139/204]	Loss 0.0788 (0.1493)	
training:	Epoch: [21][140/204]	Loss 0.0342 (0.1485)	
training:	Epoch: [21][141/204]	Loss 0.2355 (0.1491)	
training:	Epoch: [21][142/204]	Loss 0.1542 (0.1491)	
training:	Epoch: [21][143/204]	Loss 0.3613 (0.1506)	
training:	Epoch: [21][144/204]	Loss 0.2073 (0.1510)	
training:	Epoch: [21][145/204]	Loss 0.0582 (0.1504)	
training:	Epoch: [21][146/204]	Loss 0.1819 (0.1506)	
training:	Epoch: [21][147/204]	Loss 0.0537 (0.1499)	
training:	Epoch: [21][148/204]	Loss 0.1687 (0.1501)	
training:	Epoch: [21][149/204]	Loss 0.1320 (0.1499)	
training:	Epoch: [21][150/204]	Loss 0.1602 (0.1500)	
training:	Epoch: [21][151/204]	Loss 0.0636 (0.1494)	
training:	Epoch: [21][152/204]	Loss 0.1735 (0.1496)	
training:	Epoch: [21][153/204]	Loss 0.1772 (0.1498)	
training:	Epoch: [21][154/204]	Loss 0.0731 (0.1493)	
training:	Epoch: [21][155/204]	Loss 0.2393 (0.1499)	
training:	Epoch: [21][156/204]	Loss 0.2865 (0.1507)	
training:	Epoch: [21][157/204]	Loss 0.1083 (0.1505)	
training:	Epoch: [21][158/204]	Loss 0.1347 (0.1504)	
training:	Epoch: [21][159/204]	Loss 0.1131 (0.1501)	
training:	Epoch: [21][160/204]	Loss 0.1346 (0.1500)	
training:	Epoch: [21][161/204]	Loss 0.0768 (0.1496)	
training:	Epoch: [21][162/204]	Loss 0.1527 (0.1496)	
training:	Epoch: [21][163/204]	Loss 0.0814 (0.1492)	
training:	Epoch: [21][164/204]	Loss 0.0973 (0.1489)	
training:	Epoch: [21][165/204]	Loss 0.2531 (0.1495)	
training:	Epoch: [21][166/204]	Loss 0.1503 (0.1495)	
training:	Epoch: [21][167/204]	Loss 0.0627 (0.1490)	
training:	Epoch: [21][168/204]	Loss 0.1998 (0.1493)	
training:	Epoch: [21][169/204]	Loss 0.0364 (0.1486)	
training:	Epoch: [21][170/204]	Loss 0.1632 (0.1487)	
training:	Epoch: [21][171/204]	Loss 0.2886 (0.1495)	
training:	Epoch: [21][172/204]	Loss 0.1568 (0.1496)	
training:	Epoch: [21][173/204]	Loss 0.1647 (0.1496)	
training:	Epoch: [21][174/204]	Loss 0.0986 (0.1494)	
training:	Epoch: [21][175/204]	Loss 0.0290 (0.1487)	
training:	Epoch: [21][176/204]	Loss 0.0612 (0.1482)	
training:	Epoch: [21][177/204]	Loss 0.3907 (0.1495)	
training:	Epoch: [21][178/204]	Loss 0.1638 (0.1496)	
training:	Epoch: [21][179/204]	Loss 0.1341 (0.1495)	
training:	Epoch: [21][180/204]	Loss 0.1857 (0.1497)	
training:	Epoch: [21][181/204]	Loss 0.2437 (0.1503)	
training:	Epoch: [21][182/204]	Loss 0.0563 (0.1497)	
training:	Epoch: [21][183/204]	Loss 0.0505 (0.1492)	
training:	Epoch: [21][184/204]	Loss 0.0457 (0.1486)	
training:	Epoch: [21][185/204]	Loss 0.2127 (0.1490)	
training:	Epoch: [21][186/204]	Loss 0.2126 (0.1493)	
training:	Epoch: [21][187/204]	Loss 0.1673 (0.1494)	
training:	Epoch: [21][188/204]	Loss 0.3045 (0.1502)	
training:	Epoch: [21][189/204]	Loss 0.1595 (0.1503)	
training:	Epoch: [21][190/204]	Loss 0.1245 (0.1502)	
training:	Epoch: [21][191/204]	Loss 0.2741 (0.1508)	
training:	Epoch: [21][192/204]	Loss 0.1625 (0.1509)	
training:	Epoch: [21][193/204]	Loss 0.2669 (0.1515)	
training:	Epoch: [21][194/204]	Loss 0.3608 (0.1525)	
training:	Epoch: [21][195/204]	Loss 0.1126 (0.1523)	
training:	Epoch: [21][196/204]	Loss 0.0453 (0.1518)	
training:	Epoch: [21][197/204]	Loss 0.0379 (0.1512)	
training:	Epoch: [21][198/204]	Loss 0.2671 (0.1518)	
training:	Epoch: [21][199/204]	Loss 0.2083 (0.1521)	
training:	Epoch: [21][200/204]	Loss 0.4682 (0.1537)	
training:	Epoch: [21][201/204]	Loss 0.0520 (0.1532)	
training:	Epoch: [21][202/204]	Loss 0.2409 (0.1536)	
training:	Epoch: [21][203/204]	Loss 0.1519 (0.1536)	
training:	Epoch: [21][204/204]	Loss 0.1395 (0.1535)	
Training:	 Loss: 0.1533

Training:	 ACC: 0.9748 0.9749 0.9785 0.9710
Validation:	 ACC: 0.8148 0.8159 0.8393 0.7904
Validation:	 Best_BACC: 0.8326 0.8331 0.8424 0.8229
Validation:	 Loss: 0.5248
Pretraining:	Epoch 22/200
----------
training:	Epoch: [22][1/204]	Loss 0.0818 (0.0818)	
training:	Epoch: [22][2/204]	Loss 0.1956 (0.1387)	
training:	Epoch: [22][3/204]	Loss 0.0469 (0.1081)	
training:	Epoch: [22][4/204]	Loss 0.2279 (0.1380)	
training:	Epoch: [22][5/204]	Loss 0.1750 (0.1454)	
training:	Epoch: [22][6/204]	Loss 0.2253 (0.1587)	
training:	Epoch: [22][7/204]	Loss 0.1467 (0.1570)	
training:	Epoch: [22][8/204]	Loss 0.2542 (0.1692)	
training:	Epoch: [22][9/204]	Loss 0.2175 (0.1746)	
training:	Epoch: [22][10/204]	Loss 0.1313 (0.1702)	
training:	Epoch: [22][11/204]	Loss 0.0887 (0.1628)	
training:	Epoch: [22][12/204]	Loss 0.0387 (0.1525)	
training:	Epoch: [22][13/204]	Loss 0.1507 (0.1523)	
training:	Epoch: [22][14/204]	Loss 0.0476 (0.1449)	
training:	Epoch: [22][15/204]	Loss 0.0868 (0.1410)	
training:	Epoch: [22][16/204]	Loss 0.0654 (0.1363)	
training:	Epoch: [22][17/204]	Loss 0.0611 (0.1318)	
training:	Epoch: [22][18/204]	Loss 0.1425 (0.1324)	
training:	Epoch: [22][19/204]	Loss 0.0423 (0.1277)	
training:	Epoch: [22][20/204]	Loss 0.0534 (0.1240)	
training:	Epoch: [22][21/204]	Loss 0.0948 (0.1226)	
training:	Epoch: [22][22/204]	Loss 0.1018 (0.1216)	
training:	Epoch: [22][23/204]	Loss 0.1289 (0.1220)	
training:	Epoch: [22][24/204]	Loss 0.2727 (0.1282)	
training:	Epoch: [22][25/204]	Loss 0.0657 (0.1257)	
training:	Epoch: [22][26/204]	Loss 0.1638 (0.1272)	
training:	Epoch: [22][27/204]	Loss 0.1344 (0.1275)	
training:	Epoch: [22][28/204]	Loss 0.3245 (0.1345)	
training:	Epoch: [22][29/204]	Loss 0.1389 (0.1346)	
training:	Epoch: [22][30/204]	Loss 0.2886 (0.1398)	
training:	Epoch: [22][31/204]	Loss 0.0809 (0.1379)	
training:	Epoch: [22][32/204]	Loss 0.1624 (0.1386)	
training:	Epoch: [22][33/204]	Loss 0.1637 (0.1394)	
training:	Epoch: [22][34/204]	Loss 0.1163 (0.1387)	
training:	Epoch: [22][35/204]	Loss 0.2530 (0.1420)	
training:	Epoch: [22][36/204]	Loss 0.0411 (0.1392)	
training:	Epoch: [22][37/204]	Loss 0.0495 (0.1368)	
training:	Epoch: [22][38/204]	Loss 0.1978 (0.1384)	
training:	Epoch: [22][39/204]	Loss 0.2051 (0.1401)	
training:	Epoch: [22][40/204]	Loss 0.0793 (0.1386)	
training:	Epoch: [22][41/204]	Loss 0.1809 (0.1396)	
training:	Epoch: [22][42/204]	Loss 0.0613 (0.1377)	
training:	Epoch: [22][43/204]	Loss 0.1734 (0.1386)	
training:	Epoch: [22][44/204]	Loss 0.2020 (0.1400)	
training:	Epoch: [22][45/204]	Loss 0.0729 (0.1385)	
training:	Epoch: [22][46/204]	Loss 0.1617 (0.1390)	
training:	Epoch: [22][47/204]	Loss 0.1611 (0.1395)	
training:	Epoch: [22][48/204]	Loss 0.0662 (0.1380)	
training:	Epoch: [22][49/204]	Loss 0.2561 (0.1404)	
training:	Epoch: [22][50/204]	Loss 0.1907 (0.1414)	
training:	Epoch: [22][51/204]	Loss 0.2516 (0.1435)	
training:	Epoch: [22][52/204]	Loss 0.0764 (0.1422)	
training:	Epoch: [22][53/204]	Loss 0.1195 (0.1418)	
training:	Epoch: [22][54/204]	Loss 0.0410 (0.1400)	
training:	Epoch: [22][55/204]	Loss 0.1186 (0.1396)	
training:	Epoch: [22][56/204]	Loss 0.0416 (0.1378)	
training:	Epoch: [22][57/204]	Loss 0.1023 (0.1372)	
training:	Epoch: [22][58/204]	Loss 0.1954 (0.1382)	
training:	Epoch: [22][59/204]	Loss 0.0873 (0.1373)	
training:	Epoch: [22][60/204]	Loss 0.1618 (0.1377)	
training:	Epoch: [22][61/204]	Loss 0.1121 (0.1373)	
training:	Epoch: [22][62/204]	Loss 0.2659 (0.1394)	
training:	Epoch: [22][63/204]	Loss 0.0601 (0.1381)	
training:	Epoch: [22][64/204]	Loss 0.0509 (0.1368)	
training:	Epoch: [22][65/204]	Loss 0.0345 (0.1352)	
training:	Epoch: [22][66/204]	Loss 0.1333 (0.1352)	
training:	Epoch: [22][67/204]	Loss 0.1845 (0.1359)	
training:	Epoch: [22][68/204]	Loss 0.1719 (0.1364)	
training:	Epoch: [22][69/204]	Loss 0.1315 (0.1364)	
training:	Epoch: [22][70/204]	Loss 0.1116 (0.1360)	
training:	Epoch: [22][71/204]	Loss 0.1318 (0.1360)	
training:	Epoch: [22][72/204]	Loss 0.0889 (0.1353)	
training:	Epoch: [22][73/204]	Loss 0.0765 (0.1345)	
training:	Epoch: [22][74/204]	Loss 0.1704 (0.1350)	
training:	Epoch: [22][75/204]	Loss 0.1010 (0.1345)	
training:	Epoch: [22][76/204]	Loss 0.1578 (0.1348)	
training:	Epoch: [22][77/204]	Loss 0.1277 (0.1347)	
training:	Epoch: [22][78/204]	Loss 0.1534 (0.1350)	
training:	Epoch: [22][79/204]	Loss 0.1199 (0.1348)	
training:	Epoch: [22][80/204]	Loss 0.1579 (0.1351)	
training:	Epoch: [22][81/204]	Loss 0.2632 (0.1367)	
training:	Epoch: [22][82/204]	Loss 0.2287 (0.1378)	
training:	Epoch: [22][83/204]	Loss 0.1212 (0.1376)	
training:	Epoch: [22][84/204]	Loss 0.0296 (0.1363)	
training:	Epoch: [22][85/204]	Loss 0.2190 (0.1373)	
training:	Epoch: [22][86/204]	Loss 0.1280 (0.1372)	
training:	Epoch: [22][87/204]	Loss 0.0653 (0.1363)	
training:	Epoch: [22][88/204]	Loss 0.1239 (0.1362)	
training:	Epoch: [22][89/204]	Loss 0.0681 (0.1354)	
training:	Epoch: [22][90/204]	Loss 0.1718 (0.1358)	
training:	Epoch: [22][91/204]	Loss 0.1461 (0.1359)	
training:	Epoch: [22][92/204]	Loss 0.0778 (0.1353)	
training:	Epoch: [22][93/204]	Loss 0.0751 (0.1347)	
training:	Epoch: [22][94/204]	Loss 0.1419 (0.1347)	
training:	Epoch: [22][95/204]	Loss 0.1262 (0.1347)	
training:	Epoch: [22][96/204]	Loss 0.1651 (0.1350)	
training:	Epoch: [22][97/204]	Loss 0.1111 (0.1347)	
training:	Epoch: [22][98/204]	Loss 0.2175 (0.1356)	
training:	Epoch: [22][99/204]	Loss 0.0613 (0.1348)	
training:	Epoch: [22][100/204]	Loss 0.1430 (0.1349)	
training:	Epoch: [22][101/204]	Loss 0.3052 (0.1366)	
training:	Epoch: [22][102/204]	Loss 0.0407 (0.1356)	
training:	Epoch: [22][103/204]	Loss 0.1384 (0.1357)	
training:	Epoch: [22][104/204]	Loss 0.0775 (0.1351)	
training:	Epoch: [22][105/204]	Loss 0.0528 (0.1343)	
training:	Epoch: [22][106/204]	Loss 0.0924 (0.1339)	
training:	Epoch: [22][107/204]	Loss 0.1432 (0.1340)	
training:	Epoch: [22][108/204]	Loss 0.1731 (0.1344)	
training:	Epoch: [22][109/204]	Loss 0.1596 (0.1346)	
training:	Epoch: [22][110/204]	Loss 0.1911 (0.1351)	
training:	Epoch: [22][111/204]	Loss 0.0231 (0.1341)	
training:	Epoch: [22][112/204]	Loss 0.0409 (0.1333)	
training:	Epoch: [22][113/204]	Loss 0.1522 (0.1335)	
training:	Epoch: [22][114/204]	Loss 0.0492 (0.1327)	
training:	Epoch: [22][115/204]	Loss 0.1768 (0.1331)	
training:	Epoch: [22][116/204]	Loss 0.0402 (0.1323)	
training:	Epoch: [22][117/204]	Loss 0.2506 (0.1333)	
training:	Epoch: [22][118/204]	Loss 0.0242 (0.1324)	
training:	Epoch: [22][119/204]	Loss 0.0755 (0.1319)	
training:	Epoch: [22][120/204]	Loss 0.0299 (0.1311)	
training:	Epoch: [22][121/204]	Loss 0.0632 (0.1305)	
training:	Epoch: [22][122/204]	Loss 0.0539 (0.1299)	
training:	Epoch: [22][123/204]	Loss 0.0389 (0.1291)	
training:	Epoch: [22][124/204]	Loss 0.2264 (0.1299)	
training:	Epoch: [22][125/204]	Loss 0.2734 (0.1311)	
training:	Epoch: [22][126/204]	Loss 0.0332 (0.1303)	
training:	Epoch: [22][127/204]	Loss 0.2581 (0.1313)	
training:	Epoch: [22][128/204]	Loss 0.0632 (0.1308)	
training:	Epoch: [22][129/204]	Loss 0.0648 (0.1302)	
training:	Epoch: [22][130/204]	Loss 0.2261 (0.1310)	
training:	Epoch: [22][131/204]	Loss 0.1600 (0.1312)	
training:	Epoch: [22][132/204]	Loss 0.0668 (0.1307)	
training:	Epoch: [22][133/204]	Loss 0.1284 (0.1307)	
training:	Epoch: [22][134/204]	Loss 0.1858 (0.1311)	
training:	Epoch: [22][135/204]	Loss 0.1240 (0.1311)	
training:	Epoch: [22][136/204]	Loss 0.0446 (0.1304)	
training:	Epoch: [22][137/204]	Loss 0.1667 (0.1307)	
training:	Epoch: [22][138/204]	Loss 0.2651 (0.1317)	
training:	Epoch: [22][139/204]	Loss 0.1611 (0.1319)	
training:	Epoch: [22][140/204]	Loss 0.2030 (0.1324)	
training:	Epoch: [22][141/204]	Loss 0.0330 (0.1317)	
training:	Epoch: [22][142/204]	Loss 0.0740 (0.1313)	
training:	Epoch: [22][143/204]	Loss 0.0920 (0.1310)	
training:	Epoch: [22][144/204]	Loss 0.2381 (0.1317)	
training:	Epoch: [22][145/204]	Loss 0.1038 (0.1315)	
training:	Epoch: [22][146/204]	Loss 0.0807 (0.1312)	
training:	Epoch: [22][147/204]	Loss 0.0839 (0.1309)	
training:	Epoch: [22][148/204]	Loss 0.0953 (0.1306)	
training:	Epoch: [22][149/204]	Loss 0.0441 (0.1301)	
training:	Epoch: [22][150/204]	Loss 0.1553 (0.1302)	
training:	Epoch: [22][151/204]	Loss 0.3249 (0.1315)	
training:	Epoch: [22][152/204]	Loss 0.1665 (0.1317)	
training:	Epoch: [22][153/204]	Loss 0.0491 (0.1312)	
training:	Epoch: [22][154/204]	Loss 0.1383 (0.1312)	
training:	Epoch: [22][155/204]	Loss 0.2430 (0.1320)	
training:	Epoch: [22][156/204]	Loss 0.0365 (0.1314)	
training:	Epoch: [22][157/204]	Loss 0.0906 (0.1311)	
training:	Epoch: [22][158/204]	Loss 0.1934 (0.1315)	
training:	Epoch: [22][159/204]	Loss 0.1001 (0.1313)	
training:	Epoch: [22][160/204]	Loss 0.1090 (0.1312)	
training:	Epoch: [22][161/204]	Loss 0.1163 (0.1311)	
training:	Epoch: [22][162/204]	Loss 0.2670 (0.1319)	
training:	Epoch: [22][163/204]	Loss 0.0802 (0.1316)	
training:	Epoch: [22][164/204]	Loss 0.2946 (0.1326)	
training:	Epoch: [22][165/204]	Loss 0.1543 (0.1327)	
training:	Epoch: [22][166/204]	Loss 0.3352 (0.1339)	
training:	Epoch: [22][167/204]	Loss 0.1786 (0.1342)	
training:	Epoch: [22][168/204]	Loss 0.2635 (0.1350)	
training:	Epoch: [22][169/204]	Loss 0.2641 (0.1357)	
training:	Epoch: [22][170/204]	Loss 0.1456 (0.1358)	
training:	Epoch: [22][171/204]	Loss 0.0400 (0.1352)	
training:	Epoch: [22][172/204]	Loss 0.1513 (0.1353)	
training:	Epoch: [22][173/204]	Loss 0.0393 (0.1348)	
training:	Epoch: [22][174/204]	Loss 0.1237 (0.1347)	
training:	Epoch: [22][175/204]	Loss 0.0529 (0.1342)	
training:	Epoch: [22][176/204]	Loss 0.2450 (0.1349)	
training:	Epoch: [22][177/204]	Loss 0.1798 (0.1351)	
training:	Epoch: [22][178/204]	Loss 0.0881 (0.1349)	
training:	Epoch: [22][179/204]	Loss 0.1185 (0.1348)	
training:	Epoch: [22][180/204]	Loss 0.1459 (0.1348)	
training:	Epoch: [22][181/204]	Loss 0.2296 (0.1353)	
training:	Epoch: [22][182/204]	Loss 0.0721 (0.1350)	
training:	Epoch: [22][183/204]	Loss 0.0538 (0.1346)	
training:	Epoch: [22][184/204]	Loss 0.0428 (0.1341)	
training:	Epoch: [22][185/204]	Loss 0.1511 (0.1342)	
training:	Epoch: [22][186/204]	Loss 0.1319 (0.1341)	
training:	Epoch: [22][187/204]	Loss 0.4703 (0.1359)	
training:	Epoch: [22][188/204]	Loss 0.1289 (0.1359)	
training:	Epoch: [22][189/204]	Loss 0.3714 (0.1371)	
training:	Epoch: [22][190/204]	Loss 0.1588 (0.1373)	
training:	Epoch: [22][191/204]	Loss 0.1151 (0.1371)	
training:	Epoch: [22][192/204]	Loss 0.1724 (0.1373)	
training:	Epoch: [22][193/204]	Loss 0.0750 (0.1370)	
training:	Epoch: [22][194/204]	Loss 0.1489 (0.1371)	
training:	Epoch: [22][195/204]	Loss 0.0742 (0.1367)	
training:	Epoch: [22][196/204]	Loss 0.1009 (0.1366)	
training:	Epoch: [22][197/204]	Loss 0.2218 (0.1370)	
training:	Epoch: [22][198/204]	Loss 0.1229 (0.1369)	
training:	Epoch: [22][199/204]	Loss 0.1828 (0.1372)	
training:	Epoch: [22][200/204]	Loss 0.1685 (0.1373)	
training:	Epoch: [22][201/204]	Loss 0.1019 (0.1371)	
training:	Epoch: [22][202/204]	Loss 0.2788 (0.1378)	
training:	Epoch: [22][203/204]	Loss 0.0676 (0.1375)	
training:	Epoch: [22][204/204]	Loss 0.0363 (0.1370)	
Training:	 Loss: 0.1368

Training:	 ACC: 0.9804 0.9804 0.9821 0.9786
Validation:	 ACC: 0.8151 0.8165 0.8465 0.7836
Validation:	 Best_BACC: 0.8326 0.8331 0.8424 0.8229
Validation:	 Loss: 0.5344
Pretraining:	Epoch 23/200
----------
training:	Epoch: [23][1/204]	Loss 0.2207 (0.2207)	
training:	Epoch: [23][2/204]	Loss 0.0784 (0.1495)	
training:	Epoch: [23][3/204]	Loss 0.1338 (0.1443)	
training:	Epoch: [23][4/204]	Loss 0.0406 (0.1184)	
training:	Epoch: [23][5/204]	Loss 0.0531 (0.1053)	
training:	Epoch: [23][6/204]	Loss 0.0470 (0.0956)	
training:	Epoch: [23][7/204]	Loss 0.1759 (0.1071)	
training:	Epoch: [23][8/204]	Loss 0.3895 (0.1424)	
training:	Epoch: [23][9/204]	Loss 0.0468 (0.1317)	
training:	Epoch: [23][10/204]	Loss 0.0919 (0.1278)	
training:	Epoch: [23][11/204]	Loss 0.1477 (0.1296)	
training:	Epoch: [23][12/204]	Loss 0.0424 (0.1223)	
training:	Epoch: [23][13/204]	Loss 0.0429 (0.1162)	
training:	Epoch: [23][14/204]	Loss 0.1611 (0.1194)	
training:	Epoch: [23][15/204]	Loss 0.3367 (0.1339)	
training:	Epoch: [23][16/204]	Loss 0.0773 (0.1304)	
training:	Epoch: [23][17/204]	Loss 0.0575 (0.1261)	
training:	Epoch: [23][18/204]	Loss 0.0577 (0.1223)	
training:	Epoch: [23][19/204]	Loss 0.0474 (0.1183)	
training:	Epoch: [23][20/204]	Loss 0.1466 (0.1197)	
training:	Epoch: [23][21/204]	Loss 0.0723 (0.1175)	
training:	Epoch: [23][22/204]	Loss 0.0927 (0.1164)	
training:	Epoch: [23][23/204]	Loss 0.1063 (0.1159)	
training:	Epoch: [23][24/204]	Loss 0.0334 (0.1125)	
training:	Epoch: [23][25/204]	Loss 0.0376 (0.1095)	
training:	Epoch: [23][26/204]	Loss 0.1502 (0.1111)	
training:	Epoch: [23][27/204]	Loss 0.1638 (0.1130)	
training:	Epoch: [23][28/204]	Loss 0.0365 (0.1103)	
training:	Epoch: [23][29/204]	Loss 0.1498 (0.1116)	
training:	Epoch: [23][30/204]	Loss 0.0595 (0.1099)	
training:	Epoch: [23][31/204]	Loss 0.1397 (0.1109)	
training:	Epoch: [23][32/204]	Loss 0.2385 (0.1148)	
training:	Epoch: [23][33/204]	Loss 0.0425 (0.1127)	
training:	Epoch: [23][34/204]	Loss 0.1791 (0.1146)	
training:	Epoch: [23][35/204]	Loss 0.0898 (0.1139)	
training:	Epoch: [23][36/204]	Loss 0.1838 (0.1158)	
training:	Epoch: [23][37/204]	Loss 0.2049 (0.1182)	
training:	Epoch: [23][38/204]	Loss 0.0519 (0.1165)	
training:	Epoch: [23][39/204]	Loss 0.0248 (0.1142)	
training:	Epoch: [23][40/204]	Loss 0.0727 (0.1131)	
training:	Epoch: [23][41/204]	Loss 0.0364 (0.1112)	
training:	Epoch: [23][42/204]	Loss 0.0494 (0.1098)	
training:	Epoch: [23][43/204]	Loss 0.0414 (0.1082)	
training:	Epoch: [23][44/204]	Loss 0.1365 (0.1088)	
training:	Epoch: [23][45/204]	Loss 0.0622 (0.1078)	
training:	Epoch: [23][46/204]	Loss 0.1414 (0.1085)	
training:	Epoch: [23][47/204]	Loss 0.0454 (0.1072)	
training:	Epoch: [23][48/204]	Loss 0.0893 (0.1068)	
training:	Epoch: [23][49/204]	Loss 0.1653 (0.1080)	
training:	Epoch: [23][50/204]	Loss 0.0641 (0.1071)	
training:	Epoch: [23][51/204]	Loss 0.1576 (0.1081)	
training:	Epoch: [23][52/204]	Loss 0.4591 (0.1149)	
training:	Epoch: [23][53/204]	Loss 0.1654 (0.1158)	
training:	Epoch: [23][54/204]	Loss 0.1399 (0.1163)	
training:	Epoch: [23][55/204]	Loss 0.2226 (0.1182)	
training:	Epoch: [23][56/204]	Loss 0.1426 (0.1186)	
training:	Epoch: [23][57/204]	Loss 0.0356 (0.1172)	
training:	Epoch: [23][58/204]	Loss 0.1554 (0.1178)	
training:	Epoch: [23][59/204]	Loss 0.1048 (0.1176)	
training:	Epoch: [23][60/204]	Loss 0.2189 (0.1193)	
training:	Epoch: [23][61/204]	Loss 0.1349 (0.1196)	
training:	Epoch: [23][62/204]	Loss 0.1465 (0.1200)	
training:	Epoch: [23][63/204]	Loss 0.2199 (0.1216)	
training:	Epoch: [23][64/204]	Loss 0.2326 (0.1233)	
training:	Epoch: [23][65/204]	Loss 0.1076 (0.1231)	
training:	Epoch: [23][66/204]	Loss 0.1089 (0.1229)	
training:	Epoch: [23][67/204]	Loss 0.0748 (0.1221)	
training:	Epoch: [23][68/204]	Loss 0.1372 (0.1224)	
training:	Epoch: [23][69/204]	Loss 0.0491 (0.1213)	
training:	Epoch: [23][70/204]	Loss 0.1032 (0.1210)	
training:	Epoch: [23][71/204]	Loss 0.1954 (0.1221)	
training:	Epoch: [23][72/204]	Loss 0.0431 (0.1210)	
training:	Epoch: [23][73/204]	Loss 0.2648 (0.1230)	
training:	Epoch: [23][74/204]	Loss 0.0742 (0.1223)	
training:	Epoch: [23][75/204]	Loss 0.0834 (0.1218)	
training:	Epoch: [23][76/204]	Loss 0.0382 (0.1207)	
training:	Epoch: [23][77/204]	Loss 0.0887 (0.1203)	
training:	Epoch: [23][78/204]	Loss 0.0915 (0.1199)	
training:	Epoch: [23][79/204]	Loss 0.1081 (0.1197)	
training:	Epoch: [23][80/204]	Loss 0.2293 (0.1211)	
training:	Epoch: [23][81/204]	Loss 0.0352 (0.1201)	
training:	Epoch: [23][82/204]	Loss 0.0407 (0.1191)	
training:	Epoch: [23][83/204]	Loss 0.0948 (0.1188)	
training:	Epoch: [23][84/204]	Loss 0.2436 (0.1203)	
training:	Epoch: [23][85/204]	Loss 0.2171 (0.1214)	
training:	Epoch: [23][86/204]	Loss 0.2890 (0.1234)	
training:	Epoch: [23][87/204]	Loss 0.0332 (0.1223)	
training:	Epoch: [23][88/204]	Loss 0.0951 (0.1220)	
training:	Epoch: [23][89/204]	Loss 0.1147 (0.1219)	
training:	Epoch: [23][90/204]	Loss 0.0365 (0.1210)	
training:	Epoch: [23][91/204]	Loss 0.1337 (0.1211)	
training:	Epoch: [23][92/204]	Loss 0.0287 (0.1201)	
training:	Epoch: [23][93/204]	Loss 0.2263 (0.1213)	
training:	Epoch: [23][94/204]	Loss 0.1502 (0.1216)	
training:	Epoch: [23][95/204]	Loss 0.1779 (0.1222)	
training:	Epoch: [23][96/204]	Loss 0.0668 (0.1216)	
training:	Epoch: [23][97/204]	Loss 0.1150 (0.1215)	
training:	Epoch: [23][98/204]	Loss 0.1212 (0.1215)	
training:	Epoch: [23][99/204]	Loss 0.0697 (0.1210)	
training:	Epoch: [23][100/204]	Loss 0.0709 (0.1205)	
training:	Epoch: [23][101/204]	Loss 0.1728 (0.1210)	
training:	Epoch: [23][102/204]	Loss 0.0304 (0.1201)	
training:	Epoch: [23][103/204]	Loss 0.0520 (0.1195)	
training:	Epoch: [23][104/204]	Loss 0.0793 (0.1191)	
training:	Epoch: [23][105/204]	Loss 0.0307 (0.1182)	
training:	Epoch: [23][106/204]	Loss 0.1582 (0.1186)	
training:	Epoch: [23][107/204]	Loss 0.0456 (0.1179)	
training:	Epoch: [23][108/204]	Loss 0.2255 (0.1189)	
training:	Epoch: [23][109/204]	Loss 0.1820 (0.1195)	
training:	Epoch: [23][110/204]	Loss 0.0204 (0.1186)	
training:	Epoch: [23][111/204]	Loss 0.0780 (0.1182)	
training:	Epoch: [23][112/204]	Loss 0.0293 (0.1174)	
training:	Epoch: [23][113/204]	Loss 0.0972 (0.1173)	
training:	Epoch: [23][114/204]	Loss 0.0423 (0.1166)	
training:	Epoch: [23][115/204]	Loss 0.0805 (0.1163)	
training:	Epoch: [23][116/204]	Loss 0.1111 (0.1163)	
training:	Epoch: [23][117/204]	Loss 0.0333 (0.1155)	
training:	Epoch: [23][118/204]	Loss 0.1368 (0.1157)	
training:	Epoch: [23][119/204]	Loss 0.0497 (0.1152)	
training:	Epoch: [23][120/204]	Loss 0.0260 (0.1144)	
training:	Epoch: [23][121/204]	Loss 0.1045 (0.1143)	
training:	Epoch: [23][122/204]	Loss 0.1516 (0.1146)	
training:	Epoch: [23][123/204]	Loss 0.1317 (0.1148)	
training:	Epoch: [23][124/204]	Loss 0.0403 (0.1142)	
training:	Epoch: [23][125/204]	Loss 0.1498 (0.1145)	
training:	Epoch: [23][126/204]	Loss 0.0943 (0.1143)	
training:	Epoch: [23][127/204]	Loss 0.0573 (0.1139)	
training:	Epoch: [23][128/204]	Loss 0.1496 (0.1141)	
training:	Epoch: [23][129/204]	Loss 0.0189 (0.1134)	
training:	Epoch: [23][130/204]	Loss 0.0516 (0.1129)	
training:	Epoch: [23][131/204]	Loss 0.5142 (0.1160)	
training:	Epoch: [23][132/204]	Loss 0.1985 (0.1166)	
training:	Epoch: [23][133/204]	Loss 0.1644 (0.1170)	
training:	Epoch: [23][134/204]	Loss 0.1842 (0.1175)	
training:	Epoch: [23][135/204]	Loss 0.1453 (0.1177)	
training:	Epoch: [23][136/204]	Loss 0.1506 (0.1179)	
training:	Epoch: [23][137/204]	Loss 0.0484 (0.1174)	
training:	Epoch: [23][138/204]	Loss 0.0729 (0.1171)	
training:	Epoch: [23][139/204]	Loss 0.0283 (0.1165)	
training:	Epoch: [23][140/204]	Loss 0.0594 (0.1160)	
training:	Epoch: [23][141/204]	Loss 0.1764 (0.1165)	
training:	Epoch: [23][142/204]	Loss 0.0364 (0.1159)	
training:	Epoch: [23][143/204]	Loss 0.2156 (0.1166)	
training:	Epoch: [23][144/204]	Loss 0.2348 (0.1174)	
training:	Epoch: [23][145/204]	Loss 0.0382 (0.1169)	
training:	Epoch: [23][146/204]	Loss 0.1251 (0.1169)	
training:	Epoch: [23][147/204]	Loss 0.0980 (0.1168)	
training:	Epoch: [23][148/204]	Loss 0.0798 (0.1166)	
training:	Epoch: [23][149/204]	Loss 0.0958 (0.1164)	
training:	Epoch: [23][150/204]	Loss 0.0798 (0.1162)	
training:	Epoch: [23][151/204]	Loss 0.3556 (0.1178)	
training:	Epoch: [23][152/204]	Loss 0.0785 (0.1175)	
training:	Epoch: [23][153/204]	Loss 0.1882 (0.1180)	
training:	Epoch: [23][154/204]	Loss 0.1491 (0.1182)	
training:	Epoch: [23][155/204]	Loss 0.1958 (0.1187)	
training:	Epoch: [23][156/204]	Loss 0.0632 (0.1183)	
training:	Epoch: [23][157/204]	Loss 0.0570 (0.1179)	
training:	Epoch: [23][158/204]	Loss 0.1799 (0.1183)	
training:	Epoch: [23][159/204]	Loss 0.0383 (0.1178)	
training:	Epoch: [23][160/204]	Loss 0.1821 (0.1182)	
training:	Epoch: [23][161/204]	Loss 0.1001 (0.1181)	
training:	Epoch: [23][162/204]	Loss 0.1943 (0.1186)	
training:	Epoch: [23][163/204]	Loss 0.1040 (0.1185)	
training:	Epoch: [23][164/204]	Loss 0.0863 (0.1183)	
training:	Epoch: [23][165/204]	Loss 0.1411 (0.1184)	
training:	Epoch: [23][166/204]	Loss 0.0597 (0.1181)	
training:	Epoch: [23][167/204]	Loss 0.1026 (0.1180)	
training:	Epoch: [23][168/204]	Loss 0.0562 (0.1176)	
training:	Epoch: [23][169/204]	Loss 0.1445 (0.1178)	
training:	Epoch: [23][170/204]	Loss 0.0410 (0.1173)	
training:	Epoch: [23][171/204]	Loss 0.1098 (0.1173)	
training:	Epoch: [23][172/204]	Loss 0.0760 (0.1170)	
training:	Epoch: [23][173/204]	Loss 0.0412 (0.1166)	
training:	Epoch: [23][174/204]	Loss 0.0332 (0.1161)	
training:	Epoch: [23][175/204]	Loss 0.0275 (0.1156)	
training:	Epoch: [23][176/204]	Loss 0.0702 (0.1154)	
training:	Epoch: [23][177/204]	Loss 0.1590 (0.1156)	
training:	Epoch: [23][178/204]	Loss 0.0769 (0.1154)	
training:	Epoch: [23][179/204]	Loss 0.0282 (0.1149)	
training:	Epoch: [23][180/204]	Loss 0.1199 (0.1149)	
training:	Epoch: [23][181/204]	Loss 0.0433 (0.1145)	
training:	Epoch: [23][182/204]	Loss 0.0662 (0.1143)	
training:	Epoch: [23][183/204]	Loss 0.0570 (0.1139)	
training:	Epoch: [23][184/204]	Loss 0.0458 (0.1136)	
training:	Epoch: [23][185/204]	Loss 0.1195 (0.1136)	
training:	Epoch: [23][186/204]	Loss 0.0989 (0.1135)	
training:	Epoch: [23][187/204]	Loss 0.0618 (0.1133)	
training:	Epoch: [23][188/204]	Loss 0.0603 (0.1130)	
training:	Epoch: [23][189/204]	Loss 0.3374 (0.1142)	
training:	Epoch: [23][190/204]	Loss 0.1293 (0.1142)	
training:	Epoch: [23][191/204]	Loss 0.1722 (0.1145)	
training:	Epoch: [23][192/204]	Loss 0.0893 (0.1144)	
training:	Epoch: [23][193/204]	Loss 0.1532 (0.1146)	
training:	Epoch: [23][194/204]	Loss 0.2986 (0.1156)	
training:	Epoch: [23][195/204]	Loss 0.0530 (0.1152)	
training:	Epoch: [23][196/204]	Loss 0.1420 (0.1154)	
training:	Epoch: [23][197/204]	Loss 0.0605 (0.1151)	
training:	Epoch: [23][198/204]	Loss 0.0788 (0.1149)	
training:	Epoch: [23][199/204]	Loss 0.0516 (0.1146)	
training:	Epoch: [23][200/204]	Loss 0.1771 (0.1149)	
training:	Epoch: [23][201/204]	Loss 0.2459 (0.1156)	
training:	Epoch: [23][202/204]	Loss 0.0630 (0.1153)	
training:	Epoch: [23][203/204]	Loss 0.0870 (0.1152)	
training:	Epoch: [23][204/204]	Loss 0.1660 (0.1154)	
Training:	 Loss: 0.1152

Training:	 ACC: 0.9840 0.9841 0.9853 0.9828
Validation:	 ACC: 0.8133 0.8143 0.8352 0.7915
Validation:	 Best_BACC: 0.8326 0.8331 0.8424 0.8229
Validation:	 Loss: 0.5636
Pretraining:	Epoch 24/200
----------
training:	Epoch: [24][1/204]	Loss 0.0302 (0.0302)	
training:	Epoch: [24][2/204]	Loss 0.0380 (0.0341)	
training:	Epoch: [24][3/204]	Loss 0.1580 (0.0754)	
training:	Epoch: [24][4/204]	Loss 0.2800 (0.1265)	
training:	Epoch: [24][5/204]	Loss 0.0435 (0.1099)	
training:	Epoch: [24][6/204]	Loss 0.0283 (0.0963)	
training:	Epoch: [24][7/204]	Loss 0.0685 (0.0924)	
training:	Epoch: [24][8/204]	Loss 0.0504 (0.0871)	
training:	Epoch: [24][9/204]	Loss 0.0375 (0.0816)	
training:	Epoch: [24][10/204]	Loss 0.1617 (0.0896)	
training:	Epoch: [24][11/204]	Loss 0.0541 (0.0864)	
training:	Epoch: [24][12/204]	Loss 0.0293 (0.0816)	
training:	Epoch: [24][13/204]	Loss 0.0260 (0.0774)	
training:	Epoch: [24][14/204]	Loss 0.1548 (0.0829)	
training:	Epoch: [24][15/204]	Loss 0.0336 (0.0796)	
training:	Epoch: [24][16/204]	Loss 0.0322 (0.0766)	
training:	Epoch: [24][17/204]	Loss 0.0212 (0.0734)	
training:	Epoch: [24][18/204]	Loss 0.0578 (0.0725)	
training:	Epoch: [24][19/204]	Loss 0.1409 (0.0761)	
training:	Epoch: [24][20/204]	Loss 0.0232 (0.0735)	
training:	Epoch: [24][21/204]	Loss 0.0729 (0.0734)	
training:	Epoch: [24][22/204]	Loss 0.0361 (0.0717)	
training:	Epoch: [24][23/204]	Loss 0.0274 (0.0698)	
training:	Epoch: [24][24/204]	Loss 0.0799 (0.0702)	
training:	Epoch: [24][25/204]	Loss 0.1026 (0.0715)	
training:	Epoch: [24][26/204]	Loss 0.0633 (0.0712)	
training:	Epoch: [24][27/204]	Loss 0.0646 (0.0710)	
training:	Epoch: [24][28/204]	Loss 0.0406 (0.0699)	
training:	Epoch: [24][29/204]	Loss 0.1326 (0.0720)	
training:	Epoch: [24][30/204]	Loss 0.1600 (0.0750)	
training:	Epoch: [24][31/204]	Loss 0.0981 (0.0757)	
training:	Epoch: [24][32/204]	Loss 0.0628 (0.0753)	
training:	Epoch: [24][33/204]	Loss 0.1186 (0.0766)	
training:	Epoch: [24][34/204]	Loss 0.0378 (0.0755)	
training:	Epoch: [24][35/204]	Loss 0.0329 (0.0743)	
training:	Epoch: [24][36/204]	Loss 0.0461 (0.0735)	
training:	Epoch: [24][37/204]	Loss 0.0244 (0.0722)	
training:	Epoch: [24][38/204]	Loss 0.0441 (0.0714)	
training:	Epoch: [24][39/204]	Loss 0.1091 (0.0724)	
training:	Epoch: [24][40/204]	Loss 0.0495 (0.0718)	
training:	Epoch: [24][41/204]	Loss 0.1844 (0.0746)	
training:	Epoch: [24][42/204]	Loss 0.2941 (0.0798)	
training:	Epoch: [24][43/204]	Loss 0.0466 (0.0790)	
training:	Epoch: [24][44/204]	Loss 0.0320 (0.0780)	
training:	Epoch: [24][45/204]	Loss 0.1632 (0.0798)	
training:	Epoch: [24][46/204]	Loss 0.2647 (0.0839)	
training:	Epoch: [24][47/204]	Loss 0.1488 (0.0852)	
training:	Epoch: [24][48/204]	Loss 0.0385 (0.0843)	
training:	Epoch: [24][49/204]	Loss 0.0887 (0.0844)	
training:	Epoch: [24][50/204]	Loss 0.3015 (0.0887)	
training:	Epoch: [24][51/204]	Loss 0.1043 (0.0890)	
training:	Epoch: [24][52/204]	Loss 0.1317 (0.0898)	
training:	Epoch: [24][53/204]	Loss 0.2168 (0.0922)	
training:	Epoch: [24][54/204]	Loss 0.0306 (0.0911)	
training:	Epoch: [24][55/204]	Loss 0.2116 (0.0933)	
training:	Epoch: [24][56/204]	Loss 0.1414 (0.0941)	
training:	Epoch: [24][57/204]	Loss 0.1299 (0.0948)	
training:	Epoch: [24][58/204]	Loss 0.1450 (0.0956)	
training:	Epoch: [24][59/204]	Loss 0.1101 (0.0959)	
training:	Epoch: [24][60/204]	Loss 0.1949 (0.0975)	
training:	Epoch: [24][61/204]	Loss 0.0308 (0.0964)	
training:	Epoch: [24][62/204]	Loss 0.2400 (0.0987)	
training:	Epoch: [24][63/204]	Loss 0.0920 (0.0986)	
training:	Epoch: [24][64/204]	Loss 0.0789 (0.0983)	
training:	Epoch: [24][65/204]	Loss 0.0612 (0.0978)	
training:	Epoch: [24][66/204]	Loss 0.0209 (0.0966)	
training:	Epoch: [24][67/204]	Loss 0.0220 (0.0955)	
training:	Epoch: [24][68/204]	Loss 0.0528 (0.0949)	
training:	Epoch: [24][69/204]	Loss 0.0497 (0.0942)	
training:	Epoch: [24][70/204]	Loss 0.0292 (0.0933)	
training:	Epoch: [24][71/204]	Loss 0.0893 (0.0932)	
training:	Epoch: [24][72/204]	Loss 0.1864 (0.0945)	
training:	Epoch: [24][73/204]	Loss 0.0462 (0.0938)	
training:	Epoch: [24][74/204]	Loss 0.0600 (0.0934)	
training:	Epoch: [24][75/204]	Loss 0.1413 (0.0940)	
training:	Epoch: [24][76/204]	Loss 0.0637 (0.0936)	
training:	Epoch: [24][77/204]	Loss 0.0559 (0.0931)	
training:	Epoch: [24][78/204]	Loss 0.2002 (0.0945)	
training:	Epoch: [24][79/204]	Loss 0.2086 (0.0960)	
training:	Epoch: [24][80/204]	Loss 0.1544 (0.0967)	
training:	Epoch: [24][81/204]	Loss 0.2245 (0.0983)	
training:	Epoch: [24][82/204]	Loss 0.0330 (0.0975)	
training:	Epoch: [24][83/204]	Loss 0.3867 (0.1010)	
training:	Epoch: [24][84/204]	Loss 0.0562 (0.1004)	
training:	Epoch: [24][85/204]	Loss 0.1391 (0.1009)	
training:	Epoch: [24][86/204]	Loss 0.1751 (0.1017)	
training:	Epoch: [24][87/204]	Loss 0.1777 (0.1026)	
training:	Epoch: [24][88/204]	Loss 0.1297 (0.1029)	
training:	Epoch: [24][89/204]	Loss 0.2406 (0.1045)	
training:	Epoch: [24][90/204]	Loss 0.1366 (0.1048)	
training:	Epoch: [24][91/204]	Loss 0.1512 (0.1053)	
training:	Epoch: [24][92/204]	Loss 0.1048 (0.1053)	
training:	Epoch: [24][93/204]	Loss 0.0756 (0.1050)	
training:	Epoch: [24][94/204]	Loss 0.0225 (0.1041)	
training:	Epoch: [24][95/204]	Loss 0.0497 (0.1036)	
training:	Epoch: [24][96/204]	Loss 0.0480 (0.1030)	
training:	Epoch: [24][97/204]	Loss 0.2653 (0.1047)	
training:	Epoch: [24][98/204]	Loss 0.0403 (0.1040)	
training:	Epoch: [24][99/204]	Loss 0.0278 (0.1032)	
training:	Epoch: [24][100/204]	Loss 0.0289 (0.1025)	
training:	Epoch: [24][101/204]	Loss 0.0306 (0.1018)	
training:	Epoch: [24][102/204]	Loss 0.0250 (0.1010)	
training:	Epoch: [24][103/204]	Loss 0.0795 (0.1008)	
training:	Epoch: [24][104/204]	Loss 0.0680 (0.1005)	
training:	Epoch: [24][105/204]	Loss 0.0631 (0.1001)	
training:	Epoch: [24][106/204]	Loss 0.0339 (0.0995)	
training:	Epoch: [24][107/204]	Loss 0.1940 (0.1004)	
training:	Epoch: [24][108/204]	Loss 0.1118 (0.1005)	
training:	Epoch: [24][109/204]	Loss 0.1410 (0.1009)	
training:	Epoch: [24][110/204]	Loss 0.1672 (0.1015)	
training:	Epoch: [24][111/204]	Loss 0.1441 (0.1019)	
training:	Epoch: [24][112/204]	Loss 0.1054 (0.1019)	
training:	Epoch: [24][113/204]	Loss 0.0252 (0.1012)	
training:	Epoch: [24][114/204]	Loss 0.0362 (0.1006)	
training:	Epoch: [24][115/204]	Loss 0.0479 (0.1002)	
training:	Epoch: [24][116/204]	Loss 0.2086 (0.1011)	
training:	Epoch: [24][117/204]	Loss 0.1356 (0.1014)	
training:	Epoch: [24][118/204]	Loss 0.0434 (0.1009)	
training:	Epoch: [24][119/204]	Loss 0.1349 (0.1012)	
training:	Epoch: [24][120/204]	Loss 0.1570 (0.1017)	
training:	Epoch: [24][121/204]	Loss 0.0357 (0.1011)	
training:	Epoch: [24][122/204]	Loss 0.1246 (0.1013)	
training:	Epoch: [24][123/204]	Loss 0.0393 (0.1008)	
training:	Epoch: [24][124/204]	Loss 0.0239 (0.1002)	
training:	Epoch: [24][125/204]	Loss 0.0317 (0.0996)	
training:	Epoch: [24][126/204]	Loss 0.1034 (0.0997)	
training:	Epoch: [24][127/204]	Loss 0.0791 (0.0995)	
training:	Epoch: [24][128/204]	Loss 0.1717 (0.1001)	
training:	Epoch: [24][129/204]	Loss 0.3071 (0.1017)	
training:	Epoch: [24][130/204]	Loss 0.1810 (0.1023)	
training:	Epoch: [24][131/204]	Loss 0.1327 (0.1025)	
training:	Epoch: [24][132/204]	Loss 0.0592 (0.1022)	
training:	Epoch: [24][133/204]	Loss 0.0766 (0.1020)	
training:	Epoch: [24][134/204]	Loss 0.1143 (0.1021)	
training:	Epoch: [24][135/204]	Loss 0.0312 (0.1016)	
training:	Epoch: [24][136/204]	Loss 0.1992 (0.1023)	
training:	Epoch: [24][137/204]	Loss 0.0333 (0.1018)	
training:	Epoch: [24][138/204]	Loss 0.0345 (0.1013)	
training:	Epoch: [24][139/204]	Loss 0.0302 (0.1008)	
training:	Epoch: [24][140/204]	Loss 0.2197 (0.1016)	
training:	Epoch: [24][141/204]	Loss 0.0357 (0.1012)	
training:	Epoch: [24][142/204]	Loss 0.0779 (0.1010)	
training:	Epoch: [24][143/204]	Loss 0.0345 (0.1005)	
training:	Epoch: [24][144/204]	Loss 0.0346 (0.1001)	
training:	Epoch: [24][145/204]	Loss 0.0473 (0.0997)	
training:	Epoch: [24][146/204]	Loss 0.1413 (0.1000)	
training:	Epoch: [24][147/204]	Loss 0.0249 (0.0995)	
training:	Epoch: [24][148/204]	Loss 0.0988 (0.0995)	
training:	Epoch: [24][149/204]	Loss 0.0462 (0.0991)	
training:	Epoch: [24][150/204]	Loss 0.0338 (0.0987)	
training:	Epoch: [24][151/204]	Loss 0.1952 (0.0993)	
training:	Epoch: [24][152/204]	Loss 0.0261 (0.0989)	
training:	Epoch: [24][153/204]	Loss 0.0302 (0.0984)	
training:	Epoch: [24][154/204]	Loss 0.1532 (0.0988)	
training:	Epoch: [24][155/204]	Loss 0.0355 (0.0984)	
training:	Epoch: [24][156/204]	Loss 0.2112 (0.0991)	
training:	Epoch: [24][157/204]	Loss 0.0511 (0.0988)	
training:	Epoch: [24][158/204]	Loss 0.4210 (0.1008)	
training:	Epoch: [24][159/204]	Loss 0.0490 (0.1005)	
training:	Epoch: [24][160/204]	Loss 0.1535 (0.1008)	
training:	Epoch: [24][161/204]	Loss 0.1124 (0.1009)	
training:	Epoch: [24][162/204]	Loss 0.1585 (0.1012)	
training:	Epoch: [24][163/204]	Loss 0.1742 (0.1017)	
training:	Epoch: [24][164/204]	Loss 0.1316 (0.1019)	
training:	Epoch: [24][165/204]	Loss 0.1036 (0.1019)	
training:	Epoch: [24][166/204]	Loss 0.0986 (0.1019)	
training:	Epoch: [24][167/204]	Loss 0.1241 (0.1020)	
training:	Epoch: [24][168/204]	Loss 0.0993 (0.1020)	
training:	Epoch: [24][169/204]	Loss 0.1892 (0.1025)	
training:	Epoch: [24][170/204]	Loss 0.0298 (0.1021)	
training:	Epoch: [24][171/204]	Loss 0.1999 (0.1026)	
training:	Epoch: [24][172/204]	Loss 0.1892 (0.1031)	
training:	Epoch: [24][173/204]	Loss 0.0723 (0.1030)	
training:	Epoch: [24][174/204]	Loss 0.0696 (0.1028)	
training:	Epoch: [24][175/204]	Loss 0.1175 (0.1029)	
training:	Epoch: [24][176/204]	Loss 0.0627 (0.1026)	
training:	Epoch: [24][177/204]	Loss 0.0456 (0.1023)	
training:	Epoch: [24][178/204]	Loss 0.0921 (0.1022)	
training:	Epoch: [24][179/204]	Loss 0.1638 (0.1026)	
training:	Epoch: [24][180/204]	Loss 0.0783 (0.1025)	
training:	Epoch: [24][181/204]	Loss 0.2196 (0.1031)	
training:	Epoch: [24][182/204]	Loss 0.0735 (0.1029)	
training:	Epoch: [24][183/204]	Loss 0.1244 (0.1031)	
training:	Epoch: [24][184/204]	Loss 0.1808 (0.1035)	
training:	Epoch: [24][185/204]	Loss 0.0662 (0.1033)	
training:	Epoch: [24][186/204]	Loss 0.0264 (0.1029)	
training:	Epoch: [24][187/204]	Loss 0.0641 (0.1027)	
training:	Epoch: [24][188/204]	Loss 0.0971 (0.1026)	
training:	Epoch: [24][189/204]	Loss 0.1301 (0.1028)	
training:	Epoch: [24][190/204]	Loss 0.0770 (0.1026)	
training:	Epoch: [24][191/204]	Loss 0.1197 (0.1027)	
training:	Epoch: [24][192/204]	Loss 0.0332 (0.1024)	
training:	Epoch: [24][193/204]	Loss 0.0524 (0.1021)	
training:	Epoch: [24][194/204]	Loss 0.1941 (0.1026)	
training:	Epoch: [24][195/204]	Loss 0.0264 (0.1022)	
training:	Epoch: [24][196/204]	Loss 0.0219 (0.1018)	
training:	Epoch: [24][197/204]	Loss 0.0390 (0.1015)	
training:	Epoch: [24][198/204]	Loss 0.0184 (0.1010)	
training:	Epoch: [24][199/204]	Loss 0.0478 (0.1008)	
training:	Epoch: [24][200/204]	Loss 0.0617 (0.1006)	
training:	Epoch: [24][201/204]	Loss 0.0223 (0.1002)	
training:	Epoch: [24][202/204]	Loss 0.0496 (0.0999)	
training:	Epoch: [24][203/204]	Loss 0.0731 (0.0998)	
training:	Epoch: [24][204/204]	Loss 0.0283 (0.0995)	
Training:	 Loss: 0.0993

Training:	 ACC: 0.9869 0.9868 0.9862 0.9876
Validation:	 ACC: 0.8092 0.8095 0.8158 0.8027
Validation:	 Best_BACC: 0.8326 0.8331 0.8424 0.8229
Validation:	 Loss: 0.5924
Pretraining:	Epoch 25/200
----------
training:	Epoch: [25][1/204]	Loss 0.0332 (0.0332)	
training:	Epoch: [25][2/204]	Loss 0.2213 (0.1272)	
training:	Epoch: [25][3/204]	Loss 0.0352 (0.0966)	
training:	Epoch: [25][4/204]	Loss 0.1174 (0.1018)	
training:	Epoch: [25][5/204]	Loss 0.1095 (0.1033)	
training:	Epoch: [25][6/204]	Loss 0.1450 (0.1103)	
training:	Epoch: [25][7/204]	Loss 0.0368 (0.0998)	
training:	Epoch: [25][8/204]	Loss 0.0248 (0.0904)	
training:	Epoch: [25][9/204]	Loss 0.0662 (0.0877)	
training:	Epoch: [25][10/204]	Loss 0.0412 (0.0831)	
training:	Epoch: [25][11/204]	Loss 0.1574 (0.0898)	
training:	Epoch: [25][12/204]	Loss 0.0254 (0.0845)	
training:	Epoch: [25][13/204]	Loss 0.0268 (0.0800)	
training:	Epoch: [25][14/204]	Loss 0.0331 (0.0767)	
training:	Epoch: [25][15/204]	Loss 0.0249 (0.0732)	
training:	Epoch: [25][16/204]	Loss 0.1652 (0.0790)	
training:	Epoch: [25][17/204]	Loss 0.1483 (0.0830)	
training:	Epoch: [25][18/204]	Loss 0.1228 (0.0852)	
training:	Epoch: [25][19/204]	Loss 0.0242 (0.0820)	
training:	Epoch: [25][20/204]	Loss 0.0309 (0.0795)	
training:	Epoch: [25][21/204]	Loss 0.2631 (0.0882)	
training:	Epoch: [25][22/204]	Loss 0.0809 (0.0879)	
training:	Epoch: [25][23/204]	Loss 0.0209 (0.0850)	
training:	Epoch: [25][24/204]	Loss 0.0958 (0.0854)	
training:	Epoch: [25][25/204]	Loss 0.0484 (0.0839)	
training:	Epoch: [25][26/204]	Loss 0.0408 (0.0823)	
training:	Epoch: [25][27/204]	Loss 0.0915 (0.0826)	
training:	Epoch: [25][28/204]	Loss 0.0235 (0.0805)	
training:	Epoch: [25][29/204]	Loss 0.0363 (0.0790)	
training:	Epoch: [25][30/204]	Loss 0.0911 (0.0794)	
training:	Epoch: [25][31/204]	Loss 0.0218 (0.0775)	
training:	Epoch: [25][32/204]	Loss 0.0357 (0.0762)	
training:	Epoch: [25][33/204]	Loss 0.0366 (0.0750)	
training:	Epoch: [25][34/204]	Loss 0.0615 (0.0746)	
training:	Epoch: [25][35/204]	Loss 0.0320 (0.0734)	
training:	Epoch: [25][36/204]	Loss 0.0292 (0.0722)	
training:	Epoch: [25][37/204]	Loss 0.1307 (0.0738)	
training:	Epoch: [25][38/204]	Loss 0.1143 (0.0748)	
training:	Epoch: [25][39/204]	Loss 0.1694 (0.0773)	
training:	Epoch: [25][40/204]	Loss 0.0239 (0.0759)	
training:	Epoch: [25][41/204]	Loss 0.0487 (0.0753)	
training:	Epoch: [25][42/204]	Loss 0.0868 (0.0755)	
training:	Epoch: [25][43/204]	Loss 0.0645 (0.0753)	
training:	Epoch: [25][44/204]	Loss 0.0443 (0.0746)	
training:	Epoch: [25][45/204]	Loss 0.0536 (0.0741)	
training:	Epoch: [25][46/204]	Loss 0.1540 (0.0758)	
training:	Epoch: [25][47/204]	Loss 0.1460 (0.0773)	
training:	Epoch: [25][48/204]	Loss 0.1842 (0.0796)	
training:	Epoch: [25][49/204]	Loss 0.0179 (0.0783)	
training:	Epoch: [25][50/204]	Loss 0.1497 (0.0797)	
training:	Epoch: [25][51/204]	Loss 0.0164 (0.0785)	
training:	Epoch: [25][52/204]	Loss 0.0275 (0.0775)	
training:	Epoch: [25][53/204]	Loss 0.1840 (0.0795)	
training:	Epoch: [25][54/204]	Loss 0.0919 (0.0798)	
training:	Epoch: [25][55/204]	Loss 0.0415 (0.0791)	
training:	Epoch: [25][56/204]	Loss 0.0736 (0.0790)	
training:	Epoch: [25][57/204]	Loss 0.1358 (0.0800)	
training:	Epoch: [25][58/204]	Loss 0.1647 (0.0814)	
training:	Epoch: [25][59/204]	Loss 0.0333 (0.0806)	
training:	Epoch: [25][60/204]	Loss 0.0616 (0.0803)	
training:	Epoch: [25][61/204]	Loss 0.0891 (0.0804)	
training:	Epoch: [25][62/204]	Loss 0.0230 (0.0795)	
training:	Epoch: [25][63/204]	Loss 0.0311 (0.0787)	
training:	Epoch: [25][64/204]	Loss 0.0303 (0.0780)	
training:	Epoch: [25][65/204]	Loss 0.0249 (0.0772)	
training:	Epoch: [25][66/204]	Loss 0.0786 (0.0772)	
training:	Epoch: [25][67/204]	Loss 0.0323 (0.0765)	
training:	Epoch: [25][68/204]	Loss 0.0187 (0.0757)	
training:	Epoch: [25][69/204]	Loss 0.0199 (0.0749)	
training:	Epoch: [25][70/204]	Loss 0.0694 (0.0748)	
training:	Epoch: [25][71/204]	Loss 0.0348 (0.0742)	
training:	Epoch: [25][72/204]	Loss 0.0438 (0.0738)	
training:	Epoch: [25][73/204]	Loss 0.0351 (0.0733)	
training:	Epoch: [25][74/204]	Loss 0.1454 (0.0742)	
training:	Epoch: [25][75/204]	Loss 0.0560 (0.0740)	
training:	Epoch: [25][76/204]	Loss 0.0328 (0.0735)	
training:	Epoch: [25][77/204]	Loss 0.1905 (0.0750)	
training:	Epoch: [25][78/204]	Loss 0.0605 (0.0748)	
training:	Epoch: [25][79/204]	Loss 0.1231 (0.0754)	
training:	Epoch: [25][80/204]	Loss 0.1241 (0.0760)	
training:	Epoch: [25][81/204]	Loss 0.1522 (0.0769)	
training:	Epoch: [25][82/204]	Loss 0.0185 (0.0762)	
training:	Epoch: [25][83/204]	Loss 0.0259 (0.0756)	
training:	Epoch: [25][84/204]	Loss 0.1620 (0.0767)	
training:	Epoch: [25][85/204]	Loss 0.1259 (0.0772)	
training:	Epoch: [25][86/204]	Loss 0.1639 (0.0782)	
training:	Epoch: [25][87/204]	Loss 0.0443 (0.0779)	
training:	Epoch: [25][88/204]	Loss 0.0249 (0.0773)	
training:	Epoch: [25][89/204]	Loss 0.0350 (0.0768)	
training:	Epoch: [25][90/204]	Loss 0.0224 (0.0762)	
training:	Epoch: [25][91/204]	Loss 0.1511 (0.0770)	
training:	Epoch: [25][92/204]	Loss 0.1678 (0.0780)	
training:	Epoch: [25][93/204]	Loss 0.0282 (0.0774)	
training:	Epoch: [25][94/204]	Loss 0.0582 (0.0772)	
training:	Epoch: [25][95/204]	Loss 0.0280 (0.0767)	
training:	Epoch: [25][96/204]	Loss 0.0705 (0.0767)	
training:	Epoch: [25][97/204]	Loss 0.1437 (0.0774)	
training:	Epoch: [25][98/204]	Loss 0.0908 (0.0775)	
training:	Epoch: [25][99/204]	Loss 0.0741 (0.0775)	
training:	Epoch: [25][100/204]	Loss 0.2211 (0.0789)	
training:	Epoch: [25][101/204]	Loss 0.0365 (0.0785)	
training:	Epoch: [25][102/204]	Loss 0.0173 (0.0779)	
training:	Epoch: [25][103/204]	Loss 0.0240 (0.0774)	
training:	Epoch: [25][104/204]	Loss 0.0250 (0.0768)	
training:	Epoch: [25][105/204]	Loss 0.1565 (0.0776)	
training:	Epoch: [25][106/204]	Loss 0.0373 (0.0772)	
training:	Epoch: [25][107/204]	Loss 0.0390 (0.0769)	
training:	Epoch: [25][108/204]	Loss 0.0297 (0.0764)	
training:	Epoch: [25][109/204]	Loss 0.2040 (0.0776)	
training:	Epoch: [25][110/204]	Loss 0.0754 (0.0776)	
training:	Epoch: [25][111/204]	Loss 0.1451 (0.0782)	
training:	Epoch: [25][112/204]	Loss 0.0186 (0.0777)	
training:	Epoch: [25][113/204]	Loss 0.0443 (0.0774)	
training:	Epoch: [25][114/204]	Loss 0.1540 (0.0780)	
training:	Epoch: [25][115/204]	Loss 0.1519 (0.0787)	
training:	Epoch: [25][116/204]	Loss 0.1379 (0.0792)	
training:	Epoch: [25][117/204]	Loss 0.2746 (0.0809)	
training:	Epoch: [25][118/204]	Loss 0.1303 (0.0813)	
training:	Epoch: [25][119/204]	Loss 0.2745 (0.0829)	
training:	Epoch: [25][120/204]	Loss 0.0438 (0.0826)	
training:	Epoch: [25][121/204]	Loss 0.0213 (0.0821)	
training:	Epoch: [25][122/204]	Loss 0.2375 (0.0833)	
training:	Epoch: [25][123/204]	Loss 0.1000 (0.0835)	
training:	Epoch: [25][124/204]	Loss 0.1466 (0.0840)	
training:	Epoch: [25][125/204]	Loss 0.1548 (0.0846)	
training:	Epoch: [25][126/204]	Loss 0.0753 (0.0845)	
training:	Epoch: [25][127/204]	Loss 0.2400 (0.0857)	
training:	Epoch: [25][128/204]	Loss 0.0272 (0.0852)	
training:	Epoch: [25][129/204]	Loss 0.0287 (0.0848)	
training:	Epoch: [25][130/204]	Loss 0.0213 (0.0843)	
training:	Epoch: [25][131/204]	Loss 0.1734 (0.0850)	
training:	Epoch: [25][132/204]	Loss 0.0409 (0.0847)	
training:	Epoch: [25][133/204]	Loss 0.0377 (0.0843)	
training:	Epoch: [25][134/204]	Loss 0.0550 (0.0841)	
training:	Epoch: [25][135/204]	Loss 0.2750 (0.0855)	
training:	Epoch: [25][136/204]	Loss 0.1642 (0.0861)	
training:	Epoch: [25][137/204]	Loss 0.1286 (0.0864)	
training:	Epoch: [25][138/204]	Loss 0.0246 (0.0860)	
training:	Epoch: [25][139/204]	Loss 0.1637 (0.0865)	
training:	Epoch: [25][140/204]	Loss 0.0886 (0.0865)	
training:	Epoch: [25][141/204]	Loss 0.0851 (0.0865)	
training:	Epoch: [25][142/204]	Loss 0.0492 (0.0863)	
training:	Epoch: [25][143/204]	Loss 0.0291 (0.0859)	
training:	Epoch: [25][144/204]	Loss 0.2223 (0.0868)	
training:	Epoch: [25][145/204]	Loss 0.0782 (0.0867)	
training:	Epoch: [25][146/204]	Loss 0.0896 (0.0868)	
training:	Epoch: [25][147/204]	Loss 0.0188 (0.0863)	
training:	Epoch: [25][148/204]	Loss 0.0488 (0.0860)	
training:	Epoch: [25][149/204]	Loss 0.0237 (0.0856)	
training:	Epoch: [25][150/204]	Loss 0.0205 (0.0852)	
training:	Epoch: [25][151/204]	Loss 0.1918 (0.0859)	
training:	Epoch: [25][152/204]	Loss 0.1579 (0.0864)	
training:	Epoch: [25][153/204]	Loss 0.0892 (0.0864)	
training:	Epoch: [25][154/204]	Loss 0.1050 (0.0865)	
training:	Epoch: [25][155/204]	Loss 0.0303 (0.0861)	
training:	Epoch: [25][156/204]	Loss 0.0398 (0.0859)	
training:	Epoch: [25][157/204]	Loss 0.0723 (0.0858)	
training:	Epoch: [25][158/204]	Loss 0.0774 (0.0857)	
training:	Epoch: [25][159/204]	Loss 0.0493 (0.0855)	
training:	Epoch: [25][160/204]	Loss 0.0233 (0.0851)	
training:	Epoch: [25][161/204]	Loss 0.0503 (0.0849)	
training:	Epoch: [25][162/204]	Loss 0.0714 (0.0848)	
training:	Epoch: [25][163/204]	Loss 0.0375 (0.0845)	
training:	Epoch: [25][164/204]	Loss 0.0203 (0.0841)	
training:	Epoch: [25][165/204]	Loss 0.0227 (0.0837)	
training:	Epoch: [25][166/204]	Loss 0.1684 (0.0843)	
training:	Epoch: [25][167/204]	Loss 0.0598 (0.0841)	
training:	Epoch: [25][168/204]	Loss 0.1650 (0.0846)	
training:	Epoch: [25][169/204]	Loss 0.0740 (0.0845)	
training:	Epoch: [25][170/204]	Loss 0.0408 (0.0843)	
training:	Epoch: [25][171/204]	Loss 0.0427 (0.0840)	
training:	Epoch: [25][172/204]	Loss 0.0287 (0.0837)	
training:	Epoch: [25][173/204]	Loss 0.0218 (0.0833)	
training:	Epoch: [25][174/204]	Loss 0.2020 (0.0840)	
training:	Epoch: [25][175/204]	Loss 0.0706 (0.0839)	
training:	Epoch: [25][176/204]	Loss 0.2026 (0.0846)	
training:	Epoch: [25][177/204]	Loss 0.0436 (0.0844)	
training:	Epoch: [25][178/204]	Loss 0.2929 (0.0856)	
training:	Epoch: [25][179/204]	Loss 0.0217 (0.0852)	
training:	Epoch: [25][180/204]	Loss 0.1356 (0.0855)	
training:	Epoch: [25][181/204]	Loss 0.0198 (0.0851)	
training:	Epoch: [25][182/204]	Loss 0.0319 (0.0848)	
training:	Epoch: [25][183/204]	Loss 0.1149 (0.0850)	
training:	Epoch: [25][184/204]	Loss 0.0600 (0.0849)	
training:	Epoch: [25][185/204]	Loss 0.0442 (0.0846)	
training:	Epoch: [25][186/204]	Loss 0.0684 (0.0846)	
training:	Epoch: [25][187/204]	Loss 0.0268 (0.0842)	
training:	Epoch: [25][188/204]	Loss 0.1538 (0.0846)	
training:	Epoch: [25][189/204]	Loss 0.0316 (0.0843)	
training:	Epoch: [25][190/204]	Loss 0.0435 (0.0841)	
training:	Epoch: [25][191/204]	Loss 0.0288 (0.0838)	
training:	Epoch: [25][192/204]	Loss 0.0364 (0.0836)	
training:	Epoch: [25][193/204]	Loss 0.0238 (0.0833)	
training:	Epoch: [25][194/204]	Loss 0.0679 (0.0832)	
training:	Epoch: [25][195/204]	Loss 0.0364 (0.0830)	
training:	Epoch: [25][196/204]	Loss 0.1597 (0.0833)	
training:	Epoch: [25][197/204]	Loss 0.2465 (0.0842)	
training:	Epoch: [25][198/204]	Loss 0.0288 (0.0839)	
training:	Epoch: [25][199/204]	Loss 0.0361 (0.0837)	
training:	Epoch: [25][200/204]	Loss 0.1216 (0.0838)	
training:	Epoch: [25][201/204]	Loss 0.0176 (0.0835)	
training:	Epoch: [25][202/204]	Loss 0.1204 (0.0837)	
training:	Epoch: [25][203/204]	Loss 0.1646 (0.0841)	
training:	Epoch: [25][204/204]	Loss 0.1470 (0.0844)	
Training:	 Loss: 0.0843

Training:	 ACC: 0.9895 0.9894 0.9891 0.9898
Validation:	 ACC: 0.8140 0.8138 0.8096 0.8184
Validation:	 Best_BACC: 0.8326 0.8331 0.8424 0.8229
Validation:	 Loss: 0.6133
Pretraining:	Epoch 26/200
----------
training:	Epoch: [26][1/204]	Loss 0.0238 (0.0238)	
training:	Epoch: [26][2/204]	Loss 0.0429 (0.0334)	
training:	Epoch: [26][3/204]	Loss 0.0584 (0.0417)	
training:	Epoch: [26][4/204]	Loss 0.0412 (0.0416)	
training:	Epoch: [26][5/204]	Loss 0.0220 (0.0377)	
training:	Epoch: [26][6/204]	Loss 0.2058 (0.0657)	
training:	Epoch: [26][7/204]	Loss 0.0245 (0.0598)	
training:	Epoch: [26][8/204]	Loss 0.0542 (0.0591)	
training:	Epoch: [26][9/204]	Loss 0.0196 (0.0547)	
training:	Epoch: [26][10/204]	Loss 0.2932 (0.0786)	
training:	Epoch: [26][11/204]	Loss 0.0278 (0.0739)	
training:	Epoch: [26][12/204]	Loss 0.1411 (0.0795)	
training:	Epoch: [26][13/204]	Loss 0.0263 (0.0754)	
training:	Epoch: [26][14/204]	Loss 0.0205 (0.0715)	
training:	Epoch: [26][15/204]	Loss 0.1489 (0.0767)	
training:	Epoch: [26][16/204]	Loss 0.1626 (0.0821)	
training:	Epoch: [26][17/204]	Loss 0.0243 (0.0787)	
training:	Epoch: [26][18/204]	Loss 0.0650 (0.0779)	
training:	Epoch: [26][19/204]	Loss 0.0177 (0.0747)	
training:	Epoch: [26][20/204]	Loss 0.1481 (0.0784)	
training:	Epoch: [26][21/204]	Loss 0.0580 (0.0774)	
training:	Epoch: [26][22/204]	Loss 0.0175 (0.0747)	
training:	Epoch: [26][23/204]	Loss 0.0217 (0.0724)	
training:	Epoch: [26][24/204]	Loss 0.0453 (0.0713)	
training:	Epoch: [26][25/204]	Loss 0.1165 (0.0731)	
training:	Epoch: [26][26/204]	Loss 0.0685 (0.0729)	
training:	Epoch: [26][27/204]	Loss 0.0714 (0.0728)	
training:	Epoch: [26][28/204]	Loss 0.0659 (0.0726)	
training:	Epoch: [26][29/204]	Loss 0.0411 (0.0715)	
training:	Epoch: [26][30/204]	Loss 0.0235 (0.0699)	
training:	Epoch: [26][31/204]	Loss 0.1468 (0.0724)	
training:	Epoch: [26][32/204]	Loss 0.1687 (0.0754)	
training:	Epoch: [26][33/204]	Loss 0.0266 (0.0739)	
training:	Epoch: [26][34/204]	Loss 0.1607 (0.0765)	
training:	Epoch: [26][35/204]	Loss 0.0340 (0.0753)	
training:	Epoch: [26][36/204]	Loss 0.0225 (0.0738)	
training:	Epoch: [26][37/204]	Loss 0.0357 (0.0728)	
training:	Epoch: [26][38/204]	Loss 0.0502 (0.0722)	
training:	Epoch: [26][39/204]	Loss 0.1662 (0.0746)	
training:	Epoch: [26][40/204]	Loss 0.0224 (0.0733)	
training:	Epoch: [26][41/204]	Loss 0.0185 (0.0719)	
training:	Epoch: [26][42/204]	Loss 0.0227 (0.0708)	
training:	Epoch: [26][43/204]	Loss 0.0704 (0.0708)	
training:	Epoch: [26][44/204]	Loss 0.0307 (0.0698)	
training:	Epoch: [26][45/204]	Loss 0.1849 (0.0724)	
training:	Epoch: [26][46/204]	Loss 0.0264 (0.0714)	
training:	Epoch: [26][47/204]	Loss 0.0209 (0.0703)	
training:	Epoch: [26][48/204]	Loss 0.0211 (0.0693)	
training:	Epoch: [26][49/204]	Loss 0.1664 (0.0713)	
training:	Epoch: [26][50/204]	Loss 0.3070 (0.0760)	
training:	Epoch: [26][51/204]	Loss 0.0247 (0.0750)	
training:	Epoch: [26][52/204]	Loss 0.0783 (0.0751)	
training:	Epoch: [26][53/204]	Loss 0.1689 (0.0768)	
training:	Epoch: [26][54/204]	Loss 0.0839 (0.0770)	
training:	Epoch: [26][55/204]	Loss 0.0411 (0.0763)	
training:	Epoch: [26][56/204]	Loss 0.0290 (0.0755)	
training:	Epoch: [26][57/204]	Loss 0.1356 (0.0765)	
training:	Epoch: [26][58/204]	Loss 0.0346 (0.0758)	
training:	Epoch: [26][59/204]	Loss 0.1531 (0.0771)	
training:	Epoch: [26][60/204]	Loss 0.0182 (0.0761)	
training:	Epoch: [26][61/204]	Loss 0.2304 (0.0787)	
training:	Epoch: [26][62/204]	Loss 0.0245 (0.0778)	
training:	Epoch: [26][63/204]	Loss 0.0790 (0.0778)	
training:	Epoch: [26][64/204]	Loss 0.1478 (0.0789)	
training:	Epoch: [26][65/204]	Loss 0.0620 (0.0786)	
training:	Epoch: [26][66/204]	Loss 0.1591 (0.0799)	
training:	Epoch: [26][67/204]	Loss 0.0430 (0.0793)	
training:	Epoch: [26][68/204]	Loss 0.0252 (0.0785)	
training:	Epoch: [26][69/204]	Loss 0.0242 (0.0777)	
training:	Epoch: [26][70/204]	Loss 0.0223 (0.0769)	
training:	Epoch: [26][71/204]	Loss 0.1478 (0.0779)	
training:	Epoch: [26][72/204]	Loss 0.0577 (0.0776)	
training:	Epoch: [26][73/204]	Loss 0.1898 (0.0792)	
training:	Epoch: [26][74/204]	Loss 0.0237 (0.0784)	
training:	Epoch: [26][75/204]	Loss 0.0238 (0.0777)	
training:	Epoch: [26][76/204]	Loss 0.2131 (0.0795)	
training:	Epoch: [26][77/204]	Loss 0.0755 (0.0794)	
training:	Epoch: [26][78/204]	Loss 0.1679 (0.0806)	
training:	Epoch: [26][79/204]	Loss 0.0187 (0.0798)	
training:	Epoch: [26][80/204]	Loss 0.0343 (0.0792)	
training:	Epoch: [26][81/204]	Loss 0.2278 (0.0810)	
training:	Epoch: [26][82/204]	Loss 0.0292 (0.0804)	
training:	Epoch: [26][83/204]	Loss 0.0334 (0.0798)	
training:	Epoch: [26][84/204]	Loss 0.0166 (0.0791)	
training:	Epoch: [26][85/204]	Loss 0.1641 (0.0801)	
training:	Epoch: [26][86/204]	Loss 0.0154 (0.0793)	
training:	Epoch: [26][87/204]	Loss 0.0751 (0.0793)	
training:	Epoch: [26][88/204]	Loss 0.0178 (0.0786)	
training:	Epoch: [26][89/204]	Loss 0.0363 (0.0781)	
training:	Epoch: [26][90/204]	Loss 0.0288 (0.0776)	
training:	Epoch: [26][91/204]	Loss 0.1802 (0.0787)	
training:	Epoch: [26][92/204]	Loss 0.0369 (0.0782)	
training:	Epoch: [26][93/204]	Loss 0.1763 (0.0793)	
training:	Epoch: [26][94/204]	Loss 0.0153 (0.0786)	
training:	Epoch: [26][95/204]	Loss 0.0844 (0.0787)	
training:	Epoch: [26][96/204]	Loss 0.1482 (0.0794)	
training:	Epoch: [26][97/204]	Loss 0.0867 (0.0795)	
training:	Epoch: [26][98/204]	Loss 0.1486 (0.0802)	
training:	Epoch: [26][99/204]	Loss 0.1349 (0.0807)	
training:	Epoch: [26][100/204]	Loss 0.0371 (0.0803)	
training:	Epoch: [26][101/204]	Loss 0.1427 (0.0809)	
training:	Epoch: [26][102/204]	Loss 0.1903 (0.0820)	
training:	Epoch: [26][103/204]	Loss 0.1672 (0.0828)	
training:	Epoch: [26][104/204]	Loss 0.0174 (0.0822)	
training:	Epoch: [26][105/204]	Loss 0.0364 (0.0818)	
training:	Epoch: [26][106/204]	Loss 0.0220 (0.0812)	
training:	Epoch: [26][107/204]	Loss 0.1583 (0.0819)	
training:	Epoch: [26][108/204]	Loss 0.0981 (0.0821)	
training:	Epoch: [26][109/204]	Loss 0.0215 (0.0815)	
training:	Epoch: [26][110/204]	Loss 0.0344 (0.0811)	
training:	Epoch: [26][111/204]	Loss 0.1563 (0.0818)	
training:	Epoch: [26][112/204]	Loss 0.0269 (0.0813)	
training:	Epoch: [26][113/204]	Loss 0.1035 (0.0815)	
training:	Epoch: [26][114/204]	Loss 0.0899 (0.0815)	
training:	Epoch: [26][115/204]	Loss 0.0478 (0.0812)	
training:	Epoch: [26][116/204]	Loss 0.0248 (0.0808)	
training:	Epoch: [26][117/204]	Loss 0.1392 (0.0813)	
training:	Epoch: [26][118/204]	Loss 0.0628 (0.0811)	
training:	Epoch: [26][119/204]	Loss 0.1253 (0.0815)	
training:	Epoch: [26][120/204]	Loss 0.0872 (0.0815)	
training:	Epoch: [26][121/204]	Loss 0.1343 (0.0820)	
training:	Epoch: [26][122/204]	Loss 0.0377 (0.0816)	
training:	Epoch: [26][123/204]	Loss 0.1823 (0.0824)	
training:	Epoch: [26][124/204]	Loss 0.1698 (0.0831)	
training:	Epoch: [26][125/204]	Loss 0.0491 (0.0828)	
training:	Epoch: [26][126/204]	Loss 0.0425 (0.0825)	
training:	Epoch: [26][127/204]	Loss 0.0332 (0.0821)	
training:	Epoch: [26][128/204]	Loss 0.0479 (0.0819)	
training:	Epoch: [26][129/204]	Loss 0.0533 (0.0816)	
training:	Epoch: [26][130/204]	Loss 0.1923 (0.0825)	
training:	Epoch: [26][131/204]	Loss 0.0144 (0.0820)	
training:	Epoch: [26][132/204]	Loss 0.0157 (0.0815)	
training:	Epoch: [26][133/204]	Loss 0.0156 (0.0810)	
training:	Epoch: [26][134/204]	Loss 0.0205 (0.0805)	
training:	Epoch: [26][135/204]	Loss 0.0196 (0.0801)	
training:	Epoch: [26][136/204]	Loss 0.0394 (0.0798)	
training:	Epoch: [26][137/204]	Loss 0.0232 (0.0794)	
training:	Epoch: [26][138/204]	Loss 0.0608 (0.0792)	
training:	Epoch: [26][139/204]	Loss 0.0386 (0.0789)	
training:	Epoch: [26][140/204]	Loss 0.2566 (0.0802)	
training:	Epoch: [26][141/204]	Loss 0.0311 (0.0799)	
training:	Epoch: [26][142/204]	Loss 0.0237 (0.0795)	
training:	Epoch: [26][143/204]	Loss 0.1378 (0.0799)	
training:	Epoch: [26][144/204]	Loss 0.1629 (0.0805)	
training:	Epoch: [26][145/204]	Loss 0.0165 (0.0800)	
training:	Epoch: [26][146/204]	Loss 0.1066 (0.0802)	
training:	Epoch: [26][147/204]	Loss 0.1333 (0.0806)	
training:	Epoch: [26][148/204]	Loss 0.0234 (0.0802)	
training:	Epoch: [26][149/204]	Loss 0.0203 (0.0798)	
training:	Epoch: [26][150/204]	Loss 0.0143 (0.0793)	
training:	Epoch: [26][151/204]	Loss 0.1535 (0.0798)	
training:	Epoch: [26][152/204]	Loss 0.0292 (0.0795)	
training:	Epoch: [26][153/204]	Loss 0.0225 (0.0791)	
training:	Epoch: [26][154/204]	Loss 0.0660 (0.0790)	
training:	Epoch: [26][155/204]	Loss 0.0130 (0.0786)	
training:	Epoch: [26][156/204]	Loss 0.1555 (0.0791)	
training:	Epoch: [26][157/204]	Loss 0.0224 (0.0787)	
training:	Epoch: [26][158/204]	Loss 0.0613 (0.0786)	
training:	Epoch: [26][159/204]	Loss 0.1624 (0.0792)	
training:	Epoch: [26][160/204]	Loss 0.1645 (0.0797)	
training:	Epoch: [26][161/204]	Loss 0.0369 (0.0794)	
training:	Epoch: [26][162/204]	Loss 0.0873 (0.0795)	
training:	Epoch: [26][163/204]	Loss 0.1211 (0.0797)	
training:	Epoch: [26][164/204]	Loss 0.0273 (0.0794)	
training:	Epoch: [26][165/204]	Loss 0.0186 (0.0790)	
training:	Epoch: [26][166/204]	Loss 0.0286 (0.0787)	
training:	Epoch: [26][167/204]	Loss 0.0631 (0.0786)	
training:	Epoch: [26][168/204]	Loss 0.1139 (0.0788)	
training:	Epoch: [26][169/204]	Loss 0.0996 (0.0790)	
training:	Epoch: [26][170/204]	Loss 0.0303 (0.0787)	
training:	Epoch: [26][171/204]	Loss 0.0560 (0.0786)	
training:	Epoch: [26][172/204]	Loss 0.0233 (0.0782)	
training:	Epoch: [26][173/204]	Loss 0.2189 (0.0790)	
training:	Epoch: [26][174/204]	Loss 0.1832 (0.0796)	
training:	Epoch: [26][175/204]	Loss 0.1230 (0.0799)	
training:	Epoch: [26][176/204]	Loss 0.0166 (0.0795)	
training:	Epoch: [26][177/204]	Loss 0.1563 (0.0800)	
training:	Epoch: [26][178/204]	Loss 0.0227 (0.0796)	
training:	Epoch: [26][179/204]	Loss 0.1030 (0.0798)	
training:	Epoch: [26][180/204]	Loss 0.0196 (0.0794)	
training:	Epoch: [26][181/204]	Loss 0.0600 (0.0793)	
training:	Epoch: [26][182/204]	Loss 0.0395 (0.0791)	
training:	Epoch: [26][183/204]	Loss 0.0321 (0.0789)	
training:	Epoch: [26][184/204]	Loss 0.0495 (0.0787)	
training:	Epoch: [26][185/204]	Loss 0.0270 (0.0784)	
training:	Epoch: [26][186/204]	Loss 0.0168 (0.0781)	
training:	Epoch: [26][187/204]	Loss 0.0587 (0.0780)	
training:	Epoch: [26][188/204]	Loss 0.1461 (0.0783)	
training:	Epoch: [26][189/204]	Loss 0.1400 (0.0787)	
training:	Epoch: [26][190/204]	Loss 0.1282 (0.0789)	
training:	Epoch: [26][191/204]	Loss 0.0301 (0.0787)	
training:	Epoch: [26][192/204]	Loss 0.1344 (0.0790)	
training:	Epoch: [26][193/204]	Loss 0.0259 (0.0787)	
training:	Epoch: [26][194/204]	Loss 0.0467 (0.0785)	
training:	Epoch: [26][195/204]	Loss 0.0368 (0.0783)	
training:	Epoch: [26][196/204]	Loss 0.1475 (0.0787)	
training:	Epoch: [26][197/204]	Loss 0.0506 (0.0785)	
training:	Epoch: [26][198/204]	Loss 0.0952 (0.0786)	
training:	Epoch: [26][199/204]	Loss 0.0910 (0.0787)	
training:	Epoch: [26][200/204]	Loss 0.1493 (0.0790)	
training:	Epoch: [26][201/204]	Loss 0.0483 (0.0789)	
training:	Epoch: [26][202/204]	Loss 0.0140 (0.0785)	
training:	Epoch: [26][203/204]	Loss 0.0538 (0.0784)	
training:	Epoch: [26][204/204]	Loss 0.0246 (0.0782)	
Training:	 Loss: 0.0780

Training:	 ACC: 0.9898 0.9898 0.9894 0.9901
Validation:	 ACC: 0.7990 0.7983 0.7830 0.8150
Validation:	 Best_BACC: 0.8326 0.8331 0.8424 0.8229
Validation:	 Loss: 0.6533
Pretraining:	Epoch 27/200
----------
training:	Epoch: [27][1/204]	Loss 0.0232 (0.0232)	
training:	Epoch: [27][2/204]	Loss 0.0242 (0.0237)	
training:	Epoch: [27][3/204]	Loss 0.1250 (0.0575)	
training:	Epoch: [27][4/204]	Loss 0.0403 (0.0532)	
training:	Epoch: [27][5/204]	Loss 0.0334 (0.0492)	
training:	Epoch: [27][6/204]	Loss 0.0217 (0.0446)	
training:	Epoch: [27][7/204]	Loss 0.0301 (0.0426)	
training:	Epoch: [27][8/204]	Loss 0.0294 (0.0409)	
training:	Epoch: [27][9/204]	Loss 0.0223 (0.0388)	
training:	Epoch: [27][10/204]	Loss 0.1282 (0.0478)	
training:	Epoch: [27][11/204]	Loss 0.0760 (0.0503)	
training:	Epoch: [27][12/204]	Loss 0.0895 (0.0536)	
training:	Epoch: [27][13/204]	Loss 0.0388 (0.0525)	
training:	Epoch: [27][14/204]	Loss 0.0175 (0.0500)	
training:	Epoch: [27][15/204]	Loss 0.0169 (0.0478)	
training:	Epoch: [27][16/204]	Loss 0.1504 (0.0542)	
training:	Epoch: [27][17/204]	Loss 0.1033 (0.0571)	
training:	Epoch: [27][18/204]	Loss 0.1085 (0.0599)	
training:	Epoch: [27][19/204]	Loss 0.2398 (0.0694)	
training:	Epoch: [27][20/204]	Loss 0.1735 (0.0746)	
training:	Epoch: [27][21/204]	Loss 0.1706 (0.0792)	
training:	Epoch: [27][22/204]	Loss 0.1871 (0.0841)	
training:	Epoch: [27][23/204]	Loss 0.0153 (0.0811)	
training:	Epoch: [27][24/204]	Loss 0.0684 (0.0806)	
training:	Epoch: [27][25/204]	Loss 0.0219 (0.0782)	
training:	Epoch: [27][26/204]	Loss 0.1540 (0.0811)	
training:	Epoch: [27][27/204]	Loss 0.1717 (0.0845)	
training:	Epoch: [27][28/204]	Loss 0.0213 (0.0822)	
training:	Epoch: [27][29/204]	Loss 0.0209 (0.0801)	
training:	Epoch: [27][30/204]	Loss 0.0302 (0.0785)	
training:	Epoch: [27][31/204]	Loss 0.1412 (0.0805)	
training:	Epoch: [27][32/204]	Loss 0.1499 (0.0826)	
training:	Epoch: [27][33/204]	Loss 0.1954 (0.0861)	
training:	Epoch: [27][34/204]	Loss 0.2484 (0.0908)	
training:	Epoch: [27][35/204]	Loss 0.0348 (0.0892)	
training:	Epoch: [27][36/204]	Loss 0.0363 (0.0878)	
training:	Epoch: [27][37/204]	Loss 0.0240 (0.0860)	
training:	Epoch: [27][38/204]	Loss 0.0327 (0.0846)	
training:	Epoch: [27][39/204]	Loss 0.0197 (0.0830)	
training:	Epoch: [27][40/204]	Loss 0.0353 (0.0818)	
training:	Epoch: [27][41/204]	Loss 0.0729 (0.0816)	
training:	Epoch: [27][42/204]	Loss 0.0311 (0.0804)	
training:	Epoch: [27][43/204]	Loss 0.1214 (0.0813)	
training:	Epoch: [27][44/204]	Loss 0.1533 (0.0830)	
training:	Epoch: [27][45/204]	Loss 0.1539 (0.0845)	
training:	Epoch: [27][46/204]	Loss 0.1674 (0.0863)	
training:	Epoch: [27][47/204]	Loss 0.0209 (0.0849)	
training:	Epoch: [27][48/204]	Loss 0.1658 (0.0866)	
training:	Epoch: [27][49/204]	Loss 0.0941 (0.0868)	
training:	Epoch: [27][50/204]	Loss 0.1278 (0.0876)	
training:	Epoch: [27][51/204]	Loss 0.0538 (0.0869)	
training:	Epoch: [27][52/204]	Loss 0.0162 (0.0856)	
training:	Epoch: [27][53/204]	Loss 0.0416 (0.0847)	
training:	Epoch: [27][54/204]	Loss 0.0946 (0.0849)	
training:	Epoch: [27][55/204]	Loss 0.0413 (0.0841)	
training:	Epoch: [27][56/204]	Loss 0.2647 (0.0874)	
training:	Epoch: [27][57/204]	Loss 0.0167 (0.0861)	
training:	Epoch: [27][58/204]	Loss 0.0335 (0.0852)	
training:	Epoch: [27][59/204]	Loss 0.0129 (0.0840)	
training:	Epoch: [27][60/204]	Loss 0.1001 (0.0843)	
training:	Epoch: [27][61/204]	Loss 0.0184 (0.0832)	
training:	Epoch: [27][62/204]	Loss 0.1165 (0.0837)	
training:	Epoch: [27][63/204]	Loss 0.0159 (0.0826)	
training:	Epoch: [27][64/204]	Loss 0.1440 (0.0836)	
training:	Epoch: [27][65/204]	Loss 0.0180 (0.0826)	
training:	Epoch: [27][66/204]	Loss 0.0140 (0.0815)	
training:	Epoch: [27][67/204]	Loss 0.0242 (0.0807)	
training:	Epoch: [27][68/204]	Loss 0.1163 (0.0812)	
training:	Epoch: [27][69/204]	Loss 0.1603 (0.0824)	
training:	Epoch: [27][70/204]	Loss 0.1521 (0.0834)	
training:	Epoch: [27][71/204]	Loss 0.1883 (0.0848)	
training:	Epoch: [27][72/204]	Loss 0.0239 (0.0840)	
training:	Epoch: [27][73/204]	Loss 0.0344 (0.0833)	
training:	Epoch: [27][74/204]	Loss 0.0190 (0.0824)	
training:	Epoch: [27][75/204]	Loss 0.1979 (0.0840)	
training:	Epoch: [27][76/204]	Loss 0.0825 (0.0840)	
training:	Epoch: [27][77/204]	Loss 0.0361 (0.0833)	
training:	Epoch: [27][78/204]	Loss 0.0909 (0.0834)	
training:	Epoch: [27][79/204]	Loss 0.2842 (0.0860)	
training:	Epoch: [27][80/204]	Loss 0.0265 (0.0852)	
training:	Epoch: [27][81/204]	Loss 0.1605 (0.0862)	
training:	Epoch: [27][82/204]	Loss 0.0207 (0.0854)	
training:	Epoch: [27][83/204]	Loss 0.0407 (0.0848)	
training:	Epoch: [27][84/204]	Loss 0.0270 (0.0841)	
training:	Epoch: [27][85/204]	Loss 0.0266 (0.0835)	
training:	Epoch: [27][86/204]	Loss 0.0525 (0.0831)	
training:	Epoch: [27][87/204]	Loss 0.1382 (0.0837)	
training:	Epoch: [27][88/204]	Loss 0.1772 (0.0848)	
training:	Epoch: [27][89/204]	Loss 0.1591 (0.0856)	
training:	Epoch: [27][90/204]	Loss 0.0977 (0.0858)	
training:	Epoch: [27][91/204]	Loss 0.1608 (0.0866)	
training:	Epoch: [27][92/204]	Loss 0.0198 (0.0859)	
training:	Epoch: [27][93/204]	Loss 0.0607 (0.0856)	
training:	Epoch: [27][94/204]	Loss 0.0240 (0.0849)	
training:	Epoch: [27][95/204]	Loss 0.1620 (0.0857)	
training:	Epoch: [27][96/204]	Loss 0.0352 (0.0852)	
training:	Epoch: [27][97/204]	Loss 0.1520 (0.0859)	
training:	Epoch: [27][98/204]	Loss 0.0181 (0.0852)	
training:	Epoch: [27][99/204]	Loss 0.0833 (0.0852)	
training:	Epoch: [27][100/204]	Loss 0.0233 (0.0846)	
training:	Epoch: [27][101/204]	Loss 0.0168 (0.0839)	
training:	Epoch: [27][102/204]	Loss 0.0348 (0.0834)	
training:	Epoch: [27][103/204]	Loss 0.0893 (0.0835)	
training:	Epoch: [27][104/204]	Loss 0.1302 (0.0839)	
training:	Epoch: [27][105/204]	Loss 0.0219 (0.0833)	
training:	Epoch: [27][106/204]	Loss 0.0236 (0.0828)	
training:	Epoch: [27][107/204]	Loss 0.0210 (0.0822)	
training:	Epoch: [27][108/204]	Loss 0.0189 (0.0816)	
training:	Epoch: [27][109/204]	Loss 0.0182 (0.0810)	
training:	Epoch: [27][110/204]	Loss 0.2021 (0.0821)	
training:	Epoch: [27][111/204]	Loss 0.0976 (0.0823)	
training:	Epoch: [27][112/204]	Loss 0.0194 (0.0817)	
training:	Epoch: [27][113/204]	Loss 0.0301 (0.0813)	
training:	Epoch: [27][114/204]	Loss 0.0330 (0.0808)	
training:	Epoch: [27][115/204]	Loss 0.0246 (0.0803)	
training:	Epoch: [27][116/204]	Loss 0.0434 (0.0800)	
training:	Epoch: [27][117/204]	Loss 0.0320 (0.0796)	
training:	Epoch: [27][118/204]	Loss 0.0321 (0.0792)	
training:	Epoch: [27][119/204]	Loss 0.1497 (0.0798)	
training:	Epoch: [27][120/204]	Loss 0.0198 (0.0793)	
training:	Epoch: [27][121/204]	Loss 0.0204 (0.0788)	
training:	Epoch: [27][122/204]	Loss 0.0157 (0.0783)	
training:	Epoch: [27][123/204]	Loss 0.1189 (0.0786)	
training:	Epoch: [27][124/204]	Loss 0.0209 (0.0782)	
training:	Epoch: [27][125/204]	Loss 0.0262 (0.0777)	
training:	Epoch: [27][126/204]	Loss 0.0280 (0.0774)	
training:	Epoch: [27][127/204]	Loss 0.0537 (0.0772)	
training:	Epoch: [27][128/204]	Loss 0.0349 (0.0768)	
training:	Epoch: [27][129/204]	Loss 0.0309 (0.0765)	
training:	Epoch: [27][130/204]	Loss 0.0177 (0.0760)	
training:	Epoch: [27][131/204]	Loss 0.0158 (0.0756)	
training:	Epoch: [27][132/204]	Loss 0.0452 (0.0753)	
training:	Epoch: [27][133/204]	Loss 0.0205 (0.0749)	
training:	Epoch: [27][134/204]	Loss 0.1114 (0.0752)	
training:	Epoch: [27][135/204]	Loss 0.1284 (0.0756)	
training:	Epoch: [27][136/204]	Loss 0.1646 (0.0762)	
training:	Epoch: [27][137/204]	Loss 0.0153 (0.0758)	
training:	Epoch: [27][138/204]	Loss 0.0328 (0.0755)	
training:	Epoch: [27][139/204]	Loss 0.0347 (0.0752)	
training:	Epoch: [27][140/204]	Loss 0.1555 (0.0758)	
training:	Epoch: [27][141/204]	Loss 0.0429 (0.0755)	
training:	Epoch: [27][142/204]	Loss 0.0216 (0.0752)	
training:	Epoch: [27][143/204]	Loss 0.1463 (0.0757)	
training:	Epoch: [27][144/204]	Loss 0.0198 (0.0753)	
training:	Epoch: [27][145/204]	Loss 0.1459 (0.0758)	
training:	Epoch: [27][146/204]	Loss 0.0162 (0.0753)	
training:	Epoch: [27][147/204]	Loss 0.1581 (0.0759)	
training:	Epoch: [27][148/204]	Loss 0.0315 (0.0756)	
training:	Epoch: [27][149/204]	Loss 0.2234 (0.0766)	
training:	Epoch: [27][150/204]	Loss 0.0290 (0.0763)	
training:	Epoch: [27][151/204]	Loss 0.1271 (0.0766)	
training:	Epoch: [27][152/204]	Loss 0.0199 (0.0762)	
training:	Epoch: [27][153/204]	Loss 0.0188 (0.0759)	
training:	Epoch: [27][154/204]	Loss 0.0140 (0.0755)	
training:	Epoch: [27][155/204]	Loss 0.0598 (0.0754)	
training:	Epoch: [27][156/204]	Loss 0.0445 (0.0752)	
training:	Epoch: [27][157/204]	Loss 0.0468 (0.0750)	
training:	Epoch: [27][158/204]	Loss 0.0144 (0.0746)	
training:	Epoch: [27][159/204]	Loss 0.0230 (0.0743)	
training:	Epoch: [27][160/204]	Loss 0.0238 (0.0740)	
training:	Epoch: [27][161/204]	Loss 0.0691 (0.0739)	
training:	Epoch: [27][162/204]	Loss 0.0185 (0.0736)	
training:	Epoch: [27][163/204]	Loss 0.0281 (0.0733)	
training:	Epoch: [27][164/204]	Loss 0.0197 (0.0730)	
training:	Epoch: [27][165/204]	Loss 0.1569 (0.0735)	
training:	Epoch: [27][166/204]	Loss 0.0253 (0.0732)	
training:	Epoch: [27][167/204]	Loss 0.3198 (0.0747)	
training:	Epoch: [27][168/204]	Loss 0.0303 (0.0744)	
training:	Epoch: [27][169/204]	Loss 0.0206 (0.0741)	
training:	Epoch: [27][170/204]	Loss 0.0188 (0.0738)	
training:	Epoch: [27][171/204]	Loss 0.0368 (0.0736)	
training:	Epoch: [27][172/204]	Loss 0.0795 (0.0736)	
training:	Epoch: [27][173/204]	Loss 0.0167 (0.0733)	
training:	Epoch: [27][174/204]	Loss 0.0166 (0.0729)	
training:	Epoch: [27][175/204]	Loss 0.0168 (0.0726)	
training:	Epoch: [27][176/204]	Loss 0.1678 (0.0732)	
training:	Epoch: [27][177/204]	Loss 0.0424 (0.0730)	
training:	Epoch: [27][178/204]	Loss 0.1972 (0.0737)	
training:	Epoch: [27][179/204]	Loss 0.0202 (0.0734)	
training:	Epoch: [27][180/204]	Loss 0.1407 (0.0738)	
training:	Epoch: [27][181/204]	Loss 0.0152 (0.0734)	
training:	Epoch: [27][182/204]	Loss 0.0377 (0.0732)	
training:	Epoch: [27][183/204]	Loss 0.0514 (0.0731)	
training:	Epoch: [27][184/204]	Loss 0.3156 (0.0744)	
training:	Epoch: [27][185/204]	Loss 0.0154 (0.0741)	
training:	Epoch: [27][186/204]	Loss 0.1541 (0.0745)	
training:	Epoch: [27][187/204]	Loss 0.0147 (0.0742)	
training:	Epoch: [27][188/204]	Loss 0.1212 (0.0745)	
training:	Epoch: [27][189/204]	Loss 0.1205 (0.0747)	
training:	Epoch: [27][190/204]	Loss 0.0195 (0.0744)	
training:	Epoch: [27][191/204]	Loss 0.0571 (0.0743)	
training:	Epoch: [27][192/204]	Loss 0.0207 (0.0741)	
training:	Epoch: [27][193/204]	Loss 0.0226 (0.0738)	
training:	Epoch: [27][194/204]	Loss 0.0198 (0.0735)	
training:	Epoch: [27][195/204]	Loss 0.0289 (0.0733)	
training:	Epoch: [27][196/204]	Loss 0.0163 (0.0730)	
training:	Epoch: [27][197/204]	Loss 0.0829 (0.0730)	
training:	Epoch: [27][198/204]	Loss 0.1073 (0.0732)	
training:	Epoch: [27][199/204]	Loss 0.0212 (0.0730)	
training:	Epoch: [27][200/204]	Loss 0.1559 (0.0734)	
training:	Epoch: [27][201/204]	Loss 0.0155 (0.0731)	
training:	Epoch: [27][202/204]	Loss 0.0983 (0.0732)	
training:	Epoch: [27][203/204]	Loss 0.1507 (0.0736)	
training:	Epoch: [27][204/204]	Loss 0.0155 (0.0733)	
Training:	 Loss: 0.0732

Training:	 ACC: 0.9919 0.9919 0.9927 0.9911
Validation:	 ACC: 0.8047 0.8058 0.8291 0.7803
Validation:	 Best_BACC: 0.8326 0.8331 0.8424 0.8229
Validation:	 Loss: 0.6700
Pretraining:	Epoch 28/200
----------
training:	Epoch: [28][1/204]	Loss 0.0880 (0.0880)	
training:	Epoch: [28][2/204]	Loss 0.0172 (0.0526)	
training:	Epoch: [28][3/204]	Loss 0.0143 (0.0398)	
training:	Epoch: [28][4/204]	Loss 0.0175 (0.0342)	
training:	Epoch: [28][5/204]	Loss 0.0351 (0.0344)	
training:	Epoch: [28][6/204]	Loss 0.0250 (0.0328)	
training:	Epoch: [28][7/204]	Loss 0.0235 (0.0315)	
training:	Epoch: [28][8/204]	Loss 0.0433 (0.0330)	
training:	Epoch: [28][9/204]	Loss 0.1478 (0.0457)	
training:	Epoch: [28][10/204]	Loss 0.0179 (0.0430)	
training:	Epoch: [28][11/204]	Loss 0.0310 (0.0419)	
training:	Epoch: [28][12/204]	Loss 0.1331 (0.0495)	
training:	Epoch: [28][13/204]	Loss 0.0214 (0.0473)	
training:	Epoch: [28][14/204]	Loss 0.0289 (0.0460)	
training:	Epoch: [28][15/204]	Loss 0.0195 (0.0442)	
training:	Epoch: [28][16/204]	Loss 0.0576 (0.0451)	
training:	Epoch: [28][17/204]	Loss 0.0182 (0.0435)	
training:	Epoch: [28][18/204]	Loss 0.1467 (0.0492)	
training:	Epoch: [28][19/204]	Loss 0.0189 (0.0476)	
training:	Epoch: [28][20/204]	Loss 0.0142 (0.0460)	
training:	Epoch: [28][21/204]	Loss 0.0247 (0.0449)	
training:	Epoch: [28][22/204]	Loss 0.0269 (0.0441)	
training:	Epoch: [28][23/204]	Loss 0.0380 (0.0439)	
training:	Epoch: [28][24/204]	Loss 0.0204 (0.0429)	
training:	Epoch: [28][25/204]	Loss 0.1514 (0.0472)	
training:	Epoch: [28][26/204]	Loss 0.1907 (0.0527)	
training:	Epoch: [28][27/204]	Loss 0.0203 (0.0515)	
training:	Epoch: [28][28/204]	Loss 0.1631 (0.0555)	
training:	Epoch: [28][29/204]	Loss 0.0192 (0.0543)	
training:	Epoch: [28][30/204]	Loss 0.0194 (0.0531)	
training:	Epoch: [28][31/204]	Loss 0.0128 (0.0518)	
training:	Epoch: [28][32/204]	Loss 0.0208 (0.0508)	
training:	Epoch: [28][33/204]	Loss 0.0859 (0.0519)	
training:	Epoch: [28][34/204]	Loss 0.0410 (0.0516)	
training:	Epoch: [28][35/204]	Loss 0.0651 (0.0520)	
training:	Epoch: [28][36/204]	Loss 0.0239 (0.0512)	
training:	Epoch: [28][37/204]	Loss 0.0191 (0.0503)	
training:	Epoch: [28][38/204]	Loss 0.1334 (0.0525)	
training:	Epoch: [28][39/204]	Loss 0.0291 (0.0519)	
training:	Epoch: [28][40/204]	Loss 0.0188 (0.0511)	
training:	Epoch: [28][41/204]	Loss 0.0260 (0.0505)	
training:	Epoch: [28][42/204]	Loss 0.0222 (0.0498)	
training:	Epoch: [28][43/204]	Loss 0.0453 (0.0497)	
training:	Epoch: [28][44/204]	Loss 0.0187 (0.0490)	
training:	Epoch: [28][45/204]	Loss 0.0176 (0.0483)	
training:	Epoch: [28][46/204]	Loss 0.0124 (0.0475)	
training:	Epoch: [28][47/204]	Loss 0.0372 (0.0473)	
training:	Epoch: [28][48/204]	Loss 0.0511 (0.0474)	
training:	Epoch: [28][49/204]	Loss 0.0888 (0.0482)	
training:	Epoch: [28][50/204]	Loss 0.0183 (0.0476)	
training:	Epoch: [28][51/204]	Loss 0.0151 (0.0470)	
training:	Epoch: [28][52/204]	Loss 0.0244 (0.0465)	
training:	Epoch: [28][53/204]	Loss 0.1641 (0.0488)	
training:	Epoch: [28][54/204]	Loss 0.0136 (0.0481)	
training:	Epoch: [28][55/204]	Loss 0.0704 (0.0485)	
training:	Epoch: [28][56/204]	Loss 0.1608 (0.0505)	
training:	Epoch: [28][57/204]	Loss 0.1389 (0.0521)	
training:	Epoch: [28][58/204]	Loss 0.1228 (0.0533)	
training:	Epoch: [28][59/204]	Loss 0.0444 (0.0531)	
training:	Epoch: [28][60/204]	Loss 0.1236 (0.0543)	
training:	Epoch: [28][61/204]	Loss 0.0124 (0.0536)	
training:	Epoch: [28][62/204]	Loss 0.0640 (0.0538)	
training:	Epoch: [28][63/204]	Loss 0.2554 (0.0570)	
training:	Epoch: [28][64/204]	Loss 0.0787 (0.0573)	
training:	Epoch: [28][65/204]	Loss 0.0803 (0.0577)	
training:	Epoch: [28][66/204]	Loss 0.3231 (0.0617)	
training:	Epoch: [28][67/204]	Loss 0.0178 (0.0611)	
training:	Epoch: [28][68/204]	Loss 0.0963 (0.0616)	
training:	Epoch: [28][69/204]	Loss 0.0277 (0.0611)	
training:	Epoch: [28][70/204]	Loss 0.0172 (0.0605)	
training:	Epoch: [28][71/204]	Loss 0.0184 (0.0599)	
training:	Epoch: [28][72/204]	Loss 0.1066 (0.0605)	
training:	Epoch: [28][73/204]	Loss 0.0691 (0.0606)	
training:	Epoch: [28][74/204]	Loss 0.2453 (0.0631)	
training:	Epoch: [28][75/204]	Loss 0.0266 (0.0626)	
training:	Epoch: [28][76/204]	Loss 0.0403 (0.0623)	
training:	Epoch: [28][77/204]	Loss 0.0292 (0.0619)	
training:	Epoch: [28][78/204]	Loss 0.3173 (0.0652)	
training:	Epoch: [28][79/204]	Loss 0.0140 (0.0645)	
training:	Epoch: [28][80/204]	Loss 0.0207 (0.0640)	
training:	Epoch: [28][81/204]	Loss 0.0246 (0.0635)	
training:	Epoch: [28][82/204]	Loss 0.0167 (0.0629)	
training:	Epoch: [28][83/204]	Loss 0.1811 (0.0644)	
training:	Epoch: [28][84/204]	Loss 0.0263 (0.0639)	
training:	Epoch: [28][85/204]	Loss 0.1048 (0.0644)	
training:	Epoch: [28][86/204]	Loss 0.1300 (0.0652)	
training:	Epoch: [28][87/204]	Loss 0.1633 (0.0663)	
training:	Epoch: [28][88/204]	Loss 0.0434 (0.0660)	
training:	Epoch: [28][89/204]	Loss 0.1063 (0.0665)	
training:	Epoch: [28][90/204]	Loss 0.1667 (0.0676)	
training:	Epoch: [28][91/204]	Loss 0.0155 (0.0670)	
training:	Epoch: [28][92/204]	Loss 0.0414 (0.0667)	
training:	Epoch: [28][93/204]	Loss 0.0167 (0.0662)	
training:	Epoch: [28][94/204]	Loss 0.0299 (0.0658)	
training:	Epoch: [28][95/204]	Loss 0.0225 (0.0654)	
training:	Epoch: [28][96/204]	Loss 0.1158 (0.0659)	
training:	Epoch: [28][97/204]	Loss 0.0213 (0.0654)	
training:	Epoch: [28][98/204]	Loss 0.0647 (0.0654)	
training:	Epoch: [28][99/204]	Loss 0.0175 (0.0649)	
training:	Epoch: [28][100/204]	Loss 0.1449 (0.0657)	
training:	Epoch: [28][101/204]	Loss 0.1817 (0.0669)	
training:	Epoch: [28][102/204]	Loss 0.0431 (0.0666)	
training:	Epoch: [28][103/204]	Loss 0.0271 (0.0663)	
training:	Epoch: [28][104/204]	Loss 0.1202 (0.0668)	
training:	Epoch: [28][105/204]	Loss 0.1184 (0.0673)	
training:	Epoch: [28][106/204]	Loss 0.0215 (0.0668)	
training:	Epoch: [28][107/204]	Loss 0.0775 (0.0669)	
training:	Epoch: [28][108/204]	Loss 0.0280 (0.0666)	
training:	Epoch: [28][109/204]	Loss 0.0808 (0.0667)	
training:	Epoch: [28][110/204]	Loss 0.0679 (0.0667)	
training:	Epoch: [28][111/204]	Loss 0.0512 (0.0666)	
training:	Epoch: [28][112/204]	Loss 0.0166 (0.0661)	
training:	Epoch: [28][113/204]	Loss 0.0213 (0.0657)	
training:	Epoch: [28][114/204]	Loss 0.1468 (0.0664)	
training:	Epoch: [28][115/204]	Loss 0.0165 (0.0660)	
training:	Epoch: [28][116/204]	Loss 0.0409 (0.0658)	
training:	Epoch: [28][117/204]	Loss 0.0387 (0.0656)	
training:	Epoch: [28][118/204]	Loss 0.1560 (0.0663)	
training:	Epoch: [28][119/204]	Loss 0.1472 (0.0670)	
training:	Epoch: [28][120/204]	Loss 0.0151 (0.0666)	
training:	Epoch: [28][121/204]	Loss 0.0432 (0.0664)	
training:	Epoch: [28][122/204]	Loss 0.1352 (0.0670)	
training:	Epoch: [28][123/204]	Loss 0.0180 (0.0666)	
training:	Epoch: [28][124/204]	Loss 0.0215 (0.0662)	
training:	Epoch: [28][125/204]	Loss 0.0144 (0.0658)	
training:	Epoch: [28][126/204]	Loss 0.0488 (0.0656)	
training:	Epoch: [28][127/204]	Loss 0.2133 (0.0668)	
training:	Epoch: [28][128/204]	Loss 0.0160 (0.0664)	
training:	Epoch: [28][129/204]	Loss 0.0767 (0.0665)	
training:	Epoch: [28][130/204]	Loss 0.2484 (0.0679)	
training:	Epoch: [28][131/204]	Loss 0.0758 (0.0679)	
training:	Epoch: [28][132/204]	Loss 0.0737 (0.0680)	
training:	Epoch: [28][133/204]	Loss 0.1346 (0.0685)	
training:	Epoch: [28][134/204]	Loss 0.0141 (0.0681)	
training:	Epoch: [28][135/204]	Loss 0.2378 (0.0693)	
training:	Epoch: [28][136/204]	Loss 0.0446 (0.0692)	
training:	Epoch: [28][137/204]	Loss 0.0251 (0.0688)	
training:	Epoch: [28][138/204]	Loss 0.0156 (0.0685)	
training:	Epoch: [28][139/204]	Loss 0.0163 (0.0681)	
training:	Epoch: [28][140/204]	Loss 0.0273 (0.0678)	
training:	Epoch: [28][141/204]	Loss 0.0679 (0.0678)	
training:	Epoch: [28][142/204]	Loss 0.0261 (0.0675)	
training:	Epoch: [28][143/204]	Loss 0.1596 (0.0681)	
training:	Epoch: [28][144/204]	Loss 0.0214 (0.0678)	
training:	Epoch: [28][145/204]	Loss 0.0228 (0.0675)	
training:	Epoch: [28][146/204]	Loss 0.1037 (0.0678)	
training:	Epoch: [28][147/204]	Loss 0.0367 (0.0675)	
training:	Epoch: [28][148/204]	Loss 0.0631 (0.0675)	
training:	Epoch: [28][149/204]	Loss 0.2514 (0.0687)	
training:	Epoch: [28][150/204]	Loss 0.0181 (0.0684)	
training:	Epoch: [28][151/204]	Loss 0.0316 (0.0682)	
training:	Epoch: [28][152/204]	Loss 0.0196 (0.0678)	
training:	Epoch: [28][153/204]	Loss 0.0149 (0.0675)	
training:	Epoch: [28][154/204]	Loss 0.0284 (0.0672)	
training:	Epoch: [28][155/204]	Loss 0.0196 (0.0669)	
training:	Epoch: [28][156/204]	Loss 0.2004 (0.0678)	
training:	Epoch: [28][157/204]	Loss 0.0167 (0.0675)	
training:	Epoch: [28][158/204]	Loss 0.0633 (0.0674)	
training:	Epoch: [28][159/204]	Loss 0.0187 (0.0671)	
training:	Epoch: [28][160/204]	Loss 0.0201 (0.0668)	
training:	Epoch: [28][161/204]	Loss 0.0156 (0.0665)	
training:	Epoch: [28][162/204]	Loss 0.1592 (0.0671)	
training:	Epoch: [28][163/204]	Loss 0.0179 (0.0668)	
training:	Epoch: [28][164/204]	Loss 0.0230 (0.0665)	
training:	Epoch: [28][165/204]	Loss 0.0200 (0.0662)	
training:	Epoch: [28][166/204]	Loss 0.0263 (0.0660)	
training:	Epoch: [28][167/204]	Loss 0.0244 (0.0658)	
training:	Epoch: [28][168/204]	Loss 0.1703 (0.0664)	
training:	Epoch: [28][169/204]	Loss 0.1210 (0.0667)	
training:	Epoch: [28][170/204]	Loss 0.0265 (0.0665)	
training:	Epoch: [28][171/204]	Loss 0.0445 (0.0663)	
training:	Epoch: [28][172/204]	Loss 0.0820 (0.0664)	
training:	Epoch: [28][173/204]	Loss 0.0124 (0.0661)	
training:	Epoch: [28][174/204]	Loss 0.0235 (0.0659)	
training:	Epoch: [28][175/204]	Loss 0.1869 (0.0666)	
training:	Epoch: [28][176/204]	Loss 0.0180 (0.0663)	
training:	Epoch: [28][177/204]	Loss 0.0186 (0.0660)	
training:	Epoch: [28][178/204]	Loss 0.0131 (0.0657)	
training:	Epoch: [28][179/204]	Loss 0.0198 (0.0655)	
training:	Epoch: [28][180/204]	Loss 0.2843 (0.0667)	
training:	Epoch: [28][181/204]	Loss 0.0297 (0.0665)	
training:	Epoch: [28][182/204]	Loss 0.2033 (0.0672)	
training:	Epoch: [28][183/204]	Loss 0.0702 (0.0672)	
training:	Epoch: [28][184/204]	Loss 0.1331 (0.0676)	
training:	Epoch: [28][185/204]	Loss 0.0488 (0.0675)	
training:	Epoch: [28][186/204]	Loss 0.1885 (0.0681)	
training:	Epoch: [28][187/204]	Loss 0.0191 (0.0679)	
training:	Epoch: [28][188/204]	Loss 0.1079 (0.0681)	
training:	Epoch: [28][189/204]	Loss 0.1593 (0.0686)	
training:	Epoch: [28][190/204]	Loss 0.0142 (0.0683)	
training:	Epoch: [28][191/204]	Loss 0.0212 (0.0680)	
training:	Epoch: [28][192/204]	Loss 0.0232 (0.0678)	
training:	Epoch: [28][193/204]	Loss 0.0179 (0.0676)	
training:	Epoch: [28][194/204]	Loss 0.1318 (0.0679)	
training:	Epoch: [28][195/204]	Loss 0.1543 (0.0683)	
training:	Epoch: [28][196/204]	Loss 0.1494 (0.0687)	
training:	Epoch: [28][197/204]	Loss 0.0175 (0.0685)	
training:	Epoch: [28][198/204]	Loss 0.0448 (0.0684)	
training:	Epoch: [28][199/204]	Loss 0.1707 (0.0689)	
training:	Epoch: [28][200/204]	Loss 0.0184 (0.0686)	
training:	Epoch: [28][201/204]	Loss 0.1514 (0.0690)	
training:	Epoch: [28][202/204]	Loss 0.0176 (0.0688)	
training:	Epoch: [28][203/204]	Loss 0.1838 (0.0693)	
training:	Epoch: [28][204/204]	Loss 0.0132 (0.0691)	
Training:	 Loss: 0.0690

Training:	 ACC: 0.9926 0.9927 0.9938 0.9914
Validation:	 ACC: 0.8082 0.8090 0.8250 0.7915
Validation:	 Best_BACC: 0.8326 0.8331 0.8424 0.8229
Validation:	 Loss: 0.6586
Pretraining:	Epoch 29/200
----------
training:	Epoch: [29][1/204]	Loss 0.0166 (0.0166)	
training:	Epoch: [29][2/204]	Loss 0.1105 (0.0635)	
training:	Epoch: [29][3/204]	Loss 0.0174 (0.0482)	
training:	Epoch: [29][4/204]	Loss 0.0402 (0.0462)	
training:	Epoch: [29][5/204]	Loss 0.0198 (0.0409)	
training:	Epoch: [29][6/204]	Loss 0.0122 (0.0361)	
training:	Epoch: [29][7/204]	Loss 0.1523 (0.0527)	
training:	Epoch: [29][8/204]	Loss 0.0196 (0.0486)	
training:	Epoch: [29][9/204]	Loss 0.0211 (0.0455)	
training:	Epoch: [29][10/204]	Loss 0.0161 (0.0426)	
training:	Epoch: [29][11/204]	Loss 0.0249 (0.0410)	
training:	Epoch: [29][12/204]	Loss 0.1883 (0.0533)	
training:	Epoch: [29][13/204]	Loss 0.0175 (0.0505)	
training:	Epoch: [29][14/204]	Loss 0.0118 (0.0477)	
training:	Epoch: [29][15/204]	Loss 0.0655 (0.0489)	
training:	Epoch: [29][16/204]	Loss 0.0358 (0.0481)	
training:	Epoch: [29][17/204]	Loss 0.0227 (0.0466)	
training:	Epoch: [29][18/204]	Loss 0.0203 (0.0452)	
training:	Epoch: [29][19/204]	Loss 0.0269 (0.0442)	
training:	Epoch: [29][20/204]	Loss 0.0320 (0.0436)	
training:	Epoch: [29][21/204]	Loss 0.1670 (0.0495)	
training:	Epoch: [29][22/204]	Loss 0.2433 (0.0583)	
training:	Epoch: [29][23/204]	Loss 0.0163 (0.0564)	
training:	Epoch: [29][24/204]	Loss 0.0338 (0.0555)	
training:	Epoch: [29][25/204]	Loss 0.0833 (0.0566)	
training:	Epoch: [29][26/204]	Loss 0.0216 (0.0553)	
training:	Epoch: [29][27/204]	Loss 0.1713 (0.0596)	
training:	Epoch: [29][28/204]	Loss 0.0260 (0.0584)	
training:	Epoch: [29][29/204]	Loss 0.1856 (0.0628)	
training:	Epoch: [29][30/204]	Loss 0.1388 (0.0653)	
training:	Epoch: [29][31/204]	Loss 0.1407 (0.0677)	
training:	Epoch: [29][32/204]	Loss 0.0372 (0.0668)	
training:	Epoch: [29][33/204]	Loss 0.0310 (0.0657)	
training:	Epoch: [29][34/204]	Loss 0.1158 (0.0672)	
training:	Epoch: [29][35/204]	Loss 0.0659 (0.0671)	
training:	Epoch: [29][36/204]	Loss 0.0143 (0.0657)	
training:	Epoch: [29][37/204]	Loss 0.0222 (0.0645)	
training:	Epoch: [29][38/204]	Loss 0.0879 (0.0651)	
training:	Epoch: [29][39/204]	Loss 0.0221 (0.0640)	
training:	Epoch: [29][40/204]	Loss 0.0167 (0.0628)	
training:	Epoch: [29][41/204]	Loss 0.0376 (0.0622)	
training:	Epoch: [29][42/204]	Loss 0.1886 (0.0652)	
training:	Epoch: [29][43/204]	Loss 0.0224 (0.0642)	
training:	Epoch: [29][44/204]	Loss 0.1542 (0.0663)	
training:	Epoch: [29][45/204]	Loss 0.1517 (0.0682)	
training:	Epoch: [29][46/204]	Loss 0.2141 (0.0713)	
training:	Epoch: [29][47/204]	Loss 0.0239 (0.0703)	
training:	Epoch: [29][48/204]	Loss 0.0219 (0.0693)	
training:	Epoch: [29][49/204]	Loss 0.0129 (0.0682)	
training:	Epoch: [29][50/204]	Loss 0.0683 (0.0682)	
training:	Epoch: [29][51/204]	Loss 0.1636 (0.0700)	
training:	Epoch: [29][52/204]	Loss 0.0348 (0.0694)	
training:	Epoch: [29][53/204]	Loss 0.0217 (0.0685)	
training:	Epoch: [29][54/204]	Loss 0.0127 (0.0674)	
training:	Epoch: [29][55/204]	Loss 0.0125 (0.0664)	
training:	Epoch: [29][56/204]	Loss 0.0170 (0.0655)	
training:	Epoch: [29][57/204]	Loss 0.1537 (0.0671)	
training:	Epoch: [29][58/204]	Loss 0.0465 (0.0667)	
training:	Epoch: [29][59/204]	Loss 0.0168 (0.0659)	
training:	Epoch: [29][60/204]	Loss 0.1600 (0.0675)	
training:	Epoch: [29][61/204]	Loss 0.0863 (0.0678)	
training:	Epoch: [29][62/204]	Loss 0.0139 (0.0669)	
training:	Epoch: [29][63/204]	Loss 0.1424 (0.0681)	
training:	Epoch: [29][64/204]	Loss 0.0178 (0.0673)	
training:	Epoch: [29][65/204]	Loss 0.0154 (0.0665)	
training:	Epoch: [29][66/204]	Loss 0.0145 (0.0657)	
training:	Epoch: [29][67/204]	Loss 0.0430 (0.0654)	
training:	Epoch: [29][68/204]	Loss 0.0243 (0.0648)	
training:	Epoch: [29][69/204]	Loss 0.0126 (0.0640)	
training:	Epoch: [29][70/204]	Loss 0.1619 (0.0654)	
training:	Epoch: [29][71/204]	Loss 0.0177 (0.0647)	
training:	Epoch: [29][72/204]	Loss 0.0365 (0.0644)	
training:	Epoch: [29][73/204]	Loss 0.0265 (0.0638)	
training:	Epoch: [29][74/204]	Loss 0.0359 (0.0635)	
training:	Epoch: [29][75/204]	Loss 0.0242 (0.0629)	
training:	Epoch: [29][76/204]	Loss 0.0153 (0.0623)	
training:	Epoch: [29][77/204]	Loss 0.0343 (0.0619)	
training:	Epoch: [29][78/204]	Loss 0.0169 (0.0614)	
training:	Epoch: [29][79/204]	Loss 0.0148 (0.0608)	
training:	Epoch: [29][80/204]	Loss 0.0230 (0.0603)	
training:	Epoch: [29][81/204]	Loss 0.1697 (0.0617)	
training:	Epoch: [29][82/204]	Loss 0.0128 (0.0611)	
training:	Epoch: [29][83/204]	Loss 0.0148 (0.0605)	
training:	Epoch: [29][84/204]	Loss 0.0377 (0.0602)	
training:	Epoch: [29][85/204]	Loss 0.0241 (0.0598)	
training:	Epoch: [29][86/204]	Loss 0.0156 (0.0593)	
training:	Epoch: [29][87/204]	Loss 0.2091 (0.0610)	
training:	Epoch: [29][88/204]	Loss 0.0162 (0.0605)	
training:	Epoch: [29][89/204]	Loss 0.0291 (0.0602)	
training:	Epoch: [29][90/204]	Loss 0.0957 (0.0605)	
training:	Epoch: [29][91/204]	Loss 0.0122 (0.0600)	
training:	Epoch: [29][92/204]	Loss 0.0150 (0.0595)	
training:	Epoch: [29][93/204]	Loss 0.0147 (0.0590)	
training:	Epoch: [29][94/204]	Loss 0.0207 (0.0586)	
training:	Epoch: [29][95/204]	Loss 0.0148 (0.0582)	
training:	Epoch: [29][96/204]	Loss 0.1799 (0.0594)	
training:	Epoch: [29][97/204]	Loss 0.1592 (0.0605)	
training:	Epoch: [29][98/204]	Loss 0.0203 (0.0601)	
training:	Epoch: [29][99/204]	Loss 0.0463 (0.0599)	
training:	Epoch: [29][100/204]	Loss 0.0144 (0.0595)	
training:	Epoch: [29][101/204]	Loss 0.0349 (0.0592)	
training:	Epoch: [29][102/204]	Loss 0.1201 (0.0598)	
training:	Epoch: [29][103/204]	Loss 0.0768 (0.0600)	
training:	Epoch: [29][104/204]	Loss 0.0169 (0.0596)	
training:	Epoch: [29][105/204]	Loss 0.0803 (0.0598)	
training:	Epoch: [29][106/204]	Loss 0.2844 (0.0619)	
training:	Epoch: [29][107/204]	Loss 0.0365 (0.0617)	
training:	Epoch: [29][108/204]	Loss 0.0131 (0.0612)	
training:	Epoch: [29][109/204]	Loss 0.0642 (0.0612)	
training:	Epoch: [29][110/204]	Loss 0.0303 (0.0609)	
training:	Epoch: [29][111/204]	Loss 0.0162 (0.0605)	
training:	Epoch: [29][112/204]	Loss 0.0325 (0.0603)	
training:	Epoch: [29][113/204]	Loss 0.0239 (0.0600)	
training:	Epoch: [29][114/204]	Loss 0.0260 (0.0597)	
training:	Epoch: [29][115/204]	Loss 0.0250 (0.0594)	
training:	Epoch: [29][116/204]	Loss 0.1292 (0.0600)	
training:	Epoch: [29][117/204]	Loss 0.0436 (0.0598)	
training:	Epoch: [29][118/204]	Loss 0.0143 (0.0594)	
training:	Epoch: [29][119/204]	Loss 0.1395 (0.0601)	
training:	Epoch: [29][120/204]	Loss 0.0282 (0.0599)	
training:	Epoch: [29][121/204]	Loss 0.0172 (0.0595)	
training:	Epoch: [29][122/204]	Loss 0.0152 (0.0591)	
training:	Epoch: [29][123/204]	Loss 0.0127 (0.0588)	
training:	Epoch: [29][124/204]	Loss 0.0358 (0.0586)	
training:	Epoch: [29][125/204]	Loss 0.0531 (0.0585)	
training:	Epoch: [29][126/204]	Loss 0.0235 (0.0583)	
training:	Epoch: [29][127/204]	Loss 0.0122 (0.0579)	
training:	Epoch: [29][128/204]	Loss 0.0203 (0.0576)	
training:	Epoch: [29][129/204]	Loss 0.0668 (0.0577)	
training:	Epoch: [29][130/204]	Loss 0.0228 (0.0574)	
training:	Epoch: [29][131/204]	Loss 0.0406 (0.0573)	
training:	Epoch: [29][132/204]	Loss 0.0194 (0.0570)	
training:	Epoch: [29][133/204]	Loss 0.0449 (0.0569)	
training:	Epoch: [29][134/204]	Loss 0.0237 (0.0566)	
training:	Epoch: [29][135/204]	Loss 0.0352 (0.0565)	
training:	Epoch: [29][136/204]	Loss 0.2484 (0.0579)	
training:	Epoch: [29][137/204]	Loss 0.0178 (0.0576)	
training:	Epoch: [29][138/204]	Loss 0.0188 (0.0573)	
training:	Epoch: [29][139/204]	Loss 0.0096 (0.0570)	
training:	Epoch: [29][140/204]	Loss 0.0591 (0.0570)	
training:	Epoch: [29][141/204]	Loss 0.0496 (0.0569)	
training:	Epoch: [29][142/204]	Loss 0.0258 (0.0567)	
training:	Epoch: [29][143/204]	Loss 0.1894 (0.0577)	
training:	Epoch: [29][144/204]	Loss 0.0342 (0.0575)	
training:	Epoch: [29][145/204]	Loss 0.0368 (0.0573)	
training:	Epoch: [29][146/204]	Loss 0.0246 (0.0571)	
training:	Epoch: [29][147/204]	Loss 0.1312 (0.0576)	
training:	Epoch: [29][148/204]	Loss 0.1573 (0.0583)	
training:	Epoch: [29][149/204]	Loss 0.1416 (0.0589)	
training:	Epoch: [29][150/204]	Loss 0.0158 (0.0586)	
training:	Epoch: [29][151/204]	Loss 0.1478 (0.0592)	
training:	Epoch: [29][152/204]	Loss 0.0177 (0.0589)	
training:	Epoch: [29][153/204]	Loss 0.0178 (0.0586)	
training:	Epoch: [29][154/204]	Loss 0.0159 (0.0583)	
training:	Epoch: [29][155/204]	Loss 0.1732 (0.0591)	
training:	Epoch: [29][156/204]	Loss 0.0134 (0.0588)	
training:	Epoch: [29][157/204]	Loss 0.0442 (0.0587)	
training:	Epoch: [29][158/204]	Loss 0.0507 (0.0586)	
training:	Epoch: [29][159/204]	Loss 0.0127 (0.0584)	
training:	Epoch: [29][160/204]	Loss 0.0174 (0.0581)	
training:	Epoch: [29][161/204]	Loss 0.0140 (0.0578)	
training:	Epoch: [29][162/204]	Loss 0.2835 (0.0592)	
training:	Epoch: [29][163/204]	Loss 0.0124 (0.0589)	
training:	Epoch: [29][164/204]	Loss 0.0357 (0.0588)	
training:	Epoch: [29][165/204]	Loss 0.0222 (0.0586)	
training:	Epoch: [29][166/204]	Loss 0.0180 (0.0583)	
training:	Epoch: [29][167/204]	Loss 0.0182 (0.0581)	
training:	Epoch: [29][168/204]	Loss 0.0191 (0.0579)	
training:	Epoch: [29][169/204]	Loss 0.3006 (0.0593)	
training:	Epoch: [29][170/204]	Loss 0.0208 (0.0591)	
training:	Epoch: [29][171/204]	Loss 0.0149 (0.0588)	
training:	Epoch: [29][172/204]	Loss 0.0118 (0.0585)	
training:	Epoch: [29][173/204]	Loss 0.0425 (0.0584)	
training:	Epoch: [29][174/204]	Loss 0.0128 (0.0582)	
training:	Epoch: [29][175/204]	Loss 0.1097 (0.0585)	
training:	Epoch: [29][176/204]	Loss 0.0154 (0.0582)	
training:	Epoch: [29][177/204]	Loss 0.0431 (0.0581)	
training:	Epoch: [29][178/204]	Loss 0.0102 (0.0579)	
training:	Epoch: [29][179/204]	Loss 0.1744 (0.0585)	
training:	Epoch: [29][180/204]	Loss 0.1591 (0.0591)	
training:	Epoch: [29][181/204]	Loss 0.2165 (0.0600)	
training:	Epoch: [29][182/204]	Loss 0.0119 (0.0597)	
training:	Epoch: [29][183/204]	Loss 0.0592 (0.0597)	
training:	Epoch: [29][184/204]	Loss 0.0950 (0.0599)	
training:	Epoch: [29][185/204]	Loss 0.0742 (0.0600)	
training:	Epoch: [29][186/204]	Loss 0.0143 (0.0597)	
training:	Epoch: [29][187/204]	Loss 0.1564 (0.0602)	
training:	Epoch: [29][188/204]	Loss 0.0268 (0.0600)	
training:	Epoch: [29][189/204]	Loss 0.1639 (0.0606)	
training:	Epoch: [29][190/204]	Loss 0.1942 (0.0613)	
training:	Epoch: [29][191/204]	Loss 0.0140 (0.0611)	
training:	Epoch: [29][192/204]	Loss 0.0164 (0.0608)	
training:	Epoch: [29][193/204]	Loss 0.0488 (0.0608)	
training:	Epoch: [29][194/204]	Loss 0.0147 (0.0605)	
training:	Epoch: [29][195/204]	Loss 0.0148 (0.0603)	
training:	Epoch: [29][196/204]	Loss 0.1053 (0.0605)	
training:	Epoch: [29][197/204]	Loss 0.1144 (0.0608)	
training:	Epoch: [29][198/204]	Loss 0.0770 (0.0609)	
training:	Epoch: [29][199/204]	Loss 0.0148 (0.0606)	
training:	Epoch: [29][200/204]	Loss 0.2603 (0.0616)	
training:	Epoch: [29][201/204]	Loss 0.0132 (0.0614)	
training:	Epoch: [29][202/204]	Loss 0.0111 (0.0611)	
training:	Epoch: [29][203/204]	Loss 0.0222 (0.0610)	
training:	Epoch: [29][204/204]	Loss 0.1096 (0.0612)	
Training:	 Loss: 0.0611

Training:	 ACC: 0.9931 0.9931 0.9941 0.9920
Validation:	 ACC: 0.8034 0.8026 0.7851 0.8217
Validation:	 Best_BACC: 0.8326 0.8331 0.8424 0.8229
Validation:	 Loss: 0.6764
Pretraining:	Epoch 30/200
----------
training:	Epoch: [30][1/204]	Loss 0.0360 (0.0360)	
training:	Epoch: [30][2/204]	Loss 0.0181 (0.0271)	
training:	Epoch: [30][3/204]	Loss 0.0760 (0.0434)	
training:	Epoch: [30][4/204]	Loss 0.0288 (0.0397)	
training:	Epoch: [30][5/204]	Loss 0.1078 (0.0533)	
training:	Epoch: [30][6/204]	Loss 0.1226 (0.0649)	
training:	Epoch: [30][7/204]	Loss 0.0118 (0.0573)	
training:	Epoch: [30][8/204]	Loss 0.0102 (0.0514)	
training:	Epoch: [30][9/204]	Loss 0.0116 (0.0470)	
training:	Epoch: [30][10/204]	Loss 0.0248 (0.0448)	
training:	Epoch: [30][11/204]	Loss 0.0108 (0.0417)	
training:	Epoch: [30][12/204]	Loss 0.0556 (0.0428)	
training:	Epoch: [30][13/204]	Loss 0.0231 (0.0413)	
training:	Epoch: [30][14/204]	Loss 0.0156 (0.0395)	
training:	Epoch: [30][15/204]	Loss 0.0234 (0.0384)	
training:	Epoch: [30][16/204]	Loss 0.0148 (0.0369)	
training:	Epoch: [30][17/204]	Loss 0.0136 (0.0356)	
training:	Epoch: [30][18/204]	Loss 0.0785 (0.0379)	
training:	Epoch: [30][19/204]	Loss 0.0347 (0.0378)	
training:	Epoch: [30][20/204]	Loss 0.1761 (0.0447)	
training:	Epoch: [30][21/204]	Loss 0.1672 (0.0505)	
training:	Epoch: [30][22/204]	Loss 0.0151 (0.0489)	
training:	Epoch: [30][23/204]	Loss 0.1077 (0.0515)	
training:	Epoch: [30][24/204]	Loss 0.1445 (0.0553)	
training:	Epoch: [30][25/204]	Loss 0.1265 (0.0582)	
training:	Epoch: [30][26/204]	Loss 0.0143 (0.0565)	
training:	Epoch: [30][27/204]	Loss 0.0150 (0.0550)	
training:	Epoch: [30][28/204]	Loss 0.0161 (0.0536)	
training:	Epoch: [30][29/204]	Loss 0.0651 (0.0540)	
training:	Epoch: [30][30/204]	Loss 0.0139 (0.0526)	
training:	Epoch: [30][31/204]	Loss 0.0102 (0.0513)	
training:	Epoch: [30][32/204]	Loss 0.0175 (0.0502)	
training:	Epoch: [30][33/204]	Loss 0.0206 (0.0493)	
training:	Epoch: [30][34/204]	Loss 0.0156 (0.0483)	
training:	Epoch: [30][35/204]	Loss 0.0211 (0.0475)	
training:	Epoch: [30][36/204]	Loss 0.0202 (0.0468)	
training:	Epoch: [30][37/204]	Loss 0.0232 (0.0461)	
training:	Epoch: [30][38/204]	Loss 0.0142 (0.0453)	
training:	Epoch: [30][39/204]	Loss 0.0128 (0.0445)	
training:	Epoch: [30][40/204]	Loss 0.0235 (0.0439)	
training:	Epoch: [30][41/204]	Loss 0.1641 (0.0469)	
training:	Epoch: [30][42/204]	Loss 0.0176 (0.0462)	
training:	Epoch: [30][43/204]	Loss 0.0256 (0.0457)	
training:	Epoch: [30][44/204]	Loss 0.0132 (0.0450)	
training:	Epoch: [30][45/204]	Loss 0.0477 (0.0450)	
training:	Epoch: [30][46/204]	Loss 0.0147 (0.0444)	
training:	Epoch: [30][47/204]	Loss 0.0147 (0.0437)	
training:	Epoch: [30][48/204]	Loss 0.0207 (0.0433)	
training:	Epoch: [30][49/204]	Loss 0.0152 (0.0427)	
training:	Epoch: [30][50/204]	Loss 0.0188 (0.0422)	
training:	Epoch: [30][51/204]	Loss 0.0292 (0.0420)	
training:	Epoch: [30][52/204]	Loss 0.0911 (0.0429)	
training:	Epoch: [30][53/204]	Loss 0.2086 (0.0460)	
training:	Epoch: [30][54/204]	Loss 0.0228 (0.0456)	
training:	Epoch: [30][55/204]	Loss 0.0280 (0.0453)	
training:	Epoch: [30][56/204]	Loss 0.0820 (0.0459)	
training:	Epoch: [30][57/204]	Loss 0.0141 (0.0454)	
training:	Epoch: [30][58/204]	Loss 0.0145 (0.0448)	
training:	Epoch: [30][59/204]	Loss 0.0107 (0.0443)	
training:	Epoch: [30][60/204]	Loss 0.0138 (0.0438)	
training:	Epoch: [30][61/204]	Loss 0.0308 (0.0435)	
training:	Epoch: [30][62/204]	Loss 0.1537 (0.0453)	
training:	Epoch: [30][63/204]	Loss 0.0227 (0.0450)	
training:	Epoch: [30][64/204]	Loss 0.0599 (0.0452)	
training:	Epoch: [30][65/204]	Loss 0.0100 (0.0446)	
training:	Epoch: [30][66/204]	Loss 0.0140 (0.0442)	
training:	Epoch: [30][67/204]	Loss 0.0154 (0.0438)	
training:	Epoch: [30][68/204]	Loss 0.1602 (0.0455)	
training:	Epoch: [30][69/204]	Loss 0.1738 (0.0473)	
training:	Epoch: [30][70/204]	Loss 0.0165 (0.0469)	
training:	Epoch: [30][71/204]	Loss 0.0171 (0.0465)	
training:	Epoch: [30][72/204]	Loss 0.0152 (0.0460)	
training:	Epoch: [30][73/204]	Loss 0.0185 (0.0457)	
training:	Epoch: [30][74/204]	Loss 0.0101 (0.0452)	
training:	Epoch: [30][75/204]	Loss 0.0205 (0.0448)	
training:	Epoch: [30][76/204]	Loss 0.0185 (0.0445)	
training:	Epoch: [30][77/204]	Loss 0.0258 (0.0443)	
training:	Epoch: [30][78/204]	Loss 0.0182 (0.0439)	
training:	Epoch: [30][79/204]	Loss 0.0102 (0.0435)	
training:	Epoch: [30][80/204]	Loss 0.0358 (0.0434)	
training:	Epoch: [30][81/204]	Loss 0.1650 (0.0449)	
training:	Epoch: [30][82/204]	Loss 0.0107 (0.0445)	
training:	Epoch: [30][83/204]	Loss 0.1667 (0.0460)	
training:	Epoch: [30][84/204]	Loss 0.0213 (0.0457)	
training:	Epoch: [30][85/204]	Loss 0.0127 (0.0453)	
training:	Epoch: [30][86/204]	Loss 0.0152 (0.0449)	
training:	Epoch: [30][87/204]	Loss 0.0893 (0.0454)	
training:	Epoch: [30][88/204]	Loss 0.1069 (0.0461)	
training:	Epoch: [30][89/204]	Loss 0.0163 (0.0458)	
training:	Epoch: [30][90/204]	Loss 0.1123 (0.0465)	
training:	Epoch: [30][91/204]	Loss 0.0117 (0.0462)	
training:	Epoch: [30][92/204]	Loss 0.1741 (0.0475)	
training:	Epoch: [30][93/204]	Loss 0.0342 (0.0474)	
training:	Epoch: [30][94/204]	Loss 0.0322 (0.0472)	
training:	Epoch: [30][95/204]	Loss 0.0143 (0.0469)	
training:	Epoch: [30][96/204]	Loss 0.0164 (0.0466)	
training:	Epoch: [30][97/204]	Loss 0.0180 (0.0463)	
training:	Epoch: [30][98/204]	Loss 0.1669 (0.0475)	
training:	Epoch: [30][99/204]	Loss 0.0170 (0.0472)	
training:	Epoch: [30][100/204]	Loss 0.0326 (0.0471)	
training:	Epoch: [30][101/204]	Loss 0.0180 (0.0468)	
training:	Epoch: [30][102/204]	Loss 0.0178 (0.0465)	
training:	Epoch: [30][103/204]	Loss 0.0151 (0.0462)	
training:	Epoch: [30][104/204]	Loss 0.0145 (0.0459)	
training:	Epoch: [30][105/204]	Loss 0.0211 (0.0456)	
training:	Epoch: [30][106/204]	Loss 0.0483 (0.0457)	
training:	Epoch: [30][107/204]	Loss 0.0628 (0.0458)	
training:	Epoch: [30][108/204]	Loss 0.0756 (0.0461)	
training:	Epoch: [30][109/204]	Loss 0.0112 (0.0458)	
training:	Epoch: [30][110/204]	Loss 0.0997 (0.0463)	
training:	Epoch: [30][111/204]	Loss 0.0925 (0.0467)	
training:	Epoch: [30][112/204]	Loss 0.0108 (0.0464)	
training:	Epoch: [30][113/204]	Loss 0.0128 (0.0461)	
training:	Epoch: [30][114/204]	Loss 0.0519 (0.0461)	
training:	Epoch: [30][115/204]	Loss 0.0592 (0.0462)	
training:	Epoch: [30][116/204]	Loss 0.0141 (0.0460)	
training:	Epoch: [30][117/204]	Loss 0.0238 (0.0458)	
training:	Epoch: [30][118/204]	Loss 0.0129 (0.0455)	
training:	Epoch: [30][119/204]	Loss 0.1669 (0.0465)	
training:	Epoch: [30][120/204]	Loss 0.0408 (0.0465)	
training:	Epoch: [30][121/204]	Loss 0.0172 (0.0462)	
training:	Epoch: [30][122/204]	Loss 0.0721 (0.0464)	
training:	Epoch: [30][123/204]	Loss 0.0618 (0.0466)	
training:	Epoch: [30][124/204]	Loss 0.1617 (0.0475)	
training:	Epoch: [30][125/204]	Loss 0.0415 (0.0474)	
training:	Epoch: [30][126/204]	Loss 0.0243 (0.0473)	
training:	Epoch: [30][127/204]	Loss 0.1085 (0.0477)	
training:	Epoch: [30][128/204]	Loss 0.1315 (0.0484)	
training:	Epoch: [30][129/204]	Loss 0.0117 (0.0481)	
training:	Epoch: [30][130/204]	Loss 0.0125 (0.0478)	
training:	Epoch: [30][131/204]	Loss 0.0107 (0.0475)	
training:	Epoch: [30][132/204]	Loss 0.0119 (0.0473)	
training:	Epoch: [30][133/204]	Loss 0.0221 (0.0471)	
training:	Epoch: [30][134/204]	Loss 0.1620 (0.0479)	
training:	Epoch: [30][135/204]	Loss 0.1615 (0.0488)	
training:	Epoch: [30][136/204]	Loss 0.0769 (0.0490)	
training:	Epoch: [30][137/204]	Loss 0.0248 (0.0488)	
training:	Epoch: [30][138/204]	Loss 0.1538 (0.0496)	
training:	Epoch: [30][139/204]	Loss 0.1374 (0.0502)	
training:	Epoch: [30][140/204]	Loss 0.1268 (0.0508)	
training:	Epoch: [30][141/204]	Loss 0.0143 (0.0505)	
training:	Epoch: [30][142/204]	Loss 0.1285 (0.0510)	
training:	Epoch: [30][143/204]	Loss 0.0329 (0.0509)	
training:	Epoch: [30][144/204]	Loss 0.1492 (0.0516)	
training:	Epoch: [30][145/204]	Loss 0.0820 (0.0518)	
training:	Epoch: [30][146/204]	Loss 0.2176 (0.0529)	
training:	Epoch: [30][147/204]	Loss 0.1636 (0.0537)	
training:	Epoch: [30][148/204]	Loss 0.1671 (0.0545)	
training:	Epoch: [30][149/204]	Loss 0.0297 (0.0543)	
training:	Epoch: [30][150/204]	Loss 0.0228 (0.0541)	
training:	Epoch: [30][151/204]	Loss 0.1596 (0.0548)	
training:	Epoch: [30][152/204]	Loss 0.0838 (0.0550)	
training:	Epoch: [30][153/204]	Loss 0.0203 (0.0548)	
training:	Epoch: [30][154/204]	Loss 0.1081 (0.0551)	
training:	Epoch: [30][155/204]	Loss 0.0275 (0.0549)	
training:	Epoch: [30][156/204]	Loss 0.0111 (0.0546)	
training:	Epoch: [30][157/204]	Loss 0.0182 (0.0544)	
training:	Epoch: [30][158/204]	Loss 0.1478 (0.0550)	
training:	Epoch: [30][159/204]	Loss 0.2874 (0.0565)	
training:	Epoch: [30][160/204]	Loss 0.0250 (0.0563)	
training:	Epoch: [30][161/204]	Loss 0.0354 (0.0561)	
training:	Epoch: [30][162/204]	Loss 0.0143 (0.0559)	
training:	Epoch: [30][163/204]	Loss 0.1510 (0.0565)	
training:	Epoch: [30][164/204]	Loss 0.0232 (0.0563)	
training:	Epoch: [30][165/204]	Loss 0.0130 (0.0560)	
training:	Epoch: [30][166/204]	Loss 0.1483 (0.0566)	
training:	Epoch: [30][167/204]	Loss 0.0121 (0.0563)	
training:	Epoch: [30][168/204]	Loss 0.0156 (0.0560)	
training:	Epoch: [30][169/204]	Loss 0.1639 (0.0567)	
training:	Epoch: [30][170/204]	Loss 0.1792 (0.0574)	
training:	Epoch: [30][171/204]	Loss 0.0218 (0.0572)	
training:	Epoch: [30][172/204]	Loss 0.1230 (0.0576)	
training:	Epoch: [30][173/204]	Loss 0.1543 (0.0581)	
training:	Epoch: [30][174/204]	Loss 0.0115 (0.0579)	
training:	Epoch: [30][175/204]	Loss 0.0116 (0.0576)	
training:	Epoch: [30][176/204]	Loss 0.0151 (0.0574)	
training:	Epoch: [30][177/204]	Loss 0.0310 (0.0572)	
training:	Epoch: [30][178/204]	Loss 0.0121 (0.0570)	
training:	Epoch: [30][179/204]	Loss 0.1420 (0.0574)	
training:	Epoch: [30][180/204]	Loss 0.1665 (0.0580)	
training:	Epoch: [30][181/204]	Loss 0.0180 (0.0578)	
training:	Epoch: [30][182/204]	Loss 0.0389 (0.0577)	
training:	Epoch: [30][183/204]	Loss 0.1586 (0.0583)	
training:	Epoch: [30][184/204]	Loss 0.0143 (0.0580)	
training:	Epoch: [30][185/204]	Loss 0.0096 (0.0578)	
training:	Epoch: [30][186/204]	Loss 0.0274 (0.0576)	
training:	Epoch: [30][187/204]	Loss 0.0123 (0.0574)	
training:	Epoch: [30][188/204]	Loss 0.0422 (0.0573)	
training:	Epoch: [30][189/204]	Loss 0.0520 (0.0573)	
training:	Epoch: [30][190/204]	Loss 0.0132 (0.0570)	
training:	Epoch: [30][191/204]	Loss 0.0144 (0.0568)	
training:	Epoch: [30][192/204]	Loss 0.0215 (0.0566)	
training:	Epoch: [30][193/204]	Loss 0.0186 (0.0564)	
training:	Epoch: [30][194/204]	Loss 0.0162 (0.0562)	
training:	Epoch: [30][195/204]	Loss 0.0252 (0.0561)	
training:	Epoch: [30][196/204]	Loss 0.0589 (0.0561)	
training:	Epoch: [30][197/204]	Loss 0.0136 (0.0559)	
training:	Epoch: [30][198/204]	Loss 0.1328 (0.0562)	
training:	Epoch: [30][199/204]	Loss 0.1188 (0.0566)	
training:	Epoch: [30][200/204]	Loss 0.0106 (0.0563)	
training:	Epoch: [30][201/204]	Loss 0.0246 (0.0562)	
training:	Epoch: [30][202/204]	Loss 0.0383 (0.0561)	
training:	Epoch: [30][203/204]	Loss 0.0133 (0.0559)	
training:	Epoch: [30][204/204]	Loss 0.0341 (0.0558)	
Training:	 Loss: 0.0557

Training:	 ACC: 0.9932 0.9933 0.9941 0.9923
Validation:	 ACC: 0.8091 0.8085 0.7953 0.8229
Validation:	 Best_BACC: 0.8326 0.8331 0.8424 0.8229
Validation:	 Loss: 0.7039
Pretraining:	Epoch 31/200
----------
training:	Epoch: [31][1/204]	Loss 0.0109 (0.0109)	
training:	Epoch: [31][2/204]	Loss 0.0159 (0.0134)	
training:	Epoch: [31][3/204]	Loss 0.0101 (0.0123)	
training:	Epoch: [31][4/204]	Loss 0.0381 (0.0187)	
training:	Epoch: [31][5/204]	Loss 0.0149 (0.0180)	
training:	Epoch: [31][6/204]	Loss 0.0214 (0.0185)	
training:	Epoch: [31][7/204]	Loss 0.0141 (0.0179)	
training:	Epoch: [31][8/204]	Loss 0.0178 (0.0179)	
training:	Epoch: [31][9/204]	Loss 0.4127 (0.0618)	
training:	Epoch: [31][10/204]	Loss 0.0492 (0.0605)	
training:	Epoch: [31][11/204]	Loss 0.1602 (0.0696)	
training:	Epoch: [31][12/204]	Loss 0.1772 (0.0785)	
training:	Epoch: [31][13/204]	Loss 0.0490 (0.0763)	
training:	Epoch: [31][14/204]	Loss 0.0128 (0.0717)	
training:	Epoch: [31][15/204]	Loss 0.0162 (0.0680)	
training:	Epoch: [31][16/204]	Loss 0.1855 (0.0754)	
training:	Epoch: [31][17/204]	Loss 0.0354 (0.0730)	
training:	Epoch: [31][18/204]	Loss 0.0132 (0.0697)	
training:	Epoch: [31][19/204]	Loss 0.0113 (0.0666)	
training:	Epoch: [31][20/204]	Loss 0.0189 (0.0642)	
training:	Epoch: [31][21/204]	Loss 0.0101 (0.0617)	
training:	Epoch: [31][22/204]	Loss 0.0126 (0.0594)	
training:	Epoch: [31][23/204]	Loss 0.0176 (0.0576)	
training:	Epoch: [31][24/204]	Loss 0.0235 (0.0562)	
training:	Epoch: [31][25/204]	Loss 0.0236 (0.0549)	
training:	Epoch: [31][26/204]	Loss 0.0096 (0.0531)	
training:	Epoch: [31][27/204]	Loss 0.0134 (0.0517)	
training:	Epoch: [31][28/204]	Loss 0.0443 (0.0514)	
training:	Epoch: [31][29/204]	Loss 0.0165 (0.0502)	
training:	Epoch: [31][30/204]	Loss 0.0126 (0.0490)	
training:	Epoch: [31][31/204]	Loss 0.0138 (0.0478)	
training:	Epoch: [31][32/204]	Loss 0.0146 (0.0468)	
training:	Epoch: [31][33/204]	Loss 0.0183 (0.0459)	
training:	Epoch: [31][34/204]	Loss 0.0241 (0.0453)	
training:	Epoch: [31][35/204]	Loss 0.0754 (0.0461)	
training:	Epoch: [31][36/204]	Loss 0.1192 (0.0482)	
training:	Epoch: [31][37/204]	Loss 0.0171 (0.0473)	
training:	Epoch: [31][38/204]	Loss 0.1337 (0.0496)	
training:	Epoch: [31][39/204]	Loss 0.3185 (0.0565)	
training:	Epoch: [31][40/204]	Loss 0.0102 (0.0553)	
training:	Epoch: [31][41/204]	Loss 0.1432 (0.0575)	
training:	Epoch: [31][42/204]	Loss 0.0145 (0.0565)	
training:	Epoch: [31][43/204]	Loss 0.0660 (0.0567)	
training:	Epoch: [31][44/204]	Loss 0.0172 (0.0558)	
training:	Epoch: [31][45/204]	Loss 0.0361 (0.0553)	
training:	Epoch: [31][46/204]	Loss 0.0143 (0.0545)	
training:	Epoch: [31][47/204]	Loss 0.1714 (0.0569)	
training:	Epoch: [31][48/204]	Loss 0.0113 (0.0560)	
training:	Epoch: [31][49/204]	Loss 0.1563 (0.0580)	
training:	Epoch: [31][50/204]	Loss 0.0134 (0.0571)	
training:	Epoch: [31][51/204]	Loss 0.0653 (0.0573)	
training:	Epoch: [31][52/204]	Loss 0.0116 (0.0564)	
training:	Epoch: [31][53/204]	Loss 0.0160 (0.0557)	
training:	Epoch: [31][54/204]	Loss 0.1378 (0.0572)	
training:	Epoch: [31][55/204]	Loss 0.0093 (0.0563)	
training:	Epoch: [31][56/204]	Loss 0.0117 (0.0555)	
training:	Epoch: [31][57/204]	Loss 0.0262 (0.0550)	
training:	Epoch: [31][58/204]	Loss 0.0115 (0.0542)	
training:	Epoch: [31][59/204]	Loss 0.0195 (0.0537)	
training:	Epoch: [31][60/204]	Loss 0.0928 (0.0543)	
training:	Epoch: [31][61/204]	Loss 0.0220 (0.0538)	
training:	Epoch: [31][62/204]	Loss 0.0304 (0.0534)	
training:	Epoch: [31][63/204]	Loss 0.0136 (0.0528)	
training:	Epoch: [31][64/204]	Loss 0.0610 (0.0529)	
training:	Epoch: [31][65/204]	Loss 0.0336 (0.0526)	
training:	Epoch: [31][66/204]	Loss 0.0134 (0.0520)	
training:	Epoch: [31][67/204]	Loss 0.1539 (0.0535)	
training:	Epoch: [31][68/204]	Loss 0.0103 (0.0529)	
training:	Epoch: [31][69/204]	Loss 0.0901 (0.0534)	
training:	Epoch: [31][70/204]	Loss 0.1450 (0.0547)	
training:	Epoch: [31][71/204]	Loss 0.0734 (0.0550)	
training:	Epoch: [31][72/204]	Loss 0.0167 (0.0545)	
training:	Epoch: [31][73/204]	Loss 0.1219 (0.0554)	
training:	Epoch: [31][74/204]	Loss 0.0301 (0.0551)	
training:	Epoch: [31][75/204]	Loss 0.0215 (0.0546)	
training:	Epoch: [31][76/204]	Loss 0.0144 (0.0541)	
training:	Epoch: [31][77/204]	Loss 0.0114 (0.0535)	
training:	Epoch: [31][78/204]	Loss 0.0143 (0.0530)	
training:	Epoch: [31][79/204]	Loss 0.1546 (0.0543)	
training:	Epoch: [31][80/204]	Loss 0.0099 (0.0538)	
training:	Epoch: [31][81/204]	Loss 0.1608 (0.0551)	
training:	Epoch: [31][82/204]	Loss 0.0367 (0.0549)	
training:	Epoch: [31][83/204]	Loss 0.0122 (0.0543)	
training:	Epoch: [31][84/204]	Loss 0.0129 (0.0538)	
training:	Epoch: [31][85/204]	Loss 0.0673 (0.0540)	
training:	Epoch: [31][86/204]	Loss 0.0160 (0.0536)	
training:	Epoch: [31][87/204]	Loss 0.0109 (0.0531)	
training:	Epoch: [31][88/204]	Loss 0.0138 (0.0526)	
training:	Epoch: [31][89/204]	Loss 0.0118 (0.0522)	
training:	Epoch: [31][90/204]	Loss 0.0147 (0.0518)	
training:	Epoch: [31][91/204]	Loss 0.1610 (0.0530)	
training:	Epoch: [31][92/204]	Loss 0.0144 (0.0525)	
training:	Epoch: [31][93/204]	Loss 0.0198 (0.0522)	
training:	Epoch: [31][94/204]	Loss 0.0142 (0.0518)	
training:	Epoch: [31][95/204]	Loss 0.0109 (0.0513)	
training:	Epoch: [31][96/204]	Loss 0.2048 (0.0529)	
training:	Epoch: [31][97/204]	Loss 0.0149 (0.0526)	
training:	Epoch: [31][98/204]	Loss 0.1014 (0.0531)	
training:	Epoch: [31][99/204]	Loss 0.0122 (0.0526)	
training:	Epoch: [31][100/204]	Loss 0.0329 (0.0524)	
training:	Epoch: [31][101/204]	Loss 0.0095 (0.0520)	
training:	Epoch: [31][102/204]	Loss 0.0271 (0.0518)	
training:	Epoch: [31][103/204]	Loss 0.0100 (0.0514)	
training:	Epoch: [31][104/204]	Loss 0.0369 (0.0512)	
training:	Epoch: [31][105/204]	Loss 0.0223 (0.0509)	
training:	Epoch: [31][106/204]	Loss 0.1685 (0.0521)	
training:	Epoch: [31][107/204]	Loss 0.0143 (0.0517)	
training:	Epoch: [31][108/204]	Loss 0.0120 (0.0513)	
training:	Epoch: [31][109/204]	Loss 0.0133 (0.0510)	
training:	Epoch: [31][110/204]	Loss 0.0113 (0.0506)	
training:	Epoch: [31][111/204]	Loss 0.0189 (0.0503)	
training:	Epoch: [31][112/204]	Loss 0.0233 (0.0501)	
training:	Epoch: [31][113/204]	Loss 0.2458 (0.0518)	
training:	Epoch: [31][114/204]	Loss 0.0885 (0.0522)	
training:	Epoch: [31][115/204]	Loss 0.0101 (0.0518)	
training:	Epoch: [31][116/204]	Loss 0.0220 (0.0515)	
training:	Epoch: [31][117/204]	Loss 0.0113 (0.0512)	
training:	Epoch: [31][118/204]	Loss 0.0126 (0.0509)	
training:	Epoch: [31][119/204]	Loss 0.0229 (0.0506)	
training:	Epoch: [31][120/204]	Loss 0.0922 (0.0510)	
training:	Epoch: [31][121/204]	Loss 0.0163 (0.0507)	
training:	Epoch: [31][122/204]	Loss 0.0172 (0.0504)	
training:	Epoch: [31][123/204]	Loss 0.0377 (0.0503)	
training:	Epoch: [31][124/204]	Loss 0.1378 (0.0510)	
training:	Epoch: [31][125/204]	Loss 0.0367 (0.0509)	
training:	Epoch: [31][126/204]	Loss 0.0130 (0.0506)	
training:	Epoch: [31][127/204]	Loss 0.1729 (0.0516)	
training:	Epoch: [31][128/204]	Loss 0.3462 (0.0539)	
training:	Epoch: [31][129/204]	Loss 0.1241 (0.0544)	
training:	Epoch: [31][130/204]	Loss 0.0118 (0.0541)	
training:	Epoch: [31][131/204]	Loss 0.0440 (0.0540)	
training:	Epoch: [31][132/204]	Loss 0.1719 (0.0549)	
training:	Epoch: [31][133/204]	Loss 0.0136 (0.0546)	
training:	Epoch: [31][134/204]	Loss 0.0138 (0.0543)	
training:	Epoch: [31][135/204]	Loss 0.0176 (0.0540)	
training:	Epoch: [31][136/204]	Loss 0.1649 (0.0548)	
training:	Epoch: [31][137/204]	Loss 0.1613 (0.0556)	
training:	Epoch: [31][138/204]	Loss 0.1667 (0.0564)	
training:	Epoch: [31][139/204]	Loss 0.0179 (0.0561)	
training:	Epoch: [31][140/204]	Loss 0.0819 (0.0563)	
training:	Epoch: [31][141/204]	Loss 0.1892 (0.0573)	
training:	Epoch: [31][142/204]	Loss 0.0692 (0.0573)	
training:	Epoch: [31][143/204]	Loss 0.0206 (0.0571)	
training:	Epoch: [31][144/204]	Loss 0.3648 (0.0592)	
training:	Epoch: [31][145/204]	Loss 0.0526 (0.0592)	
training:	Epoch: [31][146/204]	Loss 0.0221 (0.0589)	
training:	Epoch: [31][147/204]	Loss 0.0134 (0.0586)	
training:	Epoch: [31][148/204]	Loss 0.0178 (0.0583)	
training:	Epoch: [31][149/204]	Loss 0.0267 (0.0581)	
training:	Epoch: [31][150/204]	Loss 0.0107 (0.0578)	
training:	Epoch: [31][151/204]	Loss 0.0364 (0.0577)	
training:	Epoch: [31][152/204]	Loss 0.1535 (0.0583)	
training:	Epoch: [31][153/204]	Loss 0.0315 (0.0581)	
training:	Epoch: [31][154/204]	Loss 0.0939 (0.0584)	
training:	Epoch: [31][155/204]	Loss 0.1241 (0.0588)	
training:	Epoch: [31][156/204]	Loss 0.0146 (0.0585)	
training:	Epoch: [31][157/204]	Loss 0.1906 (0.0593)	
training:	Epoch: [31][158/204]	Loss 0.0166 (0.0591)	
training:	Epoch: [31][159/204]	Loss 0.1338 (0.0595)	
training:	Epoch: [31][160/204]	Loss 0.0481 (0.0595)	
training:	Epoch: [31][161/204]	Loss 0.0191 (0.0592)	
training:	Epoch: [31][162/204]	Loss 0.0147 (0.0589)	
training:	Epoch: [31][163/204]	Loss 0.0149 (0.0587)	
training:	Epoch: [31][164/204]	Loss 0.2468 (0.0598)	
training:	Epoch: [31][165/204]	Loss 0.0508 (0.0598)	
training:	Epoch: [31][166/204]	Loss 0.0184 (0.0595)	
training:	Epoch: [31][167/204]	Loss 0.0121 (0.0592)	
training:	Epoch: [31][168/204]	Loss 0.0127 (0.0589)	
training:	Epoch: [31][169/204]	Loss 0.0174 (0.0587)	
training:	Epoch: [31][170/204]	Loss 0.0481 (0.0586)	
training:	Epoch: [31][171/204]	Loss 0.0274 (0.0585)	
training:	Epoch: [31][172/204]	Loss 0.0119 (0.0582)	
training:	Epoch: [31][173/204]	Loss 0.0101 (0.0579)	
training:	Epoch: [31][174/204]	Loss 0.0100 (0.0576)	
training:	Epoch: [31][175/204]	Loss 0.0996 (0.0579)	
training:	Epoch: [31][176/204]	Loss 0.0121 (0.0576)	
training:	Epoch: [31][177/204]	Loss 0.0104 (0.0573)	
training:	Epoch: [31][178/204]	Loss 0.1463 (0.0578)	
training:	Epoch: [31][179/204]	Loss 0.0245 (0.0577)	
training:	Epoch: [31][180/204]	Loss 0.0105 (0.0574)	
training:	Epoch: [31][181/204]	Loss 0.0590 (0.0574)	
training:	Epoch: [31][182/204]	Loss 0.0126 (0.0572)	
training:	Epoch: [31][183/204]	Loss 0.1867 (0.0579)	
training:	Epoch: [31][184/204]	Loss 0.0381 (0.0578)	
training:	Epoch: [31][185/204]	Loss 0.1984 (0.0585)	
training:	Epoch: [31][186/204]	Loss 0.1715 (0.0591)	
training:	Epoch: [31][187/204]	Loss 0.0299 (0.0590)	
training:	Epoch: [31][188/204]	Loss 0.0105 (0.0587)	
training:	Epoch: [31][189/204]	Loss 0.0187 (0.0585)	
training:	Epoch: [31][190/204]	Loss 0.0116 (0.0583)	
training:	Epoch: [31][191/204]	Loss 0.0112 (0.0580)	
training:	Epoch: [31][192/204]	Loss 0.0298 (0.0579)	
training:	Epoch: [31][193/204]	Loss 0.0105 (0.0576)	
training:	Epoch: [31][194/204]	Loss 0.0108 (0.0574)	
training:	Epoch: [31][195/204]	Loss 0.0274 (0.0572)	
training:	Epoch: [31][196/204]	Loss 0.0118 (0.0570)	
training:	Epoch: [31][197/204]	Loss 0.0101 (0.0568)	
training:	Epoch: [31][198/204]	Loss 0.0104 (0.0565)	
training:	Epoch: [31][199/204]	Loss 0.0147 (0.0563)	
training:	Epoch: [31][200/204]	Loss 0.0971 (0.0565)	
training:	Epoch: [31][201/204]	Loss 0.1367 (0.0569)	
training:	Epoch: [31][202/204]	Loss 0.1876 (0.0576)	
training:	Epoch: [31][203/204]	Loss 0.0109 (0.0573)	
training:	Epoch: [31][204/204]	Loss 0.0203 (0.0571)	
Training:	 Loss: 0.0571

Training:	 ACC: 0.9941 0.9942 0.9956 0.9927
Validation:	 ACC: 0.8063 0.8068 0.8178 0.7948
Validation:	 Best_BACC: 0.8326 0.8331 0.8424 0.8229
Validation:	 Loss: 0.7074
Pretraining:	Epoch 32/200
----------
training:	Epoch: [32][1/204]	Loss 0.0111 (0.0111)	
training:	Epoch: [32][2/204]	Loss 0.0100 (0.0105)	
training:	Epoch: [32][3/204]	Loss 0.0352 (0.0188)	
training:	Epoch: [32][4/204]	Loss 0.0162 (0.0181)	
training:	Epoch: [32][5/204]	Loss 0.0117 (0.0168)	
training:	Epoch: [32][6/204]	Loss 0.0161 (0.0167)	
training:	Epoch: [32][7/204]	Loss 0.0249 (0.0179)	
training:	Epoch: [32][8/204]	Loss 0.0719 (0.0246)	
training:	Epoch: [32][9/204]	Loss 0.0127 (0.0233)	
training:	Epoch: [32][10/204]	Loss 0.0236 (0.0234)	
training:	Epoch: [32][11/204]	Loss 0.0256 (0.0236)	
training:	Epoch: [32][12/204]	Loss 0.0107 (0.0225)	
training:	Epoch: [32][13/204]	Loss 0.1629 (0.0333)	
training:	Epoch: [32][14/204]	Loss 0.0145 (0.0319)	
training:	Epoch: [32][15/204]	Loss 0.0106 (0.0305)	
training:	Epoch: [32][16/204]	Loss 0.1342 (0.0370)	
training:	Epoch: [32][17/204]	Loss 0.0136 (0.0356)	
training:	Epoch: [32][18/204]	Loss 0.0094 (0.0342)	
training:	Epoch: [32][19/204]	Loss 0.0762 (0.0364)	
training:	Epoch: [32][20/204]	Loss 0.0098 (0.0351)	
training:	Epoch: [32][21/204]	Loss 0.1109 (0.0387)	
training:	Epoch: [32][22/204]	Loss 0.1694 (0.0446)	
training:	Epoch: [32][23/204]	Loss 0.0192 (0.0435)	
training:	Epoch: [32][24/204]	Loss 0.1154 (0.0465)	
training:	Epoch: [32][25/204]	Loss 0.0331 (0.0460)	
training:	Epoch: [32][26/204]	Loss 0.0158 (0.0448)	
training:	Epoch: [32][27/204]	Loss 0.1343 (0.0481)	
training:	Epoch: [32][28/204]	Loss 0.0114 (0.0468)	
training:	Epoch: [32][29/204]	Loss 0.0150 (0.0457)	
training:	Epoch: [32][30/204]	Loss 0.0096 (0.0445)	
training:	Epoch: [32][31/204]	Loss 0.1898 (0.0492)	
training:	Epoch: [32][32/204]	Loss 0.3765 (0.0594)	
training:	Epoch: [32][33/204]	Loss 0.0149 (0.0581)	
training:	Epoch: [32][34/204]	Loss 0.1626 (0.0611)	
training:	Epoch: [32][35/204]	Loss 0.1973 (0.0650)	
training:	Epoch: [32][36/204]	Loss 0.0146 (0.0636)	
training:	Epoch: [32][37/204]	Loss 0.0143 (0.0623)	
training:	Epoch: [32][38/204]	Loss 0.0160 (0.0611)	
training:	Epoch: [32][39/204]	Loss 0.0122 (0.0598)	
training:	Epoch: [32][40/204]	Loss 0.0184 (0.0588)	
training:	Epoch: [32][41/204]	Loss 0.0151 (0.0577)	
training:	Epoch: [32][42/204]	Loss 0.1547 (0.0600)	
training:	Epoch: [32][43/204]	Loss 0.0244 (0.0592)	
training:	Epoch: [32][44/204]	Loss 0.0122 (0.0581)	
training:	Epoch: [32][45/204]	Loss 0.0100 (0.0571)	
training:	Epoch: [32][46/204]	Loss 0.0098 (0.0560)	
training:	Epoch: [32][47/204]	Loss 0.0127 (0.0551)	
training:	Epoch: [32][48/204]	Loss 0.0381 (0.0548)	
training:	Epoch: [32][49/204]	Loss 0.0150 (0.0540)	
training:	Epoch: [32][50/204]	Loss 0.0116 (0.0531)	
training:	Epoch: [32][51/204]	Loss 0.1851 (0.0557)	
training:	Epoch: [32][52/204]	Loss 0.0116 (0.0548)	
training:	Epoch: [32][53/204]	Loss 0.0138 (0.0541)	
training:	Epoch: [32][54/204]	Loss 0.0121 (0.0533)	
training:	Epoch: [32][55/204]	Loss 0.3559 (0.0588)	
training:	Epoch: [32][56/204]	Loss 0.0114 (0.0580)	
training:	Epoch: [32][57/204]	Loss 0.0751 (0.0583)	
training:	Epoch: [32][58/204]	Loss 0.3112 (0.0626)	
training:	Epoch: [32][59/204]	Loss 0.0295 (0.0621)	
training:	Epoch: [32][60/204]	Loss 0.0183 (0.0613)	
training:	Epoch: [32][61/204]	Loss 0.0100 (0.0605)	
training:	Epoch: [32][62/204]	Loss 0.0128 (0.0597)	
training:	Epoch: [32][63/204]	Loss 0.0295 (0.0592)	
training:	Epoch: [32][64/204]	Loss 0.0128 (0.0585)	
training:	Epoch: [32][65/204]	Loss 0.0161 (0.0579)	
training:	Epoch: [32][66/204]	Loss 0.0130 (0.0572)	
training:	Epoch: [32][67/204]	Loss 0.0143 (0.0565)	
training:	Epoch: [32][68/204]	Loss 0.0144 (0.0559)	
training:	Epoch: [32][69/204]	Loss 0.0284 (0.0555)	
training:	Epoch: [32][70/204]	Loss 0.0248 (0.0551)	
training:	Epoch: [32][71/204]	Loss 0.1718 (0.0567)	
training:	Epoch: [32][72/204]	Loss 0.0587 (0.0567)	
training:	Epoch: [32][73/204]	Loss 0.0141 (0.0562)	
training:	Epoch: [32][74/204]	Loss 0.0637 (0.0563)	
training:	Epoch: [32][75/204]	Loss 0.1517 (0.0575)	
training:	Epoch: [32][76/204]	Loss 0.0329 (0.0572)	
training:	Epoch: [32][77/204]	Loss 0.0102 (0.0566)	
training:	Epoch: [32][78/204]	Loss 0.0115 (0.0560)	
training:	Epoch: [32][79/204]	Loss 0.2315 (0.0582)	
training:	Epoch: [32][80/204]	Loss 0.0159 (0.0577)	
training:	Epoch: [32][81/204]	Loss 0.0121 (0.0572)	
training:	Epoch: [32][82/204]	Loss 0.0121 (0.0566)	
training:	Epoch: [32][83/204]	Loss 0.1567 (0.0578)	
training:	Epoch: [32][84/204]	Loss 0.0181 (0.0573)	
training:	Epoch: [32][85/204]	Loss 0.0105 (0.0568)	
training:	Epoch: [32][86/204]	Loss 0.0125 (0.0563)	
training:	Epoch: [32][87/204]	Loss 0.1726 (0.0576)	
training:	Epoch: [32][88/204]	Loss 0.0170 (0.0571)	
training:	Epoch: [32][89/204]	Loss 0.0125 (0.0566)	
training:	Epoch: [32][90/204]	Loss 0.3479 (0.0599)	
training:	Epoch: [32][91/204]	Loss 0.0125 (0.0594)	
training:	Epoch: [32][92/204]	Loss 0.0148 (0.0589)	
training:	Epoch: [32][93/204]	Loss 0.0599 (0.0589)	
training:	Epoch: [32][94/204]	Loss 0.0438 (0.0587)	
training:	Epoch: [32][95/204]	Loss 0.1491 (0.0597)	
training:	Epoch: [32][96/204]	Loss 0.1442 (0.0606)	
training:	Epoch: [32][97/204]	Loss 0.0101 (0.0600)	
training:	Epoch: [32][98/204]	Loss 0.0189 (0.0596)	
training:	Epoch: [32][99/204]	Loss 0.0110 (0.0591)	
training:	Epoch: [32][100/204]	Loss 0.0110 (0.0586)	
training:	Epoch: [32][101/204]	Loss 0.1005 (0.0591)	
training:	Epoch: [32][102/204]	Loss 0.1475 (0.0599)	
training:	Epoch: [32][103/204]	Loss 0.0886 (0.0602)	
training:	Epoch: [32][104/204]	Loss 0.0115 (0.0597)	
training:	Epoch: [32][105/204]	Loss 0.0497 (0.0596)	
training:	Epoch: [32][106/204]	Loss 0.0382 (0.0594)	
training:	Epoch: [32][107/204]	Loss 0.0143 (0.0590)	
training:	Epoch: [32][108/204]	Loss 0.1470 (0.0598)	
training:	Epoch: [32][109/204]	Loss 0.0126 (0.0594)	
training:	Epoch: [32][110/204]	Loss 0.0147 (0.0590)	
training:	Epoch: [32][111/204]	Loss 0.0336 (0.0588)	
training:	Epoch: [32][112/204]	Loss 0.0095 (0.0583)	
training:	Epoch: [32][113/204]	Loss 0.0889 (0.0586)	
training:	Epoch: [32][114/204]	Loss 0.0145 (0.0582)	
training:	Epoch: [32][115/204]	Loss 0.0117 (0.0578)	
training:	Epoch: [32][116/204]	Loss 0.0102 (0.0574)	
training:	Epoch: [32][117/204]	Loss 0.0333 (0.0572)	
training:	Epoch: [32][118/204]	Loss 0.1479 (0.0580)	
training:	Epoch: [32][119/204]	Loss 0.0090 (0.0575)	
training:	Epoch: [32][120/204]	Loss 0.0102 (0.0572)	
training:	Epoch: [32][121/204]	Loss 0.0543 (0.0571)	
training:	Epoch: [32][122/204]	Loss 0.1306 (0.0577)	
training:	Epoch: [32][123/204]	Loss 0.0202 (0.0574)	
training:	Epoch: [32][124/204]	Loss 0.0133 (0.0571)	
training:	Epoch: [32][125/204]	Loss 0.0102 (0.0567)	
training:	Epoch: [32][126/204]	Loss 0.0621 (0.0567)	
training:	Epoch: [32][127/204]	Loss 0.0109 (0.0564)	
training:	Epoch: [32][128/204]	Loss 0.0083 (0.0560)	
training:	Epoch: [32][129/204]	Loss 0.1480 (0.0567)	
training:	Epoch: [32][130/204]	Loss 0.0152 (0.0564)	
training:	Epoch: [32][131/204]	Loss 0.0108 (0.0560)	
training:	Epoch: [32][132/204]	Loss 0.0760 (0.0562)	
training:	Epoch: [32][133/204]	Loss 0.1391 (0.0568)	
training:	Epoch: [32][134/204]	Loss 0.0126 (0.0565)	
training:	Epoch: [32][135/204]	Loss 0.0094 (0.0561)	
training:	Epoch: [32][136/204]	Loss 0.0092 (0.0558)	
training:	Epoch: [32][137/204]	Loss 0.1153 (0.0562)	
training:	Epoch: [32][138/204]	Loss 0.0096 (0.0559)	
training:	Epoch: [32][139/204]	Loss 0.0106 (0.0556)	
training:	Epoch: [32][140/204]	Loss 0.0114 (0.0553)	
training:	Epoch: [32][141/204]	Loss 0.0115 (0.0549)	
training:	Epoch: [32][142/204]	Loss 0.0153 (0.0547)	
training:	Epoch: [32][143/204]	Loss 0.0098 (0.0544)	
training:	Epoch: [32][144/204]	Loss 0.0247 (0.0541)	
training:	Epoch: [32][145/204]	Loss 0.0381 (0.0540)	
training:	Epoch: [32][146/204]	Loss 0.0931 (0.0543)	
training:	Epoch: [32][147/204]	Loss 0.0103 (0.0540)	
training:	Epoch: [32][148/204]	Loss 0.1173 (0.0544)	
training:	Epoch: [32][149/204]	Loss 0.1528 (0.0551)	
training:	Epoch: [32][150/204]	Loss 0.2660 (0.0565)	
training:	Epoch: [32][151/204]	Loss 0.0100 (0.0562)	
training:	Epoch: [32][152/204]	Loss 0.0127 (0.0559)	
training:	Epoch: [32][153/204]	Loss 0.0215 (0.0557)	
training:	Epoch: [32][154/204]	Loss 0.1484 (0.0563)	
training:	Epoch: [32][155/204]	Loss 0.0222 (0.0561)	
training:	Epoch: [32][156/204]	Loss 0.0273 (0.0559)	
training:	Epoch: [32][157/204]	Loss 0.0161 (0.0556)	
training:	Epoch: [32][158/204]	Loss 0.0328 (0.0555)	
training:	Epoch: [32][159/204]	Loss 0.0113 (0.0552)	
training:	Epoch: [32][160/204]	Loss 0.0161 (0.0550)	
training:	Epoch: [32][161/204]	Loss 0.0363 (0.0548)	
training:	Epoch: [32][162/204]	Loss 0.0187 (0.0546)	
training:	Epoch: [32][163/204]	Loss 0.0191 (0.0544)	
training:	Epoch: [32][164/204]	Loss 0.0111 (0.0541)	
training:	Epoch: [32][165/204]	Loss 0.0380 (0.0540)	
training:	Epoch: [32][166/204]	Loss 0.0083 (0.0538)	
training:	Epoch: [32][167/204]	Loss 0.0628 (0.0538)	
training:	Epoch: [32][168/204]	Loss 0.1573 (0.0544)	
training:	Epoch: [32][169/204]	Loss 0.0100 (0.0542)	
training:	Epoch: [32][170/204]	Loss 0.0928 (0.0544)	
training:	Epoch: [32][171/204]	Loss 0.0120 (0.0541)	
training:	Epoch: [32][172/204]	Loss 0.1412 (0.0547)	
training:	Epoch: [32][173/204]	Loss 0.0098 (0.0544)	
training:	Epoch: [32][174/204]	Loss 0.0175 (0.0542)	
training:	Epoch: [32][175/204]	Loss 0.0093 (0.0539)	
training:	Epoch: [32][176/204]	Loss 0.0152 (0.0537)	
training:	Epoch: [32][177/204]	Loss 0.0113 (0.0535)	
training:	Epoch: [32][178/204]	Loss 0.0190 (0.0533)	
training:	Epoch: [32][179/204]	Loss 0.0120 (0.0530)	
training:	Epoch: [32][180/204]	Loss 0.0098 (0.0528)	
training:	Epoch: [32][181/204]	Loss 0.0141 (0.0526)	
training:	Epoch: [32][182/204]	Loss 0.0099 (0.0524)	
training:	Epoch: [32][183/204]	Loss 0.0972 (0.0526)	
training:	Epoch: [32][184/204]	Loss 0.0225 (0.0524)	
training:	Epoch: [32][185/204]	Loss 0.0158 (0.0522)	
training:	Epoch: [32][186/204]	Loss 0.0256 (0.0521)	
training:	Epoch: [32][187/204]	Loss 0.0263 (0.0520)	
training:	Epoch: [32][188/204]	Loss 0.1592 (0.0525)	
training:	Epoch: [32][189/204]	Loss 0.0108 (0.0523)	
training:	Epoch: [32][190/204]	Loss 0.0096 (0.0521)	
training:	Epoch: [32][191/204]	Loss 0.0094 (0.0519)	
training:	Epoch: [32][192/204]	Loss 0.2288 (0.0528)	
training:	Epoch: [32][193/204]	Loss 0.0090 (0.0526)	
training:	Epoch: [32][194/204]	Loss 0.0227 (0.0524)	
training:	Epoch: [32][195/204]	Loss 0.0235 (0.0522)	
training:	Epoch: [32][196/204]	Loss 0.0220 (0.0521)	
training:	Epoch: [32][197/204]	Loss 0.1551 (0.0526)	
training:	Epoch: [32][198/204]	Loss 0.0163 (0.0524)	
training:	Epoch: [32][199/204]	Loss 0.0109 (0.0522)	
training:	Epoch: [32][200/204]	Loss 0.0109 (0.0520)	
training:	Epoch: [32][201/204]	Loss 0.1548 (0.0525)	
training:	Epoch: [32][202/204]	Loss 0.0538 (0.0525)	
training:	Epoch: [32][203/204]	Loss 0.0965 (0.0528)	
training:	Epoch: [32][204/204]	Loss 0.1079 (0.0530)	
Training:	 Loss: 0.0529

Training:	 ACC: 0.9941 0.9942 0.9956 0.9927
Validation:	 ACC: 0.8006 0.8004 0.7973 0.8038
Validation:	 Best_BACC: 0.8326 0.8331 0.8424 0.8229
Validation:	 Loss: 0.7220
Pretraining:	Epoch 33/200
----------
training:	Epoch: [33][1/204]	Loss 0.0106 (0.0106)	
training:	Epoch: [33][2/204]	Loss 0.0435 (0.0271)	
training:	Epoch: [33][3/204]	Loss 0.0087 (0.0209)	
training:	Epoch: [33][4/204]	Loss 0.0119 (0.0187)	
training:	Epoch: [33][5/204]	Loss 0.0155 (0.0180)	
training:	Epoch: [33][6/204]	Loss 0.0126 (0.0171)	
training:	Epoch: [33][7/204]	Loss 0.0101 (0.0161)	
training:	Epoch: [33][8/204]	Loss 0.0121 (0.0156)	
training:	Epoch: [33][9/204]	Loss 0.0138 (0.0154)	
training:	Epoch: [33][10/204]	Loss 0.0306 (0.0169)	
training:	Epoch: [33][11/204]	Loss 0.0223 (0.0174)	
training:	Epoch: [33][12/204]	Loss 0.0361 (0.0190)	
training:	Epoch: [33][13/204]	Loss 0.0515 (0.0215)	
training:	Epoch: [33][14/204]	Loss 0.0630 (0.0244)	
training:	Epoch: [33][15/204]	Loss 0.0090 (0.0234)	
training:	Epoch: [33][16/204]	Loss 0.2430 (0.0371)	
training:	Epoch: [33][17/204]	Loss 0.0096 (0.0355)	
training:	Epoch: [33][18/204]	Loss 0.1599 (0.0424)	
training:	Epoch: [33][19/204]	Loss 0.0361 (0.0421)	
training:	Epoch: [33][20/204]	Loss 0.0097 (0.0405)	
training:	Epoch: [33][21/204]	Loss 0.1821 (0.0472)	
training:	Epoch: [33][22/204]	Loss 0.0097 (0.0455)	
training:	Epoch: [33][23/204]	Loss 0.0095 (0.0439)	
training:	Epoch: [33][24/204]	Loss 0.0812 (0.0455)	
training:	Epoch: [33][25/204]	Loss 0.0712 (0.0465)	
training:	Epoch: [33][26/204]	Loss 0.0087 (0.0451)	
training:	Epoch: [33][27/204]	Loss 0.0123 (0.0439)	
training:	Epoch: [33][28/204]	Loss 0.0372 (0.0436)	
training:	Epoch: [33][29/204]	Loss 0.0381 (0.0434)	
training:	Epoch: [33][30/204]	Loss 0.0272 (0.0429)	
training:	Epoch: [33][31/204]	Loss 0.0127 (0.0419)	
training:	Epoch: [33][32/204]	Loss 0.0174 (0.0411)	
training:	Epoch: [33][33/204]	Loss 0.0091 (0.0402)	
training:	Epoch: [33][34/204]	Loss 0.0124 (0.0394)	
training:	Epoch: [33][35/204]	Loss 0.0280 (0.0390)	
training:	Epoch: [33][36/204]	Loss 0.0337 (0.0389)	
training:	Epoch: [33][37/204]	Loss 0.0130 (0.0382)	
training:	Epoch: [33][38/204]	Loss 0.0241 (0.0378)	
training:	Epoch: [33][39/204]	Loss 0.0919 (0.0392)	
training:	Epoch: [33][40/204]	Loss 0.0334 (0.0391)	
training:	Epoch: [33][41/204]	Loss 0.0171 (0.0385)	
training:	Epoch: [33][42/204]	Loss 0.1077 (0.0402)	
training:	Epoch: [33][43/204]	Loss 0.0183 (0.0397)	
training:	Epoch: [33][44/204]	Loss 0.0105 (0.0390)	
training:	Epoch: [33][45/204]	Loss 0.0196 (0.0386)	
training:	Epoch: [33][46/204]	Loss 0.2225 (0.0426)	
training:	Epoch: [33][47/204]	Loss 0.0145 (0.0420)	
training:	Epoch: [33][48/204]	Loss 0.0164 (0.0414)	
training:	Epoch: [33][49/204]	Loss 0.0151 (0.0409)	
training:	Epoch: [33][50/204]	Loss 0.0920 (0.0419)	
training:	Epoch: [33][51/204]	Loss 0.0089 (0.0413)	
training:	Epoch: [33][52/204]	Loss 0.1297 (0.0430)	
training:	Epoch: [33][53/204]	Loss 0.0189 (0.0425)	
training:	Epoch: [33][54/204]	Loss 0.0890 (0.0434)	
training:	Epoch: [33][55/204]	Loss 0.0092 (0.0428)	
training:	Epoch: [33][56/204]	Loss 0.0175 (0.0423)	
training:	Epoch: [33][57/204]	Loss 0.0091 (0.0417)	
training:	Epoch: [33][58/204]	Loss 0.0402 (0.0417)	
training:	Epoch: [33][59/204]	Loss 0.1511 (0.0436)	
training:	Epoch: [33][60/204]	Loss 0.0105 (0.0430)	
training:	Epoch: [33][61/204]	Loss 0.1642 (0.0450)	
training:	Epoch: [33][62/204]	Loss 0.0125 (0.0445)	
training:	Epoch: [33][63/204]	Loss 0.0814 (0.0451)	
training:	Epoch: [33][64/204]	Loss 0.0102 (0.0445)	
training:	Epoch: [33][65/204]	Loss 0.1514 (0.0462)	
training:	Epoch: [33][66/204]	Loss 0.0111 (0.0456)	
training:	Epoch: [33][67/204]	Loss 0.0101 (0.0451)	
training:	Epoch: [33][68/204]	Loss 0.0536 (0.0452)	
training:	Epoch: [33][69/204]	Loss 0.0103 (0.0447)	
training:	Epoch: [33][70/204]	Loss 0.0292 (0.0445)	
training:	Epoch: [33][71/204]	Loss 0.0440 (0.0445)	
training:	Epoch: [33][72/204]	Loss 0.0510 (0.0446)	
training:	Epoch: [33][73/204]	Loss 0.0983 (0.0453)	
training:	Epoch: [33][74/204]	Loss 0.0178 (0.0449)	
training:	Epoch: [33][75/204]	Loss 0.1424 (0.0462)	
training:	Epoch: [33][76/204]	Loss 0.0114 (0.0458)	
training:	Epoch: [33][77/204]	Loss 0.0830 (0.0463)	
training:	Epoch: [33][78/204]	Loss 0.1405 (0.0475)	
training:	Epoch: [33][79/204]	Loss 0.0092 (0.0470)	
training:	Epoch: [33][80/204]	Loss 0.0101 (0.0465)	
training:	Epoch: [33][81/204]	Loss 0.0142 (0.0461)	
training:	Epoch: [33][82/204]	Loss 0.0151 (0.0457)	
training:	Epoch: [33][83/204]	Loss 0.2407 (0.0481)	
training:	Epoch: [33][84/204]	Loss 0.0104 (0.0476)	
training:	Epoch: [33][85/204]	Loss 0.0195 (0.0473)	
training:	Epoch: [33][86/204]	Loss 0.1035 (0.0480)	
training:	Epoch: [33][87/204]	Loss 0.0262 (0.0477)	
training:	Epoch: [33][88/204]	Loss 0.0087 (0.0473)	
training:	Epoch: [33][89/204]	Loss 0.0389 (0.0472)	
training:	Epoch: [33][90/204]	Loss 0.0160 (0.0468)	
training:	Epoch: [33][91/204]	Loss 0.0093 (0.0464)	
training:	Epoch: [33][92/204]	Loss 0.0096 (0.0460)	
training:	Epoch: [33][93/204]	Loss 0.0181 (0.0457)	
training:	Epoch: [33][94/204]	Loss 0.0463 (0.0457)	
training:	Epoch: [33][95/204]	Loss 0.0150 (0.0454)	
training:	Epoch: [33][96/204]	Loss 0.0111 (0.0450)	
training:	Epoch: [33][97/204]	Loss 0.1569 (0.0462)	
training:	Epoch: [33][98/204]	Loss 0.0672 (0.0464)	
training:	Epoch: [33][99/204]	Loss 0.0106 (0.0461)	
training:	Epoch: [33][100/204]	Loss 0.0240 (0.0458)	
training:	Epoch: [33][101/204]	Loss 0.1343 (0.0467)	
training:	Epoch: [33][102/204]	Loss 0.0107 (0.0464)	
training:	Epoch: [33][103/204]	Loss 0.0077 (0.0460)	
training:	Epoch: [33][104/204]	Loss 0.0082 (0.0456)	
training:	Epoch: [33][105/204]	Loss 0.1759 (0.0469)	
training:	Epoch: [33][106/204]	Loss 0.0089 (0.0465)	
training:	Epoch: [33][107/204]	Loss 0.0121 (0.0462)	
training:	Epoch: [33][108/204]	Loss 0.1671 (0.0473)	
training:	Epoch: [33][109/204]	Loss 0.0097 (0.0470)	
training:	Epoch: [33][110/204]	Loss 0.0166 (0.0467)	
training:	Epoch: [33][111/204]	Loss 0.1411 (0.0475)	
training:	Epoch: [33][112/204]	Loss 0.2888 (0.0497)	
training:	Epoch: [33][113/204]	Loss 0.0907 (0.0500)	
training:	Epoch: [33][114/204]	Loss 0.0740 (0.0503)	
training:	Epoch: [33][115/204]	Loss 0.0155 (0.0500)	
training:	Epoch: [33][116/204]	Loss 0.0140 (0.0496)	
training:	Epoch: [33][117/204]	Loss 0.0176 (0.0494)	
training:	Epoch: [33][118/204]	Loss 0.0583 (0.0494)	
training:	Epoch: [33][119/204]	Loss 0.0179 (0.0492)	
training:	Epoch: [33][120/204]	Loss 0.1632 (0.0501)	
training:	Epoch: [33][121/204]	Loss 0.0148 (0.0498)	
training:	Epoch: [33][122/204]	Loss 0.0096 (0.0495)	
training:	Epoch: [33][123/204]	Loss 0.0197 (0.0493)	
training:	Epoch: [33][124/204]	Loss 0.0090 (0.0489)	
training:	Epoch: [33][125/204]	Loss 0.0087 (0.0486)	
training:	Epoch: [33][126/204]	Loss 0.1359 (0.0493)	
training:	Epoch: [33][127/204]	Loss 0.0122 (0.0490)	
training:	Epoch: [33][128/204]	Loss 0.0113 (0.0487)	
training:	Epoch: [33][129/204]	Loss 0.0448 (0.0487)	
training:	Epoch: [33][130/204]	Loss 0.1691 (0.0496)	
training:	Epoch: [33][131/204]	Loss 0.0095 (0.0493)	
training:	Epoch: [33][132/204]	Loss 0.0181 (0.0491)	
training:	Epoch: [33][133/204]	Loss 0.0095 (0.0488)	
training:	Epoch: [33][134/204]	Loss 0.0145 (0.0485)	
training:	Epoch: [33][135/204]	Loss 0.0097 (0.0482)	
training:	Epoch: [33][136/204]	Loss 0.0384 (0.0482)	
training:	Epoch: [33][137/204]	Loss 0.1980 (0.0493)	
training:	Epoch: [33][138/204]	Loss 0.0160 (0.0490)	
training:	Epoch: [33][139/204]	Loss 0.0116 (0.0487)	
training:	Epoch: [33][140/204]	Loss 0.0094 (0.0485)	
training:	Epoch: [33][141/204]	Loss 0.1770 (0.0494)	
training:	Epoch: [33][142/204]	Loss 0.0109 (0.0491)	
training:	Epoch: [33][143/204]	Loss 0.2006 (0.0502)	
training:	Epoch: [33][144/204]	Loss 0.0146 (0.0499)	
training:	Epoch: [33][145/204]	Loss 0.0119 (0.0497)	
training:	Epoch: [33][146/204]	Loss 0.0191 (0.0494)	
training:	Epoch: [33][147/204]	Loss 0.0084 (0.0492)	
training:	Epoch: [33][148/204]	Loss 0.0088 (0.0489)	
training:	Epoch: [33][149/204]	Loss 0.1690 (0.0497)	
training:	Epoch: [33][150/204]	Loss 0.0267 (0.0495)	
training:	Epoch: [33][151/204]	Loss 0.1796 (0.0504)	
training:	Epoch: [33][152/204]	Loss 0.0119 (0.0502)	
training:	Epoch: [33][153/204]	Loss 0.0217 (0.0500)	
training:	Epoch: [33][154/204]	Loss 0.1555 (0.0507)	
training:	Epoch: [33][155/204]	Loss 0.0114 (0.0504)	
training:	Epoch: [33][156/204]	Loss 0.0117 (0.0502)	
training:	Epoch: [33][157/204]	Loss 0.1570 (0.0508)	
training:	Epoch: [33][158/204]	Loss 0.0087 (0.0506)	
training:	Epoch: [33][159/204]	Loss 0.1570 (0.0512)	
training:	Epoch: [33][160/204]	Loss 0.0138 (0.0510)	
training:	Epoch: [33][161/204]	Loss 0.0107 (0.0508)	
training:	Epoch: [33][162/204]	Loss 0.0216 (0.0506)	
training:	Epoch: [33][163/204]	Loss 0.0093 (0.0503)	
training:	Epoch: [33][164/204]	Loss 0.0204 (0.0501)	
training:	Epoch: [33][165/204]	Loss 0.0650 (0.0502)	
training:	Epoch: [33][166/204]	Loss 0.0259 (0.0501)	
training:	Epoch: [33][167/204]	Loss 0.0130 (0.0499)	
training:	Epoch: [33][168/204]	Loss 0.0153 (0.0497)	
training:	Epoch: [33][169/204]	Loss 0.0099 (0.0494)	
training:	Epoch: [33][170/204]	Loss 0.0161 (0.0492)	
training:	Epoch: [33][171/204]	Loss 0.0109 (0.0490)	
training:	Epoch: [33][172/204]	Loss 0.0090 (0.0488)	
training:	Epoch: [33][173/204]	Loss 0.0132 (0.0486)	
training:	Epoch: [33][174/204]	Loss 0.1480 (0.0491)	
training:	Epoch: [33][175/204]	Loss 0.0274 (0.0490)	
training:	Epoch: [33][176/204]	Loss 0.0151 (0.0488)	
training:	Epoch: [33][177/204]	Loss 0.0082 (0.0486)	
training:	Epoch: [33][178/204]	Loss 0.0110 (0.0484)	
training:	Epoch: [33][179/204]	Loss 0.0098 (0.0482)	
training:	Epoch: [33][180/204]	Loss 0.1550 (0.0488)	
training:	Epoch: [33][181/204]	Loss 0.1528 (0.0493)	
training:	Epoch: [33][182/204]	Loss 0.0195 (0.0492)	
training:	Epoch: [33][183/204]	Loss 0.0178 (0.0490)	
training:	Epoch: [33][184/204]	Loss 0.0217 (0.0488)	
training:	Epoch: [33][185/204]	Loss 0.0150 (0.0487)	
training:	Epoch: [33][186/204]	Loss 0.0087 (0.0484)	
training:	Epoch: [33][187/204]	Loss 0.0115 (0.0482)	
training:	Epoch: [33][188/204]	Loss 0.0159 (0.0481)	
training:	Epoch: [33][189/204]	Loss 0.1614 (0.0487)	
training:	Epoch: [33][190/204]	Loss 0.0105 (0.0485)	
training:	Epoch: [33][191/204]	Loss 0.0110 (0.0483)	
training:	Epoch: [33][192/204]	Loss 0.0135 (0.0481)	
training:	Epoch: [33][193/204]	Loss 0.0148 (0.0479)	
training:	Epoch: [33][194/204]	Loss 0.0800 (0.0481)	
training:	Epoch: [33][195/204]	Loss 0.0566 (0.0481)	
training:	Epoch: [33][196/204]	Loss 0.0379 (0.0481)	
training:	Epoch: [33][197/204]	Loss 0.0158 (0.0479)	
training:	Epoch: [33][198/204]	Loss 0.0169 (0.0478)	
training:	Epoch: [33][199/204]	Loss 0.0091 (0.0476)	
training:	Epoch: [33][200/204]	Loss 0.0084 (0.0474)	
training:	Epoch: [33][201/204]	Loss 0.0201 (0.0472)	
training:	Epoch: [33][202/204]	Loss 0.0875 (0.0474)	
training:	Epoch: [33][203/204]	Loss 0.0532 (0.0475)	
training:	Epoch: [33][204/204]	Loss 0.1621 (0.0480)	
Training:	 Loss: 0.0480

Training:	 ACC: 0.9947 0.9948 0.9965 0.9930
Validation:	 ACC: 0.8013 0.8020 0.8178 0.7848
Validation:	 Best_BACC: 0.8326 0.8331 0.8424 0.8229
Validation:	 Loss: 0.7241
Pretraining:	Epoch 34/200
----------
training:	Epoch: [34][1/204]	Loss 0.0102 (0.0102)	
training:	Epoch: [34][2/204]	Loss 0.0132 (0.0117)	
training:	Epoch: [34][3/204]	Loss 0.0085 (0.0107)	
training:	Epoch: [34][4/204]	Loss 0.0085 (0.0101)	
training:	Epoch: [34][5/204]	Loss 0.0113 (0.0104)	
training:	Epoch: [34][6/204]	Loss 0.0195 (0.0119)	
training:	Epoch: [34][7/204]	Loss 0.0322 (0.0148)	
training:	Epoch: [34][8/204]	Loss 0.0928 (0.0245)	
training:	Epoch: [34][9/204]	Loss 0.0119 (0.0231)	
training:	Epoch: [34][10/204]	Loss 0.0114 (0.0220)	
training:	Epoch: [34][11/204]	Loss 0.0094 (0.0208)	
training:	Epoch: [34][12/204]	Loss 0.0220 (0.0209)	
training:	Epoch: [34][13/204]	Loss 0.0113 (0.0202)	
training:	Epoch: [34][14/204]	Loss 0.0483 (0.0222)	
training:	Epoch: [34][15/204]	Loss 0.0335 (0.0229)	
training:	Epoch: [34][16/204]	Loss 0.0083 (0.0220)	
training:	Epoch: [34][17/204]	Loss 0.0123 (0.0215)	
training:	Epoch: [34][18/204]	Loss 0.0097 (0.0208)	
training:	Epoch: [34][19/204]	Loss 0.0126 (0.0204)	
training:	Epoch: [34][20/204]	Loss 0.0134 (0.0200)	
training:	Epoch: [34][21/204]	Loss 0.0112 (0.0196)	
training:	Epoch: [34][22/204]	Loss 0.0091 (0.0191)	
training:	Epoch: [34][23/204]	Loss 0.0100 (0.0187)	
training:	Epoch: [34][24/204]	Loss 0.0152 (0.0186)	
training:	Epoch: [34][25/204]	Loss 0.0119 (0.0183)	
training:	Epoch: [34][26/204]	Loss 0.1727 (0.0243)	
training:	Epoch: [34][27/204]	Loss 0.0787 (0.0263)	
training:	Epoch: [34][28/204]	Loss 0.1687 (0.0314)	
training:	Epoch: [34][29/204]	Loss 0.0287 (0.0313)	
training:	Epoch: [34][30/204]	Loss 0.0459 (0.0318)	
training:	Epoch: [34][31/204]	Loss 0.0116 (0.0311)	
training:	Epoch: [34][32/204]	Loss 0.0163 (0.0306)	
training:	Epoch: [34][33/204]	Loss 0.0167 (0.0302)	
training:	Epoch: [34][34/204]	Loss 0.0088 (0.0296)	
training:	Epoch: [34][35/204]	Loss 0.0107 (0.0290)	
training:	Epoch: [34][36/204]	Loss 0.0145 (0.0286)	
training:	Epoch: [34][37/204]	Loss 0.0312 (0.0287)	
training:	Epoch: [34][38/204]	Loss 0.0097 (0.0282)	
training:	Epoch: [34][39/204]	Loss 0.0401 (0.0285)	
training:	Epoch: [34][40/204]	Loss 0.1640 (0.0319)	
training:	Epoch: [34][41/204]	Loss 0.0110 (0.0314)	
training:	Epoch: [34][42/204]	Loss 0.0404 (0.0316)	
training:	Epoch: [34][43/204]	Loss 0.0108 (0.0311)	
training:	Epoch: [34][44/204]	Loss 0.0710 (0.0320)	
training:	Epoch: [34][45/204]	Loss 0.0099 (0.0315)	
training:	Epoch: [34][46/204]	Loss 0.0113 (0.0311)	
training:	Epoch: [34][47/204]	Loss 0.0165 (0.0308)	
training:	Epoch: [34][48/204]	Loss 0.0664 (0.0315)	
training:	Epoch: [34][49/204]	Loss 0.3198 (0.0374)	
training:	Epoch: [34][50/204]	Loss 0.0135 (0.0369)	
training:	Epoch: [34][51/204]	Loss 0.0100 (0.0364)	
training:	Epoch: [34][52/204]	Loss 0.0115 (0.0359)	
training:	Epoch: [34][53/204]	Loss 0.0093 (0.0354)	
training:	Epoch: [34][54/204]	Loss 0.0114 (0.0350)	
training:	Epoch: [34][55/204]	Loss 0.0270 (0.0348)	
training:	Epoch: [34][56/204]	Loss 0.0175 (0.0345)	
training:	Epoch: [34][57/204]	Loss 0.0103 (0.0341)	
training:	Epoch: [34][58/204]	Loss 0.0432 (0.0343)	
training:	Epoch: [34][59/204]	Loss 0.0114 (0.0339)	
training:	Epoch: [34][60/204]	Loss 0.0088 (0.0335)	
training:	Epoch: [34][61/204]	Loss 0.0165 (0.0332)	
training:	Epoch: [34][62/204]	Loss 0.0090 (0.0328)	
training:	Epoch: [34][63/204]	Loss 0.0114 (0.0324)	
training:	Epoch: [34][64/204]	Loss 0.0142 (0.0322)	
training:	Epoch: [34][65/204]	Loss 0.0149 (0.0319)	
training:	Epoch: [34][66/204]	Loss 0.0069 (0.0315)	
training:	Epoch: [34][67/204]	Loss 0.0099 (0.0312)	
training:	Epoch: [34][68/204]	Loss 0.0095 (0.0309)	
training:	Epoch: [34][69/204]	Loss 0.0183 (0.0307)	
training:	Epoch: [34][70/204]	Loss 0.0083 (0.0304)	
training:	Epoch: [34][71/204]	Loss 0.0092 (0.0301)	
training:	Epoch: [34][72/204]	Loss 0.0142 (0.0299)	
training:	Epoch: [34][73/204]	Loss 0.0440 (0.0301)	
training:	Epoch: [34][74/204]	Loss 0.0117 (0.0298)	
training:	Epoch: [34][75/204]	Loss 0.0080 (0.0295)	
training:	Epoch: [34][76/204]	Loss 0.0204 (0.0294)	
training:	Epoch: [34][77/204]	Loss 0.0153 (0.0292)	
training:	Epoch: [34][78/204]	Loss 0.0099 (0.0290)	
training:	Epoch: [34][79/204]	Loss 0.1613 (0.0306)	
training:	Epoch: [34][80/204]	Loss 0.0107 (0.0304)	
training:	Epoch: [34][81/204]	Loss 0.0109 (0.0301)	
training:	Epoch: [34][82/204]	Loss 0.0088 (0.0299)	
training:	Epoch: [34][83/204]	Loss 0.0099 (0.0296)	
training:	Epoch: [34][84/204]	Loss 0.0816 (0.0303)	
training:	Epoch: [34][85/204]	Loss 0.0175 (0.0301)	
training:	Epoch: [34][86/204]	Loss 0.0107 (0.0299)	
training:	Epoch: [34][87/204]	Loss 0.0098 (0.0297)	
training:	Epoch: [34][88/204]	Loss 0.0082 (0.0294)	
training:	Epoch: [34][89/204]	Loss 0.0526 (0.0297)	
training:	Epoch: [34][90/204]	Loss 0.0169 (0.0295)	
training:	Epoch: [34][91/204]	Loss 0.0082 (0.0293)	
training:	Epoch: [34][92/204]	Loss 0.0092 (0.0291)	
training:	Epoch: [34][93/204]	Loss 0.0256 (0.0290)	
training:	Epoch: [34][94/204]	Loss 0.1574 (0.0304)	
training:	Epoch: [34][95/204]	Loss 0.0083 (0.0302)	
training:	Epoch: [34][96/204]	Loss 0.0125 (0.0300)	
training:	Epoch: [34][97/204]	Loss 0.0098 (0.0298)	
training:	Epoch: [34][98/204]	Loss 0.0073 (0.0296)	
training:	Epoch: [34][99/204]	Loss 0.0161 (0.0294)	
training:	Epoch: [34][100/204]	Loss 0.0080 (0.0292)	
training:	Epoch: [34][101/204]	Loss 0.0074 (0.0290)	
training:	Epoch: [34][102/204]	Loss 0.0095 (0.0288)	
training:	Epoch: [34][103/204]	Loss 0.0100 (0.0286)	
training:	Epoch: [34][104/204]	Loss 0.0088 (0.0284)	
training:	Epoch: [34][105/204]	Loss 0.0112 (0.0283)	
training:	Epoch: [34][106/204]	Loss 0.0091 (0.0281)	
training:	Epoch: [34][107/204]	Loss 0.1389 (0.0291)	
training:	Epoch: [34][108/204]	Loss 0.1442 (0.0302)	
training:	Epoch: [34][109/204]	Loss 0.1078 (0.0309)	
training:	Epoch: [34][110/204]	Loss 0.0079 (0.0307)	
training:	Epoch: [34][111/204]	Loss 0.0113 (0.0305)	
training:	Epoch: [34][112/204]	Loss 0.0671 (0.0308)	
training:	Epoch: [34][113/204]	Loss 0.0646 (0.0311)	
training:	Epoch: [34][114/204]	Loss 0.0250 (0.0311)	
training:	Epoch: [34][115/204]	Loss 0.0079 (0.0309)	
training:	Epoch: [34][116/204]	Loss 0.0076 (0.0307)	
training:	Epoch: [34][117/204]	Loss 0.1587 (0.0318)	
training:	Epoch: [34][118/204]	Loss 0.0149 (0.0316)	
training:	Epoch: [34][119/204]	Loss 0.1676 (0.0328)	
training:	Epoch: [34][120/204]	Loss 0.0095 (0.0326)	
training:	Epoch: [34][121/204]	Loss 0.0091 (0.0324)	
training:	Epoch: [34][122/204]	Loss 0.0476 (0.0325)	
training:	Epoch: [34][123/204]	Loss 0.1745 (0.0337)	
training:	Epoch: [34][124/204]	Loss 0.1530 (0.0346)	
training:	Epoch: [34][125/204]	Loss 0.0690 (0.0349)	
training:	Epoch: [34][126/204]	Loss 0.1602 (0.0359)	
training:	Epoch: [34][127/204]	Loss 0.0108 (0.0357)	
training:	Epoch: [34][128/204]	Loss 0.1647 (0.0367)	
training:	Epoch: [34][129/204]	Loss 0.0164 (0.0365)	
training:	Epoch: [34][130/204]	Loss 0.0107 (0.0363)	
training:	Epoch: [34][131/204]	Loss 0.0096 (0.0361)	
training:	Epoch: [34][132/204]	Loss 0.0089 (0.0359)	
training:	Epoch: [34][133/204]	Loss 0.0090 (0.0357)	
training:	Epoch: [34][134/204]	Loss 0.0080 (0.0355)	
training:	Epoch: [34][135/204]	Loss 0.0386 (0.0356)	
training:	Epoch: [34][136/204]	Loss 0.0095 (0.0354)	
training:	Epoch: [34][137/204]	Loss 0.1627 (0.0363)	
training:	Epoch: [34][138/204]	Loss 0.0172 (0.0362)	
training:	Epoch: [34][139/204]	Loss 0.0215 (0.0360)	
training:	Epoch: [34][140/204]	Loss 0.0114 (0.0359)	
training:	Epoch: [34][141/204]	Loss 0.0246 (0.0358)	
training:	Epoch: [34][142/204]	Loss 0.0085 (0.0356)	
training:	Epoch: [34][143/204]	Loss 0.1600 (0.0365)	
training:	Epoch: [34][144/204]	Loss 0.1621 (0.0373)	
training:	Epoch: [34][145/204]	Loss 0.0197 (0.0372)	
training:	Epoch: [34][146/204]	Loss 0.0117 (0.0370)	
training:	Epoch: [34][147/204]	Loss 0.0149 (0.0369)	
training:	Epoch: [34][148/204]	Loss 0.1566 (0.0377)	
training:	Epoch: [34][149/204]	Loss 0.0108 (0.0375)	
training:	Epoch: [34][150/204]	Loss 0.0092 (0.0373)	
training:	Epoch: [34][151/204]	Loss 0.0087 (0.0371)	
training:	Epoch: [34][152/204]	Loss 0.0077 (0.0369)	
training:	Epoch: [34][153/204]	Loss 0.0083 (0.0368)	
training:	Epoch: [34][154/204]	Loss 0.0200 (0.0367)	
training:	Epoch: [34][155/204]	Loss 0.0102 (0.0365)	
training:	Epoch: [34][156/204]	Loss 0.1687 (0.0373)	
training:	Epoch: [34][157/204]	Loss 0.0085 (0.0371)	
training:	Epoch: [34][158/204]	Loss 0.1092 (0.0376)	
training:	Epoch: [34][159/204]	Loss 0.0565 (0.0377)	
training:	Epoch: [34][160/204]	Loss 0.0078 (0.0375)	
training:	Epoch: [34][161/204]	Loss 0.1593 (0.0383)	
training:	Epoch: [34][162/204]	Loss 0.0086 (0.0381)	
training:	Epoch: [34][163/204]	Loss 0.1645 (0.0389)	
training:	Epoch: [34][164/204]	Loss 0.0096 (0.0387)	
training:	Epoch: [34][165/204]	Loss 0.0084 (0.0385)	
training:	Epoch: [34][166/204]	Loss 0.1594 (0.0392)	
training:	Epoch: [34][167/204]	Loss 0.0837 (0.0395)	
training:	Epoch: [34][168/204]	Loss 0.0096 (0.0393)	
training:	Epoch: [34][169/204]	Loss 0.0091 (0.0392)	
training:	Epoch: [34][170/204]	Loss 0.0105 (0.0390)	
training:	Epoch: [34][171/204]	Loss 0.0125 (0.0388)	
training:	Epoch: [34][172/204]	Loss 0.0106 (0.0387)	
training:	Epoch: [34][173/204]	Loss 0.1754 (0.0395)	
training:	Epoch: [34][174/204]	Loss 0.0182 (0.0393)	
training:	Epoch: [34][175/204]	Loss 0.0096 (0.0392)	
training:	Epoch: [34][176/204]	Loss 0.0129 (0.0390)	
training:	Epoch: [34][177/204]	Loss 0.0081 (0.0388)	
training:	Epoch: [34][178/204]	Loss 0.1620 (0.0395)	
training:	Epoch: [34][179/204]	Loss 0.0124 (0.0394)	
training:	Epoch: [34][180/204]	Loss 0.1425 (0.0400)	
training:	Epoch: [34][181/204]	Loss 0.0216 (0.0399)	
training:	Epoch: [34][182/204]	Loss 0.0100 (0.0397)	
training:	Epoch: [34][183/204]	Loss 0.0206 (0.0396)	
training:	Epoch: [34][184/204]	Loss 0.0082 (0.0394)	
training:	Epoch: [34][185/204]	Loss 0.0095 (0.0393)	
training:	Epoch: [34][186/204]	Loss 0.0119 (0.0391)	
training:	Epoch: [34][187/204]	Loss 0.0111 (0.0390)	
training:	Epoch: [34][188/204]	Loss 0.0219 (0.0389)	
training:	Epoch: [34][189/204]	Loss 0.0085 (0.0387)	
training:	Epoch: [34][190/204]	Loss 0.1771 (0.0394)	
training:	Epoch: [34][191/204]	Loss 0.0110 (0.0393)	
training:	Epoch: [34][192/204]	Loss 0.0097 (0.0391)	
training:	Epoch: [34][193/204]	Loss 0.1575 (0.0397)	
training:	Epoch: [34][194/204]	Loss 0.0101 (0.0396)	
training:	Epoch: [34][195/204]	Loss 0.1430 (0.0401)	
training:	Epoch: [34][196/204]	Loss 0.0148 (0.0400)	
training:	Epoch: [34][197/204]	Loss 0.0600 (0.0401)	
training:	Epoch: [34][198/204]	Loss 0.0196 (0.0400)	
training:	Epoch: [34][199/204]	Loss 0.0175 (0.0399)	
training:	Epoch: [34][200/204]	Loss 0.1592 (0.0405)	
training:	Epoch: [34][201/204]	Loss 0.0116 (0.0403)	
training:	Epoch: [34][202/204]	Loss 0.0097 (0.0402)	
training:	Epoch: [34][203/204]	Loss 0.0160 (0.0401)	
training:	Epoch: [34][204/204]	Loss 0.1703 (0.0407)	
Training:	 Loss: 0.0406

Training:	 ACC: 0.9953 0.9954 0.9971 0.9936
Validation:	 ACC: 0.8052 0.8063 0.8301 0.7803
Validation:	 Best_BACC: 0.8326 0.8331 0.8424 0.8229
Validation:	 Loss: 0.7332
Pretraining:	Epoch 35/200
----------
training:	Epoch: [35][1/204]	Loss 0.0499 (0.0499)	
training:	Epoch: [35][2/204]	Loss 0.0080 (0.0290)	
training:	Epoch: [35][3/204]	Loss 0.0088 (0.0222)	
training:	Epoch: [35][4/204]	Loss 0.0097 (0.0191)	
training:	Epoch: [35][5/204]	Loss 0.0094 (0.0172)	
training:	Epoch: [35][6/204]	Loss 0.0104 (0.0160)	
training:	Epoch: [35][7/204]	Loss 0.0093 (0.0151)	
training:	Epoch: [35][8/204]	Loss 0.0942 (0.0250)	
training:	Epoch: [35][9/204]	Loss 0.0111 (0.0234)	
training:	Epoch: [35][10/204]	Loss 0.0271 (0.0238)	
training:	Epoch: [35][11/204]	Loss 0.1414 (0.0345)	
training:	Epoch: [35][12/204]	Loss 0.0146 (0.0328)	
training:	Epoch: [35][13/204]	Loss 0.0121 (0.0312)	
training:	Epoch: [35][14/204]	Loss 0.1590 (0.0404)	
training:	Epoch: [35][15/204]	Loss 0.0106 (0.0384)	
training:	Epoch: [35][16/204]	Loss 0.0084 (0.0365)	
training:	Epoch: [35][17/204]	Loss 0.0191 (0.0355)	
training:	Epoch: [35][18/204]	Loss 0.2047 (0.0449)	
training:	Epoch: [35][19/204]	Loss 0.0180 (0.0435)	
training:	Epoch: [35][20/204]	Loss 0.3583 (0.0592)	
training:	Epoch: [35][21/204]	Loss 0.0297 (0.0578)	
training:	Epoch: [35][22/204]	Loss 0.0086 (0.0556)	
training:	Epoch: [35][23/204]	Loss 0.0122 (0.0537)	
training:	Epoch: [35][24/204]	Loss 0.0083 (0.0518)	
training:	Epoch: [35][25/204]	Loss 0.0114 (0.0502)	
training:	Epoch: [35][26/204]	Loss 0.0193 (0.0490)	
training:	Epoch: [35][27/204]	Loss 0.0890 (0.0505)	
training:	Epoch: [35][28/204]	Loss 0.1123 (0.0527)	
training:	Epoch: [35][29/204]	Loss 0.0978 (0.0542)	
training:	Epoch: [35][30/204]	Loss 0.0086 (0.0527)	
training:	Epoch: [35][31/204]	Loss 0.0097 (0.0513)	
training:	Epoch: [35][32/204]	Loss 0.0106 (0.0500)	
training:	Epoch: [35][33/204]	Loss 0.0123 (0.0489)	
training:	Epoch: [35][34/204]	Loss 0.0128 (0.0478)	
training:	Epoch: [35][35/204]	Loss 0.0088 (0.0467)	
training:	Epoch: [35][36/204]	Loss 0.0267 (0.0462)	
training:	Epoch: [35][37/204]	Loss 0.0118 (0.0452)	
training:	Epoch: [35][38/204]	Loss 0.0181 (0.0445)	
training:	Epoch: [35][39/204]	Loss 0.0111 (0.0437)	
training:	Epoch: [35][40/204]	Loss 0.0574 (0.0440)	
training:	Epoch: [35][41/204]	Loss 0.0095 (0.0432)	
training:	Epoch: [35][42/204]	Loss 0.0257 (0.0428)	
training:	Epoch: [35][43/204]	Loss 0.0289 (0.0424)	
training:	Epoch: [35][44/204]	Loss 0.0083 (0.0417)	
training:	Epoch: [35][45/204]	Loss 0.0082 (0.0409)	
training:	Epoch: [35][46/204]	Loss 0.1704 (0.0437)	
training:	Epoch: [35][47/204]	Loss 0.0073 (0.0430)	
training:	Epoch: [35][48/204]	Loss 0.0110 (0.0423)	
training:	Epoch: [35][49/204]	Loss 0.0128 (0.0417)	
training:	Epoch: [35][50/204]	Loss 0.0098 (0.0410)	
training:	Epoch: [35][51/204]	Loss 0.0096 (0.0404)	
training:	Epoch: [35][52/204]	Loss 0.0525 (0.0407)	
training:	Epoch: [35][53/204]	Loss 0.1524 (0.0428)	
training:	Epoch: [35][54/204]	Loss 0.0099 (0.0422)	
training:	Epoch: [35][55/204]	Loss 0.0107 (0.0416)	
training:	Epoch: [35][56/204]	Loss 0.0186 (0.0412)	
training:	Epoch: [35][57/204]	Loss 0.1597 (0.0433)	
training:	Epoch: [35][58/204]	Loss 0.2481 (0.0468)	
training:	Epoch: [35][59/204]	Loss 0.0121 (0.0462)	
training:	Epoch: [35][60/204]	Loss 0.1193 (0.0474)	
training:	Epoch: [35][61/204]	Loss 0.0083 (0.0468)	
training:	Epoch: [35][62/204]	Loss 0.0090 (0.0462)	
training:	Epoch: [35][63/204]	Loss 0.0123 (0.0456)	
training:	Epoch: [35][64/204]	Loss 0.0116 (0.0451)	
training:	Epoch: [35][65/204]	Loss 0.1705 (0.0470)	
training:	Epoch: [35][66/204]	Loss 0.0219 (0.0466)	
training:	Epoch: [35][67/204]	Loss 0.0418 (0.0466)	
training:	Epoch: [35][68/204]	Loss 0.0290 (0.0463)	
training:	Epoch: [35][69/204]	Loss 0.0091 (0.0458)	
training:	Epoch: [35][70/204]	Loss 0.0080 (0.0452)	
training:	Epoch: [35][71/204]	Loss 0.1692 (0.0470)	
training:	Epoch: [35][72/204]	Loss 0.0481 (0.0470)	
training:	Epoch: [35][73/204]	Loss 0.0597 (0.0472)	
training:	Epoch: [35][74/204]	Loss 0.0107 (0.0467)	
training:	Epoch: [35][75/204]	Loss 0.1649 (0.0483)	
training:	Epoch: [35][76/204]	Loss 0.0496 (0.0483)	
training:	Epoch: [35][77/204]	Loss 0.0153 (0.0478)	
training:	Epoch: [35][78/204]	Loss 0.0455 (0.0478)	
training:	Epoch: [35][79/204]	Loss 0.0484 (0.0478)	
training:	Epoch: [35][80/204]	Loss 0.0081 (0.0473)	
training:	Epoch: [35][81/204]	Loss 0.0083 (0.0468)	
training:	Epoch: [35][82/204]	Loss 0.0086 (0.0464)	
training:	Epoch: [35][83/204]	Loss 0.1199 (0.0473)	
training:	Epoch: [35][84/204]	Loss 0.0106 (0.0468)	
training:	Epoch: [35][85/204]	Loss 0.0084 (0.0464)	
training:	Epoch: [35][86/204]	Loss 0.1618 (0.0477)	
training:	Epoch: [35][87/204]	Loss 0.0164 (0.0474)	
training:	Epoch: [35][88/204]	Loss 0.2002 (0.0491)	
training:	Epoch: [35][89/204]	Loss 0.0108 (0.0487)	
training:	Epoch: [35][90/204]	Loss 0.0088 (0.0482)	
training:	Epoch: [35][91/204]	Loss 0.0093 (0.0478)	
training:	Epoch: [35][92/204]	Loss 0.1572 (0.0490)	
training:	Epoch: [35][93/204]	Loss 0.0121 (0.0486)	
training:	Epoch: [35][94/204]	Loss 0.0116 (0.0482)	
training:	Epoch: [35][95/204]	Loss 0.0193 (0.0479)	
training:	Epoch: [35][96/204]	Loss 0.0088 (0.0475)	
training:	Epoch: [35][97/204]	Loss 0.0178 (0.0472)	
training:	Epoch: [35][98/204]	Loss 0.0341 (0.0470)	
training:	Epoch: [35][99/204]	Loss 0.0136 (0.0467)	
training:	Epoch: [35][100/204]	Loss 0.0100 (0.0463)	
training:	Epoch: [35][101/204]	Loss 0.1404 (0.0473)	
training:	Epoch: [35][102/204]	Loss 0.0135 (0.0469)	
training:	Epoch: [35][103/204]	Loss 0.0079 (0.0466)	
training:	Epoch: [35][104/204]	Loss 0.0092 (0.0462)	
training:	Epoch: [35][105/204]	Loss 0.0166 (0.0459)	
training:	Epoch: [35][106/204]	Loss 0.0087 (0.0456)	
training:	Epoch: [35][107/204]	Loss 0.0201 (0.0453)	
training:	Epoch: [35][108/204]	Loss 0.0171 (0.0451)	
training:	Epoch: [35][109/204]	Loss 0.0087 (0.0447)	
training:	Epoch: [35][110/204]	Loss 0.0120 (0.0444)	
training:	Epoch: [35][111/204]	Loss 0.1716 (0.0456)	
training:	Epoch: [35][112/204]	Loss 0.0089 (0.0453)	
training:	Epoch: [35][113/204]	Loss 0.0110 (0.0449)	
training:	Epoch: [35][114/204]	Loss 0.0119 (0.0447)	
training:	Epoch: [35][115/204]	Loss 0.1549 (0.0456)	
training:	Epoch: [35][116/204]	Loss 0.0290 (0.0455)	
training:	Epoch: [35][117/204]	Loss 0.0091 (0.0452)	
training:	Epoch: [35][118/204]	Loss 0.0431 (0.0451)	
training:	Epoch: [35][119/204]	Loss 0.0080 (0.0448)	
training:	Epoch: [35][120/204]	Loss 0.0139 (0.0446)	
training:	Epoch: [35][121/204]	Loss 0.0593 (0.0447)	
training:	Epoch: [35][122/204]	Loss 0.0109 (0.0444)	
training:	Epoch: [35][123/204]	Loss 0.1535 (0.0453)	
training:	Epoch: [35][124/204]	Loss 0.0098 (0.0450)	
training:	Epoch: [35][125/204]	Loss 0.0190 (0.0448)	
training:	Epoch: [35][126/204]	Loss 0.0089 (0.0445)	
training:	Epoch: [35][127/204]	Loss 0.0153 (0.0443)	
training:	Epoch: [35][128/204]	Loss 0.1580 (0.0452)	
training:	Epoch: [35][129/204]	Loss 0.0170 (0.0450)	
training:	Epoch: [35][130/204]	Loss 0.0578 (0.0451)	
training:	Epoch: [35][131/204]	Loss 0.0078 (0.0448)	
training:	Epoch: [35][132/204]	Loss 0.2575 (0.0464)	
training:	Epoch: [35][133/204]	Loss 0.0207 (0.0462)	
training:	Epoch: [35][134/204]	Loss 0.0107 (0.0459)	
training:	Epoch: [35][135/204]	Loss 0.0072 (0.0456)	
training:	Epoch: [35][136/204]	Loss 0.0121 (0.0454)	
training:	Epoch: [35][137/204]	Loss 0.0082 (0.0451)	
training:	Epoch: [35][138/204]	Loss 0.0091 (0.0449)	
training:	Epoch: [35][139/204]	Loss 0.0105 (0.0446)	
training:	Epoch: [35][140/204]	Loss 0.0083 (0.0444)	
training:	Epoch: [35][141/204]	Loss 0.0170 (0.0442)	
training:	Epoch: [35][142/204]	Loss 0.0089 (0.0439)	
training:	Epoch: [35][143/204]	Loss 0.1601 (0.0447)	
training:	Epoch: [35][144/204]	Loss 0.0138 (0.0445)	
training:	Epoch: [35][145/204]	Loss 0.1378 (0.0452)	
training:	Epoch: [35][146/204]	Loss 0.0714 (0.0453)	
training:	Epoch: [35][147/204]	Loss 0.0161 (0.0451)	
training:	Epoch: [35][148/204]	Loss 0.0081 (0.0449)	
training:	Epoch: [35][149/204]	Loss 0.0757 (0.0451)	
training:	Epoch: [35][150/204]	Loss 0.0096 (0.0449)	
training:	Epoch: [35][151/204]	Loss 0.1745 (0.0457)	
training:	Epoch: [35][152/204]	Loss 0.0079 (0.0455)	
training:	Epoch: [35][153/204]	Loss 0.0233 (0.0453)	
training:	Epoch: [35][154/204]	Loss 0.0117 (0.0451)	
training:	Epoch: [35][155/204]	Loss 0.0120 (0.0449)	
training:	Epoch: [35][156/204]	Loss 0.0325 (0.0448)	
training:	Epoch: [35][157/204]	Loss 0.1699 (0.0456)	
training:	Epoch: [35][158/204]	Loss 0.0093 (0.0454)	
training:	Epoch: [35][159/204]	Loss 0.0322 (0.0453)	
training:	Epoch: [35][160/204]	Loss 0.1725 (0.0461)	
training:	Epoch: [35][161/204]	Loss 0.0119 (0.0459)	
training:	Epoch: [35][162/204]	Loss 0.0104 (0.0457)	
training:	Epoch: [35][163/204]	Loss 0.0088 (0.0454)	
training:	Epoch: [35][164/204]	Loss 0.0253 (0.0453)	
training:	Epoch: [35][165/204]	Loss 0.0093 (0.0451)	
training:	Epoch: [35][166/204]	Loss 0.0078 (0.0449)	
training:	Epoch: [35][167/204]	Loss 0.2488 (0.0461)	
training:	Epoch: [35][168/204]	Loss 0.0171 (0.0459)	
training:	Epoch: [35][169/204]	Loss 0.1590 (0.0466)	
training:	Epoch: [35][170/204]	Loss 0.0099 (0.0464)	
training:	Epoch: [35][171/204]	Loss 0.0144 (0.0462)	
training:	Epoch: [35][172/204]	Loss 0.0077 (0.0460)	
training:	Epoch: [35][173/204]	Loss 0.0086 (0.0457)	
training:	Epoch: [35][174/204]	Loss 0.0588 (0.0458)	
training:	Epoch: [35][175/204]	Loss 0.0164 (0.0457)	
training:	Epoch: [35][176/204]	Loss 0.0081 (0.0454)	
training:	Epoch: [35][177/204]	Loss 0.0082 (0.0452)	
training:	Epoch: [35][178/204]	Loss 0.0331 (0.0452)	
training:	Epoch: [35][179/204]	Loss 0.0397 (0.0451)	
training:	Epoch: [35][180/204]	Loss 0.0076 (0.0449)	
training:	Epoch: [35][181/204]	Loss 0.1613 (0.0456)	
training:	Epoch: [35][182/204]	Loss 0.0312 (0.0455)	
training:	Epoch: [35][183/204]	Loss 0.1299 (0.0459)	
training:	Epoch: [35][184/204]	Loss 0.1427 (0.0465)	
training:	Epoch: [35][185/204]	Loss 0.0855 (0.0467)	
training:	Epoch: [35][186/204]	Loss 0.0441 (0.0467)	
training:	Epoch: [35][187/204]	Loss 0.0127 (0.0465)	
training:	Epoch: [35][188/204]	Loss 0.0130 (0.0463)	
training:	Epoch: [35][189/204]	Loss 0.3123 (0.0477)	
training:	Epoch: [35][190/204]	Loss 0.0096 (0.0475)	
training:	Epoch: [35][191/204]	Loss 0.1448 (0.0480)	
training:	Epoch: [35][192/204]	Loss 0.0099 (0.0478)	
training:	Epoch: [35][193/204]	Loss 0.0224 (0.0477)	
training:	Epoch: [35][194/204]	Loss 0.1560 (0.0483)	
training:	Epoch: [35][195/204]	Loss 0.0102 (0.0481)	
training:	Epoch: [35][196/204]	Loss 0.0519 (0.0481)	
training:	Epoch: [35][197/204]	Loss 0.1730 (0.0487)	
training:	Epoch: [35][198/204]	Loss 0.0094 (0.0485)	
training:	Epoch: [35][199/204]	Loss 0.0118 (0.0483)	
training:	Epoch: [35][200/204]	Loss 0.0113 (0.0481)	
training:	Epoch: [35][201/204]	Loss 0.0154 (0.0480)	
training:	Epoch: [35][202/204]	Loss 0.1164 (0.0483)	
training:	Epoch: [35][203/204]	Loss 0.0081 (0.0481)	
training:	Epoch: [35][204/204]	Loss 0.0093 (0.0479)	
Training:	 Loss: 0.0479

Training:	 ACC: 0.9950 0.9951 0.9971 0.9930
Validation:	 ACC: 0.8043 0.8063 0.8485 0.7601
Validation:	 Best_BACC: 0.8326 0.8331 0.8424 0.8229
Validation:	 Loss: 0.7542
Pretraining:	Epoch 36/200
----------
training:	Epoch: [36][1/204]	Loss 0.1459 (0.1459)	
training:	Epoch: [36][2/204]	Loss 0.0287 (0.0873)	
training:	Epoch: [36][3/204]	Loss 0.0086 (0.0611)	
training:	Epoch: [36][4/204]	Loss 0.1737 (0.0892)	
training:	Epoch: [36][5/204]	Loss 0.0123 (0.0738)	
training:	Epoch: [36][6/204]	Loss 0.0099 (0.0632)	
training:	Epoch: [36][7/204]	Loss 0.0122 (0.0559)	
training:	Epoch: [36][8/204]	Loss 0.0080 (0.0499)	
training:	Epoch: [36][9/204]	Loss 0.0087 (0.0453)	
training:	Epoch: [36][10/204]	Loss 0.2103 (0.0618)	
training:	Epoch: [36][11/204]	Loss 0.0092 (0.0570)	
training:	Epoch: [36][12/204]	Loss 0.0538 (0.0568)	
training:	Epoch: [36][13/204]	Loss 0.0688 (0.0577)	
training:	Epoch: [36][14/204]	Loss 0.0076 (0.0541)	
training:	Epoch: [36][15/204]	Loss 0.0075 (0.0510)	
training:	Epoch: [36][16/204]	Loss 0.0093 (0.0484)	
training:	Epoch: [36][17/204]	Loss 0.0107 (0.0462)	
training:	Epoch: [36][18/204]	Loss 0.0196 (0.0447)	
training:	Epoch: [36][19/204]	Loss 0.0102 (0.0429)	
training:	Epoch: [36][20/204]	Loss 0.0188 (0.0417)	
training:	Epoch: [36][21/204]	Loss 0.0336 (0.0413)	
training:	Epoch: [36][22/204]	Loss 0.0136 (0.0400)	
training:	Epoch: [36][23/204]	Loss 0.1330 (0.0441)	
training:	Epoch: [36][24/204]	Loss 0.0129 (0.0428)	
training:	Epoch: [36][25/204]	Loss 0.0101 (0.0415)	
training:	Epoch: [36][26/204]	Loss 0.3290 (0.0525)	
training:	Epoch: [36][27/204]	Loss 0.0836 (0.0537)	
training:	Epoch: [36][28/204]	Loss 0.0182 (0.0524)	
training:	Epoch: [36][29/204]	Loss 0.0101 (0.0510)	
training:	Epoch: [36][30/204]	Loss 0.0077 (0.0495)	
training:	Epoch: [36][31/204]	Loss 0.1693 (0.0534)	
training:	Epoch: [36][32/204]	Loss 0.0127 (0.0521)	
training:	Epoch: [36][33/204]	Loss 0.1262 (0.0544)	
training:	Epoch: [36][34/204]	Loss 0.0095 (0.0530)	
training:	Epoch: [36][35/204]	Loss 0.0093 (0.0518)	
training:	Epoch: [36][36/204]	Loss 0.0136 (0.0507)	
training:	Epoch: [36][37/204]	Loss 0.0500 (0.0507)	
training:	Epoch: [36][38/204]	Loss 0.0091 (0.0496)	
training:	Epoch: [36][39/204]	Loss 0.0136 (0.0487)	
training:	Epoch: [36][40/204]	Loss 0.0093 (0.0477)	
training:	Epoch: [36][41/204]	Loss 0.0093 (0.0468)	
training:	Epoch: [36][42/204]	Loss 0.0104 (0.0459)	
training:	Epoch: [36][43/204]	Loss 0.0094 (0.0451)	
training:	Epoch: [36][44/204]	Loss 0.0089 (0.0442)	
training:	Epoch: [36][45/204]	Loss 0.0091 (0.0435)	
training:	Epoch: [36][46/204]	Loss 0.0080 (0.0427)	
training:	Epoch: [36][47/204]	Loss 0.0274 (0.0424)	
training:	Epoch: [36][48/204]	Loss 0.0594 (0.0427)	
training:	Epoch: [36][49/204]	Loss 0.0103 (0.0420)	
training:	Epoch: [36][50/204]	Loss 0.0093 (0.0414)	
training:	Epoch: [36][51/204]	Loss 0.1597 (0.0437)	
training:	Epoch: [36][52/204]	Loss 0.0180 (0.0432)	
training:	Epoch: [36][53/204]	Loss 0.0378 (0.0431)	
training:	Epoch: [36][54/204]	Loss 0.0109 (0.0425)	
training:	Epoch: [36][55/204]	Loss 0.1670 (0.0448)	
training:	Epoch: [36][56/204]	Loss 0.0124 (0.0442)	
training:	Epoch: [36][57/204]	Loss 0.0110 (0.0436)	
training:	Epoch: [36][58/204]	Loss 0.1631 (0.0457)	
training:	Epoch: [36][59/204]	Loss 0.0266 (0.0454)	
training:	Epoch: [36][60/204]	Loss 0.0085 (0.0447)	
training:	Epoch: [36][61/204]	Loss 0.0115 (0.0442)	
training:	Epoch: [36][62/204]	Loss 0.0084 (0.0436)	
training:	Epoch: [36][63/204]	Loss 0.1603 (0.0455)	
training:	Epoch: [36][64/204]	Loss 0.0082 (0.0449)	
training:	Epoch: [36][65/204]	Loss 0.0533 (0.0450)	
training:	Epoch: [36][66/204]	Loss 0.0561 (0.0452)	
training:	Epoch: [36][67/204]	Loss 0.0787 (0.0457)	
training:	Epoch: [36][68/204]	Loss 0.0519 (0.0458)	
training:	Epoch: [36][69/204]	Loss 0.0086 (0.0452)	
training:	Epoch: [36][70/204]	Loss 0.1554 (0.0468)	
training:	Epoch: [36][71/204]	Loss 0.0110 (0.0463)	
training:	Epoch: [36][72/204]	Loss 0.0073 (0.0458)	
training:	Epoch: [36][73/204]	Loss 0.0258 (0.0455)	
training:	Epoch: [36][74/204]	Loss 0.0113 (0.0450)	
training:	Epoch: [36][75/204]	Loss 0.0243 (0.0448)	
training:	Epoch: [36][76/204]	Loss 0.0242 (0.0445)	
training:	Epoch: [36][77/204]	Loss 0.0111 (0.0441)	
training:	Epoch: [36][78/204]	Loss 0.0148 (0.0437)	
training:	Epoch: [36][79/204]	Loss 0.0077 (0.0432)	
training:	Epoch: [36][80/204]	Loss 0.0099 (0.0428)	
training:	Epoch: [36][81/204]	Loss 0.0077 (0.0424)	
training:	Epoch: [36][82/204]	Loss 0.1556 (0.0438)	
training:	Epoch: [36][83/204]	Loss 0.1565 (0.0451)	
training:	Epoch: [36][84/204]	Loss 0.0369 (0.0450)	
training:	Epoch: [36][85/204]	Loss 0.0102 (0.0446)	
training:	Epoch: [36][86/204]	Loss 0.0143 (0.0443)	
training:	Epoch: [36][87/204]	Loss 0.1748 (0.0458)	
training:	Epoch: [36][88/204]	Loss 0.1524 (0.0470)	
training:	Epoch: [36][89/204]	Loss 0.1555 (0.0482)	
training:	Epoch: [36][90/204]	Loss 0.0201 (0.0479)	
training:	Epoch: [36][91/204]	Loss 0.0101 (0.0475)	
training:	Epoch: [36][92/204]	Loss 0.0151 (0.0471)	
training:	Epoch: [36][93/204]	Loss 0.1495 (0.0482)	
training:	Epoch: [36][94/204]	Loss 0.0098 (0.0478)	
training:	Epoch: [36][95/204]	Loss 0.0082 (0.0474)	
training:	Epoch: [36][96/204]	Loss 0.0174 (0.0471)	
training:	Epoch: [36][97/204]	Loss 0.0104 (0.0467)	
training:	Epoch: [36][98/204]	Loss 0.0121 (0.0463)	
training:	Epoch: [36][99/204]	Loss 0.0080 (0.0460)	
training:	Epoch: [36][100/204]	Loss 0.0076 (0.0456)	
training:	Epoch: [36][101/204]	Loss 0.0092 (0.0452)	
training:	Epoch: [36][102/204]	Loss 0.0106 (0.0449)	
training:	Epoch: [36][103/204]	Loss 0.1046 (0.0454)	
training:	Epoch: [36][104/204]	Loss 0.0149 (0.0452)	
training:	Epoch: [36][105/204]	Loss 0.0150 (0.0449)	
training:	Epoch: [36][106/204]	Loss 0.0074 (0.0445)	
training:	Epoch: [36][107/204]	Loss 0.0071 (0.0442)	
training:	Epoch: [36][108/204]	Loss 0.0454 (0.0442)	
training:	Epoch: [36][109/204]	Loss 0.0982 (0.0447)	
training:	Epoch: [36][110/204]	Loss 0.0123 (0.0444)	
training:	Epoch: [36][111/204]	Loss 0.0071 (0.0440)	
training:	Epoch: [36][112/204]	Loss 0.1035 (0.0446)	
training:	Epoch: [36][113/204]	Loss 0.0151 (0.0443)	
training:	Epoch: [36][114/204]	Loss 0.0071 (0.0440)	
training:	Epoch: [36][115/204]	Loss 0.0248 (0.0438)	
training:	Epoch: [36][116/204]	Loss 0.1579 (0.0448)	
training:	Epoch: [36][117/204]	Loss 0.0083 (0.0445)	
training:	Epoch: [36][118/204]	Loss 0.0099 (0.0442)	
training:	Epoch: [36][119/204]	Loss 0.0081 (0.0439)	
training:	Epoch: [36][120/204]	Loss 0.0119 (0.0436)	
training:	Epoch: [36][121/204]	Loss 0.1504 (0.0445)	
training:	Epoch: [36][122/204]	Loss 0.1000 (0.0450)	
training:	Epoch: [36][123/204]	Loss 0.1129 (0.0455)	
training:	Epoch: [36][124/204]	Loss 0.1544 (0.0464)	
training:	Epoch: [36][125/204]	Loss 0.1661 (0.0474)	
training:	Epoch: [36][126/204]	Loss 0.0082 (0.0470)	
training:	Epoch: [36][127/204]	Loss 0.0255 (0.0469)	
training:	Epoch: [36][128/204]	Loss 0.0401 (0.0468)	
training:	Epoch: [36][129/204]	Loss 0.0109 (0.0465)	
training:	Epoch: [36][130/204]	Loss 0.0090 (0.0463)	
training:	Epoch: [36][131/204]	Loss 0.1070 (0.0467)	
training:	Epoch: [36][132/204]	Loss 0.0071 (0.0464)	
training:	Epoch: [36][133/204]	Loss 0.0290 (0.0463)	
training:	Epoch: [36][134/204]	Loss 0.0072 (0.0460)	
training:	Epoch: [36][135/204]	Loss 0.0077 (0.0457)	
training:	Epoch: [36][136/204]	Loss 0.0507 (0.0458)	
training:	Epoch: [36][137/204]	Loss 0.0188 (0.0456)	
training:	Epoch: [36][138/204]	Loss 0.0080 (0.0453)	
training:	Epoch: [36][139/204]	Loss 0.0101 (0.0450)	
training:	Epoch: [36][140/204]	Loss 0.0087 (0.0448)	
training:	Epoch: [36][141/204]	Loss 0.0101 (0.0445)	
training:	Epoch: [36][142/204]	Loss 0.0090 (0.0443)	
training:	Epoch: [36][143/204]	Loss 0.0077 (0.0440)	
training:	Epoch: [36][144/204]	Loss 0.1427 (0.0447)	
training:	Epoch: [36][145/204]	Loss 0.1356 (0.0453)	
training:	Epoch: [36][146/204]	Loss 0.1095 (0.0458)	
training:	Epoch: [36][147/204]	Loss 0.0094 (0.0455)	
training:	Epoch: [36][148/204]	Loss 0.0973 (0.0459)	
training:	Epoch: [36][149/204]	Loss 0.1757 (0.0467)	
training:	Epoch: [36][150/204]	Loss 0.0084 (0.0465)	
training:	Epoch: [36][151/204]	Loss 0.0128 (0.0463)	
training:	Epoch: [36][152/204]	Loss 0.0234 (0.0461)	
training:	Epoch: [36][153/204]	Loss 0.0153 (0.0459)	
training:	Epoch: [36][154/204]	Loss 0.0097 (0.0457)	
training:	Epoch: [36][155/204]	Loss 0.0941 (0.0460)	
training:	Epoch: [36][156/204]	Loss 0.0092 (0.0458)	
training:	Epoch: [36][157/204]	Loss 0.0085 (0.0455)	
training:	Epoch: [36][158/204]	Loss 0.0097 (0.0453)	
training:	Epoch: [36][159/204]	Loss 0.0085 (0.0451)	
training:	Epoch: [36][160/204]	Loss 0.0095 (0.0448)	
training:	Epoch: [36][161/204]	Loss 0.1561 (0.0455)	
training:	Epoch: [36][162/204]	Loss 0.0086 (0.0453)	
training:	Epoch: [36][163/204]	Loss 0.0227 (0.0452)	
training:	Epoch: [36][164/204]	Loss 0.0182 (0.0450)	
training:	Epoch: [36][165/204]	Loss 0.0086 (0.0448)	
training:	Epoch: [36][166/204]	Loss 0.0150 (0.0446)	
training:	Epoch: [36][167/204]	Loss 0.0297 (0.0445)	
training:	Epoch: [36][168/204]	Loss 0.0102 (0.0443)	
training:	Epoch: [36][169/204]	Loss 0.0572 (0.0444)	
training:	Epoch: [36][170/204]	Loss 0.0088 (0.0442)	
training:	Epoch: [36][171/204]	Loss 0.0118 (0.0440)	
training:	Epoch: [36][172/204]	Loss 0.0083 (0.0438)	
training:	Epoch: [36][173/204]	Loss 0.0101 (0.0436)	
training:	Epoch: [36][174/204]	Loss 0.0076 (0.0434)	
training:	Epoch: [36][175/204]	Loss 0.0081 (0.0432)	
training:	Epoch: [36][176/204]	Loss 0.0069 (0.0430)	
training:	Epoch: [36][177/204]	Loss 0.0489 (0.0430)	
training:	Epoch: [36][178/204]	Loss 0.0091 (0.0428)	
training:	Epoch: [36][179/204]	Loss 0.1588 (0.0435)	
training:	Epoch: [36][180/204]	Loss 0.0080 (0.0433)	
training:	Epoch: [36][181/204]	Loss 0.0112 (0.0431)	
training:	Epoch: [36][182/204]	Loss 0.0981 (0.0434)	
training:	Epoch: [36][183/204]	Loss 0.2158 (0.0443)	
training:	Epoch: [36][184/204]	Loss 0.0077 (0.0441)	
training:	Epoch: [36][185/204]	Loss 0.0086 (0.0439)	
training:	Epoch: [36][186/204]	Loss 0.0192 (0.0438)	
training:	Epoch: [36][187/204]	Loss 0.0154 (0.0437)	
training:	Epoch: [36][188/204]	Loss 0.0070 (0.0435)	
training:	Epoch: [36][189/204]	Loss 0.0609 (0.0435)	
training:	Epoch: [36][190/204]	Loss 0.0086 (0.0434)	
training:	Epoch: [36][191/204]	Loss 0.0082 (0.0432)	
training:	Epoch: [36][192/204]	Loss 0.1707 (0.0438)	
training:	Epoch: [36][193/204]	Loss 0.0170 (0.0437)	
training:	Epoch: [36][194/204]	Loss 0.0753 (0.0439)	
training:	Epoch: [36][195/204]	Loss 0.0086 (0.0437)	
training:	Epoch: [36][196/204]	Loss 0.0110 (0.0435)	
training:	Epoch: [36][197/204]	Loss 0.0273 (0.0434)	
training:	Epoch: [36][198/204]	Loss 0.1310 (0.0439)	
training:	Epoch: [36][199/204]	Loss 0.0113 (0.0437)	
training:	Epoch: [36][200/204]	Loss 0.0084 (0.0435)	
training:	Epoch: [36][201/204]	Loss 0.0117 (0.0434)	
training:	Epoch: [36][202/204]	Loss 0.0100 (0.0432)	
training:	Epoch: [36][203/204]	Loss 0.0127 (0.0431)	
training:	Epoch: [36][204/204]	Loss 0.0072 (0.0429)	
Training:	 Loss: 0.0428

Training:	 ACC: 0.9955 0.9956 0.9968 0.9943
Validation:	 ACC: 0.8020 0.8015 0.7902 0.8139
Validation:	 Best_BACC: 0.8326 0.8331 0.8424 0.8229
Validation:	 Loss: 0.7747
Pretraining:	Epoch 37/200
----------
training:	Epoch: [37][1/204]	Loss 0.0146 (0.0146)	
training:	Epoch: [37][2/204]	Loss 0.0112 (0.0129)	
training:	Epoch: [37][3/204]	Loss 0.1309 (0.0522)	
training:	Epoch: [37][4/204]	Loss 0.0077 (0.0411)	
training:	Epoch: [37][5/204]	Loss 0.0085 (0.0346)	
training:	Epoch: [37][6/204]	Loss 0.0126 (0.0309)	
training:	Epoch: [37][7/204]	Loss 0.0377 (0.0319)	
training:	Epoch: [37][8/204]	Loss 0.0480 (0.0339)	
training:	Epoch: [37][9/204]	Loss 0.0078 (0.0310)	
training:	Epoch: [37][10/204]	Loss 0.1509 (0.0430)	
training:	Epoch: [37][11/204]	Loss 0.0090 (0.0399)	
training:	Epoch: [37][12/204]	Loss 0.0120 (0.0376)	
training:	Epoch: [37][13/204]	Loss 0.0095 (0.0354)	
training:	Epoch: [37][14/204]	Loss 0.0067 (0.0334)	
training:	Epoch: [37][15/204]	Loss 0.0219 (0.0326)	
training:	Epoch: [37][16/204]	Loss 0.0097 (0.0312)	
training:	Epoch: [37][17/204]	Loss 0.0389 (0.0316)	
training:	Epoch: [37][18/204]	Loss 0.0369 (0.0319)	
training:	Epoch: [37][19/204]	Loss 0.0454 (0.0326)	
training:	Epoch: [37][20/204]	Loss 0.3264 (0.0473)	
training:	Epoch: [37][21/204]	Loss 0.0071 (0.0454)	
training:	Epoch: [37][22/204]	Loss 0.0075 (0.0437)	
training:	Epoch: [37][23/204]	Loss 0.0103 (0.0422)	
training:	Epoch: [37][24/204]	Loss 0.0072 (0.0408)	
training:	Epoch: [37][25/204]	Loss 0.0080 (0.0395)	
training:	Epoch: [37][26/204]	Loss 0.0263 (0.0390)	
training:	Epoch: [37][27/204]	Loss 0.0094 (0.0379)	
training:	Epoch: [37][28/204]	Loss 0.1422 (0.0416)	
training:	Epoch: [37][29/204]	Loss 0.0068 (0.0404)	
training:	Epoch: [37][30/204]	Loss 0.1658 (0.0446)	
training:	Epoch: [37][31/204]	Loss 0.0069 (0.0434)	
training:	Epoch: [37][32/204]	Loss 0.0077 (0.0422)	
training:	Epoch: [37][33/204]	Loss 0.0076 (0.0412)	
training:	Epoch: [37][34/204]	Loss 0.0098 (0.0403)	
training:	Epoch: [37][35/204]	Loss 0.0068 (0.0393)	
training:	Epoch: [37][36/204]	Loss 0.0155 (0.0386)	
training:	Epoch: [37][37/204]	Loss 0.0078 (0.0378)	
training:	Epoch: [37][38/204]	Loss 0.0075 (0.0370)	
training:	Epoch: [37][39/204]	Loss 0.1624 (0.0402)	
training:	Epoch: [37][40/204]	Loss 0.0392 (0.0402)	
training:	Epoch: [37][41/204]	Loss 0.0094 (0.0395)	
training:	Epoch: [37][42/204]	Loss 0.2573 (0.0446)	
training:	Epoch: [37][43/204]	Loss 0.0088 (0.0438)	
training:	Epoch: [37][44/204]	Loss 0.0358 (0.0436)	
training:	Epoch: [37][45/204]	Loss 0.0072 (0.0428)	
training:	Epoch: [37][46/204]	Loss 0.0066 (0.0420)	
training:	Epoch: [37][47/204]	Loss 0.0087 (0.0413)	
training:	Epoch: [37][48/204]	Loss 0.0112 (0.0407)	
training:	Epoch: [37][49/204]	Loss 0.1833 (0.0436)	
training:	Epoch: [37][50/204]	Loss 0.0192 (0.0431)	
training:	Epoch: [37][51/204]	Loss 0.2996 (0.0481)	
training:	Epoch: [37][52/204]	Loss 0.0298 (0.0478)	
training:	Epoch: [37][53/204]	Loss 0.0077 (0.0470)	
training:	Epoch: [37][54/204]	Loss 0.0078 (0.0463)	
training:	Epoch: [37][55/204]	Loss 0.0112 (0.0457)	
training:	Epoch: [37][56/204]	Loss 0.0081 (0.0450)	
training:	Epoch: [37][57/204]	Loss 0.0095 (0.0444)	
training:	Epoch: [37][58/204]	Loss 0.0092 (0.0438)	
training:	Epoch: [37][59/204]	Loss 0.1199 (0.0451)	
training:	Epoch: [37][60/204]	Loss 0.0312 (0.0448)	
training:	Epoch: [37][61/204]	Loss 0.0082 (0.0442)	
training:	Epoch: [37][62/204]	Loss 0.0077 (0.0436)	
training:	Epoch: [37][63/204]	Loss 0.0159 (0.0432)	
training:	Epoch: [37][64/204]	Loss 0.1107 (0.0443)	
training:	Epoch: [37][65/204]	Loss 0.0262 (0.0440)	
training:	Epoch: [37][66/204]	Loss 0.0355 (0.0438)	
training:	Epoch: [37][67/204]	Loss 0.1603 (0.0456)	
training:	Epoch: [37][68/204]	Loss 0.0091 (0.0451)	
training:	Epoch: [37][69/204]	Loss 0.1704 (0.0469)	
training:	Epoch: [37][70/204]	Loss 0.0074 (0.0463)	
training:	Epoch: [37][71/204]	Loss 0.0080 (0.0458)	
training:	Epoch: [37][72/204]	Loss 0.0152 (0.0453)	
training:	Epoch: [37][73/204]	Loss 0.0135 (0.0449)	
training:	Epoch: [37][74/204]	Loss 0.0090 (0.0444)	
training:	Epoch: [37][75/204]	Loss 0.0084 (0.0439)	
training:	Epoch: [37][76/204]	Loss 0.0110 (0.0435)	
training:	Epoch: [37][77/204]	Loss 0.0262 (0.0433)	
training:	Epoch: [37][78/204]	Loss 0.0165 (0.0429)	
training:	Epoch: [37][79/204]	Loss 0.0068 (0.0425)	
training:	Epoch: [37][80/204]	Loss 0.0069 (0.0420)	
training:	Epoch: [37][81/204]	Loss 0.0100 (0.0416)	
training:	Epoch: [37][82/204]	Loss 0.0113 (0.0413)	
training:	Epoch: [37][83/204]	Loss 0.0063 (0.0408)	
training:	Epoch: [37][84/204]	Loss 0.0152 (0.0405)	
training:	Epoch: [37][85/204]	Loss 0.1693 (0.0421)	
training:	Epoch: [37][86/204]	Loss 0.0075 (0.0417)	
training:	Epoch: [37][87/204]	Loss 0.0063 (0.0412)	
training:	Epoch: [37][88/204]	Loss 0.0075 (0.0409)	
training:	Epoch: [37][89/204]	Loss 0.0108 (0.0405)	
training:	Epoch: [37][90/204]	Loss 0.0323 (0.0404)	
training:	Epoch: [37][91/204]	Loss 0.0084 (0.0401)	
training:	Epoch: [37][92/204]	Loss 0.0082 (0.0397)	
training:	Epoch: [37][93/204]	Loss 0.0078 (0.0394)	
training:	Epoch: [37][94/204]	Loss 0.0083 (0.0391)	
training:	Epoch: [37][95/204]	Loss 0.0311 (0.0390)	
training:	Epoch: [37][96/204]	Loss 0.0095 (0.0387)	
training:	Epoch: [37][97/204]	Loss 0.1330 (0.0396)	
training:	Epoch: [37][98/204]	Loss 0.0069 (0.0393)	
training:	Epoch: [37][99/204]	Loss 0.0079 (0.0390)	
training:	Epoch: [37][100/204]	Loss 0.0074 (0.0387)	
training:	Epoch: [37][101/204]	Loss 0.0079 (0.0384)	
training:	Epoch: [37][102/204]	Loss 0.0102 (0.0381)	
training:	Epoch: [37][103/204]	Loss 0.0088 (0.0378)	
training:	Epoch: [37][104/204]	Loss 0.0103 (0.0375)	
training:	Epoch: [37][105/204]	Loss 0.0510 (0.0377)	
training:	Epoch: [37][106/204]	Loss 0.0078 (0.0374)	
training:	Epoch: [37][107/204]	Loss 0.0196 (0.0372)	
training:	Epoch: [37][108/204]	Loss 0.1589 (0.0384)	
training:	Epoch: [37][109/204]	Loss 0.0089 (0.0381)	
training:	Epoch: [37][110/204]	Loss 0.0093 (0.0378)	
training:	Epoch: [37][111/204]	Loss 0.0168 (0.0376)	
training:	Epoch: [37][112/204]	Loss 0.0198 (0.0375)	
training:	Epoch: [37][113/204]	Loss 0.0375 (0.0375)	
training:	Epoch: [37][114/204]	Loss 0.0095 (0.0372)	
training:	Epoch: [37][115/204]	Loss 0.0069 (0.0370)	
training:	Epoch: [37][116/204]	Loss 0.0634 (0.0372)	
training:	Epoch: [37][117/204]	Loss 0.0067 (0.0369)	
training:	Epoch: [37][118/204]	Loss 0.1477 (0.0379)	
training:	Epoch: [37][119/204]	Loss 0.0331 (0.0378)	
training:	Epoch: [37][120/204]	Loss 0.0463 (0.0379)	
training:	Epoch: [37][121/204]	Loss 0.0092 (0.0377)	
training:	Epoch: [37][122/204]	Loss 0.1398 (0.0385)	
training:	Epoch: [37][123/204]	Loss 0.0078 (0.0383)	
training:	Epoch: [37][124/204]	Loss 0.0709 (0.0385)	
training:	Epoch: [37][125/204]	Loss 0.2578 (0.0403)	
training:	Epoch: [37][126/204]	Loss 0.0093 (0.0400)	
training:	Epoch: [37][127/204]	Loss 0.0110 (0.0398)	
training:	Epoch: [37][128/204]	Loss 0.0074 (0.0395)	
training:	Epoch: [37][129/204]	Loss 0.0448 (0.0396)	
training:	Epoch: [37][130/204]	Loss 0.0060 (0.0393)	
training:	Epoch: [37][131/204]	Loss 0.0091 (0.0391)	
training:	Epoch: [37][132/204]	Loss 0.1603 (0.0400)	
training:	Epoch: [37][133/204]	Loss 0.0074 (0.0398)	
training:	Epoch: [37][134/204]	Loss 0.1578 (0.0406)	
training:	Epoch: [37][135/204]	Loss 0.0197 (0.0405)	
training:	Epoch: [37][136/204]	Loss 0.0076 (0.0402)	
training:	Epoch: [37][137/204]	Loss 0.0085 (0.0400)	
training:	Epoch: [37][138/204]	Loss 0.0084 (0.0398)	
training:	Epoch: [37][139/204]	Loss 0.0114 (0.0396)	
training:	Epoch: [37][140/204]	Loss 0.1409 (0.0403)	
training:	Epoch: [37][141/204]	Loss 0.1636 (0.0412)	
training:	Epoch: [37][142/204]	Loss 0.0105 (0.0410)	
training:	Epoch: [37][143/204]	Loss 0.0071 (0.0407)	
training:	Epoch: [37][144/204]	Loss 0.0076 (0.0405)	
training:	Epoch: [37][145/204]	Loss 0.1489 (0.0412)	
training:	Epoch: [37][146/204]	Loss 0.0798 (0.0415)	
training:	Epoch: [37][147/204]	Loss 0.0169 (0.0413)	
training:	Epoch: [37][148/204]	Loss 0.0236 (0.0412)	
training:	Epoch: [37][149/204]	Loss 0.0216 (0.0411)	
training:	Epoch: [37][150/204]	Loss 0.0106 (0.0409)	
training:	Epoch: [37][151/204]	Loss 0.0104 (0.0407)	
training:	Epoch: [37][152/204]	Loss 0.0082 (0.0405)	
training:	Epoch: [37][153/204]	Loss 0.0086 (0.0403)	
training:	Epoch: [37][154/204]	Loss 0.0144 (0.0401)	
training:	Epoch: [37][155/204]	Loss 0.0093 (0.0399)	
training:	Epoch: [37][156/204]	Loss 0.0068 (0.0397)	
training:	Epoch: [37][157/204]	Loss 0.0247 (0.0396)	
training:	Epoch: [37][158/204]	Loss 0.1628 (0.0404)	
training:	Epoch: [37][159/204]	Loss 0.1782 (0.0412)	
training:	Epoch: [37][160/204]	Loss 0.0069 (0.0410)	
training:	Epoch: [37][161/204]	Loss 0.0279 (0.0409)	
training:	Epoch: [37][162/204]	Loss 0.0083 (0.0407)	
training:	Epoch: [37][163/204]	Loss 0.1398 (0.0413)	
training:	Epoch: [37][164/204]	Loss 0.0090 (0.0411)	
training:	Epoch: [37][165/204]	Loss 0.0143 (0.0410)	
training:	Epoch: [37][166/204]	Loss 0.1527 (0.0417)	
training:	Epoch: [37][167/204]	Loss 0.1701 (0.0424)	
training:	Epoch: [37][168/204]	Loss 0.0138 (0.0423)	
training:	Epoch: [37][169/204]	Loss 0.0380 (0.0422)	
training:	Epoch: [37][170/204]	Loss 0.1053 (0.0426)	
training:	Epoch: [37][171/204]	Loss 0.0102 (0.0424)	
training:	Epoch: [37][172/204]	Loss 0.1827 (0.0432)	
training:	Epoch: [37][173/204]	Loss 0.0149 (0.0431)	
training:	Epoch: [37][174/204]	Loss 0.0070 (0.0429)	
training:	Epoch: [37][175/204]	Loss 0.0070 (0.0427)	
training:	Epoch: [37][176/204]	Loss 0.0079 (0.0425)	
training:	Epoch: [37][177/204]	Loss 0.0080 (0.0423)	
training:	Epoch: [37][178/204]	Loss 0.0076 (0.0421)	
training:	Epoch: [37][179/204]	Loss 0.0110 (0.0419)	
training:	Epoch: [37][180/204]	Loss 0.0069 (0.0417)	
training:	Epoch: [37][181/204]	Loss 0.0078 (0.0415)	
training:	Epoch: [37][182/204]	Loss 0.0075 (0.0413)	
training:	Epoch: [37][183/204]	Loss 0.0959 (0.0416)	
training:	Epoch: [37][184/204]	Loss 0.0101 (0.0415)	
training:	Epoch: [37][185/204]	Loss 0.0088 (0.0413)	
training:	Epoch: [37][186/204]	Loss 0.1364 (0.0418)	
training:	Epoch: [37][187/204]	Loss 0.0103 (0.0416)	
training:	Epoch: [37][188/204]	Loss 0.0118 (0.0415)	
training:	Epoch: [37][189/204]	Loss 0.0074 (0.0413)	
training:	Epoch: [37][190/204]	Loss 0.0092 (0.0411)	
training:	Epoch: [37][191/204]	Loss 0.1506 (0.0417)	
training:	Epoch: [37][192/204]	Loss 0.1627 (0.0423)	
training:	Epoch: [37][193/204]	Loss 0.1568 (0.0429)	
training:	Epoch: [37][194/204]	Loss 0.0095 (0.0427)	
training:	Epoch: [37][195/204]	Loss 0.1676 (0.0434)	
training:	Epoch: [37][196/204]	Loss 0.0083 (0.0432)	
training:	Epoch: [37][197/204]	Loss 0.0074 (0.0430)	
training:	Epoch: [37][198/204]	Loss 0.1488 (0.0436)	
training:	Epoch: [37][199/204]	Loss 0.0138 (0.0434)	
training:	Epoch: [37][200/204]	Loss 0.0074 (0.0432)	
training:	Epoch: [37][201/204]	Loss 0.0112 (0.0431)	
training:	Epoch: [37][202/204]	Loss 0.0084 (0.0429)	
training:	Epoch: [37][203/204]	Loss 0.0074 (0.0427)	
training:	Epoch: [37][204/204]	Loss 0.0073 (0.0425)	
Training:	 Loss: 0.0425

Training:	 ACC: 0.9957 0.9957 0.9971 0.9943
Validation:	 ACC: 0.8004 0.8010 0.8137 0.7870
Validation:	 Best_BACC: 0.8326 0.8331 0.8424 0.8229
Validation:	 Loss: 0.7777
Pretraining:	Epoch 38/200
----------
training:	Epoch: [38][1/204]	Loss 0.0071 (0.0071)	
training:	Epoch: [38][2/204]	Loss 0.0080 (0.0076)	
training:	Epoch: [38][3/204]	Loss 0.3164 (0.1105)	
training:	Epoch: [38][4/204]	Loss 0.1175 (0.1122)	
training:	Epoch: [38][5/204]	Loss 0.0095 (0.0917)	
training:	Epoch: [38][6/204]	Loss 0.0073 (0.0776)	
training:	Epoch: [38][7/204]	Loss 0.0080 (0.0677)	
training:	Epoch: [38][8/204]	Loss 0.0073 (0.0601)	
training:	Epoch: [38][9/204]	Loss 0.0087 (0.0544)	
training:	Epoch: [38][10/204]	Loss 0.0097 (0.0499)	
training:	Epoch: [38][11/204]	Loss 0.0083 (0.0462)	
training:	Epoch: [38][12/204]	Loss 0.0159 (0.0436)	
training:	Epoch: [38][13/204]	Loss 0.0071 (0.0408)	
training:	Epoch: [38][14/204]	Loss 0.0095 (0.0386)	
training:	Epoch: [38][15/204]	Loss 0.0074 (0.0365)	
training:	Epoch: [38][16/204]	Loss 0.0829 (0.0394)	
training:	Epoch: [38][17/204]	Loss 0.0079 (0.0376)	
training:	Epoch: [38][18/204]	Loss 0.0075 (0.0359)	
training:	Epoch: [38][19/204]	Loss 0.1583 (0.0423)	
training:	Epoch: [38][20/204]	Loss 0.0066 (0.0405)	
training:	Epoch: [38][21/204]	Loss 0.0214 (0.0396)	
training:	Epoch: [38][22/204]	Loss 0.0118 (0.0384)	
training:	Epoch: [38][23/204]	Loss 0.0108 (0.0372)	
training:	Epoch: [38][24/204]	Loss 0.0075 (0.0359)	
training:	Epoch: [38][25/204]	Loss 0.0173 (0.0352)	
training:	Epoch: [38][26/204]	Loss 0.0073 (0.0341)	
training:	Epoch: [38][27/204]	Loss 0.0080 (0.0331)	
training:	Epoch: [38][28/204]	Loss 0.0095 (0.0323)	
training:	Epoch: [38][29/204]	Loss 0.0070 (0.0314)	
training:	Epoch: [38][30/204]	Loss 0.0071 (0.0306)	
training:	Epoch: [38][31/204]	Loss 0.1418 (0.0342)	
training:	Epoch: [38][32/204]	Loss 0.0082 (0.0334)	
training:	Epoch: [38][33/204]	Loss 0.0103 (0.0327)	
training:	Epoch: [38][34/204]	Loss 0.0106 (0.0320)	
training:	Epoch: [38][35/204]	Loss 0.0079 (0.0313)	
training:	Epoch: [38][36/204]	Loss 0.1077 (0.0335)	
training:	Epoch: [38][37/204]	Loss 0.0521 (0.0340)	
training:	Epoch: [38][38/204]	Loss 0.0620 (0.0347)	
training:	Epoch: [38][39/204]	Loss 0.0211 (0.0344)	
training:	Epoch: [38][40/204]	Loss 0.0078 (0.0337)	
training:	Epoch: [38][41/204]	Loss 0.0095 (0.0331)	
training:	Epoch: [38][42/204]	Loss 0.0299 (0.0330)	
training:	Epoch: [38][43/204]	Loss 0.0071 (0.0324)	
training:	Epoch: [38][44/204]	Loss 0.0077 (0.0319)	
training:	Epoch: [38][45/204]	Loss 0.0095 (0.0314)	
training:	Epoch: [38][46/204]	Loss 0.0119 (0.0309)	
training:	Epoch: [38][47/204]	Loss 0.0095 (0.0305)	
training:	Epoch: [38][48/204]	Loss 0.0092 (0.0300)	
training:	Epoch: [38][49/204]	Loss 0.1575 (0.0326)	
training:	Epoch: [38][50/204]	Loss 0.0153 (0.0323)	
training:	Epoch: [38][51/204]	Loss 0.0078 (0.0318)	
training:	Epoch: [38][52/204]	Loss 0.0114 (0.0314)	
training:	Epoch: [38][53/204]	Loss 0.0079 (0.0310)	
training:	Epoch: [38][54/204]	Loss 0.0182 (0.0307)	
training:	Epoch: [38][55/204]	Loss 0.0405 (0.0309)	
training:	Epoch: [38][56/204]	Loss 0.1346 (0.0328)	
training:	Epoch: [38][57/204]	Loss 0.0201 (0.0326)	
training:	Epoch: [38][58/204]	Loss 0.0131 (0.0322)	
training:	Epoch: [38][59/204]	Loss 0.0080 (0.0318)	
training:	Epoch: [38][60/204]	Loss 0.1536 (0.0338)	
training:	Epoch: [38][61/204]	Loss 0.0379 (0.0339)	
training:	Epoch: [38][62/204]	Loss 0.0071 (0.0335)	
training:	Epoch: [38][63/204]	Loss 0.0084 (0.0331)	
training:	Epoch: [38][64/204]	Loss 0.0103 (0.0327)	
training:	Epoch: [38][65/204]	Loss 0.0068 (0.0323)	
training:	Epoch: [38][66/204]	Loss 0.1783 (0.0345)	
training:	Epoch: [38][67/204]	Loss 0.0077 (0.0341)	
training:	Epoch: [38][68/204]	Loss 0.0481 (0.0343)	
training:	Epoch: [38][69/204]	Loss 0.0430 (0.0345)	
training:	Epoch: [38][70/204]	Loss 0.0100 (0.0341)	
training:	Epoch: [38][71/204]	Loss 0.1609 (0.0359)	
training:	Epoch: [38][72/204]	Loss 0.0072 (0.0355)	
training:	Epoch: [38][73/204]	Loss 0.0106 (0.0352)	
training:	Epoch: [38][74/204]	Loss 0.0118 (0.0348)	
training:	Epoch: [38][75/204]	Loss 0.0130 (0.0346)	
training:	Epoch: [38][76/204]	Loss 0.0077 (0.0342)	
training:	Epoch: [38][77/204]	Loss 0.0071 (0.0338)	
training:	Epoch: [38][78/204]	Loss 0.0165 (0.0336)	
training:	Epoch: [38][79/204]	Loss 0.0088 (0.0333)	
training:	Epoch: [38][80/204]	Loss 0.0586 (0.0336)	
training:	Epoch: [38][81/204]	Loss 0.0087 (0.0333)	
training:	Epoch: [38][82/204]	Loss 0.0067 (0.0330)	
training:	Epoch: [38][83/204]	Loss 0.1379 (0.0343)	
training:	Epoch: [38][84/204]	Loss 0.0124 (0.0340)	
training:	Epoch: [38][85/204]	Loss 0.1638 (0.0355)	
training:	Epoch: [38][86/204]	Loss 0.0079 (0.0352)	
training:	Epoch: [38][87/204]	Loss 0.0075 (0.0349)	
training:	Epoch: [38][88/204]	Loss 0.0125 (0.0346)	
training:	Epoch: [38][89/204]	Loss 0.0079 (0.0343)	
training:	Epoch: [38][90/204]	Loss 0.0225 (0.0342)	
training:	Epoch: [38][91/204]	Loss 0.0085 (0.0339)	
training:	Epoch: [38][92/204]	Loss 0.0146 (0.0337)	
training:	Epoch: [38][93/204]	Loss 0.2046 (0.0355)	
training:	Epoch: [38][94/204]	Loss 0.0071 (0.0352)	
training:	Epoch: [38][95/204]	Loss 0.0094 (0.0350)	
training:	Epoch: [38][96/204]	Loss 0.1637 (0.0363)	
training:	Epoch: [38][97/204]	Loss 0.1345 (0.0373)	
training:	Epoch: [38][98/204]	Loss 0.0065 (0.0370)	
training:	Epoch: [38][99/204]	Loss 0.1454 (0.0381)	
training:	Epoch: [38][100/204]	Loss 0.1720 (0.0394)	
training:	Epoch: [38][101/204]	Loss 0.0070 (0.0391)	
training:	Epoch: [38][102/204]	Loss 0.0079 (0.0388)	
training:	Epoch: [38][103/204]	Loss 0.0552 (0.0390)	
training:	Epoch: [38][104/204]	Loss 0.0072 (0.0387)	
training:	Epoch: [38][105/204]	Loss 0.1176 (0.0394)	
training:	Epoch: [38][106/204]	Loss 0.0069 (0.0391)	
training:	Epoch: [38][107/204]	Loss 0.0106 (0.0388)	
training:	Epoch: [38][108/204]	Loss 0.0088 (0.0386)	
training:	Epoch: [38][109/204]	Loss 0.0083 (0.0383)	
training:	Epoch: [38][110/204]	Loss 0.0080 (0.0380)	
training:	Epoch: [38][111/204]	Loss 0.1333 (0.0389)	
training:	Epoch: [38][112/204]	Loss 0.0079 (0.0386)	
training:	Epoch: [38][113/204]	Loss 0.0243 (0.0385)	
training:	Epoch: [38][114/204]	Loss 0.0099 (0.0382)	
training:	Epoch: [38][115/204]	Loss 0.0129 (0.0380)	
training:	Epoch: [38][116/204]	Loss 0.0071 (0.0377)	
training:	Epoch: [38][117/204]	Loss 0.1631 (0.0388)	
training:	Epoch: [38][118/204]	Loss 0.0081 (0.0385)	
training:	Epoch: [38][119/204]	Loss 0.3388 (0.0411)	
training:	Epoch: [38][120/204]	Loss 0.1318 (0.0418)	
training:	Epoch: [38][121/204]	Loss 0.0156 (0.0416)	
training:	Epoch: [38][122/204]	Loss 0.1167 (0.0422)	
training:	Epoch: [38][123/204]	Loss 0.0109 (0.0420)	
training:	Epoch: [38][124/204]	Loss 0.0397 (0.0420)	
training:	Epoch: [38][125/204]	Loss 0.1484 (0.0428)	
training:	Epoch: [38][126/204]	Loss 0.1671 (0.0438)	
training:	Epoch: [38][127/204]	Loss 0.1474 (0.0446)	
training:	Epoch: [38][128/204]	Loss 0.0216 (0.0444)	
training:	Epoch: [38][129/204]	Loss 0.0068 (0.0441)	
training:	Epoch: [38][130/204]	Loss 0.0221 (0.0440)	
training:	Epoch: [38][131/204]	Loss 0.0156 (0.0437)	
training:	Epoch: [38][132/204]	Loss 0.0137 (0.0435)	
training:	Epoch: [38][133/204]	Loss 0.0079 (0.0433)	
training:	Epoch: [38][134/204]	Loss 0.0075 (0.0430)	
training:	Epoch: [38][135/204]	Loss 0.1353 (0.0437)	
training:	Epoch: [38][136/204]	Loss 0.0189 (0.0435)	
training:	Epoch: [38][137/204]	Loss 0.0088 (0.0432)	
training:	Epoch: [38][138/204]	Loss 0.0350 (0.0432)	
training:	Epoch: [38][139/204]	Loss 0.0163 (0.0430)	
training:	Epoch: [38][140/204]	Loss 0.0471 (0.0430)	
training:	Epoch: [38][141/204]	Loss 0.0087 (0.0428)	
training:	Epoch: [38][142/204]	Loss 0.0078 (0.0425)	
training:	Epoch: [38][143/204]	Loss 0.0077 (0.0423)	
training:	Epoch: [38][144/204]	Loss 0.0385 (0.0423)	
training:	Epoch: [38][145/204]	Loss 0.0113 (0.0420)	
training:	Epoch: [38][146/204]	Loss 0.0097 (0.0418)	
training:	Epoch: [38][147/204]	Loss 0.0167 (0.0416)	
training:	Epoch: [38][148/204]	Loss 0.0134 (0.0415)	
training:	Epoch: [38][149/204]	Loss 0.0072 (0.0412)	
training:	Epoch: [38][150/204]	Loss 0.0108 (0.0410)	
training:	Epoch: [38][151/204]	Loss 0.0131 (0.0408)	
training:	Epoch: [38][152/204]	Loss 0.1675 (0.0417)	
training:	Epoch: [38][153/204]	Loss 0.0064 (0.0414)	
training:	Epoch: [38][154/204]	Loss 0.0079 (0.0412)	
training:	Epoch: [38][155/204]	Loss 0.0089 (0.0410)	
training:	Epoch: [38][156/204]	Loss 0.0382 (0.0410)	
training:	Epoch: [38][157/204]	Loss 0.0083 (0.0408)	
training:	Epoch: [38][158/204]	Loss 0.0335 (0.0407)	
training:	Epoch: [38][159/204]	Loss 0.0070 (0.0405)	
training:	Epoch: [38][160/204]	Loss 0.0078 (0.0403)	
training:	Epoch: [38][161/204]	Loss 0.0080 (0.0401)	
training:	Epoch: [38][162/204]	Loss 0.0068 (0.0399)	
training:	Epoch: [38][163/204]	Loss 0.0179 (0.0398)	
training:	Epoch: [38][164/204]	Loss 0.0129 (0.0396)	
training:	Epoch: [38][165/204]	Loss 0.0118 (0.0395)	
training:	Epoch: [38][166/204]	Loss 0.1719 (0.0402)	
training:	Epoch: [38][167/204]	Loss 0.0074 (0.0401)	
training:	Epoch: [38][168/204]	Loss 0.0071 (0.0399)	
training:	Epoch: [38][169/204]	Loss 0.0750 (0.0401)	
training:	Epoch: [38][170/204]	Loss 0.0071 (0.0399)	
training:	Epoch: [38][171/204]	Loss 0.0118 (0.0397)	
training:	Epoch: [38][172/204]	Loss 0.0075 (0.0395)	
training:	Epoch: [38][173/204]	Loss 0.0083 (0.0393)	
training:	Epoch: [38][174/204]	Loss 0.0075 (0.0392)	
training:	Epoch: [38][175/204]	Loss 0.0101 (0.0390)	
training:	Epoch: [38][176/204]	Loss 0.1612 (0.0397)	
training:	Epoch: [38][177/204]	Loss 0.0117 (0.0395)	
training:	Epoch: [38][178/204]	Loss 0.0079 (0.0393)	
training:	Epoch: [38][179/204]	Loss 0.0070 (0.0392)	
training:	Epoch: [38][180/204]	Loss 0.0096 (0.0390)	
training:	Epoch: [38][181/204]	Loss 0.0077 (0.0388)	
training:	Epoch: [38][182/204]	Loss 0.0075 (0.0387)	
training:	Epoch: [38][183/204]	Loss 0.0125 (0.0385)	
training:	Epoch: [38][184/204]	Loss 0.1674 (0.0392)	
training:	Epoch: [38][185/204]	Loss 0.0126 (0.0391)	
training:	Epoch: [38][186/204]	Loss 0.0065 (0.0389)	
training:	Epoch: [38][187/204]	Loss 0.0204 (0.0388)	
training:	Epoch: [38][188/204]	Loss 0.0103 (0.0386)	
training:	Epoch: [38][189/204]	Loss 0.0133 (0.0385)	
training:	Epoch: [38][190/204]	Loss 0.0069 (0.0383)	
training:	Epoch: [38][191/204]	Loss 0.0078 (0.0382)	
training:	Epoch: [38][192/204]	Loss 0.1635 (0.0388)	
training:	Epoch: [38][193/204]	Loss 0.1528 (0.0394)	
training:	Epoch: [38][194/204]	Loss 0.0072 (0.0393)	
training:	Epoch: [38][195/204]	Loss 0.0469 (0.0393)	
training:	Epoch: [38][196/204]	Loss 0.0069 (0.0391)	
training:	Epoch: [38][197/204]	Loss 0.0086 (0.0390)	
training:	Epoch: [38][198/204]	Loss 0.0188 (0.0389)	
training:	Epoch: [38][199/204]	Loss 0.1617 (0.0395)	
training:	Epoch: [38][200/204]	Loss 0.0592 (0.0396)	
training:	Epoch: [38][201/204]	Loss 0.0101 (0.0394)	
training:	Epoch: [38][202/204]	Loss 0.0298 (0.0394)	
training:	Epoch: [38][203/204]	Loss 0.0107 (0.0393)	
training:	Epoch: [38][204/204]	Loss 0.0079 (0.0391)	
Training:	 Loss: 0.0390

Training:	 ACC: 0.9961 0.9962 0.9974 0.9949
Validation:	 ACC: 0.8039 0.8042 0.8106 0.7971
Validation:	 Best_BACC: 0.8326 0.8331 0.8424 0.8229
Validation:	 Loss: 0.7729
Pretraining:	Epoch 39/200
----------
training:	Epoch: [39][1/204]	Loss 0.1208 (0.1208)	
training:	Epoch: [39][2/204]	Loss 0.0091 (0.0650)	
training:	Epoch: [39][3/204]	Loss 0.0076 (0.0459)	
training:	Epoch: [39][4/204]	Loss 0.0080 (0.0364)	
training:	Epoch: [39][5/204]	Loss 0.0078 (0.0307)	
training:	Epoch: [39][6/204]	Loss 0.0799 (0.0389)	
training:	Epoch: [39][7/204]	Loss 0.0069 (0.0343)	
training:	Epoch: [39][8/204]	Loss 0.0059 (0.0308)	
training:	Epoch: [39][9/204]	Loss 0.1513 (0.0441)	
training:	Epoch: [39][10/204]	Loss 0.0070 (0.0404)	
training:	Epoch: [39][11/204]	Loss 0.0172 (0.0383)	
training:	Epoch: [39][12/204]	Loss 0.0154 (0.0364)	
training:	Epoch: [39][13/204]	Loss 0.0061 (0.0341)	
training:	Epoch: [39][14/204]	Loss 0.0220 (0.0332)	
training:	Epoch: [39][15/204]	Loss 0.0326 (0.0332)	
training:	Epoch: [39][16/204]	Loss 0.0298 (0.0330)	
training:	Epoch: [39][17/204]	Loss 0.0982 (0.0368)	
training:	Epoch: [39][18/204]	Loss 0.0071 (0.0352)	
training:	Epoch: [39][19/204]	Loss 0.0153 (0.0341)	
training:	Epoch: [39][20/204]	Loss 0.1554 (0.0402)	
training:	Epoch: [39][21/204]	Loss 0.0067 (0.0386)	
training:	Epoch: [39][22/204]	Loss 0.0113 (0.0373)	
training:	Epoch: [39][23/204]	Loss 0.0289 (0.0370)	
training:	Epoch: [39][24/204]	Loss 0.0229 (0.0364)	
training:	Epoch: [39][25/204]	Loss 0.0084 (0.0353)	
training:	Epoch: [39][26/204]	Loss 0.0070 (0.0342)	
training:	Epoch: [39][27/204]	Loss 0.0074 (0.0332)	
training:	Epoch: [39][28/204]	Loss 0.0068 (0.0322)	
training:	Epoch: [39][29/204]	Loss 0.0155 (0.0317)	
training:	Epoch: [39][30/204]	Loss 0.1571 (0.0358)	
training:	Epoch: [39][31/204]	Loss 0.0074 (0.0349)	
training:	Epoch: [39][32/204]	Loss 0.1487 (0.0385)	
training:	Epoch: [39][33/204]	Loss 0.0155 (0.0378)	
training:	Epoch: [39][34/204]	Loss 0.0070 (0.0369)	
training:	Epoch: [39][35/204]	Loss 0.1400 (0.0398)	
training:	Epoch: [39][36/204]	Loss 0.0069 (0.0389)	
training:	Epoch: [39][37/204]	Loss 0.1613 (0.0422)	
training:	Epoch: [39][38/204]	Loss 0.0093 (0.0414)	
training:	Epoch: [39][39/204]	Loss 0.0071 (0.0405)	
training:	Epoch: [39][40/204]	Loss 0.0065 (0.0396)	
training:	Epoch: [39][41/204]	Loss 0.0092 (0.0389)	
training:	Epoch: [39][42/204]	Loss 0.3175 (0.0455)	
training:	Epoch: [39][43/204]	Loss 0.0073 (0.0446)	
training:	Epoch: [39][44/204]	Loss 0.0271 (0.0442)	
training:	Epoch: [39][45/204]	Loss 0.1561 (0.0467)	
training:	Epoch: [39][46/204]	Loss 0.0065 (0.0458)	
training:	Epoch: [39][47/204]	Loss 0.0094 (0.0451)	
training:	Epoch: [39][48/204]	Loss 0.0064 (0.0443)	
training:	Epoch: [39][49/204]	Loss 0.0067 (0.0435)	
training:	Epoch: [39][50/204]	Loss 0.0444 (0.0435)	
training:	Epoch: [39][51/204]	Loss 0.1870 (0.0463)	
training:	Epoch: [39][52/204]	Loss 0.0081 (0.0456)	
training:	Epoch: [39][53/204]	Loss 0.0160 (0.0450)	
training:	Epoch: [39][54/204]	Loss 0.0066 (0.0443)	
training:	Epoch: [39][55/204]	Loss 0.1516 (0.0463)	
training:	Epoch: [39][56/204]	Loss 0.0077 (0.0456)	
training:	Epoch: [39][57/204]	Loss 0.0080 (0.0449)	
training:	Epoch: [39][58/204]	Loss 0.0094 (0.0443)	
training:	Epoch: [39][59/204]	Loss 0.0091 (0.0437)	
training:	Epoch: [39][60/204]	Loss 0.0167 (0.0433)	
training:	Epoch: [39][61/204]	Loss 0.0138 (0.0428)	
training:	Epoch: [39][62/204]	Loss 0.0103 (0.0423)	
training:	Epoch: [39][63/204]	Loss 0.0064 (0.0417)	
training:	Epoch: [39][64/204]	Loss 0.0071 (0.0412)	
training:	Epoch: [39][65/204]	Loss 0.1377 (0.0426)	
training:	Epoch: [39][66/204]	Loss 0.0081 (0.0421)	
training:	Epoch: [39][67/204]	Loss 0.0555 (0.0423)	
training:	Epoch: [39][68/204]	Loss 0.0108 (0.0419)	
training:	Epoch: [39][69/204]	Loss 0.0078 (0.0414)	
training:	Epoch: [39][70/204]	Loss 0.0069 (0.0409)	
training:	Epoch: [39][71/204]	Loss 0.1522 (0.0424)	
training:	Epoch: [39][72/204]	Loss 0.0093 (0.0420)	
training:	Epoch: [39][73/204]	Loss 0.0071 (0.0415)	
training:	Epoch: [39][74/204]	Loss 0.0065 (0.0410)	
training:	Epoch: [39][75/204]	Loss 0.0065 (0.0406)	
training:	Epoch: [39][76/204]	Loss 0.0097 (0.0402)	
training:	Epoch: [39][77/204]	Loss 0.0102 (0.0398)	
training:	Epoch: [39][78/204]	Loss 0.0170 (0.0395)	
training:	Epoch: [39][79/204]	Loss 0.3128 (0.0429)	
training:	Epoch: [39][80/204]	Loss 0.0084 (0.0425)	
training:	Epoch: [39][81/204]	Loss 0.1080 (0.0433)	
training:	Epoch: [39][82/204]	Loss 0.0081 (0.0429)	
training:	Epoch: [39][83/204]	Loss 0.1283 (0.0439)	
training:	Epoch: [39][84/204]	Loss 0.1577 (0.0453)	
training:	Epoch: [39][85/204]	Loss 0.0116 (0.0449)	
training:	Epoch: [39][86/204]	Loss 0.0066 (0.0444)	
training:	Epoch: [39][87/204]	Loss 0.1587 (0.0457)	
training:	Epoch: [39][88/204]	Loss 0.0740 (0.0461)	
training:	Epoch: [39][89/204]	Loss 0.1356 (0.0471)	
training:	Epoch: [39][90/204]	Loss 0.0093 (0.0466)	
training:	Epoch: [39][91/204]	Loss 0.0070 (0.0462)	
training:	Epoch: [39][92/204]	Loss 0.0253 (0.0460)	
training:	Epoch: [39][93/204]	Loss 0.0077 (0.0456)	
training:	Epoch: [39][94/204]	Loss 0.0228 (0.0453)	
training:	Epoch: [39][95/204]	Loss 0.0284 (0.0452)	
training:	Epoch: [39][96/204]	Loss 0.0197 (0.0449)	
training:	Epoch: [39][97/204]	Loss 0.0079 (0.0445)	
training:	Epoch: [39][98/204]	Loss 0.0211 (0.0443)	
training:	Epoch: [39][99/204]	Loss 0.0071 (0.0439)	
training:	Epoch: [39][100/204]	Loss 0.0094 (0.0435)	
training:	Epoch: [39][101/204]	Loss 0.0067 (0.0432)	
training:	Epoch: [39][102/204]	Loss 0.0083 (0.0428)	
training:	Epoch: [39][103/204]	Loss 0.0089 (0.0425)	
training:	Epoch: [39][104/204]	Loss 0.0084 (0.0422)	
training:	Epoch: [39][105/204]	Loss 0.0333 (0.0421)	
training:	Epoch: [39][106/204]	Loss 0.0076 (0.0418)	
training:	Epoch: [39][107/204]	Loss 0.0080 (0.0415)	
training:	Epoch: [39][108/204]	Loss 0.0114 (0.0412)	
training:	Epoch: [39][109/204]	Loss 0.0120 (0.0409)	
training:	Epoch: [39][110/204]	Loss 0.0241 (0.0408)	
training:	Epoch: [39][111/204]	Loss 0.0110 (0.0405)	
training:	Epoch: [39][112/204]	Loss 0.0121 (0.0402)	
training:	Epoch: [39][113/204]	Loss 0.0211 (0.0401)	
training:	Epoch: [39][114/204]	Loss 0.0983 (0.0406)	
training:	Epoch: [39][115/204]	Loss 0.0199 (0.0404)	
training:	Epoch: [39][116/204]	Loss 0.0137 (0.0402)	
training:	Epoch: [39][117/204]	Loss 0.0107 (0.0399)	
training:	Epoch: [39][118/204]	Loss 0.0085 (0.0397)	
training:	Epoch: [39][119/204]	Loss 0.0252 (0.0395)	
training:	Epoch: [39][120/204]	Loss 0.0197 (0.0394)	
training:	Epoch: [39][121/204]	Loss 0.0070 (0.0391)	
training:	Epoch: [39][122/204]	Loss 0.0104 (0.0389)	
training:	Epoch: [39][123/204]	Loss 0.0076 (0.0386)	
training:	Epoch: [39][124/204]	Loss 0.0079 (0.0384)	
training:	Epoch: [39][125/204]	Loss 0.0060 (0.0381)	
training:	Epoch: [39][126/204]	Loss 0.0217 (0.0380)	
training:	Epoch: [39][127/204]	Loss 0.1841 (0.0391)	
training:	Epoch: [39][128/204]	Loss 0.0076 (0.0389)	
training:	Epoch: [39][129/204]	Loss 0.0104 (0.0387)	
training:	Epoch: [39][130/204]	Loss 0.0084 (0.0384)	
training:	Epoch: [39][131/204]	Loss 0.0402 (0.0384)	
training:	Epoch: [39][132/204]	Loss 0.0095 (0.0382)	
training:	Epoch: [39][133/204]	Loss 0.0366 (0.0382)	
training:	Epoch: [39][134/204]	Loss 0.0160 (0.0380)	
training:	Epoch: [39][135/204]	Loss 0.0073 (0.0378)	
training:	Epoch: [39][136/204]	Loss 0.0084 (0.0376)	
training:	Epoch: [39][137/204]	Loss 0.1541 (0.0384)	
training:	Epoch: [39][138/204]	Loss 0.0092 (0.0382)	
training:	Epoch: [39][139/204]	Loss 0.0790 (0.0385)	
training:	Epoch: [39][140/204]	Loss 0.0086 (0.0383)	
training:	Epoch: [39][141/204]	Loss 0.0064 (0.0381)	
training:	Epoch: [39][142/204]	Loss 0.0074 (0.0379)	
training:	Epoch: [39][143/204]	Loss 0.0075 (0.0377)	
training:	Epoch: [39][144/204]	Loss 0.0152 (0.0375)	
training:	Epoch: [39][145/204]	Loss 0.0252 (0.0374)	
training:	Epoch: [39][146/204]	Loss 0.0090 (0.0372)	
training:	Epoch: [39][147/204]	Loss 0.0160 (0.0371)	
training:	Epoch: [39][148/204]	Loss 0.0429 (0.0371)	
training:	Epoch: [39][149/204]	Loss 0.0073 (0.0369)	
training:	Epoch: [39][150/204]	Loss 0.2175 (0.0381)	
training:	Epoch: [39][151/204]	Loss 0.0631 (0.0383)	
training:	Epoch: [39][152/204]	Loss 0.0155 (0.0381)	
training:	Epoch: [39][153/204]	Loss 0.0108 (0.0380)	
training:	Epoch: [39][154/204]	Loss 0.0088 (0.0378)	
training:	Epoch: [39][155/204]	Loss 0.0149 (0.0376)	
training:	Epoch: [39][156/204]	Loss 0.0099 (0.0374)	
training:	Epoch: [39][157/204]	Loss 0.0086 (0.0373)	
training:	Epoch: [39][158/204]	Loss 0.0090 (0.0371)	
training:	Epoch: [39][159/204]	Loss 0.0066 (0.0369)	
training:	Epoch: [39][160/204]	Loss 0.0078 (0.0367)	
training:	Epoch: [39][161/204]	Loss 0.0069 (0.0365)	
training:	Epoch: [39][162/204]	Loss 0.0102 (0.0364)	
training:	Epoch: [39][163/204]	Loss 0.0148 (0.0362)	
training:	Epoch: [39][164/204]	Loss 0.0082 (0.0361)	
training:	Epoch: [39][165/204]	Loss 0.0070 (0.0359)	
training:	Epoch: [39][166/204]	Loss 0.0218 (0.0358)	
training:	Epoch: [39][167/204]	Loss 0.0904 (0.0361)	
training:	Epoch: [39][168/204]	Loss 0.0080 (0.0360)	
training:	Epoch: [39][169/204]	Loss 0.0068 (0.0358)	
training:	Epoch: [39][170/204]	Loss 0.0091 (0.0356)	
training:	Epoch: [39][171/204]	Loss 0.0062 (0.0355)	
training:	Epoch: [39][172/204]	Loss 0.0102 (0.0353)	
training:	Epoch: [39][173/204]	Loss 0.1048 (0.0357)	
training:	Epoch: [39][174/204]	Loss 0.0091 (0.0356)	
training:	Epoch: [39][175/204]	Loss 0.0070 (0.0354)	
training:	Epoch: [39][176/204]	Loss 0.1524 (0.0361)	
training:	Epoch: [39][177/204]	Loss 0.0702 (0.0362)	
training:	Epoch: [39][178/204]	Loss 0.0072 (0.0361)	
training:	Epoch: [39][179/204]	Loss 0.1580 (0.0368)	
training:	Epoch: [39][180/204]	Loss 0.0096 (0.0366)	
training:	Epoch: [39][181/204]	Loss 0.1911 (0.0375)	
training:	Epoch: [39][182/204]	Loss 0.0064 (0.0373)	
training:	Epoch: [39][183/204]	Loss 0.0065 (0.0371)	
training:	Epoch: [39][184/204]	Loss 0.0063 (0.0370)	
training:	Epoch: [39][185/204]	Loss 0.0067 (0.0368)	
training:	Epoch: [39][186/204]	Loss 0.0082 (0.0366)	
training:	Epoch: [39][187/204]	Loss 0.0328 (0.0366)	
training:	Epoch: [39][188/204]	Loss 0.1769 (0.0374)	
training:	Epoch: [39][189/204]	Loss 0.1729 (0.0381)	
training:	Epoch: [39][190/204]	Loss 0.0287 (0.0380)	
training:	Epoch: [39][191/204]	Loss 0.0085 (0.0379)	
training:	Epoch: [39][192/204]	Loss 0.0064 (0.0377)	
training:	Epoch: [39][193/204]	Loss 0.0067 (0.0376)	
training:	Epoch: [39][194/204]	Loss 0.0089 (0.0374)	
training:	Epoch: [39][195/204]	Loss 0.0068 (0.0373)	
training:	Epoch: [39][196/204]	Loss 0.0060 (0.0371)	
training:	Epoch: [39][197/204]	Loss 0.1702 (0.0378)	
training:	Epoch: [39][198/204]	Loss 0.0089 (0.0376)	
training:	Epoch: [39][199/204]	Loss 0.2451 (0.0387)	
training:	Epoch: [39][200/204]	Loss 0.0086 (0.0385)	
training:	Epoch: [39][201/204]	Loss 0.0100 (0.0384)	
training:	Epoch: [39][202/204]	Loss 0.0081 (0.0382)	
training:	Epoch: [39][203/204]	Loss 0.0204 (0.0381)	
training:	Epoch: [39][204/204]	Loss 0.1587 (0.0387)	
Training:	 Loss: 0.0387

Training:	 ACC: 0.9961 0.9962 0.9971 0.9952
Validation:	 ACC: 0.8061 0.8052 0.7871 0.8251
Validation:	 Best_BACC: 0.8326 0.8331 0.8424 0.8229
Validation:	 Loss: 0.7669
Pretraining:	Epoch 40/200
----------
training:	Epoch: [40][1/204]	Loss 0.0191 (0.0191)	
training:	Epoch: [40][2/204]	Loss 0.0204 (0.0197)	
training:	Epoch: [40][3/204]	Loss 0.0311 (0.0235)	
training:	Epoch: [40][4/204]	Loss 0.0089 (0.0199)	
training:	Epoch: [40][5/204]	Loss 0.0075 (0.0174)	
training:	Epoch: [40][6/204]	Loss 0.2345 (0.0536)	
training:	Epoch: [40][7/204]	Loss 0.1329 (0.0649)	
training:	Epoch: [40][8/204]	Loss 0.0079 (0.0578)	
training:	Epoch: [40][9/204]	Loss 0.0074 (0.0522)	
training:	Epoch: [40][10/204]	Loss 0.0062 (0.0476)	
training:	Epoch: [40][11/204]	Loss 0.0187 (0.0450)	
training:	Epoch: [40][12/204]	Loss 0.0194 (0.0428)	
training:	Epoch: [40][13/204]	Loss 0.0078 (0.0401)	
training:	Epoch: [40][14/204]	Loss 0.1070 (0.0449)	
training:	Epoch: [40][15/204]	Loss 0.0107 (0.0426)	
training:	Epoch: [40][16/204]	Loss 0.0141 (0.0409)	
training:	Epoch: [40][17/204]	Loss 0.0185 (0.0395)	
training:	Epoch: [40][18/204]	Loss 0.0158 (0.0382)	
training:	Epoch: [40][19/204]	Loss 0.1367 (0.0434)	
training:	Epoch: [40][20/204]	Loss 0.0165 (0.0421)	
training:	Epoch: [40][21/204]	Loss 0.0069 (0.0404)	
training:	Epoch: [40][22/204]	Loss 0.0085 (0.0389)	
training:	Epoch: [40][23/204]	Loss 0.1782 (0.0450)	
training:	Epoch: [40][24/204]	Loss 0.0345 (0.0446)	
training:	Epoch: [40][25/204]	Loss 0.0081 (0.0431)	
training:	Epoch: [40][26/204]	Loss 0.0106 (0.0418)	
training:	Epoch: [40][27/204]	Loss 0.0077 (0.0406)	
training:	Epoch: [40][28/204]	Loss 0.0070 (0.0394)	
training:	Epoch: [40][29/204]	Loss 0.0094 (0.0384)	
training:	Epoch: [40][30/204]	Loss 0.0918 (0.0401)	
training:	Epoch: [40][31/204]	Loss 0.0094 (0.0391)	
training:	Epoch: [40][32/204]	Loss 0.0318 (0.0389)	
training:	Epoch: [40][33/204]	Loss 0.0062 (0.0379)	
training:	Epoch: [40][34/204]	Loss 0.1399 (0.0409)	
training:	Epoch: [40][35/204]	Loss 0.0857 (0.0422)	
training:	Epoch: [40][36/204]	Loss 0.0073 (0.0412)	
training:	Epoch: [40][37/204]	Loss 0.0100 (0.0404)	
training:	Epoch: [40][38/204]	Loss 0.0061 (0.0395)	
training:	Epoch: [40][39/204]	Loss 0.0080 (0.0387)	
training:	Epoch: [40][40/204]	Loss 0.0892 (0.0399)	
training:	Epoch: [40][41/204]	Loss 0.0094 (0.0392)	
training:	Epoch: [40][42/204]	Loss 0.0084 (0.0385)	
training:	Epoch: [40][43/204]	Loss 0.0066 (0.0377)	
training:	Epoch: [40][44/204]	Loss 0.0069 (0.0370)	
training:	Epoch: [40][45/204]	Loss 0.0108 (0.0364)	
training:	Epoch: [40][46/204]	Loss 0.0212 (0.0361)	
training:	Epoch: [40][47/204]	Loss 0.0061 (0.0355)	
training:	Epoch: [40][48/204]	Loss 0.0081 (0.0349)	
training:	Epoch: [40][49/204]	Loss 0.0345 (0.0349)	
training:	Epoch: [40][50/204]	Loss 0.0115 (0.0344)	
training:	Epoch: [40][51/204]	Loss 0.1656 (0.0370)	
training:	Epoch: [40][52/204]	Loss 0.0078 (0.0364)	
training:	Epoch: [40][53/204]	Loss 0.0107 (0.0359)	
training:	Epoch: [40][54/204]	Loss 0.0080 (0.0354)	
training:	Epoch: [40][55/204]	Loss 0.0124 (0.0350)	
training:	Epoch: [40][56/204]	Loss 0.0064 (0.0345)	
training:	Epoch: [40][57/204]	Loss 0.0099 (0.0341)	
training:	Epoch: [40][58/204]	Loss 0.0224 (0.0339)	
training:	Epoch: [40][59/204]	Loss 0.0072 (0.0334)	
training:	Epoch: [40][60/204]	Loss 0.0079 (0.0330)	
training:	Epoch: [40][61/204]	Loss 0.0073 (0.0326)	
training:	Epoch: [40][62/204]	Loss 0.0147 (0.0323)	
training:	Epoch: [40][63/204]	Loss 0.0058 (0.0319)	
training:	Epoch: [40][64/204]	Loss 0.1310 (0.0334)	
training:	Epoch: [40][65/204]	Loss 0.0072 (0.0330)	
training:	Epoch: [40][66/204]	Loss 0.1679 (0.0351)	
training:	Epoch: [40][67/204]	Loss 0.0073 (0.0346)	
training:	Epoch: [40][68/204]	Loss 0.0079 (0.0342)	
training:	Epoch: [40][69/204]	Loss 0.0096 (0.0339)	
training:	Epoch: [40][70/204]	Loss 0.0458 (0.0341)	
training:	Epoch: [40][71/204]	Loss 0.0085 (0.0337)	
training:	Epoch: [40][72/204]	Loss 0.0077 (0.0333)	
training:	Epoch: [40][73/204]	Loss 0.0070 (0.0330)	
training:	Epoch: [40][74/204]	Loss 0.0073 (0.0326)	
training:	Epoch: [40][75/204]	Loss 0.0066 (0.0323)	
training:	Epoch: [40][76/204]	Loss 0.0188 (0.0321)	
training:	Epoch: [40][77/204]	Loss 0.0082 (0.0318)	
training:	Epoch: [40][78/204]	Loss 0.0072 (0.0315)	
training:	Epoch: [40][79/204]	Loss 0.0126 (0.0312)	
training:	Epoch: [40][80/204]	Loss 0.0073 (0.0309)	
training:	Epoch: [40][81/204]	Loss 0.0063 (0.0306)	
training:	Epoch: [40][82/204]	Loss 0.0095 (0.0304)	
training:	Epoch: [40][83/204]	Loss 0.0072 (0.0301)	
training:	Epoch: [40][84/204]	Loss 0.0479 (0.0303)	
training:	Epoch: [40][85/204]	Loss 0.1570 (0.0318)	
training:	Epoch: [40][86/204]	Loss 0.0083 (0.0315)	
training:	Epoch: [40][87/204]	Loss 0.0071 (0.0312)	
training:	Epoch: [40][88/204]	Loss 0.3179 (0.0345)	
training:	Epoch: [40][89/204]	Loss 0.2187 (0.0366)	
training:	Epoch: [40][90/204]	Loss 0.0082 (0.0363)	
training:	Epoch: [40][91/204]	Loss 0.0274 (0.0362)	
training:	Epoch: [40][92/204]	Loss 0.0065 (0.0358)	
training:	Epoch: [40][93/204]	Loss 0.0699 (0.0362)	
training:	Epoch: [40][94/204]	Loss 0.0061 (0.0359)	
training:	Epoch: [40][95/204]	Loss 0.0070 (0.0356)	
training:	Epoch: [40][96/204]	Loss 0.0146 (0.0354)	
training:	Epoch: [40][97/204]	Loss 0.1697 (0.0367)	
training:	Epoch: [40][98/204]	Loss 0.0551 (0.0369)	
training:	Epoch: [40][99/204]	Loss 0.0421 (0.0370)	
training:	Epoch: [40][100/204]	Loss 0.0074 (0.0367)	
training:	Epoch: [40][101/204]	Loss 0.1586 (0.0379)	
training:	Epoch: [40][102/204]	Loss 0.0094 (0.0376)	
training:	Epoch: [40][103/204]	Loss 0.0065 (0.0373)	
training:	Epoch: [40][104/204]	Loss 0.0091 (0.0370)	
training:	Epoch: [40][105/204]	Loss 0.0115 (0.0368)	
training:	Epoch: [40][106/204]	Loss 0.0103 (0.0366)	
training:	Epoch: [40][107/204]	Loss 0.0067 (0.0363)	
training:	Epoch: [40][108/204]	Loss 0.0065 (0.0360)	
training:	Epoch: [40][109/204]	Loss 0.0073 (0.0357)	
training:	Epoch: [40][110/204]	Loss 0.0172 (0.0356)	
training:	Epoch: [40][111/204]	Loss 0.0066 (0.0353)	
training:	Epoch: [40][112/204]	Loss 0.0073 (0.0351)	
training:	Epoch: [40][113/204]	Loss 0.1515 (0.0361)	
training:	Epoch: [40][114/204]	Loss 0.0081 (0.0358)	
training:	Epoch: [40][115/204]	Loss 0.0084 (0.0356)	
training:	Epoch: [40][116/204]	Loss 0.0086 (0.0354)	
training:	Epoch: [40][117/204]	Loss 0.0077 (0.0351)	
training:	Epoch: [40][118/204]	Loss 0.1434 (0.0360)	
training:	Epoch: [40][119/204]	Loss 0.1662 (0.0371)	
training:	Epoch: [40][120/204]	Loss 0.0426 (0.0372)	
training:	Epoch: [40][121/204]	Loss 0.1735 (0.0383)	
training:	Epoch: [40][122/204]	Loss 0.0146 (0.0381)	
training:	Epoch: [40][123/204]	Loss 0.0096 (0.0379)	
training:	Epoch: [40][124/204]	Loss 0.0113 (0.0377)	
training:	Epoch: [40][125/204]	Loss 0.0057 (0.0374)	
training:	Epoch: [40][126/204]	Loss 0.0150 (0.0372)	
training:	Epoch: [40][127/204]	Loss 0.3343 (0.0396)	
training:	Epoch: [40][128/204]	Loss 0.0075 (0.0393)	
training:	Epoch: [40][129/204]	Loss 0.0180 (0.0392)	
training:	Epoch: [40][130/204]	Loss 0.0059 (0.0389)	
training:	Epoch: [40][131/204]	Loss 0.0070 (0.0387)	
training:	Epoch: [40][132/204]	Loss 0.0215 (0.0385)	
training:	Epoch: [40][133/204]	Loss 0.0072 (0.0383)	
training:	Epoch: [40][134/204]	Loss 0.0070 (0.0381)	
training:	Epoch: [40][135/204]	Loss 0.0084 (0.0378)	
training:	Epoch: [40][136/204]	Loss 0.0188 (0.0377)	
training:	Epoch: [40][137/204]	Loss 0.0090 (0.0375)	
training:	Epoch: [40][138/204]	Loss 0.0073 (0.0373)	
training:	Epoch: [40][139/204]	Loss 0.0095 (0.0371)	
training:	Epoch: [40][140/204]	Loss 0.1650 (0.0380)	
training:	Epoch: [40][141/204]	Loss 0.0154 (0.0378)	
training:	Epoch: [40][142/204]	Loss 0.0061 (0.0376)	
training:	Epoch: [40][143/204]	Loss 0.0090 (0.0374)	
training:	Epoch: [40][144/204]	Loss 0.1280 (0.0380)	
training:	Epoch: [40][145/204]	Loss 0.0062 (0.0378)	
training:	Epoch: [40][146/204]	Loss 0.0268 (0.0377)	
training:	Epoch: [40][147/204]	Loss 0.0089 (0.0375)	
training:	Epoch: [40][148/204]	Loss 0.0074 (0.0373)	
training:	Epoch: [40][149/204]	Loss 0.0069 (0.0371)	
training:	Epoch: [40][150/204]	Loss 0.0107 (0.0370)	
training:	Epoch: [40][151/204]	Loss 0.0283 (0.0369)	
training:	Epoch: [40][152/204]	Loss 0.0059 (0.0367)	
training:	Epoch: [40][153/204]	Loss 0.0960 (0.0371)	
training:	Epoch: [40][154/204]	Loss 0.0061 (0.0369)	
training:	Epoch: [40][155/204]	Loss 0.0058 (0.0367)	
training:	Epoch: [40][156/204]	Loss 0.0895 (0.0370)	
training:	Epoch: [40][157/204]	Loss 0.0084 (0.0368)	
training:	Epoch: [40][158/204]	Loss 0.1726 (0.0377)	
training:	Epoch: [40][159/204]	Loss 0.0109 (0.0375)	
training:	Epoch: [40][160/204]	Loss 0.0405 (0.0376)	
training:	Epoch: [40][161/204]	Loss 0.0120 (0.0374)	
training:	Epoch: [40][162/204]	Loss 0.0068 (0.0372)	
training:	Epoch: [40][163/204]	Loss 0.0076 (0.0370)	
training:	Epoch: [40][164/204]	Loss 0.0064 (0.0368)	
training:	Epoch: [40][165/204]	Loss 0.0069 (0.0367)	
training:	Epoch: [40][166/204]	Loss 0.0084 (0.0365)	
training:	Epoch: [40][167/204]	Loss 0.0100 (0.0363)	
training:	Epoch: [40][168/204]	Loss 0.0857 (0.0366)	
training:	Epoch: [40][169/204]	Loss 0.0455 (0.0367)	
training:	Epoch: [40][170/204]	Loss 0.0096 (0.0365)	
training:	Epoch: [40][171/204]	Loss 0.0294 (0.0365)	
training:	Epoch: [40][172/204]	Loss 0.1596 (0.0372)	
training:	Epoch: [40][173/204]	Loss 0.0803 (0.0374)	
training:	Epoch: [40][174/204]	Loss 0.0065 (0.0373)	
training:	Epoch: [40][175/204]	Loss 0.0064 (0.0371)	
training:	Epoch: [40][176/204]	Loss 0.0079 (0.0369)	
training:	Epoch: [40][177/204]	Loss 0.0096 (0.0368)	
training:	Epoch: [40][178/204]	Loss 0.0059 (0.0366)	
training:	Epoch: [40][179/204]	Loss 0.1108 (0.0370)	
training:	Epoch: [40][180/204]	Loss 0.0103 (0.0369)	
training:	Epoch: [40][181/204]	Loss 0.0245 (0.0368)	
training:	Epoch: [40][182/204]	Loss 0.0142 (0.0367)	
training:	Epoch: [40][183/204]	Loss 0.0068 (0.0365)	
training:	Epoch: [40][184/204]	Loss 0.0085 (0.0363)	
training:	Epoch: [40][185/204]	Loss 0.0489 (0.0364)	
training:	Epoch: [40][186/204]	Loss 0.0076 (0.0363)	
training:	Epoch: [40][187/204]	Loss 0.1730 (0.0370)	
training:	Epoch: [40][188/204]	Loss 0.0071 (0.0368)	
training:	Epoch: [40][189/204]	Loss 0.0077 (0.0367)	
training:	Epoch: [40][190/204]	Loss 0.0206 (0.0366)	
training:	Epoch: [40][191/204]	Loss 0.1673 (0.0373)	
training:	Epoch: [40][192/204]	Loss 0.0064 (0.0371)	
training:	Epoch: [40][193/204]	Loss 0.0483 (0.0372)	
training:	Epoch: [40][194/204]	Loss 0.0222 (0.0371)	
training:	Epoch: [40][195/204]	Loss 0.0155 (0.0370)	
training:	Epoch: [40][196/204]	Loss 0.0068 (0.0368)	
training:	Epoch: [40][197/204]	Loss 0.0120 (0.0367)	
training:	Epoch: [40][198/204]	Loss 0.1525 (0.0373)	
training:	Epoch: [40][199/204]	Loss 0.0062 (0.0371)	
training:	Epoch: [40][200/204]	Loss 0.1473 (0.0377)	
training:	Epoch: [40][201/204]	Loss 0.0065 (0.0375)	
training:	Epoch: [40][202/204]	Loss 0.2096 (0.0384)	
training:	Epoch: [40][203/204]	Loss 0.0061 (0.0382)	
training:	Epoch: [40][204/204]	Loss 0.0066 (0.0381)	
Training:	 Loss: 0.0380

Training:	 ACC: 0.9963 0.9963 0.9979 0.9946
Validation:	 ACC: 0.8018 0.8026 0.8188 0.7848
Validation:	 Best_BACC: 0.8326 0.8331 0.8424 0.8229
Validation:	 Loss: 0.7856
Pretraining:	Epoch 41/200
----------
training:	Epoch: [41][1/204]	Loss 0.0086 (0.0086)	
training:	Epoch: [41][2/204]	Loss 0.0071 (0.0078)	
training:	Epoch: [41][3/204]	Loss 0.0103 (0.0087)	
training:	Epoch: [41][4/204]	Loss 0.0358 (0.0155)	
training:	Epoch: [41][5/204]	Loss 0.1683 (0.0460)	
training:	Epoch: [41][6/204]	Loss 0.1674 (0.0663)	
training:	Epoch: [41][7/204]	Loss 0.0077 (0.0579)	
training:	Epoch: [41][8/204]	Loss 0.0117 (0.0521)	
training:	Epoch: [41][9/204]	Loss 0.1600 (0.0641)	
training:	Epoch: [41][10/204]	Loss 0.2377 (0.0815)	
training:	Epoch: [41][11/204]	Loss 0.0089 (0.0749)	
training:	Epoch: [41][12/204]	Loss 0.0139 (0.0698)	
training:	Epoch: [41][13/204]	Loss 0.0150 (0.0656)	
training:	Epoch: [41][14/204]	Loss 0.0060 (0.0613)	
training:	Epoch: [41][15/204]	Loss 0.0061 (0.0576)	
training:	Epoch: [41][16/204]	Loss 0.0070 (0.0545)	
training:	Epoch: [41][17/204]	Loss 0.0079 (0.0517)	
training:	Epoch: [41][18/204]	Loss 0.0092 (0.0494)	
training:	Epoch: [41][19/204]	Loss 0.0070 (0.0471)	
training:	Epoch: [41][20/204]	Loss 0.0087 (0.0452)	
training:	Epoch: [41][21/204]	Loss 0.0094 (0.0435)	
training:	Epoch: [41][22/204]	Loss 0.1342 (0.0476)	
training:	Epoch: [41][23/204]	Loss 0.0100 (0.0460)	
training:	Epoch: [41][24/204]	Loss 0.0150 (0.0447)	
training:	Epoch: [41][25/204]	Loss 0.0066 (0.0432)	
training:	Epoch: [41][26/204]	Loss 0.0093 (0.0419)	
training:	Epoch: [41][27/204]	Loss 0.0073 (0.0406)	
training:	Epoch: [41][28/204]	Loss 0.0099 (0.0395)	
training:	Epoch: [41][29/204]	Loss 0.0068 (0.0384)	
training:	Epoch: [41][30/204]	Loss 0.1364 (0.0416)	
training:	Epoch: [41][31/204]	Loss 0.0086 (0.0406)	
training:	Epoch: [41][32/204]	Loss 0.0479 (0.0408)	
training:	Epoch: [41][33/204]	Loss 0.1213 (0.0432)	
training:	Epoch: [41][34/204]	Loss 0.0247 (0.0427)	
training:	Epoch: [41][35/204]	Loss 0.0059 (0.0416)	
training:	Epoch: [41][36/204]	Loss 0.0090 (0.0407)	
training:	Epoch: [41][37/204]	Loss 0.0099 (0.0399)	
training:	Epoch: [41][38/204]	Loss 0.0751 (0.0408)	
training:	Epoch: [41][39/204]	Loss 0.0117 (0.0401)	
training:	Epoch: [41][40/204]	Loss 0.0101 (0.0393)	
training:	Epoch: [41][41/204]	Loss 0.0097 (0.0386)	
training:	Epoch: [41][42/204]	Loss 0.0073 (0.0379)	
training:	Epoch: [41][43/204]	Loss 0.0072 (0.0372)	
training:	Epoch: [41][44/204]	Loss 0.0062 (0.0364)	
training:	Epoch: [41][45/204]	Loss 0.0834 (0.0375)	
training:	Epoch: [41][46/204]	Loss 0.0096 (0.0369)	
training:	Epoch: [41][47/204]	Loss 0.0074 (0.0363)	
training:	Epoch: [41][48/204]	Loss 0.0063 (0.0356)	
training:	Epoch: [41][49/204]	Loss 0.0097 (0.0351)	
training:	Epoch: [41][50/204]	Loss 0.0181 (0.0348)	
training:	Epoch: [41][51/204]	Loss 0.0214 (0.0345)	
training:	Epoch: [41][52/204]	Loss 0.0449 (0.0347)	
training:	Epoch: [41][53/204]	Loss 0.0095 (0.0342)	
training:	Epoch: [41][54/204]	Loss 0.0108 (0.0338)	
training:	Epoch: [41][55/204]	Loss 0.0073 (0.0333)	
training:	Epoch: [41][56/204]	Loss 0.0398 (0.0334)	
training:	Epoch: [41][57/204]	Loss 0.0113 (0.0330)	
training:	Epoch: [41][58/204]	Loss 0.0460 (0.0333)	
training:	Epoch: [41][59/204]	Loss 0.0092 (0.0329)	
training:	Epoch: [41][60/204]	Loss 0.0069 (0.0324)	
training:	Epoch: [41][61/204]	Loss 0.0086 (0.0320)	
training:	Epoch: [41][62/204]	Loss 0.0074 (0.0316)	
training:	Epoch: [41][63/204]	Loss 0.0640 (0.0322)	
training:	Epoch: [41][64/204]	Loss 0.0112 (0.0318)	
training:	Epoch: [41][65/204]	Loss 0.0912 (0.0327)	
training:	Epoch: [41][66/204]	Loss 0.0069 (0.0323)	
training:	Epoch: [41][67/204]	Loss 0.1609 (0.0343)	
training:	Epoch: [41][68/204]	Loss 0.0550 (0.0346)	
training:	Epoch: [41][69/204]	Loss 0.0957 (0.0355)	
training:	Epoch: [41][70/204]	Loss 0.1663 (0.0373)	
training:	Epoch: [41][71/204]	Loss 0.1541 (0.0390)	
training:	Epoch: [41][72/204]	Loss 0.0340 (0.0389)	
training:	Epoch: [41][73/204]	Loss 0.0067 (0.0385)	
training:	Epoch: [41][74/204]	Loss 0.0076 (0.0380)	
training:	Epoch: [41][75/204]	Loss 0.0068 (0.0376)	
training:	Epoch: [41][76/204]	Loss 0.0079 (0.0372)	
training:	Epoch: [41][77/204]	Loss 0.0587 (0.0375)	
training:	Epoch: [41][78/204]	Loss 0.0067 (0.0371)	
training:	Epoch: [41][79/204]	Loss 0.0087 (0.0368)	
training:	Epoch: [41][80/204]	Loss 0.0104 (0.0364)	
training:	Epoch: [41][81/204]	Loss 0.0063 (0.0361)	
training:	Epoch: [41][82/204]	Loss 0.0070 (0.0357)	
training:	Epoch: [41][83/204]	Loss 0.0074 (0.0354)	
training:	Epoch: [41][84/204]	Loss 0.0126 (0.0351)	
training:	Epoch: [41][85/204]	Loss 0.0085 (0.0348)	
training:	Epoch: [41][86/204]	Loss 0.0094 (0.0345)	
training:	Epoch: [41][87/204]	Loss 0.0057 (0.0342)	
training:	Epoch: [41][88/204]	Loss 0.0075 (0.0339)	
training:	Epoch: [41][89/204]	Loss 0.0063 (0.0335)	
training:	Epoch: [41][90/204]	Loss 0.0073 (0.0333)	
training:	Epoch: [41][91/204]	Loss 0.0204 (0.0331)	
training:	Epoch: [41][92/204]	Loss 0.0068 (0.0328)	
training:	Epoch: [41][93/204]	Loss 0.0061 (0.0325)	
training:	Epoch: [41][94/204]	Loss 0.0159 (0.0324)	
training:	Epoch: [41][95/204]	Loss 0.1043 (0.0331)	
training:	Epoch: [41][96/204]	Loss 0.0598 (0.0334)	
training:	Epoch: [41][97/204]	Loss 0.1716 (0.0348)	
training:	Epoch: [41][98/204]	Loss 0.0080 (0.0345)	
training:	Epoch: [41][99/204]	Loss 0.0099 (0.0343)	
training:	Epoch: [41][100/204]	Loss 0.1569 (0.0355)	
training:	Epoch: [41][101/204]	Loss 0.0070 (0.0352)	
training:	Epoch: [41][102/204]	Loss 0.2055 (0.0369)	
training:	Epoch: [41][103/204]	Loss 0.1056 (0.0376)	
training:	Epoch: [41][104/204]	Loss 0.0135 (0.0373)	
training:	Epoch: [41][105/204]	Loss 0.0056 (0.0370)	
training:	Epoch: [41][106/204]	Loss 0.0067 (0.0368)	
training:	Epoch: [41][107/204]	Loss 0.0176 (0.0366)	
training:	Epoch: [41][108/204]	Loss 0.0067 (0.0363)	
training:	Epoch: [41][109/204]	Loss 0.0074 (0.0360)	
training:	Epoch: [41][110/204]	Loss 0.0063 (0.0358)	
training:	Epoch: [41][111/204]	Loss 0.0069 (0.0355)	
training:	Epoch: [41][112/204]	Loss 0.0397 (0.0355)	
training:	Epoch: [41][113/204]	Loss 0.0499 (0.0357)	
training:	Epoch: [41][114/204]	Loss 0.0111 (0.0355)	
training:	Epoch: [41][115/204]	Loss 0.0238 (0.0354)	
training:	Epoch: [41][116/204]	Loss 0.0065 (0.0351)	
training:	Epoch: [41][117/204]	Loss 0.0156 (0.0349)	
training:	Epoch: [41][118/204]	Loss 0.1371 (0.0358)	
training:	Epoch: [41][119/204]	Loss 0.1946 (0.0371)	
training:	Epoch: [41][120/204]	Loss 0.0079 (0.0369)	
training:	Epoch: [41][121/204]	Loss 0.0097 (0.0367)	
training:	Epoch: [41][122/204]	Loss 0.0093 (0.0364)	
training:	Epoch: [41][123/204]	Loss 0.0067 (0.0362)	
training:	Epoch: [41][124/204]	Loss 0.1773 (0.0373)	
training:	Epoch: [41][125/204]	Loss 0.0837 (0.0377)	
training:	Epoch: [41][126/204]	Loss 0.0069 (0.0375)	
training:	Epoch: [41][127/204]	Loss 0.0077 (0.0372)	
training:	Epoch: [41][128/204]	Loss 0.0093 (0.0370)	
training:	Epoch: [41][129/204]	Loss 0.0135 (0.0368)	
training:	Epoch: [41][130/204]	Loss 0.0060 (0.0366)	
training:	Epoch: [41][131/204]	Loss 0.0057 (0.0364)	
training:	Epoch: [41][132/204]	Loss 0.0075 (0.0361)	
training:	Epoch: [41][133/204]	Loss 0.0736 (0.0364)	
training:	Epoch: [41][134/204]	Loss 0.1685 (0.0374)	
training:	Epoch: [41][135/204]	Loss 0.0057 (0.0372)	
training:	Epoch: [41][136/204]	Loss 0.0065 (0.0369)	
training:	Epoch: [41][137/204]	Loss 0.0094 (0.0367)	
training:	Epoch: [41][138/204]	Loss 0.0059 (0.0365)	
training:	Epoch: [41][139/204]	Loss 0.0058 (0.0363)	
training:	Epoch: [41][140/204]	Loss 0.0680 (0.0365)	
training:	Epoch: [41][141/204]	Loss 0.0053 (0.0363)	
training:	Epoch: [41][142/204]	Loss 0.0057 (0.0361)	
training:	Epoch: [41][143/204]	Loss 0.0257 (0.0360)	
training:	Epoch: [41][144/204]	Loss 0.0079 (0.0358)	
training:	Epoch: [41][145/204]	Loss 0.0072 (0.0356)	
training:	Epoch: [41][146/204]	Loss 0.0143 (0.0355)	
training:	Epoch: [41][147/204]	Loss 0.0069 (0.0353)	
training:	Epoch: [41][148/204]	Loss 0.0066 (0.0351)	
training:	Epoch: [41][149/204]	Loss 0.0058 (0.0349)	
training:	Epoch: [41][150/204]	Loss 0.0091 (0.0347)	
training:	Epoch: [41][151/204]	Loss 0.0105 (0.0346)	
training:	Epoch: [41][152/204]	Loss 0.1750 (0.0355)	
training:	Epoch: [41][153/204]	Loss 0.0124 (0.0353)	
training:	Epoch: [41][154/204]	Loss 0.0343 (0.0353)	
training:	Epoch: [41][155/204]	Loss 0.2156 (0.0365)	
training:	Epoch: [41][156/204]	Loss 0.0102 (0.0363)	
training:	Epoch: [41][157/204]	Loss 0.0087 (0.0361)	
training:	Epoch: [41][158/204]	Loss 0.0065 (0.0360)	
training:	Epoch: [41][159/204]	Loss 0.0066 (0.0358)	
training:	Epoch: [41][160/204]	Loss 0.0190 (0.0357)	
training:	Epoch: [41][161/204]	Loss 0.0378 (0.0357)	
training:	Epoch: [41][162/204]	Loss 0.0073 (0.0355)	
training:	Epoch: [41][163/204]	Loss 0.0067 (0.0353)	
training:	Epoch: [41][164/204]	Loss 0.1491 (0.0360)	
training:	Epoch: [41][165/204]	Loss 0.1666 (0.0368)	
training:	Epoch: [41][166/204]	Loss 0.0069 (0.0366)	
training:	Epoch: [41][167/204]	Loss 0.0083 (0.0365)	
training:	Epoch: [41][168/204]	Loss 0.0083 (0.0363)	
training:	Epoch: [41][169/204]	Loss 0.0075 (0.0361)	
training:	Epoch: [41][170/204]	Loss 0.0080 (0.0360)	
training:	Epoch: [41][171/204]	Loss 0.0069 (0.0358)	
training:	Epoch: [41][172/204]	Loss 0.0154 (0.0357)	
training:	Epoch: [41][173/204]	Loss 0.0202 (0.0356)	
training:	Epoch: [41][174/204]	Loss 0.0386 (0.0356)	
training:	Epoch: [41][175/204]	Loss 0.0060 (0.0354)	
training:	Epoch: [41][176/204]	Loss 0.0117 (0.0353)	
training:	Epoch: [41][177/204]	Loss 0.1410 (0.0359)	
training:	Epoch: [41][178/204]	Loss 0.0063 (0.0357)	
training:	Epoch: [41][179/204]	Loss 0.0074 (0.0356)	
training:	Epoch: [41][180/204]	Loss 0.0093 (0.0354)	
training:	Epoch: [41][181/204]	Loss 0.0071 (0.0353)	
training:	Epoch: [41][182/204]	Loss 0.0988 (0.0356)	
training:	Epoch: [41][183/204]	Loss 0.0066 (0.0355)	
training:	Epoch: [41][184/204]	Loss 0.1679 (0.0362)	
training:	Epoch: [41][185/204]	Loss 0.0116 (0.0360)	
training:	Epoch: [41][186/204]	Loss 0.0366 (0.0361)	
training:	Epoch: [41][187/204]	Loss 0.0070 (0.0359)	
training:	Epoch: [41][188/204]	Loss 0.0174 (0.0358)	
training:	Epoch: [41][189/204]	Loss 0.0059 (0.0356)	
training:	Epoch: [41][190/204]	Loss 0.0278 (0.0356)	
training:	Epoch: [41][191/204]	Loss 0.0702 (0.0358)	
training:	Epoch: [41][192/204]	Loss 0.0245 (0.0357)	
training:	Epoch: [41][193/204]	Loss 0.0413 (0.0357)	
training:	Epoch: [41][194/204]	Loss 0.0171 (0.0357)	
training:	Epoch: [41][195/204]	Loss 0.1298 (0.0361)	
training:	Epoch: [41][196/204]	Loss 0.0386 (0.0361)	
training:	Epoch: [41][197/204]	Loss 0.0562 (0.0363)	
training:	Epoch: [41][198/204]	Loss 0.0067 (0.0361)	
training:	Epoch: [41][199/204]	Loss 0.1818 (0.0368)	
training:	Epoch: [41][200/204]	Loss 0.1471 (0.0374)	
training:	Epoch: [41][201/204]	Loss 0.0980 (0.0377)	
training:	Epoch: [41][202/204]	Loss 0.1554 (0.0383)	
training:	Epoch: [41][203/204]	Loss 0.0070 (0.0381)	
training:	Epoch: [41][204/204]	Loss 0.0078 (0.0380)	
Training:	 Loss: 0.0379

Training:	 ACC: 0.9966 0.9966 0.9982 0.9949
Validation:	 ACC: 0.8025 0.8036 0.8270 0.7780
Validation:	 Best_BACC: 0.8326 0.8331 0.8424 0.8229
Validation:	 Loss: 0.7860
Pretraining:	Epoch 42/200
----------
training:	Epoch: [42][1/204]	Loss 0.0309 (0.0309)	
training:	Epoch: [42][2/204]	Loss 0.0062 (0.0186)	
training:	Epoch: [42][3/204]	Loss 0.0056 (0.0142)	
training:	Epoch: [42][4/204]	Loss 0.0069 (0.0124)	
training:	Epoch: [42][5/204]	Loss 0.0311 (0.0161)	
training:	Epoch: [42][6/204]	Loss 0.0066 (0.0145)	
training:	Epoch: [42][7/204]	Loss 0.0136 (0.0144)	
training:	Epoch: [42][8/204]	Loss 0.0635 (0.0205)	
training:	Epoch: [42][9/204]	Loss 0.0078 (0.0191)	
training:	Epoch: [42][10/204]	Loss 0.0865 (0.0259)	
training:	Epoch: [42][11/204]	Loss 0.0072 (0.0242)	
training:	Epoch: [42][12/204]	Loss 0.0085 (0.0229)	
training:	Epoch: [42][13/204]	Loss 0.1105 (0.0296)	
training:	Epoch: [42][14/204]	Loss 0.0076 (0.0280)	
training:	Epoch: [42][15/204]	Loss 0.0201 (0.0275)	
training:	Epoch: [42][16/204]	Loss 0.0106 (0.0264)	
training:	Epoch: [42][17/204]	Loss 0.0070 (0.0253)	
training:	Epoch: [42][18/204]	Loss 0.0074 (0.0243)	
training:	Epoch: [42][19/204]	Loss 0.0210 (0.0241)	
training:	Epoch: [42][20/204]	Loss 0.0073 (0.0233)	
training:	Epoch: [42][21/204]	Loss 0.0079 (0.0226)	
training:	Epoch: [42][22/204]	Loss 0.0067 (0.0218)	
training:	Epoch: [42][23/204]	Loss 0.0072 (0.0212)	
training:	Epoch: [42][24/204]	Loss 0.1098 (0.0249)	
training:	Epoch: [42][25/204]	Loss 0.0160 (0.0245)	
training:	Epoch: [42][26/204]	Loss 0.0082 (0.0239)	
training:	Epoch: [42][27/204]	Loss 0.0062 (0.0233)	
training:	Epoch: [42][28/204]	Loss 0.0100 (0.0228)	
training:	Epoch: [42][29/204]	Loss 0.1470 (0.0271)	
training:	Epoch: [42][30/204]	Loss 0.0096 (0.0265)	
training:	Epoch: [42][31/204]	Loss 0.0172 (0.0262)	
training:	Epoch: [42][32/204]	Loss 0.1515 (0.0301)	
training:	Epoch: [42][33/204]	Loss 0.1788 (0.0346)	
training:	Epoch: [42][34/204]	Loss 0.0132 (0.0340)	
training:	Epoch: [42][35/204]	Loss 0.0123 (0.0334)	
training:	Epoch: [42][36/204]	Loss 0.0407 (0.0336)	
training:	Epoch: [42][37/204]	Loss 0.0087 (0.0329)	
training:	Epoch: [42][38/204]	Loss 0.0117 (0.0323)	
training:	Epoch: [42][39/204]	Loss 0.0057 (0.0317)	
training:	Epoch: [42][40/204]	Loss 0.0260 (0.0315)	
training:	Epoch: [42][41/204]	Loss 0.0062 (0.0309)	
training:	Epoch: [42][42/204]	Loss 0.0086 (0.0304)	
training:	Epoch: [42][43/204]	Loss 0.0072 (0.0298)	
training:	Epoch: [42][44/204]	Loss 0.0062 (0.0293)	
training:	Epoch: [42][45/204]	Loss 0.0080 (0.0288)	
training:	Epoch: [42][46/204]	Loss 0.0067 (0.0283)	
training:	Epoch: [42][47/204]	Loss 0.1103 (0.0301)	
training:	Epoch: [42][48/204]	Loss 0.0293 (0.0301)	
training:	Epoch: [42][49/204]	Loss 0.0078 (0.0296)	
training:	Epoch: [42][50/204]	Loss 0.0065 (0.0291)	
training:	Epoch: [42][51/204]	Loss 0.0170 (0.0289)	
training:	Epoch: [42][52/204]	Loss 0.0073 (0.0285)	
training:	Epoch: [42][53/204]	Loss 0.0066 (0.0281)	
training:	Epoch: [42][54/204]	Loss 0.0274 (0.0281)	
training:	Epoch: [42][55/204]	Loss 0.0077 (0.0277)	
training:	Epoch: [42][56/204]	Loss 0.0059 (0.0273)	
training:	Epoch: [42][57/204]	Loss 0.0076 (0.0270)	
training:	Epoch: [42][58/204]	Loss 0.0102 (0.0267)	
training:	Epoch: [42][59/204]	Loss 0.0052 (0.0263)	
training:	Epoch: [42][60/204]	Loss 0.0057 (0.0260)	
training:	Epoch: [42][61/204]	Loss 0.0111 (0.0257)	
training:	Epoch: [42][62/204]	Loss 0.0547 (0.0262)	
training:	Epoch: [42][63/204]	Loss 0.1191 (0.0277)	
training:	Epoch: [42][64/204]	Loss 0.0148 (0.0275)	
training:	Epoch: [42][65/204]	Loss 0.0199 (0.0273)	
training:	Epoch: [42][66/204]	Loss 0.1663 (0.0295)	
training:	Epoch: [42][67/204]	Loss 0.0057 (0.0291)	
training:	Epoch: [42][68/204]	Loss 0.0072 (0.0288)	
training:	Epoch: [42][69/204]	Loss 0.0154 (0.0286)	
training:	Epoch: [42][70/204]	Loss 0.0148 (0.0284)	
training:	Epoch: [42][71/204]	Loss 0.0073 (0.0281)	
training:	Epoch: [42][72/204]	Loss 0.0071 (0.0278)	
training:	Epoch: [42][73/204]	Loss 0.0058 (0.0275)	
training:	Epoch: [42][74/204]	Loss 0.1562 (0.0292)	
training:	Epoch: [42][75/204]	Loss 0.0064 (0.0289)	
training:	Epoch: [42][76/204]	Loss 0.0119 (0.0287)	
training:	Epoch: [42][77/204]	Loss 0.1452 (0.0302)	
training:	Epoch: [42][78/204]	Loss 0.0058 (0.0299)	
training:	Epoch: [42][79/204]	Loss 0.0063 (0.0296)	
training:	Epoch: [42][80/204]	Loss 0.0070 (0.0293)	
training:	Epoch: [42][81/204]	Loss 0.0064 (0.0290)	
training:	Epoch: [42][82/204]	Loss 0.0071 (0.0288)	
training:	Epoch: [42][83/204]	Loss 0.0061 (0.0285)	
training:	Epoch: [42][84/204]	Loss 0.0079 (0.0283)	
training:	Epoch: [42][85/204]	Loss 0.0188 (0.0281)	
training:	Epoch: [42][86/204]	Loss 0.1908 (0.0300)	
training:	Epoch: [42][87/204]	Loss 0.0063 (0.0298)	
training:	Epoch: [42][88/204]	Loss 0.0053 (0.0295)	
training:	Epoch: [42][89/204]	Loss 0.0111 (0.0293)	
training:	Epoch: [42][90/204]	Loss 0.1708 (0.0309)	
training:	Epoch: [42][91/204]	Loss 0.0099 (0.0306)	
training:	Epoch: [42][92/204]	Loss 0.0075 (0.0304)	
training:	Epoch: [42][93/204]	Loss 0.1419 (0.0316)	
training:	Epoch: [42][94/204]	Loss 0.0076 (0.0313)	
training:	Epoch: [42][95/204]	Loss 0.0098 (0.0311)	
training:	Epoch: [42][96/204]	Loss 0.0084 (0.0309)	
training:	Epoch: [42][97/204]	Loss 0.0059 (0.0306)	
training:	Epoch: [42][98/204]	Loss 0.0116 (0.0304)	
training:	Epoch: [42][99/204]	Loss 0.0486 (0.0306)	
training:	Epoch: [42][100/204]	Loss 0.0110 (0.0304)	
training:	Epoch: [42][101/204]	Loss 0.0061 (0.0301)	
training:	Epoch: [42][102/204]	Loss 0.0063 (0.0299)	
training:	Epoch: [42][103/204]	Loss 0.0067 (0.0297)	
training:	Epoch: [42][104/204]	Loss 0.0070 (0.0295)	
training:	Epoch: [42][105/204]	Loss 0.1612 (0.0307)	
training:	Epoch: [42][106/204]	Loss 0.0509 (0.0309)	
training:	Epoch: [42][107/204]	Loss 0.0070 (0.0307)	
training:	Epoch: [42][108/204]	Loss 0.0061 (0.0305)	
training:	Epoch: [42][109/204]	Loss 0.0262 (0.0304)	
training:	Epoch: [42][110/204]	Loss 0.0084 (0.0302)	
training:	Epoch: [42][111/204]	Loss 0.0701 (0.0306)	
training:	Epoch: [42][112/204]	Loss 0.0066 (0.0304)	
training:	Epoch: [42][113/204]	Loss 0.0429 (0.0305)	
training:	Epoch: [42][114/204]	Loss 0.3271 (0.0331)	
training:	Epoch: [42][115/204]	Loss 0.0151 (0.0329)	
training:	Epoch: [42][116/204]	Loss 0.0071 (0.0327)	
training:	Epoch: [42][117/204]	Loss 0.1560 (0.0338)	
training:	Epoch: [42][118/204]	Loss 0.0073 (0.0335)	
training:	Epoch: [42][119/204]	Loss 0.0071 (0.0333)	
training:	Epoch: [42][120/204]	Loss 0.1293 (0.0341)	
training:	Epoch: [42][121/204]	Loss 0.0103 (0.0339)	
training:	Epoch: [42][122/204]	Loss 0.0129 (0.0337)	
training:	Epoch: [42][123/204]	Loss 0.0125 (0.0336)	
training:	Epoch: [42][124/204]	Loss 0.0086 (0.0334)	
training:	Epoch: [42][125/204]	Loss 0.0063 (0.0331)	
training:	Epoch: [42][126/204]	Loss 0.0297 (0.0331)	
training:	Epoch: [42][127/204]	Loss 0.0071 (0.0329)	
training:	Epoch: [42][128/204]	Loss 0.0063 (0.0327)	
training:	Epoch: [42][129/204]	Loss 0.0065 (0.0325)	
training:	Epoch: [42][130/204]	Loss 0.1621 (0.0335)	
training:	Epoch: [42][131/204]	Loss 0.0250 (0.0334)	
training:	Epoch: [42][132/204]	Loss 0.0058 (0.0332)	
training:	Epoch: [42][133/204]	Loss 0.0191 (0.0331)	
training:	Epoch: [42][134/204]	Loss 0.0077 (0.0329)	
training:	Epoch: [42][135/204]	Loss 0.0070 (0.0327)	
training:	Epoch: [42][136/204]	Loss 0.0266 (0.0327)	
training:	Epoch: [42][137/204]	Loss 0.0060 (0.0325)	
training:	Epoch: [42][138/204]	Loss 0.0083 (0.0323)	
training:	Epoch: [42][139/204]	Loss 0.0226 (0.0323)	
training:	Epoch: [42][140/204]	Loss 0.0063 (0.0321)	
training:	Epoch: [42][141/204]	Loss 0.0074 (0.0319)	
training:	Epoch: [42][142/204]	Loss 0.0724 (0.0322)	
training:	Epoch: [42][143/204]	Loss 0.0591 (0.0324)	
training:	Epoch: [42][144/204]	Loss 0.1568 (0.0332)	
training:	Epoch: [42][145/204]	Loss 0.3139 (0.0352)	
training:	Epoch: [42][146/204]	Loss 0.0122 (0.0350)	
training:	Epoch: [42][147/204]	Loss 0.0447 (0.0351)	
training:	Epoch: [42][148/204]	Loss 0.0063 (0.0349)	
training:	Epoch: [42][149/204]	Loss 0.1204 (0.0355)	
training:	Epoch: [42][150/204]	Loss 0.0059 (0.0353)	
training:	Epoch: [42][151/204]	Loss 0.0060 (0.0351)	
training:	Epoch: [42][152/204]	Loss 0.0291 (0.0350)	
training:	Epoch: [42][153/204]	Loss 0.0057 (0.0348)	
training:	Epoch: [42][154/204]	Loss 0.0092 (0.0347)	
training:	Epoch: [42][155/204]	Loss 0.0089 (0.0345)	
training:	Epoch: [42][156/204]	Loss 0.0125 (0.0344)	
training:	Epoch: [42][157/204]	Loss 0.0071 (0.0342)	
training:	Epoch: [42][158/204]	Loss 0.1521 (0.0349)	
training:	Epoch: [42][159/204]	Loss 0.0089 (0.0348)	
training:	Epoch: [42][160/204]	Loss 0.0198 (0.0347)	
training:	Epoch: [42][161/204]	Loss 0.0088 (0.0345)	
training:	Epoch: [42][162/204]	Loss 0.0138 (0.0344)	
training:	Epoch: [42][163/204]	Loss 0.0096 (0.0342)	
training:	Epoch: [42][164/204]	Loss 0.0193 (0.0341)	
training:	Epoch: [42][165/204]	Loss 0.0155 (0.0340)	
training:	Epoch: [42][166/204]	Loss 0.0322 (0.0340)	
training:	Epoch: [42][167/204]	Loss 0.0124 (0.0339)	
training:	Epoch: [42][168/204]	Loss 0.0075 (0.0337)	
training:	Epoch: [42][169/204]	Loss 0.0165 (0.0336)	
training:	Epoch: [42][170/204]	Loss 0.0055 (0.0335)	
training:	Epoch: [42][171/204]	Loss 0.0054 (0.0333)	
training:	Epoch: [42][172/204]	Loss 0.0135 (0.0332)	
training:	Epoch: [42][173/204]	Loss 0.0079 (0.0330)	
training:	Epoch: [42][174/204]	Loss 0.0974 (0.0334)	
training:	Epoch: [42][175/204]	Loss 0.1822 (0.0343)	
training:	Epoch: [42][176/204]	Loss 0.0062 (0.0341)	
training:	Epoch: [42][177/204]	Loss 0.0052 (0.0339)	
training:	Epoch: [42][178/204]	Loss 0.0086 (0.0338)	
training:	Epoch: [42][179/204]	Loss 0.0176 (0.0337)	
training:	Epoch: [42][180/204]	Loss 0.0123 (0.0336)	
training:	Epoch: [42][181/204]	Loss 0.0107 (0.0335)	
training:	Epoch: [42][182/204]	Loss 0.0077 (0.0333)	
training:	Epoch: [42][183/204]	Loss 0.0064 (0.0332)	
training:	Epoch: [42][184/204]	Loss 0.0207 (0.0331)	
training:	Epoch: [42][185/204]	Loss 0.0060 (0.0330)	
training:	Epoch: [42][186/204]	Loss 0.1372 (0.0335)	
training:	Epoch: [42][187/204]	Loss 0.0172 (0.0334)	
training:	Epoch: [42][188/204]	Loss 0.0140 (0.0333)	
training:	Epoch: [42][189/204]	Loss 0.0532 (0.0334)	
training:	Epoch: [42][190/204]	Loss 0.0082 (0.0333)	
training:	Epoch: [42][191/204]	Loss 0.0080 (0.0332)	
training:	Epoch: [42][192/204]	Loss 0.0077 (0.0330)	
training:	Epoch: [42][193/204]	Loss 0.0074 (0.0329)	
training:	Epoch: [42][194/204]	Loss 0.1740 (0.0336)	
training:	Epoch: [42][195/204]	Loss 0.0072 (0.0335)	
training:	Epoch: [42][196/204]	Loss 0.0065 (0.0334)	
training:	Epoch: [42][197/204]	Loss 0.1458 (0.0339)	
training:	Epoch: [42][198/204]	Loss 0.0772 (0.0341)	
training:	Epoch: [42][199/204]	Loss 0.0092 (0.0340)	
training:	Epoch: [42][200/204]	Loss 0.0165 (0.0339)	
training:	Epoch: [42][201/204]	Loss 0.0081 (0.0338)	
training:	Epoch: [42][202/204]	Loss 0.0065 (0.0337)	
training:	Epoch: [42][203/204]	Loss 0.0081 (0.0335)	
training:	Epoch: [42][204/204]	Loss 0.0060 (0.0334)	
Training:	 Loss: 0.0334

Training:	 ACC: 0.9966 0.9966 0.9982 0.9949
Validation:	 ACC: 0.8025 0.8047 0.8506 0.7545
Validation:	 Best_BACC: 0.8326 0.8331 0.8424 0.8229
Validation:	 Loss: 0.7840
Pretraining:	Epoch 43/200
----------
training:	Epoch: [43][1/204]	Loss 0.0124 (0.0124)	
training:	Epoch: [43][2/204]	Loss 0.0086 (0.0105)	
training:	Epoch: [43][3/204]	Loss 0.0062 (0.0090)	
training:	Epoch: [43][4/204]	Loss 0.0054 (0.0081)	
training:	Epoch: [43][5/204]	Loss 0.0788 (0.0223)	
training:	Epoch: [43][6/204]	Loss 0.0066 (0.0197)	
training:	Epoch: [43][7/204]	Loss 0.0079 (0.0180)	
training:	Epoch: [43][8/204]	Loss 0.0804 (0.0258)	
training:	Epoch: [43][9/204]	Loss 0.0055 (0.0235)	
training:	Epoch: [43][10/204]	Loss 0.0114 (0.0223)	
training:	Epoch: [43][11/204]	Loss 0.0068 (0.0209)	
training:	Epoch: [43][12/204]	Loss 0.0053 (0.0196)	
training:	Epoch: [43][13/204]	Loss 0.0106 (0.0189)	
training:	Epoch: [43][14/204]	Loss 0.0061 (0.0180)	
training:	Epoch: [43][15/204]	Loss 0.0066 (0.0172)	
training:	Epoch: [43][16/204]	Loss 0.0071 (0.0166)	
training:	Epoch: [43][17/204]	Loss 0.0073 (0.0161)	
training:	Epoch: [43][18/204]	Loss 0.1808 (0.0252)	
training:	Epoch: [43][19/204]	Loss 0.0194 (0.0249)	
training:	Epoch: [43][20/204]	Loss 0.1421 (0.0308)	
training:	Epoch: [43][21/204]	Loss 0.0262 (0.0306)	
training:	Epoch: [43][22/204]	Loss 0.0061 (0.0294)	
training:	Epoch: [43][23/204]	Loss 0.0150 (0.0288)	
training:	Epoch: [43][24/204]	Loss 0.0078 (0.0279)	
training:	Epoch: [43][25/204]	Loss 0.0053 (0.0270)	
training:	Epoch: [43][26/204]	Loss 0.0054 (0.0262)	
training:	Epoch: [43][27/204]	Loss 0.1301 (0.0300)	
training:	Epoch: [43][28/204]	Loss 0.0076 (0.0292)	
training:	Epoch: [43][29/204]	Loss 0.0052 (0.0284)	
training:	Epoch: [43][30/204]	Loss 0.0085 (0.0277)	
training:	Epoch: [43][31/204]	Loss 0.0054 (0.0270)	
training:	Epoch: [43][32/204]	Loss 0.0696 (0.0284)	
training:	Epoch: [43][33/204]	Loss 0.0090 (0.0278)	
training:	Epoch: [43][34/204]	Loss 0.0100 (0.0273)	
training:	Epoch: [43][35/204]	Loss 0.0067 (0.0267)	
training:	Epoch: [43][36/204]	Loss 0.0070 (0.0261)	
training:	Epoch: [43][37/204]	Loss 0.0051 (0.0256)	
training:	Epoch: [43][38/204]	Loss 0.0052 (0.0250)	
training:	Epoch: [43][39/204]	Loss 0.0153 (0.0248)	
training:	Epoch: [43][40/204]	Loss 0.0139 (0.0245)	
training:	Epoch: [43][41/204]	Loss 0.2806 (0.0307)	
training:	Epoch: [43][42/204]	Loss 0.0082 (0.0302)	
training:	Epoch: [43][43/204]	Loss 0.0626 (0.0310)	
training:	Epoch: [43][44/204]	Loss 0.1309 (0.0332)	
training:	Epoch: [43][45/204]	Loss 0.0053 (0.0326)	
training:	Epoch: [43][46/204]	Loss 0.0111 (0.0321)	
training:	Epoch: [43][47/204]	Loss 0.0057 (0.0316)	
training:	Epoch: [43][48/204]	Loss 0.0222 (0.0314)	
training:	Epoch: [43][49/204]	Loss 0.0064 (0.0309)	
training:	Epoch: [43][50/204]	Loss 0.0187 (0.0306)	
training:	Epoch: [43][51/204]	Loss 0.0058 (0.0301)	
training:	Epoch: [43][52/204]	Loss 0.0052 (0.0297)	
training:	Epoch: [43][53/204]	Loss 0.0100 (0.0293)	
training:	Epoch: [43][54/204]	Loss 0.0109 (0.0290)	
training:	Epoch: [43][55/204]	Loss 0.0290 (0.0290)	
training:	Epoch: [43][56/204]	Loss 0.0072 (0.0286)	
training:	Epoch: [43][57/204]	Loss 0.0051 (0.0282)	
training:	Epoch: [43][58/204]	Loss 0.0104 (0.0278)	
training:	Epoch: [43][59/204]	Loss 0.0066 (0.0275)	
training:	Epoch: [43][60/204]	Loss 0.0101 (0.0272)	
training:	Epoch: [43][61/204]	Loss 0.1973 (0.0300)	
training:	Epoch: [43][62/204]	Loss 0.1639 (0.0321)	
training:	Epoch: [43][63/204]	Loss 0.0059 (0.0317)	
training:	Epoch: [43][64/204]	Loss 0.0059 (0.0313)	
training:	Epoch: [43][65/204]	Loss 0.0067 (0.0309)	
training:	Epoch: [43][66/204]	Loss 0.0202 (0.0308)	
training:	Epoch: [43][67/204]	Loss 0.0074 (0.0304)	
training:	Epoch: [43][68/204]	Loss 0.0063 (0.0301)	
training:	Epoch: [43][69/204]	Loss 0.0095 (0.0298)	
training:	Epoch: [43][70/204]	Loss 0.1741 (0.0318)	
training:	Epoch: [43][71/204]	Loss 0.1336 (0.0333)	
training:	Epoch: [43][72/204]	Loss 0.0074 (0.0329)	
training:	Epoch: [43][73/204]	Loss 0.0066 (0.0326)	
training:	Epoch: [43][74/204]	Loss 0.0065 (0.0322)	
training:	Epoch: [43][75/204]	Loss 0.0893 (0.0330)	
training:	Epoch: [43][76/204]	Loss 0.0053 (0.0326)	
training:	Epoch: [43][77/204]	Loss 0.0068 (0.0323)	
training:	Epoch: [43][78/204]	Loss 0.0066 (0.0319)	
training:	Epoch: [43][79/204]	Loss 0.1524 (0.0335)	
training:	Epoch: [43][80/204]	Loss 0.0085 (0.0331)	
training:	Epoch: [43][81/204]	Loss 0.0054 (0.0328)	
training:	Epoch: [43][82/204]	Loss 0.0608 (0.0331)	
training:	Epoch: [43][83/204]	Loss 0.0062 (0.0328)	
training:	Epoch: [43][84/204]	Loss 0.3248 (0.0363)	
training:	Epoch: [43][85/204]	Loss 0.1510 (0.0376)	
training:	Epoch: [43][86/204]	Loss 0.0140 (0.0374)	
training:	Epoch: [43][87/204]	Loss 0.0158 (0.0371)	
training:	Epoch: [43][88/204]	Loss 0.0110 (0.0368)	
training:	Epoch: [43][89/204]	Loss 0.0058 (0.0365)	
training:	Epoch: [43][90/204]	Loss 0.0063 (0.0361)	
training:	Epoch: [43][91/204]	Loss 0.0080 (0.0358)	
training:	Epoch: [43][92/204]	Loss 0.0056 (0.0355)	
training:	Epoch: [43][93/204]	Loss 0.0063 (0.0352)	
training:	Epoch: [43][94/204]	Loss 0.0062 (0.0349)	
training:	Epoch: [43][95/204]	Loss 0.0056 (0.0346)	
training:	Epoch: [43][96/204]	Loss 0.0079 (0.0343)	
training:	Epoch: [43][97/204]	Loss 0.0226 (0.0342)	
training:	Epoch: [43][98/204]	Loss 0.0282 (0.0341)	
training:	Epoch: [43][99/204]	Loss 0.0614 (0.0344)	
training:	Epoch: [43][100/204]	Loss 0.1598 (0.0356)	
training:	Epoch: [43][101/204]	Loss 0.0068 (0.0354)	
training:	Epoch: [43][102/204]	Loss 0.0076 (0.0351)	
training:	Epoch: [43][103/204]	Loss 0.0066 (0.0348)	
training:	Epoch: [43][104/204]	Loss 0.0054 (0.0345)	
training:	Epoch: [43][105/204]	Loss 0.0070 (0.0343)	
training:	Epoch: [43][106/204]	Loss 0.2130 (0.0360)	
training:	Epoch: [43][107/204]	Loss 0.0062 (0.0357)	
training:	Epoch: [43][108/204]	Loss 0.0484 (0.0358)	
training:	Epoch: [43][109/204]	Loss 0.1427 (0.0368)	
training:	Epoch: [43][110/204]	Loss 0.0094 (0.0365)	
training:	Epoch: [43][111/204]	Loss 0.0059 (0.0362)	
training:	Epoch: [43][112/204]	Loss 0.0252 (0.0361)	
training:	Epoch: [43][113/204]	Loss 0.0197 (0.0360)	
training:	Epoch: [43][114/204]	Loss 0.0068 (0.0357)	
training:	Epoch: [43][115/204]	Loss 0.0074 (0.0355)	
training:	Epoch: [43][116/204]	Loss 0.0095 (0.0353)	
training:	Epoch: [43][117/204]	Loss 0.0443 (0.0354)	
training:	Epoch: [43][118/204]	Loss 0.1581 (0.0364)	
training:	Epoch: [43][119/204]	Loss 0.0065 (0.0361)	
training:	Epoch: [43][120/204]	Loss 0.0057 (0.0359)	
training:	Epoch: [43][121/204]	Loss 0.0764 (0.0362)	
training:	Epoch: [43][122/204]	Loss 0.0097 (0.0360)	
training:	Epoch: [43][123/204]	Loss 0.1036 (0.0366)	
training:	Epoch: [43][124/204]	Loss 0.0465 (0.0366)	
training:	Epoch: [43][125/204]	Loss 0.0067 (0.0364)	
training:	Epoch: [43][126/204]	Loss 0.1050 (0.0369)	
training:	Epoch: [43][127/204]	Loss 0.0114 (0.0367)	
training:	Epoch: [43][128/204]	Loss 0.0068 (0.0365)	
training:	Epoch: [43][129/204]	Loss 0.0063 (0.0363)	
training:	Epoch: [43][130/204]	Loss 0.0099 (0.0361)	
training:	Epoch: [43][131/204]	Loss 0.0156 (0.0359)	
training:	Epoch: [43][132/204]	Loss 0.0071 (0.0357)	
training:	Epoch: [43][133/204]	Loss 0.0052 (0.0355)	
training:	Epoch: [43][134/204]	Loss 0.0071 (0.0353)	
training:	Epoch: [43][135/204]	Loss 0.1604 (0.0362)	
training:	Epoch: [43][136/204]	Loss 0.0562 (0.0363)	
training:	Epoch: [43][137/204]	Loss 0.0155 (0.0362)	
training:	Epoch: [43][138/204]	Loss 0.0066 (0.0360)	
training:	Epoch: [43][139/204]	Loss 0.0061 (0.0357)	
training:	Epoch: [43][140/204]	Loss 0.0063 (0.0355)	
training:	Epoch: [43][141/204]	Loss 0.0062 (0.0353)	
training:	Epoch: [43][142/204]	Loss 0.0106 (0.0352)	
training:	Epoch: [43][143/204]	Loss 0.0123 (0.0350)	
training:	Epoch: [43][144/204]	Loss 0.0269 (0.0349)	
training:	Epoch: [43][145/204]	Loss 0.1670 (0.0358)	
training:	Epoch: [43][146/204]	Loss 0.0201 (0.0357)	
training:	Epoch: [43][147/204]	Loss 0.0065 (0.0355)	
training:	Epoch: [43][148/204]	Loss 0.0917 (0.0359)	
training:	Epoch: [43][149/204]	Loss 0.0064 (0.0357)	
training:	Epoch: [43][150/204]	Loss 0.0093 (0.0355)	
training:	Epoch: [43][151/204]	Loss 0.0071 (0.0354)	
training:	Epoch: [43][152/204]	Loss 0.0195 (0.0353)	
training:	Epoch: [43][153/204]	Loss 0.0096 (0.0351)	
training:	Epoch: [43][154/204]	Loss 0.0052 (0.0349)	
training:	Epoch: [43][155/204]	Loss 0.1880 (0.0359)	
training:	Epoch: [43][156/204]	Loss 0.0076 (0.0357)	
training:	Epoch: [43][157/204]	Loss 0.1701 (0.0366)	
training:	Epoch: [43][158/204]	Loss 0.0070 (0.0364)	
training:	Epoch: [43][159/204]	Loss 0.0057 (0.0362)	
training:	Epoch: [43][160/204]	Loss 0.0647 (0.0364)	
training:	Epoch: [43][161/204]	Loss 0.0109 (0.0362)	
training:	Epoch: [43][162/204]	Loss 0.0076 (0.0360)	
training:	Epoch: [43][163/204]	Loss 0.0077 (0.0358)	
training:	Epoch: [43][164/204]	Loss 0.0391 (0.0359)	
training:	Epoch: [43][165/204]	Loss 0.0057 (0.0357)	
training:	Epoch: [43][166/204]	Loss 0.0060 (0.0355)	
training:	Epoch: [43][167/204]	Loss 0.0369 (0.0355)	
training:	Epoch: [43][168/204]	Loss 0.0069 (0.0353)	
training:	Epoch: [43][169/204]	Loss 0.0484 (0.0354)	
training:	Epoch: [43][170/204]	Loss 0.0908 (0.0357)	
training:	Epoch: [43][171/204]	Loss 0.0353 (0.0357)	
training:	Epoch: [43][172/204]	Loss 0.0068 (0.0356)	
training:	Epoch: [43][173/204]	Loss 0.1655 (0.0363)	
training:	Epoch: [43][174/204]	Loss 0.0066 (0.0362)	
training:	Epoch: [43][175/204]	Loss 0.0060 (0.0360)	
training:	Epoch: [43][176/204]	Loss 0.0056 (0.0358)	
training:	Epoch: [43][177/204]	Loss 0.0081 (0.0356)	
training:	Epoch: [43][178/204]	Loss 0.0060 (0.0355)	
training:	Epoch: [43][179/204]	Loss 0.0065 (0.0353)	
training:	Epoch: [43][180/204]	Loss 0.0173 (0.0352)	
training:	Epoch: [43][181/204]	Loss 0.0070 (0.0351)	
training:	Epoch: [43][182/204]	Loss 0.0100 (0.0349)	
training:	Epoch: [43][183/204]	Loss 0.0125 (0.0348)	
training:	Epoch: [43][184/204]	Loss 0.1121 (0.0352)	
training:	Epoch: [43][185/204]	Loss 0.0221 (0.0352)	
training:	Epoch: [43][186/204]	Loss 0.0208 (0.0351)	
training:	Epoch: [43][187/204]	Loss 0.0068 (0.0349)	
training:	Epoch: [43][188/204]	Loss 0.0069 (0.0348)	
training:	Epoch: [43][189/204]	Loss 0.0062 (0.0346)	
training:	Epoch: [43][190/204]	Loss 0.1831 (0.0354)	
training:	Epoch: [43][191/204]	Loss 0.0593 (0.0355)	
training:	Epoch: [43][192/204]	Loss 0.0087 (0.0354)	
training:	Epoch: [43][193/204]	Loss 0.0062 (0.0352)	
training:	Epoch: [43][194/204]	Loss 0.0083 (0.0351)	
training:	Epoch: [43][195/204]	Loss 0.0167 (0.0350)	
training:	Epoch: [43][196/204]	Loss 0.0063 (0.0349)	
training:	Epoch: [43][197/204]	Loss 0.0170 (0.0348)	
training:	Epoch: [43][198/204]	Loss 0.0090 (0.0346)	
training:	Epoch: [43][199/204]	Loss 0.0062 (0.0345)	
training:	Epoch: [43][200/204]	Loss 0.0181 (0.0344)	
training:	Epoch: [43][201/204]	Loss 0.0091 (0.0343)	
training:	Epoch: [43][202/204]	Loss 0.0065 (0.0342)	
training:	Epoch: [43][203/204]	Loss 0.0569 (0.0343)	
training:	Epoch: [43][204/204]	Loss 0.0063 (0.0341)	
Training:	 Loss: 0.0341

Training:	 ACC: 0.9967 0.9968 0.9979 0.9955
Validation:	 ACC: 0.7957 0.7983 0.8516 0.7399
Validation:	 Best_BACC: 0.8326 0.8331 0.8424 0.8229
Validation:	 Loss: 0.8190
Pretraining:	Epoch 44/200
----------
training:	Epoch: [44][1/204]	Loss 0.0053 (0.0053)	
training:	Epoch: [44][2/204]	Loss 0.0163 (0.0108)	
training:	Epoch: [44][3/204]	Loss 0.0346 (0.0187)	
training:	Epoch: [44][4/204]	Loss 0.0094 (0.0164)	
training:	Epoch: [44][5/204]	Loss 0.0100 (0.0151)	
training:	Epoch: [44][6/204]	Loss 0.0206 (0.0160)	
training:	Epoch: [44][7/204]	Loss 0.0104 (0.0152)	
training:	Epoch: [44][8/204]	Loss 0.0428 (0.0187)	
training:	Epoch: [44][9/204]	Loss 0.0087 (0.0176)	
training:	Epoch: [44][10/204]	Loss 0.0509 (0.0209)	
training:	Epoch: [44][11/204]	Loss 0.0066 (0.0196)	
training:	Epoch: [44][12/204]	Loss 0.0172 (0.0194)	
training:	Epoch: [44][13/204]	Loss 0.1438 (0.0290)	
training:	Epoch: [44][14/204]	Loss 0.0114 (0.0277)	
training:	Epoch: [44][15/204]	Loss 0.0106 (0.0266)	
training:	Epoch: [44][16/204]	Loss 0.0101 (0.0255)	
training:	Epoch: [44][17/204]	Loss 0.0121 (0.0248)	
training:	Epoch: [44][18/204]	Loss 0.0081 (0.0238)	
training:	Epoch: [44][19/204]	Loss 0.0647 (0.0260)	
training:	Epoch: [44][20/204]	Loss 0.0051 (0.0249)	
training:	Epoch: [44][21/204]	Loss 0.0063 (0.0241)	
training:	Epoch: [44][22/204]	Loss 0.0073 (0.0233)	
training:	Epoch: [44][23/204]	Loss 0.0070 (0.0226)	
training:	Epoch: [44][24/204]	Loss 0.0063 (0.0219)	
training:	Epoch: [44][25/204]	Loss 0.0075 (0.0213)	
training:	Epoch: [44][26/204]	Loss 0.1642 (0.0268)	
training:	Epoch: [44][27/204]	Loss 0.0053 (0.0260)	
training:	Epoch: [44][28/204]	Loss 0.0054 (0.0253)	
training:	Epoch: [44][29/204]	Loss 0.0057 (0.0246)	
training:	Epoch: [44][30/204]	Loss 0.0448 (0.0253)	
training:	Epoch: [44][31/204]	Loss 0.0087 (0.0247)	
training:	Epoch: [44][32/204]	Loss 0.0050 (0.0241)	
training:	Epoch: [44][33/204]	Loss 0.1095 (0.0267)	
training:	Epoch: [44][34/204]	Loss 0.0105 (0.0262)	
training:	Epoch: [44][35/204]	Loss 0.0062 (0.0257)	
training:	Epoch: [44][36/204]	Loss 0.0055 (0.0251)	
training:	Epoch: [44][37/204]	Loss 0.0111 (0.0247)	
training:	Epoch: [44][38/204]	Loss 0.0494 (0.0254)	
training:	Epoch: [44][39/204]	Loss 0.2488 (0.0311)	
training:	Epoch: [44][40/204]	Loss 0.0085 (0.0305)	
training:	Epoch: [44][41/204]	Loss 0.0123 (0.0301)	
training:	Epoch: [44][42/204]	Loss 0.0055 (0.0295)	
training:	Epoch: [44][43/204]	Loss 0.0047 (0.0289)	
training:	Epoch: [44][44/204]	Loss 0.0049 (0.0284)	
training:	Epoch: [44][45/204]	Loss 0.1677 (0.0315)	
training:	Epoch: [44][46/204]	Loss 0.0044 (0.0309)	
training:	Epoch: [44][47/204]	Loss 0.0124 (0.0305)	
training:	Epoch: [44][48/204]	Loss 0.0056 (0.0300)	
training:	Epoch: [44][49/204]	Loss 0.0054 (0.0295)	
training:	Epoch: [44][50/204]	Loss 0.0067 (0.0290)	
training:	Epoch: [44][51/204]	Loss 0.0174 (0.0288)	
training:	Epoch: [44][52/204]	Loss 0.0068 (0.0284)	
training:	Epoch: [44][53/204]	Loss 0.0077 (0.0280)	
training:	Epoch: [44][54/204]	Loss 0.0058 (0.0276)	
training:	Epoch: [44][55/204]	Loss 0.0077 (0.0272)	
training:	Epoch: [44][56/204]	Loss 0.0082 (0.0269)	
training:	Epoch: [44][57/204]	Loss 0.0062 (0.0265)	
training:	Epoch: [44][58/204]	Loss 0.0057 (0.0262)	
training:	Epoch: [44][59/204]	Loss 0.0056 (0.0258)	
training:	Epoch: [44][60/204]	Loss 0.0085 (0.0255)	
training:	Epoch: [44][61/204]	Loss 0.0063 (0.0252)	
training:	Epoch: [44][62/204]	Loss 0.0050 (0.0249)	
training:	Epoch: [44][63/204]	Loss 0.0086 (0.0246)	
training:	Epoch: [44][64/204]	Loss 0.0060 (0.0243)	
training:	Epoch: [44][65/204]	Loss 0.2061 (0.0271)	
training:	Epoch: [44][66/204]	Loss 0.0053 (0.0268)	
training:	Epoch: [44][67/204]	Loss 0.0108 (0.0266)	
training:	Epoch: [44][68/204]	Loss 0.0132 (0.0264)	
training:	Epoch: [44][69/204]	Loss 0.0509 (0.0267)	
training:	Epoch: [44][70/204]	Loss 0.0072 (0.0264)	
training:	Epoch: [44][71/204]	Loss 0.0052 (0.0261)	
training:	Epoch: [44][72/204]	Loss 0.0058 (0.0258)	
training:	Epoch: [44][73/204]	Loss 0.0067 (0.0256)	
training:	Epoch: [44][74/204]	Loss 0.0111 (0.0254)	
training:	Epoch: [44][75/204]	Loss 0.0056 (0.0251)	
training:	Epoch: [44][76/204]	Loss 0.0058 (0.0249)	
training:	Epoch: [44][77/204]	Loss 0.0094 (0.0247)	
training:	Epoch: [44][78/204]	Loss 0.1081 (0.0257)	
training:	Epoch: [44][79/204]	Loss 0.0052 (0.0255)	
training:	Epoch: [44][80/204]	Loss 0.0097 (0.0253)	
training:	Epoch: [44][81/204]	Loss 0.0445 (0.0255)	
training:	Epoch: [44][82/204]	Loss 0.1623 (0.0272)	
training:	Epoch: [44][83/204]	Loss 0.0314 (0.0272)	
training:	Epoch: [44][84/204]	Loss 0.1357 (0.0285)	
training:	Epoch: [44][85/204]	Loss 0.0194 (0.0284)	
training:	Epoch: [44][86/204]	Loss 0.0074 (0.0282)	
training:	Epoch: [44][87/204]	Loss 0.0051 (0.0279)	
training:	Epoch: [44][88/204]	Loss 0.1233 (0.0290)	
training:	Epoch: [44][89/204]	Loss 0.0053 (0.0287)	
training:	Epoch: [44][90/204]	Loss 0.0055 (0.0285)	
training:	Epoch: [44][91/204]	Loss 0.3093 (0.0316)	
training:	Epoch: [44][92/204]	Loss 0.0153 (0.0314)	
training:	Epoch: [44][93/204]	Loss 0.1844 (0.0330)	
training:	Epoch: [44][94/204]	Loss 0.0053 (0.0327)	
training:	Epoch: [44][95/204]	Loss 0.0112 (0.0325)	
training:	Epoch: [44][96/204]	Loss 0.0083 (0.0323)	
training:	Epoch: [44][97/204]	Loss 0.1611 (0.0336)	
training:	Epoch: [44][98/204]	Loss 0.0065 (0.0333)	
training:	Epoch: [44][99/204]	Loss 0.0057 (0.0330)	
training:	Epoch: [44][100/204]	Loss 0.0068 (0.0328)	
training:	Epoch: [44][101/204]	Loss 0.0153 (0.0326)	
training:	Epoch: [44][102/204]	Loss 0.0084 (0.0324)	
training:	Epoch: [44][103/204]	Loss 0.0065 (0.0321)	
training:	Epoch: [44][104/204]	Loss 0.1757 (0.0335)	
training:	Epoch: [44][105/204]	Loss 0.0167 (0.0333)	
training:	Epoch: [44][106/204]	Loss 0.0072 (0.0331)	
training:	Epoch: [44][107/204]	Loss 0.0346 (0.0331)	
training:	Epoch: [44][108/204]	Loss 0.0059 (0.0328)	
training:	Epoch: [44][109/204]	Loss 0.0056 (0.0326)	
training:	Epoch: [44][110/204]	Loss 0.0061 (0.0324)	
training:	Epoch: [44][111/204]	Loss 0.0051 (0.0321)	
training:	Epoch: [44][112/204]	Loss 0.0057 (0.0319)	
training:	Epoch: [44][113/204]	Loss 0.0053 (0.0316)	
training:	Epoch: [44][114/204]	Loss 0.0070 (0.0314)	
training:	Epoch: [44][115/204]	Loss 0.0049 (0.0312)	
training:	Epoch: [44][116/204]	Loss 0.0058 (0.0310)	
training:	Epoch: [44][117/204]	Loss 0.0337 (0.0310)	
training:	Epoch: [44][118/204]	Loss 0.1554 (0.0320)	
training:	Epoch: [44][119/204]	Loss 0.0053 (0.0318)	
training:	Epoch: [44][120/204]	Loss 0.0062 (0.0316)	
training:	Epoch: [44][121/204]	Loss 0.0080 (0.0314)	
training:	Epoch: [44][122/204]	Loss 0.0058 (0.0312)	
training:	Epoch: [44][123/204]	Loss 0.0047 (0.0310)	
training:	Epoch: [44][124/204]	Loss 0.0056 (0.0308)	
training:	Epoch: [44][125/204]	Loss 0.0085 (0.0306)	
training:	Epoch: [44][126/204]	Loss 0.0360 (0.0306)	
training:	Epoch: [44][127/204]	Loss 0.0097 (0.0305)	
training:	Epoch: [44][128/204]	Loss 0.0050 (0.0303)	
training:	Epoch: [44][129/204]	Loss 0.0152 (0.0302)	
training:	Epoch: [44][130/204]	Loss 0.0048 (0.0300)	
training:	Epoch: [44][131/204]	Loss 0.0099 (0.0298)	
training:	Epoch: [44][132/204]	Loss 0.0097 (0.0297)	
training:	Epoch: [44][133/204]	Loss 0.0131 (0.0295)	
training:	Epoch: [44][134/204]	Loss 0.0048 (0.0294)	
training:	Epoch: [44][135/204]	Loss 0.0478 (0.0295)	
training:	Epoch: [44][136/204]	Loss 0.0860 (0.0299)	
training:	Epoch: [44][137/204]	Loss 0.1543 (0.0308)	
training:	Epoch: [44][138/204]	Loss 0.0065 (0.0306)	
training:	Epoch: [44][139/204]	Loss 0.1827 (0.0317)	
training:	Epoch: [44][140/204]	Loss 0.1541 (0.0326)	
training:	Epoch: [44][141/204]	Loss 0.0111 (0.0325)	
training:	Epoch: [44][142/204]	Loss 0.0061 (0.0323)	
training:	Epoch: [44][143/204]	Loss 0.0063 (0.0321)	
training:	Epoch: [44][144/204]	Loss 0.0049 (0.0319)	
training:	Epoch: [44][145/204]	Loss 0.0055 (0.0317)	
training:	Epoch: [44][146/204]	Loss 0.0056 (0.0315)	
training:	Epoch: [44][147/204]	Loss 0.0049 (0.0314)	
training:	Epoch: [44][148/204]	Loss 0.0078 (0.0312)	
training:	Epoch: [44][149/204]	Loss 0.0525 (0.0313)	
training:	Epoch: [44][150/204]	Loss 0.0062 (0.0312)	
training:	Epoch: [44][151/204]	Loss 0.0053 (0.0310)	
training:	Epoch: [44][152/204]	Loss 0.0065 (0.0308)	
training:	Epoch: [44][153/204]	Loss 0.0055 (0.0307)	
training:	Epoch: [44][154/204]	Loss 0.0052 (0.0305)	
training:	Epoch: [44][155/204]	Loss 0.0058 (0.0304)	
training:	Epoch: [44][156/204]	Loss 0.0078 (0.0302)	
training:	Epoch: [44][157/204]	Loss 0.0071 (0.0301)	
training:	Epoch: [44][158/204]	Loss 0.0180 (0.0300)	
training:	Epoch: [44][159/204]	Loss 0.1707 (0.0309)	
training:	Epoch: [44][160/204]	Loss 0.1516 (0.0316)	
training:	Epoch: [44][161/204]	Loss 0.0797 (0.0319)	
training:	Epoch: [44][162/204]	Loss 0.0058 (0.0318)	
training:	Epoch: [44][163/204]	Loss 0.0063 (0.0316)	
training:	Epoch: [44][164/204]	Loss 0.0058 (0.0314)	
training:	Epoch: [44][165/204]	Loss 0.0056 (0.0313)	
training:	Epoch: [44][166/204]	Loss 0.1315 (0.0319)	
training:	Epoch: [44][167/204]	Loss 0.0058 (0.0317)	
training:	Epoch: [44][168/204]	Loss 0.0076 (0.0316)	
training:	Epoch: [44][169/204]	Loss 0.0068 (0.0314)	
training:	Epoch: [44][170/204]	Loss 0.0094 (0.0313)	
training:	Epoch: [44][171/204]	Loss 0.0056 (0.0312)	
training:	Epoch: [44][172/204]	Loss 0.0267 (0.0311)	
training:	Epoch: [44][173/204]	Loss 0.0054 (0.0310)	
training:	Epoch: [44][174/204]	Loss 0.0052 (0.0308)	
training:	Epoch: [44][175/204]	Loss 0.0152 (0.0308)	
training:	Epoch: [44][176/204]	Loss 0.0341 (0.0308)	
training:	Epoch: [44][177/204]	Loss 0.1665 (0.0315)	
training:	Epoch: [44][178/204]	Loss 0.0051 (0.0314)	
training:	Epoch: [44][179/204]	Loss 0.0066 (0.0313)	
training:	Epoch: [44][180/204]	Loss 0.0078 (0.0311)	
training:	Epoch: [44][181/204]	Loss 0.0089 (0.0310)	
training:	Epoch: [44][182/204]	Loss 0.0962 (0.0314)	
training:	Epoch: [44][183/204]	Loss 0.0069 (0.0312)	
training:	Epoch: [44][184/204]	Loss 0.0102 (0.0311)	
training:	Epoch: [44][185/204]	Loss 0.0077 (0.0310)	
training:	Epoch: [44][186/204]	Loss 0.0064 (0.0309)	
training:	Epoch: [44][187/204]	Loss 0.1009 (0.0312)	
training:	Epoch: [44][188/204]	Loss 0.0166 (0.0311)	
training:	Epoch: [44][189/204]	Loss 0.0068 (0.0310)	
training:	Epoch: [44][190/204]	Loss 0.0064 (0.0309)	
training:	Epoch: [44][191/204]	Loss 0.0087 (0.0308)	
training:	Epoch: [44][192/204]	Loss 0.0061 (0.0306)	
training:	Epoch: [44][193/204]	Loss 0.0134 (0.0306)	
training:	Epoch: [44][194/204]	Loss 0.0095 (0.0304)	
training:	Epoch: [44][195/204]	Loss 0.0150 (0.0304)	
training:	Epoch: [44][196/204]	Loss 0.0051 (0.0302)	
training:	Epoch: [44][197/204]	Loss 0.1834 (0.0310)	
training:	Epoch: [44][198/204]	Loss 0.0059 (0.0309)	
training:	Epoch: [44][199/204]	Loss 0.0049 (0.0308)	
training:	Epoch: [44][200/204]	Loss 0.0124 (0.0307)	
training:	Epoch: [44][201/204]	Loss 0.0062 (0.0305)	
training:	Epoch: [44][202/204]	Loss 0.0062 (0.0304)	
training:	Epoch: [44][203/204]	Loss 0.0050 (0.0303)	
training:	Epoch: [44][204/204]	Loss 0.0072 (0.0302)	
Training:	 Loss: 0.0301

Training:	 ACC: 0.9970 0.9971 0.9982 0.9959
Validation:	 ACC: 0.8009 0.8015 0.8147 0.7870
Validation:	 Best_BACC: 0.8326 0.8331 0.8424 0.8229
Validation:	 Loss: 0.8053
Pretraining:	Epoch 45/200
----------
training:	Epoch: [45][1/204]	Loss 0.0055 (0.0055)	
training:	Epoch: [45][2/204]	Loss 0.0052 (0.0054)	
training:	Epoch: [45][3/204]	Loss 0.0056 (0.0054)	
training:	Epoch: [45][4/204]	Loss 0.0569 (0.0183)	
training:	Epoch: [45][5/204]	Loss 0.0057 (0.0158)	
training:	Epoch: [45][6/204]	Loss 0.0051 (0.0140)	
training:	Epoch: [45][7/204]	Loss 0.0054 (0.0128)	
training:	Epoch: [45][8/204]	Loss 0.0055 (0.0119)	
training:	Epoch: [45][9/204]	Loss 0.0508 (0.0162)	
training:	Epoch: [45][10/204]	Loss 0.1830 (0.0329)	
training:	Epoch: [45][11/204]	Loss 0.0488 (0.0343)	
training:	Epoch: [45][12/204]	Loss 0.0057 (0.0319)	
training:	Epoch: [45][13/204]	Loss 0.1487 (0.0409)	
training:	Epoch: [45][14/204]	Loss 0.1370 (0.0478)	
training:	Epoch: [45][15/204]	Loss 0.0057 (0.0450)	
training:	Epoch: [45][16/204]	Loss 0.1752 (0.0531)	
training:	Epoch: [45][17/204]	Loss 0.0052 (0.0503)	
training:	Epoch: [45][18/204]	Loss 0.1608 (0.0564)	
training:	Epoch: [45][19/204]	Loss 0.0052 (0.0537)	
training:	Epoch: [45][20/204]	Loss 0.0064 (0.0514)	
training:	Epoch: [45][21/204]	Loss 0.0057 (0.0492)	
training:	Epoch: [45][22/204]	Loss 0.0068 (0.0473)	
training:	Epoch: [45][23/204]	Loss 0.0078 (0.0455)	
training:	Epoch: [45][24/204]	Loss 0.0125 (0.0442)	
training:	Epoch: [45][25/204]	Loss 0.0056 (0.0426)	
training:	Epoch: [45][26/204]	Loss 0.1645 (0.0473)	
training:	Epoch: [45][27/204]	Loss 0.0737 (0.0483)	
training:	Epoch: [45][28/204]	Loss 0.0106 (0.0469)	
training:	Epoch: [45][29/204]	Loss 0.0908 (0.0485)	
training:	Epoch: [45][30/204]	Loss 0.0068 (0.0471)	
training:	Epoch: [45][31/204]	Loss 0.0099 (0.0459)	
training:	Epoch: [45][32/204]	Loss 0.0075 (0.0447)	
training:	Epoch: [45][33/204]	Loss 0.0057 (0.0435)	
training:	Epoch: [45][34/204]	Loss 0.1696 (0.0472)	
training:	Epoch: [45][35/204]	Loss 0.1010 (0.0487)	
training:	Epoch: [45][36/204]	Loss 0.0050 (0.0475)	
training:	Epoch: [45][37/204]	Loss 0.0051 (0.0464)	
training:	Epoch: [45][38/204]	Loss 0.0064 (0.0453)	
training:	Epoch: [45][39/204]	Loss 0.0070 (0.0443)	
training:	Epoch: [45][40/204]	Loss 0.0077 (0.0434)	
training:	Epoch: [45][41/204]	Loss 0.0071 (0.0425)	
training:	Epoch: [45][42/204]	Loss 0.0133 (0.0418)	
training:	Epoch: [45][43/204]	Loss 0.0246 (0.0414)	
training:	Epoch: [45][44/204]	Loss 0.0046 (0.0406)	
training:	Epoch: [45][45/204]	Loss 0.0424 (0.0406)	
training:	Epoch: [45][46/204]	Loss 0.0111 (0.0400)	
training:	Epoch: [45][47/204]	Loss 0.0075 (0.0393)	
training:	Epoch: [45][48/204]	Loss 0.0047 (0.0386)	
training:	Epoch: [45][49/204]	Loss 0.0063 (0.0379)	
training:	Epoch: [45][50/204]	Loss 0.0055 (0.0373)	
training:	Epoch: [45][51/204]	Loss 0.0084 (0.0367)	
training:	Epoch: [45][52/204]	Loss 0.0062 (0.0361)	
training:	Epoch: [45][53/204]	Loss 0.0056 (0.0356)	
training:	Epoch: [45][54/204]	Loss 0.0128 (0.0351)	
training:	Epoch: [45][55/204]	Loss 0.0052 (0.0346)	
training:	Epoch: [45][56/204]	Loss 0.0061 (0.0341)	
training:	Epoch: [45][57/204]	Loss 0.0072 (0.0336)	
training:	Epoch: [45][58/204]	Loss 0.1356 (0.0354)	
training:	Epoch: [45][59/204]	Loss 0.0255 (0.0352)	
training:	Epoch: [45][60/204]	Loss 0.1275 (0.0367)	
training:	Epoch: [45][61/204]	Loss 0.1683 (0.0389)	
training:	Epoch: [45][62/204]	Loss 0.0093 (0.0384)	
training:	Epoch: [45][63/204]	Loss 0.0057 (0.0379)	
training:	Epoch: [45][64/204]	Loss 0.0059 (0.0374)	
training:	Epoch: [45][65/204]	Loss 0.0085 (0.0370)	
training:	Epoch: [45][66/204]	Loss 0.0067 (0.0365)	
training:	Epoch: [45][67/204]	Loss 0.0065 (0.0360)	
training:	Epoch: [45][68/204]	Loss 0.0056 (0.0356)	
training:	Epoch: [45][69/204]	Loss 0.0064 (0.0352)	
training:	Epoch: [45][70/204]	Loss 0.0054 (0.0348)	
training:	Epoch: [45][71/204]	Loss 0.0087 (0.0344)	
training:	Epoch: [45][72/204]	Loss 0.0083 (0.0340)	
training:	Epoch: [45][73/204]	Loss 0.0049 (0.0336)	
training:	Epoch: [45][74/204]	Loss 0.0060 (0.0332)	
training:	Epoch: [45][75/204]	Loss 0.0082 (0.0329)	
training:	Epoch: [45][76/204]	Loss 0.0080 (0.0326)	
training:	Epoch: [45][77/204]	Loss 0.1720 (0.0344)	
training:	Epoch: [45][78/204]	Loss 0.0072 (0.0340)	
training:	Epoch: [45][79/204]	Loss 0.0106 (0.0338)	
training:	Epoch: [45][80/204]	Loss 0.0057 (0.0334)	
training:	Epoch: [45][81/204]	Loss 0.0072 (0.0331)	
training:	Epoch: [45][82/204]	Loss 0.0061 (0.0327)	
training:	Epoch: [45][83/204]	Loss 0.1646 (0.0343)	
training:	Epoch: [45][84/204]	Loss 0.0219 (0.0342)	
training:	Epoch: [45][85/204]	Loss 0.0057 (0.0339)	
training:	Epoch: [45][86/204]	Loss 0.1743 (0.0355)	
training:	Epoch: [45][87/204]	Loss 0.0059 (0.0351)	
training:	Epoch: [45][88/204]	Loss 0.1524 (0.0365)	
training:	Epoch: [45][89/204]	Loss 0.0064 (0.0361)	
training:	Epoch: [45][90/204]	Loss 0.0104 (0.0359)	
training:	Epoch: [45][91/204]	Loss 0.0054 (0.0355)	
training:	Epoch: [45][92/204]	Loss 0.0083 (0.0352)	
training:	Epoch: [45][93/204]	Loss 0.0061 (0.0349)	
training:	Epoch: [45][94/204]	Loss 0.0052 (0.0346)	
training:	Epoch: [45][95/204]	Loss 0.0059 (0.0343)	
training:	Epoch: [45][96/204]	Loss 0.0061 (0.0340)	
training:	Epoch: [45][97/204]	Loss 0.0048 (0.0337)	
training:	Epoch: [45][98/204]	Loss 0.0052 (0.0334)	
training:	Epoch: [45][99/204]	Loss 0.0057 (0.0331)	
training:	Epoch: [45][100/204]	Loss 0.1147 (0.0339)	
training:	Epoch: [45][101/204]	Loss 0.0071 (0.0337)	
training:	Epoch: [45][102/204]	Loss 0.0070 (0.0334)	
training:	Epoch: [45][103/204]	Loss 0.0051 (0.0331)	
training:	Epoch: [45][104/204]	Loss 0.0067 (0.0329)	
training:	Epoch: [45][105/204]	Loss 0.0345 (0.0329)	
training:	Epoch: [45][106/204]	Loss 0.0045 (0.0326)	
training:	Epoch: [45][107/204]	Loss 0.0051 (0.0324)	
training:	Epoch: [45][108/204]	Loss 0.0059 (0.0321)	
training:	Epoch: [45][109/204]	Loss 0.0100 (0.0319)	
training:	Epoch: [45][110/204]	Loss 0.0055 (0.0317)	
training:	Epoch: [45][111/204]	Loss 0.0060 (0.0315)	
training:	Epoch: [45][112/204]	Loss 0.1234 (0.0323)	
training:	Epoch: [45][113/204]	Loss 0.0189 (0.0322)	
training:	Epoch: [45][114/204]	Loss 0.0054 (0.0319)	
training:	Epoch: [45][115/204]	Loss 0.0067 (0.0317)	
training:	Epoch: [45][116/204]	Loss 0.0089 (0.0315)	
training:	Epoch: [45][117/204]	Loss 0.0271 (0.0315)	
training:	Epoch: [45][118/204]	Loss 0.0050 (0.0312)	
training:	Epoch: [45][119/204]	Loss 0.1553 (0.0323)	
training:	Epoch: [45][120/204]	Loss 0.0048 (0.0321)	
training:	Epoch: [45][121/204]	Loss 0.0058 (0.0318)	
training:	Epoch: [45][122/204]	Loss 0.0073 (0.0316)	
training:	Epoch: [45][123/204]	Loss 0.0061 (0.0314)	
training:	Epoch: [45][124/204]	Loss 0.1719 (0.0326)	
training:	Epoch: [45][125/204]	Loss 0.0046 (0.0323)	
training:	Epoch: [45][126/204]	Loss 0.0117 (0.0322)	
training:	Epoch: [45][127/204]	Loss 0.1088 (0.0328)	
training:	Epoch: [45][128/204]	Loss 0.0058 (0.0326)	
training:	Epoch: [45][129/204]	Loss 0.0060 (0.0324)	
training:	Epoch: [45][130/204]	Loss 0.0096 (0.0322)	
training:	Epoch: [45][131/204]	Loss 0.0178 (0.0321)	
training:	Epoch: [45][132/204]	Loss 0.1814 (0.0332)	
training:	Epoch: [45][133/204]	Loss 0.0076 (0.0330)	
training:	Epoch: [45][134/204]	Loss 0.0106 (0.0329)	
training:	Epoch: [45][135/204]	Loss 0.1542 (0.0338)	
training:	Epoch: [45][136/204]	Loss 0.0062 (0.0336)	
training:	Epoch: [45][137/204]	Loss 0.0063 (0.0334)	
training:	Epoch: [45][138/204]	Loss 0.0072 (0.0332)	
training:	Epoch: [45][139/204]	Loss 0.0052 (0.0330)	
training:	Epoch: [45][140/204]	Loss 0.0068 (0.0328)	
training:	Epoch: [45][141/204]	Loss 0.0067 (0.0326)	
training:	Epoch: [45][142/204]	Loss 0.0087 (0.0324)	
training:	Epoch: [45][143/204]	Loss 0.0053 (0.0322)	
training:	Epoch: [45][144/204]	Loss 0.0515 (0.0324)	
training:	Epoch: [45][145/204]	Loss 0.0076 (0.0322)	
training:	Epoch: [45][146/204]	Loss 0.0924 (0.0326)	
training:	Epoch: [45][147/204]	Loss 0.0067 (0.0324)	
training:	Epoch: [45][148/204]	Loss 0.0554 (0.0326)	
training:	Epoch: [45][149/204]	Loss 0.0051 (0.0324)	
training:	Epoch: [45][150/204]	Loss 0.0056 (0.0322)	
training:	Epoch: [45][151/204]	Loss 0.0050 (0.0320)	
training:	Epoch: [45][152/204]	Loss 0.0407 (0.0321)	
training:	Epoch: [45][153/204]	Loss 0.0071 (0.0319)	
training:	Epoch: [45][154/204]	Loss 0.0480 (0.0320)	
training:	Epoch: [45][155/204]	Loss 0.0060 (0.0319)	
training:	Epoch: [45][156/204]	Loss 0.0064 (0.0317)	
training:	Epoch: [45][157/204]	Loss 0.0091 (0.0316)	
training:	Epoch: [45][158/204]	Loss 0.0054 (0.0314)	
training:	Epoch: [45][159/204]	Loss 0.0132 (0.0313)	
training:	Epoch: [45][160/204]	Loss 0.0766 (0.0316)	
training:	Epoch: [45][161/204]	Loss 0.0056 (0.0314)	
training:	Epoch: [45][162/204]	Loss 0.0051 (0.0312)	
training:	Epoch: [45][163/204]	Loss 0.0113 (0.0311)	
training:	Epoch: [45][164/204]	Loss 0.1582 (0.0319)	
training:	Epoch: [45][165/204]	Loss 0.0070 (0.0317)	
training:	Epoch: [45][166/204]	Loss 0.0065 (0.0316)	
training:	Epoch: [45][167/204]	Loss 0.0087 (0.0315)	
training:	Epoch: [45][168/204]	Loss 0.0063 (0.0313)	
training:	Epoch: [45][169/204]	Loss 0.0055 (0.0312)	
training:	Epoch: [45][170/204]	Loss 0.0159 (0.0311)	
training:	Epoch: [45][171/204]	Loss 0.0115 (0.0310)	
training:	Epoch: [45][172/204]	Loss 0.0082 (0.0308)	
training:	Epoch: [45][173/204]	Loss 0.0160 (0.0307)	
training:	Epoch: [45][174/204]	Loss 0.0049 (0.0306)	
training:	Epoch: [45][175/204]	Loss 0.0061 (0.0304)	
training:	Epoch: [45][176/204]	Loss 0.0127 (0.0303)	
training:	Epoch: [45][177/204]	Loss 0.0057 (0.0302)	
training:	Epoch: [45][178/204]	Loss 0.1825 (0.0311)	
training:	Epoch: [45][179/204]	Loss 0.0765 (0.0313)	
training:	Epoch: [45][180/204]	Loss 0.0056 (0.0312)	
training:	Epoch: [45][181/204]	Loss 0.0070 (0.0310)	
training:	Epoch: [45][182/204]	Loss 0.0052 (0.0309)	
training:	Epoch: [45][183/204]	Loss 0.0079 (0.0308)	
training:	Epoch: [45][184/204]	Loss 0.0146 (0.0307)	
training:	Epoch: [45][185/204]	Loss 0.0053 (0.0305)	
training:	Epoch: [45][186/204]	Loss 0.0231 (0.0305)	
training:	Epoch: [45][187/204]	Loss 0.0900 (0.0308)	
training:	Epoch: [45][188/204]	Loss 0.0062 (0.0307)	
training:	Epoch: [45][189/204]	Loss 0.0061 (0.0306)	
training:	Epoch: [45][190/204]	Loss 0.0158 (0.0305)	
training:	Epoch: [45][191/204]	Loss 0.0105 (0.0304)	
training:	Epoch: [45][192/204]	Loss 0.0125 (0.0303)	
training:	Epoch: [45][193/204]	Loss 0.0071 (0.0302)	
training:	Epoch: [45][194/204]	Loss 0.0077 (0.0301)	
training:	Epoch: [45][195/204]	Loss 0.2193 (0.0310)	
training:	Epoch: [45][196/204]	Loss 0.0051 (0.0309)	
training:	Epoch: [45][197/204]	Loss 0.0049 (0.0308)	
training:	Epoch: [45][198/204]	Loss 0.0052 (0.0306)	
training:	Epoch: [45][199/204]	Loss 0.0049 (0.0305)	
training:	Epoch: [45][200/204]	Loss 0.0441 (0.0306)	
training:	Epoch: [45][201/204]	Loss 0.0060 (0.0304)	
training:	Epoch: [45][202/204]	Loss 0.0140 (0.0304)	
training:	Epoch: [45][203/204]	Loss 0.0058 (0.0302)	
training:	Epoch: [45][204/204]	Loss 0.0051 (0.0301)	
Training:	 Loss: 0.0301

Training:	 ACC: 0.9970 0.9971 0.9982 0.9959
Validation:	 ACC: 0.8057 0.8068 0.8301 0.7814
Validation:	 Best_BACC: 0.8326 0.8331 0.8424 0.8229
Validation:	 Loss: 0.8052
Pretraining:	Epoch 46/200
----------
training:	Epoch: [46][1/204]	Loss 0.0183 (0.0183)	
training:	Epoch: [46][2/204]	Loss 0.2018 (0.1100)	
training:	Epoch: [46][3/204]	Loss 0.0068 (0.0756)	
training:	Epoch: [46][4/204]	Loss 0.1603 (0.0968)	
training:	Epoch: [46][5/204]	Loss 0.0297 (0.0834)	
training:	Epoch: [46][6/204]	Loss 0.0243 (0.0735)	
training:	Epoch: [46][7/204]	Loss 0.1991 (0.0914)	
training:	Epoch: [46][8/204]	Loss 0.0063 (0.0808)	
training:	Epoch: [46][9/204]	Loss 0.0058 (0.0725)	
training:	Epoch: [46][10/204]	Loss 0.0046 (0.0657)	
training:	Epoch: [46][11/204]	Loss 0.0068 (0.0603)	
training:	Epoch: [46][12/204]	Loss 0.0055 (0.0558)	
training:	Epoch: [46][13/204]	Loss 0.0115 (0.0524)	
training:	Epoch: [46][14/204]	Loss 0.1274 (0.0577)	
training:	Epoch: [46][15/204]	Loss 0.0496 (0.0572)	
training:	Epoch: [46][16/204]	Loss 0.0043 (0.0539)	
training:	Epoch: [46][17/204]	Loss 0.0054 (0.0510)	
training:	Epoch: [46][18/204]	Loss 0.0056 (0.0485)	
training:	Epoch: [46][19/204]	Loss 0.0048 (0.0462)	
training:	Epoch: [46][20/204]	Loss 0.0068 (0.0442)	
training:	Epoch: [46][21/204]	Loss 0.0058 (0.0424)	
training:	Epoch: [46][22/204]	Loss 0.0050 (0.0407)	
training:	Epoch: [46][23/204]	Loss 0.0053 (0.0391)	
training:	Epoch: [46][24/204]	Loss 0.0106 (0.0380)	
training:	Epoch: [46][25/204]	Loss 0.1015 (0.0405)	
training:	Epoch: [46][26/204]	Loss 0.0056 (0.0392)	
training:	Epoch: [46][27/204]	Loss 0.0064 (0.0379)	
training:	Epoch: [46][28/204]	Loss 0.0052 (0.0368)	
training:	Epoch: [46][29/204]	Loss 0.1579 (0.0410)	
training:	Epoch: [46][30/204]	Loss 0.0251 (0.0404)	
training:	Epoch: [46][31/204]	Loss 0.0781 (0.0416)	
training:	Epoch: [46][32/204]	Loss 0.0864 (0.0430)	
training:	Epoch: [46][33/204]	Loss 0.0066 (0.0419)	
training:	Epoch: [46][34/204]	Loss 0.0045 (0.0408)	
training:	Epoch: [46][35/204]	Loss 0.0056 (0.0398)	
training:	Epoch: [46][36/204]	Loss 0.0049 (0.0389)	
training:	Epoch: [46][37/204]	Loss 0.2265 (0.0439)	
training:	Epoch: [46][38/204]	Loss 0.0416 (0.0439)	
training:	Epoch: [46][39/204]	Loss 0.0061 (0.0429)	
training:	Epoch: [46][40/204]	Loss 0.0107 (0.0421)	
training:	Epoch: [46][41/204]	Loss 0.0599 (0.0425)	
training:	Epoch: [46][42/204]	Loss 0.0051 (0.0416)	
training:	Epoch: [46][43/204]	Loss 0.0120 (0.0410)	
training:	Epoch: [46][44/204]	Loss 0.1380 (0.0432)	
training:	Epoch: [46][45/204]	Loss 0.0059 (0.0423)	
training:	Epoch: [46][46/204]	Loss 0.0064 (0.0415)	
training:	Epoch: [46][47/204]	Loss 0.0189 (0.0411)	
training:	Epoch: [46][48/204]	Loss 0.1275 (0.0429)	
training:	Epoch: [46][49/204]	Loss 0.0048 (0.0421)	
training:	Epoch: [46][50/204]	Loss 0.0163 (0.0416)	
training:	Epoch: [46][51/204]	Loss 0.2558 (0.0458)	
training:	Epoch: [46][52/204]	Loss 0.0145 (0.0452)	
training:	Epoch: [46][53/204]	Loss 0.0092 (0.0445)	
training:	Epoch: [46][54/204]	Loss 0.0055 (0.0438)	
training:	Epoch: [46][55/204]	Loss 0.0074 (0.0431)	
training:	Epoch: [46][56/204]	Loss 0.0061 (0.0424)	
training:	Epoch: [46][57/204]	Loss 0.0052 (0.0418)	
training:	Epoch: [46][58/204]	Loss 0.1670 (0.0440)	
training:	Epoch: [46][59/204]	Loss 0.0120 (0.0434)	
training:	Epoch: [46][60/204]	Loss 0.0049 (0.0428)	
training:	Epoch: [46][61/204]	Loss 0.0044 (0.0421)	
training:	Epoch: [46][62/204]	Loss 0.0050 (0.0415)	
training:	Epoch: [46][63/204]	Loss 0.1195 (0.0428)	
training:	Epoch: [46][64/204]	Loss 0.0111 (0.0423)	
training:	Epoch: [46][65/204]	Loss 0.0378 (0.0422)	
training:	Epoch: [46][66/204]	Loss 0.0130 (0.0418)	
training:	Epoch: [46][67/204]	Loss 0.0050 (0.0412)	
training:	Epoch: [46][68/204]	Loss 0.0051 (0.0407)	
training:	Epoch: [46][69/204]	Loss 0.0059 (0.0402)	
training:	Epoch: [46][70/204]	Loss 0.0071 (0.0397)	
training:	Epoch: [46][71/204]	Loss 0.0050 (0.0392)	
training:	Epoch: [46][72/204]	Loss 0.0057 (0.0388)	
training:	Epoch: [46][73/204]	Loss 0.0072 (0.0383)	
training:	Epoch: [46][74/204]	Loss 0.1826 (0.0403)	
training:	Epoch: [46][75/204]	Loss 0.1718 (0.0420)	
training:	Epoch: [46][76/204]	Loss 0.1366 (0.0433)	
training:	Epoch: [46][77/204]	Loss 0.0052 (0.0428)	
training:	Epoch: [46][78/204]	Loss 0.0068 (0.0423)	
training:	Epoch: [46][79/204]	Loss 0.0048 (0.0418)	
training:	Epoch: [46][80/204]	Loss 0.0055 (0.0414)	
training:	Epoch: [46][81/204]	Loss 0.0062 (0.0410)	
training:	Epoch: [46][82/204]	Loss 0.0062 (0.0405)	
training:	Epoch: [46][83/204]	Loss 0.0093 (0.0402)	
training:	Epoch: [46][84/204]	Loss 0.0056 (0.0397)	
training:	Epoch: [46][85/204]	Loss 0.0611 (0.0400)	
training:	Epoch: [46][86/204]	Loss 0.0059 (0.0396)	
training:	Epoch: [46][87/204]	Loss 0.0100 (0.0393)	
training:	Epoch: [46][88/204]	Loss 0.0063 (0.0389)	
training:	Epoch: [46][89/204]	Loss 0.0297 (0.0388)	
training:	Epoch: [46][90/204]	Loss 0.0141 (0.0385)	
training:	Epoch: [46][91/204]	Loss 0.0076 (0.0382)	
training:	Epoch: [46][92/204]	Loss 0.0623 (0.0384)	
training:	Epoch: [46][93/204]	Loss 0.0060 (0.0381)	
training:	Epoch: [46][94/204]	Loss 0.1535 (0.0393)	
training:	Epoch: [46][95/204]	Loss 0.0223 (0.0391)	
training:	Epoch: [46][96/204]	Loss 0.0057 (0.0388)	
training:	Epoch: [46][97/204]	Loss 0.0236 (0.0386)	
training:	Epoch: [46][98/204]	Loss 0.0062 (0.0383)	
training:	Epoch: [46][99/204]	Loss 0.0238 (0.0382)	
training:	Epoch: [46][100/204]	Loss 0.0162 (0.0379)	
training:	Epoch: [46][101/204]	Loss 0.0848 (0.0384)	
training:	Epoch: [46][102/204]	Loss 0.0053 (0.0381)	
training:	Epoch: [46][103/204]	Loss 0.0053 (0.0378)	
training:	Epoch: [46][104/204]	Loss 0.0057 (0.0374)	
training:	Epoch: [46][105/204]	Loss 0.0291 (0.0374)	
training:	Epoch: [46][106/204]	Loss 0.0066 (0.0371)	
training:	Epoch: [46][107/204]	Loss 0.0064 (0.0368)	
training:	Epoch: [46][108/204]	Loss 0.0075 (0.0365)	
training:	Epoch: [46][109/204]	Loss 0.1878 (0.0379)	
training:	Epoch: [46][110/204]	Loss 0.0053 (0.0376)	
training:	Epoch: [46][111/204]	Loss 0.0107 (0.0374)	
training:	Epoch: [46][112/204]	Loss 0.0051 (0.0371)	
training:	Epoch: [46][113/204]	Loss 0.0052 (0.0368)	
training:	Epoch: [46][114/204]	Loss 0.0066 (0.0365)	
training:	Epoch: [46][115/204]	Loss 0.0066 (0.0363)	
training:	Epoch: [46][116/204]	Loss 0.0068 (0.0360)	
training:	Epoch: [46][117/204]	Loss 0.0094 (0.0358)	
training:	Epoch: [46][118/204]	Loss 0.0070 (0.0355)	
training:	Epoch: [46][119/204]	Loss 0.0080 (0.0353)	
training:	Epoch: [46][120/204]	Loss 0.0082 (0.0351)	
training:	Epoch: [46][121/204]	Loss 0.0476 (0.0352)	
training:	Epoch: [46][122/204]	Loss 0.0172 (0.0350)	
training:	Epoch: [46][123/204]	Loss 0.1797 (0.0362)	
training:	Epoch: [46][124/204]	Loss 0.0054 (0.0360)	
training:	Epoch: [46][125/204]	Loss 0.0069 (0.0357)	
training:	Epoch: [46][126/204]	Loss 0.0055 (0.0355)	
training:	Epoch: [46][127/204]	Loss 0.0049 (0.0353)	
training:	Epoch: [46][128/204]	Loss 0.0062 (0.0350)	
training:	Epoch: [46][129/204]	Loss 0.0234 (0.0349)	
training:	Epoch: [46][130/204]	Loss 0.0140 (0.0348)	
training:	Epoch: [46][131/204]	Loss 0.1467 (0.0356)	
training:	Epoch: [46][132/204]	Loss 0.0072 (0.0354)	
training:	Epoch: [46][133/204]	Loss 0.0060 (0.0352)	
training:	Epoch: [46][134/204]	Loss 0.0056 (0.0350)	
training:	Epoch: [46][135/204]	Loss 0.0560 (0.0351)	
training:	Epoch: [46][136/204]	Loss 0.0045 (0.0349)	
training:	Epoch: [46][137/204]	Loss 0.0069 (0.0347)	
training:	Epoch: [46][138/204]	Loss 0.0443 (0.0348)	
training:	Epoch: [46][139/204]	Loss 0.0075 (0.0346)	
training:	Epoch: [46][140/204]	Loss 0.0065 (0.0344)	
training:	Epoch: [46][141/204]	Loss 0.1131 (0.0349)	
training:	Epoch: [46][142/204]	Loss 0.0056 (0.0347)	
training:	Epoch: [46][143/204]	Loss 0.0054 (0.0345)	
training:	Epoch: [46][144/204]	Loss 0.1719 (0.0355)	
training:	Epoch: [46][145/204]	Loss 0.0084 (0.0353)	
training:	Epoch: [46][146/204]	Loss 0.1233 (0.0359)	
training:	Epoch: [46][147/204]	Loss 0.0050 (0.0357)	
training:	Epoch: [46][148/204]	Loss 0.0048 (0.0355)	
training:	Epoch: [46][149/204]	Loss 0.0078 (0.0353)	
training:	Epoch: [46][150/204]	Loss 0.0454 (0.0354)	
training:	Epoch: [46][151/204]	Loss 0.0056 (0.0352)	
training:	Epoch: [46][152/204]	Loss 0.0085 (0.0350)	
training:	Epoch: [46][153/204]	Loss 0.0187 (0.0349)	
training:	Epoch: [46][154/204]	Loss 0.0060 (0.0347)	
training:	Epoch: [46][155/204]	Loss 0.0058 (0.0345)	
training:	Epoch: [46][156/204]	Loss 0.0080 (0.0343)	
training:	Epoch: [46][157/204]	Loss 0.0053 (0.0341)	
training:	Epoch: [46][158/204]	Loss 0.0052 (0.0340)	
training:	Epoch: [46][159/204]	Loss 0.0051 (0.0338)	
training:	Epoch: [46][160/204]	Loss 0.0061 (0.0336)	
training:	Epoch: [46][161/204]	Loss 0.0054 (0.0334)	
training:	Epoch: [46][162/204]	Loss 0.0052 (0.0333)	
training:	Epoch: [46][163/204]	Loss 0.0052 (0.0331)	
training:	Epoch: [46][164/204]	Loss 0.0053 (0.0329)	
training:	Epoch: [46][165/204]	Loss 0.0050 (0.0327)	
training:	Epoch: [46][166/204]	Loss 0.0052 (0.0326)	
training:	Epoch: [46][167/204]	Loss 0.0051 (0.0324)	
training:	Epoch: [46][168/204]	Loss 0.1916 (0.0334)	
training:	Epoch: [46][169/204]	Loss 0.0300 (0.0333)	
training:	Epoch: [46][170/204]	Loss 0.0051 (0.0332)	
training:	Epoch: [46][171/204]	Loss 0.1587 (0.0339)	
training:	Epoch: [46][172/204]	Loss 0.0071 (0.0338)	
training:	Epoch: [46][173/204]	Loss 0.0071 (0.0336)	
training:	Epoch: [46][174/204]	Loss 0.0060 (0.0334)	
training:	Epoch: [46][175/204]	Loss 0.0187 (0.0334)	
training:	Epoch: [46][176/204]	Loss 0.1835 (0.0342)	
training:	Epoch: [46][177/204]	Loss 0.0189 (0.0341)	
training:	Epoch: [46][178/204]	Loss 0.0073 (0.0340)	
training:	Epoch: [46][179/204]	Loss 0.0056 (0.0338)	
training:	Epoch: [46][180/204]	Loss 0.0090 (0.0337)	
training:	Epoch: [46][181/204]	Loss 0.0053 (0.0335)	
training:	Epoch: [46][182/204]	Loss 0.1801 (0.0343)	
training:	Epoch: [46][183/204]	Loss 0.0047 (0.0342)	
training:	Epoch: [46][184/204]	Loss 0.0197 (0.0341)	
training:	Epoch: [46][185/204]	Loss 0.0058 (0.0339)	
training:	Epoch: [46][186/204]	Loss 0.0047 (0.0338)	
training:	Epoch: [46][187/204]	Loss 0.0059 (0.0336)	
training:	Epoch: [46][188/204]	Loss 0.0090 (0.0335)	
training:	Epoch: [46][189/204]	Loss 0.0057 (0.0333)	
training:	Epoch: [46][190/204]	Loss 0.0051 (0.0332)	
training:	Epoch: [46][191/204]	Loss 0.0043 (0.0330)	
training:	Epoch: [46][192/204]	Loss 0.0073 (0.0329)	
training:	Epoch: [46][193/204]	Loss 0.0050 (0.0328)	
training:	Epoch: [46][194/204]	Loss 0.0052 (0.0326)	
training:	Epoch: [46][195/204]	Loss 0.0060 (0.0325)	
training:	Epoch: [46][196/204]	Loss 0.0064 (0.0324)	
training:	Epoch: [46][197/204]	Loss 0.0193 (0.0323)	
training:	Epoch: [46][198/204]	Loss 0.0142 (0.0322)	
training:	Epoch: [46][199/204]	Loss 0.1617 (0.0329)	
training:	Epoch: [46][200/204]	Loss 0.0290 (0.0328)	
training:	Epoch: [46][201/204]	Loss 0.0049 (0.0327)	
training:	Epoch: [46][202/204]	Loss 0.0061 (0.0326)	
training:	Epoch: [46][203/204]	Loss 0.0047 (0.0324)	
training:	Epoch: [46][204/204]	Loss 0.0052 (0.0323)	
Training:	 Loss: 0.0322

Training:	 ACC: 0.9972 0.9972 0.9985 0.9959
Validation:	 ACC: 0.7997 0.8015 0.8393 0.7601
Validation:	 Best_BACC: 0.8326 0.8331 0.8424 0.8229
Validation:	 Loss: 0.8264
Pretraining:	Epoch 47/200
----------
training:	Epoch: [47][1/204]	Loss 0.0075 (0.0075)	
training:	Epoch: [47][2/204]	Loss 0.0066 (0.0071)	
training:	Epoch: [47][3/204]	Loss 0.0057 (0.0066)	
training:	Epoch: [47][4/204]	Loss 0.1654 (0.0463)	
training:	Epoch: [47][5/204]	Loss 0.0042 (0.0379)	
training:	Epoch: [47][6/204]	Loss 0.0087 (0.0330)	
training:	Epoch: [47][7/204]	Loss 0.0056 (0.0291)	
training:	Epoch: [47][8/204]	Loss 0.0295 (0.0292)	
training:	Epoch: [47][9/204]	Loss 0.0061 (0.0266)	
training:	Epoch: [47][10/204]	Loss 0.0048 (0.0244)	
training:	Epoch: [47][11/204]	Loss 0.0080 (0.0229)	
training:	Epoch: [47][12/204]	Loss 0.0087 (0.0217)	
training:	Epoch: [47][13/204]	Loss 0.0297 (0.0224)	
training:	Epoch: [47][14/204]	Loss 0.0046 (0.0211)	
training:	Epoch: [47][15/204]	Loss 0.0118 (0.0205)	
training:	Epoch: [47][16/204]	Loss 0.0058 (0.0196)	
training:	Epoch: [47][17/204]	Loss 0.0049 (0.0187)	
training:	Epoch: [47][18/204]	Loss 0.0053 (0.0179)	
training:	Epoch: [47][19/204]	Loss 0.0071 (0.0174)	
training:	Epoch: [47][20/204]	Loss 0.0050 (0.0168)	
training:	Epoch: [47][21/204]	Loss 0.0057 (0.0162)	
training:	Epoch: [47][22/204]	Loss 0.0057 (0.0157)	
training:	Epoch: [47][23/204]	Loss 0.1755 (0.0227)	
training:	Epoch: [47][24/204]	Loss 0.1278 (0.0271)	
training:	Epoch: [47][25/204]	Loss 0.0071 (0.0263)	
training:	Epoch: [47][26/204]	Loss 0.0063 (0.0255)	
training:	Epoch: [47][27/204]	Loss 0.0061 (0.0248)	
training:	Epoch: [47][28/204]	Loss 0.0051 (0.0241)	
training:	Epoch: [47][29/204]	Loss 0.0058 (0.0235)	
training:	Epoch: [47][30/204]	Loss 0.0065 (0.0229)	
training:	Epoch: [47][31/204]	Loss 0.0043 (0.0223)	
training:	Epoch: [47][32/204]	Loss 0.1524 (0.0264)	
training:	Epoch: [47][33/204]	Loss 0.0691 (0.0277)	
training:	Epoch: [47][34/204]	Loss 0.0047 (0.0270)	
training:	Epoch: [47][35/204]	Loss 0.0048 (0.0263)	
training:	Epoch: [47][36/204]	Loss 0.0067 (0.0258)	
training:	Epoch: [47][37/204]	Loss 0.0055 (0.0252)	
training:	Epoch: [47][38/204]	Loss 0.0066 (0.0248)	
training:	Epoch: [47][39/204]	Loss 0.0073 (0.0243)	
training:	Epoch: [47][40/204]	Loss 0.0072 (0.0239)	
training:	Epoch: [47][41/204]	Loss 0.0073 (0.0235)	
training:	Epoch: [47][42/204]	Loss 0.0234 (0.0235)	
training:	Epoch: [47][43/204]	Loss 0.0061 (0.0231)	
training:	Epoch: [47][44/204]	Loss 0.0051 (0.0227)	
training:	Epoch: [47][45/204]	Loss 0.0089 (0.0224)	
training:	Epoch: [47][46/204]	Loss 0.0046 (0.0220)	
training:	Epoch: [47][47/204]	Loss 0.0219 (0.0220)	
training:	Epoch: [47][48/204]	Loss 0.0062 (0.0216)	
training:	Epoch: [47][49/204]	Loss 0.0121 (0.0214)	
training:	Epoch: [47][50/204]	Loss 0.0053 (0.0211)	
training:	Epoch: [47][51/204]	Loss 0.0267 (0.0212)	
training:	Epoch: [47][52/204]	Loss 0.0926 (0.0226)	
training:	Epoch: [47][53/204]	Loss 0.0047 (0.0223)	
training:	Epoch: [47][54/204]	Loss 0.0614 (0.0230)	
training:	Epoch: [47][55/204]	Loss 0.0063 (0.0227)	
training:	Epoch: [47][56/204]	Loss 0.0048 (0.0224)	
training:	Epoch: [47][57/204]	Loss 0.0053 (0.0221)	
training:	Epoch: [47][58/204]	Loss 0.0052 (0.0218)	
training:	Epoch: [47][59/204]	Loss 0.0104 (0.0216)	
training:	Epoch: [47][60/204]	Loss 0.0058 (0.0213)	
training:	Epoch: [47][61/204]	Loss 0.0043 (0.0210)	
training:	Epoch: [47][62/204]	Loss 0.0049 (0.0208)	
training:	Epoch: [47][63/204]	Loss 0.0055 (0.0205)	
training:	Epoch: [47][64/204]	Loss 0.0051 (0.0203)	
training:	Epoch: [47][65/204]	Loss 0.0074 (0.0201)	
training:	Epoch: [47][66/204]	Loss 0.0095 (0.0199)	
training:	Epoch: [47][67/204]	Loss 0.0046 (0.0197)	
training:	Epoch: [47][68/204]	Loss 0.0231 (0.0198)	
training:	Epoch: [47][69/204]	Loss 0.0060 (0.0196)	
training:	Epoch: [47][70/204]	Loss 0.0047 (0.0193)	
training:	Epoch: [47][71/204]	Loss 0.0050 (0.0191)	
training:	Epoch: [47][72/204]	Loss 0.0170 (0.0191)	
training:	Epoch: [47][73/204]	Loss 0.0048 (0.0189)	
training:	Epoch: [47][74/204]	Loss 0.0055 (0.0187)	
training:	Epoch: [47][75/204]	Loss 0.0338 (0.0189)	
training:	Epoch: [47][76/204]	Loss 0.0093 (0.0188)	
training:	Epoch: [47][77/204]	Loss 0.1704 (0.0208)	
training:	Epoch: [47][78/204]	Loss 0.0058 (0.0206)	
training:	Epoch: [47][79/204]	Loss 0.0057 (0.0204)	
training:	Epoch: [47][80/204]	Loss 0.0048 (0.0202)	
training:	Epoch: [47][81/204]	Loss 0.0079 (0.0201)	
training:	Epoch: [47][82/204]	Loss 0.0337 (0.0202)	
training:	Epoch: [47][83/204]	Loss 0.0314 (0.0204)	
training:	Epoch: [47][84/204]	Loss 0.0049 (0.0202)	
training:	Epoch: [47][85/204]	Loss 0.0059 (0.0200)	
training:	Epoch: [47][86/204]	Loss 0.0063 (0.0198)	
training:	Epoch: [47][87/204]	Loss 0.0043 (0.0197)	
training:	Epoch: [47][88/204]	Loss 0.0046 (0.0195)	
training:	Epoch: [47][89/204]	Loss 0.0097 (0.0194)	
training:	Epoch: [47][90/204]	Loss 0.0042 (0.0192)	
training:	Epoch: [47][91/204]	Loss 0.0118 (0.0191)	
training:	Epoch: [47][92/204]	Loss 0.0051 (0.0190)	
training:	Epoch: [47][93/204]	Loss 0.0083 (0.0189)	
training:	Epoch: [47][94/204]	Loss 0.0052 (0.0187)	
training:	Epoch: [47][95/204]	Loss 0.0060 (0.0186)	
training:	Epoch: [47][96/204]	Loss 0.0059 (0.0185)	
training:	Epoch: [47][97/204]	Loss 0.0095 (0.0184)	
training:	Epoch: [47][98/204]	Loss 0.0102 (0.0183)	
training:	Epoch: [47][99/204]	Loss 0.0045 (0.0181)	
training:	Epoch: [47][100/204]	Loss 0.0043 (0.0180)	
training:	Epoch: [47][101/204]	Loss 0.0060 (0.0179)	
training:	Epoch: [47][102/204]	Loss 0.0088 (0.0178)	
training:	Epoch: [47][103/204]	Loss 0.0040 (0.0177)	
training:	Epoch: [47][104/204]	Loss 0.0057 (0.0175)	
training:	Epoch: [47][105/204]	Loss 0.1831 (0.0191)	
training:	Epoch: [47][106/204]	Loss 0.0065 (0.0190)	
training:	Epoch: [47][107/204]	Loss 0.0043 (0.0189)	
training:	Epoch: [47][108/204]	Loss 0.0353 (0.0190)	
training:	Epoch: [47][109/204]	Loss 0.0047 (0.0189)	
training:	Epoch: [47][110/204]	Loss 0.0079 (0.0188)	
training:	Epoch: [47][111/204]	Loss 0.0048 (0.0187)	
training:	Epoch: [47][112/204]	Loss 0.0061 (0.0185)	
training:	Epoch: [47][113/204]	Loss 0.0320 (0.0187)	
training:	Epoch: [47][114/204]	Loss 0.0044 (0.0185)	
training:	Epoch: [47][115/204]	Loss 0.1618 (0.0198)	
training:	Epoch: [47][116/204]	Loss 0.0091 (0.0197)	
training:	Epoch: [47][117/204]	Loss 0.0148 (0.0197)	
training:	Epoch: [47][118/204]	Loss 0.0040 (0.0195)	
training:	Epoch: [47][119/204]	Loss 0.0044 (0.0194)	
training:	Epoch: [47][120/204]	Loss 0.0364 (0.0195)	
training:	Epoch: [47][121/204]	Loss 0.1706 (0.0208)	
training:	Epoch: [47][122/204]	Loss 0.0045 (0.0206)	
training:	Epoch: [47][123/204]	Loss 0.0048 (0.0205)	
training:	Epoch: [47][124/204]	Loss 0.0052 (0.0204)	
training:	Epoch: [47][125/204]	Loss 0.0045 (0.0203)	
training:	Epoch: [47][126/204]	Loss 0.0097 (0.0202)	
training:	Epoch: [47][127/204]	Loss 0.0076 (0.0201)	
training:	Epoch: [47][128/204]	Loss 0.2179 (0.0216)	
training:	Epoch: [47][129/204]	Loss 0.1825 (0.0229)	
training:	Epoch: [47][130/204]	Loss 0.0093 (0.0228)	
training:	Epoch: [47][131/204]	Loss 0.0164 (0.0227)	
training:	Epoch: [47][132/204]	Loss 0.0040 (0.0226)	
training:	Epoch: [47][133/204]	Loss 0.0050 (0.0225)	
training:	Epoch: [47][134/204]	Loss 0.1151 (0.0231)	
training:	Epoch: [47][135/204]	Loss 0.0421 (0.0233)	
training:	Epoch: [47][136/204]	Loss 0.0058 (0.0232)	
training:	Epoch: [47][137/204]	Loss 0.0044 (0.0230)	
training:	Epoch: [47][138/204]	Loss 0.0134 (0.0229)	
training:	Epoch: [47][139/204]	Loss 0.0063 (0.0228)	
training:	Epoch: [47][140/204]	Loss 0.0046 (0.0227)	
training:	Epoch: [47][141/204]	Loss 0.0238 (0.0227)	
training:	Epoch: [47][142/204]	Loss 0.0244 (0.0227)	
training:	Epoch: [47][143/204]	Loss 0.0044 (0.0226)	
training:	Epoch: [47][144/204]	Loss 0.0520 (0.0228)	
training:	Epoch: [47][145/204]	Loss 0.0063 (0.0227)	
training:	Epoch: [47][146/204]	Loss 0.0063 (0.0226)	
training:	Epoch: [47][147/204]	Loss 0.1673 (0.0236)	
training:	Epoch: [47][148/204]	Loss 0.0131 (0.0235)	
training:	Epoch: [47][149/204]	Loss 0.0067 (0.0234)	
training:	Epoch: [47][150/204]	Loss 0.0068 (0.0233)	
training:	Epoch: [47][151/204]	Loss 0.0053 (0.0231)	
training:	Epoch: [47][152/204]	Loss 0.0073 (0.0230)	
training:	Epoch: [47][153/204]	Loss 0.0057 (0.0229)	
training:	Epoch: [47][154/204]	Loss 0.0045 (0.0228)	
training:	Epoch: [47][155/204]	Loss 0.0085 (0.0227)	
training:	Epoch: [47][156/204]	Loss 0.0223 (0.0227)	
training:	Epoch: [47][157/204]	Loss 0.0052 (0.0226)	
training:	Epoch: [47][158/204]	Loss 0.0054 (0.0225)	
training:	Epoch: [47][159/204]	Loss 0.0050 (0.0224)	
training:	Epoch: [47][160/204]	Loss 0.0046 (0.0223)	
training:	Epoch: [47][161/204]	Loss 0.0044 (0.0222)	
training:	Epoch: [47][162/204]	Loss 0.0064 (0.0221)	
training:	Epoch: [47][163/204]	Loss 0.0057 (0.0220)	
training:	Epoch: [47][164/204]	Loss 0.0067 (0.0219)	
training:	Epoch: [47][165/204]	Loss 0.0060 (0.0218)	
training:	Epoch: [47][166/204]	Loss 0.0050 (0.0217)	
training:	Epoch: [47][167/204]	Loss 0.0111 (0.0216)	
training:	Epoch: [47][168/204]	Loss 0.0068 (0.0215)	
training:	Epoch: [47][169/204]	Loss 0.0047 (0.0214)	
training:	Epoch: [47][170/204]	Loss 0.1501 (0.0222)	
training:	Epoch: [47][171/204]	Loss 0.0041 (0.0221)	
training:	Epoch: [47][172/204]	Loss 0.0047 (0.0220)	
training:	Epoch: [47][173/204]	Loss 0.0040 (0.0219)	
training:	Epoch: [47][174/204]	Loss 0.0052 (0.0218)	
training:	Epoch: [47][175/204]	Loss 0.0050 (0.0217)	
training:	Epoch: [47][176/204]	Loss 0.0057 (0.0216)	
training:	Epoch: [47][177/204]	Loss 0.2003 (0.0226)	
training:	Epoch: [47][178/204]	Loss 0.0039 (0.0225)	
training:	Epoch: [47][179/204]	Loss 0.0070 (0.0224)	
training:	Epoch: [47][180/204]	Loss 0.0039 (0.0223)	
training:	Epoch: [47][181/204]	Loss 0.0065 (0.0222)	
training:	Epoch: [47][182/204]	Loss 0.0080 (0.0221)	
training:	Epoch: [47][183/204]	Loss 0.0053 (0.0220)	
training:	Epoch: [47][184/204]	Loss 0.3073 (0.0236)	
training:	Epoch: [47][185/204]	Loss 0.1839 (0.0245)	
training:	Epoch: [47][186/204]	Loss 0.0055 (0.0244)	
training:	Epoch: [47][187/204]	Loss 0.0042 (0.0242)	
training:	Epoch: [47][188/204]	Loss 0.1796 (0.0251)	
training:	Epoch: [47][189/204]	Loss 0.0062 (0.0250)	
training:	Epoch: [47][190/204]	Loss 0.0067 (0.0249)	
training:	Epoch: [47][191/204]	Loss 0.0066 (0.0248)	
training:	Epoch: [47][192/204]	Loss 0.0054 (0.0247)	
training:	Epoch: [47][193/204]	Loss 0.0066 (0.0246)	
training:	Epoch: [47][194/204]	Loss 0.0075 (0.0245)	
training:	Epoch: [47][195/204]	Loss 0.0053 (0.0244)	
training:	Epoch: [47][196/204]	Loss 0.0055 (0.0243)	
training:	Epoch: [47][197/204]	Loss 0.0048 (0.0242)	
training:	Epoch: [47][198/204]	Loss 0.0046 (0.0241)	
training:	Epoch: [47][199/204]	Loss 0.1600 (0.0248)	
training:	Epoch: [47][200/204]	Loss 0.0052 (0.0247)	
training:	Epoch: [47][201/204]	Loss 0.1234 (0.0252)	
training:	Epoch: [47][202/204]	Loss 0.0047 (0.0251)	
training:	Epoch: [47][203/204]	Loss 0.0053 (0.0250)	
training:	Epoch: [47][204/204]	Loss 0.0044 (0.0249)	
Training:	 Loss: 0.0248

Training:	 ACC: 0.9972 0.9972 0.9985 0.9959
Validation:	 ACC: 0.7978 0.7988 0.8199 0.7758
Validation:	 Best_BACC: 0.8326 0.8331 0.8424 0.8229
Validation:	 Loss: 0.8545
Pretraining:	Epoch 48/200
----------
training:	Epoch: [48][1/204]	Loss 0.0047 (0.0047)	
training:	Epoch: [48][2/204]	Loss 0.0050 (0.0048)	
training:	Epoch: [48][3/204]	Loss 0.0059 (0.0052)	
training:	Epoch: [48][4/204]	Loss 0.0051 (0.0052)	
training:	Epoch: [48][5/204]	Loss 0.0042 (0.0050)	
training:	Epoch: [48][6/204]	Loss 0.1207 (0.0243)	
training:	Epoch: [48][7/204]	Loss 0.0068 (0.0218)	
training:	Epoch: [48][8/204]	Loss 0.0060 (0.0198)	
training:	Epoch: [48][9/204]	Loss 0.0060 (0.0183)	
training:	Epoch: [48][10/204]	Loss 0.0287 (0.0193)	
training:	Epoch: [48][11/204]	Loss 0.0040 (0.0179)	
training:	Epoch: [48][12/204]	Loss 0.0041 (0.0168)	
training:	Epoch: [48][13/204]	Loss 0.1549 (0.0274)	
training:	Epoch: [48][14/204]	Loss 0.0228 (0.0271)	
training:	Epoch: [48][15/204]	Loss 0.0433 (0.0282)	
training:	Epoch: [48][16/204]	Loss 0.0061 (0.0268)	
training:	Epoch: [48][17/204]	Loss 0.0904 (0.0305)	
training:	Epoch: [48][18/204]	Loss 0.0158 (0.0297)	
training:	Epoch: [48][19/204]	Loss 0.0146 (0.0289)	
training:	Epoch: [48][20/204]	Loss 0.0060 (0.0278)	
training:	Epoch: [48][21/204]	Loss 0.0063 (0.0267)	
training:	Epoch: [48][22/204]	Loss 0.0045 (0.0257)	
training:	Epoch: [48][23/204]	Loss 0.0049 (0.0248)	
training:	Epoch: [48][24/204]	Loss 0.0042 (0.0240)	
training:	Epoch: [48][25/204]	Loss 0.0056 (0.0232)	
training:	Epoch: [48][26/204]	Loss 0.0045 (0.0225)	
training:	Epoch: [48][27/204]	Loss 0.0752 (0.0245)	
training:	Epoch: [48][28/204]	Loss 0.0048 (0.0238)	
training:	Epoch: [48][29/204]	Loss 0.0070 (0.0232)	
training:	Epoch: [48][30/204]	Loss 0.0047 (0.0226)	
training:	Epoch: [48][31/204]	Loss 0.0159 (0.0223)	
training:	Epoch: [48][32/204]	Loss 0.0053 (0.0218)	
training:	Epoch: [48][33/204]	Loss 0.0057 (0.0213)	
training:	Epoch: [48][34/204]	Loss 0.1851 (0.0261)	
training:	Epoch: [48][35/204]	Loss 0.0046 (0.0255)	
training:	Epoch: [48][36/204]	Loss 0.1847 (0.0299)	
training:	Epoch: [48][37/204]	Loss 0.0044 (0.0293)	
training:	Epoch: [48][38/204]	Loss 0.0061 (0.0286)	
training:	Epoch: [48][39/204]	Loss 0.0063 (0.0281)	
training:	Epoch: [48][40/204]	Loss 0.0054 (0.0275)	
training:	Epoch: [48][41/204]	Loss 0.0068 (0.0270)	
training:	Epoch: [48][42/204]	Loss 0.0072 (0.0265)	
training:	Epoch: [48][43/204]	Loss 0.1191 (0.0287)	
training:	Epoch: [48][44/204]	Loss 0.0044 (0.0281)	
training:	Epoch: [48][45/204]	Loss 0.0081 (0.0277)	
training:	Epoch: [48][46/204]	Loss 0.0290 (0.0277)	
training:	Epoch: [48][47/204]	Loss 0.0083 (0.0273)	
training:	Epoch: [48][48/204]	Loss 0.1847 (0.0306)	
training:	Epoch: [48][49/204]	Loss 0.0047 (0.0301)	
training:	Epoch: [48][50/204]	Loss 0.1765 (0.0330)	
training:	Epoch: [48][51/204]	Loss 0.0083 (0.0325)	
training:	Epoch: [48][52/204]	Loss 0.0150 (0.0322)	
training:	Epoch: [48][53/204]	Loss 0.0060 (0.0317)	
training:	Epoch: [48][54/204]	Loss 0.0043 (0.0312)	
training:	Epoch: [48][55/204]	Loss 0.1504 (0.0333)	
training:	Epoch: [48][56/204]	Loss 0.0048 (0.0328)	
training:	Epoch: [48][57/204]	Loss 0.0043 (0.0323)	
training:	Epoch: [48][58/204]	Loss 0.1834 (0.0349)	
training:	Epoch: [48][59/204]	Loss 0.0067 (0.0344)	
training:	Epoch: [48][60/204]	Loss 0.0035 (0.0339)	
training:	Epoch: [48][61/204]	Loss 0.0046 (0.0334)	
training:	Epoch: [48][62/204]	Loss 0.0047 (0.0330)	
training:	Epoch: [48][63/204]	Loss 0.0050 (0.0325)	
training:	Epoch: [48][64/204]	Loss 0.1592 (0.0345)	
training:	Epoch: [48][65/204]	Loss 0.0051 (0.0341)	
training:	Epoch: [48][66/204]	Loss 0.0047 (0.0336)	
training:	Epoch: [48][67/204]	Loss 0.0176 (0.0334)	
training:	Epoch: [48][68/204]	Loss 0.0047 (0.0330)	
training:	Epoch: [48][69/204]	Loss 0.0165 (0.0327)	
training:	Epoch: [48][70/204]	Loss 0.0992 (0.0337)	
training:	Epoch: [48][71/204]	Loss 0.3208 (0.0377)	
training:	Epoch: [48][72/204]	Loss 0.0055 (0.0373)	
training:	Epoch: [48][73/204]	Loss 0.0048 (0.0368)	
training:	Epoch: [48][74/204]	Loss 0.0044 (0.0364)	
training:	Epoch: [48][75/204]	Loss 0.0059 (0.0360)	
training:	Epoch: [48][76/204]	Loss 0.1848 (0.0379)	
training:	Epoch: [48][77/204]	Loss 0.0739 (0.0384)	
training:	Epoch: [48][78/204]	Loss 0.0048 (0.0380)	
training:	Epoch: [48][79/204]	Loss 0.0060 (0.0376)	
training:	Epoch: [48][80/204]	Loss 0.0052 (0.0372)	
training:	Epoch: [48][81/204]	Loss 0.0053 (0.0368)	
training:	Epoch: [48][82/204]	Loss 0.0304 (0.0367)	
training:	Epoch: [48][83/204]	Loss 0.0051 (0.0363)	
training:	Epoch: [48][84/204]	Loss 0.0051 (0.0359)	
training:	Epoch: [48][85/204]	Loss 0.1220 (0.0370)	
training:	Epoch: [48][86/204]	Loss 0.0052 (0.0366)	
training:	Epoch: [48][87/204]	Loss 0.2005 (0.0385)	
training:	Epoch: [48][88/204]	Loss 0.0052 (0.0381)	
training:	Epoch: [48][89/204]	Loss 0.0060 (0.0377)	
training:	Epoch: [48][90/204]	Loss 0.0054 (0.0374)	
training:	Epoch: [48][91/204]	Loss 0.0059 (0.0370)	
training:	Epoch: [48][92/204]	Loss 0.0114 (0.0367)	
training:	Epoch: [48][93/204]	Loss 0.0234 (0.0366)	
training:	Epoch: [48][94/204]	Loss 0.0044 (0.0363)	
training:	Epoch: [48][95/204]	Loss 0.0054 (0.0359)	
training:	Epoch: [48][96/204]	Loss 0.1365 (0.0370)	
training:	Epoch: [48][97/204]	Loss 0.0047 (0.0366)	
training:	Epoch: [48][98/204]	Loss 0.0044 (0.0363)	
training:	Epoch: [48][99/204]	Loss 0.0051 (0.0360)	
training:	Epoch: [48][100/204]	Loss 0.1626 (0.0373)	
training:	Epoch: [48][101/204]	Loss 0.1477 (0.0384)	
training:	Epoch: [48][102/204]	Loss 0.0051 (0.0380)	
training:	Epoch: [48][103/204]	Loss 0.0050 (0.0377)	
training:	Epoch: [48][104/204]	Loss 0.0106 (0.0375)	
training:	Epoch: [48][105/204]	Loss 0.1573 (0.0386)	
training:	Epoch: [48][106/204]	Loss 0.0051 (0.0383)	
training:	Epoch: [48][107/204]	Loss 0.0064 (0.0380)	
training:	Epoch: [48][108/204]	Loss 0.0563 (0.0382)	
training:	Epoch: [48][109/204]	Loss 0.0294 (0.0381)	
training:	Epoch: [48][110/204]	Loss 0.0050 (0.0378)	
training:	Epoch: [48][111/204]	Loss 0.0062 (0.0375)	
training:	Epoch: [48][112/204]	Loss 0.0175 (0.0373)	
training:	Epoch: [48][113/204]	Loss 0.0050 (0.0370)	
training:	Epoch: [48][114/204]	Loss 0.0054 (0.0367)	
training:	Epoch: [48][115/204]	Loss 0.0043 (0.0365)	
training:	Epoch: [48][116/204]	Loss 0.0051 (0.0362)	
training:	Epoch: [48][117/204]	Loss 0.0050 (0.0359)	
training:	Epoch: [48][118/204]	Loss 0.0049 (0.0357)	
training:	Epoch: [48][119/204]	Loss 0.0046 (0.0354)	
training:	Epoch: [48][120/204]	Loss 0.0049 (0.0351)	
training:	Epoch: [48][121/204]	Loss 0.0046 (0.0349)	
training:	Epoch: [48][122/204]	Loss 0.0050 (0.0347)	
training:	Epoch: [48][123/204]	Loss 0.1165 (0.0353)	
training:	Epoch: [48][124/204]	Loss 0.0316 (0.0353)	
training:	Epoch: [48][125/204]	Loss 0.0051 (0.0350)	
training:	Epoch: [48][126/204]	Loss 0.0762 (0.0354)	
training:	Epoch: [48][127/204]	Loss 0.0053 (0.0351)	
training:	Epoch: [48][128/204]	Loss 0.0057 (0.0349)	
training:	Epoch: [48][129/204]	Loss 0.0064 (0.0347)	
training:	Epoch: [48][130/204]	Loss 0.0177 (0.0346)	
training:	Epoch: [48][131/204]	Loss 0.0062 (0.0343)	
training:	Epoch: [48][132/204]	Loss 0.0044 (0.0341)	
training:	Epoch: [48][133/204]	Loss 0.0049 (0.0339)	
training:	Epoch: [48][134/204]	Loss 0.0056 (0.0337)	
training:	Epoch: [48][135/204]	Loss 0.0054 (0.0335)	
training:	Epoch: [48][136/204]	Loss 0.1009 (0.0340)	
training:	Epoch: [48][137/204]	Loss 0.0059 (0.0338)	
training:	Epoch: [48][138/204]	Loss 0.0040 (0.0335)	
training:	Epoch: [48][139/204]	Loss 0.0059 (0.0333)	
training:	Epoch: [48][140/204]	Loss 0.0068 (0.0332)	
training:	Epoch: [48][141/204]	Loss 0.0056 (0.0330)	
training:	Epoch: [48][142/204]	Loss 0.0060 (0.0328)	
training:	Epoch: [48][143/204]	Loss 0.0051 (0.0326)	
training:	Epoch: [48][144/204]	Loss 0.0064 (0.0324)	
training:	Epoch: [48][145/204]	Loss 0.0053 (0.0322)	
training:	Epoch: [48][146/204]	Loss 0.0057 (0.0320)	
training:	Epoch: [48][147/204]	Loss 0.0047 (0.0318)	
training:	Epoch: [48][148/204]	Loss 0.0059 (0.0317)	
training:	Epoch: [48][149/204]	Loss 0.0056 (0.0315)	
training:	Epoch: [48][150/204]	Loss 0.0061 (0.0313)	
training:	Epoch: [48][151/204]	Loss 0.0758 (0.0316)	
training:	Epoch: [48][152/204]	Loss 0.1798 (0.0326)	
training:	Epoch: [48][153/204]	Loss 0.0080 (0.0324)	
training:	Epoch: [48][154/204]	Loss 0.0049 (0.0323)	
training:	Epoch: [48][155/204]	Loss 0.0050 (0.0321)	
training:	Epoch: [48][156/204]	Loss 0.0059 (0.0319)	
training:	Epoch: [48][157/204]	Loss 0.2054 (0.0330)	
training:	Epoch: [48][158/204]	Loss 0.0045 (0.0328)	
training:	Epoch: [48][159/204]	Loss 0.0041 (0.0327)	
training:	Epoch: [48][160/204]	Loss 0.0071 (0.0325)	
training:	Epoch: [48][161/204]	Loss 0.0074 (0.0323)	
training:	Epoch: [48][162/204]	Loss 0.0062 (0.0322)	
training:	Epoch: [48][163/204]	Loss 0.0335 (0.0322)	
training:	Epoch: [48][164/204]	Loss 0.0045 (0.0320)	
training:	Epoch: [48][165/204]	Loss 0.1689 (0.0328)	
training:	Epoch: [48][166/204]	Loss 0.0168 (0.0327)	
training:	Epoch: [48][167/204]	Loss 0.0055 (0.0326)	
training:	Epoch: [48][168/204]	Loss 0.0068 (0.0324)	
training:	Epoch: [48][169/204]	Loss 0.0653 (0.0326)	
training:	Epoch: [48][170/204]	Loss 0.0067 (0.0325)	
training:	Epoch: [48][171/204]	Loss 0.0057 (0.0323)	
training:	Epoch: [48][172/204]	Loss 0.0199 (0.0322)	
training:	Epoch: [48][173/204]	Loss 0.0066 (0.0321)	
training:	Epoch: [48][174/204]	Loss 0.0050 (0.0319)	
training:	Epoch: [48][175/204]	Loss 0.0089 (0.0318)	
training:	Epoch: [48][176/204]	Loss 0.0056 (0.0317)	
training:	Epoch: [48][177/204]	Loss 0.0049 (0.0315)	
training:	Epoch: [48][178/204]	Loss 0.0085 (0.0314)	
training:	Epoch: [48][179/204]	Loss 0.0059 (0.0312)	
training:	Epoch: [48][180/204]	Loss 0.0067 (0.0311)	
training:	Epoch: [48][181/204]	Loss 0.0048 (0.0310)	
training:	Epoch: [48][182/204]	Loss 0.0057 (0.0308)	
training:	Epoch: [48][183/204]	Loss 0.0049 (0.0307)	
training:	Epoch: [48][184/204]	Loss 0.0042 (0.0305)	
training:	Epoch: [48][185/204]	Loss 0.0044 (0.0304)	
training:	Epoch: [48][186/204]	Loss 0.0126 (0.0303)	
training:	Epoch: [48][187/204]	Loss 0.0156 (0.0302)	
training:	Epoch: [48][188/204]	Loss 0.0167 (0.0301)	
training:	Epoch: [48][189/204]	Loss 0.0203 (0.0301)	
training:	Epoch: [48][190/204]	Loss 0.0052 (0.0300)	
training:	Epoch: [48][191/204]	Loss 0.0082 (0.0298)	
training:	Epoch: [48][192/204]	Loss 0.0053 (0.0297)	
training:	Epoch: [48][193/204]	Loss 0.0054 (0.0296)	
training:	Epoch: [48][194/204]	Loss 0.0055 (0.0295)	
training:	Epoch: [48][195/204]	Loss 0.0082 (0.0294)	
training:	Epoch: [48][196/204]	Loss 0.0060 (0.0292)	
training:	Epoch: [48][197/204]	Loss 0.0049 (0.0291)	
training:	Epoch: [48][198/204]	Loss 0.0045 (0.0290)	
training:	Epoch: [48][199/204]	Loss 0.0170 (0.0289)	
training:	Epoch: [48][200/204]	Loss 0.0088 (0.0288)	
training:	Epoch: [48][201/204]	Loss 0.0331 (0.0289)	
training:	Epoch: [48][202/204]	Loss 0.0892 (0.0292)	
training:	Epoch: [48][203/204]	Loss 0.0061 (0.0290)	
training:	Epoch: [48][204/204]	Loss 0.0061 (0.0289)	
Training:	 Loss: 0.0289

Training:	 ACC: 0.9946 0.9948 0.9985 0.9908
Validation:	 ACC: 0.7980 0.8010 0.8639 0.7321
Validation:	 Best_BACC: 0.8326 0.8331 0.8424 0.8229
Validation:	 Loss: 0.8763
Pretraining:	Epoch 49/200
----------
training:	Epoch: [49][1/204]	Loss 0.1900 (0.1900)	
training:	Epoch: [49][2/204]	Loss 0.0049 (0.0974)	
training:	Epoch: [49][3/204]	Loss 0.0100 (0.0683)	
training:	Epoch: [49][4/204]	Loss 0.0239 (0.0572)	
training:	Epoch: [49][5/204]	Loss 0.0055 (0.0469)	
training:	Epoch: [49][6/204]	Loss 0.0046 (0.0398)	
training:	Epoch: [49][7/204]	Loss 0.0051 (0.0349)	
training:	Epoch: [49][8/204]	Loss 0.0053 (0.0312)	
training:	Epoch: [49][9/204]	Loss 0.0944 (0.0382)	
training:	Epoch: [49][10/204]	Loss 0.0072 (0.0351)	
training:	Epoch: [49][11/204]	Loss 0.0067 (0.0325)	
training:	Epoch: [49][12/204]	Loss 0.0072 (0.0304)	
training:	Epoch: [49][13/204]	Loss 0.0049 (0.0284)	
training:	Epoch: [49][14/204]	Loss 0.0067 (0.0269)	
training:	Epoch: [49][15/204]	Loss 0.1867 (0.0375)	
training:	Epoch: [49][16/204]	Loss 0.0055 (0.0355)	
training:	Epoch: [49][17/204]	Loss 0.0165 (0.0344)	
training:	Epoch: [49][18/204]	Loss 0.0042 (0.0327)	
training:	Epoch: [49][19/204]	Loss 0.0694 (0.0347)	
training:	Epoch: [49][20/204]	Loss 0.0441 (0.0351)	
training:	Epoch: [49][21/204]	Loss 0.0079 (0.0339)	
training:	Epoch: [49][22/204]	Loss 0.0087 (0.0327)	
training:	Epoch: [49][23/204]	Loss 0.0043 (0.0315)	
training:	Epoch: [49][24/204]	Loss 0.0057 (0.0304)	
training:	Epoch: [49][25/204]	Loss 0.0173 (0.0299)	
training:	Epoch: [49][26/204]	Loss 0.0073 (0.0290)	
training:	Epoch: [49][27/204]	Loss 0.1599 (0.0339)	
training:	Epoch: [49][28/204]	Loss 0.0130 (0.0331)	
training:	Epoch: [49][29/204]	Loss 0.0049 (0.0321)	
training:	Epoch: [49][30/204]	Loss 0.0055 (0.0312)	
training:	Epoch: [49][31/204]	Loss 0.0080 (0.0305)	
training:	Epoch: [49][32/204]	Loss 0.0844 (0.0322)	
training:	Epoch: [49][33/204]	Loss 0.0057 (0.0314)	
training:	Epoch: [49][34/204]	Loss 0.0938 (0.0332)	
training:	Epoch: [49][35/204]	Loss 0.0069 (0.0325)	
training:	Epoch: [49][36/204]	Loss 0.0044 (0.0317)	
training:	Epoch: [49][37/204]	Loss 0.0041 (0.0309)	
training:	Epoch: [49][38/204]	Loss 0.0044 (0.0302)	
training:	Epoch: [49][39/204]	Loss 0.0044 (0.0296)	
training:	Epoch: [49][40/204]	Loss 0.0041 (0.0289)	
training:	Epoch: [49][41/204]	Loss 0.0048 (0.0283)	
training:	Epoch: [49][42/204]	Loss 0.0044 (0.0278)	
training:	Epoch: [49][43/204]	Loss 0.0047 (0.0272)	
training:	Epoch: [49][44/204]	Loss 0.0082 (0.0268)	
training:	Epoch: [49][45/204]	Loss 0.0045 (0.0263)	
training:	Epoch: [49][46/204]	Loss 0.0056 (0.0259)	
training:	Epoch: [49][47/204]	Loss 0.0232 (0.0258)	
training:	Epoch: [49][48/204]	Loss 0.0908 (0.0272)	
training:	Epoch: [49][49/204]	Loss 0.0072 (0.0268)	
training:	Epoch: [49][50/204]	Loss 0.0045 (0.0263)	
training:	Epoch: [49][51/204]	Loss 0.0044 (0.0259)	
training:	Epoch: [49][52/204]	Loss 0.1554 (0.0284)	
training:	Epoch: [49][53/204]	Loss 0.0044 (0.0279)	
training:	Epoch: [49][54/204]	Loss 0.0043 (0.0275)	
training:	Epoch: [49][55/204]	Loss 0.1678 (0.0300)	
training:	Epoch: [49][56/204]	Loss 0.0057 (0.0296)	
training:	Epoch: [49][57/204]	Loss 0.0048 (0.0292)	
training:	Epoch: [49][58/204]	Loss 0.0104 (0.0288)	
training:	Epoch: [49][59/204]	Loss 0.0048 (0.0284)	
training:	Epoch: [49][60/204]	Loss 0.0045 (0.0280)	
training:	Epoch: [49][61/204]	Loss 0.0058 (0.0277)	
training:	Epoch: [49][62/204]	Loss 0.0044 (0.0273)	
training:	Epoch: [49][63/204]	Loss 0.0045 (0.0269)	
training:	Epoch: [49][64/204]	Loss 0.0041 (0.0266)	
training:	Epoch: [49][65/204]	Loss 0.0158 (0.0264)	
training:	Epoch: [49][66/204]	Loss 0.0051 (0.0261)	
training:	Epoch: [49][67/204]	Loss 0.0050 (0.0258)	
training:	Epoch: [49][68/204]	Loss 0.0230 (0.0257)	
training:	Epoch: [49][69/204]	Loss 0.0129 (0.0255)	
training:	Epoch: [49][70/204]	Loss 0.0056 (0.0253)	
training:	Epoch: [49][71/204]	Loss 0.0045 (0.0250)	
training:	Epoch: [49][72/204]	Loss 0.0850 (0.0258)	
training:	Epoch: [49][73/204]	Loss 0.0073 (0.0255)	
training:	Epoch: [49][74/204]	Loss 0.0044 (0.0253)	
training:	Epoch: [49][75/204]	Loss 0.0047 (0.0250)	
training:	Epoch: [49][76/204]	Loss 0.0045 (0.0247)	
training:	Epoch: [49][77/204]	Loss 0.0076 (0.0245)	
training:	Epoch: [49][78/204]	Loss 0.0042 (0.0242)	
training:	Epoch: [49][79/204]	Loss 0.0048 (0.0240)	
training:	Epoch: [49][80/204]	Loss 0.0124 (0.0238)	
training:	Epoch: [49][81/204]	Loss 0.0059 (0.0236)	
training:	Epoch: [49][82/204]	Loss 0.0280 (0.0237)	
training:	Epoch: [49][83/204]	Loss 0.0043 (0.0234)	
training:	Epoch: [49][84/204]	Loss 0.0088 (0.0233)	
training:	Epoch: [49][85/204]	Loss 0.0066 (0.0231)	
training:	Epoch: [49][86/204]	Loss 0.0048 (0.0229)	
training:	Epoch: [49][87/204]	Loss 0.0049 (0.0227)	
training:	Epoch: [49][88/204]	Loss 0.0060 (0.0225)	
training:	Epoch: [49][89/204]	Loss 0.0040 (0.0223)	
training:	Epoch: [49][90/204]	Loss 0.0073 (0.0221)	
training:	Epoch: [49][91/204]	Loss 0.0070 (0.0219)	
training:	Epoch: [49][92/204]	Loss 0.1648 (0.0235)	
training:	Epoch: [49][93/204]	Loss 0.0040 (0.0233)	
training:	Epoch: [49][94/204]	Loss 0.1724 (0.0249)	
training:	Epoch: [49][95/204]	Loss 0.0054 (0.0246)	
training:	Epoch: [49][96/204]	Loss 0.0535 (0.0249)	
training:	Epoch: [49][97/204]	Loss 0.0111 (0.0248)	
training:	Epoch: [49][98/204]	Loss 0.0076 (0.0246)	
training:	Epoch: [49][99/204]	Loss 0.0042 (0.0244)	
training:	Epoch: [49][100/204]	Loss 0.0108 (0.0243)	
training:	Epoch: [49][101/204]	Loss 0.0061 (0.0241)	
training:	Epoch: [49][102/204]	Loss 0.0054 (0.0239)	
training:	Epoch: [49][103/204]	Loss 0.0083 (0.0238)	
training:	Epoch: [49][104/204]	Loss 0.0048 (0.0236)	
training:	Epoch: [49][105/204]	Loss 0.0087 (0.0234)	
training:	Epoch: [49][106/204]	Loss 0.0045 (0.0233)	
training:	Epoch: [49][107/204]	Loss 0.0059 (0.0231)	
training:	Epoch: [49][108/204]	Loss 0.0045 (0.0229)	
training:	Epoch: [49][109/204]	Loss 0.0046 (0.0228)	
training:	Epoch: [49][110/204]	Loss 0.0047 (0.0226)	
training:	Epoch: [49][111/204]	Loss 0.0074 (0.0225)	
training:	Epoch: [49][112/204]	Loss 0.0043 (0.0223)	
training:	Epoch: [49][113/204]	Loss 0.0072 (0.0222)	
training:	Epoch: [49][114/204]	Loss 0.0042 (0.0220)	
training:	Epoch: [49][115/204]	Loss 0.1868 (0.0234)	
training:	Epoch: [49][116/204]	Loss 0.0486 (0.0237)	
training:	Epoch: [49][117/204]	Loss 0.0040 (0.0235)	
training:	Epoch: [49][118/204]	Loss 0.0043 (0.0233)	
training:	Epoch: [49][119/204]	Loss 0.1018 (0.0240)	
training:	Epoch: [49][120/204]	Loss 0.0045 (0.0238)	
training:	Epoch: [49][121/204]	Loss 0.0056 (0.0237)	
training:	Epoch: [49][122/204]	Loss 0.0057 (0.0235)	
training:	Epoch: [49][123/204]	Loss 0.2079 (0.0250)	
training:	Epoch: [49][124/204]	Loss 0.0054 (0.0249)	
training:	Epoch: [49][125/204]	Loss 0.0060 (0.0247)	
training:	Epoch: [49][126/204]	Loss 0.0072 (0.0246)	
training:	Epoch: [49][127/204]	Loss 0.0200 (0.0245)	
training:	Epoch: [49][128/204]	Loss 0.0039 (0.0244)	
training:	Epoch: [49][129/204]	Loss 0.0047 (0.0242)	
training:	Epoch: [49][130/204]	Loss 0.0041 (0.0241)	
training:	Epoch: [49][131/204]	Loss 0.0049 (0.0239)	
training:	Epoch: [49][132/204]	Loss 0.0344 (0.0240)	
training:	Epoch: [49][133/204]	Loss 0.0046 (0.0239)	
training:	Epoch: [49][134/204]	Loss 0.1683 (0.0249)	
training:	Epoch: [49][135/204]	Loss 0.0038 (0.0248)	
training:	Epoch: [49][136/204]	Loss 0.0045 (0.0246)	
training:	Epoch: [49][137/204]	Loss 0.0050 (0.0245)	
training:	Epoch: [49][138/204]	Loss 0.0042 (0.0243)	
training:	Epoch: [49][139/204]	Loss 0.0980 (0.0249)	
training:	Epoch: [49][140/204]	Loss 0.0050 (0.0247)	
training:	Epoch: [49][141/204]	Loss 0.0038 (0.0246)	
training:	Epoch: [49][142/204]	Loss 0.0794 (0.0250)	
training:	Epoch: [49][143/204]	Loss 0.0049 (0.0248)	
training:	Epoch: [49][144/204]	Loss 0.0059 (0.0247)	
training:	Epoch: [49][145/204]	Loss 0.0049 (0.0246)	
training:	Epoch: [49][146/204]	Loss 0.0366 (0.0246)	
training:	Epoch: [49][147/204]	Loss 0.0101 (0.0245)	
training:	Epoch: [49][148/204]	Loss 0.1522 (0.0254)	
training:	Epoch: [49][149/204]	Loss 0.0964 (0.0259)	
training:	Epoch: [49][150/204]	Loss 0.1198 (0.0265)	
training:	Epoch: [49][151/204]	Loss 0.0038 (0.0264)	
training:	Epoch: [49][152/204]	Loss 0.0044 (0.0262)	
training:	Epoch: [49][153/204]	Loss 0.0044 (0.0261)	
training:	Epoch: [49][154/204]	Loss 0.0044 (0.0259)	
training:	Epoch: [49][155/204]	Loss 0.0964 (0.0264)	
training:	Epoch: [49][156/204]	Loss 0.0045 (0.0262)	
training:	Epoch: [49][157/204]	Loss 0.0041 (0.0261)	
training:	Epoch: [49][158/204]	Loss 0.0049 (0.0260)	
training:	Epoch: [49][159/204]	Loss 0.0046 (0.0258)	
training:	Epoch: [49][160/204]	Loss 0.0051 (0.0257)	
training:	Epoch: [49][161/204]	Loss 0.0048 (0.0256)	
training:	Epoch: [49][162/204]	Loss 0.0175 (0.0255)	
training:	Epoch: [49][163/204]	Loss 0.0072 (0.0254)	
training:	Epoch: [49][164/204]	Loss 0.0044 (0.0253)	
training:	Epoch: [49][165/204]	Loss 0.0046 (0.0252)	
training:	Epoch: [49][166/204]	Loss 0.0056 (0.0250)	
training:	Epoch: [49][167/204]	Loss 0.0355 (0.0251)	
training:	Epoch: [49][168/204]	Loss 0.1010 (0.0256)	
training:	Epoch: [49][169/204]	Loss 0.0307 (0.0256)	
training:	Epoch: [49][170/204]	Loss 0.0040 (0.0255)	
training:	Epoch: [49][171/204]	Loss 0.0042 (0.0253)	
training:	Epoch: [49][172/204]	Loss 0.0043 (0.0252)	
training:	Epoch: [49][173/204]	Loss 0.0045 (0.0251)	
training:	Epoch: [49][174/204]	Loss 0.0044 (0.0250)	
training:	Epoch: [49][175/204]	Loss 0.0069 (0.0249)	
training:	Epoch: [49][176/204]	Loss 0.0057 (0.0248)	
training:	Epoch: [49][177/204]	Loss 0.1216 (0.0253)	
training:	Epoch: [49][178/204]	Loss 0.0039 (0.0252)	
training:	Epoch: [49][179/204]	Loss 0.0052 (0.0251)	
training:	Epoch: [49][180/204]	Loss 0.0047 (0.0250)	
training:	Epoch: [49][181/204]	Loss 0.0191 (0.0249)	
training:	Epoch: [49][182/204]	Loss 0.1511 (0.0256)	
training:	Epoch: [49][183/204]	Loss 0.0074 (0.0255)	
training:	Epoch: [49][184/204]	Loss 0.1531 (0.0262)	
training:	Epoch: [49][185/204]	Loss 0.0072 (0.0261)	
training:	Epoch: [49][186/204]	Loss 0.0068 (0.0260)	
training:	Epoch: [49][187/204]	Loss 0.1684 (0.0268)	
training:	Epoch: [49][188/204]	Loss 0.0087 (0.0267)	
training:	Epoch: [49][189/204]	Loss 0.0537 (0.0268)	
training:	Epoch: [49][190/204]	Loss 0.0095 (0.0267)	
training:	Epoch: [49][191/204]	Loss 0.0039 (0.0266)	
training:	Epoch: [49][192/204]	Loss 0.0053 (0.0265)	
training:	Epoch: [49][193/204]	Loss 0.0038 (0.0264)	
training:	Epoch: [49][194/204]	Loss 0.0049 (0.0263)	
training:	Epoch: [49][195/204]	Loss 0.0081 (0.0262)	
training:	Epoch: [49][196/204]	Loss 0.0616 (0.0264)	
training:	Epoch: [49][197/204]	Loss 0.0832 (0.0266)	
training:	Epoch: [49][198/204]	Loss 0.0418 (0.0267)	
training:	Epoch: [49][199/204]	Loss 0.0238 (0.0267)	
training:	Epoch: [49][200/204]	Loss 0.0139 (0.0266)	
training:	Epoch: [49][201/204]	Loss 0.0173 (0.0266)	
training:	Epoch: [49][202/204]	Loss 0.0060 (0.0265)	
training:	Epoch: [49][203/204]	Loss 0.1813 (0.0273)	
training:	Epoch: [49][204/204]	Loss 0.0042 (0.0271)	
Training:	 Loss: 0.0271

Training:	 ACC: 0.9972 0.9972 0.9985 0.9959
Validation:	 ACC: 0.8050 0.8042 0.7861 0.8240
Validation:	 Best_BACC: 0.8326 0.8331 0.8424 0.8229
Validation:	 Loss: 0.8381
Pretraining:	Epoch 50/200
----------
training:	Epoch: [50][1/204]	Loss 0.0045 (0.0045)	
training:	Epoch: [50][2/204]	Loss 0.0058 (0.0052)	
training:	Epoch: [50][3/204]	Loss 0.0053 (0.0052)	
training:	Epoch: [50][4/204]	Loss 0.2481 (0.0659)	
training:	Epoch: [50][5/204]	Loss 0.0044 (0.0536)	
training:	Epoch: [50][6/204]	Loss 0.0053 (0.0456)	
training:	Epoch: [50][7/204]	Loss 0.0048 (0.0397)	
training:	Epoch: [50][8/204]	Loss 0.0069 (0.0356)	
training:	Epoch: [50][9/204]	Loss 0.0048 (0.0322)	
training:	Epoch: [50][10/204]	Loss 0.0048 (0.0295)	
training:	Epoch: [50][11/204]	Loss 0.0044 (0.0272)	
training:	Epoch: [50][12/204]	Loss 0.0048 (0.0253)	
training:	Epoch: [50][13/204]	Loss 0.0052 (0.0238)	
training:	Epoch: [50][14/204]	Loss 0.1073 (0.0297)	
training:	Epoch: [50][15/204]	Loss 0.0055 (0.0281)	
training:	Epoch: [50][16/204]	Loss 0.0069 (0.0268)	
training:	Epoch: [50][17/204]	Loss 0.0051 (0.0255)	
training:	Epoch: [50][18/204]	Loss 0.0042 (0.0243)	
training:	Epoch: [50][19/204]	Loss 0.1856 (0.0328)	
training:	Epoch: [50][20/204]	Loss 0.0042 (0.0314)	
training:	Epoch: [50][21/204]	Loss 0.0074 (0.0303)	
training:	Epoch: [50][22/204]	Loss 0.0079 (0.0292)	
training:	Epoch: [50][23/204]	Loss 0.1626 (0.0350)	
training:	Epoch: [50][24/204]	Loss 0.0038 (0.0337)	
training:	Epoch: [50][25/204]	Loss 0.0043 (0.0326)	
training:	Epoch: [50][26/204]	Loss 0.0075 (0.0316)	
training:	Epoch: [50][27/204]	Loss 0.2877 (0.0411)	
training:	Epoch: [50][28/204]	Loss 0.0044 (0.0398)	
training:	Epoch: [50][29/204]	Loss 0.0160 (0.0390)	
training:	Epoch: [50][30/204]	Loss 0.0038 (0.0378)	
training:	Epoch: [50][31/204]	Loss 0.0045 (0.0367)	
training:	Epoch: [50][32/204]	Loss 0.0041 (0.0357)	
training:	Epoch: [50][33/204]	Loss 0.0055 (0.0348)	
training:	Epoch: [50][34/204]	Loss 0.0048 (0.0339)	
training:	Epoch: [50][35/204]	Loss 0.0280 (0.0337)	
training:	Epoch: [50][36/204]	Loss 0.0050 (0.0329)	
training:	Epoch: [50][37/204]	Loss 0.0048 (0.0322)	
training:	Epoch: [50][38/204]	Loss 0.0084 (0.0315)	
training:	Epoch: [50][39/204]	Loss 0.0088 (0.0310)	
training:	Epoch: [50][40/204]	Loss 0.0436 (0.0313)	
training:	Epoch: [50][41/204]	Loss 0.0041 (0.0306)	
training:	Epoch: [50][42/204]	Loss 0.1131 (0.0326)	
training:	Epoch: [50][43/204]	Loss 0.0074 (0.0320)	
training:	Epoch: [50][44/204]	Loss 0.1524 (0.0347)	
training:	Epoch: [50][45/204]	Loss 0.0060 (0.0341)	
training:	Epoch: [50][46/204]	Loss 0.0041 (0.0334)	
training:	Epoch: [50][47/204]	Loss 0.0049 (0.0328)	
training:	Epoch: [50][48/204]	Loss 0.0047 (0.0322)	
training:	Epoch: [50][49/204]	Loss 0.0042 (0.0317)	
training:	Epoch: [50][50/204]	Loss 0.0047 (0.0311)	
training:	Epoch: [50][51/204]	Loss 0.0051 (0.0306)	
training:	Epoch: [50][52/204]	Loss 0.0097 (0.0302)	
training:	Epoch: [50][53/204]	Loss 0.0050 (0.0297)	
training:	Epoch: [50][54/204]	Loss 0.0049 (0.0293)	
training:	Epoch: [50][55/204]	Loss 0.0056 (0.0289)	
training:	Epoch: [50][56/204]	Loss 0.0049 (0.0284)	
training:	Epoch: [50][57/204]	Loss 0.0048 (0.0280)	
training:	Epoch: [50][58/204]	Loss 0.0071 (0.0276)	
training:	Epoch: [50][59/204]	Loss 0.0188 (0.0275)	
training:	Epoch: [50][60/204]	Loss 0.0946 (0.0286)	
training:	Epoch: [50][61/204]	Loss 0.0043 (0.0282)	
training:	Epoch: [50][62/204]	Loss 0.0050 (0.0278)	
training:	Epoch: [50][63/204]	Loss 0.0216 (0.0277)	
training:	Epoch: [50][64/204]	Loss 0.0047 (0.0274)	
training:	Epoch: [50][65/204]	Loss 0.0044 (0.0270)	
training:	Epoch: [50][66/204]	Loss 0.0050 (0.0267)	
training:	Epoch: [50][67/204]	Loss 0.0048 (0.0264)	
training:	Epoch: [50][68/204]	Loss 0.0628 (0.0269)	
training:	Epoch: [50][69/204]	Loss 0.0165 (0.0268)	
training:	Epoch: [50][70/204]	Loss 0.0048 (0.0264)	
training:	Epoch: [50][71/204]	Loss 0.0314 (0.0265)	
training:	Epoch: [50][72/204]	Loss 0.0058 (0.0262)	
training:	Epoch: [50][73/204]	Loss 0.0047 (0.0259)	
training:	Epoch: [50][74/204]	Loss 0.0043 (0.0256)	
training:	Epoch: [50][75/204]	Loss 0.0044 (0.0254)	
training:	Epoch: [50][76/204]	Loss 0.0054 (0.0251)	
training:	Epoch: [50][77/204]	Loss 0.0069 (0.0249)	
training:	Epoch: [50][78/204]	Loss 0.0055 (0.0246)	
training:	Epoch: [50][79/204]	Loss 0.1427 (0.0261)	
training:	Epoch: [50][80/204]	Loss 0.0058 (0.0258)	
training:	Epoch: [50][81/204]	Loss 0.0324 (0.0259)	
training:	Epoch: [50][82/204]	Loss 0.0047 (0.0257)	
training:	Epoch: [50][83/204]	Loss 0.0109 (0.0255)	
training:	Epoch: [50][84/204]	Loss 0.0048 (0.0252)	
training:	Epoch: [50][85/204]	Loss 0.0160 (0.0251)	
training:	Epoch: [50][86/204]	Loss 0.0634 (0.0256)	
training:	Epoch: [50][87/204]	Loss 0.3747 (0.0296)	
training:	Epoch: [50][88/204]	Loss 0.1756 (0.0313)	
training:	Epoch: [50][89/204]	Loss 0.0072 (0.0310)	
training:	Epoch: [50][90/204]	Loss 0.0065 (0.0307)	
training:	Epoch: [50][91/204]	Loss 0.0079 (0.0305)	
training:	Epoch: [50][92/204]	Loss 0.0217 (0.0304)	
training:	Epoch: [50][93/204]	Loss 0.0049 (0.0301)	
training:	Epoch: [50][94/204]	Loss 0.0565 (0.0304)	
training:	Epoch: [50][95/204]	Loss 0.0053 (0.0301)	
training:	Epoch: [50][96/204]	Loss 0.0081 (0.0299)	
training:	Epoch: [50][97/204]	Loss 0.0048 (0.0296)	
training:	Epoch: [50][98/204]	Loss 0.0052 (0.0294)	
training:	Epoch: [50][99/204]	Loss 0.0047 (0.0291)	
training:	Epoch: [50][100/204]	Loss 0.0913 (0.0297)	
training:	Epoch: [50][101/204]	Loss 0.0219 (0.0297)	
training:	Epoch: [50][102/204]	Loss 0.1878 (0.0312)	
training:	Epoch: [50][103/204]	Loss 0.0047 (0.0310)	
training:	Epoch: [50][104/204]	Loss 0.0070 (0.0307)	
training:	Epoch: [50][105/204]	Loss 0.0078 (0.0305)	
training:	Epoch: [50][106/204]	Loss 0.0092 (0.0303)	
training:	Epoch: [50][107/204]	Loss 0.0193 (0.0302)	
training:	Epoch: [50][108/204]	Loss 0.0062 (0.0300)	
training:	Epoch: [50][109/204]	Loss 0.0614 (0.0303)	
training:	Epoch: [50][110/204]	Loss 0.0446 (0.0304)	
training:	Epoch: [50][111/204]	Loss 0.0049 (0.0302)	
training:	Epoch: [50][112/204]	Loss 0.0372 (0.0302)	
training:	Epoch: [50][113/204]	Loss 0.0051 (0.0300)	
training:	Epoch: [50][114/204]	Loss 0.0045 (0.0298)	
training:	Epoch: [50][115/204]	Loss 0.0044 (0.0296)	
training:	Epoch: [50][116/204]	Loss 0.0413 (0.0297)	
training:	Epoch: [50][117/204]	Loss 0.1090 (0.0303)	
training:	Epoch: [50][118/204]	Loss 0.0049 (0.0301)	
training:	Epoch: [50][119/204]	Loss 0.0434 (0.0302)	
training:	Epoch: [50][120/204]	Loss 0.0043 (0.0300)	
training:	Epoch: [50][121/204]	Loss 0.0051 (0.0298)	
training:	Epoch: [50][122/204]	Loss 0.0381 (0.0299)	
training:	Epoch: [50][123/204]	Loss 0.1857 (0.0312)	
training:	Epoch: [50][124/204]	Loss 0.0037 (0.0309)	
training:	Epoch: [50][125/204]	Loss 0.0068 (0.0307)	
training:	Epoch: [50][126/204]	Loss 0.0082 (0.0306)	
training:	Epoch: [50][127/204]	Loss 0.0085 (0.0304)	
training:	Epoch: [50][128/204]	Loss 0.0146 (0.0303)	
training:	Epoch: [50][129/204]	Loss 0.0073 (0.0301)	
training:	Epoch: [50][130/204]	Loss 0.0049 (0.0299)	
training:	Epoch: [50][131/204]	Loss 0.0052 (0.0297)	
training:	Epoch: [50][132/204]	Loss 0.0039 (0.0295)	
training:	Epoch: [50][133/204]	Loss 0.0074 (0.0293)	
training:	Epoch: [50][134/204]	Loss 0.0076 (0.0292)	
training:	Epoch: [50][135/204]	Loss 0.0053 (0.0290)	
training:	Epoch: [50][136/204]	Loss 0.0235 (0.0290)	
training:	Epoch: [50][137/204]	Loss 0.1659 (0.0300)	
training:	Epoch: [50][138/204]	Loss 0.0083 (0.0298)	
training:	Epoch: [50][139/204]	Loss 0.0044 (0.0296)	
training:	Epoch: [50][140/204]	Loss 0.0060 (0.0295)	
training:	Epoch: [50][141/204]	Loss 0.0097 (0.0293)	
training:	Epoch: [50][142/204]	Loss 0.1691 (0.0303)	
training:	Epoch: [50][143/204]	Loss 0.0060 (0.0301)	
training:	Epoch: [50][144/204]	Loss 0.0054 (0.0300)	
training:	Epoch: [50][145/204]	Loss 0.0101 (0.0298)	
training:	Epoch: [50][146/204]	Loss 0.0412 (0.0299)	
training:	Epoch: [50][147/204]	Loss 0.0052 (0.0297)	
training:	Epoch: [50][148/204]	Loss 0.0065 (0.0296)	
training:	Epoch: [50][149/204]	Loss 0.0143 (0.0295)	
training:	Epoch: [50][150/204]	Loss 0.0048 (0.0293)	
training:	Epoch: [50][151/204]	Loss 0.0063 (0.0292)	
training:	Epoch: [50][152/204]	Loss 0.0363 (0.0292)	
training:	Epoch: [50][153/204]	Loss 0.0051 (0.0290)	
training:	Epoch: [50][154/204]	Loss 0.0049 (0.0289)	
training:	Epoch: [50][155/204]	Loss 0.0204 (0.0288)	
training:	Epoch: [50][156/204]	Loss 0.0046 (0.0287)	
training:	Epoch: [50][157/204]	Loss 0.0051 (0.0285)	
training:	Epoch: [50][158/204]	Loss 0.0258 (0.0285)	
training:	Epoch: [50][159/204]	Loss 0.0951 (0.0289)	
training:	Epoch: [50][160/204]	Loss 0.0048 (0.0288)	
training:	Epoch: [50][161/204]	Loss 0.0048 (0.0286)	
training:	Epoch: [50][162/204]	Loss 0.2243 (0.0298)	
training:	Epoch: [50][163/204]	Loss 0.0085 (0.0297)	
training:	Epoch: [50][164/204]	Loss 0.1465 (0.0304)	
training:	Epoch: [50][165/204]	Loss 0.0069 (0.0303)	
training:	Epoch: [50][166/204]	Loss 0.0104 (0.0302)	
training:	Epoch: [50][167/204]	Loss 0.0094 (0.0300)	
training:	Epoch: [50][168/204]	Loss 0.0095 (0.0299)	
training:	Epoch: [50][169/204]	Loss 0.0044 (0.0298)	
training:	Epoch: [50][170/204]	Loss 0.0042 (0.0296)	
training:	Epoch: [50][171/204]	Loss 0.0046 (0.0295)	
training:	Epoch: [50][172/204]	Loss 0.0073 (0.0293)	
training:	Epoch: [50][173/204]	Loss 0.0043 (0.0292)	
training:	Epoch: [50][174/204]	Loss 0.0047 (0.0290)	
training:	Epoch: [50][175/204]	Loss 0.0084 (0.0289)	
training:	Epoch: [50][176/204]	Loss 0.0046 (0.0288)	
training:	Epoch: [50][177/204]	Loss 0.0311 (0.0288)	
training:	Epoch: [50][178/204]	Loss 0.0051 (0.0287)	
training:	Epoch: [50][179/204]	Loss 0.0084 (0.0286)	
training:	Epoch: [50][180/204]	Loss 0.0134 (0.0285)	
training:	Epoch: [50][181/204]	Loss 0.0469 (0.0286)	
training:	Epoch: [50][182/204]	Loss 0.0046 (0.0284)	
training:	Epoch: [50][183/204]	Loss 0.0044 (0.0283)	
training:	Epoch: [50][184/204]	Loss 0.0045 (0.0282)	
training:	Epoch: [50][185/204]	Loss 0.0067 (0.0281)	
training:	Epoch: [50][186/204]	Loss 0.0039 (0.0279)	
training:	Epoch: [50][187/204]	Loss 0.0051 (0.0278)	
training:	Epoch: [50][188/204]	Loss 0.0058 (0.0277)	
training:	Epoch: [50][189/204]	Loss 0.0217 (0.0277)	
training:	Epoch: [50][190/204]	Loss 0.0046 (0.0275)	
training:	Epoch: [50][191/204]	Loss 0.0265 (0.0275)	
training:	Epoch: [50][192/204]	Loss 0.0048 (0.0274)	
training:	Epoch: [50][193/204]	Loss 0.0044 (0.0273)	
training:	Epoch: [50][194/204]	Loss 0.0046 (0.0272)	
training:	Epoch: [50][195/204]	Loss 0.0058 (0.0271)	
training:	Epoch: [50][196/204]	Loss 0.0044 (0.0270)	
training:	Epoch: [50][197/204]	Loss 0.0047 (0.0268)	
training:	Epoch: [50][198/204]	Loss 0.0046 (0.0267)	
training:	Epoch: [50][199/204]	Loss 0.0956 (0.0271)	
training:	Epoch: [50][200/204]	Loss 0.0043 (0.0270)	
training:	Epoch: [50][201/204]	Loss 0.0050 (0.0269)	
training:	Epoch: [50][202/204]	Loss 0.1679 (0.0276)	
training:	Epoch: [50][203/204]	Loss 0.0053 (0.0274)	
training:	Epoch: [50][204/204]	Loss 0.0045 (0.0273)	
Training:	 Loss: 0.0273

Training:	 ACC: 0.9974 0.9974 0.9985 0.9962
Validation:	 ACC: 0.8050 0.8047 0.7994 0.8105
Validation:	 Best_BACC: 0.8326 0.8331 0.8424 0.8229
Validation:	 Loss: 0.8386
Pretraining:	Epoch 51/200
----------
training:	Epoch: [51][1/204]	Loss 0.0725 (0.0725)	
training:	Epoch: [51][2/204]	Loss 0.0060 (0.0393)	
training:	Epoch: [51][3/204]	Loss 0.0232 (0.0339)	
training:	Epoch: [51][4/204]	Loss 0.0047 (0.0266)	
training:	Epoch: [51][5/204]	Loss 0.0042 (0.0221)	
training:	Epoch: [51][6/204]	Loss 0.0038 (0.0191)	
training:	Epoch: [51][7/204]	Loss 0.0053 (0.0171)	
training:	Epoch: [51][8/204]	Loss 0.0111 (0.0164)	
training:	Epoch: [51][9/204]	Loss 0.0046 (0.0151)	
training:	Epoch: [51][10/204]	Loss 0.0038 (0.0139)	
training:	Epoch: [51][11/204]	Loss 0.1333 (0.0248)	
training:	Epoch: [51][12/204]	Loss 0.0389 (0.0260)	
training:	Epoch: [51][13/204]	Loss 0.0054 (0.0244)	
training:	Epoch: [51][14/204]	Loss 0.0067 (0.0231)	
training:	Epoch: [51][15/204]	Loss 0.0041 (0.0218)	
training:	Epoch: [51][16/204]	Loss 0.0037 (0.0207)	
training:	Epoch: [51][17/204]	Loss 0.0055 (0.0198)	
training:	Epoch: [51][18/204]	Loss 0.0051 (0.0190)	
training:	Epoch: [51][19/204]	Loss 0.0049 (0.0183)	
