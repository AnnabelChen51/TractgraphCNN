Namespace(inputDirectory='data', outputDirectory='val', DisFile='data/dis_sort_roi2dis2000.npy', task='sex', type='binary', feature=['Nos', 'FA1'], CUDA_id='0', data_id='0', dataset='Classification', norm=True, channels=1, epochs=200, tensorboard=True, net_architecture='TractGraphormer', batch_size=16, rate=3e-05, weight=0.0, sched_step=300, sched_gamma=0.1, printing_frequency=1, seed=1, alpha=0, remix_kappa=0, remix_tau=0, loss='CE', sigma=0, k=20)
Training the 'TractGraphormer' architecture

The following parameters are used:
Batch size:	16
Number of workers:	0
Learning rate:	3e-05
Weight decay:	0.0
Scheduler steps:	300
Scheduler gamma:	0.1
Number of epochs of training:	200
Number of input channels:	1

Data preparation
Reading data from:	./data
Training set size:	6538
Validation set size:	1869

Performing calculations on:	cuda:0

Pretraining:	Epoch 1/200
----------
training:	Epoch: [1][1/408]	Loss 0.7037 (0.7037)	
training:	Epoch: [1][2/408]	Loss 0.6691 (0.6864)	
training:	Epoch: [1][3/408]	Loss 0.6855 (0.6861)	
training:	Epoch: [1][4/408]	Loss 0.7395 (0.6994)	
training:	Epoch: [1][5/408]	Loss 0.7258 (0.7047)	
training:	Epoch: [1][6/408]	Loss 0.6857 (0.7016)	
training:	Epoch: [1][7/408]	Loss 0.6991 (0.7012)	
training:	Epoch: [1][8/408]	Loss 0.6896 (0.6998)	
training:	Epoch: [1][9/408]	Loss 0.7193 (0.7019)	
training:	Epoch: [1][10/408]	Loss 0.6994 (0.7017)	
training:	Epoch: [1][11/408]	Loss 0.7078 (0.7022)	
training:	Epoch: [1][12/408]	Loss 0.6972 (0.7018)	
training:	Epoch: [1][13/408]	Loss 0.7077 (0.7023)	
training:	Epoch: [1][14/408]	Loss 0.6984 (0.7020)	
training:	Epoch: [1][15/408]	Loss 0.7138 (0.7028)	
training:	Epoch: [1][16/408]	Loss 0.6925 (0.7021)	
training:	Epoch: [1][17/408]	Loss 0.6772 (0.7007)	
training:	Epoch: [1][18/408]	Loss 0.7082 (0.7011)	
training:	Epoch: [1][19/408]	Loss 0.6952 (0.7008)	
training:	Epoch: [1][20/408]	Loss 0.6936 (0.7004)	
training:	Epoch: [1][21/408]	Loss 0.6812 (0.6995)	
training:	Epoch: [1][22/408]	Loss 0.6790 (0.6986)	
training:	Epoch: [1][23/408]	Loss 0.6709 (0.6974)	
training:	Epoch: [1][24/408]	Loss 0.6878 (0.6970)	
training:	Epoch: [1][25/408]	Loss 0.6911 (0.6967)	
training:	Epoch: [1][26/408]	Loss 0.6877 (0.6964)	
training:	Epoch: [1][27/408]	Loss 0.6749 (0.6956)	
training:	Epoch: [1][28/408]	Loss 0.6967 (0.6956)	
training:	Epoch: [1][29/408]	Loss 0.6617 (0.6945)	
training:	Epoch: [1][30/408]	Loss 0.6752 (0.6938)	
training:	Epoch: [1][31/408]	Loss 0.6852 (0.6935)	
training:	Epoch: [1][32/408]	Loss 0.7386 (0.6949)	
training:	Epoch: [1][33/408]	Loss 0.6958 (0.6950)	
training:	Epoch: [1][34/408]	Loss 0.7160 (0.6956)	
training:	Epoch: [1][35/408]	Loss 0.6902 (0.6954)	
training:	Epoch: [1][36/408]	Loss 0.6694 (0.6947)	
training:	Epoch: [1][37/408]	Loss 0.6729 (0.6941)	
training:	Epoch: [1][38/408]	Loss 0.6532 (0.6930)	
training:	Epoch: [1][39/408]	Loss 0.6478 (0.6919)	
training:	Epoch: [1][40/408]	Loss 0.6812 (0.6916)	
training:	Epoch: [1][41/408]	Loss 0.7123 (0.6921)	
training:	Epoch: [1][42/408]	Loss 0.6499 (0.6911)	
training:	Epoch: [1][43/408]	Loss 0.6996 (0.6913)	
training:	Epoch: [1][44/408]	Loss 0.7037 (0.6916)	
training:	Epoch: [1][45/408]	Loss 0.7395 (0.6927)	
training:	Epoch: [1][46/408]	Loss 0.7129 (0.6931)	
training:	Epoch: [1][47/408]	Loss 0.7335 (0.6940)	
training:	Epoch: [1][48/408]	Loss 0.7055 (0.6942)	
training:	Epoch: [1][49/408]	Loss 0.6654 (0.6936)	
training:	Epoch: [1][50/408]	Loss 0.6963 (0.6937)	
training:	Epoch: [1][51/408]	Loss 0.7046 (0.6939)	
training:	Epoch: [1][52/408]	Loss 0.6546 (0.6931)	
training:	Epoch: [1][53/408]	Loss 0.6816 (0.6929)	
training:	Epoch: [1][54/408]	Loss 0.6910 (0.6929)	
training:	Epoch: [1][55/408]	Loss 0.7102 (0.6932)	
training:	Epoch: [1][56/408]	Loss 0.6849 (0.6930)	
training:	Epoch: [1][57/408]	Loss 0.7138 (0.6934)	
training:	Epoch: [1][58/408]	Loss 0.6831 (0.6932)	
training:	Epoch: [1][59/408]	Loss 0.6994 (0.6933)	
training:	Epoch: [1][60/408]	Loss 0.7251 (0.6939)	
training:	Epoch: [1][61/408]	Loss 0.6988 (0.6939)	
training:	Epoch: [1][62/408]	Loss 0.6781 (0.6937)	
training:	Epoch: [1][63/408]	Loss 0.7022 (0.6938)	
training:	Epoch: [1][64/408]	Loss 0.7159 (0.6942)	
training:	Epoch: [1][65/408]	Loss 0.7203 (0.6946)	
training:	Epoch: [1][66/408]	Loss 0.6989 (0.6946)	
training:	Epoch: [1][67/408]	Loss 0.6750 (0.6943)	
training:	Epoch: [1][68/408]	Loss 0.6997 (0.6944)	
training:	Epoch: [1][69/408]	Loss 0.6868 (0.6943)	
training:	Epoch: [1][70/408]	Loss 0.7070 (0.6945)	
training:	Epoch: [1][71/408]	Loss 0.6873 (0.6944)	
training:	Epoch: [1][72/408]	Loss 0.6714 (0.6941)	
training:	Epoch: [1][73/408]	Loss 0.6841 (0.6939)	
training:	Epoch: [1][74/408]	Loss 0.6802 (0.6937)	
training:	Epoch: [1][75/408]	Loss 0.6888 (0.6937)	
training:	Epoch: [1][76/408]	Loss 0.6932 (0.6937)	
training:	Epoch: [1][77/408]	Loss 0.6905 (0.6936)	
training:	Epoch: [1][78/408]	Loss 0.7077 (0.6938)	
training:	Epoch: [1][79/408]	Loss 0.6746 (0.6936)	
training:	Epoch: [1][80/408]	Loss 0.6805 (0.6934)	
training:	Epoch: [1][81/408]	Loss 0.6639 (0.6930)	
training:	Epoch: [1][82/408]	Loss 0.6977 (0.6931)	
training:	Epoch: [1][83/408]	Loss 0.6806 (0.6930)	
training:	Epoch: [1][84/408]	Loss 0.6856 (0.6929)	
training:	Epoch: [1][85/408]	Loss 0.6798 (0.6927)	
training:	Epoch: [1][86/408]	Loss 0.6588 (0.6923)	
training:	Epoch: [1][87/408]	Loss 0.6812 (0.6922)	
training:	Epoch: [1][88/408]	Loss 0.7056 (0.6923)	
training:	Epoch: [1][89/408]	Loss 0.6778 (0.6922)	
training:	Epoch: [1][90/408]	Loss 0.7226 (0.6925)	
training:	Epoch: [1][91/408]	Loss 0.6913 (0.6925)	
training:	Epoch: [1][92/408]	Loss 0.6834 (0.6924)	
training:	Epoch: [1][93/408]	Loss 0.6557 (0.6920)	
training:	Epoch: [1][94/408]	Loss 0.7057 (0.6922)	
training:	Epoch: [1][95/408]	Loss 0.7118 (0.6924)	
training:	Epoch: [1][96/408]	Loss 0.6694 (0.6921)	
training:	Epoch: [1][97/408]	Loss 0.6673 (0.6919)	
training:	Epoch: [1][98/408]	Loss 0.6967 (0.6919)	
training:	Epoch: [1][99/408]	Loss 0.6678 (0.6917)	
training:	Epoch: [1][100/408]	Loss 0.6845 (0.6916)	
training:	Epoch: [1][101/408]	Loss 0.6572 (0.6913)	
training:	Epoch: [1][102/408]	Loss 0.6955 (0.6913)	
training:	Epoch: [1][103/408]	Loss 0.6610 (0.6910)	
training:	Epoch: [1][104/408]	Loss 0.7154 (0.6912)	
training:	Epoch: [1][105/408]	Loss 0.6769 (0.6911)	
training:	Epoch: [1][106/408]	Loss 0.6996 (0.6912)	
training:	Epoch: [1][107/408]	Loss 0.6774 (0.6911)	
training:	Epoch: [1][108/408]	Loss 0.6962 (0.6911)	
training:	Epoch: [1][109/408]	Loss 0.6807 (0.6910)	
training:	Epoch: [1][110/408]	Loss 0.6754 (0.6909)	
training:	Epoch: [1][111/408]	Loss 0.6924 (0.6909)	
training:	Epoch: [1][112/408]	Loss 0.6674 (0.6907)	
training:	Epoch: [1][113/408]	Loss 0.6753 (0.6905)	
training:	Epoch: [1][114/408]	Loss 0.6527 (0.6902)	
training:	Epoch: [1][115/408]	Loss 0.6934 (0.6902)	
training:	Epoch: [1][116/408]	Loss 0.6698 (0.6901)	
training:	Epoch: [1][117/408]	Loss 0.6936 (0.6901)	
training:	Epoch: [1][118/408]	Loss 0.7051 (0.6902)	
training:	Epoch: [1][119/408]	Loss 0.6634 (0.6900)	
training:	Epoch: [1][120/408]	Loss 0.6677 (0.6898)	
training:	Epoch: [1][121/408]	Loss 0.6794 (0.6897)	
training:	Epoch: [1][122/408]	Loss 0.6869 (0.6897)	
training:	Epoch: [1][123/408]	Loss 0.6855 (0.6897)	
training:	Epoch: [1][124/408]	Loss 0.6397 (0.6893)	
training:	Epoch: [1][125/408]	Loss 0.6967 (0.6893)	
training:	Epoch: [1][126/408]	Loss 0.6961 (0.6894)	
training:	Epoch: [1][127/408]	Loss 0.6677 (0.6892)	
training:	Epoch: [1][128/408]	Loss 0.6672 (0.6890)	
training:	Epoch: [1][129/408]	Loss 0.6611 (0.6888)	
training:	Epoch: [1][130/408]	Loss 0.6883 (0.6888)	
training:	Epoch: [1][131/408]	Loss 0.6691 (0.6887)	
training:	Epoch: [1][132/408]	Loss 0.6545 (0.6884)	
training:	Epoch: [1][133/408]	Loss 0.6458 (0.6881)	
training:	Epoch: [1][134/408]	Loss 0.6315 (0.6877)	
training:	Epoch: [1][135/408]	Loss 0.6956 (0.6877)	
training:	Epoch: [1][136/408]	Loss 0.6246 (0.6872)	
training:	Epoch: [1][137/408]	Loss 0.7272 (0.6875)	
training:	Epoch: [1][138/408]	Loss 0.6777 (0.6875)	
training:	Epoch: [1][139/408]	Loss 0.6684 (0.6873)	
training:	Epoch: [1][140/408]	Loss 0.6399 (0.6870)	
training:	Epoch: [1][141/408]	Loss 0.6452 (0.6867)	
training:	Epoch: [1][142/408]	Loss 0.6821 (0.6867)	
training:	Epoch: [1][143/408]	Loss 0.6412 (0.6863)	
training:	Epoch: [1][144/408]	Loss 0.6474 (0.6861)	
training:	Epoch: [1][145/408]	Loss 0.6051 (0.6855)	
training:	Epoch: [1][146/408]	Loss 0.6594 (0.6853)	
training:	Epoch: [1][147/408]	Loss 0.6431 (0.6851)	
training:	Epoch: [1][148/408]	Loss 0.6369 (0.6847)	
training:	Epoch: [1][149/408]	Loss 0.6551 (0.6845)	
training:	Epoch: [1][150/408]	Loss 0.6001 (0.6840)	
training:	Epoch: [1][151/408]	Loss 0.6448 (0.6837)	
training:	Epoch: [1][152/408]	Loss 0.6136 (0.6832)	
training:	Epoch: [1][153/408]	Loss 0.6850 (0.6833)	
training:	Epoch: [1][154/408]	Loss 0.6535 (0.6831)	
training:	Epoch: [1][155/408]	Loss 0.7115 (0.6832)	
training:	Epoch: [1][156/408]	Loss 0.6621 (0.6831)	
training:	Epoch: [1][157/408]	Loss 0.6476 (0.6829)	
training:	Epoch: [1][158/408]	Loss 0.6072 (0.6824)	
training:	Epoch: [1][159/408]	Loss 0.6749 (0.6824)	
training:	Epoch: [1][160/408]	Loss 0.6126 (0.6819)	
training:	Epoch: [1][161/408]	Loss 0.6639 (0.6818)	
training:	Epoch: [1][162/408]	Loss 0.6221 (0.6814)	
training:	Epoch: [1][163/408]	Loss 0.6392 (0.6812)	
training:	Epoch: [1][164/408]	Loss 0.6723 (0.6811)	
training:	Epoch: [1][165/408]	Loss 0.6126 (0.6807)	
training:	Epoch: [1][166/408]	Loss 0.6345 (0.6804)	
training:	Epoch: [1][167/408]	Loss 0.6336 (0.6802)	
training:	Epoch: [1][168/408]	Loss 0.6182 (0.6798)	
training:	Epoch: [1][169/408]	Loss 0.6844 (0.6798)	
training:	Epoch: [1][170/408]	Loss 0.5923 (0.6793)	
training:	Epoch: [1][171/408]	Loss 0.6194 (0.6789)	
training:	Epoch: [1][172/408]	Loss 0.6039 (0.6785)	
training:	Epoch: [1][173/408]	Loss 0.5847 (0.6780)	
training:	Epoch: [1][174/408]	Loss 0.6613 (0.6779)	
training:	Epoch: [1][175/408]	Loss 0.6144 (0.6775)	
training:	Epoch: [1][176/408]	Loss 0.6762 (0.6775)	
training:	Epoch: [1][177/408]	Loss 0.6518 (0.6774)	
training:	Epoch: [1][178/408]	Loss 0.5839 (0.6768)	
training:	Epoch: [1][179/408]	Loss 0.5603 (0.6762)	
training:	Epoch: [1][180/408]	Loss 0.7991 (0.6769)	
training:	Epoch: [1][181/408]	Loss 0.5501 (0.6762)	
training:	Epoch: [1][182/408]	Loss 0.6144 (0.6758)	
training:	Epoch: [1][183/408]	Loss 0.5880 (0.6753)	
training:	Epoch: [1][184/408]	Loss 0.6346 (0.6751)	
training:	Epoch: [1][185/408]	Loss 0.5891 (0.6747)	
training:	Epoch: [1][186/408]	Loss 0.5965 (0.6742)	
training:	Epoch: [1][187/408]	Loss 0.5693 (0.6737)	
training:	Epoch: [1][188/408]	Loss 0.6860 (0.6737)	
training:	Epoch: [1][189/408]	Loss 0.6306 (0.6735)	
training:	Epoch: [1][190/408]	Loss 0.6895 (0.6736)	
training:	Epoch: [1][191/408]	Loss 0.6223 (0.6733)	
training:	Epoch: [1][192/408]	Loss 0.5076 (0.6725)	
training:	Epoch: [1][193/408]	Loss 0.6164 (0.6722)	
training:	Epoch: [1][194/408]	Loss 0.5463 (0.6715)	
training:	Epoch: [1][195/408]	Loss 0.6741 (0.6715)	
training:	Epoch: [1][196/408]	Loss 0.6023 (0.6712)	
training:	Epoch: [1][197/408]	Loss 0.6880 (0.6713)	
training:	Epoch: [1][198/408]	Loss 0.6522 (0.6712)	
training:	Epoch: [1][199/408]	Loss 0.6080 (0.6709)	
training:	Epoch: [1][200/408]	Loss 0.5744 (0.6704)	
training:	Epoch: [1][201/408]	Loss 0.6964 (0.6705)	
training:	Epoch: [1][202/408]	Loss 0.6325 (0.6703)	
training:	Epoch: [1][203/408]	Loss 0.5611 (0.6698)	
training:	Epoch: [1][204/408]	Loss 0.5352 (0.6691)	
training:	Epoch: [1][205/408]	Loss 0.4983 (0.6683)	
training:	Epoch: [1][206/408]	Loss 0.6000 (0.6680)	
training:	Epoch: [1][207/408]	Loss 0.5495 (0.6674)	
training:	Epoch: [1][208/408]	Loss 0.6022 (0.6671)	
training:	Epoch: [1][209/408]	Loss 0.6567 (0.6670)	
training:	Epoch: [1][210/408]	Loss 0.4871 (0.6662)	
training:	Epoch: [1][211/408]	Loss 0.5097 (0.6654)	
training:	Epoch: [1][212/408]	Loss 0.6481 (0.6653)	
training:	Epoch: [1][213/408]	Loss 0.5775 (0.6649)	
training:	Epoch: [1][214/408]	Loss 0.5413 (0.6644)	
training:	Epoch: [1][215/408]	Loss 0.6583 (0.6643)	
training:	Epoch: [1][216/408]	Loss 0.5053 (0.6636)	
training:	Epoch: [1][217/408]	Loss 0.5807 (0.6632)	
training:	Epoch: [1][218/408]	Loss 0.5783 (0.6628)	
training:	Epoch: [1][219/408]	Loss 0.5585 (0.6623)	
training:	Epoch: [1][220/408]	Loss 0.6912 (0.6625)	
training:	Epoch: [1][221/408]	Loss 0.4960 (0.6617)	
training:	Epoch: [1][222/408]	Loss 0.6743 (0.6618)	
training:	Epoch: [1][223/408]	Loss 0.5800 (0.6614)	
training:	Epoch: [1][224/408]	Loss 0.8287 (0.6622)	
training:	Epoch: [1][225/408]	Loss 0.7534 (0.6626)	
training:	Epoch: [1][226/408]	Loss 0.5439 (0.6620)	
training:	Epoch: [1][227/408]	Loss 0.5980 (0.6618)	
training:	Epoch: [1][228/408]	Loss 0.6312 (0.6616)	
training:	Epoch: [1][229/408]	Loss 0.7324 (0.6619)	
training:	Epoch: [1][230/408]	Loss 0.7554 (0.6623)	
training:	Epoch: [1][231/408]	Loss 0.5420 (0.6618)	
training:	Epoch: [1][232/408]	Loss 0.5070 (0.6611)	
training:	Epoch: [1][233/408]	Loss 0.7907 (0.6617)	
training:	Epoch: [1][234/408]	Loss 0.5830 (0.6614)	
training:	Epoch: [1][235/408]	Loss 0.5963 (0.6611)	
training:	Epoch: [1][236/408]	Loss 0.5691 (0.6607)	
training:	Epoch: [1][237/408]	Loss 0.6653 (0.6607)	
training:	Epoch: [1][238/408]	Loss 0.5764 (0.6604)	
training:	Epoch: [1][239/408]	Loss 0.5796 (0.6600)	
training:	Epoch: [1][240/408]	Loss 0.5008 (0.6594)	
training:	Epoch: [1][241/408]	Loss 0.6485 (0.6593)	
training:	Epoch: [1][242/408]	Loss 0.6855 (0.6594)	
training:	Epoch: [1][243/408]	Loss 0.5285 (0.6589)	
training:	Epoch: [1][244/408]	Loss 0.4717 (0.6581)	
training:	Epoch: [1][245/408]	Loss 0.6458 (0.6581)	
training:	Epoch: [1][246/408]	Loss 0.6573 (0.6581)	
training:	Epoch: [1][247/408]	Loss 0.4640 (0.6573)	
training:	Epoch: [1][248/408]	Loss 0.5476 (0.6568)	
training:	Epoch: [1][249/408]	Loss 0.6607 (0.6569)	
training:	Epoch: [1][250/408]	Loss 0.5897 (0.6566)	
training:	Epoch: [1][251/408]	Loss 0.5786 (0.6563)	
training:	Epoch: [1][252/408]	Loss 0.4884 (0.6556)	
training:	Epoch: [1][253/408]	Loss 0.5588 (0.6552)	
training:	Epoch: [1][254/408]	Loss 0.5534 (0.6548)	
training:	Epoch: [1][255/408]	Loss 0.5887 (0.6546)	
training:	Epoch: [1][256/408]	Loss 0.5722 (0.6542)	
training:	Epoch: [1][257/408]	Loss 0.6620 (0.6543)	
training:	Epoch: [1][258/408]	Loss 0.6395 (0.6542)	
training:	Epoch: [1][259/408]	Loss 0.5522 (0.6538)	
training:	Epoch: [1][260/408]	Loss 0.4898 (0.6532)	
training:	Epoch: [1][261/408]	Loss 0.7237 (0.6535)	
training:	Epoch: [1][262/408]	Loss 0.7229 (0.6537)	
training:	Epoch: [1][263/408]	Loss 0.5787 (0.6534)	
training:	Epoch: [1][264/408]	Loss 0.5463 (0.6530)	
training:	Epoch: [1][265/408]	Loss 0.5531 (0.6527)	
training:	Epoch: [1][266/408]	Loss 0.4735 (0.6520)	
training:	Epoch: [1][267/408]	Loss 0.5100 (0.6515)	
training:	Epoch: [1][268/408]	Loss 0.4535 (0.6507)	
training:	Epoch: [1][269/408]	Loss 0.5154 (0.6502)	
training:	Epoch: [1][270/408]	Loss 0.5342 (0.6498)	
training:	Epoch: [1][271/408]	Loss 0.4940 (0.6492)	
training:	Epoch: [1][272/408]	Loss 0.6413 (0.6492)	
training:	Epoch: [1][273/408]	Loss 0.6136 (0.6490)	
training:	Epoch: [1][274/408]	Loss 0.5833 (0.6488)	
training:	Epoch: [1][275/408]	Loss 0.5151 (0.6483)	
training:	Epoch: [1][276/408]	Loss 0.5727 (0.6480)	
training:	Epoch: [1][277/408]	Loss 0.5269 (0.6476)	
training:	Epoch: [1][278/408]	Loss 0.4431 (0.6469)	
training:	Epoch: [1][279/408]	Loss 0.5523 (0.6465)	
training:	Epoch: [1][280/408]	Loss 0.6549 (0.6466)	
training:	Epoch: [1][281/408]	Loss 0.4359 (0.6458)	
training:	Epoch: [1][282/408]	Loss 0.5786 (0.6456)	
training:	Epoch: [1][283/408]	Loss 0.5659 (0.6453)	
training:	Epoch: [1][284/408]	Loss 0.5094 (0.6448)	
training:	Epoch: [1][285/408]	Loss 0.4732 (0.6442)	
training:	Epoch: [1][286/408]	Loss 0.4508 (0.6435)	
training:	Epoch: [1][287/408]	Loss 0.6524 (0.6436)	
training:	Epoch: [1][288/408]	Loss 0.5858 (0.6434)	
training:	Epoch: [1][289/408]	Loss 0.5575 (0.6431)	
training:	Epoch: [1][290/408]	Loss 0.5353 (0.6427)	
training:	Epoch: [1][291/408]	Loss 0.7584 (0.6431)	
training:	Epoch: [1][292/408]	Loss 0.6216 (0.6430)	
training:	Epoch: [1][293/408]	Loss 0.5639 (0.6428)	
training:	Epoch: [1][294/408]	Loss 0.6564 (0.6428)	
training:	Epoch: [1][295/408]	Loss 0.5412 (0.6425)	
training:	Epoch: [1][296/408]	Loss 0.4609 (0.6418)	
training:	Epoch: [1][297/408]	Loss 0.4981 (0.6414)	
training:	Epoch: [1][298/408]	Loss 0.5069 (0.6409)	
training:	Epoch: [1][299/408]	Loss 0.5718 (0.6407)	
training:	Epoch: [1][300/408]	Loss 0.6822 (0.6408)	
training:	Epoch: [1][301/408]	Loss 0.5118 (0.6404)	
training:	Epoch: [1][302/408]	Loss 0.4096 (0.6396)	
training:	Epoch: [1][303/408]	Loss 0.4562 (0.6390)	
training:	Epoch: [1][304/408]	Loss 0.5367 (0.6387)	
training:	Epoch: [1][305/408]	Loss 0.4887 (0.6382)	
training:	Epoch: [1][306/408]	Loss 0.4932 (0.6377)	
training:	Epoch: [1][307/408]	Loss 0.5115 (0.6373)	
training:	Epoch: [1][308/408]	Loss 0.5401 (0.6370)	
training:	Epoch: [1][309/408]	Loss 0.5755 (0.6368)	
training:	Epoch: [1][310/408]	Loss 0.5336 (0.6365)	
training:	Epoch: [1][311/408]	Loss 0.5116 (0.6361)	
training:	Epoch: [1][312/408]	Loss 0.7740 (0.6365)	
training:	Epoch: [1][313/408]	Loss 0.3861 (0.6357)	
training:	Epoch: [1][314/408]	Loss 0.4630 (0.6351)	
training:	Epoch: [1][315/408]	Loss 0.5377 (0.6348)	
training:	Epoch: [1][316/408]	Loss 0.5953 (0.6347)	
training:	Epoch: [1][317/408]	Loss 0.5827 (0.6345)	
training:	Epoch: [1][318/408]	Loss 0.6779 (0.6347)	
training:	Epoch: [1][319/408]	Loss 0.4561 (0.6341)	
training:	Epoch: [1][320/408]	Loss 0.5551 (0.6339)	
training:	Epoch: [1][321/408]	Loss 0.3247 (0.6329)	
training:	Epoch: [1][322/408]	Loss 0.3339 (0.6320)	
training:	Epoch: [1][323/408]	Loss 0.6530 (0.6321)	
training:	Epoch: [1][324/408]	Loss 0.6310 (0.6320)	
training:	Epoch: [1][325/408]	Loss 0.5152 (0.6317)	
training:	Epoch: [1][326/408]	Loss 0.4763 (0.6312)	
training:	Epoch: [1][327/408]	Loss 0.5936 (0.6311)	
training:	Epoch: [1][328/408]	Loss 0.3817 (0.6303)	
training:	Epoch: [1][329/408]	Loss 0.6143 (0.6303)	
training:	Epoch: [1][330/408]	Loss 0.4109 (0.6296)	
training:	Epoch: [1][331/408]	Loss 0.4582 (0.6291)	
training:	Epoch: [1][332/408]	Loss 0.4492 (0.6286)	
training:	Epoch: [1][333/408]	Loss 0.5332 (0.6283)	
training:	Epoch: [1][334/408]	Loss 0.5205 (0.6280)	
training:	Epoch: [1][335/408]	Loss 0.5958 (0.6279)	
training:	Epoch: [1][336/408]	Loss 0.6657 (0.6280)	
training:	Epoch: [1][337/408]	Loss 0.6087 (0.6279)	
training:	Epoch: [1][338/408]	Loss 0.4725 (0.6275)	
training:	Epoch: [1][339/408]	Loss 0.4350 (0.6269)	
training:	Epoch: [1][340/408]	Loss 0.4950 (0.6265)	
training:	Epoch: [1][341/408]	Loss 0.4425 (0.6260)	
training:	Epoch: [1][342/408]	Loss 0.4677 (0.6255)	
training:	Epoch: [1][343/408]	Loss 0.6396 (0.6255)	
training:	Epoch: [1][344/408]	Loss 0.5616 (0.6254)	
training:	Epoch: [1][345/408]	Loss 0.4846 (0.6249)	
training:	Epoch: [1][346/408]	Loss 0.3357 (0.6241)	
training:	Epoch: [1][347/408]	Loss 0.4160 (0.6235)	
training:	Epoch: [1][348/408]	Loss 0.2454 (0.6224)	
training:	Epoch: [1][349/408]	Loss 0.4586 (0.6220)	
training:	Epoch: [1][350/408]	Loss 0.3166 (0.6211)	
training:	Epoch: [1][351/408]	Loss 0.4734 (0.6207)	
training:	Epoch: [1][352/408]	Loss 0.3154 (0.6198)	
training:	Epoch: [1][353/408]	Loss 0.4906 (0.6194)	
training:	Epoch: [1][354/408]	Loss 0.5329 (0.6192)	
training:	Epoch: [1][355/408]	Loss 0.5443 (0.6190)	
training:	Epoch: [1][356/408]	Loss 0.3298 (0.6182)	
training:	Epoch: [1][357/408]	Loss 0.6731 (0.6183)	
training:	Epoch: [1][358/408]	Loss 0.6435 (0.6184)	
training:	Epoch: [1][359/408]	Loss 0.3565 (0.6177)	
training:	Epoch: [1][360/408]	Loss 0.4666 (0.6172)	
training:	Epoch: [1][361/408]	Loss 0.5408 (0.6170)	
training:	Epoch: [1][362/408]	Loss 0.6808 (0.6172)	
training:	Epoch: [1][363/408]	Loss 0.6856 (0.6174)	
training:	Epoch: [1][364/408]	Loss 0.6188 (0.6174)	
training:	Epoch: [1][365/408]	Loss 0.5373 (0.6172)	
training:	Epoch: [1][366/408]	Loss 0.6764 (0.6173)	
training:	Epoch: [1][367/408]	Loss 0.5723 (0.6172)	
training:	Epoch: [1][368/408]	Loss 0.4162 (0.6167)	
training:	Epoch: [1][369/408]	Loss 0.5281 (0.6164)	
training:	Epoch: [1][370/408]	Loss 0.7056 (0.6167)	
training:	Epoch: [1][371/408]	Loss 0.3141 (0.6158)	
training:	Epoch: [1][372/408]	Loss 0.4390 (0.6154)	
training:	Epoch: [1][373/408]	Loss 0.5331 (0.6152)	
training:	Epoch: [1][374/408]	Loss 0.4299 (0.6147)	
training:	Epoch: [1][375/408]	Loss 0.6071 (0.6146)	
training:	Epoch: [1][376/408]	Loss 0.7628 (0.6150)	
training:	Epoch: [1][377/408]	Loss 0.4675 (0.6146)	
training:	Epoch: [1][378/408]	Loss 0.4093 (0.6141)	
training:	Epoch: [1][379/408]	Loss 0.4350 (0.6136)	
training:	Epoch: [1][380/408]	Loss 0.6758 (0.6138)	
training:	Epoch: [1][381/408]	Loss 0.5324 (0.6136)	
training:	Epoch: [1][382/408]	Loss 0.4290 (0.6131)	
training:	Epoch: [1][383/408]	Loss 0.5917 (0.6130)	
training:	Epoch: [1][384/408]	Loss 0.3964 (0.6125)	
training:	Epoch: [1][385/408]	Loss 0.4748 (0.6121)	
training:	Epoch: [1][386/408]	Loss 0.5642 (0.6120)	
training:	Epoch: [1][387/408]	Loss 0.3809 (0.6114)	
training:	Epoch: [1][388/408]	Loss 0.5422 (0.6112)	
training:	Epoch: [1][389/408]	Loss 0.5324 (0.6110)	
training:	Epoch: [1][390/408]	Loss 0.4654 (0.6106)	
training:	Epoch: [1][391/408]	Loss 0.4220 (0.6102)	
training:	Epoch: [1][392/408]	Loss 0.5937 (0.6101)	
training:	Epoch: [1][393/408]	Loss 0.5546 (0.6100)	
training:	Epoch: [1][394/408]	Loss 0.6633 (0.6101)	
training:	Epoch: [1][395/408]	Loss 0.5393 (0.6099)	
training:	Epoch: [1][396/408]	Loss 0.4723 (0.6096)	
training:	Epoch: [1][397/408]	Loss 0.5159 (0.6093)	
training:	Epoch: [1][398/408]	Loss 0.6783 (0.6095)	
training:	Epoch: [1][399/408]	Loss 0.4159 (0.6090)	
training:	Epoch: [1][400/408]	Loss 0.7094 (0.6093)	
training:	Epoch: [1][401/408]	Loss 0.2990 (0.6085)	
training:	Epoch: [1][402/408]	Loss 0.3213 (0.6078)	
training:	Epoch: [1][403/408]	Loss 0.3634 (0.6072)	
training:	Epoch: [1][404/408]	Loss 0.5679 (0.6071)	
training:	Epoch: [1][405/408]	Loss 0.6788 (0.6073)	
training:	Epoch: [1][406/408]	Loss 0.5464 (0.6071)	
training:	Epoch: [1][407/408]	Loss 0.5544 (0.6070)	
training:	Epoch: [1][408/408]	Loss 0.3078 (0.6063)	
Training:	 Loss: 0.6053

Training:	 ACC: 0.7832 0.7830 0.7778 0.7886
Validation:	 ACC: 0.7707 0.7705 0.7656 0.7758
Validation:	 Best_BACC: 0.7707 0.7705 0.7656 0.7758
Validation:	 Loss: 0.4902
Pretraining:	Epoch 2/200
----------
training:	Epoch: [2][1/408]	Loss 0.3756 (0.3756)	
training:	Epoch: [2][2/408]	Loss 0.4907 (0.4331)	
training:	Epoch: [2][3/408]	Loss 0.5638 (0.4767)	
training:	Epoch: [2][4/408]	Loss 0.4263 (0.4641)	
training:	Epoch: [2][5/408]	Loss 0.5057 (0.4724)	
training:	Epoch: [2][6/408]	Loss 0.5429 (0.4842)	
training:	Epoch: [2][7/408]	Loss 0.6544 (0.5085)	
training:	Epoch: [2][8/408]	Loss 0.3296 (0.4861)	
training:	Epoch: [2][9/408]	Loss 0.3555 (0.4716)	
training:	Epoch: [2][10/408]	Loss 0.4511 (0.4696)	
training:	Epoch: [2][11/408]	Loss 0.4344 (0.4664)	
training:	Epoch: [2][12/408]	Loss 0.4904 (0.4684)	
training:	Epoch: [2][13/408]	Loss 0.4068 (0.4636)	
training:	Epoch: [2][14/408]	Loss 0.4411 (0.4620)	
training:	Epoch: [2][15/408]	Loss 0.4156 (0.4589)	
training:	Epoch: [2][16/408]	Loss 0.5242 (0.4630)	
training:	Epoch: [2][17/408]	Loss 0.3340 (0.4554)	
training:	Epoch: [2][18/408]	Loss 0.4861 (0.4571)	
training:	Epoch: [2][19/408]	Loss 0.6933 (0.4696)	
training:	Epoch: [2][20/408]	Loss 0.5783 (0.4750)	
training:	Epoch: [2][21/408]	Loss 0.4319 (0.4729)	
training:	Epoch: [2][22/408]	Loss 0.4727 (0.4729)	
training:	Epoch: [2][23/408]	Loss 0.4572 (0.4722)	
training:	Epoch: [2][24/408]	Loss 0.3997 (0.4692)	
training:	Epoch: [2][25/408]	Loss 0.6018 (0.4745)	
training:	Epoch: [2][26/408]	Loss 0.4829 (0.4749)	
training:	Epoch: [2][27/408]	Loss 0.5592 (0.4780)	
training:	Epoch: [2][28/408]	Loss 0.3722 (0.4742)	
training:	Epoch: [2][29/408]	Loss 0.3249 (0.4690)	
training:	Epoch: [2][30/408]	Loss 0.5579 (0.4720)	
training:	Epoch: [2][31/408]	Loss 0.3619 (0.4685)	
training:	Epoch: [2][32/408]	Loss 0.4476 (0.4678)	
training:	Epoch: [2][33/408]	Loss 0.2488 (0.4612)	
training:	Epoch: [2][34/408]	Loss 0.3537 (0.4580)	
training:	Epoch: [2][35/408]	Loss 0.4958 (0.4591)	
training:	Epoch: [2][36/408]	Loss 0.4854 (0.4598)	
training:	Epoch: [2][37/408]	Loss 0.5084 (0.4611)	
training:	Epoch: [2][38/408]	Loss 0.5034 (0.4622)	
training:	Epoch: [2][39/408]	Loss 0.6769 (0.4677)	
training:	Epoch: [2][40/408]	Loss 0.5110 (0.4688)	
training:	Epoch: [2][41/408]	Loss 0.4379 (0.4681)	
training:	Epoch: [2][42/408]	Loss 0.5832 (0.4708)	
training:	Epoch: [2][43/408]	Loss 0.5296 (0.4722)	
training:	Epoch: [2][44/408]	Loss 0.3633 (0.4697)	
training:	Epoch: [2][45/408]	Loss 0.4398 (0.4690)	
training:	Epoch: [2][46/408]	Loss 0.3981 (0.4675)	
training:	Epoch: [2][47/408]	Loss 0.5887 (0.4701)	
training:	Epoch: [2][48/408]	Loss 0.4021 (0.4687)	
training:	Epoch: [2][49/408]	Loss 0.6975 (0.4733)	
training:	Epoch: [2][50/408]	Loss 0.3808 (0.4715)	
training:	Epoch: [2][51/408]	Loss 0.4502 (0.4711)	
training:	Epoch: [2][52/408]	Loss 0.4421 (0.4705)	
training:	Epoch: [2][53/408]	Loss 0.4118 (0.4694)	
training:	Epoch: [2][54/408]	Loss 0.4675 (0.4694)	
training:	Epoch: [2][55/408]	Loss 0.2837 (0.4660)	
training:	Epoch: [2][56/408]	Loss 0.3745 (0.4644)	
training:	Epoch: [2][57/408]	Loss 0.4756 (0.4646)	
training:	Epoch: [2][58/408]	Loss 0.4375 (0.4641)	
training:	Epoch: [2][59/408]	Loss 0.6121 (0.4666)	
training:	Epoch: [2][60/408]	Loss 0.5024 (0.4672)	
training:	Epoch: [2][61/408]	Loss 0.2628 (0.4638)	
training:	Epoch: [2][62/408]	Loss 0.4719 (0.4640)	
training:	Epoch: [2][63/408]	Loss 0.2727 (0.4609)	
training:	Epoch: [2][64/408]	Loss 0.6356 (0.4637)	
training:	Epoch: [2][65/408]	Loss 0.5377 (0.4648)	
training:	Epoch: [2][66/408]	Loss 0.3374 (0.4629)	
training:	Epoch: [2][67/408]	Loss 0.4413 (0.4625)	
training:	Epoch: [2][68/408]	Loss 0.4632 (0.4626)	
training:	Epoch: [2][69/408]	Loss 0.4790 (0.4628)	
training:	Epoch: [2][70/408]	Loss 0.4148 (0.4621)	
training:	Epoch: [2][71/408]	Loss 0.6277 (0.4644)	
training:	Epoch: [2][72/408]	Loss 0.3014 (0.4622)	
training:	Epoch: [2][73/408]	Loss 0.3243 (0.4603)	
training:	Epoch: [2][74/408]	Loss 0.4242 (0.4598)	
training:	Epoch: [2][75/408]	Loss 0.5665 (0.4612)	
training:	Epoch: [2][76/408]	Loss 0.1713 (0.4574)	
training:	Epoch: [2][77/408]	Loss 0.5872 (0.4591)	
training:	Epoch: [2][78/408]	Loss 0.4460 (0.4589)	
training:	Epoch: [2][79/408]	Loss 0.3788 (0.4579)	
training:	Epoch: [2][80/408]	Loss 0.3105 (0.4561)	
training:	Epoch: [2][81/408]	Loss 0.3594 (0.4549)	
training:	Epoch: [2][82/408]	Loss 0.4266 (0.4545)	
training:	Epoch: [2][83/408]	Loss 0.4886 (0.4549)	
training:	Epoch: [2][84/408]	Loss 0.2708 (0.4528)	
training:	Epoch: [2][85/408]	Loss 0.3181 (0.4512)	
training:	Epoch: [2][86/408]	Loss 0.4809 (0.4515)	
training:	Epoch: [2][87/408]	Loss 0.5456 (0.4526)	
training:	Epoch: [2][88/408]	Loss 0.3484 (0.4514)	
training:	Epoch: [2][89/408]	Loss 0.5210 (0.4522)	
training:	Epoch: [2][90/408]	Loss 0.4206 (0.4518)	
training:	Epoch: [2][91/408]	Loss 0.4129 (0.4514)	
training:	Epoch: [2][92/408]	Loss 0.7131 (0.4543)	
training:	Epoch: [2][93/408]	Loss 0.3314 (0.4529)	
training:	Epoch: [2][94/408]	Loss 0.5191 (0.4536)	
training:	Epoch: [2][95/408]	Loss 0.4846 (0.4540)	
training:	Epoch: [2][96/408]	Loss 0.6862 (0.4564)	
training:	Epoch: [2][97/408]	Loss 0.4304 (0.4561)	
training:	Epoch: [2][98/408]	Loss 0.5691 (0.4573)	
training:	Epoch: [2][99/408]	Loss 0.4995 (0.4577)	
training:	Epoch: [2][100/408]	Loss 0.4749 (0.4579)	
training:	Epoch: [2][101/408]	Loss 0.4807 (0.4581)	
training:	Epoch: [2][102/408]	Loss 0.5437 (0.4589)	
training:	Epoch: [2][103/408]	Loss 0.6142 (0.4604)	
training:	Epoch: [2][104/408]	Loss 0.1579 (0.4575)	
training:	Epoch: [2][105/408]	Loss 0.4043 (0.4570)	
training:	Epoch: [2][106/408]	Loss 0.6852 (0.4592)	
training:	Epoch: [2][107/408]	Loss 0.7094 (0.4615)	
training:	Epoch: [2][108/408]	Loss 0.2834 (0.4599)	
training:	Epoch: [2][109/408]	Loss 0.4058 (0.4594)	
training:	Epoch: [2][110/408]	Loss 0.5210 (0.4599)	
training:	Epoch: [2][111/408]	Loss 0.1850 (0.4575)	
training:	Epoch: [2][112/408]	Loss 0.5596 (0.4584)	
training:	Epoch: [2][113/408]	Loss 0.5525 (0.4592)	
training:	Epoch: [2][114/408]	Loss 0.5773 (0.4602)	
training:	Epoch: [2][115/408]	Loss 0.2712 (0.4586)	
training:	Epoch: [2][116/408]	Loss 0.3628 (0.4578)	
training:	Epoch: [2][117/408]	Loss 0.1929 (0.4555)	
training:	Epoch: [2][118/408]	Loss 0.4630 (0.4556)	
training:	Epoch: [2][119/408]	Loss 0.5052 (0.4560)	
training:	Epoch: [2][120/408]	Loss 0.4747 (0.4561)	
training:	Epoch: [2][121/408]	Loss 0.4447 (0.4560)	
training:	Epoch: [2][122/408]	Loss 0.4812 (0.4562)	
training:	Epoch: [2][123/408]	Loss 0.3557 (0.4554)	
training:	Epoch: [2][124/408]	Loss 0.2434 (0.4537)	
training:	Epoch: [2][125/408]	Loss 0.2541 (0.4521)	
training:	Epoch: [2][126/408]	Loss 0.2742 (0.4507)	
training:	Epoch: [2][127/408]	Loss 0.4201 (0.4505)	
training:	Epoch: [2][128/408]	Loss 0.5956 (0.4516)	
training:	Epoch: [2][129/408]	Loss 0.4344 (0.4515)	
training:	Epoch: [2][130/408]	Loss 0.5822 (0.4525)	
training:	Epoch: [2][131/408]	Loss 0.3521 (0.4517)	
training:	Epoch: [2][132/408]	Loss 0.4674 (0.4518)	
training:	Epoch: [2][133/408]	Loss 0.3706 (0.4512)	
training:	Epoch: [2][134/408]	Loss 0.2697 (0.4499)	
training:	Epoch: [2][135/408]	Loss 0.3264 (0.4490)	
training:	Epoch: [2][136/408]	Loss 0.6517 (0.4504)	
training:	Epoch: [2][137/408]	Loss 0.5527 (0.4512)	
training:	Epoch: [2][138/408]	Loss 0.3014 (0.4501)	
training:	Epoch: [2][139/408]	Loss 0.4728 (0.4503)	
training:	Epoch: [2][140/408]	Loss 0.3572 (0.4496)	
training:	Epoch: [2][141/408]	Loss 0.3643 (0.4490)	
training:	Epoch: [2][142/408]	Loss 0.3162 (0.4481)	
training:	Epoch: [2][143/408]	Loss 0.5915 (0.4491)	
training:	Epoch: [2][144/408]	Loss 0.4314 (0.4489)	
training:	Epoch: [2][145/408]	Loss 0.5587 (0.4497)	
training:	Epoch: [2][146/408]	Loss 0.4986 (0.4500)	
training:	Epoch: [2][147/408]	Loss 0.4268 (0.4499)	
training:	Epoch: [2][148/408]	Loss 0.6078 (0.4509)	
training:	Epoch: [2][149/408]	Loss 0.3942 (0.4506)	
training:	Epoch: [2][150/408]	Loss 0.5778 (0.4514)	
training:	Epoch: [2][151/408]	Loss 0.3374 (0.4507)	
training:	Epoch: [2][152/408]	Loss 0.3466 (0.4500)	
training:	Epoch: [2][153/408]	Loss 0.4566 (0.4500)	
training:	Epoch: [2][154/408]	Loss 0.5757 (0.4508)	
training:	Epoch: [2][155/408]	Loss 0.2197 (0.4493)	
training:	Epoch: [2][156/408]	Loss 0.4439 (0.4493)	
training:	Epoch: [2][157/408]	Loss 0.3765 (0.4488)	
training:	Epoch: [2][158/408]	Loss 0.4054 (0.4486)	
training:	Epoch: [2][159/408]	Loss 0.5816 (0.4494)	
training:	Epoch: [2][160/408]	Loss 0.5982 (0.4503)	
training:	Epoch: [2][161/408]	Loss 0.3973 (0.4500)	
training:	Epoch: [2][162/408]	Loss 0.6290 (0.4511)	
training:	Epoch: [2][163/408]	Loss 0.3883 (0.4507)	
training:	Epoch: [2][164/408]	Loss 0.6209 (0.4518)	
training:	Epoch: [2][165/408]	Loss 0.3414 (0.4511)	
training:	Epoch: [2][166/408]	Loss 0.3576 (0.4505)	
training:	Epoch: [2][167/408]	Loss 0.3572 (0.4500)	
training:	Epoch: [2][168/408]	Loss 0.3829 (0.4496)	
training:	Epoch: [2][169/408]	Loss 0.3528 (0.4490)	
training:	Epoch: [2][170/408]	Loss 0.4520 (0.4490)	
training:	Epoch: [2][171/408]	Loss 0.3566 (0.4485)	
training:	Epoch: [2][172/408]	Loss 0.6594 (0.4497)	
training:	Epoch: [2][173/408]	Loss 0.3078 (0.4489)	
training:	Epoch: [2][174/408]	Loss 0.3400 (0.4483)	
training:	Epoch: [2][175/408]	Loss 0.3254 (0.4476)	
training:	Epoch: [2][176/408]	Loss 0.4963 (0.4478)	
training:	Epoch: [2][177/408]	Loss 0.6186 (0.4488)	
training:	Epoch: [2][178/408]	Loss 0.6204 (0.4498)	
training:	Epoch: [2][179/408]	Loss 0.3989 (0.4495)	
training:	Epoch: [2][180/408]	Loss 0.5660 (0.4501)	
training:	Epoch: [2][181/408]	Loss 0.3566 (0.4496)	
training:	Epoch: [2][182/408]	Loss 0.5969 (0.4504)	
training:	Epoch: [2][183/408]	Loss 0.6989 (0.4518)	
training:	Epoch: [2][184/408]	Loss 0.2961 (0.4509)	
training:	Epoch: [2][185/408]	Loss 0.4570 (0.4510)	
training:	Epoch: [2][186/408]	Loss 0.5592 (0.4515)	
training:	Epoch: [2][187/408]	Loss 0.3216 (0.4508)	
training:	Epoch: [2][188/408]	Loss 0.3318 (0.4502)	
training:	Epoch: [2][189/408]	Loss 0.6178 (0.4511)	
training:	Epoch: [2][190/408]	Loss 0.6717 (0.4523)	
training:	Epoch: [2][191/408]	Loss 0.5893 (0.4530)	
training:	Epoch: [2][192/408]	Loss 0.4984 (0.4532)	
training:	Epoch: [2][193/408]	Loss 0.4705 (0.4533)	
training:	Epoch: [2][194/408]	Loss 0.5854 (0.4540)	
training:	Epoch: [2][195/408]	Loss 0.3042 (0.4532)	
training:	Epoch: [2][196/408]	Loss 0.3191 (0.4525)	
training:	Epoch: [2][197/408]	Loss 0.6110 (0.4533)	
training:	Epoch: [2][198/408]	Loss 0.3130 (0.4526)	
training:	Epoch: [2][199/408]	Loss 0.3116 (0.4519)	
training:	Epoch: [2][200/408]	Loss 0.4764 (0.4520)	
training:	Epoch: [2][201/408]	Loss 0.6398 (0.4530)	
training:	Epoch: [2][202/408]	Loss 0.6632 (0.4540)	
training:	Epoch: [2][203/408]	Loss 0.4282 (0.4539)	
training:	Epoch: [2][204/408]	Loss 0.2482 (0.4529)	
training:	Epoch: [2][205/408]	Loss 0.4116 (0.4527)	
training:	Epoch: [2][206/408]	Loss 0.7200 (0.4540)	
training:	Epoch: [2][207/408]	Loss 0.3333 (0.4534)	
training:	Epoch: [2][208/408]	Loss 0.6290 (0.4542)	
training:	Epoch: [2][209/408]	Loss 0.3472 (0.4537)	
training:	Epoch: [2][210/408]	Loss 0.3717 (0.4533)	
training:	Epoch: [2][211/408]	Loss 0.5331 (0.4537)	
training:	Epoch: [2][212/408]	Loss 0.5531 (0.4542)	
training:	Epoch: [2][213/408]	Loss 0.4461 (0.4541)	
training:	Epoch: [2][214/408]	Loss 0.5853 (0.4548)	
training:	Epoch: [2][215/408]	Loss 0.5860 (0.4554)	
training:	Epoch: [2][216/408]	Loss 0.4395 (0.4553)	
training:	Epoch: [2][217/408]	Loss 0.4769 (0.4554)	
training:	Epoch: [2][218/408]	Loss 0.4441 (0.4553)	
training:	Epoch: [2][219/408]	Loss 0.4423 (0.4553)	
training:	Epoch: [2][220/408]	Loss 0.2586 (0.4544)	
training:	Epoch: [2][221/408]	Loss 0.2727 (0.4536)	
training:	Epoch: [2][222/408]	Loss 0.5283 (0.4539)	
training:	Epoch: [2][223/408]	Loss 0.5871 (0.4545)	
training:	Epoch: [2][224/408]	Loss 0.5982 (0.4551)	
training:	Epoch: [2][225/408]	Loss 0.2883 (0.4544)	
training:	Epoch: [2][226/408]	Loss 0.7633 (0.4558)	
training:	Epoch: [2][227/408]	Loss 0.3902 (0.4555)	
training:	Epoch: [2][228/408]	Loss 0.5738 (0.4560)	
training:	Epoch: [2][229/408]	Loss 0.4144 (0.4558)	
training:	Epoch: [2][230/408]	Loss 0.6827 (0.4568)	
training:	Epoch: [2][231/408]	Loss 0.6333 (0.4576)	
training:	Epoch: [2][232/408]	Loss 0.4862 (0.4577)	
training:	Epoch: [2][233/408]	Loss 0.7661 (0.4590)	
training:	Epoch: [2][234/408]	Loss 0.5746 (0.4595)	
training:	Epoch: [2][235/408]	Loss 0.4519 (0.4595)	
training:	Epoch: [2][236/408]	Loss 0.6209 (0.4602)	
training:	Epoch: [2][237/408]	Loss 0.7452 (0.4614)	
training:	Epoch: [2][238/408]	Loss 0.5447 (0.4617)	
training:	Epoch: [2][239/408]	Loss 0.3959 (0.4614)	
training:	Epoch: [2][240/408]	Loss 0.4414 (0.4614)	
training:	Epoch: [2][241/408]	Loss 0.5696 (0.4618)	
training:	Epoch: [2][242/408]	Loss 0.5180 (0.4620)	
training:	Epoch: [2][243/408]	Loss 0.4583 (0.4620)	
training:	Epoch: [2][244/408]	Loss 0.4494 (0.4620)	
training:	Epoch: [2][245/408]	Loss 0.2874 (0.4613)	
training:	Epoch: [2][246/408]	Loss 0.2840 (0.4605)	
training:	Epoch: [2][247/408]	Loss 0.3344 (0.4600)	
training:	Epoch: [2][248/408]	Loss 0.4395 (0.4599)	
training:	Epoch: [2][249/408]	Loss 0.5536 (0.4603)	
training:	Epoch: [2][250/408]	Loss 0.5033 (0.4605)	
training:	Epoch: [2][251/408]	Loss 0.5635 (0.4609)	
training:	Epoch: [2][252/408]	Loss 0.3032 (0.4603)	
training:	Epoch: [2][253/408]	Loss 0.3399 (0.4598)	
training:	Epoch: [2][254/408]	Loss 0.3771 (0.4595)	
training:	Epoch: [2][255/408]	Loss 0.2939 (0.4588)	
training:	Epoch: [2][256/408]	Loss 0.4002 (0.4586)	
training:	Epoch: [2][257/408]	Loss 0.4863 (0.4587)	
training:	Epoch: [2][258/408]	Loss 0.3567 (0.4583)	
training:	Epoch: [2][259/408]	Loss 0.4841 (0.4584)	
training:	Epoch: [2][260/408]	Loss 0.5813 (0.4589)	
training:	Epoch: [2][261/408]	Loss 0.4644 (0.4589)	
training:	Epoch: [2][262/408]	Loss 0.4255 (0.4588)	
training:	Epoch: [2][263/408]	Loss 0.3259 (0.4583)	
training:	Epoch: [2][264/408]	Loss 0.4932 (0.4584)	
training:	Epoch: [2][265/408]	Loss 0.5937 (0.4589)	
training:	Epoch: [2][266/408]	Loss 0.4408 (0.4588)	
training:	Epoch: [2][267/408]	Loss 0.4500 (0.4588)	
training:	Epoch: [2][268/408]	Loss 0.4404 (0.4587)	
training:	Epoch: [2][269/408]	Loss 0.2910 (0.4581)	
training:	Epoch: [2][270/408]	Loss 0.4555 (0.4581)	
training:	Epoch: [2][271/408]	Loss 0.3032 (0.4575)	
training:	Epoch: [2][272/408]	Loss 0.3248 (0.4570)	
training:	Epoch: [2][273/408]	Loss 0.6647 (0.4578)	
training:	Epoch: [2][274/408]	Loss 0.2936 (0.4572)	
training:	Epoch: [2][275/408]	Loss 0.4330 (0.4571)	
training:	Epoch: [2][276/408]	Loss 0.3004 (0.4565)	
training:	Epoch: [2][277/408]	Loss 0.4165 (0.4564)	
training:	Epoch: [2][278/408]	Loss 0.3850 (0.4561)	
training:	Epoch: [2][279/408]	Loss 0.4813 (0.4562)	
training:	Epoch: [2][280/408]	Loss 0.3983 (0.4560)	
training:	Epoch: [2][281/408]	Loss 0.3822 (0.4558)	
training:	Epoch: [2][282/408]	Loss 0.2152 (0.4549)	
training:	Epoch: [2][283/408]	Loss 0.2896 (0.4543)	
training:	Epoch: [2][284/408]	Loss 0.5730 (0.4547)	
training:	Epoch: [2][285/408]	Loss 0.4492 (0.4547)	
training:	Epoch: [2][286/408]	Loss 0.4418 (0.4547)	
training:	Epoch: [2][287/408]	Loss 0.3290 (0.4542)	
training:	Epoch: [2][288/408]	Loss 0.5298 (0.4545)	
training:	Epoch: [2][289/408]	Loss 0.4313 (0.4544)	
training:	Epoch: [2][290/408]	Loss 0.4321 (0.4544)	
training:	Epoch: [2][291/408]	Loss 0.6602 (0.4551)	
training:	Epoch: [2][292/408]	Loss 0.3624 (0.4547)	
training:	Epoch: [2][293/408]	Loss 0.2411 (0.4540)	
training:	Epoch: [2][294/408]	Loss 0.6251 (0.4546)	
training:	Epoch: [2][295/408]	Loss 0.5666 (0.4550)	
training:	Epoch: [2][296/408]	Loss 0.3941 (0.4548)	
training:	Epoch: [2][297/408]	Loss 0.6244 (0.4553)	
training:	Epoch: [2][298/408]	Loss 0.4129 (0.4552)	
training:	Epoch: [2][299/408]	Loss 0.5267 (0.4554)	
training:	Epoch: [2][300/408]	Loss 0.5547 (0.4558)	
training:	Epoch: [2][301/408]	Loss 0.2897 (0.4552)	
training:	Epoch: [2][302/408]	Loss 0.4031 (0.4550)	
training:	Epoch: [2][303/408]	Loss 0.3586 (0.4547)	
training:	Epoch: [2][304/408]	Loss 0.3891 (0.4545)	
training:	Epoch: [2][305/408]	Loss 0.3456 (0.4542)	
training:	Epoch: [2][306/408]	Loss 0.3031 (0.4537)	
training:	Epoch: [2][307/408]	Loss 0.4754 (0.4537)	
training:	Epoch: [2][308/408]	Loss 0.2430 (0.4530)	
training:	Epoch: [2][309/408]	Loss 0.2808 (0.4525)	
training:	Epoch: [2][310/408]	Loss 0.3633 (0.4522)	
training:	Epoch: [2][311/408]	Loss 0.2872 (0.4517)	
training:	Epoch: [2][312/408]	Loss 0.2481 (0.4510)	
training:	Epoch: [2][313/408]	Loss 0.4112 (0.4509)	
training:	Epoch: [2][314/408]	Loss 0.3472 (0.4506)	
training:	Epoch: [2][315/408]	Loss 0.4068 (0.4504)	
training:	Epoch: [2][316/408]	Loss 0.3272 (0.4500)	
training:	Epoch: [2][317/408]	Loss 0.3325 (0.4497)	
training:	Epoch: [2][318/408]	Loss 0.3344 (0.4493)	
training:	Epoch: [2][319/408]	Loss 0.5852 (0.4497)	
training:	Epoch: [2][320/408]	Loss 0.2507 (0.4491)	
training:	Epoch: [2][321/408]	Loss 0.2796 (0.4486)	
training:	Epoch: [2][322/408]	Loss 0.3157 (0.4482)	
training:	Epoch: [2][323/408]	Loss 0.4750 (0.4482)	
training:	Epoch: [2][324/408]	Loss 0.4763 (0.4483)	
training:	Epoch: [2][325/408]	Loss 0.4686 (0.4484)	
training:	Epoch: [2][326/408]	Loss 0.3585 (0.4481)	
training:	Epoch: [2][327/408]	Loss 0.3632 (0.4479)	
training:	Epoch: [2][328/408]	Loss 0.3224 (0.4475)	
training:	Epoch: [2][329/408]	Loss 0.3047 (0.4470)	
training:	Epoch: [2][330/408]	Loss 0.3179 (0.4466)	
training:	Epoch: [2][331/408]	Loss 0.3838 (0.4465)	
training:	Epoch: [2][332/408]	Loss 0.4106 (0.4464)	
training:	Epoch: [2][333/408]	Loss 0.4858 (0.4465)	
training:	Epoch: [2][334/408]	Loss 0.3584 (0.4462)	
training:	Epoch: [2][335/408]	Loss 0.3068 (0.4458)	
training:	Epoch: [2][336/408]	Loss 0.2340 (0.4452)	
training:	Epoch: [2][337/408]	Loss 0.3022 (0.4447)	
training:	Epoch: [2][338/408]	Loss 0.2608 (0.4442)	
training:	Epoch: [2][339/408]	Loss 0.4968 (0.4443)	
training:	Epoch: [2][340/408]	Loss 0.5589 (0.4447)	
training:	Epoch: [2][341/408]	Loss 0.3200 (0.4443)	
training:	Epoch: [2][342/408]	Loss 0.6972 (0.4451)	
training:	Epoch: [2][343/408]	Loss 0.4742 (0.4451)	
training:	Epoch: [2][344/408]	Loss 0.6414 (0.4457)	
training:	Epoch: [2][345/408]	Loss 0.4274 (0.4457)	
training:	Epoch: [2][346/408]	Loss 0.6309 (0.4462)	
training:	Epoch: [2][347/408]	Loss 0.6363 (0.4467)	
training:	Epoch: [2][348/408]	Loss 0.3251 (0.4464)	
training:	Epoch: [2][349/408]	Loss 0.4468 (0.4464)	
training:	Epoch: [2][350/408]	Loss 0.3708 (0.4462)	
training:	Epoch: [2][351/408]	Loss 0.2161 (0.4455)	
training:	Epoch: [2][352/408]	Loss 0.4273 (0.4455)	
training:	Epoch: [2][353/408]	Loss 0.3016 (0.4451)	
training:	Epoch: [2][354/408]	Loss 0.2332 (0.4445)	
training:	Epoch: [2][355/408]	Loss 0.4248 (0.4444)	
training:	Epoch: [2][356/408]	Loss 0.3509 (0.4441)	
training:	Epoch: [2][357/408]	Loss 0.3045 (0.4438)	
training:	Epoch: [2][358/408]	Loss 0.1817 (0.4430)	
training:	Epoch: [2][359/408]	Loss 0.3071 (0.4426)	
training:	Epoch: [2][360/408]	Loss 0.3255 (0.4423)	
training:	Epoch: [2][361/408]	Loss 0.4379 (0.4423)	
training:	Epoch: [2][362/408]	Loss 0.4007 (0.4422)	
training:	Epoch: [2][363/408]	Loss 0.2930 (0.4418)	
training:	Epoch: [2][364/408]	Loss 0.2427 (0.4412)	
training:	Epoch: [2][365/408]	Loss 0.2851 (0.4408)	
training:	Epoch: [2][366/408]	Loss 0.3418 (0.4405)	
training:	Epoch: [2][367/408]	Loss 0.3050 (0.4402)	
training:	Epoch: [2][368/408]	Loss 0.5696 (0.4405)	
training:	Epoch: [2][369/408]	Loss 0.4249 (0.4405)	
training:	Epoch: [2][370/408]	Loss 0.3691 (0.4403)	
training:	Epoch: [2][371/408]	Loss 0.5597 (0.4406)	
training:	Epoch: [2][372/408]	Loss 0.3173 (0.4403)	
training:	Epoch: [2][373/408]	Loss 0.3469 (0.4400)	
training:	Epoch: [2][374/408]	Loss 0.5807 (0.4404)	
training:	Epoch: [2][375/408]	Loss 0.4190 (0.4403)	
training:	Epoch: [2][376/408]	Loss 0.4669 (0.4404)	
training:	Epoch: [2][377/408]	Loss 0.4227 (0.4404)	
training:	Epoch: [2][378/408]	Loss 0.4728 (0.4405)	
training:	Epoch: [2][379/408]	Loss 0.6453 (0.4410)	
training:	Epoch: [2][380/408]	Loss 0.2590 (0.4405)	
training:	Epoch: [2][381/408]	Loss 0.3858 (0.4404)	
training:	Epoch: [2][382/408]	Loss 0.4310 (0.4403)	
training:	Epoch: [2][383/408]	Loss 0.4894 (0.4405)	
training:	Epoch: [2][384/408]	Loss 0.3928 (0.4404)	
training:	Epoch: [2][385/408]	Loss 0.2491 (0.4399)	
training:	Epoch: [2][386/408]	Loss 0.4693 (0.4399)	
training:	Epoch: [2][387/408]	Loss 0.4398 (0.4399)	
training:	Epoch: [2][388/408]	Loss 0.5289 (0.4402)	
training:	Epoch: [2][389/408]	Loss 0.4662 (0.4402)	
training:	Epoch: [2][390/408]	Loss 0.7347 (0.4410)	
training:	Epoch: [2][391/408]	Loss 0.7800 (0.4418)	
training:	Epoch: [2][392/408]	Loss 0.3614 (0.4416)	
training:	Epoch: [2][393/408]	Loss 0.4245 (0.4416)	
training:	Epoch: [2][394/408]	Loss 0.5576 (0.4419)	
training:	Epoch: [2][395/408]	Loss 0.3402 (0.4416)	
training:	Epoch: [2][396/408]	Loss 0.3322 (0.4414)	
training:	Epoch: [2][397/408]	Loss 0.6159 (0.4418)	
training:	Epoch: [2][398/408]	Loss 0.3513 (0.4416)	
training:	Epoch: [2][399/408]	Loss 0.3693 (0.4414)	
training:	Epoch: [2][400/408]	Loss 0.4252 (0.4414)	
training:	Epoch: [2][401/408]	Loss 0.8459 (0.4424)	
training:	Epoch: [2][402/408]	Loss 0.3196 (0.4421)	
training:	Epoch: [2][403/408]	Loss 0.4829 (0.4422)	
training:	Epoch: [2][404/408]	Loss 0.5263 (0.4424)	
training:	Epoch: [2][405/408]	Loss 0.2941 (0.4420)	
training:	Epoch: [2][406/408]	Loss 0.4482 (0.4420)	
training:	Epoch: [2][407/408]	Loss 0.2164 (0.4415)	
training:	Epoch: [2][408/408]	Loss 0.3482 (0.4412)	
Training:	 Loss: 0.4406

Training:	 ACC: 0.8362 0.8373 0.8621 0.8103
Validation:	 ACC: 0.8069 0.8085 0.8414 0.7724
Validation:	 Best_BACC: 0.8069 0.8085 0.8414 0.7724
Validation:	 Loss: 0.4271
Pretraining:	Epoch 3/200
----------
training:	Epoch: [3][1/408]	Loss 0.1888 (0.1888)	
training:	Epoch: [3][2/408]	Loss 0.2741 (0.2315)	
training:	Epoch: [3][3/408]	Loss 0.3158 (0.2596)	
training:	Epoch: [3][4/408]	Loss 0.3438 (0.2806)	
training:	Epoch: [3][5/408]	Loss 0.4632 (0.3171)	
training:	Epoch: [3][6/408]	Loss 0.5633 (0.3582)	
training:	Epoch: [3][7/408]	Loss 0.2283 (0.3396)	
training:	Epoch: [3][8/408]	Loss 0.4638 (0.3551)	
training:	Epoch: [3][9/408]	Loss 0.1991 (0.3378)	
training:	Epoch: [3][10/408]	Loss 0.3226 (0.3363)	
training:	Epoch: [3][11/408]	Loss 0.4285 (0.3447)	
training:	Epoch: [3][12/408]	Loss 0.4432 (0.3529)	
training:	Epoch: [3][13/408]	Loss 0.4283 (0.3587)	
training:	Epoch: [3][14/408]	Loss 0.4261 (0.3635)	
training:	Epoch: [3][15/408]	Loss 0.5538 (0.3762)	
training:	Epoch: [3][16/408]	Loss 0.3434 (0.3741)	
training:	Epoch: [3][17/408]	Loss 0.1658 (0.3619)	
training:	Epoch: [3][18/408]	Loss 0.2729 (0.3569)	
training:	Epoch: [3][19/408]	Loss 0.2605 (0.3519)	
training:	Epoch: [3][20/408]	Loss 0.2694 (0.3477)	
training:	Epoch: [3][21/408]	Loss 0.3599 (0.3483)	
training:	Epoch: [3][22/408]	Loss 0.3335 (0.3476)	
training:	Epoch: [3][23/408]	Loss 0.4772 (0.3533)	
training:	Epoch: [3][24/408]	Loss 0.1858 (0.3463)	
training:	Epoch: [3][25/408]	Loss 0.2385 (0.3420)	
training:	Epoch: [3][26/408]	Loss 0.3034 (0.3405)	
training:	Epoch: [3][27/408]	Loss 0.2688 (0.3378)	
training:	Epoch: [3][28/408]	Loss 0.3477 (0.3382)	
training:	Epoch: [3][29/408]	Loss 0.2121 (0.3338)	
training:	Epoch: [3][30/408]	Loss 0.1586 (0.3280)	
training:	Epoch: [3][31/408]	Loss 0.4519 (0.3320)	
training:	Epoch: [3][32/408]	Loss 0.3323 (0.3320)	
training:	Epoch: [3][33/408]	Loss 0.4542 (0.3357)	
training:	Epoch: [3][34/408]	Loss 0.2361 (0.3328)	
training:	Epoch: [3][35/408]	Loss 0.5625 (0.3393)	
training:	Epoch: [3][36/408]	Loss 0.4742 (0.3431)	
training:	Epoch: [3][37/408]	Loss 0.3382 (0.3430)	
training:	Epoch: [3][38/408]	Loss 0.5010 (0.3471)	
training:	Epoch: [3][39/408]	Loss 0.3712 (0.3477)	
training:	Epoch: [3][40/408]	Loss 0.2871 (0.3462)	
training:	Epoch: [3][41/408]	Loss 0.3352 (0.3460)	
training:	Epoch: [3][42/408]	Loss 0.2924 (0.3447)	
training:	Epoch: [3][43/408]	Loss 0.2929 (0.3435)	
training:	Epoch: [3][44/408]	Loss 0.5152 (0.3474)	
training:	Epoch: [3][45/408]	Loss 0.3596 (0.3476)	
training:	Epoch: [3][46/408]	Loss 0.1623 (0.3436)	
training:	Epoch: [3][47/408]	Loss 0.2844 (0.3424)	
training:	Epoch: [3][48/408]	Loss 0.5244 (0.3462)	
training:	Epoch: [3][49/408]	Loss 0.4537 (0.3483)	
training:	Epoch: [3][50/408]	Loss 0.1909 (0.3452)	
training:	Epoch: [3][51/408]	Loss 0.5254 (0.3487)	
training:	Epoch: [3][52/408]	Loss 0.5309 (0.3522)	
training:	Epoch: [3][53/408]	Loss 0.5740 (0.3564)	
training:	Epoch: [3][54/408]	Loss 0.3497 (0.3563)	
training:	Epoch: [3][55/408]	Loss 0.3528 (0.3562)	
training:	Epoch: [3][56/408]	Loss 0.6665 (0.3618)	
training:	Epoch: [3][57/408]	Loss 0.5802 (0.3656)	
training:	Epoch: [3][58/408]	Loss 0.4214 (0.3666)	
training:	Epoch: [3][59/408]	Loss 0.4815 (0.3685)	
training:	Epoch: [3][60/408]	Loss 0.2628 (0.3668)	
training:	Epoch: [3][61/408]	Loss 0.4095 (0.3675)	
training:	Epoch: [3][62/408]	Loss 0.2963 (0.3663)	
training:	Epoch: [3][63/408]	Loss 0.4632 (0.3678)	
training:	Epoch: [3][64/408]	Loss 0.3074 (0.3669)	
training:	Epoch: [3][65/408]	Loss 0.3133 (0.3661)	
training:	Epoch: [3][66/408]	Loss 0.3620 (0.3660)	
training:	Epoch: [3][67/408]	Loss 0.6840 (0.3708)	
training:	Epoch: [3][68/408]	Loss 0.3717 (0.3708)	
training:	Epoch: [3][69/408]	Loss 0.3149 (0.3700)	
training:	Epoch: [3][70/408]	Loss 0.3173 (0.3692)	
training:	Epoch: [3][71/408]	Loss 0.3970 (0.3696)	
training:	Epoch: [3][72/408]	Loss 0.2935 (0.3685)	
training:	Epoch: [3][73/408]	Loss 0.3467 (0.3682)	
training:	Epoch: [3][74/408]	Loss 0.4474 (0.3693)	
training:	Epoch: [3][75/408]	Loss 0.2272 (0.3674)	
training:	Epoch: [3][76/408]	Loss 0.2990 (0.3665)	
training:	Epoch: [3][77/408]	Loss 0.2766 (0.3654)	
training:	Epoch: [3][78/408]	Loss 0.4279 (0.3662)	
training:	Epoch: [3][79/408]	Loss 0.5773 (0.3688)	
training:	Epoch: [3][80/408]	Loss 0.4589 (0.3700)	
training:	Epoch: [3][81/408]	Loss 0.5143 (0.3717)	
training:	Epoch: [3][82/408]	Loss 0.3151 (0.3710)	
training:	Epoch: [3][83/408]	Loss 0.3294 (0.3705)	
training:	Epoch: [3][84/408]	Loss 0.3851 (0.3707)	
training:	Epoch: [3][85/408]	Loss 0.5137 (0.3724)	
training:	Epoch: [3][86/408]	Loss 0.3553 (0.3722)	
training:	Epoch: [3][87/408]	Loss 0.3101 (0.3715)	
training:	Epoch: [3][88/408]	Loss 0.3164 (0.3709)	
training:	Epoch: [3][89/408]	Loss 0.4310 (0.3715)	
training:	Epoch: [3][90/408]	Loss 0.3403 (0.3712)	
training:	Epoch: [3][91/408]	Loss 0.3723 (0.3712)	
training:	Epoch: [3][92/408]	Loss 0.4687 (0.3723)	
training:	Epoch: [3][93/408]	Loss 0.4313 (0.3729)	
training:	Epoch: [3][94/408]	Loss 0.3480 (0.3726)	
training:	Epoch: [3][95/408]	Loss 0.3095 (0.3720)	
training:	Epoch: [3][96/408]	Loss 0.2870 (0.3711)	
training:	Epoch: [3][97/408]	Loss 0.5066 (0.3725)	
training:	Epoch: [3][98/408]	Loss 0.5056 (0.3738)	
training:	Epoch: [3][99/408]	Loss 0.4057 (0.3742)	
training:	Epoch: [3][100/408]	Loss 0.3577 (0.3740)	
training:	Epoch: [3][101/408]	Loss 0.3400 (0.3737)	
training:	Epoch: [3][102/408]	Loss 0.4067 (0.3740)	
training:	Epoch: [3][103/408]	Loss 0.3796 (0.3740)	
training:	Epoch: [3][104/408]	Loss 0.4585 (0.3748)	
training:	Epoch: [3][105/408]	Loss 0.2642 (0.3738)	
training:	Epoch: [3][106/408]	Loss 0.1896 (0.3721)	
training:	Epoch: [3][107/408]	Loss 0.2997 (0.3714)	
training:	Epoch: [3][108/408]	Loss 0.3484 (0.3712)	
training:	Epoch: [3][109/408]	Loss 0.3990 (0.3714)	
training:	Epoch: [3][110/408]	Loss 0.3931 (0.3716)	
training:	Epoch: [3][111/408]	Loss 0.2216 (0.3703)	
training:	Epoch: [3][112/408]	Loss 0.5341 (0.3717)	
training:	Epoch: [3][113/408]	Loss 0.5241 (0.3731)	
training:	Epoch: [3][114/408]	Loss 0.3639 (0.3730)	
training:	Epoch: [3][115/408]	Loss 0.2489 (0.3719)	
training:	Epoch: [3][116/408]	Loss 0.3998 (0.3722)	
training:	Epoch: [3][117/408]	Loss 0.3570 (0.3720)	
training:	Epoch: [3][118/408]	Loss 0.1828 (0.3704)	
training:	Epoch: [3][119/408]	Loss 0.1702 (0.3687)	
training:	Epoch: [3][120/408]	Loss 0.3791 (0.3688)	
training:	Epoch: [3][121/408]	Loss 0.4397 (0.3694)	
training:	Epoch: [3][122/408]	Loss 0.4439 (0.3700)	
training:	Epoch: [3][123/408]	Loss 0.5366 (0.3714)	
training:	Epoch: [3][124/408]	Loss 0.2495 (0.3704)	
training:	Epoch: [3][125/408]	Loss 0.4468 (0.3710)	
training:	Epoch: [3][126/408]	Loss 0.5573 (0.3725)	
training:	Epoch: [3][127/408]	Loss 0.2461 (0.3715)	
training:	Epoch: [3][128/408]	Loss 0.5538 (0.3729)	
training:	Epoch: [3][129/408]	Loss 0.2845 (0.3722)	
training:	Epoch: [3][130/408]	Loss 0.1929 (0.3709)	
training:	Epoch: [3][131/408]	Loss 0.3892 (0.3710)	
training:	Epoch: [3][132/408]	Loss 0.4209 (0.3714)	
training:	Epoch: [3][133/408]	Loss 0.2545 (0.3705)	
training:	Epoch: [3][134/408]	Loss 0.3366 (0.3702)	
training:	Epoch: [3][135/408]	Loss 0.2960 (0.3697)	
training:	Epoch: [3][136/408]	Loss 0.3409 (0.3695)	
training:	Epoch: [3][137/408]	Loss 0.4601 (0.3701)	
training:	Epoch: [3][138/408]	Loss 0.2799 (0.3695)	
training:	Epoch: [3][139/408]	Loss 0.3645 (0.3694)	
training:	Epoch: [3][140/408]	Loss 0.3207 (0.3691)	
training:	Epoch: [3][141/408]	Loss 0.6331 (0.3710)	
training:	Epoch: [3][142/408]	Loss 0.3411 (0.3708)	
training:	Epoch: [3][143/408]	Loss 0.6764 (0.3729)	
training:	Epoch: [3][144/408]	Loss 0.5857 (0.3744)	
training:	Epoch: [3][145/408]	Loss 0.2704 (0.3737)	
training:	Epoch: [3][146/408]	Loss 0.2788 (0.3730)	
training:	Epoch: [3][147/408]	Loss 0.6295 (0.3748)	
training:	Epoch: [3][148/408]	Loss 0.3811 (0.3748)	
training:	Epoch: [3][149/408]	Loss 0.3370 (0.3745)	
training:	Epoch: [3][150/408]	Loss 0.3510 (0.3744)	
training:	Epoch: [3][151/408]	Loss 0.2358 (0.3735)	
training:	Epoch: [3][152/408]	Loss 0.5137 (0.3744)	
training:	Epoch: [3][153/408]	Loss 0.4867 (0.3751)	
training:	Epoch: [3][154/408]	Loss 0.3515 (0.3750)	
training:	Epoch: [3][155/408]	Loss 0.5964 (0.3764)	
training:	Epoch: [3][156/408]	Loss 0.4766 (0.3770)	
training:	Epoch: [3][157/408]	Loss 0.3592 (0.3769)	
training:	Epoch: [3][158/408]	Loss 0.4606 (0.3775)	
training:	Epoch: [3][159/408]	Loss 0.3596 (0.3773)	
training:	Epoch: [3][160/408]	Loss 0.4606 (0.3779)	
training:	Epoch: [3][161/408]	Loss 0.5487 (0.3789)	
training:	Epoch: [3][162/408]	Loss 0.3945 (0.3790)	
training:	Epoch: [3][163/408]	Loss 0.4326 (0.3794)	
training:	Epoch: [3][164/408]	Loss 0.2055 (0.3783)	
training:	Epoch: [3][165/408]	Loss 0.2337 (0.3774)	
training:	Epoch: [3][166/408]	Loss 0.3573 (0.3773)	
training:	Epoch: [3][167/408]	Loss 0.3712 (0.3773)	
training:	Epoch: [3][168/408]	Loss 0.3205 (0.3769)	
training:	Epoch: [3][169/408]	Loss 0.4425 (0.3773)	
training:	Epoch: [3][170/408]	Loss 0.3165 (0.3770)	
training:	Epoch: [3][171/408]	Loss 0.3844 (0.3770)	
training:	Epoch: [3][172/408]	Loss 0.5895 (0.3782)	
training:	Epoch: [3][173/408]	Loss 0.2527 (0.3775)	
training:	Epoch: [3][174/408]	Loss 0.3649 (0.3774)	
training:	Epoch: [3][175/408]	Loss 0.4042 (0.3776)	
training:	Epoch: [3][176/408]	Loss 0.6149 (0.3789)	
training:	Epoch: [3][177/408]	Loss 0.5934 (0.3801)	
training:	Epoch: [3][178/408]	Loss 0.4852 (0.3807)	
training:	Epoch: [3][179/408]	Loss 0.2573 (0.3800)	
training:	Epoch: [3][180/408]	Loss 0.4723 (0.3806)	
training:	Epoch: [3][181/408]	Loss 0.6769 (0.3822)	
training:	Epoch: [3][182/408]	Loss 0.5072 (0.3829)	
training:	Epoch: [3][183/408]	Loss 0.2049 (0.3819)	
training:	Epoch: [3][184/408]	Loss 0.2416 (0.3811)	
training:	Epoch: [3][185/408]	Loss 0.1757 (0.3800)	
training:	Epoch: [3][186/408]	Loss 0.3182 (0.3797)	
training:	Epoch: [3][187/408]	Loss 0.3141 (0.3794)	
training:	Epoch: [3][188/408]	Loss 0.2855 (0.3789)	
training:	Epoch: [3][189/408]	Loss 0.4748 (0.3794)	
training:	Epoch: [3][190/408]	Loss 0.5252 (0.3801)	
training:	Epoch: [3][191/408]	Loss 0.3329 (0.3799)	
training:	Epoch: [3][192/408]	Loss 0.7214 (0.3817)	
training:	Epoch: [3][193/408]	Loss 0.1957 (0.3807)	
training:	Epoch: [3][194/408]	Loss 0.4135 (0.3809)	
training:	Epoch: [3][195/408]	Loss 0.6271 (0.3821)	
training:	Epoch: [3][196/408]	Loss 0.4867 (0.3827)	
training:	Epoch: [3][197/408]	Loss 0.4382 (0.3829)	
training:	Epoch: [3][198/408]	Loss 0.3584 (0.3828)	
training:	Epoch: [3][199/408]	Loss 0.3205 (0.3825)	
training:	Epoch: [3][200/408]	Loss 0.3934 (0.3826)	
training:	Epoch: [3][201/408]	Loss 0.2400 (0.3819)	
training:	Epoch: [3][202/408]	Loss 0.4982 (0.3824)	
training:	Epoch: [3][203/408]	Loss 0.3502 (0.3823)	
training:	Epoch: [3][204/408]	Loss 0.1832 (0.3813)	
training:	Epoch: [3][205/408]	Loss 0.5385 (0.3821)	
training:	Epoch: [3][206/408]	Loss 0.3607 (0.3820)	
training:	Epoch: [3][207/408]	Loss 0.2899 (0.3815)	
training:	Epoch: [3][208/408]	Loss 0.2571 (0.3809)	
training:	Epoch: [3][209/408]	Loss 0.5673 (0.3818)	
training:	Epoch: [3][210/408]	Loss 0.3419 (0.3816)	
training:	Epoch: [3][211/408]	Loss 0.7245 (0.3832)	
training:	Epoch: [3][212/408]	Loss 0.2151 (0.3824)	
training:	Epoch: [3][213/408]	Loss 0.4695 (0.3829)	
training:	Epoch: [3][214/408]	Loss 0.5288 (0.3835)	
training:	Epoch: [3][215/408]	Loss 0.3334 (0.3833)	
training:	Epoch: [3][216/408]	Loss 0.5171 (0.3839)	
training:	Epoch: [3][217/408]	Loss 0.4575 (0.3843)	
training:	Epoch: [3][218/408]	Loss 0.4873 (0.3847)	
training:	Epoch: [3][219/408]	Loss 0.4409 (0.3850)	
training:	Epoch: [3][220/408]	Loss 0.2688 (0.3845)	
training:	Epoch: [3][221/408]	Loss 0.5083 (0.3850)	
training:	Epoch: [3][222/408]	Loss 0.3149 (0.3847)	
training:	Epoch: [3][223/408]	Loss 0.4448 (0.3850)	
training:	Epoch: [3][224/408]	Loss 0.4809 (0.3854)	
training:	Epoch: [3][225/408]	Loss 0.4442 (0.3857)	
training:	Epoch: [3][226/408]	Loss 0.6092 (0.3867)	
training:	Epoch: [3][227/408]	Loss 0.2426 (0.3860)	
training:	Epoch: [3][228/408]	Loss 0.3276 (0.3858)	
training:	Epoch: [3][229/408]	Loss 0.3766 (0.3857)	
training:	Epoch: [3][230/408]	Loss 0.2548 (0.3852)	
training:	Epoch: [3][231/408]	Loss 0.3638 (0.3851)	
training:	Epoch: [3][232/408]	Loss 0.3335 (0.3848)	
training:	Epoch: [3][233/408]	Loss 0.1537 (0.3839)	
training:	Epoch: [3][234/408]	Loss 0.1639 (0.3829)	
training:	Epoch: [3][235/408]	Loss 0.4239 (0.3831)	
training:	Epoch: [3][236/408]	Loss 0.5719 (0.3839)	
training:	Epoch: [3][237/408]	Loss 0.2214 (0.3832)	
training:	Epoch: [3][238/408]	Loss 0.5291 (0.3838)	
training:	Epoch: [3][239/408]	Loss 0.2802 (0.3834)	
training:	Epoch: [3][240/408]	Loss 0.3459 (0.3832)	
training:	Epoch: [3][241/408]	Loss 0.4744 (0.3836)	
training:	Epoch: [3][242/408]	Loss 0.4900 (0.3840)	
training:	Epoch: [3][243/408]	Loss 0.1828 (0.3832)	
training:	Epoch: [3][244/408]	Loss 0.3819 (0.3832)	
training:	Epoch: [3][245/408]	Loss 0.2824 (0.3828)	
training:	Epoch: [3][246/408]	Loss 0.3570 (0.3827)	
training:	Epoch: [3][247/408]	Loss 0.4663 (0.3830)	
training:	Epoch: [3][248/408]	Loss 0.3798 (0.3830)	
training:	Epoch: [3][249/408]	Loss 0.2691 (0.3826)	
training:	Epoch: [3][250/408]	Loss 0.3508 (0.3824)	
training:	Epoch: [3][251/408]	Loss 0.3161 (0.3822)	
training:	Epoch: [3][252/408]	Loss 0.4136 (0.3823)	
training:	Epoch: [3][253/408]	Loss 0.5346 (0.3829)	
training:	Epoch: [3][254/408]	Loss 0.3611 (0.3828)	
training:	Epoch: [3][255/408]	Loss 0.5476 (0.3835)	
training:	Epoch: [3][256/408]	Loss 0.2629 (0.3830)	
training:	Epoch: [3][257/408]	Loss 0.4041 (0.3831)	
training:	Epoch: [3][258/408]	Loss 0.2556 (0.3826)	
training:	Epoch: [3][259/408]	Loss 0.3799 (0.3826)	
training:	Epoch: [3][260/408]	Loss 0.2017 (0.3819)	
training:	Epoch: [3][261/408]	Loss 0.3119 (0.3816)	
training:	Epoch: [3][262/408]	Loss 0.3408 (0.3814)	
training:	Epoch: [3][263/408]	Loss 0.2899 (0.3811)	
training:	Epoch: [3][264/408]	Loss 0.2616 (0.3806)	
training:	Epoch: [3][265/408]	Loss 0.7156 (0.3819)	
training:	Epoch: [3][266/408]	Loss 0.6702 (0.3830)	
training:	Epoch: [3][267/408]	Loss 0.3382 (0.3828)	
training:	Epoch: [3][268/408]	Loss 0.3443 (0.3827)	
training:	Epoch: [3][269/408]	Loss 0.4397 (0.3829)	
training:	Epoch: [3][270/408]	Loss 0.1729 (0.3821)	
training:	Epoch: [3][271/408]	Loss 0.2206 (0.3815)	
training:	Epoch: [3][272/408]	Loss 0.3454 (0.3814)	
training:	Epoch: [3][273/408]	Loss 0.3003 (0.3811)	
training:	Epoch: [3][274/408]	Loss 0.3575 (0.3810)	
training:	Epoch: [3][275/408]	Loss 0.3978 (0.3811)	
training:	Epoch: [3][276/408]	Loss 0.1767 (0.3803)	
training:	Epoch: [3][277/408]	Loss 0.3208 (0.3801)	
training:	Epoch: [3][278/408]	Loss 0.4306 (0.3803)	
training:	Epoch: [3][279/408]	Loss 0.5763 (0.3810)	
training:	Epoch: [3][280/408]	Loss 0.4512 (0.3812)	
training:	Epoch: [3][281/408]	Loss 0.4033 (0.3813)	
training:	Epoch: [3][282/408]	Loss 0.2417 (0.3808)	
training:	Epoch: [3][283/408]	Loss 0.3680 (0.3808)	
training:	Epoch: [3][284/408]	Loss 0.5280 (0.3813)	
training:	Epoch: [3][285/408]	Loss 0.3518 (0.3812)	
training:	Epoch: [3][286/408]	Loss 0.4622 (0.3815)	
training:	Epoch: [3][287/408]	Loss 0.2883 (0.3812)	
training:	Epoch: [3][288/408]	Loss 0.3496 (0.3810)	
training:	Epoch: [3][289/408]	Loss 0.4784 (0.3814)	
training:	Epoch: [3][290/408]	Loss 0.3688 (0.3813)	
training:	Epoch: [3][291/408]	Loss 0.3060 (0.3811)	
training:	Epoch: [3][292/408]	Loss 0.1692 (0.3804)	
training:	Epoch: [3][293/408]	Loss 0.3711 (0.3803)	
training:	Epoch: [3][294/408]	Loss 0.1990 (0.3797)	
training:	Epoch: [3][295/408]	Loss 0.3112 (0.3795)	
training:	Epoch: [3][296/408]	Loss 0.1632 (0.3787)	
training:	Epoch: [3][297/408]	Loss 0.4765 (0.3791)	
training:	Epoch: [3][298/408]	Loss 0.3684 (0.3790)	
training:	Epoch: [3][299/408]	Loss 0.2829 (0.3787)	
training:	Epoch: [3][300/408]	Loss 0.6241 (0.3795)	
training:	Epoch: [3][301/408]	Loss 0.4986 (0.3799)	
training:	Epoch: [3][302/408]	Loss 0.3943 (0.3800)	
training:	Epoch: [3][303/408]	Loss 0.1813 (0.3793)	
training:	Epoch: [3][304/408]	Loss 0.3116 (0.3791)	
training:	Epoch: [3][305/408]	Loss 0.3543 (0.3790)	
training:	Epoch: [3][306/408]	Loss 0.2832 (0.3787)	
training:	Epoch: [3][307/408]	Loss 0.1937 (0.3781)	
training:	Epoch: [3][308/408]	Loss 0.5609 (0.3787)	
training:	Epoch: [3][309/408]	Loss 0.7488 (0.3799)	
training:	Epoch: [3][310/408]	Loss 0.3696 (0.3799)	
training:	Epoch: [3][311/408]	Loss 0.1679 (0.3792)	
training:	Epoch: [3][312/408]	Loss 0.2118 (0.3786)	
training:	Epoch: [3][313/408]	Loss 0.3379 (0.3785)	
training:	Epoch: [3][314/408]	Loss 0.2010 (0.3779)	
training:	Epoch: [3][315/408]	Loss 0.3425 (0.3778)	
training:	Epoch: [3][316/408]	Loss 0.5565 (0.3784)	
training:	Epoch: [3][317/408]	Loss 0.4896 (0.3787)	
training:	Epoch: [3][318/408]	Loss 0.4552 (0.3790)	
training:	Epoch: [3][319/408]	Loss 0.4990 (0.3794)	
training:	Epoch: [3][320/408]	Loss 0.2567 (0.3790)	
training:	Epoch: [3][321/408]	Loss 0.4774 (0.3793)	
training:	Epoch: [3][322/408]	Loss 0.2934 (0.3790)	
training:	Epoch: [3][323/408]	Loss 0.7481 (0.3802)	
training:	Epoch: [3][324/408]	Loss 0.3678 (0.3801)	
training:	Epoch: [3][325/408]	Loss 0.4343 (0.3803)	
training:	Epoch: [3][326/408]	Loss 0.4595 (0.3805)	
training:	Epoch: [3][327/408]	Loss 0.2138 (0.3800)	
training:	Epoch: [3][328/408]	Loss 0.2767 (0.3797)	
training:	Epoch: [3][329/408]	Loss 0.3325 (0.3796)	
training:	Epoch: [3][330/408]	Loss 0.3581 (0.3795)	
training:	Epoch: [3][331/408]	Loss 0.5744 (0.3801)	
training:	Epoch: [3][332/408]	Loss 0.5152 (0.3805)	
training:	Epoch: [3][333/408]	Loss 0.2291 (0.3800)	
training:	Epoch: [3][334/408]	Loss 0.4020 (0.3801)	
training:	Epoch: [3][335/408]	Loss 0.3339 (0.3800)	
training:	Epoch: [3][336/408]	Loss 0.4445 (0.3802)	
training:	Epoch: [3][337/408]	Loss 0.1984 (0.3796)	
training:	Epoch: [3][338/408]	Loss 0.6080 (0.3803)	
training:	Epoch: [3][339/408]	Loss 0.2551 (0.3799)	
training:	Epoch: [3][340/408]	Loss 0.3852 (0.3799)	
training:	Epoch: [3][341/408]	Loss 0.5142 (0.3803)	
training:	Epoch: [3][342/408]	Loss 0.5020 (0.3807)	
training:	Epoch: [3][343/408]	Loss 0.3321 (0.3806)	
training:	Epoch: [3][344/408]	Loss 0.1642 (0.3799)	
training:	Epoch: [3][345/408]	Loss 0.2597 (0.3796)	
training:	Epoch: [3][346/408]	Loss 0.5509 (0.3801)	
training:	Epoch: [3][347/408]	Loss 0.3596 (0.3800)	
training:	Epoch: [3][348/408]	Loss 0.6881 (0.3809)	
training:	Epoch: [3][349/408]	Loss 0.5085 (0.3813)	
training:	Epoch: [3][350/408]	Loss 0.3676 (0.3812)	
training:	Epoch: [3][351/408]	Loss 0.1883 (0.3807)	
training:	Epoch: [3][352/408]	Loss 0.2673 (0.3804)	
training:	Epoch: [3][353/408]	Loss 0.2729 (0.3800)	
training:	Epoch: [3][354/408]	Loss 0.4272 (0.3802)	
training:	Epoch: [3][355/408]	Loss 0.2298 (0.3798)	
training:	Epoch: [3][356/408]	Loss 0.7022 (0.3807)	
training:	Epoch: [3][357/408]	Loss 0.2290 (0.3802)	
training:	Epoch: [3][358/408]	Loss 0.2993 (0.3800)	
training:	Epoch: [3][359/408]	Loss 0.3947 (0.3801)	
training:	Epoch: [3][360/408]	Loss 0.3971 (0.3801)	
training:	Epoch: [3][361/408]	Loss 0.3353 (0.3800)	
training:	Epoch: [3][362/408]	Loss 0.6117 (0.3806)	
training:	Epoch: [3][363/408]	Loss 0.1386 (0.3799)	
training:	Epoch: [3][364/408]	Loss 0.6243 (0.3806)	
training:	Epoch: [3][365/408]	Loss 0.3400 (0.3805)	
training:	Epoch: [3][366/408]	Loss 0.1402 (0.3799)	
training:	Epoch: [3][367/408]	Loss 0.6186 (0.3805)	
training:	Epoch: [3][368/408]	Loss 0.1579 (0.3799)	
training:	Epoch: [3][369/408]	Loss 0.5607 (0.3804)	
training:	Epoch: [3][370/408]	Loss 0.2604 (0.3801)	
training:	Epoch: [3][371/408]	Loss 0.3271 (0.3799)	
training:	Epoch: [3][372/408]	Loss 0.6121 (0.3805)	
training:	Epoch: [3][373/408]	Loss 0.5669 (0.3810)	
training:	Epoch: [3][374/408]	Loss 0.4076 (0.3811)	
training:	Epoch: [3][375/408]	Loss 0.4917 (0.3814)	
training:	Epoch: [3][376/408]	Loss 0.4141 (0.3815)	
training:	Epoch: [3][377/408]	Loss 0.4147 (0.3816)	
training:	Epoch: [3][378/408]	Loss 0.1861 (0.3811)	
training:	Epoch: [3][379/408]	Loss 0.3426 (0.3810)	
training:	Epoch: [3][380/408]	Loss 0.4488 (0.3811)	
training:	Epoch: [3][381/408]	Loss 0.5685 (0.3816)	
training:	Epoch: [3][382/408]	Loss 0.4056 (0.3817)	
training:	Epoch: [3][383/408]	Loss 0.2994 (0.3815)	
training:	Epoch: [3][384/408]	Loss 0.4581 (0.3817)	
training:	Epoch: [3][385/408]	Loss 0.4241 (0.3818)	
training:	Epoch: [3][386/408]	Loss 0.5849 (0.3823)	
training:	Epoch: [3][387/408]	Loss 0.5692 (0.3828)	
training:	Epoch: [3][388/408]	Loss 0.4360 (0.3829)	
training:	Epoch: [3][389/408]	Loss 0.5021 (0.3832)	
training:	Epoch: [3][390/408]	Loss 0.4407 (0.3834)	
training:	Epoch: [3][391/408]	Loss 0.3693 (0.3834)	
training:	Epoch: [3][392/408]	Loss 0.1874 (0.3829)	
training:	Epoch: [3][393/408]	Loss 0.2713 (0.3826)	
training:	Epoch: [3][394/408]	Loss 0.2025 (0.3821)	
training:	Epoch: [3][395/408]	Loss 0.5194 (0.3825)	
training:	Epoch: [3][396/408]	Loss 0.5625 (0.3829)	
training:	Epoch: [3][397/408]	Loss 0.3753 (0.3829)	
training:	Epoch: [3][398/408]	Loss 0.2969 (0.3827)	
training:	Epoch: [3][399/408]	Loss 0.3918 (0.3827)	
training:	Epoch: [3][400/408]	Loss 0.3115 (0.3825)	
training:	Epoch: [3][401/408]	Loss 0.1888 (0.3820)	
training:	Epoch: [3][402/408]	Loss 0.2285 (0.3817)	
training:	Epoch: [3][403/408]	Loss 0.4500 (0.3818)	
training:	Epoch: [3][404/408]	Loss 0.3921 (0.3819)	
training:	Epoch: [3][405/408]	Loss 0.3609 (0.3818)	
training:	Epoch: [3][406/408]	Loss 0.2071 (0.3814)	
training:	Epoch: [3][407/408]	Loss 0.2227 (0.3810)	
training:	Epoch: [3][408/408]	Loss 0.4002 (0.3810)	
Training:	 Loss: 0.3805

Training:	 ACC: 0.8382 0.8342 0.7396 0.9369
Validation:	 ACC: 0.7913 0.7860 0.6735 0.9092
Validation:	 Best_BACC: 0.8069 0.8085 0.8414 0.7724
Validation:	 Loss: 0.4460
Pretraining:	Epoch 4/200
----------
training:	Epoch: [4][1/408]	Loss 0.4665 (0.4665)	
training:	Epoch: [4][2/408]	Loss 0.2732 (0.3698)	
training:	Epoch: [4][3/408]	Loss 0.1938 (0.3111)	
training:	Epoch: [4][4/408]	Loss 0.2859 (0.3048)	
training:	Epoch: [4][5/408]	Loss 0.4032 (0.3245)	
training:	Epoch: [4][6/408]	Loss 0.3341 (0.3261)	
training:	Epoch: [4][7/408]	Loss 0.2466 (0.3147)	
training:	Epoch: [4][8/408]	Loss 0.2359 (0.3049)	
training:	Epoch: [4][9/408]	Loss 0.6215 (0.3401)	
training:	Epoch: [4][10/408]	Loss 0.1208 (0.3181)	
training:	Epoch: [4][11/408]	Loss 0.3362 (0.3198)	
training:	Epoch: [4][12/408]	Loss 0.2077 (0.3104)	
training:	Epoch: [4][13/408]	Loss 0.3830 (0.3160)	
training:	Epoch: [4][14/408]	Loss 0.3431 (0.3180)	
training:	Epoch: [4][15/408]	Loss 0.3079 (0.3173)	
training:	Epoch: [4][16/408]	Loss 0.2685 (0.3142)	
training:	Epoch: [4][17/408]	Loss 0.1977 (0.3074)	
training:	Epoch: [4][18/408]	Loss 0.1828 (0.3005)	
training:	Epoch: [4][19/408]	Loss 0.4629 (0.3090)	
training:	Epoch: [4][20/408]	Loss 0.2218 (0.3047)	
training:	Epoch: [4][21/408]	Loss 0.3450 (0.3066)	
training:	Epoch: [4][22/408]	Loss 0.2709 (0.3050)	
training:	Epoch: [4][23/408]	Loss 0.5953 (0.3176)	
training:	Epoch: [4][24/408]	Loss 0.2970 (0.3167)	
training:	Epoch: [4][25/408]	Loss 0.1770 (0.3111)	
training:	Epoch: [4][26/408]	Loss 0.3263 (0.3117)	
training:	Epoch: [4][27/408]	Loss 0.1522 (0.3058)	
training:	Epoch: [4][28/408]	Loss 0.4855 (0.3122)	
training:	Epoch: [4][29/408]	Loss 0.5616 (0.3208)	
training:	Epoch: [4][30/408]	Loss 0.6453 (0.3316)	
training:	Epoch: [4][31/408]	Loss 0.6648 (0.3424)	
training:	Epoch: [4][32/408]	Loss 0.5247 (0.3481)	
training:	Epoch: [4][33/408]	Loss 0.3132 (0.3470)	
training:	Epoch: [4][34/408]	Loss 0.3266 (0.3464)	
training:	Epoch: [4][35/408]	Loss 0.3394 (0.3462)	
training:	Epoch: [4][36/408]	Loss 0.8884 (0.3613)	
training:	Epoch: [4][37/408]	Loss 0.3048 (0.3598)	
training:	Epoch: [4][38/408]	Loss 0.3998 (0.3608)	
training:	Epoch: [4][39/408]	Loss 0.3925 (0.3616)	
training:	Epoch: [4][40/408]	Loss 0.3153 (0.3605)	
training:	Epoch: [4][41/408]	Loss 0.2510 (0.3578)	
training:	Epoch: [4][42/408]	Loss 0.4239 (0.3594)	
training:	Epoch: [4][43/408]	Loss 0.4917 (0.3624)	
training:	Epoch: [4][44/408]	Loss 0.4606 (0.3647)	
training:	Epoch: [4][45/408]	Loss 0.2836 (0.3629)	
training:	Epoch: [4][46/408]	Loss 0.3862 (0.3634)	
training:	Epoch: [4][47/408]	Loss 0.1626 (0.3591)	
training:	Epoch: [4][48/408]	Loss 0.2317 (0.3565)	
training:	Epoch: [4][49/408]	Loss 0.4418 (0.3582)	
training:	Epoch: [4][50/408]	Loss 0.2451 (0.3559)	
training:	Epoch: [4][51/408]	Loss 0.4634 (0.3580)	
training:	Epoch: [4][52/408]	Loss 0.1602 (0.3542)	
training:	Epoch: [4][53/408]	Loss 0.1665 (0.3507)	
training:	Epoch: [4][54/408]	Loss 0.3990 (0.3516)	
training:	Epoch: [4][55/408]	Loss 0.2924 (0.3505)	
training:	Epoch: [4][56/408]	Loss 0.4287 (0.3519)	
training:	Epoch: [4][57/408]	Loss 0.6302 (0.3568)	
training:	Epoch: [4][58/408]	Loss 0.3080 (0.3560)	
training:	Epoch: [4][59/408]	Loss 0.3496 (0.3558)	
training:	Epoch: [4][60/408]	Loss 0.2813 (0.3546)	
training:	Epoch: [4][61/408]	Loss 0.4596 (0.3563)	
training:	Epoch: [4][62/408]	Loss 0.2467 (0.3546)	
training:	Epoch: [4][63/408]	Loss 0.6885 (0.3599)	
training:	Epoch: [4][64/408]	Loss 0.2861 (0.3587)	
training:	Epoch: [4][65/408]	Loss 0.3178 (0.3581)	
training:	Epoch: [4][66/408]	Loss 0.2385 (0.3563)	
training:	Epoch: [4][67/408]	Loss 0.1389 (0.3530)	
training:	Epoch: [4][68/408]	Loss 0.2891 (0.3521)	
training:	Epoch: [4][69/408]	Loss 0.4519 (0.3535)	
training:	Epoch: [4][70/408]	Loss 0.3934 (0.3541)	
training:	Epoch: [4][71/408]	Loss 0.2950 (0.3533)	
training:	Epoch: [4][72/408]	Loss 0.3758 (0.3536)	
training:	Epoch: [4][73/408]	Loss 0.2615 (0.3523)	
training:	Epoch: [4][74/408]	Loss 0.5008 (0.3543)	
training:	Epoch: [4][75/408]	Loss 0.2749 (0.3533)	
training:	Epoch: [4][76/408]	Loss 0.2713 (0.3522)	
training:	Epoch: [4][77/408]	Loss 0.5673 (0.3550)	
training:	Epoch: [4][78/408]	Loss 0.3302 (0.3547)	
training:	Epoch: [4][79/408]	Loss 0.2326 (0.3531)	
training:	Epoch: [4][80/408]	Loss 0.4261 (0.3540)	
training:	Epoch: [4][81/408]	Loss 0.2405 (0.3526)	
training:	Epoch: [4][82/408]	Loss 0.3195 (0.3522)	
training:	Epoch: [4][83/408]	Loss 0.1595 (0.3499)	
training:	Epoch: [4][84/408]	Loss 0.3263 (0.3496)	
training:	Epoch: [4][85/408]	Loss 0.4582 (0.3509)	
training:	Epoch: [4][86/408]	Loss 0.3816 (0.3512)	
training:	Epoch: [4][87/408]	Loss 0.3950 (0.3518)	
training:	Epoch: [4][88/408]	Loss 0.2071 (0.3501)	
training:	Epoch: [4][89/408]	Loss 0.1842 (0.3482)	
training:	Epoch: [4][90/408]	Loss 0.3830 (0.3486)	
training:	Epoch: [4][91/408]	Loss 0.1881 (0.3469)	
training:	Epoch: [4][92/408]	Loss 0.2195 (0.3455)	
training:	Epoch: [4][93/408]	Loss 0.5646 (0.3478)	
training:	Epoch: [4][94/408]	Loss 0.3558 (0.3479)	
training:	Epoch: [4][95/408]	Loss 0.3984 (0.3485)	
training:	Epoch: [4][96/408]	Loss 0.1952 (0.3469)	
training:	Epoch: [4][97/408]	Loss 0.3426 (0.3468)	
training:	Epoch: [4][98/408]	Loss 0.4292 (0.3477)	
training:	Epoch: [4][99/408]	Loss 0.4288 (0.3485)	
training:	Epoch: [4][100/408]	Loss 0.5350 (0.3503)	
training:	Epoch: [4][101/408]	Loss 0.1459 (0.3483)	
training:	Epoch: [4][102/408]	Loss 0.3216 (0.3481)	
training:	Epoch: [4][103/408]	Loss 0.4802 (0.3493)	
training:	Epoch: [4][104/408]	Loss 0.2646 (0.3485)	
training:	Epoch: [4][105/408]	Loss 0.3335 (0.3484)	
training:	Epoch: [4][106/408]	Loss 0.3198 (0.3481)	
training:	Epoch: [4][107/408]	Loss 0.5037 (0.3496)	
training:	Epoch: [4][108/408]	Loss 0.4170 (0.3502)	
training:	Epoch: [4][109/408]	Loss 0.2699 (0.3495)	
training:	Epoch: [4][110/408]	Loss 0.3655 (0.3496)	
training:	Epoch: [4][111/408]	Loss 0.4434 (0.3504)	
training:	Epoch: [4][112/408]	Loss 0.1560 (0.3487)	
training:	Epoch: [4][113/408]	Loss 0.6091 (0.3510)	
training:	Epoch: [4][114/408]	Loss 0.3123 (0.3507)	
training:	Epoch: [4][115/408]	Loss 0.2994 (0.3502)	
training:	Epoch: [4][116/408]	Loss 0.5343 (0.3518)	
training:	Epoch: [4][117/408]	Loss 0.5274 (0.3533)	
training:	Epoch: [4][118/408]	Loss 0.1399 (0.3515)	
training:	Epoch: [4][119/408]	Loss 0.3659 (0.3516)	
training:	Epoch: [4][120/408]	Loss 0.2687 (0.3509)	
training:	Epoch: [4][121/408]	Loss 0.2072 (0.3497)	
training:	Epoch: [4][122/408]	Loss 0.3331 (0.3496)	
training:	Epoch: [4][123/408]	Loss 0.1714 (0.3482)	
training:	Epoch: [4][124/408]	Loss 0.5035 (0.3494)	
training:	Epoch: [4][125/408]	Loss 0.1653 (0.3479)	
training:	Epoch: [4][126/408]	Loss 0.2279 (0.3470)	
training:	Epoch: [4][127/408]	Loss 0.3538 (0.3470)	
training:	Epoch: [4][128/408]	Loss 0.1906 (0.3458)	
training:	Epoch: [4][129/408]	Loss 0.1924 (0.3446)	
training:	Epoch: [4][130/408]	Loss 0.6360 (0.3469)	
training:	Epoch: [4][131/408]	Loss 0.3492 (0.3469)	
training:	Epoch: [4][132/408]	Loss 0.4947 (0.3480)	
training:	Epoch: [4][133/408]	Loss 0.3406 (0.3480)	
training:	Epoch: [4][134/408]	Loss 0.4180 (0.3485)	
training:	Epoch: [4][135/408]	Loss 0.1435 (0.3470)	
training:	Epoch: [4][136/408]	Loss 0.4053 (0.3474)	
training:	Epoch: [4][137/408]	Loss 0.2472 (0.3467)	
training:	Epoch: [4][138/408]	Loss 0.8443 (0.3503)	
training:	Epoch: [4][139/408]	Loss 0.4614 (0.3511)	
training:	Epoch: [4][140/408]	Loss 0.1429 (0.3496)	
training:	Epoch: [4][141/408]	Loss 0.5744 (0.3512)	
training:	Epoch: [4][142/408]	Loss 0.3165 (0.3509)	
training:	Epoch: [4][143/408]	Loss 0.2988 (0.3506)	
training:	Epoch: [4][144/408]	Loss 0.2662 (0.3500)	
training:	Epoch: [4][145/408]	Loss 0.4105 (0.3504)	
training:	Epoch: [4][146/408]	Loss 0.1882 (0.3493)	
training:	Epoch: [4][147/408]	Loss 0.1461 (0.3479)	
training:	Epoch: [4][148/408]	Loss 0.5090 (0.3490)	
training:	Epoch: [4][149/408]	Loss 0.4866 (0.3499)	
training:	Epoch: [4][150/408]	Loss 0.4920 (0.3509)	
training:	Epoch: [4][151/408]	Loss 0.5998 (0.3525)	
training:	Epoch: [4][152/408]	Loss 0.1857 (0.3514)	
training:	Epoch: [4][153/408]	Loss 0.1713 (0.3502)	
training:	Epoch: [4][154/408]	Loss 0.2319 (0.3495)	
training:	Epoch: [4][155/408]	Loss 0.2025 (0.3485)	
training:	Epoch: [4][156/408]	Loss 0.1551 (0.3473)	
training:	Epoch: [4][157/408]	Loss 0.1756 (0.3462)	
training:	Epoch: [4][158/408]	Loss 0.3407 (0.3461)	
training:	Epoch: [4][159/408]	Loss 0.3399 (0.3461)	
training:	Epoch: [4][160/408]	Loss 0.2046 (0.3452)	
training:	Epoch: [4][161/408]	Loss 0.1405 (0.3440)	
training:	Epoch: [4][162/408]	Loss 0.4623 (0.3447)	
training:	Epoch: [4][163/408]	Loss 0.5809 (0.3461)	
training:	Epoch: [4][164/408]	Loss 0.4247 (0.3466)	
training:	Epoch: [4][165/408]	Loss 0.1316 (0.3453)	
training:	Epoch: [4][166/408]	Loss 0.3862 (0.3456)	
training:	Epoch: [4][167/408]	Loss 0.2361 (0.3449)	
training:	Epoch: [4][168/408]	Loss 0.3173 (0.3447)	
training:	Epoch: [4][169/408]	Loss 0.4044 (0.3451)	
training:	Epoch: [4][170/408]	Loss 0.3582 (0.3452)	
training:	Epoch: [4][171/408]	Loss 0.1999 (0.3443)	
training:	Epoch: [4][172/408]	Loss 0.3965 (0.3446)	
training:	Epoch: [4][173/408]	Loss 0.2356 (0.3440)	
training:	Epoch: [4][174/408]	Loss 0.3974 (0.3443)	
training:	Epoch: [4][175/408]	Loss 0.7887 (0.3468)	
training:	Epoch: [4][176/408]	Loss 0.5803 (0.3482)	
training:	Epoch: [4][177/408]	Loss 0.3788 (0.3483)	
training:	Epoch: [4][178/408]	Loss 0.3875 (0.3486)	
training:	Epoch: [4][179/408]	Loss 0.1527 (0.3475)	
training:	Epoch: [4][180/408]	Loss 0.2782 (0.3471)	
training:	Epoch: [4][181/408]	Loss 0.2911 (0.3468)	
training:	Epoch: [4][182/408]	Loss 0.2941 (0.3465)	
training:	Epoch: [4][183/408]	Loss 0.1898 (0.3456)	
training:	Epoch: [4][184/408]	Loss 0.4273 (0.3461)	
training:	Epoch: [4][185/408]	Loss 0.3486 (0.3461)	
training:	Epoch: [4][186/408]	Loss 0.2378 (0.3455)	
training:	Epoch: [4][187/408]	Loss 0.4794 (0.3462)	
training:	Epoch: [4][188/408]	Loss 0.6030 (0.3476)	
training:	Epoch: [4][189/408]	Loss 0.1916 (0.3468)	
training:	Epoch: [4][190/408]	Loss 0.5842 (0.3480)	
training:	Epoch: [4][191/408]	Loss 0.5039 (0.3488)	
training:	Epoch: [4][192/408]	Loss 0.2334 (0.3482)	
training:	Epoch: [4][193/408]	Loss 0.7742 (0.3504)	
training:	Epoch: [4][194/408]	Loss 0.3078 (0.3502)	
training:	Epoch: [4][195/408]	Loss 0.2129 (0.3495)	
training:	Epoch: [4][196/408]	Loss 0.3016 (0.3493)	
training:	Epoch: [4][197/408]	Loss 0.2671 (0.3488)	
training:	Epoch: [4][198/408]	Loss 0.3514 (0.3489)	
training:	Epoch: [4][199/408]	Loss 0.3226 (0.3487)	
training:	Epoch: [4][200/408]	Loss 0.2910 (0.3484)	
training:	Epoch: [4][201/408]	Loss 0.1369 (0.3474)	
training:	Epoch: [4][202/408]	Loss 0.3105 (0.3472)	
training:	Epoch: [4][203/408]	Loss 0.4227 (0.3476)	
training:	Epoch: [4][204/408]	Loss 0.2376 (0.3470)	
training:	Epoch: [4][205/408]	Loss 0.3469 (0.3470)	
training:	Epoch: [4][206/408]	Loss 0.2228 (0.3464)	
training:	Epoch: [4][207/408]	Loss 0.6911 (0.3481)	
training:	Epoch: [4][208/408]	Loss 0.3117 (0.3479)	
training:	Epoch: [4][209/408]	Loss 0.5520 (0.3489)	
training:	Epoch: [4][210/408]	Loss 0.3250 (0.3488)	
training:	Epoch: [4][211/408]	Loss 0.6209 (0.3501)	
training:	Epoch: [4][212/408]	Loss 0.4127 (0.3504)	
training:	Epoch: [4][213/408]	Loss 0.4374 (0.3508)	
training:	Epoch: [4][214/408]	Loss 0.2322 (0.3502)	
training:	Epoch: [4][215/408]	Loss 0.1932 (0.3495)	
training:	Epoch: [4][216/408]	Loss 0.4690 (0.3500)	
training:	Epoch: [4][217/408]	Loss 0.1863 (0.3493)	
training:	Epoch: [4][218/408]	Loss 0.3302 (0.3492)	
training:	Epoch: [4][219/408]	Loss 0.1836 (0.3484)	
training:	Epoch: [4][220/408]	Loss 0.2828 (0.3481)	
training:	Epoch: [4][221/408]	Loss 0.1948 (0.3475)	
training:	Epoch: [4][222/408]	Loss 0.5650 (0.3484)	
training:	Epoch: [4][223/408]	Loss 0.5154 (0.3492)	
training:	Epoch: [4][224/408]	Loss 0.2927 (0.3489)	
training:	Epoch: [4][225/408]	Loss 0.3043 (0.3487)	
training:	Epoch: [4][226/408]	Loss 0.3278 (0.3486)	
training:	Epoch: [4][227/408]	Loss 0.4010 (0.3489)	
training:	Epoch: [4][228/408]	Loss 0.2125 (0.3483)	
training:	Epoch: [4][229/408]	Loss 0.1716 (0.3475)	
training:	Epoch: [4][230/408]	Loss 0.3544 (0.3475)	
training:	Epoch: [4][231/408]	Loss 0.2466 (0.3471)	
training:	Epoch: [4][232/408]	Loss 0.5097 (0.3478)	
training:	Epoch: [4][233/408]	Loss 0.1841 (0.3471)	
training:	Epoch: [4][234/408]	Loss 0.2234 (0.3466)	
training:	Epoch: [4][235/408]	Loss 0.5323 (0.3474)	
training:	Epoch: [4][236/408]	Loss 0.2605 (0.3470)	
training:	Epoch: [4][237/408]	Loss 0.2384 (0.3465)	
training:	Epoch: [4][238/408]	Loss 0.6955 (0.3480)	
training:	Epoch: [4][239/408]	Loss 0.7429 (0.3496)	
training:	Epoch: [4][240/408]	Loss 0.3976 (0.3498)	
training:	Epoch: [4][241/408]	Loss 0.2715 (0.3495)	
training:	Epoch: [4][242/408]	Loss 0.5225 (0.3502)	
training:	Epoch: [4][243/408]	Loss 0.1078 (0.3492)	
training:	Epoch: [4][244/408]	Loss 0.1832 (0.3486)	
training:	Epoch: [4][245/408]	Loss 0.4743 (0.3491)	
training:	Epoch: [4][246/408]	Loss 0.2258 (0.3486)	
training:	Epoch: [4][247/408]	Loss 0.2963 (0.3484)	
training:	Epoch: [4][248/408]	Loss 0.5147 (0.3490)	
training:	Epoch: [4][249/408]	Loss 0.2793 (0.3487)	
training:	Epoch: [4][250/408]	Loss 0.3420 (0.3487)	
training:	Epoch: [4][251/408]	Loss 0.1642 (0.3480)	
training:	Epoch: [4][252/408]	Loss 0.4424 (0.3484)	
training:	Epoch: [4][253/408]	Loss 0.2770 (0.3481)	
training:	Epoch: [4][254/408]	Loss 0.5321 (0.3488)	
training:	Epoch: [4][255/408]	Loss 0.1626 (0.3481)	
training:	Epoch: [4][256/408]	Loss 0.4632 (0.3485)	
training:	Epoch: [4][257/408]	Loss 0.2928 (0.3483)	
training:	Epoch: [4][258/408]	Loss 0.2701 (0.3480)	
training:	Epoch: [4][259/408]	Loss 0.3185 (0.3479)	
training:	Epoch: [4][260/408]	Loss 0.4267 (0.3482)	
training:	Epoch: [4][261/408]	Loss 0.4160 (0.3485)	
training:	Epoch: [4][262/408]	Loss 0.3017 (0.3483)	
training:	Epoch: [4][263/408]	Loss 0.4811 (0.3488)	
training:	Epoch: [4][264/408]	Loss 0.1463 (0.3480)	
training:	Epoch: [4][265/408]	Loss 0.3153 (0.3479)	
training:	Epoch: [4][266/408]	Loss 0.6152 (0.3489)	
training:	Epoch: [4][267/408]	Loss 0.3837 (0.3490)	
training:	Epoch: [4][268/408]	Loss 0.2255 (0.3486)	
training:	Epoch: [4][269/408]	Loss 0.3507 (0.3486)	
training:	Epoch: [4][270/408]	Loss 0.3341 (0.3485)	
training:	Epoch: [4][271/408]	Loss 0.4926 (0.3490)	
training:	Epoch: [4][272/408]	Loss 0.4195 (0.3493)	
training:	Epoch: [4][273/408]	Loss 0.4831 (0.3498)	
training:	Epoch: [4][274/408]	Loss 0.3356 (0.3497)	
training:	Epoch: [4][275/408]	Loss 0.2346 (0.3493)	
training:	Epoch: [4][276/408]	Loss 0.1927 (0.3488)	
training:	Epoch: [4][277/408]	Loss 0.2884 (0.3485)	
training:	Epoch: [4][278/408]	Loss 0.4167 (0.3488)	
training:	Epoch: [4][279/408]	Loss 0.1573 (0.3481)	
training:	Epoch: [4][280/408]	Loss 0.2888 (0.3479)	
training:	Epoch: [4][281/408]	Loss 0.4605 (0.3483)	
training:	Epoch: [4][282/408]	Loss 0.6964 (0.3495)	
training:	Epoch: [4][283/408]	Loss 0.5229 (0.3501)	
training:	Epoch: [4][284/408]	Loss 0.1916 (0.3496)	
training:	Epoch: [4][285/408]	Loss 0.4240 (0.3498)	
training:	Epoch: [4][286/408]	Loss 0.6513 (0.3509)	
training:	Epoch: [4][287/408]	Loss 0.4259 (0.3512)	
training:	Epoch: [4][288/408]	Loss 0.3332 (0.3511)	
training:	Epoch: [4][289/408]	Loss 0.2688 (0.3508)	
training:	Epoch: [4][290/408]	Loss 0.3969 (0.3510)	
training:	Epoch: [4][291/408]	Loss 0.1063 (0.3501)	
training:	Epoch: [4][292/408]	Loss 0.6152 (0.3510)	
training:	Epoch: [4][293/408]	Loss 0.3547 (0.3510)	
training:	Epoch: [4][294/408]	Loss 0.2936 (0.3508)	
training:	Epoch: [4][295/408]	Loss 0.3855 (0.3510)	
training:	Epoch: [4][296/408]	Loss 0.2900 (0.3508)	
training:	Epoch: [4][297/408]	Loss 0.1528 (0.3501)	
training:	Epoch: [4][298/408]	Loss 0.4108 (0.3503)	
training:	Epoch: [4][299/408]	Loss 0.1474 (0.3496)	
training:	Epoch: [4][300/408]	Loss 0.7075 (0.3508)	
training:	Epoch: [4][301/408]	Loss 0.4220 (0.3510)	
training:	Epoch: [4][302/408]	Loss 0.3595 (0.3511)	
training:	Epoch: [4][303/408]	Loss 0.6290 (0.3520)	
training:	Epoch: [4][304/408]	Loss 0.4189 (0.3522)	
training:	Epoch: [4][305/408]	Loss 0.1669 (0.3516)	
training:	Epoch: [4][306/408]	Loss 0.3324 (0.3515)	
training:	Epoch: [4][307/408]	Loss 0.1989 (0.3510)	
training:	Epoch: [4][308/408]	Loss 0.4765 (0.3515)	
training:	Epoch: [4][309/408]	Loss 0.3514 (0.3515)	
training:	Epoch: [4][310/408]	Loss 0.3077 (0.3513)	
training:	Epoch: [4][311/408]	Loss 0.1989 (0.3508)	
training:	Epoch: [4][312/408]	Loss 0.5742 (0.3515)	
training:	Epoch: [4][313/408]	Loss 0.2770 (0.3513)	
training:	Epoch: [4][314/408]	Loss 0.2110 (0.3509)	
training:	Epoch: [4][315/408]	Loss 0.3090 (0.3507)	
training:	Epoch: [4][316/408]	Loss 0.4402 (0.3510)	
training:	Epoch: [4][317/408]	Loss 0.2470 (0.3507)	
training:	Epoch: [4][318/408]	Loss 0.1763 (0.3501)	
training:	Epoch: [4][319/408]	Loss 0.5269 (0.3507)	
training:	Epoch: [4][320/408]	Loss 0.1813 (0.3502)	
training:	Epoch: [4][321/408]	Loss 0.3889 (0.3503)	
training:	Epoch: [4][322/408]	Loss 0.3879 (0.3504)	
training:	Epoch: [4][323/408]	Loss 0.1824 (0.3499)	
training:	Epoch: [4][324/408]	Loss 0.4520 (0.3502)	
training:	Epoch: [4][325/408]	Loss 0.4291 (0.3504)	
training:	Epoch: [4][326/408]	Loss 0.2404 (0.3501)	
training:	Epoch: [4][327/408]	Loss 0.3962 (0.3502)	
training:	Epoch: [4][328/408]	Loss 0.2110 (0.3498)	
training:	Epoch: [4][329/408]	Loss 0.2494 (0.3495)	
training:	Epoch: [4][330/408]	Loss 0.2857 (0.3493)	
training:	Epoch: [4][331/408]	Loss 0.5704 (0.3500)	
training:	Epoch: [4][332/408]	Loss 0.4027 (0.3501)	
training:	Epoch: [4][333/408]	Loss 0.6074 (0.3509)	
training:	Epoch: [4][334/408]	Loss 0.4011 (0.3511)	
training:	Epoch: [4][335/408]	Loss 0.4978 (0.3515)	
training:	Epoch: [4][336/408]	Loss 0.3394 (0.3515)	
training:	Epoch: [4][337/408]	Loss 0.4738 (0.3518)	
training:	Epoch: [4][338/408]	Loss 0.5249 (0.3523)	
training:	Epoch: [4][339/408]	Loss 0.5137 (0.3528)	
training:	Epoch: [4][340/408]	Loss 0.3244 (0.3527)	
training:	Epoch: [4][341/408]	Loss 0.3363 (0.3527)	
training:	Epoch: [4][342/408]	Loss 0.4407 (0.3529)	
training:	Epoch: [4][343/408]	Loss 0.5327 (0.3535)	
training:	Epoch: [4][344/408]	Loss 0.3678 (0.3535)	
training:	Epoch: [4][345/408]	Loss 0.7030 (0.3545)	
training:	Epoch: [4][346/408]	Loss 0.2550 (0.3542)	
training:	Epoch: [4][347/408]	Loss 0.2287 (0.3539)	
training:	Epoch: [4][348/408]	Loss 0.4503 (0.3541)	
training:	Epoch: [4][349/408]	Loss 0.2595 (0.3539)	
training:	Epoch: [4][350/408]	Loss 0.1498 (0.3533)	
training:	Epoch: [4][351/408]	Loss 0.3235 (0.3532)	
training:	Epoch: [4][352/408]	Loss 0.6036 (0.3539)	
training:	Epoch: [4][353/408]	Loss 0.1948 (0.3535)	
training:	Epoch: [4][354/408]	Loss 0.3577 (0.3535)	
training:	Epoch: [4][355/408]	Loss 0.3773 (0.3535)	
training:	Epoch: [4][356/408]	Loss 0.3182 (0.3534)	
training:	Epoch: [4][357/408]	Loss 0.3164 (0.3533)	
training:	Epoch: [4][358/408]	Loss 0.1842 (0.3529)	
training:	Epoch: [4][359/408]	Loss 0.3263 (0.3528)	
training:	Epoch: [4][360/408]	Loss 0.3832 (0.3529)	
training:	Epoch: [4][361/408]	Loss 0.1928 (0.3524)	
training:	Epoch: [4][362/408]	Loss 0.2829 (0.3522)	
training:	Epoch: [4][363/408]	Loss 0.5038 (0.3527)	
training:	Epoch: [4][364/408]	Loss 0.2267 (0.3523)	
training:	Epoch: [4][365/408]	Loss 0.5905 (0.3530)	
training:	Epoch: [4][366/408]	Loss 0.5072 (0.3534)	
training:	Epoch: [4][367/408]	Loss 0.1736 (0.3529)	
training:	Epoch: [4][368/408]	Loss 0.4120 (0.3531)	
training:	Epoch: [4][369/408]	Loss 0.3634 (0.3531)	
training:	Epoch: [4][370/408]	Loss 0.2832 (0.3529)	
training:	Epoch: [4][371/408]	Loss 0.5602 (0.3535)	
training:	Epoch: [4][372/408]	Loss 0.6436 (0.3542)	
training:	Epoch: [4][373/408]	Loss 0.2191 (0.3539)	
training:	Epoch: [4][374/408]	Loss 0.2871 (0.3537)	
training:	Epoch: [4][375/408]	Loss 0.2698 (0.3535)	
training:	Epoch: [4][376/408]	Loss 0.2614 (0.3532)	
training:	Epoch: [4][377/408]	Loss 0.1018 (0.3526)	
training:	Epoch: [4][378/408]	Loss 0.2217 (0.3522)	
training:	Epoch: [4][379/408]	Loss 0.3295 (0.3522)	
training:	Epoch: [4][380/408]	Loss 0.3555 (0.3522)	
training:	Epoch: [4][381/408]	Loss 0.3659 (0.3522)	
training:	Epoch: [4][382/408]	Loss 0.5016 (0.3526)	
training:	Epoch: [4][383/408]	Loss 0.3277 (0.3525)	
training:	Epoch: [4][384/408]	Loss 0.3009 (0.3524)	
training:	Epoch: [4][385/408]	Loss 0.1723 (0.3519)	
training:	Epoch: [4][386/408]	Loss 0.3057 (0.3518)	
training:	Epoch: [4][387/408]	Loss 0.5211 (0.3522)	
training:	Epoch: [4][388/408]	Loss 0.4256 (0.3524)	
training:	Epoch: [4][389/408]	Loss 0.6969 (0.3533)	
training:	Epoch: [4][390/408]	Loss 0.3185 (0.3532)	
training:	Epoch: [4][391/408]	Loss 0.4605 (0.3535)	
training:	Epoch: [4][392/408]	Loss 0.7224 (0.3544)	
training:	Epoch: [4][393/408]	Loss 0.4418 (0.3547)	
training:	Epoch: [4][394/408]	Loss 0.3357 (0.3546)	
training:	Epoch: [4][395/408]	Loss 0.3364 (0.3546)	
training:	Epoch: [4][396/408]	Loss 0.3241 (0.3545)	
training:	Epoch: [4][397/408]	Loss 0.2749 (0.3543)	
training:	Epoch: [4][398/408]	Loss 0.1885 (0.3539)	
training:	Epoch: [4][399/408]	Loss 0.1204 (0.3533)	
training:	Epoch: [4][400/408]	Loss 0.5651 (0.3538)	
training:	Epoch: [4][401/408]	Loss 0.3867 (0.3539)	
training:	Epoch: [4][402/408]	Loss 0.3224 (0.3538)	
training:	Epoch: [4][403/408]	Loss 0.1880 (0.3534)	
training:	Epoch: [4][404/408]	Loss 0.2973 (0.3533)	
training:	Epoch: [4][405/408]	Loss 0.0997 (0.3526)	
training:	Epoch: [4][406/408]	Loss 0.1582 (0.3522)	
training:	Epoch: [4][407/408]	Loss 0.2418 (0.3519)	
training:	Epoch: [4][408/408]	Loss 0.2625 (0.3517)	
Training:	 Loss: 0.3511

Training:	 ACC: 0.8757 0.8752 0.8624 0.8890
Validation:	 ACC: 0.8173 0.8165 0.7984 0.8363
Validation:	 Best_BACC: 0.8173 0.8165 0.7984 0.8363
Validation:	 Loss: 0.3995
Pretraining:	Epoch 5/200
----------
training:	Epoch: [5][1/408]	Loss 0.1674 (0.1674)	
training:	Epoch: [5][2/408]	Loss 0.4042 (0.2858)	
training:	Epoch: [5][3/408]	Loss 0.2792 (0.2836)	
training:	Epoch: [5][4/408]	Loss 0.1440 (0.2487)	
training:	Epoch: [5][5/408]	Loss 0.4565 (0.2903)	
training:	Epoch: [5][6/408]	Loss 0.2925 (0.2906)	
training:	Epoch: [5][7/408]	Loss 0.2342 (0.2826)	
training:	Epoch: [5][8/408]	Loss 0.1330 (0.2639)	
training:	Epoch: [5][9/408]	Loss 0.7140 (0.3139)	
training:	Epoch: [5][10/408]	Loss 0.3064 (0.3131)	
training:	Epoch: [5][11/408]	Loss 0.4464 (0.3253)	
training:	Epoch: [5][12/408]	Loss 0.2531 (0.3193)	
training:	Epoch: [5][13/408]	Loss 0.3619 (0.3225)	
training:	Epoch: [5][14/408]	Loss 0.1159 (0.3078)	
training:	Epoch: [5][15/408]	Loss 0.3616 (0.3114)	
training:	Epoch: [5][16/408]	Loss 0.1182 (0.2993)	
training:	Epoch: [5][17/408]	Loss 0.1427 (0.2901)	
training:	Epoch: [5][18/408]	Loss 0.4251 (0.2976)	
training:	Epoch: [5][19/408]	Loss 0.2940 (0.2974)	
training:	Epoch: [5][20/408]	Loss 0.2712 (0.2961)	
training:	Epoch: [5][21/408]	Loss 0.2539 (0.2941)	
training:	Epoch: [5][22/408]	Loss 0.5680 (0.3065)	
training:	Epoch: [5][23/408]	Loss 0.2084 (0.3023)	
training:	Epoch: [5][24/408]	Loss 0.5250 (0.3115)	
training:	Epoch: [5][25/408]	Loss 0.0900 (0.3027)	
training:	Epoch: [5][26/408]	Loss 0.2746 (0.3016)	
training:	Epoch: [5][27/408]	Loss 0.6880 (0.3159)	
training:	Epoch: [5][28/408]	Loss 0.2961 (0.3152)	
training:	Epoch: [5][29/408]	Loss 0.6020 (0.3251)	
training:	Epoch: [5][30/408]	Loss 0.5770 (0.3335)	
training:	Epoch: [5][31/408]	Loss 0.3516 (0.3341)	
training:	Epoch: [5][32/408]	Loss 0.1976 (0.3298)	
training:	Epoch: [5][33/408]	Loss 0.3452 (0.3303)	
training:	Epoch: [5][34/408]	Loss 0.4349 (0.3333)	
training:	Epoch: [5][35/408]	Loss 0.3276 (0.3332)	
training:	Epoch: [5][36/408]	Loss 0.3491 (0.3336)	
training:	Epoch: [5][37/408]	Loss 0.3558 (0.3342)	
training:	Epoch: [5][38/408]	Loss 0.5330 (0.3395)	
training:	Epoch: [5][39/408]	Loss 0.5469 (0.3448)	
training:	Epoch: [5][40/408]	Loss 0.2665 (0.3428)	
training:	Epoch: [5][41/408]	Loss 0.3943 (0.3441)	
training:	Epoch: [5][42/408]	Loss 0.2034 (0.3407)	
training:	Epoch: [5][43/408]	Loss 0.4295 (0.3428)	
training:	Epoch: [5][44/408]	Loss 0.4514 (0.3453)	
training:	Epoch: [5][45/408]	Loss 0.2551 (0.3433)	
training:	Epoch: [5][46/408]	Loss 0.4079 (0.3447)	
training:	Epoch: [5][47/408]	Loss 0.3166 (0.3441)	
training:	Epoch: [5][48/408]	Loss 0.0961 (0.3389)	
training:	Epoch: [5][49/408]	Loss 0.2582 (0.3372)	
training:	Epoch: [5][50/408]	Loss 0.5020 (0.3405)	
training:	Epoch: [5][51/408]	Loss 0.2823 (0.3394)	
training:	Epoch: [5][52/408]	Loss 0.4350 (0.3412)	
training:	Epoch: [5][53/408]	Loss 0.5781 (0.3457)	
training:	Epoch: [5][54/408]	Loss 0.4384 (0.3474)	
training:	Epoch: [5][55/408]	Loss 0.1847 (0.3445)	
training:	Epoch: [5][56/408]	Loss 0.3788 (0.3451)	
training:	Epoch: [5][57/408]	Loss 0.2817 (0.3440)	
training:	Epoch: [5][58/408]	Loss 0.2691 (0.3427)	
training:	Epoch: [5][59/408]	Loss 0.1394 (0.3392)	
training:	Epoch: [5][60/408]	Loss 0.1516 (0.3361)	
training:	Epoch: [5][61/408]	Loss 0.4233 (0.3375)	
training:	Epoch: [5][62/408]	Loss 0.3522 (0.3378)	
training:	Epoch: [5][63/408]	Loss 0.5396 (0.3410)	
training:	Epoch: [5][64/408]	Loss 0.1591 (0.3381)	
training:	Epoch: [5][65/408]	Loss 0.0921 (0.3343)	
training:	Epoch: [5][66/408]	Loss 0.2185 (0.3326)	
training:	Epoch: [5][67/408]	Loss 0.4449 (0.3343)	
training:	Epoch: [5][68/408]	Loss 0.4750 (0.3363)	
training:	Epoch: [5][69/408]	Loss 0.2114 (0.3345)	
training:	Epoch: [5][70/408]	Loss 0.4892 (0.3367)	
training:	Epoch: [5][71/408]	Loss 0.2940 (0.3361)	
training:	Epoch: [5][72/408]	Loss 0.4141 (0.3372)	
training:	Epoch: [5][73/408]	Loss 0.6466 (0.3415)	
training:	Epoch: [5][74/408]	Loss 0.2493 (0.3402)	
training:	Epoch: [5][75/408]	Loss 0.5958 (0.3436)	
training:	Epoch: [5][76/408]	Loss 0.3493 (0.3437)	
training:	Epoch: [5][77/408]	Loss 0.1582 (0.3413)	
training:	Epoch: [5][78/408]	Loss 0.4825 (0.3431)	
training:	Epoch: [5][79/408]	Loss 0.2111 (0.3414)	
training:	Epoch: [5][80/408]	Loss 0.5245 (0.3437)	
training:	Epoch: [5][81/408]	Loss 0.3604 (0.3439)	
training:	Epoch: [5][82/408]	Loss 0.2547 (0.3428)	
training:	Epoch: [5][83/408]	Loss 0.3880 (0.3434)	
training:	Epoch: [5][84/408]	Loss 0.2322 (0.3420)	
training:	Epoch: [5][85/408]	Loss 0.4662 (0.3435)	
training:	Epoch: [5][86/408]	Loss 0.6834 (0.3475)	
training:	Epoch: [5][87/408]	Loss 0.5099 (0.3493)	
training:	Epoch: [5][88/408]	Loss 0.4690 (0.3507)	
training:	Epoch: [5][89/408]	Loss 0.2314 (0.3493)	
training:	Epoch: [5][90/408]	Loss 0.1905 (0.3476)	
training:	Epoch: [5][91/408]	Loss 0.2660 (0.3467)	
training:	Epoch: [5][92/408]	Loss 0.5264 (0.3486)	
training:	Epoch: [5][93/408]	Loss 0.2615 (0.3477)	
training:	Epoch: [5][94/408]	Loss 0.1721 (0.3458)	
training:	Epoch: [5][95/408]	Loss 0.3083 (0.3454)	
training:	Epoch: [5][96/408]	Loss 0.7141 (0.3493)	
training:	Epoch: [5][97/408]	Loss 0.5446 (0.3513)	
training:	Epoch: [5][98/408]	Loss 0.1984 (0.3497)	
training:	Epoch: [5][99/408]	Loss 0.4465 (0.3507)	
training:	Epoch: [5][100/408]	Loss 0.5388 (0.3526)	
training:	Epoch: [5][101/408]	Loss 0.3757 (0.3528)	
training:	Epoch: [5][102/408]	Loss 0.0890 (0.3502)	
training:	Epoch: [5][103/408]	Loss 0.1996 (0.3488)	
training:	Epoch: [5][104/408]	Loss 0.5541 (0.3507)	
training:	Epoch: [5][105/408]	Loss 0.2817 (0.3501)	
training:	Epoch: [5][106/408]	Loss 0.3141 (0.3497)	
training:	Epoch: [5][107/408]	Loss 0.2647 (0.3490)	
training:	Epoch: [5][108/408]	Loss 0.1449 (0.3471)	
training:	Epoch: [5][109/408]	Loss 0.4297 (0.3478)	
training:	Epoch: [5][110/408]	Loss 0.3198 (0.3476)	
training:	Epoch: [5][111/408]	Loss 0.4568 (0.3486)	
training:	Epoch: [5][112/408]	Loss 0.4045 (0.3491)	
training:	Epoch: [5][113/408]	Loss 0.1530 (0.3473)	
training:	Epoch: [5][114/408]	Loss 0.6351 (0.3498)	
training:	Epoch: [5][115/408]	Loss 0.2619 (0.3491)	
training:	Epoch: [5][116/408]	Loss 0.3580 (0.3492)	
training:	Epoch: [5][117/408]	Loss 0.2146 (0.3480)	
training:	Epoch: [5][118/408]	Loss 0.3261 (0.3478)	
training:	Epoch: [5][119/408]	Loss 0.3224 (0.3476)	
training:	Epoch: [5][120/408]	Loss 0.1836 (0.3462)	
training:	Epoch: [5][121/408]	Loss 0.6688 (0.3489)	
training:	Epoch: [5][122/408]	Loss 0.2992 (0.3485)	
training:	Epoch: [5][123/408]	Loss 0.2374 (0.3476)	
training:	Epoch: [5][124/408]	Loss 0.3242 (0.3474)	
training:	Epoch: [5][125/408]	Loss 0.1709 (0.3460)	
training:	Epoch: [5][126/408]	Loss 0.1650 (0.3446)	
training:	Epoch: [5][127/408]	Loss 0.1911 (0.3433)	
training:	Epoch: [5][128/408]	Loss 0.3074 (0.3431)	
training:	Epoch: [5][129/408]	Loss 0.6518 (0.3455)	
training:	Epoch: [5][130/408]	Loss 0.3013 (0.3451)	
training:	Epoch: [5][131/408]	Loss 0.3912 (0.3455)	
training:	Epoch: [5][132/408]	Loss 0.2904 (0.3451)	
training:	Epoch: [5][133/408]	Loss 0.2505 (0.3443)	
training:	Epoch: [5][134/408]	Loss 0.2520 (0.3437)	
training:	Epoch: [5][135/408]	Loss 0.2489 (0.3430)	
training:	Epoch: [5][136/408]	Loss 0.3095 (0.3427)	
training:	Epoch: [5][137/408]	Loss 0.2599 (0.3421)	
training:	Epoch: [5][138/408]	Loss 0.3370 (0.3421)	
training:	Epoch: [5][139/408]	Loss 0.0895 (0.3402)	
training:	Epoch: [5][140/408]	Loss 0.3426 (0.3403)	
training:	Epoch: [5][141/408]	Loss 0.2422 (0.3396)	
training:	Epoch: [5][142/408]	Loss 0.2308 (0.3388)	
training:	Epoch: [5][143/408]	Loss 0.5554 (0.3403)	
training:	Epoch: [5][144/408]	Loss 0.2228 (0.3395)	
training:	Epoch: [5][145/408]	Loss 0.2415 (0.3388)	
training:	Epoch: [5][146/408]	Loss 0.5726 (0.3404)	
training:	Epoch: [5][147/408]	Loss 0.2983 (0.3401)	
training:	Epoch: [5][148/408]	Loss 0.3289 (0.3401)	
training:	Epoch: [5][149/408]	Loss 0.6006 (0.3418)	
training:	Epoch: [5][150/408]	Loss 0.3549 (0.3419)	
training:	Epoch: [5][151/408]	Loss 0.2229 (0.3411)	
training:	Epoch: [5][152/408]	Loss 0.2202 (0.3403)	
training:	Epoch: [5][153/408]	Loss 0.2400 (0.3397)	
training:	Epoch: [5][154/408]	Loss 0.3320 (0.3396)	
training:	Epoch: [5][155/408]	Loss 0.4454 (0.3403)	
training:	Epoch: [5][156/408]	Loss 0.3817 (0.3406)	
training:	Epoch: [5][157/408]	Loss 0.3701 (0.3407)	
training:	Epoch: [5][158/408]	Loss 0.4759 (0.3416)	
training:	Epoch: [5][159/408]	Loss 0.3240 (0.3415)	
training:	Epoch: [5][160/408]	Loss 0.5717 (0.3429)	
training:	Epoch: [5][161/408]	Loss 0.1748 (0.3419)	
training:	Epoch: [5][162/408]	Loss 0.2527 (0.3413)	
training:	Epoch: [5][163/408]	Loss 0.1650 (0.3403)	
training:	Epoch: [5][164/408]	Loss 0.3308 (0.3402)	
training:	Epoch: [5][165/408]	Loss 0.3782 (0.3404)	
training:	Epoch: [5][166/408]	Loss 0.2217 (0.3397)	
training:	Epoch: [5][167/408]	Loss 0.3146 (0.3396)	
training:	Epoch: [5][168/408]	Loss 0.1750 (0.3386)	
training:	Epoch: [5][169/408]	Loss 0.3214 (0.3385)	
training:	Epoch: [5][170/408]	Loss 0.3577 (0.3386)	
training:	Epoch: [5][171/408]	Loss 0.2569 (0.3381)	
training:	Epoch: [5][172/408]	Loss 0.1259 (0.3369)	
training:	Epoch: [5][173/408]	Loss 0.1923 (0.3360)	
training:	Epoch: [5][174/408]	Loss 0.4886 (0.3369)	
training:	Epoch: [5][175/408]	Loss 0.3141 (0.3368)	
training:	Epoch: [5][176/408]	Loss 0.2450 (0.3363)	
training:	Epoch: [5][177/408]	Loss 0.3503 (0.3364)	
training:	Epoch: [5][178/408]	Loss 0.3816 (0.3366)	
training:	Epoch: [5][179/408]	Loss 0.3716 (0.3368)	
training:	Epoch: [5][180/408]	Loss 0.3543 (0.3369)	
training:	Epoch: [5][181/408]	Loss 0.2042 (0.3362)	
training:	Epoch: [5][182/408]	Loss 0.5404 (0.3373)	
training:	Epoch: [5][183/408]	Loss 0.4660 (0.3380)	
training:	Epoch: [5][184/408]	Loss 0.4567 (0.3386)	
training:	Epoch: [5][185/408]	Loss 0.3650 (0.3388)	
training:	Epoch: [5][186/408]	Loss 0.2441 (0.3383)	
training:	Epoch: [5][187/408]	Loss 0.1477 (0.3372)	
training:	Epoch: [5][188/408]	Loss 0.3936 (0.3375)	
training:	Epoch: [5][189/408]	Loss 0.1582 (0.3366)	
training:	Epoch: [5][190/408]	Loss 0.4007 (0.3369)	
training:	Epoch: [5][191/408]	Loss 0.4764 (0.3377)	
training:	Epoch: [5][192/408]	Loss 0.3375 (0.3377)	
training:	Epoch: [5][193/408]	Loss 0.3851 (0.3379)	
training:	Epoch: [5][194/408]	Loss 0.2531 (0.3375)	
training:	Epoch: [5][195/408]	Loss 0.0909 (0.3362)	
training:	Epoch: [5][196/408]	Loss 0.1348 (0.3352)	
training:	Epoch: [5][197/408]	Loss 0.3259 (0.3351)	
training:	Epoch: [5][198/408]	Loss 0.0881 (0.3339)	
training:	Epoch: [5][199/408]	Loss 0.4390 (0.3344)	
training:	Epoch: [5][200/408]	Loss 0.2431 (0.3340)	
training:	Epoch: [5][201/408]	Loss 0.4133 (0.3344)	
training:	Epoch: [5][202/408]	Loss 0.4446 (0.3349)	
training:	Epoch: [5][203/408]	Loss 0.2920 (0.3347)	
training:	Epoch: [5][204/408]	Loss 0.4526 (0.3353)	
training:	Epoch: [5][205/408]	Loss 0.2598 (0.3349)	
training:	Epoch: [5][206/408]	Loss 0.1799 (0.3341)	
training:	Epoch: [5][207/408]	Loss 0.5834 (0.3354)	
training:	Epoch: [5][208/408]	Loss 0.3831 (0.3356)	
training:	Epoch: [5][209/408]	Loss 0.2020 (0.3349)	
training:	Epoch: [5][210/408]	Loss 0.3219 (0.3349)	
training:	Epoch: [5][211/408]	Loss 0.3940 (0.3352)	
training:	Epoch: [5][212/408]	Loss 0.2725 (0.3349)	
training:	Epoch: [5][213/408]	Loss 0.2453 (0.3344)	
training:	Epoch: [5][214/408]	Loss 0.4362 (0.3349)	
training:	Epoch: [5][215/408]	Loss 0.3627 (0.3350)	
training:	Epoch: [5][216/408]	Loss 0.1828 (0.3343)	
training:	Epoch: [5][217/408]	Loss 0.4636 (0.3349)	
training:	Epoch: [5][218/408]	Loss 0.1243 (0.3340)	
training:	Epoch: [5][219/408]	Loss 0.0917 (0.3329)	
training:	Epoch: [5][220/408]	Loss 0.1496 (0.3320)	
training:	Epoch: [5][221/408]	Loss 0.3650 (0.3322)	
training:	Epoch: [5][222/408]	Loss 0.3619 (0.3323)	
training:	Epoch: [5][223/408]	Loss 0.4230 (0.3327)	
training:	Epoch: [5][224/408]	Loss 0.0725 (0.3316)	
training:	Epoch: [5][225/408]	Loss 0.1708 (0.3308)	
training:	Epoch: [5][226/408]	Loss 0.5842 (0.3320)	
training:	Epoch: [5][227/408]	Loss 0.2397 (0.3316)	
training:	Epoch: [5][228/408]	Loss 0.0987 (0.3305)	
training:	Epoch: [5][229/408]	Loss 0.1798 (0.3299)	
training:	Epoch: [5][230/408]	Loss 0.3692 (0.3301)	
training:	Epoch: [5][231/408]	Loss 0.2355 (0.3296)	
training:	Epoch: [5][232/408]	Loss 0.2093 (0.3291)	
training:	Epoch: [5][233/408]	Loss 0.2871 (0.3289)	
training:	Epoch: [5][234/408]	Loss 0.4093 (0.3293)	
training:	Epoch: [5][235/408]	Loss 0.1174 (0.3284)	
training:	Epoch: [5][236/408]	Loss 0.3843 (0.3286)	
training:	Epoch: [5][237/408]	Loss 0.4347 (0.3291)	
training:	Epoch: [5][238/408]	Loss 0.3746 (0.3293)	
training:	Epoch: [5][239/408]	Loss 0.1052 (0.3283)	
training:	Epoch: [5][240/408]	Loss 0.7386 (0.3300)	
training:	Epoch: [5][241/408]	Loss 0.6852 (0.3315)	
training:	Epoch: [5][242/408]	Loss 0.5505 (0.3324)	
training:	Epoch: [5][243/408]	Loss 0.6545 (0.3337)	
training:	Epoch: [5][244/408]	Loss 0.3771 (0.3339)	
training:	Epoch: [5][245/408]	Loss 0.3997 (0.3342)	
training:	Epoch: [5][246/408]	Loss 0.4714 (0.3347)	
training:	Epoch: [5][247/408]	Loss 0.1462 (0.3340)	
training:	Epoch: [5][248/408]	Loss 0.2756 (0.3337)	
training:	Epoch: [5][249/408]	Loss 0.2421 (0.3334)	
training:	Epoch: [5][250/408]	Loss 0.5398 (0.3342)	
training:	Epoch: [5][251/408]	Loss 0.5735 (0.3352)	
training:	Epoch: [5][252/408]	Loss 0.1590 (0.3345)	
training:	Epoch: [5][253/408]	Loss 0.2223 (0.3340)	
training:	Epoch: [5][254/408]	Loss 0.1490 (0.3333)	
training:	Epoch: [5][255/408]	Loss 0.1852 (0.3327)	
training:	Epoch: [5][256/408]	Loss 0.2824 (0.3325)	
training:	Epoch: [5][257/408]	Loss 0.2253 (0.3321)	
training:	Epoch: [5][258/408]	Loss 0.3273 (0.3321)	
training:	Epoch: [5][259/408]	Loss 0.5149 (0.3328)	
training:	Epoch: [5][260/408]	Loss 0.1749 (0.3322)	
training:	Epoch: [5][261/408]	Loss 0.3123 (0.3321)	
training:	Epoch: [5][262/408]	Loss 0.1880 (0.3315)	
training:	Epoch: [5][263/408]	Loss 0.3507 (0.3316)	
training:	Epoch: [5][264/408]	Loss 0.2099 (0.3312)	
training:	Epoch: [5][265/408]	Loss 0.3909 (0.3314)	
training:	Epoch: [5][266/408]	Loss 0.2359 (0.3310)	
training:	Epoch: [5][267/408]	Loss 0.3922 (0.3313)	
training:	Epoch: [5][268/408]	Loss 0.3339 (0.3313)	
training:	Epoch: [5][269/408]	Loss 0.3779 (0.3314)	
training:	Epoch: [5][270/408]	Loss 0.2603 (0.3312)	
training:	Epoch: [5][271/408]	Loss 0.4323 (0.3315)	
training:	Epoch: [5][272/408]	Loss 0.5444 (0.3323)	
training:	Epoch: [5][273/408]	Loss 0.2573 (0.3321)	
training:	Epoch: [5][274/408]	Loss 0.6008 (0.3330)	
training:	Epoch: [5][275/408]	Loss 0.2095 (0.3326)	
training:	Epoch: [5][276/408]	Loss 0.4917 (0.3332)	
training:	Epoch: [5][277/408]	Loss 0.2917 (0.3330)	
training:	Epoch: [5][278/408]	Loss 0.2320 (0.3326)	
training:	Epoch: [5][279/408]	Loss 0.4467 (0.3331)	
training:	Epoch: [5][280/408]	Loss 0.2584 (0.3328)	
training:	Epoch: [5][281/408]	Loss 0.2735 (0.3326)	
training:	Epoch: [5][282/408]	Loss 0.2012 (0.3321)	
training:	Epoch: [5][283/408]	Loss 0.4151 (0.3324)	
training:	Epoch: [5][284/408]	Loss 0.2553 (0.3321)	
training:	Epoch: [5][285/408]	Loss 0.4174 (0.3324)	
training:	Epoch: [5][286/408]	Loss 0.2274 (0.3321)	
training:	Epoch: [5][287/408]	Loss 0.1336 (0.3314)	
training:	Epoch: [5][288/408]	Loss 0.3064 (0.3313)	
training:	Epoch: [5][289/408]	Loss 0.3894 (0.3315)	
training:	Epoch: [5][290/408]	Loss 0.3761 (0.3316)	
training:	Epoch: [5][291/408]	Loss 0.4297 (0.3320)	
training:	Epoch: [5][292/408]	Loss 0.2378 (0.3317)	
training:	Epoch: [5][293/408]	Loss 0.4435 (0.3320)	
training:	Epoch: [5][294/408]	Loss 0.1886 (0.3316)	
training:	Epoch: [5][295/408]	Loss 0.3123 (0.3315)	
training:	Epoch: [5][296/408]	Loss 0.3534 (0.3316)	
training:	Epoch: [5][297/408]	Loss 0.7178 (0.3329)	
training:	Epoch: [5][298/408]	Loss 0.2364 (0.3325)	
training:	Epoch: [5][299/408]	Loss 0.3002 (0.3324)	
training:	Epoch: [5][300/408]	Loss 0.1770 (0.3319)	
training:	Epoch: [5][301/408]	Loss 0.2409 (0.3316)	
training:	Epoch: [5][302/408]	Loss 0.4018 (0.3318)	
training:	Epoch: [5][303/408]	Loss 0.2602 (0.3316)	
training:	Epoch: [5][304/408]	Loss 0.5553 (0.3323)	
training:	Epoch: [5][305/408]	Loss 0.4920 (0.3329)	
training:	Epoch: [5][306/408]	Loss 0.3213 (0.3328)	
training:	Epoch: [5][307/408]	Loss 0.6314 (0.3338)	
training:	Epoch: [5][308/408]	Loss 0.5224 (0.3344)	
training:	Epoch: [5][309/408]	Loss 0.3685 (0.3345)	
training:	Epoch: [5][310/408]	Loss 0.2534 (0.3343)	
training:	Epoch: [5][311/408]	Loss 0.1899 (0.3338)	
training:	Epoch: [5][312/408]	Loss 0.2284 (0.3335)	
training:	Epoch: [5][313/408]	Loss 0.3645 (0.3336)	
training:	Epoch: [5][314/408]	Loss 0.3733 (0.3337)	
training:	Epoch: [5][315/408]	Loss 0.3199 (0.3336)	
training:	Epoch: [5][316/408]	Loss 0.2316 (0.3333)	
training:	Epoch: [5][317/408]	Loss 0.2838 (0.3332)	
training:	Epoch: [5][318/408]	Loss 0.3046 (0.3331)	
training:	Epoch: [5][319/408]	Loss 0.5463 (0.3337)	
training:	Epoch: [5][320/408]	Loss 0.3587 (0.3338)	
training:	Epoch: [5][321/408]	Loss 0.2621 (0.3336)	
training:	Epoch: [5][322/408]	Loss 0.3383 (0.3336)	
training:	Epoch: [5][323/408]	Loss 0.4094 (0.3338)	
training:	Epoch: [5][324/408]	Loss 0.3132 (0.3338)	
training:	Epoch: [5][325/408]	Loss 0.2250 (0.3334)	
training:	Epoch: [5][326/408]	Loss 0.5396 (0.3341)	
training:	Epoch: [5][327/408]	Loss 0.3662 (0.3342)	
training:	Epoch: [5][328/408]	Loss 0.2303 (0.3339)	
training:	Epoch: [5][329/408]	Loss 0.2744 (0.3337)	
training:	Epoch: [5][330/408]	Loss 0.3017 (0.3336)	
training:	Epoch: [5][331/408]	Loss 0.4560 (0.3339)	
training:	Epoch: [5][332/408]	Loss 0.1873 (0.3335)	
training:	Epoch: [5][333/408]	Loss 0.3789 (0.3336)	
training:	Epoch: [5][334/408]	Loss 0.1886 (0.3332)	
training:	Epoch: [5][335/408]	Loss 0.2590 (0.3330)	
training:	Epoch: [5][336/408]	Loss 0.2394 (0.3327)	
training:	Epoch: [5][337/408]	Loss 0.1965 (0.3323)	
training:	Epoch: [5][338/408]	Loss 0.4453 (0.3326)	
training:	Epoch: [5][339/408]	Loss 0.3310 (0.3326)	
training:	Epoch: [5][340/408]	Loss 0.2517 (0.3324)	
training:	Epoch: [5][341/408]	Loss 0.5432 (0.3330)	
training:	Epoch: [5][342/408]	Loss 0.3430 (0.3330)	
training:	Epoch: [5][343/408]	Loss 0.3167 (0.3330)	
training:	Epoch: [5][344/408]	Loss 0.3990 (0.3332)	
training:	Epoch: [5][345/408]	Loss 0.1532 (0.3327)	
training:	Epoch: [5][346/408]	Loss 0.4192 (0.3329)	
training:	Epoch: [5][347/408]	Loss 0.2142 (0.3326)	
training:	Epoch: [5][348/408]	Loss 0.3145 (0.3325)	
training:	Epoch: [5][349/408]	Loss 0.3983 (0.3327)	
training:	Epoch: [5][350/408]	Loss 0.3765 (0.3328)	
training:	Epoch: [5][351/408]	Loss 0.3509 (0.3329)	
training:	Epoch: [5][352/408]	Loss 0.3033 (0.3328)	
training:	Epoch: [5][353/408]	Loss 0.4429 (0.3331)	
training:	Epoch: [5][354/408]	Loss 0.2084 (0.3328)	
training:	Epoch: [5][355/408]	Loss 0.1794 (0.3323)	
training:	Epoch: [5][356/408]	Loss 0.2355 (0.3321)	
training:	Epoch: [5][357/408]	Loss 0.2536 (0.3318)	
training:	Epoch: [5][358/408]	Loss 0.3734 (0.3320)	
training:	Epoch: [5][359/408]	Loss 0.2521 (0.3317)	
training:	Epoch: [5][360/408]	Loss 0.2530 (0.3315)	
training:	Epoch: [5][361/408]	Loss 0.1554 (0.3310)	
training:	Epoch: [5][362/408]	Loss 0.3888 (0.3312)	
training:	Epoch: [5][363/408]	Loss 0.2089 (0.3308)	
training:	Epoch: [5][364/408]	Loss 0.5401 (0.3314)	
training:	Epoch: [5][365/408]	Loss 0.3603 (0.3315)	
training:	Epoch: [5][366/408]	Loss 0.4021 (0.3317)	
training:	Epoch: [5][367/408]	Loss 0.4553 (0.3320)	
training:	Epoch: [5][368/408]	Loss 0.3630 (0.3321)	
training:	Epoch: [5][369/408]	Loss 0.3905 (0.3323)	
training:	Epoch: [5][370/408]	Loss 0.2739 (0.3321)	
training:	Epoch: [5][371/408]	Loss 0.2584 (0.3319)	
training:	Epoch: [5][372/408]	Loss 0.2793 (0.3318)	
training:	Epoch: [5][373/408]	Loss 0.3052 (0.3317)	
training:	Epoch: [5][374/408]	Loss 0.3451 (0.3317)	
training:	Epoch: [5][375/408]	Loss 0.6713 (0.3326)	
training:	Epoch: [5][376/408]	Loss 0.2635 (0.3325)	
training:	Epoch: [5][377/408]	Loss 0.4329 (0.3327)	
training:	Epoch: [5][378/408]	Loss 0.4356 (0.3330)	
training:	Epoch: [5][379/408]	Loss 0.2264 (0.3327)	
training:	Epoch: [5][380/408]	Loss 0.3785 (0.3328)	
training:	Epoch: [5][381/408]	Loss 0.1764 (0.3324)	
training:	Epoch: [5][382/408]	Loss 0.6557 (0.3333)	
training:	Epoch: [5][383/408]	Loss 0.2937 (0.3332)	
training:	Epoch: [5][384/408]	Loss 0.1837 (0.3328)	
training:	Epoch: [5][385/408]	Loss 0.3806 (0.3329)	
training:	Epoch: [5][386/408]	Loss 0.3524 (0.3330)	
training:	Epoch: [5][387/408]	Loss 0.0691 (0.3323)	
training:	Epoch: [5][388/408]	Loss 0.4465 (0.3326)	
training:	Epoch: [5][389/408]	Loss 0.3852 (0.3327)	
training:	Epoch: [5][390/408]	Loss 0.1762 (0.3323)	
training:	Epoch: [5][391/408]	Loss 0.2670 (0.3321)	
training:	Epoch: [5][392/408]	Loss 0.2038 (0.3318)	
training:	Epoch: [5][393/408]	Loss 0.4143 (0.3320)	
training:	Epoch: [5][394/408]	Loss 0.7321 (0.3330)	
training:	Epoch: [5][395/408]	Loss 0.4750 (0.3334)	
training:	Epoch: [5][396/408]	Loss 0.2798 (0.3333)	
training:	Epoch: [5][397/408]	Loss 0.2168 (0.3330)	
training:	Epoch: [5][398/408]	Loss 0.1925 (0.3326)	
training:	Epoch: [5][399/408]	Loss 0.2945 (0.3325)	
training:	Epoch: [5][400/408]	Loss 0.3756 (0.3326)	
training:	Epoch: [5][401/408]	Loss 0.1014 (0.3320)	
training:	Epoch: [5][402/408]	Loss 0.2342 (0.3318)	
training:	Epoch: [5][403/408]	Loss 0.2938 (0.3317)	
training:	Epoch: [5][404/408]	Loss 0.3881 (0.3319)	
training:	Epoch: [5][405/408]	Loss 0.3185 (0.3318)	
training:	Epoch: [5][406/408]	Loss 0.2742 (0.3317)	
training:	Epoch: [5][407/408]	Loss 0.2117 (0.3314)	
training:	Epoch: [5][408/408]	Loss 0.1492 (0.3309)	
Training:	 Loss: 0.3304

Training:	 ACC: 0.8788 0.8804 0.9189 0.8386
Validation:	 ACC: 0.8226 0.8250 0.8772 0.7679
Validation:	 Best_BACC: 0.8226 0.8250 0.8772 0.7679
Validation:	 Loss: 0.4083
Pretraining:	Epoch 6/200
----------
training:	Epoch: [6][1/408]	Loss 0.2586 (0.2586)	
training:	Epoch: [6][2/408]	Loss 0.1714 (0.2150)	
training:	Epoch: [6][3/408]	Loss 0.5939 (0.3413)	
training:	Epoch: [6][4/408]	Loss 0.1672 (0.2978)	
training:	Epoch: [6][5/408]	Loss 0.3957 (0.3174)	
training:	Epoch: [6][6/408]	Loss 0.3390 (0.3210)	
training:	Epoch: [6][7/408]	Loss 0.3193 (0.3207)	
training:	Epoch: [6][8/408]	Loss 0.2876 (0.3166)	
training:	Epoch: [6][9/408]	Loss 0.3732 (0.3229)	
training:	Epoch: [6][10/408]	Loss 0.3133 (0.3219)	
training:	Epoch: [6][11/408]	Loss 0.2173 (0.3124)	
training:	Epoch: [6][12/408]	Loss 0.2993 (0.3113)	
training:	Epoch: [6][13/408]	Loss 0.2416 (0.3059)	
training:	Epoch: [6][14/408]	Loss 0.3061 (0.3060)	
training:	Epoch: [6][15/408]	Loss 0.5609 (0.3229)	
training:	Epoch: [6][16/408]	Loss 0.3054 (0.3219)	
training:	Epoch: [6][17/408]	Loss 0.4474 (0.3292)	
training:	Epoch: [6][18/408]	Loss 0.2691 (0.3259)	
training:	Epoch: [6][19/408]	Loss 0.0897 (0.3135)	
training:	Epoch: [6][20/408]	Loss 0.4132 (0.3185)	
training:	Epoch: [6][21/408]	Loss 0.2337 (0.3144)	
training:	Epoch: [6][22/408]	Loss 0.3181 (0.3146)	
training:	Epoch: [6][23/408]	Loss 0.5644 (0.3254)	
training:	Epoch: [6][24/408]	Loss 0.5874 (0.3364)	
training:	Epoch: [6][25/408]	Loss 0.1918 (0.3306)	
training:	Epoch: [6][26/408]	Loss 0.3378 (0.3309)	
training:	Epoch: [6][27/408]	Loss 0.2498 (0.3279)	
training:	Epoch: [6][28/408]	Loss 0.3475 (0.3286)	
training:	Epoch: [6][29/408]	Loss 0.1608 (0.3228)	
training:	Epoch: [6][30/408]	Loss 0.1325 (0.3164)	
training:	Epoch: [6][31/408]	Loss 0.1549 (0.3112)	
training:	Epoch: [6][32/408]	Loss 0.4469 (0.3155)	
training:	Epoch: [6][33/408]	Loss 0.3375 (0.3161)	
training:	Epoch: [6][34/408]	Loss 0.4652 (0.3205)	
training:	Epoch: [6][35/408]	Loss 0.2107 (0.3174)	
training:	Epoch: [6][36/408]	Loss 0.3175 (0.3174)	
training:	Epoch: [6][37/408]	Loss 0.4965 (0.3222)	
training:	Epoch: [6][38/408]	Loss 0.3030 (0.3217)	
training:	Epoch: [6][39/408]	Loss 0.5773 (0.3283)	
training:	Epoch: [6][40/408]	Loss 0.2796 (0.3270)	
training:	Epoch: [6][41/408]	Loss 0.3290 (0.3271)	
training:	Epoch: [6][42/408]	Loss 0.0792 (0.3212)	
training:	Epoch: [6][43/408]	Loss 0.2277 (0.3190)	
training:	Epoch: [6][44/408]	Loss 0.2117 (0.3166)	
training:	Epoch: [6][45/408]	Loss 0.1127 (0.3120)	
training:	Epoch: [6][46/408]	Loss 0.4083 (0.3141)	
training:	Epoch: [6][47/408]	Loss 0.2465 (0.3127)	
training:	Epoch: [6][48/408]	Loss 0.2640 (0.3117)	
training:	Epoch: [6][49/408]	Loss 0.2481 (0.3104)	
training:	Epoch: [6][50/408]	Loss 0.1692 (0.3076)	
training:	Epoch: [6][51/408]	Loss 0.2239 (0.3059)	
training:	Epoch: [6][52/408]	Loss 0.2554 (0.3050)	
training:	Epoch: [6][53/408]	Loss 0.2785 (0.3045)	
training:	Epoch: [6][54/408]	Loss 0.2365 (0.3032)	
training:	Epoch: [6][55/408]	Loss 0.3420 (0.3039)	
training:	Epoch: [6][56/408]	Loss 0.1154 (0.3005)	
training:	Epoch: [6][57/408]	Loss 0.3241 (0.3009)	
training:	Epoch: [6][58/408]	Loss 0.2265 (0.2997)	
training:	Epoch: [6][59/408]	Loss 0.4847 (0.3028)	
training:	Epoch: [6][60/408]	Loss 0.4067 (0.3045)	
training:	Epoch: [6][61/408]	Loss 0.5290 (0.3082)	
training:	Epoch: [6][62/408]	Loss 0.3119 (0.3083)	
training:	Epoch: [6][63/408]	Loss 0.0899 (0.3048)	
training:	Epoch: [6][64/408]	Loss 0.3073 (0.3048)	
training:	Epoch: [6][65/408]	Loss 0.1973 (0.3032)	
training:	Epoch: [6][66/408]	Loss 0.1737 (0.3012)	
training:	Epoch: [6][67/408]	Loss 0.3317 (0.3017)	
training:	Epoch: [6][68/408]	Loss 0.1519 (0.2995)	
training:	Epoch: [6][69/408]	Loss 0.3329 (0.3000)	
training:	Epoch: [6][70/408]	Loss 0.1254 (0.2975)	
training:	Epoch: [6][71/408]	Loss 0.3654 (0.2984)	
training:	Epoch: [6][72/408]	Loss 0.3052 (0.2985)	
training:	Epoch: [6][73/408]	Loss 0.1572 (0.2966)	
training:	Epoch: [6][74/408]	Loss 0.2661 (0.2962)	
training:	Epoch: [6][75/408]	Loss 0.4992 (0.2989)	
training:	Epoch: [6][76/408]	Loss 0.3616 (0.2997)	
training:	Epoch: [6][77/408]	Loss 0.3436 (0.3003)	
training:	Epoch: [6][78/408]	Loss 0.1354 (0.2982)	
training:	Epoch: [6][79/408]	Loss 0.2880 (0.2980)	
training:	Epoch: [6][80/408]	Loss 0.1404 (0.2961)	
training:	Epoch: [6][81/408]	Loss 0.2228 (0.2952)	
training:	Epoch: [6][82/408]	Loss 0.1146 (0.2930)	
training:	Epoch: [6][83/408]	Loss 0.0805 (0.2904)	
training:	Epoch: [6][84/408]	Loss 0.0793 (0.2879)	
training:	Epoch: [6][85/408]	Loss 0.3518 (0.2886)	
training:	Epoch: [6][86/408]	Loss 0.2099 (0.2877)	
training:	Epoch: [6][87/408]	Loss 0.2539 (0.2873)	
training:	Epoch: [6][88/408]	Loss 0.4521 (0.2892)	
training:	Epoch: [6][89/408]	Loss 0.2080 (0.2883)	
training:	Epoch: [6][90/408]	Loss 0.2241 (0.2876)	
training:	Epoch: [6][91/408]	Loss 0.1365 (0.2859)	
training:	Epoch: [6][92/408]	Loss 0.1925 (0.2849)	
training:	Epoch: [6][93/408]	Loss 0.3168 (0.2852)	
training:	Epoch: [6][94/408]	Loss 0.1796 (0.2841)	
training:	Epoch: [6][95/408]	Loss 0.2184 (0.2834)	
training:	Epoch: [6][96/408]	Loss 0.1792 (0.2823)	
training:	Epoch: [6][97/408]	Loss 0.3333 (0.2829)	
training:	Epoch: [6][98/408]	Loss 0.3164 (0.2832)	
training:	Epoch: [6][99/408]	Loss 0.3983 (0.2844)	
training:	Epoch: [6][100/408]	Loss 0.1291 (0.2828)	
training:	Epoch: [6][101/408]	Loss 1.1448 (0.2914)	
training:	Epoch: [6][102/408]	Loss 0.1158 (0.2896)	
training:	Epoch: [6][103/408]	Loss 0.2256 (0.2890)	
training:	Epoch: [6][104/408]	Loss 0.2941 (0.2891)	
training:	Epoch: [6][105/408]	Loss 0.2145 (0.2884)	
training:	Epoch: [6][106/408]	Loss 0.3386 (0.2888)	
training:	Epoch: [6][107/408]	Loss 0.1040 (0.2871)	
training:	Epoch: [6][108/408]	Loss 0.3699 (0.2879)	
training:	Epoch: [6][109/408]	Loss 0.3376 (0.2883)	
training:	Epoch: [6][110/408]	Loss 0.2014 (0.2875)	
training:	Epoch: [6][111/408]	Loss 0.2882 (0.2875)	
training:	Epoch: [6][112/408]	Loss 0.3110 (0.2877)	
training:	Epoch: [6][113/408]	Loss 0.2186 (0.2871)	
training:	Epoch: [6][114/408]	Loss 0.2569 (0.2869)	
training:	Epoch: [6][115/408]	Loss 0.1702 (0.2859)	
training:	Epoch: [6][116/408]	Loss 0.4147 (0.2870)	
training:	Epoch: [6][117/408]	Loss 0.2477 (0.2866)	
training:	Epoch: [6][118/408]	Loss 0.2303 (0.2862)	
training:	Epoch: [6][119/408]	Loss 0.6002 (0.2888)	
training:	Epoch: [6][120/408]	Loss 0.4361 (0.2900)	
training:	Epoch: [6][121/408]	Loss 0.2836 (0.2900)	
training:	Epoch: [6][122/408]	Loss 0.3578 (0.2905)	
training:	Epoch: [6][123/408]	Loss 0.3102 (0.2907)	
training:	Epoch: [6][124/408]	Loss 0.2084 (0.2900)	
training:	Epoch: [6][125/408]	Loss 0.2362 (0.2896)	
training:	Epoch: [6][126/408]	Loss 0.5789 (0.2919)	
training:	Epoch: [6][127/408]	Loss 0.3022 (0.2920)	
training:	Epoch: [6][128/408]	Loss 0.2422 (0.2916)	
training:	Epoch: [6][129/408]	Loss 0.1975 (0.2908)	
training:	Epoch: [6][130/408]	Loss 0.3825 (0.2916)	
training:	Epoch: [6][131/408]	Loss 0.5821 (0.2938)	
training:	Epoch: [6][132/408]	Loss 0.1948 (0.2930)	
training:	Epoch: [6][133/408]	Loss 0.5853 (0.2952)	
training:	Epoch: [6][134/408]	Loss 0.2768 (0.2951)	
training:	Epoch: [6][135/408]	Loss 0.2216 (0.2945)	
training:	Epoch: [6][136/408]	Loss 0.4389 (0.2956)	
training:	Epoch: [6][137/408]	Loss 0.2008 (0.2949)	
training:	Epoch: [6][138/408]	Loss 0.4682 (0.2962)	
training:	Epoch: [6][139/408]	Loss 0.3214 (0.2963)	
training:	Epoch: [6][140/408]	Loss 0.4982 (0.2978)	
training:	Epoch: [6][141/408]	Loss 0.3329 (0.2980)	
training:	Epoch: [6][142/408]	Loss 0.4314 (0.2990)	
training:	Epoch: [6][143/408]	Loss 0.1395 (0.2979)	
training:	Epoch: [6][144/408]	Loss 0.3202 (0.2980)	
training:	Epoch: [6][145/408]	Loss 0.3630 (0.2985)	
training:	Epoch: [6][146/408]	Loss 0.1503 (0.2974)	
training:	Epoch: [6][147/408]	Loss 0.2808 (0.2973)	
training:	Epoch: [6][148/408]	Loss 0.1762 (0.2965)	
training:	Epoch: [6][149/408]	Loss 0.1739 (0.2957)	
training:	Epoch: [6][150/408]	Loss 0.1867 (0.2950)	
training:	Epoch: [6][151/408]	Loss 0.2305 (0.2945)	
training:	Epoch: [6][152/408]	Loss 0.3737 (0.2951)	
training:	Epoch: [6][153/408]	Loss 0.2098 (0.2945)	
training:	Epoch: [6][154/408]	Loss 0.0855 (0.2931)	
training:	Epoch: [6][155/408]	Loss 0.1032 (0.2919)	
training:	Epoch: [6][156/408]	Loss 0.3500 (0.2923)	
training:	Epoch: [6][157/408]	Loss 0.4434 (0.2933)	
training:	Epoch: [6][158/408]	Loss 0.2343 (0.2929)	
training:	Epoch: [6][159/408]	Loss 0.2454 (0.2926)	
training:	Epoch: [6][160/408]	Loss 0.2601 (0.2924)	
training:	Epoch: [6][161/408]	Loss 0.4026 (0.2931)	
training:	Epoch: [6][162/408]	Loss 0.1678 (0.2923)	
training:	Epoch: [6][163/408]	Loss 0.2092 (0.2918)	
training:	Epoch: [6][164/408]	Loss 0.1262 (0.2908)	
training:	Epoch: [6][165/408]	Loss 0.1511 (0.2899)	
training:	Epoch: [6][166/408]	Loss 0.4375 (0.2908)	
training:	Epoch: [6][167/408]	Loss 0.3003 (0.2909)	
training:	Epoch: [6][168/408]	Loss 0.3018 (0.2909)	
training:	Epoch: [6][169/408]	Loss 0.4364 (0.2918)	
training:	Epoch: [6][170/408]	Loss 0.4175 (0.2925)	
training:	Epoch: [6][171/408]	Loss 0.5907 (0.2943)	
training:	Epoch: [6][172/408]	Loss 0.2747 (0.2942)	
training:	Epoch: [6][173/408]	Loss 0.5279 (0.2955)	
training:	Epoch: [6][174/408]	Loss 0.1896 (0.2949)	
training:	Epoch: [6][175/408]	Loss 0.1274 (0.2940)	
training:	Epoch: [6][176/408]	Loss 0.2839 (0.2939)	
training:	Epoch: [6][177/408]	Loss 0.1578 (0.2931)	
training:	Epoch: [6][178/408]	Loss 0.3279 (0.2933)	
training:	Epoch: [6][179/408]	Loss 0.5016 (0.2945)	
training:	Epoch: [6][180/408]	Loss 0.5410 (0.2959)	
training:	Epoch: [6][181/408]	Loss 0.6139 (0.2976)	
training:	Epoch: [6][182/408]	Loss 0.1916 (0.2970)	
training:	Epoch: [6][183/408]	Loss 0.4947 (0.2981)	
training:	Epoch: [6][184/408]	Loss 0.3774 (0.2985)	
training:	Epoch: [6][185/408]	Loss 0.4898 (0.2996)	
training:	Epoch: [6][186/408]	Loss 0.1769 (0.2989)	
training:	Epoch: [6][187/408]	Loss 0.1653 (0.2982)	
training:	Epoch: [6][188/408]	Loss 0.2794 (0.2981)	
training:	Epoch: [6][189/408]	Loss 0.2720 (0.2980)	
training:	Epoch: [6][190/408]	Loss 0.2051 (0.2975)	
training:	Epoch: [6][191/408]	Loss 0.1579 (0.2967)	
training:	Epoch: [6][192/408]	Loss 0.3940 (0.2972)	
training:	Epoch: [6][193/408]	Loss 0.6420 (0.2990)	
training:	Epoch: [6][194/408]	Loss 0.7022 (0.3011)	
training:	Epoch: [6][195/408]	Loss 0.1581 (0.3004)	
training:	Epoch: [6][196/408]	Loss 0.1985 (0.2999)	
training:	Epoch: [6][197/408]	Loss 0.1933 (0.2993)	
training:	Epoch: [6][198/408]	Loss 0.2821 (0.2992)	
training:	Epoch: [6][199/408]	Loss 0.2785 (0.2991)	
training:	Epoch: [6][200/408]	Loss 0.5005 (0.3001)	
training:	Epoch: [6][201/408]	Loss 0.2024 (0.2996)	
training:	Epoch: [6][202/408]	Loss 0.2657 (0.2995)	
training:	Epoch: [6][203/408]	Loss 0.4624 (0.3003)	
training:	Epoch: [6][204/408]	Loss 0.5252 (0.3014)	
training:	Epoch: [6][205/408]	Loss 0.2340 (0.3011)	
training:	Epoch: [6][206/408]	Loss 0.4800 (0.3019)	
training:	Epoch: [6][207/408]	Loss 0.2540 (0.3017)	
training:	Epoch: [6][208/408]	Loss 0.4432 (0.3024)	
training:	Epoch: [6][209/408]	Loss 0.2269 (0.3020)	
training:	Epoch: [6][210/408]	Loss 0.1707 (0.3014)	
training:	Epoch: [6][211/408]	Loss 0.0636 (0.3003)	
training:	Epoch: [6][212/408]	Loss 0.5737 (0.3016)	
training:	Epoch: [6][213/408]	Loss 0.2570 (0.3013)	
training:	Epoch: [6][214/408]	Loss 0.2869 (0.3013)	
training:	Epoch: [6][215/408]	Loss 0.4185 (0.3018)	
training:	Epoch: [6][216/408]	Loss 0.2477 (0.3016)	
training:	Epoch: [6][217/408]	Loss 0.2928 (0.3015)	
training:	Epoch: [6][218/408]	Loss 0.3979 (0.3020)	
training:	Epoch: [6][219/408]	Loss 0.3433 (0.3022)	
training:	Epoch: [6][220/408]	Loss 0.1630 (0.3015)	
training:	Epoch: [6][221/408]	Loss 0.3552 (0.3018)	
training:	Epoch: [6][222/408]	Loss 0.5357 (0.3028)	
training:	Epoch: [6][223/408]	Loss 0.2601 (0.3026)	
training:	Epoch: [6][224/408]	Loss 0.0852 (0.3017)	
training:	Epoch: [6][225/408]	Loss 0.7052 (0.3035)	
training:	Epoch: [6][226/408]	Loss 0.2735 (0.3033)	
training:	Epoch: [6][227/408]	Loss 0.1724 (0.3027)	
training:	Epoch: [6][228/408]	Loss 0.0957 (0.3018)	
training:	Epoch: [6][229/408]	Loss 0.4501 (0.3025)	
training:	Epoch: [6][230/408]	Loss 0.2265 (0.3022)	
training:	Epoch: [6][231/408]	Loss 0.1635 (0.3016)	
training:	Epoch: [6][232/408]	Loss 0.2909 (0.3015)	
training:	Epoch: [6][233/408]	Loss 0.0770 (0.3005)	
training:	Epoch: [6][234/408]	Loss 0.1863 (0.3001)	
training:	Epoch: [6][235/408]	Loss 0.3344 (0.3002)	
training:	Epoch: [6][236/408]	Loss 0.1035 (0.2994)	
training:	Epoch: [6][237/408]	Loss 0.3673 (0.2997)	
training:	Epoch: [6][238/408]	Loss 0.5097 (0.3005)	
training:	Epoch: [6][239/408]	Loss 0.3627 (0.3008)	
training:	Epoch: [6][240/408]	Loss 0.1978 (0.3004)	
training:	Epoch: [6][241/408]	Loss 0.3787 (0.3007)	
training:	Epoch: [6][242/408]	Loss 0.1043 (0.2999)	
training:	Epoch: [6][243/408]	Loss 0.2173 (0.2995)	
training:	Epoch: [6][244/408]	Loss 0.5252 (0.3005)	
training:	Epoch: [6][245/408]	Loss 0.3002 (0.3005)	
training:	Epoch: [6][246/408]	Loss 0.2618 (0.3003)	
training:	Epoch: [6][247/408]	Loss 0.1462 (0.2997)	
training:	Epoch: [6][248/408]	Loss 0.4095 (0.3001)	
training:	Epoch: [6][249/408]	Loss 0.1752 (0.2996)	
training:	Epoch: [6][250/408]	Loss 0.2786 (0.2995)	
training:	Epoch: [6][251/408]	Loss 0.2390 (0.2993)	
training:	Epoch: [6][252/408]	Loss 0.3490 (0.2995)	
training:	Epoch: [6][253/408]	Loss 0.1530 (0.2989)	
training:	Epoch: [6][254/408]	Loss 0.3282 (0.2990)	
training:	Epoch: [6][255/408]	Loss 0.4243 (0.2995)	
training:	Epoch: [6][256/408]	Loss 0.2346 (0.2993)	
training:	Epoch: [6][257/408]	Loss 0.0991 (0.2985)	
training:	Epoch: [6][258/408]	Loss 0.4697 (0.2992)	
training:	Epoch: [6][259/408]	Loss 0.1333 (0.2985)	
training:	Epoch: [6][260/408]	Loss 0.2717 (0.2984)	
training:	Epoch: [6][261/408]	Loss 0.2468 (0.2982)	
training:	Epoch: [6][262/408]	Loss 0.2391 (0.2980)	
training:	Epoch: [6][263/408]	Loss 0.2683 (0.2979)	
training:	Epoch: [6][264/408]	Loss 0.3008 (0.2979)	
training:	Epoch: [6][265/408]	Loss 0.3000 (0.2979)	
training:	Epoch: [6][266/408]	Loss 0.3526 (0.2981)	
training:	Epoch: [6][267/408]	Loss 0.1838 (0.2977)	
training:	Epoch: [6][268/408]	Loss 0.2455 (0.2975)	
training:	Epoch: [6][269/408]	Loss 0.5488 (0.2984)	
training:	Epoch: [6][270/408]	Loss 0.2735 (0.2983)	
training:	Epoch: [6][271/408]	Loss 0.0809 (0.2975)	
training:	Epoch: [6][272/408]	Loss 0.1163 (0.2969)	
training:	Epoch: [6][273/408]	Loss 0.5431 (0.2978)	
training:	Epoch: [6][274/408]	Loss 0.1917 (0.2974)	
training:	Epoch: [6][275/408]	Loss 0.2417 (0.2972)	
training:	Epoch: [6][276/408]	Loss 0.2881 (0.2971)	
training:	Epoch: [6][277/408]	Loss 0.4255 (0.2976)	
training:	Epoch: [6][278/408]	Loss 0.3522 (0.2978)	
training:	Epoch: [6][279/408]	Loss 0.3447 (0.2980)	
training:	Epoch: [6][280/408]	Loss 0.1842 (0.2976)	
training:	Epoch: [6][281/408]	Loss 0.6341 (0.2988)	
training:	Epoch: [6][282/408]	Loss 0.1380 (0.2982)	
training:	Epoch: [6][283/408]	Loss 0.3459 (0.2984)	
training:	Epoch: [6][284/408]	Loss 0.3961 (0.2987)	
training:	Epoch: [6][285/408]	Loss 0.1307 (0.2981)	
training:	Epoch: [6][286/408]	Loss 0.3376 (0.2982)	
training:	Epoch: [6][287/408]	Loss 0.4245 (0.2987)	
training:	Epoch: [6][288/408]	Loss 0.2724 (0.2986)	
training:	Epoch: [6][289/408]	Loss 0.3133 (0.2986)	
training:	Epoch: [6][290/408]	Loss 0.4650 (0.2992)	
training:	Epoch: [6][291/408]	Loss 0.2452 (0.2990)	
training:	Epoch: [6][292/408]	Loss 0.3492 (0.2992)	
training:	Epoch: [6][293/408]	Loss 0.3197 (0.2993)	
training:	Epoch: [6][294/408]	Loss 0.3930 (0.2996)	
training:	Epoch: [6][295/408]	Loss 0.4945 (0.3003)	
training:	Epoch: [6][296/408]	Loss 0.2091 (0.2999)	
training:	Epoch: [6][297/408]	Loss 0.4129 (0.3003)	
training:	Epoch: [6][298/408]	Loss 0.5933 (0.3013)	
training:	Epoch: [6][299/408]	Loss 0.2322 (0.3011)	
training:	Epoch: [6][300/408]	Loss 0.2611 (0.3009)	
training:	Epoch: [6][301/408]	Loss 0.2757 (0.3009)	
training:	Epoch: [6][302/408]	Loss 0.1411 (0.3003)	
training:	Epoch: [6][303/408]	Loss 0.0922 (0.2996)	
training:	Epoch: [6][304/408]	Loss 0.4231 (0.3001)	
training:	Epoch: [6][305/408]	Loss 0.0924 (0.2994)	
training:	Epoch: [6][306/408]	Loss 0.5044 (0.3000)	
training:	Epoch: [6][307/408]	Loss 0.4898 (0.3007)	
training:	Epoch: [6][308/408]	Loss 0.2450 (0.3005)	
training:	Epoch: [6][309/408]	Loss 0.5360 (0.3012)	
training:	Epoch: [6][310/408]	Loss 0.3724 (0.3015)	
training:	Epoch: [6][311/408]	Loss 0.4720 (0.3020)	
training:	Epoch: [6][312/408]	Loss 0.2331 (0.3018)	
training:	Epoch: [6][313/408]	Loss 0.2699 (0.3017)	
training:	Epoch: [6][314/408]	Loss 0.1553 (0.3012)	
training:	Epoch: [6][315/408]	Loss 0.1776 (0.3008)	
training:	Epoch: [6][316/408]	Loss 0.4189 (0.3012)	
training:	Epoch: [6][317/408]	Loss 0.6090 (0.3022)	
training:	Epoch: [6][318/408]	Loss 0.1696 (0.3018)	
training:	Epoch: [6][319/408]	Loss 0.5046 (0.3024)	
training:	Epoch: [6][320/408]	Loss 0.1974 (0.3021)	
training:	Epoch: [6][321/408]	Loss 0.3276 (0.3022)	
training:	Epoch: [6][322/408]	Loss 0.3957 (0.3024)	
training:	Epoch: [6][323/408]	Loss 0.3716 (0.3027)	
training:	Epoch: [6][324/408]	Loss 0.2505 (0.3025)	
training:	Epoch: [6][325/408]	Loss 0.5520 (0.3033)	
training:	Epoch: [6][326/408]	Loss 0.3246 (0.3033)	
training:	Epoch: [6][327/408]	Loss 0.2476 (0.3032)	
training:	Epoch: [6][328/408]	Loss 0.2961 (0.3031)	
training:	Epoch: [6][329/408]	Loss 0.1930 (0.3028)	
training:	Epoch: [6][330/408]	Loss 0.3835 (0.3030)	
training:	Epoch: [6][331/408]	Loss 0.3881 (0.3033)	
training:	Epoch: [6][332/408]	Loss 0.2673 (0.3032)	
training:	Epoch: [6][333/408]	Loss 0.2731 (0.3031)	
training:	Epoch: [6][334/408]	Loss 0.4041 (0.3034)	
training:	Epoch: [6][335/408]	Loss 0.2225 (0.3032)	
training:	Epoch: [6][336/408]	Loss 0.3074 (0.3032)	
training:	Epoch: [6][337/408]	Loss 0.1733 (0.3028)	
training:	Epoch: [6][338/408]	Loss 0.0531 (0.3021)	
training:	Epoch: [6][339/408]	Loss 0.1464 (0.3016)	
training:	Epoch: [6][340/408]	Loss 0.3815 (0.3018)	
training:	Epoch: [6][341/408]	Loss 0.4639 (0.3023)	
training:	Epoch: [6][342/408]	Loss 0.4341 (0.3027)	
training:	Epoch: [6][343/408]	Loss 0.2560 (0.3026)	
training:	Epoch: [6][344/408]	Loss 0.2734 (0.3025)	
training:	Epoch: [6][345/408]	Loss 0.1649 (0.3021)	
training:	Epoch: [6][346/408]	Loss 0.2064 (0.3018)	
training:	Epoch: [6][347/408]	Loss 0.3675 (0.3020)	
training:	Epoch: [6][348/408]	Loss 0.4381 (0.3024)	
training:	Epoch: [6][349/408]	Loss 0.4272 (0.3027)	
training:	Epoch: [6][350/408]	Loss 0.5015 (0.3033)	
training:	Epoch: [6][351/408]	Loss 0.4729 (0.3038)	
training:	Epoch: [6][352/408]	Loss 0.3552 (0.3039)	
training:	Epoch: [6][353/408]	Loss 0.1124 (0.3034)	
training:	Epoch: [6][354/408]	Loss 0.5184 (0.3040)	
training:	Epoch: [6][355/408]	Loss 0.2811 (0.3039)	
training:	Epoch: [6][356/408]	Loss 0.2156 (0.3037)	
training:	Epoch: [6][357/408]	Loss 0.2202 (0.3034)	
training:	Epoch: [6][358/408]	Loss 0.3487 (0.3036)	
training:	Epoch: [6][359/408]	Loss 0.4103 (0.3039)	
training:	Epoch: [6][360/408]	Loss 0.3994 (0.3041)	
training:	Epoch: [6][361/408]	Loss 0.2663 (0.3040)	
training:	Epoch: [6][362/408]	Loss 0.3911 (0.3043)	
training:	Epoch: [6][363/408]	Loss 0.3980 (0.3045)	
training:	Epoch: [6][364/408]	Loss 0.3137 (0.3046)	
training:	Epoch: [6][365/408]	Loss 0.2650 (0.3044)	
training:	Epoch: [6][366/408]	Loss 0.1001 (0.3039)	
training:	Epoch: [6][367/408]	Loss 0.2783 (0.3038)	
training:	Epoch: [6][368/408]	Loss 0.2065 (0.3036)	
training:	Epoch: [6][369/408]	Loss 0.1869 (0.3032)	
training:	Epoch: [6][370/408]	Loss 0.3751 (0.3034)	
training:	Epoch: [6][371/408]	Loss 0.3407 (0.3035)	
training:	Epoch: [6][372/408]	Loss 0.1654 (0.3032)	
training:	Epoch: [6][373/408]	Loss 0.2695 (0.3031)	
training:	Epoch: [6][374/408]	Loss 0.2282 (0.3029)	
training:	Epoch: [6][375/408]	Loss 0.1887 (0.3026)	
training:	Epoch: [6][376/408]	Loss 0.4105 (0.3029)	
training:	Epoch: [6][377/408]	Loss 0.3084 (0.3029)	
training:	Epoch: [6][378/408]	Loss 0.5655 (0.3036)	
training:	Epoch: [6][379/408]	Loss 0.3518 (0.3037)	
training:	Epoch: [6][380/408]	Loss 0.2312 (0.3035)	
training:	Epoch: [6][381/408]	Loss 0.3763 (0.3037)	
training:	Epoch: [6][382/408]	Loss 0.3856 (0.3039)	
training:	Epoch: [6][383/408]	Loss 0.4878 (0.3044)	
training:	Epoch: [6][384/408]	Loss 0.2060 (0.3041)	
training:	Epoch: [6][385/408]	Loss 0.2539 (0.3040)	
training:	Epoch: [6][386/408]	Loss 0.4145 (0.3043)	
training:	Epoch: [6][387/408]	Loss 0.2877 (0.3042)	
training:	Epoch: [6][388/408]	Loss 0.3893 (0.3045)	
training:	Epoch: [6][389/408]	Loss 0.3507 (0.3046)	
training:	Epoch: [6][390/408]	Loss 0.5975 (0.3053)	
training:	Epoch: [6][391/408]	Loss 0.4551 (0.3057)	
training:	Epoch: [6][392/408]	Loss 0.5694 (0.3064)	
training:	Epoch: [6][393/408]	Loss 0.3730 (0.3066)	
training:	Epoch: [6][394/408]	Loss 0.2585 (0.3064)	
training:	Epoch: [6][395/408]	Loss 0.2675 (0.3063)	
training:	Epoch: [6][396/408]	Loss 0.1031 (0.3058)	
training:	Epoch: [6][397/408]	Loss 0.1988 (0.3056)	
training:	Epoch: [6][398/408]	Loss 0.2814 (0.3055)	
training:	Epoch: [6][399/408]	Loss 0.3557 (0.3056)	
training:	Epoch: [6][400/408]	Loss 0.1715 (0.3053)	
training:	Epoch: [6][401/408]	Loss 0.2459 (0.3051)	
training:	Epoch: [6][402/408]	Loss 0.2911 (0.3051)	
training:	Epoch: [6][403/408]	Loss 0.3996 (0.3053)	
training:	Epoch: [6][404/408]	Loss 0.3024 (0.3053)	
training:	Epoch: [6][405/408]	Loss 0.4826 (0.3058)	
training:	Epoch: [6][406/408]	Loss 0.4932 (0.3062)	
training:	Epoch: [6][407/408]	Loss 0.3685 (0.3064)	
training:	Epoch: [6][408/408]	Loss 0.4718 (0.3068)	
Training:	 Loss: 0.3063

Training:	 ACC: 0.8809 0.8830 0.9333 0.8284
Validation:	 ACC: 0.8170 0.8202 0.8874 0.7466
Validation:	 Best_BACC: 0.8226 0.8250 0.8772 0.7679
Validation:	 Loss: 0.4199
Pretraining:	Epoch 7/200
----------
training:	Epoch: [7][1/408]	Loss 0.4065 (0.4065)	
training:	Epoch: [7][2/408]	Loss 0.3133 (0.3599)	
training:	Epoch: [7][3/408]	Loss 0.2827 (0.3342)	
training:	Epoch: [7][4/408]	Loss 0.2881 (0.3227)	
training:	Epoch: [7][5/408]	Loss 0.4621 (0.3505)	
training:	Epoch: [7][6/408]	Loss 0.3063 (0.3432)	
training:	Epoch: [7][7/408]	Loss 0.2467 (0.3294)	
training:	Epoch: [7][8/408]	Loss 0.3018 (0.3259)	
training:	Epoch: [7][9/408]	Loss 0.2581 (0.3184)	
training:	Epoch: [7][10/408]	Loss 0.2051 (0.3071)	
training:	Epoch: [7][11/408]	Loss 0.4460 (0.3197)	
training:	Epoch: [7][12/408]	Loss 0.2068 (0.3103)	
training:	Epoch: [7][13/408]	Loss 0.2302 (0.3041)	
training:	Epoch: [7][14/408]	Loss 0.2556 (0.3007)	
training:	Epoch: [7][15/408]	Loss 0.4095 (0.3079)	
training:	Epoch: [7][16/408]	Loss 0.1847 (0.3002)	
training:	Epoch: [7][17/408]	Loss 0.4307 (0.3079)	
training:	Epoch: [7][18/408]	Loss 0.3728 (0.3115)	
training:	Epoch: [7][19/408]	Loss 0.2105 (0.3062)	
training:	Epoch: [7][20/408]	Loss 0.1673 (0.2992)	
training:	Epoch: [7][21/408]	Loss 0.3492 (0.3016)	
training:	Epoch: [7][22/408]	Loss 0.3407 (0.3034)	
training:	Epoch: [7][23/408]	Loss 0.2170 (0.2996)	
training:	Epoch: [7][24/408]	Loss 0.1870 (0.2949)	
training:	Epoch: [7][25/408]	Loss 0.1550 (0.2893)	
training:	Epoch: [7][26/408]	Loss 0.5232 (0.2983)	
training:	Epoch: [7][27/408]	Loss 0.2116 (0.2951)	
training:	Epoch: [7][28/408]	Loss 0.0801 (0.2875)	
training:	Epoch: [7][29/408]	Loss 0.0707 (0.2800)	
training:	Epoch: [7][30/408]	Loss 0.2973 (0.2806)	
training:	Epoch: [7][31/408]	Loss 0.1890 (0.2776)	
training:	Epoch: [7][32/408]	Loss 0.3331 (0.2793)	
training:	Epoch: [7][33/408]	Loss 0.3410 (0.2812)	
training:	Epoch: [7][34/408]	Loss 0.3493 (0.2832)	
training:	Epoch: [7][35/408]	Loss 0.2311 (0.2817)	
training:	Epoch: [7][36/408]	Loss 0.1534 (0.2782)	
training:	Epoch: [7][37/408]	Loss 0.3502 (0.2801)	
training:	Epoch: [7][38/408]	Loss 0.1390 (0.2764)	
training:	Epoch: [7][39/408]	Loss 0.5895 (0.2844)	
training:	Epoch: [7][40/408]	Loss 0.8582 (0.2988)	
training:	Epoch: [7][41/408]	Loss 0.2187 (0.2968)	
training:	Epoch: [7][42/408]	Loss 0.1006 (0.2921)	
training:	Epoch: [7][43/408]	Loss 0.3174 (0.2927)	
training:	Epoch: [7][44/408]	Loss 0.1280 (0.2890)	
training:	Epoch: [7][45/408]	Loss 0.4286 (0.2921)	
training:	Epoch: [7][46/408]	Loss 0.3298 (0.2929)	
training:	Epoch: [7][47/408]	Loss 0.2523 (0.2920)	
training:	Epoch: [7][48/408]	Loss 0.5505 (0.2974)	
training:	Epoch: [7][49/408]	Loss 0.3243 (0.2980)	
training:	Epoch: [7][50/408]	Loss 0.2377 (0.2968)	
training:	Epoch: [7][51/408]	Loss 0.2712 (0.2963)	
training:	Epoch: [7][52/408]	Loss 0.2357 (0.2951)	
training:	Epoch: [7][53/408]	Loss 0.3251 (0.2957)	
training:	Epoch: [7][54/408]	Loss 0.2530 (0.2949)	
training:	Epoch: [7][55/408]	Loss 0.1253 (0.2918)	
training:	Epoch: [7][56/408]	Loss 0.4593 (0.2948)	
training:	Epoch: [7][57/408]	Loss 0.3567 (0.2959)	
training:	Epoch: [7][58/408]	Loss 0.2537 (0.2951)	
training:	Epoch: [7][59/408]	Loss 0.4840 (0.2983)	
training:	Epoch: [7][60/408]	Loss 0.2479 (0.2975)	
training:	Epoch: [7][61/408]	Loss 0.2955 (0.2975)	
training:	Epoch: [7][62/408]	Loss 0.1290 (0.2948)	
training:	Epoch: [7][63/408]	Loss 0.2944 (0.2947)	
training:	Epoch: [7][64/408]	Loss 0.4345 (0.2969)	
training:	Epoch: [7][65/408]	Loss 0.1040 (0.2940)	
training:	Epoch: [7][66/408]	Loss 0.1662 (0.2920)	
training:	Epoch: [7][67/408]	Loss 0.3384 (0.2927)	
training:	Epoch: [7][68/408]	Loss 0.3290 (0.2932)	
training:	Epoch: [7][69/408]	Loss 0.4327 (0.2953)	
training:	Epoch: [7][70/408]	Loss 0.0973 (0.2924)	
training:	Epoch: [7][71/408]	Loss 0.2282 (0.2915)	
training:	Epoch: [7][72/408]	Loss 0.3907 (0.2929)	
training:	Epoch: [7][73/408]	Loss 0.2305 (0.2921)	
training:	Epoch: [7][74/408]	Loss 0.4864 (0.2947)	
training:	Epoch: [7][75/408]	Loss 0.2694 (0.2943)	
training:	Epoch: [7][76/408]	Loss 0.2209 (0.2934)	
training:	Epoch: [7][77/408]	Loss 0.3356 (0.2939)	
training:	Epoch: [7][78/408]	Loss 0.2450 (0.2933)	
training:	Epoch: [7][79/408]	Loss 0.2859 (0.2932)	
training:	Epoch: [7][80/408]	Loss 0.2812 (0.2931)	
training:	Epoch: [7][81/408]	Loss 0.1728 (0.2916)	
training:	Epoch: [7][82/408]	Loss 0.2078 (0.2906)	
training:	Epoch: [7][83/408]	Loss 0.1745 (0.2892)	
training:	Epoch: [7][84/408]	Loss 0.2483 (0.2887)	
training:	Epoch: [7][85/408]	Loss 0.3173 (0.2890)	
training:	Epoch: [7][86/408]	Loss 0.1668 (0.2876)	
training:	Epoch: [7][87/408]	Loss 0.4060 (0.2889)	
training:	Epoch: [7][88/408]	Loss 0.2341 (0.2883)	
training:	Epoch: [7][89/408]	Loss 0.2025 (0.2874)	
training:	Epoch: [7][90/408]	Loss 0.1325 (0.2856)	
training:	Epoch: [7][91/408]	Loss 0.3109 (0.2859)	
training:	Epoch: [7][92/408]	Loss 0.2774 (0.2858)	
training:	Epoch: [7][93/408]	Loss 0.3227 (0.2862)	
training:	Epoch: [7][94/408]	Loss 0.1113 (0.2844)	
training:	Epoch: [7][95/408]	Loss 0.3018 (0.2845)	
training:	Epoch: [7][96/408]	Loss 0.2690 (0.2844)	
training:	Epoch: [7][97/408]	Loss 0.1230 (0.2827)	
training:	Epoch: [7][98/408]	Loss 0.5719 (0.2857)	
training:	Epoch: [7][99/408]	Loss 0.3218 (0.2860)	
training:	Epoch: [7][100/408]	Loss 0.2198 (0.2854)	
training:	Epoch: [7][101/408]	Loss 0.1208 (0.2837)	
training:	Epoch: [7][102/408]	Loss 0.3058 (0.2840)	
training:	Epoch: [7][103/408]	Loss 0.2238 (0.2834)	
training:	Epoch: [7][104/408]	Loss 0.1633 (0.2822)	
training:	Epoch: [7][105/408]	Loss 0.3150 (0.2825)	
training:	Epoch: [7][106/408]	Loss 0.1221 (0.2810)	
training:	Epoch: [7][107/408]	Loss 0.5008 (0.2831)	
training:	Epoch: [7][108/408]	Loss 0.5928 (0.2859)	
training:	Epoch: [7][109/408]	Loss 0.5126 (0.2880)	
training:	Epoch: [7][110/408]	Loss 0.2911 (0.2880)	
training:	Epoch: [7][111/408]	Loss 0.2302 (0.2875)	
training:	Epoch: [7][112/408]	Loss 0.0537 (0.2854)	
training:	Epoch: [7][113/408]	Loss 0.3549 (0.2861)	
training:	Epoch: [7][114/408]	Loss 0.2157 (0.2854)	
training:	Epoch: [7][115/408]	Loss 0.3351 (0.2859)	
training:	Epoch: [7][116/408]	Loss 0.3522 (0.2864)	
training:	Epoch: [7][117/408]	Loss 0.1843 (0.2856)	
training:	Epoch: [7][118/408]	Loss 0.1354 (0.2843)	
training:	Epoch: [7][119/408]	Loss 0.1864 (0.2835)	
training:	Epoch: [7][120/408]	Loss 0.0981 (0.2819)	
training:	Epoch: [7][121/408]	Loss 0.3056 (0.2821)	
training:	Epoch: [7][122/408]	Loss 0.4915 (0.2838)	
training:	Epoch: [7][123/408]	Loss 0.3071 (0.2840)	
training:	Epoch: [7][124/408]	Loss 0.1010 (0.2826)	
training:	Epoch: [7][125/408]	Loss 0.2489 (0.2823)	
training:	Epoch: [7][126/408]	Loss 0.2185 (0.2818)	
training:	Epoch: [7][127/408]	Loss 0.2417 (0.2815)	
training:	Epoch: [7][128/408]	Loss 0.2014 (0.2808)	
training:	Epoch: [7][129/408]	Loss 0.2836 (0.2809)	
training:	Epoch: [7][130/408]	Loss 0.1534 (0.2799)	
training:	Epoch: [7][131/408]	Loss 0.1843 (0.2791)	
training:	Epoch: [7][132/408]	Loss 0.4360 (0.2803)	
training:	Epoch: [7][133/408]	Loss 0.1698 (0.2795)	
training:	Epoch: [7][134/408]	Loss 0.2280 (0.2791)	
training:	Epoch: [7][135/408]	Loss 0.3221 (0.2794)	
training:	Epoch: [7][136/408]	Loss 0.1759 (0.2787)	
training:	Epoch: [7][137/408]	Loss 0.3140 (0.2789)	
training:	Epoch: [7][138/408]	Loss 0.3626 (0.2795)	
training:	Epoch: [7][139/408]	Loss 0.2674 (0.2795)	
training:	Epoch: [7][140/408]	Loss 0.2837 (0.2795)	
training:	Epoch: [7][141/408]	Loss 0.1431 (0.2785)	
training:	Epoch: [7][142/408]	Loss 0.1237 (0.2774)	
training:	Epoch: [7][143/408]	Loss 0.1554 (0.2766)	
training:	Epoch: [7][144/408]	Loss 0.1453 (0.2757)	
training:	Epoch: [7][145/408]	Loss 0.1867 (0.2750)	
training:	Epoch: [7][146/408]	Loss 0.3803 (0.2758)	
training:	Epoch: [7][147/408]	Loss 0.2823 (0.2758)	
training:	Epoch: [7][148/408]	Loss 0.1236 (0.2748)	
training:	Epoch: [7][149/408]	Loss 0.0741 (0.2734)	
training:	Epoch: [7][150/408]	Loss 0.1854 (0.2728)	
training:	Epoch: [7][151/408]	Loss 0.1069 (0.2717)	
training:	Epoch: [7][152/408]	Loss 0.1977 (0.2713)	
training:	Epoch: [7][153/408]	Loss 0.3744 (0.2719)	
training:	Epoch: [7][154/408]	Loss 0.4825 (0.2733)	
training:	Epoch: [7][155/408]	Loss 0.3055 (0.2735)	
training:	Epoch: [7][156/408]	Loss 0.4538 (0.2747)	
training:	Epoch: [7][157/408]	Loss 0.5214 (0.2762)	
training:	Epoch: [7][158/408]	Loss 0.0958 (0.2751)	
training:	Epoch: [7][159/408]	Loss 0.3540 (0.2756)	
training:	Epoch: [7][160/408]	Loss 0.2570 (0.2755)	
training:	Epoch: [7][161/408]	Loss 0.3385 (0.2759)	
training:	Epoch: [7][162/408]	Loss 0.3143 (0.2761)	
training:	Epoch: [7][163/408]	Loss 0.0889 (0.2750)	
training:	Epoch: [7][164/408]	Loss 0.6177 (0.2770)	
training:	Epoch: [7][165/408]	Loss 0.2840 (0.2771)	
training:	Epoch: [7][166/408]	Loss 0.3112 (0.2773)	
training:	Epoch: [7][167/408]	Loss 0.4242 (0.2782)	
training:	Epoch: [7][168/408]	Loss 0.0847 (0.2770)	
training:	Epoch: [7][169/408]	Loss 0.3120 (0.2772)	
training:	Epoch: [7][170/408]	Loss 0.3583 (0.2777)	
training:	Epoch: [7][171/408]	Loss 0.1796 (0.2771)	
training:	Epoch: [7][172/408]	Loss 0.1707 (0.2765)	
training:	Epoch: [7][173/408]	Loss 0.2874 (0.2766)	
training:	Epoch: [7][174/408]	Loss 0.2275 (0.2763)	
training:	Epoch: [7][175/408]	Loss 0.2323 (0.2760)	
training:	Epoch: [7][176/408]	Loss 0.3976 (0.2767)	
training:	Epoch: [7][177/408]	Loss 0.3417 (0.2771)	
training:	Epoch: [7][178/408]	Loss 0.2158 (0.2768)	
training:	Epoch: [7][179/408]	Loss 0.7332 (0.2793)	
training:	Epoch: [7][180/408]	Loss 0.0945 (0.2783)	
training:	Epoch: [7][181/408]	Loss 0.5267 (0.2797)	
training:	Epoch: [7][182/408]	Loss 0.2392 (0.2794)	
training:	Epoch: [7][183/408]	Loss 0.2131 (0.2791)	
training:	Epoch: [7][184/408]	Loss 0.0874 (0.2780)	
training:	Epoch: [7][185/408]	Loss 0.1704 (0.2774)	
training:	Epoch: [7][186/408]	Loss 0.1432 (0.2767)	
training:	Epoch: [7][187/408]	Loss 0.3984 (0.2774)	
training:	Epoch: [7][188/408]	Loss 0.3759 (0.2779)	
training:	Epoch: [7][189/408]	Loss 0.2743 (0.2779)	
training:	Epoch: [7][190/408]	Loss 0.3977 (0.2785)	
training:	Epoch: [7][191/408]	Loss 0.0739 (0.2774)	
training:	Epoch: [7][192/408]	Loss 0.4491 (0.2783)	
training:	Epoch: [7][193/408]	Loss 0.2105 (0.2780)	
training:	Epoch: [7][194/408]	Loss 0.2288 (0.2777)	
training:	Epoch: [7][195/408]	Loss 0.3121 (0.2779)	
training:	Epoch: [7][196/408]	Loss 0.2398 (0.2777)	
training:	Epoch: [7][197/408]	Loss 0.3997 (0.2783)	
training:	Epoch: [7][198/408]	Loss 0.4008 (0.2789)	
training:	Epoch: [7][199/408]	Loss 0.3822 (0.2795)	
training:	Epoch: [7][200/408]	Loss 0.0489 (0.2783)	
training:	Epoch: [7][201/408]	Loss 0.3437 (0.2786)	
training:	Epoch: [7][202/408]	Loss 0.1503 (0.2780)	
training:	Epoch: [7][203/408]	Loss 0.2038 (0.2776)	
training:	Epoch: [7][204/408]	Loss 0.1369 (0.2769)	
training:	Epoch: [7][205/408]	Loss 0.4380 (0.2777)	
training:	Epoch: [7][206/408]	Loss 0.4571 (0.2786)	
training:	Epoch: [7][207/408]	Loss 0.1341 (0.2779)	
training:	Epoch: [7][208/408]	Loss 0.3211 (0.2781)	
training:	Epoch: [7][209/408]	Loss 0.1545 (0.2775)	
training:	Epoch: [7][210/408]	Loss 0.0824 (0.2766)	
training:	Epoch: [7][211/408]	Loss 0.5231 (0.2778)	
training:	Epoch: [7][212/408]	Loss 0.3414 (0.2781)	
training:	Epoch: [7][213/408]	Loss 0.2879 (0.2781)	
training:	Epoch: [7][214/408]	Loss 0.4503 (0.2789)	
training:	Epoch: [7][215/408]	Loss 0.1919 (0.2785)	
training:	Epoch: [7][216/408]	Loss 0.3863 (0.2790)	
training:	Epoch: [7][217/408]	Loss 0.0694 (0.2780)	
training:	Epoch: [7][218/408]	Loss 0.7210 (0.2801)	
training:	Epoch: [7][219/408]	Loss 0.3022 (0.2802)	
training:	Epoch: [7][220/408]	Loss 0.1050 (0.2794)	
training:	Epoch: [7][221/408]	Loss 0.2351 (0.2792)	
training:	Epoch: [7][222/408]	Loss 0.4229 (0.2798)	
training:	Epoch: [7][223/408]	Loss 0.2646 (0.2798)	
training:	Epoch: [7][224/408]	Loss 0.2077 (0.2794)	
training:	Epoch: [7][225/408]	Loss 0.2609 (0.2794)	
training:	Epoch: [7][226/408]	Loss 0.1640 (0.2788)	
training:	Epoch: [7][227/408]	Loss 0.1175 (0.2781)	
training:	Epoch: [7][228/408]	Loss 0.2464 (0.2780)	
training:	Epoch: [7][229/408]	Loss 0.2581 (0.2779)	
training:	Epoch: [7][230/408]	Loss 0.1253 (0.2772)	
training:	Epoch: [7][231/408]	Loss 0.3031 (0.2774)	
training:	Epoch: [7][232/408]	Loss 0.3721 (0.2778)	
training:	Epoch: [7][233/408]	Loss 0.1620 (0.2773)	
training:	Epoch: [7][234/408]	Loss 0.5918 (0.2786)	
training:	Epoch: [7][235/408]	Loss 0.2090 (0.2783)	
training:	Epoch: [7][236/408]	Loss 0.6756 (0.2800)	
training:	Epoch: [7][237/408]	Loss 0.3327 (0.2802)	
training:	Epoch: [7][238/408]	Loss 0.1623 (0.2797)	
training:	Epoch: [7][239/408]	Loss 0.1459 (0.2792)	
training:	Epoch: [7][240/408]	Loss 0.2308 (0.2790)	
training:	Epoch: [7][241/408]	Loss 0.2791 (0.2790)	
training:	Epoch: [7][242/408]	Loss 0.5035 (0.2799)	
training:	Epoch: [7][243/408]	Loss 0.3035 (0.2800)	
training:	Epoch: [7][244/408]	Loss 0.1333 (0.2794)	
training:	Epoch: [7][245/408]	Loss 0.1737 (0.2790)	
training:	Epoch: [7][246/408]	Loss 0.2163 (0.2787)	
training:	Epoch: [7][247/408]	Loss 0.3939 (0.2792)	
training:	Epoch: [7][248/408]	Loss 0.4767 (0.2800)	
training:	Epoch: [7][249/408]	Loss 0.2111 (0.2797)	
training:	Epoch: [7][250/408]	Loss 0.3013 (0.2798)	
training:	Epoch: [7][251/408]	Loss 0.2662 (0.2797)	
training:	Epoch: [7][252/408]	Loss 0.2485 (0.2796)	
training:	Epoch: [7][253/408]	Loss 0.4287 (0.2802)	
training:	Epoch: [7][254/408]	Loss 0.8098 (0.2823)	
training:	Epoch: [7][255/408]	Loss 0.1885 (0.2819)	
training:	Epoch: [7][256/408]	Loss 0.2333 (0.2817)	
training:	Epoch: [7][257/408]	Loss 0.1943 (0.2814)	
training:	Epoch: [7][258/408]	Loss 0.3764 (0.2817)	
training:	Epoch: [7][259/408]	Loss 0.2223 (0.2815)	
training:	Epoch: [7][260/408]	Loss 0.4526 (0.2822)	
training:	Epoch: [7][261/408]	Loss 0.6807 (0.2837)	
training:	Epoch: [7][262/408]	Loss 0.2901 (0.2837)	
training:	Epoch: [7][263/408]	Loss 0.3677 (0.2840)	
training:	Epoch: [7][264/408]	Loss 0.1752 (0.2836)	
training:	Epoch: [7][265/408]	Loss 0.2680 (0.2836)	
training:	Epoch: [7][266/408]	Loss 0.4360 (0.2841)	
training:	Epoch: [7][267/408]	Loss 0.3270 (0.2843)	
training:	Epoch: [7][268/408]	Loss 0.3159 (0.2844)	
training:	Epoch: [7][269/408]	Loss 0.2299 (0.2842)	
training:	Epoch: [7][270/408]	Loss 0.7579 (0.2860)	
training:	Epoch: [7][271/408]	Loss 0.2336 (0.2858)	
training:	Epoch: [7][272/408]	Loss 0.2589 (0.2857)	
training:	Epoch: [7][273/408]	Loss 0.3120 (0.2858)	
training:	Epoch: [7][274/408]	Loss 0.1551 (0.2853)	
training:	Epoch: [7][275/408]	Loss 0.0809 (0.2846)	
training:	Epoch: [7][276/408]	Loss 0.2564 (0.2845)	
training:	Epoch: [7][277/408]	Loss 0.4468 (0.2850)	
training:	Epoch: [7][278/408]	Loss 0.1335 (0.2845)	
training:	Epoch: [7][279/408]	Loss 0.3442 (0.2847)	
training:	Epoch: [7][280/408]	Loss 0.3213 (0.2848)	
training:	Epoch: [7][281/408]	Loss 0.2468 (0.2847)	
training:	Epoch: [7][282/408]	Loss 0.3797 (0.2850)	
training:	Epoch: [7][283/408]	Loss 0.1593 (0.2846)	
training:	Epoch: [7][284/408]	Loss 0.3349 (0.2848)	
training:	Epoch: [7][285/408]	Loss 0.2771 (0.2847)	
training:	Epoch: [7][286/408]	Loss 0.1467 (0.2843)	
training:	Epoch: [7][287/408]	Loss 0.5006 (0.2850)	
training:	Epoch: [7][288/408]	Loss 0.0632 (0.2842)	
training:	Epoch: [7][289/408]	Loss 0.2632 (0.2842)	
training:	Epoch: [7][290/408]	Loss 0.2889 (0.2842)	
training:	Epoch: [7][291/408]	Loss 0.3424 (0.2844)	
training:	Epoch: [7][292/408]	Loss 0.4509 (0.2850)	
training:	Epoch: [7][293/408]	Loss 0.2267 (0.2848)	
training:	Epoch: [7][294/408]	Loss 0.3289 (0.2849)	
training:	Epoch: [7][295/408]	Loss 0.4717 (0.2855)	
training:	Epoch: [7][296/408]	Loss 0.2847 (0.2855)	
training:	Epoch: [7][297/408]	Loss 0.1916 (0.2852)	
training:	Epoch: [7][298/408]	Loss 0.3067 (0.2853)	
training:	Epoch: [7][299/408]	Loss 0.5059 (0.2860)	
training:	Epoch: [7][300/408]	Loss 0.4317 (0.2865)	
training:	Epoch: [7][301/408]	Loss 0.2591 (0.2864)	
training:	Epoch: [7][302/408]	Loss 0.3018 (0.2865)	
training:	Epoch: [7][303/408]	Loss 0.2094 (0.2862)	
training:	Epoch: [7][304/408]	Loss 0.3206 (0.2863)	
training:	Epoch: [7][305/408]	Loss 0.2126 (0.2861)	
training:	Epoch: [7][306/408]	Loss 0.2370 (0.2859)	
training:	Epoch: [7][307/408]	Loss 0.4622 (0.2865)	
training:	Epoch: [7][308/408]	Loss 0.2624 (0.2864)	
training:	Epoch: [7][309/408]	Loss 0.1868 (0.2861)	
training:	Epoch: [7][310/408]	Loss 0.3332 (0.2863)	
training:	Epoch: [7][311/408]	Loss 0.2620 (0.2862)	
training:	Epoch: [7][312/408]	Loss 0.3478 (0.2864)	
training:	Epoch: [7][313/408]	Loss 0.1938 (0.2861)	
training:	Epoch: [7][314/408]	Loss 0.3189 (0.2862)	
training:	Epoch: [7][315/408]	Loss 0.3456 (0.2864)	
training:	Epoch: [7][316/408]	Loss 0.3185 (0.2865)	
training:	Epoch: [7][317/408]	Loss 0.4233 (0.2869)	
training:	Epoch: [7][318/408]	Loss 0.1463 (0.2865)	
training:	Epoch: [7][319/408]	Loss 0.3597 (0.2867)	
training:	Epoch: [7][320/408]	Loss 0.4412 (0.2872)	
training:	Epoch: [7][321/408]	Loss 0.2071 (0.2869)	
training:	Epoch: [7][322/408]	Loss 0.1590 (0.2865)	
training:	Epoch: [7][323/408]	Loss 0.3171 (0.2866)	
training:	Epoch: [7][324/408]	Loss 0.1419 (0.2862)	
training:	Epoch: [7][325/408]	Loss 0.2272 (0.2860)	
training:	Epoch: [7][326/408]	Loss 0.1923 (0.2857)	
training:	Epoch: [7][327/408]	Loss 0.2464 (0.2856)	
training:	Epoch: [7][328/408]	Loss 0.1433 (0.2852)	
training:	Epoch: [7][329/408]	Loss 0.3054 (0.2852)	
training:	Epoch: [7][330/408]	Loss 0.2303 (0.2851)	
training:	Epoch: [7][331/408]	Loss 0.1970 (0.2848)	
training:	Epoch: [7][332/408]	Loss 0.1646 (0.2844)	
training:	Epoch: [7][333/408]	Loss 0.2788 (0.2844)	
training:	Epoch: [7][334/408]	Loss 0.2092 (0.2842)	
training:	Epoch: [7][335/408]	Loss 0.3063 (0.2843)	
training:	Epoch: [7][336/408]	Loss 0.6361 (0.2853)	
training:	Epoch: [7][337/408]	Loss 0.0461 (0.2846)	
training:	Epoch: [7][338/408]	Loss 0.0704 (0.2840)	
training:	Epoch: [7][339/408]	Loss 0.2759 (0.2839)	
training:	Epoch: [7][340/408]	Loss 0.3766 (0.2842)	
training:	Epoch: [7][341/408]	Loss 0.5199 (0.2849)	
training:	Epoch: [7][342/408]	Loss 0.1675 (0.2846)	
training:	Epoch: [7][343/408]	Loss 0.1601 (0.2842)	
training:	Epoch: [7][344/408]	Loss 0.2327 (0.2840)	
training:	Epoch: [7][345/408]	Loss 0.3462 (0.2842)	
training:	Epoch: [7][346/408]	Loss 0.0978 (0.2837)	
training:	Epoch: [7][347/408]	Loss 0.5326 (0.2844)	
training:	Epoch: [7][348/408]	Loss 0.1740 (0.2841)	
training:	Epoch: [7][349/408]	Loss 0.0887 (0.2835)	
training:	Epoch: [7][350/408]	Loss 0.4015 (0.2839)	
training:	Epoch: [7][351/408]	Loss 0.4058 (0.2842)	
training:	Epoch: [7][352/408]	Loss 0.1642 (0.2839)	
training:	Epoch: [7][353/408]	Loss 0.5467 (0.2846)	
training:	Epoch: [7][354/408]	Loss 0.4173 (0.2850)	
training:	Epoch: [7][355/408]	Loss 0.3902 (0.2853)	
training:	Epoch: [7][356/408]	Loss 0.3477 (0.2855)	
training:	Epoch: [7][357/408]	Loss 0.2020 (0.2852)	
training:	Epoch: [7][358/408]	Loss 0.1592 (0.2849)	
training:	Epoch: [7][359/408]	Loss 0.1534 (0.2845)	
training:	Epoch: [7][360/408]	Loss 0.6309 (0.2855)	
training:	Epoch: [7][361/408]	Loss 0.1782 (0.2852)	
training:	Epoch: [7][362/408]	Loss 0.6818 (0.2863)	
training:	Epoch: [7][363/408]	Loss 0.3302 (0.2864)	
training:	Epoch: [7][364/408]	Loss 0.2519 (0.2863)	
training:	Epoch: [7][365/408]	Loss 0.3558 (0.2865)	
training:	Epoch: [7][366/408]	Loss 0.5155 (0.2871)	
training:	Epoch: [7][367/408]	Loss 0.3441 (0.2873)	
training:	Epoch: [7][368/408]	Loss 0.3124 (0.2873)	
training:	Epoch: [7][369/408]	Loss 0.2495 (0.2872)	
training:	Epoch: [7][370/408]	Loss 0.9505 (0.2890)	
training:	Epoch: [7][371/408]	Loss 0.0857 (0.2885)	
training:	Epoch: [7][372/408]	Loss 0.4300 (0.2889)	
training:	Epoch: [7][373/408]	Loss 0.2883 (0.2889)	
training:	Epoch: [7][374/408]	Loss 0.1447 (0.2885)	
training:	Epoch: [7][375/408]	Loss 0.4265 (0.2888)	
training:	Epoch: [7][376/408]	Loss 0.1634 (0.2885)	
training:	Epoch: [7][377/408]	Loss 0.2277 (0.2883)	
training:	Epoch: [7][378/408]	Loss 0.2929 (0.2884)	
training:	Epoch: [7][379/408]	Loss 0.3935 (0.2886)	
training:	Epoch: [7][380/408]	Loss 0.2481 (0.2885)	
training:	Epoch: [7][381/408]	Loss 0.1858 (0.2883)	
training:	Epoch: [7][382/408]	Loss 0.4997 (0.2888)	
training:	Epoch: [7][383/408]	Loss 0.0610 (0.2882)	
training:	Epoch: [7][384/408]	Loss 0.4047 (0.2885)	
training:	Epoch: [7][385/408]	Loss 0.1413 (0.2881)	
training:	Epoch: [7][386/408]	Loss 0.1155 (0.2877)	
training:	Epoch: [7][387/408]	Loss 0.2548 (0.2876)	
training:	Epoch: [7][388/408]	Loss 0.2080 (0.2874)	
training:	Epoch: [7][389/408]	Loss 0.3620 (0.2876)	
training:	Epoch: [7][390/408]	Loss 0.0947 (0.2871)	
training:	Epoch: [7][391/408]	Loss 0.4849 (0.2876)	
training:	Epoch: [7][392/408]	Loss 0.2839 (0.2876)	
training:	Epoch: [7][393/408]	Loss 0.1895 (0.2873)	
training:	Epoch: [7][394/408]	Loss 0.3897 (0.2876)	
training:	Epoch: [7][395/408]	Loss 0.3926 (0.2879)	
training:	Epoch: [7][396/408]	Loss 0.1910 (0.2876)	
training:	Epoch: [7][397/408]	Loss 0.1719 (0.2873)	
training:	Epoch: [7][398/408]	Loss 0.2094 (0.2871)	
training:	Epoch: [7][399/408]	Loss 0.2540 (0.2870)	
training:	Epoch: [7][400/408]	Loss 0.2319 (0.2869)	
training:	Epoch: [7][401/408]	Loss 0.4540 (0.2873)	
training:	Epoch: [7][402/408]	Loss 0.1171 (0.2869)	
training:	Epoch: [7][403/408]	Loss 0.3709 (0.2871)	
training:	Epoch: [7][404/408]	Loss 0.1503 (0.2868)	
training:	Epoch: [7][405/408]	Loss 0.5897 (0.2875)	
training:	Epoch: [7][406/408]	Loss 0.3093 (0.2876)	
training:	Epoch: [7][407/408]	Loss 0.0906 (0.2871)	
training:	Epoch: [7][408/408]	Loss 0.1679 (0.2868)	
Training:	 Loss: 0.2864

Training:	 ACC: 0.9135 0.9139 0.9224 0.9047
Validation:	 ACC: 0.8201 0.8213 0.8454 0.7948
Validation:	 Best_BACC: 0.8226 0.8250 0.8772 0.7679
Validation:	 Loss: 0.4264
Pretraining:	Epoch 8/200
----------
training:	Epoch: [8][1/408]	Loss 0.0606 (0.0606)	
training:	Epoch: [8][2/408]	Loss 0.1611 (0.1109)	
training:	Epoch: [8][3/408]	Loss 0.1608 (0.1275)	
training:	Epoch: [8][4/408]	Loss 0.1330 (0.1289)	
training:	Epoch: [8][5/408]	Loss 0.3348 (0.1701)	
training:	Epoch: [8][6/408]	Loss 0.5758 (0.2377)	
training:	Epoch: [8][7/408]	Loss 0.1638 (0.2271)	
training:	Epoch: [8][8/408]	Loss 0.1821 (0.2215)	
training:	Epoch: [8][9/408]	Loss 0.2308 (0.2225)	
training:	Epoch: [8][10/408]	Loss 0.0991 (0.2102)	
training:	Epoch: [8][11/408]	Loss 0.3671 (0.2244)	
training:	Epoch: [8][12/408]	Loss 0.1932 (0.2218)	
training:	Epoch: [8][13/408]	Loss 0.1115 (0.2134)	
training:	Epoch: [8][14/408]	Loss 0.3431 (0.2226)	
training:	Epoch: [8][15/408]	Loss 0.1120 (0.2152)	
training:	Epoch: [8][16/408]	Loss 0.3243 (0.2221)	
training:	Epoch: [8][17/408]	Loss 0.0599 (0.2125)	
training:	Epoch: [8][18/408]	Loss 0.4652 (0.2266)	
training:	Epoch: [8][19/408]	Loss 0.3520 (0.2332)	
training:	Epoch: [8][20/408]	Loss 0.0788 (0.2254)	
training:	Epoch: [8][21/408]	Loss 0.4414 (0.2357)	
training:	Epoch: [8][22/408]	Loss 0.0905 (0.2291)	
training:	Epoch: [8][23/408]	Loss 0.2712 (0.2309)	
training:	Epoch: [8][24/408]	Loss 0.2760 (0.2328)	
training:	Epoch: [8][25/408]	Loss 0.1852 (0.2309)	
training:	Epoch: [8][26/408]	Loss 0.2960 (0.2334)	
training:	Epoch: [8][27/408]	Loss 0.0777 (0.2277)	
training:	Epoch: [8][28/408]	Loss 0.3520 (0.2321)	
training:	Epoch: [8][29/408]	Loss 0.2538 (0.2328)	
training:	Epoch: [8][30/408]	Loss 0.1462 (0.2300)	
training:	Epoch: [8][31/408]	Loss 0.4140 (0.2359)	
training:	Epoch: [8][32/408]	Loss 0.3350 (0.2390)	
training:	Epoch: [8][33/408]	Loss 0.1221 (0.2355)	
training:	Epoch: [8][34/408]	Loss 0.4560 (0.2419)	
training:	Epoch: [8][35/408]	Loss 0.3460 (0.2449)	
training:	Epoch: [8][36/408]	Loss 0.4173 (0.2497)	
training:	Epoch: [8][37/408]	Loss 0.2425 (0.2495)	
training:	Epoch: [8][38/408]	Loss 0.2665 (0.2500)	
training:	Epoch: [8][39/408]	Loss 0.2546 (0.2501)	
training:	Epoch: [8][40/408]	Loss 0.3818 (0.2534)	
training:	Epoch: [8][41/408]	Loss 0.2978 (0.2545)	
training:	Epoch: [8][42/408]	Loss 0.2166 (0.2536)	
training:	Epoch: [8][43/408]	Loss 0.1137 (0.2503)	
training:	Epoch: [8][44/408]	Loss 0.2655 (0.2506)	
training:	Epoch: [8][45/408]	Loss 0.3702 (0.2533)	
training:	Epoch: [8][46/408]	Loss 0.4409 (0.2574)	
training:	Epoch: [8][47/408]	Loss 0.2357 (0.2569)	
training:	Epoch: [8][48/408]	Loss 0.2966 (0.2577)	
training:	Epoch: [8][49/408]	Loss 0.3965 (0.2606)	
training:	Epoch: [8][50/408]	Loss 0.2073 (0.2595)	
training:	Epoch: [8][51/408]	Loss 0.5313 (0.2648)	
training:	Epoch: [8][52/408]	Loss 0.1882 (0.2634)	
training:	Epoch: [8][53/408]	Loss 0.3481 (0.2650)	
training:	Epoch: [8][54/408]	Loss 0.0753 (0.2615)	
training:	Epoch: [8][55/408]	Loss 0.0540 (0.2577)	
training:	Epoch: [8][56/408]	Loss 0.4487 (0.2611)	
training:	Epoch: [8][57/408]	Loss 0.2771 (0.2614)	
training:	Epoch: [8][58/408]	Loss 0.2159 (0.2606)	
training:	Epoch: [8][59/408]	Loss 0.5819 (0.2660)	
training:	Epoch: [8][60/408]	Loss 0.5506 (0.2708)	
training:	Epoch: [8][61/408]	Loss 0.0905 (0.2678)	
training:	Epoch: [8][62/408]	Loss 0.1155 (0.2654)	
training:	Epoch: [8][63/408]	Loss 0.2334 (0.2649)	
training:	Epoch: [8][64/408]	Loss 0.0964 (0.2622)	
training:	Epoch: [8][65/408]	Loss 0.1175 (0.2600)	
training:	Epoch: [8][66/408]	Loss 0.2621 (0.2600)	
training:	Epoch: [8][67/408]	Loss 0.2403 (0.2597)	
training:	Epoch: [8][68/408]	Loss 0.3840 (0.2616)	
training:	Epoch: [8][69/408]	Loss 0.1188 (0.2595)	
training:	Epoch: [8][70/408]	Loss 0.0853 (0.2570)	
training:	Epoch: [8][71/408]	Loss 0.1779 (0.2559)	
training:	Epoch: [8][72/408]	Loss 0.3808 (0.2576)	
training:	Epoch: [8][73/408]	Loss 0.3735 (0.2592)	
training:	Epoch: [8][74/408]	Loss 0.1557 (0.2578)	
training:	Epoch: [8][75/408]	Loss 0.2003 (0.2570)	
training:	Epoch: [8][76/408]	Loss 0.2354 (0.2568)	
training:	Epoch: [8][77/408]	Loss 0.2417 (0.2566)	
training:	Epoch: [8][78/408]	Loss 0.4120 (0.2586)	
training:	Epoch: [8][79/408]	Loss 0.2232 (0.2581)	
training:	Epoch: [8][80/408]	Loss 0.2496 (0.2580)	
training:	Epoch: [8][81/408]	Loss 0.1838 (0.2571)	
training:	Epoch: [8][82/408]	Loss 0.0891 (0.2550)	
training:	Epoch: [8][83/408]	Loss 0.3911 (0.2567)	
training:	Epoch: [8][84/408]	Loss 0.1141 (0.2550)	
training:	Epoch: [8][85/408]	Loss 0.1678 (0.2540)	
training:	Epoch: [8][86/408]	Loss 0.3598 (0.2552)	
training:	Epoch: [8][87/408]	Loss 0.0698 (0.2531)	
training:	Epoch: [8][88/408]	Loss 0.2328 (0.2528)	
training:	Epoch: [8][89/408]	Loss 0.0770 (0.2508)	
training:	Epoch: [8][90/408]	Loss 0.1606 (0.2498)	
training:	Epoch: [8][91/408]	Loss 0.2273 (0.2496)	
training:	Epoch: [8][92/408]	Loss 0.2284 (0.2494)	
training:	Epoch: [8][93/408]	Loss 0.1837 (0.2487)	
training:	Epoch: [8][94/408]	Loss 0.3561 (0.2498)	
training:	Epoch: [8][95/408]	Loss 0.2334 (0.2496)	
training:	Epoch: [8][96/408]	Loss 0.2149 (0.2493)	
training:	Epoch: [8][97/408]	Loss 0.0940 (0.2477)	
training:	Epoch: [8][98/408]	Loss 0.3033 (0.2482)	
training:	Epoch: [8][99/408]	Loss 0.1309 (0.2471)	
training:	Epoch: [8][100/408]	Loss 0.4216 (0.2488)	
training:	Epoch: [8][101/408]	Loss 0.2327 (0.2486)	
training:	Epoch: [8][102/408]	Loss 0.2711 (0.2489)	
training:	Epoch: [8][103/408]	Loss 0.5990 (0.2523)	
training:	Epoch: [8][104/408]	Loss 0.3099 (0.2528)	
training:	Epoch: [8][105/408]	Loss 0.4625 (0.2548)	
training:	Epoch: [8][106/408]	Loss 0.0974 (0.2533)	
training:	Epoch: [8][107/408]	Loss 0.2379 (0.2532)	
training:	Epoch: [8][108/408]	Loss 0.1772 (0.2525)	
training:	Epoch: [8][109/408]	Loss 0.2980 (0.2529)	
training:	Epoch: [8][110/408]	Loss 0.1681 (0.2521)	
training:	Epoch: [8][111/408]	Loss 0.2383 (0.2520)	
training:	Epoch: [8][112/408]	Loss 0.3171 (0.2526)	
training:	Epoch: [8][113/408]	Loss 0.6458 (0.2561)	
training:	Epoch: [8][114/408]	Loss 0.5241 (0.2584)	
training:	Epoch: [8][115/408]	Loss 0.2395 (0.2582)	
training:	Epoch: [8][116/408]	Loss 0.2280 (0.2580)	
training:	Epoch: [8][117/408]	Loss 0.0747 (0.2564)	
training:	Epoch: [8][118/408]	Loss 0.4859 (0.2584)	
training:	Epoch: [8][119/408]	Loss 0.1292 (0.2573)	
training:	Epoch: [8][120/408]	Loss 0.1524 (0.2564)	
training:	Epoch: [8][121/408]	Loss 0.2930 (0.2567)	
training:	Epoch: [8][122/408]	Loss 0.4537 (0.2583)	
training:	Epoch: [8][123/408]	Loss 0.6641 (0.2616)	
training:	Epoch: [8][124/408]	Loss 0.3247 (0.2621)	
training:	Epoch: [8][125/408]	Loss 0.3140 (0.2625)	
training:	Epoch: [8][126/408]	Loss 0.1708 (0.2618)	
training:	Epoch: [8][127/408]	Loss 0.2583 (0.2618)	
training:	Epoch: [8][128/408]	Loss 0.2690 (0.2618)	
training:	Epoch: [8][129/408]	Loss 0.3603 (0.2626)	
training:	Epoch: [8][130/408]	Loss 0.2369 (0.2624)	
training:	Epoch: [8][131/408]	Loss 0.2347 (0.2622)	
training:	Epoch: [8][132/408]	Loss 0.1401 (0.2613)	
training:	Epoch: [8][133/408]	Loss 0.2432 (0.2611)	
training:	Epoch: [8][134/408]	Loss 0.1122 (0.2600)	
training:	Epoch: [8][135/408]	Loss 0.3817 (0.2609)	
training:	Epoch: [8][136/408]	Loss 0.2599 (0.2609)	
training:	Epoch: [8][137/408]	Loss 0.2260 (0.2607)	
training:	Epoch: [8][138/408]	Loss 0.4285 (0.2619)	
training:	Epoch: [8][139/408]	Loss 0.2985 (0.2621)	
training:	Epoch: [8][140/408]	Loss 0.1384 (0.2613)	
training:	Epoch: [8][141/408]	Loss 0.2007 (0.2608)	
training:	Epoch: [8][142/408]	Loss 0.1992 (0.2604)	
training:	Epoch: [8][143/408]	Loss 0.3196 (0.2608)	
training:	Epoch: [8][144/408]	Loss 0.4819 (0.2623)	
training:	Epoch: [8][145/408]	Loss 0.3459 (0.2629)	
training:	Epoch: [8][146/408]	Loss 0.3084 (0.2632)	
training:	Epoch: [8][147/408]	Loss 0.0594 (0.2618)	
training:	Epoch: [8][148/408]	Loss 0.3047 (0.2621)	
training:	Epoch: [8][149/408]	Loss 0.4588 (0.2635)	
training:	Epoch: [8][150/408]	Loss 0.5929 (0.2657)	
training:	Epoch: [8][151/408]	Loss 0.1702 (0.2650)	
training:	Epoch: [8][152/408]	Loss 0.1176 (0.2640)	
training:	Epoch: [8][153/408]	Loss 0.1856 (0.2635)	
training:	Epoch: [8][154/408]	Loss 0.3048 (0.2638)	
training:	Epoch: [8][155/408]	Loss 0.2449 (0.2637)	
training:	Epoch: [8][156/408]	Loss 0.3098 (0.2640)	
training:	Epoch: [8][157/408]	Loss 0.0719 (0.2628)	
training:	Epoch: [8][158/408]	Loss 0.3864 (0.2635)	
training:	Epoch: [8][159/408]	Loss 0.1001 (0.2625)	
training:	Epoch: [8][160/408]	Loss 0.1226 (0.2616)	
training:	Epoch: [8][161/408]	Loss 0.1184 (0.2607)	
training:	Epoch: [8][162/408]	Loss 0.2478 (0.2607)	
training:	Epoch: [8][163/408]	Loss 0.1183 (0.2598)	
training:	Epoch: [8][164/408]	Loss 0.1290 (0.2590)	
training:	Epoch: [8][165/408]	Loss 0.0950 (0.2580)	
training:	Epoch: [8][166/408]	Loss 0.2686 (0.2581)	
training:	Epoch: [8][167/408]	Loss 0.2939 (0.2583)	
training:	Epoch: [8][168/408]	Loss 0.2444 (0.2582)	
training:	Epoch: [8][169/408]	Loss 0.3116 (0.2585)	
training:	Epoch: [8][170/408]	Loss 0.3511 (0.2591)	
training:	Epoch: [8][171/408]	Loss 0.2457 (0.2590)	
training:	Epoch: [8][172/408]	Loss 0.0708 (0.2579)	
training:	Epoch: [8][173/408]	Loss 0.1272 (0.2571)	
training:	Epoch: [8][174/408]	Loss 0.0632 (0.2560)	
training:	Epoch: [8][175/408]	Loss 0.0592 (0.2549)	
training:	Epoch: [8][176/408]	Loss 0.2621 (0.2549)	
training:	Epoch: [8][177/408]	Loss 0.0905 (0.2540)	
training:	Epoch: [8][178/408]	Loss 0.3886 (0.2548)	
training:	Epoch: [8][179/408]	Loss 0.2559 (0.2548)	
training:	Epoch: [8][180/408]	Loss 0.1494 (0.2542)	
training:	Epoch: [8][181/408]	Loss 0.2849 (0.2543)	
training:	Epoch: [8][182/408]	Loss 0.2705 (0.2544)	
training:	Epoch: [8][183/408]	Loss 0.2961 (0.2547)	
training:	Epoch: [8][184/408]	Loss 0.4043 (0.2555)	
training:	Epoch: [8][185/408]	Loss 0.1754 (0.2550)	
training:	Epoch: [8][186/408]	Loss 0.0607 (0.2540)	
training:	Epoch: [8][187/408]	Loss 0.1562 (0.2535)	
training:	Epoch: [8][188/408]	Loss 0.1781 (0.2531)	
training:	Epoch: [8][189/408]	Loss 0.2616 (0.2531)	
training:	Epoch: [8][190/408]	Loss 0.2691 (0.2532)	
training:	Epoch: [8][191/408]	Loss 0.1544 (0.2527)	
training:	Epoch: [8][192/408]	Loss 0.4917 (0.2539)	
training:	Epoch: [8][193/408]	Loss 0.1525 (0.2534)	
training:	Epoch: [8][194/408]	Loss 0.1617 (0.2529)	
training:	Epoch: [8][195/408]	Loss 0.1822 (0.2526)	
training:	Epoch: [8][196/408]	Loss 0.3869 (0.2533)	
training:	Epoch: [8][197/408]	Loss 0.4320 (0.2542)	
training:	Epoch: [8][198/408]	Loss 0.0705 (0.2532)	
training:	Epoch: [8][199/408]	Loss 0.2069 (0.2530)	
training:	Epoch: [8][200/408]	Loss 0.2608 (0.2530)	
training:	Epoch: [8][201/408]	Loss 0.2402 (0.2530)	
training:	Epoch: [8][202/408]	Loss 0.2504 (0.2530)	
training:	Epoch: [8][203/408]	Loss 0.5620 (0.2545)	
training:	Epoch: [8][204/408]	Loss 0.4546 (0.2555)	
training:	Epoch: [8][205/408]	Loss 0.5539 (0.2569)	
training:	Epoch: [8][206/408]	Loss 0.2122 (0.2567)	
training:	Epoch: [8][207/408]	Loss 0.1098 (0.2560)	
training:	Epoch: [8][208/408]	Loss 0.3481 (0.2564)	
training:	Epoch: [8][209/408]	Loss 0.1539 (0.2560)	
training:	Epoch: [8][210/408]	Loss 0.2813 (0.2561)	
training:	Epoch: [8][211/408]	Loss 0.2480 (0.2560)	
training:	Epoch: [8][212/408]	Loss 0.4531 (0.2570)	
training:	Epoch: [8][213/408]	Loss 0.3767 (0.2575)	
training:	Epoch: [8][214/408]	Loss 0.2238 (0.2574)	
training:	Epoch: [8][215/408]	Loss 0.0896 (0.2566)	
training:	Epoch: [8][216/408]	Loss 0.1016 (0.2559)	
training:	Epoch: [8][217/408]	Loss 0.2998 (0.2561)	
training:	Epoch: [8][218/408]	Loss 0.2250 (0.2559)	
training:	Epoch: [8][219/408]	Loss 0.0577 (0.2550)	
training:	Epoch: [8][220/408]	Loss 0.0979 (0.2543)	
training:	Epoch: [8][221/408]	Loss 0.0496 (0.2534)	
training:	Epoch: [8][222/408]	Loss 0.3689 (0.2539)	
training:	Epoch: [8][223/408]	Loss 0.4216 (0.2547)	
training:	Epoch: [8][224/408]	Loss 0.5796 (0.2561)	
training:	Epoch: [8][225/408]	Loss 0.2053 (0.2559)	
training:	Epoch: [8][226/408]	Loss 0.2870 (0.2560)	
training:	Epoch: [8][227/408]	Loss 0.0740 (0.2552)	
training:	Epoch: [8][228/408]	Loss 0.4495 (0.2561)	
training:	Epoch: [8][229/408]	Loss 0.3455 (0.2565)	
training:	Epoch: [8][230/408]	Loss 0.3035 (0.2567)	
training:	Epoch: [8][231/408]	Loss 0.1846 (0.2564)	
training:	Epoch: [8][232/408]	Loss 0.3201 (0.2566)	
training:	Epoch: [8][233/408]	Loss 0.3263 (0.2569)	
training:	Epoch: [8][234/408]	Loss 0.4237 (0.2576)	
training:	Epoch: [8][235/408]	Loss 0.3555 (0.2581)	
training:	Epoch: [8][236/408]	Loss 0.1145 (0.2574)	
training:	Epoch: [8][237/408]	Loss 0.3375 (0.2578)	
training:	Epoch: [8][238/408]	Loss 0.0682 (0.2570)	
training:	Epoch: [8][239/408]	Loss 0.2898 (0.2571)	
training:	Epoch: [8][240/408]	Loss 0.4907 (0.2581)	
training:	Epoch: [8][241/408]	Loss 0.2481 (0.2581)	
training:	Epoch: [8][242/408]	Loss 0.0872 (0.2573)	
training:	Epoch: [8][243/408]	Loss 0.2373 (0.2573)	
training:	Epoch: [8][244/408]	Loss 0.3837 (0.2578)	
training:	Epoch: [8][245/408]	Loss 0.2687 (0.2578)	
training:	Epoch: [8][246/408]	Loss 0.0628 (0.2570)	
training:	Epoch: [8][247/408]	Loss 0.4488 (0.2578)	
training:	Epoch: [8][248/408]	Loss 0.2834 (0.2579)	
training:	Epoch: [8][249/408]	Loss 0.0989 (0.2573)	
training:	Epoch: [8][250/408]	Loss 0.1754 (0.2570)	
training:	Epoch: [8][251/408]	Loss 0.1077 (0.2564)	
training:	Epoch: [8][252/408]	Loss 0.1811 (0.2561)	
training:	Epoch: [8][253/408]	Loss 0.3391 (0.2564)	
training:	Epoch: [8][254/408]	Loss 0.4972 (0.2573)	
training:	Epoch: [8][255/408]	Loss 0.3225 (0.2576)	
training:	Epoch: [8][256/408]	Loss 0.2871 (0.2577)	
training:	Epoch: [8][257/408]	Loss 0.1051 (0.2571)	
training:	Epoch: [8][258/408]	Loss 0.3014 (0.2573)	
training:	Epoch: [8][259/408]	Loss 0.1182 (0.2567)	
training:	Epoch: [8][260/408]	Loss 0.5672 (0.2579)	
training:	Epoch: [8][261/408]	Loss 0.2877 (0.2581)	
training:	Epoch: [8][262/408]	Loss 0.3579 (0.2584)	
training:	Epoch: [8][263/408]	Loss 0.2200 (0.2583)	
training:	Epoch: [8][264/408]	Loss 0.4336 (0.2590)	
training:	Epoch: [8][265/408]	Loss 0.1086 (0.2584)	
training:	Epoch: [8][266/408]	Loss 0.3177 (0.2586)	
training:	Epoch: [8][267/408]	Loss 0.1603 (0.2582)	
training:	Epoch: [8][268/408]	Loss 0.2524 (0.2582)	
training:	Epoch: [8][269/408]	Loss 0.1381 (0.2578)	
training:	Epoch: [8][270/408]	Loss 0.2375 (0.2577)	
training:	Epoch: [8][271/408]	Loss 0.5896 (0.2589)	
training:	Epoch: [8][272/408]	Loss 0.2978 (0.2591)	
training:	Epoch: [8][273/408]	Loss 0.7418 (0.2608)	
training:	Epoch: [8][274/408]	Loss 0.1254 (0.2603)	
training:	Epoch: [8][275/408]	Loss 0.2886 (0.2604)	
training:	Epoch: [8][276/408]	Loss 0.2112 (0.2603)	
training:	Epoch: [8][277/408]	Loss 0.6375 (0.2616)	
training:	Epoch: [8][278/408]	Loss 0.0963 (0.2610)	
training:	Epoch: [8][279/408]	Loss 0.2422 (0.2610)	
training:	Epoch: [8][280/408]	Loss 0.1063 (0.2604)	
training:	Epoch: [8][281/408]	Loss 0.5634 (0.2615)	
training:	Epoch: [8][282/408]	Loss 0.2116 (0.2613)	
training:	Epoch: [8][283/408]	Loss 0.4574 (0.2620)	
training:	Epoch: [8][284/408]	Loss 0.1513 (0.2616)	
training:	Epoch: [8][285/408]	Loss 0.1200 (0.2611)	
training:	Epoch: [8][286/408]	Loss 0.2436 (0.2611)	
training:	Epoch: [8][287/408]	Loss 0.1821 (0.2608)	
training:	Epoch: [8][288/408]	Loss 0.5963 (0.2619)	
training:	Epoch: [8][289/408]	Loss 0.4801 (0.2627)	
training:	Epoch: [8][290/408]	Loss 0.3752 (0.2631)	
training:	Epoch: [8][291/408]	Loss 0.2563 (0.2631)	
training:	Epoch: [8][292/408]	Loss 0.0762 (0.2624)	
training:	Epoch: [8][293/408]	Loss 0.5465 (0.2634)	
training:	Epoch: [8][294/408]	Loss 0.1886 (0.2631)	
training:	Epoch: [8][295/408]	Loss 0.0834 (0.2625)	
training:	Epoch: [8][296/408]	Loss 0.4411 (0.2631)	
training:	Epoch: [8][297/408]	Loss 0.1601 (0.2628)	
training:	Epoch: [8][298/408]	Loss 0.3477 (0.2631)	
training:	Epoch: [8][299/408]	Loss 0.3978 (0.2635)	
training:	Epoch: [8][300/408]	Loss 0.1103 (0.2630)	
training:	Epoch: [8][301/408]	Loss 0.1052 (0.2625)	
training:	Epoch: [8][302/408]	Loss 0.1415 (0.2621)	
training:	Epoch: [8][303/408]	Loss 0.2202 (0.2619)	
training:	Epoch: [8][304/408]	Loss 0.1717 (0.2617)	
training:	Epoch: [8][305/408]	Loss 0.2586 (0.2616)	
training:	Epoch: [8][306/408]	Loss 0.2432 (0.2616)	
training:	Epoch: [8][307/408]	Loss 0.2638 (0.2616)	
training:	Epoch: [8][308/408]	Loss 0.1561 (0.2612)	
training:	Epoch: [8][309/408]	Loss 0.2827 (0.2613)	
training:	Epoch: [8][310/408]	Loss 0.1748 (0.2610)	
training:	Epoch: [8][311/408]	Loss 0.3457 (0.2613)	
training:	Epoch: [8][312/408]	Loss 0.0716 (0.2607)	
training:	Epoch: [8][313/408]	Loss 0.1832 (0.2605)	
training:	Epoch: [8][314/408]	Loss 0.1261 (0.2600)	
training:	Epoch: [8][315/408]	Loss 0.4115 (0.2605)	
training:	Epoch: [8][316/408]	Loss 0.3410 (0.2608)	
training:	Epoch: [8][317/408]	Loss 0.2500 (0.2607)	
training:	Epoch: [8][318/408]	Loss 0.1780 (0.2605)	
training:	Epoch: [8][319/408]	Loss 0.1603 (0.2602)	
training:	Epoch: [8][320/408]	Loss 0.5822 (0.2612)	
training:	Epoch: [8][321/408]	Loss 0.3987 (0.2616)	
training:	Epoch: [8][322/408]	Loss 0.6026 (0.2626)	
training:	Epoch: [8][323/408]	Loss 0.1437 (0.2623)	
training:	Epoch: [8][324/408]	Loss 0.2167 (0.2621)	
training:	Epoch: [8][325/408]	Loss 0.2783 (0.2622)	
training:	Epoch: [8][326/408]	Loss 0.2119 (0.2620)	
training:	Epoch: [8][327/408]	Loss 0.1684 (0.2617)	
training:	Epoch: [8][328/408]	Loss 0.1856 (0.2615)	
training:	Epoch: [8][329/408]	Loss 0.1865 (0.2613)	
training:	Epoch: [8][330/408]	Loss 0.1474 (0.2609)	
training:	Epoch: [8][331/408]	Loss 0.2788 (0.2610)	
training:	Epoch: [8][332/408]	Loss 0.4249 (0.2615)	
training:	Epoch: [8][333/408]	Loss 0.2725 (0.2615)	
training:	Epoch: [8][334/408]	Loss 0.1156 (0.2611)	
training:	Epoch: [8][335/408]	Loss 0.2197 (0.2610)	
training:	Epoch: [8][336/408]	Loss 0.1815 (0.2607)	
training:	Epoch: [8][337/408]	Loss 0.4328 (0.2612)	
training:	Epoch: [8][338/408]	Loss 0.2628 (0.2612)	
training:	Epoch: [8][339/408]	Loss 0.1132 (0.2608)	
training:	Epoch: [8][340/408]	Loss 0.0646 (0.2602)	
training:	Epoch: [8][341/408]	Loss 0.2675 (0.2602)	
training:	Epoch: [8][342/408]	Loss 0.1901 (0.2600)	
training:	Epoch: [8][343/408]	Loss 0.1535 (0.2597)	
training:	Epoch: [8][344/408]	Loss 0.2698 (0.2598)	
training:	Epoch: [8][345/408]	Loss 0.2623 (0.2598)	
training:	Epoch: [8][346/408]	Loss 0.3779 (0.2601)	
training:	Epoch: [8][347/408]	Loss 0.2440 (0.2601)	
training:	Epoch: [8][348/408]	Loss 0.0790 (0.2595)	
training:	Epoch: [8][349/408]	Loss 0.3496 (0.2598)	
training:	Epoch: [8][350/408]	Loss 0.4310 (0.2603)	
training:	Epoch: [8][351/408]	Loss 0.2974 (0.2604)	
training:	Epoch: [8][352/408]	Loss 0.3116 (0.2605)	
training:	Epoch: [8][353/408]	Loss 0.4190 (0.2610)	
training:	Epoch: [8][354/408]	Loss 0.3442 (0.2612)	
training:	Epoch: [8][355/408]	Loss 0.1882 (0.2610)	
training:	Epoch: [8][356/408]	Loss 0.2460 (0.2610)	
training:	Epoch: [8][357/408]	Loss 0.2942 (0.2611)	
training:	Epoch: [8][358/408]	Loss 0.1205 (0.2607)	
training:	Epoch: [8][359/408]	Loss 0.2775 (0.2607)	
training:	Epoch: [8][360/408]	Loss 0.4935 (0.2614)	
training:	Epoch: [8][361/408]	Loss 0.1383 (0.2610)	
training:	Epoch: [8][362/408]	Loss 0.0425 (0.2604)	
training:	Epoch: [8][363/408]	Loss 0.2698 (0.2605)	
training:	Epoch: [8][364/408]	Loss 0.4773 (0.2611)	
training:	Epoch: [8][365/408]	Loss 0.2097 (0.2609)	
training:	Epoch: [8][366/408]	Loss 0.2047 (0.2608)	
training:	Epoch: [8][367/408]	Loss 0.1999 (0.2606)	
training:	Epoch: [8][368/408]	Loss 0.1868 (0.2604)	
training:	Epoch: [8][369/408]	Loss 0.1442 (0.2601)	
training:	Epoch: [8][370/408]	Loss 0.4293 (0.2605)	
training:	Epoch: [8][371/408]	Loss 0.0748 (0.2600)	
training:	Epoch: [8][372/408]	Loss 0.1030 (0.2596)	
training:	Epoch: [8][373/408]	Loss 0.4451 (0.2601)	
training:	Epoch: [8][374/408]	Loss 0.0753 (0.2596)	
training:	Epoch: [8][375/408]	Loss 0.2572 (0.2596)	
training:	Epoch: [8][376/408]	Loss 0.0412 (0.2590)	
training:	Epoch: [8][377/408]	Loss 0.1341 (0.2587)	
training:	Epoch: [8][378/408]	Loss 0.1834 (0.2585)	
training:	Epoch: [8][379/408]	Loss 0.2047 (0.2584)	
training:	Epoch: [8][380/408]	Loss 0.6824 (0.2595)	
training:	Epoch: [8][381/408]	Loss 0.3223 (0.2596)	
training:	Epoch: [8][382/408]	Loss 0.3561 (0.2599)	
training:	Epoch: [8][383/408]	Loss 0.1283 (0.2595)	
training:	Epoch: [8][384/408]	Loss 0.1383 (0.2592)	
training:	Epoch: [8][385/408]	Loss 0.2814 (0.2593)	
training:	Epoch: [8][386/408]	Loss 0.3046 (0.2594)	
training:	Epoch: [8][387/408]	Loss 0.3683 (0.2597)	
training:	Epoch: [8][388/408]	Loss 0.1807 (0.2595)	
training:	Epoch: [8][389/408]	Loss 0.1494 (0.2592)	
training:	Epoch: [8][390/408]	Loss 0.6822 (0.2603)	
training:	Epoch: [8][391/408]	Loss 0.0800 (0.2598)	
training:	Epoch: [8][392/408]	Loss 0.1665 (0.2596)	
training:	Epoch: [8][393/408]	Loss 0.3013 (0.2597)	
training:	Epoch: [8][394/408]	Loss 0.1103 (0.2593)	
training:	Epoch: [8][395/408]	Loss 0.1895 (0.2591)	
training:	Epoch: [8][396/408]	Loss 0.1890 (0.2590)	
training:	Epoch: [8][397/408]	Loss 0.0724 (0.2585)	
training:	Epoch: [8][398/408]	Loss 0.2644 (0.2585)	
training:	Epoch: [8][399/408]	Loss 0.1576 (0.2582)	
training:	Epoch: [8][400/408]	Loss 0.2065 (0.2581)	
training:	Epoch: [8][401/408]	Loss 0.0728 (0.2577)	
training:	Epoch: [8][402/408]	Loss 0.3104 (0.2578)	
training:	Epoch: [8][403/408]	Loss 0.2931 (0.2579)	
training:	Epoch: [8][404/408]	Loss 0.1726 (0.2577)	
training:	Epoch: [8][405/408]	Loss 0.4446 (0.2581)	
training:	Epoch: [8][406/408]	Loss 0.1372 (0.2578)	
training:	Epoch: [8][407/408]	Loss 0.3015 (0.2579)	
training:	Epoch: [8][408/408]	Loss 0.3764 (0.2582)	
Training:	 Loss: 0.2578

Training:	 ACC: 0.9353 0.9351 0.9312 0.9394
Validation:	 ACC: 0.8176 0.8181 0.8280 0.8072
Validation:	 Best_BACC: 0.8226 0.8250 0.8772 0.7679
Validation:	 Loss: 0.4522
Pretraining:	Epoch 9/200
----------
training:	Epoch: [9][1/408]	Loss 0.0532 (0.0532)	
training:	Epoch: [9][2/408]	Loss 0.3778 (0.2155)	
training:	Epoch: [9][3/408]	Loss 0.4918 (0.3076)	
training:	Epoch: [9][4/408]	Loss 0.1135 (0.2591)	
training:	Epoch: [9][5/408]	Loss 0.4254 (0.2923)	
training:	Epoch: [9][6/408]	Loss 0.1278 (0.2649)	
training:	Epoch: [9][7/408]	Loss 0.0392 (0.2327)	
training:	Epoch: [9][8/408]	Loss 0.1174 (0.2183)	
training:	Epoch: [9][9/408]	Loss 0.0730 (0.2021)	
training:	Epoch: [9][10/408]	Loss 0.2006 (0.2020)	
training:	Epoch: [9][11/408]	Loss 0.0417 (0.1874)	
training:	Epoch: [9][12/408]	Loss 0.2300 (0.1909)	
training:	Epoch: [9][13/408]	Loss 0.2268 (0.1937)	
training:	Epoch: [9][14/408]	Loss 0.1054 (0.1874)	
training:	Epoch: [9][15/408]	Loss 0.5672 (0.2127)	
training:	Epoch: [9][16/408]	Loss 0.1004 (0.2057)	
training:	Epoch: [9][17/408]	Loss 0.2707 (0.2095)	
training:	Epoch: [9][18/408]	Loss 0.3906 (0.2196)	
training:	Epoch: [9][19/408]	Loss 0.4420 (0.2313)	
training:	Epoch: [9][20/408]	Loss 0.3830 (0.2389)	
training:	Epoch: [9][21/408]	Loss 0.1349 (0.2339)	
training:	Epoch: [9][22/408]	Loss 0.1720 (0.2311)	
training:	Epoch: [9][23/408]	Loss 0.0742 (0.2243)	
training:	Epoch: [9][24/408]	Loss 0.3327 (0.2288)	
training:	Epoch: [9][25/408]	Loss 0.2541 (0.2298)	
training:	Epoch: [9][26/408]	Loss 0.4878 (0.2397)	
training:	Epoch: [9][27/408]	Loss 0.3686 (0.2445)	
training:	Epoch: [9][28/408]	Loss 0.0982 (0.2393)	
training:	Epoch: [9][29/408]	Loss 0.1357 (0.2357)	
training:	Epoch: [9][30/408]	Loss 0.1211 (0.2319)	
training:	Epoch: [9][31/408]	Loss 0.1431 (0.2290)	
training:	Epoch: [9][32/408]	Loss 0.2766 (0.2305)	
training:	Epoch: [9][33/408]	Loss 0.0905 (0.2263)	
training:	Epoch: [9][34/408]	Loss 0.2290 (0.2264)	
training:	Epoch: [9][35/408]	Loss 0.1126 (0.2231)	
training:	Epoch: [9][36/408]	Loss 0.2358 (0.2235)	
training:	Epoch: [9][37/408]	Loss 0.3052 (0.2257)	
training:	Epoch: [9][38/408]	Loss 0.3541 (0.2290)	
training:	Epoch: [9][39/408]	Loss 0.2150 (0.2287)	
training:	Epoch: [9][40/408]	Loss 0.0825 (0.2250)	
training:	Epoch: [9][41/408]	Loss 0.1428 (0.2230)	
training:	Epoch: [9][42/408]	Loss 0.1119 (0.2204)	
training:	Epoch: [9][43/408]	Loss 0.3825 (0.2242)	
training:	Epoch: [9][44/408]	Loss 0.2693 (0.2252)	
training:	Epoch: [9][45/408]	Loss 0.1570 (0.2237)	
training:	Epoch: [9][46/408]	Loss 0.1504 (0.2221)	
training:	Epoch: [9][47/408]	Loss 0.5966 (0.2300)	
training:	Epoch: [9][48/408]	Loss 0.1359 (0.2281)	
training:	Epoch: [9][49/408]	Loss 0.2424 (0.2284)	
training:	Epoch: [9][50/408]	Loss 0.1198 (0.2262)	
training:	Epoch: [9][51/408]	Loss 0.1180 (0.2241)	
training:	Epoch: [9][52/408]	Loss 0.1432 (0.2225)	
training:	Epoch: [9][53/408]	Loss 0.3294 (0.2245)	
training:	Epoch: [9][54/408]	Loss 0.4081 (0.2279)	
training:	Epoch: [9][55/408]	Loss 0.6357 (0.2353)	
training:	Epoch: [9][56/408]	Loss 0.2845 (0.2362)	
training:	Epoch: [9][57/408]	Loss 0.3784 (0.2387)	
training:	Epoch: [9][58/408]	Loss 0.5145 (0.2435)	
training:	Epoch: [9][59/408]	Loss 0.3467 (0.2452)	
training:	Epoch: [9][60/408]	Loss 0.4474 (0.2486)	
training:	Epoch: [9][61/408]	Loss 0.1637 (0.2472)	
training:	Epoch: [9][62/408]	Loss 0.0808 (0.2445)	
training:	Epoch: [9][63/408]	Loss 0.0876 (0.2420)	
training:	Epoch: [9][64/408]	Loss 0.0926 (0.2397)	
training:	Epoch: [9][65/408]	Loss 0.4246 (0.2425)	
training:	Epoch: [9][66/408]	Loss 0.2808 (0.2431)	
training:	Epoch: [9][67/408]	Loss 0.4356 (0.2460)	
training:	Epoch: [9][68/408]	Loss 0.2275 (0.2457)	
training:	Epoch: [9][69/408]	Loss 0.5970 (0.2508)	
training:	Epoch: [9][70/408]	Loss 0.0377 (0.2478)	
training:	Epoch: [9][71/408]	Loss 0.1039 (0.2457)	
training:	Epoch: [9][72/408]	Loss 0.3623 (0.2474)	
training:	Epoch: [9][73/408]	Loss 0.1311 (0.2458)	
training:	Epoch: [9][74/408]	Loss 0.2235 (0.2455)	
training:	Epoch: [9][75/408]	Loss 0.3281 (0.2466)	
training:	Epoch: [9][76/408]	Loss 0.0853 (0.2444)	
training:	Epoch: [9][77/408]	Loss 0.1299 (0.2430)	
training:	Epoch: [9][78/408]	Loss 0.1589 (0.2419)	
training:	Epoch: [9][79/408]	Loss 0.2242 (0.2417)	
training:	Epoch: [9][80/408]	Loss 0.1393 (0.2404)	
training:	Epoch: [9][81/408]	Loss 0.4187 (0.2426)	
training:	Epoch: [9][82/408]	Loss 0.3973 (0.2445)	
training:	Epoch: [9][83/408]	Loss 0.1246 (0.2430)	
training:	Epoch: [9][84/408]	Loss 0.5969 (0.2472)	
training:	Epoch: [9][85/408]	Loss 0.2675 (0.2475)	
training:	Epoch: [9][86/408]	Loss 0.2779 (0.2478)	
training:	Epoch: [9][87/408]	Loss 0.4278 (0.2499)	
training:	Epoch: [9][88/408]	Loss 0.4053 (0.2517)	
training:	Epoch: [9][89/408]	Loss 0.1130 (0.2501)	
training:	Epoch: [9][90/408]	Loss 0.1863 (0.2494)	
training:	Epoch: [9][91/408]	Loss 0.2934 (0.2499)	
training:	Epoch: [9][92/408]	Loss 0.2456 (0.2498)	
training:	Epoch: [9][93/408]	Loss 0.0624 (0.2478)	
training:	Epoch: [9][94/408]	Loss 0.3565 (0.2490)	
training:	Epoch: [9][95/408]	Loss 0.2155 (0.2486)	
training:	Epoch: [9][96/408]	Loss 0.1149 (0.2472)	
training:	Epoch: [9][97/408]	Loss 0.1450 (0.2462)	
training:	Epoch: [9][98/408]	Loss 0.0631 (0.2443)	
training:	Epoch: [9][99/408]	Loss 0.0966 (0.2428)	
training:	Epoch: [9][100/408]	Loss 0.4370 (0.2448)	
training:	Epoch: [9][101/408]	Loss 0.1623 (0.2439)	
training:	Epoch: [9][102/408]	Loss 0.2116 (0.2436)	
training:	Epoch: [9][103/408]	Loss 0.0923 (0.2422)	
training:	Epoch: [9][104/408]	Loss 0.2757 (0.2425)	
training:	Epoch: [9][105/408]	Loss 0.1364 (0.2415)	
training:	Epoch: [9][106/408]	Loss 0.3908 (0.2429)	
training:	Epoch: [9][107/408]	Loss 0.0858 (0.2414)	
training:	Epoch: [9][108/408]	Loss 0.0912 (0.2400)	
training:	Epoch: [9][109/408]	Loss 0.0927 (0.2387)	
training:	Epoch: [9][110/408]	Loss 0.0553 (0.2370)	
training:	Epoch: [9][111/408]	Loss 0.1444 (0.2362)	
training:	Epoch: [9][112/408]	Loss 0.1934 (0.2358)	
training:	Epoch: [9][113/408]	Loss 0.4319 (0.2375)	
training:	Epoch: [9][114/408]	Loss 0.1764 (0.2370)	
training:	Epoch: [9][115/408]	Loss 0.3610 (0.2381)	
training:	Epoch: [9][116/408]	Loss 0.3608 (0.2391)	
training:	Epoch: [9][117/408]	Loss 0.0468 (0.2375)	
training:	Epoch: [9][118/408]	Loss 0.2069 (0.2372)	
training:	Epoch: [9][119/408]	Loss 0.1706 (0.2367)	
training:	Epoch: [9][120/408]	Loss 0.0853 (0.2354)	
training:	Epoch: [9][121/408]	Loss 0.2729 (0.2357)	
training:	Epoch: [9][122/408]	Loss 0.7339 (0.2398)	
training:	Epoch: [9][123/408]	Loss 0.2249 (0.2397)	
training:	Epoch: [9][124/408]	Loss 0.2217 (0.2395)	
training:	Epoch: [9][125/408]	Loss 0.2185 (0.2394)	
training:	Epoch: [9][126/408]	Loss 0.0734 (0.2380)	
training:	Epoch: [9][127/408]	Loss 0.1094 (0.2370)	
training:	Epoch: [9][128/408]	Loss 0.1400 (0.2363)	
training:	Epoch: [9][129/408]	Loss 0.1073 (0.2353)	
training:	Epoch: [9][130/408]	Loss 0.0685 (0.2340)	
training:	Epoch: [9][131/408]	Loss 0.1853 (0.2336)	
training:	Epoch: [9][132/408]	Loss 0.1439 (0.2329)	
training:	Epoch: [9][133/408]	Loss 0.1073 (0.2320)	
training:	Epoch: [9][134/408]	Loss 0.1633 (0.2315)	
training:	Epoch: [9][135/408]	Loss 0.2629 (0.2317)	
training:	Epoch: [9][136/408]	Loss 0.2870 (0.2321)	
training:	Epoch: [9][137/408]	Loss 0.2634 (0.2323)	
training:	Epoch: [9][138/408]	Loss 0.0413 (0.2310)	
training:	Epoch: [9][139/408]	Loss 0.1861 (0.2306)	
training:	Epoch: [9][140/408]	Loss 0.2409 (0.2307)	
training:	Epoch: [9][141/408]	Loss 0.3162 (0.2313)	
training:	Epoch: [9][142/408]	Loss 0.0738 (0.2302)	
training:	Epoch: [9][143/408]	Loss 0.2959 (0.2307)	
training:	Epoch: [9][144/408]	Loss 0.0984 (0.2297)	
training:	Epoch: [9][145/408]	Loss 0.1305 (0.2291)	
training:	Epoch: [9][146/408]	Loss 0.0895 (0.2281)	
training:	Epoch: [9][147/408]	Loss 0.3144 (0.2287)	
training:	Epoch: [9][148/408]	Loss 0.0823 (0.2277)	
training:	Epoch: [9][149/408]	Loss 0.4847 (0.2294)	
training:	Epoch: [9][150/408]	Loss 0.2754 (0.2297)	
training:	Epoch: [9][151/408]	Loss 0.1499 (0.2292)	
training:	Epoch: [9][152/408]	Loss 0.2357 (0.2292)	
training:	Epoch: [9][153/408]	Loss 0.1224 (0.2285)	
training:	Epoch: [9][154/408]	Loss 0.0803 (0.2276)	
training:	Epoch: [9][155/408]	Loss 0.0769 (0.2266)	
training:	Epoch: [9][156/408]	Loss 0.3799 (0.2276)	
training:	Epoch: [9][157/408]	Loss 0.1196 (0.2269)	
training:	Epoch: [9][158/408]	Loss 0.2019 (0.2268)	
training:	Epoch: [9][159/408]	Loss 0.2796 (0.2271)	
training:	Epoch: [9][160/408]	Loss 0.7877 (0.2306)	
training:	Epoch: [9][161/408]	Loss 0.2546 (0.2307)	
training:	Epoch: [9][162/408]	Loss 0.1048 (0.2300)	
training:	Epoch: [9][163/408]	Loss 0.1583 (0.2295)	
training:	Epoch: [9][164/408]	Loss 0.3420 (0.2302)	
training:	Epoch: [9][165/408]	Loss 0.1214 (0.2295)	
training:	Epoch: [9][166/408]	Loss 0.1804 (0.2293)	
training:	Epoch: [9][167/408]	Loss 0.2405 (0.2293)	
training:	Epoch: [9][168/408]	Loss 0.0930 (0.2285)	
training:	Epoch: [9][169/408]	Loss 0.1215 (0.2279)	
training:	Epoch: [9][170/408]	Loss 0.1161 (0.2272)	
training:	Epoch: [9][171/408]	Loss 0.3075 (0.2277)	
training:	Epoch: [9][172/408]	Loss 0.1045 (0.2270)	
training:	Epoch: [9][173/408]	Loss 0.5146 (0.2286)	
training:	Epoch: [9][174/408]	Loss 0.0537 (0.2276)	
training:	Epoch: [9][175/408]	Loss 0.0710 (0.2267)	
training:	Epoch: [9][176/408]	Loss 0.1190 (0.2261)	
training:	Epoch: [9][177/408]	Loss 0.5660 (0.2280)	
training:	Epoch: [9][178/408]	Loss 0.1405 (0.2275)	
training:	Epoch: [9][179/408]	Loss 0.0626 (0.2266)	
training:	Epoch: [9][180/408]	Loss 0.3575 (0.2274)	
training:	Epoch: [9][181/408]	Loss 0.1054 (0.2267)	
training:	Epoch: [9][182/408]	Loss 0.3051 (0.2271)	
training:	Epoch: [9][183/408]	Loss 0.1744 (0.2268)	
training:	Epoch: [9][184/408]	Loss 0.0874 (0.2261)	
training:	Epoch: [9][185/408]	Loss 0.1650 (0.2257)	
training:	Epoch: [9][186/408]	Loss 0.2738 (0.2260)	
training:	Epoch: [9][187/408]	Loss 0.3361 (0.2266)	
training:	Epoch: [9][188/408]	Loss 0.1423 (0.2261)	
training:	Epoch: [9][189/408]	Loss 0.0789 (0.2254)	
training:	Epoch: [9][190/408]	Loss 0.0833 (0.2246)	
training:	Epoch: [9][191/408]	Loss 0.0578 (0.2237)	
training:	Epoch: [9][192/408]	Loss 0.2458 (0.2238)	
training:	Epoch: [9][193/408]	Loss 0.0597 (0.2230)	
training:	Epoch: [9][194/408]	Loss 0.4381 (0.2241)	
training:	Epoch: [9][195/408]	Loss 0.4367 (0.2252)	
training:	Epoch: [9][196/408]	Loss 0.1006 (0.2246)	
training:	Epoch: [9][197/408]	Loss 0.5437 (0.2262)	
training:	Epoch: [9][198/408]	Loss 0.0604 (0.2253)	
training:	Epoch: [9][199/408]	Loss 0.0704 (0.2246)	
training:	Epoch: [9][200/408]	Loss 0.1501 (0.2242)	
training:	Epoch: [9][201/408]	Loss 0.0741 (0.2234)	
training:	Epoch: [9][202/408]	Loss 0.3000 (0.2238)	
training:	Epoch: [9][203/408]	Loss 0.2439 (0.2239)	
training:	Epoch: [9][204/408]	Loss 0.1680 (0.2237)	
training:	Epoch: [9][205/408]	Loss 0.4495 (0.2248)	
training:	Epoch: [9][206/408]	Loss 0.0797 (0.2240)	
training:	Epoch: [9][207/408]	Loss 0.1726 (0.2238)	
training:	Epoch: [9][208/408]	Loss 0.0701 (0.2231)	
training:	Epoch: [9][209/408]	Loss 0.0740 (0.2223)	
training:	Epoch: [9][210/408]	Loss 0.0744 (0.2216)	
training:	Epoch: [9][211/408]	Loss 0.1202 (0.2212)	
training:	Epoch: [9][212/408]	Loss 0.0736 (0.2205)	
training:	Epoch: [9][213/408]	Loss 0.2723 (0.2207)	
training:	Epoch: [9][214/408]	Loss 0.4385 (0.2217)	
training:	Epoch: [9][215/408]	Loss 0.0797 (0.2211)	
training:	Epoch: [9][216/408]	Loss 0.2997 (0.2214)	
training:	Epoch: [9][217/408]	Loss 0.0744 (0.2208)	
training:	Epoch: [9][218/408]	Loss 0.2811 (0.2210)	
training:	Epoch: [9][219/408]	Loss 0.1085 (0.2205)	
training:	Epoch: [9][220/408]	Loss 0.0889 (0.2199)	
training:	Epoch: [9][221/408]	Loss 0.4409 (0.2209)	
training:	Epoch: [9][222/408]	Loss 0.0641 (0.2202)	
training:	Epoch: [9][223/408]	Loss 0.1821 (0.2200)	
training:	Epoch: [9][224/408]	Loss 0.3415 (0.2206)	
training:	Epoch: [9][225/408]	Loss 0.7071 (0.2227)	
training:	Epoch: [9][226/408]	Loss 0.0768 (0.2221)	
training:	Epoch: [9][227/408]	Loss 0.2228 (0.2221)	
training:	Epoch: [9][228/408]	Loss 0.0620 (0.2214)	
training:	Epoch: [9][229/408]	Loss 0.2692 (0.2216)	
training:	Epoch: [9][230/408]	Loss 0.3323 (0.2221)	
training:	Epoch: [9][231/408]	Loss 0.1295 (0.2217)	
training:	Epoch: [9][232/408]	Loss 0.0594 (0.2210)	
training:	Epoch: [9][233/408]	Loss 0.1164 (0.2205)	
training:	Epoch: [9][234/408]	Loss 0.0766 (0.2199)	
training:	Epoch: [9][235/408]	Loss 0.3130 (0.2203)	
training:	Epoch: [9][236/408]	Loss 0.0607 (0.2196)	
training:	Epoch: [9][237/408]	Loss 0.1195 (0.2192)	
training:	Epoch: [9][238/408]	Loss 0.2779 (0.2195)	
training:	Epoch: [9][239/408]	Loss 0.1284 (0.2191)	
training:	Epoch: [9][240/408]	Loss 0.0814 (0.2185)	
training:	Epoch: [9][241/408]	Loss 0.1264 (0.2181)	
training:	Epoch: [9][242/408]	Loss 0.2529 (0.2183)	
training:	Epoch: [9][243/408]	Loss 0.1874 (0.2181)	
training:	Epoch: [9][244/408]	Loss 0.1242 (0.2178)	
training:	Epoch: [9][245/408]	Loss 0.0862 (0.2172)	
training:	Epoch: [9][246/408]	Loss 0.1918 (0.2171)	
training:	Epoch: [9][247/408]	Loss 0.0855 (0.2166)	
training:	Epoch: [9][248/408]	Loss 0.7501 (0.2187)	
training:	Epoch: [9][249/408]	Loss 0.3076 (0.2191)	
training:	Epoch: [9][250/408]	Loss 0.0595 (0.2185)	
training:	Epoch: [9][251/408]	Loss 0.2127 (0.2184)	
training:	Epoch: [9][252/408]	Loss 0.1339 (0.2181)	
training:	Epoch: [9][253/408]	Loss 0.0478 (0.2174)	
training:	Epoch: [9][254/408]	Loss 0.2326 (0.2175)	
training:	Epoch: [9][255/408]	Loss 0.3478 (0.2180)	
training:	Epoch: [9][256/408]	Loss 0.2049 (0.2179)	
training:	Epoch: [9][257/408]	Loss 0.3049 (0.2183)	
training:	Epoch: [9][258/408]	Loss 0.1613 (0.2181)	
training:	Epoch: [9][259/408]	Loss 0.1602 (0.2178)	
training:	Epoch: [9][260/408]	Loss 0.2141 (0.2178)	
training:	Epoch: [9][261/408]	Loss 0.3276 (0.2182)	
training:	Epoch: [9][262/408]	Loss 0.1314 (0.2179)	
training:	Epoch: [9][263/408]	Loss 0.3017 (0.2182)	
training:	Epoch: [9][264/408]	Loss 0.3559 (0.2188)	
training:	Epoch: [9][265/408]	Loss 0.2086 (0.2187)	
training:	Epoch: [9][266/408]	Loss 0.2175 (0.2187)	
training:	Epoch: [9][267/408]	Loss 0.1133 (0.2183)	
training:	Epoch: [9][268/408]	Loss 0.1258 (0.2180)	
training:	Epoch: [9][269/408]	Loss 0.1144 (0.2176)	
training:	Epoch: [9][270/408]	Loss 0.0543 (0.2170)	
training:	Epoch: [9][271/408]	Loss 0.3572 (0.2175)	
training:	Epoch: [9][272/408]	Loss 0.0971 (0.2171)	
training:	Epoch: [9][273/408]	Loss 0.2252 (0.2171)	
training:	Epoch: [9][274/408]	Loss 0.2246 (0.2171)	
training:	Epoch: [9][275/408]	Loss 0.1346 (0.2168)	
training:	Epoch: [9][276/408]	Loss 0.2295 (0.2169)	
training:	Epoch: [9][277/408]	Loss 0.2046 (0.2168)	
training:	Epoch: [9][278/408]	Loss 0.2725 (0.2170)	
training:	Epoch: [9][279/408]	Loss 0.0912 (0.2166)	
training:	Epoch: [9][280/408]	Loss 0.1449 (0.2163)	
training:	Epoch: [9][281/408]	Loss 0.4368 (0.2171)	
training:	Epoch: [9][282/408]	Loss 0.3730 (0.2176)	
training:	Epoch: [9][283/408]	Loss 0.4086 (0.2183)	
training:	Epoch: [9][284/408]	Loss 0.3295 (0.2187)	
training:	Epoch: [9][285/408]	Loss 0.0653 (0.2182)	
training:	Epoch: [9][286/408]	Loss 0.1370 (0.2179)	
training:	Epoch: [9][287/408]	Loss 0.2424 (0.2180)	
training:	Epoch: [9][288/408]	Loss 0.6666 (0.2195)	
training:	Epoch: [9][289/408]	Loss 0.2403 (0.2196)	
training:	Epoch: [9][290/408]	Loss 0.2170 (0.2196)	
training:	Epoch: [9][291/408]	Loss 0.2693 (0.2198)	
training:	Epoch: [9][292/408]	Loss 0.3247 (0.2201)	
training:	Epoch: [9][293/408]	Loss 0.2760 (0.2203)	
training:	Epoch: [9][294/408]	Loss 0.1106 (0.2199)	
training:	Epoch: [9][295/408]	Loss 0.2780 (0.2201)	
training:	Epoch: [9][296/408]	Loss 0.1554 (0.2199)	
training:	Epoch: [9][297/408]	Loss 0.0584 (0.2194)	
training:	Epoch: [9][298/408]	Loss 0.0776 (0.2189)	
training:	Epoch: [9][299/408]	Loss 0.5272 (0.2199)	
training:	Epoch: [9][300/408]	Loss 0.1360 (0.2197)	
training:	Epoch: [9][301/408]	Loss 0.2745 (0.2198)	
training:	Epoch: [9][302/408]	Loss 0.4142 (0.2205)	
training:	Epoch: [9][303/408]	Loss 0.2530 (0.2206)	
training:	Epoch: [9][304/408]	Loss 0.4185 (0.2212)	
training:	Epoch: [9][305/408]	Loss 0.1935 (0.2211)	
training:	Epoch: [9][306/408]	Loss 0.2743 (0.2213)	
training:	Epoch: [9][307/408]	Loss 0.1254 (0.2210)	
training:	Epoch: [9][308/408]	Loss 0.5441 (0.2221)	
training:	Epoch: [9][309/408]	Loss 0.1818 (0.2219)	
training:	Epoch: [9][310/408]	Loss 0.0547 (0.2214)	
training:	Epoch: [9][311/408]	Loss 0.3611 (0.2218)	
training:	Epoch: [9][312/408]	Loss 0.3376 (0.2222)	
training:	Epoch: [9][313/408]	Loss 0.4464 (0.2229)	
training:	Epoch: [9][314/408]	Loss 0.2728 (0.2231)	
training:	Epoch: [9][315/408]	Loss 0.0746 (0.2226)	
training:	Epoch: [9][316/408]	Loss 0.2888 (0.2228)	
training:	Epoch: [9][317/408]	Loss 0.1459 (0.2226)	
training:	Epoch: [9][318/408]	Loss 0.1951 (0.2225)	
training:	Epoch: [9][319/408]	Loss 0.1249 (0.2222)	
training:	Epoch: [9][320/408]	Loss 0.1685 (0.2220)	
training:	Epoch: [9][321/408]	Loss 0.1478 (0.2218)	
training:	Epoch: [9][322/408]	Loss 0.3956 (0.2223)	
training:	Epoch: [9][323/408]	Loss 0.3279 (0.2227)	
training:	Epoch: [9][324/408]	Loss 0.3079 (0.2229)	
training:	Epoch: [9][325/408]	Loss 0.0859 (0.2225)	
training:	Epoch: [9][326/408]	Loss 0.3033 (0.2227)	
training:	Epoch: [9][327/408]	Loss 0.1087 (0.2224)	
training:	Epoch: [9][328/408]	Loss 0.2873 (0.2226)	
training:	Epoch: [9][329/408]	Loss 0.1751 (0.2225)	
training:	Epoch: [9][330/408]	Loss 0.1202 (0.2221)	
training:	Epoch: [9][331/408]	Loss 0.2555 (0.2222)	
training:	Epoch: [9][332/408]	Loss 0.3736 (0.2227)	
training:	Epoch: [9][333/408]	Loss 0.3384 (0.2230)	
training:	Epoch: [9][334/408]	Loss 0.1558 (0.2228)	
training:	Epoch: [9][335/408]	Loss 0.1756 (0.2227)	
training:	Epoch: [9][336/408]	Loss 0.3502 (0.2231)	
training:	Epoch: [9][337/408]	Loss 0.4280 (0.2237)	
training:	Epoch: [9][338/408]	Loss 0.1888 (0.2236)	
training:	Epoch: [9][339/408]	Loss 0.3363 (0.2239)	
training:	Epoch: [9][340/408]	Loss 0.0528 (0.2234)	
training:	Epoch: [9][341/408]	Loss 0.2509 (0.2235)	
training:	Epoch: [9][342/408]	Loss 0.1930 (0.2234)	
training:	Epoch: [9][343/408]	Loss 0.1151 (0.2231)	
training:	Epoch: [9][344/408]	Loss 0.2339 (0.2231)	
training:	Epoch: [9][345/408]	Loss 0.0572 (0.2226)	
training:	Epoch: [9][346/408]	Loss 0.0519 (0.2221)	
training:	Epoch: [9][347/408]	Loss 0.0820 (0.2217)	
training:	Epoch: [9][348/408]	Loss 0.2349 (0.2218)	
training:	Epoch: [9][349/408]	Loss 0.1315 (0.2215)	
training:	Epoch: [9][350/408]	Loss 0.2586 (0.2216)	
training:	Epoch: [9][351/408]	Loss 0.0530 (0.2211)	
training:	Epoch: [9][352/408]	Loss 0.2268 (0.2212)	
training:	Epoch: [9][353/408]	Loss 0.0719 (0.2207)	
training:	Epoch: [9][354/408]	Loss 0.2038 (0.2207)	
training:	Epoch: [9][355/408]	Loss 0.3163 (0.2210)	
training:	Epoch: [9][356/408]	Loss 0.1184 (0.2207)	
training:	Epoch: [9][357/408]	Loss 0.1119 (0.2204)	
training:	Epoch: [9][358/408]	Loss 0.1348 (0.2201)	
training:	Epoch: [9][359/408]	Loss 0.2550 (0.2202)	
training:	Epoch: [9][360/408]	Loss 0.2853 (0.2204)	
training:	Epoch: [9][361/408]	Loss 0.1835 (0.2203)	
training:	Epoch: [9][362/408]	Loss 0.0398 (0.2198)	
training:	Epoch: [9][363/408]	Loss 0.4964 (0.2206)	
training:	Epoch: [9][364/408]	Loss 0.2883 (0.2208)	
training:	Epoch: [9][365/408]	Loss 0.1594 (0.2206)	
training:	Epoch: [9][366/408]	Loss 0.5727 (0.2216)	
training:	Epoch: [9][367/408]	Loss 0.0834 (0.2212)	
training:	Epoch: [9][368/408]	Loss 0.2825 (0.2213)	
training:	Epoch: [9][369/408]	Loss 0.1095 (0.2210)	
training:	Epoch: [9][370/408]	Loss 0.0605 (0.2206)	
training:	Epoch: [9][371/408]	Loss 0.1054 (0.2203)	
training:	Epoch: [9][372/408]	Loss 0.4708 (0.2210)	
training:	Epoch: [9][373/408]	Loss 0.1457 (0.2208)	
training:	Epoch: [9][374/408]	Loss 0.3906 (0.2212)	
training:	Epoch: [9][375/408]	Loss 0.0820 (0.2208)	
training:	Epoch: [9][376/408]	Loss 0.1750 (0.2207)	
training:	Epoch: [9][377/408]	Loss 0.3252 (0.2210)	
training:	Epoch: [9][378/408]	Loss 0.3574 (0.2214)	
training:	Epoch: [9][379/408]	Loss 0.0387 (0.2209)	
training:	Epoch: [9][380/408]	Loss 0.0824 (0.2205)	
training:	Epoch: [9][381/408]	Loss 0.1656 (0.2204)	
training:	Epoch: [9][382/408]	Loss 0.0674 (0.2200)	
training:	Epoch: [9][383/408]	Loss 0.0977 (0.2197)	
training:	Epoch: [9][384/408]	Loss 0.0595 (0.2192)	
training:	Epoch: [9][385/408]	Loss 0.1429 (0.2190)	
training:	Epoch: [9][386/408]	Loss 0.1186 (0.2188)	
training:	Epoch: [9][387/408]	Loss 0.1909 (0.2187)	
training:	Epoch: [9][388/408]	Loss 0.4387 (0.2193)	
training:	Epoch: [9][389/408]	Loss 0.4314 (0.2198)	
training:	Epoch: [9][390/408]	Loss 0.2495 (0.2199)	
training:	Epoch: [9][391/408]	Loss 0.2236 (0.2199)	
training:	Epoch: [9][392/408]	Loss 0.4550 (0.2205)	
training:	Epoch: [9][393/408]	Loss 0.2518 (0.2206)	
training:	Epoch: [9][394/408]	Loss 0.0517 (0.2202)	
training:	Epoch: [9][395/408]	Loss 0.4931 (0.2208)	
training:	Epoch: [9][396/408]	Loss 0.0735 (0.2205)	
training:	Epoch: [9][397/408]	Loss 0.0508 (0.2200)	
training:	Epoch: [9][398/408]	Loss 0.2610 (0.2201)	
training:	Epoch: [9][399/408]	Loss 0.1461 (0.2200)	
training:	Epoch: [9][400/408]	Loss 0.5826 (0.2209)	
training:	Epoch: [9][401/408]	Loss 0.3946 (0.2213)	
training:	Epoch: [9][402/408]	Loss 0.1122 (0.2210)	
training:	Epoch: [9][403/408]	Loss 0.3582 (0.2214)	
training:	Epoch: [9][404/408]	Loss 0.4721 (0.2220)	
training:	Epoch: [9][405/408]	Loss 0.3181 (0.2222)	
training:	Epoch: [9][406/408]	Loss 0.1567 (0.2221)	
training:	Epoch: [9][407/408]	Loss 0.3004 (0.2223)	
training:	Epoch: [9][408/408]	Loss 0.2868 (0.2224)	
Training:	 Loss: 0.2221

Training:	 ACC: 0.9496 0.9492 0.9397 0.9595
Validation:	 ACC: 0.8067 0.8068 0.8106 0.8027
Validation:	 Best_BACC: 0.8226 0.8250 0.8772 0.7679
Validation:	 Loss: 0.4857
Pretraining:	Epoch 10/200
----------
training:	Epoch: [10][1/408]	Loss 0.4368 (0.4368)	
training:	Epoch: [10][2/408]	Loss 0.0743 (0.2556)	
training:	Epoch: [10][3/408]	Loss 0.0760 (0.1957)	
training:	Epoch: [10][4/408]	Loss 0.1851 (0.1930)	
training:	Epoch: [10][5/408]	Loss 0.2571 (0.2059)	
training:	Epoch: [10][6/408]	Loss 0.0529 (0.1804)	
training:	Epoch: [10][7/408]	Loss 0.2809 (0.1947)	
training:	Epoch: [10][8/408]	Loss 0.2140 (0.1971)	
training:	Epoch: [10][9/408]	Loss 0.0536 (0.1812)	
training:	Epoch: [10][10/408]	Loss 0.0511 (0.1682)	
training:	Epoch: [10][11/408]	Loss 0.1278 (0.1645)	
training:	Epoch: [10][12/408]	Loss 0.0578 (0.1556)	
training:	Epoch: [10][13/408]	Loss 0.1353 (0.1540)	
training:	Epoch: [10][14/408]	Loss 0.0891 (0.1494)	
training:	Epoch: [10][15/408]	Loss 0.0650 (0.1438)	
training:	Epoch: [10][16/408]	Loss 0.0422 (0.1374)	
training:	Epoch: [10][17/408]	Loss 0.0483 (0.1322)	
training:	Epoch: [10][18/408]	Loss 0.3014 (0.1416)	
training:	Epoch: [10][19/408]	Loss 0.0970 (0.1392)	
training:	Epoch: [10][20/408]	Loss 0.2296 (0.1438)	
training:	Epoch: [10][21/408]	Loss 0.1582 (0.1444)	
training:	Epoch: [10][22/408]	Loss 0.3076 (0.1519)	
training:	Epoch: [10][23/408]	Loss 0.3017 (0.1584)	
training:	Epoch: [10][24/408]	Loss 0.2007 (0.1601)	
training:	Epoch: [10][25/408]	Loss 0.5623 (0.1762)	
training:	Epoch: [10][26/408]	Loss 0.0552 (0.1716)	
training:	Epoch: [10][27/408]	Loss 0.3387 (0.1778)	
training:	Epoch: [10][28/408]	Loss 0.1850 (0.1780)	
training:	Epoch: [10][29/408]	Loss 0.0761 (0.1745)	
training:	Epoch: [10][30/408]	Loss 0.1978 (0.1753)	
training:	Epoch: [10][31/408]	Loss 0.2045 (0.1762)	
training:	Epoch: [10][32/408]	Loss 0.0538 (0.1724)	
training:	Epoch: [10][33/408]	Loss 0.0604 (0.1690)	
training:	Epoch: [10][34/408]	Loss 0.2361 (0.1710)	
training:	Epoch: [10][35/408]	Loss 0.1304 (0.1698)	
training:	Epoch: [10][36/408]	Loss 0.0909 (0.1676)	
training:	Epoch: [10][37/408]	Loss 0.1278 (0.1665)	
training:	Epoch: [10][38/408]	Loss 0.1709 (0.1667)	
training:	Epoch: [10][39/408]	Loss 0.3297 (0.1708)	
training:	Epoch: [10][40/408]	Loss 0.0623 (0.1681)	
training:	Epoch: [10][41/408]	Loss 0.1247 (0.1671)	
training:	Epoch: [10][42/408]	Loss 0.1732 (0.1672)	
training:	Epoch: [10][43/408]	Loss 0.1232 (0.1662)	
training:	Epoch: [10][44/408]	Loss 0.2008 (0.1670)	
training:	Epoch: [10][45/408]	Loss 0.2031 (0.1678)	
training:	Epoch: [10][46/408]	Loss 0.2473 (0.1695)	
training:	Epoch: [10][47/408]	Loss 0.2787 (0.1718)	
training:	Epoch: [10][48/408]	Loss 0.1793 (0.1720)	
training:	Epoch: [10][49/408]	Loss 0.1657 (0.1719)	
training:	Epoch: [10][50/408]	Loss 0.1917 (0.1723)	
training:	Epoch: [10][51/408]	Loss 0.0536 (0.1699)	
training:	Epoch: [10][52/408]	Loss 0.2958 (0.1724)	
training:	Epoch: [10][53/408]	Loss 0.0547 (0.1701)	
training:	Epoch: [10][54/408]	Loss 0.1207 (0.1692)	
training:	Epoch: [10][55/408]	Loss 0.1817 (0.1694)	
training:	Epoch: [10][56/408]	Loss 0.1859 (0.1697)	
training:	Epoch: [10][57/408]	Loss 0.2181 (0.1706)	
training:	Epoch: [10][58/408]	Loss 0.1873 (0.1709)	
training:	Epoch: [10][59/408]	Loss 0.1130 (0.1699)	
training:	Epoch: [10][60/408]	Loss 0.1603 (0.1697)	
training:	Epoch: [10][61/408]	Loss 0.0833 (0.1683)	
training:	Epoch: [10][62/408]	Loss 0.1416 (0.1679)	
training:	Epoch: [10][63/408]	Loss 0.0367 (0.1658)	
training:	Epoch: [10][64/408]	Loss 0.0615 (0.1642)	
training:	Epoch: [10][65/408]	Loss 0.3150 (0.1665)	
training:	Epoch: [10][66/408]	Loss 0.3048 (0.1686)	
training:	Epoch: [10][67/408]	Loss 0.0781 (0.1672)	
training:	Epoch: [10][68/408]	Loss 0.2394 (0.1683)	
training:	Epoch: [10][69/408]	Loss 0.0705 (0.1669)	
training:	Epoch: [10][70/408]	Loss 0.1521 (0.1667)	
training:	Epoch: [10][71/408]	Loss 0.3010 (0.1686)	
training:	Epoch: [10][72/408]	Loss 0.4387 (0.1723)	
training:	Epoch: [10][73/408]	Loss 0.0428 (0.1705)	
training:	Epoch: [10][74/408]	Loss 0.0588 (0.1690)	
training:	Epoch: [10][75/408]	Loss 0.0516 (0.1675)	
training:	Epoch: [10][76/408]	Loss 0.2956 (0.1692)	
training:	Epoch: [10][77/408]	Loss 0.1242 (0.1686)	
training:	Epoch: [10][78/408]	Loss 0.1416 (0.1682)	
training:	Epoch: [10][79/408]	Loss 0.1828 (0.1684)	
training:	Epoch: [10][80/408]	Loss 0.1629 (0.1683)	
training:	Epoch: [10][81/408]	Loss 0.0560 (0.1670)	
training:	Epoch: [10][82/408]	Loss 0.2569 (0.1680)	
training:	Epoch: [10][83/408]	Loss 0.0324 (0.1664)	
training:	Epoch: [10][84/408]	Loss 0.0917 (0.1655)	
training:	Epoch: [10][85/408]	Loss 0.1334 (0.1651)	
training:	Epoch: [10][86/408]	Loss 0.1029 (0.1644)	
training:	Epoch: [10][87/408]	Loss 0.1126 (0.1638)	
training:	Epoch: [10][88/408]	Loss 0.0881 (0.1630)	
training:	Epoch: [10][89/408]	Loss 0.1277 (0.1626)	
training:	Epoch: [10][90/408]	Loss 0.1767 (0.1627)	
training:	Epoch: [10][91/408]	Loss 0.4938 (0.1664)	
training:	Epoch: [10][92/408]	Loss 0.0944 (0.1656)	
training:	Epoch: [10][93/408]	Loss 0.0755 (0.1646)	
training:	Epoch: [10][94/408]	Loss 0.0669 (0.1636)	
training:	Epoch: [10][95/408]	Loss 0.2196 (0.1642)	
training:	Epoch: [10][96/408]	Loss 0.2062 (0.1646)	
training:	Epoch: [10][97/408]	Loss 0.3963 (0.1670)	
training:	Epoch: [10][98/408]	Loss 0.0915 (0.1662)	
training:	Epoch: [10][99/408]	Loss 0.0855 (0.1654)	
training:	Epoch: [10][100/408]	Loss 0.3580 (0.1673)	
training:	Epoch: [10][101/408]	Loss 0.1643 (0.1673)	
training:	Epoch: [10][102/408]	Loss 0.1271 (0.1669)	
training:	Epoch: [10][103/408]	Loss 0.0487 (0.1658)	
training:	Epoch: [10][104/408]	Loss 0.0959 (0.1651)	
training:	Epoch: [10][105/408]	Loss 0.3471 (0.1668)	
training:	Epoch: [10][106/408]	Loss 0.2833 (0.1679)	
training:	Epoch: [10][107/408]	Loss 0.1408 (0.1677)	
training:	Epoch: [10][108/408]	Loss 0.0879 (0.1669)	
training:	Epoch: [10][109/408]	Loss 0.1016 (0.1663)	
training:	Epoch: [10][110/408]	Loss 0.0328 (0.1651)	
training:	Epoch: [10][111/408]	Loss 0.0522 (0.1641)	
training:	Epoch: [10][112/408]	Loss 0.2759 (0.1651)	
training:	Epoch: [10][113/408]	Loss 0.2699 (0.1660)	
training:	Epoch: [10][114/408]	Loss 0.5357 (0.1693)	
training:	Epoch: [10][115/408]	Loss 0.1819 (0.1694)	
training:	Epoch: [10][116/408]	Loss 0.1349 (0.1691)	
training:	Epoch: [10][117/408]	Loss 0.0650 (0.1682)	
training:	Epoch: [10][118/408]	Loss 0.0427 (0.1671)	
training:	Epoch: [10][119/408]	Loss 0.0456 (0.1661)	
training:	Epoch: [10][120/408]	Loss 0.0612 (0.1652)	
training:	Epoch: [10][121/408]	Loss 0.0799 (0.1645)	
training:	Epoch: [10][122/408]	Loss 0.5956 (0.1681)	
training:	Epoch: [10][123/408]	Loss 0.1183 (0.1677)	
training:	Epoch: [10][124/408]	Loss 0.1897 (0.1678)	
training:	Epoch: [10][125/408]	Loss 0.1657 (0.1678)	
training:	Epoch: [10][126/408]	Loss 0.4295 (0.1699)	
training:	Epoch: [10][127/408]	Loss 0.1637 (0.1698)	
training:	Epoch: [10][128/408]	Loss 0.2940 (0.1708)	
training:	Epoch: [10][129/408]	Loss 0.2850 (0.1717)	
training:	Epoch: [10][130/408]	Loss 0.1570 (0.1716)	
training:	Epoch: [10][131/408]	Loss 0.5218 (0.1743)	
training:	Epoch: [10][132/408]	Loss 0.0356 (0.1732)	
training:	Epoch: [10][133/408]	Loss 0.0618 (0.1724)	
training:	Epoch: [10][134/408]	Loss 0.0525 (0.1715)	
training:	Epoch: [10][135/408]	Loss 0.0699 (0.1707)	
training:	Epoch: [10][136/408]	Loss 0.2669 (0.1714)	
training:	Epoch: [10][137/408]	Loss 0.4318 (0.1733)	
training:	Epoch: [10][138/408]	Loss 0.3275 (0.1744)	
training:	Epoch: [10][139/408]	Loss 0.2800 (0.1752)	
training:	Epoch: [10][140/408]	Loss 0.1467 (0.1750)	
training:	Epoch: [10][141/408]	Loss 0.1553 (0.1749)	
training:	Epoch: [10][142/408]	Loss 0.0389 (0.1739)	
training:	Epoch: [10][143/408]	Loss 0.1316 (0.1736)	
training:	Epoch: [10][144/408]	Loss 0.0379 (0.1727)	
training:	Epoch: [10][145/408]	Loss 0.2341 (0.1731)	
training:	Epoch: [10][146/408]	Loss 0.4818 (0.1752)	
training:	Epoch: [10][147/408]	Loss 0.6588 (0.1785)	
training:	Epoch: [10][148/408]	Loss 0.0739 (0.1778)	
training:	Epoch: [10][149/408]	Loss 0.0461 (0.1769)	
training:	Epoch: [10][150/408]	Loss 0.0498 (0.1761)	
training:	Epoch: [10][151/408]	Loss 0.3374 (0.1771)	
training:	Epoch: [10][152/408]	Loss 0.1154 (0.1767)	
training:	Epoch: [10][153/408]	Loss 0.0615 (0.1760)	
training:	Epoch: [10][154/408]	Loss 0.0288 (0.1750)	
training:	Epoch: [10][155/408]	Loss 0.1679 (0.1750)	
training:	Epoch: [10][156/408]	Loss 0.5681 (0.1775)	
training:	Epoch: [10][157/408]	Loss 0.2639 (0.1780)	
training:	Epoch: [10][158/408]	Loss 0.3044 (0.1788)	
training:	Epoch: [10][159/408]	Loss 0.0425 (0.1780)	
training:	Epoch: [10][160/408]	Loss 0.2868 (0.1787)	
training:	Epoch: [10][161/408]	Loss 0.0567 (0.1779)	
training:	Epoch: [10][162/408]	Loss 0.0469 (0.1771)	
training:	Epoch: [10][163/408]	Loss 0.1718 (0.1771)	
training:	Epoch: [10][164/408]	Loss 0.0602 (0.1763)	
training:	Epoch: [10][165/408]	Loss 0.0383 (0.1755)	
training:	Epoch: [10][166/408]	Loss 0.0400 (0.1747)	
training:	Epoch: [10][167/408]	Loss 0.4697 (0.1765)	
training:	Epoch: [10][168/408]	Loss 0.1235 (0.1761)	
training:	Epoch: [10][169/408]	Loss 0.2770 (0.1767)	
training:	Epoch: [10][170/408]	Loss 0.0871 (0.1762)	
training:	Epoch: [10][171/408]	Loss 0.0892 (0.1757)	
training:	Epoch: [10][172/408]	Loss 0.2498 (0.1761)	
training:	Epoch: [10][173/408]	Loss 0.4776 (0.1779)	
training:	Epoch: [10][174/408]	Loss 0.0557 (0.1772)	
training:	Epoch: [10][175/408]	Loss 0.2538 (0.1776)	
training:	Epoch: [10][176/408]	Loss 0.2201 (0.1779)	
training:	Epoch: [10][177/408]	Loss 0.2339 (0.1782)	
training:	Epoch: [10][178/408]	Loss 0.1654 (0.1781)	
training:	Epoch: [10][179/408]	Loss 0.3178 (0.1789)	
training:	Epoch: [10][180/408]	Loss 0.4254 (0.1803)	
training:	Epoch: [10][181/408]	Loss 0.5561 (0.1823)	
training:	Epoch: [10][182/408]	Loss 0.2105 (0.1825)	
training:	Epoch: [10][183/408]	Loss 0.0374 (0.1817)	
training:	Epoch: [10][184/408]	Loss 0.1019 (0.1813)	
training:	Epoch: [10][185/408]	Loss 0.1452 (0.1811)	
training:	Epoch: [10][186/408]	Loss 0.0655 (0.1804)	
training:	Epoch: [10][187/408]	Loss 0.1598 (0.1803)	
training:	Epoch: [10][188/408]	Loss 0.0370 (0.1796)	
training:	Epoch: [10][189/408]	Loss 0.1159 (0.1792)	
training:	Epoch: [10][190/408]	Loss 0.0428 (0.1785)	
training:	Epoch: [10][191/408]	Loss 0.1444 (0.1783)	
training:	Epoch: [10][192/408]	Loss 0.0490 (0.1777)	
training:	Epoch: [10][193/408]	Loss 0.0495 (0.1770)	
training:	Epoch: [10][194/408]	Loss 0.0541 (0.1764)	
training:	Epoch: [10][195/408]	Loss 0.0488 (0.1757)	
training:	Epoch: [10][196/408]	Loss 0.1099 (0.1754)	
training:	Epoch: [10][197/408]	Loss 0.0890 (0.1749)	
training:	Epoch: [10][198/408]	Loss 0.1362 (0.1747)	
training:	Epoch: [10][199/408]	Loss 0.2790 (0.1753)	
training:	Epoch: [10][200/408]	Loss 0.0890 (0.1748)	
training:	Epoch: [10][201/408]	Loss 0.2079 (0.1750)	
training:	Epoch: [10][202/408]	Loss 0.1476 (0.1749)	
training:	Epoch: [10][203/408]	Loss 0.1016 (0.1745)	
training:	Epoch: [10][204/408]	Loss 0.2015 (0.1746)	
training:	Epoch: [10][205/408]	Loss 0.4677 (0.1761)	
training:	Epoch: [10][206/408]	Loss 0.2367 (0.1764)	
training:	Epoch: [10][207/408]	Loss 0.2501 (0.1767)	
training:	Epoch: [10][208/408]	Loss 0.0337 (0.1760)	
training:	Epoch: [10][209/408]	Loss 0.1233 (0.1758)	
training:	Epoch: [10][210/408]	Loss 0.2738 (0.1762)	
training:	Epoch: [10][211/408]	Loss 0.3452 (0.1770)	
training:	Epoch: [10][212/408]	Loss 0.2926 (0.1776)	
training:	Epoch: [10][213/408]	Loss 0.2151 (0.1778)	
training:	Epoch: [10][214/408]	Loss 0.0434 (0.1771)	
training:	Epoch: [10][215/408]	Loss 0.4918 (0.1786)	
training:	Epoch: [10][216/408]	Loss 0.0578 (0.1780)	
training:	Epoch: [10][217/408]	Loss 0.2264 (0.1783)	
training:	Epoch: [10][218/408]	Loss 0.1252 (0.1780)	
training:	Epoch: [10][219/408]	Loss 0.2639 (0.1784)	
training:	Epoch: [10][220/408]	Loss 0.3322 (0.1791)	
training:	Epoch: [10][221/408]	Loss 0.2684 (0.1795)	
training:	Epoch: [10][222/408]	Loss 0.2051 (0.1796)	
training:	Epoch: [10][223/408]	Loss 0.0580 (0.1791)	
training:	Epoch: [10][224/408]	Loss 0.1061 (0.1788)	
training:	Epoch: [10][225/408]	Loss 0.2283 (0.1790)	
training:	Epoch: [10][226/408]	Loss 0.1212 (0.1787)	
training:	Epoch: [10][227/408]	Loss 0.5089 (0.1802)	
training:	Epoch: [10][228/408]	Loss 0.1732 (0.1801)	
training:	Epoch: [10][229/408]	Loss 0.1877 (0.1802)	
training:	Epoch: [10][230/408]	Loss 0.1362 (0.1800)	
training:	Epoch: [10][231/408]	Loss 0.2493 (0.1803)	
training:	Epoch: [10][232/408]	Loss 0.1962 (0.1804)	
training:	Epoch: [10][233/408]	Loss 0.0596 (0.1798)	
training:	Epoch: [10][234/408]	Loss 0.3624 (0.1806)	
training:	Epoch: [10][235/408]	Loss 0.0732 (0.1802)	
training:	Epoch: [10][236/408]	Loss 0.2761 (0.1806)	
training:	Epoch: [10][237/408]	Loss 0.1312 (0.1804)	
training:	Epoch: [10][238/408]	Loss 0.2998 (0.1809)	
training:	Epoch: [10][239/408]	Loss 0.2143 (0.1810)	
training:	Epoch: [10][240/408]	Loss 0.3327 (0.1816)	
training:	Epoch: [10][241/408]	Loss 0.0843 (0.1812)	
training:	Epoch: [10][242/408]	Loss 0.1095 (0.1809)	
training:	Epoch: [10][243/408]	Loss 0.2467 (0.1812)	
training:	Epoch: [10][244/408]	Loss 0.0766 (0.1808)	
training:	Epoch: [10][245/408]	Loss 0.0587 (0.1803)	
training:	Epoch: [10][246/408]	Loss 0.1457 (0.1801)	
training:	Epoch: [10][247/408]	Loss 0.0581 (0.1796)	
training:	Epoch: [10][248/408]	Loss 0.0565 (0.1791)	
training:	Epoch: [10][249/408]	Loss 0.2958 (0.1796)	
training:	Epoch: [10][250/408]	Loss 0.6836 (0.1816)	
training:	Epoch: [10][251/408]	Loss 0.0457 (0.1811)	
training:	Epoch: [10][252/408]	Loss 0.0534 (0.1806)	
training:	Epoch: [10][253/408]	Loss 0.2280 (0.1808)	
training:	Epoch: [10][254/408]	Loss 0.0402 (0.1802)	
training:	Epoch: [10][255/408]	Loss 0.3179 (0.1808)	
training:	Epoch: [10][256/408]	Loss 0.2238 (0.1809)	
training:	Epoch: [10][257/408]	Loss 0.2487 (0.1812)	
training:	Epoch: [10][258/408]	Loss 0.4572 (0.1823)	
training:	Epoch: [10][259/408]	Loss 0.0511 (0.1818)	
training:	Epoch: [10][260/408]	Loss 0.0900 (0.1814)	
training:	Epoch: [10][261/408]	Loss 0.0512 (0.1809)	
training:	Epoch: [10][262/408]	Loss 0.0410 (0.1804)	
training:	Epoch: [10][263/408]	Loss 0.0992 (0.1801)	
training:	Epoch: [10][264/408]	Loss 0.0379 (0.1795)	
training:	Epoch: [10][265/408]	Loss 0.2621 (0.1798)	
training:	Epoch: [10][266/408]	Loss 0.3494 (0.1805)	
training:	Epoch: [10][267/408]	Loss 0.0946 (0.1801)	
training:	Epoch: [10][268/408]	Loss 0.2039 (0.1802)	
training:	Epoch: [10][269/408]	Loss 0.4734 (0.1813)	
training:	Epoch: [10][270/408]	Loss 0.2052 (0.1814)	
training:	Epoch: [10][271/408]	Loss 0.3991 (0.1822)	
training:	Epoch: [10][272/408]	Loss 0.1503 (0.1821)	
training:	Epoch: [10][273/408]	Loss 0.0559 (0.1816)	
training:	Epoch: [10][274/408]	Loss 0.1050 (0.1814)	
training:	Epoch: [10][275/408]	Loss 0.3052 (0.1818)	
training:	Epoch: [10][276/408]	Loss 0.1387 (0.1817)	
training:	Epoch: [10][277/408]	Loss 0.1063 (0.1814)	
training:	Epoch: [10][278/408]	Loss 0.2706 (0.1817)	
training:	Epoch: [10][279/408]	Loss 0.2672 (0.1820)	
training:	Epoch: [10][280/408]	Loss 0.3524 (0.1826)	
training:	Epoch: [10][281/408]	Loss 0.1459 (0.1825)	
training:	Epoch: [10][282/408]	Loss 0.1364 (0.1823)	
training:	Epoch: [10][283/408]	Loss 0.3271 (0.1828)	
training:	Epoch: [10][284/408]	Loss 0.2109 (0.1829)	
training:	Epoch: [10][285/408]	Loss 0.2530 (0.1832)	
training:	Epoch: [10][286/408]	Loss 0.0369 (0.1827)	
training:	Epoch: [10][287/408]	Loss 0.1242 (0.1825)	
training:	Epoch: [10][288/408]	Loss 0.0669 (0.1821)	
training:	Epoch: [10][289/408]	Loss 0.1950 (0.1821)	
training:	Epoch: [10][290/408]	Loss 0.0584 (0.1817)	
training:	Epoch: [10][291/408]	Loss 0.3351 (0.1822)	
training:	Epoch: [10][292/408]	Loss 0.1270 (0.1820)	
training:	Epoch: [10][293/408]	Loss 0.1858 (0.1820)	
training:	Epoch: [10][294/408]	Loss 0.0659 (0.1816)	
training:	Epoch: [10][295/408]	Loss 0.2253 (0.1818)	
training:	Epoch: [10][296/408]	Loss 0.0516 (0.1813)	
training:	Epoch: [10][297/408]	Loss 0.0456 (0.1809)	
training:	Epoch: [10][298/408]	Loss 0.4043 (0.1816)	
training:	Epoch: [10][299/408]	Loss 0.2092 (0.1817)	
training:	Epoch: [10][300/408]	Loss 0.2475 (0.1819)	
training:	Epoch: [10][301/408]	Loss 0.6610 (0.1835)	
training:	Epoch: [10][302/408]	Loss 0.0934 (0.1832)	
training:	Epoch: [10][303/408]	Loss 0.5002 (0.1843)	
training:	Epoch: [10][304/408]	Loss 0.2445 (0.1845)	
training:	Epoch: [10][305/408]	Loss 0.4334 (0.1853)	
training:	Epoch: [10][306/408]	Loss 0.2624 (0.1856)	
training:	Epoch: [10][307/408]	Loss 0.3716 (0.1862)	
training:	Epoch: [10][308/408]	Loss 0.1251 (0.1860)	
training:	Epoch: [10][309/408]	Loss 0.3214 (0.1864)	
training:	Epoch: [10][310/408]	Loss 0.4965 (0.1874)	
training:	Epoch: [10][311/408]	Loss 0.0891 (0.1871)	
training:	Epoch: [10][312/408]	Loss 0.0375 (0.1866)	
training:	Epoch: [10][313/408]	Loss 0.0992 (0.1863)	
training:	Epoch: [10][314/408]	Loss 0.0598 (0.1859)	
training:	Epoch: [10][315/408]	Loss 0.0651 (0.1855)	
training:	Epoch: [10][316/408]	Loss 0.0688 (0.1852)	
training:	Epoch: [10][317/408]	Loss 0.0969 (0.1849)	
training:	Epoch: [10][318/408]	Loss 0.1366 (0.1847)	
training:	Epoch: [10][319/408]	Loss 0.3060 (0.1851)	
training:	Epoch: [10][320/408]	Loss 0.2210 (0.1852)	
training:	Epoch: [10][321/408]	Loss 0.2542 (0.1854)	
training:	Epoch: [10][322/408]	Loss 0.3762 (0.1860)	
training:	Epoch: [10][323/408]	Loss 0.2826 (0.1863)	
training:	Epoch: [10][324/408]	Loss 0.4013 (0.1870)	
training:	Epoch: [10][325/408]	Loss 0.1535 (0.1869)	
training:	Epoch: [10][326/408]	Loss 0.0443 (0.1865)	
training:	Epoch: [10][327/408]	Loss 0.2047 (0.1865)	
training:	Epoch: [10][328/408]	Loss 0.3916 (0.1871)	
training:	Epoch: [10][329/408]	Loss 0.0768 (0.1868)	
training:	Epoch: [10][330/408]	Loss 0.1288 (0.1866)	
training:	Epoch: [10][331/408]	Loss 0.1988 (0.1867)	
training:	Epoch: [10][332/408]	Loss 0.0530 (0.1863)	
training:	Epoch: [10][333/408]	Loss 0.1750 (0.1862)	
training:	Epoch: [10][334/408]	Loss 0.0397 (0.1858)	
training:	Epoch: [10][335/408]	Loss 0.1639 (0.1857)	
training:	Epoch: [10][336/408]	Loss 0.2837 (0.1860)	
training:	Epoch: [10][337/408]	Loss 0.1023 (0.1858)	
training:	Epoch: [10][338/408]	Loss 0.2644 (0.1860)	
training:	Epoch: [10][339/408]	Loss 0.5359 (0.1870)	
training:	Epoch: [10][340/408]	Loss 0.1397 (0.1869)	
training:	Epoch: [10][341/408]	Loss 0.0728 (0.1866)	
training:	Epoch: [10][342/408]	Loss 0.0562 (0.1862)	
training:	Epoch: [10][343/408]	Loss 0.0604 (0.1858)	
training:	Epoch: [10][344/408]	Loss 0.1727 (0.1858)	
training:	Epoch: [10][345/408]	Loss 0.1095 (0.1856)	
training:	Epoch: [10][346/408]	Loss 0.1044 (0.1853)	
training:	Epoch: [10][347/408]	Loss 0.3936 (0.1859)	
training:	Epoch: [10][348/408]	Loss 0.2375 (0.1861)	
training:	Epoch: [10][349/408]	Loss 0.2554 (0.1863)	
training:	Epoch: [10][350/408]	Loss 0.3892 (0.1868)	
training:	Epoch: [10][351/408]	Loss 0.0833 (0.1865)	
training:	Epoch: [10][352/408]	Loss 0.3496 (0.1870)	
training:	Epoch: [10][353/408]	Loss 0.2621 (0.1872)	
training:	Epoch: [10][354/408]	Loss 0.3494 (0.1877)	
training:	Epoch: [10][355/408]	Loss 0.1117 (0.1875)	
training:	Epoch: [10][356/408]	Loss 0.1252 (0.1873)	
training:	Epoch: [10][357/408]	Loss 0.0422 (0.1869)	
training:	Epoch: [10][358/408]	Loss 0.1590 (0.1868)	
training:	Epoch: [10][359/408]	Loss 0.0447 (0.1864)	
training:	Epoch: [10][360/408]	Loss 0.2870 (0.1867)	
training:	Epoch: [10][361/408]	Loss 0.1615 (0.1866)	
training:	Epoch: [10][362/408]	Loss 0.0601 (0.1863)	
training:	Epoch: [10][363/408]	Loss 0.0411 (0.1859)	
training:	Epoch: [10][364/408]	Loss 0.0514 (0.1855)	
training:	Epoch: [10][365/408]	Loss 0.0601 (0.1852)	
training:	Epoch: [10][366/408]	Loss 0.0749 (0.1849)	
training:	Epoch: [10][367/408]	Loss 0.1678 (0.1848)	
training:	Epoch: [10][368/408]	Loss 0.1263 (0.1847)	
training:	Epoch: [10][369/408]	Loss 0.1367 (0.1845)	
training:	Epoch: [10][370/408]	Loss 0.2586 (0.1847)	
training:	Epoch: [10][371/408]	Loss 0.2212 (0.1848)	
training:	Epoch: [10][372/408]	Loss 0.2806 (0.1851)	
training:	Epoch: [10][373/408]	Loss 0.1177 (0.1849)	
training:	Epoch: [10][374/408]	Loss 0.4470 (0.1856)	
training:	Epoch: [10][375/408]	Loss 0.1110 (0.1854)	
training:	Epoch: [10][376/408]	Loss 0.0474 (0.1850)	
training:	Epoch: [10][377/408]	Loss 0.0475 (0.1847)	
training:	Epoch: [10][378/408]	Loss 0.0875 (0.1844)	
training:	Epoch: [10][379/408]	Loss 0.2297 (0.1845)	
training:	Epoch: [10][380/408]	Loss 0.5403 (0.1855)	
training:	Epoch: [10][381/408]	Loss 0.3131 (0.1858)	
training:	Epoch: [10][382/408]	Loss 0.0529 (0.1855)	
training:	Epoch: [10][383/408]	Loss 0.3741 (0.1859)	
training:	Epoch: [10][384/408]	Loss 0.2303 (0.1861)	
training:	Epoch: [10][385/408]	Loss 0.2345 (0.1862)	
training:	Epoch: [10][386/408]	Loss 0.4052 (0.1868)	
training:	Epoch: [10][387/408]	Loss 0.1549 (0.1867)	
training:	Epoch: [10][388/408]	Loss 0.0375 (0.1863)	
training:	Epoch: [10][389/408]	Loss 0.0387 (0.1859)	
training:	Epoch: [10][390/408]	Loss 0.1199 (0.1857)	
training:	Epoch: [10][391/408]	Loss 0.1786 (0.1857)	
training:	Epoch: [10][392/408]	Loss 0.4381 (0.1864)	
training:	Epoch: [10][393/408]	Loss 0.0387 (0.1860)	
training:	Epoch: [10][394/408]	Loss 0.0352 (0.1856)	
training:	Epoch: [10][395/408]	Loss 0.0644 (0.1853)	
training:	Epoch: [10][396/408]	Loss 0.1532 (0.1852)	
training:	Epoch: [10][397/408]	Loss 0.0612 (0.1849)	
training:	Epoch: [10][398/408]	Loss 0.2589 (0.1851)	
training:	Epoch: [10][399/408]	Loss 0.0533 (0.1848)	
training:	Epoch: [10][400/408]	Loss 0.0857 (0.1845)	
training:	Epoch: [10][401/408]	Loss 0.0383 (0.1842)	
training:	Epoch: [10][402/408]	Loss 0.0485 (0.1838)	
training:	Epoch: [10][403/408]	Loss 0.2187 (0.1839)	
training:	Epoch: [10][404/408]	Loss 0.3272 (0.1843)	
training:	Epoch: [10][405/408]	Loss 0.3428 (0.1846)	
training:	Epoch: [10][406/408]	Loss 0.2418 (0.1848)	
training:	Epoch: [10][407/408]	Loss 0.2371 (0.1849)	
training:	Epoch: [10][408/408]	Loss 0.1463 (0.1848)	
Training:	 Loss: 0.1845

Training:	 ACC: 0.9634 0.9628 0.9491 0.9777
Validation:	 ACC: 0.8067 0.8063 0.7984 0.8150
Validation:	 Best_BACC: 0.8226 0.8250 0.8772 0.7679
Validation:	 Loss: 0.5464
Pretraining:	Epoch 11/200
----------
training:	Epoch: [11][1/408]	Loss 0.2378 (0.2378)	
training:	Epoch: [11][2/408]	Loss 0.0632 (0.1505)	
training:	Epoch: [11][3/408]	Loss 0.2924 (0.1978)	
training:	Epoch: [11][4/408]	Loss 0.0646 (0.1645)	
training:	Epoch: [11][5/408]	Loss 0.2256 (0.1767)	
training:	Epoch: [11][6/408]	Loss 0.2636 (0.1912)	
training:	Epoch: [11][7/408]	Loss 0.1934 (0.1915)	
training:	Epoch: [11][8/408]	Loss 0.0489 (0.1737)	
training:	Epoch: [11][9/408]	Loss 0.0630 (0.1614)	
training:	Epoch: [11][10/408]	Loss 0.0539 (0.1506)	
training:	Epoch: [11][11/408]	Loss 0.1059 (0.1466)	
training:	Epoch: [11][12/408]	Loss 0.0352 (0.1373)	
training:	Epoch: [11][13/408]	Loss 0.2829 (0.1485)	
training:	Epoch: [11][14/408]	Loss 0.2520 (0.1559)	
training:	Epoch: [11][15/408]	Loss 0.1336 (0.1544)	
training:	Epoch: [11][16/408]	Loss 0.1629 (0.1549)	
training:	Epoch: [11][17/408]	Loss 0.1820 (0.1565)	
training:	Epoch: [11][18/408]	Loss 0.0821 (0.1524)	
training:	Epoch: [11][19/408]	Loss 0.0546 (0.1472)	
training:	Epoch: [11][20/408]	Loss 0.1326 (0.1465)	
training:	Epoch: [11][21/408]	Loss 0.3717 (0.1572)	
training:	Epoch: [11][22/408]	Loss 0.5167 (0.1736)	
training:	Epoch: [11][23/408]	Loss 0.1831 (0.1740)	
training:	Epoch: [11][24/408]	Loss 0.0410 (0.1684)	
training:	Epoch: [11][25/408]	Loss 0.0480 (0.1636)	
training:	Epoch: [11][26/408]	Loss 0.2630 (0.1674)	
training:	Epoch: [11][27/408]	Loss 0.2280 (0.1697)	
training:	Epoch: [11][28/408]	Loss 0.2633 (0.1730)	
training:	Epoch: [11][29/408]	Loss 0.4102 (0.1812)	
training:	Epoch: [11][30/408]	Loss 0.0497 (0.1768)	
training:	Epoch: [11][31/408]	Loss 0.2666 (0.1797)	
training:	Epoch: [11][32/408]	Loss 0.0559 (0.1759)	
training:	Epoch: [11][33/408]	Loss 0.0541 (0.1722)	
training:	Epoch: [11][34/408]	Loss 0.0797 (0.1694)	
training:	Epoch: [11][35/408]	Loss 0.0697 (0.1666)	
training:	Epoch: [11][36/408]	Loss 0.0507 (0.1634)	
training:	Epoch: [11][37/408]	Loss 0.0616 (0.1606)	
training:	Epoch: [11][38/408]	Loss 0.1529 (0.1604)	
training:	Epoch: [11][39/408]	Loss 0.1047 (0.1590)	
training:	Epoch: [11][40/408]	Loss 0.2716 (0.1618)	
training:	Epoch: [11][41/408]	Loss 0.2517 (0.1640)	
training:	Epoch: [11][42/408]	Loss 0.0744 (0.1619)	
training:	Epoch: [11][43/408]	Loss 0.4731 (0.1691)	
training:	Epoch: [11][44/408]	Loss 0.2955 (0.1720)	
training:	Epoch: [11][45/408]	Loss 0.0630 (0.1696)	
training:	Epoch: [11][46/408]	Loss 0.0954 (0.1679)	
training:	Epoch: [11][47/408]	Loss 0.0468 (0.1654)	
training:	Epoch: [11][48/408]	Loss 0.0961 (0.1639)	
training:	Epoch: [11][49/408]	Loss 0.3947 (0.1686)	
training:	Epoch: [11][50/408]	Loss 0.0449 (0.1662)	
training:	Epoch: [11][51/408]	Loss 0.0671 (0.1642)	
training:	Epoch: [11][52/408]	Loss 0.0922 (0.1628)	
training:	Epoch: [11][53/408]	Loss 0.0574 (0.1608)	
training:	Epoch: [11][54/408]	Loss 0.4233 (0.1657)	
training:	Epoch: [11][55/408]	Loss 0.0729 (0.1640)	
training:	Epoch: [11][56/408]	Loss 0.0997 (0.1629)	
training:	Epoch: [11][57/408]	Loss 0.2307 (0.1641)	
training:	Epoch: [11][58/408]	Loss 0.3260 (0.1668)	
training:	Epoch: [11][59/408]	Loss 0.0375 (0.1647)	
training:	Epoch: [11][60/408]	Loss 0.0375 (0.1625)	
training:	Epoch: [11][61/408]	Loss 0.0755 (0.1611)	
training:	Epoch: [11][62/408]	Loss 0.0530 (0.1594)	
training:	Epoch: [11][63/408]	Loss 0.1327 (0.1589)	
training:	Epoch: [11][64/408]	Loss 0.2320 (0.1601)	
training:	Epoch: [11][65/408]	Loss 0.0583 (0.1585)	
training:	Epoch: [11][66/408]	Loss 0.0449 (0.1568)	
training:	Epoch: [11][67/408]	Loss 0.0351 (0.1550)	
training:	Epoch: [11][68/408]	Loss 0.5152 (0.1603)	
training:	Epoch: [11][69/408]	Loss 0.0907 (0.1593)	
training:	Epoch: [11][70/408]	Loss 0.0967 (0.1584)	
training:	Epoch: [11][71/408]	Loss 0.0500 (0.1568)	
training:	Epoch: [11][72/408]	Loss 0.0620 (0.1555)	
training:	Epoch: [11][73/408]	Loss 0.0355 (0.1539)	
training:	Epoch: [11][74/408]	Loss 0.0330 (0.1523)	
training:	Epoch: [11][75/408]	Loss 0.1411 (0.1521)	
training:	Epoch: [11][76/408]	Loss 0.0583 (0.1509)	
training:	Epoch: [11][77/408]	Loss 0.1200 (0.1505)	
training:	Epoch: [11][78/408]	Loss 0.2544 (0.1518)	
training:	Epoch: [11][79/408]	Loss 0.0984 (0.1511)	
training:	Epoch: [11][80/408]	Loss 0.0518 (0.1499)	
training:	Epoch: [11][81/408]	Loss 0.2593 (0.1512)	
training:	Epoch: [11][82/408]	Loss 0.3173 (0.1533)	
training:	Epoch: [11][83/408]	Loss 0.0660 (0.1522)	
training:	Epoch: [11][84/408]	Loss 0.0471 (0.1510)	
training:	Epoch: [11][85/408]	Loss 0.2523 (0.1522)	
training:	Epoch: [11][86/408]	Loss 0.0765 (0.1513)	
training:	Epoch: [11][87/408]	Loss 0.3097 (0.1531)	
training:	Epoch: [11][88/408]	Loss 0.2724 (0.1544)	
training:	Epoch: [11][89/408]	Loss 0.0349 (0.1531)	
training:	Epoch: [11][90/408]	Loss 0.1515 (0.1531)	
training:	Epoch: [11][91/408]	Loss 0.0568 (0.1520)	
training:	Epoch: [11][92/408]	Loss 0.2894 (0.1535)	
training:	Epoch: [11][93/408]	Loss 0.2188 (0.1542)	
training:	Epoch: [11][94/408]	Loss 0.0523 (0.1531)	
training:	Epoch: [11][95/408]	Loss 0.1009 (0.1526)	
training:	Epoch: [11][96/408]	Loss 0.0707 (0.1517)	
training:	Epoch: [11][97/408]	Loss 0.1815 (0.1520)	
training:	Epoch: [11][98/408]	Loss 0.0574 (0.1511)	
training:	Epoch: [11][99/408]	Loss 0.0549 (0.1501)	
training:	Epoch: [11][100/408]	Loss 0.0680 (0.1493)	
training:	Epoch: [11][101/408]	Loss 0.6177 (0.1539)	
training:	Epoch: [11][102/408]	Loss 0.0294 (0.1527)	
training:	Epoch: [11][103/408]	Loss 0.0574 (0.1518)	
training:	Epoch: [11][104/408]	Loss 0.0369 (0.1507)	
training:	Epoch: [11][105/408]	Loss 0.0360 (0.1496)	
training:	Epoch: [11][106/408]	Loss 0.0865 (0.1490)	
training:	Epoch: [11][107/408]	Loss 0.0305 (0.1479)	
training:	Epoch: [11][108/408]	Loss 0.1067 (0.1475)	
training:	Epoch: [11][109/408]	Loss 0.0401 (0.1465)	
training:	Epoch: [11][110/408]	Loss 0.1364 (0.1464)	
training:	Epoch: [11][111/408]	Loss 0.2590 (0.1474)	
training:	Epoch: [11][112/408]	Loss 0.0803 (0.1468)	
training:	Epoch: [11][113/408]	Loss 0.0789 (0.1462)	
training:	Epoch: [11][114/408]	Loss 0.0545 (0.1454)	
training:	Epoch: [11][115/408]	Loss 0.3441 (0.1472)	
training:	Epoch: [11][116/408]	Loss 0.1217 (0.1469)	
training:	Epoch: [11][117/408]	Loss 0.2891 (0.1482)	
training:	Epoch: [11][118/408]	Loss 0.0410 (0.1472)	
training:	Epoch: [11][119/408]	Loss 0.0817 (0.1467)	
training:	Epoch: [11][120/408]	Loss 0.0304 (0.1457)	
training:	Epoch: [11][121/408]	Loss 0.2409 (0.1465)	
training:	Epoch: [11][122/408]	Loss 0.2790 (0.1476)	
training:	Epoch: [11][123/408]	Loss 0.0388 (0.1467)	
training:	Epoch: [11][124/408]	Loss 0.2500 (0.1475)	
training:	Epoch: [11][125/408]	Loss 0.5424 (0.1507)	
training:	Epoch: [11][126/408]	Loss 0.3674 (0.1524)	
training:	Epoch: [11][127/408]	Loss 0.1881 (0.1527)	
training:	Epoch: [11][128/408]	Loss 0.2064 (0.1531)	
training:	Epoch: [11][129/408]	Loss 0.0803 (0.1526)	
training:	Epoch: [11][130/408]	Loss 0.0312 (0.1516)	
training:	Epoch: [11][131/408]	Loss 0.5174 (0.1544)	
training:	Epoch: [11][132/408]	Loss 0.0626 (0.1537)	
training:	Epoch: [11][133/408]	Loss 0.0410 (0.1529)	
training:	Epoch: [11][134/408]	Loss 0.0349 (0.1520)	
training:	Epoch: [11][135/408]	Loss 0.1378 (0.1519)	
training:	Epoch: [11][136/408]	Loss 0.1570 (0.1519)	
training:	Epoch: [11][137/408]	Loss 0.0650 (0.1513)	
training:	Epoch: [11][138/408]	Loss 0.0506 (0.1506)	
training:	Epoch: [11][139/408]	Loss 0.1272 (0.1504)	
training:	Epoch: [11][140/408]	Loss 0.0421 (0.1496)	
training:	Epoch: [11][141/408]	Loss 0.2817 (0.1506)	
training:	Epoch: [11][142/408]	Loss 0.2591 (0.1513)	
training:	Epoch: [11][143/408]	Loss 0.0278 (0.1505)	
training:	Epoch: [11][144/408]	Loss 0.0372 (0.1497)	
training:	Epoch: [11][145/408]	Loss 0.0281 (0.1488)	
training:	Epoch: [11][146/408]	Loss 0.1139 (0.1486)	
training:	Epoch: [11][147/408]	Loss 0.3671 (0.1501)	
training:	Epoch: [11][148/408]	Loss 0.3537 (0.1515)	
training:	Epoch: [11][149/408]	Loss 0.3052 (0.1525)	
training:	Epoch: [11][150/408]	Loss 0.4350 (0.1544)	
training:	Epoch: [11][151/408]	Loss 0.1424 (0.1543)	
training:	Epoch: [11][152/408]	Loss 0.1102 (0.1540)	
training:	Epoch: [11][153/408]	Loss 0.2351 (0.1545)	
training:	Epoch: [11][154/408]	Loss 0.0479 (0.1538)	
training:	Epoch: [11][155/408]	Loss 0.0899 (0.1534)	
training:	Epoch: [11][156/408]	Loss 0.0760 (0.1529)	
training:	Epoch: [11][157/408]	Loss 0.0996 (0.1526)	
training:	Epoch: [11][158/408]	Loss 0.3214 (0.1537)	
training:	Epoch: [11][159/408]	Loss 0.0444 (0.1530)	
training:	Epoch: [11][160/408]	Loss 0.0300 (0.1522)	
training:	Epoch: [11][161/408]	Loss 0.0723 (0.1517)	
training:	Epoch: [11][162/408]	Loss 0.2919 (0.1526)	
training:	Epoch: [11][163/408]	Loss 0.2093 (0.1529)	
training:	Epoch: [11][164/408]	Loss 0.0476 (0.1523)	
training:	Epoch: [11][165/408]	Loss 0.0846 (0.1519)	
training:	Epoch: [11][166/408]	Loss 0.1686 (0.1520)	
training:	Epoch: [11][167/408]	Loss 0.0666 (0.1515)	
training:	Epoch: [11][168/408]	Loss 0.1008 (0.1512)	
training:	Epoch: [11][169/408]	Loss 0.6086 (0.1539)	
training:	Epoch: [11][170/408]	Loss 0.0989 (0.1535)	
training:	Epoch: [11][171/408]	Loss 0.4169 (0.1551)	
training:	Epoch: [11][172/408]	Loss 0.0493 (0.1545)	
training:	Epoch: [11][173/408]	Loss 0.0708 (0.1540)	
training:	Epoch: [11][174/408]	Loss 0.0479 (0.1534)	
training:	Epoch: [11][175/408]	Loss 0.4559 (0.1551)	
training:	Epoch: [11][176/408]	Loss 0.2263 (0.1555)	
training:	Epoch: [11][177/408]	Loss 0.2565 (0.1561)	
training:	Epoch: [11][178/408]	Loss 0.0775 (0.1556)	
training:	Epoch: [11][179/408]	Loss 0.1772 (0.1558)	
training:	Epoch: [11][180/408]	Loss 0.0717 (0.1553)	
training:	Epoch: [11][181/408]	Loss 0.0539 (0.1547)	
training:	Epoch: [11][182/408]	Loss 0.0997 (0.1544)	
training:	Epoch: [11][183/408]	Loss 0.2486 (0.1549)	
training:	Epoch: [11][184/408]	Loss 0.0451 (0.1543)	
training:	Epoch: [11][185/408]	Loss 0.0577 (0.1538)	
training:	Epoch: [11][186/408]	Loss 0.1593 (0.1539)	
training:	Epoch: [11][187/408]	Loss 0.1785 (0.1540)	
training:	Epoch: [11][188/408]	Loss 0.2823 (0.1547)	
training:	Epoch: [11][189/408]	Loss 0.0393 (0.1541)	
training:	Epoch: [11][190/408]	Loss 0.0552 (0.1535)	
training:	Epoch: [11][191/408]	Loss 0.1285 (0.1534)	
training:	Epoch: [11][192/408]	Loss 0.2631 (0.1540)	
training:	Epoch: [11][193/408]	Loss 0.0469 (0.1534)	
training:	Epoch: [11][194/408]	Loss 0.2112 (0.1537)	
training:	Epoch: [11][195/408]	Loss 0.0361 (0.1531)	
training:	Epoch: [11][196/408]	Loss 0.2867 (0.1538)	
training:	Epoch: [11][197/408]	Loss 0.3257 (0.1547)	
training:	Epoch: [11][198/408]	Loss 0.2967 (0.1554)	
training:	Epoch: [11][199/408]	Loss 0.3751 (0.1565)	
training:	Epoch: [11][200/408]	Loss 0.1207 (0.1563)	
training:	Epoch: [11][201/408]	Loss 0.3119 (0.1571)	
training:	Epoch: [11][202/408]	Loss 0.0594 (0.1566)	
training:	Epoch: [11][203/408]	Loss 0.0339 (0.1560)	
training:	Epoch: [11][204/408]	Loss 0.1329 (0.1559)	
training:	Epoch: [11][205/408]	Loss 0.0473 (0.1554)	
training:	Epoch: [11][206/408]	Loss 0.1986 (0.1556)	
training:	Epoch: [11][207/408]	Loss 0.2735 (0.1561)	
training:	Epoch: [11][208/408]	Loss 0.0970 (0.1559)	
training:	Epoch: [11][209/408]	Loss 0.0367 (0.1553)	
training:	Epoch: [11][210/408]	Loss 0.1171 (0.1551)	
training:	Epoch: [11][211/408]	Loss 0.1925 (0.1553)	
training:	Epoch: [11][212/408]	Loss 0.0323 (0.1547)	
training:	Epoch: [11][213/408]	Loss 0.0475 (0.1542)	
training:	Epoch: [11][214/408]	Loss 0.1431 (0.1541)	
training:	Epoch: [11][215/408]	Loss 0.0963 (0.1539)	
training:	Epoch: [11][216/408]	Loss 0.0985 (0.1536)	
training:	Epoch: [11][217/408]	Loss 0.0596 (0.1532)	
training:	Epoch: [11][218/408]	Loss 0.0379 (0.1527)	
training:	Epoch: [11][219/408]	Loss 0.0458 (0.1522)	
training:	Epoch: [11][220/408]	Loss 0.0463 (0.1517)	
training:	Epoch: [11][221/408]	Loss 0.0622 (0.1513)	
training:	Epoch: [11][222/408]	Loss 0.0791 (0.1510)	
training:	Epoch: [11][223/408]	Loss 0.0358 (0.1504)	
training:	Epoch: [11][224/408]	Loss 0.2744 (0.1510)	
training:	Epoch: [11][225/408]	Loss 0.0501 (0.1505)	
training:	Epoch: [11][226/408]	Loss 0.1797 (0.1507)	
training:	Epoch: [11][227/408]	Loss 0.0491 (0.1502)	
training:	Epoch: [11][228/408]	Loss 0.0770 (0.1499)	
training:	Epoch: [11][229/408]	Loss 0.0608 (0.1495)	
training:	Epoch: [11][230/408]	Loss 0.0369 (0.1490)	
training:	Epoch: [11][231/408]	Loss 0.2364 (0.1494)	
training:	Epoch: [11][232/408]	Loss 0.0641 (0.1490)	
training:	Epoch: [11][233/408]	Loss 0.0373 (0.1486)	
training:	Epoch: [11][234/408]	Loss 0.5937 (0.1505)	
training:	Epoch: [11][235/408]	Loss 0.0400 (0.1500)	
training:	Epoch: [11][236/408]	Loss 0.0530 (0.1496)	
training:	Epoch: [11][237/408]	Loss 0.2620 (0.1501)	
training:	Epoch: [11][238/408]	Loss 0.0811 (0.1498)	
training:	Epoch: [11][239/408]	Loss 0.0330 (0.1493)	
training:	Epoch: [11][240/408]	Loss 0.1077 (0.1491)	
training:	Epoch: [11][241/408]	Loss 0.2249 (0.1494)	
training:	Epoch: [11][242/408]	Loss 0.3362 (0.1502)	
training:	Epoch: [11][243/408]	Loss 0.2594 (0.1506)	
training:	Epoch: [11][244/408]	Loss 0.0337 (0.1502)	
training:	Epoch: [11][245/408]	Loss 0.2797 (0.1507)	
training:	Epoch: [11][246/408]	Loss 0.2229 (0.1510)	
training:	Epoch: [11][247/408]	Loss 0.0728 (0.1507)	
training:	Epoch: [11][248/408]	Loss 0.5126 (0.1521)	
training:	Epoch: [11][249/408]	Loss 0.2533 (0.1525)	
training:	Epoch: [11][250/408]	Loss 0.0723 (0.1522)	
training:	Epoch: [11][251/408]	Loss 0.0294 (0.1517)	
training:	Epoch: [11][252/408]	Loss 0.0410 (0.1513)	
training:	Epoch: [11][253/408]	Loss 0.1978 (0.1515)	
training:	Epoch: [11][254/408]	Loss 0.0861 (0.1512)	
training:	Epoch: [11][255/408]	Loss 0.0968 (0.1510)	
training:	Epoch: [11][256/408]	Loss 0.0607 (0.1506)	
training:	Epoch: [11][257/408]	Loss 0.3827 (0.1515)	
training:	Epoch: [11][258/408]	Loss 0.0383 (0.1511)	
training:	Epoch: [11][259/408]	Loss 0.3021 (0.1517)	
training:	Epoch: [11][260/408]	Loss 0.0461 (0.1513)	
training:	Epoch: [11][261/408]	Loss 0.0493 (0.1509)	
training:	Epoch: [11][262/408]	Loss 0.0588 (0.1505)	
training:	Epoch: [11][263/408]	Loss 0.2799 (0.1510)	
training:	Epoch: [11][264/408]	Loss 0.0426 (0.1506)	
training:	Epoch: [11][265/408]	Loss 0.0333 (0.1502)	
training:	Epoch: [11][266/408]	Loss 0.0365 (0.1497)	
training:	Epoch: [11][267/408]	Loss 0.0840 (0.1495)	
training:	Epoch: [11][268/408]	Loss 0.1038 (0.1493)	
training:	Epoch: [11][269/408]	Loss 0.1197 (0.1492)	
training:	Epoch: [11][270/408]	Loss 0.4073 (0.1502)	
training:	Epoch: [11][271/408]	Loss 0.4828 (0.1514)	
training:	Epoch: [11][272/408]	Loss 0.0683 (0.1511)	
training:	Epoch: [11][273/408]	Loss 0.0977 (0.1509)	
training:	Epoch: [11][274/408]	Loss 0.3250 (0.1515)	
training:	Epoch: [11][275/408]	Loss 0.2838 (0.1520)	
training:	Epoch: [11][276/408]	Loss 0.5185 (0.1533)	
training:	Epoch: [11][277/408]	Loss 0.0602 (0.1530)	
training:	Epoch: [11][278/408]	Loss 0.2507 (0.1534)	
training:	Epoch: [11][279/408]	Loss 0.1474 (0.1533)	
training:	Epoch: [11][280/408]	Loss 0.4351 (0.1543)	
training:	Epoch: [11][281/408]	Loss 0.2705 (0.1548)	
training:	Epoch: [11][282/408]	Loss 0.0504 (0.1544)	
training:	Epoch: [11][283/408]	Loss 0.0356 (0.1540)	
training:	Epoch: [11][284/408]	Loss 0.1024 (0.1538)	
training:	Epoch: [11][285/408]	Loss 0.3957 (0.1546)	
training:	Epoch: [11][286/408]	Loss 0.0393 (0.1542)	
training:	Epoch: [11][287/408]	Loss 0.1482 (0.1542)	
training:	Epoch: [11][288/408]	Loss 0.0957 (0.1540)	
training:	Epoch: [11][289/408]	Loss 0.0288 (0.1536)	
training:	Epoch: [11][290/408]	Loss 0.0631 (0.1533)	
training:	Epoch: [11][291/408]	Loss 0.0478 (0.1529)	
training:	Epoch: [11][292/408]	Loss 0.0667 (0.1526)	
training:	Epoch: [11][293/408]	Loss 0.0961 (0.1524)	
training:	Epoch: [11][294/408]	Loss 0.0388 (0.1520)	
training:	Epoch: [11][295/408]	Loss 0.5900 (0.1535)	
training:	Epoch: [11][296/408]	Loss 0.0525 (0.1532)	
training:	Epoch: [11][297/408]	Loss 0.0650 (0.1529)	
training:	Epoch: [11][298/408]	Loss 0.0567 (0.1526)	
training:	Epoch: [11][299/408]	Loss 0.3049 (0.1531)	
training:	Epoch: [11][300/408]	Loss 0.3716 (0.1538)	
training:	Epoch: [11][301/408]	Loss 0.0805 (0.1535)	
training:	Epoch: [11][302/408]	Loss 0.3039 (0.1540)	
training:	Epoch: [11][303/408]	Loss 0.0535 (0.1537)	
training:	Epoch: [11][304/408]	Loss 0.0365 (0.1533)	
training:	Epoch: [11][305/408]	Loss 0.2163 (0.1535)	
training:	Epoch: [11][306/408]	Loss 0.1783 (0.1536)	
training:	Epoch: [11][307/408]	Loss 0.0395 (0.1532)	
training:	Epoch: [11][308/408]	Loss 0.1187 (0.1531)	
training:	Epoch: [11][309/408]	Loss 0.0606 (0.1528)	
training:	Epoch: [11][310/408]	Loss 0.0365 (0.1525)	
training:	Epoch: [11][311/408]	Loss 0.0373 (0.1521)	
training:	Epoch: [11][312/408]	Loss 0.0799 (0.1519)	
training:	Epoch: [11][313/408]	Loss 0.1036 (0.1517)	
training:	Epoch: [11][314/408]	Loss 0.0662 (0.1514)	
training:	Epoch: [11][315/408]	Loss 0.0714 (0.1512)	
training:	Epoch: [11][316/408]	Loss 0.2826 (0.1516)	
training:	Epoch: [11][317/408]	Loss 0.1060 (0.1514)	
training:	Epoch: [11][318/408]	Loss 0.2211 (0.1517)	
training:	Epoch: [11][319/408]	Loss 0.0977 (0.1515)	
training:	Epoch: [11][320/408]	Loss 0.3039 (0.1520)	
training:	Epoch: [11][321/408]	Loss 0.1309 (0.1519)	
training:	Epoch: [11][322/408]	Loss 0.0470 (0.1516)	
training:	Epoch: [11][323/408]	Loss 0.0302 (0.1512)	
training:	Epoch: [11][324/408]	Loss 0.0277 (0.1508)	
training:	Epoch: [11][325/408]	Loss 0.4455 (0.1517)	
training:	Epoch: [11][326/408]	Loss 0.0441 (0.1514)	
training:	Epoch: [11][327/408]	Loss 0.2295 (0.1516)	
training:	Epoch: [11][328/408]	Loss 0.1937 (0.1518)	
training:	Epoch: [11][329/408]	Loss 0.0662 (0.1515)	
training:	Epoch: [11][330/408]	Loss 0.0319 (0.1511)	
training:	Epoch: [11][331/408]	Loss 0.0775 (0.1509)	
training:	Epoch: [11][332/408]	Loss 0.0540 (0.1506)	
training:	Epoch: [11][333/408]	Loss 0.2481 (0.1509)	
training:	Epoch: [11][334/408]	Loss 0.5050 (0.1520)	
training:	Epoch: [11][335/408]	Loss 0.1380 (0.1519)	
training:	Epoch: [11][336/408]	Loss 0.0302 (0.1516)	
training:	Epoch: [11][337/408]	Loss 0.2467 (0.1519)	
training:	Epoch: [11][338/408]	Loss 0.1606 (0.1519)	
training:	Epoch: [11][339/408]	Loss 0.1087 (0.1518)	
training:	Epoch: [11][340/408]	Loss 0.1534 (0.1518)	
training:	Epoch: [11][341/408]	Loss 0.1635 (0.1518)	
training:	Epoch: [11][342/408]	Loss 0.3673 (0.1524)	
training:	Epoch: [11][343/408]	Loss 0.0453 (0.1521)	
training:	Epoch: [11][344/408]	Loss 0.1525 (0.1521)	
training:	Epoch: [11][345/408]	Loss 0.1631 (0.1521)	
training:	Epoch: [11][346/408]	Loss 0.3692 (0.1528)	
training:	Epoch: [11][347/408]	Loss 0.0590 (0.1525)	
training:	Epoch: [11][348/408]	Loss 0.1032 (0.1524)	
training:	Epoch: [11][349/408]	Loss 0.2124 (0.1525)	
training:	Epoch: [11][350/408]	Loss 0.1060 (0.1524)	
training:	Epoch: [11][351/408]	Loss 0.0520 (0.1521)	
training:	Epoch: [11][352/408]	Loss 0.0927 (0.1519)	
training:	Epoch: [11][353/408]	Loss 0.0798 (0.1517)	
training:	Epoch: [11][354/408]	Loss 0.0928 (0.1516)	
training:	Epoch: [11][355/408]	Loss 0.0583 (0.1513)	
training:	Epoch: [11][356/408]	Loss 0.2262 (0.1515)	
training:	Epoch: [11][357/408]	Loss 0.1449 (0.1515)	
training:	Epoch: [11][358/408]	Loss 0.3087 (0.1519)	
training:	Epoch: [11][359/408]	Loss 0.2319 (0.1522)	
training:	Epoch: [11][360/408]	Loss 0.0455 (0.1519)	
training:	Epoch: [11][361/408]	Loss 0.0355 (0.1515)	
training:	Epoch: [11][362/408]	Loss 0.0887 (0.1514)	
training:	Epoch: [11][363/408]	Loss 0.0905 (0.1512)	
training:	Epoch: [11][364/408]	Loss 0.2422 (0.1515)	
training:	Epoch: [11][365/408]	Loss 0.2900 (0.1518)	
training:	Epoch: [11][366/408]	Loss 0.1115 (0.1517)	
training:	Epoch: [11][367/408]	Loss 0.0643 (0.1515)	
training:	Epoch: [11][368/408]	Loss 0.1623 (0.1515)	
training:	Epoch: [11][369/408]	Loss 0.0498 (0.1512)	
training:	Epoch: [11][370/408]	Loss 0.0426 (0.1509)	
training:	Epoch: [11][371/408]	Loss 0.0474 (0.1507)	
training:	Epoch: [11][372/408]	Loss 0.2751 (0.1510)	
training:	Epoch: [11][373/408]	Loss 0.1239 (0.1509)	
training:	Epoch: [11][374/408]	Loss 0.2423 (0.1512)	
training:	Epoch: [11][375/408]	Loss 0.0306 (0.1509)	
training:	Epoch: [11][376/408]	Loss 0.1917 (0.1510)	
training:	Epoch: [11][377/408]	Loss 0.0476 (0.1507)	
training:	Epoch: [11][378/408]	Loss 0.0562 (0.1504)	
training:	Epoch: [11][379/408]	Loss 0.0800 (0.1503)	
training:	Epoch: [11][380/408]	Loss 0.1663 (0.1503)	
training:	Epoch: [11][381/408]	Loss 0.2551 (0.1506)	
training:	Epoch: [11][382/408]	Loss 0.0397 (0.1503)	
training:	Epoch: [11][383/408]	Loss 0.2105 (0.1504)	
training:	Epoch: [11][384/408]	Loss 0.0336 (0.1501)	
training:	Epoch: [11][385/408]	Loss 0.2605 (0.1504)	
training:	Epoch: [11][386/408]	Loss 0.0456 (0.1501)	
training:	Epoch: [11][387/408]	Loss 0.1839 (0.1502)	
training:	Epoch: [11][388/408]	Loss 0.2085 (0.1504)	
training:	Epoch: [11][389/408]	Loss 0.0445 (0.1501)	
training:	Epoch: [11][390/408]	Loss 0.0663 (0.1499)	
training:	Epoch: [11][391/408]	Loss 0.0432 (0.1496)	
training:	Epoch: [11][392/408]	Loss 0.1455 (0.1496)	
training:	Epoch: [11][393/408]	Loss 0.0860 (0.1495)	
training:	Epoch: [11][394/408]	Loss 0.1899 (0.1496)	
training:	Epoch: [11][395/408]	Loss 0.2816 (0.1499)	
training:	Epoch: [11][396/408]	Loss 0.2733 (0.1502)	
training:	Epoch: [11][397/408]	Loss 0.3678 (0.1508)	
training:	Epoch: [11][398/408]	Loss 0.3069 (0.1511)	
training:	Epoch: [11][399/408]	Loss 0.1561 (0.1512)	
training:	Epoch: [11][400/408]	Loss 0.0363 (0.1509)	
training:	Epoch: [11][401/408]	Loss 0.0692 (0.1507)	
training:	Epoch: [11][402/408]	Loss 0.5216 (0.1516)	
training:	Epoch: [11][403/408]	Loss 0.1119 (0.1515)	
training:	Epoch: [11][404/408]	Loss 0.0972 (0.1514)	
training:	Epoch: [11][405/408]	Loss 0.0257 (0.1510)	
training:	Epoch: [11][406/408]	Loss 0.0379 (0.1508)	
training:	Epoch: [11][407/408]	Loss 0.1720 (0.1508)	
training:	Epoch: [11][408/408]	Loss 0.1668 (0.1509)	
Training:	 Loss: 0.1506

Training:	 ACC: 0.9763 0.9761 0.9712 0.9815
Validation:	 ACC: 0.8015 0.8020 0.8127 0.7904
Validation:	 Best_BACC: 0.8226 0.8250 0.8772 0.7679
Validation:	 Loss: 0.5804
Pretraining:	Epoch 12/200
----------
training:	Epoch: [12][1/408]	Loss 0.0415 (0.0415)	
training:	Epoch: [12][2/408]	Loss 0.2690 (0.1553)	
training:	Epoch: [12][3/408]	Loss 0.3959 (0.2355)	
training:	Epoch: [12][4/408]	Loss 0.0272 (0.1834)	
training:	Epoch: [12][5/408]	Loss 0.2086 (0.1884)	
training:	Epoch: [12][6/408]	Loss 0.0506 (0.1655)	
training:	Epoch: [12][7/408]	Loss 0.0525 (0.1493)	
training:	Epoch: [12][8/408]	Loss 0.0555 (0.1376)	
training:	Epoch: [12][9/408]	Loss 0.0511 (0.1280)	
training:	Epoch: [12][10/408]	Loss 0.0440 (0.1196)	
training:	Epoch: [12][11/408]	Loss 0.0264 (0.1111)	
training:	Epoch: [12][12/408]	Loss 0.2736 (0.1247)	
training:	Epoch: [12][13/408]	Loss 0.0399 (0.1181)	
training:	Epoch: [12][14/408]	Loss 0.0280 (0.1117)	
training:	Epoch: [12][15/408]	Loss 0.2682 (0.1221)	
training:	Epoch: [12][16/408]	Loss 0.0354 (0.1167)	
training:	Epoch: [12][17/408]	Loss 0.2382 (0.1239)	
training:	Epoch: [12][18/408]	Loss 0.0472 (0.1196)	
training:	Epoch: [12][19/408]	Loss 0.0288 (0.1148)	
training:	Epoch: [12][20/408]	Loss 0.0287 (0.1105)	
training:	Epoch: [12][21/408]	Loss 0.0552 (0.1079)	
training:	Epoch: [12][22/408]	Loss 0.0653 (0.1059)	
training:	Epoch: [12][23/408]	Loss 0.5470 (0.1251)	
training:	Epoch: [12][24/408]	Loss 0.0333 (0.1213)	
training:	Epoch: [12][25/408]	Loss 0.2438 (0.1262)	
training:	Epoch: [12][26/408]	Loss 0.0499 (0.1233)	
training:	Epoch: [12][27/408]	Loss 0.0454 (0.1204)	
training:	Epoch: [12][28/408]	Loss 0.0889 (0.1193)	
training:	Epoch: [12][29/408]	Loss 0.2789 (0.1248)	
training:	Epoch: [12][30/408]	Loss 0.0537 (0.1224)	
training:	Epoch: [12][31/408]	Loss 0.1388 (0.1229)	
training:	Epoch: [12][32/408]	Loss 0.1342 (0.1233)	
training:	Epoch: [12][33/408]	Loss 0.0626 (0.1214)	
training:	Epoch: [12][34/408]	Loss 0.1538 (0.1224)	
training:	Epoch: [12][35/408]	Loss 0.2041 (0.1247)	
training:	Epoch: [12][36/408]	Loss 0.0519 (0.1227)	
training:	Epoch: [12][37/408]	Loss 0.0704 (0.1213)	
training:	Epoch: [12][38/408]	Loss 0.1427 (0.1218)	
training:	Epoch: [12][39/408]	Loss 0.2123 (0.1242)	
training:	Epoch: [12][40/408]	Loss 0.0263 (0.1217)	
training:	Epoch: [12][41/408]	Loss 0.0256 (0.1194)	
training:	Epoch: [12][42/408]	Loss 0.0453 (0.1176)	
training:	Epoch: [12][43/408]	Loss 0.0396 (0.1158)	
training:	Epoch: [12][44/408]	Loss 0.0307 (0.1139)	
training:	Epoch: [12][45/408]	Loss 0.1433 (0.1145)	
training:	Epoch: [12][46/408]	Loss 0.0492 (0.1131)	
training:	Epoch: [12][47/408]	Loss 0.0304 (0.1113)	
training:	Epoch: [12][48/408]	Loss 0.1487 (0.1121)	
training:	Epoch: [12][49/408]	Loss 0.1378 (0.1126)	
training:	Epoch: [12][50/408]	Loss 0.0331 (0.1110)	
training:	Epoch: [12][51/408]	Loss 0.0329 (0.1095)	
training:	Epoch: [12][52/408]	Loss 0.1139 (0.1096)	
training:	Epoch: [12][53/408]	Loss 0.1224 (0.1098)	
training:	Epoch: [12][54/408]	Loss 0.2742 (0.1129)	
training:	Epoch: [12][55/408]	Loss 0.2750 (0.1158)	
training:	Epoch: [12][56/408]	Loss 0.0292 (0.1143)	
training:	Epoch: [12][57/408]	Loss 0.0576 (0.1133)	
training:	Epoch: [12][58/408]	Loss 0.0228 (0.1117)	
training:	Epoch: [12][59/408]	Loss 0.2118 (0.1134)	
training:	Epoch: [12][60/408]	Loss 0.0493 (0.1124)	
training:	Epoch: [12][61/408]	Loss 0.1172 (0.1124)	
training:	Epoch: [12][62/408]	Loss 0.0982 (0.1122)	
training:	Epoch: [12][63/408]	Loss 0.0414 (0.1111)	
training:	Epoch: [12][64/408]	Loss 0.2737 (0.1136)	
training:	Epoch: [12][65/408]	Loss 0.2065 (0.1151)	
training:	Epoch: [12][66/408]	Loss 0.4985 (0.1209)	
training:	Epoch: [12][67/408]	Loss 0.0549 (0.1199)	
training:	Epoch: [12][68/408]	Loss 0.1020 (0.1196)	
training:	Epoch: [12][69/408]	Loss 0.0534 (0.1187)	
training:	Epoch: [12][70/408]	Loss 0.0252 (0.1173)	
training:	Epoch: [12][71/408]	Loss 0.0409 (0.1162)	
training:	Epoch: [12][72/408]	Loss 0.1326 (0.1165)	
training:	Epoch: [12][73/408]	Loss 0.3763 (0.1200)	
training:	Epoch: [12][74/408]	Loss 0.0524 (0.1191)	
training:	Epoch: [12][75/408]	Loss 0.0676 (0.1184)	
training:	Epoch: [12][76/408]	Loss 0.0495 (0.1175)	
training:	Epoch: [12][77/408]	Loss 0.0373 (0.1165)	
training:	Epoch: [12][78/408]	Loss 0.6940 (0.1239)	
training:	Epoch: [12][79/408]	Loss 0.1547 (0.1243)	
training:	Epoch: [12][80/408]	Loss 0.2902 (0.1263)	
training:	Epoch: [12][81/408]	Loss 0.0743 (0.1257)	
training:	Epoch: [12][82/408]	Loss 0.0744 (0.1251)	
training:	Epoch: [12][83/408]	Loss 0.1116 (0.1249)	
training:	Epoch: [12][84/408]	Loss 0.0385 (0.1239)	
training:	Epoch: [12][85/408]	Loss 0.1389 (0.1241)	
training:	Epoch: [12][86/408]	Loss 0.0600 (0.1233)	
training:	Epoch: [12][87/408]	Loss 0.0328 (0.1223)	
training:	Epoch: [12][88/408]	Loss 0.0394 (0.1213)	
training:	Epoch: [12][89/408]	Loss 0.3009 (0.1234)	
training:	Epoch: [12][90/408]	Loss 0.2933 (0.1252)	
training:	Epoch: [12][91/408]	Loss 0.0448 (0.1244)	
training:	Epoch: [12][92/408]	Loss 0.0368 (0.1234)	
training:	Epoch: [12][93/408]	Loss 0.3104 (0.1254)	
training:	Epoch: [12][94/408]	Loss 0.0884 (0.1250)	
training:	Epoch: [12][95/408]	Loss 0.0556 (0.1243)	
training:	Epoch: [12][96/408]	Loss 0.0507 (0.1235)	
training:	Epoch: [12][97/408]	Loss 0.0277 (0.1225)	
training:	Epoch: [12][98/408]	Loss 0.0412 (0.1217)	
training:	Epoch: [12][99/408]	Loss 0.0588 (0.1211)	
training:	Epoch: [12][100/408]	Loss 0.1470 (0.1213)	
training:	Epoch: [12][101/408]	Loss 0.0799 (0.1209)	
training:	Epoch: [12][102/408]	Loss 0.4271 (0.1239)	
training:	Epoch: [12][103/408]	Loss 0.0331 (0.1230)	
training:	Epoch: [12][104/408]	Loss 0.0605 (0.1224)	
training:	Epoch: [12][105/408]	Loss 0.3091 (0.1242)	
training:	Epoch: [12][106/408]	Loss 0.0308 (0.1233)	
training:	Epoch: [12][107/408]	Loss 0.3531 (0.1255)	
training:	Epoch: [12][108/408]	Loss 0.0531 (0.1248)	
training:	Epoch: [12][109/408]	Loss 0.0277 (0.1239)	
training:	Epoch: [12][110/408]	Loss 0.0889 (0.1236)	
training:	Epoch: [12][111/408]	Loss 0.0965 (0.1234)	
training:	Epoch: [12][112/408]	Loss 0.2703 (0.1247)	
training:	Epoch: [12][113/408]	Loss 0.2510 (0.1258)	
training:	Epoch: [12][114/408]	Loss 0.0565 (0.1252)	
training:	Epoch: [12][115/408]	Loss 0.0302 (0.1244)	
training:	Epoch: [12][116/408]	Loss 0.1147 (0.1243)	
training:	Epoch: [12][117/408]	Loss 0.0372 (0.1235)	
training:	Epoch: [12][118/408]	Loss 0.1389 (0.1237)	
training:	Epoch: [12][119/408]	Loss 0.0286 (0.1229)	
training:	Epoch: [12][120/408]	Loss 0.0308 (0.1221)	
training:	Epoch: [12][121/408]	Loss 0.0286 (0.1213)	
training:	Epoch: [12][122/408]	Loss 0.0695 (0.1209)	
training:	Epoch: [12][123/408]	Loss 0.0300 (0.1202)	
training:	Epoch: [12][124/408]	Loss 0.0346 (0.1195)	
training:	Epoch: [12][125/408]	Loss 0.2034 (0.1201)	
training:	Epoch: [12][126/408]	Loss 0.1527 (0.1204)	
training:	Epoch: [12][127/408]	Loss 0.0481 (0.1198)	
training:	Epoch: [12][128/408]	Loss 0.0841 (0.1196)	
training:	Epoch: [12][129/408]	Loss 0.0427 (0.1190)	
training:	Epoch: [12][130/408]	Loss 0.4597 (0.1216)	
training:	Epoch: [12][131/408]	Loss 0.0231 (0.1208)	
training:	Epoch: [12][132/408]	Loss 0.0290 (0.1201)	
training:	Epoch: [12][133/408]	Loss 0.0507 (0.1196)	
training:	Epoch: [12][134/408]	Loss 0.2922 (0.1209)	
training:	Epoch: [12][135/408]	Loss 0.2529 (0.1219)	
training:	Epoch: [12][136/408]	Loss 0.2834 (0.1231)	
training:	Epoch: [12][137/408]	Loss 0.2158 (0.1237)	
training:	Epoch: [12][138/408]	Loss 0.0384 (0.1231)	
training:	Epoch: [12][139/408]	Loss 0.0297 (0.1224)	
training:	Epoch: [12][140/408]	Loss 0.0272 (0.1218)	
training:	Epoch: [12][141/408]	Loss 0.0361 (0.1212)	
training:	Epoch: [12][142/408]	Loss 0.1126 (0.1211)	
training:	Epoch: [12][143/408]	Loss 0.2010 (0.1217)	
training:	Epoch: [12][144/408]	Loss 0.2042 (0.1222)	
training:	Epoch: [12][145/408]	Loss 0.0671 (0.1218)	
training:	Epoch: [12][146/408]	Loss 0.1289 (0.1219)	
training:	Epoch: [12][147/408]	Loss 0.5716 (0.1250)	
training:	Epoch: [12][148/408]	Loss 0.0396 (0.1244)	
training:	Epoch: [12][149/408]	Loss 0.0268 (0.1237)	
training:	Epoch: [12][150/408]	Loss 0.0599 (0.1233)	
training:	Epoch: [12][151/408]	Loss 0.0381 (0.1227)	
training:	Epoch: [12][152/408]	Loss 0.1315 (0.1228)	
training:	Epoch: [12][153/408]	Loss 0.0299 (0.1222)	
training:	Epoch: [12][154/408]	Loss 0.0538 (0.1217)	
training:	Epoch: [12][155/408]	Loss 0.0399 (0.1212)	
training:	Epoch: [12][156/408]	Loss 0.1479 (0.1214)	
training:	Epoch: [12][157/408]	Loss 0.4702 (0.1236)	
training:	Epoch: [12][158/408]	Loss 0.4694 (0.1258)	
training:	Epoch: [12][159/408]	Loss 0.0224 (0.1251)	
training:	Epoch: [12][160/408]	Loss 0.0325 (0.1246)	
training:	Epoch: [12][161/408]	Loss 0.1263 (0.1246)	
training:	Epoch: [12][162/408]	Loss 0.0533 (0.1241)	
training:	Epoch: [12][163/408]	Loss 0.3448 (0.1255)	
training:	Epoch: [12][164/408]	Loss 0.0313 (0.1249)	
training:	Epoch: [12][165/408]	Loss 0.2510 (0.1257)	
training:	Epoch: [12][166/408]	Loss 0.1147 (0.1256)	
training:	Epoch: [12][167/408]	Loss 0.0393 (0.1251)	
training:	Epoch: [12][168/408]	Loss 0.1033 (0.1250)	
training:	Epoch: [12][169/408]	Loss 0.0360 (0.1244)	
training:	Epoch: [12][170/408]	Loss 0.0573 (0.1240)	
training:	Epoch: [12][171/408]	Loss 0.0686 (0.1237)	
training:	Epoch: [12][172/408]	Loss 0.0339 (0.1232)	
training:	Epoch: [12][173/408]	Loss 0.0359 (0.1227)	
training:	Epoch: [12][174/408]	Loss 0.1073 (0.1226)	
training:	Epoch: [12][175/408]	Loss 0.1066 (0.1225)	
training:	Epoch: [12][176/408]	Loss 0.0296 (0.1220)	
training:	Epoch: [12][177/408]	Loss 0.2809 (0.1229)	
training:	Epoch: [12][178/408]	Loss 0.3673 (0.1243)	
training:	Epoch: [12][179/408]	Loss 0.2436 (0.1249)	
training:	Epoch: [12][180/408]	Loss 0.0287 (0.1244)	
training:	Epoch: [12][181/408]	Loss 0.0953 (0.1242)	
training:	Epoch: [12][182/408]	Loss 0.0361 (0.1237)	
training:	Epoch: [12][183/408]	Loss 0.0317 (0.1232)	
training:	Epoch: [12][184/408]	Loss 0.0428 (0.1228)	
training:	Epoch: [12][185/408]	Loss 0.0326 (0.1223)	
training:	Epoch: [12][186/408]	Loss 0.0935 (0.1222)	
training:	Epoch: [12][187/408]	Loss 0.0498 (0.1218)	
training:	Epoch: [12][188/408]	Loss 0.0379 (0.1213)	
training:	Epoch: [12][189/408]	Loss 0.0246 (0.1208)	
training:	Epoch: [12][190/408]	Loss 0.0436 (0.1204)	
training:	Epoch: [12][191/408]	Loss 0.3897 (0.1218)	
training:	Epoch: [12][192/408]	Loss 0.0252 (0.1213)	
training:	Epoch: [12][193/408]	Loss 0.0353 (0.1209)	
training:	Epoch: [12][194/408]	Loss 0.0492 (0.1205)	
training:	Epoch: [12][195/408]	Loss 0.0451 (0.1201)	
training:	Epoch: [12][196/408]	Loss 0.0243 (0.1196)	
training:	Epoch: [12][197/408]	Loss 0.0840 (0.1194)	
training:	Epoch: [12][198/408]	Loss 0.0341 (0.1190)	
training:	Epoch: [12][199/408]	Loss 0.0376 (0.1186)	
training:	Epoch: [12][200/408]	Loss 0.0334 (0.1182)	
training:	Epoch: [12][201/408]	Loss 0.0728 (0.1180)	
training:	Epoch: [12][202/408]	Loss 0.5006 (0.1198)	
training:	Epoch: [12][203/408]	Loss 0.3035 (0.1208)	
training:	Epoch: [12][204/408]	Loss 0.0809 (0.1206)	
training:	Epoch: [12][205/408]	Loss 0.0603 (0.1203)	
training:	Epoch: [12][206/408]	Loss 0.1075 (0.1202)	
training:	Epoch: [12][207/408]	Loss 0.0258 (0.1197)	
training:	Epoch: [12][208/408]	Loss 0.0711 (0.1195)	
training:	Epoch: [12][209/408]	Loss 0.0234 (0.1191)	
training:	Epoch: [12][210/408]	Loss 0.1930 (0.1194)	
training:	Epoch: [12][211/408]	Loss 0.2964 (0.1202)	
training:	Epoch: [12][212/408]	Loss 0.0342 (0.1198)	
training:	Epoch: [12][213/408]	Loss 0.2409 (0.1204)	
training:	Epoch: [12][214/408]	Loss 0.0275 (0.1200)	
training:	Epoch: [12][215/408]	Loss 0.0788 (0.1198)	
training:	Epoch: [12][216/408]	Loss 0.2853 (0.1205)	
training:	Epoch: [12][217/408]	Loss 0.3824 (0.1218)	
training:	Epoch: [12][218/408]	Loss 0.2698 (0.1224)	
training:	Epoch: [12][219/408]	Loss 0.4815 (0.1241)	
training:	Epoch: [12][220/408]	Loss 0.0281 (0.1236)	
training:	Epoch: [12][221/408]	Loss 0.0396 (0.1233)	
training:	Epoch: [12][222/408]	Loss 0.0244 (0.1228)	
training:	Epoch: [12][223/408]	Loss 0.0587 (0.1225)	
training:	Epoch: [12][224/408]	Loss 0.2125 (0.1229)	
training:	Epoch: [12][225/408]	Loss 0.0412 (0.1226)	
training:	Epoch: [12][226/408]	Loss 0.2631 (0.1232)	
training:	Epoch: [12][227/408]	Loss 0.0228 (0.1227)	
training:	Epoch: [12][228/408]	Loss 0.0425 (0.1224)	
training:	Epoch: [12][229/408]	Loss 0.0545 (0.1221)	
training:	Epoch: [12][230/408]	Loss 0.1011 (0.1220)	
training:	Epoch: [12][231/408]	Loss 0.0642 (0.1217)	
training:	Epoch: [12][232/408]	Loss 0.4970 (0.1234)	
training:	Epoch: [12][233/408]	Loss 0.0257 (0.1229)	
training:	Epoch: [12][234/408]	Loss 0.0250 (0.1225)	
training:	Epoch: [12][235/408]	Loss 0.2575 (0.1231)	
training:	Epoch: [12][236/408]	Loss 0.0336 (0.1227)	
training:	Epoch: [12][237/408]	Loss 0.0906 (0.1226)	
training:	Epoch: [12][238/408]	Loss 0.0251 (0.1222)	
training:	Epoch: [12][239/408]	Loss 0.0470 (0.1219)	
training:	Epoch: [12][240/408]	Loss 0.0886 (0.1217)	
training:	Epoch: [12][241/408]	Loss 0.0243 (0.1213)	
training:	Epoch: [12][242/408]	Loss 0.0293 (0.1209)	
training:	Epoch: [12][243/408]	Loss 0.0394 (0.1206)	
training:	Epoch: [12][244/408]	Loss 0.0385 (0.1203)	
training:	Epoch: [12][245/408]	Loss 0.2408 (0.1208)	
training:	Epoch: [12][246/408]	Loss 0.0289 (0.1204)	
training:	Epoch: [12][247/408]	Loss 0.0781 (0.1202)	
training:	Epoch: [12][248/408]	Loss 0.2744 (0.1208)	
training:	Epoch: [12][249/408]	Loss 0.0671 (0.1206)	
training:	Epoch: [12][250/408]	Loss 0.5280 (0.1223)	
training:	Epoch: [12][251/408]	Loss 0.0262 (0.1219)	
training:	Epoch: [12][252/408]	Loss 0.0270 (0.1215)	
training:	Epoch: [12][253/408]	Loss 0.0243 (0.1211)	
training:	Epoch: [12][254/408]	Loss 0.2649 (0.1217)	
training:	Epoch: [12][255/408]	Loss 0.2726 (0.1223)	
training:	Epoch: [12][256/408]	Loss 0.2537 (0.1228)	
training:	Epoch: [12][257/408]	Loss 0.2595 (0.1233)	
training:	Epoch: [12][258/408]	Loss 0.3934 (0.1244)	
training:	Epoch: [12][259/408]	Loss 0.0369 (0.1240)	
training:	Epoch: [12][260/408]	Loss 0.0355 (0.1237)	
training:	Epoch: [12][261/408]	Loss 0.0308 (0.1233)	
training:	Epoch: [12][262/408]	Loss 0.2559 (0.1238)	
training:	Epoch: [12][263/408]	Loss 0.0378 (0.1235)	
training:	Epoch: [12][264/408]	Loss 0.0273 (0.1231)	
training:	Epoch: [12][265/408]	Loss 0.0380 (0.1228)	
training:	Epoch: [12][266/408]	Loss 0.0389 (0.1225)	
training:	Epoch: [12][267/408]	Loss 0.0391 (0.1222)	
training:	Epoch: [12][268/408]	Loss 0.1802 (0.1224)	
training:	Epoch: [12][269/408]	Loss 0.0257 (0.1220)	
training:	Epoch: [12][270/408]	Loss 0.0245 (0.1217)	
training:	Epoch: [12][271/408]	Loss 0.0247 (0.1213)	
training:	Epoch: [12][272/408]	Loss 0.0737 (0.1212)	
training:	Epoch: [12][273/408]	Loss 0.0398 (0.1209)	
training:	Epoch: [12][274/408]	Loss 0.0808 (0.1207)	
training:	Epoch: [12][275/408]	Loss 0.2512 (0.1212)	
training:	Epoch: [12][276/408]	Loss 0.2291 (0.1216)	
training:	Epoch: [12][277/408]	Loss 0.0789 (0.1214)	
training:	Epoch: [12][278/408]	Loss 0.1126 (0.1214)	
training:	Epoch: [12][279/408]	Loss 0.1215 (0.1214)	
training:	Epoch: [12][280/408]	Loss 0.1627 (0.1215)	
training:	Epoch: [12][281/408]	Loss 0.1170 (0.1215)	
training:	Epoch: [12][282/408]	Loss 0.0706 (0.1213)	
training:	Epoch: [12][283/408]	Loss 0.0749 (0.1212)	
training:	Epoch: [12][284/408]	Loss 0.1252 (0.1212)	
training:	Epoch: [12][285/408]	Loss 0.0519 (0.1209)	
training:	Epoch: [12][286/408]	Loss 0.0930 (0.1208)	
training:	Epoch: [12][287/408]	Loss 0.0538 (0.1206)	
training:	Epoch: [12][288/408]	Loss 0.0571 (0.1204)	
training:	Epoch: [12][289/408]	Loss 0.0236 (0.1201)	
training:	Epoch: [12][290/408]	Loss 0.1227 (0.1201)	
training:	Epoch: [12][291/408]	Loss 0.3691 (0.1209)	
training:	Epoch: [12][292/408]	Loss 0.2643 (0.1214)	
training:	Epoch: [12][293/408]	Loss 0.2778 (0.1219)	
training:	Epoch: [12][294/408]	Loss 0.0857 (0.1218)	
training:	Epoch: [12][295/408]	Loss 0.0234 (0.1215)	
training:	Epoch: [12][296/408]	Loss 0.0322 (0.1212)	
training:	Epoch: [12][297/408]	Loss 0.2293 (0.1216)	
training:	Epoch: [12][298/408]	Loss 0.4305 (0.1226)	
training:	Epoch: [12][299/408]	Loss 0.0420 (0.1223)	
training:	Epoch: [12][300/408]	Loss 0.2809 (0.1229)	
training:	Epoch: [12][301/408]	Loss 0.0269 (0.1225)	
training:	Epoch: [12][302/408]	Loss 0.3440 (0.1233)	
training:	Epoch: [12][303/408]	Loss 0.3282 (0.1239)	
training:	Epoch: [12][304/408]	Loss 0.1712 (0.1241)	
training:	Epoch: [12][305/408]	Loss 0.0734 (0.1239)	
training:	Epoch: [12][306/408]	Loss 0.0256 (0.1236)	
training:	Epoch: [12][307/408]	Loss 0.0599 (0.1234)	
training:	Epoch: [12][308/408]	Loss 0.0664 (0.1232)	
training:	Epoch: [12][309/408]	Loss 0.0242 (0.1229)	
training:	Epoch: [12][310/408]	Loss 0.0435 (0.1226)	
training:	Epoch: [12][311/408]	Loss 0.0275 (0.1223)	
training:	Epoch: [12][312/408]	Loss 0.1817 (0.1225)	
training:	Epoch: [12][313/408]	Loss 0.0262 (0.1222)	
training:	Epoch: [12][314/408]	Loss 0.0499 (0.1220)	
training:	Epoch: [12][315/408]	Loss 0.2705 (0.1225)	
training:	Epoch: [12][316/408]	Loss 0.1192 (0.1224)	
training:	Epoch: [12][317/408]	Loss 0.6190 (0.1240)	
training:	Epoch: [12][318/408]	Loss 0.0601 (0.1238)	
training:	Epoch: [12][319/408]	Loss 0.3664 (0.1246)	
training:	Epoch: [12][320/408]	Loss 0.3713 (0.1253)	
training:	Epoch: [12][321/408]	Loss 0.0714 (0.1252)	
training:	Epoch: [12][322/408]	Loss 0.2491 (0.1256)	
training:	Epoch: [12][323/408]	Loss 0.0440 (0.1253)	
training:	Epoch: [12][324/408]	Loss 0.3754 (0.1261)	
training:	Epoch: [12][325/408]	Loss 0.2556 (0.1265)	
training:	Epoch: [12][326/408]	Loss 0.3379 (0.1271)	
training:	Epoch: [12][327/408]	Loss 0.1669 (0.1272)	
training:	Epoch: [12][328/408]	Loss 0.0441 (0.1270)	
training:	Epoch: [12][329/408]	Loss 0.1233 (0.1270)	
training:	Epoch: [12][330/408]	Loss 0.3789 (0.1277)	
training:	Epoch: [12][331/408]	Loss 0.0437 (0.1275)	
training:	Epoch: [12][332/408]	Loss 0.1237 (0.1275)	
training:	Epoch: [12][333/408]	Loss 0.0244 (0.1272)	
training:	Epoch: [12][334/408]	Loss 0.0429 (0.1269)	
training:	Epoch: [12][335/408]	Loss 0.1841 (0.1271)	
training:	Epoch: [12][336/408]	Loss 0.0395 (0.1268)	
training:	Epoch: [12][337/408]	Loss 0.0275 (0.1265)	
training:	Epoch: [12][338/408]	Loss 0.3107 (0.1271)	
training:	Epoch: [12][339/408]	Loss 0.0497 (0.1269)	
training:	Epoch: [12][340/408]	Loss 0.0776 (0.1267)	
training:	Epoch: [12][341/408]	Loss 0.0273 (0.1264)	
training:	Epoch: [12][342/408]	Loss 0.2068 (0.1267)	
training:	Epoch: [12][343/408]	Loss 0.4388 (0.1276)	
training:	Epoch: [12][344/408]	Loss 0.2418 (0.1279)	
training:	Epoch: [12][345/408]	Loss 0.2930 (0.1284)	
training:	Epoch: [12][346/408]	Loss 0.2384 (0.1287)	
training:	Epoch: [12][347/408]	Loss 0.0372 (0.1284)	
training:	Epoch: [12][348/408]	Loss 0.0483 (0.1282)	
training:	Epoch: [12][349/408]	Loss 0.1237 (0.1282)	
training:	Epoch: [12][350/408]	Loss 0.0261 (0.1279)	
training:	Epoch: [12][351/408]	Loss 0.0496 (0.1277)	
training:	Epoch: [12][352/408]	Loss 0.2697 (0.1281)	
training:	Epoch: [12][353/408]	Loss 0.0563 (0.1279)	
training:	Epoch: [12][354/408]	Loss 0.0427 (0.1276)	
training:	Epoch: [12][355/408]	Loss 0.0240 (0.1273)	
training:	Epoch: [12][356/408]	Loss 0.0320 (0.1271)	
training:	Epoch: [12][357/408]	Loss 0.0662 (0.1269)	
training:	Epoch: [12][358/408]	Loss 0.2348 (0.1272)	
training:	Epoch: [12][359/408]	Loss 0.0386 (0.1270)	
training:	Epoch: [12][360/408]	Loss 0.0402 (0.1267)	
training:	Epoch: [12][361/408]	Loss 0.0255 (0.1264)	
training:	Epoch: [12][362/408]	Loss 0.0509 (0.1262)	
training:	Epoch: [12][363/408]	Loss 0.0227 (0.1259)	
training:	Epoch: [12][364/408]	Loss 0.0259 (0.1257)	
training:	Epoch: [12][365/408]	Loss 0.2441 (0.1260)	
training:	Epoch: [12][366/408]	Loss 0.0428 (0.1258)	
training:	Epoch: [12][367/408]	Loss 0.1082 (0.1257)	
training:	Epoch: [12][368/408]	Loss 0.0784 (0.1256)	
training:	Epoch: [12][369/408]	Loss 0.0776 (0.1255)	
training:	Epoch: [12][370/408]	Loss 0.0491 (0.1252)	
training:	Epoch: [12][371/408]	Loss 0.0930 (0.1252)	
training:	Epoch: [12][372/408]	Loss 0.0413 (0.1249)	
training:	Epoch: [12][373/408]	Loss 0.0260 (0.1247)	
training:	Epoch: [12][374/408]	Loss 0.3597 (0.1253)	
training:	Epoch: [12][375/408]	Loss 0.0586 (0.1251)	
training:	Epoch: [12][376/408]	Loss 0.0309 (0.1249)	
training:	Epoch: [12][377/408]	Loss 0.0401 (0.1246)	
training:	Epoch: [12][378/408]	Loss 0.1542 (0.1247)	
training:	Epoch: [12][379/408]	Loss 0.0395 (0.1245)	
training:	Epoch: [12][380/408]	Loss 0.0412 (0.1243)	
training:	Epoch: [12][381/408]	Loss 0.0380 (0.1241)	
training:	Epoch: [12][382/408]	Loss 0.0319 (0.1238)	
training:	Epoch: [12][383/408]	Loss 0.0274 (0.1236)	
training:	Epoch: [12][384/408]	Loss 0.0580 (0.1234)	
training:	Epoch: [12][385/408]	Loss 0.0282 (0.1231)	
training:	Epoch: [12][386/408]	Loss 0.0349 (0.1229)	
training:	Epoch: [12][387/408]	Loss 0.0467 (0.1227)	
training:	Epoch: [12][388/408]	Loss 0.1193 (0.1227)	
training:	Epoch: [12][389/408]	Loss 0.2509 (0.1230)	
training:	Epoch: [12][390/408]	Loss 0.0289 (0.1228)	
training:	Epoch: [12][391/408]	Loss 0.0275 (0.1226)	
training:	Epoch: [12][392/408]	Loss 0.0360 (0.1223)	
training:	Epoch: [12][393/408]	Loss 0.0643 (0.1222)	
training:	Epoch: [12][394/408]	Loss 0.2188 (0.1224)	
training:	Epoch: [12][395/408]	Loss 0.3081 (0.1229)	
training:	Epoch: [12][396/408]	Loss 0.0383 (0.1227)	
training:	Epoch: [12][397/408]	Loss 0.1165 (0.1227)	
training:	Epoch: [12][398/408]	Loss 0.4253 (0.1234)	
training:	Epoch: [12][399/408]	Loss 0.1434 (0.1235)	
training:	Epoch: [12][400/408]	Loss 0.0747 (0.1234)	
training:	Epoch: [12][401/408]	Loss 0.0453 (0.1232)	
training:	Epoch: [12][402/408]	Loss 0.0275 (0.1229)	
training:	Epoch: [12][403/408]	Loss 0.0258 (0.1227)	
training:	Epoch: [12][404/408]	Loss 0.2890 (0.1231)	
training:	Epoch: [12][405/408]	Loss 0.0356 (0.1229)	
training:	Epoch: [12][406/408]	Loss 0.1073 (0.1228)	
training:	Epoch: [12][407/408]	Loss 0.1302 (0.1229)	
training:	Epoch: [12][408/408]	Loss 0.3206 (0.1233)	
Training:	 Loss: 0.1232

Training:	 ACC: 0.9769 0.9764 0.9653 0.9885
Validation:	 ACC: 0.7893 0.7881 0.7636 0.8150
Validation:	 Best_BACC: 0.8226 0.8250 0.8772 0.7679
Validation:	 Loss: 0.6465
Pretraining:	Epoch 13/200
----------
training:	Epoch: [13][1/408]	Loss 0.2936 (0.2936)	
training:	Epoch: [13][2/408]	Loss 0.2926 (0.2931)	
training:	Epoch: [13][3/408]	Loss 0.0351 (0.2071)	
training:	Epoch: [13][4/408]	Loss 0.0298 (0.1628)	
training:	Epoch: [13][5/408]	Loss 0.0279 (0.1358)	
training:	Epoch: [13][6/408]	Loss 0.1287 (0.1346)	
training:	Epoch: [13][7/408]	Loss 0.0316 (0.1199)	
training:	Epoch: [13][8/408]	Loss 0.1611 (0.1250)	
training:	Epoch: [13][9/408]	Loss 0.0290 (0.1144)	
training:	Epoch: [13][10/408]	Loss 0.2572 (0.1286)	
training:	Epoch: [13][11/408]	Loss 0.0230 (0.1190)	
training:	Epoch: [13][12/408]	Loss 0.0228 (0.1110)	
training:	Epoch: [13][13/408]	Loss 0.0505 (0.1064)	
training:	Epoch: [13][14/408]	Loss 0.0299 (0.1009)	
training:	Epoch: [13][15/408]	Loss 0.2578 (0.1114)	
training:	Epoch: [13][16/408]	Loss 0.2785 (0.1218)	
training:	Epoch: [13][17/408]	Loss 0.2645 (0.1302)	
training:	Epoch: [13][18/408]	Loss 0.0280 (0.1245)	
training:	Epoch: [13][19/408]	Loss 0.0249 (0.1193)	
training:	Epoch: [13][20/408]	Loss 0.0340 (0.1150)	
training:	Epoch: [13][21/408]	Loss 0.0422 (0.1115)	
training:	Epoch: [13][22/408]	Loss 0.0223 (0.1075)	
training:	Epoch: [13][23/408]	Loss 0.0331 (0.1043)	
training:	Epoch: [13][24/408]	Loss 0.1391 (0.1057)	
training:	Epoch: [13][25/408]	Loss 0.0433 (0.1032)	
training:	Epoch: [13][26/408]	Loss 0.4694 (0.1173)	
training:	Epoch: [13][27/408]	Loss 0.0334 (0.1142)	
training:	Epoch: [13][28/408]	Loss 0.2743 (0.1199)	
training:	Epoch: [13][29/408]	Loss 0.0486 (0.1174)	
training:	Epoch: [13][30/408]	Loss 0.0430 (0.1150)	
training:	Epoch: [13][31/408]	Loss 0.0281 (0.1122)	
training:	Epoch: [13][32/408]	Loss 0.2838 (0.1175)	
training:	Epoch: [13][33/408]	Loss 0.2763 (0.1223)	
training:	Epoch: [13][34/408]	Loss 0.2527 (0.1262)	
training:	Epoch: [13][35/408]	Loss 0.2712 (0.1303)	
training:	Epoch: [13][36/408]	Loss 0.0522 (0.1281)	
training:	Epoch: [13][37/408]	Loss 0.0336 (0.1256)	
training:	Epoch: [13][38/408]	Loss 0.0360 (0.1232)	
training:	Epoch: [13][39/408]	Loss 0.0235 (0.1207)	
training:	Epoch: [13][40/408]	Loss 0.2489 (0.1239)	
training:	Epoch: [13][41/408]	Loss 0.1546 (0.1246)	
training:	Epoch: [13][42/408]	Loss 0.0280 (0.1223)	
training:	Epoch: [13][43/408]	Loss 0.0395 (0.1204)	
training:	Epoch: [13][44/408]	Loss 0.0638 (0.1191)	
training:	Epoch: [13][45/408]	Loss 0.0470 (0.1175)	
training:	Epoch: [13][46/408]	Loss 0.0388 (0.1158)	
training:	Epoch: [13][47/408]	Loss 0.1037 (0.1155)	
training:	Epoch: [13][48/408]	Loss 0.2814 (0.1190)	
training:	Epoch: [13][49/408]	Loss 0.2848 (0.1224)	
training:	Epoch: [13][50/408]	Loss 0.0212 (0.1204)	
training:	Epoch: [13][51/408]	Loss 0.0237 (0.1185)	
training:	Epoch: [13][52/408]	Loss 0.1088 (0.1183)	
training:	Epoch: [13][53/408]	Loss 0.0555 (0.1171)	
training:	Epoch: [13][54/408]	Loss 0.0533 (0.1159)	
training:	Epoch: [13][55/408]	Loss 0.0418 (0.1146)	
training:	Epoch: [13][56/408]	Loss 0.3473 (0.1187)	
training:	Epoch: [13][57/408]	Loss 0.0254 (0.1171)	
training:	Epoch: [13][58/408]	Loss 0.0268 (0.1155)	
training:	Epoch: [13][59/408]	Loss 0.0251 (0.1140)	
training:	Epoch: [13][60/408]	Loss 0.2686 (0.1166)	
training:	Epoch: [13][61/408]	Loss 0.3168 (0.1199)	
training:	Epoch: [13][62/408]	Loss 0.2698 (0.1223)	
training:	Epoch: [13][63/408]	Loss 0.2289 (0.1240)	
training:	Epoch: [13][64/408]	Loss 0.0412 (0.1227)	
training:	Epoch: [13][65/408]	Loss 0.0658 (0.1218)	
training:	Epoch: [13][66/408]	Loss 0.1582 (0.1223)	
training:	Epoch: [13][67/408]	Loss 0.5317 (0.1285)	
training:	Epoch: [13][68/408]	Loss 0.0389 (0.1271)	
training:	Epoch: [13][69/408]	Loss 0.0387 (0.1259)	
training:	Epoch: [13][70/408]	Loss 0.3637 (0.1293)	
training:	Epoch: [13][71/408]	Loss 0.0618 (0.1283)	
training:	Epoch: [13][72/408]	Loss 0.2648 (0.1302)	
training:	Epoch: [13][73/408]	Loss 0.0355 (0.1289)	
training:	Epoch: [13][74/408]	Loss 0.0484 (0.1278)	
training:	Epoch: [13][75/408]	Loss 0.0237 (0.1264)	
training:	Epoch: [13][76/408]	Loss 0.0266 (0.1251)	
training:	Epoch: [13][77/408]	Loss 0.0254 (0.1238)	
training:	Epoch: [13][78/408]	Loss 0.0890 (0.1234)	
training:	Epoch: [13][79/408]	Loss 0.0703 (0.1227)	
training:	Epoch: [13][80/408]	Loss 0.0279 (0.1215)	
training:	Epoch: [13][81/408]	Loss 0.0239 (0.1203)	
training:	Epoch: [13][82/408]	Loss 0.0563 (0.1195)	
training:	Epoch: [13][83/408]	Loss 0.0246 (0.1184)	
training:	Epoch: [13][84/408]	Loss 0.0291 (0.1173)	
training:	Epoch: [13][85/408]	Loss 0.2325 (0.1187)	
training:	Epoch: [13][86/408]	Loss 0.0293 (0.1176)	
training:	Epoch: [13][87/408]	Loss 0.0410 (0.1168)	
training:	Epoch: [13][88/408]	Loss 0.0241 (0.1157)	
training:	Epoch: [13][89/408]	Loss 0.2543 (0.1173)	
training:	Epoch: [13][90/408]	Loss 0.0248 (0.1162)	
training:	Epoch: [13][91/408]	Loss 0.1148 (0.1162)	
training:	Epoch: [13][92/408]	Loss 0.2177 (0.1173)	
training:	Epoch: [13][93/408]	Loss 0.1642 (0.1178)	
training:	Epoch: [13][94/408]	Loss 0.3351 (0.1201)	
training:	Epoch: [13][95/408]	Loss 0.0219 (0.1191)	
training:	Epoch: [13][96/408]	Loss 0.0376 (0.1183)	
training:	Epoch: [13][97/408]	Loss 0.0356 (0.1174)	
training:	Epoch: [13][98/408]	Loss 0.0246 (0.1165)	
training:	Epoch: [13][99/408]	Loss 0.0346 (0.1156)	
training:	Epoch: [13][100/408]	Loss 0.0476 (0.1149)	
training:	Epoch: [13][101/408]	Loss 0.0333 (0.1141)	
training:	Epoch: [13][102/408]	Loss 0.2396 (0.1154)	
training:	Epoch: [13][103/408]	Loss 0.0299 (0.1145)	
training:	Epoch: [13][104/408]	Loss 0.0747 (0.1142)	
training:	Epoch: [13][105/408]	Loss 0.0262 (0.1133)	
training:	Epoch: [13][106/408]	Loss 0.2808 (0.1149)	
training:	Epoch: [13][107/408]	Loss 0.0752 (0.1145)	
training:	Epoch: [13][108/408]	Loss 0.0275 (0.1137)	
training:	Epoch: [13][109/408]	Loss 0.2710 (0.1152)	
training:	Epoch: [13][110/408]	Loss 0.0311 (0.1144)	
training:	Epoch: [13][111/408]	Loss 0.0214 (0.1136)	
training:	Epoch: [13][112/408]	Loss 0.0261 (0.1128)	
training:	Epoch: [13][113/408]	Loss 0.0330 (0.1121)	
training:	Epoch: [13][114/408]	Loss 0.0283 (0.1113)	
training:	Epoch: [13][115/408]	Loss 0.2750 (0.1128)	
training:	Epoch: [13][116/408]	Loss 0.2815 (0.1142)	
training:	Epoch: [13][117/408]	Loss 0.0253 (0.1135)	
training:	Epoch: [13][118/408]	Loss 0.0255 (0.1127)	
training:	Epoch: [13][119/408]	Loss 0.0889 (0.1125)	
training:	Epoch: [13][120/408]	Loss 0.0260 (0.1118)	
training:	Epoch: [13][121/408]	Loss 0.4108 (0.1143)	
training:	Epoch: [13][122/408]	Loss 0.0278 (0.1136)	
training:	Epoch: [13][123/408]	Loss 0.2469 (0.1146)	
training:	Epoch: [13][124/408]	Loss 0.0584 (0.1142)	
training:	Epoch: [13][125/408]	Loss 0.0251 (0.1135)	
training:	Epoch: [13][126/408]	Loss 0.0235 (0.1128)	
training:	Epoch: [13][127/408]	Loss 0.0435 (0.1122)	
training:	Epoch: [13][128/408]	Loss 0.0234 (0.1115)	
training:	Epoch: [13][129/408]	Loss 0.1435 (0.1118)	
training:	Epoch: [13][130/408]	Loss 0.2522 (0.1128)	
training:	Epoch: [13][131/408]	Loss 0.0285 (0.1122)	
training:	Epoch: [13][132/408]	Loss 0.0286 (0.1116)	
training:	Epoch: [13][133/408]	Loss 0.0589 (0.1112)	
training:	Epoch: [13][134/408]	Loss 0.5195 (0.1142)	
training:	Epoch: [13][135/408]	Loss 0.0202 (0.1135)	
training:	Epoch: [13][136/408]	Loss 0.0386 (0.1130)	
training:	Epoch: [13][137/408]	Loss 0.0418 (0.1125)	
training:	Epoch: [13][138/408]	Loss 0.2489 (0.1134)	
training:	Epoch: [13][139/408]	Loss 0.2754 (0.1146)	
training:	Epoch: [13][140/408]	Loss 0.1669 (0.1150)	
training:	Epoch: [13][141/408]	Loss 0.2803 (0.1162)	
training:	Epoch: [13][142/408]	Loss 0.0220 (0.1155)	
training:	Epoch: [13][143/408]	Loss 0.0271 (0.1149)	
training:	Epoch: [13][144/408]	Loss 0.0323 (0.1143)	
training:	Epoch: [13][145/408]	Loss 0.3226 (0.1157)	
training:	Epoch: [13][146/408]	Loss 0.0499 (0.1153)	
training:	Epoch: [13][147/408]	Loss 0.0483 (0.1148)	
training:	Epoch: [13][148/408]	Loss 0.0398 (0.1143)	
training:	Epoch: [13][149/408]	Loss 0.0749 (0.1141)	
training:	Epoch: [13][150/408]	Loss 0.0455 (0.1136)	
training:	Epoch: [13][151/408]	Loss 0.0231 (0.1130)	
training:	Epoch: [13][152/408]	Loss 0.0264 (0.1124)	
training:	Epoch: [13][153/408]	Loss 0.0286 (0.1119)	
training:	Epoch: [13][154/408]	Loss 0.0648 (0.1116)	
training:	Epoch: [13][155/408]	Loss 0.1504 (0.1118)	
training:	Epoch: [13][156/408]	Loss 0.0307 (0.1113)	
training:	Epoch: [13][157/408]	Loss 0.0393 (0.1109)	
training:	Epoch: [13][158/408]	Loss 0.3866 (0.1126)	
training:	Epoch: [13][159/408]	Loss 0.2839 (0.1137)	
training:	Epoch: [13][160/408]	Loss 0.0271 (0.1131)	
training:	Epoch: [13][161/408]	Loss 0.0222 (0.1126)	
training:	Epoch: [13][162/408]	Loss 0.2400 (0.1134)	
training:	Epoch: [13][163/408]	Loss 0.0353 (0.1129)	
training:	Epoch: [13][164/408]	Loss 0.0327 (0.1124)	
training:	Epoch: [13][165/408]	Loss 0.0940 (0.1123)	
training:	Epoch: [13][166/408]	Loss 0.0216 (0.1117)	
training:	Epoch: [13][167/408]	Loss 0.2323 (0.1124)	
training:	Epoch: [13][168/408]	Loss 0.0520 (0.1121)	
training:	Epoch: [13][169/408]	Loss 0.0521 (0.1117)	
training:	Epoch: [13][170/408]	Loss 0.0227 (0.1112)	
training:	Epoch: [13][171/408]	Loss 0.0411 (0.1108)	
training:	Epoch: [13][172/408]	Loss 0.0331 (0.1103)	
training:	Epoch: [13][173/408]	Loss 0.0229 (0.1098)	
training:	Epoch: [13][174/408]	Loss 0.5095 (0.1121)	
training:	Epoch: [13][175/408]	Loss 0.0206 (0.1116)	
training:	Epoch: [13][176/408]	Loss 0.0376 (0.1112)	
training:	Epoch: [13][177/408]	Loss 0.0273 (0.1107)	
training:	Epoch: [13][178/408]	Loss 0.0293 (0.1103)	
training:	Epoch: [13][179/408]	Loss 0.2467 (0.1110)	
training:	Epoch: [13][180/408]	Loss 0.0268 (0.1106)	
training:	Epoch: [13][181/408]	Loss 0.2265 (0.1112)	
training:	Epoch: [13][182/408]	Loss 0.1924 (0.1116)	
training:	Epoch: [13][183/408]	Loss 0.0277 (0.1112)	
training:	Epoch: [13][184/408]	Loss 0.2770 (0.1121)	
training:	Epoch: [13][185/408]	Loss 0.1927 (0.1125)	
training:	Epoch: [13][186/408]	Loss 0.0293 (0.1121)	
training:	Epoch: [13][187/408]	Loss 0.0429 (0.1117)	
training:	Epoch: [13][188/408]	Loss 0.0918 (0.1116)	
training:	Epoch: [13][189/408]	Loss 0.0261 (0.1111)	
training:	Epoch: [13][190/408]	Loss 0.0270 (0.1107)	
training:	Epoch: [13][191/408]	Loss 0.2081 (0.1112)	
training:	Epoch: [13][192/408]	Loss 0.0830 (0.1111)	
training:	Epoch: [13][193/408]	Loss 0.0242 (0.1106)	
training:	Epoch: [13][194/408]	Loss 0.2673 (0.1114)	
training:	Epoch: [13][195/408]	Loss 0.0320 (0.1110)	
training:	Epoch: [13][196/408]	Loss 0.1288 (0.1111)	
training:	Epoch: [13][197/408]	Loss 0.1274 (0.1112)	
training:	Epoch: [13][198/408]	Loss 0.0213 (0.1107)	
training:	Epoch: [13][199/408]	Loss 0.0666 (0.1105)	
training:	Epoch: [13][200/408]	Loss 0.0217 (0.1101)	
training:	Epoch: [13][201/408]	Loss 0.0275 (0.1097)	
training:	Epoch: [13][202/408]	Loss 0.0275 (0.1093)	
training:	Epoch: [13][203/408]	Loss 0.2866 (0.1101)	
training:	Epoch: [13][204/408]	Loss 0.0324 (0.1098)	
training:	Epoch: [13][205/408]	Loss 0.0267 (0.1093)	
training:	Epoch: [13][206/408]	Loss 0.0297 (0.1090)	
training:	Epoch: [13][207/408]	Loss 0.0371 (0.1086)	
training:	Epoch: [13][208/408]	Loss 0.5043 (0.1105)	
training:	Epoch: [13][209/408]	Loss 0.0692 (0.1103)	
training:	Epoch: [13][210/408]	Loss 0.0374 (0.1100)	
training:	Epoch: [13][211/408]	Loss 0.0609 (0.1097)	
training:	Epoch: [13][212/408]	Loss 0.2134 (0.1102)	
training:	Epoch: [13][213/408]	Loss 0.0403 (0.1099)	
training:	Epoch: [13][214/408]	Loss 0.2737 (0.1107)	
training:	Epoch: [13][215/408]	Loss 0.0463 (0.1104)	
training:	Epoch: [13][216/408]	Loss 0.2219 (0.1109)	
training:	Epoch: [13][217/408]	Loss 0.0238 (0.1105)	
training:	Epoch: [13][218/408]	Loss 0.2112 (0.1109)	
training:	Epoch: [13][219/408]	Loss 0.0319 (0.1106)	
training:	Epoch: [13][220/408]	Loss 0.0233 (0.1102)	
training:	Epoch: [13][221/408]	Loss 0.1445 (0.1103)	
training:	Epoch: [13][222/408]	Loss 0.0227 (0.1099)	
training:	Epoch: [13][223/408]	Loss 0.4954 (0.1117)	
training:	Epoch: [13][224/408]	Loss 0.0238 (0.1113)	
training:	Epoch: [13][225/408]	Loss 0.0253 (0.1109)	
training:	Epoch: [13][226/408]	Loss 0.2606 (0.1116)	
training:	Epoch: [13][227/408]	Loss 0.0904 (0.1115)	
training:	Epoch: [13][228/408]	Loss 0.0668 (0.1113)	
training:	Epoch: [13][229/408]	Loss 0.2694 (0.1120)	
training:	Epoch: [13][230/408]	Loss 0.0332 (0.1116)	
training:	Epoch: [13][231/408]	Loss 0.0488 (0.1113)	
training:	Epoch: [13][232/408]	Loss 0.0217 (0.1110)	
training:	Epoch: [13][233/408]	Loss 0.1154 (0.1110)	
training:	Epoch: [13][234/408]	Loss 0.0565 (0.1107)	
training:	Epoch: [13][235/408]	Loss 0.0286 (0.1104)	
training:	Epoch: [13][236/408]	Loss 0.0559 (0.1102)	
training:	Epoch: [13][237/408]	Loss 0.2494 (0.1108)	
training:	Epoch: [13][238/408]	Loss 0.0364 (0.1104)	
training:	Epoch: [13][239/408]	Loss 0.1926 (0.1108)	
training:	Epoch: [13][240/408]	Loss 0.0356 (0.1105)	
training:	Epoch: [13][241/408]	Loss 0.0225 (0.1101)	
training:	Epoch: [13][242/408]	Loss 0.0653 (0.1099)	
training:	Epoch: [13][243/408]	Loss 0.2992 (0.1107)	
training:	Epoch: [13][244/408]	Loss 0.2532 (0.1113)	
training:	Epoch: [13][245/408]	Loss 0.0472 (0.1110)	
training:	Epoch: [13][246/408]	Loss 0.0609 (0.1108)	
training:	Epoch: [13][247/408]	Loss 0.0328 (0.1105)	
training:	Epoch: [13][248/408]	Loss 0.0235 (0.1102)	
training:	Epoch: [13][249/408]	Loss 0.0588 (0.1099)	
training:	Epoch: [13][250/408]	Loss 0.0959 (0.1099)	
training:	Epoch: [13][251/408]	Loss 0.2706 (0.1105)	
training:	Epoch: [13][252/408]	Loss 0.0550 (0.1103)	
training:	Epoch: [13][253/408]	Loss 0.0770 (0.1102)	
training:	Epoch: [13][254/408]	Loss 0.0832 (0.1101)	
training:	Epoch: [13][255/408]	Loss 0.0342 (0.1098)	
training:	Epoch: [13][256/408]	Loss 0.2561 (0.1103)	
training:	Epoch: [13][257/408]	Loss 0.0217 (0.1100)	
training:	Epoch: [13][258/408]	Loss 0.0435 (0.1097)	
training:	Epoch: [13][259/408]	Loss 0.0545 (0.1095)	
training:	Epoch: [13][260/408]	Loss 0.1240 (0.1096)	
training:	Epoch: [13][261/408]	Loss 0.0389 (0.1093)	
training:	Epoch: [13][262/408]	Loss 0.0335 (0.1090)	
training:	Epoch: [13][263/408]	Loss 0.2575 (0.1096)	
training:	Epoch: [13][264/408]	Loss 0.0338 (0.1093)	
training:	Epoch: [13][265/408]	Loss 0.2505 (0.1098)	
training:	Epoch: [13][266/408]	Loss 0.0317 (0.1095)	
training:	Epoch: [13][267/408]	Loss 0.0642 (0.1094)	
training:	Epoch: [13][268/408]	Loss 0.2424 (0.1099)	
training:	Epoch: [13][269/408]	Loss 0.0360 (0.1096)	
training:	Epoch: [13][270/408]	Loss 0.2329 (0.1101)	
training:	Epoch: [13][271/408]	Loss 0.0571 (0.1099)	
training:	Epoch: [13][272/408]	Loss 0.0555 (0.1097)	
training:	Epoch: [13][273/408]	Loss 0.0258 (0.1093)	
training:	Epoch: [13][274/408]	Loss 0.2740 (0.1099)	
training:	Epoch: [13][275/408]	Loss 0.0317 (0.1097)	
training:	Epoch: [13][276/408]	Loss 0.0215 (0.1093)	
training:	Epoch: [13][277/408]	Loss 0.0321 (0.1091)	
training:	Epoch: [13][278/408]	Loss 0.0312 (0.1088)	
training:	Epoch: [13][279/408]	Loss 0.0253 (0.1085)	
training:	Epoch: [13][280/408]	Loss 0.0662 (0.1083)	
training:	Epoch: [13][281/408]	Loss 0.0214 (0.1080)	
training:	Epoch: [13][282/408]	Loss 0.0218 (0.1077)	
training:	Epoch: [13][283/408]	Loss 0.0205 (0.1074)	
training:	Epoch: [13][284/408]	Loss 0.2504 (0.1079)	
training:	Epoch: [13][285/408]	Loss 0.0223 (0.1076)	
training:	Epoch: [13][286/408]	Loss 0.0225 (0.1073)	
training:	Epoch: [13][287/408]	Loss 0.0297 (0.1070)	
training:	Epoch: [13][288/408]	Loss 0.0587 (0.1069)	
training:	Epoch: [13][289/408]	Loss 0.0220 (0.1066)	
training:	Epoch: [13][290/408]	Loss 0.2700 (0.1071)	
training:	Epoch: [13][291/408]	Loss 0.1001 (0.1071)	
training:	Epoch: [13][292/408]	Loss 0.0316 (0.1069)	
training:	Epoch: [13][293/408]	Loss 0.0240 (0.1066)	
training:	Epoch: [13][294/408]	Loss 0.0639 (0.1064)	
training:	Epoch: [13][295/408]	Loss 0.0413 (0.1062)	
training:	Epoch: [13][296/408]	Loss 0.0239 (0.1059)	
training:	Epoch: [13][297/408]	Loss 0.0964 (0.1059)	
training:	Epoch: [13][298/408]	Loss 0.2955 (0.1065)	
training:	Epoch: [13][299/408]	Loss 0.0844 (0.1065)	
training:	Epoch: [13][300/408]	Loss 0.2876 (0.1071)	
training:	Epoch: [13][301/408]	Loss 0.0273 (0.1068)	
training:	Epoch: [13][302/408]	Loss 0.0208 (0.1065)	
training:	Epoch: [13][303/408]	Loss 0.0230 (0.1062)	
training:	Epoch: [13][304/408]	Loss 0.0256 (0.1060)	
training:	Epoch: [13][305/408]	Loss 0.0226 (0.1057)	
training:	Epoch: [13][306/408]	Loss 0.1494 (0.1059)	
training:	Epoch: [13][307/408]	Loss 0.0214 (0.1056)	
training:	Epoch: [13][308/408]	Loss 0.0212 (0.1053)	
training:	Epoch: [13][309/408]	Loss 0.0346 (0.1051)	
training:	Epoch: [13][310/408]	Loss 0.0434 (0.1049)	
training:	Epoch: [13][311/408]	Loss 0.0262 (0.1046)	
training:	Epoch: [13][312/408]	Loss 0.0204 (0.1044)	
training:	Epoch: [13][313/408]	Loss 0.0241 (0.1041)	
training:	Epoch: [13][314/408]	Loss 0.0266 (0.1038)	
training:	Epoch: [13][315/408]	Loss 0.2054 (0.1042)	
training:	Epoch: [13][316/408]	Loss 0.0244 (0.1039)	
training:	Epoch: [13][317/408]	Loss 0.0939 (0.1039)	
training:	Epoch: [13][318/408]	Loss 0.0992 (0.1039)	
training:	Epoch: [13][319/408]	Loss 0.0204 (0.1036)	
training:	Epoch: [13][320/408]	Loss 0.0263 (0.1034)	
training:	Epoch: [13][321/408]	Loss 0.0216 (0.1031)	
training:	Epoch: [13][322/408]	Loss 0.0281 (0.1029)	
training:	Epoch: [13][323/408]	Loss 0.0206 (0.1026)	
training:	Epoch: [13][324/408]	Loss 0.0268 (0.1024)	
training:	Epoch: [13][325/408]	Loss 0.0582 (0.1023)	
training:	Epoch: [13][326/408]	Loss 0.0339 (0.1020)	
training:	Epoch: [13][327/408]	Loss 0.2347 (0.1025)	
training:	Epoch: [13][328/408]	Loss 0.0326 (0.1022)	
training:	Epoch: [13][329/408]	Loss 0.0237 (0.1020)	
training:	Epoch: [13][330/408]	Loss 0.0313 (0.1018)	
training:	Epoch: [13][331/408]	Loss 0.3928 (0.1027)	
training:	Epoch: [13][332/408]	Loss 0.4785 (0.1038)	
training:	Epoch: [13][333/408]	Loss 0.0360 (0.1036)	
training:	Epoch: [13][334/408]	Loss 0.0318 (0.1034)	
training:	Epoch: [13][335/408]	Loss 0.1634 (0.1036)	
training:	Epoch: [13][336/408]	Loss 0.0319 (0.1033)	
training:	Epoch: [13][337/408]	Loss 0.0314 (0.1031)	
training:	Epoch: [13][338/408]	Loss 0.2567 (0.1036)	
training:	Epoch: [13][339/408]	Loss 0.0254 (0.1034)	
training:	Epoch: [13][340/408]	Loss 0.0314 (0.1031)	
training:	Epoch: [13][341/408]	Loss 0.0218 (0.1029)	
training:	Epoch: [13][342/408]	Loss 0.0342 (0.1027)	
training:	Epoch: [13][343/408]	Loss 0.2656 (0.1032)	
training:	Epoch: [13][344/408]	Loss 0.0206 (0.1029)	
training:	Epoch: [13][345/408]	Loss 0.0248 (0.1027)	
training:	Epoch: [13][346/408]	Loss 0.2577 (0.1032)	
training:	Epoch: [13][347/408]	Loss 0.0253 (0.1029)	
training:	Epoch: [13][348/408]	Loss 0.0204 (0.1027)	
training:	Epoch: [13][349/408]	Loss 0.4777 (0.1038)	
training:	Epoch: [13][350/408]	Loss 0.0785 (0.1037)	
training:	Epoch: [13][351/408]	Loss 0.2460 (0.1041)	
training:	Epoch: [13][352/408]	Loss 0.0216 (0.1039)	
training:	Epoch: [13][353/408]	Loss 0.0314 (0.1037)	
training:	Epoch: [13][354/408]	Loss 0.0226 (0.1034)	
training:	Epoch: [13][355/408]	Loss 0.0232 (0.1032)	
training:	Epoch: [13][356/408]	Loss 0.2822 (0.1037)	
training:	Epoch: [13][357/408]	Loss 0.0272 (0.1035)	
training:	Epoch: [13][358/408]	Loss 0.0348 (0.1033)	
training:	Epoch: [13][359/408]	Loss 0.0197 (0.1031)	
training:	Epoch: [13][360/408]	Loss 0.0572 (0.1030)	
training:	Epoch: [13][361/408]	Loss 0.0457 (0.1028)	
training:	Epoch: [13][362/408]	Loss 0.0210 (0.1026)	
training:	Epoch: [13][363/408]	Loss 0.2417 (0.1029)	
training:	Epoch: [13][364/408]	Loss 0.1059 (0.1030)	
training:	Epoch: [13][365/408]	Loss 0.0272 (0.1027)	
training:	Epoch: [13][366/408]	Loss 0.0236 (0.1025)	
training:	Epoch: [13][367/408]	Loss 0.0249 (0.1023)	
training:	Epoch: [13][368/408]	Loss 0.0211 (0.1021)	
training:	Epoch: [13][369/408]	Loss 0.0548 (0.1020)	
training:	Epoch: [13][370/408]	Loss 0.0261 (0.1018)	
training:	Epoch: [13][371/408]	Loss 0.0643 (0.1017)	
training:	Epoch: [13][372/408]	Loss 0.0359 (0.1015)	
training:	Epoch: [13][373/408]	Loss 0.2519 (0.1019)	
training:	Epoch: [13][374/408]	Loss 0.0243 (0.1017)	
training:	Epoch: [13][375/408]	Loss 0.0230 (0.1015)	
training:	Epoch: [13][376/408]	Loss 0.2976 (0.1020)	
training:	Epoch: [13][377/408]	Loss 0.0432 (0.1018)	
training:	Epoch: [13][378/408]	Loss 0.0290 (0.1016)	
training:	Epoch: [13][379/408]	Loss 0.0234 (0.1014)	
training:	Epoch: [13][380/408]	Loss 0.0374 (0.1013)	
training:	Epoch: [13][381/408]	Loss 0.0259 (0.1011)	
training:	Epoch: [13][382/408]	Loss 0.0217 (0.1009)	
training:	Epoch: [13][383/408]	Loss 0.0211 (0.1007)	
training:	Epoch: [13][384/408]	Loss 0.0211 (0.1005)	
training:	Epoch: [13][385/408]	Loss 0.0234 (0.1003)	
training:	Epoch: [13][386/408]	Loss 0.0338 (0.1001)	
training:	Epoch: [13][387/408]	Loss 0.0266 (0.0999)	
training:	Epoch: [13][388/408]	Loss 0.0251 (0.0997)	
training:	Epoch: [13][389/408]	Loss 0.0693 (0.0996)	
training:	Epoch: [13][390/408]	Loss 0.0269 (0.0994)	
training:	Epoch: [13][391/408]	Loss 0.2498 (0.0998)	
training:	Epoch: [13][392/408]	Loss 0.0255 (0.0996)	
training:	Epoch: [13][393/408]	Loss 0.0785 (0.0996)	
training:	Epoch: [13][394/408]	Loss 0.0211 (0.0994)	
training:	Epoch: [13][395/408]	Loss 0.2842 (0.0998)	
training:	Epoch: [13][396/408]	Loss 0.0186 (0.0996)	
training:	Epoch: [13][397/408]	Loss 0.0204 (0.0994)	
training:	Epoch: [13][398/408]	Loss 0.0265 (0.0993)	
training:	Epoch: [13][399/408]	Loss 0.0184 (0.0991)	
training:	Epoch: [13][400/408]	Loss 0.0331 (0.0989)	
training:	Epoch: [13][401/408]	Loss 0.0412 (0.0987)	
training:	Epoch: [13][402/408]	Loss 0.0334 (0.0986)	
training:	Epoch: [13][403/408]	Loss 0.0834 (0.0985)	
training:	Epoch: [13][404/408]	Loss 0.0445 (0.0984)	
training:	Epoch: [13][405/408]	Loss 0.0233 (0.0982)	
training:	Epoch: [13][406/408]	Loss 0.0226 (0.0980)	
training:	Epoch: [13][407/408]	Loss 0.0206 (0.0978)	
training:	Epoch: [13][408/408]	Loss 0.0471 (0.0977)	
Training:	 Loss: 0.0976

Training:	 ACC: 0.9809 0.9806 0.9721 0.9898
Validation:	 ACC: 0.7835 0.7822 0.7554 0.8117
Validation:	 Best_BACC: 0.8226 0.8250 0.8772 0.7679
Validation:	 Loss: 0.7010
Pretraining:	Epoch 14/200
----------
training:	Epoch: [14][1/408]	Loss 0.0251 (0.0251)	
training:	Epoch: [14][2/408]	Loss 0.0223 (0.0237)	
training:	Epoch: [14][3/408]	Loss 0.0194 (0.0222)	
training:	Epoch: [14][4/408]	Loss 0.0214 (0.0220)	
training:	Epoch: [14][5/408]	Loss 0.0223 (0.0221)	
training:	Epoch: [14][6/408]	Loss 0.0207 (0.0219)	
training:	Epoch: [14][7/408]	Loss 0.0271 (0.0226)	
training:	Epoch: [14][8/408]	Loss 0.0913 (0.0312)	
training:	Epoch: [14][9/408]	Loss 0.0214 (0.0301)	
training:	Epoch: [14][10/408]	Loss 0.2501 (0.0521)	
training:	Epoch: [14][11/408]	Loss 0.0409 (0.0511)	
training:	Epoch: [14][12/408]	Loss 0.2831 (0.0704)	
training:	Epoch: [14][13/408]	Loss 0.0222 (0.0667)	
training:	Epoch: [14][14/408]	Loss 0.0207 (0.0634)	
training:	Epoch: [14][15/408]	Loss 0.0242 (0.0608)	
training:	Epoch: [14][16/408]	Loss 0.2557 (0.0730)	
training:	Epoch: [14][17/408]	Loss 0.0206 (0.0699)	
training:	Epoch: [14][18/408]	Loss 0.0252 (0.0674)	
training:	Epoch: [14][19/408]	Loss 0.0229 (0.0651)	
training:	Epoch: [14][20/408]	Loss 0.0223 (0.0629)	
training:	Epoch: [14][21/408]	Loss 0.0176 (0.0608)	
training:	Epoch: [14][22/408]	Loss 0.2537 (0.0695)	
training:	Epoch: [14][23/408]	Loss 0.0205 (0.0674)	
training:	Epoch: [14][24/408]	Loss 0.0208 (0.0655)	
training:	Epoch: [14][25/408]	Loss 0.0239 (0.0638)	
training:	Epoch: [14][26/408]	Loss 0.5299 (0.0817)	
training:	Epoch: [14][27/408]	Loss 0.0225 (0.0795)	
training:	Epoch: [14][28/408]	Loss 0.0204 (0.0774)	
training:	Epoch: [14][29/408]	Loss 0.0207 (0.0755)	
training:	Epoch: [14][30/408]	Loss 0.0471 (0.0745)	
training:	Epoch: [14][31/408]	Loss 0.0269 (0.0730)	
training:	Epoch: [14][32/408]	Loss 0.0398 (0.0720)	
training:	Epoch: [14][33/408]	Loss 0.0160 (0.0703)	
training:	Epoch: [14][34/408]	Loss 0.0215 (0.0688)	
training:	Epoch: [14][35/408]	Loss 0.0219 (0.0675)	
training:	Epoch: [14][36/408]	Loss 0.0745 (0.0677)	
training:	Epoch: [14][37/408]	Loss 0.2549 (0.0727)	
training:	Epoch: [14][38/408]	Loss 0.0207 (0.0714)	
training:	Epoch: [14][39/408]	Loss 0.0291 (0.0703)	
training:	Epoch: [14][40/408]	Loss 0.0479 (0.0697)	
training:	Epoch: [14][41/408]	Loss 0.0231 (0.0686)	
training:	Epoch: [14][42/408]	Loss 0.0208 (0.0675)	
training:	Epoch: [14][43/408]	Loss 0.0463 (0.0670)	
training:	Epoch: [14][44/408]	Loss 0.0220 (0.0659)	
training:	Epoch: [14][45/408]	Loss 0.2934 (0.0710)	
training:	Epoch: [14][46/408]	Loss 0.2838 (0.0756)	
training:	Epoch: [14][47/408]	Loss 0.0394 (0.0748)	
training:	Epoch: [14][48/408]	Loss 0.0174 (0.0736)	
training:	Epoch: [14][49/408]	Loss 0.4001 (0.0803)	
training:	Epoch: [14][50/408]	Loss 0.1894 (0.0825)	
training:	Epoch: [14][51/408]	Loss 0.0197 (0.0813)	
training:	Epoch: [14][52/408]	Loss 0.0241 (0.0802)	
training:	Epoch: [14][53/408]	Loss 0.0183 (0.0790)	
training:	Epoch: [14][54/408]	Loss 0.2642 (0.0824)	
training:	Epoch: [14][55/408]	Loss 0.3031 (0.0864)	
training:	Epoch: [14][56/408]	Loss 0.0216 (0.0853)	
training:	Epoch: [14][57/408]	Loss 0.0181 (0.0841)	
training:	Epoch: [14][58/408]	Loss 0.0200 (0.0830)	
training:	Epoch: [14][59/408]	Loss 0.2369 (0.0856)	
training:	Epoch: [14][60/408]	Loss 0.0245 (0.0846)	
training:	Epoch: [14][61/408]	Loss 0.0251 (0.0836)	
training:	Epoch: [14][62/408]	Loss 0.2581 (0.0864)	
training:	Epoch: [14][63/408]	Loss 0.0294 (0.0855)	
training:	Epoch: [14][64/408]	Loss 0.1938 (0.0872)	
training:	Epoch: [14][65/408]	Loss 0.0295 (0.0863)	
training:	Epoch: [14][66/408]	Loss 0.0383 (0.0856)	
training:	Epoch: [14][67/408]	Loss 0.0173 (0.0846)	
training:	Epoch: [14][68/408]	Loss 0.0204 (0.0836)	
training:	Epoch: [14][69/408]	Loss 0.0216 (0.0827)	
training:	Epoch: [14][70/408]	Loss 0.0275 (0.0819)	
training:	Epoch: [14][71/408]	Loss 0.0338 (0.0813)	
training:	Epoch: [14][72/408]	Loss 0.2608 (0.0838)	
training:	Epoch: [14][73/408]	Loss 0.0564 (0.0834)	
training:	Epoch: [14][74/408]	Loss 0.0206 (0.0825)	
training:	Epoch: [14][75/408]	Loss 0.0224 (0.0817)	
training:	Epoch: [14][76/408]	Loss 0.0253 (0.0810)	
training:	Epoch: [14][77/408]	Loss 0.0240 (0.0803)	
training:	Epoch: [14][78/408]	Loss 0.0215 (0.0795)	
training:	Epoch: [14][79/408]	Loss 0.0190 (0.0787)	
training:	Epoch: [14][80/408]	Loss 0.0202 (0.0780)	
training:	Epoch: [14][81/408]	Loss 0.0424 (0.0776)	
training:	Epoch: [14][82/408]	Loss 0.0632 (0.0774)	
training:	Epoch: [14][83/408]	Loss 0.0417 (0.0770)	
training:	Epoch: [14][84/408]	Loss 0.3720 (0.0805)	
training:	Epoch: [14][85/408]	Loss 0.0225 (0.0798)	
training:	Epoch: [14][86/408]	Loss 0.0193 (0.0791)	
training:	Epoch: [14][87/408]	Loss 0.2595 (0.0812)	
training:	Epoch: [14][88/408]	Loss 0.3063 (0.0837)	
training:	Epoch: [14][89/408]	Loss 0.0238 (0.0830)	
training:	Epoch: [14][90/408]	Loss 0.0354 (0.0825)	
training:	Epoch: [14][91/408]	Loss 0.0186 (0.0818)	
training:	Epoch: [14][92/408]	Loss 0.0213 (0.0812)	
training:	Epoch: [14][93/408]	Loss 0.0208 (0.0805)	
training:	Epoch: [14][94/408]	Loss 0.7881 (0.0880)	
training:	Epoch: [14][95/408]	Loss 0.0227 (0.0873)	
training:	Epoch: [14][96/408]	Loss 0.0208 (0.0867)	
training:	Epoch: [14][97/408]	Loss 0.0531 (0.0863)	
training:	Epoch: [14][98/408]	Loss 0.0246 (0.0857)	
training:	Epoch: [14][99/408]	Loss 0.0197 (0.0850)	
training:	Epoch: [14][100/408]	Loss 0.3421 (0.0876)	
training:	Epoch: [14][101/408]	Loss 0.0248 (0.0870)	
training:	Epoch: [14][102/408]	Loss 0.0314 (0.0864)	
training:	Epoch: [14][103/408]	Loss 0.0879 (0.0864)	
training:	Epoch: [14][104/408]	Loss 0.0193 (0.0858)	
training:	Epoch: [14][105/408]	Loss 0.4685 (0.0894)	
training:	Epoch: [14][106/408]	Loss 0.0624 (0.0892)	
training:	Epoch: [14][107/408]	Loss 0.0188 (0.0885)	
training:	Epoch: [14][108/408]	Loss 0.1037 (0.0887)	
training:	Epoch: [14][109/408]	Loss 0.0675 (0.0885)	
training:	Epoch: [14][110/408]	Loss 0.0194 (0.0878)	
training:	Epoch: [14][111/408]	Loss 0.0332 (0.0873)	
training:	Epoch: [14][112/408]	Loss 0.5384 (0.0914)	
training:	Epoch: [14][113/408]	Loss 0.0412 (0.0909)	
training:	Epoch: [14][114/408]	Loss 0.0233 (0.0903)	
training:	Epoch: [14][115/408]	Loss 0.0240 (0.0898)	
training:	Epoch: [14][116/408]	Loss 0.0210 (0.0892)	
training:	Epoch: [14][117/408]	Loss 0.0217 (0.0886)	
training:	Epoch: [14][118/408]	Loss 0.0217 (0.0880)	
training:	Epoch: [14][119/408]	Loss 0.0184 (0.0874)	
training:	Epoch: [14][120/408]	Loss 0.0386 (0.0870)	
training:	Epoch: [14][121/408]	Loss 0.0240 (0.0865)	
training:	Epoch: [14][122/408]	Loss 0.0218 (0.0860)	
training:	Epoch: [14][123/408]	Loss 0.0208 (0.0854)	
training:	Epoch: [14][124/408]	Loss 0.0195 (0.0849)	
training:	Epoch: [14][125/408]	Loss 0.0289 (0.0845)	
training:	Epoch: [14][126/408]	Loss 0.5196 (0.0879)	
training:	Epoch: [14][127/408]	Loss 0.0188 (0.0874)	
training:	Epoch: [14][128/408]	Loss 0.2923 (0.0890)	
training:	Epoch: [14][129/408]	Loss 0.0378 (0.0886)	
training:	Epoch: [14][130/408]	Loss 0.1993 (0.0894)	
training:	Epoch: [14][131/408]	Loss 0.0236 (0.0889)	
training:	Epoch: [14][132/408]	Loss 0.1665 (0.0895)	
training:	Epoch: [14][133/408]	Loss 0.0236 (0.0890)	
training:	Epoch: [14][134/408]	Loss 0.0223 (0.0885)	
training:	Epoch: [14][135/408]	Loss 0.0290 (0.0881)	
training:	Epoch: [14][136/408]	Loss 0.0308 (0.0877)	
training:	Epoch: [14][137/408]	Loss 0.0384 (0.0873)	
training:	Epoch: [14][138/408]	Loss 0.0366 (0.0869)	
training:	Epoch: [14][139/408]	Loss 0.0591 (0.0867)	
training:	Epoch: [14][140/408]	Loss 0.0238 (0.0863)	
training:	Epoch: [14][141/408]	Loss 0.4901 (0.0891)	
training:	Epoch: [14][142/408]	Loss 0.2886 (0.0906)	
training:	Epoch: [14][143/408]	Loss 0.0331 (0.0901)	
training:	Epoch: [14][144/408]	Loss 0.2785 (0.0915)	
training:	Epoch: [14][145/408]	Loss 0.2792 (0.0928)	
training:	Epoch: [14][146/408]	Loss 0.2916 (0.0941)	
training:	Epoch: [14][147/408]	Loss 0.0362 (0.0937)	
training:	Epoch: [14][148/408]	Loss 0.0220 (0.0932)	
training:	Epoch: [14][149/408]	Loss 0.0241 (0.0928)	
training:	Epoch: [14][150/408]	Loss 0.0268 (0.0923)	
training:	Epoch: [14][151/408]	Loss 0.0214 (0.0919)	
training:	Epoch: [14][152/408]	Loss 0.0264 (0.0914)	
training:	Epoch: [14][153/408]	Loss 0.2871 (0.0927)	
training:	Epoch: [14][154/408]	Loss 0.0184 (0.0922)	
training:	Epoch: [14][155/408]	Loss 0.0198 (0.0918)	
training:	Epoch: [14][156/408]	Loss 0.0240 (0.0913)	
training:	Epoch: [14][157/408]	Loss 0.2722 (0.0925)	
training:	Epoch: [14][158/408]	Loss 0.0236 (0.0920)	
training:	Epoch: [14][159/408]	Loss 0.0628 (0.0919)	
training:	Epoch: [14][160/408]	Loss 0.0211 (0.0914)	
training:	Epoch: [14][161/408]	Loss 0.0196 (0.0910)	
training:	Epoch: [14][162/408]	Loss 0.0199 (0.0905)	
training:	Epoch: [14][163/408]	Loss 0.0439 (0.0902)	
training:	Epoch: [14][164/408]	Loss 0.0443 (0.0900)	
training:	Epoch: [14][165/408]	Loss 0.0200 (0.0895)	
training:	Epoch: [14][166/408]	Loss 0.0221 (0.0891)	
training:	Epoch: [14][167/408]	Loss 0.0214 (0.0887)	
training:	Epoch: [14][168/408]	Loss 0.0259 (0.0884)	
training:	Epoch: [14][169/408]	Loss 0.1651 (0.0888)	
training:	Epoch: [14][170/408]	Loss 0.0202 (0.0884)	
training:	Epoch: [14][171/408]	Loss 0.0220 (0.0880)	
training:	Epoch: [14][172/408]	Loss 0.0205 (0.0876)	
training:	Epoch: [14][173/408]	Loss 0.0179 (0.0872)	
training:	Epoch: [14][174/408]	Loss 0.0186 (0.0868)	
training:	Epoch: [14][175/408]	Loss 0.0301 (0.0865)	
training:	Epoch: [14][176/408]	Loss 0.0343 (0.0862)	
training:	Epoch: [14][177/408]	Loss 0.0857 (0.0862)	
training:	Epoch: [14][178/408]	Loss 0.0227 (0.0858)	
training:	Epoch: [14][179/408]	Loss 0.0207 (0.0855)	
training:	Epoch: [14][180/408]	Loss 0.2823 (0.0866)	
training:	Epoch: [14][181/408]	Loss 0.0218 (0.0862)	
training:	Epoch: [14][182/408]	Loss 0.0198 (0.0859)	
training:	Epoch: [14][183/408]	Loss 0.0223 (0.0855)	
training:	Epoch: [14][184/408]	Loss 0.2360 (0.0863)	
training:	Epoch: [14][185/408]	Loss 0.0207 (0.0860)	
training:	Epoch: [14][186/408]	Loss 0.0185 (0.0856)	
training:	Epoch: [14][187/408]	Loss 0.2589 (0.0865)	
training:	Epoch: [14][188/408]	Loss 0.1377 (0.0868)	
training:	Epoch: [14][189/408]	Loss 0.0192 (0.0864)	
training:	Epoch: [14][190/408]	Loss 0.5171 (0.0887)	
training:	Epoch: [14][191/408]	Loss 0.0488 (0.0885)	
training:	Epoch: [14][192/408]	Loss 0.0278 (0.0882)	
training:	Epoch: [14][193/408]	Loss 0.2529 (0.0890)	
training:	Epoch: [14][194/408]	Loss 0.0197 (0.0887)	
training:	Epoch: [14][195/408]	Loss 0.2679 (0.0896)	
training:	Epoch: [14][196/408]	Loss 0.0196 (0.0892)	
training:	Epoch: [14][197/408]	Loss 0.0187 (0.0889)	
training:	Epoch: [14][198/408]	Loss 0.2542 (0.0897)	
training:	Epoch: [14][199/408]	Loss 0.0306 (0.0894)	
training:	Epoch: [14][200/408]	Loss 0.0735 (0.0893)	
training:	Epoch: [14][201/408]	Loss 0.0197 (0.0890)	
training:	Epoch: [14][202/408]	Loss 0.0202 (0.0887)	
training:	Epoch: [14][203/408]	Loss 0.5208 (0.0908)	
training:	Epoch: [14][204/408]	Loss 0.0218 (0.0904)	
training:	Epoch: [14][205/408]	Loss 0.0245 (0.0901)	
training:	Epoch: [14][206/408]	Loss 0.0238 (0.0898)	
training:	Epoch: [14][207/408]	Loss 0.0235 (0.0895)	
training:	Epoch: [14][208/408]	Loss 0.0299 (0.0892)	
training:	Epoch: [14][209/408]	Loss 0.0406 (0.0890)	
training:	Epoch: [14][210/408]	Loss 0.0554 (0.0888)	
training:	Epoch: [14][211/408]	Loss 0.1420 (0.0891)	
training:	Epoch: [14][212/408]	Loss 0.0162 (0.0887)	
training:	Epoch: [14][213/408]	Loss 0.1069 (0.0888)	
training:	Epoch: [14][214/408]	Loss 0.0201 (0.0885)	
training:	Epoch: [14][215/408]	Loss 0.0529 (0.0883)	
training:	Epoch: [14][216/408]	Loss 0.0317 (0.0881)	
training:	Epoch: [14][217/408]	Loss 0.0239 (0.0878)	
training:	Epoch: [14][218/408]	Loss 0.2579 (0.0885)	
training:	Epoch: [14][219/408]	Loss 0.0693 (0.0884)	
training:	Epoch: [14][220/408]	Loss 0.0515 (0.0883)	
training:	Epoch: [14][221/408]	Loss 0.2752 (0.0891)	
training:	Epoch: [14][222/408]	Loss 0.0200 (0.0888)	
training:	Epoch: [14][223/408]	Loss 0.0212 (0.0885)	
training:	Epoch: [14][224/408]	Loss 0.0201 (0.0882)	
training:	Epoch: [14][225/408]	Loss 0.0189 (0.0879)	
training:	Epoch: [14][226/408]	Loss 0.0185 (0.0876)	
training:	Epoch: [14][227/408]	Loss 0.0213 (0.0873)	
training:	Epoch: [14][228/408]	Loss 0.0227 (0.0870)	
training:	Epoch: [14][229/408]	Loss 0.0737 (0.0870)	
training:	Epoch: [14][230/408]	Loss 0.2497 (0.0877)	
training:	Epoch: [14][231/408]	Loss 0.0183 (0.0874)	
training:	Epoch: [14][232/408]	Loss 0.0190 (0.0871)	
training:	Epoch: [14][233/408]	Loss 0.0922 (0.0871)	
training:	Epoch: [14][234/408]	Loss 0.1361 (0.0873)	
training:	Epoch: [14][235/408]	Loss 0.2509 (0.0880)	
training:	Epoch: [14][236/408]	Loss 0.0184 (0.0877)	
training:	Epoch: [14][237/408]	Loss 0.0582 (0.0876)	
training:	Epoch: [14][238/408]	Loss 0.0247 (0.0873)	
training:	Epoch: [14][239/408]	Loss 0.0282 (0.0871)	
training:	Epoch: [14][240/408]	Loss 0.0405 (0.0869)	
training:	Epoch: [14][241/408]	Loss 0.0269 (0.0866)	
training:	Epoch: [14][242/408]	Loss 0.0183 (0.0863)	
training:	Epoch: [14][243/408]	Loss 0.0185 (0.0861)	
training:	Epoch: [14][244/408]	Loss 0.3381 (0.0871)	
training:	Epoch: [14][245/408]	Loss 0.0246 (0.0868)	
training:	Epoch: [14][246/408]	Loss 0.0175 (0.0866)	
training:	Epoch: [14][247/408]	Loss 0.0399 (0.0864)	
training:	Epoch: [14][248/408]	Loss 0.0343 (0.0862)	
training:	Epoch: [14][249/408]	Loss 0.0209 (0.0859)	
training:	Epoch: [14][250/408]	Loss 0.0335 (0.0857)	
training:	Epoch: [14][251/408]	Loss 0.0212 (0.0854)	
training:	Epoch: [14][252/408]	Loss 0.0265 (0.0852)	
training:	Epoch: [14][253/408]	Loss 0.2469 (0.0858)	
training:	Epoch: [14][254/408]	Loss 0.0245 (0.0856)	
training:	Epoch: [14][255/408]	Loss 0.0499 (0.0855)	
training:	Epoch: [14][256/408]	Loss 0.0188 (0.0852)	
training:	Epoch: [14][257/408]	Loss 0.0180 (0.0849)	
training:	Epoch: [14][258/408]	Loss 0.0239 (0.0847)	
training:	Epoch: [14][259/408]	Loss 0.0475 (0.0846)	
training:	Epoch: [14][260/408]	Loss 0.2552 (0.0852)	
training:	Epoch: [14][261/408]	Loss 0.0232 (0.0850)	
training:	Epoch: [14][262/408]	Loss 0.0351 (0.0848)	
training:	Epoch: [14][263/408]	Loss 0.0393 (0.0846)	
training:	Epoch: [14][264/408]	Loss 0.0198 (0.0844)	
training:	Epoch: [14][265/408]	Loss 0.0396 (0.0842)	
training:	Epoch: [14][266/408]	Loss 0.0215 (0.0840)	
training:	Epoch: [14][267/408]	Loss 0.0194 (0.0837)	
training:	Epoch: [14][268/408]	Loss 0.2418 (0.0843)	
training:	Epoch: [14][269/408]	Loss 0.2616 (0.0850)	
training:	Epoch: [14][270/408]	Loss 0.0281 (0.0848)	
training:	Epoch: [14][271/408]	Loss 0.0202 (0.0845)	
training:	Epoch: [14][272/408]	Loss 0.1114 (0.0846)	
training:	Epoch: [14][273/408]	Loss 0.0206 (0.0844)	
training:	Epoch: [14][274/408]	Loss 0.0183 (0.0841)	
training:	Epoch: [14][275/408]	Loss 0.0224 (0.0839)	
training:	Epoch: [14][276/408]	Loss 0.0180 (0.0837)	
training:	Epoch: [14][277/408]	Loss 0.0293 (0.0835)	
training:	Epoch: [14][278/408]	Loss 0.0263 (0.0833)	
training:	Epoch: [14][279/408]	Loss 0.0184 (0.0830)	
training:	Epoch: [14][280/408]	Loss 0.0300 (0.0829)	
training:	Epoch: [14][281/408]	Loss 0.0188 (0.0826)	
training:	Epoch: [14][282/408]	Loss 0.2437 (0.0832)	
training:	Epoch: [14][283/408]	Loss 0.0203 (0.0830)	
training:	Epoch: [14][284/408]	Loss 0.0219 (0.0828)	
training:	Epoch: [14][285/408]	Loss 0.0230 (0.0825)	
training:	Epoch: [14][286/408]	Loss 0.0212 (0.0823)	
training:	Epoch: [14][287/408]	Loss 0.0199 (0.0821)	
training:	Epoch: [14][288/408]	Loss 0.0707 (0.0821)	
training:	Epoch: [14][289/408]	Loss 0.0596 (0.0820)	
training:	Epoch: [14][290/408]	Loss 0.0198 (0.0818)	
training:	Epoch: [14][291/408]	Loss 0.0188 (0.0816)	
training:	Epoch: [14][292/408]	Loss 0.0174 (0.0813)	
training:	Epoch: [14][293/408]	Loss 0.2785 (0.0820)	
training:	Epoch: [14][294/408]	Loss 0.0198 (0.0818)	
training:	Epoch: [14][295/408]	Loss 0.0923 (0.0818)	
training:	Epoch: [14][296/408]	Loss 0.0298 (0.0817)	
training:	Epoch: [14][297/408]	Loss 0.0971 (0.0817)	
training:	Epoch: [14][298/408]	Loss 0.0222 (0.0815)	
training:	Epoch: [14][299/408]	Loss 0.2300 (0.0820)	
training:	Epoch: [14][300/408]	Loss 0.0186 (0.0818)	
training:	Epoch: [14][301/408]	Loss 0.0686 (0.0818)	
training:	Epoch: [14][302/408]	Loss 0.0240 (0.0816)	
training:	Epoch: [14][303/408]	Loss 0.2931 (0.0823)	
training:	Epoch: [14][304/408]	Loss 0.0181 (0.0821)	
training:	Epoch: [14][305/408]	Loss 0.0714 (0.0820)	
training:	Epoch: [14][306/408]	Loss 0.2829 (0.0827)	
training:	Epoch: [14][307/408]	Loss 0.0188 (0.0825)	
training:	Epoch: [14][308/408]	Loss 0.0213 (0.0823)	
training:	Epoch: [14][309/408]	Loss 0.0212 (0.0821)	
training:	Epoch: [14][310/408]	Loss 0.2873 (0.0827)	
training:	Epoch: [14][311/408]	Loss 0.0356 (0.0826)	
training:	Epoch: [14][312/408]	Loss 0.0448 (0.0825)	
training:	Epoch: [14][313/408]	Loss 0.0223 (0.0823)	
training:	Epoch: [14][314/408]	Loss 0.0177 (0.0821)	
training:	Epoch: [14][315/408]	Loss 0.0181 (0.0819)	
training:	Epoch: [14][316/408]	Loss 0.2948 (0.0825)	
training:	Epoch: [14][317/408]	Loss 0.0176 (0.0823)	
training:	Epoch: [14][318/408]	Loss 0.0386 (0.0822)	
training:	Epoch: [14][319/408]	Loss 0.2673 (0.0828)	
training:	Epoch: [14][320/408]	Loss 0.4862 (0.0840)	
training:	Epoch: [14][321/408]	Loss 0.0210 (0.0838)	
training:	Epoch: [14][322/408]	Loss 0.0258 (0.0837)	
training:	Epoch: [14][323/408]	Loss 0.0729 (0.0836)	
training:	Epoch: [14][324/408]	Loss 0.0187 (0.0834)	
training:	Epoch: [14][325/408]	Loss 0.0201 (0.0832)	
training:	Epoch: [14][326/408]	Loss 0.3067 (0.0839)	
training:	Epoch: [14][327/408]	Loss 0.0277 (0.0837)	
training:	Epoch: [14][328/408]	Loss 0.0211 (0.0836)	
training:	Epoch: [14][329/408]	Loss 0.1776 (0.0838)	
training:	Epoch: [14][330/408]	Loss 0.0314 (0.0837)	
training:	Epoch: [14][331/408]	Loss 0.0195 (0.0835)	
training:	Epoch: [14][332/408]	Loss 0.0180 (0.0833)	
training:	Epoch: [14][333/408]	Loss 0.0474 (0.0832)	
training:	Epoch: [14][334/408]	Loss 0.0178 (0.0830)	
training:	Epoch: [14][335/408]	Loss 0.0183 (0.0828)	
training:	Epoch: [14][336/408]	Loss 0.0279 (0.0826)	
training:	Epoch: [14][337/408]	Loss 0.0225 (0.0825)	
training:	Epoch: [14][338/408]	Loss 0.0201 (0.0823)	
training:	Epoch: [14][339/408]	Loss 0.0207 (0.0821)	
training:	Epoch: [14][340/408]	Loss 0.0183 (0.0819)	
training:	Epoch: [14][341/408]	Loss 0.2974 (0.0825)	
training:	Epoch: [14][342/408]	Loss 0.4940 (0.0837)	
training:	Epoch: [14][343/408]	Loss 0.0209 (0.0835)	
training:	Epoch: [14][344/408]	Loss 0.2740 (0.0841)	
training:	Epoch: [14][345/408]	Loss 0.0350 (0.0840)	
training:	Epoch: [14][346/408]	Loss 0.0203 (0.0838)	
training:	Epoch: [14][347/408]	Loss 0.2808 (0.0843)	
training:	Epoch: [14][348/408]	Loss 0.0171 (0.0842)	
training:	Epoch: [14][349/408]	Loss 0.0610 (0.0841)	
training:	Epoch: [14][350/408]	Loss 0.1319 (0.0842)	
training:	Epoch: [14][351/408]	Loss 0.0191 (0.0840)	
training:	Epoch: [14][352/408]	Loss 0.0178 (0.0838)	
training:	Epoch: [14][353/408]	Loss 0.0198 (0.0837)	
training:	Epoch: [14][354/408]	Loss 0.2590 (0.0842)	
training:	Epoch: [14][355/408]	Loss 0.0248 (0.0840)	
training:	Epoch: [14][356/408]	Loss 0.0931 (0.0840)	
training:	Epoch: [14][357/408]	Loss 0.0204 (0.0838)	
training:	Epoch: [14][358/408]	Loss 0.0865 (0.0838)	
training:	Epoch: [14][359/408]	Loss 0.0167 (0.0837)	
training:	Epoch: [14][360/408]	Loss 0.0200 (0.0835)	
training:	Epoch: [14][361/408]	Loss 0.2009 (0.0838)	
training:	Epoch: [14][362/408]	Loss 0.2656 (0.0843)	
training:	Epoch: [14][363/408]	Loss 0.2221 (0.0847)	
training:	Epoch: [14][364/408]	Loss 0.0183 (0.0845)	
training:	Epoch: [14][365/408]	Loss 0.2422 (0.0849)	
training:	Epoch: [14][366/408]	Loss 0.0556 (0.0849)	
training:	Epoch: [14][367/408]	Loss 0.0303 (0.0847)	
training:	Epoch: [14][368/408]	Loss 0.0386 (0.0846)	
training:	Epoch: [14][369/408]	Loss 0.0195 (0.0844)	
training:	Epoch: [14][370/408]	Loss 0.0245 (0.0842)	
training:	Epoch: [14][371/408]	Loss 0.0199 (0.0841)	
training:	Epoch: [14][372/408]	Loss 0.0181 (0.0839)	
training:	Epoch: [14][373/408]	Loss 0.2675 (0.0844)	
training:	Epoch: [14][374/408]	Loss 0.0186 (0.0842)	
training:	Epoch: [14][375/408]	Loss 0.2701 (0.0847)	
training:	Epoch: [14][376/408]	Loss 0.0213 (0.0845)	
training:	Epoch: [14][377/408]	Loss 0.0303 (0.0844)	
training:	Epoch: [14][378/408]	Loss 0.0192 (0.0842)	
training:	Epoch: [14][379/408]	Loss 0.0205 (0.0841)	
training:	Epoch: [14][380/408]	Loss 0.2707 (0.0845)	
training:	Epoch: [14][381/408]	Loss 0.2853 (0.0851)	
training:	Epoch: [14][382/408]	Loss 0.0244 (0.0849)	
training:	Epoch: [14][383/408]	Loss 0.0748 (0.0849)	
training:	Epoch: [14][384/408]	Loss 0.0239 (0.0847)	
training:	Epoch: [14][385/408]	Loss 0.0168 (0.0846)	
training:	Epoch: [14][386/408]	Loss 0.0247 (0.0844)	
training:	Epoch: [14][387/408]	Loss 0.0222 (0.0842)	
training:	Epoch: [14][388/408]	Loss 0.0240 (0.0841)	
training:	Epoch: [14][389/408]	Loss 0.0182 (0.0839)	
training:	Epoch: [14][390/408]	Loss 0.0174 (0.0837)	
training:	Epoch: [14][391/408]	Loss 0.0195 (0.0836)	
training:	Epoch: [14][392/408]	Loss 0.0225 (0.0834)	
training:	Epoch: [14][393/408]	Loss 0.0178 (0.0833)	
training:	Epoch: [14][394/408]	Loss 0.5136 (0.0843)	
training:	Epoch: [14][395/408]	Loss 0.0188 (0.0842)	
training:	Epoch: [14][396/408]	Loss 0.0207 (0.0840)	
training:	Epoch: [14][397/408]	Loss 0.7316 (0.0857)	
training:	Epoch: [14][398/408]	Loss 0.2739 (0.0861)	
training:	Epoch: [14][399/408]	Loss 0.0204 (0.0860)	
training:	Epoch: [14][400/408]	Loss 0.2866 (0.0865)	
training:	Epoch: [14][401/408]	Loss 0.2894 (0.0870)	
training:	Epoch: [14][402/408]	Loss 0.0680 (0.0869)	
training:	Epoch: [14][403/408]	Loss 0.0210 (0.0868)	
training:	Epoch: [14][404/408]	Loss 0.0196 (0.0866)	
training:	Epoch: [14][405/408]	Loss 0.0197 (0.0864)	
training:	Epoch: [14][406/408]	Loss 0.0215 (0.0863)	
training:	Epoch: [14][407/408]	Loss 0.0201 (0.0861)	
training:	Epoch: [14][408/408]	Loss 0.0199 (0.0859)	
Training:	 Loss: 0.0858

Training:	 ACC: 0.9847 0.9847 0.9838 0.9857
Validation:	 ACC: 0.7893 0.7919 0.8465 0.7321
Validation:	 Best_BACC: 0.8226 0.8250 0.8772 0.7679
Validation:	 Loss: 0.7285
Pretraining:	Epoch 15/200
----------
training:	Epoch: [15][1/408]	Loss 0.1091 (0.1091)	
training:	Epoch: [15][2/408]	Loss 0.0219 (0.0655)	
training:	Epoch: [15][3/408]	Loss 0.0232 (0.0514)	
training:	Epoch: [15][4/408]	Loss 0.0172 (0.0428)	
training:	Epoch: [15][5/408]	Loss 0.0440 (0.0431)	
training:	Epoch: [15][6/408]	Loss 0.0238 (0.0399)	
training:	Epoch: [15][7/408]	Loss 0.0210 (0.0372)	
training:	Epoch: [15][8/408]	Loss 0.2764 (0.0671)	
training:	Epoch: [15][9/408]	Loss 0.0341 (0.0634)	
training:	Epoch: [15][10/408]	Loss 0.0179 (0.0589)	
training:	Epoch: [15][11/408]	Loss 0.0232 (0.0556)	
training:	Epoch: [15][12/408]	Loss 0.0183 (0.0525)	
training:	Epoch: [15][13/408]	Loss 0.0244 (0.0503)	
training:	Epoch: [15][14/408]	Loss 0.0183 (0.0480)	
training:	Epoch: [15][15/408]	Loss 0.0227 (0.0464)	
training:	Epoch: [15][16/408]	Loss 0.0198 (0.0447)	
training:	Epoch: [15][17/408]	Loss 0.0254 (0.0436)	
training:	Epoch: [15][18/408]	Loss 0.0187 (0.0422)	
training:	Epoch: [15][19/408]	Loss 0.0365 (0.0419)	
training:	Epoch: [15][20/408]	Loss 0.0314 (0.0414)	
training:	Epoch: [15][21/408]	Loss 0.0179 (0.0402)	
training:	Epoch: [15][22/408]	Loss 0.0177 (0.0392)	
training:	Epoch: [15][23/408]	Loss 0.3800 (0.0540)	
training:	Epoch: [15][24/408]	Loss 0.0171 (0.0525)	
training:	Epoch: [15][25/408]	Loss 0.0181 (0.0511)	
training:	Epoch: [15][26/408]	Loss 0.2606 (0.0592)	
training:	Epoch: [15][27/408]	Loss 0.4878 (0.0751)	
training:	Epoch: [15][28/408]	Loss 0.0189 (0.0731)	
training:	Epoch: [15][29/408]	Loss 0.0215 (0.0713)	
training:	Epoch: [15][30/408]	Loss 0.0181 (0.0695)	
training:	Epoch: [15][31/408]	Loss 0.2877 (0.0765)	
training:	Epoch: [15][32/408]	Loss 0.0185 (0.0747)	
training:	Epoch: [15][33/408]	Loss 0.2554 (0.0802)	
training:	Epoch: [15][34/408]	Loss 0.0173 (0.0784)	
training:	Epoch: [15][35/408]	Loss 0.0167 (0.0766)	
training:	Epoch: [15][36/408]	Loss 0.0696 (0.0764)	
training:	Epoch: [15][37/408]	Loss 0.0262 (0.0750)	
training:	Epoch: [15][38/408]	Loss 0.0285 (0.0738)	
training:	Epoch: [15][39/408]	Loss 0.0222 (0.0725)	
training:	Epoch: [15][40/408]	Loss 0.2536 (0.0770)	
training:	Epoch: [15][41/408]	Loss 0.1723 (0.0793)	
training:	Epoch: [15][42/408]	Loss 0.0292 (0.0781)	
training:	Epoch: [15][43/408]	Loss 0.0171 (0.0767)	
training:	Epoch: [15][44/408]	Loss 0.0595 (0.0763)	
training:	Epoch: [15][45/408]	Loss 0.0194 (0.0751)	
training:	Epoch: [15][46/408]	Loss 0.0180 (0.0738)	
training:	Epoch: [15][47/408]	Loss 0.0263 (0.0728)	
training:	Epoch: [15][48/408]	Loss 0.0338 (0.0720)	
training:	Epoch: [15][49/408]	Loss 0.2936 (0.0765)	
training:	Epoch: [15][50/408]	Loss 0.0223 (0.0754)	
training:	Epoch: [15][51/408]	Loss 0.0175 (0.0743)	
training:	Epoch: [15][52/408]	Loss 0.0179 (0.0732)	
training:	Epoch: [15][53/408]	Loss 0.2535 (0.0766)	
training:	Epoch: [15][54/408]	Loss 0.0206 (0.0756)	
training:	Epoch: [15][55/408]	Loss 0.2812 (0.0793)	
training:	Epoch: [15][56/408]	Loss 0.0189 (0.0782)	
training:	Epoch: [15][57/408]	Loss 0.0181 (0.0772)	
training:	Epoch: [15][58/408]	Loss 0.0217 (0.0762)	
training:	Epoch: [15][59/408]	Loss 0.0176 (0.0752)	
training:	Epoch: [15][60/408]	Loss 0.0202 (0.0743)	
training:	Epoch: [15][61/408]	Loss 0.0698 (0.0742)	
training:	Epoch: [15][62/408]	Loss 0.0175 (0.0733)	
training:	Epoch: [15][63/408]	Loss 0.0189 (0.0725)	
training:	Epoch: [15][64/408]	Loss 0.0252 (0.0717)	
training:	Epoch: [15][65/408]	Loss 0.2175 (0.0740)	
training:	Epoch: [15][66/408]	Loss 0.0241 (0.0732)	
training:	Epoch: [15][67/408]	Loss 0.0190 (0.0724)	
training:	Epoch: [15][68/408]	Loss 0.0213 (0.0717)	
training:	Epoch: [15][69/408]	Loss 0.2818 (0.0747)	
training:	Epoch: [15][70/408]	Loss 0.0251 (0.0740)	
training:	Epoch: [15][71/408]	Loss 0.0206 (0.0732)	
training:	Epoch: [15][72/408]	Loss 0.2457 (0.0756)	
training:	Epoch: [15][73/408]	Loss 0.0169 (0.0748)	
training:	Epoch: [15][74/408]	Loss 0.0229 (0.0741)	
training:	Epoch: [15][75/408]	Loss 0.2622 (0.0766)	
training:	Epoch: [15][76/408]	Loss 0.2538 (0.0790)	
training:	Epoch: [15][77/408]	Loss 0.2591 (0.0813)	
training:	Epoch: [15][78/408]	Loss 0.2566 (0.0836)	
training:	Epoch: [15][79/408]	Loss 0.0341 (0.0829)	
training:	Epoch: [15][80/408]	Loss 0.0208 (0.0822)	
training:	Epoch: [15][81/408]	Loss 0.0692 (0.0820)	
training:	Epoch: [15][82/408]	Loss 0.1175 (0.0824)	
training:	Epoch: [15][83/408]	Loss 0.0166 (0.0816)	
training:	Epoch: [15][84/408]	Loss 0.0200 (0.0809)	
training:	Epoch: [15][85/408]	Loss 0.2638 (0.0831)	
training:	Epoch: [15][86/408]	Loss 0.2805 (0.0854)	
training:	Epoch: [15][87/408]	Loss 0.1724 (0.0864)	
training:	Epoch: [15][88/408]	Loss 0.0204 (0.0856)	
training:	Epoch: [15][89/408]	Loss 0.1507 (0.0863)	
training:	Epoch: [15][90/408]	Loss 0.0462 (0.0859)	
training:	Epoch: [15][91/408]	Loss 0.0245 (0.0852)	
training:	Epoch: [15][92/408]	Loss 0.0186 (0.0845)	
training:	Epoch: [15][93/408]	Loss 0.0158 (0.0837)	
training:	Epoch: [15][94/408]	Loss 0.0238 (0.0831)	
training:	Epoch: [15][95/408]	Loss 0.2907 (0.0853)	
training:	Epoch: [15][96/408]	Loss 0.4332 (0.0889)	
training:	Epoch: [15][97/408]	Loss 0.0191 (0.0882)	
training:	Epoch: [15][98/408]	Loss 0.0183 (0.0875)	
training:	Epoch: [15][99/408]	Loss 0.0482 (0.0871)	
training:	Epoch: [15][100/408]	Loss 0.0169 (0.0864)	
training:	Epoch: [15][101/408]	Loss 0.0215 (0.0857)	
training:	Epoch: [15][102/408]	Loss 0.2717 (0.0876)	
training:	Epoch: [15][103/408]	Loss 0.0179 (0.0869)	
training:	Epoch: [15][104/408]	Loss 0.0344 (0.0864)	
training:	Epoch: [15][105/408]	Loss 0.0231 (0.0858)	
training:	Epoch: [15][106/408]	Loss 0.0196 (0.0852)	
training:	Epoch: [15][107/408]	Loss 0.0217 (0.0846)	
training:	Epoch: [15][108/408]	Loss 0.0221 (0.0840)	
training:	Epoch: [15][109/408]	Loss 0.0202 (0.0834)	
training:	Epoch: [15][110/408]	Loss 0.0213 (0.0828)	
training:	Epoch: [15][111/408]	Loss 0.2257 (0.0841)	
training:	Epoch: [15][112/408]	Loss 0.0706 (0.0840)	
training:	Epoch: [15][113/408]	Loss 0.0254 (0.0835)	
training:	Epoch: [15][114/408]	Loss 0.0202 (0.0829)	
training:	Epoch: [15][115/408]	Loss 0.0229 (0.0824)	
training:	Epoch: [15][116/408]	Loss 0.0398 (0.0820)	
training:	Epoch: [15][117/408]	Loss 0.0168 (0.0815)	
training:	Epoch: [15][118/408]	Loss 0.0191 (0.0810)	
training:	Epoch: [15][119/408]	Loss 0.2387 (0.0823)	
training:	Epoch: [15][120/408]	Loss 0.2954 (0.0841)	
training:	Epoch: [15][121/408]	Loss 0.0234 (0.0836)	
training:	Epoch: [15][122/408]	Loss 0.3884 (0.0861)	
training:	Epoch: [15][123/408]	Loss 0.0217 (0.0855)	
training:	Epoch: [15][124/408]	Loss 0.0233 (0.0850)	
training:	Epoch: [15][125/408]	Loss 0.0233 (0.0845)	
training:	Epoch: [15][126/408]	Loss 0.0201 (0.0840)	
training:	Epoch: [15][127/408]	Loss 0.0225 (0.0835)	
training:	Epoch: [15][128/408]	Loss 0.0213 (0.0831)	
training:	Epoch: [15][129/408]	Loss 0.0243 (0.0826)	
training:	Epoch: [15][130/408]	Loss 0.0858 (0.0826)	
training:	Epoch: [15][131/408]	Loss 0.0268 (0.0822)	
training:	Epoch: [15][132/408]	Loss 0.0241 (0.0818)	
training:	Epoch: [15][133/408]	Loss 0.0179 (0.0813)	
training:	Epoch: [15][134/408]	Loss 0.0193 (0.0808)	
training:	Epoch: [15][135/408]	Loss 0.0197 (0.0804)	
training:	Epoch: [15][136/408]	Loss 0.0210 (0.0799)	
training:	Epoch: [15][137/408]	Loss 0.0201 (0.0795)	
training:	Epoch: [15][138/408]	Loss 0.0169 (0.0790)	
training:	Epoch: [15][139/408]	Loss 0.0209 (0.0786)	
training:	Epoch: [15][140/408]	Loss 0.0242 (0.0782)	
training:	Epoch: [15][141/408]	Loss 0.0224 (0.0778)	
training:	Epoch: [15][142/408]	Loss 0.0190 (0.0774)	
training:	Epoch: [15][143/408]	Loss 0.0381 (0.0771)	
training:	Epoch: [15][144/408]	Loss 0.0261 (0.0768)	
training:	Epoch: [15][145/408]	Loss 0.2228 (0.0778)	
training:	Epoch: [15][146/408]	Loss 0.0220 (0.0774)	
training:	Epoch: [15][147/408]	Loss 0.0194 (0.0770)	
training:	Epoch: [15][148/408]	Loss 0.0800 (0.0770)	
training:	Epoch: [15][149/408]	Loss 0.0358 (0.0768)	
training:	Epoch: [15][150/408]	Loss 0.2855 (0.0782)	
training:	Epoch: [15][151/408]	Loss 0.0284 (0.0778)	
training:	Epoch: [15][152/408]	Loss 0.0250 (0.0775)	
training:	Epoch: [15][153/408]	Loss 0.0173 (0.0771)	
training:	Epoch: [15][154/408]	Loss 0.0166 (0.0767)	
training:	Epoch: [15][155/408]	Loss 0.0178 (0.0763)	
training:	Epoch: [15][156/408]	Loss 0.0196 (0.0759)	
training:	Epoch: [15][157/408]	Loss 0.2538 (0.0771)	
training:	Epoch: [15][158/408]	Loss 0.0188 (0.0767)	
training:	Epoch: [15][159/408]	Loss 0.0223 (0.0764)	
training:	Epoch: [15][160/408]	Loss 0.1409 (0.0768)	
training:	Epoch: [15][161/408]	Loss 0.0184 (0.0764)	
training:	Epoch: [15][162/408]	Loss 0.0166 (0.0760)	
training:	Epoch: [15][163/408]	Loss 0.0164 (0.0757)	
training:	Epoch: [15][164/408]	Loss 0.0177 (0.0753)	
training:	Epoch: [15][165/408]	Loss 0.2926 (0.0766)	
training:	Epoch: [15][166/408]	Loss 0.1157 (0.0769)	
training:	Epoch: [15][167/408]	Loss 0.0220 (0.0765)	
training:	Epoch: [15][168/408]	Loss 0.0181 (0.0762)	
training:	Epoch: [15][169/408]	Loss 0.0210 (0.0759)	
training:	Epoch: [15][170/408]	Loss 0.0205 (0.0755)	
training:	Epoch: [15][171/408]	Loss 0.0176 (0.0752)	
training:	Epoch: [15][172/408]	Loss 0.0219 (0.0749)	
training:	Epoch: [15][173/408]	Loss 0.0284 (0.0746)	
training:	Epoch: [15][174/408]	Loss 0.0205 (0.0743)	
training:	Epoch: [15][175/408]	Loss 0.0191 (0.0740)	
training:	Epoch: [15][176/408]	Loss 0.2657 (0.0751)	
training:	Epoch: [15][177/408]	Loss 0.0209 (0.0748)	
training:	Epoch: [15][178/408]	Loss 0.0181 (0.0745)	
training:	Epoch: [15][179/408]	Loss 0.0420 (0.0743)	
training:	Epoch: [15][180/408]	Loss 0.0189 (0.0740)	
training:	Epoch: [15][181/408]	Loss 0.0168 (0.0737)	
training:	Epoch: [15][182/408]	Loss 0.0222 (0.0734)	
training:	Epoch: [15][183/408]	Loss 0.0153 (0.0731)	
training:	Epoch: [15][184/408]	Loss 0.0192 (0.0728)	
training:	Epoch: [15][185/408]	Loss 0.0197 (0.0725)	
training:	Epoch: [15][186/408]	Loss 0.0193 (0.0722)	
training:	Epoch: [15][187/408]	Loss 0.0196 (0.0719)	
training:	Epoch: [15][188/408]	Loss 0.0199 (0.0716)	
training:	Epoch: [15][189/408]	Loss 0.0170 (0.0713)	
training:	Epoch: [15][190/408]	Loss 0.0174 (0.0711)	
training:	Epoch: [15][191/408]	Loss 0.0169 (0.0708)	
training:	Epoch: [15][192/408]	Loss 0.0204 (0.0705)	
training:	Epoch: [15][193/408]	Loss 0.0171 (0.0702)	
training:	Epoch: [15][194/408]	Loss 0.2815 (0.0713)	
training:	Epoch: [15][195/408]	Loss 0.0173 (0.0711)	
training:	Epoch: [15][196/408]	Loss 0.0154 (0.0708)	
training:	Epoch: [15][197/408]	Loss 0.0291 (0.0706)	
training:	Epoch: [15][198/408]	Loss 0.0204 (0.0703)	
training:	Epoch: [15][199/408]	Loss 0.0179 (0.0700)	
training:	Epoch: [15][200/408]	Loss 0.0735 (0.0701)	
training:	Epoch: [15][201/408]	Loss 0.0795 (0.0701)	
training:	Epoch: [15][202/408]	Loss 0.0246 (0.0699)	
training:	Epoch: [15][203/408]	Loss 0.0236 (0.0697)	
training:	Epoch: [15][204/408]	Loss 0.0210 (0.0694)	
training:	Epoch: [15][205/408]	Loss 0.0201 (0.0692)	
training:	Epoch: [15][206/408]	Loss 0.2255 (0.0699)	
training:	Epoch: [15][207/408]	Loss 0.0180 (0.0697)	
training:	Epoch: [15][208/408]	Loss 0.0240 (0.0695)	
training:	Epoch: [15][209/408]	Loss 0.0516 (0.0694)	
training:	Epoch: [15][210/408]	Loss 0.0260 (0.0692)	
training:	Epoch: [15][211/408]	Loss 0.2861 (0.0702)	
training:	Epoch: [15][212/408]	Loss 0.2954 (0.0713)	
training:	Epoch: [15][213/408]	Loss 0.0274 (0.0711)	
training:	Epoch: [15][214/408]	Loss 0.0192 (0.0708)	
training:	Epoch: [15][215/408]	Loss 0.0189 (0.0706)	
training:	Epoch: [15][216/408]	Loss 0.0420 (0.0704)	
training:	Epoch: [15][217/408]	Loss 0.2612 (0.0713)	
training:	Epoch: [15][218/408]	Loss 0.0204 (0.0711)	
training:	Epoch: [15][219/408]	Loss 0.2712 (0.0720)	
training:	Epoch: [15][220/408]	Loss 0.2926 (0.0730)	
training:	Epoch: [15][221/408]	Loss 0.0198 (0.0728)	
training:	Epoch: [15][222/408]	Loss 0.2928 (0.0737)	
training:	Epoch: [15][223/408]	Loss 0.2755 (0.0747)	
training:	Epoch: [15][224/408]	Loss 0.0253 (0.0744)	
training:	Epoch: [15][225/408]	Loss 0.0161 (0.0742)	
training:	Epoch: [15][226/408]	Loss 0.0181 (0.0739)	
training:	Epoch: [15][227/408]	Loss 0.5154 (0.0759)	
training:	Epoch: [15][228/408]	Loss 0.0308 (0.0757)	
training:	Epoch: [15][229/408]	Loss 0.0305 (0.0755)	
training:	Epoch: [15][230/408]	Loss 0.0244 (0.0753)	
training:	Epoch: [15][231/408]	Loss 0.0169 (0.0750)	
training:	Epoch: [15][232/408]	Loss 0.2668 (0.0758)	
training:	Epoch: [15][233/408]	Loss 0.2912 (0.0768)	
training:	Epoch: [15][234/408]	Loss 0.0166 (0.0765)	
training:	Epoch: [15][235/408]	Loss 0.0176 (0.0762)	
training:	Epoch: [15][236/408]	Loss 0.0257 (0.0760)	
training:	Epoch: [15][237/408]	Loss 0.0656 (0.0760)	
training:	Epoch: [15][238/408]	Loss 0.0222 (0.0758)	
training:	Epoch: [15][239/408]	Loss 0.0990 (0.0759)	
training:	Epoch: [15][240/408]	Loss 0.1711 (0.0763)	
training:	Epoch: [15][241/408]	Loss 0.0163 (0.0760)	
training:	Epoch: [15][242/408]	Loss 0.0252 (0.0758)	
training:	Epoch: [15][243/408]	Loss 0.0243 (0.0756)	
training:	Epoch: [15][244/408]	Loss 0.0194 (0.0754)	
training:	Epoch: [15][245/408]	Loss 0.0719 (0.0753)	
training:	Epoch: [15][246/408]	Loss 0.0175 (0.0751)	
training:	Epoch: [15][247/408]	Loss 0.1236 (0.0753)	
training:	Epoch: [15][248/408]	Loss 0.0156 (0.0751)	
training:	Epoch: [15][249/408]	Loss 0.0234 (0.0749)	
training:	Epoch: [15][250/408]	Loss 0.2883 (0.0757)	
training:	Epoch: [15][251/408]	Loss 0.0163 (0.0755)	
training:	Epoch: [15][252/408]	Loss 0.0350 (0.0753)	
training:	Epoch: [15][253/408]	Loss 0.0175 (0.0751)	
training:	Epoch: [15][254/408]	Loss 0.0515 (0.0750)	
training:	Epoch: [15][255/408]	Loss 0.0167 (0.0748)	
training:	Epoch: [15][256/408]	Loss 0.0243 (0.0746)	
training:	Epoch: [15][257/408]	Loss 0.0218 (0.0744)	
training:	Epoch: [15][258/408]	Loss 0.2617 (0.0751)	
training:	Epoch: [15][259/408]	Loss 0.0214 (0.0749)	
training:	Epoch: [15][260/408]	Loss 0.0169 (0.0747)	
training:	Epoch: [15][261/408]	Loss 0.0256 (0.0745)	
training:	Epoch: [15][262/408]	Loss 0.0153 (0.0742)	
training:	Epoch: [15][263/408]	Loss 0.0170 (0.0740)	
training:	Epoch: [15][264/408]	Loss 0.0615 (0.0740)	
training:	Epoch: [15][265/408]	Loss 0.0169 (0.0738)	
training:	Epoch: [15][266/408]	Loss 0.0182 (0.0735)	
training:	Epoch: [15][267/408]	Loss 0.0163 (0.0733)	
training:	Epoch: [15][268/408]	Loss 0.2637 (0.0740)	
training:	Epoch: [15][269/408]	Loss 0.0169 (0.0738)	
training:	Epoch: [15][270/408]	Loss 0.0226 (0.0736)	
training:	Epoch: [15][271/408]	Loss 0.0350 (0.0735)	
training:	Epoch: [15][272/408]	Loss 0.2582 (0.0742)	
training:	Epoch: [15][273/408]	Loss 0.0158 (0.0740)	
training:	Epoch: [15][274/408]	Loss 0.0203 (0.0738)	
training:	Epoch: [15][275/408]	Loss 0.0163 (0.0736)	
training:	Epoch: [15][276/408]	Loss 0.2752 (0.0743)	
training:	Epoch: [15][277/408]	Loss 0.0173 (0.0741)	
training:	Epoch: [15][278/408]	Loss 0.0972 (0.0742)	
training:	Epoch: [15][279/408]	Loss 0.0169 (0.0740)	
training:	Epoch: [15][280/408]	Loss 0.0206 (0.0738)	
training:	Epoch: [15][281/408]	Loss 0.2666 (0.0745)	
training:	Epoch: [15][282/408]	Loss 0.0169 (0.0743)	
training:	Epoch: [15][283/408]	Loss 0.0496 (0.0742)	
training:	Epoch: [15][284/408]	Loss 0.2806 (0.0749)	
training:	Epoch: [15][285/408]	Loss 0.0219 (0.0747)	
training:	Epoch: [15][286/408]	Loss 0.0211 (0.0745)	
training:	Epoch: [15][287/408]	Loss 0.0271 (0.0744)	
training:	Epoch: [15][288/408]	Loss 0.0263 (0.0742)	
training:	Epoch: [15][289/408]	Loss 0.0176 (0.0740)	
training:	Epoch: [15][290/408]	Loss 0.0244 (0.0738)	
training:	Epoch: [15][291/408]	Loss 0.0203 (0.0736)	
training:	Epoch: [15][292/408]	Loss 0.2782 (0.0743)	
training:	Epoch: [15][293/408]	Loss 0.2969 (0.0751)	
training:	Epoch: [15][294/408]	Loss 0.0181 (0.0749)	
training:	Epoch: [15][295/408]	Loss 0.0200 (0.0747)	
training:	Epoch: [15][296/408]	Loss 0.0217 (0.0745)	
training:	Epoch: [15][297/408]	Loss 0.0159 (0.0743)	
training:	Epoch: [15][298/408]	Loss 0.0170 (0.0742)	
training:	Epoch: [15][299/408]	Loss 0.2917 (0.0749)	
training:	Epoch: [15][300/408]	Loss 0.0179 (0.0747)	
training:	Epoch: [15][301/408]	Loss 0.0192 (0.0745)	
training:	Epoch: [15][302/408]	Loss 0.0178 (0.0743)	
training:	Epoch: [15][303/408]	Loss 0.2634 (0.0749)	
training:	Epoch: [15][304/408]	Loss 0.0171 (0.0748)	
training:	Epoch: [15][305/408]	Loss 0.0167 (0.0746)	
training:	Epoch: [15][306/408]	Loss 0.0157 (0.0744)	
training:	Epoch: [15][307/408]	Loss 0.0174 (0.0742)	
training:	Epoch: [15][308/408]	Loss 0.0309 (0.0740)	
training:	Epoch: [15][309/408]	Loss 0.2576 (0.0746)	
training:	Epoch: [15][310/408]	Loss 0.0192 (0.0745)	
training:	Epoch: [15][311/408]	Loss 0.0152 (0.0743)	
training:	Epoch: [15][312/408]	Loss 0.0168 (0.0741)	
training:	Epoch: [15][313/408]	Loss 0.0189 (0.0739)	
training:	Epoch: [15][314/408]	Loss 0.2971 (0.0746)	
training:	Epoch: [15][315/408]	Loss 0.2880 (0.0753)	
training:	Epoch: [15][316/408]	Loss 0.0174 (0.0751)	
training:	Epoch: [15][317/408]	Loss 0.2557 (0.0757)	
training:	Epoch: [15][318/408]	Loss 0.0152 (0.0755)	
training:	Epoch: [15][319/408]	Loss 0.0167 (0.0753)	
training:	Epoch: [15][320/408]	Loss 0.0156 (0.0751)	
training:	Epoch: [15][321/408]	Loss 0.0174 (0.0749)	
training:	Epoch: [15][322/408]	Loss 0.0170 (0.0748)	
training:	Epoch: [15][323/408]	Loss 0.0159 (0.0746)	
training:	Epoch: [15][324/408]	Loss 0.0306 (0.0744)	
training:	Epoch: [15][325/408]	Loss 0.3027 (0.0751)	
training:	Epoch: [15][326/408]	Loss 0.0179 (0.0750)	
training:	Epoch: [15][327/408]	Loss 0.0207 (0.0748)	
training:	Epoch: [15][328/408]	Loss 0.0200 (0.0746)	
training:	Epoch: [15][329/408]	Loss 0.0217 (0.0745)	
training:	Epoch: [15][330/408]	Loss 0.0492 (0.0744)	
training:	Epoch: [15][331/408]	Loss 0.0165 (0.0742)	
training:	Epoch: [15][332/408]	Loss 0.0163 (0.0740)	
training:	Epoch: [15][333/408]	Loss 0.2982 (0.0747)	
training:	Epoch: [15][334/408]	Loss 0.0237 (0.0746)	
training:	Epoch: [15][335/408]	Loss 0.0163 (0.0744)	
training:	Epoch: [15][336/408]	Loss 0.7980 (0.0765)	
training:	Epoch: [15][337/408]	Loss 0.0196 (0.0764)	
training:	Epoch: [15][338/408]	Loss 0.2452 (0.0769)	
training:	Epoch: [15][339/408]	Loss 0.0268 (0.0767)	
training:	Epoch: [15][340/408]	Loss 0.0219 (0.0766)	
training:	Epoch: [15][341/408]	Loss 0.0214 (0.0764)	
training:	Epoch: [15][342/408]	Loss 0.0214 (0.0762)	
training:	Epoch: [15][343/408]	Loss 0.0169 (0.0761)	
training:	Epoch: [15][344/408]	Loss 0.0146 (0.0759)	
training:	Epoch: [15][345/408]	Loss 0.0216 (0.0757)	
training:	Epoch: [15][346/408]	Loss 0.0168 (0.0756)	
training:	Epoch: [15][347/408]	Loss 0.0207 (0.0754)	
training:	Epoch: [15][348/408]	Loss 0.0179 (0.0752)	
training:	Epoch: [15][349/408]	Loss 0.0153 (0.0751)	
training:	Epoch: [15][350/408]	Loss 0.0262 (0.0749)	
training:	Epoch: [15][351/408]	Loss 0.0220 (0.0748)	
training:	Epoch: [15][352/408]	Loss 0.0197 (0.0746)	
training:	Epoch: [15][353/408]	Loss 0.2718 (0.0752)	
training:	Epoch: [15][354/408]	Loss 0.0577 (0.0751)	
training:	Epoch: [15][355/408]	Loss 0.0163 (0.0750)	
training:	Epoch: [15][356/408]	Loss 0.0161 (0.0748)	
training:	Epoch: [15][357/408]	Loss 0.0172 (0.0746)	
training:	Epoch: [15][358/408]	Loss 0.2751 (0.0752)	
training:	Epoch: [15][359/408]	Loss 0.0153 (0.0750)	
training:	Epoch: [15][360/408]	Loss 0.0226 (0.0749)	
training:	Epoch: [15][361/408]	Loss 0.0237 (0.0747)	
training:	Epoch: [15][362/408]	Loss 0.0202 (0.0746)	
training:	Epoch: [15][363/408]	Loss 0.0318 (0.0745)	
training:	Epoch: [15][364/408]	Loss 0.0258 (0.0743)	
training:	Epoch: [15][365/408]	Loss 0.1293 (0.0745)	
training:	Epoch: [15][366/408]	Loss 0.2676 (0.0750)	
training:	Epoch: [15][367/408]	Loss 0.0242 (0.0749)	
training:	Epoch: [15][368/408]	Loss 0.0171 (0.0747)	
training:	Epoch: [15][369/408]	Loss 0.2850 (0.0753)	
training:	Epoch: [15][370/408]	Loss 0.2330 (0.0757)	
training:	Epoch: [15][371/408]	Loss 0.3797 (0.0765)	
training:	Epoch: [15][372/408]	Loss 0.0182 (0.0764)	
training:	Epoch: [15][373/408]	Loss 0.3975 (0.0772)	
training:	Epoch: [15][374/408]	Loss 0.0474 (0.0772)	
training:	Epoch: [15][375/408]	Loss 0.0202 (0.0770)	
training:	Epoch: [15][376/408]	Loss 0.2013 (0.0773)	
training:	Epoch: [15][377/408]	Loss 0.3027 (0.0779)	
training:	Epoch: [15][378/408]	Loss 0.0255 (0.0778)	
training:	Epoch: [15][379/408]	Loss 0.0315 (0.0777)	
training:	Epoch: [15][380/408]	Loss 0.2696 (0.0782)	
training:	Epoch: [15][381/408]	Loss 0.1831 (0.0785)	
training:	Epoch: [15][382/408]	Loss 0.2869 (0.0790)	
training:	Epoch: [15][383/408]	Loss 0.2623 (0.0795)	
training:	Epoch: [15][384/408]	Loss 0.0177 (0.0793)	
training:	Epoch: [15][385/408]	Loss 0.0206 (0.0792)	
training:	Epoch: [15][386/408]	Loss 0.0552 (0.0791)	
training:	Epoch: [15][387/408]	Loss 0.0277 (0.0790)	
training:	Epoch: [15][388/408]	Loss 0.1465 (0.0792)	
training:	Epoch: [15][389/408]	Loss 0.0168 (0.0790)	
training:	Epoch: [15][390/408]	Loss 0.0271 (0.0789)	
training:	Epoch: [15][391/408]	Loss 0.0483 (0.0788)	
training:	Epoch: [15][392/408]	Loss 0.2572 (0.0792)	
training:	Epoch: [15][393/408]	Loss 0.2547 (0.0797)	
training:	Epoch: [15][394/408]	Loss 0.0163 (0.0795)	
training:	Epoch: [15][395/408]	Loss 0.0172 (0.0794)	
training:	Epoch: [15][396/408]	Loss 0.0191 (0.0792)	
training:	Epoch: [15][397/408]	Loss 0.0171 (0.0791)	
training:	Epoch: [15][398/408]	Loss 0.2938 (0.0796)	
training:	Epoch: [15][399/408]	Loss 0.0450 (0.0795)	
training:	Epoch: [15][400/408]	Loss 0.3109 (0.0801)	
training:	Epoch: [15][401/408]	Loss 0.0232 (0.0799)	
training:	Epoch: [15][402/408]	Loss 0.0207 (0.0798)	
training:	Epoch: [15][403/408]	Loss 0.0235 (0.0797)	
training:	Epoch: [15][404/408]	Loss 0.0352 (0.0795)	
training:	Epoch: [15][405/408]	Loss 0.0293 (0.0794)	
training:	Epoch: [15][406/408]	Loss 0.0421 (0.0793)	
training:	Epoch: [15][407/408]	Loss 0.0229 (0.0792)	
training:	Epoch: [15][408/408]	Loss 0.2579 (0.0796)	
Training:	 Loss: 0.0795

Training:	 ACC: 0.9871 0.9870 0.9844 0.9898
Validation:	 ACC: 0.7864 0.7876 0.8117 0.7612
Validation:	 Best_BACC: 0.8226 0.8250 0.8772 0.7679
Validation:	 Loss: 0.7503
Pretraining:	Epoch 16/200
----------
training:	Epoch: [16][1/408]	Loss 0.2950 (0.2950)	
training:	Epoch: [16][2/408]	Loss 0.2450 (0.2700)	
training:	Epoch: [16][3/408]	Loss 0.0173 (0.1858)	
training:	Epoch: [16][4/408]	Loss 0.0202 (0.1444)	
training:	Epoch: [16][5/408]	Loss 0.2814 (0.1718)	
training:	Epoch: [16][6/408]	Loss 0.2923 (0.1919)	
training:	Epoch: [16][7/408]	Loss 0.2624 (0.2020)	
training:	Epoch: [16][8/408]	Loss 0.0229 (0.1796)	
training:	Epoch: [16][9/408]	Loss 0.0280 (0.1627)	
training:	Epoch: [16][10/408]	Loss 0.0188 (0.1483)	
training:	Epoch: [16][11/408]	Loss 0.0184 (0.1365)	
training:	Epoch: [16][12/408]	Loss 0.0168 (0.1265)	
training:	Epoch: [16][13/408]	Loss 0.0198 (0.1183)	
training:	Epoch: [16][14/408]	Loss 0.2872 (0.1304)	
training:	Epoch: [16][15/408]	Loss 0.0495 (0.1250)	
training:	Epoch: [16][16/408]	Loss 0.0165 (0.1182)	
training:	Epoch: [16][17/408]	Loss 0.0189 (0.1124)	
training:	Epoch: [16][18/408]	Loss 0.0177 (0.1071)	
training:	Epoch: [16][19/408]	Loss 0.0168 (0.1024)	
training:	Epoch: [16][20/408]	Loss 0.0196 (0.0982)	
training:	Epoch: [16][21/408]	Loss 0.0169 (0.0944)	
training:	Epoch: [16][22/408]	Loss 0.0174 (0.0909)	
training:	Epoch: [16][23/408]	Loss 0.0193 (0.0878)	
training:	Epoch: [16][24/408]	Loss 0.0223 (0.0850)	
training:	Epoch: [16][25/408]	Loss 0.0233 (0.0826)	
training:	Epoch: [16][26/408]	Loss 0.0290 (0.0805)	
training:	Epoch: [16][27/408]	Loss 0.0276 (0.0785)	
training:	Epoch: [16][28/408]	Loss 0.0479 (0.0774)	
training:	Epoch: [16][29/408]	Loss 0.0173 (0.0754)	
training:	Epoch: [16][30/408]	Loss 0.0171 (0.0734)	
training:	Epoch: [16][31/408]	Loss 0.0190 (0.0717)	
training:	Epoch: [16][32/408]	Loss 0.0180 (0.0700)	
training:	Epoch: [16][33/408]	Loss 0.0169 (0.0684)	
training:	Epoch: [16][34/408]	Loss 0.0219 (0.0670)	
training:	Epoch: [16][35/408]	Loss 0.2898 (0.0734)	
training:	Epoch: [16][36/408]	Loss 0.0184 (0.0719)	
training:	Epoch: [16][37/408]	Loss 0.0176 (0.0704)	
training:	Epoch: [16][38/408]	Loss 0.2958 (0.0763)	
training:	Epoch: [16][39/408]	Loss 0.0164 (0.0748)	
training:	Epoch: [16][40/408]	Loss 0.0173 (0.0733)	
training:	Epoch: [16][41/408]	Loss 0.0195 (0.0720)	
training:	Epoch: [16][42/408]	Loss 0.2579 (0.0765)	
training:	Epoch: [16][43/408]	Loss 0.0167 (0.0751)	
training:	Epoch: [16][44/408]	Loss 0.2603 (0.0793)	
training:	Epoch: [16][45/408]	Loss 0.0167 (0.0779)	
training:	Epoch: [16][46/408]	Loss 0.0180 (0.0766)	
training:	Epoch: [16][47/408]	Loss 0.2910 (0.0811)	
training:	Epoch: [16][48/408]	Loss 0.2614 (0.0849)	
training:	Epoch: [16][49/408]	Loss 0.2674 (0.0886)	
training:	Epoch: [16][50/408]	Loss 0.0237 (0.0873)	
training:	Epoch: [16][51/408]	Loss 0.0360 (0.0863)	
training:	Epoch: [16][52/408]	Loss 0.2634 (0.0897)	
training:	Epoch: [16][53/408]	Loss 0.0208 (0.0884)	
training:	Epoch: [16][54/408]	Loss 0.4719 (0.0955)	
training:	Epoch: [16][55/408]	Loss 0.0201 (0.0942)	
training:	Epoch: [16][56/408]	Loss 0.2940 (0.0977)	
training:	Epoch: [16][57/408]	Loss 0.0167 (0.0963)	
training:	Epoch: [16][58/408]	Loss 0.0196 (0.0950)	
training:	Epoch: [16][59/408]	Loss 0.0178 (0.0937)	
training:	Epoch: [16][60/408]	Loss 0.0160 (0.0924)	
training:	Epoch: [16][61/408]	Loss 0.2980 (0.0957)	
training:	Epoch: [16][62/408]	Loss 0.0157 (0.0945)	
training:	Epoch: [16][63/408]	Loss 0.0397 (0.0936)	
training:	Epoch: [16][64/408]	Loss 0.0150 (0.0924)	
training:	Epoch: [16][65/408]	Loss 0.0185 (0.0912)	
training:	Epoch: [16][66/408]	Loss 0.0149 (0.0901)	
training:	Epoch: [16][67/408]	Loss 0.0168 (0.0890)	
training:	Epoch: [16][68/408]	Loss 0.0253 (0.0880)	
training:	Epoch: [16][69/408]	Loss 0.0203 (0.0871)	
training:	Epoch: [16][70/408]	Loss 0.0206 (0.0861)	
training:	Epoch: [16][71/408]	Loss 0.2661 (0.0886)	
training:	Epoch: [16][72/408]	Loss 0.0164 (0.0876)	
training:	Epoch: [16][73/408]	Loss 0.0190 (0.0867)	
training:	Epoch: [16][74/408]	Loss 0.0267 (0.0859)	
training:	Epoch: [16][75/408]	Loss 0.0223 (0.0850)	
training:	Epoch: [16][76/408]	Loss 0.0155 (0.0841)	
training:	Epoch: [16][77/408]	Loss 0.0166 (0.0832)	
training:	Epoch: [16][78/408]	Loss 0.2046 (0.0848)	
training:	Epoch: [16][79/408]	Loss 0.0230 (0.0840)	
training:	Epoch: [16][80/408]	Loss 0.0176 (0.0832)	
training:	Epoch: [16][81/408]	Loss 0.0165 (0.0824)	
training:	Epoch: [16][82/408]	Loss 0.0192 (0.0816)	
training:	Epoch: [16][83/408]	Loss 0.0173 (0.0808)	
training:	Epoch: [16][84/408]	Loss 0.0216 (0.0801)	
training:	Epoch: [16][85/408]	Loss 0.0209 (0.0794)	
training:	Epoch: [16][86/408]	Loss 0.0183 (0.0787)	
training:	Epoch: [16][87/408]	Loss 0.0180 (0.0780)	
training:	Epoch: [16][88/408]	Loss 0.0217 (0.0774)	
training:	Epoch: [16][89/408]	Loss 0.2641 (0.0795)	
training:	Epoch: [16][90/408]	Loss 0.0428 (0.0791)	
training:	Epoch: [16][91/408]	Loss 0.0167 (0.0784)	
training:	Epoch: [16][92/408]	Loss 0.0241 (0.0778)	
training:	Epoch: [16][93/408]	Loss 0.3301 (0.0805)	
training:	Epoch: [16][94/408]	Loss 0.0346 (0.0800)	
training:	Epoch: [16][95/408]	Loss 0.0222 (0.0794)	
training:	Epoch: [16][96/408]	Loss 0.0188 (0.0788)	
training:	Epoch: [16][97/408]	Loss 0.0172 (0.0781)	
training:	Epoch: [16][98/408]	Loss 0.0177 (0.0775)	
training:	Epoch: [16][99/408]	Loss 0.1002 (0.0777)	
training:	Epoch: [16][100/408]	Loss 0.0173 (0.0771)	
training:	Epoch: [16][101/408]	Loss 0.0172 (0.0765)	
training:	Epoch: [16][102/408]	Loss 0.0165 (0.0760)	
training:	Epoch: [16][103/408]	Loss 0.0239 (0.0755)	
training:	Epoch: [16][104/408]	Loss 0.0152 (0.0749)	
training:	Epoch: [16][105/408]	Loss 0.0151 (0.0743)	
training:	Epoch: [16][106/408]	Loss 0.0207 (0.0738)	
training:	Epoch: [16][107/408]	Loss 0.2449 (0.0754)	
training:	Epoch: [16][108/408]	Loss 0.0190 (0.0749)	
training:	Epoch: [16][109/408]	Loss 0.0168 (0.0743)	
training:	Epoch: [16][110/408]	Loss 0.2931 (0.0763)	
training:	Epoch: [16][111/408]	Loss 0.0181 (0.0758)	
training:	Epoch: [16][112/408]	Loss 0.0168 (0.0753)	
training:	Epoch: [16][113/408]	Loss 0.2976 (0.0772)	
training:	Epoch: [16][114/408]	Loss 0.0224 (0.0768)	
training:	Epoch: [16][115/408]	Loss 0.0174 (0.0763)	
training:	Epoch: [16][116/408]	Loss 0.0191 (0.0758)	
training:	Epoch: [16][117/408]	Loss 0.0167 (0.0753)	
training:	Epoch: [16][118/408]	Loss 0.0338 (0.0749)	
training:	Epoch: [16][119/408]	Loss 0.0177 (0.0744)	
training:	Epoch: [16][120/408]	Loss 0.0254 (0.0740)	
training:	Epoch: [16][121/408]	Loss 0.0213 (0.0736)	
training:	Epoch: [16][122/408]	Loss 0.2406 (0.0749)	
training:	Epoch: [16][123/408]	Loss 0.2830 (0.0766)	
training:	Epoch: [16][124/408]	Loss 0.0168 (0.0762)	
training:	Epoch: [16][125/408]	Loss 0.0180 (0.0757)	
training:	Epoch: [16][126/408]	Loss 0.2545 (0.0771)	
training:	Epoch: [16][127/408]	Loss 0.2963 (0.0788)	
training:	Epoch: [16][128/408]	Loss 0.2910 (0.0805)	
training:	Epoch: [16][129/408]	Loss 0.0194 (0.0800)	
training:	Epoch: [16][130/408]	Loss 0.0209 (0.0796)	
training:	Epoch: [16][131/408]	Loss 0.0171 (0.0791)	
training:	Epoch: [16][132/408]	Loss 0.0221 (0.0787)	
training:	Epoch: [16][133/408]	Loss 0.0179 (0.0782)	
training:	Epoch: [16][134/408]	Loss 0.0349 (0.0779)	
training:	Epoch: [16][135/408]	Loss 0.0148 (0.0774)	
training:	Epoch: [16][136/408]	Loss 0.0220 (0.0770)	
training:	Epoch: [16][137/408]	Loss 0.0211 (0.0766)	
training:	Epoch: [16][138/408]	Loss 0.0199 (0.0762)	
training:	Epoch: [16][139/408]	Loss 0.0169 (0.0758)	
training:	Epoch: [16][140/408]	Loss 0.0180 (0.0753)	
training:	Epoch: [16][141/408]	Loss 0.0158 (0.0749)	
training:	Epoch: [16][142/408]	Loss 0.0317 (0.0746)	
training:	Epoch: [16][143/408]	Loss 0.0169 (0.0742)	
training:	Epoch: [16][144/408]	Loss 0.0160 (0.0738)	
training:	Epoch: [16][145/408]	Loss 0.0178 (0.0734)	
training:	Epoch: [16][146/408]	Loss 0.0186 (0.0730)	
training:	Epoch: [16][147/408]	Loss 0.0162 (0.0727)	
training:	Epoch: [16][148/408]	Loss 0.2684 (0.0740)	
training:	Epoch: [16][149/408]	Loss 0.0220 (0.0736)	
training:	Epoch: [16][150/408]	Loss 0.0494 (0.0735)	
training:	Epoch: [16][151/408]	Loss 0.0184 (0.0731)	
training:	Epoch: [16][152/408]	Loss 0.2778 (0.0745)	
training:	Epoch: [16][153/408]	Loss 0.0303 (0.0742)	
training:	Epoch: [16][154/408]	Loss 0.0168 (0.0738)	
training:	Epoch: [16][155/408]	Loss 0.2661 (0.0750)	
training:	Epoch: [16][156/408]	Loss 0.0164 (0.0747)	
training:	Epoch: [16][157/408]	Loss 0.0162 (0.0743)	
training:	Epoch: [16][158/408]	Loss 0.0159 (0.0739)	
training:	Epoch: [16][159/408]	Loss 0.0399 (0.0737)	
training:	Epoch: [16][160/408]	Loss 0.0202 (0.0734)	
training:	Epoch: [16][161/408]	Loss 0.2865 (0.0747)	
training:	Epoch: [16][162/408]	Loss 0.0159 (0.0743)	
training:	Epoch: [16][163/408]	Loss 0.0267 (0.0740)	
training:	Epoch: [16][164/408]	Loss 0.2984 (0.0754)	
training:	Epoch: [16][165/408]	Loss 0.0165 (0.0750)	
training:	Epoch: [16][166/408]	Loss 0.2882 (0.0763)	
training:	Epoch: [16][167/408]	Loss 0.0164 (0.0760)	
training:	Epoch: [16][168/408]	Loss 0.0149 (0.0756)	
training:	Epoch: [16][169/408]	Loss 0.2918 (0.0769)	
training:	Epoch: [16][170/408]	Loss 0.0158 (0.0765)	
training:	Epoch: [16][171/408]	Loss 0.2609 (0.0776)	
training:	Epoch: [16][172/408]	Loss 0.0172 (0.0773)	
training:	Epoch: [16][173/408]	Loss 0.0176 (0.0769)	
training:	Epoch: [16][174/408]	Loss 0.0173 (0.0766)	
training:	Epoch: [16][175/408]	Loss 0.0161 (0.0762)	
training:	Epoch: [16][176/408]	Loss 0.0154 (0.0759)	
training:	Epoch: [16][177/408]	Loss 0.0170 (0.0755)	
training:	Epoch: [16][178/408]	Loss 0.0178 (0.0752)	
training:	Epoch: [16][179/408]	Loss 0.0151 (0.0749)	
training:	Epoch: [16][180/408]	Loss 0.0306 (0.0746)	
training:	Epoch: [16][181/408]	Loss 0.0193 (0.0743)	
training:	Epoch: [16][182/408]	Loss 0.0159 (0.0740)	
training:	Epoch: [16][183/408]	Loss 0.0523 (0.0739)	
training:	Epoch: [16][184/408]	Loss 0.2512 (0.0749)	
training:	Epoch: [16][185/408]	Loss 0.0181 (0.0746)	
training:	Epoch: [16][186/408]	Loss 0.0157 (0.0742)	
training:	Epoch: [16][187/408]	Loss 0.0157 (0.0739)	
training:	Epoch: [16][188/408]	Loss 0.0198 (0.0736)	
training:	Epoch: [16][189/408]	Loss 0.0161 (0.0733)	
training:	Epoch: [16][190/408]	Loss 0.0168 (0.0730)	
training:	Epoch: [16][191/408]	Loss 0.0190 (0.0727)	
training:	Epoch: [16][192/408]	Loss 0.0171 (0.0725)	
training:	Epoch: [16][193/408]	Loss 0.0194 (0.0722)	
training:	Epoch: [16][194/408]	Loss 0.2564 (0.0731)	
training:	Epoch: [16][195/408]	Loss 0.0238 (0.0729)	
training:	Epoch: [16][196/408]	Loss 0.0448 (0.0727)	
training:	Epoch: [16][197/408]	Loss 0.0161 (0.0724)	
training:	Epoch: [16][198/408]	Loss 0.0176 (0.0722)	
training:	Epoch: [16][199/408]	Loss 0.0161 (0.0719)	
training:	Epoch: [16][200/408]	Loss 0.0224 (0.0716)	
training:	Epoch: [16][201/408]	Loss 0.0151 (0.0714)	
training:	Epoch: [16][202/408]	Loss 0.0157 (0.0711)	
training:	Epoch: [16][203/408]	Loss 0.0183 (0.0708)	
training:	Epoch: [16][204/408]	Loss 0.2617 (0.0718)	
training:	Epoch: [16][205/408]	Loss 0.0706 (0.0718)	
training:	Epoch: [16][206/408]	Loss 0.0150 (0.0715)	
training:	Epoch: [16][207/408]	Loss 0.0146 (0.0712)	
training:	Epoch: [16][208/408]	Loss 0.0166 (0.0709)	
training:	Epoch: [16][209/408]	Loss 0.2959 (0.0720)	
training:	Epoch: [16][210/408]	Loss 0.0146 (0.0717)	
training:	Epoch: [16][211/408]	Loss 0.0202 (0.0715)	
training:	Epoch: [16][212/408]	Loss 0.2560 (0.0724)	
training:	Epoch: [16][213/408]	Loss 0.0167 (0.0721)	
training:	Epoch: [16][214/408]	Loss 0.0180 (0.0719)	
training:	Epoch: [16][215/408]	Loss 0.2901 (0.0729)	
training:	Epoch: [16][216/408]	Loss 0.2831 (0.0738)	
training:	Epoch: [16][217/408]	Loss 0.2634 (0.0747)	
training:	Epoch: [16][218/408]	Loss 0.0194 (0.0745)	
training:	Epoch: [16][219/408]	Loss 0.0171 (0.0742)	
training:	Epoch: [16][220/408]	Loss 0.0247 (0.0740)	
training:	Epoch: [16][221/408]	Loss 0.0219 (0.0737)	
training:	Epoch: [16][222/408]	Loss 0.5463 (0.0759)	
training:	Epoch: [16][223/408]	Loss 0.0152 (0.0756)	
training:	Epoch: [16][224/408]	Loss 0.0185 (0.0753)	
training:	Epoch: [16][225/408]	Loss 0.0163 (0.0751)	
training:	Epoch: [16][226/408]	Loss 0.0169 (0.0748)	
training:	Epoch: [16][227/408]	Loss 0.0149 (0.0746)	
training:	Epoch: [16][228/408]	Loss 0.2612 (0.0754)	
training:	Epoch: [16][229/408]	Loss 0.2637 (0.0762)	
training:	Epoch: [16][230/408]	Loss 0.0428 (0.0761)	
training:	Epoch: [16][231/408]	Loss 0.0171 (0.0758)	
training:	Epoch: [16][232/408]	Loss 0.0200 (0.0756)	
training:	Epoch: [16][233/408]	Loss 0.0160 (0.0753)	
training:	Epoch: [16][234/408]	Loss 0.0235 (0.0751)	
training:	Epoch: [16][235/408]	Loss 0.2678 (0.0759)	
training:	Epoch: [16][236/408]	Loss 0.0168 (0.0757)	
training:	Epoch: [16][237/408]	Loss 0.0171 (0.0754)	
training:	Epoch: [16][238/408]	Loss 0.0137 (0.0751)	
training:	Epoch: [16][239/408]	Loss 0.0174 (0.0749)	
training:	Epoch: [16][240/408]	Loss 0.0158 (0.0747)	
training:	Epoch: [16][241/408]	Loss 0.0171 (0.0744)	
training:	Epoch: [16][242/408]	Loss 0.0265 (0.0742)	
training:	Epoch: [16][243/408]	Loss 0.1747 (0.0746)	
training:	Epoch: [16][244/408]	Loss 0.0170 (0.0744)	
training:	Epoch: [16][245/408]	Loss 0.2434 (0.0751)	
training:	Epoch: [16][246/408]	Loss 0.2706 (0.0759)	
training:	Epoch: [16][247/408]	Loss 0.2891 (0.0767)	
training:	Epoch: [16][248/408]	Loss 0.0174 (0.0765)	
training:	Epoch: [16][249/408]	Loss 0.2925 (0.0774)	
training:	Epoch: [16][250/408]	Loss 0.0630 (0.0773)	
training:	Epoch: [16][251/408]	Loss 0.0338 (0.0771)	
training:	Epoch: [16][252/408]	Loss 0.0215 (0.0769)	
training:	Epoch: [16][253/408]	Loss 0.1837 (0.0773)	
training:	Epoch: [16][254/408]	Loss 0.0207 (0.0771)	
training:	Epoch: [16][255/408]	Loss 0.0171 (0.0769)	
training:	Epoch: [16][256/408]	Loss 0.0174 (0.0767)	
training:	Epoch: [16][257/408]	Loss 0.0178 (0.0764)	
training:	Epoch: [16][258/408]	Loss 0.0914 (0.0765)	
training:	Epoch: [16][259/408]	Loss 0.0184 (0.0763)	
training:	Epoch: [16][260/408]	Loss 0.0210 (0.0760)	
training:	Epoch: [16][261/408]	Loss 0.2851 (0.0768)	
training:	Epoch: [16][262/408]	Loss 0.0168 (0.0766)	
training:	Epoch: [16][263/408]	Loss 0.0175 (0.0764)	
training:	Epoch: [16][264/408]	Loss 0.2590 (0.0771)	
training:	Epoch: [16][265/408]	Loss 0.0606 (0.0770)	
training:	Epoch: [16][266/408]	Loss 0.0161 (0.0768)	
training:	Epoch: [16][267/408]	Loss 0.1263 (0.0770)	
training:	Epoch: [16][268/408]	Loss 0.0175 (0.0768)	
training:	Epoch: [16][269/408]	Loss 0.0159 (0.0765)	
training:	Epoch: [16][270/408]	Loss 0.0199 (0.0763)	
training:	Epoch: [16][271/408]	Loss 0.0253 (0.0761)	
training:	Epoch: [16][272/408]	Loss 0.2395 (0.0767)	
training:	Epoch: [16][273/408]	Loss 0.3117 (0.0776)	
training:	Epoch: [16][274/408]	Loss 0.0149 (0.0774)	
training:	Epoch: [16][275/408]	Loss 0.0712 (0.0773)	
training:	Epoch: [16][276/408]	Loss 0.0254 (0.0772)	
training:	Epoch: [16][277/408]	Loss 0.0394 (0.0770)	
training:	Epoch: [16][278/408]	Loss 0.0160 (0.0768)	
training:	Epoch: [16][279/408]	Loss 0.0482 (0.0767)	
training:	Epoch: [16][280/408]	Loss 0.0164 (0.0765)	
training:	Epoch: [16][281/408]	Loss 0.0186 (0.0763)	
training:	Epoch: [16][282/408]	Loss 0.2929 (0.0770)	
training:	Epoch: [16][283/408]	Loss 0.0341 (0.0769)	
training:	Epoch: [16][284/408]	Loss 0.0161 (0.0767)	
training:	Epoch: [16][285/408]	Loss 0.0195 (0.0765)	
training:	Epoch: [16][286/408]	Loss 0.0174 (0.0763)	
training:	Epoch: [16][287/408]	Loss 0.0354 (0.0761)	
training:	Epoch: [16][288/408]	Loss 0.0193 (0.0759)	
training:	Epoch: [16][289/408]	Loss 0.0170 (0.0757)	
training:	Epoch: [16][290/408]	Loss 0.0210 (0.0755)	
training:	Epoch: [16][291/408]	Loss 0.0201 (0.0754)	
training:	Epoch: [16][292/408]	Loss 0.0200 (0.0752)	
training:	Epoch: [16][293/408]	Loss 0.2612 (0.0758)	
training:	Epoch: [16][294/408]	Loss 0.2343 (0.0763)	
training:	Epoch: [16][295/408]	Loss 0.0158 (0.0761)	
training:	Epoch: [16][296/408]	Loss 0.0172 (0.0759)	
training:	Epoch: [16][297/408]	Loss 0.0168 (0.0757)	
training:	Epoch: [16][298/408]	Loss 0.0284 (0.0756)	
training:	Epoch: [16][299/408]	Loss 0.0157 (0.0754)	
training:	Epoch: [16][300/408]	Loss 0.0183 (0.0752)	
training:	Epoch: [16][301/408]	Loss 0.0149 (0.0750)	
training:	Epoch: [16][302/408]	Loss 0.0200 (0.0748)	
training:	Epoch: [16][303/408]	Loss 0.2924 (0.0755)	
training:	Epoch: [16][304/408]	Loss 0.0197 (0.0753)	
training:	Epoch: [16][305/408]	Loss 0.0163 (0.0751)	
training:	Epoch: [16][306/408]	Loss 0.0166 (0.0749)	
training:	Epoch: [16][307/408]	Loss 0.0228 (0.0748)	
training:	Epoch: [16][308/408]	Loss 0.2956 (0.0755)	
training:	Epoch: [16][309/408]	Loss 0.0157 (0.0753)	
training:	Epoch: [16][310/408]	Loss 0.2963 (0.0760)	
training:	Epoch: [16][311/408]	Loss 0.0232 (0.0758)	
training:	Epoch: [16][312/408]	Loss 0.0160 (0.0757)	
training:	Epoch: [16][313/408]	Loss 0.0560 (0.0756)	
training:	Epoch: [16][314/408]	Loss 0.0203 (0.0754)	
training:	Epoch: [16][315/408]	Loss 0.2857 (0.0761)	
training:	Epoch: [16][316/408]	Loss 0.0157 (0.0759)	
training:	Epoch: [16][317/408]	Loss 0.0161 (0.0757)	
training:	Epoch: [16][318/408]	Loss 0.0171 (0.0755)	
training:	Epoch: [16][319/408]	Loss 0.0173 (0.0753)	
training:	Epoch: [16][320/408]	Loss 0.0161 (0.0752)	
training:	Epoch: [16][321/408]	Loss 0.0153 (0.0750)	
training:	Epoch: [16][322/408]	Loss 0.0179 (0.0748)	
training:	Epoch: [16][323/408]	Loss 0.0392 (0.0747)	
training:	Epoch: [16][324/408]	Loss 0.0261 (0.0745)	
training:	Epoch: [16][325/408]	Loss 0.0175 (0.0744)	
training:	Epoch: [16][326/408]	Loss 0.0164 (0.0742)	
training:	Epoch: [16][327/408]	Loss 0.0217 (0.0740)	
training:	Epoch: [16][328/408]	Loss 0.0189 (0.0738)	
training:	Epoch: [16][329/408]	Loss 0.0189 (0.0737)	
training:	Epoch: [16][330/408]	Loss 0.0185 (0.0735)	
training:	Epoch: [16][331/408]	Loss 0.0165 (0.0733)	
training:	Epoch: [16][332/408]	Loss 0.0153 (0.0732)	
training:	Epoch: [16][333/408]	Loss 0.0168 (0.0730)	
training:	Epoch: [16][334/408]	Loss 0.0184 (0.0728)	
training:	Epoch: [16][335/408]	Loss 0.2795 (0.0734)	
training:	Epoch: [16][336/408]	Loss 0.0231 (0.0733)	
training:	Epoch: [16][337/408]	Loss 0.0183 (0.0731)	
training:	Epoch: [16][338/408]	Loss 0.0170 (0.0730)	
training:	Epoch: [16][339/408]	Loss 0.0158 (0.0728)	
training:	Epoch: [16][340/408]	Loss 0.0143 (0.0726)	
training:	Epoch: [16][341/408]	Loss 0.0147 (0.0725)	
training:	Epoch: [16][342/408]	Loss 0.0211 (0.0723)	
training:	Epoch: [16][343/408]	Loss 0.0265 (0.0722)	
training:	Epoch: [16][344/408]	Loss 0.2661 (0.0727)	
training:	Epoch: [16][345/408]	Loss 0.5405 (0.0741)	
training:	Epoch: [16][346/408]	Loss 0.0178 (0.0739)	
training:	Epoch: [16][347/408]	Loss 0.2944 (0.0746)	
training:	Epoch: [16][348/408]	Loss 0.1581 (0.0748)	
training:	Epoch: [16][349/408]	Loss 0.0144 (0.0746)	
training:	Epoch: [16][350/408]	Loss 0.2641 (0.0752)	
training:	Epoch: [16][351/408]	Loss 0.2162 (0.0756)	
training:	Epoch: [16][352/408]	Loss 0.0596 (0.0755)	
training:	Epoch: [16][353/408]	Loss 0.0162 (0.0754)	
training:	Epoch: [16][354/408]	Loss 0.0156 (0.0752)	
training:	Epoch: [16][355/408]	Loss 0.4952 (0.0764)	
training:	Epoch: [16][356/408]	Loss 0.0157 (0.0762)	
training:	Epoch: [16][357/408]	Loss 0.2437 (0.0767)	
training:	Epoch: [16][358/408]	Loss 0.0208 (0.0765)	
training:	Epoch: [16][359/408]	Loss 0.0402 (0.0764)	
training:	Epoch: [16][360/408]	Loss 0.0150 (0.0762)	
training:	Epoch: [16][361/408]	Loss 0.0191 (0.0761)	
training:	Epoch: [16][362/408]	Loss 0.0151 (0.0759)	
training:	Epoch: [16][363/408]	Loss 0.0799 (0.0759)	
training:	Epoch: [16][364/408]	Loss 0.0321 (0.0758)	
training:	Epoch: [16][365/408]	Loss 0.0217 (0.0757)	
training:	Epoch: [16][366/408]	Loss 0.0178 (0.0755)	
training:	Epoch: [16][367/408]	Loss 0.0170 (0.0753)	
training:	Epoch: [16][368/408]	Loss 0.0173 (0.0752)	
training:	Epoch: [16][369/408]	Loss 0.2686 (0.0757)	
training:	Epoch: [16][370/408]	Loss 0.0174 (0.0756)	
training:	Epoch: [16][371/408]	Loss 0.0295 (0.0754)	
training:	Epoch: [16][372/408]	Loss 0.0225 (0.0753)	
training:	Epoch: [16][373/408]	Loss 0.0175 (0.0751)	
training:	Epoch: [16][374/408]	Loss 0.0162 (0.0750)	
training:	Epoch: [16][375/408]	Loss 0.0175 (0.0748)	
training:	Epoch: [16][376/408]	Loss 0.0239 (0.0747)	
training:	Epoch: [16][377/408]	Loss 0.0261 (0.0746)	
training:	Epoch: [16][378/408]	Loss 0.0210 (0.0744)	
training:	Epoch: [16][379/408]	Loss 0.0163 (0.0743)	
training:	Epoch: [16][380/408]	Loss 0.0201 (0.0741)	
training:	Epoch: [16][381/408]	Loss 0.0151 (0.0740)	
training:	Epoch: [16][382/408]	Loss 0.0317 (0.0739)	
training:	Epoch: [16][383/408]	Loss 0.0153 (0.0737)	
training:	Epoch: [16][384/408]	Loss 0.0196 (0.0736)	
training:	Epoch: [16][385/408]	Loss 0.0142 (0.0734)	
training:	Epoch: [16][386/408]	Loss 0.2816 (0.0739)	
training:	Epoch: [16][387/408]	Loss 0.0193 (0.0738)	
training:	Epoch: [16][388/408]	Loss 0.0501 (0.0737)	
training:	Epoch: [16][389/408]	Loss 0.0172 (0.0736)	
training:	Epoch: [16][390/408]	Loss 0.0173 (0.0735)	
training:	Epoch: [16][391/408]	Loss 0.0193 (0.0733)	
training:	Epoch: [16][392/408]	Loss 0.0158 (0.0732)	
training:	Epoch: [16][393/408]	Loss 0.0171 (0.0730)	
training:	Epoch: [16][394/408]	Loss 0.1048 (0.0731)	
training:	Epoch: [16][395/408]	Loss 0.0176 (0.0730)	
training:	Epoch: [16][396/408]	Loss 0.0255 (0.0728)	
training:	Epoch: [16][397/408]	Loss 0.2981 (0.0734)	
training:	Epoch: [16][398/408]	Loss 0.0264 (0.0733)	
training:	Epoch: [16][399/408]	Loss 0.2634 (0.0738)	
training:	Epoch: [16][400/408]	Loss 0.0155 (0.0736)	
training:	Epoch: [16][401/408]	Loss 0.0171 (0.0735)	
training:	Epoch: [16][402/408]	Loss 0.0799 (0.0735)	
training:	Epoch: [16][403/408]	Loss 0.0153 (0.0734)	
training:	Epoch: [16][404/408]	Loss 0.0167 (0.0732)	
training:	Epoch: [16][405/408]	Loss 0.0220 (0.0731)	
training:	Epoch: [16][406/408]	Loss 0.0218 (0.0730)	
training:	Epoch: [16][407/408]	Loss 0.0166 (0.0728)	
training:	Epoch: [16][408/408]	Loss 0.0266 (0.0727)	
Training:	 Loss: 0.0726

Training:	 ACC: 0.9875 0.9873 0.9838 0.9911
Validation:	 ACC: 0.7815 0.7817 0.7851 0.7780
Validation:	 Best_BACC: 0.8226 0.8250 0.8772 0.7679
Validation:	 Loss: 0.7973
Pretraining:	Epoch 17/200
----------
training:	Epoch: [17][1/408]	Loss 0.0138 (0.0138)	
training:	Epoch: [17][2/408]	Loss 0.0159 (0.0148)	
training:	Epoch: [17][3/408]	Loss 0.0139 (0.0145)	
training:	Epoch: [17][4/408]	Loss 0.0184 (0.0155)	
training:	Epoch: [17][5/408]	Loss 0.0150 (0.0154)	
training:	Epoch: [17][6/408]	Loss 0.3062 (0.0639)	
training:	Epoch: [17][7/408]	Loss 0.0166 (0.0571)	
training:	Epoch: [17][8/408]	Loss 0.2615 (0.0827)	
training:	Epoch: [17][9/408]	Loss 0.0170 (0.0754)	
training:	Epoch: [17][10/408]	Loss 0.0187 (0.0697)	
training:	Epoch: [17][11/408]	Loss 0.2027 (0.0818)	
training:	Epoch: [17][12/408]	Loss 0.2895 (0.0991)	
training:	Epoch: [17][13/408]	Loss 0.0164 (0.0927)	
training:	Epoch: [17][14/408]	Loss 0.0154 (0.0872)	
training:	Epoch: [17][15/408]	Loss 0.0176 (0.0826)	
training:	Epoch: [17][16/408]	Loss 0.0152 (0.0784)	
training:	Epoch: [17][17/408]	Loss 0.0164 (0.0747)	
training:	Epoch: [17][18/408]	Loss 0.0191 (0.0716)	
training:	Epoch: [17][19/408]	Loss 0.0188 (0.0689)	
training:	Epoch: [17][20/408]	Loss 0.0150 (0.0662)	
training:	Epoch: [17][21/408]	Loss 0.0165 (0.0638)	
training:	Epoch: [17][22/408]	Loss 0.0153 (0.0616)	
training:	Epoch: [17][23/408]	Loss 0.0163 (0.0596)	
training:	Epoch: [17][24/408]	Loss 0.0156 (0.0578)	
training:	Epoch: [17][25/408]	Loss 0.0181 (0.0562)	
training:	Epoch: [17][26/408]	Loss 0.0166 (0.0547)	
training:	Epoch: [17][27/408]	Loss 0.0137 (0.0532)	
training:	Epoch: [17][28/408]	Loss 0.0171 (0.0519)	
training:	Epoch: [17][29/408]	Loss 0.0163 (0.0506)	
training:	Epoch: [17][30/408]	Loss 0.0265 (0.0498)	
training:	Epoch: [17][31/408]	Loss 0.0187 (0.0488)	
training:	Epoch: [17][32/408]	Loss 0.0159 (0.0478)	
training:	Epoch: [17][33/408]	Loss 0.3042 (0.0556)	
training:	Epoch: [17][34/408]	Loss 0.0181 (0.0545)	
training:	Epoch: [17][35/408]	Loss 0.3079 (0.0617)	
training:	Epoch: [17][36/408]	Loss 0.0184 (0.0605)	
training:	Epoch: [17][37/408]	Loss 0.0484 (0.0602)	
training:	Epoch: [17][38/408]	Loss 0.0155 (0.0590)	
training:	Epoch: [17][39/408]	Loss 0.0174 (0.0579)	
training:	Epoch: [17][40/408]	Loss 0.2685 (0.0632)	
training:	Epoch: [17][41/408]	Loss 0.0145 (0.0620)	
training:	Epoch: [17][42/408]	Loss 0.0143 (0.0609)	
training:	Epoch: [17][43/408]	Loss 0.0149 (0.0598)	
training:	Epoch: [17][44/408]	Loss 0.0161 (0.0588)	
training:	Epoch: [17][45/408]	Loss 0.0155 (0.0579)	
training:	Epoch: [17][46/408]	Loss 0.0159 (0.0569)	
training:	Epoch: [17][47/408]	Loss 0.2650 (0.0614)	
training:	Epoch: [17][48/408]	Loss 0.2602 (0.0655)	
training:	Epoch: [17][49/408]	Loss 0.0172 (0.0645)	
training:	Epoch: [17][50/408]	Loss 0.0468 (0.0642)	
training:	Epoch: [17][51/408]	Loss 0.0143 (0.0632)	
training:	Epoch: [17][52/408]	Loss 0.0136 (0.0622)	
training:	Epoch: [17][53/408]	Loss 0.0134 (0.0613)	
training:	Epoch: [17][54/408]	Loss 0.0164 (0.0605)	
training:	Epoch: [17][55/408]	Loss 0.0207 (0.0598)	
training:	Epoch: [17][56/408]	Loss 0.0152 (0.0590)	
training:	Epoch: [17][57/408]	Loss 0.0141 (0.0582)	
training:	Epoch: [17][58/408]	Loss 0.2672 (0.0618)	
training:	Epoch: [17][59/408]	Loss 0.0165 (0.0610)	
training:	Epoch: [17][60/408]	Loss 0.2916 (0.0649)	
training:	Epoch: [17][61/408]	Loss 0.0187 (0.0641)	
training:	Epoch: [17][62/408]	Loss 0.0378 (0.0637)	
training:	Epoch: [17][63/408]	Loss 0.0141 (0.0629)	
training:	Epoch: [17][64/408]	Loss 0.0193 (0.0622)	
training:	Epoch: [17][65/408]	Loss 0.0263 (0.0617)	
training:	Epoch: [17][66/408]	Loss 0.0511 (0.0615)	
training:	Epoch: [17][67/408]	Loss 0.0137 (0.0608)	
training:	Epoch: [17][68/408]	Loss 0.0164 (0.0601)	
training:	Epoch: [17][69/408]	Loss 0.0237 (0.0596)	
training:	Epoch: [17][70/408]	Loss 0.2332 (0.0621)	
training:	Epoch: [17][71/408]	Loss 0.0213 (0.0615)	
training:	Epoch: [17][72/408]	Loss 0.0163 (0.0609)	
training:	Epoch: [17][73/408]	Loss 0.2498 (0.0635)	
training:	Epoch: [17][74/408]	Loss 0.0141 (0.0628)	
training:	Epoch: [17][75/408]	Loss 0.0228 (0.0623)	
training:	Epoch: [17][76/408]	Loss 0.0136 (0.0616)	
training:	Epoch: [17][77/408]	Loss 0.2623 (0.0642)	
training:	Epoch: [17][78/408]	Loss 0.0157 (0.0636)	
training:	Epoch: [17][79/408]	Loss 0.2617 (0.0661)	
training:	Epoch: [17][80/408]	Loss 0.0157 (0.0655)	
training:	Epoch: [17][81/408]	Loss 0.0151 (0.0649)	
training:	Epoch: [17][82/408]	Loss 0.2989 (0.0677)	
training:	Epoch: [17][83/408]	Loss 0.0155 (0.0671)	
training:	Epoch: [17][84/408]	Loss 0.2877 (0.0697)	
training:	Epoch: [17][85/408]	Loss 0.0311 (0.0693)	
training:	Epoch: [17][86/408]	Loss 0.0143 (0.0686)	
training:	Epoch: [17][87/408]	Loss 0.2990 (0.0713)	
training:	Epoch: [17][88/408]	Loss 0.0268 (0.0708)	
training:	Epoch: [17][89/408]	Loss 0.0195 (0.0702)	
training:	Epoch: [17][90/408]	Loss 0.0165 (0.0696)	
training:	Epoch: [17][91/408]	Loss 0.0144 (0.0690)	
training:	Epoch: [17][92/408]	Loss 0.0178 (0.0684)	
training:	Epoch: [17][93/408]	Loss 0.0167 (0.0679)	
training:	Epoch: [17][94/408]	Loss 0.0166 (0.0673)	
training:	Epoch: [17][95/408]	Loss 0.0169 (0.0668)	
training:	Epoch: [17][96/408]	Loss 0.0193 (0.0663)	
training:	Epoch: [17][97/408]	Loss 0.0148 (0.0658)	
training:	Epoch: [17][98/408]	Loss 0.0151 (0.0653)	
training:	Epoch: [17][99/408]	Loss 0.0206 (0.0648)	
training:	Epoch: [17][100/408]	Loss 0.0159 (0.0643)	
training:	Epoch: [17][101/408]	Loss 0.0223 (0.0639)	
training:	Epoch: [17][102/408]	Loss 0.2146 (0.0654)	
training:	Epoch: [17][103/408]	Loss 0.0185 (0.0649)	
training:	Epoch: [17][104/408]	Loss 0.2615 (0.0668)	
training:	Epoch: [17][105/408]	Loss 0.0214 (0.0664)	
training:	Epoch: [17][106/408]	Loss 0.0150 (0.0659)	
training:	Epoch: [17][107/408]	Loss 0.0162 (0.0654)	
training:	Epoch: [17][108/408]	Loss 0.0212 (0.0650)	
training:	Epoch: [17][109/408]	Loss 0.0163 (0.0646)	
training:	Epoch: [17][110/408]	Loss 0.0149 (0.0641)	
training:	Epoch: [17][111/408]	Loss 0.0171 (0.0637)	
training:	Epoch: [17][112/408]	Loss 0.0188 (0.0633)	
training:	Epoch: [17][113/408]	Loss 0.0134 (0.0629)	
training:	Epoch: [17][114/408]	Loss 0.0193 (0.0625)	
training:	Epoch: [17][115/408]	Loss 0.5366 (0.0666)	
training:	Epoch: [17][116/408]	Loss 0.0145 (0.0661)	
training:	Epoch: [17][117/408]	Loss 0.0173 (0.0657)	
training:	Epoch: [17][118/408]	Loss 0.0141 (0.0653)	
training:	Epoch: [17][119/408]	Loss 0.0180 (0.0649)	
training:	Epoch: [17][120/408]	Loss 0.0139 (0.0645)	
training:	Epoch: [17][121/408]	Loss 0.0150 (0.0641)	
training:	Epoch: [17][122/408]	Loss 0.0154 (0.0637)	
training:	Epoch: [17][123/408]	Loss 0.0162 (0.0633)	
training:	Epoch: [17][124/408]	Loss 0.0156 (0.0629)	
training:	Epoch: [17][125/408]	Loss 0.0139 (0.0625)	
training:	Epoch: [17][126/408]	Loss 0.0167 (0.0621)	
training:	Epoch: [17][127/408]	Loss 0.0162 (0.0618)	
training:	Epoch: [17][128/408]	Loss 0.0248 (0.0615)	
training:	Epoch: [17][129/408]	Loss 0.0155 (0.0611)	
training:	Epoch: [17][130/408]	Loss 0.0170 (0.0608)	
training:	Epoch: [17][131/408]	Loss 0.0505 (0.0607)	
training:	Epoch: [17][132/408]	Loss 0.0136 (0.0604)	
training:	Epoch: [17][133/408]	Loss 0.2957 (0.0621)	
training:	Epoch: [17][134/408]	Loss 0.0157 (0.0618)	
training:	Epoch: [17][135/408]	Loss 0.0145 (0.0614)	
training:	Epoch: [17][136/408]	Loss 0.2636 (0.0629)	
training:	Epoch: [17][137/408]	Loss 0.0163 (0.0626)	
training:	Epoch: [17][138/408]	Loss 0.0150 (0.0622)	
training:	Epoch: [17][139/408]	Loss 0.4962 (0.0653)	
training:	Epoch: [17][140/408]	Loss 0.0159 (0.0650)	
training:	Epoch: [17][141/408]	Loss 0.0148 (0.0646)	
training:	Epoch: [17][142/408]	Loss 0.0370 (0.0644)	
training:	Epoch: [17][143/408]	Loss 0.2632 (0.0658)	
training:	Epoch: [17][144/408]	Loss 0.0159 (0.0655)	
training:	Epoch: [17][145/408]	Loss 0.0149 (0.0651)	
training:	Epoch: [17][146/408]	Loss 0.2924 (0.0667)	
training:	Epoch: [17][147/408]	Loss 0.5247 (0.0698)	
training:	Epoch: [17][148/408]	Loss 0.0162 (0.0695)	
training:	Epoch: [17][149/408]	Loss 0.0149 (0.0691)	
training:	Epoch: [17][150/408]	Loss 0.0178 (0.0687)	
training:	Epoch: [17][151/408]	Loss 0.0159 (0.0684)	
training:	Epoch: [17][152/408]	Loss 0.0185 (0.0681)	
training:	Epoch: [17][153/408]	Loss 0.2962 (0.0696)	
training:	Epoch: [17][154/408]	Loss 0.0154 (0.0692)	
training:	Epoch: [17][155/408]	Loss 0.2519 (0.0704)	
training:	Epoch: [17][156/408]	Loss 0.0150 (0.0700)	
training:	Epoch: [17][157/408]	Loss 0.5187 (0.0729)	
training:	Epoch: [17][158/408]	Loss 0.0155 (0.0725)	
training:	Epoch: [17][159/408]	Loss 0.0190 (0.0722)	
training:	Epoch: [17][160/408]	Loss 0.3048 (0.0736)	
training:	Epoch: [17][161/408]	Loss 0.0186 (0.0733)	
training:	Epoch: [17][162/408]	Loss 0.0309 (0.0730)	
training:	Epoch: [17][163/408]	Loss 0.3112 (0.0745)	
training:	Epoch: [17][164/408]	Loss 0.2989 (0.0759)	
training:	Epoch: [17][165/408]	Loss 0.0704 (0.0758)	
training:	Epoch: [17][166/408]	Loss 0.0151 (0.0755)	
training:	Epoch: [17][167/408]	Loss 0.2667 (0.0766)	
training:	Epoch: [17][168/408]	Loss 0.0150 (0.0762)	
training:	Epoch: [17][169/408]	Loss 0.0156 (0.0759)	
training:	Epoch: [17][170/408]	Loss 0.2565 (0.0769)	
training:	Epoch: [17][171/408]	Loss 0.0137 (0.0766)	
training:	Epoch: [17][172/408]	Loss 0.2959 (0.0779)	
training:	Epoch: [17][173/408]	Loss 0.0156 (0.0775)	
training:	Epoch: [17][174/408]	Loss 0.0149 (0.0771)	
training:	Epoch: [17][175/408]	Loss 0.0191 (0.0768)	
training:	Epoch: [17][176/408]	Loss 0.0154 (0.0765)	
training:	Epoch: [17][177/408]	Loss 0.0136 (0.0761)	
training:	Epoch: [17][178/408]	Loss 0.0253 (0.0758)	
training:	Epoch: [17][179/408]	Loss 0.0176 (0.0755)	
training:	Epoch: [17][180/408]	Loss 0.0159 (0.0752)	
training:	Epoch: [17][181/408]	Loss 0.0162 (0.0748)	
training:	Epoch: [17][182/408]	Loss 0.0155 (0.0745)	
training:	Epoch: [17][183/408]	Loss 0.0161 (0.0742)	
training:	Epoch: [17][184/408]	Loss 0.0667 (0.0741)	
training:	Epoch: [17][185/408]	Loss 0.0186 (0.0738)	
training:	Epoch: [17][186/408]	Loss 0.2690 (0.0749)	
training:	Epoch: [17][187/408]	Loss 0.0163 (0.0746)	
training:	Epoch: [17][188/408]	Loss 0.0166 (0.0743)	
training:	Epoch: [17][189/408]	Loss 0.0145 (0.0740)	
training:	Epoch: [17][190/408]	Loss 0.0140 (0.0736)	
training:	Epoch: [17][191/408]	Loss 0.0156 (0.0733)	
training:	Epoch: [17][192/408]	Loss 0.2626 (0.0743)	
training:	Epoch: [17][193/408]	Loss 0.0259 (0.0741)	
training:	Epoch: [17][194/408]	Loss 0.2046 (0.0747)	
training:	Epoch: [17][195/408]	Loss 0.0646 (0.0747)	
training:	Epoch: [17][196/408]	Loss 0.0153 (0.0744)	
training:	Epoch: [17][197/408]	Loss 0.0158 (0.0741)	
training:	Epoch: [17][198/408]	Loss 0.0171 (0.0738)	
training:	Epoch: [17][199/408]	Loss 0.2614 (0.0747)	
training:	Epoch: [17][200/408]	Loss 0.0171 (0.0745)	
training:	Epoch: [17][201/408]	Loss 0.0202 (0.0742)	
training:	Epoch: [17][202/408]	Loss 0.0174 (0.0739)	
training:	Epoch: [17][203/408]	Loss 0.0158 (0.0736)	
training:	Epoch: [17][204/408]	Loss 0.2511 (0.0745)	
training:	Epoch: [17][205/408]	Loss 0.0379 (0.0743)	
training:	Epoch: [17][206/408]	Loss 0.0149 (0.0740)	
training:	Epoch: [17][207/408]	Loss 0.0135 (0.0737)	
training:	Epoch: [17][208/408]	Loss 0.0347 (0.0735)	
training:	Epoch: [17][209/408]	Loss 0.0161 (0.0733)	
training:	Epoch: [17][210/408]	Loss 0.3377 (0.0745)	
training:	Epoch: [17][211/408]	Loss 0.0326 (0.0743)	
training:	Epoch: [17][212/408]	Loss 0.0662 (0.0743)	
training:	Epoch: [17][213/408]	Loss 0.3120 (0.0754)	
training:	Epoch: [17][214/408]	Loss 0.0160 (0.0751)	
training:	Epoch: [17][215/408]	Loss 0.1802 (0.0756)	
training:	Epoch: [17][216/408]	Loss 0.0173 (0.0753)	
training:	Epoch: [17][217/408]	Loss 0.0147 (0.0751)	
training:	Epoch: [17][218/408]	Loss 0.0188 (0.0748)	
training:	Epoch: [17][219/408]	Loss 0.2701 (0.0757)	
training:	Epoch: [17][220/408]	Loss 0.0171 (0.0754)	
training:	Epoch: [17][221/408]	Loss 0.0157 (0.0752)	
training:	Epoch: [17][222/408]	Loss 0.0166 (0.0749)	
training:	Epoch: [17][223/408]	Loss 0.0150 (0.0746)	
training:	Epoch: [17][224/408]	Loss 0.0156 (0.0744)	
training:	Epoch: [17][225/408]	Loss 0.0145 (0.0741)	
training:	Epoch: [17][226/408]	Loss 0.0173 (0.0739)	
training:	Epoch: [17][227/408]	Loss 0.0130 (0.0736)	
training:	Epoch: [17][228/408]	Loss 0.0795 (0.0736)	
training:	Epoch: [17][229/408]	Loss 0.0227 (0.0734)	
training:	Epoch: [17][230/408]	Loss 0.0150 (0.0731)	
training:	Epoch: [17][231/408]	Loss 0.0144 (0.0729)	
training:	Epoch: [17][232/408]	Loss 0.0170 (0.0726)	
training:	Epoch: [17][233/408]	Loss 0.0141 (0.0724)	
training:	Epoch: [17][234/408]	Loss 0.0175 (0.0722)	
training:	Epoch: [17][235/408]	Loss 0.0170 (0.0719)	
training:	Epoch: [17][236/408]	Loss 0.0187 (0.0717)	
training:	Epoch: [17][237/408]	Loss 0.0139 (0.0714)	
training:	Epoch: [17][238/408]	Loss 0.0199 (0.0712)	
training:	Epoch: [17][239/408]	Loss 0.0160 (0.0710)	
training:	Epoch: [17][240/408]	Loss 0.0139 (0.0708)	
training:	Epoch: [17][241/408]	Loss 0.0221 (0.0706)	
training:	Epoch: [17][242/408]	Loss 0.0163 (0.0703)	
training:	Epoch: [17][243/408]	Loss 0.0140 (0.0701)	
training:	Epoch: [17][244/408]	Loss 0.0135 (0.0699)	
training:	Epoch: [17][245/408]	Loss 0.0178 (0.0697)	
training:	Epoch: [17][246/408]	Loss 0.0173 (0.0694)	
training:	Epoch: [17][247/408]	Loss 0.0152 (0.0692)	
training:	Epoch: [17][248/408]	Loss 0.2972 (0.0701)	
training:	Epoch: [17][249/408]	Loss 0.0132 (0.0699)	
training:	Epoch: [17][250/408]	Loss 0.1075 (0.0701)	
training:	Epoch: [17][251/408]	Loss 0.2754 (0.0709)	
training:	Epoch: [17][252/408]	Loss 0.2680 (0.0717)	
training:	Epoch: [17][253/408]	Loss 0.0160 (0.0714)	
training:	Epoch: [17][254/408]	Loss 0.0174 (0.0712)	
training:	Epoch: [17][255/408]	Loss 0.0147 (0.0710)	
training:	Epoch: [17][256/408]	Loss 0.0135 (0.0708)	
training:	Epoch: [17][257/408]	Loss 0.0182 (0.0706)	
training:	Epoch: [17][258/408]	Loss 0.0363 (0.0705)	
training:	Epoch: [17][259/408]	Loss 0.0147 (0.0702)	
training:	Epoch: [17][260/408]	Loss 0.0148 (0.0700)	
training:	Epoch: [17][261/408]	Loss 0.2674 (0.0708)	
training:	Epoch: [17][262/408]	Loss 0.0136 (0.0706)	
training:	Epoch: [17][263/408]	Loss 0.0563 (0.0705)	
training:	Epoch: [17][264/408]	Loss 0.0136 (0.0703)	
training:	Epoch: [17][265/408]	Loss 0.3012 (0.0712)	
training:	Epoch: [17][266/408]	Loss 0.0962 (0.0713)	
training:	Epoch: [17][267/408]	Loss 0.0154 (0.0710)	
training:	Epoch: [17][268/408]	Loss 0.0135 (0.0708)	
training:	Epoch: [17][269/408]	Loss 0.0198 (0.0706)	
training:	Epoch: [17][270/408]	Loss 0.2657 (0.0714)	
training:	Epoch: [17][271/408]	Loss 0.0186 (0.0712)	
training:	Epoch: [17][272/408]	Loss 0.2655 (0.0719)	
training:	Epoch: [17][273/408]	Loss 0.2217 (0.0724)	
training:	Epoch: [17][274/408]	Loss 0.0732 (0.0724)	
training:	Epoch: [17][275/408]	Loss 0.2875 (0.0732)	
training:	Epoch: [17][276/408]	Loss 0.0311 (0.0731)	
training:	Epoch: [17][277/408]	Loss 0.0135 (0.0729)	
training:	Epoch: [17][278/408]	Loss 0.0224 (0.0727)	
training:	Epoch: [17][279/408]	Loss 0.0275 (0.0725)	
training:	Epoch: [17][280/408]	Loss 0.0268 (0.0723)	
training:	Epoch: [17][281/408]	Loss 0.2936 (0.0731)	
training:	Epoch: [17][282/408]	Loss 0.2094 (0.0736)	
training:	Epoch: [17][283/408]	Loss 0.0198 (0.0734)	
training:	Epoch: [17][284/408]	Loss 0.0201 (0.0732)	
training:	Epoch: [17][285/408]	Loss 0.0151 (0.0730)	
training:	Epoch: [17][286/408]	Loss 0.0154 (0.0728)	
training:	Epoch: [17][287/408]	Loss 0.0142 (0.0726)	
training:	Epoch: [17][288/408]	Loss 0.0148 (0.0724)	
training:	Epoch: [17][289/408]	Loss 0.2349 (0.0730)	
training:	Epoch: [17][290/408]	Loss 0.1929 (0.0734)	
training:	Epoch: [17][291/408]	Loss 0.2917 (0.0742)	
training:	Epoch: [17][292/408]	Loss 0.0184 (0.0740)	
training:	Epoch: [17][293/408]	Loss 0.0169 (0.0738)	
training:	Epoch: [17][294/408]	Loss 0.0162 (0.0736)	
training:	Epoch: [17][295/408]	Loss 0.0221 (0.0734)	
training:	Epoch: [17][296/408]	Loss 0.0139 (0.0732)	
training:	Epoch: [17][297/408]	Loss 0.0141 (0.0730)	
training:	Epoch: [17][298/408]	Loss 0.0157 (0.0728)	
training:	Epoch: [17][299/408]	Loss 0.0716 (0.0728)	
training:	Epoch: [17][300/408]	Loss 0.2689 (0.0735)	
training:	Epoch: [17][301/408]	Loss 0.0173 (0.0733)	
training:	Epoch: [17][302/408]	Loss 0.0162 (0.0731)	
training:	Epoch: [17][303/408]	Loss 0.0267 (0.0729)	
training:	Epoch: [17][304/408]	Loss 0.0155 (0.0727)	
training:	Epoch: [17][305/408]	Loss 0.0140 (0.0725)	
training:	Epoch: [17][306/408]	Loss 0.0134 (0.0724)	
training:	Epoch: [17][307/408]	Loss 0.0315 (0.0722)	
training:	Epoch: [17][308/408]	Loss 0.0134 (0.0720)	
training:	Epoch: [17][309/408]	Loss 0.0142 (0.0718)	
training:	Epoch: [17][310/408]	Loss 0.0174 (0.0717)	
training:	Epoch: [17][311/408]	Loss 0.1724 (0.0720)	
training:	Epoch: [17][312/408]	Loss 0.0144 (0.0718)	
training:	Epoch: [17][313/408]	Loss 0.0710 (0.0718)	
training:	Epoch: [17][314/408]	Loss 0.1576 (0.0721)	
training:	Epoch: [17][315/408]	Loss 0.0167 (0.0719)	
training:	Epoch: [17][316/408]	Loss 0.0931 (0.0720)	
training:	Epoch: [17][317/408]	Loss 0.1277 (0.0721)	
training:	Epoch: [17][318/408]	Loss 0.0156 (0.0720)	
training:	Epoch: [17][319/408]	Loss 0.0162 (0.0718)	
training:	Epoch: [17][320/408]	Loss 0.0153 (0.0716)	
training:	Epoch: [17][321/408]	Loss 0.0780 (0.0716)	
training:	Epoch: [17][322/408]	Loss 0.1176 (0.0718)	
training:	Epoch: [17][323/408]	Loss 0.2922 (0.0725)	
training:	Epoch: [17][324/408]	Loss 0.0289 (0.0723)	
training:	Epoch: [17][325/408]	Loss 0.0160 (0.0722)	
training:	Epoch: [17][326/408]	Loss 0.0569 (0.0721)	
training:	Epoch: [17][327/408]	Loss 0.5067 (0.0734)	
training:	Epoch: [17][328/408]	Loss 0.0195 (0.0733)	
training:	Epoch: [17][329/408]	Loss 0.0139 (0.0731)	
training:	Epoch: [17][330/408]	Loss 0.0664 (0.0731)	
training:	Epoch: [17][331/408]	Loss 0.0151 (0.0729)	
training:	Epoch: [17][332/408]	Loss 0.2481 (0.0734)	
training:	Epoch: [17][333/408]	Loss 0.0179 (0.0733)	
training:	Epoch: [17][334/408]	Loss 0.0269 (0.0731)	
training:	Epoch: [17][335/408]	Loss 0.0176 (0.0729)	
training:	Epoch: [17][336/408]	Loss 0.0144 (0.0728)	
training:	Epoch: [17][337/408]	Loss 0.0757 (0.0728)	
training:	Epoch: [17][338/408]	Loss 0.0164 (0.0726)	
training:	Epoch: [17][339/408]	Loss 0.0186 (0.0725)	
training:	Epoch: [17][340/408]	Loss 0.0181 (0.0723)	
training:	Epoch: [17][341/408]	Loss 0.0203 (0.0721)	
training:	Epoch: [17][342/408]	Loss 0.0164 (0.0720)	
training:	Epoch: [17][343/408]	Loss 0.0166 (0.0718)	
training:	Epoch: [17][344/408]	Loss 0.0138 (0.0717)	
training:	Epoch: [17][345/408]	Loss 0.0359 (0.0715)	
training:	Epoch: [17][346/408]	Loss 0.2688 (0.0721)	
training:	Epoch: [17][347/408]	Loss 0.0339 (0.0720)	
training:	Epoch: [17][348/408]	Loss 0.0138 (0.0718)	
training:	Epoch: [17][349/408]	Loss 0.0141 (0.0717)	
training:	Epoch: [17][350/408]	Loss 0.0247 (0.0715)	
training:	Epoch: [17][351/408]	Loss 0.0175 (0.0714)	
training:	Epoch: [17][352/408]	Loss 0.2947 (0.0720)	
training:	Epoch: [17][353/408]	Loss 0.0158 (0.0719)	
training:	Epoch: [17][354/408]	Loss 0.0162 (0.0717)	
training:	Epoch: [17][355/408]	Loss 0.0150 (0.0715)	
training:	Epoch: [17][356/408]	Loss 0.0160 (0.0714)	
training:	Epoch: [17][357/408]	Loss 0.0160 (0.0712)	
training:	Epoch: [17][358/408]	Loss 0.0429 (0.0712)	
training:	Epoch: [17][359/408]	Loss 0.0158 (0.0710)	
training:	Epoch: [17][360/408]	Loss 0.0158 (0.0708)	
training:	Epoch: [17][361/408]	Loss 0.0150 (0.0707)	
training:	Epoch: [17][362/408]	Loss 0.2914 (0.0713)	
training:	Epoch: [17][363/408]	Loss 0.4821 (0.0724)	
training:	Epoch: [17][364/408]	Loss 0.0151 (0.0723)	
training:	Epoch: [17][365/408]	Loss 0.0160 (0.0721)	
training:	Epoch: [17][366/408]	Loss 0.0157 (0.0720)	
training:	Epoch: [17][367/408]	Loss 0.0189 (0.0718)	
training:	Epoch: [17][368/408]	Loss 0.0134 (0.0717)	
training:	Epoch: [17][369/408]	Loss 0.2713 (0.0722)	
training:	Epoch: [17][370/408]	Loss 0.0252 (0.0721)	
training:	Epoch: [17][371/408]	Loss 0.0169 (0.0719)	
training:	Epoch: [17][372/408]	Loss 0.0344 (0.0718)	
training:	Epoch: [17][373/408]	Loss 0.0167 (0.0717)	
training:	Epoch: [17][374/408]	Loss 0.3019 (0.0723)	
training:	Epoch: [17][375/408]	Loss 0.0216 (0.0722)	
training:	Epoch: [17][376/408]	Loss 0.1988 (0.0725)	
training:	Epoch: [17][377/408]	Loss 0.2761 (0.0730)	
training:	Epoch: [17][378/408]	Loss 0.0194 (0.0729)	
training:	Epoch: [17][379/408]	Loss 0.0139 (0.0727)	
training:	Epoch: [17][380/408]	Loss 0.0157 (0.0726)	
training:	Epoch: [17][381/408]	Loss 0.0203 (0.0725)	
training:	Epoch: [17][382/408]	Loss 0.0188 (0.0723)	
training:	Epoch: [17][383/408]	Loss 0.0177 (0.0722)	
training:	Epoch: [17][384/408]	Loss 0.0156 (0.0720)	
training:	Epoch: [17][385/408]	Loss 0.0357 (0.0719)	
training:	Epoch: [17][386/408]	Loss 0.0221 (0.0718)	
training:	Epoch: [17][387/408]	Loss 0.0160 (0.0717)	
training:	Epoch: [17][388/408]	Loss 0.0166 (0.0715)	
training:	Epoch: [17][389/408]	Loss 0.0495 (0.0715)	
training:	Epoch: [17][390/408]	Loss 0.2567 (0.0719)	
training:	Epoch: [17][391/408]	Loss 0.0195 (0.0718)	
training:	Epoch: [17][392/408]	Loss 0.0149 (0.0717)	
training:	Epoch: [17][393/408]	Loss 0.2950 (0.0722)	
training:	Epoch: [17][394/408]	Loss 0.0141 (0.0721)	
training:	Epoch: [17][395/408]	Loss 0.0226 (0.0719)	
training:	Epoch: [17][396/408]	Loss 0.2520 (0.0724)	
training:	Epoch: [17][397/408]	Loss 0.0191 (0.0723)	
training:	Epoch: [17][398/408]	Loss 0.0191 (0.0721)	
training:	Epoch: [17][399/408]	Loss 0.0140 (0.0720)	
training:	Epoch: [17][400/408]	Loss 0.1015 (0.0721)	
training:	Epoch: [17][401/408]	Loss 0.2765 (0.0726)	
training:	Epoch: [17][402/408]	Loss 0.0166 (0.0724)	
training:	Epoch: [17][403/408]	Loss 0.3034 (0.0730)	
training:	Epoch: [17][404/408]	Loss 0.0150 (0.0729)	
training:	Epoch: [17][405/408]	Loss 0.0139 (0.0727)	
training:	Epoch: [17][406/408]	Loss 0.0182 (0.0726)	
training:	Epoch: [17][407/408]	Loss 0.0232 (0.0725)	
training:	Epoch: [17][408/408]	Loss 0.0196 (0.0723)	
Training:	 Loss: 0.0722

Training:	 ACC: 0.9819 0.9821 0.9868 0.9770
Validation:	 ACC: 0.7889 0.7919 0.8536 0.7242
Validation:	 Best_BACC: 0.8226 0.8250 0.8772 0.7679
Validation:	 Loss: 0.8186
Pretraining:	Epoch 18/200
----------
training:	Epoch: [18][1/408]	Loss 0.0155 (0.0155)	
training:	Epoch: [18][2/408]	Loss 0.0183 (0.0169)	
training:	Epoch: [18][3/408]	Loss 0.0141 (0.0160)	
training:	Epoch: [18][4/408]	Loss 0.2977 (0.0864)	
training:	Epoch: [18][5/408]	Loss 0.0603 (0.0812)	
training:	Epoch: [18][6/408]	Loss 0.1854 (0.0985)	
training:	Epoch: [18][7/408]	Loss 0.0166 (0.0868)	
training:	Epoch: [18][8/408]	Loss 0.0208 (0.0786)	
training:	Epoch: [18][9/408]	Loss 0.0148 (0.0715)	
training:	Epoch: [18][10/408]	Loss 0.0147 (0.0658)	
training:	Epoch: [18][11/408]	Loss 0.0166 (0.0613)	
training:	Epoch: [18][12/408]	Loss 0.1204 (0.0663)	
training:	Epoch: [18][13/408]	Loss 0.0167 (0.0625)	
training:	Epoch: [18][14/408]	Loss 0.0735 (0.0632)	
training:	Epoch: [18][15/408]	Loss 0.0152 (0.0600)	
training:	Epoch: [18][16/408]	Loss 0.0173 (0.0574)	
training:	Epoch: [18][17/408]	Loss 0.0174 (0.0550)	
training:	Epoch: [18][18/408]	Loss 0.0147 (0.0528)	
training:	Epoch: [18][19/408]	Loss 0.0135 (0.0507)	
training:	Epoch: [18][20/408]	Loss 0.2941 (0.0629)	
training:	Epoch: [18][21/408]	Loss 0.0150 (0.0606)	
training:	Epoch: [18][22/408]	Loss 0.2627 (0.0698)	
training:	Epoch: [18][23/408]	Loss 0.0202 (0.0676)	
training:	Epoch: [18][24/408]	Loss 0.2544 (0.0754)	
training:	Epoch: [18][25/408]	Loss 0.0144 (0.0730)	
training:	Epoch: [18][26/408]	Loss 0.0149 (0.0707)	
training:	Epoch: [18][27/408]	Loss 0.0415 (0.0696)	
training:	Epoch: [18][28/408]	Loss 0.0137 (0.0677)	
training:	Epoch: [18][29/408]	Loss 0.0254 (0.0662)	
training:	Epoch: [18][30/408]	Loss 0.0154 (0.0645)	
training:	Epoch: [18][31/408]	Loss 0.3411 (0.0734)	
training:	Epoch: [18][32/408]	Loss 0.0139 (0.0716)	
training:	Epoch: [18][33/408]	Loss 0.1247 (0.0732)	
training:	Epoch: [18][34/408]	Loss 0.0145 (0.0714)	
training:	Epoch: [18][35/408]	Loss 0.2156 (0.0756)	
training:	Epoch: [18][36/408]	Loss 0.0180 (0.0740)	
training:	Epoch: [18][37/408]	Loss 0.1030 (0.0747)	
training:	Epoch: [18][38/408]	Loss 0.0144 (0.0732)	
training:	Epoch: [18][39/408]	Loss 0.0157 (0.0717)	
training:	Epoch: [18][40/408]	Loss 0.0353 (0.0708)	
training:	Epoch: [18][41/408]	Loss 0.0144 (0.0694)	
training:	Epoch: [18][42/408]	Loss 0.0188 (0.0682)	
training:	Epoch: [18][43/408]	Loss 0.0432 (0.0676)	
training:	Epoch: [18][44/408]	Loss 0.0217 (0.0666)	
training:	Epoch: [18][45/408]	Loss 0.0232 (0.0656)	
training:	Epoch: [18][46/408]	Loss 0.0279 (0.0648)	
training:	Epoch: [18][47/408]	Loss 0.0170 (0.0638)	
training:	Epoch: [18][48/408]	Loss 0.0158 (0.0628)	
training:	Epoch: [18][49/408]	Loss 0.0391 (0.0623)	
training:	Epoch: [18][50/408]	Loss 0.0172 (0.0614)	
training:	Epoch: [18][51/408]	Loss 0.0188 (0.0606)	
training:	Epoch: [18][52/408]	Loss 0.0214 (0.0598)	
training:	Epoch: [18][53/408]	Loss 0.0553 (0.0597)	
training:	Epoch: [18][54/408]	Loss 0.0264 (0.0591)	
training:	Epoch: [18][55/408]	Loss 0.0191 (0.0584)	
training:	Epoch: [18][56/408]	Loss 0.2971 (0.0626)	
training:	Epoch: [18][57/408]	Loss 0.0193 (0.0619)	
training:	Epoch: [18][58/408]	Loss 0.0131 (0.0610)	
training:	Epoch: [18][59/408]	Loss 0.0150 (0.0603)	
training:	Epoch: [18][60/408]	Loss 0.0163 (0.0595)	
training:	Epoch: [18][61/408]	Loss 0.0138 (0.0588)	
training:	Epoch: [18][62/408]	Loss 0.0157 (0.0581)	
training:	Epoch: [18][63/408]	Loss 0.0131 (0.0574)	
training:	Epoch: [18][64/408]	Loss 0.0241 (0.0568)	
training:	Epoch: [18][65/408]	Loss 0.0169 (0.0562)	
training:	Epoch: [18][66/408]	Loss 0.1217 (0.0572)	
training:	Epoch: [18][67/408]	Loss 0.0153 (0.0566)	
training:	Epoch: [18][68/408]	Loss 0.0223 (0.0561)	
training:	Epoch: [18][69/408]	Loss 0.2932 (0.0595)	
training:	Epoch: [18][70/408]	Loss 0.0162 (0.0589)	
training:	Epoch: [18][71/408]	Loss 0.0206 (0.0584)	
training:	Epoch: [18][72/408]	Loss 0.2909 (0.0616)	
training:	Epoch: [18][73/408]	Loss 0.0142 (0.0609)	
training:	Epoch: [18][74/408]	Loss 0.0135 (0.0603)	
training:	Epoch: [18][75/408]	Loss 0.0402 (0.0600)	
training:	Epoch: [18][76/408]	Loss 0.0128 (0.0594)	
training:	Epoch: [18][77/408]	Loss 0.0889 (0.0598)	
training:	Epoch: [18][78/408]	Loss 0.0141 (0.0592)	
training:	Epoch: [18][79/408]	Loss 0.0307 (0.0589)	
training:	Epoch: [18][80/408]	Loss 0.2339 (0.0610)	
training:	Epoch: [18][81/408]	Loss 0.0891 (0.0614)	
training:	Epoch: [18][82/408]	Loss 0.0149 (0.0608)	
training:	Epoch: [18][83/408]	Loss 0.0144 (0.0603)	
training:	Epoch: [18][84/408]	Loss 0.0174 (0.0598)	
training:	Epoch: [18][85/408]	Loss 0.0822 (0.0600)	
training:	Epoch: [18][86/408]	Loss 0.0143 (0.0595)	
training:	Epoch: [18][87/408]	Loss 0.0165 (0.0590)	
training:	Epoch: [18][88/408]	Loss 0.0408 (0.0588)	
training:	Epoch: [18][89/408]	Loss 0.0146 (0.0583)	
training:	Epoch: [18][90/408]	Loss 0.0153 (0.0578)	
training:	Epoch: [18][91/408]	Loss 0.0130 (0.0573)	
training:	Epoch: [18][92/408]	Loss 0.0150 (0.0569)	
training:	Epoch: [18][93/408]	Loss 0.0739 (0.0570)	
training:	Epoch: [18][94/408]	Loss 0.0127 (0.0566)	
training:	Epoch: [18][95/408]	Loss 0.2699 (0.0588)	
training:	Epoch: [18][96/408]	Loss 0.0241 (0.0585)	
training:	Epoch: [18][97/408]	Loss 0.0142 (0.0580)	
training:	Epoch: [18][98/408]	Loss 0.2992 (0.0605)	
training:	Epoch: [18][99/408]	Loss 0.1203 (0.0611)	
training:	Epoch: [18][100/408]	Loss 0.2981 (0.0634)	
training:	Epoch: [18][101/408]	Loss 0.0179 (0.0630)	
training:	Epoch: [18][102/408]	Loss 0.3164 (0.0655)	
training:	Epoch: [18][103/408]	Loss 0.0186 (0.0650)	
training:	Epoch: [18][104/408]	Loss 0.0185 (0.0646)	
training:	Epoch: [18][105/408]	Loss 0.0137 (0.0641)	
training:	Epoch: [18][106/408]	Loss 0.0134 (0.0636)	
training:	Epoch: [18][107/408]	Loss 0.3056 (0.0659)	
training:	Epoch: [18][108/408]	Loss 0.2769 (0.0678)	
training:	Epoch: [18][109/408]	Loss 0.0139 (0.0673)	
training:	Epoch: [18][110/408]	Loss 0.0173 (0.0669)	
training:	Epoch: [18][111/408]	Loss 0.0150 (0.0664)	
training:	Epoch: [18][112/408]	Loss 0.0159 (0.0659)	
training:	Epoch: [18][113/408]	Loss 0.0162 (0.0655)	
training:	Epoch: [18][114/408]	Loss 0.0148 (0.0651)	
training:	Epoch: [18][115/408]	Loss 0.0138 (0.0646)	
training:	Epoch: [18][116/408]	Loss 0.0141 (0.0642)	
training:	Epoch: [18][117/408]	Loss 0.0138 (0.0638)	
training:	Epoch: [18][118/408]	Loss 0.2844 (0.0656)	
training:	Epoch: [18][119/408]	Loss 0.0139 (0.0652)	
training:	Epoch: [18][120/408]	Loss 0.0180 (0.0648)	
training:	Epoch: [18][121/408]	Loss 0.2780 (0.0666)	
training:	Epoch: [18][122/408]	Loss 0.0130 (0.0661)	
training:	Epoch: [18][123/408]	Loss 0.2680 (0.0678)	
training:	Epoch: [18][124/408]	Loss 0.0148 (0.0673)	
training:	Epoch: [18][125/408]	Loss 0.0133 (0.0669)	
training:	Epoch: [18][126/408]	Loss 0.0161 (0.0665)	
training:	Epoch: [18][127/408]	Loss 0.0153 (0.0661)	
training:	Epoch: [18][128/408]	Loss 0.0287 (0.0658)	
training:	Epoch: [18][129/408]	Loss 0.5244 (0.0694)	
training:	Epoch: [18][130/408]	Loss 0.0159 (0.0689)	
training:	Epoch: [18][131/408]	Loss 0.0155 (0.0685)	
training:	Epoch: [18][132/408]	Loss 0.0176 (0.0682)	
training:	Epoch: [18][133/408]	Loss 0.0133 (0.0677)	
training:	Epoch: [18][134/408]	Loss 0.2932 (0.0694)	
training:	Epoch: [18][135/408]	Loss 0.2685 (0.0709)	
training:	Epoch: [18][136/408]	Loss 0.0138 (0.0705)	
training:	Epoch: [18][137/408]	Loss 0.0142 (0.0701)	
training:	Epoch: [18][138/408]	Loss 0.2868 (0.0716)	
training:	Epoch: [18][139/408]	Loss 0.0142 (0.0712)	
training:	Epoch: [18][140/408]	Loss 0.0192 (0.0709)	
training:	Epoch: [18][141/408]	Loss 0.0199 (0.0705)	
training:	Epoch: [18][142/408]	Loss 0.0297 (0.0702)	
training:	Epoch: [18][143/408]	Loss 0.0124 (0.0698)	
training:	Epoch: [18][144/408]	Loss 0.0155 (0.0694)	
training:	Epoch: [18][145/408]	Loss 0.0438 (0.0692)	
training:	Epoch: [18][146/408]	Loss 0.2726 (0.0706)	
training:	Epoch: [18][147/408]	Loss 0.0131 (0.0702)	
training:	Epoch: [18][148/408]	Loss 0.0122 (0.0699)	
training:	Epoch: [18][149/408]	Loss 0.0135 (0.0695)	
training:	Epoch: [18][150/408]	Loss 0.2608 (0.0708)	
training:	Epoch: [18][151/408]	Loss 0.0128 (0.0704)	
training:	Epoch: [18][152/408]	Loss 0.0141 (0.0700)	
training:	Epoch: [18][153/408]	Loss 0.1949 (0.0708)	
training:	Epoch: [18][154/408]	Loss 0.4446 (0.0732)	
training:	Epoch: [18][155/408]	Loss 0.0147 (0.0729)	
training:	Epoch: [18][156/408]	Loss 0.2707 (0.0741)	
training:	Epoch: [18][157/408]	Loss 0.0162 (0.0738)	
training:	Epoch: [18][158/408]	Loss 0.2968 (0.0752)	
training:	Epoch: [18][159/408]	Loss 0.0136 (0.0748)	
training:	Epoch: [18][160/408]	Loss 0.0127 (0.0744)	
training:	Epoch: [18][161/408]	Loss 0.0151 (0.0740)	
training:	Epoch: [18][162/408]	Loss 0.3067 (0.0755)	
training:	Epoch: [18][163/408]	Loss 0.0172 (0.0751)	
training:	Epoch: [18][164/408]	Loss 0.3009 (0.0765)	
training:	Epoch: [18][165/408]	Loss 0.0184 (0.0761)	
training:	Epoch: [18][166/408]	Loss 0.0157 (0.0758)	
training:	Epoch: [18][167/408]	Loss 0.0148 (0.0754)	
training:	Epoch: [18][168/408]	Loss 0.0144 (0.0750)	
training:	Epoch: [18][169/408]	Loss 0.0176 (0.0747)	
training:	Epoch: [18][170/408]	Loss 0.0128 (0.0743)	
training:	Epoch: [18][171/408]	Loss 0.0116 (0.0740)	
training:	Epoch: [18][172/408]	Loss 0.0209 (0.0737)	
training:	Epoch: [18][173/408]	Loss 0.2592 (0.0747)	
training:	Epoch: [18][174/408]	Loss 0.0184 (0.0744)	
training:	Epoch: [18][175/408]	Loss 0.0189 (0.0741)	
training:	Epoch: [18][176/408]	Loss 0.0148 (0.0738)	
training:	Epoch: [18][177/408]	Loss 0.2524 (0.0748)	
training:	Epoch: [18][178/408]	Loss 0.0133 (0.0744)	
training:	Epoch: [18][179/408]	Loss 0.0144 (0.0741)	
training:	Epoch: [18][180/408]	Loss 0.0123 (0.0737)	
training:	Epoch: [18][181/408]	Loss 0.0217 (0.0735)	
training:	Epoch: [18][182/408]	Loss 0.0159 (0.0731)	
training:	Epoch: [18][183/408]	Loss 0.1995 (0.0738)	
training:	Epoch: [18][184/408]	Loss 0.0284 (0.0736)	
training:	Epoch: [18][185/408]	Loss 0.0160 (0.0733)	
training:	Epoch: [18][186/408]	Loss 0.0135 (0.0729)	
training:	Epoch: [18][187/408]	Loss 0.0152 (0.0726)	
training:	Epoch: [18][188/408]	Loss 0.2989 (0.0738)	
training:	Epoch: [18][189/408]	Loss 0.0140 (0.0735)	
training:	Epoch: [18][190/408]	Loss 0.0204 (0.0732)	
training:	Epoch: [18][191/408]	Loss 0.0177 (0.0730)	
training:	Epoch: [18][192/408]	Loss 0.0174 (0.0727)	
training:	Epoch: [18][193/408]	Loss 0.0157 (0.0724)	
training:	Epoch: [18][194/408]	Loss 0.0135 (0.0721)	
training:	Epoch: [18][195/408]	Loss 0.0250 (0.0718)	
training:	Epoch: [18][196/408]	Loss 0.0231 (0.0716)	
training:	Epoch: [18][197/408]	Loss 0.0291 (0.0714)	
training:	Epoch: [18][198/408]	Loss 0.0226 (0.0711)	
training:	Epoch: [18][199/408]	Loss 0.0163 (0.0708)	
training:	Epoch: [18][200/408]	Loss 0.0408 (0.0707)	
training:	Epoch: [18][201/408]	Loss 0.0128 (0.0704)	
training:	Epoch: [18][202/408]	Loss 0.0537 (0.0703)	
training:	Epoch: [18][203/408]	Loss 0.2617 (0.0713)	
training:	Epoch: [18][204/408]	Loss 0.1574 (0.0717)	
training:	Epoch: [18][205/408]	Loss 0.0147 (0.0714)	
training:	Epoch: [18][206/408]	Loss 0.0154 (0.0711)	
training:	Epoch: [18][207/408]	Loss 0.0409 (0.0710)	
training:	Epoch: [18][208/408]	Loss 0.0127 (0.0707)	
training:	Epoch: [18][209/408]	Loss 0.0142 (0.0704)	
training:	Epoch: [18][210/408]	Loss 0.0154 (0.0702)	
training:	Epoch: [18][211/408]	Loss 0.0213 (0.0699)	
training:	Epoch: [18][212/408]	Loss 0.0131 (0.0697)	
training:	Epoch: [18][213/408]	Loss 0.3023 (0.0708)	
training:	Epoch: [18][214/408]	Loss 0.0135 (0.0705)	
training:	Epoch: [18][215/408]	Loss 0.2844 (0.0715)	
training:	Epoch: [18][216/408]	Loss 0.0169 (0.0712)	
training:	Epoch: [18][217/408]	Loss 0.0183 (0.0710)	
training:	Epoch: [18][218/408]	Loss 0.0151 (0.0707)	
training:	Epoch: [18][219/408]	Loss 0.0206 (0.0705)	
training:	Epoch: [18][220/408]	Loss 0.0150 (0.0703)	
training:	Epoch: [18][221/408]	Loss 0.0446 (0.0701)	
training:	Epoch: [18][222/408]	Loss 0.0140 (0.0699)	
training:	Epoch: [18][223/408]	Loss 0.1364 (0.0702)	
training:	Epoch: [18][224/408]	Loss 0.0146 (0.0699)	
training:	Epoch: [18][225/408]	Loss 0.0254 (0.0697)	
training:	Epoch: [18][226/408]	Loss 0.3541 (0.0710)	
training:	Epoch: [18][227/408]	Loss 0.0170 (0.0708)	
training:	Epoch: [18][228/408]	Loss 0.0138 (0.0705)	
training:	Epoch: [18][229/408]	Loss 0.2636 (0.0714)	
training:	Epoch: [18][230/408]	Loss 0.0354 (0.0712)	
training:	Epoch: [18][231/408]	Loss 0.0483 (0.0711)	
training:	Epoch: [18][232/408]	Loss 0.0156 (0.0709)	
training:	Epoch: [18][233/408]	Loss 0.2944 (0.0718)	
training:	Epoch: [18][234/408]	Loss 0.0140 (0.0716)	
training:	Epoch: [18][235/408]	Loss 0.0187 (0.0714)	
training:	Epoch: [18][236/408]	Loss 0.0134 (0.0711)	
training:	Epoch: [18][237/408]	Loss 0.0210 (0.0709)	
training:	Epoch: [18][238/408]	Loss 0.0130 (0.0707)	
training:	Epoch: [18][239/408]	Loss 0.0158 (0.0704)	
training:	Epoch: [18][240/408]	Loss 0.2572 (0.0712)	
training:	Epoch: [18][241/408]	Loss 0.3040 (0.0722)	
training:	Epoch: [18][242/408]	Loss 0.0140 (0.0719)	
training:	Epoch: [18][243/408]	Loss 0.2930 (0.0728)	
training:	Epoch: [18][244/408]	Loss 0.0144 (0.0726)	
training:	Epoch: [18][245/408]	Loss 0.0134 (0.0724)	
training:	Epoch: [18][246/408]	Loss 0.0132 (0.0721)	
training:	Epoch: [18][247/408]	Loss 0.0149 (0.0719)	
training:	Epoch: [18][248/408]	Loss 0.0157 (0.0717)	
training:	Epoch: [18][249/408]	Loss 0.0142 (0.0714)	
training:	Epoch: [18][250/408]	Loss 0.0151 (0.0712)	
training:	Epoch: [18][251/408]	Loss 0.0137 (0.0710)	
training:	Epoch: [18][252/408]	Loss 0.2990 (0.0719)	
training:	Epoch: [18][253/408]	Loss 0.0163 (0.0717)	
training:	Epoch: [18][254/408]	Loss 0.0143 (0.0714)	
training:	Epoch: [18][255/408]	Loss 0.0134 (0.0712)	
training:	Epoch: [18][256/408]	Loss 0.0155 (0.0710)	
training:	Epoch: [18][257/408]	Loss 0.0131 (0.0708)	
training:	Epoch: [18][258/408]	Loss 0.0135 (0.0705)	
training:	Epoch: [18][259/408]	Loss 0.2714 (0.0713)	
training:	Epoch: [18][260/408]	Loss 0.0176 (0.0711)	
training:	Epoch: [18][261/408]	Loss 0.2672 (0.0719)	
training:	Epoch: [18][262/408]	Loss 0.0144 (0.0716)	
training:	Epoch: [18][263/408]	Loss 0.0137 (0.0714)	
training:	Epoch: [18][264/408]	Loss 0.0129 (0.0712)	
training:	Epoch: [18][265/408]	Loss 0.0229 (0.0710)	
training:	Epoch: [18][266/408]	Loss 0.0149 (0.0708)	
training:	Epoch: [18][267/408]	Loss 0.0135 (0.0706)	
training:	Epoch: [18][268/408]	Loss 0.3007 (0.0714)	
training:	Epoch: [18][269/408]	Loss 0.0125 (0.0712)	
training:	Epoch: [18][270/408]	Loss 0.2671 (0.0720)	
training:	Epoch: [18][271/408]	Loss 0.0148 (0.0717)	
training:	Epoch: [18][272/408]	Loss 0.0154 (0.0715)	
training:	Epoch: [18][273/408]	Loss 0.0173 (0.0713)	
training:	Epoch: [18][274/408]	Loss 0.0199 (0.0712)	
training:	Epoch: [18][275/408]	Loss 0.0139 (0.0709)	
training:	Epoch: [18][276/408]	Loss 0.0138 (0.0707)	
training:	Epoch: [18][277/408]	Loss 0.0131 (0.0705)	
training:	Epoch: [18][278/408]	Loss 0.0170 (0.0703)	
training:	Epoch: [18][279/408]	Loss 0.0148 (0.0701)	
training:	Epoch: [18][280/408]	Loss 0.0180 (0.0699)	
training:	Epoch: [18][281/408]	Loss 0.2697 (0.0707)	
training:	Epoch: [18][282/408]	Loss 0.0136 (0.0705)	
training:	Epoch: [18][283/408]	Loss 0.2605 (0.0711)	
training:	Epoch: [18][284/408]	Loss 0.0141 (0.0709)	
training:	Epoch: [18][285/408]	Loss 0.5095 (0.0725)	
training:	Epoch: [18][286/408]	Loss 0.0148 (0.0723)	
training:	Epoch: [18][287/408]	Loss 0.0178 (0.0721)	
training:	Epoch: [18][288/408]	Loss 0.0146 (0.0719)	
training:	Epoch: [18][289/408]	Loss 0.0131 (0.0717)	
training:	Epoch: [18][290/408]	Loss 0.0140 (0.0715)	
training:	Epoch: [18][291/408]	Loss 0.2618 (0.0721)	
training:	Epoch: [18][292/408]	Loss 0.2748 (0.0728)	
training:	Epoch: [18][293/408]	Loss 0.0154 (0.0726)	
training:	Epoch: [18][294/408]	Loss 0.0123 (0.0724)	
training:	Epoch: [18][295/408]	Loss 0.0126 (0.0722)	
training:	Epoch: [18][296/408]	Loss 0.0137 (0.0720)	
training:	Epoch: [18][297/408]	Loss 0.2919 (0.0728)	
training:	Epoch: [18][298/408]	Loss 0.0157 (0.0726)	
training:	Epoch: [18][299/408]	Loss 0.0156 (0.0724)	
training:	Epoch: [18][300/408]	Loss 0.0144 (0.0722)	
training:	Epoch: [18][301/408]	Loss 0.2973 (0.0729)	
training:	Epoch: [18][302/408]	Loss 0.0138 (0.0727)	
training:	Epoch: [18][303/408]	Loss 0.1539 (0.0730)	
training:	Epoch: [18][304/408]	Loss 0.0133 (0.0728)	
training:	Epoch: [18][305/408]	Loss 0.0127 (0.0726)	
training:	Epoch: [18][306/408]	Loss 0.2540 (0.0732)	
training:	Epoch: [18][307/408]	Loss 0.0145 (0.0730)	
training:	Epoch: [18][308/408]	Loss 0.0143 (0.0728)	
training:	Epoch: [18][309/408]	Loss 0.0135 (0.0726)	
training:	Epoch: [18][310/408]	Loss 0.0151 (0.0724)	
training:	Epoch: [18][311/408]	Loss 0.0141 (0.0723)	
training:	Epoch: [18][312/408]	Loss 0.0125 (0.0721)	
training:	Epoch: [18][313/408]	Loss 0.2988 (0.0728)	
training:	Epoch: [18][314/408]	Loss 0.0150 (0.0726)	
training:	Epoch: [18][315/408]	Loss 0.0135 (0.0724)	
training:	Epoch: [18][316/408]	Loss 0.0148 (0.0722)	
training:	Epoch: [18][317/408]	Loss 0.0193 (0.0721)	
training:	Epoch: [18][318/408]	Loss 0.1077 (0.0722)	
training:	Epoch: [18][319/408]	Loss 0.3006 (0.0729)	
training:	Epoch: [18][320/408]	Loss 0.0147 (0.0727)	
training:	Epoch: [18][321/408]	Loss 0.0149 (0.0725)	
training:	Epoch: [18][322/408]	Loss 0.0141 (0.0724)	
training:	Epoch: [18][323/408]	Loss 0.0151 (0.0722)	
training:	Epoch: [18][324/408]	Loss 0.0159 (0.0720)	
training:	Epoch: [18][325/408]	Loss 0.0150 (0.0718)	
training:	Epoch: [18][326/408]	Loss 0.0144 (0.0717)	
training:	Epoch: [18][327/408]	Loss 0.0180 (0.0715)	
training:	Epoch: [18][328/408]	Loss 0.0161 (0.0713)	
training:	Epoch: [18][329/408]	Loss 0.0198 (0.0712)	
training:	Epoch: [18][330/408]	Loss 0.1789 (0.0715)	
training:	Epoch: [18][331/408]	Loss 0.2504 (0.0720)	
training:	Epoch: [18][332/408]	Loss 0.2735 (0.0726)	
training:	Epoch: [18][333/408]	Loss 0.0142 (0.0725)	
training:	Epoch: [18][334/408]	Loss 0.0145 (0.0723)	
training:	Epoch: [18][335/408]	Loss 0.2740 (0.0729)	
training:	Epoch: [18][336/408]	Loss 0.0182 (0.0727)	
training:	Epoch: [18][337/408]	Loss 0.0147 (0.0726)	
training:	Epoch: [18][338/408]	Loss 0.0133 (0.0724)	
training:	Epoch: [18][339/408]	Loss 0.0133 (0.0722)	
training:	Epoch: [18][340/408]	Loss 0.4411 (0.0733)	
training:	Epoch: [18][341/408]	Loss 0.0160 (0.0731)	
training:	Epoch: [18][342/408]	Loss 0.0189 (0.0730)	
training:	Epoch: [18][343/408]	Loss 0.0171 (0.0728)	
training:	Epoch: [18][344/408]	Loss 0.0153 (0.0726)	
training:	Epoch: [18][345/408]	Loss 0.0377 (0.0725)	
training:	Epoch: [18][346/408]	Loss 0.1269 (0.0727)	
training:	Epoch: [18][347/408]	Loss 0.0179 (0.0725)	
training:	Epoch: [18][348/408]	Loss 0.0126 (0.0724)	
training:	Epoch: [18][349/408]	Loss 0.0174 (0.0722)	
training:	Epoch: [18][350/408]	Loss 0.0142 (0.0720)	
training:	Epoch: [18][351/408]	Loss 0.0154 (0.0719)	
training:	Epoch: [18][352/408]	Loss 0.0165 (0.0717)	
training:	Epoch: [18][353/408]	Loss 0.0317 (0.0716)	
training:	Epoch: [18][354/408]	Loss 0.0486 (0.0715)	
training:	Epoch: [18][355/408]	Loss 0.0160 (0.0714)	
training:	Epoch: [18][356/408]	Loss 0.0270 (0.0713)	
training:	Epoch: [18][357/408]	Loss 0.0159 (0.0711)	
training:	Epoch: [18][358/408]	Loss 0.0163 (0.0709)	
training:	Epoch: [18][359/408]	Loss 0.0202 (0.0708)	
training:	Epoch: [18][360/408]	Loss 0.0171 (0.0707)	
training:	Epoch: [18][361/408]	Loss 0.0180 (0.0705)	
training:	Epoch: [18][362/408]	Loss 0.3023 (0.0712)	
training:	Epoch: [18][363/408]	Loss 0.0166 (0.0710)	
training:	Epoch: [18][364/408]	Loss 0.0158 (0.0709)	
training:	Epoch: [18][365/408]	Loss 0.0142 (0.0707)	
training:	Epoch: [18][366/408]	Loss 0.0157 (0.0705)	
training:	Epoch: [18][367/408]	Loss 0.0149 (0.0704)	
training:	Epoch: [18][368/408]	Loss 0.2757 (0.0710)	
training:	Epoch: [18][369/408]	Loss 0.0232 (0.0708)	
training:	Epoch: [18][370/408]	Loss 0.0155 (0.0707)	
training:	Epoch: [18][371/408]	Loss 0.0164 (0.0705)	
training:	Epoch: [18][372/408]	Loss 0.0177 (0.0704)	
training:	Epoch: [18][373/408]	Loss 0.0169 (0.0702)	
training:	Epoch: [18][374/408]	Loss 0.0156 (0.0701)	
training:	Epoch: [18][375/408]	Loss 0.0157 (0.0699)	
training:	Epoch: [18][376/408]	Loss 0.0200 (0.0698)	
training:	Epoch: [18][377/408]	Loss 0.0143 (0.0697)	
training:	Epoch: [18][378/408]	Loss 0.0152 (0.0695)	
training:	Epoch: [18][379/408]	Loss 0.0151 (0.0694)	
training:	Epoch: [18][380/408]	Loss 0.0162 (0.0692)	
training:	Epoch: [18][381/408]	Loss 0.0218 (0.0691)	
training:	Epoch: [18][382/408]	Loss 0.0145 (0.0690)	
training:	Epoch: [18][383/408]	Loss 0.0156 (0.0688)	
training:	Epoch: [18][384/408]	Loss 0.0148 (0.0687)	
training:	Epoch: [18][385/408]	Loss 0.0189 (0.0686)	
training:	Epoch: [18][386/408]	Loss 0.0201 (0.0684)	
training:	Epoch: [18][387/408]	Loss 0.0119 (0.0683)	
training:	Epoch: [18][388/408]	Loss 0.2327 (0.0687)	
training:	Epoch: [18][389/408]	Loss 0.0690 (0.0687)	
training:	Epoch: [18][390/408]	Loss 0.0122 (0.0686)	
training:	Epoch: [18][391/408]	Loss 0.0145 (0.0684)	
training:	Epoch: [18][392/408]	Loss 0.0124 (0.0683)	
training:	Epoch: [18][393/408]	Loss 0.0132 (0.0682)	
training:	Epoch: [18][394/408]	Loss 0.2654 (0.0687)	
training:	Epoch: [18][395/408]	Loss 0.2967 (0.0692)	
training:	Epoch: [18][396/408]	Loss 0.0148 (0.0691)	
training:	Epoch: [18][397/408]	Loss 0.0148 (0.0690)	
training:	Epoch: [18][398/408]	Loss 0.0169 (0.0688)	
training:	Epoch: [18][399/408]	Loss 0.0140 (0.0687)	
training:	Epoch: [18][400/408]	Loss 0.0181 (0.0686)	
training:	Epoch: [18][401/408]	Loss 0.0142 (0.0684)	
training:	Epoch: [18][402/408]	Loss 0.0181 (0.0683)	
training:	Epoch: [18][403/408]	Loss 0.0159 (0.0682)	
training:	Epoch: [18][404/408]	Loss 0.0369 (0.0681)	
training:	Epoch: [18][405/408]	Loss 0.0128 (0.0680)	
training:	Epoch: [18][406/408]	Loss 0.0483 (0.0679)	
training:	Epoch: [18][407/408]	Loss 0.0124 (0.0678)	
training:	Epoch: [18][408/408]	Loss 0.0367 (0.0677)	
Training:	 Loss: 0.0676

Training:	 ACC: 0.9891 0.9890 0.9868 0.9914
Validation:	 ACC: 0.7879 0.7876 0.7810 0.7948
Validation:	 Best_BACC: 0.8226 0.8250 0.8772 0.7679
Validation:	 Loss: 0.7862
Pretraining:	Epoch 19/200
----------
training:	Epoch: [19][1/408]	Loss 0.0142 (0.0142)	
training:	Epoch: [19][2/408]	Loss 0.0136 (0.0139)	
training:	Epoch: [19][3/408]	Loss 0.0466 (0.0248)	
training:	Epoch: [19][4/408]	Loss 0.0182 (0.0232)	
training:	Epoch: [19][5/408]	Loss 0.0141 (0.0214)	
training:	Epoch: [19][6/408]	Loss 0.0168 (0.0206)	
training:	Epoch: [19][7/408]	Loss 0.0132 (0.0195)	
training:	Epoch: [19][8/408]	Loss 0.0143 (0.0189)	
training:	Epoch: [19][9/408]	Loss 0.0134 (0.0183)	
training:	Epoch: [19][10/408]	Loss 0.0146 (0.0179)	
training:	Epoch: [19][11/408]	Loss 0.0168 (0.0178)	
training:	Epoch: [19][12/408]	Loss 0.5495 (0.0621)	
training:	Epoch: [19][13/408]	Loss 0.0161 (0.0586)	
training:	Epoch: [19][14/408]	Loss 0.0130 (0.0553)	
training:	Epoch: [19][15/408]	Loss 0.1044 (0.0586)	
training:	Epoch: [19][16/408]	Loss 0.0128 (0.0557)	
training:	Epoch: [19][17/408]	Loss 0.0142 (0.0533)	
training:	Epoch: [19][18/408]	Loss 0.0185 (0.0514)	
training:	Epoch: [19][19/408]	Loss 0.0128 (0.0493)	
training:	Epoch: [19][20/408]	Loss 0.2392 (0.0588)	
training:	Epoch: [19][21/408]	Loss 0.0644 (0.0591)	
training:	Epoch: [19][22/408]	Loss 0.0145 (0.0571)	
training:	Epoch: [19][23/408]	Loss 0.2795 (0.0667)	
training:	Epoch: [19][24/408]	Loss 0.0229 (0.0649)	
training:	Epoch: [19][25/408]	Loss 0.0135 (0.0628)	
training:	Epoch: [19][26/408]	Loss 0.0162 (0.0611)	
training:	Epoch: [19][27/408]	Loss 0.0127 (0.0593)	
training:	Epoch: [19][28/408]	Loss 0.0134 (0.0576)	
training:	Epoch: [19][29/408]	Loss 0.0159 (0.0562)	
training:	Epoch: [19][30/408]	Loss 0.0129 (0.0547)	
training:	Epoch: [19][31/408]	Loss 0.0137 (0.0534)	
training:	Epoch: [19][32/408]	Loss 0.0153 (0.0522)	
training:	Epoch: [19][33/408]	Loss 0.0154 (0.0511)	
training:	Epoch: [19][34/408]	Loss 0.0272 (0.0504)	
training:	Epoch: [19][35/408]	Loss 0.0183 (0.0495)	
training:	Epoch: [19][36/408]	Loss 0.0117 (0.0484)	
training:	Epoch: [19][37/408]	Loss 0.0124 (0.0475)	
training:	Epoch: [19][38/408]	Loss 0.0140 (0.0466)	
training:	Epoch: [19][39/408]	Loss 0.4606 (0.0572)	
training:	Epoch: [19][40/408]	Loss 0.0167 (0.0562)	
training:	Epoch: [19][41/408]	Loss 0.0146 (0.0552)	
training:	Epoch: [19][42/408]	Loss 0.0122 (0.0542)	
training:	Epoch: [19][43/408]	Loss 0.0132 (0.0532)	
training:	Epoch: [19][44/408]	Loss 0.0132 (0.0523)	
training:	Epoch: [19][45/408]	Loss 0.0131 (0.0514)	
training:	Epoch: [19][46/408]	Loss 0.0173 (0.0507)	
training:	Epoch: [19][47/408]	Loss 0.0132 (0.0499)	
training:	Epoch: [19][48/408]	Loss 0.0136 (0.0491)	
training:	Epoch: [19][49/408]	Loss 0.0126 (0.0484)	
training:	Epoch: [19][50/408]	Loss 0.3002 (0.0534)	
training:	Epoch: [19][51/408]	Loss 0.0744 (0.0538)	
training:	Epoch: [19][52/408]	Loss 0.0561 (0.0539)	
training:	Epoch: [19][53/408]	Loss 0.0149 (0.0531)	
training:	Epoch: [19][54/408]	Loss 0.0136 (0.0524)	
training:	Epoch: [19][55/408]	Loss 0.0134 (0.0517)	
training:	Epoch: [19][56/408]	Loss 0.0140 (0.0510)	
training:	Epoch: [19][57/408]	Loss 0.0144 (0.0504)	
training:	Epoch: [19][58/408]	Loss 0.0147 (0.0498)	
training:	Epoch: [19][59/408]	Loss 0.0128 (0.0491)	
training:	Epoch: [19][60/408]	Loss 0.0124 (0.0485)	
training:	Epoch: [19][61/408]	Loss 0.0112 (0.0479)	
training:	Epoch: [19][62/408]	Loss 0.0151 (0.0474)	
training:	Epoch: [19][63/408]	Loss 0.3136 (0.0516)	
training:	Epoch: [19][64/408]	Loss 0.0164 (0.0511)	
training:	Epoch: [19][65/408]	Loss 0.1752 (0.0530)	
training:	Epoch: [19][66/408]	Loss 0.0217 (0.0525)	
training:	Epoch: [19][67/408]	Loss 0.2715 (0.0558)	
training:	Epoch: [19][68/408]	Loss 0.0168 (0.0552)	
training:	Epoch: [19][69/408]	Loss 0.0130 (0.0546)	
training:	Epoch: [19][70/408]	Loss 0.0132 (0.0540)	
training:	Epoch: [19][71/408]	Loss 0.0134 (0.0534)	
training:	Epoch: [19][72/408]	Loss 0.3045 (0.0569)	
training:	Epoch: [19][73/408]	Loss 0.0138 (0.0563)	
training:	Epoch: [19][74/408]	Loss 0.0144 (0.0558)	
training:	Epoch: [19][75/408]	Loss 0.0131 (0.0552)	
training:	Epoch: [19][76/408]	Loss 0.0160 (0.0547)	
training:	Epoch: [19][77/408]	Loss 0.3057 (0.0579)	
training:	Epoch: [19][78/408]	Loss 0.0508 (0.0578)	
training:	Epoch: [19][79/408]	Loss 0.0168 (0.0573)	
training:	Epoch: [19][80/408]	Loss 0.0417 (0.0571)	
training:	Epoch: [19][81/408]	Loss 0.0126 (0.0566)	
training:	Epoch: [19][82/408]	Loss 0.0202 (0.0561)	
training:	Epoch: [19][83/408]	Loss 0.0135 (0.0556)	
training:	Epoch: [19][84/408]	Loss 0.0225 (0.0552)	
training:	Epoch: [19][85/408]	Loss 0.0138 (0.0547)	
training:	Epoch: [19][86/408]	Loss 0.0137 (0.0543)	
training:	Epoch: [19][87/408]	Loss 0.2688 (0.0567)	
training:	Epoch: [19][88/408]	Loss 0.0139 (0.0562)	
training:	Epoch: [19][89/408]	Loss 0.0231 (0.0559)	
training:	Epoch: [19][90/408]	Loss 0.0266 (0.0555)	
training:	Epoch: [19][91/408]	Loss 0.0130 (0.0551)	
training:	Epoch: [19][92/408]	Loss 0.0167 (0.0547)	
training:	Epoch: [19][93/408]	Loss 0.0149 (0.0542)	
training:	Epoch: [19][94/408]	Loss 0.0125 (0.0538)	
training:	Epoch: [19][95/408]	Loss 0.0142 (0.0534)	
training:	Epoch: [19][96/408]	Loss 0.0129 (0.0529)	
training:	Epoch: [19][97/408]	Loss 0.0132 (0.0525)	
training:	Epoch: [19][98/408]	Loss 0.0132 (0.0521)	
training:	Epoch: [19][99/408]	Loss 0.3038 (0.0547)	
training:	Epoch: [19][100/408]	Loss 0.0140 (0.0543)	
training:	Epoch: [19][101/408]	Loss 0.0153 (0.0539)	
training:	Epoch: [19][102/408]	Loss 0.0153 (0.0535)	
training:	Epoch: [19][103/408]	Loss 0.2730 (0.0556)	
training:	Epoch: [19][104/408]	Loss 0.0119 (0.0552)	
training:	Epoch: [19][105/408]	Loss 0.0138 (0.0548)	
training:	Epoch: [19][106/408]	Loss 0.0126 (0.0544)	
training:	Epoch: [19][107/408]	Loss 0.0142 (0.0540)	
training:	Epoch: [19][108/408]	Loss 0.0422 (0.0539)	
training:	Epoch: [19][109/408]	Loss 0.0125 (0.0536)	
training:	Epoch: [19][110/408]	Loss 0.0142 (0.0532)	
training:	Epoch: [19][111/408]	Loss 0.0121 (0.0528)	
training:	Epoch: [19][112/408]	Loss 0.2842 (0.0549)	
training:	Epoch: [19][113/408]	Loss 0.0132 (0.0545)	
training:	Epoch: [19][114/408]	Loss 0.2921 (0.0566)	
training:	Epoch: [19][115/408]	Loss 0.0262 (0.0563)	
training:	Epoch: [19][116/408]	Loss 0.0237 (0.0561)	
training:	Epoch: [19][117/408]	Loss 0.0122 (0.0557)	
training:	Epoch: [19][118/408]	Loss 0.0139 (0.0553)	
training:	Epoch: [19][119/408]	Loss 0.0140 (0.0550)	
training:	Epoch: [19][120/408]	Loss 0.0170 (0.0547)	
training:	Epoch: [19][121/408]	Loss 0.0126 (0.0543)	
training:	Epoch: [19][122/408]	Loss 0.0133 (0.0540)	
training:	Epoch: [19][123/408]	Loss 0.0124 (0.0536)	
training:	Epoch: [19][124/408]	Loss 0.0376 (0.0535)	
training:	Epoch: [19][125/408]	Loss 0.0138 (0.0532)	
training:	Epoch: [19][126/408]	Loss 0.0157 (0.0529)	
training:	Epoch: [19][127/408]	Loss 0.0137 (0.0526)	
training:	Epoch: [19][128/408]	Loss 0.2778 (0.0544)	
training:	Epoch: [19][129/408]	Loss 0.0158 (0.0541)	
training:	Epoch: [19][130/408]	Loss 0.0126 (0.0537)	
training:	Epoch: [19][131/408]	Loss 0.2755 (0.0554)	
training:	Epoch: [19][132/408]	Loss 0.0128 (0.0551)	
training:	Epoch: [19][133/408]	Loss 0.0358 (0.0550)	
training:	Epoch: [19][134/408]	Loss 0.3936 (0.0575)	
training:	Epoch: [19][135/408]	Loss 0.0187 (0.0572)	
training:	Epoch: [19][136/408]	Loss 0.2557 (0.0587)	
training:	Epoch: [19][137/408]	Loss 0.2567 (0.0601)	
training:	Epoch: [19][138/408]	Loss 0.0139 (0.0598)	
training:	Epoch: [19][139/408]	Loss 0.0195 (0.0595)	
training:	Epoch: [19][140/408]	Loss 0.0128 (0.0591)	
training:	Epoch: [19][141/408]	Loss 0.0190 (0.0589)	
training:	Epoch: [19][142/408]	Loss 0.0333 (0.0587)	
training:	Epoch: [19][143/408]	Loss 0.0136 (0.0584)	
training:	Epoch: [19][144/408]	Loss 0.0119 (0.0580)	
training:	Epoch: [19][145/408]	Loss 0.0122 (0.0577)	
training:	Epoch: [19][146/408]	Loss 0.0124 (0.0574)	
training:	Epoch: [19][147/408]	Loss 0.1558 (0.0581)	
training:	Epoch: [19][148/408]	Loss 0.0268 (0.0579)	
training:	Epoch: [19][149/408]	Loss 0.0125 (0.0576)	
training:	Epoch: [19][150/408]	Loss 0.0124 (0.0573)	
training:	Epoch: [19][151/408]	Loss 0.0129 (0.0570)	
training:	Epoch: [19][152/408]	Loss 0.0132 (0.0567)	
training:	Epoch: [19][153/408]	Loss 0.0132 (0.0564)	
training:	Epoch: [19][154/408]	Loss 0.0138 (0.0561)	
training:	Epoch: [19][155/408]	Loss 0.3210 (0.0578)	
training:	Epoch: [19][156/408]	Loss 0.0252 (0.0576)	
training:	Epoch: [19][157/408]	Loss 0.1220 (0.0580)	
training:	Epoch: [19][158/408]	Loss 0.0207 (0.0578)	
training:	Epoch: [19][159/408]	Loss 0.0158 (0.0575)	
training:	Epoch: [19][160/408]	Loss 0.0131 (0.0573)	
training:	Epoch: [19][161/408]	Loss 0.0160 (0.0570)	
training:	Epoch: [19][162/408]	Loss 0.0265 (0.0568)	
training:	Epoch: [19][163/408]	Loss 0.0137 (0.0565)	
training:	Epoch: [19][164/408]	Loss 0.0126 (0.0563)	
training:	Epoch: [19][165/408]	Loss 0.0121 (0.0560)	
training:	Epoch: [19][166/408]	Loss 0.0124 (0.0558)	
training:	Epoch: [19][167/408]	Loss 0.0131 (0.0555)	
training:	Epoch: [19][168/408]	Loss 0.2759 (0.0568)	
training:	Epoch: [19][169/408]	Loss 0.0139 (0.0566)	
training:	Epoch: [19][170/408]	Loss 0.0272 (0.0564)	
training:	Epoch: [19][171/408]	Loss 0.0126 (0.0561)	
training:	Epoch: [19][172/408]	Loss 0.0119 (0.0559)	
training:	Epoch: [19][173/408]	Loss 0.3848 (0.0578)	
training:	Epoch: [19][174/408]	Loss 0.2918 (0.0591)	
training:	Epoch: [19][175/408]	Loss 0.2446 (0.0602)	
training:	Epoch: [19][176/408]	Loss 0.0139 (0.0599)	
training:	Epoch: [19][177/408]	Loss 0.5431 (0.0626)	
training:	Epoch: [19][178/408]	Loss 0.0121 (0.0624)	
training:	Epoch: [19][179/408]	Loss 0.0932 (0.0625)	
training:	Epoch: [19][180/408]	Loss 0.0166 (0.0623)	
training:	Epoch: [19][181/408]	Loss 0.0119 (0.0620)	
training:	Epoch: [19][182/408]	Loss 0.0128 (0.0617)	
training:	Epoch: [19][183/408]	Loss 0.0136 (0.0615)	
training:	Epoch: [19][184/408]	Loss 0.0145 (0.0612)	
training:	Epoch: [19][185/408]	Loss 0.0151 (0.0610)	
training:	Epoch: [19][186/408]	Loss 0.0130 (0.0607)	
training:	Epoch: [19][187/408]	Loss 0.0134 (0.0604)	
training:	Epoch: [19][188/408]	Loss 0.0155 (0.0602)	
training:	Epoch: [19][189/408]	Loss 0.0137 (0.0600)	
training:	Epoch: [19][190/408]	Loss 0.0128 (0.0597)	
training:	Epoch: [19][191/408]	Loss 0.0135 (0.0595)	
training:	Epoch: [19][192/408]	Loss 0.1876 (0.0601)	
training:	Epoch: [19][193/408]	Loss 0.5652 (0.0628)	
training:	Epoch: [19][194/408]	Loss 0.0158 (0.0625)	
training:	Epoch: [19][195/408]	Loss 0.0137 (0.0623)	
training:	Epoch: [19][196/408]	Loss 0.0688 (0.0623)	
training:	Epoch: [19][197/408]	Loss 0.0204 (0.0621)	
training:	Epoch: [19][198/408]	Loss 0.0154 (0.0618)	
training:	Epoch: [19][199/408]	Loss 0.0180 (0.0616)	
training:	Epoch: [19][200/408]	Loss 0.0120 (0.0614)	
training:	Epoch: [19][201/408]	Loss 0.2731 (0.0624)	
training:	Epoch: [19][202/408]	Loss 0.0132 (0.0622)	
training:	Epoch: [19][203/408]	Loss 0.0120 (0.0619)	
training:	Epoch: [19][204/408]	Loss 0.2982 (0.0631)	
training:	Epoch: [19][205/408]	Loss 0.0127 (0.0629)	
training:	Epoch: [19][206/408]	Loss 0.0112 (0.0626)	
training:	Epoch: [19][207/408]	Loss 0.0129 (0.0624)	
training:	Epoch: [19][208/408]	Loss 0.0753 (0.0624)	
training:	Epoch: [19][209/408]	Loss 0.0152 (0.0622)	
training:	Epoch: [19][210/408]	Loss 0.0157 (0.0620)	
training:	Epoch: [19][211/408]	Loss 0.0184 (0.0618)	
training:	Epoch: [19][212/408]	Loss 0.0143 (0.0615)	
training:	Epoch: [19][213/408]	Loss 0.0157 (0.0613)	
training:	Epoch: [19][214/408]	Loss 0.0138 (0.0611)	
training:	Epoch: [19][215/408]	Loss 0.0583 (0.0611)	
training:	Epoch: [19][216/408]	Loss 0.0118 (0.0609)	
training:	Epoch: [19][217/408]	Loss 0.0132 (0.0606)	
training:	Epoch: [19][218/408]	Loss 0.2803 (0.0617)	
training:	Epoch: [19][219/408]	Loss 0.0142 (0.0614)	
training:	Epoch: [19][220/408]	Loss 0.5545 (0.0637)	
training:	Epoch: [19][221/408]	Loss 0.2909 (0.0647)	
training:	Epoch: [19][222/408]	Loss 0.0117 (0.0645)	
training:	Epoch: [19][223/408]	Loss 0.0183 (0.0643)	
training:	Epoch: [19][224/408]	Loss 0.0207 (0.0641)	
training:	Epoch: [19][225/408]	Loss 0.0129 (0.0638)	
training:	Epoch: [19][226/408]	Loss 0.0126 (0.0636)	
training:	Epoch: [19][227/408]	Loss 0.0157 (0.0634)	
training:	Epoch: [19][228/408]	Loss 0.0125 (0.0632)	
training:	Epoch: [19][229/408]	Loss 0.0139 (0.0630)	
training:	Epoch: [19][230/408]	Loss 0.5281 (0.0650)	
training:	Epoch: [19][231/408]	Loss 0.0128 (0.0648)	
training:	Epoch: [19][232/408]	Loss 0.0140 (0.0645)	
training:	Epoch: [19][233/408]	Loss 0.0129 (0.0643)	
training:	Epoch: [19][234/408]	Loss 0.0153 (0.0641)	
training:	Epoch: [19][235/408]	Loss 0.5614 (0.0662)	
training:	Epoch: [19][236/408]	Loss 0.0142 (0.0660)	
training:	Epoch: [19][237/408]	Loss 0.0130 (0.0658)	
training:	Epoch: [19][238/408]	Loss 0.0141 (0.0656)	
training:	Epoch: [19][239/408]	Loss 0.0172 (0.0654)	
training:	Epoch: [19][240/408]	Loss 0.0138 (0.0651)	
training:	Epoch: [19][241/408]	Loss 0.2275 (0.0658)	
training:	Epoch: [19][242/408]	Loss 0.0136 (0.0656)	
training:	Epoch: [19][243/408]	Loss 0.0163 (0.0654)	
training:	Epoch: [19][244/408]	Loss 0.0130 (0.0652)	
training:	Epoch: [19][245/408]	Loss 0.0137 (0.0650)	
training:	Epoch: [19][246/408]	Loss 0.0209 (0.0648)	
training:	Epoch: [19][247/408]	Loss 0.0119 (0.0646)	
training:	Epoch: [19][248/408]	Loss 0.5412 (0.0665)	
training:	Epoch: [19][249/408]	Loss 0.2353 (0.0672)	
training:	Epoch: [19][250/408]	Loss 0.0151 (0.0670)	
training:	Epoch: [19][251/408]	Loss 0.0126 (0.0668)	
training:	Epoch: [19][252/408]	Loss 0.0126 (0.0665)	
training:	Epoch: [19][253/408]	Loss 0.0122 (0.0663)	
training:	Epoch: [19][254/408]	Loss 0.2564 (0.0671)	
training:	Epoch: [19][255/408]	Loss 0.0159 (0.0669)	
training:	Epoch: [19][256/408]	Loss 0.0163 (0.0667)	
training:	Epoch: [19][257/408]	Loss 0.0147 (0.0665)	
training:	Epoch: [19][258/408]	Loss 0.0124 (0.0663)	
training:	Epoch: [19][259/408]	Loss 0.0126 (0.0661)	
training:	Epoch: [19][260/408]	Loss 0.0138 (0.0659)	
training:	Epoch: [19][261/408]	Loss 0.2732 (0.0667)	
training:	Epoch: [19][262/408]	Loss 0.0155 (0.0665)	
training:	Epoch: [19][263/408]	Loss 0.2585 (0.0672)	
training:	Epoch: [19][264/408]	Loss 0.0136 (0.0670)	
training:	Epoch: [19][265/408]	Loss 0.0140 (0.0668)	
training:	Epoch: [19][266/408]	Loss 0.0160 (0.0666)	
training:	Epoch: [19][267/408]	Loss 0.0163 (0.0664)	
training:	Epoch: [19][268/408]	Loss 0.0162 (0.0662)	
training:	Epoch: [19][269/408]	Loss 0.2221 (0.0668)	
training:	Epoch: [19][270/408]	Loss 0.2415 (0.0674)	
training:	Epoch: [19][271/408]	Loss 0.0342 (0.0673)	
training:	Epoch: [19][272/408]	Loss 0.0145 (0.0671)	
training:	Epoch: [19][273/408]	Loss 0.0276 (0.0670)	
training:	Epoch: [19][274/408]	Loss 0.0144 (0.0668)	
training:	Epoch: [19][275/408]	Loss 0.0177 (0.0666)	
training:	Epoch: [19][276/408]	Loss 0.0140 (0.0664)	
training:	Epoch: [19][277/408]	Loss 0.1154 (0.0666)	
training:	Epoch: [19][278/408]	Loss 0.0125 (0.0664)	
training:	Epoch: [19][279/408]	Loss 0.0136 (0.0662)	
training:	Epoch: [19][280/408]	Loss 0.0137 (0.0660)	
training:	Epoch: [19][281/408]	Loss 0.0192 (0.0659)	
training:	Epoch: [19][282/408]	Loss 0.0469 (0.0658)	
training:	Epoch: [19][283/408]	Loss 0.0134 (0.0656)	
training:	Epoch: [19][284/408]	Loss 0.0128 (0.0654)	
training:	Epoch: [19][285/408]	Loss 0.0139 (0.0652)	
training:	Epoch: [19][286/408]	Loss 0.0199 (0.0651)	
training:	Epoch: [19][287/408]	Loss 0.0653 (0.0651)	
training:	Epoch: [19][288/408]	Loss 0.0139 (0.0649)	
training:	Epoch: [19][289/408]	Loss 0.0140 (0.0647)	
training:	Epoch: [19][290/408]	Loss 0.0119 (0.0645)	
training:	Epoch: [19][291/408]	Loss 0.5664 (0.0663)	
training:	Epoch: [19][292/408]	Loss 0.0158 (0.0661)	
training:	Epoch: [19][293/408]	Loss 0.0129 (0.0659)	
training:	Epoch: [19][294/408]	Loss 0.0197 (0.0658)	
training:	Epoch: [19][295/408]	Loss 0.0227 (0.0656)	
training:	Epoch: [19][296/408]	Loss 0.0159 (0.0654)	
training:	Epoch: [19][297/408]	Loss 0.0134 (0.0653)	
training:	Epoch: [19][298/408]	Loss 0.2967 (0.0661)	
training:	Epoch: [19][299/408]	Loss 0.0197 (0.0659)	
training:	Epoch: [19][300/408]	Loss 0.0250 (0.0658)	
training:	Epoch: [19][301/408]	Loss 0.0135 (0.0656)	
training:	Epoch: [19][302/408]	Loss 0.0128 (0.0654)	
training:	Epoch: [19][303/408]	Loss 0.0206 (0.0653)	
training:	Epoch: [19][304/408]	Loss 0.0163 (0.0651)	
training:	Epoch: [19][305/408]	Loss 0.2900 (0.0658)	
training:	Epoch: [19][306/408]	Loss 0.1919 (0.0663)	
training:	Epoch: [19][307/408]	Loss 0.0136 (0.0661)	
training:	Epoch: [19][308/408]	Loss 0.0130 (0.0659)	
training:	Epoch: [19][309/408]	Loss 0.2750 (0.0666)	
training:	Epoch: [19][310/408]	Loss 0.0130 (0.0664)	
training:	Epoch: [19][311/408]	Loss 0.0288 (0.0663)	
training:	Epoch: [19][312/408]	Loss 0.0337 (0.0662)	
training:	Epoch: [19][313/408]	Loss 0.0158 (0.0660)	
training:	Epoch: [19][314/408]	Loss 0.0183 (0.0659)	
training:	Epoch: [19][315/408]	Loss 0.1070 (0.0660)	
training:	Epoch: [19][316/408]	Loss 0.0163 (0.0658)	
training:	Epoch: [19][317/408]	Loss 0.0143 (0.0657)	
training:	Epoch: [19][318/408]	Loss 0.0951 (0.0658)	
training:	Epoch: [19][319/408]	Loss 0.0161 (0.0656)	
training:	Epoch: [19][320/408]	Loss 0.0136 (0.0655)	
training:	Epoch: [19][321/408]	Loss 0.0141 (0.0653)	
training:	Epoch: [19][322/408]	Loss 0.0156 (0.0651)	
training:	Epoch: [19][323/408]	Loss 0.0142 (0.0650)	
training:	Epoch: [19][324/408]	Loss 0.2995 (0.0657)	
training:	Epoch: [19][325/408]	Loss 0.0167 (0.0656)	
training:	Epoch: [19][326/408]	Loss 0.0767 (0.0656)	
training:	Epoch: [19][327/408]	Loss 0.0157 (0.0654)	
training:	Epoch: [19][328/408]	Loss 0.0118 (0.0653)	
training:	Epoch: [19][329/408]	Loss 0.0125 (0.0651)	
training:	Epoch: [19][330/408]	Loss 0.0150 (0.0650)	
training:	Epoch: [19][331/408]	Loss 0.2975 (0.0657)	
training:	Epoch: [19][332/408]	Loss 0.0133 (0.0655)	
training:	Epoch: [19][333/408]	Loss 0.0121 (0.0653)	
training:	Epoch: [19][334/408]	Loss 0.0166 (0.0652)	
training:	Epoch: [19][335/408]	Loss 0.0123 (0.0650)	
training:	Epoch: [19][336/408]	Loss 0.5292 (0.0664)	
training:	Epoch: [19][337/408]	Loss 0.0142 (0.0663)	
training:	Epoch: [19][338/408]	Loss 0.2797 (0.0669)	
training:	Epoch: [19][339/408]	Loss 0.0176 (0.0668)	
training:	Epoch: [19][340/408]	Loss 0.0162 (0.0666)	
training:	Epoch: [19][341/408]	Loss 0.0240 (0.0665)	
training:	Epoch: [19][342/408]	Loss 0.0131 (0.0663)	
training:	Epoch: [19][343/408]	Loss 0.0136 (0.0662)	
training:	Epoch: [19][344/408]	Loss 0.0150 (0.0660)	
training:	Epoch: [19][345/408]	Loss 0.3112 (0.0667)	
training:	Epoch: [19][346/408]	Loss 0.3002 (0.0674)	
training:	Epoch: [19][347/408]	Loss 0.0155 (0.0673)	
training:	Epoch: [19][348/408]	Loss 0.2089 (0.0677)	
training:	Epoch: [19][349/408]	Loss 0.0140 (0.0675)	
training:	Epoch: [19][350/408]	Loss 0.0112 (0.0674)	
training:	Epoch: [19][351/408]	Loss 0.3010 (0.0680)	
training:	Epoch: [19][352/408]	Loss 0.0121 (0.0679)	
training:	Epoch: [19][353/408]	Loss 0.0160 (0.0677)	
training:	Epoch: [19][354/408]	Loss 0.0113 (0.0676)	
training:	Epoch: [19][355/408]	Loss 0.0122 (0.0674)	
training:	Epoch: [19][356/408]	Loss 0.0201 (0.0673)	
training:	Epoch: [19][357/408]	Loss 0.0135 (0.0671)	
training:	Epoch: [19][358/408]	Loss 0.0269 (0.0670)	
training:	Epoch: [19][359/408]	Loss 0.0131 (0.0669)	
training:	Epoch: [19][360/408]	Loss 0.0141 (0.0667)	
training:	Epoch: [19][361/408]	Loss 0.0157 (0.0666)	
training:	Epoch: [19][362/408]	Loss 0.0147 (0.0664)	
training:	Epoch: [19][363/408]	Loss 0.3032 (0.0671)	
training:	Epoch: [19][364/408]	Loss 0.0131 (0.0669)	
training:	Epoch: [19][365/408]	Loss 0.0170 (0.0668)	
training:	Epoch: [19][366/408]	Loss 0.2662 (0.0673)	
training:	Epoch: [19][367/408]	Loss 0.0140 (0.0672)	
training:	Epoch: [19][368/408]	Loss 0.0175 (0.0671)	
training:	Epoch: [19][369/408]	Loss 0.2411 (0.0675)	
training:	Epoch: [19][370/408]	Loss 0.0134 (0.0674)	
training:	Epoch: [19][371/408]	Loss 0.0137 (0.0672)	
training:	Epoch: [19][372/408]	Loss 0.0269 (0.0671)	
training:	Epoch: [19][373/408]	Loss 0.0136 (0.0670)	
training:	Epoch: [19][374/408]	Loss 0.0162 (0.0668)	
training:	Epoch: [19][375/408]	Loss 0.0119 (0.0667)	
training:	Epoch: [19][376/408]	Loss 0.0130 (0.0666)	
training:	Epoch: [19][377/408]	Loss 0.0134 (0.0664)	
training:	Epoch: [19][378/408]	Loss 0.0128 (0.0663)	
training:	Epoch: [19][379/408]	Loss 0.0245 (0.0662)	
training:	Epoch: [19][380/408]	Loss 0.0164 (0.0660)	
training:	Epoch: [19][381/408]	Loss 0.0129 (0.0659)	
training:	Epoch: [19][382/408]	Loss 0.0140 (0.0658)	
training:	Epoch: [19][383/408]	Loss 0.0135 (0.0656)	
training:	Epoch: [19][384/408]	Loss 0.0151 (0.0655)	
training:	Epoch: [19][385/408]	Loss 0.0115 (0.0653)	
training:	Epoch: [19][386/408]	Loss 0.0138 (0.0652)	
training:	Epoch: [19][387/408]	Loss 0.0162 (0.0651)	
training:	Epoch: [19][388/408]	Loss 0.0141 (0.0650)	
training:	Epoch: [19][389/408]	Loss 0.0180 (0.0648)	
training:	Epoch: [19][390/408]	Loss 0.0133 (0.0647)	
training:	Epoch: [19][391/408]	Loss 0.0125 (0.0646)	
training:	Epoch: [19][392/408]	Loss 0.2831 (0.0651)	
training:	Epoch: [19][393/408]	Loss 0.0127 (0.0650)	
training:	Epoch: [19][394/408]	Loss 0.2680 (0.0655)	
training:	Epoch: [19][395/408]	Loss 0.2996 (0.0661)	
training:	Epoch: [19][396/408]	Loss 0.0128 (0.0660)	
training:	Epoch: [19][397/408]	Loss 0.0131 (0.0658)	
training:	Epoch: [19][398/408]	Loss 0.0189 (0.0657)	
training:	Epoch: [19][399/408]	Loss 0.5363 (0.0669)	
training:	Epoch: [19][400/408]	Loss 0.0136 (0.0668)	
training:	Epoch: [19][401/408]	Loss 0.0115 (0.0666)	
training:	Epoch: [19][402/408]	Loss 0.0141 (0.0665)	
training:	Epoch: [19][403/408]	Loss 0.0121 (0.0664)	
training:	Epoch: [19][404/408]	Loss 0.0121 (0.0662)	
training:	Epoch: [19][405/408]	Loss 0.0118 (0.0661)	
training:	Epoch: [19][406/408]	Loss 0.0410 (0.0660)	
training:	Epoch: [19][407/408]	Loss 0.0151 (0.0659)	
training:	Epoch: [19][408/408]	Loss 0.0157 (0.0658)	
Training:	 Loss: 0.0657

Training:	 ACC: 0.9895 0.9894 0.9879 0.9911
Validation:	 ACC: 0.7866 0.7876 0.8086 0.7646
Validation:	 Best_BACC: 0.8226 0.8250 0.8772 0.7679
Validation:	 Loss: 0.8285
Pretraining:	Epoch 20/200
----------
training:	Epoch: [20][1/408]	Loss 0.2777 (0.2777)	
training:	Epoch: [20][2/408]	Loss 0.0145 (0.1461)	
training:	Epoch: [20][3/408]	Loss 0.0131 (0.1018)	
training:	Epoch: [20][4/408]	Loss 0.0140 (0.0798)	
training:	Epoch: [20][5/408]	Loss 0.0141 (0.0667)	
training:	Epoch: [20][6/408]	Loss 0.2625 (0.0993)	
training:	Epoch: [20][7/408]	Loss 0.0130 (0.0870)	
training:	Epoch: [20][8/408]	Loss 0.3086 (0.1147)	
training:	Epoch: [20][9/408]	Loss 0.0159 (0.1037)	
training:	Epoch: [20][10/408]	Loss 0.0114 (0.0945)	
training:	Epoch: [20][11/408]	Loss 0.0126 (0.0870)	
training:	Epoch: [20][12/408]	Loss 0.2497 (0.1006)	
training:	Epoch: [20][13/408]	Loss 0.1817 (0.1068)	
training:	Epoch: [20][14/408]	Loss 0.0125 (0.1001)	
training:	Epoch: [20][15/408]	Loss 0.0132 (0.0943)	
training:	Epoch: [20][16/408]	Loss 0.0144 (0.0893)	
training:	Epoch: [20][17/408]	Loss 0.0130 (0.0848)	
training:	Epoch: [20][18/408]	Loss 0.0133 (0.0809)	
training:	Epoch: [20][19/408]	Loss 0.0125 (0.0773)	
training:	Epoch: [20][20/408]	Loss 0.1159 (0.0792)	
training:	Epoch: [20][21/408]	Loss 0.0140 (0.0761)	
training:	Epoch: [20][22/408]	Loss 0.0159 (0.0734)	
training:	Epoch: [20][23/408]	Loss 0.2720 (0.0820)	
training:	Epoch: [20][24/408]	Loss 0.0153 (0.0792)	
training:	Epoch: [20][25/408]	Loss 0.2627 (0.0866)	
training:	Epoch: [20][26/408]	Loss 0.2404 (0.0925)	
training:	Epoch: [20][27/408]	Loss 0.0136 (0.0895)	
training:	Epoch: [20][28/408]	Loss 0.0116 (0.0868)	
training:	Epoch: [20][29/408]	Loss 0.3148 (0.0946)	
training:	Epoch: [20][30/408]	Loss 0.0123 (0.0919)	
training:	Epoch: [20][31/408]	Loss 0.0167 (0.0895)	
training:	Epoch: [20][32/408]	Loss 0.0148 (0.0871)	
training:	Epoch: [20][33/408]	Loss 0.0150 (0.0849)	
training:	Epoch: [20][34/408]	Loss 0.3238 (0.0920)	
training:	Epoch: [20][35/408]	Loss 0.0134 (0.0897)	
training:	Epoch: [20][36/408]	Loss 0.0150 (0.0876)	
training:	Epoch: [20][37/408]	Loss 0.0548 (0.0868)	
training:	Epoch: [20][38/408]	Loss 0.0127 (0.0848)	
training:	Epoch: [20][39/408]	Loss 0.0150 (0.0830)	
training:	Epoch: [20][40/408]	Loss 0.0147 (0.0813)	
training:	Epoch: [20][41/408]	Loss 0.0237 (0.0799)	
training:	Epoch: [20][42/408]	Loss 0.0135 (0.0783)	
training:	Epoch: [20][43/408]	Loss 0.0159 (0.0769)	
training:	Epoch: [20][44/408]	Loss 0.0151 (0.0755)	
training:	Epoch: [20][45/408]	Loss 0.0138 (0.0741)	
training:	Epoch: [20][46/408]	Loss 0.0132 (0.0728)	
training:	Epoch: [20][47/408]	Loss 0.2632 (0.0768)	
training:	Epoch: [20][48/408]	Loss 0.0522 (0.0763)	
training:	Epoch: [20][49/408]	Loss 0.3057 (0.0810)	
training:	Epoch: [20][50/408]	Loss 0.0142 (0.0797)	
training:	Epoch: [20][51/408]	Loss 0.0159 (0.0784)	
training:	Epoch: [20][52/408]	Loss 0.0144 (0.0772)	
training:	Epoch: [20][53/408]	Loss 0.0172 (0.0760)	
training:	Epoch: [20][54/408]	Loss 0.0128 (0.0749)	
training:	Epoch: [20][55/408]	Loss 0.0150 (0.0738)	
training:	Epoch: [20][56/408]	Loss 0.0157 (0.0727)	
training:	Epoch: [20][57/408]	Loss 0.0178 (0.0718)	
training:	Epoch: [20][58/408]	Loss 0.2711 (0.0752)	
training:	Epoch: [20][59/408]	Loss 0.0173 (0.0742)	
training:	Epoch: [20][60/408]	Loss 0.2756 (0.0776)	
training:	Epoch: [20][61/408]	Loss 0.0131 (0.0765)	
training:	Epoch: [20][62/408]	Loss 0.0153 (0.0756)	
training:	Epoch: [20][63/408]	Loss 0.0125 (0.0746)	
training:	Epoch: [20][64/408]	Loss 0.0130 (0.0736)	
training:	Epoch: [20][65/408]	Loss 0.3296 (0.0775)	
training:	Epoch: [20][66/408]	Loss 0.0136 (0.0766)	
training:	Epoch: [20][67/408]	Loss 0.0122 (0.0756)	
training:	Epoch: [20][68/408]	Loss 0.0131 (0.0747)	
training:	Epoch: [20][69/408]	Loss 0.0127 (0.0738)	
training:	Epoch: [20][70/408]	Loss 0.5346 (0.0804)	
training:	Epoch: [20][71/408]	Loss 0.0116 (0.0794)	
training:	Epoch: [20][72/408]	Loss 0.0795 (0.0794)	
training:	Epoch: [20][73/408]	Loss 0.2718 (0.0820)	
training:	Epoch: [20][74/408]	Loss 0.0160 (0.0811)	
training:	Epoch: [20][75/408]	Loss 0.0122 (0.0802)	
training:	Epoch: [20][76/408]	Loss 0.0113 (0.0793)	
training:	Epoch: [20][77/408]	Loss 0.0141 (0.0785)	
training:	Epoch: [20][78/408]	Loss 0.0133 (0.0776)	
training:	Epoch: [20][79/408]	Loss 0.2304 (0.0796)	
training:	Epoch: [20][80/408]	Loss 0.0151 (0.0788)	
training:	Epoch: [20][81/408]	Loss 0.2555 (0.0809)	
training:	Epoch: [20][82/408]	Loss 0.0170 (0.0802)	
training:	Epoch: [20][83/408]	Loss 0.0183 (0.0794)	
training:	Epoch: [20][84/408]	Loss 0.0117 (0.0786)	
training:	Epoch: [20][85/408]	Loss 0.0305 (0.0780)	
training:	Epoch: [20][86/408]	Loss 0.0145 (0.0773)	
training:	Epoch: [20][87/408]	Loss 0.2540 (0.0793)	
training:	Epoch: [20][88/408]	Loss 0.0124 (0.0786)	
training:	Epoch: [20][89/408]	Loss 0.0129 (0.0778)	
training:	Epoch: [20][90/408]	Loss 0.0219 (0.0772)	
training:	Epoch: [20][91/408]	Loss 0.1158 (0.0776)	
training:	Epoch: [20][92/408]	Loss 0.2968 (0.0800)	
training:	Epoch: [20][93/408]	Loss 0.2645 (0.0820)	
training:	Epoch: [20][94/408]	Loss 0.2729 (0.0840)	
training:	Epoch: [20][95/408]	Loss 0.0161 (0.0833)	
training:	Epoch: [20][96/408]	Loss 0.2656 (0.0852)	
training:	Epoch: [20][97/408]	Loss 0.2967 (0.0874)	
training:	Epoch: [20][98/408]	Loss 0.0155 (0.0867)	
training:	Epoch: [20][99/408]	Loss 0.0133 (0.0859)	
training:	Epoch: [20][100/408]	Loss 0.0144 (0.0852)	
training:	Epoch: [20][101/408]	Loss 0.0138 (0.0845)	
training:	Epoch: [20][102/408]	Loss 0.0219 (0.0839)	
training:	Epoch: [20][103/408]	Loss 0.0155 (0.0832)	
training:	Epoch: [20][104/408]	Loss 0.0184 (0.0826)	
training:	Epoch: [20][105/408]	Loss 0.0166 (0.0820)	
training:	Epoch: [20][106/408]	Loss 0.0129 (0.0813)	
training:	Epoch: [20][107/408]	Loss 0.2992 (0.0834)	
training:	Epoch: [20][108/408]	Loss 0.1685 (0.0841)	
training:	Epoch: [20][109/408]	Loss 0.4969 (0.0879)	
training:	Epoch: [20][110/408]	Loss 0.0618 (0.0877)	
training:	Epoch: [20][111/408]	Loss 0.0138 (0.0870)	
training:	Epoch: [20][112/408]	Loss 0.0150 (0.0864)	
training:	Epoch: [20][113/408]	Loss 0.0155 (0.0858)	
training:	Epoch: [20][114/408]	Loss 0.0270 (0.0852)	
training:	Epoch: [20][115/408]	Loss 0.0149 (0.0846)	
training:	Epoch: [20][116/408]	Loss 0.0158 (0.0840)	
training:	Epoch: [20][117/408]	Loss 0.0142 (0.0834)	
training:	Epoch: [20][118/408]	Loss 0.0151 (0.0829)	
training:	Epoch: [20][119/408]	Loss 0.0402 (0.0825)	
training:	Epoch: [20][120/408]	Loss 0.0137 (0.0819)	
training:	Epoch: [20][121/408]	Loss 0.0142 (0.0814)	
training:	Epoch: [20][122/408]	Loss 0.0145 (0.0808)	
training:	Epoch: [20][123/408]	Loss 0.0240 (0.0804)	
training:	Epoch: [20][124/408]	Loss 0.3138 (0.0822)	
training:	Epoch: [20][125/408]	Loss 0.0141 (0.0817)	
training:	Epoch: [20][126/408]	Loss 0.0147 (0.0812)	
training:	Epoch: [20][127/408]	Loss 0.0182 (0.0807)	
training:	Epoch: [20][128/408]	Loss 0.0128 (0.0801)	
training:	Epoch: [20][129/408]	Loss 0.0148 (0.0796)	
training:	Epoch: [20][130/408]	Loss 0.0234 (0.0792)	
training:	Epoch: [20][131/408]	Loss 0.0131 (0.0787)	
training:	Epoch: [20][132/408]	Loss 0.4489 (0.0815)	
training:	Epoch: [20][133/408]	Loss 0.0143 (0.0810)	
training:	Epoch: [20][134/408]	Loss 0.0142 (0.0805)	
training:	Epoch: [20][135/408]	Loss 0.0154 (0.0800)	
training:	Epoch: [20][136/408]	Loss 0.0130 (0.0795)	
training:	Epoch: [20][137/408]	Loss 0.0156 (0.0791)	
training:	Epoch: [20][138/408]	Loss 0.0215 (0.0786)	
training:	Epoch: [20][139/408]	Loss 0.0145 (0.0782)	
training:	Epoch: [20][140/408]	Loss 0.0159 (0.0777)	
training:	Epoch: [20][141/408]	Loss 0.0134 (0.0773)	
training:	Epoch: [20][142/408]	Loss 0.0181 (0.0769)	
training:	Epoch: [20][143/408]	Loss 0.2577 (0.0781)	
training:	Epoch: [20][144/408]	Loss 0.0125 (0.0777)	
training:	Epoch: [20][145/408]	Loss 0.0161 (0.0772)	
training:	Epoch: [20][146/408]	Loss 0.0189 (0.0768)	
training:	Epoch: [20][147/408]	Loss 0.0139 (0.0764)	
training:	Epoch: [20][148/408]	Loss 0.0113 (0.0760)	
training:	Epoch: [20][149/408]	Loss 0.0122 (0.0756)	
training:	Epoch: [20][150/408]	Loss 0.0156 (0.0752)	
training:	Epoch: [20][151/408]	Loss 0.0441 (0.0749)	
training:	Epoch: [20][152/408]	Loss 0.0131 (0.0745)	
training:	Epoch: [20][153/408]	Loss 0.0139 (0.0741)	
training:	Epoch: [20][154/408]	Loss 0.0191 (0.0738)	
training:	Epoch: [20][155/408]	Loss 0.0199 (0.0734)	
training:	Epoch: [20][156/408]	Loss 0.0181 (0.0731)	
training:	Epoch: [20][157/408]	Loss 0.2210 (0.0740)	
training:	Epoch: [20][158/408]	Loss 0.3085 (0.0755)	
training:	Epoch: [20][159/408]	Loss 0.0142 (0.0751)	
training:	Epoch: [20][160/408]	Loss 0.2794 (0.0764)	
training:	Epoch: [20][161/408]	Loss 0.0130 (0.0760)	
training:	Epoch: [20][162/408]	Loss 0.0153 (0.0756)	
training:	Epoch: [20][163/408]	Loss 0.0117 (0.0752)	
training:	Epoch: [20][164/408]	Loss 0.0146 (0.0749)	
training:	Epoch: [20][165/408]	Loss 0.0144 (0.0745)	
training:	Epoch: [20][166/408]	Loss 0.2809 (0.0757)	
training:	Epoch: [20][167/408]	Loss 0.0120 (0.0754)	
training:	Epoch: [20][168/408]	Loss 0.0172 (0.0750)	
training:	Epoch: [20][169/408]	Loss 0.0132 (0.0747)	
training:	Epoch: [20][170/408]	Loss 0.0126 (0.0743)	
training:	Epoch: [20][171/408]	Loss 0.0135 (0.0739)	
training:	Epoch: [20][172/408]	Loss 0.0132 (0.0736)	
training:	Epoch: [20][173/408]	Loss 0.0125 (0.0732)	
training:	Epoch: [20][174/408]	Loss 0.0147 (0.0729)	
training:	Epoch: [20][175/408]	Loss 0.0488 (0.0728)	
training:	Epoch: [20][176/408]	Loss 0.0126 (0.0724)	
training:	Epoch: [20][177/408]	Loss 0.0155 (0.0721)	
training:	Epoch: [20][178/408]	Loss 0.0143 (0.0718)	
training:	Epoch: [20][179/408]	Loss 0.2951 (0.0730)	
training:	Epoch: [20][180/408]	Loss 0.0139 (0.0727)	
training:	Epoch: [20][181/408]	Loss 0.0146 (0.0724)	
training:	Epoch: [20][182/408]	Loss 0.0147 (0.0720)	
training:	Epoch: [20][183/408]	Loss 0.0135 (0.0717)	
training:	Epoch: [20][184/408]	Loss 0.0123 (0.0714)	
training:	Epoch: [20][185/408]	Loss 0.0307 (0.0712)	
training:	Epoch: [20][186/408]	Loss 0.0121 (0.0709)	
training:	Epoch: [20][187/408]	Loss 0.0289 (0.0706)	
training:	Epoch: [20][188/408]	Loss 0.0180 (0.0704)	
training:	Epoch: [20][189/408]	Loss 0.0131 (0.0701)	
training:	Epoch: [20][190/408]	Loss 0.2862 (0.0712)	
training:	Epoch: [20][191/408]	Loss 0.5999 (0.0740)	
training:	Epoch: [20][192/408]	Loss 0.0134 (0.0736)	
training:	Epoch: [20][193/408]	Loss 0.0122 (0.0733)	
training:	Epoch: [20][194/408]	Loss 0.0150 (0.0730)	
training:	Epoch: [20][195/408]	Loss 0.0223 (0.0728)	
training:	Epoch: [20][196/408]	Loss 0.0127 (0.0725)	
training:	Epoch: [20][197/408]	Loss 0.0117 (0.0722)	
training:	Epoch: [20][198/408]	Loss 0.0156 (0.0719)	
training:	Epoch: [20][199/408]	Loss 0.0144 (0.0716)	
training:	Epoch: [20][200/408]	Loss 0.0130 (0.0713)	
training:	Epoch: [20][201/408]	Loss 0.0144 (0.0710)	
training:	Epoch: [20][202/408]	Loss 0.0119 (0.0707)	
training:	Epoch: [20][203/408]	Loss 0.0129 (0.0704)	
training:	Epoch: [20][204/408]	Loss 0.0125 (0.0701)	
training:	Epoch: [20][205/408]	Loss 0.0141 (0.0699)	
training:	Epoch: [20][206/408]	Loss 0.2987 (0.0710)	
training:	Epoch: [20][207/408]	Loss 0.0147 (0.0707)	
training:	Epoch: [20][208/408]	Loss 0.0263 (0.0705)	
training:	Epoch: [20][209/408]	Loss 0.5245 (0.0727)	
training:	Epoch: [20][210/408]	Loss 0.2635 (0.0736)	
training:	Epoch: [20][211/408]	Loss 0.0150 (0.0733)	
training:	Epoch: [20][212/408]	Loss 0.0155 (0.0730)	
training:	Epoch: [20][213/408]	Loss 0.0498 (0.0729)	
training:	Epoch: [20][214/408]	Loss 0.0130 (0.0726)	
training:	Epoch: [20][215/408]	Loss 0.0154 (0.0724)	
training:	Epoch: [20][216/408]	Loss 0.0143 (0.0721)	
training:	Epoch: [20][217/408]	Loss 0.0161 (0.0718)	
training:	Epoch: [20][218/408]	Loss 0.1488 (0.0722)	
training:	Epoch: [20][219/408]	Loss 0.2973 (0.0732)	
training:	Epoch: [20][220/408]	Loss 0.0137 (0.0730)	
training:	Epoch: [20][221/408]	Loss 0.0135 (0.0727)	
training:	Epoch: [20][222/408]	Loss 0.0127 (0.0724)	
training:	Epoch: [20][223/408]	Loss 0.0119 (0.0721)	
training:	Epoch: [20][224/408]	Loss 0.0123 (0.0719)	
training:	Epoch: [20][225/408]	Loss 0.0133 (0.0716)	
training:	Epoch: [20][226/408]	Loss 0.0383 (0.0715)	
training:	Epoch: [20][227/408]	Loss 0.0278 (0.0713)	
training:	Epoch: [20][228/408]	Loss 0.0358 (0.0711)	
training:	Epoch: [20][229/408]	Loss 0.0111 (0.0709)	
training:	Epoch: [20][230/408]	Loss 0.0153 (0.0706)	
training:	Epoch: [20][231/408]	Loss 0.0167 (0.0704)	
training:	Epoch: [20][232/408]	Loss 0.0121 (0.0701)	
training:	Epoch: [20][233/408]	Loss 0.0141 (0.0699)	
training:	Epoch: [20][234/408]	Loss 0.0134 (0.0696)	
training:	Epoch: [20][235/408]	Loss 0.0124 (0.0694)	
training:	Epoch: [20][236/408]	Loss 0.0128 (0.0692)	
training:	Epoch: [20][237/408]	Loss 0.0260 (0.0690)	
training:	Epoch: [20][238/408]	Loss 0.2797 (0.0699)	
training:	Epoch: [20][239/408]	Loss 0.0120 (0.0696)	
training:	Epoch: [20][240/408]	Loss 0.0137 (0.0694)	
training:	Epoch: [20][241/408]	Loss 0.3030 (0.0704)	
training:	Epoch: [20][242/408]	Loss 0.0132 (0.0701)	
training:	Epoch: [20][243/408]	Loss 0.0125 (0.0699)	
training:	Epoch: [20][244/408]	Loss 0.0125 (0.0697)	
training:	Epoch: [20][245/408]	Loss 0.0131 (0.0694)	
training:	Epoch: [20][246/408]	Loss 0.0404 (0.0693)	
training:	Epoch: [20][247/408]	Loss 0.0119 (0.0691)	
training:	Epoch: [20][248/408]	Loss 0.0181 (0.0689)	
training:	Epoch: [20][249/408]	Loss 0.0487 (0.0688)	
training:	Epoch: [20][250/408]	Loss 0.2982 (0.0697)	
training:	Epoch: [20][251/408]	Loss 0.1487 (0.0700)	
training:	Epoch: [20][252/408]	Loss 0.0233 (0.0698)	
training:	Epoch: [20][253/408]	Loss 0.0266 (0.0697)	
training:	Epoch: [20][254/408]	Loss 0.0118 (0.0694)	
training:	Epoch: [20][255/408]	Loss 0.0154 (0.0692)	
training:	Epoch: [20][256/408]	Loss 0.0132 (0.0690)	
training:	Epoch: [20][257/408]	Loss 0.0133 (0.0688)	
training:	Epoch: [20][258/408]	Loss 0.0351 (0.0687)	
training:	Epoch: [20][259/408]	Loss 0.0385 (0.0685)	
training:	Epoch: [20][260/408]	Loss 0.0141 (0.0683)	
training:	Epoch: [20][261/408]	Loss 0.0151 (0.0681)	
training:	Epoch: [20][262/408]	Loss 0.0180 (0.0679)	
training:	Epoch: [20][263/408]	Loss 0.0126 (0.0677)	
training:	Epoch: [20][264/408]	Loss 0.0120 (0.0675)	
training:	Epoch: [20][265/408]	Loss 0.0132 (0.0673)	
training:	Epoch: [20][266/408]	Loss 0.0139 (0.0671)	
training:	Epoch: [20][267/408]	Loss 0.0148 (0.0669)	
training:	Epoch: [20][268/408]	Loss 0.0149 (0.0667)	
training:	Epoch: [20][269/408]	Loss 0.0191 (0.0665)	
training:	Epoch: [20][270/408]	Loss 0.0138 (0.0663)	
training:	Epoch: [20][271/408]	Loss 0.0111 (0.0661)	
training:	Epoch: [20][272/408]	Loss 0.0123 (0.0659)	
training:	Epoch: [20][273/408]	Loss 0.0184 (0.0658)	
training:	Epoch: [20][274/408]	Loss 0.0726 (0.0658)	
training:	Epoch: [20][275/408]	Loss 0.0129 (0.0656)	
training:	Epoch: [20][276/408]	Loss 0.0139 (0.0654)	
training:	Epoch: [20][277/408]	Loss 0.0131 (0.0652)	
training:	Epoch: [20][278/408]	Loss 0.0122 (0.0650)	
training:	Epoch: [20][279/408]	Loss 0.0229 (0.0649)	
training:	Epoch: [20][280/408]	Loss 0.2987 (0.0657)	
training:	Epoch: [20][281/408]	Loss 0.5561 (0.0675)	
training:	Epoch: [20][282/408]	Loss 0.0124 (0.0673)	
training:	Epoch: [20][283/408]	Loss 0.0131 (0.0671)	
training:	Epoch: [20][284/408]	Loss 0.0165 (0.0669)	
training:	Epoch: [20][285/408]	Loss 0.2864 (0.0677)	
training:	Epoch: [20][286/408]	Loss 0.0118 (0.0675)	
training:	Epoch: [20][287/408]	Loss 0.0131 (0.0673)	
training:	Epoch: [20][288/408]	Loss 0.0127 (0.0671)	
training:	Epoch: [20][289/408]	Loss 0.0115 (0.0669)	
training:	Epoch: [20][290/408]	Loss 0.0122 (0.0667)	
training:	Epoch: [20][291/408]	Loss 0.0136 (0.0665)	
training:	Epoch: [20][292/408]	Loss 0.0136 (0.0664)	
training:	Epoch: [20][293/408]	Loss 0.0202 (0.0662)	
training:	Epoch: [20][294/408]	Loss 0.0110 (0.0660)	
training:	Epoch: [20][295/408]	Loss 0.0136 (0.0658)	
training:	Epoch: [20][296/408]	Loss 0.0117 (0.0656)	
training:	Epoch: [20][297/408]	Loss 0.0134 (0.0655)	
training:	Epoch: [20][298/408]	Loss 0.0135 (0.0653)	
training:	Epoch: [20][299/408]	Loss 0.0134 (0.0651)	
training:	Epoch: [20][300/408]	Loss 0.0127 (0.0649)	
training:	Epoch: [20][301/408]	Loss 0.2977 (0.0657)	
training:	Epoch: [20][302/408]	Loss 0.0137 (0.0655)	
training:	Epoch: [20][303/408]	Loss 0.0195 (0.0654)	
training:	Epoch: [20][304/408]	Loss 0.0122 (0.0652)	
training:	Epoch: [20][305/408]	Loss 0.0131 (0.0650)	
training:	Epoch: [20][306/408]	Loss 0.0133 (0.0649)	
training:	Epoch: [20][307/408]	Loss 0.0135 (0.0647)	
training:	Epoch: [20][308/408]	Loss 0.0135 (0.0645)	
training:	Epoch: [20][309/408]	Loss 0.0129 (0.0644)	
training:	Epoch: [20][310/408]	Loss 0.0127 (0.0642)	
training:	Epoch: [20][311/408]	Loss 0.0136 (0.0640)	
training:	Epoch: [20][312/408]	Loss 0.0125 (0.0639)	
training:	Epoch: [20][313/408]	Loss 0.0300 (0.0638)	
training:	Epoch: [20][314/408]	Loss 0.0169 (0.0636)	
training:	Epoch: [20][315/408]	Loss 0.0113 (0.0635)	
training:	Epoch: [20][316/408]	Loss 0.0142 (0.0633)	
training:	Epoch: [20][317/408]	Loss 0.0125 (0.0631)	
training:	Epoch: [20][318/408]	Loss 0.0109 (0.0630)	
training:	Epoch: [20][319/408]	Loss 0.0141 (0.0628)	
training:	Epoch: [20][320/408]	Loss 0.0216 (0.0627)	
training:	Epoch: [20][321/408]	Loss 0.0240 (0.0626)	
training:	Epoch: [20][322/408]	Loss 0.0114 (0.0624)	
training:	Epoch: [20][323/408]	Loss 0.0173 (0.0623)	
training:	Epoch: [20][324/408]	Loss 0.0115 (0.0621)	
training:	Epoch: [20][325/408]	Loss 0.0126 (0.0620)	
training:	Epoch: [20][326/408]	Loss 0.4084 (0.0630)	
training:	Epoch: [20][327/408]	Loss 0.0119 (0.0629)	
training:	Epoch: [20][328/408]	Loss 0.0128 (0.0627)	
training:	Epoch: [20][329/408]	Loss 0.0131 (0.0626)	
training:	Epoch: [20][330/408]	Loss 0.0122 (0.0624)	
training:	Epoch: [20][331/408]	Loss 0.0149 (0.0623)	
training:	Epoch: [20][332/408]	Loss 0.0218 (0.0622)	
training:	Epoch: [20][333/408]	Loss 0.0114 (0.0620)	
training:	Epoch: [20][334/408]	Loss 0.3175 (0.0628)	
training:	Epoch: [20][335/408]	Loss 0.0167 (0.0626)	
training:	Epoch: [20][336/408]	Loss 0.0118 (0.0625)	
training:	Epoch: [20][337/408]	Loss 0.1673 (0.0628)	
training:	Epoch: [20][338/408]	Loss 0.0129 (0.0626)	
training:	Epoch: [20][339/408]	Loss 0.3111 (0.0634)	
training:	Epoch: [20][340/408]	Loss 0.0111 (0.0632)	
training:	Epoch: [20][341/408]	Loss 0.0678 (0.0632)	
training:	Epoch: [20][342/408]	Loss 0.0124 (0.0631)	
training:	Epoch: [20][343/408]	Loss 0.0122 (0.0629)	
training:	Epoch: [20][344/408]	Loss 0.0110 (0.0628)	
training:	Epoch: [20][345/408]	Loss 0.0170 (0.0627)	
training:	Epoch: [20][346/408]	Loss 0.0146 (0.0625)	
training:	Epoch: [20][347/408]	Loss 0.0119 (0.0624)	
training:	Epoch: [20][348/408]	Loss 0.0133 (0.0622)	
training:	Epoch: [20][349/408]	Loss 0.0119 (0.0621)	
training:	Epoch: [20][350/408]	Loss 0.0260 (0.0620)	
training:	Epoch: [20][351/408]	Loss 0.0158 (0.0618)	
training:	Epoch: [20][352/408]	Loss 0.0138 (0.0617)	
training:	Epoch: [20][353/408]	Loss 0.2781 (0.0623)	
training:	Epoch: [20][354/408]	Loss 0.3002 (0.0630)	
training:	Epoch: [20][355/408]	Loss 0.0125 (0.0629)	
training:	Epoch: [20][356/408]	Loss 0.0137 (0.0627)	
training:	Epoch: [20][357/408]	Loss 0.0118 (0.0626)	
training:	Epoch: [20][358/408]	Loss 0.0647 (0.0626)	
training:	Epoch: [20][359/408]	Loss 0.0187 (0.0625)	
training:	Epoch: [20][360/408]	Loss 0.0131 (0.0623)	
training:	Epoch: [20][361/408]	Loss 0.0130 (0.0622)	
training:	Epoch: [20][362/408]	Loss 0.2723 (0.0628)	
training:	Epoch: [20][363/408]	Loss 0.0142 (0.0626)	
training:	Epoch: [20][364/408]	Loss 0.0125 (0.0625)	
training:	Epoch: [20][365/408]	Loss 0.0148 (0.0624)	
training:	Epoch: [20][366/408]	Loss 0.0122 (0.0622)	
training:	Epoch: [20][367/408]	Loss 0.0160 (0.0621)	
training:	Epoch: [20][368/408]	Loss 0.0124 (0.0620)	
training:	Epoch: [20][369/408]	Loss 0.0148 (0.0618)	
training:	Epoch: [20][370/408]	Loss 0.0396 (0.0618)	
training:	Epoch: [20][371/408]	Loss 0.0115 (0.0616)	
training:	Epoch: [20][372/408]	Loss 0.0127 (0.0615)	
training:	Epoch: [20][373/408]	Loss 0.0149 (0.0614)	
training:	Epoch: [20][374/408]	Loss 0.0123 (0.0613)	
training:	Epoch: [20][375/408]	Loss 0.0117 (0.0611)	
training:	Epoch: [20][376/408]	Loss 0.0131 (0.0610)	
training:	Epoch: [20][377/408]	Loss 0.0130 (0.0609)	
training:	Epoch: [20][378/408]	Loss 0.1738 (0.0612)	
training:	Epoch: [20][379/408]	Loss 0.0127 (0.0610)	
training:	Epoch: [20][380/408]	Loss 0.0125 (0.0609)	
training:	Epoch: [20][381/408]	Loss 0.0445 (0.0609)	
training:	Epoch: [20][382/408]	Loss 0.0125 (0.0607)	
training:	Epoch: [20][383/408]	Loss 0.2818 (0.0613)	
training:	Epoch: [20][384/408]	Loss 0.0240 (0.0612)	
training:	Epoch: [20][385/408]	Loss 0.2796 (0.0618)	
training:	Epoch: [20][386/408]	Loss 0.0388 (0.0617)	
training:	Epoch: [20][387/408]	Loss 0.0131 (0.0616)	
training:	Epoch: [20][388/408]	Loss 0.0118 (0.0615)	
training:	Epoch: [20][389/408]	Loss 0.0139 (0.0613)	
training:	Epoch: [20][390/408]	Loss 0.0112 (0.0612)	
training:	Epoch: [20][391/408]	Loss 0.2893 (0.0618)	
training:	Epoch: [20][392/408]	Loss 0.0116 (0.0617)	
training:	Epoch: [20][393/408]	Loss 0.0139 (0.0616)	
training:	Epoch: [20][394/408]	Loss 0.0503 (0.0615)	
training:	Epoch: [20][395/408]	Loss 0.0330 (0.0615)	
training:	Epoch: [20][396/408]	Loss 0.0168 (0.0613)	
training:	Epoch: [20][397/408]	Loss 0.0246 (0.0612)	
training:	Epoch: [20][398/408]	Loss 0.2872 (0.0618)	
training:	Epoch: [20][399/408]	Loss 0.0118 (0.0617)	
training:	Epoch: [20][400/408]	Loss 0.0491 (0.0617)	
training:	Epoch: [20][401/408]	Loss 0.0132 (0.0615)	
training:	Epoch: [20][402/408]	Loss 0.0122 (0.0614)	
training:	Epoch: [20][403/408]	Loss 0.0632 (0.0614)	
training:	Epoch: [20][404/408]	Loss 0.0139 (0.0613)	
training:	Epoch: [20][405/408]	Loss 0.0114 (0.0612)	
training:	Epoch: [20][406/408]	Loss 0.0118 (0.0611)	
training:	Epoch: [20][407/408]	Loss 0.0214 (0.0610)	
training:	Epoch: [20][408/408]	Loss 0.0118 (0.0608)	
Training:	 Loss: 0.0607

Training:	 ACC: 0.9894 0.9894 0.9894 0.9895
Validation:	 ACC: 0.7839 0.7860 0.8301 0.7377
Validation:	 Best_BACC: 0.8226 0.8250 0.8772 0.7679
Validation:	 Loss: 0.8433
Pretraining:	Epoch 21/200
----------
training:	Epoch: [21][1/408]	Loss 0.0119 (0.0119)	
training:	Epoch: [21][2/408]	Loss 0.0117 (0.0118)	
training:	Epoch: [21][3/408]	Loss 0.0136 (0.0124)	
training:	Epoch: [21][4/408]	Loss 0.0116 (0.0122)	
training:	Epoch: [21][5/408]	Loss 0.0122 (0.0122)	
training:	Epoch: [21][6/408]	Loss 0.0122 (0.0122)	
training:	Epoch: [21][7/408]	Loss 0.0119 (0.0122)	
training:	Epoch: [21][8/408]	Loss 0.0129 (0.0123)	
training:	Epoch: [21][9/408]	Loss 0.3947 (0.0548)	
training:	Epoch: [21][10/408]	Loss 0.0269 (0.0520)	
training:	Epoch: [21][11/408]	Loss 0.0114 (0.0483)	
training:	Epoch: [21][12/408]	Loss 0.0127 (0.0453)	
training:	Epoch: [21][13/408]	Loss 0.2416 (0.0604)	
training:	Epoch: [21][14/408]	Loss 0.2947 (0.0772)	
training:	Epoch: [21][15/408]	Loss 0.0118 (0.0728)	
training:	Epoch: [21][16/408]	Loss 0.0120 (0.0690)	
training:	Epoch: [21][17/408]	Loss 0.0128 (0.0657)	
training:	Epoch: [21][18/408]	Loss 0.2284 (0.0747)	
training:	Epoch: [21][19/408]	Loss 0.0320 (0.0725)	
training:	Epoch: [21][20/408]	Loss 0.0246 (0.0701)	
training:	Epoch: [21][21/408]	Loss 0.0132 (0.0674)	
training:	Epoch: [21][22/408]	Loss 0.2833 (0.0772)	
training:	Epoch: [21][23/408]	Loss 0.0116 (0.0743)	
training:	Epoch: [21][24/408]	Loss 0.0141 (0.0718)	
training:	Epoch: [21][25/408]	Loss 0.2832 (0.0803)	
training:	Epoch: [21][26/408]	Loss 0.0107 (0.0776)	
training:	Epoch: [21][27/408]	Loss 0.0112 (0.0752)	
training:	Epoch: [21][28/408]	Loss 0.0151 (0.0730)	
training:	Epoch: [21][29/408]	Loss 0.0132 (0.0709)	
training:	Epoch: [21][30/408]	Loss 0.1859 (0.0748)	
training:	Epoch: [21][31/408]	Loss 0.0146 (0.0728)	
training:	Epoch: [21][32/408]	Loss 0.0115 (0.0709)	
training:	Epoch: [21][33/408]	Loss 0.0144 (0.0692)	
training:	Epoch: [21][34/408]	Loss 0.0107 (0.0675)	
training:	Epoch: [21][35/408]	Loss 0.0156 (0.0660)	
training:	Epoch: [21][36/408]	Loss 0.0271 (0.0649)	
training:	Epoch: [21][37/408]	Loss 0.0129 (0.0635)	
training:	Epoch: [21][38/408]	Loss 0.3111 (0.0700)	
training:	Epoch: [21][39/408]	Loss 0.0130 (0.0686)	
training:	Epoch: [21][40/408]	Loss 0.0114 (0.0671)	
training:	Epoch: [21][41/408]	Loss 0.0126 (0.0658)	
training:	Epoch: [21][42/408]	Loss 0.0145 (0.0646)	
training:	Epoch: [21][43/408]	Loss 0.0157 (0.0634)	
training:	Epoch: [21][44/408]	Loss 0.0160 (0.0624)	
training:	Epoch: [21][45/408]	Loss 0.0160 (0.0613)	
training:	Epoch: [21][46/408]	Loss 0.0120 (0.0603)	
training:	Epoch: [21][47/408]	Loss 0.0135 (0.0593)	
training:	Epoch: [21][48/408]	Loss 0.0111 (0.0583)	
training:	Epoch: [21][49/408]	Loss 0.2647 (0.0625)	
training:	Epoch: [21][50/408]	Loss 0.0285 (0.0618)	
training:	Epoch: [21][51/408]	Loss 0.0126 (0.0608)	
training:	Epoch: [21][52/408]	Loss 0.0116 (0.0599)	
training:	Epoch: [21][53/408]	Loss 0.0116 (0.0590)	
training:	Epoch: [21][54/408]	Loss 0.0174 (0.0582)	
training:	Epoch: [21][55/408]	Loss 0.0140 (0.0574)	
training:	Epoch: [21][56/408]	Loss 0.3543 (0.0627)	
training:	Epoch: [21][57/408]	Loss 0.0125 (0.0618)	
training:	Epoch: [21][58/408]	Loss 0.0148 (0.0610)	
training:	Epoch: [21][59/408]	Loss 0.0157 (0.0602)	
training:	Epoch: [21][60/408]	Loss 0.0116 (0.0594)	
training:	Epoch: [21][61/408]	Loss 0.0155 (0.0587)	
training:	Epoch: [21][62/408]	Loss 0.2834 (0.0623)	
training:	Epoch: [21][63/408]	Loss 0.1100 (0.0631)	
training:	Epoch: [21][64/408]	Loss 0.0117 (0.0623)	
training:	Epoch: [21][65/408]	Loss 0.0128 (0.0615)	
training:	Epoch: [21][66/408]	Loss 0.0125 (0.0608)	
training:	Epoch: [21][67/408]	Loss 0.0117 (0.0601)	
training:	Epoch: [21][68/408]	Loss 0.5809 (0.0677)	
training:	Epoch: [21][69/408]	Loss 0.0112 (0.0669)	
training:	Epoch: [21][70/408]	Loss 0.0132 (0.0661)	
training:	Epoch: [21][71/408]	Loss 0.0119 (0.0654)	
training:	Epoch: [21][72/408]	Loss 0.0107 (0.0646)	
training:	Epoch: [21][73/408]	Loss 0.1418 (0.0657)	
training:	Epoch: [21][74/408]	Loss 0.0117 (0.0649)	
training:	Epoch: [21][75/408]	Loss 0.0112 (0.0642)	
training:	Epoch: [21][76/408]	Loss 0.2953 (0.0673)	
training:	Epoch: [21][77/408]	Loss 0.0123 (0.0665)	
training:	Epoch: [21][78/408]	Loss 0.2952 (0.0695)	
training:	Epoch: [21][79/408]	Loss 0.0107 (0.0687)	
training:	Epoch: [21][80/408]	Loss 0.0126 (0.0680)	
training:	Epoch: [21][81/408]	Loss 0.0109 (0.0673)	
training:	Epoch: [21][82/408]	Loss 0.2036 (0.0690)	
training:	Epoch: [21][83/408]	Loss 0.0131 (0.0683)	
training:	Epoch: [21][84/408]	Loss 0.2798 (0.0708)	
training:	Epoch: [21][85/408]	Loss 0.0149 (0.0702)	
training:	Epoch: [21][86/408]	Loss 0.0121 (0.0695)	
training:	Epoch: [21][87/408]	Loss 0.0122 (0.0688)	
training:	Epoch: [21][88/408]	Loss 0.0119 (0.0682)	
training:	Epoch: [21][89/408]	Loss 0.0151 (0.0676)	
training:	Epoch: [21][90/408]	Loss 0.0132 (0.0670)	
training:	Epoch: [21][91/408]	Loss 0.0121 (0.0664)	
training:	Epoch: [21][92/408]	Loss 0.8606 (0.0750)	
training:	Epoch: [21][93/408]	Loss 0.0131 (0.0744)	
training:	Epoch: [21][94/408]	Loss 0.0119 (0.0737)	
training:	Epoch: [21][95/408]	Loss 0.0127 (0.0730)	
training:	Epoch: [21][96/408]	Loss 0.0124 (0.0724)	
training:	Epoch: [21][97/408]	Loss 0.0121 (0.0718)	
training:	Epoch: [21][98/408]	Loss 0.0128 (0.0712)	
training:	Epoch: [21][99/408]	Loss 0.0131 (0.0706)	
training:	Epoch: [21][100/408]	Loss 0.0123 (0.0700)	
training:	Epoch: [21][101/408]	Loss 0.0122 (0.0694)	
training:	Epoch: [21][102/408]	Loss 0.0116 (0.0689)	
training:	Epoch: [21][103/408]	Loss 0.0147 (0.0684)	
training:	Epoch: [21][104/408]	Loss 0.0131 (0.0678)	
training:	Epoch: [21][105/408]	Loss 0.0114 (0.0673)	
training:	Epoch: [21][106/408]	Loss 0.0123 (0.0668)	
training:	Epoch: [21][107/408]	Loss 0.2788 (0.0687)	
training:	Epoch: [21][108/408]	Loss 0.0356 (0.0684)	
training:	Epoch: [21][109/408]	Loss 0.0120 (0.0679)	
training:	Epoch: [21][110/408]	Loss 0.0853 (0.0681)	
training:	Epoch: [21][111/408]	Loss 0.0117 (0.0676)	
training:	Epoch: [21][112/408]	Loss 0.0111 (0.0671)	
training:	Epoch: [21][113/408]	Loss 0.2751 (0.0689)	
training:	Epoch: [21][114/408]	Loss 0.3161 (0.0711)	
training:	Epoch: [21][115/408]	Loss 0.0114 (0.0706)	
training:	Epoch: [21][116/408]	Loss 0.0123 (0.0701)	
training:	Epoch: [21][117/408]	Loss 0.0122 (0.0696)	
training:	Epoch: [21][118/408]	Loss 0.0292 (0.0692)	
training:	Epoch: [21][119/408]	Loss 0.0126 (0.0687)	
training:	Epoch: [21][120/408]	Loss 0.0137 (0.0683)	
training:	Epoch: [21][121/408]	Loss 0.0113 (0.0678)	
training:	Epoch: [21][122/408]	Loss 0.0115 (0.0674)	
training:	Epoch: [21][123/408]	Loss 0.0110 (0.0669)	
training:	Epoch: [21][124/408]	Loss 0.2971 (0.0688)	
training:	Epoch: [21][125/408]	Loss 0.0106 (0.0683)	
training:	Epoch: [21][126/408]	Loss 0.0141 (0.0679)	
training:	Epoch: [21][127/408]	Loss 0.0128 (0.0674)	
training:	Epoch: [21][128/408]	Loss 0.0117 (0.0670)	
training:	Epoch: [21][129/408]	Loss 0.0133 (0.0666)	
training:	Epoch: [21][130/408]	Loss 0.0128 (0.0662)	
training:	Epoch: [21][131/408]	Loss 0.0143 (0.0658)	
training:	Epoch: [21][132/408]	Loss 0.0132 (0.0654)	
training:	Epoch: [21][133/408]	Loss 0.0137 (0.0650)	
training:	Epoch: [21][134/408]	Loss 0.0117 (0.0646)	
training:	Epoch: [21][135/408]	Loss 0.0115 (0.0642)	
training:	Epoch: [21][136/408]	Loss 0.0130 (0.0638)	
training:	Epoch: [21][137/408]	Loss 0.2859 (0.0654)	
training:	Epoch: [21][138/408]	Loss 0.0180 (0.0651)	
training:	Epoch: [21][139/408]	Loss 0.0117 (0.0647)	
training:	Epoch: [21][140/408]	Loss 0.0413 (0.0645)	
training:	Epoch: [21][141/408]	Loss 0.0702 (0.0646)	
training:	Epoch: [21][142/408]	Loss 0.0118 (0.0642)	
training:	Epoch: [21][143/408]	Loss 0.0133 (0.0638)	
training:	Epoch: [21][144/408]	Loss 0.2917 (0.0654)	
training:	Epoch: [21][145/408]	Loss 0.0123 (0.0651)	
training:	Epoch: [21][146/408]	Loss 0.0118 (0.0647)	
training:	Epoch: [21][147/408]	Loss 0.0119 (0.0643)	
training:	Epoch: [21][148/408]	Loss 0.0120 (0.0640)	
training:	Epoch: [21][149/408]	Loss 0.0114 (0.0636)	
training:	Epoch: [21][150/408]	Loss 0.0117 (0.0633)	
training:	Epoch: [21][151/408]	Loss 0.0122 (0.0629)	
training:	Epoch: [21][152/408]	Loss 0.0115 (0.0626)	
training:	Epoch: [21][153/408]	Loss 0.0127 (0.0623)	
training:	Epoch: [21][154/408]	Loss 0.0123 (0.0620)	
training:	Epoch: [21][155/408]	Loss 0.0132 (0.0616)	
training:	Epoch: [21][156/408]	Loss 0.0187 (0.0614)	
training:	Epoch: [21][157/408]	Loss 0.0122 (0.0611)	
training:	Epoch: [21][158/408]	Loss 0.0165 (0.0608)	
training:	Epoch: [21][159/408]	Loss 0.0117 (0.0605)	
training:	Epoch: [21][160/408]	Loss 0.0118 (0.0602)	
training:	Epoch: [21][161/408]	Loss 0.0119 (0.0599)	
training:	Epoch: [21][162/408]	Loss 0.0139 (0.0596)	
training:	Epoch: [21][163/408]	Loss 0.2886 (0.0610)	
training:	Epoch: [21][164/408]	Loss 0.0138 (0.0607)	
training:	Epoch: [21][165/408]	Loss 0.2830 (0.0620)	
training:	Epoch: [21][166/408]	Loss 0.0115 (0.0617)	
training:	Epoch: [21][167/408]	Loss 0.0124 (0.0614)	
training:	Epoch: [21][168/408]	Loss 0.0120 (0.0611)	
training:	Epoch: [21][169/408]	Loss 0.0121 (0.0609)	
training:	Epoch: [21][170/408]	Loss 0.0117 (0.0606)	
training:	Epoch: [21][171/408]	Loss 0.2907 (0.0619)	
training:	Epoch: [21][172/408]	Loss 0.0186 (0.0617)	
training:	Epoch: [21][173/408]	Loss 0.0117 (0.0614)	
training:	Epoch: [21][174/408]	Loss 0.3054 (0.0628)	
training:	Epoch: [21][175/408]	Loss 0.0107 (0.0625)	
training:	Epoch: [21][176/408]	Loss 0.0124 (0.0622)	
training:	Epoch: [21][177/408]	Loss 0.0373 (0.0621)	
training:	Epoch: [21][178/408]	Loss 0.0118 (0.0618)	
training:	Epoch: [21][179/408]	Loss 0.2752 (0.0630)	
training:	Epoch: [21][180/408]	Loss 0.0118 (0.0627)	
training:	Epoch: [21][181/408]	Loss 0.0113 (0.0624)	
training:	Epoch: [21][182/408]	Loss 0.0120 (0.0621)	
training:	Epoch: [21][183/408]	Loss 0.0114 (0.0618)	
training:	Epoch: [21][184/408]	Loss 0.0112 (0.0616)	
training:	Epoch: [21][185/408]	Loss 0.0142 (0.0613)	
training:	Epoch: [21][186/408]	Loss 0.0124 (0.0610)	
training:	Epoch: [21][187/408]	Loss 0.0135 (0.0608)	
training:	Epoch: [21][188/408]	Loss 0.0834 (0.0609)	
training:	Epoch: [21][189/408]	Loss 0.0157 (0.0607)	
training:	Epoch: [21][190/408]	Loss 0.2803 (0.0618)	
training:	Epoch: [21][191/408]	Loss 0.2371 (0.0627)	
training:	Epoch: [21][192/408]	Loss 0.0117 (0.0625)	
training:	Epoch: [21][193/408]	Loss 0.0108 (0.0622)	
training:	Epoch: [21][194/408]	Loss 0.0112 (0.0620)	
training:	Epoch: [21][195/408]	Loss 0.0194 (0.0617)	
training:	Epoch: [21][196/408]	Loss 0.0111 (0.0615)	
training:	Epoch: [21][197/408]	Loss 0.0115 (0.0612)	
training:	Epoch: [21][198/408]	Loss 0.0123 (0.0610)	
training:	Epoch: [21][199/408]	Loss 0.0115 (0.0607)	
training:	Epoch: [21][200/408]	Loss 0.0540 (0.0607)	
training:	Epoch: [21][201/408]	Loss 0.2694 (0.0617)	
training:	Epoch: [21][202/408]	Loss 0.0123 (0.0615)	
training:	Epoch: [21][203/408]	Loss 0.2910 (0.0626)	
training:	Epoch: [21][204/408]	Loss 0.0120 (0.0624)	
training:	Epoch: [21][205/408]	Loss 0.0121 (0.0621)	
training:	Epoch: [21][206/408]	Loss 0.0132 (0.0619)	
training:	Epoch: [21][207/408]	Loss 0.0116 (0.0616)	
training:	Epoch: [21][208/408]	Loss 0.0122 (0.0614)	
training:	Epoch: [21][209/408]	Loss 0.0242 (0.0612)	
training:	Epoch: [21][210/408]	Loss 0.0183 (0.0610)	
training:	Epoch: [21][211/408]	Loss 0.0141 (0.0608)	
training:	Epoch: [21][212/408]	Loss 0.0115 (0.0606)	
training:	Epoch: [21][213/408]	Loss 0.0113 (0.0603)	
training:	Epoch: [21][214/408]	Loss 0.2886 (0.0614)	
training:	Epoch: [21][215/408]	Loss 0.0114 (0.0612)	
training:	Epoch: [21][216/408]	Loss 0.3072 (0.0623)	
training:	Epoch: [21][217/408]	Loss 0.0111 (0.0621)	
training:	Epoch: [21][218/408]	Loss 0.0226 (0.0619)	
training:	Epoch: [21][219/408]	Loss 0.0111 (0.0617)	
training:	Epoch: [21][220/408]	Loss 0.0116 (0.0614)	
training:	Epoch: [21][221/408]	Loss 0.0115 (0.0612)	
training:	Epoch: [21][222/408]	Loss 0.0116 (0.0610)	
training:	Epoch: [21][223/408]	Loss 0.0123 (0.0608)	
training:	Epoch: [21][224/408]	Loss 0.0116 (0.0605)	
training:	Epoch: [21][225/408]	Loss 0.2753 (0.0615)	
training:	Epoch: [21][226/408]	Loss 0.0114 (0.0613)	
training:	Epoch: [21][227/408]	Loss 0.0177 (0.0611)	
training:	Epoch: [21][228/408]	Loss 0.0117 (0.0609)	
training:	Epoch: [21][229/408]	Loss 0.3063 (0.0619)	
training:	Epoch: [21][230/408]	Loss 0.0112 (0.0617)	
training:	Epoch: [21][231/408]	Loss 0.2807 (0.0627)	
training:	Epoch: [21][232/408]	Loss 0.0105 (0.0624)	
training:	Epoch: [21][233/408]	Loss 0.0183 (0.0623)	
training:	Epoch: [21][234/408]	Loss 0.0363 (0.0621)	
training:	Epoch: [21][235/408]	Loss 0.3006 (0.0632)	
training:	Epoch: [21][236/408]	Loss 0.0169 (0.0630)	
training:	Epoch: [21][237/408]	Loss 0.0198 (0.0628)	
training:	Epoch: [21][238/408]	Loss 0.0108 (0.0626)	
training:	Epoch: [21][239/408]	Loss 0.3048 (0.0636)	
training:	Epoch: [21][240/408]	Loss 0.0124 (0.0634)	
training:	Epoch: [21][241/408]	Loss 0.0146 (0.0632)	
training:	Epoch: [21][242/408]	Loss 0.0149 (0.0630)	
training:	Epoch: [21][243/408]	Loss 0.0125 (0.0628)	
training:	Epoch: [21][244/408]	Loss 0.2796 (0.0636)	
training:	Epoch: [21][245/408]	Loss 0.0131 (0.0634)	
training:	Epoch: [21][246/408]	Loss 0.0117 (0.0632)	
training:	Epoch: [21][247/408]	Loss 0.0125 (0.0630)	
training:	Epoch: [21][248/408]	Loss 0.0156 (0.0628)	
training:	Epoch: [21][249/408]	Loss 0.1016 (0.0630)	
training:	Epoch: [21][250/408]	Loss 0.0108 (0.0628)	
training:	Epoch: [21][251/408]	Loss 0.0116 (0.0626)	
training:	Epoch: [21][252/408]	Loss 0.0141 (0.0624)	
training:	Epoch: [21][253/408]	Loss 0.2155 (0.0630)	
training:	Epoch: [21][254/408]	Loss 0.0128 (0.0628)	
training:	Epoch: [21][255/408]	Loss 0.0115 (0.0626)	
training:	Epoch: [21][256/408]	Loss 0.0119 (0.0624)	
training:	Epoch: [21][257/408]	Loss 0.0119 (0.0622)	
training:	Epoch: [21][258/408]	Loss 0.0127 (0.0620)	
training:	Epoch: [21][259/408]	Loss 0.0119 (0.0618)	
training:	Epoch: [21][260/408]	Loss 0.0122 (0.0616)	
training:	Epoch: [21][261/408]	Loss 0.0108 (0.0614)	
training:	Epoch: [21][262/408]	Loss 0.0185 (0.0613)	
training:	Epoch: [21][263/408]	Loss 0.0116 (0.0611)	
training:	Epoch: [21][264/408]	Loss 0.2828 (0.0619)	
training:	Epoch: [21][265/408]	Loss 0.0100 (0.0617)	
training:	Epoch: [21][266/408]	Loss 0.2867 (0.0626)	
training:	Epoch: [21][267/408]	Loss 0.0121 (0.0624)	
training:	Epoch: [21][268/408]	Loss 0.0112 (0.0622)	
training:	Epoch: [21][269/408]	Loss 0.0120 (0.0620)	
training:	Epoch: [21][270/408]	Loss 0.0345 (0.0619)	
training:	Epoch: [21][271/408]	Loss 0.0105 (0.0617)	
training:	Epoch: [21][272/408]	Loss 0.2798 (0.0625)	
training:	Epoch: [21][273/408]	Loss 0.0123 (0.0623)	
training:	Epoch: [21][274/408]	Loss 0.0118 (0.0621)	
training:	Epoch: [21][275/408]	Loss 0.0110 (0.0619)	
training:	Epoch: [21][276/408]	Loss 0.0113 (0.0618)	
training:	Epoch: [21][277/408]	Loss 0.1952 (0.0622)	
training:	Epoch: [21][278/408]	Loss 0.0112 (0.0621)	
training:	Epoch: [21][279/408]	Loss 0.0111 (0.0619)	
training:	Epoch: [21][280/408]	Loss 0.0119 (0.0617)	
training:	Epoch: [21][281/408]	Loss 0.0154 (0.0615)	
training:	Epoch: [21][282/408]	Loss 0.0124 (0.0614)	
training:	Epoch: [21][283/408]	Loss 0.0122 (0.0612)	
training:	Epoch: [21][284/408]	Loss 0.0109 (0.0610)	
training:	Epoch: [21][285/408]	Loss 0.2884 (0.0618)	
training:	Epoch: [21][286/408]	Loss 0.0124 (0.0616)	
training:	Epoch: [21][287/408]	Loss 0.1985 (0.0621)	
training:	Epoch: [21][288/408]	Loss 0.2548 (0.0628)	
training:	Epoch: [21][289/408]	Loss 0.0117 (0.0626)	
training:	Epoch: [21][290/408]	Loss 0.0115 (0.0624)	
training:	Epoch: [21][291/408]	Loss 0.0121 (0.0623)	
training:	Epoch: [21][292/408]	Loss 0.0113 (0.0621)	
training:	Epoch: [21][293/408]	Loss 0.2830 (0.0628)	
training:	Epoch: [21][294/408]	Loss 0.0120 (0.0627)	
training:	Epoch: [21][295/408]	Loss 0.0117 (0.0625)	
training:	Epoch: [21][296/408]	Loss 0.0450 (0.0624)	
training:	Epoch: [21][297/408]	Loss 0.2899 (0.0632)	
training:	Epoch: [21][298/408]	Loss 0.0114 (0.0630)	
training:	Epoch: [21][299/408]	Loss 0.0127 (0.0629)	
training:	Epoch: [21][300/408]	Loss 0.0138 (0.0627)	
training:	Epoch: [21][301/408]	Loss 0.0133 (0.0625)	
training:	Epoch: [21][302/408]	Loss 0.0119 (0.0624)	
training:	Epoch: [21][303/408]	Loss 0.0139 (0.0622)	
training:	Epoch: [21][304/408]	Loss 0.2975 (0.0630)	
training:	Epoch: [21][305/408]	Loss 0.0191 (0.0628)	
training:	Epoch: [21][306/408]	Loss 0.2964 (0.0636)	
training:	Epoch: [21][307/408]	Loss 0.0201 (0.0635)	
training:	Epoch: [21][308/408]	Loss 0.0152 (0.0633)	
training:	Epoch: [21][309/408]	Loss 0.0120 (0.0631)	
training:	Epoch: [21][310/408]	Loss 0.0125 (0.0630)	
training:	Epoch: [21][311/408]	Loss 0.0137 (0.0628)	
training:	Epoch: [21][312/408]	Loss 0.0271 (0.0627)	
training:	Epoch: [21][313/408]	Loss 0.0135 (0.0625)	
training:	Epoch: [21][314/408]	Loss 0.2829 (0.0632)	
training:	Epoch: [21][315/408]	Loss 0.0113 (0.0631)	
training:	Epoch: [21][316/408]	Loss 0.3600 (0.0640)	
training:	Epoch: [21][317/408]	Loss 0.0118 (0.0638)	
training:	Epoch: [21][318/408]	Loss 0.0125 (0.0637)	
training:	Epoch: [21][319/408]	Loss 0.0125 (0.0635)	
training:	Epoch: [21][320/408]	Loss 0.0116 (0.0634)	
training:	Epoch: [21][321/408]	Loss 0.0254 (0.0632)	
training:	Epoch: [21][322/408]	Loss 0.0119 (0.0631)	
training:	Epoch: [21][323/408]	Loss 0.0112 (0.0629)	
training:	Epoch: [21][324/408]	Loss 0.0111 (0.0628)	
training:	Epoch: [21][325/408]	Loss 0.0119 (0.0626)	
training:	Epoch: [21][326/408]	Loss 0.0133 (0.0625)	
training:	Epoch: [21][327/408]	Loss 0.0122 (0.0623)	
training:	Epoch: [21][328/408]	Loss 0.3002 (0.0630)	
training:	Epoch: [21][329/408]	Loss 0.2865 (0.0637)	
training:	Epoch: [21][330/408]	Loss 0.0119 (0.0636)	
training:	Epoch: [21][331/408]	Loss 0.0115 (0.0634)	
training:	Epoch: [21][332/408]	Loss 0.0277 (0.0633)	
training:	Epoch: [21][333/408]	Loss 0.0185 (0.0632)	
training:	Epoch: [21][334/408]	Loss 0.0128 (0.0630)	
training:	Epoch: [21][335/408]	Loss 0.0115 (0.0628)	
training:	Epoch: [21][336/408]	Loss 0.0139 (0.0627)	
training:	Epoch: [21][337/408]	Loss 0.0121 (0.0626)	
training:	Epoch: [21][338/408]	Loss 0.0117 (0.0624)	
training:	Epoch: [21][339/408]	Loss 0.0129 (0.0623)	
training:	Epoch: [21][340/408]	Loss 0.2859 (0.0629)	
training:	Epoch: [21][341/408]	Loss 0.0203 (0.0628)	
training:	Epoch: [21][342/408]	Loss 0.0133 (0.0626)	
training:	Epoch: [21][343/408]	Loss 0.0116 (0.0625)	
training:	Epoch: [21][344/408]	Loss 0.2554 (0.0631)	
training:	Epoch: [21][345/408]	Loss 0.0136 (0.0629)	
training:	Epoch: [21][346/408]	Loss 0.0123 (0.0628)	
training:	Epoch: [21][347/408]	Loss 0.0127 (0.0626)	
training:	Epoch: [21][348/408]	Loss 0.0125 (0.0625)	
training:	Epoch: [21][349/408]	Loss 0.0112 (0.0623)	
training:	Epoch: [21][350/408]	Loss 0.0165 (0.0622)	
training:	Epoch: [21][351/408]	Loss 0.0127 (0.0621)	
training:	Epoch: [21][352/408]	Loss 0.2700 (0.0626)	
training:	Epoch: [21][353/408]	Loss 0.0115 (0.0625)	
training:	Epoch: [21][354/408]	Loss 0.0159 (0.0624)	
training:	Epoch: [21][355/408]	Loss 0.0113 (0.0622)	
training:	Epoch: [21][356/408]	Loss 0.0108 (0.0621)	
training:	Epoch: [21][357/408]	Loss 0.0105 (0.0619)	
training:	Epoch: [21][358/408]	Loss 0.0128 (0.0618)	
training:	Epoch: [21][359/408]	Loss 0.0123 (0.0617)	
training:	Epoch: [21][360/408]	Loss 0.0103 (0.0615)	
training:	Epoch: [21][361/408]	Loss 0.0136 (0.0614)	
training:	Epoch: [21][362/408]	Loss 0.2846 (0.0620)	
training:	Epoch: [21][363/408]	Loss 0.0141 (0.0619)	
training:	Epoch: [21][364/408]	Loss 0.0122 (0.0617)	
training:	Epoch: [21][365/408]	Loss 0.0117 (0.0616)	
training:	Epoch: [21][366/408]	Loss 0.0126 (0.0615)	
training:	Epoch: [21][367/408]	Loss 0.0119 (0.0613)	
training:	Epoch: [21][368/408]	Loss 0.0116 (0.0612)	
training:	Epoch: [21][369/408]	Loss 0.3065 (0.0619)	
training:	Epoch: [21][370/408]	Loss 0.0136 (0.0617)	
training:	Epoch: [21][371/408]	Loss 0.0135 (0.0616)	
training:	Epoch: [21][372/408]	Loss 0.2902 (0.0622)	
training:	Epoch: [21][373/408]	Loss 0.0140 (0.0621)	
training:	Epoch: [21][374/408]	Loss 0.0118 (0.0620)	
training:	Epoch: [21][375/408]	Loss 0.0132 (0.0618)	
training:	Epoch: [21][376/408]	Loss 0.0138 (0.0617)	
training:	Epoch: [21][377/408]	Loss 0.0120 (0.0616)	
training:	Epoch: [21][378/408]	Loss 0.0311 (0.0615)	
training:	Epoch: [21][379/408]	Loss 0.0114 (0.0613)	
training:	Epoch: [21][380/408]	Loss 0.0114 (0.0612)	
training:	Epoch: [21][381/408]	Loss 0.3055 (0.0619)	
training:	Epoch: [21][382/408]	Loss 0.0109 (0.0617)	
training:	Epoch: [21][383/408]	Loss 0.0119 (0.0616)	
training:	Epoch: [21][384/408]	Loss 0.0135 (0.0615)	
training:	Epoch: [21][385/408]	Loss 0.3006 (0.0621)	
training:	Epoch: [21][386/408]	Loss 0.2940 (0.0627)	
training:	Epoch: [21][387/408]	Loss 0.0106 (0.0626)	
training:	Epoch: [21][388/408]	Loss 0.0112 (0.0624)	
training:	Epoch: [21][389/408]	Loss 0.0126 (0.0623)	
training:	Epoch: [21][390/408]	Loss 0.2737 (0.0628)	
training:	Epoch: [21][391/408]	Loss 0.0121 (0.0627)	
training:	Epoch: [21][392/408]	Loss 0.0123 (0.0626)	
training:	Epoch: [21][393/408]	Loss 0.0113 (0.0625)	
training:	Epoch: [21][394/408]	Loss 0.0103 (0.0623)	
training:	Epoch: [21][395/408]	Loss 0.0119 (0.0622)	
training:	Epoch: [21][396/408]	Loss 0.0423 (0.0621)	
training:	Epoch: [21][397/408]	Loss 0.0111 (0.0620)	
training:	Epoch: [21][398/408]	Loss 0.0138 (0.0619)	
training:	Epoch: [21][399/408]	Loss 0.0119 (0.0618)	
training:	Epoch: [21][400/408]	Loss 0.0121 (0.0616)	
training:	Epoch: [21][401/408]	Loss 0.1635 (0.0619)	
training:	Epoch: [21][402/408]	Loss 0.0114 (0.0618)	
training:	Epoch: [21][403/408]	Loss 0.0108 (0.0616)	
training:	Epoch: [21][404/408]	Loss 0.0106 (0.0615)	
training:	Epoch: [21][405/408]	Loss 0.0113 (0.0614)	
training:	Epoch: [21][406/408]	Loss 0.0113 (0.0613)	
training:	Epoch: [21][407/408]	Loss 0.0242 (0.0612)	
training:	Epoch: [21][408/408]	Loss 0.0116 (0.0611)	
Training:	 Loss: 0.0610

Training:	 ACC: 0.9902 0.9902 0.9900 0.9904
Validation:	 ACC: 0.7870 0.7892 0.8352 0.7388
Validation:	 Best_BACC: 0.8226 0.8250 0.8772 0.7679
Validation:	 Loss: 0.8289
Pretraining:	Epoch 22/200
----------
training:	Epoch: [22][1/408]	Loss 0.0114 (0.0114)	
training:	Epoch: [22][2/408]	Loss 0.0111 (0.0112)	
training:	Epoch: [22][3/408]	Loss 0.0418 (0.0214)	
training:	Epoch: [22][4/408]	Loss 0.2663 (0.0827)	
training:	Epoch: [22][5/408]	Loss 0.2801 (0.1222)	
training:	Epoch: [22][6/408]	Loss 0.0109 (0.1036)	
training:	Epoch: [22][7/408]	Loss 0.0115 (0.0904)	
training:	Epoch: [22][8/408]	Loss 0.0139 (0.0809)	
training:	Epoch: [22][9/408]	Loss 0.0111 (0.0731)	
training:	Epoch: [22][10/408]	Loss 0.0111 (0.0669)	
training:	Epoch: [22][11/408]	Loss 0.0122 (0.0619)	
training:	Epoch: [22][12/408]	Loss 0.0129 (0.0579)	
training:	Epoch: [22][13/408]	Loss 0.0125 (0.0544)	
training:	Epoch: [22][14/408]	Loss 0.0210 (0.0520)	
training:	Epoch: [22][15/408]	Loss 0.0122 (0.0493)	
training:	Epoch: [22][16/408]	Loss 0.0107 (0.0469)	
training:	Epoch: [22][17/408]	Loss 0.0110 (0.0448)	
training:	Epoch: [22][18/408]	Loss 0.0116 (0.0430)	
training:	Epoch: [22][19/408]	Loss 0.0112 (0.0413)	
training:	Epoch: [22][20/408]	Loss 0.3050 (0.0545)	
training:	Epoch: [22][21/408]	Loss 0.0124 (0.0525)	
training:	Epoch: [22][22/408]	Loss 0.0115 (0.0506)	
training:	Epoch: [22][23/408]	Loss 0.2842 (0.0608)	
training:	Epoch: [22][24/408]	Loss 0.0127 (0.0588)	
training:	Epoch: [22][25/408]	Loss 0.0122 (0.0569)	
training:	Epoch: [22][26/408]	Loss 0.1171 (0.0592)	
training:	Epoch: [22][27/408]	Loss 0.0113 (0.0574)	
training:	Epoch: [22][28/408]	Loss 0.0478 (0.0571)	
training:	Epoch: [22][29/408]	Loss 0.0102 (0.0555)	
training:	Epoch: [22][30/408]	Loss 0.0129 (0.0541)	
training:	Epoch: [22][31/408]	Loss 0.0114 (0.0527)	
training:	Epoch: [22][32/408]	Loss 0.0115 (0.0514)	
training:	Epoch: [22][33/408]	Loss 0.0175 (0.0504)	
training:	Epoch: [22][34/408]	Loss 0.0112 (0.0492)	
training:	Epoch: [22][35/408]	Loss 0.0127 (0.0482)	
training:	Epoch: [22][36/408]	Loss 0.0111 (0.0471)	
training:	Epoch: [22][37/408]	Loss 0.0136 (0.0462)	
training:	Epoch: [22][38/408]	Loss 0.0134 (0.0454)	
training:	Epoch: [22][39/408]	Loss 0.0118 (0.0445)	
training:	Epoch: [22][40/408]	Loss 0.3108 (0.0512)	
training:	Epoch: [22][41/408]	Loss 0.0113 (0.0502)	
training:	Epoch: [22][42/408]	Loss 0.0137 (0.0493)	
training:	Epoch: [22][43/408]	Loss 0.0129 (0.0485)	
training:	Epoch: [22][44/408]	Loss 0.0185 (0.0478)	
training:	Epoch: [22][45/408]	Loss 0.0126 (0.0470)	
training:	Epoch: [22][46/408]	Loss 0.0109 (0.0462)	
training:	Epoch: [22][47/408]	Loss 0.0112 (0.0455)	
training:	Epoch: [22][48/408]	Loss 0.0114 (0.0448)	
training:	Epoch: [22][49/408]	Loss 0.0125 (0.0441)	
training:	Epoch: [22][50/408]	Loss 0.0191 (0.0436)	
training:	Epoch: [22][51/408]	Loss 0.0123 (0.0430)	
training:	Epoch: [22][52/408]	Loss 0.0113 (0.0424)	
training:	Epoch: [22][53/408]	Loss 0.0128 (0.0418)	
training:	Epoch: [22][54/408]	Loss 0.0105 (0.0413)	
training:	Epoch: [22][55/408]	Loss 0.0114 (0.0407)	
training:	Epoch: [22][56/408]	Loss 0.0131 (0.0402)	
training:	Epoch: [22][57/408]	Loss 0.0127 (0.0397)	
training:	Epoch: [22][58/408]	Loss 0.0148 (0.0393)	
training:	Epoch: [22][59/408]	Loss 0.0122 (0.0389)	
training:	Epoch: [22][60/408]	Loss 0.0123 (0.0384)	
training:	Epoch: [22][61/408]	Loss 0.0144 (0.0380)	
training:	Epoch: [22][62/408]	Loss 0.0148 (0.0376)	
training:	Epoch: [22][63/408]	Loss 0.0111 (0.0372)	
training:	Epoch: [22][64/408]	Loss 0.0166 (0.0369)	
training:	Epoch: [22][65/408]	Loss 0.0153 (0.0366)	
training:	Epoch: [22][66/408]	Loss 0.0110 (0.0362)	
training:	Epoch: [22][67/408]	Loss 0.0120 (0.0358)	
training:	Epoch: [22][68/408]	Loss 0.0136 (0.0355)	
training:	Epoch: [22][69/408]	Loss 0.2957 (0.0393)	
training:	Epoch: [22][70/408]	Loss 0.0129 (0.0389)	
training:	Epoch: [22][71/408]	Loss 0.0146 (0.0385)	
training:	Epoch: [22][72/408]	Loss 0.0113 (0.0382)	
training:	Epoch: [22][73/408]	Loss 0.3034 (0.0418)	
training:	Epoch: [22][74/408]	Loss 0.0107 (0.0414)	
training:	Epoch: [22][75/408]	Loss 0.2879 (0.0447)	
training:	Epoch: [22][76/408]	Loss 0.0111 (0.0442)	
training:	Epoch: [22][77/408]	Loss 0.0116 (0.0438)	
training:	Epoch: [22][78/408]	Loss 0.0113 (0.0434)	
training:	Epoch: [22][79/408]	Loss 0.0098 (0.0430)	
training:	Epoch: [22][80/408]	Loss 0.0106 (0.0426)	
training:	Epoch: [22][81/408]	Loss 0.0121 (0.0422)	
training:	Epoch: [22][82/408]	Loss 0.2744 (0.0450)	
training:	Epoch: [22][83/408]	Loss 0.0105 (0.0446)	
training:	Epoch: [22][84/408]	Loss 0.5691 (0.0508)	
training:	Epoch: [22][85/408]	Loss 0.0115 (0.0504)	
training:	Epoch: [22][86/408]	Loss 0.0143 (0.0500)	
training:	Epoch: [22][87/408]	Loss 0.0138 (0.0495)	
training:	Epoch: [22][88/408]	Loss 0.0108 (0.0491)	
training:	Epoch: [22][89/408]	Loss 0.3685 (0.0527)	
training:	Epoch: [22][90/408]	Loss 0.0112 (0.0522)	
training:	Epoch: [22][91/408]	Loss 0.0111 (0.0518)	
training:	Epoch: [22][92/408]	Loss 0.0113 (0.0513)	
training:	Epoch: [22][93/408]	Loss 0.0122 (0.0509)	
training:	Epoch: [22][94/408]	Loss 0.0113 (0.0505)	
training:	Epoch: [22][95/408]	Loss 0.2943 (0.0531)	
training:	Epoch: [22][96/408]	Loss 0.0144 (0.0527)	
training:	Epoch: [22][97/408]	Loss 0.0157 (0.0523)	
training:	Epoch: [22][98/408]	Loss 0.0201 (0.0519)	
training:	Epoch: [22][99/408]	Loss 0.0118 (0.0515)	
training:	Epoch: [22][100/408]	Loss 0.0126 (0.0512)	
training:	Epoch: [22][101/408]	Loss 0.0108 (0.0508)	
training:	Epoch: [22][102/408]	Loss 0.0124 (0.0504)	
training:	Epoch: [22][103/408]	Loss 0.2838 (0.0526)	
training:	Epoch: [22][104/408]	Loss 0.1123 (0.0532)	
training:	Epoch: [22][105/408]	Loss 0.0111 (0.0528)	
training:	Epoch: [22][106/408]	Loss 0.0309 (0.0526)	
training:	Epoch: [22][107/408]	Loss 0.1868 (0.0539)	
training:	Epoch: [22][108/408]	Loss 0.0116 (0.0535)	
training:	Epoch: [22][109/408]	Loss 0.1010 (0.0539)	
training:	Epoch: [22][110/408]	Loss 0.0118 (0.0535)	
training:	Epoch: [22][111/408]	Loss 0.0136 (0.0532)	
training:	Epoch: [22][112/408]	Loss 0.0119 (0.0528)	
training:	Epoch: [22][113/408]	Loss 0.0598 (0.0529)	
training:	Epoch: [22][114/408]	Loss 0.0786 (0.0531)	
training:	Epoch: [22][115/408]	Loss 0.0125 (0.0527)	
training:	Epoch: [22][116/408]	Loss 0.1160 (0.0533)	
training:	Epoch: [22][117/408]	Loss 0.0118 (0.0529)	
training:	Epoch: [22][118/408]	Loss 0.2772 (0.0548)	
training:	Epoch: [22][119/408]	Loss 0.4286 (0.0580)	
training:	Epoch: [22][120/408]	Loss 0.0119 (0.0576)	
training:	Epoch: [22][121/408]	Loss 0.0124 (0.0572)	
training:	Epoch: [22][122/408]	Loss 0.0124 (0.0568)	
training:	Epoch: [22][123/408]	Loss 0.0263 (0.0566)	
training:	Epoch: [22][124/408]	Loss 0.0108 (0.0562)	
training:	Epoch: [22][125/408]	Loss 0.0113 (0.0559)	
training:	Epoch: [22][126/408]	Loss 0.0108 (0.0555)	
training:	Epoch: [22][127/408]	Loss 0.0112 (0.0552)	
training:	Epoch: [22][128/408]	Loss 0.0114 (0.0548)	
training:	Epoch: [22][129/408]	Loss 0.0207 (0.0545)	
training:	Epoch: [22][130/408]	Loss 0.0244 (0.0543)	
training:	Epoch: [22][131/408]	Loss 0.0175 (0.0540)	
training:	Epoch: [22][132/408]	Loss 0.0095 (0.0537)	
training:	Epoch: [22][133/408]	Loss 0.0116 (0.0534)	
training:	Epoch: [22][134/408]	Loss 0.2643 (0.0550)	
training:	Epoch: [22][135/408]	Loss 0.0107 (0.0546)	
training:	Epoch: [22][136/408]	Loss 0.2751 (0.0562)	
training:	Epoch: [22][137/408]	Loss 0.0107 (0.0559)	
training:	Epoch: [22][138/408]	Loss 0.0115 (0.0556)	
training:	Epoch: [22][139/408]	Loss 0.0131 (0.0553)	
training:	Epoch: [22][140/408]	Loss 0.0186 (0.0550)	
training:	Epoch: [22][141/408]	Loss 0.0139 (0.0547)	
training:	Epoch: [22][142/408]	Loss 0.0110 (0.0544)	
training:	Epoch: [22][143/408]	Loss 0.0138 (0.0541)	
training:	Epoch: [22][144/408]	Loss 0.2568 (0.0556)	
training:	Epoch: [22][145/408]	Loss 0.0113 (0.0552)	
training:	Epoch: [22][146/408]	Loss 0.2995 (0.0569)	
training:	Epoch: [22][147/408]	Loss 0.0115 (0.0566)	
training:	Epoch: [22][148/408]	Loss 0.0117 (0.0563)	
training:	Epoch: [22][149/408]	Loss 0.0117 (0.0560)	
training:	Epoch: [22][150/408]	Loss 0.0115 (0.0557)	
training:	Epoch: [22][151/408]	Loss 0.0126 (0.0554)	
training:	Epoch: [22][152/408]	Loss 0.0119 (0.0551)	
training:	Epoch: [22][153/408]	Loss 0.0142 (0.0549)	
training:	Epoch: [22][154/408]	Loss 0.3528 (0.0568)	
training:	Epoch: [22][155/408]	Loss 0.0129 (0.0565)	
training:	Epoch: [22][156/408]	Loss 0.3168 (0.0582)	
training:	Epoch: [22][157/408]	Loss 0.2861 (0.0596)	
training:	Epoch: [22][158/408]	Loss 0.3099 (0.0612)	
training:	Epoch: [22][159/408]	Loss 0.0158 (0.0609)	
training:	Epoch: [22][160/408]	Loss 0.0115 (0.0606)	
training:	Epoch: [22][161/408]	Loss 0.0165 (0.0604)	
training:	Epoch: [22][162/408]	Loss 0.0131 (0.0601)	
training:	Epoch: [22][163/408]	Loss 0.3056 (0.0616)	
training:	Epoch: [22][164/408]	Loss 0.0105 (0.0613)	
training:	Epoch: [22][165/408]	Loss 0.2614 (0.0625)	
training:	Epoch: [22][166/408]	Loss 0.0124 (0.0622)	
training:	Epoch: [22][167/408]	Loss 0.2882 (0.0635)	
training:	Epoch: [22][168/408]	Loss 0.2827 (0.0648)	
training:	Epoch: [22][169/408]	Loss 0.0130 (0.0645)	
training:	Epoch: [22][170/408]	Loss 0.0120 (0.0642)	
training:	Epoch: [22][171/408]	Loss 0.0108 (0.0639)	
training:	Epoch: [22][172/408]	Loss 0.0371 (0.0637)	
training:	Epoch: [22][173/408]	Loss 0.0135 (0.0635)	
training:	Epoch: [22][174/408]	Loss 0.0115 (0.0632)	
training:	Epoch: [22][175/408]	Loss 0.3030 (0.0645)	
training:	Epoch: [22][176/408]	Loss 0.0171 (0.0643)	
training:	Epoch: [22][177/408]	Loss 0.0125 (0.0640)	
training:	Epoch: [22][178/408]	Loss 0.2963 (0.0653)	
training:	Epoch: [22][179/408]	Loss 0.0111 (0.0650)	
training:	Epoch: [22][180/408]	Loss 0.0113 (0.0647)	
training:	Epoch: [22][181/408]	Loss 0.0128 (0.0644)	
training:	Epoch: [22][182/408]	Loss 0.0119 (0.0641)	
training:	Epoch: [22][183/408]	Loss 0.0126 (0.0638)	
training:	Epoch: [22][184/408]	Loss 0.0118 (0.0635)	
training:	Epoch: [22][185/408]	Loss 0.0238 (0.0633)	
training:	Epoch: [22][186/408]	Loss 0.2766 (0.0645)	
training:	Epoch: [22][187/408]	Loss 0.0123 (0.0642)	
training:	Epoch: [22][188/408]	Loss 0.0135 (0.0639)	
training:	Epoch: [22][189/408]	Loss 0.0147 (0.0637)	
training:	Epoch: [22][190/408]	Loss 0.0116 (0.0634)	
training:	Epoch: [22][191/408]	Loss 0.0127 (0.0631)	
training:	Epoch: [22][192/408]	Loss 0.0158 (0.0629)	
training:	Epoch: [22][193/408]	Loss 0.0139 (0.0626)	
training:	Epoch: [22][194/408]	Loss 0.0158 (0.0624)	
training:	Epoch: [22][195/408]	Loss 0.0118 (0.0621)	
training:	Epoch: [22][196/408]	Loss 0.2862 (0.0633)	
training:	Epoch: [22][197/408]	Loss 0.0117 (0.0630)	
training:	Epoch: [22][198/408]	Loss 0.0141 (0.0627)	
training:	Epoch: [22][199/408]	Loss 0.0132 (0.0625)	
training:	Epoch: [22][200/408]	Loss 0.0114 (0.0622)	
training:	Epoch: [22][201/408]	Loss 0.0129 (0.0620)	
training:	Epoch: [22][202/408]	Loss 0.0112 (0.0617)	
training:	Epoch: [22][203/408]	Loss 0.1645 (0.0623)	
training:	Epoch: [22][204/408]	Loss 0.0122 (0.0620)	
training:	Epoch: [22][205/408]	Loss 0.2769 (0.0631)	
training:	Epoch: [22][206/408]	Loss 0.0122 (0.0628)	
training:	Epoch: [22][207/408]	Loss 0.0147 (0.0626)	
training:	Epoch: [22][208/408]	Loss 0.0103 (0.0623)	
training:	Epoch: [22][209/408]	Loss 0.0127 (0.0621)	
training:	Epoch: [22][210/408]	Loss 0.3029 (0.0632)	
training:	Epoch: [22][211/408]	Loss 0.0121 (0.0630)	
training:	Epoch: [22][212/408]	Loss 0.0704 (0.0630)	
training:	Epoch: [22][213/408]	Loss 0.0134 (0.0628)	
training:	Epoch: [22][214/408]	Loss 0.0151 (0.0626)	
training:	Epoch: [22][215/408]	Loss 0.0124 (0.0623)	
training:	Epoch: [22][216/408]	Loss 0.0250 (0.0622)	
training:	Epoch: [22][217/408]	Loss 0.0174 (0.0620)	
training:	Epoch: [22][218/408]	Loss 0.0184 (0.0618)	
training:	Epoch: [22][219/408]	Loss 0.0119 (0.0615)	
training:	Epoch: [22][220/408]	Loss 0.0126 (0.0613)	
training:	Epoch: [22][221/408]	Loss 0.0116 (0.0611)	
training:	Epoch: [22][222/408]	Loss 0.0112 (0.0609)	
training:	Epoch: [22][223/408]	Loss 0.0231 (0.0607)	
training:	Epoch: [22][224/408]	Loss 0.0122 (0.0605)	
training:	Epoch: [22][225/408]	Loss 0.0138 (0.0603)	
training:	Epoch: [22][226/408]	Loss 0.0117 (0.0600)	
training:	Epoch: [22][227/408]	Loss 0.2392 (0.0608)	
training:	Epoch: [22][228/408]	Loss 0.2767 (0.0618)	
training:	Epoch: [22][229/408]	Loss 0.0106 (0.0616)	
training:	Epoch: [22][230/408]	Loss 0.0137 (0.0614)	
training:	Epoch: [22][231/408]	Loss 0.2966 (0.0624)	
training:	Epoch: [22][232/408]	Loss 0.0108 (0.0622)	
training:	Epoch: [22][233/408]	Loss 0.0128 (0.0619)	
training:	Epoch: [22][234/408]	Loss 0.0102 (0.0617)	
training:	Epoch: [22][235/408]	Loss 0.0345 (0.0616)	
training:	Epoch: [22][236/408]	Loss 0.0119 (0.0614)	
training:	Epoch: [22][237/408]	Loss 0.0146 (0.0612)	
training:	Epoch: [22][238/408]	Loss 0.0112 (0.0610)	
training:	Epoch: [22][239/408]	Loss 0.0108 (0.0608)	
training:	Epoch: [22][240/408]	Loss 0.0117 (0.0606)	
training:	Epoch: [22][241/408]	Loss 0.0145 (0.0604)	
training:	Epoch: [22][242/408]	Loss 0.0101 (0.0602)	
training:	Epoch: [22][243/408]	Loss 0.0106 (0.0600)	
training:	Epoch: [22][244/408]	Loss 0.0124 (0.0598)	
training:	Epoch: [22][245/408]	Loss 0.0602 (0.0598)	
training:	Epoch: [22][246/408]	Loss 0.0246 (0.0596)	
training:	Epoch: [22][247/408]	Loss 0.0116 (0.0594)	
training:	Epoch: [22][248/408]	Loss 0.0117 (0.0592)	
training:	Epoch: [22][249/408]	Loss 0.2857 (0.0602)	
training:	Epoch: [22][250/408]	Loss 0.0101 (0.0600)	
training:	Epoch: [22][251/408]	Loss 0.0112 (0.0598)	
training:	Epoch: [22][252/408]	Loss 0.0153 (0.0596)	
training:	Epoch: [22][253/408]	Loss 0.0126 (0.0594)	
training:	Epoch: [22][254/408]	Loss 0.0129 (0.0592)	
training:	Epoch: [22][255/408]	Loss 0.0117 (0.0590)	
training:	Epoch: [22][256/408]	Loss 0.2746 (0.0599)	
training:	Epoch: [22][257/408]	Loss 0.0124 (0.0597)	
training:	Epoch: [22][258/408]	Loss 0.0151 (0.0595)	
training:	Epoch: [22][259/408]	Loss 0.0134 (0.0593)	
training:	Epoch: [22][260/408]	Loss 0.0103 (0.0591)	
training:	Epoch: [22][261/408]	Loss 0.0106 (0.0590)	
training:	Epoch: [22][262/408]	Loss 0.0156 (0.0588)	
training:	Epoch: [22][263/408]	Loss 0.0159 (0.0586)	
training:	Epoch: [22][264/408]	Loss 0.0127 (0.0585)	
training:	Epoch: [22][265/408]	Loss 0.1569 (0.0588)	
training:	Epoch: [22][266/408]	Loss 0.3097 (0.0598)	
training:	Epoch: [22][267/408]	Loss 0.0124 (0.0596)	
training:	Epoch: [22][268/408]	Loss 0.0112 (0.0594)	
training:	Epoch: [22][269/408]	Loss 0.0107 (0.0592)	
training:	Epoch: [22][270/408]	Loss 0.0164 (0.0591)	
training:	Epoch: [22][271/408]	Loss 0.0101 (0.0589)	
training:	Epoch: [22][272/408]	Loss 0.0111 (0.0587)	
training:	Epoch: [22][273/408]	Loss 0.0122 (0.0585)	
training:	Epoch: [22][274/408]	Loss 0.0241 (0.0584)	
training:	Epoch: [22][275/408]	Loss 0.0115 (0.0583)	
training:	Epoch: [22][276/408]	Loss 0.0156 (0.0581)	
training:	Epoch: [22][277/408]	Loss 0.3105 (0.0590)	
training:	Epoch: [22][278/408]	Loss 0.0118 (0.0588)	
training:	Epoch: [22][279/408]	Loss 0.0109 (0.0587)	
training:	Epoch: [22][280/408]	Loss 0.0365 (0.0586)	
training:	Epoch: [22][281/408]	Loss 0.0128 (0.0584)	
training:	Epoch: [22][282/408]	Loss 0.0132 (0.0583)	
training:	Epoch: [22][283/408]	Loss 0.0119 (0.0581)	
training:	Epoch: [22][284/408]	Loss 0.0118 (0.0579)	
training:	Epoch: [22][285/408]	Loss 0.0124 (0.0578)	
training:	Epoch: [22][286/408]	Loss 0.0119 (0.0576)	
training:	Epoch: [22][287/408]	Loss 0.0105 (0.0575)	
training:	Epoch: [22][288/408]	Loss 0.0117 (0.0573)	
training:	Epoch: [22][289/408]	Loss 0.0122 (0.0571)	
training:	Epoch: [22][290/408]	Loss 0.0116 (0.0570)	
training:	Epoch: [22][291/408]	Loss 0.3124 (0.0579)	
training:	Epoch: [22][292/408]	Loss 0.2956 (0.0587)	
training:	Epoch: [22][293/408]	Loss 0.0108 (0.0585)	
training:	Epoch: [22][294/408]	Loss 0.0347 (0.0584)	
training:	Epoch: [22][295/408]	Loss 0.0118 (0.0583)	
training:	Epoch: [22][296/408]	Loss 0.0112 (0.0581)	
training:	Epoch: [22][297/408]	Loss 0.1532 (0.0584)	
training:	Epoch: [22][298/408]	Loss 0.0117 (0.0583)	
training:	Epoch: [22][299/408]	Loss 0.0127 (0.0581)	
training:	Epoch: [22][300/408]	Loss 0.0209 (0.0580)	
training:	Epoch: [22][301/408]	Loss 0.0111 (0.0578)	
training:	Epoch: [22][302/408]	Loss 0.0112 (0.0577)	
training:	Epoch: [22][303/408]	Loss 0.0130 (0.0575)	
training:	Epoch: [22][304/408]	Loss 0.0224 (0.0574)	
training:	Epoch: [22][305/408]	Loss 0.2689 (0.0581)	
training:	Epoch: [22][306/408]	Loss 0.0111 (0.0580)	
training:	Epoch: [22][307/408]	Loss 0.0128 (0.0578)	
training:	Epoch: [22][308/408]	Loss 0.0114 (0.0577)	
training:	Epoch: [22][309/408]	Loss 0.0205 (0.0575)	
training:	Epoch: [22][310/408]	Loss 0.0231 (0.0574)	
training:	Epoch: [22][311/408]	Loss 0.2690 (0.0581)	
training:	Epoch: [22][312/408]	Loss 0.3051 (0.0589)	
training:	Epoch: [22][313/408]	Loss 0.2965 (0.0597)	
training:	Epoch: [22][314/408]	Loss 0.0111 (0.0595)	
training:	Epoch: [22][315/408]	Loss 0.0187 (0.0594)	
training:	Epoch: [22][316/408]	Loss 0.0112 (0.0592)	
training:	Epoch: [22][317/408]	Loss 0.0142 (0.0591)	
training:	Epoch: [22][318/408]	Loss 0.0123 (0.0589)	
training:	Epoch: [22][319/408]	Loss 0.2972 (0.0597)	
training:	Epoch: [22][320/408]	Loss 0.0121 (0.0595)	
training:	Epoch: [22][321/408]	Loss 0.0106 (0.0594)	
training:	Epoch: [22][322/408]	Loss 0.0105 (0.0592)	
training:	Epoch: [22][323/408]	Loss 0.0298 (0.0591)	
training:	Epoch: [22][324/408]	Loss 0.2587 (0.0598)	
training:	Epoch: [22][325/408]	Loss 0.0115 (0.0596)	
training:	Epoch: [22][326/408]	Loss 0.0368 (0.0595)	
training:	Epoch: [22][327/408]	Loss 0.0140 (0.0594)	
training:	Epoch: [22][328/408]	Loss 0.0105 (0.0593)	
training:	Epoch: [22][329/408]	Loss 0.0127 (0.0591)	
training:	Epoch: [22][330/408]	Loss 0.0109 (0.0590)	
training:	Epoch: [22][331/408]	Loss 0.0189 (0.0588)	
training:	Epoch: [22][332/408]	Loss 0.0429 (0.0588)	
training:	Epoch: [22][333/408]	Loss 0.0121 (0.0587)	
training:	Epoch: [22][334/408]	Loss 0.0131 (0.0585)	
training:	Epoch: [22][335/408]	Loss 0.0363 (0.0585)	
training:	Epoch: [22][336/408]	Loss 0.0111 (0.0583)	
training:	Epoch: [22][337/408]	Loss 0.0121 (0.0582)	
training:	Epoch: [22][338/408]	Loss 0.2810 (0.0588)	
training:	Epoch: [22][339/408]	Loss 0.0221 (0.0587)	
training:	Epoch: [22][340/408]	Loss 0.0108 (0.0586)	
training:	Epoch: [22][341/408]	Loss 0.0120 (0.0584)	
training:	Epoch: [22][342/408]	Loss 0.0119 (0.0583)	
training:	Epoch: [22][343/408]	Loss 0.0120 (0.0582)	
training:	Epoch: [22][344/408]	Loss 0.3047 (0.0589)	
training:	Epoch: [22][345/408]	Loss 0.0116 (0.0588)	
training:	Epoch: [22][346/408]	Loss 0.0129 (0.0586)	
training:	Epoch: [22][347/408]	Loss 0.0107 (0.0585)	
training:	Epoch: [22][348/408]	Loss 0.0121 (0.0584)	
training:	Epoch: [22][349/408]	Loss 0.0726 (0.0584)	
training:	Epoch: [22][350/408]	Loss 0.0307 (0.0583)	
training:	Epoch: [22][351/408]	Loss 0.0136 (0.0582)	
training:	Epoch: [22][352/408]	Loss 0.0115 (0.0581)	
training:	Epoch: [22][353/408]	Loss 0.5563 (0.0595)	
training:	Epoch: [22][354/408]	Loss 0.0106 (0.0593)	
training:	Epoch: [22][355/408]	Loss 0.0110 (0.0592)	
training:	Epoch: [22][356/408]	Loss 0.0108 (0.0591)	
training:	Epoch: [22][357/408]	Loss 0.0121 (0.0589)	
training:	Epoch: [22][358/408]	Loss 0.0141 (0.0588)	
training:	Epoch: [22][359/408]	Loss 0.0117 (0.0587)	
training:	Epoch: [22][360/408]	Loss 0.0108 (0.0585)	
training:	Epoch: [22][361/408]	Loss 0.0168 (0.0584)	
training:	Epoch: [22][362/408]	Loss 0.0116 (0.0583)	
training:	Epoch: [22][363/408]	Loss 0.0120 (0.0582)	
training:	Epoch: [22][364/408]	Loss 0.0117 (0.0580)	
training:	Epoch: [22][365/408]	Loss 0.0107 (0.0579)	
training:	Epoch: [22][366/408]	Loss 0.0341 (0.0578)	
training:	Epoch: [22][367/408]	Loss 0.0157 (0.0577)	
training:	Epoch: [22][368/408]	Loss 0.0154 (0.0576)	
training:	Epoch: [22][369/408]	Loss 0.0118 (0.0575)	
training:	Epoch: [22][370/408]	Loss 0.2882 (0.0581)	
training:	Epoch: [22][371/408]	Loss 0.0148 (0.0580)	
training:	Epoch: [22][372/408]	Loss 0.0229 (0.0579)	
training:	Epoch: [22][373/408]	Loss 0.3032 (0.0586)	
training:	Epoch: [22][374/408]	Loss 0.0183 (0.0584)	
training:	Epoch: [22][375/408]	Loss 0.1481 (0.0587)	
training:	Epoch: [22][376/408]	Loss 0.0121 (0.0586)	
training:	Epoch: [22][377/408]	Loss 0.0117 (0.0584)	
training:	Epoch: [22][378/408]	Loss 0.0127 (0.0583)	
training:	Epoch: [22][379/408]	Loss 0.0115 (0.0582)	
training:	Epoch: [22][380/408]	Loss 0.2915 (0.0588)	
training:	Epoch: [22][381/408]	Loss 0.0129 (0.0587)	
training:	Epoch: [22][382/408]	Loss 0.0152 (0.0586)	
training:	Epoch: [22][383/408]	Loss 0.0115 (0.0585)	
training:	Epoch: [22][384/408]	Loss 0.0125 (0.0583)	
training:	Epoch: [22][385/408]	Loss 0.0113 (0.0582)	
training:	Epoch: [22][386/408]	Loss 0.2726 (0.0588)	
training:	Epoch: [22][387/408]	Loss 0.0145 (0.0586)	
training:	Epoch: [22][388/408]	Loss 0.2998 (0.0593)	
training:	Epoch: [22][389/408]	Loss 0.0116 (0.0591)	
training:	Epoch: [22][390/408]	Loss 0.2005 (0.0595)	
training:	Epoch: [22][391/408]	Loss 0.0113 (0.0594)	
training:	Epoch: [22][392/408]	Loss 0.0119 (0.0593)	
training:	Epoch: [22][393/408]	Loss 0.2850 (0.0598)	
training:	Epoch: [22][394/408]	Loss 0.0212 (0.0597)	
training:	Epoch: [22][395/408]	Loss 0.2975 (0.0603)	
training:	Epoch: [22][396/408]	Loss 0.0108 (0.0602)	
training:	Epoch: [22][397/408]	Loss 0.0110 (0.0601)	
training:	Epoch: [22][398/408]	Loss 0.0112 (0.0600)	
training:	Epoch: [22][399/408]	Loss 0.0106 (0.0598)	
training:	Epoch: [22][400/408]	Loss 0.0105 (0.0597)	
training:	Epoch: [22][401/408]	Loss 0.0107 (0.0596)	
training:	Epoch: [22][402/408]	Loss 0.0121 (0.0595)	
training:	Epoch: [22][403/408]	Loss 0.0110 (0.0594)	
training:	Epoch: [22][404/408]	Loss 0.0113 (0.0592)	
training:	Epoch: [22][405/408]	Loss 0.0117 (0.0591)	
training:	Epoch: [22][406/408]	Loss 0.0107 (0.0590)	
training:	Epoch: [22][407/408]	Loss 0.0197 (0.0589)	
training:	Epoch: [22][408/408]	Loss 0.0136 (0.0588)	
Training:	 Loss: 0.0587

Training:	 ACC: 0.9905 0.9905 0.9906 0.9904
Validation:	 ACC: 0.7843 0.7860 0.8209 0.7478
Validation:	 Best_BACC: 0.8226 0.8250 0.8772 0.7679
Validation:	 Loss: 0.8634
Pretraining:	Epoch 23/200
----------
training:	Epoch: [23][1/408]	Loss 0.0123 (0.0123)	
training:	Epoch: [23][2/408]	Loss 0.3097 (0.1610)	
training:	Epoch: [23][3/408]	Loss 0.0106 (0.1109)	
training:	Epoch: [23][4/408]	Loss 0.0107 (0.0858)	
training:	Epoch: [23][5/408]	Loss 0.0111 (0.0709)	
training:	Epoch: [23][6/408]	Loss 0.0107 (0.0609)	
training:	Epoch: [23][7/408]	Loss 0.0350 (0.0572)	
training:	Epoch: [23][8/408]	Loss 0.2928 (0.0866)	
training:	Epoch: [23][9/408]	Loss 0.1247 (0.0909)	
training:	Epoch: [23][10/408]	Loss 0.0098 (0.0827)	
training:	Epoch: [23][11/408]	Loss 0.0110 (0.0762)	
training:	Epoch: [23][12/408]	Loss 0.0116 (0.0708)	
training:	Epoch: [23][13/408]	Loss 0.0108 (0.0662)	
training:	Epoch: [23][14/408]	Loss 0.0113 (0.0623)	
training:	Epoch: [23][15/408]	Loss 0.0111 (0.0589)	
training:	Epoch: [23][16/408]	Loss 0.0120 (0.0560)	
training:	Epoch: [23][17/408]	Loss 0.0111 (0.0533)	
training:	Epoch: [23][18/408]	Loss 0.0134 (0.0511)	
training:	Epoch: [23][19/408]	Loss 0.0109 (0.0490)	
training:	Epoch: [23][20/408]	Loss 0.0117 (0.0471)	
training:	Epoch: [23][21/408]	Loss 0.0545 (0.0475)	
training:	Epoch: [23][22/408]	Loss 0.2877 (0.0584)	
training:	Epoch: [23][23/408]	Loss 0.2847 (0.0682)	
training:	Epoch: [23][24/408]	Loss 0.0114 (0.0659)	
training:	Epoch: [23][25/408]	Loss 0.0114 (0.0637)	
training:	Epoch: [23][26/408]	Loss 0.0135 (0.0618)	
training:	Epoch: [23][27/408]	Loss 0.0112 (0.0599)	
training:	Epoch: [23][28/408]	Loss 0.0114 (0.0582)	
training:	Epoch: [23][29/408]	Loss 0.0122 (0.0566)	
training:	Epoch: [23][30/408]	Loss 0.0835 (0.0575)	
training:	Epoch: [23][31/408]	Loss 0.0132 (0.0560)	
training:	Epoch: [23][32/408]	Loss 0.0145 (0.0547)	
training:	Epoch: [23][33/408]	Loss 0.0113 (0.0534)	
training:	Epoch: [23][34/408]	Loss 0.0104 (0.0522)	
training:	Epoch: [23][35/408]	Loss 0.0135 (0.0511)	
training:	Epoch: [23][36/408]	Loss 0.0105 (0.0499)	
training:	Epoch: [23][37/408]	Loss 0.0105 (0.0489)	
training:	Epoch: [23][38/408]	Loss 0.0107 (0.0479)	
training:	Epoch: [23][39/408]	Loss 0.0112 (0.0469)	
training:	Epoch: [23][40/408]	Loss 0.0123 (0.0461)	
training:	Epoch: [23][41/408]	Loss 0.2725 (0.0516)	
training:	Epoch: [23][42/408]	Loss 0.0105 (0.0506)	
training:	Epoch: [23][43/408]	Loss 0.0120 (0.0497)	
training:	Epoch: [23][44/408]	Loss 0.0141 (0.0489)	
training:	Epoch: [23][45/408]	Loss 0.0139 (0.0481)	
training:	Epoch: [23][46/408]	Loss 0.0129 (0.0473)	
training:	Epoch: [23][47/408]	Loss 0.0107 (0.0466)	
training:	Epoch: [23][48/408]	Loss 0.6221 (0.0586)	
training:	Epoch: [23][49/408]	Loss 0.0116 (0.0576)	
training:	Epoch: [23][50/408]	Loss 0.0109 (0.0567)	
training:	Epoch: [23][51/408]	Loss 0.0128 (0.0558)	
training:	Epoch: [23][52/408]	Loss 0.0111 (0.0549)	
training:	Epoch: [23][53/408]	Loss 0.0126 (0.0541)	
training:	Epoch: [23][54/408]	Loss 0.0112 (0.0534)	
training:	Epoch: [23][55/408]	Loss 0.0101 (0.0526)	
training:	Epoch: [23][56/408]	Loss 0.0103 (0.0518)	
training:	Epoch: [23][57/408]	Loss 0.0150 (0.0512)	
training:	Epoch: [23][58/408]	Loss 0.0100 (0.0505)	
training:	Epoch: [23][59/408]	Loss 0.2448 (0.0538)	
training:	Epoch: [23][60/408]	Loss 0.2731 (0.0574)	
training:	Epoch: [23][61/408]	Loss 0.0107 (0.0566)	
training:	Epoch: [23][62/408]	Loss 0.0114 (0.0559)	
training:	Epoch: [23][63/408]	Loss 0.0124 (0.0552)	
training:	Epoch: [23][64/408]	Loss 0.0200 (0.0547)	
training:	Epoch: [23][65/408]	Loss 0.0114 (0.0540)	
training:	Epoch: [23][66/408]	Loss 0.0104 (0.0533)	
training:	Epoch: [23][67/408]	Loss 0.2932 (0.0569)	
training:	Epoch: [23][68/408]	Loss 0.0115 (0.0563)	
training:	Epoch: [23][69/408]	Loss 0.0100 (0.0556)	
training:	Epoch: [23][70/408]	Loss 0.0119 (0.0550)	
training:	Epoch: [23][71/408]	Loss 0.1338 (0.0561)	
training:	Epoch: [23][72/408]	Loss 0.0167 (0.0555)	
training:	Epoch: [23][73/408]	Loss 0.0109 (0.0549)	
training:	Epoch: [23][74/408]	Loss 0.0115 (0.0543)	
training:	Epoch: [23][75/408]	Loss 0.0109 (0.0537)	
training:	Epoch: [23][76/408]	Loss 0.0115 (0.0532)	
training:	Epoch: [23][77/408]	Loss 0.0117 (0.0527)	
training:	Epoch: [23][78/408]	Loss 0.0112 (0.0521)	
training:	Epoch: [23][79/408]	Loss 0.3454 (0.0558)	
training:	Epoch: [23][80/408]	Loss 0.0105 (0.0553)	
training:	Epoch: [23][81/408]	Loss 0.0181 (0.0548)	
training:	Epoch: [23][82/408]	Loss 0.0121 (0.0543)	
training:	Epoch: [23][83/408]	Loss 0.0929 (0.0548)	
training:	Epoch: [23][84/408]	Loss 0.0219 (0.0544)	
training:	Epoch: [23][85/408]	Loss 0.0120 (0.0539)	
training:	Epoch: [23][86/408]	Loss 0.0114 (0.0534)	
training:	Epoch: [23][87/408]	Loss 0.0203 (0.0530)	
training:	Epoch: [23][88/408]	Loss 0.0131 (0.0525)	
training:	Epoch: [23][89/408]	Loss 0.0185 (0.0522)	
training:	Epoch: [23][90/408]	Loss 0.0124 (0.0517)	
training:	Epoch: [23][91/408]	Loss 0.0127 (0.0513)	
training:	Epoch: [23][92/408]	Loss 0.0136 (0.0509)	
training:	Epoch: [23][93/408]	Loss 0.0119 (0.0505)	
training:	Epoch: [23][94/408]	Loss 0.0171 (0.0501)	
training:	Epoch: [23][95/408]	Loss 0.3062 (0.0528)	
training:	Epoch: [23][96/408]	Loss 0.0109 (0.0524)	
training:	Epoch: [23][97/408]	Loss 0.2979 (0.0549)	
training:	Epoch: [23][98/408]	Loss 0.3164 (0.0576)	
training:	Epoch: [23][99/408]	Loss 0.0115 (0.0571)	
training:	Epoch: [23][100/408]	Loss 0.0120 (0.0566)	
training:	Epoch: [23][101/408]	Loss 0.0107 (0.0562)	
training:	Epoch: [23][102/408]	Loss 0.0216 (0.0559)	
training:	Epoch: [23][103/408]	Loss 0.0159 (0.0555)	
training:	Epoch: [23][104/408]	Loss 0.0115 (0.0550)	
training:	Epoch: [23][105/408]	Loss 0.0145 (0.0547)	
training:	Epoch: [23][106/408]	Loss 0.0107 (0.0542)	
training:	Epoch: [23][107/408]	Loss 0.3085 (0.0566)	
training:	Epoch: [23][108/408]	Loss 0.0116 (0.0562)	
training:	Epoch: [23][109/408]	Loss 0.0121 (0.0558)	
training:	Epoch: [23][110/408]	Loss 0.3135 (0.0581)	
training:	Epoch: [23][111/408]	Loss 0.2397 (0.0598)	
training:	Epoch: [23][112/408]	Loss 0.0109 (0.0593)	
training:	Epoch: [23][113/408]	Loss 0.0116 (0.0589)	
training:	Epoch: [23][114/408]	Loss 0.0119 (0.0585)	
training:	Epoch: [23][115/408]	Loss 0.0126 (0.0581)	
training:	Epoch: [23][116/408]	Loss 0.0103 (0.0577)	
training:	Epoch: [23][117/408]	Loss 0.2990 (0.0598)	
training:	Epoch: [23][118/408]	Loss 0.0144 (0.0594)	
training:	Epoch: [23][119/408]	Loss 0.0130 (0.0590)	
training:	Epoch: [23][120/408]	Loss 0.0110 (0.0586)	
training:	Epoch: [23][121/408]	Loss 0.0121 (0.0582)	
training:	Epoch: [23][122/408]	Loss 0.0098 (0.0578)	
training:	Epoch: [23][123/408]	Loss 0.0138 (0.0574)	
training:	Epoch: [23][124/408]	Loss 0.2625 (0.0591)	
training:	Epoch: [23][125/408]	Loss 0.0110 (0.0587)	
training:	Epoch: [23][126/408]	Loss 0.0107 (0.0583)	
training:	Epoch: [23][127/408]	Loss 0.0111 (0.0580)	
training:	Epoch: [23][128/408]	Loss 0.0879 (0.0582)	
training:	Epoch: [23][129/408]	Loss 0.0134 (0.0578)	
training:	Epoch: [23][130/408]	Loss 0.0141 (0.0575)	
training:	Epoch: [23][131/408]	Loss 0.0108 (0.0571)	
training:	Epoch: [23][132/408]	Loss 0.0837 (0.0574)	
training:	Epoch: [23][133/408]	Loss 0.1119 (0.0578)	
training:	Epoch: [23][134/408]	Loss 0.0116 (0.0574)	
training:	Epoch: [23][135/408]	Loss 0.0127 (0.0571)	
training:	Epoch: [23][136/408]	Loss 0.0110 (0.0567)	
training:	Epoch: [23][137/408]	Loss 0.2957 (0.0585)	
training:	Epoch: [23][138/408]	Loss 0.0119 (0.0582)	
training:	Epoch: [23][139/408]	Loss 0.0132 (0.0578)	
training:	Epoch: [23][140/408]	Loss 0.0108 (0.0575)	
training:	Epoch: [23][141/408]	Loss 0.0099 (0.0572)	
training:	Epoch: [23][142/408]	Loss 0.0108 (0.0568)	
training:	Epoch: [23][143/408]	Loss 0.2832 (0.0584)	
training:	Epoch: [23][144/408]	Loss 0.2835 (0.0600)	
training:	Epoch: [23][145/408]	Loss 0.0113 (0.0596)	
training:	Epoch: [23][146/408]	Loss 0.0254 (0.0594)	
training:	Epoch: [23][147/408]	Loss 0.0123 (0.0591)	
training:	Epoch: [23][148/408]	Loss 0.0109 (0.0588)	
training:	Epoch: [23][149/408]	Loss 0.2986 (0.0604)	
training:	Epoch: [23][150/408]	Loss 0.2851 (0.0619)	
training:	Epoch: [23][151/408]	Loss 0.0123 (0.0615)	
training:	Epoch: [23][152/408]	Loss 0.0157 (0.0612)	
training:	Epoch: [23][153/408]	Loss 0.2852 (0.0627)	
training:	Epoch: [23][154/408]	Loss 0.0177 (0.0624)	
training:	Epoch: [23][155/408]	Loss 0.0127 (0.0621)	
training:	Epoch: [23][156/408]	Loss 0.0891 (0.0623)	
training:	Epoch: [23][157/408]	Loss 0.0103 (0.0619)	
training:	Epoch: [23][158/408]	Loss 0.0104 (0.0616)	
training:	Epoch: [23][159/408]	Loss 0.3227 (0.0632)	
training:	Epoch: [23][160/408]	Loss 0.0535 (0.0632)	
training:	Epoch: [23][161/408]	Loss 0.0324 (0.0630)	
training:	Epoch: [23][162/408]	Loss 0.0109 (0.0627)	
training:	Epoch: [23][163/408]	Loss 0.0110 (0.0624)	
training:	Epoch: [23][164/408]	Loss 0.0106 (0.0620)	
training:	Epoch: [23][165/408]	Loss 0.0111 (0.0617)	
training:	Epoch: [23][166/408]	Loss 0.0106 (0.0614)	
training:	Epoch: [23][167/408]	Loss 0.1990 (0.0622)	
training:	Epoch: [23][168/408]	Loss 0.0178 (0.0620)	
training:	Epoch: [23][169/408]	Loss 0.0118 (0.0617)	
training:	Epoch: [23][170/408]	Loss 0.0542 (0.0616)	
training:	Epoch: [23][171/408]	Loss 0.0106 (0.0613)	
training:	Epoch: [23][172/408]	Loss 0.0113 (0.0611)	
training:	Epoch: [23][173/408]	Loss 0.2999 (0.0624)	
training:	Epoch: [23][174/408]	Loss 0.0160 (0.0622)	
training:	Epoch: [23][175/408]	Loss 0.0118 (0.0619)	
training:	Epoch: [23][176/408]	Loss 0.0114 (0.0616)	
training:	Epoch: [23][177/408]	Loss 0.0113 (0.0613)	
training:	Epoch: [23][178/408]	Loss 0.0105 (0.0610)	
training:	Epoch: [23][179/408]	Loss 0.0249 (0.0608)	
training:	Epoch: [23][180/408]	Loss 0.0105 (0.0605)	
training:	Epoch: [23][181/408]	Loss 0.0109 (0.0603)	
training:	Epoch: [23][182/408]	Loss 0.0190 (0.0600)	
training:	Epoch: [23][183/408]	Loss 0.0134 (0.0598)	
training:	Epoch: [23][184/408]	Loss 0.0095 (0.0595)	
training:	Epoch: [23][185/408]	Loss 0.2926 (0.0608)	
training:	Epoch: [23][186/408]	Loss 0.0131 (0.0605)	
training:	Epoch: [23][187/408]	Loss 0.0111 (0.0603)	
training:	Epoch: [23][188/408]	Loss 0.1687 (0.0608)	
training:	Epoch: [23][189/408]	Loss 0.0129 (0.0606)	
training:	Epoch: [23][190/408]	Loss 0.0133 (0.0603)	
training:	Epoch: [23][191/408]	Loss 0.0109 (0.0601)	
training:	Epoch: [23][192/408]	Loss 0.0141 (0.0598)	
training:	Epoch: [23][193/408]	Loss 0.0100 (0.0596)	
training:	Epoch: [23][194/408]	Loss 0.0104 (0.0593)	
training:	Epoch: [23][195/408]	Loss 0.2670 (0.0604)	
training:	Epoch: [23][196/408]	Loss 0.0103 (0.0601)	
training:	Epoch: [23][197/408]	Loss 0.0119 (0.0599)	
training:	Epoch: [23][198/408]	Loss 0.0395 (0.0598)	
training:	Epoch: [23][199/408]	Loss 0.0115 (0.0595)	
training:	Epoch: [23][200/408]	Loss 0.0115 (0.0593)	
training:	Epoch: [23][201/408]	Loss 0.0182 (0.0591)	
training:	Epoch: [23][202/408]	Loss 0.0107 (0.0589)	
training:	Epoch: [23][203/408]	Loss 0.0106 (0.0586)	
training:	Epoch: [23][204/408]	Loss 0.0111 (0.0584)	
training:	Epoch: [23][205/408]	Loss 0.0103 (0.0581)	
training:	Epoch: [23][206/408]	Loss 0.0109 (0.0579)	
training:	Epoch: [23][207/408]	Loss 0.0130 (0.0577)	
training:	Epoch: [23][208/408]	Loss 0.2829 (0.0588)	
training:	Epoch: [23][209/408]	Loss 0.0123 (0.0586)	
training:	Epoch: [23][210/408]	Loss 0.3092 (0.0598)	
training:	Epoch: [23][211/408]	Loss 0.5358 (0.0620)	
training:	Epoch: [23][212/408]	Loss 0.0103 (0.0618)	
training:	Epoch: [23][213/408]	Loss 0.0107 (0.0615)	
training:	Epoch: [23][214/408]	Loss 0.0152 (0.0613)	
training:	Epoch: [23][215/408]	Loss 0.0117 (0.0611)	
training:	Epoch: [23][216/408]	Loss 0.0105 (0.0608)	
training:	Epoch: [23][217/408]	Loss 0.0126 (0.0606)	
training:	Epoch: [23][218/408]	Loss 0.0115 (0.0604)	
training:	Epoch: [23][219/408]	Loss 0.0142 (0.0602)	
training:	Epoch: [23][220/408]	Loss 0.0113 (0.0600)	
training:	Epoch: [23][221/408]	Loss 0.0163 (0.0598)	
training:	Epoch: [23][222/408]	Loss 0.2880 (0.0608)	
training:	Epoch: [23][223/408]	Loss 0.0132 (0.0606)	
training:	Epoch: [23][224/408]	Loss 0.0116 (0.0604)	
training:	Epoch: [23][225/408]	Loss 0.0118 (0.0601)	
training:	Epoch: [23][226/408]	Loss 0.0116 (0.0599)	
training:	Epoch: [23][227/408]	Loss 0.0136 (0.0597)	
training:	Epoch: [23][228/408]	Loss 0.0117 (0.0595)	
training:	Epoch: [23][229/408]	Loss 0.1850 (0.0601)	
training:	Epoch: [23][230/408]	Loss 0.0113 (0.0599)	
training:	Epoch: [23][231/408]	Loss 0.0108 (0.0596)	
training:	Epoch: [23][232/408]	Loss 0.0117 (0.0594)	
training:	Epoch: [23][233/408]	Loss 0.3139 (0.0605)	
training:	Epoch: [23][234/408]	Loss 0.0122 (0.0603)	
training:	Epoch: [23][235/408]	Loss 0.0779 (0.0604)	
training:	Epoch: [23][236/408]	Loss 0.2423 (0.0612)	
training:	Epoch: [23][237/408]	Loss 0.2892 (0.0621)	
training:	Epoch: [23][238/408]	Loss 0.2810 (0.0630)	
training:	Epoch: [23][239/408]	Loss 0.0116 (0.0628)	
training:	Epoch: [23][240/408]	Loss 0.0108 (0.0626)	
training:	Epoch: [23][241/408]	Loss 0.0117 (0.0624)	
training:	Epoch: [23][242/408]	Loss 0.0113 (0.0622)	
training:	Epoch: [23][243/408]	Loss 0.0123 (0.0620)	
training:	Epoch: [23][244/408]	Loss 0.0121 (0.0618)	
training:	Epoch: [23][245/408]	Loss 0.0186 (0.0616)	
training:	Epoch: [23][246/408]	Loss 0.0181 (0.0614)	
training:	Epoch: [23][247/408]	Loss 0.0129 (0.0612)	
training:	Epoch: [23][248/408]	Loss 0.0136 (0.0610)	
training:	Epoch: [23][249/408]	Loss 0.0119 (0.0608)	
training:	Epoch: [23][250/408]	Loss 0.0158 (0.0607)	
training:	Epoch: [23][251/408]	Loss 0.0166 (0.0605)	
training:	Epoch: [23][252/408]	Loss 0.0106 (0.0603)	
training:	Epoch: [23][253/408]	Loss 0.0126 (0.0601)	
training:	Epoch: [23][254/408]	Loss 0.0156 (0.0599)	
training:	Epoch: [23][255/408]	Loss 0.0175 (0.0598)	
training:	Epoch: [23][256/408]	Loss 0.0180 (0.0596)	
training:	Epoch: [23][257/408]	Loss 0.0144 (0.0594)	
training:	Epoch: [23][258/408]	Loss 0.0141 (0.0592)	
training:	Epoch: [23][259/408]	Loss 0.0167 (0.0591)	
training:	Epoch: [23][260/408]	Loss 0.0133 (0.0589)	
training:	Epoch: [23][261/408]	Loss 0.0120 (0.0587)	
training:	Epoch: [23][262/408]	Loss 0.3067 (0.0597)	
training:	Epoch: [23][263/408]	Loss 0.0092 (0.0595)	
training:	Epoch: [23][264/408]	Loss 0.0113 (0.0593)	
training:	Epoch: [23][265/408]	Loss 0.0102 (0.0591)	
training:	Epoch: [23][266/408]	Loss 0.0123 (0.0589)	
training:	Epoch: [23][267/408]	Loss 0.0261 (0.0588)	
training:	Epoch: [23][268/408]	Loss 0.0200 (0.0587)	
training:	Epoch: [23][269/408]	Loss 0.0123 (0.0585)	
training:	Epoch: [23][270/408]	Loss 0.0162 (0.0583)	
training:	Epoch: [23][271/408]	Loss 0.0106 (0.0582)	
training:	Epoch: [23][272/408]	Loss 0.0163 (0.0580)	
training:	Epoch: [23][273/408]	Loss 0.0132 (0.0578)	
training:	Epoch: [23][274/408]	Loss 0.0120 (0.0577)	
training:	Epoch: [23][275/408]	Loss 0.0098 (0.0575)	
training:	Epoch: [23][276/408]	Loss 0.0152 (0.0573)	
training:	Epoch: [23][277/408]	Loss 0.0100 (0.0572)	
training:	Epoch: [23][278/408]	Loss 0.0110 (0.0570)	
training:	Epoch: [23][279/408]	Loss 0.0112 (0.0568)	
training:	Epoch: [23][280/408]	Loss 0.1302 (0.0571)	
training:	Epoch: [23][281/408]	Loss 0.0120 (0.0569)	
training:	Epoch: [23][282/408]	Loss 0.0113 (0.0568)	
training:	Epoch: [23][283/408]	Loss 0.0106 (0.0566)	
training:	Epoch: [23][284/408]	Loss 0.0110 (0.0565)	
training:	Epoch: [23][285/408]	Loss 0.0104 (0.0563)	
training:	Epoch: [23][286/408]	Loss 0.0121 (0.0561)	
training:	Epoch: [23][287/408]	Loss 0.0147 (0.0560)	
training:	Epoch: [23][288/408]	Loss 0.0606 (0.0560)	
training:	Epoch: [23][289/408]	Loss 0.0123 (0.0559)	
training:	Epoch: [23][290/408]	Loss 0.0352 (0.0558)	
training:	Epoch: [23][291/408]	Loss 0.0111 (0.0556)	
training:	Epoch: [23][292/408]	Loss 0.0100 (0.0555)	
training:	Epoch: [23][293/408]	Loss 0.0115 (0.0553)	
training:	Epoch: [23][294/408]	Loss 0.0221 (0.0552)	
training:	Epoch: [23][295/408]	Loss 0.0110 (0.0551)	
training:	Epoch: [23][296/408]	Loss 0.0101 (0.0549)	
training:	Epoch: [23][297/408]	Loss 0.0100 (0.0548)	
training:	Epoch: [23][298/408]	Loss 0.0146 (0.0546)	
training:	Epoch: [23][299/408]	Loss 0.0104 (0.0545)	
training:	Epoch: [23][300/408]	Loss 0.0116 (0.0543)	
training:	Epoch: [23][301/408]	Loss 0.0111 (0.0542)	
training:	Epoch: [23][302/408]	Loss 0.0103 (0.0541)	
training:	Epoch: [23][303/408]	Loss 0.0112 (0.0539)	
training:	Epoch: [23][304/408]	Loss 0.0106 (0.0538)	
training:	Epoch: [23][305/408]	Loss 0.0096 (0.0536)	
training:	Epoch: [23][306/408]	Loss 0.0106 (0.0535)	
training:	Epoch: [23][307/408]	Loss 0.0111 (0.0533)	
training:	Epoch: [23][308/408]	Loss 0.0096 (0.0532)	
training:	Epoch: [23][309/408]	Loss 0.1451 (0.0535)	
training:	Epoch: [23][310/408]	Loss 0.0118 (0.0534)	
training:	Epoch: [23][311/408]	Loss 0.0104 (0.0532)	
training:	Epoch: [23][312/408]	Loss 0.0114 (0.0531)	
training:	Epoch: [23][313/408]	Loss 0.0113 (0.0530)	
training:	Epoch: [23][314/408]	Loss 0.0129 (0.0528)	
training:	Epoch: [23][315/408]	Loss 0.0098 (0.0527)	
training:	Epoch: [23][316/408]	Loss 0.0099 (0.0526)	
training:	Epoch: [23][317/408]	Loss 0.0215 (0.0525)	
training:	Epoch: [23][318/408]	Loss 0.0107 (0.0523)	
training:	Epoch: [23][319/408]	Loss 0.0110 (0.0522)	
training:	Epoch: [23][320/408]	Loss 0.0119 (0.0521)	
training:	Epoch: [23][321/408]	Loss 0.0110 (0.0520)	
training:	Epoch: [23][322/408]	Loss 0.0099 (0.0518)	
training:	Epoch: [23][323/408]	Loss 0.0105 (0.0517)	
training:	Epoch: [23][324/408]	Loss 0.3100 (0.0525)	
training:	Epoch: [23][325/408]	Loss 0.3045 (0.0533)	
training:	Epoch: [23][326/408]	Loss 0.3054 (0.0540)	
training:	Epoch: [23][327/408]	Loss 0.0106 (0.0539)	
training:	Epoch: [23][328/408]	Loss 0.0101 (0.0538)	
training:	Epoch: [23][329/408]	Loss 0.0101 (0.0536)	
training:	Epoch: [23][330/408]	Loss 0.0136 (0.0535)	
training:	Epoch: [23][331/408]	Loss 0.5933 (0.0551)	
training:	Epoch: [23][332/408]	Loss 0.0104 (0.0550)	
training:	Epoch: [23][333/408]	Loss 0.0378 (0.0550)	
training:	Epoch: [23][334/408]	Loss 0.2809 (0.0556)	
training:	Epoch: [23][335/408]	Loss 0.0107 (0.0555)	
training:	Epoch: [23][336/408]	Loss 0.0196 (0.0554)	
training:	Epoch: [23][337/408]	Loss 0.0107 (0.0553)	
training:	Epoch: [23][338/408]	Loss 0.0102 (0.0551)	
training:	Epoch: [23][339/408]	Loss 0.0109 (0.0550)	
training:	Epoch: [23][340/408]	Loss 0.0101 (0.0549)	
training:	Epoch: [23][341/408]	Loss 0.0150 (0.0548)	
training:	Epoch: [23][342/408]	Loss 0.5729 (0.0563)	
training:	Epoch: [23][343/408]	Loss 0.0103 (0.0561)	
training:	Epoch: [23][344/408]	Loss 0.0121 (0.0560)	
training:	Epoch: [23][345/408]	Loss 0.2600 (0.0566)	
training:	Epoch: [23][346/408]	Loss 0.0099 (0.0565)	
training:	Epoch: [23][347/408]	Loss 0.0158 (0.0563)	
training:	Epoch: [23][348/408]	Loss 0.0109 (0.0562)	
training:	Epoch: [23][349/408]	Loss 0.0099 (0.0561)	
training:	Epoch: [23][350/408]	Loss 0.0112 (0.0560)	
training:	Epoch: [23][351/408]	Loss 0.0138 (0.0558)	
training:	Epoch: [23][352/408]	Loss 0.0133 (0.0557)	
training:	Epoch: [23][353/408]	Loss 0.0201 (0.0556)	
training:	Epoch: [23][354/408]	Loss 0.0155 (0.0555)	
training:	Epoch: [23][355/408]	Loss 0.0206 (0.0554)	
training:	Epoch: [23][356/408]	Loss 0.0149 (0.0553)	
training:	Epoch: [23][357/408]	Loss 0.0103 (0.0552)	
training:	Epoch: [23][358/408]	Loss 0.2992 (0.0558)	
training:	Epoch: [23][359/408]	Loss 0.0113 (0.0557)	
training:	Epoch: [23][360/408]	Loss 0.0111 (0.0556)	
training:	Epoch: [23][361/408]	Loss 0.0188 (0.0555)	
training:	Epoch: [23][362/408]	Loss 0.0118 (0.0554)	
training:	Epoch: [23][363/408]	Loss 0.0109 (0.0552)	
training:	Epoch: [23][364/408]	Loss 0.3000 (0.0559)	
training:	Epoch: [23][365/408]	Loss 0.0163 (0.0558)	
training:	Epoch: [23][366/408]	Loss 0.0109 (0.0557)	
training:	Epoch: [23][367/408]	Loss 0.0122 (0.0556)	
training:	Epoch: [23][368/408]	Loss 0.8407 (0.0577)	
training:	Epoch: [23][369/408]	Loss 0.0140 (0.0576)	
training:	Epoch: [23][370/408]	Loss 0.0160 (0.0575)	
training:	Epoch: [23][371/408]	Loss 0.0115 (0.0573)	
training:	Epoch: [23][372/408]	Loss 0.0107 (0.0572)	
training:	Epoch: [23][373/408]	Loss 0.0149 (0.0571)	
training:	Epoch: [23][374/408]	Loss 0.0098 (0.0570)	
training:	Epoch: [23][375/408]	Loss 0.2949 (0.0576)	
training:	Epoch: [23][376/408]	Loss 0.0111 (0.0575)	
training:	Epoch: [23][377/408]	Loss 0.2956 (0.0581)	
training:	Epoch: [23][378/408]	Loss 0.0123 (0.0580)	
training:	Epoch: [23][379/408]	Loss 0.2895 (0.0586)	
training:	Epoch: [23][380/408]	Loss 0.0116 (0.0585)	
training:	Epoch: [23][381/408]	Loss 0.0105 (0.0584)	
training:	Epoch: [23][382/408]	Loss 0.0128 (0.0582)	
training:	Epoch: [23][383/408]	Loss 0.0338 (0.0582)	
training:	Epoch: [23][384/408]	Loss 0.0133 (0.0581)	
training:	Epoch: [23][385/408]	Loss 0.0108 (0.0579)	
training:	Epoch: [23][386/408]	Loss 0.0124 (0.0578)	
training:	Epoch: [23][387/408]	Loss 0.2984 (0.0584)	
training:	Epoch: [23][388/408]	Loss 0.0176 (0.0583)	
training:	Epoch: [23][389/408]	Loss 0.0165 (0.0582)	
training:	Epoch: [23][390/408]	Loss 0.0111 (0.0581)	
training:	Epoch: [23][391/408]	Loss 0.0103 (0.0580)	
training:	Epoch: [23][392/408]	Loss 0.0104 (0.0579)	
training:	Epoch: [23][393/408]	Loss 0.2858 (0.0584)	
training:	Epoch: [23][394/408]	Loss 0.0177 (0.0583)	
training:	Epoch: [23][395/408]	Loss 0.0107 (0.0582)	
training:	Epoch: [23][396/408]	Loss 0.0111 (0.0581)	
training:	Epoch: [23][397/408]	Loss 0.1377 (0.0583)	
training:	Epoch: [23][398/408]	Loss 0.0118 (0.0582)	
training:	Epoch: [23][399/408]	Loss 0.0106 (0.0581)	
training:	Epoch: [23][400/408]	Loss 0.0102 (0.0580)	
training:	Epoch: [23][401/408]	Loss 0.0107 (0.0578)	
training:	Epoch: [23][402/408]	Loss 0.0106 (0.0577)	
training:	Epoch: [23][403/408]	Loss 0.0104 (0.0576)	
training:	Epoch: [23][404/408]	Loss 0.0106 (0.0575)	
training:	Epoch: [23][405/408]	Loss 0.0100 (0.0574)	
training:	Epoch: [23][406/408]	Loss 0.3962 (0.0582)	
training:	Epoch: [23][407/408]	Loss 0.2967 (0.0588)	
training:	Epoch: [23][408/408]	Loss 0.3095 (0.0594)	
Training:	 Loss: 0.0593

Training:	 ACC: 0.9902 0.9902 0.9906 0.9898
Validation:	 ACC: 0.7824 0.7849 0.8383 0.7265
Validation:	 Best_BACC: 0.8226 0.8250 0.8772 0.7679
Validation:	 Loss: 0.8762
Pretraining:	Epoch 24/200
----------
training:	Epoch: [24][1/408]	Loss 0.0126 (0.0126)	
training:	Epoch: [24][2/408]	Loss 0.3068 (0.1597)	
training:	Epoch: [24][3/408]	Loss 0.0107 (0.1100)	
training:	Epoch: [24][4/408]	Loss 0.0098 (0.0849)	
training:	Epoch: [24][5/408]	Loss 0.0113 (0.0702)	
training:	Epoch: [24][6/408]	Loss 0.0525 (0.0673)	
training:	Epoch: [24][7/408]	Loss 0.1457 (0.0785)	
training:	Epoch: [24][8/408]	Loss 0.0103 (0.0700)	
training:	Epoch: [24][9/408]	Loss 0.0108 (0.0634)	
training:	Epoch: [24][10/408]	Loss 0.0111 (0.0582)	
training:	Epoch: [24][11/408]	Loss 0.0096 (0.0537)	
training:	Epoch: [24][12/408]	Loss 0.0155 (0.0505)	
training:	Epoch: [24][13/408]	Loss 0.0114 (0.0475)	
training:	Epoch: [24][14/408]	Loss 0.0106 (0.0449)	
training:	Epoch: [24][15/408]	Loss 0.0123 (0.0427)	
training:	Epoch: [24][16/408]	Loss 0.0103 (0.0407)	
training:	Epoch: [24][17/408]	Loss 0.0400 (0.0407)	
training:	Epoch: [24][18/408]	Loss 0.0153 (0.0392)	
training:	Epoch: [24][19/408]	Loss 0.0295 (0.0387)	
training:	Epoch: [24][20/408]	Loss 0.2610 (0.0498)	
training:	Epoch: [24][21/408]	Loss 0.0109 (0.0480)	
training:	Epoch: [24][22/408]	Loss 0.0123 (0.0464)	
training:	Epoch: [24][23/408]	Loss 0.0108 (0.0448)	
training:	Epoch: [24][24/408]	Loss 0.0115 (0.0434)	
training:	Epoch: [24][25/408]	Loss 0.0203 (0.0425)	
training:	Epoch: [24][26/408]	Loss 0.0135 (0.0414)	
training:	Epoch: [24][27/408]	Loss 0.3059 (0.0512)	
training:	Epoch: [24][28/408]	Loss 0.0113 (0.0498)	
training:	Epoch: [24][29/408]	Loss 0.0107 (0.0484)	
training:	Epoch: [24][30/408]	Loss 0.0110 (0.0472)	
training:	Epoch: [24][31/408]	Loss 0.0111 (0.0460)	
training:	Epoch: [24][32/408]	Loss 0.1303 (0.0486)	
training:	Epoch: [24][33/408]	Loss 0.0098 (0.0475)	
training:	Epoch: [24][34/408]	Loss 0.2861 (0.0545)	
training:	Epoch: [24][35/408]	Loss 0.2912 (0.0612)	
training:	Epoch: [24][36/408]	Loss 0.0113 (0.0599)	
training:	Epoch: [24][37/408]	Loss 0.0187 (0.0587)	
training:	Epoch: [24][38/408]	Loss 0.0099 (0.0575)	
training:	Epoch: [24][39/408]	Loss 0.0139 (0.0563)	
training:	Epoch: [24][40/408]	Loss 0.0190 (0.0554)	
training:	Epoch: [24][41/408]	Loss 0.0108 (0.0543)	
training:	Epoch: [24][42/408]	Loss 0.0118 (0.0533)	
training:	Epoch: [24][43/408]	Loss 0.0101 (0.0523)	
training:	Epoch: [24][44/408]	Loss 0.0112 (0.0514)	
training:	Epoch: [24][45/408]	Loss 0.1907 (0.0545)	
training:	Epoch: [24][46/408]	Loss 0.1618 (0.0568)	
training:	Epoch: [24][47/408]	Loss 0.0105 (0.0558)	
training:	Epoch: [24][48/408]	Loss 0.0107 (0.0549)	
training:	Epoch: [24][49/408]	Loss 0.0100 (0.0540)	
training:	Epoch: [24][50/408]	Loss 0.0197 (0.0533)	
training:	Epoch: [24][51/408]	Loss 0.0102 (0.0524)	
training:	Epoch: [24][52/408]	Loss 0.5596 (0.0622)	
training:	Epoch: [24][53/408]	Loss 0.3027 (0.0667)	
training:	Epoch: [24][54/408]	Loss 0.0111 (0.0657)	
training:	Epoch: [24][55/408]	Loss 0.0099 (0.0647)	
training:	Epoch: [24][56/408]	Loss 0.0130 (0.0638)	
training:	Epoch: [24][57/408]	Loss 0.2762 (0.0675)	
training:	Epoch: [24][58/408]	Loss 0.0122 (0.0665)	
training:	Epoch: [24][59/408]	Loss 0.0108 (0.0656)	
training:	Epoch: [24][60/408]	Loss 0.0111 (0.0647)	
training:	Epoch: [24][61/408]	Loss 0.0108 (0.0638)	
training:	Epoch: [24][62/408]	Loss 0.0194 (0.0631)	
training:	Epoch: [24][63/408]	Loss 0.4136 (0.0686)	
training:	Epoch: [24][64/408]	Loss 0.0189 (0.0679)	
training:	Epoch: [24][65/408]	Loss 0.0106 (0.0670)	
training:	Epoch: [24][66/408]	Loss 0.0111 (0.0661)	
training:	Epoch: [24][67/408]	Loss 0.0116 (0.0653)	
training:	Epoch: [24][68/408]	Loss 0.0113 (0.0645)	
training:	Epoch: [24][69/408]	Loss 0.0109 (0.0638)	
training:	Epoch: [24][70/408]	Loss 0.0108 (0.0630)	
training:	Epoch: [24][71/408]	Loss 0.0114 (0.0623)	
training:	Epoch: [24][72/408]	Loss 0.0102 (0.0615)	
training:	Epoch: [24][73/408]	Loss 0.0104 (0.0608)	
training:	Epoch: [24][74/408]	Loss 0.0108 (0.0602)	
training:	Epoch: [24][75/408]	Loss 0.0103 (0.0595)	
training:	Epoch: [24][76/408]	Loss 0.0108 (0.0589)	
training:	Epoch: [24][77/408]	Loss 0.1470 (0.0600)	
training:	Epoch: [24][78/408]	Loss 0.0127 (0.0594)	
training:	Epoch: [24][79/408]	Loss 0.2883 (0.0623)	
training:	Epoch: [24][80/408]	Loss 0.0106 (0.0617)	
training:	Epoch: [24][81/408]	Loss 0.0483 (0.0615)	
training:	Epoch: [24][82/408]	Loss 0.0126 (0.0609)	
training:	Epoch: [24][83/408]	Loss 0.2901 (0.0637)	
training:	Epoch: [24][84/408]	Loss 0.0112 (0.0630)	
training:	Epoch: [24][85/408]	Loss 0.0271 (0.0626)	
training:	Epoch: [24][86/408]	Loss 0.0109 (0.0620)	
training:	Epoch: [24][87/408]	Loss 0.0116 (0.0614)	
training:	Epoch: [24][88/408]	Loss 0.0108 (0.0609)	
training:	Epoch: [24][89/408]	Loss 0.0109 (0.0603)	
training:	Epoch: [24][90/408]	Loss 0.0115 (0.0597)	
training:	Epoch: [24][91/408]	Loss 0.0117 (0.0592)	
training:	Epoch: [24][92/408]	Loss 0.0109 (0.0587)	
training:	Epoch: [24][93/408]	Loss 0.0104 (0.0582)	
training:	Epoch: [24][94/408]	Loss 0.0121 (0.0577)	
training:	Epoch: [24][95/408]	Loss 0.2874 (0.0601)	
training:	Epoch: [24][96/408]	Loss 0.0120 (0.0596)	
training:	Epoch: [24][97/408]	Loss 0.0108 (0.0591)	
training:	Epoch: [24][98/408]	Loss 0.0109 (0.0586)	
training:	Epoch: [24][99/408]	Loss 0.0110 (0.0581)	
training:	Epoch: [24][100/408]	Loss 0.3344 (0.0609)	
training:	Epoch: [24][101/408]	Loss 0.0121 (0.0604)	
training:	Epoch: [24][102/408]	Loss 0.0109 (0.0599)	
training:	Epoch: [24][103/408]	Loss 0.2883 (0.0621)	
training:	Epoch: [24][104/408]	Loss 0.0109 (0.0616)	
training:	Epoch: [24][105/408]	Loss 0.2982 (0.0639)	
training:	Epoch: [24][106/408]	Loss 0.0109 (0.0634)	
training:	Epoch: [24][107/408]	Loss 0.0112 (0.0629)	
training:	Epoch: [24][108/408]	Loss 0.0324 (0.0626)	
training:	Epoch: [24][109/408]	Loss 0.0146 (0.0622)	
training:	Epoch: [24][110/408]	Loss 0.0119 (0.0617)	
training:	Epoch: [24][111/408]	Loss 0.0101 (0.0613)	
training:	Epoch: [24][112/408]	Loss 0.0114 (0.0608)	
training:	Epoch: [24][113/408]	Loss 0.1107 (0.0613)	
training:	Epoch: [24][114/408]	Loss 0.0111 (0.0608)	
training:	Epoch: [24][115/408]	Loss 0.0151 (0.0604)	
training:	Epoch: [24][116/408]	Loss 0.3015 (0.0625)	
training:	Epoch: [24][117/408]	Loss 0.0111 (0.0621)	
training:	Epoch: [24][118/408]	Loss 0.0167 (0.0617)	
training:	Epoch: [24][119/408]	Loss 0.0110 (0.0613)	
training:	Epoch: [24][120/408]	Loss 0.0106 (0.0608)	
training:	Epoch: [24][121/408]	Loss 0.3049 (0.0628)	
training:	Epoch: [24][122/408]	Loss 0.0114 (0.0624)	
training:	Epoch: [24][123/408]	Loss 0.0117 (0.0620)	
training:	Epoch: [24][124/408]	Loss 0.2128 (0.0632)	
training:	Epoch: [24][125/408]	Loss 0.0113 (0.0628)	
training:	Epoch: [24][126/408]	Loss 0.0226 (0.0625)	
training:	Epoch: [24][127/408]	Loss 0.0123 (0.0621)	
training:	Epoch: [24][128/408]	Loss 0.0135 (0.0617)	
training:	Epoch: [24][129/408]	Loss 0.0110 (0.0613)	
training:	Epoch: [24][130/408]	Loss 0.0129 (0.0610)	
training:	Epoch: [24][131/408]	Loss 0.2862 (0.0627)	
training:	Epoch: [24][132/408]	Loss 0.0124 (0.0623)	
training:	Epoch: [24][133/408]	Loss 0.0126 (0.0619)	
training:	Epoch: [24][134/408]	Loss 0.0114 (0.0615)	
training:	Epoch: [24][135/408]	Loss 0.0101 (0.0612)	
training:	Epoch: [24][136/408]	Loss 0.0109 (0.0608)	
training:	Epoch: [24][137/408]	Loss 0.0111 (0.0604)	
training:	Epoch: [24][138/408]	Loss 0.0112 (0.0601)	
training:	Epoch: [24][139/408]	Loss 0.0105 (0.0597)	
training:	Epoch: [24][140/408]	Loss 0.0109 (0.0594)	
training:	Epoch: [24][141/408]	Loss 0.0106 (0.0590)	
training:	Epoch: [24][142/408]	Loss 0.0124 (0.0587)	
training:	Epoch: [24][143/408]	Loss 0.2565 (0.0601)	
training:	Epoch: [24][144/408]	Loss 0.0128 (0.0597)	
training:	Epoch: [24][145/408]	Loss 0.2120 (0.0608)	
training:	Epoch: [24][146/408]	Loss 0.0191 (0.0605)	
training:	Epoch: [24][147/408]	Loss 0.0108 (0.0602)	
training:	Epoch: [24][148/408]	Loss 0.0114 (0.0598)	
training:	Epoch: [24][149/408]	Loss 0.0151 (0.0595)	
training:	Epoch: [24][150/408]	Loss 0.0158 (0.0593)	
training:	Epoch: [24][151/408]	Loss 0.0163 (0.0590)	
training:	Epoch: [24][152/408]	Loss 0.1844 (0.0598)	
training:	Epoch: [24][153/408]	Loss 0.1360 (0.0603)	
training:	Epoch: [24][154/408]	Loss 0.0106 (0.0600)	
training:	Epoch: [24][155/408]	Loss 0.0141 (0.0597)	
training:	Epoch: [24][156/408]	Loss 0.0125 (0.0594)	
training:	Epoch: [24][157/408]	Loss 0.0115 (0.0591)	
training:	Epoch: [24][158/408]	Loss 0.0173 (0.0588)	
training:	Epoch: [24][159/408]	Loss 0.0492 (0.0587)	
training:	Epoch: [24][160/408]	Loss 0.2707 (0.0601)	
training:	Epoch: [24][161/408]	Loss 0.5685 (0.0632)	
training:	Epoch: [24][162/408]	Loss 0.0523 (0.0632)	
training:	Epoch: [24][163/408]	Loss 0.0112 (0.0628)	
training:	Epoch: [24][164/408]	Loss 0.0108 (0.0625)	
training:	Epoch: [24][165/408]	Loss 0.0121 (0.0622)	
training:	Epoch: [24][166/408]	Loss 0.0101 (0.0619)	
training:	Epoch: [24][167/408]	Loss 0.0102 (0.0616)	
training:	Epoch: [24][168/408]	Loss 0.0230 (0.0614)	
training:	Epoch: [24][169/408]	Loss 0.0586 (0.0613)	
training:	Epoch: [24][170/408]	Loss 0.2952 (0.0627)	
training:	Epoch: [24][171/408]	Loss 0.2885 (0.0640)	
training:	Epoch: [24][172/408]	Loss 0.0101 (0.0637)	
training:	Epoch: [24][173/408]	Loss 0.0117 (0.0634)	
training:	Epoch: [24][174/408]	Loss 0.0111 (0.0631)	
training:	Epoch: [24][175/408]	Loss 0.0130 (0.0628)	
training:	Epoch: [24][176/408]	Loss 0.0107 (0.0625)	
training:	Epoch: [24][177/408]	Loss 0.0108 (0.0623)	
training:	Epoch: [24][178/408]	Loss 0.0104 (0.0620)	
training:	Epoch: [24][179/408]	Loss 0.0107 (0.0617)	
training:	Epoch: [24][180/408]	Loss 0.0296 (0.0615)	
training:	Epoch: [24][181/408]	Loss 0.0264 (0.0613)	
training:	Epoch: [24][182/408]	Loss 0.0156 (0.0610)	
training:	Epoch: [24][183/408]	Loss 0.3010 (0.0624)	
training:	Epoch: [24][184/408]	Loss 0.0153 (0.0621)	
training:	Epoch: [24][185/408]	Loss 0.0117 (0.0618)	
training:	Epoch: [24][186/408]	Loss 0.0103 (0.0616)	
training:	Epoch: [24][187/408]	Loss 0.0107 (0.0613)	
training:	Epoch: [24][188/408]	Loss 0.0111 (0.0610)	
training:	Epoch: [24][189/408]	Loss 0.2913 (0.0622)	
training:	Epoch: [24][190/408]	Loss 0.0145 (0.0620)	
training:	Epoch: [24][191/408]	Loss 0.0104 (0.0617)	
training:	Epoch: [24][192/408]	Loss 0.0332 (0.0616)	
training:	Epoch: [24][193/408]	Loss 0.0157 (0.0613)	
training:	Epoch: [24][194/408]	Loss 0.0110 (0.0611)	
training:	Epoch: [24][195/408]	Loss 0.0114 (0.0608)	
training:	Epoch: [24][196/408]	Loss 0.0131 (0.0606)	
training:	Epoch: [24][197/408]	Loss 0.0110 (0.0603)	
training:	Epoch: [24][198/408]	Loss 0.0563 (0.0603)	
training:	Epoch: [24][199/408]	Loss 0.0105 (0.0600)	
training:	Epoch: [24][200/408]	Loss 0.0110 (0.0598)	
training:	Epoch: [24][201/408]	Loss 0.3005 (0.0610)	
training:	Epoch: [24][202/408]	Loss 0.2528 (0.0619)	
training:	Epoch: [24][203/408]	Loss 0.0095 (0.0617)	
training:	Epoch: [24][204/408]	Loss 0.0122 (0.0614)	
training:	Epoch: [24][205/408]	Loss 0.0124 (0.0612)	
training:	Epoch: [24][206/408]	Loss 0.0108 (0.0610)	
training:	Epoch: [24][207/408]	Loss 0.0118 (0.0607)	
training:	Epoch: [24][208/408]	Loss 0.0106 (0.0605)	
training:	Epoch: [24][209/408]	Loss 0.0135 (0.0603)	
training:	Epoch: [24][210/408]	Loss 0.1062 (0.0605)	
training:	Epoch: [24][211/408]	Loss 0.0153 (0.0603)	
training:	Epoch: [24][212/408]	Loss 0.0112 (0.0600)	
training:	Epoch: [24][213/408]	Loss 0.0123 (0.0598)	
training:	Epoch: [24][214/408]	Loss 0.0098 (0.0596)	
training:	Epoch: [24][215/408]	Loss 0.0128 (0.0594)	
training:	Epoch: [24][216/408]	Loss 0.0112 (0.0591)	
training:	Epoch: [24][217/408]	Loss 0.0107 (0.0589)	
training:	Epoch: [24][218/408]	Loss 0.0105 (0.0587)	
training:	Epoch: [24][219/408]	Loss 0.0975 (0.0589)	
training:	Epoch: [24][220/408]	Loss 0.0114 (0.0587)	
training:	Epoch: [24][221/408]	Loss 0.0282 (0.0585)	
training:	Epoch: [24][222/408]	Loss 0.3022 (0.0596)	
training:	Epoch: [24][223/408]	Loss 0.0111 (0.0594)	
training:	Epoch: [24][224/408]	Loss 0.0593 (0.0594)	
training:	Epoch: [24][225/408]	Loss 0.0111 (0.0592)	
training:	Epoch: [24][226/408]	Loss 0.0101 (0.0590)	
training:	Epoch: [24][227/408]	Loss 0.0162 (0.0588)	
training:	Epoch: [24][228/408]	Loss 0.0275 (0.0586)	
training:	Epoch: [24][229/408]	Loss 0.0102 (0.0584)	
training:	Epoch: [24][230/408]	Loss 0.0242 (0.0583)	
training:	Epoch: [24][231/408]	Loss 0.0314 (0.0582)	
training:	Epoch: [24][232/408]	Loss 0.0106 (0.0580)	
training:	Epoch: [24][233/408]	Loss 0.0096 (0.0577)	
training:	Epoch: [24][234/408]	Loss 0.2791 (0.0587)	
training:	Epoch: [24][235/408]	Loss 0.0109 (0.0585)	
training:	Epoch: [24][236/408]	Loss 0.0158 (0.0583)	
training:	Epoch: [24][237/408]	Loss 0.0121 (0.0581)	
training:	Epoch: [24][238/408]	Loss 0.0101 (0.0579)	
training:	Epoch: [24][239/408]	Loss 0.0137 (0.0577)	
training:	Epoch: [24][240/408]	Loss 0.0114 (0.0575)	
training:	Epoch: [24][241/408]	Loss 0.0094 (0.0573)	
training:	Epoch: [24][242/408]	Loss 0.0095 (0.0571)	
training:	Epoch: [24][243/408]	Loss 0.0113 (0.0569)	
training:	Epoch: [24][244/408]	Loss 0.0104 (0.0568)	
training:	Epoch: [24][245/408]	Loss 0.0122 (0.0566)	
training:	Epoch: [24][246/408]	Loss 0.0107 (0.0564)	
training:	Epoch: [24][247/408]	Loss 0.0107 (0.0562)	
training:	Epoch: [24][248/408]	Loss 0.0100 (0.0560)	
training:	Epoch: [24][249/408]	Loss 0.0156 (0.0559)	
training:	Epoch: [24][250/408]	Loss 0.0133 (0.0557)	
training:	Epoch: [24][251/408]	Loss 0.0201 (0.0555)	
training:	Epoch: [24][252/408]	Loss 0.0102 (0.0554)	
training:	Epoch: [24][253/408]	Loss 0.0106 (0.0552)	
training:	Epoch: [24][254/408]	Loss 0.0115 (0.0550)	
training:	Epoch: [24][255/408]	Loss 0.0175 (0.0549)	
training:	Epoch: [24][256/408]	Loss 0.0190 (0.0547)	
training:	Epoch: [24][257/408]	Loss 0.0110 (0.0546)	
training:	Epoch: [24][258/408]	Loss 0.0113 (0.0544)	
training:	Epoch: [24][259/408]	Loss 0.0127 (0.0542)	
training:	Epoch: [24][260/408]	Loss 0.2839 (0.0551)	
training:	Epoch: [24][261/408]	Loss 0.0105 (0.0549)	
training:	Epoch: [24][262/408]	Loss 0.0111 (0.0548)	
training:	Epoch: [24][263/408]	Loss 0.0106 (0.0546)	
training:	Epoch: [24][264/408]	Loss 0.0115 (0.0544)	
training:	Epoch: [24][265/408]	Loss 0.2696 (0.0553)	
training:	Epoch: [24][266/408]	Loss 0.0133 (0.0551)	
training:	Epoch: [24][267/408]	Loss 0.0119 (0.0549)	
training:	Epoch: [24][268/408]	Loss 0.0111 (0.0548)	
training:	Epoch: [24][269/408]	Loss 0.0099 (0.0546)	
training:	Epoch: [24][270/408]	Loss 0.0109 (0.0544)	
training:	Epoch: [24][271/408]	Loss 0.3089 (0.0554)	
training:	Epoch: [24][272/408]	Loss 0.0105 (0.0552)	
training:	Epoch: [24][273/408]	Loss 0.0111 (0.0551)	
training:	Epoch: [24][274/408]	Loss 0.2675 (0.0558)	
training:	Epoch: [24][275/408]	Loss 0.2929 (0.0567)	
training:	Epoch: [24][276/408]	Loss 0.0105 (0.0565)	
training:	Epoch: [24][277/408]	Loss 0.2998 (0.0574)	
training:	Epoch: [24][278/408]	Loss 0.0134 (0.0572)	
training:	Epoch: [24][279/408]	Loss 0.0122 (0.0571)	
training:	Epoch: [24][280/408]	Loss 0.0118 (0.0569)	
training:	Epoch: [24][281/408]	Loss 0.2847 (0.0577)	
training:	Epoch: [24][282/408]	Loss 0.0131 (0.0576)	
training:	Epoch: [24][283/408]	Loss 0.0109 (0.0574)	
training:	Epoch: [24][284/408]	Loss 0.0109 (0.0572)	
training:	Epoch: [24][285/408]	Loss 0.0135 (0.0571)	
training:	Epoch: [24][286/408]	Loss 0.0112 (0.0569)	
training:	Epoch: [24][287/408]	Loss 0.0102 (0.0568)	
training:	Epoch: [24][288/408]	Loss 0.0118 (0.0566)	
training:	Epoch: [24][289/408]	Loss 0.0132 (0.0565)	
training:	Epoch: [24][290/408]	Loss 0.0103 (0.0563)	
training:	Epoch: [24][291/408]	Loss 0.0110 (0.0561)	
training:	Epoch: [24][292/408]	Loss 0.0109 (0.0560)	
training:	Epoch: [24][293/408]	Loss 0.0149 (0.0559)	
training:	Epoch: [24][294/408]	Loss 0.0104 (0.0557)	
training:	Epoch: [24][295/408]	Loss 0.2685 (0.0564)	
training:	Epoch: [24][296/408]	Loss 0.0119 (0.0563)	
training:	Epoch: [24][297/408]	Loss 0.0107 (0.0561)	
training:	Epoch: [24][298/408]	Loss 0.0105 (0.0560)	
training:	Epoch: [24][299/408]	Loss 0.0106 (0.0558)	
training:	Epoch: [24][300/408]	Loss 0.0097 (0.0557)	
training:	Epoch: [24][301/408]	Loss 0.0256 (0.0556)	
training:	Epoch: [24][302/408]	Loss 0.0109 (0.0554)	
training:	Epoch: [24][303/408]	Loss 0.0099 (0.0553)	
training:	Epoch: [24][304/408]	Loss 0.0110 (0.0551)	
training:	Epoch: [24][305/408]	Loss 0.2819 (0.0559)	
training:	Epoch: [24][306/408]	Loss 0.0109 (0.0557)	
training:	Epoch: [24][307/408]	Loss 0.0127 (0.0556)	
training:	Epoch: [24][308/408]	Loss 0.0113 (0.0554)	
training:	Epoch: [24][309/408]	Loss 0.0121 (0.0553)	
training:	Epoch: [24][310/408]	Loss 0.0120 (0.0551)	
training:	Epoch: [24][311/408]	Loss 0.0421 (0.0551)	
training:	Epoch: [24][312/408]	Loss 0.0106 (0.0550)	
training:	Epoch: [24][313/408]	Loss 0.0102 (0.0548)	
training:	Epoch: [24][314/408]	Loss 0.0101 (0.0547)	
training:	Epoch: [24][315/408]	Loss 0.0120 (0.0545)	
training:	Epoch: [24][316/408]	Loss 0.0131 (0.0544)	
training:	Epoch: [24][317/408]	Loss 0.0110 (0.0543)	
training:	Epoch: [24][318/408]	Loss 0.2956 (0.0550)	
training:	Epoch: [24][319/408]	Loss 0.0096 (0.0549)	
training:	Epoch: [24][320/408]	Loss 0.0120 (0.0548)	
training:	Epoch: [24][321/408]	Loss 0.3013 (0.0555)	
training:	Epoch: [24][322/408]	Loss 0.0152 (0.0554)	
training:	Epoch: [24][323/408]	Loss 0.2442 (0.0560)	
training:	Epoch: [24][324/408]	Loss 0.0104 (0.0558)	
training:	Epoch: [24][325/408]	Loss 0.3134 (0.0566)	
training:	Epoch: [24][326/408]	Loss 0.0104 (0.0565)	
training:	Epoch: [24][327/408]	Loss 0.0183 (0.0564)	
training:	Epoch: [24][328/408]	Loss 0.0269 (0.0563)	
training:	Epoch: [24][329/408]	Loss 0.0437 (0.0562)	
training:	Epoch: [24][330/408]	Loss 0.0185 (0.0561)	
training:	Epoch: [24][331/408]	Loss 0.0192 (0.0560)	
training:	Epoch: [24][332/408]	Loss 0.0107 (0.0559)	
training:	Epoch: [24][333/408]	Loss 0.0095 (0.0557)	
training:	Epoch: [24][334/408]	Loss 0.0154 (0.0556)	
training:	Epoch: [24][335/408]	Loss 0.0103 (0.0555)	
training:	Epoch: [24][336/408]	Loss 0.0110 (0.0554)	
training:	Epoch: [24][337/408]	Loss 0.0558 (0.0554)	
training:	Epoch: [24][338/408]	Loss 0.2860 (0.0560)	
training:	Epoch: [24][339/408]	Loss 0.2923 (0.0567)	
training:	Epoch: [24][340/408]	Loss 0.0104 (0.0566)	
training:	Epoch: [24][341/408]	Loss 0.0105 (0.0565)	
training:	Epoch: [24][342/408]	Loss 0.0108 (0.0563)	
training:	Epoch: [24][343/408]	Loss 0.0097 (0.0562)	
training:	Epoch: [24][344/408]	Loss 0.0108 (0.0561)	
training:	Epoch: [24][345/408]	Loss 0.0102 (0.0559)	
training:	Epoch: [24][346/408]	Loss 0.0120 (0.0558)	
training:	Epoch: [24][347/408]	Loss 0.0106 (0.0557)	
training:	Epoch: [24][348/408]	Loss 0.0101 (0.0555)	
training:	Epoch: [24][349/408]	Loss 0.0122 (0.0554)	
training:	Epoch: [24][350/408]	Loss 0.0109 (0.0553)	
training:	Epoch: [24][351/408]	Loss 0.0104 (0.0552)	
training:	Epoch: [24][352/408]	Loss 0.2631 (0.0558)	
training:	Epoch: [24][353/408]	Loss 0.0120 (0.0556)	
training:	Epoch: [24][354/408]	Loss 0.0137 (0.0555)	
training:	Epoch: [24][355/408]	Loss 0.0791 (0.0556)	
training:	Epoch: [24][356/408]	Loss 0.0118 (0.0555)	
training:	Epoch: [24][357/408]	Loss 0.0141 (0.0553)	
training:	Epoch: [24][358/408]	Loss 0.0314 (0.0553)	
training:	Epoch: [24][359/408]	Loss 0.0117 (0.0552)	
training:	Epoch: [24][360/408]	Loss 0.0114 (0.0550)	
training:	Epoch: [24][361/408]	Loss 0.0372 (0.0550)	
training:	Epoch: [24][362/408]	Loss 0.2668 (0.0556)	
training:	Epoch: [24][363/408]	Loss 0.3052 (0.0563)	
training:	Epoch: [24][364/408]	Loss 0.3045 (0.0569)	
training:	Epoch: [24][365/408]	Loss 0.3045 (0.0576)	
training:	Epoch: [24][366/408]	Loss 0.0100 (0.0575)	
training:	Epoch: [24][367/408]	Loss 0.0100 (0.0574)	
training:	Epoch: [24][368/408]	Loss 0.0129 (0.0572)	
training:	Epoch: [24][369/408]	Loss 0.0111 (0.0571)	
training:	Epoch: [24][370/408]	Loss 0.0105 (0.0570)	
training:	Epoch: [24][371/408]	Loss 0.0114 (0.0569)	
training:	Epoch: [24][372/408]	Loss 0.0131 (0.0567)	
training:	Epoch: [24][373/408]	Loss 0.0128 (0.0566)	
training:	Epoch: [24][374/408]	Loss 0.0120 (0.0565)	
training:	Epoch: [24][375/408]	Loss 0.0105 (0.0564)	
training:	Epoch: [24][376/408]	Loss 0.0100 (0.0563)	
training:	Epoch: [24][377/408]	Loss 0.0110 (0.0561)	
training:	Epoch: [24][378/408]	Loss 0.0115 (0.0560)	
training:	Epoch: [24][379/408]	Loss 0.0096 (0.0559)	
training:	Epoch: [24][380/408]	Loss 0.2325 (0.0564)	
training:	Epoch: [24][381/408]	Loss 0.3124 (0.0570)	
training:	Epoch: [24][382/408]	Loss 0.2873 (0.0576)	
training:	Epoch: [24][383/408]	Loss 0.0112 (0.0575)	
training:	Epoch: [24][384/408]	Loss 0.0111 (0.0574)	
training:	Epoch: [24][385/408]	Loss 0.0135 (0.0573)	
training:	Epoch: [24][386/408]	Loss 0.0101 (0.0572)	
training:	Epoch: [24][387/408]	Loss 0.0202 (0.0571)	
training:	Epoch: [24][388/408]	Loss 0.0114 (0.0569)	
training:	Epoch: [24][389/408]	Loss 0.0170 (0.0568)	
training:	Epoch: [24][390/408]	Loss 0.0925 (0.0569)	
training:	Epoch: [24][391/408]	Loss 0.2751 (0.0575)	
training:	Epoch: [24][392/408]	Loss 0.0119 (0.0574)	
training:	Epoch: [24][393/408]	Loss 0.0108 (0.0573)	
training:	Epoch: [24][394/408]	Loss 0.2734 (0.0578)	
training:	Epoch: [24][395/408]	Loss 0.0121 (0.0577)	
training:	Epoch: [24][396/408]	Loss 0.0103 (0.0576)	
training:	Epoch: [24][397/408]	Loss 0.0128 (0.0575)	
training:	Epoch: [24][398/408]	Loss 0.0126 (0.0573)	
training:	Epoch: [24][399/408]	Loss 0.0118 (0.0572)	
training:	Epoch: [24][400/408]	Loss 0.3033 (0.0578)	
training:	Epoch: [24][401/408]	Loss 0.0182 (0.0577)	
training:	Epoch: [24][402/408]	Loss 0.0118 (0.0576)	
training:	Epoch: [24][403/408]	Loss 0.0124 (0.0575)	
training:	Epoch: [24][404/408]	Loss 0.0884 (0.0576)	
training:	Epoch: [24][405/408]	Loss 0.0569 (0.0576)	
training:	Epoch: [24][406/408]	Loss 0.3119 (0.0582)	
training:	Epoch: [24][407/408]	Loss 0.0103 (0.0581)	
training:	Epoch: [24][408/408]	Loss 0.0111 (0.0580)	
Training:	 Loss: 0.0579

Training:	 ACC: 0.9913 0.9913 0.9909 0.9917
Validation:	 ACC: 0.7890 0.7897 0.8045 0.7735
Validation:	 Best_BACC: 0.8226 0.8250 0.8772 0.7679
Validation:	 Loss: 0.8498
Pretraining:	Epoch 25/200
----------
training:	Epoch: [25][1/408]	Loss 0.0102 (0.0102)	
training:	Epoch: [25][2/408]	Loss 0.0116 (0.0109)	
training:	Epoch: [25][3/408]	Loss 0.0108 (0.0109)	
training:	Epoch: [25][4/408]	Loss 0.2305 (0.0658)	
training:	Epoch: [25][5/408]	Loss 0.0135 (0.0553)	
training:	Epoch: [25][6/408]	Loss 0.0104 (0.0478)	
training:	Epoch: [25][7/408]	Loss 0.0618 (0.0498)	
training:	Epoch: [25][8/408]	Loss 0.0805 (0.0537)	
training:	Epoch: [25][9/408]	Loss 0.2980 (0.0808)	
training:	Epoch: [25][10/408]	Loss 0.2949 (0.1022)	
training:	Epoch: [25][11/408]	Loss 0.0106 (0.0939)	
training:	Epoch: [25][12/408]	Loss 0.2840 (0.1097)	
training:	Epoch: [25][13/408]	Loss 0.0238 (0.1031)	
training:	Epoch: [25][14/408]	Loss 0.0127 (0.0967)	
training:	Epoch: [25][15/408]	Loss 0.0118 (0.0910)	
training:	Epoch: [25][16/408]	Loss 0.0133 (0.0862)	
training:	Epoch: [25][17/408]	Loss 0.0100 (0.0817)	
training:	Epoch: [25][18/408]	Loss 0.0138 (0.0779)	
training:	Epoch: [25][19/408]	Loss 0.0123 (0.0744)	
training:	Epoch: [25][20/408]	Loss 0.0093 (0.0712)	
training:	Epoch: [25][21/408]	Loss 0.0128 (0.0684)	
training:	Epoch: [25][22/408]	Loss 0.2864 (0.0783)	
training:	Epoch: [25][23/408]	Loss 0.0114 (0.0754)	
training:	Epoch: [25][24/408]	Loss 0.0115 (0.0727)	
training:	Epoch: [25][25/408]	Loss 0.0117 (0.0703)	
training:	Epoch: [25][26/408]	Loss 0.0128 (0.0681)	
training:	Epoch: [25][27/408]	Loss 0.0102 (0.0660)	
training:	Epoch: [25][28/408]	Loss 0.0106 (0.0640)	
training:	Epoch: [25][29/408]	Loss 0.0136 (0.0622)	
training:	Epoch: [25][30/408]	Loss 0.1635 (0.0656)	
training:	Epoch: [25][31/408]	Loss 0.0121 (0.0639)	
training:	Epoch: [25][32/408]	Loss 0.0130 (0.0623)	
training:	Epoch: [25][33/408]	Loss 0.0119 (0.0608)	
training:	Epoch: [25][34/408]	Loss 0.3059 (0.0680)	
training:	Epoch: [25][35/408]	Loss 0.0107 (0.0663)	
training:	Epoch: [25][36/408]	Loss 0.0170 (0.0650)	
training:	Epoch: [25][37/408]	Loss 0.0107 (0.0635)	
training:	Epoch: [25][38/408]	Loss 0.0118 (0.0621)	
training:	Epoch: [25][39/408]	Loss 0.0132 (0.0609)	
training:	Epoch: [25][40/408]	Loss 0.2873 (0.0665)	
training:	Epoch: [25][41/408]	Loss 0.0130 (0.0652)	
training:	Epoch: [25][42/408]	Loss 0.0124 (0.0640)	
training:	Epoch: [25][43/408]	Loss 0.2957 (0.0694)	
training:	Epoch: [25][44/408]	Loss 0.0162 (0.0682)	
training:	Epoch: [25][45/408]	Loss 0.0103 (0.0669)	
training:	Epoch: [25][46/408]	Loss 0.0100 (0.0656)	
training:	Epoch: [25][47/408]	Loss 0.0126 (0.0645)	
training:	Epoch: [25][48/408]	Loss 0.0128 (0.0634)	
training:	Epoch: [25][49/408]	Loss 0.0123 (0.0624)	
training:	Epoch: [25][50/408]	Loss 0.0112 (0.0614)	
training:	Epoch: [25][51/408]	Loss 0.0135 (0.0604)	
training:	Epoch: [25][52/408]	Loss 0.0109 (0.0595)	
training:	Epoch: [25][53/408]	Loss 0.0128 (0.0586)	
training:	Epoch: [25][54/408]	Loss 0.0121 (0.0577)	
training:	Epoch: [25][55/408]	Loss 0.0111 (0.0569)	
training:	Epoch: [25][56/408]	Loss 0.0105 (0.0561)	
training:	Epoch: [25][57/408]	Loss 0.0094 (0.0552)	
training:	Epoch: [25][58/408]	Loss 0.0105 (0.0545)	
training:	Epoch: [25][59/408]	Loss 0.0102 (0.0537)	
training:	Epoch: [25][60/408]	Loss 0.0106 (0.0530)	
training:	Epoch: [25][61/408]	Loss 0.0132 (0.0523)	
training:	Epoch: [25][62/408]	Loss 0.0115 (0.0517)	
training:	Epoch: [25][63/408]	Loss 0.0115 (0.0510)	
training:	Epoch: [25][64/408]	Loss 0.0098 (0.0504)	
training:	Epoch: [25][65/408]	Loss 0.0110 (0.0498)	
training:	Epoch: [25][66/408]	Loss 0.0110 (0.0492)	
training:	Epoch: [25][67/408]	Loss 0.0112 (0.0486)	
training:	Epoch: [25][68/408]	Loss 0.0188 (0.0482)	
training:	Epoch: [25][69/408]	Loss 0.2972 (0.0518)	
training:	Epoch: [25][70/408]	Loss 0.0105 (0.0512)	
training:	Epoch: [25][71/408]	Loss 0.2858 (0.0545)	
training:	Epoch: [25][72/408]	Loss 0.0290 (0.0542)	
training:	Epoch: [25][73/408]	Loss 0.0116 (0.0536)	
training:	Epoch: [25][74/408]	Loss 0.0760 (0.0539)	
training:	Epoch: [25][75/408]	Loss 0.0111 (0.0533)	
training:	Epoch: [25][76/408]	Loss 0.0107 (0.0528)	
training:	Epoch: [25][77/408]	Loss 0.0099 (0.0522)	
training:	Epoch: [25][78/408]	Loss 0.0104 (0.0517)	
training:	Epoch: [25][79/408]	Loss 0.0133 (0.0512)	
training:	Epoch: [25][80/408]	Loss 0.0113 (0.0507)	
training:	Epoch: [25][81/408]	Loss 0.0113 (0.0502)	
training:	Epoch: [25][82/408]	Loss 0.0119 (0.0497)	
training:	Epoch: [25][83/408]	Loss 0.0101 (0.0493)	
training:	Epoch: [25][84/408]	Loss 0.0102 (0.0488)	
training:	Epoch: [25][85/408]	Loss 0.2896 (0.0516)	
training:	Epoch: [25][86/408]	Loss 0.2720 (0.0542)	
training:	Epoch: [25][87/408]	Loss 0.0137 (0.0537)	
training:	Epoch: [25][88/408]	Loss 0.0104 (0.0532)	
training:	Epoch: [25][89/408]	Loss 0.0112 (0.0528)	
training:	Epoch: [25][90/408]	Loss 0.0109 (0.0523)	
training:	Epoch: [25][91/408]	Loss 0.0123 (0.0518)	
training:	Epoch: [25][92/408]	Loss 0.0113 (0.0514)	
training:	Epoch: [25][93/408]	Loss 0.0119 (0.0510)	
training:	Epoch: [25][94/408]	Loss 0.0103 (0.0506)	
training:	Epoch: [25][95/408]	Loss 0.0117 (0.0501)	
training:	Epoch: [25][96/408]	Loss 0.0110 (0.0497)	
training:	Epoch: [25][97/408]	Loss 0.0171 (0.0494)	
training:	Epoch: [25][98/408]	Loss 0.0311 (0.0492)	
training:	Epoch: [25][99/408]	Loss 0.1282 (0.0500)	
training:	Epoch: [25][100/408]	Loss 0.0116 (0.0496)	
training:	Epoch: [25][101/408]	Loss 0.0129 (0.0493)	
training:	Epoch: [25][102/408]	Loss 0.0143 (0.0489)	
training:	Epoch: [25][103/408]	Loss 0.0097 (0.0485)	
training:	Epoch: [25][104/408]	Loss 0.0100 (0.0482)	
training:	Epoch: [25][105/408]	Loss 0.0116 (0.0478)	
training:	Epoch: [25][106/408]	Loss 0.0118 (0.0475)	
training:	Epoch: [25][107/408]	Loss 0.0114 (0.0471)	
training:	Epoch: [25][108/408]	Loss 0.2992 (0.0495)	
training:	Epoch: [25][109/408]	Loss 0.0105 (0.0491)	
training:	Epoch: [25][110/408]	Loss 0.2341 (0.0508)	
training:	Epoch: [25][111/408]	Loss 0.0106 (0.0504)	
training:	Epoch: [25][112/408]	Loss 0.0110 (0.0501)	
training:	Epoch: [25][113/408]	Loss 0.0214 (0.0498)	
training:	Epoch: [25][114/408]	Loss 0.0107 (0.0495)	
training:	Epoch: [25][115/408]	Loss 0.0102 (0.0491)	
training:	Epoch: [25][116/408]	Loss 0.0108 (0.0488)	
training:	Epoch: [25][117/408]	Loss 0.0103 (0.0485)	
training:	Epoch: [25][118/408]	Loss 0.2933 (0.0506)	
training:	Epoch: [25][119/408]	Loss 0.0136 (0.0503)	
training:	Epoch: [25][120/408]	Loss 0.0110 (0.0499)	
training:	Epoch: [25][121/408]	Loss 0.0102 (0.0496)	
training:	Epoch: [25][122/408]	Loss 0.0108 (0.0493)	
training:	Epoch: [25][123/408]	Loss 0.0094 (0.0490)	
training:	Epoch: [25][124/408]	Loss 0.0105 (0.0486)	
training:	Epoch: [25][125/408]	Loss 0.0120 (0.0484)	
training:	Epoch: [25][126/408]	Loss 0.0118 (0.0481)	
training:	Epoch: [25][127/408]	Loss 0.0112 (0.0478)	
training:	Epoch: [25][128/408]	Loss 0.3049 (0.0498)	
training:	Epoch: [25][129/408]	Loss 0.0115 (0.0495)	
training:	Epoch: [25][130/408]	Loss 0.0115 (0.0492)	
training:	Epoch: [25][131/408]	Loss 0.0099 (0.0489)	
training:	Epoch: [25][132/408]	Loss 0.0119 (0.0486)	
training:	Epoch: [25][133/408]	Loss 0.0109 (0.0483)	
training:	Epoch: [25][134/408]	Loss 0.0110 (0.0480)	
training:	Epoch: [25][135/408]	Loss 0.0109 (0.0478)	
training:	Epoch: [25][136/408]	Loss 0.0102 (0.0475)	
training:	Epoch: [25][137/408]	Loss 0.0105 (0.0472)	
training:	Epoch: [25][138/408]	Loss 0.0105 (0.0470)	
training:	Epoch: [25][139/408]	Loss 0.0097 (0.0467)	
training:	Epoch: [25][140/408]	Loss 0.2896 (0.0484)	
training:	Epoch: [25][141/408]	Loss 0.0106 (0.0482)	
training:	Epoch: [25][142/408]	Loss 0.0159 (0.0479)	
training:	Epoch: [25][143/408]	Loss 0.0099 (0.0477)	
training:	Epoch: [25][144/408]	Loss 0.0115 (0.0474)	
training:	Epoch: [25][145/408]	Loss 0.0105 (0.0472)	
training:	Epoch: [25][146/408]	Loss 0.2638 (0.0486)	
training:	Epoch: [25][147/408]	Loss 0.0097 (0.0484)	
training:	Epoch: [25][148/408]	Loss 0.0112 (0.0481)	
training:	Epoch: [25][149/408]	Loss 0.0134 (0.0479)	
training:	Epoch: [25][150/408]	Loss 0.0105 (0.0476)	
training:	Epoch: [25][151/408]	Loss 0.0099 (0.0474)	
training:	Epoch: [25][152/408]	Loss 0.0109 (0.0472)	
training:	Epoch: [25][153/408]	Loss 0.0100 (0.0469)	
training:	Epoch: [25][154/408]	Loss 0.0100 (0.0467)	
training:	Epoch: [25][155/408]	Loss 0.3039 (0.0483)	
training:	Epoch: [25][156/408]	Loss 0.0095 (0.0481)	
training:	Epoch: [25][157/408]	Loss 0.0108 (0.0478)	
training:	Epoch: [25][158/408]	Loss 0.2973 (0.0494)	
training:	Epoch: [25][159/408]	Loss 0.0094 (0.0492)	
training:	Epoch: [25][160/408]	Loss 0.0107 (0.0489)	
training:	Epoch: [25][161/408]	Loss 0.0112 (0.0487)	
training:	Epoch: [25][162/408]	Loss 0.0138 (0.0485)	
training:	Epoch: [25][163/408]	Loss 0.3087 (0.0501)	
training:	Epoch: [25][164/408]	Loss 0.0112 (0.0498)	
training:	Epoch: [25][165/408]	Loss 0.0097 (0.0496)	
training:	Epoch: [25][166/408]	Loss 0.0101 (0.0494)	
training:	Epoch: [25][167/408]	Loss 0.0123 (0.0491)	
training:	Epoch: [25][168/408]	Loss 0.0125 (0.0489)	
training:	Epoch: [25][169/408]	Loss 0.0101 (0.0487)	
training:	Epoch: [25][170/408]	Loss 0.0111 (0.0485)	
training:	Epoch: [25][171/408]	Loss 0.0093 (0.0482)	
training:	Epoch: [25][172/408]	Loss 0.0096 (0.0480)	
training:	Epoch: [25][173/408]	Loss 0.0104 (0.0478)	
training:	Epoch: [25][174/408]	Loss 0.0107 (0.0476)	
training:	Epoch: [25][175/408]	Loss 0.0108 (0.0474)	
training:	Epoch: [25][176/408]	Loss 0.0099 (0.0472)	
training:	Epoch: [25][177/408]	Loss 0.0096 (0.0470)	
training:	Epoch: [25][178/408]	Loss 0.1700 (0.0476)	
training:	Epoch: [25][179/408]	Loss 0.0101 (0.0474)	
training:	Epoch: [25][180/408]	Loss 0.2858 (0.0488)	
training:	Epoch: [25][181/408]	Loss 0.0106 (0.0485)	
training:	Epoch: [25][182/408]	Loss 0.0102 (0.0483)	
training:	Epoch: [25][183/408]	Loss 0.0120 (0.0481)	
training:	Epoch: [25][184/408]	Loss 0.0100 (0.0479)	
training:	Epoch: [25][185/408]	Loss 0.0114 (0.0477)	
training:	Epoch: [25][186/408]	Loss 0.2929 (0.0490)	
training:	Epoch: [25][187/408]	Loss 0.0092 (0.0488)	
training:	Epoch: [25][188/408]	Loss 0.0099 (0.0486)	
training:	Epoch: [25][189/408]	Loss 0.0104 (0.0484)	
training:	Epoch: [25][190/408]	Loss 0.0158 (0.0483)	
training:	Epoch: [25][191/408]	Loss 0.0099 (0.0481)	
training:	Epoch: [25][192/408]	Loss 0.0097 (0.0479)	
training:	Epoch: [25][193/408]	Loss 0.0109 (0.0477)	
training:	Epoch: [25][194/408]	Loss 0.0170 (0.0475)	
training:	Epoch: [25][195/408]	Loss 0.0395 (0.0475)	
training:	Epoch: [25][196/408]	Loss 0.0101 (0.0473)	
training:	Epoch: [25][197/408]	Loss 0.0101 (0.0471)	
training:	Epoch: [25][198/408]	Loss 0.0097 (0.0469)	
training:	Epoch: [25][199/408]	Loss 0.0123 (0.0467)	
training:	Epoch: [25][200/408]	Loss 0.0123 (0.0465)	
training:	Epoch: [25][201/408]	Loss 0.0657 (0.0466)	
training:	Epoch: [25][202/408]	Loss 0.0104 (0.0465)	
training:	Epoch: [25][203/408]	Loss 0.0099 (0.0463)	
training:	Epoch: [25][204/408]	Loss 0.2801 (0.0474)	
training:	Epoch: [25][205/408]	Loss 0.0102 (0.0472)	
training:	Epoch: [25][206/408]	Loss 0.0115 (0.0471)	
training:	Epoch: [25][207/408]	Loss 0.2828 (0.0482)	
training:	Epoch: [25][208/408]	Loss 0.0091 (0.0480)	
training:	Epoch: [25][209/408]	Loss 0.0112 (0.0479)	
training:	Epoch: [25][210/408]	Loss 0.0514 (0.0479)	
training:	Epoch: [25][211/408]	Loss 0.3047 (0.0491)	
training:	Epoch: [25][212/408]	Loss 0.0100 (0.0489)	
training:	Epoch: [25][213/408]	Loss 0.0101 (0.0487)	
training:	Epoch: [25][214/408]	Loss 0.0170 (0.0486)	
training:	Epoch: [25][215/408]	Loss 0.0099 (0.0484)	
training:	Epoch: [25][216/408]	Loss 0.0107 (0.0482)	
training:	Epoch: [25][217/408]	Loss 0.0104 (0.0480)	
training:	Epoch: [25][218/408]	Loss 0.0110 (0.0479)	
training:	Epoch: [25][219/408]	Loss 0.0100 (0.0477)	
training:	Epoch: [25][220/408]	Loss 0.0088 (0.0475)	
training:	Epoch: [25][221/408]	Loss 0.0114 (0.0474)	
training:	Epoch: [25][222/408]	Loss 0.0116 (0.0472)	
training:	Epoch: [25][223/408]	Loss 0.0109 (0.0470)	
training:	Epoch: [25][224/408]	Loss 0.0096 (0.0469)	
training:	Epoch: [25][225/408]	Loss 0.0093 (0.0467)	
training:	Epoch: [25][226/408]	Loss 0.0103 (0.0465)	
training:	Epoch: [25][227/408]	Loss 0.5174 (0.0486)	
training:	Epoch: [25][228/408]	Loss 0.4134 (0.0502)	
training:	Epoch: [25][229/408]	Loss 0.0098 (0.0500)	
training:	Epoch: [25][230/408]	Loss 0.0104 (0.0499)	
training:	Epoch: [25][231/408]	Loss 0.0122 (0.0497)	
training:	Epoch: [25][232/408]	Loss 0.0120 (0.0495)	
training:	Epoch: [25][233/408]	Loss 0.0097 (0.0494)	
training:	Epoch: [25][234/408]	Loss 0.0122 (0.0492)	
training:	Epoch: [25][235/408]	Loss 0.0102 (0.0490)	
training:	Epoch: [25][236/408]	Loss 0.2858 (0.0500)	
training:	Epoch: [25][237/408]	Loss 0.0094 (0.0499)	
training:	Epoch: [25][238/408]	Loss 0.0110 (0.0497)	
training:	Epoch: [25][239/408]	Loss 0.2885 (0.0507)	
training:	Epoch: [25][240/408]	Loss 0.0112 (0.0505)	
training:	Epoch: [25][241/408]	Loss 0.0138 (0.0504)	
training:	Epoch: [25][242/408]	Loss 0.0830 (0.0505)	
training:	Epoch: [25][243/408]	Loss 0.0121 (0.0504)	
training:	Epoch: [25][244/408]	Loss 0.2900 (0.0514)	
training:	Epoch: [25][245/408]	Loss 0.0136 (0.0512)	
training:	Epoch: [25][246/408]	Loss 0.3110 (0.0523)	
training:	Epoch: [25][247/408]	Loss 0.0104 (0.0521)	
training:	Epoch: [25][248/408]	Loss 0.0111 (0.0519)	
training:	Epoch: [25][249/408]	Loss 0.0155 (0.0518)	
training:	Epoch: [25][250/408]	Loss 0.0107 (0.0516)	
training:	Epoch: [25][251/408]	Loss 0.0126 (0.0515)	
training:	Epoch: [25][252/408]	Loss 0.0107 (0.0513)	
training:	Epoch: [25][253/408]	Loss 0.0108 (0.0511)	
training:	Epoch: [25][254/408]	Loss 0.0206 (0.0510)	
training:	Epoch: [25][255/408]	Loss 0.0155 (0.0509)	
training:	Epoch: [25][256/408]	Loss 0.2549 (0.0517)	
training:	Epoch: [25][257/408]	Loss 0.0113 (0.0515)	
training:	Epoch: [25][258/408]	Loss 0.0103 (0.0514)	
training:	Epoch: [25][259/408]	Loss 0.0124 (0.0512)	
training:	Epoch: [25][260/408]	Loss 0.0103 (0.0510)	
training:	Epoch: [25][261/408]	Loss 0.0103 (0.0509)	
training:	Epoch: [25][262/408]	Loss 0.0118 (0.0507)	
training:	Epoch: [25][263/408]	Loss 0.0099 (0.0506)	
training:	Epoch: [25][264/408]	Loss 0.3028 (0.0515)	
training:	Epoch: [25][265/408]	Loss 0.0107 (0.0514)	
training:	Epoch: [25][266/408]	Loss 0.0132 (0.0512)	
training:	Epoch: [25][267/408]	Loss 0.0103 (0.0511)	
training:	Epoch: [25][268/408]	Loss 0.0087 (0.0509)	
training:	Epoch: [25][269/408]	Loss 0.0106 (0.0508)	
training:	Epoch: [25][270/408]	Loss 0.0096 (0.0506)	
training:	Epoch: [25][271/408]	Loss 0.1650 (0.0510)	
training:	Epoch: [25][272/408]	Loss 0.0103 (0.0509)	
training:	Epoch: [25][273/408]	Loss 0.0138 (0.0508)	
training:	Epoch: [25][274/408]	Loss 0.0712 (0.0508)	
training:	Epoch: [25][275/408]	Loss 0.0108 (0.0507)	
training:	Epoch: [25][276/408]	Loss 0.0104 (0.0505)	
training:	Epoch: [25][277/408]	Loss 0.0184 (0.0504)	
training:	Epoch: [25][278/408]	Loss 0.0102 (0.0503)	
training:	Epoch: [25][279/408]	Loss 0.0096 (0.0501)	
training:	Epoch: [25][280/408]	Loss 0.0105 (0.0500)	
training:	Epoch: [25][281/408]	Loss 0.2880 (0.0508)	
training:	Epoch: [25][282/408]	Loss 0.0125 (0.0507)	
training:	Epoch: [25][283/408]	Loss 0.0104 (0.0506)	
training:	Epoch: [25][284/408]	Loss 0.0101 (0.0504)	
training:	Epoch: [25][285/408]	Loss 0.0102 (0.0503)	
training:	Epoch: [25][286/408]	Loss 0.0106 (0.0501)	
training:	Epoch: [25][287/408]	Loss 0.0287 (0.0501)	
training:	Epoch: [25][288/408]	Loss 0.0224 (0.0500)	
training:	Epoch: [25][289/408]	Loss 0.2925 (0.0508)	
training:	Epoch: [25][290/408]	Loss 0.0102 (0.0507)	
training:	Epoch: [25][291/408]	Loss 0.0106 (0.0505)	
training:	Epoch: [25][292/408]	Loss 0.0091 (0.0504)	
training:	Epoch: [25][293/408]	Loss 0.0144 (0.0503)	
training:	Epoch: [25][294/408]	Loss 0.0093 (0.0501)	
training:	Epoch: [25][295/408]	Loss 0.0107 (0.0500)	
training:	Epoch: [25][296/408]	Loss 0.0122 (0.0499)	
training:	Epoch: [25][297/408]	Loss 0.2936 (0.0507)	
training:	Epoch: [25][298/408]	Loss 0.0113 (0.0506)	
training:	Epoch: [25][299/408]	Loss 0.0105 (0.0504)	
training:	Epoch: [25][300/408]	Loss 0.0156 (0.0503)	
training:	Epoch: [25][301/408]	Loss 0.0100 (0.0502)	
training:	Epoch: [25][302/408]	Loss 0.2973 (0.0510)	
training:	Epoch: [25][303/408]	Loss 0.0107 (0.0509)	
training:	Epoch: [25][304/408]	Loss 0.0100 (0.0507)	
training:	Epoch: [25][305/408]	Loss 0.0103 (0.0506)	
training:	Epoch: [25][306/408]	Loss 0.0727 (0.0507)	
training:	Epoch: [25][307/408]	Loss 0.0137 (0.0505)	
training:	Epoch: [25][308/408]	Loss 0.0118 (0.0504)	
training:	Epoch: [25][309/408]	Loss 0.0097 (0.0503)	
training:	Epoch: [25][310/408]	Loss 0.0118 (0.0502)	
training:	Epoch: [25][311/408]	Loss 0.0099 (0.0500)	
training:	Epoch: [25][312/408]	Loss 0.0100 (0.0499)	
training:	Epoch: [25][313/408]	Loss 0.0096 (0.0498)	
training:	Epoch: [25][314/408]	Loss 0.0131 (0.0497)	
training:	Epoch: [25][315/408]	Loss 0.0098 (0.0495)	
training:	Epoch: [25][316/408]	Loss 0.0106 (0.0494)	
training:	Epoch: [25][317/408]	Loss 0.0156 (0.0493)	
training:	Epoch: [25][318/408]	Loss 0.0284 (0.0492)	
training:	Epoch: [25][319/408]	Loss 0.0096 (0.0491)	
training:	Epoch: [25][320/408]	Loss 0.0198 (0.0490)	
training:	Epoch: [25][321/408]	Loss 0.0099 (0.0489)	
training:	Epoch: [25][322/408]	Loss 0.0122 (0.0488)	
training:	Epoch: [25][323/408]	Loss 0.0100 (0.0487)	
training:	Epoch: [25][324/408]	Loss 0.0108 (0.0485)	
training:	Epoch: [25][325/408]	Loss 0.0097 (0.0484)	
training:	Epoch: [25][326/408]	Loss 0.0106 (0.0483)	
training:	Epoch: [25][327/408]	Loss 0.0099 (0.0482)	
training:	Epoch: [25][328/408]	Loss 0.0119 (0.0481)	
training:	Epoch: [25][329/408]	Loss 0.0096 (0.0480)	
training:	Epoch: [25][330/408]	Loss 0.0109 (0.0479)	
training:	Epoch: [25][331/408]	Loss 0.0132 (0.0478)	
training:	Epoch: [25][332/408]	Loss 0.0103 (0.0476)	
training:	Epoch: [25][333/408]	Loss 0.0109 (0.0475)	
training:	Epoch: [25][334/408]	Loss 0.0098 (0.0474)	
training:	Epoch: [25][335/408]	Loss 0.2808 (0.0481)	
training:	Epoch: [25][336/408]	Loss 0.5675 (0.0497)	
training:	Epoch: [25][337/408]	Loss 0.0129 (0.0495)	
training:	Epoch: [25][338/408]	Loss 0.0104 (0.0494)	
training:	Epoch: [25][339/408]	Loss 0.0102 (0.0493)	
training:	Epoch: [25][340/408]	Loss 0.0103 (0.0492)	
training:	Epoch: [25][341/408]	Loss 0.0102 (0.0491)	
training:	Epoch: [25][342/408]	Loss 0.2909 (0.0498)	
training:	Epoch: [25][343/408]	Loss 0.3003 (0.0505)	
training:	Epoch: [25][344/408]	Loss 0.0104 (0.0504)	
training:	Epoch: [25][345/408]	Loss 0.0112 (0.0503)	
training:	Epoch: [25][346/408]	Loss 0.0104 (0.0502)	
training:	Epoch: [25][347/408]	Loss 0.2876 (0.0509)	
training:	Epoch: [25][348/408]	Loss 0.0099 (0.0507)	
training:	Epoch: [25][349/408]	Loss 0.0120 (0.0506)	
training:	Epoch: [25][350/408]	Loss 0.0101 (0.0505)	
training:	Epoch: [25][351/408]	Loss 0.0110 (0.0504)	
training:	Epoch: [25][352/408]	Loss 0.2899 (0.0511)	
training:	Epoch: [25][353/408]	Loss 0.0102 (0.0510)	
training:	Epoch: [25][354/408]	Loss 0.0150 (0.0509)	
training:	Epoch: [25][355/408]	Loss 0.0101 (0.0508)	
training:	Epoch: [25][356/408]	Loss 0.0098 (0.0506)	
training:	Epoch: [25][357/408]	Loss 0.0093 (0.0505)	
training:	Epoch: [25][358/408]	Loss 0.3082 (0.0512)	
training:	Epoch: [25][359/408]	Loss 0.0123 (0.0511)	
training:	Epoch: [25][360/408]	Loss 0.0116 (0.0510)	
training:	Epoch: [25][361/408]	Loss 0.2943 (0.0517)	
training:	Epoch: [25][362/408]	Loss 0.0109 (0.0516)	
training:	Epoch: [25][363/408]	Loss 0.0108 (0.0515)	
training:	Epoch: [25][364/408]	Loss 0.0104 (0.0514)	
training:	Epoch: [25][365/408]	Loss 0.3004 (0.0520)	
training:	Epoch: [25][366/408]	Loss 0.3084 (0.0527)	
training:	Epoch: [25][367/408]	Loss 0.0105 (0.0526)	
training:	Epoch: [25][368/408]	Loss 0.0104 (0.0525)	
training:	Epoch: [25][369/408]	Loss 0.0097 (0.0524)	
training:	Epoch: [25][370/408]	Loss 0.0105 (0.0523)	
training:	Epoch: [25][371/408]	Loss 0.3662 (0.0531)	
training:	Epoch: [25][372/408]	Loss 0.0233 (0.0531)	
training:	Epoch: [25][373/408]	Loss 0.0142 (0.0529)	
training:	Epoch: [25][374/408]	Loss 0.0110 (0.0528)	
training:	Epoch: [25][375/408]	Loss 0.0199 (0.0527)	
training:	Epoch: [25][376/408]	Loss 0.0109 (0.0526)	
training:	Epoch: [25][377/408]	Loss 0.0111 (0.0525)	
training:	Epoch: [25][378/408]	Loss 0.0104 (0.0524)	
training:	Epoch: [25][379/408]	Loss 0.0106 (0.0523)	
training:	Epoch: [25][380/408]	Loss 0.0104 (0.0522)	
training:	Epoch: [25][381/408]	Loss 0.0112 (0.0521)	
training:	Epoch: [25][382/408]	Loss 0.0107 (0.0520)	
training:	Epoch: [25][383/408]	Loss 0.0104 (0.0519)	
training:	Epoch: [25][384/408]	Loss 0.0278 (0.0518)	
training:	Epoch: [25][385/408]	Loss 0.0113 (0.0517)	
training:	Epoch: [25][386/408]	Loss 0.0416 (0.0517)	
training:	Epoch: [25][387/408]	Loss 0.0106 (0.0516)	
training:	Epoch: [25][388/408]	Loss 0.2806 (0.0522)	
training:	Epoch: [25][389/408]	Loss 0.0103 (0.0521)	
training:	Epoch: [25][390/408]	Loss 0.0113 (0.0519)	
training:	Epoch: [25][391/408]	Loss 0.0107 (0.0518)	
training:	Epoch: [25][392/408]	Loss 0.0110 (0.0517)	
training:	Epoch: [25][393/408]	Loss 0.0309 (0.0517)	
training:	Epoch: [25][394/408]	Loss 0.0250 (0.0516)	
training:	Epoch: [25][395/408]	Loss 0.2702 (0.0522)	
training:	Epoch: [25][396/408]	Loss 0.0103 (0.0521)	
training:	Epoch: [25][397/408]	Loss 0.0101 (0.0520)	
training:	Epoch: [25][398/408]	Loss 0.0112 (0.0519)	
training:	Epoch: [25][399/408]	Loss 0.2858 (0.0524)	
training:	Epoch: [25][400/408]	Loss 0.0109 (0.0523)	
training:	Epoch: [25][401/408]	Loss 0.0098 (0.0522)	
training:	Epoch: [25][402/408]	Loss 0.0107 (0.0521)	
training:	Epoch: [25][403/408]	Loss 0.0115 (0.0520)	
training:	Epoch: [25][404/408]	Loss 0.2758 (0.0526)	
training:	Epoch: [25][405/408]	Loss 0.0099 (0.0525)	
training:	Epoch: [25][406/408]	Loss 0.0099 (0.0524)	
training:	Epoch: [25][407/408]	Loss 0.0112 (0.0523)	
training:	Epoch: [25][408/408]	Loss 0.1749 (0.0526)	
Training:	 Loss: 0.0525

Training:	 ACC: 0.9910 0.9910 0.9909 0.9911
Validation:	 ACC: 0.7838 0.7854 0.8199 0.7478
Validation:	 Best_BACC: 0.8226 0.8250 0.8772 0.7679
Validation:	 Loss: 0.8607
Pretraining:	Epoch 26/200
----------
training:	Epoch: [26][1/408]	Loss 0.0110 (0.0110)	
training:	Epoch: [26][2/408]	Loss 0.0095 (0.0102)	
training:	Epoch: [26][3/408]	Loss 0.0103 (0.0102)	
training:	Epoch: [26][4/408]	Loss 0.0107 (0.0104)	
training:	Epoch: [26][5/408]	Loss 0.0118 (0.0107)	
training:	Epoch: [26][6/408]	Loss 0.0106 (0.0106)	
training:	Epoch: [26][7/408]	Loss 0.0112 (0.0107)	
training:	Epoch: [26][8/408]	Loss 0.0108 (0.0107)	
training:	Epoch: [26][9/408]	Loss 0.0102 (0.0107)	
training:	Epoch: [26][10/408]	Loss 0.0106 (0.0107)	
training:	Epoch: [26][11/408]	Loss 0.0099 (0.0106)	
training:	Epoch: [26][12/408]	Loss 0.0101 (0.0106)	
training:	Epoch: [26][13/408]	Loss 0.0121 (0.0107)	
training:	Epoch: [26][14/408]	Loss 0.0151 (0.0110)	
training:	Epoch: [26][15/408]	Loss 0.0098 (0.0109)	
training:	Epoch: [26][16/408]	Loss 0.0110 (0.0109)	
training:	Epoch: [26][17/408]	Loss 0.0107 (0.0109)	
training:	Epoch: [26][18/408]	Loss 0.2757 (0.0256)	
training:	Epoch: [26][19/408]	Loss 0.0123 (0.0249)	
training:	Epoch: [26][20/408]	Loss 0.2910 (0.0382)	
training:	Epoch: [26][21/408]	Loss 0.0107 (0.0369)	
training:	Epoch: [26][22/408]	Loss 0.0108 (0.0357)	
training:	Epoch: [26][23/408]	Loss 0.0123 (0.0347)	
training:	Epoch: [26][24/408]	Loss 0.0100 (0.0337)	
training:	Epoch: [26][25/408]	Loss 0.0106 (0.0328)	
training:	Epoch: [26][26/408]	Loss 0.2879 (0.0426)	
training:	Epoch: [26][27/408]	Loss 0.0101 (0.0414)	
training:	Epoch: [26][28/408]	Loss 0.0104 (0.0403)	
training:	Epoch: [26][29/408]	Loss 0.0166 (0.0394)	
training:	Epoch: [26][30/408]	Loss 0.0576 (0.0401)	
training:	Epoch: [26][31/408]	Loss 0.0107 (0.0391)	
training:	Epoch: [26][32/408]	Loss 0.0118 (0.0383)	
training:	Epoch: [26][33/408]	Loss 0.2788 (0.0455)	
training:	Epoch: [26][34/408]	Loss 0.3026 (0.0531)	
training:	Epoch: [26][35/408]	Loss 0.0104 (0.0519)	
training:	Epoch: [26][36/408]	Loss 0.0110 (0.0507)	
training:	Epoch: [26][37/408]	Loss 0.0113 (0.0497)	
training:	Epoch: [26][38/408]	Loss 0.0103 (0.0486)	
training:	Epoch: [26][39/408]	Loss 0.0119 (0.0477)	
training:	Epoch: [26][40/408]	Loss 0.0116 (0.0468)	
training:	Epoch: [26][41/408]	Loss 0.0817 (0.0476)	
training:	Epoch: [26][42/408]	Loss 0.0105 (0.0468)	
training:	Epoch: [26][43/408]	Loss 0.0460 (0.0467)	
training:	Epoch: [26][44/408]	Loss 0.3098 (0.0527)	
training:	Epoch: [26][45/408]	Loss 0.0110 (0.0518)	
training:	Epoch: [26][46/408]	Loss 0.0104 (0.0509)	
training:	Epoch: [26][47/408]	Loss 0.2816 (0.0558)	
training:	Epoch: [26][48/408]	Loss 0.0110 (0.0549)	
training:	Epoch: [26][49/408]	Loss 0.0098 (0.0539)	
training:	Epoch: [26][50/408]	Loss 0.0105 (0.0531)	
training:	Epoch: [26][51/408]	Loss 0.0100 (0.0522)	
training:	Epoch: [26][52/408]	Loss 0.0111 (0.0514)	
training:	Epoch: [26][53/408]	Loss 0.2698 (0.0556)	
training:	Epoch: [26][54/408]	Loss 0.0101 (0.0547)	
training:	Epoch: [26][55/408]	Loss 0.0111 (0.0539)	
training:	Epoch: [26][56/408]	Loss 0.0103 (0.0532)	
training:	Epoch: [26][57/408]	Loss 0.0103 (0.0524)	
training:	Epoch: [26][58/408]	Loss 0.0103 (0.0517)	
training:	Epoch: [26][59/408]	Loss 0.0109 (0.0510)	
training:	Epoch: [26][60/408]	Loss 0.0105 (0.0503)	
training:	Epoch: [26][61/408]	Loss 0.0103 (0.0497)	
training:	Epoch: [26][62/408]	Loss 0.0147 (0.0491)	
training:	Epoch: [26][63/408]	Loss 0.2952 (0.0530)	
training:	Epoch: [26][64/408]	Loss 0.0106 (0.0523)	
training:	Epoch: [26][65/408]	Loss 0.0097 (0.0517)	
training:	Epoch: [26][66/408]	Loss 0.0105 (0.0511)	
training:	Epoch: [26][67/408]	Loss 0.0166 (0.0505)	
training:	Epoch: [26][68/408]	Loss 0.0116 (0.0500)	
training:	Epoch: [26][69/408]	Loss 0.0103 (0.0494)	
training:	Epoch: [26][70/408]	Loss 0.0107 (0.0488)	
training:	Epoch: [26][71/408]	Loss 0.0104 (0.0483)	
training:	Epoch: [26][72/408]	Loss 0.0104 (0.0478)	
training:	Epoch: [26][73/408]	Loss 0.0097 (0.0472)	
training:	Epoch: [26][74/408]	Loss 0.0110 (0.0468)	
training:	Epoch: [26][75/408]	Loss 0.2857 (0.0499)	
training:	Epoch: [26][76/408]	Loss 0.0098 (0.0494)	
training:	Epoch: [26][77/408]	Loss 0.0153 (0.0490)	
training:	Epoch: [26][78/408]	Loss 0.0108 (0.0485)	
training:	Epoch: [26][79/408]	Loss 0.0105 (0.0480)	
training:	Epoch: [26][80/408]	Loss 0.0115 (0.0475)	
training:	Epoch: [26][81/408]	Loss 0.0100 (0.0471)	
training:	Epoch: [26][82/408]	Loss 0.0095 (0.0466)	
training:	Epoch: [26][83/408]	Loss 0.0121 (0.0462)	
training:	Epoch: [26][84/408]	Loss 0.0101 (0.0458)	
training:	Epoch: [26][85/408]	Loss 0.3050 (0.0488)	
training:	Epoch: [26][86/408]	Loss 0.3003 (0.0518)	
training:	Epoch: [26][87/408]	Loss 0.0114 (0.0513)	
training:	Epoch: [26][88/408]	Loss 0.2611 (0.0537)	
training:	Epoch: [26][89/408]	Loss 0.0109 (0.0532)	
training:	Epoch: [26][90/408]	Loss 0.0106 (0.0527)	
training:	Epoch: [26][91/408]	Loss 0.0110 (0.0523)	
training:	Epoch: [26][92/408]	Loss 0.2932 (0.0549)	
training:	Epoch: [26][93/408]	Loss 0.0101 (0.0544)	
training:	Epoch: [26][94/408]	Loss 0.2888 (0.0569)	
training:	Epoch: [26][95/408]	Loss 0.0109 (0.0564)	
training:	Epoch: [26][96/408]	Loss 0.0135 (0.0560)	
training:	Epoch: [26][97/408]	Loss 0.2993 (0.0585)	
training:	Epoch: [26][98/408]	Loss 0.0098 (0.0580)	
training:	Epoch: [26][99/408]	Loss 0.0435 (0.0578)	
training:	Epoch: [26][100/408]	Loss 0.0102 (0.0574)	
training:	Epoch: [26][101/408]	Loss 0.0213 (0.0570)	
training:	Epoch: [26][102/408]	Loss 0.0099 (0.0565)	
training:	Epoch: [26][103/408]	Loss 0.0113 (0.0561)	
training:	Epoch: [26][104/408]	Loss 0.0108 (0.0557)	
training:	Epoch: [26][105/408]	Loss 0.0172 (0.0553)	
training:	Epoch: [26][106/408]	Loss 0.0105 (0.0549)	
training:	Epoch: [26][107/408]	Loss 0.0104 (0.0545)	
training:	Epoch: [26][108/408]	Loss 0.0108 (0.0541)	
training:	Epoch: [26][109/408]	Loss 0.0099 (0.0536)	
training:	Epoch: [26][110/408]	Loss 0.2883 (0.0558)	
training:	Epoch: [26][111/408]	Loss 0.0095 (0.0554)	
training:	Epoch: [26][112/408]	Loss 0.2906 (0.0575)	
training:	Epoch: [26][113/408]	Loss 0.0107 (0.0570)	
training:	Epoch: [26][114/408]	Loss 0.0106 (0.0566)	
training:	Epoch: [26][115/408]	Loss 0.0099 (0.0562)	
training:	Epoch: [26][116/408]	Loss 0.0119 (0.0559)	
training:	Epoch: [26][117/408]	Loss 0.0222 (0.0556)	
training:	Epoch: [26][118/408]	Loss 0.0233 (0.0553)	
training:	Epoch: [26][119/408]	Loss 0.0106 (0.0549)	
training:	Epoch: [26][120/408]	Loss 0.0109 (0.0545)	
training:	Epoch: [26][121/408]	Loss 0.0109 (0.0542)	
training:	Epoch: [26][122/408]	Loss 0.0103 (0.0538)	
training:	Epoch: [26][123/408]	Loss 0.2814 (0.0557)	
training:	Epoch: [26][124/408]	Loss 0.0102 (0.0553)	
training:	Epoch: [26][125/408]	Loss 0.0102 (0.0549)	
training:	Epoch: [26][126/408]	Loss 0.0146 (0.0546)	
training:	Epoch: [26][127/408]	Loss 0.5554 (0.0586)	
training:	Epoch: [26][128/408]	Loss 0.0110 (0.0582)	
training:	Epoch: [26][129/408]	Loss 0.0108 (0.0578)	
training:	Epoch: [26][130/408]	Loss 0.0118 (0.0575)	
training:	Epoch: [26][131/408]	Loss 0.0107 (0.0571)	
training:	Epoch: [26][132/408]	Loss 0.0106 (0.0568)	
training:	Epoch: [26][133/408]	Loss 0.0095 (0.0564)	
training:	Epoch: [26][134/408]	Loss 0.0104 (0.0561)	
training:	Epoch: [26][135/408]	Loss 0.0106 (0.0557)	
training:	Epoch: [26][136/408]	Loss 0.0102 (0.0554)	
training:	Epoch: [26][137/408]	Loss 0.1347 (0.0560)	
training:	Epoch: [26][138/408]	Loss 0.0111 (0.0557)	
training:	Epoch: [26][139/408]	Loss 0.0097 (0.0553)	
training:	Epoch: [26][140/408]	Loss 0.0109 (0.0550)	
training:	Epoch: [26][141/408]	Loss 0.0105 (0.0547)	
training:	Epoch: [26][142/408]	Loss 0.0113 (0.0544)	
training:	Epoch: [26][143/408]	Loss 0.3087 (0.0562)	
training:	Epoch: [26][144/408]	Loss 0.0126 (0.0559)	
training:	Epoch: [26][145/408]	Loss 0.2966 (0.0575)	
training:	Epoch: [26][146/408]	Loss 0.1372 (0.0581)	
training:	Epoch: [26][147/408]	Loss 0.0103 (0.0577)	
training:	Epoch: [26][148/408]	Loss 0.2992 (0.0594)	
training:	Epoch: [26][149/408]	Loss 0.0236 (0.0591)	
training:	Epoch: [26][150/408]	Loss 0.1670 (0.0599)	
training:	Epoch: [26][151/408]	Loss 0.0115 (0.0595)	
training:	Epoch: [26][152/408]	Loss 0.0160 (0.0592)	
training:	Epoch: [26][153/408]	Loss 0.0179 (0.0590)	
training:	Epoch: [26][154/408]	Loss 0.0240 (0.0587)	
training:	Epoch: [26][155/408]	Loss 0.0145 (0.0585)	
training:	Epoch: [26][156/408]	Loss 0.3768 (0.0605)	
training:	Epoch: [26][157/408]	Loss 0.0105 (0.0602)	
training:	Epoch: [26][158/408]	Loss 0.0122 (0.0599)	
training:	Epoch: [26][159/408]	Loss 0.2561 (0.0611)	
training:	Epoch: [26][160/408]	Loss 0.0108 (0.0608)	
training:	Epoch: [26][161/408]	Loss 0.0112 (0.0605)	
training:	Epoch: [26][162/408]	Loss 0.0128 (0.0602)	
training:	Epoch: [26][163/408]	Loss 0.0095 (0.0599)	
training:	Epoch: [26][164/408]	Loss 0.0190 (0.0596)	
training:	Epoch: [26][165/408]	Loss 0.0387 (0.0595)	
training:	Epoch: [26][166/408]	Loss 0.0142 (0.0592)	
training:	Epoch: [26][167/408]	Loss 0.0107 (0.0589)	
training:	Epoch: [26][168/408]	Loss 0.0123 (0.0587)	
training:	Epoch: [26][169/408]	Loss 0.0109 (0.0584)	
training:	Epoch: [26][170/408]	Loss 0.0109 (0.0581)	
training:	Epoch: [26][171/408]	Loss 0.0105 (0.0578)	
training:	Epoch: [26][172/408]	Loss 0.0104 (0.0576)	
training:	Epoch: [26][173/408]	Loss 0.0100 (0.0573)	
training:	Epoch: [26][174/408]	Loss 0.0118 (0.0570)	
training:	Epoch: [26][175/408]	Loss 0.1584 (0.0576)	
training:	Epoch: [26][176/408]	Loss 0.0107 (0.0573)	
training:	Epoch: [26][177/408]	Loss 0.0112 (0.0571)	
training:	Epoch: [26][178/408]	Loss 0.0107 (0.0568)	
training:	Epoch: [26][179/408]	Loss 0.0137 (0.0566)	
training:	Epoch: [26][180/408]	Loss 0.0302 (0.0564)	
training:	Epoch: [26][181/408]	Loss 0.2787 (0.0577)	
training:	Epoch: [26][182/408]	Loss 0.0100 (0.0574)	
training:	Epoch: [26][183/408]	Loss 0.0124 (0.0571)	
training:	Epoch: [26][184/408]	Loss 0.1299 (0.0575)	
training:	Epoch: [26][185/408]	Loss 0.0117 (0.0573)	
training:	Epoch: [26][186/408]	Loss 0.0099 (0.0570)	
training:	Epoch: [26][187/408]	Loss 0.0179 (0.0568)	
training:	Epoch: [26][188/408]	Loss 0.0115 (0.0566)	
training:	Epoch: [26][189/408]	Loss 0.0095 (0.0563)	
training:	Epoch: [26][190/408]	Loss 0.0102 (0.0561)	
training:	Epoch: [26][191/408]	Loss 0.0109 (0.0559)	
training:	Epoch: [26][192/408]	Loss 0.0451 (0.0558)	
training:	Epoch: [26][193/408]	Loss 0.0114 (0.0556)	
training:	Epoch: [26][194/408]	Loss 0.0164 (0.0554)	
training:	Epoch: [26][195/408]	Loss 0.0109 (0.0551)	
training:	Epoch: [26][196/408]	Loss 0.0505 (0.0551)	
training:	Epoch: [26][197/408]	Loss 0.0112 (0.0549)	
training:	Epoch: [26][198/408]	Loss 0.0118 (0.0547)	
training:	Epoch: [26][199/408]	Loss 0.0113 (0.0545)	
training:	Epoch: [26][200/408]	Loss 0.0362 (0.0544)	
training:	Epoch: [26][201/408]	Loss 0.0095 (0.0541)	
training:	Epoch: [26][202/408]	Loss 0.0110 (0.0539)	
training:	Epoch: [26][203/408]	Loss 0.2646 (0.0550)	
training:	Epoch: [26][204/408]	Loss 0.0110 (0.0548)	
training:	Epoch: [26][205/408]	Loss 0.2733 (0.0558)	
training:	Epoch: [26][206/408]	Loss 0.0122 (0.0556)	
training:	Epoch: [26][207/408]	Loss 0.0106 (0.0554)	
training:	Epoch: [26][208/408]	Loss 0.0115 (0.0552)	
training:	Epoch: [26][209/408]	Loss 0.0108 (0.0550)	
training:	Epoch: [26][210/408]	Loss 0.2983 (0.0561)	
training:	Epoch: [26][211/408]	Loss 0.0099 (0.0559)	
training:	Epoch: [26][212/408]	Loss 0.0106 (0.0557)	
training:	Epoch: [26][213/408]	Loss 0.0092 (0.0555)	
training:	Epoch: [26][214/408]	Loss 0.0095 (0.0553)	
training:	Epoch: [26][215/408]	Loss 0.2895 (0.0563)	
training:	Epoch: [26][216/408]	Loss 0.3049 (0.0575)	
training:	Epoch: [26][217/408]	Loss 0.0118 (0.0573)	
training:	Epoch: [26][218/408]	Loss 0.0105 (0.0571)	
training:	Epoch: [26][219/408]	Loss 0.0109 (0.0569)	
training:	Epoch: [26][220/408]	Loss 0.0104 (0.0567)	
training:	Epoch: [26][221/408]	Loss 0.0092 (0.0564)	
training:	Epoch: [26][222/408]	Loss 0.0104 (0.0562)	
training:	Epoch: [26][223/408]	Loss 0.0116 (0.0560)	
training:	Epoch: [26][224/408]	Loss 0.0108 (0.0558)	
training:	Epoch: [26][225/408]	Loss 0.3138 (0.0570)	
training:	Epoch: [26][226/408]	Loss 0.0118 (0.0568)	
training:	Epoch: [26][227/408]	Loss 0.0099 (0.0566)	
training:	Epoch: [26][228/408]	Loss 0.0100 (0.0564)	
training:	Epoch: [26][229/408]	Loss 0.0101 (0.0562)	
training:	Epoch: [26][230/408]	Loss 0.1296 (0.0565)	
training:	Epoch: [26][231/408]	Loss 0.0098 (0.0563)	
training:	Epoch: [26][232/408]	Loss 0.0128 (0.0561)	
training:	Epoch: [26][233/408]	Loss 0.0101 (0.0559)	
training:	Epoch: [26][234/408]	Loss 0.0116 (0.0557)	
training:	Epoch: [26][235/408]	Loss 0.0140 (0.0555)	
training:	Epoch: [26][236/408]	Loss 0.0101 (0.0553)	
training:	Epoch: [26][237/408]	Loss 0.0119 (0.0552)	
training:	Epoch: [26][238/408]	Loss 0.0108 (0.0550)	
training:	Epoch: [26][239/408]	Loss 0.4447 (0.0566)	
training:	Epoch: [26][240/408]	Loss 0.0433 (0.0565)	
training:	Epoch: [26][241/408]	Loss 0.0099 (0.0563)	
training:	Epoch: [26][242/408]	Loss 0.0113 (0.0562)	
training:	Epoch: [26][243/408]	Loss 0.0120 (0.0560)	
training:	Epoch: [26][244/408]	Loss 0.0111 (0.0558)	
training:	Epoch: [26][245/408]	Loss 0.0112 (0.0556)	
training:	Epoch: [26][246/408]	Loss 0.2800 (0.0565)	
training:	Epoch: [26][247/408]	Loss 0.1458 (0.0569)	
training:	Epoch: [26][248/408]	Loss 0.0124 (0.0567)	
training:	Epoch: [26][249/408]	Loss 0.0246 (0.0566)	
training:	Epoch: [26][250/408]	Loss 0.0324 (0.0565)	
training:	Epoch: [26][251/408]	Loss 0.0162 (0.0563)	
training:	Epoch: [26][252/408]	Loss 0.2772 (0.0572)	
training:	Epoch: [26][253/408]	Loss 0.0101 (0.0570)	
training:	Epoch: [26][254/408]	Loss 0.0102 (0.0568)	
training:	Epoch: [26][255/408]	Loss 0.0096 (0.0566)	
training:	Epoch: [26][256/408]	Loss 0.1489 (0.0570)	
training:	Epoch: [26][257/408]	Loss 0.0104 (0.0568)	
training:	Epoch: [26][258/408]	Loss 0.0110 (0.0566)	
training:	Epoch: [26][259/408]	Loss 0.0115 (0.0565)	
training:	Epoch: [26][260/408]	Loss 0.0155 (0.0563)	
training:	Epoch: [26][261/408]	Loss 0.0120 (0.0561)	
training:	Epoch: [26][262/408]	Loss 0.0096 (0.0560)	
training:	Epoch: [26][263/408]	Loss 0.0324 (0.0559)	
training:	Epoch: [26][264/408]	Loss 0.0120 (0.0557)	
training:	Epoch: [26][265/408]	Loss 0.0108 (0.0555)	
training:	Epoch: [26][266/408]	Loss 0.3135 (0.0565)	
training:	Epoch: [26][267/408]	Loss 0.0095 (0.0563)	
training:	Epoch: [26][268/408]	Loss 0.0106 (0.0562)	
training:	Epoch: [26][269/408]	Loss 0.0953 (0.0563)	
training:	Epoch: [26][270/408]	Loss 0.0127 (0.0561)	
training:	Epoch: [26][271/408]	Loss 0.0098 (0.0560)	
training:	Epoch: [26][272/408]	Loss 0.2800 (0.0568)	
training:	Epoch: [26][273/408]	Loss 0.3039 (0.0577)	
training:	Epoch: [26][274/408]	Loss 0.0100 (0.0575)	
training:	Epoch: [26][275/408]	Loss 0.0098 (0.0574)	
training:	Epoch: [26][276/408]	Loss 0.0102 (0.0572)	
training:	Epoch: [26][277/408]	Loss 0.0103 (0.0570)	
training:	Epoch: [26][278/408]	Loss 0.0100 (0.0568)	
training:	Epoch: [26][279/408]	Loss 0.2872 (0.0577)	
training:	Epoch: [26][280/408]	Loss 0.0206 (0.0575)	
training:	Epoch: [26][281/408]	Loss 0.0118 (0.0574)	
training:	Epoch: [26][282/408]	Loss 0.0129 (0.0572)	
training:	Epoch: [26][283/408]	Loss 0.2943 (0.0581)	
training:	Epoch: [26][284/408]	Loss 0.0120 (0.0579)	
training:	Epoch: [26][285/408]	Loss 0.0108 (0.0577)	
training:	Epoch: [26][286/408]	Loss 0.1003 (0.0579)	
training:	Epoch: [26][287/408]	Loss 0.0107 (0.0577)	
training:	Epoch: [26][288/408]	Loss 0.0097 (0.0575)	
training:	Epoch: [26][289/408]	Loss 0.0125 (0.0574)	
training:	Epoch: [26][290/408]	Loss 0.0103 (0.0572)	
training:	Epoch: [26][291/408]	Loss 0.0105 (0.0571)	
training:	Epoch: [26][292/408]	Loss 0.0100 (0.0569)	
training:	Epoch: [26][293/408]	Loss 0.0108 (0.0567)	
training:	Epoch: [26][294/408]	Loss 0.0108 (0.0566)	
training:	Epoch: [26][295/408]	Loss 0.0094 (0.0564)	
training:	Epoch: [26][296/408]	Loss 0.0106 (0.0563)	
training:	Epoch: [26][297/408]	Loss 0.0097 (0.0561)	
training:	Epoch: [26][298/408]	Loss 0.0106 (0.0560)	
training:	Epoch: [26][299/408]	Loss 0.0120 (0.0558)	
training:	Epoch: [26][300/408]	Loss 0.0095 (0.0557)	
training:	Epoch: [26][301/408]	Loss 0.0117 (0.0555)	
training:	Epoch: [26][302/408]	Loss 0.0096 (0.0554)	
training:	Epoch: [26][303/408]	Loss 0.0142 (0.0552)	
training:	Epoch: [26][304/408]	Loss 0.0100 (0.0551)	
training:	Epoch: [26][305/408]	Loss 0.3113 (0.0559)	
training:	Epoch: [26][306/408]	Loss 0.0108 (0.0558)	
training:	Epoch: [26][307/408]	Loss 0.0096 (0.0556)	
training:	Epoch: [26][308/408]	Loss 0.0103 (0.0555)	
training:	Epoch: [26][309/408]	Loss 0.3005 (0.0563)	
training:	Epoch: [26][310/408]	Loss 0.1175 (0.0565)	
training:	Epoch: [26][311/408]	Loss 0.0114 (0.0563)	
training:	Epoch: [26][312/408]	Loss 0.3069 (0.0571)	
training:	Epoch: [26][313/408]	Loss 0.0099 (0.0570)	
training:	Epoch: [26][314/408]	Loss 0.0111 (0.0568)	
training:	Epoch: [26][315/408]	Loss 0.0105 (0.0567)	
training:	Epoch: [26][316/408]	Loss 0.0125 (0.0565)	
training:	Epoch: [26][317/408]	Loss 0.0095 (0.0564)	
training:	Epoch: [26][318/408]	Loss 0.0109 (0.0563)	
training:	Epoch: [26][319/408]	Loss 0.0109 (0.0561)	
training:	Epoch: [26][320/408]	Loss 0.0714 (0.0562)	
training:	Epoch: [26][321/408]	Loss 0.0111 (0.0560)	
training:	Epoch: [26][322/408]	Loss 0.0112 (0.0559)	
training:	Epoch: [26][323/408]	Loss 0.0096 (0.0557)	
training:	Epoch: [26][324/408]	Loss 0.0106 (0.0556)	
training:	Epoch: [26][325/408]	Loss 0.0093 (0.0555)	
training:	Epoch: [26][326/408]	Loss 0.0140 (0.0553)	
training:	Epoch: [26][327/408]	Loss 0.0125 (0.0552)	
training:	Epoch: [26][328/408]	Loss 0.2365 (0.0557)	
training:	Epoch: [26][329/408]	Loss 0.0103 (0.0556)	
training:	Epoch: [26][330/408]	Loss 0.0107 (0.0555)	
training:	Epoch: [26][331/408]	Loss 0.0089 (0.0553)	
training:	Epoch: [26][332/408]	Loss 0.0096 (0.0552)	
training:	Epoch: [26][333/408]	Loss 0.0107 (0.0551)	
training:	Epoch: [26][334/408]	Loss 0.0103 (0.0549)	
training:	Epoch: [26][335/408]	Loss 0.0145 (0.0548)	
training:	Epoch: [26][336/408]	Loss 0.0227 (0.0547)	
training:	Epoch: [26][337/408]	Loss 0.0111 (0.0546)	
training:	Epoch: [26][338/408]	Loss 0.4123 (0.0556)	
training:	Epoch: [26][339/408]	Loss 0.0106 (0.0555)	
training:	Epoch: [26][340/408]	Loss 0.1116 (0.0557)	
training:	Epoch: [26][341/408]	Loss 0.0117 (0.0555)	
training:	Epoch: [26][342/408]	Loss 0.1224 (0.0557)	
training:	Epoch: [26][343/408]	Loss 0.0105 (0.0556)	
training:	Epoch: [26][344/408]	Loss 0.0128 (0.0555)	
training:	Epoch: [26][345/408]	Loss 0.2775 (0.0561)	
training:	Epoch: [26][346/408]	Loss 0.0119 (0.0560)	
training:	Epoch: [26][347/408]	Loss 0.0095 (0.0559)	
training:	Epoch: [26][348/408]	Loss 0.0120 (0.0557)	
training:	Epoch: [26][349/408]	Loss 0.0175 (0.0556)	
training:	Epoch: [26][350/408]	Loss 0.0180 (0.0555)	
training:	Epoch: [26][351/408]	Loss 0.5370 (0.0569)	
training:	Epoch: [26][352/408]	Loss 0.0104 (0.0568)	
training:	Epoch: [26][353/408]	Loss 0.2698 (0.0574)	
training:	Epoch: [26][354/408]	Loss 0.0105 (0.0572)	
training:	Epoch: [26][355/408]	Loss 0.1760 (0.0576)	
training:	Epoch: [26][356/408]	Loss 0.0133 (0.0574)	
training:	Epoch: [26][357/408]	Loss 0.2819 (0.0581)	
training:	Epoch: [26][358/408]	Loss 0.0133 (0.0579)	
training:	Epoch: [26][359/408]	Loss 0.0140 (0.0578)	
training:	Epoch: [26][360/408]	Loss 0.0109 (0.0577)	
training:	Epoch: [26][361/408]	Loss 0.0105 (0.0576)	
training:	Epoch: [26][362/408]	Loss 0.0174 (0.0575)	
training:	Epoch: [26][363/408]	Loss 0.0103 (0.0573)	
training:	Epoch: [26][364/408]	Loss 0.0431 (0.0573)	
training:	Epoch: [26][365/408]	Loss 0.0119 (0.0572)	
training:	Epoch: [26][366/408]	Loss 0.3106 (0.0579)	
training:	Epoch: [26][367/408]	Loss 0.0108 (0.0577)	
training:	Epoch: [26][368/408]	Loss 0.0095 (0.0576)	
training:	Epoch: [26][369/408]	Loss 0.0131 (0.0575)	
training:	Epoch: [26][370/408]	Loss 0.0120 (0.0573)	
training:	Epoch: [26][371/408]	Loss 0.0125 (0.0572)	
training:	Epoch: [26][372/408]	Loss 0.0111 (0.0571)	
training:	Epoch: [26][373/408]	Loss 0.0170 (0.0570)	
training:	Epoch: [26][374/408]	Loss 0.3046 (0.0577)	
training:	Epoch: [26][375/408]	Loss 0.0105 (0.0575)	
training:	Epoch: [26][376/408]	Loss 0.0122 (0.0574)	
training:	Epoch: [26][377/408]	Loss 0.0123 (0.0573)	
training:	Epoch: [26][378/408]	Loss 0.1510 (0.0575)	
training:	Epoch: [26][379/408]	Loss 0.0095 (0.0574)	
training:	Epoch: [26][380/408]	Loss 0.0105 (0.0573)	
training:	Epoch: [26][381/408]	Loss 0.0096 (0.0572)	
training:	Epoch: [26][382/408]	Loss 0.4246 (0.0581)	
training:	Epoch: [26][383/408]	Loss 0.0117 (0.0580)	
training:	Epoch: [26][384/408]	Loss 0.2821 (0.0586)	
training:	Epoch: [26][385/408]	Loss 0.0097 (0.0585)	
training:	Epoch: [26][386/408]	Loss 0.0126 (0.0583)	
training:	Epoch: [26][387/408]	Loss 0.0101 (0.0582)	
training:	Epoch: [26][388/408]	Loss 0.0094 (0.0581)	
training:	Epoch: [26][389/408]	Loss 0.0127 (0.0580)	
training:	Epoch: [26][390/408]	Loss 0.0115 (0.0579)	
training:	Epoch: [26][391/408]	Loss 0.0092 (0.0577)	
training:	Epoch: [26][392/408]	Loss 0.0186 (0.0576)	
training:	Epoch: [26][393/408]	Loss 0.0096 (0.0575)	
training:	Epoch: [26][394/408]	Loss 0.2930 (0.0581)	
training:	Epoch: [26][395/408]	Loss 0.2748 (0.0587)	
training:	Epoch: [26][396/408]	Loss 0.2904 (0.0592)	
training:	Epoch: [26][397/408]	Loss 0.0161 (0.0591)	
training:	Epoch: [26][398/408]	Loss 0.0216 (0.0590)	
training:	Epoch: [26][399/408]	Loss 0.1541 (0.0593)	
training:	Epoch: [26][400/408]	Loss 0.0109 (0.0592)	
training:	Epoch: [26][401/408]	Loss 0.0195 (0.0591)	
training:	Epoch: [26][402/408]	Loss 0.0108 (0.0589)	
training:	Epoch: [26][403/408]	Loss 0.0116 (0.0588)	
training:	Epoch: [26][404/408]	Loss 0.0088 (0.0587)	
training:	Epoch: [26][405/408]	Loss 0.0215 (0.0586)	
training:	Epoch: [26][406/408]	Loss 0.0102 (0.0585)	
training:	Epoch: [26][407/408]	Loss 0.0100 (0.0584)	
training:	Epoch: [26][408/408]	Loss 0.0105 (0.0582)	
Training:	 Loss: 0.0582

Training:	 ACC: 0.9914 0.9914 0.9912 0.9917
Validation:	 ACC: 0.7783 0.7790 0.7953 0.7612
Validation:	 Best_BACC: 0.8226 0.8250 0.8772 0.7679
Validation:	 Loss: 0.8613
Pretraining:	Epoch 27/200
----------
training:	Epoch: [27][1/408]	Loss 0.3016 (0.3016)	
training:	Epoch: [27][2/408]	Loss 0.3859 (0.3437)	
training:	Epoch: [27][3/408]	Loss 0.0105 (0.2327)	
training:	Epoch: [27][4/408]	Loss 0.0111 (0.1773)	
training:	Epoch: [27][5/408]	Loss 0.0108 (0.1440)	
training:	Epoch: [27][6/408]	Loss 0.0102 (0.1217)	
training:	Epoch: [27][7/408]	Loss 0.0102 (0.1057)	
training:	Epoch: [27][8/408]	Loss 0.0103 (0.0938)	
training:	Epoch: [27][9/408]	Loss 0.0110 (0.0846)	
training:	Epoch: [27][10/408]	Loss 0.0113 (0.0773)	
training:	Epoch: [27][11/408]	Loss 0.0104 (0.0712)	
training:	Epoch: [27][12/408]	Loss 0.0101 (0.0661)	
training:	Epoch: [27][13/408]	Loss 0.0098 (0.0618)	
training:	Epoch: [27][14/408]	Loss 0.0176 (0.0586)	
training:	Epoch: [27][15/408]	Loss 0.2567 (0.0718)	
training:	Epoch: [27][16/408]	Loss 0.0793 (0.0723)	
training:	Epoch: [27][17/408]	Loss 0.0132 (0.0688)	
training:	Epoch: [27][18/408]	Loss 0.3008 (0.0817)	
training:	Epoch: [27][19/408]	Loss 0.0090 (0.0779)	
training:	Epoch: [27][20/408]	Loss 0.0101 (0.0745)	
training:	Epoch: [27][21/408]	Loss 0.0159 (0.0717)	
training:	Epoch: [27][22/408]	Loss 0.0099 (0.0689)	
training:	Epoch: [27][23/408]	Loss 0.0119 (0.0664)	
training:	Epoch: [27][24/408]	Loss 0.0115 (0.0641)	
training:	Epoch: [27][25/408]	Loss 0.0096 (0.0619)	
training:	Epoch: [27][26/408]	Loss 0.0443 (0.0613)	
training:	Epoch: [27][27/408]	Loss 0.0116 (0.0594)	
training:	Epoch: [27][28/408]	Loss 0.0101 (0.0577)	
training:	Epoch: [27][29/408]	Loss 0.0103 (0.0560)	
training:	Epoch: [27][30/408]	Loss 0.0096 (0.0545)	
training:	Epoch: [27][31/408]	Loss 0.0107 (0.0531)	
training:	Epoch: [27][32/408]	Loss 0.0094 (0.0517)	
training:	Epoch: [27][33/408]	Loss 0.0110 (0.0505)	
training:	Epoch: [27][34/408]	Loss 0.0098 (0.0493)	
training:	Epoch: [27][35/408]	Loss 0.0219 (0.0485)	
training:	Epoch: [27][36/408]	Loss 0.0237 (0.0478)	
training:	Epoch: [27][37/408]	Loss 0.0178 (0.0470)	
training:	Epoch: [27][38/408]	Loss 0.3044 (0.0538)	
training:	Epoch: [27][39/408]	Loss 0.0094 (0.0526)	
training:	Epoch: [27][40/408]	Loss 0.0175 (0.0517)	
training:	Epoch: [27][41/408]	Loss 0.0117 (0.0508)	
training:	Epoch: [27][42/408]	Loss 0.0103 (0.0498)	
training:	Epoch: [27][43/408]	Loss 0.0097 (0.0489)	
training:	Epoch: [27][44/408]	Loss 0.0114 (0.0480)	
training:	Epoch: [27][45/408]	Loss 0.0099 (0.0472)	
training:	Epoch: [27][46/408]	Loss 0.0099 (0.0464)	
training:	Epoch: [27][47/408]	Loss 0.0103 (0.0456)	
training:	Epoch: [27][48/408]	Loss 0.0091 (0.0448)	
training:	Epoch: [27][49/408]	Loss 0.5800 (0.0558)	
training:	Epoch: [27][50/408]	Loss 0.0108 (0.0549)	
training:	Epoch: [27][51/408]	Loss 0.0125 (0.0540)	
training:	Epoch: [27][52/408]	Loss 0.0104 (0.0532)	
training:	Epoch: [27][53/408]	Loss 0.0101 (0.0524)	
training:	Epoch: [27][54/408]	Loss 0.0144 (0.0517)	
training:	Epoch: [27][55/408]	Loss 0.0142 (0.0510)	
training:	Epoch: [27][56/408]	Loss 0.0110 (0.0503)	
training:	Epoch: [27][57/408]	Loss 0.0101 (0.0496)	
training:	Epoch: [27][58/408]	Loss 0.0102 (0.0489)	
training:	Epoch: [27][59/408]	Loss 0.0097 (0.0482)	
training:	Epoch: [27][60/408]	Loss 0.2738 (0.0520)	
training:	Epoch: [27][61/408]	Loss 0.0099 (0.0513)	
training:	Epoch: [27][62/408]	Loss 0.0098 (0.0506)	
training:	Epoch: [27][63/408]	Loss 0.0109 (0.0500)	
training:	Epoch: [27][64/408]	Loss 0.0107 (0.0494)	
training:	Epoch: [27][65/408]	Loss 0.0103 (0.0488)	
training:	Epoch: [27][66/408]	Loss 0.0118 (0.0482)	
training:	Epoch: [27][67/408]	Loss 0.0096 (0.0476)	
training:	Epoch: [27][68/408]	Loss 0.0116 (0.0471)	
training:	Epoch: [27][69/408]	Loss 0.0097 (0.0466)	
training:	Epoch: [27][70/408]	Loss 0.0098 (0.0460)	
training:	Epoch: [27][71/408]	Loss 0.2999 (0.0496)	
training:	Epoch: [27][72/408]	Loss 0.0119 (0.0491)	
training:	Epoch: [27][73/408]	Loss 0.0094 (0.0486)	
training:	Epoch: [27][74/408]	Loss 0.0111 (0.0480)	
training:	Epoch: [27][75/408]	Loss 0.0107 (0.0475)	
training:	Epoch: [27][76/408]	Loss 0.0103 (0.0471)	
training:	Epoch: [27][77/408]	Loss 0.2541 (0.0497)	
training:	Epoch: [27][78/408]	Loss 0.0095 (0.0492)	
training:	Epoch: [27][79/408]	Loss 0.0091 (0.0487)	
training:	Epoch: [27][80/408]	Loss 0.0099 (0.0482)	
training:	Epoch: [27][81/408]	Loss 0.0091 (0.0478)	
training:	Epoch: [27][82/408]	Loss 0.0122 (0.0473)	
training:	Epoch: [27][83/408]	Loss 0.0095 (0.0469)	
training:	Epoch: [27][84/408]	Loss 0.0104 (0.0464)	
training:	Epoch: [27][85/408]	Loss 0.0117 (0.0460)	
training:	Epoch: [27][86/408]	Loss 0.0094 (0.0456)	
training:	Epoch: [27][87/408]	Loss 0.0306 (0.0454)	
training:	Epoch: [27][88/408]	Loss 0.0094 (0.0450)	
training:	Epoch: [27][89/408]	Loss 0.0565 (0.0451)	
training:	Epoch: [27][90/408]	Loss 0.0114 (0.0448)	
training:	Epoch: [27][91/408]	Loss 0.0098 (0.0444)	
training:	Epoch: [27][92/408]	Loss 0.2306 (0.0464)	
training:	Epoch: [27][93/408]	Loss 0.0122 (0.0460)	
training:	Epoch: [27][94/408]	Loss 0.0092 (0.0457)	
training:	Epoch: [27][95/408]	Loss 0.0107 (0.0453)	
training:	Epoch: [27][96/408]	Loss 0.5130 (0.0502)	
training:	Epoch: [27][97/408]	Loss 0.0103 (0.0497)	
training:	Epoch: [27][98/408]	Loss 0.0094 (0.0493)	
training:	Epoch: [27][99/408]	Loss 0.2277 (0.0511)	
training:	Epoch: [27][100/408]	Loss 0.0094 (0.0507)	
training:	Epoch: [27][101/408]	Loss 0.0101 (0.0503)	
training:	Epoch: [27][102/408]	Loss 0.0108 (0.0499)	
training:	Epoch: [27][103/408]	Loss 0.0092 (0.0495)	
training:	Epoch: [27][104/408]	Loss 0.0109 (0.0492)	
training:	Epoch: [27][105/408]	Loss 0.0118 (0.0488)	
training:	Epoch: [27][106/408]	Loss 0.0127 (0.0485)	
training:	Epoch: [27][107/408]	Loss 0.0091 (0.0481)	
training:	Epoch: [27][108/408]	Loss 0.0116 (0.0478)	
training:	Epoch: [27][109/408]	Loss 0.2984 (0.0501)	
training:	Epoch: [27][110/408]	Loss 0.0225 (0.0498)	
training:	Epoch: [27][111/408]	Loss 0.0116 (0.0495)	
training:	Epoch: [27][112/408]	Loss 0.0187 (0.0492)	
training:	Epoch: [27][113/408]	Loss 0.0118 (0.0489)	
training:	Epoch: [27][114/408]	Loss 0.0438 (0.0488)	
training:	Epoch: [27][115/408]	Loss 0.0119 (0.0485)	
training:	Epoch: [27][116/408]	Loss 0.0094 (0.0482)	
training:	Epoch: [27][117/408]	Loss 0.0107 (0.0478)	
training:	Epoch: [27][118/408]	Loss 0.0171 (0.0476)	
training:	Epoch: [27][119/408]	Loss 0.0234 (0.0474)	
training:	Epoch: [27][120/408]	Loss 0.0110 (0.0471)	
training:	Epoch: [27][121/408]	Loss 0.0097 (0.0468)	
training:	Epoch: [27][122/408]	Loss 0.0101 (0.0465)	
training:	Epoch: [27][123/408]	Loss 0.0093 (0.0462)	
training:	Epoch: [27][124/408]	Loss 0.0123 (0.0459)	
training:	Epoch: [27][125/408]	Loss 0.0094 (0.0456)	
training:	Epoch: [27][126/408]	Loss 0.0109 (0.0453)	
training:	Epoch: [27][127/408]	Loss 0.2947 (0.0473)	
training:	Epoch: [27][128/408]	Loss 0.0097 (0.0470)	
training:	Epoch: [27][129/408]	Loss 0.0117 (0.0467)	
training:	Epoch: [27][130/408]	Loss 0.0196 (0.0465)	
training:	Epoch: [27][131/408]	Loss 0.0359 (0.0464)	
training:	Epoch: [27][132/408]	Loss 0.0194 (0.0462)	
training:	Epoch: [27][133/408]	Loss 0.0096 (0.0459)	
training:	Epoch: [27][134/408]	Loss 0.0114 (0.0457)	
training:	Epoch: [27][135/408]	Loss 0.0112 (0.0454)	
training:	Epoch: [27][136/408]	Loss 0.3034 (0.0473)	
training:	Epoch: [27][137/408]	Loss 0.0097 (0.0471)	
training:	Epoch: [27][138/408]	Loss 0.0091 (0.0468)	
training:	Epoch: [27][139/408]	Loss 0.0093 (0.0465)	
training:	Epoch: [27][140/408]	Loss 0.0092 (0.0462)	
training:	Epoch: [27][141/408]	Loss 0.0206 (0.0461)	
training:	Epoch: [27][142/408]	Loss 0.0100 (0.0458)	
training:	Epoch: [27][143/408]	Loss 0.0105 (0.0456)	
training:	Epoch: [27][144/408]	Loss 0.3006 (0.0473)	
training:	Epoch: [27][145/408]	Loss 0.0150 (0.0471)	
training:	Epoch: [27][146/408]	Loss 0.2306 (0.0484)	
training:	Epoch: [27][147/408]	Loss 0.0092 (0.0481)	
training:	Epoch: [27][148/408]	Loss 0.0095 (0.0478)	
training:	Epoch: [27][149/408]	Loss 0.0092 (0.0476)	
training:	Epoch: [27][150/408]	Loss 0.0137 (0.0474)	
training:	Epoch: [27][151/408]	Loss 0.0095 (0.0471)	
training:	Epoch: [27][152/408]	Loss 0.0102 (0.0469)	
training:	Epoch: [27][153/408]	Loss 0.0102 (0.0466)	
training:	Epoch: [27][154/408]	Loss 0.0148 (0.0464)	
training:	Epoch: [27][155/408]	Loss 0.0102 (0.0462)	
training:	Epoch: [27][156/408]	Loss 0.0086 (0.0459)	
training:	Epoch: [27][157/408]	Loss 0.0111 (0.0457)	
training:	Epoch: [27][158/408]	Loss 0.2864 (0.0472)	
training:	Epoch: [27][159/408]	Loss 0.0100 (0.0470)	
training:	Epoch: [27][160/408]	Loss 0.0103 (0.0468)	
training:	Epoch: [27][161/408]	Loss 0.0095 (0.0465)	
training:	Epoch: [27][162/408]	Loss 0.2786 (0.0480)	
training:	Epoch: [27][163/408]	Loss 0.2666 (0.0493)	
training:	Epoch: [27][164/408]	Loss 0.2912 (0.0508)	
training:	Epoch: [27][165/408]	Loss 0.0125 (0.0506)	
training:	Epoch: [27][166/408]	Loss 0.0091 (0.0503)	
training:	Epoch: [27][167/408]	Loss 0.0099 (0.0501)	
training:	Epoch: [27][168/408]	Loss 0.0103 (0.0498)	
training:	Epoch: [27][169/408]	Loss 0.0215 (0.0497)	
training:	Epoch: [27][170/408]	Loss 0.0105 (0.0494)	
training:	Epoch: [27][171/408]	Loss 0.0106 (0.0492)	
training:	Epoch: [27][172/408]	Loss 0.0097 (0.0490)	
training:	Epoch: [27][173/408]	Loss 0.0109 (0.0488)	
training:	Epoch: [27][174/408]	Loss 0.0103 (0.0485)	
training:	Epoch: [27][175/408]	Loss 0.0120 (0.0483)	
training:	Epoch: [27][176/408]	Loss 0.0094 (0.0481)	
training:	Epoch: [27][177/408]	Loss 0.0102 (0.0479)	
training:	Epoch: [27][178/408]	Loss 0.0085 (0.0477)	
training:	Epoch: [27][179/408]	Loss 0.0094 (0.0475)	
training:	Epoch: [27][180/408]	Loss 0.0105 (0.0473)	
training:	Epoch: [27][181/408]	Loss 0.0101 (0.0470)	
training:	Epoch: [27][182/408]	Loss 0.0121 (0.0469)	
training:	Epoch: [27][183/408]	Loss 0.0097 (0.0467)	
training:	Epoch: [27][184/408]	Loss 0.0102 (0.0465)	
training:	Epoch: [27][185/408]	Loss 0.0107 (0.0463)	
training:	Epoch: [27][186/408]	Loss 0.3099 (0.0477)	
training:	Epoch: [27][187/408]	Loss 0.0105 (0.0475)	
training:	Epoch: [27][188/408]	Loss 0.0102 (0.0473)	
training:	Epoch: [27][189/408]	Loss 0.0099 (0.0471)	
training:	Epoch: [27][190/408]	Loss 0.0106 (0.0469)	
training:	Epoch: [27][191/408]	Loss 0.0097 (0.0467)	
training:	Epoch: [27][192/408]	Loss 0.0125 (0.0465)	
training:	Epoch: [27][193/408]	Loss 0.0100 (0.0463)	
training:	Epoch: [27][194/408]	Loss 0.0105 (0.0461)	
training:	Epoch: [27][195/408]	Loss 0.1810 (0.0468)	
training:	Epoch: [27][196/408]	Loss 0.0135 (0.0467)	
training:	Epoch: [27][197/408]	Loss 0.0102 (0.0465)	
training:	Epoch: [27][198/408]	Loss 0.0101 (0.0463)	
training:	Epoch: [27][199/408]	Loss 0.0109 (0.0461)	
training:	Epoch: [27][200/408]	Loss 0.0101 (0.0459)	
training:	Epoch: [27][201/408]	Loss 0.0108 (0.0458)	
training:	Epoch: [27][202/408]	Loss 0.0103 (0.0456)	
training:	Epoch: [27][203/408]	Loss 0.0089 (0.0454)	
training:	Epoch: [27][204/408]	Loss 0.0115 (0.0452)	
training:	Epoch: [27][205/408]	Loss 0.0232 (0.0451)	
training:	Epoch: [27][206/408]	Loss 0.0103 (0.0450)	
training:	Epoch: [27][207/408]	Loss 0.0097 (0.0448)	
training:	Epoch: [27][208/408]	Loss 0.0097 (0.0446)	
training:	Epoch: [27][209/408]	Loss 0.0104 (0.0445)	
training:	Epoch: [27][210/408]	Loss 0.0095 (0.0443)	
training:	Epoch: [27][211/408]	Loss 0.0102 (0.0441)	
training:	Epoch: [27][212/408]	Loss 0.3071 (0.0454)	
training:	Epoch: [27][213/408]	Loss 0.3661 (0.0469)	
training:	Epoch: [27][214/408]	Loss 0.0094 (0.0467)	
training:	Epoch: [27][215/408]	Loss 0.0096 (0.0465)	
training:	Epoch: [27][216/408]	Loss 0.0280 (0.0464)	
training:	Epoch: [27][217/408]	Loss 0.0104 (0.0463)	
training:	Epoch: [27][218/408]	Loss 0.0115 (0.0461)	
training:	Epoch: [27][219/408]	Loss 0.0106 (0.0460)	
training:	Epoch: [27][220/408]	Loss 0.0095 (0.0458)	
training:	Epoch: [27][221/408]	Loss 0.5761 (0.0482)	
training:	Epoch: [27][222/408]	Loss 0.0091 (0.0480)	
training:	Epoch: [27][223/408]	Loss 0.0193 (0.0479)	
training:	Epoch: [27][224/408]	Loss 0.0125 (0.0477)	
training:	Epoch: [27][225/408]	Loss 0.0104 (0.0476)	
training:	Epoch: [27][226/408]	Loss 0.2840 (0.0486)	
training:	Epoch: [27][227/408]	Loss 0.0097 (0.0484)	
training:	Epoch: [27][228/408]	Loss 0.2776 (0.0494)	
training:	Epoch: [27][229/408]	Loss 0.0107 (0.0493)	
training:	Epoch: [27][230/408]	Loss 0.0442 (0.0493)	
training:	Epoch: [27][231/408]	Loss 0.0085 (0.0491)	
training:	Epoch: [27][232/408]	Loss 0.0097 (0.0489)	
training:	Epoch: [27][233/408]	Loss 0.0093 (0.0487)	
training:	Epoch: [27][234/408]	Loss 0.0137 (0.0486)	
training:	Epoch: [27][235/408]	Loss 0.0105 (0.0484)	
training:	Epoch: [27][236/408]	Loss 0.1073 (0.0487)	
training:	Epoch: [27][237/408]	Loss 0.0105 (0.0485)	
training:	Epoch: [27][238/408]	Loss 0.0115 (0.0484)	
training:	Epoch: [27][239/408]	Loss 0.0106 (0.0482)	
training:	Epoch: [27][240/408]	Loss 0.0108 (0.0480)	
training:	Epoch: [27][241/408]	Loss 0.0097 (0.0479)	
training:	Epoch: [27][242/408]	Loss 0.0103 (0.0477)	
training:	Epoch: [27][243/408]	Loss 0.0253 (0.0476)	
training:	Epoch: [27][244/408]	Loss 0.0093 (0.0475)	
training:	Epoch: [27][245/408]	Loss 0.0094 (0.0473)	
training:	Epoch: [27][246/408]	Loss 0.0084 (0.0472)	
training:	Epoch: [27][247/408]	Loss 0.0084 (0.0470)	
training:	Epoch: [27][248/408]	Loss 0.3143 (0.0481)	
training:	Epoch: [27][249/408]	Loss 0.0497 (0.0481)	
training:	Epoch: [27][250/408]	Loss 0.0093 (0.0479)	
training:	Epoch: [27][251/408]	Loss 0.0153 (0.0478)	
training:	Epoch: [27][252/408]	Loss 0.0115 (0.0477)	
training:	Epoch: [27][253/408]	Loss 0.0138 (0.0475)	
training:	Epoch: [27][254/408]	Loss 0.2773 (0.0484)	
training:	Epoch: [27][255/408]	Loss 0.0149 (0.0483)	
training:	Epoch: [27][256/408]	Loss 0.0134 (0.0482)	
training:	Epoch: [27][257/408]	Loss 0.0339 (0.0481)	
training:	Epoch: [27][258/408]	Loss 0.0100 (0.0480)	
training:	Epoch: [27][259/408]	Loss 0.0098 (0.0478)	
training:	Epoch: [27][260/408]	Loss 0.0092 (0.0477)	
training:	Epoch: [27][261/408]	Loss 0.0113 (0.0475)	
training:	Epoch: [27][262/408]	Loss 0.0092 (0.0474)	
training:	Epoch: [27][263/408]	Loss 0.3186 (0.0484)	
training:	Epoch: [27][264/408]	Loss 0.0096 (0.0483)	
training:	Epoch: [27][265/408]	Loss 0.3082 (0.0492)	
training:	Epoch: [27][266/408]	Loss 0.2886 (0.0501)	
training:	Epoch: [27][267/408]	Loss 0.0104 (0.0500)	
training:	Epoch: [27][268/408]	Loss 0.0098 (0.0498)	
training:	Epoch: [27][269/408]	Loss 0.0095 (0.0497)	
training:	Epoch: [27][270/408]	Loss 0.0104 (0.0496)	
training:	Epoch: [27][271/408]	Loss 0.0095 (0.0494)	
training:	Epoch: [27][272/408]	Loss 0.0235 (0.0493)	
training:	Epoch: [27][273/408]	Loss 0.0109 (0.0492)	
training:	Epoch: [27][274/408]	Loss 0.0095 (0.0490)	
training:	Epoch: [27][275/408]	Loss 0.0099 (0.0489)	
training:	Epoch: [27][276/408]	Loss 0.2928 (0.0498)	
training:	Epoch: [27][277/408]	Loss 0.0094 (0.0496)	
training:	Epoch: [27][278/408]	Loss 0.2759 (0.0504)	
training:	Epoch: [27][279/408]	Loss 0.0105 (0.0503)	
training:	Epoch: [27][280/408]	Loss 0.0101 (0.0501)	
training:	Epoch: [27][281/408]	Loss 0.0101 (0.0500)	
training:	Epoch: [27][282/408]	Loss 0.0098 (0.0499)	
training:	Epoch: [27][283/408]	Loss 0.0110 (0.0497)	
training:	Epoch: [27][284/408]	Loss 0.0102 (0.0496)	
training:	Epoch: [27][285/408]	Loss 0.0098 (0.0494)	
training:	Epoch: [27][286/408]	Loss 0.0088 (0.0493)	
training:	Epoch: [27][287/408]	Loss 0.0119 (0.0492)	
training:	Epoch: [27][288/408]	Loss 0.0107 (0.0490)	
training:	Epoch: [27][289/408]	Loss 0.0103 (0.0489)	
training:	Epoch: [27][290/408]	Loss 0.0104 (0.0488)	
training:	Epoch: [27][291/408]	Loss 0.0102 (0.0486)	
training:	Epoch: [27][292/408]	Loss 0.0106 (0.0485)	
training:	Epoch: [27][293/408]	Loss 0.0118 (0.0484)	
training:	Epoch: [27][294/408]	Loss 0.0108 (0.0483)	
training:	Epoch: [27][295/408]	Loss 0.0094 (0.0481)	
training:	Epoch: [27][296/408]	Loss 0.0088 (0.0480)	
training:	Epoch: [27][297/408]	Loss 0.2982 (0.0488)	
training:	Epoch: [27][298/408]	Loss 0.0096 (0.0487)	
training:	Epoch: [27][299/408]	Loss 0.0104 (0.0486)	
training:	Epoch: [27][300/408]	Loss 0.0091 (0.0484)	
training:	Epoch: [27][301/408]	Loss 0.0105 (0.0483)	
training:	Epoch: [27][302/408]	Loss 0.0112 (0.0482)	
training:	Epoch: [27][303/408]	Loss 0.0117 (0.0481)	
training:	Epoch: [27][304/408]	Loss 0.0102 (0.0479)	
training:	Epoch: [27][305/408]	Loss 0.0092 (0.0478)	
training:	Epoch: [27][306/408]	Loss 0.0170 (0.0477)	
training:	Epoch: [27][307/408]	Loss 0.0112 (0.0476)	
training:	Epoch: [27][308/408]	Loss 0.0088 (0.0475)	
training:	Epoch: [27][309/408]	Loss 0.0092 (0.0474)	
training:	Epoch: [27][310/408]	Loss 0.0365 (0.0473)	
training:	Epoch: [27][311/408]	Loss 0.0109 (0.0472)	
training:	Epoch: [27][312/408]	Loss 0.0085 (0.0471)	
training:	Epoch: [27][313/408]	Loss 0.4779 (0.0485)	
training:	Epoch: [27][314/408]	Loss 0.0128 (0.0483)	
training:	Epoch: [27][315/408]	Loss 0.0096 (0.0482)	
training:	Epoch: [27][316/408]	Loss 0.0132 (0.0481)	
training:	Epoch: [27][317/408]	Loss 0.2789 (0.0488)	
training:	Epoch: [27][318/408]	Loss 0.0094 (0.0487)	
training:	Epoch: [27][319/408]	Loss 0.0098 (0.0486)	
training:	Epoch: [27][320/408]	Loss 0.0200 (0.0485)	
training:	Epoch: [27][321/408]	Loss 0.0098 (0.0484)	
training:	Epoch: [27][322/408]	Loss 0.0091 (0.0483)	
training:	Epoch: [27][323/408]	Loss 0.0135 (0.0481)	
training:	Epoch: [27][324/408]	Loss 0.0087 (0.0480)	
training:	Epoch: [27][325/408]	Loss 0.0096 (0.0479)	
training:	Epoch: [27][326/408]	Loss 0.0100 (0.0478)	
training:	Epoch: [27][327/408]	Loss 0.0090 (0.0477)	
training:	Epoch: [27][328/408]	Loss 0.0090 (0.0476)	
training:	Epoch: [27][329/408]	Loss 0.0094 (0.0474)	
training:	Epoch: [27][330/408]	Loss 0.0096 (0.0473)	
training:	Epoch: [27][331/408]	Loss 0.0102 (0.0472)	
training:	Epoch: [27][332/408]	Loss 0.0091 (0.0471)	
training:	Epoch: [27][333/408]	Loss 0.0093 (0.0470)	
training:	Epoch: [27][334/408]	Loss 0.0092 (0.0469)	
training:	Epoch: [27][335/408]	Loss 0.3091 (0.0477)	
training:	Epoch: [27][336/408]	Loss 0.0091 (0.0475)	
training:	Epoch: [27][337/408]	Loss 0.0100 (0.0474)	
training:	Epoch: [27][338/408]	Loss 0.0083 (0.0473)	
training:	Epoch: [27][339/408]	Loss 0.0089 (0.0472)	
training:	Epoch: [27][340/408]	Loss 0.0091 (0.0471)	
training:	Epoch: [27][341/408]	Loss 0.3093 (0.0479)	
training:	Epoch: [27][342/408]	Loss 0.0109 (0.0477)	
training:	Epoch: [27][343/408]	Loss 0.2967 (0.0485)	
training:	Epoch: [27][344/408]	Loss 0.0091 (0.0484)	
training:	Epoch: [27][345/408]	Loss 0.0104 (0.0482)	
training:	Epoch: [27][346/408]	Loss 0.0122 (0.0481)	
training:	Epoch: [27][347/408]	Loss 0.0094 (0.0480)	
training:	Epoch: [27][348/408]	Loss 0.0095 (0.0479)	
training:	Epoch: [27][349/408]	Loss 0.0103 (0.0478)	
training:	Epoch: [27][350/408]	Loss 0.3089 (0.0486)	
training:	Epoch: [27][351/408]	Loss 0.0102 (0.0485)	
training:	Epoch: [27][352/408]	Loss 0.0103 (0.0483)	
training:	Epoch: [27][353/408]	Loss 0.0095 (0.0482)	
training:	Epoch: [27][354/408]	Loss 0.0111 (0.0481)	
training:	Epoch: [27][355/408]	Loss 0.0100 (0.0480)	
training:	Epoch: [27][356/408]	Loss 0.0096 (0.0479)	
training:	Epoch: [27][357/408]	Loss 0.0090 (0.0478)	
training:	Epoch: [27][358/408]	Loss 0.0086 (0.0477)	
training:	Epoch: [27][359/408]	Loss 0.0091 (0.0476)	
training:	Epoch: [27][360/408]	Loss 0.0101 (0.0475)	
training:	Epoch: [27][361/408]	Loss 0.0093 (0.0474)	
training:	Epoch: [27][362/408]	Loss 0.3110 (0.0481)	
training:	Epoch: [27][363/408]	Loss 0.0090 (0.0480)	
training:	Epoch: [27][364/408]	Loss 0.0129 (0.0479)	
training:	Epoch: [27][365/408]	Loss 0.0101 (0.0478)	
training:	Epoch: [27][366/408]	Loss 0.0087 (0.0477)	
training:	Epoch: [27][367/408]	Loss 0.0132 (0.0476)	
training:	Epoch: [27][368/408]	Loss 0.3028 (0.0483)	
training:	Epoch: [27][369/408]	Loss 0.0092 (0.0482)	
training:	Epoch: [27][370/408]	Loss 0.0096 (0.0481)	
training:	Epoch: [27][371/408]	Loss 0.0111 (0.0480)	
training:	Epoch: [27][372/408]	Loss 0.3367 (0.0488)	
training:	Epoch: [27][373/408]	Loss 0.0105 (0.0487)	
training:	Epoch: [27][374/408]	Loss 0.0096 (0.0485)	
training:	Epoch: [27][375/408]	Loss 0.0102 (0.0484)	
training:	Epoch: [27][376/408]	Loss 0.0102 (0.0483)	
training:	Epoch: [27][377/408]	Loss 0.0088 (0.0482)	
training:	Epoch: [27][378/408]	Loss 0.0089 (0.0481)	
training:	Epoch: [27][379/408]	Loss 0.2908 (0.0488)	
training:	Epoch: [27][380/408]	Loss 0.0100 (0.0487)	
training:	Epoch: [27][381/408]	Loss 0.0103 (0.0486)	
training:	Epoch: [27][382/408]	Loss 0.0099 (0.0485)	
training:	Epoch: [27][383/408]	Loss 0.0101 (0.0484)	
training:	Epoch: [27][384/408]	Loss 0.3579 (0.0492)	
training:	Epoch: [27][385/408]	Loss 0.0100 (0.0491)	
training:	Epoch: [27][386/408]	Loss 0.0097 (0.0490)	
training:	Epoch: [27][387/408]	Loss 0.0110 (0.0489)	
training:	Epoch: [27][388/408]	Loss 0.0109 (0.0488)	
training:	Epoch: [27][389/408]	Loss 0.0105 (0.0487)	
training:	Epoch: [27][390/408]	Loss 0.3041 (0.0493)	
training:	Epoch: [27][391/408]	Loss 0.0095 (0.0492)	
training:	Epoch: [27][392/408]	Loss 0.2707 (0.0498)	
training:	Epoch: [27][393/408]	Loss 0.0149 (0.0497)	
training:	Epoch: [27][394/408]	Loss 0.0101 (0.0496)	
training:	Epoch: [27][395/408]	Loss 0.0089 (0.0495)	
training:	Epoch: [27][396/408]	Loss 0.0101 (0.0494)	
training:	Epoch: [27][397/408]	Loss 0.0160 (0.0493)	
training:	Epoch: [27][398/408]	Loss 0.0101 (0.0492)	
training:	Epoch: [27][399/408]	Loss 0.0096 (0.0491)	
training:	Epoch: [27][400/408]	Loss 0.0101 (0.0490)	
training:	Epoch: [27][401/408]	Loss 0.0107 (0.0489)	
training:	Epoch: [27][402/408]	Loss 0.0109 (0.0488)	
training:	Epoch: [27][403/408]	Loss 0.0113 (0.0487)	
training:	Epoch: [27][404/408]	Loss 0.3060 (0.0494)	
training:	Epoch: [27][405/408]	Loss 0.0092 (0.0493)	
training:	Epoch: [27][406/408]	Loss 0.1009 (0.0494)	
training:	Epoch: [27][407/408]	Loss 0.0122 (0.0493)	
training:	Epoch: [27][408/408]	Loss 0.0100 (0.0492)	
Training:	 Loss: 0.0491

Training:	 ACC: 0.9925 0.9925 0.9924 0.9927
Validation:	 ACC: 0.7830 0.7838 0.8025 0.7635
Validation:	 Best_BACC: 0.8226 0.8250 0.8772 0.7679
Validation:	 Loss: 0.8917
Pretraining:	Epoch 28/200
----------
training:	Epoch: [28][1/408]	Loss 0.0122 (0.0122)	
training:	Epoch: [28][2/408]	Loss 0.3082 (0.1602)	
training:	Epoch: [28][3/408]	Loss 0.0105 (0.1103)	
training:	Epoch: [28][4/408]	Loss 0.0113 (0.0855)	
training:	Epoch: [28][5/408]	Loss 0.0088 (0.0702)	
training:	Epoch: [28][6/408]	Loss 0.0100 (0.0602)	
training:	Epoch: [28][7/408]	Loss 0.0101 (0.0530)	
training:	Epoch: [28][8/408]	Loss 0.0101 (0.0477)	
training:	Epoch: [28][9/408]	Loss 0.0162 (0.0442)	
training:	Epoch: [28][10/408]	Loss 0.1354 (0.0533)	
training:	Epoch: [28][11/408]	Loss 0.0102 (0.0494)	
training:	Epoch: [28][12/408]	Loss 0.0100 (0.0461)	
training:	Epoch: [28][13/408]	Loss 0.0103 (0.0433)	
training:	Epoch: [28][14/408]	Loss 0.0124 (0.0411)	
training:	Epoch: [28][15/408]	Loss 0.0097 (0.0390)	
training:	Epoch: [28][16/408]	Loss 0.0090 (0.0372)	
training:	Epoch: [28][17/408]	Loss 0.0109 (0.0356)	
training:	Epoch: [28][18/408]	Loss 0.0096 (0.0342)	
training:	Epoch: [28][19/408]	Loss 0.0106 (0.0329)	
training:	Epoch: [28][20/408]	Loss 0.2921 (0.0459)	
training:	Epoch: [28][21/408]	Loss 0.3131 (0.0586)	
training:	Epoch: [28][22/408]	Loss 0.0093 (0.0564)	
training:	Epoch: [28][23/408]	Loss 0.2692 (0.0656)	
training:	Epoch: [28][24/408]	Loss 0.0117 (0.0634)	
training:	Epoch: [28][25/408]	Loss 0.0097 (0.0612)	
training:	Epoch: [28][26/408]	Loss 0.0113 (0.0593)	
training:	Epoch: [28][27/408]	Loss 0.0091 (0.0575)	
training:	Epoch: [28][28/408]	Loss 0.0094 (0.0557)	
training:	Epoch: [28][29/408]	Loss 0.0097 (0.0542)	
training:	Epoch: [28][30/408]	Loss 0.0097 (0.0527)	
training:	Epoch: [28][31/408]	Loss 0.0098 (0.0513)	
training:	Epoch: [28][32/408]	Loss 0.0096 (0.0500)	
training:	Epoch: [28][33/408]	Loss 0.0159 (0.0490)	
training:	Epoch: [28][34/408]	Loss 0.0132 (0.0479)	
training:	Epoch: [28][35/408]	Loss 0.2958 (0.0550)	
training:	Epoch: [28][36/408]	Loss 0.2964 (0.0617)	
training:	Epoch: [28][37/408]	Loss 0.0096 (0.0603)	
training:	Epoch: [28][38/408]	Loss 0.0102 (0.0590)	
training:	Epoch: [28][39/408]	Loss 0.0091 (0.0577)	
training:	Epoch: [28][40/408]	Loss 0.0108 (0.0565)	
training:	Epoch: [28][41/408]	Loss 0.1379 (0.0585)	
training:	Epoch: [28][42/408]	Loss 0.0111 (0.0574)	
training:	Epoch: [28][43/408]	Loss 0.2837 (0.0626)	
training:	Epoch: [28][44/408]	Loss 0.0106 (0.0615)	
training:	Epoch: [28][45/408]	Loss 0.0091 (0.0603)	
training:	Epoch: [28][46/408]	Loss 0.1163 (0.0615)	
training:	Epoch: [28][47/408]	Loss 0.0102 (0.0604)	
training:	Epoch: [28][48/408]	Loss 0.0099 (0.0594)	
training:	Epoch: [28][49/408]	Loss 0.0103 (0.0584)	
training:	Epoch: [28][50/408]	Loss 0.0107 (0.0574)	
training:	Epoch: [28][51/408]	Loss 0.0093 (0.0565)	
training:	Epoch: [28][52/408]	Loss 0.1498 (0.0583)	
training:	Epoch: [28][53/408]	Loss 0.0109 (0.0574)	
training:	Epoch: [28][54/408]	Loss 0.2981 (0.0618)	
training:	Epoch: [28][55/408]	Loss 0.0774 (0.0621)	
training:	Epoch: [28][56/408]	Loss 0.0150 (0.0613)	
training:	Epoch: [28][57/408]	Loss 0.0093 (0.0604)	
training:	Epoch: [28][58/408]	Loss 0.0100 (0.0595)	
training:	Epoch: [28][59/408]	Loss 0.3026 (0.0636)	
training:	Epoch: [28][60/408]	Loss 0.0093 (0.0627)	
training:	Epoch: [28][61/408]	Loss 0.0120 (0.0619)	
training:	Epoch: [28][62/408]	Loss 0.0118 (0.0611)	
training:	Epoch: [28][63/408]	Loss 0.0099 (0.0603)	
training:	Epoch: [28][64/408]	Loss 0.0090 (0.0595)	
training:	Epoch: [28][65/408]	Loss 0.0123 (0.0587)	
training:	Epoch: [28][66/408]	Loss 0.0100 (0.0580)	
training:	Epoch: [28][67/408]	Loss 0.0088 (0.0573)	
training:	Epoch: [28][68/408]	Loss 0.0113 (0.0566)	
training:	Epoch: [28][69/408]	Loss 0.0096 (0.0559)	
training:	Epoch: [28][70/408]	Loss 0.2880 (0.0592)	
training:	Epoch: [28][71/408]	Loss 0.0101 (0.0585)	
training:	Epoch: [28][72/408]	Loss 0.0101 (0.0578)	
training:	Epoch: [28][73/408]	Loss 0.2991 (0.0612)	
training:	Epoch: [28][74/408]	Loss 0.0094 (0.0605)	
training:	Epoch: [28][75/408]	Loss 0.0100 (0.0598)	
training:	Epoch: [28][76/408]	Loss 0.0093 (0.0591)	
training:	Epoch: [28][77/408]	Loss 0.0101 (0.0585)	
training:	Epoch: [28][78/408]	Loss 0.0112 (0.0579)	
training:	Epoch: [28][79/408]	Loss 0.0098 (0.0573)	
training:	Epoch: [28][80/408]	Loss 0.0097 (0.0567)	
training:	Epoch: [28][81/408]	Loss 0.0096 (0.0561)	
training:	Epoch: [28][82/408]	Loss 0.0196 (0.0556)	
training:	Epoch: [28][83/408]	Loss 0.0093 (0.0551)	
training:	Epoch: [28][84/408]	Loss 0.0108 (0.0546)	
training:	Epoch: [28][85/408]	Loss 0.0105 (0.0540)	
training:	Epoch: [28][86/408]	Loss 0.0130 (0.0536)	
training:	Epoch: [28][87/408]	Loss 0.0102 (0.0531)	
training:	Epoch: [28][88/408]	Loss 0.0094 (0.0526)	
training:	Epoch: [28][89/408]	Loss 0.0112 (0.0521)	
training:	Epoch: [28][90/408]	Loss 0.0092 (0.0516)	
training:	Epoch: [28][91/408]	Loss 0.0101 (0.0512)	
training:	Epoch: [28][92/408]	Loss 0.0112 (0.0507)	
training:	Epoch: [28][93/408]	Loss 0.0102 (0.0503)	
training:	Epoch: [28][94/408]	Loss 0.0098 (0.0499)	
training:	Epoch: [28][95/408]	Loss 0.0962 (0.0504)	
training:	Epoch: [28][96/408]	Loss 0.0089 (0.0499)	
training:	Epoch: [28][97/408]	Loss 0.0109 (0.0495)	
training:	Epoch: [28][98/408]	Loss 0.3126 (0.0522)	
training:	Epoch: [28][99/408]	Loss 0.2812 (0.0545)	
training:	Epoch: [28][100/408]	Loss 0.0086 (0.0541)	
training:	Epoch: [28][101/408]	Loss 0.0100 (0.0536)	
training:	Epoch: [28][102/408]	Loss 0.0311 (0.0534)	
training:	Epoch: [28][103/408]	Loss 0.0087 (0.0530)	
training:	Epoch: [28][104/408]	Loss 0.0120 (0.0526)	
training:	Epoch: [28][105/408]	Loss 0.0095 (0.0522)	
training:	Epoch: [28][106/408]	Loss 0.2817 (0.0543)	
training:	Epoch: [28][107/408]	Loss 0.0110 (0.0539)	
training:	Epoch: [28][108/408]	Loss 0.0094 (0.0535)	
training:	Epoch: [28][109/408]	Loss 0.0096 (0.0531)	
training:	Epoch: [28][110/408]	Loss 0.0097 (0.0527)	
training:	Epoch: [28][111/408]	Loss 0.0107 (0.0523)	
training:	Epoch: [28][112/408]	Loss 0.0089 (0.0520)	
training:	Epoch: [28][113/408]	Loss 0.0104 (0.0516)	
training:	Epoch: [28][114/408]	Loss 0.0110 (0.0512)	
training:	Epoch: [28][115/408]	Loss 0.0093 (0.0509)	
training:	Epoch: [28][116/408]	Loss 0.0085 (0.0505)	
training:	Epoch: [28][117/408]	Loss 0.0100 (0.0502)	
training:	Epoch: [28][118/408]	Loss 0.0100 (0.0498)	
training:	Epoch: [28][119/408]	Loss 0.0135 (0.0495)	
training:	Epoch: [28][120/408]	Loss 0.2680 (0.0513)	
training:	Epoch: [28][121/408]	Loss 0.0098 (0.0510)	
training:	Epoch: [28][122/408]	Loss 0.0100 (0.0507)	
training:	Epoch: [28][123/408]	Loss 0.0088 (0.0503)	
training:	Epoch: [28][124/408]	Loss 0.0092 (0.0500)	
training:	Epoch: [28][125/408]	Loss 0.0094 (0.0497)	
training:	Epoch: [28][126/408]	Loss 0.0092 (0.0493)	
training:	Epoch: [28][127/408]	Loss 0.0105 (0.0490)	
training:	Epoch: [28][128/408]	Loss 0.0094 (0.0487)	
training:	Epoch: [28][129/408]	Loss 0.0125 (0.0484)	
training:	Epoch: [28][130/408]	Loss 0.0114 (0.0482)	
training:	Epoch: [28][131/408]	Loss 0.0098 (0.0479)	
training:	Epoch: [28][132/408]	Loss 0.5671 (0.0518)	
training:	Epoch: [28][133/408]	Loss 0.0099 (0.0515)	
training:	Epoch: [28][134/408]	Loss 0.0109 (0.0512)	
training:	Epoch: [28][135/408]	Loss 0.0098 (0.0509)	
training:	Epoch: [28][136/408]	Loss 0.0108 (0.0506)	
training:	Epoch: [28][137/408]	Loss 0.0098 (0.0503)	
training:	Epoch: [28][138/408]	Loss 0.0094 (0.0500)	
training:	Epoch: [28][139/408]	Loss 0.0105 (0.0497)	
training:	Epoch: [28][140/408]	Loss 0.0096 (0.0494)	
training:	Epoch: [28][141/408]	Loss 0.0102 (0.0491)	
training:	Epoch: [28][142/408]	Loss 0.0096 (0.0489)	
training:	Epoch: [28][143/408]	Loss 0.0095 (0.0486)	
training:	Epoch: [28][144/408]	Loss 0.8160 (0.0539)	
training:	Epoch: [28][145/408]	Loss 0.0254 (0.0537)	
training:	Epoch: [28][146/408]	Loss 0.0103 (0.0534)	
training:	Epoch: [28][147/408]	Loss 0.0090 (0.0531)	
training:	Epoch: [28][148/408]	Loss 0.0099 (0.0528)	
training:	Epoch: [28][149/408]	Loss 0.0092 (0.0525)	
training:	Epoch: [28][150/408]	Loss 0.0183 (0.0523)	
training:	Epoch: [28][151/408]	Loss 0.0101 (0.0520)	
training:	Epoch: [28][152/408]	Loss 0.0127 (0.0518)	
training:	Epoch: [28][153/408]	Loss 0.0104 (0.0515)	
training:	Epoch: [28][154/408]	Loss 0.0116 (0.0512)	
training:	Epoch: [28][155/408]	Loss 0.0090 (0.0510)	
training:	Epoch: [28][156/408]	Loss 0.0131 (0.0507)	
training:	Epoch: [28][157/408]	Loss 0.0101 (0.0505)	
training:	Epoch: [28][158/408]	Loss 0.0113 (0.0502)	
training:	Epoch: [28][159/408]	Loss 0.0093 (0.0500)	
training:	Epoch: [28][160/408]	Loss 0.0097 (0.0497)	
training:	Epoch: [28][161/408]	Loss 0.0093 (0.0494)	
training:	Epoch: [28][162/408]	Loss 0.0115 (0.0492)	
training:	Epoch: [28][163/408]	Loss 0.0099 (0.0490)	
training:	Epoch: [28][164/408]	Loss 0.0171 (0.0488)	
training:	Epoch: [28][165/408]	Loss 0.0154 (0.0486)	
training:	Epoch: [28][166/408]	Loss 0.0098 (0.0483)	
training:	Epoch: [28][167/408]	Loss 0.0210 (0.0482)	
training:	Epoch: [28][168/408]	Loss 0.0093 (0.0479)	
training:	Epoch: [28][169/408]	Loss 0.2892 (0.0494)	
training:	Epoch: [28][170/408]	Loss 0.0086 (0.0491)	
training:	Epoch: [28][171/408]	Loss 0.0090 (0.0489)	
training:	Epoch: [28][172/408]	Loss 0.0099 (0.0487)	
training:	Epoch: [28][173/408]	Loss 0.2752 (0.0500)	
training:	Epoch: [28][174/408]	Loss 0.0104 (0.0498)	
training:	Epoch: [28][175/408]	Loss 0.0097 (0.0495)	
training:	Epoch: [28][176/408]	Loss 0.0088 (0.0493)	
training:	Epoch: [28][177/408]	Loss 0.2973 (0.0507)	
training:	Epoch: [28][178/408]	Loss 0.3077 (0.0521)	
training:	Epoch: [28][179/408]	Loss 0.0097 (0.0519)	
training:	Epoch: [28][180/408]	Loss 0.0094 (0.0517)	
training:	Epoch: [28][181/408]	Loss 0.0125 (0.0515)	
training:	Epoch: [28][182/408]	Loss 0.0095 (0.0512)	
training:	Epoch: [28][183/408]	Loss 0.0090 (0.0510)	
training:	Epoch: [28][184/408]	Loss 0.0096 (0.0508)	
training:	Epoch: [28][185/408]	Loss 0.0101 (0.0505)	
training:	Epoch: [28][186/408]	Loss 0.0097 (0.0503)	
training:	Epoch: [28][187/408]	Loss 0.0083 (0.0501)	
training:	Epoch: [28][188/408]	Loss 0.0087 (0.0499)	
training:	Epoch: [28][189/408]	Loss 0.0094 (0.0497)	
training:	Epoch: [28][190/408]	Loss 0.3106 (0.0510)	
training:	Epoch: [28][191/408]	Loss 0.0100 (0.0508)	
training:	Epoch: [28][192/408]	Loss 0.0091 (0.0506)	
training:	Epoch: [28][193/408]	Loss 0.0100 (0.0504)	
training:	Epoch: [28][194/408]	Loss 0.0111 (0.0502)	
training:	Epoch: [28][195/408]	Loss 0.0096 (0.0500)	
training:	Epoch: [28][196/408]	Loss 0.0182 (0.0498)	
training:	Epoch: [28][197/408]	Loss 0.0101 (0.0496)	
training:	Epoch: [28][198/408]	Loss 0.0091 (0.0494)	
training:	Epoch: [28][199/408]	Loss 0.0339 (0.0493)	
training:	Epoch: [28][200/408]	Loss 0.3206 (0.0507)	
training:	Epoch: [28][201/408]	Loss 0.0119 (0.0505)	
training:	Epoch: [28][202/408]	Loss 0.0090 (0.0503)	
training:	Epoch: [28][203/408]	Loss 0.0185 (0.0501)	
training:	Epoch: [28][204/408]	Loss 0.0100 (0.0499)	
training:	Epoch: [28][205/408]	Loss 0.0095 (0.0497)	
training:	Epoch: [28][206/408]	Loss 0.0105 (0.0496)	
training:	Epoch: [28][207/408]	Loss 0.0102 (0.0494)	
training:	Epoch: [28][208/408]	Loss 0.0134 (0.0492)	
training:	Epoch: [28][209/408]	Loss 0.0120 (0.0490)	
training:	Epoch: [28][210/408]	Loss 0.0104 (0.0488)	
training:	Epoch: [28][211/408]	Loss 0.0099 (0.0486)	
training:	Epoch: [28][212/408]	Loss 0.0102 (0.0485)	
training:	Epoch: [28][213/408]	Loss 0.0094 (0.0483)	
training:	Epoch: [28][214/408]	Loss 0.0197 (0.0481)	
training:	Epoch: [28][215/408]	Loss 0.0133 (0.0480)	
training:	Epoch: [28][216/408]	Loss 0.0101 (0.0478)	
training:	Epoch: [28][217/408]	Loss 0.0109 (0.0476)	
training:	Epoch: [28][218/408]	Loss 0.0098 (0.0475)	
training:	Epoch: [28][219/408]	Loss 0.0089 (0.0473)	
training:	Epoch: [28][220/408]	Loss 0.0095 (0.0471)	
training:	Epoch: [28][221/408]	Loss 0.0092 (0.0469)	
training:	Epoch: [28][222/408]	Loss 0.0096 (0.0468)	
training:	Epoch: [28][223/408]	Loss 0.0096 (0.0466)	
training:	Epoch: [28][224/408]	Loss 0.0112 (0.0465)	
training:	Epoch: [28][225/408]	Loss 0.2602 (0.0474)	
training:	Epoch: [28][226/408]	Loss 0.0107 (0.0472)	
training:	Epoch: [28][227/408]	Loss 0.0083 (0.0471)	
training:	Epoch: [28][228/408]	Loss 0.0102 (0.0469)	
training:	Epoch: [28][229/408]	Loss 0.0093 (0.0467)	
training:	Epoch: [28][230/408]	Loss 0.0101 (0.0466)	
training:	Epoch: [28][231/408]	Loss 0.2909 (0.0476)	
training:	Epoch: [28][232/408]	Loss 0.0099 (0.0475)	
training:	Epoch: [28][233/408]	Loss 0.0095 (0.0473)	
training:	Epoch: [28][234/408]	Loss 0.0099 (0.0472)	
training:	Epoch: [28][235/408]	Loss 0.0090 (0.0470)	
training:	Epoch: [28][236/408]	Loss 0.0098 (0.0468)	
training:	Epoch: [28][237/408]	Loss 0.0087 (0.0467)	
training:	Epoch: [28][238/408]	Loss 0.0102 (0.0465)	
training:	Epoch: [28][239/408]	Loss 0.0118 (0.0464)	
training:	Epoch: [28][240/408]	Loss 0.0213 (0.0463)	
training:	Epoch: [28][241/408]	Loss 0.0094 (0.0461)	
training:	Epoch: [28][242/408]	Loss 0.0094 (0.0460)	
training:	Epoch: [28][243/408]	Loss 0.0114 (0.0458)	
training:	Epoch: [28][244/408]	Loss 0.0095 (0.0457)	
training:	Epoch: [28][245/408]	Loss 0.0084 (0.0455)	
training:	Epoch: [28][246/408]	Loss 0.0118 (0.0454)	
training:	Epoch: [28][247/408]	Loss 0.0101 (0.0452)	
training:	Epoch: [28][248/408]	Loss 0.0090 (0.0451)	
training:	Epoch: [28][249/408]	Loss 0.0101 (0.0450)	
training:	Epoch: [28][250/408]	Loss 0.0091 (0.0448)	
training:	Epoch: [28][251/408]	Loss 0.0095 (0.0447)	
training:	Epoch: [28][252/408]	Loss 0.0103 (0.0445)	
training:	Epoch: [28][253/408]	Loss 0.0101 (0.0444)	
training:	Epoch: [28][254/408]	Loss 0.0105 (0.0443)	
training:	Epoch: [28][255/408]	Loss 0.0098 (0.0441)	
training:	Epoch: [28][256/408]	Loss 0.0097 (0.0440)	
training:	Epoch: [28][257/408]	Loss 0.0100 (0.0439)	
training:	Epoch: [28][258/408]	Loss 0.0108 (0.0437)	
training:	Epoch: [28][259/408]	Loss 0.0091 (0.0436)	
training:	Epoch: [28][260/408]	Loss 0.0089 (0.0435)	
training:	Epoch: [28][261/408]	Loss 0.2979 (0.0444)	
training:	Epoch: [28][262/408]	Loss 0.0082 (0.0443)	
training:	Epoch: [28][263/408]	Loss 0.0097 (0.0442)	
training:	Epoch: [28][264/408]	Loss 0.0105 (0.0440)	
training:	Epoch: [28][265/408]	Loss 0.0085 (0.0439)	
training:	Epoch: [28][266/408]	Loss 0.0207 (0.0438)	
training:	Epoch: [28][267/408]	Loss 0.0088 (0.0437)	
training:	Epoch: [28][268/408]	Loss 0.0091 (0.0436)	
training:	Epoch: [28][269/408]	Loss 0.0085 (0.0434)	
training:	Epoch: [28][270/408]	Loss 0.0090 (0.0433)	
training:	Epoch: [28][271/408]	Loss 0.0144 (0.0432)	
training:	Epoch: [28][272/408]	Loss 0.0096 (0.0431)	
training:	Epoch: [28][273/408]	Loss 0.0087 (0.0430)	
training:	Epoch: [28][274/408]	Loss 0.3081 (0.0439)	
training:	Epoch: [28][275/408]	Loss 0.0083 (0.0438)	
training:	Epoch: [28][276/408]	Loss 0.0092 (0.0437)	
training:	Epoch: [28][277/408]	Loss 0.0093 (0.0435)	
training:	Epoch: [28][278/408]	Loss 0.0084 (0.0434)	
training:	Epoch: [28][279/408]	Loss 0.0097 (0.0433)	
training:	Epoch: [28][280/408]	Loss 0.0093 (0.0432)	
training:	Epoch: [28][281/408]	Loss 0.0089 (0.0431)	
training:	Epoch: [28][282/408]	Loss 0.0092 (0.0429)	
training:	Epoch: [28][283/408]	Loss 0.0090 (0.0428)	
training:	Epoch: [28][284/408]	Loss 0.2944 (0.0437)	
training:	Epoch: [28][285/408]	Loss 0.0148 (0.0436)	
training:	Epoch: [28][286/408]	Loss 0.2968 (0.0445)	
training:	Epoch: [28][287/408]	Loss 0.0092 (0.0444)	
training:	Epoch: [28][288/408]	Loss 0.0091 (0.0442)	
training:	Epoch: [28][289/408]	Loss 0.3030 (0.0451)	
training:	Epoch: [28][290/408]	Loss 0.3039 (0.0460)	
training:	Epoch: [28][291/408]	Loss 0.2998 (0.0469)	
training:	Epoch: [28][292/408]	Loss 0.0093 (0.0468)	
training:	Epoch: [28][293/408]	Loss 0.0085 (0.0466)	
training:	Epoch: [28][294/408]	Loss 0.2959 (0.0475)	
training:	Epoch: [28][295/408]	Loss 0.0107 (0.0474)	
training:	Epoch: [28][296/408]	Loss 0.0089 (0.0472)	
training:	Epoch: [28][297/408]	Loss 0.3166 (0.0481)	
training:	Epoch: [28][298/408]	Loss 0.0096 (0.0480)	
training:	Epoch: [28][299/408]	Loss 0.0094 (0.0479)	
training:	Epoch: [28][300/408]	Loss 0.0088 (0.0477)	
training:	Epoch: [28][301/408]	Loss 0.0097 (0.0476)	
training:	Epoch: [28][302/408]	Loss 0.0097 (0.0475)	
training:	Epoch: [28][303/408]	Loss 0.0092 (0.0474)	
training:	Epoch: [28][304/408]	Loss 0.0088 (0.0472)	
training:	Epoch: [28][305/408]	Loss 0.0094 (0.0471)	
training:	Epoch: [28][306/408]	Loss 0.2978 (0.0479)	
training:	Epoch: [28][307/408]	Loss 0.0105 (0.0478)	
training:	Epoch: [28][308/408]	Loss 0.0098 (0.0477)	
training:	Epoch: [28][309/408]	Loss 0.0100 (0.0476)	
training:	Epoch: [28][310/408]	Loss 0.0101 (0.0474)	
training:	Epoch: [28][311/408]	Loss 0.0100 (0.0473)	
training:	Epoch: [28][312/408]	Loss 0.0090 (0.0472)	
training:	Epoch: [28][313/408]	Loss 0.0095 (0.0471)	
training:	Epoch: [28][314/408]	Loss 0.0090 (0.0470)	
training:	Epoch: [28][315/408]	Loss 0.0099 (0.0468)	
training:	Epoch: [28][316/408]	Loss 0.0095 (0.0467)	
training:	Epoch: [28][317/408]	Loss 0.0086 (0.0466)	
training:	Epoch: [28][318/408]	Loss 0.0092 (0.0465)	
training:	Epoch: [28][319/408]	Loss 0.0085 (0.0464)	
training:	Epoch: [28][320/408]	Loss 0.0088 (0.0463)	
training:	Epoch: [28][321/408]	Loss 0.0094 (0.0461)	
training:	Epoch: [28][322/408]	Loss 0.0092 (0.0460)	
training:	Epoch: [28][323/408]	Loss 0.0093 (0.0459)	
training:	Epoch: [28][324/408]	Loss 0.0091 (0.0458)	
training:	Epoch: [28][325/408]	Loss 0.2884 (0.0465)	
training:	Epoch: [28][326/408]	Loss 0.0098 (0.0464)	
training:	Epoch: [28][327/408]	Loss 0.2984 (0.0472)	
training:	Epoch: [28][328/408]	Loss 0.0101 (0.0471)	
training:	Epoch: [28][329/408]	Loss 0.0097 (0.0470)	
training:	Epoch: [28][330/408]	Loss 0.0093 (0.0469)	
training:	Epoch: [28][331/408]	Loss 0.0082 (0.0467)	
training:	Epoch: [28][332/408]	Loss 0.0099 (0.0466)	
training:	Epoch: [28][333/408]	Loss 0.0091 (0.0465)	
training:	Epoch: [28][334/408]	Loss 0.0099 (0.0464)	
training:	Epoch: [28][335/408]	Loss 0.0094 (0.0463)	
training:	Epoch: [28][336/408]	Loss 0.0090 (0.0462)	
training:	Epoch: [28][337/408]	Loss 0.0095 (0.0461)	
training:	Epoch: [28][338/408]	Loss 0.0113 (0.0460)	
training:	Epoch: [28][339/408]	Loss 0.0088 (0.0459)	
training:	Epoch: [28][340/408]	Loss 0.0105 (0.0458)	
training:	Epoch: [28][341/408]	Loss 0.0099 (0.0457)	
training:	Epoch: [28][342/408]	Loss 0.0095 (0.0456)	
training:	Epoch: [28][343/408]	Loss 0.0092 (0.0454)	
training:	Epoch: [28][344/408]	Loss 0.0089 (0.0453)	
training:	Epoch: [28][345/408]	Loss 0.2989 (0.0461)	
training:	Epoch: [28][346/408]	Loss 0.0102 (0.0460)	
training:	Epoch: [28][347/408]	Loss 0.0087 (0.0459)	
training:	Epoch: [28][348/408]	Loss 0.0099 (0.0458)	
training:	Epoch: [28][349/408]	Loss 0.0100 (0.0457)	
training:	Epoch: [28][350/408]	Loss 0.0094 (0.0456)	
training:	Epoch: [28][351/408]	Loss 0.0093 (0.0455)	
training:	Epoch: [28][352/408]	Loss 0.0096 (0.0453)	
training:	Epoch: [28][353/408]	Loss 0.0101 (0.0452)	
training:	Epoch: [28][354/408]	Loss 0.0091 (0.0451)	
training:	Epoch: [28][355/408]	Loss 0.0106 (0.0450)	
training:	Epoch: [28][356/408]	Loss 0.0099 (0.0450)	
training:	Epoch: [28][357/408]	Loss 0.0088 (0.0448)	
training:	Epoch: [28][358/408]	Loss 0.0094 (0.0448)	
training:	Epoch: [28][359/408]	Loss 0.0128 (0.0447)	
training:	Epoch: [28][360/408]	Loss 0.0092 (0.0446)	
training:	Epoch: [28][361/408]	Loss 0.0141 (0.0445)	
training:	Epoch: [28][362/408]	Loss 0.0099 (0.0444)	
training:	Epoch: [28][363/408]	Loss 0.0101 (0.0443)	
training:	Epoch: [28][364/408]	Loss 0.0100 (0.0442)	
training:	Epoch: [28][365/408]	Loss 0.0085 (0.0441)	
training:	Epoch: [28][366/408]	Loss 0.0095 (0.0440)	
training:	Epoch: [28][367/408]	Loss 0.0091 (0.0439)	
training:	Epoch: [28][368/408]	Loss 0.0100 (0.0438)	
training:	Epoch: [28][369/408]	Loss 0.2985 (0.0445)	
training:	Epoch: [28][370/408]	Loss 0.0097 (0.0444)	
training:	Epoch: [28][371/408]	Loss 0.0086 (0.0443)	
training:	Epoch: [28][372/408]	Loss 0.0091 (0.0442)	
training:	Epoch: [28][373/408]	Loss 0.0090 (0.0441)	
training:	Epoch: [28][374/408]	Loss 0.0088 (0.0440)	
training:	Epoch: [28][375/408]	Loss 0.2621 (0.0446)	
training:	Epoch: [28][376/408]	Loss 0.0097 (0.0445)	
training:	Epoch: [28][377/408]	Loss 0.2965 (0.0452)	
training:	Epoch: [28][378/408]	Loss 0.0091 (0.0451)	
training:	Epoch: [28][379/408]	Loss 0.0119 (0.0450)	
training:	Epoch: [28][380/408]	Loss 0.0124 (0.0449)	
training:	Epoch: [28][381/408]	Loss 0.0091 (0.0448)	
training:	Epoch: [28][382/408]	Loss 0.0089 (0.0447)	
training:	Epoch: [28][383/408]	Loss 0.0093 (0.0446)	
training:	Epoch: [28][384/408]	Loss 0.3081 (0.0453)	
training:	Epoch: [28][385/408]	Loss 0.0096 (0.0452)	
training:	Epoch: [28][386/408]	Loss 0.0097 (0.0451)	
training:	Epoch: [28][387/408]	Loss 0.0084 (0.0450)	
training:	Epoch: [28][388/408]	Loss 0.0096 (0.0450)	
training:	Epoch: [28][389/408]	Loss 0.0104 (0.0449)	
training:	Epoch: [28][390/408]	Loss 0.0088 (0.0448)	
training:	Epoch: [28][391/408]	Loss 0.2919 (0.0454)	
training:	Epoch: [28][392/408]	Loss 0.0124 (0.0453)	
training:	Epoch: [28][393/408]	Loss 0.0082 (0.0452)	
training:	Epoch: [28][394/408]	Loss 0.0090 (0.0451)	
training:	Epoch: [28][395/408]	Loss 0.0091 (0.0450)	
training:	Epoch: [28][396/408]	Loss 0.0100 (0.0450)	
training:	Epoch: [28][397/408]	Loss 0.2922 (0.0456)	
training:	Epoch: [28][398/408]	Loss 0.0095 (0.0455)	
training:	Epoch: [28][399/408]	Loss 0.0099 (0.0454)	
training:	Epoch: [28][400/408]	Loss 0.0091 (0.0453)	
training:	Epoch: [28][401/408]	Loss 0.0088 (0.0452)	
training:	Epoch: [28][402/408]	Loss 0.0103 (0.0451)	
training:	Epoch: [28][403/408]	Loss 0.0089 (0.0450)	
training:	Epoch: [28][404/408]	Loss 0.0085 (0.0449)	
training:	Epoch: [28][405/408]	Loss 0.0090 (0.0449)	
training:	Epoch: [28][406/408]	Loss 0.2957 (0.0455)	
training:	Epoch: [28][407/408]	Loss 0.2902 (0.0461)	
training:	Epoch: [28][408/408]	Loss 0.0181 (0.0460)	
Training:	 Loss: 0.0459

Training:	 ACC: 0.9925 0.9925 0.9924 0.9927
Validation:	 ACC: 0.7826 0.7833 0.7973 0.7679
Validation:	 Best_BACC: 0.8226 0.8250 0.8772 0.7679
Validation:	 Loss: 0.9005
Pretraining:	Epoch 29/200
----------
training:	Epoch: [29][1/408]	Loss 0.0088 (0.0088)	
training:	Epoch: [29][2/408]	Loss 0.0080 (0.0084)	
training:	Epoch: [29][3/408]	Loss 0.0096 (0.0088)	
training:	Epoch: [29][4/408]	Loss 0.0097 (0.0090)	
training:	Epoch: [29][5/408]	Loss 0.0089 (0.0090)	
training:	Epoch: [29][6/408]	Loss 0.0093 (0.0090)	
training:	Epoch: [29][7/408]	Loss 0.0093 (0.0091)	
training:	Epoch: [29][8/408]	Loss 0.0086 (0.0090)	
training:	Epoch: [29][9/408]	Loss 0.0097 (0.0091)	
training:	Epoch: [29][10/408]	Loss 0.0097 (0.0092)	
training:	Epoch: [29][11/408]	Loss 0.0097 (0.0092)	
training:	Epoch: [29][12/408]	Loss 0.0086 (0.0092)	
training:	Epoch: [29][13/408]	Loss 0.0090 (0.0091)	
training:	Epoch: [29][14/408]	Loss 0.0088 (0.0091)	
training:	Epoch: [29][15/408]	Loss 0.0091 (0.0091)	
training:	Epoch: [29][16/408]	Loss 0.3012 (0.0274)	
training:	Epoch: [29][17/408]	Loss 0.0085 (0.0263)	
training:	Epoch: [29][18/408]	Loss 0.0098 (0.0253)	
training:	Epoch: [29][19/408]	Loss 0.0088 (0.0245)	
training:	Epoch: [29][20/408]	Loss 0.0097 (0.0237)	
training:	Epoch: [29][21/408]	Loss 0.2923 (0.0365)	
training:	Epoch: [29][22/408]	Loss 0.0088 (0.0353)	
training:	Epoch: [29][23/408]	Loss 0.0090 (0.0341)	
training:	Epoch: [29][24/408]	Loss 0.0086 (0.0331)	
training:	Epoch: [29][25/408]	Loss 0.2570 (0.0420)	
training:	Epoch: [29][26/408]	Loss 0.2877 (0.0515)	
training:	Epoch: [29][27/408]	Loss 0.0087 (0.0499)	
training:	Epoch: [29][28/408]	Loss 0.0088 (0.0484)	
training:	Epoch: [29][29/408]	Loss 0.0108 (0.0471)	
training:	Epoch: [29][30/408]	Loss 0.0081 (0.0458)	
training:	Epoch: [29][31/408]	Loss 0.0093 (0.0446)	
training:	Epoch: [29][32/408]	Loss 0.3130 (0.0530)	
training:	Epoch: [29][33/408]	Loss 0.2753 (0.0598)	
training:	Epoch: [29][34/408]	Loss 0.0242 (0.0587)	
training:	Epoch: [29][35/408]	Loss 0.0090 (0.0573)	
training:	Epoch: [29][36/408]	Loss 0.0088 (0.0560)	
training:	Epoch: [29][37/408]	Loss 0.2807 (0.0620)	
training:	Epoch: [29][38/408]	Loss 0.0101 (0.0607)	
training:	Epoch: [29][39/408]	Loss 0.0103 (0.0594)	
training:	Epoch: [29][40/408]	Loss 0.0108 (0.0582)	
training:	Epoch: [29][41/408]	Loss 0.0101 (0.0570)	
training:	Epoch: [29][42/408]	Loss 0.2899 (0.0625)	
training:	Epoch: [29][43/408]	Loss 0.0092 (0.0613)	
training:	Epoch: [29][44/408]	Loss 0.0091 (0.0601)	
training:	Epoch: [29][45/408]	Loss 0.2433 (0.0642)	
training:	Epoch: [29][46/408]	Loss 0.0101 (0.0630)	
training:	Epoch: [29][47/408]	Loss 0.0103 (0.0619)	
training:	Epoch: [29][48/408]	Loss 0.0096 (0.0608)	
training:	Epoch: [29][49/408]	Loss 0.0096 (0.0597)	
training:	Epoch: [29][50/408]	Loss 0.0105 (0.0588)	
training:	Epoch: [29][51/408]	Loss 0.0109 (0.0578)	
training:	Epoch: [29][52/408]	Loss 0.0095 (0.0569)	
training:	Epoch: [29][53/408]	Loss 0.0111 (0.0560)	
training:	Epoch: [29][54/408]	Loss 0.0094 (0.0552)	
training:	Epoch: [29][55/408]	Loss 0.0095 (0.0543)	
training:	Epoch: [29][56/408]	Loss 0.0111 (0.0536)	
training:	Epoch: [29][57/408]	Loss 0.0085 (0.0528)	
training:	Epoch: [29][58/408]	Loss 0.0106 (0.0520)	
training:	Epoch: [29][59/408]	Loss 0.0102 (0.0513)	
training:	Epoch: [29][60/408]	Loss 0.0095 (0.0506)	
training:	Epoch: [29][61/408]	Loss 0.0100 (0.0500)	
training:	Epoch: [29][62/408]	Loss 0.0098 (0.0493)	
training:	Epoch: [29][63/408]	Loss 0.0093 (0.0487)	
training:	Epoch: [29][64/408]	Loss 0.0100 (0.0481)	
training:	Epoch: [29][65/408]	Loss 0.0087 (0.0475)	
training:	Epoch: [29][66/408]	Loss 0.2938 (0.0512)	
training:	Epoch: [29][67/408]	Loss 0.0096 (0.0506)	
training:	Epoch: [29][68/408]	Loss 0.0095 (0.0500)	
training:	Epoch: [29][69/408]	Loss 0.0101 (0.0494)	
training:	Epoch: [29][70/408]	Loss 0.0102 (0.0488)	
training:	Epoch: [29][71/408]	Loss 0.0115 (0.0483)	
training:	Epoch: [29][72/408]	Loss 0.0102 (0.0478)	
training:	Epoch: [29][73/408]	Loss 0.0097 (0.0473)	
training:	Epoch: [29][74/408]	Loss 0.2897 (0.0505)	
training:	Epoch: [29][75/408]	Loss 0.0100 (0.0500)	
training:	Epoch: [29][76/408]	Loss 0.0088 (0.0495)	
training:	Epoch: [29][77/408]	Loss 0.0102 (0.0489)	
training:	Epoch: [29][78/408]	Loss 0.3174 (0.0524)	
training:	Epoch: [29][79/408]	Loss 0.0098 (0.0519)	
training:	Epoch: [29][80/408]	Loss 0.0096 (0.0513)	
training:	Epoch: [29][81/408]	Loss 0.0098 (0.0508)	
training:	Epoch: [29][82/408]	Loss 0.0098 (0.0503)	
training:	Epoch: [29][83/408]	Loss 0.3002 (0.0533)	
training:	Epoch: [29][84/408]	Loss 0.0095 (0.0528)	
training:	Epoch: [29][85/408]	Loss 0.0090 (0.0523)	
training:	Epoch: [29][86/408]	Loss 0.0110 (0.0518)	
training:	Epoch: [29][87/408]	Loss 0.0096 (0.0513)	
training:	Epoch: [29][88/408]	Loss 0.0096 (0.0508)	
training:	Epoch: [29][89/408]	Loss 0.0104 (0.0504)	
training:	Epoch: [29][90/408]	Loss 0.0121 (0.0500)	
training:	Epoch: [29][91/408]	Loss 0.0098 (0.0495)	
training:	Epoch: [29][92/408]	Loss 0.0096 (0.0491)	
training:	Epoch: [29][93/408]	Loss 0.0101 (0.0487)	
training:	Epoch: [29][94/408]	Loss 0.0091 (0.0483)	
training:	Epoch: [29][95/408]	Loss 0.0103 (0.0479)	
training:	Epoch: [29][96/408]	Loss 0.2913 (0.0504)	
training:	Epoch: [29][97/408]	Loss 0.0092 (0.0500)	
training:	Epoch: [29][98/408]	Loss 0.0090 (0.0495)	
training:	Epoch: [29][99/408]	Loss 0.3044 (0.0521)	
training:	Epoch: [29][100/408]	Loss 0.0089 (0.0517)	
training:	Epoch: [29][101/408]	Loss 0.0096 (0.0513)	
training:	Epoch: [29][102/408]	Loss 0.0105 (0.0509)	
training:	Epoch: [29][103/408]	Loss 0.0100 (0.0505)	
training:	Epoch: [29][104/408]	Loss 0.0082 (0.0501)	
training:	Epoch: [29][105/408]	Loss 0.0089 (0.0497)	
training:	Epoch: [29][106/408]	Loss 0.0095 (0.0493)	
training:	Epoch: [29][107/408]	Loss 0.0091 (0.0489)	
training:	Epoch: [29][108/408]	Loss 0.0105 (0.0486)	
training:	Epoch: [29][109/408]	Loss 0.2750 (0.0506)	
training:	Epoch: [29][110/408]	Loss 0.2941 (0.0529)	
training:	Epoch: [29][111/408]	Loss 0.0102 (0.0525)	
training:	Epoch: [29][112/408]	Loss 0.3094 (0.0548)	
training:	Epoch: [29][113/408]	Loss 0.0092 (0.0544)	
training:	Epoch: [29][114/408]	Loss 0.2811 (0.0564)	
training:	Epoch: [29][115/408]	Loss 0.0113 (0.0560)	
training:	Epoch: [29][116/408]	Loss 0.0105 (0.0556)	
training:	Epoch: [29][117/408]	Loss 0.0092 (0.0552)	
training:	Epoch: [29][118/408]	Loss 0.0094 (0.0548)	
training:	Epoch: [29][119/408]	Loss 0.0101 (0.0544)	
training:	Epoch: [29][120/408]	Loss 0.0105 (0.0540)	
training:	Epoch: [29][121/408]	Loss 0.0096 (0.0537)	
training:	Epoch: [29][122/408]	Loss 0.0091 (0.0533)	
training:	Epoch: [29][123/408]	Loss 0.0100 (0.0530)	
training:	Epoch: [29][124/408]	Loss 0.0091 (0.0526)	
training:	Epoch: [29][125/408]	Loss 0.2989 (0.0546)	
training:	Epoch: [29][126/408]	Loss 0.0089 (0.0542)	
training:	Epoch: [29][127/408]	Loss 0.0116 (0.0539)	
training:	Epoch: [29][128/408]	Loss 0.0097 (0.0535)	
training:	Epoch: [29][129/408]	Loss 0.0096 (0.0532)	
training:	Epoch: [29][130/408]	Loss 0.0094 (0.0529)	
training:	Epoch: [29][131/408]	Loss 0.0086 (0.0525)	
training:	Epoch: [29][132/408]	Loss 0.0100 (0.0522)	
training:	Epoch: [29][133/408]	Loss 0.0090 (0.0519)	
training:	Epoch: [29][134/408]	Loss 0.0093 (0.0516)	
training:	Epoch: [29][135/408]	Loss 0.0095 (0.0512)	
training:	Epoch: [29][136/408]	Loss 0.0092 (0.0509)	
training:	Epoch: [29][137/408]	Loss 0.2814 (0.0526)	
training:	Epoch: [29][138/408]	Loss 0.0106 (0.0523)	
training:	Epoch: [29][139/408]	Loss 0.0099 (0.0520)	
training:	Epoch: [29][140/408]	Loss 0.0096 (0.0517)	
training:	Epoch: [29][141/408]	Loss 0.3099 (0.0535)	
training:	Epoch: [29][142/408]	Loss 0.0101 (0.0532)	
training:	Epoch: [29][143/408]	Loss 0.0091 (0.0529)	
training:	Epoch: [29][144/408]	Loss 0.0097 (0.0526)	
training:	Epoch: [29][145/408]	Loss 0.0090 (0.0523)	
training:	Epoch: [29][146/408]	Loss 0.0095 (0.0520)	
training:	Epoch: [29][147/408]	Loss 0.0089 (0.0517)	
training:	Epoch: [29][148/408]	Loss 0.2950 (0.0534)	
training:	Epoch: [29][149/408]	Loss 0.0103 (0.0531)	
training:	Epoch: [29][150/408]	Loss 0.0088 (0.0528)	
training:	Epoch: [29][151/408]	Loss 0.0099 (0.0525)	
training:	Epoch: [29][152/408]	Loss 0.0101 (0.0522)	
training:	Epoch: [29][153/408]	Loss 0.0086 (0.0519)	
training:	Epoch: [29][154/408]	Loss 0.0097 (0.0517)	
training:	Epoch: [29][155/408]	Loss 0.0095 (0.0514)	
training:	Epoch: [29][156/408]	Loss 0.0096 (0.0511)	
training:	Epoch: [29][157/408]	Loss 0.0100 (0.0509)	
training:	Epoch: [29][158/408]	Loss 0.0104 (0.0506)	
training:	Epoch: [29][159/408]	Loss 0.0091 (0.0503)	
training:	Epoch: [29][160/408]	Loss 0.0102 (0.0501)	
training:	Epoch: [29][161/408]	Loss 0.0101 (0.0498)	
training:	Epoch: [29][162/408]	Loss 0.0088 (0.0496)	
training:	Epoch: [29][163/408]	Loss 0.0097 (0.0493)	
training:	Epoch: [29][164/408]	Loss 0.0091 (0.0491)	
training:	Epoch: [29][165/408]	Loss 0.0090 (0.0489)	
training:	Epoch: [29][166/408]	Loss 0.0091 (0.0486)	
training:	Epoch: [29][167/408]	Loss 0.0174 (0.0484)	
training:	Epoch: [29][168/408]	Loss 0.0094 (0.0482)	
training:	Epoch: [29][169/408]	Loss 0.0095 (0.0480)	
training:	Epoch: [29][170/408]	Loss 0.0094 (0.0477)	
training:	Epoch: [29][171/408]	Loss 0.0093 (0.0475)	
training:	Epoch: [29][172/408]	Loss 0.0100 (0.0473)	
training:	Epoch: [29][173/408]	Loss 0.3005 (0.0488)	
training:	Epoch: [29][174/408]	Loss 0.0090 (0.0485)	
training:	Epoch: [29][175/408]	Loss 0.0089 (0.0483)	
training:	Epoch: [29][176/408]	Loss 0.0096 (0.0481)	
training:	Epoch: [29][177/408]	Loss 0.0102 (0.0479)	
training:	Epoch: [29][178/408]	Loss 0.0091 (0.0477)	
training:	Epoch: [29][179/408]	Loss 0.0095 (0.0474)	
training:	Epoch: [29][180/408]	Loss 0.0101 (0.0472)	
training:	Epoch: [29][181/408]	Loss 0.0150 (0.0471)	
training:	Epoch: [29][182/408]	Loss 0.0094 (0.0469)	
training:	Epoch: [29][183/408]	Loss 0.0095 (0.0466)	
training:	Epoch: [29][184/408]	Loss 0.0101 (0.0465)	
training:	Epoch: [29][185/408]	Loss 0.0088 (0.0462)	
training:	Epoch: [29][186/408]	Loss 0.0082 (0.0460)	
training:	Epoch: [29][187/408]	Loss 0.0086 (0.0458)	
training:	Epoch: [29][188/408]	Loss 0.0095 (0.0456)	
training:	Epoch: [29][189/408]	Loss 0.0134 (0.0455)	
training:	Epoch: [29][190/408]	Loss 0.0083 (0.0453)	
training:	Epoch: [29][191/408]	Loss 0.5865 (0.0481)	
training:	Epoch: [29][192/408]	Loss 0.0095 (0.0479)	
training:	Epoch: [29][193/408]	Loss 0.2997 (0.0492)	
training:	Epoch: [29][194/408]	Loss 0.0090 (0.0490)	
training:	Epoch: [29][195/408]	Loss 0.0088 (0.0488)	
training:	Epoch: [29][196/408]	Loss 0.2481 (0.0498)	
training:	Epoch: [29][197/408]	Loss 0.0091 (0.0496)	
training:	Epoch: [29][198/408]	Loss 0.0098 (0.0494)	
training:	Epoch: [29][199/408]	Loss 0.0091 (0.0492)	
training:	Epoch: [29][200/408]	Loss 0.0093 (0.0490)	
training:	Epoch: [29][201/408]	Loss 0.0111 (0.0488)	
training:	Epoch: [29][202/408]	Loss 0.0095 (0.0486)	
training:	Epoch: [29][203/408]	Loss 0.0098 (0.0484)	
training:	Epoch: [29][204/408]	Loss 0.0093 (0.0482)	
training:	Epoch: [29][205/408]	Loss 0.0094 (0.0481)	
training:	Epoch: [29][206/408]	Loss 0.0091 (0.0479)	
training:	Epoch: [29][207/408]	Loss 0.0093 (0.0477)	
training:	Epoch: [29][208/408]	Loss 0.0158 (0.0475)	
training:	Epoch: [29][209/408]	Loss 0.0094 (0.0473)	
training:	Epoch: [29][210/408]	Loss 0.0098 (0.0472)	
training:	Epoch: [29][211/408]	Loss 0.0094 (0.0470)	
training:	Epoch: [29][212/408]	Loss 0.2944 (0.0482)	
training:	Epoch: [29][213/408]	Loss 0.0146 (0.0480)	
training:	Epoch: [29][214/408]	Loss 0.0087 (0.0478)	
training:	Epoch: [29][215/408]	Loss 0.0087 (0.0476)	
training:	Epoch: [29][216/408]	Loss 0.2804 (0.0487)	
training:	Epoch: [29][217/408]	Loss 0.0134 (0.0485)	
training:	Epoch: [29][218/408]	Loss 0.0100 (0.0484)	
training:	Epoch: [29][219/408]	Loss 0.0093 (0.0482)	
training:	Epoch: [29][220/408]	Loss 0.0107 (0.0480)	
training:	Epoch: [29][221/408]	Loss 0.0088 (0.0478)	
training:	Epoch: [29][222/408]	Loss 0.0095 (0.0477)	
training:	Epoch: [29][223/408]	Loss 0.0084 (0.0475)	
training:	Epoch: [29][224/408]	Loss 0.0092 (0.0473)	
training:	Epoch: [29][225/408]	Loss 0.0092 (0.0472)	
training:	Epoch: [29][226/408]	Loss 0.0082 (0.0470)	
training:	Epoch: [29][227/408]	Loss 0.0125 (0.0468)	
training:	Epoch: [29][228/408]	Loss 0.0091 (0.0467)	
training:	Epoch: [29][229/408]	Loss 0.2984 (0.0478)	
training:	Epoch: [29][230/408]	Loss 0.0108 (0.0476)	
training:	Epoch: [29][231/408]	Loss 0.2870 (0.0486)	
training:	Epoch: [29][232/408]	Loss 0.0091 (0.0485)	
training:	Epoch: [29][233/408]	Loss 0.0114 (0.0483)	
training:	Epoch: [29][234/408]	Loss 0.0095 (0.0481)	
training:	Epoch: [29][235/408]	Loss 0.0101 (0.0480)	
training:	Epoch: [29][236/408]	Loss 0.0090 (0.0478)	
training:	Epoch: [29][237/408]	Loss 0.0087 (0.0477)	
training:	Epoch: [29][238/408]	Loss 0.0089 (0.0475)	
training:	Epoch: [29][239/408]	Loss 0.0096 (0.0473)	
training:	Epoch: [29][240/408]	Loss 0.0097 (0.0472)	
training:	Epoch: [29][241/408]	Loss 0.2886 (0.0482)	
training:	Epoch: [29][242/408]	Loss 0.0093 (0.0480)	
training:	Epoch: [29][243/408]	Loss 0.2875 (0.0490)	
training:	Epoch: [29][244/408]	Loss 0.2898 (0.0500)	
training:	Epoch: [29][245/408]	Loss 0.0102 (0.0498)	
training:	Epoch: [29][246/408]	Loss 0.0095 (0.0497)	
training:	Epoch: [29][247/408]	Loss 0.0089 (0.0495)	
training:	Epoch: [29][248/408]	Loss 0.0092 (0.0493)	
training:	Epoch: [29][249/408]	Loss 0.0091 (0.0492)	
training:	Epoch: [29][250/408]	Loss 0.0095 (0.0490)	
training:	Epoch: [29][251/408]	Loss 0.0100 (0.0489)	
training:	Epoch: [29][252/408]	Loss 0.0092 (0.0487)	
training:	Epoch: [29][253/408]	Loss 0.0089 (0.0485)	
training:	Epoch: [29][254/408]	Loss 0.0105 (0.0484)	
training:	Epoch: [29][255/408]	Loss 0.0099 (0.0482)	
training:	Epoch: [29][256/408]	Loss 0.0098 (0.0481)	
training:	Epoch: [29][257/408]	Loss 0.0090 (0.0479)	
training:	Epoch: [29][258/408]	Loss 0.0093 (0.0478)	
training:	Epoch: [29][259/408]	Loss 0.0097 (0.0476)	
training:	Epoch: [29][260/408]	Loss 0.0088 (0.0475)	
training:	Epoch: [29][261/408]	Loss 0.0111 (0.0474)	
training:	Epoch: [29][262/408]	Loss 0.0094 (0.0472)	
training:	Epoch: [29][263/408]	Loss 0.0349 (0.0472)	
training:	Epoch: [29][264/408]	Loss 0.0115 (0.0470)	
training:	Epoch: [29][265/408]	Loss 0.0121 (0.0469)	
training:	Epoch: [29][266/408]	Loss 0.0097 (0.0468)	
training:	Epoch: [29][267/408]	Loss 0.0083 (0.0466)	
training:	Epoch: [29][268/408]	Loss 0.0094 (0.0465)	
training:	Epoch: [29][269/408]	Loss 0.0092 (0.0463)	
training:	Epoch: [29][270/408]	Loss 0.0090 (0.0462)	
training:	Epoch: [29][271/408]	Loss 0.0093 (0.0461)	
training:	Epoch: [29][272/408]	Loss 0.0093 (0.0459)	
training:	Epoch: [29][273/408]	Loss 0.0093 (0.0458)	
training:	Epoch: [29][274/408]	Loss 0.0093 (0.0457)	
training:	Epoch: [29][275/408]	Loss 0.0086 (0.0455)	
training:	Epoch: [29][276/408]	Loss 0.0095 (0.0454)	
training:	Epoch: [29][277/408]	Loss 0.0083 (0.0453)	
training:	Epoch: [29][278/408]	Loss 0.0093 (0.0451)	
training:	Epoch: [29][279/408]	Loss 0.0100 (0.0450)	
training:	Epoch: [29][280/408]	Loss 0.0097 (0.0449)	
training:	Epoch: [29][281/408]	Loss 0.0097 (0.0448)	
training:	Epoch: [29][282/408]	Loss 0.0096 (0.0446)	
training:	Epoch: [29][283/408]	Loss 0.2851 (0.0455)	
training:	Epoch: [29][284/408]	Loss 0.0100 (0.0454)	
training:	Epoch: [29][285/408]	Loss 0.5831 (0.0472)	
training:	Epoch: [29][286/408]	Loss 0.0105 (0.0471)	
training:	Epoch: [29][287/408]	Loss 0.0089 (0.0470)	
training:	Epoch: [29][288/408]	Loss 0.0087 (0.0468)	
training:	Epoch: [29][289/408]	Loss 0.0081 (0.0467)	
training:	Epoch: [29][290/408]	Loss 0.0099 (0.0466)	
training:	Epoch: [29][291/408]	Loss 0.0092 (0.0465)	
training:	Epoch: [29][292/408]	Loss 0.0091 (0.0463)	
training:	Epoch: [29][293/408]	Loss 0.0102 (0.0462)	
training:	Epoch: [29][294/408]	Loss 0.0091 (0.0461)	
training:	Epoch: [29][295/408]	Loss 0.0085 (0.0459)	
training:	Epoch: [29][296/408]	Loss 0.0105 (0.0458)	
training:	Epoch: [29][297/408]	Loss 0.0087 (0.0457)	
training:	Epoch: [29][298/408]	Loss 0.0090 (0.0456)	
training:	Epoch: [29][299/408]	Loss 0.0086 (0.0455)	
training:	Epoch: [29][300/408]	Loss 0.0098 (0.0453)	
training:	Epoch: [29][301/408]	Loss 0.0097 (0.0452)	
training:	Epoch: [29][302/408]	Loss 0.1180 (0.0455)	
training:	Epoch: [29][303/408]	Loss 0.0096 (0.0453)	
training:	Epoch: [29][304/408]	Loss 0.0095 (0.0452)	
training:	Epoch: [29][305/408]	Loss 0.0085 (0.0451)	
training:	Epoch: [29][306/408]	Loss 0.0094 (0.0450)	
training:	Epoch: [29][307/408]	Loss 0.6089 (0.0468)	
training:	Epoch: [29][308/408]	Loss 0.0093 (0.0467)	
training:	Epoch: [29][309/408]	Loss 0.0079 (0.0466)	
training:	Epoch: [29][310/408]	Loss 0.0087 (0.0465)	
training:	Epoch: [29][311/408]	Loss 0.0173 (0.0464)	
training:	Epoch: [29][312/408]	Loss 0.0174 (0.0463)	
training:	Epoch: [29][313/408]	Loss 0.2902 (0.0470)	
training:	Epoch: [29][314/408]	Loss 0.0103 (0.0469)	
training:	Epoch: [29][315/408]	Loss 0.0105 (0.0468)	
training:	Epoch: [29][316/408]	Loss 0.0089 (0.0467)	
training:	Epoch: [29][317/408]	Loss 0.0101 (0.0466)	
training:	Epoch: [29][318/408]	Loss 0.0100 (0.0465)	
training:	Epoch: [29][319/408]	Loss 0.0097 (0.0463)	
training:	Epoch: [29][320/408]	Loss 0.0092 (0.0462)	
training:	Epoch: [29][321/408]	Loss 0.0096 (0.0461)	
training:	Epoch: [29][322/408]	Loss 0.0104 (0.0460)	
training:	Epoch: [29][323/408]	Loss 0.0112 (0.0459)	
training:	Epoch: [29][324/408]	Loss 0.0103 (0.0458)	
training:	Epoch: [29][325/408]	Loss 0.0092 (0.0457)	
training:	Epoch: [29][326/408]	Loss 0.0083 (0.0456)	
training:	Epoch: [29][327/408]	Loss 0.0094 (0.0455)	
training:	Epoch: [29][328/408]	Loss 0.0084 (0.0453)	
training:	Epoch: [29][329/408]	Loss 0.0091 (0.0452)	
training:	Epoch: [29][330/408]	Loss 0.0100 (0.0451)	
training:	Epoch: [29][331/408]	Loss 0.0094 (0.0450)	
training:	Epoch: [29][332/408]	Loss 0.0089 (0.0449)	
training:	Epoch: [29][333/408]	Loss 0.0089 (0.0448)	
training:	Epoch: [29][334/408]	Loss 0.0118 (0.0447)	
training:	Epoch: [29][335/408]	Loss 0.0095 (0.0446)	
training:	Epoch: [29][336/408]	Loss 0.0084 (0.0445)	
training:	Epoch: [29][337/408]	Loss 0.0084 (0.0444)	
training:	Epoch: [29][338/408]	Loss 0.0090 (0.0443)	
training:	Epoch: [29][339/408]	Loss 0.2994 (0.0450)	
training:	Epoch: [29][340/408]	Loss 0.0097 (0.0449)	
training:	Epoch: [29][341/408]	Loss 0.0096 (0.0448)	
training:	Epoch: [29][342/408]	Loss 0.0095 (0.0447)	
training:	Epoch: [29][343/408]	Loss 0.0093 (0.0446)	
training:	Epoch: [29][344/408]	Loss 0.0090 (0.0445)	
training:	Epoch: [29][345/408]	Loss 0.0089 (0.0444)	
training:	Epoch: [29][346/408]	Loss 0.0096 (0.0443)	
training:	Epoch: [29][347/408]	Loss 0.0111 (0.0442)	
training:	Epoch: [29][348/408]	Loss 0.0089 (0.0441)	
training:	Epoch: [29][349/408]	Loss 0.0090 (0.0440)	
training:	Epoch: [29][350/408]	Loss 0.2860 (0.0447)	
training:	Epoch: [29][351/408]	Loss 0.0098 (0.0446)	
training:	Epoch: [29][352/408]	Loss 0.0092 (0.0445)	
training:	Epoch: [29][353/408]	Loss 0.0091 (0.0444)	
training:	Epoch: [29][354/408]	Loss 0.0089 (0.0443)	
training:	Epoch: [29][355/408]	Loss 0.0092 (0.0442)	
training:	Epoch: [29][356/408]	Loss 0.0085 (0.0441)	
training:	Epoch: [29][357/408]	Loss 0.0100 (0.0440)	
training:	Epoch: [29][358/408]	Loss 0.0093 (0.0439)	
training:	Epoch: [29][359/408]	Loss 0.0085 (0.0438)	
training:	Epoch: [29][360/408]	Loss 0.0096 (0.0437)	
training:	Epoch: [29][361/408]	Loss 0.0090 (0.0436)	
training:	Epoch: [29][362/408]	Loss 0.0092 (0.0435)	
training:	Epoch: [29][363/408]	Loss 0.0088 (0.0434)	
training:	Epoch: [29][364/408]	Loss 0.0107 (0.0433)	
training:	Epoch: [29][365/408]	Loss 0.0086 (0.0432)	
training:	Epoch: [29][366/408]	Loss 0.0089 (0.0431)	
training:	Epoch: [29][367/408]	Loss 0.0093 (0.0431)	
training:	Epoch: [29][368/408]	Loss 0.2975 (0.0437)	
training:	Epoch: [29][369/408]	Loss 0.0078 (0.0436)	
training:	Epoch: [29][370/408]	Loss 0.0081 (0.0436)	
training:	Epoch: [29][371/408]	Loss 0.0090 (0.0435)	
training:	Epoch: [29][372/408]	Loss 0.0090 (0.0434)	
training:	Epoch: [29][373/408]	Loss 0.0092 (0.0433)	
training:	Epoch: [29][374/408]	Loss 0.0088 (0.0432)	
training:	Epoch: [29][375/408]	Loss 0.0086 (0.0431)	
training:	Epoch: [29][376/408]	Loss 0.2795 (0.0437)	
training:	Epoch: [29][377/408]	Loss 0.0086 (0.0436)	
training:	Epoch: [29][378/408]	Loss 0.0103 (0.0435)	
training:	Epoch: [29][379/408]	Loss 0.0081 (0.0434)	
training:	Epoch: [29][380/408]	Loss 0.0094 (0.0434)	
training:	Epoch: [29][381/408]	Loss 0.0094 (0.0433)	
training:	Epoch: [29][382/408]	Loss 0.0083 (0.0432)	
training:	Epoch: [29][383/408]	Loss 0.0099 (0.0431)	
training:	Epoch: [29][384/408]	Loss 0.0091 (0.0430)	
training:	Epoch: [29][385/408]	Loss 0.2979 (0.0437)	
training:	Epoch: [29][386/408]	Loss 0.0077 (0.0436)	
training:	Epoch: [29][387/408]	Loss 0.0082 (0.0435)	
training:	Epoch: [29][388/408]	Loss 0.0082 (0.0434)	
training:	Epoch: [29][389/408]	Loss 0.0094 (0.0433)	
training:	Epoch: [29][390/408]	Loss 0.0095 (0.0432)	
training:	Epoch: [29][391/408]	Loss 0.0085 (0.0431)	
training:	Epoch: [29][392/408]	Loss 0.0111 (0.0430)	
training:	Epoch: [29][393/408]	Loss 0.0083 (0.0430)	
training:	Epoch: [29][394/408]	Loss 0.0095 (0.0429)	
training:	Epoch: [29][395/408]	Loss 0.0083 (0.0428)	
training:	Epoch: [29][396/408]	Loss 0.0081 (0.0427)	
training:	Epoch: [29][397/408]	Loss 0.0086 (0.0426)	
training:	Epoch: [29][398/408]	Loss 0.0090 (0.0425)	
training:	Epoch: [29][399/408]	Loss 0.0091 (0.0424)	
training:	Epoch: [29][400/408]	Loss 0.0092 (0.0424)	
training:	Epoch: [29][401/408]	Loss 0.0084 (0.0423)	
training:	Epoch: [29][402/408]	Loss 0.0100 (0.0422)	
training:	Epoch: [29][403/408]	Loss 0.7446 (0.0439)	
training:	Epoch: [29][404/408]	Loss 0.0092 (0.0438)	
training:	Epoch: [29][405/408]	Loss 0.2906 (0.0445)	
training:	Epoch: [29][406/408]	Loss 0.0090 (0.0444)	
training:	Epoch: [29][407/408]	Loss 0.0130 (0.0443)	
training:	Epoch: [29][408/408]	Loss 0.0092 (0.0442)	
Training:	 Loss: 0.0441

Training:	 ACC: 0.9925 0.9925 0.9924 0.9927
Validation:	 ACC: 0.7862 0.7881 0.8280 0.7444
Validation:	 Best_BACC: 0.8226 0.8250 0.8772 0.7679
Validation:	 Loss: 0.9002
Pretraining:	Epoch 30/200
----------
training:	Epoch: [30][1/408]	Loss 0.0096 (0.0096)	
training:	Epoch: [30][2/408]	Loss 0.0076 (0.0086)	
training:	Epoch: [30][3/408]	Loss 0.0115 (0.0096)	
training:	Epoch: [30][4/408]	Loss 0.0083 (0.0093)	
training:	Epoch: [30][5/408]	Loss 0.2768 (0.0628)	
training:	Epoch: [30][6/408]	Loss 0.0088 (0.0538)	
training:	Epoch: [30][7/408]	Loss 0.0095 (0.0475)	
training:	Epoch: [30][8/408]	Loss 0.0088 (0.0426)	
training:	Epoch: [30][9/408]	Loss 0.0090 (0.0389)	
training:	Epoch: [30][10/408]	Loss 0.0095 (0.0360)	
training:	Epoch: [30][11/408]	Loss 0.0093 (0.0335)	
training:	Epoch: [30][12/408]	Loss 0.0098 (0.0316)	
training:	Epoch: [30][13/408]	Loss 0.0096 (0.0299)	
training:	Epoch: [30][14/408]	Loss 0.0109 (0.0285)	
training:	Epoch: [30][15/408]	Loss 0.0089 (0.0272)	
training:	Epoch: [30][16/408]	Loss 0.0080 (0.0260)	
training:	Epoch: [30][17/408]	Loss 0.0099 (0.0251)	
training:	Epoch: [30][18/408]	Loss 0.0089 (0.0242)	
training:	Epoch: [30][19/408]	Loss 0.0090 (0.0234)	
training:	Epoch: [30][20/408]	Loss 0.0102 (0.0227)	
training:	Epoch: [30][21/408]	Loss 0.0136 (0.0223)	
training:	Epoch: [30][22/408]	Loss 0.0117 (0.0218)	
training:	Epoch: [30][23/408]	Loss 0.3019 (0.0340)	
training:	Epoch: [30][24/408]	Loss 0.0112 (0.0330)	
training:	Epoch: [30][25/408]	Loss 0.0075 (0.0320)	
training:	Epoch: [30][26/408]	Loss 0.0089 (0.0311)	
training:	Epoch: [30][27/408]	Loss 0.0091 (0.0303)	
training:	Epoch: [30][28/408]	Loss 0.0109 (0.0296)	
training:	Epoch: [30][29/408]	Loss 0.0093 (0.0289)	
training:	Epoch: [30][30/408]	Loss 0.0084 (0.0282)	
training:	Epoch: [30][31/408]	Loss 0.0082 (0.0276)	
training:	Epoch: [30][32/408]	Loss 0.2889 (0.0357)	
training:	Epoch: [30][33/408]	Loss 0.0091 (0.0349)	
training:	Epoch: [30][34/408]	Loss 0.0097 (0.0342)	
training:	Epoch: [30][35/408]	Loss 0.3005 (0.0418)	
training:	Epoch: [30][36/408]	Loss 0.0092 (0.0409)	
training:	Epoch: [30][37/408]	Loss 0.0099 (0.0401)	
training:	Epoch: [30][38/408]	Loss 0.0088 (0.0392)	
training:	Epoch: [30][39/408]	Loss 0.0088 (0.0385)	
training:	Epoch: [30][40/408]	Loss 0.0084 (0.0377)	
training:	Epoch: [30][41/408]	Loss 0.0081 (0.0370)	
training:	Epoch: [30][42/408]	Loss 0.0084 (0.0363)	
training:	Epoch: [30][43/408]	Loss 0.0084 (0.0357)	
training:	Epoch: [30][44/408]	Loss 0.0089 (0.0350)	
training:	Epoch: [30][45/408]	Loss 0.2956 (0.0408)	
training:	Epoch: [30][46/408]	Loss 0.0089 (0.0401)	
training:	Epoch: [30][47/408]	Loss 0.0353 (0.0400)	
training:	Epoch: [30][48/408]	Loss 0.0085 (0.0394)	
training:	Epoch: [30][49/408]	Loss 0.0101 (0.0388)	
training:	Epoch: [30][50/408]	Loss 0.2974 (0.0439)	
training:	Epoch: [30][51/408]	Loss 0.0092 (0.0433)	
training:	Epoch: [30][52/408]	Loss 0.0085 (0.0426)	
training:	Epoch: [30][53/408]	Loss 0.3180 (0.0478)	
training:	Epoch: [30][54/408]	Loss 0.0096 (0.0471)	
training:	Epoch: [30][55/408]	Loss 0.0096 (0.0464)	
training:	Epoch: [30][56/408]	Loss 0.0109 (0.0458)	
training:	Epoch: [30][57/408]	Loss 0.0086 (0.0451)	
training:	Epoch: [30][58/408]	Loss 0.0100 (0.0445)	
training:	Epoch: [30][59/408]	Loss 0.0093 (0.0439)	
training:	Epoch: [30][60/408]	Loss 0.0088 (0.0433)	
training:	Epoch: [30][61/408]	Loss 0.0092 (0.0428)	
training:	Epoch: [30][62/408]	Loss 0.0427 (0.0428)	
training:	Epoch: [30][63/408]	Loss 0.0091 (0.0422)	
training:	Epoch: [30][64/408]	Loss 0.0092 (0.0417)	
training:	Epoch: [30][65/408]	Loss 0.0095 (0.0412)	
training:	Epoch: [30][66/408]	Loss 0.0086 (0.0407)	
training:	Epoch: [30][67/408]	Loss 0.2840 (0.0444)	
training:	Epoch: [30][68/408]	Loss 0.0094 (0.0438)	
training:	Epoch: [30][69/408]	Loss 0.0093 (0.0433)	
training:	Epoch: [30][70/408]	Loss 0.0103 (0.0429)	
training:	Epoch: [30][71/408]	Loss 0.3000 (0.0465)	
training:	Epoch: [30][72/408]	Loss 0.0094 (0.0460)	
training:	Epoch: [30][73/408]	Loss 0.0095 (0.0455)	
training:	Epoch: [30][74/408]	Loss 0.0083 (0.0450)	
training:	Epoch: [30][75/408]	Loss 0.0086 (0.0445)	
training:	Epoch: [30][76/408]	Loss 0.0096 (0.0440)	
training:	Epoch: [30][77/408]	Loss 0.0085 (0.0436)	
training:	Epoch: [30][78/408]	Loss 0.0080 (0.0431)	
training:	Epoch: [30][79/408]	Loss 0.2877 (0.0462)	
training:	Epoch: [30][80/408]	Loss 0.0093 (0.0457)	
training:	Epoch: [30][81/408]	Loss 0.0085 (0.0453)	
training:	Epoch: [30][82/408]	Loss 0.3020 (0.0484)	
training:	Epoch: [30][83/408]	Loss 0.0087 (0.0479)	
training:	Epoch: [30][84/408]	Loss 0.0099 (0.0475)	
training:	Epoch: [30][85/408]	Loss 0.0088 (0.0470)	
training:	Epoch: [30][86/408]	Loss 0.0274 (0.0468)	
training:	Epoch: [30][87/408]	Loss 0.0085 (0.0464)	
training:	Epoch: [30][88/408]	Loss 0.0091 (0.0459)	
training:	Epoch: [30][89/408]	Loss 0.0082 (0.0455)	
training:	Epoch: [30][90/408]	Loss 0.0095 (0.0451)	
training:	Epoch: [30][91/408]	Loss 0.0093 (0.0447)	
training:	Epoch: [30][92/408]	Loss 0.0085 (0.0443)	
training:	Epoch: [30][93/408]	Loss 0.0122 (0.0440)	
training:	Epoch: [30][94/408]	Loss 0.3118 (0.0468)	
training:	Epoch: [30][95/408]	Loss 0.0091 (0.0464)	
training:	Epoch: [30][96/408]	Loss 0.0087 (0.0460)	
training:	Epoch: [30][97/408]	Loss 0.0090 (0.0457)	
training:	Epoch: [30][98/408]	Loss 0.3009 (0.0483)	
training:	Epoch: [30][99/408]	Loss 0.0100 (0.0479)	
training:	Epoch: [30][100/408]	Loss 0.0119 (0.0475)	
training:	Epoch: [30][101/408]	Loss 0.0091 (0.0471)	
training:	Epoch: [30][102/408]	Loss 0.0097 (0.0468)	
training:	Epoch: [30][103/408]	Loss 0.0096 (0.0464)	
training:	Epoch: [30][104/408]	Loss 0.0093 (0.0461)	
training:	Epoch: [30][105/408]	Loss 0.2657 (0.0481)	
training:	Epoch: [30][106/408]	Loss 0.0093 (0.0478)	
training:	Epoch: [30][107/408]	Loss 0.2927 (0.0501)	
training:	Epoch: [30][108/408]	Loss 0.2916 (0.0523)	
training:	Epoch: [30][109/408]	Loss 0.0089 (0.0519)	
training:	Epoch: [30][110/408]	Loss 0.5959 (0.0569)	
training:	Epoch: [30][111/408]	Loss 0.0099 (0.0564)	
training:	Epoch: [30][112/408]	Loss 0.0084 (0.0560)	
training:	Epoch: [30][113/408]	Loss 0.2760 (0.0579)	
training:	Epoch: [30][114/408]	Loss 0.0103 (0.0575)	
training:	Epoch: [30][115/408]	Loss 0.0092 (0.0571)	
training:	Epoch: [30][116/408]	Loss 0.0113 (0.0567)	
training:	Epoch: [30][117/408]	Loss 0.0107 (0.0563)	
training:	Epoch: [30][118/408]	Loss 0.0091 (0.0559)	
training:	Epoch: [30][119/408]	Loss 0.0110 (0.0555)	
training:	Epoch: [30][120/408]	Loss 0.0093 (0.0552)	
training:	Epoch: [30][121/408]	Loss 0.0103 (0.0548)	
training:	Epoch: [30][122/408]	Loss 0.0096 (0.0544)	
training:	Epoch: [30][123/408]	Loss 0.0088 (0.0540)	
training:	Epoch: [30][124/408]	Loss 0.0103 (0.0537)	
training:	Epoch: [30][125/408]	Loss 0.0111 (0.0534)	
training:	Epoch: [30][126/408]	Loss 0.2885 (0.0552)	
training:	Epoch: [30][127/408]	Loss 0.2944 (0.0571)	
training:	Epoch: [30][128/408]	Loss 0.0108 (0.0567)	
training:	Epoch: [30][129/408]	Loss 0.0103 (0.0564)	
training:	Epoch: [30][130/408]	Loss 0.2964 (0.0582)	
training:	Epoch: [30][131/408]	Loss 0.2915 (0.0600)	
training:	Epoch: [30][132/408]	Loss 0.2925 (0.0618)	
training:	Epoch: [30][133/408]	Loss 0.0099 (0.0614)	
training:	Epoch: [30][134/408]	Loss 0.0098 (0.0610)	
training:	Epoch: [30][135/408]	Loss 0.0090 (0.0606)	
training:	Epoch: [30][136/408]	Loss 0.0107 (0.0602)	
training:	Epoch: [30][137/408]	Loss 0.0099 (0.0599)	
training:	Epoch: [30][138/408]	Loss 0.0131 (0.0595)	
training:	Epoch: [30][139/408]	Loss 0.0096 (0.0592)	
training:	Epoch: [30][140/408]	Loss 0.0105 (0.0588)	
training:	Epoch: [30][141/408]	Loss 0.0107 (0.0585)	
training:	Epoch: [30][142/408]	Loss 0.0100 (0.0581)	
training:	Epoch: [30][143/408]	Loss 0.0095 (0.0578)	
training:	Epoch: [30][144/408]	Loss 0.0096 (0.0575)	
training:	Epoch: [30][145/408]	Loss 0.0092 (0.0571)	
training:	Epoch: [30][146/408]	Loss 0.0125 (0.0568)	
training:	Epoch: [30][147/408]	Loss 0.0108 (0.0565)	
training:	Epoch: [30][148/408]	Loss 0.0096 (0.0562)	
training:	Epoch: [30][149/408]	Loss 0.0100 (0.0559)	
training:	Epoch: [30][150/408]	Loss 0.0097 (0.0556)	
training:	Epoch: [30][151/408]	Loss 0.0115 (0.0553)	
training:	Epoch: [30][152/408]	Loss 0.0091 (0.0550)	
training:	Epoch: [30][153/408]	Loss 0.0107 (0.0547)	
training:	Epoch: [30][154/408]	Loss 0.0094 (0.0544)	
training:	Epoch: [30][155/408]	Loss 0.0096 (0.0541)	
training:	Epoch: [30][156/408]	Loss 0.0105 (0.0538)	
training:	Epoch: [30][157/408]	Loss 0.0108 (0.0536)	
training:	Epoch: [30][158/408]	Loss 0.0110 (0.0533)	
training:	Epoch: [30][159/408]	Loss 0.0087 (0.0530)	
training:	Epoch: [30][160/408]	Loss 0.0092 (0.0527)	
training:	Epoch: [30][161/408]	Loss 0.0101 (0.0525)	
training:	Epoch: [30][162/408]	Loss 0.3063 (0.0540)	
training:	Epoch: [30][163/408]	Loss 0.0102 (0.0538)	
training:	Epoch: [30][164/408]	Loss 0.0093 (0.0535)	
training:	Epoch: [30][165/408]	Loss 0.0092 (0.0532)	
training:	Epoch: [30][166/408]	Loss 0.0100 (0.0530)	
training:	Epoch: [30][167/408]	Loss 0.0105 (0.0527)	
training:	Epoch: [30][168/408]	Loss 0.0103 (0.0525)	
training:	Epoch: [30][169/408]	Loss 0.0094 (0.0522)	
training:	Epoch: [30][170/408]	Loss 0.0099 (0.0520)	
training:	Epoch: [30][171/408]	Loss 0.0085 (0.0517)	
training:	Epoch: [30][172/408]	Loss 0.0106 (0.0515)	
training:	Epoch: [30][173/408]	Loss 0.0113 (0.0512)	
training:	Epoch: [30][174/408]	Loss 0.0097 (0.0510)	
training:	Epoch: [30][175/408]	Loss 0.0092 (0.0508)	
training:	Epoch: [30][176/408]	Loss 0.0102 (0.0505)	
training:	Epoch: [30][177/408]	Loss 0.0087 (0.0503)	
training:	Epoch: [30][178/408]	Loss 0.0097 (0.0501)	
training:	Epoch: [30][179/408]	Loss 0.0086 (0.0498)	
training:	Epoch: [30][180/408]	Loss 0.0089 (0.0496)	
training:	Epoch: [30][181/408]	Loss 0.0096 (0.0494)	
training:	Epoch: [30][182/408]	Loss 0.0088 (0.0492)	
training:	Epoch: [30][183/408]	Loss 0.0093 (0.0489)	
training:	Epoch: [30][184/408]	Loss 0.0082 (0.0487)	
training:	Epoch: [30][185/408]	Loss 0.0091 (0.0485)	
training:	Epoch: [30][186/408]	Loss 0.0104 (0.0483)	
training:	Epoch: [30][187/408]	Loss 0.0081 (0.0481)	
training:	Epoch: [30][188/408]	Loss 0.0088 (0.0479)	
training:	Epoch: [30][189/408]	Loss 0.0091 (0.0477)	
training:	Epoch: [30][190/408]	Loss 0.0089 (0.0475)	
training:	Epoch: [30][191/408]	Loss 0.2772 (0.0487)	
training:	Epoch: [30][192/408]	Loss 0.0098 (0.0485)	
training:	Epoch: [30][193/408]	Loss 0.0095 (0.0483)	
training:	Epoch: [30][194/408]	Loss 0.3119 (0.0496)	
training:	Epoch: [30][195/408]	Loss 0.0089 (0.0494)	
training:	Epoch: [30][196/408]	Loss 0.0092 (0.0492)	
training:	Epoch: [30][197/408]	Loss 0.0090 (0.0490)	
training:	Epoch: [30][198/408]	Loss 0.0107 (0.0488)	
training:	Epoch: [30][199/408]	Loss 0.0087 (0.0486)	
training:	Epoch: [30][200/408]	Loss 0.0097 (0.0484)	
training:	Epoch: [30][201/408]	Loss 0.0097 (0.0482)	
training:	Epoch: [30][202/408]	Loss 0.0093 (0.0480)	
training:	Epoch: [30][203/408]	Loss 0.2924 (0.0492)	
training:	Epoch: [30][204/408]	Loss 0.0095 (0.0490)	
training:	Epoch: [30][205/408]	Loss 0.0094 (0.0488)	
training:	Epoch: [30][206/408]	Loss 0.0103 (0.0487)	
training:	Epoch: [30][207/408]	Loss 0.0091 (0.0485)	
training:	Epoch: [30][208/408]	Loss 0.0080 (0.0483)	
training:	Epoch: [30][209/408]	Loss 0.0091 (0.0481)	
training:	Epoch: [30][210/408]	Loss 0.0090 (0.0479)	
training:	Epoch: [30][211/408]	Loss 0.0098 (0.0477)	
training:	Epoch: [30][212/408]	Loss 0.0086 (0.0475)	
training:	Epoch: [30][213/408]	Loss 0.0094 (0.0474)	
training:	Epoch: [30][214/408]	Loss 0.0108 (0.0472)	
training:	Epoch: [30][215/408]	Loss 0.0094 (0.0470)	
training:	Epoch: [30][216/408]	Loss 0.0092 (0.0468)	
training:	Epoch: [30][217/408]	Loss 0.0090 (0.0467)	
training:	Epoch: [30][218/408]	Loss 0.0092 (0.0465)	
training:	Epoch: [30][219/408]	Loss 0.0088 (0.0463)	
training:	Epoch: [30][220/408]	Loss 0.0098 (0.0462)	
training:	Epoch: [30][221/408]	Loss 0.0087 (0.0460)	
training:	Epoch: [30][222/408]	Loss 0.0096 (0.0458)	
training:	Epoch: [30][223/408]	Loss 0.0088 (0.0457)	
training:	Epoch: [30][224/408]	Loss 0.0085 (0.0455)	
training:	Epoch: [30][225/408]	Loss 0.0084 (0.0453)	
training:	Epoch: [30][226/408]	Loss 0.2884 (0.0464)	
training:	Epoch: [30][227/408]	Loss 0.0089 (0.0462)	
training:	Epoch: [30][228/408]	Loss 0.0087 (0.0461)	
training:	Epoch: [30][229/408]	Loss 0.0084 (0.0459)	
training:	Epoch: [30][230/408]	Loss 0.0088 (0.0457)	
training:	Epoch: [30][231/408]	Loss 0.0101 (0.0456)	
training:	Epoch: [30][232/408]	Loss 0.2786 (0.0466)	
training:	Epoch: [30][233/408]	Loss 0.0092 (0.0464)	
training:	Epoch: [30][234/408]	Loss 0.0089 (0.0463)	
training:	Epoch: [30][235/408]	Loss 0.0085 (0.0461)	
training:	Epoch: [30][236/408]	Loss 0.0082 (0.0459)	
training:	Epoch: [30][237/408]	Loss 0.0092 (0.0458)	
training:	Epoch: [30][238/408]	Loss 0.0101 (0.0456)	
training:	Epoch: [30][239/408]	Loss 0.0091 (0.0455)	
training:	Epoch: [30][240/408]	Loss 0.0082 (0.0453)	
training:	Epoch: [30][241/408]	Loss 0.0096 (0.0452)	
training:	Epoch: [30][242/408]	Loss 0.0087 (0.0450)	
training:	Epoch: [30][243/408]	Loss 0.0084 (0.0449)	
training:	Epoch: [30][244/408]	Loss 0.0101 (0.0447)	
training:	Epoch: [30][245/408]	Loss 0.0091 (0.0446)	
training:	Epoch: [30][246/408]	Loss 0.0093 (0.0445)	
training:	Epoch: [30][247/408]	Loss 0.0085 (0.0443)	
training:	Epoch: [30][248/408]	Loss 0.0089 (0.0442)	
training:	Epoch: [30][249/408]	Loss 0.0080 (0.0440)	
training:	Epoch: [30][250/408]	Loss 0.0100 (0.0439)	
training:	Epoch: [30][251/408]	Loss 0.0082 (0.0437)	
training:	Epoch: [30][252/408]	Loss 0.0076 (0.0436)	
training:	Epoch: [30][253/408]	Loss 0.0083 (0.0435)	
training:	Epoch: [30][254/408]	Loss 0.0082 (0.0433)	
training:	Epoch: [30][255/408]	Loss 0.0085 (0.0432)	
training:	Epoch: [30][256/408]	Loss 0.0078 (0.0430)	
training:	Epoch: [30][257/408]	Loss 0.0085 (0.0429)	
training:	Epoch: [30][258/408]	Loss 0.0088 (0.0428)	
training:	Epoch: [30][259/408]	Loss 0.0083 (0.0426)	
training:	Epoch: [30][260/408]	Loss 0.0086 (0.0425)	
training:	Epoch: [30][261/408]	Loss 0.0082 (0.0424)	
training:	Epoch: [30][262/408]	Loss 0.0087 (0.0423)	
training:	Epoch: [30][263/408]	Loss 0.0086 (0.0421)	
training:	Epoch: [30][264/408]	Loss 0.0083 (0.0420)	
training:	Epoch: [30][265/408]	Loss 0.0090 (0.0419)	
training:	Epoch: [30][266/408]	Loss 0.0092 (0.0418)	
training:	Epoch: [30][267/408]	Loss 0.0088 (0.0416)	
training:	Epoch: [30][268/408]	Loss 0.0088 (0.0415)	
training:	Epoch: [30][269/408]	Loss 0.0081 (0.0414)	
training:	Epoch: [30][270/408]	Loss 0.0084 (0.0413)	
training:	Epoch: [30][271/408]	Loss 0.0118 (0.0412)	
training:	Epoch: [30][272/408]	Loss 0.0086 (0.0410)	
training:	Epoch: [30][273/408]	Loss 0.0093 (0.0409)	
training:	Epoch: [30][274/408]	Loss 0.0084 (0.0408)	
training:	Epoch: [30][275/408]	Loss 0.0083 (0.0407)	
training:	Epoch: [30][276/408]	Loss 0.0076 (0.0406)	
training:	Epoch: [30][277/408]	Loss 0.0084 (0.0404)	
training:	Epoch: [30][278/408]	Loss 0.0265 (0.0404)	
training:	Epoch: [30][279/408]	Loss 0.2851 (0.0413)	
training:	Epoch: [30][280/408]	Loss 0.0088 (0.0412)	
training:	Epoch: [30][281/408]	Loss 0.0083 (0.0410)	
training:	Epoch: [30][282/408]	Loss 0.0365 (0.0410)	
training:	Epoch: [30][283/408]	Loss 0.0088 (0.0409)	
training:	Epoch: [30][284/408]	Loss 0.0086 (0.0408)	
training:	Epoch: [30][285/408]	Loss 0.0080 (0.0407)	
training:	Epoch: [30][286/408]	Loss 0.0092 (0.0406)	
training:	Epoch: [30][287/408]	Loss 0.3108 (0.0415)	
training:	Epoch: [30][288/408]	Loss 0.2705 (0.0423)	
training:	Epoch: [30][289/408]	Loss 0.0079 (0.0422)	
training:	Epoch: [30][290/408]	Loss 0.0083 (0.0421)	
training:	Epoch: [30][291/408]	Loss 0.0089 (0.0420)	
training:	Epoch: [30][292/408]	Loss 0.0081 (0.0418)	
training:	Epoch: [30][293/408]	Loss 0.3020 (0.0427)	
training:	Epoch: [30][294/408]	Loss 0.0082 (0.0426)	
training:	Epoch: [30][295/408]	Loss 0.0077 (0.0425)	
training:	Epoch: [30][296/408]	Loss 0.0082 (0.0424)	
training:	Epoch: [30][297/408]	Loss 0.0088 (0.0423)	
training:	Epoch: [30][298/408]	Loss 0.0085 (0.0422)	
training:	Epoch: [30][299/408]	Loss 0.0084 (0.0420)	
training:	Epoch: [30][300/408]	Loss 0.0087 (0.0419)	
training:	Epoch: [30][301/408]	Loss 0.0090 (0.0418)	
training:	Epoch: [30][302/408]	Loss 0.0081 (0.0417)	
training:	Epoch: [30][303/408]	Loss 0.0086 (0.0416)	
training:	Epoch: [30][304/408]	Loss 0.0084 (0.0415)	
training:	Epoch: [30][305/408]	Loss 0.0082 (0.0414)	
training:	Epoch: [30][306/408]	Loss 0.0080 (0.0413)	
training:	Epoch: [30][307/408]	Loss 0.0080 (0.0412)	
training:	Epoch: [30][308/408]	Loss 0.0086 (0.0411)	
training:	Epoch: [30][309/408]	Loss 0.0084 (0.0409)	
training:	Epoch: [30][310/408]	Loss 0.0085 (0.0408)	
training:	Epoch: [30][311/408]	Loss 0.0166 (0.0408)	
training:	Epoch: [30][312/408]	Loss 0.2926 (0.0416)	
training:	Epoch: [30][313/408]	Loss 0.0078 (0.0415)	
training:	Epoch: [30][314/408]	Loss 0.0077 (0.0414)	
training:	Epoch: [30][315/408]	Loss 0.0083 (0.0413)	
training:	Epoch: [30][316/408]	Loss 0.0083 (0.0411)	
training:	Epoch: [30][317/408]	Loss 0.0084 (0.0410)	
training:	Epoch: [30][318/408]	Loss 0.0077 (0.0409)	
training:	Epoch: [30][319/408]	Loss 0.0089 (0.0408)	
training:	Epoch: [30][320/408]	Loss 0.3086 (0.0417)	
training:	Epoch: [30][321/408]	Loss 0.0094 (0.0416)	
training:	Epoch: [30][322/408]	Loss 0.3001 (0.0424)	
training:	Epoch: [30][323/408]	Loss 0.2948 (0.0432)	
training:	Epoch: [30][324/408]	Loss 0.0085 (0.0431)	
training:	Epoch: [30][325/408]	Loss 0.3003 (0.0438)	
training:	Epoch: [30][326/408]	Loss 0.0084 (0.0437)	
training:	Epoch: [30][327/408]	Loss 0.0113 (0.0436)	
training:	Epoch: [30][328/408]	Loss 0.0085 (0.0435)	
training:	Epoch: [30][329/408]	Loss 0.0074 (0.0434)	
training:	Epoch: [30][330/408]	Loss 0.0084 (0.0433)	
training:	Epoch: [30][331/408]	Loss 0.0090 (0.0432)	
training:	Epoch: [30][332/408]	Loss 0.0087 (0.0431)	
training:	Epoch: [30][333/408]	Loss 0.0114 (0.0430)	
training:	Epoch: [30][334/408]	Loss 0.2922 (0.0438)	
training:	Epoch: [30][335/408]	Loss 0.0083 (0.0437)	
training:	Epoch: [30][336/408]	Loss 0.0087 (0.0435)	
training:	Epoch: [30][337/408]	Loss 0.2610 (0.0442)	
training:	Epoch: [30][338/408]	Loss 0.2805 (0.0449)	
training:	Epoch: [30][339/408]	Loss 0.0082 (0.0448)	
training:	Epoch: [30][340/408]	Loss 0.0096 (0.0447)	
training:	Epoch: [30][341/408]	Loss 0.0099 (0.0446)	
training:	Epoch: [30][342/408]	Loss 0.0085 (0.0445)	
training:	Epoch: [30][343/408]	Loss 0.0088 (0.0444)	
training:	Epoch: [30][344/408]	Loss 0.0086 (0.0443)	
training:	Epoch: [30][345/408]	Loss 0.0090 (0.0442)	
training:	Epoch: [30][346/408]	Loss 0.0089 (0.0441)	
training:	Epoch: [30][347/408]	Loss 0.0079 (0.0440)	
training:	Epoch: [30][348/408]	Loss 0.0095 (0.0439)	
training:	Epoch: [30][349/408]	Loss 0.0085 (0.0438)	
training:	Epoch: [30][350/408]	Loss 0.0093 (0.0437)	
training:	Epoch: [30][351/408]	Loss 0.2844 (0.0443)	
training:	Epoch: [30][352/408]	Loss 0.0091 (0.0442)	
training:	Epoch: [30][353/408]	Loss 0.0083 (0.0441)	
training:	Epoch: [30][354/408]	Loss 0.0087 (0.0440)	
training:	Epoch: [30][355/408]	Loss 0.0102 (0.0439)	
training:	Epoch: [30][356/408]	Loss 0.0102 (0.0439)	
training:	Epoch: [30][357/408]	Loss 0.0078 (0.0437)	
training:	Epoch: [30][358/408]	Loss 0.0091 (0.0437)	
training:	Epoch: [30][359/408]	Loss 0.0084 (0.0436)	
training:	Epoch: [30][360/408]	Loss 0.0202 (0.0435)	
training:	Epoch: [30][361/408]	Loss 0.0095 (0.0434)	
training:	Epoch: [30][362/408]	Loss 0.0086 (0.0433)	
training:	Epoch: [30][363/408]	Loss 0.0091 (0.0432)	
training:	Epoch: [30][364/408]	Loss 0.0078 (0.0431)	
training:	Epoch: [30][365/408]	Loss 0.0083 (0.0430)	
training:	Epoch: [30][366/408]	Loss 0.0086 (0.0429)	
training:	Epoch: [30][367/408]	Loss 0.0094 (0.0428)	
training:	Epoch: [30][368/408]	Loss 0.2532 (0.0434)	
training:	Epoch: [30][369/408]	Loss 0.0090 (0.0433)	
training:	Epoch: [30][370/408]	Loss 0.3036 (0.0440)	
training:	Epoch: [30][371/408]	Loss 0.0075 (0.0439)	
training:	Epoch: [30][372/408]	Loss 0.0084 (0.0438)	
training:	Epoch: [30][373/408]	Loss 0.0085 (0.0437)	
training:	Epoch: [30][374/408]	Loss 0.0085 (0.0436)	
training:	Epoch: [30][375/408]	Loss 0.0080 (0.0435)	
training:	Epoch: [30][376/408]	Loss 0.0114 (0.0434)	
training:	Epoch: [30][377/408]	Loss 0.0089 (0.0434)	
training:	Epoch: [30][378/408]	Loss 0.0090 (0.0433)	
training:	Epoch: [30][379/408]	Loss 0.0086 (0.0432)	
training:	Epoch: [30][380/408]	Loss 0.2969 (0.0438)	
training:	Epoch: [30][381/408]	Loss 0.0081 (0.0437)	
training:	Epoch: [30][382/408]	Loss 0.0108 (0.0437)	
training:	Epoch: [30][383/408]	Loss 0.0103 (0.0436)	
training:	Epoch: [30][384/408]	Loss 0.0098 (0.0435)	
training:	Epoch: [30][385/408]	Loss 0.0099 (0.0434)	
training:	Epoch: [30][386/408]	Loss 0.0100 (0.0433)	
training:	Epoch: [30][387/408]	Loss 0.0087 (0.0432)	
training:	Epoch: [30][388/408]	Loss 0.0086 (0.0431)	
training:	Epoch: [30][389/408]	Loss 0.0267 (0.0431)	
training:	Epoch: [30][390/408]	Loss 0.0097 (0.0430)	
training:	Epoch: [30][391/408]	Loss 0.0091 (0.0429)	
training:	Epoch: [30][392/408]	Loss 0.0082 (0.0428)	
training:	Epoch: [30][393/408]	Loss 0.0075 (0.0427)	
training:	Epoch: [30][394/408]	Loss 0.2861 (0.0434)	
training:	Epoch: [30][395/408]	Loss 0.0087 (0.0433)	
training:	Epoch: [30][396/408]	Loss 0.0083 (0.0432)	
training:	Epoch: [30][397/408]	Loss 0.0083 (0.0431)	
training:	Epoch: [30][398/408]	Loss 0.2743 (0.0437)	
training:	Epoch: [30][399/408]	Loss 0.0087 (0.0436)	
training:	Epoch: [30][400/408]	Loss 0.0090 (0.0435)	
training:	Epoch: [30][401/408]	Loss 0.0096 (0.0434)	
training:	Epoch: [30][402/408]	Loss 0.0103 (0.0433)	
training:	Epoch: [30][403/408]	Loss 0.2904 (0.0439)	
training:	Epoch: [30][404/408]	Loss 0.0105 (0.0439)	
training:	Epoch: [30][405/408]	Loss 0.0091 (0.0438)	
training:	Epoch: [30][406/408]	Loss 0.0106 (0.0437)	
training:	Epoch: [30][407/408]	Loss 0.0099 (0.0436)	
training:	Epoch: [30][408/408]	Loss 0.0088 (0.0435)	
Training:	 Loss: 0.0435

Training:	 ACC: 0.9925 0.9925 0.9924 0.9927
Validation:	 ACC: 0.7820 0.7822 0.7881 0.7758
Validation:	 Best_BACC: 0.8226 0.8250 0.8772 0.7679
Validation:	 Loss: 0.9257
Pretraining:	Epoch 31/200
----------
training:	Epoch: [31][1/408]	Loss 0.0080 (0.0080)	
training:	Epoch: [31][2/408]	Loss 0.0082 (0.0081)	
training:	Epoch: [31][3/408]	Loss 0.0078 (0.0080)	
training:	Epoch: [31][4/408]	Loss 0.0095 (0.0084)	
training:	Epoch: [31][5/408]	Loss 0.0082 (0.0084)	
training:	Epoch: [31][6/408]	Loss 0.0091 (0.0085)	
training:	Epoch: [31][7/408]	Loss 0.0096 (0.0086)	
training:	Epoch: [31][8/408]	Loss 0.0086 (0.0086)	
training:	Epoch: [31][9/408]	Loss 0.0089 (0.0087)	
training:	Epoch: [31][10/408]	Loss 0.0093 (0.0087)	
training:	Epoch: [31][11/408]	Loss 0.0094 (0.0088)	
training:	Epoch: [31][12/408]	Loss 0.0080 (0.0087)	
training:	Epoch: [31][13/408]	Loss 0.0085 (0.0087)	
training:	Epoch: [31][14/408]	Loss 0.3122 (0.0304)	
training:	Epoch: [31][15/408]	Loss 0.0089 (0.0290)	
training:	Epoch: [31][16/408]	Loss 0.0094 (0.0277)	
training:	Epoch: [31][17/408]	Loss 0.0093 (0.0266)	
training:	Epoch: [31][18/408]	Loss 0.0089 (0.0257)	
training:	Epoch: [31][19/408]	Loss 0.0093 (0.0248)	
training:	Epoch: [31][20/408]	Loss 0.0085 (0.0240)	
training:	Epoch: [31][21/408]	Loss 0.0083 (0.0232)	
training:	Epoch: [31][22/408]	Loss 0.0092 (0.0226)	
training:	Epoch: [31][23/408]	Loss 0.0094 (0.0220)	
training:	Epoch: [31][24/408]	Loss 0.0084 (0.0215)	
training:	Epoch: [31][25/408]	Loss 0.0078 (0.0209)	
training:	Epoch: [31][26/408]	Loss 0.0083 (0.0204)	
training:	Epoch: [31][27/408]	Loss 0.0085 (0.0200)	
training:	Epoch: [31][28/408]	Loss 0.0098 (0.0196)	
training:	Epoch: [31][29/408]	Loss 0.0089 (0.0193)	
training:	Epoch: [31][30/408]	Loss 0.3047 (0.0288)	
training:	Epoch: [31][31/408]	Loss 0.0085 (0.0281)	
training:	Epoch: [31][32/408]	Loss 0.0097 (0.0275)	
training:	Epoch: [31][33/408]	Loss 0.0095 (0.0270)	
training:	Epoch: [31][34/408]	Loss 0.0083 (0.0264)	
training:	Epoch: [31][35/408]	Loss 0.3087 (0.0345)	
training:	Epoch: [31][36/408]	Loss 0.0087 (0.0338)	
training:	Epoch: [31][37/408]	Loss 0.0094 (0.0331)	
training:	Epoch: [31][38/408]	Loss 0.0090 (0.0325)	
training:	Epoch: [31][39/408]	Loss 0.0090 (0.0319)	
training:	Epoch: [31][40/408]	Loss 0.0090 (0.0313)	
training:	Epoch: [31][41/408]	Loss 0.0083 (0.0308)	
training:	Epoch: [31][42/408]	Loss 0.0082 (0.0302)	
training:	Epoch: [31][43/408]	Loss 0.0085 (0.0297)	
training:	Epoch: [31][44/408]	Loss 0.2808 (0.0354)	
training:	Epoch: [31][45/408]	Loss 0.0085 (0.0348)	
training:	Epoch: [31][46/408]	Loss 0.0081 (0.0342)	
training:	Epoch: [31][47/408]	Loss 0.0087 (0.0337)	
training:	Epoch: [31][48/408]	Loss 0.0082 (0.0332)	
training:	Epoch: [31][49/408]	Loss 0.0086 (0.0327)	
training:	Epoch: [31][50/408]	Loss 0.3062 (0.0381)	
training:	Epoch: [31][51/408]	Loss 0.0089 (0.0376)	
training:	Epoch: [31][52/408]	Loss 0.0084 (0.0370)	
training:	Epoch: [31][53/408]	Loss 0.0089 (0.0365)	
training:	Epoch: [31][54/408]	Loss 0.0081 (0.0360)	
training:	Epoch: [31][55/408]	Loss 0.2936 (0.0406)	
training:	Epoch: [31][56/408]	Loss 0.3070 (0.0454)	
training:	Epoch: [31][57/408]	Loss 0.0082 (0.0447)	
training:	Epoch: [31][58/408]	Loss 0.0101 (0.0441)	
training:	Epoch: [31][59/408]	Loss 0.0100 (0.0436)	
training:	Epoch: [31][60/408]	Loss 0.0093 (0.0430)	
training:	Epoch: [31][61/408]	Loss 0.0090 (0.0424)	
training:	Epoch: [31][62/408]	Loss 0.2806 (0.0463)	
training:	Epoch: [31][63/408]	Loss 0.0090 (0.0457)	
training:	Epoch: [31][64/408]	Loss 0.0087 (0.0451)	
training:	Epoch: [31][65/408]	Loss 0.2975 (0.0490)	
training:	Epoch: [31][66/408]	Loss 0.0094 (0.0484)	
training:	Epoch: [31][67/408]	Loss 0.0079 (0.0478)	
training:	Epoch: [31][68/408]	Loss 0.0084 (0.0472)	
training:	Epoch: [31][69/408]	Loss 0.0088 (0.0466)	
training:	Epoch: [31][70/408]	Loss 0.0093 (0.0461)	
training:	Epoch: [31][71/408]	Loss 0.0083 (0.0456)	
training:	Epoch: [31][72/408]	Loss 0.5599 (0.0527)	
training:	Epoch: [31][73/408]	Loss 0.0083 (0.0521)	
training:	Epoch: [31][74/408]	Loss 0.0084 (0.0515)	
training:	Epoch: [31][75/408]	Loss 0.0088 (0.0510)	
training:	Epoch: [31][76/408]	Loss 0.0103 (0.0504)	
training:	Epoch: [31][77/408]	Loss 0.0091 (0.0499)	
training:	Epoch: [31][78/408]	Loss 0.2837 (0.0529)	
training:	Epoch: [31][79/408]	Loss 0.0088 (0.0523)	
training:	Epoch: [31][80/408]	Loss 0.0089 (0.0518)	
training:	Epoch: [31][81/408]	Loss 0.0095 (0.0513)	
training:	Epoch: [31][82/408]	Loss 0.0087 (0.0507)	
training:	Epoch: [31][83/408]	Loss 0.0081 (0.0502)	
training:	Epoch: [31][84/408]	Loss 0.0091 (0.0497)	
training:	Epoch: [31][85/408]	Loss 0.0093 (0.0493)	
training:	Epoch: [31][86/408]	Loss 0.0094 (0.0488)	
training:	Epoch: [31][87/408]	Loss 0.0085 (0.0483)	
training:	Epoch: [31][88/408]	Loss 0.2910 (0.0511)	
training:	Epoch: [31][89/408]	Loss 0.2956 (0.0538)	
training:	Epoch: [31][90/408]	Loss 0.0081 (0.0533)	
training:	Epoch: [31][91/408]	Loss 0.0089 (0.0528)	
training:	Epoch: [31][92/408]	Loss 0.0094 (0.0524)	
training:	Epoch: [31][93/408]	Loss 0.0096 (0.0519)	
training:	Epoch: [31][94/408]	Loss 0.0079 (0.0514)	
training:	Epoch: [31][95/408]	Loss 0.0086 (0.0510)	
training:	Epoch: [31][96/408]	Loss 0.0087 (0.0506)	
training:	Epoch: [31][97/408]	Loss 0.0086 (0.0501)	
training:	Epoch: [31][98/408]	Loss 0.0094 (0.0497)	
training:	Epoch: [31][99/408]	Loss 0.0100 (0.0493)	
training:	Epoch: [31][100/408]	Loss 0.0085 (0.0489)	
training:	Epoch: [31][101/408]	Loss 0.0088 (0.0485)	
training:	Epoch: [31][102/408]	Loss 0.0089 (0.0481)	
training:	Epoch: [31][103/408]	Loss 0.3039 (0.0506)	
training:	Epoch: [31][104/408]	Loss 0.0097 (0.0502)	
training:	Epoch: [31][105/408]	Loss 0.0091 (0.0498)	
training:	Epoch: [31][106/408]	Loss 0.0088 (0.0494)	
training:	Epoch: [31][107/408]	Loss 0.0083 (0.0490)	
training:	Epoch: [31][108/408]	Loss 0.0088 (0.0487)	
training:	Epoch: [31][109/408]	Loss 0.0089 (0.0483)	
training:	Epoch: [31][110/408]	Loss 0.0094 (0.0480)	
training:	Epoch: [31][111/408]	Loss 0.0079 (0.0476)	
training:	Epoch: [31][112/408]	Loss 0.0083 (0.0472)	
training:	Epoch: [31][113/408]	Loss 0.0100 (0.0469)	
training:	Epoch: [31][114/408]	Loss 0.0089 (0.0466)	
training:	Epoch: [31][115/408]	Loss 0.0088 (0.0462)	
training:	Epoch: [31][116/408]	Loss 0.0092 (0.0459)	
training:	Epoch: [31][117/408]	Loss 0.0099 (0.0456)	
training:	Epoch: [31][118/408]	Loss 0.3043 (0.0478)	
training:	Epoch: [31][119/408]	Loss 0.0086 (0.0475)	
training:	Epoch: [31][120/408]	Loss 0.0085 (0.0472)	
training:	Epoch: [31][121/408]	Loss 0.0089 (0.0468)	
training:	Epoch: [31][122/408]	Loss 0.0085 (0.0465)	
training:	Epoch: [31][123/408]	Loss 0.0088 (0.0462)	
training:	Epoch: [31][124/408]	Loss 0.0095 (0.0459)	
training:	Epoch: [31][125/408]	Loss 0.1935 (0.0471)	
training:	Epoch: [31][126/408]	Loss 0.0082 (0.0468)	
training:	Epoch: [31][127/408]	Loss 0.0087 (0.0465)	
training:	Epoch: [31][128/408]	Loss 0.0089 (0.0462)	
training:	Epoch: [31][129/408]	Loss 0.0096 (0.0459)	
training:	Epoch: [31][130/408]	Loss 0.0074 (0.0456)	
training:	Epoch: [31][131/408]	Loss 0.0086 (0.0453)	
training:	Epoch: [31][132/408]	Loss 0.0085 (0.0451)	
training:	Epoch: [31][133/408]	Loss 0.0126 (0.0448)	
training:	Epoch: [31][134/408]	Loss 0.0086 (0.0445)	
training:	Epoch: [31][135/408]	Loss 0.0093 (0.0443)	
training:	Epoch: [31][136/408]	Loss 0.0088 (0.0440)	
training:	Epoch: [31][137/408]	Loss 0.0102 (0.0438)	
training:	Epoch: [31][138/408]	Loss 0.0187 (0.0436)	
training:	Epoch: [31][139/408]	Loss 0.0084 (0.0433)	
training:	Epoch: [31][140/408]	Loss 0.3062 (0.0452)	
training:	Epoch: [31][141/408]	Loss 0.2642 (0.0468)	
training:	Epoch: [31][142/408]	Loss 0.0076 (0.0465)	
training:	Epoch: [31][143/408]	Loss 0.0083 (0.0462)	
training:	Epoch: [31][144/408]	Loss 0.0102 (0.0460)	
training:	Epoch: [31][145/408]	Loss 0.0093 (0.0457)	
training:	Epoch: [31][146/408]	Loss 0.0094 (0.0455)	
training:	Epoch: [31][147/408]	Loss 0.0087 (0.0452)	
training:	Epoch: [31][148/408]	Loss 0.0085 (0.0450)	
training:	Epoch: [31][149/408]	Loss 0.0089 (0.0447)	
training:	Epoch: [31][150/408]	Loss 0.0090 (0.0445)	
training:	Epoch: [31][151/408]	Loss 0.0083 (0.0443)	
training:	Epoch: [31][152/408]	Loss 0.0128 (0.0441)	
training:	Epoch: [31][153/408]	Loss 0.0103 (0.0438)	
training:	Epoch: [31][154/408]	Loss 0.0090 (0.0436)	
training:	Epoch: [31][155/408]	Loss 0.0088 (0.0434)	
training:	Epoch: [31][156/408]	Loss 0.0100 (0.0432)	
training:	Epoch: [31][157/408]	Loss 0.0091 (0.0430)	
training:	Epoch: [31][158/408]	Loss 0.0095 (0.0427)	
training:	Epoch: [31][159/408]	Loss 0.0088 (0.0425)	
training:	Epoch: [31][160/408]	Loss 0.0092 (0.0423)	
training:	Epoch: [31][161/408]	Loss 0.0247 (0.0422)	
training:	Epoch: [31][162/408]	Loss 0.2571 (0.0435)	
training:	Epoch: [31][163/408]	Loss 0.0078 (0.0433)	
training:	Epoch: [31][164/408]	Loss 0.0098 (0.0431)	
training:	Epoch: [31][165/408]	Loss 0.0095 (0.0429)	
training:	Epoch: [31][166/408]	Loss 0.0083 (0.0427)	
training:	Epoch: [31][167/408]	Loss 0.0097 (0.0425)	
training:	Epoch: [31][168/408]	Loss 0.3218 (0.0442)	
training:	Epoch: [31][169/408]	Loss 0.0113 (0.0440)	
training:	Epoch: [31][170/408]	Loss 0.0742 (0.0441)	
training:	Epoch: [31][171/408]	Loss 0.0089 (0.0439)	
training:	Epoch: [31][172/408]	Loss 0.0095 (0.0437)	
training:	Epoch: [31][173/408]	Loss 0.0093 (0.0435)	
training:	Epoch: [31][174/408]	Loss 0.0078 (0.0433)	
training:	Epoch: [31][175/408]	Loss 0.1026 (0.0437)	
training:	Epoch: [31][176/408]	Loss 0.0086 (0.0435)	
training:	Epoch: [31][177/408]	Loss 0.0081 (0.0433)	
training:	Epoch: [31][178/408]	Loss 0.0074 (0.0431)	
training:	Epoch: [31][179/408]	Loss 0.0081 (0.0429)	
training:	Epoch: [31][180/408]	Loss 0.0079 (0.0427)	
training:	Epoch: [31][181/408]	Loss 0.0314 (0.0426)	
training:	Epoch: [31][182/408]	Loss 0.0095 (0.0424)	
training:	Epoch: [31][183/408]	Loss 0.0101 (0.0423)	
training:	Epoch: [31][184/408]	Loss 0.0100 (0.0421)	
training:	Epoch: [31][185/408]	Loss 0.0085 (0.0419)	
training:	Epoch: [31][186/408]	Loss 0.0077 (0.0417)	
training:	Epoch: [31][187/408]	Loss 0.0083 (0.0415)	
training:	Epoch: [31][188/408]	Loss 0.1503 (0.0421)	
training:	Epoch: [31][189/408]	Loss 0.0093 (0.0419)	
training:	Epoch: [31][190/408]	Loss 0.0081 (0.0418)	
training:	Epoch: [31][191/408]	Loss 0.0119 (0.0416)	
training:	Epoch: [31][192/408]	Loss 0.0088 (0.0414)	
training:	Epoch: [31][193/408]	Loss 0.0089 (0.0413)	
training:	Epoch: [31][194/408]	Loss 0.2649 (0.0424)	
training:	Epoch: [31][195/408]	Loss 0.0110 (0.0423)	
training:	Epoch: [31][196/408]	Loss 0.0090 (0.0421)	
training:	Epoch: [31][197/408]	Loss 0.0187 (0.0420)	
training:	Epoch: [31][198/408]	Loss 0.0086 (0.0418)	
training:	Epoch: [31][199/408]	Loss 0.0096 (0.0416)	
training:	Epoch: [31][200/408]	Loss 0.0085 (0.0415)	
training:	Epoch: [31][201/408]	Loss 0.1245 (0.0419)	
training:	Epoch: [31][202/408]	Loss 0.0106 (0.0417)	
training:	Epoch: [31][203/408]	Loss 0.2955 (0.0430)	
training:	Epoch: [31][204/408]	Loss 0.0102 (0.0428)	
training:	Epoch: [31][205/408]	Loss 0.0078 (0.0427)	
training:	Epoch: [31][206/408]	Loss 0.0079 (0.0425)	
training:	Epoch: [31][207/408]	Loss 0.0086 (0.0423)	
training:	Epoch: [31][208/408]	Loss 0.0089 (0.0422)	
training:	Epoch: [31][209/408]	Loss 0.0110 (0.0420)	
training:	Epoch: [31][210/408]	Loss 0.0078 (0.0419)	
training:	Epoch: [31][211/408]	Loss 0.0091 (0.0417)	
training:	Epoch: [31][212/408]	Loss 0.0956 (0.0420)	
training:	Epoch: [31][213/408]	Loss 0.0095 (0.0418)	
training:	Epoch: [31][214/408]	Loss 0.0082 (0.0416)	
training:	Epoch: [31][215/408]	Loss 0.0089 (0.0415)	
training:	Epoch: [31][216/408]	Loss 0.2885 (0.0426)	
training:	Epoch: [31][217/408]	Loss 0.0077 (0.0425)	
training:	Epoch: [31][218/408]	Loss 0.0084 (0.0423)	
training:	Epoch: [31][219/408]	Loss 0.0100 (0.0422)	
training:	Epoch: [31][220/408]	Loss 0.0082 (0.0420)	
training:	Epoch: [31][221/408]	Loss 0.0315 (0.0420)	
training:	Epoch: [31][222/408]	Loss 0.0105 (0.0418)	
training:	Epoch: [31][223/408]	Loss 0.0085 (0.0417)	
training:	Epoch: [31][224/408]	Loss 0.0089 (0.0415)	
training:	Epoch: [31][225/408]	Loss 0.0079 (0.0414)	
training:	Epoch: [31][226/408]	Loss 0.0079 (0.0412)	
training:	Epoch: [31][227/408]	Loss 0.0084 (0.0411)	
training:	Epoch: [31][228/408]	Loss 0.0093 (0.0409)	
training:	Epoch: [31][229/408]	Loss 0.3134 (0.0421)	
training:	Epoch: [31][230/408]	Loss 0.0082 (0.0420)	
training:	Epoch: [31][231/408]	Loss 0.0105 (0.0419)	
training:	Epoch: [31][232/408]	Loss 0.0089 (0.0417)	
training:	Epoch: [31][233/408]	Loss 0.1107 (0.0420)	
training:	Epoch: [31][234/408]	Loss 0.0082 (0.0419)	
training:	Epoch: [31][235/408]	Loss 0.0088 (0.0417)	
training:	Epoch: [31][236/408]	Loss 0.0077 (0.0416)	
training:	Epoch: [31][237/408]	Loss 0.0081 (0.0414)	
training:	Epoch: [31][238/408]	Loss 0.0080 (0.0413)	
training:	Epoch: [31][239/408]	Loss 0.0093 (0.0412)	
training:	Epoch: [31][240/408]	Loss 0.0089 (0.0410)	
training:	Epoch: [31][241/408]	Loss 0.0086 (0.0409)	
training:	Epoch: [31][242/408]	Loss 0.0084 (0.0408)	
training:	Epoch: [31][243/408]	Loss 0.0080 (0.0406)	
training:	Epoch: [31][244/408]	Loss 0.0075 (0.0405)	
training:	Epoch: [31][245/408]	Loss 0.0096 (0.0404)	
training:	Epoch: [31][246/408]	Loss 0.3132 (0.0415)	
training:	Epoch: [31][247/408]	Loss 0.0077 (0.0413)	
training:	Epoch: [31][248/408]	Loss 0.0076 (0.0412)	
training:	Epoch: [31][249/408]	Loss 0.0110 (0.0411)	
training:	Epoch: [31][250/408]	Loss 0.0072 (0.0409)	
training:	Epoch: [31][251/408]	Loss 0.0097 (0.0408)	
training:	Epoch: [31][252/408]	Loss 0.0106 (0.0407)	
training:	Epoch: [31][253/408]	Loss 0.2771 (0.0416)	
training:	Epoch: [31][254/408]	Loss 0.0082 (0.0415)	
training:	Epoch: [31][255/408]	Loss 0.0136 (0.0414)	
training:	Epoch: [31][256/408]	Loss 0.0077 (0.0413)	
training:	Epoch: [31][257/408]	Loss 0.0112 (0.0411)	
training:	Epoch: [31][258/408]	Loss 0.0082 (0.0410)	
training:	Epoch: [31][259/408]	Loss 0.0082 (0.0409)	
training:	Epoch: [31][260/408]	Loss 0.0116 (0.0408)	
training:	Epoch: [31][261/408]	Loss 0.0082 (0.0406)	
training:	Epoch: [31][262/408]	Loss 0.3073 (0.0417)	
training:	Epoch: [31][263/408]	Loss 0.0108 (0.0415)	
training:	Epoch: [31][264/408]	Loss 0.2853 (0.0425)	
training:	Epoch: [31][265/408]	Loss 0.2861 (0.0434)	
training:	Epoch: [31][266/408]	Loss 0.0105 (0.0433)	
training:	Epoch: [31][267/408]	Loss 0.0084 (0.0431)	
training:	Epoch: [31][268/408]	Loss 0.0091 (0.0430)	
training:	Epoch: [31][269/408]	Loss 0.3043 (0.0440)	
training:	Epoch: [31][270/408]	Loss 0.3014 (0.0449)	
training:	Epoch: [31][271/408]	Loss 0.0086 (0.0448)	
training:	Epoch: [31][272/408]	Loss 0.0228 (0.0447)	
training:	Epoch: [31][273/408]	Loss 0.0082 (0.0446)	
training:	Epoch: [31][274/408]	Loss 0.3020 (0.0455)	
training:	Epoch: [31][275/408]	Loss 0.2947 (0.0464)	
training:	Epoch: [31][276/408]	Loss 0.0090 (0.0463)	
training:	Epoch: [31][277/408]	Loss 0.0090 (0.0462)	
training:	Epoch: [31][278/408]	Loss 0.0091 (0.0460)	
training:	Epoch: [31][279/408]	Loss 0.0112 (0.0459)	
training:	Epoch: [31][280/408]	Loss 0.0162 (0.0458)	
training:	Epoch: [31][281/408]	Loss 0.0093 (0.0457)	
training:	Epoch: [31][282/408]	Loss 0.0082 (0.0455)	
training:	Epoch: [31][283/408]	Loss 0.0091 (0.0454)	
training:	Epoch: [31][284/408]	Loss 0.0082 (0.0453)	
training:	Epoch: [31][285/408]	Loss 0.0132 (0.0452)	
training:	Epoch: [31][286/408]	Loss 0.0092 (0.0450)	
training:	Epoch: [31][287/408]	Loss 0.0094 (0.0449)	
training:	Epoch: [31][288/408]	Loss 0.2835 (0.0457)	
training:	Epoch: [31][289/408]	Loss 0.0074 (0.0456)	
training:	Epoch: [31][290/408]	Loss 0.3135 (0.0465)	
training:	Epoch: [31][291/408]	Loss 0.0104 (0.0464)	
training:	Epoch: [31][292/408]	Loss 0.0084 (0.0463)	
training:	Epoch: [31][293/408]	Loss 0.0081 (0.0461)	
training:	Epoch: [31][294/408]	Loss 0.0281 (0.0461)	
training:	Epoch: [31][295/408]	Loss 0.0086 (0.0460)	
training:	Epoch: [31][296/408]	Loss 0.2967 (0.0468)	
training:	Epoch: [31][297/408]	Loss 0.0106 (0.0467)	
training:	Epoch: [31][298/408]	Loss 0.0099 (0.0466)	
training:	Epoch: [31][299/408]	Loss 0.0079 (0.0464)	
training:	Epoch: [31][300/408]	Loss 0.0094 (0.0463)	
training:	Epoch: [31][301/408]	Loss 0.0093 (0.0462)	
training:	Epoch: [31][302/408]	Loss 0.0124 (0.0461)	
training:	Epoch: [31][303/408]	Loss 0.0475 (0.0461)	
training:	Epoch: [31][304/408]	Loss 0.0319 (0.0460)	
training:	Epoch: [31][305/408]	Loss 0.0085 (0.0459)	
training:	Epoch: [31][306/408]	Loss 0.0100 (0.0458)	
training:	Epoch: [31][307/408]	Loss 0.0108 (0.0457)	
training:	Epoch: [31][308/408]	Loss 0.0079 (0.0456)	
training:	Epoch: [31][309/408]	Loss 0.0094 (0.0454)	
training:	Epoch: [31][310/408]	Loss 0.3067 (0.0463)	
training:	Epoch: [31][311/408]	Loss 0.0083 (0.0462)	
training:	Epoch: [31][312/408]	Loss 0.0098 (0.0460)	
training:	Epoch: [31][313/408]	Loss 0.0101 (0.0459)	
training:	Epoch: [31][314/408]	Loss 0.0259 (0.0459)	
training:	Epoch: [31][315/408]	Loss 0.0097 (0.0457)	
training:	Epoch: [31][316/408]	Loss 0.0090 (0.0456)	
training:	Epoch: [31][317/408]	Loss 0.0080 (0.0455)	
training:	Epoch: [31][318/408]	Loss 0.0078 (0.0454)	
training:	Epoch: [31][319/408]	Loss 0.1088 (0.0456)	
training:	Epoch: [31][320/408]	Loss 0.0087 (0.0455)	
training:	Epoch: [31][321/408]	Loss 0.0092 (0.0454)	
training:	Epoch: [31][322/408]	Loss 0.0091 (0.0453)	
training:	Epoch: [31][323/408]	Loss 0.0088 (0.0451)	
training:	Epoch: [31][324/408]	Loss 0.0082 (0.0450)	
training:	Epoch: [31][325/408]	Loss 0.0096 (0.0449)	
training:	Epoch: [31][326/408]	Loss 0.0089 (0.0448)	
training:	Epoch: [31][327/408]	Loss 0.3400 (0.0457)	
training:	Epoch: [31][328/408]	Loss 0.0095 (0.0456)	
training:	Epoch: [31][329/408]	Loss 0.0091 (0.0455)	
training:	Epoch: [31][330/408]	Loss 0.0077 (0.0454)	
training:	Epoch: [31][331/408]	Loss 0.0117 (0.0453)	
training:	Epoch: [31][332/408]	Loss 0.0077 (0.0452)	
training:	Epoch: [31][333/408]	Loss 0.0085 (0.0450)	
training:	Epoch: [31][334/408]	Loss 0.0411 (0.0450)	
training:	Epoch: [31][335/408]	Loss 0.0137 (0.0449)	
training:	Epoch: [31][336/408]	Loss 0.0098 (0.0448)	
training:	Epoch: [31][337/408]	Loss 0.0091 (0.0447)	
training:	Epoch: [31][338/408]	Loss 0.0082 (0.0446)	
training:	Epoch: [31][339/408]	Loss 0.2801 (0.0453)	
training:	Epoch: [31][340/408]	Loss 0.0098 (0.0452)	
training:	Epoch: [31][341/408]	Loss 0.0121 (0.0451)	
training:	Epoch: [31][342/408]	Loss 0.0090 (0.0450)	
training:	Epoch: [31][343/408]	Loss 0.0131 (0.0449)	
training:	Epoch: [31][344/408]	Loss 0.0535 (0.0449)	
training:	Epoch: [31][345/408]	Loss 0.0082 (0.0448)	
training:	Epoch: [31][346/408]	Loss 0.0086 (0.0447)	
training:	Epoch: [31][347/408]	Loss 0.2853 (0.0454)	
training:	Epoch: [31][348/408]	Loss 0.3049 (0.0462)	
training:	Epoch: [31][349/408]	Loss 0.0086 (0.0461)	
training:	Epoch: [31][350/408]	Loss 0.3086 (0.0468)	
training:	Epoch: [31][351/408]	Loss 0.0082 (0.0467)	
training:	Epoch: [31][352/408]	Loss 0.0094 (0.0466)	
training:	Epoch: [31][353/408]	Loss 0.2855 (0.0473)	
training:	Epoch: [31][354/408]	Loss 0.0078 (0.0472)	
training:	Epoch: [31][355/408]	Loss 0.0087 (0.0471)	
training:	Epoch: [31][356/408]	Loss 0.0088 (0.0469)	
training:	Epoch: [31][357/408]	Loss 0.0087 (0.0468)	
training:	Epoch: [31][358/408]	Loss 0.0094 (0.0467)	
training:	Epoch: [31][359/408]	Loss 0.0094 (0.0466)	
training:	Epoch: [31][360/408]	Loss 0.0088 (0.0465)	
training:	Epoch: [31][361/408]	Loss 0.0082 (0.0464)	
training:	Epoch: [31][362/408]	Loss 0.0103 (0.0463)	
training:	Epoch: [31][363/408]	Loss 0.0142 (0.0462)	
training:	Epoch: [31][364/408]	Loss 0.0091 (0.0461)	
training:	Epoch: [31][365/408]	Loss 0.0080 (0.0460)	
training:	Epoch: [31][366/408]	Loss 0.0081 (0.0459)	
training:	Epoch: [31][367/408]	Loss 0.0125 (0.0458)	
training:	Epoch: [31][368/408]	Loss 0.0091 (0.0457)	
training:	Epoch: [31][369/408]	Loss 0.3031 (0.0464)	
training:	Epoch: [31][370/408]	Loss 0.0083 (0.0463)	
training:	Epoch: [31][371/408]	Loss 0.0083 (0.0462)	
training:	Epoch: [31][372/408]	Loss 0.0535 (0.0462)	
training:	Epoch: [31][373/408]	Loss 0.0093 (0.0461)	
training:	Epoch: [31][374/408]	Loss 0.0091 (0.0460)	
training:	Epoch: [31][375/408]	Loss 0.0094 (0.0459)	
training:	Epoch: [31][376/408]	Loss 0.0088 (0.0458)	
training:	Epoch: [31][377/408]	Loss 0.0083 (0.0457)	
training:	Epoch: [31][378/408]	Loss 0.0084 (0.0457)	
training:	Epoch: [31][379/408]	Loss 0.0093 (0.0456)	
training:	Epoch: [31][380/408]	Loss 0.0082 (0.0455)	
training:	Epoch: [31][381/408]	Loss 0.2805 (0.0461)	
training:	Epoch: [31][382/408]	Loss 0.0088 (0.0460)	
training:	Epoch: [31][383/408]	Loss 0.0093 (0.0459)	
training:	Epoch: [31][384/408]	Loss 0.0092 (0.0458)	
training:	Epoch: [31][385/408]	Loss 0.0108 (0.0457)	
training:	Epoch: [31][386/408]	Loss 0.0083 (0.0456)	
training:	Epoch: [31][387/408]	Loss 0.0099 (0.0455)	
training:	Epoch: [31][388/408]	Loss 0.0113 (0.0454)	
training:	Epoch: [31][389/408]	Loss 0.0100 (0.0453)	
training:	Epoch: [31][390/408]	Loss 0.0396 (0.0453)	
training:	Epoch: [31][391/408]	Loss 0.0095 (0.0452)	
training:	Epoch: [31][392/408]	Loss 0.0082 (0.0451)	
training:	Epoch: [31][393/408]	Loss 0.0081 (0.0450)	
training:	Epoch: [31][394/408]	Loss 0.3192 (0.0457)	
training:	Epoch: [31][395/408]	Loss 0.0100 (0.0456)	
training:	Epoch: [31][396/408]	Loss 0.0095 (0.0455)	
training:	Epoch: [31][397/408]	Loss 0.0081 (0.0454)	
training:	Epoch: [31][398/408]	Loss 0.0090 (0.0454)	
training:	Epoch: [31][399/408]	Loss 0.0090 (0.0453)	
training:	Epoch: [31][400/408]	Loss 0.0084 (0.0452)	
training:	Epoch: [31][401/408]	Loss 0.3046 (0.0458)	
training:	Epoch: [31][402/408]	Loss 0.0087 (0.0457)	
training:	Epoch: [31][403/408]	Loss 0.0084 (0.0456)	
training:	Epoch: [31][404/408]	Loss 0.0092 (0.0455)	
training:	Epoch: [31][405/408]	Loss 0.0331 (0.0455)	
training:	Epoch: [31][406/408]	Loss 0.3130 (0.0462)	
training:	Epoch: [31][407/408]	Loss 0.0121 (0.0461)	
training:	Epoch: [31][408/408]	Loss 0.0118 (0.0460)	
Training:	 Loss: 0.0459

Training:	 ACC: 0.9918 0.9917 0.9906 0.9930
Validation:	 ACC: 0.7802 0.7796 0.7656 0.7948
Validation:	 Best_BACC: 0.8226 0.8250 0.8772 0.7679
Validation:	 Loss: 0.9355
Pretraining:	Epoch 32/200
----------
training:	Epoch: [32][1/408]	Loss 0.0095 (0.0095)	
training:	Epoch: [32][2/408]	Loss 0.0099 (0.0097)	
training:	Epoch: [32][3/408]	Loss 0.2731 (0.0975)	
training:	Epoch: [32][4/408]	Loss 0.0083 (0.0752)	
training:	Epoch: [32][5/408]	Loss 0.0086 (0.0619)	
training:	Epoch: [32][6/408]	Loss 0.0086 (0.0530)	
training:	Epoch: [32][7/408]	Loss 0.0093 (0.0467)	
training:	Epoch: [32][8/408]	Loss 0.0095 (0.0421)	
training:	Epoch: [32][9/408]	Loss 0.0086 (0.0384)	
training:	Epoch: [32][10/408]	Loss 0.0085 (0.0354)	
training:	Epoch: [32][11/408]	Loss 0.0087 (0.0330)	
training:	Epoch: [32][12/408]	Loss 0.0090 (0.0310)	
training:	Epoch: [32][13/408]	Loss 0.0092 (0.0293)	
training:	Epoch: [32][14/408]	Loss 0.0104 (0.0279)	
training:	Epoch: [32][15/408]	Loss 0.0088 (0.0267)	
training:	Epoch: [32][16/408]	Loss 0.0080 (0.0255)	
training:	Epoch: [32][17/408]	Loss 0.0096 (0.0246)	
training:	Epoch: [32][18/408]	Loss 0.0076 (0.0236)	
training:	Epoch: [32][19/408]	Loss 0.0097 (0.0229)	
training:	Epoch: [32][20/408]	Loss 0.0088 (0.0222)	
training:	Epoch: [32][21/408]	Loss 0.0085 (0.0215)	
training:	Epoch: [32][22/408]	Loss 0.0078 (0.0209)	
training:	Epoch: [32][23/408]	Loss 0.0087 (0.0204)	
training:	Epoch: [32][24/408]	Loss 0.0082 (0.0199)	
training:	Epoch: [32][25/408]	Loss 0.0149 (0.0197)	
training:	Epoch: [32][26/408]	Loss 0.0083 (0.0192)	
training:	Epoch: [32][27/408]	Loss 0.0079 (0.0188)	
training:	Epoch: [32][28/408]	Loss 0.0085 (0.0184)	
training:	Epoch: [32][29/408]	Loss 0.1384 (0.0226)	
training:	Epoch: [32][30/408]	Loss 0.0488 (0.0235)	
training:	Epoch: [32][31/408]	Loss 0.0156 (0.0232)	
training:	Epoch: [32][32/408]	Loss 0.0099 (0.0228)	
training:	Epoch: [32][33/408]	Loss 0.0081 (0.0223)	
training:	Epoch: [32][34/408]	Loss 0.0145 (0.0221)	
training:	Epoch: [32][35/408]	Loss 0.0133 (0.0219)	
training:	Epoch: [32][36/408]	Loss 0.0221 (0.0219)	
training:	Epoch: [32][37/408]	Loss 0.0834 (0.0235)	
training:	Epoch: [32][38/408]	Loss 0.5003 (0.0361)	
training:	Epoch: [32][39/408]	Loss 0.0088 (0.0354)	
training:	Epoch: [32][40/408]	Loss 0.3572 (0.0434)	
training:	Epoch: [32][41/408]	Loss 0.0094 (0.0426)	
training:	Epoch: [32][42/408]	Loss 0.0120 (0.0419)	
training:	Epoch: [32][43/408]	Loss 0.2969 (0.0478)	
training:	Epoch: [32][44/408]	Loss 0.0076 (0.0469)	
training:	Epoch: [32][45/408]	Loss 0.0089 (0.0460)	
training:	Epoch: [32][46/408]	Loss 0.0084 (0.0452)	
training:	Epoch: [32][47/408]	Loss 0.0086 (0.0444)	
training:	Epoch: [32][48/408]	Loss 0.0122 (0.0438)	
training:	Epoch: [32][49/408]	Loss 0.0550 (0.0440)	
training:	Epoch: [32][50/408]	Loss 0.0079 (0.0433)	
training:	Epoch: [32][51/408]	Loss 0.0189 (0.0428)	
training:	Epoch: [32][52/408]	Loss 0.0089 (0.0421)	
training:	Epoch: [32][53/408]	Loss 0.1529 (0.0442)	
training:	Epoch: [32][54/408]	Loss 0.0090 (0.0436)	
training:	Epoch: [32][55/408]	Loss 0.0082 (0.0429)	
training:	Epoch: [32][56/408]	Loss 0.0081 (0.0423)	
training:	Epoch: [32][57/408]	Loss 0.0083 (0.0417)	
training:	Epoch: [32][58/408]	Loss 0.0090 (0.0412)	
training:	Epoch: [32][59/408]	Loss 0.0078 (0.0406)	
training:	Epoch: [32][60/408]	Loss 0.5958 (0.0498)	
training:	Epoch: [32][61/408]	Loss 0.0091 (0.0492)	
training:	Epoch: [32][62/408]	Loss 0.0094 (0.0485)	
training:	Epoch: [32][63/408]	Loss 0.0087 (0.0479)	
training:	Epoch: [32][64/408]	Loss 0.0086 (0.0473)	
training:	Epoch: [32][65/408]	Loss 0.0089 (0.0467)	
training:	Epoch: [32][66/408]	Loss 0.0091 (0.0461)	
training:	Epoch: [32][67/408]	Loss 0.3060 (0.0500)	
training:	Epoch: [32][68/408]	Loss 0.0086 (0.0494)	
training:	Epoch: [32][69/408]	Loss 0.0094 (0.0488)	
training:	Epoch: [32][70/408]	Loss 0.0085 (0.0482)	
training:	Epoch: [32][71/408]	Loss 0.0078 (0.0477)	
training:	Epoch: [32][72/408]	Loss 0.0114 (0.0472)	
training:	Epoch: [32][73/408]	Loss 0.0083 (0.0466)	
training:	Epoch: [32][74/408]	Loss 0.0089 (0.0461)	
training:	Epoch: [32][75/408]	Loss 0.0096 (0.0456)	
training:	Epoch: [32][76/408]	Loss 0.0079 (0.0451)	
training:	Epoch: [32][77/408]	Loss 0.2176 (0.0474)	
training:	Epoch: [32][78/408]	Loss 0.0095 (0.0469)	
training:	Epoch: [32][79/408]	Loss 0.1335 (0.0480)	
training:	Epoch: [32][80/408]	Loss 0.0095 (0.0475)	
training:	Epoch: [32][81/408]	Loss 0.0322 (0.0473)	
training:	Epoch: [32][82/408]	Loss 0.0073 (0.0468)	
training:	Epoch: [32][83/408]	Loss 0.0099 (0.0464)	
training:	Epoch: [32][84/408]	Loss 0.0087 (0.0459)	
training:	Epoch: [32][85/408]	Loss 0.0608 (0.0461)	
training:	Epoch: [32][86/408]	Loss 0.2912 (0.0490)	
training:	Epoch: [32][87/408]	Loss 0.0083 (0.0485)	
training:	Epoch: [32][88/408]	Loss 0.2675 (0.0510)	
training:	Epoch: [32][89/408]	Loss 0.0143 (0.0506)	
training:	Epoch: [32][90/408]	Loss 0.0107 (0.0501)	
training:	Epoch: [32][91/408]	Loss 0.0360 (0.0500)	
training:	Epoch: [32][92/408]	Loss 0.0089 (0.0495)	
training:	Epoch: [32][93/408]	Loss 0.0094 (0.0491)	
training:	Epoch: [32][94/408]	Loss 0.0087 (0.0487)	
training:	Epoch: [32][95/408]	Loss 0.0088 (0.0483)	
training:	Epoch: [32][96/408]	Loss 0.0088 (0.0478)	
training:	Epoch: [32][97/408]	Loss 0.0145 (0.0475)	
training:	Epoch: [32][98/408]	Loss 0.0367 (0.0474)	
training:	Epoch: [32][99/408]	Loss 0.0434 (0.0473)	
training:	Epoch: [32][100/408]	Loss 0.2903 (0.0498)	
training:	Epoch: [32][101/408]	Loss 0.0081 (0.0494)	
training:	Epoch: [32][102/408]	Loss 0.0087 (0.0490)	
training:	Epoch: [32][103/408]	Loss 0.0914 (0.0494)	
training:	Epoch: [32][104/408]	Loss 0.0162 (0.0491)	
training:	Epoch: [32][105/408]	Loss 0.0105 (0.0487)	
training:	Epoch: [32][106/408]	Loss 0.0087 (0.0483)	
training:	Epoch: [32][107/408]	Loss 0.0113 (0.0480)	
training:	Epoch: [32][108/408]	Loss 0.0090 (0.0476)	
training:	Epoch: [32][109/408]	Loss 0.0076 (0.0472)	
training:	Epoch: [32][110/408]	Loss 0.3018 (0.0496)	
training:	Epoch: [32][111/408]	Loss 0.0189 (0.0493)	
training:	Epoch: [32][112/408]	Loss 0.0077 (0.0489)	
training:	Epoch: [32][113/408]	Loss 0.5315 (0.0532)	
training:	Epoch: [32][114/408]	Loss 0.0074 (0.0528)	
training:	Epoch: [32][115/408]	Loss 0.0099 (0.0524)	
training:	Epoch: [32][116/408]	Loss 0.0078 (0.0520)	
training:	Epoch: [32][117/408]	Loss 0.0152 (0.0517)	
training:	Epoch: [32][118/408]	Loss 0.0086 (0.0513)	
training:	Epoch: [32][119/408]	Loss 0.0096 (0.0510)	
training:	Epoch: [32][120/408]	Loss 0.0100 (0.0506)	
training:	Epoch: [32][121/408]	Loss 0.6187 (0.0553)	
training:	Epoch: [32][122/408]	Loss 0.0085 (0.0550)	
training:	Epoch: [32][123/408]	Loss 0.0095 (0.0546)	
training:	Epoch: [32][124/408]	Loss 0.0077 (0.0542)	
training:	Epoch: [32][125/408]	Loss 0.0122 (0.0539)	
training:	Epoch: [32][126/408]	Loss 0.0099 (0.0535)	
training:	Epoch: [32][127/408]	Loss 0.0452 (0.0535)	
training:	Epoch: [32][128/408]	Loss 0.0085 (0.0531)	
training:	Epoch: [32][129/408]	Loss 0.0087 (0.0528)	
training:	Epoch: [32][130/408]	Loss 0.0088 (0.0524)	
training:	Epoch: [32][131/408]	Loss 0.0094 (0.0521)	
training:	Epoch: [32][132/408]	Loss 0.0089 (0.0518)	
training:	Epoch: [32][133/408]	Loss 0.0091 (0.0514)	
training:	Epoch: [32][134/408]	Loss 0.2948 (0.0533)	
training:	Epoch: [32][135/408]	Loss 0.0090 (0.0529)	
training:	Epoch: [32][136/408]	Loss 0.0088 (0.0526)	
training:	Epoch: [32][137/408]	Loss 0.0084 (0.0523)	
training:	Epoch: [32][138/408]	Loss 0.0620 (0.0524)	
training:	Epoch: [32][139/408]	Loss 0.0096 (0.0520)	
training:	Epoch: [32][140/408]	Loss 0.2948 (0.0538)	
training:	Epoch: [32][141/408]	Loss 0.0106 (0.0535)	
training:	Epoch: [32][142/408]	Loss 0.0094 (0.0532)	
training:	Epoch: [32][143/408]	Loss 0.0084 (0.0529)	
training:	Epoch: [32][144/408]	Loss 0.0090 (0.0525)	
training:	Epoch: [32][145/408]	Loss 0.0082 (0.0522)	
training:	Epoch: [32][146/408]	Loss 0.0091 (0.0519)	
training:	Epoch: [32][147/408]	Loss 0.1186 (0.0524)	
training:	Epoch: [32][148/408]	Loss 0.0085 (0.0521)	
training:	Epoch: [32][149/408]	Loss 0.0092 (0.0518)	
training:	Epoch: [32][150/408]	Loss 0.0070 (0.0515)	
training:	Epoch: [32][151/408]	Loss 0.0091 (0.0512)	
training:	Epoch: [32][152/408]	Loss 0.2845 (0.0528)	
training:	Epoch: [32][153/408]	Loss 0.0147 (0.0525)	
training:	Epoch: [32][154/408]	Loss 0.0132 (0.0523)	
training:	Epoch: [32][155/408]	Loss 0.0068 (0.0520)	
training:	Epoch: [32][156/408]	Loss 0.0087 (0.0517)	
training:	Epoch: [32][157/408]	Loss 0.0089 (0.0514)	
training:	Epoch: [32][158/408]	Loss 0.0094 (0.0512)	
training:	Epoch: [32][159/408]	Loss 0.2890 (0.0527)	
training:	Epoch: [32][160/408]	Loss 0.0097 (0.0524)	
training:	Epoch: [32][161/408]	Loss 0.0565 (0.0524)	
training:	Epoch: [32][162/408]	Loss 0.0504 (0.0524)	
training:	Epoch: [32][163/408]	Loss 0.0094 (0.0521)	
training:	Epoch: [32][164/408]	Loss 0.0087 (0.0519)	
training:	Epoch: [32][165/408]	Loss 0.0089 (0.0516)	
training:	Epoch: [32][166/408]	Loss 0.0080 (0.0513)	
training:	Epoch: [32][167/408]	Loss 0.0092 (0.0511)	
training:	Epoch: [32][168/408]	Loss 0.0083 (0.0508)	
training:	Epoch: [32][169/408]	Loss 0.0085 (0.0506)	
training:	Epoch: [32][170/408]	Loss 0.0083 (0.0503)	
training:	Epoch: [32][171/408]	Loss 0.2119 (0.0513)	
training:	Epoch: [32][172/408]	Loss 0.0104 (0.0510)	
training:	Epoch: [32][173/408]	Loss 0.0087 (0.0508)	
training:	Epoch: [32][174/408]	Loss 0.0097 (0.0506)	
training:	Epoch: [32][175/408]	Loss 0.0095 (0.0503)	
training:	Epoch: [32][176/408]	Loss 0.2622 (0.0515)	
training:	Epoch: [32][177/408]	Loss 0.0083 (0.0513)	
training:	Epoch: [32][178/408]	Loss 0.0153 (0.0511)	
training:	Epoch: [32][179/408]	Loss 0.0070 (0.0508)	
training:	Epoch: [32][180/408]	Loss 0.0079 (0.0506)	
training:	Epoch: [32][181/408]	Loss 0.0101 (0.0504)	
training:	Epoch: [32][182/408]	Loss 0.2766 (0.0516)	
training:	Epoch: [32][183/408]	Loss 0.0248 (0.0515)	
training:	Epoch: [32][184/408]	Loss 0.0108 (0.0513)	
training:	Epoch: [32][185/408]	Loss 0.0088 (0.0510)	
training:	Epoch: [32][186/408]	Loss 0.0090 (0.0508)	
training:	Epoch: [32][187/408]	Loss 0.0086 (0.0506)	
training:	Epoch: [32][188/408]	Loss 0.0086 (0.0504)	
training:	Epoch: [32][189/408]	Loss 0.0083 (0.0501)	
training:	Epoch: [32][190/408]	Loss 0.0090 (0.0499)	
training:	Epoch: [32][191/408]	Loss 0.0081 (0.0497)	
training:	Epoch: [32][192/408]	Loss 0.0101 (0.0495)	
training:	Epoch: [32][193/408]	Loss 0.0096 (0.0493)	
training:	Epoch: [32][194/408]	Loss 0.0093 (0.0491)	
training:	Epoch: [32][195/408]	Loss 0.0096 (0.0489)	
training:	Epoch: [32][196/408]	Loss 0.0093 (0.0487)	
training:	Epoch: [32][197/408]	Loss 0.0085 (0.0485)	
training:	Epoch: [32][198/408]	Loss 0.0089 (0.0483)	
training:	Epoch: [32][199/408]	Loss 0.0103 (0.0481)	
training:	Epoch: [32][200/408]	Loss 0.0088 (0.0479)	
training:	Epoch: [32][201/408]	Loss 0.0084 (0.0477)	
training:	Epoch: [32][202/408]	Loss 0.0094 (0.0475)	
training:	Epoch: [32][203/408]	Loss 0.0092 (0.0473)	
training:	Epoch: [32][204/408]	Loss 0.2740 (0.0484)	
training:	Epoch: [32][205/408]	Loss 0.0084 (0.0482)	
training:	Epoch: [32][206/408]	Loss 0.0234 (0.0481)	
training:	Epoch: [32][207/408]	Loss 0.0214 (0.0480)	
training:	Epoch: [32][208/408]	Loss 0.0090 (0.0478)	
training:	Epoch: [32][209/408]	Loss 0.0088 (0.0476)	
training:	Epoch: [32][210/408]	Loss 0.0085 (0.0474)	
training:	Epoch: [32][211/408]	Loss 0.0077 (0.0472)	
training:	Epoch: [32][212/408]	Loss 0.0088 (0.0470)	
training:	Epoch: [32][213/408]	Loss 0.3032 (0.0482)	
training:	Epoch: [32][214/408]	Loss 0.0085 (0.0481)	
training:	Epoch: [32][215/408]	Loss 0.2872 (0.0492)	
training:	Epoch: [32][216/408]	Loss 0.0334 (0.0491)	
training:	Epoch: [32][217/408]	Loss 0.0073 (0.0489)	
training:	Epoch: [32][218/408]	Loss 0.0091 (0.0487)	
training:	Epoch: [32][219/408]	Loss 0.0079 (0.0485)	
training:	Epoch: [32][220/408]	Loss 0.0087 (0.0484)	
training:	Epoch: [32][221/408]	Loss 0.0090 (0.0482)	
training:	Epoch: [32][222/408]	Loss 0.0092 (0.0480)	
training:	Epoch: [32][223/408]	Loss 0.0092 (0.0478)	
training:	Epoch: [32][224/408]	Loss 0.0160 (0.0477)	
training:	Epoch: [32][225/408]	Loss 0.0076 (0.0475)	
training:	Epoch: [32][226/408]	Loss 0.0089 (0.0473)	
training:	Epoch: [32][227/408]	Loss 0.0089 (0.0472)	
training:	Epoch: [32][228/408]	Loss 0.0098 (0.0470)	
training:	Epoch: [32][229/408]	Loss 0.3042 (0.0481)	
training:	Epoch: [32][230/408]	Loss 0.0651 (0.0482)	
training:	Epoch: [32][231/408]	Loss 0.0143 (0.0481)	
training:	Epoch: [32][232/408]	Loss 0.2895 (0.0491)	
training:	Epoch: [32][233/408]	Loss 0.0092 (0.0489)	
training:	Epoch: [32][234/408]	Loss 0.0095 (0.0488)	
training:	Epoch: [32][235/408]	Loss 0.0131 (0.0486)	
training:	Epoch: [32][236/408]	Loss 0.2660 (0.0495)	
training:	Epoch: [32][237/408]	Loss 0.0094 (0.0494)	
training:	Epoch: [32][238/408]	Loss 0.0087 (0.0492)	
training:	Epoch: [32][239/408]	Loss 0.0079 (0.0490)	
training:	Epoch: [32][240/408]	Loss 0.0090 (0.0488)	
training:	Epoch: [32][241/408]	Loss 0.0106 (0.0487)	
training:	Epoch: [32][242/408]	Loss 0.0123 (0.0485)	
training:	Epoch: [32][243/408]	Loss 0.0093 (0.0484)	
training:	Epoch: [32][244/408]	Loss 0.0091 (0.0482)	
training:	Epoch: [32][245/408]	Loss 0.0099 (0.0481)	
training:	Epoch: [32][246/408]	Loss 0.0772 (0.0482)	
training:	Epoch: [32][247/408]	Loss 0.0319 (0.0481)	
training:	Epoch: [32][248/408]	Loss 0.0959 (0.0483)	
training:	Epoch: [32][249/408]	Loss 0.0084 (0.0481)	
training:	Epoch: [32][250/408]	Loss 0.0116 (0.0480)	
training:	Epoch: [32][251/408]	Loss 0.0090 (0.0478)	
training:	Epoch: [32][252/408]	Loss 0.0131 (0.0477)	
training:	Epoch: [32][253/408]	Loss 0.0085 (0.0475)	
training:	Epoch: [32][254/408]	Loss 0.0128 (0.0474)	
training:	Epoch: [32][255/408]	Loss 0.2373 (0.0482)	
training:	Epoch: [32][256/408]	Loss 0.3137 (0.0492)	
training:	Epoch: [32][257/408]	Loss 0.0072 (0.0490)	
training:	Epoch: [32][258/408]	Loss 0.3036 (0.0500)	
training:	Epoch: [32][259/408]	Loss 0.0112 (0.0499)	
training:	Epoch: [32][260/408]	Loss 0.0382 (0.0498)	
training:	Epoch: [32][261/408]	Loss 0.0100 (0.0497)	
training:	Epoch: [32][262/408]	Loss 0.0094 (0.0495)	
training:	Epoch: [32][263/408]	Loss 0.0170 (0.0494)	
training:	Epoch: [32][264/408]	Loss 0.0093 (0.0492)	
training:	Epoch: [32][265/408]	Loss 0.0129 (0.0491)	
training:	Epoch: [32][266/408]	Loss 0.0129 (0.0490)	
training:	Epoch: [32][267/408]	Loss 0.0097 (0.0488)	
training:	Epoch: [32][268/408]	Loss 0.0239 (0.0487)	
training:	Epoch: [32][269/408]	Loss 0.2165 (0.0493)	
training:	Epoch: [32][270/408]	Loss 0.3125 (0.0503)	
training:	Epoch: [32][271/408]	Loss 0.0133 (0.0502)	
training:	Epoch: [32][272/408]	Loss 0.0116 (0.0500)	
training:	Epoch: [32][273/408]	Loss 0.0116 (0.0499)	
training:	Epoch: [32][274/408]	Loss 0.0100 (0.0498)	
training:	Epoch: [32][275/408]	Loss 0.0108 (0.0496)	
training:	Epoch: [32][276/408]	Loss 0.0088 (0.0495)	
training:	Epoch: [32][277/408]	Loss 0.0075 (0.0493)	
training:	Epoch: [32][278/408]	Loss 0.0083 (0.0492)	
training:	Epoch: [32][279/408]	Loss 0.0094 (0.0490)	
training:	Epoch: [32][280/408]	Loss 0.0096 (0.0489)	
training:	Epoch: [32][281/408]	Loss 0.0141 (0.0488)	
training:	Epoch: [32][282/408]	Loss 0.2945 (0.0496)	
training:	Epoch: [32][283/408]	Loss 0.0151 (0.0495)	
training:	Epoch: [32][284/408]	Loss 0.0086 (0.0494)	
training:	Epoch: [32][285/408]	Loss 0.0090 (0.0492)	
training:	Epoch: [32][286/408]	Loss 0.0093 (0.0491)	
training:	Epoch: [32][287/408]	Loss 0.0103 (0.0489)	
training:	Epoch: [32][288/408]	Loss 0.3049 (0.0498)	
training:	Epoch: [32][289/408]	Loss 0.0099 (0.0497)	
training:	Epoch: [32][290/408]	Loss 0.0081 (0.0496)	
training:	Epoch: [32][291/408]	Loss 0.0108 (0.0494)	
training:	Epoch: [32][292/408]	Loss 0.0110 (0.0493)	
training:	Epoch: [32][293/408]	Loss 0.6769 (0.0514)	
training:	Epoch: [32][294/408]	Loss 0.2810 (0.0522)	
training:	Epoch: [32][295/408]	Loss 0.0094 (0.0521)	
training:	Epoch: [32][296/408]	Loss 0.0097 (0.0519)	
training:	Epoch: [32][297/408]	Loss 0.3012 (0.0528)	
training:	Epoch: [32][298/408]	Loss 0.2813 (0.0535)	
training:	Epoch: [32][299/408]	Loss 0.0103 (0.0534)	
training:	Epoch: [32][300/408]	Loss 0.0104 (0.0532)	
training:	Epoch: [32][301/408]	Loss 0.0084 (0.0531)	
training:	Epoch: [32][302/408]	Loss 0.0097 (0.0530)	
training:	Epoch: [32][303/408]	Loss 0.0144 (0.0528)	
training:	Epoch: [32][304/408]	Loss 0.0098 (0.0527)	
training:	Epoch: [32][305/408]	Loss 0.2686 (0.0534)	
training:	Epoch: [32][306/408]	Loss 0.0120 (0.0533)	
training:	Epoch: [32][307/408]	Loss 0.2933 (0.0540)	
training:	Epoch: [32][308/408]	Loss 0.0244 (0.0539)	
training:	Epoch: [32][309/408]	Loss 0.0123 (0.0538)	
training:	Epoch: [32][310/408]	Loss 0.0088 (0.0537)	
training:	Epoch: [32][311/408]	Loss 0.0095 (0.0535)	
training:	Epoch: [32][312/408]	Loss 0.0195 (0.0534)	
training:	Epoch: [32][313/408]	Loss 0.2882 (0.0542)	
training:	Epoch: [32][314/408]	Loss 0.0099 (0.0540)	
training:	Epoch: [32][315/408]	Loss 0.0092 (0.0539)	
training:	Epoch: [32][316/408]	Loss 0.0103 (0.0537)	
training:	Epoch: [32][317/408]	Loss 0.0119 (0.0536)	
training:	Epoch: [32][318/408]	Loss 0.0098 (0.0535)	
training:	Epoch: [32][319/408]	Loss 0.0088 (0.0533)	
training:	Epoch: [32][320/408]	Loss 0.0090 (0.0532)	
training:	Epoch: [32][321/408]	Loss 0.0114 (0.0531)	
training:	Epoch: [32][322/408]	Loss 0.0113 (0.0529)	
training:	Epoch: [32][323/408]	Loss 0.0092 (0.0528)	
training:	Epoch: [32][324/408]	Loss 0.3962 (0.0539)	
training:	Epoch: [32][325/408]	Loss 0.0124 (0.0537)	
training:	Epoch: [32][326/408]	Loss 0.0101 (0.0536)	
training:	Epoch: [32][327/408]	Loss 0.0096 (0.0535)	
training:	Epoch: [32][328/408]	Loss 0.0095 (0.0533)	
training:	Epoch: [32][329/408]	Loss 0.0095 (0.0532)	
training:	Epoch: [32][330/408]	Loss 0.0100 (0.0531)	
training:	Epoch: [32][331/408]	Loss 0.0104 (0.0529)	
training:	Epoch: [32][332/408]	Loss 0.0095 (0.0528)	
training:	Epoch: [32][333/408]	Loss 0.0098 (0.0527)	
training:	Epoch: [32][334/408]	Loss 0.0089 (0.0525)	
training:	Epoch: [32][335/408]	Loss 0.0098 (0.0524)	
training:	Epoch: [32][336/408]	Loss 0.0092 (0.0523)	
training:	Epoch: [32][337/408]	Loss 0.0097 (0.0522)	
training:	Epoch: [32][338/408]	Loss 0.5419 (0.0536)	
training:	Epoch: [32][339/408]	Loss 0.0129 (0.0535)	
training:	Epoch: [32][340/408]	Loss 0.0107 (0.0534)	
training:	Epoch: [32][341/408]	Loss 0.0110 (0.0532)	
training:	Epoch: [32][342/408]	Loss 0.0110 (0.0531)	
training:	Epoch: [32][343/408]	Loss 0.0094 (0.0530)	
training:	Epoch: [32][344/408]	Loss 0.0093 (0.0529)	
training:	Epoch: [32][345/408]	Loss 0.0106 (0.0527)	
training:	Epoch: [32][346/408]	Loss 0.0103 (0.0526)	
training:	Epoch: [32][347/408]	Loss 0.0114 (0.0525)	
training:	Epoch: [32][348/408]	Loss 0.0103 (0.0524)	
training:	Epoch: [32][349/408]	Loss 0.3023 (0.0531)	
training:	Epoch: [32][350/408]	Loss 0.0720 (0.0531)	
training:	Epoch: [32][351/408]	Loss 0.0115 (0.0530)	
training:	Epoch: [32][352/408]	Loss 0.0108 (0.0529)	
training:	Epoch: [32][353/408]	Loss 0.0101 (0.0528)	
training:	Epoch: [32][354/408]	Loss 0.0159 (0.0527)	
training:	Epoch: [32][355/408]	Loss 0.0099 (0.0526)	
training:	Epoch: [32][356/408]	Loss 0.0105 (0.0524)	
training:	Epoch: [32][357/408]	Loss 0.0239 (0.0524)	
training:	Epoch: [32][358/408]	Loss 0.0092 (0.0522)	
training:	Epoch: [32][359/408]	Loss 0.0104 (0.0521)	
training:	Epoch: [32][360/408]	Loss 0.0104 (0.0520)	
training:	Epoch: [32][361/408]	Loss 0.0102 (0.0519)	
training:	Epoch: [32][362/408]	Loss 0.0106 (0.0518)	
training:	Epoch: [32][363/408]	Loss 0.0095 (0.0517)	
training:	Epoch: [32][364/408]	Loss 0.0088 (0.0515)	
training:	Epoch: [32][365/408]	Loss 0.0099 (0.0514)	
training:	Epoch: [32][366/408]	Loss 0.0083 (0.0513)	
training:	Epoch: [32][367/408]	Loss 0.0114 (0.0512)	
training:	Epoch: [32][368/408]	Loss 0.0104 (0.0511)	
training:	Epoch: [32][369/408]	Loss 0.0110 (0.0510)	
training:	Epoch: [32][370/408]	Loss 0.0105 (0.0509)	
training:	Epoch: [32][371/408]	Loss 0.0113 (0.0508)	
training:	Epoch: [32][372/408]	Loss 0.0092 (0.0507)	
training:	Epoch: [32][373/408]	Loss 0.5538 (0.0520)	
training:	Epoch: [32][374/408]	Loss 0.0088 (0.0519)	
training:	Epoch: [32][375/408]	Loss 0.0089 (0.0518)	
training:	Epoch: [32][376/408]	Loss 0.0164 (0.0517)	
training:	Epoch: [32][377/408]	Loss 0.0116 (0.0516)	
training:	Epoch: [32][378/408]	Loss 0.0158 (0.0515)	
training:	Epoch: [32][379/408]	Loss 0.2697 (0.0521)	
training:	Epoch: [32][380/408]	Loss 0.0092 (0.0519)	
training:	Epoch: [32][381/408]	Loss 0.3103 (0.0526)	
training:	Epoch: [32][382/408]	Loss 0.0091 (0.0525)	
training:	Epoch: [32][383/408]	Loss 0.3411 (0.0533)	
training:	Epoch: [32][384/408]	Loss 0.0105 (0.0531)	
training:	Epoch: [32][385/408]	Loss 0.0091 (0.0530)	
training:	Epoch: [32][386/408]	Loss 0.0092 (0.0529)	
training:	Epoch: [32][387/408]	Loss 0.0088 (0.0528)	
training:	Epoch: [32][388/408]	Loss 0.0103 (0.0527)	
training:	Epoch: [32][389/408]	Loss 0.0127 (0.0526)	
training:	Epoch: [32][390/408]	Loss 0.0825 (0.0527)	
training:	Epoch: [32][391/408]	Loss 0.0102 (0.0526)	
training:	Epoch: [32][392/408]	Loss 0.4537 (0.0536)	
training:	Epoch: [32][393/408]	Loss 0.0092 (0.0535)	
training:	Epoch: [32][394/408]	Loss 0.0093 (0.0534)	
training:	Epoch: [32][395/408]	Loss 0.0108 (0.0533)	
training:	Epoch: [32][396/408]	Loss 0.0104 (0.0531)	
training:	Epoch: [32][397/408]	Loss 0.0116 (0.0530)	
training:	Epoch: [32][398/408]	Loss 0.0126 (0.0529)	
training:	Epoch: [32][399/408]	Loss 0.0099 (0.0528)	
training:	Epoch: [32][400/408]	Loss 0.2387 (0.0533)	
training:	Epoch: [32][401/408]	Loss 0.0088 (0.0532)	
training:	Epoch: [32][402/408]	Loss 0.1266 (0.0534)	
training:	Epoch: [32][403/408]	Loss 0.0129 (0.0533)	
training:	Epoch: [32][404/408]	Loss 0.0091 (0.0532)	
training:	Epoch: [32][405/408]	Loss 0.0943 (0.0533)	
training:	Epoch: [32][406/408]	Loss 0.1515 (0.0535)	
training:	Epoch: [32][407/408]	Loss 0.0108 (0.0534)	
training:	Epoch: [32][408/408]	Loss 0.0206 (0.0533)	
Training:	 Loss: 0.0532

Training:	 ACC: 0.9920 0.9920 0.9927 0.9914
Validation:	 ACC: 0.7813 0.7838 0.8373 0.7253
Validation:	 Best_BACC: 0.8226 0.8250 0.8772 0.7679
Validation:	 Loss: 0.8709
Pretraining:	Epoch 33/200
----------
training:	Epoch: [33][1/408]	Loss 0.0094 (0.0094)	
training:	Epoch: [33][2/408]	Loss 0.0095 (0.0094)	
training:	Epoch: [33][3/408]	Loss 0.0126 (0.0105)	
training:	Epoch: [33][4/408]	Loss 0.2993 (0.0827)	
training:	Epoch: [33][5/408]	Loss 0.0089 (0.0679)	
training:	Epoch: [33][6/408]	Loss 0.1431 (0.0805)	
training:	Epoch: [33][7/408]	Loss 0.0091 (0.0703)	
training:	Epoch: [33][8/408]	Loss 0.2847 (0.0971)	
training:	Epoch: [33][9/408]	Loss 0.0115 (0.0876)	
training:	Epoch: [33][10/408]	Loss 0.0085 (0.0797)	
training:	Epoch: [33][11/408]	Loss 0.0109 (0.0734)	
training:	Epoch: [33][12/408]	Loss 0.0092 (0.0680)	
training:	Epoch: [33][13/408]	Loss 0.0118 (0.0637)	
training:	Epoch: [33][14/408]	Loss 0.0099 (0.0599)	
training:	Epoch: [33][15/408]	Loss 0.0096 (0.0565)	
training:	Epoch: [33][16/408]	Loss 0.0096 (0.0536)	
training:	Epoch: [33][17/408]	Loss 0.0094 (0.0510)	
training:	Epoch: [33][18/408]	Loss 0.0105 (0.0487)	
training:	Epoch: [33][19/408]	Loss 0.0081 (0.0466)	
training:	Epoch: [33][20/408]	Loss 0.0180 (0.0452)	
training:	Epoch: [33][21/408]	Loss 0.2694 (0.0559)	
training:	Epoch: [33][22/408]	Loss 0.4089 (0.0719)	
training:	Epoch: [33][23/408]	Loss 0.0088 (0.0692)	
training:	Epoch: [33][24/408]	Loss 0.0104 (0.0667)	
training:	Epoch: [33][25/408]	Loss 0.0088 (0.0644)	
training:	Epoch: [33][26/408]	Loss 0.2752 (0.0725)	
training:	Epoch: [33][27/408]	Loss 0.0095 (0.0702)	
training:	Epoch: [33][28/408]	Loss 0.0271 (0.0686)	
training:	Epoch: [33][29/408]	Loss 0.0121 (0.0667)	
training:	Epoch: [33][30/408]	Loss 0.0094 (0.0648)	
training:	Epoch: [33][31/408]	Loss 0.2973 (0.0723)	
training:	Epoch: [33][32/408]	Loss 0.0111 (0.0704)	
training:	Epoch: [33][33/408]	Loss 0.2270 (0.0751)	
training:	Epoch: [33][34/408]	Loss 0.0113 (0.0732)	
training:	Epoch: [33][35/408]	Loss 0.0132 (0.0715)	
training:	Epoch: [33][36/408]	Loss 0.0105 (0.0698)	
training:	Epoch: [33][37/408]	Loss 0.0337 (0.0688)	
training:	Epoch: [33][38/408]	Loss 0.0273 (0.0678)	
training:	Epoch: [33][39/408]	Loss 0.0110 (0.0663)	
training:	Epoch: [33][40/408]	Loss 0.0140 (0.0650)	
training:	Epoch: [33][41/408]	Loss 0.0109 (0.0637)	
training:	Epoch: [33][42/408]	Loss 0.0331 (0.0629)	
training:	Epoch: [33][43/408]	Loss 0.0101 (0.0617)	
training:	Epoch: [33][44/408]	Loss 0.0116 (0.0606)	
training:	Epoch: [33][45/408]	Loss 0.0094 (0.0594)	
training:	Epoch: [33][46/408]	Loss 0.0368 (0.0590)	
training:	Epoch: [33][47/408]	Loss 0.2643 (0.0633)	
training:	Epoch: [33][48/408]	Loss 0.0099 (0.0622)	
training:	Epoch: [33][49/408]	Loss 0.0089 (0.0611)	
training:	Epoch: [33][50/408]	Loss 0.0118 (0.0601)	
training:	Epoch: [33][51/408]	Loss 0.0126 (0.0592)	
training:	Epoch: [33][52/408]	Loss 0.2989 (0.0638)	
training:	Epoch: [33][53/408]	Loss 0.0103 (0.0628)	
training:	Epoch: [33][54/408]	Loss 0.0130 (0.0619)	
training:	Epoch: [33][55/408]	Loss 0.0122 (0.0610)	
training:	Epoch: [33][56/408]	Loss 0.0270 (0.0604)	
training:	Epoch: [33][57/408]	Loss 0.0091 (0.0595)	
training:	Epoch: [33][58/408]	Loss 0.0307 (0.0590)	
training:	Epoch: [33][59/408]	Loss 0.3801 (0.0644)	
training:	Epoch: [33][60/408]	Loss 0.0090 (0.0635)	
training:	Epoch: [33][61/408]	Loss 0.0103 (0.0626)	
training:	Epoch: [33][62/408]	Loss 0.0126 (0.0618)	
training:	Epoch: [33][63/408]	Loss 0.0112 (0.0610)	
training:	Epoch: [33][64/408]	Loss 0.1315 (0.0621)	
training:	Epoch: [33][65/408]	Loss 0.0109 (0.0613)	
training:	Epoch: [33][66/408]	Loss 0.0094 (0.0605)	
training:	Epoch: [33][67/408]	Loss 0.0110 (0.0598)	
training:	Epoch: [33][68/408]	Loss 0.0087 (0.0591)	
training:	Epoch: [33][69/408]	Loss 0.0195 (0.0585)	
training:	Epoch: [33][70/408]	Loss 0.0309 (0.0581)	
training:	Epoch: [33][71/408]	Loss 0.0305 (0.0577)	
training:	Epoch: [33][72/408]	Loss 0.0101 (0.0570)	
training:	Epoch: [33][73/408]	Loss 0.0112 (0.0564)	
training:	Epoch: [33][74/408]	Loss 0.0094 (0.0558)	
training:	Epoch: [33][75/408]	Loss 0.0097 (0.0552)	
training:	Epoch: [33][76/408]	Loss 0.0103 (0.0546)	
training:	Epoch: [33][77/408]	Loss 0.0108 (0.0540)	
training:	Epoch: [33][78/408]	Loss 0.0097 (0.0534)	
training:	Epoch: [33][79/408]	Loss 0.0091 (0.0529)	
training:	Epoch: [33][80/408]	Loss 0.2759 (0.0557)	
training:	Epoch: [33][81/408]	Loss 0.0107 (0.0551)	
training:	Epoch: [33][82/408]	Loss 0.0093 (0.0545)	
training:	Epoch: [33][83/408]	Loss 0.0150 (0.0541)	
training:	Epoch: [33][84/408]	Loss 0.0147 (0.0536)	
training:	Epoch: [33][85/408]	Loss 0.0131 (0.0531)	
training:	Epoch: [33][86/408]	Loss 0.0093 (0.0526)	
training:	Epoch: [33][87/408]	Loss 0.0115 (0.0521)	
training:	Epoch: [33][88/408]	Loss 0.5169 (0.0574)	
training:	Epoch: [33][89/408]	Loss 0.0091 (0.0569)	
training:	Epoch: [33][90/408]	Loss 0.0110 (0.0564)	
training:	Epoch: [33][91/408]	Loss 0.0096 (0.0559)	
training:	Epoch: [33][92/408]	Loss 0.0084 (0.0553)	
training:	Epoch: [33][93/408]	Loss 0.2663 (0.0576)	
training:	Epoch: [33][94/408]	Loss 0.0104 (0.0571)	
training:	Epoch: [33][95/408]	Loss 0.0105 (0.0566)	
training:	Epoch: [33][96/408]	Loss 0.0118 (0.0561)	
training:	Epoch: [33][97/408]	Loss 0.0210 (0.0558)	
training:	Epoch: [33][98/408]	Loss 0.0106 (0.0553)	
training:	Epoch: [33][99/408]	Loss 0.0085 (0.0548)	
training:	Epoch: [33][100/408]	Loss 0.0097 (0.0544)	
training:	Epoch: [33][101/408]	Loss 0.0104 (0.0540)	
training:	Epoch: [33][102/408]	Loss 0.0138 (0.0536)	
training:	Epoch: [33][103/408]	Loss 0.0099 (0.0531)	
training:	Epoch: [33][104/408]	Loss 0.2971 (0.0555)	
training:	Epoch: [33][105/408]	Loss 0.0107 (0.0551)	
training:	Epoch: [33][106/408]	Loss 0.0085 (0.0546)	
training:	Epoch: [33][107/408]	Loss 0.2744 (0.0567)	
training:	Epoch: [33][108/408]	Loss 0.0103 (0.0562)	
training:	Epoch: [33][109/408]	Loss 0.0096 (0.0558)	
training:	Epoch: [33][110/408]	Loss 0.0089 (0.0554)	
training:	Epoch: [33][111/408]	Loss 0.0090 (0.0550)	
training:	Epoch: [33][112/408]	Loss 0.0100 (0.0546)	
training:	Epoch: [33][113/408]	Loss 0.0099 (0.0542)	
training:	Epoch: [33][114/408]	Loss 0.0122 (0.0538)	
training:	Epoch: [33][115/408]	Loss 0.0089 (0.0534)	
training:	Epoch: [33][116/408]	Loss 0.2903 (0.0555)	
training:	Epoch: [33][117/408]	Loss 0.0104 (0.0551)	
training:	Epoch: [33][118/408]	Loss 0.0090 (0.0547)	
training:	Epoch: [33][119/408]	Loss 0.0076 (0.0543)	
training:	Epoch: [33][120/408]	Loss 0.0285 (0.0541)	
training:	Epoch: [33][121/408]	Loss 0.0098 (0.0537)	
training:	Epoch: [33][122/408]	Loss 0.3107 (0.0558)	
training:	Epoch: [33][123/408]	Loss 0.0101 (0.0554)	
training:	Epoch: [33][124/408]	Loss 0.0092 (0.0551)	
training:	Epoch: [33][125/408]	Loss 0.2861 (0.0569)	
training:	Epoch: [33][126/408]	Loss 0.0098 (0.0565)	
training:	Epoch: [33][127/408]	Loss 0.0088 (0.0562)	
training:	Epoch: [33][128/408]	Loss 0.0099 (0.0558)	
training:	Epoch: [33][129/408]	Loss 0.0193 (0.0555)	
training:	Epoch: [33][130/408]	Loss 0.0102 (0.0552)	
training:	Epoch: [33][131/408]	Loss 0.0107 (0.0548)	
training:	Epoch: [33][132/408]	Loss 0.0143 (0.0545)	
training:	Epoch: [33][133/408]	Loss 0.0095 (0.0542)	
training:	Epoch: [33][134/408]	Loss 0.0129 (0.0539)	
training:	Epoch: [33][135/408]	Loss 0.0111 (0.0536)	
training:	Epoch: [33][136/408]	Loss 0.0080 (0.0532)	
training:	Epoch: [33][137/408]	Loss 0.0115 (0.0529)	
training:	Epoch: [33][138/408]	Loss 0.0088 (0.0526)	
training:	Epoch: [33][139/408]	Loss 0.0098 (0.0523)	
training:	Epoch: [33][140/408]	Loss 0.0094 (0.0520)	
training:	Epoch: [33][141/408]	Loss 0.0125 (0.0517)	
training:	Epoch: [33][142/408]	Loss 0.0094 (0.0514)	
training:	Epoch: [33][143/408]	Loss 0.0088 (0.0511)	
training:	Epoch: [33][144/408]	Loss 0.0075 (0.0508)	
training:	Epoch: [33][145/408]	Loss 0.0085 (0.0505)	
training:	Epoch: [33][146/408]	Loss 0.0146 (0.0503)	
training:	Epoch: [33][147/408]	Loss 0.0099 (0.0500)	
training:	Epoch: [33][148/408]	Loss 0.0088 (0.0497)	
training:	Epoch: [33][149/408]	Loss 0.0152 (0.0495)	
training:	Epoch: [33][150/408]	Loss 0.0093 (0.0492)	
training:	Epoch: [33][151/408]	Loss 0.0103 (0.0490)	
training:	Epoch: [33][152/408]	Loss 0.2850 (0.0505)	
training:	Epoch: [33][153/408]	Loss 0.2713 (0.0520)	
training:	Epoch: [33][154/408]	Loss 0.2210 (0.0531)	
training:	Epoch: [33][155/408]	Loss 0.0097 (0.0528)	
training:	Epoch: [33][156/408]	Loss 0.0096 (0.0525)	
training:	Epoch: [33][157/408]	Loss 0.0110 (0.0522)	
training:	Epoch: [33][158/408]	Loss 0.2986 (0.0538)	
training:	Epoch: [33][159/408]	Loss 0.0179 (0.0536)	
training:	Epoch: [33][160/408]	Loss 0.0095 (0.0533)	
training:	Epoch: [33][161/408]	Loss 0.0085 (0.0530)	
training:	Epoch: [33][162/408]	Loss 0.0087 (0.0527)	
training:	Epoch: [33][163/408]	Loss 0.0250 (0.0526)	
training:	Epoch: [33][164/408]	Loss 0.0102 (0.0523)	
training:	Epoch: [33][165/408]	Loss 0.0085 (0.0521)	
training:	Epoch: [33][166/408]	Loss 0.0085 (0.0518)	
training:	Epoch: [33][167/408]	Loss 0.0658 (0.0519)	
training:	Epoch: [33][168/408]	Loss 0.0123 (0.0516)	
training:	Epoch: [33][169/408]	Loss 0.0094 (0.0514)	
training:	Epoch: [33][170/408]	Loss 0.0097 (0.0511)	
training:	Epoch: [33][171/408]	Loss 0.3008 (0.0526)	
training:	Epoch: [33][172/408]	Loss 0.0087 (0.0523)	
training:	Epoch: [33][173/408]	Loss 0.3085 (0.0538)	
training:	Epoch: [33][174/408]	Loss 0.0101 (0.0536)	
training:	Epoch: [33][175/408]	Loss 0.0080 (0.0533)	
training:	Epoch: [33][176/408]	Loss 0.1116 (0.0536)	
training:	Epoch: [33][177/408]	Loss 0.0090 (0.0534)	
training:	Epoch: [33][178/408]	Loss 0.0135 (0.0532)	
training:	Epoch: [33][179/408]	Loss 0.0084 (0.0529)	
training:	Epoch: [33][180/408]	Loss 0.0109 (0.0527)	
training:	Epoch: [33][181/408]	Loss 0.0130 (0.0525)	
training:	Epoch: [33][182/408]	Loss 0.0098 (0.0522)	
training:	Epoch: [33][183/408]	Loss 0.0113 (0.0520)	
training:	Epoch: [33][184/408]	Loss 0.0123 (0.0518)	
training:	Epoch: [33][185/408]	Loss 0.0106 (0.0516)	
training:	Epoch: [33][186/408]	Loss 0.0087 (0.0513)	
training:	Epoch: [33][187/408]	Loss 0.0091 (0.0511)	
training:	Epoch: [33][188/408]	Loss 0.0094 (0.0509)	
training:	Epoch: [33][189/408]	Loss 0.0083 (0.0507)	
training:	Epoch: [33][190/408]	Loss 0.0111 (0.0505)	
training:	Epoch: [33][191/408]	Loss 0.0093 (0.0502)	
training:	Epoch: [33][192/408]	Loss 0.0099 (0.0500)	
training:	Epoch: [33][193/408]	Loss 0.2811 (0.0512)	
training:	Epoch: [33][194/408]	Loss 0.0092 (0.0510)	
training:	Epoch: [33][195/408]	Loss 0.3261 (0.0524)	
training:	Epoch: [33][196/408]	Loss 0.0110 (0.0522)	
training:	Epoch: [33][197/408]	Loss 0.0103 (0.0520)	
training:	Epoch: [33][198/408]	Loss 0.0101 (0.0518)	
training:	Epoch: [33][199/408]	Loss 0.0236 (0.0516)	
training:	Epoch: [33][200/408]	Loss 0.0089 (0.0514)	
training:	Epoch: [33][201/408]	Loss 0.0089 (0.0512)	
training:	Epoch: [33][202/408]	Loss 0.0107 (0.0510)	
training:	Epoch: [33][203/408]	Loss 0.0099 (0.0508)	
training:	Epoch: [33][204/408]	Loss 0.0101 (0.0506)	
training:	Epoch: [33][205/408]	Loss 0.0089 (0.0504)	
training:	Epoch: [33][206/408]	Loss 0.0105 (0.0502)	
training:	Epoch: [33][207/408]	Loss 0.0094 (0.0500)	
training:	Epoch: [33][208/408]	Loss 0.0080 (0.0498)	
training:	Epoch: [33][209/408]	Loss 0.0223 (0.0497)	
training:	Epoch: [33][210/408]	Loss 0.0343 (0.0496)	
training:	Epoch: [33][211/408]	Loss 0.3263 (0.0509)	
training:	Epoch: [33][212/408]	Loss 0.0088 (0.0507)	
training:	Epoch: [33][213/408]	Loss 0.0102 (0.0505)	
training:	Epoch: [33][214/408]	Loss 0.0100 (0.0504)	
training:	Epoch: [33][215/408]	Loss 0.0102 (0.0502)	
training:	Epoch: [33][216/408]	Loss 0.0082 (0.0500)	
training:	Epoch: [33][217/408]	Loss 0.0077 (0.0498)	
training:	Epoch: [33][218/408]	Loss 0.0084 (0.0496)	
training:	Epoch: [33][219/408]	Loss 0.0101 (0.0494)	
training:	Epoch: [33][220/408]	Loss 0.0137 (0.0492)	
training:	Epoch: [33][221/408]	Loss 0.0099 (0.0491)	
training:	Epoch: [33][222/408]	Loss 0.0081 (0.0489)	
training:	Epoch: [33][223/408]	Loss 0.0091 (0.0487)	
training:	Epoch: [33][224/408]	Loss 0.0102 (0.0485)	
training:	Epoch: [33][225/408]	Loss 0.0083 (0.0484)	
training:	Epoch: [33][226/408]	Loss 0.2200 (0.0491)	
training:	Epoch: [33][227/408]	Loss 0.0104 (0.0489)	
training:	Epoch: [33][228/408]	Loss 0.0090 (0.0488)	
training:	Epoch: [33][229/408]	Loss 0.0095 (0.0486)	
training:	Epoch: [33][230/408]	Loss 0.0075 (0.0484)	
training:	Epoch: [33][231/408]	Loss 0.0111 (0.0483)	
training:	Epoch: [33][232/408]	Loss 0.0076 (0.0481)	
training:	Epoch: [33][233/408]	Loss 0.0101 (0.0479)	
training:	Epoch: [33][234/408]	Loss 0.0103 (0.0478)	
training:	Epoch: [33][235/408]	Loss 0.0112 (0.0476)	
training:	Epoch: [33][236/408]	Loss 0.0085 (0.0474)	
training:	Epoch: [33][237/408]	Loss 0.0108 (0.0473)	
training:	Epoch: [33][238/408]	Loss 0.0085 (0.0471)	
training:	Epoch: [33][239/408]	Loss 0.0115 (0.0470)	
training:	Epoch: [33][240/408]	Loss 0.0108 (0.0468)	
training:	Epoch: [33][241/408]	Loss 0.2732 (0.0478)	
training:	Epoch: [33][242/408]	Loss 0.0074 (0.0476)	
training:	Epoch: [33][243/408]	Loss 0.0112 (0.0474)	
training:	Epoch: [33][244/408]	Loss 0.2594 (0.0483)	
training:	Epoch: [33][245/408]	Loss 0.3211 (0.0494)	
training:	Epoch: [33][246/408]	Loss 0.0089 (0.0493)	
training:	Epoch: [33][247/408]	Loss 0.0084 (0.0491)	
training:	Epoch: [33][248/408]	Loss 0.0082 (0.0489)	
training:	Epoch: [33][249/408]	Loss 0.0100 (0.0488)	
training:	Epoch: [33][250/408]	Loss 0.0083 (0.0486)	
training:	Epoch: [33][251/408]	Loss 0.0086 (0.0484)	
training:	Epoch: [33][252/408]	Loss 0.0095 (0.0483)	
training:	Epoch: [33][253/408]	Loss 0.0092 (0.0481)	
training:	Epoch: [33][254/408]	Loss 0.0097 (0.0480)	
training:	Epoch: [33][255/408]	Loss 0.2739 (0.0489)	
training:	Epoch: [33][256/408]	Loss 0.0093 (0.0487)	
training:	Epoch: [33][257/408]	Loss 0.0080 (0.0486)	
training:	Epoch: [33][258/408]	Loss 0.0087 (0.0484)	
training:	Epoch: [33][259/408]	Loss 0.0101 (0.0483)	
training:	Epoch: [33][260/408]	Loss 0.0079 (0.0481)	
training:	Epoch: [33][261/408]	Loss 0.0092 (0.0480)	
training:	Epoch: [33][262/408]	Loss 0.0081 (0.0478)	
training:	Epoch: [33][263/408]	Loss 0.0085 (0.0477)	
training:	Epoch: [33][264/408]	Loss 0.0075 (0.0475)	
training:	Epoch: [33][265/408]	Loss 0.0105 (0.0474)	
training:	Epoch: [33][266/408]	Loss 0.0086 (0.0472)	
training:	Epoch: [33][267/408]	Loss 0.0092 (0.0471)	
training:	Epoch: [33][268/408]	Loss 0.0080 (0.0469)	
training:	Epoch: [33][269/408]	Loss 0.0104 (0.0468)	
training:	Epoch: [33][270/408]	Loss 0.0094 (0.0467)	
training:	Epoch: [33][271/408]	Loss 0.3076 (0.0476)	
training:	Epoch: [33][272/408]	Loss 0.0073 (0.0475)	
training:	Epoch: [33][273/408]	Loss 0.0085 (0.0473)	
training:	Epoch: [33][274/408]	Loss 0.0088 (0.0472)	
training:	Epoch: [33][275/408]	Loss 0.0085 (0.0470)	
training:	Epoch: [33][276/408]	Loss 0.0101 (0.0469)	
training:	Epoch: [33][277/408]	Loss 0.0156 (0.0468)	
training:	Epoch: [33][278/408]	Loss 0.0083 (0.0467)	
training:	Epoch: [33][279/408]	Loss 0.0082 (0.0465)	
training:	Epoch: [33][280/408]	Loss 0.0077 (0.0464)	
training:	Epoch: [33][281/408]	Loss 0.0096 (0.0463)	
training:	Epoch: [33][282/408]	Loss 0.2981 (0.0471)	
training:	Epoch: [33][283/408]	Loss 0.0083 (0.0470)	
training:	Epoch: [33][284/408]	Loss 0.0082 (0.0469)	
training:	Epoch: [33][285/408]	Loss 0.0078 (0.0467)	
training:	Epoch: [33][286/408]	Loss 0.0084 (0.0466)	
training:	Epoch: [33][287/408]	Loss 0.0075 (0.0465)	
training:	Epoch: [33][288/408]	Loss 0.0080 (0.0463)	
training:	Epoch: [33][289/408]	Loss 0.0074 (0.0462)	
training:	Epoch: [33][290/408]	Loss 0.0091 (0.0461)	
training:	Epoch: [33][291/408]	Loss 0.0099 (0.0459)	
training:	Epoch: [33][292/408]	Loss 0.0083 (0.0458)	
training:	Epoch: [33][293/408]	Loss 0.0079 (0.0457)	
training:	Epoch: [33][294/408]	Loss 0.0081 (0.0456)	
training:	Epoch: [33][295/408]	Loss 0.0100 (0.0454)	
training:	Epoch: [33][296/408]	Loss 0.0098 (0.0453)	
training:	Epoch: [33][297/408]	Loss 0.0090 (0.0452)	
training:	Epoch: [33][298/408]	Loss 0.0088 (0.0451)	
training:	Epoch: [33][299/408]	Loss 0.3035 (0.0459)	
training:	Epoch: [33][300/408]	Loss 0.0077 (0.0458)	
training:	Epoch: [33][301/408]	Loss 0.0118 (0.0457)	
training:	Epoch: [33][302/408]	Loss 0.0082 (0.0456)	
training:	Epoch: [33][303/408]	Loss 0.0080 (0.0454)	
training:	Epoch: [33][304/408]	Loss 0.0084 (0.0453)	
training:	Epoch: [33][305/408]	Loss 0.0087 (0.0452)	
training:	Epoch: [33][306/408]	Loss 0.0093 (0.0451)	
training:	Epoch: [33][307/408]	Loss 0.0088 (0.0450)	
training:	Epoch: [33][308/408]	Loss 0.0091 (0.0449)	
training:	Epoch: [33][309/408]	Loss 0.0075 (0.0447)	
training:	Epoch: [33][310/408]	Loss 0.3186 (0.0456)	
training:	Epoch: [33][311/408]	Loss 0.0324 (0.0456)	
training:	Epoch: [33][312/408]	Loss 0.0086 (0.0455)	
training:	Epoch: [33][313/408]	Loss 0.0087 (0.0453)	
training:	Epoch: [33][314/408]	Loss 0.2785 (0.0461)	
training:	Epoch: [33][315/408]	Loss 0.0094 (0.0460)	
training:	Epoch: [33][316/408]	Loss 0.0079 (0.0458)	
training:	Epoch: [33][317/408]	Loss 0.0080 (0.0457)	
training:	Epoch: [33][318/408]	Loss 0.0080 (0.0456)	
training:	Epoch: [33][319/408]	Loss 0.0097 (0.0455)	
training:	Epoch: [33][320/408]	Loss 0.0089 (0.0454)	
training:	Epoch: [33][321/408]	Loss 0.0095 (0.0453)	
training:	Epoch: [33][322/408]	Loss 0.2974 (0.0460)	
training:	Epoch: [33][323/408]	Loss 0.0078 (0.0459)	
training:	Epoch: [33][324/408]	Loss 0.2953 (0.0467)	
training:	Epoch: [33][325/408]	Loss 0.0343 (0.0467)	
training:	Epoch: [33][326/408]	Loss 0.3007 (0.0474)	
training:	Epoch: [33][327/408]	Loss 0.0082 (0.0473)	
training:	Epoch: [33][328/408]	Loss 0.0122 (0.0472)	
training:	Epoch: [33][329/408]	Loss 0.0092 (0.0471)	
training:	Epoch: [33][330/408]	Loss 0.0076 (0.0470)	
training:	Epoch: [33][331/408]	Loss 0.0081 (0.0469)	
training:	Epoch: [33][332/408]	Loss 0.0093 (0.0467)	
training:	Epoch: [33][333/408]	Loss 0.0088 (0.0466)	
training:	Epoch: [33][334/408]	Loss 0.0087 (0.0465)	
training:	Epoch: [33][335/408]	Loss 0.0080 (0.0464)	
training:	Epoch: [33][336/408]	Loss 0.0084 (0.0463)	
training:	Epoch: [33][337/408]	Loss 0.0093 (0.0462)	
training:	Epoch: [33][338/408]	Loss 0.0096 (0.0461)	
training:	Epoch: [33][339/408]	Loss 0.0086 (0.0460)	
training:	Epoch: [33][340/408]	Loss 0.2951 (0.0467)	
training:	Epoch: [33][341/408]	Loss 0.0090 (0.0466)	
training:	Epoch: [33][342/408]	Loss 0.0108 (0.0465)	
training:	Epoch: [33][343/408]	Loss 0.0136 (0.0464)	
training:	Epoch: [33][344/408]	Loss 0.0104 (0.0463)	
training:	Epoch: [33][345/408]	Loss 0.0081 (0.0462)	
training:	Epoch: [33][346/408]	Loss 0.0085 (0.0461)	
training:	Epoch: [33][347/408]	Loss 0.0094 (0.0460)	
training:	Epoch: [33][348/408]	Loss 0.0100 (0.0459)	
training:	Epoch: [33][349/408]	Loss 0.2909 (0.0466)	
training:	Epoch: [33][350/408]	Loss 0.0119 (0.0465)	
training:	Epoch: [33][351/408]	Loss 0.0072 (0.0463)	
training:	Epoch: [33][352/408]	Loss 0.0707 (0.0464)	
training:	Epoch: [33][353/408]	Loss 0.2697 (0.0470)	
training:	Epoch: [33][354/408]	Loss 0.0121 (0.0469)	
training:	Epoch: [33][355/408]	Loss 0.0079 (0.0468)	
training:	Epoch: [33][356/408]	Loss 0.0074 (0.0467)	
training:	Epoch: [33][357/408]	Loss 0.0110 (0.0466)	
training:	Epoch: [33][358/408]	Loss 0.0099 (0.0465)	
training:	Epoch: [33][359/408]	Loss 0.0083 (0.0464)	
training:	Epoch: [33][360/408]	Loss 0.0086 (0.0463)	
training:	Epoch: [33][361/408]	Loss 0.0165 (0.0462)	
training:	Epoch: [33][362/408]	Loss 0.2798 (0.0469)	
training:	Epoch: [33][363/408]	Loss 0.0089 (0.0468)	
training:	Epoch: [33][364/408]	Loss 0.0080 (0.0467)	
training:	Epoch: [33][365/408]	Loss 0.0085 (0.0466)	
training:	Epoch: [33][366/408]	Loss 0.0293 (0.0465)	
training:	Epoch: [33][367/408]	Loss 0.2844 (0.0472)	
training:	Epoch: [33][368/408]	Loss 0.0093 (0.0471)	
training:	Epoch: [33][369/408]	Loss 0.3007 (0.0477)	
training:	Epoch: [33][370/408]	Loss 0.0459 (0.0477)	
training:	Epoch: [33][371/408]	Loss 0.0092 (0.0476)	
training:	Epoch: [33][372/408]	Loss 0.0071 (0.0475)	
training:	Epoch: [33][373/408]	Loss 0.2772 (0.0481)	
training:	Epoch: [33][374/408]	Loss 0.0084 (0.0480)	
training:	Epoch: [33][375/408]	Loss 0.0376 (0.0480)	
training:	Epoch: [33][376/408]	Loss 0.0291 (0.0480)	
training:	Epoch: [33][377/408]	Loss 0.0094 (0.0479)	
training:	Epoch: [33][378/408]	Loss 0.0095 (0.0478)	
training:	Epoch: [33][379/408]	Loss 0.0085 (0.0477)	
training:	Epoch: [33][380/408]	Loss 0.0095 (0.0476)	
training:	Epoch: [33][381/408]	Loss 0.0094 (0.0475)	
training:	Epoch: [33][382/408]	Loss 0.0097 (0.0474)	
training:	Epoch: [33][383/408]	Loss 0.0147 (0.0473)	
training:	Epoch: [33][384/408]	Loss 0.0077 (0.0472)	
training:	Epoch: [33][385/408]	Loss 0.0087 (0.0471)	
training:	Epoch: [33][386/408]	Loss 0.0099 (0.0470)	
training:	Epoch: [33][387/408]	Loss 0.0091 (0.0469)	
training:	Epoch: [33][388/408]	Loss 0.0085 (0.0468)	
training:	Epoch: [33][389/408]	Loss 0.0109 (0.0467)	
training:	Epoch: [33][390/408]	Loss 0.0092 (0.0466)	
training:	Epoch: [33][391/408]	Loss 0.0647 (0.0466)	
training:	Epoch: [33][392/408]	Loss 0.0205 (0.0466)	
training:	Epoch: [33][393/408]	Loss 0.0369 (0.0465)	
training:	Epoch: [33][394/408]	Loss 0.0083 (0.0464)	
training:	Epoch: [33][395/408]	Loss 0.0185 (0.0464)	
training:	Epoch: [33][396/408]	Loss 0.2993 (0.0470)	
training:	Epoch: [33][397/408]	Loss 0.0095 (0.0469)	
training:	Epoch: [33][398/408]	Loss 0.0095 (0.0468)	
training:	Epoch: [33][399/408]	Loss 0.0108 (0.0467)	
training:	Epoch: [33][400/408]	Loss 0.0082 (0.0466)	
training:	Epoch: [33][401/408]	Loss 0.0078 (0.0465)	
training:	Epoch: [33][402/408]	Loss 0.0088 (0.0464)	
training:	Epoch: [33][403/408]	Loss 0.0078 (0.0463)	
training:	Epoch: [33][404/408]	Loss 0.0093 (0.0463)	
training:	Epoch: [33][405/408]	Loss 0.0077 (0.0462)	
training:	Epoch: [33][406/408]	Loss 0.0107 (0.0461)	
training:	Epoch: [33][407/408]	Loss 0.0113 (0.0460)	
training:	Epoch: [33][408/408]	Loss 0.0083 (0.0459)	
Training:	 Loss: 0.0458

Training:	 ACC: 0.9919 0.9919 0.9927 0.9911
Validation:	 ACC: 0.7838 0.7854 0.8209 0.7466
Validation:	 Best_BACC: 0.8226 0.8250 0.8772 0.7679
Validation:	 Loss: 0.9212
Pretraining:	Epoch 34/200
----------
training:	Epoch: [34][1/408]	Loss 0.0082 (0.0082)	
training:	Epoch: [34][2/408]	Loss 0.0094 (0.0088)	
training:	Epoch: [34][3/408]	Loss 0.0088 (0.0088)	
training:	Epoch: [34][4/408]	Loss 0.0080 (0.0086)	
training:	Epoch: [34][5/408]	Loss 0.0118 (0.0092)	
training:	Epoch: [34][6/408]	Loss 0.0092 (0.0092)	
training:	Epoch: [34][7/408]	Loss 0.0088 (0.0092)	
training:	Epoch: [34][8/408]	Loss 0.0133 (0.0097)	
training:	Epoch: [34][9/408]	Loss 0.0076 (0.0095)	
training:	Epoch: [34][10/408]	Loss 0.1010 (0.0186)	
training:	Epoch: [34][11/408]	Loss 0.2504 (0.0397)	
training:	Epoch: [34][12/408]	Loss 0.0096 (0.0372)	
training:	Epoch: [34][13/408]	Loss 0.0100 (0.0351)	
training:	Epoch: [34][14/408]	Loss 0.0088 (0.0332)	
training:	Epoch: [34][15/408]	Loss 0.0085 (0.0316)	
training:	Epoch: [34][16/408]	Loss 0.0101 (0.0302)	
training:	Epoch: [34][17/408]	Loss 0.0082 (0.0289)	
training:	Epoch: [34][18/408]	Loss 0.0082 (0.0278)	
training:	Epoch: [34][19/408]	Loss 0.0104 (0.0269)	
training:	Epoch: [34][20/408]	Loss 0.0086 (0.0259)	
training:	Epoch: [34][21/408]	Loss 0.1079 (0.0298)	
training:	Epoch: [34][22/408]	Loss 0.0080 (0.0288)	
training:	Epoch: [34][23/408]	Loss 0.0082 (0.0280)	
training:	Epoch: [34][24/408]	Loss 0.0085 (0.0271)	
training:	Epoch: [34][25/408]	Loss 0.0082 (0.0264)	
training:	Epoch: [34][26/408]	Loss 0.0090 (0.0257)	
training:	Epoch: [34][27/408]	Loss 0.0092 (0.0251)	
training:	Epoch: [34][28/408]	Loss 0.0069 (0.0245)	
training:	Epoch: [34][29/408]	Loss 0.0089 (0.0239)	
training:	Epoch: [34][30/408]	Loss 0.0085 (0.0234)	
training:	Epoch: [34][31/408]	Loss 0.0099 (0.0230)	
training:	Epoch: [34][32/408]	Loss 0.0095 (0.0225)	
training:	Epoch: [34][33/408]	Loss 0.0083 (0.0221)	
training:	Epoch: [34][34/408]	Loss 0.0618 (0.0233)	
training:	Epoch: [34][35/408]	Loss 0.0077 (0.0228)	
training:	Epoch: [34][36/408]	Loss 0.0113 (0.0225)	
training:	Epoch: [34][37/408]	Loss 0.0106 (0.0222)	
training:	Epoch: [34][38/408]	Loss 0.0081 (0.0218)	
training:	Epoch: [34][39/408]	Loss 0.0086 (0.0215)	
training:	Epoch: [34][40/408]	Loss 0.0086 (0.0212)	
training:	Epoch: [34][41/408]	Loss 0.0108 (0.0209)	
training:	Epoch: [34][42/408]	Loss 0.3022 (0.0276)	
training:	Epoch: [34][43/408]	Loss 0.0080 (0.0272)	
training:	Epoch: [34][44/408]	Loss 0.0071 (0.0267)	
training:	Epoch: [34][45/408]	Loss 0.0076 (0.0263)	
training:	Epoch: [34][46/408]	Loss 0.0077 (0.0259)	
training:	Epoch: [34][47/408]	Loss 0.0083 (0.0255)	
training:	Epoch: [34][48/408]	Loss 0.0083 (0.0251)	
training:	Epoch: [34][49/408]	Loss 0.0095 (0.0248)	
training:	Epoch: [34][50/408]	Loss 0.0092 (0.0245)	
training:	Epoch: [34][51/408]	Loss 0.3108 (0.0301)	
training:	Epoch: [34][52/408]	Loss 0.0084 (0.0297)	
training:	Epoch: [34][53/408]	Loss 0.0072 (0.0293)	
training:	Epoch: [34][54/408]	Loss 0.0072 (0.0289)	
training:	Epoch: [34][55/408]	Loss 0.0084 (0.0285)	
training:	Epoch: [34][56/408]	Loss 0.0087 (0.0281)	
training:	Epoch: [34][57/408]	Loss 0.0094 (0.0278)	
training:	Epoch: [34][58/408]	Loss 0.0105 (0.0275)	
training:	Epoch: [34][59/408]	Loss 0.0076 (0.0272)	
training:	Epoch: [34][60/408]	Loss 0.0095 (0.0269)	
training:	Epoch: [34][61/408]	Loss 0.0083 (0.0266)	
training:	Epoch: [34][62/408]	Loss 0.0096 (0.0263)	
training:	Epoch: [34][63/408]	Loss 0.0079 (0.0260)	
training:	Epoch: [34][64/408]	Loss 0.0073 (0.0257)	
training:	Epoch: [34][65/408]	Loss 0.2842 (0.0297)	
training:	Epoch: [34][66/408]	Loss 0.0082 (0.0294)	
training:	Epoch: [34][67/408]	Loss 0.0077 (0.0290)	
training:	Epoch: [34][68/408]	Loss 0.0074 (0.0287)	
training:	Epoch: [34][69/408]	Loss 0.0083 (0.0284)	
training:	Epoch: [34][70/408]	Loss 0.0087 (0.0282)	
training:	Epoch: [34][71/408]	Loss 0.0080 (0.0279)	
training:	Epoch: [34][72/408]	Loss 0.0088 (0.0276)	
training:	Epoch: [34][73/408]	Loss 0.0083 (0.0273)	
training:	Epoch: [34][74/408]	Loss 0.0082 (0.0271)	
training:	Epoch: [34][75/408]	Loss 0.0078 (0.0268)	
training:	Epoch: [34][76/408]	Loss 0.0077 (0.0266)	
training:	Epoch: [34][77/408]	Loss 0.3148 (0.0303)	
training:	Epoch: [34][78/408]	Loss 0.0080 (0.0300)	
training:	Epoch: [34][79/408]	Loss 0.0096 (0.0298)	
training:	Epoch: [34][80/408]	Loss 0.0077 (0.0295)	
training:	Epoch: [34][81/408]	Loss 0.0092 (0.0292)	
training:	Epoch: [34][82/408]	Loss 0.0088 (0.0290)	
training:	Epoch: [34][83/408]	Loss 0.0082 (0.0287)	
training:	Epoch: [34][84/408]	Loss 0.0080 (0.0285)	
training:	Epoch: [34][85/408]	Loss 0.0082 (0.0283)	
training:	Epoch: [34][86/408]	Loss 0.0117 (0.0281)	
training:	Epoch: [34][87/408]	Loss 0.0072 (0.0278)	
training:	Epoch: [34][88/408]	Loss 0.0079 (0.0276)	
training:	Epoch: [34][89/408]	Loss 0.0084 (0.0274)	
training:	Epoch: [34][90/408]	Loss 0.0084 (0.0272)	
training:	Epoch: [34][91/408]	Loss 0.0079 (0.0270)	
training:	Epoch: [34][92/408]	Loss 0.0066 (0.0267)	
training:	Epoch: [34][93/408]	Loss 0.0099 (0.0266)	
training:	Epoch: [34][94/408]	Loss 0.2498 (0.0289)	
training:	Epoch: [34][95/408]	Loss 0.0072 (0.0287)	
training:	Epoch: [34][96/408]	Loss 0.0075 (0.0285)	
training:	Epoch: [34][97/408]	Loss 0.0089 (0.0283)	
training:	Epoch: [34][98/408]	Loss 0.0079 (0.0281)	
training:	Epoch: [34][99/408]	Loss 0.3058 (0.0309)	
training:	Epoch: [34][100/408]	Loss 0.0072 (0.0306)	
training:	Epoch: [34][101/408]	Loss 0.0081 (0.0304)	
training:	Epoch: [34][102/408]	Loss 0.0077 (0.0302)	
training:	Epoch: [34][103/408]	Loss 0.0076 (0.0300)	
training:	Epoch: [34][104/408]	Loss 0.0065 (0.0298)	
training:	Epoch: [34][105/408]	Loss 0.0078 (0.0295)	
training:	Epoch: [34][106/408]	Loss 0.0070 (0.0293)	
training:	Epoch: [34][107/408]	Loss 0.0090 (0.0291)	
training:	Epoch: [34][108/408]	Loss 0.0085 (0.0290)	
training:	Epoch: [34][109/408]	Loss 0.0070 (0.0287)	
training:	Epoch: [34][110/408]	Loss 0.0097 (0.0286)	
training:	Epoch: [34][111/408]	Loss 0.0076 (0.0284)	
training:	Epoch: [34][112/408]	Loss 0.0076 (0.0282)	
training:	Epoch: [34][113/408]	Loss 0.0076 (0.0280)	
training:	Epoch: [34][114/408]	Loss 0.2886 (0.0303)	
training:	Epoch: [34][115/408]	Loss 0.3126 (0.0328)	
training:	Epoch: [34][116/408]	Loss 0.0078 (0.0325)	
training:	Epoch: [34][117/408]	Loss 0.3300 (0.0351)	
training:	Epoch: [34][118/408]	Loss 0.0087 (0.0349)	
training:	Epoch: [34][119/408]	Loss 0.0075 (0.0346)	
training:	Epoch: [34][120/408]	Loss 0.0092 (0.0344)	
training:	Epoch: [34][121/408]	Loss 0.0072 (0.0342)	
training:	Epoch: [34][122/408]	Loss 0.0075 (0.0340)	
training:	Epoch: [34][123/408]	Loss 0.0079 (0.0338)	
training:	Epoch: [34][124/408]	Loss 0.0076 (0.0336)	
training:	Epoch: [34][125/408]	Loss 0.3135 (0.0358)	
training:	Epoch: [34][126/408]	Loss 0.0083 (0.0356)	
training:	Epoch: [34][127/408]	Loss 0.0068 (0.0353)	
training:	Epoch: [34][128/408]	Loss 0.2878 (0.0373)	
training:	Epoch: [34][129/408]	Loss 0.0087 (0.0371)	
training:	Epoch: [34][130/408]	Loss 0.0070 (0.0369)	
training:	Epoch: [34][131/408]	Loss 0.0081 (0.0366)	
training:	Epoch: [34][132/408]	Loss 0.0073 (0.0364)	
training:	Epoch: [34][133/408]	Loss 0.0073 (0.0362)	
training:	Epoch: [34][134/408]	Loss 0.2433 (0.0378)	
training:	Epoch: [34][135/408]	Loss 0.0075 (0.0375)	
training:	Epoch: [34][136/408]	Loss 0.0074 (0.0373)	
training:	Epoch: [34][137/408]	Loss 0.2741 (0.0390)	
training:	Epoch: [34][138/408]	Loss 0.0088 (0.0388)	
training:	Epoch: [34][139/408]	Loss 0.0079 (0.0386)	
training:	Epoch: [34][140/408]	Loss 0.0075 (0.0384)	
training:	Epoch: [34][141/408]	Loss 0.0077 (0.0382)	
training:	Epoch: [34][142/408]	Loss 0.0093 (0.0380)	
training:	Epoch: [34][143/408]	Loss 0.0082 (0.0377)	
training:	Epoch: [34][144/408]	Loss 0.0085 (0.0375)	
training:	Epoch: [34][145/408]	Loss 0.0088 (0.0373)	
training:	Epoch: [34][146/408]	Loss 0.1037 (0.0378)	
training:	Epoch: [34][147/408]	Loss 0.0096 (0.0376)	
training:	Epoch: [34][148/408]	Loss 0.2890 (0.0393)	
training:	Epoch: [34][149/408]	Loss 0.0079 (0.0391)	
training:	Epoch: [34][150/408]	Loss 0.0086 (0.0389)	
training:	Epoch: [34][151/408]	Loss 0.0100 (0.0387)	
training:	Epoch: [34][152/408]	Loss 0.0085 (0.0385)	
training:	Epoch: [34][153/408]	Loss 0.1060 (0.0389)	
training:	Epoch: [34][154/408]	Loss 0.0085 (0.0387)	
training:	Epoch: [34][155/408]	Loss 0.2695 (0.0402)	
training:	Epoch: [34][156/408]	Loss 0.0095 (0.0400)	
training:	Epoch: [34][157/408]	Loss 0.0090 (0.0398)	
training:	Epoch: [34][158/408]	Loss 0.5092 (0.0428)	
training:	Epoch: [34][159/408]	Loss 0.0092 (0.0426)	
training:	Epoch: [34][160/408]	Loss 0.0081 (0.0424)	
training:	Epoch: [34][161/408]	Loss 0.0070 (0.0422)	
training:	Epoch: [34][162/408]	Loss 0.0096 (0.0420)	
training:	Epoch: [34][163/408]	Loss 0.0107 (0.0418)	
training:	Epoch: [34][164/408]	Loss 0.0083 (0.0416)	
training:	Epoch: [34][165/408]	Loss 0.0088 (0.0414)	
training:	Epoch: [34][166/408]	Loss 0.0087 (0.0412)	
training:	Epoch: [34][167/408]	Loss 0.0082 (0.0410)	
training:	Epoch: [34][168/408]	Loss 0.0094 (0.0408)	
training:	Epoch: [34][169/408]	Loss 0.0238 (0.0407)	
training:	Epoch: [34][170/408]	Loss 0.0091 (0.0405)	
training:	Epoch: [34][171/408]	Loss 0.0104 (0.0403)	
training:	Epoch: [34][172/408]	Loss 0.0068 (0.0401)	
training:	Epoch: [34][173/408]	Loss 0.6337 (0.0436)	
training:	Epoch: [34][174/408]	Loss 0.0106 (0.0434)	
training:	Epoch: [34][175/408]	Loss 0.0079 (0.0432)	
training:	Epoch: [34][176/408]	Loss 0.0094 (0.0430)	
training:	Epoch: [34][177/408]	Loss 0.0759 (0.0432)	
training:	Epoch: [34][178/408]	Loss 0.0091 (0.0430)	
training:	Epoch: [34][179/408]	Loss 0.0093 (0.0428)	
training:	Epoch: [34][180/408]	Loss 0.0102 (0.0426)	
training:	Epoch: [34][181/408]	Loss 0.0107 (0.0424)	
training:	Epoch: [34][182/408]	Loss 0.0096 (0.0422)	
training:	Epoch: [34][183/408]	Loss 0.0194 (0.0421)	
training:	Epoch: [34][184/408]	Loss 0.0104 (0.0419)	
training:	Epoch: [34][185/408]	Loss 0.0084 (0.0418)	
training:	Epoch: [34][186/408]	Loss 0.0115 (0.0416)	
training:	Epoch: [34][187/408]	Loss 0.0088 (0.0414)	
training:	Epoch: [34][188/408]	Loss 0.0118 (0.0413)	
training:	Epoch: [34][189/408]	Loss 0.1069 (0.0416)	
training:	Epoch: [34][190/408]	Loss 0.2923 (0.0429)	
training:	Epoch: [34][191/408]	Loss 0.0097 (0.0428)	
training:	Epoch: [34][192/408]	Loss 0.0091 (0.0426)	
training:	Epoch: [34][193/408]	Loss 0.0137 (0.0424)	
training:	Epoch: [34][194/408]	Loss 0.0089 (0.0423)	
training:	Epoch: [34][195/408]	Loss 0.0087 (0.0421)	
training:	Epoch: [34][196/408]	Loss 0.0100 (0.0419)	
training:	Epoch: [34][197/408]	Loss 0.0071 (0.0417)	
training:	Epoch: [34][198/408]	Loss 0.0083 (0.0416)	
training:	Epoch: [34][199/408]	Loss 0.0098 (0.0414)	
training:	Epoch: [34][200/408]	Loss 0.0083 (0.0413)	
training:	Epoch: [34][201/408]	Loss 0.2815 (0.0425)	
training:	Epoch: [34][202/408]	Loss 0.0085 (0.0423)	
training:	Epoch: [34][203/408]	Loss 0.0084 (0.0421)	
training:	Epoch: [34][204/408]	Loss 0.2937 (0.0433)	
training:	Epoch: [34][205/408]	Loss 0.0084 (0.0432)	
training:	Epoch: [34][206/408]	Loss 0.0097 (0.0430)	
training:	Epoch: [34][207/408]	Loss 0.0089 (0.0429)	
training:	Epoch: [34][208/408]	Loss 0.0083 (0.0427)	
training:	Epoch: [34][209/408]	Loss 0.0074 (0.0425)	
training:	Epoch: [34][210/408]	Loss 0.0081 (0.0424)	
training:	Epoch: [34][211/408]	Loss 0.0098 (0.0422)	
training:	Epoch: [34][212/408]	Loss 0.2603 (0.0432)	
training:	Epoch: [34][213/408]	Loss 0.0253 (0.0431)	
training:	Epoch: [34][214/408]	Loss 0.0138 (0.0430)	
training:	Epoch: [34][215/408]	Loss 0.0094 (0.0428)	
training:	Epoch: [34][216/408]	Loss 0.0107 (0.0427)	
training:	Epoch: [34][217/408]	Loss 0.0080 (0.0425)	
training:	Epoch: [34][218/408]	Loss 0.0087 (0.0424)	
training:	Epoch: [34][219/408]	Loss 0.0092 (0.0422)	
training:	Epoch: [34][220/408]	Loss 0.0083 (0.0421)	
training:	Epoch: [34][221/408]	Loss 0.0088 (0.0419)	
training:	Epoch: [34][222/408]	Loss 0.0119 (0.0418)	
training:	Epoch: [34][223/408]	Loss 0.0085 (0.0416)	
training:	Epoch: [34][224/408]	Loss 0.0093 (0.0415)	
training:	Epoch: [34][225/408]	Loss 0.0350 (0.0415)	
training:	Epoch: [34][226/408]	Loss 0.0081 (0.0413)	
training:	Epoch: [34][227/408]	Loss 0.0099 (0.0412)	
training:	Epoch: [34][228/408]	Loss 0.0110 (0.0411)	
training:	Epoch: [34][229/408]	Loss 0.0105 (0.0409)	
training:	Epoch: [34][230/408]	Loss 0.0126 (0.0408)	
training:	Epoch: [34][231/408]	Loss 0.0090 (0.0407)	
training:	Epoch: [34][232/408]	Loss 0.0085 (0.0405)	
training:	Epoch: [34][233/408]	Loss 0.0096 (0.0404)	
training:	Epoch: [34][234/408]	Loss 0.0090 (0.0403)	
training:	Epoch: [34][235/408]	Loss 0.3007 (0.0414)	
training:	Epoch: [34][236/408]	Loss 0.0089 (0.0412)	
training:	Epoch: [34][237/408]	Loss 0.0110 (0.0411)	
training:	Epoch: [34][238/408]	Loss 0.0081 (0.0410)	
training:	Epoch: [34][239/408]	Loss 0.0088 (0.0408)	
training:	Epoch: [34][240/408]	Loss 0.0126 (0.0407)	
training:	Epoch: [34][241/408]	Loss 0.0084 (0.0406)	
training:	Epoch: [34][242/408]	Loss 0.0086 (0.0404)	
training:	Epoch: [34][243/408]	Loss 0.0087 (0.0403)	
training:	Epoch: [34][244/408]	Loss 0.0086 (0.0402)	
training:	Epoch: [34][245/408]	Loss 0.0083 (0.0400)	
training:	Epoch: [34][246/408]	Loss 0.0084 (0.0399)	
training:	Epoch: [34][247/408]	Loss 0.0077 (0.0398)	
training:	Epoch: [34][248/408]	Loss 0.0097 (0.0397)	
training:	Epoch: [34][249/408]	Loss 0.0136 (0.0396)	
training:	Epoch: [34][250/408]	Loss 0.0078 (0.0394)	
training:	Epoch: [34][251/408]	Loss 0.0395 (0.0394)	
training:	Epoch: [34][252/408]	Loss 0.0080 (0.0393)	
training:	Epoch: [34][253/408]	Loss 0.0070 (0.0392)	
training:	Epoch: [34][254/408]	Loss 0.0099 (0.0391)	
training:	Epoch: [34][255/408]	Loss 0.0106 (0.0390)	
training:	Epoch: [34][256/408]	Loss 0.0074 (0.0388)	
training:	Epoch: [34][257/408]	Loss 0.0074 (0.0387)	
training:	Epoch: [34][258/408]	Loss 0.0066 (0.0386)	
training:	Epoch: [34][259/408]	Loss 0.2987 (0.0396)	
training:	Epoch: [34][260/408]	Loss 0.0086 (0.0395)	
training:	Epoch: [34][261/408]	Loss 0.0082 (0.0394)	
training:	Epoch: [34][262/408]	Loss 0.0071 (0.0392)	
training:	Epoch: [34][263/408]	Loss 0.0082 (0.0391)	
training:	Epoch: [34][264/408]	Loss 0.2984 (0.0401)	
training:	Epoch: [34][265/408]	Loss 0.0082 (0.0400)	
training:	Epoch: [34][266/408]	Loss 0.0094 (0.0399)	
training:	Epoch: [34][267/408]	Loss 0.2971 (0.0408)	
training:	Epoch: [34][268/408]	Loss 0.0071 (0.0407)	
training:	Epoch: [34][269/408]	Loss 0.0377 (0.0407)	
training:	Epoch: [34][270/408]	Loss 0.0089 (0.0406)	
training:	Epoch: [34][271/408]	Loss 0.0098 (0.0405)	
training:	Epoch: [34][272/408]	Loss 0.2890 (0.0414)	
training:	Epoch: [34][273/408]	Loss 0.0094 (0.0412)	
training:	Epoch: [34][274/408]	Loss 0.0079 (0.0411)	
training:	Epoch: [34][275/408]	Loss 0.0095 (0.0410)	
training:	Epoch: [34][276/408]	Loss 0.0093 (0.0409)	
training:	Epoch: [34][277/408]	Loss 0.0077 (0.0408)	
training:	Epoch: [34][278/408]	Loss 0.0072 (0.0407)	
training:	Epoch: [34][279/408]	Loss 0.0077 (0.0405)	
training:	Epoch: [34][280/408]	Loss 0.0074 (0.0404)	
training:	Epoch: [34][281/408]	Loss 0.0070 (0.0403)	
training:	Epoch: [34][282/408]	Loss 0.0076 (0.0402)	
training:	Epoch: [34][283/408]	Loss 0.0075 (0.0401)	
training:	Epoch: [34][284/408]	Loss 0.0065 (0.0400)	
training:	Epoch: [34][285/408]	Loss 0.0085 (0.0398)	
training:	Epoch: [34][286/408]	Loss 0.0093 (0.0397)	
training:	Epoch: [34][287/408]	Loss 0.0140 (0.0396)	
training:	Epoch: [34][288/408]	Loss 0.0176 (0.0396)	
training:	Epoch: [34][289/408]	Loss 0.0082 (0.0395)	
training:	Epoch: [34][290/408]	Loss 0.0076 (0.0394)	
training:	Epoch: [34][291/408]	Loss 0.0078 (0.0392)	
training:	Epoch: [34][292/408]	Loss 0.2888 (0.0401)	
training:	Epoch: [34][293/408]	Loss 0.0175 (0.0400)	
training:	Epoch: [34][294/408]	Loss 0.2911 (0.0409)	
training:	Epoch: [34][295/408]	Loss 0.0069 (0.0408)	
training:	Epoch: [34][296/408]	Loss 0.0077 (0.0406)	
training:	Epoch: [34][297/408]	Loss 0.0082 (0.0405)	
training:	Epoch: [34][298/408]	Loss 0.0072 (0.0404)	
training:	Epoch: [34][299/408]	Loss 0.0083 (0.0403)	
training:	Epoch: [34][300/408]	Loss 0.6035 (0.0422)	
training:	Epoch: [34][301/408]	Loss 0.1910 (0.0427)	
training:	Epoch: [34][302/408]	Loss 0.0086 (0.0426)	
training:	Epoch: [34][303/408]	Loss 0.0078 (0.0425)	
training:	Epoch: [34][304/408]	Loss 0.0071 (0.0423)	
training:	Epoch: [34][305/408]	Loss 0.0071 (0.0422)	
training:	Epoch: [34][306/408]	Loss 0.0076 (0.0421)	
training:	Epoch: [34][307/408]	Loss 0.0079 (0.0420)	
training:	Epoch: [34][308/408]	Loss 0.0088 (0.0419)	
training:	Epoch: [34][309/408]	Loss 0.0082 (0.0418)	
training:	Epoch: [34][310/408]	Loss 0.0079 (0.0417)	
training:	Epoch: [34][311/408]	Loss 0.0076 (0.0416)	
training:	Epoch: [34][312/408]	Loss 0.0098 (0.0415)	
training:	Epoch: [34][313/408]	Loss 0.0083 (0.0414)	
training:	Epoch: [34][314/408]	Loss 0.0085 (0.0413)	
training:	Epoch: [34][315/408]	Loss 0.3249 (0.0422)	
training:	Epoch: [34][316/408]	Loss 0.0110 (0.0421)	
training:	Epoch: [34][317/408]	Loss 0.0135 (0.0420)	
training:	Epoch: [34][318/408]	Loss 0.0084 (0.0419)	
training:	Epoch: [34][319/408]	Loss 0.2994 (0.0427)	
training:	Epoch: [34][320/408]	Loss 0.0087 (0.0426)	
training:	Epoch: [34][321/408]	Loss 0.0075 (0.0425)	
training:	Epoch: [34][322/408]	Loss 0.0093 (0.0424)	
training:	Epoch: [34][323/408]	Loss 0.0996 (0.0425)	
training:	Epoch: [34][324/408]	Loss 0.0081 (0.0424)	
training:	Epoch: [34][325/408]	Loss 0.0078 (0.0423)	
training:	Epoch: [34][326/408]	Loss 0.1779 (0.0427)	
training:	Epoch: [34][327/408]	Loss 0.0074 (0.0426)	
training:	Epoch: [34][328/408]	Loss 0.4428 (0.0438)	
training:	Epoch: [34][329/408]	Loss 0.2872 (0.0446)	
training:	Epoch: [34][330/408]	Loss 0.0079 (0.0445)	
training:	Epoch: [34][331/408]	Loss 0.0075 (0.0444)	
training:	Epoch: [34][332/408]	Loss 0.2324 (0.0449)	
training:	Epoch: [34][333/408]	Loss 0.2917 (0.0457)	
training:	Epoch: [34][334/408]	Loss 0.0101 (0.0456)	
training:	Epoch: [34][335/408]	Loss 0.0254 (0.0455)	
training:	Epoch: [34][336/408]	Loss 0.0086 (0.0454)	
training:	Epoch: [34][337/408]	Loss 0.0108 (0.0453)	
training:	Epoch: [34][338/408]	Loss 0.0081 (0.0452)	
training:	Epoch: [34][339/408]	Loss 0.2934 (0.0459)	
training:	Epoch: [34][340/408]	Loss 0.0111 (0.0458)	
training:	Epoch: [34][341/408]	Loss 0.0098 (0.0457)	
training:	Epoch: [34][342/408]	Loss 0.0076 (0.0456)	
training:	Epoch: [34][343/408]	Loss 0.0199 (0.0455)	
training:	Epoch: [34][344/408]	Loss 0.0128 (0.0454)	
training:	Epoch: [34][345/408]	Loss 0.0086 (0.0453)	
training:	Epoch: [34][346/408]	Loss 0.0146 (0.0452)	
training:	Epoch: [34][347/408]	Loss 0.0079 (0.0451)	
training:	Epoch: [34][348/408]	Loss 0.0078 (0.0450)	
training:	Epoch: [34][349/408]	Loss 0.0218 (0.0449)	
training:	Epoch: [34][350/408]	Loss 0.0079 (0.0448)	
training:	Epoch: [34][351/408]	Loss 0.0081 (0.0447)	
training:	Epoch: [34][352/408]	Loss 0.0083 (0.0446)	
training:	Epoch: [34][353/408]	Loss 0.0091 (0.0445)	
training:	Epoch: [34][354/408]	Loss 0.0098 (0.0444)	
training:	Epoch: [34][355/408]	Loss 0.0074 (0.0443)	
training:	Epoch: [34][356/408]	Loss 0.0083 (0.0442)	
training:	Epoch: [34][357/408]	Loss 0.0071 (0.0441)	
training:	Epoch: [34][358/408]	Loss 0.0089 (0.0440)	
training:	Epoch: [34][359/408]	Loss 0.0079 (0.0439)	
training:	Epoch: [34][360/408]	Loss 0.0085 (0.0438)	
training:	Epoch: [34][361/408]	Loss 0.0072 (0.0437)	
training:	Epoch: [34][362/408]	Loss 0.0083 (0.0436)	
training:	Epoch: [34][363/408]	Loss 0.0104 (0.0435)	
training:	Epoch: [34][364/408]	Loss 0.0083 (0.0434)	
training:	Epoch: [34][365/408]	Loss 0.0577 (0.0435)	
training:	Epoch: [34][366/408]	Loss 0.0081 (0.0434)	
training:	Epoch: [34][367/408]	Loss 0.0084 (0.0433)	
training:	Epoch: [34][368/408]	Loss 0.3098 (0.0440)	
training:	Epoch: [34][369/408]	Loss 0.0073 (0.0439)	
training:	Epoch: [34][370/408]	Loss 0.0158 (0.0438)	
training:	Epoch: [34][371/408]	Loss 0.2809 (0.0445)	
training:	Epoch: [34][372/408]	Loss 0.0066 (0.0444)	
training:	Epoch: [34][373/408]	Loss 0.0091 (0.0443)	
training:	Epoch: [34][374/408]	Loss 0.0077 (0.0442)	
training:	Epoch: [34][375/408]	Loss 0.0086 (0.0441)	
training:	Epoch: [34][376/408]	Loss 0.0079 (0.0440)	
training:	Epoch: [34][377/408]	Loss 0.0095 (0.0439)	
training:	Epoch: [34][378/408]	Loss 0.0139 (0.0438)	
training:	Epoch: [34][379/408]	Loss 0.3126 (0.0445)	
training:	Epoch: [34][380/408]	Loss 0.2958 (0.0452)	
training:	Epoch: [34][381/408]	Loss 0.0087 (0.0451)	
training:	Epoch: [34][382/408]	Loss 0.0080 (0.0450)	
training:	Epoch: [34][383/408]	Loss 0.0092 (0.0449)	
training:	Epoch: [34][384/408]	Loss 0.0095 (0.0448)	
training:	Epoch: [34][385/408]	Loss 0.0076 (0.0447)	
training:	Epoch: [34][386/408]	Loss 0.0071 (0.0446)	
training:	Epoch: [34][387/408]	Loss 0.0091 (0.0445)	
training:	Epoch: [34][388/408]	Loss 0.0141 (0.0444)	
training:	Epoch: [34][389/408]	Loss 0.0083 (0.0444)	
training:	Epoch: [34][390/408]	Loss 0.0072 (0.0443)	
training:	Epoch: [34][391/408]	Loss 0.0084 (0.0442)	
training:	Epoch: [34][392/408]	Loss 0.0092 (0.0441)	
training:	Epoch: [34][393/408]	Loss 0.0076 (0.0440)	
training:	Epoch: [34][394/408]	Loss 0.0177 (0.0439)	
training:	Epoch: [34][395/408]	Loss 0.1797 (0.0443)	
training:	Epoch: [34][396/408]	Loss 0.0081 (0.0442)	
training:	Epoch: [34][397/408]	Loss 0.0091 (0.0441)	
training:	Epoch: [34][398/408]	Loss 0.0071 (0.0440)	
training:	Epoch: [34][399/408]	Loss 0.0080 (0.0439)	
training:	Epoch: [34][400/408]	Loss 0.3049 (0.0445)	
training:	Epoch: [34][401/408]	Loss 0.0081 (0.0445)	
training:	Epoch: [34][402/408]	Loss 0.0078 (0.0444)	
training:	Epoch: [34][403/408]	Loss 0.2777 (0.0449)	
training:	Epoch: [34][404/408]	Loss 0.0541 (0.0450)	
training:	Epoch: [34][405/408]	Loss 0.0088 (0.0449)	
training:	Epoch: [34][406/408]	Loss 0.0110 (0.0448)	
training:	Epoch: [34][407/408]	Loss 0.0086 (0.0447)	
training:	Epoch: [34][408/408]	Loss 0.0096 (0.0446)	
Training:	 Loss: 0.0446

Training:	 ACC: 0.9930 0.9930 0.9927 0.9933
Validation:	 ACC: 0.7841 0.7849 0.8014 0.7668
Validation:	 Best_BACC: 0.8226 0.8250 0.8772 0.7679
Validation:	 Loss: 0.9044
Pretraining:	Epoch 35/200
----------
training:	Epoch: [35][1/408]	Loss 0.0069 (0.0069)	
training:	Epoch: [35][2/408]	Loss 0.0113 (0.0091)	
training:	Epoch: [35][3/408]	Loss 0.0068 (0.0083)	
training:	Epoch: [35][4/408]	Loss 0.0137 (0.0097)	
training:	Epoch: [35][5/408]	Loss 0.0078 (0.0093)	
training:	Epoch: [35][6/408]	Loss 0.0088 (0.0092)	
training:	Epoch: [35][7/408]	Loss 0.0081 (0.0090)	
training:	Epoch: [35][8/408]	Loss 0.0071 (0.0088)	
training:	Epoch: [35][9/408]	Loss 0.0081 (0.0087)	
training:	Epoch: [35][10/408]	Loss 0.2834 (0.0362)	
training:	Epoch: [35][11/408]	Loss 0.0078 (0.0336)	
training:	Epoch: [35][12/408]	Loss 0.0066 (0.0314)	
training:	Epoch: [35][13/408]	Loss 0.0071 (0.0295)	
training:	Epoch: [35][14/408]	Loss 0.0099 (0.0281)	
training:	Epoch: [35][15/408]	Loss 0.3069 (0.0467)	
training:	Epoch: [35][16/408]	Loss 0.0090 (0.0443)	
training:	Epoch: [35][17/408]	Loss 0.2800 (0.0582)	
training:	Epoch: [35][18/408]	Loss 0.0193 (0.0560)	
training:	Epoch: [35][19/408]	Loss 0.0085 (0.0535)	
training:	Epoch: [35][20/408]	Loss 0.0167 (0.0517)	
training:	Epoch: [35][21/408]	Loss 0.0080 (0.0496)	
training:	Epoch: [35][22/408]	Loss 0.0087 (0.0477)	
training:	Epoch: [35][23/408]	Loss 0.0080 (0.0460)	
training:	Epoch: [35][24/408]	Loss 0.0101 (0.0445)	
training:	Epoch: [35][25/408]	Loss 0.0085 (0.0431)	
training:	Epoch: [35][26/408]	Loss 0.0090 (0.0418)	
training:	Epoch: [35][27/408]	Loss 0.0100 (0.0406)	
training:	Epoch: [35][28/408]	Loss 0.0085 (0.0394)	
training:	Epoch: [35][29/408]	Loss 0.0095 (0.0384)	
training:	Epoch: [35][30/408]	Loss 0.0079 (0.0374)	
training:	Epoch: [35][31/408]	Loss 0.0091 (0.0365)	
training:	Epoch: [35][32/408]	Loss 0.0103 (0.0357)	
training:	Epoch: [35][33/408]	Loss 0.0131 (0.0350)	
training:	Epoch: [35][34/408]	Loss 0.0080 (0.0342)	
training:	Epoch: [35][35/408]	Loss 0.0099 (0.0335)	
training:	Epoch: [35][36/408]	Loss 0.0074 (0.0328)	
training:	Epoch: [35][37/408]	Loss 0.0085 (0.0321)	
training:	Epoch: [35][38/408]	Loss 0.0093 (0.0315)	
training:	Epoch: [35][39/408]	Loss 0.0079 (0.0309)	
training:	Epoch: [35][40/408]	Loss 0.3238 (0.0382)	
training:	Epoch: [35][41/408]	Loss 0.0081 (0.0375)	
training:	Epoch: [35][42/408]	Loss 0.0076 (0.0368)	
training:	Epoch: [35][43/408]	Loss 0.0425 (0.0369)	
training:	Epoch: [35][44/408]	Loss 0.2789 (0.0424)	
training:	Epoch: [35][45/408]	Loss 0.0086 (0.0417)	
training:	Epoch: [35][46/408]	Loss 0.0065 (0.0409)	
training:	Epoch: [35][47/408]	Loss 0.1859 (0.0440)	
training:	Epoch: [35][48/408]	Loss 0.0097 (0.0433)	
training:	Epoch: [35][49/408]	Loss 0.0074 (0.0425)	
training:	Epoch: [35][50/408]	Loss 0.0067 (0.0418)	
training:	Epoch: [35][51/408]	Loss 0.0132 (0.0413)	
training:	Epoch: [35][52/408]	Loss 0.0076 (0.0406)	
training:	Epoch: [35][53/408]	Loss 0.0090 (0.0400)	
training:	Epoch: [35][54/408]	Loss 0.0081 (0.0394)	
training:	Epoch: [35][55/408]	Loss 0.0095 (0.0389)	
training:	Epoch: [35][56/408]	Loss 0.0083 (0.0383)	
training:	Epoch: [35][57/408]	Loss 0.0090 (0.0378)	
training:	Epoch: [35][58/408]	Loss 0.0079 (0.0373)	
training:	Epoch: [35][59/408]	Loss 0.0099 (0.0368)	
training:	Epoch: [35][60/408]	Loss 0.0121 (0.0364)	
training:	Epoch: [35][61/408]	Loss 0.0108 (0.0360)	
training:	Epoch: [35][62/408]	Loss 0.0090 (0.0356)	
training:	Epoch: [35][63/408]	Loss 0.0088 (0.0351)	
training:	Epoch: [35][64/408]	Loss 0.3126 (0.0395)	
training:	Epoch: [35][65/408]	Loss 0.0146 (0.0391)	
training:	Epoch: [35][66/408]	Loss 0.0083 (0.0386)	
training:	Epoch: [35][67/408]	Loss 0.0082 (0.0382)	
training:	Epoch: [35][68/408]	Loss 0.0090 (0.0377)	
training:	Epoch: [35][69/408]	Loss 0.3325 (0.0420)	
training:	Epoch: [35][70/408]	Loss 0.2391 (0.0448)	
training:	Epoch: [35][71/408]	Loss 0.2846 (0.0482)	
training:	Epoch: [35][72/408]	Loss 0.2866 (0.0515)	
training:	Epoch: [35][73/408]	Loss 0.0087 (0.0509)	
training:	Epoch: [35][74/408]	Loss 0.0094 (0.0504)	
training:	Epoch: [35][75/408]	Loss 0.0074 (0.0498)	
training:	Epoch: [35][76/408]	Loss 0.2781 (0.0528)	
training:	Epoch: [35][77/408]	Loss 0.0087 (0.0522)	
training:	Epoch: [35][78/408]	Loss 0.0087 (0.0517)	
training:	Epoch: [35][79/408]	Loss 0.0078 (0.0511)	
training:	Epoch: [35][80/408]	Loss 0.0082 (0.0506)	
training:	Epoch: [35][81/408]	Loss 0.0083 (0.0501)	
training:	Epoch: [35][82/408]	Loss 0.2945 (0.0530)	
training:	Epoch: [35][83/408]	Loss 0.0097 (0.0525)	
training:	Epoch: [35][84/408]	Loss 0.0086 (0.0520)	
training:	Epoch: [35][85/408]	Loss 0.0077 (0.0515)	
training:	Epoch: [35][86/408]	Loss 0.0102 (0.0510)	
training:	Epoch: [35][87/408]	Loss 0.0088 (0.0505)	
training:	Epoch: [35][88/408]	Loss 0.0092 (0.0500)	
training:	Epoch: [35][89/408]	Loss 0.0082 (0.0496)	
training:	Epoch: [35][90/408]	Loss 0.0093 (0.0491)	
training:	Epoch: [35][91/408]	Loss 0.0083 (0.0487)	
training:	Epoch: [35][92/408]	Loss 0.0103 (0.0483)	
training:	Epoch: [35][93/408]	Loss 0.0075 (0.0478)	
training:	Epoch: [35][94/408]	Loss 0.0076 (0.0474)	
training:	Epoch: [35][95/408]	Loss 0.0090 (0.0470)	
training:	Epoch: [35][96/408]	Loss 0.0164 (0.0467)	
training:	Epoch: [35][97/408]	Loss 0.0083 (0.0463)	
training:	Epoch: [35][98/408]	Loss 0.0079 (0.0459)	
training:	Epoch: [35][99/408]	Loss 0.0080 (0.0455)	
training:	Epoch: [35][100/408]	Loss 0.0083 (0.0451)	
training:	Epoch: [35][101/408]	Loss 0.0091 (0.0448)	
training:	Epoch: [35][102/408]	Loss 0.0094 (0.0444)	
training:	Epoch: [35][103/408]	Loss 0.3061 (0.0470)	
training:	Epoch: [35][104/408]	Loss 0.0098 (0.0466)	
training:	Epoch: [35][105/408]	Loss 0.2972 (0.0490)	
training:	Epoch: [35][106/408]	Loss 0.0112 (0.0486)	
training:	Epoch: [35][107/408]	Loss 0.0090 (0.0483)	
training:	Epoch: [35][108/408]	Loss 0.0073 (0.0479)	
training:	Epoch: [35][109/408]	Loss 0.0077 (0.0475)	
training:	Epoch: [35][110/408]	Loss 0.0087 (0.0472)	
training:	Epoch: [35][111/408]	Loss 0.0087 (0.0468)	
training:	Epoch: [35][112/408]	Loss 0.0084 (0.0465)	
training:	Epoch: [35][113/408]	Loss 0.0076 (0.0461)	
training:	Epoch: [35][114/408]	Loss 0.3138 (0.0485)	
training:	Epoch: [35][115/408]	Loss 0.0083 (0.0481)	
training:	Epoch: [35][116/408]	Loss 0.0084 (0.0478)	
training:	Epoch: [35][117/408]	Loss 0.0081 (0.0474)	
training:	Epoch: [35][118/408]	Loss 0.0077 (0.0471)	
training:	Epoch: [35][119/408]	Loss 0.0074 (0.0468)	
training:	Epoch: [35][120/408]	Loss 0.0111 (0.0465)	
training:	Epoch: [35][121/408]	Loss 0.2837 (0.0484)	
training:	Epoch: [35][122/408]	Loss 0.0088 (0.0481)	
training:	Epoch: [35][123/408]	Loss 0.0081 (0.0478)	
training:	Epoch: [35][124/408]	Loss 0.0082 (0.0475)	
training:	Epoch: [35][125/408]	Loss 0.0080 (0.0472)	
training:	Epoch: [35][126/408]	Loss 0.0073 (0.0468)	
training:	Epoch: [35][127/408]	Loss 0.0096 (0.0465)	
training:	Epoch: [35][128/408]	Loss 0.0085 (0.0462)	
training:	Epoch: [35][129/408]	Loss 0.0080 (0.0459)	
training:	Epoch: [35][130/408]	Loss 0.0096 (0.0457)	
training:	Epoch: [35][131/408]	Loss 0.0087 (0.0454)	
training:	Epoch: [35][132/408]	Loss 0.0082 (0.0451)	
training:	Epoch: [35][133/408]	Loss 0.0074 (0.0448)	
training:	Epoch: [35][134/408]	Loss 0.0087 (0.0446)	
training:	Epoch: [35][135/408]	Loss 0.3001 (0.0464)	
training:	Epoch: [35][136/408]	Loss 0.0077 (0.0462)	
training:	Epoch: [35][137/408]	Loss 0.0087 (0.0459)	
training:	Epoch: [35][138/408]	Loss 0.0073 (0.0456)	
training:	Epoch: [35][139/408]	Loss 0.0083 (0.0453)	
training:	Epoch: [35][140/408]	Loss 0.0095 (0.0451)	
training:	Epoch: [35][141/408]	Loss 0.0082 (0.0448)	
training:	Epoch: [35][142/408]	Loss 0.0089 (0.0446)	
training:	Epoch: [35][143/408]	Loss 0.0086 (0.0443)	
training:	Epoch: [35][144/408]	Loss 0.0071 (0.0441)	
training:	Epoch: [35][145/408]	Loss 0.0082 (0.0438)	
training:	Epoch: [35][146/408]	Loss 0.0131 (0.0436)	
training:	Epoch: [35][147/408]	Loss 0.0088 (0.0434)	
training:	Epoch: [35][148/408]	Loss 0.0080 (0.0431)	
training:	Epoch: [35][149/408]	Loss 0.0087 (0.0429)	
training:	Epoch: [35][150/408]	Loss 0.3147 (0.0447)	
training:	Epoch: [35][151/408]	Loss 0.0069 (0.0445)	
training:	Epoch: [35][152/408]	Loss 0.0095 (0.0442)	
training:	Epoch: [35][153/408]	Loss 0.0075 (0.0440)	
training:	Epoch: [35][154/408]	Loss 0.0086 (0.0438)	
training:	Epoch: [35][155/408]	Loss 0.0080 (0.0435)	
training:	Epoch: [35][156/408]	Loss 0.0082 (0.0433)	
training:	Epoch: [35][157/408]	Loss 0.0092 (0.0431)	
training:	Epoch: [35][158/408]	Loss 0.0096 (0.0429)	
training:	Epoch: [35][159/408]	Loss 0.0092 (0.0427)	
training:	Epoch: [35][160/408]	Loss 0.0080 (0.0424)	
training:	Epoch: [35][161/408]	Loss 0.0084 (0.0422)	
training:	Epoch: [35][162/408]	Loss 0.0088 (0.0420)	
training:	Epoch: [35][163/408]	Loss 0.0096 (0.0418)	
training:	Epoch: [35][164/408]	Loss 0.0079 (0.0416)	
training:	Epoch: [35][165/408]	Loss 0.0084 (0.0414)	
training:	Epoch: [35][166/408]	Loss 0.0079 (0.0412)	
training:	Epoch: [35][167/408]	Loss 0.0082 (0.0410)	
training:	Epoch: [35][168/408]	Loss 0.2987 (0.0426)	
training:	Epoch: [35][169/408]	Loss 0.0083 (0.0423)	
training:	Epoch: [35][170/408]	Loss 0.0089 (0.0422)	
training:	Epoch: [35][171/408]	Loss 0.6051 (0.0454)	
training:	Epoch: [35][172/408]	Loss 0.0072 (0.0452)	
training:	Epoch: [35][173/408]	Loss 0.0108 (0.0450)	
training:	Epoch: [35][174/408]	Loss 0.0082 (0.0448)	
training:	Epoch: [35][175/408]	Loss 0.0082 (0.0446)	
training:	Epoch: [35][176/408]	Loss 0.0088 (0.0444)	
training:	Epoch: [35][177/408]	Loss 0.0086 (0.0442)	
training:	Epoch: [35][178/408]	Loss 0.0081 (0.0440)	
training:	Epoch: [35][179/408]	Loss 0.0084 (0.0438)	
training:	Epoch: [35][180/408]	Loss 0.0086 (0.0436)	
training:	Epoch: [35][181/408]	Loss 0.2037 (0.0445)	
training:	Epoch: [35][182/408]	Loss 0.0070 (0.0443)	
training:	Epoch: [35][183/408]	Loss 0.0095 (0.0441)	
training:	Epoch: [35][184/408]	Loss 0.0078 (0.0439)	
training:	Epoch: [35][185/408]	Loss 0.0089 (0.0437)	
training:	Epoch: [35][186/408]	Loss 0.0086 (0.0435)	
training:	Epoch: [35][187/408]	Loss 0.0080 (0.0433)	
training:	Epoch: [35][188/408]	Loss 0.0100 (0.0431)	
training:	Epoch: [35][189/408]	Loss 0.0077 (0.0430)	
training:	Epoch: [35][190/408]	Loss 0.0094 (0.0428)	
training:	Epoch: [35][191/408]	Loss 0.0075 (0.0426)	
training:	Epoch: [35][192/408]	Loss 0.0074 (0.0424)	
training:	Epoch: [35][193/408]	Loss 0.0081 (0.0422)	
training:	Epoch: [35][194/408]	Loss 0.3059 (0.0436)	
training:	Epoch: [35][195/408]	Loss 0.0081 (0.0434)	
training:	Epoch: [35][196/408]	Loss 0.0117 (0.0432)	
training:	Epoch: [35][197/408]	Loss 0.0090 (0.0431)	
training:	Epoch: [35][198/408]	Loss 0.0162 (0.0429)	
training:	Epoch: [35][199/408]	Loss 0.0085 (0.0428)	
training:	Epoch: [35][200/408]	Loss 0.2865 (0.0440)	
training:	Epoch: [35][201/408]	Loss 0.2271 (0.0449)	
training:	Epoch: [35][202/408]	Loss 0.0092 (0.0447)	
training:	Epoch: [35][203/408]	Loss 0.0080 (0.0445)	
training:	Epoch: [35][204/408]	Loss 0.0091 (0.0444)	
training:	Epoch: [35][205/408]	Loss 0.0100 (0.0442)	
training:	Epoch: [35][206/408]	Loss 0.0167 (0.0441)	
training:	Epoch: [35][207/408]	Loss 0.0084 (0.0439)	
training:	Epoch: [35][208/408]	Loss 0.0077 (0.0437)	
training:	Epoch: [35][209/408]	Loss 0.0121 (0.0436)	
training:	Epoch: [35][210/408]	Loss 0.2978 (0.0448)	
training:	Epoch: [35][211/408]	Loss 0.0075 (0.0446)	
training:	Epoch: [35][212/408]	Loss 0.0076 (0.0444)	
training:	Epoch: [35][213/408]	Loss 0.0104 (0.0443)	
training:	Epoch: [35][214/408]	Loss 0.0079 (0.0441)	
training:	Epoch: [35][215/408]	Loss 0.0086 (0.0439)	
training:	Epoch: [35][216/408]	Loss 0.0068 (0.0438)	
training:	Epoch: [35][217/408]	Loss 0.0254 (0.0437)	
training:	Epoch: [35][218/408]	Loss 0.0081 (0.0435)	
training:	Epoch: [35][219/408]	Loss 0.0086 (0.0434)	
training:	Epoch: [35][220/408]	Loss 0.0087 (0.0432)	
training:	Epoch: [35][221/408]	Loss 0.0084 (0.0430)	
training:	Epoch: [35][222/408]	Loss 0.0074 (0.0429)	
training:	Epoch: [35][223/408]	Loss 0.0080 (0.0427)	
training:	Epoch: [35][224/408]	Loss 0.0100 (0.0426)	
training:	Epoch: [35][225/408]	Loss 0.0092 (0.0424)	
training:	Epoch: [35][226/408]	Loss 0.0078 (0.0423)	
training:	Epoch: [35][227/408]	Loss 0.0080 (0.0421)	
training:	Epoch: [35][228/408]	Loss 0.0084 (0.0420)	
training:	Epoch: [35][229/408]	Loss 0.0076 (0.0418)	
training:	Epoch: [35][230/408]	Loss 0.0113 (0.0417)	
training:	Epoch: [35][231/408]	Loss 0.0088 (0.0415)	
training:	Epoch: [35][232/408]	Loss 0.0062 (0.0414)	
training:	Epoch: [35][233/408]	Loss 0.0081 (0.0413)	
training:	Epoch: [35][234/408]	Loss 0.0073 (0.0411)	
training:	Epoch: [35][235/408]	Loss 0.0086 (0.0410)	
training:	Epoch: [35][236/408]	Loss 0.0086 (0.0408)	
training:	Epoch: [35][237/408]	Loss 0.0083 (0.0407)	
training:	Epoch: [35][238/408]	Loss 0.0083 (0.0406)	
training:	Epoch: [35][239/408]	Loss 0.3003 (0.0416)	
training:	Epoch: [35][240/408]	Loss 0.0081 (0.0415)	
training:	Epoch: [35][241/408]	Loss 0.0081 (0.0414)	
training:	Epoch: [35][242/408]	Loss 0.0088 (0.0412)	
training:	Epoch: [35][243/408]	Loss 0.3791 (0.0426)	
training:	Epoch: [35][244/408]	Loss 0.0077 (0.0425)	
training:	Epoch: [35][245/408]	Loss 0.0083 (0.0423)	
training:	Epoch: [35][246/408]	Loss 0.0081 (0.0422)	
training:	Epoch: [35][247/408]	Loss 0.0078 (0.0421)	
training:	Epoch: [35][248/408]	Loss 0.0086 (0.0419)	
training:	Epoch: [35][249/408]	Loss 0.0070 (0.0418)	
training:	Epoch: [35][250/408]	Loss 0.0080 (0.0417)	
training:	Epoch: [35][251/408]	Loss 0.0085 (0.0415)	
training:	Epoch: [35][252/408]	Loss 0.2809 (0.0425)	
training:	Epoch: [35][253/408]	Loss 0.0085 (0.0423)	
training:	Epoch: [35][254/408]	Loss 0.0134 (0.0422)	
training:	Epoch: [35][255/408]	Loss 0.2776 (0.0431)	
training:	Epoch: [35][256/408]	Loss 0.0073 (0.0430)	
training:	Epoch: [35][257/408]	Loss 0.0065 (0.0429)	
training:	Epoch: [35][258/408]	Loss 0.3174 (0.0439)	
training:	Epoch: [35][259/408]	Loss 0.0099 (0.0438)	
training:	Epoch: [35][260/408]	Loss 0.0087 (0.0437)	
training:	Epoch: [35][261/408]	Loss 0.0091 (0.0435)	
training:	Epoch: [35][262/408]	Loss 0.0087 (0.0434)	
training:	Epoch: [35][263/408]	Loss 0.0098 (0.0433)	
training:	Epoch: [35][264/408]	Loss 0.0092 (0.0431)	
training:	Epoch: [35][265/408]	Loss 0.0094 (0.0430)	
training:	Epoch: [35][266/408]	Loss 0.0112 (0.0429)	
training:	Epoch: [35][267/408]	Loss 0.0078 (0.0428)	
training:	Epoch: [35][268/408]	Loss 0.0073 (0.0426)	
training:	Epoch: [35][269/408]	Loss 0.0090 (0.0425)	
training:	Epoch: [35][270/408]	Loss 0.0084 (0.0424)	
training:	Epoch: [35][271/408]	Loss 0.0071 (0.0422)	
training:	Epoch: [35][272/408]	Loss 0.0144 (0.0421)	
training:	Epoch: [35][273/408]	Loss 0.0081 (0.0420)	
training:	Epoch: [35][274/408]	Loss 0.0099 (0.0419)	
training:	Epoch: [35][275/408]	Loss 0.0081 (0.0418)	
training:	Epoch: [35][276/408]	Loss 0.0090 (0.0417)	
training:	Epoch: [35][277/408]	Loss 0.0083 (0.0415)	
training:	Epoch: [35][278/408]	Loss 0.0073 (0.0414)	
training:	Epoch: [35][279/408]	Loss 0.0084 (0.0413)	
training:	Epoch: [35][280/408]	Loss 0.0084 (0.0412)	
training:	Epoch: [35][281/408]	Loss 0.0075 (0.0411)	
training:	Epoch: [35][282/408]	Loss 0.0089 (0.0409)	
training:	Epoch: [35][283/408]	Loss 0.0113 (0.0408)	
training:	Epoch: [35][284/408]	Loss 0.0077 (0.0407)	
training:	Epoch: [35][285/408]	Loss 0.0105 (0.0406)	
training:	Epoch: [35][286/408]	Loss 0.0077 (0.0405)	
training:	Epoch: [35][287/408]	Loss 0.0088 (0.0404)	
training:	Epoch: [35][288/408]	Loss 0.0084 (0.0403)	
training:	Epoch: [35][289/408]	Loss 0.0081 (0.0402)	
training:	Epoch: [35][290/408]	Loss 0.0089 (0.0401)	
training:	Epoch: [35][291/408]	Loss 0.0076 (0.0400)	
training:	Epoch: [35][292/408]	Loss 0.0093 (0.0398)	
training:	Epoch: [35][293/408]	Loss 0.0090 (0.0397)	
training:	Epoch: [35][294/408]	Loss 0.0070 (0.0396)	
training:	Epoch: [35][295/408]	Loss 0.0111 (0.0395)	
training:	Epoch: [35][296/408]	Loss 0.0091 (0.0394)	
training:	Epoch: [35][297/408]	Loss 0.0071 (0.0393)	
training:	Epoch: [35][298/408]	Loss 0.0075 (0.0392)	
training:	Epoch: [35][299/408]	Loss 0.0070 (0.0391)	
training:	Epoch: [35][300/408]	Loss 0.0073 (0.0390)	
training:	Epoch: [35][301/408]	Loss 0.0072 (0.0389)	
training:	Epoch: [35][302/408]	Loss 0.0074 (0.0388)	
training:	Epoch: [35][303/408]	Loss 0.0096 (0.0387)	
training:	Epoch: [35][304/408]	Loss 0.0075 (0.0386)	
training:	Epoch: [35][305/408]	Loss 0.0079 (0.0385)	
training:	Epoch: [35][306/408]	Loss 0.0087 (0.0384)	
training:	Epoch: [35][307/408]	Loss 0.0076 (0.0383)	
training:	Epoch: [35][308/408]	Loss 0.0087 (0.0382)	
training:	Epoch: [35][309/408]	Loss 0.0076 (0.0381)	
training:	Epoch: [35][310/408]	Loss 0.0109 (0.0380)	
training:	Epoch: [35][311/408]	Loss 0.0070 (0.0379)	
training:	Epoch: [35][312/408]	Loss 0.0085 (0.0378)	
training:	Epoch: [35][313/408]	Loss 0.0083 (0.0377)	
training:	Epoch: [35][314/408]	Loss 0.0067 (0.0376)	
training:	Epoch: [35][315/408]	Loss 0.0087 (0.0375)	
training:	Epoch: [35][316/408]	Loss 0.0074 (0.0374)	
training:	Epoch: [35][317/408]	Loss 0.0074 (0.0373)	
training:	Epoch: [35][318/408]	Loss 0.0062 (0.0372)	
training:	Epoch: [35][319/408]	Loss 0.0083 (0.0372)	
training:	Epoch: [35][320/408]	Loss 0.0318 (0.0371)	
training:	Epoch: [35][321/408]	Loss 0.0075 (0.0370)	
training:	Epoch: [35][322/408]	Loss 0.2880 (0.0378)	
training:	Epoch: [35][323/408]	Loss 0.0105 (0.0377)	
training:	Epoch: [35][324/408]	Loss 0.0079 (0.0376)	
training:	Epoch: [35][325/408]	Loss 0.0075 (0.0376)	
training:	Epoch: [35][326/408]	Loss 0.0079 (0.0375)	
training:	Epoch: [35][327/408]	Loss 0.0076 (0.0374)	
training:	Epoch: [35][328/408]	Loss 0.0071 (0.0373)	
training:	Epoch: [35][329/408]	Loss 0.0064 (0.0372)	
training:	Epoch: [35][330/408]	Loss 0.0074 (0.0371)	
training:	Epoch: [35][331/408]	Loss 0.0079 (0.0370)	
training:	Epoch: [35][332/408]	Loss 0.0472 (0.0370)	
training:	Epoch: [35][333/408]	Loss 0.0118 (0.0370)	
training:	Epoch: [35][334/408]	Loss 0.0072 (0.0369)	
training:	Epoch: [35][335/408]	Loss 0.0076 (0.0368)	
training:	Epoch: [35][336/408]	Loss 0.0087 (0.0367)	
training:	Epoch: [35][337/408]	Loss 0.0069 (0.0366)	
training:	Epoch: [35][338/408]	Loss 0.0073 (0.0365)	
training:	Epoch: [35][339/408]	Loss 0.0071 (0.0364)	
training:	Epoch: [35][340/408]	Loss 0.0076 (0.0364)	
training:	Epoch: [35][341/408]	Loss 0.0075 (0.0363)	
training:	Epoch: [35][342/408]	Loss 0.0074 (0.0362)	
training:	Epoch: [35][343/408]	Loss 0.0072 (0.0361)	
training:	Epoch: [35][344/408]	Loss 0.0073 (0.0360)	
training:	Epoch: [35][345/408]	Loss 0.0072 (0.0359)	
training:	Epoch: [35][346/408]	Loss 0.0074 (0.0359)	
training:	Epoch: [35][347/408]	Loss 0.0060 (0.0358)	
training:	Epoch: [35][348/408]	Loss 0.0065 (0.0357)	
training:	Epoch: [35][349/408]	Loss 0.0077 (0.0356)	
training:	Epoch: [35][350/408]	Loss 0.0100 (0.0355)	
training:	Epoch: [35][351/408]	Loss 0.0067 (0.0354)	
training:	Epoch: [35][352/408]	Loss 0.6293 (0.0371)	
training:	Epoch: [35][353/408]	Loss 0.0072 (0.0371)	
training:	Epoch: [35][354/408]	Loss 0.0695 (0.0371)	
training:	Epoch: [35][355/408]	Loss 0.0094 (0.0371)	
training:	Epoch: [35][356/408]	Loss 0.0071 (0.0370)	
training:	Epoch: [35][357/408]	Loss 0.0107 (0.0369)	
training:	Epoch: [35][358/408]	Loss 0.0140 (0.0368)	
training:	Epoch: [35][359/408]	Loss 0.0569 (0.0369)	
training:	Epoch: [35][360/408]	Loss 0.0074 (0.0368)	
training:	Epoch: [35][361/408]	Loss 0.0084 (0.0367)	
training:	Epoch: [35][362/408]	Loss 0.3221 (0.0375)	
training:	Epoch: [35][363/408]	Loss 0.0233 (0.0375)	
training:	Epoch: [35][364/408]	Loss 0.0692 (0.0376)	
training:	Epoch: [35][365/408]	Loss 0.0083 (0.0375)	
training:	Epoch: [35][366/408]	Loss 0.0071 (0.0374)	
training:	Epoch: [35][367/408]	Loss 0.0079 (0.0373)	
training:	Epoch: [35][368/408]	Loss 0.0081 (0.0373)	
training:	Epoch: [35][369/408]	Loss 0.3118 (0.0380)	
training:	Epoch: [35][370/408]	Loss 0.0073 (0.0379)	
training:	Epoch: [35][371/408]	Loss 0.0072 (0.0378)	
training:	Epoch: [35][372/408]	Loss 0.0084 (0.0377)	
training:	Epoch: [35][373/408]	Loss 0.0077 (0.0377)	
training:	Epoch: [35][374/408]	Loss 0.0076 (0.0376)	
training:	Epoch: [35][375/408]	Loss 0.0076 (0.0375)	
training:	Epoch: [35][376/408]	Loss 0.0067 (0.0374)	
training:	Epoch: [35][377/408]	Loss 0.2869 (0.0381)	
training:	Epoch: [35][378/408]	Loss 0.0066 (0.0380)	
training:	Epoch: [35][379/408]	Loss 0.0182 (0.0380)	
training:	Epoch: [35][380/408]	Loss 0.0078 (0.0379)	
training:	Epoch: [35][381/408]	Loss 0.0111 (0.0378)	
training:	Epoch: [35][382/408]	Loss 0.2875 (0.0385)	
training:	Epoch: [35][383/408]	Loss 0.0063 (0.0384)	
training:	Epoch: [35][384/408]	Loss 0.5966 (0.0398)	
training:	Epoch: [35][385/408]	Loss 0.0073 (0.0397)	
training:	Epoch: [35][386/408]	Loss 0.0066 (0.0397)	
training:	Epoch: [35][387/408]	Loss 0.0067 (0.0396)	
training:	Epoch: [35][388/408]	Loss 0.2944 (0.0402)	
training:	Epoch: [35][389/408]	Loss 0.0070 (0.0401)	
training:	Epoch: [35][390/408]	Loss 0.2353 (0.0406)	
training:	Epoch: [35][391/408]	Loss 0.0089 (0.0406)	
training:	Epoch: [35][392/408]	Loss 0.0085 (0.0405)	
training:	Epoch: [35][393/408]	Loss 0.0073 (0.0404)	
training:	Epoch: [35][394/408]	Loss 0.0079 (0.0403)	
training:	Epoch: [35][395/408]	Loss 0.0083 (0.0402)	
training:	Epoch: [35][396/408]	Loss 0.0072 (0.0401)	
training:	Epoch: [35][397/408]	Loss 0.0069 (0.0401)	
training:	Epoch: [35][398/408]	Loss 0.0084 (0.0400)	
training:	Epoch: [35][399/408]	Loss 0.0063 (0.0399)	
training:	Epoch: [35][400/408]	Loss 0.0093 (0.0398)	
training:	Epoch: [35][401/408]	Loss 0.0068 (0.0397)	
training:	Epoch: [35][402/408]	Loss 0.0081 (0.0397)	
training:	Epoch: [35][403/408]	Loss 0.0065 (0.0396)	
training:	Epoch: [35][404/408]	Loss 0.0079 (0.0395)	
training:	Epoch: [35][405/408]	Loss 0.0069 (0.0394)	
training:	Epoch: [35][406/408]	Loss 0.5524 (0.0407)	
training:	Epoch: [35][407/408]	Loss 0.0074 (0.0406)	
training:	Epoch: [35][408/408]	Loss 0.2576 (0.0411)	
Training:	 Loss: 0.0411

Training:	 ACC: 0.9933 0.9933 0.9929 0.9936
Validation:	 ACC: 0.7870 0.7881 0.8106 0.7635
Validation:	 Best_BACC: 0.8226 0.8250 0.8772 0.7679
Validation:	 Loss: 0.9394
Pretraining:	Epoch 36/200
----------
training:	Epoch: [36][1/408]	Loss 0.0087 (0.0087)	
training:	Epoch: [36][2/408]	Loss 0.0080 (0.0084)	
training:	Epoch: [36][3/408]	Loss 0.0100 (0.0089)	
training:	Epoch: [36][4/408]	Loss 0.0074 (0.0085)	
training:	Epoch: [36][5/408]	Loss 0.0081 (0.0085)	
training:	Epoch: [36][6/408]	Loss 0.0076 (0.0083)	
training:	Epoch: [36][7/408]	Loss 0.0079 (0.0083)	
training:	Epoch: [36][8/408]	Loss 0.0092 (0.0084)	
training:	Epoch: [36][9/408]	Loss 0.0071 (0.0082)	
training:	Epoch: [36][10/408]	Loss 0.0095 (0.0084)	
training:	Epoch: [36][11/408]	Loss 0.0075 (0.0083)	
training:	Epoch: [36][12/408]	Loss 0.0080 (0.0083)	
training:	Epoch: [36][13/408]	Loss 0.0082 (0.0083)	
training:	Epoch: [36][14/408]	Loss 0.0085 (0.0083)	
training:	Epoch: [36][15/408]	Loss 0.0090 (0.0083)	
training:	Epoch: [36][16/408]	Loss 0.0070 (0.0082)	
training:	Epoch: [36][17/408]	Loss 0.0082 (0.0082)	
training:	Epoch: [36][18/408]	Loss 0.3109 (0.0250)	
training:	Epoch: [36][19/408]	Loss 0.2960 (0.0393)	
training:	Epoch: [36][20/408]	Loss 0.0082 (0.0378)	
training:	Epoch: [36][21/408]	Loss 0.0105 (0.0365)	
training:	Epoch: [36][22/408]	Loss 0.0089 (0.0352)	
training:	Epoch: [36][23/408]	Loss 0.0092 (0.0341)	
training:	Epoch: [36][24/408]	Loss 0.3271 (0.0463)	
training:	Epoch: [36][25/408]	Loss 0.0103 (0.0448)	
training:	Epoch: [36][26/408]	Loss 0.0069 (0.0434)	
training:	Epoch: [36][27/408]	Loss 0.3162 (0.0535)	
training:	Epoch: [36][28/408]	Loss 0.0071 (0.0518)	
training:	Epoch: [36][29/408]	Loss 0.0081 (0.0503)	
training:	Epoch: [36][30/408]	Loss 0.0100 (0.0490)	
training:	Epoch: [36][31/408]	Loss 0.0083 (0.0477)	
training:	Epoch: [36][32/408]	Loss 0.0074 (0.0464)	
training:	Epoch: [36][33/408]	Loss 0.0070 (0.0452)	
training:	Epoch: [36][34/408]	Loss 0.0092 (0.0442)	
training:	Epoch: [36][35/408]	Loss 0.0080 (0.0431)	
training:	Epoch: [36][36/408]	Loss 0.0087 (0.0422)	
training:	Epoch: [36][37/408]	Loss 0.0080 (0.0412)	
training:	Epoch: [36][38/408]	Loss 0.0082 (0.0404)	
training:	Epoch: [36][39/408]	Loss 0.2861 (0.0467)	
training:	Epoch: [36][40/408]	Loss 0.2958 (0.0529)	
training:	Epoch: [36][41/408]	Loss 0.0073 (0.0518)	
training:	Epoch: [36][42/408]	Loss 0.0088 (0.0508)	
training:	Epoch: [36][43/408]	Loss 0.0082 (0.0498)	
training:	Epoch: [36][44/408]	Loss 0.0073 (0.0488)	
training:	Epoch: [36][45/408]	Loss 0.0076 (0.0479)	
training:	Epoch: [36][46/408]	Loss 0.0118 (0.0471)	
training:	Epoch: [36][47/408]	Loss 0.0098 (0.0463)	
training:	Epoch: [36][48/408]	Loss 0.0077 (0.0455)	
training:	Epoch: [36][49/408]	Loss 0.0080 (0.0447)	
training:	Epoch: [36][50/408]	Loss 0.0080 (0.0440)	
training:	Epoch: [36][51/408]	Loss 0.0083 (0.0433)	
training:	Epoch: [36][52/408]	Loss 0.0096 (0.0427)	
training:	Epoch: [36][53/408]	Loss 0.0074 (0.0420)	
training:	Epoch: [36][54/408]	Loss 0.0066 (0.0413)	
training:	Epoch: [36][55/408]	Loss 0.0088 (0.0407)	
training:	Epoch: [36][56/408]	Loss 0.0099 (0.0402)	
training:	Epoch: [36][57/408]	Loss 0.0067 (0.0396)	
training:	Epoch: [36][58/408]	Loss 0.0206 (0.0393)	
training:	Epoch: [36][59/408]	Loss 0.0084 (0.0388)	
training:	Epoch: [36][60/408]	Loss 0.0083 (0.0383)	
training:	Epoch: [36][61/408]	Loss 0.0077 (0.0378)	
training:	Epoch: [36][62/408]	Loss 0.0078 (0.0373)	
training:	Epoch: [36][63/408]	Loss 0.0082 (0.0368)	
training:	Epoch: [36][64/408]	Loss 0.0082 (0.0364)	
training:	Epoch: [36][65/408]	Loss 0.0093 (0.0359)	
training:	Epoch: [36][66/408]	Loss 0.0081 (0.0355)	
training:	Epoch: [36][67/408]	Loss 0.0084 (0.0351)	
training:	Epoch: [36][68/408]	Loss 0.0083 (0.0347)	
training:	Epoch: [36][69/408]	Loss 0.0091 (0.0343)	
training:	Epoch: [36][70/408]	Loss 0.0078 (0.0340)	
training:	Epoch: [36][71/408]	Loss 0.0077 (0.0336)	
training:	Epoch: [36][72/408]	Loss 0.0082 (0.0332)	
training:	Epoch: [36][73/408]	Loss 0.0073 (0.0329)	
training:	Epoch: [36][74/408]	Loss 0.0084 (0.0326)	
training:	Epoch: [36][75/408]	Loss 0.2952 (0.0361)	
training:	Epoch: [36][76/408]	Loss 0.2630 (0.0390)	
training:	Epoch: [36][77/408]	Loss 0.0071 (0.0386)	
training:	Epoch: [36][78/408]	Loss 0.0073 (0.0382)	
training:	Epoch: [36][79/408]	Loss 0.0075 (0.0378)	
training:	Epoch: [36][80/408]	Loss 0.0093 (0.0375)	
training:	Epoch: [36][81/408]	Loss 0.0086 (0.0371)	
training:	Epoch: [36][82/408]	Loss 0.0091 (0.0368)	
training:	Epoch: [36][83/408]	Loss 0.0067 (0.0364)	
training:	Epoch: [36][84/408]	Loss 0.0097 (0.0361)	
training:	Epoch: [36][85/408]	Loss 0.0096 (0.0358)	
training:	Epoch: [36][86/408]	Loss 0.2970 (0.0388)	
training:	Epoch: [36][87/408]	Loss 0.0104 (0.0385)	
training:	Epoch: [36][88/408]	Loss 0.0074 (0.0382)	
training:	Epoch: [36][89/408]	Loss 0.0423 (0.0382)	
training:	Epoch: [36][90/408]	Loss 0.0087 (0.0379)	
training:	Epoch: [36][91/408]	Loss 0.0095 (0.0376)	
training:	Epoch: [36][92/408]	Loss 0.0093 (0.0373)	
training:	Epoch: [36][93/408]	Loss 0.0073 (0.0369)	
training:	Epoch: [36][94/408]	Loss 0.0088 (0.0366)	
training:	Epoch: [36][95/408]	Loss 0.0086 (0.0363)	
training:	Epoch: [36][96/408]	Loss 0.0075 (0.0360)	
training:	Epoch: [36][97/408]	Loss 0.0081 (0.0357)	
training:	Epoch: [36][98/408]	Loss 0.0078 (0.0355)	
training:	Epoch: [36][99/408]	Loss 0.2965 (0.0381)	
training:	Epoch: [36][100/408]	Loss 0.0080 (0.0378)	
training:	Epoch: [36][101/408]	Loss 0.0078 (0.0375)	
training:	Epoch: [36][102/408]	Loss 0.0071 (0.0372)	
training:	Epoch: [36][103/408]	Loss 0.0086 (0.0369)	
training:	Epoch: [36][104/408]	Loss 0.0092 (0.0367)	
training:	Epoch: [36][105/408]	Loss 0.1541 (0.0378)	
training:	Epoch: [36][106/408]	Loss 0.0079 (0.0375)	
training:	Epoch: [36][107/408]	Loss 0.0073 (0.0372)	
training:	Epoch: [36][108/408]	Loss 0.0067 (0.0369)	
training:	Epoch: [36][109/408]	Loss 0.0098 (0.0367)	
training:	Epoch: [36][110/408]	Loss 0.0073 (0.0364)	
training:	Epoch: [36][111/408]	Loss 0.0079 (0.0362)	
training:	Epoch: [36][112/408]	Loss 0.0071 (0.0359)	
training:	Epoch: [36][113/408]	Loss 0.0085 (0.0357)	
training:	Epoch: [36][114/408]	Loss 0.0091 (0.0354)	
training:	Epoch: [36][115/408]	Loss 0.0087 (0.0352)	
training:	Epoch: [36][116/408]	Loss 0.2396 (0.0370)	
training:	Epoch: [36][117/408]	Loss 0.0079 (0.0367)	
training:	Epoch: [36][118/408]	Loss 0.0099 (0.0365)	
training:	Epoch: [36][119/408]	Loss 0.0085 (0.0362)	
training:	Epoch: [36][120/408]	Loss 0.0073 (0.0360)	
training:	Epoch: [36][121/408]	Loss 0.0089 (0.0358)	
training:	Epoch: [36][122/408]	Loss 0.0082 (0.0355)	
training:	Epoch: [36][123/408]	Loss 0.0095 (0.0353)	
training:	Epoch: [36][124/408]	Loss 0.0075 (0.0351)	
training:	Epoch: [36][125/408]	Loss 0.2998 (0.0372)	
training:	Epoch: [36][126/408]	Loss 0.0220 (0.0371)	
training:	Epoch: [36][127/408]	Loss 0.0245 (0.0370)	
training:	Epoch: [36][128/408]	Loss 0.0091 (0.0368)	
training:	Epoch: [36][129/408]	Loss 0.0087 (0.0366)	
training:	Epoch: [36][130/408]	Loss 0.0081 (0.0364)	
training:	Epoch: [36][131/408]	Loss 0.0094 (0.0362)	
training:	Epoch: [36][132/408]	Loss 0.0080 (0.0359)	
training:	Epoch: [36][133/408]	Loss 0.0097 (0.0357)	
training:	Epoch: [36][134/408]	Loss 0.0124 (0.0356)	
training:	Epoch: [36][135/408]	Loss 0.0095 (0.0354)	
training:	Epoch: [36][136/408]	Loss 0.0082 (0.0352)	
training:	Epoch: [36][137/408]	Loss 0.0069 (0.0350)	
training:	Epoch: [36][138/408]	Loss 0.0077 (0.0348)	
training:	Epoch: [36][139/408]	Loss 0.0077 (0.0346)	
training:	Epoch: [36][140/408]	Loss 0.0086 (0.0344)	
training:	Epoch: [36][141/408]	Loss 0.0069 (0.0342)	
training:	Epoch: [36][142/408]	Loss 0.0113 (0.0340)	
training:	Epoch: [36][143/408]	Loss 0.0086 (0.0339)	
training:	Epoch: [36][144/408]	Loss 0.0063 (0.0337)	
training:	Epoch: [36][145/408]	Loss 0.0072 (0.0335)	
training:	Epoch: [36][146/408]	Loss 0.0168 (0.0334)	
training:	Epoch: [36][147/408]	Loss 0.2823 (0.0351)	
training:	Epoch: [36][148/408]	Loss 0.0123 (0.0349)	
training:	Epoch: [36][149/408]	Loss 0.0071 (0.0347)	
training:	Epoch: [36][150/408]	Loss 0.0076 (0.0345)	
training:	Epoch: [36][151/408]	Loss 0.0085 (0.0344)	
training:	Epoch: [36][152/408]	Loss 0.0073 (0.0342)	
training:	Epoch: [36][153/408]	Loss 0.0069 (0.0340)	
training:	Epoch: [36][154/408]	Loss 0.2826 (0.0356)	
training:	Epoch: [36][155/408]	Loss 0.0098 (0.0355)	
training:	Epoch: [36][156/408]	Loss 0.0071 (0.0353)	
training:	Epoch: [36][157/408]	Loss 0.0071 (0.0351)	
training:	Epoch: [36][158/408]	Loss 0.0077 (0.0349)	
training:	Epoch: [36][159/408]	Loss 0.0080 (0.0348)	
training:	Epoch: [36][160/408]	Loss 0.0108 (0.0346)	
training:	Epoch: [36][161/408]	Loss 0.0080 (0.0344)	
training:	Epoch: [36][162/408]	Loss 0.0078 (0.0343)	
training:	Epoch: [36][163/408]	Loss 0.0068 (0.0341)	
training:	Epoch: [36][164/408]	Loss 0.0080 (0.0339)	
training:	Epoch: [36][165/408]	Loss 0.0071 (0.0338)	
training:	Epoch: [36][166/408]	Loss 0.0074 (0.0336)	
training:	Epoch: [36][167/408]	Loss 0.0086 (0.0335)	
training:	Epoch: [36][168/408]	Loss 0.0075 (0.0333)	
training:	Epoch: [36][169/408]	Loss 0.0076 (0.0332)	
training:	Epoch: [36][170/408]	Loss 0.0083 (0.0330)	
training:	Epoch: [36][171/408]	Loss 0.0080 (0.0329)	
training:	Epoch: [36][172/408]	Loss 0.0072 (0.0327)	
training:	Epoch: [36][173/408]	Loss 0.0082 (0.0326)	
training:	Epoch: [36][174/408]	Loss 0.0072 (0.0324)	
training:	Epoch: [36][175/408]	Loss 0.0068 (0.0323)	
training:	Epoch: [36][176/408]	Loss 0.0082 (0.0322)	
training:	Epoch: [36][177/408]	Loss 0.0073 (0.0320)	
training:	Epoch: [36][178/408]	Loss 0.0075 (0.0319)	
training:	Epoch: [36][179/408]	Loss 0.0075 (0.0317)	
training:	Epoch: [36][180/408]	Loss 0.0079 (0.0316)	
training:	Epoch: [36][181/408]	Loss 0.0081 (0.0315)	
training:	Epoch: [36][182/408]	Loss 0.2985 (0.0329)	
training:	Epoch: [36][183/408]	Loss 0.0066 (0.0328)	
training:	Epoch: [36][184/408]	Loss 0.0063 (0.0327)	
training:	Epoch: [36][185/408]	Loss 0.0080 (0.0325)	
training:	Epoch: [36][186/408]	Loss 0.0076 (0.0324)	
training:	Epoch: [36][187/408]	Loss 0.0370 (0.0324)	
training:	Epoch: [36][188/408]	Loss 0.0069 (0.0323)	
training:	Epoch: [36][189/408]	Loss 0.0083 (0.0322)	
training:	Epoch: [36][190/408]	Loss 0.0077 (0.0320)	
training:	Epoch: [36][191/408]	Loss 0.0069 (0.0319)	
training:	Epoch: [36][192/408]	Loss 0.0120 (0.0318)	
training:	Epoch: [36][193/408]	Loss 0.0072 (0.0317)	
training:	Epoch: [36][194/408]	Loss 0.0867 (0.0319)	
training:	Epoch: [36][195/408]	Loss 0.0075 (0.0318)	
training:	Epoch: [36][196/408]	Loss 0.0072 (0.0317)	
training:	Epoch: [36][197/408]	Loss 0.3036 (0.0331)	
training:	Epoch: [36][198/408]	Loss 0.0070 (0.0329)	
training:	Epoch: [36][199/408]	Loss 0.0079 (0.0328)	
training:	Epoch: [36][200/408]	Loss 0.2990 (0.0342)	
training:	Epoch: [36][201/408]	Loss 0.0070 (0.0340)	
training:	Epoch: [36][202/408]	Loss 0.0092 (0.0339)	
training:	Epoch: [36][203/408]	Loss 0.0080 (0.0338)	
training:	Epoch: [36][204/408]	Loss 0.0075 (0.0336)	
training:	Epoch: [36][205/408]	Loss 0.2919 (0.0349)	
training:	Epoch: [36][206/408]	Loss 0.0090 (0.0348)	
training:	Epoch: [36][207/408]	Loss 0.0063 (0.0346)	
training:	Epoch: [36][208/408]	Loss 0.0075 (0.0345)	
training:	Epoch: [36][209/408]	Loss 0.0121 (0.0344)	
training:	Epoch: [36][210/408]	Loss 0.2738 (0.0355)	
training:	Epoch: [36][211/408]	Loss 0.2785 (0.0367)	
training:	Epoch: [36][212/408]	Loss 0.0080 (0.0366)	
training:	Epoch: [36][213/408]	Loss 0.0068 (0.0364)	
training:	Epoch: [36][214/408]	Loss 0.0078 (0.0363)	
training:	Epoch: [36][215/408]	Loss 0.0083 (0.0361)	
training:	Epoch: [36][216/408]	Loss 0.0071 (0.0360)	
training:	Epoch: [36][217/408]	Loss 0.0068 (0.0359)	
training:	Epoch: [36][218/408]	Loss 0.0074 (0.0357)	
training:	Epoch: [36][219/408]	Loss 0.0102 (0.0356)	
training:	Epoch: [36][220/408]	Loss 0.0140 (0.0355)	
training:	Epoch: [36][221/408]	Loss 0.0081 (0.0354)	
training:	Epoch: [36][222/408]	Loss 0.0080 (0.0353)	
training:	Epoch: [36][223/408]	Loss 0.3090 (0.0365)	
training:	Epoch: [36][224/408]	Loss 0.0099 (0.0364)	
training:	Epoch: [36][225/408]	Loss 0.0455 (0.0364)	
training:	Epoch: [36][226/408]	Loss 0.0082 (0.0363)	
training:	Epoch: [36][227/408]	Loss 0.0077 (0.0362)	
training:	Epoch: [36][228/408]	Loss 0.0099 (0.0361)	
training:	Epoch: [36][229/408]	Loss 0.0070 (0.0359)	
training:	Epoch: [36][230/408]	Loss 0.0074 (0.0358)	
training:	Epoch: [36][231/408]	Loss 0.0072 (0.0357)	
training:	Epoch: [36][232/408]	Loss 0.0085 (0.0356)	
training:	Epoch: [36][233/408]	Loss 0.0100 (0.0355)	
training:	Epoch: [36][234/408]	Loss 0.0074 (0.0353)	
training:	Epoch: [36][235/408]	Loss 0.0071 (0.0352)	
training:	Epoch: [36][236/408]	Loss 0.0072 (0.0351)	
training:	Epoch: [36][237/408]	Loss 0.3214 (0.0363)	
training:	Epoch: [36][238/408]	Loss 0.2986 (0.0374)	
training:	Epoch: [36][239/408]	Loss 0.0084 (0.0373)	
training:	Epoch: [36][240/408]	Loss 0.2836 (0.0383)	
training:	Epoch: [36][241/408]	Loss 0.0092 (0.0382)	
training:	Epoch: [36][242/408]	Loss 0.0148 (0.0381)	
training:	Epoch: [36][243/408]	Loss 0.0071 (0.0380)	
training:	Epoch: [36][244/408]	Loss 0.0077 (0.0379)	
training:	Epoch: [36][245/408]	Loss 0.0072 (0.0377)	
training:	Epoch: [36][246/408]	Loss 0.0092 (0.0376)	
training:	Epoch: [36][247/408]	Loss 0.0072 (0.0375)	
training:	Epoch: [36][248/408]	Loss 0.0067 (0.0374)	
training:	Epoch: [36][249/408]	Loss 0.0082 (0.0372)	
training:	Epoch: [36][250/408]	Loss 0.0079 (0.0371)	
training:	Epoch: [36][251/408]	Loss 0.0072 (0.0370)	
training:	Epoch: [36][252/408]	Loss 0.0072 (0.0369)	
training:	Epoch: [36][253/408]	Loss 0.0105 (0.0368)	
training:	Epoch: [36][254/408]	Loss 0.3093 (0.0379)	
training:	Epoch: [36][255/408]	Loss 0.0071 (0.0377)	
training:	Epoch: [36][256/408]	Loss 0.2260 (0.0385)	
training:	Epoch: [36][257/408]	Loss 0.0069 (0.0384)	
training:	Epoch: [36][258/408]	Loss 0.0087 (0.0382)	
training:	Epoch: [36][259/408]	Loss 0.0071 (0.0381)	
training:	Epoch: [36][260/408]	Loss 0.0144 (0.0380)	
training:	Epoch: [36][261/408]	Loss 0.0074 (0.0379)	
training:	Epoch: [36][262/408]	Loss 0.0095 (0.0378)	
training:	Epoch: [36][263/408]	Loss 0.0081 (0.0377)	
training:	Epoch: [36][264/408]	Loss 0.0072 (0.0376)	
training:	Epoch: [36][265/408]	Loss 0.0114 (0.0375)	
training:	Epoch: [36][266/408]	Loss 0.0074 (0.0374)	
training:	Epoch: [36][267/408]	Loss 0.0096 (0.0373)	
training:	Epoch: [36][268/408]	Loss 0.0064 (0.0371)	
training:	Epoch: [36][269/408]	Loss 0.0083 (0.0370)	
training:	Epoch: [36][270/408]	Loss 0.0081 (0.0369)	
training:	Epoch: [36][271/408]	Loss 0.0076 (0.0368)	
training:	Epoch: [36][272/408]	Loss 0.3048 (0.0378)	
training:	Epoch: [36][273/408]	Loss 0.3084 (0.0388)	
training:	Epoch: [36][274/408]	Loss 0.5579 (0.0407)	
training:	Epoch: [36][275/408]	Loss 0.0096 (0.0406)	
training:	Epoch: [36][276/408]	Loss 0.0083 (0.0405)	
training:	Epoch: [36][277/408]	Loss 0.0062 (0.0403)	
training:	Epoch: [36][278/408]	Loss 0.0233 (0.0403)	
training:	Epoch: [36][279/408]	Loss 0.0075 (0.0402)	
training:	Epoch: [36][280/408]	Loss 0.0072 (0.0400)	
training:	Epoch: [36][281/408]	Loss 0.0079 (0.0399)	
training:	Epoch: [36][282/408]	Loss 0.0103 (0.0398)	
training:	Epoch: [36][283/408]	Loss 0.0087 (0.0397)	
training:	Epoch: [36][284/408]	Loss 0.0100 (0.0396)	
training:	Epoch: [36][285/408]	Loss 0.0077 (0.0395)	
training:	Epoch: [36][286/408]	Loss 0.2924 (0.0404)	
training:	Epoch: [36][287/408]	Loss 0.0070 (0.0403)	
training:	Epoch: [36][288/408]	Loss 0.0100 (0.0402)	
training:	Epoch: [36][289/408]	Loss 0.0086 (0.0400)	
training:	Epoch: [36][290/408]	Loss 0.0091 (0.0399)	
training:	Epoch: [36][291/408]	Loss 0.0101 (0.0398)	
training:	Epoch: [36][292/408]	Loss 0.0099 (0.0397)	
training:	Epoch: [36][293/408]	Loss 0.0080 (0.0396)	
training:	Epoch: [36][294/408]	Loss 0.0091 (0.0395)	
training:	Epoch: [36][295/408]	Loss 0.0090 (0.0394)	
training:	Epoch: [36][296/408]	Loss 0.0450 (0.0394)	
training:	Epoch: [36][297/408]	Loss 0.0083 (0.0393)	
training:	Epoch: [36][298/408]	Loss 0.2725 (0.0401)	
training:	Epoch: [36][299/408]	Loss 0.0068 (0.0400)	
training:	Epoch: [36][300/408]	Loss 0.2551 (0.0407)	
training:	Epoch: [36][301/408]	Loss 0.0081 (0.0406)	
training:	Epoch: [36][302/408]	Loss 0.0092 (0.0405)	
training:	Epoch: [36][303/408]	Loss 0.0094 (0.0404)	
training:	Epoch: [36][304/408]	Loss 0.0084 (0.0403)	
training:	Epoch: [36][305/408]	Loss 0.2960 (0.0411)	
training:	Epoch: [36][306/408]	Loss 0.0085 (0.0410)	
training:	Epoch: [36][307/408]	Loss 0.0096 (0.0409)	
training:	Epoch: [36][308/408]	Loss 0.0086 (0.0408)	
training:	Epoch: [36][309/408]	Loss 0.0132 (0.0407)	
training:	Epoch: [36][310/408]	Loss 0.0096 (0.0406)	
training:	Epoch: [36][311/408]	Loss 0.0082 (0.0405)	
training:	Epoch: [36][312/408]	Loss 0.0081 (0.0404)	
training:	Epoch: [36][313/408]	Loss 0.0096 (0.0403)	
training:	Epoch: [36][314/408]	Loss 0.0108 (0.0402)	
training:	Epoch: [36][315/408]	Loss 0.2708 (0.0410)	
training:	Epoch: [36][316/408]	Loss 0.0073 (0.0409)	
training:	Epoch: [36][317/408]	Loss 0.0102 (0.0408)	
training:	Epoch: [36][318/408]	Loss 0.0108 (0.0407)	
training:	Epoch: [36][319/408]	Loss 0.0083 (0.0406)	
training:	Epoch: [36][320/408]	Loss 0.0081 (0.0405)	
training:	Epoch: [36][321/408]	Loss 0.3177 (0.0413)	
training:	Epoch: [36][322/408]	Loss 0.0089 (0.0412)	
training:	Epoch: [36][323/408]	Loss 0.0088 (0.0411)	
training:	Epoch: [36][324/408]	Loss 0.0083 (0.0410)	
training:	Epoch: [36][325/408]	Loss 0.0087 (0.0409)	
training:	Epoch: [36][326/408]	Loss 0.0112 (0.0408)	
training:	Epoch: [36][327/408]	Loss 0.0068 (0.0407)	
training:	Epoch: [36][328/408]	Loss 0.1146 (0.0410)	
training:	Epoch: [36][329/408]	Loss 0.0103 (0.0409)	
training:	Epoch: [36][330/408]	Loss 0.0079 (0.0408)	
training:	Epoch: [36][331/408]	Loss 0.1090 (0.0410)	
training:	Epoch: [36][332/408]	Loss 0.0197 (0.0409)	
training:	Epoch: [36][333/408]	Loss 0.0082 (0.0408)	
training:	Epoch: [36][334/408]	Loss 0.0079 (0.0407)	
training:	Epoch: [36][335/408]	Loss 0.0128 (0.0406)	
training:	Epoch: [36][336/408]	Loss 0.0152 (0.0406)	
training:	Epoch: [36][337/408]	Loss 0.0103 (0.0405)	
training:	Epoch: [36][338/408]	Loss 0.0093 (0.0404)	
training:	Epoch: [36][339/408]	Loss 0.2901 (0.0411)	
training:	Epoch: [36][340/408]	Loss 0.0526 (0.0411)	
training:	Epoch: [36][341/408]	Loss 0.1536 (0.0415)	
training:	Epoch: [36][342/408]	Loss 0.1851 (0.0419)	
training:	Epoch: [36][343/408]	Loss 0.0135 (0.0418)	
training:	Epoch: [36][344/408]	Loss 0.0149 (0.0417)	
training:	Epoch: [36][345/408]	Loss 0.0083 (0.0416)	
training:	Epoch: [36][346/408]	Loss 0.0083 (0.0415)	
training:	Epoch: [36][347/408]	Loss 0.0076 (0.0414)	
training:	Epoch: [36][348/408]	Loss 0.0075 (0.0413)	
training:	Epoch: [36][349/408]	Loss 0.0090 (0.0412)	
training:	Epoch: [36][350/408]	Loss 0.0080 (0.0412)	
training:	Epoch: [36][351/408]	Loss 0.0106 (0.0411)	
training:	Epoch: [36][352/408]	Loss 0.0091 (0.0410)	
training:	Epoch: [36][353/408]	Loss 0.0090 (0.0409)	
training:	Epoch: [36][354/408]	Loss 0.2600 (0.0415)	
training:	Epoch: [36][355/408]	Loss 0.0106 (0.0414)	
training:	Epoch: [36][356/408]	Loss 0.0553 (0.0415)	
training:	Epoch: [36][357/408]	Loss 0.0121 (0.0414)	
training:	Epoch: [36][358/408]	Loss 0.0129 (0.0413)	
training:	Epoch: [36][359/408]	Loss 0.2888 (0.0420)	
training:	Epoch: [36][360/408]	Loss 0.0088 (0.0419)	
training:	Epoch: [36][361/408]	Loss 0.0081 (0.0418)	
training:	Epoch: [36][362/408]	Loss 0.0071 (0.0417)	
training:	Epoch: [36][363/408]	Loss 0.0085 (0.0416)	
training:	Epoch: [36][364/408]	Loss 0.1033 (0.0418)	
training:	Epoch: [36][365/408]	Loss 0.2802 (0.0424)	
training:	Epoch: [36][366/408]	Loss 0.0075 (0.0423)	
training:	Epoch: [36][367/408]	Loss 0.0086 (0.0422)	
training:	Epoch: [36][368/408]	Loss 0.0077 (0.0422)	
training:	Epoch: [36][369/408]	Loss 0.0088 (0.0421)	
training:	Epoch: [36][370/408]	Loss 0.0175 (0.0420)	
training:	Epoch: [36][371/408]	Loss 0.0090 (0.0419)	
training:	Epoch: [36][372/408]	Loss 0.0089 (0.0418)	
training:	Epoch: [36][373/408]	Loss 0.0109 (0.0417)	
training:	Epoch: [36][374/408]	Loss 0.0295 (0.0417)	
training:	Epoch: [36][375/408]	Loss 0.0563 (0.0417)	
training:	Epoch: [36][376/408]	Loss 0.0080 (0.0416)	
training:	Epoch: [36][377/408]	Loss 0.0102 (0.0416)	
training:	Epoch: [36][378/408]	Loss 0.0076 (0.0415)	
training:	Epoch: [36][379/408]	Loss 0.0083 (0.0414)	
training:	Epoch: [36][380/408]	Loss 0.2765 (0.0420)	
training:	Epoch: [36][381/408]	Loss 0.2558 (0.0426)	
training:	Epoch: [36][382/408]	Loss 0.0080 (0.0425)	
training:	Epoch: [36][383/408]	Loss 0.0706 (0.0426)	
training:	Epoch: [36][384/408]	Loss 0.0087 (0.0425)	
training:	Epoch: [36][385/408]	Loss 0.1885 (0.0428)	
training:	Epoch: [36][386/408]	Loss 0.0078 (0.0428)	
training:	Epoch: [36][387/408]	Loss 0.0074 (0.0427)	
training:	Epoch: [36][388/408]	Loss 0.0087 (0.0426)	
training:	Epoch: [36][389/408]	Loss 0.0085 (0.0425)	
training:	Epoch: [36][390/408]	Loss 0.0076 (0.0424)	
training:	Epoch: [36][391/408]	Loss 0.2699 (0.0430)	
training:	Epoch: [36][392/408]	Loss 0.0078 (0.0429)	
training:	Epoch: [36][393/408]	Loss 0.0077 (0.0428)	
training:	Epoch: [36][394/408]	Loss 0.0097 (0.0427)	
training:	Epoch: [36][395/408]	Loss 0.0088 (0.0426)	
training:	Epoch: [36][396/408]	Loss 0.0088 (0.0425)	
training:	Epoch: [36][397/408]	Loss 0.0448 (0.0425)	
training:	Epoch: [36][398/408]	Loss 0.0098 (0.0425)	
training:	Epoch: [36][399/408]	Loss 0.0079 (0.0424)	
training:	Epoch: [36][400/408]	Loss 0.0085 (0.0423)	
training:	Epoch: [36][401/408]	Loss 0.0100 (0.0422)	
training:	Epoch: [36][402/408]	Loss 0.0088 (0.0421)	
training:	Epoch: [36][403/408]	Loss 0.0088 (0.0420)	
training:	Epoch: [36][404/408]	Loss 0.0069 (0.0420)	
training:	Epoch: [36][405/408]	Loss 0.0080 (0.0419)	
training:	Epoch: [36][406/408]	Loss 0.0079 (0.0418)	
training:	Epoch: [36][407/408]	Loss 0.0086 (0.0417)	
training:	Epoch: [36][408/408]	Loss 0.0096 (0.0416)	
Training:	 Loss: 0.0416

Training:	 ACC: 0.9920 0.9919 0.9903 0.9936
Validation:	 ACC: 0.7879 0.7876 0.7810 0.7948
Validation:	 Best_BACC: 0.8226 0.8250 0.8772 0.7679
Validation:	 Loss: 0.9215
Pretraining:	Epoch 37/200
----------
training:	Epoch: [37][1/408]	Loss 0.0088 (0.0088)	
training:	Epoch: [37][2/408]	Loss 0.0086 (0.0087)	
training:	Epoch: [37][3/408]	Loss 0.0090 (0.0088)	
training:	Epoch: [37][4/408]	Loss 0.0136 (0.0100)	
training:	Epoch: [37][5/408]	Loss 0.0083 (0.0097)	
training:	Epoch: [37][6/408]	Loss 0.0087 (0.0095)	
training:	Epoch: [37][7/408]	Loss 0.0087 (0.0094)	
training:	Epoch: [37][8/408]	Loss 0.0089 (0.0093)	
training:	Epoch: [37][9/408]	Loss 0.0076 (0.0091)	
training:	Epoch: [37][10/408]	Loss 0.0086 (0.0091)	
training:	Epoch: [37][11/408]	Loss 0.0090 (0.0091)	
training:	Epoch: [37][12/408]	Loss 0.0097 (0.0091)	
training:	Epoch: [37][13/408]	Loss 0.0099 (0.0092)	
training:	Epoch: [37][14/408]	Loss 0.0085 (0.0091)	
training:	Epoch: [37][15/408]	Loss 0.0081 (0.0091)	
training:	Epoch: [37][16/408]	Loss 0.0070 (0.0089)	
training:	Epoch: [37][17/408]	Loss 0.0962 (0.0141)	
training:	Epoch: [37][18/408]	Loss 0.0094 (0.0138)	
training:	Epoch: [37][19/408]	Loss 0.0080 (0.0135)	
training:	Epoch: [37][20/408]	Loss 0.0086 (0.0133)	
training:	Epoch: [37][21/408]	Loss 0.2954 (0.0267)	
training:	Epoch: [37][22/408]	Loss 0.0078 (0.0258)	
training:	Epoch: [37][23/408]	Loss 0.0105 (0.0252)	
training:	Epoch: [37][24/408]	Loss 0.0077 (0.0244)	
training:	Epoch: [37][25/408]	Loss 0.0081 (0.0238)	
training:	Epoch: [37][26/408]	Loss 0.0074 (0.0232)	
training:	Epoch: [37][27/408]	Loss 0.0074 (0.0226)	
training:	Epoch: [37][28/408]	Loss 0.0082 (0.0221)	
training:	Epoch: [37][29/408]	Loss 0.0132 (0.0218)	
training:	Epoch: [37][30/408]	Loss 0.0093 (0.0213)	
training:	Epoch: [37][31/408]	Loss 0.0076 (0.0209)	
training:	Epoch: [37][32/408]	Loss 0.0087 (0.0205)	
training:	Epoch: [37][33/408]	Loss 0.0081 (0.0201)	
training:	Epoch: [37][34/408]	Loss 0.3141 (0.0288)	
training:	Epoch: [37][35/408]	Loss 0.0070 (0.0282)	
training:	Epoch: [37][36/408]	Loss 0.0720 (0.0294)	
training:	Epoch: [37][37/408]	Loss 0.0076 (0.0288)	
training:	Epoch: [37][38/408]	Loss 0.0115 (0.0283)	
training:	Epoch: [37][39/408]	Loss 0.0069 (0.0278)	
training:	Epoch: [37][40/408]	Loss 0.0079 (0.0273)	
training:	Epoch: [37][41/408]	Loss 0.2931 (0.0338)	
training:	Epoch: [37][42/408]	Loss 0.2558 (0.0391)	
training:	Epoch: [37][43/408]	Loss 0.0088 (0.0384)	
training:	Epoch: [37][44/408]	Loss 0.0073 (0.0377)	
training:	Epoch: [37][45/408]	Loss 0.0098 (0.0370)	
training:	Epoch: [37][46/408]	Loss 0.0082 (0.0364)	
training:	Epoch: [37][47/408]	Loss 0.0075 (0.0358)	
training:	Epoch: [37][48/408]	Loss 0.0081 (0.0352)	
training:	Epoch: [37][49/408]	Loss 0.0085 (0.0347)	
training:	Epoch: [37][50/408]	Loss 0.0094 (0.0342)	
training:	Epoch: [37][51/408]	Loss 0.0095 (0.0337)	
training:	Epoch: [37][52/408]	Loss 0.0068 (0.0332)	
training:	Epoch: [37][53/408]	Loss 0.0080 (0.0327)	
training:	Epoch: [37][54/408]	Loss 0.0073 (0.0322)	
training:	Epoch: [37][55/408]	Loss 0.0085 (0.0318)	
training:	Epoch: [37][56/408]	Loss 0.0078 (0.0314)	
training:	Epoch: [37][57/408]	Loss 0.0103 (0.0310)	
training:	Epoch: [37][58/408]	Loss 0.0072 (0.0306)	
training:	Epoch: [37][59/408]	Loss 0.0072 (0.0302)	
training:	Epoch: [37][60/408]	Loss 0.0078 (0.0298)	
training:	Epoch: [37][61/408]	Loss 0.0070 (0.0294)	
training:	Epoch: [37][62/408]	Loss 0.0078 (0.0291)	
training:	Epoch: [37][63/408]	Loss 0.0072 (0.0287)	
training:	Epoch: [37][64/408]	Loss 0.0079 (0.0284)	
training:	Epoch: [37][65/408]	Loss 0.0075 (0.0281)	
training:	Epoch: [37][66/408]	Loss 0.0076 (0.0278)	
training:	Epoch: [37][67/408]	Loss 0.0083 (0.0275)	
training:	Epoch: [37][68/408]	Loss 0.2673 (0.0310)	
training:	Epoch: [37][69/408]	Loss 0.0086 (0.0307)	
training:	Epoch: [37][70/408]	Loss 0.0074 (0.0304)	
training:	Epoch: [37][71/408]	Loss 0.3344 (0.0346)	
training:	Epoch: [37][72/408]	Loss 0.0080 (0.0343)	
training:	Epoch: [37][73/408]	Loss 0.2972 (0.0379)	
training:	Epoch: [37][74/408]	Loss 0.0177 (0.0376)	
training:	Epoch: [37][75/408]	Loss 0.2768 (0.0408)	
training:	Epoch: [37][76/408]	Loss 0.2810 (0.0439)	
training:	Epoch: [37][77/408]	Loss 0.0091 (0.0435)	
training:	Epoch: [37][78/408]	Loss 0.0063 (0.0430)	
training:	Epoch: [37][79/408]	Loss 0.0067 (0.0426)	
training:	Epoch: [37][80/408]	Loss 0.0089 (0.0421)	
training:	Epoch: [37][81/408]	Loss 0.0088 (0.0417)	
training:	Epoch: [37][82/408]	Loss 0.0067 (0.0413)	
training:	Epoch: [37][83/408]	Loss 0.0091 (0.0409)	
training:	Epoch: [37][84/408]	Loss 0.0096 (0.0405)	
training:	Epoch: [37][85/408]	Loss 0.0091 (0.0402)	
training:	Epoch: [37][86/408]	Loss 0.0094 (0.0398)	
training:	Epoch: [37][87/408]	Loss 0.0082 (0.0394)	
training:	Epoch: [37][88/408]	Loss 0.0851 (0.0400)	
training:	Epoch: [37][89/408]	Loss 0.0155 (0.0397)	
training:	Epoch: [37][90/408]	Loss 0.0229 (0.0395)	
training:	Epoch: [37][91/408]	Loss 0.0081 (0.0392)	
training:	Epoch: [37][92/408]	Loss 0.0087 (0.0388)	
training:	Epoch: [37][93/408]	Loss 0.0086 (0.0385)	
training:	Epoch: [37][94/408]	Loss 0.0074 (0.0382)	
training:	Epoch: [37][95/408]	Loss 0.2705 (0.0406)	
training:	Epoch: [37][96/408]	Loss 0.0082 (0.0403)	
training:	Epoch: [37][97/408]	Loss 0.0082 (0.0400)	
training:	Epoch: [37][98/408]	Loss 0.0101 (0.0396)	
training:	Epoch: [37][99/408]	Loss 0.0090 (0.0393)	
training:	Epoch: [37][100/408]	Loss 0.0087 (0.0390)	
training:	Epoch: [37][101/408]	Loss 0.0097 (0.0387)	
training:	Epoch: [37][102/408]	Loss 0.2770 (0.0411)	
training:	Epoch: [37][103/408]	Loss 0.1893 (0.0425)	
training:	Epoch: [37][104/408]	Loss 0.0089 (0.0422)	
training:	Epoch: [37][105/408]	Loss 0.0089 (0.0419)	
training:	Epoch: [37][106/408]	Loss 0.0108 (0.0416)	
training:	Epoch: [37][107/408]	Loss 0.0092 (0.0413)	
training:	Epoch: [37][108/408]	Loss 0.3185 (0.0438)	
training:	Epoch: [37][109/408]	Loss 0.0066 (0.0435)	
training:	Epoch: [37][110/408]	Loss 0.0087 (0.0432)	
training:	Epoch: [37][111/408]	Loss 0.2833 (0.0454)	
training:	Epoch: [37][112/408]	Loss 0.0096 (0.0450)	
training:	Epoch: [37][113/408]	Loss 0.0184 (0.0448)	
training:	Epoch: [37][114/408]	Loss 0.0106 (0.0445)	
training:	Epoch: [37][115/408]	Loss 0.0110 (0.0442)	
training:	Epoch: [37][116/408]	Loss 0.0097 (0.0439)	
training:	Epoch: [37][117/408]	Loss 0.0074 (0.0436)	
training:	Epoch: [37][118/408]	Loss 0.0094 (0.0433)	
training:	Epoch: [37][119/408]	Loss 0.0155 (0.0431)	
training:	Epoch: [37][120/408]	Loss 0.0108 (0.0428)	
training:	Epoch: [37][121/408]	Loss 0.0083 (0.0425)	
training:	Epoch: [37][122/408]	Loss 0.0077 (0.0422)	
training:	Epoch: [37][123/408]	Loss 0.0087 (0.0420)	
training:	Epoch: [37][124/408]	Loss 0.5679 (0.0462)	
training:	Epoch: [37][125/408]	Loss 0.0080 (0.0459)	
training:	Epoch: [37][126/408]	Loss 0.0101 (0.0456)	
training:	Epoch: [37][127/408]	Loss 0.0078 (0.0453)	
training:	Epoch: [37][128/408]	Loss 0.0093 (0.0450)	
training:	Epoch: [37][129/408]	Loss 0.0091 (0.0448)	
training:	Epoch: [37][130/408]	Loss 0.0096 (0.0445)	
training:	Epoch: [37][131/408]	Loss 0.2801 (0.0463)	
training:	Epoch: [37][132/408]	Loss 0.0594 (0.0464)	
training:	Epoch: [37][133/408]	Loss 0.2903 (0.0482)	
training:	Epoch: [37][134/408]	Loss 0.0071 (0.0479)	
training:	Epoch: [37][135/408]	Loss 0.0107 (0.0476)	
training:	Epoch: [37][136/408]	Loss 0.0064 (0.0473)	
training:	Epoch: [37][137/408]	Loss 0.3256 (0.0494)	
training:	Epoch: [37][138/408]	Loss 0.0096 (0.0491)	
training:	Epoch: [37][139/408]	Loss 0.0096 (0.0488)	
training:	Epoch: [37][140/408]	Loss 0.0085 (0.0485)	
training:	Epoch: [37][141/408]	Loss 0.0114 (0.0482)	
training:	Epoch: [37][142/408]	Loss 0.2950 (0.0500)	
training:	Epoch: [37][143/408]	Loss 0.0092 (0.0497)	
training:	Epoch: [37][144/408]	Loss 0.0086 (0.0494)	
training:	Epoch: [37][145/408]	Loss 0.0080 (0.0491)	
training:	Epoch: [37][146/408]	Loss 0.0081 (0.0488)	
training:	Epoch: [37][147/408]	Loss 0.0080 (0.0486)	
training:	Epoch: [37][148/408]	Loss 0.2465 (0.0499)	
training:	Epoch: [37][149/408]	Loss 0.0091 (0.0496)	
training:	Epoch: [37][150/408]	Loss 0.0100 (0.0494)	
training:	Epoch: [37][151/408]	Loss 0.0084 (0.0491)	
training:	Epoch: [37][152/408]	Loss 0.0109 (0.0488)	
training:	Epoch: [37][153/408]	Loss 0.0348 (0.0487)	
training:	Epoch: [37][154/408]	Loss 0.0111 (0.0485)	
training:	Epoch: [37][155/408]	Loss 0.1969 (0.0495)	
training:	Epoch: [37][156/408]	Loss 0.0075 (0.0492)	
training:	Epoch: [37][157/408]	Loss 0.0108 (0.0489)	
training:	Epoch: [37][158/408]	Loss 0.0078 (0.0487)	
training:	Epoch: [37][159/408]	Loss 0.0072 (0.0484)	
training:	Epoch: [37][160/408]	Loss 0.2671 (0.0498)	
training:	Epoch: [37][161/408]	Loss 0.0081 (0.0495)	
training:	Epoch: [37][162/408]	Loss 0.0705 (0.0497)	
training:	Epoch: [37][163/408]	Loss 0.0114 (0.0494)	
training:	Epoch: [37][164/408]	Loss 0.0084 (0.0492)	
training:	Epoch: [37][165/408]	Loss 0.0091 (0.0489)	
training:	Epoch: [37][166/408]	Loss 0.0120 (0.0487)	
training:	Epoch: [37][167/408]	Loss 0.0092 (0.0485)	
training:	Epoch: [37][168/408]	Loss 0.0098 (0.0482)	
training:	Epoch: [37][169/408]	Loss 0.0109 (0.0480)	
training:	Epoch: [37][170/408]	Loss 0.0105 (0.0478)	
training:	Epoch: [37][171/408]	Loss 0.0109 (0.0476)	
training:	Epoch: [37][172/408]	Loss 0.0105 (0.0474)	
training:	Epoch: [37][173/408]	Loss 0.0078 (0.0471)	
training:	Epoch: [37][174/408]	Loss 0.0656 (0.0472)	
training:	Epoch: [37][175/408]	Loss 0.0066 (0.0470)	
training:	Epoch: [37][176/408]	Loss 0.0096 (0.0468)	
training:	Epoch: [37][177/408]	Loss 0.0107 (0.0466)	
training:	Epoch: [37][178/408]	Loss 0.0076 (0.0464)	
training:	Epoch: [37][179/408]	Loss 0.0077 (0.0462)	
training:	Epoch: [37][180/408]	Loss 0.0083 (0.0460)	
training:	Epoch: [37][181/408]	Loss 0.0085 (0.0457)	
training:	Epoch: [37][182/408]	Loss 0.0091 (0.0455)	
training:	Epoch: [37][183/408]	Loss 0.0092 (0.0453)	
training:	Epoch: [37][184/408]	Loss 0.0095 (0.0452)	
training:	Epoch: [37][185/408]	Loss 0.0102 (0.0450)	
training:	Epoch: [37][186/408]	Loss 0.0623 (0.0451)	
training:	Epoch: [37][187/408]	Loss 0.0095 (0.0449)	
training:	Epoch: [37][188/408]	Loss 0.0086 (0.0447)	
training:	Epoch: [37][189/408]	Loss 0.0069 (0.0445)	
training:	Epoch: [37][190/408]	Loss 0.0084 (0.0443)	
training:	Epoch: [37][191/408]	Loss 0.3087 (0.0457)	
training:	Epoch: [37][192/408]	Loss 0.0098 (0.0455)	
training:	Epoch: [37][193/408]	Loss 0.0091 (0.0453)	
training:	Epoch: [37][194/408]	Loss 0.0098 (0.0451)	
training:	Epoch: [37][195/408]	Loss 0.0081 (0.0449)	
training:	Epoch: [37][196/408]	Loss 0.0063 (0.0447)	
training:	Epoch: [37][197/408]	Loss 0.0098 (0.0445)	
training:	Epoch: [37][198/408]	Loss 0.0085 (0.0444)	
training:	Epoch: [37][199/408]	Loss 0.0070 (0.0442)	
training:	Epoch: [37][200/408]	Loss 0.0106 (0.0440)	
training:	Epoch: [37][201/408]	Loss 0.0091 (0.0438)	
training:	Epoch: [37][202/408]	Loss 0.0094 (0.0437)	
training:	Epoch: [37][203/408]	Loss 0.0118 (0.0435)	
training:	Epoch: [37][204/408]	Loss 0.0077 (0.0433)	
training:	Epoch: [37][205/408]	Loss 0.0073 (0.0432)	
training:	Epoch: [37][206/408]	Loss 0.0086 (0.0430)	
training:	Epoch: [37][207/408]	Loss 0.0138 (0.0428)	
training:	Epoch: [37][208/408]	Loss 0.0082 (0.0427)	
training:	Epoch: [37][209/408]	Loss 0.0081 (0.0425)	
training:	Epoch: [37][210/408]	Loss 0.0083 (0.0424)	
training:	Epoch: [37][211/408]	Loss 0.0096 (0.0422)	
training:	Epoch: [37][212/408]	Loss 0.0079 (0.0420)	
training:	Epoch: [37][213/408]	Loss 0.0077 (0.0419)	
training:	Epoch: [37][214/408]	Loss 0.0103 (0.0417)	
training:	Epoch: [37][215/408]	Loss 0.0084 (0.0416)	
training:	Epoch: [37][216/408]	Loss 0.0088 (0.0414)	
training:	Epoch: [37][217/408]	Loss 0.0079 (0.0413)	
training:	Epoch: [37][218/408]	Loss 0.0077 (0.0411)	
training:	Epoch: [37][219/408]	Loss 0.1801 (0.0417)	
training:	Epoch: [37][220/408]	Loss 0.0090 (0.0416)	
training:	Epoch: [37][221/408]	Loss 0.2199 (0.0424)	
training:	Epoch: [37][222/408]	Loss 0.0082 (0.0422)	
training:	Epoch: [37][223/408]	Loss 0.0112 (0.0421)	
training:	Epoch: [37][224/408]	Loss 0.0083 (0.0420)	
training:	Epoch: [37][225/408]	Loss 0.0076 (0.0418)	
training:	Epoch: [37][226/408]	Loss 0.0084 (0.0417)	
training:	Epoch: [37][227/408]	Loss 0.0087 (0.0415)	
training:	Epoch: [37][228/408]	Loss 0.2511 (0.0424)	
training:	Epoch: [37][229/408]	Loss 0.0974 (0.0427)	
training:	Epoch: [37][230/408]	Loss 0.0071 (0.0425)	
training:	Epoch: [37][231/408]	Loss 0.0088 (0.0424)	
training:	Epoch: [37][232/408]	Loss 0.2741 (0.0434)	
training:	Epoch: [37][233/408]	Loss 0.0081 (0.0432)	
training:	Epoch: [37][234/408]	Loss 0.2989 (0.0443)	
training:	Epoch: [37][235/408]	Loss 0.0068 (0.0442)	
training:	Epoch: [37][236/408]	Loss 0.0076 (0.0440)	
training:	Epoch: [37][237/408]	Loss 0.0069 (0.0438)	
training:	Epoch: [37][238/408]	Loss 0.0088 (0.0437)	
training:	Epoch: [37][239/408]	Loss 0.0071 (0.0435)	
training:	Epoch: [37][240/408]	Loss 0.0077 (0.0434)	
training:	Epoch: [37][241/408]	Loss 0.0056 (0.0432)	
training:	Epoch: [37][242/408]	Loss 0.0092 (0.0431)	
training:	Epoch: [37][243/408]	Loss 0.0089 (0.0430)	
training:	Epoch: [37][244/408]	Loss 0.0081 (0.0428)	
training:	Epoch: [37][245/408]	Loss 0.0068 (0.0427)	
training:	Epoch: [37][246/408]	Loss 0.0072 (0.0425)	
training:	Epoch: [37][247/408]	Loss 0.0081 (0.0424)	
training:	Epoch: [37][248/408]	Loss 0.0172 (0.0423)	
training:	Epoch: [37][249/408]	Loss 0.0059 (0.0421)	
training:	Epoch: [37][250/408]	Loss 0.0075 (0.0420)	
training:	Epoch: [37][251/408]	Loss 0.0106 (0.0419)	
training:	Epoch: [37][252/408]	Loss 0.0071 (0.0417)	
training:	Epoch: [37][253/408]	Loss 0.0078 (0.0416)	
training:	Epoch: [37][254/408]	Loss 0.0077 (0.0415)	
training:	Epoch: [37][255/408]	Loss 0.0088 (0.0413)	
training:	Epoch: [37][256/408]	Loss 0.0090 (0.0412)	
training:	Epoch: [37][257/408]	Loss 0.0124 (0.0411)	
training:	Epoch: [37][258/408]	Loss 0.0098 (0.0410)	
training:	Epoch: [37][259/408]	Loss 0.0122 (0.0409)	
training:	Epoch: [37][260/408]	Loss 0.0095 (0.0407)	
training:	Epoch: [37][261/408]	Loss 0.0092 (0.0406)	
training:	Epoch: [37][262/408]	Loss 0.0064 (0.0405)	
training:	Epoch: [37][263/408]	Loss 0.0083 (0.0404)	
training:	Epoch: [37][264/408]	Loss 0.0074 (0.0402)	
training:	Epoch: [37][265/408]	Loss 0.0126 (0.0401)	
training:	Epoch: [37][266/408]	Loss 0.0138 (0.0400)	
training:	Epoch: [37][267/408]	Loss 0.0062 (0.0399)	
training:	Epoch: [37][268/408]	Loss 0.0078 (0.0398)	
training:	Epoch: [37][269/408]	Loss 0.0094 (0.0397)	
training:	Epoch: [37][270/408]	Loss 0.0088 (0.0396)	
training:	Epoch: [37][271/408]	Loss 0.0114 (0.0395)	
training:	Epoch: [37][272/408]	Loss 0.0076 (0.0393)	
training:	Epoch: [37][273/408]	Loss 0.0082 (0.0392)	
training:	Epoch: [37][274/408]	Loss 0.0084 (0.0391)	
training:	Epoch: [37][275/408]	Loss 0.0092 (0.0390)	
training:	Epoch: [37][276/408]	Loss 0.0066 (0.0389)	
training:	Epoch: [37][277/408]	Loss 0.0093 (0.0388)	
training:	Epoch: [37][278/408]	Loss 0.0082 (0.0387)	
training:	Epoch: [37][279/408]	Loss 0.0067 (0.0386)	
training:	Epoch: [37][280/408]	Loss 0.0092 (0.0385)	
training:	Epoch: [37][281/408]	Loss 0.0070 (0.0383)	
training:	Epoch: [37][282/408]	Loss 0.0078 (0.0382)	
training:	Epoch: [37][283/408]	Loss 0.0066 (0.0381)	
training:	Epoch: [37][284/408]	Loss 0.0062 (0.0380)	
training:	Epoch: [37][285/408]	Loss 0.0065 (0.0379)	
training:	Epoch: [37][286/408]	Loss 0.0070 (0.0378)	
training:	Epoch: [37][287/408]	Loss 0.0082 (0.0377)	
training:	Epoch: [37][288/408]	Loss 0.3076 (0.0386)	
training:	Epoch: [37][289/408]	Loss 0.0835 (0.0388)	
training:	Epoch: [37][290/408]	Loss 0.0119 (0.0387)	
training:	Epoch: [37][291/408]	Loss 0.0080 (0.0386)	
training:	Epoch: [37][292/408]	Loss 0.2865 (0.0394)	
training:	Epoch: [37][293/408]	Loss 0.0059 (0.0393)	
training:	Epoch: [37][294/408]	Loss 0.2805 (0.0401)	
training:	Epoch: [37][295/408]	Loss 0.0081 (0.0400)	
training:	Epoch: [37][296/408]	Loss 0.2966 (0.0409)	
training:	Epoch: [37][297/408]	Loss 0.0116 (0.0408)	
training:	Epoch: [37][298/408]	Loss 0.0073 (0.0407)	
training:	Epoch: [37][299/408]	Loss 0.0064 (0.0406)	
training:	Epoch: [37][300/408]	Loss 0.0074 (0.0405)	
training:	Epoch: [37][301/408]	Loss 0.0066 (0.0404)	
training:	Epoch: [37][302/408]	Loss 0.0087 (0.0402)	
training:	Epoch: [37][303/408]	Loss 0.0080 (0.0401)	
training:	Epoch: [37][304/408]	Loss 0.0097 (0.0400)	
training:	Epoch: [37][305/408]	Loss 0.0075 (0.0399)	
training:	Epoch: [37][306/408]	Loss 0.0093 (0.0398)	
training:	Epoch: [37][307/408]	Loss 0.0094 (0.0397)	
training:	Epoch: [37][308/408]	Loss 0.0093 (0.0396)	
training:	Epoch: [37][309/408]	Loss 0.0090 (0.0395)	
training:	Epoch: [37][310/408]	Loss 0.0071 (0.0394)	
training:	Epoch: [37][311/408]	Loss 0.0096 (0.0393)	
training:	Epoch: [37][312/408]	Loss 0.0135 (0.0393)	
training:	Epoch: [37][313/408]	Loss 0.0069 (0.0391)	
training:	Epoch: [37][314/408]	Loss 0.5049 (0.0406)	
training:	Epoch: [37][315/408]	Loss 0.0097 (0.0405)	
training:	Epoch: [37][316/408]	Loss 0.0104 (0.0404)	
training:	Epoch: [37][317/408]	Loss 0.0064 (0.0403)	
training:	Epoch: [37][318/408]	Loss 0.1571 (0.0407)	
training:	Epoch: [37][319/408]	Loss 0.0071 (0.0406)	
training:	Epoch: [37][320/408]	Loss 0.0117 (0.0405)	
training:	Epoch: [37][321/408]	Loss 0.0081 (0.0404)	
training:	Epoch: [37][322/408]	Loss 0.0082 (0.0403)	
training:	Epoch: [37][323/408]	Loss 0.0065 (0.0402)	
training:	Epoch: [37][324/408]	Loss 0.0095 (0.0401)	
training:	Epoch: [37][325/408]	Loss 0.0070 (0.0400)	
training:	Epoch: [37][326/408]	Loss 0.3184 (0.0409)	
training:	Epoch: [37][327/408]	Loss 0.0081 (0.0408)	
training:	Epoch: [37][328/408]	Loss 0.0112 (0.0407)	
training:	Epoch: [37][329/408]	Loss 0.0069 (0.0406)	
training:	Epoch: [37][330/408]	Loss 0.0074 (0.0405)	
training:	Epoch: [37][331/408]	Loss 0.0078 (0.0404)	
training:	Epoch: [37][332/408]	Loss 0.0078 (0.0403)	
training:	Epoch: [37][333/408]	Loss 0.0173 (0.0402)	
training:	Epoch: [37][334/408]	Loss 0.3089 (0.0410)	
training:	Epoch: [37][335/408]	Loss 0.0062 (0.0409)	
training:	Epoch: [37][336/408]	Loss 0.2449 (0.0415)	
training:	Epoch: [37][337/408]	Loss 0.0081 (0.0414)	
training:	Epoch: [37][338/408]	Loss 0.0079 (0.0413)	
training:	Epoch: [37][339/408]	Loss 0.0103 (0.0412)	
training:	Epoch: [37][340/408]	Loss 0.0108 (0.0411)	
training:	Epoch: [37][341/408]	Loss 0.0094 (0.0410)	
training:	Epoch: [37][342/408]	Loss 0.0072 (0.0409)	
training:	Epoch: [37][343/408]	Loss 0.0079 (0.0408)	
training:	Epoch: [37][344/408]	Loss 0.0074 (0.0407)	
training:	Epoch: [37][345/408]	Loss 0.0063 (0.0406)	
training:	Epoch: [37][346/408]	Loss 0.2737 (0.0413)	
training:	Epoch: [37][347/408]	Loss 0.0077 (0.0412)	
training:	Epoch: [37][348/408]	Loss 0.0230 (0.0412)	
training:	Epoch: [37][349/408]	Loss 0.0080 (0.0411)	
training:	Epoch: [37][350/408]	Loss 0.0067 (0.0410)	
training:	Epoch: [37][351/408]	Loss 0.0208 (0.0409)	
training:	Epoch: [37][352/408]	Loss 0.0083 (0.0408)	
training:	Epoch: [37][353/408]	Loss 0.0073 (0.0407)	
training:	Epoch: [37][354/408]	Loss 0.0088 (0.0406)	
training:	Epoch: [37][355/408]	Loss 0.0085 (0.0405)	
training:	Epoch: [37][356/408]	Loss 0.0078 (0.0405)	
training:	Epoch: [37][357/408]	Loss 0.0121 (0.0404)	
training:	Epoch: [37][358/408]	Loss 0.0071 (0.0403)	
training:	Epoch: [37][359/408]	Loss 0.0077 (0.0402)	
training:	Epoch: [37][360/408]	Loss 0.0078 (0.0401)	
training:	Epoch: [37][361/408]	Loss 0.0073 (0.0400)	
training:	Epoch: [37][362/408]	Loss 0.0076 (0.0399)	
training:	Epoch: [37][363/408]	Loss 0.0066 (0.0398)	
training:	Epoch: [37][364/408]	Loss 0.0079 (0.0397)	
training:	Epoch: [37][365/408]	Loss 0.0079 (0.0397)	
training:	Epoch: [37][366/408]	Loss 0.0079 (0.0396)	
training:	Epoch: [37][367/408]	Loss 0.0066 (0.0395)	
training:	Epoch: [37][368/408]	Loss 0.0078 (0.0394)	
training:	Epoch: [37][369/408]	Loss 0.0098 (0.0393)	
training:	Epoch: [37][370/408]	Loss 0.0075 (0.0392)	
training:	Epoch: [37][371/408]	Loss 0.0066 (0.0391)	
training:	Epoch: [37][372/408]	Loss 0.0070 (0.0390)	
training:	Epoch: [37][373/408]	Loss 0.0071 (0.0390)	
training:	Epoch: [37][374/408]	Loss 0.0079 (0.0389)	
training:	Epoch: [37][375/408]	Loss 0.2609 (0.0395)	
training:	Epoch: [37][376/408]	Loss 0.0067 (0.0394)	
training:	Epoch: [37][377/408]	Loss 0.0811 (0.0395)	
training:	Epoch: [37][378/408]	Loss 0.0073 (0.0394)	
training:	Epoch: [37][379/408]	Loss 0.2793 (0.0400)	
training:	Epoch: [37][380/408]	Loss 0.0098 (0.0400)	
training:	Epoch: [37][381/408]	Loss 0.0088 (0.0399)	
training:	Epoch: [37][382/408]	Loss 0.0076 (0.0398)	
training:	Epoch: [37][383/408]	Loss 0.0075 (0.0397)	
training:	Epoch: [37][384/408]	Loss 0.2661 (0.0403)	
training:	Epoch: [37][385/408]	Loss 0.3022 (0.0410)	
training:	Epoch: [37][386/408]	Loss 0.3119 (0.0417)	
training:	Epoch: [37][387/408]	Loss 0.0079 (0.0416)	
training:	Epoch: [37][388/408]	Loss 0.0072 (0.0415)	
training:	Epoch: [37][389/408]	Loss 0.3090 (0.0422)	
training:	Epoch: [37][390/408]	Loss 0.0095 (0.0421)	
training:	Epoch: [37][391/408]	Loss 0.2957 (0.0428)	
training:	Epoch: [37][392/408]	Loss 0.0209 (0.0427)	
training:	Epoch: [37][393/408]	Loss 0.0074 (0.0426)	
training:	Epoch: [37][394/408]	Loss 0.0099 (0.0425)	
training:	Epoch: [37][395/408]	Loss 0.0085 (0.0424)	
training:	Epoch: [37][396/408]	Loss 0.0132 (0.0424)	
training:	Epoch: [37][397/408]	Loss 0.0074 (0.0423)	
training:	Epoch: [37][398/408]	Loss 0.0065 (0.0422)	
training:	Epoch: [37][399/408]	Loss 0.0084 (0.0421)	
training:	Epoch: [37][400/408]	Loss 0.0078 (0.0420)	
training:	Epoch: [37][401/408]	Loss 0.0094 (0.0419)	
training:	Epoch: [37][402/408]	Loss 0.0083 (0.0419)	
training:	Epoch: [37][403/408]	Loss 0.0076 (0.0418)	
training:	Epoch: [37][404/408]	Loss 0.0081 (0.0417)	
training:	Epoch: [37][405/408]	Loss 0.0081 (0.0416)	
training:	Epoch: [37][406/408]	Loss 0.0097 (0.0415)	
training:	Epoch: [37][407/408]	Loss 0.2862 (0.0421)	
training:	Epoch: [37][408/408]	Loss 0.0103 (0.0421)	
Training:	 Loss: 0.0420

Training:	 ACC: 0.9935 0.9934 0.9927 0.9943
Validation:	 ACC: 0.7822 0.7822 0.7820 0.7825
Validation:	 Best_BACC: 0.8226 0.8250 0.8772 0.7679
Validation:	 Loss: 0.9315
Pretraining:	Epoch 38/200
----------
training:	Epoch: [38][1/408]	Loss 0.0088 (0.0088)	
training:	Epoch: [38][2/408]	Loss 0.0076 (0.0082)	
training:	Epoch: [38][3/408]	Loss 0.0072 (0.0079)	
training:	Epoch: [38][4/408]	Loss 0.0081 (0.0079)	
training:	Epoch: [38][5/408]	Loss 0.0079 (0.0079)	
training:	Epoch: [38][6/408]	Loss 0.0108 (0.0084)	
training:	Epoch: [38][7/408]	Loss 0.2891 (0.0485)	
training:	Epoch: [38][8/408]	Loss 0.0083 (0.0435)	
training:	Epoch: [38][9/408]	Loss 0.0077 (0.0395)	
training:	Epoch: [38][10/408]	Loss 0.0068 (0.0362)	
training:	Epoch: [38][11/408]	Loss 0.2765 (0.0581)	
training:	Epoch: [38][12/408]	Loss 0.0092 (0.0540)	
training:	Epoch: [38][13/408]	Loss 0.3018 (0.0731)	
training:	Epoch: [38][14/408]	Loss 0.0091 (0.0685)	
training:	Epoch: [38][15/408]	Loss 0.0083 (0.0645)	
training:	Epoch: [38][16/408]	Loss 0.0071 (0.0609)	
training:	Epoch: [38][17/408]	Loss 0.0091 (0.0578)	
training:	Epoch: [38][18/408]	Loss 0.0071 (0.0550)	
training:	Epoch: [38][19/408]	Loss 0.0077 (0.0525)	
training:	Epoch: [38][20/408]	Loss 0.0085 (0.0503)	
training:	Epoch: [38][21/408]	Loss 0.0089 (0.0484)	
training:	Epoch: [38][22/408]	Loss 0.0093 (0.0466)	
training:	Epoch: [38][23/408]	Loss 0.0084 (0.0449)	
training:	Epoch: [38][24/408]	Loss 0.0102 (0.0435)	
training:	Epoch: [38][25/408]	Loss 0.0089 (0.0421)	
training:	Epoch: [38][26/408]	Loss 0.0086 (0.0408)	
training:	Epoch: [38][27/408]	Loss 0.0094 (0.0396)	
training:	Epoch: [38][28/408]	Loss 0.0078 (0.0385)	
training:	Epoch: [38][29/408]	Loss 0.0116 (0.0376)	
training:	Epoch: [38][30/408]	Loss 0.0086 (0.0366)	
training:	Epoch: [38][31/408]	Loss 0.0107 (0.0358)	
training:	Epoch: [38][32/408]	Loss 0.0090 (0.0349)	
training:	Epoch: [38][33/408]	Loss 0.0077 (0.0341)	
training:	Epoch: [38][34/408]	Loss 0.0748 (0.0353)	
training:	Epoch: [38][35/408]	Loss 0.2608 (0.0418)	
training:	Epoch: [38][36/408]	Loss 0.0089 (0.0408)	
training:	Epoch: [38][37/408]	Loss 0.2746 (0.0472)	
training:	Epoch: [38][38/408]	Loss 0.0095 (0.0462)	
training:	Epoch: [38][39/408]	Loss 0.0079 (0.0452)	
training:	Epoch: [38][40/408]	Loss 0.0094 (0.0443)	
training:	Epoch: [38][41/408]	Loss 0.0074 (0.0434)	
training:	Epoch: [38][42/408]	Loss 0.0090 (0.0426)	
training:	Epoch: [38][43/408]	Loss 0.0091 (0.0418)	
training:	Epoch: [38][44/408]	Loss 0.0082 (0.0410)	
training:	Epoch: [38][45/408]	Loss 0.0085 (0.0403)	
training:	Epoch: [38][46/408]	Loss 0.0090 (0.0396)	
training:	Epoch: [38][47/408]	Loss 0.0083 (0.0390)	
training:	Epoch: [38][48/408]	Loss 0.0101 (0.0384)	
training:	Epoch: [38][49/408]	Loss 0.0091 (0.0378)	
training:	Epoch: [38][50/408]	Loss 0.0096 (0.0372)	
training:	Epoch: [38][51/408]	Loss 0.0206 (0.0369)	
training:	Epoch: [38][52/408]	Loss 0.0074 (0.0363)	
training:	Epoch: [38][53/408]	Loss 0.0090 (0.0358)	
training:	Epoch: [38][54/408]	Loss 0.0092 (0.0353)	
training:	Epoch: [38][55/408]	Loss 0.2867 (0.0399)	
training:	Epoch: [38][56/408]	Loss 0.0079 (0.0393)	
training:	Epoch: [38][57/408]	Loss 0.0095 (0.0388)	
training:	Epoch: [38][58/408]	Loss 0.0088 (0.0383)	
training:	Epoch: [38][59/408]	Loss 0.0097 (0.0378)	
training:	Epoch: [38][60/408]	Loss 0.5465 (0.0463)	
training:	Epoch: [38][61/408]	Loss 0.0092 (0.0456)	
training:	Epoch: [38][62/408]	Loss 0.0087 (0.0451)	
training:	Epoch: [38][63/408]	Loss 0.0080 (0.0445)	
training:	Epoch: [38][64/408]	Loss 0.0081 (0.0439)	
training:	Epoch: [38][65/408]	Loss 0.0078 (0.0433)	
training:	Epoch: [38][66/408]	Loss 0.0093 (0.0428)	
training:	Epoch: [38][67/408]	Loss 0.0085 (0.0423)	
training:	Epoch: [38][68/408]	Loss 0.2752 (0.0457)	
training:	Epoch: [38][69/408]	Loss 0.0086 (0.0452)	
training:	Epoch: [38][70/408]	Loss 0.0094 (0.0447)	
training:	Epoch: [38][71/408]	Loss 0.0076 (0.0442)	
training:	Epoch: [38][72/408]	Loss 0.0085 (0.0437)	
training:	Epoch: [38][73/408]	Loss 0.0073 (0.0432)	
training:	Epoch: [38][74/408]	Loss 0.0067 (0.0427)	
training:	Epoch: [38][75/408]	Loss 0.0081 (0.0422)	
training:	Epoch: [38][76/408]	Loss 0.0079 (0.0418)	
training:	Epoch: [38][77/408]	Loss 0.0069 (0.0413)	
training:	Epoch: [38][78/408]	Loss 0.0086 (0.0409)	
training:	Epoch: [38][79/408]	Loss 0.0083 (0.0405)	
training:	Epoch: [38][80/408]	Loss 0.0077 (0.0401)	
training:	Epoch: [38][81/408]	Loss 0.0088 (0.0397)	
training:	Epoch: [38][82/408]	Loss 0.0081 (0.0393)	
training:	Epoch: [38][83/408]	Loss 0.0091 (0.0389)	
training:	Epoch: [38][84/408]	Loss 0.0077 (0.0386)	
training:	Epoch: [38][85/408]	Loss 0.0064 (0.0382)	
training:	Epoch: [38][86/408]	Loss 0.0073 (0.0378)	
training:	Epoch: [38][87/408]	Loss 0.0102 (0.0375)	
training:	Epoch: [38][88/408]	Loss 0.0072 (0.0372)	
training:	Epoch: [38][89/408]	Loss 0.0099 (0.0369)	
training:	Epoch: [38][90/408]	Loss 0.0076 (0.0365)	
training:	Epoch: [38][91/408]	Loss 0.3116 (0.0396)	
training:	Epoch: [38][92/408]	Loss 0.0101 (0.0392)	
training:	Epoch: [38][93/408]	Loss 0.0112 (0.0389)	
training:	Epoch: [38][94/408]	Loss 0.0088 (0.0386)	
training:	Epoch: [38][95/408]	Loss 0.0067 (0.0383)	
training:	Epoch: [38][96/408]	Loss 0.0083 (0.0380)	
training:	Epoch: [38][97/408]	Loss 0.2831 (0.0405)	
training:	Epoch: [38][98/408]	Loss 0.0079 (0.0402)	
training:	Epoch: [38][99/408]	Loss 0.0086 (0.0398)	
training:	Epoch: [38][100/408]	Loss 0.0083 (0.0395)	
training:	Epoch: [38][101/408]	Loss 0.0079 (0.0392)	
training:	Epoch: [38][102/408]	Loss 0.0186 (0.0390)	
training:	Epoch: [38][103/408]	Loss 0.0095 (0.0387)	
training:	Epoch: [38][104/408]	Loss 0.0087 (0.0384)	
training:	Epoch: [38][105/408]	Loss 0.2517 (0.0405)	
training:	Epoch: [38][106/408]	Loss 0.0074 (0.0402)	
training:	Epoch: [38][107/408]	Loss 0.2798 (0.0424)	
training:	Epoch: [38][108/408]	Loss 0.0091 (0.0421)	
training:	Epoch: [38][109/408]	Loss 0.0073 (0.0418)	
training:	Epoch: [38][110/408]	Loss 0.0085 (0.0415)	
training:	Epoch: [38][111/408]	Loss 0.0070 (0.0412)	
training:	Epoch: [38][112/408]	Loss 0.0092 (0.0409)	
training:	Epoch: [38][113/408]	Loss 0.0315 (0.0408)	
training:	Epoch: [38][114/408]	Loss 0.0710 (0.0410)	
training:	Epoch: [38][115/408]	Loss 0.0064 (0.0407)	
training:	Epoch: [38][116/408]	Loss 0.6041 (0.0456)	
training:	Epoch: [38][117/408]	Loss 0.0075 (0.0453)	
training:	Epoch: [38][118/408]	Loss 0.0091 (0.0450)	
training:	Epoch: [38][119/408]	Loss 0.0082 (0.0447)	
training:	Epoch: [38][120/408]	Loss 0.0124 (0.0444)	
training:	Epoch: [38][121/408]	Loss 0.0084 (0.0441)	
training:	Epoch: [38][122/408]	Loss 0.0081 (0.0438)	
training:	Epoch: [38][123/408]	Loss 0.0078 (0.0435)	
training:	Epoch: [38][124/408]	Loss 0.0083 (0.0432)	
training:	Epoch: [38][125/408]	Loss 0.0087 (0.0429)	
training:	Epoch: [38][126/408]	Loss 0.0463 (0.0430)	
training:	Epoch: [38][127/408]	Loss 0.0088 (0.0427)	
training:	Epoch: [38][128/408]	Loss 0.0092 (0.0424)	
training:	Epoch: [38][129/408]	Loss 0.0074 (0.0422)	
training:	Epoch: [38][130/408]	Loss 0.0098 (0.0419)	
training:	Epoch: [38][131/408]	Loss 0.2950 (0.0439)	
training:	Epoch: [38][132/408]	Loss 0.0089 (0.0436)	
training:	Epoch: [38][133/408]	Loss 0.0088 (0.0433)	
training:	Epoch: [38][134/408]	Loss 0.0079 (0.0431)	
training:	Epoch: [38][135/408]	Loss 0.0107 (0.0428)	
training:	Epoch: [38][136/408]	Loss 0.0089 (0.0426)	
training:	Epoch: [38][137/408]	Loss 0.0122 (0.0424)	
training:	Epoch: [38][138/408]	Loss 0.0075 (0.0421)	
training:	Epoch: [38][139/408]	Loss 0.0065 (0.0418)	
training:	Epoch: [38][140/408]	Loss 0.0082 (0.0416)	
training:	Epoch: [38][141/408]	Loss 0.0074 (0.0414)	
training:	Epoch: [38][142/408]	Loss 0.0086 (0.0411)	
training:	Epoch: [38][143/408]	Loss 0.0162 (0.0410)	
training:	Epoch: [38][144/408]	Loss 0.0075 (0.0407)	
training:	Epoch: [38][145/408]	Loss 0.0066 (0.0405)	
training:	Epoch: [38][146/408]	Loss 0.0082 (0.0403)	
training:	Epoch: [38][147/408]	Loss 0.0078 (0.0400)	
training:	Epoch: [38][148/408]	Loss 0.0144 (0.0399)	
training:	Epoch: [38][149/408]	Loss 0.0082 (0.0397)	
training:	Epoch: [38][150/408]	Loss 0.0098 (0.0395)	
training:	Epoch: [38][151/408]	Loss 0.0090 (0.0393)	
training:	Epoch: [38][152/408]	Loss 0.0083 (0.0391)	
training:	Epoch: [38][153/408]	Loss 0.0091 (0.0389)	
training:	Epoch: [38][154/408]	Loss 0.0065 (0.0387)	
training:	Epoch: [38][155/408]	Loss 0.0080 (0.0385)	
training:	Epoch: [38][156/408]	Loss 0.0072 (0.0383)	
training:	Epoch: [38][157/408]	Loss 0.0080 (0.0381)	
training:	Epoch: [38][158/408]	Loss 0.0076 (0.0379)	
training:	Epoch: [38][159/408]	Loss 0.0087 (0.0377)	
training:	Epoch: [38][160/408]	Loss 0.0080 (0.0375)	
training:	Epoch: [38][161/408]	Loss 0.0073 (0.0373)	
training:	Epoch: [38][162/408]	Loss 0.0077 (0.0371)	
training:	Epoch: [38][163/408]	Loss 0.0231 (0.0370)	
training:	Epoch: [38][164/408]	Loss 0.0115 (0.0369)	
training:	Epoch: [38][165/408]	Loss 0.0090 (0.0367)	
training:	Epoch: [38][166/408]	Loss 0.0065 (0.0365)	
training:	Epoch: [38][167/408]	Loss 0.0077 (0.0364)	
training:	Epoch: [38][168/408]	Loss 0.0085 (0.0362)	
training:	Epoch: [38][169/408]	Loss 0.0081 (0.0360)	
training:	Epoch: [38][170/408]	Loss 0.0068 (0.0359)	
training:	Epoch: [38][171/408]	Loss 0.0072 (0.0357)	
training:	Epoch: [38][172/408]	Loss 0.0087 (0.0355)	
training:	Epoch: [38][173/408]	Loss 0.0063 (0.0354)	
training:	Epoch: [38][174/408]	Loss 0.0077 (0.0352)	
training:	Epoch: [38][175/408]	Loss 0.2740 (0.0366)	
training:	Epoch: [38][176/408]	Loss 0.0078 (0.0364)	
training:	Epoch: [38][177/408]	Loss 0.0079 (0.0362)	
training:	Epoch: [38][178/408]	Loss 0.0067 (0.0361)	
training:	Epoch: [38][179/408]	Loss 0.2307 (0.0372)	
training:	Epoch: [38][180/408]	Loss 0.0071 (0.0370)	
training:	Epoch: [38][181/408]	Loss 0.0092 (0.0368)	
training:	Epoch: [38][182/408]	Loss 0.0072 (0.0367)	
training:	Epoch: [38][183/408]	Loss 0.0066 (0.0365)	
training:	Epoch: [38][184/408]	Loss 0.0086 (0.0364)	
training:	Epoch: [38][185/408]	Loss 0.0075 (0.0362)	
training:	Epoch: [38][186/408]	Loss 0.0064 (0.0361)	
training:	Epoch: [38][187/408]	Loss 0.0075 (0.0359)	
training:	Epoch: [38][188/408]	Loss 0.1934 (0.0367)	
training:	Epoch: [38][189/408]	Loss 0.0073 (0.0366)	
training:	Epoch: [38][190/408]	Loss 0.0083 (0.0364)	
training:	Epoch: [38][191/408]	Loss 0.0075 (0.0363)	
training:	Epoch: [38][192/408]	Loss 0.0071 (0.0361)	
training:	Epoch: [38][193/408]	Loss 0.0067 (0.0360)	
training:	Epoch: [38][194/408]	Loss 0.0066 (0.0358)	
training:	Epoch: [38][195/408]	Loss 0.0072 (0.0357)	
training:	Epoch: [38][196/408]	Loss 0.0173 (0.0356)	
training:	Epoch: [38][197/408]	Loss 0.0073 (0.0354)	
training:	Epoch: [38][198/408]	Loss 0.0059 (0.0353)	
training:	Epoch: [38][199/408]	Loss 0.0067 (0.0351)	
training:	Epoch: [38][200/408]	Loss 0.0072 (0.0350)	
training:	Epoch: [38][201/408]	Loss 0.0075 (0.0349)	
training:	Epoch: [38][202/408]	Loss 0.0087 (0.0347)	
training:	Epoch: [38][203/408]	Loss 0.2858 (0.0360)	
training:	Epoch: [38][204/408]	Loss 0.0086 (0.0358)	
training:	Epoch: [38][205/408]	Loss 0.0075 (0.0357)	
training:	Epoch: [38][206/408]	Loss 0.3033 (0.0370)	
training:	Epoch: [38][207/408]	Loss 0.0066 (0.0369)	
training:	Epoch: [38][208/408]	Loss 0.3105 (0.0382)	
training:	Epoch: [38][209/408]	Loss 0.0073 (0.0380)	
training:	Epoch: [38][210/408]	Loss 0.0077 (0.0379)	
training:	Epoch: [38][211/408]	Loss 0.0087 (0.0377)	
training:	Epoch: [38][212/408]	Loss 0.0072 (0.0376)	
training:	Epoch: [38][213/408]	Loss 0.0073 (0.0375)	
training:	Epoch: [38][214/408]	Loss 0.2983 (0.0387)	
training:	Epoch: [38][215/408]	Loss 0.0075 (0.0385)	
training:	Epoch: [38][216/408]	Loss 0.0084 (0.0384)	
training:	Epoch: [38][217/408]	Loss 0.0089 (0.0383)	
training:	Epoch: [38][218/408]	Loss 0.0066 (0.0381)	
training:	Epoch: [38][219/408]	Loss 0.0085 (0.0380)	
training:	Epoch: [38][220/408]	Loss 0.0080 (0.0378)	
training:	Epoch: [38][221/408]	Loss 0.2900 (0.0390)	
training:	Epoch: [38][222/408]	Loss 0.0075 (0.0388)	
training:	Epoch: [38][223/408]	Loss 0.0103 (0.0387)	
training:	Epoch: [38][224/408]	Loss 0.0074 (0.0386)	
training:	Epoch: [38][225/408]	Loss 0.0088 (0.0384)	
training:	Epoch: [38][226/408]	Loss 0.0069 (0.0383)	
training:	Epoch: [38][227/408]	Loss 0.0071 (0.0382)	
training:	Epoch: [38][228/408]	Loss 0.0069 (0.0380)	
training:	Epoch: [38][229/408]	Loss 0.0074 (0.0379)	
training:	Epoch: [38][230/408]	Loss 0.0091 (0.0378)	
training:	Epoch: [38][231/408]	Loss 0.0096 (0.0376)	
training:	Epoch: [38][232/408]	Loss 0.0074 (0.0375)	
training:	Epoch: [38][233/408]	Loss 0.0103 (0.0374)	
training:	Epoch: [38][234/408]	Loss 0.0079 (0.0373)	
training:	Epoch: [38][235/408]	Loss 0.0060 (0.0371)	
training:	Epoch: [38][236/408]	Loss 0.0069 (0.0370)	
training:	Epoch: [38][237/408]	Loss 0.0072 (0.0369)	
training:	Epoch: [38][238/408]	Loss 0.2737 (0.0379)	
training:	Epoch: [38][239/408]	Loss 0.0090 (0.0378)	
training:	Epoch: [38][240/408]	Loss 0.0073 (0.0376)	
training:	Epoch: [38][241/408]	Loss 0.0201 (0.0376)	
training:	Epoch: [38][242/408]	Loss 0.0081 (0.0374)	
training:	Epoch: [38][243/408]	Loss 0.0072 (0.0373)	
training:	Epoch: [38][244/408]	Loss 0.0087 (0.0372)	
training:	Epoch: [38][245/408]	Loss 0.0080 (0.0371)	
training:	Epoch: [38][246/408]	Loss 0.0088 (0.0370)	
training:	Epoch: [38][247/408]	Loss 0.0081 (0.0368)	
training:	Epoch: [38][248/408]	Loss 0.0096 (0.0367)	
training:	Epoch: [38][249/408]	Loss 0.0106 (0.0366)	
training:	Epoch: [38][250/408]	Loss 0.0079 (0.0365)	
training:	Epoch: [38][251/408]	Loss 0.0076 (0.0364)	
training:	Epoch: [38][252/408]	Loss 0.0091 (0.0363)	
training:	Epoch: [38][253/408]	Loss 0.0082 (0.0362)	
training:	Epoch: [38][254/408]	Loss 0.0075 (0.0361)	
training:	Epoch: [38][255/408]	Loss 0.0087 (0.0360)	
training:	Epoch: [38][256/408]	Loss 0.0108 (0.0359)	
training:	Epoch: [38][257/408]	Loss 0.0192 (0.0358)	
training:	Epoch: [38][258/408]	Loss 0.0076 (0.0357)	
training:	Epoch: [38][259/408]	Loss 0.0086 (0.0356)	
training:	Epoch: [38][260/408]	Loss 0.0093 (0.0355)	
training:	Epoch: [38][261/408]	Loss 0.2857 (0.0364)	
training:	Epoch: [38][262/408]	Loss 0.0088 (0.0363)	
training:	Epoch: [38][263/408]	Loss 0.0078 (0.0362)	
training:	Epoch: [38][264/408]	Loss 0.0070 (0.0361)	
training:	Epoch: [38][265/408]	Loss 0.0088 (0.0360)	
training:	Epoch: [38][266/408]	Loss 0.0080 (0.0359)	
training:	Epoch: [38][267/408]	Loss 0.0067 (0.0358)	
training:	Epoch: [38][268/408]	Loss 0.0070 (0.0357)	
training:	Epoch: [38][269/408]	Loss 0.0069 (0.0356)	
training:	Epoch: [38][270/408]	Loss 0.0083 (0.0355)	
training:	Epoch: [38][271/408]	Loss 0.0086 (0.0354)	
training:	Epoch: [38][272/408]	Loss 0.0070 (0.0353)	
training:	Epoch: [38][273/408]	Loss 0.0077 (0.0352)	
training:	Epoch: [38][274/408]	Loss 0.3476 (0.0363)	
training:	Epoch: [38][275/408]	Loss 0.0064 (0.0362)	
training:	Epoch: [38][276/408]	Loss 0.0082 (0.0361)	
training:	Epoch: [38][277/408]	Loss 0.2632 (0.0369)	
training:	Epoch: [38][278/408]	Loss 0.0076 (0.0368)	
training:	Epoch: [38][279/408]	Loss 0.0084 (0.0367)	
training:	Epoch: [38][280/408]	Loss 0.2821 (0.0376)	
training:	Epoch: [38][281/408]	Loss 0.0072 (0.0375)	
training:	Epoch: [38][282/408]	Loss 0.0084 (0.0374)	
training:	Epoch: [38][283/408]	Loss 0.0074 (0.0373)	
training:	Epoch: [38][284/408]	Loss 0.0166 (0.0372)	
training:	Epoch: [38][285/408]	Loss 0.0083 (0.0371)	
training:	Epoch: [38][286/408]	Loss 0.0065 (0.0370)	
training:	Epoch: [38][287/408]	Loss 0.0075 (0.0369)	
training:	Epoch: [38][288/408]	Loss 0.0079 (0.0368)	
training:	Epoch: [38][289/408]	Loss 0.0065 (0.0367)	
training:	Epoch: [38][290/408]	Loss 0.2476 (0.0374)	
training:	Epoch: [38][291/408]	Loss 0.0057 (0.0373)	
training:	Epoch: [38][292/408]	Loss 0.0115 (0.0372)	
training:	Epoch: [38][293/408]	Loss 0.0092 (0.0371)	
training:	Epoch: [38][294/408]	Loss 0.0075 (0.0370)	
training:	Epoch: [38][295/408]	Loss 0.0073 (0.0369)	
training:	Epoch: [38][296/408]	Loss 0.0079 (0.0368)	
training:	Epoch: [38][297/408]	Loss 0.2831 (0.0377)	
training:	Epoch: [38][298/408]	Loss 0.0067 (0.0375)	
training:	Epoch: [38][299/408]	Loss 0.0085 (0.0375)	
training:	Epoch: [38][300/408]	Loss 0.0081 (0.0374)	
training:	Epoch: [38][301/408]	Loss 0.0071 (0.0373)	
training:	Epoch: [38][302/408]	Loss 0.0079 (0.0372)	
training:	Epoch: [38][303/408]	Loss 0.0078 (0.0371)	
training:	Epoch: [38][304/408]	Loss 0.0076 (0.0370)	
training:	Epoch: [38][305/408]	Loss 0.0239 (0.0369)	
training:	Epoch: [38][306/408]	Loss 0.0082 (0.0368)	
training:	Epoch: [38][307/408]	Loss 0.0078 (0.0367)	
training:	Epoch: [38][308/408]	Loss 0.0104 (0.0366)	
training:	Epoch: [38][309/408]	Loss 0.0064 (0.0365)	
training:	Epoch: [38][310/408]	Loss 0.3185 (0.0375)	
training:	Epoch: [38][311/408]	Loss 0.0078 (0.0374)	
training:	Epoch: [38][312/408]	Loss 0.0103 (0.0373)	
training:	Epoch: [38][313/408]	Loss 0.0068 (0.0372)	
training:	Epoch: [38][314/408]	Loss 0.0080 (0.0371)	
training:	Epoch: [38][315/408]	Loss 0.0082 (0.0370)	
training:	Epoch: [38][316/408]	Loss 0.0085 (0.0369)	
training:	Epoch: [38][317/408]	Loss 0.0084 (0.0368)	
training:	Epoch: [38][318/408]	Loss 0.0080 (0.0367)	
training:	Epoch: [38][319/408]	Loss 0.0060 (0.0366)	
training:	Epoch: [38][320/408]	Loss 0.0075 (0.0365)	
training:	Epoch: [38][321/408]	Loss 0.0065 (0.0364)	
training:	Epoch: [38][322/408]	Loss 0.0098 (0.0364)	
training:	Epoch: [38][323/408]	Loss 0.0077 (0.0363)	
training:	Epoch: [38][324/408]	Loss 0.0076 (0.0362)	
training:	Epoch: [38][325/408]	Loss 0.0077 (0.0361)	
training:	Epoch: [38][326/408]	Loss 0.0097 (0.0360)	
training:	Epoch: [38][327/408]	Loss 0.0078 (0.0359)	
training:	Epoch: [38][328/408]	Loss 0.0081 (0.0358)	
training:	Epoch: [38][329/408]	Loss 0.3276 (0.0367)	
training:	Epoch: [38][330/408]	Loss 0.0063 (0.0366)	
training:	Epoch: [38][331/408]	Loss 0.0074 (0.0365)	
training:	Epoch: [38][332/408]	Loss 0.0073 (0.0365)	
training:	Epoch: [38][333/408]	Loss 0.0085 (0.0364)	
training:	Epoch: [38][334/408]	Loss 0.2846 (0.0371)	
training:	Epoch: [38][335/408]	Loss 0.0054 (0.0370)	
training:	Epoch: [38][336/408]	Loss 0.2468 (0.0376)	
training:	Epoch: [38][337/408]	Loss 0.0066 (0.0376)	
training:	Epoch: [38][338/408]	Loss 0.0089 (0.0375)	
training:	Epoch: [38][339/408]	Loss 0.0074 (0.0374)	
training:	Epoch: [38][340/408]	Loss 0.0085 (0.0373)	
training:	Epoch: [38][341/408]	Loss 0.0076 (0.0372)	
training:	Epoch: [38][342/408]	Loss 0.0064 (0.0371)	
training:	Epoch: [38][343/408]	Loss 0.3062 (0.0379)	
training:	Epoch: [38][344/408]	Loss 0.0077 (0.0378)	
training:	Epoch: [38][345/408]	Loss 0.0072 (0.0377)	
training:	Epoch: [38][346/408]	Loss 0.3095 (0.0385)	
training:	Epoch: [38][347/408]	Loss 0.0103 (0.0384)	
training:	Epoch: [38][348/408]	Loss 0.0062 (0.0383)	
training:	Epoch: [38][349/408]	Loss 0.0088 (0.0383)	
training:	Epoch: [38][350/408]	Loss 0.0067 (0.0382)	
training:	Epoch: [38][351/408]	Loss 0.0068 (0.0381)	
training:	Epoch: [38][352/408]	Loss 0.0096 (0.0380)	
training:	Epoch: [38][353/408]	Loss 0.0076 (0.0379)	
training:	Epoch: [38][354/408]	Loss 0.0653 (0.0380)	
training:	Epoch: [38][355/408]	Loss 0.0064 (0.0379)	
training:	Epoch: [38][356/408]	Loss 0.0057 (0.0378)	
training:	Epoch: [38][357/408]	Loss 0.0080 (0.0377)	
training:	Epoch: [38][358/408]	Loss 0.0070 (0.0376)	
training:	Epoch: [38][359/408]	Loss 0.0071 (0.0376)	
training:	Epoch: [38][360/408]	Loss 0.0087 (0.0375)	
training:	Epoch: [38][361/408]	Loss 0.0072 (0.0374)	
training:	Epoch: [38][362/408]	Loss 0.0081 (0.0373)	
training:	Epoch: [38][363/408]	Loss 0.0086 (0.0372)	
training:	Epoch: [38][364/408]	Loss 0.0081 (0.0371)	
training:	Epoch: [38][365/408]	Loss 0.0530 (0.0372)	
training:	Epoch: [38][366/408]	Loss 0.0073 (0.0371)	
training:	Epoch: [38][367/408]	Loss 0.0084 (0.0370)	
training:	Epoch: [38][368/408]	Loss 0.0260 (0.0370)	
training:	Epoch: [38][369/408]	Loss 0.0078 (0.0369)	
training:	Epoch: [38][370/408]	Loss 0.3277 (0.0377)	
training:	Epoch: [38][371/408]	Loss 0.1774 (0.0381)	
training:	Epoch: [38][372/408]	Loss 0.0074 (0.0380)	
training:	Epoch: [38][373/408]	Loss 0.0070 (0.0379)	
training:	Epoch: [38][374/408]	Loss 0.0111 (0.0378)	
training:	Epoch: [38][375/408]	Loss 0.0084 (0.0378)	
training:	Epoch: [38][376/408]	Loss 0.2680 (0.0384)	
training:	Epoch: [38][377/408]	Loss 0.0063 (0.0383)	
training:	Epoch: [38][378/408]	Loss 0.0073 (0.0382)	
training:	Epoch: [38][379/408]	Loss 0.0094 (0.0381)	
training:	Epoch: [38][380/408]	Loss 0.0076 (0.0381)	
training:	Epoch: [38][381/408]	Loss 0.0076 (0.0380)	
training:	Epoch: [38][382/408]	Loss 0.0085 (0.0379)	
training:	Epoch: [38][383/408]	Loss 0.0117 (0.0378)	
training:	Epoch: [38][384/408]	Loss 0.3145 (0.0386)	
training:	Epoch: [38][385/408]	Loss 0.0081 (0.0385)	
training:	Epoch: [38][386/408]	Loss 0.0067 (0.0384)	
training:	Epoch: [38][387/408]	Loss 0.0070 (0.0383)	
training:	Epoch: [38][388/408]	Loss 0.0082 (0.0382)	
training:	Epoch: [38][389/408]	Loss 0.0097 (0.0382)	
training:	Epoch: [38][390/408]	Loss 0.0074 (0.0381)	
training:	Epoch: [38][391/408]	Loss 0.0320 (0.0381)	
training:	Epoch: [38][392/408]	Loss 0.0070 (0.0380)	
training:	Epoch: [38][393/408]	Loss 0.0081 (0.0379)	
training:	Epoch: [38][394/408]	Loss 0.0098 (0.0378)	
training:	Epoch: [38][395/408]	Loss 0.0058 (0.0378)	
training:	Epoch: [38][396/408]	Loss 0.0143 (0.0377)	
training:	Epoch: [38][397/408]	Loss 0.0124 (0.0376)	
training:	Epoch: [38][398/408]	Loss 0.0092 (0.0376)	
training:	Epoch: [38][399/408]	Loss 0.0111 (0.0375)	
training:	Epoch: [38][400/408]	Loss 0.0101 (0.0374)	
training:	Epoch: [38][401/408]	Loss 0.0085 (0.0374)	
training:	Epoch: [38][402/408]	Loss 0.0092 (0.0373)	
training:	Epoch: [38][403/408]	Loss 0.0091 (0.0372)	
training:	Epoch: [38][404/408]	Loss 0.0091 (0.0371)	
training:	Epoch: [38][405/408]	Loss 0.0071 (0.0371)	
training:	Epoch: [38][406/408]	Loss 0.0070 (0.0370)	
training:	Epoch: [38][407/408]	Loss 0.0060 (0.0369)	
training:	Epoch: [38][408/408]	Loss 0.0081 (0.0369)	
Training:	 Loss: 0.0368

Training:	 ACC: 0.9938 0.9937 0.9929 0.9946
Validation:	 ACC: 0.7850 0.7854 0.7953 0.7747
Validation:	 Best_BACC: 0.8226 0.8250 0.8772 0.7679
Validation:	 Loss: 0.9601
Pretraining:	Epoch 39/200
----------
training:	Epoch: [39][1/408]	Loss 0.0087 (0.0087)	
training:	Epoch: [39][2/408]	Loss 0.0072 (0.0079)	
training:	Epoch: [39][3/408]	Loss 0.2963 (0.1041)	
training:	Epoch: [39][4/408]	Loss 0.0075 (0.0799)	
training:	Epoch: [39][5/408]	Loss 0.0077 (0.0655)	
training:	Epoch: [39][6/408]	Loss 0.0075 (0.0558)	
training:	Epoch: [39][7/408]	Loss 0.0076 (0.0489)	
training:	Epoch: [39][8/408]	Loss 0.0091 (0.0440)	
training:	Epoch: [39][9/408]	Loss 0.0068 (0.0398)	
training:	Epoch: [39][10/408]	Loss 0.0079 (0.0366)	
training:	Epoch: [39][11/408]	Loss 0.0064 (0.0339)	
training:	Epoch: [39][12/408]	Loss 0.0084 (0.0318)	
training:	Epoch: [39][13/408]	Loss 0.0062 (0.0298)	
training:	Epoch: [39][14/408]	Loss 0.3173 (0.0503)	
training:	Epoch: [39][15/408]	Loss 0.0085 (0.0475)	
training:	Epoch: [39][16/408]	Loss 0.0083 (0.0451)	
training:	Epoch: [39][17/408]	Loss 0.0111 (0.0431)	
training:	Epoch: [39][18/408]	Loss 0.0081 (0.0412)	
training:	Epoch: [39][19/408]	Loss 0.0132 (0.0397)	
training:	Epoch: [39][20/408]	Loss 0.0071 (0.0381)	
training:	Epoch: [39][21/408]	Loss 0.0080 (0.0366)	
training:	Epoch: [39][22/408]	Loss 0.2692 (0.0472)	
training:	Epoch: [39][23/408]	Loss 0.0069 (0.0454)	
training:	Epoch: [39][24/408]	Loss 0.0083 (0.0439)	
training:	Epoch: [39][25/408]	Loss 0.0068 (0.0424)	
training:	Epoch: [39][26/408]	Loss 0.0074 (0.0411)	
training:	Epoch: [39][27/408]	Loss 0.0081 (0.0398)	
training:	Epoch: [39][28/408]	Loss 0.0075 (0.0387)	
training:	Epoch: [39][29/408]	Loss 0.0077 (0.0376)	
training:	Epoch: [39][30/408]	Loss 0.0095 (0.0367)	
training:	Epoch: [39][31/408]	Loss 0.0083 (0.0358)	
training:	Epoch: [39][32/408]	Loss 0.0071 (0.0349)	
training:	Epoch: [39][33/408]	Loss 0.0071 (0.0340)	
training:	Epoch: [39][34/408]	Loss 0.0066 (0.0332)	
training:	Epoch: [39][35/408]	Loss 0.0091 (0.0325)	
training:	Epoch: [39][36/408]	Loss 0.0077 (0.0318)	
training:	Epoch: [39][37/408]	Loss 0.0079 (0.0312)	
training:	Epoch: [39][38/408]	Loss 0.0096 (0.0306)	
training:	Epoch: [39][39/408]	Loss 0.0075 (0.0300)	
training:	Epoch: [39][40/408]	Loss 0.0080 (0.0295)	
training:	Epoch: [39][41/408]	Loss 0.0077 (0.0290)	
training:	Epoch: [39][42/408]	Loss 0.0063 (0.0284)	
training:	Epoch: [39][43/408]	Loss 0.0081 (0.0279)	
training:	Epoch: [39][44/408]	Loss 0.0088 (0.0275)	
training:	Epoch: [39][45/408]	Loss 0.0082 (0.0271)	
training:	Epoch: [39][46/408]	Loss 0.0068 (0.0266)	
training:	Epoch: [39][47/408]	Loss 0.0070 (0.0262)	
training:	Epoch: [39][48/408]	Loss 0.3151 (0.0322)	
training:	Epoch: [39][49/408]	Loss 0.0079 (0.0317)	
training:	Epoch: [39][50/408]	Loss 0.0070 (0.0312)	
training:	Epoch: [39][51/408]	Loss 0.0068 (0.0308)	
training:	Epoch: [39][52/408]	Loss 0.0063 (0.0303)	
training:	Epoch: [39][53/408]	Loss 0.0075 (0.0299)	
training:	Epoch: [39][54/408]	Loss 0.0066 (0.0294)	
training:	Epoch: [39][55/408]	Loss 0.0071 (0.0290)	
training:	Epoch: [39][56/408]	Loss 0.0065 (0.0286)	
training:	Epoch: [39][57/408]	Loss 0.0077 (0.0283)	
training:	Epoch: [39][58/408]	Loss 0.0077 (0.0279)	
training:	Epoch: [39][59/408]	Loss 0.0090 (0.0276)	
training:	Epoch: [39][60/408]	Loss 0.0071 (0.0272)	
training:	Epoch: [39][61/408]	Loss 0.0060 (0.0269)	
training:	Epoch: [39][62/408]	Loss 0.0066 (0.0266)	
training:	Epoch: [39][63/408]	Loss 0.3115 (0.0311)	
training:	Epoch: [39][64/408]	Loss 0.0070 (0.0307)	
training:	Epoch: [39][65/408]	Loss 0.0086 (0.0304)	
training:	Epoch: [39][66/408]	Loss 0.0076 (0.0300)	
training:	Epoch: [39][67/408]	Loss 0.0067 (0.0297)	
training:	Epoch: [39][68/408]	Loss 0.0074 (0.0294)	
training:	Epoch: [39][69/408]	Loss 0.0072 (0.0290)	
training:	Epoch: [39][70/408]	Loss 0.0072 (0.0287)	
training:	Epoch: [39][71/408]	Loss 0.0072 (0.0284)	
training:	Epoch: [39][72/408]	Loss 0.2911 (0.0321)	
training:	Epoch: [39][73/408]	Loss 0.0088 (0.0318)	
training:	Epoch: [39][74/408]	Loss 0.0075 (0.0314)	
training:	Epoch: [39][75/408]	Loss 0.0070 (0.0311)	
training:	Epoch: [39][76/408]	Loss 0.0071 (0.0308)	
training:	Epoch: [39][77/408]	Loss 0.0081 (0.0305)	
training:	Epoch: [39][78/408]	Loss 0.0068 (0.0302)	
training:	Epoch: [39][79/408]	Loss 0.0072 (0.0299)	
training:	Epoch: [39][80/408]	Loss 0.0059 (0.0296)	
training:	Epoch: [39][81/408]	Loss 0.0080 (0.0293)	
training:	Epoch: [39][82/408]	Loss 0.0066 (0.0290)	
training:	Epoch: [39][83/408]	Loss 0.0077 (0.0288)	
training:	Epoch: [39][84/408]	Loss 0.0072 (0.0285)	
training:	Epoch: [39][85/408]	Loss 0.0081 (0.0283)	
training:	Epoch: [39][86/408]	Loss 0.0084 (0.0281)	
training:	Epoch: [39][87/408]	Loss 0.3225 (0.0314)	
training:	Epoch: [39][88/408]	Loss 0.0068 (0.0312)	
training:	Epoch: [39][89/408]	Loss 0.0068 (0.0309)	
training:	Epoch: [39][90/408]	Loss 0.0065 (0.0306)	
training:	Epoch: [39][91/408]	Loss 0.0064 (0.0304)	
training:	Epoch: [39][92/408]	Loss 0.0064 (0.0301)	
training:	Epoch: [39][93/408]	Loss 0.0064 (0.0298)	
training:	Epoch: [39][94/408]	Loss 0.0062 (0.0296)	
training:	Epoch: [39][95/408]	Loss 0.0070 (0.0294)	
training:	Epoch: [39][96/408]	Loss 0.0065 (0.0291)	
training:	Epoch: [39][97/408]	Loss 0.0063 (0.0289)	
training:	Epoch: [39][98/408]	Loss 0.0062 (0.0286)	
training:	Epoch: [39][99/408]	Loss 0.0058 (0.0284)	
training:	Epoch: [39][100/408]	Loss 0.0058 (0.0282)	
training:	Epoch: [39][101/408]	Loss 0.0079 (0.0280)	
training:	Epoch: [39][102/408]	Loss 0.0065 (0.0278)	
training:	Epoch: [39][103/408]	Loss 0.0064 (0.0276)	
training:	Epoch: [39][104/408]	Loss 0.0071 (0.0274)	
training:	Epoch: [39][105/408]	Loss 0.0058 (0.0272)	
training:	Epoch: [39][106/408]	Loss 0.0082 (0.0270)	
training:	Epoch: [39][107/408]	Loss 0.0069 (0.0268)	
training:	Epoch: [39][108/408]	Loss 0.0079 (0.0266)	
training:	Epoch: [39][109/408]	Loss 0.3242 (0.0294)	
training:	Epoch: [39][110/408]	Loss 0.0081 (0.0292)	
training:	Epoch: [39][111/408]	Loss 0.0065 (0.0290)	
training:	Epoch: [39][112/408]	Loss 0.0077 (0.0288)	
training:	Epoch: [39][113/408]	Loss 0.0054 (0.0286)	
training:	Epoch: [39][114/408]	Loss 0.0078 (0.0284)	
training:	Epoch: [39][115/408]	Loss 0.0073 (0.0282)	
training:	Epoch: [39][116/408]	Loss 0.0060 (0.0280)	
training:	Epoch: [39][117/408]	Loss 0.0068 (0.0278)	
training:	Epoch: [39][118/408]	Loss 0.0081 (0.0277)	
training:	Epoch: [39][119/408]	Loss 0.0065 (0.0275)	
training:	Epoch: [39][120/408]	Loss 0.0072 (0.0273)	
training:	Epoch: [39][121/408]	Loss 0.0066 (0.0271)	
training:	Epoch: [39][122/408]	Loss 0.0069 (0.0270)	
training:	Epoch: [39][123/408]	Loss 0.0079 (0.0268)	
training:	Epoch: [39][124/408]	Loss 0.3072 (0.0291)	
training:	Epoch: [39][125/408]	Loss 0.0061 (0.0289)	
training:	Epoch: [39][126/408]	Loss 0.0074 (0.0287)	
training:	Epoch: [39][127/408]	Loss 0.0178 (0.0286)	
training:	Epoch: [39][128/408]	Loss 0.0085 (0.0285)	
training:	Epoch: [39][129/408]	Loss 0.2582 (0.0303)	
training:	Epoch: [39][130/408]	Loss 0.0068 (0.0301)	
training:	Epoch: [39][131/408]	Loss 0.0071 (0.0299)	
training:	Epoch: [39][132/408]	Loss 0.0079 (0.0297)	
training:	Epoch: [39][133/408]	Loss 0.0059 (0.0296)	
training:	Epoch: [39][134/408]	Loss 0.2768 (0.0314)	
training:	Epoch: [39][135/408]	Loss 0.0067 (0.0312)	
training:	Epoch: [39][136/408]	Loss 0.0070 (0.0310)	
training:	Epoch: [39][137/408]	Loss 0.0065 (0.0309)	
training:	Epoch: [39][138/408]	Loss 0.0079 (0.0307)	
training:	Epoch: [39][139/408]	Loss 0.0064 (0.0305)	
training:	Epoch: [39][140/408]	Loss 0.2846 (0.0323)	
training:	Epoch: [39][141/408]	Loss 0.0071 (0.0322)	
training:	Epoch: [39][142/408]	Loss 0.3319 (0.0343)	
training:	Epoch: [39][143/408]	Loss 0.0071 (0.0341)	
training:	Epoch: [39][144/408]	Loss 0.0059 (0.0339)	
training:	Epoch: [39][145/408]	Loss 0.0102 (0.0337)	
training:	Epoch: [39][146/408]	Loss 0.0105 (0.0336)	
training:	Epoch: [39][147/408]	Loss 0.0073 (0.0334)	
training:	Epoch: [39][148/408]	Loss 0.0072 (0.0332)	
training:	Epoch: [39][149/408]	Loss 0.0070 (0.0330)	
training:	Epoch: [39][150/408]	Loss 0.0086 (0.0329)	
training:	Epoch: [39][151/408]	Loss 0.0070 (0.0327)	
training:	Epoch: [39][152/408]	Loss 0.0079 (0.0325)	
training:	Epoch: [39][153/408]	Loss 0.0077 (0.0324)	
training:	Epoch: [39][154/408]	Loss 0.0064 (0.0322)	
training:	Epoch: [39][155/408]	Loss 0.2064 (0.0333)	
training:	Epoch: [39][156/408]	Loss 0.0075 (0.0332)	
training:	Epoch: [39][157/408]	Loss 0.0079 (0.0330)	
training:	Epoch: [39][158/408]	Loss 0.0076 (0.0328)	
training:	Epoch: [39][159/408]	Loss 0.2944 (0.0345)	
training:	Epoch: [39][160/408]	Loss 0.0075 (0.0343)	
training:	Epoch: [39][161/408]	Loss 0.0077 (0.0342)	
training:	Epoch: [39][162/408]	Loss 0.3105 (0.0359)	
training:	Epoch: [39][163/408]	Loss 0.0119 (0.0357)	
training:	Epoch: [39][164/408]	Loss 0.0075 (0.0355)	
training:	Epoch: [39][165/408]	Loss 0.0073 (0.0354)	
training:	Epoch: [39][166/408]	Loss 0.1215 (0.0359)	
training:	Epoch: [39][167/408]	Loss 0.0084 (0.0357)	
training:	Epoch: [39][168/408]	Loss 0.0089 (0.0356)	
training:	Epoch: [39][169/408]	Loss 0.0084 (0.0354)	
training:	Epoch: [39][170/408]	Loss 0.0095 (0.0352)	
training:	Epoch: [39][171/408]	Loss 0.0090 (0.0351)	
training:	Epoch: [39][172/408]	Loss 0.0094 (0.0349)	
training:	Epoch: [39][173/408]	Loss 0.0216 (0.0349)	
training:	Epoch: [39][174/408]	Loss 0.0329 (0.0349)	
training:	Epoch: [39][175/408]	Loss 0.0068 (0.0347)	
training:	Epoch: [39][176/408]	Loss 0.0069 (0.0345)	
training:	Epoch: [39][177/408]	Loss 0.0063 (0.0344)	
training:	Epoch: [39][178/408]	Loss 0.0088 (0.0342)	
training:	Epoch: [39][179/408]	Loss 0.0087 (0.0341)	
training:	Epoch: [39][180/408]	Loss 0.0068 (0.0339)	
training:	Epoch: [39][181/408]	Loss 0.0190 (0.0339)	
training:	Epoch: [39][182/408]	Loss 0.0092 (0.0337)	
training:	Epoch: [39][183/408]	Loss 0.0095 (0.0336)	
training:	Epoch: [39][184/408]	Loss 0.0081 (0.0335)	
training:	Epoch: [39][185/408]	Loss 0.0057 (0.0333)	
training:	Epoch: [39][186/408]	Loss 0.0092 (0.0332)	
training:	Epoch: [39][187/408]	Loss 0.0070 (0.0330)	
training:	Epoch: [39][188/408]	Loss 0.2823 (0.0344)	
training:	Epoch: [39][189/408]	Loss 0.0071 (0.0342)	
training:	Epoch: [39][190/408]	Loss 0.0275 (0.0342)	
training:	Epoch: [39][191/408]	Loss 0.0099 (0.0341)	
training:	Epoch: [39][192/408]	Loss 0.0089 (0.0339)	
training:	Epoch: [39][193/408]	Loss 0.0084 (0.0338)	
training:	Epoch: [39][194/408]	Loss 0.0097 (0.0337)	
training:	Epoch: [39][195/408]	Loss 0.0079 (0.0335)	
training:	Epoch: [39][196/408]	Loss 0.0068 (0.0334)	
training:	Epoch: [39][197/408]	Loss 0.0082 (0.0333)	
training:	Epoch: [39][198/408]	Loss 0.0078 (0.0331)	
training:	Epoch: [39][199/408]	Loss 0.0063 (0.0330)	
training:	Epoch: [39][200/408]	Loss 0.0090 (0.0329)	
training:	Epoch: [39][201/408]	Loss 0.0074 (0.0328)	
training:	Epoch: [39][202/408]	Loss 0.0091 (0.0326)	
training:	Epoch: [39][203/408]	Loss 0.0083 (0.0325)	
training:	Epoch: [39][204/408]	Loss 0.1924 (0.0333)	
training:	Epoch: [39][205/408]	Loss 0.0081 (0.0332)	
training:	Epoch: [39][206/408]	Loss 0.0088 (0.0331)	
training:	Epoch: [39][207/408]	Loss 0.2614 (0.0342)	
training:	Epoch: [39][208/408]	Loss 0.0084 (0.0340)	
training:	Epoch: [39][209/408]	Loss 0.0107 (0.0339)	
training:	Epoch: [39][210/408]	Loss 0.0058 (0.0338)	
training:	Epoch: [39][211/408]	Loss 0.0107 (0.0337)	
training:	Epoch: [39][212/408]	Loss 0.0059 (0.0336)	
training:	Epoch: [39][213/408]	Loss 0.0072 (0.0334)	
training:	Epoch: [39][214/408]	Loss 0.0057 (0.0333)	
training:	Epoch: [39][215/408]	Loss 0.0069 (0.0332)	
training:	Epoch: [39][216/408]	Loss 0.0070 (0.0331)	
training:	Epoch: [39][217/408]	Loss 0.0077 (0.0329)	
training:	Epoch: [39][218/408]	Loss 0.0118 (0.0328)	
training:	Epoch: [39][219/408]	Loss 0.3162 (0.0341)	
training:	Epoch: [39][220/408]	Loss 0.3034 (0.0354)	
training:	Epoch: [39][221/408]	Loss 0.0079 (0.0352)	
training:	Epoch: [39][222/408]	Loss 0.0076 (0.0351)	
training:	Epoch: [39][223/408]	Loss 0.0077 (0.0350)	
training:	Epoch: [39][224/408]	Loss 0.0091 (0.0349)	
training:	Epoch: [39][225/408]	Loss 0.0090 (0.0348)	
training:	Epoch: [39][226/408]	Loss 0.0090 (0.0346)	
training:	Epoch: [39][227/408]	Loss 0.0081 (0.0345)	
training:	Epoch: [39][228/408]	Loss 0.0098 (0.0344)	
training:	Epoch: [39][229/408]	Loss 0.0067 (0.0343)	
training:	Epoch: [39][230/408]	Loss 0.0103 (0.0342)	
training:	Epoch: [39][231/408]	Loss 0.0094 (0.0341)	
training:	Epoch: [39][232/408]	Loss 0.0113 (0.0340)	
training:	Epoch: [39][233/408]	Loss 0.0086 (0.0339)	
training:	Epoch: [39][234/408]	Loss 0.0074 (0.0338)	
training:	Epoch: [39][235/408]	Loss 0.0081 (0.0337)	
training:	Epoch: [39][236/408]	Loss 0.0088 (0.0336)	
training:	Epoch: [39][237/408]	Loss 0.0074 (0.0334)	
training:	Epoch: [39][238/408]	Loss 0.0104 (0.0333)	
training:	Epoch: [39][239/408]	Loss 0.0077 (0.0332)	
training:	Epoch: [39][240/408]	Loss 0.0075 (0.0331)	
training:	Epoch: [39][241/408]	Loss 0.0067 (0.0330)	
training:	Epoch: [39][242/408]	Loss 0.0066 (0.0329)	
training:	Epoch: [39][243/408]	Loss 0.0060 (0.0328)	
training:	Epoch: [39][244/408]	Loss 0.0078 (0.0327)	
training:	Epoch: [39][245/408]	Loss 0.0077 (0.0326)	
training:	Epoch: [39][246/408]	Loss 0.2750 (0.0336)	
training:	Epoch: [39][247/408]	Loss 0.0068 (0.0335)	
training:	Epoch: [39][248/408]	Loss 0.0081 (0.0334)	
training:	Epoch: [39][249/408]	Loss 0.0065 (0.0333)	
training:	Epoch: [39][250/408]	Loss 0.0099 (0.0332)	
training:	Epoch: [39][251/408]	Loss 0.0067 (0.0331)	
training:	Epoch: [39][252/408]	Loss 0.0076 (0.0330)	
training:	Epoch: [39][253/408]	Loss 0.0095 (0.0329)	
training:	Epoch: [39][254/408]	Loss 0.0061 (0.0328)	
training:	Epoch: [39][255/408]	Loss 0.0064 (0.0327)	
training:	Epoch: [39][256/408]	Loss 0.0079 (0.0326)	
training:	Epoch: [39][257/408]	Loss 0.0072 (0.0325)	
training:	Epoch: [39][258/408]	Loss 0.0073 (0.0324)	
training:	Epoch: [39][259/408]	Loss 0.0074 (0.0323)	
training:	Epoch: [39][260/408]	Loss 0.0081 (0.0322)	
training:	Epoch: [39][261/408]	Loss 0.0078 (0.0321)	
training:	Epoch: [39][262/408]	Loss 0.0093 (0.0320)	
training:	Epoch: [39][263/408]	Loss 0.0062 (0.0319)	
training:	Epoch: [39][264/408]	Loss 0.0065 (0.0318)	
training:	Epoch: [39][265/408]	Loss 0.0083 (0.0317)	
training:	Epoch: [39][266/408]	Loss 0.0063 (0.0316)	
training:	Epoch: [39][267/408]	Loss 0.0062 (0.0315)	
training:	Epoch: [39][268/408]	Loss 0.0064 (0.0314)	
training:	Epoch: [39][269/408]	Loss 0.0070 (0.0313)	
training:	Epoch: [39][270/408]	Loss 0.0069 (0.0313)	
training:	Epoch: [39][271/408]	Loss 0.0080 (0.0312)	
training:	Epoch: [39][272/408]	Loss 0.0077 (0.0311)	
training:	Epoch: [39][273/408]	Loss 0.0059 (0.0310)	
training:	Epoch: [39][274/408]	Loss 0.0059 (0.0309)	
training:	Epoch: [39][275/408]	Loss 0.0069 (0.0308)	
training:	Epoch: [39][276/408]	Loss 0.0070 (0.0307)	
training:	Epoch: [39][277/408]	Loss 0.0082 (0.0306)	
training:	Epoch: [39][278/408]	Loss 0.0074 (0.0306)	
training:	Epoch: [39][279/408]	Loss 0.0075 (0.0305)	
training:	Epoch: [39][280/408]	Loss 0.0067 (0.0304)	
training:	Epoch: [39][281/408]	Loss 0.0074 (0.0303)	
training:	Epoch: [39][282/408]	Loss 0.0066 (0.0302)	
training:	Epoch: [39][283/408]	Loss 0.0071 (0.0301)	
training:	Epoch: [39][284/408]	Loss 0.0081 (0.0301)	
training:	Epoch: [39][285/408]	Loss 0.0075 (0.0300)	
training:	Epoch: [39][286/408]	Loss 0.0066 (0.0299)	
training:	Epoch: [39][287/408]	Loss 0.0061 (0.0298)	
training:	Epoch: [39][288/408]	Loss 0.2796 (0.0307)	
training:	Epoch: [39][289/408]	Loss 0.0061 (0.0306)	
training:	Epoch: [39][290/408]	Loss 0.0062 (0.0305)	
training:	Epoch: [39][291/408]	Loss 0.0065 (0.0304)	
training:	Epoch: [39][292/408]	Loss 0.0061 (0.0304)	
training:	Epoch: [39][293/408]	Loss 0.0068 (0.0303)	
training:	Epoch: [39][294/408]	Loss 0.0074 (0.0302)	
training:	Epoch: [39][295/408]	Loss 0.0074 (0.0301)	
training:	Epoch: [39][296/408]	Loss 0.0064 (0.0300)	
training:	Epoch: [39][297/408]	Loss 0.0074 (0.0300)	
training:	Epoch: [39][298/408]	Loss 0.0060 (0.0299)	
training:	Epoch: [39][299/408]	Loss 0.0076 (0.0298)	
training:	Epoch: [39][300/408]	Loss 0.0071 (0.0297)	
training:	Epoch: [39][301/408]	Loss 0.3130 (0.0307)	
training:	Epoch: [39][302/408]	Loss 0.0078 (0.0306)	
training:	Epoch: [39][303/408]	Loss 0.0064 (0.0305)	
training:	Epoch: [39][304/408]	Loss 0.0074 (0.0304)	
training:	Epoch: [39][305/408]	Loss 0.0067 (0.0304)	
training:	Epoch: [39][306/408]	Loss 0.0069 (0.0303)	
training:	Epoch: [39][307/408]	Loss 0.0065 (0.0302)	
training:	Epoch: [39][308/408]	Loss 0.0067 (0.0301)	
training:	Epoch: [39][309/408]	Loss 0.0065 (0.0301)	
training:	Epoch: [39][310/408]	Loss 0.0070 (0.0300)	
training:	Epoch: [39][311/408]	Loss 0.0057 (0.0299)	
training:	Epoch: [39][312/408]	Loss 0.0066 (0.0298)	
training:	Epoch: [39][313/408]	Loss 0.0086 (0.0298)	
training:	Epoch: [39][314/408]	Loss 0.0057 (0.0297)	
training:	Epoch: [39][315/408]	Loss 0.0067 (0.0296)	
training:	Epoch: [39][316/408]	Loss 0.2866 (0.0304)	
training:	Epoch: [39][317/408]	Loss 0.0060 (0.0303)	
training:	Epoch: [39][318/408]	Loss 0.0070 (0.0303)	
training:	Epoch: [39][319/408]	Loss 0.0070 (0.0302)	
training:	Epoch: [39][320/408]	Loss 0.0059 (0.0301)	
training:	Epoch: [39][321/408]	Loss 0.0063 (0.0300)	
training:	Epoch: [39][322/408]	Loss 0.3353 (0.0310)	
training:	Epoch: [39][323/408]	Loss 0.0066 (0.0309)	
training:	Epoch: [39][324/408]	Loss 0.0061 (0.0308)	
training:	Epoch: [39][325/408]	Loss 0.0067 (0.0308)	
training:	Epoch: [39][326/408]	Loss 0.0072 (0.0307)	
training:	Epoch: [39][327/408]	Loss 0.3091 (0.0316)	
training:	Epoch: [39][328/408]	Loss 0.2948 (0.0324)	
training:	Epoch: [39][329/408]	Loss 0.0066 (0.0323)	
training:	Epoch: [39][330/408]	Loss 0.0056 (0.0322)	
training:	Epoch: [39][331/408]	Loss 0.8695 (0.0347)	
training:	Epoch: [39][332/408]	Loss 0.0067 (0.0346)	
training:	Epoch: [39][333/408]	Loss 0.0068 (0.0346)	
training:	Epoch: [39][334/408]	Loss 0.0070 (0.0345)	
training:	Epoch: [39][335/408]	Loss 0.0075 (0.0344)	
training:	Epoch: [39][336/408]	Loss 0.0071 (0.0343)	
training:	Epoch: [39][337/408]	Loss 0.0067 (0.0342)	
training:	Epoch: [39][338/408]	Loss 0.0063 (0.0341)	
training:	Epoch: [39][339/408]	Loss 0.0067 (0.0341)	
training:	Epoch: [39][340/408]	Loss 0.0082 (0.0340)	
training:	Epoch: [39][341/408]	Loss 0.0098 (0.0339)	
training:	Epoch: [39][342/408]	Loss 0.0065 (0.0338)	
training:	Epoch: [39][343/408]	Loss 0.0073 (0.0338)	
training:	Epoch: [39][344/408]	Loss 0.0065 (0.0337)	
training:	Epoch: [39][345/408]	Loss 0.0058 (0.0336)	
training:	Epoch: [39][346/408]	Loss 0.0066 (0.0335)	
training:	Epoch: [39][347/408]	Loss 0.0063 (0.0334)	
training:	Epoch: [39][348/408]	Loss 0.0063 (0.0334)	
training:	Epoch: [39][349/408]	Loss 0.0069 (0.0333)	
training:	Epoch: [39][350/408]	Loss 0.0063 (0.0332)	
training:	Epoch: [39][351/408]	Loss 0.0068 (0.0331)	
training:	Epoch: [39][352/408]	Loss 0.0082 (0.0331)	
training:	Epoch: [39][353/408]	Loss 0.0076 (0.0330)	
training:	Epoch: [39][354/408]	Loss 0.0079 (0.0329)	
training:	Epoch: [39][355/408]	Loss 0.0057 (0.0328)	
training:	Epoch: [39][356/408]	Loss 0.0070 (0.0328)	
training:	Epoch: [39][357/408]	Loss 0.2711 (0.0334)	
training:	Epoch: [39][358/408]	Loss 0.2568 (0.0341)	
training:	Epoch: [39][359/408]	Loss 0.0075 (0.0340)	
training:	Epoch: [39][360/408]	Loss 0.1809 (0.0344)	
training:	Epoch: [39][361/408]	Loss 0.0064 (0.0343)	
training:	Epoch: [39][362/408]	Loss 0.2469 (0.0349)	
training:	Epoch: [39][363/408]	Loss 0.0082 (0.0348)	
training:	Epoch: [39][364/408]	Loss 0.0078 (0.0348)	
training:	Epoch: [39][365/408]	Loss 0.0062 (0.0347)	
training:	Epoch: [39][366/408]	Loss 0.0081 (0.0346)	
training:	Epoch: [39][367/408]	Loss 0.0069 (0.0345)	
training:	Epoch: [39][368/408]	Loss 0.0085 (0.0345)	
training:	Epoch: [39][369/408]	Loss 0.0068 (0.0344)	
training:	Epoch: [39][370/408]	Loss 0.0083 (0.0343)	
training:	Epoch: [39][371/408]	Loss 0.0092 (0.0343)	
training:	Epoch: [39][372/408]	Loss 0.0098 (0.0342)	
training:	Epoch: [39][373/408]	Loss 0.0085 (0.0341)	
training:	Epoch: [39][374/408]	Loss 0.0083 (0.0340)	
training:	Epoch: [39][375/408]	Loss 0.3273 (0.0348)	
training:	Epoch: [39][376/408]	Loss 0.0060 (0.0348)	
training:	Epoch: [39][377/408]	Loss 0.0075 (0.0347)	
training:	Epoch: [39][378/408]	Loss 0.0116 (0.0346)	
training:	Epoch: [39][379/408]	Loss 0.2595 (0.0352)	
training:	Epoch: [39][380/408]	Loss 0.0091 (0.0351)	
training:	Epoch: [39][381/408]	Loss 0.0094 (0.0351)	
training:	Epoch: [39][382/408]	Loss 0.3148 (0.0358)	
training:	Epoch: [39][383/408]	Loss 0.0072 (0.0357)	
training:	Epoch: [39][384/408]	Loss 0.0088 (0.0357)	
training:	Epoch: [39][385/408]	Loss 0.0089 (0.0356)	
training:	Epoch: [39][386/408]	Loss 0.0185 (0.0356)	
training:	Epoch: [39][387/408]	Loss 0.0086 (0.0355)	
training:	Epoch: [39][388/408]	Loss 0.0128 (0.0354)	
training:	Epoch: [39][389/408]	Loss 0.0069 (0.0354)	
training:	Epoch: [39][390/408]	Loss 0.0140 (0.0353)	
training:	Epoch: [39][391/408]	Loss 0.0078 (0.0352)	
training:	Epoch: [39][392/408]	Loss 0.0065 (0.0352)	
training:	Epoch: [39][393/408]	Loss 0.0095 (0.0351)	
training:	Epoch: [39][394/408]	Loss 0.0075 (0.0350)	
training:	Epoch: [39][395/408]	Loss 0.0073 (0.0349)	
training:	Epoch: [39][396/408]	Loss 0.0112 (0.0349)	
training:	Epoch: [39][397/408]	Loss 0.0089 (0.0348)	
training:	Epoch: [39][398/408]	Loss 0.0064 (0.0347)	
training:	Epoch: [39][399/408]	Loss 0.2943 (0.0354)	
training:	Epoch: [39][400/408]	Loss 0.0072 (0.0353)	
training:	Epoch: [39][401/408]	Loss 0.0076 (0.0353)	
training:	Epoch: [39][402/408]	Loss 0.2910 (0.0359)	
training:	Epoch: [39][403/408]	Loss 0.0079 (0.0358)	
training:	Epoch: [39][404/408]	Loss 0.0092 (0.0358)	
training:	Epoch: [39][405/408]	Loss 0.0098 (0.0357)	
training:	Epoch: [39][406/408]	Loss 0.0086 (0.0356)	
training:	Epoch: [39][407/408]	Loss 0.0064 (0.0356)	
training:	Epoch: [39][408/408]	Loss 0.0087 (0.0355)	
Training:	 Loss: 0.0354

Training:	 ACC: 0.9939 0.9939 0.9932 0.9946
Validation:	 ACC: 0.7877 0.7881 0.7963 0.7791
Validation:	 Best_BACC: 0.8226 0.8250 0.8772 0.7679
Validation:	 Loss: 0.9478
Pretraining:	Epoch 40/200
----------
training:	Epoch: [40][1/408]	Loss 0.0098 (0.0098)	
training:	Epoch: [40][2/408]	Loss 0.0069 (0.0084)	
training:	Epoch: [40][3/408]	Loss 0.0099 (0.0089)	
training:	Epoch: [40][4/408]	Loss 0.0068 (0.0084)	
training:	Epoch: [40][5/408]	Loss 0.0076 (0.0082)	
training:	Epoch: [40][6/408]	Loss 0.0067 (0.0080)	
training:	Epoch: [40][7/408]	Loss 0.0082 (0.0080)	
training:	Epoch: [40][8/408]	Loss 0.0070 (0.0079)	
training:	Epoch: [40][9/408]	Loss 0.0084 (0.0079)	
training:	Epoch: [40][10/408]	Loss 0.0093 (0.0081)	
training:	Epoch: [40][11/408]	Loss 0.0104 (0.0083)	
training:	Epoch: [40][12/408]	Loss 0.0072 (0.0082)	
training:	Epoch: [40][13/408]	Loss 0.0077 (0.0082)	
training:	Epoch: [40][14/408]	Loss 0.0068 (0.0081)	
training:	Epoch: [40][15/408]	Loss 0.0076 (0.0080)	
training:	Epoch: [40][16/408]	Loss 0.0076 (0.0080)	
training:	Epoch: [40][17/408]	Loss 0.0070 (0.0079)	
training:	Epoch: [40][18/408]	Loss 0.0072 (0.0079)	
training:	Epoch: [40][19/408]	Loss 0.0082 (0.0079)	
training:	Epoch: [40][20/408]	Loss 0.0080 (0.0079)	
training:	Epoch: [40][21/408]	Loss 0.0068 (0.0079)	
training:	Epoch: [40][22/408]	Loss 0.0081 (0.0079)	
training:	Epoch: [40][23/408]	Loss 0.0085 (0.0079)	
training:	Epoch: [40][24/408]	Loss 0.0078 (0.0079)	
training:	Epoch: [40][25/408]	Loss 0.0061 (0.0078)	
training:	Epoch: [40][26/408]	Loss 0.0083 (0.0078)	
training:	Epoch: [40][27/408]	Loss 0.0079 (0.0078)	
training:	Epoch: [40][28/408]	Loss 0.0064 (0.0078)	
training:	Epoch: [40][29/408]	Loss 0.0072 (0.0078)	
training:	Epoch: [40][30/408]	Loss 0.0058 (0.0077)	
training:	Epoch: [40][31/408]	Loss 0.0072 (0.0077)	
training:	Epoch: [40][32/408]	Loss 0.0068 (0.0077)	
training:	Epoch: [40][33/408]	Loss 0.2847 (0.0161)	
training:	Epoch: [40][34/408]	Loss 0.3169 (0.0249)	
training:	Epoch: [40][35/408]	Loss 0.0068 (0.0244)	
training:	Epoch: [40][36/408]	Loss 0.6440 (0.0416)	
training:	Epoch: [40][37/408]	Loss 0.0087 (0.0407)	
training:	Epoch: [40][38/408]	Loss 0.0067 (0.0398)	
training:	Epoch: [40][39/408]	Loss 0.0084 (0.0390)	
training:	Epoch: [40][40/408]	Loss 0.0072 (0.0382)	
training:	Epoch: [40][41/408]	Loss 0.0068 (0.0375)	
training:	Epoch: [40][42/408]	Loss 0.0082 (0.0368)	
training:	Epoch: [40][43/408]	Loss 0.0068 (0.0361)	
training:	Epoch: [40][44/408]	Loss 0.3141 (0.0424)	
training:	Epoch: [40][45/408]	Loss 0.0074 (0.0416)	
training:	Epoch: [40][46/408]	Loss 0.0076 (0.0409)	
training:	Epoch: [40][47/408]	Loss 0.0086 (0.0402)	
training:	Epoch: [40][48/408]	Loss 0.0064 (0.0395)	
training:	Epoch: [40][49/408]	Loss 0.0090 (0.0389)	
training:	Epoch: [40][50/408]	Loss 0.0075 (0.0382)	
training:	Epoch: [40][51/408]	Loss 0.2990 (0.0433)	
training:	Epoch: [40][52/408]	Loss 0.0079 (0.0427)	
training:	Epoch: [40][53/408]	Loss 0.0099 (0.0420)	
training:	Epoch: [40][54/408]	Loss 0.0082 (0.0414)	
training:	Epoch: [40][55/408]	Loss 0.0076 (0.0408)	
training:	Epoch: [40][56/408]	Loss 0.0065 (0.0402)	
training:	Epoch: [40][57/408]	Loss 0.0061 (0.0396)	
training:	Epoch: [40][58/408]	Loss 0.0069 (0.0390)	
training:	Epoch: [40][59/408]	Loss 0.2700 (0.0429)	
training:	Epoch: [40][60/408]	Loss 0.0067 (0.0423)	
training:	Epoch: [40][61/408]	Loss 0.0065 (0.0418)	
training:	Epoch: [40][62/408]	Loss 0.0093 (0.0412)	
training:	Epoch: [40][63/408]	Loss 0.0080 (0.0407)	
training:	Epoch: [40][64/408]	Loss 0.0079 (0.0402)	
training:	Epoch: [40][65/408]	Loss 0.2711 (0.0437)	
training:	Epoch: [40][66/408]	Loss 0.0081 (0.0432)	
training:	Epoch: [40][67/408]	Loss 0.0076 (0.0427)	
training:	Epoch: [40][68/408]	Loss 0.0085 (0.0422)	
training:	Epoch: [40][69/408]	Loss 0.2648 (0.0454)	
training:	Epoch: [40][70/408]	Loss 0.0081 (0.0449)	
training:	Epoch: [40][71/408]	Loss 0.0078 (0.0443)	
training:	Epoch: [40][72/408]	Loss 0.0071 (0.0438)	
training:	Epoch: [40][73/408]	Loss 0.0058 (0.0433)	
training:	Epoch: [40][74/408]	Loss 0.0065 (0.0428)	
training:	Epoch: [40][75/408]	Loss 0.0067 (0.0423)	
training:	Epoch: [40][76/408]	Loss 0.0076 (0.0419)	
training:	Epoch: [40][77/408]	Loss 0.0095 (0.0414)	
training:	Epoch: [40][78/408]	Loss 0.2706 (0.0444)	
training:	Epoch: [40][79/408]	Loss 0.1741 (0.0460)	
training:	Epoch: [40][80/408]	Loss 0.0078 (0.0455)	
training:	Epoch: [40][81/408]	Loss 0.0064 (0.0451)	
training:	Epoch: [40][82/408]	Loss 0.0089 (0.0446)	
training:	Epoch: [40][83/408]	Loss 0.0055 (0.0441)	
training:	Epoch: [40][84/408]	Loss 0.0079 (0.0437)	
training:	Epoch: [40][85/408]	Loss 0.0084 (0.0433)	
training:	Epoch: [40][86/408]	Loss 0.0083 (0.0429)	
training:	Epoch: [40][87/408]	Loss 0.0068 (0.0425)	
training:	Epoch: [40][88/408]	Loss 0.0089 (0.0421)	
training:	Epoch: [40][89/408]	Loss 0.2786 (0.0448)	
training:	Epoch: [40][90/408]	Loss 0.0102 (0.0444)	
training:	Epoch: [40][91/408]	Loss 0.0072 (0.0440)	
training:	Epoch: [40][92/408]	Loss 0.0087 (0.0436)	
training:	Epoch: [40][93/408]	Loss 0.0091 (0.0432)	
training:	Epoch: [40][94/408]	Loss 0.0065 (0.0428)	
training:	Epoch: [40][95/408]	Loss 0.0090 (0.0425)	
training:	Epoch: [40][96/408]	Loss 0.0137 (0.0422)	
training:	Epoch: [40][97/408]	Loss 0.0074 (0.0418)	
training:	Epoch: [40][98/408]	Loss 0.0077 (0.0415)	
training:	Epoch: [40][99/408]	Loss 0.0073 (0.0411)	
training:	Epoch: [40][100/408]	Loss 0.0095 (0.0408)	
training:	Epoch: [40][101/408]	Loss 0.0075 (0.0405)	
training:	Epoch: [40][102/408]	Loss 0.0075 (0.0401)	
training:	Epoch: [40][103/408]	Loss 0.0093 (0.0398)	
training:	Epoch: [40][104/408]	Loss 0.0086 (0.0395)	
training:	Epoch: [40][105/408]	Loss 0.0081 (0.0392)	
training:	Epoch: [40][106/408]	Loss 0.0069 (0.0389)	
training:	Epoch: [40][107/408]	Loss 0.0071 (0.0386)	
training:	Epoch: [40][108/408]	Loss 0.0061 (0.0383)	
training:	Epoch: [40][109/408]	Loss 0.0094 (0.0381)	
training:	Epoch: [40][110/408]	Loss 0.2807 (0.0403)	
training:	Epoch: [40][111/408]	Loss 0.0071 (0.0400)	
training:	Epoch: [40][112/408]	Loss 0.0090 (0.0397)	
training:	Epoch: [40][113/408]	Loss 0.0078 (0.0394)	
training:	Epoch: [40][114/408]	Loss 0.0070 (0.0391)	
training:	Epoch: [40][115/408]	Loss 0.0071 (0.0389)	
training:	Epoch: [40][116/408]	Loss 0.0076 (0.0386)	
training:	Epoch: [40][117/408]	Loss 0.0076 (0.0383)	
training:	Epoch: [40][118/408]	Loss 0.0072 (0.0381)	
training:	Epoch: [40][119/408]	Loss 0.7243 (0.0438)	
training:	Epoch: [40][120/408]	Loss 0.0059 (0.0435)	
training:	Epoch: [40][121/408]	Loss 0.0060 (0.0432)	
training:	Epoch: [40][122/408]	Loss 0.0088 (0.0429)	
training:	Epoch: [40][123/408]	Loss 0.0073 (0.0426)	
training:	Epoch: [40][124/408]	Loss 0.0085 (0.0424)	
training:	Epoch: [40][125/408]	Loss 0.0107 (0.0421)	
training:	Epoch: [40][126/408]	Loss 0.0074 (0.0418)	
training:	Epoch: [40][127/408]	Loss 0.0079 (0.0416)	
training:	Epoch: [40][128/408]	Loss 0.0075 (0.0413)	
training:	Epoch: [40][129/408]	Loss 0.0094 (0.0410)	
training:	Epoch: [40][130/408]	Loss 0.2421 (0.0426)	
training:	Epoch: [40][131/408]	Loss 0.0081 (0.0423)	
training:	Epoch: [40][132/408]	Loss 0.0112 (0.0421)	
training:	Epoch: [40][133/408]	Loss 0.0071 (0.0418)	
training:	Epoch: [40][134/408]	Loss 0.0106 (0.0416)	
training:	Epoch: [40][135/408]	Loss 0.0096 (0.0414)	
training:	Epoch: [40][136/408]	Loss 0.2877 (0.0432)	
training:	Epoch: [40][137/408]	Loss 0.0088 (0.0429)	
training:	Epoch: [40][138/408]	Loss 0.0075 (0.0427)	
training:	Epoch: [40][139/408]	Loss 0.0082 (0.0424)	
training:	Epoch: [40][140/408]	Loss 0.0095 (0.0422)	
training:	Epoch: [40][141/408]	Loss 0.0085 (0.0419)	
training:	Epoch: [40][142/408]	Loss 0.0072 (0.0417)	
training:	Epoch: [40][143/408]	Loss 0.2732 (0.0433)	
training:	Epoch: [40][144/408]	Loss 0.0070 (0.0431)	
training:	Epoch: [40][145/408]	Loss 0.0091 (0.0428)	
training:	Epoch: [40][146/408]	Loss 0.3086 (0.0447)	
training:	Epoch: [40][147/408]	Loss 0.0104 (0.0444)	
training:	Epoch: [40][148/408]	Loss 0.0161 (0.0442)	
training:	Epoch: [40][149/408]	Loss 0.0096 (0.0440)	
training:	Epoch: [40][150/408]	Loss 0.0083 (0.0438)	
training:	Epoch: [40][151/408]	Loss 0.0072 (0.0435)	
training:	Epoch: [40][152/408]	Loss 0.0081 (0.0433)	
training:	Epoch: [40][153/408]	Loss 0.0080 (0.0431)	
training:	Epoch: [40][154/408]	Loss 0.0076 (0.0428)	
training:	Epoch: [40][155/408]	Loss 0.0093 (0.0426)	
training:	Epoch: [40][156/408]	Loss 0.0085 (0.0424)	
training:	Epoch: [40][157/408]	Loss 0.0082 (0.0422)	
training:	Epoch: [40][158/408]	Loss 0.0082 (0.0420)	
training:	Epoch: [40][159/408]	Loss 0.0081 (0.0417)	
training:	Epoch: [40][160/408]	Loss 0.0116 (0.0416)	
training:	Epoch: [40][161/408]	Loss 0.0085 (0.0413)	
training:	Epoch: [40][162/408]	Loss 0.0085 (0.0411)	
training:	Epoch: [40][163/408]	Loss 0.0080 (0.0409)	
training:	Epoch: [40][164/408]	Loss 0.0093 (0.0407)	
training:	Epoch: [40][165/408]	Loss 0.0062 (0.0405)	
training:	Epoch: [40][166/408]	Loss 0.0109 (0.0404)	
training:	Epoch: [40][167/408]	Loss 0.0078 (0.0402)	
training:	Epoch: [40][168/408]	Loss 0.0092 (0.0400)	
training:	Epoch: [40][169/408]	Loss 0.0085 (0.0398)	
training:	Epoch: [40][170/408]	Loss 0.0082 (0.0396)	
training:	Epoch: [40][171/408]	Loss 0.0085 (0.0394)	
training:	Epoch: [40][172/408]	Loss 0.2470 (0.0406)	
training:	Epoch: [40][173/408]	Loss 0.0076 (0.0404)	
training:	Epoch: [40][174/408]	Loss 0.0066 (0.0402)	
training:	Epoch: [40][175/408]	Loss 0.0090 (0.0401)	
training:	Epoch: [40][176/408]	Loss 0.0123 (0.0399)	
training:	Epoch: [40][177/408]	Loss 0.0075 (0.0397)	
training:	Epoch: [40][178/408]	Loss 0.0078 (0.0396)	
training:	Epoch: [40][179/408]	Loss 0.0071 (0.0394)	
training:	Epoch: [40][180/408]	Loss 0.0082 (0.0392)	
training:	Epoch: [40][181/408]	Loss 0.0066 (0.0390)	
training:	Epoch: [40][182/408]	Loss 0.0095 (0.0389)	
training:	Epoch: [40][183/408]	Loss 0.0088 (0.0387)	
training:	Epoch: [40][184/408]	Loss 0.0059 (0.0385)	
training:	Epoch: [40][185/408]	Loss 0.0084 (0.0383)	
training:	Epoch: [40][186/408]	Loss 0.0072 (0.0382)	
training:	Epoch: [40][187/408]	Loss 0.0077 (0.0380)	
training:	Epoch: [40][188/408]	Loss 0.0089 (0.0379)	
training:	Epoch: [40][189/408]	Loss 0.0117 (0.0377)	
training:	Epoch: [40][190/408]	Loss 0.0116 (0.0376)	
training:	Epoch: [40][191/408]	Loss 0.0082 (0.0374)	
training:	Epoch: [40][192/408]	Loss 0.0080 (0.0373)	
training:	Epoch: [40][193/408]	Loss 0.0078 (0.0371)	
training:	Epoch: [40][194/408]	Loss 0.0130 (0.0370)	
training:	Epoch: [40][195/408]	Loss 0.0102 (0.0369)	
training:	Epoch: [40][196/408]	Loss 0.0103 (0.0367)	
training:	Epoch: [40][197/408]	Loss 0.0098 (0.0366)	
training:	Epoch: [40][198/408]	Loss 0.0066 (0.0364)	
training:	Epoch: [40][199/408]	Loss 0.0230 (0.0364)	
training:	Epoch: [40][200/408]	Loss 0.0080 (0.0362)	
training:	Epoch: [40][201/408]	Loss 0.0074 (0.0361)	
training:	Epoch: [40][202/408]	Loss 0.0069 (0.0359)	
training:	Epoch: [40][203/408]	Loss 0.0072 (0.0358)	
training:	Epoch: [40][204/408]	Loss 0.0075 (0.0357)	
training:	Epoch: [40][205/408]	Loss 0.0082 (0.0355)	
training:	Epoch: [40][206/408]	Loss 0.0061 (0.0354)	
training:	Epoch: [40][207/408]	Loss 0.0075 (0.0353)	
training:	Epoch: [40][208/408]	Loss 0.2882 (0.0365)	
training:	Epoch: [40][209/408]	Loss 0.0063 (0.0363)	
training:	Epoch: [40][210/408]	Loss 0.5819 (0.0389)	
training:	Epoch: [40][211/408]	Loss 0.0074 (0.0388)	
training:	Epoch: [40][212/408]	Loss 0.0071 (0.0386)	
training:	Epoch: [40][213/408]	Loss 0.0069 (0.0385)	
training:	Epoch: [40][214/408]	Loss 0.0059 (0.0383)	
training:	Epoch: [40][215/408]	Loss 0.0095 (0.0382)	
training:	Epoch: [40][216/408]	Loss 0.0081 (0.0381)	
training:	Epoch: [40][217/408]	Loss 0.0093 (0.0379)	
training:	Epoch: [40][218/408]	Loss 0.2817 (0.0390)	
training:	Epoch: [40][219/408]	Loss 0.0067 (0.0389)	
training:	Epoch: [40][220/408]	Loss 0.0064 (0.0387)	
training:	Epoch: [40][221/408]	Loss 0.0071 (0.0386)	
training:	Epoch: [40][222/408]	Loss 0.0080 (0.0385)	
training:	Epoch: [40][223/408]	Loss 0.0077 (0.0383)	
training:	Epoch: [40][224/408]	Loss 0.0065 (0.0382)	
training:	Epoch: [40][225/408]	Loss 0.0085 (0.0380)	
training:	Epoch: [40][226/408]	Loss 0.0073 (0.0379)	
training:	Epoch: [40][227/408]	Loss 0.0063 (0.0378)	
training:	Epoch: [40][228/408]	Loss 0.0080 (0.0376)	
training:	Epoch: [40][229/408]	Loss 0.0076 (0.0375)	
training:	Epoch: [40][230/408]	Loss 0.0104 (0.0374)	
training:	Epoch: [40][231/408]	Loss 0.0076 (0.0373)	
training:	Epoch: [40][232/408]	Loss 0.0077 (0.0371)	
training:	Epoch: [40][233/408]	Loss 0.0075 (0.0370)	
training:	Epoch: [40][234/408]	Loss 0.0086 (0.0369)	
training:	Epoch: [40][235/408]	Loss 0.0084 (0.0368)	
training:	Epoch: [40][236/408]	Loss 0.0085 (0.0366)	
training:	Epoch: [40][237/408]	Loss 0.0068 (0.0365)	
training:	Epoch: [40][238/408]	Loss 0.0070 (0.0364)	
training:	Epoch: [40][239/408]	Loss 0.0076 (0.0363)	
training:	Epoch: [40][240/408]	Loss 0.0061 (0.0361)	
training:	Epoch: [40][241/408]	Loss 0.0068 (0.0360)	
training:	Epoch: [40][242/408]	Loss 0.0077 (0.0359)	
training:	Epoch: [40][243/408]	Loss 0.0076 (0.0358)	
training:	Epoch: [40][244/408]	Loss 0.0085 (0.0357)	
training:	Epoch: [40][245/408]	Loss 0.0072 (0.0356)	
training:	Epoch: [40][246/408]	Loss 0.0072 (0.0355)	
training:	Epoch: [40][247/408]	Loss 0.0090 (0.0353)	
training:	Epoch: [40][248/408]	Loss 0.2812 (0.0363)	
training:	Epoch: [40][249/408]	Loss 0.0070 (0.0362)	
training:	Epoch: [40][250/408]	Loss 0.0081 (0.0361)	
training:	Epoch: [40][251/408]	Loss 0.2622 (0.0370)	
training:	Epoch: [40][252/408]	Loss 0.0090 (0.0369)	
training:	Epoch: [40][253/408]	Loss 0.0056 (0.0368)	
training:	Epoch: [40][254/408]	Loss 0.0078 (0.0367)	
training:	Epoch: [40][255/408]	Loss 0.0075 (0.0365)	
training:	Epoch: [40][256/408]	Loss 0.0064 (0.0364)	
training:	Epoch: [40][257/408]	Loss 0.0080 (0.0363)	
training:	Epoch: [40][258/408]	Loss 0.3090 (0.0374)	
training:	Epoch: [40][259/408]	Loss 0.0072 (0.0373)	
training:	Epoch: [40][260/408]	Loss 0.2876 (0.0382)	
training:	Epoch: [40][261/408]	Loss 0.0061 (0.0381)	
training:	Epoch: [40][262/408]	Loss 0.0074 (0.0380)	
training:	Epoch: [40][263/408]	Loss 0.0066 (0.0379)	
training:	Epoch: [40][264/408]	Loss 0.2535 (0.0387)	
training:	Epoch: [40][265/408]	Loss 0.0077 (0.0386)	
training:	Epoch: [40][266/408]	Loss 0.0072 (0.0384)	
training:	Epoch: [40][267/408]	Loss 0.0088 (0.0383)	
training:	Epoch: [40][268/408]	Loss 0.0062 (0.0382)	
training:	Epoch: [40][269/408]	Loss 0.0072 (0.0381)	
training:	Epoch: [40][270/408]	Loss 0.0059 (0.0380)	
training:	Epoch: [40][271/408]	Loss 0.0065 (0.0379)	
training:	Epoch: [40][272/408]	Loss 0.0059 (0.0377)	
training:	Epoch: [40][273/408]	Loss 0.0076 (0.0376)	
training:	Epoch: [40][274/408]	Loss 0.0091 (0.0375)	
training:	Epoch: [40][275/408]	Loss 0.0079 (0.0374)	
training:	Epoch: [40][276/408]	Loss 0.0071 (0.0373)	
training:	Epoch: [40][277/408]	Loss 0.0074 (0.0372)	
training:	Epoch: [40][278/408]	Loss 0.0067 (0.0371)	
training:	Epoch: [40][279/408]	Loss 0.0068 (0.0370)	
training:	Epoch: [40][280/408]	Loss 0.0082 (0.0369)	
training:	Epoch: [40][281/408]	Loss 0.0092 (0.0368)	
training:	Epoch: [40][282/408]	Loss 0.1712 (0.0373)	
training:	Epoch: [40][283/408]	Loss 0.0066 (0.0371)	
training:	Epoch: [40][284/408]	Loss 0.0088 (0.0370)	
training:	Epoch: [40][285/408]	Loss 0.0101 (0.0370)	
training:	Epoch: [40][286/408]	Loss 0.0089 (0.0369)	
training:	Epoch: [40][287/408]	Loss 0.0052 (0.0367)	
training:	Epoch: [40][288/408]	Loss 0.0065 (0.0366)	
training:	Epoch: [40][289/408]	Loss 0.0081 (0.0365)	
training:	Epoch: [40][290/408]	Loss 0.2386 (0.0372)	
training:	Epoch: [40][291/408]	Loss 0.0087 (0.0371)	
training:	Epoch: [40][292/408]	Loss 0.0099 (0.0370)	
training:	Epoch: [40][293/408]	Loss 0.0085 (0.0370)	
training:	Epoch: [40][294/408]	Loss 0.0092 (0.0369)	
training:	Epoch: [40][295/408]	Loss 0.0105 (0.0368)	
training:	Epoch: [40][296/408]	Loss 0.3239 (0.0377)	
training:	Epoch: [40][297/408]	Loss 0.0088 (0.0376)	
training:	Epoch: [40][298/408]	Loss 0.0088 (0.0375)	
training:	Epoch: [40][299/408]	Loss 0.0073 (0.0374)	
training:	Epoch: [40][300/408]	Loss 0.0055 (0.0373)	
training:	Epoch: [40][301/408]	Loss 0.0058 (0.0372)	
training:	Epoch: [40][302/408]	Loss 0.0062 (0.0371)	
training:	Epoch: [40][303/408]	Loss 0.0070 (0.0370)	
training:	Epoch: [40][304/408]	Loss 0.0063 (0.0369)	
training:	Epoch: [40][305/408]	Loss 0.0068 (0.0368)	
training:	Epoch: [40][306/408]	Loss 0.0073 (0.0367)	
training:	Epoch: [40][307/408]	Loss 0.0082 (0.0366)	
training:	Epoch: [40][308/408]	Loss 0.0082 (0.0365)	
training:	Epoch: [40][309/408]	Loss 0.0088 (0.0365)	
training:	Epoch: [40][310/408]	Loss 0.0091 (0.0364)	
training:	Epoch: [40][311/408]	Loss 0.0076 (0.0363)	
training:	Epoch: [40][312/408]	Loss 0.0090 (0.0362)	
training:	Epoch: [40][313/408]	Loss 0.0086 (0.0361)	
training:	Epoch: [40][314/408]	Loss 0.0081 (0.0360)	
training:	Epoch: [40][315/408]	Loss 0.0075 (0.0359)	
training:	Epoch: [40][316/408]	Loss 0.0072 (0.0358)	
training:	Epoch: [40][317/408]	Loss 0.0072 (0.0357)	
training:	Epoch: [40][318/408]	Loss 0.0059 (0.0356)	
training:	Epoch: [40][319/408]	Loss 0.0093 (0.0356)	
training:	Epoch: [40][320/408]	Loss 0.0082 (0.0355)	
training:	Epoch: [40][321/408]	Loss 0.0057 (0.0354)	
training:	Epoch: [40][322/408]	Loss 0.0062 (0.0353)	
training:	Epoch: [40][323/408]	Loss 0.0062 (0.0352)	
training:	Epoch: [40][324/408]	Loss 0.0065 (0.0351)	
training:	Epoch: [40][325/408]	Loss 0.0073 (0.0350)	
training:	Epoch: [40][326/408]	Loss 0.0075 (0.0349)	
training:	Epoch: [40][327/408]	Loss 0.0060 (0.0349)	
training:	Epoch: [40][328/408]	Loss 0.0084 (0.0348)	
training:	Epoch: [40][329/408]	Loss 0.0073 (0.0347)	
training:	Epoch: [40][330/408]	Loss 0.0058 (0.0346)	
training:	Epoch: [40][331/408]	Loss 0.1210 (0.0349)	
training:	Epoch: [40][332/408]	Loss 0.0057 (0.0348)	
training:	Epoch: [40][333/408]	Loss 0.5789 (0.0364)	
training:	Epoch: [40][334/408]	Loss 0.0076 (0.0363)	
training:	Epoch: [40][335/408]	Loss 0.0121 (0.0363)	
training:	Epoch: [40][336/408]	Loss 0.0065 (0.0362)	
training:	Epoch: [40][337/408]	Loss 0.0065 (0.0361)	
training:	Epoch: [40][338/408]	Loss 0.0085 (0.0360)	
training:	Epoch: [40][339/408]	Loss 0.0067 (0.0359)	
training:	Epoch: [40][340/408]	Loss 0.0126 (0.0358)	
training:	Epoch: [40][341/408]	Loss 0.0074 (0.0358)	
training:	Epoch: [40][342/408]	Loss 0.0076 (0.0357)	
training:	Epoch: [40][343/408]	Loss 0.2991 (0.0364)	
training:	Epoch: [40][344/408]	Loss 0.0070 (0.0364)	
training:	Epoch: [40][345/408]	Loss 0.0086 (0.0363)	
training:	Epoch: [40][346/408]	Loss 0.0082 (0.0362)	
training:	Epoch: [40][347/408]	Loss 0.0062 (0.0361)	
training:	Epoch: [40][348/408]	Loss 0.0080 (0.0360)	
training:	Epoch: [40][349/408]	Loss 0.0061 (0.0359)	
training:	Epoch: [40][350/408]	Loss 0.0072 (0.0359)	
training:	Epoch: [40][351/408]	Loss 0.0097 (0.0358)	
training:	Epoch: [40][352/408]	Loss 0.0130 (0.0357)	
training:	Epoch: [40][353/408]	Loss 0.0077 (0.0356)	
training:	Epoch: [40][354/408]	Loss 0.0141 (0.0356)	
training:	Epoch: [40][355/408]	Loss 0.0068 (0.0355)	
training:	Epoch: [40][356/408]	Loss 0.0101 (0.0354)	
training:	Epoch: [40][357/408]	Loss 0.0083 (0.0354)	
training:	Epoch: [40][358/408]	Loss 0.0061 (0.0353)	
training:	Epoch: [40][359/408]	Loss 0.0073 (0.0352)	
training:	Epoch: [40][360/408]	Loss 0.0076 (0.0351)	
training:	Epoch: [40][361/408]	Loss 0.0068 (0.0350)	
training:	Epoch: [40][362/408]	Loss 0.0058 (0.0350)	
training:	Epoch: [40][363/408]	Loss 0.0092 (0.0349)	
training:	Epoch: [40][364/408]	Loss 0.0063 (0.0348)	
training:	Epoch: [40][365/408]	Loss 0.0064 (0.0347)	
training:	Epoch: [40][366/408]	Loss 0.0072 (0.0347)	
training:	Epoch: [40][367/408]	Loss 0.0069 (0.0346)	
training:	Epoch: [40][368/408]	Loss 0.0081 (0.0345)	
training:	Epoch: [40][369/408]	Loss 0.0069 (0.0344)	
training:	Epoch: [40][370/408]	Loss 0.0079 (0.0344)	
training:	Epoch: [40][371/408]	Loss 0.0074 (0.0343)	
training:	Epoch: [40][372/408]	Loss 0.0068 (0.0342)	
training:	Epoch: [40][373/408]	Loss 0.3188 (0.0350)	
training:	Epoch: [40][374/408]	Loss 0.0082 (0.0349)	
training:	Epoch: [40][375/408]	Loss 0.0069 (0.0348)	
training:	Epoch: [40][376/408]	Loss 0.0078 (0.0348)	
training:	Epoch: [40][377/408]	Loss 0.0072 (0.0347)	
training:	Epoch: [40][378/408]	Loss 0.0057 (0.0346)	
training:	Epoch: [40][379/408]	Loss 0.0062 (0.0345)	
training:	Epoch: [40][380/408]	Loss 0.0062 (0.0345)	
training:	Epoch: [40][381/408]	Loss 0.0071 (0.0344)	
training:	Epoch: [40][382/408]	Loss 0.0072 (0.0343)	
training:	Epoch: [40][383/408]	Loss 0.0066 (0.0342)	
training:	Epoch: [40][384/408]	Loss 0.0073 (0.0342)	
training:	Epoch: [40][385/408]	Loss 0.0074 (0.0341)	
training:	Epoch: [40][386/408]	Loss 0.3292 (0.0349)	
training:	Epoch: [40][387/408]	Loss 0.0061 (0.0348)	
training:	Epoch: [40][388/408]	Loss 0.2905 (0.0355)	
training:	Epoch: [40][389/408]	Loss 0.0053 (0.0354)	
training:	Epoch: [40][390/408]	Loss 0.0075 (0.0353)	
training:	Epoch: [40][391/408]	Loss 0.0064 (0.0352)	
training:	Epoch: [40][392/408]	Loss 0.0073 (0.0352)	
training:	Epoch: [40][393/408]	Loss 0.0065 (0.0351)	
training:	Epoch: [40][394/408]	Loss 0.0136 (0.0350)	
training:	Epoch: [40][395/408]	Loss 0.0067 (0.0350)	
training:	Epoch: [40][396/408]	Loss 0.0058 (0.0349)	
training:	Epoch: [40][397/408]	Loss 0.0054 (0.0348)	
training:	Epoch: [40][398/408]	Loss 0.0067 (0.0347)	
training:	Epoch: [40][399/408]	Loss 0.0063 (0.0347)	
training:	Epoch: [40][400/408]	Loss 0.0092 (0.0346)	
training:	Epoch: [40][401/408]	Loss 0.0059 (0.0345)	
training:	Epoch: [40][402/408]	Loss 0.0072 (0.0345)	
training:	Epoch: [40][403/408]	Loss 0.0067 (0.0344)	
training:	Epoch: [40][404/408]	Loss 0.0062 (0.0343)	
training:	Epoch: [40][405/408]	Loss 0.0069 (0.0343)	
training:	Epoch: [40][406/408]	Loss 0.0077 (0.0342)	
training:	Epoch: [40][407/408]	Loss 0.0060 (0.0341)	
training:	Epoch: [40][408/408]	Loss 0.0068 (0.0341)	
Training:	 Loss: 0.0340

Training:	 ACC: 0.9942 0.9942 0.9935 0.9949
Validation:	 ACC: 0.7861 0.7876 0.8188 0.7534
Validation:	 Best_BACC: 0.8226 0.8250 0.8772 0.7679
Validation:	 Loss: 0.9738
Pretraining:	Epoch 41/200
----------
training:	Epoch: [41][1/408]	Loss 0.0064 (0.0064)	
training:	Epoch: [41][2/408]	Loss 0.0062 (0.0063)	
training:	Epoch: [41][3/408]	Loss 0.0091 (0.0072)	
training:	Epoch: [41][4/408]	Loss 0.0065 (0.0071)	
training:	Epoch: [41][5/408]	Loss 0.0051 (0.0067)	
training:	Epoch: [41][6/408]	Loss 0.3175 (0.0585)	
training:	Epoch: [41][7/408]	Loss 0.0059 (0.0510)	
training:	Epoch: [41][8/408]	Loss 0.0077 (0.0456)	
training:	Epoch: [41][9/408]	Loss 0.0072 (0.0413)	
training:	Epoch: [41][10/408]	Loss 0.0068 (0.0378)	
training:	Epoch: [41][11/408]	Loss 0.0058 (0.0349)	
training:	Epoch: [41][12/408]	Loss 0.0068 (0.0326)	
training:	Epoch: [41][13/408]	Loss 0.0066 (0.0306)	
training:	Epoch: [41][14/408]	Loss 0.0054 (0.0288)	
training:	Epoch: [41][15/408]	Loss 0.0071 (0.0273)	
training:	Epoch: [41][16/408]	Loss 0.0071 (0.0261)	
training:	Epoch: [41][17/408]	Loss 0.0079 (0.0250)	
training:	Epoch: [41][18/408]	Loss 0.0073 (0.0240)	
training:	Epoch: [41][19/408]	Loss 0.0081 (0.0232)	
training:	Epoch: [41][20/408]	Loss 0.0074 (0.0224)	
training:	Epoch: [41][21/408]	Loss 0.0073 (0.0217)	
training:	Epoch: [41][22/408]	Loss 0.0061 (0.0210)	
training:	Epoch: [41][23/408]	Loss 0.0058 (0.0203)	
training:	Epoch: [41][24/408]	Loss 0.0064 (0.0197)	
training:	Epoch: [41][25/408]	Loss 0.0069 (0.0192)	
training:	Epoch: [41][26/408]	Loss 0.0059 (0.0187)	
training:	Epoch: [41][27/408]	Loss 0.0061 (0.0182)	
training:	Epoch: [41][28/408]	Loss 0.0062 (0.0178)	
training:	Epoch: [41][29/408]	Loss 0.0071 (0.0174)	
training:	Epoch: [41][30/408]	Loss 0.2878 (0.0265)	
training:	Epoch: [41][31/408]	Loss 0.0069 (0.0258)	
training:	Epoch: [41][32/408]	Loss 0.0076 (0.0253)	
training:	Epoch: [41][33/408]	Loss 0.1881 (0.0302)	
training:	Epoch: [41][34/408]	Loss 0.0061 (0.0295)	
training:	Epoch: [41][35/408]	Loss 0.0063 (0.0288)	
training:	Epoch: [41][36/408]	Loss 0.0066 (0.0282)	
training:	Epoch: [41][37/408]	Loss 0.0057 (0.0276)	
training:	Epoch: [41][38/408]	Loss 0.0079 (0.0271)	
training:	Epoch: [41][39/408]	Loss 0.0061 (0.0265)	
training:	Epoch: [41][40/408]	Loss 0.0073 (0.0261)	
training:	Epoch: [41][41/408]	Loss 0.2727 (0.0321)	
training:	Epoch: [41][42/408]	Loss 0.0059 (0.0314)	
training:	Epoch: [41][43/408]	Loss 0.0072 (0.0309)	
training:	Epoch: [41][44/408]	Loss 0.0064 (0.0303)	
training:	Epoch: [41][45/408]	Loss 0.0065 (0.0298)	
training:	Epoch: [41][46/408]	Loss 0.0067 (0.0293)	
training:	Epoch: [41][47/408]	Loss 0.0083 (0.0288)	
training:	Epoch: [41][48/408]	Loss 0.0071 (0.0284)	
training:	Epoch: [41][49/408]	Loss 0.0065 (0.0279)	
training:	Epoch: [41][50/408]	Loss 0.0081 (0.0276)	
training:	Epoch: [41][51/408]	Loss 0.0068 (0.0271)	
training:	Epoch: [41][52/408]	Loss 0.0054 (0.0267)	
training:	Epoch: [41][53/408]	Loss 0.0072 (0.0264)	
training:	Epoch: [41][54/408]	Loss 0.0051 (0.0260)	
training:	Epoch: [41][55/408]	Loss 0.0056 (0.0256)	
training:	Epoch: [41][56/408]	Loss 0.0063 (0.0252)	
training:	Epoch: [41][57/408]	Loss 0.0061 (0.0249)	
training:	Epoch: [41][58/408]	Loss 0.0053 (0.0246)	
training:	Epoch: [41][59/408]	Loss 0.0070 (0.0243)	
training:	Epoch: [41][60/408]	Loss 0.0077 (0.0240)	
training:	Epoch: [41][61/408]	Loss 0.0074 (0.0237)	
training:	Epoch: [41][62/408]	Loss 0.0063 (0.0234)	
training:	Epoch: [41][63/408]	Loss 0.0058 (0.0232)	
training:	Epoch: [41][64/408]	Loss 0.6086 (0.0323)	
training:	Epoch: [41][65/408]	Loss 0.0062 (0.0319)	
training:	Epoch: [41][66/408]	Loss 0.0061 (0.0315)	
training:	Epoch: [41][67/408]	Loss 0.0063 (0.0311)	
training:	Epoch: [41][68/408]	Loss 0.0073 (0.0308)	
training:	Epoch: [41][69/408]	Loss 0.0075 (0.0305)	
training:	Epoch: [41][70/408]	Loss 0.0068 (0.0301)	
training:	Epoch: [41][71/408]	Loss 0.2388 (0.0331)	
training:	Epoch: [41][72/408]	Loss 0.0061 (0.0327)	
training:	Epoch: [41][73/408]	Loss 0.0075 (0.0323)	
training:	Epoch: [41][74/408]	Loss 0.0076 (0.0320)	
training:	Epoch: [41][75/408]	Loss 0.0073 (0.0317)	
training:	Epoch: [41][76/408]	Loss 0.0064 (0.0313)	
training:	Epoch: [41][77/408]	Loss 0.0065 (0.0310)	
training:	Epoch: [41][78/408]	Loss 0.2906 (0.0343)	
training:	Epoch: [41][79/408]	Loss 0.0073 (0.0340)	
training:	Epoch: [41][80/408]	Loss 0.0146 (0.0338)	
training:	Epoch: [41][81/408]	Loss 0.0058 (0.0334)	
training:	Epoch: [41][82/408]	Loss 0.0061 (0.0331)	
training:	Epoch: [41][83/408]	Loss 0.0063 (0.0328)	
training:	Epoch: [41][84/408]	Loss 0.0069 (0.0325)	
training:	Epoch: [41][85/408]	Loss 0.0073 (0.0322)	
training:	Epoch: [41][86/408]	Loss 0.0056 (0.0319)	
training:	Epoch: [41][87/408]	Loss 0.0084 (0.0316)	
training:	Epoch: [41][88/408]	Loss 0.0053 (0.0313)	
training:	Epoch: [41][89/408]	Loss 0.0061 (0.0310)	
training:	Epoch: [41][90/408]	Loss 0.0067 (0.0307)	
training:	Epoch: [41][91/408]	Loss 0.0070 (0.0305)	
training:	Epoch: [41][92/408]	Loss 0.2710 (0.0331)	
training:	Epoch: [41][93/408]	Loss 0.0066 (0.0328)	
training:	Epoch: [41][94/408]	Loss 0.0083 (0.0325)	
training:	Epoch: [41][95/408]	Loss 0.0086 (0.0323)	
training:	Epoch: [41][96/408]	Loss 0.2864 (0.0349)	
training:	Epoch: [41][97/408]	Loss 0.0067 (0.0346)	
training:	Epoch: [41][98/408]	Loss 0.2824 (0.0372)	
training:	Epoch: [41][99/408]	Loss 0.0085 (0.0369)	
training:	Epoch: [41][100/408]	Loss 0.0071 (0.0366)	
training:	Epoch: [41][101/408]	Loss 0.0050 (0.0363)	
training:	Epoch: [41][102/408]	Loss 0.0072 (0.0360)	
training:	Epoch: [41][103/408]	Loss 0.0079 (0.0357)	
training:	Epoch: [41][104/408]	Loss 0.0053 (0.0354)	
training:	Epoch: [41][105/408]	Loss 0.0079 (0.0352)	
training:	Epoch: [41][106/408]	Loss 0.0117 (0.0349)	
training:	Epoch: [41][107/408]	Loss 0.0084 (0.0347)	
training:	Epoch: [41][108/408]	Loss 0.0092 (0.0345)	
training:	Epoch: [41][109/408]	Loss 0.0084 (0.0342)	
training:	Epoch: [41][110/408]	Loss 0.0084 (0.0340)	
training:	Epoch: [41][111/408]	Loss 0.0079 (0.0337)	
training:	Epoch: [41][112/408]	Loss 0.0067 (0.0335)	
training:	Epoch: [41][113/408]	Loss 0.2979 (0.0358)	
training:	Epoch: [41][114/408]	Loss 0.0059 (0.0356)	
training:	Epoch: [41][115/408]	Loss 0.0082 (0.0353)	
training:	Epoch: [41][116/408]	Loss 0.0091 (0.0351)	
training:	Epoch: [41][117/408]	Loss 0.0047 (0.0349)	
training:	Epoch: [41][118/408]	Loss 0.0095 (0.0346)	
training:	Epoch: [41][119/408]	Loss 0.0189 (0.0345)	
training:	Epoch: [41][120/408]	Loss 0.0080 (0.0343)	
training:	Epoch: [41][121/408]	Loss 0.3037 (0.0365)	
training:	Epoch: [41][122/408]	Loss 0.0076 (0.0363)	
training:	Epoch: [41][123/408]	Loss 0.0065 (0.0360)	
training:	Epoch: [41][124/408]	Loss 0.0067 (0.0358)	
training:	Epoch: [41][125/408]	Loss 0.0068 (0.0356)	
training:	Epoch: [41][126/408]	Loss 0.0069 (0.0353)	
training:	Epoch: [41][127/408]	Loss 0.0076 (0.0351)	
training:	Epoch: [41][128/408]	Loss 0.0074 (0.0349)	
training:	Epoch: [41][129/408]	Loss 0.0072 (0.0347)	
training:	Epoch: [41][130/408]	Loss 0.0073 (0.0345)	
training:	Epoch: [41][131/408]	Loss 0.0087 (0.0343)	
training:	Epoch: [41][132/408]	Loss 0.0086 (0.0341)	
training:	Epoch: [41][133/408]	Loss 0.0082 (0.0339)	
training:	Epoch: [41][134/408]	Loss 0.0054 (0.0337)	
training:	Epoch: [41][135/408]	Loss 0.0062 (0.0335)	
training:	Epoch: [41][136/408]	Loss 0.0067 (0.0333)	
training:	Epoch: [41][137/408]	Loss 0.2439 (0.0348)	
training:	Epoch: [41][138/408]	Loss 0.0070 (0.0346)	
training:	Epoch: [41][139/408]	Loss 0.0079 (0.0344)	
training:	Epoch: [41][140/408]	Loss 0.0070 (0.0342)	
training:	Epoch: [41][141/408]	Loss 0.0073 (0.0340)	
training:	Epoch: [41][142/408]	Loss 0.0071 (0.0338)	
training:	Epoch: [41][143/408]	Loss 0.0062 (0.0337)	
training:	Epoch: [41][144/408]	Loss 0.0071 (0.0335)	
training:	Epoch: [41][145/408]	Loss 0.0092 (0.0333)	
training:	Epoch: [41][146/408]	Loss 0.0080 (0.0331)	
training:	Epoch: [41][147/408]	Loss 0.0064 (0.0329)	
training:	Epoch: [41][148/408]	Loss 0.0052 (0.0328)	
training:	Epoch: [41][149/408]	Loss 0.0070 (0.0326)	
training:	Epoch: [41][150/408]	Loss 0.0071 (0.0324)	
training:	Epoch: [41][151/408]	Loss 0.0069 (0.0322)	
training:	Epoch: [41][152/408]	Loss 0.0061 (0.0321)	
training:	Epoch: [41][153/408]	Loss 0.0090 (0.0319)	
training:	Epoch: [41][154/408]	Loss 0.3059 (0.0337)	
training:	Epoch: [41][155/408]	Loss 0.0084 (0.0335)	
training:	Epoch: [41][156/408]	Loss 0.0076 (0.0334)	
training:	Epoch: [41][157/408]	Loss 0.0069 (0.0332)	
training:	Epoch: [41][158/408]	Loss 0.0065 (0.0330)	
training:	Epoch: [41][159/408]	Loss 0.0059 (0.0329)	
training:	Epoch: [41][160/408]	Loss 0.0100 (0.0327)	
training:	Epoch: [41][161/408]	Loss 0.0082 (0.0326)	
training:	Epoch: [41][162/408]	Loss 0.0081 (0.0324)	
training:	Epoch: [41][163/408]	Loss 0.0055 (0.0323)	
training:	Epoch: [41][164/408]	Loss 0.0061 (0.0321)	
training:	Epoch: [41][165/408]	Loss 0.0075 (0.0319)	
training:	Epoch: [41][166/408]	Loss 0.0076 (0.0318)	
training:	Epoch: [41][167/408]	Loss 0.0066 (0.0316)	
training:	Epoch: [41][168/408]	Loss 0.0098 (0.0315)	
training:	Epoch: [41][169/408]	Loss 0.0093 (0.0314)	
training:	Epoch: [41][170/408]	Loss 0.0070 (0.0312)	
training:	Epoch: [41][171/408]	Loss 0.5551 (0.0343)	
training:	Epoch: [41][172/408]	Loss 0.0145 (0.0342)	
training:	Epoch: [41][173/408]	Loss 0.0058 (0.0340)	
training:	Epoch: [41][174/408]	Loss 0.0055 (0.0339)	
training:	Epoch: [41][175/408]	Loss 0.0046 (0.0337)	
training:	Epoch: [41][176/408]	Loss 0.0071 (0.0335)	
training:	Epoch: [41][177/408]	Loss 0.0055 (0.0334)	
training:	Epoch: [41][178/408]	Loss 0.0065 (0.0332)	
training:	Epoch: [41][179/408]	Loss 0.0073 (0.0331)	
training:	Epoch: [41][180/408]	Loss 0.0058 (0.0329)	
training:	Epoch: [41][181/408]	Loss 0.0067 (0.0328)	
training:	Epoch: [41][182/408]	Loss 0.0079 (0.0327)	
training:	Epoch: [41][183/408]	Loss 0.0067 (0.0325)	
training:	Epoch: [41][184/408]	Loss 0.0060 (0.0324)	
training:	Epoch: [41][185/408]	Loss 0.0071 (0.0322)	
training:	Epoch: [41][186/408]	Loss 0.0065 (0.0321)	
training:	Epoch: [41][187/408]	Loss 0.2723 (0.0334)	
training:	Epoch: [41][188/408]	Loss 0.0091 (0.0333)	
training:	Epoch: [41][189/408]	Loss 0.0069 (0.0331)	
training:	Epoch: [41][190/408]	Loss 0.0069 (0.0330)	
training:	Epoch: [41][191/408]	Loss 0.0065 (0.0328)	
training:	Epoch: [41][192/408]	Loss 0.0094 (0.0327)	
training:	Epoch: [41][193/408]	Loss 0.0063 (0.0326)	
training:	Epoch: [41][194/408]	Loss 0.0078 (0.0324)	
training:	Epoch: [41][195/408]	Loss 0.0077 (0.0323)	
training:	Epoch: [41][196/408]	Loss 0.0071 (0.0322)	
training:	Epoch: [41][197/408]	Loss 0.0070 (0.0321)	
training:	Epoch: [41][198/408]	Loss 0.0077 (0.0319)	
training:	Epoch: [41][199/408]	Loss 0.0077 (0.0318)	
training:	Epoch: [41][200/408]	Loss 0.0081 (0.0317)	
training:	Epoch: [41][201/408]	Loss 0.0060 (0.0316)	
training:	Epoch: [41][202/408]	Loss 0.0066 (0.0315)	
training:	Epoch: [41][203/408]	Loss 0.0055 (0.0313)	
training:	Epoch: [41][204/408]	Loss 0.0096 (0.0312)	
training:	Epoch: [41][205/408]	Loss 0.0075 (0.0311)	
training:	Epoch: [41][206/408]	Loss 0.0061 (0.0310)	
training:	Epoch: [41][207/408]	Loss 0.0088 (0.0309)	
training:	Epoch: [41][208/408]	Loss 0.0068 (0.0308)	
training:	Epoch: [41][209/408]	Loss 0.0076 (0.0306)	
training:	Epoch: [41][210/408]	Loss 0.0068 (0.0305)	
training:	Epoch: [41][211/408]	Loss 0.0054 (0.0304)	
training:	Epoch: [41][212/408]	Loss 0.0062 (0.0303)	
training:	Epoch: [41][213/408]	Loss 0.0056 (0.0302)	
training:	Epoch: [41][214/408]	Loss 0.0073 (0.0301)	
training:	Epoch: [41][215/408]	Loss 0.0068 (0.0300)	
training:	Epoch: [41][216/408]	Loss 0.0070 (0.0299)	
training:	Epoch: [41][217/408]	Loss 0.0060 (0.0298)	
training:	Epoch: [41][218/408]	Loss 0.0054 (0.0296)	
training:	Epoch: [41][219/408]	Loss 0.3373 (0.0310)	
training:	Epoch: [41][220/408]	Loss 0.0078 (0.0309)	
training:	Epoch: [41][221/408]	Loss 0.0074 (0.0308)	
training:	Epoch: [41][222/408]	Loss 0.0053 (0.0307)	
training:	Epoch: [41][223/408]	Loss 0.0060 (0.0306)	
training:	Epoch: [41][224/408]	Loss 0.0072 (0.0305)	
training:	Epoch: [41][225/408]	Loss 0.2682 (0.0316)	
training:	Epoch: [41][226/408]	Loss 0.0057 (0.0314)	
training:	Epoch: [41][227/408]	Loss 0.0067 (0.0313)	
training:	Epoch: [41][228/408]	Loss 0.0070 (0.0312)	
training:	Epoch: [41][229/408]	Loss 0.0072 (0.0311)	
training:	Epoch: [41][230/408]	Loss 0.2609 (0.0321)	
training:	Epoch: [41][231/408]	Loss 0.0077 (0.0320)	
training:	Epoch: [41][232/408]	Loss 0.0063 (0.0319)	
training:	Epoch: [41][233/408]	Loss 0.0064 (0.0318)	
training:	Epoch: [41][234/408]	Loss 0.3276 (0.0331)	
training:	Epoch: [41][235/408]	Loss 0.0055 (0.0329)	
training:	Epoch: [41][236/408]	Loss 0.0067 (0.0328)	
training:	Epoch: [41][237/408]	Loss 0.0067 (0.0327)	
training:	Epoch: [41][238/408]	Loss 0.0063 (0.0326)	
training:	Epoch: [41][239/408]	Loss 0.0062 (0.0325)	
training:	Epoch: [41][240/408]	Loss 0.0061 (0.0324)	
training:	Epoch: [41][241/408]	Loss 0.2577 (0.0333)	
training:	Epoch: [41][242/408]	Loss 0.0070 (0.0332)	
training:	Epoch: [41][243/408]	Loss 0.0070 (0.0331)	
training:	Epoch: [41][244/408]	Loss 0.3121 (0.0343)	
training:	Epoch: [41][245/408]	Loss 0.0078 (0.0341)	
training:	Epoch: [41][246/408]	Loss 0.0077 (0.0340)	
training:	Epoch: [41][247/408]	Loss 0.0066 (0.0339)	
training:	Epoch: [41][248/408]	Loss 0.0071 (0.0338)	
training:	Epoch: [41][249/408]	Loss 0.0067 (0.0337)	
training:	Epoch: [41][250/408]	Loss 0.0117 (0.0336)	
training:	Epoch: [41][251/408]	Loss 0.0069 (0.0335)	
training:	Epoch: [41][252/408]	Loss 0.0077 (0.0334)	
training:	Epoch: [41][253/408]	Loss 0.0053 (0.0333)	
training:	Epoch: [41][254/408]	Loss 0.0070 (0.0332)	
training:	Epoch: [41][255/408]	Loss 0.0090 (0.0331)	
training:	Epoch: [41][256/408]	Loss 0.0091 (0.0330)	
training:	Epoch: [41][257/408]	Loss 0.0072 (0.0329)	
training:	Epoch: [41][258/408]	Loss 0.0049 (0.0328)	
training:	Epoch: [41][259/408]	Loss 0.0060 (0.0327)	
training:	Epoch: [41][260/408]	Loss 0.2705 (0.0336)	
training:	Epoch: [41][261/408]	Loss 0.0073 (0.0335)	
training:	Epoch: [41][262/408]	Loss 0.0074 (0.0334)	
training:	Epoch: [41][263/408]	Loss 0.0072 (0.0333)	
training:	Epoch: [41][264/408]	Loss 0.0067 (0.0332)	
training:	Epoch: [41][265/408]	Loss 0.0077 (0.0331)	
training:	Epoch: [41][266/408]	Loss 0.0066 (0.0330)	
training:	Epoch: [41][267/408]	Loss 0.0064 (0.0329)	
training:	Epoch: [41][268/408]	Loss 0.0065 (0.0328)	
training:	Epoch: [41][269/408]	Loss 0.0075 (0.0327)	
training:	Epoch: [41][270/408]	Loss 0.0049 (0.0326)	
training:	Epoch: [41][271/408]	Loss 0.0074 (0.0325)	
training:	Epoch: [41][272/408]	Loss 0.0057 (0.0324)	
training:	Epoch: [41][273/408]	Loss 0.0071 (0.0323)	
training:	Epoch: [41][274/408]	Loss 0.0068 (0.0322)	
training:	Epoch: [41][275/408]	Loss 0.0082 (0.0322)	
training:	Epoch: [41][276/408]	Loss 0.0050 (0.0321)	
training:	Epoch: [41][277/408]	Loss 0.2876 (0.0330)	
training:	Epoch: [41][278/408]	Loss 0.0064 (0.0329)	
training:	Epoch: [41][279/408]	Loss 0.0070 (0.0328)	
training:	Epoch: [41][280/408]	Loss 0.0066 (0.0327)	
training:	Epoch: [41][281/408]	Loss 0.0063 (0.0326)	
training:	Epoch: [41][282/408]	Loss 0.3129 (0.0336)	
training:	Epoch: [41][283/408]	Loss 0.0067 (0.0335)	
training:	Epoch: [41][284/408]	Loss 0.0084 (0.0334)	
training:	Epoch: [41][285/408]	Loss 0.0062 (0.0333)	
training:	Epoch: [41][286/408]	Loss 0.0087 (0.0332)	
training:	Epoch: [41][287/408]	Loss 0.0084 (0.0331)	
training:	Epoch: [41][288/408]	Loss 0.1938 (0.0337)	
training:	Epoch: [41][289/408]	Loss 0.0074 (0.0336)	
training:	Epoch: [41][290/408]	Loss 0.0062 (0.0335)	
training:	Epoch: [41][291/408]	Loss 0.0073 (0.0334)	
training:	Epoch: [41][292/408]	Loss 0.3185 (0.0344)	
training:	Epoch: [41][293/408]	Loss 0.2828 (0.0352)	
training:	Epoch: [41][294/408]	Loss 0.0086 (0.0352)	
training:	Epoch: [41][295/408]	Loss 0.0075 (0.0351)	
training:	Epoch: [41][296/408]	Loss 0.0087 (0.0350)	
training:	Epoch: [41][297/408]	Loss 0.0074 (0.0349)	
training:	Epoch: [41][298/408]	Loss 0.0076 (0.0348)	
training:	Epoch: [41][299/408]	Loss 0.0074 (0.0347)	
training:	Epoch: [41][300/408]	Loss 0.2498 (0.0354)	
training:	Epoch: [41][301/408]	Loss 0.0078 (0.0353)	
training:	Epoch: [41][302/408]	Loss 0.0101 (0.0352)	
training:	Epoch: [41][303/408]	Loss 0.0066 (0.0351)	
training:	Epoch: [41][304/408]	Loss 0.2537 (0.0359)	
training:	Epoch: [41][305/408]	Loss 0.0067 (0.0358)	
training:	Epoch: [41][306/408]	Loss 0.0068 (0.0357)	
training:	Epoch: [41][307/408]	Loss 0.0064 (0.0356)	
training:	Epoch: [41][308/408]	Loss 0.0067 (0.0355)	
training:	Epoch: [41][309/408]	Loss 0.0069 (0.0354)	
training:	Epoch: [41][310/408]	Loss 0.0068 (0.0353)	
training:	Epoch: [41][311/408]	Loss 0.0075 (0.0352)	
training:	Epoch: [41][312/408]	Loss 0.0073 (0.0351)	
training:	Epoch: [41][313/408]	Loss 0.0056 (0.0350)	
training:	Epoch: [41][314/408]	Loss 0.0076 (0.0349)	
training:	Epoch: [41][315/408]	Loss 0.0081 (0.0349)	
training:	Epoch: [41][316/408]	Loss 0.0084 (0.0348)	
training:	Epoch: [41][317/408]	Loss 0.0076 (0.0347)	
training:	Epoch: [41][318/408]	Loss 0.0085 (0.0346)	
training:	Epoch: [41][319/408]	Loss 0.0143 (0.0345)	
training:	Epoch: [41][320/408]	Loss 0.0114 (0.0345)	
training:	Epoch: [41][321/408]	Loss 0.0076 (0.0344)	
training:	Epoch: [41][322/408]	Loss 0.0094 (0.0343)	
training:	Epoch: [41][323/408]	Loss 0.0080 (0.0342)	
training:	Epoch: [41][324/408]	Loss 0.0062 (0.0341)	
training:	Epoch: [41][325/408]	Loss 0.0064 (0.0341)	
training:	Epoch: [41][326/408]	Loss 0.0063 (0.0340)	
training:	Epoch: [41][327/408]	Loss 0.0101 (0.0339)	
training:	Epoch: [41][328/408]	Loss 0.0075 (0.0338)	
training:	Epoch: [41][329/408]	Loss 0.0072 (0.0337)	
training:	Epoch: [41][330/408]	Loss 0.0053 (0.0336)	
training:	Epoch: [41][331/408]	Loss 0.0072 (0.0336)	
training:	Epoch: [41][332/408]	Loss 0.0075 (0.0335)	
training:	Epoch: [41][333/408]	Loss 0.0063 (0.0334)	
training:	Epoch: [41][334/408]	Loss 0.0083 (0.0333)	
training:	Epoch: [41][335/408]	Loss 0.0065 (0.0333)	
training:	Epoch: [41][336/408]	Loss 0.0056 (0.0332)	
training:	Epoch: [41][337/408]	Loss 0.0068 (0.0331)	
training:	Epoch: [41][338/408]	Loss 0.0095 (0.0330)	
training:	Epoch: [41][339/408]	Loss 0.0115 (0.0330)	
training:	Epoch: [41][340/408]	Loss 0.0071 (0.0329)	
training:	Epoch: [41][341/408]	Loss 0.0062 (0.0328)	
training:	Epoch: [41][342/408]	Loss 0.3213 (0.0336)	
training:	Epoch: [41][343/408]	Loss 0.0067 (0.0336)	
training:	Epoch: [41][344/408]	Loss 0.0057 (0.0335)	
training:	Epoch: [41][345/408]	Loss 0.0066 (0.0334)	
training:	Epoch: [41][346/408]	Loss 0.0079 (0.0333)	
training:	Epoch: [41][347/408]	Loss 0.0069 (0.0333)	
training:	Epoch: [41][348/408]	Loss 0.0054 (0.0332)	
training:	Epoch: [41][349/408]	Loss 0.0054 (0.0331)	
training:	Epoch: [41][350/408]	Loss 0.0084 (0.0330)	
training:	Epoch: [41][351/408]	Loss 0.0066 (0.0330)	
training:	Epoch: [41][352/408]	Loss 0.0064 (0.0329)	
training:	Epoch: [41][353/408]	Loss 0.1878 (0.0333)	
training:	Epoch: [41][354/408]	Loss 0.0071 (0.0332)	
training:	Epoch: [41][355/408]	Loss 0.1117 (0.0335)	
training:	Epoch: [41][356/408]	Loss 0.0070 (0.0334)	
training:	Epoch: [41][357/408]	Loss 0.0072 (0.0333)	
training:	Epoch: [41][358/408]	Loss 0.2776 (0.0340)	
training:	Epoch: [41][359/408]	Loss 0.0067 (0.0339)	
training:	Epoch: [41][360/408]	Loss 0.3185 (0.0347)	
training:	Epoch: [41][361/408]	Loss 0.0088 (0.0346)	
training:	Epoch: [41][362/408]	Loss 0.0075 (0.0346)	
training:	Epoch: [41][363/408]	Loss 0.0077 (0.0345)	
training:	Epoch: [41][364/408]	Loss 0.0058 (0.0344)	
training:	Epoch: [41][365/408]	Loss 0.0086 (0.0343)	
training:	Epoch: [41][366/408]	Loss 0.0060 (0.0343)	
training:	Epoch: [41][367/408]	Loss 0.0059 (0.0342)	
training:	Epoch: [41][368/408]	Loss 0.0065 (0.0341)	
training:	Epoch: [41][369/408]	Loss 0.0055 (0.0340)	
training:	Epoch: [41][370/408]	Loss 0.0084 (0.0340)	
training:	Epoch: [41][371/408]	Loss 0.0053 (0.0339)	
training:	Epoch: [41][372/408]	Loss 0.0229 (0.0339)	
training:	Epoch: [41][373/408]	Loss 0.0168 (0.0338)	
training:	Epoch: [41][374/408]	Loss 0.0075 (0.0337)	
training:	Epoch: [41][375/408]	Loss 0.0098 (0.0337)	
training:	Epoch: [41][376/408]	Loss 0.0067 (0.0336)	
training:	Epoch: [41][377/408]	Loss 0.0083 (0.0335)	
training:	Epoch: [41][378/408]	Loss 0.0059 (0.0335)	
training:	Epoch: [41][379/408]	Loss 0.0071 (0.0334)	
training:	Epoch: [41][380/408]	Loss 0.0078 (0.0333)	
training:	Epoch: [41][381/408]	Loss 0.0076 (0.0333)	
training:	Epoch: [41][382/408]	Loss 0.0064 (0.0332)	
training:	Epoch: [41][383/408]	Loss 0.0079 (0.0331)	
training:	Epoch: [41][384/408]	Loss 0.0062 (0.0331)	
training:	Epoch: [41][385/408]	Loss 0.0072 (0.0330)	
training:	Epoch: [41][386/408]	Loss 0.0070 (0.0329)	
training:	Epoch: [41][387/408]	Loss 0.0056 (0.0329)	
training:	Epoch: [41][388/408]	Loss 0.0065 (0.0328)	
training:	Epoch: [41][389/408]	Loss 0.0069 (0.0327)	
training:	Epoch: [41][390/408]	Loss 0.0088 (0.0327)	
training:	Epoch: [41][391/408]	Loss 0.0065 (0.0326)	
training:	Epoch: [41][392/408]	Loss 0.0087 (0.0325)	
training:	Epoch: [41][393/408]	Loss 0.0064 (0.0325)	
training:	Epoch: [41][394/408]	Loss 0.0079 (0.0324)	
training:	Epoch: [41][395/408]	Loss 0.0049 (0.0323)	
training:	Epoch: [41][396/408]	Loss 0.0050 (0.0323)	
training:	Epoch: [41][397/408]	Loss 0.0063 (0.0322)	
training:	Epoch: [41][398/408]	Loss 0.0075 (0.0321)	
training:	Epoch: [41][399/408]	Loss 0.2605 (0.0327)	
training:	Epoch: [41][400/408]	Loss 0.0064 (0.0326)	
training:	Epoch: [41][401/408]	Loss 0.0080 (0.0326)	
training:	Epoch: [41][402/408]	Loss 0.0066 (0.0325)	
training:	Epoch: [41][403/408]	Loss 0.0069 (0.0325)	
training:	Epoch: [41][404/408]	Loss 0.0063 (0.0324)	
training:	Epoch: [41][405/408]	Loss 0.2931 (0.0330)	
training:	Epoch: [41][406/408]	Loss 0.0058 (0.0330)	
training:	Epoch: [41][407/408]	Loss 0.0068 (0.0329)	
training:	Epoch: [41][408/408]	Loss 0.0080 (0.0328)	
Training:	 Loss: 0.0328

Training:	 ACC: 0.9945 0.9945 0.9938 0.9952
Validation:	 ACC: 0.7894 0.7897 0.7973 0.7814
Validation:	 Best_BACC: 0.8226 0.8250 0.8772 0.7679
Validation:	 Loss: 0.9722
Pretraining:	Epoch 42/200
----------
training:	Epoch: [42][1/408]	Loss 0.0057 (0.0057)	
training:	Epoch: [42][2/408]	Loss 0.0063 (0.0060)	
training:	Epoch: [42][3/408]	Loss 0.0080 (0.0067)	
training:	Epoch: [42][4/408]	Loss 0.0060 (0.0065)	
training:	Epoch: [42][5/408]	Loss 0.0063 (0.0065)	
training:	Epoch: [42][6/408]	Loss 0.0082 (0.0068)	
training:	Epoch: [42][7/408]	Loss 0.0072 (0.0068)	
training:	Epoch: [42][8/408]	Loss 0.0078 (0.0069)	
training:	Epoch: [42][9/408]	Loss 0.0069 (0.0069)	
training:	Epoch: [42][10/408]	Loss 0.0061 (0.0069)	
training:	Epoch: [42][11/408]	Loss 0.0057 (0.0067)	
training:	Epoch: [42][12/408]	Loss 0.0070 (0.0068)	
training:	Epoch: [42][13/408]	Loss 0.0480 (0.0099)	
training:	Epoch: [42][14/408]	Loss 0.0062 (0.0097)	
training:	Epoch: [42][15/408]	Loss 0.0077 (0.0095)	
training:	Epoch: [42][16/408]	Loss 0.0073 (0.0094)	
training:	Epoch: [42][17/408]	Loss 0.2774 (0.0252)	
training:	Epoch: [42][18/408]	Loss 0.0068 (0.0241)	
training:	Epoch: [42][19/408]	Loss 0.0074 (0.0233)	
training:	Epoch: [42][20/408]	Loss 0.0094 (0.0226)	
training:	Epoch: [42][21/408]	Loss 0.0057 (0.0218)	
training:	Epoch: [42][22/408]	Loss 0.0054 (0.0210)	
training:	Epoch: [42][23/408]	Loss 0.0106 (0.0206)	
training:	Epoch: [42][24/408]	Loss 0.0067 (0.0200)	
training:	Epoch: [42][25/408]	Loss 0.0756 (0.0222)	
training:	Epoch: [42][26/408]	Loss 0.0070 (0.0216)	
training:	Epoch: [42][27/408]	Loss 0.0065 (0.0211)	
training:	Epoch: [42][28/408]	Loss 0.0053 (0.0205)	
training:	Epoch: [42][29/408]	Loss 0.0068 (0.0200)	
training:	Epoch: [42][30/408]	Loss 0.0058 (0.0196)	
training:	Epoch: [42][31/408]	Loss 0.0057 (0.0191)	
training:	Epoch: [42][32/408]	Loss 0.0065 (0.0187)	
training:	Epoch: [42][33/408]	Loss 0.0056 (0.0183)	
training:	Epoch: [42][34/408]	Loss 0.0064 (0.0180)	
training:	Epoch: [42][35/408]	Loss 0.3204 (0.0266)	
training:	Epoch: [42][36/408]	Loss 0.0071 (0.0261)	
training:	Epoch: [42][37/408]	Loss 0.3154 (0.0339)	
training:	Epoch: [42][38/408]	Loss 0.0061 (0.0332)	
training:	Epoch: [42][39/408]	Loss 0.0057 (0.0325)	
training:	Epoch: [42][40/408]	Loss 0.0062 (0.0318)	
training:	Epoch: [42][41/408]	Loss 0.0065 (0.0312)	
training:	Epoch: [42][42/408]	Loss 0.1039 (0.0329)	
training:	Epoch: [42][43/408]	Loss 0.0099 (0.0324)	
training:	Epoch: [42][44/408]	Loss 0.0064 (0.0318)	
training:	Epoch: [42][45/408]	Loss 0.0067 (0.0312)	
training:	Epoch: [42][46/408]	Loss 0.0227 (0.0310)	
training:	Epoch: [42][47/408]	Loss 0.0089 (0.0306)	
training:	Epoch: [42][48/408]	Loss 0.0526 (0.0310)	
training:	Epoch: [42][49/408]	Loss 0.0063 (0.0305)	
training:	Epoch: [42][50/408]	Loss 0.0065 (0.0300)	
training:	Epoch: [42][51/408]	Loss 0.0060 (0.0296)	
training:	Epoch: [42][52/408]	Loss 0.0055 (0.0291)	
training:	Epoch: [42][53/408]	Loss 0.0067 (0.0287)	
training:	Epoch: [42][54/408]	Loss 0.3128 (0.0339)	
training:	Epoch: [42][55/408]	Loss 0.0059 (0.0334)	
training:	Epoch: [42][56/408]	Loss 0.0064 (0.0330)	
training:	Epoch: [42][57/408]	Loss 0.0115 (0.0326)	
training:	Epoch: [42][58/408]	Loss 0.0115 (0.0322)	
training:	Epoch: [42][59/408]	Loss 0.0063 (0.0318)	
training:	Epoch: [42][60/408]	Loss 0.0096 (0.0314)	
training:	Epoch: [42][61/408]	Loss 0.0119 (0.0311)	
training:	Epoch: [42][62/408]	Loss 0.0067 (0.0307)	
training:	Epoch: [42][63/408]	Loss 0.0067 (0.0303)	
training:	Epoch: [42][64/408]	Loss 0.0073 (0.0300)	
training:	Epoch: [42][65/408]	Loss 0.0063 (0.0296)	
training:	Epoch: [42][66/408]	Loss 0.2622 (0.0331)	
training:	Epoch: [42][67/408]	Loss 0.0072 (0.0327)	
training:	Epoch: [42][68/408]	Loss 0.0058 (0.0323)	
training:	Epoch: [42][69/408]	Loss 0.0459 (0.0325)	
training:	Epoch: [42][70/408]	Loss 0.0081 (0.0322)	
training:	Epoch: [42][71/408]	Loss 0.0069 (0.0318)	
training:	Epoch: [42][72/408]	Loss 0.0063 (0.0315)	
training:	Epoch: [42][73/408]	Loss 0.0079 (0.0311)	
training:	Epoch: [42][74/408]	Loss 0.0068 (0.0308)	
training:	Epoch: [42][75/408]	Loss 0.0067 (0.0305)	
training:	Epoch: [42][76/408]	Loss 0.0101 (0.0302)	
training:	Epoch: [42][77/408]	Loss 0.0065 (0.0299)	
training:	Epoch: [42][78/408]	Loss 0.0085 (0.0296)	
training:	Epoch: [42][79/408]	Loss 0.0080 (0.0294)	
training:	Epoch: [42][80/408]	Loss 0.0071 (0.0291)	
training:	Epoch: [42][81/408]	Loss 0.0053 (0.0288)	
training:	Epoch: [42][82/408]	Loss 0.2876 (0.0320)	
training:	Epoch: [42][83/408]	Loss 0.0076 (0.0317)	
training:	Epoch: [42][84/408]	Loss 0.0079 (0.0314)	
training:	Epoch: [42][85/408]	Loss 0.0069 (0.0311)	
training:	Epoch: [42][86/408]	Loss 0.0298 (0.0311)	
training:	Epoch: [42][87/408]	Loss 0.2574 (0.0337)	
training:	Epoch: [42][88/408]	Loss 0.0064 (0.0334)	
training:	Epoch: [42][89/408]	Loss 0.3023 (0.0364)	
training:	Epoch: [42][90/408]	Loss 0.0229 (0.0362)	
training:	Epoch: [42][91/408]	Loss 0.0063 (0.0359)	
training:	Epoch: [42][92/408]	Loss 0.0066 (0.0356)	
training:	Epoch: [42][93/408]	Loss 0.0071 (0.0353)	
training:	Epoch: [42][94/408]	Loss 0.0074 (0.0350)	
training:	Epoch: [42][95/408]	Loss 0.0055 (0.0347)	
training:	Epoch: [42][96/408]	Loss 0.0067 (0.0344)	
training:	Epoch: [42][97/408]	Loss 0.0109 (0.0341)	
training:	Epoch: [42][98/408]	Loss 0.0068 (0.0339)	
training:	Epoch: [42][99/408]	Loss 0.0085 (0.0336)	
training:	Epoch: [42][100/408]	Loss 0.0680 (0.0340)	
training:	Epoch: [42][101/408]	Loss 0.0092 (0.0337)	
training:	Epoch: [42][102/408]	Loss 0.2509 (0.0358)	
training:	Epoch: [42][103/408]	Loss 0.0048 (0.0355)	
training:	Epoch: [42][104/408]	Loss 0.0063 (0.0353)	
training:	Epoch: [42][105/408]	Loss 0.0056 (0.0350)	
training:	Epoch: [42][106/408]	Loss 0.0077 (0.0347)	
training:	Epoch: [42][107/408]	Loss 0.0080 (0.0345)	
training:	Epoch: [42][108/408]	Loss 0.0064 (0.0342)	
training:	Epoch: [42][109/408]	Loss 0.0248 (0.0341)	
training:	Epoch: [42][110/408]	Loss 0.3007 (0.0365)	
training:	Epoch: [42][111/408]	Loss 0.0091 (0.0363)	
training:	Epoch: [42][112/408]	Loss 0.0145 (0.0361)	
training:	Epoch: [42][113/408]	Loss 0.3509 (0.0389)	
training:	Epoch: [42][114/408]	Loss 0.0065 (0.0386)	
training:	Epoch: [42][115/408]	Loss 0.2390 (0.0403)	
training:	Epoch: [42][116/408]	Loss 0.2383 (0.0421)	
training:	Epoch: [42][117/408]	Loss 0.0874 (0.0424)	
training:	Epoch: [42][118/408]	Loss 0.0078 (0.0421)	
training:	Epoch: [42][119/408]	Loss 0.0242 (0.0420)	
training:	Epoch: [42][120/408]	Loss 0.0096 (0.0417)	
training:	Epoch: [42][121/408]	Loss 0.0062 (0.0414)	
training:	Epoch: [42][122/408]	Loss 0.0065 (0.0411)	
training:	Epoch: [42][123/408]	Loss 0.0076 (0.0409)	
training:	Epoch: [42][124/408]	Loss 0.1147 (0.0415)	
training:	Epoch: [42][125/408]	Loss 0.0464 (0.0415)	
training:	Epoch: [42][126/408]	Loss 0.2242 (0.0430)	
training:	Epoch: [42][127/408]	Loss 0.0083 (0.0427)	
training:	Epoch: [42][128/408]	Loss 0.0063 (0.0424)	
training:	Epoch: [42][129/408]	Loss 0.3086 (0.0445)	
training:	Epoch: [42][130/408]	Loss 0.0125 (0.0442)	
training:	Epoch: [42][131/408]	Loss 0.1057 (0.0447)	
training:	Epoch: [42][132/408]	Loss 0.0068 (0.0444)	
training:	Epoch: [42][133/408]	Loss 0.0601 (0.0445)	
training:	Epoch: [42][134/408]	Loss 0.2811 (0.0463)	
training:	Epoch: [42][135/408]	Loss 0.0062 (0.0460)	
training:	Epoch: [42][136/408]	Loss 0.0060 (0.0457)	
training:	Epoch: [42][137/408]	Loss 0.0052 (0.0454)	
training:	Epoch: [42][138/408]	Loss 0.0081 (0.0451)	
training:	Epoch: [42][139/408]	Loss 0.0126 (0.0449)	
training:	Epoch: [42][140/408]	Loss 0.1781 (0.0458)	
training:	Epoch: [42][141/408]	Loss 0.0543 (0.0459)	
training:	Epoch: [42][142/408]	Loss 0.0124 (0.0457)	
training:	Epoch: [42][143/408]	Loss 0.0052 (0.0454)	
training:	Epoch: [42][144/408]	Loss 0.0070 (0.0451)	
training:	Epoch: [42][145/408]	Loss 0.0062 (0.0449)	
training:	Epoch: [42][146/408]	Loss 0.0075 (0.0446)	
training:	Epoch: [42][147/408]	Loss 0.0050 (0.0443)	
training:	Epoch: [42][148/408]	Loss 0.0700 (0.0445)	
training:	Epoch: [42][149/408]	Loss 0.0089 (0.0443)	
training:	Epoch: [42][150/408]	Loss 0.0051 (0.0440)	
training:	Epoch: [42][151/408]	Loss 0.0248 (0.0439)	
training:	Epoch: [42][152/408]	Loss 0.0132 (0.0437)	
training:	Epoch: [42][153/408]	Loss 0.0074 (0.0434)	
training:	Epoch: [42][154/408]	Loss 0.0087 (0.0432)	
training:	Epoch: [42][155/408]	Loss 0.0084 (0.0430)	
training:	Epoch: [42][156/408]	Loss 0.3234 (0.0448)	
training:	Epoch: [42][157/408]	Loss 0.0103 (0.0446)	
training:	Epoch: [42][158/408]	Loss 0.0080 (0.0443)	
training:	Epoch: [42][159/408]	Loss 0.0109 (0.0441)	
training:	Epoch: [42][160/408]	Loss 0.0149 (0.0439)	
training:	Epoch: [42][161/408]	Loss 0.0058 (0.0437)	
training:	Epoch: [42][162/408]	Loss 0.0062 (0.0435)	
training:	Epoch: [42][163/408]	Loss 0.0056 (0.0432)	
training:	Epoch: [42][164/408]	Loss 0.1968 (0.0442)	
training:	Epoch: [42][165/408]	Loss 0.0079 (0.0440)	
training:	Epoch: [42][166/408]	Loss 0.0056 (0.0437)	
training:	Epoch: [42][167/408]	Loss 0.0065 (0.0435)	
training:	Epoch: [42][168/408]	Loss 0.0060 (0.0433)	
training:	Epoch: [42][169/408]	Loss 0.0076 (0.0431)	
training:	Epoch: [42][170/408]	Loss 0.2992 (0.0446)	
training:	Epoch: [42][171/408]	Loss 0.0085 (0.0444)	
training:	Epoch: [42][172/408]	Loss 0.1244 (0.0448)	
training:	Epoch: [42][173/408]	Loss 0.0061 (0.0446)	
training:	Epoch: [42][174/408]	Loss 0.0056 (0.0444)	
training:	Epoch: [42][175/408]	Loss 0.0076 (0.0442)	
training:	Epoch: [42][176/408]	Loss 0.0065 (0.0440)	
training:	Epoch: [42][177/408]	Loss 0.0285 (0.0439)	
training:	Epoch: [42][178/408]	Loss 0.0059 (0.0437)	
training:	Epoch: [42][179/408]	Loss 0.0066 (0.0434)	
training:	Epoch: [42][180/408]	Loss 0.5668 (0.0464)	
training:	Epoch: [42][181/408]	Loss 0.0058 (0.0461)	
training:	Epoch: [42][182/408]	Loss 0.0059 (0.0459)	
training:	Epoch: [42][183/408]	Loss 0.2838 (0.0472)	
training:	Epoch: [42][184/408]	Loss 0.0063 (0.0470)	
training:	Epoch: [42][185/408]	Loss 0.1281 (0.0474)	
training:	Epoch: [42][186/408]	Loss 0.0061 (0.0472)	
training:	Epoch: [42][187/408]	Loss 0.0066 (0.0470)	
training:	Epoch: [42][188/408]	Loss 0.0397 (0.0469)	
training:	Epoch: [42][189/408]	Loss 0.0067 (0.0467)	
training:	Epoch: [42][190/408]	Loss 0.0087 (0.0465)	
training:	Epoch: [42][191/408]	Loss 0.0067 (0.0463)	
training:	Epoch: [42][192/408]	Loss 0.0077 (0.0461)	
training:	Epoch: [42][193/408]	Loss 0.0049 (0.0459)	
training:	Epoch: [42][194/408]	Loss 0.3203 (0.0473)	
training:	Epoch: [42][195/408]	Loss 0.2950 (0.0486)	
training:	Epoch: [42][196/408]	Loss 0.0076 (0.0484)	
training:	Epoch: [42][197/408]	Loss 0.0076 (0.0482)	
training:	Epoch: [42][198/408]	Loss 0.0069 (0.0480)	
training:	Epoch: [42][199/408]	Loss 0.0104 (0.0478)	
training:	Epoch: [42][200/408]	Loss 0.0080 (0.0476)	
training:	Epoch: [42][201/408]	Loss 0.0067 (0.0474)	
training:	Epoch: [42][202/408]	Loss 0.0084 (0.0472)	
training:	Epoch: [42][203/408]	Loss 0.0101 (0.0470)	
training:	Epoch: [42][204/408]	Loss 0.2820 (0.0482)	
training:	Epoch: [42][205/408]	Loss 0.3239 (0.0495)	
training:	Epoch: [42][206/408]	Loss 0.2445 (0.0504)	
training:	Epoch: [42][207/408]	Loss 0.0323 (0.0504)	
training:	Epoch: [42][208/408]	Loss 0.0068 (0.0501)	
training:	Epoch: [42][209/408]	Loss 0.2639 (0.0512)	
training:	Epoch: [42][210/408]	Loss 0.0065 (0.0510)	
training:	Epoch: [42][211/408]	Loss 0.0071 (0.0507)	
training:	Epoch: [42][212/408]	Loss 0.0073 (0.0505)	
training:	Epoch: [42][213/408]	Loss 0.0252 (0.0504)	
training:	Epoch: [42][214/408]	Loss 0.0082 (0.0502)	
training:	Epoch: [42][215/408]	Loss 0.0084 (0.0500)	
training:	Epoch: [42][216/408]	Loss 0.0065 (0.0498)	
training:	Epoch: [42][217/408]	Loss 0.3152 (0.0511)	
training:	Epoch: [42][218/408]	Loss 0.0065 (0.0509)	
training:	Epoch: [42][219/408]	Loss 0.0077 (0.0507)	
training:	Epoch: [42][220/408]	Loss 0.0164 (0.0505)	
training:	Epoch: [42][221/408]	Loss 0.0083 (0.0503)	
training:	Epoch: [42][222/408]	Loss 0.0162 (0.0502)	
training:	Epoch: [42][223/408]	Loss 0.1846 (0.0508)	
training:	Epoch: [42][224/408]	Loss 0.0130 (0.0506)	
training:	Epoch: [42][225/408]	Loss 0.0079 (0.0504)	
training:	Epoch: [42][226/408]	Loss 0.0066 (0.0502)	
training:	Epoch: [42][227/408]	Loss 0.0064 (0.0500)	
training:	Epoch: [42][228/408]	Loss 0.0074 (0.0498)	
training:	Epoch: [42][229/408]	Loss 0.0094 (0.0496)	
training:	Epoch: [42][230/408]	Loss 0.0081 (0.0495)	
training:	Epoch: [42][231/408]	Loss 0.0093 (0.0493)	
training:	Epoch: [42][232/408]	Loss 0.0158 (0.0491)	
training:	Epoch: [42][233/408]	Loss 0.0089 (0.0490)	
training:	Epoch: [42][234/408]	Loss 0.0078 (0.0488)	
training:	Epoch: [42][235/408]	Loss 0.0104 (0.0486)	
training:	Epoch: [42][236/408]	Loss 0.0149 (0.0485)	
training:	Epoch: [42][237/408]	Loss 0.0062 (0.0483)	
training:	Epoch: [42][238/408]	Loss 0.0083 (0.0481)	
training:	Epoch: [42][239/408]	Loss 0.0072 (0.0480)	
training:	Epoch: [42][240/408]	Loss 0.0078 (0.0478)	
training:	Epoch: [42][241/408]	Loss 0.0173 (0.0477)	
training:	Epoch: [42][242/408]	Loss 0.0084 (0.0475)	
training:	Epoch: [42][243/408]	Loss 0.0062 (0.0473)	
training:	Epoch: [42][244/408]	Loss 0.0075 (0.0472)	
training:	Epoch: [42][245/408]	Loss 0.0102 (0.0470)	
training:	Epoch: [42][246/408]	Loss 0.0069 (0.0469)	
training:	Epoch: [42][247/408]	Loss 0.0082 (0.0467)	
training:	Epoch: [42][248/408]	Loss 0.0081 (0.0466)	
training:	Epoch: [42][249/408]	Loss 0.0057 (0.0464)	
training:	Epoch: [42][250/408]	Loss 0.0074 (0.0462)	
training:	Epoch: [42][251/408]	Loss 0.0080 (0.0461)	
training:	Epoch: [42][252/408]	Loss 0.0263 (0.0460)	
training:	Epoch: [42][253/408]	Loss 0.0104 (0.0459)	
training:	Epoch: [42][254/408]	Loss 0.0129 (0.0457)	
training:	Epoch: [42][255/408]	Loss 0.0086 (0.0456)	
training:	Epoch: [42][256/408]	Loss 0.0073 (0.0454)	
training:	Epoch: [42][257/408]	Loss 0.0079 (0.0453)	
training:	Epoch: [42][258/408]	Loss 0.0098 (0.0452)	
training:	Epoch: [42][259/408]	Loss 0.0062 (0.0450)	
training:	Epoch: [42][260/408]	Loss 0.0092 (0.0449)	
training:	Epoch: [42][261/408]	Loss 0.0098 (0.0447)	
training:	Epoch: [42][262/408]	Loss 0.2565 (0.0455)	
training:	Epoch: [42][263/408]	Loss 0.2755 (0.0464)	
training:	Epoch: [42][264/408]	Loss 0.0157 (0.0463)	
training:	Epoch: [42][265/408]	Loss 0.2357 (0.0470)	
training:	Epoch: [42][266/408]	Loss 0.0063 (0.0469)	
training:	Epoch: [42][267/408]	Loss 0.0094 (0.0467)	
training:	Epoch: [42][268/408]	Loss 0.0056 (0.0466)	
training:	Epoch: [42][269/408]	Loss 0.0078 (0.0464)	
training:	Epoch: [42][270/408]	Loss 0.0125 (0.0463)	
training:	Epoch: [42][271/408]	Loss 0.0062 (0.0462)	
training:	Epoch: [42][272/408]	Loss 0.0081 (0.0460)	
training:	Epoch: [42][273/408]	Loss 0.0062 (0.0459)	
training:	Epoch: [42][274/408]	Loss 0.0104 (0.0457)	
training:	Epoch: [42][275/408]	Loss 0.0118 (0.0456)	
training:	Epoch: [42][276/408]	Loss 0.0074 (0.0455)	
training:	Epoch: [42][277/408]	Loss 0.0065 (0.0453)	
training:	Epoch: [42][278/408]	Loss 0.0086 (0.0452)	
training:	Epoch: [42][279/408]	Loss 0.0080 (0.0451)	
training:	Epoch: [42][280/408]	Loss 0.0066 (0.0449)	
training:	Epoch: [42][281/408]	Loss 0.0080 (0.0448)	
training:	Epoch: [42][282/408]	Loss 0.0068 (0.0447)	
training:	Epoch: [42][283/408]	Loss 0.0059 (0.0445)	
training:	Epoch: [42][284/408]	Loss 0.0050 (0.0444)	
training:	Epoch: [42][285/408]	Loss 0.0087 (0.0443)	
training:	Epoch: [42][286/408]	Loss 0.0055 (0.0441)	
training:	Epoch: [42][287/408]	Loss 0.0071 (0.0440)	
training:	Epoch: [42][288/408]	Loss 0.0098 (0.0439)	
training:	Epoch: [42][289/408]	Loss 0.0061 (0.0438)	
training:	Epoch: [42][290/408]	Loss 0.2479 (0.0445)	
training:	Epoch: [42][291/408]	Loss 0.0071 (0.0443)	
training:	Epoch: [42][292/408]	Loss 0.0077 (0.0442)	
training:	Epoch: [42][293/408]	Loss 0.0061 (0.0441)	
training:	Epoch: [42][294/408]	Loss 0.0072 (0.0439)	
training:	Epoch: [42][295/408]	Loss 0.0296 (0.0439)	
training:	Epoch: [42][296/408]	Loss 0.0095 (0.0438)	
training:	Epoch: [42][297/408]	Loss 0.0068 (0.0437)	
training:	Epoch: [42][298/408]	Loss 0.0071 (0.0435)	
training:	Epoch: [42][299/408]	Loss 0.0073 (0.0434)	
training:	Epoch: [42][300/408]	Loss 0.0067 (0.0433)	
training:	Epoch: [42][301/408]	Loss 0.0082 (0.0432)	
training:	Epoch: [42][302/408]	Loss 0.0060 (0.0431)	
training:	Epoch: [42][303/408]	Loss 0.0086 (0.0429)	
training:	Epoch: [42][304/408]	Loss 0.0086 (0.0428)	
training:	Epoch: [42][305/408]	Loss 0.0063 (0.0427)	
training:	Epoch: [42][306/408]	Loss 0.0102 (0.0426)	
training:	Epoch: [42][307/408]	Loss 0.0132 (0.0425)	
training:	Epoch: [42][308/408]	Loss 0.0070 (0.0424)	
training:	Epoch: [42][309/408]	Loss 0.0089 (0.0423)	
training:	Epoch: [42][310/408]	Loss 0.0115 (0.0422)	
training:	Epoch: [42][311/408]	Loss 0.0077 (0.0421)	
training:	Epoch: [42][312/408]	Loss 0.2417 (0.0427)	
training:	Epoch: [42][313/408]	Loss 0.0093 (0.0426)	
training:	Epoch: [42][314/408]	Loss 0.2882 (0.0434)	
training:	Epoch: [42][315/408]	Loss 0.2635 (0.0441)	
training:	Epoch: [42][316/408]	Loss 0.2452 (0.0447)	
training:	Epoch: [42][317/408]	Loss 0.0145 (0.0446)	
training:	Epoch: [42][318/408]	Loss 0.0061 (0.0445)	
training:	Epoch: [42][319/408]	Loss 0.0066 (0.0444)	
training:	Epoch: [42][320/408]	Loss 0.0077 (0.0443)	
training:	Epoch: [42][321/408]	Loss 0.0075 (0.0442)	
training:	Epoch: [42][322/408]	Loss 0.0057 (0.0440)	
training:	Epoch: [42][323/408]	Loss 0.0091 (0.0439)	
training:	Epoch: [42][324/408]	Loss 0.2992 (0.0447)	
training:	Epoch: [42][325/408]	Loss 0.0067 (0.0446)	
training:	Epoch: [42][326/408]	Loss 0.0070 (0.0445)	
training:	Epoch: [42][327/408]	Loss 0.0055 (0.0444)	
training:	Epoch: [42][328/408]	Loss 0.0077 (0.0443)	
training:	Epoch: [42][329/408]	Loss 0.0106 (0.0441)	
training:	Epoch: [42][330/408]	Loss 0.0072 (0.0440)	
training:	Epoch: [42][331/408]	Loss 0.0084 (0.0439)	
training:	Epoch: [42][332/408]	Loss 0.5227 (0.0454)	
training:	Epoch: [42][333/408]	Loss 0.0074 (0.0453)	
training:	Epoch: [42][334/408]	Loss 0.0105 (0.0452)	
training:	Epoch: [42][335/408]	Loss 0.2508 (0.0458)	
training:	Epoch: [42][336/408]	Loss 0.0075 (0.0457)	
training:	Epoch: [42][337/408]	Loss 0.0863 (0.0458)	
training:	Epoch: [42][338/408]	Loss 0.0066 (0.0457)	
training:	Epoch: [42][339/408]	Loss 0.0071 (0.0455)	
training:	Epoch: [42][340/408]	Loss 0.2864 (0.0463)	
training:	Epoch: [42][341/408]	Loss 0.0079 (0.0461)	
training:	Epoch: [42][342/408]	Loss 0.0116 (0.0460)	
training:	Epoch: [42][343/408]	Loss 0.0081 (0.0459)	
training:	Epoch: [42][344/408]	Loss 0.2162 (0.0464)	
training:	Epoch: [42][345/408]	Loss 0.0072 (0.0463)	
training:	Epoch: [42][346/408]	Loss 0.0078 (0.0462)	
training:	Epoch: [42][347/408]	Loss 0.0076 (0.0461)	
training:	Epoch: [42][348/408]	Loss 0.5144 (0.0474)	
training:	Epoch: [42][349/408]	Loss 0.0080 (0.0473)	
training:	Epoch: [42][350/408]	Loss 0.0075 (0.0472)	
training:	Epoch: [42][351/408]	Loss 0.0080 (0.0471)	
training:	Epoch: [42][352/408]	Loss 0.0075 (0.0470)	
training:	Epoch: [42][353/408]	Loss 0.0082 (0.0469)	
training:	Epoch: [42][354/408]	Loss 0.0073 (0.0468)	
training:	Epoch: [42][355/408]	Loss 0.0075 (0.0467)	
training:	Epoch: [42][356/408]	Loss 0.0076 (0.0465)	
training:	Epoch: [42][357/408]	Loss 0.0079 (0.0464)	
training:	Epoch: [42][358/408]	Loss 0.0071 (0.0463)	
training:	Epoch: [42][359/408]	Loss 0.0095 (0.0462)	
training:	Epoch: [42][360/408]	Loss 0.0118 (0.0461)	
training:	Epoch: [42][361/408]	Loss 0.0109 (0.0460)	
training:	Epoch: [42][362/408]	Loss 0.0081 (0.0459)	
training:	Epoch: [42][363/408]	Loss 0.0088 (0.0458)	
training:	Epoch: [42][364/408]	Loss 0.0060 (0.0457)	
training:	Epoch: [42][365/408]	Loss 0.0077 (0.0456)	
training:	Epoch: [42][366/408]	Loss 0.0089 (0.0455)	
training:	Epoch: [42][367/408]	Loss 0.0373 (0.0455)	
training:	Epoch: [42][368/408]	Loss 0.0096 (0.0454)	
training:	Epoch: [42][369/408]	Loss 0.1041 (0.0455)	
training:	Epoch: [42][370/408]	Loss 0.0147 (0.0455)	
training:	Epoch: [42][371/408]	Loss 0.0074 (0.0454)	
training:	Epoch: [42][372/408]	Loss 0.0100 (0.0453)	
training:	Epoch: [42][373/408]	Loss 0.0082 (0.0452)	
training:	Epoch: [42][374/408]	Loss 0.0089 (0.0451)	
training:	Epoch: [42][375/408]	Loss 0.0083 (0.0450)	
training:	Epoch: [42][376/408]	Loss 0.0115 (0.0449)	
training:	Epoch: [42][377/408]	Loss 0.0062 (0.0448)	
training:	Epoch: [42][378/408]	Loss 0.0070 (0.0447)	
training:	Epoch: [42][379/408]	Loss 0.0065 (0.0446)	
training:	Epoch: [42][380/408]	Loss 0.0058 (0.0445)	
training:	Epoch: [42][381/408]	Loss 0.0062 (0.0444)	
training:	Epoch: [42][382/408]	Loss 0.0085 (0.0443)	
training:	Epoch: [42][383/408]	Loss 0.0070 (0.0442)	
training:	Epoch: [42][384/408]	Loss 0.0066 (0.0441)	
training:	Epoch: [42][385/408]	Loss 0.0467 (0.0441)	
training:	Epoch: [42][386/408]	Loss 0.2074 (0.0445)	
training:	Epoch: [42][387/408]	Loss 0.0114 (0.0444)	
training:	Epoch: [42][388/408]	Loss 0.0063 (0.0443)	
training:	Epoch: [42][389/408]	Loss 0.2684 (0.0449)	
training:	Epoch: [42][390/408]	Loss 0.0083 (0.0448)	
training:	Epoch: [42][391/408]	Loss 0.0082 (0.0447)	
training:	Epoch: [42][392/408]	Loss 0.0089 (0.0446)	
training:	Epoch: [42][393/408]	Loss 0.0064 (0.0445)	
training:	Epoch: [42][394/408]	Loss 0.0134 (0.0445)	
training:	Epoch: [42][395/408]	Loss 0.0062 (0.0444)	
training:	Epoch: [42][396/408]	Loss 0.0090 (0.0443)	
training:	Epoch: [42][397/408]	Loss 0.0066 (0.0442)	
training:	Epoch: [42][398/408]	Loss 0.0074 (0.0441)	
training:	Epoch: [42][399/408]	Loss 0.0073 (0.0440)	
training:	Epoch: [42][400/408]	Loss 0.0080 (0.0439)	
training:	Epoch: [42][401/408]	Loss 0.0096 (0.0438)	
training:	Epoch: [42][402/408]	Loss 0.0093 (0.0437)	
training:	Epoch: [42][403/408]	Loss 0.0098 (0.0436)	
training:	Epoch: [42][404/408]	Loss 0.0082 (0.0436)	
training:	Epoch: [42][405/408]	Loss 0.0075 (0.0435)	
training:	Epoch: [42][406/408]	Loss 0.0076 (0.0434)	
training:	Epoch: [42][407/408]	Loss 0.0064 (0.0433)	
training:	Epoch: [42][408/408]	Loss 0.0459 (0.0433)	
Training:	 Loss: 0.0432

Training:	 ACC: 0.9929 0.9928 0.9906 0.9952
Validation:	 ACC: 0.7792 0.7780 0.7523 0.8061
Validation:	 Best_BACC: 0.8226 0.8250 0.8772 0.7679
Validation:	 Loss: 0.9478
Pretraining:	Epoch 43/200
----------
training:	Epoch: [43][1/408]	Loss 0.2996 (0.2996)	
training:	Epoch: [43][2/408]	Loss 0.0059 (0.1528)	
training:	Epoch: [43][3/408]	Loss 0.0088 (0.1048)	
training:	Epoch: [43][4/408]	Loss 0.0496 (0.0910)	
training:	Epoch: [43][5/408]	Loss 0.0083 (0.0745)	
training:	Epoch: [43][6/408]	Loss 0.0062 (0.0631)	
training:	Epoch: [43][7/408]	Loss 0.0074 (0.0551)	
training:	Epoch: [43][8/408]	Loss 0.0095 (0.0494)	
training:	Epoch: [43][9/408]	Loss 0.0063 (0.0446)	
training:	Epoch: [43][10/408]	Loss 0.0080 (0.0410)	
training:	Epoch: [43][11/408]	Loss 0.0063 (0.0378)	
training:	Epoch: [43][12/408]	Loss 0.0078 (0.0353)	
training:	Epoch: [43][13/408]	Loss 0.0264 (0.0346)	
training:	Epoch: [43][14/408]	Loss 0.0092 (0.0328)	
training:	Epoch: [43][15/408]	Loss 0.0067 (0.0311)	
training:	Epoch: [43][16/408]	Loss 0.0050 (0.0295)	
training:	Epoch: [43][17/408]	Loss 0.0099 (0.0283)	
training:	Epoch: [43][18/408]	Loss 0.0069 (0.0271)	
training:	Epoch: [43][19/408]	Loss 0.0049 (0.0259)	
training:	Epoch: [43][20/408]	Loss 0.0074 (0.0250)	
training:	Epoch: [43][21/408]	Loss 0.0136 (0.0245)	
training:	Epoch: [43][22/408]	Loss 0.0074 (0.0237)	
training:	Epoch: [43][23/408]	Loss 0.0056 (0.0229)	
training:	Epoch: [43][24/408]	Loss 0.1778 (0.0294)	
training:	Epoch: [43][25/408]	Loss 0.0089 (0.0286)	
training:	Epoch: [43][26/408]	Loss 0.0088 (0.0278)	
training:	Epoch: [43][27/408]	Loss 0.0081 (0.0271)	
training:	Epoch: [43][28/408]	Loss 0.0054 (0.0263)	
training:	Epoch: [43][29/408]	Loss 0.0060 (0.0256)	
training:	Epoch: [43][30/408]	Loss 0.0139 (0.0252)	
training:	Epoch: [43][31/408]	Loss 0.0078 (0.0246)	
training:	Epoch: [43][32/408]	Loss 0.0064 (0.0241)	
training:	Epoch: [43][33/408]	Loss 0.0059 (0.0235)	
training:	Epoch: [43][34/408]	Loss 0.0070 (0.0230)	
training:	Epoch: [43][35/408]	Loss 0.0093 (0.0226)	
training:	Epoch: [43][36/408]	Loss 0.0068 (0.0222)	
training:	Epoch: [43][37/408]	Loss 0.0096 (0.0219)	
training:	Epoch: [43][38/408]	Loss 0.0082 (0.0215)	
training:	Epoch: [43][39/408]	Loss 0.0076 (0.0211)	
training:	Epoch: [43][40/408]	Loss 0.0090 (0.0208)	
training:	Epoch: [43][41/408]	Loss 0.0061 (0.0205)	
training:	Epoch: [43][42/408]	Loss 0.0075 (0.0202)	
training:	Epoch: [43][43/408]	Loss 0.0067 (0.0199)	
training:	Epoch: [43][44/408]	Loss 0.1193 (0.0221)	
training:	Epoch: [43][45/408]	Loss 0.3358 (0.0291)	
training:	Epoch: [43][46/408]	Loss 0.0057 (0.0286)	
training:	Epoch: [43][47/408]	Loss 0.0071 (0.0281)	
training:	Epoch: [43][48/408]	Loss 0.0056 (0.0277)	
training:	Epoch: [43][49/408]	Loss 0.0062 (0.0272)	
training:	Epoch: [43][50/408]	Loss 0.0071 (0.0268)	
training:	Epoch: [43][51/408]	Loss 0.0076 (0.0264)	
training:	Epoch: [43][52/408]	Loss 0.0073 (0.0261)	
training:	Epoch: [43][53/408]	Loss 0.0057 (0.0257)	
training:	Epoch: [43][54/408]	Loss 0.0068 (0.0253)	
training:	Epoch: [43][55/408]	Loss 0.0057 (0.0250)	
training:	Epoch: [43][56/408]	Loss 0.0063 (0.0246)	
training:	Epoch: [43][57/408]	Loss 0.0072 (0.0243)	
training:	Epoch: [43][58/408]	Loss 0.2357 (0.0280)	
training:	Epoch: [43][59/408]	Loss 0.1054 (0.0293)	
training:	Epoch: [43][60/408]	Loss 0.0100 (0.0290)	
training:	Epoch: [43][61/408]	Loss 0.0060 (0.0286)	
training:	Epoch: [43][62/408]	Loss 0.3152 (0.0332)	
training:	Epoch: [43][63/408]	Loss 0.0061 (0.0328)	
training:	Epoch: [43][64/408]	Loss 0.0073 (0.0324)	
training:	Epoch: [43][65/408]	Loss 0.0068 (0.0320)	
training:	Epoch: [43][66/408]	Loss 0.0071 (0.0316)	
training:	Epoch: [43][67/408]	Loss 0.0072 (0.0313)	
training:	Epoch: [43][68/408]	Loss 0.0076 (0.0309)	
training:	Epoch: [43][69/408]	Loss 0.0070 (0.0306)	
training:	Epoch: [43][70/408]	Loss 0.0075 (0.0302)	
training:	Epoch: [43][71/408]	Loss 0.0069 (0.0299)	
training:	Epoch: [43][72/408]	Loss 0.0066 (0.0296)	
training:	Epoch: [43][73/408]	Loss 0.0047 (0.0292)	
training:	Epoch: [43][74/408]	Loss 0.0070 (0.0289)	
training:	Epoch: [43][75/408]	Loss 0.0071 (0.0286)	
training:	Epoch: [43][76/408]	Loss 0.0070 (0.0284)	
training:	Epoch: [43][77/408]	Loss 0.0070 (0.0281)	
training:	Epoch: [43][78/408]	Loss 0.0063 (0.0278)	
training:	Epoch: [43][79/408]	Loss 0.0071 (0.0275)	
training:	Epoch: [43][80/408]	Loss 0.0067 (0.0273)	
training:	Epoch: [43][81/408]	Loss 0.0091 (0.0271)	
training:	Epoch: [43][82/408]	Loss 0.0073 (0.0268)	
training:	Epoch: [43][83/408]	Loss 0.0063 (0.0266)	
training:	Epoch: [43][84/408]	Loss 0.0079 (0.0263)	
training:	Epoch: [43][85/408]	Loss 0.0078 (0.0261)	
training:	Epoch: [43][86/408]	Loss 0.0055 (0.0259)	
training:	Epoch: [43][87/408]	Loss 0.0064 (0.0257)	
training:	Epoch: [43][88/408]	Loss 0.0070 (0.0255)	
training:	Epoch: [43][89/408]	Loss 0.0069 (0.0252)	
training:	Epoch: [43][90/408]	Loss 0.0415 (0.0254)	
training:	Epoch: [43][91/408]	Loss 0.0079 (0.0252)	
training:	Epoch: [43][92/408]	Loss 0.0468 (0.0255)	
training:	Epoch: [43][93/408]	Loss 0.0077 (0.0253)	
training:	Epoch: [43][94/408]	Loss 0.0076 (0.0251)	
training:	Epoch: [43][95/408]	Loss 0.0065 (0.0249)	
training:	Epoch: [43][96/408]	Loss 0.0059 (0.0247)	
training:	Epoch: [43][97/408]	Loss 0.0063 (0.0245)	
training:	Epoch: [43][98/408]	Loss 0.0056 (0.0243)	
training:	Epoch: [43][99/408]	Loss 0.0067 (0.0241)	
training:	Epoch: [43][100/408]	Loss 0.0070 (0.0240)	
training:	Epoch: [43][101/408]	Loss 0.0077 (0.0238)	
training:	Epoch: [43][102/408]	Loss 0.0067 (0.0236)	
training:	Epoch: [43][103/408]	Loss 0.0047 (0.0235)	
training:	Epoch: [43][104/408]	Loss 0.0068 (0.0233)	
training:	Epoch: [43][105/408]	Loss 0.2525 (0.0255)	
training:	Epoch: [43][106/408]	Loss 0.0075 (0.0253)	
training:	Epoch: [43][107/408]	Loss 0.0056 (0.0251)	
training:	Epoch: [43][108/408]	Loss 0.0101 (0.0250)	
training:	Epoch: [43][109/408]	Loss 0.0075 (0.0248)	
training:	Epoch: [43][110/408]	Loss 0.0058 (0.0246)	
training:	Epoch: [43][111/408]	Loss 0.0530 (0.0249)	
training:	Epoch: [43][112/408]	Loss 0.0081 (0.0248)	
training:	Epoch: [43][113/408]	Loss 0.0079 (0.0246)	
training:	Epoch: [43][114/408]	Loss 0.0056 (0.0244)	
training:	Epoch: [43][115/408]	Loss 0.0081 (0.0243)	
training:	Epoch: [43][116/408]	Loss 0.0108 (0.0242)	
training:	Epoch: [43][117/408]	Loss 0.0079 (0.0240)	
training:	Epoch: [43][118/408]	Loss 0.0070 (0.0239)	
training:	Epoch: [43][119/408]	Loss 0.0070 (0.0238)	
training:	Epoch: [43][120/408]	Loss 0.2806 (0.0259)	
training:	Epoch: [43][121/408]	Loss 0.0065 (0.0257)	
training:	Epoch: [43][122/408]	Loss 0.0061 (0.0256)	
training:	Epoch: [43][123/408]	Loss 0.0074 (0.0254)	
training:	Epoch: [43][124/408]	Loss 0.0064 (0.0253)	
training:	Epoch: [43][125/408]	Loss 0.0058 (0.0251)	
training:	Epoch: [43][126/408]	Loss 0.0078 (0.0250)	
training:	Epoch: [43][127/408]	Loss 0.0091 (0.0249)	
training:	Epoch: [43][128/408]	Loss 0.0062 (0.0247)	
training:	Epoch: [43][129/408]	Loss 0.0080 (0.0246)	
training:	Epoch: [43][130/408]	Loss 0.0075 (0.0244)	
training:	Epoch: [43][131/408]	Loss 0.0083 (0.0243)	
training:	Epoch: [43][132/408]	Loss 0.0068 (0.0242)	
training:	Epoch: [43][133/408]	Loss 0.0058 (0.0240)	
training:	Epoch: [43][134/408]	Loss 0.2767 (0.0259)	
training:	Epoch: [43][135/408]	Loss 0.0050 (0.0258)	
training:	Epoch: [43][136/408]	Loss 0.0064 (0.0256)	
training:	Epoch: [43][137/408]	Loss 0.0047 (0.0255)	
training:	Epoch: [43][138/408]	Loss 0.0068 (0.0253)	
training:	Epoch: [43][139/408]	Loss 0.0068 (0.0252)	
training:	Epoch: [43][140/408]	Loss 0.0054 (0.0251)	
training:	Epoch: [43][141/408]	Loss 0.0057 (0.0249)	
training:	Epoch: [43][142/408]	Loss 0.0055 (0.0248)	
training:	Epoch: [43][143/408]	Loss 0.0057 (0.0247)	
training:	Epoch: [43][144/408]	Loss 0.0050 (0.0245)	
training:	Epoch: [43][145/408]	Loss 0.0056 (0.0244)	
training:	Epoch: [43][146/408]	Loss 0.0075 (0.0243)	
training:	Epoch: [43][147/408]	Loss 0.0075 (0.0242)	
training:	Epoch: [43][148/408]	Loss 0.0059 (0.0240)	
training:	Epoch: [43][149/408]	Loss 0.2419 (0.0255)	
training:	Epoch: [43][150/408]	Loss 0.0051 (0.0254)	
training:	Epoch: [43][151/408]	Loss 0.0076 (0.0253)	
training:	Epoch: [43][152/408]	Loss 0.0075 (0.0251)	
training:	Epoch: [43][153/408]	Loss 0.0051 (0.0250)	
training:	Epoch: [43][154/408]	Loss 0.0071 (0.0249)	
training:	Epoch: [43][155/408]	Loss 0.0064 (0.0248)	
training:	Epoch: [43][156/408]	Loss 0.0050 (0.0246)	
training:	Epoch: [43][157/408]	Loss 0.0064 (0.0245)	
training:	Epoch: [43][158/408]	Loss 0.2730 (0.0261)	
training:	Epoch: [43][159/408]	Loss 0.0082 (0.0260)	
training:	Epoch: [43][160/408]	Loss 0.0086 (0.0259)	
training:	Epoch: [43][161/408]	Loss 0.0048 (0.0257)	
training:	Epoch: [43][162/408]	Loss 0.0087 (0.0256)	
training:	Epoch: [43][163/408]	Loss 0.0063 (0.0255)	
training:	Epoch: [43][164/408]	Loss 0.0053 (0.0254)	
training:	Epoch: [43][165/408]	Loss 0.0085 (0.0253)	
training:	Epoch: [43][166/408]	Loss 0.0092 (0.0252)	
training:	Epoch: [43][167/408]	Loss 0.0090 (0.0251)	
training:	Epoch: [43][168/408]	Loss 0.0067 (0.0250)	
training:	Epoch: [43][169/408]	Loss 0.0057 (0.0249)	
training:	Epoch: [43][170/408]	Loss 0.0051 (0.0248)	
training:	Epoch: [43][171/408]	Loss 0.2688 (0.0262)	
training:	Epoch: [43][172/408]	Loss 0.0052 (0.0261)	
training:	Epoch: [43][173/408]	Loss 0.0063 (0.0260)	
training:	Epoch: [43][174/408]	Loss 0.0046 (0.0258)	
training:	Epoch: [43][175/408]	Loss 0.0062 (0.0257)	
training:	Epoch: [43][176/408]	Loss 0.0095 (0.0256)	
training:	Epoch: [43][177/408]	Loss 0.0052 (0.0255)	
training:	Epoch: [43][178/408]	Loss 0.0058 (0.0254)	
training:	Epoch: [43][179/408]	Loss 0.0085 (0.0253)	
training:	Epoch: [43][180/408]	Loss 0.0055 (0.0252)	
training:	Epoch: [43][181/408]	Loss 0.0084 (0.0251)	
training:	Epoch: [43][182/408]	Loss 0.0063 (0.0250)	
training:	Epoch: [43][183/408]	Loss 0.0065 (0.0249)	
training:	Epoch: [43][184/408]	Loss 0.0069 (0.0248)	
training:	Epoch: [43][185/408]	Loss 0.0071 (0.0247)	
training:	Epoch: [43][186/408]	Loss 0.0097 (0.0246)	
training:	Epoch: [43][187/408]	Loss 0.0078 (0.0245)	
training:	Epoch: [43][188/408]	Loss 0.0060 (0.0244)	
training:	Epoch: [43][189/408]	Loss 0.0101 (0.0244)	
training:	Epoch: [43][190/408]	Loss 0.0069 (0.0243)	
training:	Epoch: [43][191/408]	Loss 0.0096 (0.0242)	
training:	Epoch: [43][192/408]	Loss 0.2277 (0.0253)	
training:	Epoch: [43][193/408]	Loss 0.0071 (0.0252)	
training:	Epoch: [43][194/408]	Loss 0.0103 (0.0251)	
training:	Epoch: [43][195/408]	Loss 0.0084 (0.0250)	
training:	Epoch: [43][196/408]	Loss 0.0078 (0.0249)	
training:	Epoch: [43][197/408]	Loss 0.3111 (0.0264)	
training:	Epoch: [43][198/408]	Loss 0.0064 (0.0263)	
training:	Epoch: [43][199/408]	Loss 0.0064 (0.0262)	
training:	Epoch: [43][200/408]	Loss 0.6642 (0.0294)	
training:	Epoch: [43][201/408]	Loss 0.0076 (0.0292)	
training:	Epoch: [43][202/408]	Loss 0.0073 (0.0291)	
training:	Epoch: [43][203/408]	Loss 0.0072 (0.0290)	
training:	Epoch: [43][204/408]	Loss 0.0085 (0.0289)	
training:	Epoch: [43][205/408]	Loss 0.0065 (0.0288)	
training:	Epoch: [43][206/408]	Loss 0.0065 (0.0287)	
training:	Epoch: [43][207/408]	Loss 0.0060 (0.0286)	
training:	Epoch: [43][208/408]	Loss 0.0059 (0.0285)	
training:	Epoch: [43][209/408]	Loss 0.0056 (0.0284)	
training:	Epoch: [43][210/408]	Loss 0.0061 (0.0283)	
training:	Epoch: [43][211/408]	Loss 0.0073 (0.0282)	
training:	Epoch: [43][212/408]	Loss 0.0061 (0.0281)	
training:	Epoch: [43][213/408]	Loss 0.0053 (0.0280)	
training:	Epoch: [43][214/408]	Loss 0.0058 (0.0279)	
training:	Epoch: [43][215/408]	Loss 0.0066 (0.0278)	
training:	Epoch: [43][216/408]	Loss 0.0072 (0.0277)	
training:	Epoch: [43][217/408]	Loss 0.0079 (0.0276)	
training:	Epoch: [43][218/408]	Loss 0.0072 (0.0275)	
training:	Epoch: [43][219/408]	Loss 0.0056 (0.0274)	
training:	Epoch: [43][220/408]	Loss 0.0050 (0.0273)	
training:	Epoch: [43][221/408]	Loss 0.3253 (0.0286)	
training:	Epoch: [43][222/408]	Loss 0.0052 (0.0285)	
training:	Epoch: [43][223/408]	Loss 0.0065 (0.0284)	
training:	Epoch: [43][224/408]	Loss 0.3388 (0.0298)	
training:	Epoch: [43][225/408]	Loss 0.0062 (0.0297)	
training:	Epoch: [43][226/408]	Loss 0.0062 (0.0296)	
training:	Epoch: [43][227/408]	Loss 0.0079 (0.0295)	
training:	Epoch: [43][228/408]	Loss 0.0069 (0.0294)	
training:	Epoch: [43][229/408]	Loss 0.0073 (0.0293)	
training:	Epoch: [43][230/408]	Loss 0.0073 (0.0292)	
training:	Epoch: [43][231/408]	Loss 0.0059 (0.0291)	
training:	Epoch: [43][232/408]	Loss 0.0050 (0.0290)	
training:	Epoch: [43][233/408]	Loss 0.0047 (0.0289)	
training:	Epoch: [43][234/408]	Loss 0.0066 (0.0288)	
training:	Epoch: [43][235/408]	Loss 0.0066 (0.0287)	
training:	Epoch: [43][236/408]	Loss 0.0082 (0.0286)	
training:	Epoch: [43][237/408]	Loss 0.0052 (0.0285)	
training:	Epoch: [43][238/408]	Loss 0.0061 (0.0284)	
training:	Epoch: [43][239/408]	Loss 0.0138 (0.0284)	
training:	Epoch: [43][240/408]	Loss 0.0050 (0.0283)	
training:	Epoch: [43][241/408]	Loss 0.0098 (0.0282)	
training:	Epoch: [43][242/408]	Loss 0.0068 (0.0281)	
training:	Epoch: [43][243/408]	Loss 0.2727 (0.0291)	
training:	Epoch: [43][244/408]	Loss 0.0061 (0.0290)	
training:	Epoch: [43][245/408]	Loss 0.0056 (0.0289)	
training:	Epoch: [43][246/408]	Loss 0.0055 (0.0288)	
training:	Epoch: [43][247/408]	Loss 0.0073 (0.0287)	
training:	Epoch: [43][248/408]	Loss 0.0073 (0.0287)	
training:	Epoch: [43][249/408]	Loss 0.0083 (0.0286)	
training:	Epoch: [43][250/408]	Loss 0.0292 (0.0286)	
training:	Epoch: [43][251/408]	Loss 0.0056 (0.0285)	
training:	Epoch: [43][252/408]	Loss 0.0069 (0.0284)	
training:	Epoch: [43][253/408]	Loss 0.0055 (0.0283)	
training:	Epoch: [43][254/408]	Loss 0.0091 (0.0282)	
training:	Epoch: [43][255/408]	Loss 0.0078 (0.0282)	
training:	Epoch: [43][256/408]	Loss 0.0054 (0.0281)	
training:	Epoch: [43][257/408]	Loss 0.0066 (0.0280)	
training:	Epoch: [43][258/408]	Loss 0.0057 (0.0279)	
training:	Epoch: [43][259/408]	Loss 0.0056 (0.0278)	
training:	Epoch: [43][260/408]	Loss 0.0078 (0.0277)	
training:	Epoch: [43][261/408]	Loss 0.0063 (0.0277)	
training:	Epoch: [43][262/408]	Loss 0.0066 (0.0276)	
training:	Epoch: [43][263/408]	Loss 0.0061 (0.0275)	
training:	Epoch: [43][264/408]	Loss 0.0084 (0.0274)	
training:	Epoch: [43][265/408]	Loss 0.2955 (0.0284)	
training:	Epoch: [43][266/408]	Loss 0.0072 (0.0284)	
training:	Epoch: [43][267/408]	Loss 0.2412 (0.0291)	
training:	Epoch: [43][268/408]	Loss 0.0067 (0.0291)	
training:	Epoch: [43][269/408]	Loss 0.0061 (0.0290)	
training:	Epoch: [43][270/408]	Loss 0.2852 (0.0299)	
training:	Epoch: [43][271/408]	Loss 0.0067 (0.0298)	
training:	Epoch: [43][272/408]	Loss 0.0075 (0.0298)	
training:	Epoch: [43][273/408]	Loss 0.0066 (0.0297)	
training:	Epoch: [43][274/408]	Loss 0.0064 (0.0296)	
training:	Epoch: [43][275/408]	Loss 0.0057 (0.0295)	
training:	Epoch: [43][276/408]	Loss 0.1516 (0.0299)	
training:	Epoch: [43][277/408]	Loss 0.0072 (0.0299)	
training:	Epoch: [43][278/408]	Loss 0.0059 (0.0298)	
training:	Epoch: [43][279/408]	Loss 0.0084 (0.0297)	
training:	Epoch: [43][280/408]	Loss 0.0073 (0.0296)	
training:	Epoch: [43][281/408]	Loss 0.0059 (0.0295)	
training:	Epoch: [43][282/408]	Loss 0.2402 (0.0303)	
training:	Epoch: [43][283/408]	Loss 0.0080 (0.0302)	
training:	Epoch: [43][284/408]	Loss 0.0057 (0.0301)	
training:	Epoch: [43][285/408]	Loss 0.0070 (0.0300)	
training:	Epoch: [43][286/408]	Loss 0.0069 (0.0300)	
training:	Epoch: [43][287/408]	Loss 0.0056 (0.0299)	
training:	Epoch: [43][288/408]	Loss 0.0079 (0.0298)	
training:	Epoch: [43][289/408]	Loss 0.0082 (0.0297)	
training:	Epoch: [43][290/408]	Loss 0.0365 (0.0297)	
training:	Epoch: [43][291/408]	Loss 0.0151 (0.0297)	
training:	Epoch: [43][292/408]	Loss 0.0065 (0.0296)	
training:	Epoch: [43][293/408]	Loss 0.0086 (0.0295)	
training:	Epoch: [43][294/408]	Loss 0.0390 (0.0296)	
training:	Epoch: [43][295/408]	Loss 0.0080 (0.0295)	
training:	Epoch: [43][296/408]	Loss 0.0102 (0.0294)	
training:	Epoch: [43][297/408]	Loss 0.0068 (0.0294)	
training:	Epoch: [43][298/408]	Loss 0.0064 (0.0293)	
training:	Epoch: [43][299/408]	Loss 0.0048 (0.0292)	
training:	Epoch: [43][300/408]	Loss 0.0085 (0.0291)	
training:	Epoch: [43][301/408]	Loss 0.2073 (0.0297)	
training:	Epoch: [43][302/408]	Loss 0.0073 (0.0296)	
training:	Epoch: [43][303/408]	Loss 0.0077 (0.0296)	
training:	Epoch: [43][304/408]	Loss 0.0065 (0.0295)	
training:	Epoch: [43][305/408]	Loss 0.0073 (0.0294)	
training:	Epoch: [43][306/408]	Loss 0.0069 (0.0294)	
training:	Epoch: [43][307/408]	Loss 0.0058 (0.0293)	
training:	Epoch: [43][308/408]	Loss 0.0063 (0.0292)	
training:	Epoch: [43][309/408]	Loss 0.0047 (0.0291)	
training:	Epoch: [43][310/408]	Loss 0.0055 (0.0290)	
training:	Epoch: [43][311/408]	Loss 0.0052 (0.0290)	
training:	Epoch: [43][312/408]	Loss 0.0061 (0.0289)	
training:	Epoch: [43][313/408]	Loss 0.0101 (0.0288)	
training:	Epoch: [43][314/408]	Loss 0.0066 (0.0288)	
training:	Epoch: [43][315/408]	Loss 0.0089 (0.0287)	
training:	Epoch: [43][316/408]	Loss 0.0140 (0.0287)	
training:	Epoch: [43][317/408]	Loss 0.0063 (0.0286)	
training:	Epoch: [43][318/408]	Loss 0.0072 (0.0285)	
training:	Epoch: [43][319/408]	Loss 0.2722 (0.0293)	
training:	Epoch: [43][320/408]	Loss 0.0050 (0.0292)	
training:	Epoch: [43][321/408]	Loss 0.0197 (0.0292)	
training:	Epoch: [43][322/408]	Loss 0.0062 (0.0291)	
training:	Epoch: [43][323/408]	Loss 0.0074 (0.0290)	
training:	Epoch: [43][324/408]	Loss 0.0057 (0.0290)	
training:	Epoch: [43][325/408]	Loss 0.0049 (0.0289)	
training:	Epoch: [43][326/408]	Loss 0.0058 (0.0288)	
training:	Epoch: [43][327/408]	Loss 0.0076 (0.0288)	
training:	Epoch: [43][328/408]	Loss 0.3112 (0.0296)	
training:	Epoch: [43][329/408]	Loss 0.0066 (0.0295)	
training:	Epoch: [43][330/408]	Loss 0.0063 (0.0295)	
training:	Epoch: [43][331/408]	Loss 0.0078 (0.0294)	
training:	Epoch: [43][332/408]	Loss 0.0059 (0.0293)	
training:	Epoch: [43][333/408]	Loss 0.1126 (0.0296)	
training:	Epoch: [43][334/408]	Loss 0.0069 (0.0295)	
training:	Epoch: [43][335/408]	Loss 0.0081 (0.0295)	
training:	Epoch: [43][336/408]	Loss 0.0071 (0.0294)	
training:	Epoch: [43][337/408]	Loss 0.2738 (0.0301)	
training:	Epoch: [43][338/408]	Loss 0.0059 (0.0300)	
training:	Epoch: [43][339/408]	Loss 0.2368 (0.0307)	
training:	Epoch: [43][340/408]	Loss 0.0044 (0.0306)	
training:	Epoch: [43][341/408]	Loss 0.0076 (0.0305)	
training:	Epoch: [43][342/408]	Loss 0.0075 (0.0304)	
training:	Epoch: [43][343/408]	Loss 0.0066 (0.0304)	
training:	Epoch: [43][344/408]	Loss 0.0063 (0.0303)	
training:	Epoch: [43][345/408]	Loss 0.2988 (0.0311)	
training:	Epoch: [43][346/408]	Loss 0.0070 (0.0310)	
training:	Epoch: [43][347/408]	Loss 0.0059 (0.0309)	
training:	Epoch: [43][348/408]	Loss 0.0046 (0.0309)	
training:	Epoch: [43][349/408]	Loss 0.0057 (0.0308)	
training:	Epoch: [43][350/408]	Loss 0.0068 (0.0307)	
training:	Epoch: [43][351/408]	Loss 0.0100 (0.0307)	
training:	Epoch: [43][352/408]	Loss 0.3024 (0.0314)	
training:	Epoch: [43][353/408]	Loss 0.0083 (0.0314)	
training:	Epoch: [43][354/408]	Loss 0.0079 (0.0313)	
training:	Epoch: [43][355/408]	Loss 0.0070 (0.0312)	
training:	Epoch: [43][356/408]	Loss 0.0058 (0.0312)	
training:	Epoch: [43][357/408]	Loss 0.0069 (0.0311)	
training:	Epoch: [43][358/408]	Loss 0.0090 (0.0310)	
training:	Epoch: [43][359/408]	Loss 0.0917 (0.0312)	
training:	Epoch: [43][360/408]	Loss 0.0083 (0.0311)	
training:	Epoch: [43][361/408]	Loss 0.0068 (0.0311)	
training:	Epoch: [43][362/408]	Loss 0.2472 (0.0317)	
training:	Epoch: [43][363/408]	Loss 0.0061 (0.0316)	
training:	Epoch: [43][364/408]	Loss 0.0098 (0.0315)	
training:	Epoch: [43][365/408]	Loss 0.0066 (0.0315)	
training:	Epoch: [43][366/408]	Loss 0.0188 (0.0314)	
training:	Epoch: [43][367/408]	Loss 0.0066 (0.0314)	
training:	Epoch: [43][368/408]	Loss 0.0114 (0.0313)	
training:	Epoch: [43][369/408]	Loss 0.0072 (0.0313)	
training:	Epoch: [43][370/408]	Loss 0.0065 (0.0312)	
training:	Epoch: [43][371/408]	Loss 0.0071 (0.0311)	
training:	Epoch: [43][372/408]	Loss 0.0070 (0.0311)	
training:	Epoch: [43][373/408]	Loss 0.2904 (0.0317)	
training:	Epoch: [43][374/408]	Loss 0.0069 (0.0317)	
training:	Epoch: [43][375/408]	Loss 0.0063 (0.0316)	
training:	Epoch: [43][376/408]	Loss 0.0081 (0.0316)	
training:	Epoch: [43][377/408]	Loss 0.0056 (0.0315)	
training:	Epoch: [43][378/408]	Loss 0.3140 (0.0322)	
training:	Epoch: [43][379/408]	Loss 0.0081 (0.0322)	
training:	Epoch: [43][380/408]	Loss 0.0082 (0.0321)	
training:	Epoch: [43][381/408]	Loss 0.0078 (0.0320)	
training:	Epoch: [43][382/408]	Loss 0.2643 (0.0326)	
training:	Epoch: [43][383/408]	Loss 0.0063 (0.0326)	
training:	Epoch: [43][384/408]	Loss 0.0062 (0.0325)	
training:	Epoch: [43][385/408]	Loss 0.0068 (0.0324)	
training:	Epoch: [43][386/408]	Loss 0.0205 (0.0324)	
training:	Epoch: [43][387/408]	Loss 0.0056 (0.0323)	
training:	Epoch: [43][388/408]	Loss 0.0079 (0.0323)	
training:	Epoch: [43][389/408]	Loss 0.0104 (0.0322)	
training:	Epoch: [43][390/408]	Loss 0.0072 (0.0322)	
training:	Epoch: [43][391/408]	Loss 0.0097 (0.0321)	
training:	Epoch: [43][392/408]	Loss 0.2708 (0.0327)	
training:	Epoch: [43][393/408]	Loss 0.0090 (0.0327)	
training:	Epoch: [43][394/408]	Loss 0.0078 (0.0326)	
training:	Epoch: [43][395/408]	Loss 0.0198 (0.0326)	
training:	Epoch: [43][396/408]	Loss 0.0064 (0.0325)	
training:	Epoch: [43][397/408]	Loss 0.0198 (0.0325)	
training:	Epoch: [43][398/408]	Loss 0.0107 (0.0324)	
training:	Epoch: [43][399/408]	Loss 0.0087 (0.0323)	
training:	Epoch: [43][400/408]	Loss 0.0061 (0.0323)	
training:	Epoch: [43][401/408]	Loss 0.0083 (0.0322)	
training:	Epoch: [43][402/408]	Loss 0.2644 (0.0328)	
training:	Epoch: [43][403/408]	Loss 0.0081 (0.0327)	
training:	Epoch: [43][404/408]	Loss 0.2385 (0.0332)	
training:	Epoch: [43][405/408]	Loss 0.0070 (0.0332)	
training:	Epoch: [43][406/408]	Loss 0.0084 (0.0331)	
training:	Epoch: [43][407/408]	Loss 0.0069 (0.0331)	
training:	Epoch: [43][408/408]	Loss 0.0093 (0.0330)	
Training:	 Loss: 0.0329

Training:	 ACC: 0.9945 0.9945 0.9938 0.9952
Validation:	 ACC: 0.7765 0.7769 0.7840 0.7691
Validation:	 Best_BACC: 0.8226 0.8250 0.8772 0.7679
Validation:	 Loss: 0.9874
Pretraining:	Epoch 44/200
----------
training:	Epoch: [44][1/408]	Loss 0.0083 (0.0083)	
training:	Epoch: [44][2/408]	Loss 0.2791 (0.1437)	
training:	Epoch: [44][3/408]	Loss 0.0084 (0.0986)	
training:	Epoch: [44][4/408]	Loss 0.0082 (0.0760)	
training:	Epoch: [44][5/408]	Loss 0.0104 (0.0629)	
training:	Epoch: [44][6/408]	Loss 0.2956 (0.1017)	
training:	Epoch: [44][7/408]	Loss 0.0058 (0.0880)	
training:	Epoch: [44][8/408]	Loss 0.0094 (0.0782)	
training:	Epoch: [44][9/408]	Loss 0.0091 (0.0705)	
training:	Epoch: [44][10/408]	Loss 0.0107 (0.0645)	
training:	Epoch: [44][11/408]	Loss 0.0061 (0.0592)	
training:	Epoch: [44][12/408]	Loss 0.0089 (0.0550)	
training:	Epoch: [44][13/408]	Loss 0.0085 (0.0514)	
training:	Epoch: [44][14/408]	Loss 0.0071 (0.0483)	
training:	Epoch: [44][15/408]	Loss 0.0077 (0.0456)	
training:	Epoch: [44][16/408]	Loss 0.0080 (0.0432)	
training:	Epoch: [44][17/408]	Loss 0.0080 (0.0411)	
training:	Epoch: [44][18/408]	Loss 0.0063 (0.0392)	
training:	Epoch: [44][19/408]	Loss 0.0073 (0.0375)	
training:	Epoch: [44][20/408]	Loss 0.2396 (0.0476)	
training:	Epoch: [44][21/408]	Loss 0.0074 (0.0457)	
training:	Epoch: [44][22/408]	Loss 0.0108 (0.0441)	
training:	Epoch: [44][23/408]	Loss 0.0083 (0.0426)	
training:	Epoch: [44][24/408]	Loss 0.0115 (0.0413)	
training:	Epoch: [44][25/408]	Loss 0.0068 (0.0399)	
training:	Epoch: [44][26/408]	Loss 0.2529 (0.0481)	
training:	Epoch: [44][27/408]	Loss 0.0078 (0.0466)	
training:	Epoch: [44][28/408]	Loss 0.0109 (0.0453)	
training:	Epoch: [44][29/408]	Loss 0.0086 (0.0441)	
training:	Epoch: [44][30/408]	Loss 0.0079 (0.0429)	
training:	Epoch: [44][31/408]	Loss 0.0088 (0.0418)	
training:	Epoch: [44][32/408]	Loss 0.0077 (0.0407)	
training:	Epoch: [44][33/408]	Loss 0.0074 (0.0397)	
training:	Epoch: [44][34/408]	Loss 0.2768 (0.0467)	
training:	Epoch: [44][35/408]	Loss 0.0061 (0.0455)	
training:	Epoch: [44][36/408]	Loss 0.0067 (0.0444)	
training:	Epoch: [44][37/408]	Loss 0.0097 (0.0435)	
training:	Epoch: [44][38/408]	Loss 0.0082 (0.0426)	
training:	Epoch: [44][39/408]	Loss 0.0078 (0.0417)	
training:	Epoch: [44][40/408]	Loss 0.0091 (0.0408)	
training:	Epoch: [44][41/408]	Loss 0.0079 (0.0400)	
training:	Epoch: [44][42/408]	Loss 0.1952 (0.0437)	
training:	Epoch: [44][43/408]	Loss 0.0115 (0.0430)	
training:	Epoch: [44][44/408]	Loss 0.2603 (0.0479)	
training:	Epoch: [44][45/408]	Loss 0.0137 (0.0472)	
training:	Epoch: [44][46/408]	Loss 0.0069 (0.0463)	
training:	Epoch: [44][47/408]	Loss 0.0113 (0.0455)	
training:	Epoch: [44][48/408]	Loss 0.0093 (0.0448)	
training:	Epoch: [44][49/408]	Loss 0.0089 (0.0441)	
training:	Epoch: [44][50/408]	Loss 0.0074 (0.0433)	
training:	Epoch: [44][51/408]	Loss 0.0105 (0.0427)	
training:	Epoch: [44][52/408]	Loss 0.0063 (0.0420)	
training:	Epoch: [44][53/408]	Loss 0.0065 (0.0413)	
training:	Epoch: [44][54/408]	Loss 0.2322 (0.0448)	
training:	Epoch: [44][55/408]	Loss 0.0069 (0.0442)	
training:	Epoch: [44][56/408]	Loss 0.0095 (0.0435)	
training:	Epoch: [44][57/408]	Loss 0.0060 (0.0429)	
training:	Epoch: [44][58/408]	Loss 0.0069 (0.0423)	
training:	Epoch: [44][59/408]	Loss 0.0088 (0.0417)	
training:	Epoch: [44][60/408]	Loss 0.0089 (0.0411)	
training:	Epoch: [44][61/408]	Loss 0.0090 (0.0406)	
training:	Epoch: [44][62/408]	Loss 0.0095 (0.0401)	
training:	Epoch: [44][63/408]	Loss 0.0088 (0.0396)	
training:	Epoch: [44][64/408]	Loss 0.0078 (0.0391)	
training:	Epoch: [44][65/408]	Loss 0.0050 (0.0386)	
training:	Epoch: [44][66/408]	Loss 0.0235 (0.0384)	
training:	Epoch: [44][67/408]	Loss 0.2799 (0.0420)	
training:	Epoch: [44][68/408]	Loss 0.0071 (0.0415)	
training:	Epoch: [44][69/408]	Loss 0.0086 (0.0410)	
training:	Epoch: [44][70/408]	Loss 0.0076 (0.0405)	
training:	Epoch: [44][71/408]	Loss 0.0112 (0.0401)	
training:	Epoch: [44][72/408]	Loss 0.0076 (0.0396)	
training:	Epoch: [44][73/408]	Loss 0.0063 (0.0392)	
training:	Epoch: [44][74/408]	Loss 0.0074 (0.0388)	
training:	Epoch: [44][75/408]	Loss 0.0092 (0.0384)	
training:	Epoch: [44][76/408]	Loss 0.0102 (0.0380)	
training:	Epoch: [44][77/408]	Loss 0.2882 (0.0412)	
training:	Epoch: [44][78/408]	Loss 0.0061 (0.0408)	
training:	Epoch: [44][79/408]	Loss 0.0063 (0.0404)	
training:	Epoch: [44][80/408]	Loss 0.0098 (0.0400)	
training:	Epoch: [44][81/408]	Loss 0.0110 (0.0396)	
training:	Epoch: [44][82/408]	Loss 0.0068 (0.0392)	
training:	Epoch: [44][83/408]	Loss 0.0084 (0.0388)	
training:	Epoch: [44][84/408]	Loss 0.0088 (0.0385)	
training:	Epoch: [44][85/408]	Loss 0.0090 (0.0381)	
training:	Epoch: [44][86/408]	Loss 0.0070 (0.0378)	
training:	Epoch: [44][87/408]	Loss 0.0070 (0.0374)	
training:	Epoch: [44][88/408]	Loss 0.0067 (0.0371)	
training:	Epoch: [44][89/408]	Loss 0.0105 (0.0368)	
training:	Epoch: [44][90/408]	Loss 0.0083 (0.0365)	
training:	Epoch: [44][91/408]	Loss 0.0077 (0.0361)	
training:	Epoch: [44][92/408]	Loss 0.0080 (0.0358)	
training:	Epoch: [44][93/408]	Loss 0.0074 (0.0355)	
training:	Epoch: [44][94/408]	Loss 0.3270 (0.0386)	
training:	Epoch: [44][95/408]	Loss 0.0840 (0.0391)	
training:	Epoch: [44][96/408]	Loss 0.0077 (0.0388)	
training:	Epoch: [44][97/408]	Loss 0.0058 (0.0384)	
training:	Epoch: [44][98/408]	Loss 0.0075 (0.0381)	
training:	Epoch: [44][99/408]	Loss 0.0065 (0.0378)	
training:	Epoch: [44][100/408]	Loss 0.0070 (0.0375)	
training:	Epoch: [44][101/408]	Loss 0.0073 (0.0372)	
training:	Epoch: [44][102/408]	Loss 0.0066 (0.0369)	
training:	Epoch: [44][103/408]	Loss 0.0064 (0.0366)	
training:	Epoch: [44][104/408]	Loss 0.0081 (0.0363)	
training:	Epoch: [44][105/408]	Loss 0.0061 (0.0360)	
training:	Epoch: [44][106/408]	Loss 0.0089 (0.0358)	
training:	Epoch: [44][107/408]	Loss 0.2866 (0.0381)	
training:	Epoch: [44][108/408]	Loss 0.0084 (0.0379)	
training:	Epoch: [44][109/408]	Loss 0.0065 (0.0376)	
training:	Epoch: [44][110/408]	Loss 0.0069 (0.0373)	
training:	Epoch: [44][111/408]	Loss 0.2614 (0.0393)	
training:	Epoch: [44][112/408]	Loss 0.0079 (0.0390)	
training:	Epoch: [44][113/408]	Loss 0.0096 (0.0388)	
training:	Epoch: [44][114/408]	Loss 0.0071 (0.0385)	
training:	Epoch: [44][115/408]	Loss 0.0068 (0.0382)	
training:	Epoch: [44][116/408]	Loss 0.0089 (0.0380)	
training:	Epoch: [44][117/408]	Loss 0.0085 (0.0377)	
training:	Epoch: [44][118/408]	Loss 0.0074 (0.0375)	
training:	Epoch: [44][119/408]	Loss 0.0099 (0.0372)	
training:	Epoch: [44][120/408]	Loss 0.0068 (0.0370)	
training:	Epoch: [44][121/408]	Loss 0.1166 (0.0376)	
training:	Epoch: [44][122/408]	Loss 0.0109 (0.0374)	
training:	Epoch: [44][123/408]	Loss 0.2358 (0.0390)	
training:	Epoch: [44][124/408]	Loss 0.0076 (0.0388)	
training:	Epoch: [44][125/408]	Loss 0.0103 (0.0385)	
training:	Epoch: [44][126/408]	Loss 0.0068 (0.0383)	
training:	Epoch: [44][127/408]	Loss 0.0073 (0.0380)	
training:	Epoch: [44][128/408]	Loss 0.0450 (0.0381)	
training:	Epoch: [44][129/408]	Loss 0.0060 (0.0378)	
training:	Epoch: [44][130/408]	Loss 0.0063 (0.0376)	
training:	Epoch: [44][131/408]	Loss 0.0067 (0.0374)	
training:	Epoch: [44][132/408]	Loss 0.0070 (0.0371)	
training:	Epoch: [44][133/408]	Loss 0.0084 (0.0369)	
training:	Epoch: [44][134/408]	Loss 0.0052 (0.0367)	
training:	Epoch: [44][135/408]	Loss 0.0079 (0.0365)	
training:	Epoch: [44][136/408]	Loss 0.0066 (0.0363)	
training:	Epoch: [44][137/408]	Loss 0.0088 (0.0361)	
training:	Epoch: [44][138/408]	Loss 0.0075 (0.0358)	
training:	Epoch: [44][139/408]	Loss 0.0066 (0.0356)	
training:	Epoch: [44][140/408]	Loss 0.0076 (0.0354)	
training:	Epoch: [44][141/408]	Loss 0.0075 (0.0352)	
training:	Epoch: [44][142/408]	Loss 0.2637 (0.0368)	
training:	Epoch: [44][143/408]	Loss 0.0097 (0.0367)	
training:	Epoch: [44][144/408]	Loss 0.0063 (0.0364)	
training:	Epoch: [44][145/408]	Loss 0.0094 (0.0363)	
training:	Epoch: [44][146/408]	Loss 0.0053 (0.0360)	
training:	Epoch: [44][147/408]	Loss 0.0072 (0.0359)	
training:	Epoch: [44][148/408]	Loss 0.0062 (0.0357)	
training:	Epoch: [44][149/408]	Loss 0.0108 (0.0355)	
training:	Epoch: [44][150/408]	Loss 0.0094 (0.0353)	
training:	Epoch: [44][151/408]	Loss 0.0082 (0.0351)	
training:	Epoch: [44][152/408]	Loss 0.0070 (0.0349)	
training:	Epoch: [44][153/408]	Loss 0.0046 (0.0347)	
training:	Epoch: [44][154/408]	Loss 0.2130 (0.0359)	
training:	Epoch: [44][155/408]	Loss 0.0058 (0.0357)	
training:	Epoch: [44][156/408]	Loss 0.0073 (0.0355)	
training:	Epoch: [44][157/408]	Loss 0.0070 (0.0353)	
training:	Epoch: [44][158/408]	Loss 0.0053 (0.0352)	
training:	Epoch: [44][159/408]	Loss 0.0062 (0.0350)	
training:	Epoch: [44][160/408]	Loss 0.0082 (0.0348)	
training:	Epoch: [44][161/408]	Loss 0.0058 (0.0346)	
training:	Epoch: [44][162/408]	Loss 0.0135 (0.0345)	
training:	Epoch: [44][163/408]	Loss 0.3059 (0.0362)	
training:	Epoch: [44][164/408]	Loss 0.0069 (0.0360)	
training:	Epoch: [44][165/408]	Loss 0.0150 (0.0359)	
training:	Epoch: [44][166/408]	Loss 0.2250 (0.0370)	
training:	Epoch: [44][167/408]	Loss 0.0082 (0.0368)	
training:	Epoch: [44][168/408]	Loss 0.0076 (0.0366)	
training:	Epoch: [44][169/408]	Loss 0.0092 (0.0365)	
training:	Epoch: [44][170/408]	Loss 0.0074 (0.0363)	
training:	Epoch: [44][171/408]	Loss 0.0175 (0.0362)	
training:	Epoch: [44][172/408]	Loss 0.0066 (0.0360)	
training:	Epoch: [44][173/408]	Loss 0.0075 (0.0359)	
training:	Epoch: [44][174/408]	Loss 0.0080 (0.0357)	
training:	Epoch: [44][175/408]	Loss 0.0073 (0.0355)	
training:	Epoch: [44][176/408]	Loss 0.0089 (0.0354)	
training:	Epoch: [44][177/408]	Loss 0.2638 (0.0367)	
training:	Epoch: [44][178/408]	Loss 0.0072 (0.0365)	
training:	Epoch: [44][179/408]	Loss 0.0078 (0.0364)	
training:	Epoch: [44][180/408]	Loss 0.0074 (0.0362)	
training:	Epoch: [44][181/408]	Loss 0.0210 (0.0361)	
training:	Epoch: [44][182/408]	Loss 0.0078 (0.0360)	
training:	Epoch: [44][183/408]	Loss 0.0107 (0.0358)	
training:	Epoch: [44][184/408]	Loss 0.0083 (0.0357)	
training:	Epoch: [44][185/408]	Loss 0.2302 (0.0367)	
training:	Epoch: [44][186/408]	Loss 0.0076 (0.0366)	
training:	Epoch: [44][187/408]	Loss 0.0058 (0.0364)	
training:	Epoch: [44][188/408]	Loss 0.2372 (0.0375)	
training:	Epoch: [44][189/408]	Loss 0.0095 (0.0373)	
training:	Epoch: [44][190/408]	Loss 0.0086 (0.0372)	
training:	Epoch: [44][191/408]	Loss 0.0076 (0.0370)	
training:	Epoch: [44][192/408]	Loss 0.0076 (0.0369)	
training:	Epoch: [44][193/408]	Loss 0.0082 (0.0367)	
training:	Epoch: [44][194/408]	Loss 0.0077 (0.0366)	
training:	Epoch: [44][195/408]	Loss 0.0088 (0.0364)	
training:	Epoch: [44][196/408]	Loss 0.0064 (0.0363)	
training:	Epoch: [44][197/408]	Loss 0.0076 (0.0361)	
training:	Epoch: [44][198/408]	Loss 0.0069 (0.0360)	
training:	Epoch: [44][199/408]	Loss 0.0060 (0.0358)	
training:	Epoch: [44][200/408]	Loss 0.0090 (0.0357)	
training:	Epoch: [44][201/408]	Loss 0.0083 (0.0356)	
training:	Epoch: [44][202/408]	Loss 0.0073 (0.0354)	
training:	Epoch: [44][203/408]	Loss 0.0161 (0.0353)	
training:	Epoch: [44][204/408]	Loss 0.0071 (0.0352)	
training:	Epoch: [44][205/408]	Loss 0.0080 (0.0351)	
training:	Epoch: [44][206/408]	Loss 0.0091 (0.0349)	
training:	Epoch: [44][207/408]	Loss 0.0071 (0.0348)	
training:	Epoch: [44][208/408]	Loss 0.0062 (0.0347)	
training:	Epoch: [44][209/408]	Loss 0.0103 (0.0345)	
training:	Epoch: [44][210/408]	Loss 0.0085 (0.0344)	
training:	Epoch: [44][211/408]	Loss 0.0117 (0.0343)	
training:	Epoch: [44][212/408]	Loss 0.0085 (0.0342)	
training:	Epoch: [44][213/408]	Loss 0.0067 (0.0341)	
training:	Epoch: [44][214/408]	Loss 0.0077 (0.0339)	
training:	Epoch: [44][215/408]	Loss 0.0085 (0.0338)	
training:	Epoch: [44][216/408]	Loss 0.0060 (0.0337)	
training:	Epoch: [44][217/408]	Loss 0.0062 (0.0336)	
training:	Epoch: [44][218/408]	Loss 0.0076 (0.0334)	
training:	Epoch: [44][219/408]	Loss 0.0063 (0.0333)	
training:	Epoch: [44][220/408]	Loss 0.0076 (0.0332)	
training:	Epoch: [44][221/408]	Loss 0.0085 (0.0331)	
training:	Epoch: [44][222/408]	Loss 0.0044 (0.0330)	
training:	Epoch: [44][223/408]	Loss 0.0077 (0.0328)	
training:	Epoch: [44][224/408]	Loss 0.0067 (0.0327)	
training:	Epoch: [44][225/408]	Loss 0.0063 (0.0326)	
training:	Epoch: [44][226/408]	Loss 0.0092 (0.0325)	
training:	Epoch: [44][227/408]	Loss 0.0065 (0.0324)	
training:	Epoch: [44][228/408]	Loss 0.0053 (0.0323)	
training:	Epoch: [44][229/408]	Loss 0.0057 (0.0322)	
training:	Epoch: [44][230/408]	Loss 0.2734 (0.0332)	
training:	Epoch: [44][231/408]	Loss 0.0077 (0.0331)	
training:	Epoch: [44][232/408]	Loss 0.0120 (0.0330)	
training:	Epoch: [44][233/408]	Loss 0.0047 (0.0329)	
training:	Epoch: [44][234/408]	Loss 0.0070 (0.0328)	
training:	Epoch: [44][235/408]	Loss 0.0120 (0.0327)	
training:	Epoch: [44][236/408]	Loss 0.0053 (0.0326)	
training:	Epoch: [44][237/408]	Loss 0.0065 (0.0325)	
training:	Epoch: [44][238/408]	Loss 0.0077 (0.0324)	
training:	Epoch: [44][239/408]	Loss 0.0079 (0.0323)	
training:	Epoch: [44][240/408]	Loss 0.0070 (0.0321)	
training:	Epoch: [44][241/408]	Loss 0.0118 (0.0321)	
training:	Epoch: [44][242/408]	Loss 0.0090 (0.0320)	
training:	Epoch: [44][243/408]	Loss 0.0083 (0.0319)	
training:	Epoch: [44][244/408]	Loss 0.0079 (0.0318)	
training:	Epoch: [44][245/408]	Loss 0.0098 (0.0317)	
training:	Epoch: [44][246/408]	Loss 0.2704 (0.0327)	
training:	Epoch: [44][247/408]	Loss 0.2429 (0.0335)	
training:	Epoch: [44][248/408]	Loss 0.0049 (0.0334)	
training:	Epoch: [44][249/408]	Loss 0.0081 (0.0333)	
training:	Epoch: [44][250/408]	Loss 0.0089 (0.0332)	
training:	Epoch: [44][251/408]	Loss 0.0048 (0.0331)	
training:	Epoch: [44][252/408]	Loss 0.0074 (0.0330)	
training:	Epoch: [44][253/408]	Loss 0.0090 (0.0329)	
training:	Epoch: [44][254/408]	Loss 0.0079 (0.0328)	
training:	Epoch: [44][255/408]	Loss 0.0053 (0.0327)	
training:	Epoch: [44][256/408]	Loss 0.2795 (0.0336)	
training:	Epoch: [44][257/408]	Loss 0.0062 (0.0335)	
training:	Epoch: [44][258/408]	Loss 0.0076 (0.0334)	
training:	Epoch: [44][259/408]	Loss 0.0095 (0.0333)	
training:	Epoch: [44][260/408]	Loss 0.0093 (0.0332)	
training:	Epoch: [44][261/408]	Loss 0.0106 (0.0332)	
training:	Epoch: [44][262/408]	Loss 0.0069 (0.0331)	
training:	Epoch: [44][263/408]	Loss 0.0064 (0.0330)	
training:	Epoch: [44][264/408]	Loss 0.0065 (0.0329)	
training:	Epoch: [44][265/408]	Loss 0.0056 (0.0328)	
training:	Epoch: [44][266/408]	Loss 0.0052 (0.0326)	
training:	Epoch: [44][267/408]	Loss 0.0075 (0.0326)	
training:	Epoch: [44][268/408]	Loss 0.0064 (0.0325)	
training:	Epoch: [44][269/408]	Loss 0.3103 (0.0335)	
training:	Epoch: [44][270/408]	Loss 0.0080 (0.0334)	
training:	Epoch: [44][271/408]	Loss 0.0078 (0.0333)	
training:	Epoch: [44][272/408]	Loss 0.0084 (0.0332)	
training:	Epoch: [44][273/408]	Loss 0.0081 (0.0331)	
training:	Epoch: [44][274/408]	Loss 0.0054 (0.0330)	
training:	Epoch: [44][275/408]	Loss 0.0057 (0.0329)	
training:	Epoch: [44][276/408]	Loss 0.0067 (0.0328)	
training:	Epoch: [44][277/408]	Loss 0.0040 (0.0327)	
training:	Epoch: [44][278/408]	Loss 0.0057 (0.0326)	
training:	Epoch: [44][279/408]	Loss 0.0082 (0.0325)	
training:	Epoch: [44][280/408]	Loss 0.0052 (0.0324)	
training:	Epoch: [44][281/408]	Loss 0.0060 (0.0323)	
training:	Epoch: [44][282/408]	Loss 0.0050 (0.0322)	
training:	Epoch: [44][283/408]	Loss 0.0063 (0.0322)	
training:	Epoch: [44][284/408]	Loss 0.0102 (0.0321)	
training:	Epoch: [44][285/408]	Loss 0.2855 (0.0330)	
training:	Epoch: [44][286/408]	Loss 0.0083 (0.0329)	
training:	Epoch: [44][287/408]	Loss 0.2399 (0.0336)	
training:	Epoch: [44][288/408]	Loss 0.0066 (0.0335)	
training:	Epoch: [44][289/408]	Loss 0.0063 (0.0334)	
training:	Epoch: [44][290/408]	Loss 0.0076 (0.0333)	
training:	Epoch: [44][291/408]	Loss 0.0076 (0.0332)	
training:	Epoch: [44][292/408]	Loss 0.0099 (0.0332)	
training:	Epoch: [44][293/408]	Loss 0.2435 (0.0339)	
training:	Epoch: [44][294/408]	Loss 0.0087 (0.0338)	
training:	Epoch: [44][295/408]	Loss 0.0093 (0.0337)	
training:	Epoch: [44][296/408]	Loss 0.0058 (0.0336)	
training:	Epoch: [44][297/408]	Loss 0.0070 (0.0335)	
training:	Epoch: [44][298/408]	Loss 0.0074 (0.0334)	
training:	Epoch: [44][299/408]	Loss 0.0066 (0.0333)	
training:	Epoch: [44][300/408]	Loss 0.0073 (0.0333)	
training:	Epoch: [44][301/408]	Loss 0.0067 (0.0332)	
training:	Epoch: [44][302/408]	Loss 0.0101 (0.0331)	
training:	Epoch: [44][303/408]	Loss 0.0096 (0.0330)	
training:	Epoch: [44][304/408]	Loss 0.0060 (0.0329)	
training:	Epoch: [44][305/408]	Loss 0.0067 (0.0328)	
training:	Epoch: [44][306/408]	Loss 0.0082 (0.0328)	
training:	Epoch: [44][307/408]	Loss 0.0067 (0.0327)	
training:	Epoch: [44][308/408]	Loss 0.0056 (0.0326)	
training:	Epoch: [44][309/408]	Loss 0.0068 (0.0325)	
training:	Epoch: [44][310/408]	Loss 0.0072 (0.0324)	
training:	Epoch: [44][311/408]	Loss 0.0104 (0.0323)	
training:	Epoch: [44][312/408]	Loss 0.0068 (0.0323)	
training:	Epoch: [44][313/408]	Loss 0.0117 (0.0322)	
training:	Epoch: [44][314/408]	Loss 0.0078 (0.0321)	
training:	Epoch: [44][315/408]	Loss 0.3100 (0.0330)	
training:	Epoch: [44][316/408]	Loss 0.0092 (0.0329)	
training:	Epoch: [44][317/408]	Loss 0.0056 (0.0328)	
training:	Epoch: [44][318/408]	Loss 0.0055 (0.0328)	
training:	Epoch: [44][319/408]	Loss 0.0060 (0.0327)	
training:	Epoch: [44][320/408]	Loss 0.0077 (0.0326)	
training:	Epoch: [44][321/408]	Loss 0.0068 (0.0325)	
training:	Epoch: [44][322/408]	Loss 0.0064 (0.0324)	
training:	Epoch: [44][323/408]	Loss 0.0054 (0.0324)	
training:	Epoch: [44][324/408]	Loss 0.0089 (0.0323)	
training:	Epoch: [44][325/408]	Loss 0.0069 (0.0322)	
training:	Epoch: [44][326/408]	Loss 0.0056 (0.0321)	
training:	Epoch: [44][327/408]	Loss 0.0072 (0.0320)	
training:	Epoch: [44][328/408]	Loss 0.0074 (0.0320)	
training:	Epoch: [44][329/408]	Loss 0.0060 (0.0319)	
training:	Epoch: [44][330/408]	Loss 0.0066 (0.0318)	
training:	Epoch: [44][331/408]	Loss 0.0062 (0.0317)	
training:	Epoch: [44][332/408]	Loss 0.0074 (0.0317)	
training:	Epoch: [44][333/408]	Loss 0.0052 (0.0316)	
training:	Epoch: [44][334/408]	Loss 0.0055 (0.0315)	
training:	Epoch: [44][335/408]	Loss 0.0116 (0.0314)	
training:	Epoch: [44][336/408]	Loss 0.0055 (0.0314)	
training:	Epoch: [44][337/408]	Loss 0.0065 (0.0313)	
training:	Epoch: [44][338/408]	Loss 0.0067 (0.0312)	
training:	Epoch: [44][339/408]	Loss 0.0068 (0.0311)	
training:	Epoch: [44][340/408]	Loss 0.0050 (0.0311)	
training:	Epoch: [44][341/408]	Loss 0.0068 (0.0310)	
training:	Epoch: [44][342/408]	Loss 0.0055 (0.0309)	
training:	Epoch: [44][343/408]	Loss 0.0056 (0.0309)	
training:	Epoch: [44][344/408]	Loss 0.0066 (0.0308)	
training:	Epoch: [44][345/408]	Loss 0.0051 (0.0307)	
training:	Epoch: [44][346/408]	Loss 0.0060 (0.0306)	
training:	Epoch: [44][347/408]	Loss 0.0075 (0.0306)	
training:	Epoch: [44][348/408]	Loss 0.0058 (0.0305)	
training:	Epoch: [44][349/408]	Loss 0.0088 (0.0304)	
training:	Epoch: [44][350/408]	Loss 0.0155 (0.0304)	
training:	Epoch: [44][351/408]	Loss 0.0054 (0.0303)	
training:	Epoch: [44][352/408]	Loss 0.0055 (0.0303)	
training:	Epoch: [44][353/408]	Loss 0.0068 (0.0302)	
training:	Epoch: [44][354/408]	Loss 0.0063 (0.0301)	
training:	Epoch: [44][355/408]	Loss 0.3096 (0.0309)	
training:	Epoch: [44][356/408]	Loss 0.0052 (0.0308)	
training:	Epoch: [44][357/408]	Loss 0.0064 (0.0308)	
training:	Epoch: [44][358/408]	Loss 0.0062 (0.0307)	
training:	Epoch: [44][359/408]	Loss 0.0063 (0.0306)	
training:	Epoch: [44][360/408]	Loss 0.0058 (0.0306)	
training:	Epoch: [44][361/408]	Loss 0.0066 (0.0305)	
training:	Epoch: [44][362/408]	Loss 0.0427 (0.0305)	
training:	Epoch: [44][363/408]	Loss 0.0083 (0.0305)	
training:	Epoch: [44][364/408]	Loss 0.0054 (0.0304)	
training:	Epoch: [44][365/408]	Loss 0.0072 (0.0303)	
training:	Epoch: [44][366/408]	Loss 0.0054 (0.0303)	
training:	Epoch: [44][367/408]	Loss 0.0063 (0.0302)	
training:	Epoch: [44][368/408]	Loss 0.5020 (0.0315)	
training:	Epoch: [44][369/408]	Loss 0.0087 (0.0314)	
training:	Epoch: [44][370/408]	Loss 0.0071 (0.0314)	
training:	Epoch: [44][371/408]	Loss 0.0054 (0.0313)	
training:	Epoch: [44][372/408]	Loss 0.2430 (0.0319)	
training:	Epoch: [44][373/408]	Loss 0.0065 (0.0318)	
training:	Epoch: [44][374/408]	Loss 0.0058 (0.0317)	
training:	Epoch: [44][375/408]	Loss 0.0055 (0.0316)	
training:	Epoch: [44][376/408]	Loss 0.0125 (0.0316)	
training:	Epoch: [44][377/408]	Loss 0.0087 (0.0315)	
training:	Epoch: [44][378/408]	Loss 0.0058 (0.0315)	
training:	Epoch: [44][379/408]	Loss 0.0054 (0.0314)	
training:	Epoch: [44][380/408]	Loss 0.0097 (0.0313)	
training:	Epoch: [44][381/408]	Loss 0.0058 (0.0313)	
training:	Epoch: [44][382/408]	Loss 0.0073 (0.0312)	
training:	Epoch: [44][383/408]	Loss 0.0044 (0.0311)	
training:	Epoch: [44][384/408]	Loss 0.0074 (0.0311)	
training:	Epoch: [44][385/408]	Loss 0.0052 (0.0310)	
training:	Epoch: [44][386/408]	Loss 0.0094 (0.0310)	
training:	Epoch: [44][387/408]	Loss 0.0078 (0.0309)	
training:	Epoch: [44][388/408]	Loss 0.0080 (0.0308)	
training:	Epoch: [44][389/408]	Loss 0.0057 (0.0308)	
training:	Epoch: [44][390/408]	Loss 0.0080 (0.0307)	
training:	Epoch: [44][391/408]	Loss 0.0056 (0.0306)	
training:	Epoch: [44][392/408]	Loss 0.0083 (0.0306)	
training:	Epoch: [44][393/408]	Loss 0.0056 (0.0305)	
training:	Epoch: [44][394/408]	Loss 0.0067 (0.0305)	
training:	Epoch: [44][395/408]	Loss 0.0055 (0.0304)	
training:	Epoch: [44][396/408]	Loss 0.2289 (0.0309)	
training:	Epoch: [44][397/408]	Loss 0.0069 (0.0308)	
training:	Epoch: [44][398/408]	Loss 0.0079 (0.0308)	
training:	Epoch: [44][399/408]	Loss 0.0045 (0.0307)	
training:	Epoch: [44][400/408]	Loss 0.0059 (0.0307)	
training:	Epoch: [44][401/408]	Loss 0.0062 (0.0306)	
training:	Epoch: [44][402/408]	Loss 0.0056 (0.0305)	
training:	Epoch: [44][403/408]	Loss 0.0051 (0.0305)	
training:	Epoch: [44][404/408]	Loss 0.0066 (0.0304)	
training:	Epoch: [44][405/408]	Loss 0.0067 (0.0304)	
training:	Epoch: [44][406/408]	Loss 0.0086 (0.0303)	
training:	Epoch: [44][407/408]	Loss 0.0072 (0.0302)	
training:	Epoch: [44][408/408]	Loss 0.0067 (0.0302)	
Training:	 Loss: 0.0301

Training:	 ACC: 0.9945 0.9945 0.9938 0.9952
Validation:	 ACC: 0.7824 0.7822 0.7789 0.7859
Validation:	 Best_BACC: 0.8226 0.8250 0.8772 0.7679
Validation:	 Loss: 1.0221
Pretraining:	Epoch 45/200
----------
training:	Epoch: [45][1/408]	Loss 0.0058 (0.0058)	
training:	Epoch: [45][2/408]	Loss 0.0069 (0.0064)	
training:	Epoch: [45][3/408]	Loss 0.0065 (0.0064)	
training:	Epoch: [45][4/408]	Loss 0.0066 (0.0065)	
training:	Epoch: [45][5/408]	Loss 0.0043 (0.0060)	
training:	Epoch: [45][6/408]	Loss 0.0071 (0.0062)	
training:	Epoch: [45][7/408]	Loss 0.2311 (0.0383)	
training:	Epoch: [45][8/408]	Loss 0.0056 (0.0342)	
training:	Epoch: [45][9/408]	Loss 0.0080 (0.0313)	
training:	Epoch: [45][10/408]	Loss 0.0066 (0.0289)	
training:	Epoch: [45][11/408]	Loss 0.0049 (0.0267)	
training:	Epoch: [45][12/408]	Loss 0.0068 (0.0250)	
training:	Epoch: [45][13/408]	Loss 0.0059 (0.0236)	
training:	Epoch: [45][14/408]	Loss 0.0051 (0.0222)	
training:	Epoch: [45][15/408]	Loss 0.0051 (0.0211)	
training:	Epoch: [45][16/408]	Loss 0.0055 (0.0201)	
training:	Epoch: [45][17/408]	Loss 0.0055 (0.0193)	
training:	Epoch: [45][18/408]	Loss 0.0053 (0.0185)	
training:	Epoch: [45][19/408]	Loss 0.0069 (0.0179)	
training:	Epoch: [45][20/408]	Loss 0.0061 (0.0173)	
training:	Epoch: [45][21/408]	Loss 0.0076 (0.0168)	
training:	Epoch: [45][22/408]	Loss 0.0076 (0.0164)	
training:	Epoch: [45][23/408]	Loss 0.0049 (0.0159)	
training:	Epoch: [45][24/408]	Loss 0.0051 (0.0155)	
training:	Epoch: [45][25/408]	Loss 0.0060 (0.0151)	
training:	Epoch: [45][26/408]	Loss 0.0055 (0.0147)	
training:	Epoch: [45][27/408]	Loss 0.0065 (0.0144)	
training:	Epoch: [45][28/408]	Loss 0.0066 (0.0141)	
training:	Epoch: [45][29/408]	Loss 0.0054 (0.0138)	
training:	Epoch: [45][30/408]	Loss 0.0067 (0.0136)	
training:	Epoch: [45][31/408]	Loss 0.0060 (0.0133)	
training:	Epoch: [45][32/408]	Loss 0.0050 (0.0131)	
training:	Epoch: [45][33/408]	Loss 0.0058 (0.0129)	
training:	Epoch: [45][34/408]	Loss 0.0077 (0.0127)	
training:	Epoch: [45][35/408]	Loss 0.0068 (0.0125)	
training:	Epoch: [45][36/408]	Loss 0.0057 (0.0123)	
training:	Epoch: [45][37/408]	Loss 0.0070 (0.0122)	
training:	Epoch: [45][38/408]	Loss 0.0054 (0.0120)	
training:	Epoch: [45][39/408]	Loss 0.0070 (0.0119)	
training:	Epoch: [45][40/408]	Loss 0.0061 (0.0117)	
training:	Epoch: [45][41/408]	Loss 0.0063 (0.0116)	
training:	Epoch: [45][42/408]	Loss 0.0047 (0.0114)	
training:	Epoch: [45][43/408]	Loss 0.0045 (0.0113)	
training:	Epoch: [45][44/408]	Loss 0.0264 (0.0116)	
training:	Epoch: [45][45/408]	Loss 0.0047 (0.0115)	
training:	Epoch: [45][46/408]	Loss 0.0071 (0.0114)	
training:	Epoch: [45][47/408]	Loss 0.0069 (0.0113)	
training:	Epoch: [45][48/408]	Loss 0.0045 (0.0111)	
training:	Epoch: [45][49/408]	Loss 0.0056 (0.0110)	
training:	Epoch: [45][50/408]	Loss 0.0050 (0.0109)	
training:	Epoch: [45][51/408]	Loss 0.0059 (0.0108)	
training:	Epoch: [45][52/408]	Loss 0.0057 (0.0107)	
training:	Epoch: [45][53/408]	Loss 0.3188 (0.0165)	
training:	Epoch: [45][54/408]	Loss 0.0069 (0.0163)	
training:	Epoch: [45][55/408]	Loss 0.0057 (0.0162)	
training:	Epoch: [45][56/408]	Loss 0.0074 (0.0160)	
training:	Epoch: [45][57/408]	Loss 0.0054 (0.0158)	
training:	Epoch: [45][58/408]	Loss 0.0044 (0.0156)	
training:	Epoch: [45][59/408]	Loss 0.0056 (0.0154)	
training:	Epoch: [45][60/408]	Loss 0.0054 (0.0153)	
training:	Epoch: [45][61/408]	Loss 0.0045 (0.0151)	
training:	Epoch: [45][62/408]	Loss 0.0051 (0.0149)	
training:	Epoch: [45][63/408]	Loss 0.0066 (0.0148)	
training:	Epoch: [45][64/408]	Loss 0.2526 (0.0185)	
training:	Epoch: [45][65/408]	Loss 0.0052 (0.0183)	
training:	Epoch: [45][66/408]	Loss 0.0048 (0.0181)	
training:	Epoch: [45][67/408]	Loss 0.0067 (0.0179)	
training:	Epoch: [45][68/408]	Loss 0.0051 (0.0178)	
training:	Epoch: [45][69/408]	Loss 0.0058 (0.0176)	
training:	Epoch: [45][70/408]	Loss 0.0078 (0.0174)	
training:	Epoch: [45][71/408]	Loss 0.5639 (0.0251)	
training:	Epoch: [45][72/408]	Loss 0.0065 (0.0249)	
training:	Epoch: [45][73/408]	Loss 0.0058 (0.0246)	
training:	Epoch: [45][74/408]	Loss 0.0112 (0.0244)	
training:	Epoch: [45][75/408]	Loss 0.0042 (0.0242)	
training:	Epoch: [45][76/408]	Loss 0.0090 (0.0240)	
training:	Epoch: [45][77/408]	Loss 0.0053 (0.0237)	
training:	Epoch: [45][78/408]	Loss 0.0050 (0.0235)	
training:	Epoch: [45][79/408]	Loss 0.0060 (0.0233)	
training:	Epoch: [45][80/408]	Loss 0.0071 (0.0231)	
training:	Epoch: [45][81/408]	Loss 0.0065 (0.0229)	
training:	Epoch: [45][82/408]	Loss 0.2524 (0.0257)	
training:	Epoch: [45][83/408]	Loss 0.0055 (0.0254)	
training:	Epoch: [45][84/408]	Loss 0.0059 (0.0252)	
training:	Epoch: [45][85/408]	Loss 0.0054 (0.0249)	
training:	Epoch: [45][86/408]	Loss 0.0070 (0.0247)	
training:	Epoch: [45][87/408]	Loss 0.0052 (0.0245)	
training:	Epoch: [45][88/408]	Loss 0.0049 (0.0243)	
training:	Epoch: [45][89/408]	Loss 0.2429 (0.0267)	
training:	Epoch: [45][90/408]	Loss 0.0052 (0.0265)	
training:	Epoch: [45][91/408]	Loss 0.0060 (0.0263)	
training:	Epoch: [45][92/408]	Loss 0.0055 (0.0261)	
training:	Epoch: [45][93/408]	Loss 0.3104 (0.0291)	
training:	Epoch: [45][94/408]	Loss 0.0099 (0.0289)	
training:	Epoch: [45][95/408]	Loss 0.0044 (0.0287)	
training:	Epoch: [45][96/408]	Loss 0.0062 (0.0284)	
training:	Epoch: [45][97/408]	Loss 0.0073 (0.0282)	
training:	Epoch: [45][98/408]	Loss 0.0095 (0.0280)	
training:	Epoch: [45][99/408]	Loss 0.0057 (0.0278)	
training:	Epoch: [45][100/408]	Loss 0.0075 (0.0276)	
training:	Epoch: [45][101/408]	Loss 0.0043 (0.0274)	
training:	Epoch: [45][102/408]	Loss 0.0061 (0.0271)	
training:	Epoch: [45][103/408]	Loss 0.0067 (0.0269)	
training:	Epoch: [45][104/408]	Loss 0.0073 (0.0268)	
training:	Epoch: [45][105/408]	Loss 0.0077 (0.0266)	
training:	Epoch: [45][106/408]	Loss 0.0077 (0.0264)	
training:	Epoch: [45][107/408]	Loss 0.0058 (0.0262)	
training:	Epoch: [45][108/408]	Loss 0.0060 (0.0260)	
training:	Epoch: [45][109/408]	Loss 0.0056 (0.0258)	
training:	Epoch: [45][110/408]	Loss 0.0060 (0.0256)	
training:	Epoch: [45][111/408]	Loss 0.0050 (0.0255)	
training:	Epoch: [45][112/408]	Loss 0.0068 (0.0253)	
training:	Epoch: [45][113/408]	Loss 0.0076 (0.0251)	
training:	Epoch: [45][114/408]	Loss 0.2861 (0.0274)	
training:	Epoch: [45][115/408]	Loss 0.0076 (0.0273)	
training:	Epoch: [45][116/408]	Loss 0.0065 (0.0271)	
training:	Epoch: [45][117/408]	Loss 0.3454 (0.0298)	
training:	Epoch: [45][118/408]	Loss 0.0065 (0.0296)	
training:	Epoch: [45][119/408]	Loss 0.0068 (0.0294)	
training:	Epoch: [45][120/408]	Loss 0.2872 (0.0316)	
training:	Epoch: [45][121/408]	Loss 0.0090 (0.0314)	
training:	Epoch: [45][122/408]	Loss 0.0061 (0.0312)	
training:	Epoch: [45][123/408]	Loss 0.2497 (0.0329)	
training:	Epoch: [45][124/408]	Loss 0.0064 (0.0327)	
training:	Epoch: [45][125/408]	Loss 0.0060 (0.0325)	
training:	Epoch: [45][126/408]	Loss 0.0050 (0.0323)	
training:	Epoch: [45][127/408]	Loss 0.0055 (0.0321)	
training:	Epoch: [45][128/408]	Loss 0.2509 (0.0338)	
training:	Epoch: [45][129/408]	Loss 0.0056 (0.0336)	
training:	Epoch: [45][130/408]	Loss 0.0080 (0.0334)	
training:	Epoch: [45][131/408]	Loss 0.0070 (0.0332)	
training:	Epoch: [45][132/408]	Loss 0.0082 (0.0330)	
training:	Epoch: [45][133/408]	Loss 0.0083 (0.0328)	
training:	Epoch: [45][134/408]	Loss 0.0062 (0.0326)	
training:	Epoch: [45][135/408]	Loss 0.0086 (0.0324)	
training:	Epoch: [45][136/408]	Loss 0.0051 (0.0322)	
training:	Epoch: [45][137/408]	Loss 0.3021 (0.0342)	
training:	Epoch: [45][138/408]	Loss 0.0075 (0.0340)	
training:	Epoch: [45][139/408]	Loss 0.0054 (0.0338)	
training:	Epoch: [45][140/408]	Loss 0.0082 (0.0336)	
training:	Epoch: [45][141/408]	Loss 0.0071 (0.0334)	
training:	Epoch: [45][142/408]	Loss 0.0096 (0.0333)	
training:	Epoch: [45][143/408]	Loss 0.2211 (0.0346)	
training:	Epoch: [45][144/408]	Loss 0.0050 (0.0344)	
training:	Epoch: [45][145/408]	Loss 0.0067 (0.0342)	
training:	Epoch: [45][146/408]	Loss 0.0113 (0.0340)	
training:	Epoch: [45][147/408]	Loss 0.0058 (0.0338)	
training:	Epoch: [45][148/408]	Loss 0.0096 (0.0337)	
training:	Epoch: [45][149/408]	Loss 0.0070 (0.0335)	
training:	Epoch: [45][150/408]	Loss 0.0059 (0.0333)	
training:	Epoch: [45][151/408]	Loss 0.0055 (0.0331)	
training:	Epoch: [45][152/408]	Loss 0.0051 (0.0329)	
training:	Epoch: [45][153/408]	Loss 0.0082 (0.0328)	
training:	Epoch: [45][154/408]	Loss 0.0058 (0.0326)	
training:	Epoch: [45][155/408]	Loss 0.2125 (0.0338)	
training:	Epoch: [45][156/408]	Loss 0.0088 (0.0336)	
training:	Epoch: [45][157/408]	Loss 0.0080 (0.0334)	
training:	Epoch: [45][158/408]	Loss 0.0080 (0.0333)	
training:	Epoch: [45][159/408]	Loss 0.0110 (0.0331)	
training:	Epoch: [45][160/408]	Loss 0.0055 (0.0330)	
training:	Epoch: [45][161/408]	Loss 0.0108 (0.0328)	
training:	Epoch: [45][162/408]	Loss 0.0048 (0.0326)	
training:	Epoch: [45][163/408]	Loss 0.0098 (0.0325)	
training:	Epoch: [45][164/408]	Loss 0.0098 (0.0324)	
training:	Epoch: [45][165/408]	Loss 0.0086 (0.0322)	
training:	Epoch: [45][166/408]	Loss 0.0061 (0.0321)	
training:	Epoch: [45][167/408]	Loss 0.0077 (0.0319)	
training:	Epoch: [45][168/408]	Loss 0.0073 (0.0318)	
training:	Epoch: [45][169/408]	Loss 0.0076 (0.0316)	
training:	Epoch: [45][170/408]	Loss 0.0076 (0.0315)	
training:	Epoch: [45][171/408]	Loss 0.2461 (0.0327)	
training:	Epoch: [45][172/408]	Loss 0.2239 (0.0339)	
training:	Epoch: [45][173/408]	Loss 0.0072 (0.0337)	
training:	Epoch: [45][174/408]	Loss 0.0092 (0.0336)	
training:	Epoch: [45][175/408]	Loss 0.0044 (0.0334)	
training:	Epoch: [45][176/408]	Loss 0.0062 (0.0332)	
training:	Epoch: [45][177/408]	Loss 0.0091 (0.0331)	
training:	Epoch: [45][178/408]	Loss 0.0062 (0.0330)	
training:	Epoch: [45][179/408]	Loss 0.0100 (0.0328)	
training:	Epoch: [45][180/408]	Loss 0.0097 (0.0327)	
training:	Epoch: [45][181/408]	Loss 0.0083 (0.0326)	
training:	Epoch: [45][182/408]	Loss 0.2215 (0.0336)	
training:	Epoch: [45][183/408]	Loss 0.0076 (0.0335)	
training:	Epoch: [45][184/408]	Loss 0.0061 (0.0333)	
training:	Epoch: [45][185/408]	Loss 0.0083 (0.0332)	
training:	Epoch: [45][186/408]	Loss 0.0062 (0.0330)	
training:	Epoch: [45][187/408]	Loss 0.2074 (0.0340)	
training:	Epoch: [45][188/408]	Loss 0.0085 (0.0338)	
training:	Epoch: [45][189/408]	Loss 0.0096 (0.0337)	
training:	Epoch: [45][190/408]	Loss 0.0071 (0.0336)	
training:	Epoch: [45][191/408]	Loss 0.0062 (0.0334)	
training:	Epoch: [45][192/408]	Loss 0.0072 (0.0333)	
training:	Epoch: [45][193/408]	Loss 0.0080 (0.0331)	
training:	Epoch: [45][194/408]	Loss 0.0077 (0.0330)	
training:	Epoch: [45][195/408]	Loss 0.0076 (0.0329)	
training:	Epoch: [45][196/408]	Loss 0.0108 (0.0328)	
training:	Epoch: [45][197/408]	Loss 0.3107 (0.0342)	
training:	Epoch: [45][198/408]	Loss 0.0051 (0.0340)	
training:	Epoch: [45][199/408]	Loss 0.0089 (0.0339)	
training:	Epoch: [45][200/408]	Loss 0.0065 (0.0338)	
training:	Epoch: [45][201/408]	Loss 0.0095 (0.0337)	
training:	Epoch: [45][202/408]	Loss 0.0164 (0.0336)	
training:	Epoch: [45][203/408]	Loss 0.0058 (0.0334)	
training:	Epoch: [45][204/408]	Loss 0.0095 (0.0333)	
training:	Epoch: [45][205/408]	Loss 0.0059 (0.0332)	
training:	Epoch: [45][206/408]	Loss 0.0101 (0.0331)	
training:	Epoch: [45][207/408]	Loss 0.0082 (0.0329)	
training:	Epoch: [45][208/408]	Loss 0.0075 (0.0328)	
training:	Epoch: [45][209/408]	Loss 0.0111 (0.0327)	
training:	Epoch: [45][210/408]	Loss 0.0097 (0.0326)	
training:	Epoch: [45][211/408]	Loss 0.0056 (0.0325)	
training:	Epoch: [45][212/408]	Loss 0.0085 (0.0324)	
training:	Epoch: [45][213/408]	Loss 0.0074 (0.0323)	
training:	Epoch: [45][214/408]	Loss 0.0083 (0.0321)	
training:	Epoch: [45][215/408]	Loss 0.0088 (0.0320)	
training:	Epoch: [45][216/408]	Loss 0.2730 (0.0331)	
training:	Epoch: [45][217/408]	Loss 0.0081 (0.0330)	
training:	Epoch: [45][218/408]	Loss 0.0081 (0.0329)	
training:	Epoch: [45][219/408]	Loss 0.0072 (0.0328)	
training:	Epoch: [45][220/408]	Loss 0.0086 (0.0327)	
training:	Epoch: [45][221/408]	Loss 0.2380 (0.0336)	
training:	Epoch: [45][222/408]	Loss 0.0086 (0.0335)	
training:	Epoch: [45][223/408]	Loss 0.0089 (0.0334)	
training:	Epoch: [45][224/408]	Loss 0.0104 (0.0333)	
training:	Epoch: [45][225/408]	Loss 0.2215 (0.0341)	
training:	Epoch: [45][226/408]	Loss 0.5766 (0.0365)	
training:	Epoch: [45][227/408]	Loss 0.0086 (0.0364)	
training:	Epoch: [45][228/408]	Loss 0.0064 (0.0363)	
training:	Epoch: [45][229/408]	Loss 0.0090 (0.0362)	
training:	Epoch: [45][230/408]	Loss 0.0062 (0.0360)	
training:	Epoch: [45][231/408]	Loss 0.0083 (0.0359)	
training:	Epoch: [45][232/408]	Loss 0.0074 (0.0358)	
training:	Epoch: [45][233/408]	Loss 0.0066 (0.0357)	
training:	Epoch: [45][234/408]	Loss 0.0065 (0.0355)	
training:	Epoch: [45][235/408]	Loss 0.0078 (0.0354)	
training:	Epoch: [45][236/408]	Loss 0.0077 (0.0353)	
training:	Epoch: [45][237/408]	Loss 0.0064 (0.0352)	
training:	Epoch: [45][238/408]	Loss 0.0076 (0.0351)	
training:	Epoch: [45][239/408]	Loss 0.0095 (0.0350)	
training:	Epoch: [45][240/408]	Loss 0.0086 (0.0348)	
training:	Epoch: [45][241/408]	Loss 0.0034 (0.0347)	
training:	Epoch: [45][242/408]	Loss 0.0145 (0.0346)	
training:	Epoch: [45][243/408]	Loss 0.0063 (0.0345)	
training:	Epoch: [45][244/408]	Loss 0.0095 (0.0344)	
training:	Epoch: [45][245/408]	Loss 0.0084 (0.0343)	
training:	Epoch: [45][246/408]	Loss 0.0060 (0.0342)	
training:	Epoch: [45][247/408]	Loss 0.0384 (0.0342)	
training:	Epoch: [45][248/408]	Loss 0.0095 (0.0341)	
training:	Epoch: [45][249/408]	Loss 0.0072 (0.0340)	
training:	Epoch: [45][250/408]	Loss 0.0067 (0.0339)	
training:	Epoch: [45][251/408]	Loss 0.0109 (0.0338)	
training:	Epoch: [45][252/408]	Loss 0.0066 (0.0337)	
training:	Epoch: [45][253/408]	Loss 0.0110 (0.0336)	
training:	Epoch: [45][254/408]	Loss 0.0071 (0.0335)	
training:	Epoch: [45][255/408]	Loss 0.0063 (0.0334)	
training:	Epoch: [45][256/408]	Loss 0.0054 (0.0333)	
training:	Epoch: [45][257/408]	Loss 0.0086 (0.0332)	
training:	Epoch: [45][258/408]	Loss 0.0058 (0.0331)	
training:	Epoch: [45][259/408]	Loss 0.0072 (0.0330)	
training:	Epoch: [45][260/408]	Loss 0.0078 (0.0329)	
training:	Epoch: [45][261/408]	Loss 0.0067 (0.0328)	
training:	Epoch: [45][262/408]	Loss 0.0059 (0.0327)	
training:	Epoch: [45][263/408]	Loss 0.0078 (0.0326)	
training:	Epoch: [45][264/408]	Loss 0.0102 (0.0325)	
training:	Epoch: [45][265/408]	Loss 0.0060 (0.0324)	
training:	Epoch: [45][266/408]	Loss 0.0071 (0.0323)	
training:	Epoch: [45][267/408]	Loss 0.0073 (0.0322)	
training:	Epoch: [45][268/408]	Loss 0.0073 (0.0321)	
training:	Epoch: [45][269/408]	Loss 0.0075 (0.0320)	
training:	Epoch: [45][270/408]	Loss 0.2100 (0.0327)	
training:	Epoch: [45][271/408]	Loss 0.0057 (0.0326)	
training:	Epoch: [45][272/408]	Loss 0.0070 (0.0325)	
training:	Epoch: [45][273/408]	Loss 0.0078 (0.0324)	
training:	Epoch: [45][274/408]	Loss 0.0052 (0.0323)	
training:	Epoch: [45][275/408]	Loss 0.0072 (0.0322)	
training:	Epoch: [45][276/408]	Loss 0.0074 (0.0321)	
training:	Epoch: [45][277/408]	Loss 0.0059 (0.0320)	
training:	Epoch: [45][278/408]	Loss 0.0057 (0.0319)	
training:	Epoch: [45][279/408]	Loss 0.0055 (0.0318)	
training:	Epoch: [45][280/408]	Loss 0.0046 (0.0317)	
training:	Epoch: [45][281/408]	Loss 0.0075 (0.0317)	
training:	Epoch: [45][282/408]	Loss 0.0072 (0.0316)	
training:	Epoch: [45][283/408]	Loss 0.0058 (0.0315)	
training:	Epoch: [45][284/408]	Loss 0.0069 (0.0314)	
training:	Epoch: [45][285/408]	Loss 0.0083 (0.0313)	
training:	Epoch: [45][286/408]	Loss 0.0074 (0.0312)	
training:	Epoch: [45][287/408]	Loss 0.0085 (0.0311)	
training:	Epoch: [45][288/408]	Loss 0.0105 (0.0311)	
training:	Epoch: [45][289/408]	Loss 0.0054 (0.0310)	
training:	Epoch: [45][290/408]	Loss 0.0052 (0.0309)	
training:	Epoch: [45][291/408]	Loss 0.0100 (0.0308)	
training:	Epoch: [45][292/408]	Loss 0.0092 (0.0308)	
training:	Epoch: [45][293/408]	Loss 0.0055 (0.0307)	
training:	Epoch: [45][294/408]	Loss 0.0072 (0.0306)	
training:	Epoch: [45][295/408]	Loss 0.1973 (0.0311)	
training:	Epoch: [45][296/408]	Loss 0.0090 (0.0311)	
training:	Epoch: [45][297/408]	Loss 0.0106 (0.0310)	
training:	Epoch: [45][298/408]	Loss 0.2715 (0.0318)	
training:	Epoch: [45][299/408]	Loss 0.0085 (0.0317)	
training:	Epoch: [45][300/408]	Loss 0.0066 (0.0317)	
training:	Epoch: [45][301/408]	Loss 0.0084 (0.0316)	
training:	Epoch: [45][302/408]	Loss 0.0089 (0.0315)	
training:	Epoch: [45][303/408]	Loss 0.0062 (0.0314)	
training:	Epoch: [45][304/408]	Loss 0.0062 (0.0313)	
training:	Epoch: [45][305/408]	Loss 0.0070 (0.0313)	
training:	Epoch: [45][306/408]	Loss 0.0049 (0.0312)	
training:	Epoch: [45][307/408]	Loss 0.0070 (0.0311)	
training:	Epoch: [45][308/408]	Loss 0.0086 (0.0310)	
training:	Epoch: [45][309/408]	Loss 0.0071 (0.0309)	
training:	Epoch: [45][310/408]	Loss 0.0063 (0.0309)	
training:	Epoch: [45][311/408]	Loss 0.0103 (0.0308)	
training:	Epoch: [45][312/408]	Loss 0.0050 (0.0307)	
training:	Epoch: [45][313/408]	Loss 0.2583 (0.0314)	
training:	Epoch: [45][314/408]	Loss 0.0060 (0.0314)	
training:	Epoch: [45][315/408]	Loss 0.0083 (0.0313)	
training:	Epoch: [45][316/408]	Loss 0.0095 (0.0312)	
training:	Epoch: [45][317/408]	Loss 0.0058 (0.0311)	
training:	Epoch: [45][318/408]	Loss 0.0071 (0.0311)	
training:	Epoch: [45][319/408]	Loss 0.0084 (0.0310)	
training:	Epoch: [45][320/408]	Loss 0.0067 (0.0309)	
training:	Epoch: [45][321/408]	Loss 0.0076 (0.0308)	
training:	Epoch: [45][322/408]	Loss 0.0074 (0.0308)	
training:	Epoch: [45][323/408]	Loss 0.0078 (0.0307)	
training:	Epoch: [45][324/408]	Loss 0.0089 (0.0306)	
training:	Epoch: [45][325/408]	Loss 0.0100 (0.0306)	
training:	Epoch: [45][326/408]	Loss 0.0065 (0.0305)	
training:	Epoch: [45][327/408]	Loss 0.0083 (0.0304)	
training:	Epoch: [45][328/408]	Loss 0.0084 (0.0304)	
training:	Epoch: [45][329/408]	Loss 0.0056 (0.0303)	
training:	Epoch: [45][330/408]	Loss 0.0052 (0.0302)	
training:	Epoch: [45][331/408]	Loss 0.0097 (0.0301)	
training:	Epoch: [45][332/408]	Loss 0.0088 (0.0301)	
training:	Epoch: [45][333/408]	Loss 0.0055 (0.0300)	
training:	Epoch: [45][334/408]	Loss 0.0059 (0.0299)	
training:	Epoch: [45][335/408]	Loss 0.0054 (0.0299)	
training:	Epoch: [45][336/408]	Loss 0.0064 (0.0298)	
training:	Epoch: [45][337/408]	Loss 0.0051 (0.0297)	
training:	Epoch: [45][338/408]	Loss 0.0077 (0.0297)	
training:	Epoch: [45][339/408]	Loss 0.0054 (0.0296)	
training:	Epoch: [45][340/408]	Loss 0.0074 (0.0295)	
training:	Epoch: [45][341/408]	Loss 0.0044 (0.0294)	
training:	Epoch: [45][342/408]	Loss 0.0070 (0.0294)	
training:	Epoch: [45][343/408]	Loss 0.0054 (0.0293)	
training:	Epoch: [45][344/408]	Loss 0.0051 (0.0292)	
training:	Epoch: [45][345/408]	Loss 0.0050 (0.0292)	
training:	Epoch: [45][346/408]	Loss 0.2175 (0.0297)	
training:	Epoch: [45][347/408]	Loss 0.0062 (0.0296)	
training:	Epoch: [45][348/408]	Loss 0.0045 (0.0296)	
training:	Epoch: [45][349/408]	Loss 0.0051 (0.0295)	
training:	Epoch: [45][350/408]	Loss 0.0052 (0.0294)	
training:	Epoch: [45][351/408]	Loss 0.0050 (0.0294)	
training:	Epoch: [45][352/408]	Loss 0.0082 (0.0293)	
training:	Epoch: [45][353/408]	Loss 0.0076 (0.0292)	
training:	Epoch: [45][354/408]	Loss 0.0063 (0.0292)	
training:	Epoch: [45][355/408]	Loss 0.0063 (0.0291)	
training:	Epoch: [45][356/408]	Loss 0.0051 (0.0290)	
training:	Epoch: [45][357/408]	Loss 0.0048 (0.0290)	
training:	Epoch: [45][358/408]	Loss 0.0051 (0.0289)	
training:	Epoch: [45][359/408]	Loss 0.0063 (0.0288)	
training:	Epoch: [45][360/408]	Loss 0.0059 (0.0288)	
training:	Epoch: [45][361/408]	Loss 0.0075 (0.0287)	
training:	Epoch: [45][362/408]	Loss 0.0096 (0.0287)	
training:	Epoch: [45][363/408]	Loss 0.0055 (0.0286)	
training:	Epoch: [45][364/408]	Loss 0.0071 (0.0285)	
training:	Epoch: [45][365/408]	Loss 0.0051 (0.0285)	
training:	Epoch: [45][366/408]	Loss 0.0053 (0.0284)	
training:	Epoch: [45][367/408]	Loss 0.0089 (0.0284)	
training:	Epoch: [45][368/408]	Loss 0.0054 (0.0283)	
training:	Epoch: [45][369/408]	Loss 0.0060 (0.0282)	
training:	Epoch: [45][370/408]	Loss 0.0055 (0.0282)	
training:	Epoch: [45][371/408]	Loss 0.0070 (0.0281)	
training:	Epoch: [45][372/408]	Loss 0.0065 (0.0281)	
training:	Epoch: [45][373/408]	Loss 0.0067 (0.0280)	
training:	Epoch: [45][374/408]	Loss 0.0060 (0.0279)	
training:	Epoch: [45][375/408]	Loss 0.0054 (0.0279)	
training:	Epoch: [45][376/408]	Loss 0.0065 (0.0278)	
training:	Epoch: [45][377/408]	Loss 0.0066 (0.0278)	
training:	Epoch: [45][378/408]	Loss 0.0043 (0.0277)	
training:	Epoch: [45][379/408]	Loss 0.0062 (0.0277)	
training:	Epoch: [45][380/408]	Loss 0.0047 (0.0276)	
training:	Epoch: [45][381/408]	Loss 0.0079 (0.0275)	
training:	Epoch: [45][382/408]	Loss 0.0068 (0.0275)	
training:	Epoch: [45][383/408]	Loss 0.0068 (0.0274)	
training:	Epoch: [45][384/408]	Loss 0.0073 (0.0274)	
training:	Epoch: [45][385/408]	Loss 0.0044 (0.0273)	
training:	Epoch: [45][386/408]	Loss 0.0067 (0.0273)	
training:	Epoch: [45][387/408]	Loss 0.0042 (0.0272)	
training:	Epoch: [45][388/408]	Loss 0.0052 (0.0272)	
training:	Epoch: [45][389/408]	Loss 0.0046 (0.0271)	
training:	Epoch: [45][390/408]	Loss 0.0044 (0.0270)	
training:	Epoch: [45][391/408]	Loss 0.0058 (0.0270)	
training:	Epoch: [45][392/408]	Loss 0.0062 (0.0269)	
training:	Epoch: [45][393/408]	Loss 0.0102 (0.0269)	
training:	Epoch: [45][394/408]	Loss 0.0040 (0.0268)	
training:	Epoch: [45][395/408]	Loss 0.3129 (0.0276)	
training:	Epoch: [45][396/408]	Loss 0.0068 (0.0275)	
training:	Epoch: [45][397/408]	Loss 0.2575 (0.0281)	
training:	Epoch: [45][398/408]	Loss 0.2895 (0.0287)	
training:	Epoch: [45][399/408]	Loss 0.0057 (0.0287)	
training:	Epoch: [45][400/408]	Loss 0.0056 (0.0286)	
training:	Epoch: [45][401/408]	Loss 0.3112 (0.0293)	
training:	Epoch: [45][402/408]	Loss 0.0103 (0.0293)	
training:	Epoch: [45][403/408]	Loss 0.0051 (0.0292)	
training:	Epoch: [45][404/408]	Loss 0.0082 (0.0292)	
training:	Epoch: [45][405/408]	Loss 0.0052 (0.0291)	
training:	Epoch: [45][406/408]	Loss 0.0047 (0.0290)	
training:	Epoch: [45][407/408]	Loss 0.0045 (0.0290)	
training:	Epoch: [45][408/408]	Loss 0.0059 (0.0289)	
Training:	 Loss: 0.0289

Training:	 ACC: 0.9947 0.9946 0.9941 0.9952
Validation:	 ACC: 0.7770 0.7780 0.7984 0.7556
Validation:	 Best_BACC: 0.8226 0.8250 0.8772 0.7679
Validation:	 Loss: 1.0355
Pretraining:	Epoch 46/200
----------
training:	Epoch: [46][1/408]	Loss 0.0054 (0.0054)	
training:	Epoch: [46][2/408]	Loss 0.0061 (0.0058)	
training:	Epoch: [46][3/408]	Loss 0.0061 (0.0059)	
training:	Epoch: [46][4/408]	Loss 0.2882 (0.0765)	
training:	Epoch: [46][5/408]	Loss 0.0043 (0.0620)	
training:	Epoch: [46][6/408]	Loss 0.0067 (0.0528)	
training:	Epoch: [46][7/408]	Loss 0.0054 (0.0460)	
training:	Epoch: [46][8/408]	Loss 0.0052 (0.0409)	
training:	Epoch: [46][9/408]	Loss 0.0049 (0.0369)	
training:	Epoch: [46][10/408]	Loss 0.0051 (0.0338)	
training:	Epoch: [46][11/408]	Loss 0.0050 (0.0311)	
training:	Epoch: [46][12/408]	Loss 0.0058 (0.0290)	
training:	Epoch: [46][13/408]	Loss 0.0057 (0.0272)	
training:	Epoch: [46][14/408]	Loss 0.0041 (0.0256)	
training:	Epoch: [46][15/408]	Loss 0.0058 (0.0243)	
training:	Epoch: [46][16/408]	Loss 0.0048 (0.0230)	
training:	Epoch: [46][17/408]	Loss 0.0057 (0.0220)	
training:	Epoch: [46][18/408]	Loss 0.0054 (0.0211)	
training:	Epoch: [46][19/408]	Loss 0.0080 (0.0204)	
training:	Epoch: [46][20/408]	Loss 0.0072 (0.0197)	
training:	Epoch: [46][21/408]	Loss 0.0058 (0.0191)	
training:	Epoch: [46][22/408]	Loss 0.0060 (0.0185)	
training:	Epoch: [46][23/408]	Loss 0.0047 (0.0179)	
training:	Epoch: [46][24/408]	Loss 0.0053 (0.0174)	
training:	Epoch: [46][25/408]	Loss 0.0077 (0.0170)	
training:	Epoch: [46][26/408]	Loss 0.0069 (0.0166)	
training:	Epoch: [46][27/408]	Loss 0.0052 (0.0162)	
training:	Epoch: [46][28/408]	Loss 0.0067 (0.0158)	
training:	Epoch: [46][29/408]	Loss 0.0051 (0.0155)	
training:	Epoch: [46][30/408]	Loss 0.0058 (0.0151)	
training:	Epoch: [46][31/408]	Loss 0.2213 (0.0218)	
training:	Epoch: [46][32/408]	Loss 0.0062 (0.0213)	
training:	Epoch: [46][33/408]	Loss 0.2526 (0.0283)	
training:	Epoch: [46][34/408]	Loss 0.0057 (0.0276)	
training:	Epoch: [46][35/408]	Loss 0.2199 (0.0331)	
training:	Epoch: [46][36/408]	Loss 0.0074 (0.0324)	
training:	Epoch: [46][37/408]	Loss 0.0046 (0.0317)	
training:	Epoch: [46][38/408]	Loss 0.0071 (0.0310)	
training:	Epoch: [46][39/408]	Loss 0.0062 (0.0304)	
training:	Epoch: [46][40/408]	Loss 0.2121 (0.0349)	
training:	Epoch: [46][41/408]	Loss 0.0067 (0.0342)	
training:	Epoch: [46][42/408]	Loss 0.0092 (0.0336)	
training:	Epoch: [46][43/408]	Loss 0.0101 (0.0331)	
training:	Epoch: [46][44/408]	Loss 0.0060 (0.0325)	
training:	Epoch: [46][45/408]	Loss 0.0058 (0.0319)	
training:	Epoch: [46][46/408]	Loss 0.0054 (0.0313)	
training:	Epoch: [46][47/408]	Loss 0.0058 (0.0308)	
training:	Epoch: [46][48/408]	Loss 0.0061 (0.0303)	
training:	Epoch: [46][49/408]	Loss 0.1947 (0.0336)	
training:	Epoch: [46][50/408]	Loss 0.0085 (0.0331)	
training:	Epoch: [46][51/408]	Loss 0.0073 (0.0326)	
training:	Epoch: [46][52/408]	Loss 0.0056 (0.0321)	
training:	Epoch: [46][53/408]	Loss 0.0070 (0.0316)	
training:	Epoch: [46][54/408]	Loss 0.0116 (0.0312)	
training:	Epoch: [46][55/408]	Loss 0.0063 (0.0308)	
training:	Epoch: [46][56/408]	Loss 0.0057 (0.0303)	
training:	Epoch: [46][57/408]	Loss 0.0088 (0.0300)	
training:	Epoch: [46][58/408]	Loss 0.0066 (0.0296)	
training:	Epoch: [46][59/408]	Loss 0.0077 (0.0292)	
training:	Epoch: [46][60/408]	Loss 0.0083 (0.0288)	
training:	Epoch: [46][61/408]	Loss 0.0091 (0.0285)	
training:	Epoch: [46][62/408]	Loss 0.0058 (0.0281)	
training:	Epoch: [46][63/408]	Loss 0.0084 (0.0278)	
training:	Epoch: [46][64/408]	Loss 0.0099 (0.0276)	
training:	Epoch: [46][65/408]	Loss 0.0069 (0.0272)	
training:	Epoch: [46][66/408]	Loss 0.0046 (0.0269)	
training:	Epoch: [46][67/408]	Loss 0.0078 (0.0266)	
training:	Epoch: [46][68/408]	Loss 0.0094 (0.0264)	
training:	Epoch: [46][69/408]	Loss 0.0077 (0.0261)	
training:	Epoch: [46][70/408]	Loss 0.0085 (0.0258)	
training:	Epoch: [46][71/408]	Loss 0.0059 (0.0256)	
training:	Epoch: [46][72/408]	Loss 0.0079 (0.0253)	
training:	Epoch: [46][73/408]	Loss 0.2417 (0.0283)	
training:	Epoch: [46][74/408]	Loss 0.0092 (0.0280)	
training:	Epoch: [46][75/408]	Loss 0.0091 (0.0278)	
training:	Epoch: [46][76/408]	Loss 0.0073 (0.0275)	
training:	Epoch: [46][77/408]	Loss 0.0044 (0.0272)	
training:	Epoch: [46][78/408]	Loss 0.0074 (0.0269)	
training:	Epoch: [46][79/408]	Loss 0.0087 (0.0267)	
training:	Epoch: [46][80/408]	Loss 0.3292 (0.0305)	
training:	Epoch: [46][81/408]	Loss 0.0080 (0.0302)	
training:	Epoch: [46][82/408]	Loss 0.0086 (0.0299)	
training:	Epoch: [46][83/408]	Loss 0.0062 (0.0297)	
training:	Epoch: [46][84/408]	Loss 0.0066 (0.0294)	
training:	Epoch: [46][85/408]	Loss 0.0048 (0.0291)	
training:	Epoch: [46][86/408]	Loss 0.0078 (0.0288)	
training:	Epoch: [46][87/408]	Loss 0.0092 (0.0286)	
training:	Epoch: [46][88/408]	Loss 0.0058 (0.0284)	
training:	Epoch: [46][89/408]	Loss 0.0058 (0.0281)	
training:	Epoch: [46][90/408]	Loss 0.0072 (0.0279)	
training:	Epoch: [46][91/408]	Loss 0.0086 (0.0277)	
training:	Epoch: [46][92/408]	Loss 0.0048 (0.0274)	
training:	Epoch: [46][93/408]	Loss 0.0053 (0.0272)	
training:	Epoch: [46][94/408]	Loss 0.0056 (0.0269)	
training:	Epoch: [46][95/408]	Loss 0.0050 (0.0267)	
training:	Epoch: [46][96/408]	Loss 0.0053 (0.0265)	
training:	Epoch: [46][97/408]	Loss 0.0067 (0.0263)	
training:	Epoch: [46][98/408]	Loss 0.0049 (0.0261)	
training:	Epoch: [46][99/408]	Loss 0.0052 (0.0259)	
training:	Epoch: [46][100/408]	Loss 0.0050 (0.0257)	
training:	Epoch: [46][101/408]	Loss 0.0086 (0.0255)	
training:	Epoch: [46][102/408]	Loss 0.0053 (0.0253)	
training:	Epoch: [46][103/408]	Loss 0.0063 (0.0251)	
training:	Epoch: [46][104/408]	Loss 0.0047 (0.0249)	
training:	Epoch: [46][105/408]	Loss 0.0071 (0.0247)	
training:	Epoch: [46][106/408]	Loss 0.0072 (0.0246)	
training:	Epoch: [46][107/408]	Loss 0.0047 (0.0244)	
training:	Epoch: [46][108/408]	Loss 0.0071 (0.0242)	
training:	Epoch: [46][109/408]	Loss 0.0067 (0.0241)	
training:	Epoch: [46][110/408]	Loss 0.0049 (0.0239)	
training:	Epoch: [46][111/408]	Loss 0.0050 (0.0237)	
training:	Epoch: [46][112/408]	Loss 0.3099 (0.0263)	
training:	Epoch: [46][113/408]	Loss 0.0042 (0.0261)	
training:	Epoch: [46][114/408]	Loss 0.0042 (0.0259)	
training:	Epoch: [46][115/408]	Loss 0.0086 (0.0257)	
training:	Epoch: [46][116/408]	Loss 0.0074 (0.0256)	
training:	Epoch: [46][117/408]	Loss 0.0058 (0.0254)	
training:	Epoch: [46][118/408]	Loss 0.0069 (0.0253)	
training:	Epoch: [46][119/408]	Loss 0.0148 (0.0252)	
training:	Epoch: [46][120/408]	Loss 0.0051 (0.0250)	
training:	Epoch: [46][121/408]	Loss 0.0046 (0.0248)	
training:	Epoch: [46][122/408]	Loss 0.0073 (0.0247)	
training:	Epoch: [46][123/408]	Loss 0.0073 (0.0245)	
training:	Epoch: [46][124/408]	Loss 0.0063 (0.0244)	
training:	Epoch: [46][125/408]	Loss 0.0063 (0.0243)	
training:	Epoch: [46][126/408]	Loss 0.0049 (0.0241)	
training:	Epoch: [46][127/408]	Loss 0.0065 (0.0240)	
training:	Epoch: [46][128/408]	Loss 0.2484 (0.0257)	
training:	Epoch: [46][129/408]	Loss 0.0043 (0.0255)	
training:	Epoch: [46][130/408]	Loss 0.0071 (0.0254)	
training:	Epoch: [46][131/408]	Loss 0.0051 (0.0253)	
training:	Epoch: [46][132/408]	Loss 0.0080 (0.0251)	
training:	Epoch: [46][133/408]	Loss 0.2408 (0.0267)	
training:	Epoch: [46][134/408]	Loss 0.0053 (0.0266)	
training:	Epoch: [46][135/408]	Loss 0.0043 (0.0264)	
training:	Epoch: [46][136/408]	Loss 0.0048 (0.0263)	
training:	Epoch: [46][137/408]	Loss 0.0044 (0.0261)	
training:	Epoch: [46][138/408]	Loss 0.0043 (0.0259)	
training:	Epoch: [46][139/408]	Loss 0.0042 (0.0258)	
training:	Epoch: [46][140/408]	Loss 0.2731 (0.0276)	
training:	Epoch: [46][141/408]	Loss 0.0055 (0.0274)	
training:	Epoch: [46][142/408]	Loss 0.0090 (0.0273)	
training:	Epoch: [46][143/408]	Loss 0.0060 (0.0271)	
training:	Epoch: [46][144/408]	Loss 0.0062 (0.0270)	
training:	Epoch: [46][145/408]	Loss 0.0076 (0.0268)	
training:	Epoch: [46][146/408]	Loss 0.0070 (0.0267)	
training:	Epoch: [46][147/408]	Loss 0.0076 (0.0266)	
training:	Epoch: [46][148/408]	Loss 0.0071 (0.0264)	
training:	Epoch: [46][149/408]	Loss 0.0087 (0.0263)	
training:	Epoch: [46][150/408]	Loss 0.0064 (0.0262)	
training:	Epoch: [46][151/408]	Loss 0.0072 (0.0261)	
training:	Epoch: [46][152/408]	Loss 0.0041 (0.0259)	
training:	Epoch: [46][153/408]	Loss 0.0064 (0.0258)	
training:	Epoch: [46][154/408]	Loss 0.0064 (0.0257)	
training:	Epoch: [46][155/408]	Loss 0.0065 (0.0255)	
training:	Epoch: [46][156/408]	Loss 0.2462 (0.0270)	
training:	Epoch: [46][157/408]	Loss 0.0042 (0.0268)	
training:	Epoch: [46][158/408]	Loss 0.2669 (0.0283)	
training:	Epoch: [46][159/408]	Loss 0.0051 (0.0282)	
training:	Epoch: [46][160/408]	Loss 0.0051 (0.0280)	
training:	Epoch: [46][161/408]	Loss 0.0069 (0.0279)	
training:	Epoch: [46][162/408]	Loss 0.0059 (0.0278)	
training:	Epoch: [46][163/408]	Loss 0.0063 (0.0276)	
training:	Epoch: [46][164/408]	Loss 0.0054 (0.0275)	
training:	Epoch: [46][165/408]	Loss 0.0034 (0.0274)	
training:	Epoch: [46][166/408]	Loss 0.0098 (0.0273)	
training:	Epoch: [46][167/408]	Loss 0.0060 (0.0271)	
training:	Epoch: [46][168/408]	Loss 0.0052 (0.0270)	
training:	Epoch: [46][169/408]	Loss 0.0066 (0.0269)	
training:	Epoch: [46][170/408]	Loss 0.0076 (0.0268)	
training:	Epoch: [46][171/408]	Loss 0.0055 (0.0266)	
training:	Epoch: [46][172/408]	Loss 0.0045 (0.0265)	
training:	Epoch: [46][173/408]	Loss 0.0040 (0.0264)	
training:	Epoch: [46][174/408]	Loss 0.0079 (0.0263)	
training:	Epoch: [46][175/408]	Loss 0.0049 (0.0261)	
training:	Epoch: [46][176/408]	Loss 0.0064 (0.0260)	
training:	Epoch: [46][177/408]	Loss 0.0054 (0.0259)	
training:	Epoch: [46][178/408]	Loss 0.0066 (0.0258)	
training:	Epoch: [46][179/408]	Loss 0.0073 (0.0257)	
training:	Epoch: [46][180/408]	Loss 0.0058 (0.0256)	
training:	Epoch: [46][181/408]	Loss 0.0220 (0.0256)	
training:	Epoch: [46][182/408]	Loss 0.0074 (0.0255)	
training:	Epoch: [46][183/408]	Loss 0.0068 (0.0254)	
training:	Epoch: [46][184/408]	Loss 0.2686 (0.0267)	
training:	Epoch: [46][185/408]	Loss 0.0064 (0.0266)	
training:	Epoch: [46][186/408]	Loss 0.0063 (0.0265)	
training:	Epoch: [46][187/408]	Loss 0.0070 (0.0264)	
training:	Epoch: [46][188/408]	Loss 0.0138 (0.0263)	
training:	Epoch: [46][189/408]	Loss 0.0055 (0.0262)	
training:	Epoch: [46][190/408]	Loss 0.1008 (0.0266)	
training:	Epoch: [46][191/408]	Loss 0.0068 (0.0265)	
training:	Epoch: [46][192/408]	Loss 0.0055 (0.0264)	
training:	Epoch: [46][193/408]	Loss 0.2890 (0.0277)	
training:	Epoch: [46][194/408]	Loss 0.0064 (0.0276)	
training:	Epoch: [46][195/408]	Loss 0.0052 (0.0275)	
training:	Epoch: [46][196/408]	Loss 0.0084 (0.0274)	
training:	Epoch: [46][197/408]	Loss 0.0081 (0.0273)	
training:	Epoch: [46][198/408]	Loss 0.0184 (0.0273)	
training:	Epoch: [46][199/408]	Loss 0.0040 (0.0272)	
training:	Epoch: [46][200/408]	Loss 0.0091 (0.0271)	
training:	Epoch: [46][201/408]	Loss 0.0069 (0.0270)	
training:	Epoch: [46][202/408]	Loss 0.0097 (0.0269)	
training:	Epoch: [46][203/408]	Loss 0.0130 (0.0268)	
training:	Epoch: [46][204/408]	Loss 0.0046 (0.0267)	
training:	Epoch: [46][205/408]	Loss 0.0081 (0.0266)	
training:	Epoch: [46][206/408]	Loss 0.0078 (0.0265)	
training:	Epoch: [46][207/408]	Loss 0.0059 (0.0264)	
training:	Epoch: [46][208/408]	Loss 0.0062 (0.0263)	
training:	Epoch: [46][209/408]	Loss 0.0055 (0.0262)	
training:	Epoch: [46][210/408]	Loss 0.0071 (0.0261)	
training:	Epoch: [46][211/408]	Loss 0.0061 (0.0260)	
training:	Epoch: [46][212/408]	Loss 0.0062 (0.0259)	
training:	Epoch: [46][213/408]	Loss 0.0054 (0.0258)	
training:	Epoch: [46][214/408]	Loss 0.0105 (0.0258)	
training:	Epoch: [46][215/408]	Loss 0.0203 (0.0258)	
training:	Epoch: [46][216/408]	Loss 0.0065 (0.0257)	
training:	Epoch: [46][217/408]	Loss 0.0150 (0.0256)	
training:	Epoch: [46][218/408]	Loss 0.2379 (0.0266)	
training:	Epoch: [46][219/408]	Loss 0.0067 (0.0265)	
training:	Epoch: [46][220/408]	Loss 0.0070 (0.0264)	
training:	Epoch: [46][221/408]	Loss 0.0099 (0.0263)	
training:	Epoch: [46][222/408]	Loss 0.0108 (0.0263)	
training:	Epoch: [46][223/408]	Loss 0.0067 (0.0262)	
training:	Epoch: [46][224/408]	Loss 0.0174 (0.0261)	
training:	Epoch: [46][225/408]	Loss 0.0062 (0.0260)	
training:	Epoch: [46][226/408]	Loss 0.0050 (0.0260)	
training:	Epoch: [46][227/408]	Loss 0.2417 (0.0269)	
training:	Epoch: [46][228/408]	Loss 0.0087 (0.0268)	
training:	Epoch: [46][229/408]	Loss 0.0064 (0.0267)	
training:	Epoch: [46][230/408]	Loss 0.0061 (0.0266)	
training:	Epoch: [46][231/408]	Loss 0.1725 (0.0273)	
training:	Epoch: [46][232/408]	Loss 0.0040 (0.0272)	
training:	Epoch: [46][233/408]	Loss 0.0063 (0.0271)	
training:	Epoch: [46][234/408]	Loss 0.0075 (0.0270)	
training:	Epoch: [46][235/408]	Loss 0.0044 (0.0269)	
training:	Epoch: [46][236/408]	Loss 0.0091 (0.0268)	
training:	Epoch: [46][237/408]	Loss 0.0068 (0.0267)	
training:	Epoch: [46][238/408]	Loss 0.0047 (0.0267)	
training:	Epoch: [46][239/408]	Loss 0.0062 (0.0266)	
training:	Epoch: [46][240/408]	Loss 0.0044 (0.0265)	
training:	Epoch: [46][241/408]	Loss 0.0081 (0.0264)	
training:	Epoch: [46][242/408]	Loss 0.0191 (0.0264)	
training:	Epoch: [46][243/408]	Loss 0.0064 (0.0263)	
training:	Epoch: [46][244/408]	Loss 0.0069 (0.0262)	
training:	Epoch: [46][245/408]	Loss 0.0055 (0.0261)	
training:	Epoch: [46][246/408]	Loss 0.0070 (0.0260)	
training:	Epoch: [46][247/408]	Loss 0.0112 (0.0260)	
training:	Epoch: [46][248/408]	Loss 0.0089 (0.0259)	
training:	Epoch: [46][249/408]	Loss 0.0077 (0.0258)	
training:	Epoch: [46][250/408]	Loss 0.0120 (0.0258)	
training:	Epoch: [46][251/408]	Loss 0.0061 (0.0257)	
training:	Epoch: [46][252/408]	Loss 0.0072 (0.0256)	
training:	Epoch: [46][253/408]	Loss 0.0080 (0.0256)	
training:	Epoch: [46][254/408]	Loss 0.0072 (0.0255)	
training:	Epoch: [46][255/408]	Loss 0.0040 (0.0254)	
training:	Epoch: [46][256/408]	Loss 0.0081 (0.0253)	
training:	Epoch: [46][257/408]	Loss 0.0081 (0.0253)	
training:	Epoch: [46][258/408]	Loss 0.0047 (0.0252)	
training:	Epoch: [46][259/408]	Loss 0.0061 (0.0251)	
training:	Epoch: [46][260/408]	Loss 0.1765 (0.0257)	
training:	Epoch: [46][261/408]	Loss 0.0082 (0.0256)	
training:	Epoch: [46][262/408]	Loss 0.0055 (0.0256)	
training:	Epoch: [46][263/408]	Loss 0.0053 (0.0255)	
training:	Epoch: [46][264/408]	Loss 0.0085 (0.0254)	
training:	Epoch: [46][265/408]	Loss 0.0076 (0.0254)	
training:	Epoch: [46][266/408]	Loss 0.0070 (0.0253)	
training:	Epoch: [46][267/408]	Loss 0.0072 (0.0252)	
training:	Epoch: [46][268/408]	Loss 0.0073 (0.0251)	
training:	Epoch: [46][269/408]	Loss 0.0054 (0.0251)	
training:	Epoch: [46][270/408]	Loss 0.3180 (0.0262)	
training:	Epoch: [46][271/408]	Loss 0.0055 (0.0261)	
training:	Epoch: [46][272/408]	Loss 0.0062 (0.0260)	
training:	Epoch: [46][273/408]	Loss 0.0092 (0.0259)	
training:	Epoch: [46][274/408]	Loss 0.0068 (0.0259)	
training:	Epoch: [46][275/408]	Loss 0.0052 (0.0258)	
training:	Epoch: [46][276/408]	Loss 0.0065 (0.0257)	
training:	Epoch: [46][277/408]	Loss 0.0065 (0.0257)	
training:	Epoch: [46][278/408]	Loss 0.0056 (0.0256)	
training:	Epoch: [46][279/408]	Loss 0.0057 (0.0255)	
training:	Epoch: [46][280/408]	Loss 0.0056 (0.0254)	
training:	Epoch: [46][281/408]	Loss 0.0053 (0.0254)	
training:	Epoch: [46][282/408]	Loss 0.2486 (0.0262)	
training:	Epoch: [46][283/408]	Loss 0.0048 (0.0261)	
training:	Epoch: [46][284/408]	Loss 0.0076 (0.0260)	
training:	Epoch: [46][285/408]	Loss 0.3098 (0.0270)	
training:	Epoch: [46][286/408]	Loss 0.0099 (0.0270)	
training:	Epoch: [46][287/408]	Loss 0.0060 (0.0269)	
training:	Epoch: [46][288/408]	Loss 0.2439 (0.0276)	
training:	Epoch: [46][289/408]	Loss 0.0074 (0.0276)	
training:	Epoch: [46][290/408]	Loss 0.0075 (0.0275)	
training:	Epoch: [46][291/408]	Loss 0.0038 (0.0274)	
training:	Epoch: [46][292/408]	Loss 0.0062 (0.0274)	
training:	Epoch: [46][293/408]	Loss 0.0075 (0.0273)	
training:	Epoch: [46][294/408]	Loss 0.0041 (0.0272)	
training:	Epoch: [46][295/408]	Loss 0.0076 (0.0271)	
training:	Epoch: [46][296/408]	Loss 0.0092 (0.0271)	
training:	Epoch: [46][297/408]	Loss 0.0072 (0.0270)	
training:	Epoch: [46][298/408]	Loss 0.0073 (0.0269)	
training:	Epoch: [46][299/408]	Loss 0.0097 (0.0269)	
training:	Epoch: [46][300/408]	Loss 0.0109 (0.0268)	
training:	Epoch: [46][301/408]	Loss 0.0050 (0.0268)	
training:	Epoch: [46][302/408]	Loss 0.0114 (0.0267)	
training:	Epoch: [46][303/408]	Loss 0.0056 (0.0266)	
training:	Epoch: [46][304/408]	Loss 0.0043 (0.0266)	
training:	Epoch: [46][305/408]	Loss 0.3357 (0.0276)	
training:	Epoch: [46][306/408]	Loss 0.0059 (0.0275)	
training:	Epoch: [46][307/408]	Loss 0.0081 (0.0274)	
training:	Epoch: [46][308/408]	Loss 0.0068 (0.0274)	
training:	Epoch: [46][309/408]	Loss 0.0066 (0.0273)	
training:	Epoch: [46][310/408]	Loss 0.0067 (0.0272)	
training:	Epoch: [46][311/408]	Loss 0.2370 (0.0279)	
training:	Epoch: [46][312/408]	Loss 0.0067 (0.0279)	
training:	Epoch: [46][313/408]	Loss 0.0063 (0.0278)	
training:	Epoch: [46][314/408]	Loss 0.2207 (0.0284)	
training:	Epoch: [46][315/408]	Loss 0.0064 (0.0283)	
training:	Epoch: [46][316/408]	Loss 0.0053 (0.0283)	
training:	Epoch: [46][317/408]	Loss 0.0089 (0.0282)	
training:	Epoch: [46][318/408]	Loss 0.2525 (0.0289)	
training:	Epoch: [46][319/408]	Loss 0.2387 (0.0296)	
training:	Epoch: [46][320/408]	Loss 0.0078 (0.0295)	
training:	Epoch: [46][321/408]	Loss 0.0056 (0.0294)	
training:	Epoch: [46][322/408]	Loss 0.0071 (0.0293)	
training:	Epoch: [46][323/408]	Loss 0.0075 (0.0293)	
training:	Epoch: [46][324/408]	Loss 0.0053 (0.0292)	
training:	Epoch: [46][325/408]	Loss 0.0075 (0.0291)	
training:	Epoch: [46][326/408]	Loss 0.0085 (0.0291)	
training:	Epoch: [46][327/408]	Loss 0.0128 (0.0290)	
training:	Epoch: [46][328/408]	Loss 0.0067 (0.0290)	
training:	Epoch: [46][329/408]	Loss 0.0086 (0.0289)	
training:	Epoch: [46][330/408]	Loss 0.0063 (0.0288)	
training:	Epoch: [46][331/408]	Loss 0.0078 (0.0288)	
training:	Epoch: [46][332/408]	Loss 0.0058 (0.0287)	
training:	Epoch: [46][333/408]	Loss 0.0058 (0.0286)	
training:	Epoch: [46][334/408]	Loss 0.2237 (0.0292)	
training:	Epoch: [46][335/408]	Loss 0.0048 (0.0291)	
training:	Epoch: [46][336/408]	Loss 0.0104 (0.0291)	
training:	Epoch: [46][337/408]	Loss 0.0110 (0.0290)	
training:	Epoch: [46][338/408]	Loss 0.0062 (0.0290)	
training:	Epoch: [46][339/408]	Loss 0.0065 (0.0289)	
training:	Epoch: [46][340/408]	Loss 0.0094 (0.0288)	
training:	Epoch: [46][341/408]	Loss 0.0087 (0.0288)	
training:	Epoch: [46][342/408]	Loss 0.0059 (0.0287)	
training:	Epoch: [46][343/408]	Loss 0.0164 (0.0287)	
training:	Epoch: [46][344/408]	Loss 0.0066 (0.0286)	
training:	Epoch: [46][345/408]	Loss 0.0057 (0.0285)	
training:	Epoch: [46][346/408]	Loss 0.0059 (0.0285)	
training:	Epoch: [46][347/408]	Loss 0.0043 (0.0284)	
training:	Epoch: [46][348/408]	Loss 0.0094 (0.0284)	
training:	Epoch: [46][349/408]	Loss 0.0064 (0.0283)	
training:	Epoch: [46][350/408]	Loss 0.0068 (0.0282)	
training:	Epoch: [46][351/408]	Loss 0.0104 (0.0282)	
training:	Epoch: [46][352/408]	Loss 0.0058 (0.0281)	
training:	Epoch: [46][353/408]	Loss 0.0064 (0.0281)	
training:	Epoch: [46][354/408]	Loss 0.0064 (0.0280)	
training:	Epoch: [46][355/408]	Loss 0.0063 (0.0279)	
training:	Epoch: [46][356/408]	Loss 0.0053 (0.0279)	
training:	Epoch: [46][357/408]	Loss 0.0075 (0.0278)	
training:	Epoch: [46][358/408]	Loss 0.2291 (0.0284)	
training:	Epoch: [46][359/408]	Loss 0.0083 (0.0283)	
training:	Epoch: [46][360/408]	Loss 0.2841 (0.0290)	
training:	Epoch: [46][361/408]	Loss 0.0084 (0.0290)	
training:	Epoch: [46][362/408]	Loss 0.0080 (0.0289)	
training:	Epoch: [46][363/408]	Loss 0.0039 (0.0288)	
training:	Epoch: [46][364/408]	Loss 0.0061 (0.0288)	
training:	Epoch: [46][365/408]	Loss 0.0063 (0.0287)	
training:	Epoch: [46][366/408]	Loss 0.0057 (0.0287)	
training:	Epoch: [46][367/408]	Loss 0.0074 (0.0286)	
training:	Epoch: [46][368/408]	Loss 0.0083 (0.0285)	
training:	Epoch: [46][369/408]	Loss 0.0058 (0.0285)	
training:	Epoch: [46][370/408]	Loss 0.0068 (0.0284)	
training:	Epoch: [46][371/408]	Loss 0.0057 (0.0284)	
training:	Epoch: [46][372/408]	Loss 0.0094 (0.0283)	
training:	Epoch: [46][373/408]	Loss 0.0072 (0.0283)	
training:	Epoch: [46][374/408]	Loss 0.0051 (0.0282)	
training:	Epoch: [46][375/408]	Loss 0.0054 (0.0281)	
training:	Epoch: [46][376/408]	Loss 0.0073 (0.0281)	
training:	Epoch: [46][377/408]	Loss 0.0093 (0.0280)	
training:	Epoch: [46][378/408]	Loss 0.1983 (0.0285)	
training:	Epoch: [46][379/408]	Loss 0.0089 (0.0284)	
training:	Epoch: [46][380/408]	Loss 0.0066 (0.0284)	
training:	Epoch: [46][381/408]	Loss 0.0067 (0.0283)	
training:	Epoch: [46][382/408]	Loss 0.0051 (0.0282)	
training:	Epoch: [46][383/408]	Loss 0.0060 (0.0282)	
training:	Epoch: [46][384/408]	Loss 0.0054 (0.0281)	
training:	Epoch: [46][385/408]	Loss 0.0091 (0.0281)	
training:	Epoch: [46][386/408]	Loss 0.0074 (0.0280)	
training:	Epoch: [46][387/408]	Loss 0.0075 (0.0280)	
training:	Epoch: [46][388/408]	Loss 0.0069 (0.0279)	
training:	Epoch: [46][389/408]	Loss 0.0061 (0.0279)	
training:	Epoch: [46][390/408]	Loss 0.0065 (0.0278)	
training:	Epoch: [46][391/408]	Loss 0.0059 (0.0278)	
training:	Epoch: [46][392/408]	Loss 0.0095 (0.0277)	
training:	Epoch: [46][393/408]	Loss 0.0101 (0.0277)	
training:	Epoch: [46][394/408]	Loss 0.0247 (0.0277)	
training:	Epoch: [46][395/408]	Loss 0.0085 (0.0276)	
training:	Epoch: [46][396/408]	Loss 0.0084 (0.0276)	
training:	Epoch: [46][397/408]	Loss 0.0106 (0.0275)	
training:	Epoch: [46][398/408]	Loss 0.2090 (0.0280)	
training:	Epoch: [46][399/408]	Loss 0.0073 (0.0279)	
training:	Epoch: [46][400/408]	Loss 0.0048 (0.0279)	
training:	Epoch: [46][401/408]	Loss 0.0115 (0.0278)	
training:	Epoch: [46][402/408]	Loss 0.0075 (0.0278)	
training:	Epoch: [46][403/408]	Loss 0.0085 (0.0277)	
training:	Epoch: [46][404/408]	Loss 0.0055 (0.0277)	
training:	Epoch: [46][405/408]	Loss 0.0090 (0.0276)	
training:	Epoch: [46][406/408]	Loss 0.0049 (0.0276)	
training:	Epoch: [46][407/408]	Loss 0.0062 (0.0275)	
training:	Epoch: [46][408/408]	Loss 0.0069 (0.0275)	
Training:	 Loss: 0.0274

Training:	 ACC: 0.9947 0.9946 0.9941 0.9952
Validation:	 ACC: 0.7798 0.7796 0.7738 0.7859
Validation:	 Best_BACC: 0.8226 0.8250 0.8772 0.7679
Validation:	 Loss: 1.0269
Pretraining:	Epoch 47/200
----------
training:	Epoch: [47][1/408]	Loss 0.0159 (0.0159)	
training:	Epoch: [47][2/408]	Loss 0.0074 (0.0117)	
training:	Epoch: [47][3/408]	Loss 0.0071 (0.0101)	
training:	Epoch: [47][4/408]	Loss 0.2234 (0.0635)	
training:	Epoch: [47][5/408]	Loss 0.0063 (0.0520)	
training:	Epoch: [47][6/408]	Loss 0.0039 (0.0440)	
training:	Epoch: [47][7/408]	Loss 0.0077 (0.0388)	
training:	Epoch: [47][8/408]	Loss 0.0046 (0.0345)	
training:	Epoch: [47][9/408]	Loss 0.0066 (0.0314)	
training:	Epoch: [47][10/408]	Loss 0.0074 (0.0290)	
training:	Epoch: [47][11/408]	Loss 0.0048 (0.0268)	
training:	Epoch: [47][12/408]	Loss 0.0107 (0.0255)	
training:	Epoch: [47][13/408]	Loss 0.0098 (0.0243)	
training:	Epoch: [47][14/408]	Loss 0.0100 (0.0233)	
training:	Epoch: [47][15/408]	Loss 0.0080 (0.0222)	
training:	Epoch: [47][16/408]	Loss 0.0086 (0.0214)	
training:	Epoch: [47][17/408]	Loss 0.0105 (0.0207)	
training:	Epoch: [47][18/408]	Loss 0.0120 (0.0203)	
training:	Epoch: [47][19/408]	Loss 0.0055 (0.0195)	
training:	Epoch: [47][20/408]	Loss 0.0060 (0.0188)	
training:	Epoch: [47][21/408]	Loss 0.0117 (0.0185)	
training:	Epoch: [47][22/408]	Loss 0.0068 (0.0179)	
training:	Epoch: [47][23/408]	Loss 0.0049 (0.0174)	
training:	Epoch: [47][24/408]	Loss 0.0074 (0.0170)	
training:	Epoch: [47][25/408]	Loss 0.0049 (0.0165)	
training:	Epoch: [47][26/408]	Loss 0.0066 (0.0161)	
training:	Epoch: [47][27/408]	Loss 0.0075 (0.0158)	
training:	Epoch: [47][28/408]	Loss 0.0054 (0.0154)	
training:	Epoch: [47][29/408]	Loss 0.0040 (0.0150)	
training:	Epoch: [47][30/408]	Loss 0.0088 (0.0148)	
training:	Epoch: [47][31/408]	Loss 0.0036 (0.0145)	
training:	Epoch: [47][32/408]	Loss 0.0074 (0.0142)	
training:	Epoch: [47][33/408]	Loss 0.0047 (0.0139)	
training:	Epoch: [47][34/408]	Loss 0.3323 (0.0233)	
training:	Epoch: [47][35/408]	Loss 0.2203 (0.0289)	
training:	Epoch: [47][36/408]	Loss 0.0040 (0.0282)	
training:	Epoch: [47][37/408]	Loss 0.1550 (0.0317)	
training:	Epoch: [47][38/408]	Loss 0.0045 (0.0309)	
training:	Epoch: [47][39/408]	Loss 0.0050 (0.0303)	
training:	Epoch: [47][40/408]	Loss 0.0041 (0.0296)	
training:	Epoch: [47][41/408]	Loss 0.0047 (0.0290)	
training:	Epoch: [47][42/408]	Loss 0.0045 (0.0284)	
training:	Epoch: [47][43/408]	Loss 0.3106 (0.0350)	
training:	Epoch: [47][44/408]	Loss 0.0046 (0.0343)	
training:	Epoch: [47][45/408]	Loss 0.0051 (0.0337)	
training:	Epoch: [47][46/408]	Loss 0.0041 (0.0330)	
training:	Epoch: [47][47/408]	Loss 0.0056 (0.0324)	
training:	Epoch: [47][48/408]	Loss 0.0105 (0.0320)	
training:	Epoch: [47][49/408]	Loss 0.0086 (0.0315)	
training:	Epoch: [47][50/408]	Loss 0.0051 (0.0310)	
training:	Epoch: [47][51/408]	Loss 0.0047 (0.0305)	
training:	Epoch: [47][52/408]	Loss 0.0045 (0.0300)	
training:	Epoch: [47][53/408]	Loss 0.0073 (0.0295)	
training:	Epoch: [47][54/408]	Loss 0.0065 (0.0291)	
training:	Epoch: [47][55/408]	Loss 0.0081 (0.0287)	
training:	Epoch: [47][56/408]	Loss 0.2158 (0.0321)	
training:	Epoch: [47][57/408]	Loss 0.0074 (0.0316)	
training:	Epoch: [47][58/408]	Loss 0.0116 (0.0313)	
training:	Epoch: [47][59/408]	Loss 0.0091 (0.0309)	
training:	Epoch: [47][60/408]	Loss 0.0070 (0.0305)	
training:	Epoch: [47][61/408]	Loss 0.0064 (0.0301)	
training:	Epoch: [47][62/408]	Loss 0.0051 (0.0297)	
training:	Epoch: [47][63/408]	Loss 0.0080 (0.0294)	
training:	Epoch: [47][64/408]	Loss 0.0054 (0.0290)	
training:	Epoch: [47][65/408]	Loss 0.0079 (0.0287)	
training:	Epoch: [47][66/408]	Loss 0.0076 (0.0283)	
training:	Epoch: [47][67/408]	Loss 0.0048 (0.0280)	
training:	Epoch: [47][68/408]	Loss 0.0069 (0.0277)	
training:	Epoch: [47][69/408]	Loss 0.0060 (0.0274)	
training:	Epoch: [47][70/408]	Loss 0.0099 (0.0271)	
training:	Epoch: [47][71/408]	Loss 0.0052 (0.0268)	
training:	Epoch: [47][72/408]	Loss 0.0060 (0.0265)	
training:	Epoch: [47][73/408]	Loss 0.0051 (0.0262)	
training:	Epoch: [47][74/408]	Loss 0.0051 (0.0259)	
training:	Epoch: [47][75/408]	Loss 0.0042 (0.0257)	
training:	Epoch: [47][76/408]	Loss 0.0045 (0.0254)	
training:	Epoch: [47][77/408]	Loss 0.0046 (0.0251)	
training:	Epoch: [47][78/408]	Loss 0.0041 (0.0248)	
training:	Epoch: [47][79/408]	Loss 0.0084 (0.0246)	
training:	Epoch: [47][80/408]	Loss 0.3054 (0.0281)	
training:	Epoch: [47][81/408]	Loss 0.1835 (0.0301)	
training:	Epoch: [47][82/408]	Loss 0.0054 (0.0298)	
training:	Epoch: [47][83/408]	Loss 0.0106 (0.0295)	
training:	Epoch: [47][84/408]	Loss 0.0061 (0.0292)	
training:	Epoch: [47][85/408]	Loss 0.0073 (0.0290)	
training:	Epoch: [47][86/408]	Loss 0.0092 (0.0288)	
training:	Epoch: [47][87/408]	Loss 0.3092 (0.0320)	
training:	Epoch: [47][88/408]	Loss 0.0049 (0.0317)	
training:	Epoch: [47][89/408]	Loss 0.0059 (0.0314)	
training:	Epoch: [47][90/408]	Loss 0.2605 (0.0339)	
training:	Epoch: [47][91/408]	Loss 0.0079 (0.0336)	
training:	Epoch: [47][92/408]	Loss 0.0061 (0.0333)	
training:	Epoch: [47][93/408]	Loss 0.0062 (0.0330)	
training:	Epoch: [47][94/408]	Loss 0.0073 (0.0328)	
training:	Epoch: [47][95/408]	Loss 0.0102 (0.0325)	
training:	Epoch: [47][96/408]	Loss 0.0058 (0.0323)	
training:	Epoch: [47][97/408]	Loss 0.0096 (0.0320)	
training:	Epoch: [47][98/408]	Loss 0.0083 (0.0318)	
training:	Epoch: [47][99/408]	Loss 0.1974 (0.0335)	
training:	Epoch: [47][100/408]	Loss 0.0071 (0.0332)	
training:	Epoch: [47][101/408]	Loss 0.0065 (0.0329)	
training:	Epoch: [47][102/408]	Loss 0.0057 (0.0327)	
training:	Epoch: [47][103/408]	Loss 0.0067 (0.0324)	
training:	Epoch: [47][104/408]	Loss 0.0050 (0.0321)	
training:	Epoch: [47][105/408]	Loss 0.0080 (0.0319)	
training:	Epoch: [47][106/408]	Loss 0.0057 (0.0317)	
training:	Epoch: [47][107/408]	Loss 0.0053 (0.0314)	
training:	Epoch: [47][108/408]	Loss 0.0073 (0.0312)	
training:	Epoch: [47][109/408]	Loss 0.0041 (0.0310)	
training:	Epoch: [47][110/408]	Loss 0.0041 (0.0307)	
training:	Epoch: [47][111/408]	Loss 0.2810 (0.0330)	
training:	Epoch: [47][112/408]	Loss 0.0104 (0.0328)	
training:	Epoch: [47][113/408]	Loss 0.0122 (0.0326)	
training:	Epoch: [47][114/408]	Loss 0.0079 (0.0324)	
training:	Epoch: [47][115/408]	Loss 0.0094 (0.0322)	
training:	Epoch: [47][116/408]	Loss 0.0074 (0.0320)	
training:	Epoch: [47][117/408]	Loss 0.0063 (0.0317)	
training:	Epoch: [47][118/408]	Loss 0.0079 (0.0315)	
training:	Epoch: [47][119/408]	Loss 0.0094 (0.0313)	
training:	Epoch: [47][120/408]	Loss 0.0047 (0.0311)	
training:	Epoch: [47][121/408]	Loss 0.0084 (0.0309)	
training:	Epoch: [47][122/408]	Loss 0.0061 (0.0307)	
training:	Epoch: [47][123/408]	Loss 0.0060 (0.0305)	
training:	Epoch: [47][124/408]	Loss 0.0054 (0.0303)	
training:	Epoch: [47][125/408]	Loss 0.0076 (0.0301)	
training:	Epoch: [47][126/408]	Loss 0.2316 (0.0317)	
training:	Epoch: [47][127/408]	Loss 0.0122 (0.0316)	
training:	Epoch: [47][128/408]	Loss 0.0069 (0.0314)	
training:	Epoch: [47][129/408]	Loss 0.0069 (0.0312)	
training:	Epoch: [47][130/408]	Loss 0.0071 (0.0310)	
training:	Epoch: [47][131/408]	Loss 0.0205 (0.0309)	
training:	Epoch: [47][132/408]	Loss 0.0058 (0.0307)	
training:	Epoch: [47][133/408]	Loss 0.0078 (0.0306)	
training:	Epoch: [47][134/408]	Loss 0.0064 (0.0304)	
training:	Epoch: [47][135/408]	Loss 0.0044 (0.0302)	
training:	Epoch: [47][136/408]	Loss 0.0060 (0.0300)	
training:	Epoch: [47][137/408]	Loss 0.0081 (0.0299)	
training:	Epoch: [47][138/408]	Loss 0.0039 (0.0297)	
training:	Epoch: [47][139/408]	Loss 0.0072 (0.0295)	
training:	Epoch: [47][140/408]	Loss 0.0058 (0.0293)	
training:	Epoch: [47][141/408]	Loss 0.0064 (0.0292)	
training:	Epoch: [47][142/408]	Loss 0.0065 (0.0290)	
training:	Epoch: [47][143/408]	Loss 0.0083 (0.0289)	
training:	Epoch: [47][144/408]	Loss 0.0071 (0.0287)	
training:	Epoch: [47][145/408]	Loss 0.0036 (0.0286)	
training:	Epoch: [47][146/408]	Loss 0.2896 (0.0303)	
training:	Epoch: [47][147/408]	Loss 0.2354 (0.0317)	
training:	Epoch: [47][148/408]	Loss 0.0060 (0.0316)	
training:	Epoch: [47][149/408]	Loss 0.0069 (0.0314)	
training:	Epoch: [47][150/408]	Loss 0.0061 (0.0312)	
training:	Epoch: [47][151/408]	Loss 0.2673 (0.0328)	
training:	Epoch: [47][152/408]	Loss 0.0066 (0.0326)	
training:	Epoch: [47][153/408]	Loss 0.2581 (0.0341)	
training:	Epoch: [47][154/408]	Loss 0.0077 (0.0339)	
training:	Epoch: [47][155/408]	Loss 0.0098 (0.0338)	
training:	Epoch: [47][156/408]	Loss 0.0062 (0.0336)	
training:	Epoch: [47][157/408]	Loss 0.0058 (0.0334)	
training:	Epoch: [47][158/408]	Loss 0.0064 (0.0332)	
training:	Epoch: [47][159/408]	Loss 0.0067 (0.0331)	
training:	Epoch: [47][160/408]	Loss 0.0083 (0.0329)	
training:	Epoch: [47][161/408]	Loss 0.0105 (0.0328)	
training:	Epoch: [47][162/408]	Loss 0.0038 (0.0326)	
training:	Epoch: [47][163/408]	Loss 0.0162 (0.0325)	
training:	Epoch: [47][164/408]	Loss 0.0068 (0.0323)	
training:	Epoch: [47][165/408]	Loss 0.0051 (0.0322)	
training:	Epoch: [47][166/408]	Loss 0.0103 (0.0321)	
training:	Epoch: [47][167/408]	Loss 0.0077 (0.0319)	
training:	Epoch: [47][168/408]	Loss 0.0076 (0.0318)	
training:	Epoch: [47][169/408]	Loss 0.0065 (0.0316)	
training:	Epoch: [47][170/408]	Loss 0.2459 (0.0329)	
training:	Epoch: [47][171/408]	Loss 0.0073 (0.0327)	
training:	Epoch: [47][172/408]	Loss 0.0083 (0.0326)	
training:	Epoch: [47][173/408]	Loss 0.0056 (0.0324)	
training:	Epoch: [47][174/408]	Loss 0.0086 (0.0323)	
training:	Epoch: [47][175/408]	Loss 0.0094 (0.0322)	
training:	Epoch: [47][176/408]	Loss 0.0088 (0.0320)	
training:	Epoch: [47][177/408]	Loss 0.0060 (0.0319)	
training:	Epoch: [47][178/408]	Loss 0.0062 (0.0317)	
training:	Epoch: [47][179/408]	Loss 0.0115 (0.0316)	
training:	Epoch: [47][180/408]	Loss 0.0057 (0.0315)	
training:	Epoch: [47][181/408]	Loss 0.0060 (0.0313)	
training:	Epoch: [47][182/408]	Loss 0.0060 (0.0312)	
training:	Epoch: [47][183/408]	Loss 0.0052 (0.0311)	
training:	Epoch: [47][184/408]	Loss 0.0054 (0.0309)	
training:	Epoch: [47][185/408]	Loss 0.0078 (0.0308)	
training:	Epoch: [47][186/408]	Loss 0.0106 (0.0307)	
training:	Epoch: [47][187/408]	Loss 0.0080 (0.0306)	
training:	Epoch: [47][188/408]	Loss 0.0092 (0.0304)	
training:	Epoch: [47][189/408]	Loss 0.0069 (0.0303)	
training:	Epoch: [47][190/408]	Loss 0.0062 (0.0302)	
training:	Epoch: [47][191/408]	Loss 0.0117 (0.0301)	
training:	Epoch: [47][192/408]	Loss 0.0057 (0.0300)	
training:	Epoch: [47][193/408]	Loss 0.0092 (0.0299)	
training:	Epoch: [47][194/408]	Loss 0.0066 (0.0297)	
training:	Epoch: [47][195/408]	Loss 0.0045 (0.0296)	
training:	Epoch: [47][196/408]	Loss 0.0056 (0.0295)	
training:	Epoch: [47][197/408]	Loss 0.0108 (0.0294)	
training:	Epoch: [47][198/408]	Loss 0.0071 (0.0293)	
training:	Epoch: [47][199/408]	Loss 0.0084 (0.0292)	
training:	Epoch: [47][200/408]	Loss 0.0060 (0.0291)	
training:	Epoch: [47][201/408]	Loss 0.0083 (0.0290)	
training:	Epoch: [47][202/408]	Loss 0.0091 (0.0289)	
training:	Epoch: [47][203/408]	Loss 0.0073 (0.0288)	
training:	Epoch: [47][204/408]	Loss 0.2307 (0.0297)	
training:	Epoch: [47][205/408]	Loss 0.0059 (0.0296)	
training:	Epoch: [47][206/408]	Loss 0.0076 (0.0295)	
training:	Epoch: [47][207/408]	Loss 0.0065 (0.0294)	
training:	Epoch: [47][208/408]	Loss 0.0066 (0.0293)	
training:	Epoch: [47][209/408]	Loss 0.0066 (0.0292)	
training:	Epoch: [47][210/408]	Loss 0.0064 (0.0291)	
training:	Epoch: [47][211/408]	Loss 0.0038 (0.0290)	
training:	Epoch: [47][212/408]	Loss 0.0061 (0.0289)	
training:	Epoch: [47][213/408]	Loss 0.0044 (0.0287)	
training:	Epoch: [47][214/408]	Loss 0.0068 (0.0286)	
training:	Epoch: [47][215/408]	Loss 0.0061 (0.0285)	
training:	Epoch: [47][216/408]	Loss 0.0054 (0.0284)	
training:	Epoch: [47][217/408]	Loss 0.0073 (0.0283)	
training:	Epoch: [47][218/408]	Loss 0.0047 (0.0282)	
training:	Epoch: [47][219/408]	Loss 0.0076 (0.0281)	
training:	Epoch: [47][220/408]	Loss 0.0056 (0.0280)	
training:	Epoch: [47][221/408]	Loss 0.0049 (0.0279)	
training:	Epoch: [47][222/408]	Loss 0.0054 (0.0278)	
training:	Epoch: [47][223/408]	Loss 0.0054 (0.0277)	
training:	Epoch: [47][224/408]	Loss 0.0057 (0.0276)	
training:	Epoch: [47][225/408]	Loss 0.0055 (0.0275)	
training:	Epoch: [47][226/408]	Loss 0.0060 (0.0274)	
training:	Epoch: [47][227/408]	Loss 0.0067 (0.0273)	
training:	Epoch: [47][228/408]	Loss 0.0070 (0.0272)	
training:	Epoch: [47][229/408]	Loss 0.0034 (0.0271)	
training:	Epoch: [47][230/408]	Loss 0.0055 (0.0270)	
training:	Epoch: [47][231/408]	Loss 0.0075 (0.0270)	
training:	Epoch: [47][232/408]	Loss 0.0038 (0.0269)	
training:	Epoch: [47][233/408]	Loss 0.0057 (0.0268)	
training:	Epoch: [47][234/408]	Loss 0.0067 (0.0267)	
training:	Epoch: [47][235/408]	Loss 0.2758 (0.0277)	
training:	Epoch: [47][236/408]	Loss 0.0040 (0.0276)	
training:	Epoch: [47][237/408]	Loss 0.0052 (0.0275)	
training:	Epoch: [47][238/408]	Loss 0.0077 (0.0275)	
training:	Epoch: [47][239/408]	Loss 0.0056 (0.0274)	
training:	Epoch: [47][240/408]	Loss 0.0046 (0.0273)	
training:	Epoch: [47][241/408]	Loss 0.0056 (0.0272)	
training:	Epoch: [47][242/408]	Loss 0.0064 (0.0271)	
training:	Epoch: [47][243/408]	Loss 0.2461 (0.0280)	
training:	Epoch: [47][244/408]	Loss 0.0050 (0.0279)	
training:	Epoch: [47][245/408]	Loss 0.0085 (0.0278)	
training:	Epoch: [47][246/408]	Loss 0.0058 (0.0277)	
training:	Epoch: [47][247/408]	Loss 0.0059 (0.0277)	
training:	Epoch: [47][248/408]	Loss 0.0086 (0.0276)	
training:	Epoch: [47][249/408]	Loss 0.0064 (0.0275)	
training:	Epoch: [47][250/408]	Loss 0.0049 (0.0274)	
training:	Epoch: [47][251/408]	Loss 0.0039 (0.0273)	
training:	Epoch: [47][252/408]	Loss 0.0053 (0.0272)	
training:	Epoch: [47][253/408]	Loss 0.0043 (0.0271)	
training:	Epoch: [47][254/408]	Loss 0.0060 (0.0270)	
training:	Epoch: [47][255/408]	Loss 0.0069 (0.0270)	
training:	Epoch: [47][256/408]	Loss 0.0056 (0.0269)	
training:	Epoch: [47][257/408]	Loss 0.0059 (0.0268)	
training:	Epoch: [47][258/408]	Loss 0.2272 (0.0276)	
training:	Epoch: [47][259/408]	Loss 0.0055 (0.0275)	
training:	Epoch: [47][260/408]	Loss 0.0070 (0.0274)	
training:	Epoch: [47][261/408]	Loss 0.0052 (0.0273)	
training:	Epoch: [47][262/408]	Loss 0.0046 (0.0272)	
training:	Epoch: [47][263/408]	Loss 0.3632 (0.0285)	
training:	Epoch: [47][264/408]	Loss 0.0065 (0.0284)	
training:	Epoch: [47][265/408]	Loss 0.0056 (0.0284)	
training:	Epoch: [47][266/408]	Loss 0.0051 (0.0283)	
training:	Epoch: [47][267/408]	Loss 0.0055 (0.0282)	
training:	Epoch: [47][268/408]	Loss 0.0041 (0.0281)	
training:	Epoch: [47][269/408]	Loss 0.0047 (0.0280)	
training:	Epoch: [47][270/408]	Loss 0.0055 (0.0279)	
training:	Epoch: [47][271/408]	Loss 0.0106 (0.0279)	
training:	Epoch: [47][272/408]	Loss 0.0108 (0.0278)	
training:	Epoch: [47][273/408]	Loss 0.0055 (0.0277)	
training:	Epoch: [47][274/408]	Loss 0.0069 (0.0276)	
training:	Epoch: [47][275/408]	Loss 0.0068 (0.0276)	
training:	Epoch: [47][276/408]	Loss 0.2204 (0.0283)	
training:	Epoch: [47][277/408]	Loss 0.0070 (0.0282)	
training:	Epoch: [47][278/408]	Loss 0.0052 (0.0281)	
training:	Epoch: [47][279/408]	Loss 0.0070 (0.0280)	
training:	Epoch: [47][280/408]	Loss 0.0092 (0.0280)	
training:	Epoch: [47][281/408]	Loss 0.0367 (0.0280)	
training:	Epoch: [47][282/408]	Loss 0.0051 (0.0279)	
training:	Epoch: [47][283/408]	Loss 0.0055 (0.0278)	
training:	Epoch: [47][284/408]	Loss 0.0089 (0.0278)	
training:	Epoch: [47][285/408]	Loss 0.0058 (0.0277)	
training:	Epoch: [47][286/408]	Loss 0.0054 (0.0276)	
training:	Epoch: [47][287/408]	Loss 0.0051 (0.0275)	
training:	Epoch: [47][288/408]	Loss 0.0103 (0.0275)	
training:	Epoch: [47][289/408]	Loss 0.0058 (0.0274)	
training:	Epoch: [47][290/408]	Loss 0.2293 (0.0281)	
training:	Epoch: [47][291/408]	Loss 0.2264 (0.0288)	
training:	Epoch: [47][292/408]	Loss 0.0065 (0.0287)	
training:	Epoch: [47][293/408]	Loss 0.0059 (0.0286)	
training:	Epoch: [47][294/408]	Loss 0.0058 (0.0285)	
training:	Epoch: [47][295/408]	Loss 0.0038 (0.0285)	
training:	Epoch: [47][296/408]	Loss 0.0085 (0.0284)	
training:	Epoch: [47][297/408]	Loss 0.0048 (0.0283)	
training:	Epoch: [47][298/408]	Loss 0.0535 (0.0284)	
training:	Epoch: [47][299/408]	Loss 0.0049 (0.0283)	
training:	Epoch: [47][300/408]	Loss 0.0192 (0.0283)	
training:	Epoch: [47][301/408]	Loss 0.1865 (0.0288)	
training:	Epoch: [47][302/408]	Loss 0.3320 (0.0298)	
training:	Epoch: [47][303/408]	Loss 0.0104 (0.0297)	
training:	Epoch: [47][304/408]	Loss 0.0054 (0.0297)	
training:	Epoch: [47][305/408]	Loss 0.0095 (0.0296)	
training:	Epoch: [47][306/408]	Loss 0.0060 (0.0295)	
training:	Epoch: [47][307/408]	Loss 0.0076 (0.0295)	
training:	Epoch: [47][308/408]	Loss 0.0144 (0.0294)	
training:	Epoch: [47][309/408]	Loss 0.0075 (0.0293)	
training:	Epoch: [47][310/408]	Loss 0.0070 (0.0293)	
training:	Epoch: [47][311/408]	Loss 0.0121 (0.0292)	
training:	Epoch: [47][312/408]	Loss 0.0092 (0.0291)	
training:	Epoch: [47][313/408]	Loss 0.0080 (0.0291)	
training:	Epoch: [47][314/408]	Loss 0.1827 (0.0296)	
training:	Epoch: [47][315/408]	Loss 0.0059 (0.0295)	
training:	Epoch: [47][316/408]	Loss 0.0068 (0.0294)	
training:	Epoch: [47][317/408]	Loss 0.0097 (0.0294)	
training:	Epoch: [47][318/408]	Loss 0.0088 (0.0293)	
training:	Epoch: [47][319/408]	Loss 0.0044 (0.0292)	
training:	Epoch: [47][320/408]	Loss 0.0060 (0.0291)	
training:	Epoch: [47][321/408]	Loss 0.0072 (0.0291)	
training:	Epoch: [47][322/408]	Loss 0.0069 (0.0290)	
training:	Epoch: [47][323/408]	Loss 0.0037 (0.0289)	
training:	Epoch: [47][324/408]	Loss 0.0074 (0.0289)	
training:	Epoch: [47][325/408]	Loss 0.0072 (0.0288)	
training:	Epoch: [47][326/408]	Loss 0.2035 (0.0293)	
training:	Epoch: [47][327/408]	Loss 0.0063 (0.0293)	
training:	Epoch: [47][328/408]	Loss 0.0063 (0.0292)	
training:	Epoch: [47][329/408]	Loss 0.2461 (0.0298)	
training:	Epoch: [47][330/408]	Loss 0.0069 (0.0298)	
training:	Epoch: [47][331/408]	Loss 0.0102 (0.0297)	
training:	Epoch: [47][332/408]	Loss 0.0069 (0.0296)	
training:	Epoch: [47][333/408]	Loss 0.0061 (0.0296)	
training:	Epoch: [47][334/408]	Loss 0.0078 (0.0295)	
training:	Epoch: [47][335/408]	Loss 0.0038 (0.0294)	
training:	Epoch: [47][336/408]	Loss 0.0080 (0.0294)	
training:	Epoch: [47][337/408]	Loss 0.0078 (0.0293)	
training:	Epoch: [47][338/408]	Loss 0.0073 (0.0292)	
training:	Epoch: [47][339/408]	Loss 0.0048 (0.0292)	
training:	Epoch: [47][340/408]	Loss 0.0070 (0.0291)	
training:	Epoch: [47][341/408]	Loss 0.0051 (0.0290)	
training:	Epoch: [47][342/408]	Loss 0.0091 (0.0290)	
training:	Epoch: [47][343/408]	Loss 0.0064 (0.0289)	
training:	Epoch: [47][344/408]	Loss 0.0046 (0.0288)	
training:	Epoch: [47][345/408]	Loss 0.0087 (0.0288)	
training:	Epoch: [47][346/408]	Loss 0.0112 (0.0287)	
training:	Epoch: [47][347/408]	Loss 0.2599 (0.0294)	
training:	Epoch: [47][348/408]	Loss 0.0044 (0.0293)	
training:	Epoch: [47][349/408]	Loss 0.0074 (0.0293)	
training:	Epoch: [47][350/408]	Loss 0.0074 (0.0292)	
training:	Epoch: [47][351/408]	Loss 0.0067 (0.0291)	
training:	Epoch: [47][352/408]	Loss 0.2073 (0.0296)	
training:	Epoch: [47][353/408]	Loss 0.0068 (0.0296)	
training:	Epoch: [47][354/408]	Loss 0.0065 (0.0295)	
training:	Epoch: [47][355/408]	Loss 0.0119 (0.0295)	
training:	Epoch: [47][356/408]	Loss 0.0058 (0.0294)	
training:	Epoch: [47][357/408]	Loss 0.0075 (0.0293)	
training:	Epoch: [47][358/408]	Loss 0.0142 (0.0293)	
training:	Epoch: [47][359/408]	Loss 0.0044 (0.0292)	
training:	Epoch: [47][360/408]	Loss 0.1893 (0.0297)	
training:	Epoch: [47][361/408]	Loss 0.0093 (0.0296)	
training:	Epoch: [47][362/408]	Loss 0.0080 (0.0296)	
training:	Epoch: [47][363/408]	Loss 0.0115 (0.0295)	
training:	Epoch: [47][364/408]	Loss 0.0041 (0.0294)	
training:	Epoch: [47][365/408]	Loss 0.0051 (0.0294)	
training:	Epoch: [47][366/408]	Loss 0.0067 (0.0293)	
training:	Epoch: [47][367/408]	Loss 0.0060 (0.0292)	
training:	Epoch: [47][368/408]	Loss 0.0206 (0.0292)	
training:	Epoch: [47][369/408]	Loss 0.0134 (0.0292)	
training:	Epoch: [47][370/408]	Loss 0.0106 (0.0291)	
training:	Epoch: [47][371/408]	Loss 0.0099 (0.0291)	
training:	Epoch: [47][372/408]	Loss 0.0097 (0.0290)	
training:	Epoch: [47][373/408]	Loss 0.0067 (0.0290)	
training:	Epoch: [47][374/408]	Loss 0.0074 (0.0289)	
training:	Epoch: [47][375/408]	Loss 0.0053 (0.0288)	
training:	Epoch: [47][376/408]	Loss 0.0067 (0.0288)	
training:	Epoch: [47][377/408]	Loss 0.0065 (0.0287)	
training:	Epoch: [47][378/408]	Loss 0.0094 (0.0287)	
training:	Epoch: [47][379/408]	Loss 0.0100 (0.0286)	
training:	Epoch: [47][380/408]	Loss 0.0068 (0.0286)	
training:	Epoch: [47][381/408]	Loss 0.1591 (0.0289)	
training:	Epoch: [47][382/408]	Loss 0.0050 (0.0288)	
training:	Epoch: [47][383/408]	Loss 0.0060 (0.0288)	
training:	Epoch: [47][384/408]	Loss 0.0054 (0.0287)	
training:	Epoch: [47][385/408]	Loss 0.0061 (0.0287)	
training:	Epoch: [47][386/408]	Loss 0.0152 (0.0286)	
training:	Epoch: [47][387/408]	Loss 0.0331 (0.0286)	
training:	Epoch: [47][388/408]	Loss 0.0047 (0.0286)	
training:	Epoch: [47][389/408]	Loss 0.0047 (0.0285)	
training:	Epoch: [47][390/408]	Loss 0.0046 (0.0285)	
training:	Epoch: [47][391/408]	Loss 0.0109 (0.0284)	
training:	Epoch: [47][392/408]	Loss 0.0058 (0.0284)	
training:	Epoch: [47][393/408]	Loss 0.0085 (0.0283)	
training:	Epoch: [47][394/408]	Loss 0.1744 (0.0287)	
training:	Epoch: [47][395/408]	Loss 0.0127 (0.0286)	
training:	Epoch: [47][396/408]	Loss 0.0074 (0.0286)	
training:	Epoch: [47][397/408]	Loss 0.0039 (0.0285)	
training:	Epoch: [47][398/408]	Loss 0.0059 (0.0285)	
training:	Epoch: [47][399/408]	Loss 0.0083 (0.0284)	
training:	Epoch: [47][400/408]	Loss 0.0079 (0.0284)	
training:	Epoch: [47][401/408]	Loss 0.0052 (0.0283)	
training:	Epoch: [47][402/408]	Loss 0.0060 (0.0282)	
training:	Epoch: [47][403/408]	Loss 0.0053 (0.0282)	
training:	Epoch: [47][404/408]	Loss 0.0067 (0.0281)	
training:	Epoch: [47][405/408]	Loss 0.0084 (0.0281)	
training:	Epoch: [47][406/408]	Loss 0.0102 (0.0280)	
training:	Epoch: [47][407/408]	Loss 0.0119 (0.0280)	
training:	Epoch: [47][408/408]	Loss 0.0045 (0.0279)	
Training:	 Loss: 0.0279

Training:	 ACC: 0.9944 0.9943 0.9935 0.9952
Validation:	 ACC: 0.7801 0.7796 0.7687 0.7915
Validation:	 Best_BACC: 0.8226 0.8250 0.8772 0.7679
Validation:	 Loss: 1.0272
Pretraining:	Epoch 48/200
----------
training:	Epoch: [48][1/408]	Loss 0.0060 (0.0060)	
training:	Epoch: [48][2/408]	Loss 0.0078 (0.0069)	
training:	Epoch: [48][3/408]	Loss 0.0049 (0.0063)	
training:	Epoch: [48][4/408]	Loss 0.0055 (0.0061)	
training:	Epoch: [48][5/408]	Loss 0.0060 (0.0061)	
training:	Epoch: [48][6/408]	Loss 0.0046 (0.0058)	
training:	Epoch: [48][7/408]	Loss 0.5067 (0.0774)	
training:	Epoch: [48][8/408]	Loss 0.0083 (0.0687)	
training:	Epoch: [48][9/408]	Loss 0.0036 (0.0615)	
training:	Epoch: [48][10/408]	Loss 0.0064 (0.0560)	
training:	Epoch: [48][11/408]	Loss 0.2985 (0.0780)	
training:	Epoch: [48][12/408]	Loss 0.0047 (0.0719)	
training:	Epoch: [48][13/408]	Loss 0.2374 (0.0847)	
training:	Epoch: [48][14/408]	Loss 0.0082 (0.0792)	
training:	Epoch: [48][15/408]	Loss 0.0061 (0.0743)	
training:	Epoch: [48][16/408]	Loss 0.2231 (0.0836)	
training:	Epoch: [48][17/408]	Loss 0.0087 (0.0792)	
training:	Epoch: [48][18/408]	Loss 0.0057 (0.0751)	
training:	Epoch: [48][19/408]	Loss 0.0066 (0.0715)	
training:	Epoch: [48][20/408]	Loss 0.0065 (0.0683)	
training:	Epoch: [48][21/408]	Loss 0.2065 (0.0749)	
training:	Epoch: [48][22/408]	Loss 0.0064 (0.0718)	
training:	Epoch: [48][23/408]	Loss 0.0097 (0.0691)	
training:	Epoch: [48][24/408]	Loss 0.2474 (0.0765)	
training:	Epoch: [48][25/408]	Loss 0.0042 (0.0736)	
training:	Epoch: [48][26/408]	Loss 0.0243 (0.0717)	
training:	Epoch: [48][27/408]	Loss 0.0192 (0.0698)	
training:	Epoch: [48][28/408]	Loss 0.0083 (0.0676)	
training:	Epoch: [48][29/408]	Loss 0.0055 (0.0654)	
training:	Epoch: [48][30/408]	Loss 0.0120 (0.0636)	
training:	Epoch: [48][31/408]	Loss 0.0082 (0.0618)	
training:	Epoch: [48][32/408]	Loss 0.0097 (0.0602)	
training:	Epoch: [48][33/408]	Loss 0.0075 (0.0586)	
training:	Epoch: [48][34/408]	Loss 0.0082 (0.0571)	
training:	Epoch: [48][35/408]	Loss 0.0039 (0.0556)	
training:	Epoch: [48][36/408]	Loss 0.0048 (0.0542)	
training:	Epoch: [48][37/408]	Loss 0.0127 (0.0531)	
training:	Epoch: [48][38/408]	Loss 0.0097 (0.0519)	
training:	Epoch: [48][39/408]	Loss 0.0120 (0.0509)	
training:	Epoch: [48][40/408]	Loss 0.0213 (0.0502)	
training:	Epoch: [48][41/408]	Loss 0.0045 (0.0491)	
training:	Epoch: [48][42/408]	Loss 0.0137 (0.0482)	
training:	Epoch: [48][43/408]	Loss 0.0042 (0.0472)	
training:	Epoch: [48][44/408]	Loss 0.0110 (0.0464)	
training:	Epoch: [48][45/408]	Loss 0.0110 (0.0456)	
training:	Epoch: [48][46/408]	Loss 0.0090 (0.0448)	
training:	Epoch: [48][47/408]	Loss 0.1476 (0.0470)	
training:	Epoch: [48][48/408]	Loss 0.0062 (0.0461)	
training:	Epoch: [48][49/408]	Loss 0.0127 (0.0454)	
training:	Epoch: [48][50/408]	Loss 0.0045 (0.0446)	
training:	Epoch: [48][51/408]	Loss 0.0059 (0.0439)	
training:	Epoch: [48][52/408]	Loss 0.0050 (0.0431)	
training:	Epoch: [48][53/408]	Loss 0.0037 (0.0424)	
training:	Epoch: [48][54/408]	Loss 0.0079 (0.0417)	
training:	Epoch: [48][55/408]	Loss 0.0040 (0.0411)	
training:	Epoch: [48][56/408]	Loss 0.0085 (0.0405)	
training:	Epoch: [48][57/408]	Loss 0.0080 (0.0399)	
training:	Epoch: [48][58/408]	Loss 0.0089 (0.0394)	
training:	Epoch: [48][59/408]	Loss 0.0064 (0.0388)	
training:	Epoch: [48][60/408]	Loss 0.0053 (0.0383)	
training:	Epoch: [48][61/408]	Loss 0.0046 (0.0377)	
training:	Epoch: [48][62/408]	Loss 0.0074 (0.0372)	
training:	Epoch: [48][63/408]	Loss 0.0049 (0.0367)	
training:	Epoch: [48][64/408]	Loss 0.0048 (0.0362)	
training:	Epoch: [48][65/408]	Loss 0.0058 (0.0357)	
training:	Epoch: [48][66/408]	Loss 0.0064 (0.0353)	
training:	Epoch: [48][67/408]	Loss 0.0088 (0.0349)	
training:	Epoch: [48][68/408]	Loss 0.0135 (0.0346)	
training:	Epoch: [48][69/408]	Loss 0.0059 (0.0342)	
training:	Epoch: [48][70/408]	Loss 0.2902 (0.0378)	
training:	Epoch: [48][71/408]	Loss 0.0054 (0.0374)	
training:	Epoch: [48][72/408]	Loss 0.0055 (0.0369)	
training:	Epoch: [48][73/408]	Loss 0.0069 (0.0365)	
training:	Epoch: [48][74/408]	Loss 0.0053 (0.0361)	
training:	Epoch: [48][75/408]	Loss 0.0064 (0.0357)	
training:	Epoch: [48][76/408]	Loss 0.0065 (0.0353)	
training:	Epoch: [48][77/408]	Loss 0.0084 (0.0350)	
training:	Epoch: [48][78/408]	Loss 0.0065 (0.0346)	
training:	Epoch: [48][79/408]	Loss 0.0072 (0.0342)	
training:	Epoch: [48][80/408]	Loss 0.0068 (0.0339)	
training:	Epoch: [48][81/408]	Loss 0.2354 (0.0364)	
training:	Epoch: [48][82/408]	Loss 0.0085 (0.0361)	
training:	Epoch: [48][83/408]	Loss 0.0056 (0.0357)	
training:	Epoch: [48][84/408]	Loss 0.0050 (0.0353)	
training:	Epoch: [48][85/408]	Loss 0.0070 (0.0350)	
training:	Epoch: [48][86/408]	Loss 0.0066 (0.0347)	
training:	Epoch: [48][87/408]	Loss 0.0085 (0.0344)	
training:	Epoch: [48][88/408]	Loss 0.0047 (0.0340)	
training:	Epoch: [48][89/408]	Loss 0.0054 (0.0337)	
training:	Epoch: [48][90/408]	Loss 0.0060 (0.0334)	
training:	Epoch: [48][91/408]	Loss 0.0038 (0.0331)	
training:	Epoch: [48][92/408]	Loss 0.0078 (0.0328)	
training:	Epoch: [48][93/408]	Loss 0.0044 (0.0325)	
training:	Epoch: [48][94/408]	Loss 0.0045 (0.0322)	
training:	Epoch: [48][95/408]	Loss 0.0068 (0.0319)	
training:	Epoch: [48][96/408]	Loss 0.0077 (0.0317)	
training:	Epoch: [48][97/408]	Loss 0.0061 (0.0314)	
training:	Epoch: [48][98/408]	Loss 0.0043 (0.0311)	
training:	Epoch: [48][99/408]	Loss 0.0070 (0.0309)	
training:	Epoch: [48][100/408]	Loss 0.0156 (0.0307)	
training:	Epoch: [48][101/408]	Loss 0.0077 (0.0305)	
training:	Epoch: [48][102/408]	Loss 0.0053 (0.0303)	
training:	Epoch: [48][103/408]	Loss 0.3059 (0.0329)	
training:	Epoch: [48][104/408]	Loss 0.0065 (0.0327)	
training:	Epoch: [48][105/408]	Loss 0.0046 (0.0324)	
training:	Epoch: [48][106/408]	Loss 0.0063 (0.0322)	
training:	Epoch: [48][107/408]	Loss 0.2271 (0.0340)	
training:	Epoch: [48][108/408]	Loss 0.0062 (0.0337)	
training:	Epoch: [48][109/408]	Loss 0.0061 (0.0335)	
training:	Epoch: [48][110/408]	Loss 0.0062 (0.0332)	
training:	Epoch: [48][111/408]	Loss 0.0043 (0.0330)	
training:	Epoch: [48][112/408]	Loss 0.0046 (0.0327)	
training:	Epoch: [48][113/408]	Loss 0.0058 (0.0325)	
training:	Epoch: [48][114/408]	Loss 0.0053 (0.0322)	
training:	Epoch: [48][115/408]	Loss 0.0071 (0.0320)	
training:	Epoch: [48][116/408]	Loss 0.0064 (0.0318)	
training:	Epoch: [48][117/408]	Loss 0.0055 (0.0316)	
training:	Epoch: [48][118/408]	Loss 0.0081 (0.0314)	
training:	Epoch: [48][119/408]	Loss 0.0041 (0.0311)	
training:	Epoch: [48][120/408]	Loss 0.0095 (0.0310)	
training:	Epoch: [48][121/408]	Loss 0.0102 (0.0308)	
training:	Epoch: [48][122/408]	Loss 0.2154 (0.0323)	
training:	Epoch: [48][123/408]	Loss 0.0059 (0.0321)	
training:	Epoch: [48][124/408]	Loss 0.0063 (0.0319)	
training:	Epoch: [48][125/408]	Loss 0.0151 (0.0317)	
training:	Epoch: [48][126/408]	Loss 0.0091 (0.0316)	
training:	Epoch: [48][127/408]	Loss 0.0085 (0.0314)	
training:	Epoch: [48][128/408]	Loss 0.0099 (0.0312)	
training:	Epoch: [48][129/408]	Loss 0.0037 (0.0310)	
training:	Epoch: [48][130/408]	Loss 0.0053 (0.0308)	
training:	Epoch: [48][131/408]	Loss 0.0055 (0.0306)	
training:	Epoch: [48][132/408]	Loss 0.0042 (0.0304)	
training:	Epoch: [48][133/408]	Loss 0.0054 (0.0302)	
training:	Epoch: [48][134/408]	Loss 0.0173 (0.0301)	
training:	Epoch: [48][135/408]	Loss 0.0053 (0.0299)	
training:	Epoch: [48][136/408]	Loss 0.0089 (0.0298)	
training:	Epoch: [48][137/408]	Loss 0.0063 (0.0296)	
training:	Epoch: [48][138/408]	Loss 0.0058 (0.0294)	
training:	Epoch: [48][139/408]	Loss 0.0037 (0.0293)	
training:	Epoch: [48][140/408]	Loss 0.0065 (0.0291)	
training:	Epoch: [48][141/408]	Loss 0.0039 (0.0289)	
training:	Epoch: [48][142/408]	Loss 0.2631 (0.0306)	
training:	Epoch: [48][143/408]	Loss 0.0090 (0.0304)	
training:	Epoch: [48][144/408]	Loss 0.0086 (0.0303)	
training:	Epoch: [48][145/408]	Loss 0.0072 (0.0301)	
training:	Epoch: [48][146/408]	Loss 0.0056 (0.0299)	
training:	Epoch: [48][147/408]	Loss 0.0060 (0.0298)	
training:	Epoch: [48][148/408]	Loss 0.0051 (0.0296)	
training:	Epoch: [48][149/408]	Loss 0.0044 (0.0294)	
training:	Epoch: [48][150/408]	Loss 0.0044 (0.0293)	
training:	Epoch: [48][151/408]	Loss 0.0080 (0.0291)	
training:	Epoch: [48][152/408]	Loss 0.0043 (0.0290)	
training:	Epoch: [48][153/408]	Loss 0.0076 (0.0288)	
training:	Epoch: [48][154/408]	Loss 0.0095 (0.0287)	
training:	Epoch: [48][155/408]	Loss 0.0099 (0.0286)	
training:	Epoch: [48][156/408]	Loss 0.0052 (0.0284)	
training:	Epoch: [48][157/408]	Loss 0.0062 (0.0283)	
training:	Epoch: [48][158/408]	Loss 0.0038 (0.0281)	
training:	Epoch: [48][159/408]	Loss 0.0089 (0.0280)	
training:	Epoch: [48][160/408]	Loss 0.0079 (0.0279)	
training:	Epoch: [48][161/408]	Loss 0.0069 (0.0278)	
training:	Epoch: [48][162/408]	Loss 0.0055 (0.0276)	
training:	Epoch: [48][163/408]	Loss 0.0038 (0.0275)	
training:	Epoch: [48][164/408]	Loss 0.0047 (0.0273)	
training:	Epoch: [48][165/408]	Loss 0.0069 (0.0272)	
training:	Epoch: [48][166/408]	Loss 0.0043 (0.0271)	
training:	Epoch: [48][167/408]	Loss 0.0066 (0.0270)	
training:	Epoch: [48][168/408]	Loss 0.2283 (0.0282)	
training:	Epoch: [48][169/408]	Loss 0.0066 (0.0280)	
training:	Epoch: [48][170/408]	Loss 0.0065 (0.0279)	
training:	Epoch: [48][171/408]	Loss 0.0039 (0.0278)	
training:	Epoch: [48][172/408]	Loss 0.0055 (0.0276)	
training:	Epoch: [48][173/408]	Loss 0.0049 (0.0275)	
training:	Epoch: [48][174/408]	Loss 0.0034 (0.0274)	
training:	Epoch: [48][175/408]	Loss 0.0055 (0.0272)	
training:	Epoch: [48][176/408]	Loss 0.0050 (0.0271)	
training:	Epoch: [48][177/408]	Loss 0.0068 (0.0270)	
training:	Epoch: [48][178/408]	Loss 0.0064 (0.0269)	
training:	Epoch: [48][179/408]	Loss 0.0039 (0.0267)	
training:	Epoch: [48][180/408]	Loss 0.0031 (0.0266)	
training:	Epoch: [48][181/408]	Loss 0.0039 (0.0265)	
training:	Epoch: [48][182/408]	Loss 0.0052 (0.0264)	
training:	Epoch: [48][183/408]	Loss 0.0041 (0.0263)	
training:	Epoch: [48][184/408]	Loss 0.0051 (0.0261)	
training:	Epoch: [48][185/408]	Loss 0.0039 (0.0260)	
training:	Epoch: [48][186/408]	Loss 0.0034 (0.0259)	
training:	Epoch: [48][187/408]	Loss 0.0078 (0.0258)	
training:	Epoch: [48][188/408]	Loss 0.0045 (0.0257)	
training:	Epoch: [48][189/408]	Loss 0.0038 (0.0256)	
training:	Epoch: [48][190/408]	Loss 0.1691 (0.0263)	
training:	Epoch: [48][191/408]	Loss 0.0034 (0.0262)	
training:	Epoch: [48][192/408]	Loss 0.0053 (0.0261)	
training:	Epoch: [48][193/408]	Loss 0.0048 (0.0260)	
training:	Epoch: [48][194/408]	Loss 0.0045 (0.0259)	
training:	Epoch: [48][195/408]	Loss 0.3027 (0.0273)	
training:	Epoch: [48][196/408]	Loss 0.2029 (0.0282)	
training:	Epoch: [48][197/408]	Loss 0.0054 (0.0281)	
training:	Epoch: [48][198/408]	Loss 0.0064 (0.0280)	
training:	Epoch: [48][199/408]	Loss 0.0050 (0.0279)	
training:	Epoch: [48][200/408]	Loss 0.3024 (0.0292)	
training:	Epoch: [48][201/408]	Loss 0.0062 (0.0291)	
training:	Epoch: [48][202/408]	Loss 0.0043 (0.0290)	
training:	Epoch: [48][203/408]	Loss 0.0076 (0.0289)	
training:	Epoch: [48][204/408]	Loss 0.0118 (0.0288)	
training:	Epoch: [48][205/408]	Loss 0.0037 (0.0287)	
training:	Epoch: [48][206/408]	Loss 0.0069 (0.0286)	
training:	Epoch: [48][207/408]	Loss 0.1854 (0.0293)	
training:	Epoch: [48][208/408]	Loss 0.0061 (0.0292)	
training:	Epoch: [48][209/408]	Loss 0.0051 (0.0291)	
training:	Epoch: [48][210/408]	Loss 0.0057 (0.0290)	
training:	Epoch: [48][211/408]	Loss 0.0063 (0.0289)	
training:	Epoch: [48][212/408]	Loss 0.0069 (0.0288)	
training:	Epoch: [48][213/408]	Loss 0.0032 (0.0287)	
training:	Epoch: [48][214/408]	Loss 0.0054 (0.0285)	
training:	Epoch: [48][215/408]	Loss 0.0074 (0.0284)	
training:	Epoch: [48][216/408]	Loss 0.1314 (0.0289)	
training:	Epoch: [48][217/408]	Loss 0.0093 (0.0288)	
training:	Epoch: [48][218/408]	Loss 0.0137 (0.0288)	
training:	Epoch: [48][219/408]	Loss 0.0085 (0.0287)	
training:	Epoch: [48][220/408]	Loss 0.0046 (0.0286)	
training:	Epoch: [48][221/408]	Loss 0.0064 (0.0285)	
training:	Epoch: [48][222/408]	Loss 0.0049 (0.0284)	
training:	Epoch: [48][223/408]	Loss 0.0078 (0.0283)	
training:	Epoch: [48][224/408]	Loss 0.0045 (0.0282)	
training:	Epoch: [48][225/408]	Loss 0.0043 (0.0281)	
training:	Epoch: [48][226/408]	Loss 0.0042 (0.0279)	
training:	Epoch: [48][227/408]	Loss 0.1660 (0.0286)	
training:	Epoch: [48][228/408]	Loss 0.0072 (0.0285)	
training:	Epoch: [48][229/408]	Loss 0.0042 (0.0284)	
training:	Epoch: [48][230/408]	Loss 0.0061 (0.0283)	
training:	Epoch: [48][231/408]	Loss 0.0081 (0.0282)	
training:	Epoch: [48][232/408]	Loss 0.0061 (0.0281)	
training:	Epoch: [48][233/408]	Loss 0.0046 (0.0280)	
training:	Epoch: [48][234/408]	Loss 0.0111 (0.0279)	
training:	Epoch: [48][235/408]	Loss 0.0067 (0.0278)	
training:	Epoch: [48][236/408]	Loss 0.0050 (0.0277)	
training:	Epoch: [48][237/408]	Loss 0.0096 (0.0276)	
training:	Epoch: [48][238/408]	Loss 0.0074 (0.0276)	
training:	Epoch: [48][239/408]	Loss 0.0035 (0.0275)	
training:	Epoch: [48][240/408]	Loss 0.0042 (0.0274)	
training:	Epoch: [48][241/408]	Loss 0.0065 (0.0273)	
training:	Epoch: [48][242/408]	Loss 0.0060 (0.0272)	
training:	Epoch: [48][243/408]	Loss 0.0068 (0.0271)	
training:	Epoch: [48][244/408]	Loss 0.0074 (0.0270)	
training:	Epoch: [48][245/408]	Loss 0.0062 (0.0269)	
training:	Epoch: [48][246/408]	Loss 0.0085 (0.0269)	
training:	Epoch: [48][247/408]	Loss 0.0083 (0.0268)	
training:	Epoch: [48][248/408]	Loss 0.0106 (0.0267)	
training:	Epoch: [48][249/408]	Loss 0.2724 (0.0277)	
training:	Epoch: [48][250/408]	Loss 0.0033 (0.0276)	
training:	Epoch: [48][251/408]	Loss 0.2135 (0.0283)	
training:	Epoch: [48][252/408]	Loss 0.0082 (0.0283)	
training:	Epoch: [48][253/408]	Loss 0.0065 (0.0282)	
training:	Epoch: [48][254/408]	Loss 0.1624 (0.0287)	
training:	Epoch: [48][255/408]	Loss 0.0057 (0.0286)	
training:	Epoch: [48][256/408]	Loss 0.0035 (0.0285)	
training:	Epoch: [48][257/408]	Loss 0.0042 (0.0284)	
training:	Epoch: [48][258/408]	Loss 0.0089 (0.0284)	
training:	Epoch: [48][259/408]	Loss 0.0115 (0.0283)	
training:	Epoch: [48][260/408]	Loss 0.0089 (0.0282)	
training:	Epoch: [48][261/408]	Loss 0.0090 (0.0281)	
training:	Epoch: [48][262/408]	Loss 0.0082 (0.0281)	
training:	Epoch: [48][263/408]	Loss 0.0045 (0.0280)	
training:	Epoch: [48][264/408]	Loss 0.0114 (0.0279)	
training:	Epoch: [48][265/408]	Loss 0.0068 (0.0278)	
training:	Epoch: [48][266/408]	Loss 0.0117 (0.0278)	
training:	Epoch: [48][267/408]	Loss 0.0056 (0.0277)	
training:	Epoch: [48][268/408]	Loss 0.0072 (0.0276)	
training:	Epoch: [48][269/408]	Loss 0.0085 (0.0275)	
training:	Epoch: [48][270/408]	Loss 0.1761 (0.0281)	
training:	Epoch: [48][271/408]	Loss 0.0117 (0.0280)	
training:	Epoch: [48][272/408]	Loss 0.0071 (0.0280)	
training:	Epoch: [48][273/408]	Loss 0.0061 (0.0279)	
training:	Epoch: [48][274/408]	Loss 0.0108 (0.0278)	
training:	Epoch: [48][275/408]	Loss 0.0062 (0.0277)	
training:	Epoch: [48][276/408]	Loss 0.0077 (0.0277)	
training:	Epoch: [48][277/408]	Loss 0.0091 (0.0276)	
training:	Epoch: [48][278/408]	Loss 0.2025 (0.0282)	
training:	Epoch: [48][279/408]	Loss 0.0061 (0.0281)	
training:	Epoch: [48][280/408]	Loss 0.0081 (0.0281)	
training:	Epoch: [48][281/408]	Loss 0.0040 (0.0280)	
training:	Epoch: [48][282/408]	Loss 0.0059 (0.0279)	
training:	Epoch: [48][283/408]	Loss 0.0360 (0.0279)	
training:	Epoch: [48][284/408]	Loss 0.0088 (0.0279)	
training:	Epoch: [48][285/408]	Loss 0.0091 (0.0278)	
training:	Epoch: [48][286/408]	Loss 0.0109 (0.0277)	
training:	Epoch: [48][287/408]	Loss 0.0091 (0.0277)	
training:	Epoch: [48][288/408]	Loss 0.0088 (0.0276)	
training:	Epoch: [48][289/408]	Loss 0.0171 (0.0276)	
training:	Epoch: [48][290/408]	Loss 0.0066 (0.0275)	
training:	Epoch: [48][291/408]	Loss 0.0130 (0.0275)	
training:	Epoch: [48][292/408]	Loss 0.0096 (0.0274)	
training:	Epoch: [48][293/408]	Loss 0.0101 (0.0273)	
training:	Epoch: [48][294/408]	Loss 0.0172 (0.0273)	
training:	Epoch: [48][295/408]	Loss 0.0052 (0.0272)	
training:	Epoch: [48][296/408]	Loss 0.0086 (0.0272)	
training:	Epoch: [48][297/408]	Loss 0.0079 (0.0271)	
training:	Epoch: [48][298/408]	Loss 0.0109 (0.0270)	
training:	Epoch: [48][299/408]	Loss 0.0066 (0.0270)	
training:	Epoch: [48][300/408]	Loss 0.0060 (0.0269)	
training:	Epoch: [48][301/408]	Loss 0.0058 (0.0268)	
training:	Epoch: [48][302/408]	Loss 0.0081 (0.0268)	
training:	Epoch: [48][303/408]	Loss 0.0059 (0.0267)	
training:	Epoch: [48][304/408]	Loss 0.0169 (0.0267)	
training:	Epoch: [48][305/408]	Loss 0.0147 (0.0266)	
training:	Epoch: [48][306/408]	Loss 0.0055 (0.0266)	
training:	Epoch: [48][307/408]	Loss 0.0039 (0.0265)	
training:	Epoch: [48][308/408]	Loss 0.0037 (0.0264)	
training:	Epoch: [48][309/408]	Loss 0.0055 (0.0263)	
training:	Epoch: [48][310/408]	Loss 0.0051 (0.0263)	
training:	Epoch: [48][311/408]	Loss 0.0038 (0.0262)	
training:	Epoch: [48][312/408]	Loss 0.2155 (0.0268)	
training:	Epoch: [48][313/408]	Loss 0.0073 (0.0268)	
training:	Epoch: [48][314/408]	Loss 0.0062 (0.0267)	
training:	Epoch: [48][315/408]	Loss 0.0069 (0.0266)	
training:	Epoch: [48][316/408]	Loss 0.0063 (0.0266)	
training:	Epoch: [48][317/408]	Loss 0.4450 (0.0279)	
training:	Epoch: [48][318/408]	Loss 0.0042 (0.0278)	
training:	Epoch: [48][319/408]	Loss 0.0076 (0.0277)	
training:	Epoch: [48][320/408]	Loss 0.0074 (0.0277)	
training:	Epoch: [48][321/408]	Loss 0.0084 (0.0276)	
training:	Epoch: [48][322/408]	Loss 0.0099 (0.0276)	
training:	Epoch: [48][323/408]	Loss 0.0049 (0.0275)	
training:	Epoch: [48][324/408]	Loss 0.0050 (0.0274)	
training:	Epoch: [48][325/408]	Loss 0.1865 (0.0279)	
training:	Epoch: [48][326/408]	Loss 0.0047 (0.0278)	
training:	Epoch: [48][327/408]	Loss 0.0067 (0.0278)	
training:	Epoch: [48][328/408]	Loss 0.0061 (0.0277)	
training:	Epoch: [48][329/408]	Loss 0.0037 (0.0276)	
training:	Epoch: [48][330/408]	Loss 0.0126 (0.0276)	
training:	Epoch: [48][331/408]	Loss 0.0114 (0.0275)	
training:	Epoch: [48][332/408]	Loss 0.0046 (0.0275)	
training:	Epoch: [48][333/408]	Loss 0.0079 (0.0274)	
training:	Epoch: [48][334/408]	Loss 0.0082 (0.0274)	
training:	Epoch: [48][335/408]	Loss 0.0046 (0.0273)	
training:	Epoch: [48][336/408]	Loss 0.0062 (0.0272)	
training:	Epoch: [48][337/408]	Loss 0.0074 (0.0272)	
training:	Epoch: [48][338/408]	Loss 0.0074 (0.0271)	
training:	Epoch: [48][339/408]	Loss 0.0193 (0.0271)	
training:	Epoch: [48][340/408]	Loss 0.0099 (0.0270)	
training:	Epoch: [48][341/408]	Loss 0.0076 (0.0270)	
training:	Epoch: [48][342/408]	Loss 0.0088 (0.0269)	
training:	Epoch: [48][343/408]	Loss 0.0081 (0.0269)	
training:	Epoch: [48][344/408]	Loss 0.0043 (0.0268)	
training:	Epoch: [48][345/408]	Loss 0.0081 (0.0268)	
training:	Epoch: [48][346/408]	Loss 0.0052 (0.0267)	
training:	Epoch: [48][347/408]	Loss 0.0052 (0.0266)	
training:	Epoch: [48][348/408]	Loss 0.0063 (0.0266)	
training:	Epoch: [48][349/408]	Loss 0.0079 (0.0265)	
training:	Epoch: [48][350/408]	Loss 0.0093 (0.0265)	
training:	Epoch: [48][351/408]	Loss 0.0066 (0.0264)	
training:	Epoch: [48][352/408]	Loss 0.0084 (0.0264)	
training:	Epoch: [48][353/408]	Loss 0.0059 (0.0263)	
training:	Epoch: [48][354/408]	Loss 0.0057 (0.0262)	
training:	Epoch: [48][355/408]	Loss 0.0039 (0.0262)	
training:	Epoch: [48][356/408]	Loss 0.0067 (0.0261)	
training:	Epoch: [48][357/408]	Loss 0.0038 (0.0261)	
training:	Epoch: [48][358/408]	Loss 0.0088 (0.0260)	
training:	Epoch: [48][359/408]	Loss 0.0053 (0.0260)	
training:	Epoch: [48][360/408]	Loss 0.0045 (0.0259)	
training:	Epoch: [48][361/408]	Loss 0.0060 (0.0258)	
training:	Epoch: [48][362/408]	Loss 0.1657 (0.0262)	
training:	Epoch: [48][363/408]	Loss 0.0049 (0.0262)	
training:	Epoch: [48][364/408]	Loss 0.0062 (0.0261)	
training:	Epoch: [48][365/408]	Loss 0.2665 (0.0268)	
training:	Epoch: [48][366/408]	Loss 0.0047 (0.0267)	
training:	Epoch: [48][367/408]	Loss 0.0038 (0.0266)	
training:	Epoch: [48][368/408]	Loss 0.0037 (0.0266)	
training:	Epoch: [48][369/408]	Loss 0.0047 (0.0265)	
training:	Epoch: [48][370/408]	Loss 0.0120 (0.0265)	
training:	Epoch: [48][371/408]	Loss 0.0049 (0.0264)	
training:	Epoch: [48][372/408]	Loss 0.1661 (0.0268)	
training:	Epoch: [48][373/408]	Loss 0.0108 (0.0268)	
training:	Epoch: [48][374/408]	Loss 0.0034 (0.0267)	
training:	Epoch: [48][375/408]	Loss 0.0057 (0.0266)	
training:	Epoch: [48][376/408]	Loss 0.0045 (0.0266)	
training:	Epoch: [48][377/408]	Loss 0.0050 (0.0265)	
training:	Epoch: [48][378/408]	Loss 0.0042 (0.0265)	
training:	Epoch: [48][379/408]	Loss 0.0033 (0.0264)	
training:	Epoch: [48][380/408]	Loss 0.0080 (0.0264)	
training:	Epoch: [48][381/408]	Loss 0.0046 (0.0263)	
training:	Epoch: [48][382/408]	Loss 0.0037 (0.0262)	
training:	Epoch: [48][383/408]	Loss 0.0107 (0.0262)	
training:	Epoch: [48][384/408]	Loss 0.0046 (0.0261)	
training:	Epoch: [48][385/408]	Loss 0.0104 (0.0261)	
training:	Epoch: [48][386/408]	Loss 0.0082 (0.0261)	
training:	Epoch: [48][387/408]	Loss 0.0053 (0.0260)	
training:	Epoch: [48][388/408]	Loss 0.0038 (0.0259)	
training:	Epoch: [48][389/408]	Loss 0.0058 (0.0259)	
training:	Epoch: [48][390/408]	Loss 0.0062 (0.0258)	
training:	Epoch: [48][391/408]	Loss 0.0064 (0.0258)	
training:	Epoch: [48][392/408]	Loss 0.0039 (0.0257)	
training:	Epoch: [48][393/408]	Loss 0.0033 (0.0257)	
training:	Epoch: [48][394/408]	Loss 0.1508 (0.0260)	
training:	Epoch: [48][395/408]	Loss 0.0058 (0.0259)	
training:	Epoch: [48][396/408]	Loss 0.0055 (0.0259)	
training:	Epoch: [48][397/408]	Loss 0.0044 (0.0258)	
training:	Epoch: [48][398/408]	Loss 0.0248 (0.0258)	
training:	Epoch: [48][399/408]	Loss 0.0039 (0.0258)	
training:	Epoch: [48][400/408]	Loss 0.0081 (0.0257)	
training:	Epoch: [48][401/408]	Loss 0.0044 (0.0257)	
training:	Epoch: [48][402/408]	Loss 0.0107 (0.0257)	
training:	Epoch: [48][403/408]	Loss 0.0048 (0.0256)	
training:	Epoch: [48][404/408]	Loss 0.0041 (0.0255)	
training:	Epoch: [48][405/408]	Loss 0.0059 (0.0255)	
training:	Epoch: [48][406/408]	Loss 0.0046 (0.0254)	
training:	Epoch: [48][407/408]	Loss 0.0081 (0.0254)	
training:	Epoch: [48][408/408]	Loss 0.0066 (0.0254)	
Training:	 Loss: 0.0253

Training:	 ACC: 0.9948 0.9948 0.9941 0.9955
Validation:	 ACC: 0.7702 0.7731 0.8342 0.7063
Validation:	 Best_BACC: 0.8226 0.8250 0.8772 0.7679
Validation:	 Loss: 1.0669
Pretraining:	Epoch 49/200
----------
training:	Epoch: [49][1/408]	Loss 0.0050 (0.0050)	
training:	Epoch: [49][2/408]	Loss 0.0116 (0.0083)	
training:	Epoch: [49][3/408]	Loss 0.0392 (0.0186)	
training:	Epoch: [49][4/408]	Loss 0.0060 (0.0154)	
training:	Epoch: [49][5/408]	Loss 0.0069 (0.0137)	
training:	Epoch: [49][6/408]	Loss 0.0078 (0.0127)	
training:	Epoch: [49][7/408]	Loss 0.0065 (0.0118)	
training:	Epoch: [49][8/408]	Loss 0.0072 (0.0113)	
training:	Epoch: [49][9/408]	Loss 0.0042 (0.0105)	
training:	Epoch: [49][10/408]	Loss 0.0030 (0.0097)	
training:	Epoch: [49][11/408]	Loss 0.0049 (0.0093)	
training:	Epoch: [49][12/408]	Loss 0.0056 (0.0090)	
training:	Epoch: [49][13/408]	Loss 0.0050 (0.0087)	
training:	Epoch: [49][14/408]	Loss 0.0043 (0.0084)	
training:	Epoch: [49][15/408]	Loss 0.0056 (0.0082)	
training:	Epoch: [49][16/408]	Loss 0.0060 (0.0080)	
training:	Epoch: [49][17/408]	Loss 0.0034 (0.0078)	
training:	Epoch: [49][18/408]	Loss 0.0037 (0.0075)	
training:	Epoch: [49][19/408]	Loss 0.0073 (0.0075)	
training:	Epoch: [49][20/408]	Loss 0.0031 (0.0073)	
training:	Epoch: [49][21/408]	Loss 0.0038 (0.0071)	
training:	Epoch: [49][22/408]	Loss 0.0036 (0.0070)	
training:	Epoch: [49][23/408]	Loss 0.3045 (0.0199)	
training:	Epoch: [49][24/408]	Loss 0.0036 (0.0192)	
training:	Epoch: [49][25/408]	Loss 0.0082 (0.0188)	
training:	Epoch: [49][26/408]	Loss 0.2748 (0.0286)	
training:	Epoch: [49][27/408]	Loss 0.0047 (0.0278)	
training:	Epoch: [49][28/408]	Loss 0.0046 (0.0269)	
training:	Epoch: [49][29/408]	Loss 0.0035 (0.0261)	
training:	Epoch: [49][30/408]	Loss 0.0045 (0.0254)	
training:	Epoch: [49][31/408]	Loss 0.0042 (0.0247)	
training:	Epoch: [49][32/408]	Loss 0.0042 (0.0241)	
training:	Epoch: [49][33/408]	Loss 0.0066 (0.0235)	
training:	Epoch: [49][34/408]	Loss 0.0051 (0.0230)	
training:	Epoch: [49][35/408]	Loss 0.0047 (0.0225)	
training:	Epoch: [49][36/408]	Loss 0.0044 (0.0220)	
training:	Epoch: [49][37/408]	Loss 0.0034 (0.0215)	
training:	Epoch: [49][38/408]	Loss 0.0042 (0.0210)	
training:	Epoch: [49][39/408]	Loss 0.0061 (0.0206)	
training:	Epoch: [49][40/408]	Loss 0.0056 (0.0203)	
training:	Epoch: [49][41/408]	Loss 0.0092 (0.0200)	
training:	Epoch: [49][42/408]	Loss 0.0042 (0.0196)	
training:	Epoch: [49][43/408]	Loss 0.2112 (0.0241)	
training:	Epoch: [49][44/408]	Loss 0.0088 (0.0237)	
training:	Epoch: [49][45/408]	Loss 0.0046 (0.0233)	
training:	Epoch: [49][46/408]	Loss 0.0048 (0.0229)	
training:	Epoch: [49][47/408]	Loss 0.0079 (0.0226)	
training:	Epoch: [49][48/408]	Loss 0.0031 (0.0222)	
training:	Epoch: [49][49/408]	Loss 0.0054 (0.0218)	
training:	Epoch: [49][50/408]	Loss 0.0038 (0.0215)	
training:	Epoch: [49][51/408]	Loss 0.0062 (0.0212)	
training:	Epoch: [49][52/408]	Loss 0.0067 (0.0209)	
training:	Epoch: [49][53/408]	Loss 0.0063 (0.0206)	
training:	Epoch: [49][54/408]	Loss 0.0044 (0.0203)	
training:	Epoch: [49][55/408]	Loss 0.0059 (0.0200)	
training:	Epoch: [49][56/408]	Loss 0.0067 (0.0198)	
training:	Epoch: [49][57/408]	Loss 0.0081 (0.0196)	
training:	Epoch: [49][58/408]	Loss 0.0035 (0.0193)	
training:	Epoch: [49][59/408]	Loss 0.0050 (0.0191)	
training:	Epoch: [49][60/408]	Loss 0.0032 (0.0188)	
training:	Epoch: [49][61/408]	Loss 0.0054 (0.0186)	
training:	Epoch: [49][62/408]	Loss 0.1491 (0.0207)	
training:	Epoch: [49][63/408]	Loss 0.0072 (0.0205)	
training:	Epoch: [49][64/408]	Loss 0.0040 (0.0202)	
training:	Epoch: [49][65/408]	Loss 0.0047 (0.0200)	
training:	Epoch: [49][66/408]	Loss 0.0067 (0.0198)	
training:	Epoch: [49][67/408]	Loss 0.3244 (0.0243)	
training:	Epoch: [49][68/408]	Loss 0.0040 (0.0240)	
training:	Epoch: [49][69/408]	Loss 0.0055 (0.0238)	
training:	Epoch: [49][70/408]	Loss 0.0049 (0.0235)	
training:	Epoch: [49][71/408]	Loss 0.0043 (0.0232)	
training:	Epoch: [49][72/408]	Loss 0.0060 (0.0230)	
training:	Epoch: [49][73/408]	Loss 0.0048 (0.0227)	
training:	Epoch: [49][74/408]	Loss 0.0061 (0.0225)	
training:	Epoch: [49][75/408]	Loss 0.0064 (0.0223)	
training:	Epoch: [49][76/408]	Loss 0.0042 (0.0221)	
training:	Epoch: [49][77/408]	Loss 0.0062 (0.0219)	
training:	Epoch: [49][78/408]	Loss 0.0042 (0.0216)	
training:	Epoch: [49][79/408]	Loss 0.2439 (0.0244)	
training:	Epoch: [49][80/408]	Loss 0.0032 (0.0242)	
training:	Epoch: [49][81/408]	Loss 0.0047 (0.0239)	
training:	Epoch: [49][82/408]	Loss 0.0046 (0.0237)	
training:	Epoch: [49][83/408]	Loss 0.0067 (0.0235)	
training:	Epoch: [49][84/408]	Loss 0.0039 (0.0233)	
training:	Epoch: [49][85/408]	Loss 0.0079 (0.0231)	
training:	Epoch: [49][86/408]	Loss 0.0032 (0.0229)	
training:	Epoch: [49][87/408]	Loss 0.0035 (0.0226)	
training:	Epoch: [49][88/408]	Loss 0.0064 (0.0224)	
training:	Epoch: [49][89/408]	Loss 0.0046 (0.0222)	
training:	Epoch: [49][90/408]	Loss 0.0036 (0.0220)	
training:	Epoch: [49][91/408]	Loss 0.0050 (0.0219)	
training:	Epoch: [49][92/408]	Loss 0.0064 (0.0217)	
training:	Epoch: [49][93/408]	Loss 0.0268 (0.0217)	
training:	Epoch: [49][94/408]	Loss 0.0036 (0.0215)	
training:	Epoch: [49][95/408]	Loss 0.0040 (0.0214)	
training:	Epoch: [49][96/408]	Loss 0.0051 (0.0212)	
training:	Epoch: [49][97/408]	Loss 0.0039 (0.0210)	
training:	Epoch: [49][98/408]	Loss 0.0046 (0.0208)	
training:	Epoch: [49][99/408]	Loss 0.0060 (0.0207)	
training:	Epoch: [49][100/408]	Loss 0.0062 (0.0206)	
training:	Epoch: [49][101/408]	Loss 0.0075 (0.0204)	
training:	Epoch: [49][102/408]	Loss 0.0048 (0.0203)	
training:	Epoch: [49][103/408]	Loss 0.0037 (0.0201)	
training:	Epoch: [49][104/408]	Loss 0.0086 (0.0200)	
training:	Epoch: [49][105/408]	Loss 0.0079 (0.0199)	
training:	Epoch: [49][106/408]	Loss 0.1786 (0.0214)	
training:	Epoch: [49][107/408]	Loss 0.0057 (0.0212)	
training:	Epoch: [49][108/408]	Loss 0.0060 (0.0211)	
training:	Epoch: [49][109/408]	Loss 0.0061 (0.0210)	
training:	Epoch: [49][110/408]	Loss 0.0055 (0.0208)	
training:	Epoch: [49][111/408]	Loss 0.0035 (0.0207)	
training:	Epoch: [49][112/408]	Loss 0.0050 (0.0205)	
training:	Epoch: [49][113/408]	Loss 0.2421 (0.0225)	
training:	Epoch: [49][114/408]	Loss 0.0076 (0.0223)	
training:	Epoch: [49][115/408]	Loss 0.1438 (0.0234)	
training:	Epoch: [49][116/408]	Loss 0.0040 (0.0232)	
training:	Epoch: [49][117/408]	Loss 0.1464 (0.0243)	
training:	Epoch: [49][118/408]	Loss 0.0064 (0.0241)	
training:	Epoch: [49][119/408]	Loss 0.0052 (0.0240)	
training:	Epoch: [49][120/408]	Loss 0.0045 (0.0238)	
training:	Epoch: [49][121/408]	Loss 0.0121 (0.0237)	
training:	Epoch: [49][122/408]	Loss 0.0152 (0.0237)	
training:	Epoch: [49][123/408]	Loss 0.0092 (0.0235)	
training:	Epoch: [49][124/408]	Loss 0.0084 (0.0234)	
training:	Epoch: [49][125/408]	Loss 0.0043 (0.0233)	
training:	Epoch: [49][126/408]	Loss 0.0092 (0.0231)	
training:	Epoch: [49][127/408]	Loss 0.0053 (0.0230)	
training:	Epoch: [49][128/408]	Loss 0.0090 (0.0229)	
training:	Epoch: [49][129/408]	Loss 0.0044 (0.0228)	
training:	Epoch: [49][130/408]	Loss 0.0035 (0.0226)	
training:	Epoch: [49][131/408]	Loss 0.0055 (0.0225)	
training:	Epoch: [49][132/408]	Loss 0.0047 (0.0223)	
training:	Epoch: [49][133/408]	Loss 0.0144 (0.0223)	
training:	Epoch: [49][134/408]	Loss 0.0039 (0.0221)	
training:	Epoch: [49][135/408]	Loss 0.0109 (0.0221)	
training:	Epoch: [49][136/408]	Loss 0.0053 (0.0219)	
training:	Epoch: [49][137/408]	Loss 0.0037 (0.0218)	
training:	Epoch: [49][138/408]	Loss 0.0054 (0.0217)	
training:	Epoch: [49][139/408]	Loss 0.0075 (0.0216)	
training:	Epoch: [49][140/408]	Loss 0.0074 (0.0215)	
training:	Epoch: [49][141/408]	Loss 0.0058 (0.0214)	
training:	Epoch: [49][142/408]	Loss 0.0095 (0.0213)	
training:	Epoch: [49][143/408]	Loss 0.0100 (0.0212)	
training:	Epoch: [49][144/408]	Loss 0.2513 (0.0228)	
training:	Epoch: [49][145/408]	Loss 0.0101 (0.0227)	
training:	Epoch: [49][146/408]	Loss 0.0102 (0.0226)	
training:	Epoch: [49][147/408]	Loss 0.0078 (0.0225)	
training:	Epoch: [49][148/408]	Loss 0.0048 (0.0224)	
training:	Epoch: [49][149/408]	Loss 0.0037 (0.0223)	
training:	Epoch: [49][150/408]	Loss 0.0055 (0.0222)	
training:	Epoch: [49][151/408]	Loss 0.0041 (0.0221)	
training:	Epoch: [49][152/408]	Loss 0.0064 (0.0220)	
training:	Epoch: [49][153/408]	Loss 0.0061 (0.0218)	
training:	Epoch: [49][154/408]	Loss 0.0042 (0.0217)	
training:	Epoch: [49][155/408]	Loss 0.0030 (0.0216)	
training:	Epoch: [49][156/408]	Loss 0.0050 (0.0215)	
training:	Epoch: [49][157/408]	Loss 0.0060 (0.0214)	
training:	Epoch: [49][158/408]	Loss 0.0066 (0.0213)	
training:	Epoch: [49][159/408]	Loss 0.0036 (0.0212)	
training:	Epoch: [49][160/408]	Loss 0.0045 (0.0211)	
training:	Epoch: [49][161/408]	Loss 0.0064 (0.0210)	
training:	Epoch: [49][162/408]	Loss 0.0046 (0.0209)	
training:	Epoch: [49][163/408]	Loss 0.0057 (0.0208)	
training:	Epoch: [49][164/408]	Loss 0.0074 (0.0207)	
training:	Epoch: [49][165/408]	Loss 0.0032 (0.0206)	
training:	Epoch: [49][166/408]	Loss 0.0037 (0.0205)	
training:	Epoch: [49][167/408]	Loss 0.0032 (0.0204)	
training:	Epoch: [49][168/408]	Loss 0.0052 (0.0203)	
training:	Epoch: [49][169/408]	Loss 0.0054 (0.0202)	
training:	Epoch: [49][170/408]	Loss 0.0034 (0.0201)	
training:	Epoch: [49][171/408]	Loss 0.0041 (0.0200)	
training:	Epoch: [49][172/408]	Loss 0.0045 (0.0200)	
training:	Epoch: [49][173/408]	Loss 0.0051 (0.0199)	
training:	Epoch: [49][174/408]	Loss 0.0066 (0.0198)	
training:	Epoch: [49][175/408]	Loss 0.0627 (0.0200)	
training:	Epoch: [49][176/408]	Loss 0.0038 (0.0199)	
training:	Epoch: [49][177/408]	Loss 0.0068 (0.0199)	
training:	Epoch: [49][178/408]	Loss 0.0166 (0.0199)	
training:	Epoch: [49][179/408]	Loss 0.0043 (0.0198)	
training:	Epoch: [49][180/408]	Loss 0.0052 (0.0197)	
training:	Epoch: [49][181/408]	Loss 0.0033 (0.0196)	
training:	Epoch: [49][182/408]	Loss 0.0071 (0.0195)	
training:	Epoch: [49][183/408]	Loss 0.0027 (0.0194)	
training:	Epoch: [49][184/408]	Loss 0.0033 (0.0193)	
training:	Epoch: [49][185/408]	Loss 0.2168 (0.0204)	
training:	Epoch: [49][186/408]	Loss 0.0046 (0.0203)	
training:	Epoch: [49][187/408]	Loss 0.0048 (0.0202)	
training:	Epoch: [49][188/408]	Loss 0.0079 (0.0202)	
training:	Epoch: [49][189/408]	Loss 0.0069 (0.0201)	
training:	Epoch: [49][190/408]	Loss 0.0035 (0.0200)	
training:	Epoch: [49][191/408]	Loss 0.0081 (0.0200)	
training:	Epoch: [49][192/408]	Loss 0.0032 (0.0199)	
training:	Epoch: [49][193/408]	Loss 0.0035 (0.0198)	
training:	Epoch: [49][194/408]	Loss 0.0040 (0.0197)	
training:	Epoch: [49][195/408]	Loss 0.0074 (0.0196)	
training:	Epoch: [49][196/408]	Loss 0.2447 (0.0208)	
training:	Epoch: [49][197/408]	Loss 0.0029 (0.0207)	
training:	Epoch: [49][198/408]	Loss 0.2146 (0.0217)	
training:	Epoch: [49][199/408]	Loss 0.0044 (0.0216)	
training:	Epoch: [49][200/408]	Loss 0.0058 (0.0215)	
training:	Epoch: [49][201/408]	Loss 0.0040 (0.0214)	
training:	Epoch: [49][202/408]	Loss 0.0028 (0.0213)	
training:	Epoch: [49][203/408]	Loss 0.2268 (0.0223)	
training:	Epoch: [49][204/408]	Loss 0.0042 (0.0223)	
training:	Epoch: [49][205/408]	Loss 0.0040 (0.0222)	
training:	Epoch: [49][206/408]	Loss 0.0051 (0.0221)	
training:	Epoch: [49][207/408]	Loss 0.0033 (0.0220)	
training:	Epoch: [49][208/408]	Loss 0.1834 (0.0228)	
training:	Epoch: [49][209/408]	Loss 0.2854 (0.0240)	
training:	Epoch: [49][210/408]	Loss 0.0053 (0.0239)	
training:	Epoch: [49][211/408]	Loss 0.0035 (0.0238)	
training:	Epoch: [49][212/408]	Loss 0.0046 (0.0238)	
training:	Epoch: [49][213/408]	Loss 0.0094 (0.0237)	
training:	Epoch: [49][214/408]	Loss 0.0055 (0.0236)	
training:	Epoch: [49][215/408]	Loss 0.0106 (0.0235)	
training:	Epoch: [49][216/408]	Loss 0.0069 (0.0235)	
training:	Epoch: [49][217/408]	Loss 0.0043 (0.0234)	
training:	Epoch: [49][218/408]	Loss 0.0046 (0.0233)	
training:	Epoch: [49][219/408]	Loss 0.0099 (0.0232)	
training:	Epoch: [49][220/408]	Loss 0.0055 (0.0231)	
training:	Epoch: [49][221/408]	Loss 0.0039 (0.0231)	
training:	Epoch: [49][222/408]	Loss 0.0097 (0.0230)	
training:	Epoch: [49][223/408]	Loss 0.0073 (0.0229)	
training:	Epoch: [49][224/408]	Loss 0.0060 (0.0229)	
training:	Epoch: [49][225/408]	Loss 0.0082 (0.0228)	
training:	Epoch: [49][226/408]	Loss 0.0066 (0.0227)	
training:	Epoch: [49][227/408]	Loss 0.0062 (0.0226)	
training:	Epoch: [49][228/408]	Loss 0.0025 (0.0226)	
training:	Epoch: [49][229/408]	Loss 0.0112 (0.0225)	
training:	Epoch: [49][230/408]	Loss 0.0040 (0.0224)	
training:	Epoch: [49][231/408]	Loss 0.0083 (0.0224)	
training:	Epoch: [49][232/408]	Loss 0.0040 (0.0223)	
training:	Epoch: [49][233/408]	Loss 0.0074 (0.0222)	
training:	Epoch: [49][234/408]	Loss 0.0031 (0.0221)	
training:	Epoch: [49][235/408]	Loss 0.0066 (0.0221)	
training:	Epoch: [49][236/408]	Loss 0.0042 (0.0220)	
training:	Epoch: [49][237/408]	Loss 0.0039 (0.0219)	
training:	Epoch: [49][238/408]	Loss 0.0070 (0.0219)	
training:	Epoch: [49][239/408]	Loss 0.1415 (0.0224)	
training:	Epoch: [49][240/408]	Loss 0.0049 (0.0223)	
training:	Epoch: [49][241/408]	Loss 0.0025 (0.0222)	
training:	Epoch: [49][242/408]	Loss 0.0058 (0.0221)	
training:	Epoch: [49][243/408]	Loss 0.0046 (0.0221)	
training:	Epoch: [49][244/408]	Loss 0.0050 (0.0220)	
training:	Epoch: [49][245/408]	Loss 0.0037 (0.0219)	
training:	Epoch: [49][246/408]	Loss 0.1743 (0.0225)	
training:	Epoch: [49][247/408]	Loss 0.0039 (0.0225)	
training:	Epoch: [49][248/408]	Loss 0.0036 (0.0224)	
training:	Epoch: [49][249/408]	Loss 0.2162 (0.0232)	
training:	Epoch: [49][250/408]	Loss 0.0036 (0.0231)	
training:	Epoch: [49][251/408]	Loss 0.1347 (0.0235)	
training:	Epoch: [49][252/408]	Loss 0.0045 (0.0235)	
training:	Epoch: [49][253/408]	Loss 0.0080 (0.0234)	
training:	Epoch: [49][254/408]	Loss 0.0040 (0.0233)	
training:	Epoch: [49][255/408]	Loss 0.0074 (0.0233)	
training:	Epoch: [49][256/408]	Loss 0.2162 (0.0240)	
training:	Epoch: [49][257/408]	Loss 0.0057 (0.0239)	
training:	Epoch: [49][258/408]	Loss 0.3195 (0.0251)	
training:	Epoch: [49][259/408]	Loss 0.0040 (0.0250)	
training:	Epoch: [49][260/408]	Loss 0.0106 (0.0249)	
training:	Epoch: [49][261/408]	Loss 0.0180 (0.0249)	
training:	Epoch: [49][262/408]	Loss 0.0034 (0.0248)	
training:	Epoch: [49][263/408]	Loss 0.0421 (0.0249)	
training:	Epoch: [49][264/408]	Loss 0.0111 (0.0249)	
training:	Epoch: [49][265/408]	Loss 0.0095 (0.0248)	
training:	Epoch: [49][266/408]	Loss 0.1972 (0.0254)	
training:	Epoch: [49][267/408]	Loss 0.0120 (0.0254)	
training:	Epoch: [49][268/408]	Loss 0.0057 (0.0253)	
training:	Epoch: [49][269/408]	Loss 0.2215 (0.0260)	
training:	Epoch: [49][270/408]	Loss 0.0089 (0.0260)	
training:	Epoch: [49][271/408]	Loss 0.0051 (0.0259)	
training:	Epoch: [49][272/408]	Loss 0.0186 (0.0259)	
training:	Epoch: [49][273/408]	Loss 0.0039 (0.0258)	
training:	Epoch: [49][274/408]	Loss 0.0105 (0.0257)	
training:	Epoch: [49][275/408]	Loss 0.0079 (0.0257)	
training:	Epoch: [49][276/408]	Loss 0.0085 (0.0256)	
training:	Epoch: [49][277/408]	Loss 0.0058 (0.0255)	
training:	Epoch: [49][278/408]	Loss 0.0111 (0.0255)	
training:	Epoch: [49][279/408]	Loss 0.0202 (0.0255)	
training:	Epoch: [49][280/408]	Loss 0.0124 (0.0254)	
training:	Epoch: [49][281/408]	Loss 0.0050 (0.0254)	
training:	Epoch: [49][282/408]	Loss 0.0035 (0.0253)	
training:	Epoch: [49][283/408]	Loss 0.0070 (0.0252)	
training:	Epoch: [49][284/408]	Loss 0.0100 (0.0252)	
training:	Epoch: [49][285/408]	Loss 0.0106 (0.0251)	
training:	Epoch: [49][286/408]	Loss 0.0091 (0.0251)	
training:	Epoch: [49][287/408]	Loss 0.0045 (0.0250)	
training:	Epoch: [49][288/408]	Loss 0.0039 (0.0249)	
training:	Epoch: [49][289/408]	Loss 0.2745 (0.0258)	
training:	Epoch: [49][290/408]	Loss 0.0068 (0.0257)	
training:	Epoch: [49][291/408]	Loss 0.0126 (0.0257)	
training:	Epoch: [49][292/408]	Loss 0.0072 (0.0256)	
training:	Epoch: [49][293/408]	Loss 0.0105 (0.0255)	
training:	Epoch: [49][294/408]	Loss 0.0078 (0.0255)	
training:	Epoch: [49][295/408]	Loss 0.0046 (0.0254)	
training:	Epoch: [49][296/408]	Loss 0.0085 (0.0254)	
training:	Epoch: [49][297/408]	Loss 0.0032 (0.0253)	
training:	Epoch: [49][298/408]	Loss 0.1540 (0.0257)	
training:	Epoch: [49][299/408]	Loss 0.0073 (0.0257)	
training:	Epoch: [49][300/408]	Loss 0.0040 (0.0256)	
training:	Epoch: [49][301/408]	Loss 0.0076 (0.0255)	
training:	Epoch: [49][302/408]	Loss 0.0038 (0.0254)	
training:	Epoch: [49][303/408]	Loss 0.0044 (0.0254)	
training:	Epoch: [49][304/408]	Loss 0.0111 (0.0253)	
training:	Epoch: [49][305/408]	Loss 0.0050 (0.0253)	
training:	Epoch: [49][306/408]	Loss 0.0054 (0.0252)	
training:	Epoch: [49][307/408]	Loss 0.0035 (0.0251)	
training:	Epoch: [49][308/408]	Loss 0.0054 (0.0251)	
training:	Epoch: [49][309/408]	Loss 0.0041 (0.0250)	
training:	Epoch: [49][310/408]	Loss 0.1658 (0.0255)	
training:	Epoch: [49][311/408]	Loss 0.0074 (0.0254)	
training:	Epoch: [49][312/408]	Loss 0.0045 (0.0253)	
training:	Epoch: [49][313/408]	Loss 0.0078 (0.0253)	
training:	Epoch: [49][314/408]	Loss 0.0042 (0.0252)	
training:	Epoch: [49][315/408]	Loss 0.0074 (0.0251)	
training:	Epoch: [49][316/408]	Loss 0.0083 (0.0251)	
training:	Epoch: [49][317/408]	Loss 0.0092 (0.0250)	
training:	Epoch: [49][318/408]	Loss 0.0069 (0.0250)	
training:	Epoch: [49][319/408]	Loss 0.0118 (0.0249)	
training:	Epoch: [49][320/408]	Loss 0.0064 (0.0249)	
training:	Epoch: [49][321/408]	Loss 0.1433 (0.0253)	
training:	Epoch: [49][322/408]	Loss 0.0106 (0.0252)	
training:	Epoch: [49][323/408]	Loss 0.0042 (0.0251)	
training:	Epoch: [49][324/408]	Loss 0.0157 (0.0251)	
training:	Epoch: [49][325/408]	Loss 0.0045 (0.0251)	
training:	Epoch: [49][326/408]	Loss 0.0058 (0.0250)	
training:	Epoch: [49][327/408]	Loss 0.0028 (0.0249)	
training:	Epoch: [49][328/408]	Loss 0.0035 (0.0249)	
training:	Epoch: [49][329/408]	Loss 0.0079 (0.0248)	
training:	Epoch: [49][330/408]	Loss 0.1534 (0.0252)	
training:	Epoch: [49][331/408]	Loss 0.0048 (0.0251)	
training:	Epoch: [49][332/408]	Loss 0.0039 (0.0251)	
training:	Epoch: [49][333/408]	Loss 0.0060 (0.0250)	
training:	Epoch: [49][334/408]	Loss 0.0061 (0.0250)	
training:	Epoch: [49][335/408]	Loss 0.0055 (0.0249)	
training:	Epoch: [49][336/408]	Loss 0.0040 (0.0248)	
training:	Epoch: [49][337/408]	Loss 0.0154 (0.0248)	
training:	Epoch: [49][338/408]	Loss 0.0113 (0.0248)	
training:	Epoch: [49][339/408]	Loss 0.0065 (0.0247)	
training:	Epoch: [49][340/408]	Loss 0.0056 (0.0247)	
training:	Epoch: [49][341/408]	Loss 0.0052 (0.0246)	
training:	Epoch: [49][342/408]	Loss 0.0132 (0.0246)	
training:	Epoch: [49][343/408]	Loss 0.0131 (0.0245)	
training:	Epoch: [49][344/408]	Loss 0.0096 (0.0245)	
training:	Epoch: [49][345/408]	Loss 0.0117 (0.0245)	
training:	Epoch: [49][346/408]	Loss 0.0071 (0.0244)	
training:	Epoch: [49][347/408]	Loss 0.2748 (0.0251)	
training:	Epoch: [49][348/408]	Loss 0.0054 (0.0251)	
training:	Epoch: [49][349/408]	Loss 0.0058 (0.0250)	
training:	Epoch: [49][350/408]	Loss 0.0128 (0.0250)	
training:	Epoch: [49][351/408]	Loss 0.0056 (0.0249)	
training:	Epoch: [49][352/408]	Loss 0.0045 (0.0249)	
training:	Epoch: [49][353/408]	Loss 0.0070 (0.0248)	
training:	Epoch: [49][354/408]	Loss 0.0083 (0.0248)	
training:	Epoch: [49][355/408]	Loss 0.0217 (0.0248)	
training:	Epoch: [49][356/408]	Loss 0.0042 (0.0247)	
training:	Epoch: [49][357/408]	Loss 0.0065 (0.0247)	
training:	Epoch: [49][358/408]	Loss 0.0058 (0.0246)	
training:	Epoch: [49][359/408]	Loss 0.0028 (0.0245)	
training:	Epoch: [49][360/408]	Loss 0.0086 (0.0245)	
training:	Epoch: [49][361/408]	Loss 0.0112 (0.0245)	
training:	Epoch: [49][362/408]	Loss 0.0051 (0.0244)	
training:	Epoch: [49][363/408]	Loss 0.0081 (0.0244)	
training:	Epoch: [49][364/408]	Loss 0.0079 (0.0243)	
training:	Epoch: [49][365/408]	Loss 0.0038 (0.0243)	
training:	Epoch: [49][366/408]	Loss 0.0062 (0.0242)	
training:	Epoch: [49][367/408]	Loss 0.0086 (0.0242)	
training:	Epoch: [49][368/408]	Loss 0.0036 (0.0241)	
training:	Epoch: [49][369/408]	Loss 0.0060 (0.0241)	
training:	Epoch: [49][370/408]	Loss 0.0084 (0.0240)	
training:	Epoch: [49][371/408]	Loss 0.0042 (0.0240)	
training:	Epoch: [49][372/408]	Loss 0.0094 (0.0239)	
training:	Epoch: [49][373/408]	Loss 0.0081 (0.0239)	
training:	Epoch: [49][374/408]	Loss 0.0047 (0.0238)	
training:	Epoch: [49][375/408]	Loss 0.0037 (0.0238)	
training:	Epoch: [49][376/408]	Loss 0.0125 (0.0238)	
training:	Epoch: [49][377/408]	Loss 0.0099 (0.0237)	
training:	Epoch: [49][378/408]	Loss 0.0058 (0.0237)	
training:	Epoch: [49][379/408]	Loss 0.0049 (0.0236)	
training:	Epoch: [49][380/408]	Loss 0.0080 (0.0236)	
training:	Epoch: [49][381/408]	Loss 0.0530 (0.0237)	
training:	Epoch: [49][382/408]	Loss 0.0070 (0.0236)	
training:	Epoch: [49][383/408]	Loss 0.0047 (0.0236)	
training:	Epoch: [49][384/408]	Loss 0.0038 (0.0235)	
training:	Epoch: [49][385/408]	Loss 0.0071 (0.0235)	
training:	Epoch: [49][386/408]	Loss 0.0040 (0.0234)	
training:	Epoch: [49][387/408]	Loss 0.0066 (0.0234)	
training:	Epoch: [49][388/408]	Loss 0.0040 (0.0233)	
training:	Epoch: [49][389/408]	Loss 0.1750 (0.0237)	
training:	Epoch: [49][390/408]	Loss 0.0062 (0.0237)	
training:	Epoch: [49][391/408]	Loss 0.0034 (0.0236)	
training:	Epoch: [49][392/408]	Loss 0.0041 (0.0236)	
training:	Epoch: [49][393/408]	Loss 0.0083 (0.0235)	
training:	Epoch: [49][394/408]	Loss 0.0034 (0.0235)	
training:	Epoch: [49][395/408]	Loss 0.0094 (0.0234)	
training:	Epoch: [49][396/408]	Loss 0.0027 (0.0234)	
training:	Epoch: [49][397/408]	Loss 0.0034 (0.0233)	
training:	Epoch: [49][398/408]	Loss 0.0156 (0.0233)	
training:	Epoch: [49][399/408]	Loss 0.1696 (0.0237)	
training:	Epoch: [49][400/408]	Loss 0.0195 (0.0237)	
training:	Epoch: [49][401/408]	Loss 0.0052 (0.0236)	
training:	Epoch: [49][402/408]	Loss 0.0094 (0.0236)	
training:	Epoch: [49][403/408]	Loss 0.0033 (0.0235)	
training:	Epoch: [49][404/408]	Loss 0.0070 (0.0235)	
training:	Epoch: [49][405/408]	Loss 0.0049 (0.0235)	
training:	Epoch: [49][406/408]	Loss 0.0056 (0.0234)	
training:	Epoch: [49][407/408]	Loss 0.0047 (0.0234)	
training:	Epoch: [49][408/408]	Loss 0.0032 (0.0233)	
Training:	 Loss: 0.0233

Training:	 ACC: 0.9948 0.9948 0.9941 0.9955
Validation:	 ACC: 0.7809 0.7822 0.8106 0.7511
Validation:	 Best_BACC: 0.8226 0.8250 0.8772 0.7679
Validation:	 Loss: 1.0701
Pretraining:	Epoch 50/200
----------
training:	Epoch: [50][1/408]	Loss 0.0038 (0.0038)	
training:	Epoch: [50][2/408]	Loss 0.0041 (0.0040)	
training:	Epoch: [50][3/408]	Loss 0.0039 (0.0040)	
training:	Epoch: [50][4/408]	Loss 0.0027 (0.0036)	
training:	Epoch: [50][5/408]	Loss 0.0047 (0.0038)	
training:	Epoch: [50][6/408]	Loss 0.0898 (0.0182)	
training:	Epoch: [50][7/408]	Loss 0.0045 (0.0162)	
training:	Epoch: [50][8/408]	Loss 0.0052 (0.0149)	
training:	Epoch: [50][9/408]	Loss 0.0092 (0.0142)	
training:	Epoch: [50][10/408]	Loss 0.0116 (0.0140)	
training:	Epoch: [50][11/408]	Loss 0.0053 (0.0132)	
training:	Epoch: [50][12/408]	Loss 0.0046 (0.0125)	
training:	Epoch: [50][13/408]	Loss 0.0080 (0.0121)	
training:	Epoch: [50][14/408]	Loss 0.0039 (0.0115)	
training:	Epoch: [50][15/408]	Loss 0.0044 (0.0111)	
training:	Epoch: [50][16/408]	Loss 0.0068 (0.0108)	
training:	Epoch: [50][17/408]	Loss 0.0035 (0.0104)	
training:	Epoch: [50][18/408]	Loss 0.0039 (0.0100)	
training:	Epoch: [50][19/408]	Loss 0.0095 (0.0100)	
training:	Epoch: [50][20/408]	Loss 0.0033 (0.0097)	
training:	Epoch: [50][21/408]	Loss 0.0067 (0.0095)	
training:	Epoch: [50][22/408]	Loss 0.0030 (0.0092)	
training:	Epoch: [50][23/408]	Loss 0.0054 (0.0090)	
training:	Epoch: [50][24/408]	Loss 0.0043 (0.0088)	
training:	Epoch: [50][25/408]	Loss 0.0034 (0.0086)	
training:	Epoch: [50][26/408]	Loss 0.0043 (0.0085)	
training:	Epoch: [50][27/408]	Loss 0.0039 (0.0083)	
training:	Epoch: [50][28/408]	Loss 0.1390 (0.0130)	
training:	Epoch: [50][29/408]	Loss 0.0048 (0.0127)	
training:	Epoch: [50][30/408]	Loss 0.0066 (0.0125)	
training:	Epoch: [50][31/408]	Loss 0.0046 (0.0122)	
training:	Epoch: [50][32/408]	Loss 0.0051 (0.0120)	
training:	Epoch: [50][33/408]	Loss 0.0041 (0.0118)	
training:	Epoch: [50][34/408]	Loss 0.0078 (0.0116)	
training:	Epoch: [50][35/408]	Loss 0.0030 (0.0114)	
training:	Epoch: [50][36/408]	Loss 0.0045 (0.0112)	
training:	Epoch: [50][37/408]	Loss 0.0041 (0.0110)	
training:	Epoch: [50][38/408]	Loss 0.0037 (0.0108)	
training:	Epoch: [50][39/408]	Loss 0.0048 (0.0107)	
training:	Epoch: [50][40/408]	Loss 0.0045 (0.0105)	
training:	Epoch: [50][41/408]	Loss 0.0027 (0.0103)	
training:	Epoch: [50][42/408]	Loss 0.0036 (0.0102)	
training:	Epoch: [50][43/408]	Loss 0.0061 (0.0101)	
training:	Epoch: [50][44/408]	Loss 0.0070 (0.0100)	
training:	Epoch: [50][45/408]	Loss 0.0027 (0.0098)	
training:	Epoch: [50][46/408]	Loss 0.0040 (0.0097)	
training:	Epoch: [50][47/408]	Loss 0.1452 (0.0126)	
training:	Epoch: [50][48/408]	Loss 0.0038 (0.0124)	
training:	Epoch: [50][49/408]	Loss 0.0077 (0.0123)	
training:	Epoch: [50][50/408]	Loss 0.0043 (0.0121)	
training:	Epoch: [50][51/408]	Loss 0.0069 (0.0120)	
training:	Epoch: [50][52/408]	Loss 0.0030 (0.0119)	
training:	Epoch: [50][53/408]	Loss 0.2498 (0.0164)	
training:	Epoch: [50][54/408]	Loss 0.0081 (0.0162)	
training:	Epoch: [50][55/408]	Loss 0.0071 (0.0160)	
training:	Epoch: [50][56/408]	Loss 0.0064 (0.0159)	
training:	Epoch: [50][57/408]	Loss 0.0038 (0.0157)	
training:	Epoch: [50][58/408]	Loss 0.0114 (0.0156)	
training:	Epoch: [50][59/408]	Loss 0.0074 (0.0154)	
training:	Epoch: [50][60/408]	Loss 0.0040 (0.0153)	
training:	Epoch: [50][61/408]	Loss 0.0040 (0.0151)	
training:	Epoch: [50][62/408]	Loss 0.0056 (0.0149)	
training:	Epoch: [50][63/408]	Loss 0.0050 (0.0148)	
training:	Epoch: [50][64/408]	Loss 0.0031 (0.0146)	
training:	Epoch: [50][65/408]	Loss 0.0025 (0.0144)	
training:	Epoch: [50][66/408]	Loss 0.1322 (0.0162)	
training:	Epoch: [50][67/408]	Loss 0.0026 (0.0160)	
training:	Epoch: [50][68/408]	Loss 0.0060 (0.0158)	
training:	Epoch: [50][69/408]	Loss 0.0029 (0.0156)	
training:	Epoch: [50][70/408]	Loss 0.0033 (0.0155)	
training:	Epoch: [50][71/408]	Loss 0.0056 (0.0153)	
training:	Epoch: [50][72/408]	Loss 0.0090 (0.0152)	
training:	Epoch: [50][73/408]	Loss 0.3338 (0.0196)	
training:	Epoch: [50][74/408]	Loss 0.0070 (0.0194)	
training:	Epoch: [50][75/408]	Loss 0.1301 (0.0209)	
training:	Epoch: [50][76/408]	Loss 0.0034 (0.0207)	
training:	Epoch: [50][77/408]	Loss 0.0874 (0.0215)	
training:	Epoch: [50][78/408]	Loss 0.0039 (0.0213)	
training:	Epoch: [50][79/408]	Loss 0.0044 (0.0211)	
training:	Epoch: [50][80/408]	Loss 0.0157 (0.0210)	
training:	Epoch: [50][81/408]	Loss 0.0099 (0.0209)	
training:	Epoch: [50][82/408]	Loss 0.0040 (0.0207)	
training:	Epoch: [50][83/408]	Loss 0.0044 (0.0205)	
training:	Epoch: [50][84/408]	Loss 0.0069 (0.0203)	
training:	Epoch: [50][85/408]	Loss 0.0048 (0.0201)	
training:	Epoch: [50][86/408]	Loss 0.0107 (0.0200)	
training:	Epoch: [50][87/408]	Loss 0.0075 (0.0199)	
training:	Epoch: [50][88/408]	Loss 0.0051 (0.0197)	
training:	Epoch: [50][89/408]	Loss 0.0029 (0.0195)	
training:	Epoch: [50][90/408]	Loss 0.0040 (0.0194)	
training:	Epoch: [50][91/408]	Loss 0.0046 (0.0192)	
training:	Epoch: [50][92/408]	Loss 0.0034 (0.0190)	
training:	Epoch: [50][93/408]	Loss 0.0124 (0.0190)	
training:	Epoch: [50][94/408]	Loss 0.0084 (0.0188)	
training:	Epoch: [50][95/408]	Loss 0.0036 (0.0187)	
training:	Epoch: [50][96/408]	Loss 0.3423 (0.0221)	
training:	Epoch: [50][97/408]	Loss 0.0042 (0.0219)	
training:	Epoch: [50][98/408]	Loss 0.0058 (0.0217)	
training:	Epoch: [50][99/408]	Loss 0.0040 (0.0215)	
training:	Epoch: [50][100/408]	Loss 0.1532 (0.0228)	
training:	Epoch: [50][101/408]	Loss 0.0067 (0.0227)	
training:	Epoch: [50][102/408]	Loss 0.0034 (0.0225)	
training:	Epoch: [50][103/408]	Loss 0.0065 (0.0223)	
training:	Epoch: [50][104/408]	Loss 0.0031 (0.0222)	
training:	Epoch: [50][105/408]	Loss 0.0050 (0.0220)	
training:	Epoch: [50][106/408]	Loss 0.0074 (0.0219)	
training:	Epoch: [50][107/408]	Loss 0.0048 (0.0217)	
training:	Epoch: [50][108/408]	Loss 0.0068 (0.0216)	
training:	Epoch: [50][109/408]	Loss 0.0111 (0.0215)	
training:	Epoch: [50][110/408]	Loss 0.0045 (0.0213)	
training:	Epoch: [50][111/408]	Loss 0.0033 (0.0211)	
training:	Epoch: [50][112/408]	Loss 0.1509 (0.0223)	
training:	Epoch: [50][113/408]	Loss 0.0087 (0.0222)	
training:	Epoch: [50][114/408]	Loss 0.0037 (0.0220)	
training:	Epoch: [50][115/408]	Loss 0.0079 (0.0219)	
training:	Epoch: [50][116/408]	Loss 0.0041 (0.0218)	
training:	Epoch: [50][117/408]	Loss 0.0039 (0.0216)	
training:	Epoch: [50][118/408]	Loss 0.0053 (0.0215)	
training:	Epoch: [50][119/408]	Loss 0.0046 (0.0213)	
training:	Epoch: [50][120/408]	Loss 0.0057 (0.0212)	
training:	Epoch: [50][121/408]	Loss 0.0029 (0.0210)	
training:	Epoch: [50][122/408]	Loss 0.0041 (0.0209)	
training:	Epoch: [50][123/408]	Loss 0.0039 (0.0208)	
training:	Epoch: [50][124/408]	Loss 0.0162 (0.0207)	
training:	Epoch: [50][125/408]	Loss 0.0039 (0.0206)	
training:	Epoch: [50][126/408]	Loss 0.0077 (0.0205)	
training:	Epoch: [50][127/408]	Loss 0.0023 (0.0203)	
training:	Epoch: [50][128/408]	Loss 0.0102 (0.0203)	
training:	Epoch: [50][129/408]	Loss 0.0047 (0.0201)	
training:	Epoch: [50][130/408]	Loss 0.2937 (0.0222)	
training:	Epoch: [50][131/408]	Loss 0.0056 (0.0221)	
training:	Epoch: [50][132/408]	Loss 0.0033 (0.0220)	
training:	Epoch: [50][133/408]	Loss 0.0030 (0.0218)	
training:	Epoch: [50][134/408]	Loss 0.0047 (0.0217)	
training:	Epoch: [50][135/408]	Loss 0.0047 (0.0216)	
training:	Epoch: [50][136/408]	Loss 0.0084 (0.0215)	
training:	Epoch: [50][137/408]	Loss 0.0042 (0.0214)	
training:	Epoch: [50][138/408]	Loss 0.0054 (0.0212)	
training:	Epoch: [50][139/408]	Loss 0.0040 (0.0211)	
training:	Epoch: [50][140/408]	Loss 0.0103 (0.0210)	
training:	Epoch: [50][141/408]	Loss 0.0037 (0.0209)	
training:	Epoch: [50][142/408]	Loss 0.0070 (0.0208)	
training:	Epoch: [50][143/408]	Loss 0.0031 (0.0207)	
training:	Epoch: [50][144/408]	Loss 0.0064 (0.0206)	
training:	Epoch: [50][145/408]	Loss 0.0132 (0.0205)	
training:	Epoch: [50][146/408]	Loss 0.0061 (0.0204)	
training:	Epoch: [50][147/408]	Loss 0.0039 (0.0203)	
training:	Epoch: [50][148/408]	Loss 0.0031 (0.0202)	
training:	Epoch: [50][149/408]	Loss 0.0053 (0.0201)	
training:	Epoch: [50][150/408]	Loss 0.0032 (0.0200)	
training:	Epoch: [50][151/408]	Loss 0.0047 (0.0199)	
training:	Epoch: [50][152/408]	Loss 0.0025 (0.0198)	
training:	Epoch: [50][153/408]	Loss 0.0038 (0.0197)	
training:	Epoch: [50][154/408]	Loss 0.0043 (0.0196)	
training:	Epoch: [50][155/408]	Loss 0.0057 (0.0195)	
training:	Epoch: [50][156/408]	Loss 0.0030 (0.0194)	
training:	Epoch: [50][157/408]	Loss 0.0036 (0.0193)	
training:	Epoch: [50][158/408]	Loss 0.0062 (0.0192)	
training:	Epoch: [50][159/408]	Loss 0.0032 (0.0191)	
training:	Epoch: [50][160/408]	Loss 0.0042 (0.0190)	
training:	Epoch: [50][161/408]	Loss 0.0034 (0.0189)	
training:	Epoch: [50][162/408]	Loss 0.0040 (0.0188)	
training:	Epoch: [50][163/408]	Loss 0.0030 (0.0187)	
training:	Epoch: [50][164/408]	Loss 0.0148 (0.0187)	
training:	Epoch: [50][165/408]	Loss 0.2932 (0.0204)	
training:	Epoch: [50][166/408]	Loss 0.0038 (0.0203)	
training:	Epoch: [50][167/408]	Loss 0.0081 (0.0202)	
training:	Epoch: [50][168/408]	Loss 0.0044 (0.0201)	
training:	Epoch: [50][169/408]	Loss 0.0049 (0.0200)	
training:	Epoch: [50][170/408]	Loss 0.0022 (0.0199)	
training:	Epoch: [50][171/408]	Loss 0.0045 (0.0198)	
training:	Epoch: [50][172/408]	Loss 0.0041 (0.0197)	
training:	Epoch: [50][173/408]	Loss 0.0038 (0.0196)	
training:	Epoch: [50][174/408]	Loss 0.2028 (0.0207)	
training:	Epoch: [50][175/408]	Loss 0.0047 (0.0206)	
training:	Epoch: [50][176/408]	Loss 0.0054 (0.0205)	
training:	Epoch: [50][177/408]	Loss 0.0119 (0.0205)	
training:	Epoch: [50][178/408]	Loss 0.0032 (0.0204)	
training:	Epoch: [50][179/408]	Loss 0.0040 (0.0203)	
training:	Epoch: [50][180/408]	Loss 0.0038 (0.0202)	
training:	Epoch: [50][181/408]	Loss 0.0083 (0.0201)	
training:	Epoch: [50][182/408]	Loss 0.0038 (0.0200)	
training:	Epoch: [50][183/408]	Loss 0.0037 (0.0199)	
training:	Epoch: [50][184/408]	Loss 0.0037 (0.0198)	
training:	Epoch: [50][185/408]	Loss 0.0053 (0.0198)	
training:	Epoch: [50][186/408]	Loss 0.0079 (0.0197)	
training:	Epoch: [50][187/408]	Loss 0.0048 (0.0196)	
training:	Epoch: [50][188/408]	Loss 0.1551 (0.0203)	
training:	Epoch: [50][189/408]	Loss 0.0051 (0.0203)	
training:	Epoch: [50][190/408]	Loss 0.0035 (0.0202)	
training:	Epoch: [50][191/408]	Loss 0.0032 (0.0201)	
training:	Epoch: [50][192/408]	Loss 0.0074 (0.0200)	
training:	Epoch: [50][193/408]	Loss 0.0042 (0.0199)	
training:	Epoch: [50][194/408]	Loss 0.0071 (0.0199)	
training:	Epoch: [50][195/408]	Loss 0.0055 (0.0198)	
training:	Epoch: [50][196/408]	Loss 0.0083 (0.0197)	
training:	Epoch: [50][197/408]	Loss 0.0048 (0.0197)	
training:	Epoch: [50][198/408]	Loss 0.0038 (0.0196)	
training:	Epoch: [50][199/408]	Loss 0.0081 (0.0195)	
training:	Epoch: [50][200/408]	Loss 0.0042 (0.0194)	
training:	Epoch: [50][201/408]	Loss 0.0038 (0.0194)	
training:	Epoch: [50][202/408]	Loss 0.0054 (0.0193)	
training:	Epoch: [50][203/408]	Loss 0.0042 (0.0192)	
training:	Epoch: [50][204/408]	Loss 0.0044 (0.0192)	
training:	Epoch: [50][205/408]	Loss 0.0046 (0.0191)	
training:	Epoch: [50][206/408]	Loss 0.0025 (0.0190)	
training:	Epoch: [50][207/408]	Loss 0.0041 (0.0189)	
training:	Epoch: [50][208/408]	Loss 0.0030 (0.0189)	
training:	Epoch: [50][209/408]	Loss 0.1397 (0.0194)	
training:	Epoch: [50][210/408]	Loss 0.0050 (0.0194)	
training:	Epoch: [50][211/408]	Loss 0.0028 (0.0193)	
training:	Epoch: [50][212/408]	Loss 0.0057 (0.0192)	
training:	Epoch: [50][213/408]	Loss 0.0040 (0.0192)	
training:	Epoch: [50][214/408]	Loss 0.0077 (0.0191)	
training:	Epoch: [50][215/408]	Loss 0.0073 (0.0190)	
training:	Epoch: [50][216/408]	Loss 0.0037 (0.0190)	
training:	Epoch: [50][217/408]	Loss 0.0079 (0.0189)	
training:	Epoch: [50][218/408]	Loss 0.0057 (0.0189)	
training:	Epoch: [50][219/408]	Loss 0.0034 (0.0188)	
training:	Epoch: [50][220/408]	Loss 0.0039 (0.0187)	
training:	Epoch: [50][221/408]	Loss 0.0059 (0.0187)	
training:	Epoch: [50][222/408]	Loss 0.0026 (0.0186)	
training:	Epoch: [50][223/408]	Loss 0.0988 (0.0190)	
training:	Epoch: [50][224/408]	Loss 0.0632 (0.0191)	
training:	Epoch: [50][225/408]	Loss 0.0042 (0.0191)	
training:	Epoch: [50][226/408]	Loss 0.0027 (0.0190)	
training:	Epoch: [50][227/408]	Loss 0.0052 (0.0189)	
training:	Epoch: [50][228/408]	Loss 0.0091 (0.0189)	
training:	Epoch: [50][229/408]	Loss 0.0125 (0.0189)	
training:	Epoch: [50][230/408]	Loss 0.0031 (0.0188)	
training:	Epoch: [50][231/408]	Loss 0.2477 (0.0198)	
training:	Epoch: [50][232/408]	Loss 0.0064 (0.0197)	
training:	Epoch: [50][233/408]	Loss 0.0038 (0.0197)	
training:	Epoch: [50][234/408]	Loss 0.0041 (0.0196)	
training:	Epoch: [50][235/408]	Loss 0.0079 (0.0196)	
training:	Epoch: [50][236/408]	Loss 0.0038 (0.0195)	
training:	Epoch: [50][237/408]	Loss 0.0245 (0.0195)	
training:	Epoch: [50][238/408]	Loss 0.0389 (0.0196)	
training:	Epoch: [50][239/408]	Loss 0.0076 (0.0195)	
training:	Epoch: [50][240/408]	Loss 0.0058 (0.0195)	
training:	Epoch: [50][241/408]	Loss 0.0051 (0.0194)	
training:	Epoch: [50][242/408]	Loss 0.0956 (0.0197)	
training:	Epoch: [50][243/408]	Loss 0.0078 (0.0197)	
training:	Epoch: [50][244/408]	Loss 0.0046 (0.0196)	
training:	Epoch: [50][245/408]	Loss 0.0041 (0.0196)	
training:	Epoch: [50][246/408]	Loss 0.0088 (0.0195)	
training:	Epoch: [50][247/408]	Loss 0.0065 (0.0195)	
training:	Epoch: [50][248/408]	Loss 0.0055 (0.0194)	
training:	Epoch: [50][249/408]	Loss 0.0062 (0.0194)	
training:	Epoch: [50][250/408]	Loss 0.0038 (0.0193)	
training:	Epoch: [50][251/408]	Loss 0.0053 (0.0192)	
training:	Epoch: [50][252/408]	Loss 0.0050 (0.0192)	
training:	Epoch: [50][253/408]	Loss 0.0046 (0.0191)	
training:	Epoch: [50][254/408]	Loss 0.0043 (0.0191)	
training:	Epoch: [50][255/408]	Loss 0.0143 (0.0190)	
training:	Epoch: [50][256/408]	Loss 0.0029 (0.0190)	
training:	Epoch: [50][257/408]	Loss 0.0039 (0.0189)	
training:	Epoch: [50][258/408]	Loss 0.0506 (0.0191)	
training:	Epoch: [50][259/408]	Loss 0.0038 (0.0190)	
training:	Epoch: [50][260/408]	Loss 0.0036 (0.0189)	
training:	Epoch: [50][261/408]	Loss 0.0056 (0.0189)	
training:	Epoch: [50][262/408]	Loss 0.0054 (0.0188)	
training:	Epoch: [50][263/408]	Loss 0.0050 (0.0188)	
training:	Epoch: [50][264/408]	Loss 0.0033 (0.0187)	
training:	Epoch: [50][265/408]	Loss 0.0042 (0.0187)	
training:	Epoch: [50][266/408]	Loss 0.0034 (0.0186)	
training:	Epoch: [50][267/408]	Loss 0.0046 (0.0186)	
training:	Epoch: [50][268/408]	Loss 0.0060 (0.0185)	
training:	Epoch: [50][269/408]	Loss 0.0040 (0.0185)	
training:	Epoch: [50][270/408]	Loss 0.0041 (0.0184)	
training:	Epoch: [50][271/408]	Loss 0.0052 (0.0184)	
training:	Epoch: [50][272/408]	Loss 0.0049 (0.0183)	
training:	Epoch: [50][273/408]	Loss 0.0584 (0.0184)	
training:	Epoch: [50][274/408]	Loss 0.0133 (0.0184)	
training:	Epoch: [50][275/408]	Loss 0.0043 (0.0184)	
training:	Epoch: [50][276/408]	Loss 0.0066 (0.0183)	
training:	Epoch: [50][277/408]	Loss 0.1707 (0.0189)	
training:	Epoch: [50][278/408]	Loss 0.0050 (0.0188)	
training:	Epoch: [50][279/408]	Loss 0.0062 (0.0188)	
training:	Epoch: [50][280/408]	Loss 0.0026 (0.0187)	
training:	Epoch: [50][281/408]	Loss 0.0022 (0.0187)	
training:	Epoch: [50][282/408]	Loss 0.0060 (0.0186)	
training:	Epoch: [50][283/408]	Loss 0.0062 (0.0186)	
training:	Epoch: [50][284/408]	Loss 0.0800 (0.0188)	
training:	Epoch: [50][285/408]	Loss 0.1809 (0.0194)	
training:	Epoch: [50][286/408]	Loss 0.0063 (0.0193)	
training:	Epoch: [50][287/408]	Loss 0.0080 (0.0193)	
training:	Epoch: [50][288/408]	Loss 0.0049 (0.0192)	
training:	Epoch: [50][289/408]	Loss 0.0126 (0.0192)	
training:	Epoch: [50][290/408]	Loss 0.0451 (0.0193)	
training:	Epoch: [50][291/408]	Loss 0.2306 (0.0200)	
training:	Epoch: [50][292/408]	Loss 0.0087 (0.0200)	
training:	Epoch: [50][293/408]	Loss 0.0466 (0.0201)	
training:	Epoch: [50][294/408]	Loss 0.0065 (0.0200)	
training:	Epoch: [50][295/408]	Loss 0.0026 (0.0200)	
training:	Epoch: [50][296/408]	Loss 0.0125 (0.0199)	
training:	Epoch: [50][297/408]	Loss 0.0638 (0.0201)	
training:	Epoch: [50][298/408]	Loss 0.0093 (0.0201)	
training:	Epoch: [50][299/408]	Loss 0.0066 (0.0200)	
training:	Epoch: [50][300/408]	Loss 0.0077 (0.0200)	
training:	Epoch: [50][301/408]	Loss 0.0027 (0.0199)	
training:	Epoch: [50][302/408]	Loss 0.0137 (0.0199)	
training:	Epoch: [50][303/408]	Loss 0.0211 (0.0199)	
training:	Epoch: [50][304/408]	Loss 0.0029 (0.0198)	
training:	Epoch: [50][305/408]	Loss 0.0026 (0.0198)	
training:	Epoch: [50][306/408]	Loss 0.0039 (0.0197)	
training:	Epoch: [50][307/408]	Loss 0.0034 (0.0197)	
training:	Epoch: [50][308/408]	Loss 0.0555 (0.0198)	
training:	Epoch: [50][309/408]	Loss 0.3099 (0.0207)	
training:	Epoch: [50][310/408]	Loss 0.0042 (0.0207)	
training:	Epoch: [50][311/408]	Loss 0.0063 (0.0206)	
training:	Epoch: [50][312/408]	Loss 0.0039 (0.0206)	
training:	Epoch: [50][313/408]	Loss 0.1872 (0.0211)	
training:	Epoch: [50][314/408]	Loss 0.0042 (0.0211)	
training:	Epoch: [50][315/408]	Loss 0.0071 (0.0210)	
training:	Epoch: [50][316/408]	Loss 0.0102 (0.0210)	
training:	Epoch: [50][317/408]	Loss 0.0054 (0.0209)	
training:	Epoch: [50][318/408]	Loss 0.0123 (0.0209)	
training:	Epoch: [50][319/408]	Loss 0.0028 (0.0209)	
training:	Epoch: [50][320/408]	Loss 0.0024 (0.0208)	
training:	Epoch: [50][321/408]	Loss 0.0044 (0.0207)	
training:	Epoch: [50][322/408]	Loss 0.0040 (0.0207)	
training:	Epoch: [50][323/408]	Loss 0.0046 (0.0206)	
training:	Epoch: [50][324/408]	Loss 0.0287 (0.0207)	
training:	Epoch: [50][325/408]	Loss 0.0037 (0.0206)	
training:	Epoch: [50][326/408]	Loss 0.0041 (0.0206)	
training:	Epoch: [50][327/408]	Loss 0.0132 (0.0205)	
training:	Epoch: [50][328/408]	Loss 0.0038 (0.0205)	
training:	Epoch: [50][329/408]	Loss 0.0040 (0.0204)	
training:	Epoch: [50][330/408]	Loss 0.0080 (0.0204)	
training:	Epoch: [50][331/408]	Loss 0.0036 (0.0204)	
training:	Epoch: [50][332/408]	Loss 0.2684 (0.0211)	
training:	Epoch: [50][333/408]	Loss 0.0066 (0.0211)	
training:	Epoch: [50][334/408]	Loss 0.0044 (0.0210)	
training:	Epoch: [50][335/408]	Loss 0.2480 (0.0217)	
training:	Epoch: [50][336/408]	Loss 0.0028 (0.0216)	
training:	Epoch: [50][337/408]	Loss 0.0040 (0.0216)	
training:	Epoch: [50][338/408]	Loss 0.0045 (0.0215)	
training:	Epoch: [50][339/408]	Loss 0.0067 (0.0215)	
training:	Epoch: [50][340/408]	Loss 0.0078 (0.0214)	
training:	Epoch: [50][341/408]	Loss 0.0065 (0.0214)	
training:	Epoch: [50][342/408]	Loss 0.0036 (0.0213)	
training:	Epoch: [50][343/408]	Loss 0.0055 (0.0213)	
training:	Epoch: [50][344/408]	Loss 0.0461 (0.0214)	
training:	Epoch: [50][345/408]	Loss 0.0169 (0.0214)	
training:	Epoch: [50][346/408]	Loss 0.0164 (0.0213)	
training:	Epoch: [50][347/408]	Loss 0.0040 (0.0213)	
training:	Epoch: [50][348/408]	Loss 0.0057 (0.0212)	
training:	Epoch: [50][349/408]	Loss 0.0052 (0.0212)	
training:	Epoch: [50][350/408]	Loss 0.0073 (0.0212)	
training:	Epoch: [50][351/408]	Loss 0.0047 (0.0211)	
training:	Epoch: [50][352/408]	Loss 0.0038 (0.0211)	
training:	Epoch: [50][353/408]	Loss 0.1477 (0.0214)	
training:	Epoch: [50][354/408]	Loss 0.0086 (0.0214)	
training:	Epoch: [50][355/408]	Loss 0.0387 (0.0214)	
training:	Epoch: [50][356/408]	Loss 0.0056 (0.0214)	
training:	Epoch: [50][357/408]	Loss 0.0050 (0.0213)	
training:	Epoch: [50][358/408]	Loss 0.0103 (0.0213)	
training:	Epoch: [50][359/408]	Loss 0.0045 (0.0213)	
training:	Epoch: [50][360/408]	Loss 0.0032 (0.0212)	
training:	Epoch: [50][361/408]	Loss 0.0055 (0.0212)	
training:	Epoch: [50][362/408]	Loss 0.2095 (0.0217)	
training:	Epoch: [50][363/408]	Loss 0.0046 (0.0216)	
training:	Epoch: [50][364/408]	Loss 0.0056 (0.0216)	
training:	Epoch: [50][365/408]	Loss 0.0035 (0.0216)	
training:	Epoch: [50][366/408]	Loss 0.0022 (0.0215)	
training:	Epoch: [50][367/408]	Loss 0.0039 (0.0215)	
training:	Epoch: [50][368/408]	Loss 0.0051 (0.0214)	
training:	Epoch: [50][369/408]	Loss 0.0042 (0.0214)	
training:	Epoch: [50][370/408]	Loss 0.1408 (0.0217)	
training:	Epoch: [50][371/408]	Loss 0.0549 (0.0218)	
training:	Epoch: [50][372/408]	Loss 0.0069 (0.0217)	
training:	Epoch: [50][373/408]	Loss 0.0030 (0.0217)	
training:	Epoch: [50][374/408]	Loss 0.0024 (0.0216)	
training:	Epoch: [50][375/408]	Loss 0.0102 (0.0216)	
training:	Epoch: [50][376/408]	Loss 0.0030 (0.0216)	
training:	Epoch: [50][377/408]	Loss 0.2419 (0.0221)	
training:	Epoch: [50][378/408]	Loss 0.0045 (0.0221)	
training:	Epoch: [50][379/408]	Loss 0.0093 (0.0221)	
training:	Epoch: [50][380/408]	Loss 0.2722 (0.0227)	
training:	Epoch: [50][381/408]	Loss 0.1254 (0.0230)	
training:	Epoch: [50][382/408]	Loss 0.1850 (0.0234)	
training:	Epoch: [50][383/408]	Loss 0.1698 (0.0238)	
training:	Epoch: [50][384/408]	Loss 0.0345 (0.0238)	
training:	Epoch: [50][385/408]	Loss 0.0030 (0.0238)	
training:	Epoch: [50][386/408]	Loss 0.0029 (0.0237)	
training:	Epoch: [50][387/408]	Loss 0.0030 (0.0237)	
training:	Epoch: [50][388/408]	Loss 0.0157 (0.0236)	
training:	Epoch: [50][389/408]	Loss 0.0041 (0.0236)	
training:	Epoch: [50][390/408]	Loss 0.0057 (0.0235)	
training:	Epoch: [50][391/408]	Loss 0.1023 (0.0237)	
training:	Epoch: [50][392/408]	Loss 0.1013 (0.0239)	
training:	Epoch: [50][393/408]	Loss 0.0126 (0.0239)	
training:	Epoch: [50][394/408]	Loss 0.0032 (0.0239)	
training:	Epoch: [50][395/408]	Loss 0.0107 (0.0238)	
training:	Epoch: [50][396/408]	Loss 0.0037 (0.0238)	
training:	Epoch: [50][397/408]	Loss 0.1835 (0.0242)	
training:	Epoch: [50][398/408]	Loss 0.4989 (0.0254)	
training:	Epoch: [50][399/408]	Loss 0.0065 (0.0253)	
training:	Epoch: [50][400/408]	Loss 0.0087 (0.0253)	
training:	Epoch: [50][401/408]	Loss 0.0095 (0.0252)	
training:	Epoch: [50][402/408]	Loss 0.0742 (0.0254)	
training:	Epoch: [50][403/408]	Loss 0.0055 (0.0253)	
training:	Epoch: [50][404/408]	Loss 0.0083 (0.0253)	
training:	Epoch: [50][405/408]	Loss 0.2369 (0.0258)	
training:	Epoch: [50][406/408]	Loss 0.0061 (0.0257)	
training:	Epoch: [50][407/408]	Loss 0.2549 (0.0263)	
training:	Epoch: [50][408/408]	Loss 0.0059 (0.0263)	
Training:	 Loss: 0.0262

Training:	 ACC: 0.9919 0.9917 0.9888 0.9949
Validation:	 ACC: 0.7848 0.7844 0.7748 0.7948
Validation:	 Best_BACC: 0.8226 0.8250 0.8772 0.7679
Validation:	 Loss: 1.0405
Pretraining:	Epoch 51/200
----------
training:	Epoch: [51][1/408]	Loss 0.1467 (0.1467)	
training:	Epoch: [51][2/408]	Loss 0.0040 (0.0753)	
training:	Epoch: [51][3/408]	Loss 0.0115 (0.0541)	
training:	Epoch: [51][4/408]	Loss 0.0203 (0.0456)	
training:	Epoch: [51][5/408]	Loss 0.0081 (0.0381)	
training:	Epoch: [51][6/408]	Loss 0.0045 (0.0325)	
training:	Epoch: [51][7/408]	Loss 0.0097 (0.0292)	
training:	Epoch: [51][8/408]	Loss 0.0060 (0.0263)	
training:	Epoch: [51][9/408]	Loss 0.0041 (0.0239)	
training:	Epoch: [51][10/408]	Loss 0.0440 (0.0259)	
training:	Epoch: [51][11/408]	Loss 0.1240 (0.0348)	
training:	Epoch: [51][12/408]	Loss 0.0091 (0.0327)	
training:	Epoch: [51][13/408]	Loss 0.3256 (0.0552)	
training:	Epoch: [51][14/408]	Loss 0.0028 (0.0514)	
training:	Epoch: [51][15/408]	Loss 0.0075 (0.0485)	
training:	Epoch: [51][16/408]	Loss 0.0049 (0.0458)	
training:	Epoch: [51][17/408]	Loss 0.0034 (0.0433)	
training:	Epoch: [51][18/408]	Loss 0.0027 (0.0410)	
training:	Epoch: [51][19/408]	Loss 0.0351 (0.0407)	
training:	Epoch: [51][20/408]	Loss 0.1691 (0.0471)	
training:	Epoch: [51][21/408]	Loss 0.0104 (0.0454)	
training:	Epoch: [51][22/408]	Loss 0.0031 (0.0435)	
training:	Epoch: [51][23/408]	Loss 0.0101 (0.0420)	
training:	Epoch: [51][24/408]	Loss 0.0037 (0.0404)	
training:	Epoch: [51][25/408]	Loss 0.0758 (0.0418)	
training:	Epoch: [51][26/408]	Loss 0.0048 (0.0404)	
training:	Epoch: [51][27/408]	Loss 0.0116 (0.0394)	
training:	Epoch: [51][28/408]	Loss 0.0028 (0.0380)	
training:	Epoch: [51][29/408]	Loss 0.0042 (0.0369)	
training:	Epoch: [51][30/408]	Loss 0.0319 (0.0367)	
training:	Epoch: [51][31/408]	Loss 0.0032 (0.0356)	
training:	Epoch: [51][32/408]	Loss 0.0041 (0.0346)	
training:	Epoch: [51][33/408]	Loss 0.0049 (0.0337)	
training:	Epoch: [51][34/408]	Loss 0.0093 (0.0330)	
training:	Epoch: [51][35/408]	Loss 0.0071 (0.0323)	
training:	Epoch: [51][36/408]	Loss 0.0108 (0.0317)	
training:	Epoch: [51][37/408]	Loss 0.0051 (0.0310)	
training:	Epoch: [51][38/408]	Loss 0.0126 (0.0305)	
training:	Epoch: [51][39/408]	Loss 0.0048 (0.0298)	
training:	Epoch: [51][40/408]	Loss 0.0049 (0.0292)	
training:	Epoch: [51][41/408]	Loss 0.0093 (0.0287)	
training:	Epoch: [51][42/408]	Loss 0.0061 (0.0282)	
training:	Epoch: [51][43/408]	Loss 0.0125 (0.0278)	
training:	Epoch: [51][44/408]	Loss 0.0100 (0.0274)	
training:	Epoch: [51][45/408]	Loss 0.0063 (0.0269)	
training:	Epoch: [51][46/408]	Loss 0.0089 (0.0266)	
training:	Epoch: [51][47/408]	Loss 0.0084 (0.0262)	
training:	Epoch: [51][48/408]	Loss 0.0056 (0.0257)	
training:	Epoch: [51][49/408]	Loss 0.1146 (0.0275)	
training:	Epoch: [51][50/408]	Loss 0.0051 (0.0271)	
training:	Epoch: [51][51/408]	Loss 0.2331 (0.0311)	
training:	Epoch: [51][52/408]	Loss 0.0039 (0.0306)	
training:	Epoch: [51][53/408]	Loss 0.0075 (0.0302)	
training:	Epoch: [51][54/408]	Loss 0.0048 (0.0297)	
training:	Epoch: [51][55/408]	Loss 0.1470 (0.0318)	
training:	Epoch: [51][56/408]	Loss 0.0113 (0.0315)	
training:	Epoch: [51][57/408]	Loss 0.0919 (0.0325)	
training:	Epoch: [51][58/408]	Loss 0.2049 (0.0355)	
training:	Epoch: [51][59/408]	Loss 0.0073 (0.0350)	
training:	Epoch: [51][60/408]	Loss 0.0117 (0.0346)	
training:	Epoch: [51][61/408]	Loss 0.0035 (0.0341)	
training:	Epoch: [51][62/408]	Loss 0.0057 (0.0337)	
training:	Epoch: [51][63/408]	Loss 0.0073 (0.0333)	
training:	Epoch: [51][64/408]	Loss 0.0076 (0.0328)	
training:	Epoch: [51][65/408]	Loss 0.0023 (0.0324)	
training:	Epoch: [51][66/408]	Loss 0.0046 (0.0320)	
training:	Epoch: [51][67/408]	Loss 0.0818 (0.0327)	
training:	Epoch: [51][68/408]	Loss 0.0035 (0.0323)	
training:	Epoch: [51][69/408]	Loss 0.0175 (0.0321)	
training:	Epoch: [51][70/408]	Loss 0.0087 (0.0317)	
training:	Epoch: [51][71/408]	Loss 0.3302 (0.0359)	
training:	Epoch: [51][72/408]	Loss 0.0178 (0.0357)	
training:	Epoch: [51][73/408]	Loss 0.0056 (0.0353)	
training:	Epoch: [51][74/408]	Loss 0.0038 (0.0348)	
training:	Epoch: [51][75/408]	Loss 0.0028 (0.0344)	
training:	Epoch: [51][76/408]	Loss 0.0081 (0.0341)	
training:	Epoch: [51][77/408]	Loss 0.0029 (0.0337)	
training:	Epoch: [51][78/408]	Loss 0.0058 (0.0333)	
training:	Epoch: [51][79/408]	Loss 0.0138 (0.0331)	
training:	Epoch: [51][80/408]	Loss 0.0035 (0.0327)	
training:	Epoch: [51][81/408]	Loss 0.0164 (0.0325)	
training:	Epoch: [51][82/408]	Loss 0.1927 (0.0344)	
training:	Epoch: [51][83/408]	Loss 0.0081 (0.0341)	
training:	Epoch: [51][84/408]	Loss 0.0148 (0.0339)	
training:	Epoch: [51][85/408]	Loss 0.0056 (0.0336)	
training:	Epoch: [51][86/408]	Loss 0.0039 (0.0332)	
training:	Epoch: [51][87/408]	Loss 0.0048 (0.0329)	
training:	Epoch: [51][88/408]	Loss 0.0984 (0.0336)	
training:	Epoch: [51][89/408]	Loss 0.1291 (0.0347)	
training:	Epoch: [51][90/408]	Loss 0.1655 (0.0362)	
training:	Epoch: [51][91/408]	Loss 0.0036 (0.0358)	
training:	Epoch: [51][92/408]	Loss 0.0096 (0.0355)	
training:	Epoch: [51][93/408]	Loss 0.0050 (0.0352)	
training:	Epoch: [51][94/408]	Loss 0.0071 (0.0349)	
training:	Epoch: [51][95/408]	Loss 0.0081 (0.0346)	
training:	Epoch: [51][96/408]	Loss 0.0061 (0.0343)	
training:	Epoch: [51][97/408]	Loss 0.4601 (0.0387)	
training:	Epoch: [51][98/408]	Loss 0.0070 (0.0384)	
training:	Epoch: [51][99/408]	Loss 0.2027 (0.0400)	
training:	Epoch: [51][100/408]	Loss 0.0156 (0.0398)	
training:	Epoch: [51][101/408]	Loss 0.0084 (0.0395)	
training:	Epoch: [51][102/408]	Loss 0.0057 (0.0391)	
training:	Epoch: [51][103/408]	Loss 0.0056 (0.0388)	
training:	Epoch: [51][104/408]	Loss 0.1391 (0.0398)	
training:	Epoch: [51][105/408]	Loss 0.0054 (0.0395)	
training:	Epoch: [51][106/408]	Loss 0.0080 (0.0392)	
training:	Epoch: [51][107/408]	Loss 0.0097 (0.0389)	
training:	Epoch: [51][108/408]	Loss 0.0044 (0.0386)	
training:	Epoch: [51][109/408]	Loss 0.0182 (0.0384)	
training:	Epoch: [51][110/408]	Loss 0.0156 (0.0382)	
training:	Epoch: [51][111/408]	Loss 0.0095 (0.0379)	
training:	Epoch: [51][112/408]	Loss 0.0691 (0.0382)	
training:	Epoch: [51][113/408]	Loss 0.0075 (0.0379)	
training:	Epoch: [51][114/408]	Loss 0.0074 (0.0377)	
training:	Epoch: [51][115/408]	Loss 0.0068 (0.0374)	
training:	Epoch: [51][116/408]	Loss 0.0175 (0.0372)	
training:	Epoch: [51][117/408]	Loss 0.0026 (0.0369)	
training:	Epoch: [51][118/408]	Loss 0.0074 (0.0367)	
training:	Epoch: [51][119/408]	Loss 0.0041 (0.0364)	
training:	Epoch: [51][120/408]	Loss 0.0063 (0.0361)	
training:	Epoch: [51][121/408]	Loss 0.0060 (0.0359)	
training:	Epoch: [51][122/408]	Loss 0.0208 (0.0358)	
training:	Epoch: [51][123/408]	Loss 0.0045 (0.0355)	
training:	Epoch: [51][124/408]	Loss 0.0059 (0.0353)	
training:	Epoch: [51][125/408]	Loss 0.0608 (0.0355)	
training:	Epoch: [51][126/408]	Loss 0.0031 (0.0352)	
training:	Epoch: [51][127/408]	Loss 0.0088 (0.0350)	
training:	Epoch: [51][128/408]	Loss 0.0073 (0.0348)	
training:	Epoch: [51][129/408]	Loss 0.0023 (0.0346)	
training:	Epoch: [51][130/408]	Loss 0.0035 (0.0343)	
training:	Epoch: [51][131/408]	Loss 0.0046 (0.0341)	
training:	Epoch: [51][132/408]	Loss 0.0047 (0.0339)	
training:	Epoch: [51][133/408]	Loss 0.0576 (0.0340)	
training:	Epoch: [51][134/408]	Loss 0.0037 (0.0338)	
training:	Epoch: [51][135/408]	Loss 0.0066 (0.0336)	
training:	Epoch: [51][136/408]	Loss 0.0041 (0.0334)	
training:	Epoch: [51][137/408]	Loss 0.0133 (0.0332)	
training:	Epoch: [51][138/408]	Loss 0.0100 (0.0331)	
training:	Epoch: [51][139/408]	Loss 0.0036 (0.0329)	
training:	Epoch: [51][140/408]	Loss 0.0024 (0.0327)	
training:	Epoch: [51][141/408]	Loss 0.0060 (0.0325)	
training:	Epoch: [51][142/408]	Loss 0.0051 (0.0323)	
training:	Epoch: [51][143/408]	Loss 0.0169 (0.0322)	
training:	Epoch: [51][144/408]	Loss 0.0028 (0.0320)	
training:	Epoch: [51][145/408]	Loss 0.0038 (0.0318)	
training:	Epoch: [51][146/408]	Loss 0.0061 (0.0316)	
training:	Epoch: [51][147/408]	Loss 0.0076 (0.0314)	
training:	Epoch: [51][148/408]	Loss 0.0782 (0.0317)	
training:	Epoch: [51][149/408]	Loss 0.1024 (0.0322)	
training:	Epoch: [51][150/408]	Loss 0.0041 (0.0320)	
training:	Epoch: [51][151/408]	Loss 0.0099 (0.0319)	
training:	Epoch: [51][152/408]	Loss 0.0036 (0.0317)	
training:	Epoch: [51][153/408]	Loss 0.0042 (0.0315)	
training:	Epoch: [51][154/408]	Loss 0.0054 (0.0313)	
training:	Epoch: [51][155/408]	Loss 0.0035 (0.0312)	
training:	Epoch: [51][156/408]	Loss 0.0576 (0.0313)	
training:	Epoch: [51][157/408]	Loss 0.2220 (0.0326)	
training:	Epoch: [51][158/408]	Loss 0.0082 (0.0324)	
training:	Epoch: [51][159/408]	Loss 0.0171 (0.0323)	
training:	Epoch: [51][160/408]	Loss 0.1732 (0.0332)	
training:	Epoch: [51][161/408]	Loss 0.2131 (0.0343)	
training:	Epoch: [51][162/408]	Loss 0.0042 (0.0341)	
training:	Epoch: [51][163/408]	Loss 0.0036 (0.0339)	
training:	Epoch: [51][164/408]	Loss 0.0084 (0.0338)	
training:	Epoch: [51][165/408]	Loss 0.0035 (0.0336)	
training:	Epoch: [51][166/408]	Loss 0.0154 (0.0335)	
training:	Epoch: [51][167/408]	Loss 0.0114 (0.0333)	
training:	Epoch: [51][168/408]	Loss 0.0038 (0.0332)	
training:	Epoch: [51][169/408]	Loss 0.0095 (0.0330)	
training:	Epoch: [51][170/408]	Loss 0.0180 (0.0329)	
training:	Epoch: [51][171/408]	Loss 0.0126 (0.0328)	
training:	Epoch: [51][172/408]	Loss 0.0052 (0.0327)	
training:	Epoch: [51][173/408]	Loss 0.0069 (0.0325)	
training:	Epoch: [51][174/408]	Loss 0.1118 (0.0330)	
training:	Epoch: [51][175/408]	Loss 0.0045 (0.0328)	
training:	Epoch: [51][176/408]	Loss 0.0023 (0.0326)	
training:	Epoch: [51][177/408]	Loss 0.0029 (0.0325)	
training:	Epoch: [51][178/408]	Loss 0.0057 (0.0323)	
training:	Epoch: [51][179/408]	Loss 0.0131 (0.0322)	
training:	Epoch: [51][180/408]	Loss 0.0120 (0.0321)	
training:	Epoch: [51][181/408]	Loss 0.0156 (0.0320)	
training:	Epoch: [51][182/408]	Loss 0.0044 (0.0319)	
training:	Epoch: [51][183/408]	Loss 0.0140 (0.0318)	
training:	Epoch: [51][184/408]	Loss 0.0037 (0.0316)	
training:	Epoch: [51][185/408]	Loss 0.0191 (0.0315)	
training:	Epoch: [51][186/408]	Loss 0.0040 (0.0314)	
training:	Epoch: [51][187/408]	Loss 0.0178 (0.0313)	
training:	Epoch: [51][188/408]	Loss 0.0223 (0.0313)	
training:	Epoch: [51][189/408]	Loss 0.0040 (0.0311)	
training:	Epoch: [51][190/408]	Loss 0.2136 (0.0321)	
training:	Epoch: [51][191/408]	Loss 0.0045 (0.0319)	
training:	Epoch: [51][192/408]	Loss 0.2199 (0.0329)	
training:	Epoch: [51][193/408]	Loss 0.0727 (0.0331)	
training:	Epoch: [51][194/408]	Loss 0.0050 (0.0330)	
training:	Epoch: [51][195/408]	Loss 0.0062 (0.0328)	
training:	Epoch: [51][196/408]	Loss 0.0038 (0.0327)	
training:	Epoch: [51][197/408]	Loss 0.0140 (0.0326)	
training:	Epoch: [51][198/408]	Loss 0.0061 (0.0325)	
training:	Epoch: [51][199/408]	Loss 0.0058 (0.0323)	
training:	Epoch: [51][200/408]	Loss 0.0030 (0.0322)	
training:	Epoch: [51][201/408]	Loss 0.0061 (0.0321)	
training:	Epoch: [51][202/408]	Loss 0.0033 (0.0319)	
training:	Epoch: [51][203/408]	Loss 0.0079 (0.0318)	
training:	Epoch: [51][204/408]	Loss 0.0117 (0.0317)	
training:	Epoch: [51][205/408]	Loss 0.0053 (0.0316)	
training:	Epoch: [51][206/408]	Loss 0.0050 (0.0314)	
training:	Epoch: [51][207/408]	Loss 0.0069 (0.0313)	
training:	Epoch: [51][208/408]	Loss 0.0056 (0.0312)	
training:	Epoch: [51][209/408]	Loss 0.0029 (0.0311)	
training:	Epoch: [51][210/408]	Loss 0.0050 (0.0309)	
training:	Epoch: [51][211/408]	Loss 0.0091 (0.0308)	
training:	Epoch: [51][212/408]	Loss 0.0056 (0.0307)	
training:	Epoch: [51][213/408]	Loss 0.0024 (0.0306)	
training:	Epoch: [51][214/408]	Loss 0.0124 (0.0305)	
training:	Epoch: [51][215/408]	Loss 0.0026 (0.0304)	
training:	Epoch: [51][216/408]	Loss 0.0029 (0.0302)	
training:	Epoch: [51][217/408]	Loss 0.0028 (0.0301)	
training:	Epoch: [51][218/408]	Loss 0.0289 (0.0301)	
training:	Epoch: [51][219/408]	Loss 0.3259 (0.0315)	
training:	Epoch: [51][220/408]	Loss 0.0054 (0.0313)	
training:	Epoch: [51][221/408]	Loss 0.0039 (0.0312)	
training:	Epoch: [51][222/408]	Loss 0.0087 (0.0311)	
training:	Epoch: [51][223/408]	Loss 0.0028 (0.0310)	
training:	Epoch: [51][224/408]	Loss 0.0104 (0.0309)	
training:	Epoch: [51][225/408]	Loss 0.0024 (0.0308)	
training:	Epoch: [51][226/408]	Loss 0.0095 (0.0307)	
training:	Epoch: [51][227/408]	Loss 0.0103 (0.0306)	
training:	Epoch: [51][228/408]	Loss 0.0050 (0.0305)	
training:	Epoch: [51][229/408]	Loss 0.0040 (0.0304)	
training:	Epoch: [51][230/408]	Loss 0.0180 (0.0303)	
training:	Epoch: [51][231/408]	Loss 0.0739 (0.0305)	
training:	Epoch: [51][232/408]	Loss 0.0035 (0.0304)	
training:	Epoch: [51][233/408]	Loss 0.0030 (0.0303)	
training:	Epoch: [51][234/408]	Loss 0.0104 (0.0302)	
training:	Epoch: [51][235/408]	Loss 0.0031 (0.0301)	
training:	Epoch: [51][236/408]	Loss 0.0031 (0.0299)	
training:	Epoch: [51][237/408]	Loss 0.0039 (0.0298)	
training:	Epoch: [51][238/408]	Loss 0.0050 (0.0297)	
training:	Epoch: [51][239/408]	Loss 0.0036 (0.0296)	
training:	Epoch: [51][240/408]	Loss 0.0030 (0.0295)	
training:	Epoch: [51][241/408]	Loss 0.2444 (0.0304)	
training:	Epoch: [51][242/408]	Loss 0.0116 (0.0303)	
training:	Epoch: [51][243/408]	Loss 0.0041 (0.0302)	
training:	Epoch: [51][244/408]	Loss 0.1220 (0.0306)	
training:	Epoch: [51][245/408]	Loss 0.0047 (0.0305)	
training:	Epoch: [51][246/408]	Loss 0.0050 (0.0304)	
training:	Epoch: [51][247/408]	Loss 0.0155 (0.0303)	
training:	Epoch: [51][248/408]	Loss 0.0048 (0.0302)	
training:	Epoch: [51][249/408]	Loss 0.0057 (0.0301)	
training:	Epoch: [51][250/408]	Loss 0.0041 (0.0300)	
training:	Epoch: [51][251/408]	Loss 0.0068 (0.0299)	
training:	Epoch: [51][252/408]	Loss 0.0105 (0.0298)	
training:	Epoch: [51][253/408]	Loss 0.0061 (0.0298)	
training:	Epoch: [51][254/408]	Loss 0.0031 (0.0296)	
training:	Epoch: [51][255/408]	Loss 0.0097 (0.0296)	
training:	Epoch: [51][256/408]	Loss 0.0047 (0.0295)	
training:	Epoch: [51][257/408]	Loss 0.0960 (0.0297)	
training:	Epoch: [51][258/408]	Loss 0.0026 (0.0296)	
training:	Epoch: [51][259/408]	Loss 0.0030 (0.0295)	
training:	Epoch: [51][260/408]	Loss 0.0074 (0.0294)	
training:	Epoch: [51][261/408]	Loss 0.0056 (0.0293)	
training:	Epoch: [51][262/408]	Loss 0.0065 (0.0293)	
training:	Epoch: [51][263/408]	Loss 0.0021 (0.0292)	
training:	Epoch: [51][264/408]	Loss 0.0154 (0.0291)	
training:	Epoch: [51][265/408]	Loss 0.0038 (0.0290)	
training:	Epoch: [51][266/408]	Loss 0.0113 (0.0289)	
training:	Epoch: [51][267/408]	Loss 0.0046 (0.0288)	
training:	Epoch: [51][268/408]	Loss 0.0043 (0.0288)	
training:	Epoch: [51][269/408]	Loss 0.0063 (0.0287)	
training:	Epoch: [51][270/408]	Loss 0.0034 (0.0286)	
training:	Epoch: [51][271/408]	Loss 0.0085 (0.0285)	
training:	Epoch: [51][272/408]	Loss 0.1464 (0.0289)	
training:	Epoch: [51][273/408]	Loss 0.0078 (0.0289)	
training:	Epoch: [51][274/408]	Loss 0.0049 (0.0288)	
training:	Epoch: [51][275/408]	Loss 0.0035 (0.0287)	
training:	Epoch: [51][276/408]	Loss 0.1859 (0.0293)	
training:	Epoch: [51][277/408]	Loss 0.0036 (0.0292)	
training:	Epoch: [51][278/408]	Loss 0.0060 (0.0291)	
training:	Epoch: [51][279/408]	Loss 0.0078 (0.0290)	
training:	Epoch: [51][280/408]	Loss 0.0048 (0.0289)	
training:	Epoch: [51][281/408]	Loss 0.2537 (0.0297)	
training:	Epoch: [51][282/408]	Loss 0.0041 (0.0296)	
training:	Epoch: [51][283/408]	Loss 0.0039 (0.0295)	
training:	Epoch: [51][284/408]	Loss 0.0103 (0.0295)	
training:	Epoch: [51][285/408]	Loss 0.0031 (0.0294)	
training:	Epoch: [51][286/408]	Loss 0.0028 (0.0293)	
training:	Epoch: [51][287/408]	Loss 0.0033 (0.0292)	
training:	Epoch: [51][288/408]	Loss 0.0044 (0.0291)	
training:	Epoch: [51][289/408]	Loss 0.0035 (0.0290)	
training:	Epoch: [51][290/408]	Loss 0.0056 (0.0289)	
training:	Epoch: [51][291/408]	Loss 0.0081 (0.0289)	
training:	Epoch: [51][292/408]	Loss 0.0086 (0.0288)	
training:	Epoch: [51][293/408]	Loss 0.0038 (0.0287)	
training:	Epoch: [51][294/408]	Loss 0.0032 (0.0286)	
training:	Epoch: [51][295/408]	Loss 0.0035 (0.0285)	
training:	Epoch: [51][296/408]	Loss 0.0035 (0.0285)	
training:	Epoch: [51][297/408]	Loss 0.0029 (0.0284)	
training:	Epoch: [51][298/408]	Loss 0.1456 (0.0288)	
training:	Epoch: [51][299/408]	Loss 0.0023 (0.0287)	
training:	Epoch: [51][300/408]	Loss 0.0051 (0.0286)	
training:	Epoch: [51][301/408]	Loss 0.0043 (0.0285)	
training:	Epoch: [51][302/408]	Loss 0.0043 (0.0284)	
training:	Epoch: [51][303/408]	Loss 0.0025 (0.0283)	
training:	Epoch: [51][304/408]	Loss 0.0173 (0.0283)	
training:	Epoch: [51][305/408]	Loss 0.0077 (0.0282)	
training:	Epoch: [51][306/408]	Loss 0.0038 (0.0282)	
training:	Epoch: [51][307/408]	Loss 0.0081 (0.0281)	
training:	Epoch: [51][308/408]	Loss 0.0036 (0.0280)	
training:	Epoch: [51][309/408]	Loss 0.0055 (0.0279)	
training:	Epoch: [51][310/408]	Loss 0.0026 (0.0279)	
training:	Epoch: [51][311/408]	Loss 0.0024 (0.0278)	
training:	Epoch: [51][312/408]	Loss 0.0053 (0.0277)	
training:	Epoch: [51][313/408]	Loss 0.1738 (0.0282)	
training:	Epoch: [51][314/408]	Loss 0.0204 (0.0281)	
training:	Epoch: [51][315/408]	Loss 0.0042 (0.0281)	
training:	Epoch: [51][316/408]	Loss 0.0038 (0.0280)	
training:	Epoch: [51][317/408]	Loss 0.0094 (0.0279)	
training:	Epoch: [51][318/408]	Loss 0.0095 (0.0279)	
training:	Epoch: [51][319/408]	Loss 0.0055 (0.0278)	
training:	Epoch: [51][320/408]	Loss 0.0025 (0.0277)	
training:	Epoch: [51][321/408]	Loss 0.0052 (0.0277)	
training:	Epoch: [51][322/408]	Loss 0.0258 (0.0277)	
training:	Epoch: [51][323/408]	Loss 0.0059 (0.0276)	
training:	Epoch: [51][324/408]	Loss 0.0026 (0.0275)	
training:	Epoch: [51][325/408]	Loss 0.0045 (0.0274)	
training:	Epoch: [51][326/408]	Loss 0.0081 (0.0274)	
training:	Epoch: [51][327/408]	Loss 0.0028 (0.0273)	
training:	Epoch: [51][328/408]	Loss 0.2191 (0.0279)	
training:	Epoch: [51][329/408]	Loss 0.0091 (0.0278)	
training:	Epoch: [51][330/408]	Loss 0.0091 (0.0278)	
training:	Epoch: [51][331/408]	Loss 0.0047 (0.0277)	
training:	Epoch: [51][332/408]	Loss 0.0112 (0.0277)	
training:	Epoch: [51][333/408]	Loss 0.0045 (0.0276)	
training:	Epoch: [51][334/408]	Loss 0.0028 (0.0275)	
training:	Epoch: [51][335/408]	Loss 0.0038 (0.0274)	
training:	Epoch: [51][336/408]	Loss 0.0067 (0.0274)	
training:	Epoch: [51][337/408]	Loss 0.0068 (0.0273)	
training:	Epoch: [51][338/408]	Loss 0.0053 (0.0273)	
training:	Epoch: [51][339/408]	Loss 0.0908 (0.0274)	
training:	Epoch: [51][340/408]	Loss 0.0033 (0.0274)	
training:	Epoch: [51][341/408]	Loss 0.0107 (0.0273)	
training:	Epoch: [51][342/408]	Loss 0.0032 (0.0272)	
training:	Epoch: [51][343/408]	Loss 0.0030 (0.0272)	
training:	Epoch: [51][344/408]	Loss 0.0063 (0.0271)	
training:	Epoch: [51][345/408]	Loss 0.0046 (0.0271)	
training:	Epoch: [51][346/408]	Loss 0.0048 (0.0270)	
training:	Epoch: [51][347/408]	Loss 0.0035 (0.0269)	
training:	Epoch: [51][348/408]	Loss 0.0139 (0.0269)	
training:	Epoch: [51][349/408]	Loss 0.0033 (0.0268)	
training:	Epoch: [51][350/408]	Loss 0.0041 (0.0268)	
training:	Epoch: [51][351/408]	Loss 0.0028 (0.0267)	
training:	Epoch: [51][352/408]	Loss 0.0092 (0.0266)	
training:	Epoch: [51][353/408]	Loss 0.0025 (0.0266)	
training:	Epoch: [51][354/408]	Loss 0.0049 (0.0265)	
training:	Epoch: [51][355/408]	Loss 0.0032 (0.0264)	
training:	Epoch: [51][356/408]	Loss 0.0036 (0.0264)	
training:	Epoch: [51][357/408]	Loss 0.0040 (0.0263)	
training:	Epoch: [51][358/408]	Loss 0.0071 (0.0263)	
training:	Epoch: [51][359/408]	Loss 0.0045 (0.0262)	
training:	Epoch: [51][360/408]	Loss 0.2482 (0.0268)	
training:	Epoch: [51][361/408]	Loss 0.0092 (0.0268)	
training:	Epoch: [51][362/408]	Loss 0.0816 (0.0269)	
training:	Epoch: [51][363/408]	Loss 0.0069 (0.0269)	
training:	Epoch: [51][364/408]	Loss 0.0036 (0.0268)	
training:	Epoch: [51][365/408]	Loss 0.0048 (0.0267)	
training:	Epoch: [51][366/408]	Loss 0.0085 (0.0267)	
training:	Epoch: [51][367/408]	Loss 0.0028 (0.0266)	
training:	Epoch: [51][368/408]	Loss 0.0035 (0.0266)	
training:	Epoch: [51][369/408]	Loss 0.0126 (0.0265)	
training:	Epoch: [51][370/408]	Loss 0.0030 (0.0265)	
training:	Epoch: [51][371/408]	Loss 0.0064 (0.0264)	
training:	Epoch: [51][372/408]	Loss 0.0028 (0.0263)	
training:	Epoch: [51][373/408]	Loss 0.0065 (0.0263)	
training:	Epoch: [51][374/408]	Loss 0.0041 (0.0262)	
training:	Epoch: [51][375/408]	Loss 0.0036 (0.0262)	
training:	Epoch: [51][376/408]	Loss 0.0072 (0.0261)	
training:	Epoch: [51][377/408]	Loss 0.0040 (0.0261)	
training:	Epoch: [51][378/408]	Loss 0.0035 (0.0260)	
training:	Epoch: [51][379/408]	Loss 0.0034 (0.0259)	
training:	Epoch: [51][380/408]	Loss 0.0026 (0.0259)	
training:	Epoch: [51][381/408]	Loss 0.0060 (0.0258)	
training:	Epoch: [51][382/408]	Loss 0.0037 (0.0258)	
training:	Epoch: [51][383/408]	Loss 0.0059 (0.0257)	
training:	Epoch: [51][384/408]	Loss 0.0031 (0.0257)	
training:	Epoch: [51][385/408]	Loss 0.0043 (0.0256)	
training:	Epoch: [51][386/408]	Loss 0.0032 (0.0255)	
training:	Epoch: [51][387/408]	Loss 0.0034 (0.0255)	
training:	Epoch: [51][388/408]	Loss 0.0059 (0.0254)	
training:	Epoch: [51][389/408]	Loss 0.0073 (0.0254)	
training:	Epoch: [51][390/408]	Loss 0.0047 (0.0253)	
training:	Epoch: [51][391/408]	Loss 0.0041 (0.0253)	
training:	Epoch: [51][392/408]	Loss 0.0038 (0.0252)	
training:	Epoch: [51][393/408]	Loss 0.1452 (0.0255)	
training:	Epoch: [51][394/408]	Loss 0.0019 (0.0255)	
training:	Epoch: [51][395/408]	Loss 0.0062 (0.0254)	
training:	Epoch: [51][396/408]	Loss 0.0043 (0.0254)	
training:	Epoch: [51][397/408]	Loss 0.0027 (0.0253)	
training:	Epoch: [51][398/408]	Loss 0.0026 (0.0253)	
training:	Epoch: [51][399/408]	Loss 0.0028 (0.0252)	
training:	Epoch: [51][400/408]	Loss 0.0078 (0.0252)	
training:	Epoch: [51][401/408]	Loss 0.0338 (0.0252)	
training:	Epoch: [51][402/408]	Loss 0.0814 (0.0253)	
training:	Epoch: [51][403/408]	Loss 0.0073 (0.0253)	
training:	Epoch: [51][404/408]	Loss 0.0043 (0.0252)	
training:	Epoch: [51][405/408]	Loss 0.0048 (0.0252)	
training:	Epoch: [51][406/408]	Loss 0.0044 (0.0251)	
training:	Epoch: [51][407/408]	Loss 0.0022 (0.0251)	
training:	Epoch: [51][408/408]	Loss 0.0025 (0.0250)	
Training:	 Loss: 0.0250

Training:	 ACC: 0.9948 0.9948 0.9941 0.9955
Validation:	 ACC: 0.7777 0.7796 0.8178 0.7377
Validation:	 Best_BACC: 0.8226 0.8250 0.8772 0.7679
Validation:	 Loss: 1.0344
Pretraining:	Epoch 52/200
----------
training:	Epoch: [52][1/408]	Loss 0.0138 (0.0138)	
training:	Epoch: [52][2/408]	Loss 0.2528 (0.1333)	
training:	Epoch: [52][3/408]	Loss 0.0039 (0.0902)	
training:	Epoch: [52][4/408]	Loss 0.0048 (0.0688)	
training:	Epoch: [52][5/408]	Loss 0.0045 (0.0560)	
training:	Epoch: [52][6/408]	Loss 0.0031 (0.0471)	
training:	Epoch: [52][7/408]	Loss 0.0042 (0.0410)	
training:	Epoch: [52][8/408]	Loss 0.0029 (0.0363)	
training:	Epoch: [52][9/408]	Loss 0.0938 (0.0426)	
training:	Epoch: [52][10/408]	Loss 0.0043 (0.0388)	
training:	Epoch: [52][11/408]	Loss 0.0103 (0.0362)	
training:	Epoch: [52][12/408]	Loss 0.0033 (0.0335)	
training:	Epoch: [52][13/408]	Loss 0.1978 (0.0461)	
training:	Epoch: [52][14/408]	Loss 0.1272 (0.0519)	
training:	Epoch: [52][15/408]	Loss 0.1545 (0.0587)	
training:	Epoch: [52][16/408]	Loss 0.0041 (0.0553)	
training:	Epoch: [52][17/408]	Loss 0.0056 (0.0524)	
training:	Epoch: [52][18/408]	Loss 0.0082 (0.0499)	
training:	Epoch: [52][19/408]	Loss 0.0152 (0.0481)	
training:	Epoch: [52][20/408]	Loss 0.0043 (0.0459)	
training:	Epoch: [52][21/408]	Loss 0.0056 (0.0440)	
training:	Epoch: [52][22/408]	Loss 0.0032 (0.0422)	
training:	Epoch: [52][23/408]	Loss 0.0040 (0.0405)	
training:	Epoch: [52][24/408]	Loss 0.0095 (0.0392)	
training:	Epoch: [52][25/408]	Loss 0.0033 (0.0378)	
training:	Epoch: [52][26/408]	Loss 0.0065 (0.0366)	
training:	Epoch: [52][27/408]	Loss 0.2149 (0.0432)	
training:	Epoch: [52][28/408]	Loss 0.0056 (0.0418)	
training:	Epoch: [52][29/408]	Loss 0.0031 (0.0405)	
training:	Epoch: [52][30/408]	Loss 0.0046 (0.0393)	
training:	Epoch: [52][31/408]	Loss 0.0068 (0.0382)	
training:	Epoch: [52][32/408]	Loss 0.0048 (0.0372)	
training:	Epoch: [52][33/408]	Loss 0.0053 (0.0362)	
training:	Epoch: [52][34/408]	Loss 0.0049 (0.0353)	
training:	Epoch: [52][35/408]	Loss 0.0073 (0.0345)	
training:	Epoch: [52][36/408]	Loss 0.0035 (0.0336)	
training:	Epoch: [52][37/408]	Loss 0.0041 (0.0329)	
training:	Epoch: [52][38/408]	Loss 0.0020 (0.0320)	
training:	Epoch: [52][39/408]	Loss 0.0042 (0.0313)	
training:	Epoch: [52][40/408]	Loss 0.0103 (0.0308)	
training:	Epoch: [52][41/408]	Loss 0.0039 (0.0301)	
training:	Epoch: [52][42/408]	Loss 0.0052 (0.0295)	
training:	Epoch: [52][43/408]	Loss 0.0098 (0.0291)	
training:	Epoch: [52][44/408]	Loss 0.0030 (0.0285)	
training:	Epoch: [52][45/408]	Loss 0.0040 (0.0280)	
training:	Epoch: [52][46/408]	Loss 0.0580 (0.0286)	
training:	Epoch: [52][47/408]	Loss 0.0091 (0.0282)	
training:	Epoch: [52][48/408]	Loss 0.0039 (0.0277)	
training:	Epoch: [52][49/408]	Loss 0.0076 (0.0273)	
training:	Epoch: [52][50/408]	Loss 0.0083 (0.0269)	
training:	Epoch: [52][51/408]	Loss 0.2204 (0.0307)	
training:	Epoch: [52][52/408]	Loss 0.0142 (0.0304)	
training:	Epoch: [52][53/408]	Loss 0.0442 (0.0306)	
training:	Epoch: [52][54/408]	Loss 0.0046 (0.0302)	
training:	Epoch: [52][55/408]	Loss 0.0060 (0.0297)	
training:	Epoch: [52][56/408]	Loss 0.0029 (0.0292)	
training:	Epoch: [52][57/408]	Loss 0.0037 (0.0288)	
training:	Epoch: [52][58/408]	Loss 0.0061 (0.0284)	
training:	Epoch: [52][59/408]	Loss 0.0072 (0.0280)	
training:	Epoch: [52][60/408]	Loss 0.0044 (0.0276)	
training:	Epoch: [52][61/408]	Loss 0.0043 (0.0273)	
training:	Epoch: [52][62/408]	Loss 0.0041 (0.0269)	
training:	Epoch: [52][63/408]	Loss 0.0032 (0.0265)	
training:	Epoch: [52][64/408]	Loss 0.0037 (0.0262)	
training:	Epoch: [52][65/408]	Loss 0.0030 (0.0258)	
training:	Epoch: [52][66/408]	Loss 0.0161 (0.0256)	
training:	Epoch: [52][67/408]	Loss 0.0079 (0.0254)	
training:	Epoch: [52][68/408]	Loss 0.0033 (0.0251)	
training:	Epoch: [52][69/408]	Loss 0.0106 (0.0248)	
training:	Epoch: [52][70/408]	Loss 0.0066 (0.0246)	
training:	Epoch: [52][71/408]	Loss 0.0094 (0.0244)	
training:	Epoch: [52][72/408]	Loss 0.0051 (0.0241)	
training:	Epoch: [52][73/408]	Loss 0.0034 (0.0238)	
training:	Epoch: [52][74/408]	Loss 0.0052 (0.0236)	
training:	Epoch: [52][75/408]	Loss 0.0130 (0.0234)	
training:	Epoch: [52][76/408]	Loss 0.0076 (0.0232)	
training:	Epoch: [52][77/408]	Loss 0.0051 (0.0230)	
training:	Epoch: [52][78/408]	Loss 0.0043 (0.0227)	
training:	Epoch: [52][79/408]	Loss 0.0027 (0.0225)	
training:	Epoch: [52][80/408]	Loss 0.0030 (0.0223)	
training:	Epoch: [52][81/408]	Loss 0.0159 (0.0222)	
training:	Epoch: [52][82/408]	Loss 0.0048 (0.0220)	
training:	Epoch: [52][83/408]	Loss 0.0037 (0.0217)	
training:	Epoch: [52][84/408]	Loss 0.0032 (0.0215)	
training:	Epoch: [52][85/408]	Loss 0.0063 (0.0213)	
training:	Epoch: [52][86/408]	Loss 0.0053 (0.0212)	
training:	Epoch: [52][87/408]	Loss 0.0061 (0.0210)	
training:	Epoch: [52][88/408]	Loss 0.0024 (0.0208)	
training:	Epoch: [52][89/408]	Loss 0.0044 (0.0206)	
training:	Epoch: [52][90/408]	Loss 0.0673 (0.0211)	
training:	Epoch: [52][91/408]	Loss 0.0064 (0.0209)	
training:	Epoch: [52][92/408]	Loss 0.0119 (0.0208)	
training:	Epoch: [52][93/408]	Loss 0.0024 (0.0206)	
training:	Epoch: [52][94/408]	Loss 0.0063 (0.0205)	
training:	Epoch: [52][95/408]	Loss 0.0999 (0.0213)	
training:	Epoch: [52][96/408]	Loss 0.0058 (0.0212)	
training:	Epoch: [52][97/408]	Loss 0.0106 (0.0211)	
training:	Epoch: [52][98/408]	Loss 0.0038 (0.0209)	
training:	Epoch: [52][99/408]	Loss 0.0071 (0.0207)	
training:	Epoch: [52][100/408]	Loss 0.0054 (0.0206)	
training:	Epoch: [52][101/408]	Loss 0.0051 (0.0204)	
training:	Epoch: [52][102/408]	Loss 0.0037 (0.0203)	
training:	Epoch: [52][103/408]	Loss 0.0667 (0.0207)	
training:	Epoch: [52][104/408]	Loss 0.0093 (0.0206)	
training:	Epoch: [52][105/408]	Loss 0.0036 (0.0205)	
training:	Epoch: [52][106/408]	Loss 0.0038 (0.0203)	
training:	Epoch: [52][107/408]	Loss 0.0040 (0.0201)	
training:	Epoch: [52][108/408]	Loss 0.1566 (0.0214)	
training:	Epoch: [52][109/408]	Loss 0.0212 (0.0214)	
training:	Epoch: [52][110/408]	Loss 0.0055 (0.0213)	
training:	Epoch: [52][111/408]	Loss 0.0071 (0.0211)	
training:	Epoch: [52][112/408]	Loss 0.0038 (0.0210)	
training:	Epoch: [52][113/408]	Loss 0.1361 (0.0220)	
training:	Epoch: [52][114/408]	Loss 0.0095 (0.0219)	
training:	Epoch: [52][115/408]	Loss 0.0028 (0.0217)	
training:	Epoch: [52][116/408]	Loss 0.0027 (0.0216)	
training:	Epoch: [52][117/408]	Loss 0.0075 (0.0214)	
training:	Epoch: [52][118/408]	Loss 0.0372 (0.0216)	
training:	Epoch: [52][119/408]	Loss 0.0052 (0.0214)	
training:	Epoch: [52][120/408]	Loss 0.0050 (0.0213)	
training:	Epoch: [52][121/408]	Loss 0.0026 (0.0211)	
training:	Epoch: [52][122/408]	Loss 0.2491 (0.0230)	
training:	Epoch: [52][123/408]	Loss 0.0032 (0.0228)	
training:	Epoch: [52][124/408]	Loss 0.0053 (0.0227)	
training:	Epoch: [52][125/408]	Loss 0.0025 (0.0225)	
training:	Epoch: [52][126/408]	Loss 0.0047 (0.0224)	
training:	Epoch: [52][127/408]	Loss 0.0051 (0.0223)	
training:	Epoch: [52][128/408]	Loss 0.0049 (0.0221)	
training:	Epoch: [52][129/408]	Loss 0.0064 (0.0220)	
training:	Epoch: [52][130/408]	Loss 0.0037 (0.0219)	
training:	Epoch: [52][131/408]	Loss 0.0018 (0.0217)	
training:	Epoch: [52][132/408]	Loss 0.0052 (0.0216)	
training:	Epoch: [52][133/408]	Loss 0.0058 (0.0215)	
training:	Epoch: [52][134/408]	Loss 0.0030 (0.0213)	
training:	Epoch: [52][135/408]	Loss 0.0068 (0.0212)	
training:	Epoch: [52][136/408]	Loss 0.0028 (0.0211)	
training:	Epoch: [52][137/408]	Loss 0.0053 (0.0210)	
training:	Epoch: [52][138/408]	Loss 0.0046 (0.0209)	
training:	Epoch: [52][139/408]	Loss 0.0024 (0.0207)	
training:	Epoch: [52][140/408]	Loss 0.0030 (0.0206)	
training:	Epoch: [52][141/408]	Loss 0.0117 (0.0205)	
training:	Epoch: [52][142/408]	Loss 0.0040 (0.0204)	
training:	Epoch: [52][143/408]	Loss 0.0061 (0.0203)	
training:	Epoch: [52][144/408]	Loss 0.0101 (0.0202)	
training:	Epoch: [52][145/408]	Loss 0.0033 (0.0201)	
training:	Epoch: [52][146/408]	Loss 0.0045 (0.0200)	
training:	Epoch: [52][147/408]	Loss 0.0079 (0.0199)	
training:	Epoch: [52][148/408]	Loss 0.0085 (0.0199)	
training:	Epoch: [52][149/408]	Loss 0.0032 (0.0198)	
training:	Epoch: [52][150/408]	Loss 0.0045 (0.0197)	
training:	Epoch: [52][151/408]	Loss 0.0022 (0.0195)	
training:	Epoch: [52][152/408]	Loss 0.0033 (0.0194)	
training:	Epoch: [52][153/408]	Loss 0.0048 (0.0193)	
training:	Epoch: [52][154/408]	Loss 0.0058 (0.0192)	
training:	Epoch: [52][155/408]	Loss 0.0050 (0.0192)	
training:	Epoch: [52][156/408]	Loss 0.1129 (0.0198)	
training:	Epoch: [52][157/408]	Loss 0.0056 (0.0197)	
training:	Epoch: [52][158/408]	Loss 0.0065 (0.0196)	
training:	Epoch: [52][159/408]	Loss 0.0035 (0.0195)	
training:	Epoch: [52][160/408]	Loss 0.0708 (0.0198)	
training:	Epoch: [52][161/408]	Loss 0.0035 (0.0197)	
training:	Epoch: [52][162/408]	Loss 0.0029 (0.0196)	
training:	Epoch: [52][163/408]	Loss 0.0022 (0.0195)	
training:	Epoch: [52][164/408]	Loss 0.0070 (0.0194)	
training:	Epoch: [52][165/408]	Loss 0.0047 (0.0193)	
training:	Epoch: [52][166/408]	Loss 0.0065 (0.0192)	
training:	Epoch: [52][167/408]	Loss 0.0046 (0.0192)	
training:	Epoch: [52][168/408]	Loss 0.0061 (0.0191)	
training:	Epoch: [52][169/408]	Loss 0.0019 (0.0190)	
training:	Epoch: [52][170/408]	Loss 0.0049 (0.0189)	
training:	Epoch: [52][171/408]	Loss 0.2001 (0.0200)	
training:	Epoch: [52][172/408]	Loss 0.0589 (0.0202)	
training:	Epoch: [52][173/408]	Loss 0.0048 (0.0201)	
training:	Epoch: [52][174/408]	Loss 0.0059 (0.0200)	
training:	Epoch: [52][175/408]	Loss 0.0047 (0.0199)	
training:	Epoch: [52][176/408]	Loss 0.0398 (0.0200)	
training:	Epoch: [52][177/408]	Loss 0.0088 (0.0200)	
training:	Epoch: [52][178/408]	Loss 0.0037 (0.0199)	
training:	Epoch: [52][179/408]	Loss 0.0044 (0.0198)	
training:	Epoch: [52][180/408]	Loss 0.0039 (0.0197)	
training:	Epoch: [52][181/408]	Loss 0.0076 (0.0196)	
training:	Epoch: [52][182/408]	Loss 0.0050 (0.0196)	
training:	Epoch: [52][183/408]	Loss 0.0499 (0.0197)	
training:	Epoch: [52][184/408]	Loss 0.0028 (0.0196)	
training:	Epoch: [52][185/408]	Loss 0.0024 (0.0195)	
training:	Epoch: [52][186/408]	Loss 0.0028 (0.0194)	
training:	Epoch: [52][187/408]	Loss 0.0073 (0.0194)	
training:	Epoch: [52][188/408]	Loss 0.0038 (0.0193)	
training:	Epoch: [52][189/408]	Loss 0.0053 (0.0192)	
training:	Epoch: [52][190/408]	Loss 0.0025 (0.0191)	
training:	Epoch: [52][191/408]	Loss 0.1391 (0.0198)	
training:	Epoch: [52][192/408]	Loss 0.0121 (0.0197)	
training:	Epoch: [52][193/408]	Loss 0.0039 (0.0196)	
training:	Epoch: [52][194/408]	Loss 0.0617 (0.0199)	
training:	Epoch: [52][195/408]	Loss 0.0045 (0.0198)	
training:	Epoch: [52][196/408]	Loss 0.0150 (0.0198)	
training:	Epoch: [52][197/408]	Loss 0.0065 (0.0197)	
training:	Epoch: [52][198/408]	Loss 0.0057 (0.0196)	
training:	Epoch: [52][199/408]	Loss 0.0074 (0.0196)	
training:	Epoch: [52][200/408]	Loss 0.0038 (0.0195)	
training:	Epoch: [52][201/408]	Loss 0.0167 (0.0195)	
training:	Epoch: [52][202/408]	Loss 0.0054 (0.0194)	
training:	Epoch: [52][203/408]	Loss 0.0043 (0.0193)	
training:	Epoch: [52][204/408]	Loss 0.0082 (0.0193)	
training:	Epoch: [52][205/408]	Loss 0.0049 (0.0192)	
training:	Epoch: [52][206/408]	Loss 0.0031 (0.0191)	
training:	Epoch: [52][207/408]	Loss 0.0042 (0.0190)	
training:	Epoch: [52][208/408]	Loss 0.0041 (0.0190)	
training:	Epoch: [52][209/408]	Loss 0.0027 (0.0189)	
training:	Epoch: [52][210/408]	Loss 0.0047 (0.0188)	
training:	Epoch: [52][211/408]	Loss 0.0103 (0.0188)	
training:	Epoch: [52][212/408]	Loss 0.0059 (0.0187)	
training:	Epoch: [52][213/408]	Loss 0.0032 (0.0187)	
training:	Epoch: [52][214/408]	Loss 0.0037 (0.0186)	
training:	Epoch: [52][215/408]	Loss 0.0856 (0.0189)	
training:	Epoch: [52][216/408]	Loss 0.0030 (0.0188)	
training:	Epoch: [52][217/408]	Loss 0.0050 (0.0188)	
training:	Epoch: [52][218/408]	Loss 0.0033 (0.0187)	
training:	Epoch: [52][219/408]	Loss 0.0037 (0.0186)	
training:	Epoch: [52][220/408]	Loss 0.1273 (0.0191)	
training:	Epoch: [52][221/408]	Loss 0.0043 (0.0190)	
training:	Epoch: [52][222/408]	Loss 0.0064 (0.0190)	
training:	Epoch: [52][223/408]	Loss 0.0076 (0.0189)	
training:	Epoch: [52][224/408]	Loss 0.0028 (0.0189)	
training:	Epoch: [52][225/408]	Loss 0.0124 (0.0188)	
training:	Epoch: [52][226/408]	Loss 0.0045 (0.0188)	
training:	Epoch: [52][227/408]	Loss 0.0149 (0.0188)	
training:	Epoch: [52][228/408]	Loss 0.0045 (0.0187)	
training:	Epoch: [52][229/408]	Loss 0.0049 (0.0186)	
training:	Epoch: [52][230/408]	Loss 0.0038 (0.0186)	
training:	Epoch: [52][231/408]	Loss 0.0028 (0.0185)	
training:	Epoch: [52][232/408]	Loss 0.0046 (0.0184)	
training:	Epoch: [52][233/408]	Loss 0.0028 (0.0184)	
training:	Epoch: [52][234/408]	Loss 0.0116 (0.0183)	
training:	Epoch: [52][235/408]	Loss 0.0024 (0.0183)	
training:	Epoch: [52][236/408]	Loss 0.0032 (0.0182)	
training:	Epoch: [52][237/408]	Loss 0.0026 (0.0181)	
training:	Epoch: [52][238/408]	Loss 0.0054 (0.0181)	
training:	Epoch: [52][239/408]	Loss 0.0173 (0.0181)	
training:	Epoch: [52][240/408]	Loss 0.0043 (0.0180)	
training:	Epoch: [52][241/408]	Loss 0.0029 (0.0180)	
training:	Epoch: [52][242/408]	Loss 0.0077 (0.0179)	
training:	Epoch: [52][243/408]	Loss 0.0023 (0.0179)	
training:	Epoch: [52][244/408]	Loss 0.0035 (0.0178)	
training:	Epoch: [52][245/408]	Loss 0.0039 (0.0178)	
training:	Epoch: [52][246/408]	Loss 0.0033 (0.0177)	
training:	Epoch: [52][247/408]	Loss 0.0024 (0.0176)	
training:	Epoch: [52][248/408]	Loss 0.0040 (0.0176)	
training:	Epoch: [52][249/408]	Loss 0.0024 (0.0175)	
training:	Epoch: [52][250/408]	Loss 0.0035 (0.0175)	
training:	Epoch: [52][251/408]	Loss 0.0031 (0.0174)	
training:	Epoch: [52][252/408]	Loss 0.0061 (0.0174)	
training:	Epoch: [52][253/408]	Loss 0.0062 (0.0173)	
training:	Epoch: [52][254/408]	Loss 0.0021 (0.0173)	
training:	Epoch: [52][255/408]	Loss 0.0021 (0.0172)	
training:	Epoch: [52][256/408]	Loss 0.0053 (0.0171)	
training:	Epoch: [52][257/408]	Loss 0.0028 (0.0171)	
training:	Epoch: [52][258/408]	Loss 0.0027 (0.0170)	
training:	Epoch: [52][259/408]	Loss 0.0022 (0.0170)	
training:	Epoch: [52][260/408]	Loss 0.0022 (0.0169)	
training:	Epoch: [52][261/408]	Loss 0.0087 (0.0169)	
training:	Epoch: [52][262/408]	Loss 0.0055 (0.0168)	
training:	Epoch: [52][263/408]	Loss 0.0024 (0.0168)	
training:	Epoch: [52][264/408]	Loss 0.0096 (0.0168)	
training:	Epoch: [52][265/408]	Loss 0.0033 (0.0167)	
training:	Epoch: [52][266/408]	Loss 0.0043 (0.0167)	
training:	Epoch: [52][267/408]	Loss 0.0028 (0.0166)	
training:	Epoch: [52][268/408]	Loss 0.0019 (0.0166)	
training:	Epoch: [52][269/408]	Loss 0.0053 (0.0165)	
training:	Epoch: [52][270/408]	Loss 0.0030 (0.0165)	
training:	Epoch: [52][271/408]	Loss 0.0040 (0.0164)	
training:	Epoch: [52][272/408]	Loss 0.0030 (0.0164)	
training:	Epoch: [52][273/408]	Loss 0.0022 (0.0163)	
training:	Epoch: [52][274/408]	Loss 0.0047 (0.0163)	
training:	Epoch: [52][275/408]	Loss 0.0045 (0.0162)	
training:	Epoch: [52][276/408]	Loss 0.0064 (0.0162)	
training:	Epoch: [52][277/408]	Loss 0.0035 (0.0162)	
training:	Epoch: [52][278/408]	Loss 0.0039 (0.0161)	
training:	Epoch: [52][279/408]	Loss 0.0035 (0.0161)	
training:	Epoch: [52][280/408]	Loss 0.1669 (0.0166)	
training:	Epoch: [52][281/408]	Loss 0.0158 (0.0166)	
training:	Epoch: [52][282/408]	Loss 0.0041 (0.0166)	
training:	Epoch: [52][283/408]	Loss 0.0032 (0.0165)	
training:	Epoch: [52][284/408]	Loss 0.0043 (0.0165)	
training:	Epoch: [52][285/408]	Loss 0.0036 (0.0164)	
training:	Epoch: [52][286/408]	Loss 0.0032 (0.0164)	
training:	Epoch: [52][287/408]	Loss 0.0030 (0.0163)	
training:	Epoch: [52][288/408]	Loss 0.0046 (0.0163)	
training:	Epoch: [52][289/408]	Loss 0.0026 (0.0162)	
training:	Epoch: [52][290/408]	Loss 0.1179 (0.0166)	
training:	Epoch: [52][291/408]	Loss 0.0024 (0.0165)	
training:	Epoch: [52][292/408]	Loss 0.0035 (0.0165)	
training:	Epoch: [52][293/408]	Loss 0.0033 (0.0165)	
training:	Epoch: [52][294/408]	Loss 0.0027 (0.0164)	
training:	Epoch: [52][295/408]	Loss 0.0085 (0.0164)	
training:	Epoch: [52][296/408]	Loss 0.0049 (0.0163)	
training:	Epoch: [52][297/408]	Loss 0.0054 (0.0163)	
training:	Epoch: [52][298/408]	Loss 0.0049 (0.0163)	
training:	Epoch: [52][299/408]	Loss 0.0048 (0.0162)	
training:	Epoch: [52][300/408]	Loss 0.1028 (0.0165)	
training:	Epoch: [52][301/408]	Loss 0.0028 (0.0165)	
training:	Epoch: [52][302/408]	Loss 0.0027 (0.0164)	
training:	Epoch: [52][303/408]	Loss 0.1691 (0.0169)	
training:	Epoch: [52][304/408]	Loss 0.0035 (0.0169)	
training:	Epoch: [52][305/408]	Loss 0.0027 (0.0168)	
training:	Epoch: [52][306/408]	Loss 0.0183 (0.0168)	
training:	Epoch: [52][307/408]	Loss 0.0160 (0.0168)	
training:	Epoch: [52][308/408]	Loss 0.1940 (0.0174)	
training:	Epoch: [52][309/408]	Loss 0.0035 (0.0174)	
training:	Epoch: [52][310/408]	Loss 0.0395 (0.0174)	
training:	Epoch: [52][311/408]	Loss 0.0028 (0.0174)	
training:	Epoch: [52][312/408]	Loss 0.0033 (0.0173)	
training:	Epoch: [52][313/408]	Loss 0.0057 (0.0173)	
training:	Epoch: [52][314/408]	Loss 0.0032 (0.0173)	
training:	Epoch: [52][315/408]	Loss 0.0051 (0.0172)	
training:	Epoch: [52][316/408]	Loss 0.0063 (0.0172)	
training:	Epoch: [52][317/408]	Loss 0.0033 (0.0171)	
training:	Epoch: [52][318/408]	Loss 0.0063 (0.0171)	
training:	Epoch: [52][319/408]	Loss 0.0045 (0.0171)	
training:	Epoch: [52][320/408]	Loss 0.0034 (0.0170)	
training:	Epoch: [52][321/408]	Loss 0.1751 (0.0175)	
training:	Epoch: [52][322/408]	Loss 0.0023 (0.0175)	
training:	Epoch: [52][323/408]	Loss 0.0128 (0.0175)	
training:	Epoch: [52][324/408]	Loss 0.0032 (0.0174)	
training:	Epoch: [52][325/408]	Loss 0.0081 (0.0174)	
training:	Epoch: [52][326/408]	Loss 0.0030 (0.0173)	
training:	Epoch: [52][327/408]	Loss 0.0115 (0.0173)	
training:	Epoch: [52][328/408]	Loss 0.0028 (0.0173)	
training:	Epoch: [52][329/408]	Loss 0.0030 (0.0172)	
training:	Epoch: [52][330/408]	Loss 0.0048 (0.0172)	
training:	Epoch: [52][331/408]	Loss 0.0027 (0.0172)	
training:	Epoch: [52][332/408]	Loss 0.0066 (0.0171)	
training:	Epoch: [52][333/408]	Loss 0.0040 (0.0171)	
training:	Epoch: [52][334/408]	Loss 0.0073 (0.0171)	
training:	Epoch: [52][335/408]	Loss 0.0040 (0.0170)	
training:	Epoch: [52][336/408]	Loss 0.0044 (0.0170)	
training:	Epoch: [52][337/408]	Loss 0.0074 (0.0170)	
training:	Epoch: [52][338/408]	Loss 0.0046 (0.0169)	
training:	Epoch: [52][339/408]	Loss 0.0053 (0.0169)	
training:	Epoch: [52][340/408]	Loss 0.0052 (0.0168)	
training:	Epoch: [52][341/408]	Loss 0.0038 (0.0168)	
training:	Epoch: [52][342/408]	Loss 0.0029 (0.0168)	
training:	Epoch: [52][343/408]	Loss 0.0094 (0.0167)	
training:	Epoch: [52][344/408]	Loss 0.0023 (0.0167)	
training:	Epoch: [52][345/408]	Loss 0.0026 (0.0167)	
training:	Epoch: [52][346/408]	Loss 0.0039 (0.0166)	
training:	Epoch: [52][347/408]	Loss 0.0053 (0.0166)	
training:	Epoch: [52][348/408]	Loss 0.0091 (0.0166)	
training:	Epoch: [52][349/408]	Loss 0.0550 (0.0167)	
training:	Epoch: [52][350/408]	Loss 0.0027 (0.0166)	
training:	Epoch: [52][351/408]	Loss 0.0070 (0.0166)	
training:	Epoch: [52][352/408]	Loss 0.0101 (0.0166)	
training:	Epoch: [52][353/408]	Loss 0.0039 (0.0166)	
training:	Epoch: [52][354/408]	Loss 0.0045 (0.0165)	
training:	Epoch: [52][355/408]	Loss 0.0027 (0.0165)	
training:	Epoch: [52][356/408]	Loss 0.0032 (0.0165)	
training:	Epoch: [52][357/408]	Loss 0.0025 (0.0164)	
training:	Epoch: [52][358/408]	Loss 0.0070 (0.0164)	
training:	Epoch: [52][359/408]	Loss 0.0049 (0.0164)	
training:	Epoch: [52][360/408]	Loss 0.0040 (0.0163)	
training:	Epoch: [52][361/408]	Loss 0.2118 (0.0169)	
training:	Epoch: [52][362/408]	Loss 0.0030 (0.0168)	
training:	Epoch: [52][363/408]	Loss 0.0027 (0.0168)	
training:	Epoch: [52][364/408]	Loss 0.0026 (0.0167)	
training:	Epoch: [52][365/408]	Loss 0.0067 (0.0167)	
training:	Epoch: [52][366/408]	Loss 0.0045 (0.0167)	
training:	Epoch: [52][367/408]	Loss 0.0158 (0.0167)	
training:	Epoch: [52][368/408]	Loss 0.0048 (0.0166)	
training:	Epoch: [52][369/408]	Loss 0.0128 (0.0166)	
training:	Epoch: [52][370/408]	Loss 0.0020 (0.0166)	
training:	Epoch: [52][371/408]	Loss 0.0054 (0.0166)	
training:	Epoch: [52][372/408]	Loss 0.0066 (0.0165)	
training:	Epoch: [52][373/408]	Loss 0.0029 (0.0165)	
training:	Epoch: [52][374/408]	Loss 0.0026 (0.0165)	
training:	Epoch: [52][375/408]	Loss 0.0019 (0.0164)	
training:	Epoch: [52][376/408]	Loss 0.0025 (0.0164)	
training:	Epoch: [52][377/408]	Loss 0.0052 (0.0164)	
training:	Epoch: [52][378/408]	Loss 0.0057 (0.0163)	
training:	Epoch: [52][379/408]	Loss 0.0083 (0.0163)	
training:	Epoch: [52][380/408]	Loss 0.1271 (0.0166)	
training:	Epoch: [52][381/408]	Loss 0.0023 (0.0166)	
training:	Epoch: [52][382/408]	Loss 0.1042 (0.0168)	
training:	Epoch: [52][383/408]	Loss 0.0027 (0.0168)	
training:	Epoch: [52][384/408]	Loss 0.0026 (0.0167)	
training:	Epoch: [52][385/408]	Loss 0.0040 (0.0167)	
training:	Epoch: [52][386/408]	Loss 0.0025 (0.0167)	
training:	Epoch: [52][387/408]	Loss 0.0029 (0.0166)	
training:	Epoch: [52][388/408]	Loss 0.0046 (0.0166)	
training:	Epoch: [52][389/408]	Loss 0.0095 (0.0166)	
training:	Epoch: [52][390/408]	Loss 0.0026 (0.0165)	
training:	Epoch: [52][391/408]	Loss 0.0038 (0.0165)	
training:	Epoch: [52][392/408]	Loss 0.0041 (0.0165)	
training:	Epoch: [52][393/408]	Loss 0.0057 (0.0164)	
training:	Epoch: [52][394/408]	Loss 0.0118 (0.0164)	
training:	Epoch: [52][395/408]	Loss 0.0044 (0.0164)	
training:	Epoch: [52][396/408]	Loss 0.0037 (0.0164)	
training:	Epoch: [52][397/408]	Loss 0.0519 (0.0165)	
training:	Epoch: [52][398/408]	Loss 0.0024 (0.0164)	
training:	Epoch: [52][399/408]	Loss 0.0047 (0.0164)	
training:	Epoch: [52][400/408]	Loss 0.0037 (0.0164)	
training:	Epoch: [52][401/408]	Loss 0.0142 (0.0164)	
training:	Epoch: [52][402/408]	Loss 0.0042 (0.0163)	
training:	Epoch: [52][403/408]	Loss 0.0999 (0.0165)	
training:	Epoch: [52][404/408]	Loss 0.0039 (0.0165)	
training:	Epoch: [52][405/408]	Loss 0.0042 (0.0165)	
training:	Epoch: [52][406/408]	Loss 0.0147 (0.0165)	
training:	Epoch: [52][407/408]	Loss 0.0045 (0.0164)	
training:	Epoch: [52][408/408]	Loss 0.0026 (0.0164)	
Training:	 Loss: 0.0164

Training:	 ACC: 0.9962 0.9962 0.9968 0.9955
Validation:	 ACC: 0.7746 0.7753 0.7892 0.7601
Validation:	 Best_BACC: 0.8226 0.8250 0.8772 0.7679
Validation:	 Loss: 1.0855
Pretraining:	Epoch 53/200
----------
training:	Epoch: [53][1/408]	Loss 0.0022 (0.0022)	
training:	Epoch: [53][2/408]	Loss 0.0032 (0.0027)	
training:	Epoch: [53][3/408]	Loss 0.0315 (0.0123)	
training:	Epoch: [53][4/408]	Loss 0.0044 (0.0103)	
training:	Epoch: [53][5/408]	Loss 0.0046 (0.0092)	
training:	Epoch: [53][6/408]	Loss 0.0044 (0.0084)	
training:	Epoch: [53][7/408]	Loss 0.0020 (0.0075)	
training:	Epoch: [53][8/408]	Loss 0.0054 (0.0072)	
training:	Epoch: [53][9/408]	Loss 0.0049 (0.0070)	
training:	Epoch: [53][10/408]	Loss 0.0021 (0.0065)	
training:	Epoch: [53][11/408]	Loss 0.0018 (0.0060)	
training:	Epoch: [53][12/408]	Loss 0.0056 (0.0060)	
training:	Epoch: [53][13/408]	Loss 0.0059 (0.0060)	
training:	Epoch: [53][14/408]	Loss 0.0037 (0.0058)	
training:	Epoch: [53][15/408]	Loss 0.0052 (0.0058)	
training:	Epoch: [53][16/408]	Loss 0.0035 (0.0056)	
training:	Epoch: [53][17/408]	Loss 0.0056 (0.0056)	
training:	Epoch: [53][18/408]	Loss 0.0033 (0.0055)	
training:	Epoch: [53][19/408]	Loss 0.0035 (0.0054)	
training:	Epoch: [53][20/408]	Loss 0.0024 (0.0053)	
training:	Epoch: [53][21/408]	Loss 0.0027 (0.0051)	
training:	Epoch: [53][22/408]	Loss 0.0100 (0.0054)	
training:	Epoch: [53][23/408]	Loss 0.0030 (0.0053)	
training:	Epoch: [53][24/408]	Loss 0.0031 (0.0052)	
training:	Epoch: [53][25/408]	Loss 0.0027 (0.0051)	
training:	Epoch: [53][26/408]	Loss 0.0027 (0.0050)	
training:	Epoch: [53][27/408]	Loss 0.0066 (0.0050)	
training:	Epoch: [53][28/408]	Loss 0.0019 (0.0049)	
training:	Epoch: [53][29/408]	Loss 0.0361 (0.0060)	
training:	Epoch: [53][30/408]	Loss 0.0019 (0.0059)	
training:	Epoch: [53][31/408]	Loss 0.0040 (0.0058)	
training:	Epoch: [53][32/408]	Loss 0.0025 (0.0057)	
training:	Epoch: [53][33/408]	Loss 0.0725 (0.0077)	
training:	Epoch: [53][34/408]	Loss 0.0037 (0.0076)	
training:	Epoch: [53][35/408]	Loss 0.0023 (0.0075)	
training:	Epoch: [53][36/408]	Loss 0.0038 (0.0074)	
training:	Epoch: [53][37/408]	Loss 0.0031 (0.0072)	
training:	Epoch: [53][38/408]	Loss 0.0034 (0.0071)	
training:	Epoch: [53][39/408]	Loss 0.0027 (0.0070)	
training:	Epoch: [53][40/408]	Loss 0.1693 (0.0111)	
training:	Epoch: [53][41/408]	Loss 0.0021 (0.0109)	
training:	Epoch: [53][42/408]	Loss 0.0051 (0.0107)	
training:	Epoch: [53][43/408]	Loss 0.0112 (0.0107)	
training:	Epoch: [53][44/408]	Loss 0.0033 (0.0106)	
training:	Epoch: [53][45/408]	Loss 0.0031 (0.0104)	
training:	Epoch: [53][46/408]	Loss 0.0102 (0.0104)	
training:	Epoch: [53][47/408]	Loss 0.0024 (0.0102)	
training:	Epoch: [53][48/408]	Loss 0.0025 (0.0101)	
training:	Epoch: [53][49/408]	Loss 0.0031 (0.0099)	
training:	Epoch: [53][50/408]	Loss 0.0052 (0.0098)	
training:	Epoch: [53][51/408]	Loss 0.0032 (0.0097)	
training:	Epoch: [53][52/408]	Loss 0.0064 (0.0096)	
training:	Epoch: [53][53/408]	Loss 0.0071 (0.0096)	
training:	Epoch: [53][54/408]	Loss 0.0928 (0.0111)	
training:	Epoch: [53][55/408]	Loss 0.0022 (0.0110)	
training:	Epoch: [53][56/408]	Loss 0.0038 (0.0108)	
training:	Epoch: [53][57/408]	Loss 0.0029 (0.0107)	
training:	Epoch: [53][58/408]	Loss 0.0024 (0.0106)	
training:	Epoch: [53][59/408]	Loss 0.0022 (0.0104)	
training:	Epoch: [53][60/408]	Loss 0.0063 (0.0103)	
training:	Epoch: [53][61/408]	Loss 0.0023 (0.0102)	
training:	Epoch: [53][62/408]	Loss 0.0065 (0.0102)	
training:	Epoch: [53][63/408]	Loss 0.0077 (0.0101)	
training:	Epoch: [53][64/408]	Loss 0.0019 (0.0100)	
training:	Epoch: [53][65/408]	Loss 0.0027 (0.0099)	
training:	Epoch: [53][66/408]	Loss 0.0105 (0.0099)	
training:	Epoch: [53][67/408]	Loss 0.0098 (0.0099)	
training:	Epoch: [53][68/408]	Loss 0.0023 (0.0098)	
training:	Epoch: [53][69/408]	Loss 0.0029 (0.0097)	
training:	Epoch: [53][70/408]	Loss 0.0030 (0.0096)	
training:	Epoch: [53][71/408]	Loss 0.1387 (0.0114)	
training:	Epoch: [53][72/408]	Loss 0.0021 (0.0113)	
training:	Epoch: [53][73/408]	Loss 0.0033 (0.0112)	
training:	Epoch: [53][74/408]	Loss 0.0049 (0.0111)	
training:	Epoch: [53][75/408]	Loss 0.0063 (0.0110)	
training:	Epoch: [53][76/408]	Loss 0.0036 (0.0109)	
training:	Epoch: [53][77/408]	Loss 0.0026 (0.0108)	
training:	Epoch: [53][78/408]	Loss 0.0047 (0.0107)	
training:	Epoch: [53][79/408]	Loss 0.0029 (0.0106)	
training:	Epoch: [53][80/408]	Loss 0.0168 (0.0107)	
training:	Epoch: [53][81/408]	Loss 0.0035 (0.0106)	
training:	Epoch: [53][82/408]	Loss 0.0142 (0.0107)	
training:	Epoch: [53][83/408]	Loss 0.1047 (0.0118)	
training:	Epoch: [53][84/408]	Loss 0.0032 (0.0117)	
training:	Epoch: [53][85/408]	Loss 0.0032 (0.0116)	
training:	Epoch: [53][86/408]	Loss 0.0043 (0.0115)	
training:	Epoch: [53][87/408]	Loss 0.0106 (0.0115)	
training:	Epoch: [53][88/408]	Loss 0.0029 (0.0114)	
training:	Epoch: [53][89/408]	Loss 0.0026 (0.0113)	
training:	Epoch: [53][90/408]	Loss 0.0101 (0.0113)	
training:	Epoch: [53][91/408]	Loss 0.0052 (0.0112)	
training:	Epoch: [53][92/408]	Loss 0.0026 (0.0111)	
training:	Epoch: [53][93/408]	Loss 0.0035 (0.0110)	
training:	Epoch: [53][94/408]	Loss 0.0029 (0.0110)	
training:	Epoch: [53][95/408]	Loss 0.0078 (0.0109)	
training:	Epoch: [53][96/408]	Loss 0.2023 (0.0129)	
training:	Epoch: [53][97/408]	Loss 0.0028 (0.0128)	
training:	Epoch: [53][98/408]	Loss 0.1580 (0.0143)	
training:	Epoch: [53][99/408]	Loss 0.0031 (0.0142)	
training:	Epoch: [53][100/408]	Loss 0.0020 (0.0141)	
training:	Epoch: [53][101/408]	Loss 0.0057 (0.0140)	
training:	Epoch: [53][102/408]	Loss 0.0039 (0.0139)	
training:	Epoch: [53][103/408]	Loss 0.0024 (0.0138)	
training:	Epoch: [53][104/408]	Loss 0.0118 (0.0137)	
training:	Epoch: [53][105/408]	Loss 0.0026 (0.0136)	
training:	Epoch: [53][106/408]	Loss 0.0072 (0.0136)	
training:	Epoch: [53][107/408]	Loss 0.0058 (0.0135)	
training:	Epoch: [53][108/408]	Loss 0.0023 (0.0134)	
training:	Epoch: [53][109/408]	Loss 0.0022 (0.0133)	
training:	Epoch: [53][110/408]	Loss 0.0035 (0.0132)	
training:	Epoch: [53][111/408]	Loss 0.1630 (0.0146)	
training:	Epoch: [53][112/408]	Loss 0.0020 (0.0144)	
training:	Epoch: [53][113/408]	Loss 0.0092 (0.0144)	
training:	Epoch: [53][114/408]	Loss 0.0022 (0.0143)	
training:	Epoch: [53][115/408]	Loss 0.1287 (0.0153)	
training:	Epoch: [53][116/408]	Loss 0.0147 (0.0153)	
training:	Epoch: [53][117/408]	Loss 0.0426 (0.0155)	
training:	Epoch: [53][118/408]	Loss 0.0030 (0.0154)	
training:	Epoch: [53][119/408]	Loss 0.0027 (0.0153)	
training:	Epoch: [53][120/408]	Loss 0.0057 (0.0152)	
training:	Epoch: [53][121/408]	Loss 0.0044 (0.0151)	
training:	Epoch: [53][122/408]	Loss 0.0023 (0.0150)	
training:	Epoch: [53][123/408]	Loss 0.0048 (0.0149)	
training:	Epoch: [53][124/408]	Loss 0.0605 (0.0153)	
training:	Epoch: [53][125/408]	Loss 0.0024 (0.0152)	
training:	Epoch: [53][126/408]	Loss 0.0039 (0.0151)	
training:	Epoch: [53][127/408]	Loss 0.0051 (0.0150)	
training:	Epoch: [53][128/408]	Loss 0.0028 (0.0149)	
training:	Epoch: [53][129/408]	Loss 0.0260 (0.0150)	
training:	Epoch: [53][130/408]	Loss 0.0045 (0.0150)	
training:	Epoch: [53][131/408]	Loss 0.0038 (0.0149)	
training:	Epoch: [53][132/408]	Loss 0.0019 (0.0148)	
training:	Epoch: [53][133/408]	Loss 0.0079 (0.0147)	
training:	Epoch: [53][134/408]	Loss 0.0298 (0.0148)	
training:	Epoch: [53][135/408]	Loss 0.0053 (0.0148)	
training:	Epoch: [53][136/408]	Loss 0.0077 (0.0147)	
training:	Epoch: [53][137/408]	Loss 0.0145 (0.0147)	
training:	Epoch: [53][138/408]	Loss 0.0135 (0.0147)	
training:	Epoch: [53][139/408]	Loss 0.0026 (0.0146)	
training:	Epoch: [53][140/408]	Loss 0.0278 (0.0147)	
training:	Epoch: [53][141/408]	Loss 0.0068 (0.0146)	
training:	Epoch: [53][142/408]	Loss 0.2453 (0.0163)	
training:	Epoch: [53][143/408]	Loss 0.0052 (0.0162)	
training:	Epoch: [53][144/408]	Loss 0.0058 (0.0161)	
training:	Epoch: [53][145/408]	Loss 0.0027 (0.0160)	
training:	Epoch: [53][146/408]	Loss 0.0055 (0.0160)	
training:	Epoch: [53][147/408]	Loss 0.0023 (0.0159)	
training:	Epoch: [53][148/408]	Loss 0.0122 (0.0158)	
training:	Epoch: [53][149/408]	Loss 0.1708 (0.0169)	
training:	Epoch: [53][150/408]	Loss 0.0036 (0.0168)	
training:	Epoch: [53][151/408]	Loss 0.1886 (0.0179)	
training:	Epoch: [53][152/408]	Loss 0.0105 (0.0179)	
training:	Epoch: [53][153/408]	Loss 0.0057 (0.0178)	
training:	Epoch: [53][154/408]	Loss 0.0169 (0.0178)	
training:	Epoch: [53][155/408]	Loss 0.0070 (0.0177)	
training:	Epoch: [53][156/408]	Loss 0.0025 (0.0176)	
training:	Epoch: [53][157/408]	Loss 0.0024 (0.0175)	
training:	Epoch: [53][158/408]	Loss 0.0062 (0.0175)	
training:	Epoch: [53][159/408]	Loss 0.0171 (0.0175)	
training:	Epoch: [53][160/408]	Loss 0.0047 (0.0174)	
training:	Epoch: [53][161/408]	Loss 0.0049 (0.0173)	
training:	Epoch: [53][162/408]	Loss 0.0050 (0.0172)	
training:	Epoch: [53][163/408]	Loss 0.1303 (0.0179)	
training:	Epoch: [53][164/408]	Loss 0.0059 (0.0178)	
training:	Epoch: [53][165/408]	Loss 0.0038 (0.0178)	
training:	Epoch: [53][166/408]	Loss 0.0026 (0.0177)	
training:	Epoch: [53][167/408]	Loss 0.0091 (0.0176)	
training:	Epoch: [53][168/408]	Loss 0.0051 (0.0175)	
training:	Epoch: [53][169/408]	Loss 0.0087 (0.0175)	
training:	Epoch: [53][170/408]	Loss 0.0058 (0.0174)	
training:	Epoch: [53][171/408]	Loss 0.0078 (0.0174)	
training:	Epoch: [53][172/408]	Loss 0.0044 (0.0173)	
training:	Epoch: [53][173/408]	Loss 0.0044 (0.0172)	
training:	Epoch: [53][174/408]	Loss 0.0023 (0.0171)	
training:	Epoch: [53][175/408]	Loss 0.0111 (0.0171)	
training:	Epoch: [53][176/408]	Loss 0.0031 (0.0170)	
training:	Epoch: [53][177/408]	Loss 0.0036 (0.0169)	
training:	Epoch: [53][178/408]	Loss 0.1290 (0.0176)	
training:	Epoch: [53][179/408]	Loss 0.2253 (0.0187)	
training:	Epoch: [53][180/408]	Loss 0.0032 (0.0186)	
training:	Epoch: [53][181/408]	Loss 0.0091 (0.0186)	
training:	Epoch: [53][182/408]	Loss 0.0018 (0.0185)	
training:	Epoch: [53][183/408]	Loss 0.0033 (0.0184)	
training:	Epoch: [53][184/408]	Loss 0.0064 (0.0183)	
training:	Epoch: [53][185/408]	Loss 0.0054 (0.0183)	
training:	Epoch: [53][186/408]	Loss 0.0043 (0.0182)	
training:	Epoch: [53][187/408]	Loss 0.0026 (0.0181)	
training:	Epoch: [53][188/408]	Loss 0.0043 (0.0180)	
training:	Epoch: [53][189/408]	Loss 0.0126 (0.0180)	
training:	Epoch: [53][190/408]	Loss 0.0122 (0.0180)	
training:	Epoch: [53][191/408]	Loss 0.0070 (0.0179)	
training:	Epoch: [53][192/408]	Loss 0.0040 (0.0179)	
training:	Epoch: [53][193/408]	Loss 0.0121 (0.0178)	
training:	Epoch: [53][194/408]	Loss 0.0022 (0.0177)	
training:	Epoch: [53][195/408]	Loss 0.0097 (0.0177)	
training:	Epoch: [53][196/408]	Loss 0.0033 (0.0176)	
training:	Epoch: [53][197/408]	Loss 0.0485 (0.0178)	
training:	Epoch: [53][198/408]	Loss 0.0055 (0.0177)	
training:	Epoch: [53][199/408]	Loss 0.0192 (0.0177)	
training:	Epoch: [53][200/408]	Loss 0.0049 (0.0177)	
training:	Epoch: [53][201/408]	Loss 0.0058 (0.0176)	
training:	Epoch: [53][202/408]	Loss 0.0073 (0.0176)	
training:	Epoch: [53][203/408]	Loss 0.0034 (0.0175)	
training:	Epoch: [53][204/408]	Loss 0.0853 (0.0178)	
training:	Epoch: [53][205/408]	Loss 0.0102 (0.0178)	
training:	Epoch: [53][206/408]	Loss 0.0110 (0.0178)	
training:	Epoch: [53][207/408]	Loss 0.0024 (0.0177)	
training:	Epoch: [53][208/408]	Loss 0.0066 (0.0176)	
training:	Epoch: [53][209/408]	Loss 0.0041 (0.0176)	
training:	Epoch: [53][210/408]	Loss 0.0120 (0.0175)	
training:	Epoch: [53][211/408]	Loss 0.0025 (0.0175)	
training:	Epoch: [53][212/408]	Loss 0.0077 (0.0174)	
training:	Epoch: [53][213/408]	Loss 0.0056 (0.0174)	
training:	Epoch: [53][214/408]	Loss 0.0059 (0.0173)	
training:	Epoch: [53][215/408]	Loss 0.0073 (0.0173)	
training:	Epoch: [53][216/408]	Loss 0.0030 (0.0172)	
training:	Epoch: [53][217/408]	Loss 0.0970 (0.0176)	
training:	Epoch: [53][218/408]	Loss 0.0048 (0.0175)	
training:	Epoch: [53][219/408]	Loss 0.0025 (0.0174)	
training:	Epoch: [53][220/408]	Loss 0.0091 (0.0174)	
training:	Epoch: [53][221/408]	Loss 0.0029 (0.0173)	
training:	Epoch: [53][222/408]	Loss 0.0062 (0.0173)	
training:	Epoch: [53][223/408]	Loss 0.0057 (0.0172)	
training:	Epoch: [53][224/408]	Loss 0.0023 (0.0172)	
training:	Epoch: [53][225/408]	Loss 0.0126 (0.0171)	
training:	Epoch: [53][226/408]	Loss 0.0052 (0.0171)	
training:	Epoch: [53][227/408]	Loss 0.0023 (0.0170)	
training:	Epoch: [53][228/408]	Loss 0.0044 (0.0170)	
training:	Epoch: [53][229/408]	Loss 0.0053 (0.0169)	
training:	Epoch: [53][230/408]	Loss 0.0071 (0.0169)	
training:	Epoch: [53][231/408]	Loss 0.0068 (0.0168)	
training:	Epoch: [53][232/408]	Loss 0.0073 (0.0168)	
training:	Epoch: [53][233/408]	Loss 0.0029 (0.0167)	
training:	Epoch: [53][234/408]	Loss 0.0097 (0.0167)	
training:	Epoch: [53][235/408]	Loss 0.0048 (0.0166)	
training:	Epoch: [53][236/408]	Loss 0.0076 (0.0166)	
training:	Epoch: [53][237/408]	Loss 0.0054 (0.0166)	
training:	Epoch: [53][238/408]	Loss 0.0021 (0.0165)	
training:	Epoch: [53][239/408]	Loss 0.0030 (0.0164)	
training:	Epoch: [53][240/408]	Loss 0.1054 (0.0168)	
training:	Epoch: [53][241/408]	Loss 0.1597 (0.0174)	
training:	Epoch: [53][242/408]	Loss 0.0019 (0.0173)	
training:	Epoch: [53][243/408]	Loss 0.0031 (0.0173)	
training:	Epoch: [53][244/408]	Loss 0.0018 (0.0172)	
training:	Epoch: [53][245/408]	Loss 0.0018 (0.0172)	
training:	Epoch: [53][246/408]	Loss 0.0019 (0.0171)	
training:	Epoch: [53][247/408]	Loss 0.0032 (0.0170)	
training:	Epoch: [53][248/408]	Loss 0.0024 (0.0170)	
training:	Epoch: [53][249/408]	Loss 0.0034 (0.0169)	
training:	Epoch: [53][250/408]	Loss 0.0026 (0.0169)	
training:	Epoch: [53][251/408]	Loss 0.0022 (0.0168)	
training:	Epoch: [53][252/408]	Loss 0.0035 (0.0168)	
training:	Epoch: [53][253/408]	Loss 0.0032 (0.0167)	
training:	Epoch: [53][254/408]	Loss 0.0050 (0.0167)	
training:	Epoch: [53][255/408]	Loss 0.0036 (0.0166)	
training:	Epoch: [53][256/408]	Loss 0.0020 (0.0166)	
training:	Epoch: [53][257/408]	Loss 0.0084 (0.0165)	
training:	Epoch: [53][258/408]	Loss 0.0081 (0.0165)	
training:	Epoch: [53][259/408]	Loss 0.0024 (0.0164)	
training:	Epoch: [53][260/408]	Loss 0.0023 (0.0164)	
training:	Epoch: [53][261/408]	Loss 0.0054 (0.0163)	
training:	Epoch: [53][262/408]	Loss 0.0457 (0.0164)	
training:	Epoch: [53][263/408]	Loss 0.0036 (0.0164)	
training:	Epoch: [53][264/408]	Loss 0.0040 (0.0164)	
training:	Epoch: [53][265/408]	Loss 0.0047 (0.0163)	
training:	Epoch: [53][266/408]	Loss 0.0039 (0.0163)	
training:	Epoch: [53][267/408]	Loss 0.0063 (0.0162)	
training:	Epoch: [53][268/408]	Loss 0.0051 (0.0162)	
training:	Epoch: [53][269/408]	Loss 0.0035 (0.0161)	
training:	Epoch: [53][270/408]	Loss 0.0039 (0.0161)	
training:	Epoch: [53][271/408]	Loss 0.0787 (0.0163)	
training:	Epoch: [53][272/408]	Loss 0.0020 (0.0163)	
training:	Epoch: [53][273/408]	Loss 0.0024 (0.0162)	
training:	Epoch: [53][274/408]	Loss 0.0025 (0.0162)	
training:	Epoch: [53][275/408]	Loss 0.0055 (0.0161)	
training:	Epoch: [53][276/408]	Loss 0.0047 (0.0161)	
training:	Epoch: [53][277/408]	Loss 0.0025 (0.0160)	
training:	Epoch: [53][278/408]	Loss 0.0027 (0.0160)	
training:	Epoch: [53][279/408]	Loss 0.0045 (0.0160)	
training:	Epoch: [53][280/408]	Loss 0.0039 (0.0159)	
training:	Epoch: [53][281/408]	Loss 0.0330 (0.0160)	
training:	Epoch: [53][282/408]	Loss 0.0150 (0.0160)	
training:	Epoch: [53][283/408]	Loss 0.0037 (0.0159)	
training:	Epoch: [53][284/408]	Loss 0.0033 (0.0159)	
training:	Epoch: [53][285/408]	Loss 0.0047 (0.0158)	
training:	Epoch: [53][286/408]	Loss 0.0031 (0.0158)	
training:	Epoch: [53][287/408]	Loss 0.0468 (0.0159)	
training:	Epoch: [53][288/408]	Loss 0.0021 (0.0159)	
training:	Epoch: [53][289/408]	Loss 0.0020 (0.0158)	
training:	Epoch: [53][290/408]	Loss 0.0032 (0.0158)	
training:	Epoch: [53][291/408]	Loss 0.0022 (0.0157)	
training:	Epoch: [53][292/408]	Loss 0.0018 (0.0157)	
training:	Epoch: [53][293/408]	Loss 0.0035 (0.0156)	
training:	Epoch: [53][294/408]	Loss 0.0019 (0.0156)	
training:	Epoch: [53][295/408]	Loss 0.0081 (0.0156)	
training:	Epoch: [53][296/408]	Loss 0.0097 (0.0155)	
training:	Epoch: [53][297/408]	Loss 0.0017 (0.0155)	
training:	Epoch: [53][298/408]	Loss 0.0026 (0.0154)	
training:	Epoch: [53][299/408]	Loss 0.0024 (0.0154)	
training:	Epoch: [53][300/408]	Loss 0.0022 (0.0154)	
training:	Epoch: [53][301/408]	Loss 0.0035 (0.0153)	
training:	Epoch: [53][302/408]	Loss 0.0026 (0.0153)	
training:	Epoch: [53][303/408]	Loss 0.0044 (0.0152)	
training:	Epoch: [53][304/408]	Loss 0.0020 (0.0152)	
training:	Epoch: [53][305/408]	Loss 0.0125 (0.0152)	
training:	Epoch: [53][306/408]	Loss 0.0024 (0.0151)	
training:	Epoch: [53][307/408]	Loss 0.0029 (0.0151)	
training:	Epoch: [53][308/408]	Loss 0.0070 (0.0151)	
training:	Epoch: [53][309/408]	Loss 0.0095 (0.0151)	
training:	Epoch: [53][310/408]	Loss 0.0022 (0.0150)	
training:	Epoch: [53][311/408]	Loss 0.0041 (0.0150)	
training:	Epoch: [53][312/408]	Loss 0.0023 (0.0149)	
training:	Epoch: [53][313/408]	Loss 0.0018 (0.0149)	
training:	Epoch: [53][314/408]	Loss 0.0033 (0.0149)	
training:	Epoch: [53][315/408]	Loss 0.0036 (0.0148)	
training:	Epoch: [53][316/408]	Loss 0.0054 (0.0148)	
training:	Epoch: [53][317/408]	Loss 0.0021 (0.0148)	
training:	Epoch: [53][318/408]	Loss 0.0027 (0.0147)	
training:	Epoch: [53][319/408]	Loss 0.0038 (0.0147)	
training:	Epoch: [53][320/408]	Loss 0.0050 (0.0147)	
training:	Epoch: [53][321/408]	Loss 0.0067 (0.0146)	
training:	Epoch: [53][322/408]	Loss 0.0020 (0.0146)	
training:	Epoch: [53][323/408]	Loss 0.0024 (0.0146)	
training:	Epoch: [53][324/408]	Loss 0.0063 (0.0145)	
training:	Epoch: [53][325/408]	Loss 0.0046 (0.0145)	
training:	Epoch: [53][326/408]	Loss 0.0216 (0.0145)	
training:	Epoch: [53][327/408]	Loss 0.0026 (0.0145)	
training:	Epoch: [53][328/408]	Loss 0.0029 (0.0144)	
training:	Epoch: [53][329/408]	Loss 0.0020 (0.0144)	
training:	Epoch: [53][330/408]	Loss 0.0024 (0.0144)	
training:	Epoch: [53][331/408]	Loss 0.0191 (0.0144)	
training:	Epoch: [53][332/408]	Loss 0.0054 (0.0144)	
training:	Epoch: [53][333/408]	Loss 0.0039 (0.0143)	
training:	Epoch: [53][334/408]	Loss 0.0026 (0.0143)	
training:	Epoch: [53][335/408]	Loss 0.1800 (0.0148)	
training:	Epoch: [53][336/408]	Loss 0.0029 (0.0148)	
training:	Epoch: [53][337/408]	Loss 0.0016 (0.0147)	
training:	Epoch: [53][338/408]	Loss 0.0027 (0.0147)	
training:	Epoch: [53][339/408]	Loss 0.0035 (0.0146)	
training:	Epoch: [53][340/408]	Loss 0.0022 (0.0146)	
training:	Epoch: [53][341/408]	Loss 0.0021 (0.0146)	
training:	Epoch: [53][342/408]	Loss 0.0324 (0.0146)	
training:	Epoch: [53][343/408]	Loss 0.0030 (0.0146)	
training:	Epoch: [53][344/408]	Loss 0.0018 (0.0146)	
training:	Epoch: [53][345/408]	Loss 0.0022 (0.0145)	
training:	Epoch: [53][346/408]	Loss 0.0025 (0.0145)	
training:	Epoch: [53][347/408]	Loss 0.0055 (0.0145)	
training:	Epoch: [53][348/408]	Loss 0.0118 (0.0145)	
training:	Epoch: [53][349/408]	Loss 0.0024 (0.0144)	
training:	Epoch: [53][350/408]	Loss 0.0018 (0.0144)	
training:	Epoch: [53][351/408]	Loss 0.0521 (0.0145)	
training:	Epoch: [53][352/408]	Loss 0.0020 (0.0145)	
training:	Epoch: [53][353/408]	Loss 0.0037 (0.0144)	
training:	Epoch: [53][354/408]	Loss 0.0111 (0.0144)	
training:	Epoch: [53][355/408]	Loss 0.0863 (0.0146)	
training:	Epoch: [53][356/408]	Loss 0.0240 (0.0146)	
training:	Epoch: [53][357/408]	Loss 0.0091 (0.0146)	
training:	Epoch: [53][358/408]	Loss 0.0411 (0.0147)	
training:	Epoch: [53][359/408]	Loss 0.0016 (0.0147)	
training:	Epoch: [53][360/408]	Loss 0.0034 (0.0146)	
training:	Epoch: [53][361/408]	Loss 0.0017 (0.0146)	
training:	Epoch: [53][362/408]	Loss 0.0022 (0.0146)	
training:	Epoch: [53][363/408]	Loss 0.0021 (0.0145)	
training:	Epoch: [53][364/408]	Loss 0.0023 (0.0145)	
training:	Epoch: [53][365/408]	Loss 0.0027 (0.0145)	
training:	Epoch: [53][366/408]	Loss 0.0028 (0.0144)	
training:	Epoch: [53][367/408]	Loss 0.0022 (0.0144)	
training:	Epoch: [53][368/408]	Loss 0.0016 (0.0144)	
training:	Epoch: [53][369/408]	Loss 0.0027 (0.0143)	
training:	Epoch: [53][370/408]	Loss 0.0025 (0.0143)	
training:	Epoch: [53][371/408]	Loss 0.0022 (0.0143)	
training:	Epoch: [53][372/408]	Loss 0.2907 (0.0150)	
training:	Epoch: [53][373/408]	Loss 0.0020 (0.0150)	
training:	Epoch: [53][374/408]	Loss 0.0032 (0.0149)	
training:	Epoch: [53][375/408]	Loss 0.0048 (0.0149)	
training:	Epoch: [53][376/408]	Loss 0.0018 (0.0149)	
training:	Epoch: [53][377/408]	Loss 0.0053 (0.0149)	
training:	Epoch: [53][378/408]	Loss 0.0041 (0.0148)	
training:	Epoch: [53][379/408]	Loss 0.0023 (0.0148)	
training:	Epoch: [53][380/408]	Loss 0.0022 (0.0148)	
training:	Epoch: [53][381/408]	Loss 0.0020 (0.0147)	
training:	Epoch: [53][382/408]	Loss 0.0065 (0.0147)	
training:	Epoch: [53][383/408]	Loss 0.0026 (0.0147)	
training:	Epoch: [53][384/408]	Loss 0.0025 (0.0146)	
training:	Epoch: [53][385/408]	Loss 0.0066 (0.0146)	
training:	Epoch: [53][386/408]	Loss 0.0036 (0.0146)	
training:	Epoch: [53][387/408]	Loss 0.0043 (0.0146)	
training:	Epoch: [53][388/408]	Loss 0.0023 (0.0145)	
training:	Epoch: [53][389/408]	Loss 0.0036 (0.0145)	
training:	Epoch: [53][390/408]	Loss 0.0036 (0.0145)	
training:	Epoch: [53][391/408]	Loss 0.0048 (0.0145)	
training:	Epoch: [53][392/408]	Loss 0.0026 (0.0144)	
training:	Epoch: [53][393/408]	Loss 0.0036 (0.0144)	
training:	Epoch: [53][394/408]	Loss 0.0032 (0.0144)	
training:	Epoch: [53][395/408]	Loss 0.0024 (0.0143)	
training:	Epoch: [53][396/408]	Loss 0.0036 (0.0143)	
training:	Epoch: [53][397/408]	Loss 0.0024 (0.0143)	
training:	Epoch: [53][398/408]	Loss 0.0034 (0.0143)	
training:	Epoch: [53][399/408]	Loss 0.0035 (0.0142)	
training:	Epoch: [53][400/408]	Loss 0.0020 (0.0142)	
training:	Epoch: [53][401/408]	Loss 0.0038 (0.0142)	
training:	Epoch: [53][402/408]	Loss 0.0039 (0.0141)	
training:	Epoch: [53][403/408]	Loss 0.0041 (0.0141)	
training:	Epoch: [53][404/408]	Loss 0.0047 (0.0141)	
training:	Epoch: [53][405/408]	Loss 0.0024 (0.0141)	
training:	Epoch: [53][406/408]	Loss 0.0045 (0.0140)	
training:	Epoch: [53][407/408]	Loss 0.0049 (0.0140)	
training:	Epoch: [53][408/408]	Loss 0.0035 (0.0140)	
Training:	 Loss: 0.0140

Training:	 ACC: 0.9968 0.9968 0.9976 0.9959
Validation:	 ACC: 0.7781 0.7796 0.8106 0.7455
Validation:	 Best_BACC: 0.8226 0.8250 0.8772 0.7679
Validation:	 Loss: 1.0915
Pretraining:	Epoch 54/200
----------
training:	Epoch: [54][1/408]	Loss 0.0025 (0.0025)	
training:	Epoch: [54][2/408]	Loss 0.0056 (0.0041)	
training:	Epoch: [54][3/408]	Loss 0.0018 (0.0033)	
training:	Epoch: [54][4/408]	Loss 0.0395 (0.0124)	
training:	Epoch: [54][5/408]	Loss 0.0049 (0.0109)	
training:	Epoch: [54][6/408]	Loss 0.0214 (0.0126)	
training:	Epoch: [54][7/408]	Loss 0.0019 (0.0111)	
training:	Epoch: [54][8/408]	Loss 0.0019 (0.0100)	
training:	Epoch: [54][9/408]	Loss 0.0018 (0.0090)	
training:	Epoch: [54][10/408]	Loss 0.0016 (0.0083)	
training:	Epoch: [54][11/408]	Loss 0.0025 (0.0078)	
training:	Epoch: [54][12/408]	Loss 0.2473 (0.0277)	
training:	Epoch: [54][13/408]	Loss 0.0021 (0.0258)	
training:	Epoch: [54][14/408]	Loss 0.0019 (0.0241)	
training:	Epoch: [54][15/408]	Loss 0.0023 (0.0226)	
training:	Epoch: [54][16/408]	Loss 0.0040 (0.0214)	
training:	Epoch: [54][17/408]	Loss 0.0054 (0.0205)	
training:	Epoch: [54][18/408]	Loss 0.0036 (0.0196)	
training:	Epoch: [54][19/408]	Loss 0.0018 (0.0186)	
training:	Epoch: [54][20/408]	Loss 0.0020 (0.0178)	
training:	Epoch: [54][21/408]	Loss 0.0043 (0.0172)	
training:	Epoch: [54][22/408]	Loss 0.0020 (0.0165)	
training:	Epoch: [54][23/408]	Loss 0.0026 (0.0159)	
training:	Epoch: [54][24/408]	Loss 0.0016 (0.0153)	
training:	Epoch: [54][25/408]	Loss 0.0122 (0.0151)	
training:	Epoch: [54][26/408]	Loss 0.0023 (0.0147)	
training:	Epoch: [54][27/408]	Loss 0.0036 (0.0142)	
training:	Epoch: [54][28/408]	Loss 0.0029 (0.0138)	
training:	Epoch: [54][29/408]	Loss 0.0026 (0.0134)	
training:	Epoch: [54][30/408]	Loss 0.0023 (0.0131)	
training:	Epoch: [54][31/408]	Loss 0.0027 (0.0127)	
training:	Epoch: [54][32/408]	Loss 0.0035 (0.0125)	
training:	Epoch: [54][33/408]	Loss 0.0031 (0.0122)	
training:	Epoch: [54][34/408]	Loss 0.0027 (0.0119)	
training:	Epoch: [54][35/408]	Loss 0.0018 (0.0116)	
training:	Epoch: [54][36/408]	Loss 0.0035 (0.0114)	
training:	Epoch: [54][37/408]	Loss 0.0061 (0.0112)	
training:	Epoch: [54][38/408]	Loss 0.1004 (0.0136)	
training:	Epoch: [54][39/408]	Loss 0.0027 (0.0133)	
training:	Epoch: [54][40/408]	Loss 0.0026 (0.0130)	
training:	Epoch: [54][41/408]	Loss 0.0028 (0.0128)	
training:	Epoch: [54][42/408]	Loss 0.0024 (0.0125)	
training:	Epoch: [54][43/408]	Loss 0.0027 (0.0123)	
training:	Epoch: [54][44/408]	Loss 0.0025 (0.0121)	
training:	Epoch: [54][45/408]	Loss 0.0057 (0.0119)	
training:	Epoch: [54][46/408]	Loss 0.0055 (0.0118)	
training:	Epoch: [54][47/408]	Loss 0.0019 (0.0116)	
training:	Epoch: [54][48/408]	Loss 0.0016 (0.0114)	
training:	Epoch: [54][49/408]	Loss 0.0022 (0.0112)	
training:	Epoch: [54][50/408]	Loss 0.0027 (0.0110)	
training:	Epoch: [54][51/408]	Loss 0.0024 (0.0109)	
training:	Epoch: [54][52/408]	Loss 0.0211 (0.0111)	
training:	Epoch: [54][53/408]	Loss 0.0029 (0.0109)	
training:	Epoch: [54][54/408]	Loss 0.0025 (0.0107)	
training:	Epoch: [54][55/408]	Loss 0.0016 (0.0106)	
training:	Epoch: [54][56/408]	Loss 0.0029 (0.0104)	
training:	Epoch: [54][57/408]	Loss 0.0025 (0.0103)	
training:	Epoch: [54][58/408]	Loss 0.0027 (0.0102)	
training:	Epoch: [54][59/408]	Loss 0.0026 (0.0100)	
training:	Epoch: [54][60/408]	Loss 0.0018 (0.0099)	
training:	Epoch: [54][61/408]	Loss 0.0031 (0.0098)	
training:	Epoch: [54][62/408]	Loss 0.0018 (0.0097)	
training:	Epoch: [54][63/408]	Loss 0.0025 (0.0096)	
training:	Epoch: [54][64/408]	Loss 0.0808 (0.0107)	
training:	Epoch: [54][65/408]	Loss 0.0045 (0.0106)	
training:	Epoch: [54][66/408]	Loss 0.0015 (0.0104)	
training:	Epoch: [54][67/408]	Loss 0.0104 (0.0104)	
training:	Epoch: [54][68/408]	Loss 0.0030 (0.0103)	
training:	Epoch: [54][69/408]	Loss 0.0026 (0.0102)	
training:	Epoch: [54][70/408]	Loss 0.0019 (0.0101)	
training:	Epoch: [54][71/408]	Loss 0.0036 (0.0100)	
training:	Epoch: [54][72/408]	Loss 0.0029 (0.0099)	
training:	Epoch: [54][73/408]	Loss 0.0027 (0.0098)	
training:	Epoch: [54][74/408]	Loss 0.0021 (0.0097)	
training:	Epoch: [54][75/408]	Loss 0.0032 (0.0096)	
training:	Epoch: [54][76/408]	Loss 0.0166 (0.0097)	
training:	Epoch: [54][77/408]	Loss 0.0019 (0.0096)	
training:	Epoch: [54][78/408]	Loss 0.0023 (0.0095)	
training:	Epoch: [54][79/408]	Loss 0.0081 (0.0095)	
training:	Epoch: [54][80/408]	Loss 0.0025 (0.0094)	
training:	Epoch: [54][81/408]	Loss 0.0024 (0.0093)	
training:	Epoch: [54][82/408]	Loss 0.0046 (0.0093)	
training:	Epoch: [54][83/408]	Loss 0.0027 (0.0092)	
training:	Epoch: [54][84/408]	Loss 0.0022 (0.0091)	
training:	Epoch: [54][85/408]	Loss 0.0375 (0.0094)	
training:	Epoch: [54][86/408]	Loss 0.0023 (0.0093)	
training:	Epoch: [54][87/408]	Loss 0.0855 (0.0102)	
training:	Epoch: [54][88/408]	Loss 0.0029 (0.0101)	
training:	Epoch: [54][89/408]	Loss 0.0053 (0.0101)	
training:	Epoch: [54][90/408]	Loss 0.0045 (0.0100)	
training:	Epoch: [54][91/408]	Loss 0.0030 (0.0099)	
training:	Epoch: [54][92/408]	Loss 0.0017 (0.0099)	
training:	Epoch: [54][93/408]	Loss 0.1301 (0.0112)	
training:	Epoch: [54][94/408]	Loss 0.0030 (0.0111)	
training:	Epoch: [54][95/408]	Loss 0.0029 (0.0110)	
training:	Epoch: [54][96/408]	Loss 0.0018 (0.0109)	
training:	Epoch: [54][97/408]	Loss 0.0045 (0.0108)	
training:	Epoch: [54][98/408]	Loss 0.0025 (0.0107)	
training:	Epoch: [54][99/408]	Loss 0.0017 (0.0106)	
training:	Epoch: [54][100/408]	Loss 0.0021 (0.0106)	
training:	Epoch: [54][101/408]	Loss 0.0020 (0.0105)	
training:	Epoch: [54][102/408]	Loss 0.0057 (0.0104)	
training:	Epoch: [54][103/408]	Loss 0.0027 (0.0103)	
training:	Epoch: [54][104/408]	Loss 0.0188 (0.0104)	
training:	Epoch: [54][105/408]	Loss 0.0030 (0.0104)	
training:	Epoch: [54][106/408]	Loss 0.0041 (0.0103)	
training:	Epoch: [54][107/408]	Loss 0.0032 (0.0102)	
training:	Epoch: [54][108/408]	Loss 0.0031 (0.0102)	
training:	Epoch: [54][109/408]	Loss 0.0024 (0.0101)	
training:	Epoch: [54][110/408]	Loss 0.0024 (0.0100)	
training:	Epoch: [54][111/408]	Loss 0.0426 (0.0103)	
training:	Epoch: [54][112/408]	Loss 0.0016 (0.0102)	
training:	Epoch: [54][113/408]	Loss 0.0026 (0.0102)	
training:	Epoch: [54][114/408]	Loss 0.0023 (0.0101)	
training:	Epoch: [54][115/408]	Loss 0.0059 (0.0101)	
training:	Epoch: [54][116/408]	Loss 0.0070 (0.0100)	
training:	Epoch: [54][117/408]	Loss 0.0025 (0.0100)	
training:	Epoch: [54][118/408]	Loss 0.0019 (0.0099)	
training:	Epoch: [54][119/408]	Loss 0.0028 (0.0098)	
training:	Epoch: [54][120/408]	Loss 0.0036 (0.0098)	
training:	Epoch: [54][121/408]	Loss 0.0028 (0.0097)	
training:	Epoch: [54][122/408]	Loss 0.0020 (0.0097)	
training:	Epoch: [54][123/408]	Loss 0.0046 (0.0096)	
training:	Epoch: [54][124/408]	Loss 0.0018 (0.0096)	
training:	Epoch: [54][125/408]	Loss 0.0027 (0.0095)	
training:	Epoch: [54][126/408]	Loss 0.0036 (0.0095)	
training:	Epoch: [54][127/408]	Loss 0.0196 (0.0095)	
training:	Epoch: [54][128/408]	Loss 0.0036 (0.0095)	
training:	Epoch: [54][129/408]	Loss 0.0094 (0.0095)	
training:	Epoch: [54][130/408]	Loss 0.0026 (0.0094)	
training:	Epoch: [54][131/408]	Loss 0.0020 (0.0094)	
training:	Epoch: [54][132/408]	Loss 0.0044 (0.0094)	
training:	Epoch: [54][133/408]	Loss 0.0028 (0.0093)	
training:	Epoch: [54][134/408]	Loss 0.0023 (0.0093)	
training:	Epoch: [54][135/408]	Loss 0.0021 (0.0092)	
training:	Epoch: [54][136/408]	Loss 0.0044 (0.0092)	
training:	Epoch: [54][137/408]	Loss 0.0027 (0.0091)	
training:	Epoch: [54][138/408]	Loss 0.0029 (0.0091)	
training:	Epoch: [54][139/408]	Loss 0.0214 (0.0092)	
training:	Epoch: [54][140/408]	Loss 0.0041 (0.0091)	
training:	Epoch: [54][141/408]	Loss 0.0016 (0.0091)	
training:	Epoch: [54][142/408]	Loss 0.0064 (0.0091)	
training:	Epoch: [54][143/408]	Loss 0.0058 (0.0090)	
training:	Epoch: [54][144/408]	Loss 0.0137 (0.0091)	
training:	Epoch: [54][145/408]	Loss 0.0019 (0.0090)	
training:	Epoch: [54][146/408]	Loss 0.0373 (0.0092)	
training:	Epoch: [54][147/408]	Loss 0.0030 (0.0092)	
training:	Epoch: [54][148/408]	Loss 0.0017 (0.0091)	
training:	Epoch: [54][149/408]	Loss 0.1969 (0.0104)	
training:	Epoch: [54][150/408]	Loss 0.0017 (0.0103)	
training:	Epoch: [54][151/408]	Loss 0.0022 (0.0103)	
training:	Epoch: [54][152/408]	Loss 0.0034 (0.0102)	
training:	Epoch: [54][153/408]	Loss 0.0020 (0.0102)	
training:	Epoch: [54][154/408]	Loss 0.0055 (0.0101)	
training:	Epoch: [54][155/408]	Loss 0.0019 (0.0101)	
training:	Epoch: [54][156/408]	Loss 0.0040 (0.0100)	
training:	Epoch: [54][157/408]	Loss 0.0091 (0.0100)	
training:	Epoch: [54][158/408]	Loss 0.0042 (0.0100)	
training:	Epoch: [54][159/408]	Loss 0.0016 (0.0099)	
training:	Epoch: [54][160/408]	Loss 0.0037 (0.0099)	
training:	Epoch: [54][161/408]	Loss 0.0026 (0.0099)	
training:	Epoch: [54][162/408]	Loss 0.0047 (0.0098)	
training:	Epoch: [54][163/408]	Loss 0.0017 (0.0098)	
training:	Epoch: [54][164/408]	Loss 0.0020 (0.0097)	
training:	Epoch: [54][165/408]	Loss 0.0018 (0.0097)	
training:	Epoch: [54][166/408]	Loss 0.0785 (0.0101)	
training:	Epoch: [54][167/408]	Loss 0.0043 (0.0101)	
training:	Epoch: [54][168/408]	Loss 0.0023 (0.0100)	
training:	Epoch: [54][169/408]	Loss 0.0070 (0.0100)	
training:	Epoch: [54][170/408]	Loss 0.0025 (0.0100)	
training:	Epoch: [54][171/408]	Loss 0.0022 (0.0099)	
training:	Epoch: [54][172/408]	Loss 0.0024 (0.0099)	
training:	Epoch: [54][173/408]	Loss 0.0019 (0.0098)	
training:	Epoch: [54][174/408]	Loss 0.0024 (0.0098)	
training:	Epoch: [54][175/408]	Loss 0.0039 (0.0097)	
training:	Epoch: [54][176/408]	Loss 0.0052 (0.0097)	
training:	Epoch: [54][177/408]	Loss 0.0043 (0.0097)	
training:	Epoch: [54][178/408]	Loss 0.1081 (0.0102)	
training:	Epoch: [54][179/408]	Loss 0.0028 (0.0102)	
training:	Epoch: [54][180/408]	Loss 0.0034 (0.0102)	
training:	Epoch: [54][181/408]	Loss 0.0043 (0.0101)	
training:	Epoch: [54][182/408]	Loss 0.0018 (0.0101)	
training:	Epoch: [54][183/408]	Loss 0.0015 (0.0100)	
training:	Epoch: [54][184/408]	Loss 0.0056 (0.0100)	
training:	Epoch: [54][185/408]	Loss 0.0024 (0.0100)	
training:	Epoch: [54][186/408]	Loss 0.0017 (0.0099)	
training:	Epoch: [54][187/408]	Loss 0.0092 (0.0099)	
training:	Epoch: [54][188/408]	Loss 0.0032 (0.0099)	
training:	Epoch: [54][189/408]	Loss 0.0122 (0.0099)	
training:	Epoch: [54][190/408]	Loss 0.0028 (0.0099)	
training:	Epoch: [54][191/408]	Loss 0.0024 (0.0098)	
training:	Epoch: [54][192/408]	Loss 0.1802 (0.0107)	
training:	Epoch: [54][193/408]	Loss 0.0600 (0.0110)	
training:	Epoch: [54][194/408]	Loss 0.0036 (0.0109)	
training:	Epoch: [54][195/408]	Loss 0.0257 (0.0110)	
training:	Epoch: [54][196/408]	Loss 0.0026 (0.0110)	
training:	Epoch: [54][197/408]	Loss 0.0343 (0.0111)	
training:	Epoch: [54][198/408]	Loss 0.0020 (0.0110)	
training:	Epoch: [54][199/408]	Loss 0.0040 (0.0110)	
training:	Epoch: [54][200/408]	Loss 0.0022 (0.0110)	
training:	Epoch: [54][201/408]	Loss 0.0160 (0.0110)	
training:	Epoch: [54][202/408]	Loss 0.0026 (0.0109)	
training:	Epoch: [54][203/408]	Loss 0.0098 (0.0109)	
training:	Epoch: [54][204/408]	Loss 0.0192 (0.0110)	
training:	Epoch: [54][205/408]	Loss 0.0018 (0.0109)	
training:	Epoch: [54][206/408]	Loss 0.2031 (0.0119)	
training:	Epoch: [54][207/408]	Loss 0.0029 (0.0118)	
training:	Epoch: [54][208/408]	Loss 0.0049 (0.0118)	
training:	Epoch: [54][209/408]	Loss 0.0039 (0.0117)	
training:	Epoch: [54][210/408]	Loss 0.0039 (0.0117)	
training:	Epoch: [54][211/408]	Loss 0.0022 (0.0117)	
training:	Epoch: [54][212/408]	Loss 0.0020 (0.0116)	
training:	Epoch: [54][213/408]	Loss 0.0020 (0.0116)	
training:	Epoch: [54][214/408]	Loss 0.0036 (0.0115)	
training:	Epoch: [54][215/408]	Loss 0.0026 (0.0115)	
training:	Epoch: [54][216/408]	Loss 0.0035 (0.0115)	
training:	Epoch: [54][217/408]	Loss 0.0040 (0.0114)	
training:	Epoch: [54][218/408]	Loss 0.0039 (0.0114)	
training:	Epoch: [54][219/408]	Loss 0.0024 (0.0113)	
training:	Epoch: [54][220/408]	Loss 0.0019 (0.0113)	
training:	Epoch: [54][221/408]	Loss 0.0029 (0.0113)	
training:	Epoch: [54][222/408]	Loss 0.0040 (0.0112)	
training:	Epoch: [54][223/408]	Loss 0.0075 (0.0112)	
training:	Epoch: [54][224/408]	Loss 0.0017 (0.0112)	
training:	Epoch: [54][225/408]	Loss 0.0025 (0.0111)	
training:	Epoch: [54][226/408]	Loss 0.0111 (0.0111)	
training:	Epoch: [54][227/408]	Loss 0.0181 (0.0112)	
training:	Epoch: [54][228/408]	Loss 0.0094 (0.0112)	
training:	Epoch: [54][229/408]	Loss 0.0083 (0.0111)	
training:	Epoch: [54][230/408]	Loss 0.0029 (0.0111)	
training:	Epoch: [54][231/408]	Loss 0.0029 (0.0111)	
training:	Epoch: [54][232/408]	Loss 0.1519 (0.0117)	
training:	Epoch: [54][233/408]	Loss 0.0075 (0.0117)	
training:	Epoch: [54][234/408]	Loss 0.0870 (0.0120)	
training:	Epoch: [54][235/408]	Loss 0.0054 (0.0120)	
training:	Epoch: [54][236/408]	Loss 0.0059 (0.0119)	
training:	Epoch: [54][237/408]	Loss 0.0032 (0.0119)	
training:	Epoch: [54][238/408]	Loss 0.0030 (0.0119)	
training:	Epoch: [54][239/408]	Loss 0.0025 (0.0118)	
training:	Epoch: [54][240/408]	Loss 0.0041 (0.0118)	
training:	Epoch: [54][241/408]	Loss 0.0086 (0.0118)	
training:	Epoch: [54][242/408]	Loss 0.0048 (0.0117)	
training:	Epoch: [54][243/408]	Loss 0.0037 (0.0117)	
training:	Epoch: [54][244/408]	Loss 0.0033 (0.0117)	
training:	Epoch: [54][245/408]	Loss 0.0063 (0.0117)	
training:	Epoch: [54][246/408]	Loss 0.0021 (0.0116)	
training:	Epoch: [54][247/408]	Loss 0.0016 (0.0116)	
training:	Epoch: [54][248/408]	Loss 0.0015 (0.0115)	
training:	Epoch: [54][249/408]	Loss 0.0110 (0.0115)	
training:	Epoch: [54][250/408]	Loss 0.0033 (0.0115)	
training:	Epoch: [54][251/408]	Loss 0.0016 (0.0115)	
training:	Epoch: [54][252/408]	Loss 0.0049 (0.0114)	
training:	Epoch: [54][253/408]	Loss 0.0033 (0.0114)	
training:	Epoch: [54][254/408]	Loss 0.0068 (0.0114)	
training:	Epoch: [54][255/408]	Loss 0.0022 (0.0113)	
training:	Epoch: [54][256/408]	Loss 0.0040 (0.0113)	
training:	Epoch: [54][257/408]	Loss 0.1099 (0.0117)	
training:	Epoch: [54][258/408]	Loss 0.0030 (0.0117)	
training:	Epoch: [54][259/408]	Loss 0.0180 (0.0117)	
training:	Epoch: [54][260/408]	Loss 0.0030 (0.0117)	
training:	Epoch: [54][261/408]	Loss 0.0045 (0.0116)	
training:	Epoch: [54][262/408]	Loss 0.0082 (0.0116)	
training:	Epoch: [54][263/408]	Loss 0.0058 (0.0116)	
training:	Epoch: [54][264/408]	Loss 0.0041 (0.0116)	
training:	Epoch: [54][265/408]	Loss 0.0030 (0.0115)	
training:	Epoch: [54][266/408]	Loss 0.0094 (0.0115)	
training:	Epoch: [54][267/408]	Loss 0.1044 (0.0119)	
training:	Epoch: [54][268/408]	Loss 0.0070 (0.0119)	
training:	Epoch: [54][269/408]	Loss 0.0062 (0.0118)	
training:	Epoch: [54][270/408]	Loss 0.0025 (0.0118)	
training:	Epoch: [54][271/408]	Loss 0.0054 (0.0118)	
training:	Epoch: [54][272/408]	Loss 0.0028 (0.0117)	
training:	Epoch: [54][273/408]	Loss 0.0017 (0.0117)	
training:	Epoch: [54][274/408]	Loss 0.0056 (0.0117)	
training:	Epoch: [54][275/408]	Loss 0.0039 (0.0117)	
training:	Epoch: [54][276/408]	Loss 0.0103 (0.0117)	
training:	Epoch: [54][277/408]	Loss 0.0019 (0.0116)	
training:	Epoch: [54][278/408]	Loss 0.0016 (0.0116)	
training:	Epoch: [54][279/408]	Loss 0.0021 (0.0115)	
training:	Epoch: [54][280/408]	Loss 0.0016 (0.0115)	
training:	Epoch: [54][281/408]	Loss 0.0842 (0.0118)	
training:	Epoch: [54][282/408]	Loss 0.0093 (0.0118)	
training:	Epoch: [54][283/408]	Loss 0.0022 (0.0117)	
training:	Epoch: [54][284/408]	Loss 0.0019 (0.0117)	
training:	Epoch: [54][285/408]	Loss 0.0085 (0.0117)	
training:	Epoch: [54][286/408]	Loss 0.0026 (0.0117)	
training:	Epoch: [54][287/408]	Loss 0.0018 (0.0116)	
training:	Epoch: [54][288/408]	Loss 0.0069 (0.0116)	
training:	Epoch: [54][289/408]	Loss 0.0028 (0.0116)	
training:	Epoch: [54][290/408]	Loss 0.0024 (0.0115)	
training:	Epoch: [54][291/408]	Loss 0.0019 (0.0115)	
training:	Epoch: [54][292/408]	Loss 0.0426 (0.0116)	
training:	Epoch: [54][293/408]	Loss 0.0056 (0.0116)	
training:	Epoch: [54][294/408]	Loss 0.0066 (0.0116)	
training:	Epoch: [54][295/408]	Loss 0.0022 (0.0115)	
training:	Epoch: [54][296/408]	Loss 0.0027 (0.0115)	
training:	Epoch: [54][297/408]	Loss 0.0016 (0.0115)	
training:	Epoch: [54][298/408]	Loss 0.0034 (0.0115)	
training:	Epoch: [54][299/408]	Loss 0.0056 (0.0114)	
training:	Epoch: [54][300/408]	Loss 0.0052 (0.0114)	
training:	Epoch: [54][301/408]	Loss 0.0020 (0.0114)	
training:	Epoch: [54][302/408]	Loss 0.0017 (0.0113)	
training:	Epoch: [54][303/408]	Loss 0.0022 (0.0113)	
training:	Epoch: [54][304/408]	Loss 0.0026 (0.0113)	
training:	Epoch: [54][305/408]	Loss 0.0050 (0.0113)	
training:	Epoch: [54][306/408]	Loss 0.1210 (0.0116)	
training:	Epoch: [54][307/408]	Loss 0.0034 (0.0116)	
training:	Epoch: [54][308/408]	Loss 0.0015 (0.0116)	
training:	Epoch: [54][309/408]	Loss 0.0021 (0.0115)	
training:	Epoch: [54][310/408]	Loss 0.0041 (0.0115)	
training:	Epoch: [54][311/408]	Loss 0.0029 (0.0115)	
training:	Epoch: [54][312/408]	Loss 0.0043 (0.0115)	
training:	Epoch: [54][313/408]	Loss 0.0073 (0.0115)	
training:	Epoch: [54][314/408]	Loss 0.0056 (0.0114)	
training:	Epoch: [54][315/408]	Loss 0.0018 (0.0114)	
training:	Epoch: [54][316/408]	Loss 0.0015 (0.0114)	
training:	Epoch: [54][317/408]	Loss 0.0047 (0.0113)	
training:	Epoch: [54][318/408]	Loss 0.0039 (0.0113)	
training:	Epoch: [54][319/408]	Loss 0.0016 (0.0113)	
training:	Epoch: [54][320/408]	Loss 0.0015 (0.0113)	
training:	Epoch: [54][321/408]	Loss 0.0015 (0.0112)	
training:	Epoch: [54][322/408]	Loss 0.0027 (0.0112)	
training:	Epoch: [54][323/408]	Loss 0.0018 (0.0112)	
training:	Epoch: [54][324/408]	Loss 0.0029 (0.0112)	
training:	Epoch: [54][325/408]	Loss 0.0019 (0.0111)	
training:	Epoch: [54][326/408]	Loss 0.0215 (0.0112)	
training:	Epoch: [54][327/408]	Loss 0.0093 (0.0112)	
training:	Epoch: [54][328/408]	Loss 0.0058 (0.0111)	
training:	Epoch: [54][329/408]	Loss 0.0019 (0.0111)	
training:	Epoch: [54][330/408]	Loss 0.0014 (0.0111)	
training:	Epoch: [54][331/408]	Loss 0.0027 (0.0111)	
training:	Epoch: [54][332/408]	Loss 0.0037 (0.0110)	
training:	Epoch: [54][333/408]	Loss 0.0019 (0.0110)	
training:	Epoch: [54][334/408]	Loss 0.0042 (0.0110)	
training:	Epoch: [54][335/408]	Loss 0.0033 (0.0110)	
training:	Epoch: [54][336/408]	Loss 0.0044 (0.0109)	
training:	Epoch: [54][337/408]	Loss 0.0030 (0.0109)	
training:	Epoch: [54][338/408]	Loss 0.0029 (0.0109)	
training:	Epoch: [54][339/408]	Loss 0.0021 (0.0109)	
training:	Epoch: [54][340/408]	Loss 0.0033 (0.0108)	
training:	Epoch: [54][341/408]	Loss 0.0030 (0.0108)	
training:	Epoch: [54][342/408]	Loss 0.0027 (0.0108)	
training:	Epoch: [54][343/408]	Loss 0.0038 (0.0108)	
training:	Epoch: [54][344/408]	Loss 0.0078 (0.0108)	
training:	Epoch: [54][345/408]	Loss 0.0017 (0.0107)	
training:	Epoch: [54][346/408]	Loss 0.0024 (0.0107)	
training:	Epoch: [54][347/408]	Loss 0.0024 (0.0107)	
training:	Epoch: [54][348/408]	Loss 0.0019 (0.0107)	
training:	Epoch: [54][349/408]	Loss 0.0030 (0.0106)	
training:	Epoch: [54][350/408]	Loss 0.0023 (0.0106)	
training:	Epoch: [54][351/408]	Loss 0.0062 (0.0106)	
training:	Epoch: [54][352/408]	Loss 0.0025 (0.0106)	
training:	Epoch: [54][353/408]	Loss 0.0015 (0.0106)	
training:	Epoch: [54][354/408]	Loss 0.0088 (0.0106)	
training:	Epoch: [54][355/408]	Loss 0.0018 (0.0105)	
training:	Epoch: [54][356/408]	Loss 0.0022 (0.0105)	
training:	Epoch: [54][357/408]	Loss 0.0034 (0.0105)	
training:	Epoch: [54][358/408]	Loss 0.0021 (0.0105)	
training:	Epoch: [54][359/408]	Loss 0.0018 (0.0104)	
training:	Epoch: [54][360/408]	Loss 0.0016 (0.0104)	
training:	Epoch: [54][361/408]	Loss 0.0024 (0.0104)	
training:	Epoch: [54][362/408]	Loss 0.0019 (0.0104)	
training:	Epoch: [54][363/408]	Loss 0.0041 (0.0104)	
training:	Epoch: [54][364/408]	Loss 0.0017 (0.0103)	
training:	Epoch: [54][365/408]	Loss 0.0018 (0.0103)	
training:	Epoch: [54][366/408]	Loss 0.0048 (0.0103)	
training:	Epoch: [54][367/408]	Loss 0.0041 (0.0103)	
training:	Epoch: [54][368/408]	Loss 0.0042 (0.0103)	
training:	Epoch: [54][369/408]	Loss 0.0018 (0.0102)	
training:	Epoch: [54][370/408]	Loss 0.1577 (0.0106)	
training:	Epoch: [54][371/408]	Loss 0.0032 (0.0106)	
training:	Epoch: [54][372/408]	Loss 0.0021 (0.0106)	
training:	Epoch: [54][373/408]	Loss 0.0039 (0.0106)	
training:	Epoch: [54][374/408]	Loss 0.0099 (0.0106)	
training:	Epoch: [54][375/408]	Loss 0.0025 (0.0105)	
training:	Epoch: [54][376/408]	Loss 0.0018 (0.0105)	
training:	Epoch: [54][377/408]	Loss 0.0018 (0.0105)	
training:	Epoch: [54][378/408]	Loss 0.0018 (0.0105)	
training:	Epoch: [54][379/408]	Loss 0.0029 (0.0105)	
training:	Epoch: [54][380/408]	Loss 0.0026 (0.0104)	
training:	Epoch: [54][381/408]	Loss 0.0076 (0.0104)	
training:	Epoch: [54][382/408]	Loss 0.0031 (0.0104)	
training:	Epoch: [54][383/408]	Loss 0.0031 (0.0104)	
training:	Epoch: [54][384/408]	Loss 0.0089 (0.0104)	
training:	Epoch: [54][385/408]	Loss 0.0020 (0.0104)	
training:	Epoch: [54][386/408]	Loss 0.0019 (0.0103)	
training:	Epoch: [54][387/408]	Loss 0.0027 (0.0103)	
training:	Epoch: [54][388/408]	Loss 0.0017 (0.0103)	
training:	Epoch: [54][389/408]	Loss 0.0017 (0.0103)	
training:	Epoch: [54][390/408]	Loss 0.0016 (0.0103)	
training:	Epoch: [54][391/408]	Loss 0.0033 (0.0102)	
training:	Epoch: [54][392/408]	Loss 0.0042 (0.0102)	
training:	Epoch: [54][393/408]	Loss 0.1233 (0.0105)	
training:	Epoch: [54][394/408]	Loss 0.0016 (0.0105)	
training:	Epoch: [54][395/408]	Loss 0.0015 (0.0105)	
training:	Epoch: [54][396/408]	Loss 0.0021 (0.0104)	
training:	Epoch: [54][397/408]	Loss 0.0020 (0.0104)	
training:	Epoch: [54][398/408]	Loss 0.0027 (0.0104)	
training:	Epoch: [54][399/408]	Loss 0.0172 (0.0104)	
training:	Epoch: [54][400/408]	Loss 0.0025 (0.0104)	
training:	Epoch: [54][401/408]	Loss 0.1398 (0.0107)	
training:	Epoch: [54][402/408]	Loss 0.0021 (0.0107)	
training:	Epoch: [54][403/408]	Loss 0.0040 (0.0107)	
training:	Epoch: [54][404/408]	Loss 0.0021 (0.0107)	
training:	Epoch: [54][405/408]	Loss 0.0036 (0.0106)	
training:	Epoch: [54][406/408]	Loss 0.0032 (0.0106)	
training:	Epoch: [54][407/408]	Loss 0.0015 (0.0106)	
training:	Epoch: [54][408/408]	Loss 0.0023 (0.0106)	
Training:	 Loss: 0.0106

Training:	 ACC: 0.9979 0.9980 0.9997 0.9962
Validation:	 ACC: 0.7785 0.7790 0.7902 0.7668
Validation:	 Best_BACC: 0.8226 0.8250 0.8772 0.7679
Validation:	 Loss: 1.1033
Pretraining:	Epoch 55/200
----------
training:	Epoch: [55][1/408]	Loss 0.0029 (0.0029)	
training:	Epoch: [55][2/408]	Loss 0.0031 (0.0030)	
training:	Epoch: [55][3/408]	Loss 0.0022 (0.0027)	
training:	Epoch: [55][4/408]	Loss 0.0022 (0.0026)	
training:	Epoch: [55][5/408]	Loss 0.0079 (0.0037)	
training:	Epoch: [55][6/408]	Loss 0.0079 (0.0044)	
training:	Epoch: [55][7/408]	Loss 0.0044 (0.0044)	
training:	Epoch: [55][8/408]	Loss 0.0016 (0.0040)	
training:	Epoch: [55][9/408]	Loss 0.0025 (0.0039)	
training:	Epoch: [55][10/408]	Loss 0.0040 (0.0039)	
training:	Epoch: [55][11/408]	Loss 0.0015 (0.0037)	
training:	Epoch: [55][12/408]	Loss 0.0038 (0.0037)	
training:	Epoch: [55][13/408]	Loss 0.0015 (0.0035)	
training:	Epoch: [55][14/408]	Loss 0.0019 (0.0034)	
training:	Epoch: [55][15/408]	Loss 0.0051 (0.0035)	
training:	Epoch: [55][16/408]	Loss 0.0035 (0.0035)	
training:	Epoch: [55][17/408]	Loss 0.0194 (0.0044)	
training:	Epoch: [55][18/408]	Loss 0.0019 (0.0043)	
training:	Epoch: [55][19/408]	Loss 0.0021 (0.0042)	
training:	Epoch: [55][20/408]	Loss 0.0024 (0.0041)	
training:	Epoch: [55][21/408]	Loss 0.0308 (0.0054)	
training:	Epoch: [55][22/408]	Loss 0.0016 (0.0052)	
training:	Epoch: [55][23/408]	Loss 0.0018 (0.0051)	
training:	Epoch: [55][24/408]	Loss 0.0024 (0.0049)	
training:	Epoch: [55][25/408]	Loss 0.0311 (0.0060)	
training:	Epoch: [55][26/408]	Loss 0.0035 (0.0059)	
training:	Epoch: [55][27/408]	Loss 0.0022 (0.0058)	
training:	Epoch: [55][28/408]	Loss 0.0158 (0.0061)	
training:	Epoch: [55][29/408]	Loss 0.0019 (0.0060)	
training:	Epoch: [55][30/408]	Loss 0.0029 (0.0059)	
training:	Epoch: [55][31/408]	Loss 0.0040 (0.0058)	
training:	Epoch: [55][32/408]	Loss 0.0023 (0.0057)	
training:	Epoch: [55][33/408]	Loss 0.0016 (0.0056)	
training:	Epoch: [55][34/408]	Loss 0.0027 (0.0055)	
training:	Epoch: [55][35/408]	Loss 0.0016 (0.0054)	
training:	Epoch: [55][36/408]	Loss 0.0019 (0.0053)	
training:	Epoch: [55][37/408]	Loss 0.0024 (0.0052)	
training:	Epoch: [55][38/408]	Loss 0.0041 (0.0052)	
training:	Epoch: [55][39/408]	Loss 0.0020 (0.0051)	
training:	Epoch: [55][40/408]	Loss 0.1636 (0.0091)	
training:	Epoch: [55][41/408]	Loss 0.0025 (0.0089)	
training:	Epoch: [55][42/408]	Loss 0.0048 (0.0088)	
training:	Epoch: [55][43/408]	Loss 0.0027 (0.0087)	
training:	Epoch: [55][44/408]	Loss 0.0094 (0.0087)	
training:	Epoch: [55][45/408]	Loss 0.0032 (0.0085)	
training:	Epoch: [55][46/408]	Loss 0.0016 (0.0084)	
training:	Epoch: [55][47/408]	Loss 0.0017 (0.0083)	
training:	Epoch: [55][48/408]	Loss 0.0019 (0.0081)	
training:	Epoch: [55][49/408]	Loss 0.0031 (0.0080)	
training:	Epoch: [55][50/408]	Loss 0.0049 (0.0080)	
training:	Epoch: [55][51/408]	Loss 0.0026 (0.0079)	
training:	Epoch: [55][52/408]	Loss 0.0027 (0.0078)	
training:	Epoch: [55][53/408]	Loss 0.0039 (0.0077)	
training:	Epoch: [55][54/408]	Loss 0.0267 (0.0080)	
training:	Epoch: [55][55/408]	Loss 0.0344 (0.0085)	
training:	Epoch: [55][56/408]	Loss 0.0014 (0.0084)	
training:	Epoch: [55][57/408]	Loss 0.0040 (0.0083)	
training:	Epoch: [55][58/408]	Loss 0.0015 (0.0082)	
training:	Epoch: [55][59/408]	Loss 0.0016 (0.0081)	
training:	Epoch: [55][60/408]	Loss 0.0506 (0.0088)	
training:	Epoch: [55][61/408]	Loss 0.0553 (0.0095)	
training:	Epoch: [55][62/408]	Loss 0.0028 (0.0094)	
training:	Epoch: [55][63/408]	Loss 0.0040 (0.0094)	
training:	Epoch: [55][64/408]	Loss 0.0040 (0.0093)	
training:	Epoch: [55][65/408]	Loss 0.0021 (0.0092)	
training:	Epoch: [55][66/408]	Loss 0.1895 (0.0119)	
training:	Epoch: [55][67/408]	Loss 0.0021 (0.0117)	
training:	Epoch: [55][68/408]	Loss 0.0017 (0.0116)	
training:	Epoch: [55][69/408]	Loss 0.0024 (0.0115)	
training:	Epoch: [55][70/408]	Loss 0.0353 (0.0118)	
training:	Epoch: [55][71/408]	Loss 0.0147 (0.0118)	
training:	Epoch: [55][72/408]	Loss 0.0044 (0.0117)	
training:	Epoch: [55][73/408]	Loss 0.0014 (0.0116)	
training:	Epoch: [55][74/408]	Loss 0.0018 (0.0115)	
training:	Epoch: [55][75/408]	Loss 0.0128 (0.0115)	
training:	Epoch: [55][76/408]	Loss 0.0068 (0.0114)	
training:	Epoch: [55][77/408]	Loss 0.0197 (0.0115)	
training:	Epoch: [55][78/408]	Loss 0.0019 (0.0114)	
training:	Epoch: [55][79/408]	Loss 0.0029 (0.0113)	
training:	Epoch: [55][80/408]	Loss 0.0043 (0.0112)	
training:	Epoch: [55][81/408]	Loss 0.0020 (0.0111)	
training:	Epoch: [55][82/408]	Loss 0.0039 (0.0110)	
training:	Epoch: [55][83/408]	Loss 0.0025 (0.0109)	
training:	Epoch: [55][84/408]	Loss 0.0019 (0.0108)	
training:	Epoch: [55][85/408]	Loss 0.0034 (0.0107)	
training:	Epoch: [55][86/408]	Loss 0.0031 (0.0106)	
training:	Epoch: [55][87/408]	Loss 0.0027 (0.0105)	
training:	Epoch: [55][88/408]	Loss 0.0060 (0.0105)	
training:	Epoch: [55][89/408]	Loss 0.0015 (0.0104)	
training:	Epoch: [55][90/408]	Loss 0.0023 (0.0103)	
training:	Epoch: [55][91/408]	Loss 0.0029 (0.0102)	
training:	Epoch: [55][92/408]	Loss 0.0250 (0.0104)	
training:	Epoch: [55][93/408]	Loss 0.0018 (0.0103)	
training:	Epoch: [55][94/408]	Loss 0.0024 (0.0102)	
training:	Epoch: [55][95/408]	Loss 0.0018 (0.0101)	
training:	Epoch: [55][96/408]	Loss 0.0046 (0.0101)	
training:	Epoch: [55][97/408]	Loss 0.0026 (0.0100)	
training:	Epoch: [55][98/408]	Loss 0.0050 (0.0099)	
training:	Epoch: [55][99/408]	Loss 0.0079 (0.0099)	
training:	Epoch: [55][100/408]	Loss 0.0015 (0.0098)	
training:	Epoch: [55][101/408]	Loss 0.0025 (0.0097)	
training:	Epoch: [55][102/408]	Loss 0.0019 (0.0097)	
training:	Epoch: [55][103/408]	Loss 0.0065 (0.0096)	
training:	Epoch: [55][104/408]	Loss 0.0023 (0.0096)	
training:	Epoch: [55][105/408]	Loss 0.0019 (0.0095)	
training:	Epoch: [55][106/408]	Loss 0.0044 (0.0094)	
training:	Epoch: [55][107/408]	Loss 0.0016 (0.0094)	
training:	Epoch: [55][108/408]	Loss 0.0138 (0.0094)	
training:	Epoch: [55][109/408]	Loss 0.0051 (0.0094)	
training:	Epoch: [55][110/408]	Loss 0.0015 (0.0093)	
training:	Epoch: [55][111/408]	Loss 0.0139 (0.0093)	
training:	Epoch: [55][112/408]	Loss 0.0063 (0.0093)	
training:	Epoch: [55][113/408]	Loss 0.0016 (0.0092)	
training:	Epoch: [55][114/408]	Loss 0.0012 (0.0092)	
training:	Epoch: [55][115/408]	Loss 0.0041 (0.0091)	
training:	Epoch: [55][116/408]	Loss 0.0013 (0.0091)	
training:	Epoch: [55][117/408]	Loss 0.0021 (0.0090)	
training:	Epoch: [55][118/408]	Loss 0.0065 (0.0090)	
training:	Epoch: [55][119/408]	Loss 0.0040 (0.0089)	
training:	Epoch: [55][120/408]	Loss 0.0019 (0.0089)	
training:	Epoch: [55][121/408]	Loss 0.0037 (0.0088)	
training:	Epoch: [55][122/408]	Loss 0.0063 (0.0088)	
training:	Epoch: [55][123/408]	Loss 0.0131 (0.0089)	
training:	Epoch: [55][124/408]	Loss 0.0028 (0.0088)	
training:	Epoch: [55][125/408]	Loss 0.0013 (0.0087)	
training:	Epoch: [55][126/408]	Loss 0.0018 (0.0087)	
training:	Epoch: [55][127/408]	Loss 0.0024 (0.0086)	
training:	Epoch: [55][128/408]	Loss 0.0029 (0.0086)	
training:	Epoch: [55][129/408]	Loss 0.0026 (0.0086)	
training:	Epoch: [55][130/408]	Loss 0.0020 (0.0085)	
training:	Epoch: [55][131/408]	Loss 0.0038 (0.0085)	
training:	Epoch: [55][132/408]	Loss 0.0018 (0.0084)	
training:	Epoch: [55][133/408]	Loss 0.0016 (0.0084)	
training:	Epoch: [55][134/408]	Loss 0.0018 (0.0083)	
training:	Epoch: [55][135/408]	Loss 0.0014 (0.0083)	
training:	Epoch: [55][136/408]	Loss 0.0024 (0.0082)	
training:	Epoch: [55][137/408]	Loss 0.0114 (0.0082)	
training:	Epoch: [55][138/408]	Loss 0.0050 (0.0082)	
training:	Epoch: [55][139/408]	Loss 0.0415 (0.0085)	
training:	Epoch: [55][140/408]	Loss 0.0024 (0.0084)	
training:	Epoch: [55][141/408]	Loss 0.0031 (0.0084)	
training:	Epoch: [55][142/408]	Loss 0.0013 (0.0083)	
training:	Epoch: [55][143/408]	Loss 0.0047 (0.0083)	
training:	Epoch: [55][144/408]	Loss 0.0017 (0.0083)	
training:	Epoch: [55][145/408]	Loss 0.0030 (0.0082)	
training:	Epoch: [55][146/408]	Loss 0.0022 (0.0082)	
training:	Epoch: [55][147/408]	Loss 0.0034 (0.0081)	
training:	Epoch: [55][148/408]	Loss 0.0022 (0.0081)	
training:	Epoch: [55][149/408]	Loss 0.0015 (0.0081)	
training:	Epoch: [55][150/408]	Loss 0.0015 (0.0080)	
training:	Epoch: [55][151/408]	Loss 0.0032 (0.0080)	
training:	Epoch: [55][152/408]	Loss 0.0015 (0.0079)	
training:	Epoch: [55][153/408]	Loss 0.0016 (0.0079)	
training:	Epoch: [55][154/408]	Loss 0.0036 (0.0079)	
training:	Epoch: [55][155/408]	Loss 0.0052 (0.0079)	
training:	Epoch: [55][156/408]	Loss 0.0028 (0.0078)	
training:	Epoch: [55][157/408]	Loss 0.0039 (0.0078)	
training:	Epoch: [55][158/408]	Loss 0.0030 (0.0078)	
training:	Epoch: [55][159/408]	Loss 0.0045 (0.0077)	
training:	Epoch: [55][160/408]	Loss 0.0025 (0.0077)	
training:	Epoch: [55][161/408]	Loss 0.0015 (0.0077)	
training:	Epoch: [55][162/408]	Loss 0.0028 (0.0076)	
training:	Epoch: [55][163/408]	Loss 0.0039 (0.0076)	
training:	Epoch: [55][164/408]	Loss 0.0021 (0.0076)	
training:	Epoch: [55][165/408]	Loss 0.0014 (0.0076)	
training:	Epoch: [55][166/408]	Loss 0.0030 (0.0075)	
training:	Epoch: [55][167/408]	Loss 0.0015 (0.0075)	
training:	Epoch: [55][168/408]	Loss 0.0022 (0.0075)	
training:	Epoch: [55][169/408]	Loss 0.0015 (0.0074)	
training:	Epoch: [55][170/408]	Loss 0.0016 (0.0074)	
training:	Epoch: [55][171/408]	Loss 0.0021 (0.0074)	
training:	Epoch: [55][172/408]	Loss 0.0024 (0.0073)	
training:	Epoch: [55][173/408]	Loss 0.0022 (0.0073)	
training:	Epoch: [55][174/408]	Loss 0.0014 (0.0073)	
training:	Epoch: [55][175/408]	Loss 0.0034 (0.0072)	
training:	Epoch: [55][176/408]	Loss 0.0018 (0.0072)	
training:	Epoch: [55][177/408]	Loss 0.0017 (0.0072)	
training:	Epoch: [55][178/408]	Loss 0.0014 (0.0071)	
training:	Epoch: [55][179/408]	Loss 0.0015 (0.0071)	
training:	Epoch: [55][180/408]	Loss 0.0018 (0.0071)	
training:	Epoch: [55][181/408]	Loss 0.0014 (0.0071)	
training:	Epoch: [55][182/408]	Loss 0.0038 (0.0070)	
training:	Epoch: [55][183/408]	Loss 0.0014 (0.0070)	
training:	Epoch: [55][184/408]	Loss 0.0017 (0.0070)	
training:	Epoch: [55][185/408]	Loss 0.0018 (0.0070)	
training:	Epoch: [55][186/408]	Loss 0.0027 (0.0069)	
training:	Epoch: [55][187/408]	Loss 0.0014 (0.0069)	
training:	Epoch: [55][188/408]	Loss 0.0016 (0.0069)	
training:	Epoch: [55][189/408]	Loss 0.0014 (0.0068)	
training:	Epoch: [55][190/408]	Loss 0.0015 (0.0068)	
training:	Epoch: [55][191/408]	Loss 0.0020 (0.0068)	
training:	Epoch: [55][192/408]	Loss 0.0023 (0.0068)	
training:	Epoch: [55][193/408]	Loss 0.0018 (0.0067)	
training:	Epoch: [55][194/408]	Loss 0.0015 (0.0067)	
training:	Epoch: [55][195/408]	Loss 0.0039 (0.0067)	
training:	Epoch: [55][196/408]	Loss 0.0018 (0.0067)	
training:	Epoch: [55][197/408]	Loss 0.0014 (0.0066)	
training:	Epoch: [55][198/408]	Loss 0.0016 (0.0066)	
training:	Epoch: [55][199/408]	Loss 0.0015 (0.0066)	
training:	Epoch: [55][200/408]	Loss 0.0020 (0.0066)	
training:	Epoch: [55][201/408]	Loss 0.0016 (0.0065)	
training:	Epoch: [55][202/408]	Loss 0.0016 (0.0065)	
training:	Epoch: [55][203/408]	Loss 0.0021 (0.0065)	
training:	Epoch: [55][204/408]	Loss 0.0016 (0.0065)	
training:	Epoch: [55][205/408]	Loss 0.0012 (0.0065)	
training:	Epoch: [55][206/408]	Loss 0.1457 (0.0071)	
training:	Epoch: [55][207/408]	Loss 0.0014 (0.0071)	
training:	Epoch: [55][208/408]	Loss 0.0014 (0.0071)	
training:	Epoch: [55][209/408]	Loss 0.0014 (0.0070)	
training:	Epoch: [55][210/408]	Loss 0.0019 (0.0070)	
training:	Epoch: [55][211/408]	Loss 0.0013 (0.0070)	
training:	Epoch: [55][212/408]	Loss 0.0017 (0.0070)	
training:	Epoch: [55][213/408]	Loss 0.0012 (0.0069)	
training:	Epoch: [55][214/408]	Loss 0.0019 (0.0069)	
training:	Epoch: [55][215/408]	Loss 0.0021 (0.0069)	
training:	Epoch: [55][216/408]	Loss 0.1013 (0.0073)	
training:	Epoch: [55][217/408]	Loss 0.0081 (0.0073)	
training:	Epoch: [55][218/408]	Loss 0.0025 (0.0073)	
training:	Epoch: [55][219/408]	Loss 0.0016 (0.0073)	
training:	Epoch: [55][220/408]	Loss 0.0030 (0.0073)	
training:	Epoch: [55][221/408]	Loss 0.0030 (0.0072)	
training:	Epoch: [55][222/408]	Loss 0.0024 (0.0072)	
training:	Epoch: [55][223/408]	Loss 0.0014 (0.0072)	
training:	Epoch: [55][224/408]	Loss 0.0021 (0.0072)	
training:	Epoch: [55][225/408]	Loss 0.0014 (0.0072)	
training:	Epoch: [55][226/408]	Loss 0.0015 (0.0071)	
training:	Epoch: [55][227/408]	Loss 0.0022 (0.0071)	
training:	Epoch: [55][228/408]	Loss 0.0048 (0.0071)	
training:	Epoch: [55][229/408]	Loss 0.0017 (0.0071)	
training:	Epoch: [55][230/408]	Loss 0.1061 (0.0075)	
training:	Epoch: [55][231/408]	Loss 0.0018 (0.0075)	
training:	Epoch: [55][232/408]	Loss 0.0020 (0.0075)	
training:	Epoch: [55][233/408]	Loss 0.0042 (0.0074)	
training:	Epoch: [55][234/408]	Loss 0.0027 (0.0074)	
training:	Epoch: [55][235/408]	Loss 0.0017 (0.0074)	
training:	Epoch: [55][236/408]	Loss 0.0021 (0.0074)	
training:	Epoch: [55][237/408]	Loss 0.0019 (0.0074)	
training:	Epoch: [55][238/408]	Loss 0.0058 (0.0073)	
training:	Epoch: [55][239/408]	Loss 0.0014 (0.0073)	
training:	Epoch: [55][240/408]	Loss 0.0013 (0.0073)	
training:	Epoch: [55][241/408]	Loss 0.0014 (0.0073)	
training:	Epoch: [55][242/408]	Loss 0.0021 (0.0072)	
training:	Epoch: [55][243/408]	Loss 0.0159 (0.0073)	
training:	Epoch: [55][244/408]	Loss 0.0017 (0.0073)	
training:	Epoch: [55][245/408]	Loss 0.0068 (0.0073)	
training:	Epoch: [55][246/408]	Loss 0.0023 (0.0072)	
training:	Epoch: [55][247/408]	Loss 0.0027 (0.0072)	
training:	Epoch: [55][248/408]	Loss 0.0025 (0.0072)	
training:	Epoch: [55][249/408]	Loss 0.0022 (0.0072)	
training:	Epoch: [55][250/408]	Loss 0.0014 (0.0072)	
training:	Epoch: [55][251/408]	Loss 0.0015 (0.0071)	
training:	Epoch: [55][252/408]	Loss 0.0014 (0.0071)	
training:	Epoch: [55][253/408]	Loss 0.0925 (0.0075)	
training:	Epoch: [55][254/408]	Loss 0.0378 (0.0076)	
training:	Epoch: [55][255/408]	Loss 0.0025 (0.0075)	
training:	Epoch: [55][256/408]	Loss 0.0019 (0.0075)	
training:	Epoch: [55][257/408]	Loss 0.0013 (0.0075)	
training:	Epoch: [55][258/408]	Loss 0.0048 (0.0075)	
training:	Epoch: [55][259/408]	Loss 0.0023 (0.0075)	
training:	Epoch: [55][260/408]	Loss 0.0017 (0.0075)	
training:	Epoch: [55][261/408]	Loss 0.0017 (0.0074)	
training:	Epoch: [55][262/408]	Loss 0.0064 (0.0074)	
training:	Epoch: [55][263/408]	Loss 0.0047 (0.0074)	
training:	Epoch: [55][264/408]	Loss 0.0041 (0.0074)	
training:	Epoch: [55][265/408]	Loss 0.0016 (0.0074)	
training:	Epoch: [55][266/408]	Loss 0.0569 (0.0076)	
training:	Epoch: [55][267/408]	Loss 0.0019 (0.0075)	
training:	Epoch: [55][268/408]	Loss 0.0032 (0.0075)	
training:	Epoch: [55][269/408]	Loss 0.0014 (0.0075)	
training:	Epoch: [55][270/408]	Loss 0.0017 (0.0075)	
training:	Epoch: [55][271/408]	Loss 0.0021 (0.0075)	
training:	Epoch: [55][272/408]	Loss 0.0017 (0.0074)	
training:	Epoch: [55][273/408]	Loss 0.0021 (0.0074)	
training:	Epoch: [55][274/408]	Loss 0.0145 (0.0074)	
training:	Epoch: [55][275/408]	Loss 0.0037 (0.0074)	
training:	Epoch: [55][276/408]	Loss 0.0055 (0.0074)	
training:	Epoch: [55][277/408]	Loss 0.0028 (0.0074)	
training:	Epoch: [55][278/408]	Loss 0.0016 (0.0074)	
training:	Epoch: [55][279/408]	Loss 0.0025 (0.0074)	
training:	Epoch: [55][280/408]	Loss 0.0014 (0.0074)	
training:	Epoch: [55][281/408]	Loss 0.0020 (0.0073)	
training:	Epoch: [55][282/408]	Loss 0.0024 (0.0073)	
training:	Epoch: [55][283/408]	Loss 0.0034 (0.0073)	
training:	Epoch: [55][284/408]	Loss 0.0024 (0.0073)	
training:	Epoch: [55][285/408]	Loss 0.0892 (0.0076)	
training:	Epoch: [55][286/408]	Loss 0.0509 (0.0077)	
training:	Epoch: [55][287/408]	Loss 0.0073 (0.0077)	
training:	Epoch: [55][288/408]	Loss 0.0017 (0.0077)	
training:	Epoch: [55][289/408]	Loss 0.0016 (0.0077)	
training:	Epoch: [55][290/408]	Loss 0.0018 (0.0077)	
training:	Epoch: [55][291/408]	Loss 0.0047 (0.0076)	
training:	Epoch: [55][292/408]	Loss 0.0019 (0.0076)	
training:	Epoch: [55][293/408]	Loss 0.0017 (0.0076)	
training:	Epoch: [55][294/408]	Loss 0.0019 (0.0076)	
training:	Epoch: [55][295/408]	Loss 0.0021 (0.0076)	
training:	Epoch: [55][296/408]	Loss 0.0167 (0.0076)	
training:	Epoch: [55][297/408]	Loss 0.0531 (0.0078)	
training:	Epoch: [55][298/408]	Loss 0.0018 (0.0077)	
training:	Epoch: [55][299/408]	Loss 0.0080 (0.0077)	
training:	Epoch: [55][300/408]	Loss 0.0023 (0.0077)	
training:	Epoch: [55][301/408]	Loss 0.0014 (0.0077)	
training:	Epoch: [55][302/408]	Loss 0.0111 (0.0077)	
training:	Epoch: [55][303/408]	Loss 0.0028 (0.0077)	
training:	Epoch: [55][304/408]	Loss 0.0029 (0.0077)	
training:	Epoch: [55][305/408]	Loss 0.0083 (0.0077)	
training:	Epoch: [55][306/408]	Loss 0.0438 (0.0078)	
training:	Epoch: [55][307/408]	Loss 0.1440 (0.0082)	
training:	Epoch: [55][308/408]	Loss 0.0015 (0.0082)	
training:	Epoch: [55][309/408]	Loss 0.0248 (0.0083)	
training:	Epoch: [55][310/408]	Loss 0.0026 (0.0083)	
training:	Epoch: [55][311/408]	Loss 0.0014 (0.0082)	
training:	Epoch: [55][312/408]	Loss 0.0017 (0.0082)	
training:	Epoch: [55][313/408]	Loss 0.0030 (0.0082)	
training:	Epoch: [55][314/408]	Loss 0.0032 (0.0082)	
training:	Epoch: [55][315/408]	Loss 0.0018 (0.0082)	
training:	Epoch: [55][316/408]	Loss 0.0018 (0.0081)	
training:	Epoch: [55][317/408]	Loss 0.0050 (0.0081)	
training:	Epoch: [55][318/408]	Loss 0.0021 (0.0081)	
training:	Epoch: [55][319/408]	Loss 0.0586 (0.0083)	
training:	Epoch: [55][320/408]	Loss 0.0017 (0.0082)	
training:	Epoch: [55][321/408]	Loss 0.0043 (0.0082)	
training:	Epoch: [55][322/408]	Loss 0.0024 (0.0082)	
training:	Epoch: [55][323/408]	Loss 0.0151 (0.0082)	
training:	Epoch: [55][324/408]	Loss 0.0020 (0.0082)	
training:	Epoch: [55][325/408]	Loss 0.0023 (0.0082)	
training:	Epoch: [55][326/408]	Loss 0.0041 (0.0082)	
training:	Epoch: [55][327/408]	Loss 0.0020 (0.0082)	
training:	Epoch: [55][328/408]	Loss 0.0402 (0.0083)	
training:	Epoch: [55][329/408]	Loss 0.0043 (0.0083)	
training:	Epoch: [55][330/408]	Loss 0.0097 (0.0083)	
training:	Epoch: [55][331/408]	Loss 0.0023 (0.0082)	
training:	Epoch: [55][332/408]	Loss 0.0058 (0.0082)	
training:	Epoch: [55][333/408]	Loss 0.0048 (0.0082)	
training:	Epoch: [55][334/408]	Loss 0.0019 (0.0082)	
training:	Epoch: [55][335/408]	Loss 0.0015 (0.0082)	
training:	Epoch: [55][336/408]	Loss 0.0041 (0.0082)	
training:	Epoch: [55][337/408]	Loss 0.0034 (0.0082)	
training:	Epoch: [55][338/408]	Loss 0.0014 (0.0081)	
training:	Epoch: [55][339/408]	Loss 0.0012 (0.0081)	
training:	Epoch: [55][340/408]	Loss 0.0015 (0.0081)	
training:	Epoch: [55][341/408]	Loss 0.0017 (0.0081)	
training:	Epoch: [55][342/408]	Loss 0.0026 (0.0081)	
training:	Epoch: [55][343/408]	Loss 0.0016 (0.0080)	
training:	Epoch: [55][344/408]	Loss 0.0014 (0.0080)	
training:	Epoch: [55][345/408]	Loss 0.0020 (0.0080)	
training:	Epoch: [55][346/408]	Loss 0.0017 (0.0080)	
training:	Epoch: [55][347/408]	Loss 0.0033 (0.0080)	
training:	Epoch: [55][348/408]	Loss 0.0026 (0.0080)	
training:	Epoch: [55][349/408]	Loss 0.0015 (0.0079)	
training:	Epoch: [55][350/408]	Loss 0.0017 (0.0079)	
training:	Epoch: [55][351/408]	Loss 0.0017 (0.0079)	
training:	Epoch: [55][352/408]	Loss 0.0020 (0.0079)	
training:	Epoch: [55][353/408]	Loss 0.0014 (0.0079)	
training:	Epoch: [55][354/408]	Loss 0.0026 (0.0079)	
training:	Epoch: [55][355/408]	Loss 0.0037 (0.0078)	
training:	Epoch: [55][356/408]	Loss 0.0018 (0.0078)	
training:	Epoch: [55][357/408]	Loss 0.0462 (0.0079)	
training:	Epoch: [55][358/408]	Loss 0.0142 (0.0080)	
training:	Epoch: [55][359/408]	Loss 0.0013 (0.0079)	
training:	Epoch: [55][360/408]	Loss 0.0122 (0.0079)	
training:	Epoch: [55][361/408]	Loss 0.0018 (0.0079)	
training:	Epoch: [55][362/408]	Loss 0.0116 (0.0079)	
training:	Epoch: [55][363/408]	Loss 0.0025 (0.0079)	
training:	Epoch: [55][364/408]	Loss 0.0013 (0.0079)	
training:	Epoch: [55][365/408]	Loss 0.0023 (0.0079)	
training:	Epoch: [55][366/408]	Loss 0.0025 (0.0079)	
training:	Epoch: [55][367/408]	Loss 0.0122 (0.0079)	
training:	Epoch: [55][368/408]	Loss 0.0024 (0.0079)	
training:	Epoch: [55][369/408]	Loss 0.0018 (0.0079)	
training:	Epoch: [55][370/408]	Loss 0.0018 (0.0078)	
training:	Epoch: [55][371/408]	Loss 0.0013 (0.0078)	
training:	Epoch: [55][372/408]	Loss 0.0287 (0.0079)	
training:	Epoch: [55][373/408]	Loss 0.0012 (0.0079)	
training:	Epoch: [55][374/408]	Loss 0.0024 (0.0078)	
training:	Epoch: [55][375/408]	Loss 0.0043 (0.0078)	
training:	Epoch: [55][376/408]	Loss 0.0018 (0.0078)	
training:	Epoch: [55][377/408]	Loss 0.0093 (0.0078)	
training:	Epoch: [55][378/408]	Loss 0.0012 (0.0078)	
training:	Epoch: [55][379/408]	Loss 0.0102 (0.0078)	
training:	Epoch: [55][380/408]	Loss 0.0025 (0.0078)	
training:	Epoch: [55][381/408]	Loss 0.0018 (0.0078)	
training:	Epoch: [55][382/408]	Loss 0.0120 (0.0078)	
training:	Epoch: [55][383/408]	Loss 0.0015 (0.0078)	
training:	Epoch: [55][384/408]	Loss 0.0019 (0.0078)	
training:	Epoch: [55][385/408]	Loss 0.0052 (0.0078)	
training:	Epoch: [55][386/408]	Loss 0.0013 (0.0077)	
training:	Epoch: [55][387/408]	Loss 0.1226 (0.0080)	
training:	Epoch: [55][388/408]	Loss 0.0020 (0.0080)	
training:	Epoch: [55][389/408]	Loss 0.0040 (0.0080)	
training:	Epoch: [55][390/408]	Loss 0.0016 (0.0080)	
training:	Epoch: [55][391/408]	Loss 0.0107 (0.0080)	
training:	Epoch: [55][392/408]	Loss 0.0015 (0.0080)	
training:	Epoch: [55][393/408]	Loss 0.0025 (0.0080)	
training:	Epoch: [55][394/408]	Loss 0.0031 (0.0080)	
training:	Epoch: [55][395/408]	Loss 0.0021 (0.0079)	
training:	Epoch: [55][396/408]	Loss 0.0022 (0.0079)	
training:	Epoch: [55][397/408]	Loss 0.0028 (0.0079)	
training:	Epoch: [55][398/408]	Loss 0.0016 (0.0079)	
training:	Epoch: [55][399/408]	Loss 0.0030 (0.0079)	
training:	Epoch: [55][400/408]	Loss 0.0021 (0.0079)	
training:	Epoch: [55][401/408]	Loss 0.0041 (0.0079)	
training:	Epoch: [55][402/408]	Loss 0.0323 (0.0079)	
training:	Epoch: [55][403/408]	Loss 0.1534 (0.0083)	
training:	Epoch: [55][404/408]	Loss 0.0018 (0.0083)	
training:	Epoch: [55][405/408]	Loss 0.0020 (0.0083)	
training:	Epoch: [55][406/408]	Loss 0.0020 (0.0082)	
training:	Epoch: [55][407/408]	Loss 0.0019 (0.0082)	
training:	Epoch: [55][408/408]	Loss 0.0177 (0.0082)	
Training:	 Loss: 0.0082

Training:	 ACC: 0.9986 0.9986 0.9994 0.9978
Validation:	 ACC: 0.7859 0.7838 0.7410 0.8307
Validation:	 Best_BACC: 0.8226 0.8250 0.8772 0.7679
Validation:	 Loss: 1.1176
Pretraining:	Epoch 56/200
----------
training:	Epoch: [56][1/408]	Loss 0.0122 (0.0122)	
training:	Epoch: [56][2/408]	Loss 0.0050 (0.0086)	
training:	Epoch: [56][3/408]	Loss 0.0021 (0.0064)	
training:	Epoch: [56][4/408]	Loss 0.1463 (0.0414)	
training:	Epoch: [56][5/408]	Loss 0.0072 (0.0346)	
training:	Epoch: [56][6/408]	Loss 0.0012 (0.0290)	
training:	Epoch: [56][7/408]	Loss 0.0502 (0.0320)	
training:	Epoch: [56][8/408]	Loss 0.0065 (0.0289)	
training:	Epoch: [56][9/408]	Loss 0.0059 (0.0263)	
training:	Epoch: [56][10/408]	Loss 0.0903 (0.0327)	
training:	Epoch: [56][11/408]	Loss 0.0195 (0.0315)	
training:	Epoch: [56][12/408]	Loss 0.0094 (0.0297)	
training:	Epoch: [56][13/408]	Loss 0.0027 (0.0276)	
training:	Epoch: [56][14/408]	Loss 0.0063 (0.0261)	
training:	Epoch: [56][15/408]	Loss 0.0014 (0.0244)	
training:	Epoch: [56][16/408]	Loss 0.0016 (0.0230)	
training:	Epoch: [56][17/408]	Loss 0.0012 (0.0217)	
training:	Epoch: [56][18/408]	Loss 0.0124 (0.0212)	
training:	Epoch: [56][19/408]	Loss 0.0031 (0.0202)	
training:	Epoch: [56][20/408]	Loss 0.0027 (0.0194)	
training:	Epoch: [56][21/408]	Loss 0.0095 (0.0189)	
training:	Epoch: [56][22/408]	Loss 0.0014 (0.0181)	
training:	Epoch: [56][23/408]	Loss 0.0016 (0.0174)	
training:	Epoch: [56][24/408]	Loss 0.0259 (0.0177)	
training:	Epoch: [56][25/408]	Loss 0.0015 (0.0171)	
training:	Epoch: [56][26/408]	Loss 0.0012 (0.0165)	
training:	Epoch: [56][27/408]	Loss 0.0022 (0.0160)	
training:	Epoch: [56][28/408]	Loss 0.1414 (0.0204)	
training:	Epoch: [56][29/408]	Loss 0.0049 (0.0199)	
training:	Epoch: [56][30/408]	Loss 0.0023 (0.0193)	
training:	Epoch: [56][31/408]	Loss 0.0043 (0.0188)	
training:	Epoch: [56][32/408]	Loss 0.0021 (0.0183)	
training:	Epoch: [56][33/408]	Loss 0.0016 (0.0178)	
training:	Epoch: [56][34/408]	Loss 0.0019 (0.0173)	
training:	Epoch: [56][35/408]	Loss 0.0014 (0.0169)	
training:	Epoch: [56][36/408]	Loss 0.0034 (0.0165)	
training:	Epoch: [56][37/408]	Loss 0.0023 (0.0161)	
training:	Epoch: [56][38/408]	Loss 0.0108 (0.0160)	
training:	Epoch: [56][39/408]	Loss 0.0015 (0.0156)	
training:	Epoch: [56][40/408]	Loss 0.0016 (0.0153)	
training:	Epoch: [56][41/408]	Loss 0.0196 (0.0154)	
training:	Epoch: [56][42/408]	Loss 0.0017 (0.0150)	
training:	Epoch: [56][43/408]	Loss 0.0013 (0.0147)	
training:	Epoch: [56][44/408]	Loss 0.0064 (0.0145)	
training:	Epoch: [56][45/408]	Loss 0.0022 (0.0143)	
training:	Epoch: [56][46/408]	Loss 0.0315 (0.0146)	
training:	Epoch: [56][47/408]	Loss 0.0319 (0.0150)	
training:	Epoch: [56][48/408]	Loss 0.1507 (0.0178)	
training:	Epoch: [56][49/408]	Loss 0.0018 (0.0175)	
training:	Epoch: [56][50/408]	Loss 0.0016 (0.0172)	
training:	Epoch: [56][51/408]	Loss 0.0014 (0.0169)	
training:	Epoch: [56][52/408]	Loss 0.0015 (0.0166)	
training:	Epoch: [56][53/408]	Loss 0.0024 (0.0163)	
training:	Epoch: [56][54/408]	Loss 0.0029 (0.0161)	
training:	Epoch: [56][55/408]	Loss 0.0128 (0.0160)	
training:	Epoch: [56][56/408]	Loss 0.1353 (0.0181)	
training:	Epoch: [56][57/408]	Loss 0.0035 (0.0179)	
training:	Epoch: [56][58/408]	Loss 0.0025 (0.0176)	
training:	Epoch: [56][59/408]	Loss 0.0033 (0.0174)	
training:	Epoch: [56][60/408]	Loss 0.0036 (0.0171)	
training:	Epoch: [56][61/408]	Loss 0.0023 (0.0169)	
training:	Epoch: [56][62/408]	Loss 0.0015 (0.0166)	
training:	Epoch: [56][63/408]	Loss 0.0035 (0.0164)	
training:	Epoch: [56][64/408]	Loss 0.0022 (0.0162)	
training:	Epoch: [56][65/408]	Loss 0.0058 (0.0161)	
training:	Epoch: [56][66/408]	Loss 0.0678 (0.0168)	
training:	Epoch: [56][67/408]	Loss 0.0076 (0.0167)	
training:	Epoch: [56][68/408]	Loss 0.0046 (0.0165)	
training:	Epoch: [56][69/408]	Loss 0.0021 (0.0163)	
training:	Epoch: [56][70/408]	Loss 0.0258 (0.0164)	
training:	Epoch: [56][71/408]	Loss 0.0040 (0.0163)	
training:	Epoch: [56][72/408]	Loss 0.0022 (0.0161)	
training:	Epoch: [56][73/408]	Loss 0.0488 (0.0165)	
training:	Epoch: [56][74/408]	Loss 0.0321 (0.0167)	
training:	Epoch: [56][75/408]	Loss 0.1316 (0.0183)	
training:	Epoch: [56][76/408]	Loss 0.0046 (0.0181)	
training:	Epoch: [56][77/408]	Loss 0.0017 (0.0179)	
training:	Epoch: [56][78/408]	Loss 0.0148 (0.0178)	
training:	Epoch: [56][79/408]	Loss 0.0517 (0.0183)	
training:	Epoch: [56][80/408]	Loss 0.0014 (0.0181)	
training:	Epoch: [56][81/408]	Loss 0.0020 (0.0179)	
training:	Epoch: [56][82/408]	Loss 0.0016 (0.0177)	
training:	Epoch: [56][83/408]	Loss 0.0034 (0.0175)	
training:	Epoch: [56][84/408]	Loss 0.0352 (0.0177)	
training:	Epoch: [56][85/408]	Loss 0.0064 (0.0176)	
training:	Epoch: [56][86/408]	Loss 0.0102 (0.0175)	
training:	Epoch: [56][87/408]	Loss 0.0018 (0.0173)	
training:	Epoch: [56][88/408]	Loss 0.0014 (0.0171)	
training:	Epoch: [56][89/408]	Loss 0.0016 (0.0169)	
training:	Epoch: [56][90/408]	Loss 0.0036 (0.0168)	
training:	Epoch: [56][91/408]	Loss 0.0021 (0.0166)	
training:	Epoch: [56][92/408]	Loss 0.0015 (0.0165)	
training:	Epoch: [56][93/408]	Loss 0.0020 (0.0163)	
training:	Epoch: [56][94/408]	Loss 0.0094 (0.0162)	
training:	Epoch: [56][95/408]	Loss 0.0023 (0.0161)	
training:	Epoch: [56][96/408]	Loss 0.0023 (0.0160)	
training:	Epoch: [56][97/408]	Loss 0.0015 (0.0158)	
training:	Epoch: [56][98/408]	Loss 0.0044 (0.0157)	
training:	Epoch: [56][99/408]	Loss 0.0017 (0.0155)	
training:	Epoch: [56][100/408]	Loss 0.0238 (0.0156)	
training:	Epoch: [56][101/408]	Loss 0.0016 (0.0155)	
training:	Epoch: [56][102/408]	Loss 0.0019 (0.0154)	
training:	Epoch: [56][103/408]	Loss 0.0015 (0.0152)	
training:	Epoch: [56][104/408]	Loss 0.0181 (0.0152)	
training:	Epoch: [56][105/408]	Loss 0.0029 (0.0151)	
training:	Epoch: [56][106/408]	Loss 0.0018 (0.0150)	
training:	Epoch: [56][107/408]	Loss 0.0037 (0.0149)	
training:	Epoch: [56][108/408]	Loss 0.0016 (0.0148)	
training:	Epoch: [56][109/408]	Loss 0.0022 (0.0147)	
training:	Epoch: [56][110/408]	Loss 0.0013 (0.0145)	
training:	Epoch: [56][111/408]	Loss 0.0021 (0.0144)	
training:	Epoch: [56][112/408]	Loss 0.0018 (0.0143)	
training:	Epoch: [56][113/408]	Loss 0.0084 (0.0143)	
training:	Epoch: [56][114/408]	Loss 0.0264 (0.0144)	
training:	Epoch: [56][115/408]	Loss 0.0029 (0.0143)	
training:	Epoch: [56][116/408]	Loss 0.0042 (0.0142)	
training:	Epoch: [56][117/408]	Loss 0.0036 (0.0141)	
training:	Epoch: [56][118/408]	Loss 0.0021 (0.0140)	
training:	Epoch: [56][119/408]	Loss 0.0011 (0.0139)	
training:	Epoch: [56][120/408]	Loss 0.0013 (0.0138)	
training:	Epoch: [56][121/408]	Loss 0.0034 (0.0137)	
training:	Epoch: [56][122/408]	Loss 0.0016 (0.0136)	
training:	Epoch: [56][123/408]	Loss 0.0013 (0.0135)	
training:	Epoch: [56][124/408]	Loss 0.0012 (0.0134)	
training:	Epoch: [56][125/408]	Loss 0.0032 (0.0133)	
training:	Epoch: [56][126/408]	Loss 0.0094 (0.0133)	
training:	Epoch: [56][127/408]	Loss 0.0032 (0.0132)	
training:	Epoch: [56][128/408]	Loss 0.0012 (0.0131)	
training:	Epoch: [56][129/408]	Loss 0.0032 (0.0130)	
training:	Epoch: [56][130/408]	Loss 0.0051 (0.0130)	
training:	Epoch: [56][131/408]	Loss 0.0015 (0.0129)	
training:	Epoch: [56][132/408]	Loss 0.0023 (0.0128)	
training:	Epoch: [56][133/408]	Loss 0.0017 (0.0127)	
training:	Epoch: [56][134/408]	Loss 0.0015 (0.0126)	
training:	Epoch: [56][135/408]	Loss 0.0014 (0.0126)	
training:	Epoch: [56][136/408]	Loss 0.0193 (0.0126)	
training:	Epoch: [56][137/408]	Loss 0.0085 (0.0126)	
training:	Epoch: [56][138/408]	Loss 0.1517 (0.0136)	
training:	Epoch: [56][139/408]	Loss 0.0022 (0.0135)	
training:	Epoch: [56][140/408]	Loss 0.0014 (0.0134)	
training:	Epoch: [56][141/408]	Loss 0.0068 (0.0134)	
training:	Epoch: [56][142/408]	Loss 0.0013 (0.0133)	
training:	Epoch: [56][143/408]	Loss 0.0037 (0.0132)	
training:	Epoch: [56][144/408]	Loss 0.0019 (0.0131)	
training:	Epoch: [56][145/408]	Loss 0.0022 (0.0131)	
training:	Epoch: [56][146/408]	Loss 0.0033 (0.0130)	
training:	Epoch: [56][147/408]	Loss 0.0098 (0.0130)	
training:	Epoch: [56][148/408]	Loss 0.0027 (0.0129)	
training:	Epoch: [56][149/408]	Loss 0.0018 (0.0128)	
training:	Epoch: [56][150/408]	Loss 0.0068 (0.0128)	
training:	Epoch: [56][151/408]	Loss 0.0029 (0.0127)	
training:	Epoch: [56][152/408]	Loss 0.0013 (0.0126)	
training:	Epoch: [56][153/408]	Loss 0.0016 (0.0126)	
training:	Epoch: [56][154/408]	Loss 0.0044 (0.0125)	
training:	Epoch: [56][155/408]	Loss 0.0320 (0.0126)	
training:	Epoch: [56][156/408]	Loss 0.0034 (0.0126)	
training:	Epoch: [56][157/408]	Loss 0.0023 (0.0125)	
training:	Epoch: [56][158/408]	Loss 0.0018 (0.0125)	
training:	Epoch: [56][159/408]	Loss 0.0015 (0.0124)	
training:	Epoch: [56][160/408]	Loss 0.0034 (0.0123)	
training:	Epoch: [56][161/408]	Loss 0.0013 (0.0123)	
training:	Epoch: [56][162/408]	Loss 0.0023 (0.0122)	
training:	Epoch: [56][163/408]	Loss 0.0030 (0.0121)	
training:	Epoch: [56][164/408]	Loss 0.0017 (0.0121)	
training:	Epoch: [56][165/408]	Loss 0.0014 (0.0120)	
training:	Epoch: [56][166/408]	Loss 0.0020 (0.0119)	
training:	Epoch: [56][167/408]	Loss 0.0021 (0.0119)	
training:	Epoch: [56][168/408]	Loss 0.0013 (0.0118)	
training:	Epoch: [56][169/408]	Loss 0.0020 (0.0118)	
training:	Epoch: [56][170/408]	Loss 0.0014 (0.0117)	
training:	Epoch: [56][171/408]	Loss 0.0136 (0.0117)	
training:	Epoch: [56][172/408]	Loss 0.0020 (0.0117)	
training:	Epoch: [56][173/408]	Loss 0.0020 (0.0116)	
training:	Epoch: [56][174/408]	Loss 0.0086 (0.0116)	
training:	Epoch: [56][175/408]	Loss 0.0021 (0.0115)	
training:	Epoch: [56][176/408]	Loss 0.0020 (0.0115)	
training:	Epoch: [56][177/408]	Loss 0.0019 (0.0114)	
training:	Epoch: [56][178/408]	Loss 0.0082 (0.0114)	
training:	Epoch: [56][179/408]	Loss 0.0017 (0.0114)	
training:	Epoch: [56][180/408]	Loss 0.0020 (0.0113)	
training:	Epoch: [56][181/408]	Loss 0.0022 (0.0113)	
training:	Epoch: [56][182/408]	Loss 0.0015 (0.0112)	
training:	Epoch: [56][183/408]	Loss 0.0027 (0.0112)	
training:	Epoch: [56][184/408]	Loss 0.0014 (0.0111)	
training:	Epoch: [56][185/408]	Loss 0.0030 (0.0111)	
training:	Epoch: [56][186/408]	Loss 0.0093 (0.0110)	
training:	Epoch: [56][187/408]	Loss 0.0012 (0.0110)	
training:	Epoch: [56][188/408]	Loss 0.0061 (0.0110)	
training:	Epoch: [56][189/408]	Loss 0.0019 (0.0109)	
training:	Epoch: [56][190/408]	Loss 0.0018 (0.0109)	
training:	Epoch: [56][191/408]	Loss 0.0016 (0.0108)	
training:	Epoch: [56][192/408]	Loss 0.0012 (0.0108)	
training:	Epoch: [56][193/408]	Loss 0.0042 (0.0107)	
training:	Epoch: [56][194/408]	Loss 0.0015 (0.0107)	
training:	Epoch: [56][195/408]	Loss 0.0037 (0.0107)	
training:	Epoch: [56][196/408]	Loss 0.0013 (0.0106)	
training:	Epoch: [56][197/408]	Loss 0.0012 (0.0106)	
training:	Epoch: [56][198/408]	Loss 0.0013 (0.0105)	
training:	Epoch: [56][199/408]	Loss 0.0014 (0.0105)	
training:	Epoch: [56][200/408]	Loss 0.0013 (0.0104)	
training:	Epoch: [56][201/408]	Loss 0.0044 (0.0104)	
training:	Epoch: [56][202/408]	Loss 0.0015 (0.0103)	
training:	Epoch: [56][203/408]	Loss 0.0043 (0.0103)	
training:	Epoch: [56][204/408]	Loss 0.0014 (0.0103)	
training:	Epoch: [56][205/408]	Loss 0.0022 (0.0102)	
training:	Epoch: [56][206/408]	Loss 0.0040 (0.0102)	
training:	Epoch: [56][207/408]	Loss 0.0020 (0.0102)	
training:	Epoch: [56][208/408]	Loss 0.0038 (0.0101)	
training:	Epoch: [56][209/408]	Loss 0.0061 (0.0101)	
training:	Epoch: [56][210/408]	Loss 0.0025 (0.0101)	
training:	Epoch: [56][211/408]	Loss 0.0020 (0.0100)	
training:	Epoch: [56][212/408]	Loss 0.0016 (0.0100)	
training:	Epoch: [56][213/408]	Loss 0.0016 (0.0100)	
training:	Epoch: [56][214/408]	Loss 0.0026 (0.0099)	
training:	Epoch: [56][215/408]	Loss 0.0014 (0.0099)	
training:	Epoch: [56][216/408]	Loss 0.0032 (0.0099)	
training:	Epoch: [56][217/408]	Loss 0.0016 (0.0098)	
training:	Epoch: [56][218/408]	Loss 0.0021 (0.0098)	
training:	Epoch: [56][219/408]	Loss 0.0068 (0.0098)	
training:	Epoch: [56][220/408]	Loss 0.0015 (0.0097)	
training:	Epoch: [56][221/408]	Loss 0.0017 (0.0097)	
training:	Epoch: [56][222/408]	Loss 0.0026 (0.0097)	
training:	Epoch: [56][223/408]	Loss 0.0022 (0.0096)	
training:	Epoch: [56][224/408]	Loss 0.0014 (0.0096)	
training:	Epoch: [56][225/408]	Loss 0.0011 (0.0096)	
training:	Epoch: [56][226/408]	Loss 0.0039 (0.0095)	
training:	Epoch: [56][227/408]	Loss 0.0013 (0.0095)	
training:	Epoch: [56][228/408]	Loss 0.0014 (0.0095)	
training:	Epoch: [56][229/408]	Loss 0.0013 (0.0094)	
training:	Epoch: [56][230/408]	Loss 0.0051 (0.0094)	
training:	Epoch: [56][231/408]	Loss 0.0018 (0.0094)	
training:	Epoch: [56][232/408]	Loss 0.0050 (0.0094)	
training:	Epoch: [56][233/408]	Loss 0.0018 (0.0093)	
training:	Epoch: [56][234/408]	Loss 0.0020 (0.0093)	
training:	Epoch: [56][235/408]	Loss 0.0017 (0.0093)	
training:	Epoch: [56][236/408]	Loss 0.0013 (0.0092)	
training:	Epoch: [56][237/408]	Loss 0.0432 (0.0094)	
training:	Epoch: [56][238/408]	Loss 0.0014 (0.0093)	
training:	Epoch: [56][239/408]	Loss 0.0029 (0.0093)	
training:	Epoch: [56][240/408]	Loss 0.0019 (0.0093)	
training:	Epoch: [56][241/408]	Loss 0.0512 (0.0094)	
training:	Epoch: [56][242/408]	Loss 0.0015 (0.0094)	
training:	Epoch: [56][243/408]	Loss 0.0016 (0.0094)	
training:	Epoch: [56][244/408]	Loss 0.0011 (0.0093)	
training:	Epoch: [56][245/408]	Loss 0.0012 (0.0093)	
training:	Epoch: [56][246/408]	Loss 0.0475 (0.0095)	
training:	Epoch: [56][247/408]	Loss 0.0019 (0.0094)	
training:	Epoch: [56][248/408]	Loss 0.0019 (0.0094)	
training:	Epoch: [56][249/408]	Loss 0.0021 (0.0094)	
training:	Epoch: [56][250/408]	Loss 0.0303 (0.0095)	
training:	Epoch: [56][251/408]	Loss 0.0031 (0.0094)	
training:	Epoch: [56][252/408]	Loss 0.0012 (0.0094)	
training:	Epoch: [56][253/408]	Loss 0.0041 (0.0094)	
training:	Epoch: [56][254/408]	Loss 0.0015 (0.0094)	
training:	Epoch: [56][255/408]	Loss 0.0459 (0.0095)	
training:	Epoch: [56][256/408]	Loss 0.0012 (0.0095)	
training:	Epoch: [56][257/408]	Loss 0.0012 (0.0094)	
training:	Epoch: [56][258/408]	Loss 0.0438 (0.0096)	
training:	Epoch: [56][259/408]	Loss 0.0015 (0.0095)	
training:	Epoch: [56][260/408]	Loss 0.0014 (0.0095)	
training:	Epoch: [56][261/408]	Loss 0.0027 (0.0095)	
training:	Epoch: [56][262/408]	Loss 0.0017 (0.0094)	
training:	Epoch: [56][263/408]	Loss 0.0017 (0.0094)	
training:	Epoch: [56][264/408]	Loss 0.0014 (0.0094)	
training:	Epoch: [56][265/408]	Loss 0.0015 (0.0094)	
training:	Epoch: [56][266/408]	Loss 0.0019 (0.0093)	
training:	Epoch: [56][267/408]	Loss 0.0088 (0.0093)	
training:	Epoch: [56][268/408]	Loss 0.0190 (0.0094)	
training:	Epoch: [56][269/408]	Loss 0.0019 (0.0093)	
training:	Epoch: [56][270/408]	Loss 0.0028 (0.0093)	
training:	Epoch: [56][271/408]	Loss 0.0015 (0.0093)	
training:	Epoch: [56][272/408]	Loss 0.0082 (0.0093)	
training:	Epoch: [56][273/408]	Loss 0.0027 (0.0093)	
training:	Epoch: [56][274/408]	Loss 0.0020 (0.0092)	
training:	Epoch: [56][275/408]	Loss 0.0015 (0.0092)	
training:	Epoch: [56][276/408]	Loss 0.0018 (0.0092)	
training:	Epoch: [56][277/408]	Loss 0.0018 (0.0091)	
training:	Epoch: [56][278/408]	Loss 0.0039 (0.0091)	
training:	Epoch: [56][279/408]	Loss 0.0316 (0.0092)	
training:	Epoch: [56][280/408]	Loss 0.0021 (0.0092)	
training:	Epoch: [56][281/408]	Loss 0.0023 (0.0092)	
training:	Epoch: [56][282/408]	Loss 0.0258 (0.0092)	
training:	Epoch: [56][283/408]	Loss 0.0183 (0.0093)	
training:	Epoch: [56][284/408]	Loss 0.0012 (0.0092)	
training:	Epoch: [56][285/408]	Loss 0.0021 (0.0092)	
training:	Epoch: [56][286/408]	Loss 0.0015 (0.0092)	
training:	Epoch: [56][287/408]	Loss 0.0025 (0.0091)	
training:	Epoch: [56][288/408]	Loss 0.0016 (0.0091)	
training:	Epoch: [56][289/408]	Loss 0.1144 (0.0095)	
training:	Epoch: [56][290/408]	Loss 0.0014 (0.0095)	
training:	Epoch: [56][291/408]	Loss 0.0016 (0.0094)	
training:	Epoch: [56][292/408]	Loss 0.0024 (0.0094)	
training:	Epoch: [56][293/408]	Loss 0.0014 (0.0094)	
training:	Epoch: [56][294/408]	Loss 0.0015 (0.0094)	
training:	Epoch: [56][295/408]	Loss 0.0013 (0.0093)	
training:	Epoch: [56][296/408]	Loss 0.0024 (0.0093)	
training:	Epoch: [56][297/408]	Loss 0.0033 (0.0093)	
training:	Epoch: [56][298/408]	Loss 0.0029 (0.0093)	
training:	Epoch: [56][299/408]	Loss 0.0014 (0.0092)	
training:	Epoch: [56][300/408]	Loss 0.0387 (0.0093)	
training:	Epoch: [56][301/408]	Loss 0.0254 (0.0094)	
training:	Epoch: [56][302/408]	Loss 0.0024 (0.0094)	
training:	Epoch: [56][303/408]	Loss 0.0334 (0.0094)	
training:	Epoch: [56][304/408]	Loss 0.0016 (0.0094)	
training:	Epoch: [56][305/408]	Loss 0.0019 (0.0094)	
training:	Epoch: [56][306/408]	Loss 0.0017 (0.0094)	
training:	Epoch: [56][307/408]	Loss 0.0131 (0.0094)	
training:	Epoch: [56][308/408]	Loss 0.0053 (0.0094)	
training:	Epoch: [56][309/408]	Loss 0.0012 (0.0093)	
training:	Epoch: [56][310/408]	Loss 0.0013 (0.0093)	
training:	Epoch: [56][311/408]	Loss 0.0038 (0.0093)	
training:	Epoch: [56][312/408]	Loss 0.0017 (0.0093)	
training:	Epoch: [56][313/408]	Loss 0.0014 (0.0092)	
training:	Epoch: [56][314/408]	Loss 0.0014 (0.0092)	
training:	Epoch: [56][315/408]	Loss 0.0024 (0.0092)	
training:	Epoch: [56][316/408]	Loss 0.0022 (0.0092)	
training:	Epoch: [56][317/408]	Loss 0.0098 (0.0092)	
training:	Epoch: [56][318/408]	Loss 0.0040 (0.0092)	
training:	Epoch: [56][319/408]	Loss 0.0012 (0.0091)	
training:	Epoch: [56][320/408]	Loss 0.0074 (0.0091)	
training:	Epoch: [56][321/408]	Loss 0.0042 (0.0091)	
training:	Epoch: [56][322/408]	Loss 0.0042 (0.0091)	
training:	Epoch: [56][323/408]	Loss 0.0044 (0.0091)	
training:	Epoch: [56][324/408]	Loss 0.0012 (0.0091)	
training:	Epoch: [56][325/408]	Loss 0.0108 (0.0091)	
training:	Epoch: [56][326/408]	Loss 0.0023 (0.0090)	
training:	Epoch: [56][327/408]	Loss 0.0103 (0.0091)	
training:	Epoch: [56][328/408]	Loss 0.0121 (0.0091)	
training:	Epoch: [56][329/408]	Loss 0.0025 (0.0090)	
training:	Epoch: [56][330/408]	Loss 0.0073 (0.0090)	
training:	Epoch: [56][331/408]	Loss 0.0028 (0.0090)	
training:	Epoch: [56][332/408]	Loss 0.0013 (0.0090)	
training:	Epoch: [56][333/408]	Loss 0.0019 (0.0090)	
training:	Epoch: [56][334/408]	Loss 0.0014 (0.0089)	
training:	Epoch: [56][335/408]	Loss 0.0022 (0.0089)	
training:	Epoch: [56][336/408]	Loss 0.0016 (0.0089)	
training:	Epoch: [56][337/408]	Loss 0.0028 (0.0089)	
training:	Epoch: [56][338/408]	Loss 0.0013 (0.0089)	
training:	Epoch: [56][339/408]	Loss 0.0018 (0.0088)	
training:	Epoch: [56][340/408]	Loss 0.0012 (0.0088)	
training:	Epoch: [56][341/408]	Loss 0.0011 (0.0088)	
training:	Epoch: [56][342/408]	Loss 0.0043 (0.0088)	
training:	Epoch: [56][343/408]	Loss 0.0014 (0.0088)	
training:	Epoch: [56][344/408]	Loss 0.0012 (0.0087)	
training:	Epoch: [56][345/408]	Loss 0.0017 (0.0087)	
training:	Epoch: [56][346/408]	Loss 0.0031 (0.0087)	
training:	Epoch: [56][347/408]	Loss 0.0013 (0.0087)	
training:	Epoch: [56][348/408]	Loss 0.0013 (0.0087)	
training:	Epoch: [56][349/408]	Loss 0.0016 (0.0086)	
training:	Epoch: [56][350/408]	Loss 0.0014 (0.0086)	
training:	Epoch: [56][351/408]	Loss 0.0018 (0.0086)	
training:	Epoch: [56][352/408]	Loss 0.0012 (0.0086)	
training:	Epoch: [56][353/408]	Loss 0.0022 (0.0086)	
training:	Epoch: [56][354/408]	Loss 0.0400 (0.0087)	
training:	Epoch: [56][355/408]	Loss 0.0012 (0.0086)	
training:	Epoch: [56][356/408]	Loss 0.0017 (0.0086)	
training:	Epoch: [56][357/408]	Loss 0.0013 (0.0086)	
training:	Epoch: [56][358/408]	Loss 0.0012 (0.0086)	
training:	Epoch: [56][359/408]	Loss 0.0039 (0.0086)	
training:	Epoch: [56][360/408]	Loss 0.0019 (0.0085)	
training:	Epoch: [56][361/408]	Loss 0.0013 (0.0085)	
training:	Epoch: [56][362/408]	Loss 0.0012 (0.0085)	
training:	Epoch: [56][363/408]	Loss 0.0025 (0.0085)	
training:	Epoch: [56][364/408]	Loss 0.0030 (0.0085)	
training:	Epoch: [56][365/408]	Loss 0.0069 (0.0085)	
training:	Epoch: [56][366/408]	Loss 0.0095 (0.0085)	
training:	Epoch: [56][367/408]	Loss 0.0020 (0.0084)	
training:	Epoch: [56][368/408]	Loss 0.0010 (0.0084)	
training:	Epoch: [56][369/408]	Loss 0.0055 (0.0084)	
training:	Epoch: [56][370/408]	Loss 0.0049 (0.0084)	
training:	Epoch: [56][371/408]	Loss 0.0013 (0.0084)	
training:	Epoch: [56][372/408]	Loss 0.0106 (0.0084)	
training:	Epoch: [56][373/408]	Loss 0.0026 (0.0084)	
training:	Epoch: [56][374/408]	Loss 0.0011 (0.0084)	
training:	Epoch: [56][375/408]	Loss 0.0012 (0.0083)	
training:	Epoch: [56][376/408]	Loss 0.1507 (0.0087)	
training:	Epoch: [56][377/408]	Loss 0.0012 (0.0087)	
training:	Epoch: [56][378/408]	Loss 0.0017 (0.0087)	
training:	Epoch: [56][379/408]	Loss 0.0030 (0.0087)	
training:	Epoch: [56][380/408]	Loss 0.0018 (0.0087)	
training:	Epoch: [56][381/408]	Loss 0.0014 (0.0086)	
training:	Epoch: [56][382/408]	Loss 0.0143 (0.0086)	
training:	Epoch: [56][383/408]	Loss 0.0063 (0.0086)	
training:	Epoch: [56][384/408]	Loss 0.0022 (0.0086)	
training:	Epoch: [56][385/408]	Loss 0.0012 (0.0086)	
training:	Epoch: [56][386/408]	Loss 0.0026 (0.0086)	
training:	Epoch: [56][387/408]	Loss 0.0017 (0.0086)	
training:	Epoch: [56][388/408]	Loss 0.0062 (0.0086)	
training:	Epoch: [56][389/408]	Loss 0.0052 (0.0086)	
training:	Epoch: [56][390/408]	Loss 0.0013 (0.0085)	
training:	Epoch: [56][391/408]	Loss 0.0010 (0.0085)	
training:	Epoch: [56][392/408]	Loss 0.0015 (0.0085)	
training:	Epoch: [56][393/408]	Loss 0.0015 (0.0085)	
training:	Epoch: [56][394/408]	Loss 0.0012 (0.0085)	
training:	Epoch: [56][395/408]	Loss 0.0014 (0.0084)	
training:	Epoch: [56][396/408]	Loss 0.0095 (0.0084)	
training:	Epoch: [56][397/408]	Loss 0.0076 (0.0084)	
training:	Epoch: [56][398/408]	Loss 0.0020 (0.0084)	
training:	Epoch: [56][399/408]	Loss 0.0020 (0.0084)	
training:	Epoch: [56][400/408]	Loss 0.0022 (0.0084)	
training:	Epoch: [56][401/408]	Loss 0.0025 (0.0084)	
training:	Epoch: [56][402/408]	Loss 0.0015 (0.0084)	
training:	Epoch: [56][403/408]	Loss 0.0035 (0.0084)	
training:	Epoch: [56][404/408]	Loss 0.0014 (0.0083)	
training:	Epoch: [56][405/408]	Loss 0.0010 (0.0083)	
training:	Epoch: [56][406/408]	Loss 0.0011 (0.0083)	
training:	Epoch: [56][407/408]	Loss 0.0015 (0.0083)	
training:	Epoch: [56][408/408]	Loss 0.0012 (0.0083)	
Training:	 Loss: 0.0083

Training:	 ACC: 0.9994 0.9994 0.9997 0.9990
Validation:	 ACC: 0.7851 0.7860 0.8035 0.7668
Validation:	 Best_BACC: 0.8226 0.8250 0.8772 0.7679
Validation:	 Loss: 1.1338
Pretraining:	Epoch 57/200
----------
training:	Epoch: [57][1/408]	Loss 0.0012 (0.0012)	
training:	Epoch: [57][2/408]	Loss 0.0013 (0.0013)	
training:	Epoch: [57][3/408]	Loss 0.0016 (0.0014)	
training:	Epoch: [57][4/408]	Loss 0.0019 (0.0015)	
training:	Epoch: [57][5/408]	Loss 0.0010 (0.0014)	
training:	Epoch: [57][6/408]	Loss 0.0014 (0.0014)	
training:	Epoch: [57][7/408]	Loss 0.0011 (0.0014)	
training:	Epoch: [57][8/408]	Loss 0.0134 (0.0029)	
training:	Epoch: [57][9/408]	Loss 0.0022 (0.0028)	
training:	Epoch: [57][10/408]	Loss 0.0179 (0.0043)	
training:	Epoch: [57][11/408]	Loss 0.0014 (0.0040)	
training:	Epoch: [57][12/408]	Loss 0.0013 (0.0038)	
training:	Epoch: [57][13/408]	Loss 0.0014 (0.0036)	
training:	Epoch: [57][14/408]	Loss 0.0014 (0.0035)	
training:	Epoch: [57][15/408]	Loss 0.0021 (0.0034)	
training:	Epoch: [57][16/408]	Loss 0.0013 (0.0033)	
training:	Epoch: [57][17/408]	Loss 0.0019 (0.0032)	
training:	Epoch: [57][18/408]	Loss 0.0015 (0.0031)	
training:	Epoch: [57][19/408]	Loss 0.0019 (0.0030)	
training:	Epoch: [57][20/408]	Loss 0.0039 (0.0031)	
training:	Epoch: [57][21/408]	Loss 0.0016 (0.0030)	
training:	Epoch: [57][22/408]	Loss 0.0011 (0.0029)	
training:	Epoch: [57][23/408]	Loss 0.0017 (0.0029)	
training:	Epoch: [57][24/408]	Loss 0.0014 (0.0028)	
training:	Epoch: [57][25/408]	Loss 0.0017 (0.0028)	
training:	Epoch: [57][26/408]	Loss 0.0022 (0.0027)	
training:	Epoch: [57][27/408]	Loss 0.0012 (0.0027)	
training:	Epoch: [57][28/408]	Loss 0.0013 (0.0026)	
training:	Epoch: [57][29/408]	Loss 0.0062 (0.0027)	
training:	Epoch: [57][30/408]	Loss 0.0015 (0.0027)	
training:	Epoch: [57][31/408]	Loss 0.0011 (0.0027)	
training:	Epoch: [57][32/408]	Loss 0.0011 (0.0026)	
training:	Epoch: [57][33/408]	Loss 0.0015 (0.0026)	
training:	Epoch: [57][34/408]	Loss 0.0024 (0.0026)	
training:	Epoch: [57][35/408]	Loss 0.0012 (0.0025)	
training:	Epoch: [57][36/408]	Loss 0.0019 (0.0025)	
training:	Epoch: [57][37/408]	Loss 0.0011 (0.0025)	
training:	Epoch: [57][38/408]	Loss 0.0013 (0.0024)	
training:	Epoch: [57][39/408]	Loss 0.0034 (0.0025)	
training:	Epoch: [57][40/408]	Loss 0.0016 (0.0024)	
training:	Epoch: [57][41/408]	Loss 0.0011 (0.0024)	
training:	Epoch: [57][42/408]	Loss 0.0020 (0.0024)	
training:	Epoch: [57][43/408]	Loss 0.0011 (0.0024)	
training:	Epoch: [57][44/408]	Loss 0.0012 (0.0023)	
training:	Epoch: [57][45/408]	Loss 0.0018 (0.0023)	
training:	Epoch: [57][46/408]	Loss 0.0013 (0.0023)	
training:	Epoch: [57][47/408]	Loss 0.0306 (0.0029)	
training:	Epoch: [57][48/408]	Loss 0.0013 (0.0029)	
training:	Epoch: [57][49/408]	Loss 0.0011 (0.0028)	
training:	Epoch: [57][50/408]	Loss 0.0017 (0.0028)	
training:	Epoch: [57][51/408]	Loss 0.0014 (0.0028)	
training:	Epoch: [57][52/408]	Loss 0.0065 (0.0029)	
training:	Epoch: [57][53/408]	Loss 0.0187 (0.0032)	
training:	Epoch: [57][54/408]	Loss 0.0021 (0.0031)	
training:	Epoch: [57][55/408]	Loss 0.0037 (0.0032)	
training:	Epoch: [57][56/408]	Loss 0.0023 (0.0031)	
training:	Epoch: [57][57/408]	Loss 0.0022 (0.0031)	
training:	Epoch: [57][58/408]	Loss 0.0042 (0.0031)	
training:	Epoch: [57][59/408]	Loss 0.0012 (0.0031)	
training:	Epoch: [57][60/408]	Loss 0.0013 (0.0031)	
training:	Epoch: [57][61/408]	Loss 0.0011 (0.0030)	
training:	Epoch: [57][62/408]	Loss 0.0013 (0.0030)	
training:	Epoch: [57][63/408]	Loss 0.0144 (0.0032)	
training:	Epoch: [57][64/408]	Loss 0.0014 (0.0032)	
training:	Epoch: [57][65/408]	Loss 0.0013 (0.0031)	
training:	Epoch: [57][66/408]	Loss 0.0010 (0.0031)	
training:	Epoch: [57][67/408]	Loss 0.0015 (0.0031)	
training:	Epoch: [57][68/408]	Loss 0.0014 (0.0031)	
training:	Epoch: [57][69/408]	Loss 0.0022 (0.0031)	
training:	Epoch: [57][70/408]	Loss 0.0015 (0.0030)	
training:	Epoch: [57][71/408]	Loss 0.0016 (0.0030)	
training:	Epoch: [57][72/408]	Loss 0.0013 (0.0030)	
training:	Epoch: [57][73/408]	Loss 0.0012 (0.0030)	
training:	Epoch: [57][74/408]	Loss 0.0013 (0.0029)	
training:	Epoch: [57][75/408]	Loss 0.0012 (0.0029)	
training:	Epoch: [57][76/408]	Loss 0.0019 (0.0029)	
training:	Epoch: [57][77/408]	Loss 0.0012 (0.0029)	
training:	Epoch: [57][78/408]	Loss 0.0014 (0.0029)	
training:	Epoch: [57][79/408]	Loss 0.0012 (0.0028)	
training:	Epoch: [57][80/408]	Loss 0.0012 (0.0028)	
training:	Epoch: [57][81/408]	Loss 0.0021 (0.0028)	
training:	Epoch: [57][82/408]	Loss 0.0017 (0.0028)	
training:	Epoch: [57][83/408]	Loss 0.0012 (0.0028)	
training:	Epoch: [57][84/408]	Loss 0.0012 (0.0028)	
training:	Epoch: [57][85/408]	Loss 0.0017 (0.0027)	
training:	Epoch: [57][86/408]	Loss 0.0012 (0.0027)	
training:	Epoch: [57][87/408]	Loss 0.0015 (0.0027)	
training:	Epoch: [57][88/408]	Loss 0.0017 (0.0027)	
training:	Epoch: [57][89/408]	Loss 0.0011 (0.0027)	
training:	Epoch: [57][90/408]	Loss 0.0147 (0.0028)	
training:	Epoch: [57][91/408]	Loss 0.0015 (0.0028)	
training:	Epoch: [57][92/408]	Loss 0.0014 (0.0028)	
training:	Epoch: [57][93/408]	Loss 0.0014 (0.0028)	
training:	Epoch: [57][94/408]	Loss 0.0011 (0.0028)	
training:	Epoch: [57][95/408]	Loss 0.0013 (0.0027)	
training:	Epoch: [57][96/408]	Loss 0.0021 (0.0027)	
training:	Epoch: [57][97/408]	Loss 0.0012 (0.0027)	
training:	Epoch: [57][98/408]	Loss 0.0018 (0.0027)	
training:	Epoch: [57][99/408]	Loss 0.0022 (0.0027)	
training:	Epoch: [57][100/408]	Loss 0.0015 (0.0027)	
training:	Epoch: [57][101/408]	Loss 0.0012 (0.0027)	
training:	Epoch: [57][102/408]	Loss 0.0012 (0.0027)	
training:	Epoch: [57][103/408]	Loss 0.0026 (0.0027)	
training:	Epoch: [57][104/408]	Loss 0.0012 (0.0026)	
training:	Epoch: [57][105/408]	Loss 0.0029 (0.0027)	
training:	Epoch: [57][106/408]	Loss 0.0012 (0.0026)	
training:	Epoch: [57][107/408]	Loss 0.0018 (0.0026)	
training:	Epoch: [57][108/408]	Loss 0.0014 (0.0026)	
training:	Epoch: [57][109/408]	Loss 0.0012 (0.0026)	
training:	Epoch: [57][110/408]	Loss 0.0018 (0.0026)	
training:	Epoch: [57][111/408]	Loss 0.0012 (0.0026)	
training:	Epoch: [57][112/408]	Loss 0.0018 (0.0026)	
training:	Epoch: [57][113/408]	Loss 0.0013 (0.0026)	
training:	Epoch: [57][114/408]	Loss 0.0013 (0.0026)	
training:	Epoch: [57][115/408]	Loss 0.0014 (0.0025)	
training:	Epoch: [57][116/408]	Loss 0.0012 (0.0025)	
training:	Epoch: [57][117/408]	Loss 0.0018 (0.0025)	
training:	Epoch: [57][118/408]	Loss 0.0013 (0.0025)	
training:	Epoch: [57][119/408]	Loss 0.0014 (0.0025)	
training:	Epoch: [57][120/408]	Loss 0.0106 (0.0026)	
training:	Epoch: [57][121/408]	Loss 0.0016 (0.0026)	
training:	Epoch: [57][122/408]	Loss 0.0013 (0.0026)	
training:	Epoch: [57][123/408]	Loss 0.0024 (0.0026)	
training:	Epoch: [57][124/408]	Loss 0.0011 (0.0025)	
training:	Epoch: [57][125/408]	Loss 0.0014 (0.0025)	
training:	Epoch: [57][126/408]	Loss 0.0014 (0.0025)	
training:	Epoch: [57][127/408]	Loss 0.0011 (0.0025)	
training:	Epoch: [57][128/408]	Loss 0.0012 (0.0025)	
training:	Epoch: [57][129/408]	Loss 0.0015 (0.0025)	
training:	Epoch: [57][130/408]	Loss 0.0010 (0.0025)	
training:	Epoch: [57][131/408]	Loss 0.0018 (0.0025)	
training:	Epoch: [57][132/408]	Loss 0.0045 (0.0025)	
training:	Epoch: [57][133/408]	Loss 0.0013 (0.0025)	
training:	Epoch: [57][134/408]	Loss 0.0015 (0.0025)	
training:	Epoch: [57][135/408]	Loss 0.0012 (0.0025)	
training:	Epoch: [57][136/408]	Loss 0.0014 (0.0025)	
training:	Epoch: [57][137/408]	Loss 0.0026 (0.0025)	
training:	Epoch: [57][138/408]	Loss 0.0014 (0.0025)	
training:	Epoch: [57][139/408]	Loss 0.0010 (0.0024)	
training:	Epoch: [57][140/408]	Loss 0.0013 (0.0024)	
training:	Epoch: [57][141/408]	Loss 0.0014 (0.0024)	
training:	Epoch: [57][142/408]	Loss 0.0015 (0.0024)	
training:	Epoch: [57][143/408]	Loss 0.0020 (0.0024)	
training:	Epoch: [57][144/408]	Loss 0.0011 (0.0024)	
training:	Epoch: [57][145/408]	Loss 0.0016 (0.0024)	
training:	Epoch: [57][146/408]	Loss 0.0012 (0.0024)	
training:	Epoch: [57][147/408]	Loss 0.0014 (0.0024)	
training:	Epoch: [57][148/408]	Loss 0.0010 (0.0024)	
training:	Epoch: [57][149/408]	Loss 0.0015 (0.0024)	
training:	Epoch: [57][150/408]	Loss 0.0013 (0.0024)	
training:	Epoch: [57][151/408]	Loss 0.0015 (0.0024)	
training:	Epoch: [57][152/408]	Loss 0.0012 (0.0024)	
training:	Epoch: [57][153/408]	Loss 0.0011 (0.0023)	
training:	Epoch: [57][154/408]	Loss 0.0314 (0.0025)	
training:	Epoch: [57][155/408]	Loss 0.0015 (0.0025)	
training:	Epoch: [57][156/408]	Loss 0.0012 (0.0025)	
training:	Epoch: [57][157/408]	Loss 0.0024 (0.0025)	
training:	Epoch: [57][158/408]	Loss 0.0593 (0.0029)	
training:	Epoch: [57][159/408]	Loss 0.0430 (0.0031)	
training:	Epoch: [57][160/408]	Loss 0.0013 (0.0031)	
training:	Epoch: [57][161/408]	Loss 0.0011 (0.0031)	
training:	Epoch: [57][162/408]	Loss 0.0014 (0.0031)	
training:	Epoch: [57][163/408]	Loss 0.0012 (0.0031)	
training:	Epoch: [57][164/408]	Loss 0.0011 (0.0031)	
training:	Epoch: [57][165/408]	Loss 0.0026 (0.0031)	
training:	Epoch: [57][166/408]	Loss 0.0012 (0.0031)	
training:	Epoch: [57][167/408]	Loss 0.0076 (0.0031)	
training:	Epoch: [57][168/408]	Loss 0.0012 (0.0031)	
training:	Epoch: [57][169/408]	Loss 0.0010 (0.0031)	
training:	Epoch: [57][170/408]	Loss 0.0013 (0.0030)	
training:	Epoch: [57][171/408]	Loss 0.0062 (0.0031)	
training:	Epoch: [57][172/408]	Loss 0.0118 (0.0031)	
training:	Epoch: [57][173/408]	Loss 0.0021 (0.0031)	
training:	Epoch: [57][174/408]	Loss 0.0012 (0.0031)	
training:	Epoch: [57][175/408]	Loss 0.0011 (0.0031)	
training:	Epoch: [57][176/408]	Loss 0.0024 (0.0031)	
training:	Epoch: [57][177/408]	Loss 0.0016 (0.0031)	
training:	Epoch: [57][178/408]	Loss 0.0013 (0.0031)	
training:	Epoch: [57][179/408]	Loss 0.0012 (0.0031)	
training:	Epoch: [57][180/408]	Loss 0.0021 (0.0031)	
training:	Epoch: [57][181/408]	Loss 0.0043 (0.0031)	
training:	Epoch: [57][182/408]	Loss 0.0022 (0.0031)	
training:	Epoch: [57][183/408]	Loss 0.0013 (0.0030)	
training:	Epoch: [57][184/408]	Loss 0.0015 (0.0030)	
training:	Epoch: [57][185/408]	Loss 0.0122 (0.0031)	
training:	Epoch: [57][186/408]	Loss 0.0010 (0.0031)	
training:	Epoch: [57][187/408]	Loss 0.0010 (0.0031)	
training:	Epoch: [57][188/408]	Loss 0.0136 (0.0031)	
training:	Epoch: [57][189/408]	Loss 0.0015 (0.0031)	
training:	Epoch: [57][190/408]	Loss 0.0013 (0.0031)	
training:	Epoch: [57][191/408]	Loss 0.0015 (0.0031)	
training:	Epoch: [57][192/408]	Loss 0.0078 (0.0031)	
training:	Epoch: [57][193/408]	Loss 0.0055 (0.0031)	
training:	Epoch: [57][194/408]	Loss 0.0026 (0.0031)	
training:	Epoch: [57][195/408]	Loss 0.0010 (0.0031)	
training:	Epoch: [57][196/408]	Loss 0.0358 (0.0033)	
training:	Epoch: [57][197/408]	Loss 0.0011 (0.0033)	
training:	Epoch: [57][198/408]	Loss 0.0015 (0.0033)	
training:	Epoch: [57][199/408]	Loss 0.0011 (0.0033)	
training:	Epoch: [57][200/408]	Loss 0.0010 (0.0032)	
training:	Epoch: [57][201/408]	Loss 0.0014 (0.0032)	
training:	Epoch: [57][202/408]	Loss 0.0019 (0.0032)	
training:	Epoch: [57][203/408]	Loss 0.0011 (0.0032)	
training:	Epoch: [57][204/408]	Loss 0.0012 (0.0032)	
training:	Epoch: [57][205/408]	Loss 0.0029 (0.0032)	
training:	Epoch: [57][206/408]	Loss 0.0010 (0.0032)	
training:	Epoch: [57][207/408]	Loss 0.0011 (0.0032)	
training:	Epoch: [57][208/408]	Loss 0.0010 (0.0032)	
training:	Epoch: [57][209/408]	Loss 0.0020 (0.0032)	
training:	Epoch: [57][210/408]	Loss 0.0013 (0.0032)	
training:	Epoch: [57][211/408]	Loss 0.0016 (0.0031)	
training:	Epoch: [57][212/408]	Loss 0.0014 (0.0031)	
training:	Epoch: [57][213/408]	Loss 0.0013 (0.0031)	
training:	Epoch: [57][214/408]	Loss 0.0011 (0.0031)	
training:	Epoch: [57][215/408]	Loss 0.0016 (0.0031)	
training:	Epoch: [57][216/408]	Loss 0.0028 (0.0031)	
training:	Epoch: [57][217/408]	Loss 0.0279 (0.0032)	
training:	Epoch: [57][218/408]	Loss 0.0025 (0.0032)	
training:	Epoch: [57][219/408]	Loss 0.0017 (0.0032)	
training:	Epoch: [57][220/408]	Loss 0.0032 (0.0032)	
training:	Epoch: [57][221/408]	Loss 0.0010 (0.0032)	
training:	Epoch: [57][222/408]	Loss 0.0015 (0.0032)	
training:	Epoch: [57][223/408]	Loss 0.0015 (0.0032)	
training:	Epoch: [57][224/408]	Loss 0.0014 (0.0032)	
training:	Epoch: [57][225/408]	Loss 0.0138 (0.0032)	
training:	Epoch: [57][226/408]	Loss 0.0011 (0.0032)	
training:	Epoch: [57][227/408]	Loss 0.0014 (0.0032)	
training:	Epoch: [57][228/408]	Loss 0.0010 (0.0032)	
training:	Epoch: [57][229/408]	Loss 0.0019 (0.0032)	
training:	Epoch: [57][230/408]	Loss 0.0012 (0.0032)	
training:	Epoch: [57][231/408]	Loss 0.0011 (0.0032)	
training:	Epoch: [57][232/408]	Loss 0.0027 (0.0032)	
training:	Epoch: [57][233/408]	Loss 0.0012 (0.0032)	
training:	Epoch: [57][234/408]	Loss 0.0014 (0.0032)	
training:	Epoch: [57][235/408]	Loss 0.0013 (0.0032)	
training:	Epoch: [57][236/408]	Loss 0.0017 (0.0031)	
training:	Epoch: [57][237/408]	Loss 0.0011 (0.0031)	
training:	Epoch: [57][238/408]	Loss 0.0016 (0.0031)	
training:	Epoch: [57][239/408]	Loss 0.0066 (0.0031)	
training:	Epoch: [57][240/408]	Loss 0.0127 (0.0032)	
training:	Epoch: [57][241/408]	Loss 0.0019 (0.0032)	
training:	Epoch: [57][242/408]	Loss 0.0015 (0.0032)	
training:	Epoch: [57][243/408]	Loss 0.0011 (0.0032)	
training:	Epoch: [57][244/408]	Loss 0.0014 (0.0032)	
training:	Epoch: [57][245/408]	Loss 0.0024 (0.0032)	
training:	Epoch: [57][246/408]	Loss 0.0012 (0.0031)	
training:	Epoch: [57][247/408]	Loss 0.0017 (0.0031)	
training:	Epoch: [57][248/408]	Loss 0.0012 (0.0031)	
training:	Epoch: [57][249/408]	Loss 0.0010 (0.0031)	
training:	Epoch: [57][250/408]	Loss 0.0012 (0.0031)	
training:	Epoch: [57][251/408]	Loss 0.0009 (0.0031)	
training:	Epoch: [57][252/408]	Loss 0.0011 (0.0031)	
training:	Epoch: [57][253/408]	Loss 0.0011 (0.0031)	
training:	Epoch: [57][254/408]	Loss 0.0012 (0.0031)	
training:	Epoch: [57][255/408]	Loss 0.0013 (0.0031)	
training:	Epoch: [57][256/408]	Loss 0.0015 (0.0031)	
training:	Epoch: [57][257/408]	Loss 0.0017 (0.0031)	
training:	Epoch: [57][258/408]	Loss 0.0016 (0.0031)	
training:	Epoch: [57][259/408]	Loss 0.0013 (0.0031)	
training:	Epoch: [57][260/408]	Loss 0.0010 (0.0030)	
training:	Epoch: [57][261/408]	Loss 0.0011 (0.0030)	
training:	Epoch: [57][262/408]	Loss 0.0010 (0.0030)	
training:	Epoch: [57][263/408]	Loss 0.0011 (0.0030)	
training:	Epoch: [57][264/408]	Loss 0.0012 (0.0030)	
training:	Epoch: [57][265/408]	Loss 0.0014 (0.0030)	
training:	Epoch: [57][266/408]	Loss 0.0017 (0.0030)	
training:	Epoch: [57][267/408]	Loss 0.0020 (0.0030)	
training:	Epoch: [57][268/408]	Loss 0.0011 (0.0030)	
training:	Epoch: [57][269/408]	Loss 0.0018 (0.0030)	
training:	Epoch: [57][270/408]	Loss 0.0010 (0.0030)	
training:	Epoch: [57][271/408]	Loss 0.0017 (0.0030)	
training:	Epoch: [57][272/408]	Loss 0.0020 (0.0030)	
training:	Epoch: [57][273/408]	Loss 0.0012 (0.0030)	
training:	Epoch: [57][274/408]	Loss 0.0449 (0.0031)	
training:	Epoch: [57][275/408]	Loss 0.0010 (0.0031)	
training:	Epoch: [57][276/408]	Loss 0.0012 (0.0031)	
training:	Epoch: [57][277/408]	Loss 0.0012 (0.0031)	
training:	Epoch: [57][278/408]	Loss 0.0130 (0.0031)	
training:	Epoch: [57][279/408]	Loss 0.0218 (0.0032)	
training:	Epoch: [57][280/408]	Loss 0.0222 (0.0033)	
training:	Epoch: [57][281/408]	Loss 0.0013 (0.0033)	
training:	Epoch: [57][282/408]	Loss 0.0018 (0.0033)	
training:	Epoch: [57][283/408]	Loss 0.0010 (0.0033)	
training:	Epoch: [57][284/408]	Loss 0.0022 (0.0032)	
training:	Epoch: [57][285/408]	Loss 0.0011 (0.0032)	
training:	Epoch: [57][286/408]	Loss 0.0011 (0.0032)	
training:	Epoch: [57][287/408]	Loss 0.0010 (0.0032)	
training:	Epoch: [57][288/408]	Loss 0.1761 (0.0038)	
training:	Epoch: [57][289/408]	Loss 0.0011 (0.0038)	
training:	Epoch: [57][290/408]	Loss 0.0019 (0.0038)	
training:	Epoch: [57][291/408]	Loss 0.0012 (0.0038)	
training:	Epoch: [57][292/408]	Loss 0.0019 (0.0038)	
training:	Epoch: [57][293/408]	Loss 0.0145 (0.0038)	
training:	Epoch: [57][294/408]	Loss 0.0014 (0.0038)	
training:	Epoch: [57][295/408]	Loss 0.0030 (0.0038)	
training:	Epoch: [57][296/408]	Loss 0.0010 (0.0038)	
training:	Epoch: [57][297/408]	Loss 0.0011 (0.0038)	
training:	Epoch: [57][298/408]	Loss 0.0012 (0.0038)	
training:	Epoch: [57][299/408]	Loss 0.0011 (0.0038)	
training:	Epoch: [57][300/408]	Loss 0.0010 (0.0038)	
training:	Epoch: [57][301/408]	Loss 0.0012 (0.0038)	
training:	Epoch: [57][302/408]	Loss 0.0013 (0.0038)	
training:	Epoch: [57][303/408]	Loss 0.0011 (0.0037)	
training:	Epoch: [57][304/408]	Loss 0.0013 (0.0037)	
training:	Epoch: [57][305/408]	Loss 0.0016 (0.0037)	
training:	Epoch: [57][306/408]	Loss 0.0063 (0.0037)	
training:	Epoch: [57][307/408]	Loss 0.0015 (0.0037)	
training:	Epoch: [57][308/408]	Loss 0.0047 (0.0037)	
training:	Epoch: [57][309/408]	Loss 0.0023 (0.0037)	
training:	Epoch: [57][310/408]	Loss 0.0011 (0.0037)	
training:	Epoch: [57][311/408]	Loss 0.0072 (0.0037)	
training:	Epoch: [57][312/408]	Loss 0.0151 (0.0038)	
training:	Epoch: [57][313/408]	Loss 0.0015 (0.0038)	
training:	Epoch: [57][314/408]	Loss 0.0112 (0.0038)	
training:	Epoch: [57][315/408]	Loss 0.0019 (0.0038)	
training:	Epoch: [57][316/408]	Loss 0.0009 (0.0038)	
training:	Epoch: [57][317/408]	Loss 0.0129 (0.0038)	
training:	Epoch: [57][318/408]	Loss 0.0013 (0.0038)	
training:	Epoch: [57][319/408]	Loss 0.0015 (0.0038)	
training:	Epoch: [57][320/408]	Loss 0.0020 (0.0038)	
training:	Epoch: [57][321/408]	Loss 0.0021 (0.0038)	
training:	Epoch: [57][322/408]	Loss 0.0012 (0.0038)	
training:	Epoch: [57][323/408]	Loss 0.0013 (0.0038)	
training:	Epoch: [57][324/408]	Loss 0.0091 (0.0038)	
training:	Epoch: [57][325/408]	Loss 0.0013 (0.0038)	
training:	Epoch: [57][326/408]	Loss 0.0393 (0.0039)	
training:	Epoch: [57][327/408]	Loss 0.0015 (0.0039)	
training:	Epoch: [57][328/408]	Loss 0.0011 (0.0039)	
training:	Epoch: [57][329/408]	Loss 0.0019 (0.0039)	
training:	Epoch: [57][330/408]	Loss 0.0015 (0.0038)	
training:	Epoch: [57][331/408]	Loss 0.0014 (0.0038)	
training:	Epoch: [57][332/408]	Loss 0.0163 (0.0039)	
training:	Epoch: [57][333/408]	Loss 0.0030 (0.0039)	
training:	Epoch: [57][334/408]	Loss 0.0102 (0.0039)	
training:	Epoch: [57][335/408]	Loss 0.0055 (0.0039)	
training:	Epoch: [57][336/408]	Loss 0.0059 (0.0039)	
training:	Epoch: [57][337/408]	Loss 0.0028 (0.0039)	
training:	Epoch: [57][338/408]	Loss 0.0010 (0.0039)	
training:	Epoch: [57][339/408]	Loss 0.0020 (0.0039)	
training:	Epoch: [57][340/408]	Loss 0.0019 (0.0039)	
training:	Epoch: [57][341/408]	Loss 0.0013 (0.0039)	
training:	Epoch: [57][342/408]	Loss 0.0094 (0.0039)	
training:	Epoch: [57][343/408]	Loss 0.0016 (0.0039)	
training:	Epoch: [57][344/408]	Loss 0.0010 (0.0039)	
training:	Epoch: [57][345/408]	Loss 0.0018 (0.0039)	
training:	Epoch: [57][346/408]	Loss 0.0012 (0.0039)	
training:	Epoch: [57][347/408]	Loss 0.0071 (0.0039)	
training:	Epoch: [57][348/408]	Loss 0.0481 (0.0040)	
training:	Epoch: [57][349/408]	Loss 0.0024 (0.0040)	
training:	Epoch: [57][350/408]	Loss 0.0018 (0.0040)	
training:	Epoch: [57][351/408]	Loss 0.0011 (0.0040)	
training:	Epoch: [57][352/408]	Loss 0.0034 (0.0040)	
training:	Epoch: [57][353/408]	Loss 0.0024 (0.0040)	
training:	Epoch: [57][354/408]	Loss 0.0030 (0.0040)	
training:	Epoch: [57][355/408]	Loss 0.0018 (0.0040)	
training:	Epoch: [57][356/408]	Loss 0.0011 (0.0040)	
training:	Epoch: [57][357/408]	Loss 0.0850 (0.0042)	
training:	Epoch: [57][358/408]	Loss 0.0014 (0.0042)	
training:	Epoch: [57][359/408]	Loss 0.0010 (0.0042)	
training:	Epoch: [57][360/408]	Loss 0.0011 (0.0042)	
training:	Epoch: [57][361/408]	Loss 0.0009 (0.0042)	
training:	Epoch: [57][362/408]	Loss 0.0018 (0.0041)	
training:	Epoch: [57][363/408]	Loss 0.0012 (0.0041)	
training:	Epoch: [57][364/408]	Loss 0.0020 (0.0041)	
training:	Epoch: [57][365/408]	Loss 0.0012 (0.0041)	
training:	Epoch: [57][366/408]	Loss 0.0013 (0.0041)	
training:	Epoch: [57][367/408]	Loss 0.0018 (0.0041)	
training:	Epoch: [57][368/408]	Loss 0.0183 (0.0041)	
training:	Epoch: [57][369/408]	Loss 0.0012 (0.0041)	
training:	Epoch: [57][370/408]	Loss 0.0011 (0.0041)	
training:	Epoch: [57][371/408]	Loss 0.0033 (0.0041)	
training:	Epoch: [57][372/408]	Loss 0.0126 (0.0042)	
training:	Epoch: [57][373/408]	Loss 0.0024 (0.0041)	
training:	Epoch: [57][374/408]	Loss 0.0034 (0.0041)	
training:	Epoch: [57][375/408]	Loss 0.0048 (0.0041)	
training:	Epoch: [57][376/408]	Loss 0.0011 (0.0041)	
training:	Epoch: [57][377/408]	Loss 0.0011 (0.0041)	
training:	Epoch: [57][378/408]	Loss 0.0019 (0.0041)	
training:	Epoch: [57][379/408]	Loss 0.0600 (0.0043)	
training:	Epoch: [57][380/408]	Loss 0.0010 (0.0043)	
training:	Epoch: [57][381/408]	Loss 0.0013 (0.0043)	
training:	Epoch: [57][382/408]	Loss 0.0012 (0.0042)	
training:	Epoch: [57][383/408]	Loss 0.0010 (0.0042)	
training:	Epoch: [57][384/408]	Loss 0.0012 (0.0042)	
training:	Epoch: [57][385/408]	Loss 0.0040 (0.0042)	
training:	Epoch: [57][386/408]	Loss 0.0010 (0.0042)	
training:	Epoch: [57][387/408]	Loss 0.0016 (0.0042)	
training:	Epoch: [57][388/408]	Loss 0.0014 (0.0042)	
training:	Epoch: [57][389/408]	Loss 0.0010 (0.0042)	
training:	Epoch: [57][390/408]	Loss 0.0013 (0.0042)	
training:	Epoch: [57][391/408]	Loss 0.0011 (0.0042)	
training:	Epoch: [57][392/408]	Loss 0.0106 (0.0042)	
training:	Epoch: [57][393/408]	Loss 0.0062 (0.0042)	
training:	Epoch: [57][394/408]	Loss 0.0029 (0.0042)	
training:	Epoch: [57][395/408]	Loss 0.0043 (0.0042)	
training:	Epoch: [57][396/408]	Loss 0.0139 (0.0042)	
training:	Epoch: [57][397/408]	Loss 0.0017 (0.0042)	
training:	Epoch: [57][398/408]	Loss 0.0334 (0.0043)	
training:	Epoch: [57][399/408]	Loss 0.0136 (0.0043)	
training:	Epoch: [57][400/408]	Loss 0.0035 (0.0043)	
training:	Epoch: [57][401/408]	Loss 0.0017 (0.0043)	
training:	Epoch: [57][402/408]	Loss 0.0167 (0.0043)	
training:	Epoch: [57][403/408]	Loss 0.0480 (0.0044)	
training:	Epoch: [57][404/408]	Loss 0.0011 (0.0044)	
training:	Epoch: [57][405/408]	Loss 0.0016 (0.0044)	
training:	Epoch: [57][406/408]	Loss 0.0029 (0.0044)	
training:	Epoch: [57][407/408]	Loss 0.0071 (0.0044)	
training:	Epoch: [57][408/408]	Loss 0.0012 (0.0044)	
Training:	 Loss: 0.0044

Training:	 ACC: 0.9998 0.9998 1.0000 0.9997
Validation:	 ACC: 0.7813 0.7822 0.8025 0.7601
Validation:	 Best_BACC: 0.8226 0.8250 0.8772 0.7679
Validation:	 Loss: 1.1700
Pretraining:	Epoch 58/200
----------
training:	Epoch: [58][1/408]	Loss 0.0014 (0.0014)	
training:	Epoch: [58][2/408]	Loss 0.0013 (0.0013)	
training:	Epoch: [58][3/408]	Loss 0.0015 (0.0014)	
training:	Epoch: [58][4/408]	Loss 0.0014 (0.0014)	
training:	Epoch: [58][5/408]	Loss 0.0014 (0.0014)	
training:	Epoch: [58][6/408]	Loss 0.0014 (0.0014)	
training:	Epoch: [58][7/408]	Loss 0.0019 (0.0015)	
training:	Epoch: [58][8/408]	Loss 0.0013 (0.0015)	
training:	Epoch: [58][9/408]	Loss 0.0010 (0.0014)	
training:	Epoch: [58][10/408]	Loss 0.0010 (0.0014)	
training:	Epoch: [58][11/408]	Loss 0.0013 (0.0014)	
training:	Epoch: [58][12/408]	Loss 0.0010 (0.0013)	
training:	Epoch: [58][13/408]	Loss 0.0010 (0.0013)	
training:	Epoch: [58][14/408]	Loss 0.0012 (0.0013)	
training:	Epoch: [58][15/408]	Loss 0.0015 (0.0013)	
training:	Epoch: [58][16/408]	Loss 0.0010 (0.0013)	
training:	Epoch: [58][17/408]	Loss 0.0063 (0.0016)	
training:	Epoch: [58][18/408]	Loss 0.0011 (0.0016)	
training:	Epoch: [58][19/408]	Loss 0.0012 (0.0015)	
training:	Epoch: [58][20/408]	Loss 0.0011 (0.0015)	
training:	Epoch: [58][21/408]	Loss 0.0010 (0.0015)	
training:	Epoch: [58][22/408]	Loss 0.0012 (0.0015)	
training:	Epoch: [58][23/408]	Loss 0.0011 (0.0015)	
training:	Epoch: [58][24/408]	Loss 0.0201 (0.0022)	
training:	Epoch: [58][25/408]	Loss 0.0045 (0.0023)	
training:	Epoch: [58][26/408]	Loss 0.0031 (0.0024)	
training:	Epoch: [58][27/408]	Loss 0.0014 (0.0023)	
training:	Epoch: [58][28/408]	Loss 0.0022 (0.0023)	
training:	Epoch: [58][29/408]	Loss 0.0011 (0.0023)	
training:	Epoch: [58][30/408]	Loss 0.0014 (0.0022)	
training:	Epoch: [58][31/408]	Loss 0.0011 (0.0022)	
training:	Epoch: [58][32/408]	Loss 0.0013 (0.0022)	
training:	Epoch: [58][33/408]	Loss 0.0017 (0.0022)	
training:	Epoch: [58][34/408]	Loss 0.0012 (0.0021)	
training:	Epoch: [58][35/408]	Loss 0.0048 (0.0022)	
training:	Epoch: [58][36/408]	Loss 0.0012 (0.0022)	
training:	Epoch: [58][37/408]	Loss 0.0034 (0.0022)	
training:	Epoch: [58][38/408]	Loss 0.0010 (0.0022)	
training:	Epoch: [58][39/408]	Loss 0.0015 (0.0022)	
training:	Epoch: [58][40/408]	Loss 0.0016 (0.0022)	
training:	Epoch: [58][41/408]	Loss 0.0010 (0.0021)	
training:	Epoch: [58][42/408]	Loss 0.0009 (0.0021)	
training:	Epoch: [58][43/408]	Loss 0.0009 (0.0021)	
training:	Epoch: [58][44/408]	Loss 0.0303 (0.0027)	
training:	Epoch: [58][45/408]	Loss 0.0013 (0.0027)	
training:	Epoch: [58][46/408]	Loss 0.0011 (0.0026)	
training:	Epoch: [58][47/408]	Loss 0.0014 (0.0026)	
training:	Epoch: [58][48/408]	Loss 0.0012 (0.0026)	
training:	Epoch: [58][49/408]	Loss 0.0070 (0.0027)	
training:	Epoch: [58][50/408]	Loss 0.0009 (0.0026)	
training:	Epoch: [58][51/408]	Loss 0.0057 (0.0027)	
training:	Epoch: [58][52/408]	Loss 0.0016 (0.0027)	
training:	Epoch: [58][53/408]	Loss 0.0010 (0.0026)	
training:	Epoch: [58][54/408]	Loss 0.0015 (0.0026)	
training:	Epoch: [58][55/408]	Loss 0.0010 (0.0026)	
training:	Epoch: [58][56/408]	Loss 0.0011 (0.0026)	
training:	Epoch: [58][57/408]	Loss 0.0094 (0.0027)	
training:	Epoch: [58][58/408]	Loss 0.0012 (0.0027)	
training:	Epoch: [58][59/408]	Loss 0.0015 (0.0026)	
training:	Epoch: [58][60/408]	Loss 0.0014 (0.0026)	
training:	Epoch: [58][61/408]	Loss 0.0012 (0.0026)	
training:	Epoch: [58][62/408]	Loss 0.0011 (0.0026)	
training:	Epoch: [58][63/408]	Loss 0.0017 (0.0026)	
training:	Epoch: [58][64/408]	Loss 0.0015 (0.0025)	
training:	Epoch: [58][65/408]	Loss 0.0017 (0.0025)	
training:	Epoch: [58][66/408]	Loss 0.0009 (0.0025)	
training:	Epoch: [58][67/408]	Loss 0.0011 (0.0025)	
training:	Epoch: [58][68/408]	Loss 0.0065 (0.0025)	
training:	Epoch: [58][69/408]	Loss 0.0010 (0.0025)	
training:	Epoch: [58][70/408]	Loss 0.0013 (0.0025)	
training:	Epoch: [58][71/408]	Loss 0.0010 (0.0025)	
training:	Epoch: [58][72/408]	Loss 0.0009 (0.0025)	
training:	Epoch: [58][73/408]	Loss 0.0009 (0.0024)	
training:	Epoch: [58][74/408]	Loss 0.0030 (0.0024)	
training:	Epoch: [58][75/408]	Loss 0.0031 (0.0025)	
training:	Epoch: [58][76/408]	Loss 0.0009 (0.0024)	
training:	Epoch: [58][77/408]	Loss 0.0010 (0.0024)	
training:	Epoch: [58][78/408]	Loss 0.0065 (0.0025)	
training:	Epoch: [58][79/408]	Loss 0.0010 (0.0025)	
training:	Epoch: [58][80/408]	Loss 0.0015 (0.0024)	
training:	Epoch: [58][81/408]	Loss 0.0014 (0.0024)	
training:	Epoch: [58][82/408]	Loss 0.0011 (0.0024)	
training:	Epoch: [58][83/408]	Loss 0.0011 (0.0024)	
training:	Epoch: [58][84/408]	Loss 0.0009 (0.0024)	
training:	Epoch: [58][85/408]	Loss 0.0016 (0.0024)	
training:	Epoch: [58][86/408]	Loss 0.0013 (0.0024)	
training:	Epoch: [58][87/408]	Loss 0.0010 (0.0023)	
training:	Epoch: [58][88/408]	Loss 0.0009 (0.0023)	
training:	Epoch: [58][89/408]	Loss 0.0010 (0.0023)	
training:	Epoch: [58][90/408]	Loss 0.0013 (0.0023)	
training:	Epoch: [58][91/408]	Loss 0.0012 (0.0023)	
training:	Epoch: [58][92/408]	Loss 0.0052 (0.0023)	
training:	Epoch: [58][93/408]	Loss 0.0013 (0.0023)	
training:	Epoch: [58][94/408]	Loss 0.0009 (0.0023)	
training:	Epoch: [58][95/408]	Loss 0.0012 (0.0023)	
training:	Epoch: [58][96/408]	Loss 0.0011 (0.0023)	
training:	Epoch: [58][97/408]	Loss 0.0020 (0.0023)	
training:	Epoch: [58][98/408]	Loss 0.0011 (0.0023)	
training:	Epoch: [58][99/408]	Loss 0.0011 (0.0022)	
training:	Epoch: [58][100/408]	Loss 0.0015 (0.0022)	
training:	Epoch: [58][101/408]	Loss 0.0009 (0.0022)	
training:	Epoch: [58][102/408]	Loss 0.0021 (0.0022)	
training:	Epoch: [58][103/408]	Loss 0.0130 (0.0023)	
training:	Epoch: [58][104/408]	Loss 0.0090 (0.0024)	
training:	Epoch: [58][105/408]	Loss 0.0009 (0.0024)	
training:	Epoch: [58][106/408]	Loss 0.0016 (0.0024)	
training:	Epoch: [58][107/408]	Loss 0.0010 (0.0024)	
training:	Epoch: [58][108/408]	Loss 0.0081 (0.0024)	
training:	Epoch: [58][109/408]	Loss 0.0333 (0.0027)	
training:	Epoch: [58][110/408]	Loss 0.0493 (0.0031)	
training:	Epoch: [58][111/408]	Loss 0.0010 (0.0031)	
training:	Epoch: [58][112/408]	Loss 0.0038 (0.0031)	
training:	Epoch: [58][113/408]	Loss 0.0012 (0.0031)	
training:	Epoch: [58][114/408]	Loss 0.0037 (0.0031)	
training:	Epoch: [58][115/408]	Loss 0.0015 (0.0031)	
training:	Epoch: [58][116/408]	Loss 0.0009 (0.0031)	
training:	Epoch: [58][117/408]	Loss 0.0013 (0.0030)	
training:	Epoch: [58][118/408]	Loss 0.0013 (0.0030)	
training:	Epoch: [58][119/408]	Loss 0.0011 (0.0030)	
training:	Epoch: [58][120/408]	Loss 0.0022 (0.0030)	
training:	Epoch: [58][121/408]	Loss 0.0016 (0.0030)	
training:	Epoch: [58][122/408]	Loss 0.0031 (0.0030)	
training:	Epoch: [58][123/408]	Loss 0.0018 (0.0030)	
training:	Epoch: [58][124/408]	Loss 0.0012 (0.0030)	
training:	Epoch: [58][125/408]	Loss 0.0016 (0.0030)	
training:	Epoch: [58][126/408]	Loss 0.0010 (0.0029)	
training:	Epoch: [58][127/408]	Loss 0.0010 (0.0029)	
training:	Epoch: [58][128/408]	Loss 0.0010 (0.0029)	
training:	Epoch: [58][129/408]	Loss 0.0010 (0.0029)	
training:	Epoch: [58][130/408]	Loss 0.0029 (0.0029)	
training:	Epoch: [58][131/408]	Loss 0.0018 (0.0029)	
training:	Epoch: [58][132/408]	Loss 0.0011 (0.0029)	
training:	Epoch: [58][133/408]	Loss 0.0010 (0.0029)	
training:	Epoch: [58][134/408]	Loss 0.0011 (0.0029)	
training:	Epoch: [58][135/408]	Loss 0.0010 (0.0028)	
training:	Epoch: [58][136/408]	Loss 0.0010 (0.0028)	
training:	Epoch: [58][137/408]	Loss 0.0010 (0.0028)	
training:	Epoch: [58][138/408]	Loss 0.0010 (0.0028)	
training:	Epoch: [58][139/408]	Loss 0.0010 (0.0028)	
training:	Epoch: [58][140/408]	Loss 0.0011 (0.0028)	
training:	Epoch: [58][141/408]	Loss 0.0051 (0.0028)	
training:	Epoch: [58][142/408]	Loss 0.0016 (0.0028)	
training:	Epoch: [58][143/408]	Loss 0.0009 (0.0028)	
training:	Epoch: [58][144/408]	Loss 0.0011 (0.0028)	
training:	Epoch: [58][145/408]	Loss 0.0013 (0.0027)	
training:	Epoch: [58][146/408]	Loss 0.0012 (0.0027)	
training:	Epoch: [58][147/408]	Loss 0.0243 (0.0029)	
training:	Epoch: [58][148/408]	Loss 0.0010 (0.0029)	
training:	Epoch: [58][149/408]	Loss 0.0014 (0.0029)	
training:	Epoch: [58][150/408]	Loss 0.0012 (0.0029)	
training:	Epoch: [58][151/408]	Loss 0.0024 (0.0028)	
training:	Epoch: [58][152/408]	Loss 0.0011 (0.0028)	
training:	Epoch: [58][153/408]	Loss 0.0069 (0.0029)	
training:	Epoch: [58][154/408]	Loss 0.0009 (0.0029)	
training:	Epoch: [58][155/408]	Loss 0.0011 (0.0028)	
training:	Epoch: [58][156/408]	Loss 0.0020 (0.0028)	
training:	Epoch: [58][157/408]	Loss 0.0010 (0.0028)	
training:	Epoch: [58][158/408]	Loss 0.0035 (0.0028)	
training:	Epoch: [58][159/408]	Loss 0.0012 (0.0028)	
training:	Epoch: [58][160/408]	Loss 0.0027 (0.0028)	
training:	Epoch: [58][161/408]	Loss 0.0013 (0.0028)	
training:	Epoch: [58][162/408]	Loss 0.0010 (0.0028)	
training:	Epoch: [58][163/408]	Loss 0.0010 (0.0028)	
training:	Epoch: [58][164/408]	Loss 0.0111 (0.0028)	
training:	Epoch: [58][165/408]	Loss 0.0010 (0.0028)	
training:	Epoch: [58][166/408]	Loss 0.0010 (0.0028)	
training:	Epoch: [58][167/408]	Loss 0.0019 (0.0028)	
training:	Epoch: [58][168/408]	Loss 0.0010 (0.0028)	
training:	Epoch: [58][169/408]	Loss 0.0011 (0.0028)	
training:	Epoch: [58][170/408]	Loss 0.0011 (0.0028)	
training:	Epoch: [58][171/408]	Loss 0.0015 (0.0028)	
training:	Epoch: [58][172/408]	Loss 0.0009 (0.0028)	
training:	Epoch: [58][173/408]	Loss 0.0010 (0.0027)	
training:	Epoch: [58][174/408]	Loss 0.0267 (0.0029)	
training:	Epoch: [58][175/408]	Loss 0.0014 (0.0029)	
training:	Epoch: [58][176/408]	Loss 0.0014 (0.0029)	
training:	Epoch: [58][177/408]	Loss 0.0026 (0.0029)	
training:	Epoch: [58][178/408]	Loss 0.0010 (0.0029)	
training:	Epoch: [58][179/408]	Loss 0.0017 (0.0028)	
training:	Epoch: [58][180/408]	Loss 0.0026 (0.0028)	
training:	Epoch: [58][181/408]	Loss 0.0009 (0.0028)	
training:	Epoch: [58][182/408]	Loss 0.0016 (0.0028)	
training:	Epoch: [58][183/408]	Loss 0.0011 (0.0028)	
training:	Epoch: [58][184/408]	Loss 0.0010 (0.0028)	
training:	Epoch: [58][185/408]	Loss 0.0014 (0.0028)	
training:	Epoch: [58][186/408]	Loss 0.0020 (0.0028)	
training:	Epoch: [58][187/408]	Loss 0.0020 (0.0028)	
training:	Epoch: [58][188/408]	Loss 0.0011 (0.0028)	
training:	Epoch: [58][189/408]	Loss 0.0011 (0.0028)	
training:	Epoch: [58][190/408]	Loss 0.0012 (0.0028)	
training:	Epoch: [58][191/408]	Loss 0.0067 (0.0028)	
training:	Epoch: [58][192/408]	Loss 0.0011 (0.0028)	
training:	Epoch: [58][193/408]	Loss 0.0010 (0.0028)	
training:	Epoch: [58][194/408]	Loss 0.0010 (0.0028)	
training:	Epoch: [58][195/408]	Loss 0.0013 (0.0028)	
training:	Epoch: [58][196/408]	Loss 0.0163 (0.0028)	
training:	Epoch: [58][197/408]	Loss 0.0010 (0.0028)	
training:	Epoch: [58][198/408]	Loss 0.0016 (0.0028)	
training:	Epoch: [58][199/408]	Loss 0.0013 (0.0028)	
training:	Epoch: [58][200/408]	Loss 0.0011 (0.0028)	
training:	Epoch: [58][201/408]	Loss 0.0021 (0.0028)	
training:	Epoch: [58][202/408]	Loss 0.0009 (0.0028)	
training:	Epoch: [58][203/408]	Loss 0.0018 (0.0028)	
training:	Epoch: [58][204/408]	Loss 0.0010 (0.0028)	
training:	Epoch: [58][205/408]	Loss 0.0058 (0.0028)	
training:	Epoch: [58][206/408]	Loss 0.0014 (0.0028)	
training:	Epoch: [58][207/408]	Loss 0.0013 (0.0028)	
training:	Epoch: [58][208/408]	Loss 0.0012 (0.0028)	
training:	Epoch: [58][209/408]	Loss 0.0071 (0.0028)	
training:	Epoch: [58][210/408]	Loss 0.0036 (0.0028)	
training:	Epoch: [58][211/408]	Loss 0.0052 (0.0028)	
training:	Epoch: [58][212/408]	Loss 0.0012 (0.0028)	
training:	Epoch: [58][213/408]	Loss 0.0011 (0.0028)	
training:	Epoch: [58][214/408]	Loss 0.0871 (0.0032)	
training:	Epoch: [58][215/408]	Loss 0.0017 (0.0032)	
training:	Epoch: [58][216/408]	Loss 0.0010 (0.0032)	
training:	Epoch: [58][217/408]	Loss 0.0017 (0.0032)	
training:	Epoch: [58][218/408]	Loss 0.0011 (0.0031)	
training:	Epoch: [58][219/408]	Loss 0.0014 (0.0031)	
training:	Epoch: [58][220/408]	Loss 0.0431 (0.0033)	
training:	Epoch: [58][221/408]	Loss 0.0011 (0.0033)	
training:	Epoch: [58][222/408]	Loss 0.0012 (0.0033)	
training:	Epoch: [58][223/408]	Loss 0.0011 (0.0033)	
training:	Epoch: [58][224/408]	Loss 0.0033 (0.0033)	
training:	Epoch: [58][225/408]	Loss 0.0099 (0.0033)	
training:	Epoch: [58][226/408]	Loss 0.0024 (0.0033)	
training:	Epoch: [58][227/408]	Loss 0.0020 (0.0033)	
training:	Epoch: [58][228/408]	Loss 0.0016 (0.0033)	
training:	Epoch: [58][229/408]	Loss 0.0011 (0.0033)	
training:	Epoch: [58][230/408]	Loss 0.0024 (0.0033)	
training:	Epoch: [58][231/408]	Loss 0.0020 (0.0033)	
training:	Epoch: [58][232/408]	Loss 0.0010 (0.0033)	
training:	Epoch: [58][233/408]	Loss 0.0014 (0.0033)	
training:	Epoch: [58][234/408]	Loss 0.0011 (0.0033)	
training:	Epoch: [58][235/408]	Loss 0.0307 (0.0034)	
training:	Epoch: [58][236/408]	Loss 0.0009 (0.0034)	
training:	Epoch: [58][237/408]	Loss 0.0010 (0.0033)	
training:	Epoch: [58][238/408]	Loss 0.0011 (0.0033)	
training:	Epoch: [58][239/408]	Loss 0.0898 (0.0037)	
training:	Epoch: [58][240/408]	Loss 0.0011 (0.0037)	
training:	Epoch: [58][241/408]	Loss 0.0023 (0.0037)	
training:	Epoch: [58][242/408]	Loss 0.0009 (0.0037)	
training:	Epoch: [58][243/408]	Loss 0.0011 (0.0037)	
training:	Epoch: [58][244/408]	Loss 0.0009 (0.0037)	
training:	Epoch: [58][245/408]	Loss 0.0108 (0.0037)	
training:	Epoch: [58][246/408]	Loss 0.0010 (0.0037)	
training:	Epoch: [58][247/408]	Loss 0.0010 (0.0037)	
training:	Epoch: [58][248/408]	Loss 0.0023 (0.0037)	
training:	Epoch: [58][249/408]	Loss 0.0009 (0.0036)	
training:	Epoch: [58][250/408]	Loss 0.0043 (0.0036)	
training:	Epoch: [58][251/408]	Loss 0.0074 (0.0037)	
training:	Epoch: [58][252/408]	Loss 0.0020 (0.0037)	
training:	Epoch: [58][253/408]	Loss 0.0010 (0.0036)	
training:	Epoch: [58][254/408]	Loss 0.0011 (0.0036)	
training:	Epoch: [58][255/408]	Loss 0.0017 (0.0036)	
training:	Epoch: [58][256/408]	Loss 0.0028 (0.0036)	
training:	Epoch: [58][257/408]	Loss 0.0012 (0.0036)	
training:	Epoch: [58][258/408]	Loss 0.0009 (0.0036)	
training:	Epoch: [58][259/408]	Loss 0.0025 (0.0036)	
training:	Epoch: [58][260/408]	Loss 0.0013 (0.0036)	
training:	Epoch: [58][261/408]	Loss 0.0009 (0.0036)	
training:	Epoch: [58][262/408]	Loss 0.0011 (0.0036)	
training:	Epoch: [58][263/408]	Loss 0.0009 (0.0036)	
training:	Epoch: [58][264/408]	Loss 0.0065 (0.0036)	
training:	Epoch: [58][265/408]	Loss 0.0058 (0.0036)	
training:	Epoch: [58][266/408]	Loss 0.0009 (0.0036)	
training:	Epoch: [58][267/408]	Loss 0.0010 (0.0036)	
training:	Epoch: [58][268/408]	Loss 0.0019 (0.0036)	
training:	Epoch: [58][269/408]	Loss 0.0014 (0.0035)	
training:	Epoch: [58][270/408]	Loss 0.0014 (0.0035)	
training:	Epoch: [58][271/408]	Loss 0.0009 (0.0035)	
training:	Epoch: [58][272/408]	Loss 0.0021 (0.0035)	
training:	Epoch: [58][273/408]	Loss 0.0010 (0.0035)	
training:	Epoch: [58][274/408]	Loss 0.0011 (0.0035)	
training:	Epoch: [58][275/408]	Loss 0.0025 (0.0035)	
training:	Epoch: [58][276/408]	Loss 0.0019 (0.0035)	
training:	Epoch: [58][277/408]	Loss 0.0013 (0.0035)	
training:	Epoch: [58][278/408]	Loss 0.0015 (0.0035)	
training:	Epoch: [58][279/408]	Loss 0.0014 (0.0035)	
training:	Epoch: [58][280/408]	Loss 0.0050 (0.0035)	
training:	Epoch: [58][281/408]	Loss 0.0348 (0.0036)	
training:	Epoch: [58][282/408]	Loss 0.0022 (0.0036)	
training:	Epoch: [58][283/408]	Loss 0.0012 (0.0036)	
training:	Epoch: [58][284/408]	Loss 0.0013 (0.0036)	
training:	Epoch: [58][285/408]	Loss 0.0062 (0.0036)	
training:	Epoch: [58][286/408]	Loss 0.0017 (0.0036)	
training:	Epoch: [58][287/408]	Loss 0.0010 (0.0036)	
training:	Epoch: [58][288/408]	Loss 0.0069 (0.0036)	
training:	Epoch: [58][289/408]	Loss 0.0016 (0.0036)	
training:	Epoch: [58][290/408]	Loss 0.0010 (0.0036)	
training:	Epoch: [58][291/408]	Loss 0.0028 (0.0036)	
training:	Epoch: [58][292/408]	Loss 0.0021 (0.0035)	
training:	Epoch: [58][293/408]	Loss 0.0012 (0.0035)	
training:	Epoch: [58][294/408]	Loss 0.0009 (0.0035)	
training:	Epoch: [58][295/408]	Loss 0.0422 (0.0037)	
training:	Epoch: [58][296/408]	Loss 0.0021 (0.0037)	
training:	Epoch: [58][297/408]	Loss 0.0013 (0.0036)	
training:	Epoch: [58][298/408]	Loss 0.0013 (0.0036)	
training:	Epoch: [58][299/408]	Loss 0.0090 (0.0037)	
training:	Epoch: [58][300/408]	Loss 0.0017 (0.0037)	
training:	Epoch: [58][301/408]	Loss 0.0010 (0.0036)	
training:	Epoch: [58][302/408]	Loss 0.0009 (0.0036)	
training:	Epoch: [58][303/408]	Loss 0.0016 (0.0036)	
training:	Epoch: [58][304/408]	Loss 0.0013 (0.0036)	
training:	Epoch: [58][305/408]	Loss 0.0032 (0.0036)	
training:	Epoch: [58][306/408]	Loss 0.0012 (0.0036)	
training:	Epoch: [58][307/408]	Loss 0.0015 (0.0036)	
training:	Epoch: [58][308/408]	Loss 0.0020 (0.0036)	
training:	Epoch: [58][309/408]	Loss 0.0014 (0.0036)	
training:	Epoch: [58][310/408]	Loss 0.0016 (0.0036)	
training:	Epoch: [58][311/408]	Loss 0.0106 (0.0036)	
training:	Epoch: [58][312/408]	Loss 0.0022 (0.0036)	
training:	Epoch: [58][313/408]	Loss 0.0009 (0.0036)	
training:	Epoch: [58][314/408]	Loss 0.0010 (0.0036)	
training:	Epoch: [58][315/408]	Loss 0.0019 (0.0036)	
training:	Epoch: [58][316/408]	Loss 0.0013 (0.0036)	
training:	Epoch: [58][317/408]	Loss 0.0010 (0.0036)	
training:	Epoch: [58][318/408]	Loss 0.0010 (0.0036)	
training:	Epoch: [58][319/408]	Loss 0.0027 (0.0036)	
training:	Epoch: [58][320/408]	Loss 0.0012 (0.0035)	
training:	Epoch: [58][321/408]	Loss 0.0010 (0.0035)	
training:	Epoch: [58][322/408]	Loss 0.0015 (0.0035)	
training:	Epoch: [58][323/408]	Loss 0.0031 (0.0035)	
training:	Epoch: [58][324/408]	Loss 0.0082 (0.0035)	
training:	Epoch: [58][325/408]	Loss 0.0017 (0.0035)	
training:	Epoch: [58][326/408]	Loss 0.0010 (0.0035)	
training:	Epoch: [58][327/408]	Loss 0.0013 (0.0035)	
training:	Epoch: [58][328/408]	Loss 0.0010 (0.0035)	
training:	Epoch: [58][329/408]	Loss 0.0023 (0.0035)	
training:	Epoch: [58][330/408]	Loss 0.0130 (0.0035)	
training:	Epoch: [58][331/408]	Loss 0.0016 (0.0035)	
training:	Epoch: [58][332/408]	Loss 0.0016 (0.0035)	
training:	Epoch: [58][333/408]	Loss 0.0014 (0.0035)	
training:	Epoch: [58][334/408]	Loss 0.0010 (0.0035)	
training:	Epoch: [58][335/408]	Loss 0.0009 (0.0035)	
training:	Epoch: [58][336/408]	Loss 0.0014 (0.0035)	
training:	Epoch: [58][337/408]	Loss 0.0022 (0.0035)	
training:	Epoch: [58][338/408]	Loss 0.0011 (0.0035)	
training:	Epoch: [58][339/408]	Loss 0.0013 (0.0035)	
training:	Epoch: [58][340/408]	Loss 0.0019 (0.0035)	
training:	Epoch: [58][341/408]	Loss 0.0010 (0.0035)	
training:	Epoch: [58][342/408]	Loss 0.0015 (0.0035)	
training:	Epoch: [58][343/408]	Loss 0.0020 (0.0035)	
training:	Epoch: [58][344/408]	Loss 0.0012 (0.0035)	
training:	Epoch: [58][345/408]	Loss 0.0010 (0.0035)	
training:	Epoch: [58][346/408]	Loss 0.0548 (0.0036)	
training:	Epoch: [58][347/408]	Loss 0.0020 (0.0036)	
training:	Epoch: [58][348/408]	Loss 0.0017 (0.0036)	
training:	Epoch: [58][349/408]	Loss 0.0014 (0.0036)	
training:	Epoch: [58][350/408]	Loss 0.0010 (0.0036)	
training:	Epoch: [58][351/408]	Loss 0.0010 (0.0036)	
training:	Epoch: [58][352/408]	Loss 0.0009 (0.0036)	
training:	Epoch: [58][353/408]	Loss 0.0026 (0.0036)	
training:	Epoch: [58][354/408]	Loss 0.0009 (0.0036)	
training:	Epoch: [58][355/408]	Loss 0.0009 (0.0035)	
training:	Epoch: [58][356/408]	Loss 0.0019 (0.0035)	
training:	Epoch: [58][357/408]	Loss 0.0013 (0.0035)	
training:	Epoch: [58][358/408]	Loss 0.0009 (0.0035)	
training:	Epoch: [58][359/408]	Loss 0.0015 (0.0035)	
training:	Epoch: [58][360/408]	Loss 0.0011 (0.0035)	
training:	Epoch: [58][361/408]	Loss 0.0009 (0.0035)	
training:	Epoch: [58][362/408]	Loss 0.0016 (0.0035)	
training:	Epoch: [58][363/408]	Loss 0.0100 (0.0035)	
training:	Epoch: [58][364/408]	Loss 0.0015 (0.0035)	
training:	Epoch: [58][365/408]	Loss 0.0010 (0.0035)	
training:	Epoch: [58][366/408]	Loss 0.0014 (0.0035)	
training:	Epoch: [58][367/408]	Loss 0.0011 (0.0035)	
training:	Epoch: [58][368/408]	Loss 0.0034 (0.0035)	
training:	Epoch: [58][369/408]	Loss 0.0015 (0.0035)	
training:	Epoch: [58][370/408]	Loss 0.0009 (0.0035)	
training:	Epoch: [58][371/408]	Loss 0.0033 (0.0035)	
training:	Epoch: [58][372/408]	Loss 0.0009 (0.0035)	
training:	Epoch: [58][373/408]	Loss 0.0009 (0.0035)	
training:	Epoch: [58][374/408]	Loss 0.0008 (0.0035)	
training:	Epoch: [58][375/408]	Loss 0.0025 (0.0035)	
training:	Epoch: [58][376/408]	Loss 0.0010 (0.0035)	
training:	Epoch: [58][377/408]	Loss 0.0010 (0.0034)	
training:	Epoch: [58][378/408]	Loss 0.0372 (0.0035)	
training:	Epoch: [58][379/408]	Loss 0.0011 (0.0035)	
training:	Epoch: [58][380/408]	Loss 0.0014 (0.0035)	
training:	Epoch: [58][381/408]	Loss 0.0011 (0.0035)	
training:	Epoch: [58][382/408]	Loss 0.0011 (0.0035)	
training:	Epoch: [58][383/408]	Loss 0.0016 (0.0035)	
training:	Epoch: [58][384/408]	Loss 0.0009 (0.0035)	
training:	Epoch: [58][385/408]	Loss 0.0008 (0.0035)	
training:	Epoch: [58][386/408]	Loss 0.0067 (0.0035)	
training:	Epoch: [58][387/408]	Loss 0.0013 (0.0035)	
training:	Epoch: [58][388/408]	Loss 0.0010 (0.0035)	
training:	Epoch: [58][389/408]	Loss 0.0009 (0.0035)	
training:	Epoch: [58][390/408]	Loss 0.0028 (0.0035)	
training:	Epoch: [58][391/408]	Loss 0.0010 (0.0035)	
training:	Epoch: [58][392/408]	Loss 0.0009 (0.0035)	
training:	Epoch: [58][393/408]	Loss 0.0020 (0.0035)	
training:	Epoch: [58][394/408]	Loss 0.1025 (0.0037)	
training:	Epoch: [58][395/408]	Loss 0.0008 (0.0037)	
training:	Epoch: [58][396/408]	Loss 0.0011 (0.0037)	
training:	Epoch: [58][397/408]	Loss 0.0018 (0.0037)	
training:	Epoch: [58][398/408]	Loss 0.0008 (0.0037)	
training:	Epoch: [58][399/408]	Loss 0.0015 (0.0037)	
training:	Epoch: [58][400/408]	Loss 0.0015 (0.0037)	
training:	Epoch: [58][401/408]	Loss 0.0062 (0.0037)	
training:	Epoch: [58][402/408]	Loss 0.0026 (0.0037)	
training:	Epoch: [58][403/408]	Loss 0.2799 (0.0044)	
training:	Epoch: [58][404/408]	Loss 0.0022 (0.0044)	
training:	Epoch: [58][405/408]	Loss 0.0010 (0.0044)	
training:	Epoch: [58][406/408]	Loss 0.0010 (0.0043)	
training:	Epoch: [58][407/408]	Loss 0.0012 (0.0043)	
training:	Epoch: [58][408/408]	Loss 0.0025 (0.0043)	
Training:	 Loss: 0.0043

Training:	 ACC: 1.0000 1.0000 1.0000 1.0000
Validation:	 ACC: 0.7734 0.7742 0.7912 0.7556
Validation:	 Best_BACC: 0.8226 0.8250 0.8772 0.7679
Validation:	 Loss: 1.2196
Pretraining:	Epoch 59/200
----------
training:	Epoch: [59][1/408]	Loss 0.0037 (0.0037)	
training:	Epoch: [59][2/408]	Loss 0.0305 (0.0171)	
training:	Epoch: [59][3/408]	Loss 0.0012 (0.0118)	
training:	Epoch: [59][4/408]	Loss 0.0043 (0.0099)	
training:	Epoch: [59][5/408]	Loss 0.0009 (0.0081)	
training:	Epoch: [59][6/408]	Loss 0.0061 (0.0078)	
training:	Epoch: [59][7/408]	Loss 0.0019 (0.0069)	
training:	Epoch: [59][8/408]	Loss 0.0020 (0.0063)	
training:	Epoch: [59][9/408]	Loss 0.0009 (0.0057)	
training:	Epoch: [59][10/408]	Loss 0.0011 (0.0053)	
training:	Epoch: [59][11/408]	Loss 0.0014 (0.0049)	
training:	Epoch: [59][12/408]	Loss 0.0074 (0.0051)	
training:	Epoch: [59][13/408]	Loss 0.3846 (0.0343)	
training:	Epoch: [59][14/408]	Loss 0.0010 (0.0319)	
training:	Epoch: [59][15/408]	Loss 0.0553 (0.0335)	
training:	Epoch: [59][16/408]	Loss 0.0014 (0.0315)	
training:	Epoch: [59][17/408]	Loss 0.0010 (0.0297)	
training:	Epoch: [59][18/408]	Loss 0.0009 (0.0281)	
training:	Epoch: [59][19/408]	Loss 0.0033 (0.0268)	
training:	Epoch: [59][20/408]	Loss 0.0013 (0.0255)	
training:	Epoch: [59][21/408]	Loss 0.0011 (0.0243)	
training:	Epoch: [59][22/408]	Loss 0.0023 (0.0233)	
training:	Epoch: [59][23/408]	Loss 0.0139 (0.0229)	
training:	Epoch: [59][24/408]	Loss 0.0108 (0.0224)	
training:	Epoch: [59][25/408]	Loss 0.0194 (0.0223)	
training:	Epoch: [59][26/408]	Loss 0.0018 (0.0215)	
training:	Epoch: [59][27/408]	Loss 0.0023 (0.0208)	
training:	Epoch: [59][28/408]	Loss 0.0014 (0.0201)	
training:	Epoch: [59][29/408]	Loss 0.0053 (0.0196)	
training:	Epoch: [59][30/408]	Loss 0.0061 (0.0192)	
training:	Epoch: [59][31/408]	Loss 0.0011 (0.0186)	
training:	Epoch: [59][32/408]	Loss 0.0123 (0.0184)	
training:	Epoch: [59][33/408]	Loss 0.0026 (0.0179)	
training:	Epoch: [59][34/408]	Loss 0.0161 (0.0178)	
training:	Epoch: [59][35/408]	Loss 0.0069 (0.0175)	
training:	Epoch: [59][36/408]	Loss 0.0009 (0.0171)	
training:	Epoch: [59][37/408]	Loss 0.1967 (0.0219)	
training:	Epoch: [59][38/408]	Loss 0.0011 (0.0214)	
training:	Epoch: [59][39/408]	Loss 0.0046 (0.0210)	
training:	Epoch: [59][40/408]	Loss 0.0088 (0.0206)	
training:	Epoch: [59][41/408]	Loss 0.0019 (0.0202)	
training:	Epoch: [59][42/408]	Loss 0.0017 (0.0198)	
training:	Epoch: [59][43/408]	Loss 0.0011 (0.0193)	
training:	Epoch: [59][44/408]	Loss 0.0180 (0.0193)	
training:	Epoch: [59][45/408]	Loss 0.0009 (0.0189)	
training:	Epoch: [59][46/408]	Loss 0.0013 (0.0185)	
training:	Epoch: [59][47/408]	Loss 0.0012 (0.0181)	
training:	Epoch: [59][48/408]	Loss 0.0008 (0.0178)	
training:	Epoch: [59][49/408]	Loss 0.0010 (0.0174)	
training:	Epoch: [59][50/408]	Loss 0.0120 (0.0173)	
training:	Epoch: [59][51/408]	Loss 0.0026 (0.0170)	
training:	Epoch: [59][52/408]	Loss 0.0046 (0.0168)	
training:	Epoch: [59][53/408]	Loss 0.0019 (0.0165)	
training:	Epoch: [59][54/408]	Loss 0.0011 (0.0162)	
training:	Epoch: [59][55/408]	Loss 0.0033 (0.0160)	
training:	Epoch: [59][56/408]	Loss 0.1184 (0.0178)	
training:	Epoch: [59][57/408]	Loss 0.0010 (0.0175)	
training:	Epoch: [59][58/408]	Loss 0.0013 (0.0172)	
training:	Epoch: [59][59/408]	Loss 0.0015 (0.0170)	
training:	Epoch: [59][60/408]	Loss 0.0011 (0.0167)	
training:	Epoch: [59][61/408]	Loss 0.0013 (0.0165)	
training:	Epoch: [59][62/408]	Loss 0.0011 (0.0162)	
training:	Epoch: [59][63/408]	Loss 0.0008 (0.0160)	
training:	Epoch: [59][64/408]	Loss 0.0008 (0.0157)	
training:	Epoch: [59][65/408]	Loss 0.0015 (0.0155)	
training:	Epoch: [59][66/408]	Loss 0.0013 (0.0153)	
training:	Epoch: [59][67/408]	Loss 0.0011 (0.0151)	
training:	Epoch: [59][68/408]	Loss 0.0008 (0.0149)	
training:	Epoch: [59][69/408]	Loss 0.0010 (0.0147)	
training:	Epoch: [59][70/408]	Loss 0.0012 (0.0145)	
training:	Epoch: [59][71/408]	Loss 0.0012 (0.0143)	
training:	Epoch: [59][72/408]	Loss 0.0029 (0.0141)	
training:	Epoch: [59][73/408]	Loss 0.0009 (0.0140)	
training:	Epoch: [59][74/408]	Loss 0.0010 (0.0138)	
training:	Epoch: [59][75/408]	Loss 0.0017 (0.0136)	
training:	Epoch: [59][76/408]	Loss 0.0016 (0.0135)	
training:	Epoch: [59][77/408]	Loss 0.0008 (0.0133)	
training:	Epoch: [59][78/408]	Loss 0.0014 (0.0131)	
training:	Epoch: [59][79/408]	Loss 0.0011 (0.0130)	
training:	Epoch: [59][80/408]	Loss 0.0036 (0.0129)	
training:	Epoch: [59][81/408]	Loss 0.0011 (0.0127)	
training:	Epoch: [59][82/408]	Loss 0.1333 (0.0142)	
training:	Epoch: [59][83/408]	Loss 0.0010 (0.0140)	
training:	Epoch: [59][84/408]	Loss 0.0009 (0.0139)	
training:	Epoch: [59][85/408]	Loss 0.1216 (0.0151)	
training:	Epoch: [59][86/408]	Loss 0.0011 (0.0150)	
training:	Epoch: [59][87/408]	Loss 0.0124 (0.0150)	
training:	Epoch: [59][88/408]	Loss 0.0010 (0.0148)	
training:	Epoch: [59][89/408]	Loss 0.0015 (0.0146)	
training:	Epoch: [59][90/408]	Loss 0.0010 (0.0145)	
training:	Epoch: [59][91/408]	Loss 0.0044 (0.0144)	
training:	Epoch: [59][92/408]	Loss 0.0012 (0.0142)	
training:	Epoch: [59][93/408]	Loss 0.0016 (0.0141)	
training:	Epoch: [59][94/408]	Loss 0.0011 (0.0140)	
training:	Epoch: [59][95/408]	Loss 0.0021 (0.0138)	
training:	Epoch: [59][96/408]	Loss 0.0102 (0.0138)	
training:	Epoch: [59][97/408]	Loss 0.0015 (0.0137)	
training:	Epoch: [59][98/408]	Loss 0.0014 (0.0136)	
training:	Epoch: [59][99/408]	Loss 0.0022 (0.0134)	
training:	Epoch: [59][100/408]	Loss 0.0010 (0.0133)	
training:	Epoch: [59][101/408]	Loss 0.0039 (0.0132)	
training:	Epoch: [59][102/408]	Loss 0.0012 (0.0131)	
training:	Epoch: [59][103/408]	Loss 0.0027 (0.0130)	
training:	Epoch: [59][104/408]	Loss 0.0027 (0.0129)	
training:	Epoch: [59][105/408]	Loss 0.0026 (0.0128)	
training:	Epoch: [59][106/408]	Loss 0.0020 (0.0127)	
training:	Epoch: [59][107/408]	Loss 0.0023 (0.0126)	
training:	Epoch: [59][108/408]	Loss 0.0010 (0.0125)	
training:	Epoch: [59][109/408]	Loss 0.0015 (0.0124)	
training:	Epoch: [59][110/408]	Loss 0.0014 (0.0123)	
training:	Epoch: [59][111/408]	Loss 0.0054 (0.0122)	
training:	Epoch: [59][112/408]	Loss 0.0017 (0.0121)	
training:	Epoch: [59][113/408]	Loss 0.0012 (0.0120)	
training:	Epoch: [59][114/408]	Loss 0.0012 (0.0119)	
training:	Epoch: [59][115/408]	Loss 0.0014 (0.0119)	
training:	Epoch: [59][116/408]	Loss 0.0053 (0.0118)	
training:	Epoch: [59][117/408]	Loss 0.0095 (0.0118)	
training:	Epoch: [59][118/408]	Loss 0.0012 (0.0117)	
training:	Epoch: [59][119/408]	Loss 0.0013 (0.0116)	
training:	Epoch: [59][120/408]	Loss 0.0011 (0.0115)	
training:	Epoch: [59][121/408]	Loss 0.0027 (0.0114)	
training:	Epoch: [59][122/408]	Loss 0.0085 (0.0114)	
training:	Epoch: [59][123/408]	Loss 0.0010 (0.0113)	
training:	Epoch: [59][124/408]	Loss 0.0022 (0.0113)	
training:	Epoch: [59][125/408]	Loss 0.0011 (0.0112)	
training:	Epoch: [59][126/408]	Loss 0.0013 (0.0111)	
training:	Epoch: [59][127/408]	Loss 0.0010 (0.0110)	
training:	Epoch: [59][128/408]	Loss 0.0012 (0.0109)	
training:	Epoch: [59][129/408]	Loss 0.0072 (0.0109)	
training:	Epoch: [59][130/408]	Loss 0.0054 (0.0109)	
training:	Epoch: [59][131/408]	Loss 0.0010 (0.0108)	
training:	Epoch: [59][132/408]	Loss 0.0009 (0.0107)	
training:	Epoch: [59][133/408]	Loss 0.0017 (0.0107)	
training:	Epoch: [59][134/408]	Loss 0.0011 (0.0106)	
training:	Epoch: [59][135/408]	Loss 0.0046 (0.0105)	
training:	Epoch: [59][136/408]	Loss 0.0011 (0.0105)	
training:	Epoch: [59][137/408]	Loss 0.0013 (0.0104)	
training:	Epoch: [59][138/408]	Loss 0.0010 (0.0103)	
training:	Epoch: [59][139/408]	Loss 0.0013 (0.0103)	
training:	Epoch: [59][140/408]	Loss 0.0045 (0.0102)	
training:	Epoch: [59][141/408]	Loss 0.0008 (0.0102)	
training:	Epoch: [59][142/408]	Loss 0.0010 (0.0101)	
training:	Epoch: [59][143/408]	Loss 0.0009 (0.0100)	
training:	Epoch: [59][144/408]	Loss 0.0016 (0.0100)	
training:	Epoch: [59][145/408]	Loss 0.0010 (0.0099)	
training:	Epoch: [59][146/408]	Loss 0.0013 (0.0099)	
training:	Epoch: [59][147/408]	Loss 0.0013 (0.0098)	
training:	Epoch: [59][148/408]	Loss 0.0014 (0.0097)	
training:	Epoch: [59][149/408]	Loss 0.0927 (0.0103)	
training:	Epoch: [59][150/408]	Loss 0.0011 (0.0102)	
training:	Epoch: [59][151/408]	Loss 0.0015 (0.0102)	
training:	Epoch: [59][152/408]	Loss 0.0009 (0.0101)	
training:	Epoch: [59][153/408]	Loss 0.0028 (0.0101)	
training:	Epoch: [59][154/408]	Loss 0.0034 (0.0100)	
training:	Epoch: [59][155/408]	Loss 0.0013 (0.0100)	
training:	Epoch: [59][156/408]	Loss 0.0048 (0.0099)	
training:	Epoch: [59][157/408]	Loss 0.0013 (0.0099)	
training:	Epoch: [59][158/408]	Loss 0.0010 (0.0098)	
training:	Epoch: [59][159/408]	Loss 0.0258 (0.0099)	
training:	Epoch: [59][160/408]	Loss 0.0024 (0.0099)	
training:	Epoch: [59][161/408]	Loss 0.0009 (0.0098)	
training:	Epoch: [59][162/408]	Loss 0.0022 (0.0098)	
training:	Epoch: [59][163/408]	Loss 0.0483 (0.0100)	
training:	Epoch: [59][164/408]	Loss 0.0011 (0.0100)	
training:	Epoch: [59][165/408]	Loss 0.0008 (0.0099)	
training:	Epoch: [59][166/408]	Loss 0.0022 (0.0099)	
training:	Epoch: [59][167/408]	Loss 0.0013 (0.0098)	
training:	Epoch: [59][168/408]	Loss 0.0009 (0.0097)	
training:	Epoch: [59][169/408]	Loss 0.0605 (0.0100)	
training:	Epoch: [59][170/408]	Loss 0.0014 (0.0100)	
training:	Epoch: [59][171/408]	Loss 0.0017 (0.0100)	
training:	Epoch: [59][172/408]	Loss 0.0021 (0.0099)	
training:	Epoch: [59][173/408]	Loss 0.0009 (0.0099)	
training:	Epoch: [59][174/408]	Loss 0.0012 (0.0098)	
training:	Epoch: [59][175/408]	Loss 0.0013 (0.0098)	
training:	Epoch: [59][176/408]	Loss 0.0009 (0.0097)	
training:	Epoch: [59][177/408]	Loss 0.0015 (0.0097)	
training:	Epoch: [59][178/408]	Loss 0.0008 (0.0096)	
training:	Epoch: [59][179/408]	Loss 0.2626 (0.0110)	
training:	Epoch: [59][180/408]	Loss 0.0028 (0.0110)	
training:	Epoch: [59][181/408]	Loss 0.0014 (0.0109)	
training:	Epoch: [59][182/408]	Loss 0.0013 (0.0109)	
training:	Epoch: [59][183/408]	Loss 0.0008 (0.0108)	
training:	Epoch: [59][184/408]	Loss 0.0032 (0.0108)	
training:	Epoch: [59][185/408]	Loss 0.0009 (0.0107)	
training:	Epoch: [59][186/408]	Loss 0.0010 (0.0107)	
training:	Epoch: [59][187/408]	Loss 0.0013 (0.0106)	
training:	Epoch: [59][188/408]	Loss 0.0009 (0.0106)	
training:	Epoch: [59][189/408]	Loss 0.0015 (0.0105)	
training:	Epoch: [59][190/408]	Loss 0.0212 (0.0106)	
training:	Epoch: [59][191/408]	Loss 0.0141 (0.0106)	
training:	Epoch: [59][192/408]	Loss 0.0076 (0.0106)	
training:	Epoch: [59][193/408]	Loss 0.0049 (0.0105)	
training:	Epoch: [59][194/408]	Loss 0.0015 (0.0105)	
training:	Epoch: [59][195/408]	Loss 0.0011 (0.0105)	
training:	Epoch: [59][196/408]	Loss 0.0014 (0.0104)	
training:	Epoch: [59][197/408]	Loss 0.0013 (0.0104)	
training:	Epoch: [59][198/408]	Loss 0.0010 (0.0103)	
training:	Epoch: [59][199/408]	Loss 0.0016 (0.0103)	
training:	Epoch: [59][200/408]	Loss 0.0011 (0.0102)	
training:	Epoch: [59][201/408]	Loss 0.0023 (0.0102)	
training:	Epoch: [59][202/408]	Loss 0.0112 (0.0102)	
training:	Epoch: [59][203/408]	Loss 0.0014 (0.0101)	
training:	Epoch: [59][204/408]	Loss 0.0009 (0.0101)	
training:	Epoch: [59][205/408]	Loss 0.0203 (0.0102)	
training:	Epoch: [59][206/408]	Loss 0.0010 (0.0101)	
training:	Epoch: [59][207/408]	Loss 0.0010 (0.0101)	
training:	Epoch: [59][208/408]	Loss 0.0014 (0.0100)	
training:	Epoch: [59][209/408]	Loss 0.0019 (0.0100)	
training:	Epoch: [59][210/408]	Loss 0.0012 (0.0099)	
training:	Epoch: [59][211/408]	Loss 0.0012 (0.0099)	
training:	Epoch: [59][212/408]	Loss 0.0014 (0.0099)	
training:	Epoch: [59][213/408]	Loss 0.0010 (0.0098)	
training:	Epoch: [59][214/408]	Loss 0.0010 (0.0098)	
training:	Epoch: [59][215/408]	Loss 0.0046 (0.0098)	
training:	Epoch: [59][216/408]	Loss 0.1874 (0.0106)	
training:	Epoch: [59][217/408]	Loss 0.0038 (0.0105)	
training:	Epoch: [59][218/408]	Loss 0.0034 (0.0105)	
training:	Epoch: [59][219/408]	Loss 0.0020 (0.0105)	
training:	Epoch: [59][220/408]	Loss 0.0010 (0.0104)	
training:	Epoch: [59][221/408]	Loss 0.0009 (0.0104)	
training:	Epoch: [59][222/408]	Loss 0.0012 (0.0103)	
training:	Epoch: [59][223/408]	Loss 0.0330 (0.0104)	
training:	Epoch: [59][224/408]	Loss 0.0010 (0.0104)	
training:	Epoch: [59][225/408]	Loss 0.0009 (0.0104)	
training:	Epoch: [59][226/408]	Loss 0.0125 (0.0104)	
training:	Epoch: [59][227/408]	Loss 0.0013 (0.0103)	
training:	Epoch: [59][228/408]	Loss 0.0018 (0.0103)	
training:	Epoch: [59][229/408]	Loss 0.0010 (0.0103)	
training:	Epoch: [59][230/408]	Loss 0.0018 (0.0102)	
training:	Epoch: [59][231/408]	Loss 0.0256 (0.0103)	
training:	Epoch: [59][232/408]	Loss 0.0194 (0.0103)	
training:	Epoch: [59][233/408]	Loss 0.0023 (0.0103)	
training:	Epoch: [59][234/408]	Loss 0.0011 (0.0102)	
training:	Epoch: [59][235/408]	Loss 0.0048 (0.0102)	
training:	Epoch: [59][236/408]	Loss 0.0071 (0.0102)	
training:	Epoch: [59][237/408]	Loss 0.0020 (0.0102)	
training:	Epoch: [59][238/408]	Loss 0.0016 (0.0101)	
training:	Epoch: [59][239/408]	Loss 0.0013 (0.0101)	
training:	Epoch: [59][240/408]	Loss 0.0010 (0.0101)	
training:	Epoch: [59][241/408]	Loss 0.0011 (0.0100)	
training:	Epoch: [59][242/408]	Loss 0.0011 (0.0100)	
training:	Epoch: [59][243/408]	Loss 0.0013 (0.0100)	
training:	Epoch: [59][244/408]	Loss 0.0011 (0.0099)	
training:	Epoch: [59][245/408]	Loss 0.0083 (0.0099)	
training:	Epoch: [59][246/408]	Loss 0.0113 (0.0099)	
training:	Epoch: [59][247/408]	Loss 0.0010 (0.0099)	
training:	Epoch: [59][248/408]	Loss 0.0063 (0.0099)	
training:	Epoch: [59][249/408]	Loss 0.0378 (0.0100)	
training:	Epoch: [59][250/408]	Loss 0.0009 (0.0099)	
training:	Epoch: [59][251/408]	Loss 0.0009 (0.0099)	
training:	Epoch: [59][252/408]	Loss 0.0011 (0.0099)	
training:	Epoch: [59][253/408]	Loss 0.0009 (0.0098)	
training:	Epoch: [59][254/408]	Loss 0.0213 (0.0099)	
training:	Epoch: [59][255/408]	Loss 0.0020 (0.0099)	
training:	Epoch: [59][256/408]	Loss 0.0040 (0.0098)	
training:	Epoch: [59][257/408]	Loss 0.0991 (0.0102)	
training:	Epoch: [59][258/408]	Loss 0.0009 (0.0101)	
training:	Epoch: [59][259/408]	Loss 0.0026 (0.0101)	
training:	Epoch: [59][260/408]	Loss 0.0013 (0.0101)	
training:	Epoch: [59][261/408]	Loss 0.0009 (0.0100)	
training:	Epoch: [59][262/408]	Loss 0.0066 (0.0100)	
training:	Epoch: [59][263/408]	Loss 0.0014 (0.0100)	
training:	Epoch: [59][264/408]	Loss 0.0017 (0.0100)	
training:	Epoch: [59][265/408]	Loss 0.0096 (0.0100)	
training:	Epoch: [59][266/408]	Loss 0.0049 (0.0099)	
training:	Epoch: [59][267/408]	Loss 0.0014 (0.0099)	
training:	Epoch: [59][268/408]	Loss 0.0017 (0.0099)	
training:	Epoch: [59][269/408]	Loss 0.0932 (0.0102)	
training:	Epoch: [59][270/408]	Loss 0.0023 (0.0102)	
training:	Epoch: [59][271/408]	Loss 0.0994 (0.0105)	
training:	Epoch: [59][272/408]	Loss 0.0256 (0.0105)	
training:	Epoch: [59][273/408]	Loss 0.1882 (0.0112)	
training:	Epoch: [59][274/408]	Loss 0.0009 (0.0112)	
training:	Epoch: [59][275/408]	Loss 0.0317 (0.0112)	
training:	Epoch: [59][276/408]	Loss 0.0011 (0.0112)	
training:	Epoch: [59][277/408]	Loss 0.0014 (0.0112)	
training:	Epoch: [59][278/408]	Loss 0.0014 (0.0111)	
training:	Epoch: [59][279/408]	Loss 0.0019 (0.0111)	
training:	Epoch: [59][280/408]	Loss 0.0080 (0.0111)	
training:	Epoch: [59][281/408]	Loss 0.0023 (0.0111)	
training:	Epoch: [59][282/408]	Loss 0.3865 (0.0124)	
training:	Epoch: [59][283/408]	Loss 0.2145 (0.0131)	
training:	Epoch: [59][284/408]	Loss 0.0010 (0.0131)	
training:	Epoch: [59][285/408]	Loss 0.0070 (0.0130)	
training:	Epoch: [59][286/408]	Loss 0.0515 (0.0132)	
training:	Epoch: [59][287/408]	Loss 0.0024 (0.0131)	
training:	Epoch: [59][288/408]	Loss 0.1215 (0.0135)	
training:	Epoch: [59][289/408]	Loss 0.0018 (0.0135)	
training:	Epoch: [59][290/408]	Loss 0.0020 (0.0134)	
training:	Epoch: [59][291/408]	Loss 0.0611 (0.0136)	
training:	Epoch: [59][292/408]	Loss 0.0011 (0.0135)	
training:	Epoch: [59][293/408]	Loss 0.0030 (0.0135)	
training:	Epoch: [59][294/408]	Loss 0.0253 (0.0136)	
training:	Epoch: [59][295/408]	Loss 0.3472 (0.0147)	
training:	Epoch: [59][296/408]	Loss 0.0595 (0.0148)	
training:	Epoch: [59][297/408]	Loss 0.0106 (0.0148)	
training:	Epoch: [59][298/408]	Loss 0.0141 (0.0148)	
training:	Epoch: [59][299/408]	Loss 0.0897 (0.0151)	
training:	Epoch: [59][300/408]	Loss 0.0019 (0.0150)	
training:	Epoch: [59][301/408]	Loss 0.0013 (0.0150)	
training:	Epoch: [59][302/408]	Loss 0.0120 (0.0150)	
training:	Epoch: [59][303/408]	Loss 0.0027 (0.0149)	
training:	Epoch: [59][304/408]	Loss 0.0008 (0.0149)	
training:	Epoch: [59][305/408]	Loss 0.0640 (0.0150)	
training:	Epoch: [59][306/408]	Loss 0.0010 (0.0150)	
training:	Epoch: [59][307/408]	Loss 0.0021 (0.0150)	
training:	Epoch: [59][308/408]	Loss 0.0014 (0.0149)	
training:	Epoch: [59][309/408]	Loss 0.0248 (0.0149)	
training:	Epoch: [59][310/408]	Loss 0.0012 (0.0149)	
training:	Epoch: [59][311/408]	Loss 0.0027 (0.0149)	
training:	Epoch: [59][312/408]	Loss 0.0012 (0.0148)	
training:	Epoch: [59][313/408]	Loss 0.0065 (0.0148)	
training:	Epoch: [59][314/408]	Loss 0.0034 (0.0148)	
training:	Epoch: [59][315/408]	Loss 0.0289 (0.0148)	
training:	Epoch: [59][316/408]	Loss 0.0125 (0.0148)	
training:	Epoch: [59][317/408]	Loss 0.0129 (0.0148)	
training:	Epoch: [59][318/408]	Loss 0.0009 (0.0147)	
training:	Epoch: [59][319/408]	Loss 0.0019 (0.0147)	
training:	Epoch: [59][320/408]	Loss 0.0010 (0.0147)	
training:	Epoch: [59][321/408]	Loss 0.0024 (0.0146)	
training:	Epoch: [59][322/408]	Loss 0.0043 (0.0146)	
training:	Epoch: [59][323/408]	Loss 0.0010 (0.0145)	
training:	Epoch: [59][324/408]	Loss 0.0041 (0.0145)	
training:	Epoch: [59][325/408]	Loss 0.0011 (0.0145)	
training:	Epoch: [59][326/408]	Loss 0.0011 (0.0144)	
training:	Epoch: [59][327/408]	Loss 0.0012 (0.0144)	
training:	Epoch: [59][328/408]	Loss 0.0015 (0.0144)	
training:	Epoch: [59][329/408]	Loss 0.0059 (0.0143)	
training:	Epoch: [59][330/408]	Loss 0.0009 (0.0143)	
training:	Epoch: [59][331/408]	Loss 0.0018 (0.0142)	
training:	Epoch: [59][332/408]	Loss 0.0011 (0.0142)	
training:	Epoch: [59][333/408]	Loss 0.0009 (0.0142)	
training:	Epoch: [59][334/408]	Loss 0.0042 (0.0141)	
training:	Epoch: [59][335/408]	Loss 0.0010 (0.0141)	
training:	Epoch: [59][336/408]	Loss 0.0215 (0.0141)	
training:	Epoch: [59][337/408]	Loss 0.0013 (0.0141)	
training:	Epoch: [59][338/408]	Loss 0.0012 (0.0140)	
training:	Epoch: [59][339/408]	Loss 0.0010 (0.0140)	
training:	Epoch: [59][340/408]	Loss 0.1133 (0.0143)	
training:	Epoch: [59][341/408]	Loss 0.0331 (0.0144)	
training:	Epoch: [59][342/408]	Loss 0.0011 (0.0143)	
training:	Epoch: [59][343/408]	Loss 0.0010 (0.0143)	
training:	Epoch: [59][344/408]	Loss 0.0012 (0.0142)	
training:	Epoch: [59][345/408]	Loss 0.0010 (0.0142)	
training:	Epoch: [59][346/408]	Loss 0.0015 (0.0142)	
training:	Epoch: [59][347/408]	Loss 0.0437 (0.0142)	
training:	Epoch: [59][348/408]	Loss 0.0093 (0.0142)	
training:	Epoch: [59][349/408]	Loss 0.0010 (0.0142)	
training:	Epoch: [59][350/408]	Loss 0.0021 (0.0142)	
training:	Epoch: [59][351/408]	Loss 0.0050 (0.0141)	
training:	Epoch: [59][352/408]	Loss 0.0010 (0.0141)	
training:	Epoch: [59][353/408]	Loss 0.0011 (0.0141)	
training:	Epoch: [59][354/408]	Loss 0.0124 (0.0141)	
training:	Epoch: [59][355/408]	Loss 0.0130 (0.0141)	
training:	Epoch: [59][356/408]	Loss 0.0015 (0.0140)	
training:	Epoch: [59][357/408]	Loss 0.0041 (0.0140)	
training:	Epoch: [59][358/408]	Loss 0.0010 (0.0140)	
training:	Epoch: [59][359/408]	Loss 0.0008 (0.0139)	
training:	Epoch: [59][360/408]	Loss 0.1871 (0.0144)	
training:	Epoch: [59][361/408]	Loss 0.0012 (0.0144)	
training:	Epoch: [59][362/408]	Loss 0.0010 (0.0143)	
training:	Epoch: [59][363/408]	Loss 0.0033 (0.0143)	
training:	Epoch: [59][364/408]	Loss 0.0104 (0.0143)	
training:	Epoch: [59][365/408]	Loss 0.0012 (0.0142)	
training:	Epoch: [59][366/408]	Loss 0.0015 (0.0142)	
training:	Epoch: [59][367/408]	Loss 0.0019 (0.0142)	
training:	Epoch: [59][368/408]	Loss 0.0024 (0.0141)	
training:	Epoch: [59][369/408]	Loss 0.0018 (0.0141)	
training:	Epoch: [59][370/408]	Loss 0.0024 (0.0141)	
training:	Epoch: [59][371/408]	Loss 0.0014 (0.0140)	
training:	Epoch: [59][372/408]	Loss 0.0010 (0.0140)	
training:	Epoch: [59][373/408]	Loss 0.0043 (0.0140)	
training:	Epoch: [59][374/408]	Loss 0.0011 (0.0140)	
training:	Epoch: [59][375/408]	Loss 0.0018 (0.0139)	
training:	Epoch: [59][376/408]	Loss 0.0020 (0.0139)	
training:	Epoch: [59][377/408]	Loss 0.0022 (0.0139)	
training:	Epoch: [59][378/408]	Loss 0.0012 (0.0138)	
training:	Epoch: [59][379/408]	Loss 0.0049 (0.0138)	
training:	Epoch: [59][380/408]	Loss 0.0055 (0.0138)	
training:	Epoch: [59][381/408]	Loss 0.0035 (0.0138)	
training:	Epoch: [59][382/408]	Loss 0.0024 (0.0137)	
training:	Epoch: [59][383/408]	Loss 0.0009 (0.0137)	
training:	Epoch: [59][384/408]	Loss 0.0387 (0.0138)	
training:	Epoch: [59][385/408]	Loss 0.0008 (0.0137)	
training:	Epoch: [59][386/408]	Loss 0.0012 (0.0137)	
training:	Epoch: [59][387/408]	Loss 0.0010 (0.0137)	
training:	Epoch: [59][388/408]	Loss 0.0009 (0.0136)	
training:	Epoch: [59][389/408]	Loss 0.0069 (0.0136)	
training:	Epoch: [59][390/408]	Loss 0.0019 (0.0136)	
training:	Epoch: [59][391/408]	Loss 0.0009 (0.0135)	
training:	Epoch: [59][392/408]	Loss 0.0082 (0.0135)	
training:	Epoch: [59][393/408]	Loss 0.0014 (0.0135)	
training:	Epoch: [59][394/408]	Loss 0.0343 (0.0136)	
training:	Epoch: [59][395/408]	Loss 0.0009 (0.0135)	
training:	Epoch: [59][396/408]	Loss 0.0747 (0.0137)	
training:	Epoch: [59][397/408]	Loss 0.0018 (0.0136)	
training:	Epoch: [59][398/408]	Loss 0.0064 (0.0136)	
training:	Epoch: [59][399/408]	Loss 0.0011 (0.0136)	
training:	Epoch: [59][400/408]	Loss 0.0018 (0.0136)	
training:	Epoch: [59][401/408]	Loss 0.0013 (0.0135)	
training:	Epoch: [59][402/408]	Loss 0.0011 (0.0135)	
training:	Epoch: [59][403/408]	Loss 0.0016 (0.0135)	
training:	Epoch: [59][404/408]	Loss 0.0016 (0.0134)	
training:	Epoch: [59][405/408]	Loss 0.0010 (0.0134)	
training:	Epoch: [59][406/408]	Loss 0.0013 (0.0134)	
training:	Epoch: [59][407/408]	Loss 0.0008 (0.0134)	
training:	Epoch: [59][408/408]	Loss 0.0011 (0.0133)	
Training:	 Loss: 0.0133

Training:	 ACC: 0.9991 0.9991 0.9982 1.0000
Validation:	 ACC: 0.7883 0.7871 0.7605 0.8161
Validation:	 Best_BACC: 0.8226 0.8250 0.8772 0.7679
Validation:	 Loss: 1.1454
Pretraining:	Epoch 60/200
----------
training:	Epoch: [60][1/408]	Loss 0.0012 (0.0012)	
training:	Epoch: [60][2/408]	Loss 0.0012 (0.0012)	
training:	Epoch: [60][3/408]	Loss 0.0009 (0.0011)	
training:	Epoch: [60][4/408]	Loss 0.0011 (0.0011)	
training:	Epoch: [60][5/408]	Loss 0.0012 (0.0011)	
training:	Epoch: [60][6/408]	Loss 0.0009 (0.0011)	
training:	Epoch: [60][7/408]	Loss 0.0069 (0.0019)	
training:	Epoch: [60][8/408]	Loss 0.0013 (0.0018)	
training:	Epoch: [60][9/408]	Loss 0.0014 (0.0018)	
training:	Epoch: [60][10/408]	Loss 0.0009 (0.0017)	
training:	Epoch: [60][11/408]	Loss 0.0019 (0.0017)	
training:	Epoch: [60][12/408]	Loss 0.0011 (0.0017)	
training:	Epoch: [60][13/408]	Loss 0.0022 (0.0017)	
training:	Epoch: [60][14/408]	Loss 0.0032 (0.0018)	
training:	Epoch: [60][15/408]	Loss 0.0011 (0.0018)	
training:	Epoch: [60][16/408]	Loss 0.0012 (0.0017)	
training:	Epoch: [60][17/408]	Loss 0.0085 (0.0021)	
training:	Epoch: [60][18/408]	Loss 0.0017 (0.0021)	
training:	Epoch: [60][19/408]	Loss 0.0023 (0.0021)	
training:	Epoch: [60][20/408]	Loss 0.0011 (0.0021)	
training:	Epoch: [60][21/408]	Loss 0.0018 (0.0020)	
training:	Epoch: [60][22/408]	Loss 0.0073 (0.0023)	
training:	Epoch: [60][23/408]	Loss 0.0105 (0.0026)	
training:	Epoch: [60][24/408]	Loss 0.0015 (0.0026)	
training:	Epoch: [60][25/408]	Loss 0.0015 (0.0025)	
training:	Epoch: [60][26/408]	Loss 0.0035 (0.0026)	
training:	Epoch: [60][27/408]	Loss 0.0013 (0.0025)	
training:	Epoch: [60][28/408]	Loss 0.0031 (0.0026)	
training:	Epoch: [60][29/408]	Loss 0.0012 (0.0025)	
training:	Epoch: [60][30/408]	Loss 0.0015 (0.0025)	
training:	Epoch: [60][31/408]	Loss 0.0010 (0.0024)	
training:	Epoch: [60][32/408]	Loss 0.0075 (0.0026)	
training:	Epoch: [60][33/408]	Loss 0.0012 (0.0025)	
training:	Epoch: [60][34/408]	Loss 0.0010 (0.0025)	
training:	Epoch: [60][35/408]	Loss 0.0144 (0.0028)	
training:	Epoch: [60][36/408]	Loss 0.0011 (0.0028)	
training:	Epoch: [60][37/408]	Loss 0.0036 (0.0028)	
training:	Epoch: [60][38/408]	Loss 0.0010 (0.0028)	
training:	Epoch: [60][39/408]	Loss 0.0012 (0.0027)	
training:	Epoch: [60][40/408]	Loss 0.0061 (0.0028)	
training:	Epoch: [60][41/408]	Loss 0.0011 (0.0028)	
training:	Epoch: [60][42/408]	Loss 0.0016 (0.0027)	
training:	Epoch: [60][43/408]	Loss 0.0014 (0.0027)	
training:	Epoch: [60][44/408]	Loss 0.0052 (0.0028)	
training:	Epoch: [60][45/408]	Loss 0.0009 (0.0027)	
training:	Epoch: [60][46/408]	Loss 0.0028 (0.0027)	
training:	Epoch: [60][47/408]	Loss 0.0145 (0.0030)	
training:	Epoch: [60][48/408]	Loss 0.0012 (0.0029)	
training:	Epoch: [60][49/408]	Loss 0.0119 (0.0031)	
training:	Epoch: [60][50/408]	Loss 0.0009 (0.0031)	
training:	Epoch: [60][51/408]	Loss 0.0016 (0.0030)	
training:	Epoch: [60][52/408]	Loss 0.0013 (0.0030)	
training:	Epoch: [60][53/408]	Loss 0.0017 (0.0030)	
training:	Epoch: [60][54/408]	Loss 0.0018 (0.0030)	
training:	Epoch: [60][55/408]	Loss 0.0058 (0.0030)	
training:	Epoch: [60][56/408]	Loss 0.0012 (0.0030)	
training:	Epoch: [60][57/408]	Loss 0.0047 (0.0030)	
training:	Epoch: [60][58/408]	Loss 0.0141 (0.0032)	
training:	Epoch: [60][59/408]	Loss 0.0010 (0.0032)	
training:	Epoch: [60][60/408]	Loss 0.0008 (0.0031)	
training:	Epoch: [60][61/408]	Loss 0.0784 (0.0044)	
training:	Epoch: [60][62/408]	Loss 0.0025 (0.0043)	
training:	Epoch: [60][63/408]	Loss 0.0014 (0.0043)	
training:	Epoch: [60][64/408]	Loss 0.0010 (0.0042)	
training:	Epoch: [60][65/408]	Loss 0.0018 (0.0042)	
training:	Epoch: [60][66/408]	Loss 0.0011 (0.0042)	
training:	Epoch: [60][67/408]	Loss 0.0013 (0.0041)	
training:	Epoch: [60][68/408]	Loss 0.0057 (0.0041)	
training:	Epoch: [60][69/408]	Loss 0.0009 (0.0041)	
training:	Epoch: [60][70/408]	Loss 0.0019 (0.0041)	
training:	Epoch: [60][71/408]	Loss 0.0011 (0.0040)	
training:	Epoch: [60][72/408]	Loss 0.0018 (0.0040)	
training:	Epoch: [60][73/408]	Loss 0.0012 (0.0039)	
training:	Epoch: [60][74/408]	Loss 0.0009 (0.0039)	
training:	Epoch: [60][75/408]	Loss 0.0105 (0.0040)	
training:	Epoch: [60][76/408]	Loss 0.0013 (0.0040)	
training:	Epoch: [60][77/408]	Loss 0.0506 (0.0046)	
training:	Epoch: [60][78/408]	Loss 0.0012 (0.0045)	
training:	Epoch: [60][79/408]	Loss 0.0028 (0.0045)	
training:	Epoch: [60][80/408]	Loss 0.0013 (0.0045)	
training:	Epoch: [60][81/408]	Loss 0.0013 (0.0044)	
training:	Epoch: [60][82/408]	Loss 0.0010 (0.0044)	
training:	Epoch: [60][83/408]	Loss 0.0017 (0.0043)	
training:	Epoch: [60][84/408]	Loss 0.0010 (0.0043)	
training:	Epoch: [60][85/408]	Loss 0.0010 (0.0043)	
training:	Epoch: [60][86/408]	Loss 0.0008 (0.0042)	
training:	Epoch: [60][87/408]	Loss 0.0012 (0.0042)	
training:	Epoch: [60][88/408]	Loss 0.0015 (0.0042)	
training:	Epoch: [60][89/408]	Loss 0.0009 (0.0041)	
training:	Epoch: [60][90/408]	Loss 0.0088 (0.0042)	
training:	Epoch: [60][91/408]	Loss 0.0010 (0.0041)	
training:	Epoch: [60][92/408]	Loss 0.0009 (0.0041)	
training:	Epoch: [60][93/408]	Loss 0.0013 (0.0041)	
training:	Epoch: [60][94/408]	Loss 0.0016 (0.0040)	
training:	Epoch: [60][95/408]	Loss 0.0010 (0.0040)	
training:	Epoch: [60][96/408]	Loss 0.0008 (0.0040)	
training:	Epoch: [60][97/408]	Loss 0.0105 (0.0040)	
training:	Epoch: [60][98/408]	Loss 0.0009 (0.0040)	
training:	Epoch: [60][99/408]	Loss 0.0008 (0.0040)	
training:	Epoch: [60][100/408]	Loss 0.0009 (0.0040)	
training:	Epoch: [60][101/408]	Loss 0.0010 (0.0039)	
training:	Epoch: [60][102/408]	Loss 0.0028 (0.0039)	
training:	Epoch: [60][103/408]	Loss 0.0064 (0.0039)	
training:	Epoch: [60][104/408]	Loss 0.0014 (0.0039)	
training:	Epoch: [60][105/408]	Loss 0.0009 (0.0039)	
training:	Epoch: [60][106/408]	Loss 0.0010 (0.0039)	
training:	Epoch: [60][107/408]	Loss 0.0010 (0.0038)	
training:	Epoch: [60][108/408]	Loss 0.0011 (0.0038)	
training:	Epoch: [60][109/408]	Loss 0.0012 (0.0038)	
training:	Epoch: [60][110/408]	Loss 0.0008 (0.0038)	
training:	Epoch: [60][111/408]	Loss 0.0059 (0.0038)	
training:	Epoch: [60][112/408]	Loss 0.0010 (0.0037)	
training:	Epoch: [60][113/408]	Loss 0.0024 (0.0037)	
training:	Epoch: [60][114/408]	Loss 0.0011 (0.0037)	
training:	Epoch: [60][115/408]	Loss 0.0008 (0.0037)	
training:	Epoch: [60][116/408]	Loss 0.0012 (0.0037)	
training:	Epoch: [60][117/408]	Loss 0.0009 (0.0036)	
training:	Epoch: [60][118/408]	Loss 0.0012 (0.0036)	
training:	Epoch: [60][119/408]	Loss 0.0781 (0.0042)	
training:	Epoch: [60][120/408]	Loss 0.0104 (0.0043)	
training:	Epoch: [60][121/408]	Loss 0.0019 (0.0043)	
training:	Epoch: [60][122/408]	Loss 0.0011 (0.0043)	
training:	Epoch: [60][123/408]	Loss 0.0014 (0.0042)	
training:	Epoch: [60][124/408]	Loss 0.0009 (0.0042)	
training:	Epoch: [60][125/408]	Loss 0.0025 (0.0042)	
training:	Epoch: [60][126/408]	Loss 0.1268 (0.0052)	
training:	Epoch: [60][127/408]	Loss 0.0025 (0.0051)	
training:	Epoch: [60][128/408]	Loss 0.0014 (0.0051)	
training:	Epoch: [60][129/408]	Loss 0.0082 (0.0051)	
training:	Epoch: [60][130/408]	Loss 0.0489 (0.0055)	
training:	Epoch: [60][131/408]	Loss 0.0009 (0.0054)	
training:	Epoch: [60][132/408]	Loss 0.0011 (0.0054)	
training:	Epoch: [60][133/408]	Loss 0.0011 (0.0054)	
training:	Epoch: [60][134/408]	Loss 0.0088 (0.0054)	
training:	Epoch: [60][135/408]	Loss 0.0009 (0.0054)	
training:	Epoch: [60][136/408]	Loss 0.0012 (0.0053)	
training:	Epoch: [60][137/408]	Loss 0.0011 (0.0053)	
training:	Epoch: [60][138/408]	Loss 0.0009 (0.0053)	
training:	Epoch: [60][139/408]	Loss 0.0023 (0.0052)	
training:	Epoch: [60][140/408]	Loss 0.0043 (0.0052)	
training:	Epoch: [60][141/408]	Loss 0.0014 (0.0052)	
training:	Epoch: [60][142/408]	Loss 0.0009 (0.0052)	
training:	Epoch: [60][143/408]	Loss 0.0029 (0.0052)	
training:	Epoch: [60][144/408]	Loss 0.0267 (0.0053)	
training:	Epoch: [60][145/408]	Loss 0.0013 (0.0053)	
training:	Epoch: [60][146/408]	Loss 0.0010 (0.0053)	
training:	Epoch: [60][147/408]	Loss 0.0070 (0.0053)	
training:	Epoch: [60][148/408]	Loss 0.1134 (0.0060)	
training:	Epoch: [60][149/408]	Loss 0.0024 (0.0060)	
training:	Epoch: [60][150/408]	Loss 0.0009 (0.0059)	
training:	Epoch: [60][151/408]	Loss 0.0012 (0.0059)	
training:	Epoch: [60][152/408]	Loss 0.0014 (0.0059)	
training:	Epoch: [60][153/408]	Loss 0.0013 (0.0059)	
training:	Epoch: [60][154/408]	Loss 0.0014 (0.0058)	
training:	Epoch: [60][155/408]	Loss 0.0008 (0.0058)	
training:	Epoch: [60][156/408]	Loss 0.0010 (0.0058)	
training:	Epoch: [60][157/408]	Loss 0.0011 (0.0057)	
training:	Epoch: [60][158/408]	Loss 0.0009 (0.0057)	
training:	Epoch: [60][159/408]	Loss 0.0025 (0.0057)	
training:	Epoch: [60][160/408]	Loss 0.0026 (0.0057)	
training:	Epoch: [60][161/408]	Loss 0.0012 (0.0056)	
training:	Epoch: [60][162/408]	Loss 0.0009 (0.0056)	
training:	Epoch: [60][163/408]	Loss 0.0009 (0.0056)	
training:	Epoch: [60][164/408]	Loss 0.0137 (0.0056)	
training:	Epoch: [60][165/408]	Loss 0.0013 (0.0056)	
training:	Epoch: [60][166/408]	Loss 0.0011 (0.0056)	
training:	Epoch: [60][167/408]	Loss 0.0015 (0.0055)	
training:	Epoch: [60][168/408]	Loss 0.0010 (0.0055)	
training:	Epoch: [60][169/408]	Loss 0.0076 (0.0055)	
training:	Epoch: [60][170/408]	Loss 0.0009 (0.0055)	
training:	Epoch: [60][171/408]	Loss 0.0011 (0.0055)	
training:	Epoch: [60][172/408]	Loss 0.0037 (0.0055)	
training:	Epoch: [60][173/408]	Loss 0.0009 (0.0054)	
training:	Epoch: [60][174/408]	Loss 0.0009 (0.0054)	
training:	Epoch: [60][175/408]	Loss 0.0104 (0.0054)	
training:	Epoch: [60][176/408]	Loss 0.0015 (0.0054)	
training:	Epoch: [60][177/408]	Loss 0.0028 (0.0054)	
training:	Epoch: [60][178/408]	Loss 0.0013 (0.0054)	
training:	Epoch: [60][179/408]	Loss 0.0010 (0.0054)	
training:	Epoch: [60][180/408]	Loss 0.0009 (0.0053)	
training:	Epoch: [60][181/408]	Loss 0.0008 (0.0053)	
training:	Epoch: [60][182/408]	Loss 0.0479 (0.0055)	
training:	Epoch: [60][183/408]	Loss 0.0010 (0.0055)	
training:	Epoch: [60][184/408]	Loss 0.0191 (0.0056)	
training:	Epoch: [60][185/408]	Loss 0.0008 (0.0056)	
training:	Epoch: [60][186/408]	Loss 0.0072 (0.0056)	
training:	Epoch: [60][187/408]	Loss 0.0030 (0.0056)	
training:	Epoch: [60][188/408]	Loss 0.0008 (0.0055)	
training:	Epoch: [60][189/408]	Loss 0.0010 (0.0055)	
training:	Epoch: [60][190/408]	Loss 0.0008 (0.0055)	
training:	Epoch: [60][191/408]	Loss 0.0009 (0.0055)	
training:	Epoch: [60][192/408]	Loss 0.0009 (0.0054)	
training:	Epoch: [60][193/408]	Loss 0.0010 (0.0054)	
training:	Epoch: [60][194/408]	Loss 0.0011 (0.0054)	
training:	Epoch: [60][195/408]	Loss 0.0010 (0.0054)	
training:	Epoch: [60][196/408]	Loss 0.1597 (0.0062)	
training:	Epoch: [60][197/408]	Loss 0.0011 (0.0061)	
training:	Epoch: [60][198/408]	Loss 0.0020 (0.0061)	
training:	Epoch: [60][199/408]	Loss 0.0020 (0.0061)	
training:	Epoch: [60][200/408]	Loss 0.0107 (0.0061)	
training:	Epoch: [60][201/408]	Loss 0.0009 (0.0061)	
training:	Epoch: [60][202/408]	Loss 0.0014 (0.0061)	
training:	Epoch: [60][203/408]	Loss 0.0017 (0.0060)	
training:	Epoch: [60][204/408]	Loss 0.0009 (0.0060)	
training:	Epoch: [60][205/408]	Loss 0.0011 (0.0060)	
training:	Epoch: [60][206/408]	Loss 0.0008 (0.0060)	
training:	Epoch: [60][207/408]	Loss 0.0789 (0.0063)	
training:	Epoch: [60][208/408]	Loss 0.0013 (0.0063)	
training:	Epoch: [60][209/408]	Loss 0.0020 (0.0063)	
training:	Epoch: [60][210/408]	Loss 0.0014 (0.0063)	
training:	Epoch: [60][211/408]	Loss 0.0165 (0.0063)	
training:	Epoch: [60][212/408]	Loss 0.0042 (0.0063)	
training:	Epoch: [60][213/408]	Loss 0.0009 (0.0063)	
training:	Epoch: [60][214/408]	Loss 0.3412 (0.0078)	
training:	Epoch: [60][215/408]	Loss 0.0011 (0.0078)	
training:	Epoch: [60][216/408]	Loss 0.0009 (0.0078)	
training:	Epoch: [60][217/408]	Loss 0.0014 (0.0077)	
training:	Epoch: [60][218/408]	Loss 0.0011 (0.0077)	
training:	Epoch: [60][219/408]	Loss 0.0120 (0.0077)	
training:	Epoch: [60][220/408]	Loss 0.0011 (0.0077)	
training:	Epoch: [60][221/408]	Loss 0.0015 (0.0077)	
training:	Epoch: [60][222/408]	Loss 0.0008 (0.0076)	
training:	Epoch: [60][223/408]	Loss 0.0009 (0.0076)	
training:	Epoch: [60][224/408]	Loss 0.0013 (0.0076)	
training:	Epoch: [60][225/408]	Loss 0.0041 (0.0076)	
training:	Epoch: [60][226/408]	Loss 0.0011 (0.0075)	
training:	Epoch: [60][227/408]	Loss 0.0008 (0.0075)	
training:	Epoch: [60][228/408]	Loss 0.0016 (0.0075)	
training:	Epoch: [60][229/408]	Loss 0.0041 (0.0075)	
training:	Epoch: [60][230/408]	Loss 0.0115 (0.0075)	
training:	Epoch: [60][231/408]	Loss 0.0013 (0.0075)	
training:	Epoch: [60][232/408]	Loss 0.0012 (0.0074)	
training:	Epoch: [60][233/408]	Loss 0.0024 (0.0074)	
training:	Epoch: [60][234/408]	Loss 0.0028 (0.0074)	
training:	Epoch: [60][235/408]	Loss 0.0082 (0.0074)	
training:	Epoch: [60][236/408]	Loss 0.1756 (0.0081)	
training:	Epoch: [60][237/408]	Loss 0.0014 (0.0081)	
training:	Epoch: [60][238/408]	Loss 0.0011 (0.0080)	
training:	Epoch: [60][239/408]	Loss 0.0017 (0.0080)	
training:	Epoch: [60][240/408]	Loss 0.0038 (0.0080)	
training:	Epoch: [60][241/408]	Loss 0.0020 (0.0080)	
training:	Epoch: [60][242/408]	Loss 0.0013 (0.0080)	
training:	Epoch: [60][243/408]	Loss 0.0008 (0.0079)	
training:	Epoch: [60][244/408]	Loss 0.0015 (0.0079)	
training:	Epoch: [60][245/408]	Loss 0.0011 (0.0079)	
training:	Epoch: [60][246/408]	Loss 0.0024 (0.0078)	
training:	Epoch: [60][247/408]	Loss 0.0010 (0.0078)	
training:	Epoch: [60][248/408]	Loss 0.0009 (0.0078)	
training:	Epoch: [60][249/408]	Loss 0.0009 (0.0078)	
training:	Epoch: [60][250/408]	Loss 0.0013 (0.0077)	
training:	Epoch: [60][251/408]	Loss 0.0019 (0.0077)	
training:	Epoch: [60][252/408]	Loss 0.0011 (0.0077)	
training:	Epoch: [60][253/408]	Loss 0.0262 (0.0078)	
training:	Epoch: [60][254/408]	Loss 0.0013 (0.0077)	
training:	Epoch: [60][255/408]	Loss 0.0011 (0.0077)	
training:	Epoch: [60][256/408]	Loss 0.0028 (0.0077)	
training:	Epoch: [60][257/408]	Loss 0.0008 (0.0077)	
training:	Epoch: [60][258/408]	Loss 0.0041 (0.0076)	
training:	Epoch: [60][259/408]	Loss 0.1302 (0.0081)	
training:	Epoch: [60][260/408]	Loss 0.0046 (0.0081)	
training:	Epoch: [60][261/408]	Loss 0.0021 (0.0081)	
training:	Epoch: [60][262/408]	Loss 0.0010 (0.0081)	
training:	Epoch: [60][263/408]	Loss 0.0087 (0.0081)	
training:	Epoch: [60][264/408]	Loss 0.0336 (0.0082)	
training:	Epoch: [60][265/408]	Loss 0.0012 (0.0081)	
training:	Epoch: [60][266/408]	Loss 0.0008 (0.0081)	
training:	Epoch: [60][267/408]	Loss 0.3242 (0.0093)	
training:	Epoch: [60][268/408]	Loss 0.0013 (0.0093)	
training:	Epoch: [60][269/408]	Loss 0.0228 (0.0093)	
training:	Epoch: [60][270/408]	Loss 0.0016 (0.0093)	
training:	Epoch: [60][271/408]	Loss 0.0016 (0.0093)	
training:	Epoch: [60][272/408]	Loss 0.0010 (0.0092)	
training:	Epoch: [60][273/408]	Loss 0.0012 (0.0092)	
training:	Epoch: [60][274/408]	Loss 0.0017 (0.0092)	
training:	Epoch: [60][275/408]	Loss 0.0009 (0.0091)	
training:	Epoch: [60][276/408]	Loss 0.0011 (0.0091)	
training:	Epoch: [60][277/408]	Loss 0.0011 (0.0091)	
training:	Epoch: [60][278/408]	Loss 0.0014 (0.0090)	
training:	Epoch: [60][279/408]	Loss 0.0019 (0.0090)	
training:	Epoch: [60][280/408]	Loss 0.0008 (0.0090)	
training:	Epoch: [60][281/408]	Loss 0.0025 (0.0090)	
training:	Epoch: [60][282/408]	Loss 0.0123 (0.0090)	
training:	Epoch: [60][283/408]	Loss 0.0009 (0.0090)	
training:	Epoch: [60][284/408]	Loss 0.0012 (0.0089)	
training:	Epoch: [60][285/408]	Loss 0.0010 (0.0089)	
training:	Epoch: [60][286/408]	Loss 0.0289 (0.0090)	
training:	Epoch: [60][287/408]	Loss 0.0010 (0.0089)	
training:	Epoch: [60][288/408]	Loss 0.0012 (0.0089)	
training:	Epoch: [60][289/408]	Loss 0.0012 (0.0089)	
training:	Epoch: [60][290/408]	Loss 0.0014 (0.0089)	
training:	Epoch: [60][291/408]	Loss 0.0012 (0.0088)	
training:	Epoch: [60][292/408]	Loss 0.0011 (0.0088)	
training:	Epoch: [60][293/408]	Loss 0.0008 (0.0088)	
training:	Epoch: [60][294/408]	Loss 0.0012 (0.0088)	
training:	Epoch: [60][295/408]	Loss 0.0010 (0.0087)	
training:	Epoch: [60][296/408]	Loss 0.0016 (0.0087)	
training:	Epoch: [60][297/408]	Loss 0.0046 (0.0087)	
training:	Epoch: [60][298/408]	Loss 0.0012 (0.0087)	
training:	Epoch: [60][299/408]	Loss 0.0038 (0.0086)	
training:	Epoch: [60][300/408]	Loss 0.0014 (0.0086)	
training:	Epoch: [60][301/408]	Loss 0.0064 (0.0086)	
training:	Epoch: [60][302/408]	Loss 0.0012 (0.0086)	
training:	Epoch: [60][303/408]	Loss 0.0017 (0.0086)	
training:	Epoch: [60][304/408]	Loss 0.0018 (0.0085)	
training:	Epoch: [60][305/408]	Loss 0.0020 (0.0085)	
training:	Epoch: [60][306/408]	Loss 0.0011 (0.0085)	
training:	Epoch: [60][307/408]	Loss 0.0013 (0.0085)	
training:	Epoch: [60][308/408]	Loss 0.0013 (0.0085)	
training:	Epoch: [60][309/408]	Loss 0.0018 (0.0084)	
training:	Epoch: [60][310/408]	Loss 0.0201 (0.0085)	
training:	Epoch: [60][311/408]	Loss 0.0012 (0.0084)	
training:	Epoch: [60][312/408]	Loss 0.0011 (0.0084)	
training:	Epoch: [60][313/408]	Loss 0.0017 (0.0084)	
training:	Epoch: [60][314/408]	Loss 0.0013 (0.0084)	
training:	Epoch: [60][315/408]	Loss 0.0009 (0.0084)	
training:	Epoch: [60][316/408]	Loss 0.0023 (0.0083)	
training:	Epoch: [60][317/408]	Loss 0.0012 (0.0083)	
training:	Epoch: [60][318/408]	Loss 0.0009 (0.0083)	
training:	Epoch: [60][319/408]	Loss 0.0011 (0.0083)	
training:	Epoch: [60][320/408]	Loss 0.0016 (0.0082)	
training:	Epoch: [60][321/408]	Loss 0.0015 (0.0082)	
training:	Epoch: [60][322/408]	Loss 0.0009 (0.0082)	
training:	Epoch: [60][323/408]	Loss 0.0035 (0.0082)	
training:	Epoch: [60][324/408]	Loss 0.0009 (0.0082)	
training:	Epoch: [60][325/408]	Loss 0.0017 (0.0081)	
training:	Epoch: [60][326/408]	Loss 0.0033 (0.0081)	
training:	Epoch: [60][327/408]	Loss 0.0059 (0.0081)	
training:	Epoch: [60][328/408]	Loss 0.0103 (0.0081)	
training:	Epoch: [60][329/408]	Loss 0.0014 (0.0081)	
training:	Epoch: [60][330/408]	Loss 0.0020 (0.0081)	
training:	Epoch: [60][331/408]	Loss 0.0059 (0.0081)	
training:	Epoch: [60][332/408]	Loss 0.0051 (0.0081)	
training:	Epoch: [60][333/408]	Loss 0.0011 (0.0081)	
training:	Epoch: [60][334/408]	Loss 0.0018 (0.0080)	
training:	Epoch: [60][335/408]	Loss 0.0010 (0.0080)	
training:	Epoch: [60][336/408]	Loss 0.0014 (0.0080)	
training:	Epoch: [60][337/408]	Loss 0.0011 (0.0080)	
training:	Epoch: [60][338/408]	Loss 0.0011 (0.0080)	
training:	Epoch: [60][339/408]	Loss 0.0012 (0.0079)	
training:	Epoch: [60][340/408]	Loss 0.0011 (0.0079)	
training:	Epoch: [60][341/408]	Loss 0.0013 (0.0079)	
training:	Epoch: [60][342/408]	Loss 0.0011 (0.0079)	
training:	Epoch: [60][343/408]	Loss 0.0010 (0.0079)	
training:	Epoch: [60][344/408]	Loss 0.0055 (0.0079)	
training:	Epoch: [60][345/408]	Loss 0.0011 (0.0078)	
training:	Epoch: [60][346/408]	Loss 0.0009 (0.0078)	
training:	Epoch: [60][347/408]	Loss 0.0036 (0.0078)	
training:	Epoch: [60][348/408]	Loss 0.0015 (0.0078)	
training:	Epoch: [60][349/408]	Loss 0.0034 (0.0078)	
training:	Epoch: [60][350/408]	Loss 0.0009 (0.0077)	
training:	Epoch: [60][351/408]	Loss 0.0014 (0.0077)	
training:	Epoch: [60][352/408]	Loss 0.0011 (0.0077)	
training:	Epoch: [60][353/408]	Loss 0.0039 (0.0077)	
training:	Epoch: [60][354/408]	Loss 0.0020 (0.0077)	
training:	Epoch: [60][355/408]	Loss 0.0013 (0.0077)	
training:	Epoch: [60][356/408]	Loss 0.0015 (0.0076)	
training:	Epoch: [60][357/408]	Loss 0.0078 (0.0076)	
training:	Epoch: [60][358/408]	Loss 0.0009 (0.0076)	
training:	Epoch: [60][359/408]	Loss 0.0258 (0.0077)	
training:	Epoch: [60][360/408]	Loss 0.0044 (0.0077)	
training:	Epoch: [60][361/408]	Loss 0.0008 (0.0077)	
training:	Epoch: [60][362/408]	Loss 0.0011 (0.0076)	
training:	Epoch: [60][363/408]	Loss 0.0009 (0.0076)	
training:	Epoch: [60][364/408]	Loss 0.0020 (0.0076)	
training:	Epoch: [60][365/408]	Loss 0.0014 (0.0076)	
training:	Epoch: [60][366/408]	Loss 0.0008 (0.0076)	
training:	Epoch: [60][367/408]	Loss 0.0010 (0.0075)	
training:	Epoch: [60][368/408]	Loss 0.0014 (0.0075)	
training:	Epoch: [60][369/408]	Loss 0.0011 (0.0075)	
training:	Epoch: [60][370/408]	Loss 0.0011 (0.0075)	
training:	Epoch: [60][371/408]	Loss 0.0014 (0.0075)	
training:	Epoch: [60][372/408]	Loss 0.0011 (0.0075)	
training:	Epoch: [60][373/408]	Loss 0.0011 (0.0074)	
training:	Epoch: [60][374/408]	Loss 0.0008 (0.0074)	
training:	Epoch: [60][375/408]	Loss 0.0010 (0.0074)	
training:	Epoch: [60][376/408]	Loss 0.0021 (0.0074)	
training:	Epoch: [60][377/408]	Loss 0.0022 (0.0074)	
training:	Epoch: [60][378/408]	Loss 0.0010 (0.0074)	
training:	Epoch: [60][379/408]	Loss 0.0009 (0.0073)	
training:	Epoch: [60][380/408]	Loss 0.0009 (0.0073)	
training:	Epoch: [60][381/408]	Loss 0.0010 (0.0073)	
training:	Epoch: [60][382/408]	Loss 0.0010 (0.0073)	
training:	Epoch: [60][383/408]	Loss 0.0094 (0.0073)	
training:	Epoch: [60][384/408]	Loss 0.0024 (0.0073)	
training:	Epoch: [60][385/408]	Loss 0.0009 (0.0073)	
training:	Epoch: [60][386/408]	Loss 0.0021 (0.0073)	
training:	Epoch: [60][387/408]	Loss 0.0008 (0.0072)	
training:	Epoch: [60][388/408]	Loss 0.0026 (0.0072)	
training:	Epoch: [60][389/408]	Loss 0.0016 (0.0072)	
training:	Epoch: [60][390/408]	Loss 0.0008 (0.0072)	
training:	Epoch: [60][391/408]	Loss 0.0012 (0.0072)	
training:	Epoch: [60][392/408]	Loss 0.0027 (0.0072)	
training:	Epoch: [60][393/408]	Loss 0.0012 (0.0072)	
training:	Epoch: [60][394/408]	Loss 0.0008 (0.0071)	
training:	Epoch: [60][395/408]	Loss 0.0014 (0.0071)	
training:	Epoch: [60][396/408]	Loss 0.0055 (0.0071)	
training:	Epoch: [60][397/408]	Loss 0.0010 (0.0071)	
training:	Epoch: [60][398/408]	Loss 0.0009 (0.0071)	
training:	Epoch: [60][399/408]	Loss 0.0009 (0.0071)	
training:	Epoch: [60][400/408]	Loss 0.0016 (0.0071)	
training:	Epoch: [60][401/408]	Loss 0.0013 (0.0071)	
training:	Epoch: [60][402/408]	Loss 0.0031 (0.0070)	
training:	Epoch: [60][403/408]	Loss 0.0015 (0.0070)	
training:	Epoch: [60][404/408]	Loss 0.0016 (0.0070)	
training:	Epoch: [60][405/408]	Loss 0.0014 (0.0070)	
training:	Epoch: [60][406/408]	Loss 0.0008 (0.0070)	
training:	Epoch: [60][407/408]	Loss 0.0011 (0.0070)	
training:	Epoch: [60][408/408]	Loss 0.0008 (0.0070)	
Training:	 Loss: 0.0069

Training:	 ACC: 1.0000 1.0000 1.0000 1.0000
Validation:	 ACC: 0.7853 0.7860 0.7994 0.7713
Validation:	 Best_BACC: 0.8226 0.8250 0.8772 0.7679
Validation:	 Loss: 1.1268
Pretraining:	Epoch 61/200
----------
training:	Epoch: [61][1/408]	Loss 0.0051 (0.0051)	
training:	Epoch: [61][2/408]	Loss 0.0009 (0.0030)	
training:	Epoch: [61][3/408]	Loss 0.0009 (0.0023)	
training:	Epoch: [61][4/408]	Loss 0.0010 (0.0020)	
training:	Epoch: [61][5/408]	Loss 0.0007 (0.0017)	
training:	Epoch: [61][6/408]	Loss 0.0068 (0.0026)	
training:	Epoch: [61][7/408]	Loss 0.0009 (0.0023)	
training:	Epoch: [61][8/408]	Loss 0.0009 (0.0022)	
training:	Epoch: [61][9/408]	Loss 0.0008 (0.0020)	
training:	Epoch: [61][10/408]	Loss 0.0014 (0.0019)	
training:	Epoch: [61][11/408]	Loss 0.0014 (0.0019)	
training:	Epoch: [61][12/408]	Loss 0.0009 (0.0018)	
training:	Epoch: [61][13/408]	Loss 0.0012 (0.0018)	
training:	Epoch: [61][14/408]	Loss 0.0012 (0.0017)	
training:	Epoch: [61][15/408]	Loss 0.0012 (0.0017)	
training:	Epoch: [61][16/408]	Loss 0.0017 (0.0017)	
training:	Epoch: [61][17/408]	Loss 0.0010 (0.0016)	
training:	Epoch: [61][18/408]	Loss 0.0010 (0.0016)	
training:	Epoch: [61][19/408]	Loss 0.0009 (0.0016)	
training:	Epoch: [61][20/408]	Loss 0.0010 (0.0015)	
training:	Epoch: [61][21/408]	Loss 0.0009 (0.0015)	
training:	Epoch: [61][22/408]	Loss 0.0061 (0.0017)	
training:	Epoch: [61][23/408]	Loss 0.0010 (0.0017)	
training:	Epoch: [61][24/408]	Loss 0.0009 (0.0017)	
training:	Epoch: [61][25/408]	Loss 0.0010 (0.0016)	
training:	Epoch: [61][26/408]	Loss 0.0052 (0.0018)	
training:	Epoch: [61][27/408]	Loss 0.0010 (0.0017)	
training:	Epoch: [61][28/408]	Loss 0.0023 (0.0018)	
training:	Epoch: [61][29/408]	Loss 0.0049 (0.0019)	
training:	Epoch: [61][30/408]	Loss 0.0059 (0.0020)	
training:	Epoch: [61][31/408]	Loss 0.0049 (0.0021)	
training:	Epoch: [61][32/408]	Loss 0.0058 (0.0022)	
training:	Epoch: [61][33/408]	Loss 0.0009 (0.0022)	
training:	Epoch: [61][34/408]	Loss 0.0009 (0.0021)	
training:	Epoch: [61][35/408]	Loss 0.0009 (0.0021)	
training:	Epoch: [61][36/408]	Loss 0.0010 (0.0021)	
training:	Epoch: [61][37/408]	Loss 0.0620 (0.0037)	
training:	Epoch: [61][38/408]	Loss 0.0009 (0.0036)	
training:	Epoch: [61][39/408]	Loss 0.0008 (0.0035)	
training:	Epoch: [61][40/408]	Loss 0.0016 (0.0035)	
training:	Epoch: [61][41/408]	Loss 0.0014 (0.0034)	
training:	Epoch: [61][42/408]	Loss 0.0009 (0.0034)	
training:	Epoch: [61][43/408]	Loss 0.0011 (0.0033)	
training:	Epoch: [61][44/408]	Loss 0.0010 (0.0033)	
training:	Epoch: [61][45/408]	Loss 0.0010 (0.0032)	
training:	Epoch: [61][46/408]	Loss 0.0009 (0.0032)	
training:	Epoch: [61][47/408]	Loss 0.0012 (0.0031)	
training:	Epoch: [61][48/408]	Loss 0.0012 (0.0031)	
training:	Epoch: [61][49/408]	Loss 0.0122 (0.0033)	
training:	Epoch: [61][50/408]	Loss 0.0009 (0.0032)	
training:	Epoch: [61][51/408]	Loss 0.0008 (0.0032)	
training:	Epoch: [61][52/408]	Loss 0.0015 (0.0032)	
training:	Epoch: [61][53/408]	Loss 0.0012 (0.0031)	
training:	Epoch: [61][54/408]	Loss 0.0020 (0.0031)	
training:	Epoch: [61][55/408]	Loss 0.0010 (0.0031)	
training:	Epoch: [61][56/408]	Loss 0.0103 (0.0032)	
training:	Epoch: [61][57/408]	Loss 0.0014 (0.0032)	
training:	Epoch: [61][58/408]	Loss 0.0061 (0.0032)	
training:	Epoch: [61][59/408]	Loss 0.0023 (0.0032)	
training:	Epoch: [61][60/408]	Loss 0.0032 (0.0032)	
training:	Epoch: [61][61/408]	Loss 0.0008 (0.0032)	
training:	Epoch: [61][62/408]	Loss 0.0012 (0.0031)	
training:	Epoch: [61][63/408]	Loss 0.0026 (0.0031)	
training:	Epoch: [61][64/408]	Loss 0.0009 (0.0031)	
training:	Epoch: [61][65/408]	Loss 0.0010 (0.0031)	
training:	Epoch: [61][66/408]	Loss 0.0010 (0.0030)	
training:	Epoch: [61][67/408]	Loss 0.0014 (0.0030)	
training:	Epoch: [61][68/408]	Loss 0.0011 (0.0030)	
training:	Epoch: [61][69/408]	Loss 0.0011 (0.0029)	
training:	Epoch: [61][70/408]	Loss 0.0011 (0.0029)	
training:	Epoch: [61][71/408]	Loss 0.0008 (0.0029)	
training:	Epoch: [61][72/408]	Loss 0.0038 (0.0029)	
training:	Epoch: [61][73/408]	Loss 0.0009 (0.0029)	
training:	Epoch: [61][74/408]	Loss 0.0009 (0.0028)	
training:	Epoch: [61][75/408]	Loss 0.0010 (0.0028)	
training:	Epoch: [61][76/408]	Loss 0.0008 (0.0028)	
training:	Epoch: [61][77/408]	Loss 0.0039 (0.0028)	
training:	Epoch: [61][78/408]	Loss 0.0045 (0.0028)	
training:	Epoch: [61][79/408]	Loss 0.0026 (0.0028)	
training:	Epoch: [61][80/408]	Loss 0.0011 (0.0028)	
training:	Epoch: [61][81/408]	Loss 0.0009 (0.0028)	
training:	Epoch: [61][82/408]	Loss 0.0010 (0.0028)	
training:	Epoch: [61][83/408]	Loss 0.0008 (0.0027)	
training:	Epoch: [61][84/408]	Loss 0.0010 (0.0027)	
training:	Epoch: [61][85/408]	Loss 0.0009 (0.0027)	
training:	Epoch: [61][86/408]	Loss 0.0018 (0.0027)	
training:	Epoch: [61][87/408]	Loss 0.0009 (0.0027)	
training:	Epoch: [61][88/408]	Loss 0.0009 (0.0026)	
training:	Epoch: [61][89/408]	Loss 0.0037 (0.0027)	
training:	Epoch: [61][90/408]	Loss 0.0010 (0.0026)	
training:	Epoch: [61][91/408]	Loss 0.0040 (0.0026)	
training:	Epoch: [61][92/408]	Loss 0.0010 (0.0026)	
training:	Epoch: [61][93/408]	Loss 0.0011 (0.0026)	
training:	Epoch: [61][94/408]	Loss 0.0042 (0.0026)	
training:	Epoch: [61][95/408]	Loss 0.0014 (0.0026)	
training:	Epoch: [61][96/408]	Loss 0.0007 (0.0026)	
training:	Epoch: [61][97/408]	Loss 0.0012 (0.0026)	
training:	Epoch: [61][98/408]	Loss 0.0016 (0.0026)	
training:	Epoch: [61][99/408]	Loss 0.0011 (0.0026)	
training:	Epoch: [61][100/408]	Loss 0.0011 (0.0025)	
training:	Epoch: [61][101/408]	Loss 0.0013 (0.0025)	
training:	Epoch: [61][102/408]	Loss 0.0010 (0.0025)	
training:	Epoch: [61][103/408]	Loss 0.0008 (0.0025)	
training:	Epoch: [61][104/408]	Loss 0.0021 (0.0025)	
training:	Epoch: [61][105/408]	Loss 0.0191 (0.0027)	
training:	Epoch: [61][106/408]	Loss 0.0009 (0.0026)	
training:	Epoch: [61][107/408]	Loss 0.0011 (0.0026)	
training:	Epoch: [61][108/408]	Loss 0.0012 (0.0026)	
training:	Epoch: [61][109/408]	Loss 0.0009 (0.0026)	
training:	Epoch: [61][110/408]	Loss 0.0017 (0.0026)	
training:	Epoch: [61][111/408]	Loss 0.0009 (0.0026)	
training:	Epoch: [61][112/408]	Loss 0.0008 (0.0026)	
training:	Epoch: [61][113/408]	Loss 0.0011 (0.0025)	
training:	Epoch: [61][114/408]	Loss 0.0009 (0.0025)	
training:	Epoch: [61][115/408]	Loss 0.0011 (0.0025)	
training:	Epoch: [61][116/408]	Loss 0.0008 (0.0025)	
training:	Epoch: [61][117/408]	Loss 0.0012 (0.0025)	
training:	Epoch: [61][118/408]	Loss 0.0009 (0.0025)	
training:	Epoch: [61][119/408]	Loss 0.0010 (0.0025)	
training:	Epoch: [61][120/408]	Loss 0.0009 (0.0025)	
training:	Epoch: [61][121/408]	Loss 0.0015 (0.0024)	
training:	Epoch: [61][122/408]	Loss 0.0009 (0.0024)	
training:	Epoch: [61][123/408]	Loss 0.0011 (0.0024)	
training:	Epoch: [61][124/408]	Loss 0.0010 (0.0024)	
training:	Epoch: [61][125/408]	Loss 0.0009 (0.0024)	
training:	Epoch: [61][126/408]	Loss 0.0008 (0.0024)	
training:	Epoch: [61][127/408]	Loss 0.0025 (0.0024)	
training:	Epoch: [61][128/408]	Loss 0.0009 (0.0024)	
training:	Epoch: [61][129/408]	Loss 0.0007 (0.0024)	
training:	Epoch: [61][130/408]	Loss 0.0020 (0.0024)	
training:	Epoch: [61][131/408]	Loss 0.0113 (0.0024)	
training:	Epoch: [61][132/408]	Loss 0.0008 (0.0024)	
training:	Epoch: [61][133/408]	Loss 0.0012 (0.0024)	
training:	Epoch: [61][134/408]	Loss 0.0008 (0.0024)	
training:	Epoch: [61][135/408]	Loss 0.0009 (0.0024)	
training:	Epoch: [61][136/408]	Loss 0.0037 (0.0024)	
training:	Epoch: [61][137/408]	Loss 0.0007 (0.0024)	
training:	Epoch: [61][138/408]	Loss 0.0008 (0.0024)	
training:	Epoch: [61][139/408]	Loss 0.0013 (0.0024)	
training:	Epoch: [61][140/408]	Loss 0.0012 (0.0024)	
training:	Epoch: [61][141/408]	Loss 0.0030 (0.0024)	
training:	Epoch: [61][142/408]	Loss 0.0009 (0.0023)	
training:	Epoch: [61][143/408]	Loss 0.0009 (0.0023)	
training:	Epoch: [61][144/408]	Loss 0.0009 (0.0023)	
training:	Epoch: [61][145/408]	Loss 0.0013 (0.0023)	
training:	Epoch: [61][146/408]	Loss 0.0011 (0.0023)	
training:	Epoch: [61][147/408]	Loss 0.0008 (0.0023)	
training:	Epoch: [61][148/408]	Loss 0.0076 (0.0023)	
training:	Epoch: [61][149/408]	Loss 0.0008 (0.0023)	
training:	Epoch: [61][150/408]	Loss 0.0009 (0.0023)	
training:	Epoch: [61][151/408]	Loss 0.0009 (0.0023)	
training:	Epoch: [61][152/408]	Loss 0.0013 (0.0023)	
training:	Epoch: [61][153/408]	Loss 0.0007 (0.0023)	
training:	Epoch: [61][154/408]	Loss 0.0008 (0.0023)	
training:	Epoch: [61][155/408]	Loss 0.0012 (0.0023)	
training:	Epoch: [61][156/408]	Loss 0.0009 (0.0023)	
training:	Epoch: [61][157/408]	Loss 0.0009 (0.0023)	
training:	Epoch: [61][158/408]	Loss 0.0010 (0.0022)	
training:	Epoch: [61][159/408]	Loss 0.0013 (0.0022)	
training:	Epoch: [61][160/408]	Loss 0.0008 (0.0022)	
training:	Epoch: [61][161/408]	Loss 0.0008 (0.0022)	
training:	Epoch: [61][162/408]	Loss 0.0008 (0.0022)	
training:	Epoch: [61][163/408]	Loss 0.0008 (0.0022)	
training:	Epoch: [61][164/408]	Loss 0.0013 (0.0022)	
training:	Epoch: [61][165/408]	Loss 0.0010 (0.0022)	
training:	Epoch: [61][166/408]	Loss 0.0009 (0.0022)	
training:	Epoch: [61][167/408]	Loss 0.0009 (0.0022)	
training:	Epoch: [61][168/408]	Loss 0.0013 (0.0022)	
training:	Epoch: [61][169/408]	Loss 0.0012 (0.0022)	
training:	Epoch: [61][170/408]	Loss 0.0020 (0.0022)	
training:	Epoch: [61][171/408]	Loss 0.0009 (0.0022)	
training:	Epoch: [61][172/408]	Loss 0.0013 (0.0022)	
training:	Epoch: [61][173/408]	Loss 0.0009 (0.0021)	
training:	Epoch: [61][174/408]	Loss 0.0009 (0.0021)	
training:	Epoch: [61][175/408]	Loss 0.0020 (0.0021)	
training:	Epoch: [61][176/408]	Loss 0.0009 (0.0021)	
training:	Epoch: [61][177/408]	Loss 0.0010 (0.0021)	
training:	Epoch: [61][178/408]	Loss 0.0009 (0.0021)	
training:	Epoch: [61][179/408]	Loss 0.0026 (0.0021)	
training:	Epoch: [61][180/408]	Loss 0.0009 (0.0021)	
training:	Epoch: [61][181/408]	Loss 0.0008 (0.0021)	
training:	Epoch: [61][182/408]	Loss 0.0008 (0.0021)	
training:	Epoch: [61][183/408]	Loss 0.0374 (0.0023)	
training:	Epoch: [61][184/408]	Loss 0.0009 (0.0023)	
training:	Epoch: [61][185/408]	Loss 0.0217 (0.0024)	
training:	Epoch: [61][186/408]	Loss 0.0015 (0.0024)	
training:	Epoch: [61][187/408]	Loss 0.0027 (0.0024)	
training:	Epoch: [61][188/408]	Loss 0.0018 (0.0024)	
training:	Epoch: [61][189/408]	Loss 0.0016 (0.0024)	
training:	Epoch: [61][190/408]	Loss 0.0018 (0.0024)	
training:	Epoch: [61][191/408]	Loss 0.0008 (0.0024)	
training:	Epoch: [61][192/408]	Loss 0.0011 (0.0024)	
training:	Epoch: [61][193/408]	Loss 0.0010 (0.0024)	
training:	Epoch: [61][194/408]	Loss 0.0007 (0.0023)	
training:	Epoch: [61][195/408]	Loss 0.0010 (0.0023)	
training:	Epoch: [61][196/408]	Loss 0.0007 (0.0023)	
training:	Epoch: [61][197/408]	Loss 0.0008 (0.0023)	
training:	Epoch: [61][198/408]	Loss 0.0011 (0.0023)	
training:	Epoch: [61][199/408]	Loss 0.0009 (0.0023)	
training:	Epoch: [61][200/408]	Loss 0.0011 (0.0023)	
training:	Epoch: [61][201/408]	Loss 0.0011 (0.0023)	
training:	Epoch: [61][202/408]	Loss 0.0029 (0.0023)	
training:	Epoch: [61][203/408]	Loss 0.0010 (0.0023)	
training:	Epoch: [61][204/408]	Loss 0.0009 (0.0023)	
training:	Epoch: [61][205/408]	Loss 0.0008 (0.0023)	
training:	Epoch: [61][206/408]	Loss 0.0009 (0.0023)	
training:	Epoch: [61][207/408]	Loss 0.0008 (0.0023)	
training:	Epoch: [61][208/408]	Loss 0.0009 (0.0023)	
training:	Epoch: [61][209/408]	Loss 0.0014 (0.0023)	
training:	Epoch: [61][210/408]	Loss 0.0008 (0.0022)	
training:	Epoch: [61][211/408]	Loss 0.0008 (0.0022)	
training:	Epoch: [61][212/408]	Loss 0.0010 (0.0022)	
training:	Epoch: [61][213/408]	Loss 0.0008 (0.0022)	
training:	Epoch: [61][214/408]	Loss 0.0012 (0.0022)	
training:	Epoch: [61][215/408]	Loss 0.0008 (0.0022)	
training:	Epoch: [61][216/408]	Loss 0.0009 (0.0022)	
training:	Epoch: [61][217/408]	Loss 0.0012 (0.0022)	
training:	Epoch: [61][218/408]	Loss 0.0015 (0.0022)	
training:	Epoch: [61][219/408]	Loss 0.0022 (0.0022)	
training:	Epoch: [61][220/408]	Loss 0.0009 (0.0022)	
training:	Epoch: [61][221/408]	Loss 0.0010 (0.0022)	
training:	Epoch: [61][222/408]	Loss 0.0009 (0.0022)	
training:	Epoch: [61][223/408]	Loss 0.0008 (0.0022)	
training:	Epoch: [61][224/408]	Loss 0.0008 (0.0022)	
training:	Epoch: [61][225/408]	Loss 0.0009 (0.0022)	
training:	Epoch: [61][226/408]	Loss 0.0059 (0.0022)	
training:	Epoch: [61][227/408]	Loss 0.0008 (0.0022)	
training:	Epoch: [61][228/408]	Loss 0.0012 (0.0022)	
training:	Epoch: [61][229/408]	Loss 0.0046 (0.0022)	
training:	Epoch: [61][230/408]	Loss 0.0047 (0.0022)	
training:	Epoch: [61][231/408]	Loss 0.0016 (0.0022)	
training:	Epoch: [61][232/408]	Loss 0.0008 (0.0022)	
training:	Epoch: [61][233/408]	Loss 0.0009 (0.0022)	
training:	Epoch: [61][234/408]	Loss 0.0009 (0.0022)	
training:	Epoch: [61][235/408]	Loss 0.0008 (0.0022)	
training:	Epoch: [61][236/408]	Loss 0.0008 (0.0022)	
training:	Epoch: [61][237/408]	Loss 0.0010 (0.0022)	
training:	Epoch: [61][238/408]	Loss 0.0008 (0.0022)	
training:	Epoch: [61][239/408]	Loss 0.0009 (0.0021)	
training:	Epoch: [61][240/408]	Loss 0.0008 (0.0021)	
training:	Epoch: [61][241/408]	Loss 0.0018 (0.0021)	
training:	Epoch: [61][242/408]	Loss 0.0008 (0.0021)	
training:	Epoch: [61][243/408]	Loss 0.0011 (0.0021)	
training:	Epoch: [61][244/408]	Loss 0.0009 (0.0021)	
training:	Epoch: [61][245/408]	Loss 0.0010 (0.0021)	
training:	Epoch: [61][246/408]	Loss 0.0015 (0.0021)	
training:	Epoch: [61][247/408]	Loss 0.0008 (0.0021)	
training:	Epoch: [61][248/408]	Loss 0.0030 (0.0021)	
training:	Epoch: [61][249/408]	Loss 0.0010 (0.0021)	
training:	Epoch: [61][250/408]	Loss 0.0009 (0.0021)	
training:	Epoch: [61][251/408]	Loss 0.0050 (0.0021)	
training:	Epoch: [61][252/408]	Loss 0.0007 (0.0021)	
training:	Epoch: [61][253/408]	Loss 0.0012 (0.0021)	
training:	Epoch: [61][254/408]	Loss 0.0078 (0.0021)	
training:	Epoch: [61][255/408]	Loss 0.0015 (0.0021)	
training:	Epoch: [61][256/408]	Loss 0.0029 (0.0021)	
training:	Epoch: [61][257/408]	Loss 0.0013 (0.0021)	
training:	Epoch: [61][258/408]	Loss 0.0012 (0.0021)	
training:	Epoch: [61][259/408]	Loss 0.0011 (0.0021)	
training:	Epoch: [61][260/408]	Loss 0.0016 (0.0021)	
training:	Epoch: [61][261/408]	Loss 0.0008 (0.0021)	
training:	Epoch: [61][262/408]	Loss 0.0008 (0.0021)	
training:	Epoch: [61][263/408]	Loss 0.0012 (0.0021)	
training:	Epoch: [61][264/408]	Loss 0.0010 (0.0021)	
training:	Epoch: [61][265/408]	Loss 0.0007 (0.0021)	
training:	Epoch: [61][266/408]	Loss 0.0008 (0.0021)	
training:	Epoch: [61][267/408]	Loss 0.0014 (0.0021)	
training:	Epoch: [61][268/408]	Loss 0.0037 (0.0021)	
training:	Epoch: [61][269/408]	Loss 0.0007 (0.0021)	
training:	Epoch: [61][270/408]	Loss 0.0182 (0.0022)	
training:	Epoch: [61][271/408]	Loss 0.0012 (0.0021)	
training:	Epoch: [61][272/408]	Loss 0.0193 (0.0022)	
training:	Epoch: [61][273/408]	Loss 0.0007 (0.0022)	
training:	Epoch: [61][274/408]	Loss 0.0031 (0.0022)	
training:	Epoch: [61][275/408]	Loss 0.0013 (0.0022)	
training:	Epoch: [61][276/408]	Loss 0.0009 (0.0022)	
training:	Epoch: [61][277/408]	Loss 0.0018 (0.0022)	
training:	Epoch: [61][278/408]	Loss 0.0008 (0.0022)	
training:	Epoch: [61][279/408]	Loss 0.0010 (0.0022)	
training:	Epoch: [61][280/408]	Loss 0.0012 (0.0022)	
training:	Epoch: [61][281/408]	Loss 0.0009 (0.0022)	
training:	Epoch: [61][282/408]	Loss 0.0177 (0.0022)	
training:	Epoch: [61][283/408]	Loss 0.0011 (0.0022)	
training:	Epoch: [61][284/408]	Loss 0.0008 (0.0022)	
training:	Epoch: [61][285/408]	Loss 0.0025 (0.0022)	
training:	Epoch: [61][286/408]	Loss 0.0008 (0.0022)	
training:	Epoch: [61][287/408]	Loss 0.0008 (0.0022)	
training:	Epoch: [61][288/408]	Loss 0.0009 (0.0022)	
training:	Epoch: [61][289/408]	Loss 0.0008 (0.0022)	
training:	Epoch: [61][290/408]	Loss 0.0017 (0.0022)	
training:	Epoch: [61][291/408]	Loss 0.0009 (0.0022)	
training:	Epoch: [61][292/408]	Loss 0.0016 (0.0022)	
training:	Epoch: [61][293/408]	Loss 0.0054 (0.0022)	
training:	Epoch: [61][294/408]	Loss 0.0009 (0.0022)	
training:	Epoch: [61][295/408]	Loss 0.0012 (0.0022)	
training:	Epoch: [61][296/408]	Loss 0.0009 (0.0022)	
training:	Epoch: [61][297/408]	Loss 0.0009 (0.0022)	
training:	Epoch: [61][298/408]	Loss 0.0017 (0.0022)	
training:	Epoch: [61][299/408]	Loss 0.0011 (0.0022)	
training:	Epoch: [61][300/408]	Loss 0.0010 (0.0022)	
training:	Epoch: [61][301/408]	Loss 0.0012 (0.0022)	
training:	Epoch: [61][302/408]	Loss 0.0008 (0.0022)	
training:	Epoch: [61][303/408]	Loss 0.0012 (0.0022)	
training:	Epoch: [61][304/408]	Loss 0.0009 (0.0022)	
training:	Epoch: [61][305/408]	Loss 0.0007 (0.0022)	
training:	Epoch: [61][306/408]	Loss 0.0015 (0.0022)	
training:	Epoch: [61][307/408]	Loss 0.0017 (0.0022)	
training:	Epoch: [61][308/408]	Loss 0.0008 (0.0022)	
training:	Epoch: [61][309/408]	Loss 0.0015 (0.0022)	
training:	Epoch: [61][310/408]	Loss 0.0007 (0.0021)	
training:	Epoch: [61][311/408]	Loss 0.0008 (0.0021)	
training:	Epoch: [61][312/408]	Loss 0.0009 (0.0021)	
training:	Epoch: [61][313/408]	Loss 0.0007 (0.0021)	
training:	Epoch: [61][314/408]	Loss 0.0011 (0.0021)	
training:	Epoch: [61][315/408]	Loss 0.0010 (0.0021)	
training:	Epoch: [61][316/408]	Loss 0.0008 (0.0021)	
training:	Epoch: [61][317/408]	Loss 0.0007 (0.0021)	
training:	Epoch: [61][318/408]	Loss 0.0012 (0.0021)	
training:	Epoch: [61][319/408]	Loss 0.0010 (0.0021)	
training:	Epoch: [61][320/408]	Loss 0.0011 (0.0021)	
training:	Epoch: [61][321/408]	Loss 0.0008 (0.0021)	
training:	Epoch: [61][322/408]	Loss 0.0009 (0.0021)	
training:	Epoch: [61][323/408]	Loss 0.0008 (0.0021)	
training:	Epoch: [61][324/408]	Loss 0.0017 (0.0021)	
training:	Epoch: [61][325/408]	Loss 0.0013 (0.0021)	
training:	Epoch: [61][326/408]	Loss 0.0031 (0.0021)	
training:	Epoch: [61][327/408]	Loss 0.0009 (0.0021)	
training:	Epoch: [61][328/408]	Loss 0.0011 (0.0021)	
training:	Epoch: [61][329/408]	Loss 0.0009 (0.0021)	
training:	Epoch: [61][330/408]	Loss 0.0008 (0.0021)	
training:	Epoch: [61][331/408]	Loss 0.0010 (0.0021)	
training:	Epoch: [61][332/408]	Loss 0.0008 (0.0021)	
training:	Epoch: [61][333/408]	Loss 0.0095 (0.0021)	
training:	Epoch: [61][334/408]	Loss 0.0012 (0.0021)	
training:	Epoch: [61][335/408]	Loss 0.0010 (0.0021)	
training:	Epoch: [61][336/408]	Loss 0.0010 (0.0021)	
training:	Epoch: [61][337/408]	Loss 0.0009 (0.0021)	
training:	Epoch: [61][338/408]	Loss 0.0013 (0.0021)	
training:	Epoch: [61][339/408]	Loss 0.0008 (0.0021)	
training:	Epoch: [61][340/408]	Loss 0.0011 (0.0021)	
training:	Epoch: [61][341/408]	Loss 0.0012 (0.0021)	
training:	Epoch: [61][342/408]	Loss 0.0024 (0.0021)	
training:	Epoch: [61][343/408]	Loss 0.0008 (0.0021)	
training:	Epoch: [61][344/408]	Loss 0.0009 (0.0021)	
training:	Epoch: [61][345/408]	Loss 0.0017 (0.0021)	
training:	Epoch: [61][346/408]	Loss 0.0036 (0.0021)	
training:	Epoch: [61][347/408]	Loss 0.0011 (0.0021)	
training:	Epoch: [61][348/408]	Loss 0.0007 (0.0021)	
training:	Epoch: [61][349/408]	Loss 0.0008 (0.0021)	
training:	Epoch: [61][350/408]	Loss 0.0009 (0.0021)	
training:	Epoch: [61][351/408]	Loss 0.0056 (0.0021)	
training:	Epoch: [61][352/408]	Loss 0.0008 (0.0021)	
training:	Epoch: [61][353/408]	Loss 0.0043 (0.0021)	
training:	Epoch: [61][354/408]	Loss 0.0009 (0.0021)	
training:	Epoch: [61][355/408]	Loss 0.0061 (0.0021)	
training:	Epoch: [61][356/408]	Loss 0.0009 (0.0021)	
training:	Epoch: [61][357/408]	Loss 0.0011 (0.0021)	
training:	Epoch: [61][358/408]	Loss 0.0041 (0.0021)	
training:	Epoch: [61][359/408]	Loss 0.0015 (0.0021)	
training:	Epoch: [61][360/408]	Loss 0.0018 (0.0021)	
training:	Epoch: [61][361/408]	Loss 0.0009 (0.0021)	
training:	Epoch: [61][362/408]	Loss 0.0023 (0.0021)	
training:	Epoch: [61][363/408]	Loss 0.0007 (0.0021)	
training:	Epoch: [61][364/408]	Loss 0.0019 (0.0021)	
training:	Epoch: [61][365/408]	Loss 0.0030 (0.0021)	
training:	Epoch: [61][366/408]	Loss 0.0007 (0.0021)	
training:	Epoch: [61][367/408]	Loss 0.0008 (0.0021)	
training:	Epoch: [61][368/408]	Loss 0.0008 (0.0021)	
training:	Epoch: [61][369/408]	Loss 0.0029 (0.0021)	
training:	Epoch: [61][370/408]	Loss 0.0007 (0.0021)	
training:	Epoch: [61][371/408]	Loss 0.0014 (0.0021)	
training:	Epoch: [61][372/408]	Loss 0.0008 (0.0021)	
training:	Epoch: [61][373/408]	Loss 0.0064 (0.0021)	
training:	Epoch: [61][374/408]	Loss 0.0011 (0.0021)	
training:	Epoch: [61][375/408]	Loss 0.0011 (0.0021)	
training:	Epoch: [61][376/408]	Loss 0.0009 (0.0021)	
training:	Epoch: [61][377/408]	Loss 0.0018 (0.0021)	
training:	Epoch: [61][378/408]	Loss 0.0007 (0.0021)	
training:	Epoch: [61][379/408]	Loss 0.0008 (0.0021)	
training:	Epoch: [61][380/408]	Loss 0.0008 (0.0020)	
training:	Epoch: [61][381/408]	Loss 0.0070 (0.0021)	
training:	Epoch: [61][382/408]	Loss 0.0007 (0.0021)	
training:	Epoch: [61][383/408]	Loss 0.0009 (0.0021)	
training:	Epoch: [61][384/408]	Loss 0.0008 (0.0021)	
training:	Epoch: [61][385/408]	Loss 0.0007 (0.0020)	
training:	Epoch: [61][386/408]	Loss 0.0008 (0.0020)	
training:	Epoch: [61][387/408]	Loss 0.0008 (0.0020)	
training:	Epoch: [61][388/408]	Loss 0.0009 (0.0020)	
training:	Epoch: [61][389/408]	Loss 0.0010 (0.0020)	
training:	Epoch: [61][390/408]	Loss 0.0022 (0.0020)	
training:	Epoch: [61][391/408]	Loss 0.0009 (0.0020)	
training:	Epoch: [61][392/408]	Loss 0.0033 (0.0020)	
training:	Epoch: [61][393/408]	Loss 0.0008 (0.0020)	
training:	Epoch: [61][394/408]	Loss 0.0007 (0.0020)	
training:	Epoch: [61][395/408]	Loss 0.0007 (0.0020)	
training:	Epoch: [61][396/408]	Loss 0.0011 (0.0020)	
training:	Epoch: [61][397/408]	Loss 0.0008 (0.0020)	
training:	Epoch: [61][398/408]	Loss 0.0009 (0.0020)	
training:	Epoch: [61][399/408]	Loss 0.0023 (0.0020)	
training:	Epoch: [61][400/408]	Loss 0.0014 (0.0020)	
training:	Epoch: [61][401/408]	Loss 0.0007 (0.0020)	
training:	Epoch: [61][402/408]	Loss 0.0007 (0.0020)	
training:	Epoch: [61][403/408]	Loss 0.0010 (0.0020)	
training:	Epoch: [61][404/408]	Loss 0.0011 (0.0020)	
training:	Epoch: [61][405/408]	Loss 0.0016 (0.0020)	
training:	Epoch: [61][406/408]	Loss 0.0011 (0.0020)	
training:	Epoch: [61][407/408]	Loss 0.0011 (0.0020)	
training:	Epoch: [61][408/408]	Loss 0.0008 (0.0020)	
Training:	 Loss: 0.0020

Training:	 ACC: 1.0000 1.0000 1.0000 1.0000
Validation:	 ACC: 0.7864 0.7871 0.8004 0.7724
Validation:	 Best_BACC: 0.8226 0.8250 0.8772 0.7679
Validation:	 Loss: 1.1676
Pretraining:	Epoch 62/200
----------
training:	Epoch: [62][1/408]	Loss 0.0008 (0.0008)	
training:	Epoch: [62][2/408]	Loss 0.0008 (0.0008)	
training:	Epoch: [62][3/408]	Loss 0.0007 (0.0008)	
training:	Epoch: [62][4/408]	Loss 0.0009 (0.0008)	
training:	Epoch: [62][5/408]	Loss 0.0008 (0.0008)	
training:	Epoch: [62][6/408]	Loss 0.0008 (0.0008)	
training:	Epoch: [62][7/408]	Loss 0.0007 (0.0008)	
training:	Epoch: [62][8/408]	Loss 0.0010 (0.0008)	
training:	Epoch: [62][9/408]	Loss 0.0097 (0.0018)	
training:	Epoch: [62][10/408]	Loss 0.0008 (0.0017)	
training:	Epoch: [62][11/408]	Loss 0.0007 (0.0016)	
training:	Epoch: [62][12/408]	Loss 0.0007 (0.0015)	
training:	Epoch: [62][13/408]	Loss 0.0008 (0.0015)	
training:	Epoch: [62][14/408]	Loss 0.0008 (0.0014)	
training:	Epoch: [62][15/408]	Loss 0.0008 (0.0014)	
training:	Epoch: [62][16/408]	Loss 0.0021 (0.0014)	
training:	Epoch: [62][17/408]	Loss 0.0011 (0.0014)	
training:	Epoch: [62][18/408]	Loss 0.0023 (0.0015)	
training:	Epoch: [62][19/408]	Loss 0.0043 (0.0016)	
training:	Epoch: [62][20/408]	Loss 0.0008 (0.0016)	
training:	Epoch: [62][21/408]	Loss 0.0008 (0.0015)	
training:	Epoch: [62][22/408]	Loss 0.0008 (0.0015)	
training:	Epoch: [62][23/408]	Loss 0.0016 (0.0015)	
training:	Epoch: [62][24/408]	Loss 0.0008 (0.0015)	
training:	Epoch: [62][25/408]	Loss 0.0008 (0.0014)	
training:	Epoch: [62][26/408]	Loss 0.0007 (0.0014)	
training:	Epoch: [62][27/408]	Loss 0.0009 (0.0014)	
training:	Epoch: [62][28/408]	Loss 0.0033 (0.0015)	
training:	Epoch: [62][29/408]	Loss 0.0015 (0.0015)	
training:	Epoch: [62][30/408]	Loss 0.0008 (0.0014)	
training:	Epoch: [62][31/408]	Loss 0.0007 (0.0014)	
training:	Epoch: [62][32/408]	Loss 0.0018 (0.0014)	
training:	Epoch: [62][33/408]	Loss 0.0008 (0.0014)	
training:	Epoch: [62][34/408]	Loss 0.0011 (0.0014)	
training:	Epoch: [62][35/408]	Loss 0.0009 (0.0014)	
training:	Epoch: [62][36/408]	Loss 0.0010 (0.0014)	
training:	Epoch: [62][37/408]	Loss 0.0007 (0.0014)	
training:	Epoch: [62][38/408]	Loss 0.0008 (0.0013)	
training:	Epoch: [62][39/408]	Loss 0.0008 (0.0013)	
training:	Epoch: [62][40/408]	Loss 0.0008 (0.0013)	
training:	Epoch: [62][41/408]	Loss 0.0011 (0.0013)	
training:	Epoch: [62][42/408]	Loss 0.0031 (0.0014)	
training:	Epoch: [62][43/408]	Loss 0.0021 (0.0014)	
training:	Epoch: [62][44/408]	Loss 0.0009 (0.0014)	
training:	Epoch: [62][45/408]	Loss 0.0011 (0.0014)	
training:	Epoch: [62][46/408]	Loss 0.0009 (0.0013)	
training:	Epoch: [62][47/408]	Loss 0.0009 (0.0013)	
training:	Epoch: [62][48/408]	Loss 0.0008 (0.0013)	
training:	Epoch: [62][49/408]	Loss 0.0008 (0.0013)	
training:	Epoch: [62][50/408]	Loss 0.0007 (0.0013)	
training:	Epoch: [62][51/408]	Loss 0.0007 (0.0013)	
training:	Epoch: [62][52/408]	Loss 0.0007 (0.0013)	
training:	Epoch: [62][53/408]	Loss 0.0010 (0.0013)	
training:	Epoch: [62][54/408]	Loss 0.0011 (0.0013)	
training:	Epoch: [62][55/408]	Loss 0.0015 (0.0013)	
training:	Epoch: [62][56/408]	Loss 0.0014 (0.0013)	
training:	Epoch: [62][57/408]	Loss 0.0011 (0.0013)	
training:	Epoch: [62][58/408]	Loss 0.0014 (0.0013)	
training:	Epoch: [62][59/408]	Loss 0.0007 (0.0013)	
training:	Epoch: [62][60/408]	Loss 0.0078 (0.0014)	
training:	Epoch: [62][61/408]	Loss 0.0012 (0.0014)	
training:	Epoch: [62][62/408]	Loss 0.0009 (0.0014)	
training:	Epoch: [62][63/408]	Loss 0.0007 (0.0014)	
training:	Epoch: [62][64/408]	Loss 0.0007 (0.0013)	
training:	Epoch: [62][65/408]	Loss 0.0009 (0.0013)	
training:	Epoch: [62][66/408]	Loss 0.0008 (0.0013)	
training:	Epoch: [62][67/408]	Loss 0.0010 (0.0013)	
training:	Epoch: [62][68/408]	Loss 0.0008 (0.0013)	
training:	Epoch: [62][69/408]	Loss 0.0007 (0.0013)	
training:	Epoch: [62][70/408]	Loss 0.0008 (0.0013)	
training:	Epoch: [62][71/408]	Loss 0.0010 (0.0013)	
training:	Epoch: [62][72/408]	Loss 0.0008 (0.0013)	
training:	Epoch: [62][73/408]	Loss 0.0018 (0.0013)	
training:	Epoch: [62][74/408]	Loss 0.0012 (0.0013)	
training:	Epoch: [62][75/408]	Loss 0.0008 (0.0013)	
training:	Epoch: [62][76/408]	Loss 0.0054 (0.0013)	
training:	Epoch: [62][77/408]	Loss 0.0010 (0.0013)	
training:	Epoch: [62][78/408]	Loss 0.0010 (0.0013)	
training:	Epoch: [62][79/408]	Loss 0.0012 (0.0013)	
training:	Epoch: [62][80/408]	Loss 0.0010 (0.0013)	
training:	Epoch: [62][81/408]	Loss 0.0046 (0.0014)	
training:	Epoch: [62][82/408]	Loss 0.0011 (0.0014)	
training:	Epoch: [62][83/408]	Loss 0.0008 (0.0014)	
training:	Epoch: [62][84/408]	Loss 0.0006 (0.0013)	
training:	Epoch: [62][85/408]	Loss 0.0009 (0.0013)	
training:	Epoch: [62][86/408]	Loss 0.0007 (0.0013)	
training:	Epoch: [62][87/408]	Loss 0.0009 (0.0013)	
training:	Epoch: [62][88/408]	Loss 0.0009 (0.0013)	
training:	Epoch: [62][89/408]	Loss 0.0007 (0.0013)	
training:	Epoch: [62][90/408]	Loss 0.0007 (0.0013)	
training:	Epoch: [62][91/408]	Loss 0.0011 (0.0013)	
training:	Epoch: [62][92/408]	Loss 0.0007 (0.0013)	
training:	Epoch: [62][93/408]	Loss 0.0008 (0.0013)	
training:	Epoch: [62][94/408]	Loss 0.0008 (0.0013)	
training:	Epoch: [62][95/408]	Loss 0.0008 (0.0013)	
training:	Epoch: [62][96/408]	Loss 0.0007 (0.0013)	
training:	Epoch: [62][97/408]	Loss 0.0013 (0.0013)	
training:	Epoch: [62][98/408]	Loss 0.0008 (0.0013)	
training:	Epoch: [62][99/408]	Loss 0.0007 (0.0013)	
training:	Epoch: [62][100/408]	Loss 0.0010 (0.0013)	
training:	Epoch: [62][101/408]	Loss 0.0008 (0.0013)	
training:	Epoch: [62][102/408]	Loss 0.0007 (0.0013)	
training:	Epoch: [62][103/408]	Loss 0.0008 (0.0013)	
training:	Epoch: [62][104/408]	Loss 0.0018 (0.0013)	
training:	Epoch: [62][105/408]	Loss 0.0022 (0.0013)	
training:	Epoch: [62][106/408]	Loss 0.0010 (0.0013)	
training:	Epoch: [62][107/408]	Loss 0.0010 (0.0013)	
training:	Epoch: [62][108/408]	Loss 0.0009 (0.0013)	
training:	Epoch: [62][109/408]	Loss 0.0007 (0.0013)	
training:	Epoch: [62][110/408]	Loss 0.0008 (0.0012)	
training:	Epoch: [62][111/408]	Loss 0.0010 (0.0012)	
training:	Epoch: [62][112/408]	Loss 0.0036 (0.0013)	
training:	Epoch: [62][113/408]	Loss 0.0009 (0.0013)	
training:	Epoch: [62][114/408]	Loss 0.0009 (0.0013)	
training:	Epoch: [62][115/408]	Loss 0.0015 (0.0013)	
training:	Epoch: [62][116/408]	Loss 0.0010 (0.0013)	
training:	Epoch: [62][117/408]	Loss 0.0015 (0.0013)	
training:	Epoch: [62][118/408]	Loss 0.0015 (0.0013)	
training:	Epoch: [62][119/408]	Loss 0.0008 (0.0013)	
training:	Epoch: [62][120/408]	Loss 0.0007 (0.0013)	
training:	Epoch: [62][121/408]	Loss 0.0009 (0.0013)	
training:	Epoch: [62][122/408]	Loss 0.0007 (0.0012)	
training:	Epoch: [62][123/408]	Loss 0.0010 (0.0012)	
training:	Epoch: [62][124/408]	Loss 0.0013 (0.0012)	
training:	Epoch: [62][125/408]	Loss 0.0009 (0.0012)	
training:	Epoch: [62][126/408]	Loss 0.0010 (0.0012)	
training:	Epoch: [62][127/408]	Loss 0.0008 (0.0012)	
training:	Epoch: [62][128/408]	Loss 0.0013 (0.0012)	
training:	Epoch: [62][129/408]	Loss 0.0010 (0.0012)	
training:	Epoch: [62][130/408]	Loss 0.0007 (0.0012)	
training:	Epoch: [62][131/408]	Loss 0.0007 (0.0012)	
training:	Epoch: [62][132/408]	Loss 0.0007 (0.0012)	
training:	Epoch: [62][133/408]	Loss 0.0007 (0.0012)	
training:	Epoch: [62][134/408]	Loss 0.0009 (0.0012)	
training:	Epoch: [62][135/408]	Loss 0.0007 (0.0012)	
training:	Epoch: [62][136/408]	Loss 0.0014 (0.0012)	
training:	Epoch: [62][137/408]	Loss 0.0008 (0.0012)	
training:	Epoch: [62][138/408]	Loss 0.0007 (0.0012)	
training:	Epoch: [62][139/408]	Loss 0.0007 (0.0012)	
training:	Epoch: [62][140/408]	Loss 0.0008 (0.0012)	
training:	Epoch: [62][141/408]	Loss 0.0015 (0.0012)	
training:	Epoch: [62][142/408]	Loss 0.0007 (0.0012)	
training:	Epoch: [62][143/408]	Loss 0.0007 (0.0012)	
training:	Epoch: [62][144/408]	Loss 0.0046 (0.0012)	
training:	Epoch: [62][145/408]	Loss 0.0007 (0.0012)	
training:	Epoch: [62][146/408]	Loss 0.0007 (0.0012)	
training:	Epoch: [62][147/408]	Loss 0.0008 (0.0012)	
training:	Epoch: [62][148/408]	Loss 0.0007 (0.0012)	
training:	Epoch: [62][149/408]	Loss 0.0007 (0.0012)	
training:	Epoch: [62][150/408]	Loss 0.0006 (0.0012)	
training:	Epoch: [62][151/408]	Loss 0.0007 (0.0012)	
training:	Epoch: [62][152/408]	Loss 0.0009 (0.0012)	
training:	Epoch: [62][153/408]	Loss 0.0008 (0.0012)	
training:	Epoch: [62][154/408]	Loss 0.0144 (0.0013)	
training:	Epoch: [62][155/408]	Loss 0.0011 (0.0013)	
training:	Epoch: [62][156/408]	Loss 0.0007 (0.0013)	
training:	Epoch: [62][157/408]	Loss 0.0007 (0.0013)	
training:	Epoch: [62][158/408]	Loss 0.0009 (0.0013)	
training:	Epoch: [62][159/408]	Loss 0.0031 (0.0013)	
training:	Epoch: [62][160/408]	Loss 0.0007 (0.0013)	
training:	Epoch: [62][161/408]	Loss 0.0043 (0.0013)	
training:	Epoch: [62][162/408]	Loss 0.0024 (0.0013)	
training:	Epoch: [62][163/408]	Loss 0.0025 (0.0013)	
training:	Epoch: [62][164/408]	Loss 0.0010 (0.0013)	
training:	Epoch: [62][165/408]	Loss 0.0009 (0.0013)	
training:	Epoch: [62][166/408]	Loss 0.0007 (0.0013)	
training:	Epoch: [62][167/408]	Loss 0.0007 (0.0013)	
training:	Epoch: [62][168/408]	Loss 0.0006 (0.0013)	
training:	Epoch: [62][169/408]	Loss 0.0098 (0.0013)	
training:	Epoch: [62][170/408]	Loss 0.0009 (0.0013)	
training:	Epoch: [62][171/408]	Loss 0.0007 (0.0013)	
training:	Epoch: [62][172/408]	Loss 0.0011 (0.0013)	
training:	Epoch: [62][173/408]	Loss 0.0008 (0.0013)	
training:	Epoch: [62][174/408]	Loss 0.0007 (0.0013)	
training:	Epoch: [62][175/408]	Loss 0.0007 (0.0013)	
training:	Epoch: [62][176/408]	Loss 0.0008 (0.0013)	
training:	Epoch: [62][177/408]	Loss 0.0010 (0.0013)	
training:	Epoch: [62][178/408]	Loss 0.0008 (0.0013)	
training:	Epoch: [62][179/408]	Loss 0.0016 (0.0013)	
training:	Epoch: [62][180/408]	Loss 0.0007 (0.0013)	
training:	Epoch: [62][181/408]	Loss 0.0007 (0.0013)	
training:	Epoch: [62][182/408]	Loss 0.0007 (0.0013)	
training:	Epoch: [62][183/408]	Loss 0.0008 (0.0013)	
training:	Epoch: [62][184/408]	Loss 0.0007 (0.0013)	
training:	Epoch: [62][185/408]	Loss 0.0012 (0.0013)	
training:	Epoch: [62][186/408]	Loss 0.0045 (0.0013)	
training:	Epoch: [62][187/408]	Loss 0.0007 (0.0013)	
training:	Epoch: [62][188/408]	Loss 0.0034 (0.0013)	
training:	Epoch: [62][189/408]	Loss 0.0008 (0.0013)	
training:	Epoch: [62][190/408]	Loss 0.0009 (0.0013)	
training:	Epoch: [62][191/408]	Loss 0.0031 (0.0013)	
training:	Epoch: [62][192/408]	Loss 0.0013 (0.0013)	
training:	Epoch: [62][193/408]	Loss 0.0009 (0.0013)	
training:	Epoch: [62][194/408]	Loss 0.0012 (0.0013)	
training:	Epoch: [62][195/408]	Loss 0.0010 (0.0013)	
training:	Epoch: [62][196/408]	Loss 0.0008 (0.0013)	
training:	Epoch: [62][197/408]	Loss 0.0008 (0.0013)	
training:	Epoch: [62][198/408]	Loss 0.0007 (0.0013)	
training:	Epoch: [62][199/408]	Loss 0.0008 (0.0013)	
training:	Epoch: [62][200/408]	Loss 0.0008 (0.0013)	
training:	Epoch: [62][201/408]	Loss 0.0006 (0.0013)	
training:	Epoch: [62][202/408]	Loss 0.0010 (0.0013)	
training:	Epoch: [62][203/408]	Loss 0.0007 (0.0013)	
training:	Epoch: [62][204/408]	Loss 0.0061 (0.0013)	
training:	Epoch: [62][205/408]	Loss 0.0007 (0.0013)	
training:	Epoch: [62][206/408]	Loss 0.0013 (0.0013)	
training:	Epoch: [62][207/408]	Loss 0.0009 (0.0013)	
training:	Epoch: [62][208/408]	Loss 0.0015 (0.0013)	
training:	Epoch: [62][209/408]	Loss 0.0008 (0.0013)	
training:	Epoch: [62][210/408]	Loss 0.0008 (0.0013)	
training:	Epoch: [62][211/408]	Loss 0.0028 (0.0013)	
training:	Epoch: [62][212/408]	Loss 0.0006 (0.0013)	
training:	Epoch: [62][213/408]	Loss 0.0011 (0.0013)	
training:	Epoch: [62][214/408]	Loss 0.0007 (0.0013)	
training:	Epoch: [62][215/408]	Loss 0.0017 (0.0013)	
training:	Epoch: [62][216/408]	Loss 0.0007 (0.0013)	
training:	Epoch: [62][217/408]	Loss 0.0011 (0.0013)	
training:	Epoch: [62][218/408]	Loss 0.0009 (0.0013)	
training:	Epoch: [62][219/408]	Loss 0.0007 (0.0013)	
training:	Epoch: [62][220/408]	Loss 0.0011 (0.0013)	
training:	Epoch: [62][221/408]	Loss 0.0009 (0.0013)	
training:	Epoch: [62][222/408]	Loss 0.0007 (0.0013)	
training:	Epoch: [62][223/408]	Loss 0.0008 (0.0013)	
training:	Epoch: [62][224/408]	Loss 0.0008 (0.0013)	
training:	Epoch: [62][225/408]	Loss 0.0008 (0.0013)	
training:	Epoch: [62][226/408]	Loss 0.0008 (0.0013)	
training:	Epoch: [62][227/408]	Loss 0.0008 (0.0013)	
training:	Epoch: [62][228/408]	Loss 0.0009 (0.0013)	
training:	Epoch: [62][229/408]	Loss 0.0011 (0.0013)	
training:	Epoch: [62][230/408]	Loss 0.0031 (0.0013)	
training:	Epoch: [62][231/408]	Loss 0.0008 (0.0013)	
training:	Epoch: [62][232/408]	Loss 0.0007 (0.0013)	
training:	Epoch: [62][233/408]	Loss 0.0007 (0.0013)	
training:	Epoch: [62][234/408]	Loss 0.0009 (0.0013)	
training:	Epoch: [62][235/408]	Loss 0.0008 (0.0013)	
training:	Epoch: [62][236/408]	Loss 0.0008 (0.0013)	
training:	Epoch: [62][237/408]	Loss 0.0007 (0.0013)	
training:	Epoch: [62][238/408]	Loss 0.0007 (0.0013)	
training:	Epoch: [62][239/408]	Loss 0.0007 (0.0013)	
training:	Epoch: [62][240/408]	Loss 0.0007 (0.0013)	
training:	Epoch: [62][241/408]	Loss 0.0009 (0.0013)	
training:	Epoch: [62][242/408]	Loss 0.0006 (0.0013)	
training:	Epoch: [62][243/408]	Loss 0.0030 (0.0013)	
training:	Epoch: [62][244/408]	Loss 0.0103 (0.0013)	
training:	Epoch: [62][245/408]	Loss 0.0008 (0.0013)	
training:	Epoch: [62][246/408]	Loss 0.0008 (0.0013)	
training:	Epoch: [62][247/408]	Loss 0.0010 (0.0013)	
training:	Epoch: [62][248/408]	Loss 0.0009 (0.0013)	
training:	Epoch: [62][249/408]	Loss 0.0007 (0.0013)	
training:	Epoch: [62][250/408]	Loss 0.0008 (0.0013)	
training:	Epoch: [62][251/408]	Loss 0.0009 (0.0013)	
training:	Epoch: [62][252/408]	Loss 0.0007 (0.0013)	
training:	Epoch: [62][253/408]	Loss 0.0007 (0.0013)	
training:	Epoch: [62][254/408]	Loss 0.0008 (0.0013)	
training:	Epoch: [62][255/408]	Loss 0.0007 (0.0013)	
training:	Epoch: [62][256/408]	Loss 0.0010 (0.0013)	
training:	Epoch: [62][257/408]	Loss 0.0007 (0.0013)	
training:	Epoch: [62][258/408]	Loss 0.0009 (0.0013)	
training:	Epoch: [62][259/408]	Loss 0.0008 (0.0013)	
training:	Epoch: [62][260/408]	Loss 0.0008 (0.0013)	
training:	Epoch: [62][261/408]	Loss 0.0007 (0.0013)	
training:	Epoch: [62][262/408]	Loss 0.0009 (0.0013)	
training:	Epoch: [62][263/408]	Loss 0.0006 (0.0013)	
training:	Epoch: [62][264/408]	Loss 0.0009 (0.0013)	
training:	Epoch: [62][265/408]	Loss 0.0008 (0.0013)	
training:	Epoch: [62][266/408]	Loss 0.0010 (0.0013)	
training:	Epoch: [62][267/408]	Loss 0.0007 (0.0013)	
training:	Epoch: [62][268/408]	Loss 0.0007 (0.0013)	
training:	Epoch: [62][269/408]	Loss 0.0012 (0.0013)	
training:	Epoch: [62][270/408]	Loss 0.0015 (0.0013)	
training:	Epoch: [62][271/408]	Loss 0.0007 (0.0013)	
training:	Epoch: [62][272/408]	Loss 0.0036 (0.0013)	
training:	Epoch: [62][273/408]	Loss 0.0047 (0.0013)	
training:	Epoch: [62][274/408]	Loss 0.0006 (0.0013)	
training:	Epoch: [62][275/408]	Loss 0.0030 (0.0013)	
training:	Epoch: [62][276/408]	Loss 0.0010 (0.0013)	
training:	Epoch: [62][277/408]	Loss 0.0007 (0.0013)	
training:	Epoch: [62][278/408]	Loss 0.0007 (0.0013)	
training:	Epoch: [62][279/408]	Loss 0.0008 (0.0013)	
training:	Epoch: [62][280/408]	Loss 0.0008 (0.0013)	
training:	Epoch: [62][281/408]	Loss 0.0008 (0.0013)	
training:	Epoch: [62][282/408]	Loss 0.0006 (0.0013)	
training:	Epoch: [62][283/408]	Loss 0.0009 (0.0013)	
training:	Epoch: [62][284/408]	Loss 0.0007 (0.0013)	
training:	Epoch: [62][285/408]	Loss 0.0010 (0.0013)	
training:	Epoch: [62][286/408]	Loss 0.0032 (0.0013)	
training:	Epoch: [62][287/408]	Loss 0.0008 (0.0013)	
training:	Epoch: [62][288/408]	Loss 0.0008 (0.0013)	
training:	Epoch: [62][289/408]	Loss 0.0008 (0.0013)	
training:	Epoch: [62][290/408]	Loss 0.0008 (0.0013)	
training:	Epoch: [62][291/408]	Loss 0.0007 (0.0013)	
training:	Epoch: [62][292/408]	Loss 0.0007 (0.0013)	
training:	Epoch: [62][293/408]	Loss 0.0007 (0.0013)	
training:	Epoch: [62][294/408]	Loss 0.0009 (0.0013)	
training:	Epoch: [62][295/408]	Loss 0.0009 (0.0013)	
training:	Epoch: [62][296/408]	Loss 0.0006 (0.0013)	
training:	Epoch: [62][297/408]	Loss 0.0019 (0.0013)	
training:	Epoch: [62][298/408]	Loss 0.0008 (0.0013)	
training:	Epoch: [62][299/408]	Loss 0.0006 (0.0013)	
training:	Epoch: [62][300/408]	Loss 0.0008 (0.0013)	
training:	Epoch: [62][301/408]	Loss 0.0011 (0.0013)	
training:	Epoch: [62][302/408]	Loss 0.0007 (0.0013)	
training:	Epoch: [62][303/408]	Loss 0.0106 (0.0013)	
training:	Epoch: [62][304/408]	Loss 0.0010 (0.0013)	
training:	Epoch: [62][305/408]	Loss 0.0052 (0.0013)	
training:	Epoch: [62][306/408]	Loss 0.0008 (0.0013)	
training:	Epoch: [62][307/408]	Loss 0.0007 (0.0013)	
training:	Epoch: [62][308/408]	Loss 0.0007 (0.0013)	
training:	Epoch: [62][309/408]	Loss 0.0019 (0.0013)	
training:	Epoch: [62][310/408]	Loss 0.0007 (0.0013)	
training:	Epoch: [62][311/408]	Loss 0.0007 (0.0013)	
training:	Epoch: [62][312/408]	Loss 0.0007 (0.0013)	
training:	Epoch: [62][313/408]	Loss 0.0006 (0.0013)	
training:	Epoch: [62][314/408]	Loss 0.0024 (0.0013)	
training:	Epoch: [62][315/408]	Loss 0.0009 (0.0013)	
training:	Epoch: [62][316/408]	Loss 0.0028 (0.0013)	
training:	Epoch: [62][317/408]	Loss 0.0008 (0.0013)	
training:	Epoch: [62][318/408]	Loss 0.0076 (0.0013)	
training:	Epoch: [62][319/408]	Loss 0.0008 (0.0013)	
training:	Epoch: [62][320/408]	Loss 0.0008 (0.0013)	
training:	Epoch: [62][321/408]	Loss 0.0008 (0.0013)	
training:	Epoch: [62][322/408]	Loss 0.0009 (0.0013)	
training:	Epoch: [62][323/408]	Loss 0.0016 (0.0013)	
training:	Epoch: [62][324/408]	Loss 0.0006 (0.0013)	
training:	Epoch: [62][325/408]	Loss 0.0010 (0.0013)	
training:	Epoch: [62][326/408]	Loss 0.0007 (0.0013)	
training:	Epoch: [62][327/408]	Loss 0.0007 (0.0013)	
training:	Epoch: [62][328/408]	Loss 0.0007 (0.0013)	
training:	Epoch: [62][329/408]	Loss 0.0011 (0.0013)	
training:	Epoch: [62][330/408]	Loss 0.0009 (0.0013)	
training:	Epoch: [62][331/408]	Loss 0.0008 (0.0013)	
training:	Epoch: [62][332/408]	Loss 0.0008 (0.0013)	
training:	Epoch: [62][333/408]	Loss 0.0007 (0.0013)	
training:	Epoch: [62][334/408]	Loss 0.0008 (0.0013)	
training:	Epoch: [62][335/408]	Loss 0.0010 (0.0013)	
training:	Epoch: [62][336/408]	Loss 0.0008 (0.0013)	
training:	Epoch: [62][337/408]	Loss 0.0008 (0.0013)	
training:	Epoch: [62][338/408]	Loss 0.0011 (0.0013)	
training:	Epoch: [62][339/408]	Loss 0.0007 (0.0013)	
training:	Epoch: [62][340/408]	Loss 0.0011 (0.0013)	
training:	Epoch: [62][341/408]	Loss 0.0008 (0.0013)	
training:	Epoch: [62][342/408]	Loss 0.0007 (0.0013)	
training:	Epoch: [62][343/408]	Loss 0.0010 (0.0013)	
training:	Epoch: [62][344/408]	Loss 0.0008 (0.0013)	
training:	Epoch: [62][345/408]	Loss 0.0008 (0.0013)	
training:	Epoch: [62][346/408]	Loss 0.0006 (0.0013)	
training:	Epoch: [62][347/408]	Loss 0.0171 (0.0013)	
training:	Epoch: [62][348/408]	Loss 0.0011 (0.0013)	
training:	Epoch: [62][349/408]	Loss 0.0034 (0.0013)	
training:	Epoch: [62][350/408]	Loss 0.0007 (0.0013)	
training:	Epoch: [62][351/408]	Loss 0.0009 (0.0013)	
training:	Epoch: [62][352/408]	Loss 0.0039 (0.0013)	
training:	Epoch: [62][353/408]	Loss 0.0007 (0.0013)	
training:	Epoch: [62][354/408]	Loss 0.0019 (0.0013)	
training:	Epoch: [62][355/408]	Loss 0.0010 (0.0013)	
training:	Epoch: [62][356/408]	Loss 0.0006 (0.0013)	
training:	Epoch: [62][357/408]	Loss 0.0025 (0.0013)	
training:	Epoch: [62][358/408]	Loss 0.0016 (0.0013)	
training:	Epoch: [62][359/408]	Loss 0.0006 (0.0013)	
training:	Epoch: [62][360/408]	Loss 0.0007 (0.0013)	
training:	Epoch: [62][361/408]	Loss 0.0008 (0.0013)	
training:	Epoch: [62][362/408]	Loss 0.0007 (0.0013)	
training:	Epoch: [62][363/408]	Loss 0.0029 (0.0013)	
training:	Epoch: [62][364/408]	Loss 0.0007 (0.0013)	
training:	Epoch: [62][365/408]	Loss 0.0021 (0.0013)	
training:	Epoch: [62][366/408]	Loss 0.0035 (0.0013)	
training:	Epoch: [62][367/408]	Loss 0.0032 (0.0013)	
training:	Epoch: [62][368/408]	Loss 0.0034 (0.0014)	
training:	Epoch: [62][369/408]	Loss 0.0007 (0.0013)	
training:	Epoch: [62][370/408]	Loss 0.0014 (0.0013)	
training:	Epoch: [62][371/408]	Loss 0.0007 (0.0013)	
training:	Epoch: [62][372/408]	Loss 0.0008 (0.0013)	
training:	Epoch: [62][373/408]	Loss 0.0024 (0.0013)	
training:	Epoch: [62][374/408]	Loss 0.0006 (0.0013)	
training:	Epoch: [62][375/408]	Loss 0.0007 (0.0013)	
training:	Epoch: [62][376/408]	Loss 0.0006 (0.0013)	
training:	Epoch: [62][377/408]	Loss 0.0008 (0.0013)	
training:	Epoch: [62][378/408]	Loss 0.0007 (0.0013)	
training:	Epoch: [62][379/408]	Loss 0.0006 (0.0013)	
training:	Epoch: [62][380/408]	Loss 0.0007 (0.0013)	
training:	Epoch: [62][381/408]	Loss 0.0009 (0.0013)	
training:	Epoch: [62][382/408]	Loss 0.0006 (0.0013)	
training:	Epoch: [62][383/408]	Loss 0.0006 (0.0013)	
training:	Epoch: [62][384/408]	Loss 0.0009 (0.0013)	
training:	Epoch: [62][385/408]	Loss 0.0007 (0.0013)	
training:	Epoch: [62][386/408]	Loss 0.0008 (0.0013)	
training:	Epoch: [62][387/408]	Loss 0.0007 (0.0013)	
training:	Epoch: [62][388/408]	Loss 0.0009 (0.0013)	
training:	Epoch: [62][389/408]	Loss 0.0007 (0.0013)	
training:	Epoch: [62][390/408]	Loss 0.0007 (0.0013)	
training:	Epoch: [62][391/408]	Loss 0.0006 (0.0013)	
training:	Epoch: [62][392/408]	Loss 0.0009 (0.0013)	
training:	Epoch: [62][393/408]	Loss 0.0011 (0.0013)	
training:	Epoch: [62][394/408]	Loss 0.0006 (0.0013)	
training:	Epoch: [62][395/408]	Loss 0.0011 (0.0013)	
training:	Epoch: [62][396/408]	Loss 0.0010 (0.0013)	
training:	Epoch: [62][397/408]	Loss 0.0007 (0.0013)	
training:	Epoch: [62][398/408]	Loss 0.0007 (0.0013)	
training:	Epoch: [62][399/408]	Loss 0.0015 (0.0013)	
training:	Epoch: [62][400/408]	Loss 0.0008 (0.0013)	
training:	Epoch: [62][401/408]	Loss 0.0009 (0.0013)	
training:	Epoch: [62][402/408]	Loss 0.0011 (0.0013)	
training:	Epoch: [62][403/408]	Loss 0.0012 (0.0013)	
training:	Epoch: [62][404/408]	Loss 0.0007 (0.0013)	
training:	Epoch: [62][405/408]	Loss 0.0007 (0.0013)	
training:	Epoch: [62][406/408]	Loss 0.0007 (0.0013)	
training:	Epoch: [62][407/408]	Loss 0.0048 (0.0013)	
training:	Epoch: [62][408/408]	Loss 0.0007 (0.0013)	
Training:	 Loss: 0.0013

Training:	 ACC: 1.0000 1.0000 1.0000 1.0000
Validation:	 ACC: 0.7831 0.7838 0.7984 0.7679
Validation:	 Best_BACC: 0.8226 0.8250 0.8772 0.7679
Validation:	 Loss: 1.2166
Pretraining:	Epoch 63/200
----------
training:	Epoch: [63][1/408]	Loss 0.0044 (0.0044)	
training:	Epoch: [63][2/408]	Loss 0.0016 (0.0030)	
training:	Epoch: [63][3/408]	Loss 0.0048 (0.0036)	
training:	Epoch: [63][4/408]	Loss 0.0006 (0.0028)	
training:	Epoch: [63][5/408]	Loss 0.0007 (0.0024)	
training:	Epoch: [63][6/408]	Loss 0.0013 (0.0022)	
training:	Epoch: [63][7/408]	Loss 0.0081 (0.0031)	
training:	Epoch: [63][8/408]	Loss 0.0007 (0.0028)	
training:	Epoch: [63][9/408]	Loss 0.0011 (0.0026)	
training:	Epoch: [63][10/408]	Loss 0.0016 (0.0025)	
training:	Epoch: [63][11/408]	Loss 0.0008 (0.0023)	
training:	Epoch: [63][12/408]	Loss 0.0006 (0.0022)	
training:	Epoch: [63][13/408]	Loss 0.0011 (0.0021)	
training:	Epoch: [63][14/408]	Loss 0.0012 (0.0020)	
training:	Epoch: [63][15/408]	Loss 0.0007 (0.0020)	
training:	Epoch: [63][16/408]	Loss 0.0006 (0.0019)	
training:	Epoch: [63][17/408]	Loss 0.0007 (0.0018)	
training:	Epoch: [63][18/408]	Loss 0.0006 (0.0017)	
training:	Epoch: [63][19/408]	Loss 0.0007 (0.0017)	
training:	Epoch: [63][20/408]	Loss 0.0006 (0.0016)	
training:	Epoch: [63][21/408]	Loss 0.0007 (0.0016)	
training:	Epoch: [63][22/408]	Loss 0.0010 (0.0015)	
training:	Epoch: [63][23/408]	Loss 0.0007 (0.0015)	
training:	Epoch: [63][24/408]	Loss 0.0006 (0.0015)	
training:	Epoch: [63][25/408]	Loss 0.0007 (0.0014)	
training:	Epoch: [63][26/408]	Loss 0.0007 (0.0014)	
training:	Epoch: [63][27/408]	Loss 0.0009 (0.0014)	
training:	Epoch: [63][28/408]	Loss 0.0040 (0.0015)	
training:	Epoch: [63][29/408]	Loss 0.0010 (0.0015)	
training:	Epoch: [63][30/408]	Loss 0.0006 (0.0014)	
training:	Epoch: [63][31/408]	Loss 0.0007 (0.0014)	
training:	Epoch: [63][32/408]	Loss 0.0007 (0.0014)	
training:	Epoch: [63][33/408]	Loss 0.0006 (0.0014)	
training:	Epoch: [63][34/408]	Loss 0.0031 (0.0014)	
training:	Epoch: [63][35/408]	Loss 0.0009 (0.0014)	
training:	Epoch: [63][36/408]	Loss 0.0006 (0.0014)	
training:	Epoch: [63][37/408]	Loss 0.0006 (0.0014)	
training:	Epoch: [63][38/408]	Loss 0.0007 (0.0013)	
training:	Epoch: [63][39/408]	Loss 0.0007 (0.0013)	
training:	Epoch: [63][40/408]	Loss 0.0011 (0.0013)	
training:	Epoch: [63][41/408]	Loss 0.0014 (0.0013)	
training:	Epoch: [63][42/408]	Loss 0.0043 (0.0014)	
training:	Epoch: [63][43/408]	Loss 0.0007 (0.0014)	
training:	Epoch: [63][44/408]	Loss 0.0006 (0.0014)	
training:	Epoch: [63][45/408]	Loss 0.0007 (0.0013)	
training:	Epoch: [63][46/408]	Loss 0.0007 (0.0013)	
training:	Epoch: [63][47/408]	Loss 0.0007 (0.0013)	
training:	Epoch: [63][48/408]	Loss 0.0009 (0.0013)	
training:	Epoch: [63][49/408]	Loss 0.0007 (0.0013)	
training:	Epoch: [63][50/408]	Loss 0.0006 (0.0013)	
training:	Epoch: [63][51/408]	Loss 0.0011 (0.0013)	
training:	Epoch: [63][52/408]	Loss 0.0007 (0.0013)	
training:	Epoch: [63][53/408]	Loss 0.0006 (0.0013)	
training:	Epoch: [63][54/408]	Loss 0.0010 (0.0013)	
training:	Epoch: [63][55/408]	Loss 0.0007 (0.0012)	
training:	Epoch: [63][56/408]	Loss 0.0007 (0.0012)	
training:	Epoch: [63][57/408]	Loss 0.0008 (0.0012)	
training:	Epoch: [63][58/408]	Loss 0.0008 (0.0012)	
training:	Epoch: [63][59/408]	Loss 0.0006 (0.0012)	
training:	Epoch: [63][60/408]	Loss 0.0007 (0.0012)	
training:	Epoch: [63][61/408]	Loss 0.0007 (0.0012)	
training:	Epoch: [63][62/408]	Loss 0.0008 (0.0012)	
training:	Epoch: [63][63/408]	Loss 0.0008 (0.0012)	
training:	Epoch: [63][64/408]	Loss 0.0012 (0.0012)	
training:	Epoch: [63][65/408]	Loss 0.0009 (0.0012)	
training:	Epoch: [63][66/408]	Loss 0.0041 (0.0012)	
training:	Epoch: [63][67/408]	Loss 0.0008 (0.0012)	
training:	Epoch: [63][68/408]	Loss 0.0008 (0.0012)	
training:	Epoch: [63][69/408]	Loss 0.0008 (0.0012)	
training:	Epoch: [63][70/408]	Loss 0.0008 (0.0012)	
training:	Epoch: [63][71/408]	Loss 0.0024 (0.0012)	
training:	Epoch: [63][72/408]	Loss 0.0007 (0.0012)	
training:	Epoch: [63][73/408]	Loss 0.0007 (0.0012)	
training:	Epoch: [63][74/408]	Loss 0.0006 (0.0012)	
training:	Epoch: [63][75/408]	Loss 0.0007 (0.0012)	
training:	Epoch: [63][76/408]	Loss 0.0030 (0.0012)	
training:	Epoch: [63][77/408]	Loss 0.0008 (0.0012)	
training:	Epoch: [63][78/408]	Loss 0.0008 (0.0012)	
training:	Epoch: [63][79/408]	Loss 0.0027 (0.0012)	
training:	Epoch: [63][80/408]	Loss 0.0007 (0.0012)	
training:	Epoch: [63][81/408]	Loss 0.0006 (0.0012)	
training:	Epoch: [63][82/408]	Loss 0.0009 (0.0012)	
training:	Epoch: [63][83/408]	Loss 0.0006 (0.0012)	
training:	Epoch: [63][84/408]	Loss 0.0008 (0.0012)	
training:	Epoch: [63][85/408]	Loss 0.0007 (0.0012)	
training:	Epoch: [63][86/408]	Loss 0.0009 (0.0012)	
training:	Epoch: [63][87/408]	Loss 0.0009 (0.0012)	
training:	Epoch: [63][88/408]	Loss 0.0020 (0.0012)	
training:	Epoch: [63][89/408]	Loss 0.0008 (0.0012)	
training:	Epoch: [63][90/408]	Loss 0.0010 (0.0012)	
training:	Epoch: [63][91/408]	Loss 0.0008 (0.0012)	
training:	Epoch: [63][92/408]	Loss 0.0006 (0.0012)	
training:	Epoch: [63][93/408]	Loss 0.0007 (0.0012)	
training:	Epoch: [63][94/408]	Loss 0.0008 (0.0012)	
training:	Epoch: [63][95/408]	Loss 0.0008 (0.0012)	
training:	Epoch: [63][96/408]	Loss 0.0006 (0.0011)	
training:	Epoch: [63][97/408]	Loss 0.0011 (0.0011)	
training:	Epoch: [63][98/408]	Loss 0.0014 (0.0012)	
training:	Epoch: [63][99/408]	Loss 0.0007 (0.0011)	
training:	Epoch: [63][100/408]	Loss 0.0007 (0.0011)	
training:	Epoch: [63][101/408]	Loss 0.0030 (0.0012)	
training:	Epoch: [63][102/408]	Loss 0.0007 (0.0012)	
training:	Epoch: [63][103/408]	Loss 0.0007 (0.0012)	
training:	Epoch: [63][104/408]	Loss 0.0007 (0.0011)	
training:	Epoch: [63][105/408]	Loss 0.0006 (0.0011)	
training:	Epoch: [63][106/408]	Loss 0.0008 (0.0011)	
training:	Epoch: [63][107/408]	Loss 0.0010 (0.0011)	
training:	Epoch: [63][108/408]	Loss 0.0007 (0.0011)	
training:	Epoch: [63][109/408]	Loss 0.0006 (0.0011)	
training:	Epoch: [63][110/408]	Loss 0.0008 (0.0011)	
training:	Epoch: [63][111/408]	Loss 0.0006 (0.0011)	
training:	Epoch: [63][112/408]	Loss 0.0008 (0.0011)	
training:	Epoch: [63][113/408]	Loss 0.0007 (0.0011)	
training:	Epoch: [63][114/408]	Loss 0.0010 (0.0011)	
training:	Epoch: [63][115/408]	Loss 0.0010 (0.0011)	
training:	Epoch: [63][116/408]	Loss 0.0007 (0.0011)	
training:	Epoch: [63][117/408]	Loss 0.0008 (0.0011)	
training:	Epoch: [63][118/408]	Loss 0.0008 (0.0011)	
training:	Epoch: [63][119/408]	Loss 0.0008 (0.0011)	
training:	Epoch: [63][120/408]	Loss 0.0057 (0.0011)	
training:	Epoch: [63][121/408]	Loss 0.0008 (0.0011)	
training:	Epoch: [63][122/408]	Loss 0.0007 (0.0011)	
training:	Epoch: [63][123/408]	Loss 0.0007 (0.0011)	
training:	Epoch: [63][124/408]	Loss 0.0006 (0.0011)	
training:	Epoch: [63][125/408]	Loss 0.0011 (0.0011)	
training:	Epoch: [63][126/408]	Loss 0.0009 (0.0011)	
training:	Epoch: [63][127/408]	Loss 0.0009 (0.0011)	
training:	Epoch: [63][128/408]	Loss 0.0006 (0.0011)	
training:	Epoch: [63][129/408]	Loss 0.0007 (0.0011)	
training:	Epoch: [63][130/408]	Loss 0.0008 (0.0011)	
training:	Epoch: [63][131/408]	Loss 0.0006 (0.0011)	
training:	Epoch: [63][132/408]	Loss 0.0006 (0.0011)	
training:	Epoch: [63][133/408]	Loss 0.0008 (0.0011)	
training:	Epoch: [63][134/408]	Loss 0.0006 (0.0011)	
training:	Epoch: [63][135/408]	Loss 0.0007 (0.0011)	
training:	Epoch: [63][136/408]	Loss 0.0010 (0.0011)	
training:	Epoch: [63][137/408]	Loss 0.0006 (0.0011)	
training:	Epoch: [63][138/408]	Loss 0.0006 (0.0011)	
training:	Epoch: [63][139/408]	Loss 0.0006 (0.0011)	
training:	Epoch: [63][140/408]	Loss 0.0012 (0.0011)	
training:	Epoch: [63][141/408]	Loss 0.0006 (0.0011)	
training:	Epoch: [63][142/408]	Loss 0.0007 (0.0011)	
training:	Epoch: [63][143/408]	Loss 0.0008 (0.0011)	
training:	Epoch: [63][144/408]	Loss 0.0009 (0.0011)	
training:	Epoch: [63][145/408]	Loss 0.0008 (0.0011)	
training:	Epoch: [63][146/408]	Loss 0.0006 (0.0011)	
training:	Epoch: [63][147/408]	Loss 0.0007 (0.0011)	
training:	Epoch: [63][148/408]	Loss 0.0007 (0.0011)	
training:	Epoch: [63][149/408]	Loss 0.0008 (0.0011)	
training:	Epoch: [63][150/408]	Loss 0.0007 (0.0011)	
training:	Epoch: [63][151/408]	Loss 0.0007 (0.0011)	
training:	Epoch: [63][152/408]	Loss 0.0007 (0.0011)	
training:	Epoch: [63][153/408]	Loss 0.0007 (0.0011)	
training:	Epoch: [63][154/408]	Loss 0.0007 (0.0011)	
training:	Epoch: [63][155/408]	Loss 0.0048 (0.0011)	
training:	Epoch: [63][156/408]	Loss 0.0008 (0.0011)	
training:	Epoch: [63][157/408]	Loss 0.0038 (0.0011)	
training:	Epoch: [63][158/408]	Loss 0.0007 (0.0011)	
training:	Epoch: [63][159/408]	Loss 0.0007 (0.0011)	
training:	Epoch: [63][160/408]	Loss 0.0007 (0.0011)	
training:	Epoch: [63][161/408]	Loss 0.0011 (0.0011)	
training:	Epoch: [63][162/408]	Loss 0.0007 (0.0011)	
training:	Epoch: [63][163/408]	Loss 0.0007 (0.0011)	
training:	Epoch: [63][164/408]	Loss 0.0007 (0.0011)	
training:	Epoch: [63][165/408]	Loss 0.0008 (0.0011)	
training:	Epoch: [63][166/408]	Loss 0.0007 (0.0011)	
training:	Epoch: [63][167/408]	Loss 0.0006 (0.0011)	
training:	Epoch: [63][168/408]	Loss 0.0008 (0.0011)	
training:	Epoch: [63][169/408]	Loss 0.0007 (0.0011)	
training:	Epoch: [63][170/408]	Loss 0.0006 (0.0011)	
training:	Epoch: [63][171/408]	Loss 0.0008 (0.0011)	
training:	Epoch: [63][172/408]	Loss 0.0006 (0.0011)	
training:	Epoch: [63][173/408]	Loss 0.0034 (0.0011)	
training:	Epoch: [63][174/408]	Loss 0.0006 (0.0011)	
training:	Epoch: [63][175/408]	Loss 0.0007 (0.0011)	
training:	Epoch: [63][176/408]	Loss 0.0008 (0.0011)	
training:	Epoch: [63][177/408]	Loss 0.0006 (0.0011)	
training:	Epoch: [63][178/408]	Loss 0.0010 (0.0011)	
training:	Epoch: [63][179/408]	Loss 0.0007 (0.0011)	
training:	Epoch: [63][180/408]	Loss 0.0081 (0.0011)	
training:	Epoch: [63][181/408]	Loss 0.0007 (0.0011)	
training:	Epoch: [63][182/408]	Loss 0.0006 (0.0011)	
training:	Epoch: [63][183/408]	Loss 0.0006 (0.0011)	
training:	Epoch: [63][184/408]	Loss 0.0007 (0.0011)	
training:	Epoch: [63][185/408]	Loss 0.0007 (0.0011)	
training:	Epoch: [63][186/408]	Loss 0.0006 (0.0011)	
training:	Epoch: [63][187/408]	Loss 0.0006 (0.0011)	
training:	Epoch: [63][188/408]	Loss 0.0008 (0.0011)	
training:	Epoch: [63][189/408]	Loss 0.0032 (0.0011)	
training:	Epoch: [63][190/408]	Loss 0.0006 (0.0011)	
training:	Epoch: [63][191/408]	Loss 0.0010 (0.0011)	
training:	Epoch: [63][192/408]	Loss 0.0006 (0.0011)	
training:	Epoch: [63][193/408]	Loss 0.0008 (0.0011)	
training:	Epoch: [63][194/408]	Loss 0.0007 (0.0011)	
training:	Epoch: [63][195/408]	Loss 0.0009 (0.0011)	
training:	Epoch: [63][196/408]	Loss 0.0006 (0.0011)	
training:	Epoch: [63][197/408]	Loss 0.0006 (0.0011)	
training:	Epoch: [63][198/408]	Loss 0.0006 (0.0011)	
training:	Epoch: [63][199/408]	Loss 0.0012 (0.0011)	
training:	Epoch: [63][200/408]	Loss 0.0007 (0.0011)	
training:	Epoch: [63][201/408]	Loss 0.0006 (0.0011)	
training:	Epoch: [63][202/408]	Loss 0.0011 (0.0011)	
training:	Epoch: [63][203/408]	Loss 0.0006 (0.0011)	
training:	Epoch: [63][204/408]	Loss 0.0010 (0.0011)	
training:	Epoch: [63][205/408]	Loss 0.0025 (0.0011)	
training:	Epoch: [63][206/408]	Loss 0.0007 (0.0011)	
training:	Epoch: [63][207/408]	Loss 0.0022 (0.0011)	
training:	Epoch: [63][208/408]	Loss 0.0006 (0.0011)	
training:	Epoch: [63][209/408]	Loss 0.0007 (0.0011)	
training:	Epoch: [63][210/408]	Loss 0.0008 (0.0011)	
training:	Epoch: [63][211/408]	Loss 0.0007 (0.0011)	
training:	Epoch: [63][212/408]	Loss 0.0007 (0.0011)	
training:	Epoch: [63][213/408]	Loss 0.0006 (0.0011)	
training:	Epoch: [63][214/408]	Loss 0.0007 (0.0011)	
training:	Epoch: [63][215/408]	Loss 0.0010 (0.0011)	
training:	Epoch: [63][216/408]	Loss 0.0011 (0.0011)	
training:	Epoch: [63][217/408]	Loss 0.0012 (0.0011)	
training:	Epoch: [63][218/408]	Loss 0.0006 (0.0011)	
training:	Epoch: [63][219/408]	Loss 0.0007 (0.0011)	
training:	Epoch: [63][220/408]	Loss 0.0006 (0.0011)	
training:	Epoch: [63][221/408]	Loss 0.0006 (0.0011)	
training:	Epoch: [63][222/408]	Loss 0.0010 (0.0011)	
training:	Epoch: [63][223/408]	Loss 0.0008 (0.0011)	
training:	Epoch: [63][224/408]	Loss 0.0007 (0.0011)	
training:	Epoch: [63][225/408]	Loss 0.0008 (0.0011)	
training:	Epoch: [63][226/408]	Loss 0.0010 (0.0011)	
training:	Epoch: [63][227/408]	Loss 0.0095 (0.0011)	
training:	Epoch: [63][228/408]	Loss 0.0007 (0.0011)	
training:	Epoch: [63][229/408]	Loss 0.0006 (0.0011)	
training:	Epoch: [63][230/408]	Loss 0.0006 (0.0011)	
training:	Epoch: [63][231/408]	Loss 0.0006 (0.0011)	
training:	Epoch: [63][232/408]	Loss 0.0006 (0.0011)	
training:	Epoch: [63][233/408]	Loss 0.0006 (0.0011)	
training:	Epoch: [63][234/408]	Loss 0.0008 (0.0011)	
training:	Epoch: [63][235/408]	Loss 0.0007 (0.0011)	
training:	Epoch: [63][236/408]	Loss 0.0006 (0.0011)	
training:	Epoch: [63][237/408]	Loss 0.0006 (0.0011)	
training:	Epoch: [63][238/408]	Loss 0.0007 (0.0011)	
training:	Epoch: [63][239/408]	Loss 0.0024 (0.0011)	
training:	Epoch: [63][240/408]	Loss 0.0007 (0.0011)	
training:	Epoch: [63][241/408]	Loss 0.0006 (0.0011)	
training:	Epoch: [63][242/408]	Loss 0.0008 (0.0011)	
training:	Epoch: [63][243/408]	Loss 0.0006 (0.0011)	
training:	Epoch: [63][244/408]	Loss 0.0007 (0.0011)	
training:	Epoch: [63][245/408]	Loss 0.0009 (0.0011)	
training:	Epoch: [63][246/408]	Loss 0.0009 (0.0011)	
training:	Epoch: [63][247/408]	Loss 0.0008 (0.0011)	
training:	Epoch: [63][248/408]	Loss 0.0006 (0.0011)	
training:	Epoch: [63][249/408]	Loss 0.0008 (0.0011)	
training:	Epoch: [63][250/408]	Loss 0.0029 (0.0011)	
training:	Epoch: [63][251/408]	Loss 0.0007 (0.0011)	
training:	Epoch: [63][252/408]	Loss 0.0006 (0.0011)	
training:	Epoch: [63][253/408]	Loss 0.0013 (0.0011)	
training:	Epoch: [63][254/408]	Loss 0.0006 (0.0011)	
training:	Epoch: [63][255/408]	Loss 0.0006 (0.0011)	
training:	Epoch: [63][256/408]	Loss 0.0008 (0.0011)	
training:	Epoch: [63][257/408]	Loss 0.0009 (0.0011)	
training:	Epoch: [63][258/408]	Loss 0.0007 (0.0011)	
training:	Epoch: [63][259/408]	Loss 0.0007 (0.0011)	
training:	Epoch: [63][260/408]	Loss 0.0009 (0.0011)	
training:	Epoch: [63][261/408]	Loss 0.0007 (0.0011)	
training:	Epoch: [63][262/408]	Loss 0.0009 (0.0011)	
training:	Epoch: [63][263/408]	Loss 0.0016 (0.0011)	
training:	Epoch: [63][264/408]	Loss 0.0007 (0.0011)	
training:	Epoch: [63][265/408]	Loss 0.0007 (0.0011)	
training:	Epoch: [63][266/408]	Loss 0.0006 (0.0011)	
training:	Epoch: [63][267/408]	Loss 0.0016 (0.0011)	
training:	Epoch: [63][268/408]	Loss 0.0006 (0.0011)	
training:	Epoch: [63][269/408]	Loss 0.0009 (0.0011)	
training:	Epoch: [63][270/408]	Loss 0.0006 (0.0011)	
training:	Epoch: [63][271/408]	Loss 0.0021 (0.0011)	
training:	Epoch: [63][272/408]	Loss 0.0011 (0.0011)	
training:	Epoch: [63][273/408]	Loss 0.0010 (0.0011)	
training:	Epoch: [63][274/408]	Loss 0.0006 (0.0011)	
training:	Epoch: [63][275/408]	Loss 0.0006 (0.0011)	
training:	Epoch: [63][276/408]	Loss 0.0006 (0.0011)	
training:	Epoch: [63][277/408]	Loss 0.0006 (0.0011)	
training:	Epoch: [63][278/408]	Loss 0.0007 (0.0011)	
training:	Epoch: [63][279/408]	Loss 0.0006 (0.0011)	
training:	Epoch: [63][280/408]	Loss 0.0011 (0.0011)	
training:	Epoch: [63][281/408]	Loss 0.0006 (0.0010)	
training:	Epoch: [63][282/408]	Loss 0.0006 (0.0010)	
training:	Epoch: [63][283/408]	Loss 0.0006 (0.0010)	
training:	Epoch: [63][284/408]	Loss 0.0007 (0.0010)	
training:	Epoch: [63][285/408]	Loss 0.0025 (0.0011)	
training:	Epoch: [63][286/408]	Loss 0.0007 (0.0010)	
training:	Epoch: [63][287/408]	Loss 0.0008 (0.0010)	
training:	Epoch: [63][288/408]	Loss 0.0007 (0.0010)	
training:	Epoch: [63][289/408]	Loss 0.0008 (0.0010)	
training:	Epoch: [63][290/408]	Loss 0.0006 (0.0010)	
training:	Epoch: [63][291/408]	Loss 0.0006 (0.0010)	
training:	Epoch: [63][292/408]	Loss 0.0025 (0.0010)	
training:	Epoch: [63][293/408]	Loss 0.0008 (0.0010)	
training:	Epoch: [63][294/408]	Loss 0.0006 (0.0010)	
training:	Epoch: [63][295/408]	Loss 0.0012 (0.0010)	
training:	Epoch: [63][296/408]	Loss 0.0007 (0.0010)	
training:	Epoch: [63][297/408]	Loss 0.0006 (0.0010)	
training:	Epoch: [63][298/408]	Loss 0.0016 (0.0010)	
training:	Epoch: [63][299/408]	Loss 0.0006 (0.0010)	
training:	Epoch: [63][300/408]	Loss 0.0010 (0.0010)	
training:	Epoch: [63][301/408]	Loss 0.0006 (0.0010)	
training:	Epoch: [63][302/408]	Loss 0.0007 (0.0010)	
training:	Epoch: [63][303/408]	Loss 0.0024 (0.0010)	
training:	Epoch: [63][304/408]	Loss 0.0008 (0.0010)	
training:	Epoch: [63][305/408]	Loss 0.0006 (0.0010)	
training:	Epoch: [63][306/408]	Loss 0.0007 (0.0010)	
training:	Epoch: [63][307/408]	Loss 0.0007 (0.0010)	
training:	Epoch: [63][308/408]	Loss 0.0006 (0.0010)	
training:	Epoch: [63][309/408]	Loss 0.0006 (0.0010)	
training:	Epoch: [63][310/408]	Loss 0.0006 (0.0010)	
training:	Epoch: [63][311/408]	Loss 0.0023 (0.0010)	
training:	Epoch: [63][312/408]	Loss 0.0006 (0.0010)	
training:	Epoch: [63][313/408]	Loss 0.0010 (0.0010)	
training:	Epoch: [63][314/408]	Loss 0.0007 (0.0010)	
training:	Epoch: [63][315/408]	Loss 0.0019 (0.0010)	
training:	Epoch: [63][316/408]	Loss 0.0006 (0.0010)	
training:	Epoch: [63][317/408]	Loss 0.0006 (0.0010)	
training:	Epoch: [63][318/408]	Loss 0.0007 (0.0010)	
training:	Epoch: [63][319/408]	Loss 0.0005 (0.0010)	
training:	Epoch: [63][320/408]	Loss 0.0006 (0.0010)	
training:	Epoch: [63][321/408]	Loss 0.0009 (0.0010)	
training:	Epoch: [63][322/408]	Loss 0.0008 (0.0010)	
training:	Epoch: [63][323/408]	Loss 0.0006 (0.0010)	
training:	Epoch: [63][324/408]	Loss 0.0006 (0.0010)	
training:	Epoch: [63][325/408]	Loss 0.0008 (0.0010)	
training:	Epoch: [63][326/408]	Loss 0.0011 (0.0010)	
training:	Epoch: [63][327/408]	Loss 0.0008 (0.0010)	
training:	Epoch: [63][328/408]	Loss 0.0008 (0.0010)	
training:	Epoch: [63][329/408]	Loss 0.0007 (0.0010)	
training:	Epoch: [63][330/408]	Loss 0.0028 (0.0010)	
training:	Epoch: [63][331/408]	Loss 0.0006 (0.0010)	
training:	Epoch: [63][332/408]	Loss 0.0007 (0.0010)	
training:	Epoch: [63][333/408]	Loss 0.0006 (0.0010)	
training:	Epoch: [63][334/408]	Loss 0.0006 (0.0010)	
training:	Epoch: [63][335/408]	Loss 0.0007 (0.0010)	
training:	Epoch: [63][336/408]	Loss 0.0009 (0.0010)	
training:	Epoch: [63][337/408]	Loss 0.0006 (0.0010)	
training:	Epoch: [63][338/408]	Loss 0.0006 (0.0010)	
training:	Epoch: [63][339/408]	Loss 0.0006 (0.0010)	
training:	Epoch: [63][340/408]	Loss 0.0009 (0.0010)	
training:	Epoch: [63][341/408]	Loss 0.0006 (0.0010)	
training:	Epoch: [63][342/408]	Loss 0.0006 (0.0010)	
training:	Epoch: [63][343/408]	Loss 0.0006 (0.0010)	
training:	Epoch: [63][344/408]	Loss 0.0006 (0.0010)	
training:	Epoch: [63][345/408]	Loss 0.0007 (0.0010)	
training:	Epoch: [63][346/408]	Loss 0.0006 (0.0010)	
training:	Epoch: [63][347/408]	Loss 0.0006 (0.0010)	
training:	Epoch: [63][348/408]	Loss 0.0008 (0.0010)	
training:	Epoch: [63][349/408]	Loss 0.0005 (0.0010)	
training:	Epoch: [63][350/408]	Loss 0.0006 (0.0010)	
training:	Epoch: [63][351/408]	Loss 0.0010 (0.0010)	
training:	Epoch: [63][352/408]	Loss 0.0006 (0.0010)	
training:	Epoch: [63][353/408]	Loss 0.0010 (0.0010)	
training:	Epoch: [63][354/408]	Loss 0.0006 (0.0010)	
training:	Epoch: [63][355/408]	Loss 0.0008 (0.0010)	
training:	Epoch: [63][356/408]	Loss 0.0059 (0.0010)	
training:	Epoch: [63][357/408]	Loss 0.0036 (0.0010)	
training:	Epoch: [63][358/408]	Loss 0.0006 (0.0010)	
training:	Epoch: [63][359/408]	Loss 0.0007 (0.0010)	
training:	Epoch: [63][360/408]	Loss 0.0006 (0.0010)	
training:	Epoch: [63][361/408]	Loss 0.0008 (0.0010)	
training:	Epoch: [63][362/408]	Loss 0.0006 (0.0010)	
training:	Epoch: [63][363/408]	Loss 0.0029 (0.0010)	
training:	Epoch: [63][364/408]	Loss 0.0006 (0.0010)	
training:	Epoch: [63][365/408]	Loss 0.0006 (0.0010)	
training:	Epoch: [63][366/408]	Loss 0.0023 (0.0010)	
training:	Epoch: [63][367/408]	Loss 0.0006 (0.0010)	
training:	Epoch: [63][368/408]	Loss 0.0009 (0.0010)	
training:	Epoch: [63][369/408]	Loss 0.0006 (0.0010)	
training:	Epoch: [63][370/408]	Loss 0.0007 (0.0010)	
training:	Epoch: [63][371/408]	Loss 0.0006 (0.0010)	
training:	Epoch: [63][372/408]	Loss 0.0006 (0.0010)	
training:	Epoch: [63][373/408]	Loss 0.0008 (0.0010)	
training:	Epoch: [63][374/408]	Loss 0.0041 (0.0010)	
training:	Epoch: [63][375/408]	Loss 0.0007 (0.0010)	
training:	Epoch: [63][376/408]	Loss 0.0006 (0.0010)	
training:	Epoch: [63][377/408]	Loss 0.0007 (0.0010)	
training:	Epoch: [63][378/408]	Loss 0.0007 (0.0010)	
training:	Epoch: [63][379/408]	Loss 0.0009 (0.0010)	
training:	Epoch: [63][380/408]	Loss 0.0008 (0.0010)	
training:	Epoch: [63][381/408]	Loss 0.0012 (0.0010)	
training:	Epoch: [63][382/408]	Loss 0.0009 (0.0010)	
training:	Epoch: [63][383/408]	Loss 0.0008 (0.0010)	
training:	Epoch: [63][384/408]	Loss 0.0007 (0.0010)	
training:	Epoch: [63][385/408]	Loss 0.0006 (0.0010)	
training:	Epoch: [63][386/408]	Loss 0.0008 (0.0010)	
training:	Epoch: [63][387/408]	Loss 0.0007 (0.0010)	
training:	Epoch: [63][388/408]	Loss 0.0007 (0.0010)	
training:	Epoch: [63][389/408]	Loss 0.0032 (0.0010)	
training:	Epoch: [63][390/408]	Loss 0.0009 (0.0010)	
training:	Epoch: [63][391/408]	Loss 0.0006 (0.0010)	
training:	Epoch: [63][392/408]	Loss 0.0006 (0.0010)	
training:	Epoch: [63][393/408]	Loss 0.0011 (0.0010)	
training:	Epoch: [63][394/408]	Loss 0.0008 (0.0010)	
training:	Epoch: [63][395/408]	Loss 0.0020 (0.0010)	
training:	Epoch: [63][396/408]	Loss 0.0009 (0.0010)	
training:	Epoch: [63][397/408]	Loss 0.0006 (0.0010)	
training:	Epoch: [63][398/408]	Loss 0.0006 (0.0010)	
training:	Epoch: [63][399/408]	Loss 0.0007 (0.0010)	
training:	Epoch: [63][400/408]	Loss 0.0008 (0.0010)	
training:	Epoch: [63][401/408]	Loss 0.0006 (0.0010)	
training:	Epoch: [63][402/408]	Loss 0.0015 (0.0010)	
training:	Epoch: [63][403/408]	Loss 0.0032 (0.0010)	
training:	Epoch: [63][404/408]	Loss 0.0006 (0.0010)	
training:	Epoch: [63][405/408]	Loss 0.0006 (0.0010)	
training:	Epoch: [63][406/408]	Loss 0.0013 (0.0010)	
training:	Epoch: [63][407/408]	Loss 0.0007 (0.0010)	
training:	Epoch: [63][408/408]	Loss 0.0007 (0.0010)	
Training:	 Loss: 0.0010

Training:	 ACC: 1.0000 1.0000 1.0000 1.0000
Validation:	 ACC: 0.7855 0.7860 0.7963 0.7747
Validation:	 Best_BACC: 0.8226 0.8250 0.8772 0.7679
Validation:	 Loss: 1.2470
Pretraining:	Epoch 64/200
----------
training:	Epoch: [64][1/408]	Loss 0.0006 (0.0006)	
training:	Epoch: [64][2/408]	Loss 0.0584 (0.0295)	
training:	Epoch: [64][3/408]	Loss 0.0006 (0.0198)	
training:	Epoch: [64][4/408]	Loss 0.0018 (0.0153)	
training:	Epoch: [64][5/408]	Loss 0.0006 (0.0124)	
training:	Epoch: [64][6/408]	Loss 0.0007 (0.0104)	
training:	Epoch: [64][7/408]	Loss 0.0009 (0.0091)	
training:	Epoch: [64][8/408]	Loss 0.0008 (0.0080)	
training:	Epoch: [64][9/408]	Loss 0.0008 (0.0072)	
training:	Epoch: [64][10/408]	Loss 0.0006 (0.0066)	
training:	Epoch: [64][11/408]	Loss 0.0008 (0.0060)	
training:	Epoch: [64][12/408]	Loss 0.0020 (0.0057)	
training:	Epoch: [64][13/408]	Loss 0.0008 (0.0053)	
training:	Epoch: [64][14/408]	Loss 0.0008 (0.0050)	
training:	Epoch: [64][15/408]	Loss 0.0005 (0.0047)	
training:	Epoch: [64][16/408]	Loss 0.0006 (0.0044)	
training:	Epoch: [64][17/408]	Loss 0.0006 (0.0042)	
training:	Epoch: [64][18/408]	Loss 0.0010 (0.0040)	
training:	Epoch: [64][19/408]	Loss 0.0008 (0.0039)	
training:	Epoch: [64][20/408]	Loss 0.0008 (0.0037)	
training:	Epoch: [64][21/408]	Loss 0.0014 (0.0036)	
training:	Epoch: [64][22/408]	Loss 0.0007 (0.0035)	
training:	Epoch: [64][23/408]	Loss 0.0008 (0.0034)	
training:	Epoch: [64][24/408]	Loss 0.0006 (0.0032)	
training:	Epoch: [64][25/408]	Loss 0.0005 (0.0031)	
training:	Epoch: [64][26/408]	Loss 0.0009 (0.0031)	
training:	Epoch: [64][27/408]	Loss 0.0007 (0.0030)	
training:	Epoch: [64][28/408]	Loss 0.0008 (0.0029)	
training:	Epoch: [64][29/408]	Loss 0.0015 (0.0028)	
training:	Epoch: [64][30/408]	Loss 0.0006 (0.0028)	
training:	Epoch: [64][31/408]	Loss 0.0010 (0.0027)	
training:	Epoch: [64][32/408]	Loss 0.0006 (0.0026)	
training:	Epoch: [64][33/408]	Loss 0.0007 (0.0026)	
training:	Epoch: [64][34/408]	Loss 0.0007 (0.0025)	
training:	Epoch: [64][35/408]	Loss 0.0015 (0.0025)	
training:	Epoch: [64][36/408]	Loss 0.0006 (0.0024)	
training:	Epoch: [64][37/408]	Loss 0.0006 (0.0024)	
training:	Epoch: [64][38/408]	Loss 0.0007 (0.0024)	
training:	Epoch: [64][39/408]	Loss 0.0010 (0.0023)	
training:	Epoch: [64][40/408]	Loss 0.0032 (0.0023)	
training:	Epoch: [64][41/408]	Loss 0.0006 (0.0023)	
training:	Epoch: [64][42/408]	Loss 0.0006 (0.0023)	
training:	Epoch: [64][43/408]	Loss 0.0006 (0.0022)	
training:	Epoch: [64][44/408]	Loss 0.0007 (0.0022)	
training:	Epoch: [64][45/408]	Loss 0.0022 (0.0022)	
training:	Epoch: [64][46/408]	Loss 0.0008 (0.0022)	
training:	Epoch: [64][47/408]	Loss 0.0007 (0.0021)	
training:	Epoch: [64][48/408]	Loss 0.0006 (0.0021)	
training:	Epoch: [64][49/408]	Loss 0.0007 (0.0021)	
training:	Epoch: [64][50/408]	Loss 0.0051 (0.0021)	
training:	Epoch: [64][51/408]	Loss 0.0007 (0.0021)	
training:	Epoch: [64][52/408]	Loss 0.0008 (0.0021)	
training:	Epoch: [64][53/408]	Loss 0.0007 (0.0020)	
training:	Epoch: [64][54/408]	Loss 0.0010 (0.0020)	
training:	Epoch: [64][55/408]	Loss 0.0006 (0.0020)	
training:	Epoch: [64][56/408]	Loss 0.0006 (0.0020)	
training:	Epoch: [64][57/408]	Loss 0.0010 (0.0020)	
training:	Epoch: [64][58/408]	Loss 0.0005 (0.0019)	
training:	Epoch: [64][59/408]	Loss 0.0006 (0.0019)	
training:	Epoch: [64][60/408]	Loss 0.0007 (0.0019)	
training:	Epoch: [64][61/408]	Loss 0.0006 (0.0019)	
training:	Epoch: [64][62/408]	Loss 0.0030 (0.0019)	
training:	Epoch: [64][63/408]	Loss 0.0006 (0.0019)	
training:	Epoch: [64][64/408]	Loss 0.0007 (0.0018)	
training:	Epoch: [64][65/408]	Loss 0.0016 (0.0018)	
training:	Epoch: [64][66/408]	Loss 0.0169 (0.0021)	
training:	Epoch: [64][67/408]	Loss 0.0019 (0.0021)	
training:	Epoch: [64][68/408]	Loss 0.0025 (0.0021)	
training:	Epoch: [64][69/408]	Loss 0.0009 (0.0021)	
training:	Epoch: [64][70/408]	Loss 0.0006 (0.0020)	
training:	Epoch: [64][71/408]	Loss 0.0029 (0.0021)	
training:	Epoch: [64][72/408]	Loss 0.0007 (0.0020)	
training:	Epoch: [64][73/408]	Loss 0.0006 (0.0020)	
training:	Epoch: [64][74/408]	Loss 0.0005 (0.0020)	
training:	Epoch: [64][75/408]	Loss 0.0005 (0.0020)	
training:	Epoch: [64][76/408]	Loss 0.0006 (0.0020)	
training:	Epoch: [64][77/408]	Loss 0.0030 (0.0020)	
training:	Epoch: [64][78/408]	Loss 0.0009 (0.0020)	
training:	Epoch: [64][79/408]	Loss 0.0008 (0.0019)	
training:	Epoch: [64][80/408]	Loss 0.0006 (0.0019)	
training:	Epoch: [64][81/408]	Loss 0.0006 (0.0019)	
training:	Epoch: [64][82/408]	Loss 0.0006 (0.0019)	
training:	Epoch: [64][83/408]	Loss 0.0006 (0.0019)	
training:	Epoch: [64][84/408]	Loss 0.0006 (0.0019)	
training:	Epoch: [64][85/408]	Loss 0.0028 (0.0019)	
training:	Epoch: [64][86/408]	Loss 0.0007 (0.0019)	
training:	Epoch: [64][87/408]	Loss 0.0008 (0.0018)	
training:	Epoch: [64][88/408]	Loss 0.0007 (0.0018)	
training:	Epoch: [64][89/408]	Loss 0.0007 (0.0018)	
training:	Epoch: [64][90/408]	Loss 0.0006 (0.0018)	
training:	Epoch: [64][91/408]	Loss 0.0006 (0.0018)	
training:	Epoch: [64][92/408]	Loss 0.0007 (0.0018)	
training:	Epoch: [64][93/408]	Loss 0.0033 (0.0018)	
training:	Epoch: [64][94/408]	Loss 0.0007 (0.0018)	
training:	Epoch: [64][95/408]	Loss 0.0008 (0.0018)	
training:	Epoch: [64][96/408]	Loss 0.0006 (0.0018)	
training:	Epoch: [64][97/408]	Loss 0.0007 (0.0018)	
training:	Epoch: [64][98/408]	Loss 0.0007 (0.0017)	
training:	Epoch: [64][99/408]	Loss 0.0006 (0.0017)	
training:	Epoch: [64][100/408]	Loss 0.0007 (0.0017)	
training:	Epoch: [64][101/408]	Loss 0.0006 (0.0017)	
training:	Epoch: [64][102/408]	Loss 0.0006 (0.0017)	
training:	Epoch: [64][103/408]	Loss 0.0006 (0.0017)	
training:	Epoch: [64][104/408]	Loss 0.0008 (0.0017)	
training:	Epoch: [64][105/408]	Loss 0.0011 (0.0017)	
training:	Epoch: [64][106/408]	Loss 0.0005 (0.0017)	
training:	Epoch: [64][107/408]	Loss 0.0010 (0.0017)	
training:	Epoch: [64][108/408]	Loss 0.0006 (0.0016)	
training:	Epoch: [64][109/408]	Loss 0.0006 (0.0016)	
training:	Epoch: [64][110/408]	Loss 0.0006 (0.0016)	
training:	Epoch: [64][111/408]	Loss 0.0006 (0.0016)	
training:	Epoch: [64][112/408]	Loss 0.0006 (0.0016)	
training:	Epoch: [64][113/408]	Loss 0.0006 (0.0016)	
training:	Epoch: [64][114/408]	Loss 0.0020 (0.0016)	
training:	Epoch: [64][115/408]	Loss 0.0009 (0.0016)	
training:	Epoch: [64][116/408]	Loss 0.0007 (0.0016)	
training:	Epoch: [64][117/408]	Loss 0.0006 (0.0016)	
training:	Epoch: [64][118/408]	Loss 0.0007 (0.0016)	
training:	Epoch: [64][119/408]	Loss 0.0026 (0.0016)	
training:	Epoch: [64][120/408]	Loss 0.0006 (0.0016)	
training:	Epoch: [64][121/408]	Loss 0.0006 (0.0016)	
training:	Epoch: [64][122/408]	Loss 0.0005 (0.0016)	
training:	Epoch: [64][123/408]	Loss 0.0006 (0.0016)	
training:	Epoch: [64][124/408]	Loss 0.0007 (0.0015)	
training:	Epoch: [64][125/408]	Loss 0.0007 (0.0015)	
training:	Epoch: [64][126/408]	Loss 0.0009 (0.0015)	
training:	Epoch: [64][127/408]	Loss 0.0006 (0.0015)	
training:	Epoch: [64][128/408]	Loss 0.0006 (0.0015)	
training:	Epoch: [64][129/408]	Loss 0.0070 (0.0016)	
training:	Epoch: [64][130/408]	Loss 0.0047 (0.0016)	
training:	Epoch: [64][131/408]	Loss 0.0025 (0.0016)	
training:	Epoch: [64][132/408]	Loss 0.0007 (0.0016)	
training:	Epoch: [64][133/408]	Loss 0.0007 (0.0016)	
training:	Epoch: [64][134/408]	Loss 0.0009 (0.0016)	
training:	Epoch: [64][135/408]	Loss 0.0007 (0.0016)	
training:	Epoch: [64][136/408]	Loss 0.0006 (0.0016)	
training:	Epoch: [64][137/408]	Loss 0.0006 (0.0016)	
training:	Epoch: [64][138/408]	Loss 0.0005 (0.0015)	
training:	Epoch: [64][139/408]	Loss 0.0006 (0.0015)	
training:	Epoch: [64][140/408]	Loss 0.0007 (0.0015)	
training:	Epoch: [64][141/408]	Loss 0.0006 (0.0015)	
training:	Epoch: [64][142/408]	Loss 0.0018 (0.0015)	
training:	Epoch: [64][143/408]	Loss 0.0008 (0.0015)	
training:	Epoch: [64][144/408]	Loss 0.0006 (0.0015)	
training:	Epoch: [64][145/408]	Loss 0.0006 (0.0015)	
training:	Epoch: [64][146/408]	Loss 0.0009 (0.0015)	
training:	Epoch: [64][147/408]	Loss 0.0005 (0.0015)	
training:	Epoch: [64][148/408]	Loss 0.0017 (0.0015)	
training:	Epoch: [64][149/408]	Loss 0.0006 (0.0015)	
training:	Epoch: [64][150/408]	Loss 0.0010 (0.0015)	
training:	Epoch: [64][151/408]	Loss 0.0006 (0.0015)	
training:	Epoch: [64][152/408]	Loss 0.0006 (0.0015)	
training:	Epoch: [64][153/408]	Loss 0.0008 (0.0015)	
training:	Epoch: [64][154/408]	Loss 0.0009 (0.0015)	
training:	Epoch: [64][155/408]	Loss 0.0005 (0.0015)	
training:	Epoch: [64][156/408]	Loss 0.0006 (0.0015)	
training:	Epoch: [64][157/408]	Loss 0.0006 (0.0015)	
training:	Epoch: [64][158/408]	Loss 0.0007 (0.0015)	
training:	Epoch: [64][159/408]	Loss 0.0006 (0.0014)	
training:	Epoch: [64][160/408]	Loss 0.0006 (0.0014)	
training:	Epoch: [64][161/408]	Loss 0.0008 (0.0014)	
training:	Epoch: [64][162/408]	Loss 0.0007 (0.0014)	
training:	Epoch: [64][163/408]	Loss 0.0006 (0.0014)	
training:	Epoch: [64][164/408]	Loss 0.0007 (0.0014)	
training:	Epoch: [64][165/408]	Loss 0.0006 (0.0014)	
training:	Epoch: [64][166/408]	Loss 0.0007 (0.0014)	
training:	Epoch: [64][167/408]	Loss 0.0010 (0.0014)	
training:	Epoch: [64][168/408]	Loss 0.0007 (0.0014)	
training:	Epoch: [64][169/408]	Loss 0.0006 (0.0014)	
training:	Epoch: [64][170/408]	Loss 0.0005 (0.0014)	
training:	Epoch: [64][171/408]	Loss 0.0006 (0.0014)	
training:	Epoch: [64][172/408]	Loss 0.0006 (0.0014)	
training:	Epoch: [64][173/408]	Loss 0.0011 (0.0014)	
training:	Epoch: [64][174/408]	Loss 0.0006 (0.0014)	
training:	Epoch: [64][175/408]	Loss 0.0011 (0.0014)	
training:	Epoch: [64][176/408]	Loss 0.0041 (0.0014)	
training:	Epoch: [64][177/408]	Loss 0.0009 (0.0014)	
training:	Epoch: [64][178/408]	Loss 0.0007 (0.0014)	
training:	Epoch: [64][179/408]	Loss 0.0005 (0.0014)	
training:	Epoch: [64][180/408]	Loss 0.0005 (0.0014)	
training:	Epoch: [64][181/408]	Loss 0.0023 (0.0014)	
training:	Epoch: [64][182/408]	Loss 0.0011 (0.0014)	
training:	Epoch: [64][183/408]	Loss 0.0006 (0.0014)	
training:	Epoch: [64][184/408]	Loss 0.0006 (0.0014)	
training:	Epoch: [64][185/408]	Loss 0.0010 (0.0014)	
training:	Epoch: [64][186/408]	Loss 0.0008 (0.0014)	
training:	Epoch: [64][187/408]	Loss 0.0006 (0.0014)	
training:	Epoch: [64][188/408]	Loss 0.0008 (0.0014)	
training:	Epoch: [64][189/408]	Loss 0.0006 (0.0014)	
training:	Epoch: [64][190/408]	Loss 0.0012 (0.0014)	
training:	Epoch: [64][191/408]	Loss 0.0006 (0.0014)	
training:	Epoch: [64][192/408]	Loss 0.0006 (0.0013)	
training:	Epoch: [64][193/408]	Loss 0.0006 (0.0013)	
training:	Epoch: [64][194/408]	Loss 0.0005 (0.0013)	
training:	Epoch: [64][195/408]	Loss 0.0005 (0.0013)	
training:	Epoch: [64][196/408]	Loss 0.0006 (0.0013)	
training:	Epoch: [64][197/408]	Loss 0.0006 (0.0013)	
training:	Epoch: [64][198/408]	Loss 0.0032 (0.0013)	
training:	Epoch: [64][199/408]	Loss 0.0005 (0.0013)	
training:	Epoch: [64][200/408]	Loss 0.0006 (0.0013)	
training:	Epoch: [64][201/408]	Loss 0.0006 (0.0013)	
training:	Epoch: [64][202/408]	Loss 0.0006 (0.0013)	
training:	Epoch: [64][203/408]	Loss 0.0077 (0.0014)	
training:	Epoch: [64][204/408]	Loss 0.0011 (0.0014)	
training:	Epoch: [64][205/408]	Loss 0.0006 (0.0013)	
training:	Epoch: [64][206/408]	Loss 0.0007 (0.0013)	
training:	Epoch: [64][207/408]	Loss 0.0005 (0.0013)	
training:	Epoch: [64][208/408]	Loss 0.0005 (0.0013)	
training:	Epoch: [64][209/408]	Loss 0.0005 (0.0013)	
training:	Epoch: [64][210/408]	Loss 0.0006 (0.0013)	
training:	Epoch: [64][211/408]	Loss 0.0005 (0.0013)	
training:	Epoch: [64][212/408]	Loss 0.0005 (0.0013)	
training:	Epoch: [64][213/408]	Loss 0.0011 (0.0013)	
training:	Epoch: [64][214/408]	Loss 0.0007 (0.0013)	
training:	Epoch: [64][215/408]	Loss 0.0007 (0.0013)	
training:	Epoch: [64][216/408]	Loss 0.0021 (0.0013)	
training:	Epoch: [64][217/408]	Loss 0.0006 (0.0013)	
training:	Epoch: [64][218/408]	Loss 0.0086 (0.0013)	
training:	Epoch: [64][219/408]	Loss 0.0007 (0.0013)	
training:	Epoch: [64][220/408]	Loss 0.0007 (0.0013)	
training:	Epoch: [64][221/408]	Loss 0.0005 (0.0013)	
training:	Epoch: [64][222/408]	Loss 0.0006 (0.0013)	
training:	Epoch: [64][223/408]	Loss 0.0006 (0.0013)	
training:	Epoch: [64][224/408]	Loss 0.0009 (0.0013)	
training:	Epoch: [64][225/408]	Loss 0.0024 (0.0013)	
training:	Epoch: [64][226/408]	Loss 0.0007 (0.0013)	
training:	Epoch: [64][227/408]	Loss 0.0006 (0.0013)	
training:	Epoch: [64][228/408]	Loss 0.0006 (0.0013)	
training:	Epoch: [64][229/408]	Loss 0.0008 (0.0013)	
training:	Epoch: [64][230/408]	Loss 0.0006 (0.0013)	
training:	Epoch: [64][231/408]	Loss 0.0022 (0.0013)	
training:	Epoch: [64][232/408]	Loss 0.0006 (0.0013)	
training:	Epoch: [64][233/408]	Loss 0.0006 (0.0013)	
training:	Epoch: [64][234/408]	Loss 0.0006 (0.0013)	
training:	Epoch: [64][235/408]	Loss 0.0007 (0.0013)	
training:	Epoch: [64][236/408]	Loss 0.0006 (0.0013)	
training:	Epoch: [64][237/408]	Loss 0.0006 (0.0013)	
training:	Epoch: [64][238/408]	Loss 0.0006 (0.0013)	
training:	Epoch: [64][239/408]	Loss 0.0005 (0.0013)	
training:	Epoch: [64][240/408]	Loss 0.0007 (0.0013)	
training:	Epoch: [64][241/408]	Loss 0.0011 (0.0013)	
training:	Epoch: [64][242/408]	Loss 0.0005 (0.0013)	
training:	Epoch: [64][243/408]	Loss 0.0011 (0.0013)	
training:	Epoch: [64][244/408]	Loss 0.0005 (0.0013)	
training:	Epoch: [64][245/408]	Loss 0.0007 (0.0013)	
training:	Epoch: [64][246/408]	Loss 0.0006 (0.0013)	
training:	Epoch: [64][247/408]	Loss 0.0007 (0.0013)	
training:	Epoch: [64][248/408]	Loss 0.0009 (0.0013)	
training:	Epoch: [64][249/408]	Loss 0.0007 (0.0013)	
training:	Epoch: [64][250/408]	Loss 0.0010 (0.0013)	
training:	Epoch: [64][251/408]	Loss 0.0006 (0.0013)	
training:	Epoch: [64][252/408]	Loss 0.0028 (0.0013)	
training:	Epoch: [64][253/408]	Loss 0.0005 (0.0013)	
training:	Epoch: [64][254/408]	Loss 0.0006 (0.0013)	
training:	Epoch: [64][255/408]	Loss 0.0006 (0.0013)	
training:	Epoch: [64][256/408]	Loss 0.0006 (0.0013)	
training:	Epoch: [64][257/408]	Loss 0.0005 (0.0013)	
training:	Epoch: [64][258/408]	Loss 0.0006 (0.0013)	
training:	Epoch: [64][259/408]	Loss 0.0005 (0.0013)	
training:	Epoch: [64][260/408]	Loss 0.0008 (0.0013)	
training:	Epoch: [64][261/408]	Loss 0.0006 (0.0013)	
training:	Epoch: [64][262/408]	Loss 0.0007 (0.0013)	
training:	Epoch: [64][263/408]	Loss 0.0005 (0.0013)	
training:	Epoch: [64][264/408]	Loss 0.0007 (0.0013)	
training:	Epoch: [64][265/408]	Loss 0.0005 (0.0012)	
training:	Epoch: [64][266/408]	Loss 0.0006 (0.0012)	
training:	Epoch: [64][267/408]	Loss 0.0005 (0.0012)	
training:	Epoch: [64][268/408]	Loss 0.0006 (0.0012)	
training:	Epoch: [64][269/408]	Loss 0.0006 (0.0012)	
training:	Epoch: [64][270/408]	Loss 0.0006 (0.0012)	
training:	Epoch: [64][271/408]	Loss 0.0009 (0.0012)	
training:	Epoch: [64][272/408]	Loss 0.0006 (0.0012)	
training:	Epoch: [64][273/408]	Loss 0.0005 (0.0012)	
training:	Epoch: [64][274/408]	Loss 0.0008 (0.0012)	
training:	Epoch: [64][275/408]	Loss 0.0006 (0.0012)	
training:	Epoch: [64][276/408]	Loss 0.0006 (0.0012)	
training:	Epoch: [64][277/408]	Loss 0.0365 (0.0014)	
training:	Epoch: [64][278/408]	Loss 0.0005 (0.0013)	
training:	Epoch: [64][279/408]	Loss 0.0006 (0.0013)	
training:	Epoch: [64][280/408]	Loss 0.0007 (0.0013)	
training:	Epoch: [64][281/408]	Loss 0.0006 (0.0013)	
training:	Epoch: [64][282/408]	Loss 0.0006 (0.0013)	
training:	Epoch: [64][283/408]	Loss 0.0019 (0.0013)	
training:	Epoch: [64][284/408]	Loss 0.0005 (0.0013)	
training:	Epoch: [64][285/408]	Loss 0.0016 (0.0013)	
training:	Epoch: [64][286/408]	Loss 0.0009 (0.0013)	
training:	Epoch: [64][287/408]	Loss 0.0007 (0.0013)	
training:	Epoch: [64][288/408]	Loss 0.0010 (0.0013)	
training:	Epoch: [64][289/408]	Loss 0.0208 (0.0014)	
training:	Epoch: [64][290/408]	Loss 0.0010 (0.0014)	
training:	Epoch: [64][291/408]	Loss 0.0006 (0.0014)	
training:	Epoch: [64][292/408]	Loss 0.0007 (0.0014)	
training:	Epoch: [64][293/408]	Loss 0.0019 (0.0014)	
training:	Epoch: [64][294/408]	Loss 0.0007 (0.0014)	
training:	Epoch: [64][295/408]	Loss 0.0007 (0.0014)	
training:	Epoch: [64][296/408]	Loss 0.0007 (0.0014)	
training:	Epoch: [64][297/408]	Loss 0.0006 (0.0014)	
training:	Epoch: [64][298/408]	Loss 0.0068 (0.0014)	
training:	Epoch: [64][299/408]	Loss 0.0005 (0.0014)	
training:	Epoch: [64][300/408]	Loss 0.0006 (0.0014)	
training:	Epoch: [64][301/408]	Loss 0.0006 (0.0014)	
training:	Epoch: [64][302/408]	Loss 0.0006 (0.0014)	
training:	Epoch: [64][303/408]	Loss 0.0009 (0.0014)	
training:	Epoch: [64][304/408]	Loss 0.0008 (0.0014)	
training:	Epoch: [64][305/408]	Loss 0.0006 (0.0014)	
training:	Epoch: [64][306/408]	Loss 0.0008 (0.0014)	
training:	Epoch: [64][307/408]	Loss 0.0006 (0.0014)	
training:	Epoch: [64][308/408]	Loss 0.0006 (0.0014)	
training:	Epoch: [64][309/408]	Loss 0.0006 (0.0014)	
training:	Epoch: [64][310/408]	Loss 0.0008 (0.0014)	
training:	Epoch: [64][311/408]	Loss 0.0006 (0.0014)	
training:	Epoch: [64][312/408]	Loss 0.0006 (0.0014)	
training:	Epoch: [64][313/408]	Loss 0.0008 (0.0014)	
training:	Epoch: [64][314/408]	Loss 0.0005 (0.0014)	
training:	Epoch: [64][315/408]	Loss 0.0007 (0.0014)	
training:	Epoch: [64][316/408]	Loss 0.0005 (0.0014)	
training:	Epoch: [64][317/408]	Loss 0.0005 (0.0014)	
training:	Epoch: [64][318/408]	Loss 0.0006 (0.0014)	
training:	Epoch: [64][319/408]	Loss 0.0007 (0.0014)	
training:	Epoch: [64][320/408]	Loss 0.0006 (0.0014)	
training:	Epoch: [64][321/408]	Loss 0.0007 (0.0013)	
training:	Epoch: [64][322/408]	Loss 0.0005 (0.0013)	
training:	Epoch: [64][323/408]	Loss 0.0009 (0.0013)	
training:	Epoch: [64][324/408]	Loss 0.0005 (0.0013)	
training:	Epoch: [64][325/408]	Loss 0.0007 (0.0013)	
training:	Epoch: [64][326/408]	Loss 0.0005 (0.0013)	
training:	Epoch: [64][327/408]	Loss 0.0019 (0.0013)	
training:	Epoch: [64][328/408]	Loss 0.0006 (0.0013)	
training:	Epoch: [64][329/408]	Loss 0.0007 (0.0013)	
training:	Epoch: [64][330/408]	Loss 0.0012 (0.0013)	
training:	Epoch: [64][331/408]	Loss 0.0011 (0.0013)	
training:	Epoch: [64][332/408]	Loss 0.0009 (0.0013)	
training:	Epoch: [64][333/408]	Loss 0.0006 (0.0013)	
training:	Epoch: [64][334/408]	Loss 0.0013 (0.0013)	
training:	Epoch: [64][335/408]	Loss 0.0006 (0.0013)	
training:	Epoch: [64][336/408]	Loss 0.0006 (0.0013)	
training:	Epoch: [64][337/408]	Loss 0.0005 (0.0013)	
training:	Epoch: [64][338/408]	Loss 0.0006 (0.0013)	
training:	Epoch: [64][339/408]	Loss 0.0006 (0.0013)	
training:	Epoch: [64][340/408]	Loss 0.0007 (0.0013)	
training:	Epoch: [64][341/408]	Loss 0.0005 (0.0013)	
training:	Epoch: [64][342/408]	Loss 0.0005 (0.0013)	
training:	Epoch: [64][343/408]	Loss 0.0006 (0.0013)	
training:	Epoch: [64][344/408]	Loss 0.0006 (0.0013)	
training:	Epoch: [64][345/408]	Loss 0.0006 (0.0013)	
training:	Epoch: [64][346/408]	Loss 0.0063 (0.0013)	
training:	Epoch: [64][347/408]	Loss 0.0007 (0.0013)	
training:	Epoch: [64][348/408]	Loss 0.0006 (0.0013)	
training:	Epoch: [64][349/408]	Loss 0.0005 (0.0013)	
training:	Epoch: [64][350/408]	Loss 0.0005 (0.0013)	
training:	Epoch: [64][351/408]	Loss 0.0005 (0.0013)	
training:	Epoch: [64][352/408]	Loss 0.0007 (0.0013)	
training:	Epoch: [64][353/408]	Loss 0.0006 (0.0013)	
training:	Epoch: [64][354/408]	Loss 0.0008 (0.0013)	
training:	Epoch: [64][355/408]	Loss 0.0006 (0.0013)	
training:	Epoch: [64][356/408]	Loss 0.0005 (0.0013)	
training:	Epoch: [64][357/408]	Loss 0.0005 (0.0013)	
training:	Epoch: [64][358/408]	Loss 0.0006 (0.0013)	
training:	Epoch: [64][359/408]	Loss 0.0005 (0.0013)	
training:	Epoch: [64][360/408]	Loss 0.0020 (0.0013)	
training:	Epoch: [64][361/408]	Loss 0.0010 (0.0013)	
training:	Epoch: [64][362/408]	Loss 0.0007 (0.0013)	
training:	Epoch: [64][363/408]	Loss 0.0006 (0.0013)	
training:	Epoch: [64][364/408]	Loss 0.0006 (0.0013)	
training:	Epoch: [64][365/408]	Loss 0.0006 (0.0013)	
training:	Epoch: [64][366/408]	Loss 0.0005 (0.0013)	
training:	Epoch: [64][367/408]	Loss 0.0139 (0.0013)	
training:	Epoch: [64][368/408]	Loss 0.0006 (0.0013)	
training:	Epoch: [64][369/408]	Loss 0.0005 (0.0013)	
training:	Epoch: [64][370/408]	Loss 0.0005 (0.0013)	
training:	Epoch: [64][371/408]	Loss 0.0007 (0.0013)	
training:	Epoch: [64][372/408]	Loss 0.0005 (0.0013)	
training:	Epoch: [64][373/408]	Loss 0.0006 (0.0013)	
training:	Epoch: [64][374/408]	Loss 0.0006 (0.0013)	
training:	Epoch: [64][375/408]	Loss 0.0006 (0.0013)	
training:	Epoch: [64][376/408]	Loss 0.0010 (0.0013)	
training:	Epoch: [64][377/408]	Loss 0.0006 (0.0013)	
training:	Epoch: [64][378/408]	Loss 0.0006 (0.0013)	
training:	Epoch: [64][379/408]	Loss 0.0005 (0.0013)	
training:	Epoch: [64][380/408]	Loss 0.0006 (0.0013)	
training:	Epoch: [64][381/408]	Loss 0.0005 (0.0013)	
training:	Epoch: [64][382/408]	Loss 0.0006 (0.0013)	
training:	Epoch: [64][383/408]	Loss 0.0010 (0.0013)	
training:	Epoch: [64][384/408]	Loss 0.0006 (0.0013)	
training:	Epoch: [64][385/408]	Loss 0.0005 (0.0013)	
training:	Epoch: [64][386/408]	Loss 0.0007 (0.0013)	
training:	Epoch: [64][387/408]	Loss 0.0005 (0.0013)	
training:	Epoch: [64][388/408]	Loss 0.0008 (0.0013)	
training:	Epoch: [64][389/408]	Loss 0.0006 (0.0013)	
training:	Epoch: [64][390/408]	Loss 0.0006 (0.0013)	
training:	Epoch: [64][391/408]	Loss 0.0006 (0.0013)	
training:	Epoch: [64][392/408]	Loss 0.0077 (0.0013)	
training:	Epoch: [64][393/408]	Loss 0.0005 (0.0013)	
training:	Epoch: [64][394/408]	Loss 0.0006 (0.0013)	
training:	Epoch: [64][395/408]	Loss 0.0005 (0.0013)	
training:	Epoch: [64][396/408]	Loss 0.0117 (0.0013)	
training:	Epoch: [64][397/408]	Loss 0.0006 (0.0013)	
training:	Epoch: [64][398/408]	Loss 0.0016 (0.0013)	
training:	Epoch: [64][399/408]	Loss 0.0006 (0.0013)	
training:	Epoch: [64][400/408]	Loss 0.0005 (0.0013)	
training:	Epoch: [64][401/408]	Loss 0.0005 (0.0013)	
training:	Epoch: [64][402/408]	Loss 0.0005 (0.0013)	
training:	Epoch: [64][403/408]	Loss 0.0057 (0.0013)	
training:	Epoch: [64][404/408]	Loss 0.0070 (0.0013)	
training:	Epoch: [64][405/408]	Loss 0.0009 (0.0013)	
training:	Epoch: [64][406/408]	Loss 0.0005 (0.0013)	
training:	Epoch: [64][407/408]	Loss 0.0006 (0.0013)	
training:	Epoch: [64][408/408]	Loss 0.0005 (0.0013)	
Training:	 Loss: 0.0013

Training:	 ACC: 1.0000 1.0000 1.0000 1.0000
Validation:	 ACC: 0.7823 0.7822 0.7799 0.7848
Validation:	 Best_BACC: 0.8226 0.8250 0.8772 0.7679
Validation:	 Loss: 1.2745
Pretraining:	Epoch 65/200
----------
training:	Epoch: [65][1/408]	Loss 0.0005 (0.0005)	
training:	Epoch: [65][2/408]	Loss 0.0007 (0.0006)	
training:	Epoch: [65][3/408]	Loss 0.0006 (0.0006)	
training:	Epoch: [65][4/408]	Loss 0.0030 (0.0012)	
training:	Epoch: [65][5/408]	Loss 0.0007 (0.0011)	
training:	Epoch: [65][6/408]	Loss 0.0017 (0.0012)	
training:	Epoch: [65][7/408]	Loss 0.0005 (0.0011)	
training:	Epoch: [65][8/408]	Loss 0.0020 (0.0012)	
training:	Epoch: [65][9/408]	Loss 0.0006 (0.0012)	
training:	Epoch: [65][10/408]	Loss 0.0006 (0.0011)	
training:	Epoch: [65][11/408]	Loss 0.0005 (0.0010)	
training:	Epoch: [65][12/408]	Loss 0.0006 (0.0010)	
training:	Epoch: [65][13/408]	Loss 0.0026 (0.0011)	
training:	Epoch: [65][14/408]	Loss 0.0006 (0.0011)	
training:	Epoch: [65][15/408]	Loss 0.0006 (0.0011)	
training:	Epoch: [65][16/408]	Loss 0.0006 (0.0010)	
training:	Epoch: [65][17/408]	Loss 0.0005 (0.0010)	
training:	Epoch: [65][18/408]	Loss 0.0005 (0.0010)	
training:	Epoch: [65][19/408]	Loss 0.0005 (0.0009)	
training:	Epoch: [65][20/408]	Loss 0.0005 (0.0009)	
training:	Epoch: [65][21/408]	Loss 0.0006 (0.0009)	
training:	Epoch: [65][22/408]	Loss 0.0006 (0.0009)	
training:	Epoch: [65][23/408]	Loss 0.0007 (0.0009)	
training:	Epoch: [65][24/408]	Loss 0.0006 (0.0009)	
training:	Epoch: [65][25/408]	Loss 0.0006 (0.0009)	
training:	Epoch: [65][26/408]	Loss 0.0005 (0.0009)	
training:	Epoch: [65][27/408]	Loss 0.0016 (0.0009)	
training:	Epoch: [65][28/408]	Loss 0.0006 (0.0009)	
training:	Epoch: [65][29/408]	Loss 0.0006 (0.0009)	
training:	Epoch: [65][30/408]	Loss 0.0012 (0.0009)	
training:	Epoch: [65][31/408]	Loss 0.0006 (0.0009)	
training:	Epoch: [65][32/408]	Loss 0.0005 (0.0009)	
training:	Epoch: [65][33/408]	Loss 0.0007 (0.0008)	
training:	Epoch: [65][34/408]	Loss 0.0006 (0.0008)	
training:	Epoch: [65][35/408]	Loss 0.0023 (0.0009)	
training:	Epoch: [65][36/408]	Loss 0.0005 (0.0009)	
training:	Epoch: [65][37/408]	Loss 0.0005 (0.0009)	
training:	Epoch: [65][38/408]	Loss 0.0005 (0.0009)	
training:	Epoch: [65][39/408]	Loss 0.0025 (0.0009)	
training:	Epoch: [65][40/408]	Loss 0.0006 (0.0009)	
training:	Epoch: [65][41/408]	Loss 0.0005 (0.0009)	
training:	Epoch: [65][42/408]	Loss 0.0006 (0.0009)	
training:	Epoch: [65][43/408]	Loss 0.0005 (0.0009)	
training:	Epoch: [65][44/408]	Loss 0.0005 (0.0009)	
training:	Epoch: [65][45/408]	Loss 0.0006 (0.0009)	
training:	Epoch: [65][46/408]	Loss 0.0007 (0.0008)	
training:	Epoch: [65][47/408]	Loss 0.0005 (0.0008)	
training:	Epoch: [65][48/408]	Loss 0.0006 (0.0008)	
training:	Epoch: [65][49/408]	Loss 0.0006 (0.0008)	
training:	Epoch: [65][50/408]	Loss 0.0005 (0.0008)	
training:	Epoch: [65][51/408]	Loss 0.0005 (0.0008)	
training:	Epoch: [65][52/408]	Loss 0.0006 (0.0008)	
training:	Epoch: [65][53/408]	Loss 0.0005 (0.0008)	
training:	Epoch: [65][54/408]	Loss 0.0006 (0.0008)	
training:	Epoch: [65][55/408]	Loss 0.0006 (0.0008)	
training:	Epoch: [65][56/408]	Loss 0.0029 (0.0008)	
training:	Epoch: [65][57/408]	Loss 0.0005 (0.0008)	
training:	Epoch: [65][58/408]	Loss 0.0005 (0.0008)	
training:	Epoch: [65][59/408]	Loss 0.0036 (0.0009)	
training:	Epoch: [65][60/408]	Loss 0.0006 (0.0009)	
training:	Epoch: [65][61/408]	Loss 0.0016 (0.0009)	
training:	Epoch: [65][62/408]	Loss 0.0005 (0.0009)	
training:	Epoch: [65][63/408]	Loss 0.0005 (0.0009)	
training:	Epoch: [65][64/408]	Loss 0.0005 (0.0009)	
training:	Epoch: [65][65/408]	Loss 0.0005 (0.0009)	
training:	Epoch: [65][66/408]	Loss 0.0007 (0.0009)	
training:	Epoch: [65][67/408]	Loss 0.0005 (0.0009)	
training:	Epoch: [65][68/408]	Loss 0.0007 (0.0009)	
training:	Epoch: [65][69/408]	Loss 0.0006 (0.0008)	
training:	Epoch: [65][70/408]	Loss 0.0006 (0.0008)	
training:	Epoch: [65][71/408]	Loss 0.0021 (0.0009)	
training:	Epoch: [65][72/408]	Loss 0.0005 (0.0009)	
training:	Epoch: [65][73/408]	Loss 0.0050 (0.0009)	
training:	Epoch: [65][74/408]	Loss 0.0006 (0.0009)	
training:	Epoch: [65][75/408]	Loss 0.0005 (0.0009)	
training:	Epoch: [65][76/408]	Loss 0.0005 (0.0009)	
training:	Epoch: [65][77/408]	Loss 0.0006 (0.0009)	
training:	Epoch: [65][78/408]	Loss 0.0017 (0.0009)	
training:	Epoch: [65][79/408]	Loss 0.0005 (0.0009)	
training:	Epoch: [65][80/408]	Loss 0.0011 (0.0009)	
training:	Epoch: [65][81/408]	Loss 0.0017 (0.0009)	
training:	Epoch: [65][82/408]	Loss 0.0006 (0.0009)	
training:	Epoch: [65][83/408]	Loss 0.0005 (0.0009)	
training:	Epoch: [65][84/408]	Loss 0.0006 (0.0009)	
training:	Epoch: [65][85/408]	Loss 0.0005 (0.0009)	
training:	Epoch: [65][86/408]	Loss 0.0005 (0.0009)	
training:	Epoch: [65][87/408]	Loss 0.0005 (0.0009)	
training:	Epoch: [65][88/408]	Loss 0.0005 (0.0009)	
training:	Epoch: [65][89/408]	Loss 0.0006 (0.0009)	
training:	Epoch: [65][90/408]	Loss 0.0006 (0.0009)	
training:	Epoch: [65][91/408]	Loss 0.0005 (0.0009)	
training:	Epoch: [65][92/408]	Loss 0.0005 (0.0009)	
training:	Epoch: [65][93/408]	Loss 0.0006 (0.0009)	
training:	Epoch: [65][94/408]	Loss 0.0006 (0.0009)	
training:	Epoch: [65][95/408]	Loss 0.0006 (0.0009)	
training:	Epoch: [65][96/408]	Loss 0.0006 (0.0009)	
training:	Epoch: [65][97/408]	Loss 0.0005 (0.0009)	
training:	Epoch: [65][98/408]	Loss 0.0005 (0.0009)	
training:	Epoch: [65][99/408]	Loss 0.0005 (0.0008)	
training:	Epoch: [65][100/408]	Loss 0.0006 (0.0008)	
training:	Epoch: [65][101/408]	Loss 0.0075 (0.0009)	
training:	Epoch: [65][102/408]	Loss 0.0005 (0.0009)	
training:	Epoch: [65][103/408]	Loss 0.0016 (0.0009)	
training:	Epoch: [65][104/408]	Loss 0.0010 (0.0009)	
training:	Epoch: [65][105/408]	Loss 0.0005 (0.0009)	
training:	Epoch: [65][106/408]	Loss 0.0008 (0.0009)	
training:	Epoch: [65][107/408]	Loss 0.0007 (0.0009)	
training:	Epoch: [65][108/408]	Loss 0.0005 (0.0009)	
training:	Epoch: [65][109/408]	Loss 0.0007 (0.0009)	
training:	Epoch: [65][110/408]	Loss 0.0006 (0.0009)	
training:	Epoch: [65][111/408]	Loss 0.0005 (0.0009)	
training:	Epoch: [65][112/408]	Loss 0.0005 (0.0009)	
training:	Epoch: [65][113/408]	Loss 0.0009 (0.0009)	
training:	Epoch: [65][114/408]	Loss 0.0007 (0.0009)	
training:	Epoch: [65][115/408]	Loss 0.0005 (0.0009)	
training:	Epoch: [65][116/408]	Loss 0.0006 (0.0009)	
training:	Epoch: [65][117/408]	Loss 0.0006 (0.0009)	
training:	Epoch: [65][118/408]	Loss 0.0005 (0.0009)	
training:	Epoch: [65][119/408]	Loss 0.0005 (0.0009)	
training:	Epoch: [65][120/408]	Loss 0.0007 (0.0009)	
training:	Epoch: [65][121/408]	Loss 0.0006 (0.0009)	
training:	Epoch: [65][122/408]	Loss 0.0012 (0.0009)	
training:	Epoch: [65][123/408]	Loss 0.0006 (0.0009)	
training:	Epoch: [65][124/408]	Loss 0.0008 (0.0009)	
training:	Epoch: [65][125/408]	Loss 0.0006 (0.0009)	
training:	Epoch: [65][126/408]	Loss 0.0005 (0.0009)	
training:	Epoch: [65][127/408]	Loss 0.0005 (0.0009)	
training:	Epoch: [65][128/408]	Loss 0.0005 (0.0009)	
training:	Epoch: [65][129/408]	Loss 0.0007 (0.0009)	
training:	Epoch: [65][130/408]	Loss 0.0007 (0.0009)	
training:	Epoch: [65][131/408]	Loss 0.0005 (0.0009)	
training:	Epoch: [65][132/408]	Loss 0.0005 (0.0009)	
training:	Epoch: [65][133/408]	Loss 0.0009 (0.0009)	
training:	Epoch: [65][134/408]	Loss 0.0005 (0.0009)	
training:	Epoch: [65][135/408]	Loss 0.0007 (0.0008)	
training:	Epoch: [65][136/408]	Loss 0.0005 (0.0008)	
training:	Epoch: [65][137/408]	Loss 0.0007 (0.0008)	
training:	Epoch: [65][138/408]	Loss 0.0005 (0.0008)	
training:	Epoch: [65][139/408]	Loss 0.0005 (0.0008)	
training:	Epoch: [65][140/408]	Loss 0.0006 (0.0008)	
training:	Epoch: [65][141/408]	Loss 0.0006 (0.0008)	
training:	Epoch: [65][142/408]	Loss 0.0019 (0.0008)	
training:	Epoch: [65][143/408]	Loss 0.0011 (0.0008)	
training:	Epoch: [65][144/408]	Loss 0.0005 (0.0008)	
training:	Epoch: [65][145/408]	Loss 0.0005 (0.0008)	
training:	Epoch: [65][146/408]	Loss 0.0005 (0.0008)	
training:	Epoch: [65][147/408]	Loss 0.0005 (0.0008)	
training:	Epoch: [65][148/408]	Loss 0.0006 (0.0008)	
training:	Epoch: [65][149/408]	Loss 0.0005 (0.0008)	
training:	Epoch: [65][150/408]	Loss 0.0006 (0.0008)	
training:	Epoch: [65][151/408]	Loss 0.0005 (0.0008)	
training:	Epoch: [65][152/408]	Loss 0.0006 (0.0008)	
training:	Epoch: [65][153/408]	Loss 0.0005 (0.0008)	
training:	Epoch: [65][154/408]	Loss 0.0009 (0.0008)	
training:	Epoch: [65][155/408]	Loss 0.0006 (0.0008)	
training:	Epoch: [65][156/408]	Loss 0.0005 (0.0008)	
training:	Epoch: [65][157/408]	Loss 0.0005 (0.0008)	
training:	Epoch: [65][158/408]	Loss 0.0005 (0.0008)	
training:	Epoch: [65][159/408]	Loss 0.0005 (0.0008)	
training:	Epoch: [65][160/408]	Loss 0.0005 (0.0008)	
training:	Epoch: [65][161/408]	Loss 0.0006 (0.0008)	
training:	Epoch: [65][162/408]	Loss 0.0005 (0.0008)	
training:	Epoch: [65][163/408]	Loss 0.0005 (0.0008)	
training:	Epoch: [65][164/408]	Loss 0.0023 (0.0008)	
training:	Epoch: [65][165/408]	Loss 0.0008 (0.0008)	
training:	Epoch: [65][166/408]	Loss 0.0007 (0.0008)	
training:	Epoch: [65][167/408]	Loss 0.0005 (0.0008)	
training:	Epoch: [65][168/408]	Loss 0.0006 (0.0008)	
training:	Epoch: [65][169/408]	Loss 0.0005 (0.0008)	
training:	Epoch: [65][170/408]	Loss 0.0005 (0.0008)	
training:	Epoch: [65][171/408]	Loss 0.0005 (0.0008)	
training:	Epoch: [65][172/408]	Loss 0.0006 (0.0008)	
training:	Epoch: [65][173/408]	Loss 0.0005 (0.0008)	
training:	Epoch: [65][174/408]	Loss 0.0005 (0.0008)	
training:	Epoch: [65][175/408]	Loss 0.0005 (0.0008)	
training:	Epoch: [65][176/408]	Loss 0.0005 (0.0008)	
training:	Epoch: [65][177/408]	Loss 0.0007 (0.0008)	
training:	Epoch: [65][178/408]	Loss 0.0005 (0.0008)	
training:	Epoch: [65][179/408]	Loss 0.0005 (0.0008)	
training:	Epoch: [65][180/408]	Loss 0.0014 (0.0008)	
training:	Epoch: [65][181/408]	Loss 0.0006 (0.0008)	
training:	Epoch: [65][182/408]	Loss 0.0005 (0.0008)	
training:	Epoch: [65][183/408]	Loss 0.0005 (0.0008)	
training:	Epoch: [65][184/408]	Loss 0.0005 (0.0008)	
training:	Epoch: [65][185/408]	Loss 0.0011 (0.0008)	
training:	Epoch: [65][186/408]	Loss 0.0020 (0.0008)	
training:	Epoch: [65][187/408]	Loss 0.0287 (0.0010)	
training:	Epoch: [65][188/408]	Loss 0.0005 (0.0009)	
training:	Epoch: [65][189/408]	Loss 0.0006 (0.0009)	
training:	Epoch: [65][190/408]	Loss 0.0006 (0.0009)	
training:	Epoch: [65][191/408]	Loss 0.0005 (0.0009)	
training:	Epoch: [65][192/408]	Loss 0.0007 (0.0009)	
training:	Epoch: [65][193/408]	Loss 0.0006 (0.0009)	
training:	Epoch: [65][194/408]	Loss 0.0005 (0.0009)	
training:	Epoch: [65][195/408]	Loss 0.0006 (0.0009)	
training:	Epoch: [65][196/408]	Loss 0.0006 (0.0009)	
training:	Epoch: [65][197/408]	Loss 0.0009 (0.0009)	
training:	Epoch: [65][198/408]	Loss 0.0085 (0.0010)	
training:	Epoch: [65][199/408]	Loss 0.0009 (0.0010)	
training:	Epoch: [65][200/408]	Loss 0.0009 (0.0010)	
training:	Epoch: [65][201/408]	Loss 0.0017 (0.0010)	
training:	Epoch: [65][202/408]	Loss 0.0005 (0.0010)	
training:	Epoch: [65][203/408]	Loss 0.0005 (0.0010)	
training:	Epoch: [65][204/408]	Loss 0.0005 (0.0010)	
training:	Epoch: [65][205/408]	Loss 0.0006 (0.0010)	
training:	Epoch: [65][206/408]	Loss 0.0005 (0.0010)	
training:	Epoch: [65][207/408]	Loss 0.0005 (0.0010)	
training:	Epoch: [65][208/408]	Loss 0.0007 (0.0010)	
training:	Epoch: [65][209/408]	Loss 0.0006 (0.0010)	
training:	Epoch: [65][210/408]	Loss 0.0005 (0.0010)	
training:	Epoch: [65][211/408]	Loss 0.0007 (0.0010)	
training:	Epoch: [65][212/408]	Loss 0.0005 (0.0010)	
training:	Epoch: [65][213/408]	Loss 0.0006 (0.0010)	
training:	Epoch: [65][214/408]	Loss 0.0006 (0.0010)	
training:	Epoch: [65][215/408]	Loss 0.0014 (0.0010)	
training:	Epoch: [65][216/408]	Loss 0.0006 (0.0010)	
training:	Epoch: [65][217/408]	Loss 0.0006 (0.0009)	
training:	Epoch: [65][218/408]	Loss 0.0037 (0.0010)	
training:	Epoch: [65][219/408]	Loss 0.0005 (0.0010)	
training:	Epoch: [65][220/408]	Loss 0.0008 (0.0010)	
training:	Epoch: [65][221/408]	Loss 0.0005 (0.0010)	
training:	Epoch: [65][222/408]	Loss 0.0008 (0.0010)	
training:	Epoch: [65][223/408]	Loss 0.0006 (0.0010)	
training:	Epoch: [65][224/408]	Loss 0.0005 (0.0010)	
training:	Epoch: [65][225/408]	Loss 0.0005 (0.0010)	
training:	Epoch: [65][226/408]	Loss 0.0007 (0.0009)	
training:	Epoch: [65][227/408]	Loss 0.0004 (0.0009)	
training:	Epoch: [65][228/408]	Loss 0.0006 (0.0009)	
training:	Epoch: [65][229/408]	Loss 0.0005 (0.0009)	
training:	Epoch: [65][230/408]	Loss 0.0009 (0.0009)	
training:	Epoch: [65][231/408]	Loss 0.0005 (0.0009)	
training:	Epoch: [65][232/408]	Loss 0.0006 (0.0009)	
training:	Epoch: [65][233/408]	Loss 0.0005 (0.0009)	
training:	Epoch: [65][234/408]	Loss 0.0005 (0.0009)	
training:	Epoch: [65][235/408]	Loss 0.0005 (0.0009)	
training:	Epoch: [65][236/408]	Loss 0.0005 (0.0009)	
training:	Epoch: [65][237/408]	Loss 0.0006 (0.0009)	
training:	Epoch: [65][238/408]	Loss 0.0005 (0.0009)	
training:	Epoch: [65][239/408]	Loss 0.0008 (0.0009)	
training:	Epoch: [65][240/408]	Loss 0.0005 (0.0009)	
training:	Epoch: [65][241/408]	Loss 0.0005 (0.0009)	
training:	Epoch: [65][242/408]	Loss 0.0005 (0.0009)	
training:	Epoch: [65][243/408]	Loss 0.0005 (0.0009)	
training:	Epoch: [65][244/408]	Loss 0.0007 (0.0009)	
training:	Epoch: [65][245/408]	Loss 0.0006 (0.0009)	
training:	Epoch: [65][246/408]	Loss 0.0005 (0.0009)	
training:	Epoch: [65][247/408]	Loss 0.0007 (0.0009)	
training:	Epoch: [65][248/408]	Loss 0.0006 (0.0009)	
training:	Epoch: [65][249/408]	Loss 0.0005 (0.0009)	
training:	Epoch: [65][250/408]	Loss 0.0006 (0.0009)	
training:	Epoch: [65][251/408]	Loss 0.0005 (0.0009)	
training:	Epoch: [65][252/408]	Loss 0.0005 (0.0009)	
training:	Epoch: [65][253/408]	Loss 0.0006 (0.0009)	
training:	Epoch: [65][254/408]	Loss 0.0007 (0.0009)	
training:	Epoch: [65][255/408]	Loss 0.0005 (0.0009)	
training:	Epoch: [65][256/408]	Loss 0.0005 (0.0009)	
training:	Epoch: [65][257/408]	Loss 0.0006 (0.0009)	
training:	Epoch: [65][258/408]	Loss 0.0005 (0.0009)	
training:	Epoch: [65][259/408]	Loss 0.0006 (0.0009)	
training:	Epoch: [65][260/408]	Loss 0.0005 (0.0009)	
training:	Epoch: [65][261/408]	Loss 0.0005 (0.0009)	
training:	Epoch: [65][262/408]	Loss 0.0005 (0.0009)	
training:	Epoch: [65][263/408]	Loss 0.0005 (0.0009)	
training:	Epoch: [65][264/408]	Loss 0.0008 (0.0009)	
training:	Epoch: [65][265/408]	Loss 0.0005 (0.0009)	
training:	Epoch: [65][266/408]	Loss 0.0005 (0.0009)	
training:	Epoch: [65][267/408]	Loss 0.0103 (0.0009)	
training:	Epoch: [65][268/408]	Loss 0.0005 (0.0009)	
training:	Epoch: [65][269/408]	Loss 0.0006 (0.0009)	
training:	Epoch: [65][270/408]	Loss 0.0006 (0.0009)	
training:	Epoch: [65][271/408]	Loss 0.0007 (0.0009)	
training:	Epoch: [65][272/408]	Loss 0.0015 (0.0009)	
training:	Epoch: [65][273/408]	Loss 0.0005 (0.0009)	
training:	Epoch: [65][274/408]	Loss 0.0006 (0.0009)	
training:	Epoch: [65][275/408]	Loss 0.0016 (0.0009)	
training:	Epoch: [65][276/408]	Loss 0.0005 (0.0009)	
training:	Epoch: [65][277/408]	Loss 0.0005 (0.0009)	
training:	Epoch: [65][278/408]	Loss 0.0005 (0.0009)	
training:	Epoch: [65][279/408]	Loss 0.0005 (0.0009)	
training:	Epoch: [65][280/408]	Loss 0.0012 (0.0009)	
training:	Epoch: [65][281/408]	Loss 0.0005 (0.0009)	
training:	Epoch: [65][282/408]	Loss 0.0005 (0.0009)	
training:	Epoch: [65][283/408]	Loss 0.0005 (0.0009)	
training:	Epoch: [65][284/408]	Loss 0.0007 (0.0009)	
training:	Epoch: [65][285/408]	Loss 0.0006 (0.0009)	
training:	Epoch: [65][286/408]	Loss 0.0005 (0.0009)	
training:	Epoch: [65][287/408]	Loss 0.0005 (0.0009)	
training:	Epoch: [65][288/408]	Loss 0.0009 (0.0009)	
training:	Epoch: [65][289/408]	Loss 0.0009 (0.0009)	
training:	Epoch: [65][290/408]	Loss 0.0007 (0.0009)	
training:	Epoch: [65][291/408]	Loss 0.0005 (0.0009)	
training:	Epoch: [65][292/408]	Loss 0.0005 (0.0009)	
training:	Epoch: [65][293/408]	Loss 0.0008 (0.0009)	
training:	Epoch: [65][294/408]	Loss 0.0005 (0.0009)	
training:	Epoch: [65][295/408]	Loss 0.0006 (0.0009)	
training:	Epoch: [65][296/408]	Loss 0.0005 (0.0009)	
training:	Epoch: [65][297/408]	Loss 0.0005 (0.0009)	
training:	Epoch: [65][298/408]	Loss 0.0005 (0.0009)	
training:	Epoch: [65][299/408]	Loss 0.0031 (0.0009)	
training:	Epoch: [65][300/408]	Loss 0.0005 (0.0009)	
training:	Epoch: [65][301/408]	Loss 0.0005 (0.0009)	
training:	Epoch: [65][302/408]	Loss 0.0005 (0.0009)	
training:	Epoch: [65][303/408]	Loss 0.0015 (0.0009)	
training:	Epoch: [65][304/408]	Loss 0.0005 (0.0009)	
training:	Epoch: [65][305/408]	Loss 0.0006 (0.0009)	
training:	Epoch: [65][306/408]	Loss 0.0004 (0.0009)	
training:	Epoch: [65][307/408]	Loss 0.0005 (0.0009)	
training:	Epoch: [65][308/408]	Loss 0.0007 (0.0009)	
training:	Epoch: [65][309/408]	Loss 0.0047 (0.0009)	
training:	Epoch: [65][310/408]	Loss 0.0007 (0.0009)	
training:	Epoch: [65][311/408]	Loss 0.0006 (0.0009)	
training:	Epoch: [65][312/408]	Loss 0.0005 (0.0009)	
training:	Epoch: [65][313/408]	Loss 0.0028 (0.0009)	
training:	Epoch: [65][314/408]	Loss 0.0007 (0.0009)	
training:	Epoch: [65][315/408]	Loss 0.0005 (0.0009)	
training:	Epoch: [65][316/408]	Loss 0.0005 (0.0009)	
training:	Epoch: [65][317/408]	Loss 0.0005 (0.0009)	
training:	Epoch: [65][318/408]	Loss 0.0007 (0.0009)	
training:	Epoch: [65][319/408]	Loss 0.0005 (0.0009)	
training:	Epoch: [65][320/408]	Loss 0.0016 (0.0009)	
training:	Epoch: [65][321/408]	Loss 0.0005 (0.0009)	
training:	Epoch: [65][322/408]	Loss 0.0005 (0.0009)	
training:	Epoch: [65][323/408]	Loss 0.0007 (0.0009)	
training:	Epoch: [65][324/408]	Loss 0.0005 (0.0009)	
training:	Epoch: [65][325/408]	Loss 0.0005 (0.0009)	
training:	Epoch: [65][326/408]	Loss 0.0007 (0.0009)	
training:	Epoch: [65][327/408]	Loss 0.0006 (0.0009)	
training:	Epoch: [65][328/408]	Loss 0.0037 (0.0009)	
training:	Epoch: [65][329/408]	Loss 0.0005 (0.0009)	
training:	Epoch: [65][330/408]	Loss 0.0005 (0.0009)	
training:	Epoch: [65][331/408]	Loss 0.0008 (0.0009)	
training:	Epoch: [65][332/408]	Loss 0.0005 (0.0009)	
training:	Epoch: [65][333/408]	Loss 0.0005 (0.0009)	
training:	Epoch: [65][334/408]	Loss 0.0006 (0.0009)	
training:	Epoch: [65][335/408]	Loss 0.0009 (0.0009)	
training:	Epoch: [65][336/408]	Loss 0.0005 (0.0009)	
training:	Epoch: [65][337/408]	Loss 0.0004 (0.0009)	
training:	Epoch: [65][338/408]	Loss 0.0005 (0.0009)	
training:	Epoch: [65][339/408]	Loss 0.0005 (0.0009)	
training:	Epoch: [65][340/408]	Loss 0.0006 (0.0009)	
training:	Epoch: [65][341/408]	Loss 0.0010 (0.0009)	
training:	Epoch: [65][342/408]	Loss 0.0005 (0.0009)	
training:	Epoch: [65][343/408]	Loss 0.0005 (0.0009)	
training:	Epoch: [65][344/408]	Loss 0.0008 (0.0009)	
training:	Epoch: [65][345/408]	Loss 0.0005 (0.0009)	
training:	Epoch: [65][346/408]	Loss 0.0005 (0.0009)	
training:	Epoch: [65][347/408]	Loss 0.0006 (0.0009)	
training:	Epoch: [65][348/408]	Loss 0.0006 (0.0009)	
training:	Epoch: [65][349/408]	Loss 0.0007 (0.0009)	
training:	Epoch: [65][350/408]	Loss 0.0005 (0.0009)	
training:	Epoch: [65][351/408]	Loss 0.0006 (0.0009)	
training:	Epoch: [65][352/408]	Loss 0.0005 (0.0009)	
training:	Epoch: [65][353/408]	Loss 0.0005 (0.0009)	
training:	Epoch: [65][354/408]	Loss 0.0005 (0.0009)	
training:	Epoch: [65][355/408]	Loss 0.0005 (0.0009)	
training:	Epoch: [65][356/408]	Loss 0.0006 (0.0009)	
training:	Epoch: [65][357/408]	Loss 0.0005 (0.0009)	
training:	Epoch: [65][358/408]	Loss 0.0007 (0.0009)	
training:	Epoch: [65][359/408]	Loss 0.0006 (0.0009)	
training:	Epoch: [65][360/408]	Loss 0.0005 (0.0009)	
training:	Epoch: [65][361/408]	Loss 0.0005 (0.0009)	
training:	Epoch: [65][362/408]	Loss 0.0006 (0.0009)	
training:	Epoch: [65][363/408]	Loss 0.0007 (0.0009)	
training:	Epoch: [65][364/408]	Loss 0.0006 (0.0009)	
training:	Epoch: [65][365/408]	Loss 0.0004 (0.0009)	
training:	Epoch: [65][366/408]	Loss 0.0005 (0.0009)	
training:	Epoch: [65][367/408]	Loss 0.0005 (0.0009)	
training:	Epoch: [65][368/408]	Loss 0.0005 (0.0009)	
training:	Epoch: [65][369/408]	Loss 0.0015 (0.0009)	
training:	Epoch: [65][370/408]	Loss 0.0005 (0.0009)	
training:	Epoch: [65][371/408]	Loss 0.0005 (0.0009)	
training:	Epoch: [65][372/408]	Loss 0.0005 (0.0009)	
training:	Epoch: [65][373/408]	Loss 0.0005 (0.0009)	
training:	Epoch: [65][374/408]	Loss 0.0005 (0.0009)	
training:	Epoch: [65][375/408]	Loss 0.0006 (0.0009)	
training:	Epoch: [65][376/408]	Loss 0.0005 (0.0009)	
training:	Epoch: [65][377/408]	Loss 0.0005 (0.0009)	
training:	Epoch: [65][378/408]	Loss 0.0005 (0.0009)	
training:	Epoch: [65][379/408]	Loss 0.0004 (0.0009)	
training:	Epoch: [65][380/408]	Loss 0.0005 (0.0009)	
training:	Epoch: [65][381/408]	Loss 0.0005 (0.0009)	
training:	Epoch: [65][382/408]	Loss 0.0005 (0.0009)	
training:	Epoch: [65][383/408]	Loss 0.0006 (0.0009)	
training:	Epoch: [65][384/408]	Loss 0.0005 (0.0009)	
training:	Epoch: [65][385/408]	Loss 0.0005 (0.0009)	
training:	Epoch: [65][386/408]	Loss 0.0005 (0.0009)	
training:	Epoch: [65][387/408]	Loss 0.0012 (0.0009)	
training:	Epoch: [65][388/408]	Loss 0.0006 (0.0009)	
training:	Epoch: [65][389/408]	Loss 0.0005 (0.0009)	
training:	Epoch: [65][390/408]	Loss 0.0024 (0.0009)	
training:	Epoch: [65][391/408]	Loss 0.0005 (0.0009)	
training:	Epoch: [65][392/408]	Loss 0.0005 (0.0009)	
training:	Epoch: [65][393/408]	Loss 0.0005 (0.0009)	
training:	Epoch: [65][394/408]	Loss 0.0007 (0.0009)	
training:	Epoch: [65][395/408]	Loss 0.0004 (0.0009)	
training:	Epoch: [65][396/408]	Loss 0.0014 (0.0009)	
training:	Epoch: [65][397/408]	Loss 0.0015 (0.0009)	
training:	Epoch: [65][398/408]	Loss 0.0005 (0.0009)	
training:	Epoch: [65][399/408]	Loss 0.0004 (0.0009)	
training:	Epoch: [65][400/408]	Loss 0.0005 (0.0009)	
training:	Epoch: [65][401/408]	Loss 0.0005 (0.0009)	
training:	Epoch: [65][402/408]	Loss 0.0004 (0.0009)	
training:	Epoch: [65][403/408]	Loss 0.0005 (0.0009)	
training:	Epoch: [65][404/408]	Loss 0.0005 (0.0009)	
training:	Epoch: [65][405/408]	Loss 0.0005 (0.0009)	
training:	Epoch: [65][406/408]	Loss 0.0006 (0.0009)	
training:	Epoch: [65][407/408]	Loss 0.0005 (0.0008)	
training:	Epoch: [65][408/408]	Loss 0.0005 (0.0008)	
Training:	 Loss: 0.0008

Training:	 ACC: 1.0000 1.0000 1.0000 1.0000
Validation:	 ACC: 0.7843 0.7849 0.7984 0.7702
Validation:	 Best_BACC: 0.8226 0.8250 0.8772 0.7679
Validation:	 Loss: 1.3026
Pretraining:	Epoch 66/200
----------
training:	Epoch: [66][1/408]	Loss 0.0005 (0.0005)	
training:	Epoch: [66][2/408]	Loss 0.0005 (0.0005)	
training:	Epoch: [66][3/408]	Loss 0.0004 (0.0005)	
training:	Epoch: [66][4/408]	Loss 0.0005 (0.0005)	
training:	Epoch: [66][5/408]	Loss 0.0040 (0.0012)	
training:	Epoch: [66][6/408]	Loss 0.0005 (0.0011)	
training:	Epoch: [66][7/408]	Loss 0.0240 (0.0043)	
training:	Epoch: [66][8/408]	Loss 0.0006 (0.0039)	
training:	Epoch: [66][9/408]	Loss 0.0005 (0.0035)	
training:	Epoch: [66][10/408]	Loss 0.0006 (0.0032)	
training:	Epoch: [66][11/408]	Loss 0.0009 (0.0030)	
training:	Epoch: [66][12/408]	Loss 0.0005 (0.0028)	
training:	Epoch: [66][13/408]	Loss 0.0012 (0.0027)	
training:	Epoch: [66][14/408]	Loss 0.0006 (0.0025)	
training:	Epoch: [66][15/408]	Loss 0.0007 (0.0024)	
training:	Epoch: [66][16/408]	Loss 0.0023 (0.0024)	
training:	Epoch: [66][17/408]	Loss 0.0005 (0.0023)	
training:	Epoch: [66][18/408]	Loss 0.0006 (0.0022)	
training:	Epoch: [66][19/408]	Loss 0.0012 (0.0021)	
training:	Epoch: [66][20/408]	Loss 0.0005 (0.0021)	
training:	Epoch: [66][21/408]	Loss 0.0006 (0.0020)	
training:	Epoch: [66][22/408]	Loss 0.0041 (0.0021)	
training:	Epoch: [66][23/408]	Loss 0.0006 (0.0020)	
training:	Epoch: [66][24/408]	Loss 0.0006 (0.0020)	
training:	Epoch: [66][25/408]	Loss 0.0027 (0.0020)	
training:	Epoch: [66][26/408]	Loss 0.0020 (0.0020)	
training:	Epoch: [66][27/408]	Loss 0.0006 (0.0019)	
training:	Epoch: [66][28/408]	Loss 0.0005 (0.0019)	
training:	Epoch: [66][29/408]	Loss 0.0008 (0.0019)	
training:	Epoch: [66][30/408]	Loss 0.0011 (0.0018)	
training:	Epoch: [66][31/408]	Loss 0.0006 (0.0018)	
training:	Epoch: [66][32/408]	Loss 0.0005 (0.0017)	
training:	Epoch: [66][33/408]	Loss 0.0017 (0.0017)	
training:	Epoch: [66][34/408]	Loss 0.0005 (0.0017)	
training:	Epoch: [66][35/408]	Loss 0.0005 (0.0017)	
training:	Epoch: [66][36/408]	Loss 0.0005 (0.0016)	
training:	Epoch: [66][37/408]	Loss 0.0005 (0.0016)	
training:	Epoch: [66][38/408]	Loss 0.0005 (0.0016)	
training:	Epoch: [66][39/408]	Loss 0.0049 (0.0017)	
training:	Epoch: [66][40/408]	Loss 0.0005 (0.0016)	
training:	Epoch: [66][41/408]	Loss 0.0012 (0.0016)	
training:	Epoch: [66][42/408]	Loss 0.0033 (0.0017)	
training:	Epoch: [66][43/408]	Loss 0.0016 (0.0017)	
training:	Epoch: [66][44/408]	Loss 0.0005 (0.0016)	
training:	Epoch: [66][45/408]	Loss 0.0006 (0.0016)	
training:	Epoch: [66][46/408]	Loss 0.0012 (0.0016)	
training:	Epoch: [66][47/408]	Loss 0.0005 (0.0016)	
training:	Epoch: [66][48/408]	Loss 0.0017 (0.0016)	
training:	Epoch: [66][49/408]	Loss 0.0007 (0.0016)	
training:	Epoch: [66][50/408]	Loss 0.0005 (0.0015)	
training:	Epoch: [66][51/408]	Loss 0.0006 (0.0015)	
training:	Epoch: [66][52/408]	Loss 0.0005 (0.0015)	
training:	Epoch: [66][53/408]	Loss 0.0005 (0.0015)	
training:	Epoch: [66][54/408]	Loss 0.0010 (0.0015)	
training:	Epoch: [66][55/408]	Loss 0.0005 (0.0015)	
training:	Epoch: [66][56/408]	Loss 0.0005 (0.0014)	
training:	Epoch: [66][57/408]	Loss 0.0014 (0.0014)	
training:	Epoch: [66][58/408]	Loss 0.0004 (0.0014)	
training:	Epoch: [66][59/408]	Loss 0.0004 (0.0014)	
training:	Epoch: [66][60/408]	Loss 0.0006 (0.0014)	
training:	Epoch: [66][61/408]	Loss 0.0005 (0.0014)	
training:	Epoch: [66][62/408]	Loss 0.0015 (0.0014)	
training:	Epoch: [66][63/408]	Loss 0.0005 (0.0014)	
training:	Epoch: [66][64/408]	Loss 0.0007 (0.0014)	
training:	Epoch: [66][65/408]	Loss 0.0006 (0.0013)	
training:	Epoch: [66][66/408]	Loss 0.0007 (0.0013)	
training:	Epoch: [66][67/408]	Loss 0.0005 (0.0013)	
training:	Epoch: [66][68/408]	Loss 0.0019 (0.0013)	
training:	Epoch: [66][69/408]	Loss 0.0013 (0.0013)	
training:	Epoch: [66][70/408]	Loss 0.0005 (0.0013)	
training:	Epoch: [66][71/408]	Loss 0.0008 (0.0013)	
training:	Epoch: [66][72/408]	Loss 0.0020 (0.0013)	
training:	Epoch: [66][73/408]	Loss 0.0005 (0.0013)	
training:	Epoch: [66][74/408]	Loss 0.0005 (0.0013)	
training:	Epoch: [66][75/408]	Loss 0.0017 (0.0013)	
training:	Epoch: [66][76/408]	Loss 0.0004 (0.0013)	
training:	Epoch: [66][77/408]	Loss 0.0005 (0.0013)	
training:	Epoch: [66][78/408]	Loss 0.0007 (0.0013)	
training:	Epoch: [66][79/408]	Loss 0.0005 (0.0013)	
training:	Epoch: [66][80/408]	Loss 0.0037 (0.0013)	
training:	Epoch: [66][81/408]	Loss 0.0005 (0.0013)	
training:	Epoch: [66][82/408]	Loss 0.0006 (0.0013)	
training:	Epoch: [66][83/408]	Loss 0.0004 (0.0013)	
training:	Epoch: [66][84/408]	Loss 0.0005 (0.0013)	
training:	Epoch: [66][85/408]	Loss 0.0009 (0.0013)	
training:	Epoch: [66][86/408]	Loss 0.0005 (0.0012)	
training:	Epoch: [66][87/408]	Loss 0.0005 (0.0012)	
training:	Epoch: [66][88/408]	Loss 0.0005 (0.0012)	
training:	Epoch: [66][89/408]	Loss 0.0005 (0.0012)	
training:	Epoch: [66][90/408]	Loss 0.0005 (0.0012)	
training:	Epoch: [66][91/408]	Loss 0.0011 (0.0012)	
training:	Epoch: [66][92/408]	Loss 0.0008 (0.0012)	
training:	Epoch: [66][93/408]	Loss 0.0005 (0.0012)	
training:	Epoch: [66][94/408]	Loss 0.0005 (0.0012)	
training:	Epoch: [66][95/408]	Loss 0.0007 (0.0012)	
training:	Epoch: [66][96/408]	Loss 0.0006 (0.0012)	
training:	Epoch: [66][97/408]	Loss 0.0007 (0.0012)	
training:	Epoch: [66][98/408]	Loss 0.0011 (0.0012)	
training:	Epoch: [66][99/408]	Loss 0.0008 (0.0012)	
training:	Epoch: [66][100/408]	Loss 0.0016 (0.0012)	
training:	Epoch: [66][101/408]	Loss 0.0005 (0.0012)	
training:	Epoch: [66][102/408]	Loss 0.0005 (0.0012)	
training:	Epoch: [66][103/408]	Loss 0.0006 (0.0012)	
training:	Epoch: [66][104/408]	Loss 0.0005 (0.0011)	
training:	Epoch: [66][105/408]	Loss 0.0005 (0.0011)	
training:	Epoch: [66][106/408]	Loss 0.0005 (0.0011)	
training:	Epoch: [66][107/408]	Loss 0.0005 (0.0011)	
training:	Epoch: [66][108/408]	Loss 0.0004 (0.0011)	
training:	Epoch: [66][109/408]	Loss 0.0028 (0.0011)	
training:	Epoch: [66][110/408]	Loss 0.0007 (0.0011)	
training:	Epoch: [66][111/408]	Loss 0.0006 (0.0011)	
training:	Epoch: [66][112/408]	Loss 0.0004 (0.0011)	
training:	Epoch: [66][113/408]	Loss 0.0005 (0.0011)	
training:	Epoch: [66][114/408]	Loss 0.0007 (0.0011)	
training:	Epoch: [66][115/408]	Loss 0.0006 (0.0011)	
training:	Epoch: [66][116/408]	Loss 0.0005 (0.0011)	
training:	Epoch: [66][117/408]	Loss 0.0005 (0.0011)	
training:	Epoch: [66][118/408]	Loss 0.0006 (0.0011)	
training:	Epoch: [66][119/408]	Loss 0.0005 (0.0011)	
training:	Epoch: [66][120/408]	Loss 0.0020 (0.0011)	
training:	Epoch: [66][121/408]	Loss 0.0005 (0.0011)	
training:	Epoch: [66][122/408]	Loss 0.0004 (0.0011)	
training:	Epoch: [66][123/408]	Loss 0.0004 (0.0011)	
training:	Epoch: [66][124/408]	Loss 0.0005 (0.0011)	
training:	Epoch: [66][125/408]	Loss 0.0005 (0.0011)	
training:	Epoch: [66][126/408]	Loss 0.0005 (0.0011)	
training:	Epoch: [66][127/408]	Loss 0.0007 (0.0011)	
training:	Epoch: [66][128/408]	Loss 0.0004 (0.0011)	
training:	Epoch: [66][129/408]	Loss 0.0005 (0.0011)	
training:	Epoch: [66][130/408]	Loss 0.0015 (0.0011)	
training:	Epoch: [66][131/408]	Loss 0.0005 (0.0011)	
training:	Epoch: [66][132/408]	Loss 0.0006 (0.0011)	
training:	Epoch: [66][133/408]	Loss 0.0004 (0.0010)	
training:	Epoch: [66][134/408]	Loss 0.0006 (0.0010)	
training:	Epoch: [66][135/408]	Loss 0.0004 (0.0010)	
training:	Epoch: [66][136/408]	Loss 0.0004 (0.0010)	
training:	Epoch: [66][137/408]	Loss 0.0005 (0.0010)	
training:	Epoch: [66][138/408]	Loss 0.0004 (0.0010)	
training:	Epoch: [66][139/408]	Loss 0.0004 (0.0010)	
training:	Epoch: [66][140/408]	Loss 0.0006 (0.0010)	
training:	Epoch: [66][141/408]	Loss 0.0007 (0.0010)	
training:	Epoch: [66][142/408]	Loss 0.0005 (0.0010)	
training:	Epoch: [66][143/408]	Loss 0.0009 (0.0010)	
training:	Epoch: [66][144/408]	Loss 0.0006 (0.0010)	
training:	Epoch: [66][145/408]	Loss 0.0004 (0.0010)	
training:	Epoch: [66][146/408]	Loss 0.0027 (0.0010)	
training:	Epoch: [66][147/408]	Loss 0.0005 (0.0010)	
training:	Epoch: [66][148/408]	Loss 0.0006 (0.0010)	
training:	Epoch: [66][149/408]	Loss 0.0005 (0.0010)	
training:	Epoch: [66][150/408]	Loss 0.0006 (0.0010)	
training:	Epoch: [66][151/408]	Loss 0.0005 (0.0010)	
training:	Epoch: [66][152/408]	Loss 0.0006 (0.0010)	
training:	Epoch: [66][153/408]	Loss 0.0004 (0.0010)	
training:	Epoch: [66][154/408]	Loss 0.0005 (0.0010)	
training:	Epoch: [66][155/408]	Loss 0.0041 (0.0010)	
training:	Epoch: [66][156/408]	Loss 0.0005 (0.0010)	
training:	Epoch: [66][157/408]	Loss 0.0005 (0.0010)	
training:	Epoch: [66][158/408]	Loss 0.0005 (0.0010)	
training:	Epoch: [66][159/408]	Loss 0.0005 (0.0010)	
training:	Epoch: [66][160/408]	Loss 0.0032 (0.0010)	
training:	Epoch: [66][161/408]	Loss 0.0004 (0.0010)	
training:	Epoch: [66][162/408]	Loss 0.0005 (0.0010)	
training:	Epoch: [66][163/408]	Loss 0.0005 (0.0010)	
training:	Epoch: [66][164/408]	Loss 0.0004 (0.0010)	
training:	Epoch: [66][165/408]	Loss 0.0028 (0.0010)	
training:	Epoch: [66][166/408]	Loss 0.0004 (0.0010)	
training:	Epoch: [66][167/408]	Loss 0.0005 (0.0010)	
training:	Epoch: [66][168/408]	Loss 0.0005 (0.0010)	
training:	Epoch: [66][169/408]	Loss 0.0005 (0.0010)	
training:	Epoch: [66][170/408]	Loss 0.0005 (0.0010)	
training:	Epoch: [66][171/408]	Loss 0.0004 (0.0010)	
training:	Epoch: [66][172/408]	Loss 0.0006 (0.0010)	
training:	Epoch: [66][173/408]	Loss 0.0004 (0.0010)	
training:	Epoch: [66][174/408]	Loss 0.0034 (0.0010)	
training:	Epoch: [66][175/408]	Loss 0.0005 (0.0010)	
training:	Epoch: [66][176/408]	Loss 0.0004 (0.0010)	
training:	Epoch: [66][177/408]	Loss 0.0007 (0.0010)	
training:	Epoch: [66][178/408]	Loss 0.0005 (0.0010)	
training:	Epoch: [66][179/408]	Loss 0.0004 (0.0010)	
training:	Epoch: [66][180/408]	Loss 0.0004 (0.0010)	
training:	Epoch: [66][181/408]	Loss 0.0004 (0.0010)	
training:	Epoch: [66][182/408]	Loss 0.0004 (0.0010)	
training:	Epoch: [66][183/408]	Loss 0.0004 (0.0010)	
training:	Epoch: [66][184/408]	Loss 0.0005 (0.0010)	
training:	Epoch: [66][185/408]	Loss 0.0005 (0.0010)	
training:	Epoch: [66][186/408]	Loss 0.0004 (0.0010)	
training:	Epoch: [66][187/408]	Loss 0.0005 (0.0010)	
training:	Epoch: [66][188/408]	Loss 0.0005 (0.0010)	
training:	Epoch: [66][189/408]	Loss 0.0004 (0.0010)	
training:	Epoch: [66][190/408]	Loss 0.0004 (0.0010)	
training:	Epoch: [66][191/408]	Loss 0.0004 (0.0010)	
training:	Epoch: [66][192/408]	Loss 0.0004 (0.0009)	
training:	Epoch: [66][193/408]	Loss 0.0005 (0.0009)	
training:	Epoch: [66][194/408]	Loss 0.0004 (0.0009)	
training:	Epoch: [66][195/408]	Loss 0.0007 (0.0009)	
training:	Epoch: [66][196/408]	Loss 0.0009 (0.0009)	
training:	Epoch: [66][197/408]	Loss 0.0005 (0.0009)	
training:	Epoch: [66][198/408]	Loss 0.0004 (0.0009)	
training:	Epoch: [66][199/408]	Loss 0.0004 (0.0009)	
training:	Epoch: [66][200/408]	Loss 0.0005 (0.0009)	
training:	Epoch: [66][201/408]	Loss 0.0004 (0.0009)	
training:	Epoch: [66][202/408]	Loss 0.0004 (0.0009)	
training:	Epoch: [66][203/408]	Loss 0.0004 (0.0009)	
training:	Epoch: [66][204/408]	Loss 0.0004 (0.0009)	
training:	Epoch: [66][205/408]	Loss 0.0017 (0.0009)	
training:	Epoch: [66][206/408]	Loss 0.0005 (0.0009)	
training:	Epoch: [66][207/408]	Loss 0.0005 (0.0009)	
training:	Epoch: [66][208/408]	Loss 0.0004 (0.0009)	
training:	Epoch: [66][209/408]	Loss 0.0006 (0.0009)	
training:	Epoch: [66][210/408]	Loss 0.0005 (0.0009)	
training:	Epoch: [66][211/408]	Loss 0.0005 (0.0009)	
training:	Epoch: [66][212/408]	Loss 0.0005 (0.0009)	
training:	Epoch: [66][213/408]	Loss 0.0009 (0.0009)	
training:	Epoch: [66][214/408]	Loss 0.0005 (0.0009)	
training:	Epoch: [66][215/408]	Loss 0.0004 (0.0009)	
training:	Epoch: [66][216/408]	Loss 0.0031 (0.0009)	
training:	Epoch: [66][217/408]	Loss 0.0004 (0.0009)	
training:	Epoch: [66][218/408]	Loss 0.0005 (0.0009)	
training:	Epoch: [66][219/408]	Loss 0.0005 (0.0009)	
training:	Epoch: [66][220/408]	Loss 0.0005 (0.0009)	
training:	Epoch: [66][221/408]	Loss 0.0007 (0.0009)	
training:	Epoch: [66][222/408]	Loss 0.0006 (0.0009)	
training:	Epoch: [66][223/408]	Loss 0.0005 (0.0009)	
training:	Epoch: [66][224/408]	Loss 0.0011 (0.0009)	
training:	Epoch: [66][225/408]	Loss 0.0015 (0.0009)	
training:	Epoch: [66][226/408]	Loss 0.0015 (0.0009)	
training:	Epoch: [66][227/408]	Loss 0.0006 (0.0009)	
training:	Epoch: [66][228/408]	Loss 0.0005 (0.0009)	
training:	Epoch: [66][229/408]	Loss 0.0004 (0.0009)	
training:	Epoch: [66][230/408]	Loss 0.0005 (0.0009)	
training:	Epoch: [66][231/408]	Loss 0.0005 (0.0009)	
training:	Epoch: [66][232/408]	Loss 0.0006 (0.0009)	
training:	Epoch: [66][233/408]	Loss 0.0009 (0.0009)	
training:	Epoch: [66][234/408]	Loss 0.0004 (0.0009)	
training:	Epoch: [66][235/408]	Loss 0.0005 (0.0009)	
training:	Epoch: [66][236/408]	Loss 0.0005 (0.0009)	
training:	Epoch: [66][237/408]	Loss 0.0004 (0.0009)	
training:	Epoch: [66][238/408]	Loss 0.0006 (0.0009)	
training:	Epoch: [66][239/408]	Loss 0.0005 (0.0009)	
training:	Epoch: [66][240/408]	Loss 0.0007 (0.0009)	
training:	Epoch: [66][241/408]	Loss 0.0005 (0.0009)	
training:	Epoch: [66][242/408]	Loss 0.0004 (0.0009)	
training:	Epoch: [66][243/408]	Loss 0.0005 (0.0009)	
training:	Epoch: [66][244/408]	Loss 0.0008 (0.0009)	
training:	Epoch: [66][245/408]	Loss 0.0014 (0.0009)	
training:	Epoch: [66][246/408]	Loss 0.0005 (0.0009)	
training:	Epoch: [66][247/408]	Loss 0.0004 (0.0009)	
training:	Epoch: [66][248/408]	Loss 0.0005 (0.0009)	
training:	Epoch: [66][249/408]	Loss 0.0005 (0.0009)	
training:	Epoch: [66][250/408]	Loss 0.0005 (0.0009)	
training:	Epoch: [66][251/408]	Loss 0.0004 (0.0009)	
training:	Epoch: [66][252/408]	Loss 0.0005 (0.0009)	
training:	Epoch: [66][253/408]	Loss 0.0004 (0.0009)	
training:	Epoch: [66][254/408]	Loss 0.0004 (0.0009)	
training:	Epoch: [66][255/408]	Loss 0.0004 (0.0009)	
training:	Epoch: [66][256/408]	Loss 0.0012 (0.0009)	
training:	Epoch: [66][257/408]	Loss 0.0004 (0.0009)	
training:	Epoch: [66][258/408]	Loss 0.0005 (0.0009)	
training:	Epoch: [66][259/408]	Loss 0.0004 (0.0009)	
training:	Epoch: [66][260/408]	Loss 0.0005 (0.0009)	
training:	Epoch: [66][261/408]	Loss 0.0004 (0.0009)	
training:	Epoch: [66][262/408]	Loss 0.0004 (0.0009)	
training:	Epoch: [66][263/408]	Loss 0.0005 (0.0009)	
training:	Epoch: [66][264/408]	Loss 0.0004 (0.0009)	
training:	Epoch: [66][265/408]	Loss 0.0005 (0.0009)	
training:	Epoch: [66][266/408]	Loss 0.0014 (0.0009)	
training:	Epoch: [66][267/408]	Loss 0.0005 (0.0009)	
training:	Epoch: [66][268/408]	Loss 0.0005 (0.0009)	
training:	Epoch: [66][269/408]	Loss 0.0004 (0.0009)	
training:	Epoch: [66][270/408]	Loss 0.0004 (0.0009)	
training:	Epoch: [66][271/408]	Loss 0.0007 (0.0009)	
training:	Epoch: [66][272/408]	Loss 0.0005 (0.0009)	
training:	Epoch: [66][273/408]	Loss 0.0006 (0.0009)	
training:	Epoch: [66][274/408]	Loss 0.0005 (0.0008)	
training:	Epoch: [66][275/408]	Loss 0.0005 (0.0008)	
training:	Epoch: [66][276/408]	Loss 0.0004 (0.0008)	
training:	Epoch: [66][277/408]	Loss 0.0004 (0.0008)	
training:	Epoch: [66][278/408]	Loss 0.0005 (0.0008)	
training:	Epoch: [66][279/408]	Loss 0.0006 (0.0008)	
training:	Epoch: [66][280/408]	Loss 0.0004 (0.0008)	
training:	Epoch: [66][281/408]	Loss 0.0005 (0.0008)	
training:	Epoch: [66][282/408]	Loss 0.0005 (0.0008)	
training:	Epoch: [66][283/408]	Loss 0.0005 (0.0008)	
training:	Epoch: [66][284/408]	Loss 0.0004 (0.0008)	
training:	Epoch: [66][285/408]	Loss 0.0004 (0.0008)	
training:	Epoch: [66][286/408]	Loss 0.0005 (0.0008)	
training:	Epoch: [66][287/408]	Loss 0.0004 (0.0008)	
training:	Epoch: [66][288/408]	Loss 0.0007 (0.0008)	
training:	Epoch: [66][289/408]	Loss 0.0004 (0.0008)	
training:	Epoch: [66][290/408]	Loss 0.0006 (0.0008)	
training:	Epoch: [66][291/408]	Loss 0.0005 (0.0008)	
training:	Epoch: [66][292/408]	Loss 0.0006 (0.0008)	
training:	Epoch: [66][293/408]	Loss 0.0005 (0.0008)	
training:	Epoch: [66][294/408]	Loss 0.0053 (0.0008)	
training:	Epoch: [66][295/408]	Loss 0.0005 (0.0008)	
training:	Epoch: [66][296/408]	Loss 0.0004 (0.0008)	
training:	Epoch: [66][297/408]	Loss 0.0004 (0.0008)	
training:	Epoch: [66][298/408]	Loss 0.0004 (0.0008)	
training:	Epoch: [66][299/408]	Loss 0.0004 (0.0008)	
training:	Epoch: [66][300/408]	Loss 0.0006 (0.0008)	
training:	Epoch: [66][301/408]	Loss 0.0004 (0.0008)	
training:	Epoch: [66][302/408]	Loss 0.0007 (0.0008)	
training:	Epoch: [66][303/408]	Loss 0.0005 (0.0008)	
training:	Epoch: [66][304/408]	Loss 0.0004 (0.0008)	
training:	Epoch: [66][305/408]	Loss 0.0004 (0.0008)	
training:	Epoch: [66][306/408]	Loss 0.0004 (0.0008)	
training:	Epoch: [66][307/408]	Loss 0.0004 (0.0008)	
training:	Epoch: [66][308/408]	Loss 0.0005 (0.0008)	
training:	Epoch: [66][309/408]	Loss 0.0004 (0.0008)	
training:	Epoch: [66][310/408]	Loss 0.0005 (0.0008)	
training:	Epoch: [66][311/408]	Loss 0.0004 (0.0008)	
training:	Epoch: [66][312/408]	Loss 0.0004 (0.0008)	
training:	Epoch: [66][313/408]	Loss 0.0004 (0.0008)	
training:	Epoch: [66][314/408]	Loss 0.0007 (0.0008)	
training:	Epoch: [66][315/408]	Loss 0.0005 (0.0008)	
training:	Epoch: [66][316/408]	Loss 0.0006 (0.0008)	
training:	Epoch: [66][317/408]	Loss 0.0004 (0.0008)	
training:	Epoch: [66][318/408]	Loss 0.0004 (0.0008)	
training:	Epoch: [66][319/408]	Loss 0.0004 (0.0008)	
training:	Epoch: [66][320/408]	Loss 0.0005 (0.0008)	
training:	Epoch: [66][321/408]	Loss 0.0004 (0.0008)	
training:	Epoch: [66][322/408]	Loss 0.0004 (0.0008)	
training:	Epoch: [66][323/408]	Loss 0.0004 (0.0008)	
training:	Epoch: [66][324/408]	Loss 0.0005 (0.0008)	
training:	Epoch: [66][325/408]	Loss 0.0004 (0.0008)	
training:	Epoch: [66][326/408]	Loss 0.0004 (0.0008)	
training:	Epoch: [66][327/408]	Loss 0.0006 (0.0008)	
training:	Epoch: [66][328/408]	Loss 0.0004 (0.0008)	
training:	Epoch: [66][329/408]	Loss 0.0268 (0.0009)	
training:	Epoch: [66][330/408]	Loss 0.0005 (0.0009)	
training:	Epoch: [66][331/408]	Loss 0.0004 (0.0009)	
training:	Epoch: [66][332/408]	Loss 0.0005 (0.0009)	
training:	Epoch: [66][333/408]	Loss 0.0007 (0.0009)	
training:	Epoch: [66][334/408]	Loss 0.0005 (0.0009)	
training:	Epoch: [66][335/408]	Loss 0.0005 (0.0009)	
training:	Epoch: [66][336/408]	Loss 0.0004 (0.0009)	
training:	Epoch: [66][337/408]	Loss 0.0006 (0.0009)	
training:	Epoch: [66][338/408]	Loss 0.0005 (0.0009)	
training:	Epoch: [66][339/408]	Loss 0.0004 (0.0009)	
training:	Epoch: [66][340/408]	Loss 0.0005 (0.0009)	
training:	Epoch: [66][341/408]	Loss 0.0012 (0.0009)	
training:	Epoch: [66][342/408]	Loss 0.0005 (0.0009)	
training:	Epoch: [66][343/408]	Loss 0.0005 (0.0009)	
training:	Epoch: [66][344/408]	Loss 0.0005 (0.0009)	
training:	Epoch: [66][345/408]	Loss 0.0009 (0.0009)	
training:	Epoch: [66][346/408]	Loss 0.0004 (0.0009)	
training:	Epoch: [66][347/408]	Loss 0.0016 (0.0009)	
training:	Epoch: [66][348/408]	Loss 0.0007 (0.0009)	
training:	Epoch: [66][349/408]	Loss 0.0005 (0.0009)	
training:	Epoch: [66][350/408]	Loss 0.0005 (0.0009)	
training:	Epoch: [66][351/408]	Loss 0.0006 (0.0009)	
training:	Epoch: [66][352/408]	Loss 0.0005 (0.0009)	
training:	Epoch: [66][353/408]	Loss 0.0005 (0.0009)	
training:	Epoch: [66][354/408]	Loss 0.0004 (0.0009)	
training:	Epoch: [66][355/408]	Loss 0.0004 (0.0009)	
training:	Epoch: [66][356/408]	Loss 0.0004 (0.0009)	
training:	Epoch: [66][357/408]	Loss 0.0011 (0.0009)	
training:	Epoch: [66][358/408]	Loss 0.0004 (0.0009)	
training:	Epoch: [66][359/408]	Loss 0.0012 (0.0009)	
training:	Epoch: [66][360/408]	Loss 0.0005 (0.0009)	
training:	Epoch: [66][361/408]	Loss 0.0007 (0.0009)	
training:	Epoch: [66][362/408]	Loss 0.0006 (0.0009)	
training:	Epoch: [66][363/408]	Loss 0.0005 (0.0009)	
training:	Epoch: [66][364/408]	Loss 0.0004 (0.0009)	
training:	Epoch: [66][365/408]	Loss 0.0006 (0.0009)	
training:	Epoch: [66][366/408]	Loss 0.0006 (0.0009)	
training:	Epoch: [66][367/408]	Loss 0.0004 (0.0009)	
training:	Epoch: [66][368/408]	Loss 0.0005 (0.0008)	
training:	Epoch: [66][369/408]	Loss 0.0008 (0.0008)	
training:	Epoch: [66][370/408]	Loss 0.0013 (0.0009)	
training:	Epoch: [66][371/408]	Loss 0.0004 (0.0008)	
training:	Epoch: [66][372/408]	Loss 0.0004 (0.0008)	
training:	Epoch: [66][373/408]	Loss 0.0006 (0.0008)	
training:	Epoch: [66][374/408]	Loss 0.0005 (0.0008)	
training:	Epoch: [66][375/408]	Loss 0.0004 (0.0008)	
training:	Epoch: [66][376/408]	Loss 0.0004 (0.0008)	
training:	Epoch: [66][377/408]	Loss 0.0004 (0.0008)	
training:	Epoch: [66][378/408]	Loss 0.0007 (0.0008)	
training:	Epoch: [66][379/408]	Loss 0.0005 (0.0008)	
training:	Epoch: [66][380/408]	Loss 0.0006 (0.0008)	
training:	Epoch: [66][381/408]	Loss 0.0004 (0.0008)	
training:	Epoch: [66][382/408]	Loss 0.0007 (0.0008)	
training:	Epoch: [66][383/408]	Loss 0.0006 (0.0008)	
training:	Epoch: [66][384/408]	Loss 0.0009 (0.0008)	
training:	Epoch: [66][385/408]	Loss 0.0037 (0.0008)	
training:	Epoch: [66][386/408]	Loss 0.0004 (0.0008)	
training:	Epoch: [66][387/408]	Loss 0.0005 (0.0008)	
training:	Epoch: [66][388/408]	Loss 0.0006 (0.0008)	
training:	Epoch: [66][389/408]	Loss 0.0014 (0.0008)	
training:	Epoch: [66][390/408]	Loss 0.0005 (0.0008)	
training:	Epoch: [66][391/408]	Loss 0.0004 (0.0008)	
training:	Epoch: [66][392/408]	Loss 0.0004 (0.0008)	
training:	Epoch: [66][393/408]	Loss 0.0013 (0.0008)	
training:	Epoch: [66][394/408]	Loss 0.0006 (0.0008)	
training:	Epoch: [66][395/408]	Loss 0.0004 (0.0008)	
training:	Epoch: [66][396/408]	Loss 0.0005 (0.0008)	
training:	Epoch: [66][397/408]	Loss 0.0004 (0.0008)	
training:	Epoch: [66][398/408]	Loss 0.0007 (0.0008)	
training:	Epoch: [66][399/408]	Loss 0.0007 (0.0008)	
training:	Epoch: [66][400/408]	Loss 0.0027 (0.0008)	
training:	Epoch: [66][401/408]	Loss 0.0006 (0.0008)	
training:	Epoch: [66][402/408]	Loss 0.0022 (0.0008)	
training:	Epoch: [66][403/408]	Loss 0.0004 (0.0008)	
training:	Epoch: [66][404/408]	Loss 0.0004 (0.0008)	
training:	Epoch: [66][405/408]	Loss 0.0004 (0.0008)	
training:	Epoch: [66][406/408]	Loss 0.0013 (0.0008)	
training:	Epoch: [66][407/408]	Loss 0.0005 (0.0008)	
training:	Epoch: [66][408/408]	Loss 0.0004 (0.0008)	
Training:	 Loss: 0.0008

Training:	 ACC: 1.0000 1.0000 1.0000 1.0000
Validation:	 ACC: 0.7812 0.7817 0.7922 0.7702
Validation:	 Best_BACC: 0.8226 0.8250 0.8772 0.7679
Validation:	 Loss: 1.3240
Pretraining:	Epoch 67/200
----------
training:	Epoch: [67][1/408]	Loss 0.0004 (0.0004)	
training:	Epoch: [67][2/408]	Loss 0.0004 (0.0004)	
training:	Epoch: [67][3/408]	Loss 0.0005 (0.0004)	
training:	Epoch: [67][4/408]	Loss 0.0005 (0.0005)	
training:	Epoch: [67][5/408]	Loss 0.0004 (0.0004)	
training:	Epoch: [67][6/408]	Loss 0.0005 (0.0005)	
training:	Epoch: [67][7/408]	Loss 0.0005 (0.0005)	
training:	Epoch: [67][8/408]	Loss 0.0007 (0.0005)	
training:	Epoch: [67][9/408]	Loss 0.0005 (0.0005)	
training:	Epoch: [67][10/408]	Loss 0.0005 (0.0005)	
training:	Epoch: [67][11/408]	Loss 0.0015 (0.0006)	
training:	Epoch: [67][12/408]	Loss 0.0035 (0.0008)	
training:	Epoch: [67][13/408]	Loss 0.0004 (0.0008)	
training:	Epoch: [67][14/408]	Loss 0.0005 (0.0008)	
training:	Epoch: [67][15/408]	Loss 0.0005 (0.0008)	
training:	Epoch: [67][16/408]	Loss 0.0005 (0.0007)	
training:	Epoch: [67][17/408]	Loss 0.0004 (0.0007)	
training:	Epoch: [67][18/408]	Loss 0.0006 (0.0007)	
training:	Epoch: [67][19/408]	Loss 0.0004 (0.0007)	
training:	Epoch: [67][20/408]	Loss 0.0005 (0.0007)	
training:	Epoch: [67][21/408]	Loss 0.0021 (0.0008)	
training:	Epoch: [67][22/408]	Loss 0.0004 (0.0007)	
training:	Epoch: [67][23/408]	Loss 0.0005 (0.0007)	
training:	Epoch: [67][24/408]	Loss 0.0005 (0.0007)	
training:	Epoch: [67][25/408]	Loss 0.0005 (0.0007)	
training:	Epoch: [67][26/408]	Loss 0.0004 (0.0007)	
training:	Epoch: [67][27/408]	Loss 0.0004 (0.0007)	
training:	Epoch: [67][28/408]	Loss 0.0005 (0.0007)	
training:	Epoch: [67][29/408]	Loss 0.0006 (0.0007)	
training:	Epoch: [67][30/408]	Loss 0.0004 (0.0007)	
training:	Epoch: [67][31/408]	Loss 0.0006 (0.0007)	
training:	Epoch: [67][32/408]	Loss 0.0005 (0.0007)	
training:	Epoch: [67][33/408]	Loss 0.0005 (0.0007)	
training:	Epoch: [67][34/408]	Loss 0.0004 (0.0007)	
training:	Epoch: [67][35/408]	Loss 0.0005 (0.0006)	
training:	Epoch: [67][36/408]	Loss 0.0006 (0.0006)	
training:	Epoch: [67][37/408]	Loss 0.0005 (0.0006)	
training:	Epoch: [67][38/408]	Loss 0.0014 (0.0007)	
training:	Epoch: [67][39/408]	Loss 0.0004 (0.0007)	
training:	Epoch: [67][40/408]	Loss 0.0009 (0.0007)	
training:	Epoch: [67][41/408]	Loss 0.0004 (0.0007)	
training:	Epoch: [67][42/408]	Loss 0.0004 (0.0007)	
training:	Epoch: [67][43/408]	Loss 0.0004 (0.0006)	
training:	Epoch: [67][44/408]	Loss 0.0004 (0.0006)	
training:	Epoch: [67][45/408]	Loss 0.0006 (0.0006)	
training:	Epoch: [67][46/408]	Loss 0.0005 (0.0006)	
training:	Epoch: [67][47/408]	Loss 0.0005 (0.0006)	
training:	Epoch: [67][48/408]	Loss 0.0004 (0.0006)	
training:	Epoch: [67][49/408]	Loss 0.0005 (0.0006)	
training:	Epoch: [67][50/408]	Loss 0.0007 (0.0006)	
training:	Epoch: [67][51/408]	Loss 0.0004 (0.0006)	
training:	Epoch: [67][52/408]	Loss 0.0004 (0.0006)	
training:	Epoch: [67][53/408]	Loss 0.0006 (0.0006)	
training:	Epoch: [67][54/408]	Loss 0.0004 (0.0006)	
training:	Epoch: [67][55/408]	Loss 0.0006 (0.0006)	
training:	Epoch: [67][56/408]	Loss 0.0005 (0.0006)	
training:	Epoch: [67][57/408]	Loss 0.0004 (0.0006)	
training:	Epoch: [67][58/408]	Loss 0.0011 (0.0006)	
training:	Epoch: [67][59/408]	Loss 0.0004 (0.0006)	
training:	Epoch: [67][60/408]	Loss 0.0004 (0.0006)	
training:	Epoch: [67][61/408]	Loss 0.0004 (0.0006)	
training:	Epoch: [67][62/408]	Loss 0.0004 (0.0006)	
training:	Epoch: [67][63/408]	Loss 0.0004 (0.0006)	
training:	Epoch: [67][64/408]	Loss 0.0005 (0.0006)	
training:	Epoch: [67][65/408]	Loss 0.0004 (0.0006)	
training:	Epoch: [67][66/408]	Loss 0.0020 (0.0006)	
training:	Epoch: [67][67/408]	Loss 0.0004 (0.0006)	
training:	Epoch: [67][68/408]	Loss 0.0004 (0.0006)	
training:	Epoch: [67][69/408]	Loss 0.0004 (0.0006)	
training:	Epoch: [67][70/408]	Loss 0.0005 (0.0006)	
training:	Epoch: [67][71/408]	Loss 0.0005 (0.0006)	
training:	Epoch: [67][72/408]	Loss 0.0004 (0.0006)	
training:	Epoch: [67][73/408]	Loss 0.0007 (0.0006)	
training:	Epoch: [67][74/408]	Loss 0.0004 (0.0006)	
training:	Epoch: [67][75/408]	Loss 0.0004 (0.0006)	
training:	Epoch: [67][76/408]	Loss 0.0005 (0.0006)	
training:	Epoch: [67][77/408]	Loss 0.0004 (0.0006)	
training:	Epoch: [67][78/408]	Loss 0.0006 (0.0006)	
training:	Epoch: [67][79/408]	Loss 0.0004 (0.0006)	
training:	Epoch: [67][80/408]	Loss 0.0005 (0.0006)	
training:	Epoch: [67][81/408]	Loss 0.0004 (0.0006)	
training:	Epoch: [67][82/408]	Loss 0.0004 (0.0006)	
training:	Epoch: [67][83/408]	Loss 0.0005 (0.0006)	
training:	Epoch: [67][84/408]	Loss 0.0005 (0.0006)	
training:	Epoch: [67][85/408]	Loss 0.0005 (0.0006)	
training:	Epoch: [67][86/408]	Loss 0.0004 (0.0006)	
training:	Epoch: [67][87/408]	Loss 0.0004 (0.0006)	
training:	Epoch: [67][88/408]	Loss 0.0004 (0.0006)	
training:	Epoch: [67][89/408]	Loss 0.0005 (0.0006)	
training:	Epoch: [67][90/408]	Loss 0.0004 (0.0006)	
training:	Epoch: [67][91/408]	Loss 0.0014 (0.0006)	
training:	Epoch: [67][92/408]	Loss 0.0004 (0.0006)	
training:	Epoch: [67][93/408]	Loss 0.0006 (0.0006)	
training:	Epoch: [67][94/408]	Loss 0.0005 (0.0006)	
training:	Epoch: [67][95/408]	Loss 0.0009 (0.0006)	
training:	Epoch: [67][96/408]	Loss 0.0004 (0.0006)	
training:	Epoch: [67][97/408]	Loss 0.0005 (0.0006)	
training:	Epoch: [67][98/408]	Loss 0.0004 (0.0006)	
training:	Epoch: [67][99/408]	Loss 0.0006 (0.0006)	
training:	Epoch: [67][100/408]	Loss 0.0004 (0.0006)	
training:	Epoch: [67][101/408]	Loss 0.0004 (0.0006)	
training:	Epoch: [67][102/408]	Loss 0.0007 (0.0006)	
training:	Epoch: [67][103/408]	Loss 0.0009 (0.0006)	
training:	Epoch: [67][104/408]	Loss 0.0004 (0.0006)	
training:	Epoch: [67][105/408]	Loss 0.0004 (0.0006)	
training:	Epoch: [67][106/408]	Loss 0.0004 (0.0006)	
training:	Epoch: [67][107/408]	Loss 0.0005 (0.0006)	
training:	Epoch: [67][108/408]	Loss 0.0008 (0.0006)	
training:	Epoch: [67][109/408]	Loss 0.0004 (0.0006)	
training:	Epoch: [67][110/408]	Loss 0.0004 (0.0006)	
training:	Epoch: [67][111/408]	Loss 0.0004 (0.0006)	
training:	Epoch: [67][112/408]	Loss 0.0004 (0.0006)	
training:	Epoch: [67][113/408]	Loss 0.0019 (0.0006)	
training:	Epoch: [67][114/408]	Loss 0.0004 (0.0006)	
training:	Epoch: [67][115/408]	Loss 0.0004 (0.0006)	
training:	Epoch: [67][116/408]	Loss 0.0004 (0.0006)	
training:	Epoch: [67][117/408]	Loss 0.0004 (0.0006)	
training:	Epoch: [67][118/408]	Loss 0.0004 (0.0006)	
training:	Epoch: [67][119/408]	Loss 0.0006 (0.0006)	
training:	Epoch: [67][120/408]	Loss 0.0010 (0.0006)	
training:	Epoch: [67][121/408]	Loss 0.0005 (0.0006)	
training:	Epoch: [67][122/408]	Loss 0.0008 (0.0006)	
training:	Epoch: [67][123/408]	Loss 0.0004 (0.0006)	
training:	Epoch: [67][124/408]	Loss 0.0005 (0.0006)	
training:	Epoch: [67][125/408]	Loss 0.0005 (0.0006)	
training:	Epoch: [67][126/408]	Loss 0.0004 (0.0006)	
training:	Epoch: [67][127/408]	Loss 0.0004 (0.0006)	
training:	Epoch: [67][128/408]	Loss 0.0010 (0.0006)	
training:	Epoch: [67][129/408]	Loss 0.0030 (0.0006)	
training:	Epoch: [67][130/408]	Loss 0.0006 (0.0006)	
training:	Epoch: [67][131/408]	Loss 0.0004 (0.0006)	
training:	Epoch: [67][132/408]	Loss 0.0005 (0.0006)	
training:	Epoch: [67][133/408]	Loss 0.0004 (0.0006)	
training:	Epoch: [67][134/408]	Loss 0.0004 (0.0006)	
training:	Epoch: [67][135/408]	Loss 0.0004 (0.0006)	
training:	Epoch: [67][136/408]	Loss 0.0005 (0.0006)	
training:	Epoch: [67][137/408]	Loss 0.0004 (0.0006)	
training:	Epoch: [67][138/408]	Loss 0.0006 (0.0006)	
training:	Epoch: [67][139/408]	Loss 0.0004 (0.0006)	
training:	Epoch: [67][140/408]	Loss 0.0004 (0.0006)	
training:	Epoch: [67][141/408]	Loss 0.0004 (0.0006)	
training:	Epoch: [67][142/408]	Loss 0.0005 (0.0006)	
training:	Epoch: [67][143/408]	Loss 0.0006 (0.0006)	
training:	Epoch: [67][144/408]	Loss 0.0004 (0.0006)	
training:	Epoch: [67][145/408]	Loss 0.0004 (0.0006)	
training:	Epoch: [67][146/408]	Loss 0.0004 (0.0006)	
training:	Epoch: [67][147/408]	Loss 0.0004 (0.0006)	
training:	Epoch: [67][148/408]	Loss 0.0004 (0.0006)	
training:	Epoch: [67][149/408]	Loss 0.0004 (0.0006)	
training:	Epoch: [67][150/408]	Loss 0.0018 (0.0006)	
training:	Epoch: [67][151/408]	Loss 0.0006 (0.0006)	
training:	Epoch: [67][152/408]	Loss 0.0004 (0.0006)	
training:	Epoch: [67][153/408]	Loss 0.0005 (0.0006)	
training:	Epoch: [67][154/408]	Loss 0.0004 (0.0006)	
training:	Epoch: [67][155/408]	Loss 0.0005 (0.0006)	
training:	Epoch: [67][156/408]	Loss 0.0005 (0.0006)	
training:	Epoch: [67][157/408]	Loss 0.0038 (0.0006)	
training:	Epoch: [67][158/408]	Loss 0.0004 (0.0006)	
training:	Epoch: [67][159/408]	Loss 0.0023 (0.0006)	
training:	Epoch: [67][160/408]	Loss 0.0005 (0.0006)	
training:	Epoch: [67][161/408]	Loss 0.0004 (0.0006)	
training:	Epoch: [67][162/408]	Loss 0.0018 (0.0006)	
training:	Epoch: [67][163/408]	Loss 0.0004 (0.0006)	
training:	Epoch: [67][164/408]	Loss 0.0006 (0.0006)	
training:	Epoch: [67][165/408]	Loss 0.0005 (0.0006)	
training:	Epoch: [67][166/408]	Loss 0.0005 (0.0006)	
training:	Epoch: [67][167/408]	Loss 0.0004 (0.0006)	
training:	Epoch: [67][168/408]	Loss 0.0005 (0.0006)	
training:	Epoch: [67][169/408]	Loss 0.0005 (0.0006)	
training:	Epoch: [67][170/408]	Loss 0.0004 (0.0006)	
training:	Epoch: [67][171/408]	Loss 0.0005 (0.0006)	
training:	Epoch: [67][172/408]	Loss 0.0015 (0.0006)	
training:	Epoch: [67][173/408]	Loss 0.0004 (0.0006)	
training:	Epoch: [67][174/408]	Loss 0.0028 (0.0006)	
training:	Epoch: [67][175/408]	Loss 0.0004 (0.0006)	
training:	Epoch: [67][176/408]	Loss 0.0005 (0.0006)	
training:	Epoch: [67][177/408]	Loss 0.0004 (0.0006)	
training:	Epoch: [67][178/408]	Loss 0.0057 (0.0006)	
training:	Epoch: [67][179/408]	Loss 0.0006 (0.0006)	
training:	Epoch: [67][180/408]	Loss 0.0004 (0.0006)	
training:	Epoch: [67][181/408]	Loss 0.0006 (0.0006)	
training:	Epoch: [67][182/408]	Loss 0.0004 (0.0006)	
training:	Epoch: [67][183/408]	Loss 0.0004 (0.0006)	
training:	Epoch: [67][184/408]	Loss 0.0021 (0.0007)	
training:	Epoch: [67][185/408]	Loss 0.0011 (0.0007)	
training:	Epoch: [67][186/408]	Loss 0.0005 (0.0007)	
training:	Epoch: [67][187/408]	Loss 0.0004 (0.0007)	
training:	Epoch: [67][188/408]	Loss 0.0010 (0.0007)	
training:	Epoch: [67][189/408]	Loss 0.0004 (0.0007)	
training:	Epoch: [67][190/408]	Loss 0.0004 (0.0007)	
training:	Epoch: [67][191/408]	Loss 0.0006 (0.0007)	
training:	Epoch: [67][192/408]	Loss 0.0005 (0.0007)	
training:	Epoch: [67][193/408]	Loss 0.0004 (0.0006)	
training:	Epoch: [67][194/408]	Loss 0.0004 (0.0006)	
training:	Epoch: [67][195/408]	Loss 0.0004 (0.0006)	
training:	Epoch: [67][196/408]	Loss 0.0004 (0.0006)	
training:	Epoch: [67][197/408]	Loss 0.0004 (0.0006)	
training:	Epoch: [67][198/408]	Loss 0.0004 (0.0006)	
training:	Epoch: [67][199/408]	Loss 0.0004 (0.0006)	
training:	Epoch: [67][200/408]	Loss 0.0004 (0.0006)	
training:	Epoch: [67][201/408]	Loss 0.0004 (0.0006)	
training:	Epoch: [67][202/408]	Loss 0.0004 (0.0006)	
training:	Epoch: [67][203/408]	Loss 0.0004 (0.0006)	
training:	Epoch: [67][204/408]	Loss 0.0007 (0.0006)	
training:	Epoch: [67][205/408]	Loss 0.0004 (0.0006)	
training:	Epoch: [67][206/408]	Loss 0.0004 (0.0006)	
training:	Epoch: [67][207/408]	Loss 0.0005 (0.0006)	
training:	Epoch: [67][208/408]	Loss 0.0004 (0.0006)	
training:	Epoch: [67][209/408]	Loss 0.0004 (0.0006)	
training:	Epoch: [67][210/408]	Loss 0.0004 (0.0006)	
training:	Epoch: [67][211/408]	Loss 0.0004 (0.0006)	
training:	Epoch: [67][212/408]	Loss 0.0004 (0.0006)	
training:	Epoch: [67][213/408]	Loss 0.0004 (0.0006)	
training:	Epoch: [67][214/408]	Loss 0.0008 (0.0006)	
training:	Epoch: [67][215/408]	Loss 0.0004 (0.0006)	
training:	Epoch: [67][216/408]	Loss 0.0004 (0.0006)	
training:	Epoch: [67][217/408]	Loss 0.0014 (0.0006)	
training:	Epoch: [67][218/408]	Loss 0.0004 (0.0006)	
training:	Epoch: [67][219/408]	Loss 0.0022 (0.0006)	
training:	Epoch: [67][220/408]	Loss 0.0004 (0.0006)	
training:	Epoch: [67][221/408]	Loss 0.0004 (0.0006)	
training:	Epoch: [67][222/408]	Loss 0.0004 (0.0006)	
training:	Epoch: [67][223/408]	Loss 0.0005 (0.0006)	
training:	Epoch: [67][224/408]	Loss 0.0004 (0.0006)	
training:	Epoch: [67][225/408]	Loss 0.0004 (0.0006)	
training:	Epoch: [67][226/408]	Loss 0.0004 (0.0006)	
training:	Epoch: [67][227/408]	Loss 0.0004 (0.0006)	
training:	Epoch: [67][228/408]	Loss 0.0004 (0.0006)	
training:	Epoch: [67][229/408]	Loss 0.0005 (0.0006)	
training:	Epoch: [67][230/408]	Loss 0.0004 (0.0006)	
training:	Epoch: [67][231/408]	Loss 0.0004 (0.0006)	
training:	Epoch: [67][232/408]	Loss 0.0013 (0.0006)	
training:	Epoch: [67][233/408]	Loss 0.0005 (0.0006)	
training:	Epoch: [67][234/408]	Loss 0.0004 (0.0006)	
training:	Epoch: [67][235/408]	Loss 0.0004 (0.0006)	
training:	Epoch: [67][236/408]	Loss 0.0005 (0.0006)	
training:	Epoch: [67][237/408]	Loss 0.0005 (0.0006)	
training:	Epoch: [67][238/408]	Loss 0.0059 (0.0006)	
training:	Epoch: [67][239/408]	Loss 0.0004 (0.0006)	
training:	Epoch: [67][240/408]	Loss 0.0004 (0.0006)	
training:	Epoch: [67][241/408]	Loss 0.0004 (0.0006)	
training:	Epoch: [67][242/408]	Loss 0.0013 (0.0006)	
training:	Epoch: [67][243/408]	Loss 0.0004 (0.0006)	
training:	Epoch: [67][244/408]	Loss 0.0004 (0.0006)	
training:	Epoch: [67][245/408]	Loss 0.0004 (0.0006)	
training:	Epoch: [67][246/408]	Loss 0.0004 (0.0006)	
training:	Epoch: [67][247/408]	Loss 0.0004 (0.0006)	
training:	Epoch: [67][248/408]	Loss 0.0004 (0.0006)	
training:	Epoch: [67][249/408]	Loss 0.0004 (0.0006)	
training:	Epoch: [67][250/408]	Loss 0.0004 (0.0006)	
training:	Epoch: [67][251/408]	Loss 0.0004 (0.0006)	
training:	Epoch: [67][252/408]	Loss 0.0004 (0.0006)	
training:	Epoch: [67][253/408]	Loss 0.0003 (0.0006)	
training:	Epoch: [67][254/408]	Loss 0.0004 (0.0006)	
training:	Epoch: [67][255/408]	Loss 0.0004 (0.0006)	
training:	Epoch: [67][256/408]	Loss 0.0004 (0.0006)	
training:	Epoch: [67][257/408]	Loss 0.0004 (0.0006)	
training:	Epoch: [67][258/408]	Loss 0.0006 (0.0006)	
training:	Epoch: [67][259/408]	Loss 0.0005 (0.0006)	
training:	Epoch: [67][260/408]	Loss 0.0004 (0.0006)	
training:	Epoch: [67][261/408]	Loss 0.0004 (0.0006)	
training:	Epoch: [67][262/408]	Loss 0.0006 (0.0006)	
training:	Epoch: [67][263/408]	Loss 0.0004 (0.0006)	
training:	Epoch: [67][264/408]	Loss 0.0006 (0.0006)	
training:	Epoch: [67][265/408]	Loss 0.0004 (0.0006)	
training:	Epoch: [67][266/408]	Loss 0.0004 (0.0006)	
training:	Epoch: [67][267/408]	Loss 0.0005 (0.0006)	
training:	Epoch: [67][268/408]	Loss 0.0004 (0.0006)	
training:	Epoch: [67][269/408]	Loss 0.0004 (0.0006)	
training:	Epoch: [67][270/408]	Loss 0.0004 (0.0006)	
training:	Epoch: [67][271/408]	Loss 0.0005 (0.0006)	
training:	Epoch: [67][272/408]	Loss 0.0004 (0.0006)	
training:	Epoch: [67][273/408]	Loss 0.0004 (0.0006)	
training:	Epoch: [67][274/408]	Loss 0.0005 (0.0006)	
training:	Epoch: [67][275/408]	Loss 0.0005 (0.0006)	
training:	Epoch: [67][276/408]	Loss 0.0004 (0.0006)	
training:	Epoch: [67][277/408]	Loss 0.0005 (0.0006)	
training:	Epoch: [67][278/408]	Loss 0.0004 (0.0006)	
training:	Epoch: [67][279/408]	Loss 0.0004 (0.0006)	
training:	Epoch: [67][280/408]	Loss 0.0008 (0.0006)	
training:	Epoch: [67][281/408]	Loss 0.0011 (0.0006)	
training:	Epoch: [67][282/408]	Loss 0.0006 (0.0006)	
training:	Epoch: [67][283/408]	Loss 0.0004 (0.0006)	
training:	Epoch: [67][284/408]	Loss 0.0004 (0.0006)	
training:	Epoch: [67][285/408]	Loss 0.0004 (0.0006)	
training:	Epoch: [67][286/408]	Loss 0.0004 (0.0006)	
training:	Epoch: [67][287/408]	Loss 0.0004 (0.0006)	
training:	Epoch: [67][288/408]	Loss 0.0004 (0.0006)	
training:	Epoch: [67][289/408]	Loss 0.0004 (0.0006)	
training:	Epoch: [67][290/408]	Loss 0.0006 (0.0006)	
training:	Epoch: [67][291/408]	Loss 0.0005 (0.0006)	
training:	Epoch: [67][292/408]	Loss 0.0004 (0.0006)	
training:	Epoch: [67][293/408]	Loss 0.0004 (0.0006)	
training:	Epoch: [67][294/408]	Loss 0.0007 (0.0006)	
training:	Epoch: [67][295/408]	Loss 0.0004 (0.0006)	
training:	Epoch: [67][296/408]	Loss 0.0004 (0.0006)	
training:	Epoch: [67][297/408]	Loss 0.0004 (0.0006)	
training:	Epoch: [67][298/408]	Loss 0.0004 (0.0006)	
training:	Epoch: [67][299/408]	Loss 0.0016 (0.0006)	
training:	Epoch: [67][300/408]	Loss 0.0004 (0.0006)	
training:	Epoch: [67][301/408]	Loss 0.0142 (0.0007)	
training:	Epoch: [67][302/408]	Loss 0.0004 (0.0007)	
training:	Epoch: [67][303/408]	Loss 0.0004 (0.0007)	
training:	Epoch: [67][304/408]	Loss 0.0004 (0.0007)	
training:	Epoch: [67][305/408]	Loss 0.0004 (0.0007)	
training:	Epoch: [67][306/408]	Loss 0.0004 (0.0007)	
training:	Epoch: [67][307/408]	Loss 0.0005 (0.0007)	
training:	Epoch: [67][308/408]	Loss 0.0004 (0.0007)	
training:	Epoch: [67][309/408]	Loss 0.0004 (0.0007)	
training:	Epoch: [67][310/408]	Loss 0.0005 (0.0006)	
training:	Epoch: [67][311/408]	Loss 0.0014 (0.0007)	
training:	Epoch: [67][312/408]	Loss 0.0004 (0.0007)	
training:	Epoch: [67][313/408]	Loss 0.0005 (0.0007)	
training:	Epoch: [67][314/408]	Loss 0.0011 (0.0007)	
training:	Epoch: [67][315/408]	Loss 0.0022 (0.0007)	
training:	Epoch: [67][316/408]	Loss 0.0006 (0.0007)	
training:	Epoch: [67][317/408]	Loss 0.0005 (0.0007)	
training:	Epoch: [67][318/408]	Loss 0.0004 (0.0007)	
training:	Epoch: [67][319/408]	Loss 0.0004 (0.0007)	
training:	Epoch: [67][320/408]	Loss 0.0004 (0.0007)	
training:	Epoch: [67][321/408]	Loss 0.0004 (0.0007)	
training:	Epoch: [67][322/408]	Loss 0.0004 (0.0007)	
training:	Epoch: [67][323/408]	Loss 0.0004 (0.0007)	
training:	Epoch: [67][324/408]	Loss 0.0004 (0.0007)	
training:	Epoch: [67][325/408]	Loss 0.0004 (0.0007)	
training:	Epoch: [67][326/408]	Loss 0.0004 (0.0006)	
training:	Epoch: [67][327/408]	Loss 0.0004 (0.0006)	
training:	Epoch: [67][328/408]	Loss 0.0006 (0.0006)	
training:	Epoch: [67][329/408]	Loss 0.0004 (0.0006)	
training:	Epoch: [67][330/408]	Loss 0.0004 (0.0006)	
training:	Epoch: [67][331/408]	Loss 0.0008 (0.0006)	
training:	Epoch: [67][332/408]	Loss 0.0005 (0.0006)	
training:	Epoch: [67][333/408]	Loss 0.0004 (0.0006)	
training:	Epoch: [67][334/408]	Loss 0.0013 (0.0006)	
training:	Epoch: [67][335/408]	Loss 0.0006 (0.0006)	
training:	Epoch: [67][336/408]	Loss 0.0004 (0.0006)	
training:	Epoch: [67][337/408]	Loss 0.0004 (0.0006)	
training:	Epoch: [67][338/408]	Loss 0.0005 (0.0006)	
training:	Epoch: [67][339/408]	Loss 0.0004 (0.0006)	
training:	Epoch: [67][340/408]	Loss 0.0004 (0.0006)	
training:	Epoch: [67][341/408]	Loss 0.0014 (0.0006)	
training:	Epoch: [67][342/408]	Loss 0.0004 (0.0006)	
training:	Epoch: [67][343/408]	Loss 0.0009 (0.0006)	
training:	Epoch: [67][344/408]	Loss 0.0006 (0.0006)	
training:	Epoch: [67][345/408]	Loss 0.0004 (0.0006)	
training:	Epoch: [67][346/408]	Loss 0.0004 (0.0006)	
training:	Epoch: [67][347/408]	Loss 0.0005 (0.0006)	
training:	Epoch: [67][348/408]	Loss 0.0009 (0.0006)	
training:	Epoch: [67][349/408]	Loss 0.0004 (0.0006)	
training:	Epoch: [67][350/408]	Loss 0.0006 (0.0006)	
training:	Epoch: [67][351/408]	Loss 0.0004 (0.0006)	
training:	Epoch: [67][352/408]	Loss 0.0004 (0.0006)	
training:	Epoch: [67][353/408]	Loss 0.0006 (0.0006)	
training:	Epoch: [67][354/408]	Loss 0.0004 (0.0006)	
training:	Epoch: [67][355/408]	Loss 0.0045 (0.0007)	
training:	Epoch: [67][356/408]	Loss 0.0004 (0.0007)	
training:	Epoch: [67][357/408]	Loss 0.0004 (0.0007)	
training:	Epoch: [67][358/408]	Loss 0.0004 (0.0007)	
training:	Epoch: [67][359/408]	Loss 0.0005 (0.0007)	
training:	Epoch: [67][360/408]	Loss 0.0004 (0.0007)	
training:	Epoch: [67][361/408]	Loss 0.0004 (0.0006)	
training:	Epoch: [67][362/408]	Loss 0.0005 (0.0006)	
training:	Epoch: [67][363/408]	Loss 0.0004 (0.0006)	
training:	Epoch: [67][364/408]	Loss 0.0004 (0.0006)	
training:	Epoch: [67][365/408]	Loss 0.0005 (0.0006)	
training:	Epoch: [67][366/408]	Loss 0.0005 (0.0006)	
training:	Epoch: [67][367/408]	Loss 0.0012 (0.0006)	
training:	Epoch: [67][368/408]	Loss 0.0004 (0.0006)	
training:	Epoch: [67][369/408]	Loss 0.0010 (0.0006)	
training:	Epoch: [67][370/408]	Loss 0.0004 (0.0006)	
training:	Epoch: [67][371/408]	Loss 0.0062 (0.0007)	
training:	Epoch: [67][372/408]	Loss 0.0003 (0.0007)	
training:	Epoch: [67][373/408]	Loss 0.0012 (0.0007)	
training:	Epoch: [67][374/408]	Loss 0.0004 (0.0007)	
training:	Epoch: [67][375/408]	Loss 0.0003 (0.0007)	
training:	Epoch: [67][376/408]	Loss 0.0004 (0.0007)	
training:	Epoch: [67][377/408]	Loss 0.0005 (0.0007)	
training:	Epoch: [67][378/408]	Loss 0.0004 (0.0007)	
training:	Epoch: [67][379/408]	Loss 0.0004 (0.0007)	
training:	Epoch: [67][380/408]	Loss 0.0004 (0.0007)	
training:	Epoch: [67][381/408]	Loss 0.0004 (0.0007)	
training:	Epoch: [67][382/408]	Loss 0.0007 (0.0007)	
training:	Epoch: [67][383/408]	Loss 0.0004 (0.0007)	
training:	Epoch: [67][384/408]	Loss 0.0006 (0.0007)	
training:	Epoch: [67][385/408]	Loss 0.0043 (0.0007)	
training:	Epoch: [67][386/408]	Loss 0.0004 (0.0007)	
training:	Epoch: [67][387/408]	Loss 0.0004 (0.0007)	
training:	Epoch: [67][388/408]	Loss 0.0012 (0.0007)	
training:	Epoch: [67][389/408]	Loss 0.0004 (0.0007)	
training:	Epoch: [67][390/408]	Loss 0.0007 (0.0007)	
training:	Epoch: [67][391/408]	Loss 0.0009 (0.0007)	
training:	Epoch: [67][392/408]	Loss 0.0011 (0.0007)	
training:	Epoch: [67][393/408]	Loss 0.0007 (0.0007)	
training:	Epoch: [67][394/408]	Loss 0.0005 (0.0007)	
training:	Epoch: [67][395/408]	Loss 0.0004 (0.0007)	
training:	Epoch: [67][396/408]	Loss 0.0004 (0.0007)	
training:	Epoch: [67][397/408]	Loss 0.0005 (0.0007)	
training:	Epoch: [67][398/408]	Loss 0.0004 (0.0007)	
training:	Epoch: [67][399/408]	Loss 0.0004 (0.0007)	
training:	Epoch: [67][400/408]	Loss 0.0004 (0.0007)	
training:	Epoch: [67][401/408]	Loss 0.0004 (0.0007)	
training:	Epoch: [67][402/408]	Loss 0.0004 (0.0007)	
training:	Epoch: [67][403/408]	Loss 0.0004 (0.0007)	
training:	Epoch: [67][404/408]	Loss 0.0004 (0.0007)	
training:	Epoch: [67][405/408]	Loss 0.0004 (0.0007)	
training:	Epoch: [67][406/408]	Loss 0.0005 (0.0007)	
training:	Epoch: [67][407/408]	Loss 0.0004 (0.0007)	
training:	Epoch: [67][408/408]	Loss 0.0006 (0.0007)	
Training:	 Loss: 0.0007

Training:	 ACC: 1.0000 1.0000 1.0000 1.0000
Validation:	 ACC: 0.7781 0.7790 0.7994 0.7567
Validation:	 Best_BACC: 0.8226 0.8250 0.8772 0.7679
Validation:	 Loss: 1.3708
Pretraining:	Epoch 68/200
----------
training:	Epoch: [68][1/408]	Loss 0.0004 (0.0004)	
training:	Epoch: [68][2/408]	Loss 0.0004 (0.0004)	
training:	Epoch: [68][3/408]	Loss 0.0004 (0.0004)	
training:	Epoch: [68][4/408]	Loss 0.0005 (0.0004)	
training:	Epoch: [68][5/408]	Loss 0.0006 (0.0004)	
training:	Epoch: [68][6/408]	Loss 0.0006 (0.0005)	
training:	Epoch: [68][7/408]	Loss 0.0004 (0.0005)	
training:	Epoch: [68][8/408]	Loss 0.0004 (0.0004)	
training:	Epoch: [68][9/408]	Loss 0.0004 (0.0004)	
training:	Epoch: [68][10/408]	Loss 0.0007 (0.0005)	
training:	Epoch: [68][11/408]	Loss 0.0004 (0.0005)	
training:	Epoch: [68][12/408]	Loss 0.0005 (0.0005)	
training:	Epoch: [68][13/408]	Loss 0.0005 (0.0005)	
training:	Epoch: [68][14/408]	Loss 0.0004 (0.0005)	
training:	Epoch: [68][15/408]	Loss 0.0005 (0.0005)	
training:	Epoch: [68][16/408]	Loss 0.0006 (0.0005)	
training:	Epoch: [68][17/408]	Loss 0.0004 (0.0005)	
training:	Epoch: [68][18/408]	Loss 0.0004 (0.0005)	
training:	Epoch: [68][19/408]	Loss 0.0005 (0.0005)	
training:	Epoch: [68][20/408]	Loss 0.0004 (0.0005)	
training:	Epoch: [68][21/408]	Loss 0.0004 (0.0005)	
training:	Epoch: [68][22/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [68][23/408]	Loss 0.0005 (0.0004)	
training:	Epoch: [68][24/408]	Loss 0.0006 (0.0005)	
training:	Epoch: [68][25/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [68][26/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [68][27/408]	Loss 0.0010 (0.0005)	
training:	Epoch: [68][28/408]	Loss 0.0004 (0.0005)	
training:	Epoch: [68][29/408]	Loss 0.0004 (0.0005)	
training:	Epoch: [68][30/408]	Loss 0.0005 (0.0005)	
training:	Epoch: [68][31/408]	Loss 0.0051 (0.0006)	
training:	Epoch: [68][32/408]	Loss 0.0004 (0.0006)	
training:	Epoch: [68][33/408]	Loss 0.0005 (0.0006)	
training:	Epoch: [68][34/408]	Loss 0.0004 (0.0006)	
training:	Epoch: [68][35/408]	Loss 0.0004 (0.0006)	
training:	Epoch: [68][36/408]	Loss 0.0004 (0.0006)	
training:	Epoch: [68][37/408]	Loss 0.0005 (0.0006)	
training:	Epoch: [68][38/408]	Loss 0.0019 (0.0006)	
training:	Epoch: [68][39/408]	Loss 0.0003 (0.0006)	
training:	Epoch: [68][40/408]	Loss 0.0004 (0.0006)	
training:	Epoch: [68][41/408]	Loss 0.0004 (0.0006)	
training:	Epoch: [68][42/408]	Loss 0.0005 (0.0006)	
training:	Epoch: [68][43/408]	Loss 0.0004 (0.0006)	
training:	Epoch: [68][44/408]	Loss 0.0004 (0.0006)	
training:	Epoch: [68][45/408]	Loss 0.0004 (0.0006)	
training:	Epoch: [68][46/408]	Loss 0.0004 (0.0006)	
training:	Epoch: [68][47/408]	Loss 0.0003 (0.0006)	
training:	Epoch: [68][48/408]	Loss 0.0004 (0.0006)	
training:	Epoch: [68][49/408]	Loss 0.0004 (0.0006)	
training:	Epoch: [68][50/408]	Loss 0.0004 (0.0006)	
training:	Epoch: [68][51/408]	Loss 0.0004 (0.0006)	
training:	Epoch: [68][52/408]	Loss 0.0004 (0.0006)	
training:	Epoch: [68][53/408]	Loss 0.0007 (0.0006)	
training:	Epoch: [68][54/408]	Loss 0.0004 (0.0006)	
training:	Epoch: [68][55/408]	Loss 0.0003 (0.0005)	
training:	Epoch: [68][56/408]	Loss 0.0004 (0.0005)	
training:	Epoch: [68][57/408]	Loss 0.0004 (0.0005)	
training:	Epoch: [68][58/408]	Loss 0.0006 (0.0005)	
training:	Epoch: [68][59/408]	Loss 0.0005 (0.0005)	
training:	Epoch: [68][60/408]	Loss 0.0011 (0.0006)	
training:	Epoch: [68][61/408]	Loss 0.0008 (0.0006)	
training:	Epoch: [68][62/408]	Loss 0.0004 (0.0006)	
training:	Epoch: [68][63/408]	Loss 0.0004 (0.0006)	
training:	Epoch: [68][64/408]	Loss 0.0004 (0.0005)	
training:	Epoch: [68][65/408]	Loss 0.0004 (0.0005)	
training:	Epoch: [68][66/408]	Loss 0.0004 (0.0005)	
training:	Epoch: [68][67/408]	Loss 0.0004 (0.0005)	
training:	Epoch: [68][68/408]	Loss 0.0004 (0.0005)	
training:	Epoch: [68][69/408]	Loss 0.0004 (0.0005)	
training:	Epoch: [68][70/408]	Loss 0.0003 (0.0005)	
training:	Epoch: [68][71/408]	Loss 0.0004 (0.0005)	
training:	Epoch: [68][72/408]	Loss 0.0004 (0.0005)	
training:	Epoch: [68][73/408]	Loss 0.0005 (0.0005)	
training:	Epoch: [68][74/408]	Loss 0.0004 (0.0005)	
training:	Epoch: [68][75/408]	Loss 0.0004 (0.0005)	
training:	Epoch: [68][76/408]	Loss 0.0004 (0.0005)	
training:	Epoch: [68][77/408]	Loss 0.0004 (0.0005)	
training:	Epoch: [68][78/408]	Loss 0.0004 (0.0005)	
training:	Epoch: [68][79/408]	Loss 0.0004 (0.0005)	
training:	Epoch: [68][80/408]	Loss 0.0012 (0.0005)	
training:	Epoch: [68][81/408]	Loss 0.0003 (0.0005)	
training:	Epoch: [68][82/408]	Loss 0.0004 (0.0005)	
training:	Epoch: [68][83/408]	Loss 0.0021 (0.0005)	
training:	Epoch: [68][84/408]	Loss 0.0004 (0.0005)	
training:	Epoch: [68][85/408]	Loss 0.0004 (0.0005)	
training:	Epoch: [68][86/408]	Loss 0.0004 (0.0005)	
training:	Epoch: [68][87/408]	Loss 0.0003 (0.0005)	
training:	Epoch: [68][88/408]	Loss 0.0004 (0.0005)	
training:	Epoch: [68][89/408]	Loss 0.0013 (0.0005)	
training:	Epoch: [68][90/408]	Loss 0.0004 (0.0005)	
training:	Epoch: [68][91/408]	Loss 0.0004 (0.0005)	
training:	Epoch: [68][92/408]	Loss 0.0004 (0.0005)	
training:	Epoch: [68][93/408]	Loss 0.0004 (0.0005)	
training:	Epoch: [68][94/408]	Loss 0.0008 (0.0005)	
training:	Epoch: [68][95/408]	Loss 0.0003 (0.0005)	
training:	Epoch: [68][96/408]	Loss 0.0006 (0.0005)	
training:	Epoch: [68][97/408]	Loss 0.0004 (0.0005)	
training:	Epoch: [68][98/408]	Loss 0.0005 (0.0005)	
training:	Epoch: [68][99/408]	Loss 0.0004 (0.0005)	
training:	Epoch: [68][100/408]	Loss 0.0003 (0.0005)	
training:	Epoch: [68][101/408]	Loss 0.0004 (0.0005)	
training:	Epoch: [68][102/408]	Loss 0.0005 (0.0005)	
training:	Epoch: [68][103/408]	Loss 0.0004 (0.0005)	
training:	Epoch: [68][104/408]	Loss 0.0004 (0.0005)	
training:	Epoch: [68][105/408]	Loss 0.0004 (0.0005)	
training:	Epoch: [68][106/408]	Loss 0.0026 (0.0005)	
training:	Epoch: [68][107/408]	Loss 0.0003 (0.0005)	
training:	Epoch: [68][108/408]	Loss 0.0004 (0.0005)	
training:	Epoch: [68][109/408]	Loss 0.0004 (0.0005)	
training:	Epoch: [68][110/408]	Loss 0.0004 (0.0005)	
training:	Epoch: [68][111/408]	Loss 0.0004 (0.0005)	
training:	Epoch: [68][112/408]	Loss 0.0004 (0.0005)	
training:	Epoch: [68][113/408]	Loss 0.0005 (0.0005)	
training:	Epoch: [68][114/408]	Loss 0.0004 (0.0005)	
training:	Epoch: [68][115/408]	Loss 0.0004 (0.0005)	
training:	Epoch: [68][116/408]	Loss 0.0004 (0.0005)	
training:	Epoch: [68][117/408]	Loss 0.0004 (0.0005)	
training:	Epoch: [68][118/408]	Loss 0.0004 (0.0005)	
training:	Epoch: [68][119/408]	Loss 0.0010 (0.0005)	
training:	Epoch: [68][120/408]	Loss 0.0003 (0.0005)	
training:	Epoch: [68][121/408]	Loss 0.0004 (0.0005)	
training:	Epoch: [68][122/408]	Loss 0.0004 (0.0005)	
training:	Epoch: [68][123/408]	Loss 0.0004 (0.0005)	
training:	Epoch: [68][124/408]	Loss 0.0004 (0.0005)	
training:	Epoch: [68][125/408]	Loss 0.0004 (0.0005)	
training:	Epoch: [68][126/408]	Loss 0.0003 (0.0005)	
training:	Epoch: [68][127/408]	Loss 0.0005 (0.0005)	
training:	Epoch: [68][128/408]	Loss 0.0003 (0.0005)	
training:	Epoch: [68][129/408]	Loss 0.0003 (0.0005)	
training:	Epoch: [68][130/408]	Loss 0.0004 (0.0005)	
training:	Epoch: [68][131/408]	Loss 0.0003 (0.0005)	
training:	Epoch: [68][132/408]	Loss 0.0009 (0.0005)	
training:	Epoch: [68][133/408]	Loss 0.0004 (0.0005)	
training:	Epoch: [68][134/408]	Loss 0.0004 (0.0005)	
training:	Epoch: [68][135/408]	Loss 0.0003 (0.0005)	
training:	Epoch: [68][136/408]	Loss 0.0003 (0.0005)	
training:	Epoch: [68][137/408]	Loss 0.0003 (0.0005)	
training:	Epoch: [68][138/408]	Loss 0.0003 (0.0005)	
training:	Epoch: [68][139/408]	Loss 0.0003 (0.0005)	
training:	Epoch: [68][140/408]	Loss 0.0004 (0.0005)	
training:	Epoch: [68][141/408]	Loss 0.0003 (0.0005)	
training:	Epoch: [68][142/408]	Loss 0.0004 (0.0005)	
training:	Epoch: [68][143/408]	Loss 0.0006 (0.0005)	
training:	Epoch: [68][144/408]	Loss 0.0004 (0.0005)	
training:	Epoch: [68][145/408]	Loss 0.0003 (0.0005)	
training:	Epoch: [68][146/408]	Loss 0.0005 (0.0005)	
training:	Epoch: [68][147/408]	Loss 0.0004 (0.0005)	
training:	Epoch: [68][148/408]	Loss 0.0003 (0.0005)	
training:	Epoch: [68][149/408]	Loss 0.0003 (0.0005)	
training:	Epoch: [68][150/408]	Loss 0.0004 (0.0005)	
training:	Epoch: [68][151/408]	Loss 0.0004 (0.0005)	
training:	Epoch: [68][152/408]	Loss 0.0003 (0.0005)	
training:	Epoch: [68][153/408]	Loss 0.0003 (0.0005)	
training:	Epoch: [68][154/408]	Loss 0.0004 (0.0005)	
training:	Epoch: [68][155/408]	Loss 0.0004 (0.0005)	
training:	Epoch: [68][156/408]	Loss 0.0004 (0.0005)	
training:	Epoch: [68][157/408]	Loss 0.0008 (0.0005)	
training:	Epoch: [68][158/408]	Loss 0.0005 (0.0005)	
training:	Epoch: [68][159/408]	Loss 0.0003 (0.0005)	
training:	Epoch: [68][160/408]	Loss 0.0005 (0.0005)	
training:	Epoch: [68][161/408]	Loss 0.0003 (0.0005)	
training:	Epoch: [68][162/408]	Loss 0.0004 (0.0005)	
training:	Epoch: [68][163/408]	Loss 0.0004 (0.0005)	
training:	Epoch: [68][164/408]	Loss 0.0005 (0.0005)	
training:	Epoch: [68][165/408]	Loss 0.0003 (0.0005)	
training:	Epoch: [68][166/408]	Loss 0.0004 (0.0005)	
training:	Epoch: [68][167/408]	Loss 0.0008 (0.0005)	
training:	Epoch: [68][168/408]	Loss 0.0004 (0.0005)	
training:	Epoch: [68][169/408]	Loss 0.0004 (0.0005)	
training:	Epoch: [68][170/408]	Loss 0.0004 (0.0005)	
training:	Epoch: [68][171/408]	Loss 0.0005 (0.0005)	
training:	Epoch: [68][172/408]	Loss 0.0005 (0.0005)	
training:	Epoch: [68][173/408]	Loss 0.0005 (0.0005)	
training:	Epoch: [68][174/408]	Loss 0.0006 (0.0005)	
training:	Epoch: [68][175/408]	Loss 0.0005 (0.0005)	
training:	Epoch: [68][176/408]	Loss 0.0003 (0.0005)	
training:	Epoch: [68][177/408]	Loss 0.0004 (0.0005)	
training:	Epoch: [68][178/408]	Loss 0.0003 (0.0005)	
training:	Epoch: [68][179/408]	Loss 0.0073 (0.0005)	
training:	Epoch: [68][180/408]	Loss 0.0004 (0.0005)	
training:	Epoch: [68][181/408]	Loss 0.0003 (0.0005)	
training:	Epoch: [68][182/408]	Loss 0.0004 (0.0005)	
training:	Epoch: [68][183/408]	Loss 0.0003 (0.0005)	
training:	Epoch: [68][184/408]	Loss 0.0003 (0.0005)	
training:	Epoch: [68][185/408]	Loss 0.0012 (0.0005)	
training:	Epoch: [68][186/408]	Loss 0.0003 (0.0005)	
training:	Epoch: [68][187/408]	Loss 0.0003 (0.0005)	
training:	Epoch: [68][188/408]	Loss 0.0004 (0.0005)	
training:	Epoch: [68][189/408]	Loss 0.0004 (0.0005)	
training:	Epoch: [68][190/408]	Loss 0.0010 (0.0005)	
training:	Epoch: [68][191/408]	Loss 0.0004 (0.0005)	
training:	Epoch: [68][192/408]	Loss 0.0004 (0.0005)	
training:	Epoch: [68][193/408]	Loss 0.0003 (0.0005)	
training:	Epoch: [68][194/408]	Loss 0.0005 (0.0005)	
training:	Epoch: [68][195/408]	Loss 0.0004 (0.0005)	
training:	Epoch: [68][196/408]	Loss 0.0009 (0.0005)	
training:	Epoch: [68][197/408]	Loss 0.0004 (0.0005)	
training:	Epoch: [68][198/408]	Loss 0.0005 (0.0005)	
training:	Epoch: [68][199/408]	Loss 0.0003 (0.0005)	
training:	Epoch: [68][200/408]	Loss 0.0003 (0.0005)	
training:	Epoch: [68][201/408]	Loss 0.0003 (0.0005)	
training:	Epoch: [68][202/408]	Loss 0.0003 (0.0005)	
training:	Epoch: [68][203/408]	Loss 0.0004 (0.0005)	
training:	Epoch: [68][204/408]	Loss 0.0003 (0.0005)	
training:	Epoch: [68][205/408]	Loss 0.0004 (0.0005)	
training:	Epoch: [68][206/408]	Loss 0.0004 (0.0005)	
training:	Epoch: [68][207/408]	Loss 0.0003 (0.0005)	
training:	Epoch: [68][208/408]	Loss 0.0011 (0.0005)	
training:	Epoch: [68][209/408]	Loss 0.0004 (0.0005)	
training:	Epoch: [68][210/408]	Loss 0.0003 (0.0005)	
training:	Epoch: [68][211/408]	Loss 0.0024 (0.0005)	
training:	Epoch: [68][212/408]	Loss 0.0003 (0.0005)	
training:	Epoch: [68][213/408]	Loss 0.0004 (0.0005)	
training:	Epoch: [68][214/408]	Loss 0.0003 (0.0005)	
training:	Epoch: [68][215/408]	Loss 0.0005 (0.0005)	
training:	Epoch: [68][216/408]	Loss 0.0004 (0.0005)	
training:	Epoch: [68][217/408]	Loss 0.0006 (0.0005)	
training:	Epoch: [68][218/408]	Loss 0.0004 (0.0005)	
training:	Epoch: [68][219/408]	Loss 0.0004 (0.0005)	
training:	Epoch: [68][220/408]	Loss 0.0009 (0.0005)	
training:	Epoch: [68][221/408]	Loss 0.0019 (0.0005)	
training:	Epoch: [68][222/408]	Loss 0.0004 (0.0005)	
training:	Epoch: [68][223/408]	Loss 0.0004 (0.0005)	
training:	Epoch: [68][224/408]	Loss 0.0004 (0.0005)	
training:	Epoch: [68][225/408]	Loss 0.0005 (0.0005)	
training:	Epoch: [68][226/408]	Loss 0.0003 (0.0005)	
training:	Epoch: [68][227/408]	Loss 0.0004 (0.0005)	
training:	Epoch: [68][228/408]	Loss 0.0005 (0.0005)	
training:	Epoch: [68][229/408]	Loss 0.0004 (0.0005)	
training:	Epoch: [68][230/408]	Loss 0.0004 (0.0005)	
training:	Epoch: [68][231/408]	Loss 0.0004 (0.0005)	
training:	Epoch: [68][232/408]	Loss 0.0004 (0.0005)	
training:	Epoch: [68][233/408]	Loss 0.0003 (0.0005)	
training:	Epoch: [68][234/408]	Loss 0.0004 (0.0005)	
training:	Epoch: [68][235/408]	Loss 0.0004 (0.0005)	
training:	Epoch: [68][236/408]	Loss 0.0003 (0.0005)	
training:	Epoch: [68][237/408]	Loss 0.0005 (0.0005)	
training:	Epoch: [68][238/408]	Loss 0.0003 (0.0005)	
training:	Epoch: [68][239/408]	Loss 0.0005 (0.0005)	
training:	Epoch: [68][240/408]	Loss 0.0003 (0.0005)	
training:	Epoch: [68][241/408]	Loss 0.0003 (0.0005)	
training:	Epoch: [68][242/408]	Loss 0.0003 (0.0005)	
training:	Epoch: [68][243/408]	Loss 0.0003 (0.0005)	
training:	Epoch: [68][244/408]	Loss 0.0004 (0.0005)	
training:	Epoch: [68][245/408]	Loss 0.0004 (0.0005)	
training:	Epoch: [68][246/408]	Loss 0.0003 (0.0005)	
training:	Epoch: [68][247/408]	Loss 0.0003 (0.0005)	
training:	Epoch: [68][248/408]	Loss 0.0004 (0.0005)	
training:	Epoch: [68][249/408]	Loss 0.0003 (0.0005)	
training:	Epoch: [68][250/408]	Loss 0.0003 (0.0005)	
training:	Epoch: [68][251/408]	Loss 0.0003 (0.0005)	
training:	Epoch: [68][252/408]	Loss 0.0003 (0.0005)	
training:	Epoch: [68][253/408]	Loss 0.0004 (0.0005)	
training:	Epoch: [68][254/408]	Loss 0.0012 (0.0005)	
training:	Epoch: [68][255/408]	Loss 0.0004 (0.0005)	
training:	Epoch: [68][256/408]	Loss 0.0003 (0.0005)	
training:	Epoch: [68][257/408]	Loss 0.0004 (0.0005)	
training:	Epoch: [68][258/408]	Loss 0.0003 (0.0005)	
training:	Epoch: [68][259/408]	Loss 0.0006 (0.0005)	
training:	Epoch: [68][260/408]	Loss 0.0003 (0.0005)	
training:	Epoch: [68][261/408]	Loss 0.0004 (0.0005)	
training:	Epoch: [68][262/408]	Loss 0.0004 (0.0005)	
training:	Epoch: [68][263/408]	Loss 0.0004 (0.0005)	
training:	Epoch: [68][264/408]	Loss 0.0003 (0.0005)	
training:	Epoch: [68][265/408]	Loss 0.0003 (0.0005)	
training:	Epoch: [68][266/408]	Loss 0.0003 (0.0005)	
training:	Epoch: [68][267/408]	Loss 0.0004 (0.0005)	
training:	Epoch: [68][268/408]	Loss 0.0004 (0.0005)	
training:	Epoch: [68][269/408]	Loss 0.0004 (0.0005)	
training:	Epoch: [68][270/408]	Loss 0.0004 (0.0005)	
training:	Epoch: [68][271/408]	Loss 0.0003 (0.0005)	
training:	Epoch: [68][272/408]	Loss 0.0003 (0.0005)	
training:	Epoch: [68][273/408]	Loss 0.0004 (0.0005)	
training:	Epoch: [68][274/408]	Loss 0.0003 (0.0005)	
training:	Epoch: [68][275/408]	Loss 0.0004 (0.0005)	
training:	Epoch: [68][276/408]	Loss 0.0011 (0.0005)	
training:	Epoch: [68][277/408]	Loss 0.0003 (0.0005)	
training:	Epoch: [68][278/408]	Loss 0.0013 (0.0005)	
training:	Epoch: [68][279/408]	Loss 0.0007 (0.0005)	
training:	Epoch: [68][280/408]	Loss 0.0004 (0.0005)	
training:	Epoch: [68][281/408]	Loss 0.0004 (0.0005)	
training:	Epoch: [68][282/408]	Loss 0.0003 (0.0005)	
training:	Epoch: [68][283/408]	Loss 0.0003 (0.0005)	
training:	Epoch: [68][284/408]	Loss 0.0004 (0.0005)	
training:	Epoch: [68][285/408]	Loss 0.0011 (0.0005)	
training:	Epoch: [68][286/408]	Loss 0.0004 (0.0005)	
training:	Epoch: [68][287/408]	Loss 0.0003 (0.0005)	
training:	Epoch: [68][288/408]	Loss 0.0003 (0.0005)	
training:	Epoch: [68][289/408]	Loss 0.0003 (0.0005)	
training:	Epoch: [68][290/408]	Loss 0.0008 (0.0005)	
training:	Epoch: [68][291/408]	Loss 0.0003 (0.0005)	
training:	Epoch: [68][292/408]	Loss 0.0022 (0.0005)	
training:	Epoch: [68][293/408]	Loss 0.0015 (0.0005)	
training:	Epoch: [68][294/408]	Loss 0.0003 (0.0005)	
training:	Epoch: [68][295/408]	Loss 0.0003 (0.0005)	
training:	Epoch: [68][296/408]	Loss 0.0003 (0.0005)	
training:	Epoch: [68][297/408]	Loss 0.0003 (0.0005)	
training:	Epoch: [68][298/408]	Loss 0.0004 (0.0005)	
training:	Epoch: [68][299/408]	Loss 0.0003 (0.0005)	
training:	Epoch: [68][300/408]	Loss 0.0003 (0.0005)	
training:	Epoch: [68][301/408]	Loss 0.0004 (0.0005)	
training:	Epoch: [68][302/408]	Loss 0.0004 (0.0005)	
training:	Epoch: [68][303/408]	Loss 0.0007 (0.0005)	
training:	Epoch: [68][304/408]	Loss 0.0015 (0.0005)	
training:	Epoch: [68][305/408]	Loss 0.0003 (0.0005)	
training:	Epoch: [68][306/408]	Loss 0.0003 (0.0005)	
training:	Epoch: [68][307/408]	Loss 0.0003 (0.0005)	
training:	Epoch: [68][308/408]	Loss 0.0004 (0.0005)	
training:	Epoch: [68][309/408]	Loss 0.0003 (0.0005)	
training:	Epoch: [68][310/408]	Loss 0.0004 (0.0005)	
training:	Epoch: [68][311/408]	Loss 0.0004 (0.0005)	
training:	Epoch: [68][312/408]	Loss 0.0004 (0.0005)	
training:	Epoch: [68][313/408]	Loss 0.0003 (0.0005)	
training:	Epoch: [68][314/408]	Loss 0.0003 (0.0005)	
training:	Epoch: [68][315/408]	Loss 0.0003 (0.0005)	
training:	Epoch: [68][316/408]	Loss 0.0003 (0.0005)	
training:	Epoch: [68][317/408]	Loss 0.0003 (0.0005)	
training:	Epoch: [68][318/408]	Loss 0.0003 (0.0005)	
training:	Epoch: [68][319/408]	Loss 0.0003 (0.0005)	
training:	Epoch: [68][320/408]	Loss 0.0005 (0.0005)	
training:	Epoch: [68][321/408]	Loss 0.0003 (0.0005)	
training:	Epoch: [68][322/408]	Loss 0.0003 (0.0005)	
training:	Epoch: [68][323/408]	Loss 0.0004 (0.0005)	
training:	Epoch: [68][324/408]	Loss 0.0004 (0.0005)	
training:	Epoch: [68][325/408]	Loss 0.0004 (0.0005)	
training:	Epoch: [68][326/408]	Loss 0.0003 (0.0005)	
training:	Epoch: [68][327/408]	Loss 0.0003 (0.0005)	
training:	Epoch: [68][328/408]	Loss 0.0008 (0.0005)	
training:	Epoch: [68][329/408]	Loss 0.0003 (0.0005)	
training:	Epoch: [68][330/408]	Loss 0.0003 (0.0005)	
training:	Epoch: [68][331/408]	Loss 0.0004 (0.0005)	
training:	Epoch: [68][332/408]	Loss 0.0005 (0.0005)	
training:	Epoch: [68][333/408]	Loss 0.0004 (0.0005)	
training:	Epoch: [68][334/408]	Loss 0.0003 (0.0005)	
training:	Epoch: [68][335/408]	Loss 0.0003 (0.0005)	
training:	Epoch: [68][336/408]	Loss 0.0005 (0.0005)	
training:	Epoch: [68][337/408]	Loss 0.0003 (0.0005)	
training:	Epoch: [68][338/408]	Loss 0.0004 (0.0005)	
training:	Epoch: [68][339/408]	Loss 0.0004 (0.0005)	
training:	Epoch: [68][340/408]	Loss 0.0004 (0.0005)	
training:	Epoch: [68][341/408]	Loss 0.0003 (0.0005)	
training:	Epoch: [68][342/408]	Loss 0.0005 (0.0005)	
training:	Epoch: [68][343/408]	Loss 0.0003 (0.0005)	
training:	Epoch: [68][344/408]	Loss 0.0003 (0.0005)	
training:	Epoch: [68][345/408]	Loss 0.0004 (0.0005)	
training:	Epoch: [68][346/408]	Loss 0.0005 (0.0005)	
training:	Epoch: [68][347/408]	Loss 0.0004 (0.0005)	
training:	Epoch: [68][348/408]	Loss 0.0004 (0.0005)	
training:	Epoch: [68][349/408]	Loss 0.0004 (0.0005)	
training:	Epoch: [68][350/408]	Loss 0.0003 (0.0005)	
training:	Epoch: [68][351/408]	Loss 0.0003 (0.0005)	
training:	Epoch: [68][352/408]	Loss 0.0003 (0.0005)	
training:	Epoch: [68][353/408]	Loss 0.0003 (0.0005)	
training:	Epoch: [68][354/408]	Loss 0.0004 (0.0005)	
training:	Epoch: [68][355/408]	Loss 0.0004 (0.0005)	
training:	Epoch: [68][356/408]	Loss 0.0004 (0.0005)	
training:	Epoch: [68][357/408]	Loss 0.0003 (0.0005)	
training:	Epoch: [68][358/408]	Loss 0.0003 (0.0005)	
training:	Epoch: [68][359/408]	Loss 0.0004 (0.0005)	
training:	Epoch: [68][360/408]	Loss 0.0015 (0.0005)	
training:	Epoch: [68][361/408]	Loss 0.0003 (0.0005)	
training:	Epoch: [68][362/408]	Loss 0.0004 (0.0005)	
training:	Epoch: [68][363/408]	Loss 0.0005 (0.0005)	
training:	Epoch: [68][364/408]	Loss 0.0003 (0.0005)	
training:	Epoch: [68][365/408]	Loss 0.0133 (0.0005)	
training:	Epoch: [68][366/408]	Loss 0.0005 (0.0005)	
training:	Epoch: [68][367/408]	Loss 0.0004 (0.0005)	
training:	Epoch: [68][368/408]	Loss 0.0005 (0.0005)	
training:	Epoch: [68][369/408]	Loss 0.0004 (0.0005)	
training:	Epoch: [68][370/408]	Loss 0.0003 (0.0005)	
training:	Epoch: [68][371/408]	Loss 0.0017 (0.0005)	
training:	Epoch: [68][372/408]	Loss 0.0005 (0.0005)	
training:	Epoch: [68][373/408]	Loss 0.0003 (0.0005)	
training:	Epoch: [68][374/408]	Loss 0.0004 (0.0005)	
training:	Epoch: [68][375/408]	Loss 0.0003 (0.0005)	
training:	Epoch: [68][376/408]	Loss 0.0003 (0.0005)	
training:	Epoch: [68][377/408]	Loss 0.0003 (0.0005)	
training:	Epoch: [68][378/408]	Loss 0.0011 (0.0005)	
training:	Epoch: [68][379/408]	Loss 0.0011 (0.0005)	
training:	Epoch: [68][380/408]	Loss 0.0003 (0.0005)	
training:	Epoch: [68][381/408]	Loss 0.0051 (0.0005)	
training:	Epoch: [68][382/408]	Loss 0.0003 (0.0005)	
training:	Epoch: [68][383/408]	Loss 0.0004 (0.0005)	
training:	Epoch: [68][384/408]	Loss 0.0003 (0.0005)	
training:	Epoch: [68][385/408]	Loss 0.0003 (0.0005)	
training:	Epoch: [68][386/408]	Loss 0.0009 (0.0005)	
training:	Epoch: [68][387/408]	Loss 0.0004 (0.0005)	
training:	Epoch: [68][388/408]	Loss 0.0005 (0.0005)	
training:	Epoch: [68][389/408]	Loss 0.0003 (0.0005)	
training:	Epoch: [68][390/408]	Loss 0.0008 (0.0005)	
training:	Epoch: [68][391/408]	Loss 0.0003 (0.0005)	
training:	Epoch: [68][392/408]	Loss 0.0003 (0.0005)	
training:	Epoch: [68][393/408]	Loss 0.0004 (0.0005)	
training:	Epoch: [68][394/408]	Loss 0.0004 (0.0005)	
training:	Epoch: [68][395/408]	Loss 0.0004 (0.0005)	
training:	Epoch: [68][396/408]	Loss 0.0004 (0.0005)	
training:	Epoch: [68][397/408]	Loss 0.0004 (0.0005)	
training:	Epoch: [68][398/408]	Loss 0.0003 (0.0005)	
training:	Epoch: [68][399/408]	Loss 0.0004 (0.0005)	
training:	Epoch: [68][400/408]	Loss 0.0009 (0.0005)	
training:	Epoch: [68][401/408]	Loss 0.0010 (0.0005)	
training:	Epoch: [68][402/408]	Loss 0.0003 (0.0005)	
training:	Epoch: [68][403/408]	Loss 0.0011 (0.0005)	
training:	Epoch: [68][404/408]	Loss 0.0004 (0.0005)	
training:	Epoch: [68][405/408]	Loss 0.0003 (0.0005)	
training:	Epoch: [68][406/408]	Loss 0.0003 (0.0005)	
training:	Epoch: [68][407/408]	Loss 0.0005 (0.0005)	
training:	Epoch: [68][408/408]	Loss 0.0003 (0.0005)	
Training:	 Loss: 0.0005

Training:	 ACC: 1.0000 1.0000 1.0000 1.0000
Validation:	 ACC: 0.7805 0.7812 0.7943 0.7668
Validation:	 Best_BACC: 0.8226 0.8250 0.8772 0.7679
Validation:	 Loss: 1.3880
Pretraining:	Epoch 69/200
----------
training:	Epoch: [69][1/408]	Loss 0.0003 (0.0003)	
training:	Epoch: [69][2/408]	Loss 0.0003 (0.0003)	
training:	Epoch: [69][3/408]	Loss 0.0004 (0.0003)	
training:	Epoch: [69][4/408]	Loss 0.0004 (0.0003)	
training:	Epoch: [69][5/408]	Loss 0.0003 (0.0003)	
training:	Epoch: [69][6/408]	Loss 0.0004 (0.0004)	
training:	Epoch: [69][7/408]	Loss 0.0004 (0.0004)	
training:	Epoch: [69][8/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [69][9/408]	Loss 0.0078 (0.0012)	
training:	Epoch: [69][10/408]	Loss 0.0004 (0.0011)	
training:	Epoch: [69][11/408]	Loss 0.0003 (0.0010)	
training:	Epoch: [69][12/408]	Loss 0.0006 (0.0010)	
training:	Epoch: [69][13/408]	Loss 0.0003 (0.0009)	
training:	Epoch: [69][14/408]	Loss 0.0010 (0.0009)	
training:	Epoch: [69][15/408]	Loss 0.0003 (0.0009)	
training:	Epoch: [69][16/408]	Loss 0.0003 (0.0009)	
training:	Epoch: [69][17/408]	Loss 0.0003 (0.0008)	
training:	Epoch: [69][18/408]	Loss 0.0006 (0.0008)	
training:	Epoch: [69][19/408]	Loss 0.0003 (0.0008)	
training:	Epoch: [69][20/408]	Loss 0.0005 (0.0008)	
training:	Epoch: [69][21/408]	Loss 0.0004 (0.0008)	
training:	Epoch: [69][22/408]	Loss 0.0003 (0.0007)	
training:	Epoch: [69][23/408]	Loss 0.0004 (0.0007)	
training:	Epoch: [69][24/408]	Loss 0.0003 (0.0007)	
training:	Epoch: [69][25/408]	Loss 0.0004 (0.0007)	
training:	Epoch: [69][26/408]	Loss 0.0003 (0.0007)	
training:	Epoch: [69][27/408]	Loss 0.0011 (0.0007)	
training:	Epoch: [69][28/408]	Loss 0.0003 (0.0007)	
training:	Epoch: [69][29/408]	Loss 0.0005 (0.0007)	
training:	Epoch: [69][30/408]	Loss 0.0004 (0.0007)	
training:	Epoch: [69][31/408]	Loss 0.0004 (0.0007)	
training:	Epoch: [69][32/408]	Loss 0.0004 (0.0006)	
training:	Epoch: [69][33/408]	Loss 0.0003 (0.0006)	
training:	Epoch: [69][34/408]	Loss 0.0015 (0.0007)	
training:	Epoch: [69][35/408]	Loss 0.0004 (0.0007)	
training:	Epoch: [69][36/408]	Loss 0.0004 (0.0006)	
training:	Epoch: [69][37/408]	Loss 0.0010 (0.0007)	
training:	Epoch: [69][38/408]	Loss 0.0010 (0.0007)	
training:	Epoch: [69][39/408]	Loss 0.0004 (0.0007)	
training:	Epoch: [69][40/408]	Loss 0.0004 (0.0007)	
training:	Epoch: [69][41/408]	Loss 0.0003 (0.0006)	
training:	Epoch: [69][42/408]	Loss 0.0004 (0.0006)	
training:	Epoch: [69][43/408]	Loss 0.0004 (0.0006)	
training:	Epoch: [69][44/408]	Loss 0.0003 (0.0006)	
training:	Epoch: [69][45/408]	Loss 0.0003 (0.0006)	
training:	Epoch: [69][46/408]	Loss 0.0003 (0.0006)	
training:	Epoch: [69][47/408]	Loss 0.0004 (0.0006)	
training:	Epoch: [69][48/408]	Loss 0.0003 (0.0006)	
training:	Epoch: [69][49/408]	Loss 0.0006 (0.0006)	
training:	Epoch: [69][50/408]	Loss 0.0003 (0.0006)	
training:	Epoch: [69][51/408]	Loss 0.0003 (0.0006)	
training:	Epoch: [69][52/408]	Loss 0.0004 (0.0006)	
training:	Epoch: [69][53/408]	Loss 0.0003 (0.0006)	
training:	Epoch: [69][54/408]	Loss 0.0004 (0.0006)	
training:	Epoch: [69][55/408]	Loss 0.0004 (0.0006)	
training:	Epoch: [69][56/408]	Loss 0.0003 (0.0006)	
training:	Epoch: [69][57/408]	Loss 0.0004 (0.0006)	
training:	Epoch: [69][58/408]	Loss 0.0003 (0.0006)	
training:	Epoch: [69][59/408]	Loss 0.0003 (0.0006)	
training:	Epoch: [69][60/408]	Loss 0.0003 (0.0006)	
training:	Epoch: [69][61/408]	Loss 0.0006 (0.0006)	
training:	Epoch: [69][62/408]	Loss 0.0003 (0.0005)	
training:	Epoch: [69][63/408]	Loss 0.0003 (0.0005)	
training:	Epoch: [69][64/408]	Loss 0.0003 (0.0005)	
training:	Epoch: [69][65/408]	Loss 0.0003 (0.0005)	
training:	Epoch: [69][66/408]	Loss 0.0005 (0.0005)	
training:	Epoch: [69][67/408]	Loss 0.0004 (0.0005)	
training:	Epoch: [69][68/408]	Loss 0.0005 (0.0005)	
training:	Epoch: [69][69/408]	Loss 0.0015 (0.0005)	
training:	Epoch: [69][70/408]	Loss 0.0003 (0.0005)	
training:	Epoch: [69][71/408]	Loss 0.0004 (0.0005)	
training:	Epoch: [69][72/408]	Loss 0.0007 (0.0005)	
training:	Epoch: [69][73/408]	Loss 0.0003 (0.0005)	
training:	Epoch: [69][74/408]	Loss 0.0003 (0.0005)	
training:	Epoch: [69][75/408]	Loss 0.0003 (0.0005)	
training:	Epoch: [69][76/408]	Loss 0.0003 (0.0005)	
training:	Epoch: [69][77/408]	Loss 0.0003 (0.0005)	
training:	Epoch: [69][78/408]	Loss 0.0003 (0.0005)	
training:	Epoch: [69][79/408]	Loss 0.0008 (0.0005)	
training:	Epoch: [69][80/408]	Loss 0.0003 (0.0005)	
training:	Epoch: [69][81/408]	Loss 0.0003 (0.0005)	
training:	Epoch: [69][82/408]	Loss 0.0005 (0.0005)	
training:	Epoch: [69][83/408]	Loss 0.0003 (0.0005)	
training:	Epoch: [69][84/408]	Loss 0.0004 (0.0005)	
training:	Epoch: [69][85/408]	Loss 0.0003 (0.0005)	
training:	Epoch: [69][86/408]	Loss 0.0004 (0.0005)	
training:	Epoch: [69][87/408]	Loss 0.0003 (0.0005)	
training:	Epoch: [69][88/408]	Loss 0.0003 (0.0005)	
training:	Epoch: [69][89/408]	Loss 0.0016 (0.0005)	
training:	Epoch: [69][90/408]	Loss 0.0003 (0.0005)	
training:	Epoch: [69][91/408]	Loss 0.0003 (0.0005)	
training:	Epoch: [69][92/408]	Loss 0.0003 (0.0005)	
training:	Epoch: [69][93/408]	Loss 0.0003 (0.0005)	
training:	Epoch: [69][94/408]	Loss 0.0004 (0.0005)	
training:	Epoch: [69][95/408]	Loss 0.0004 (0.0005)	
training:	Epoch: [69][96/408]	Loss 0.0004 (0.0005)	
training:	Epoch: [69][97/408]	Loss 0.0003 (0.0005)	
training:	Epoch: [69][98/408]	Loss 0.0003 (0.0005)	
training:	Epoch: [69][99/408]	Loss 0.0003 (0.0005)	
training:	Epoch: [69][100/408]	Loss 0.0007 (0.0005)	
training:	Epoch: [69][101/408]	Loss 0.0003 (0.0005)	
training:	Epoch: [69][102/408]	Loss 0.0004 (0.0005)	
training:	Epoch: [69][103/408]	Loss 0.0003 (0.0005)	
training:	Epoch: [69][104/408]	Loss 0.0009 (0.0005)	
training:	Epoch: [69][105/408]	Loss 0.0004 (0.0005)	
training:	Epoch: [69][106/408]	Loss 0.0003 (0.0005)	
training:	Epoch: [69][107/408]	Loss 0.0003 (0.0005)	
training:	Epoch: [69][108/408]	Loss 0.0004 (0.0005)	
training:	Epoch: [69][109/408]	Loss 0.0004 (0.0005)	
training:	Epoch: [69][110/408]	Loss 0.0004 (0.0005)	
training:	Epoch: [69][111/408]	Loss 0.0004 (0.0005)	
training:	Epoch: [69][112/408]	Loss 0.0003 (0.0005)	
training:	Epoch: [69][113/408]	Loss 0.0003 (0.0005)	
training:	Epoch: [69][114/408]	Loss 0.0004 (0.0005)	
training:	Epoch: [69][115/408]	Loss 0.0008 (0.0005)	
training:	Epoch: [69][116/408]	Loss 0.0011 (0.0005)	
training:	Epoch: [69][117/408]	Loss 0.0003 (0.0005)	
training:	Epoch: [69][118/408]	Loss 0.0003 (0.0005)	
training:	Epoch: [69][119/408]	Loss 0.0014 (0.0005)	
training:	Epoch: [69][120/408]	Loss 0.0003 (0.0005)	
training:	Epoch: [69][121/408]	Loss 0.0003 (0.0005)	
training:	Epoch: [69][122/408]	Loss 0.0003 (0.0005)	
training:	Epoch: [69][123/408]	Loss 0.0004 (0.0005)	
training:	Epoch: [69][124/408]	Loss 0.0003 (0.0005)	
training:	Epoch: [69][125/408]	Loss 0.0004 (0.0005)	
training:	Epoch: [69][126/408]	Loss 0.0003 (0.0005)	
training:	Epoch: [69][127/408]	Loss 0.0009 (0.0005)	
training:	Epoch: [69][128/408]	Loss 0.0003 (0.0005)	
training:	Epoch: [69][129/408]	Loss 0.0006 (0.0005)	
training:	Epoch: [69][130/408]	Loss 0.0004 (0.0005)	
training:	Epoch: [69][131/408]	Loss 0.0006 (0.0005)	
training:	Epoch: [69][132/408]	Loss 0.0003 (0.0005)	
training:	Epoch: [69][133/408]	Loss 0.0003 (0.0005)	
training:	Epoch: [69][134/408]	Loss 0.0003 (0.0005)	
training:	Epoch: [69][135/408]	Loss 0.0003 (0.0005)	
training:	Epoch: [69][136/408]	Loss 0.0003 (0.0005)	
training:	Epoch: [69][137/408]	Loss 0.0003 (0.0005)	
training:	Epoch: [69][138/408]	Loss 0.0004 (0.0005)	
training:	Epoch: [69][139/408]	Loss 0.0003 (0.0005)	
training:	Epoch: [69][140/408]	Loss 0.0004 (0.0005)	
training:	Epoch: [69][141/408]	Loss 0.0004 (0.0005)	
training:	Epoch: [69][142/408]	Loss 0.0003 (0.0005)	
training:	Epoch: [69][143/408]	Loss 0.0003 (0.0005)	
training:	Epoch: [69][144/408]	Loss 0.0004 (0.0005)	
training:	Epoch: [69][145/408]	Loss 0.0003 (0.0005)	
training:	Epoch: [69][146/408]	Loss 0.0004 (0.0005)	
training:	Epoch: [69][147/408]	Loss 0.0003 (0.0005)	
training:	Epoch: [69][148/408]	Loss 0.0009 (0.0005)	
training:	Epoch: [69][149/408]	Loss 0.0004 (0.0005)	
training:	Epoch: [69][150/408]	Loss 0.0003 (0.0005)	
training:	Epoch: [69][151/408]	Loss 0.0003 (0.0005)	
training:	Epoch: [69][152/408]	Loss 0.0003 (0.0005)	
training:	Epoch: [69][153/408]	Loss 0.0003 (0.0005)	
training:	Epoch: [69][154/408]	Loss 0.0004 (0.0005)	
training:	Epoch: [69][155/408]	Loss 0.0003 (0.0005)	
training:	Epoch: [69][156/408]	Loss 0.0009 (0.0005)	
training:	Epoch: [69][157/408]	Loss 0.0003 (0.0005)	
training:	Epoch: [69][158/408]	Loss 0.0004 (0.0005)	
training:	Epoch: [69][159/408]	Loss 0.0003 (0.0005)	
training:	Epoch: [69][160/408]	Loss 0.0003 (0.0005)	
training:	Epoch: [69][161/408]	Loss 0.0004 (0.0005)	
training:	Epoch: [69][162/408]	Loss 0.0004 (0.0005)	
training:	Epoch: [69][163/408]	Loss 0.0003 (0.0005)	
training:	Epoch: [69][164/408]	Loss 0.0003 (0.0005)	
training:	Epoch: [69][165/408]	Loss 0.0003 (0.0005)	
training:	Epoch: [69][166/408]	Loss 0.0003 (0.0005)	
training:	Epoch: [69][167/408]	Loss 0.0003 (0.0005)	
training:	Epoch: [69][168/408]	Loss 0.0003 (0.0005)	
training:	Epoch: [69][169/408]	Loss 0.0003 (0.0005)	
training:	Epoch: [69][170/408]	Loss 0.0003 (0.0005)	
training:	Epoch: [69][171/408]	Loss 0.0004 (0.0005)	
training:	Epoch: [69][172/408]	Loss 0.0003 (0.0005)	
training:	Epoch: [69][173/408]	Loss 0.0003 (0.0005)	
training:	Epoch: [69][174/408]	Loss 0.0003 (0.0005)	
training:	Epoch: [69][175/408]	Loss 0.0003 (0.0005)	
training:	Epoch: [69][176/408]	Loss 0.0003 (0.0005)	
training:	Epoch: [69][177/408]	Loss 0.0004 (0.0005)	
training:	Epoch: [69][178/408]	Loss 0.0003 (0.0005)	
training:	Epoch: [69][179/408]	Loss 0.0003 (0.0005)	
training:	Epoch: [69][180/408]	Loss 0.0004 (0.0005)	
training:	Epoch: [69][181/408]	Loss 0.0008 (0.0005)	
training:	Epoch: [69][182/408]	Loss 0.0004 (0.0005)	
training:	Epoch: [69][183/408]	Loss 0.0004 (0.0005)	
training:	Epoch: [69][184/408]	Loss 0.0004 (0.0005)	
training:	Epoch: [69][185/408]	Loss 0.0008 (0.0005)	
training:	Epoch: [69][186/408]	Loss 0.0003 (0.0005)	
training:	Epoch: [69][187/408]	Loss 0.0003 (0.0005)	
training:	Epoch: [69][188/408]	Loss 0.0003 (0.0005)	
training:	Epoch: [69][189/408]	Loss 0.0003 (0.0005)	
training:	Epoch: [69][190/408]	Loss 0.0004 (0.0005)	
training:	Epoch: [69][191/408]	Loss 0.0009 (0.0005)	
training:	Epoch: [69][192/408]	Loss 0.0003 (0.0005)	
training:	Epoch: [69][193/408]	Loss 0.0003 (0.0005)	
training:	Epoch: [69][194/408]	Loss 0.0003 (0.0005)	
training:	Epoch: [69][195/408]	Loss 0.0003 (0.0005)	
training:	Epoch: [69][196/408]	Loss 0.0003 (0.0005)	
training:	Epoch: [69][197/408]	Loss 0.0004 (0.0005)	
training:	Epoch: [69][198/408]	Loss 0.0003 (0.0005)	
training:	Epoch: [69][199/408]	Loss 0.0003 (0.0005)	
training:	Epoch: [69][200/408]	Loss 0.0003 (0.0005)	
training:	Epoch: [69][201/408]	Loss 0.0004 (0.0005)	
training:	Epoch: [69][202/408]	Loss 0.0004 (0.0005)	
training:	Epoch: [69][203/408]	Loss 0.0003 (0.0005)	
training:	Epoch: [69][204/408]	Loss 0.0003 (0.0005)	
training:	Epoch: [69][205/408]	Loss 0.0004 (0.0005)	
training:	Epoch: [69][206/408]	Loss 0.0004 (0.0005)	
training:	Epoch: [69][207/408]	Loss 0.0003 (0.0005)	
training:	Epoch: [69][208/408]	Loss 0.0003 (0.0005)	
training:	Epoch: [69][209/408]	Loss 0.0003 (0.0005)	
training:	Epoch: [69][210/408]	Loss 0.0009 (0.0005)	
training:	Epoch: [69][211/408]	Loss 0.0005 (0.0005)	
training:	Epoch: [69][212/408]	Loss 0.0004 (0.0005)	
training:	Epoch: [69][213/408]	Loss 0.0003 (0.0005)	
training:	Epoch: [69][214/408]	Loss 0.0003 (0.0005)	
training:	Epoch: [69][215/408]	Loss 0.0003 (0.0005)	
training:	Epoch: [69][216/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [69][217/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [69][218/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [69][219/408]	Loss 0.0011 (0.0005)	
training:	Epoch: [69][220/408]	Loss 0.0005 (0.0005)	
training:	Epoch: [69][221/408]	Loss 0.0003 (0.0005)	
training:	Epoch: [69][222/408]	Loss 0.0003 (0.0005)	
training:	Epoch: [69][223/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [69][224/408]	Loss 0.0010 (0.0005)	
training:	Epoch: [69][225/408]	Loss 0.0003 (0.0005)	
training:	Epoch: [69][226/408]	Loss 0.0003 (0.0005)	
training:	Epoch: [69][227/408]	Loss 0.0003 (0.0005)	
training:	Epoch: [69][228/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [69][229/408]	Loss 0.0004 (0.0004)	
training:	Epoch: [69][230/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [69][231/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [69][232/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [69][233/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [69][234/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [69][235/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [69][236/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [69][237/408]	Loss 0.0004 (0.0004)	
training:	Epoch: [69][238/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [69][239/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [69][240/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [69][241/408]	Loss 0.0004 (0.0004)	
training:	Epoch: [69][242/408]	Loss 0.0005 (0.0004)	
training:	Epoch: [69][243/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [69][244/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [69][245/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [69][246/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [69][247/408]	Loss 0.0004 (0.0004)	
training:	Epoch: [69][248/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [69][249/408]	Loss 0.0009 (0.0004)	
training:	Epoch: [69][250/408]	Loss 0.0004 (0.0004)	
training:	Epoch: [69][251/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [69][252/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [69][253/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [69][254/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [69][255/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [69][256/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [69][257/408]	Loss 0.0004 (0.0004)	
training:	Epoch: [69][258/408]	Loss 0.0005 (0.0004)	
training:	Epoch: [69][259/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [69][260/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [69][261/408]	Loss 0.0012 (0.0004)	
training:	Epoch: [69][262/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [69][263/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [69][264/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [69][265/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [69][266/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [69][267/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [69][268/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [69][269/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [69][270/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [69][271/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [69][272/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [69][273/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [69][274/408]	Loss 0.0004 (0.0004)	
training:	Epoch: [69][275/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [69][276/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [69][277/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [69][278/408]	Loss 0.0004 (0.0004)	
training:	Epoch: [69][279/408]	Loss 0.0005 (0.0004)	
training:	Epoch: [69][280/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [69][281/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [69][282/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [69][283/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [69][284/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [69][285/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [69][286/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [69][287/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [69][288/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [69][289/408]	Loss 0.0004 (0.0004)	
training:	Epoch: [69][290/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [69][291/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [69][292/408]	Loss 0.0006 (0.0004)	
training:	Epoch: [69][293/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [69][294/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [69][295/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [69][296/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [69][297/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [69][298/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [69][299/408]	Loss 0.0004 (0.0004)	
training:	Epoch: [69][300/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [69][301/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [69][302/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [69][303/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [69][304/408]	Loss 0.0004 (0.0004)	
training:	Epoch: [69][305/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [69][306/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [69][307/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [69][308/408]	Loss 0.0004 (0.0004)	
training:	Epoch: [69][309/408]	Loss 0.0004 (0.0004)	
training:	Epoch: [69][310/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [69][311/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [69][312/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [69][313/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [69][314/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [69][315/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [69][316/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [69][317/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [69][318/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [69][319/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [69][320/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [69][321/408]	Loss 0.0007 (0.0004)	
training:	Epoch: [69][322/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [69][323/408]	Loss 0.0008 (0.0004)	
training:	Epoch: [69][324/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [69][325/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [69][326/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [69][327/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [69][328/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [69][329/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [69][330/408]	Loss 0.0012 (0.0004)	
training:	Epoch: [69][331/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [69][332/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [69][333/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [69][334/408]	Loss 0.0013 (0.0004)	
training:	Epoch: [69][335/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [69][336/408]	Loss 0.0004 (0.0004)	
training:	Epoch: [69][337/408]	Loss 0.0004 (0.0004)	
training:	Epoch: [69][338/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [69][339/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [69][340/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [69][341/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [69][342/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [69][343/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [69][344/408]	Loss 0.0005 (0.0004)	
training:	Epoch: [69][345/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [69][346/408]	Loss 0.0004 (0.0004)	
training:	Epoch: [69][347/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [69][348/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [69][349/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [69][350/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [69][351/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [69][352/408]	Loss 0.0014 (0.0004)	
training:	Epoch: [69][353/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [69][354/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [69][355/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [69][356/408]	Loss 0.0004 (0.0004)	
training:	Epoch: [69][357/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [69][358/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [69][359/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [69][360/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [69][361/408]	Loss 0.0004 (0.0004)	
training:	Epoch: [69][362/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [69][363/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [69][364/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [69][365/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [69][366/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [69][367/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [69][368/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [69][369/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [69][370/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [69][371/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [69][372/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [69][373/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [69][374/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [69][375/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [69][376/408]	Loss 0.0004 (0.0004)	
training:	Epoch: [69][377/408]	Loss 0.0009 (0.0004)	
training:	Epoch: [69][378/408]	Loss 0.0005 (0.0004)	
training:	Epoch: [69][379/408]	Loss 0.0005 (0.0004)	
training:	Epoch: [69][380/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [69][381/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [69][382/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [69][383/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [69][384/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [69][385/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [69][386/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [69][387/408]	Loss 0.0004 (0.0004)	
training:	Epoch: [69][388/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [69][389/408]	Loss 0.0008 (0.0004)	
training:	Epoch: [69][390/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [69][391/408]	Loss 0.0004 (0.0004)	
training:	Epoch: [69][392/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [69][393/408]	Loss 0.0010 (0.0004)	
training:	Epoch: [69][394/408]	Loss 0.0011 (0.0004)	
training:	Epoch: [69][395/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [69][396/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [69][397/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [69][398/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [69][399/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [69][400/408]	Loss 0.0011 (0.0004)	
training:	Epoch: [69][401/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [69][402/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [69][403/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [69][404/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [69][405/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [69][406/408]	Loss 0.0010 (0.0004)	
training:	Epoch: [69][407/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [69][408/408]	Loss 0.0003 (0.0004)	
Training:	 Loss: 0.0004

Training:	 ACC: 1.0000 1.0000 1.0000 1.0000
Validation:	 ACC: 0.7821 0.7828 0.7973 0.7668
Validation:	 Best_BACC: 0.8226 0.8250 0.8772 0.7679
Validation:	 Loss: 1.4174
Pretraining:	Epoch 70/200
----------
training:	Epoch: [70][1/408]	Loss 0.0003 (0.0003)	
training:	Epoch: [70][2/408]	Loss 0.0003 (0.0003)	
training:	Epoch: [70][3/408]	Loss 0.0003 (0.0003)	
training:	Epoch: [70][4/408]	Loss 0.0003 (0.0003)	
training:	Epoch: [70][5/408]	Loss 0.0003 (0.0003)	
training:	Epoch: [70][6/408]	Loss 0.0003 (0.0003)	
training:	Epoch: [70][7/408]	Loss 0.0003 (0.0003)	
training:	Epoch: [70][8/408]	Loss 0.0003 (0.0003)	
training:	Epoch: [70][9/408]	Loss 0.0003 (0.0003)	
training:	Epoch: [70][10/408]	Loss 0.0003 (0.0003)	
training:	Epoch: [70][11/408]	Loss 0.0003 (0.0003)	
training:	Epoch: [70][12/408]	Loss 0.0003 (0.0003)	
training:	Epoch: [70][13/408]	Loss 0.0003 (0.0003)	
training:	Epoch: [70][14/408]	Loss 0.0003 (0.0003)	
training:	Epoch: [70][15/408]	Loss 0.0004 (0.0003)	
training:	Epoch: [70][16/408]	Loss 0.0003 (0.0003)	
training:	Epoch: [70][17/408]	Loss 0.0003 (0.0003)	
training:	Epoch: [70][18/408]	Loss 0.0003 (0.0003)	
training:	Epoch: [70][19/408]	Loss 0.0003 (0.0003)	
training:	Epoch: [70][20/408]	Loss 0.0004 (0.0003)	
training:	Epoch: [70][21/408]	Loss 0.0003 (0.0003)	
training:	Epoch: [70][22/408]	Loss 0.0003 (0.0003)	
training:	Epoch: [70][23/408]	Loss 0.0003 (0.0003)	
training:	Epoch: [70][24/408]	Loss 0.0003 (0.0003)	
training:	Epoch: [70][25/408]	Loss 0.0003 (0.0003)	
training:	Epoch: [70][26/408]	Loss 0.0009 (0.0003)	
training:	Epoch: [70][27/408]	Loss 0.0003 (0.0003)	
training:	Epoch: [70][28/408]	Loss 0.0003 (0.0003)	
training:	Epoch: [70][29/408]	Loss 0.0008 (0.0003)	
training:	Epoch: [70][30/408]	Loss 0.0003 (0.0003)	
training:	Epoch: [70][31/408]	Loss 0.0003 (0.0003)	
training:	Epoch: [70][32/408]	Loss 0.0003 (0.0003)	
training:	Epoch: [70][33/408]	Loss 0.0003 (0.0003)	
training:	Epoch: [70][34/408]	Loss 0.0003 (0.0003)	
training:	Epoch: [70][35/408]	Loss 0.0003 (0.0003)	
training:	Epoch: [70][36/408]	Loss 0.0003 (0.0003)	
training:	Epoch: [70][37/408]	Loss 0.0003 (0.0003)	
training:	Epoch: [70][38/408]	Loss 0.0004 (0.0003)	
training:	Epoch: [70][39/408]	Loss 0.0003 (0.0003)	
training:	Epoch: [70][40/408]	Loss 0.0004 (0.0003)	
training:	Epoch: [70][41/408]	Loss 0.0003 (0.0003)	
training:	Epoch: [70][42/408]	Loss 0.0003 (0.0003)	
training:	Epoch: [70][43/408]	Loss 0.0003 (0.0003)	
training:	Epoch: [70][44/408]	Loss 0.0003 (0.0003)	
training:	Epoch: [70][45/408]	Loss 0.0003 (0.0003)	
training:	Epoch: [70][46/408]	Loss 0.0003 (0.0003)	
training:	Epoch: [70][47/408]	Loss 0.0003 (0.0003)	
training:	Epoch: [70][48/408]	Loss 0.0003 (0.0003)	
training:	Epoch: [70][49/408]	Loss 0.0004 (0.0003)	
training:	Epoch: [70][50/408]	Loss 0.0004 (0.0003)	
training:	Epoch: [70][51/408]	Loss 0.0003 (0.0003)	
training:	Epoch: [70][52/408]	Loss 0.0003 (0.0003)	
training:	Epoch: [70][53/408]	Loss 0.0003 (0.0003)	
training:	Epoch: [70][54/408]	Loss 0.0003 (0.0003)	
training:	Epoch: [70][55/408]	Loss 0.0003 (0.0003)	
training:	Epoch: [70][56/408]	Loss 0.0007 (0.0003)	
training:	Epoch: [70][57/408]	Loss 0.0003 (0.0003)	
training:	Epoch: [70][58/408]	Loss 0.0003 (0.0003)	
training:	Epoch: [70][59/408]	Loss 0.0003 (0.0003)	
training:	Epoch: [70][60/408]	Loss 0.0003 (0.0003)	
training:	Epoch: [70][61/408]	Loss 0.0003 (0.0003)	
training:	Epoch: [70][62/408]	Loss 0.0003 (0.0003)	
training:	Epoch: [70][63/408]	Loss 0.0003 (0.0003)	
training:	Epoch: [70][64/408]	Loss 0.0003 (0.0003)	
training:	Epoch: [70][65/408]	Loss 0.0003 (0.0003)	
training:	Epoch: [70][66/408]	Loss 0.0003 (0.0003)	
training:	Epoch: [70][67/408]	Loss 0.0011 (0.0003)	
training:	Epoch: [70][68/408]	Loss 0.0003 (0.0003)	
training:	Epoch: [70][69/408]	Loss 0.0003 (0.0003)	
training:	Epoch: [70][70/408]	Loss 0.0003 (0.0003)	
training:	Epoch: [70][71/408]	Loss 0.0009 (0.0003)	
training:	Epoch: [70][72/408]	Loss 0.0003 (0.0003)	
training:	Epoch: [70][73/408]	Loss 0.0003 (0.0003)	
training:	Epoch: [70][74/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [70][75/408]	Loss 0.0003 (0.0003)	
training:	Epoch: [70][76/408]	Loss 0.0003 (0.0003)	
training:	Epoch: [70][77/408]	Loss 0.0003 (0.0003)	
training:	Epoch: [70][78/408]	Loss 0.0003 (0.0003)	
training:	Epoch: [70][79/408]	Loss 0.0003 (0.0003)	
training:	Epoch: [70][80/408]	Loss 0.0003 (0.0003)	
training:	Epoch: [70][81/408]	Loss 0.0003 (0.0003)	
training:	Epoch: [70][82/408]	Loss 0.0003 (0.0003)	
training:	Epoch: [70][83/408]	Loss 0.0003 (0.0003)	
training:	Epoch: [70][84/408]	Loss 0.0003 (0.0003)	
training:	Epoch: [70][85/408]	Loss 0.0003 (0.0003)	
training:	Epoch: [70][86/408]	Loss 0.0003 (0.0003)	
training:	Epoch: [70][87/408]	Loss 0.0003 (0.0003)	
training:	Epoch: [70][88/408]	Loss 0.0010 (0.0003)	
training:	Epoch: [70][89/408]	Loss 0.0003 (0.0003)	
training:	Epoch: [70][90/408]	Loss 0.0004 (0.0003)	
training:	Epoch: [70][91/408]	Loss 0.0003 (0.0003)	
training:	Epoch: [70][92/408]	Loss 0.0003 (0.0003)	
training:	Epoch: [70][93/408]	Loss 0.0003 (0.0003)	
training:	Epoch: [70][94/408]	Loss 0.0004 (0.0003)	
training:	Epoch: [70][95/408]	Loss 0.0003 (0.0003)	
training:	Epoch: [70][96/408]	Loss 0.0003 (0.0003)	
training:	Epoch: [70][97/408]	Loss 0.0003 (0.0003)	
training:	Epoch: [70][98/408]	Loss 0.0003 (0.0003)	
training:	Epoch: [70][99/408]	Loss 0.0003 (0.0003)	
training:	Epoch: [70][100/408]	Loss 0.0003 (0.0003)	
training:	Epoch: [70][101/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [70][102/408]	Loss 0.0004 (0.0003)	
training:	Epoch: [70][103/408]	Loss 0.0005 (0.0003)	
training:	Epoch: [70][104/408]	Loss 0.0003 (0.0003)	
training:	Epoch: [70][105/408]	Loss 0.0010 (0.0003)	
training:	Epoch: [70][106/408]	Loss 0.0003 (0.0003)	
training:	Epoch: [70][107/408]	Loss 0.0003 (0.0003)	
training:	Epoch: [70][108/408]	Loss 0.0007 (0.0003)	
training:	Epoch: [70][109/408]	Loss 0.0003 (0.0003)	
training:	Epoch: [70][110/408]	Loss 0.0003 (0.0003)	
training:	Epoch: [70][111/408]	Loss 0.0003 (0.0003)	
training:	Epoch: [70][112/408]	Loss 0.0007 (0.0003)	
training:	Epoch: [70][113/408]	Loss 0.0003 (0.0003)	
training:	Epoch: [70][114/408]	Loss 0.0003 (0.0003)	
training:	Epoch: [70][115/408]	Loss 0.0003 (0.0003)	
training:	Epoch: [70][116/408]	Loss 0.0004 (0.0003)	
training:	Epoch: [70][117/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [70][118/408]	Loss 0.0013 (0.0003)	
training:	Epoch: [70][119/408]	Loss 0.0003 (0.0003)	
training:	Epoch: [70][120/408]	Loss 0.0003 (0.0003)	
training:	Epoch: [70][121/408]	Loss 0.0003 (0.0003)	
training:	Epoch: [70][122/408]	Loss 0.0003 (0.0003)	
training:	Epoch: [70][123/408]	Loss 0.0004 (0.0003)	
training:	Epoch: [70][124/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [70][125/408]	Loss 0.0003 (0.0003)	
training:	Epoch: [70][126/408]	Loss 0.0003 (0.0003)	
training:	Epoch: [70][127/408]	Loss 0.0003 (0.0003)	
training:	Epoch: [70][128/408]	Loss 0.0003 (0.0003)	
training:	Epoch: [70][129/408]	Loss 0.0003 (0.0003)	
training:	Epoch: [70][130/408]	Loss 0.0003 (0.0003)	
training:	Epoch: [70][131/408]	Loss 0.0003 (0.0003)	
training:	Epoch: [70][132/408]	Loss 0.0003 (0.0003)	
training:	Epoch: [70][133/408]	Loss 0.0003 (0.0003)	
training:	Epoch: [70][134/408]	Loss 0.0015 (0.0003)	
training:	Epoch: [70][135/408]	Loss 0.0003 (0.0003)	
training:	Epoch: [70][136/408]	Loss 0.0003 (0.0003)	
training:	Epoch: [70][137/408]	Loss 0.0003 (0.0003)	
training:	Epoch: [70][138/408]	Loss 0.0003 (0.0003)	
training:	Epoch: [70][139/408]	Loss 0.0006 (0.0003)	
training:	Epoch: [70][140/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [70][141/408]	Loss 0.0003 (0.0003)	
training:	Epoch: [70][142/408]	Loss 0.0003 (0.0003)	
training:	Epoch: [70][143/408]	Loss 0.0003 (0.0003)	
training:	Epoch: [70][144/408]	Loss 0.0003 (0.0003)	
training:	Epoch: [70][145/408]	Loss 0.0003 (0.0003)	
training:	Epoch: [70][146/408]	Loss 0.0003 (0.0003)	
training:	Epoch: [70][147/408]	Loss 0.0003 (0.0003)	
training:	Epoch: [70][148/408]	Loss 0.0003 (0.0003)	
training:	Epoch: [70][149/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [70][150/408]	Loss 0.0003 (0.0003)	
training:	Epoch: [70][151/408]	Loss 0.0003 (0.0003)	
training:	Epoch: [70][152/408]	Loss 0.0003 (0.0003)	
training:	Epoch: [70][153/408]	Loss 0.0003 (0.0003)	
training:	Epoch: [70][154/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [70][155/408]	Loss 0.0003 (0.0003)	
training:	Epoch: [70][156/408]	Loss 0.0004 (0.0003)	
training:	Epoch: [70][157/408]	Loss 0.0003 (0.0003)	
training:	Epoch: [70][158/408]	Loss 0.0005 (0.0003)	
training:	Epoch: [70][159/408]	Loss 0.0009 (0.0003)	
training:	Epoch: [70][160/408]	Loss 0.0003 (0.0003)	
training:	Epoch: [70][161/408]	Loss 0.0003 (0.0003)	
training:	Epoch: [70][162/408]	Loss 0.0003 (0.0003)	
training:	Epoch: [70][163/408]	Loss 0.0003 (0.0003)	
training:	Epoch: [70][164/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [70][165/408]	Loss 0.0003 (0.0003)	
training:	Epoch: [70][166/408]	Loss 0.0003 (0.0003)	
training:	Epoch: [70][167/408]	Loss 0.0003 (0.0003)	
training:	Epoch: [70][168/408]	Loss 0.0003 (0.0003)	
training:	Epoch: [70][169/408]	Loss 0.0004 (0.0003)	
training:	Epoch: [70][170/408]	Loss 0.0003 (0.0003)	
training:	Epoch: [70][171/408]	Loss 0.0003 (0.0003)	
training:	Epoch: [70][172/408]	Loss 0.0003 (0.0003)	
training:	Epoch: [70][173/408]	Loss 0.0003 (0.0003)	
training:	Epoch: [70][174/408]	Loss 0.0003 (0.0003)	
training:	Epoch: [70][175/408]	Loss 0.0009 (0.0003)	
training:	Epoch: [70][176/408]	Loss 0.0003 (0.0003)	
training:	Epoch: [70][177/408]	Loss 0.0003 (0.0003)	
training:	Epoch: [70][178/408]	Loss 0.0003 (0.0003)	
training:	Epoch: [70][179/408]	Loss 0.0003 (0.0003)	
training:	Epoch: [70][180/408]	Loss 0.0003 (0.0003)	
training:	Epoch: [70][181/408]	Loss 0.0003 (0.0003)	
training:	Epoch: [70][182/408]	Loss 0.0095 (0.0004)	
training:	Epoch: [70][183/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [70][184/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [70][185/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [70][186/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [70][187/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [70][188/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [70][189/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [70][190/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [70][191/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [70][192/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [70][193/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [70][194/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [70][195/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [70][196/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [70][197/408]	Loss 0.0087 (0.0004)	
training:	Epoch: [70][198/408]	Loss 0.0008 (0.0004)	
training:	Epoch: [70][199/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [70][200/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [70][201/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [70][202/408]	Loss 0.0004 (0.0004)	
training:	Epoch: [70][203/408]	Loss 0.0004 (0.0004)	
training:	Epoch: [70][204/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [70][205/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [70][206/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [70][207/408]	Loss 0.0017 (0.0004)	
training:	Epoch: [70][208/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [70][209/408]	Loss 0.0004 (0.0004)	
training:	Epoch: [70][210/408]	Loss 0.0014 (0.0004)	
training:	Epoch: [70][211/408]	Loss 0.0010 (0.0004)	
training:	Epoch: [70][212/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [70][213/408]	Loss 0.0767 (0.0008)	
training:	Epoch: [70][214/408]	Loss 0.0003 (0.0008)	
training:	Epoch: [70][215/408]	Loss 0.0003 (0.0008)	
training:	Epoch: [70][216/408]	Loss 0.0005 (0.0008)	
training:	Epoch: [70][217/408]	Loss 0.0007 (0.0008)	
training:	Epoch: [70][218/408]	Loss 0.0003 (0.0008)	
training:	Epoch: [70][219/408]	Loss 0.0019 (0.0008)	
training:	Epoch: [70][220/408]	Loss 0.0005 (0.0008)	
training:	Epoch: [70][221/408]	Loss 0.0046 (0.0008)	
training:	Epoch: [70][222/408]	Loss 0.0192 (0.0009)	
training:	Epoch: [70][223/408]	Loss 0.4062 (0.0027)	
training:	Epoch: [70][224/408]	Loss 0.0011 (0.0027)	
training:	Epoch: [70][225/408]	Loss 0.0003 (0.0027)	
training:	Epoch: [70][226/408]	Loss 0.0008 (0.0027)	
training:	Epoch: [70][227/408]	Loss 0.0033 (0.0027)	
training:	Epoch: [70][228/408]	Loss 0.0012 (0.0027)	
training:	Epoch: [70][229/408]	Loss 0.0822 (0.0030)	
training:	Epoch: [70][230/408]	Loss 0.4522 (0.0050)	
training:	Epoch: [70][231/408]	Loss 0.0015 (0.0050)	
training:	Epoch: [70][232/408]	Loss 0.0005 (0.0049)	
training:	Epoch: [70][233/408]	Loss 0.2114 (0.0058)	
training:	Epoch: [70][234/408]	Loss 0.0003 (0.0058)	
training:	Epoch: [70][235/408]	Loss 0.0076 (0.0058)	
training:	Epoch: [70][236/408]	Loss 0.0075 (0.0058)	
training:	Epoch: [70][237/408]	Loss 0.0302 (0.0059)	
training:	Epoch: [70][238/408]	Loss 0.0003 (0.0059)	
training:	Epoch: [70][239/408]	Loss 0.0004 (0.0059)	
training:	Epoch: [70][240/408]	Loss 0.0003 (0.0058)	
training:	Epoch: [70][241/408]	Loss 0.0602 (0.0061)	
training:	Epoch: [70][242/408]	Loss 0.0073 (0.0061)	
training:	Epoch: [70][243/408]	Loss 0.0449 (0.0062)	
training:	Epoch: [70][244/408]	Loss 0.5247 (0.0084)	
training:	Epoch: [70][245/408]	Loss 0.0013 (0.0083)	
training:	Epoch: [70][246/408]	Loss 0.0003 (0.0083)	
training:	Epoch: [70][247/408]	Loss 0.0961 (0.0087)	
training:	Epoch: [70][248/408]	Loss 0.1989 (0.0094)	
training:	Epoch: [70][249/408]	Loss 0.3250 (0.0107)	
training:	Epoch: [70][250/408]	Loss 0.0006 (0.0107)	
training:	Epoch: [70][251/408]	Loss 0.0003 (0.0106)	
training:	Epoch: [70][252/408]	Loss 0.0041 (0.0106)	
training:	Epoch: [70][253/408]	Loss 0.0004 (0.0105)	
training:	Epoch: [70][254/408]	Loss 0.0097 (0.0105)	
training:	Epoch: [70][255/408]	Loss 0.0038 (0.0105)	
training:	Epoch: [70][256/408]	Loss 0.0005 (0.0105)	
training:	Epoch: [70][257/408]	Loss 0.0007 (0.0104)	
training:	Epoch: [70][258/408]	Loss 0.6693 (0.0130)	
training:	Epoch: [70][259/408]	Loss 0.0008 (0.0129)	
training:	Epoch: [70][260/408]	Loss 0.0361 (0.0130)	
training:	Epoch: [70][261/408]	Loss 0.1682 (0.0136)	
training:	Epoch: [70][262/408]	Loss 0.3027 (0.0147)	
training:	Epoch: [70][263/408]	Loss 0.0225 (0.0148)	
training:	Epoch: [70][264/408]	Loss 0.0006 (0.0147)	
training:	Epoch: [70][265/408]	Loss 0.1431 (0.0152)	
training:	Epoch: [70][266/408]	Loss 0.0003 (0.0151)	
training:	Epoch: [70][267/408]	Loss 0.0005 (0.0151)	
training:	Epoch: [70][268/408]	Loss 0.0029 (0.0150)	
training:	Epoch: [70][269/408]	Loss 0.3929 (0.0164)	
training:	Epoch: [70][270/408]	Loss 0.0117 (0.0164)	
training:	Epoch: [70][271/408]	Loss 0.6611 (0.0188)	
training:	Epoch: [70][272/408]	Loss 0.6646 (0.0212)	
training:	Epoch: [70][273/408]	Loss 0.3214 (0.0223)	
training:	Epoch: [70][274/408]	Loss 0.0129 (0.0222)	
training:	Epoch: [70][275/408]	Loss 0.1006 (0.0225)	
training:	Epoch: [70][276/408]	Loss 0.0003 (0.0224)	
training:	Epoch: [70][277/408]	Loss 0.0042 (0.0224)	
training:	Epoch: [70][278/408]	Loss 0.0034 (0.0223)	
training:	Epoch: [70][279/408]	Loss 0.0749 (0.0225)	
training:	Epoch: [70][280/408]	Loss 0.2471 (0.0233)	
training:	Epoch: [70][281/408]	Loss 0.2128 (0.0240)	
training:	Epoch: [70][282/408]	Loss 0.1987 (0.0246)	
training:	Epoch: [70][283/408]	Loss 0.4502 (0.0261)	
training:	Epoch: [70][284/408]	Loss 0.2930 (0.0270)	
training:	Epoch: [70][285/408]	Loss 0.1496 (0.0275)	
training:	Epoch: [70][286/408]	Loss 0.0004 (0.0274)	
training:	Epoch: [70][287/408]	Loss 0.0022 (0.0273)	
training:	Epoch: [70][288/408]	Loss 0.3512 (0.0284)	
training:	Epoch: [70][289/408]	Loss 0.0023 (0.0283)	
training:	Epoch: [70][290/408]	Loss 0.0073 (0.0282)	
training:	Epoch: [70][291/408]	Loss 0.0005 (0.0282)	
training:	Epoch: [70][292/408]	Loss 0.0028 (0.0281)	
training:	Epoch: [70][293/408]	Loss 0.0091 (0.0280)	
training:	Epoch: [70][294/408]	Loss 0.0009 (0.0279)	
training:	Epoch: [70][295/408]	Loss 0.0026 (0.0278)	
training:	Epoch: [70][296/408]	Loss 0.0029 (0.0277)	
training:	Epoch: [70][297/408]	Loss 0.0012 (0.0277)	
training:	Epoch: [70][298/408]	Loss 0.0012 (0.0276)	
training:	Epoch: [70][299/408]	Loss 0.0027 (0.0275)	
training:	Epoch: [70][300/408]	Loss 0.0196 (0.0275)	
training:	Epoch: [70][301/408]	Loss 0.0920 (0.0277)	
training:	Epoch: [70][302/408]	Loss 0.0758 (0.0278)	
training:	Epoch: [70][303/408]	Loss 0.0025 (0.0277)	
training:	Epoch: [70][304/408]	Loss 0.0075 (0.0277)	
training:	Epoch: [70][305/408]	Loss 0.0298 (0.0277)	
training:	Epoch: [70][306/408]	Loss 0.0027 (0.0276)	
training:	Epoch: [70][307/408]	Loss 0.0025 (0.0275)	
training:	Epoch: [70][308/408]	Loss 0.0920 (0.0277)	
training:	Epoch: [70][309/408]	Loss 0.0303 (0.0277)	
training:	Epoch: [70][310/408]	Loss 0.0023 (0.0277)	
training:	Epoch: [70][311/408]	Loss 0.0164 (0.0276)	
training:	Epoch: [70][312/408]	Loss 0.0088 (0.0276)	
training:	Epoch: [70][313/408]	Loss 0.0009 (0.0275)	
training:	Epoch: [70][314/408]	Loss 0.0015 (0.0274)	
training:	Epoch: [70][315/408]	Loss 0.0036 (0.0273)	
training:	Epoch: [70][316/408]	Loss 0.1492 (0.0277)	
training:	Epoch: [70][317/408]	Loss 0.0089 (0.0276)	
training:	Epoch: [70][318/408]	Loss 0.0016 (0.0276)	
training:	Epoch: [70][319/408]	Loss 0.0007 (0.0275)	
training:	Epoch: [70][320/408]	Loss 0.0688 (0.0276)	
training:	Epoch: [70][321/408]	Loss 0.0096 (0.0275)	
training:	Epoch: [70][322/408]	Loss 0.0006 (0.0275)	
training:	Epoch: [70][323/408]	Loss 0.0013 (0.0274)	
training:	Epoch: [70][324/408]	Loss 0.0122 (0.0273)	
training:	Epoch: [70][325/408]	Loss 0.0029 (0.0273)	
training:	Epoch: [70][326/408]	Loss 0.1468 (0.0276)	
training:	Epoch: [70][327/408]	Loss 0.0019 (0.0276)	
training:	Epoch: [70][328/408]	Loss 0.0458 (0.0276)	
training:	Epoch: [70][329/408]	Loss 0.0006 (0.0275)	
training:	Epoch: [70][330/408]	Loss 0.0189 (0.0275)	
training:	Epoch: [70][331/408]	Loss 0.0421 (0.0275)	
training:	Epoch: [70][332/408]	Loss 0.0009 (0.0275)	
training:	Epoch: [70][333/408]	Loss 0.0009 (0.0274)	
training:	Epoch: [70][334/408]	Loss 0.0024 (0.0273)	
training:	Epoch: [70][335/408]	Loss 0.1068 (0.0275)	
training:	Epoch: [70][336/408]	Loss 0.0030 (0.0275)	
training:	Epoch: [70][337/408]	Loss 0.0021 (0.0274)	
training:	Epoch: [70][338/408]	Loss 0.1542 (0.0278)	
training:	Epoch: [70][339/408]	Loss 0.0019 (0.0277)	
training:	Epoch: [70][340/408]	Loss 0.0010 (0.0276)	
training:	Epoch: [70][341/408]	Loss 0.0012 (0.0275)	
training:	Epoch: [70][342/408]	Loss 0.0079 (0.0275)	
training:	Epoch: [70][343/408]	Loss 0.0007 (0.0274)	
training:	Epoch: [70][344/408]	Loss 0.0011 (0.0273)	
training:	Epoch: [70][345/408]	Loss 0.0082 (0.0273)	
training:	Epoch: [70][346/408]	Loss 0.0096 (0.0272)	
training:	Epoch: [70][347/408]	Loss 0.0024 (0.0271)	
training:	Epoch: [70][348/408]	Loss 0.0056 (0.0271)	
training:	Epoch: [70][349/408]	Loss 0.0010 (0.0270)	
training:	Epoch: [70][350/408]	Loss 0.0098 (0.0270)	
training:	Epoch: [70][351/408]	Loss 0.0031 (0.0269)	
training:	Epoch: [70][352/408]	Loss 0.0086 (0.0268)	
training:	Epoch: [70][353/408]	Loss 0.0007 (0.0268)	
training:	Epoch: [70][354/408]	Loss 0.0008 (0.0267)	
training:	Epoch: [70][355/408]	Loss 0.0207 (0.0267)	
training:	Epoch: [70][356/408]	Loss 0.0040 (0.0266)	
training:	Epoch: [70][357/408]	Loss 0.0012 (0.0265)	
training:	Epoch: [70][358/408]	Loss 0.0008 (0.0265)	
training:	Epoch: [70][359/408]	Loss 0.0006 (0.0264)	
training:	Epoch: [70][360/408]	Loss 0.0748 (0.0265)	
training:	Epoch: [70][361/408]	Loss 0.0010 (0.0265)	
training:	Epoch: [70][362/408]	Loss 0.0008 (0.0264)	
training:	Epoch: [70][363/408]	Loss 0.0004 (0.0263)	
training:	Epoch: [70][364/408]	Loss 0.0232 (0.0263)	
training:	Epoch: [70][365/408]	Loss 0.0014 (0.0262)	
training:	Epoch: [70][366/408]	Loss 0.0216 (0.0262)	
training:	Epoch: [70][367/408]	Loss 0.0007 (0.0262)	
training:	Epoch: [70][368/408]	Loss 0.0014 (0.0261)	
training:	Epoch: [70][369/408]	Loss 0.0008 (0.0260)	
training:	Epoch: [70][370/408]	Loss 0.0023 (0.0260)	
training:	Epoch: [70][371/408]	Loss 0.0017 (0.0259)	
training:	Epoch: [70][372/408]	Loss 0.0019 (0.0258)	
training:	Epoch: [70][373/408]	Loss 0.0033 (0.0258)	
training:	Epoch: [70][374/408]	Loss 0.0015 (0.0257)	
training:	Epoch: [70][375/408]	Loss 0.0015 (0.0256)	
training:	Epoch: [70][376/408]	Loss 0.0005 (0.0256)	
training:	Epoch: [70][377/408]	Loss 0.0007 (0.0255)	
training:	Epoch: [70][378/408]	Loss 0.0038 (0.0255)	
training:	Epoch: [70][379/408]	Loss 0.0010 (0.0254)	
training:	Epoch: [70][380/408]	Loss 0.0012 (0.0253)	
training:	Epoch: [70][381/408]	Loss 0.0029 (0.0253)	
training:	Epoch: [70][382/408]	Loss 0.0180 (0.0252)	
training:	Epoch: [70][383/408]	Loss 0.0011 (0.0252)	
training:	Epoch: [70][384/408]	Loss 0.0012 (0.0251)	
training:	Epoch: [70][385/408]	Loss 0.0032 (0.0251)	
training:	Epoch: [70][386/408]	Loss 0.0009 (0.0250)	
training:	Epoch: [70][387/408]	Loss 0.0011 (0.0249)	
training:	Epoch: [70][388/408]	Loss 0.0007 (0.0249)	
training:	Epoch: [70][389/408]	Loss 0.0017 (0.0248)	
training:	Epoch: [70][390/408]	Loss 0.0015 (0.0248)	
training:	Epoch: [70][391/408]	Loss 0.0123 (0.0247)	
training:	Epoch: [70][392/408]	Loss 0.0009 (0.0247)	
training:	Epoch: [70][393/408]	Loss 0.0007 (0.0246)	
training:	Epoch: [70][394/408]	Loss 0.0008 (0.0245)	
training:	Epoch: [70][395/408]	Loss 0.0006 (0.0245)	
training:	Epoch: [70][396/408]	Loss 0.0013 (0.0244)	
training:	Epoch: [70][397/408]	Loss 0.0170 (0.0244)	
training:	Epoch: [70][398/408]	Loss 0.0006 (0.0243)	
training:	Epoch: [70][399/408]	Loss 0.0014 (0.0243)	
training:	Epoch: [70][400/408]	Loss 0.0005 (0.0242)	
training:	Epoch: [70][401/408]	Loss 0.0011 (0.0242)	
training:	Epoch: [70][402/408]	Loss 0.0009 (0.0241)	
training:	Epoch: [70][403/408]	Loss 0.0008 (0.0241)	
training:	Epoch: [70][404/408]	Loss 0.0011 (0.0240)	
training:	Epoch: [70][405/408]	Loss 0.0019 (0.0239)	
training:	Epoch: [70][406/408]	Loss 0.1652 (0.0243)	
training:	Epoch: [70][407/408]	Loss 0.0019 (0.0242)	
training:	Epoch: [70][408/408]	Loss 0.0009 (0.0242)	
Training:	 Loss: 0.0241

Training:	 ACC: 0.9988 0.9988 0.9982 0.9994
Validation:	 ACC: 0.7789 0.7790 0.7810 0.7769
Validation:	 Best_BACC: 0.8226 0.8250 0.8772 0.7679
Validation:	 Loss: 1.1230
Pretraining:	Epoch 71/200
----------
training:	Epoch: [71][1/408]	Loss 0.0544 (0.0544)	
training:	Epoch: [71][2/408]	Loss 0.0019 (0.0281)	
training:	Epoch: [71][3/408]	Loss 0.0041 (0.0201)	
training:	Epoch: [71][4/408]	Loss 0.2313 (0.0729)	
training:	Epoch: [71][5/408]	Loss 0.0050 (0.0593)	
training:	Epoch: [71][6/408]	Loss 0.0053 (0.0503)	
training:	Epoch: [71][7/408]	Loss 0.0013 (0.0433)	
training:	Epoch: [71][8/408]	Loss 0.0313 (0.0418)	
training:	Epoch: [71][9/408]	Loss 0.0009 (0.0373)	
training:	Epoch: [71][10/408]	Loss 0.0013 (0.0337)	
training:	Epoch: [71][11/408]	Loss 0.0949 (0.0392)	
training:	Epoch: [71][12/408]	Loss 0.0026 (0.0362)	
training:	Epoch: [71][13/408]	Loss 0.0036 (0.0337)	
training:	Epoch: [71][14/408]	Loss 0.0010 (0.0313)	
training:	Epoch: [71][15/408]	Loss 0.0010 (0.0293)	
training:	Epoch: [71][16/408]	Loss 0.0030 (0.0277)	
training:	Epoch: [71][17/408]	Loss 0.0010 (0.0261)	
training:	Epoch: [71][18/408]	Loss 0.0008 (0.0247)	
training:	Epoch: [71][19/408]	Loss 0.0011 (0.0235)	
training:	Epoch: [71][20/408]	Loss 0.0007 (0.0223)	
training:	Epoch: [71][21/408]	Loss 0.0006 (0.0213)	
training:	Epoch: [71][22/408]	Loss 0.0007 (0.0203)	
training:	Epoch: [71][23/408]	Loss 0.0007 (0.0195)	
training:	Epoch: [71][24/408]	Loss 0.0008 (0.0187)	
training:	Epoch: [71][25/408]	Loss 0.0017 (0.0180)	
training:	Epoch: [71][26/408]	Loss 0.0006 (0.0174)	
training:	Epoch: [71][27/408]	Loss 0.0014 (0.0168)	
training:	Epoch: [71][28/408]	Loss 0.0014 (0.0162)	
training:	Epoch: [71][29/408]	Loss 0.0024 (0.0157)	
training:	Epoch: [71][30/408]	Loss 0.0028 (0.0153)	
training:	Epoch: [71][31/408]	Loss 0.0015 (0.0149)	
training:	Epoch: [71][32/408]	Loss 0.0007 (0.0144)	
training:	Epoch: [71][33/408]	Loss 0.0007 (0.0140)	
training:	Epoch: [71][34/408]	Loss 0.0007 (0.0136)	
training:	Epoch: [71][35/408]	Loss 0.0015 (0.0133)	
training:	Epoch: [71][36/408]	Loss 0.0008 (0.0129)	
training:	Epoch: [71][37/408]	Loss 0.0052 (0.0127)	
training:	Epoch: [71][38/408]	Loss 0.0040 (0.0125)	
training:	Epoch: [71][39/408]	Loss 0.0066 (0.0123)	
training:	Epoch: [71][40/408]	Loss 0.0050 (0.0122)	
training:	Epoch: [71][41/408]	Loss 0.0007 (0.0119)	
training:	Epoch: [71][42/408]	Loss 0.0127 (0.0119)	
training:	Epoch: [71][43/408]	Loss 0.0006 (0.0116)	
training:	Epoch: [71][44/408]	Loss 0.0010 (0.0114)	
training:	Epoch: [71][45/408]	Loss 0.0018 (0.0112)	
training:	Epoch: [71][46/408]	Loss 0.0040 (0.0110)	
training:	Epoch: [71][47/408]	Loss 0.0012 (0.0108)	
training:	Epoch: [71][48/408]	Loss 0.0010 (0.0106)	
training:	Epoch: [71][49/408]	Loss 0.0030 (0.0104)	
training:	Epoch: [71][50/408]	Loss 0.0009 (0.0103)	
training:	Epoch: [71][51/408]	Loss 0.0008 (0.0101)	
training:	Epoch: [71][52/408]	Loss 0.0013 (0.0099)	
training:	Epoch: [71][53/408]	Loss 0.0008 (0.0097)	
training:	Epoch: [71][54/408]	Loss 0.0009 (0.0096)	
training:	Epoch: [71][55/408]	Loss 0.0006 (0.0094)	
training:	Epoch: [71][56/408]	Loss 0.0006 (0.0092)	
training:	Epoch: [71][57/408]	Loss 0.0008 (0.0091)	
training:	Epoch: [71][58/408]	Loss 0.0008 (0.0090)	
training:	Epoch: [71][59/408]	Loss 0.0006 (0.0088)	
training:	Epoch: [71][60/408]	Loss 0.0005 (0.0087)	
training:	Epoch: [71][61/408]	Loss 0.0009 (0.0086)	
training:	Epoch: [71][62/408]	Loss 0.0026 (0.0085)	
training:	Epoch: [71][63/408]	Loss 0.0008 (0.0083)	
training:	Epoch: [71][64/408]	Loss 0.0009 (0.0082)	
training:	Epoch: [71][65/408]	Loss 0.0029 (0.0081)	
training:	Epoch: [71][66/408]	Loss 0.0006 (0.0080)	
training:	Epoch: [71][67/408]	Loss 0.0007 (0.0079)	
training:	Epoch: [71][68/408]	Loss 0.0035 (0.0078)	
training:	Epoch: [71][69/408]	Loss 0.0008 (0.0077)	
training:	Epoch: [71][70/408]	Loss 0.0758 (0.0087)	
training:	Epoch: [71][71/408]	Loss 0.0006 (0.0086)	
training:	Epoch: [71][72/408]	Loss 0.0007 (0.0085)	
training:	Epoch: [71][73/408]	Loss 0.0162 (0.0086)	
training:	Epoch: [71][74/408]	Loss 0.0025 (0.0085)	
training:	Epoch: [71][75/408]	Loss 0.0099 (0.0085)	
training:	Epoch: [71][76/408]	Loss 0.0004 (0.0084)	
training:	Epoch: [71][77/408]	Loss 0.0006 (0.0083)	
training:	Epoch: [71][78/408]	Loss 0.0007 (0.0082)	
training:	Epoch: [71][79/408]	Loss 0.0009 (0.0081)	
training:	Epoch: [71][80/408]	Loss 0.0072 (0.0081)	
training:	Epoch: [71][81/408]	Loss 0.0005 (0.0080)	
training:	Epoch: [71][82/408]	Loss 0.0007 (0.0079)	
training:	Epoch: [71][83/408]	Loss 0.0006 (0.0079)	
training:	Epoch: [71][84/408]	Loss 0.0011 (0.0078)	
training:	Epoch: [71][85/408]	Loss 0.0013 (0.0077)	
training:	Epoch: [71][86/408]	Loss 0.0053 (0.0077)	
training:	Epoch: [71][87/408]	Loss 0.0018 (0.0076)	
training:	Epoch: [71][88/408]	Loss 0.0056 (0.0076)	
training:	Epoch: [71][89/408]	Loss 0.0029 (0.0075)	
training:	Epoch: [71][90/408]	Loss 0.0007 (0.0074)	
training:	Epoch: [71][91/408]	Loss 0.0345 (0.0077)	
training:	Epoch: [71][92/408]	Loss 0.0022 (0.0077)	
training:	Epoch: [71][93/408]	Loss 0.0010 (0.0076)	
training:	Epoch: [71][94/408]	Loss 0.0069 (0.0076)	
training:	Epoch: [71][95/408]	Loss 0.0133 (0.0077)	
training:	Epoch: [71][96/408]	Loss 0.0017 (0.0076)	
training:	Epoch: [71][97/408]	Loss 0.0005 (0.0075)	
training:	Epoch: [71][98/408]	Loss 0.0011 (0.0075)	
training:	Epoch: [71][99/408]	Loss 0.0008 (0.0074)	
training:	Epoch: [71][100/408]	Loss 0.0026 (0.0074)	
training:	Epoch: [71][101/408]	Loss 0.0008 (0.0073)	
training:	Epoch: [71][102/408]	Loss 0.0004 (0.0072)	
training:	Epoch: [71][103/408]	Loss 0.0007 (0.0072)	
training:	Epoch: [71][104/408]	Loss 0.0006 (0.0071)	
training:	Epoch: [71][105/408]	Loss 0.0006 (0.0070)	
training:	Epoch: [71][106/408]	Loss 0.0018 (0.0070)	
training:	Epoch: [71][107/408]	Loss 0.0026 (0.0069)	
training:	Epoch: [71][108/408]	Loss 0.0026 (0.0069)	
training:	Epoch: [71][109/408]	Loss 0.0005 (0.0068)	
training:	Epoch: [71][110/408]	Loss 0.0030 (0.0068)	
training:	Epoch: [71][111/408]	Loss 0.0007 (0.0068)	
training:	Epoch: [71][112/408]	Loss 0.0015 (0.0067)	
training:	Epoch: [71][113/408]	Loss 0.0011 (0.0067)	
training:	Epoch: [71][114/408]	Loss 0.0007 (0.0066)	
training:	Epoch: [71][115/408]	Loss 0.0008 (0.0066)	
training:	Epoch: [71][116/408]	Loss 0.0006 (0.0065)	
training:	Epoch: [71][117/408]	Loss 0.0035 (0.0065)	
training:	Epoch: [71][118/408]	Loss 0.0011 (0.0064)	
training:	Epoch: [71][119/408]	Loss 0.0007 (0.0064)	
training:	Epoch: [71][120/408]	Loss 0.0007 (0.0063)	
training:	Epoch: [71][121/408]	Loss 0.1801 (0.0078)	
training:	Epoch: [71][122/408]	Loss 0.0008 (0.0077)	
training:	Epoch: [71][123/408]	Loss 0.0004 (0.0077)	
training:	Epoch: [71][124/408]	Loss 0.0007 (0.0076)	
training:	Epoch: [71][125/408]	Loss 0.0019 (0.0076)	
training:	Epoch: [71][126/408]	Loss 0.0017 (0.0075)	
training:	Epoch: [71][127/408]	Loss 0.0005 (0.0075)	
training:	Epoch: [71][128/408]	Loss 0.0004 (0.0074)	
training:	Epoch: [71][129/408]	Loss 0.0007 (0.0073)	
training:	Epoch: [71][130/408]	Loss 0.0011 (0.0073)	
training:	Epoch: [71][131/408]	Loss 0.0004 (0.0072)	
training:	Epoch: [71][132/408]	Loss 0.0033 (0.0072)	
training:	Epoch: [71][133/408]	Loss 0.0009 (0.0072)	
training:	Epoch: [71][134/408]	Loss 0.0008 (0.0071)	
training:	Epoch: [71][135/408]	Loss 0.0012 (0.0071)	
training:	Epoch: [71][136/408]	Loss 0.0009 (0.0070)	
training:	Epoch: [71][137/408]	Loss 0.0006 (0.0070)	
training:	Epoch: [71][138/408]	Loss 0.0022 (0.0069)	
training:	Epoch: [71][139/408]	Loss 0.0004 (0.0069)	
training:	Epoch: [71][140/408]	Loss 0.0033 (0.0069)	
training:	Epoch: [71][141/408]	Loss 0.0009 (0.0068)	
training:	Epoch: [71][142/408]	Loss 0.0064 (0.0068)	
training:	Epoch: [71][143/408]	Loss 0.0025 (0.0068)	
training:	Epoch: [71][144/408]	Loss 0.0014 (0.0068)	
training:	Epoch: [71][145/408]	Loss 0.0011 (0.0067)	
training:	Epoch: [71][146/408]	Loss 0.0006 (0.0067)	
training:	Epoch: [71][147/408]	Loss 0.0011 (0.0066)	
training:	Epoch: [71][148/408]	Loss 0.0233 (0.0068)	
training:	Epoch: [71][149/408]	Loss 0.0007 (0.0067)	
training:	Epoch: [71][150/408]	Loss 0.0072 (0.0067)	
training:	Epoch: [71][151/408]	Loss 0.0005 (0.0067)	
training:	Epoch: [71][152/408]	Loss 0.0017 (0.0066)	
training:	Epoch: [71][153/408]	Loss 0.0013 (0.0066)	
training:	Epoch: [71][154/408]	Loss 0.0006 (0.0066)	
training:	Epoch: [71][155/408]	Loss 0.0007 (0.0065)	
training:	Epoch: [71][156/408]	Loss 0.1387 (0.0074)	
training:	Epoch: [71][157/408]	Loss 0.0030 (0.0074)	
training:	Epoch: [71][158/408]	Loss 0.0006 (0.0073)	
training:	Epoch: [71][159/408]	Loss 0.0007 (0.0073)	
training:	Epoch: [71][160/408]	Loss 0.0019 (0.0072)	
training:	Epoch: [71][161/408]	Loss 0.0066 (0.0072)	
training:	Epoch: [71][162/408]	Loss 0.0009 (0.0072)	
training:	Epoch: [71][163/408]	Loss 0.0007 (0.0072)	
training:	Epoch: [71][164/408]	Loss 0.0006 (0.0071)	
training:	Epoch: [71][165/408]	Loss 0.0009 (0.0071)	
training:	Epoch: [71][166/408]	Loss 0.0006 (0.0070)	
training:	Epoch: [71][167/408]	Loss 0.0013 (0.0070)	
training:	Epoch: [71][168/408]	Loss 0.0005 (0.0070)	
training:	Epoch: [71][169/408]	Loss 0.0104 (0.0070)	
training:	Epoch: [71][170/408]	Loss 0.0007 (0.0069)	
training:	Epoch: [71][171/408]	Loss 0.0011 (0.0069)	
training:	Epoch: [71][172/408]	Loss 0.0048 (0.0069)	
training:	Epoch: [71][173/408]	Loss 0.0009 (0.0069)	
training:	Epoch: [71][174/408]	Loss 0.0008 (0.0068)	
training:	Epoch: [71][175/408]	Loss 0.0007 (0.0068)	
training:	Epoch: [71][176/408]	Loss 0.0032 (0.0068)	
training:	Epoch: [71][177/408]	Loss 0.0004 (0.0067)	
training:	Epoch: [71][178/408]	Loss 0.0005 (0.0067)	
training:	Epoch: [71][179/408]	Loss 0.0067 (0.0067)	
training:	Epoch: [71][180/408]	Loss 0.0013 (0.0067)	
training:	Epoch: [71][181/408]	Loss 0.0014 (0.0066)	
training:	Epoch: [71][182/408]	Loss 0.0018 (0.0066)	
training:	Epoch: [71][183/408]	Loss 0.0012 (0.0066)	
training:	Epoch: [71][184/408]	Loss 0.0019 (0.0066)	
training:	Epoch: [71][185/408]	Loss 0.0196 (0.0066)	
training:	Epoch: [71][186/408]	Loss 0.0015 (0.0066)	
training:	Epoch: [71][187/408]	Loss 0.0014 (0.0066)	
training:	Epoch: [71][188/408]	Loss 0.0062 (0.0066)	
training:	Epoch: [71][189/408]	Loss 0.0005 (0.0065)	
training:	Epoch: [71][190/408]	Loss 0.0011 (0.0065)	
training:	Epoch: [71][191/408]	Loss 0.0038 (0.0065)	
training:	Epoch: [71][192/408]	Loss 0.0005 (0.0065)	
training:	Epoch: [71][193/408]	Loss 0.0011 (0.0064)	
training:	Epoch: [71][194/408]	Loss 0.0005 (0.0064)	
training:	Epoch: [71][195/408]	Loss 0.0006 (0.0064)	
training:	Epoch: [71][196/408]	Loss 0.0006 (0.0064)	
training:	Epoch: [71][197/408]	Loss 0.0004 (0.0063)	
training:	Epoch: [71][198/408]	Loss 0.0004 (0.0063)	
training:	Epoch: [71][199/408]	Loss 0.0005 (0.0063)	
training:	Epoch: [71][200/408]	Loss 0.0026 (0.0062)	
training:	Epoch: [71][201/408]	Loss 0.0025 (0.0062)	
training:	Epoch: [71][202/408]	Loss 0.0008 (0.0062)	
training:	Epoch: [71][203/408]	Loss 0.0007 (0.0062)	
training:	Epoch: [71][204/408]	Loss 0.0009 (0.0061)	
training:	Epoch: [71][205/408]	Loss 0.0008 (0.0061)	
training:	Epoch: [71][206/408]	Loss 0.0024 (0.0061)	
training:	Epoch: [71][207/408]	Loss 0.0055 (0.0061)	
training:	Epoch: [71][208/408]	Loss 0.0005 (0.0061)	
training:	Epoch: [71][209/408]	Loss 0.0008 (0.0060)	
training:	Epoch: [71][210/408]	Loss 0.0011 (0.0060)	
training:	Epoch: [71][211/408]	Loss 0.0010 (0.0060)	
training:	Epoch: [71][212/408]	Loss 0.0005 (0.0060)	
training:	Epoch: [71][213/408]	Loss 0.0011 (0.0059)	
training:	Epoch: [71][214/408]	Loss 0.0036 (0.0059)	
training:	Epoch: [71][215/408]	Loss 0.0005 (0.0059)	
training:	Epoch: [71][216/408]	Loss 0.0007 (0.0059)	
training:	Epoch: [71][217/408]	Loss 0.0005 (0.0059)	
training:	Epoch: [71][218/408]	Loss 0.0014 (0.0058)	
training:	Epoch: [71][219/408]	Loss 0.0012 (0.0058)	
training:	Epoch: [71][220/408]	Loss 0.0004 (0.0058)	
training:	Epoch: [71][221/408]	Loss 0.0011 (0.0058)	
training:	Epoch: [71][222/408]	Loss 0.0065 (0.0058)	
training:	Epoch: [71][223/408]	Loss 0.0008 (0.0058)	
training:	Epoch: [71][224/408]	Loss 0.0009 (0.0057)	
training:	Epoch: [71][225/408]	Loss 0.0005 (0.0057)	
training:	Epoch: [71][226/408]	Loss 0.0008 (0.0057)	
training:	Epoch: [71][227/408]	Loss 0.0011 (0.0057)	
training:	Epoch: [71][228/408]	Loss 0.0214 (0.0057)	
training:	Epoch: [71][229/408]	Loss 0.0006 (0.0057)	
training:	Epoch: [71][230/408]	Loss 0.0006 (0.0057)	
training:	Epoch: [71][231/408]	Loss 0.0015 (0.0057)	
training:	Epoch: [71][232/408]	Loss 0.0014 (0.0057)	
training:	Epoch: [71][233/408]	Loss 0.0007 (0.0056)	
training:	Epoch: [71][234/408]	Loss 0.0011 (0.0056)	
training:	Epoch: [71][235/408]	Loss 0.0008 (0.0056)	
training:	Epoch: [71][236/408]	Loss 0.0016 (0.0056)	
training:	Epoch: [71][237/408]	Loss 0.0008 (0.0056)	
training:	Epoch: [71][238/408]	Loss 0.0009 (0.0055)	
training:	Epoch: [71][239/408]	Loss 0.0005 (0.0055)	
training:	Epoch: [71][240/408]	Loss 0.0005 (0.0055)	
training:	Epoch: [71][241/408]	Loss 0.0010 (0.0055)	
training:	Epoch: [71][242/408]	Loss 0.0007 (0.0055)	
training:	Epoch: [71][243/408]	Loss 0.0005 (0.0054)	
training:	Epoch: [71][244/408]	Loss 0.0005 (0.0054)	
training:	Epoch: [71][245/408]	Loss 0.0012 (0.0054)	
training:	Epoch: [71][246/408]	Loss 0.0008 (0.0054)	
training:	Epoch: [71][247/408]	Loss 0.0006 (0.0054)	
training:	Epoch: [71][248/408]	Loss 0.0012 (0.0053)	
training:	Epoch: [71][249/408]	Loss 0.0011 (0.0053)	
training:	Epoch: [71][250/408]	Loss 0.0007 (0.0053)	
training:	Epoch: [71][251/408]	Loss 0.0005 (0.0053)	
training:	Epoch: [71][252/408]	Loss 0.0011 (0.0053)	
training:	Epoch: [71][253/408]	Loss 0.0010 (0.0053)	
training:	Epoch: [71][254/408]	Loss 0.0039 (0.0053)	
training:	Epoch: [71][255/408]	Loss 0.0012 (0.0052)	
training:	Epoch: [71][256/408]	Loss 0.0013 (0.0052)	
training:	Epoch: [71][257/408]	Loss 0.0007 (0.0052)	
training:	Epoch: [71][258/408]	Loss 0.0008 (0.0052)	
training:	Epoch: [71][259/408]	Loss 0.0266 (0.0053)	
training:	Epoch: [71][260/408]	Loss 0.0006 (0.0053)	
training:	Epoch: [71][261/408]	Loss 0.0969 (0.0056)	
training:	Epoch: [71][262/408]	Loss 0.0004 (0.0056)	
training:	Epoch: [71][263/408]	Loss 0.0007 (0.0056)	
training:	Epoch: [71][264/408]	Loss 0.0075 (0.0056)	
training:	Epoch: [71][265/408]	Loss 0.0021 (0.0056)	
training:	Epoch: [71][266/408]	Loss 0.0004 (0.0055)	
training:	Epoch: [71][267/408]	Loss 0.0010 (0.0055)	
training:	Epoch: [71][268/408]	Loss 0.0011 (0.0055)	
training:	Epoch: [71][269/408]	Loss 0.0092 (0.0055)	
training:	Epoch: [71][270/408]	Loss 0.0123 (0.0055)	
training:	Epoch: [71][271/408]	Loss 0.0006 (0.0055)	
training:	Epoch: [71][272/408]	Loss 0.0017 (0.0055)	
training:	Epoch: [71][273/408]	Loss 0.0009 (0.0055)	
training:	Epoch: [71][274/408]	Loss 0.1228 (0.0059)	
training:	Epoch: [71][275/408]	Loss 0.0013 (0.0059)	
training:	Epoch: [71][276/408]	Loss 0.0007 (0.0059)	
training:	Epoch: [71][277/408]	Loss 0.0029 (0.0059)	
training:	Epoch: [71][278/408]	Loss 0.0004 (0.0059)	
training:	Epoch: [71][279/408]	Loss 0.1218 (0.0063)	
training:	Epoch: [71][280/408]	Loss 0.0079 (0.0063)	
training:	Epoch: [71][281/408]	Loss 0.0010 (0.0063)	
training:	Epoch: [71][282/408]	Loss 0.0019 (0.0062)	
training:	Epoch: [71][283/408]	Loss 0.0011 (0.0062)	
training:	Epoch: [71][284/408]	Loss 0.0005 (0.0062)	
training:	Epoch: [71][285/408]	Loss 0.0006 (0.0062)	
training:	Epoch: [71][286/408]	Loss 0.0012 (0.0062)	
training:	Epoch: [71][287/408]	Loss 0.0005 (0.0061)	
training:	Epoch: [71][288/408]	Loss 0.0033 (0.0061)	
training:	Epoch: [71][289/408]	Loss 0.0008 (0.0061)	
training:	Epoch: [71][290/408]	Loss 0.0010 (0.0061)	
training:	Epoch: [71][291/408]	Loss 0.0008 (0.0061)	
training:	Epoch: [71][292/408]	Loss 0.0013 (0.0061)	
training:	Epoch: [71][293/408]	Loss 0.0056 (0.0061)	
training:	Epoch: [71][294/408]	Loss 0.0032 (0.0061)	
training:	Epoch: [71][295/408]	Loss 0.0019 (0.0060)	
training:	Epoch: [71][296/408]	Loss 0.0005 (0.0060)	
training:	Epoch: [71][297/408]	Loss 0.0006 (0.0060)	
training:	Epoch: [71][298/408]	Loss 0.0080 (0.0060)	
training:	Epoch: [71][299/408]	Loss 0.0045 (0.0060)	
training:	Epoch: [71][300/408]	Loss 0.0030 (0.0060)	
training:	Epoch: [71][301/408]	Loss 0.0004 (0.0060)	
training:	Epoch: [71][302/408]	Loss 0.0008 (0.0060)	
training:	Epoch: [71][303/408]	Loss 0.0019 (0.0059)	
training:	Epoch: [71][304/408]	Loss 0.0170 (0.0060)	
training:	Epoch: [71][305/408]	Loss 0.0035 (0.0060)	
training:	Epoch: [71][306/408]	Loss 0.0474 (0.0061)	
training:	Epoch: [71][307/408]	Loss 0.0004 (0.0061)	
training:	Epoch: [71][308/408]	Loss 0.0023 (0.0061)	
training:	Epoch: [71][309/408]	Loss 0.0014 (0.0061)	
training:	Epoch: [71][310/408]	Loss 0.0995 (0.0064)	
training:	Epoch: [71][311/408]	Loss 0.0005 (0.0063)	
training:	Epoch: [71][312/408]	Loss 0.0010 (0.0063)	
training:	Epoch: [71][313/408]	Loss 0.0007 (0.0063)	
training:	Epoch: [71][314/408]	Loss 0.0191 (0.0064)	
training:	Epoch: [71][315/408]	Loss 0.0265 (0.0064)	
training:	Epoch: [71][316/408]	Loss 0.0007 (0.0064)	
training:	Epoch: [71][317/408]	Loss 0.0050 (0.0064)	
training:	Epoch: [71][318/408]	Loss 0.0011 (0.0064)	
training:	Epoch: [71][319/408]	Loss 0.0020 (0.0064)	
training:	Epoch: [71][320/408]	Loss 0.0005 (0.0063)	
training:	Epoch: [71][321/408]	Loss 0.0006 (0.0063)	
training:	Epoch: [71][322/408]	Loss 0.0006 (0.0063)	
training:	Epoch: [71][323/408]	Loss 0.0004 (0.0063)	
training:	Epoch: [71][324/408]	Loss 0.0006 (0.0063)	
training:	Epoch: [71][325/408]	Loss 0.0050 (0.0063)	
training:	Epoch: [71][326/408]	Loss 0.0007 (0.0063)	
training:	Epoch: [71][327/408]	Loss 0.0008 (0.0062)	
training:	Epoch: [71][328/408]	Loss 0.0005 (0.0062)	
training:	Epoch: [71][329/408]	Loss 0.0005 (0.0062)	
training:	Epoch: [71][330/408]	Loss 0.0018 (0.0062)	
training:	Epoch: [71][331/408]	Loss 0.0008 (0.0062)	
training:	Epoch: [71][332/408]	Loss 0.0009 (0.0062)	
training:	Epoch: [71][333/408]	Loss 0.0005 (0.0061)	
training:	Epoch: [71][334/408]	Loss 0.0016 (0.0061)	
training:	Epoch: [71][335/408]	Loss 0.0011 (0.0061)	
training:	Epoch: [71][336/408]	Loss 0.0011 (0.0061)	
training:	Epoch: [71][337/408]	Loss 0.0201 (0.0061)	
training:	Epoch: [71][338/408]	Loss 0.0008 (0.0061)	
training:	Epoch: [71][339/408]	Loss 0.0008 (0.0061)	
training:	Epoch: [71][340/408]	Loss 0.0004 (0.0061)	
training:	Epoch: [71][341/408]	Loss 0.0004 (0.0061)	
training:	Epoch: [71][342/408]	Loss 0.0007 (0.0061)	
training:	Epoch: [71][343/408]	Loss 0.0053 (0.0061)	
training:	Epoch: [71][344/408]	Loss 0.1695 (0.0065)	
training:	Epoch: [71][345/408]	Loss 0.0006 (0.0065)	
training:	Epoch: [71][346/408]	Loss 0.0056 (0.0065)	
training:	Epoch: [71][347/408]	Loss 0.0020 (0.0065)	
training:	Epoch: [71][348/408]	Loss 0.0014 (0.0065)	
training:	Epoch: [71][349/408]	Loss 0.0004 (0.0065)	
training:	Epoch: [71][350/408]	Loss 0.0006 (0.0064)	
training:	Epoch: [71][351/408]	Loss 0.0004 (0.0064)	
training:	Epoch: [71][352/408]	Loss 0.0019 (0.0064)	
training:	Epoch: [71][353/408]	Loss 0.0005 (0.0064)	
training:	Epoch: [71][354/408]	Loss 0.0032 (0.0064)	
training:	Epoch: [71][355/408]	Loss 0.0017 (0.0064)	
training:	Epoch: [71][356/408]	Loss 0.0074 (0.0064)	
training:	Epoch: [71][357/408]	Loss 0.0009 (0.0064)	
training:	Epoch: [71][358/408]	Loss 0.0272 (0.0064)	
training:	Epoch: [71][359/408]	Loss 0.0009 (0.0064)	
training:	Epoch: [71][360/408]	Loss 0.0007 (0.0064)	
training:	Epoch: [71][361/408]	Loss 0.0005 (0.0064)	
training:	Epoch: [71][362/408]	Loss 0.0006 (0.0064)	
training:	Epoch: [71][363/408]	Loss 0.0015 (0.0063)	
training:	Epoch: [71][364/408]	Loss 0.0005 (0.0063)	
training:	Epoch: [71][365/408]	Loss 0.0067 (0.0063)	
training:	Epoch: [71][366/408]	Loss 0.0009 (0.0063)	
training:	Epoch: [71][367/408]	Loss 0.0007 (0.0063)	
training:	Epoch: [71][368/408]	Loss 0.0010 (0.0063)	
training:	Epoch: [71][369/408]	Loss 0.0007 (0.0063)	
training:	Epoch: [71][370/408]	Loss 0.0009 (0.0063)	
training:	Epoch: [71][371/408]	Loss 0.0029 (0.0062)	
training:	Epoch: [71][372/408]	Loss 0.0028 (0.0062)	
training:	Epoch: [71][373/408]	Loss 0.0286 (0.0063)	
training:	Epoch: [71][374/408]	Loss 0.0034 (0.0063)	
training:	Epoch: [71][375/408]	Loss 0.0008 (0.0063)	
training:	Epoch: [71][376/408]	Loss 0.0004 (0.0063)	
training:	Epoch: [71][377/408]	Loss 0.0010 (0.0062)	
training:	Epoch: [71][378/408]	Loss 0.0006 (0.0062)	
training:	Epoch: [71][379/408]	Loss 0.0004 (0.0062)	
training:	Epoch: [71][380/408]	Loss 0.0043 (0.0062)	
training:	Epoch: [71][381/408]	Loss 0.0006 (0.0062)	
training:	Epoch: [71][382/408]	Loss 0.0013 (0.0062)	
training:	Epoch: [71][383/408]	Loss 0.0004 (0.0062)	
training:	Epoch: [71][384/408]	Loss 0.0045 (0.0062)	
training:	Epoch: [71][385/408]	Loss 0.0008 (0.0062)	
training:	Epoch: [71][386/408]	Loss 0.0009 (0.0061)	
training:	Epoch: [71][387/408]	Loss 0.0004 (0.0061)	
training:	Epoch: [71][388/408]	Loss 0.0006 (0.0061)	
training:	Epoch: [71][389/408]	Loss 0.0004 (0.0061)	
training:	Epoch: [71][390/408]	Loss 0.0006 (0.0061)	
training:	Epoch: [71][391/408]	Loss 0.0005 (0.0061)	
training:	Epoch: [71][392/408]	Loss 0.0005 (0.0061)	
training:	Epoch: [71][393/408]	Loss 0.0004 (0.0060)	
training:	Epoch: [71][394/408]	Loss 0.0004 (0.0060)	
training:	Epoch: [71][395/408]	Loss 0.0017 (0.0060)	
training:	Epoch: [71][396/408]	Loss 0.0006 (0.0060)	
training:	Epoch: [71][397/408]	Loss 0.0004 (0.0060)	
training:	Epoch: [71][398/408]	Loss 0.0006 (0.0060)	
training:	Epoch: [71][399/408]	Loss 0.0097 (0.0060)	
training:	Epoch: [71][400/408]	Loss 0.0005 (0.0060)	
training:	Epoch: [71][401/408]	Loss 0.0017 (0.0060)	
training:	Epoch: [71][402/408]	Loss 0.0025 (0.0059)	
training:	Epoch: [71][403/408]	Loss 0.0026 (0.0059)	
training:	Epoch: [71][404/408]	Loss 0.0004 (0.0059)	
training:	Epoch: [71][405/408]	Loss 0.0005 (0.0059)	
training:	Epoch: [71][406/408]	Loss 0.0182 (0.0059)	
training:	Epoch: [71][407/408]	Loss 0.0013 (0.0059)	
training:	Epoch: [71][408/408]	Loss 0.0008 (0.0059)	
Training:	 Loss: 0.0059

Training:	 ACC: 0.9999 0.9998 0.9997 1.0000
Validation:	 ACC: 0.7817 0.7822 0.7943 0.7691
Validation:	 Best_BACC: 0.8226 0.8250 0.8772 0.7679
Validation:	 Loss: 1.2037
Pretraining:	Epoch 72/200
----------
training:	Epoch: [72][1/408]	Loss 0.0005 (0.0005)	
training:	Epoch: [72][2/408]	Loss 0.0011 (0.0008)	
training:	Epoch: [72][3/408]	Loss 0.0005 (0.0007)	
training:	Epoch: [72][4/408]	Loss 0.0006 (0.0006)	
training:	Epoch: [72][5/408]	Loss 0.0006 (0.0006)	
training:	Epoch: [72][6/408]	Loss 0.0022 (0.0009)	
training:	Epoch: [72][7/408]	Loss 0.0010 (0.0009)	
training:	Epoch: [72][8/408]	Loss 0.0006 (0.0009)	
training:	Epoch: [72][9/408]	Loss 0.0005 (0.0008)	
training:	Epoch: [72][10/408]	Loss 0.0004 (0.0008)	
training:	Epoch: [72][11/408]	Loss 0.0005 (0.0008)	
training:	Epoch: [72][12/408]	Loss 0.0006 (0.0007)	
training:	Epoch: [72][13/408]	Loss 0.0009 (0.0008)	
training:	Epoch: [72][14/408]	Loss 0.0004 (0.0007)	
training:	Epoch: [72][15/408]	Loss 0.0005 (0.0007)	
training:	Epoch: [72][16/408]	Loss 0.0006 (0.0007)	
training:	Epoch: [72][17/408]	Loss 0.0026 (0.0008)	
training:	Epoch: [72][18/408]	Loss 0.0006 (0.0008)	
training:	Epoch: [72][19/408]	Loss 0.0010 (0.0008)	
training:	Epoch: [72][20/408]	Loss 0.0004 (0.0008)	
training:	Epoch: [72][21/408]	Loss 0.0009 (0.0008)	
training:	Epoch: [72][22/408]	Loss 0.0004 (0.0008)	
training:	Epoch: [72][23/408]	Loss 0.0007 (0.0008)	
training:	Epoch: [72][24/408]	Loss 0.0004 (0.0008)	
training:	Epoch: [72][25/408]	Loss 0.0006 (0.0008)	
training:	Epoch: [72][26/408]	Loss 0.0007 (0.0008)	
training:	Epoch: [72][27/408]	Loss 0.0025 (0.0008)	
training:	Epoch: [72][28/408]	Loss 0.0008 (0.0008)	
training:	Epoch: [72][29/408]	Loss 0.0004 (0.0008)	
training:	Epoch: [72][30/408]	Loss 0.0004 (0.0008)	
training:	Epoch: [72][31/408]	Loss 0.0006 (0.0008)	
training:	Epoch: [72][32/408]	Loss 0.0016 (0.0008)	
training:	Epoch: [72][33/408]	Loss 0.0008 (0.0008)	
training:	Epoch: [72][34/408]	Loss 0.0008 (0.0008)	
training:	Epoch: [72][35/408]	Loss 0.0006 (0.0008)	
training:	Epoch: [72][36/408]	Loss 0.0006 (0.0008)	
training:	Epoch: [72][37/408]	Loss 0.0008 (0.0008)	
training:	Epoch: [72][38/408]	Loss 0.0022 (0.0008)	
training:	Epoch: [72][39/408]	Loss 0.0008 (0.0008)	
training:	Epoch: [72][40/408]	Loss 0.0005 (0.0008)	
training:	Epoch: [72][41/408]	Loss 0.0004 (0.0008)	
training:	Epoch: [72][42/408]	Loss 0.0011 (0.0008)	
training:	Epoch: [72][43/408]	Loss 0.0003 (0.0008)	
training:	Epoch: [72][44/408]	Loss 0.0006 (0.0008)	
training:	Epoch: [72][45/408]	Loss 0.0004 (0.0008)	
training:	Epoch: [72][46/408]	Loss 0.0005 (0.0008)	
training:	Epoch: [72][47/408]	Loss 0.0063 (0.0009)	
training:	Epoch: [72][48/408]	Loss 0.0009 (0.0009)	
training:	Epoch: [72][49/408]	Loss 0.0005 (0.0009)	
training:	Epoch: [72][50/408]	Loss 0.0024 (0.0009)	
training:	Epoch: [72][51/408]	Loss 0.0012 (0.0009)	
training:	Epoch: [72][52/408]	Loss 0.0004 (0.0009)	
training:	Epoch: [72][53/408]	Loss 0.0004 (0.0009)	
training:	Epoch: [72][54/408]	Loss 0.0004 (0.0009)	
training:	Epoch: [72][55/408]	Loss 0.0008 (0.0009)	
training:	Epoch: [72][56/408]	Loss 0.0006 (0.0009)	
training:	Epoch: [72][57/408]	Loss 0.0003 (0.0009)	
training:	Epoch: [72][58/408]	Loss 0.0005 (0.0009)	
training:	Epoch: [72][59/408]	Loss 0.0008 (0.0009)	
training:	Epoch: [72][60/408]	Loss 0.0007 (0.0009)	
training:	Epoch: [72][61/408]	Loss 0.0009 (0.0009)	
training:	Epoch: [72][62/408]	Loss 0.0031 (0.0009)	
training:	Epoch: [72][63/408]	Loss 0.0007 (0.0009)	
training:	Epoch: [72][64/408]	Loss 0.0003 (0.0009)	
training:	Epoch: [72][65/408]	Loss 0.0005 (0.0009)	
training:	Epoch: [72][66/408]	Loss 0.0004 (0.0009)	
training:	Epoch: [72][67/408]	Loss 0.0004 (0.0009)	
training:	Epoch: [72][68/408]	Loss 0.0006 (0.0009)	
training:	Epoch: [72][69/408]	Loss 0.0052 (0.0009)	
training:	Epoch: [72][70/408]	Loss 0.0011 (0.0009)	
training:	Epoch: [72][71/408]	Loss 0.0009 (0.0009)	
training:	Epoch: [72][72/408]	Loss 0.0007 (0.0009)	
training:	Epoch: [72][73/408]	Loss 0.0013 (0.0009)	
training:	Epoch: [72][74/408]	Loss 0.0004 (0.0009)	
training:	Epoch: [72][75/408]	Loss 0.0008 (0.0009)	
training:	Epoch: [72][76/408]	Loss 0.0018 (0.0009)	
training:	Epoch: [72][77/408]	Loss 0.0020 (0.0010)	
training:	Epoch: [72][78/408]	Loss 0.0004 (0.0010)	
training:	Epoch: [72][79/408]	Loss 0.0005 (0.0009)	
training:	Epoch: [72][80/408]	Loss 0.0012 (0.0009)	
training:	Epoch: [72][81/408]	Loss 0.0005 (0.0009)	
training:	Epoch: [72][82/408]	Loss 0.0010 (0.0009)	
training:	Epoch: [72][83/408]	Loss 0.0004 (0.0009)	
training:	Epoch: [72][84/408]	Loss 0.0004 (0.0009)	
training:	Epoch: [72][85/408]	Loss 0.0004 (0.0009)	
training:	Epoch: [72][86/408]	Loss 0.0007 (0.0009)	
training:	Epoch: [72][87/408]	Loss 0.0006 (0.0009)	
training:	Epoch: [72][88/408]	Loss 0.0006 (0.0009)	
training:	Epoch: [72][89/408]	Loss 0.0005 (0.0009)	
training:	Epoch: [72][90/408]	Loss 0.0004 (0.0009)	
training:	Epoch: [72][91/408]	Loss 0.0005 (0.0009)	
training:	Epoch: [72][92/408]	Loss 0.0005 (0.0009)	
training:	Epoch: [72][93/408]	Loss 0.0004 (0.0009)	
training:	Epoch: [72][94/408]	Loss 0.0006 (0.0009)	
training:	Epoch: [72][95/408]	Loss 0.0004 (0.0009)	
training:	Epoch: [72][96/408]	Loss 0.0009 (0.0009)	
training:	Epoch: [72][97/408]	Loss 0.0005 (0.0009)	
training:	Epoch: [72][98/408]	Loss 0.0004 (0.0009)	
training:	Epoch: [72][99/408]	Loss 0.0004 (0.0009)	
training:	Epoch: [72][100/408]	Loss 0.0005 (0.0009)	
training:	Epoch: [72][101/408]	Loss 0.0005 (0.0009)	
training:	Epoch: [72][102/408]	Loss 0.0005 (0.0009)	
training:	Epoch: [72][103/408]	Loss 0.0010 (0.0009)	
training:	Epoch: [72][104/408]	Loss 0.0016 (0.0009)	
training:	Epoch: [72][105/408]	Loss 0.0005 (0.0009)	
training:	Epoch: [72][106/408]	Loss 0.0003 (0.0009)	
training:	Epoch: [72][107/408]	Loss 0.0014 (0.0009)	
training:	Epoch: [72][108/408]	Loss 0.0008 (0.0009)	
training:	Epoch: [72][109/408]	Loss 0.0006 (0.0009)	
training:	Epoch: [72][110/408]	Loss 0.0007 (0.0009)	
training:	Epoch: [72][111/408]	Loss 0.0004 (0.0009)	
training:	Epoch: [72][112/408]	Loss 0.0005 (0.0009)	
training:	Epoch: [72][113/408]	Loss 0.0012 (0.0009)	
training:	Epoch: [72][114/408]	Loss 0.0005 (0.0008)	
training:	Epoch: [72][115/408]	Loss 0.0004 (0.0008)	
training:	Epoch: [72][116/408]	Loss 0.0005 (0.0008)	
training:	Epoch: [72][117/408]	Loss 0.0005 (0.0008)	
training:	Epoch: [72][118/408]	Loss 0.0004 (0.0008)	
training:	Epoch: [72][119/408]	Loss 0.0004 (0.0008)	
training:	Epoch: [72][120/408]	Loss 0.0009 (0.0008)	
training:	Epoch: [72][121/408]	Loss 0.0005 (0.0008)	
training:	Epoch: [72][122/408]	Loss 0.0020 (0.0008)	
training:	Epoch: [72][123/408]	Loss 0.0005 (0.0008)	
training:	Epoch: [72][124/408]	Loss 0.0005 (0.0008)	
training:	Epoch: [72][125/408]	Loss 0.0014 (0.0008)	
training:	Epoch: [72][126/408]	Loss 0.0012 (0.0008)	
training:	Epoch: [72][127/408]	Loss 0.0032 (0.0009)	
training:	Epoch: [72][128/408]	Loss 0.0004 (0.0009)	
training:	Epoch: [72][129/408]	Loss 0.0008 (0.0009)	
training:	Epoch: [72][130/408]	Loss 0.0005 (0.0009)	
training:	Epoch: [72][131/408]	Loss 0.0006 (0.0009)	
training:	Epoch: [72][132/408]	Loss 0.0014 (0.0009)	
training:	Epoch: [72][133/408]	Loss 0.0004 (0.0009)	
training:	Epoch: [72][134/408]	Loss 0.0003 (0.0008)	
training:	Epoch: [72][135/408]	Loss 0.0004 (0.0008)	
training:	Epoch: [72][136/408]	Loss 0.0004 (0.0008)	
training:	Epoch: [72][137/408]	Loss 0.0005 (0.0008)	
training:	Epoch: [72][138/408]	Loss 0.0005 (0.0008)	
training:	Epoch: [72][139/408]	Loss 0.0013 (0.0008)	
training:	Epoch: [72][140/408]	Loss 0.0006 (0.0008)	
training:	Epoch: [72][141/408]	Loss 0.0009 (0.0008)	
training:	Epoch: [72][142/408]	Loss 0.0009 (0.0008)	
training:	Epoch: [72][143/408]	Loss 0.0009 (0.0008)	
training:	Epoch: [72][144/408]	Loss 0.0004 (0.0008)	
training:	Epoch: [72][145/408]	Loss 0.0007 (0.0008)	
training:	Epoch: [72][146/408]	Loss 0.0022 (0.0008)	
training:	Epoch: [72][147/408]	Loss 0.0006 (0.0008)	
training:	Epoch: [72][148/408]	Loss 0.0015 (0.0008)	
training:	Epoch: [72][149/408]	Loss 0.0012 (0.0008)	
training:	Epoch: [72][150/408]	Loss 0.0004 (0.0008)	
training:	Epoch: [72][151/408]	Loss 0.0005 (0.0008)	
training:	Epoch: [72][152/408]	Loss 0.0011 (0.0008)	
training:	Epoch: [72][153/408]	Loss 0.0005 (0.0008)	
training:	Epoch: [72][154/408]	Loss 0.0030 (0.0009)	
training:	Epoch: [72][155/408]	Loss 0.0017 (0.0009)	
training:	Epoch: [72][156/408]	Loss 0.0004 (0.0009)	
training:	Epoch: [72][157/408]	Loss 0.0004 (0.0009)	
training:	Epoch: [72][158/408]	Loss 0.0007 (0.0009)	
training:	Epoch: [72][159/408]	Loss 0.0008 (0.0009)	
training:	Epoch: [72][160/408]	Loss 0.0057 (0.0009)	
training:	Epoch: [72][161/408]	Loss 0.0199 (0.0010)	
training:	Epoch: [72][162/408]	Loss 0.0003 (0.0010)	
training:	Epoch: [72][163/408]	Loss 0.0004 (0.0010)	
training:	Epoch: [72][164/408]	Loss 0.0003 (0.0010)	
training:	Epoch: [72][165/408]	Loss 0.0005 (0.0010)	
training:	Epoch: [72][166/408]	Loss 0.0003 (0.0010)	
training:	Epoch: [72][167/408]	Loss 0.0006 (0.0010)	
training:	Epoch: [72][168/408]	Loss 0.0013 (0.0010)	
training:	Epoch: [72][169/408]	Loss 0.0007 (0.0010)	
training:	Epoch: [72][170/408]	Loss 0.0039 (0.0010)	
training:	Epoch: [72][171/408]	Loss 0.0006 (0.0010)	
training:	Epoch: [72][172/408]	Loss 0.0006 (0.0010)	
training:	Epoch: [72][173/408]	Loss 0.0012 (0.0010)	
training:	Epoch: [72][174/408]	Loss 0.0010 (0.0010)	
training:	Epoch: [72][175/408]	Loss 0.0027 (0.0010)	
training:	Epoch: [72][176/408]	Loss 0.0009 (0.0010)	
training:	Epoch: [72][177/408]	Loss 0.0006 (0.0010)	
training:	Epoch: [72][178/408]	Loss 0.0033 (0.0010)	
training:	Epoch: [72][179/408]	Loss 0.0005 (0.0010)	
training:	Epoch: [72][180/408]	Loss 0.0004 (0.0010)	
training:	Epoch: [72][181/408]	Loss 0.0023 (0.0010)	
training:	Epoch: [72][182/408]	Loss 0.0152 (0.0011)	
training:	Epoch: [72][183/408]	Loss 0.0006 (0.0011)	
training:	Epoch: [72][184/408]	Loss 0.0012 (0.0011)	
training:	Epoch: [72][185/408]	Loss 0.0005 (0.0011)	
training:	Epoch: [72][186/408]	Loss 0.0004 (0.0011)	
training:	Epoch: [72][187/408]	Loss 0.0005 (0.0011)	
training:	Epoch: [72][188/408]	Loss 0.0004 (0.0011)	
training:	Epoch: [72][189/408]	Loss 0.0004 (0.0011)	
training:	Epoch: [72][190/408]	Loss 0.0006 (0.0011)	
training:	Epoch: [72][191/408]	Loss 0.0005 (0.0011)	
training:	Epoch: [72][192/408]	Loss 0.0005 (0.0011)	
training:	Epoch: [72][193/408]	Loss 0.0006 (0.0011)	
training:	Epoch: [72][194/408]	Loss 0.0010 (0.0011)	
training:	Epoch: [72][195/408]	Loss 0.0004 (0.0011)	
training:	Epoch: [72][196/408]	Loss 0.0004 (0.0011)	
training:	Epoch: [72][197/408]	Loss 0.0005 (0.0011)	
training:	Epoch: [72][198/408]	Loss 0.0007 (0.0011)	
training:	Epoch: [72][199/408]	Loss 0.0006 (0.0010)	
training:	Epoch: [72][200/408]	Loss 0.0004 (0.0010)	
training:	Epoch: [72][201/408]	Loss 0.0003 (0.0010)	
training:	Epoch: [72][202/408]	Loss 0.0008 (0.0010)	
training:	Epoch: [72][203/408]	Loss 0.0004 (0.0010)	
training:	Epoch: [72][204/408]	Loss 0.0006 (0.0010)	
training:	Epoch: [72][205/408]	Loss 0.0011 (0.0010)	
training:	Epoch: [72][206/408]	Loss 0.0004 (0.0010)	
training:	Epoch: [72][207/408]	Loss 0.0003 (0.0010)	
training:	Epoch: [72][208/408]	Loss 0.0005 (0.0010)	
training:	Epoch: [72][209/408]	Loss 0.0033 (0.0010)	
training:	Epoch: [72][210/408]	Loss 0.0009 (0.0010)	
training:	Epoch: [72][211/408]	Loss 0.0006 (0.0010)	
training:	Epoch: [72][212/408]	Loss 0.0004 (0.0010)	
training:	Epoch: [72][213/408]	Loss 0.0005 (0.0010)	
training:	Epoch: [72][214/408]	Loss 0.0007 (0.0010)	
training:	Epoch: [72][215/408]	Loss 0.0004 (0.0010)	
training:	Epoch: [72][216/408]	Loss 0.0023 (0.0010)	
training:	Epoch: [72][217/408]	Loss 0.0003 (0.0010)	
training:	Epoch: [72][218/408]	Loss 0.0008 (0.0010)	
training:	Epoch: [72][219/408]	Loss 0.0007 (0.0010)	
training:	Epoch: [72][220/408]	Loss 0.0006 (0.0010)	
training:	Epoch: [72][221/408]	Loss 0.0007 (0.0010)	
training:	Epoch: [72][222/408]	Loss 0.0004 (0.0010)	
training:	Epoch: [72][223/408]	Loss 0.0003 (0.0010)	
training:	Epoch: [72][224/408]	Loss 0.0004 (0.0010)	
training:	Epoch: [72][225/408]	Loss 0.0003 (0.0010)	
training:	Epoch: [72][226/408]	Loss 0.0004 (0.0010)	
training:	Epoch: [72][227/408]	Loss 0.0005 (0.0010)	
training:	Epoch: [72][228/408]	Loss 0.0004 (0.0010)	
training:	Epoch: [72][229/408]	Loss 0.0003 (0.0010)	
training:	Epoch: [72][230/408]	Loss 0.0004 (0.0010)	
training:	Epoch: [72][231/408]	Loss 0.0008 (0.0010)	
training:	Epoch: [72][232/408]	Loss 0.0006 (0.0010)	
training:	Epoch: [72][233/408]	Loss 0.0011 (0.0010)	
training:	Epoch: [72][234/408]	Loss 0.0006 (0.0010)	
training:	Epoch: [72][235/408]	Loss 0.0060 (0.0010)	
training:	Epoch: [72][236/408]	Loss 0.0017 (0.0010)	
training:	Epoch: [72][237/408]	Loss 0.0004 (0.0010)	
training:	Epoch: [72][238/408]	Loss 0.0003 (0.0010)	
training:	Epoch: [72][239/408]	Loss 0.0005 (0.0010)	
training:	Epoch: [72][240/408]	Loss 0.0012 (0.0010)	
training:	Epoch: [72][241/408]	Loss 0.0004 (0.0010)	
training:	Epoch: [72][242/408]	Loss 0.0004 (0.0010)	
training:	Epoch: [72][243/408]	Loss 0.0007 (0.0010)	
training:	Epoch: [72][244/408]	Loss 0.0005 (0.0010)	
training:	Epoch: [72][245/408]	Loss 0.0004 (0.0010)	
training:	Epoch: [72][246/408]	Loss 0.0003 (0.0010)	
training:	Epoch: [72][247/408]	Loss 0.0004 (0.0010)	
training:	Epoch: [72][248/408]	Loss 0.0006 (0.0010)	
training:	Epoch: [72][249/408]	Loss 0.0004 (0.0010)	
training:	Epoch: [72][250/408]	Loss 0.0006 (0.0010)	
training:	Epoch: [72][251/408]	Loss 0.0027 (0.0010)	
training:	Epoch: [72][252/408]	Loss 0.0003 (0.0010)	
training:	Epoch: [72][253/408]	Loss 0.0021 (0.0010)	
training:	Epoch: [72][254/408]	Loss 0.0004 (0.0010)	
training:	Epoch: [72][255/408]	Loss 0.0003 (0.0010)	
training:	Epoch: [72][256/408]	Loss 0.0005 (0.0010)	
training:	Epoch: [72][257/408]	Loss 0.0005 (0.0010)	
training:	Epoch: [72][258/408]	Loss 0.0004 (0.0010)	
training:	Epoch: [72][259/408]	Loss 0.0010 (0.0010)	
training:	Epoch: [72][260/408]	Loss 0.0007 (0.0010)	
training:	Epoch: [72][261/408]	Loss 0.0006 (0.0010)	
training:	Epoch: [72][262/408]	Loss 0.0004 (0.0010)	
training:	Epoch: [72][263/408]	Loss 0.0007 (0.0010)	
training:	Epoch: [72][264/408]	Loss 0.0005 (0.0010)	
training:	Epoch: [72][265/408]	Loss 0.0003 (0.0010)	
training:	Epoch: [72][266/408]	Loss 0.0004 (0.0010)	
training:	Epoch: [72][267/408]	Loss 0.0012 (0.0010)	
training:	Epoch: [72][268/408]	Loss 0.0005 (0.0010)	
training:	Epoch: [72][269/408]	Loss 0.0005 (0.0010)	
training:	Epoch: [72][270/408]	Loss 0.0003 (0.0010)	
training:	Epoch: [72][271/408]	Loss 0.0005 (0.0010)	
training:	Epoch: [72][272/408]	Loss 0.0003 (0.0010)	
training:	Epoch: [72][273/408]	Loss 0.0004 (0.0010)	
training:	Epoch: [72][274/408]	Loss 0.0007 (0.0010)	
training:	Epoch: [72][275/408]	Loss 0.0004 (0.0010)	
training:	Epoch: [72][276/408]	Loss 0.0004 (0.0010)	
training:	Epoch: [72][277/408]	Loss 0.0008 (0.0010)	
training:	Epoch: [72][278/408]	Loss 0.0003 (0.0010)	
training:	Epoch: [72][279/408]	Loss 0.0005 (0.0010)	
training:	Epoch: [72][280/408]	Loss 0.0034 (0.0010)	
training:	Epoch: [72][281/408]	Loss 0.0004 (0.0010)	
training:	Epoch: [72][282/408]	Loss 0.0004 (0.0010)	
training:	Epoch: [72][283/408]	Loss 0.0007 (0.0010)	
training:	Epoch: [72][284/408]	Loss 0.0011 (0.0010)	
training:	Epoch: [72][285/408]	Loss 0.0004 (0.0010)	
training:	Epoch: [72][286/408]	Loss 0.0005 (0.0010)	
training:	Epoch: [72][287/408]	Loss 0.0006 (0.0010)	
training:	Epoch: [72][288/408]	Loss 0.0004 (0.0010)	
training:	Epoch: [72][289/408]	Loss 0.0006 (0.0010)	
training:	Epoch: [72][290/408]	Loss 0.0019 (0.0010)	
training:	Epoch: [72][291/408]	Loss 0.0008 (0.0010)	
training:	Epoch: [72][292/408]	Loss 0.0004 (0.0010)	
training:	Epoch: [72][293/408]	Loss 0.0003 (0.0009)	
training:	Epoch: [72][294/408]	Loss 0.0013 (0.0010)	
training:	Epoch: [72][295/408]	Loss 0.0003 (0.0009)	
training:	Epoch: [72][296/408]	Loss 0.0003 (0.0009)	
training:	Epoch: [72][297/408]	Loss 0.0004 (0.0009)	
training:	Epoch: [72][298/408]	Loss 0.0004 (0.0009)	
training:	Epoch: [72][299/408]	Loss 0.0005 (0.0009)	
training:	Epoch: [72][300/408]	Loss 0.0003 (0.0009)	
training:	Epoch: [72][301/408]	Loss 0.0005 (0.0009)	
training:	Epoch: [72][302/408]	Loss 0.0004 (0.0009)	
training:	Epoch: [72][303/408]	Loss 0.0006 (0.0009)	
training:	Epoch: [72][304/408]	Loss 0.0004 (0.0009)	
training:	Epoch: [72][305/408]	Loss 0.0004 (0.0009)	
training:	Epoch: [72][306/408]	Loss 0.0006 (0.0009)	
training:	Epoch: [72][307/408]	Loss 0.0007 (0.0009)	
training:	Epoch: [72][308/408]	Loss 0.0005 (0.0009)	
training:	Epoch: [72][309/408]	Loss 0.0004 (0.0009)	
training:	Epoch: [72][310/408]	Loss 0.0004 (0.0009)	
training:	Epoch: [72][311/408]	Loss 0.0005 (0.0009)	
training:	Epoch: [72][312/408]	Loss 0.0010 (0.0009)	
training:	Epoch: [72][313/408]	Loss 0.0003 (0.0009)	
training:	Epoch: [72][314/408]	Loss 0.0007 (0.0009)	
training:	Epoch: [72][315/408]	Loss 0.0005 (0.0009)	
training:	Epoch: [72][316/408]	Loss 0.0135 (0.0010)	
training:	Epoch: [72][317/408]	Loss 0.0004 (0.0010)	
training:	Epoch: [72][318/408]	Loss 0.0004 (0.0010)	
training:	Epoch: [72][319/408]	Loss 0.0004 (0.0010)	
training:	Epoch: [72][320/408]	Loss 0.0004 (0.0010)	
training:	Epoch: [72][321/408]	Loss 0.0005 (0.0010)	
training:	Epoch: [72][322/408]	Loss 0.0006 (0.0009)	
training:	Epoch: [72][323/408]	Loss 0.0003 (0.0009)	
training:	Epoch: [72][324/408]	Loss 0.0003 (0.0009)	
training:	Epoch: [72][325/408]	Loss 0.0003 (0.0009)	
training:	Epoch: [72][326/408]	Loss 0.0015 (0.0009)	
training:	Epoch: [72][327/408]	Loss 0.0004 (0.0009)	
training:	Epoch: [72][328/408]	Loss 0.0003 (0.0009)	
training:	Epoch: [72][329/408]	Loss 0.0010 (0.0009)	
training:	Epoch: [72][330/408]	Loss 0.0003 (0.0009)	
training:	Epoch: [72][331/408]	Loss 0.0012 (0.0009)	
training:	Epoch: [72][332/408]	Loss 0.0003 (0.0009)	
training:	Epoch: [72][333/408]	Loss 0.0004 (0.0009)	
training:	Epoch: [72][334/408]	Loss 0.0022 (0.0009)	
training:	Epoch: [72][335/408]	Loss 0.0006 (0.0009)	
training:	Epoch: [72][336/408]	Loss 0.0008 (0.0009)	
training:	Epoch: [72][337/408]	Loss 0.0005 (0.0009)	
training:	Epoch: [72][338/408]	Loss 0.0004 (0.0009)	
training:	Epoch: [72][339/408]	Loss 0.0003 (0.0009)	
training:	Epoch: [72][340/408]	Loss 0.0008 (0.0009)	
training:	Epoch: [72][341/408]	Loss 0.0004 (0.0009)	
training:	Epoch: [72][342/408]	Loss 0.0037 (0.0009)	
training:	Epoch: [72][343/408]	Loss 0.0007 (0.0009)	
training:	Epoch: [72][344/408]	Loss 0.0009 (0.0009)	
training:	Epoch: [72][345/408]	Loss 0.0003 (0.0009)	
training:	Epoch: [72][346/408]	Loss 0.0006 (0.0009)	
training:	Epoch: [72][347/408]	Loss 0.0006 (0.0009)	
training:	Epoch: [72][348/408]	Loss 0.0011 (0.0009)	
training:	Epoch: [72][349/408]	Loss 0.0010 (0.0009)	
training:	Epoch: [72][350/408]	Loss 0.0082 (0.0010)	
training:	Epoch: [72][351/408]	Loss 0.0004 (0.0010)	
training:	Epoch: [72][352/408]	Loss 0.0012 (0.0010)	
training:	Epoch: [72][353/408]	Loss 0.0030 (0.0010)	
training:	Epoch: [72][354/408]	Loss 0.0022 (0.0010)	
training:	Epoch: [72][355/408]	Loss 0.0006 (0.0010)	
training:	Epoch: [72][356/408]	Loss 0.0004 (0.0010)	
training:	Epoch: [72][357/408]	Loss 0.0004 (0.0010)	
training:	Epoch: [72][358/408]	Loss 0.0004 (0.0010)	
training:	Epoch: [72][359/408]	Loss 0.0003 (0.0010)	
training:	Epoch: [72][360/408]	Loss 0.0004 (0.0010)	
training:	Epoch: [72][361/408]	Loss 0.0003 (0.0010)	
training:	Epoch: [72][362/408]	Loss 0.0004 (0.0010)	
training:	Epoch: [72][363/408]	Loss 0.0015 (0.0010)	
training:	Epoch: [72][364/408]	Loss 0.0006 (0.0010)	
training:	Epoch: [72][365/408]	Loss 0.0028 (0.0010)	
training:	Epoch: [72][366/408]	Loss 0.0020 (0.0010)	
training:	Epoch: [72][367/408]	Loss 0.0004 (0.0010)	
training:	Epoch: [72][368/408]	Loss 0.0004 (0.0010)	
training:	Epoch: [72][369/408]	Loss 0.0006 (0.0010)	
training:	Epoch: [72][370/408]	Loss 0.0009 (0.0010)	
training:	Epoch: [72][371/408]	Loss 0.0008 (0.0010)	
training:	Epoch: [72][372/408]	Loss 0.0013 (0.0010)	
training:	Epoch: [72][373/408]	Loss 0.0004 (0.0010)	
training:	Epoch: [72][374/408]	Loss 0.0010 (0.0010)	
training:	Epoch: [72][375/408]	Loss 0.0004 (0.0010)	
training:	Epoch: [72][376/408]	Loss 0.0004 (0.0010)	
training:	Epoch: [72][377/408]	Loss 0.0004 (0.0010)	
training:	Epoch: [72][378/408]	Loss 0.0006 (0.0010)	
training:	Epoch: [72][379/408]	Loss 0.0004 (0.0009)	
training:	Epoch: [72][380/408]	Loss 0.0003 (0.0009)	
training:	Epoch: [72][381/408]	Loss 0.0004 (0.0009)	
training:	Epoch: [72][382/408]	Loss 0.0004 (0.0009)	
training:	Epoch: [72][383/408]	Loss 0.0004 (0.0009)	
training:	Epoch: [72][384/408]	Loss 0.0007 (0.0009)	
training:	Epoch: [72][385/408]	Loss 0.0003 (0.0009)	
training:	Epoch: [72][386/408]	Loss 0.0004 (0.0009)	
training:	Epoch: [72][387/408]	Loss 0.0003 (0.0009)	
training:	Epoch: [72][388/408]	Loss 0.0003 (0.0009)	
training:	Epoch: [72][389/408]	Loss 0.0004 (0.0009)	
training:	Epoch: [72][390/408]	Loss 0.0004 (0.0009)	
training:	Epoch: [72][391/408]	Loss 0.0007 (0.0009)	
training:	Epoch: [72][392/408]	Loss 0.0005 (0.0009)	
training:	Epoch: [72][393/408]	Loss 0.0003 (0.0009)	
training:	Epoch: [72][394/408]	Loss 0.0011 (0.0009)	
training:	Epoch: [72][395/408]	Loss 0.0003 (0.0009)	
training:	Epoch: [72][396/408]	Loss 0.0004 (0.0009)	
training:	Epoch: [72][397/408]	Loss 0.0005 (0.0009)	
training:	Epoch: [72][398/408]	Loss 0.0003 (0.0009)	
training:	Epoch: [72][399/408]	Loss 0.0004 (0.0009)	
training:	Epoch: [72][400/408]	Loss 0.0004 (0.0009)	
training:	Epoch: [72][401/408]	Loss 0.0006 (0.0009)	
training:	Epoch: [72][402/408]	Loss 0.0004 (0.0009)	
training:	Epoch: [72][403/408]	Loss 0.0003 (0.0009)	
training:	Epoch: [72][404/408]	Loss 0.0005 (0.0009)	
training:	Epoch: [72][405/408]	Loss 0.0003 (0.0009)	
training:	Epoch: [72][406/408]	Loss 0.0009 (0.0009)	
training:	Epoch: [72][407/408]	Loss 0.0004 (0.0009)	
training:	Epoch: [72][408/408]	Loss 0.0003 (0.0009)	
Training:	 Loss: 0.0009

Training:	 ACC: 1.0000 1.0000 1.0000 1.0000
Validation:	 ACC: 0.7764 0.7774 0.7994 0.7534
Validation:	 Best_BACC: 0.8226 0.8250 0.8772 0.7679
Validation:	 Loss: 1.2866
Pretraining:	Epoch 73/200
----------
training:	Epoch: [73][1/408]	Loss 0.0004 (0.0004)	
training:	Epoch: [73][2/408]	Loss 0.0009 (0.0007)	
training:	Epoch: [73][3/408]	Loss 0.0005 (0.0006)	
training:	Epoch: [73][4/408]	Loss 0.0010 (0.0007)	
training:	Epoch: [73][5/408]	Loss 0.0013 (0.0008)	
training:	Epoch: [73][6/408]	Loss 0.0004 (0.0007)	
training:	Epoch: [73][7/408]	Loss 0.0004 (0.0007)	
training:	Epoch: [73][8/408]	Loss 0.0004 (0.0007)	
training:	Epoch: [73][9/408]	Loss 0.0004 (0.0006)	
training:	Epoch: [73][10/408]	Loss 0.0004 (0.0006)	
training:	Epoch: [73][11/408]	Loss 0.0004 (0.0006)	
training:	Epoch: [73][12/408]	Loss 0.0004 (0.0006)	
training:	Epoch: [73][13/408]	Loss 0.0003 (0.0005)	
training:	Epoch: [73][14/408]	Loss 0.0003 (0.0005)	
training:	Epoch: [73][15/408]	Loss 0.0004 (0.0005)	
training:	Epoch: [73][16/408]	Loss 0.0007 (0.0005)	
training:	Epoch: [73][17/408]	Loss 0.0004 (0.0005)	
training:	Epoch: [73][18/408]	Loss 0.0012 (0.0006)	
training:	Epoch: [73][19/408]	Loss 0.0003 (0.0005)	
training:	Epoch: [73][20/408]	Loss 0.0004 (0.0005)	
training:	Epoch: [73][21/408]	Loss 0.0003 (0.0005)	
training:	Epoch: [73][22/408]	Loss 0.0003 (0.0005)	
training:	Epoch: [73][23/408]	Loss 0.0045 (0.0007)	
training:	Epoch: [73][24/408]	Loss 0.0003 (0.0007)	
training:	Epoch: [73][25/408]	Loss 0.0004 (0.0007)	
training:	Epoch: [73][26/408]	Loss 0.0010 (0.0007)	
training:	Epoch: [73][27/408]	Loss 0.0008 (0.0007)	
training:	Epoch: [73][28/408]	Loss 0.0004 (0.0007)	
training:	Epoch: [73][29/408]	Loss 0.0005 (0.0007)	
training:	Epoch: [73][30/408]	Loss 0.0012 (0.0007)	
training:	Epoch: [73][31/408]	Loss 0.0004 (0.0007)	
training:	Epoch: [73][32/408]	Loss 0.0007 (0.0007)	
training:	Epoch: [73][33/408]	Loss 0.0003 (0.0007)	
training:	Epoch: [73][34/408]	Loss 0.0005 (0.0007)	
training:	Epoch: [73][35/408]	Loss 0.0004 (0.0006)	
training:	Epoch: [73][36/408]	Loss 0.0007 (0.0006)	
training:	Epoch: [73][37/408]	Loss 0.0003 (0.0006)	
training:	Epoch: [73][38/408]	Loss 0.0004 (0.0006)	
training:	Epoch: [73][39/408]	Loss 0.0003 (0.0006)	
training:	Epoch: [73][40/408]	Loss 0.0040 (0.0007)	
training:	Epoch: [73][41/408]	Loss 0.0003 (0.0007)	
training:	Epoch: [73][42/408]	Loss 0.0003 (0.0007)	
training:	Epoch: [73][43/408]	Loss 0.0007 (0.0007)	
training:	Epoch: [73][44/408]	Loss 0.0007 (0.0007)	
training:	Epoch: [73][45/408]	Loss 0.0017 (0.0007)	
training:	Epoch: [73][46/408]	Loss 0.0003 (0.0007)	
training:	Epoch: [73][47/408]	Loss 0.0006 (0.0007)	
training:	Epoch: [73][48/408]	Loss 0.0003 (0.0007)	
training:	Epoch: [73][49/408]	Loss 0.0004 (0.0007)	
training:	Epoch: [73][50/408]	Loss 0.0006 (0.0007)	
training:	Epoch: [73][51/408]	Loss 0.0004 (0.0007)	
training:	Epoch: [73][52/408]	Loss 0.0006 (0.0007)	
training:	Epoch: [73][53/408]	Loss 0.0006 (0.0007)	
training:	Epoch: [73][54/408]	Loss 0.0003 (0.0007)	
training:	Epoch: [73][55/408]	Loss 0.0003 (0.0007)	
training:	Epoch: [73][56/408]	Loss 0.0003 (0.0007)	
training:	Epoch: [73][57/408]	Loss 0.0004 (0.0007)	
training:	Epoch: [73][58/408]	Loss 0.0006 (0.0007)	
training:	Epoch: [73][59/408]	Loss 0.0004 (0.0006)	
training:	Epoch: [73][60/408]	Loss 0.0003 (0.0006)	
training:	Epoch: [73][61/408]	Loss 0.0010 (0.0006)	
training:	Epoch: [73][62/408]	Loss 0.0004 (0.0006)	
training:	Epoch: [73][63/408]	Loss 0.0005 (0.0006)	
training:	Epoch: [73][64/408]	Loss 0.0003 (0.0006)	
training:	Epoch: [73][65/408]	Loss 0.0003 (0.0006)	
training:	Epoch: [73][66/408]	Loss 0.0004 (0.0006)	
training:	Epoch: [73][67/408]	Loss 0.0004 (0.0006)	
training:	Epoch: [73][68/408]	Loss 0.0004 (0.0006)	
training:	Epoch: [73][69/408]	Loss 0.0004 (0.0006)	
training:	Epoch: [73][70/408]	Loss 0.0003 (0.0006)	
training:	Epoch: [73][71/408]	Loss 0.0003 (0.0006)	
training:	Epoch: [73][72/408]	Loss 0.0004 (0.0006)	
training:	Epoch: [73][73/408]	Loss 0.0004 (0.0006)	
training:	Epoch: [73][74/408]	Loss 0.0004 (0.0006)	
training:	Epoch: [73][75/408]	Loss 0.0003 (0.0006)	
training:	Epoch: [73][76/408]	Loss 0.0003 (0.0006)	
training:	Epoch: [73][77/408]	Loss 0.0004 (0.0006)	
training:	Epoch: [73][78/408]	Loss 0.0003 (0.0006)	
training:	Epoch: [73][79/408]	Loss 0.0004 (0.0006)	
training:	Epoch: [73][80/408]	Loss 0.0003 (0.0006)	
training:	Epoch: [73][81/408]	Loss 0.0003 (0.0006)	
training:	Epoch: [73][82/408]	Loss 0.0003 (0.0006)	
training:	Epoch: [73][83/408]	Loss 0.0006 (0.0006)	
training:	Epoch: [73][84/408]	Loss 0.0005 (0.0006)	
training:	Epoch: [73][85/408]	Loss 0.0003 (0.0006)	
training:	Epoch: [73][86/408]	Loss 0.0012 (0.0006)	
training:	Epoch: [73][87/408]	Loss 0.0003 (0.0006)	
training:	Epoch: [73][88/408]	Loss 0.0003 (0.0006)	
training:	Epoch: [73][89/408]	Loss 0.0003 (0.0006)	
training:	Epoch: [73][90/408]	Loss 0.0003 (0.0006)	
training:	Epoch: [73][91/408]	Loss 0.0007 (0.0006)	
training:	Epoch: [73][92/408]	Loss 0.0003 (0.0006)	
training:	Epoch: [73][93/408]	Loss 0.0004 (0.0006)	
training:	Epoch: [73][94/408]	Loss 0.0010 (0.0006)	
training:	Epoch: [73][95/408]	Loss 0.0005 (0.0006)	
training:	Epoch: [73][96/408]	Loss 0.0003 (0.0006)	
training:	Epoch: [73][97/408]	Loss 0.0004 (0.0006)	
training:	Epoch: [73][98/408]	Loss 0.0004 (0.0006)	
training:	Epoch: [73][99/408]	Loss 0.0004 (0.0006)	
training:	Epoch: [73][100/408]	Loss 0.0005 (0.0006)	
training:	Epoch: [73][101/408]	Loss 0.0007 (0.0006)	
training:	Epoch: [73][102/408]	Loss 0.0003 (0.0006)	
training:	Epoch: [73][103/408]	Loss 0.0003 (0.0006)	
training:	Epoch: [73][104/408]	Loss 0.0003 (0.0006)	
training:	Epoch: [73][105/408]	Loss 0.0006 (0.0006)	
training:	Epoch: [73][106/408]	Loss 0.0015 (0.0006)	
training:	Epoch: [73][107/408]	Loss 0.0011 (0.0006)	
training:	Epoch: [73][108/408]	Loss 0.0004 (0.0006)	
training:	Epoch: [73][109/408]	Loss 0.0127 (0.0007)	
training:	Epoch: [73][110/408]	Loss 0.0006 (0.0007)	
training:	Epoch: [73][111/408]	Loss 0.0006 (0.0007)	
training:	Epoch: [73][112/408]	Loss 0.0010 (0.0007)	
training:	Epoch: [73][113/408]	Loss 0.0003 (0.0007)	
training:	Epoch: [73][114/408]	Loss 0.0003 (0.0007)	
training:	Epoch: [73][115/408]	Loss 0.0007 (0.0007)	
training:	Epoch: [73][116/408]	Loss 0.0004 (0.0007)	
training:	Epoch: [73][117/408]	Loss 0.0004 (0.0007)	
training:	Epoch: [73][118/408]	Loss 0.0004 (0.0007)	
training:	Epoch: [73][119/408]	Loss 0.0003 (0.0007)	
training:	Epoch: [73][120/408]	Loss 0.0007 (0.0007)	
training:	Epoch: [73][121/408]	Loss 0.0004 (0.0007)	
training:	Epoch: [73][122/408]	Loss 0.0006 (0.0007)	
training:	Epoch: [73][123/408]	Loss 0.0004 (0.0007)	
training:	Epoch: [73][124/408]	Loss 0.0019 (0.0007)	
training:	Epoch: [73][125/408]	Loss 0.0004 (0.0007)	
training:	Epoch: [73][126/408]	Loss 0.0006 (0.0007)	
training:	Epoch: [73][127/408]	Loss 0.0038 (0.0007)	
training:	Epoch: [73][128/408]	Loss 0.0004 (0.0007)	
training:	Epoch: [73][129/408]	Loss 0.0003 (0.0007)	
training:	Epoch: [73][130/408]	Loss 0.0003 (0.0007)	
training:	Epoch: [73][131/408]	Loss 0.0005 (0.0007)	
training:	Epoch: [73][132/408]	Loss 0.0003 (0.0007)	
training:	Epoch: [73][133/408]	Loss 0.0006 (0.0007)	
training:	Epoch: [73][134/408]	Loss 0.0003 (0.0007)	
training:	Epoch: [73][135/408]	Loss 0.0004 (0.0007)	
training:	Epoch: [73][136/408]	Loss 0.0012 (0.0007)	
training:	Epoch: [73][137/408]	Loss 0.0005 (0.0007)	
training:	Epoch: [73][138/408]	Loss 0.0031 (0.0007)	
training:	Epoch: [73][139/408]	Loss 0.0004 (0.0007)	
training:	Epoch: [73][140/408]	Loss 0.0011 (0.0007)	
training:	Epoch: [73][141/408]	Loss 0.0004 (0.0007)	
training:	Epoch: [73][142/408]	Loss 0.0004 (0.0007)	
training:	Epoch: [73][143/408]	Loss 0.0005 (0.0007)	
training:	Epoch: [73][144/408]	Loss 0.0003 (0.0007)	
training:	Epoch: [73][145/408]	Loss 0.0004 (0.0007)	
training:	Epoch: [73][146/408]	Loss 0.0004 (0.0007)	
training:	Epoch: [73][147/408]	Loss 0.0004 (0.0007)	
training:	Epoch: [73][148/408]	Loss 0.0008 (0.0007)	
training:	Epoch: [73][149/408]	Loss 0.0004 (0.0007)	
training:	Epoch: [73][150/408]	Loss 0.0003 (0.0007)	
training:	Epoch: [73][151/408]	Loss 0.0005 (0.0007)	
training:	Epoch: [73][152/408]	Loss 0.0003 (0.0007)	
training:	Epoch: [73][153/408]	Loss 0.0003 (0.0007)	
training:	Epoch: [73][154/408]	Loss 0.0005 (0.0007)	
training:	Epoch: [73][155/408]	Loss 0.0007 (0.0007)	
training:	Epoch: [73][156/408]	Loss 0.0003 (0.0007)	
training:	Epoch: [73][157/408]	Loss 0.0004 (0.0007)	
training:	Epoch: [73][158/408]	Loss 0.0008 (0.0007)	
training:	Epoch: [73][159/408]	Loss 0.0005 (0.0007)	
training:	Epoch: [73][160/408]	Loss 0.0005 (0.0007)	
training:	Epoch: [73][161/408]	Loss 0.0004 (0.0007)	
training:	Epoch: [73][162/408]	Loss 0.0003 (0.0007)	
training:	Epoch: [73][163/408]	Loss 0.0004 (0.0007)	
training:	Epoch: [73][164/408]	Loss 0.0003 (0.0007)	
training:	Epoch: [73][165/408]	Loss 0.0004 (0.0007)	
training:	Epoch: [73][166/408]	Loss 0.0004 (0.0007)	
training:	Epoch: [73][167/408]	Loss 0.0003 (0.0007)	
training:	Epoch: [73][168/408]	Loss 0.0003 (0.0006)	
training:	Epoch: [73][169/408]	Loss 0.0003 (0.0006)	
training:	Epoch: [73][170/408]	Loss 0.0007 (0.0006)	
training:	Epoch: [73][171/408]	Loss 0.0004 (0.0006)	
training:	Epoch: [73][172/408]	Loss 0.0046 (0.0007)	
training:	Epoch: [73][173/408]	Loss 0.0004 (0.0007)	
training:	Epoch: [73][174/408]	Loss 0.0004 (0.0007)	
training:	Epoch: [73][175/408]	Loss 0.0003 (0.0007)	
training:	Epoch: [73][176/408]	Loss 0.0003 (0.0007)	
training:	Epoch: [73][177/408]	Loss 0.0003 (0.0007)	
training:	Epoch: [73][178/408]	Loss 0.0004 (0.0007)	
training:	Epoch: [73][179/408]	Loss 0.0004 (0.0007)	
training:	Epoch: [73][180/408]	Loss 0.0003 (0.0007)	
training:	Epoch: [73][181/408]	Loss 0.0003 (0.0007)	
training:	Epoch: [73][182/408]	Loss 0.0004 (0.0007)	
training:	Epoch: [73][183/408]	Loss 0.0027 (0.0007)	
training:	Epoch: [73][184/408]	Loss 0.0003 (0.0007)	
training:	Epoch: [73][185/408]	Loss 0.0008 (0.0007)	
training:	Epoch: [73][186/408]	Loss 0.0003 (0.0007)	
training:	Epoch: [73][187/408]	Loss 0.0004 (0.0007)	
training:	Epoch: [73][188/408]	Loss 0.0003 (0.0007)	
training:	Epoch: [73][189/408]	Loss 0.0009 (0.0007)	
training:	Epoch: [73][190/408]	Loss 0.0004 (0.0007)	
training:	Epoch: [73][191/408]	Loss 0.0004 (0.0007)	
training:	Epoch: [73][192/408]	Loss 0.0003 (0.0007)	
training:	Epoch: [73][193/408]	Loss 0.0005 (0.0007)	
training:	Epoch: [73][194/408]	Loss 0.0004 (0.0007)	
training:	Epoch: [73][195/408]	Loss 0.0008 (0.0007)	
training:	Epoch: [73][196/408]	Loss 0.0003 (0.0007)	
training:	Epoch: [73][197/408]	Loss 0.0004 (0.0006)	
training:	Epoch: [73][198/408]	Loss 0.0003 (0.0006)	
training:	Epoch: [73][199/408]	Loss 0.0007 (0.0006)	
training:	Epoch: [73][200/408]	Loss 0.0004 (0.0006)	
training:	Epoch: [73][201/408]	Loss 0.0003 (0.0006)	
training:	Epoch: [73][202/408]	Loss 0.0003 (0.0006)	
training:	Epoch: [73][203/408]	Loss 0.0021 (0.0007)	
training:	Epoch: [73][204/408]	Loss 0.0005 (0.0006)	
training:	Epoch: [73][205/408]	Loss 0.0004 (0.0006)	
training:	Epoch: [73][206/408]	Loss 0.0003 (0.0006)	
training:	Epoch: [73][207/408]	Loss 0.0005 (0.0006)	
training:	Epoch: [73][208/408]	Loss 0.0005 (0.0006)	
training:	Epoch: [73][209/408]	Loss 0.0004 (0.0006)	
training:	Epoch: [73][210/408]	Loss 0.0010 (0.0006)	
training:	Epoch: [73][211/408]	Loss 0.0006 (0.0006)	
training:	Epoch: [73][212/408]	Loss 0.0003 (0.0006)	
training:	Epoch: [73][213/408]	Loss 0.0003 (0.0006)	
training:	Epoch: [73][214/408]	Loss 0.0006 (0.0006)	
training:	Epoch: [73][215/408]	Loss 0.0006 (0.0006)	
training:	Epoch: [73][216/408]	Loss 0.0003 (0.0006)	
training:	Epoch: [73][217/408]	Loss 0.0003 (0.0006)	
training:	Epoch: [73][218/408]	Loss 0.0011 (0.0006)	
training:	Epoch: [73][219/408]	Loss 0.0009 (0.0006)	
training:	Epoch: [73][220/408]	Loss 0.0003 (0.0006)	
training:	Epoch: [73][221/408]	Loss 0.0004 (0.0006)	
training:	Epoch: [73][222/408]	Loss 0.0003 (0.0006)	
training:	Epoch: [73][223/408]	Loss 0.0005 (0.0006)	
training:	Epoch: [73][224/408]	Loss 0.0005 (0.0006)	
training:	Epoch: [73][225/408]	Loss 0.0003 (0.0006)	
training:	Epoch: [73][226/408]	Loss 0.0004 (0.0006)	
training:	Epoch: [73][227/408]	Loss 0.0004 (0.0006)	
training:	Epoch: [73][228/408]	Loss 0.0004 (0.0006)	
training:	Epoch: [73][229/408]	Loss 0.0004 (0.0006)	
training:	Epoch: [73][230/408]	Loss 0.0010 (0.0006)	
training:	Epoch: [73][231/408]	Loss 0.0003 (0.0006)	
training:	Epoch: [73][232/408]	Loss 0.0003 (0.0006)	
training:	Epoch: [73][233/408]	Loss 0.0003 (0.0006)	
training:	Epoch: [73][234/408]	Loss 0.0005 (0.0006)	
training:	Epoch: [73][235/408]	Loss 0.0003 (0.0006)	
training:	Epoch: [73][236/408]	Loss 0.0006 (0.0006)	
training:	Epoch: [73][237/408]	Loss 0.0003 (0.0006)	
training:	Epoch: [73][238/408]	Loss 0.0007 (0.0006)	
training:	Epoch: [73][239/408]	Loss 0.0005 (0.0006)	
training:	Epoch: [73][240/408]	Loss 0.0004 (0.0006)	
training:	Epoch: [73][241/408]	Loss 0.0003 (0.0006)	
training:	Epoch: [73][242/408]	Loss 0.0004 (0.0006)	
training:	Epoch: [73][243/408]	Loss 0.0005 (0.0006)	
training:	Epoch: [73][244/408]	Loss 0.0004 (0.0006)	
training:	Epoch: [73][245/408]	Loss 0.0003 (0.0006)	
training:	Epoch: [73][246/408]	Loss 0.0003 (0.0006)	
training:	Epoch: [73][247/408]	Loss 0.0003 (0.0006)	
training:	Epoch: [73][248/408]	Loss 0.0008 (0.0006)	
training:	Epoch: [73][249/408]	Loss 0.0004 (0.0006)	
training:	Epoch: [73][250/408]	Loss 0.0029 (0.0006)	
training:	Epoch: [73][251/408]	Loss 0.0005 (0.0006)	
training:	Epoch: [73][252/408]	Loss 0.0003 (0.0006)	
training:	Epoch: [73][253/408]	Loss 0.0003 (0.0006)	
training:	Epoch: [73][254/408]	Loss 0.0003 (0.0006)	
training:	Epoch: [73][255/408]	Loss 0.0005 (0.0006)	
training:	Epoch: [73][256/408]	Loss 0.0003 (0.0006)	
training:	Epoch: [73][257/408]	Loss 0.0004 (0.0006)	
training:	Epoch: [73][258/408]	Loss 0.0006 (0.0006)	
training:	Epoch: [73][259/408]	Loss 0.0003 (0.0006)	
training:	Epoch: [73][260/408]	Loss 0.0012 (0.0006)	
training:	Epoch: [73][261/408]	Loss 0.0004 (0.0006)	
training:	Epoch: [73][262/408]	Loss 0.0004 (0.0006)	
training:	Epoch: [73][263/408]	Loss 0.0003 (0.0006)	
training:	Epoch: [73][264/408]	Loss 0.0009 (0.0006)	
training:	Epoch: [73][265/408]	Loss 0.0004 (0.0006)	
training:	Epoch: [73][266/408]	Loss 0.0004 (0.0006)	
training:	Epoch: [73][267/408]	Loss 0.0003 (0.0006)	
training:	Epoch: [73][268/408]	Loss 0.0003 (0.0006)	
training:	Epoch: [73][269/408]	Loss 0.0003 (0.0006)	
training:	Epoch: [73][270/408]	Loss 0.0003 (0.0006)	
training:	Epoch: [73][271/408]	Loss 0.0004 (0.0006)	
training:	Epoch: [73][272/408]	Loss 0.0005 (0.0006)	
training:	Epoch: [73][273/408]	Loss 0.0016 (0.0006)	
training:	Epoch: [73][274/408]	Loss 0.0004 (0.0006)	
training:	Epoch: [73][275/408]	Loss 0.0004 (0.0006)	
training:	Epoch: [73][276/408]	Loss 0.0006 (0.0006)	
training:	Epoch: [73][277/408]	Loss 0.0003 (0.0006)	
training:	Epoch: [73][278/408]	Loss 0.0006 (0.0006)	
training:	Epoch: [73][279/408]	Loss 0.0004 (0.0006)	
training:	Epoch: [73][280/408]	Loss 0.0003 (0.0006)	
training:	Epoch: [73][281/408]	Loss 0.0005 (0.0006)	
training:	Epoch: [73][282/408]	Loss 0.0003 (0.0006)	
training:	Epoch: [73][283/408]	Loss 0.0003 (0.0006)	
training:	Epoch: [73][284/408]	Loss 0.0003 (0.0006)	
training:	Epoch: [73][285/408]	Loss 0.0003 (0.0006)	
training:	Epoch: [73][286/408]	Loss 0.0003 (0.0006)	
training:	Epoch: [73][287/408]	Loss 0.0003 (0.0006)	
training:	Epoch: [73][288/408]	Loss 0.0003 (0.0006)	
training:	Epoch: [73][289/408]	Loss 0.0003 (0.0006)	
training:	Epoch: [73][290/408]	Loss 0.0003 (0.0006)	
training:	Epoch: [73][291/408]	Loss 0.0003 (0.0006)	
training:	Epoch: [73][292/408]	Loss 0.0023 (0.0006)	
training:	Epoch: [73][293/408]	Loss 0.0004 (0.0006)	
training:	Epoch: [73][294/408]	Loss 0.0003 (0.0006)	
training:	Epoch: [73][295/408]	Loss 0.0003 (0.0006)	
training:	Epoch: [73][296/408]	Loss 0.0004 (0.0006)	
training:	Epoch: [73][297/408]	Loss 0.0004 (0.0006)	
training:	Epoch: [73][298/408]	Loss 0.0005 (0.0006)	
training:	Epoch: [73][299/408]	Loss 0.0006 (0.0006)	
training:	Epoch: [73][300/408]	Loss 0.0003 (0.0006)	
training:	Epoch: [73][301/408]	Loss 0.0004 (0.0006)	
training:	Epoch: [73][302/408]	Loss 0.0004 (0.0006)	
training:	Epoch: [73][303/408]	Loss 0.0003 (0.0006)	
training:	Epoch: [73][304/408]	Loss 0.0005 (0.0006)	
training:	Epoch: [73][305/408]	Loss 0.0004 (0.0006)	
training:	Epoch: [73][306/408]	Loss 0.0003 (0.0006)	
training:	Epoch: [73][307/408]	Loss 0.0003 (0.0006)	
training:	Epoch: [73][308/408]	Loss 0.0003 (0.0006)	
training:	Epoch: [73][309/408]	Loss 0.0004 (0.0006)	
training:	Epoch: [73][310/408]	Loss 0.0004 (0.0006)	
training:	Epoch: [73][311/408]	Loss 0.0005 (0.0006)	
training:	Epoch: [73][312/408]	Loss 0.0003 (0.0006)	
training:	Epoch: [73][313/408]	Loss 0.0004 (0.0006)	
training:	Epoch: [73][314/408]	Loss 0.0003 (0.0006)	
training:	Epoch: [73][315/408]	Loss 0.0003 (0.0006)	
training:	Epoch: [73][316/408]	Loss 0.0004 (0.0006)	
training:	Epoch: [73][317/408]	Loss 0.0004 (0.0006)	
training:	Epoch: [73][318/408]	Loss 0.0003 (0.0006)	
training:	Epoch: [73][319/408]	Loss 0.0003 (0.0006)	
training:	Epoch: [73][320/408]	Loss 0.0007 (0.0006)	
training:	Epoch: [73][321/408]	Loss 0.0003 (0.0006)	
training:	Epoch: [73][322/408]	Loss 0.0003 (0.0006)	
training:	Epoch: [73][323/408]	Loss 0.0004 (0.0006)	
training:	Epoch: [73][324/408]	Loss 0.0004 (0.0006)	
training:	Epoch: [73][325/408]	Loss 0.0003 (0.0006)	
training:	Epoch: [73][326/408]	Loss 0.0003 (0.0006)	
training:	Epoch: [73][327/408]	Loss 0.0003 (0.0006)	
training:	Epoch: [73][328/408]	Loss 0.0003 (0.0006)	
training:	Epoch: [73][329/408]	Loss 0.0004 (0.0006)	
training:	Epoch: [73][330/408]	Loss 0.0004 (0.0006)	
training:	Epoch: [73][331/408]	Loss 0.0003 (0.0006)	
training:	Epoch: [73][332/408]	Loss 0.0004 (0.0006)	
training:	Epoch: [73][333/408]	Loss 0.0003 (0.0006)	
training:	Epoch: [73][334/408]	Loss 0.0003 (0.0006)	
training:	Epoch: [73][335/408]	Loss 0.0005 (0.0006)	
training:	Epoch: [73][336/408]	Loss 0.0003 (0.0006)	
training:	Epoch: [73][337/408]	Loss 0.0004 (0.0006)	
training:	Epoch: [73][338/408]	Loss 0.0011 (0.0006)	
training:	Epoch: [73][339/408]	Loss 0.0004 (0.0006)	
training:	Epoch: [73][340/408]	Loss 0.0003 (0.0006)	
training:	Epoch: [73][341/408]	Loss 0.0004 (0.0006)	
training:	Epoch: [73][342/408]	Loss 0.0005 (0.0006)	
training:	Epoch: [73][343/408]	Loss 0.0004 (0.0006)	
training:	Epoch: [73][344/408]	Loss 0.0004 (0.0006)	
training:	Epoch: [73][345/408]	Loss 0.0003 (0.0006)	
training:	Epoch: [73][346/408]	Loss 0.0003 (0.0006)	
training:	Epoch: [73][347/408]	Loss 0.0004 (0.0006)	
training:	Epoch: [73][348/408]	Loss 0.0003 (0.0006)	
training:	Epoch: [73][349/408]	Loss 0.0008 (0.0006)	
training:	Epoch: [73][350/408]	Loss 0.0003 (0.0006)	
training:	Epoch: [73][351/408]	Loss 0.0003 (0.0006)	
training:	Epoch: [73][352/408]	Loss 0.0007 (0.0006)	
training:	Epoch: [73][353/408]	Loss 0.0003 (0.0006)	
training:	Epoch: [73][354/408]	Loss 0.0018 (0.0006)	
training:	Epoch: [73][355/408]	Loss 0.0003 (0.0006)	
training:	Epoch: [73][356/408]	Loss 0.0010 (0.0006)	
training:	Epoch: [73][357/408]	Loss 0.0004 (0.0006)	
training:	Epoch: [73][358/408]	Loss 0.0004 (0.0006)	
training:	Epoch: [73][359/408]	Loss 0.0003 (0.0006)	
training:	Epoch: [73][360/408]	Loss 0.0004 (0.0006)	
training:	Epoch: [73][361/408]	Loss 0.0003 (0.0006)	
training:	Epoch: [73][362/408]	Loss 0.0005 (0.0006)	
training:	Epoch: [73][363/408]	Loss 0.0006 (0.0006)	
training:	Epoch: [73][364/408]	Loss 0.0004 (0.0006)	
training:	Epoch: [73][365/408]	Loss 0.0008 (0.0006)	
training:	Epoch: [73][366/408]	Loss 0.0003 (0.0006)	
training:	Epoch: [73][367/408]	Loss 0.0011 (0.0006)	
training:	Epoch: [73][368/408]	Loss 0.0004 (0.0006)	
training:	Epoch: [73][369/408]	Loss 0.0003 (0.0006)	
training:	Epoch: [73][370/408]	Loss 0.0005 (0.0006)	
training:	Epoch: [73][371/408]	Loss 0.0008 (0.0006)	
training:	Epoch: [73][372/408]	Loss 0.0006 (0.0006)	
training:	Epoch: [73][373/408]	Loss 0.0004 (0.0006)	
training:	Epoch: [73][374/408]	Loss 0.0003 (0.0006)	
training:	Epoch: [73][375/408]	Loss 0.0003 (0.0006)	
training:	Epoch: [73][376/408]	Loss 0.0003 (0.0006)	
training:	Epoch: [73][377/408]	Loss 0.0003 (0.0006)	
training:	Epoch: [73][378/408]	Loss 0.0003 (0.0006)	
training:	Epoch: [73][379/408]	Loss 0.0003 (0.0006)	
training:	Epoch: [73][380/408]	Loss 0.0004 (0.0006)	
training:	Epoch: [73][381/408]	Loss 0.0004 (0.0006)	
training:	Epoch: [73][382/408]	Loss 0.0003 (0.0006)	
training:	Epoch: [73][383/408]	Loss 0.0020 (0.0006)	
training:	Epoch: [73][384/408]	Loss 0.0005 (0.0006)	
training:	Epoch: [73][385/408]	Loss 0.0003 (0.0006)	
training:	Epoch: [73][386/408]	Loss 0.0004 (0.0006)	
training:	Epoch: [73][387/408]	Loss 0.0003 (0.0006)	
training:	Epoch: [73][388/408]	Loss 0.0012 (0.0006)	
training:	Epoch: [73][389/408]	Loss 0.0004 (0.0006)	
training:	Epoch: [73][390/408]	Loss 0.0003 (0.0006)	
training:	Epoch: [73][391/408]	Loss 0.0003 (0.0006)	
training:	Epoch: [73][392/408]	Loss 0.0004 (0.0006)	
training:	Epoch: [73][393/408]	Loss 0.0004 (0.0006)	
training:	Epoch: [73][394/408]	Loss 0.0004 (0.0006)	
training:	Epoch: [73][395/408]	Loss 0.0003 (0.0006)	
training:	Epoch: [73][396/408]	Loss 0.0003 (0.0006)	
training:	Epoch: [73][397/408]	Loss 0.0007 (0.0006)	
training:	Epoch: [73][398/408]	Loss 0.0005 (0.0006)	
training:	Epoch: [73][399/408]	Loss 0.0007 (0.0006)	
training:	Epoch: [73][400/408]	Loss 0.0008 (0.0006)	
training:	Epoch: [73][401/408]	Loss 0.0008 (0.0006)	
training:	Epoch: [73][402/408]	Loss 0.0003 (0.0006)	
training:	Epoch: [73][403/408]	Loss 0.0003 (0.0006)	
training:	Epoch: [73][404/408]	Loss 0.0003 (0.0006)	
training:	Epoch: [73][405/408]	Loss 0.0006 (0.0006)	
training:	Epoch: [73][406/408]	Loss 0.0004 (0.0006)	
training:	Epoch: [73][407/408]	Loss 0.0003 (0.0006)	
training:	Epoch: [73][408/408]	Loss 0.0003 (0.0006)	
Training:	 Loss: 0.0006

Training:	 ACC: 1.0000 1.0000 1.0000 1.0000
Validation:	 ACC: 0.7784 0.7790 0.7912 0.7657
Validation:	 Best_BACC: 0.8226 0.8250 0.8772 0.7679
Validation:	 Loss: 1.3228
Pretraining:	Epoch 74/200
----------
training:	Epoch: [74][1/408]	Loss 0.0004 (0.0004)	
training:	Epoch: [74][2/408]	Loss 0.0005 (0.0004)	
training:	Epoch: [74][3/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [74][4/408]	Loss 0.0007 (0.0005)	
training:	Epoch: [74][5/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [74][6/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [74][7/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [74][8/408]	Loss 0.0005 (0.0004)	
training:	Epoch: [74][9/408]	Loss 0.0008 (0.0005)	
training:	Epoch: [74][10/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [74][11/408]	Loss 0.0004 (0.0004)	
training:	Epoch: [74][12/408]	Loss 0.0008 (0.0005)	
training:	Epoch: [74][13/408]	Loss 0.0007 (0.0005)	
training:	Epoch: [74][14/408]	Loss 0.0007 (0.0005)	
training:	Epoch: [74][15/408]	Loss 0.0003 (0.0005)	
training:	Epoch: [74][16/408]	Loss 0.0003 (0.0005)	
training:	Epoch: [74][17/408]	Loss 0.0003 (0.0005)	
training:	Epoch: [74][18/408]	Loss 0.0003 (0.0005)	
training:	Epoch: [74][19/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [74][20/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [74][21/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [74][22/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [74][23/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [74][24/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [74][25/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [74][26/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [74][27/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [74][28/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [74][29/408]	Loss 0.0008 (0.0004)	
training:	Epoch: [74][30/408]	Loss 0.0009 (0.0004)	
training:	Epoch: [74][31/408]	Loss 0.0006 (0.0004)	
training:	Epoch: [74][32/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [74][33/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [74][34/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [74][35/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [74][36/408]	Loss 0.0005 (0.0004)	
training:	Epoch: [74][37/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [74][38/408]	Loss 0.0004 (0.0004)	
training:	Epoch: [74][39/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [74][40/408]	Loss 0.0007 (0.0004)	
training:	Epoch: [74][41/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [74][42/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [74][43/408]	Loss 0.0020 (0.0005)	
training:	Epoch: [74][44/408]	Loss 0.0004 (0.0005)	
training:	Epoch: [74][45/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [74][46/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [74][47/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [74][48/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [74][49/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [74][50/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [74][51/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [74][52/408]	Loss 0.0006 (0.0004)	
training:	Epoch: [74][53/408]	Loss 0.0004 (0.0004)	
training:	Epoch: [74][54/408]	Loss 0.0004 (0.0004)	
training:	Epoch: [74][55/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [74][56/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [74][57/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [74][58/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [74][59/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [74][60/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [74][61/408]	Loss 0.0007 (0.0004)	
training:	Epoch: [74][62/408]	Loss 0.0004 (0.0004)	
training:	Epoch: [74][63/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [74][64/408]	Loss 0.0005 (0.0004)	
training:	Epoch: [74][65/408]	Loss 0.0004 (0.0004)	
training:	Epoch: [74][66/408]	Loss 0.0004 (0.0004)	
training:	Epoch: [74][67/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [74][68/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [74][69/408]	Loss 0.0008 (0.0004)	
training:	Epoch: [74][70/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [74][71/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [74][72/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [74][73/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [74][74/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [74][75/408]	Loss 0.0005 (0.0004)	
training:	Epoch: [74][76/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [74][77/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [74][78/408]	Loss 0.0006 (0.0004)	
training:	Epoch: [74][79/408]	Loss 0.0007 (0.0004)	
training:	Epoch: [74][80/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [74][81/408]	Loss 0.0005 (0.0004)	
training:	Epoch: [74][82/408]	Loss 0.0014 (0.0004)	
training:	Epoch: [74][83/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [74][84/408]	Loss 0.0004 (0.0004)	
training:	Epoch: [74][85/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [74][86/408]	Loss 0.0004 (0.0004)	
training:	Epoch: [74][87/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [74][88/408]	Loss 0.0004 (0.0004)	
training:	Epoch: [74][89/408]	Loss 0.0007 (0.0004)	
training:	Epoch: [74][90/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [74][91/408]	Loss 0.0007 (0.0004)	
training:	Epoch: [74][92/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [74][93/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [74][94/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [74][95/408]	Loss 0.0004 (0.0004)	
training:	Epoch: [74][96/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [74][97/408]	Loss 0.0004 (0.0004)	
training:	Epoch: [74][98/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [74][99/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [74][100/408]	Loss 0.0004 (0.0004)	
training:	Epoch: [74][101/408]	Loss 0.0005 (0.0004)	
training:	Epoch: [74][102/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [74][103/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [74][104/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [74][105/408]	Loss 0.0009 (0.0004)	
training:	Epoch: [74][106/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [74][107/408]	Loss 0.0005 (0.0004)	
training:	Epoch: [74][108/408]	Loss 0.0004 (0.0004)	
training:	Epoch: [74][109/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [74][110/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [74][111/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [74][112/408]	Loss 0.0017 (0.0004)	
training:	Epoch: [74][113/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [74][114/408]	Loss 0.0007 (0.0004)	
training:	Epoch: [74][115/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [74][116/408]	Loss 0.0004 (0.0004)	
training:	Epoch: [74][117/408]	Loss 0.0005 (0.0004)	
training:	Epoch: [74][118/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [74][119/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [74][120/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [74][121/408]	Loss 0.0004 (0.0004)	
training:	Epoch: [74][122/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [74][123/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [74][124/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [74][125/408]	Loss 0.0004 (0.0004)	
training:	Epoch: [74][126/408]	Loss 0.0006 (0.0004)	
training:	Epoch: [74][127/408]	Loss 0.0005 (0.0004)	
training:	Epoch: [74][128/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [74][129/408]	Loss 0.0006 (0.0004)	
training:	Epoch: [74][130/408]	Loss 0.0004 (0.0004)	
training:	Epoch: [74][131/408]	Loss 0.0004 (0.0004)	
training:	Epoch: [74][132/408]	Loss 0.0005 (0.0004)	
training:	Epoch: [74][133/408]	Loss 0.0005 (0.0004)	
training:	Epoch: [74][134/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [74][135/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [74][136/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [74][137/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [74][138/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [74][139/408]	Loss 0.0004 (0.0004)	
training:	Epoch: [74][140/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [74][141/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [74][142/408]	Loss 0.0004 (0.0004)	
training:	Epoch: [74][143/408]	Loss 0.0004 (0.0004)	
training:	Epoch: [74][144/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [74][145/408]	Loss 0.0005 (0.0004)	
training:	Epoch: [74][146/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [74][147/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [74][148/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [74][149/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [74][150/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [74][151/408]	Loss 0.0004 (0.0004)	
training:	Epoch: [74][152/408]	Loss 0.0005 (0.0004)	
training:	Epoch: [74][153/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [74][154/408]	Loss 0.0004 (0.0004)	
training:	Epoch: [74][155/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [74][156/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [74][157/408]	Loss 0.0005 (0.0004)	
training:	Epoch: [74][158/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [74][159/408]	Loss 0.0009 (0.0004)	
training:	Epoch: [74][160/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [74][161/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [74][162/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [74][163/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [74][164/408]	Loss 0.0012 (0.0004)	
training:	Epoch: [74][165/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [74][166/408]	Loss 0.0062 (0.0004)	
training:	Epoch: [74][167/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [74][168/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [74][169/408]	Loss 0.0004 (0.0004)	
training:	Epoch: [74][170/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [74][171/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [74][172/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [74][173/408]	Loss 0.0004 (0.0004)	
training:	Epoch: [74][174/408]	Loss 0.0010 (0.0004)	
training:	Epoch: [74][175/408]	Loss 0.0004 (0.0004)	
training:	Epoch: [74][176/408]	Loss 0.0005 (0.0004)	
training:	Epoch: [74][177/408]	Loss 0.0005 (0.0004)	
training:	Epoch: [74][178/408]	Loss 0.0004 (0.0004)	
training:	Epoch: [74][179/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [74][180/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [74][181/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [74][182/408]	Loss 0.0009 (0.0004)	
training:	Epoch: [74][183/408]	Loss 0.0008 (0.0005)	
training:	Epoch: [74][184/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [74][185/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [74][186/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [74][187/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [74][188/408]	Loss 0.0004 (0.0004)	
training:	Epoch: [74][189/408]	Loss 0.0007 (0.0004)	
training:	Epoch: [74][190/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [74][191/408]	Loss 0.0006 (0.0004)	
training:	Epoch: [74][192/408]	Loss 0.0007 (0.0004)	
training:	Epoch: [74][193/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [74][194/408]	Loss 0.0005 (0.0004)	
training:	Epoch: [74][195/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [74][196/408]	Loss 0.0004 (0.0004)	
training:	Epoch: [74][197/408]	Loss 0.0004 (0.0004)	
training:	Epoch: [74][198/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [74][199/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [74][200/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [74][201/408]	Loss 0.0007 (0.0004)	
training:	Epoch: [74][202/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [74][203/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [74][204/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [74][205/408]	Loss 0.0008 (0.0004)	
training:	Epoch: [74][206/408]	Loss 0.0006 (0.0004)	
training:	Epoch: [74][207/408]	Loss 0.0009 (0.0004)	
training:	Epoch: [74][208/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [74][209/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [74][210/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [74][211/408]	Loss 0.0004 (0.0004)	
training:	Epoch: [74][212/408]	Loss 0.0007 (0.0004)	
training:	Epoch: [74][213/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [74][214/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [74][215/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [74][216/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [74][217/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [74][218/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [74][219/408]	Loss 0.0010 (0.0004)	
training:	Epoch: [74][220/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [74][221/408]	Loss 0.0004 (0.0004)	
training:	Epoch: [74][222/408]	Loss 0.0004 (0.0004)	
training:	Epoch: [74][223/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [74][224/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [74][225/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [74][226/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [74][227/408]	Loss 0.0004 (0.0004)	
training:	Epoch: [74][228/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [74][229/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [74][230/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [74][231/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [74][232/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [74][233/408]	Loss 0.0009 (0.0004)	
training:	Epoch: [74][234/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [74][235/408]	Loss 0.0009 (0.0004)	
training:	Epoch: [74][236/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [74][237/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [74][238/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [74][239/408]	Loss 0.0004 (0.0004)	
training:	Epoch: [74][240/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [74][241/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [74][242/408]	Loss 0.0006 (0.0004)	
training:	Epoch: [74][243/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [74][244/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [74][245/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [74][246/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [74][247/408]	Loss 0.0007 (0.0004)	
training:	Epoch: [74][248/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [74][249/408]	Loss 0.0007 (0.0004)	
training:	Epoch: [74][250/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [74][251/408]	Loss 0.0004 (0.0004)	
training:	Epoch: [74][252/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [74][253/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [74][254/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [74][255/408]	Loss 0.0011 (0.0004)	
training:	Epoch: [74][256/408]	Loss 0.0004 (0.0004)	
training:	Epoch: [74][257/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [74][258/408]	Loss 0.0006 (0.0004)	
training:	Epoch: [74][259/408]	Loss 0.0004 (0.0004)	
training:	Epoch: [74][260/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [74][261/408]	Loss 0.0016 (0.0004)	
training:	Epoch: [74][262/408]	Loss 0.0005 (0.0004)	
training:	Epoch: [74][263/408]	Loss 0.0004 (0.0004)	
training:	Epoch: [74][264/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [74][265/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [74][266/408]	Loss 0.0006 (0.0004)	
training:	Epoch: [74][267/408]	Loss 0.0004 (0.0004)	
training:	Epoch: [74][268/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [74][269/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [74][270/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [74][271/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [74][272/408]	Loss 0.0004 (0.0004)	
training:	Epoch: [74][273/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [74][274/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [74][275/408]	Loss 0.0004 (0.0004)	
training:	Epoch: [74][276/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [74][277/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [74][278/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [74][279/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [74][280/408]	Loss 0.0010 (0.0004)	
training:	Epoch: [74][281/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [74][282/408]	Loss 0.0004 (0.0004)	
training:	Epoch: [74][283/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [74][284/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [74][285/408]	Loss 0.0005 (0.0004)	
training:	Epoch: [74][286/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [74][287/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [74][288/408]	Loss 0.0008 (0.0004)	
training:	Epoch: [74][289/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [74][290/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [74][291/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [74][292/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [74][293/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [74][294/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [74][295/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [74][296/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [74][297/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [74][298/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [74][299/408]	Loss 0.0006 (0.0004)	
training:	Epoch: [74][300/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [74][301/408]	Loss 0.0004 (0.0004)	
training:	Epoch: [74][302/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [74][303/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [74][304/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [74][305/408]	Loss 0.0013 (0.0004)	
training:	Epoch: [74][306/408]	Loss 0.0004 (0.0004)	
training:	Epoch: [74][307/408]	Loss 0.0005 (0.0004)	
training:	Epoch: [74][308/408]	Loss 0.0004 (0.0004)	
training:	Epoch: [74][309/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [74][310/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [74][311/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [74][312/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [74][313/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [74][314/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [74][315/408]	Loss 0.0004 (0.0004)	
training:	Epoch: [74][316/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [74][317/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [74][318/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [74][319/408]	Loss 0.0004 (0.0004)	
training:	Epoch: [74][320/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [74][321/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [74][322/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [74][323/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [74][324/408]	Loss 0.0004 (0.0004)	
training:	Epoch: [74][325/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [74][326/408]	Loss 0.0008 (0.0004)	
training:	Epoch: [74][327/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [74][328/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [74][329/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [74][330/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [74][331/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [74][332/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [74][333/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [74][334/408]	Loss 0.0004 (0.0004)	
training:	Epoch: [74][335/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [74][336/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [74][337/408]	Loss 0.0069 (0.0004)	
training:	Epoch: [74][338/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [74][339/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [74][340/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [74][341/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [74][342/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [74][343/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [74][344/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [74][345/408]	Loss 0.0004 (0.0004)	
training:	Epoch: [74][346/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [74][347/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [74][348/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [74][349/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [74][350/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [74][351/408]	Loss 0.0010 (0.0004)	
training:	Epoch: [74][352/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [74][353/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [74][354/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [74][355/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [74][356/408]	Loss 0.0004 (0.0004)	
training:	Epoch: [74][357/408]	Loss 0.0004 (0.0004)	
training:	Epoch: [74][358/408]	Loss 0.0040 (0.0004)	
training:	Epoch: [74][359/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [74][360/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [74][361/408]	Loss 0.0008 (0.0004)	
training:	Epoch: [74][362/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [74][363/408]	Loss 0.0005 (0.0004)	
training:	Epoch: [74][364/408]	Loss 0.0004 (0.0004)	
training:	Epoch: [74][365/408]	Loss 0.0004 (0.0004)	
training:	Epoch: [74][366/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [74][367/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [74][368/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [74][369/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [74][370/408]	Loss 0.0005 (0.0004)	
training:	Epoch: [74][371/408]	Loss 0.0004 (0.0004)	
training:	Epoch: [74][372/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [74][373/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [74][374/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [74][375/408]	Loss 0.0004 (0.0004)	
training:	Epoch: [74][376/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [74][377/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [74][378/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [74][379/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [74][380/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [74][381/408]	Loss 0.0004 (0.0004)	
training:	Epoch: [74][382/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [74][383/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [74][384/408]	Loss 0.0004 (0.0004)	
training:	Epoch: [74][385/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [74][386/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [74][387/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [74][388/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [74][389/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [74][390/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [74][391/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [74][392/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [74][393/408]	Loss 0.0004 (0.0004)	
training:	Epoch: [74][394/408]	Loss 0.0007 (0.0004)	
training:	Epoch: [74][395/408]	Loss 0.0005 (0.0004)	
training:	Epoch: [74][396/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [74][397/408]	Loss 0.0004 (0.0004)	
training:	Epoch: [74][398/408]	Loss 0.0014 (0.0004)	
training:	Epoch: [74][399/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [74][400/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [74][401/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [74][402/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [74][403/408]	Loss 0.0013 (0.0004)	
training:	Epoch: [74][404/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [74][405/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [74][406/408]	Loss 0.0004 (0.0004)	
training:	Epoch: [74][407/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [74][408/408]	Loss 0.0003 (0.0004)	
Training:	 Loss: 0.0004

Training:	 ACC: 1.0000 1.0000 1.0000 1.0000
Validation:	 ACC: 0.7796 0.7801 0.7912 0.7679
Validation:	 Best_BACC: 0.8226 0.8250 0.8772 0.7679
Validation:	 Loss: 1.3499
Pretraining:	Epoch 75/200
----------
training:	Epoch: [75][1/408]	Loss 0.0003 (0.0003)	
training:	Epoch: [75][2/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [75][3/408]	Loss 0.0007 (0.0004)	
training:	Epoch: [75][4/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [75][5/408]	Loss 0.0007 (0.0004)	
training:	Epoch: [75][6/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [75][7/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [75][8/408]	Loss 0.0004 (0.0004)	
training:	Epoch: [75][9/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [75][10/408]	Loss 0.0007 (0.0004)	
training:	Epoch: [75][11/408]	Loss 0.0005 (0.0004)	
training:	Epoch: [75][12/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [75][13/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [75][14/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [75][15/408]	Loss 0.0007 (0.0004)	
training:	Epoch: [75][16/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [75][17/408]	Loss 0.0006 (0.0004)	
training:	Epoch: [75][18/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [75][19/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [75][20/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [75][21/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [75][22/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [75][23/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [75][24/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [75][25/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [75][26/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [75][27/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [75][28/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [75][29/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [75][30/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [75][31/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [75][32/408]	Loss 0.0004 (0.0004)	
training:	Epoch: [75][33/408]	Loss 0.0005 (0.0004)	
training:	Epoch: [75][34/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [75][35/408]	Loss 0.0012 (0.0004)	
training:	Epoch: [75][36/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [75][37/408]	Loss 0.0005 (0.0004)	
training:	Epoch: [75][38/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [75][39/408]	Loss 0.0005 (0.0004)	
training:	Epoch: [75][40/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [75][41/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [75][42/408]	Loss 0.0005 (0.0004)	
training:	Epoch: [75][43/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [75][44/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [75][45/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [75][46/408]	Loss 0.0004 (0.0004)	
training:	Epoch: [75][47/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [75][48/408]	Loss 0.0004 (0.0004)	
training:	Epoch: [75][49/408]	Loss 0.0004 (0.0004)	
training:	Epoch: [75][50/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [75][51/408]	Loss 0.0004 (0.0004)	
training:	Epoch: [75][52/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [75][53/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [75][54/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [75][55/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [75][56/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [75][57/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [75][58/408]	Loss 0.0006 (0.0004)	
training:	Epoch: [75][59/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [75][60/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [75][61/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [75][62/408]	Loss 0.0004 (0.0004)	
training:	Epoch: [75][63/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [75][64/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [75][65/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [75][66/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [75][67/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [75][68/408]	Loss 0.0008 (0.0004)	
training:	Epoch: [75][69/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [75][70/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [75][71/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [75][72/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [75][73/408]	Loss 0.0010 (0.0004)	
training:	Epoch: [75][74/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [75][75/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [75][76/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [75][77/408]	Loss 0.0007 (0.0004)	
training:	Epoch: [75][78/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [75][79/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [75][80/408]	Loss 0.0007 (0.0004)	
training:	Epoch: [75][81/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [75][82/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [75][83/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [75][84/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [75][85/408]	Loss 0.0005 (0.0004)	
training:	Epoch: [75][86/408]	Loss 0.0008 (0.0004)	
training:	Epoch: [75][87/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [75][88/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [75][89/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [75][90/408]	Loss 0.0005 (0.0004)	
training:	Epoch: [75][91/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [75][92/408]	Loss 0.0008 (0.0004)	
training:	Epoch: [75][93/408]	Loss 0.0004 (0.0004)	
training:	Epoch: [75][94/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [75][95/408]	Loss 0.0008 (0.0004)	
training:	Epoch: [75][96/408]	Loss 0.0008 (0.0004)	
training:	Epoch: [75][97/408]	Loss 0.0005 (0.0004)	
training:	Epoch: [75][98/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [75][99/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [75][100/408]	Loss 0.0008 (0.0004)	
training:	Epoch: [75][101/408]	Loss 0.0004 (0.0004)	
training:	Epoch: [75][102/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [75][103/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [75][104/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [75][105/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [75][106/408]	Loss 0.0005 (0.0004)	
training:	Epoch: [75][107/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [75][108/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [75][109/408]	Loss 0.0007 (0.0004)	
training:	Epoch: [75][110/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [75][111/408]	Loss 0.0004 (0.0004)	
training:	Epoch: [75][112/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [75][113/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [75][114/408]	Loss 0.0007 (0.0004)	
training:	Epoch: [75][115/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [75][116/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [75][117/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [75][118/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [75][119/408]	Loss 0.0004 (0.0004)	
training:	Epoch: [75][120/408]	Loss 0.0014 (0.0004)	
training:	Epoch: [75][121/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [75][122/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [75][123/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [75][124/408]	Loss 0.0008 (0.0004)	
training:	Epoch: [75][125/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [75][126/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [75][127/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [75][128/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [75][129/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [75][130/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [75][131/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [75][132/408]	Loss 0.0004 (0.0004)	
training:	Epoch: [75][133/408]	Loss 0.0004 (0.0004)	
training:	Epoch: [75][134/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [75][135/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [75][136/408]	Loss 0.0007 (0.0004)	
training:	Epoch: [75][137/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [75][138/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [75][139/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [75][140/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [75][141/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [75][142/408]	Loss 0.0004 (0.0004)	
training:	Epoch: [75][143/408]	Loss 0.0004 (0.0004)	
training:	Epoch: [75][144/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [75][145/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [75][146/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [75][147/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [75][148/408]	Loss 0.0006 (0.0004)	
training:	Epoch: [75][149/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [75][150/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [75][151/408]	Loss 0.0004 (0.0004)	
training:	Epoch: [75][152/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [75][153/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [75][154/408]	Loss 0.0005 (0.0004)	
training:	Epoch: [75][155/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [75][156/408]	Loss 0.0005 (0.0004)	
training:	Epoch: [75][157/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [75][158/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [75][159/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [75][160/408]	Loss 0.0005 (0.0004)	
training:	Epoch: [75][161/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [75][162/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [75][163/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [75][164/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [75][165/408]	Loss 0.0004 (0.0004)	
training:	Epoch: [75][166/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [75][167/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [75][168/408]	Loss 0.0135 (0.0005)	
training:	Epoch: [75][169/408]	Loss 0.0002 (0.0005)	
training:	Epoch: [75][170/408]	Loss 0.0003 (0.0005)	
training:	Epoch: [75][171/408]	Loss 0.0003 (0.0005)	
training:	Epoch: [75][172/408]	Loss 0.0003 (0.0005)	
training:	Epoch: [75][173/408]	Loss 0.0002 (0.0005)	
training:	Epoch: [75][174/408]	Loss 0.0004 (0.0005)	
training:	Epoch: [75][175/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [75][176/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [75][177/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [75][178/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [75][179/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [75][180/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [75][181/408]	Loss 0.0004 (0.0004)	
training:	Epoch: [75][182/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [75][183/408]	Loss 0.0004 (0.0004)	
training:	Epoch: [75][184/408]	Loss 0.0004 (0.0004)	
training:	Epoch: [75][185/408]	Loss 0.0013 (0.0004)	
training:	Epoch: [75][186/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [75][187/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [75][188/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [75][189/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [75][190/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [75][191/408]	Loss 0.0004 (0.0004)	
training:	Epoch: [75][192/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [75][193/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [75][194/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [75][195/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [75][196/408]	Loss 0.0007 (0.0004)	
training:	Epoch: [75][197/408]	Loss 0.0004 (0.0004)	
training:	Epoch: [75][198/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [75][199/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [75][200/408]	Loss 0.0004 (0.0004)	
training:	Epoch: [75][201/408]	Loss 0.0004 (0.0004)	
training:	Epoch: [75][202/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [75][203/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [75][204/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [75][205/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [75][206/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [75][207/408]	Loss 0.0006 (0.0004)	
training:	Epoch: [75][208/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [75][209/408]	Loss 0.0006 (0.0004)	
training:	Epoch: [75][210/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [75][211/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [75][212/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [75][213/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [75][214/408]	Loss 0.0013 (0.0004)	
training:	Epoch: [75][215/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [75][216/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [75][217/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [75][218/408]	Loss 0.0006 (0.0004)	
training:	Epoch: [75][219/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [75][220/408]	Loss 0.0012 (0.0004)	
training:	Epoch: [75][221/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [75][222/408]	Loss 0.0004 (0.0004)	
training:	Epoch: [75][223/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [75][224/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [75][225/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [75][226/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [75][227/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [75][228/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [75][229/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [75][230/408]	Loss 0.0010 (0.0004)	
training:	Epoch: [75][231/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [75][232/408]	Loss 0.0015 (0.0004)	
training:	Epoch: [75][233/408]	Loss 0.0006 (0.0004)	
training:	Epoch: [75][234/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [75][235/408]	Loss 0.0004 (0.0004)	
training:	Epoch: [75][236/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [75][237/408]	Loss 0.0015 (0.0004)	
training:	Epoch: [75][238/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [75][239/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [75][240/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [75][241/408]	Loss 0.0007 (0.0004)	
training:	Epoch: [75][242/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [75][243/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [75][244/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [75][245/408]	Loss 0.0006 (0.0004)	
training:	Epoch: [75][246/408]	Loss 0.0010 (0.0004)	
training:	Epoch: [75][247/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [75][248/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [75][249/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [75][250/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [75][251/408]	Loss 0.0010 (0.0004)	
training:	Epoch: [75][252/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [75][253/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [75][254/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [75][255/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [75][256/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [75][257/408]	Loss 0.0009 (0.0004)	
training:	Epoch: [75][258/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [75][259/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [75][260/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [75][261/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [75][262/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [75][263/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [75][264/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [75][265/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [75][266/408]	Loss 0.0004 (0.0004)	
training:	Epoch: [75][267/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [75][268/408]	Loss 0.0005 (0.0004)	
training:	Epoch: [75][269/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [75][270/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [75][271/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [75][272/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [75][273/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [75][274/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [75][275/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [75][276/408]	Loss 0.0007 (0.0004)	
training:	Epoch: [75][277/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [75][278/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [75][279/408]	Loss 0.0004 (0.0004)	
training:	Epoch: [75][280/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [75][281/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [75][282/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [75][283/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [75][284/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [75][285/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [75][286/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [75][287/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [75][288/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [75][289/408]	Loss 0.0011 (0.0004)	
training:	Epoch: [75][290/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [75][291/408]	Loss 0.0004 (0.0004)	
training:	Epoch: [75][292/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [75][293/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [75][294/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [75][295/408]	Loss 0.0008 (0.0004)	
training:	Epoch: [75][296/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [75][297/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [75][298/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [75][299/408]	Loss 0.0004 (0.0004)	
training:	Epoch: [75][300/408]	Loss 0.0006 (0.0004)	
training:	Epoch: [75][301/408]	Loss 0.0010 (0.0004)	
training:	Epoch: [75][302/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [75][303/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [75][304/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [75][305/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [75][306/408]	Loss 0.0005 (0.0004)	
training:	Epoch: [75][307/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [75][308/408]	Loss 0.0005 (0.0004)	
training:	Epoch: [75][309/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [75][310/408]	Loss 0.0007 (0.0004)	
training:	Epoch: [75][311/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [75][312/408]	Loss 0.0004 (0.0004)	
training:	Epoch: [75][313/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [75][314/408]	Loss 0.0006 (0.0004)	
training:	Epoch: [75][315/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [75][316/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [75][317/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [75][318/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [75][319/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [75][320/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [75][321/408]	Loss 0.0017 (0.0004)	
training:	Epoch: [75][322/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [75][323/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [75][324/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [75][325/408]	Loss 0.0007 (0.0004)	
training:	Epoch: [75][326/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [75][327/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [75][328/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [75][329/408]	Loss 0.0004 (0.0004)	
training:	Epoch: [75][330/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [75][331/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [75][332/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [75][333/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [75][334/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [75][335/408]	Loss 0.0004 (0.0004)	
training:	Epoch: [75][336/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [75][337/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [75][338/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [75][339/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [75][340/408]	Loss 0.0024 (0.0004)	
training:	Epoch: [75][341/408]	Loss 0.0012 (0.0004)	
training:	Epoch: [75][342/408]	Loss 0.0014 (0.0004)	
training:	Epoch: [75][343/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [75][344/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [75][345/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [75][346/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [75][347/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [75][348/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [75][349/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [75][350/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [75][351/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [75][352/408]	Loss 0.0004 (0.0004)	
training:	Epoch: [75][353/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [75][354/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [75][355/408]	Loss 0.0006 (0.0004)	
training:	Epoch: [75][356/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [75][357/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [75][358/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [75][359/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [75][360/408]	Loss 0.0004 (0.0004)	
training:	Epoch: [75][361/408]	Loss 0.0004 (0.0004)	
training:	Epoch: [75][362/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [75][363/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [75][364/408]	Loss 0.0007 (0.0004)	
training:	Epoch: [75][365/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [75][366/408]	Loss 0.0007 (0.0004)	
training:	Epoch: [75][367/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [75][368/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [75][369/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [75][370/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [75][371/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [75][372/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [75][373/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [75][374/408]	Loss 0.0006 (0.0004)	
training:	Epoch: [75][375/408]	Loss 0.0005 (0.0004)	
training:	Epoch: [75][376/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [75][377/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [75][378/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [75][379/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [75][380/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [75][381/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [75][382/408]	Loss 0.0006 (0.0004)	
training:	Epoch: [75][383/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [75][384/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [75][385/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [75][386/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [75][387/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [75][388/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [75][389/408]	Loss 0.0004 (0.0004)	
training:	Epoch: [75][390/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [75][391/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [75][392/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [75][393/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [75][394/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [75][395/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [75][396/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [75][397/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [75][398/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [75][399/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [75][400/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [75][401/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [75][402/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [75][403/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [75][404/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [75][405/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [75][406/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [75][407/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [75][408/408]	Loss 0.0002 (0.0004)	
Training:	 Loss: 0.0004

Training:	 ACC: 1.0000 1.0000 1.0000 1.0000
Validation:	 ACC: 0.7783 0.7790 0.7943 0.7623
Validation:	 Best_BACC: 0.8226 0.8250 0.8772 0.7679
Validation:	 Loss: 1.3952
Pretraining:	Epoch 76/200
----------
training:	Epoch: [76][1/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [76][2/408]	Loss 0.0004 (0.0003)	
training:	Epoch: [76][3/408]	Loss 0.0003 (0.0003)	
training:	Epoch: [76][4/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [76][5/408]	Loss 0.0005 (0.0003)	
training:	Epoch: [76][6/408]	Loss 0.0003 (0.0003)	
training:	Epoch: [76][7/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [76][8/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [76][9/408]	Loss 0.0003 (0.0003)	
training:	Epoch: [76][10/408]	Loss 0.0003 (0.0003)	
training:	Epoch: [76][11/408]	Loss 0.0003 (0.0003)	
training:	Epoch: [76][12/408]	Loss 0.0008 (0.0003)	
training:	Epoch: [76][13/408]	Loss 0.0003 (0.0003)	
training:	Epoch: [76][14/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [76][15/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [76][16/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [76][17/408]	Loss 0.0003 (0.0003)	
training:	Epoch: [76][18/408]	Loss 0.0003 (0.0003)	
training:	Epoch: [76][19/408]	Loss 0.0006 (0.0003)	
training:	Epoch: [76][20/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [76][21/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [76][22/408]	Loss 0.0003 (0.0003)	
training:	Epoch: [76][23/408]	Loss 0.0003 (0.0003)	
training:	Epoch: [76][24/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [76][25/408]	Loss 0.0005 (0.0003)	
training:	Epoch: [76][26/408]	Loss 0.0004 (0.0003)	
training:	Epoch: [76][27/408]	Loss 0.0003 (0.0003)	
training:	Epoch: [76][28/408]	Loss 0.0003 (0.0003)	
training:	Epoch: [76][29/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [76][30/408]	Loss 0.0003 (0.0003)	
training:	Epoch: [76][31/408]	Loss 0.0004 (0.0003)	
training:	Epoch: [76][32/408]	Loss 0.0003 (0.0003)	
training:	Epoch: [76][33/408]	Loss 0.0003 (0.0003)	
training:	Epoch: [76][34/408]	Loss 0.0003 (0.0003)	
training:	Epoch: [76][35/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [76][36/408]	Loss 0.0007 (0.0003)	
training:	Epoch: [76][37/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [76][38/408]	Loss 0.0003 (0.0003)	
training:	Epoch: [76][39/408]	Loss 0.0003 (0.0003)	
training:	Epoch: [76][40/408]	Loss 0.0003 (0.0003)	
training:	Epoch: [76][41/408]	Loss 0.0003 (0.0003)	
training:	Epoch: [76][42/408]	Loss 0.0003 (0.0003)	
training:	Epoch: [76][43/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [76][44/408]	Loss 0.0003 (0.0003)	
training:	Epoch: [76][45/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [76][46/408]	Loss 0.0004 (0.0003)	
training:	Epoch: [76][47/408]	Loss 0.0003 (0.0003)	
training:	Epoch: [76][48/408]	Loss 0.0003 (0.0003)	
training:	Epoch: [76][49/408]	Loss 0.0003 (0.0003)	
training:	Epoch: [76][50/408]	Loss 0.0003 (0.0003)	
training:	Epoch: [76][51/408]	Loss 0.0010 (0.0003)	
training:	Epoch: [76][52/408]	Loss 0.0003 (0.0003)	
training:	Epoch: [76][53/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [76][54/408]	Loss 0.0003 (0.0003)	
training:	Epoch: [76][55/408]	Loss 0.0003 (0.0003)	
training:	Epoch: [76][56/408]	Loss 0.0003 (0.0003)	
training:	Epoch: [76][57/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [76][58/408]	Loss 0.0003 (0.0003)	
training:	Epoch: [76][59/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [76][60/408]	Loss 0.0003 (0.0003)	
training:	Epoch: [76][61/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [76][62/408]	Loss 0.0005 (0.0003)	
training:	Epoch: [76][63/408]	Loss 0.0003 (0.0003)	
training:	Epoch: [76][64/408]	Loss 0.0003 (0.0003)	
training:	Epoch: [76][65/408]	Loss 0.0005 (0.0003)	
training:	Epoch: [76][66/408]	Loss 0.0004 (0.0003)	
training:	Epoch: [76][67/408]	Loss 0.0005 (0.0003)	
training:	Epoch: [76][68/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [76][69/408]	Loss 0.0003 (0.0003)	
training:	Epoch: [76][70/408]	Loss 0.0003 (0.0003)	
training:	Epoch: [76][71/408]	Loss 0.0003 (0.0003)	
training:	Epoch: [76][72/408]	Loss 0.0003 (0.0003)	
training:	Epoch: [76][73/408]	Loss 0.0003 (0.0003)	
training:	Epoch: [76][74/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [76][75/408]	Loss 0.0003 (0.0003)	
training:	Epoch: [76][76/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [76][77/408]	Loss 0.0005 (0.0003)	
training:	Epoch: [76][78/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [76][79/408]	Loss 0.0003 (0.0003)	
training:	Epoch: [76][80/408]	Loss 0.0003 (0.0003)	
training:	Epoch: [76][81/408]	Loss 0.0003 (0.0003)	
training:	Epoch: [76][82/408]	Loss 0.0009 (0.0003)	
training:	Epoch: [76][83/408]	Loss 0.0006 (0.0003)	
training:	Epoch: [76][84/408]	Loss 0.0003 (0.0003)	
training:	Epoch: [76][85/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [76][86/408]	Loss 0.0004 (0.0003)	
training:	Epoch: [76][87/408]	Loss 0.0004 (0.0003)	
training:	Epoch: [76][88/408]	Loss 0.0003 (0.0003)	
training:	Epoch: [76][89/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [76][90/408]	Loss 0.0003 (0.0003)	
training:	Epoch: [76][91/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [76][92/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [76][93/408]	Loss 0.0003 (0.0003)	
training:	Epoch: [76][94/408]	Loss 0.0003 (0.0003)	
training:	Epoch: [76][95/408]	Loss 0.0007 (0.0003)	
training:	Epoch: [76][96/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [76][97/408]	Loss 0.0003 (0.0003)	
training:	Epoch: [76][98/408]	Loss 0.0003 (0.0003)	
training:	Epoch: [76][99/408]	Loss 0.0003 (0.0003)	
training:	Epoch: [76][100/408]	Loss 0.0012 (0.0003)	
training:	Epoch: [76][101/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [76][102/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [76][103/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [76][104/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [76][105/408]	Loss 0.0003 (0.0003)	
training:	Epoch: [76][106/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [76][107/408]	Loss 0.0003 (0.0003)	
training:	Epoch: [76][108/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [76][109/408]	Loss 0.0003 (0.0003)	
training:	Epoch: [76][110/408]	Loss 0.0003 (0.0003)	
training:	Epoch: [76][111/408]	Loss 0.0003 (0.0003)	
training:	Epoch: [76][112/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [76][113/408]	Loss 0.0007 (0.0003)	
training:	Epoch: [76][114/408]	Loss 0.0003 (0.0003)	
training:	Epoch: [76][115/408]	Loss 0.0003 (0.0003)	
training:	Epoch: [76][116/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [76][117/408]	Loss 0.0003 (0.0003)	
training:	Epoch: [76][118/408]	Loss 0.0005 (0.0003)	
training:	Epoch: [76][119/408]	Loss 0.0003 (0.0003)	
training:	Epoch: [76][120/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [76][121/408]	Loss 0.0005 (0.0003)	
training:	Epoch: [76][122/408]	Loss 0.0003 (0.0003)	
training:	Epoch: [76][123/408]	Loss 0.0003 (0.0003)	
training:	Epoch: [76][124/408]	Loss 0.0008 (0.0003)	
training:	Epoch: [76][125/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [76][126/408]	Loss 0.0003 (0.0003)	
training:	Epoch: [76][127/408]	Loss 0.0004 (0.0003)	
training:	Epoch: [76][128/408]	Loss 0.0003 (0.0003)	
training:	Epoch: [76][129/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [76][130/408]	Loss 0.0003 (0.0003)	
training:	Epoch: [76][131/408]	Loss 0.0003 (0.0003)	
training:	Epoch: [76][132/408]	Loss 0.0003 (0.0003)	
training:	Epoch: [76][133/408]	Loss 0.0003 (0.0003)	
training:	Epoch: [76][134/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [76][135/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [76][136/408]	Loss 0.0003 (0.0003)	
training:	Epoch: [76][137/408]	Loss 0.0003 (0.0003)	
training:	Epoch: [76][138/408]	Loss 0.0003 (0.0003)	
training:	Epoch: [76][139/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [76][140/408]	Loss 0.0004 (0.0003)	
training:	Epoch: [76][141/408]	Loss 0.0003 (0.0003)	
training:	Epoch: [76][142/408]	Loss 0.0003 (0.0003)	
training:	Epoch: [76][143/408]	Loss 0.0005 (0.0003)	
training:	Epoch: [76][144/408]	Loss 0.0003 (0.0003)	
training:	Epoch: [76][145/408]	Loss 0.0003 (0.0003)	
training:	Epoch: [76][146/408]	Loss 0.0003 (0.0003)	
training:	Epoch: [76][147/408]	Loss 0.0005 (0.0003)	
training:	Epoch: [76][148/408]	Loss 0.0003 (0.0003)	
training:	Epoch: [76][149/408]	Loss 0.0003 (0.0003)	
training:	Epoch: [76][150/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [76][151/408]	Loss 0.0016 (0.0003)	
training:	Epoch: [76][152/408]	Loss 0.0007 (0.0003)	
training:	Epoch: [76][153/408]	Loss 0.0003 (0.0003)	
training:	Epoch: [76][154/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [76][155/408]	Loss 0.0003 (0.0003)	
training:	Epoch: [76][156/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [76][157/408]	Loss 0.0003 (0.0003)	
training:	Epoch: [76][158/408]	Loss 0.0005 (0.0003)	
training:	Epoch: [76][159/408]	Loss 0.0004 (0.0003)	
training:	Epoch: [76][160/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [76][161/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [76][162/408]	Loss 0.0003 (0.0003)	
training:	Epoch: [76][163/408]	Loss 0.0003 (0.0003)	
training:	Epoch: [76][164/408]	Loss 0.0003 (0.0003)	
training:	Epoch: [76][165/408]	Loss 0.0004 (0.0003)	
training:	Epoch: [76][166/408]	Loss 0.0003 (0.0003)	
training:	Epoch: [76][167/408]	Loss 0.0003 (0.0003)	
training:	Epoch: [76][168/408]	Loss 0.0003 (0.0003)	
training:	Epoch: [76][169/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [76][170/408]	Loss 0.0003 (0.0003)	
training:	Epoch: [76][171/408]	Loss 0.0007 (0.0003)	
training:	Epoch: [76][172/408]	Loss 0.0003 (0.0003)	
training:	Epoch: [76][173/408]	Loss 0.0003 (0.0003)	
training:	Epoch: [76][174/408]	Loss 0.0003 (0.0003)	
training:	Epoch: [76][175/408]	Loss 0.0003 (0.0003)	
training:	Epoch: [76][176/408]	Loss 0.0003 (0.0003)	
training:	Epoch: [76][177/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [76][178/408]	Loss 0.0003 (0.0003)	
training:	Epoch: [76][179/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [76][180/408]	Loss 0.0003 (0.0003)	
training:	Epoch: [76][181/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [76][182/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [76][183/408]	Loss 0.0003 (0.0003)	
training:	Epoch: [76][184/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [76][185/408]	Loss 0.0011 (0.0003)	
training:	Epoch: [76][186/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [76][187/408]	Loss 0.0003 (0.0003)	
training:	Epoch: [76][188/408]	Loss 0.0003 (0.0003)	
training:	Epoch: [76][189/408]	Loss 0.0003 (0.0003)	
training:	Epoch: [76][190/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [76][191/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [76][192/408]	Loss 0.0003 (0.0003)	
training:	Epoch: [76][193/408]	Loss 0.0003 (0.0003)	
training:	Epoch: [76][194/408]	Loss 0.0013 (0.0003)	
training:	Epoch: [76][195/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [76][196/408]	Loss 0.0003 (0.0003)	
training:	Epoch: [76][197/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [76][198/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [76][199/408]	Loss 0.0003 (0.0003)	
training:	Epoch: [76][200/408]	Loss 0.0004 (0.0003)	
training:	Epoch: [76][201/408]	Loss 0.0003 (0.0003)	
training:	Epoch: [76][202/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [76][203/408]	Loss 0.0003 (0.0003)	
training:	Epoch: [76][204/408]	Loss 0.0003 (0.0003)	
training:	Epoch: [76][205/408]	Loss 0.0003 (0.0003)	
training:	Epoch: [76][206/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [76][207/408]	Loss 0.0003 (0.0003)	
training:	Epoch: [76][208/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [76][209/408]	Loss 0.0004 (0.0003)	
training:	Epoch: [76][210/408]	Loss 0.0003 (0.0003)	
training:	Epoch: [76][211/408]	Loss 0.0003 (0.0003)	
training:	Epoch: [76][212/408]	Loss 0.0005 (0.0003)	
training:	Epoch: [76][213/408]	Loss 0.0729 (0.0007)	
training:	Epoch: [76][214/408]	Loss 0.0002 (0.0007)	
training:	Epoch: [76][215/408]	Loss 0.0003 (0.0007)	
training:	Epoch: [76][216/408]	Loss 0.0003 (0.0007)	
training:	Epoch: [76][217/408]	Loss 0.0004 (0.0007)	
training:	Epoch: [76][218/408]	Loss 0.0004 (0.0007)	
training:	Epoch: [76][219/408]	Loss 0.0004 (0.0007)	
training:	Epoch: [76][220/408]	Loss 0.0003 (0.0007)	
training:	Epoch: [76][221/408]	Loss 0.0394 (0.0008)	
training:	Epoch: [76][222/408]	Loss 0.0003 (0.0008)	
training:	Epoch: [76][223/408]	Loss 0.0003 (0.0008)	
training:	Epoch: [76][224/408]	Loss 0.0002 (0.0008)	
training:	Epoch: [76][225/408]	Loss 0.0045 (0.0008)	
training:	Epoch: [76][226/408]	Loss 0.0005 (0.0008)	
training:	Epoch: [76][227/408]	Loss 0.0003 (0.0008)	
training:	Epoch: [76][228/408]	Loss 0.0003 (0.0008)	
training:	Epoch: [76][229/408]	Loss 0.0002 (0.0008)	
training:	Epoch: [76][230/408]	Loss 0.0003 (0.0008)	
training:	Epoch: [76][231/408]	Loss 0.0003 (0.0008)	
training:	Epoch: [76][232/408]	Loss 0.0008 (0.0008)	
training:	Epoch: [76][233/408]	Loss 0.0003 (0.0008)	
training:	Epoch: [76][234/408]	Loss 0.0003 (0.0008)	
training:	Epoch: [76][235/408]	Loss 0.0006 (0.0008)	
training:	Epoch: [76][236/408]	Loss 0.0025 (0.0008)	
training:	Epoch: [76][237/408]	Loss 0.0003 (0.0008)	
training:	Epoch: [76][238/408]	Loss 0.0004 (0.0008)	
training:	Epoch: [76][239/408]	Loss 0.0003 (0.0008)	
training:	Epoch: [76][240/408]	Loss 0.0003 (0.0008)	
training:	Epoch: [76][241/408]	Loss 0.0003 (0.0008)	
training:	Epoch: [76][242/408]	Loss 0.0002 (0.0008)	
training:	Epoch: [76][243/408]	Loss 0.0003 (0.0008)	
training:	Epoch: [76][244/408]	Loss 0.0002 (0.0008)	
training:	Epoch: [76][245/408]	Loss 0.0002 (0.0008)	
training:	Epoch: [76][246/408]	Loss 0.0002 (0.0008)	
training:	Epoch: [76][247/408]	Loss 0.0003 (0.0008)	
training:	Epoch: [76][248/408]	Loss 0.0003 (0.0008)	
training:	Epoch: [76][249/408]	Loss 0.0002 (0.0008)	
training:	Epoch: [76][250/408]	Loss 0.0003 (0.0008)	
training:	Epoch: [76][251/408]	Loss 0.0003 (0.0008)	
training:	Epoch: [76][252/408]	Loss 0.0003 (0.0008)	
training:	Epoch: [76][253/408]	Loss 0.0003 (0.0008)	
training:	Epoch: [76][254/408]	Loss 0.0003 (0.0008)	
training:	Epoch: [76][255/408]	Loss 0.0002 (0.0008)	
training:	Epoch: [76][256/408]	Loss 0.0002 (0.0008)	
training:	Epoch: [76][257/408]	Loss 0.0006 (0.0008)	
training:	Epoch: [76][258/408]	Loss 0.0003 (0.0008)	
training:	Epoch: [76][259/408]	Loss 0.0002 (0.0008)	
training:	Epoch: [76][260/408]	Loss 0.0009 (0.0008)	
training:	Epoch: [76][261/408]	Loss 0.0003 (0.0008)	
training:	Epoch: [76][262/408]	Loss 0.0003 (0.0008)	
training:	Epoch: [76][263/408]	Loss 0.0002 (0.0008)	
training:	Epoch: [76][264/408]	Loss 0.0003 (0.0008)	
training:	Epoch: [76][265/408]	Loss 0.0003 (0.0008)	
training:	Epoch: [76][266/408]	Loss 0.0003 (0.0008)	
training:	Epoch: [76][267/408]	Loss 0.0003 (0.0008)	
training:	Epoch: [76][268/408]	Loss 0.0003 (0.0008)	
training:	Epoch: [76][269/408]	Loss 0.0002 (0.0008)	
training:	Epoch: [76][270/408]	Loss 0.0004 (0.0008)	
training:	Epoch: [76][271/408]	Loss 0.0003 (0.0008)	
training:	Epoch: [76][272/408]	Loss 0.0003 (0.0008)	
training:	Epoch: [76][273/408]	Loss 0.0003 (0.0008)	
training:	Epoch: [76][274/408]	Loss 0.0002 (0.0008)	
training:	Epoch: [76][275/408]	Loss 0.0003 (0.0008)	
training:	Epoch: [76][276/408]	Loss 0.0003 (0.0008)	
training:	Epoch: [76][277/408]	Loss 0.0002 (0.0008)	
training:	Epoch: [76][278/408]	Loss 0.0003 (0.0008)	
training:	Epoch: [76][279/408]	Loss 0.0003 (0.0008)	
training:	Epoch: [76][280/408]	Loss 0.0002 (0.0007)	
training:	Epoch: [76][281/408]	Loss 0.0002 (0.0007)	
training:	Epoch: [76][282/408]	Loss 0.0003 (0.0007)	
training:	Epoch: [76][283/408]	Loss 0.0003 (0.0007)	
training:	Epoch: [76][284/408]	Loss 0.0009 (0.0007)	
training:	Epoch: [76][285/408]	Loss 0.0004 (0.0007)	
training:	Epoch: [76][286/408]	Loss 0.0004 (0.0007)	
training:	Epoch: [76][287/408]	Loss 0.0005 (0.0007)	
training:	Epoch: [76][288/408]	Loss 0.0003 (0.0007)	
training:	Epoch: [76][289/408]	Loss 0.0002 (0.0007)	
training:	Epoch: [76][290/408]	Loss 0.0002 (0.0007)	
training:	Epoch: [76][291/408]	Loss 0.0003 (0.0007)	
training:	Epoch: [76][292/408]	Loss 0.0010 (0.0007)	
training:	Epoch: [76][293/408]	Loss 0.0002 (0.0007)	
training:	Epoch: [76][294/408]	Loss 0.0002 (0.0007)	
training:	Epoch: [76][295/408]	Loss 0.0003 (0.0007)	
training:	Epoch: [76][296/408]	Loss 0.0002 (0.0007)	
training:	Epoch: [76][297/408]	Loss 0.0003 (0.0007)	
training:	Epoch: [76][298/408]	Loss 0.0011 (0.0007)	
training:	Epoch: [76][299/408]	Loss 0.0003 (0.0007)	
training:	Epoch: [76][300/408]	Loss 0.0003 (0.0007)	
training:	Epoch: [76][301/408]	Loss 0.0003 (0.0007)	
training:	Epoch: [76][302/408]	Loss 0.0003 (0.0007)	
training:	Epoch: [76][303/408]	Loss 0.0002 (0.0007)	
training:	Epoch: [76][304/408]	Loss 0.0003 (0.0007)	
training:	Epoch: [76][305/408]	Loss 0.0006 (0.0007)	
training:	Epoch: [76][306/408]	Loss 0.0004 (0.0007)	
training:	Epoch: [76][307/408]	Loss 0.0002 (0.0007)	
training:	Epoch: [76][308/408]	Loss 0.0005 (0.0007)	
training:	Epoch: [76][309/408]	Loss 0.0002 (0.0007)	
training:	Epoch: [76][310/408]	Loss 0.0003 (0.0007)	
training:	Epoch: [76][311/408]	Loss 0.0004 (0.0007)	
training:	Epoch: [76][312/408]	Loss 0.0003 (0.0007)	
training:	Epoch: [76][313/408]	Loss 0.0004 (0.0007)	
training:	Epoch: [76][314/408]	Loss 0.0002 (0.0007)	
training:	Epoch: [76][315/408]	Loss 0.0002 (0.0007)	
training:	Epoch: [76][316/408]	Loss 0.0002 (0.0007)	
training:	Epoch: [76][317/408]	Loss 0.0003 (0.0007)	
training:	Epoch: [76][318/408]	Loss 0.0004 (0.0007)	
training:	Epoch: [76][319/408]	Loss 0.0002 (0.0007)	
training:	Epoch: [76][320/408]	Loss 0.0002 (0.0007)	
training:	Epoch: [76][321/408]	Loss 0.0006 (0.0007)	
training:	Epoch: [76][322/408]	Loss 0.0003 (0.0007)	
training:	Epoch: [76][323/408]	Loss 0.0003 (0.0007)	
training:	Epoch: [76][324/408]	Loss 0.0003 (0.0007)	
training:	Epoch: [76][325/408]	Loss 0.0002 (0.0007)	
training:	Epoch: [76][326/408]	Loss 0.0002 (0.0007)	
training:	Epoch: [76][327/408]	Loss 0.0002 (0.0007)	
training:	Epoch: [76][328/408]	Loss 0.0002 (0.0007)	
training:	Epoch: [76][329/408]	Loss 0.0002 (0.0007)	
training:	Epoch: [76][330/408]	Loss 0.0002 (0.0007)	
training:	Epoch: [76][331/408]	Loss 0.0002 (0.0007)	
training:	Epoch: [76][332/408]	Loss 0.0003 (0.0007)	
training:	Epoch: [76][333/408]	Loss 0.0004 (0.0007)	
training:	Epoch: [76][334/408]	Loss 0.0005 (0.0007)	
training:	Epoch: [76][335/408]	Loss 0.0003 (0.0007)	
training:	Epoch: [76][336/408]	Loss 0.0003 (0.0007)	
training:	Epoch: [76][337/408]	Loss 0.0002 (0.0007)	
training:	Epoch: [76][338/408]	Loss 0.0002 (0.0007)	
training:	Epoch: [76][339/408]	Loss 0.0035 (0.0007)	
training:	Epoch: [76][340/408]	Loss 0.0003 (0.0007)	
training:	Epoch: [76][341/408]	Loss 0.0002 (0.0007)	
training:	Epoch: [76][342/408]	Loss 0.0002 (0.0007)	
training:	Epoch: [76][343/408]	Loss 0.0002 (0.0007)	
training:	Epoch: [76][344/408]	Loss 0.0003 (0.0007)	
training:	Epoch: [76][345/408]	Loss 0.0005 (0.0007)	
training:	Epoch: [76][346/408]	Loss 0.0003 (0.0007)	
training:	Epoch: [76][347/408]	Loss 0.0003 (0.0007)	
training:	Epoch: [76][348/408]	Loss 0.0003 (0.0007)	
training:	Epoch: [76][349/408]	Loss 0.0002 (0.0007)	
training:	Epoch: [76][350/408]	Loss 0.0003 (0.0007)	
training:	Epoch: [76][351/408]	Loss 0.0003 (0.0007)	
training:	Epoch: [76][352/408]	Loss 0.0006 (0.0007)	
training:	Epoch: [76][353/408]	Loss 0.0003 (0.0007)	
training:	Epoch: [76][354/408]	Loss 0.0002 (0.0007)	
training:	Epoch: [76][355/408]	Loss 0.0003 (0.0007)	
training:	Epoch: [76][356/408]	Loss 0.0002 (0.0007)	
training:	Epoch: [76][357/408]	Loss 0.0002 (0.0007)	
training:	Epoch: [76][358/408]	Loss 0.0003 (0.0007)	
training:	Epoch: [76][359/408]	Loss 0.0004 (0.0007)	
training:	Epoch: [76][360/408]	Loss 0.0002 (0.0007)	
training:	Epoch: [76][361/408]	Loss 0.0003 (0.0007)	
training:	Epoch: [76][362/408]	Loss 0.0003 (0.0007)	
training:	Epoch: [76][363/408]	Loss 0.0003 (0.0007)	
training:	Epoch: [76][364/408]	Loss 0.0003 (0.0007)	
training:	Epoch: [76][365/408]	Loss 0.0002 (0.0007)	
training:	Epoch: [76][366/408]	Loss 0.0003 (0.0007)	
training:	Epoch: [76][367/408]	Loss 0.0002 (0.0007)	
training:	Epoch: [76][368/408]	Loss 0.0003 (0.0007)	
training:	Epoch: [76][369/408]	Loss 0.0002 (0.0007)	
training:	Epoch: [76][370/408]	Loss 0.0003 (0.0007)	
training:	Epoch: [76][371/408]	Loss 0.0003 (0.0007)	
training:	Epoch: [76][372/408]	Loss 0.0003 (0.0007)	
training:	Epoch: [76][373/408]	Loss 0.0002 (0.0006)	
training:	Epoch: [76][374/408]	Loss 0.0002 (0.0006)	
training:	Epoch: [76][375/408]	Loss 0.0003 (0.0006)	
training:	Epoch: [76][376/408]	Loss 0.0002 (0.0006)	
training:	Epoch: [76][377/408]	Loss 0.0002 (0.0006)	
training:	Epoch: [76][378/408]	Loss 0.0003 (0.0006)	
training:	Epoch: [76][379/408]	Loss 0.0006 (0.0006)	
training:	Epoch: [76][380/408]	Loss 0.0005 (0.0006)	
training:	Epoch: [76][381/408]	Loss 0.0002 (0.0006)	
training:	Epoch: [76][382/408]	Loss 0.0003 (0.0006)	
training:	Epoch: [76][383/408]	Loss 0.0002 (0.0006)	
training:	Epoch: [76][384/408]	Loss 0.0006 (0.0006)	
training:	Epoch: [76][385/408]	Loss 0.0003 (0.0006)	
training:	Epoch: [76][386/408]	Loss 0.0003 (0.0006)	
training:	Epoch: [76][387/408]	Loss 0.0003 (0.0006)	
training:	Epoch: [76][388/408]	Loss 0.0002 (0.0006)	
training:	Epoch: [76][389/408]	Loss 0.0004 (0.0006)	
training:	Epoch: [76][390/408]	Loss 0.0015 (0.0006)	
training:	Epoch: [76][391/408]	Loss 0.0003 (0.0006)	
training:	Epoch: [76][392/408]	Loss 0.0024 (0.0006)	
training:	Epoch: [76][393/408]	Loss 0.0002 (0.0006)	
training:	Epoch: [76][394/408]	Loss 0.0002 (0.0006)	
training:	Epoch: [76][395/408]	Loss 0.0002 (0.0006)	
training:	Epoch: [76][396/408]	Loss 0.0006 (0.0006)	
training:	Epoch: [76][397/408]	Loss 0.0004 (0.0006)	
training:	Epoch: [76][398/408]	Loss 0.0003 (0.0006)	
training:	Epoch: [76][399/408]	Loss 0.0003 (0.0006)	
training:	Epoch: [76][400/408]	Loss 0.0002 (0.0006)	
training:	Epoch: [76][401/408]	Loss 0.0002 (0.0006)	
training:	Epoch: [76][402/408]	Loss 0.0009 (0.0006)	
training:	Epoch: [76][403/408]	Loss 0.0003 (0.0006)	
training:	Epoch: [76][404/408]	Loss 0.0003 (0.0006)	
training:	Epoch: [76][405/408]	Loss 0.0003 (0.0006)	
training:	Epoch: [76][406/408]	Loss 0.0003 (0.0006)	
training:	Epoch: [76][407/408]	Loss 0.0003 (0.0006)	
training:	Epoch: [76][408/408]	Loss 0.0002 (0.0006)	
Training:	 Loss: 0.0006

Training:	 ACC: 1.0000 1.0000 1.0000 1.0000
Validation:	 ACC: 0.7776 0.7785 0.7973 0.7578
Validation:	 Best_BACC: 0.8226 0.8250 0.8772 0.7679
Validation:	 Loss: 1.4335
Pretraining:	Epoch 77/200
----------
training:	Epoch: [77][1/408]	Loss 0.0003 (0.0003)	
training:	Epoch: [77][2/408]	Loss 0.0004 (0.0004)	
training:	Epoch: [77][3/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [77][4/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [77][5/408]	Loss 0.0008 (0.0004)	
training:	Epoch: [77][6/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [77][7/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [77][8/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [77][9/408]	Loss 0.0003 (0.0003)	
training:	Epoch: [77][10/408]	Loss 0.0186 (0.0022)	
training:	Epoch: [77][11/408]	Loss 0.0005 (0.0020)	
training:	Epoch: [77][12/408]	Loss 0.0005 (0.0019)	
training:	Epoch: [77][13/408]	Loss 0.0014 (0.0018)	
training:	Epoch: [77][14/408]	Loss 0.0003 (0.0017)	
training:	Epoch: [77][15/408]	Loss 0.0002 (0.0016)	
training:	Epoch: [77][16/408]	Loss 0.0005 (0.0016)	
training:	Epoch: [77][17/408]	Loss 0.0005 (0.0015)	
training:	Epoch: [77][18/408]	Loss 0.0002 (0.0014)	
training:	Epoch: [77][19/408]	Loss 0.0003 (0.0014)	
training:	Epoch: [77][20/408]	Loss 0.0004 (0.0013)	
training:	Epoch: [77][21/408]	Loss 0.0004 (0.0013)	
training:	Epoch: [77][22/408]	Loss 0.0002 (0.0012)	
training:	Epoch: [77][23/408]	Loss 0.0004 (0.0012)	
training:	Epoch: [77][24/408]	Loss 0.0003 (0.0012)	
training:	Epoch: [77][25/408]	Loss 0.0002 (0.0011)	
training:	Epoch: [77][26/408]	Loss 0.0002 (0.0011)	
training:	Epoch: [77][27/408]	Loss 0.0008 (0.0011)	
training:	Epoch: [77][28/408]	Loss 0.0014 (0.0011)	
training:	Epoch: [77][29/408]	Loss 0.0004 (0.0011)	
training:	Epoch: [77][30/408]	Loss 0.0013 (0.0011)	
training:	Epoch: [77][31/408]	Loss 0.0002 (0.0010)	
training:	Epoch: [77][32/408]	Loss 0.0216 (0.0017)	
training:	Epoch: [77][33/408]	Loss 0.0003 (0.0016)	
training:	Epoch: [77][34/408]	Loss 0.0005 (0.0016)	
training:	Epoch: [77][35/408]	Loss 0.0014 (0.0016)	
training:	Epoch: [77][36/408]	Loss 0.0003 (0.0016)	
training:	Epoch: [77][37/408]	Loss 0.0002 (0.0015)	
training:	Epoch: [77][38/408]	Loss 0.0003 (0.0015)	
training:	Epoch: [77][39/408]	Loss 0.0002 (0.0015)	
training:	Epoch: [77][40/408]	Loss 0.0004 (0.0014)	
training:	Epoch: [77][41/408]	Loss 0.0005 (0.0014)	
training:	Epoch: [77][42/408]	Loss 0.0003 (0.0014)	
training:	Epoch: [77][43/408]	Loss 0.0002 (0.0014)	
training:	Epoch: [77][44/408]	Loss 0.0002 (0.0013)	
training:	Epoch: [77][45/408]	Loss 0.0004 (0.0013)	
training:	Epoch: [77][46/408]	Loss 0.0002 (0.0013)	
training:	Epoch: [77][47/408]	Loss 0.0003 (0.0013)	
training:	Epoch: [77][48/408]	Loss 0.0002 (0.0012)	
training:	Epoch: [77][49/408]	Loss 0.0007 (0.0012)	
training:	Epoch: [77][50/408]	Loss 0.0003 (0.0012)	
training:	Epoch: [77][51/408]	Loss 0.0003 (0.0012)	
training:	Epoch: [77][52/408]	Loss 0.0003 (0.0012)	
training:	Epoch: [77][53/408]	Loss 0.0082 (0.0013)	
training:	Epoch: [77][54/408]	Loss 0.0002 (0.0013)	
training:	Epoch: [77][55/408]	Loss 0.0005 (0.0013)	
training:	Epoch: [77][56/408]	Loss 0.0003 (0.0013)	
training:	Epoch: [77][57/408]	Loss 0.0007 (0.0012)	
training:	Epoch: [77][58/408]	Loss 0.0002 (0.0012)	
training:	Epoch: [77][59/408]	Loss 0.0006 (0.0012)	
training:	Epoch: [77][60/408]	Loss 0.0005 (0.0012)	
training:	Epoch: [77][61/408]	Loss 0.0006 (0.0012)	
training:	Epoch: [77][62/408]	Loss 0.0006 (0.0012)	
training:	Epoch: [77][63/408]	Loss 0.2254 (0.0047)	
training:	Epoch: [77][64/408]	Loss 0.0002 (0.0047)	
training:	Epoch: [77][65/408]	Loss 0.0018 (0.0046)	
training:	Epoch: [77][66/408]	Loss 0.0003 (0.0046)	
training:	Epoch: [77][67/408]	Loss 0.0027 (0.0045)	
training:	Epoch: [77][68/408]	Loss 0.0005 (0.0045)	
training:	Epoch: [77][69/408]	Loss 0.0003 (0.0044)	
training:	Epoch: [77][70/408]	Loss 0.0039 (0.0044)	
training:	Epoch: [77][71/408]	Loss 0.0071 (0.0045)	
training:	Epoch: [77][72/408]	Loss 0.0540 (0.0051)	
training:	Epoch: [77][73/408]	Loss 0.2729 (0.0088)	
training:	Epoch: [77][74/408]	Loss 0.0081 (0.0088)	
training:	Epoch: [77][75/408]	Loss 0.0003 (0.0087)	
training:	Epoch: [77][76/408]	Loss 0.0003 (0.0086)	
training:	Epoch: [77][77/408]	Loss 0.0005 (0.0085)	
training:	Epoch: [77][78/408]	Loss 0.0005 (0.0084)	
training:	Epoch: [77][79/408]	Loss 0.0004 (0.0083)	
training:	Epoch: [77][80/408]	Loss 0.0002 (0.0082)	
training:	Epoch: [77][81/408]	Loss 0.0139 (0.0082)	
training:	Epoch: [77][82/408]	Loss 0.0004 (0.0081)	
training:	Epoch: [77][83/408]	Loss 0.0363 (0.0085)	
training:	Epoch: [77][84/408]	Loss 0.0002 (0.0084)	
training:	Epoch: [77][85/408]	Loss 0.0002 (0.0083)	
training:	Epoch: [77][86/408]	Loss 0.0002 (0.0082)	
training:	Epoch: [77][87/408]	Loss 0.0134 (0.0083)	
training:	Epoch: [77][88/408]	Loss 0.0004 (0.0082)	
training:	Epoch: [77][89/408]	Loss 0.0002 (0.0081)	
training:	Epoch: [77][90/408]	Loss 0.0003 (0.0080)	
training:	Epoch: [77][91/408]	Loss 0.0003 (0.0079)	
training:	Epoch: [77][92/408]	Loss 0.0003 (0.0078)	
training:	Epoch: [77][93/408]	Loss 0.0003 (0.0077)	
training:	Epoch: [77][94/408]	Loss 0.0003 (0.0077)	
training:	Epoch: [77][95/408]	Loss 0.0014 (0.0076)	
training:	Epoch: [77][96/408]	Loss 0.0015 (0.0075)	
training:	Epoch: [77][97/408]	Loss 0.0006 (0.0075)	
training:	Epoch: [77][98/408]	Loss 0.0003 (0.0074)	
training:	Epoch: [77][99/408]	Loss 0.0003 (0.0073)	
training:	Epoch: [77][100/408]	Loss 0.0011 (0.0072)	
training:	Epoch: [77][101/408]	Loss 0.0009 (0.0072)	
training:	Epoch: [77][102/408]	Loss 0.0004 (0.0071)	
training:	Epoch: [77][103/408]	Loss 0.0007 (0.0071)	
training:	Epoch: [77][104/408]	Loss 0.0003 (0.0070)	
training:	Epoch: [77][105/408]	Loss 0.0002 (0.0069)	
training:	Epoch: [77][106/408]	Loss 0.0003 (0.0069)	
training:	Epoch: [77][107/408]	Loss 0.0003 (0.0068)	
training:	Epoch: [77][108/408]	Loss 0.0003 (0.0067)	
training:	Epoch: [77][109/408]	Loss 0.0006 (0.0067)	
training:	Epoch: [77][110/408]	Loss 0.0005 (0.0066)	
training:	Epoch: [77][111/408]	Loss 0.0003 (0.0066)	
training:	Epoch: [77][112/408]	Loss 0.0003 (0.0065)	
training:	Epoch: [77][113/408]	Loss 0.0015 (0.0065)	
training:	Epoch: [77][114/408]	Loss 0.0002 (0.0064)	
training:	Epoch: [77][115/408]	Loss 0.0003 (0.0064)	
training:	Epoch: [77][116/408]	Loss 0.0007 (0.0063)	
training:	Epoch: [77][117/408]	Loss 0.0005 (0.0063)	
training:	Epoch: [77][118/408]	Loss 0.0003 (0.0062)	
training:	Epoch: [77][119/408]	Loss 0.0015 (0.0062)	
training:	Epoch: [77][120/408]	Loss 0.0011 (0.0061)	
training:	Epoch: [77][121/408]	Loss 0.0002 (0.0061)	
training:	Epoch: [77][122/408]	Loss 0.0003 (0.0060)	
training:	Epoch: [77][123/408]	Loss 0.0010 (0.0060)	
training:	Epoch: [77][124/408]	Loss 0.0002 (0.0060)	
training:	Epoch: [77][125/408]	Loss 0.0003 (0.0059)	
training:	Epoch: [77][126/408]	Loss 0.0036 (0.0059)	
training:	Epoch: [77][127/408]	Loss 0.0002 (0.0058)	
training:	Epoch: [77][128/408]	Loss 0.0010 (0.0058)	
training:	Epoch: [77][129/408]	Loss 0.0003 (0.0058)	
training:	Epoch: [77][130/408]	Loss 0.0003 (0.0057)	
training:	Epoch: [77][131/408]	Loss 0.0008 (0.0057)	
training:	Epoch: [77][132/408]	Loss 0.0002 (0.0056)	
training:	Epoch: [77][133/408]	Loss 0.0002 (0.0056)	
training:	Epoch: [77][134/408]	Loss 0.0008 (0.0056)	
training:	Epoch: [77][135/408]	Loss 0.0003 (0.0055)	
training:	Epoch: [77][136/408]	Loss 0.0003 (0.0055)	
training:	Epoch: [77][137/408]	Loss 0.0003 (0.0054)	
training:	Epoch: [77][138/408]	Loss 0.0002 (0.0054)	
training:	Epoch: [77][139/408]	Loss 0.0008 (0.0054)	
training:	Epoch: [77][140/408]	Loss 0.0002 (0.0053)	
training:	Epoch: [77][141/408]	Loss 0.0002 (0.0053)	
training:	Epoch: [77][142/408]	Loss 0.0003 (0.0053)	
training:	Epoch: [77][143/408]	Loss 0.0002 (0.0052)	
training:	Epoch: [77][144/408]	Loss 0.0007 (0.0052)	
training:	Epoch: [77][145/408]	Loss 0.0064 (0.0052)	
training:	Epoch: [77][146/408]	Loss 0.0003 (0.0052)	
training:	Epoch: [77][147/408]	Loss 0.0003 (0.0051)	
training:	Epoch: [77][148/408]	Loss 0.0809 (0.0057)	
training:	Epoch: [77][149/408]	Loss 0.0006 (0.0056)	
training:	Epoch: [77][150/408]	Loss 0.0003 (0.0056)	
training:	Epoch: [77][151/408]	Loss 0.0005 (0.0056)	
training:	Epoch: [77][152/408]	Loss 0.0008 (0.0055)	
training:	Epoch: [77][153/408]	Loss 0.0003 (0.0055)	
training:	Epoch: [77][154/408]	Loss 0.0003 (0.0055)	
training:	Epoch: [77][155/408]	Loss 0.0004 (0.0054)	
training:	Epoch: [77][156/408]	Loss 0.0002 (0.0054)	
training:	Epoch: [77][157/408]	Loss 0.0004 (0.0054)	
training:	Epoch: [77][158/408]	Loss 0.0004 (0.0053)	
training:	Epoch: [77][159/408]	Loss 0.0008 (0.0053)	
training:	Epoch: [77][160/408]	Loss 0.0007 (0.0053)	
training:	Epoch: [77][161/408]	Loss 0.0004 (0.0052)	
training:	Epoch: [77][162/408]	Loss 0.0003 (0.0052)	
training:	Epoch: [77][163/408]	Loss 0.0003 (0.0052)	
training:	Epoch: [77][164/408]	Loss 0.0003 (0.0051)	
training:	Epoch: [77][165/408]	Loss 0.0092 (0.0052)	
training:	Epoch: [77][166/408]	Loss 0.0002 (0.0051)	
training:	Epoch: [77][167/408]	Loss 0.0004 (0.0051)	
training:	Epoch: [77][168/408]	Loss 0.0023 (0.0051)	
training:	Epoch: [77][169/408]	Loss 0.0002 (0.0051)	
training:	Epoch: [77][170/408]	Loss 0.0003 (0.0050)	
training:	Epoch: [77][171/408]	Loss 0.1761 (0.0060)	
training:	Epoch: [77][172/408]	Loss 0.0003 (0.0060)	
training:	Epoch: [77][173/408]	Loss 0.0003 (0.0060)	
training:	Epoch: [77][174/408]	Loss 0.0003 (0.0059)	
training:	Epoch: [77][175/408]	Loss 0.0002 (0.0059)	
training:	Epoch: [77][176/408]	Loss 0.0003 (0.0059)	
training:	Epoch: [77][177/408]	Loss 0.0008 (0.0058)	
training:	Epoch: [77][178/408]	Loss 0.0013 (0.0058)	
training:	Epoch: [77][179/408]	Loss 0.0003 (0.0058)	
training:	Epoch: [77][180/408]	Loss 0.0052 (0.0058)	
training:	Epoch: [77][181/408]	Loss 0.0009 (0.0058)	
training:	Epoch: [77][182/408]	Loss 0.0006 (0.0057)	
training:	Epoch: [77][183/408]	Loss 0.0141 (0.0058)	
training:	Epoch: [77][184/408]	Loss 0.4745 (0.0083)	
training:	Epoch: [77][185/408]	Loss 0.0004 (0.0083)	
training:	Epoch: [77][186/408]	Loss 0.3298 (0.0100)	
training:	Epoch: [77][187/408]	Loss 0.0027 (0.0100)	
training:	Epoch: [77][188/408]	Loss 0.0005 (0.0099)	
training:	Epoch: [77][189/408]	Loss 0.0003 (0.0099)	
training:	Epoch: [77][190/408]	Loss 0.0003 (0.0098)	
training:	Epoch: [77][191/408]	Loss 0.0008 (0.0098)	
training:	Epoch: [77][192/408]	Loss 0.0003 (0.0097)	
training:	Epoch: [77][193/408]	Loss 0.0003 (0.0097)	
training:	Epoch: [77][194/408]	Loss 0.0003 (0.0096)	
training:	Epoch: [77][195/408]	Loss 0.0005 (0.0096)	
training:	Epoch: [77][196/408]	Loss 0.0005 (0.0095)	
training:	Epoch: [77][197/408]	Loss 0.0004 (0.0095)	
training:	Epoch: [77][198/408]	Loss 0.0008 (0.0094)	
training:	Epoch: [77][199/408]	Loss 0.0017 (0.0094)	
training:	Epoch: [77][200/408]	Loss 0.0005 (0.0094)	
training:	Epoch: [77][201/408]	Loss 0.0004 (0.0093)	
training:	Epoch: [77][202/408]	Loss 0.0064 (0.0093)	
training:	Epoch: [77][203/408]	Loss 0.0073 (0.0093)	
training:	Epoch: [77][204/408]	Loss 0.2990 (0.0107)	
training:	Epoch: [77][205/408]	Loss 0.0228 (0.0108)	
training:	Epoch: [77][206/408]	Loss 0.0004 (0.0107)	
training:	Epoch: [77][207/408]	Loss 0.0004 (0.0107)	
training:	Epoch: [77][208/408]	Loss 0.0008 (0.0106)	
training:	Epoch: [77][209/408]	Loss 0.2501 (0.0118)	
training:	Epoch: [77][210/408]	Loss 0.0029 (0.0117)	
training:	Epoch: [77][211/408]	Loss 0.0024 (0.0117)	
training:	Epoch: [77][212/408]	Loss 0.0191 (0.0117)	
training:	Epoch: [77][213/408]	Loss 0.0002 (0.0117)	
training:	Epoch: [77][214/408]	Loss 0.0013 (0.0116)	
training:	Epoch: [77][215/408]	Loss 0.0009 (0.0116)	
training:	Epoch: [77][216/408]	Loss 0.0004 (0.0115)	
training:	Epoch: [77][217/408]	Loss 0.0031 (0.0115)	
training:	Epoch: [77][218/408]	Loss 0.0005 (0.0114)	
training:	Epoch: [77][219/408]	Loss 0.0026 (0.0114)	
training:	Epoch: [77][220/408]	Loss 0.2350 (0.0124)	
training:	Epoch: [77][221/408]	Loss 0.0004 (0.0123)	
training:	Epoch: [77][222/408]	Loss 0.0137 (0.0124)	
training:	Epoch: [77][223/408]	Loss 0.0004 (0.0123)	
training:	Epoch: [77][224/408]	Loss 0.0006 (0.0122)	
training:	Epoch: [77][225/408]	Loss 0.0002 (0.0122)	
training:	Epoch: [77][226/408]	Loss 0.0003 (0.0121)	
training:	Epoch: [77][227/408]	Loss 0.0006 (0.0121)	
training:	Epoch: [77][228/408]	Loss 0.0030 (0.0120)	
training:	Epoch: [77][229/408]	Loss 0.0064 (0.0120)	
training:	Epoch: [77][230/408]	Loss 0.0004 (0.0120)	
training:	Epoch: [77][231/408]	Loss 0.0003 (0.0119)	
training:	Epoch: [77][232/408]	Loss 0.0006 (0.0119)	
training:	Epoch: [77][233/408]	Loss 0.0003 (0.0118)	
training:	Epoch: [77][234/408]	Loss 0.0003 (0.0118)	
training:	Epoch: [77][235/408]	Loss 0.0007 (0.0117)	
training:	Epoch: [77][236/408]	Loss 0.0070 (0.0117)	
training:	Epoch: [77][237/408]	Loss 0.0005 (0.0117)	
training:	Epoch: [77][238/408]	Loss 0.0019 (0.0116)	
training:	Epoch: [77][239/408]	Loss 0.0011 (0.0116)	
training:	Epoch: [77][240/408]	Loss 0.0011 (0.0115)	
training:	Epoch: [77][241/408]	Loss 0.0017 (0.0115)	
training:	Epoch: [77][242/408]	Loss 0.0015 (0.0115)	
training:	Epoch: [77][243/408]	Loss 0.0004 (0.0114)	
training:	Epoch: [77][244/408]	Loss 0.0028 (0.0114)	
training:	Epoch: [77][245/408]	Loss 0.0016 (0.0113)	
training:	Epoch: [77][246/408]	Loss 0.0088 (0.0113)	
training:	Epoch: [77][247/408]	Loss 0.2458 (0.0123)	
training:	Epoch: [77][248/408]	Loss 0.0004 (0.0122)	
training:	Epoch: [77][249/408]	Loss 0.0008 (0.0122)	
training:	Epoch: [77][250/408]	Loss 0.0004 (0.0121)	
training:	Epoch: [77][251/408]	Loss 0.0003 (0.0121)	
training:	Epoch: [77][252/408]	Loss 0.0756 (0.0123)	
training:	Epoch: [77][253/408]	Loss 0.0014 (0.0123)	
training:	Epoch: [77][254/408]	Loss 0.0011 (0.0122)	
training:	Epoch: [77][255/408]	Loss 0.0011 (0.0122)	
training:	Epoch: [77][256/408]	Loss 0.0005 (0.0122)	
training:	Epoch: [77][257/408]	Loss 0.0081 (0.0121)	
training:	Epoch: [77][258/408]	Loss 0.0004 (0.0121)	
training:	Epoch: [77][259/408]	Loss 0.0361 (0.0122)	
training:	Epoch: [77][260/408]	Loss 0.0009 (0.0121)	
training:	Epoch: [77][261/408]	Loss 0.0023 (0.0121)	
training:	Epoch: [77][262/408]	Loss 0.0004 (0.0121)	
training:	Epoch: [77][263/408]	Loss 0.0004 (0.0120)	
training:	Epoch: [77][264/408]	Loss 0.0003 (0.0120)	
training:	Epoch: [77][265/408]	Loss 0.0004 (0.0119)	
training:	Epoch: [77][266/408]	Loss 0.0043 (0.0119)	
training:	Epoch: [77][267/408]	Loss 0.0008 (0.0119)	
training:	Epoch: [77][268/408]	Loss 0.0006 (0.0118)	
training:	Epoch: [77][269/408]	Loss 0.0009 (0.0118)	
training:	Epoch: [77][270/408]	Loss 0.0006 (0.0117)	
training:	Epoch: [77][271/408]	Loss 0.1756 (0.0123)	
training:	Epoch: [77][272/408]	Loss 0.0005 (0.0123)	
training:	Epoch: [77][273/408]	Loss 0.0004 (0.0123)	
training:	Epoch: [77][274/408]	Loss 0.0005 (0.0122)	
training:	Epoch: [77][275/408]	Loss 0.0006 (0.0122)	
training:	Epoch: [77][276/408]	Loss 0.0005 (0.0121)	
training:	Epoch: [77][277/408]	Loss 0.0028 (0.0121)	
training:	Epoch: [77][278/408]	Loss 0.2412 (0.0129)	
training:	Epoch: [77][279/408]	Loss 0.0062 (0.0129)	
training:	Epoch: [77][280/408]	Loss 0.0031 (0.0129)	
training:	Epoch: [77][281/408]	Loss 0.0084 (0.0128)	
training:	Epoch: [77][282/408]	Loss 0.0427 (0.0129)	
training:	Epoch: [77][283/408]	Loss 0.0003 (0.0129)	
training:	Epoch: [77][284/408]	Loss 0.0004 (0.0129)	
training:	Epoch: [77][285/408]	Loss 0.0098 (0.0128)	
training:	Epoch: [77][286/408]	Loss 0.0013 (0.0128)	
training:	Epoch: [77][287/408]	Loss 0.0005 (0.0128)	
training:	Epoch: [77][288/408]	Loss 0.0003 (0.0127)	
training:	Epoch: [77][289/408]	Loss 0.2566 (0.0136)	
training:	Epoch: [77][290/408]	Loss 0.0013 (0.0135)	
training:	Epoch: [77][291/408]	Loss 0.0009 (0.0135)	
training:	Epoch: [77][292/408]	Loss 0.0017 (0.0134)	
training:	Epoch: [77][293/408]	Loss 0.0013 (0.0134)	
training:	Epoch: [77][294/408]	Loss 0.0020 (0.0134)	
training:	Epoch: [77][295/408]	Loss 0.0241 (0.0134)	
training:	Epoch: [77][296/408]	Loss 0.0006 (0.0134)	
training:	Epoch: [77][297/408]	Loss 0.0111 (0.0133)	
training:	Epoch: [77][298/408]	Loss 0.0287 (0.0134)	
training:	Epoch: [77][299/408]	Loss 0.0009 (0.0134)	
training:	Epoch: [77][300/408]	Loss 0.0019 (0.0133)	
training:	Epoch: [77][301/408]	Loss 0.0073 (0.0133)	
training:	Epoch: [77][302/408]	Loss 0.0013 (0.0133)	
training:	Epoch: [77][303/408]	Loss 0.0004 (0.0132)	
training:	Epoch: [77][304/408]	Loss 0.0006 (0.0132)	
training:	Epoch: [77][305/408]	Loss 0.0006 (0.0131)	
training:	Epoch: [77][306/408]	Loss 0.0030 (0.0131)	
training:	Epoch: [77][307/408]	Loss 0.0020 (0.0131)	
training:	Epoch: [77][308/408]	Loss 0.0012 (0.0130)	
training:	Epoch: [77][309/408]	Loss 0.0355 (0.0131)	
training:	Epoch: [77][310/408]	Loss 0.0243 (0.0131)	
training:	Epoch: [77][311/408]	Loss 0.0009 (0.0131)	
training:	Epoch: [77][312/408]	Loss 0.0011 (0.0131)	
training:	Epoch: [77][313/408]	Loss 0.0007 (0.0130)	
training:	Epoch: [77][314/408]	Loss 0.0023 (0.0130)	
training:	Epoch: [77][315/408]	Loss 0.0005 (0.0129)	
training:	Epoch: [77][316/408]	Loss 0.0066 (0.0129)	
training:	Epoch: [77][317/408]	Loss 0.0003 (0.0129)	
training:	Epoch: [77][318/408]	Loss 0.0005 (0.0128)	
training:	Epoch: [77][319/408]	Loss 0.0004 (0.0128)	
training:	Epoch: [77][320/408]	Loss 0.0014 (0.0128)	
training:	Epoch: [77][321/408]	Loss 0.0006 (0.0127)	
training:	Epoch: [77][322/408]	Loss 0.0006 (0.0127)	
training:	Epoch: [77][323/408]	Loss 0.0018 (0.0127)	
training:	Epoch: [77][324/408]	Loss 0.0004 (0.0126)	
training:	Epoch: [77][325/408]	Loss 0.0044 (0.0126)	
training:	Epoch: [77][326/408]	Loss 0.0008 (0.0126)	
training:	Epoch: [77][327/408]	Loss 0.0005 (0.0125)	
training:	Epoch: [77][328/408]	Loss 0.0004 (0.0125)	
training:	Epoch: [77][329/408]	Loss 0.0004 (0.0124)	
training:	Epoch: [77][330/408]	Loss 0.0789 (0.0126)	
training:	Epoch: [77][331/408]	Loss 0.0006 (0.0126)	
training:	Epoch: [77][332/408]	Loss 0.0010 (0.0126)	
training:	Epoch: [77][333/408]	Loss 0.0005 (0.0125)	
training:	Epoch: [77][334/408]	Loss 0.0010 (0.0125)	
training:	Epoch: [77][335/408]	Loss 0.0075 (0.0125)	
training:	Epoch: [77][336/408]	Loss 0.0005 (0.0125)	
training:	Epoch: [77][337/408]	Loss 0.0007 (0.0124)	
training:	Epoch: [77][338/408]	Loss 0.0015 (0.0124)	
training:	Epoch: [77][339/408]	Loss 0.0065 (0.0124)	
training:	Epoch: [77][340/408]	Loss 0.0007 (0.0123)	
training:	Epoch: [77][341/408]	Loss 0.0006 (0.0123)	
training:	Epoch: [77][342/408]	Loss 0.0004 (0.0123)	
training:	Epoch: [77][343/408]	Loss 0.0006 (0.0122)	
training:	Epoch: [77][344/408]	Loss 0.0004 (0.0122)	
training:	Epoch: [77][345/408]	Loss 0.0007 (0.0122)	
training:	Epoch: [77][346/408]	Loss 0.0006 (0.0121)	
training:	Epoch: [77][347/408]	Loss 0.0005 (0.0121)	
training:	Epoch: [77][348/408]	Loss 0.0003 (0.0121)	
training:	Epoch: [77][349/408]	Loss 0.0011 (0.0120)	
training:	Epoch: [77][350/408]	Loss 0.0006 (0.0120)	
training:	Epoch: [77][351/408]	Loss 0.0003 (0.0120)	
training:	Epoch: [77][352/408]	Loss 0.0004 (0.0119)	
training:	Epoch: [77][353/408]	Loss 0.0074 (0.0119)	
training:	Epoch: [77][354/408]	Loss 0.0005 (0.0119)	
training:	Epoch: [77][355/408]	Loss 0.0006 (0.0119)	
training:	Epoch: [77][356/408]	Loss 0.0005 (0.0118)	
training:	Epoch: [77][357/408]	Loss 0.0007 (0.0118)	
training:	Epoch: [77][358/408]	Loss 0.0003 (0.0118)	
training:	Epoch: [77][359/408]	Loss 0.0010 (0.0117)	
training:	Epoch: [77][360/408]	Loss 0.2033 (0.0123)	
training:	Epoch: [77][361/408]	Loss 0.0007 (0.0122)	
training:	Epoch: [77][362/408]	Loss 0.0008 (0.0122)	
training:	Epoch: [77][363/408]	Loss 0.0007 (0.0122)	
training:	Epoch: [77][364/408]	Loss 0.0003 (0.0121)	
training:	Epoch: [77][365/408]	Loss 0.0042 (0.0121)	
training:	Epoch: [77][366/408]	Loss 0.0003 (0.0121)	
training:	Epoch: [77][367/408]	Loss 0.0004 (0.0121)	
training:	Epoch: [77][368/408]	Loss 0.0003 (0.0120)	
training:	Epoch: [77][369/408]	Loss 0.0004 (0.0120)	
training:	Epoch: [77][370/408]	Loss 0.0022 (0.0120)	
training:	Epoch: [77][371/408]	Loss 0.0017 (0.0119)	
training:	Epoch: [77][372/408]	Loss 0.0009 (0.0119)	
training:	Epoch: [77][373/408]	Loss 0.1771 (0.0123)	
training:	Epoch: [77][374/408]	Loss 0.0009 (0.0123)	
training:	Epoch: [77][375/408]	Loss 0.0004 (0.0123)	
training:	Epoch: [77][376/408]	Loss 0.0029 (0.0123)	
training:	Epoch: [77][377/408]	Loss 0.0004 (0.0122)	
training:	Epoch: [77][378/408]	Loss 0.0012 (0.0122)	
training:	Epoch: [77][379/408]	Loss 0.0003 (0.0122)	
training:	Epoch: [77][380/408]	Loss 0.0004 (0.0121)	
training:	Epoch: [77][381/408]	Loss 0.0004 (0.0121)	
training:	Epoch: [77][382/408]	Loss 0.0036 (0.0121)	
training:	Epoch: [77][383/408]	Loss 0.0038 (0.0121)	
training:	Epoch: [77][384/408]	Loss 0.0007 (0.0120)	
training:	Epoch: [77][385/408]	Loss 0.0005 (0.0120)	
training:	Epoch: [77][386/408]	Loss 0.0004 (0.0120)	
training:	Epoch: [77][387/408]	Loss 0.0007 (0.0119)	
training:	Epoch: [77][388/408]	Loss 0.0028 (0.0119)	
training:	Epoch: [77][389/408]	Loss 0.0004 (0.0119)	
training:	Epoch: [77][390/408]	Loss 0.0011 (0.0119)	
training:	Epoch: [77][391/408]	Loss 0.0005 (0.0118)	
training:	Epoch: [77][392/408]	Loss 0.0005 (0.0118)	
training:	Epoch: [77][393/408]	Loss 0.0036 (0.0118)	
training:	Epoch: [77][394/408]	Loss 0.0008 (0.0118)	
training:	Epoch: [77][395/408]	Loss 0.0011 (0.0117)	
training:	Epoch: [77][396/408]	Loss 0.0029 (0.0117)	
training:	Epoch: [77][397/408]	Loss 0.0006 (0.0117)	
training:	Epoch: [77][398/408]	Loss 0.0004 (0.0117)	
training:	Epoch: [77][399/408]	Loss 0.0967 (0.0119)	
training:	Epoch: [77][400/408]	Loss 0.0004 (0.0118)	
training:	Epoch: [77][401/408]	Loss 0.0005 (0.0118)	
training:	Epoch: [77][402/408]	Loss 0.0341 (0.0119)	
training:	Epoch: [77][403/408]	Loss 0.0004 (0.0118)	
training:	Epoch: [77][404/408]	Loss 0.0007 (0.0118)	
training:	Epoch: [77][405/408]	Loss 0.0232 (0.0118)	
training:	Epoch: [77][406/408]	Loss 0.0008 (0.0118)	
training:	Epoch: [77][407/408]	Loss 0.0005 (0.0118)	
training:	Epoch: [77][408/408]	Loss 0.0018 (0.0118)	
Training:	 Loss: 0.0117

Training:	 ACC: 0.9978 0.9979 1.0000 0.9955
Validation:	 ACC: 0.7706 0.7737 0.8383 0.7029
Validation:	 Best_BACC: 0.8226 0.8250 0.8772 0.7679
Validation:	 Loss: 1.2316
Pretraining:	Epoch 78/200
----------
training:	Epoch: [78][1/408]	Loss 0.0006 (0.0006)	
training:	Epoch: [78][2/408]	Loss 0.0016 (0.0011)	
training:	Epoch: [78][3/408]	Loss 0.0162 (0.0061)	
training:	Epoch: [78][4/408]	Loss 0.0004 (0.0047)	
training:	Epoch: [78][5/408]	Loss 0.0024 (0.0042)	
training:	Epoch: [78][6/408]	Loss 0.0093 (0.0051)	
training:	Epoch: [78][7/408]	Loss 0.0016 (0.0046)	
training:	Epoch: [78][8/408]	Loss 0.0019 (0.0042)	
training:	Epoch: [78][9/408]	Loss 0.0018 (0.0040)	
training:	Epoch: [78][10/408]	Loss 0.0046 (0.0040)	
training:	Epoch: [78][11/408]	Loss 0.0041 (0.0040)	
training:	Epoch: [78][12/408]	Loss 0.0011 (0.0038)	
training:	Epoch: [78][13/408]	Loss 0.0009 (0.0036)	
training:	Epoch: [78][14/408]	Loss 0.0009 (0.0034)	
training:	Epoch: [78][15/408]	Loss 0.0026 (0.0033)	
training:	Epoch: [78][16/408]	Loss 0.0005 (0.0032)	
training:	Epoch: [78][17/408]	Loss 0.0006 (0.0030)	
training:	Epoch: [78][18/408]	Loss 0.0010 (0.0029)	
training:	Epoch: [78][19/408]	Loss 0.0011 (0.0028)	
training:	Epoch: [78][20/408]	Loss 0.0004 (0.0027)	
training:	Epoch: [78][21/408]	Loss 0.0008 (0.0026)	
training:	Epoch: [78][22/408]	Loss 0.0008 (0.0025)	
training:	Epoch: [78][23/408]	Loss 0.0010 (0.0024)	
training:	Epoch: [78][24/408]	Loss 0.0011 (0.0024)	
training:	Epoch: [78][25/408]	Loss 0.0002 (0.0023)	
training:	Epoch: [78][26/408]	Loss 0.0012 (0.0023)	
training:	Epoch: [78][27/408]	Loss 0.0014 (0.0022)	
training:	Epoch: [78][28/408]	Loss 0.0008 (0.0022)	
training:	Epoch: [78][29/408]	Loss 0.0123 (0.0025)	
training:	Epoch: [78][30/408]	Loss 0.0006 (0.0025)	
training:	Epoch: [78][31/408]	Loss 0.0013 (0.0024)	
training:	Epoch: [78][32/408]	Loss 0.0006 (0.0024)	
training:	Epoch: [78][33/408]	Loss 0.0031 (0.0024)	
training:	Epoch: [78][34/408]	Loss 0.0004 (0.0023)	
training:	Epoch: [78][35/408]	Loss 0.0009 (0.0023)	
training:	Epoch: [78][36/408]	Loss 0.0006 (0.0022)	
training:	Epoch: [78][37/408]	Loss 0.0004 (0.0022)	
training:	Epoch: [78][38/408]	Loss 0.0006 (0.0022)	
training:	Epoch: [78][39/408]	Loss 0.0007 (0.0021)	
training:	Epoch: [78][40/408]	Loss 0.0009 (0.0021)	
training:	Epoch: [78][41/408]	Loss 0.0009 (0.0021)	
training:	Epoch: [78][42/408]	Loss 0.0007 (0.0020)	
training:	Epoch: [78][43/408]	Loss 0.0003 (0.0020)	
training:	Epoch: [78][44/408]	Loss 0.0005 (0.0020)	
training:	Epoch: [78][45/408]	Loss 0.0018 (0.0019)	
training:	Epoch: [78][46/408]	Loss 0.0004 (0.0019)	
training:	Epoch: [78][47/408]	Loss 0.0021 (0.0019)	
training:	Epoch: [78][48/408]	Loss 0.0007 (0.0019)	
training:	Epoch: [78][49/408]	Loss 0.0008 (0.0019)	
training:	Epoch: [78][50/408]	Loss 0.0011 (0.0019)	
training:	Epoch: [78][51/408]	Loss 0.0005 (0.0018)	
training:	Epoch: [78][52/408]	Loss 0.0004 (0.0018)	
training:	Epoch: [78][53/408]	Loss 0.0004 (0.0018)	
training:	Epoch: [78][54/408]	Loss 0.0007 (0.0018)	
training:	Epoch: [78][55/408]	Loss 0.0008 (0.0017)	
training:	Epoch: [78][56/408]	Loss 0.0013 (0.0017)	
training:	Epoch: [78][57/408]	Loss 0.0004 (0.0017)	
training:	Epoch: [78][58/408]	Loss 0.0004 (0.0017)	
training:	Epoch: [78][59/408]	Loss 0.0006 (0.0017)	
training:	Epoch: [78][60/408]	Loss 0.0004 (0.0016)	
training:	Epoch: [78][61/408]	Loss 0.0351 (0.0022)	
training:	Epoch: [78][62/408]	Loss 0.0049 (0.0022)	
training:	Epoch: [78][63/408]	Loss 0.0004 (0.0022)	
training:	Epoch: [78][64/408]	Loss 0.0003 (0.0022)	
training:	Epoch: [78][65/408]	Loss 0.0004 (0.0021)	
training:	Epoch: [78][66/408]	Loss 0.0005 (0.0021)	
training:	Epoch: [78][67/408]	Loss 0.0022 (0.0021)	
training:	Epoch: [78][68/408]	Loss 0.0008 (0.0021)	
training:	Epoch: [78][69/408]	Loss 0.0003 (0.0021)	
training:	Epoch: [78][70/408]	Loss 0.0003 (0.0021)	
training:	Epoch: [78][71/408]	Loss 0.0006 (0.0020)	
training:	Epoch: [78][72/408]	Loss 0.0003 (0.0020)	
training:	Epoch: [78][73/408]	Loss 0.0004 (0.0020)	
training:	Epoch: [78][74/408]	Loss 0.0004 (0.0020)	
training:	Epoch: [78][75/408]	Loss 0.0007 (0.0019)	
training:	Epoch: [78][76/408]	Loss 0.0003 (0.0019)	
training:	Epoch: [78][77/408]	Loss 0.0003 (0.0019)	
training:	Epoch: [78][78/408]	Loss 0.0004 (0.0019)	
training:	Epoch: [78][79/408]	Loss 0.0010 (0.0019)	
training:	Epoch: [78][80/408]	Loss 0.0006 (0.0019)	
training:	Epoch: [78][81/408]	Loss 0.0005 (0.0018)	
training:	Epoch: [78][82/408]	Loss 0.0032 (0.0019)	
training:	Epoch: [78][83/408]	Loss 0.0106 (0.0020)	
training:	Epoch: [78][84/408]	Loss 0.0004 (0.0019)	
training:	Epoch: [78][85/408]	Loss 0.0003 (0.0019)	
training:	Epoch: [78][86/408]	Loss 0.0009 (0.0019)	
training:	Epoch: [78][87/408]	Loss 0.0006 (0.0019)	
training:	Epoch: [78][88/408]	Loss 0.0003 (0.0019)	
training:	Epoch: [78][89/408]	Loss 0.0003 (0.0019)	
training:	Epoch: [78][90/408]	Loss 0.0004 (0.0018)	
training:	Epoch: [78][91/408]	Loss 0.0006 (0.0018)	
training:	Epoch: [78][92/408]	Loss 0.0009 (0.0018)	
training:	Epoch: [78][93/408]	Loss 0.0003 (0.0018)	
training:	Epoch: [78][94/408]	Loss 0.0006 (0.0018)	
training:	Epoch: [78][95/408]	Loss 0.0006 (0.0018)	
training:	Epoch: [78][96/408]	Loss 0.0005 (0.0018)	
training:	Epoch: [78][97/408]	Loss 0.0011 (0.0018)	
training:	Epoch: [78][98/408]	Loss 0.0003 (0.0017)	
training:	Epoch: [78][99/408]	Loss 0.0002 (0.0017)	
training:	Epoch: [78][100/408]	Loss 0.0005 (0.0017)	
training:	Epoch: [78][101/408]	Loss 0.0010 (0.0017)	
training:	Epoch: [78][102/408]	Loss 0.0045 (0.0017)	
training:	Epoch: [78][103/408]	Loss 0.0003 (0.0017)	
training:	Epoch: [78][104/408]	Loss 0.0027 (0.0017)	
training:	Epoch: [78][105/408]	Loss 0.0018 (0.0017)	
training:	Epoch: [78][106/408]	Loss 0.0009 (0.0017)	
training:	Epoch: [78][107/408]	Loss 0.0003 (0.0017)	
training:	Epoch: [78][108/408]	Loss 0.0010 (0.0017)	
training:	Epoch: [78][109/408]	Loss 0.0005 (0.0017)	
training:	Epoch: [78][110/408]	Loss 0.0865 (0.0025)	
training:	Epoch: [78][111/408]	Loss 0.0009 (0.0025)	
training:	Epoch: [78][112/408]	Loss 0.0003 (0.0024)	
training:	Epoch: [78][113/408]	Loss 0.0005 (0.0024)	
training:	Epoch: [78][114/408]	Loss 0.0004 (0.0024)	
training:	Epoch: [78][115/408]	Loss 0.0003 (0.0024)	
training:	Epoch: [78][116/408]	Loss 0.0006 (0.0024)	
training:	Epoch: [78][117/408]	Loss 0.0003 (0.0023)	
training:	Epoch: [78][118/408]	Loss 0.0005 (0.0023)	
training:	Epoch: [78][119/408]	Loss 0.0004 (0.0023)	
training:	Epoch: [78][120/408]	Loss 0.0011 (0.0023)	
training:	Epoch: [78][121/408]	Loss 0.0010 (0.0023)	
training:	Epoch: [78][122/408]	Loss 0.0004 (0.0023)	
training:	Epoch: [78][123/408]	Loss 0.0005 (0.0023)	
training:	Epoch: [78][124/408]	Loss 0.0095 (0.0023)	
training:	Epoch: [78][125/408]	Loss 0.0203 (0.0025)	
training:	Epoch: [78][126/408]	Loss 0.0003 (0.0025)	
training:	Epoch: [78][127/408]	Loss 0.0007 (0.0024)	
training:	Epoch: [78][128/408]	Loss 0.0026 (0.0024)	
training:	Epoch: [78][129/408]	Loss 0.0005 (0.0024)	
training:	Epoch: [78][130/408]	Loss 0.0007 (0.0024)	
training:	Epoch: [78][131/408]	Loss 0.0009 (0.0024)	
training:	Epoch: [78][132/408]	Loss 0.0003 (0.0024)	
training:	Epoch: [78][133/408]	Loss 0.0192 (0.0025)	
training:	Epoch: [78][134/408]	Loss 0.0003 (0.0025)	
training:	Epoch: [78][135/408]	Loss 0.0007 (0.0025)	
training:	Epoch: [78][136/408]	Loss 0.0008 (0.0025)	
training:	Epoch: [78][137/408]	Loss 0.0006 (0.0025)	
training:	Epoch: [78][138/408]	Loss 0.0015 (0.0024)	
training:	Epoch: [78][139/408]	Loss 0.0003 (0.0024)	
training:	Epoch: [78][140/408]	Loss 0.0014 (0.0024)	
training:	Epoch: [78][141/408]	Loss 0.0007 (0.0024)	
training:	Epoch: [78][142/408]	Loss 0.0004 (0.0024)	
training:	Epoch: [78][143/408]	Loss 0.0014 (0.0024)	
training:	Epoch: [78][144/408]	Loss 0.0006 (0.0024)	
training:	Epoch: [78][145/408]	Loss 0.0006 (0.0024)	
training:	Epoch: [78][146/408]	Loss 0.0005 (0.0024)	
training:	Epoch: [78][147/408]	Loss 0.0004 (0.0023)	
training:	Epoch: [78][148/408]	Loss 0.0004 (0.0023)	
training:	Epoch: [78][149/408]	Loss 0.0004 (0.0023)	
training:	Epoch: [78][150/408]	Loss 0.0003 (0.0023)	
training:	Epoch: [78][151/408]	Loss 0.0006 (0.0023)	
training:	Epoch: [78][152/408]	Loss 0.0013 (0.0023)	
training:	Epoch: [78][153/408]	Loss 0.0005 (0.0023)	
training:	Epoch: [78][154/408]	Loss 0.0020 (0.0023)	
training:	Epoch: [78][155/408]	Loss 0.0005 (0.0023)	
training:	Epoch: [78][156/408]	Loss 0.0003 (0.0022)	
training:	Epoch: [78][157/408]	Loss 0.0184 (0.0023)	
training:	Epoch: [78][158/408]	Loss 0.0004 (0.0023)	
training:	Epoch: [78][159/408]	Loss 0.0025 (0.0023)	
training:	Epoch: [78][160/408]	Loss 0.0003 (0.0023)	
training:	Epoch: [78][161/408]	Loss 0.0003 (0.0023)	
training:	Epoch: [78][162/408]	Loss 0.0030 (0.0023)	
training:	Epoch: [78][163/408]	Loss 0.0003 (0.0023)	
training:	Epoch: [78][164/408]	Loss 0.0007 (0.0023)	
training:	Epoch: [78][165/408]	Loss 0.0004 (0.0023)	
training:	Epoch: [78][166/408]	Loss 0.0015 (0.0023)	
training:	Epoch: [78][167/408]	Loss 0.0003 (0.0023)	
training:	Epoch: [78][168/408]	Loss 0.0004 (0.0023)	
training:	Epoch: [78][169/408]	Loss 0.0007 (0.0022)	
training:	Epoch: [78][170/408]	Loss 0.0008 (0.0022)	
training:	Epoch: [78][171/408]	Loss 0.0007 (0.0022)	
training:	Epoch: [78][172/408]	Loss 0.0004 (0.0022)	
training:	Epoch: [78][173/408]	Loss 0.0004 (0.0022)	
training:	Epoch: [78][174/408]	Loss 0.0005 (0.0022)	
training:	Epoch: [78][175/408]	Loss 0.0014 (0.0022)	
training:	Epoch: [78][176/408]	Loss 0.0003 (0.0022)	
training:	Epoch: [78][177/408]	Loss 0.0002 (0.0022)	
training:	Epoch: [78][178/408]	Loss 0.0003 (0.0022)	
training:	Epoch: [78][179/408]	Loss 0.0003 (0.0021)	
training:	Epoch: [78][180/408]	Loss 0.0003 (0.0021)	
training:	Epoch: [78][181/408]	Loss 0.0002 (0.0021)	
training:	Epoch: [78][182/408]	Loss 0.0007 (0.0021)	
training:	Epoch: [78][183/408]	Loss 0.0007 (0.0021)	
training:	Epoch: [78][184/408]	Loss 0.0108 (0.0022)	
training:	Epoch: [78][185/408]	Loss 0.0006 (0.0022)	
training:	Epoch: [78][186/408]	Loss 0.0093 (0.0022)	
training:	Epoch: [78][187/408]	Loss 0.0004 (0.0022)	
training:	Epoch: [78][188/408]	Loss 0.0003 (0.0022)	
training:	Epoch: [78][189/408]	Loss 0.0003 (0.0022)	
training:	Epoch: [78][190/408]	Loss 0.0003 (0.0022)	
training:	Epoch: [78][191/408]	Loss 0.0004 (0.0021)	
training:	Epoch: [78][192/408]	Loss 0.0009 (0.0021)	
training:	Epoch: [78][193/408]	Loss 0.0007 (0.0021)	
training:	Epoch: [78][194/408]	Loss 0.0005 (0.0021)	
training:	Epoch: [78][195/408]	Loss 0.0018 (0.0021)	
training:	Epoch: [78][196/408]	Loss 0.0003 (0.0021)	
training:	Epoch: [78][197/408]	Loss 0.0004 (0.0021)	
training:	Epoch: [78][198/408]	Loss 0.0003 (0.0021)	
training:	Epoch: [78][199/408]	Loss 0.0003 (0.0021)	
training:	Epoch: [78][200/408]	Loss 0.0009 (0.0021)	
training:	Epoch: [78][201/408]	Loss 0.0004 (0.0021)	
training:	Epoch: [78][202/408]	Loss 0.0006 (0.0021)	
training:	Epoch: [78][203/408]	Loss 0.0003 (0.0021)	
training:	Epoch: [78][204/408]	Loss 0.0004 (0.0020)	
training:	Epoch: [78][205/408]	Loss 0.0008 (0.0020)	
training:	Epoch: [78][206/408]	Loss 0.0005 (0.0020)	
training:	Epoch: [78][207/408]	Loss 0.0004 (0.0020)	
training:	Epoch: [78][208/408]	Loss 0.0008 (0.0020)	
training:	Epoch: [78][209/408]	Loss 0.0041 (0.0020)	
training:	Epoch: [78][210/408]	Loss 0.0004 (0.0020)	
training:	Epoch: [78][211/408]	Loss 0.0004 (0.0020)	
training:	Epoch: [78][212/408]	Loss 0.0054 (0.0020)	
training:	Epoch: [78][213/408]	Loss 0.0007 (0.0020)	
training:	Epoch: [78][214/408]	Loss 0.0002 (0.0020)	
training:	Epoch: [78][215/408]	Loss 0.0006 (0.0020)	
training:	Epoch: [78][216/408]	Loss 0.0154 (0.0021)	
training:	Epoch: [78][217/408]	Loss 0.0006 (0.0021)	
training:	Epoch: [78][218/408]	Loss 0.0003 (0.0021)	
training:	Epoch: [78][219/408]	Loss 0.0006 (0.0020)	
training:	Epoch: [78][220/408]	Loss 0.0020 (0.0020)	
training:	Epoch: [78][221/408]	Loss 0.0003 (0.0020)	
training:	Epoch: [78][222/408]	Loss 0.0248 (0.0021)	
training:	Epoch: [78][223/408]	Loss 0.0009 (0.0021)	
training:	Epoch: [78][224/408]	Loss 0.0005 (0.0021)	
training:	Epoch: [78][225/408]	Loss 0.0007 (0.0021)	
training:	Epoch: [78][226/408]	Loss 0.0004 (0.0021)	
training:	Epoch: [78][227/408]	Loss 0.0003 (0.0021)	
training:	Epoch: [78][228/408]	Loss 0.0004 (0.0021)	
training:	Epoch: [78][229/408]	Loss 0.0002 (0.0021)	
training:	Epoch: [78][230/408]	Loss 0.0018 (0.0021)	
training:	Epoch: [78][231/408]	Loss 0.0004 (0.0021)	
training:	Epoch: [78][232/408]	Loss 0.0002 (0.0021)	
training:	Epoch: [78][233/408]	Loss 0.0004 (0.0021)	
training:	Epoch: [78][234/408]	Loss 0.0005 (0.0021)	
training:	Epoch: [78][235/408]	Loss 0.0030 (0.0021)	
training:	Epoch: [78][236/408]	Loss 0.0004 (0.0021)	
training:	Epoch: [78][237/408]	Loss 0.0004 (0.0020)	
training:	Epoch: [78][238/408]	Loss 0.0007 (0.0020)	
training:	Epoch: [78][239/408]	Loss 0.0003 (0.0020)	
training:	Epoch: [78][240/408]	Loss 0.0008 (0.0020)	
training:	Epoch: [78][241/408]	Loss 0.0003 (0.0020)	
training:	Epoch: [78][242/408]	Loss 0.0003 (0.0020)	
training:	Epoch: [78][243/408]	Loss 0.0006 (0.0020)	
training:	Epoch: [78][244/408]	Loss 0.0004 (0.0020)	
training:	Epoch: [78][245/408]	Loss 0.0207 (0.0021)	
training:	Epoch: [78][246/408]	Loss 0.0006 (0.0021)	
training:	Epoch: [78][247/408]	Loss 0.0003 (0.0021)	
training:	Epoch: [78][248/408]	Loss 0.0003 (0.0021)	
training:	Epoch: [78][249/408]	Loss 0.0024 (0.0021)	
training:	Epoch: [78][250/408]	Loss 0.0003 (0.0021)	
training:	Epoch: [78][251/408]	Loss 0.0005 (0.0020)	
training:	Epoch: [78][252/408]	Loss 0.0004 (0.0020)	
training:	Epoch: [78][253/408]	Loss 0.0002 (0.0020)	
training:	Epoch: [78][254/408]	Loss 0.0050 (0.0020)	
training:	Epoch: [78][255/408]	Loss 0.0003 (0.0020)	
training:	Epoch: [78][256/408]	Loss 0.0016 (0.0020)	
training:	Epoch: [78][257/408]	Loss 0.0004 (0.0020)	
training:	Epoch: [78][258/408]	Loss 0.0003 (0.0020)	
training:	Epoch: [78][259/408]	Loss 0.0008 (0.0020)	
training:	Epoch: [78][260/408]	Loss 0.0002 (0.0020)	
training:	Epoch: [78][261/408]	Loss 0.0003 (0.0020)	
training:	Epoch: [78][262/408]	Loss 0.0003 (0.0020)	
training:	Epoch: [78][263/408]	Loss 0.0007 (0.0020)	
training:	Epoch: [78][264/408]	Loss 0.0003 (0.0020)	
training:	Epoch: [78][265/408]	Loss 0.0003 (0.0020)	
training:	Epoch: [78][266/408]	Loss 0.0003 (0.0020)	
training:	Epoch: [78][267/408]	Loss 0.0004 (0.0020)	
training:	Epoch: [78][268/408]	Loss 0.0004 (0.0020)	
training:	Epoch: [78][269/408]	Loss 0.0002 (0.0020)	
training:	Epoch: [78][270/408]	Loss 0.0004 (0.0020)	
training:	Epoch: [78][271/408]	Loss 0.0015 (0.0020)	
training:	Epoch: [78][272/408]	Loss 0.0119 (0.0020)	
training:	Epoch: [78][273/408]	Loss 0.0004 (0.0020)	
training:	Epoch: [78][274/408]	Loss 0.0004 (0.0020)	
training:	Epoch: [78][275/408]	Loss 0.0062 (0.0020)	
training:	Epoch: [78][276/408]	Loss 0.0003 (0.0020)	
training:	Epoch: [78][277/408]	Loss 0.0003 (0.0020)	
training:	Epoch: [78][278/408]	Loss 0.0002 (0.0020)	
training:	Epoch: [78][279/408]	Loss 0.0027 (0.0020)	
training:	Epoch: [78][280/408]	Loss 0.0009 (0.0020)	
training:	Epoch: [78][281/408]	Loss 0.0002 (0.0020)	
training:	Epoch: [78][282/408]	Loss 0.0007 (0.0020)	
training:	Epoch: [78][283/408]	Loss 0.0010 (0.0020)	
training:	Epoch: [78][284/408]	Loss 0.0032 (0.0020)	
training:	Epoch: [78][285/408]	Loss 0.0005 (0.0020)	
training:	Epoch: [78][286/408]	Loss 0.0011 (0.0020)	
training:	Epoch: [78][287/408]	Loss 0.0003 (0.0019)	
training:	Epoch: [78][288/408]	Loss 0.0005 (0.0019)	
training:	Epoch: [78][289/408]	Loss 0.0004 (0.0019)	
training:	Epoch: [78][290/408]	Loss 0.0003 (0.0019)	
training:	Epoch: [78][291/408]	Loss 0.0008 (0.0019)	
training:	Epoch: [78][292/408]	Loss 0.0003 (0.0019)	
training:	Epoch: [78][293/408]	Loss 0.0003 (0.0019)	
training:	Epoch: [78][294/408]	Loss 0.0003 (0.0019)	
training:	Epoch: [78][295/408]	Loss 0.0014 (0.0019)	
training:	Epoch: [78][296/408]	Loss 0.0017 (0.0019)	
training:	Epoch: [78][297/408]	Loss 0.0003 (0.0019)	
training:	Epoch: [78][298/408]	Loss 0.0002 (0.0019)	
training:	Epoch: [78][299/408]	Loss 0.0002 (0.0019)	
training:	Epoch: [78][300/408]	Loss 0.0002 (0.0019)	
training:	Epoch: [78][301/408]	Loss 0.0004 (0.0019)	
training:	Epoch: [78][302/408]	Loss 0.0003 (0.0019)	
training:	Epoch: [78][303/408]	Loss 0.0003 (0.0019)	
training:	Epoch: [78][304/408]	Loss 0.0010 (0.0019)	
training:	Epoch: [78][305/408]	Loss 0.0002 (0.0019)	
training:	Epoch: [78][306/408]	Loss 0.0037 (0.0019)	
training:	Epoch: [78][307/408]	Loss 0.0003 (0.0019)	
training:	Epoch: [78][308/408]	Loss 0.0004 (0.0019)	
training:	Epoch: [78][309/408]	Loss 0.0005 (0.0019)	
training:	Epoch: [78][310/408]	Loss 0.0003 (0.0018)	
training:	Epoch: [78][311/408]	Loss 0.0030 (0.0019)	
training:	Epoch: [78][312/408]	Loss 0.0003 (0.0018)	
training:	Epoch: [78][313/408]	Loss 0.0004 (0.0018)	
training:	Epoch: [78][314/408]	Loss 0.0007 (0.0018)	
training:	Epoch: [78][315/408]	Loss 0.0003 (0.0018)	
training:	Epoch: [78][316/408]	Loss 0.0003 (0.0018)	
training:	Epoch: [78][317/408]	Loss 0.0017 (0.0018)	
training:	Epoch: [78][318/408]	Loss 0.0003 (0.0018)	
training:	Epoch: [78][319/408]	Loss 0.0003 (0.0018)	
training:	Epoch: [78][320/408]	Loss 0.0003 (0.0018)	
training:	Epoch: [78][321/408]	Loss 0.0003 (0.0018)	
training:	Epoch: [78][322/408]	Loss 0.0002 (0.0018)	
training:	Epoch: [78][323/408]	Loss 0.0003 (0.0018)	
training:	Epoch: [78][324/408]	Loss 0.0003 (0.0018)	
training:	Epoch: [78][325/408]	Loss 0.0011 (0.0018)	
training:	Epoch: [78][326/408]	Loss 0.0013 (0.0018)	
training:	Epoch: [78][327/408]	Loss 0.0003 (0.0018)	
training:	Epoch: [78][328/408]	Loss 0.0004 (0.0018)	
training:	Epoch: [78][329/408]	Loss 0.0003 (0.0018)	
training:	Epoch: [78][330/408]	Loss 0.0004 (0.0018)	
training:	Epoch: [78][331/408]	Loss 0.0004 (0.0018)	
training:	Epoch: [78][332/408]	Loss 0.0003 (0.0018)	
training:	Epoch: [78][333/408]	Loss 0.0007 (0.0018)	
training:	Epoch: [78][334/408]	Loss 0.0003 (0.0018)	
training:	Epoch: [78][335/408]	Loss 0.0003 (0.0018)	
training:	Epoch: [78][336/408]	Loss 0.0003 (0.0018)	
training:	Epoch: [78][337/408]	Loss 0.0007 (0.0017)	
training:	Epoch: [78][338/408]	Loss 0.0003 (0.0017)	
training:	Epoch: [78][339/408]	Loss 0.0009 (0.0017)	
training:	Epoch: [78][340/408]	Loss 0.0003 (0.0017)	
training:	Epoch: [78][341/408]	Loss 0.0003 (0.0017)	
training:	Epoch: [78][342/408]	Loss 0.0003 (0.0017)	
training:	Epoch: [78][343/408]	Loss 0.0003 (0.0017)	
training:	Epoch: [78][344/408]	Loss 0.0040 (0.0017)	
training:	Epoch: [78][345/408]	Loss 0.0010 (0.0017)	
training:	Epoch: [78][346/408]	Loss 0.0003 (0.0017)	
training:	Epoch: [78][347/408]	Loss 0.0003 (0.0017)	
training:	Epoch: [78][348/408]	Loss 0.0003 (0.0017)	
training:	Epoch: [78][349/408]	Loss 0.0003 (0.0017)	
training:	Epoch: [78][350/408]	Loss 0.0009 (0.0017)	
training:	Epoch: [78][351/408]	Loss 0.0006 (0.0017)	
training:	Epoch: [78][352/408]	Loss 0.0117 (0.0017)	
training:	Epoch: [78][353/408]	Loss 0.0003 (0.0017)	
training:	Epoch: [78][354/408]	Loss 0.0003 (0.0017)	
training:	Epoch: [78][355/408]	Loss 0.0002 (0.0017)	
training:	Epoch: [78][356/408]	Loss 0.0016 (0.0017)	
training:	Epoch: [78][357/408]	Loss 0.0005 (0.0017)	
training:	Epoch: [78][358/408]	Loss 0.0003 (0.0017)	
training:	Epoch: [78][359/408]	Loss 0.0003 (0.0017)	
training:	Epoch: [78][360/408]	Loss 0.0003 (0.0017)	
training:	Epoch: [78][361/408]	Loss 0.0006 (0.0017)	
training:	Epoch: [78][362/408]	Loss 0.0003 (0.0017)	
training:	Epoch: [78][363/408]	Loss 0.0003 (0.0017)	
training:	Epoch: [78][364/408]	Loss 0.0005 (0.0017)	
training:	Epoch: [78][365/408]	Loss 0.0007 (0.0017)	
training:	Epoch: [78][366/408]	Loss 0.0162 (0.0017)	
training:	Epoch: [78][367/408]	Loss 0.0003 (0.0017)	
training:	Epoch: [78][368/408]	Loss 0.0040 (0.0017)	
training:	Epoch: [78][369/408]	Loss 0.0004 (0.0017)	
training:	Epoch: [78][370/408]	Loss 0.0005 (0.0017)	
training:	Epoch: [78][371/408]	Loss 0.0003 (0.0017)	
training:	Epoch: [78][372/408]	Loss 0.0008 (0.0017)	
training:	Epoch: [78][373/408]	Loss 0.0009 (0.0017)	
training:	Epoch: [78][374/408]	Loss 0.0004 (0.0017)	
training:	Epoch: [78][375/408]	Loss 0.0003 (0.0017)	
training:	Epoch: [78][376/408]	Loss 0.0008 (0.0017)	
training:	Epoch: [78][377/408]	Loss 0.0005 (0.0017)	
training:	Epoch: [78][378/408]	Loss 0.0008 (0.0017)	
training:	Epoch: [78][379/408]	Loss 0.0007 (0.0017)	
training:	Epoch: [78][380/408]	Loss 0.0020 (0.0017)	
training:	Epoch: [78][381/408]	Loss 0.0005 (0.0017)	
training:	Epoch: [78][382/408]	Loss 0.0002 (0.0017)	
training:	Epoch: [78][383/408]	Loss 0.0002 (0.0017)	
training:	Epoch: [78][384/408]	Loss 0.0004 (0.0017)	
training:	Epoch: [78][385/408]	Loss 0.0009 (0.0017)	
training:	Epoch: [78][386/408]	Loss 0.0050 (0.0017)	
training:	Epoch: [78][387/408]	Loss 0.0004 (0.0017)	
training:	Epoch: [78][388/408]	Loss 0.0002 (0.0017)	
training:	Epoch: [78][389/408]	Loss 0.0003 (0.0017)	
training:	Epoch: [78][390/408]	Loss 0.0005 (0.0017)	
training:	Epoch: [78][391/408]	Loss 0.0003 (0.0017)	
training:	Epoch: [78][392/408]	Loss 0.0018 (0.0017)	
training:	Epoch: [78][393/408]	Loss 0.0004 (0.0017)	
training:	Epoch: [78][394/408]	Loss 0.0006 (0.0017)	
training:	Epoch: [78][395/408]	Loss 0.0006 (0.0017)	
training:	Epoch: [78][396/408]	Loss 0.0012 (0.0017)	
training:	Epoch: [78][397/408]	Loss 0.0003 (0.0017)	
training:	Epoch: [78][398/408]	Loss 0.0119 (0.0017)	
training:	Epoch: [78][399/408]	Loss 0.0002 (0.0017)	
training:	Epoch: [78][400/408]	Loss 0.0002 (0.0017)	
training:	Epoch: [78][401/408]	Loss 0.0016 (0.0017)	
training:	Epoch: [78][402/408]	Loss 0.0003 (0.0017)	
training:	Epoch: [78][403/408]	Loss 0.0019 (0.0017)	
training:	Epoch: [78][404/408]	Loss 0.0003 (0.0017)	
training:	Epoch: [78][405/408]	Loss 0.0006 (0.0017)	
training:	Epoch: [78][406/408]	Loss 0.0004 (0.0017)	
training:	Epoch: [78][407/408]	Loss 0.0003 (0.0017)	
training:	Epoch: [78][408/408]	Loss 0.0005 (0.0017)	
Training:	 Loss: 0.0017

Training:	 ACC: 0.9998 0.9998 1.0000 0.9997
Validation:	 ACC: 0.7785 0.7796 0.8014 0.7556
Validation:	 Best_BACC: 0.8226 0.8250 0.8772 0.7679
Validation:	 Loss: 1.3389
Pretraining:	Epoch 79/200
----------
training:	Epoch: [79][1/408]	Loss 0.0003 (0.0003)	
training:	Epoch: [79][2/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [79][3/408]	Loss 0.0008 (0.0004)	
training:	Epoch: [79][4/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [79][5/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [79][6/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [79][7/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [79][8/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [79][9/408]	Loss 0.0003 (0.0003)	
training:	Epoch: [79][10/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [79][11/408]	Loss 0.0004 (0.0003)	
training:	Epoch: [79][12/408]	Loss 0.0015 (0.0004)	
training:	Epoch: [79][13/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [79][14/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [79][15/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [79][16/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [79][17/408]	Loss 0.0004 (0.0004)	
training:	Epoch: [79][18/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [79][19/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [79][20/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [79][21/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [79][22/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [79][23/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [79][24/408]	Loss 0.0005 (0.0004)	
training:	Epoch: [79][25/408]	Loss 0.0005 (0.0004)	
training:	Epoch: [79][26/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [79][27/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [79][28/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [79][29/408]	Loss 0.0003 (0.0003)	
training:	Epoch: [79][30/408]	Loss 0.0005 (0.0004)	
training:	Epoch: [79][31/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [79][32/408]	Loss 0.0008 (0.0004)	
training:	Epoch: [79][33/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [79][34/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [79][35/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [79][36/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [79][37/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [79][38/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [79][39/408]	Loss 0.0010 (0.0004)	
training:	Epoch: [79][40/408]	Loss 0.0004 (0.0004)	
training:	Epoch: [79][41/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [79][42/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [79][43/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [79][44/408]	Loss 0.0004 (0.0004)	
training:	Epoch: [79][45/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [79][46/408]	Loss 0.0005 (0.0004)	
training:	Epoch: [79][47/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [79][48/408]	Loss 0.0005 (0.0004)	
training:	Epoch: [79][49/408]	Loss 0.0005 (0.0004)	
training:	Epoch: [79][50/408]	Loss 0.0035 (0.0004)	
training:	Epoch: [79][51/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [79][52/408]	Loss 0.0005 (0.0004)	
training:	Epoch: [79][53/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [79][54/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [79][55/408]	Loss 0.0016 (0.0004)	
training:	Epoch: [79][56/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [79][57/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [79][58/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [79][59/408]	Loss 0.0025 (0.0005)	
training:	Epoch: [79][60/408]	Loss 0.0003 (0.0005)	
training:	Epoch: [79][61/408]	Loss 0.2134 (0.0040)	
training:	Epoch: [79][62/408]	Loss 0.0003 (0.0039)	
training:	Epoch: [79][63/408]	Loss 0.0003 (0.0038)	
training:	Epoch: [79][64/408]	Loss 0.0006 (0.0038)	
training:	Epoch: [79][65/408]	Loss 0.0003 (0.0037)	
training:	Epoch: [79][66/408]	Loss 0.0002 (0.0037)	
training:	Epoch: [79][67/408]	Loss 0.0011 (0.0036)	
training:	Epoch: [79][68/408]	Loss 0.0002 (0.0036)	
training:	Epoch: [79][69/408]	Loss 0.0003 (0.0035)	
training:	Epoch: [79][70/408]	Loss 0.0007 (0.0035)	
training:	Epoch: [79][71/408]	Loss 0.0002 (0.0035)	
training:	Epoch: [79][72/408]	Loss 0.0002 (0.0034)	
training:	Epoch: [79][73/408]	Loss 0.0003 (0.0034)	
training:	Epoch: [79][74/408]	Loss 0.0006 (0.0033)	
training:	Epoch: [79][75/408]	Loss 0.0003 (0.0033)	
training:	Epoch: [79][76/408]	Loss 0.0006 (0.0033)	
training:	Epoch: [79][77/408]	Loss 0.0003 (0.0032)	
training:	Epoch: [79][78/408]	Loss 0.0002 (0.0032)	
training:	Epoch: [79][79/408]	Loss 0.0003 (0.0031)	
training:	Epoch: [79][80/408]	Loss 0.0003 (0.0031)	
training:	Epoch: [79][81/408]	Loss 0.0003 (0.0031)	
training:	Epoch: [79][82/408]	Loss 0.0004 (0.0030)	
training:	Epoch: [79][83/408]	Loss 0.0002 (0.0030)	
training:	Epoch: [79][84/408]	Loss 0.0012 (0.0030)	
training:	Epoch: [79][85/408]	Loss 0.0003 (0.0030)	
training:	Epoch: [79][86/408]	Loss 0.0002 (0.0029)	
training:	Epoch: [79][87/408]	Loss 0.0363 (0.0033)	
training:	Epoch: [79][88/408]	Loss 0.0003 (0.0033)	
training:	Epoch: [79][89/408]	Loss 0.0002 (0.0032)	
training:	Epoch: [79][90/408]	Loss 0.0002 (0.0032)	
training:	Epoch: [79][91/408]	Loss 0.0003 (0.0032)	
training:	Epoch: [79][92/408]	Loss 0.0003 (0.0031)	
training:	Epoch: [79][93/408]	Loss 0.0004 (0.0031)	
training:	Epoch: [79][94/408]	Loss 0.0003 (0.0031)	
training:	Epoch: [79][95/408]	Loss 0.0027 (0.0031)	
training:	Epoch: [79][96/408]	Loss 0.0011 (0.0031)	
training:	Epoch: [79][97/408]	Loss 0.0002 (0.0030)	
training:	Epoch: [79][98/408]	Loss 0.0019 (0.0030)	
training:	Epoch: [79][99/408]	Loss 0.0003 (0.0030)	
training:	Epoch: [79][100/408]	Loss 0.0005 (0.0030)	
training:	Epoch: [79][101/408]	Loss 0.0005 (0.0029)	
training:	Epoch: [79][102/408]	Loss 0.0003 (0.0029)	
training:	Epoch: [79][103/408]	Loss 0.0003 (0.0029)	
training:	Epoch: [79][104/408]	Loss 0.0010 (0.0029)	
training:	Epoch: [79][105/408]	Loss 0.0005 (0.0028)	
training:	Epoch: [79][106/408]	Loss 0.0018 (0.0028)	
training:	Epoch: [79][107/408]	Loss 0.0003 (0.0028)	
training:	Epoch: [79][108/408]	Loss 0.0004 (0.0028)	
training:	Epoch: [79][109/408]	Loss 0.0003 (0.0028)	
training:	Epoch: [79][110/408]	Loss 0.0003 (0.0027)	
training:	Epoch: [79][111/408]	Loss 0.0003 (0.0027)	
training:	Epoch: [79][112/408]	Loss 0.0003 (0.0027)	
training:	Epoch: [79][113/408]	Loss 0.0003 (0.0027)	
training:	Epoch: [79][114/408]	Loss 0.0004 (0.0027)	
training:	Epoch: [79][115/408]	Loss 0.0003 (0.0026)	
training:	Epoch: [79][116/408]	Loss 0.0002 (0.0026)	
training:	Epoch: [79][117/408]	Loss 0.0003 (0.0026)	
training:	Epoch: [79][118/408]	Loss 0.0002 (0.0026)	
training:	Epoch: [79][119/408]	Loss 0.0007 (0.0026)	
training:	Epoch: [79][120/408]	Loss 0.0564 (0.0030)	
training:	Epoch: [79][121/408]	Loss 0.0004 (0.0030)	
training:	Epoch: [79][122/408]	Loss 0.0002 (0.0030)	
training:	Epoch: [79][123/408]	Loss 0.0003 (0.0029)	
training:	Epoch: [79][124/408]	Loss 0.0009 (0.0029)	
training:	Epoch: [79][125/408]	Loss 0.0003 (0.0029)	
training:	Epoch: [79][126/408]	Loss 0.0002 (0.0029)	
training:	Epoch: [79][127/408]	Loss 0.0003 (0.0029)	
training:	Epoch: [79][128/408]	Loss 0.0003 (0.0028)	
training:	Epoch: [79][129/408]	Loss 0.0003 (0.0028)	
training:	Epoch: [79][130/408]	Loss 0.0006 (0.0028)	
training:	Epoch: [79][131/408]	Loss 0.0013 (0.0028)	
training:	Epoch: [79][132/408]	Loss 0.0003 (0.0028)	
training:	Epoch: [79][133/408]	Loss 0.0030 (0.0028)	
training:	Epoch: [79][134/408]	Loss 0.0003 (0.0028)	
training:	Epoch: [79][135/408]	Loss 0.0004 (0.0027)	
training:	Epoch: [79][136/408]	Loss 0.0004 (0.0027)	
training:	Epoch: [79][137/408]	Loss 0.0007 (0.0027)	
training:	Epoch: [79][138/408]	Loss 0.0012 (0.0027)	
training:	Epoch: [79][139/408]	Loss 0.0005 (0.0027)	
training:	Epoch: [79][140/408]	Loss 0.0002 (0.0027)	
training:	Epoch: [79][141/408]	Loss 0.0017 (0.0027)	
training:	Epoch: [79][142/408]	Loss 0.0004 (0.0026)	
training:	Epoch: [79][143/408]	Loss 0.0004 (0.0026)	
training:	Epoch: [79][144/408]	Loss 0.0003 (0.0026)	
training:	Epoch: [79][145/408]	Loss 0.0003 (0.0026)	
training:	Epoch: [79][146/408]	Loss 0.0005 (0.0026)	
training:	Epoch: [79][147/408]	Loss 0.0012 (0.0026)	
training:	Epoch: [79][148/408]	Loss 0.0003 (0.0026)	
training:	Epoch: [79][149/408]	Loss 0.0002 (0.0025)	
training:	Epoch: [79][150/408]	Loss 0.0003 (0.0025)	
training:	Epoch: [79][151/408]	Loss 0.0008 (0.0025)	
training:	Epoch: [79][152/408]	Loss 0.0003 (0.0025)	
training:	Epoch: [79][153/408]	Loss 0.0006 (0.0025)	
training:	Epoch: [79][154/408]	Loss 0.0003 (0.0025)	
training:	Epoch: [79][155/408]	Loss 0.0003 (0.0025)	
training:	Epoch: [79][156/408]	Loss 0.0005 (0.0024)	
training:	Epoch: [79][157/408]	Loss 0.0003 (0.0024)	
training:	Epoch: [79][158/408]	Loss 0.0002 (0.0024)	
training:	Epoch: [79][159/408]	Loss 0.0010 (0.0024)	
training:	Epoch: [79][160/408]	Loss 0.0003 (0.0024)	
training:	Epoch: [79][161/408]	Loss 0.0002 (0.0024)	
training:	Epoch: [79][162/408]	Loss 0.0003 (0.0024)	
training:	Epoch: [79][163/408]	Loss 0.0006 (0.0024)	
training:	Epoch: [79][164/408]	Loss 0.0003 (0.0023)	
training:	Epoch: [79][165/408]	Loss 0.0004 (0.0023)	
training:	Epoch: [79][166/408]	Loss 0.0002 (0.0023)	
training:	Epoch: [79][167/408]	Loss 0.0002 (0.0023)	
training:	Epoch: [79][168/408]	Loss 0.0009 (0.0023)	
training:	Epoch: [79][169/408]	Loss 0.0003 (0.0023)	
training:	Epoch: [79][170/408]	Loss 0.0458 (0.0025)	
training:	Epoch: [79][171/408]	Loss 0.0002 (0.0025)	
training:	Epoch: [79][172/408]	Loss 0.0003 (0.0025)	
training:	Epoch: [79][173/408]	Loss 0.0007 (0.0025)	
training:	Epoch: [79][174/408]	Loss 0.0003 (0.0025)	
training:	Epoch: [79][175/408]	Loss 0.0002 (0.0025)	
training:	Epoch: [79][176/408]	Loss 0.0002 (0.0025)	
training:	Epoch: [79][177/408]	Loss 0.0003 (0.0025)	
training:	Epoch: [79][178/408]	Loss 0.0005 (0.0024)	
training:	Epoch: [79][179/408]	Loss 0.0005 (0.0024)	
training:	Epoch: [79][180/408]	Loss 0.0008 (0.0024)	
training:	Epoch: [79][181/408]	Loss 0.0002 (0.0024)	
training:	Epoch: [79][182/408]	Loss 0.0002 (0.0024)	
training:	Epoch: [79][183/408]	Loss 0.0002 (0.0024)	
training:	Epoch: [79][184/408]	Loss 0.0002 (0.0024)	
training:	Epoch: [79][185/408]	Loss 0.0002 (0.0024)	
training:	Epoch: [79][186/408]	Loss 0.0003 (0.0024)	
training:	Epoch: [79][187/408]	Loss 0.0003 (0.0023)	
training:	Epoch: [79][188/408]	Loss 0.0002 (0.0023)	
training:	Epoch: [79][189/408]	Loss 0.0003 (0.0023)	
training:	Epoch: [79][190/408]	Loss 0.0002 (0.0023)	
training:	Epoch: [79][191/408]	Loss 0.0148 (0.0024)	
training:	Epoch: [79][192/408]	Loss 0.0016 (0.0024)	
training:	Epoch: [79][193/408]	Loss 0.0002 (0.0024)	
training:	Epoch: [79][194/408]	Loss 0.0003 (0.0024)	
training:	Epoch: [79][195/408]	Loss 0.0009 (0.0023)	
training:	Epoch: [79][196/408]	Loss 0.0002 (0.0023)	
training:	Epoch: [79][197/408]	Loss 0.0002 (0.0023)	
training:	Epoch: [79][198/408]	Loss 0.0003 (0.0023)	
training:	Epoch: [79][199/408]	Loss 0.0003 (0.0023)	
training:	Epoch: [79][200/408]	Loss 0.0076 (0.0023)	
training:	Epoch: [79][201/408]	Loss 0.0006 (0.0023)	
training:	Epoch: [79][202/408]	Loss 0.0002 (0.0023)	
training:	Epoch: [79][203/408]	Loss 0.0003 (0.0023)	
training:	Epoch: [79][204/408]	Loss 0.0003 (0.0023)	
training:	Epoch: [79][205/408]	Loss 0.0004 (0.0023)	
training:	Epoch: [79][206/408]	Loss 0.0002 (0.0023)	
training:	Epoch: [79][207/408]	Loss 0.0003 (0.0023)	
training:	Epoch: [79][208/408]	Loss 0.0003 (0.0023)	
training:	Epoch: [79][209/408]	Loss 0.0003 (0.0022)	
training:	Epoch: [79][210/408]	Loss 0.0002 (0.0022)	
training:	Epoch: [79][211/408]	Loss 0.0002 (0.0022)	
training:	Epoch: [79][212/408]	Loss 0.0003 (0.0022)	
training:	Epoch: [79][213/408]	Loss 0.0003 (0.0022)	
training:	Epoch: [79][214/408]	Loss 0.0004 (0.0022)	
training:	Epoch: [79][215/408]	Loss 0.0002 (0.0022)	
training:	Epoch: [79][216/408]	Loss 0.0003 (0.0022)	
training:	Epoch: [79][217/408]	Loss 0.0002 (0.0022)	
training:	Epoch: [79][218/408]	Loss 0.0003 (0.0022)	
training:	Epoch: [79][219/408]	Loss 0.0002 (0.0022)	
training:	Epoch: [79][220/408]	Loss 0.0003 (0.0021)	
training:	Epoch: [79][221/408]	Loss 0.0002 (0.0021)	
training:	Epoch: [79][222/408]	Loss 0.0009 (0.0021)	
training:	Epoch: [79][223/408]	Loss 0.0002 (0.0021)	
training:	Epoch: [79][224/408]	Loss 0.0002 (0.0021)	
training:	Epoch: [79][225/408]	Loss 0.0003 (0.0021)	
training:	Epoch: [79][226/408]	Loss 0.0004 (0.0021)	
training:	Epoch: [79][227/408]	Loss 0.0003 (0.0021)	
training:	Epoch: [79][228/408]	Loss 0.0003 (0.0021)	
training:	Epoch: [79][229/408]	Loss 0.0017 (0.0021)	
training:	Epoch: [79][230/408]	Loss 0.0006 (0.0021)	
training:	Epoch: [79][231/408]	Loss 0.0003 (0.0021)	
training:	Epoch: [79][232/408]	Loss 0.0010 (0.0021)	
training:	Epoch: [79][233/408]	Loss 0.0002 (0.0021)	
training:	Epoch: [79][234/408]	Loss 0.0003 (0.0020)	
training:	Epoch: [79][235/408]	Loss 0.0002 (0.0020)	
training:	Epoch: [79][236/408]	Loss 0.0005 (0.0020)	
training:	Epoch: [79][237/408]	Loss 0.0007 (0.0020)	
training:	Epoch: [79][238/408]	Loss 0.0003 (0.0020)	
training:	Epoch: [79][239/408]	Loss 0.0002 (0.0020)	
training:	Epoch: [79][240/408]	Loss 0.0002 (0.0020)	
training:	Epoch: [79][241/408]	Loss 0.0003 (0.0020)	
training:	Epoch: [79][242/408]	Loss 0.0002 (0.0020)	
training:	Epoch: [79][243/408]	Loss 0.0004 (0.0020)	
training:	Epoch: [79][244/408]	Loss 0.0007 (0.0020)	
training:	Epoch: [79][245/408]	Loss 0.0004 (0.0020)	
training:	Epoch: [79][246/408]	Loss 0.0008 (0.0020)	
training:	Epoch: [79][247/408]	Loss 0.0003 (0.0020)	
training:	Epoch: [79][248/408]	Loss 0.0002 (0.0020)	
training:	Epoch: [79][249/408]	Loss 0.0002 (0.0019)	
training:	Epoch: [79][250/408]	Loss 0.0011 (0.0019)	
training:	Epoch: [79][251/408]	Loss 0.0002 (0.0019)	
training:	Epoch: [79][252/408]	Loss 0.0003 (0.0019)	
training:	Epoch: [79][253/408]	Loss 0.0004 (0.0019)	
training:	Epoch: [79][254/408]	Loss 0.0004 (0.0019)	
training:	Epoch: [79][255/408]	Loss 0.0003 (0.0019)	
training:	Epoch: [79][256/408]	Loss 0.0002 (0.0019)	
training:	Epoch: [79][257/408]	Loss 0.0005 (0.0019)	
training:	Epoch: [79][258/408]	Loss 0.0004 (0.0019)	
training:	Epoch: [79][259/408]	Loss 0.0004 (0.0019)	
training:	Epoch: [79][260/408]	Loss 0.0003 (0.0019)	
training:	Epoch: [79][261/408]	Loss 0.0005 (0.0019)	
training:	Epoch: [79][262/408]	Loss 0.0011 (0.0019)	
training:	Epoch: [79][263/408]	Loss 0.0004 (0.0019)	
training:	Epoch: [79][264/408]	Loss 0.0004 (0.0019)	
training:	Epoch: [79][265/408]	Loss 0.0003 (0.0019)	
training:	Epoch: [79][266/408]	Loss 0.0003 (0.0018)	
training:	Epoch: [79][267/408]	Loss 0.0006 (0.0018)	
training:	Epoch: [79][268/408]	Loss 0.0002 (0.0018)	
training:	Epoch: [79][269/408]	Loss 0.0003 (0.0018)	
training:	Epoch: [79][270/408]	Loss 0.0002 (0.0018)	
training:	Epoch: [79][271/408]	Loss 0.0006 (0.0018)	
training:	Epoch: [79][272/408]	Loss 0.0002 (0.0018)	
training:	Epoch: [79][273/408]	Loss 0.0002 (0.0018)	
training:	Epoch: [79][274/408]	Loss 0.0002 (0.0018)	
training:	Epoch: [79][275/408]	Loss 0.0029 (0.0018)	
training:	Epoch: [79][276/408]	Loss 0.0003 (0.0018)	
training:	Epoch: [79][277/408]	Loss 0.0002 (0.0018)	
training:	Epoch: [79][278/408]	Loss 0.0017 (0.0018)	
training:	Epoch: [79][279/408]	Loss 0.0006 (0.0018)	
training:	Epoch: [79][280/408]	Loss 0.0007 (0.0018)	
training:	Epoch: [79][281/408]	Loss 0.0002 (0.0018)	
training:	Epoch: [79][282/408]	Loss 0.0019 (0.0018)	
training:	Epoch: [79][283/408]	Loss 0.0003 (0.0018)	
training:	Epoch: [79][284/408]	Loss 0.0003 (0.0018)	
training:	Epoch: [79][285/408]	Loss 0.0003 (0.0018)	
training:	Epoch: [79][286/408]	Loss 0.0012 (0.0018)	
training:	Epoch: [79][287/408]	Loss 0.0002 (0.0018)	
training:	Epoch: [79][288/408]	Loss 0.0003 (0.0018)	
training:	Epoch: [79][289/408]	Loss 0.0003 (0.0018)	
training:	Epoch: [79][290/408]	Loss 0.0004 (0.0017)	
training:	Epoch: [79][291/408]	Loss 0.0004 (0.0017)	
training:	Epoch: [79][292/408]	Loss 0.0003 (0.0017)	
training:	Epoch: [79][293/408]	Loss 0.0003 (0.0017)	
training:	Epoch: [79][294/408]	Loss 0.0009 (0.0017)	
training:	Epoch: [79][295/408]	Loss 0.0003 (0.0017)	
training:	Epoch: [79][296/408]	Loss 0.0004 (0.0017)	
training:	Epoch: [79][297/408]	Loss 0.0005 (0.0017)	
training:	Epoch: [79][298/408]	Loss 0.0003 (0.0017)	
training:	Epoch: [79][299/408]	Loss 0.0002 (0.0017)	
training:	Epoch: [79][300/408]	Loss 0.0005 (0.0017)	
training:	Epoch: [79][301/408]	Loss 0.0004 (0.0017)	
training:	Epoch: [79][302/408]	Loss 0.0003 (0.0017)	
training:	Epoch: [79][303/408]	Loss 0.0003 (0.0017)	
training:	Epoch: [79][304/408]	Loss 0.0003 (0.0017)	
training:	Epoch: [79][305/408]	Loss 0.0002 (0.0017)	
training:	Epoch: [79][306/408]	Loss 0.0003 (0.0017)	
training:	Epoch: [79][307/408]	Loss 0.0002 (0.0017)	
training:	Epoch: [79][308/408]	Loss 0.0003 (0.0017)	
training:	Epoch: [79][309/408]	Loss 0.0003 (0.0017)	
training:	Epoch: [79][310/408]	Loss 0.0004 (0.0017)	
training:	Epoch: [79][311/408]	Loss 0.0002 (0.0017)	
training:	Epoch: [79][312/408]	Loss 0.0002 (0.0016)	
training:	Epoch: [79][313/408]	Loss 0.0002 (0.0016)	
training:	Epoch: [79][314/408]	Loss 0.0002 (0.0016)	
training:	Epoch: [79][315/408]	Loss 0.0104 (0.0017)	
training:	Epoch: [79][316/408]	Loss 0.0008 (0.0017)	
training:	Epoch: [79][317/408]	Loss 0.0002 (0.0017)	
training:	Epoch: [79][318/408]	Loss 0.0002 (0.0017)	
training:	Epoch: [79][319/408]	Loss 0.0003 (0.0016)	
training:	Epoch: [79][320/408]	Loss 0.0002 (0.0016)	
training:	Epoch: [79][321/408]	Loss 0.0002 (0.0016)	
training:	Epoch: [79][322/408]	Loss 0.0003 (0.0016)	
training:	Epoch: [79][323/408]	Loss 0.0002 (0.0016)	
training:	Epoch: [79][324/408]	Loss 0.0005 (0.0016)	
training:	Epoch: [79][325/408]	Loss 0.0002 (0.0016)	
training:	Epoch: [79][326/408]	Loss 0.0002 (0.0016)	
training:	Epoch: [79][327/408]	Loss 0.0007 (0.0016)	
training:	Epoch: [79][328/408]	Loss 0.0002 (0.0016)	
training:	Epoch: [79][329/408]	Loss 0.0003 (0.0016)	
training:	Epoch: [79][330/408]	Loss 0.0002 (0.0016)	
training:	Epoch: [79][331/408]	Loss 0.0003 (0.0016)	
training:	Epoch: [79][332/408]	Loss 0.0003 (0.0016)	
training:	Epoch: [79][333/408]	Loss 0.0002 (0.0016)	
training:	Epoch: [79][334/408]	Loss 0.0002 (0.0016)	
training:	Epoch: [79][335/408]	Loss 0.0004 (0.0016)	
training:	Epoch: [79][336/408]	Loss 0.0003 (0.0016)	
training:	Epoch: [79][337/408]	Loss 0.0007 (0.0016)	
training:	Epoch: [79][338/408]	Loss 0.0003 (0.0016)	
training:	Epoch: [79][339/408]	Loss 0.0016 (0.0016)	
training:	Epoch: [79][340/408]	Loss 0.0005 (0.0016)	
training:	Epoch: [79][341/408]	Loss 0.0002 (0.0016)	
training:	Epoch: [79][342/408]	Loss 0.0002 (0.0016)	
training:	Epoch: [79][343/408]	Loss 0.0007 (0.0016)	
training:	Epoch: [79][344/408]	Loss 0.0003 (0.0016)	
training:	Epoch: [79][345/408]	Loss 0.0007 (0.0016)	
training:	Epoch: [79][346/408]	Loss 0.0002 (0.0016)	
training:	Epoch: [79][347/408]	Loss 0.0002 (0.0015)	
training:	Epoch: [79][348/408]	Loss 0.0003 (0.0015)	
training:	Epoch: [79][349/408]	Loss 0.0002 (0.0015)	
training:	Epoch: [79][350/408]	Loss 0.0002 (0.0015)	
training:	Epoch: [79][351/408]	Loss 0.0004 (0.0015)	
training:	Epoch: [79][352/408]	Loss 0.0003 (0.0015)	
training:	Epoch: [79][353/408]	Loss 0.0006 (0.0015)	
training:	Epoch: [79][354/408]	Loss 0.0003 (0.0015)	
training:	Epoch: [79][355/408]	Loss 0.0004 (0.0015)	
training:	Epoch: [79][356/408]	Loss 0.0003 (0.0015)	
training:	Epoch: [79][357/408]	Loss 0.0003 (0.0015)	
training:	Epoch: [79][358/408]	Loss 0.0002 (0.0015)	
training:	Epoch: [79][359/408]	Loss 0.0003 (0.0015)	
training:	Epoch: [79][360/408]	Loss 0.0002 (0.0015)	
training:	Epoch: [79][361/408]	Loss 0.0004 (0.0015)	
training:	Epoch: [79][362/408]	Loss 0.0002 (0.0015)	
training:	Epoch: [79][363/408]	Loss 0.0006 (0.0015)	
training:	Epoch: [79][364/408]	Loss 0.0003 (0.0015)	
training:	Epoch: [79][365/408]	Loss 0.0002 (0.0015)	
training:	Epoch: [79][366/408]	Loss 0.0002 (0.0015)	
training:	Epoch: [79][367/408]	Loss 0.0002 (0.0015)	
training:	Epoch: [79][368/408]	Loss 0.0010 (0.0015)	
training:	Epoch: [79][369/408]	Loss 0.0003 (0.0015)	
training:	Epoch: [79][370/408]	Loss 0.0005 (0.0015)	
training:	Epoch: [79][371/408]	Loss 0.0004 (0.0015)	
training:	Epoch: [79][372/408]	Loss 0.0007 (0.0015)	
training:	Epoch: [79][373/408]	Loss 0.0002 (0.0015)	
training:	Epoch: [79][374/408]	Loss 0.0002 (0.0015)	
training:	Epoch: [79][375/408]	Loss 0.0002 (0.0015)	
training:	Epoch: [79][376/408]	Loss 0.0003 (0.0015)	
training:	Epoch: [79][377/408]	Loss 0.0005 (0.0015)	
training:	Epoch: [79][378/408]	Loss 0.0003 (0.0014)	
training:	Epoch: [79][379/408]	Loss 0.0002 (0.0014)	
training:	Epoch: [79][380/408]	Loss 0.0002 (0.0014)	
training:	Epoch: [79][381/408]	Loss 0.0002 (0.0014)	
training:	Epoch: [79][382/408]	Loss 0.0003 (0.0014)	
training:	Epoch: [79][383/408]	Loss 0.0002 (0.0014)	
training:	Epoch: [79][384/408]	Loss 0.0003 (0.0014)	
training:	Epoch: [79][385/408]	Loss 0.0002 (0.0014)	
training:	Epoch: [79][386/408]	Loss 0.0002 (0.0014)	
training:	Epoch: [79][387/408]	Loss 0.0004 (0.0014)	
training:	Epoch: [79][388/408]	Loss 0.0004 (0.0014)	
training:	Epoch: [79][389/408]	Loss 0.0002 (0.0014)	
training:	Epoch: [79][390/408]	Loss 0.0002 (0.0014)	
training:	Epoch: [79][391/408]	Loss 0.0002 (0.0014)	
training:	Epoch: [79][392/408]	Loss 0.0002 (0.0014)	
training:	Epoch: [79][393/408]	Loss 0.0005 (0.0014)	
training:	Epoch: [79][394/408]	Loss 0.0002 (0.0014)	
training:	Epoch: [79][395/408]	Loss 0.0005 (0.0014)	
training:	Epoch: [79][396/408]	Loss 0.0006 (0.0014)	
training:	Epoch: [79][397/408]	Loss 0.0003 (0.0014)	
training:	Epoch: [79][398/408]	Loss 0.0002 (0.0014)	
training:	Epoch: [79][399/408]	Loss 0.0007 (0.0014)	
training:	Epoch: [79][400/408]	Loss 0.0002 (0.0014)	
training:	Epoch: [79][401/408]	Loss 0.0003 (0.0014)	
training:	Epoch: [79][402/408]	Loss 0.0005 (0.0014)	
training:	Epoch: [79][403/408]	Loss 0.0006 (0.0014)	
training:	Epoch: [79][404/408]	Loss 0.0002 (0.0014)	
training:	Epoch: [79][405/408]	Loss 0.0010 (0.0014)	
training:	Epoch: [79][406/408]	Loss 0.0004 (0.0014)	
training:	Epoch: [79][407/408]	Loss 0.0002 (0.0014)	
training:	Epoch: [79][408/408]	Loss 0.0019 (0.0014)	
Training:	 Loss: 0.0014

Training:	 ACC: 1.0000 1.0000 1.0000 1.0000
Validation:	 ACC: 0.7800 0.7812 0.8066 0.7534
Validation:	 Best_BACC: 0.8226 0.8250 0.8772 0.7679
Validation:	 Loss: 1.3919
Pretraining:	Epoch 80/200
----------
training:	Epoch: [80][1/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [80][2/408]	Loss 0.0003 (0.0002)	
training:	Epoch: [80][3/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [80][4/408]	Loss 0.0003 (0.0002)	
training:	Epoch: [80][5/408]	Loss 0.0003 (0.0002)	
training:	Epoch: [80][6/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [80][7/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [80][8/408]	Loss 0.0003 (0.0002)	
training:	Epoch: [80][9/408]	Loss 0.0003 (0.0002)	
training:	Epoch: [80][10/408]	Loss 0.0005 (0.0003)	
training:	Epoch: [80][11/408]	Loss 0.0003 (0.0003)	
training:	Epoch: [80][12/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [80][13/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [80][14/408]	Loss 0.0006 (0.0003)	
training:	Epoch: [80][15/408]	Loss 0.0004 (0.0003)	
training:	Epoch: [80][16/408]	Loss 0.0005 (0.0003)	
training:	Epoch: [80][17/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [80][18/408]	Loss 0.0003 (0.0003)	
training:	Epoch: [80][19/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [80][20/408]	Loss 0.0008 (0.0003)	
training:	Epoch: [80][21/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [80][22/408]	Loss 0.0006 (0.0003)	
training:	Epoch: [80][23/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [80][24/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [80][25/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [80][26/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [80][27/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [80][28/408]	Loss 0.0004 (0.0003)	
training:	Epoch: [80][29/408]	Loss 0.0003 (0.0003)	
training:	Epoch: [80][30/408]	Loss 0.0003 (0.0003)	
training:	Epoch: [80][31/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [80][32/408]	Loss 0.0006 (0.0003)	
training:	Epoch: [80][33/408]	Loss 0.0004 (0.0003)	
training:	Epoch: [80][34/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [80][35/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [80][36/408]	Loss 0.0004 (0.0003)	
training:	Epoch: [80][37/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [80][38/408]	Loss 0.0005 (0.0003)	
training:	Epoch: [80][39/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [80][40/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [80][41/408]	Loss 0.0004 (0.0003)	
training:	Epoch: [80][42/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [80][43/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [80][44/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [80][45/408]	Loss 0.0003 (0.0003)	
training:	Epoch: [80][46/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [80][47/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [80][48/408]	Loss 0.0003 (0.0003)	
training:	Epoch: [80][49/408]	Loss 0.0003 (0.0003)	
training:	Epoch: [80][50/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [80][51/408]	Loss 0.0003 (0.0003)	
training:	Epoch: [80][52/408]	Loss 0.0006 (0.0003)	
training:	Epoch: [80][53/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [80][54/408]	Loss 0.0003 (0.0003)	
training:	Epoch: [80][55/408]	Loss 0.0003 (0.0003)	
training:	Epoch: [80][56/408]	Loss 0.0003 (0.0003)	
training:	Epoch: [80][57/408]	Loss 0.0004 (0.0003)	
training:	Epoch: [80][58/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [80][59/408]	Loss 0.0004 (0.0003)	
training:	Epoch: [80][60/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [80][61/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [80][62/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [80][63/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [80][64/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [80][65/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [80][66/408]	Loss 0.0003 (0.0003)	
training:	Epoch: [80][67/408]	Loss 0.0019 (0.0003)	
training:	Epoch: [80][68/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [80][69/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [80][70/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [80][71/408]	Loss 0.0005 (0.0003)	
training:	Epoch: [80][72/408]	Loss 0.0003 (0.0003)	
training:	Epoch: [80][73/408]	Loss 0.0006 (0.0003)	
training:	Epoch: [80][74/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [80][75/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [80][76/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [80][77/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [80][78/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [80][79/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [80][80/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [80][81/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [80][82/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [80][83/408]	Loss 0.0005 (0.0003)	
training:	Epoch: [80][84/408]	Loss 0.0016 (0.0003)	
training:	Epoch: [80][85/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [80][86/408]	Loss 0.0003 (0.0003)	
training:	Epoch: [80][87/408]	Loss 0.0003 (0.0003)	
training:	Epoch: [80][88/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [80][89/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [80][90/408]	Loss 0.0004 (0.0003)	
training:	Epoch: [80][91/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [80][92/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [80][93/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [80][94/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [80][95/408]	Loss 0.0004 (0.0003)	
training:	Epoch: [80][96/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [80][97/408]	Loss 0.0004 (0.0003)	
training:	Epoch: [80][98/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [80][99/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [80][100/408]	Loss 0.0004 (0.0003)	
training:	Epoch: [80][101/408]	Loss 0.0003 (0.0003)	
training:	Epoch: [80][102/408]	Loss 0.0003 (0.0003)	
training:	Epoch: [80][103/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [80][104/408]	Loss 0.0003 (0.0003)	
training:	Epoch: [80][105/408]	Loss 0.0005 (0.0003)	
training:	Epoch: [80][106/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [80][107/408]	Loss 0.0003 (0.0003)	
training:	Epoch: [80][108/408]	Loss 0.0008 (0.0003)	
training:	Epoch: [80][109/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [80][110/408]	Loss 0.0004 (0.0003)	
training:	Epoch: [80][111/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [80][112/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [80][113/408]	Loss 0.0004 (0.0003)	
training:	Epoch: [80][114/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [80][115/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [80][116/408]	Loss 0.0003 (0.0003)	
training:	Epoch: [80][117/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [80][118/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [80][119/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [80][120/408]	Loss 0.0004 (0.0003)	
training:	Epoch: [80][121/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [80][122/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [80][123/408]	Loss 0.0003 (0.0003)	
training:	Epoch: [80][124/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [80][125/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [80][126/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [80][127/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [80][128/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [80][129/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [80][130/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [80][131/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [80][132/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [80][133/408]	Loss 0.0004 (0.0003)	
training:	Epoch: [80][134/408]	Loss 0.0009 (0.0003)	
training:	Epoch: [80][135/408]	Loss 0.0003 (0.0003)	
training:	Epoch: [80][136/408]	Loss 0.0008 (0.0003)	
training:	Epoch: [80][137/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [80][138/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [80][139/408]	Loss 0.0003 (0.0003)	
training:	Epoch: [80][140/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [80][141/408]	Loss 0.0003 (0.0003)	
training:	Epoch: [80][142/408]	Loss 0.0003 (0.0003)	
training:	Epoch: [80][143/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [80][144/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [80][145/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [80][146/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [80][147/408]	Loss 0.0006 (0.0003)	
training:	Epoch: [80][148/408]	Loss 0.0003 (0.0003)	
training:	Epoch: [80][149/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [80][150/408]	Loss 0.0003 (0.0003)	
training:	Epoch: [80][151/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [80][152/408]	Loss 0.0010 (0.0003)	
training:	Epoch: [80][153/408]	Loss 0.0004 (0.0003)	
training:	Epoch: [80][154/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [80][155/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [80][156/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [80][157/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [80][158/408]	Loss 0.0004 (0.0003)	
training:	Epoch: [80][159/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [80][160/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [80][161/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [80][162/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [80][163/408]	Loss 0.0003 (0.0003)	
training:	Epoch: [80][164/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [80][165/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [80][166/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [80][167/408]	Loss 0.0003 (0.0003)	
training:	Epoch: [80][168/408]	Loss 0.0003 (0.0003)	
training:	Epoch: [80][169/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [80][170/408]	Loss 0.0005 (0.0003)	
training:	Epoch: [80][171/408]	Loss 0.0003 (0.0003)	
training:	Epoch: [80][172/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [80][173/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [80][174/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [80][175/408]	Loss 0.0003 (0.0003)	
training:	Epoch: [80][176/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [80][177/408]	Loss 0.0007 (0.0003)	
training:	Epoch: [80][178/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [80][179/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [80][180/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [80][181/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [80][182/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [80][183/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [80][184/408]	Loss 0.0098 (0.0004)	
training:	Epoch: [80][185/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [80][186/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [80][187/408]	Loss 0.0016 (0.0004)	
training:	Epoch: [80][188/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [80][189/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [80][190/408]	Loss 0.0004 (0.0004)	
training:	Epoch: [80][191/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [80][192/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [80][193/408]	Loss 0.0020 (0.0004)	
training:	Epoch: [80][194/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [80][195/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [80][196/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [80][197/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [80][198/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [80][199/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [80][200/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [80][201/408]	Loss 0.0027 (0.0004)	
training:	Epoch: [80][202/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [80][203/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [80][204/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [80][205/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [80][206/408]	Loss 0.0007 (0.0004)	
training:	Epoch: [80][207/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [80][208/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [80][209/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [80][210/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [80][211/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [80][212/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [80][213/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [80][214/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [80][215/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [80][216/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [80][217/408]	Loss 0.0006 (0.0004)	
training:	Epoch: [80][218/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [80][219/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [80][220/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [80][221/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [80][222/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [80][223/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [80][224/408]	Loss 0.0006 (0.0004)	
training:	Epoch: [80][225/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [80][226/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [80][227/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [80][228/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [80][229/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [80][230/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [80][231/408]	Loss 0.0129 (0.0004)	
training:	Epoch: [80][232/408]	Loss 0.0004 (0.0004)	
training:	Epoch: [80][233/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [80][234/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [80][235/408]	Loss 0.0006 (0.0004)	
training:	Epoch: [80][236/408]	Loss 0.0005 (0.0004)	
training:	Epoch: [80][237/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [80][238/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [80][239/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [80][240/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [80][241/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [80][242/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [80][243/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [80][244/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [80][245/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [80][246/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [80][247/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [80][248/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [80][249/408]	Loss 0.0006 (0.0004)	
training:	Epoch: [80][250/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [80][251/408]	Loss 0.0020 (0.0004)	
training:	Epoch: [80][252/408]	Loss 0.0012 (0.0004)	
training:	Epoch: [80][253/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [80][254/408]	Loss 0.0006 (0.0004)	
training:	Epoch: [80][255/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [80][256/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [80][257/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [80][258/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [80][259/408]	Loss 0.0004 (0.0004)	
training:	Epoch: [80][260/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [80][261/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [80][262/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [80][263/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [80][264/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [80][265/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [80][266/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [80][267/408]	Loss 0.0010 (0.0004)	
training:	Epoch: [80][268/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [80][269/408]	Loss 0.0004 (0.0004)	
training:	Epoch: [80][270/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [80][271/408]	Loss 0.0007 (0.0004)	
training:	Epoch: [80][272/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [80][273/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [80][274/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [80][275/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [80][276/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [80][277/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [80][278/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [80][279/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [80][280/408]	Loss 0.0006 (0.0004)	
training:	Epoch: [80][281/408]	Loss 0.0012 (0.0004)	
training:	Epoch: [80][282/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [80][283/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [80][284/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [80][285/408]	Loss 0.0008 (0.0004)	
training:	Epoch: [80][286/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [80][287/408]	Loss 0.0004 (0.0004)	
training:	Epoch: [80][288/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [80][289/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [80][290/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [80][291/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [80][292/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [80][293/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [80][294/408]	Loss 0.0006 (0.0004)	
training:	Epoch: [80][295/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [80][296/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [80][297/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [80][298/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [80][299/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [80][300/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [80][301/408]	Loss 0.0009 (0.0004)	
training:	Epoch: [80][302/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [80][303/408]	Loss 0.0004 (0.0004)	
training:	Epoch: [80][304/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [80][305/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [80][306/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [80][307/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [80][308/408]	Loss 0.0005 (0.0004)	
training:	Epoch: [80][309/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [80][310/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [80][311/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [80][312/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [80][313/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [80][314/408]	Loss 0.0005 (0.0004)	
training:	Epoch: [80][315/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [80][316/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [80][317/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [80][318/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [80][319/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [80][320/408]	Loss 0.0062 (0.0004)	
training:	Epoch: [80][321/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [80][322/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [80][323/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [80][324/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [80][325/408]	Loss 0.0008 (0.0004)	
training:	Epoch: [80][326/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [80][327/408]	Loss 0.0014 (0.0004)	
training:	Epoch: [80][328/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [80][329/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [80][330/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [80][331/408]	Loss 0.0023 (0.0004)	
training:	Epoch: [80][332/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [80][333/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [80][334/408]	Loss 0.0008 (0.0004)	
training:	Epoch: [80][335/408]	Loss 0.0004 (0.0004)	
training:	Epoch: [80][336/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [80][337/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [80][338/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [80][339/408]	Loss 0.0154 (0.0005)	
training:	Epoch: [80][340/408]	Loss 0.0003 (0.0005)	
training:	Epoch: [80][341/408]	Loss 0.0011 (0.0005)	
training:	Epoch: [80][342/408]	Loss 0.0003 (0.0005)	
training:	Epoch: [80][343/408]	Loss 0.0042 (0.0005)	
training:	Epoch: [80][344/408]	Loss 0.0002 (0.0005)	
training:	Epoch: [80][345/408]	Loss 0.0002 (0.0005)	
training:	Epoch: [80][346/408]	Loss 0.0002 (0.0005)	
training:	Epoch: [80][347/408]	Loss 0.0003 (0.0005)	
training:	Epoch: [80][348/408]	Loss 0.0002 (0.0005)	
training:	Epoch: [80][349/408]	Loss 0.0003 (0.0005)	
training:	Epoch: [80][350/408]	Loss 0.0002 (0.0005)	
training:	Epoch: [80][351/408]	Loss 0.0002 (0.0005)	
training:	Epoch: [80][352/408]	Loss 0.0002 (0.0005)	
training:	Epoch: [80][353/408]	Loss 0.0002 (0.0005)	
training:	Epoch: [80][354/408]	Loss 0.0003 (0.0005)	
training:	Epoch: [80][355/408]	Loss 0.0003 (0.0005)	
training:	Epoch: [80][356/408]	Loss 0.0003 (0.0005)	
training:	Epoch: [80][357/408]	Loss 0.0004 (0.0005)	
training:	Epoch: [80][358/408]	Loss 0.0003 (0.0005)	
training:	Epoch: [80][359/408]	Loss 0.0002 (0.0005)	
training:	Epoch: [80][360/408]	Loss 0.0002 (0.0005)	
training:	Epoch: [80][361/408]	Loss 0.0002 (0.0005)	
training:	Epoch: [80][362/408]	Loss 0.0003 (0.0005)	
training:	Epoch: [80][363/408]	Loss 0.0002 (0.0005)	
training:	Epoch: [80][364/408]	Loss 0.0005 (0.0005)	
training:	Epoch: [80][365/408]	Loss 0.0005 (0.0005)	
training:	Epoch: [80][366/408]	Loss 0.0002 (0.0005)	
training:	Epoch: [80][367/408]	Loss 0.0003 (0.0005)	
training:	Epoch: [80][368/408]	Loss 0.0003 (0.0005)	
training:	Epoch: [80][369/408]	Loss 0.0119 (0.0005)	
training:	Epoch: [80][370/408]	Loss 0.0003 (0.0005)	
training:	Epoch: [80][371/408]	Loss 0.0002 (0.0005)	
training:	Epoch: [80][372/408]	Loss 0.0004 (0.0005)	
training:	Epoch: [80][373/408]	Loss 0.0002 (0.0005)	
training:	Epoch: [80][374/408]	Loss 0.0006 (0.0005)	
training:	Epoch: [80][375/408]	Loss 0.0002 (0.0005)	
training:	Epoch: [80][376/408]	Loss 0.0002 (0.0005)	
training:	Epoch: [80][377/408]	Loss 0.0002 (0.0005)	
training:	Epoch: [80][378/408]	Loss 0.0002 (0.0005)	
training:	Epoch: [80][379/408]	Loss 0.0003 (0.0005)	
training:	Epoch: [80][380/408]	Loss 0.0002 (0.0005)	
training:	Epoch: [80][381/408]	Loss 0.0002 (0.0005)	
training:	Epoch: [80][382/408]	Loss 0.0003 (0.0005)	
training:	Epoch: [80][383/408]	Loss 0.0081 (0.0005)	
training:	Epoch: [80][384/408]	Loss 0.0002 (0.0005)	
training:	Epoch: [80][385/408]	Loss 0.0002 (0.0005)	
training:	Epoch: [80][386/408]	Loss 0.0009 (0.0005)	
training:	Epoch: [80][387/408]	Loss 0.0002 (0.0005)	
training:	Epoch: [80][388/408]	Loss 0.0005 (0.0005)	
training:	Epoch: [80][389/408]	Loss 0.0002 (0.0005)	
training:	Epoch: [80][390/408]	Loss 0.0002 (0.0005)	
training:	Epoch: [80][391/408]	Loss 0.0002 (0.0005)	
training:	Epoch: [80][392/408]	Loss 0.0050 (0.0005)	
training:	Epoch: [80][393/408]	Loss 0.0002 (0.0005)	
training:	Epoch: [80][394/408]	Loss 0.0011 (0.0005)	
training:	Epoch: [80][395/408]	Loss 0.0002 (0.0005)	
training:	Epoch: [80][396/408]	Loss 0.0002 (0.0005)	
training:	Epoch: [80][397/408]	Loss 0.0002 (0.0005)	
training:	Epoch: [80][398/408]	Loss 0.0003 (0.0005)	
training:	Epoch: [80][399/408]	Loss 0.0004 (0.0005)	
training:	Epoch: [80][400/408]	Loss 0.0011 (0.0005)	
training:	Epoch: [80][401/408]	Loss 0.0002 (0.0005)	
training:	Epoch: [80][402/408]	Loss 0.0002 (0.0005)	
training:	Epoch: [80][403/408]	Loss 0.0002 (0.0005)	
training:	Epoch: [80][404/408]	Loss 0.0002 (0.0005)	
training:	Epoch: [80][405/408]	Loss 0.0003 (0.0005)	
training:	Epoch: [80][406/408]	Loss 0.0003 (0.0005)	
training:	Epoch: [80][407/408]	Loss 0.0002 (0.0005)	
training:	Epoch: [80][408/408]	Loss 0.0012 (0.0005)	
Training:	 Loss: 0.0005

Training:	 ACC: 1.0000 1.0000 1.0000 1.0000
Validation:	 ACC: 0.7763 0.7780 0.8127 0.7399
Validation:	 Best_BACC: 0.8226 0.8250 0.8772 0.7679
Validation:	 Loss: 1.4411
Pretraining:	Epoch 81/200
----------
training:	Epoch: [81][1/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [81][2/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [81][3/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [81][4/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [81][5/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [81][6/408]	Loss 0.0003 (0.0002)	
training:	Epoch: [81][7/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [81][8/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [81][9/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [81][10/408]	Loss 0.0009 (0.0003)	
training:	Epoch: [81][11/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [81][12/408]	Loss 0.0005 (0.0003)	
training:	Epoch: [81][13/408]	Loss 0.0003 (0.0003)	
training:	Epoch: [81][14/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [81][15/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [81][16/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [81][17/408]	Loss 0.0003 (0.0003)	
training:	Epoch: [81][18/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [81][19/408]	Loss 0.0003 (0.0003)	
training:	Epoch: [81][20/408]	Loss 0.0003 (0.0003)	
training:	Epoch: [81][21/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [81][22/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [81][23/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [81][24/408]	Loss 0.0007 (0.0003)	
training:	Epoch: [81][25/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [81][26/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [81][27/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [81][28/408]	Loss 0.0004 (0.0003)	
training:	Epoch: [81][29/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [81][30/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [81][31/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [81][32/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [81][33/408]	Loss 0.0003 (0.0003)	
training:	Epoch: [81][34/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [81][35/408]	Loss 0.0003 (0.0003)	
training:	Epoch: [81][36/408]	Loss 0.0003 (0.0003)	
training:	Epoch: [81][37/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [81][38/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [81][39/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [81][40/408]	Loss 0.0003 (0.0003)	
training:	Epoch: [81][41/408]	Loss 0.0003 (0.0003)	
training:	Epoch: [81][42/408]	Loss 0.0004 (0.0003)	
training:	Epoch: [81][43/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [81][44/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [81][45/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [81][46/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [81][47/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [81][48/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [81][49/408]	Loss 0.0003 (0.0003)	
training:	Epoch: [81][50/408]	Loss 0.0004 (0.0003)	
training:	Epoch: [81][51/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [81][52/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [81][53/408]	Loss 0.0010 (0.0003)	
training:	Epoch: [81][54/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [81][55/408]	Loss 0.0100 (0.0005)	
training:	Epoch: [81][56/408]	Loss 0.0002 (0.0005)	
training:	Epoch: [81][57/408]	Loss 0.0003 (0.0005)	
training:	Epoch: [81][58/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [81][59/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [81][60/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [81][61/408]	Loss 0.0004 (0.0004)	
training:	Epoch: [81][62/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [81][63/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [81][64/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [81][65/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [81][66/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [81][67/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [81][68/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [81][69/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [81][70/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [81][71/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [81][72/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [81][73/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [81][74/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [81][75/408]	Loss 0.0009 (0.0004)	
training:	Epoch: [81][76/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [81][77/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [81][78/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [81][79/408]	Loss 0.0016 (0.0004)	
training:	Epoch: [81][80/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [81][81/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [81][82/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [81][83/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [81][84/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [81][85/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [81][86/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [81][87/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [81][88/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [81][89/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [81][90/408]	Loss 0.0004 (0.0004)	
training:	Epoch: [81][91/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [81][92/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [81][93/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [81][94/408]	Loss 0.0026 (0.0004)	
training:	Epoch: [81][95/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [81][96/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [81][97/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [81][98/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [81][99/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [81][100/408]	Loss 0.0006 (0.0004)	
training:	Epoch: [81][101/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [81][102/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [81][103/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [81][104/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [81][105/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [81][106/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [81][107/408]	Loss 0.0004 (0.0004)	
training:	Epoch: [81][108/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [81][109/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [81][110/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [81][111/408]	Loss 0.0110 (0.0005)	
training:	Epoch: [81][112/408]	Loss 0.0002 (0.0005)	
training:	Epoch: [81][113/408]	Loss 0.0002 (0.0005)	
training:	Epoch: [81][114/408]	Loss 0.0002 (0.0005)	
training:	Epoch: [81][115/408]	Loss 0.0002 (0.0005)	
training:	Epoch: [81][116/408]	Loss 0.0002 (0.0005)	
training:	Epoch: [81][117/408]	Loss 0.0002 (0.0005)	
training:	Epoch: [81][118/408]	Loss 0.0002 (0.0005)	
training:	Epoch: [81][119/408]	Loss 0.0003 (0.0005)	
training:	Epoch: [81][120/408]	Loss 0.0002 (0.0005)	
training:	Epoch: [81][121/408]	Loss 0.0055 (0.0005)	
training:	Epoch: [81][122/408]	Loss 0.0002 (0.0005)	
training:	Epoch: [81][123/408]	Loss 0.0002 (0.0005)	
training:	Epoch: [81][124/408]	Loss 0.0002 (0.0005)	
training:	Epoch: [81][125/408]	Loss 0.0002 (0.0005)	
training:	Epoch: [81][126/408]	Loss 0.0003 (0.0005)	
training:	Epoch: [81][127/408]	Loss 0.0004 (0.0005)	
training:	Epoch: [81][128/408]	Loss 0.0002 (0.0005)	
training:	Epoch: [81][129/408]	Loss 0.0002 (0.0005)	
training:	Epoch: [81][130/408]	Loss 0.0007 (0.0005)	
training:	Epoch: [81][131/408]	Loss 0.0002 (0.0005)	
training:	Epoch: [81][132/408]	Loss 0.0002 (0.0005)	
training:	Epoch: [81][133/408]	Loss 0.0002 (0.0005)	
training:	Epoch: [81][134/408]	Loss 0.0002 (0.0005)	
training:	Epoch: [81][135/408]	Loss 0.0002 (0.0005)	
training:	Epoch: [81][136/408]	Loss 0.0002 (0.0005)	
training:	Epoch: [81][137/408]	Loss 0.0003 (0.0005)	
training:	Epoch: [81][138/408]	Loss 0.0002 (0.0005)	
training:	Epoch: [81][139/408]	Loss 0.0002 (0.0005)	
training:	Epoch: [81][140/408]	Loss 0.0002 (0.0005)	
training:	Epoch: [81][141/408]	Loss 0.0003 (0.0005)	
training:	Epoch: [81][142/408]	Loss 0.0002 (0.0005)	
training:	Epoch: [81][143/408]	Loss 0.0002 (0.0005)	
training:	Epoch: [81][144/408]	Loss 0.0002 (0.0005)	
training:	Epoch: [81][145/408]	Loss 0.0002 (0.0005)	
training:	Epoch: [81][146/408]	Loss 0.0002 (0.0005)	
training:	Epoch: [81][147/408]	Loss 0.0002 (0.0005)	
training:	Epoch: [81][148/408]	Loss 0.0002 (0.0005)	
training:	Epoch: [81][149/408]	Loss 0.0003 (0.0005)	
training:	Epoch: [81][150/408]	Loss 0.0002 (0.0005)	
training:	Epoch: [81][151/408]	Loss 0.0002 (0.0005)	
training:	Epoch: [81][152/408]	Loss 0.0002 (0.0005)	
training:	Epoch: [81][153/408]	Loss 0.0002 (0.0005)	
training:	Epoch: [81][154/408]	Loss 0.0006 (0.0005)	
training:	Epoch: [81][155/408]	Loss 0.0003 (0.0005)	
training:	Epoch: [81][156/408]	Loss 0.0003 (0.0005)	
training:	Epoch: [81][157/408]	Loss 0.0002 (0.0005)	
training:	Epoch: [81][158/408]	Loss 0.0003 (0.0005)	
training:	Epoch: [81][159/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [81][160/408]	Loss 0.0008 (0.0005)	
training:	Epoch: [81][161/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [81][162/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [81][163/408]	Loss 0.0005 (0.0004)	
training:	Epoch: [81][164/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [81][165/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [81][166/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [81][167/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [81][168/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [81][169/408]	Loss 0.0005 (0.0004)	
training:	Epoch: [81][170/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [81][171/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [81][172/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [81][173/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [81][174/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [81][175/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [81][176/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [81][177/408]	Loss 0.0007 (0.0004)	
training:	Epoch: [81][178/408]	Loss 0.0006 (0.0004)	
training:	Epoch: [81][179/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [81][180/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [81][181/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [81][182/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [81][183/408]	Loss 0.0008 (0.0004)	
training:	Epoch: [81][184/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [81][185/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [81][186/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [81][187/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [81][188/408]	Loss 0.0008 (0.0004)	
training:	Epoch: [81][189/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [81][190/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [81][191/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [81][192/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [81][193/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [81][194/408]	Loss 0.0006 (0.0004)	
training:	Epoch: [81][195/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [81][196/408]	Loss 0.0007 (0.0004)	
training:	Epoch: [81][197/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [81][198/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [81][199/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [81][200/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [81][201/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [81][202/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [81][203/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [81][204/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [81][205/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [81][206/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [81][207/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [81][208/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [81][209/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [81][210/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [81][211/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [81][212/408]	Loss 0.0007 (0.0004)	
training:	Epoch: [81][213/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [81][214/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [81][215/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [81][216/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [81][217/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [81][218/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [81][219/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [81][220/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [81][221/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [81][222/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [81][223/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [81][224/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [81][225/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [81][226/408]	Loss 0.0004 (0.0004)	
training:	Epoch: [81][227/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [81][228/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [81][229/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [81][230/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [81][231/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [81][232/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [81][233/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [81][234/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [81][235/408]	Loss 0.0006 (0.0004)	
training:	Epoch: [81][236/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [81][237/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [81][238/408]	Loss 0.0008 (0.0004)	
training:	Epoch: [81][239/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [81][240/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [81][241/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [81][242/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [81][243/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [81][244/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [81][245/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [81][246/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [81][247/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [81][248/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [81][249/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [81][250/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [81][251/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [81][252/408]	Loss 0.0019 (0.0004)	
training:	Epoch: [81][253/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [81][254/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [81][255/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [81][256/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [81][257/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [81][258/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [81][259/408]	Loss 0.0007 (0.0004)	
training:	Epoch: [81][260/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [81][261/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [81][262/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [81][263/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [81][264/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [81][265/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [81][266/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [81][267/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [81][268/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [81][269/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [81][270/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [81][271/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [81][272/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [81][273/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [81][274/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [81][275/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [81][276/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [81][277/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [81][278/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [81][279/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [81][280/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [81][281/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [81][282/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [81][283/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [81][284/408]	Loss 0.0005 (0.0004)	
training:	Epoch: [81][285/408]	Loss 0.0036 (0.0004)	
training:	Epoch: [81][286/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [81][287/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [81][288/408]	Loss 0.0013 (0.0004)	
training:	Epoch: [81][289/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [81][290/408]	Loss 0.0005 (0.0004)	
training:	Epoch: [81][291/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [81][292/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [81][293/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [81][294/408]	Loss 0.0007 (0.0004)	
training:	Epoch: [81][295/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [81][296/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [81][297/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [81][298/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [81][299/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [81][300/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [81][301/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [81][302/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [81][303/408]	Loss 0.0010 (0.0004)	
training:	Epoch: [81][304/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [81][305/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [81][306/408]	Loss 0.0004 (0.0004)	
training:	Epoch: [81][307/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [81][308/408]	Loss 0.0004 (0.0004)	
training:	Epoch: [81][309/408]	Loss 0.0006 (0.0004)	
training:	Epoch: [81][310/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [81][311/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [81][312/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [81][313/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [81][314/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [81][315/408]	Loss 0.0005 (0.0004)	
training:	Epoch: [81][316/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [81][317/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [81][318/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [81][319/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [81][320/408]	Loss 0.0012 (0.0004)	
training:	Epoch: [81][321/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [81][322/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [81][323/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [81][324/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [81][325/408]	Loss 0.0006 (0.0004)	
training:	Epoch: [81][326/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [81][327/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [81][328/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [81][329/408]	Loss 0.0010 (0.0004)	
training:	Epoch: [81][330/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [81][331/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [81][332/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [81][333/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [81][334/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [81][335/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [81][336/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [81][337/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [81][338/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [81][339/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [81][340/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [81][341/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [81][342/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [81][343/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [81][344/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [81][345/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [81][346/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [81][347/408]	Loss 0.0030 (0.0004)	
training:	Epoch: [81][348/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [81][349/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [81][350/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [81][351/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [81][352/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [81][353/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [81][354/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [81][355/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [81][356/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [81][357/408]	Loss 0.0006 (0.0004)	
training:	Epoch: [81][358/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [81][359/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [81][360/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [81][361/408]	Loss 0.0005 (0.0004)	
training:	Epoch: [81][362/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [81][363/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [81][364/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [81][365/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [81][366/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [81][367/408]	Loss 0.0022 (0.0004)	
training:	Epoch: [81][368/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [81][369/408]	Loss 0.0004 (0.0004)	
training:	Epoch: [81][370/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [81][371/408]	Loss 0.0006 (0.0004)	
training:	Epoch: [81][372/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [81][373/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [81][374/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [81][375/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [81][376/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [81][377/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [81][378/408]	Loss 0.0015 (0.0004)	
training:	Epoch: [81][379/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [81][380/408]	Loss 0.0005 (0.0004)	
training:	Epoch: [81][381/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [81][382/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [81][383/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [81][384/408]	Loss 0.0006 (0.0004)	
training:	Epoch: [81][385/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [81][386/408]	Loss 0.0007 (0.0004)	
training:	Epoch: [81][387/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [81][388/408]	Loss 0.0004 (0.0004)	
training:	Epoch: [81][389/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [81][390/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [81][391/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [81][392/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [81][393/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [81][394/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [81][395/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [81][396/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [81][397/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [81][398/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [81][399/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [81][400/408]	Loss 0.0005 (0.0004)	
training:	Epoch: [81][401/408]	Loss 0.0037 (0.0004)	
training:	Epoch: [81][402/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [81][403/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [81][404/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [81][405/408]	Loss 0.0047 (0.0004)	
training:	Epoch: [81][406/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [81][407/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [81][408/408]	Loss 0.0002 (0.0004)	
Training:	 Loss: 0.0004

Training:	 ACC: 1.0000 1.0000 1.0000 1.0000
Validation:	 ACC: 0.7796 0.7801 0.7912 0.7679
Validation:	 Best_BACC: 0.8226 0.8250 0.8772 0.7679
Validation:	 Loss: 1.4460
Pretraining:	Epoch 82/200
----------
training:	Epoch: [82][1/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [82][2/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [82][3/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [82][4/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [82][5/408]	Loss 0.0013 (0.0004)	
training:	Epoch: [82][6/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [82][7/408]	Loss 0.0005 (0.0004)	
training:	Epoch: [82][8/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [82][9/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [82][10/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [82][11/408]	Loss 0.0034 (0.0006)	
training:	Epoch: [82][12/408]	Loss 0.0002 (0.0006)	
training:	Epoch: [82][13/408]	Loss 0.0002 (0.0006)	
training:	Epoch: [82][14/408]	Loss 0.0002 (0.0005)	
training:	Epoch: [82][15/408]	Loss 0.0003 (0.0005)	
training:	Epoch: [82][16/408]	Loss 0.0002 (0.0005)	
training:	Epoch: [82][17/408]	Loss 0.0002 (0.0005)	
training:	Epoch: [82][18/408]	Loss 0.0002 (0.0005)	
training:	Epoch: [82][19/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [82][20/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [82][21/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [82][22/408]	Loss 0.0011 (0.0005)	
training:	Epoch: [82][23/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [82][24/408]	Loss 0.0007 (0.0005)	
training:	Epoch: [82][25/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [82][26/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [82][27/408]	Loss 0.0007 (0.0004)	
training:	Epoch: [82][28/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [82][29/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [82][30/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [82][31/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [82][32/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [82][33/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [82][34/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [82][35/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [82][36/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [82][37/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [82][38/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [82][39/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [82][40/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [82][41/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [82][42/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [82][43/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [82][44/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [82][45/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [82][46/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [82][47/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [82][48/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [82][49/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [82][50/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [82][51/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [82][52/408]	Loss 0.0238 (0.0008)	
training:	Epoch: [82][53/408]	Loss 0.0002 (0.0008)	
training:	Epoch: [82][54/408]	Loss 0.0002 (0.0008)	
training:	Epoch: [82][55/408]	Loss 0.0004 (0.0008)	
training:	Epoch: [82][56/408]	Loss 0.0004 (0.0008)	
training:	Epoch: [82][57/408]	Loss 0.0002 (0.0007)	
training:	Epoch: [82][58/408]	Loss 0.0002 (0.0007)	
training:	Epoch: [82][59/408]	Loss 0.0002 (0.0007)	
training:	Epoch: [82][60/408]	Loss 0.0002 (0.0007)	
training:	Epoch: [82][61/408]	Loss 0.0002 (0.0007)	
training:	Epoch: [82][62/408]	Loss 0.0002 (0.0007)	
training:	Epoch: [82][63/408]	Loss 0.0002 (0.0007)	
training:	Epoch: [82][64/408]	Loss 0.0002 (0.0007)	
training:	Epoch: [82][65/408]	Loss 0.0002 (0.0007)	
training:	Epoch: [82][66/408]	Loss 0.0002 (0.0007)	
training:	Epoch: [82][67/408]	Loss 0.0002 (0.0007)	
training:	Epoch: [82][68/408]	Loss 0.0002 (0.0007)	
training:	Epoch: [82][69/408]	Loss 0.0002 (0.0006)	
training:	Epoch: [82][70/408]	Loss 0.0002 (0.0006)	
training:	Epoch: [82][71/408]	Loss 0.0002 (0.0006)	
training:	Epoch: [82][72/408]	Loss 0.0002 (0.0006)	
training:	Epoch: [82][73/408]	Loss 0.0003 (0.0006)	
training:	Epoch: [82][74/408]	Loss 0.0002 (0.0006)	
training:	Epoch: [82][75/408]	Loss 0.0002 (0.0006)	
training:	Epoch: [82][76/408]	Loss 0.0002 (0.0006)	
training:	Epoch: [82][77/408]	Loss 0.0002 (0.0006)	
training:	Epoch: [82][78/408]	Loss 0.0002 (0.0006)	
training:	Epoch: [82][79/408]	Loss 0.0005 (0.0006)	
training:	Epoch: [82][80/408]	Loss 0.0003 (0.0006)	
training:	Epoch: [82][81/408]	Loss 0.0002 (0.0006)	
training:	Epoch: [82][82/408]	Loss 0.0006 (0.0006)	
training:	Epoch: [82][83/408]	Loss 0.0002 (0.0006)	
training:	Epoch: [82][84/408]	Loss 0.0002 (0.0006)	
training:	Epoch: [82][85/408]	Loss 0.0005 (0.0006)	
training:	Epoch: [82][86/408]	Loss 0.0002 (0.0006)	
training:	Epoch: [82][87/408]	Loss 0.0006 (0.0006)	
training:	Epoch: [82][88/408]	Loss 0.0002 (0.0006)	
training:	Epoch: [82][89/408]	Loss 0.0002 (0.0006)	
training:	Epoch: [82][90/408]	Loss 0.0006 (0.0006)	
training:	Epoch: [82][91/408]	Loss 0.0003 (0.0006)	
training:	Epoch: [82][92/408]	Loss 0.0002 (0.0006)	
training:	Epoch: [82][93/408]	Loss 0.0003 (0.0006)	
training:	Epoch: [82][94/408]	Loss 0.0003 (0.0006)	
training:	Epoch: [82][95/408]	Loss 0.0002 (0.0005)	
training:	Epoch: [82][96/408]	Loss 0.0002 (0.0005)	
training:	Epoch: [82][97/408]	Loss 0.0002 (0.0005)	
training:	Epoch: [82][98/408]	Loss 0.0006 (0.0005)	
training:	Epoch: [82][99/408]	Loss 0.0002 (0.0005)	
training:	Epoch: [82][100/408]	Loss 0.0002 (0.0005)	
training:	Epoch: [82][101/408]	Loss 0.0004 (0.0005)	
training:	Epoch: [82][102/408]	Loss 0.0002 (0.0005)	
training:	Epoch: [82][103/408]	Loss 0.0003 (0.0005)	
training:	Epoch: [82][104/408]	Loss 0.0002 (0.0005)	
training:	Epoch: [82][105/408]	Loss 0.0002 (0.0005)	
training:	Epoch: [82][106/408]	Loss 0.0002 (0.0005)	
training:	Epoch: [82][107/408]	Loss 0.0002 (0.0005)	
training:	Epoch: [82][108/408]	Loss 0.0002 (0.0005)	
training:	Epoch: [82][109/408]	Loss 0.0002 (0.0005)	
training:	Epoch: [82][110/408]	Loss 0.0002 (0.0005)	
training:	Epoch: [82][111/408]	Loss 0.0005 (0.0005)	
training:	Epoch: [82][112/408]	Loss 0.0002 (0.0005)	
training:	Epoch: [82][113/408]	Loss 0.0002 (0.0005)	
training:	Epoch: [82][114/408]	Loss 0.0002 (0.0005)	
training:	Epoch: [82][115/408]	Loss 0.0003 (0.0005)	
training:	Epoch: [82][116/408]	Loss 0.0002 (0.0005)	
training:	Epoch: [82][117/408]	Loss 0.0002 (0.0005)	
training:	Epoch: [82][118/408]	Loss 0.0002 (0.0005)	
training:	Epoch: [82][119/408]	Loss 0.0002 (0.0005)	
training:	Epoch: [82][120/408]	Loss 0.0004 (0.0005)	
training:	Epoch: [82][121/408]	Loss 0.0006 (0.0005)	
training:	Epoch: [82][122/408]	Loss 0.0002 (0.0005)	
training:	Epoch: [82][123/408]	Loss 0.0002 (0.0005)	
training:	Epoch: [82][124/408]	Loss 0.0002 (0.0005)	
training:	Epoch: [82][125/408]	Loss 0.0003 (0.0005)	
training:	Epoch: [82][126/408]	Loss 0.0005 (0.0005)	
training:	Epoch: [82][127/408]	Loss 0.0002 (0.0005)	
training:	Epoch: [82][128/408]	Loss 0.0002 (0.0005)	
training:	Epoch: [82][129/408]	Loss 0.0002 (0.0005)	
training:	Epoch: [82][130/408]	Loss 0.0002 (0.0005)	
training:	Epoch: [82][131/408]	Loss 0.0002 (0.0005)	
training:	Epoch: [82][132/408]	Loss 0.0002 (0.0005)	
training:	Epoch: [82][133/408]	Loss 0.0002 (0.0005)	
training:	Epoch: [82][134/408]	Loss 0.0002 (0.0005)	
training:	Epoch: [82][135/408]	Loss 0.0002 (0.0005)	
training:	Epoch: [82][136/408]	Loss 0.0004 (0.0005)	
training:	Epoch: [82][137/408]	Loss 0.0002 (0.0005)	
training:	Epoch: [82][138/408]	Loss 0.0002 (0.0005)	
training:	Epoch: [82][139/408]	Loss 0.0002 (0.0005)	
training:	Epoch: [82][140/408]	Loss 0.0010 (0.0005)	
training:	Epoch: [82][141/408]	Loss 0.0002 (0.0005)	
training:	Epoch: [82][142/408]	Loss 0.0002 (0.0005)	
training:	Epoch: [82][143/408]	Loss 0.0003 (0.0005)	
training:	Epoch: [82][144/408]	Loss 0.0003 (0.0005)	
training:	Epoch: [82][145/408]	Loss 0.0002 (0.0005)	
training:	Epoch: [82][146/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [82][147/408]	Loss 0.0005 (0.0004)	
training:	Epoch: [82][148/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [82][149/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [82][150/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [82][151/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [82][152/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [82][153/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [82][154/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [82][155/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [82][156/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [82][157/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [82][158/408]	Loss 0.0004 (0.0004)	
training:	Epoch: [82][159/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [82][160/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [82][161/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [82][162/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [82][163/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [82][164/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [82][165/408]	Loss 0.0004 (0.0004)	
training:	Epoch: [82][166/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [82][167/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [82][168/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [82][169/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [82][170/408]	Loss 0.0005 (0.0004)	
training:	Epoch: [82][171/408]	Loss 0.0005 (0.0004)	
training:	Epoch: [82][172/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [82][173/408]	Loss 0.0005 (0.0004)	
training:	Epoch: [82][174/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [82][175/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [82][176/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [82][177/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [82][178/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [82][179/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [82][180/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [82][181/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [82][182/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [82][183/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [82][184/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [82][185/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [82][186/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [82][187/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [82][188/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [82][189/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [82][190/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [82][191/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [82][192/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [82][193/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [82][194/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [82][195/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [82][196/408]	Loss 0.0005 (0.0004)	
training:	Epoch: [82][197/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [82][198/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [82][199/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [82][200/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [82][201/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [82][202/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [82][203/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [82][204/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [82][205/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [82][206/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [82][207/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [82][208/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [82][209/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [82][210/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [82][211/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [82][212/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [82][213/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [82][214/408]	Loss 0.0005 (0.0004)	
training:	Epoch: [82][215/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [82][216/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [82][217/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [82][218/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [82][219/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [82][220/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [82][221/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [82][222/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [82][223/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [82][224/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [82][225/408]	Loss 0.0006 (0.0004)	
training:	Epoch: [82][226/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [82][227/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [82][228/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [82][229/408]	Loss 0.0004 (0.0004)	
training:	Epoch: [82][230/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [82][231/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [82][232/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [82][233/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [82][234/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [82][235/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [82][236/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [82][237/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [82][238/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [82][239/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [82][240/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [82][241/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [82][242/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [82][243/408]	Loss 0.0005 (0.0004)	
training:	Epoch: [82][244/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [82][245/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [82][246/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [82][247/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [82][248/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [82][249/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [82][250/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [82][251/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [82][252/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [82][253/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [82][254/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [82][255/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [82][256/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [82][257/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [82][258/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [82][259/408]	Loss 0.0005 (0.0004)	
training:	Epoch: [82][260/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [82][261/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [82][262/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [82][263/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [82][264/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [82][265/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [82][266/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [82][267/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [82][268/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [82][269/408]	Loss 0.0005 (0.0004)	
training:	Epoch: [82][270/408]	Loss 0.0012 (0.0004)	
training:	Epoch: [82][271/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [82][272/408]	Loss 0.0005 (0.0004)	
training:	Epoch: [82][273/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [82][274/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [82][275/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [82][276/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [82][277/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [82][278/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [82][279/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [82][280/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [82][281/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [82][282/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [82][283/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [82][284/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [82][285/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [82][286/408]	Loss 0.0004 (0.0004)	
training:	Epoch: [82][287/408]	Loss 0.0011 (0.0004)	
training:	Epoch: [82][288/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [82][289/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [82][290/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [82][291/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [82][292/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [82][293/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [82][294/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [82][295/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [82][296/408]	Loss 0.0005 (0.0004)	
training:	Epoch: [82][297/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [82][298/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [82][299/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [82][300/408]	Loss 0.0003 (0.0003)	
training:	Epoch: [82][301/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [82][302/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [82][303/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [82][304/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [82][305/408]	Loss 0.0003 (0.0003)	
training:	Epoch: [82][306/408]	Loss 0.0008 (0.0003)	
training:	Epoch: [82][307/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [82][308/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [82][309/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [82][310/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [82][311/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [82][312/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [82][313/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [82][314/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [82][315/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [82][316/408]	Loss 0.0003 (0.0003)	
training:	Epoch: [82][317/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [82][318/408]	Loss 0.0005 (0.0003)	
training:	Epoch: [82][319/408]	Loss 0.0008 (0.0003)	
training:	Epoch: [82][320/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [82][321/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [82][322/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [82][323/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [82][324/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [82][325/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [82][326/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [82][327/408]	Loss 0.0006 (0.0003)	
training:	Epoch: [82][328/408]	Loss 0.0004 (0.0003)	
training:	Epoch: [82][329/408]	Loss 0.0004 (0.0003)	
training:	Epoch: [82][330/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [82][331/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [82][332/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [82][333/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [82][334/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [82][335/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [82][336/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [82][337/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [82][338/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [82][339/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [82][340/408]	Loss 0.0004 (0.0003)	
training:	Epoch: [82][341/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [82][342/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [82][343/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [82][344/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [82][345/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [82][346/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [82][347/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [82][348/408]	Loss 0.0003 (0.0003)	
training:	Epoch: [82][349/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [82][350/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [82][351/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [82][352/408]	Loss 0.0011 (0.0003)	
training:	Epoch: [82][353/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [82][354/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [82][355/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [82][356/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [82][357/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [82][358/408]	Loss 0.0003 (0.0003)	
training:	Epoch: [82][359/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [82][360/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [82][361/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [82][362/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [82][363/408]	Loss 0.0004 (0.0003)	
training:	Epoch: [82][364/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [82][365/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [82][366/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [82][367/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [82][368/408]	Loss 0.0006 (0.0003)	
training:	Epoch: [82][369/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [82][370/408]	Loss 0.0008 (0.0003)	
training:	Epoch: [82][371/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [82][372/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [82][373/408]	Loss 0.0003 (0.0003)	
training:	Epoch: [82][374/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [82][375/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [82][376/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [82][377/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [82][378/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [82][379/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [82][380/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [82][381/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [82][382/408]	Loss 0.0003 (0.0003)	
training:	Epoch: [82][383/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [82][384/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [82][385/408]	Loss 0.0004 (0.0003)	
training:	Epoch: [82][386/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [82][387/408]	Loss 0.0003 (0.0003)	
training:	Epoch: [82][388/408]	Loss 0.0003 (0.0003)	
training:	Epoch: [82][389/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [82][390/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [82][391/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [82][392/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [82][393/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [82][394/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [82][395/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [82][396/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [82][397/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [82][398/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [82][399/408]	Loss 0.0003 (0.0003)	
training:	Epoch: [82][400/408]	Loss 0.0003 (0.0003)	
training:	Epoch: [82][401/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [82][402/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [82][403/408]	Loss 0.0005 (0.0003)	
training:	Epoch: [82][404/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [82][405/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [82][406/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [82][407/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [82][408/408]	Loss 0.0002 (0.0003)	
Training:	 Loss: 0.0003

Training:	 ACC: 1.0000 1.0000 1.0000 1.0000
Validation:	 ACC: 0.7810 0.7817 0.7973 0.7646
Validation:	 Best_BACC: 0.8226 0.8250 0.8772 0.7679
Validation:	 Loss: 1.4691
Pretraining:	Epoch 83/200
----------
training:	Epoch: [83][1/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [83][2/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [83][3/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [83][4/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [83][5/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [83][6/408]	Loss 0.0003 (0.0002)	
training:	Epoch: [83][7/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [83][8/408]	Loss 0.0005 (0.0002)	
training:	Epoch: [83][9/408]	Loss 0.0004 (0.0003)	
training:	Epoch: [83][10/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [83][11/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [83][12/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [83][13/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [83][14/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [83][15/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [83][16/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [83][17/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [83][18/408]	Loss 0.0003 (0.0002)	
training:	Epoch: [83][19/408]	Loss 0.0003 (0.0002)	
training:	Epoch: [83][20/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [83][21/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [83][22/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [83][23/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [83][24/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [83][25/408]	Loss 0.0003 (0.0002)	
training:	Epoch: [83][26/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [83][27/408]	Loss 0.0007 (0.0002)	
training:	Epoch: [83][28/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [83][29/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [83][30/408]	Loss 0.0004 (0.0002)	
training:	Epoch: [83][31/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [83][32/408]	Loss 0.0004 (0.0002)	
training:	Epoch: [83][33/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [83][34/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [83][35/408]	Loss 0.0006 (0.0003)	
training:	Epoch: [83][36/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [83][37/408]	Loss 0.0008 (0.0003)	
training:	Epoch: [83][38/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [83][39/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [83][40/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [83][41/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [83][42/408]	Loss 0.0006 (0.0003)	
training:	Epoch: [83][43/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [83][44/408]	Loss 0.0003 (0.0003)	
training:	Epoch: [83][45/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [83][46/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [83][47/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [83][48/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [83][49/408]	Loss 0.0006 (0.0003)	
training:	Epoch: [83][50/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [83][51/408]	Loss 0.0005 (0.0003)	
training:	Epoch: [83][52/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [83][53/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [83][54/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [83][55/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [83][56/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [83][57/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [83][58/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [83][59/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [83][60/408]	Loss 0.0004 (0.0003)	
training:	Epoch: [83][61/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [83][62/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [83][63/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [83][64/408]	Loss 0.0003 (0.0003)	
training:	Epoch: [83][65/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [83][66/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [83][67/408]	Loss 0.0003 (0.0003)	
training:	Epoch: [83][68/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [83][69/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [83][70/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [83][71/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [83][72/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [83][73/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [83][74/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [83][75/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [83][76/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [83][77/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [83][78/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [83][79/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [83][80/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [83][81/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [83][82/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [83][83/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [83][84/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [83][85/408]	Loss 0.0003 (0.0002)	
training:	Epoch: [83][86/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [83][87/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [83][88/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [83][89/408]	Loss 0.0003 (0.0002)	
training:	Epoch: [83][90/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [83][91/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [83][92/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [83][93/408]	Loss 0.0003 (0.0002)	
training:	Epoch: [83][94/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [83][95/408]	Loss 0.0005 (0.0002)	
training:	Epoch: [83][96/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [83][97/408]	Loss 0.0005 (0.0002)	
training:	Epoch: [83][98/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [83][99/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [83][100/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [83][101/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [83][102/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [83][103/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [83][104/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [83][105/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [83][106/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [83][107/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [83][108/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [83][109/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [83][110/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [83][111/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [83][112/408]	Loss 0.0003 (0.0002)	
training:	Epoch: [83][113/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [83][114/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [83][115/408]	Loss 0.0004 (0.0002)	
training:	Epoch: [83][116/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [83][117/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [83][118/408]	Loss 0.0004 (0.0002)	
training:	Epoch: [83][119/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [83][120/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [83][121/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [83][122/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [83][123/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [83][124/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [83][125/408]	Loss 0.0016 (0.0003)	
training:	Epoch: [83][126/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [83][127/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [83][128/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [83][129/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [83][130/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [83][131/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [83][132/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [83][133/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [83][134/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [83][135/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [83][136/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [83][137/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [83][138/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [83][139/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [83][140/408]	Loss 0.0003 (0.0002)	
training:	Epoch: [83][141/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [83][142/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [83][143/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [83][144/408]	Loss 0.0003 (0.0002)	
training:	Epoch: [83][145/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [83][146/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [83][147/408]	Loss 0.0003 (0.0002)	
training:	Epoch: [83][148/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [83][149/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [83][150/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [83][151/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [83][152/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [83][153/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [83][154/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [83][155/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [83][156/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [83][157/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [83][158/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [83][159/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [83][160/408]	Loss 0.0003 (0.0002)	
training:	Epoch: [83][161/408]	Loss 0.0010 (0.0002)	
training:	Epoch: [83][162/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [83][163/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [83][164/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [83][165/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [83][166/408]	Loss 0.0005 (0.0002)	
training:	Epoch: [83][167/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [83][168/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [83][169/408]	Loss 0.0003 (0.0002)	
training:	Epoch: [83][170/408]	Loss 0.0005 (0.0002)	
training:	Epoch: [83][171/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [83][172/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [83][173/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [83][174/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [83][175/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [83][176/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [83][177/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [83][178/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [83][179/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [83][180/408]	Loss 0.0003 (0.0002)	
training:	Epoch: [83][181/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [83][182/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [83][183/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [83][184/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [83][185/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [83][186/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [83][187/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [83][188/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [83][189/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [83][190/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [83][191/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [83][192/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [83][193/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [83][194/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [83][195/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [83][196/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [83][197/408]	Loss 0.0007 (0.0002)	
training:	Epoch: [83][198/408]	Loss 0.0005 (0.0002)	
training:	Epoch: [83][199/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [83][200/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [83][201/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [83][202/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [83][203/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [83][204/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [83][205/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [83][206/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [83][207/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [83][208/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [83][209/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [83][210/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [83][211/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [83][212/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [83][213/408]	Loss 0.0003 (0.0002)	
training:	Epoch: [83][214/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [83][215/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [83][216/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [83][217/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [83][218/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [83][219/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [83][220/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [83][221/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [83][222/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [83][223/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [83][224/408]	Loss 0.0003 (0.0002)	
training:	Epoch: [83][225/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [83][226/408]	Loss 0.0004 (0.0002)	
training:	Epoch: [83][227/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [83][228/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [83][229/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [83][230/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [83][231/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [83][232/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [83][233/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [83][234/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [83][235/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [83][236/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [83][237/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [83][238/408]	Loss 0.0004 (0.0002)	
training:	Epoch: [83][239/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [83][240/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [83][241/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [83][242/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [83][243/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [83][244/408]	Loss 0.0005 (0.0002)	
training:	Epoch: [83][245/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [83][246/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [83][247/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [83][248/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [83][249/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [83][250/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [83][251/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [83][252/408]	Loss 0.0008 (0.0002)	
training:	Epoch: [83][253/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [83][254/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [83][255/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [83][256/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [83][257/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [83][258/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [83][259/408]	Loss 0.0005 (0.0002)	
training:	Epoch: [83][260/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [83][261/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [83][262/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [83][263/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [83][264/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [83][265/408]	Loss 0.0003 (0.0002)	
training:	Epoch: [83][266/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [83][267/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [83][268/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [83][269/408]	Loss 0.0006 (0.0002)	
training:	Epoch: [83][270/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [83][271/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [83][272/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [83][273/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [83][274/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [83][275/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [83][276/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [83][277/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [83][278/408]	Loss 0.0003 (0.0002)	
training:	Epoch: [83][279/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [83][280/408]	Loss 0.0003 (0.0002)	
training:	Epoch: [83][281/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [83][282/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [83][283/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [83][284/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [83][285/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [83][286/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [83][287/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [83][288/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [83][289/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [83][290/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [83][291/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [83][292/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [83][293/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [83][294/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [83][295/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [83][296/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [83][297/408]	Loss 0.0003 (0.0002)	
training:	Epoch: [83][298/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [83][299/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [83][300/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [83][301/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [83][302/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [83][303/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [83][304/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [83][305/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [83][306/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [83][307/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [83][308/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [83][309/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [83][310/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [83][311/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [83][312/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [83][313/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [83][314/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [83][315/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [83][316/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [83][317/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [83][318/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [83][319/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [83][320/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [83][321/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [83][322/408]	Loss 0.0005 (0.0002)	
training:	Epoch: [83][323/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [83][324/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [83][325/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [83][326/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [83][327/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [83][328/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [83][329/408]	Loss 0.0018 (0.0002)	
training:	Epoch: [83][330/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [83][331/408]	Loss 0.0004 (0.0002)	
training:	Epoch: [83][332/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [83][333/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [83][334/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [83][335/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [83][336/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [83][337/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [83][338/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [83][339/408]	Loss 0.0004 (0.0002)	
training:	Epoch: [83][340/408]	Loss 0.0003 (0.0002)	
training:	Epoch: [83][341/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [83][342/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [83][343/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [83][344/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [83][345/408]	Loss 0.0003 (0.0002)	
training:	Epoch: [83][346/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [83][347/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [83][348/408]	Loss 0.0004 (0.0002)	
training:	Epoch: [83][349/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [83][350/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [83][351/408]	Loss 0.0010 (0.0002)	
training:	Epoch: [83][352/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [83][353/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [83][354/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [83][355/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [83][356/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [83][357/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [83][358/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [83][359/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [83][360/408]	Loss 0.0006 (0.0002)	
training:	Epoch: [83][361/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [83][362/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [83][363/408]	Loss 0.0003 (0.0002)	
training:	Epoch: [83][364/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [83][365/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [83][366/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [83][367/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [83][368/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [83][369/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [83][370/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [83][371/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [83][372/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [83][373/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [83][374/408]	Loss 0.0004 (0.0002)	
training:	Epoch: [83][375/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [83][376/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [83][377/408]	Loss 0.0010 (0.0002)	
training:	Epoch: [83][378/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [83][379/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [83][380/408]	Loss 0.0004 (0.0002)	
training:	Epoch: [83][381/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [83][382/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [83][383/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [83][384/408]	Loss 0.0004 (0.0002)	
training:	Epoch: [83][385/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [83][386/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [83][387/408]	Loss 0.0003 (0.0002)	
training:	Epoch: [83][388/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [83][389/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [83][390/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [83][391/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [83][392/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [83][393/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [83][394/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [83][395/408]	Loss 0.0003 (0.0002)	
training:	Epoch: [83][396/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [83][397/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [83][398/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [83][399/408]	Loss 0.0003 (0.0002)	
training:	Epoch: [83][400/408]	Loss 0.0003 (0.0002)	
training:	Epoch: [83][401/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [83][402/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [83][403/408]	Loss 0.0013 (0.0002)	
training:	Epoch: [83][404/408]	Loss 0.0004 (0.0002)	
training:	Epoch: [83][405/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [83][406/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [83][407/408]	Loss 0.0004 (0.0002)	
training:	Epoch: [83][408/408]	Loss 0.0002 (0.0002)	
Training:	 Loss: 0.0002

Training:	 ACC: 1.0000 1.0000 1.0000 1.0000
Validation:	 ACC: 0.7830 0.7838 0.8025 0.7635
Validation:	 Best_BACC: 0.8226 0.8250 0.8772 0.7679
Validation:	 Loss: 1.4840
Pretraining:	Epoch 84/200
----------
training:	Epoch: [84][1/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [84][2/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [84][3/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [84][4/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [84][5/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [84][6/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [84][7/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [84][8/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [84][9/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [84][10/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [84][11/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [84][12/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [84][13/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [84][14/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [84][15/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [84][16/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [84][17/408]	Loss 0.0004 (0.0002)	
training:	Epoch: [84][18/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [84][19/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [84][20/408]	Loss 0.0005 (0.0002)	
training:	Epoch: [84][21/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [84][22/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [84][23/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [84][24/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [84][25/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [84][26/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [84][27/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [84][28/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [84][29/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [84][30/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [84][31/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [84][32/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [84][33/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [84][34/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [84][35/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [84][36/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [84][37/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [84][38/408]	Loss 0.0003 (0.0002)	
training:	Epoch: [84][39/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [84][40/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [84][41/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [84][42/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [84][43/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [84][44/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [84][45/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [84][46/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [84][47/408]	Loss 0.0003 (0.0002)	
training:	Epoch: [84][48/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [84][49/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [84][50/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [84][51/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [84][52/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [84][53/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [84][54/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [84][55/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [84][56/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [84][57/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [84][58/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [84][59/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [84][60/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [84][61/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [84][62/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [84][63/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [84][64/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [84][65/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [84][66/408]	Loss 0.0004 (0.0002)	
training:	Epoch: [84][67/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [84][68/408]	Loss 0.0006 (0.0002)	
training:	Epoch: [84][69/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [84][70/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [84][71/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [84][72/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [84][73/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [84][74/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [84][75/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [84][76/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [84][77/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [84][78/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [84][79/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [84][80/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [84][81/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [84][82/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [84][83/408]	Loss 0.0003 (0.0002)	
training:	Epoch: [84][84/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [84][85/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [84][86/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [84][87/408]	Loss 0.0004 (0.0002)	
training:	Epoch: [84][88/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [84][89/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [84][90/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [84][91/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [84][92/408]	Loss 0.0007 (0.0002)	
training:	Epoch: [84][93/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [84][94/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [84][95/408]	Loss 0.0006 (0.0002)	
training:	Epoch: [84][96/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [84][97/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [84][98/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [84][99/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [84][100/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [84][101/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [84][102/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [84][103/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [84][104/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [84][105/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [84][106/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [84][107/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [84][108/408]	Loss 0.0003 (0.0002)	
training:	Epoch: [84][109/408]	Loss 0.0008 (0.0002)	
training:	Epoch: [84][110/408]	Loss 0.0004 (0.0002)	
training:	Epoch: [84][111/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [84][112/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [84][113/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [84][114/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [84][115/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [84][116/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [84][117/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [84][118/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [84][119/408]	Loss 0.0004 (0.0002)	
training:	Epoch: [84][120/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [84][121/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [84][122/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [84][123/408]	Loss 0.0004 (0.0002)	
training:	Epoch: [84][124/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [84][125/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [84][126/408]	Loss 0.0003 (0.0002)	
training:	Epoch: [84][127/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [84][128/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [84][129/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [84][130/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [84][131/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [84][132/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [84][133/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [84][134/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [84][135/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [84][136/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [84][137/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [84][138/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [84][139/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [84][140/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [84][141/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [84][142/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [84][143/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [84][144/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [84][145/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [84][146/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [84][147/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [84][148/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [84][149/408]	Loss 0.0005 (0.0002)	
training:	Epoch: [84][150/408]	Loss 0.0004 (0.0002)	
training:	Epoch: [84][151/408]	Loss 0.0004 (0.0002)	
training:	Epoch: [84][152/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [84][153/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [84][154/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [84][155/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [84][156/408]	Loss 0.0004 (0.0002)	
training:	Epoch: [84][157/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [84][158/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [84][159/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [84][160/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [84][161/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [84][162/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [84][163/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [84][164/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [84][165/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [84][166/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [84][167/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [84][168/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [84][169/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [84][170/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [84][171/408]	Loss 0.0003 (0.0002)	
training:	Epoch: [84][172/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [84][173/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [84][174/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [84][175/408]	Loss 0.0005 (0.0002)	
training:	Epoch: [84][176/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [84][177/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [84][178/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [84][179/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [84][180/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [84][181/408]	Loss 0.0003 (0.0002)	
training:	Epoch: [84][182/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [84][183/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [84][184/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [84][185/408]	Loss 0.0004 (0.0002)	
training:	Epoch: [84][186/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [84][187/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [84][188/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [84][189/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [84][190/408]	Loss 0.0003 (0.0002)	
training:	Epoch: [84][191/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [84][192/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [84][193/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [84][194/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [84][195/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [84][196/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [84][197/408]	Loss 0.0004 (0.0002)	
training:	Epoch: [84][198/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [84][199/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [84][200/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [84][201/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [84][202/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [84][203/408]	Loss 0.0014 (0.0002)	
training:	Epoch: [84][204/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [84][205/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [84][206/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [84][207/408]	Loss 0.0003 (0.0002)	
training:	Epoch: [84][208/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [84][209/408]	Loss 0.0007 (0.0002)	
training:	Epoch: [84][210/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [84][211/408]	Loss 0.0004 (0.0002)	
training:	Epoch: [84][212/408]	Loss 0.0009 (0.0002)	
training:	Epoch: [84][213/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [84][214/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [84][215/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [84][216/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [84][217/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [84][218/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [84][219/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [84][220/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [84][221/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [84][222/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [84][223/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [84][224/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [84][225/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [84][226/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [84][227/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [84][228/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [84][229/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [84][230/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [84][231/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [84][232/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [84][233/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [84][234/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [84][235/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [84][236/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [84][237/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [84][238/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [84][239/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [84][240/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [84][241/408]	Loss 0.0003 (0.0002)	
training:	Epoch: [84][242/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [84][243/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [84][244/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [84][245/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [84][246/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [84][247/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [84][248/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [84][249/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [84][250/408]	Loss 0.0004 (0.0002)	
training:	Epoch: [84][251/408]	Loss 0.0004 (0.0002)	
training:	Epoch: [84][252/408]	Loss 0.0007 (0.0002)	
training:	Epoch: [84][253/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [84][254/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [84][255/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [84][256/408]	Loss 0.0003 (0.0002)	
training:	Epoch: [84][257/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [84][258/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [84][259/408]	Loss 0.0001 (0.0002)	
training:	Epoch: [84][260/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [84][261/408]	Loss 0.0010 (0.0002)	
training:	Epoch: [84][262/408]	Loss 0.0006 (0.0002)	
training:	Epoch: [84][263/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [84][264/408]	Loss 0.0003 (0.0002)	
training:	Epoch: [84][265/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [84][266/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [84][267/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [84][268/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [84][269/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [84][270/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [84][271/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [84][272/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [84][273/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [84][274/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [84][275/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [84][276/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [84][277/408]	Loss 0.0006 (0.0002)	
training:	Epoch: [84][278/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [84][279/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [84][280/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [84][281/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [84][282/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [84][283/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [84][284/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [84][285/408]	Loss 0.0003 (0.0002)	
training:	Epoch: [84][286/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [84][287/408]	Loss 0.0004 (0.0002)	
training:	Epoch: [84][288/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [84][289/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [84][290/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [84][291/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [84][292/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [84][293/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [84][294/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [84][295/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [84][296/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [84][297/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [84][298/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [84][299/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [84][300/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [84][301/408]	Loss 0.0006 (0.0002)	
training:	Epoch: [84][302/408]	Loss 0.0003 (0.0002)	
training:	Epoch: [84][303/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [84][304/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [84][305/408]	Loss 0.0001 (0.0002)	
training:	Epoch: [84][306/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [84][307/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [84][308/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [84][309/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [84][310/408]	Loss 0.0001 (0.0002)	
training:	Epoch: [84][311/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [84][312/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [84][313/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [84][314/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [84][315/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [84][316/408]	Loss 0.0006 (0.0002)	
training:	Epoch: [84][317/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [84][318/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [84][319/408]	Loss 0.0004 (0.0002)	
training:	Epoch: [84][320/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [84][321/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [84][322/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [84][323/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [84][324/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [84][325/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [84][326/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [84][327/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [84][328/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [84][329/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [84][330/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [84][331/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [84][332/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [84][333/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [84][334/408]	Loss 0.0004 (0.0002)	
training:	Epoch: [84][335/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [84][336/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [84][337/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [84][338/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [84][339/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [84][340/408]	Loss 0.0001 (0.0002)	
training:	Epoch: [84][341/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [84][342/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [84][343/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [84][344/408]	Loss 0.0005 (0.0002)	
training:	Epoch: [84][345/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [84][346/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [84][347/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [84][348/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [84][349/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [84][350/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [84][351/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [84][352/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [84][353/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [84][354/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [84][355/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [84][356/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [84][357/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [84][358/408]	Loss 0.0003 (0.0002)	
training:	Epoch: [84][359/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [84][360/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [84][361/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [84][362/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [84][363/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [84][364/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [84][365/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [84][366/408]	Loss 0.0012 (0.0002)	
training:	Epoch: [84][367/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [84][368/408]	Loss 0.0005 (0.0002)	
training:	Epoch: [84][369/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [84][370/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [84][371/408]	Loss 0.0006 (0.0002)	
training:	Epoch: [84][372/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [84][373/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [84][374/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [84][375/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [84][376/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [84][377/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [84][378/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [84][379/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [84][380/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [84][381/408]	Loss 0.0003 (0.0002)	
training:	Epoch: [84][382/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [84][383/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [84][384/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [84][385/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [84][386/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [84][387/408]	Loss 0.0001 (0.0002)	
training:	Epoch: [84][388/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [84][389/408]	Loss 0.0001 (0.0002)	
training:	Epoch: [84][390/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [84][391/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [84][392/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [84][393/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [84][394/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [84][395/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [84][396/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [84][397/408]	Loss 0.0003 (0.0002)	
training:	Epoch: [84][398/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [84][399/408]	Loss 0.0003 (0.0002)	
training:	Epoch: [84][400/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [84][401/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [84][402/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [84][403/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [84][404/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [84][405/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [84][406/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [84][407/408]	Loss 0.0003 (0.0002)	
training:	Epoch: [84][408/408]	Loss 0.0002 (0.0002)	
Training:	 Loss: 0.0002

Training:	 ACC: 1.0000 1.0000 1.0000 1.0000
Validation:	 ACC: 0.7832 0.7838 0.7973 0.7691
Validation:	 Best_BACC: 0.8226 0.8250 0.8772 0.7679
Validation:	 Loss: 1.5034
Pretraining:	Epoch 85/200
----------
training:	Epoch: [85][1/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [85][2/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [85][3/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [85][4/408]	Loss 0.0003 (0.0002)	
training:	Epoch: [85][5/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [85][6/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [85][7/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [85][8/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [85][9/408]	Loss 0.0005 (0.0002)	
training:	Epoch: [85][10/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [85][11/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [85][12/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [85][13/408]	Loss 0.0003 (0.0002)	
training:	Epoch: [85][14/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [85][15/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [85][16/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [85][17/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [85][18/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [85][19/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [85][20/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [85][21/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [85][22/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [85][23/408]	Loss 0.3129 (0.0138)	
training:	Epoch: [85][24/408]	Loss 0.0002 (0.0132)	
training:	Epoch: [85][25/408]	Loss 0.0002 (0.0127)	
training:	Epoch: [85][26/408]	Loss 0.0007 (0.0122)	
training:	Epoch: [85][27/408]	Loss 0.0002 (0.0118)	
training:	Epoch: [85][28/408]	Loss 0.0002 (0.0114)	
training:	Epoch: [85][29/408]	Loss 0.0002 (0.0110)	
training:	Epoch: [85][30/408]	Loss 0.0002 (0.0106)	
training:	Epoch: [85][31/408]	Loss 0.0002 (0.0103)	
training:	Epoch: [85][32/408]	Loss 0.0012 (0.0100)	
training:	Epoch: [85][33/408]	Loss 0.0002 (0.0097)	
training:	Epoch: [85][34/408]	Loss 0.0003 (0.0094)	
training:	Epoch: [85][35/408]	Loss 0.0002 (0.0092)	
training:	Epoch: [85][36/408]	Loss 0.0006 (0.0089)	
training:	Epoch: [85][37/408]	Loss 0.0003 (0.0087)	
training:	Epoch: [85][38/408]	Loss 0.0066 (0.0087)	
training:	Epoch: [85][39/408]	Loss 0.1240 (0.0116)	
training:	Epoch: [85][40/408]	Loss 0.0012 (0.0113)	
training:	Epoch: [85][41/408]	Loss 0.0002 (0.0111)	
training:	Epoch: [85][42/408]	Loss 0.0002 (0.0108)	
training:	Epoch: [85][43/408]	Loss 0.0002 (0.0106)	
training:	Epoch: [85][44/408]	Loss 0.0002 (0.0103)	
training:	Epoch: [85][45/408]	Loss 0.0006 (0.0101)	
training:	Epoch: [85][46/408]	Loss 0.0002 (0.0099)	
training:	Epoch: [85][47/408]	Loss 0.0002 (0.0097)	
training:	Epoch: [85][48/408]	Loss 0.0002 (0.0095)	
training:	Epoch: [85][49/408]	Loss 0.0002 (0.0093)	
training:	Epoch: [85][50/408]	Loss 0.0003 (0.0091)	
training:	Epoch: [85][51/408]	Loss 0.0002 (0.0090)	
training:	Epoch: [85][52/408]	Loss 0.0343 (0.0094)	
training:	Epoch: [85][53/408]	Loss 0.0053 (0.0094)	
training:	Epoch: [85][54/408]	Loss 0.0002 (0.0092)	
training:	Epoch: [85][55/408]	Loss 0.0002 (0.0090)	
training:	Epoch: [85][56/408]	Loss 0.0002 (0.0089)	
training:	Epoch: [85][57/408]	Loss 0.0009 (0.0087)	
training:	Epoch: [85][58/408]	Loss 0.0017 (0.0086)	
training:	Epoch: [85][59/408]	Loss 0.0309 (0.0090)	
training:	Epoch: [85][60/408]	Loss 0.0014 (0.0089)	
training:	Epoch: [85][61/408]	Loss 0.0068 (0.0088)	
training:	Epoch: [85][62/408]	Loss 0.0002 (0.0087)	
training:	Epoch: [85][63/408]	Loss 0.0003 (0.0086)	
training:	Epoch: [85][64/408]	Loss 0.0002 (0.0084)	
training:	Epoch: [85][65/408]	Loss 0.0002 (0.0083)	
training:	Epoch: [85][66/408]	Loss 0.0759 (0.0093)	
training:	Epoch: [85][67/408]	Loss 0.0002 (0.0092)	
training:	Epoch: [85][68/408]	Loss 0.3146 (0.0137)	
training:	Epoch: [85][69/408]	Loss 0.0005 (0.0135)	
training:	Epoch: [85][70/408]	Loss 0.0002 (0.0133)	
training:	Epoch: [85][71/408]	Loss 0.0002 (0.0131)	
training:	Epoch: [85][72/408]	Loss 0.0008 (0.0129)	
training:	Epoch: [85][73/408]	Loss 0.0004 (0.0128)	
training:	Epoch: [85][74/408]	Loss 0.0002 (0.0126)	
training:	Epoch: [85][75/408]	Loss 0.0002 (0.0124)	
training:	Epoch: [85][76/408]	Loss 0.6990 (0.0215)	
training:	Epoch: [85][77/408]	Loss 0.0002 (0.0212)	
training:	Epoch: [85][78/408]	Loss 0.0005 (0.0209)	
training:	Epoch: [85][79/408]	Loss 0.0019 (0.0207)	
training:	Epoch: [85][80/408]	Loss 0.0002 (0.0204)	
training:	Epoch: [85][81/408]	Loss 0.0002 (0.0202)	
training:	Epoch: [85][82/408]	Loss 0.0003 (0.0199)	
training:	Epoch: [85][83/408]	Loss 0.0002 (0.0197)	
training:	Epoch: [85][84/408]	Loss 0.0040 (0.0195)	
training:	Epoch: [85][85/408]	Loss 0.0003 (0.0193)	
training:	Epoch: [85][86/408]	Loss 0.0006 (0.0191)	
training:	Epoch: [85][87/408]	Loss 0.0004 (0.0189)	
training:	Epoch: [85][88/408]	Loss 0.2239 (0.0212)	
training:	Epoch: [85][89/408]	Loss 0.0003 (0.0209)	
training:	Epoch: [85][90/408]	Loss 0.1581 (0.0225)	
training:	Epoch: [85][91/408]	Loss 0.0004 (0.0222)	
training:	Epoch: [85][92/408]	Loss 0.0013 (0.0220)	
training:	Epoch: [85][93/408]	Loss 0.0003 (0.0218)	
training:	Epoch: [85][94/408]	Loss 0.0006 (0.0215)	
training:	Epoch: [85][95/408]	Loss 0.0003 (0.0213)	
training:	Epoch: [85][96/408]	Loss 0.1816 (0.0230)	
training:	Epoch: [85][97/408]	Loss 0.0002 (0.0228)	
training:	Epoch: [85][98/408]	Loss 0.0002 (0.0225)	
training:	Epoch: [85][99/408]	Loss 0.0003 (0.0223)	
training:	Epoch: [85][100/408]	Loss 0.0002 (0.0221)	
training:	Epoch: [85][101/408]	Loss 0.0938 (0.0228)	
training:	Epoch: [85][102/408]	Loss 0.0334 (0.0229)	
training:	Epoch: [85][103/408]	Loss 0.2516 (0.0251)	
training:	Epoch: [85][104/408]	Loss 0.0002 (0.0249)	
training:	Epoch: [85][105/408]	Loss 0.0004 (0.0246)	
training:	Epoch: [85][106/408]	Loss 0.0002 (0.0244)	
training:	Epoch: [85][107/408]	Loss 0.0008 (0.0242)	
training:	Epoch: [85][108/408]	Loss 0.0009 (0.0240)	
training:	Epoch: [85][109/408]	Loss 0.0002 (0.0238)	
training:	Epoch: [85][110/408]	Loss 0.0066 (0.0236)	
training:	Epoch: [85][111/408]	Loss 0.1842 (0.0250)	
training:	Epoch: [85][112/408]	Loss 0.2232 (0.0268)	
training:	Epoch: [85][113/408]	Loss 0.0016 (0.0266)	
training:	Epoch: [85][114/408]	Loss 0.0509 (0.0268)	
training:	Epoch: [85][115/408]	Loss 0.3659 (0.0298)	
training:	Epoch: [85][116/408]	Loss 0.0003 (0.0295)	
training:	Epoch: [85][117/408]	Loss 0.0003 (0.0293)	
training:	Epoch: [85][118/408]	Loss 0.0012 (0.0290)	
training:	Epoch: [85][119/408]	Loss 0.3300 (0.0315)	
training:	Epoch: [85][120/408]	Loss 0.0019 (0.0313)	
training:	Epoch: [85][121/408]	Loss 0.0251 (0.0312)	
training:	Epoch: [85][122/408]	Loss 0.3685 (0.0340)	
training:	Epoch: [85][123/408]	Loss 0.1106 (0.0346)	
training:	Epoch: [85][124/408]	Loss 0.0035 (0.0344)	
training:	Epoch: [85][125/408]	Loss 0.0060 (0.0342)	
training:	Epoch: [85][126/408]	Loss 0.0773 (0.0345)	
training:	Epoch: [85][127/408]	Loss 0.0023 (0.0342)	
training:	Epoch: [85][128/408]	Loss 0.0011 (0.0340)	
training:	Epoch: [85][129/408]	Loss 0.0003 (0.0337)	
training:	Epoch: [85][130/408]	Loss 0.0006 (0.0335)	
training:	Epoch: [85][131/408]	Loss 0.0174 (0.0333)	
training:	Epoch: [85][132/408]	Loss 0.0008 (0.0331)	
training:	Epoch: [85][133/408]	Loss 0.0072 (0.0329)	
training:	Epoch: [85][134/408]	Loss 0.0199 (0.0328)	
training:	Epoch: [85][135/408]	Loss 0.0331 (0.0328)	
training:	Epoch: [85][136/408]	Loss 0.2399 (0.0343)	
training:	Epoch: [85][137/408]	Loss 0.0006 (0.0341)	
training:	Epoch: [85][138/408]	Loss 0.0124 (0.0339)	
training:	Epoch: [85][139/408]	Loss 0.0004 (0.0337)	
training:	Epoch: [85][140/408]	Loss 0.0056 (0.0335)	
training:	Epoch: [85][141/408]	Loss 0.0004 (0.0333)	
training:	Epoch: [85][142/408]	Loss 0.0005 (0.0330)	
training:	Epoch: [85][143/408]	Loss 0.0003 (0.0328)	
training:	Epoch: [85][144/408]	Loss 0.0007 (0.0326)	
training:	Epoch: [85][145/408]	Loss 0.0007 (0.0324)	
training:	Epoch: [85][146/408]	Loss 0.0004 (0.0321)	
training:	Epoch: [85][147/408]	Loss 0.0003 (0.0319)	
training:	Epoch: [85][148/408]	Loss 0.0010 (0.0317)	
training:	Epoch: [85][149/408]	Loss 0.0089 (0.0316)	
training:	Epoch: [85][150/408]	Loss 0.1358 (0.0322)	
training:	Epoch: [85][151/408]	Loss 0.0056 (0.0321)	
training:	Epoch: [85][152/408]	Loss 0.0011 (0.0319)	
training:	Epoch: [85][153/408]	Loss 0.0015 (0.0317)	
training:	Epoch: [85][154/408]	Loss 0.0004 (0.0315)	
training:	Epoch: [85][155/408]	Loss 0.0007 (0.0313)	
training:	Epoch: [85][156/408]	Loss 0.0003 (0.0311)	
training:	Epoch: [85][157/408]	Loss 0.0003 (0.0309)	
training:	Epoch: [85][158/408]	Loss 0.0002 (0.0307)	
training:	Epoch: [85][159/408]	Loss 0.0004 (0.0305)	
training:	Epoch: [85][160/408]	Loss 0.0036 (0.0303)	
training:	Epoch: [85][161/408]	Loss 0.1133 (0.0308)	
training:	Epoch: [85][162/408]	Loss 0.0012 (0.0307)	
training:	Epoch: [85][163/408]	Loss 0.0123 (0.0305)	
training:	Epoch: [85][164/408]	Loss 0.0075 (0.0304)	
training:	Epoch: [85][165/408]	Loss 0.1875 (0.0314)	
training:	Epoch: [85][166/408]	Loss 0.0006 (0.0312)	
training:	Epoch: [85][167/408]	Loss 0.0003 (0.0310)	
training:	Epoch: [85][168/408]	Loss 0.0040 (0.0308)	
training:	Epoch: [85][169/408]	Loss 0.0003 (0.0306)	
training:	Epoch: [85][170/408]	Loss 0.0372 (0.0307)	
training:	Epoch: [85][171/408]	Loss 0.0005 (0.0305)	
training:	Epoch: [85][172/408]	Loss 0.0006 (0.0303)	
training:	Epoch: [85][173/408]	Loss 0.0837 (0.0306)	
training:	Epoch: [85][174/408]	Loss 0.0034 (0.0305)	
training:	Epoch: [85][175/408]	Loss 0.0007 (0.0303)	
training:	Epoch: [85][176/408]	Loss 0.0006 (0.0301)	
training:	Epoch: [85][177/408]	Loss 0.0005 (0.0300)	
training:	Epoch: [85][178/408]	Loss 0.0012 (0.0298)	
training:	Epoch: [85][179/408]	Loss 0.0009 (0.0297)	
training:	Epoch: [85][180/408]	Loss 0.0003 (0.0295)	
training:	Epoch: [85][181/408]	Loss 0.0413 (0.0296)	
training:	Epoch: [85][182/408]	Loss 0.0007 (0.0294)	
training:	Epoch: [85][183/408]	Loss 0.0088 (0.0293)	
training:	Epoch: [85][184/408]	Loss 0.0538 (0.0294)	
training:	Epoch: [85][185/408]	Loss 0.0005 (0.0293)	
training:	Epoch: [85][186/408]	Loss 0.0012 (0.0291)	
training:	Epoch: [85][187/408]	Loss 0.0003 (0.0290)	
training:	Epoch: [85][188/408]	Loss 0.0011 (0.0288)	
training:	Epoch: [85][189/408]	Loss 0.0006 (0.0287)	
training:	Epoch: [85][190/408]	Loss 0.0220 (0.0286)	
training:	Epoch: [85][191/408]	Loss 0.0007 (0.0285)	
training:	Epoch: [85][192/408]	Loss 0.0003 (0.0283)	
training:	Epoch: [85][193/408]	Loss 0.0023 (0.0282)	
training:	Epoch: [85][194/408]	Loss 0.0004 (0.0281)	
training:	Epoch: [85][195/408]	Loss 0.0002 (0.0279)	
training:	Epoch: [85][196/408]	Loss 0.0307 (0.0279)	
training:	Epoch: [85][197/408]	Loss 0.0003 (0.0278)	
training:	Epoch: [85][198/408]	Loss 0.0003 (0.0276)	
training:	Epoch: [85][199/408]	Loss 0.0002 (0.0275)	
training:	Epoch: [85][200/408]	Loss 0.0006 (0.0274)	
training:	Epoch: [85][201/408]	Loss 0.0446 (0.0275)	
training:	Epoch: [85][202/408]	Loss 0.0075 (0.0274)	
training:	Epoch: [85][203/408]	Loss 0.0003 (0.0272)	
training:	Epoch: [85][204/408]	Loss 0.0002 (0.0271)	
training:	Epoch: [85][205/408]	Loss 0.0058 (0.0270)	
training:	Epoch: [85][206/408]	Loss 0.1259 (0.0275)	
training:	Epoch: [85][207/408]	Loss 0.2933 (0.0288)	
training:	Epoch: [85][208/408]	Loss 0.0003 (0.0286)	
training:	Epoch: [85][209/408]	Loss 0.0024 (0.0285)	
training:	Epoch: [85][210/408]	Loss 0.0006 (0.0284)	
training:	Epoch: [85][211/408]	Loss 0.0051 (0.0282)	
training:	Epoch: [85][212/408]	Loss 0.0004 (0.0281)	
training:	Epoch: [85][213/408]	Loss 0.0005 (0.0280)	
training:	Epoch: [85][214/408]	Loss 0.1338 (0.0285)	
training:	Epoch: [85][215/408]	Loss 0.0036 (0.0284)	
training:	Epoch: [85][216/408]	Loss 0.0157 (0.0283)	
training:	Epoch: [85][217/408]	Loss 0.0017 (0.0282)	
training:	Epoch: [85][218/408]	Loss 0.0556 (0.0283)	
training:	Epoch: [85][219/408]	Loss 0.0022 (0.0282)	
training:	Epoch: [85][220/408]	Loss 0.0003 (0.0281)	
training:	Epoch: [85][221/408]	Loss 0.0079 (0.0280)	
training:	Epoch: [85][222/408]	Loss 0.0007 (0.0278)	
training:	Epoch: [85][223/408]	Loss 0.0040 (0.0277)	
training:	Epoch: [85][224/408]	Loss 0.0017 (0.0276)	
training:	Epoch: [85][225/408]	Loss 0.0005 (0.0275)	
training:	Epoch: [85][226/408]	Loss 0.0003 (0.0274)	
training:	Epoch: [85][227/408]	Loss 0.0016 (0.0273)	
training:	Epoch: [85][228/408]	Loss 0.0003 (0.0272)	
training:	Epoch: [85][229/408]	Loss 0.0005 (0.0270)	
training:	Epoch: [85][230/408]	Loss 0.0004 (0.0269)	
training:	Epoch: [85][231/408]	Loss 0.0007 (0.0268)	
training:	Epoch: [85][232/408]	Loss 0.0012 (0.0267)	
training:	Epoch: [85][233/408]	Loss 0.0051 (0.0266)	
training:	Epoch: [85][234/408]	Loss 0.0005 (0.0265)	
training:	Epoch: [85][235/408]	Loss 0.0006 (0.0264)	
training:	Epoch: [85][236/408]	Loss 0.0566 (0.0265)	
training:	Epoch: [85][237/408]	Loss 0.0004 (0.0264)	
training:	Epoch: [85][238/408]	Loss 0.0006 (0.0263)	
training:	Epoch: [85][239/408]	Loss 0.0002 (0.0262)	
training:	Epoch: [85][240/408]	Loss 0.0017 (0.0261)	
training:	Epoch: [85][241/408]	Loss 0.0020 (0.0260)	
training:	Epoch: [85][242/408]	Loss 0.0009 (0.0259)	
training:	Epoch: [85][243/408]	Loss 0.0005 (0.0258)	
training:	Epoch: [85][244/408]	Loss 0.0022 (0.0257)	
training:	Epoch: [85][245/408]	Loss 0.0011 (0.0256)	
training:	Epoch: [85][246/408]	Loss 0.0004 (0.0255)	
training:	Epoch: [85][247/408]	Loss 0.0014 (0.0254)	
training:	Epoch: [85][248/408]	Loss 0.0002 (0.0253)	
training:	Epoch: [85][249/408]	Loss 0.0006 (0.0252)	
training:	Epoch: [85][250/408]	Loss 0.0004 (0.0251)	
training:	Epoch: [85][251/408]	Loss 0.0011 (0.0250)	
training:	Epoch: [85][252/408]	Loss 0.0002 (0.0249)	
training:	Epoch: [85][253/408]	Loss 0.0012 (0.0248)	
training:	Epoch: [85][254/408]	Loss 0.0313 (0.0248)	
training:	Epoch: [85][255/408]	Loss 0.0002 (0.0247)	
training:	Epoch: [85][256/408]	Loss 0.0006 (0.0246)	
training:	Epoch: [85][257/408]	Loss 0.0022 (0.0245)	
training:	Epoch: [85][258/408]	Loss 0.0003 (0.0244)	
training:	Epoch: [85][259/408]	Loss 0.0003 (0.0243)	
training:	Epoch: [85][260/408]	Loss 0.0010 (0.0243)	
training:	Epoch: [85][261/408]	Loss 0.0006 (0.0242)	
training:	Epoch: [85][262/408]	Loss 0.0006 (0.0241)	
training:	Epoch: [85][263/408]	Loss 0.0026 (0.0240)	
training:	Epoch: [85][264/408]	Loss 0.0021 (0.0239)	
training:	Epoch: [85][265/408]	Loss 0.0003 (0.0238)	
training:	Epoch: [85][266/408]	Loss 0.0002 (0.0237)	
training:	Epoch: [85][267/408]	Loss 0.0003 (0.0236)	
training:	Epoch: [85][268/408]	Loss 0.0003 (0.0236)	
training:	Epoch: [85][269/408]	Loss 0.0016 (0.0235)	
training:	Epoch: [85][270/408]	Loss 0.0003 (0.0234)	
training:	Epoch: [85][271/408]	Loss 0.0005 (0.0233)	
training:	Epoch: [85][272/408]	Loss 0.0002 (0.0232)	
training:	Epoch: [85][273/408]	Loss 0.0005 (0.0231)	
training:	Epoch: [85][274/408]	Loss 0.0037 (0.0231)	
training:	Epoch: [85][275/408]	Loss 0.0003 (0.0230)	
training:	Epoch: [85][276/408]	Loss 0.0016 (0.0229)	
training:	Epoch: [85][277/408]	Loss 0.0002 (0.0228)	
training:	Epoch: [85][278/408]	Loss 0.0004 (0.0227)	
training:	Epoch: [85][279/408]	Loss 0.0004 (0.0227)	
training:	Epoch: [85][280/408]	Loss 0.0005 (0.0226)	
training:	Epoch: [85][281/408]	Loss 0.0075 (0.0225)	
training:	Epoch: [85][282/408]	Loss 0.0002 (0.0225)	
training:	Epoch: [85][283/408]	Loss 0.0010 (0.0224)	
training:	Epoch: [85][284/408]	Loss 0.0012 (0.0223)	
training:	Epoch: [85][285/408]	Loss 0.0014 (0.0222)	
training:	Epoch: [85][286/408]	Loss 0.0010 (0.0222)	
training:	Epoch: [85][287/408]	Loss 0.0002 (0.0221)	
training:	Epoch: [85][288/408]	Loss 0.0011 (0.0220)	
training:	Epoch: [85][289/408]	Loss 0.0003 (0.0219)	
training:	Epoch: [85][290/408]	Loss 0.0006 (0.0219)	
training:	Epoch: [85][291/408]	Loss 0.0052 (0.0218)	
training:	Epoch: [85][292/408]	Loss 0.0006 (0.0217)	
training:	Epoch: [85][293/408]	Loss 0.0003 (0.0217)	
training:	Epoch: [85][294/408]	Loss 0.0003 (0.0216)	
training:	Epoch: [85][295/408]	Loss 0.0013 (0.0215)	
training:	Epoch: [85][296/408]	Loss 0.0018 (0.0214)	
training:	Epoch: [85][297/408]	Loss 0.0002 (0.0214)	
training:	Epoch: [85][298/408]	Loss 0.0003 (0.0213)	
training:	Epoch: [85][299/408]	Loss 0.0003 (0.0212)	
training:	Epoch: [85][300/408]	Loss 0.0045 (0.0212)	
training:	Epoch: [85][301/408]	Loss 0.0006 (0.0211)	
training:	Epoch: [85][302/408]	Loss 0.0003 (0.0210)	
training:	Epoch: [85][303/408]	Loss 0.0015 (0.0210)	
training:	Epoch: [85][304/408]	Loss 0.0003 (0.0209)	
training:	Epoch: [85][305/408]	Loss 0.0066 (0.0209)	
training:	Epoch: [85][306/408]	Loss 0.0002 (0.0208)	
training:	Epoch: [85][307/408]	Loss 0.0052 (0.0207)	
training:	Epoch: [85][308/408]	Loss 0.0002 (0.0207)	
training:	Epoch: [85][309/408]	Loss 0.0003 (0.0206)	
training:	Epoch: [85][310/408]	Loss 0.0005 (0.0205)	
training:	Epoch: [85][311/408]	Loss 0.0003 (0.0205)	
training:	Epoch: [85][312/408]	Loss 0.0049 (0.0204)	
training:	Epoch: [85][313/408]	Loss 0.0006 (0.0204)	
training:	Epoch: [85][314/408]	Loss 0.0002 (0.0203)	
training:	Epoch: [85][315/408]	Loss 0.0002 (0.0202)	
training:	Epoch: [85][316/408]	Loss 0.0008 (0.0202)	
training:	Epoch: [85][317/408]	Loss 0.0003 (0.0201)	
training:	Epoch: [85][318/408]	Loss 0.0006 (0.0201)	
training:	Epoch: [85][319/408]	Loss 0.0554 (0.0202)	
training:	Epoch: [85][320/408]	Loss 0.0002 (0.0201)	
training:	Epoch: [85][321/408]	Loss 0.0003 (0.0200)	
training:	Epoch: [85][322/408]	Loss 0.0022 (0.0200)	
training:	Epoch: [85][323/408]	Loss 0.0006 (0.0199)	
training:	Epoch: [85][324/408]	Loss 0.0002 (0.0199)	
training:	Epoch: [85][325/408]	Loss 0.0042 (0.0198)	
training:	Epoch: [85][326/408]	Loss 0.0003 (0.0198)	
training:	Epoch: [85][327/408]	Loss 0.0006 (0.0197)	
training:	Epoch: [85][328/408]	Loss 0.0003 (0.0196)	
training:	Epoch: [85][329/408]	Loss 0.0189 (0.0196)	
training:	Epoch: [85][330/408]	Loss 0.0003 (0.0196)	
training:	Epoch: [85][331/408]	Loss 0.0027 (0.0195)	
training:	Epoch: [85][332/408]	Loss 0.0079 (0.0195)	
training:	Epoch: [85][333/408]	Loss 0.0004 (0.0194)	
training:	Epoch: [85][334/408]	Loss 0.2685 (0.0202)	
training:	Epoch: [85][335/408]	Loss 0.0005 (0.0201)	
training:	Epoch: [85][336/408]	Loss 0.0006 (0.0201)	
training:	Epoch: [85][337/408]	Loss 0.0002 (0.0200)	
training:	Epoch: [85][338/408]	Loss 0.0040 (0.0200)	
training:	Epoch: [85][339/408]	Loss 0.0003 (0.0199)	
training:	Epoch: [85][340/408]	Loss 0.0220 (0.0199)	
training:	Epoch: [85][341/408]	Loss 0.0006 (0.0199)	
training:	Epoch: [85][342/408]	Loss 0.0002 (0.0198)	
training:	Epoch: [85][343/408]	Loss 0.0004 (0.0197)	
training:	Epoch: [85][344/408]	Loss 0.0008 (0.0197)	
training:	Epoch: [85][345/408]	Loss 0.0002 (0.0196)	
training:	Epoch: [85][346/408]	Loss 0.0002 (0.0196)	
training:	Epoch: [85][347/408]	Loss 0.0009 (0.0195)	
training:	Epoch: [85][348/408]	Loss 0.0058 (0.0195)	
training:	Epoch: [85][349/408]	Loss 0.0003 (0.0194)	
training:	Epoch: [85][350/408]	Loss 0.0014 (0.0194)	
training:	Epoch: [85][351/408]	Loss 0.0419 (0.0194)	
training:	Epoch: [85][352/408]	Loss 0.0003 (0.0194)	
training:	Epoch: [85][353/408]	Loss 0.0396 (0.0194)	
training:	Epoch: [85][354/408]	Loss 0.0010 (0.0194)	
training:	Epoch: [85][355/408]	Loss 0.0005 (0.0193)	
training:	Epoch: [85][356/408]	Loss 0.0004 (0.0193)	
training:	Epoch: [85][357/408]	Loss 0.0003 (0.0192)	
training:	Epoch: [85][358/408]	Loss 0.0016 (0.0192)	
training:	Epoch: [85][359/408]	Loss 0.0004 (0.0191)	
training:	Epoch: [85][360/408]	Loss 0.0037 (0.0191)	
training:	Epoch: [85][361/408]	Loss 0.0006 (0.0190)	
training:	Epoch: [85][362/408]	Loss 0.0004 (0.0190)	
training:	Epoch: [85][363/408]	Loss 0.0004 (0.0189)	
training:	Epoch: [85][364/408]	Loss 0.0003 (0.0189)	
training:	Epoch: [85][365/408]	Loss 0.0006 (0.0188)	
training:	Epoch: [85][366/408]	Loss 0.0002 (0.0188)	
training:	Epoch: [85][367/408]	Loss 0.0105 (0.0188)	
training:	Epoch: [85][368/408]	Loss 0.0003 (0.0187)	
training:	Epoch: [85][369/408]	Loss 0.0012 (0.0187)	
training:	Epoch: [85][370/408]	Loss 0.0003 (0.0186)	
training:	Epoch: [85][371/408]	Loss 0.0046 (0.0186)	
training:	Epoch: [85][372/408]	Loss 0.0033 (0.0185)	
training:	Epoch: [85][373/408]	Loss 0.0010 (0.0185)	
training:	Epoch: [85][374/408]	Loss 0.0066 (0.0184)	
training:	Epoch: [85][375/408]	Loss 0.0003 (0.0184)	
training:	Epoch: [85][376/408]	Loss 0.0024 (0.0184)	
training:	Epoch: [85][377/408]	Loss 0.0042 (0.0183)	
training:	Epoch: [85][378/408]	Loss 0.0060 (0.0183)	
training:	Epoch: [85][379/408]	Loss 0.0438 (0.0184)	
training:	Epoch: [85][380/408]	Loss 0.0003 (0.0183)	
training:	Epoch: [85][381/408]	Loss 0.0580 (0.0184)	
training:	Epoch: [85][382/408]	Loss 0.0003 (0.0184)	
training:	Epoch: [85][383/408]	Loss 0.0023 (0.0183)	
training:	Epoch: [85][384/408]	Loss 0.0660 (0.0184)	
training:	Epoch: [85][385/408]	Loss 0.0003 (0.0184)	
training:	Epoch: [85][386/408]	Loss 0.0005 (0.0184)	
training:	Epoch: [85][387/408]	Loss 0.0508 (0.0184)	
training:	Epoch: [85][388/408]	Loss 0.0002 (0.0184)	
training:	Epoch: [85][389/408]	Loss 0.0002 (0.0183)	
training:	Epoch: [85][390/408]	Loss 0.0007 (0.0183)	
training:	Epoch: [85][391/408]	Loss 0.0003 (0.0182)	
training:	Epoch: [85][392/408]	Loss 0.0002 (0.0182)	
training:	Epoch: [85][393/408]	Loss 0.0008 (0.0182)	
training:	Epoch: [85][394/408]	Loss 0.0003 (0.0181)	
training:	Epoch: [85][395/408]	Loss 0.0002 (0.0181)	
training:	Epoch: [85][396/408]	Loss 0.0047 (0.0180)	
training:	Epoch: [85][397/408]	Loss 0.0008 (0.0180)	
training:	Epoch: [85][398/408]	Loss 0.0003 (0.0179)	
training:	Epoch: [85][399/408]	Loss 0.0003 (0.0179)	
training:	Epoch: [85][400/408]	Loss 0.0003 (0.0179)	
training:	Epoch: [85][401/408]	Loss 0.0005 (0.0178)	
training:	Epoch: [85][402/408]	Loss 0.0007 (0.0178)	
training:	Epoch: [85][403/408]	Loss 0.0012 (0.0177)	
training:	Epoch: [85][404/408]	Loss 0.0008 (0.0177)	
training:	Epoch: [85][405/408]	Loss 0.0005 (0.0176)	
training:	Epoch: [85][406/408]	Loss 0.0004 (0.0176)	
training:	Epoch: [85][407/408]	Loss 0.0883 (0.0178)	
training:	Epoch: [85][408/408]	Loss 0.0005 (0.0177)	
Training:	 Loss: 0.0177

Training:	 ACC: 0.9974 0.9976 1.0000 0.9949
Validation:	 ACC: 0.7653 0.7689 0.8434 0.6872
Validation:	 Best_BACC: 0.8226 0.8250 0.8772 0.7679
Validation:	 Loss: 1.4045
Pretraining:	Epoch 86/200
----------
training:	Epoch: [86][1/408]	Loss 0.1364 (0.1364)	
training:	Epoch: [86][2/408]	Loss 0.0005 (0.0685)	
training:	Epoch: [86][3/408]	Loss 0.0004 (0.0458)	
training:	Epoch: [86][4/408]	Loss 0.0009 (0.0346)	
training:	Epoch: [86][5/408]	Loss 0.0005 (0.0278)	
training:	Epoch: [86][6/408]	Loss 0.0003 (0.0232)	
training:	Epoch: [86][7/408]	Loss 0.0004 (0.0199)	
training:	Epoch: [86][8/408]	Loss 0.0004 (0.0175)	
training:	Epoch: [86][9/408]	Loss 0.0003 (0.0156)	
training:	Epoch: [86][10/408]	Loss 0.0018 (0.0142)	
training:	Epoch: [86][11/408]	Loss 0.0002 (0.0129)	
training:	Epoch: [86][12/408]	Loss 0.0008 (0.0119)	
training:	Epoch: [86][13/408]	Loss 0.0002 (0.0110)	
training:	Epoch: [86][14/408]	Loss 0.0294 (0.0123)	
training:	Epoch: [86][15/408]	Loss 0.0027 (0.0117)	
training:	Epoch: [86][16/408]	Loss 0.0004 (0.0110)	
training:	Epoch: [86][17/408]	Loss 0.0003 (0.0104)	
training:	Epoch: [86][18/408]	Loss 0.0037 (0.0100)	
training:	Epoch: [86][19/408]	Loss 0.0045 (0.0097)	
training:	Epoch: [86][20/408]	Loss 0.0010 (0.0093)	
training:	Epoch: [86][21/408]	Loss 0.0006 (0.0088)	
training:	Epoch: [86][22/408]	Loss 0.0002 (0.0085)	
training:	Epoch: [86][23/408]	Loss 0.0003 (0.0081)	
training:	Epoch: [86][24/408]	Loss 0.0114 (0.0082)	
training:	Epoch: [86][25/408]	Loss 0.0003 (0.0079)	
training:	Epoch: [86][26/408]	Loss 0.0004 (0.0076)	
training:	Epoch: [86][27/408]	Loss 0.0003 (0.0074)	
training:	Epoch: [86][28/408]	Loss 0.0018 (0.0072)	
training:	Epoch: [86][29/408]	Loss 0.0003 (0.0069)	
training:	Epoch: [86][30/408]	Loss 0.0003 (0.0067)	
training:	Epoch: [86][31/408]	Loss 0.0003 (0.0065)	
training:	Epoch: [86][32/408]	Loss 0.0004 (0.0063)	
training:	Epoch: [86][33/408]	Loss 0.0008 (0.0061)	
training:	Epoch: [86][34/408]	Loss 0.0039 (0.0061)	
training:	Epoch: [86][35/408]	Loss 0.0003 (0.0059)	
training:	Epoch: [86][36/408]	Loss 0.0022 (0.0058)	
training:	Epoch: [86][37/408]	Loss 0.0003 (0.0057)	
training:	Epoch: [86][38/408]	Loss 0.0015 (0.0055)	
training:	Epoch: [86][39/408]	Loss 0.0005 (0.0054)	
training:	Epoch: [86][40/408]	Loss 0.0200 (0.0058)	
training:	Epoch: [86][41/408]	Loss 0.0003 (0.0056)	
training:	Epoch: [86][42/408]	Loss 0.0005 (0.0055)	
training:	Epoch: [86][43/408]	Loss 0.0003 (0.0054)	
training:	Epoch: [86][44/408]	Loss 0.0009 (0.0053)	
training:	Epoch: [86][45/408]	Loss 0.0004 (0.0052)	
training:	Epoch: [86][46/408]	Loss 0.0004 (0.0051)	
training:	Epoch: [86][47/408]	Loss 0.0003 (0.0050)	
training:	Epoch: [86][48/408]	Loss 0.0002 (0.0049)	
training:	Epoch: [86][49/408]	Loss 0.0003 (0.0048)	
training:	Epoch: [86][50/408]	Loss 0.0018 (0.0047)	
training:	Epoch: [86][51/408]	Loss 0.2196 (0.0089)	
training:	Epoch: [86][52/408]	Loss 0.0025 (0.0088)	
training:	Epoch: [86][53/408]	Loss 0.0002 (0.0087)	
training:	Epoch: [86][54/408]	Loss 0.0022 (0.0085)	
training:	Epoch: [86][55/408]	Loss 0.0002 (0.0084)	
training:	Epoch: [86][56/408]	Loss 0.0057 (0.0083)	
training:	Epoch: [86][57/408]	Loss 0.0025 (0.0082)	
training:	Epoch: [86][58/408]	Loss 0.0005 (0.0081)	
training:	Epoch: [86][59/408]	Loss 0.0004 (0.0080)	
training:	Epoch: [86][60/408]	Loss 0.2429 (0.0119)	
training:	Epoch: [86][61/408]	Loss 0.0030 (0.0117)	
training:	Epoch: [86][62/408]	Loss 0.0010 (0.0116)	
training:	Epoch: [86][63/408]	Loss 0.0005 (0.0114)	
training:	Epoch: [86][64/408]	Loss 0.0002 (0.0112)	
training:	Epoch: [86][65/408]	Loss 0.0007 (0.0111)	
training:	Epoch: [86][66/408]	Loss 0.0002 (0.0109)	
training:	Epoch: [86][67/408]	Loss 0.0002 (0.0107)	
training:	Epoch: [86][68/408]	Loss 0.0329 (0.0111)	
training:	Epoch: [86][69/408]	Loss 0.0002 (0.0109)	
training:	Epoch: [86][70/408]	Loss 0.0017 (0.0108)	
training:	Epoch: [86][71/408]	Loss 0.0004 (0.0106)	
training:	Epoch: [86][72/408]	Loss 0.0002 (0.0105)	
training:	Epoch: [86][73/408]	Loss 0.0009 (0.0104)	
training:	Epoch: [86][74/408]	Loss 0.0003 (0.0102)	
training:	Epoch: [86][75/408]	Loss 0.0011 (0.0101)	
training:	Epoch: [86][76/408]	Loss 0.0004 (0.0100)	
training:	Epoch: [86][77/408]	Loss 0.0005 (0.0098)	
training:	Epoch: [86][78/408]	Loss 0.0007 (0.0097)	
training:	Epoch: [86][79/408]	Loss 0.0010 (0.0096)	
training:	Epoch: [86][80/408]	Loss 0.0005 (0.0095)	
training:	Epoch: [86][81/408]	Loss 0.0003 (0.0094)	
training:	Epoch: [86][82/408]	Loss 0.0012 (0.0093)	
training:	Epoch: [86][83/408]	Loss 0.0301 (0.0095)	
training:	Epoch: [86][84/408]	Loss 0.0003 (0.0094)	
training:	Epoch: [86][85/408]	Loss 0.0011 (0.0093)	
training:	Epoch: [86][86/408]	Loss 0.0003 (0.0092)	
training:	Epoch: [86][87/408]	Loss 0.0002 (0.0091)	
training:	Epoch: [86][88/408]	Loss 0.0003 (0.0090)	
training:	Epoch: [86][89/408]	Loss 0.0033 (0.0090)	
training:	Epoch: [86][90/408]	Loss 0.0003 (0.0089)	
training:	Epoch: [86][91/408]	Loss 0.0005 (0.0088)	
training:	Epoch: [86][92/408]	Loss 0.0006 (0.0087)	
training:	Epoch: [86][93/408]	Loss 0.0004 (0.0086)	
training:	Epoch: [86][94/408]	Loss 0.0002 (0.0085)	
training:	Epoch: [86][95/408]	Loss 0.0042 (0.0085)	
training:	Epoch: [86][96/408]	Loss 0.0002 (0.0084)	
training:	Epoch: [86][97/408]	Loss 0.0003 (0.0083)	
training:	Epoch: [86][98/408]	Loss 0.0002 (0.0082)	
training:	Epoch: [86][99/408]	Loss 0.0019 (0.0081)	
training:	Epoch: [86][100/408]	Loss 0.0008 (0.0081)	
training:	Epoch: [86][101/408]	Loss 0.0016 (0.0080)	
training:	Epoch: [86][102/408]	Loss 0.0040 (0.0080)	
training:	Epoch: [86][103/408]	Loss 0.0002 (0.0079)	
training:	Epoch: [86][104/408]	Loss 0.0003 (0.0078)	
training:	Epoch: [86][105/408]	Loss 0.0003 (0.0077)	
training:	Epoch: [86][106/408]	Loss 0.0156 (0.0078)	
training:	Epoch: [86][107/408]	Loss 0.0003 (0.0077)	
training:	Epoch: [86][108/408]	Loss 0.0005 (0.0077)	
training:	Epoch: [86][109/408]	Loss 0.0002 (0.0076)	
training:	Epoch: [86][110/408]	Loss 0.0002 (0.0075)	
training:	Epoch: [86][111/408]	Loss 0.0002 (0.0075)	
training:	Epoch: [86][112/408]	Loss 0.0443 (0.0078)	
training:	Epoch: [86][113/408]	Loss 0.0004 (0.0077)	
training:	Epoch: [86][114/408]	Loss 0.0003 (0.0077)	
training:	Epoch: [86][115/408]	Loss 0.0007 (0.0076)	
training:	Epoch: [86][116/408]	Loss 0.0002 (0.0076)	
training:	Epoch: [86][117/408]	Loss 0.0009 (0.0075)	
training:	Epoch: [86][118/408]	Loss 0.0005 (0.0074)	
training:	Epoch: [86][119/408]	Loss 0.0003 (0.0074)	
training:	Epoch: [86][120/408]	Loss 0.0004 (0.0073)	
training:	Epoch: [86][121/408]	Loss 0.0002 (0.0073)	
training:	Epoch: [86][122/408]	Loss 0.0006 (0.0072)	
training:	Epoch: [86][123/408]	Loss 0.0008 (0.0072)	
training:	Epoch: [86][124/408]	Loss 0.0003 (0.0071)	
training:	Epoch: [86][125/408]	Loss 0.0003 (0.0070)	
training:	Epoch: [86][126/408]	Loss 0.0002 (0.0070)	
training:	Epoch: [86][127/408]	Loss 0.0014 (0.0069)	
training:	Epoch: [86][128/408]	Loss 0.0007 (0.0069)	
training:	Epoch: [86][129/408]	Loss 0.0003 (0.0068)	
training:	Epoch: [86][130/408]	Loss 0.0003 (0.0068)	
training:	Epoch: [86][131/408]	Loss 0.0003 (0.0067)	
training:	Epoch: [86][132/408]	Loss 0.0003 (0.0067)	
training:	Epoch: [86][133/408]	Loss 0.0003 (0.0066)	
training:	Epoch: [86][134/408]	Loss 0.0114 (0.0067)	
training:	Epoch: [86][135/408]	Loss 0.0002 (0.0066)	
training:	Epoch: [86][136/408]	Loss 0.0002 (0.0066)	
training:	Epoch: [86][137/408]	Loss 0.0002 (0.0065)	
training:	Epoch: [86][138/408]	Loss 0.0003 (0.0065)	
training:	Epoch: [86][139/408]	Loss 0.0009 (0.0065)	
training:	Epoch: [86][140/408]	Loss 0.0003 (0.0064)	
training:	Epoch: [86][141/408]	Loss 0.0003 (0.0064)	
training:	Epoch: [86][142/408]	Loss 0.0002 (0.0063)	
training:	Epoch: [86][143/408]	Loss 0.0014 (0.0063)	
training:	Epoch: [86][144/408]	Loss 0.0009 (0.0063)	
training:	Epoch: [86][145/408]	Loss 0.0006 (0.0062)	
training:	Epoch: [86][146/408]	Loss 0.0011 (0.0062)	
training:	Epoch: [86][147/408]	Loss 0.0004 (0.0061)	
training:	Epoch: [86][148/408]	Loss 0.0002 (0.0061)	
training:	Epoch: [86][149/408]	Loss 0.0005 (0.0061)	
training:	Epoch: [86][150/408]	Loss 0.0016 (0.0060)	
training:	Epoch: [86][151/408]	Loss 0.0003 (0.0060)	
training:	Epoch: [86][152/408]	Loss 0.0003 (0.0060)	
training:	Epoch: [86][153/408]	Loss 0.0003 (0.0059)	
training:	Epoch: [86][154/408]	Loss 0.0002 (0.0059)	
training:	Epoch: [86][155/408]	Loss 0.0003 (0.0058)	
training:	Epoch: [86][156/408]	Loss 0.0007 (0.0058)	
training:	Epoch: [86][157/408]	Loss 0.0077 (0.0058)	
training:	Epoch: [86][158/408]	Loss 0.0003 (0.0058)	
training:	Epoch: [86][159/408]	Loss 0.0057 (0.0058)	
training:	Epoch: [86][160/408]	Loss 0.0004 (0.0058)	
training:	Epoch: [86][161/408]	Loss 0.0005 (0.0057)	
training:	Epoch: [86][162/408]	Loss 0.0003 (0.0057)	
training:	Epoch: [86][163/408]	Loss 0.0008 (0.0057)	
training:	Epoch: [86][164/408]	Loss 0.0002 (0.0056)	
training:	Epoch: [86][165/408]	Loss 0.0004 (0.0056)	
training:	Epoch: [86][166/408]	Loss 0.0002 (0.0056)	
training:	Epoch: [86][167/408]	Loss 0.0006 (0.0055)	
training:	Epoch: [86][168/408]	Loss 0.0003 (0.0055)	
training:	Epoch: [86][169/408]	Loss 0.0006 (0.0055)	
training:	Epoch: [86][170/408]	Loss 0.0007 (0.0054)	
training:	Epoch: [86][171/408]	Loss 0.0004 (0.0054)	
training:	Epoch: [86][172/408]	Loss 0.0003 (0.0054)	
training:	Epoch: [86][173/408]	Loss 0.0005 (0.0054)	
training:	Epoch: [86][174/408]	Loss 0.0006 (0.0053)	
training:	Epoch: [86][175/408]	Loss 0.0002 (0.0053)	
training:	Epoch: [86][176/408]	Loss 0.0002 (0.0053)	
training:	Epoch: [86][177/408]	Loss 0.0004 (0.0052)	
training:	Epoch: [86][178/408]	Loss 0.0006 (0.0052)	
training:	Epoch: [86][179/408]	Loss 0.0003 (0.0052)	
training:	Epoch: [86][180/408]	Loss 0.0002 (0.0052)	
training:	Epoch: [86][181/408]	Loss 0.0002 (0.0051)	
training:	Epoch: [86][182/408]	Loss 0.0002 (0.0051)	
training:	Epoch: [86][183/408]	Loss 0.0003 (0.0051)	
training:	Epoch: [86][184/408]	Loss 0.0003 (0.0051)	
training:	Epoch: [86][185/408]	Loss 0.0002 (0.0050)	
training:	Epoch: [86][186/408]	Loss 0.0013 (0.0050)	
training:	Epoch: [86][187/408]	Loss 0.0002 (0.0050)	
training:	Epoch: [86][188/408]	Loss 0.0003 (0.0050)	
training:	Epoch: [86][189/408]	Loss 0.0002 (0.0049)	
training:	Epoch: [86][190/408]	Loss 0.0003 (0.0049)	
training:	Epoch: [86][191/408]	Loss 0.0006 (0.0049)	
training:	Epoch: [86][192/408]	Loss 0.0018 (0.0049)	
training:	Epoch: [86][193/408]	Loss 0.0002 (0.0048)	
training:	Epoch: [86][194/408]	Loss 0.0004 (0.0048)	
training:	Epoch: [86][195/408]	Loss 0.0003 (0.0048)	
training:	Epoch: [86][196/408]	Loss 0.0003 (0.0048)	
training:	Epoch: [86][197/408]	Loss 0.0003 (0.0048)	
training:	Epoch: [86][198/408]	Loss 0.0003 (0.0047)	
training:	Epoch: [86][199/408]	Loss 0.0009 (0.0047)	
training:	Epoch: [86][200/408]	Loss 0.0003 (0.0047)	
training:	Epoch: [86][201/408]	Loss 0.0005 (0.0047)	
training:	Epoch: [86][202/408]	Loss 0.0002 (0.0047)	
training:	Epoch: [86][203/408]	Loss 0.0003 (0.0046)	
training:	Epoch: [86][204/408]	Loss 0.0005 (0.0046)	
training:	Epoch: [86][205/408]	Loss 0.0003 (0.0046)	
training:	Epoch: [86][206/408]	Loss 0.0004 (0.0046)	
training:	Epoch: [86][207/408]	Loss 0.0002 (0.0045)	
training:	Epoch: [86][208/408]	Loss 0.0010 (0.0045)	
training:	Epoch: [86][209/408]	Loss 0.0002 (0.0045)	
training:	Epoch: [86][210/408]	Loss 0.0003 (0.0045)	
training:	Epoch: [86][211/408]	Loss 0.0002 (0.0045)	
training:	Epoch: [86][212/408]	Loss 0.0002 (0.0044)	
training:	Epoch: [86][213/408]	Loss 0.0003 (0.0044)	
training:	Epoch: [86][214/408]	Loss 0.0002 (0.0044)	
training:	Epoch: [86][215/408]	Loss 0.0004 (0.0044)	
training:	Epoch: [86][216/408]	Loss 0.0004 (0.0044)	
training:	Epoch: [86][217/408]	Loss 0.0002 (0.0044)	
training:	Epoch: [86][218/408]	Loss 0.0031 (0.0043)	
training:	Epoch: [86][219/408]	Loss 0.0003 (0.0043)	
training:	Epoch: [86][220/408]	Loss 0.0003 (0.0043)	
training:	Epoch: [86][221/408]	Loss 0.0003 (0.0043)	
training:	Epoch: [86][222/408]	Loss 0.0003 (0.0043)	
training:	Epoch: [86][223/408]	Loss 0.0003 (0.0043)	
training:	Epoch: [86][224/408]	Loss 0.0006 (0.0042)	
training:	Epoch: [86][225/408]	Loss 0.0002 (0.0042)	
training:	Epoch: [86][226/408]	Loss 0.0002 (0.0042)	
training:	Epoch: [86][227/408]	Loss 0.0003 (0.0042)	
training:	Epoch: [86][228/408]	Loss 0.0002 (0.0042)	
training:	Epoch: [86][229/408]	Loss 0.0003 (0.0042)	
training:	Epoch: [86][230/408]	Loss 0.0002 (0.0041)	
training:	Epoch: [86][231/408]	Loss 0.0011 (0.0041)	
training:	Epoch: [86][232/408]	Loss 0.0002 (0.0041)	
training:	Epoch: [86][233/408]	Loss 0.0002 (0.0041)	
training:	Epoch: [86][234/408]	Loss 0.0003 (0.0041)	
training:	Epoch: [86][235/408]	Loss 0.0003 (0.0041)	
training:	Epoch: [86][236/408]	Loss 0.0002 (0.0040)	
training:	Epoch: [86][237/408]	Loss 0.0002 (0.0040)	
training:	Epoch: [86][238/408]	Loss 0.0008 (0.0040)	
training:	Epoch: [86][239/408]	Loss 0.0029 (0.0040)	
training:	Epoch: [86][240/408]	Loss 0.0003 (0.0040)	
training:	Epoch: [86][241/408]	Loss 0.0006 (0.0040)	
training:	Epoch: [86][242/408]	Loss 0.0002 (0.0040)	
training:	Epoch: [86][243/408]	Loss 0.0004 (0.0039)	
training:	Epoch: [86][244/408]	Loss 0.0003 (0.0039)	
training:	Epoch: [86][245/408]	Loss 0.0023 (0.0039)	
training:	Epoch: [86][246/408]	Loss 0.0002 (0.0039)	
training:	Epoch: [86][247/408]	Loss 0.0005 (0.0039)	
training:	Epoch: [86][248/408]	Loss 0.0005 (0.0039)	
training:	Epoch: [86][249/408]	Loss 0.0014 (0.0039)	
training:	Epoch: [86][250/408]	Loss 0.0003 (0.0039)	
training:	Epoch: [86][251/408]	Loss 0.0003 (0.0038)	
training:	Epoch: [86][252/408]	Loss 0.0002 (0.0038)	
training:	Epoch: [86][253/408]	Loss 0.0015 (0.0038)	
training:	Epoch: [86][254/408]	Loss 0.0003 (0.0038)	
training:	Epoch: [86][255/408]	Loss 0.0002 (0.0038)	
training:	Epoch: [86][256/408]	Loss 0.0004 (0.0038)	
training:	Epoch: [86][257/408]	Loss 0.0002 (0.0038)	
training:	Epoch: [86][258/408]	Loss 0.0002 (0.0038)	
training:	Epoch: [86][259/408]	Loss 0.0003 (0.0037)	
training:	Epoch: [86][260/408]	Loss 0.0002 (0.0037)	
training:	Epoch: [86][261/408]	Loss 0.0002 (0.0037)	
training:	Epoch: [86][262/408]	Loss 0.0023 (0.0037)	
training:	Epoch: [86][263/408]	Loss 0.0002 (0.0037)	
training:	Epoch: [86][264/408]	Loss 0.0002 (0.0037)	
training:	Epoch: [86][265/408]	Loss 0.0002 (0.0037)	
training:	Epoch: [86][266/408]	Loss 0.0002 (0.0037)	
training:	Epoch: [86][267/408]	Loss 0.0007 (0.0036)	
training:	Epoch: [86][268/408]	Loss 0.0002 (0.0036)	
training:	Epoch: [86][269/408]	Loss 0.0002 (0.0036)	
training:	Epoch: [86][270/408]	Loss 0.0006 (0.0036)	
training:	Epoch: [86][271/408]	Loss 0.0008 (0.0036)	
training:	Epoch: [86][272/408]	Loss 0.0039 (0.0036)	
training:	Epoch: [86][273/408]	Loss 0.0003 (0.0036)	
training:	Epoch: [86][274/408]	Loss 0.0003 (0.0036)	
training:	Epoch: [86][275/408]	Loss 0.0002 (0.0036)	
training:	Epoch: [86][276/408]	Loss 0.0006 (0.0035)	
training:	Epoch: [86][277/408]	Loss 0.0031 (0.0035)	
training:	Epoch: [86][278/408]	Loss 0.0003 (0.0035)	
training:	Epoch: [86][279/408]	Loss 0.0004 (0.0035)	
training:	Epoch: [86][280/408]	Loss 0.0002 (0.0035)	
training:	Epoch: [86][281/408]	Loss 0.0004 (0.0035)	
training:	Epoch: [86][282/408]	Loss 0.0002 (0.0035)	
training:	Epoch: [86][283/408]	Loss 0.0004 (0.0035)	
training:	Epoch: [86][284/408]	Loss 0.0013 (0.0035)	
training:	Epoch: [86][285/408]	Loss 0.0002 (0.0035)	
training:	Epoch: [86][286/408]	Loss 0.0002 (0.0034)	
training:	Epoch: [86][287/408]	Loss 0.0002 (0.0034)	
training:	Epoch: [86][288/408]	Loss 0.0010 (0.0034)	
training:	Epoch: [86][289/408]	Loss 0.0002 (0.0034)	
training:	Epoch: [86][290/408]	Loss 0.0002 (0.0034)	
training:	Epoch: [86][291/408]	Loss 0.0005 (0.0034)	
training:	Epoch: [86][292/408]	Loss 0.0009 (0.0034)	
training:	Epoch: [86][293/408]	Loss 0.0013 (0.0034)	
training:	Epoch: [86][294/408]	Loss 0.0002 (0.0034)	
training:	Epoch: [86][295/408]	Loss 0.0003 (0.0034)	
training:	Epoch: [86][296/408]	Loss 0.0009 (0.0034)	
training:	Epoch: [86][297/408]	Loss 0.0002 (0.0033)	
training:	Epoch: [86][298/408]	Loss 0.0003 (0.0033)	
training:	Epoch: [86][299/408]	Loss 0.0234 (0.0034)	
training:	Epoch: [86][300/408]	Loss 0.0005 (0.0034)	
training:	Epoch: [86][301/408]	Loss 0.0038 (0.0034)	
training:	Epoch: [86][302/408]	Loss 0.0002 (0.0034)	
training:	Epoch: [86][303/408]	Loss 0.0003 (0.0034)	
training:	Epoch: [86][304/408]	Loss 0.0004 (0.0034)	
training:	Epoch: [86][305/408]	Loss 0.0003 (0.0033)	
training:	Epoch: [86][306/408]	Loss 0.0004 (0.0033)	
training:	Epoch: [86][307/408]	Loss 0.0004 (0.0033)	
training:	Epoch: [86][308/408]	Loss 0.0021 (0.0033)	
training:	Epoch: [86][309/408]	Loss 0.0005 (0.0033)	
training:	Epoch: [86][310/408]	Loss 0.0002 (0.0033)	
training:	Epoch: [86][311/408]	Loss 0.0004 (0.0033)	
training:	Epoch: [86][312/408]	Loss 0.0003 (0.0033)	
training:	Epoch: [86][313/408]	Loss 0.0003 (0.0033)	
training:	Epoch: [86][314/408]	Loss 0.0003 (0.0033)	
training:	Epoch: [86][315/408]	Loss 0.0004 (0.0033)	
training:	Epoch: [86][316/408]	Loss 0.0328 (0.0034)	
training:	Epoch: [86][317/408]	Loss 0.0002 (0.0033)	
training:	Epoch: [86][318/408]	Loss 0.0003 (0.0033)	
training:	Epoch: [86][319/408]	Loss 0.0003 (0.0033)	
training:	Epoch: [86][320/408]	Loss 0.0014 (0.0033)	
training:	Epoch: [86][321/408]	Loss 0.0006 (0.0033)	
training:	Epoch: [86][322/408]	Loss 0.0003 (0.0033)	
training:	Epoch: [86][323/408]	Loss 0.0003 (0.0033)	
training:	Epoch: [86][324/408]	Loss 0.0003 (0.0033)	
training:	Epoch: [86][325/408]	Loss 0.0002 (0.0033)	
training:	Epoch: [86][326/408]	Loss 0.0002 (0.0033)	
training:	Epoch: [86][327/408]	Loss 0.0018 (0.0033)	
training:	Epoch: [86][328/408]	Loss 0.0011 (0.0033)	
training:	Epoch: [86][329/408]	Loss 0.0005 (0.0032)	
training:	Epoch: [86][330/408]	Loss 0.0002 (0.0032)	
training:	Epoch: [86][331/408]	Loss 0.0006 (0.0032)	
training:	Epoch: [86][332/408]	Loss 0.0003 (0.0032)	
training:	Epoch: [86][333/408]	Loss 0.0002 (0.0032)	
training:	Epoch: [86][334/408]	Loss 0.0002 (0.0032)	
training:	Epoch: [86][335/408]	Loss 0.0002 (0.0032)	
training:	Epoch: [86][336/408]	Loss 0.0003 (0.0032)	
training:	Epoch: [86][337/408]	Loss 0.0005 (0.0032)	
training:	Epoch: [86][338/408]	Loss 0.0015 (0.0032)	
training:	Epoch: [86][339/408]	Loss 0.0003 (0.0032)	
training:	Epoch: [86][340/408]	Loss 0.0002 (0.0032)	
training:	Epoch: [86][341/408]	Loss 0.0002 (0.0031)	
training:	Epoch: [86][342/408]	Loss 0.0002 (0.0031)	
training:	Epoch: [86][343/408]	Loss 0.0004 (0.0031)	
training:	Epoch: [86][344/408]	Loss 0.0003 (0.0031)	
training:	Epoch: [86][345/408]	Loss 0.0016 (0.0031)	
training:	Epoch: [86][346/408]	Loss 0.0002 (0.0031)	
training:	Epoch: [86][347/408]	Loss 0.0002 (0.0031)	
training:	Epoch: [86][348/408]	Loss 0.0005 (0.0031)	
training:	Epoch: [86][349/408]	Loss 0.0002 (0.0031)	
training:	Epoch: [86][350/408]	Loss 0.0002 (0.0031)	
training:	Epoch: [86][351/408]	Loss 0.0002 (0.0031)	
training:	Epoch: [86][352/408]	Loss 0.0002 (0.0031)	
training:	Epoch: [86][353/408]	Loss 0.0003 (0.0030)	
training:	Epoch: [86][354/408]	Loss 0.0004 (0.0030)	
training:	Epoch: [86][355/408]	Loss 0.0002 (0.0030)	
training:	Epoch: [86][356/408]	Loss 0.0005 (0.0030)	
training:	Epoch: [86][357/408]	Loss 0.0005 (0.0030)	
training:	Epoch: [86][358/408]	Loss 0.0004 (0.0030)	
training:	Epoch: [86][359/408]	Loss 0.0003 (0.0030)	
training:	Epoch: [86][360/408]	Loss 0.0004 (0.0030)	
training:	Epoch: [86][361/408]	Loss 0.0002 (0.0030)	
training:	Epoch: [86][362/408]	Loss 0.0002 (0.0030)	
training:	Epoch: [86][363/408]	Loss 0.0002 (0.0030)	
training:	Epoch: [86][364/408]	Loss 0.0006 (0.0030)	
training:	Epoch: [86][365/408]	Loss 0.0003 (0.0030)	
training:	Epoch: [86][366/408]	Loss 0.0003 (0.0030)	
training:	Epoch: [86][367/408]	Loss 0.0002 (0.0029)	
training:	Epoch: [86][368/408]	Loss 0.0003 (0.0029)	
training:	Epoch: [86][369/408]	Loss 0.0013 (0.0029)	
training:	Epoch: [86][370/408]	Loss 0.0003 (0.0029)	
training:	Epoch: [86][371/408]	Loss 0.0009 (0.0029)	
training:	Epoch: [86][372/408]	Loss 0.0003 (0.0029)	
training:	Epoch: [86][373/408]	Loss 0.0008 (0.0029)	
training:	Epoch: [86][374/408]	Loss 0.0003 (0.0029)	
training:	Epoch: [86][375/408]	Loss 0.0002 (0.0029)	
training:	Epoch: [86][376/408]	Loss 0.0003 (0.0029)	
training:	Epoch: [86][377/408]	Loss 0.0002 (0.0029)	
training:	Epoch: [86][378/408]	Loss 0.0002 (0.0029)	
training:	Epoch: [86][379/408]	Loss 0.0002 (0.0029)	
training:	Epoch: [86][380/408]	Loss 0.0007 (0.0029)	
training:	Epoch: [86][381/408]	Loss 0.0003 (0.0029)	
training:	Epoch: [86][382/408]	Loss 0.0004 (0.0028)	
training:	Epoch: [86][383/408]	Loss 0.0002 (0.0028)	
training:	Epoch: [86][384/408]	Loss 0.0004 (0.0028)	
training:	Epoch: [86][385/408]	Loss 0.0002 (0.0028)	
training:	Epoch: [86][386/408]	Loss 0.0006 (0.0028)	
training:	Epoch: [86][387/408]	Loss 0.0003 (0.0028)	
training:	Epoch: [86][388/408]	Loss 0.0002 (0.0028)	
training:	Epoch: [86][389/408]	Loss 0.0002 (0.0028)	
training:	Epoch: [86][390/408]	Loss 0.0002 (0.0028)	
training:	Epoch: [86][391/408]	Loss 0.0003 (0.0028)	
training:	Epoch: [86][392/408]	Loss 0.0003 (0.0028)	
training:	Epoch: [86][393/408]	Loss 0.0003 (0.0028)	
training:	Epoch: [86][394/408]	Loss 0.0005 (0.0028)	
training:	Epoch: [86][395/408]	Loss 0.0003 (0.0028)	
training:	Epoch: [86][396/408]	Loss 0.0002 (0.0028)	
training:	Epoch: [86][397/408]	Loss 0.0013 (0.0028)	
training:	Epoch: [86][398/408]	Loss 0.0002 (0.0027)	
training:	Epoch: [86][399/408]	Loss 0.0003 (0.0027)	
training:	Epoch: [86][400/408]	Loss 0.0002 (0.0027)	
training:	Epoch: [86][401/408]	Loss 0.0003 (0.0027)	
training:	Epoch: [86][402/408]	Loss 0.0002 (0.0027)	
training:	Epoch: [86][403/408]	Loss 0.0003 (0.0027)	
training:	Epoch: [86][404/408]	Loss 0.0002 (0.0027)	
training:	Epoch: [86][405/408]	Loss 0.0002 (0.0027)	
training:	Epoch: [86][406/408]	Loss 0.0003 (0.0027)	
training:	Epoch: [86][407/408]	Loss 0.0003 (0.0027)	
training:	Epoch: [86][408/408]	Loss 0.0003 (0.0027)	
Training:	 Loss: 0.0027

Training:	 ACC: 1.0000 1.0000 1.0000 1.0000
Validation:	 ACC: 0.7769 0.7769 0.7769 0.7769
Validation:	 Best_BACC: 0.8226 0.8250 0.8772 0.7679
Validation:	 Loss: 1.3721
Pretraining:	Epoch 87/200
----------
training:	Epoch: [87][1/408]	Loss 0.0003 (0.0003)	
training:	Epoch: [87][2/408]	Loss 0.0004 (0.0004)	
training:	Epoch: [87][3/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [87][4/408]	Loss 0.0003 (0.0003)	
training:	Epoch: [87][5/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [87][6/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [87][7/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [87][8/408]	Loss 0.0003 (0.0003)	
training:	Epoch: [87][9/408]	Loss 0.0003 (0.0003)	
training:	Epoch: [87][10/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [87][11/408]	Loss 0.0003 (0.0003)	
training:	Epoch: [87][12/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [87][13/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [87][14/408]	Loss 0.0008 (0.0003)	
training:	Epoch: [87][15/408]	Loss 0.0003 (0.0003)	
training:	Epoch: [87][16/408]	Loss 0.0006 (0.0003)	
training:	Epoch: [87][17/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [87][18/408]	Loss 0.0005 (0.0003)	
training:	Epoch: [87][19/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [87][20/408]	Loss 0.0003 (0.0003)	
training:	Epoch: [87][21/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [87][22/408]	Loss 0.0003 (0.0003)	
training:	Epoch: [87][23/408]	Loss 0.0004 (0.0003)	
training:	Epoch: [87][24/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [87][25/408]	Loss 0.0003 (0.0003)	
training:	Epoch: [87][26/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [87][27/408]	Loss 0.0003 (0.0003)	
training:	Epoch: [87][28/408]	Loss 0.0003 (0.0003)	
training:	Epoch: [87][29/408]	Loss 0.0003 (0.0003)	
training:	Epoch: [87][30/408]	Loss 0.0006 (0.0003)	
training:	Epoch: [87][31/408]	Loss 0.0003 (0.0003)	
training:	Epoch: [87][32/408]	Loss 0.0003 (0.0003)	
training:	Epoch: [87][33/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [87][34/408]	Loss 0.0175 (0.0008)	
training:	Epoch: [87][35/408]	Loss 0.0002 (0.0008)	
training:	Epoch: [87][36/408]	Loss 0.0005 (0.0008)	
training:	Epoch: [87][37/408]	Loss 0.0002 (0.0008)	
training:	Epoch: [87][38/408]	Loss 0.0003 (0.0008)	
training:	Epoch: [87][39/408]	Loss 0.0004 (0.0008)	
training:	Epoch: [87][40/408]	Loss 0.0004 (0.0007)	
training:	Epoch: [87][41/408]	Loss 0.0002 (0.0007)	
training:	Epoch: [87][42/408]	Loss 0.0003 (0.0007)	
training:	Epoch: [87][43/408]	Loss 0.0008 (0.0007)	
training:	Epoch: [87][44/408]	Loss 0.0004 (0.0007)	
training:	Epoch: [87][45/408]	Loss 0.0005 (0.0007)	
training:	Epoch: [87][46/408]	Loss 0.0002 (0.0007)	
training:	Epoch: [87][47/408]	Loss 0.0002 (0.0007)	
training:	Epoch: [87][48/408]	Loss 0.0002 (0.0007)	
training:	Epoch: [87][49/408]	Loss 0.0002 (0.0007)	
training:	Epoch: [87][50/408]	Loss 0.0002 (0.0007)	
training:	Epoch: [87][51/408]	Loss 0.0002 (0.0007)	
training:	Epoch: [87][52/408]	Loss 0.0002 (0.0006)	
training:	Epoch: [87][53/408]	Loss 0.0002 (0.0006)	
training:	Epoch: [87][54/408]	Loss 0.0002 (0.0006)	
training:	Epoch: [87][55/408]	Loss 0.0002 (0.0006)	
training:	Epoch: [87][56/408]	Loss 0.0002 (0.0006)	
training:	Epoch: [87][57/408]	Loss 0.0003 (0.0006)	
training:	Epoch: [87][58/408]	Loss 0.0002 (0.0006)	
training:	Epoch: [87][59/408]	Loss 0.0007 (0.0006)	
training:	Epoch: [87][60/408]	Loss 0.0007 (0.0006)	
training:	Epoch: [87][61/408]	Loss 0.0005 (0.0006)	
training:	Epoch: [87][62/408]	Loss 0.0002 (0.0006)	
training:	Epoch: [87][63/408]	Loss 0.0004 (0.0006)	
training:	Epoch: [87][64/408]	Loss 0.0005 (0.0006)	
training:	Epoch: [87][65/408]	Loss 0.0006 (0.0006)	
training:	Epoch: [87][66/408]	Loss 0.0002 (0.0006)	
training:	Epoch: [87][67/408]	Loss 0.0008 (0.0006)	
training:	Epoch: [87][68/408]	Loss 0.0003 (0.0006)	
training:	Epoch: [87][69/408]	Loss 0.0002 (0.0006)	
training:	Epoch: [87][70/408]	Loss 0.0002 (0.0006)	
training:	Epoch: [87][71/408]	Loss 0.0002 (0.0006)	
training:	Epoch: [87][72/408]	Loss 0.0006 (0.0006)	
training:	Epoch: [87][73/408]	Loss 0.0002 (0.0006)	
training:	Epoch: [87][74/408]	Loss 0.0002 (0.0006)	
training:	Epoch: [87][75/408]	Loss 0.0002 (0.0006)	
training:	Epoch: [87][76/408]	Loss 0.0004 (0.0006)	
training:	Epoch: [87][77/408]	Loss 0.0006 (0.0006)	
training:	Epoch: [87][78/408]	Loss 0.0018 (0.0006)	
training:	Epoch: [87][79/408]	Loss 0.0007 (0.0006)	
training:	Epoch: [87][80/408]	Loss 0.0003 (0.0006)	
training:	Epoch: [87][81/408]	Loss 0.0002 (0.0006)	
training:	Epoch: [87][82/408]	Loss 0.0191 (0.0008)	
training:	Epoch: [87][83/408]	Loss 0.0002 (0.0008)	
training:	Epoch: [87][84/408]	Loss 0.0005 (0.0008)	
training:	Epoch: [87][85/408]	Loss 0.0002 (0.0008)	
training:	Epoch: [87][86/408]	Loss 0.0003 (0.0008)	
training:	Epoch: [87][87/408]	Loss 0.0002 (0.0008)	
training:	Epoch: [87][88/408]	Loss 0.0002 (0.0008)	
training:	Epoch: [87][89/408]	Loss 0.0008 (0.0008)	
training:	Epoch: [87][90/408]	Loss 0.0007 (0.0008)	
training:	Epoch: [87][91/408]	Loss 0.0003 (0.0007)	
training:	Epoch: [87][92/408]	Loss 0.0016 (0.0008)	
training:	Epoch: [87][93/408]	Loss 0.0004 (0.0008)	
training:	Epoch: [87][94/408]	Loss 0.0006 (0.0008)	
training:	Epoch: [87][95/408]	Loss 0.0003 (0.0007)	
training:	Epoch: [87][96/408]	Loss 0.0003 (0.0007)	
training:	Epoch: [87][97/408]	Loss 0.0003 (0.0007)	
training:	Epoch: [87][98/408]	Loss 0.0008 (0.0007)	
training:	Epoch: [87][99/408]	Loss 0.0002 (0.0007)	
training:	Epoch: [87][100/408]	Loss 0.0004 (0.0007)	
training:	Epoch: [87][101/408]	Loss 0.0003 (0.0007)	
training:	Epoch: [87][102/408]	Loss 0.0002 (0.0007)	
training:	Epoch: [87][103/408]	Loss 0.0003 (0.0007)	
training:	Epoch: [87][104/408]	Loss 0.0002 (0.0007)	
training:	Epoch: [87][105/408]	Loss 0.0002 (0.0007)	
training:	Epoch: [87][106/408]	Loss 0.0246 (0.0009)	
training:	Epoch: [87][107/408]	Loss 0.0006 (0.0009)	
training:	Epoch: [87][108/408]	Loss 0.0004 (0.0009)	
training:	Epoch: [87][109/408]	Loss 0.0006 (0.0009)	
training:	Epoch: [87][110/408]	Loss 0.0002 (0.0009)	
training:	Epoch: [87][111/408]	Loss 0.0007 (0.0009)	
training:	Epoch: [87][112/408]	Loss 0.0002 (0.0009)	
training:	Epoch: [87][113/408]	Loss 0.0006 (0.0009)	
training:	Epoch: [87][114/408]	Loss 0.0005 (0.0009)	
training:	Epoch: [87][115/408]	Loss 0.0002 (0.0009)	
training:	Epoch: [87][116/408]	Loss 0.0003 (0.0009)	
training:	Epoch: [87][117/408]	Loss 0.0002 (0.0009)	
training:	Epoch: [87][118/408]	Loss 0.0006 (0.0009)	
training:	Epoch: [87][119/408]	Loss 0.0002 (0.0009)	
training:	Epoch: [87][120/408]	Loss 0.0003 (0.0009)	
training:	Epoch: [87][121/408]	Loss 0.0007 (0.0009)	
training:	Epoch: [87][122/408]	Loss 0.1141 (0.0018)	
training:	Epoch: [87][123/408]	Loss 0.0005 (0.0018)	
training:	Epoch: [87][124/408]	Loss 0.0002 (0.0018)	
training:	Epoch: [87][125/408]	Loss 0.0007 (0.0018)	
training:	Epoch: [87][126/408]	Loss 0.0005 (0.0018)	
training:	Epoch: [87][127/408]	Loss 0.0004 (0.0017)	
training:	Epoch: [87][128/408]	Loss 0.0006 (0.0017)	
training:	Epoch: [87][129/408]	Loss 0.0003 (0.0017)	
training:	Epoch: [87][130/408]	Loss 0.0012 (0.0017)	
training:	Epoch: [87][131/408]	Loss 0.0004 (0.0017)	
training:	Epoch: [87][132/408]	Loss 0.0005 (0.0017)	
training:	Epoch: [87][133/408]	Loss 0.0005 (0.0017)	
training:	Epoch: [87][134/408]	Loss 0.0002 (0.0017)	
training:	Epoch: [87][135/408]	Loss 0.0003 (0.0017)	
training:	Epoch: [87][136/408]	Loss 0.0006 (0.0017)	
training:	Epoch: [87][137/408]	Loss 0.0002 (0.0017)	
training:	Epoch: [87][138/408]	Loss 0.0002 (0.0016)	
training:	Epoch: [87][139/408]	Loss 0.0002 (0.0016)	
training:	Epoch: [87][140/408]	Loss 0.0020 (0.0016)	
training:	Epoch: [87][141/408]	Loss 0.0003 (0.0016)	
training:	Epoch: [87][142/408]	Loss 0.0004 (0.0016)	
training:	Epoch: [87][143/408]	Loss 0.0002 (0.0016)	
training:	Epoch: [87][144/408]	Loss 0.0002 (0.0016)	
training:	Epoch: [87][145/408]	Loss 0.0007 (0.0016)	
training:	Epoch: [87][146/408]	Loss 0.0002 (0.0016)	
training:	Epoch: [87][147/408]	Loss 0.0004 (0.0016)	
training:	Epoch: [87][148/408]	Loss 0.0002 (0.0016)	
training:	Epoch: [87][149/408]	Loss 0.0003 (0.0016)	
training:	Epoch: [87][150/408]	Loss 0.0003 (0.0015)	
training:	Epoch: [87][151/408]	Loss 0.0003 (0.0015)	
training:	Epoch: [87][152/408]	Loss 0.0003 (0.0015)	
training:	Epoch: [87][153/408]	Loss 0.0006 (0.0015)	
training:	Epoch: [87][154/408]	Loss 0.0017 (0.0015)	
training:	Epoch: [87][155/408]	Loss 0.0003 (0.0015)	
training:	Epoch: [87][156/408]	Loss 0.0032 (0.0015)	
training:	Epoch: [87][157/408]	Loss 0.0002 (0.0015)	
training:	Epoch: [87][158/408]	Loss 0.0002 (0.0015)	
training:	Epoch: [87][159/408]	Loss 0.0005 (0.0015)	
training:	Epoch: [87][160/408]	Loss 0.0002 (0.0015)	
training:	Epoch: [87][161/408]	Loss 0.0004 (0.0015)	
training:	Epoch: [87][162/408]	Loss 0.0003 (0.0015)	
training:	Epoch: [87][163/408]	Loss 0.0007 (0.0015)	
training:	Epoch: [87][164/408]	Loss 0.0571 (0.0018)	
training:	Epoch: [87][165/408]	Loss 0.0002 (0.0018)	
training:	Epoch: [87][166/408]	Loss 0.0004 (0.0018)	
training:	Epoch: [87][167/408]	Loss 0.0002 (0.0018)	
training:	Epoch: [87][168/408]	Loss 0.0002 (0.0018)	
training:	Epoch: [87][169/408]	Loss 0.0006 (0.0018)	
training:	Epoch: [87][170/408]	Loss 0.0004 (0.0018)	
training:	Epoch: [87][171/408]	Loss 0.0004 (0.0018)	
training:	Epoch: [87][172/408]	Loss 0.0003 (0.0017)	
training:	Epoch: [87][173/408]	Loss 0.0002 (0.0017)	
training:	Epoch: [87][174/408]	Loss 0.0005 (0.0017)	
training:	Epoch: [87][175/408]	Loss 0.0002 (0.0017)	
training:	Epoch: [87][176/408]	Loss 0.0002 (0.0017)	
training:	Epoch: [87][177/408]	Loss 0.0006 (0.0017)	
training:	Epoch: [87][178/408]	Loss 0.0003 (0.0017)	
training:	Epoch: [87][179/408]	Loss 0.0002 (0.0017)	
training:	Epoch: [87][180/408]	Loss 0.0003 (0.0017)	
training:	Epoch: [87][181/408]	Loss 0.0002 (0.0017)	
training:	Epoch: [87][182/408]	Loss 0.0009 (0.0017)	
training:	Epoch: [87][183/408]	Loss 0.0004 (0.0017)	
training:	Epoch: [87][184/408]	Loss 0.0007 (0.0017)	
training:	Epoch: [87][185/408]	Loss 0.0002 (0.0017)	
training:	Epoch: [87][186/408]	Loss 0.0004 (0.0016)	
training:	Epoch: [87][187/408]	Loss 0.0004 (0.0016)	
training:	Epoch: [87][188/408]	Loss 0.0005 (0.0016)	
training:	Epoch: [87][189/408]	Loss 0.0005 (0.0016)	
training:	Epoch: [87][190/408]	Loss 0.0005 (0.0016)	
training:	Epoch: [87][191/408]	Loss 0.0006 (0.0016)	
training:	Epoch: [87][192/408]	Loss 0.0003 (0.0016)	
training:	Epoch: [87][193/408]	Loss 0.0002 (0.0016)	
training:	Epoch: [87][194/408]	Loss 0.0002 (0.0016)	
training:	Epoch: [87][195/408]	Loss 0.0003 (0.0016)	
training:	Epoch: [87][196/408]	Loss 0.0006 (0.0016)	
training:	Epoch: [87][197/408]	Loss 0.0004 (0.0016)	
training:	Epoch: [87][198/408]	Loss 0.0002 (0.0016)	
training:	Epoch: [87][199/408]	Loss 0.0003 (0.0016)	
training:	Epoch: [87][200/408]	Loss 0.0003 (0.0016)	
training:	Epoch: [87][201/408]	Loss 0.0002 (0.0015)	
training:	Epoch: [87][202/408]	Loss 0.0033 (0.0016)	
training:	Epoch: [87][203/408]	Loss 0.0003 (0.0016)	
training:	Epoch: [87][204/408]	Loss 0.0006 (0.0015)	
training:	Epoch: [87][205/408]	Loss 0.0002 (0.0015)	
training:	Epoch: [87][206/408]	Loss 0.0005 (0.0015)	
training:	Epoch: [87][207/408]	Loss 0.0002 (0.0015)	
training:	Epoch: [87][208/408]	Loss 0.0005 (0.0015)	
training:	Epoch: [87][209/408]	Loss 0.0010 (0.0015)	
training:	Epoch: [87][210/408]	Loss 0.0004 (0.0015)	
training:	Epoch: [87][211/408]	Loss 0.0002 (0.0015)	
training:	Epoch: [87][212/408]	Loss 0.0002 (0.0015)	
training:	Epoch: [87][213/408]	Loss 0.0002 (0.0015)	
training:	Epoch: [87][214/408]	Loss 0.0002 (0.0015)	
training:	Epoch: [87][215/408]	Loss 0.0002 (0.0015)	
training:	Epoch: [87][216/408]	Loss 0.0003 (0.0015)	
training:	Epoch: [87][217/408]	Loss 0.0002 (0.0015)	
training:	Epoch: [87][218/408]	Loss 0.0005 (0.0015)	
training:	Epoch: [87][219/408]	Loss 0.0002 (0.0015)	
training:	Epoch: [87][220/408]	Loss 0.0003 (0.0015)	
training:	Epoch: [87][221/408]	Loss 0.0002 (0.0015)	
training:	Epoch: [87][222/408]	Loss 0.0002 (0.0014)	
training:	Epoch: [87][223/408]	Loss 0.0003 (0.0014)	
training:	Epoch: [87][224/408]	Loss 0.0003 (0.0014)	
training:	Epoch: [87][225/408]	Loss 0.0002 (0.0014)	
training:	Epoch: [87][226/408]	Loss 0.0004 (0.0014)	
training:	Epoch: [87][227/408]	Loss 0.0007 (0.0014)	
training:	Epoch: [87][228/408]	Loss 0.0002 (0.0014)	
training:	Epoch: [87][229/408]	Loss 0.0002 (0.0014)	
training:	Epoch: [87][230/408]	Loss 0.0002 (0.0014)	
training:	Epoch: [87][231/408]	Loss 0.0014 (0.0014)	
training:	Epoch: [87][232/408]	Loss 0.0002 (0.0014)	
training:	Epoch: [87][233/408]	Loss 0.0002 (0.0014)	
training:	Epoch: [87][234/408]	Loss 0.0002 (0.0014)	
training:	Epoch: [87][235/408]	Loss 0.0003 (0.0014)	
training:	Epoch: [87][236/408]	Loss 0.0005 (0.0014)	
training:	Epoch: [87][237/408]	Loss 0.0006 (0.0014)	
training:	Epoch: [87][238/408]	Loss 0.0002 (0.0014)	
training:	Epoch: [87][239/408]	Loss 0.0010 (0.0014)	
training:	Epoch: [87][240/408]	Loss 0.0003 (0.0014)	
training:	Epoch: [87][241/408]	Loss 0.0003 (0.0014)	
training:	Epoch: [87][242/408]	Loss 0.0002 (0.0014)	
training:	Epoch: [87][243/408]	Loss 0.0003 (0.0014)	
training:	Epoch: [87][244/408]	Loss 0.0002 (0.0014)	
training:	Epoch: [87][245/408]	Loss 0.0003 (0.0013)	
training:	Epoch: [87][246/408]	Loss 0.0004 (0.0013)	
training:	Epoch: [87][247/408]	Loss 0.0005 (0.0013)	
training:	Epoch: [87][248/408]	Loss 0.0002 (0.0013)	
training:	Epoch: [87][249/408]	Loss 0.0002 (0.0013)	
training:	Epoch: [87][250/408]	Loss 0.0004 (0.0013)	
training:	Epoch: [87][251/408]	Loss 0.0002 (0.0013)	
training:	Epoch: [87][252/408]	Loss 0.0003 (0.0013)	
training:	Epoch: [87][253/408]	Loss 0.0004 (0.0013)	
training:	Epoch: [87][254/408]	Loss 0.0002 (0.0013)	
training:	Epoch: [87][255/408]	Loss 0.0002 (0.0013)	
training:	Epoch: [87][256/408]	Loss 0.0004 (0.0013)	
training:	Epoch: [87][257/408]	Loss 0.0002 (0.0013)	
training:	Epoch: [87][258/408]	Loss 0.0002 (0.0013)	
training:	Epoch: [87][259/408]	Loss 0.0002 (0.0013)	
training:	Epoch: [87][260/408]	Loss 0.0003 (0.0013)	
training:	Epoch: [87][261/408]	Loss 0.0003 (0.0013)	
training:	Epoch: [87][262/408]	Loss 0.0003 (0.0013)	
training:	Epoch: [87][263/408]	Loss 0.0002 (0.0013)	
training:	Epoch: [87][264/408]	Loss 0.0002 (0.0013)	
training:	Epoch: [87][265/408]	Loss 0.0003 (0.0013)	
training:	Epoch: [87][266/408]	Loss 0.0030 (0.0013)	
training:	Epoch: [87][267/408]	Loss 0.0004 (0.0013)	
training:	Epoch: [87][268/408]	Loss 0.0003 (0.0013)	
training:	Epoch: [87][269/408]	Loss 0.0007 (0.0013)	
training:	Epoch: [87][270/408]	Loss 0.0002 (0.0013)	
training:	Epoch: [87][271/408]	Loss 0.0002 (0.0013)	
training:	Epoch: [87][272/408]	Loss 0.0018 (0.0013)	
training:	Epoch: [87][273/408]	Loss 0.0004 (0.0013)	
training:	Epoch: [87][274/408]	Loss 0.0003 (0.0013)	
training:	Epoch: [87][275/408]	Loss 0.0003 (0.0012)	
training:	Epoch: [87][276/408]	Loss 0.0002 (0.0012)	
training:	Epoch: [87][277/408]	Loss 0.0134 (0.0013)	
training:	Epoch: [87][278/408]	Loss 0.0002 (0.0013)	
training:	Epoch: [87][279/408]	Loss 0.0002 (0.0013)	
training:	Epoch: [87][280/408]	Loss 0.0002 (0.0013)	
training:	Epoch: [87][281/408]	Loss 0.0002 (0.0013)	
training:	Epoch: [87][282/408]	Loss 0.0002 (0.0013)	
training:	Epoch: [87][283/408]	Loss 0.0002 (0.0013)	
training:	Epoch: [87][284/408]	Loss 0.0002 (0.0013)	
training:	Epoch: [87][285/408]	Loss 0.0004 (0.0013)	
training:	Epoch: [87][286/408]	Loss 0.0002 (0.0013)	
training:	Epoch: [87][287/408]	Loss 0.0003 (0.0013)	
training:	Epoch: [87][288/408]	Loss 0.0002 (0.0012)	
training:	Epoch: [87][289/408]	Loss 0.0002 (0.0012)	
training:	Epoch: [87][290/408]	Loss 0.0002 (0.0012)	
training:	Epoch: [87][291/408]	Loss 0.0002 (0.0012)	
training:	Epoch: [87][292/408]	Loss 0.0002 (0.0012)	
training:	Epoch: [87][293/408]	Loss 0.0002 (0.0012)	
training:	Epoch: [87][294/408]	Loss 0.0002 (0.0012)	
training:	Epoch: [87][295/408]	Loss 0.0008 (0.0012)	
training:	Epoch: [87][296/408]	Loss 0.0012 (0.0012)	
training:	Epoch: [87][297/408]	Loss 0.0002 (0.0012)	
training:	Epoch: [87][298/408]	Loss 0.0002 (0.0012)	
training:	Epoch: [87][299/408]	Loss 0.0003 (0.0012)	
training:	Epoch: [87][300/408]	Loss 0.0002 (0.0012)	
training:	Epoch: [87][301/408]	Loss 0.0002 (0.0012)	
training:	Epoch: [87][302/408]	Loss 0.0002 (0.0012)	
training:	Epoch: [87][303/408]	Loss 0.0002 (0.0012)	
training:	Epoch: [87][304/408]	Loss 0.0002 (0.0012)	
training:	Epoch: [87][305/408]	Loss 0.0002 (0.0012)	
training:	Epoch: [87][306/408]	Loss 0.0002 (0.0012)	
training:	Epoch: [87][307/408]	Loss 0.0004 (0.0012)	
training:	Epoch: [87][308/408]	Loss 0.0002 (0.0012)	
training:	Epoch: [87][309/408]	Loss 0.0002 (0.0012)	
training:	Epoch: [87][310/408]	Loss 0.0004 (0.0012)	
training:	Epoch: [87][311/408]	Loss 0.0005 (0.0012)	
training:	Epoch: [87][312/408]	Loss 0.0003 (0.0012)	
training:	Epoch: [87][313/408]	Loss 0.0021 (0.0012)	
training:	Epoch: [87][314/408]	Loss 0.0002 (0.0012)	
training:	Epoch: [87][315/408]	Loss 0.0003 (0.0012)	
training:	Epoch: [87][316/408]	Loss 0.0006 (0.0012)	
training:	Epoch: [87][317/408]	Loss 0.0177 (0.0012)	
training:	Epoch: [87][318/408]	Loss 0.0003 (0.0012)	
training:	Epoch: [87][319/408]	Loss 0.0002 (0.0012)	
training:	Epoch: [87][320/408]	Loss 0.0002 (0.0012)	
training:	Epoch: [87][321/408]	Loss 0.0002 (0.0012)	
training:	Epoch: [87][322/408]	Loss 0.0004 (0.0012)	
training:	Epoch: [87][323/408]	Loss 0.0003 (0.0012)	
training:	Epoch: [87][324/408]	Loss 0.0008 (0.0012)	
training:	Epoch: [87][325/408]	Loss 0.0021 (0.0012)	
training:	Epoch: [87][326/408]	Loss 0.0002 (0.0012)	
training:	Epoch: [87][327/408]	Loss 0.0010 (0.0012)	
training:	Epoch: [87][328/408]	Loss 0.0002 (0.0012)	
training:	Epoch: [87][329/408]	Loss 0.0004 (0.0012)	
training:	Epoch: [87][330/408]	Loss 0.0002 (0.0012)	
training:	Epoch: [87][331/408]	Loss 0.0003 (0.0012)	
training:	Epoch: [87][332/408]	Loss 0.0003 (0.0012)	
training:	Epoch: [87][333/408]	Loss 0.0002 (0.0012)	
training:	Epoch: [87][334/408]	Loss 0.0002 (0.0012)	
training:	Epoch: [87][335/408]	Loss 0.0004 (0.0012)	
training:	Epoch: [87][336/408]	Loss 0.0002 (0.0012)	
training:	Epoch: [87][337/408]	Loss 0.0002 (0.0012)	
training:	Epoch: [87][338/408]	Loss 0.0002 (0.0012)	
training:	Epoch: [87][339/408]	Loss 0.0005 (0.0012)	
training:	Epoch: [87][340/408]	Loss 0.0002 (0.0012)	
training:	Epoch: [87][341/408]	Loss 0.0002 (0.0012)	
training:	Epoch: [87][342/408]	Loss 0.0002 (0.0012)	
training:	Epoch: [87][343/408]	Loss 0.0002 (0.0012)	
training:	Epoch: [87][344/408]	Loss 0.0003 (0.0012)	
training:	Epoch: [87][345/408]	Loss 0.0010 (0.0012)	
training:	Epoch: [87][346/408]	Loss 0.0002 (0.0012)	
training:	Epoch: [87][347/408]	Loss 0.0002 (0.0011)	
training:	Epoch: [87][348/408]	Loss 0.0017 (0.0012)	
training:	Epoch: [87][349/408]	Loss 0.0004 (0.0011)	
training:	Epoch: [87][350/408]	Loss 0.0002 (0.0011)	
training:	Epoch: [87][351/408]	Loss 0.0002 (0.0011)	
training:	Epoch: [87][352/408]	Loss 0.0005 (0.0011)	
training:	Epoch: [87][353/408]	Loss 0.0002 (0.0011)	
training:	Epoch: [87][354/408]	Loss 0.0002 (0.0011)	
training:	Epoch: [87][355/408]	Loss 0.0002 (0.0011)	
training:	Epoch: [87][356/408]	Loss 0.0002 (0.0011)	
training:	Epoch: [87][357/408]	Loss 0.0003 (0.0011)	
training:	Epoch: [87][358/408]	Loss 0.0003 (0.0011)	
training:	Epoch: [87][359/408]	Loss 0.0002 (0.0011)	
training:	Epoch: [87][360/408]	Loss 0.0020 (0.0011)	
training:	Epoch: [87][361/408]	Loss 0.0002 (0.0011)	
training:	Epoch: [87][362/408]	Loss 0.0002 (0.0011)	
training:	Epoch: [87][363/408]	Loss 0.0002 (0.0011)	
training:	Epoch: [87][364/408]	Loss 0.0003 (0.0011)	
training:	Epoch: [87][365/408]	Loss 0.0002 (0.0011)	
training:	Epoch: [87][366/408]	Loss 0.0007 (0.0011)	
training:	Epoch: [87][367/408]	Loss 0.0005 (0.0011)	
training:	Epoch: [87][368/408]	Loss 0.0003 (0.0011)	
training:	Epoch: [87][369/408]	Loss 0.0002 (0.0011)	
training:	Epoch: [87][370/408]	Loss 0.0003 (0.0011)	
training:	Epoch: [87][371/408]	Loss 0.0002 (0.0011)	
training:	Epoch: [87][372/408]	Loss 0.0002 (0.0011)	
training:	Epoch: [87][373/408]	Loss 0.0002 (0.0011)	
training:	Epoch: [87][374/408]	Loss 0.0002 (0.0011)	
training:	Epoch: [87][375/408]	Loss 0.0008 (0.0011)	
training:	Epoch: [87][376/408]	Loss 0.0002 (0.0011)	
training:	Epoch: [87][377/408]	Loss 0.0004 (0.0011)	
training:	Epoch: [87][378/408]	Loss 0.0002 (0.0011)	
training:	Epoch: [87][379/408]	Loss 0.0002 (0.0011)	
training:	Epoch: [87][380/408]	Loss 0.0009 (0.0011)	
training:	Epoch: [87][381/408]	Loss 0.0002 (0.0011)	
training:	Epoch: [87][382/408]	Loss 0.0005 (0.0011)	
training:	Epoch: [87][383/408]	Loss 0.0014 (0.0011)	
training:	Epoch: [87][384/408]	Loss 0.0002 (0.0011)	
training:	Epoch: [87][385/408]	Loss 0.0002 (0.0011)	
training:	Epoch: [87][386/408]	Loss 0.0003 (0.0011)	
training:	Epoch: [87][387/408]	Loss 0.0002 (0.0011)	
training:	Epoch: [87][388/408]	Loss 0.0002 (0.0011)	
training:	Epoch: [87][389/408]	Loss 0.0003 (0.0011)	
training:	Epoch: [87][390/408]	Loss 0.0003 (0.0011)	
training:	Epoch: [87][391/408]	Loss 0.0002 (0.0011)	
training:	Epoch: [87][392/408]	Loss 0.0002 (0.0011)	
training:	Epoch: [87][393/408]	Loss 0.0002 (0.0011)	
training:	Epoch: [87][394/408]	Loss 0.0002 (0.0011)	
training:	Epoch: [87][395/408]	Loss 0.0002 (0.0011)	
training:	Epoch: [87][396/408]	Loss 0.0003 (0.0011)	
training:	Epoch: [87][397/408]	Loss 0.0002 (0.0011)	
training:	Epoch: [87][398/408]	Loss 0.0002 (0.0010)	
training:	Epoch: [87][399/408]	Loss 0.0002 (0.0010)	
training:	Epoch: [87][400/408]	Loss 0.0004 (0.0010)	
training:	Epoch: [87][401/408]	Loss 0.0003 (0.0010)	
training:	Epoch: [87][402/408]	Loss 0.0011 (0.0010)	
training:	Epoch: [87][403/408]	Loss 0.0002 (0.0010)	
training:	Epoch: [87][404/408]	Loss 0.0003 (0.0010)	
training:	Epoch: [87][405/408]	Loss 0.0005 (0.0010)	
training:	Epoch: [87][406/408]	Loss 0.0003 (0.0010)	
training:	Epoch: [87][407/408]	Loss 0.0007 (0.0010)	
training:	Epoch: [87][408/408]	Loss 0.0002 (0.0010)	
Training:	 Loss: 0.0010

Training:	 ACC: 1.0000 1.0000 1.0000 1.0000
Validation:	 ACC: 0.7767 0.7774 0.7932 0.7601
Validation:	 Best_BACC: 0.8226 0.8250 0.8772 0.7679
Validation:	 Loss: 1.4267
Pretraining:	Epoch 88/200
----------
training:	Epoch: [88][1/408]	Loss 0.0005 (0.0005)	
training:	Epoch: [88][2/408]	Loss 0.0008 (0.0006)	
training:	Epoch: [88][3/408]	Loss 0.0002 (0.0005)	
training:	Epoch: [88][4/408]	Loss 0.0021 (0.0009)	
training:	Epoch: [88][5/408]	Loss 0.0002 (0.0008)	
training:	Epoch: [88][6/408]	Loss 0.0005 (0.0007)	
training:	Epoch: [88][7/408]	Loss 0.0002 (0.0007)	
training:	Epoch: [88][8/408]	Loss 0.0004 (0.0006)	
training:	Epoch: [88][9/408]	Loss 0.0002 (0.0006)	
training:	Epoch: [88][10/408]	Loss 0.0002 (0.0005)	
training:	Epoch: [88][11/408]	Loss 0.0002 (0.0005)	
training:	Epoch: [88][12/408]	Loss 0.0002 (0.0005)	
training:	Epoch: [88][13/408]	Loss 0.0002 (0.0005)	
training:	Epoch: [88][14/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [88][15/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [88][16/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [88][17/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [88][18/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [88][19/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [88][20/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [88][21/408]	Loss 0.0005 (0.0004)	
training:	Epoch: [88][22/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [88][23/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [88][24/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [88][25/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [88][26/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [88][27/408]	Loss 0.0005 (0.0004)	
training:	Epoch: [88][28/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [88][29/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [88][30/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [88][31/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [88][32/408]	Loss 0.0003 (0.0003)	
training:	Epoch: [88][33/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [88][34/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [88][35/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [88][36/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [88][37/408]	Loss 0.0003 (0.0003)	
training:	Epoch: [88][38/408]	Loss 0.0003 (0.0003)	
training:	Epoch: [88][39/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [88][40/408]	Loss 0.0008 (0.0003)	
training:	Epoch: [88][41/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [88][42/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [88][43/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [88][44/408]	Loss 0.0009 (0.0003)	
training:	Epoch: [88][45/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [88][46/408]	Loss 0.0003 (0.0003)	
training:	Epoch: [88][47/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [88][48/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [88][49/408]	Loss 0.0006 (0.0003)	
training:	Epoch: [88][50/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [88][51/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [88][52/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [88][53/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [88][54/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [88][55/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [88][56/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [88][57/408]	Loss 0.0003 (0.0003)	
training:	Epoch: [88][58/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [88][59/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [88][60/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [88][61/408]	Loss 0.0020 (0.0003)	
training:	Epoch: [88][62/408]	Loss 0.0003 (0.0003)	
training:	Epoch: [88][63/408]	Loss 0.0005 (0.0003)	
training:	Epoch: [88][64/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [88][65/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [88][66/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [88][67/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [88][68/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [88][69/408]	Loss 0.0003 (0.0003)	
training:	Epoch: [88][70/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [88][71/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [88][72/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [88][73/408]	Loss 0.0003 (0.0003)	
training:	Epoch: [88][74/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [88][75/408]	Loss 0.0003 (0.0003)	
training:	Epoch: [88][76/408]	Loss 0.0005 (0.0003)	
training:	Epoch: [88][77/408]	Loss 0.0006 (0.0003)	
training:	Epoch: [88][78/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [88][79/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [88][80/408]	Loss 0.0004 (0.0003)	
training:	Epoch: [88][81/408]	Loss 0.0003 (0.0003)	
training:	Epoch: [88][82/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [88][83/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [88][84/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [88][85/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [88][86/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [88][87/408]	Loss 0.0011 (0.0003)	
training:	Epoch: [88][88/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [88][89/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [88][90/408]	Loss 0.0016 (0.0003)	
training:	Epoch: [88][91/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [88][92/408]	Loss 0.0003 (0.0003)	
training:	Epoch: [88][93/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [88][94/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [88][95/408]	Loss 0.0003 (0.0003)	
training:	Epoch: [88][96/408]	Loss 0.0007 (0.0003)	
training:	Epoch: [88][97/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [88][98/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [88][99/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [88][100/408]	Loss 0.0008 (0.0003)	
training:	Epoch: [88][101/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [88][102/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [88][103/408]	Loss 0.0004 (0.0003)	
training:	Epoch: [88][104/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [88][105/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [88][106/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [88][107/408]	Loss 0.0004 (0.0003)	
training:	Epoch: [88][108/408]	Loss 0.0052 (0.0004)	
training:	Epoch: [88][109/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [88][110/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [88][111/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [88][112/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [88][113/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [88][114/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [88][115/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [88][116/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [88][117/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [88][118/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [88][119/408]	Loss 0.0005 (0.0004)	
training:	Epoch: [88][120/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [88][121/408]	Loss 0.0004 (0.0004)	
training:	Epoch: [88][122/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [88][123/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [88][124/408]	Loss 0.0007 (0.0004)	
training:	Epoch: [88][125/408]	Loss 0.0005 (0.0004)	
training:	Epoch: [88][126/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [88][127/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [88][128/408]	Loss 0.0004 (0.0004)	
training:	Epoch: [88][129/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [88][130/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [88][131/408]	Loss 0.0006 (0.0004)	
training:	Epoch: [88][132/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [88][133/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [88][134/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [88][135/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [88][136/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [88][137/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [88][138/408]	Loss 0.0004 (0.0004)	
training:	Epoch: [88][139/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [88][140/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [88][141/408]	Loss 0.0006 (0.0004)	
training:	Epoch: [88][142/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [88][143/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [88][144/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [88][145/408]	Loss 0.0004 (0.0003)	
training:	Epoch: [88][146/408]	Loss 0.0004 (0.0004)	
training:	Epoch: [88][147/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [88][148/408]	Loss 0.0003 (0.0003)	
training:	Epoch: [88][149/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [88][150/408]	Loss 0.0003 (0.0003)	
training:	Epoch: [88][151/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [88][152/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [88][153/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [88][154/408]	Loss 0.0004 (0.0003)	
training:	Epoch: [88][155/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [88][156/408]	Loss 0.0005 (0.0003)	
training:	Epoch: [88][157/408]	Loss 0.0005 (0.0003)	
training:	Epoch: [88][158/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [88][159/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [88][160/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [88][161/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [88][162/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [88][163/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [88][164/408]	Loss 0.0003 (0.0003)	
training:	Epoch: [88][165/408]	Loss 0.0003 (0.0003)	
training:	Epoch: [88][166/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [88][167/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [88][168/408]	Loss 0.0003 (0.0003)	
training:	Epoch: [88][169/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [88][170/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [88][171/408]	Loss 0.0003 (0.0003)	
training:	Epoch: [88][172/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [88][173/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [88][174/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [88][175/408]	Loss 0.0003 (0.0003)	
training:	Epoch: [88][176/408]	Loss 0.0006 (0.0003)	
training:	Epoch: [88][177/408]	Loss 0.0018 (0.0003)	
training:	Epoch: [88][178/408]	Loss 0.0005 (0.0003)	
training:	Epoch: [88][179/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [88][180/408]	Loss 0.0005 (0.0003)	
training:	Epoch: [88][181/408]	Loss 0.0003 (0.0003)	
training:	Epoch: [88][182/408]	Loss 0.0005 (0.0003)	
training:	Epoch: [88][183/408]	Loss 0.0008 (0.0003)	
training:	Epoch: [88][184/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [88][185/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [88][186/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [88][187/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [88][188/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [88][189/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [88][190/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [88][191/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [88][192/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [88][193/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [88][194/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [88][195/408]	Loss 0.0004 (0.0003)	
training:	Epoch: [88][196/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [88][197/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [88][198/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [88][199/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [88][200/408]	Loss 0.0005 (0.0003)	
training:	Epoch: [88][201/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [88][202/408]	Loss 0.0011 (0.0003)	
training:	Epoch: [88][203/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [88][204/408]	Loss 0.0003 (0.0003)	
training:	Epoch: [88][205/408]	Loss 0.0003 (0.0003)	
training:	Epoch: [88][206/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [88][207/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [88][208/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [88][209/408]	Loss 0.0003 (0.0003)	
training:	Epoch: [88][210/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [88][211/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [88][212/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [88][213/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [88][214/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [88][215/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [88][216/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [88][217/408]	Loss 0.0005 (0.0003)	
training:	Epoch: [88][218/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [88][219/408]	Loss 0.0003 (0.0003)	
training:	Epoch: [88][220/408]	Loss 0.0006 (0.0003)	
training:	Epoch: [88][221/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [88][222/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [88][223/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [88][224/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [88][225/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [88][226/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [88][227/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [88][228/408]	Loss 0.0013 (0.0003)	
training:	Epoch: [88][229/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [88][230/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [88][231/408]	Loss 0.0003 (0.0003)	
training:	Epoch: [88][232/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [88][233/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [88][234/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [88][235/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [88][236/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [88][237/408]	Loss 0.0008 (0.0003)	
training:	Epoch: [88][238/408]	Loss 0.0003 (0.0003)	
training:	Epoch: [88][239/408]	Loss 0.0005 (0.0003)	
training:	Epoch: [88][240/408]	Loss 0.0004 (0.0003)	
training:	Epoch: [88][241/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [88][242/408]	Loss 0.0003 (0.0003)	
training:	Epoch: [88][243/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [88][244/408]	Loss 0.0004 (0.0003)	
training:	Epoch: [88][245/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [88][246/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [88][247/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [88][248/408]	Loss 0.0003 (0.0003)	
training:	Epoch: [88][249/408]	Loss 0.0003 (0.0003)	
training:	Epoch: [88][250/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [88][251/408]	Loss 0.0007 (0.0003)	
training:	Epoch: [88][252/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [88][253/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [88][254/408]	Loss 0.0003 (0.0003)	
training:	Epoch: [88][255/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [88][256/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [88][257/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [88][258/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [88][259/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [88][260/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [88][261/408]	Loss 0.0003 (0.0003)	
training:	Epoch: [88][262/408]	Loss 0.0005 (0.0003)	
training:	Epoch: [88][263/408]	Loss 0.0004 (0.0003)	
training:	Epoch: [88][264/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [88][265/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [88][266/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [88][267/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [88][268/408]	Loss 0.0003 (0.0003)	
training:	Epoch: [88][269/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [88][270/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [88][271/408]	Loss 0.0012 (0.0003)	
training:	Epoch: [88][272/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [88][273/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [88][274/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [88][275/408]	Loss 0.0006 (0.0003)	
training:	Epoch: [88][276/408]	Loss 0.0003 (0.0003)	
training:	Epoch: [88][277/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [88][278/408]	Loss 0.0007 (0.0003)	
training:	Epoch: [88][279/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [88][280/408]	Loss 0.0003 (0.0003)	
training:	Epoch: [88][281/408]	Loss 0.0003 (0.0003)	
training:	Epoch: [88][282/408]	Loss 0.0003 (0.0003)	
training:	Epoch: [88][283/408]	Loss 0.0004 (0.0003)	
training:	Epoch: [88][284/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [88][285/408]	Loss 0.0003 (0.0003)	
training:	Epoch: [88][286/408]	Loss 0.0023 (0.0003)	
training:	Epoch: [88][287/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [88][288/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [88][289/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [88][290/408]	Loss 0.0005 (0.0003)	
training:	Epoch: [88][291/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [88][292/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [88][293/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [88][294/408]	Loss 0.0005 (0.0003)	
training:	Epoch: [88][295/408]	Loss 0.0003 (0.0003)	
training:	Epoch: [88][296/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [88][297/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [88][298/408]	Loss 0.0005 (0.0003)	
training:	Epoch: [88][299/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [88][300/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [88][301/408]	Loss 0.0038 (0.0003)	
training:	Epoch: [88][302/408]	Loss 0.0003 (0.0003)	
training:	Epoch: [88][303/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [88][304/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [88][305/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [88][306/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [88][307/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [88][308/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [88][309/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [88][310/408]	Loss 0.0003 (0.0003)	
training:	Epoch: [88][311/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [88][312/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [88][313/408]	Loss 0.0003 (0.0003)	
training:	Epoch: [88][314/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [88][315/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [88][316/408]	Loss 0.0003 (0.0003)	
training:	Epoch: [88][317/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [88][318/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [88][319/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [88][320/408]	Loss 0.0004 (0.0003)	
training:	Epoch: [88][321/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [88][322/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [88][323/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [88][324/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [88][325/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [88][326/408]	Loss 0.0003 (0.0003)	
training:	Epoch: [88][327/408]	Loss 0.0012 (0.0003)	
training:	Epoch: [88][328/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [88][329/408]	Loss 0.0003 (0.0003)	
training:	Epoch: [88][330/408]	Loss 0.0004 (0.0003)	
training:	Epoch: [88][331/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [88][332/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [88][333/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [88][334/408]	Loss 0.0003 (0.0003)	
training:	Epoch: [88][335/408]	Loss 0.0004 (0.0003)	
training:	Epoch: [88][336/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [88][337/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [88][338/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [88][339/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [88][340/408]	Loss 0.0004 (0.0003)	
training:	Epoch: [88][341/408]	Loss 0.0005 (0.0003)	
training:	Epoch: [88][342/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [88][343/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [88][344/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [88][345/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [88][346/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [88][347/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [88][348/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [88][349/408]	Loss 0.0003 (0.0003)	
training:	Epoch: [88][350/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [88][351/408]	Loss 0.0003 (0.0003)	
training:	Epoch: [88][352/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [88][353/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [88][354/408]	Loss 0.0005 (0.0003)	
training:	Epoch: [88][355/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [88][356/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [88][357/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [88][358/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [88][359/408]	Loss 0.0003 (0.0003)	
training:	Epoch: [88][360/408]	Loss 0.0015 (0.0003)	
training:	Epoch: [88][361/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [88][362/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [88][363/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [88][364/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [88][365/408]	Loss 0.0003 (0.0003)	
training:	Epoch: [88][366/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [88][367/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [88][368/408]	Loss 0.0003 (0.0003)	
training:	Epoch: [88][369/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [88][370/408]	Loss 0.0033 (0.0003)	
training:	Epoch: [88][371/408]	Loss 0.0069 (0.0004)	
training:	Epoch: [88][372/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [88][373/408]	Loss 0.0005 (0.0004)	
training:	Epoch: [88][374/408]	Loss 0.0006 (0.0004)	
training:	Epoch: [88][375/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [88][376/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [88][377/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [88][378/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [88][379/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [88][380/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [88][381/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [88][382/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [88][383/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [88][384/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [88][385/408]	Loss 0.0067 (0.0004)	
training:	Epoch: [88][386/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [88][387/408]	Loss 0.0005 (0.0004)	
training:	Epoch: [88][388/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [88][389/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [88][390/408]	Loss 0.0004 (0.0004)	
training:	Epoch: [88][391/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [88][392/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [88][393/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [88][394/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [88][395/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [88][396/408]	Loss 0.0015 (0.0004)	
training:	Epoch: [88][397/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [88][398/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [88][399/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [88][400/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [88][401/408]	Loss 0.0004 (0.0004)	
training:	Epoch: [88][402/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [88][403/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [88][404/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [88][405/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [88][406/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [88][407/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [88][408/408]	Loss 0.0002 (0.0004)	
Training:	 Loss: 0.0004

Training:	 ACC: 1.0000 1.0000 1.0000 1.0000
Validation:	 ACC: 0.7775 0.7774 0.7748 0.7803
Validation:	 Best_BACC: 0.8226 0.8250 0.8772 0.7679
Validation:	 Loss: 1.4500
Pretraining:	Epoch 89/200
----------
training:	Epoch: [89][1/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [89][2/408]	Loss 0.0007 (0.0004)	
training:	Epoch: [89][3/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [89][4/408]	Loss 0.0003 (0.0003)	
training:	Epoch: [89][5/408]	Loss 0.0006 (0.0004)	
training:	Epoch: [89][6/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [89][7/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [89][8/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [89][9/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [89][10/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [89][11/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [89][12/408]	Loss 0.0016 (0.0004)	
training:	Epoch: [89][13/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [89][14/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [89][15/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [89][16/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [89][17/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [89][18/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [89][19/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [89][20/408]	Loss 0.0019 (0.0004)	
training:	Epoch: [89][21/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [89][22/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [89][23/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [89][24/408]	Loss 0.0004 (0.0004)	
training:	Epoch: [89][25/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [89][26/408]	Loss 0.0004 (0.0004)	
training:	Epoch: [89][27/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [89][28/408]	Loss 0.0005 (0.0004)	
training:	Epoch: [89][29/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [89][30/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [89][31/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [89][32/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [89][33/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [89][34/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [89][35/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [89][36/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [89][37/408]	Loss 0.0005 (0.0003)	
training:	Epoch: [89][38/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [89][39/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [89][40/408]	Loss 0.0003 (0.0003)	
training:	Epoch: [89][41/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [89][42/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [89][43/408]	Loss 0.0005 (0.0003)	
training:	Epoch: [89][44/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [89][45/408]	Loss 0.0003 (0.0003)	
training:	Epoch: [89][46/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [89][47/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [89][48/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [89][49/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [89][50/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [89][51/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [89][52/408]	Loss 0.0003 (0.0003)	
training:	Epoch: [89][53/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [89][54/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [89][55/408]	Loss 0.0003 (0.0003)	
training:	Epoch: [89][56/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [89][57/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [89][58/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [89][59/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [89][60/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [89][61/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [89][62/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [89][63/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [89][64/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [89][65/408]	Loss 0.0265 (0.0007)	
training:	Epoch: [89][66/408]	Loss 0.0002 (0.0007)	
training:	Epoch: [89][67/408]	Loss 0.0002 (0.0007)	
training:	Epoch: [89][68/408]	Loss 0.0002 (0.0007)	
training:	Epoch: [89][69/408]	Loss 0.0007 (0.0007)	
training:	Epoch: [89][70/408]	Loss 0.0004 (0.0007)	
training:	Epoch: [89][71/408]	Loss 0.0008 (0.0007)	
training:	Epoch: [89][72/408]	Loss 0.0002 (0.0007)	
training:	Epoch: [89][73/408]	Loss 0.0003 (0.0006)	
training:	Epoch: [89][74/408]	Loss 0.0002 (0.0006)	
training:	Epoch: [89][75/408]	Loss 0.0002 (0.0006)	
training:	Epoch: [89][76/408]	Loss 0.0013 (0.0006)	
training:	Epoch: [89][77/408]	Loss 0.0002 (0.0006)	
training:	Epoch: [89][78/408]	Loss 0.0002 (0.0006)	
training:	Epoch: [89][79/408]	Loss 0.0002 (0.0006)	
training:	Epoch: [89][80/408]	Loss 0.0002 (0.0006)	
training:	Epoch: [89][81/408]	Loss 0.0002 (0.0006)	
training:	Epoch: [89][82/408]	Loss 0.0002 (0.0006)	
training:	Epoch: [89][83/408]	Loss 0.0002 (0.0006)	
training:	Epoch: [89][84/408]	Loss 0.0003 (0.0006)	
training:	Epoch: [89][85/408]	Loss 0.0002 (0.0006)	
training:	Epoch: [89][86/408]	Loss 0.0009 (0.0006)	
training:	Epoch: [89][87/408]	Loss 0.0002 (0.0006)	
training:	Epoch: [89][88/408]	Loss 0.0003 (0.0006)	
training:	Epoch: [89][89/408]	Loss 0.0002 (0.0006)	
training:	Epoch: [89][90/408]	Loss 0.0002 (0.0006)	
training:	Epoch: [89][91/408]	Loss 0.0002 (0.0006)	
training:	Epoch: [89][92/408]	Loss 0.0002 (0.0006)	
training:	Epoch: [89][93/408]	Loss 0.0002 (0.0006)	
training:	Epoch: [89][94/408]	Loss 0.0002 (0.0006)	
training:	Epoch: [89][95/408]	Loss 0.0003 (0.0006)	
training:	Epoch: [89][96/408]	Loss 0.0003 (0.0006)	
training:	Epoch: [89][97/408]	Loss 0.0002 (0.0006)	
training:	Epoch: [89][98/408]	Loss 0.0006 (0.0006)	
training:	Epoch: [89][99/408]	Loss 0.0002 (0.0006)	
training:	Epoch: [89][100/408]	Loss 0.0002 (0.0005)	
training:	Epoch: [89][101/408]	Loss 0.0001 (0.0005)	
training:	Epoch: [89][102/408]	Loss 0.0002 (0.0005)	
training:	Epoch: [89][103/408]	Loss 0.0002 (0.0005)	
training:	Epoch: [89][104/408]	Loss 0.0002 (0.0005)	
training:	Epoch: [89][105/408]	Loss 0.0002 (0.0005)	
training:	Epoch: [89][106/408]	Loss 0.0002 (0.0005)	
training:	Epoch: [89][107/408]	Loss 0.0003 (0.0005)	
training:	Epoch: [89][108/408]	Loss 0.0002 (0.0005)	
training:	Epoch: [89][109/408]	Loss 0.0002 (0.0005)	
training:	Epoch: [89][110/408]	Loss 0.0002 (0.0005)	
training:	Epoch: [89][111/408]	Loss 0.0002 (0.0005)	
training:	Epoch: [89][112/408]	Loss 0.0002 (0.0005)	
training:	Epoch: [89][113/408]	Loss 0.0003 (0.0005)	
training:	Epoch: [89][114/408]	Loss 0.0002 (0.0005)	
training:	Epoch: [89][115/408]	Loss 0.0002 (0.0005)	
training:	Epoch: [89][116/408]	Loss 0.0002 (0.0005)	
training:	Epoch: [89][117/408]	Loss 0.0002 (0.0005)	
training:	Epoch: [89][118/408]	Loss 0.0002 (0.0005)	
training:	Epoch: [89][119/408]	Loss 0.0002 (0.0005)	
training:	Epoch: [89][120/408]	Loss 0.0009 (0.0005)	
training:	Epoch: [89][121/408]	Loss 0.0002 (0.0005)	
training:	Epoch: [89][122/408]	Loss 0.0002 (0.0005)	
training:	Epoch: [89][123/408]	Loss 0.0002 (0.0005)	
training:	Epoch: [89][124/408]	Loss 0.0002 (0.0005)	
training:	Epoch: [89][125/408]	Loss 0.0002 (0.0005)	
training:	Epoch: [89][126/408]	Loss 0.0002 (0.0005)	
training:	Epoch: [89][127/408]	Loss 0.0002 (0.0005)	
training:	Epoch: [89][128/408]	Loss 0.0003 (0.0005)	
training:	Epoch: [89][129/408]	Loss 0.0007 (0.0005)	
training:	Epoch: [89][130/408]	Loss 0.0002 (0.0005)	
training:	Epoch: [89][131/408]	Loss 0.0004 (0.0005)	
training:	Epoch: [89][132/408]	Loss 0.0002 (0.0005)	
training:	Epoch: [89][133/408]	Loss 0.0002 (0.0005)	
training:	Epoch: [89][134/408]	Loss 0.0002 (0.0005)	
training:	Epoch: [89][135/408]	Loss 0.0002 (0.0005)	
training:	Epoch: [89][136/408]	Loss 0.0008 (0.0005)	
training:	Epoch: [89][137/408]	Loss 0.0002 (0.0005)	
training:	Epoch: [89][138/408]	Loss 0.0002 (0.0005)	
training:	Epoch: [89][139/408]	Loss 0.0002 (0.0005)	
training:	Epoch: [89][140/408]	Loss 0.0002 (0.0005)	
training:	Epoch: [89][141/408]	Loss 0.0002 (0.0005)	
training:	Epoch: [89][142/408]	Loss 0.0002 (0.0005)	
training:	Epoch: [89][143/408]	Loss 0.0002 (0.0005)	
training:	Epoch: [89][144/408]	Loss 0.0002 (0.0005)	
training:	Epoch: [89][145/408]	Loss 0.0008 (0.0005)	
training:	Epoch: [89][146/408]	Loss 0.0002 (0.0005)	
training:	Epoch: [89][147/408]	Loss 0.0002 (0.0005)	
training:	Epoch: [89][148/408]	Loss 0.0002 (0.0005)	
training:	Epoch: [89][149/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [89][150/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [89][151/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [89][152/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [89][153/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [89][154/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [89][155/408]	Loss 0.0007 (0.0004)	
training:	Epoch: [89][156/408]	Loss 0.0004 (0.0004)	
training:	Epoch: [89][157/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [89][158/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [89][159/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [89][160/408]	Loss 0.0007 (0.0004)	
training:	Epoch: [89][161/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [89][162/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [89][163/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [89][164/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [89][165/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [89][166/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [89][167/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [89][168/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [89][169/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [89][170/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [89][171/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [89][172/408]	Loss 0.0004 (0.0004)	
training:	Epoch: [89][173/408]	Loss 0.0008 (0.0004)	
training:	Epoch: [89][174/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [89][175/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [89][176/408]	Loss 0.0005 (0.0004)	
training:	Epoch: [89][177/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [89][178/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [89][179/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [89][180/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [89][181/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [89][182/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [89][183/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [89][184/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [89][185/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [89][186/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [89][187/408]	Loss 0.0004 (0.0004)	
training:	Epoch: [89][188/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [89][189/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [89][190/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [89][191/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [89][192/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [89][193/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [89][194/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [89][195/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [89][196/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [89][197/408]	Loss 0.0005 (0.0004)	
training:	Epoch: [89][198/408]	Loss 0.0005 (0.0004)	
training:	Epoch: [89][199/408]	Loss 0.0004 (0.0004)	
training:	Epoch: [89][200/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [89][201/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [89][202/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [89][203/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [89][204/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [89][205/408]	Loss 0.0004 (0.0004)	
training:	Epoch: [89][206/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [89][207/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [89][208/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [89][209/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [89][210/408]	Loss 0.0005 (0.0004)	
training:	Epoch: [89][211/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [89][212/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [89][213/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [89][214/408]	Loss 0.0005 (0.0004)	
training:	Epoch: [89][215/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [89][216/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [89][217/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [89][218/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [89][219/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [89][220/408]	Loss 0.0004 (0.0004)	
training:	Epoch: [89][221/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [89][222/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [89][223/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [89][224/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [89][225/408]	Loss 0.0006 (0.0004)	
training:	Epoch: [89][226/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [89][227/408]	Loss 0.0004 (0.0004)	
training:	Epoch: [89][228/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [89][229/408]	Loss 0.0006 (0.0004)	
training:	Epoch: [89][230/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [89][231/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [89][232/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [89][233/408]	Loss 0.0004 (0.0004)	
training:	Epoch: [89][234/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [89][235/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [89][236/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [89][237/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [89][238/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [89][239/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [89][240/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [89][241/408]	Loss 0.0007 (0.0004)	
training:	Epoch: [89][242/408]	Loss 0.0006 (0.0004)	
training:	Epoch: [89][243/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [89][244/408]	Loss 0.0001 (0.0004)	
training:	Epoch: [89][245/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [89][246/408]	Loss 0.0008 (0.0004)	
training:	Epoch: [89][247/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [89][248/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [89][249/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [89][250/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [89][251/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [89][252/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [89][253/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [89][254/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [89][255/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [89][256/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [89][257/408]	Loss 0.0004 (0.0004)	
training:	Epoch: [89][258/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [89][259/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [89][260/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [89][261/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [89][262/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [89][263/408]	Loss 0.0038 (0.0004)	
training:	Epoch: [89][264/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [89][265/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [89][266/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [89][267/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [89][268/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [89][269/408]	Loss 0.0004 (0.0004)	
training:	Epoch: [89][270/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [89][271/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [89][272/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [89][273/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [89][274/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [89][275/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [89][276/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [89][277/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [89][278/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [89][279/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [89][280/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [89][281/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [89][282/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [89][283/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [89][284/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [89][285/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [89][286/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [89][287/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [89][288/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [89][289/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [89][290/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [89][291/408]	Loss 0.0001 (0.0004)	
training:	Epoch: [89][292/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [89][293/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [89][294/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [89][295/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [89][296/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [89][297/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [89][298/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [89][299/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [89][300/408]	Loss 0.0136 (0.0004)	
training:	Epoch: [89][301/408]	Loss 0.0005 (0.0004)	
training:	Epoch: [89][302/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [89][303/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [89][304/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [89][305/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [89][306/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [89][307/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [89][308/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [89][309/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [89][310/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [89][311/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [89][312/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [89][313/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [89][314/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [89][315/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [89][316/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [89][317/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [89][318/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [89][319/408]	Loss 0.0004 (0.0004)	
training:	Epoch: [89][320/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [89][321/408]	Loss 0.0007 (0.0004)	
training:	Epoch: [89][322/408]	Loss 0.0009 (0.0004)	
training:	Epoch: [89][323/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [89][324/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [89][325/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [89][326/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [89][327/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [89][328/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [89][329/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [89][330/408]	Loss 0.0006 (0.0004)	
training:	Epoch: [89][331/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [89][332/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [89][333/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [89][334/408]	Loss 0.0005 (0.0004)	
training:	Epoch: [89][335/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [89][336/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [89][337/408]	Loss 0.0001 (0.0004)	
training:	Epoch: [89][338/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [89][339/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [89][340/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [89][341/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [89][342/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [89][343/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [89][344/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [89][345/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [89][346/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [89][347/408]	Loss 0.0004 (0.0004)	
training:	Epoch: [89][348/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [89][349/408]	Loss 0.0006 (0.0004)	
training:	Epoch: [89][350/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [89][351/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [89][352/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [89][353/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [89][354/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [89][355/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [89][356/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [89][357/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [89][358/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [89][359/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [89][360/408]	Loss 0.0004 (0.0004)	
training:	Epoch: [89][361/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [89][362/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [89][363/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [89][364/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [89][365/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [89][366/408]	Loss 0.0020 (0.0004)	
training:	Epoch: [89][367/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [89][368/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [89][369/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [89][370/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [89][371/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [89][372/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [89][373/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [89][374/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [89][375/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [89][376/408]	Loss 0.0006 (0.0004)	
training:	Epoch: [89][377/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [89][378/408]	Loss 0.0007 (0.0004)	
training:	Epoch: [89][379/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [89][380/408]	Loss 0.0036 (0.0004)	
training:	Epoch: [89][381/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [89][382/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [89][383/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [89][384/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [89][385/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [89][386/408]	Loss 0.0006 (0.0004)	
training:	Epoch: [89][387/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [89][388/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [89][389/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [89][390/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [89][391/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [89][392/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [89][393/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [89][394/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [89][395/408]	Loss 0.0004 (0.0004)	
training:	Epoch: [89][396/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [89][397/408]	Loss 0.0004 (0.0004)	
training:	Epoch: [89][398/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [89][399/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [89][400/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [89][401/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [89][402/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [89][403/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [89][404/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [89][405/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [89][406/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [89][407/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [89][408/408]	Loss 0.0002 (0.0004)	
Training:	 Loss: 0.0004

Training:	 ACC: 1.0000 1.0000 1.0000 1.0000
Validation:	 ACC: 0.7746 0.7753 0.7902 0.7590
Validation:	 Best_BACC: 0.8226 0.8250 0.8772 0.7679
Validation:	 Loss: 1.4845
Pretraining:	Epoch 90/200
----------
training:	Epoch: [90][1/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [90][2/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [90][3/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [90][4/408]	Loss 0.0001 (0.0002)	
training:	Epoch: [90][5/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [90][6/408]	Loss 0.0003 (0.0002)	
training:	Epoch: [90][7/408]	Loss 0.0011 (0.0003)	
training:	Epoch: [90][8/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [90][9/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [90][10/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [90][11/408]	Loss 0.0041 (0.0006)	
training:	Epoch: [90][12/408]	Loss 0.0002 (0.0006)	
training:	Epoch: [90][13/408]	Loss 0.0002 (0.0006)	
training:	Epoch: [90][14/408]	Loss 0.0002 (0.0005)	
training:	Epoch: [90][15/408]	Loss 0.0002 (0.0005)	
training:	Epoch: [90][16/408]	Loss 0.0005 (0.0005)	
training:	Epoch: [90][17/408]	Loss 0.0002 (0.0005)	
training:	Epoch: [90][18/408]	Loss 0.0002 (0.0005)	
training:	Epoch: [90][19/408]	Loss 0.0002 (0.0005)	
training:	Epoch: [90][20/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [90][21/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [90][22/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [90][23/408]	Loss 0.0013 (0.0005)	
training:	Epoch: [90][24/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [90][25/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [90][26/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [90][27/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [90][28/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [90][29/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [90][30/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [90][31/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [90][32/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [90][33/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [90][34/408]	Loss 0.0004 (0.0004)	
training:	Epoch: [90][35/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [90][36/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [90][37/408]	Loss 0.0064 (0.0005)	
training:	Epoch: [90][38/408]	Loss 0.0002 (0.0005)	
training:	Epoch: [90][39/408]	Loss 0.0002 (0.0005)	
training:	Epoch: [90][40/408]	Loss 0.0002 (0.0005)	
training:	Epoch: [90][41/408]	Loss 0.0002 (0.0005)	
training:	Epoch: [90][42/408]	Loss 0.0002 (0.0005)	
training:	Epoch: [90][43/408]	Loss 0.0002 (0.0005)	
training:	Epoch: [90][44/408]	Loss 0.0002 (0.0005)	
training:	Epoch: [90][45/408]	Loss 0.0002 (0.0005)	
training:	Epoch: [90][46/408]	Loss 0.0003 (0.0005)	
training:	Epoch: [90][47/408]	Loss 0.0003 (0.0005)	
training:	Epoch: [90][48/408]	Loss 0.0003 (0.0005)	
training:	Epoch: [90][49/408]	Loss 0.0002 (0.0005)	
training:	Epoch: [90][50/408]	Loss 0.0002 (0.0005)	
training:	Epoch: [90][51/408]	Loss 0.0013 (0.0005)	
training:	Epoch: [90][52/408]	Loss 0.0002 (0.0005)	
training:	Epoch: [90][53/408]	Loss 0.0003 (0.0005)	
training:	Epoch: [90][54/408]	Loss 0.0002 (0.0005)	
training:	Epoch: [90][55/408]	Loss 0.0002 (0.0005)	
training:	Epoch: [90][56/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [90][57/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [90][58/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [90][59/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [90][60/408]	Loss 0.0005 (0.0004)	
training:	Epoch: [90][61/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [90][62/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [90][63/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [90][64/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [90][65/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [90][66/408]	Loss 0.0001 (0.0004)	
training:	Epoch: [90][67/408]	Loss 0.0005 (0.0004)	
training:	Epoch: [90][68/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [90][69/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [90][70/408]	Loss 0.0112 (0.0006)	
training:	Epoch: [90][71/408]	Loss 0.0002 (0.0006)	
training:	Epoch: [90][72/408]	Loss 0.0002 (0.0005)	
training:	Epoch: [90][73/408]	Loss 0.0003 (0.0005)	
training:	Epoch: [90][74/408]	Loss 0.0002 (0.0005)	
training:	Epoch: [90][75/408]	Loss 0.0002 (0.0005)	
training:	Epoch: [90][76/408]	Loss 0.0016 (0.0006)	
training:	Epoch: [90][77/408]	Loss 0.0002 (0.0005)	
training:	Epoch: [90][78/408]	Loss 0.0002 (0.0005)	
training:	Epoch: [90][79/408]	Loss 0.0002 (0.0005)	
training:	Epoch: [90][80/408]	Loss 0.0002 (0.0005)	
training:	Epoch: [90][81/408]	Loss 0.0002 (0.0005)	
training:	Epoch: [90][82/408]	Loss 0.0003 (0.0005)	
training:	Epoch: [90][83/408]	Loss 0.0002 (0.0005)	
training:	Epoch: [90][84/408]	Loss 0.0004 (0.0005)	
training:	Epoch: [90][85/408]	Loss 0.0002 (0.0005)	
training:	Epoch: [90][86/408]	Loss 0.0003 (0.0005)	
training:	Epoch: [90][87/408]	Loss 0.0002 (0.0005)	
training:	Epoch: [90][88/408]	Loss 0.0002 (0.0005)	
training:	Epoch: [90][89/408]	Loss 0.0002 (0.0005)	
training:	Epoch: [90][90/408]	Loss 0.0003 (0.0005)	
training:	Epoch: [90][91/408]	Loss 0.0002 (0.0005)	
training:	Epoch: [90][92/408]	Loss 0.0003 (0.0005)	
training:	Epoch: [90][93/408]	Loss 0.0003 (0.0005)	
training:	Epoch: [90][94/408]	Loss 0.0002 (0.0005)	
training:	Epoch: [90][95/408]	Loss 0.0028 (0.0005)	
training:	Epoch: [90][96/408]	Loss 0.0002 (0.0005)	
training:	Epoch: [90][97/408]	Loss 0.0002 (0.0005)	
training:	Epoch: [90][98/408]	Loss 0.0004 (0.0005)	
training:	Epoch: [90][99/408]	Loss 0.0002 (0.0005)	
training:	Epoch: [90][100/408]	Loss 0.0002 (0.0005)	
training:	Epoch: [90][101/408]	Loss 0.0028 (0.0005)	
training:	Epoch: [90][102/408]	Loss 0.0002 (0.0005)	
training:	Epoch: [90][103/408]	Loss 0.0002 (0.0005)	
training:	Epoch: [90][104/408]	Loss 0.0006 (0.0005)	
training:	Epoch: [90][105/408]	Loss 0.0002 (0.0005)	
training:	Epoch: [90][106/408]	Loss 0.0002 (0.0005)	
training:	Epoch: [90][107/408]	Loss 0.0002 (0.0005)	
training:	Epoch: [90][108/408]	Loss 0.0002 (0.0005)	
training:	Epoch: [90][109/408]	Loss 0.0002 (0.0005)	
training:	Epoch: [90][110/408]	Loss 0.0005 (0.0005)	
training:	Epoch: [90][111/408]	Loss 0.0009 (0.0005)	
training:	Epoch: [90][112/408]	Loss 0.0003 (0.0005)	
training:	Epoch: [90][113/408]	Loss 0.0002 (0.0005)	
training:	Epoch: [90][114/408]	Loss 0.0001 (0.0005)	
training:	Epoch: [90][115/408]	Loss 0.0002 (0.0005)	
training:	Epoch: [90][116/408]	Loss 0.0002 (0.0005)	
training:	Epoch: [90][117/408]	Loss 0.0004 (0.0005)	
training:	Epoch: [90][118/408]	Loss 0.0013 (0.0005)	
training:	Epoch: [90][119/408]	Loss 0.0003 (0.0005)	
training:	Epoch: [90][120/408]	Loss 0.0002 (0.0005)	
training:	Epoch: [90][121/408]	Loss 0.0011 (0.0005)	
training:	Epoch: [90][122/408]	Loss 0.0019 (0.0005)	
training:	Epoch: [90][123/408]	Loss 0.0002 (0.0005)	
training:	Epoch: [90][124/408]	Loss 0.0002 (0.0005)	
training:	Epoch: [90][125/408]	Loss 0.0002 (0.0005)	
training:	Epoch: [90][126/408]	Loss 0.0002 (0.0005)	
training:	Epoch: [90][127/408]	Loss 0.0002 (0.0005)	
training:	Epoch: [90][128/408]	Loss 0.0003 (0.0005)	
training:	Epoch: [90][129/408]	Loss 0.0004 (0.0005)	
training:	Epoch: [90][130/408]	Loss 0.0002 (0.0005)	
training:	Epoch: [90][131/408]	Loss 0.0002 (0.0005)	
training:	Epoch: [90][132/408]	Loss 0.0002 (0.0005)	
training:	Epoch: [90][133/408]	Loss 0.0002 (0.0005)	
training:	Epoch: [90][134/408]	Loss 0.0002 (0.0005)	
training:	Epoch: [90][135/408]	Loss 0.0003 (0.0005)	
training:	Epoch: [90][136/408]	Loss 0.0002 (0.0005)	
training:	Epoch: [90][137/408]	Loss 0.0002 (0.0005)	
training:	Epoch: [90][138/408]	Loss 0.0003 (0.0005)	
training:	Epoch: [90][139/408]	Loss 0.0002 (0.0005)	
training:	Epoch: [90][140/408]	Loss 0.0002 (0.0005)	
training:	Epoch: [90][141/408]	Loss 0.0002 (0.0005)	
training:	Epoch: [90][142/408]	Loss 0.0012 (0.0005)	
training:	Epoch: [90][143/408]	Loss 0.0002 (0.0005)	
training:	Epoch: [90][144/408]	Loss 0.0021 (0.0005)	
training:	Epoch: [90][145/408]	Loss 0.0002 (0.0005)	
training:	Epoch: [90][146/408]	Loss 0.0002 (0.0005)	
training:	Epoch: [90][147/408]	Loss 0.0002 (0.0005)	
training:	Epoch: [90][148/408]	Loss 0.0002 (0.0005)	
training:	Epoch: [90][149/408]	Loss 0.0002 (0.0005)	
training:	Epoch: [90][150/408]	Loss 0.0002 (0.0005)	
training:	Epoch: [90][151/408]	Loss 0.0005 (0.0005)	
training:	Epoch: [90][152/408]	Loss 0.0002 (0.0005)	
training:	Epoch: [90][153/408]	Loss 0.0002 (0.0005)	
training:	Epoch: [90][154/408]	Loss 0.0002 (0.0005)	
training:	Epoch: [90][155/408]	Loss 0.0002 (0.0005)	
training:	Epoch: [90][156/408]	Loss 0.0002 (0.0005)	
training:	Epoch: [90][157/408]	Loss 0.0002 (0.0005)	
training:	Epoch: [90][158/408]	Loss 0.0002 (0.0005)	
training:	Epoch: [90][159/408]	Loss 0.0002 (0.0005)	
training:	Epoch: [90][160/408]	Loss 0.0002 (0.0005)	
training:	Epoch: [90][161/408]	Loss 0.0002 (0.0005)	
training:	Epoch: [90][162/408]	Loss 0.0002 (0.0005)	
training:	Epoch: [90][163/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [90][164/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [90][165/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [90][166/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [90][167/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [90][168/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [90][169/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [90][170/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [90][171/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [90][172/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [90][173/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [90][174/408]	Loss 0.0001 (0.0004)	
training:	Epoch: [90][175/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [90][176/408]	Loss 0.0005 (0.0004)	
training:	Epoch: [90][177/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [90][178/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [90][179/408]	Loss 0.0001 (0.0004)	
training:	Epoch: [90][180/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [90][181/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [90][182/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [90][183/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [90][184/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [90][185/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [90][186/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [90][187/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [90][188/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [90][189/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [90][190/408]	Loss 0.0001 (0.0004)	
training:	Epoch: [90][191/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [90][192/408]	Loss 0.0001 (0.0004)	
training:	Epoch: [90][193/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [90][194/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [90][195/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [90][196/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [90][197/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [90][198/408]	Loss 0.0001 (0.0004)	
training:	Epoch: [90][199/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [90][200/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [90][201/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [90][202/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [90][203/408]	Loss 0.0001 (0.0004)	
training:	Epoch: [90][204/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [90][205/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [90][206/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [90][207/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [90][208/408]	Loss 0.0001 (0.0004)	
training:	Epoch: [90][209/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [90][210/408]	Loss 0.0001 (0.0004)	
training:	Epoch: [90][211/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [90][212/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [90][213/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [90][214/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [90][215/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [90][216/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [90][217/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [90][218/408]	Loss 0.0004 (0.0004)	
training:	Epoch: [90][219/408]	Loss 0.0005 (0.0004)	
training:	Epoch: [90][220/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [90][221/408]	Loss 0.0004 (0.0004)	
training:	Epoch: [90][222/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [90][223/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [90][224/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [90][225/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [90][226/408]	Loss 0.0004 (0.0004)	
training:	Epoch: [90][227/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [90][228/408]	Loss 0.0005 (0.0004)	
training:	Epoch: [90][229/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [90][230/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [90][231/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [90][232/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [90][233/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [90][234/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [90][235/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [90][236/408]	Loss 0.0004 (0.0004)	
training:	Epoch: [90][237/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [90][238/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [90][239/408]	Loss 0.0004 (0.0004)	
training:	Epoch: [90][240/408]	Loss 0.0004 (0.0004)	
training:	Epoch: [90][241/408]	Loss 0.0007 (0.0004)	
training:	Epoch: [90][242/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [90][243/408]	Loss 0.0005 (0.0004)	
training:	Epoch: [90][244/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [90][245/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [90][246/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [90][247/408]	Loss 0.0004 (0.0004)	
training:	Epoch: [90][248/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [90][249/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [90][250/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [90][251/408]	Loss 0.0006 (0.0004)	
training:	Epoch: [90][252/408]	Loss 0.0006 (0.0004)	
training:	Epoch: [90][253/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [90][254/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [90][255/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [90][256/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [90][257/408]	Loss 0.0004 (0.0004)	
training:	Epoch: [90][258/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [90][259/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [90][260/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [90][261/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [90][262/408]	Loss 0.0001 (0.0004)	
training:	Epoch: [90][263/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [90][264/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [90][265/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [90][266/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [90][267/408]	Loss 0.0005 (0.0004)	
training:	Epoch: [90][268/408]	Loss 0.0004 (0.0004)	
training:	Epoch: [90][269/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [90][270/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [90][271/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [90][272/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [90][273/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [90][274/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [90][275/408]	Loss 0.0001 (0.0004)	
training:	Epoch: [90][276/408]	Loss 0.0001 (0.0004)	
training:	Epoch: [90][277/408]	Loss 0.0001 (0.0004)	
training:	Epoch: [90][278/408]	Loss 0.0005 (0.0004)	
training:	Epoch: [90][279/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [90][280/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [90][281/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [90][282/408]	Loss 0.0004 (0.0004)	
training:	Epoch: [90][283/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [90][284/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [90][285/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [90][286/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [90][287/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [90][288/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [90][289/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [90][290/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [90][291/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [90][292/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [90][293/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [90][294/408]	Loss 0.0013 (0.0004)	
training:	Epoch: [90][295/408]	Loss 0.0001 (0.0004)	
training:	Epoch: [90][296/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [90][297/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [90][298/408]	Loss 0.0033 (0.0004)	
training:	Epoch: [90][299/408]	Loss 0.0022 (0.0004)	
training:	Epoch: [90][300/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [90][301/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [90][302/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [90][303/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [90][304/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [90][305/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [90][306/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [90][307/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [90][308/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [90][309/408]	Loss 0.0001 (0.0004)	
training:	Epoch: [90][310/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [90][311/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [90][312/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [90][313/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [90][314/408]	Loss 0.0001 (0.0004)	
training:	Epoch: [90][315/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [90][316/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [90][317/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [90][318/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [90][319/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [90][320/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [90][321/408]	Loss 0.0004 (0.0004)	
training:	Epoch: [90][322/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [90][323/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [90][324/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [90][325/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [90][326/408]	Loss 0.0001 (0.0004)	
training:	Epoch: [90][327/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [90][328/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [90][329/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [90][330/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [90][331/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [90][332/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [90][333/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [90][334/408]	Loss 0.0004 (0.0004)	
training:	Epoch: [90][335/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [90][336/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [90][337/408]	Loss 0.0001 (0.0003)	
training:	Epoch: [90][338/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [90][339/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [90][340/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [90][341/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [90][342/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [90][343/408]	Loss 0.0003 (0.0003)	
training:	Epoch: [90][344/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [90][345/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [90][346/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [90][347/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [90][348/408]	Loss 0.0001 (0.0003)	
training:	Epoch: [90][349/408]	Loss 0.0003 (0.0003)	
training:	Epoch: [90][350/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [90][351/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [90][352/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [90][353/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [90][354/408]	Loss 0.0024 (0.0003)	
training:	Epoch: [90][355/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [90][356/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [90][357/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [90][358/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [90][359/408]	Loss 0.0008 (0.0003)	
training:	Epoch: [90][360/408]	Loss 0.0026 (0.0004)	
training:	Epoch: [90][361/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [90][362/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [90][363/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [90][364/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [90][365/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [90][366/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [90][367/408]	Loss 0.0004 (0.0004)	
training:	Epoch: [90][368/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [90][369/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [90][370/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [90][371/408]	Loss 0.0001 (0.0003)	
training:	Epoch: [90][372/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [90][373/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [90][374/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [90][375/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [90][376/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [90][377/408]	Loss 0.0003 (0.0003)	
training:	Epoch: [90][378/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [90][379/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [90][380/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [90][381/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [90][382/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [90][383/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [90][384/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [90][385/408]	Loss 0.0004 (0.0003)	
training:	Epoch: [90][386/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [90][387/408]	Loss 0.0001 (0.0003)	
training:	Epoch: [90][388/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [90][389/408]	Loss 0.0005 (0.0003)	
training:	Epoch: [90][390/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [90][391/408]	Loss 0.0001 (0.0003)	
training:	Epoch: [90][392/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [90][393/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [90][394/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [90][395/408]	Loss 0.0001 (0.0003)	
training:	Epoch: [90][396/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [90][397/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [90][398/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [90][399/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [90][400/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [90][401/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [90][402/408]	Loss 0.0001 (0.0003)	
training:	Epoch: [90][403/408]	Loss 0.0001 (0.0003)	
training:	Epoch: [90][404/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [90][405/408]	Loss 0.0004 (0.0003)	
training:	Epoch: [90][406/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [90][407/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [90][408/408]	Loss 0.0002 (0.0003)	
Training:	 Loss: 0.0003

Training:	 ACC: 1.0000 1.0000 1.0000 1.0000
Validation:	 ACC: 0.7694 0.7694 0.7697 0.7691
Validation:	 Best_BACC: 0.8226 0.8250 0.8772 0.7679
Validation:	 Loss: 1.5196
Pretraining:	Epoch 91/200
----------
training:	Epoch: [91][1/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [91][2/408]	Loss 0.0003 (0.0002)	
training:	Epoch: [91][3/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [91][4/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [91][5/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [91][6/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [91][7/408]	Loss 0.0004 (0.0002)	
training:	Epoch: [91][8/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [91][9/408]	Loss 0.0047 (0.0007)	
training:	Epoch: [91][10/408]	Loss 0.0001 (0.0007)	
training:	Epoch: [91][11/408]	Loss 0.0003 (0.0006)	
training:	Epoch: [91][12/408]	Loss 0.0002 (0.0006)	
training:	Epoch: [91][13/408]	Loss 0.0004 (0.0006)	
training:	Epoch: [91][14/408]	Loss 0.0003 (0.0006)	
training:	Epoch: [91][15/408]	Loss 0.0001 (0.0005)	
training:	Epoch: [91][16/408]	Loss 0.0002 (0.0005)	
training:	Epoch: [91][17/408]	Loss 0.0002 (0.0005)	
training:	Epoch: [91][18/408]	Loss 0.0002 (0.0005)	
training:	Epoch: [91][19/408]	Loss 0.0004 (0.0005)	
training:	Epoch: [91][20/408]	Loss 0.0002 (0.0005)	
training:	Epoch: [91][21/408]	Loss 0.0001 (0.0004)	
training:	Epoch: [91][22/408]	Loss 0.0007 (0.0005)	
training:	Epoch: [91][23/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [91][24/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [91][25/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [91][26/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [91][27/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [91][28/408]	Loss 0.0007 (0.0004)	
training:	Epoch: [91][29/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [91][30/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [91][31/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [91][32/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [91][33/408]	Loss 0.0001 (0.0004)	
training:	Epoch: [91][34/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [91][35/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [91][36/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [91][37/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [91][38/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [91][39/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [91][40/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [91][41/408]	Loss 0.0001 (0.0003)	
training:	Epoch: [91][42/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [91][43/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [91][44/408]	Loss 0.0004 (0.0003)	
training:	Epoch: [91][45/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [91][46/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [91][47/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [91][48/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [91][49/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [91][50/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [91][51/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [91][52/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [91][53/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [91][54/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [91][55/408]	Loss 0.0001 (0.0003)	
training:	Epoch: [91][56/408]	Loss 0.0001 (0.0003)	
training:	Epoch: [91][57/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [91][58/408]	Loss 0.0001 (0.0003)	
training:	Epoch: [91][59/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [91][60/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [91][61/408]	Loss 0.0003 (0.0003)	
training:	Epoch: [91][62/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [91][63/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [91][64/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [91][65/408]	Loss 0.0001 (0.0003)	
training:	Epoch: [91][66/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [91][67/408]	Loss 0.0003 (0.0003)	
training:	Epoch: [91][68/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [91][69/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [91][70/408]	Loss 0.0001 (0.0003)	
training:	Epoch: [91][71/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [91][72/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [91][73/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [91][74/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [91][75/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [91][76/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [91][77/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [91][78/408]	Loss 0.0003 (0.0003)	
training:	Epoch: [91][79/408]	Loss 0.0001 (0.0003)	
training:	Epoch: [91][80/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [91][81/408]	Loss 0.0004 (0.0003)	
training:	Epoch: [91][82/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [91][83/408]	Loss 0.0172 (0.0005)	
training:	Epoch: [91][84/408]	Loss 0.0002 (0.0005)	
training:	Epoch: [91][85/408]	Loss 0.0002 (0.0005)	
training:	Epoch: [91][86/408]	Loss 0.0002 (0.0005)	
training:	Epoch: [91][87/408]	Loss 0.0002 (0.0005)	
training:	Epoch: [91][88/408]	Loss 0.0020 (0.0005)	
training:	Epoch: [91][89/408]	Loss 0.0001 (0.0005)	
training:	Epoch: [91][90/408]	Loss 0.0002 (0.0005)	
training:	Epoch: [91][91/408]	Loss 0.0002 (0.0005)	
training:	Epoch: [91][92/408]	Loss 0.0002 (0.0005)	
training:	Epoch: [91][93/408]	Loss 0.0004 (0.0005)	
training:	Epoch: [91][94/408]	Loss 0.0005 (0.0005)	
training:	Epoch: [91][95/408]	Loss 0.0003 (0.0005)	
training:	Epoch: [91][96/408]	Loss 0.0002 (0.0005)	
training:	Epoch: [91][97/408]	Loss 0.0002 (0.0005)	
training:	Epoch: [91][98/408]	Loss 0.0001 (0.0004)	
training:	Epoch: [91][99/408]	Loss 0.0007 (0.0005)	
training:	Epoch: [91][100/408]	Loss 0.0003 (0.0005)	
training:	Epoch: [91][101/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [91][102/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [91][103/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [91][104/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [91][105/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [91][106/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [91][107/408]	Loss 0.0001 (0.0004)	
training:	Epoch: [91][108/408]	Loss 0.0001 (0.0004)	
training:	Epoch: [91][109/408]	Loss 0.0001 (0.0004)	
training:	Epoch: [91][110/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [91][111/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [91][112/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [91][113/408]	Loss 0.0007 (0.0004)	
training:	Epoch: [91][114/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [91][115/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [91][116/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [91][117/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [91][118/408]	Loss 0.0001 (0.0004)	
training:	Epoch: [91][119/408]	Loss 0.0001 (0.0004)	
training:	Epoch: [91][120/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [91][121/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [91][122/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [91][123/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [91][124/408]	Loss 0.0001 (0.0004)	
training:	Epoch: [91][125/408]	Loss 0.0004 (0.0004)	
training:	Epoch: [91][126/408]	Loss 0.0010 (0.0004)	
training:	Epoch: [91][127/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [91][128/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [91][129/408]	Loss 0.0009 (0.0004)	
training:	Epoch: [91][130/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [91][131/408]	Loss 0.0012 (0.0004)	
training:	Epoch: [91][132/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [91][133/408]	Loss 0.0053 (0.0005)	
training:	Epoch: [91][134/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [91][135/408]	Loss 0.0001 (0.0004)	
training:	Epoch: [91][136/408]	Loss 0.0001 (0.0004)	
training:	Epoch: [91][137/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [91][138/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [91][139/408]	Loss 0.0007 (0.0004)	
training:	Epoch: [91][140/408]	Loss 0.0008 (0.0004)	
training:	Epoch: [91][141/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [91][142/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [91][143/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [91][144/408]	Loss 0.0001 (0.0004)	
training:	Epoch: [91][145/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [91][146/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [91][147/408]	Loss 0.0006 (0.0004)	
training:	Epoch: [91][148/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [91][149/408]	Loss 0.0001 (0.0004)	
training:	Epoch: [91][150/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [91][151/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [91][152/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [91][153/408]	Loss 0.0014 (0.0004)	
training:	Epoch: [91][154/408]	Loss 0.0001 (0.0004)	
training:	Epoch: [91][155/408]	Loss 0.0001 (0.0004)	
training:	Epoch: [91][156/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [91][157/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [91][158/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [91][159/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [91][160/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [91][161/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [91][162/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [91][163/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [91][164/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [91][165/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [91][166/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [91][167/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [91][168/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [91][169/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [91][170/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [91][171/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [91][172/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [91][173/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [91][174/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [91][175/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [91][176/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [91][177/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [91][178/408]	Loss 0.0005 (0.0004)	
training:	Epoch: [91][179/408]	Loss 0.0006 (0.0004)	
training:	Epoch: [91][180/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [91][181/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [91][182/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [91][183/408]	Loss 0.0001 (0.0004)	
training:	Epoch: [91][184/408]	Loss 0.0005 (0.0004)	
training:	Epoch: [91][185/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [91][186/408]	Loss 0.0004 (0.0004)	
training:	Epoch: [91][187/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [91][188/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [91][189/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [91][190/408]	Loss 0.0001 (0.0004)	
training:	Epoch: [91][191/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [91][192/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [91][193/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [91][194/408]	Loss 0.0001 (0.0004)	
training:	Epoch: [91][195/408]	Loss 0.0001 (0.0004)	
training:	Epoch: [91][196/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [91][197/408]	Loss 0.0001 (0.0004)	
training:	Epoch: [91][198/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [91][199/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [91][200/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [91][201/408]	Loss 0.0001 (0.0004)	
training:	Epoch: [91][202/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [91][203/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [91][204/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [91][205/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [91][206/408]	Loss 0.0001 (0.0004)	
training:	Epoch: [91][207/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [91][208/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [91][209/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [91][210/408]	Loss 0.0001 (0.0004)	
training:	Epoch: [91][211/408]	Loss 0.0007 (0.0004)	
training:	Epoch: [91][212/408]	Loss 0.0001 (0.0004)	
training:	Epoch: [91][213/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [91][214/408]	Loss 0.0001 (0.0004)	
training:	Epoch: [91][215/408]	Loss 0.0004 (0.0004)	
training:	Epoch: [91][216/408]	Loss 0.0004 (0.0004)	
training:	Epoch: [91][217/408]	Loss 0.0001 (0.0004)	
training:	Epoch: [91][218/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [91][219/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [91][220/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [91][221/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [91][222/408]	Loss 0.0004 (0.0004)	
training:	Epoch: [91][223/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [91][224/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [91][225/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [91][226/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [91][227/408]	Loss 0.0007 (0.0004)	
training:	Epoch: [91][228/408]	Loss 0.0001 (0.0004)	
training:	Epoch: [91][229/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [91][230/408]	Loss 0.0001 (0.0004)	
training:	Epoch: [91][231/408]	Loss 0.0001 (0.0004)	
training:	Epoch: [91][232/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [91][233/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [91][234/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [91][235/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [91][236/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [91][237/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [91][238/408]	Loss 0.0004 (0.0004)	
training:	Epoch: [91][239/408]	Loss 0.0001 (0.0004)	
training:	Epoch: [91][240/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [91][241/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [91][242/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [91][243/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [91][244/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [91][245/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [91][246/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [91][247/408]	Loss 0.0001 (0.0004)	
training:	Epoch: [91][248/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [91][249/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [91][250/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [91][251/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [91][252/408]	Loss 0.0001 (0.0003)	
training:	Epoch: [91][253/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [91][254/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [91][255/408]	Loss 0.0001 (0.0003)	
training:	Epoch: [91][256/408]	Loss 0.0001 (0.0003)	
training:	Epoch: [91][257/408]	Loss 0.0001 (0.0003)	
training:	Epoch: [91][258/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [91][259/408]	Loss 0.0005 (0.0003)	
training:	Epoch: [91][260/408]	Loss 0.0001 (0.0003)	
training:	Epoch: [91][261/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [91][262/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [91][263/408]	Loss 0.0003 (0.0003)	
training:	Epoch: [91][264/408]	Loss 0.0004 (0.0003)	
training:	Epoch: [91][265/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [91][266/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [91][267/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [91][268/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [91][269/408]	Loss 0.0001 (0.0003)	
training:	Epoch: [91][270/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [91][271/408]	Loss 0.0004 (0.0003)	
training:	Epoch: [91][272/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [91][273/408]	Loss 0.0007 (0.0003)	
training:	Epoch: [91][274/408]	Loss 0.0001 (0.0003)	
training:	Epoch: [91][275/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [91][276/408]	Loss 0.0003 (0.0003)	
training:	Epoch: [91][277/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [91][278/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [91][279/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [91][280/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [91][281/408]	Loss 0.0003 (0.0003)	
training:	Epoch: [91][282/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [91][283/408]	Loss 0.0003 (0.0003)	
training:	Epoch: [91][284/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [91][285/408]	Loss 0.0004 (0.0003)	
training:	Epoch: [91][286/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [91][287/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [91][288/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [91][289/408]	Loss 0.0001 (0.0003)	
training:	Epoch: [91][290/408]	Loss 0.0087 (0.0004)	
training:	Epoch: [91][291/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [91][292/408]	Loss 0.0001 (0.0004)	
training:	Epoch: [91][293/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [91][294/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [91][295/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [91][296/408]	Loss 0.0001 (0.0004)	
training:	Epoch: [91][297/408]	Loss 0.0001 (0.0004)	
training:	Epoch: [91][298/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [91][299/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [91][300/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [91][301/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [91][302/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [91][303/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [91][304/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [91][305/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [91][306/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [91][307/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [91][308/408]	Loss 0.0009 (0.0004)	
training:	Epoch: [91][309/408]	Loss 0.0003 (0.0004)	
training:	Epoch: [91][310/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [91][311/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [91][312/408]	Loss 0.0001 (0.0004)	
training:	Epoch: [91][313/408]	Loss 0.0003 (0.0003)	
training:	Epoch: [91][314/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [91][315/408]	Loss 0.0003 (0.0003)	
training:	Epoch: [91][316/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [91][317/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [91][318/408]	Loss 0.0003 (0.0003)	
training:	Epoch: [91][319/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [91][320/408]	Loss 0.0003 (0.0003)	
training:	Epoch: [91][321/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [91][322/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [91][323/408]	Loss 0.0005 (0.0003)	
training:	Epoch: [91][324/408]	Loss 0.0001 (0.0003)	
training:	Epoch: [91][325/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [91][326/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [91][327/408]	Loss 0.0009 (0.0003)	
training:	Epoch: [91][328/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [91][329/408]	Loss 0.0003 (0.0003)	
training:	Epoch: [91][330/408]	Loss 0.0001 (0.0003)	
training:	Epoch: [91][331/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [91][332/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [91][333/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [91][334/408]	Loss 0.0003 (0.0003)	
training:	Epoch: [91][335/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [91][336/408]	Loss 0.0012 (0.0003)	
training:	Epoch: [91][337/408]	Loss 0.0005 (0.0003)	
training:	Epoch: [91][338/408]	Loss 0.0001 (0.0003)	
training:	Epoch: [91][339/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [91][340/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [91][341/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [91][342/408]	Loss 0.0004 (0.0003)	
training:	Epoch: [91][343/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [91][344/408]	Loss 0.0001 (0.0003)	
training:	Epoch: [91][345/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [91][346/408]	Loss 0.0001 (0.0003)	
training:	Epoch: [91][347/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [91][348/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [91][349/408]	Loss 0.0001 (0.0003)	
training:	Epoch: [91][350/408]	Loss 0.0001 (0.0003)	
training:	Epoch: [91][351/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [91][352/408]	Loss 0.0001 (0.0003)	
training:	Epoch: [91][353/408]	Loss 0.0003 (0.0003)	
training:	Epoch: [91][354/408]	Loss 0.0001 (0.0003)	
training:	Epoch: [91][355/408]	Loss 0.0006 (0.0003)	
training:	Epoch: [91][356/408]	Loss 0.0001 (0.0003)	
training:	Epoch: [91][357/408]	Loss 0.0004 (0.0003)	
training:	Epoch: [91][358/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [91][359/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [91][360/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [91][361/408]	Loss 0.0001 (0.0003)	
training:	Epoch: [91][362/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [91][363/408]	Loss 0.0001 (0.0003)	
training:	Epoch: [91][364/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [91][365/408]	Loss 0.0001 (0.0003)	
training:	Epoch: [91][366/408]	Loss 0.0003 (0.0003)	
training:	Epoch: [91][367/408]	Loss 0.0001 (0.0003)	
training:	Epoch: [91][368/408]	Loss 0.0001 (0.0003)	
training:	Epoch: [91][369/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [91][370/408]	Loss 0.0001 (0.0003)	
training:	Epoch: [91][371/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [91][372/408]	Loss 0.0007 (0.0003)	
training:	Epoch: [91][373/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [91][374/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [91][375/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [91][376/408]	Loss 0.0001 (0.0003)	
training:	Epoch: [91][377/408]	Loss 0.0003 (0.0003)	
training:	Epoch: [91][378/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [91][379/408]	Loss 0.0004 (0.0003)	
training:	Epoch: [91][380/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [91][381/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [91][382/408]	Loss 0.0001 (0.0003)	
training:	Epoch: [91][383/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [91][384/408]	Loss 0.0001 (0.0003)	
training:	Epoch: [91][385/408]	Loss 0.0001 (0.0003)	
training:	Epoch: [91][386/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [91][387/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [91][388/408]	Loss 0.0001 (0.0003)	
training:	Epoch: [91][389/408]	Loss 0.0003 (0.0003)	
training:	Epoch: [91][390/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [91][391/408]	Loss 0.0001 (0.0003)	
training:	Epoch: [91][392/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [91][393/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [91][394/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [91][395/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [91][396/408]	Loss 0.0003 (0.0003)	
training:	Epoch: [91][397/408]	Loss 0.0005 (0.0003)	
training:	Epoch: [91][398/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [91][399/408]	Loss 0.0001 (0.0003)	
training:	Epoch: [91][400/408]	Loss 0.0001 (0.0003)	
training:	Epoch: [91][401/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [91][402/408]	Loss 0.0003 (0.0003)	
training:	Epoch: [91][403/408]	Loss 0.0001 (0.0003)	
training:	Epoch: [91][404/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [91][405/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [91][406/408]	Loss 0.0001 (0.0003)	
training:	Epoch: [91][407/408]	Loss 0.0001 (0.0003)	
training:	Epoch: [91][408/408]	Loss 0.0014 (0.0003)	
Training:	 Loss: 0.0003

Training:	 ACC: 1.0000 1.0000 1.0000 1.0000
Validation:	 ACC: 0.7730 0.7726 0.7636 0.7825
Validation:	 Best_BACC: 0.8226 0.8250 0.8772 0.7679
Validation:	 Loss: 1.5613
Pretraining:	Epoch 92/200
----------
training:	Epoch: [92][1/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [92][2/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [92][3/408]	Loss 0.0001 (0.0002)	
training:	Epoch: [92][4/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [92][5/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [92][6/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [92][7/408]	Loss 0.0004 (0.0002)	
training:	Epoch: [92][8/408]	Loss 0.0001 (0.0002)	
training:	Epoch: [92][9/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [92][10/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [92][11/408]	Loss 0.0001 (0.0002)	
training:	Epoch: [92][12/408]	Loss 0.0001 (0.0002)	
training:	Epoch: [92][13/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [92][14/408]	Loss 0.0001 (0.0002)	
training:	Epoch: [92][15/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [92][16/408]	Loss 0.0001 (0.0002)	
training:	Epoch: [92][17/408]	Loss 0.0005 (0.0002)	
training:	Epoch: [92][18/408]	Loss 0.0001 (0.0002)	
training:	Epoch: [92][19/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [92][20/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [92][21/408]	Loss 0.0001 (0.0002)	
training:	Epoch: [92][22/408]	Loss 0.0001 (0.0002)	
training:	Epoch: [92][23/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [92][24/408]	Loss 0.0001 (0.0002)	
training:	Epoch: [92][25/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [92][26/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [92][27/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [92][28/408]	Loss 0.0004 (0.0002)	
training:	Epoch: [92][29/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [92][30/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [92][31/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [92][32/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [92][33/408]	Loss 0.0001 (0.0002)	
training:	Epoch: [92][34/408]	Loss 0.0001 (0.0002)	
training:	Epoch: [92][35/408]	Loss 0.0001 (0.0002)	
training:	Epoch: [92][36/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [92][37/408]	Loss 0.0001 (0.0002)	
training:	Epoch: [92][38/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [92][39/408]	Loss 0.0006 (0.0002)	
training:	Epoch: [92][40/408]	Loss 0.0001 (0.0002)	
training:	Epoch: [92][41/408]	Loss 0.0001 (0.0002)	
training:	Epoch: [92][42/408]	Loss 0.0001 (0.0002)	
training:	Epoch: [92][43/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [92][44/408]	Loss 0.0001 (0.0002)	
training:	Epoch: [92][45/408]	Loss 0.0001 (0.0002)	
training:	Epoch: [92][46/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [92][47/408]	Loss 0.0001 (0.0002)	
training:	Epoch: [92][48/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [92][49/408]	Loss 0.0001 (0.0002)	
training:	Epoch: [92][50/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [92][51/408]	Loss 0.0001 (0.0002)	
training:	Epoch: [92][52/408]	Loss 0.0004 (0.0002)	
training:	Epoch: [92][53/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [92][54/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [92][55/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [92][56/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [92][57/408]	Loss 0.0001 (0.0002)	
training:	Epoch: [92][58/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [92][59/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [92][60/408]	Loss 0.0001 (0.0002)	
training:	Epoch: [92][61/408]	Loss 0.0001 (0.0002)	
training:	Epoch: [92][62/408]	Loss 0.0012 (0.0002)	
training:	Epoch: [92][63/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [92][64/408]	Loss 0.0008 (0.0002)	
training:	Epoch: [92][65/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [92][66/408]	Loss 0.0004 (0.0002)	
training:	Epoch: [92][67/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [92][68/408]	Loss 0.0001 (0.0002)	
training:	Epoch: [92][69/408]	Loss 0.0001 (0.0002)	
training:	Epoch: [92][70/408]	Loss 0.0001 (0.0002)	
training:	Epoch: [92][71/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [92][72/408]	Loss 0.0027 (0.0002)	
training:	Epoch: [92][73/408]	Loss 0.0001 (0.0002)	
training:	Epoch: [92][74/408]	Loss 0.0003 (0.0002)	
training:	Epoch: [92][75/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [92][76/408]	Loss 0.0001 (0.0002)	
training:	Epoch: [92][77/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [92][78/408]	Loss 0.0003 (0.0002)	
training:	Epoch: [92][79/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [92][80/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [92][81/408]	Loss 0.0004 (0.0002)	
training:	Epoch: [92][82/408]	Loss 0.0001 (0.0002)	
training:	Epoch: [92][83/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [92][84/408]	Loss 0.0001 (0.0002)	
training:	Epoch: [92][85/408]	Loss 0.0001 (0.0002)	
training:	Epoch: [92][86/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [92][87/408]	Loss 0.0004 (0.0002)	
training:	Epoch: [92][88/408]	Loss 0.0001 (0.0002)	
training:	Epoch: [92][89/408]	Loss 0.0001 (0.0002)	
training:	Epoch: [92][90/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [92][91/408]	Loss 0.0001 (0.0002)	
training:	Epoch: [92][92/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [92][93/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [92][94/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [92][95/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [92][96/408]	Loss 0.0001 (0.0002)	
training:	Epoch: [92][97/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [92][98/408]	Loss 0.0001 (0.0002)	
training:	Epoch: [92][99/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [92][100/408]	Loss 0.0011 (0.0002)	
training:	Epoch: [92][101/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [92][102/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [92][103/408]	Loss 0.0001 (0.0002)	
training:	Epoch: [92][104/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [92][105/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [92][106/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [92][107/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [92][108/408]	Loss 0.0003 (0.0002)	
training:	Epoch: [92][109/408]	Loss 0.0003 (0.0002)	
training:	Epoch: [92][110/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [92][111/408]	Loss 0.0003 (0.0002)	
training:	Epoch: [92][112/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [92][113/408]	Loss 0.0001 (0.0002)	
training:	Epoch: [92][114/408]	Loss 0.0001 (0.0002)	
training:	Epoch: [92][115/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [92][116/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [92][117/408]	Loss 0.0001 (0.0002)	
training:	Epoch: [92][118/408]	Loss 0.0001 (0.0002)	
training:	Epoch: [92][119/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [92][120/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [92][121/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [92][122/408]	Loss 0.0001 (0.0002)	
training:	Epoch: [92][123/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [92][124/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [92][125/408]	Loss 0.0004 (0.0002)	
training:	Epoch: [92][126/408]	Loss 0.0001 (0.0002)	
training:	Epoch: [92][127/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [92][128/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [92][129/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [92][130/408]	Loss 0.0001 (0.0002)	
training:	Epoch: [92][131/408]	Loss 0.0001 (0.0002)	
training:	Epoch: [92][132/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [92][133/408]	Loss 0.0004 (0.0002)	
training:	Epoch: [92][134/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [92][135/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [92][136/408]	Loss 0.0001 (0.0002)	
training:	Epoch: [92][137/408]	Loss 0.0001 (0.0002)	
training:	Epoch: [92][138/408]	Loss 0.0001 (0.0002)	
training:	Epoch: [92][139/408]	Loss 0.0001 (0.0002)	
training:	Epoch: [92][140/408]	Loss 0.0001 (0.0002)	
training:	Epoch: [92][141/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [92][142/408]	Loss 0.0001 (0.0002)	
training:	Epoch: [92][143/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [92][144/408]	Loss 0.0003 (0.0002)	
training:	Epoch: [92][145/408]	Loss 0.0001 (0.0002)	
training:	Epoch: [92][146/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [92][147/408]	Loss 0.0001 (0.0002)	
training:	Epoch: [92][148/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [92][149/408]	Loss 0.0003 (0.0002)	
training:	Epoch: [92][150/408]	Loss 0.0001 (0.0002)	
training:	Epoch: [92][151/408]	Loss 0.0001 (0.0002)	
training:	Epoch: [92][152/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [92][153/408]	Loss 0.0001 (0.0002)	
training:	Epoch: [92][154/408]	Loss 0.0001 (0.0002)	
training:	Epoch: [92][155/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [92][156/408]	Loss 0.0005 (0.0002)	
training:	Epoch: [92][157/408]	Loss 0.0001 (0.0002)	
training:	Epoch: [92][158/408]	Loss 0.0001 (0.0002)	
training:	Epoch: [92][159/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [92][160/408]	Loss 0.0001 (0.0002)	
training:	Epoch: [92][161/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [92][162/408]	Loss 0.0005 (0.0002)	
training:	Epoch: [92][163/408]	Loss 0.0003 (0.0002)	
training:	Epoch: [92][164/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [92][165/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [92][166/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [92][167/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [92][168/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [92][169/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [92][170/408]	Loss 0.0003 (0.0002)	
training:	Epoch: [92][171/408]	Loss 0.0001 (0.0002)	
training:	Epoch: [92][172/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [92][173/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [92][174/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [92][175/408]	Loss 0.0001 (0.0002)	
training:	Epoch: [92][176/408]	Loss 0.0001 (0.0002)	
training:	Epoch: [92][177/408]	Loss 0.0001 (0.0002)	
training:	Epoch: [92][178/408]	Loss 0.0003 (0.0002)	
training:	Epoch: [92][179/408]	Loss 0.0003 (0.0002)	
training:	Epoch: [92][180/408]	Loss 0.0001 (0.0002)	
training:	Epoch: [92][181/408]	Loss 0.0001 (0.0002)	
training:	Epoch: [92][182/408]	Loss 0.0001 (0.0002)	
training:	Epoch: [92][183/408]	Loss 0.0001 (0.0002)	
training:	Epoch: [92][184/408]	Loss 0.0001 (0.0002)	
training:	Epoch: [92][185/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [92][186/408]	Loss 0.0004 (0.0002)	
training:	Epoch: [92][187/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [92][188/408]	Loss 0.0001 (0.0002)	
training:	Epoch: [92][189/408]	Loss 0.0001 (0.0002)	
training:	Epoch: [92][190/408]	Loss 0.0003 (0.0002)	
training:	Epoch: [92][191/408]	Loss 0.0001 (0.0002)	
training:	Epoch: [92][192/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [92][193/408]	Loss 0.0001 (0.0002)	
training:	Epoch: [92][194/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [92][195/408]	Loss 0.0001 (0.0002)	
training:	Epoch: [92][196/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [92][197/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [92][198/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [92][199/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [92][200/408]	Loss 0.0001 (0.0002)	
training:	Epoch: [92][201/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [92][202/408]	Loss 0.0001 (0.0002)	
training:	Epoch: [92][203/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [92][204/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [92][205/408]	Loss 0.0001 (0.0002)	
training:	Epoch: [92][206/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [92][207/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [92][208/408]	Loss 0.0001 (0.0002)	
training:	Epoch: [92][209/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [92][210/408]	Loss 0.0001 (0.0002)	
training:	Epoch: [92][211/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [92][212/408]	Loss 0.0001 (0.0002)	
training:	Epoch: [92][213/408]	Loss 0.0005 (0.0002)	
training:	Epoch: [92][214/408]	Loss 0.0001 (0.0002)	
training:	Epoch: [92][215/408]	Loss 0.0004 (0.0002)	
training:	Epoch: [92][216/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [92][217/408]	Loss 0.0001 (0.0002)	
training:	Epoch: [92][218/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [92][219/408]	Loss 0.0001 (0.0002)	
training:	Epoch: [92][220/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [92][221/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [92][222/408]	Loss 0.0008 (0.0002)	
training:	Epoch: [92][223/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [92][224/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [92][225/408]	Loss 0.0001 (0.0002)	
training:	Epoch: [92][226/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [92][227/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [92][228/408]	Loss 0.0001 (0.0002)	
training:	Epoch: [92][229/408]	Loss 0.0001 (0.0002)	
training:	Epoch: [92][230/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [92][231/408]	Loss 0.0001 (0.0002)	
training:	Epoch: [92][232/408]	Loss 0.0001 (0.0002)	
training:	Epoch: [92][233/408]	Loss 0.0015 (0.0002)	
training:	Epoch: [92][234/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [92][235/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [92][236/408]	Loss 0.0003 (0.0002)	
training:	Epoch: [92][237/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [92][238/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [92][239/408]	Loss 0.0003 (0.0002)	
training:	Epoch: [92][240/408]	Loss 0.0001 (0.0002)	
training:	Epoch: [92][241/408]	Loss 0.0001 (0.0002)	
training:	Epoch: [92][242/408]	Loss 0.0006 (0.0002)	
training:	Epoch: [92][243/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [92][244/408]	Loss 0.0001 (0.0002)	
training:	Epoch: [92][245/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [92][246/408]	Loss 0.0001 (0.0002)	
training:	Epoch: [92][247/408]	Loss 0.0001 (0.0002)	
training:	Epoch: [92][248/408]	Loss 0.0001 (0.0002)	
training:	Epoch: [92][249/408]	Loss 0.0003 (0.0002)	
training:	Epoch: [92][250/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [92][251/408]	Loss 0.0001 (0.0002)	
training:	Epoch: [92][252/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [92][253/408]	Loss 0.0001 (0.0002)	
training:	Epoch: [92][254/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [92][255/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [92][256/408]	Loss 0.0001 (0.0002)	
training:	Epoch: [92][257/408]	Loss 0.0001 (0.0002)	
training:	Epoch: [92][258/408]	Loss 0.0001 (0.0002)	
training:	Epoch: [92][259/408]	Loss 0.0001 (0.0002)	
training:	Epoch: [92][260/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [92][261/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [92][262/408]	Loss 0.0001 (0.0002)	
training:	Epoch: [92][263/408]	Loss 0.0007 (0.0002)	
training:	Epoch: [92][264/408]	Loss 0.0001 (0.0002)	
training:	Epoch: [92][265/408]	Loss 0.0003 (0.0002)	
training:	Epoch: [92][266/408]	Loss 0.0001 (0.0002)	
training:	Epoch: [92][267/408]	Loss 0.0001 (0.0002)	
training:	Epoch: [92][268/408]	Loss 0.0001 (0.0002)	
training:	Epoch: [92][269/408]	Loss 0.0001 (0.0002)	
training:	Epoch: [92][270/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [92][271/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [92][272/408]	Loss 0.0001 (0.0002)	
training:	Epoch: [92][273/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [92][274/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [92][275/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [92][276/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [92][277/408]	Loss 0.0001 (0.0002)	
training:	Epoch: [92][278/408]	Loss 0.0001 (0.0002)	
training:	Epoch: [92][279/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [92][280/408]	Loss 0.0001 (0.0002)	
training:	Epoch: [92][281/408]	Loss 0.0001 (0.0002)	
training:	Epoch: [92][282/408]	Loss 0.0001 (0.0002)	
training:	Epoch: [92][283/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [92][284/408]	Loss 0.0001 (0.0002)	
training:	Epoch: [92][285/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [92][286/408]	Loss 0.0003 (0.0002)	
training:	Epoch: [92][287/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [92][288/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [92][289/408]	Loss 0.0001 (0.0002)	
training:	Epoch: [92][290/408]	Loss 0.0001 (0.0002)	
training:	Epoch: [92][291/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [92][292/408]	Loss 0.0001 (0.0002)	
training:	Epoch: [92][293/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [92][294/408]	Loss 0.0001 (0.0002)	
training:	Epoch: [92][295/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [92][296/408]	Loss 0.0001 (0.0002)	
training:	Epoch: [92][297/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [92][298/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [92][299/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [92][300/408]	Loss 0.0001 (0.0002)	
training:	Epoch: [92][301/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [92][302/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [92][303/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [92][304/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [92][305/408]	Loss 0.0001 (0.0002)	
training:	Epoch: [92][306/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [92][307/408]	Loss 0.0001 (0.0002)	
training:	Epoch: [92][308/408]	Loss 0.0001 (0.0002)	
training:	Epoch: [92][309/408]	Loss 0.0001 (0.0002)	
training:	Epoch: [92][310/408]	Loss 0.0001 (0.0002)	
training:	Epoch: [92][311/408]	Loss 0.0001 (0.0002)	
training:	Epoch: [92][312/408]	Loss 0.0001 (0.0002)	
training:	Epoch: [92][313/408]	Loss 0.0001 (0.0002)	
training:	Epoch: [92][314/408]	Loss 0.0001 (0.0002)	
training:	Epoch: [92][315/408]	Loss 0.0001 (0.0002)	
training:	Epoch: [92][316/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [92][317/408]	Loss 0.0001 (0.0002)	
training:	Epoch: [92][318/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [92][319/408]	Loss 0.0001 (0.0002)	
training:	Epoch: [92][320/408]	Loss 0.0001 (0.0002)	
training:	Epoch: [92][321/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [92][322/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [92][323/408]	Loss 0.0001 (0.0002)	
training:	Epoch: [92][324/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [92][325/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [92][326/408]	Loss 0.0001 (0.0002)	
training:	Epoch: [92][327/408]	Loss 0.0001 (0.0002)	
training:	Epoch: [92][328/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [92][329/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [92][330/408]	Loss 0.0001 (0.0002)	
training:	Epoch: [92][331/408]	Loss 0.0001 (0.0002)	
training:	Epoch: [92][332/408]	Loss 0.0001 (0.0002)	
training:	Epoch: [92][333/408]	Loss 0.0001 (0.0002)	
training:	Epoch: [92][334/408]	Loss 0.0001 (0.0002)	
training:	Epoch: [92][335/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [92][336/408]	Loss 0.0001 (0.0002)	
training:	Epoch: [92][337/408]	Loss 0.0001 (0.0002)	
training:	Epoch: [92][338/408]	Loss 0.0001 (0.0002)	
training:	Epoch: [92][339/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [92][340/408]	Loss 0.0001 (0.0002)	
training:	Epoch: [92][341/408]	Loss 0.0001 (0.0002)	
training:	Epoch: [92][342/408]	Loss 0.0001 (0.0002)	
training:	Epoch: [92][343/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [92][344/408]	Loss 0.0001 (0.0002)	
training:	Epoch: [92][345/408]	Loss 0.0001 (0.0002)	
training:	Epoch: [92][346/408]	Loss 0.0001 (0.0002)	
training:	Epoch: [92][347/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [92][348/408]	Loss 0.0001 (0.0002)	
training:	Epoch: [92][349/408]	Loss 0.0001 (0.0002)	
training:	Epoch: [92][350/408]	Loss 0.0001 (0.0002)	
training:	Epoch: [92][351/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [92][352/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [92][353/408]	Loss 0.0001 (0.0002)	
training:	Epoch: [92][354/408]	Loss 0.0014 (0.0002)	
training:	Epoch: [92][355/408]	Loss 0.0001 (0.0002)	
training:	Epoch: [92][356/408]	Loss 0.0008 (0.0002)	
training:	Epoch: [92][357/408]	Loss 0.0001 (0.0002)	
training:	Epoch: [92][358/408]	Loss 0.0001 (0.0002)	
training:	Epoch: [92][359/408]	Loss 0.0009 (0.0002)	
training:	Epoch: [92][360/408]	Loss 0.0003 (0.0002)	
training:	Epoch: [92][361/408]	Loss 0.0001 (0.0002)	
training:	Epoch: [92][362/408]	Loss 0.0001 (0.0002)	
training:	Epoch: [92][363/408]	Loss 0.0001 (0.0002)	
training:	Epoch: [92][364/408]	Loss 0.0001 (0.0002)	
training:	Epoch: [92][365/408]	Loss 0.0003 (0.0002)	
training:	Epoch: [92][366/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [92][367/408]	Loss 0.0003 (0.0002)	
training:	Epoch: [92][368/408]	Loss 0.0003 (0.0002)	
training:	Epoch: [92][369/408]	Loss 0.0001 (0.0002)	
training:	Epoch: [92][370/408]	Loss 0.0001 (0.0002)	
training:	Epoch: [92][371/408]	Loss 0.0001 (0.0002)	
training:	Epoch: [92][372/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [92][373/408]	Loss 0.0001 (0.0002)	
training:	Epoch: [92][374/408]	Loss 0.0004 (0.0002)	
training:	Epoch: [92][375/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [92][376/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [92][377/408]	Loss 0.0001 (0.0002)	
training:	Epoch: [92][378/408]	Loss 0.0001 (0.0002)	
training:	Epoch: [92][379/408]	Loss 0.0003 (0.0002)	
training:	Epoch: [92][380/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [92][381/408]	Loss 0.0001 (0.0002)	
training:	Epoch: [92][382/408]	Loss 0.0001 (0.0002)	
training:	Epoch: [92][383/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [92][384/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [92][385/408]	Loss 0.0001 (0.0002)	
training:	Epoch: [92][386/408]	Loss 0.0001 (0.0002)	
training:	Epoch: [92][387/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [92][388/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [92][389/408]	Loss 0.0001 (0.0002)	
training:	Epoch: [92][390/408]	Loss 0.0003 (0.0002)	
training:	Epoch: [92][391/408]	Loss 0.0001 (0.0002)	
training:	Epoch: [92][392/408]	Loss 0.0001 (0.0002)	
training:	Epoch: [92][393/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [92][394/408]	Loss 0.0001 (0.0002)	
training:	Epoch: [92][395/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [92][396/408]	Loss 0.0001 (0.0002)	
training:	Epoch: [92][397/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [92][398/408]	Loss 0.0004 (0.0002)	
training:	Epoch: [92][399/408]	Loss 0.0001 (0.0002)	
training:	Epoch: [92][400/408]	Loss 0.0001 (0.0002)	
training:	Epoch: [92][401/408]	Loss 0.0001 (0.0002)	
training:	Epoch: [92][402/408]	Loss 0.0024 (0.0002)	
training:	Epoch: [92][403/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [92][404/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [92][405/408]	Loss 0.0001 (0.0002)	
training:	Epoch: [92][406/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [92][407/408]	Loss 0.0001 (0.0002)	
training:	Epoch: [92][408/408]	Loss 0.0001 (0.0002)	
Training:	 Loss: 0.0002

Training:	 ACC: 1.0000 1.0000 1.0000 1.0000
Validation:	 ACC: 0.7772 0.7774 0.7830 0.7713
Validation:	 Best_BACC: 0.8226 0.8250 0.8772 0.7679
Validation:	 Loss: 1.5710
Pretraining:	Epoch 93/200
----------
training:	Epoch: [93][1/408]	Loss 0.0006 (0.0006)	
training:	Epoch: [93][2/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [93][3/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [93][4/408]	Loss 0.0006 (0.0004)	
training:	Epoch: [93][5/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [93][6/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [93][7/408]	Loss 0.0001 (0.0003)	
training:	Epoch: [93][8/408]	Loss 0.0001 (0.0003)	
training:	Epoch: [93][9/408]	Loss 0.0001 (0.0003)	
training:	Epoch: [93][10/408]	Loss 0.0001 (0.0002)	
training:	Epoch: [93][11/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [93][12/408]	Loss 0.0001 (0.0002)	
training:	Epoch: [93][13/408]	Loss 0.0005 (0.0003)	
training:	Epoch: [93][14/408]	Loss 0.0001 (0.0002)	
training:	Epoch: [93][15/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [93][16/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [93][17/408]	Loss 0.0001 (0.0002)	
training:	Epoch: [93][18/408]	Loss 0.0001 (0.0002)	
training:	Epoch: [93][19/408]	Loss 0.0003 (0.0002)	
training:	Epoch: [93][20/408]	Loss 0.0001 (0.0002)	
training:	Epoch: [93][21/408]	Loss 0.0001 (0.0002)	
training:	Epoch: [93][22/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [93][23/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [93][24/408]	Loss 0.0001 (0.0002)	
training:	Epoch: [93][25/408]	Loss 0.0001 (0.0002)	
training:	Epoch: [93][26/408]	Loss 0.0001 (0.0002)	
training:	Epoch: [93][27/408]	Loss 0.0001 (0.0002)	
training:	Epoch: [93][28/408]	Loss 0.0001 (0.0002)	
training:	Epoch: [93][29/408]	Loss 0.0001 (0.0002)	
training:	Epoch: [93][30/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [93][31/408]	Loss 0.0001 (0.0002)	
training:	Epoch: [93][32/408]	Loss 0.0004 (0.0002)	
training:	Epoch: [93][33/408]	Loss 0.0003 (0.0002)	
training:	Epoch: [93][34/408]	Loss 0.0001 (0.0002)	
training:	Epoch: [93][35/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [93][36/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [93][37/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [93][38/408]	Loss 0.0001 (0.0002)	
training:	Epoch: [93][39/408]	Loss 0.0001 (0.0002)	
training:	Epoch: [93][40/408]	Loss 0.0001 (0.0002)	
training:	Epoch: [93][41/408]	Loss 0.0001 (0.0002)	
training:	Epoch: [93][42/408]	Loss 0.0001 (0.0002)	
training:	Epoch: [93][43/408]	Loss 0.0001 (0.0002)	
training:	Epoch: [93][44/408]	Loss 0.0001 (0.0002)	
training:	Epoch: [93][45/408]	Loss 0.0001 (0.0002)	
training:	Epoch: [93][46/408]	Loss 0.0001 (0.0002)	
training:	Epoch: [93][47/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [93][48/408]	Loss 0.0001 (0.0002)	
training:	Epoch: [93][49/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [93][50/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [93][51/408]	Loss 0.0001 (0.0002)	
training:	Epoch: [93][52/408]	Loss 0.0001 (0.0002)	
training:	Epoch: [93][53/408]	Loss 0.0001 (0.0002)	
training:	Epoch: [93][54/408]	Loss 0.0001 (0.0002)	
training:	Epoch: [93][55/408]	Loss 0.0001 (0.0002)	
training:	Epoch: [93][56/408]	Loss 0.0004 (0.0002)	
training:	Epoch: [93][57/408]	Loss 0.0001 (0.0002)	
training:	Epoch: [93][58/408]	Loss 0.0001 (0.0002)	
training:	Epoch: [93][59/408]	Loss 0.0001 (0.0002)	
training:	Epoch: [93][60/408]	Loss 0.0001 (0.0002)	
training:	Epoch: [93][61/408]	Loss 0.0001 (0.0002)	
training:	Epoch: [93][62/408]	Loss 0.0003 (0.0002)	
training:	Epoch: [93][63/408]	Loss 0.0001 (0.0002)	
training:	Epoch: [93][64/408]	Loss 0.0001 (0.0002)	
training:	Epoch: [93][65/408]	Loss 0.0001 (0.0002)	
training:	Epoch: [93][66/408]	Loss 0.0001 (0.0002)	
training:	Epoch: [93][67/408]	Loss 0.0001 (0.0002)	
training:	Epoch: [93][68/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [93][69/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [93][70/408]	Loss 0.0001 (0.0002)	
training:	Epoch: [93][71/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [93][72/408]	Loss 0.0001 (0.0002)	
training:	Epoch: [93][73/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [93][74/408]	Loss 0.0001 (0.0002)	
training:	Epoch: [93][75/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [93][76/408]	Loss 0.0001 (0.0002)	
training:	Epoch: [93][77/408]	Loss 0.0001 (0.0002)	
training:	Epoch: [93][78/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [93][79/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [93][80/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [93][81/408]	Loss 0.0001 (0.0002)	
training:	Epoch: [93][82/408]	Loss 0.0001 (0.0002)	
training:	Epoch: [93][83/408]	Loss 0.0001 (0.0002)	
training:	Epoch: [93][84/408]	Loss 0.0001 (0.0002)	
training:	Epoch: [93][85/408]	Loss 0.0001 (0.0002)	
training:	Epoch: [93][86/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [93][87/408]	Loss 0.0001 (0.0002)	
training:	Epoch: [93][88/408]	Loss 0.0001 (0.0002)	
training:	Epoch: [93][89/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [93][90/408]	Loss 0.0001 (0.0002)	
training:	Epoch: [93][91/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [93][92/408]	Loss 0.0004 (0.0002)	
training:	Epoch: [93][93/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [93][94/408]	Loss 0.0004 (0.0002)	
training:	Epoch: [93][95/408]	Loss 0.0003 (0.0002)	
training:	Epoch: [93][96/408]	Loss 0.0001 (0.0002)	
training:	Epoch: [93][97/408]	Loss 0.0001 (0.0002)	
training:	Epoch: [93][98/408]	Loss 0.0001 (0.0002)	
training:	Epoch: [93][99/408]	Loss 0.0001 (0.0002)	
training:	Epoch: [93][100/408]	Loss 0.0001 (0.0002)	
training:	Epoch: [93][101/408]	Loss 0.0007 (0.0002)	
training:	Epoch: [93][102/408]	Loss 0.0001 (0.0002)	
training:	Epoch: [93][103/408]	Loss 0.0003 (0.0002)	
training:	Epoch: [93][104/408]	Loss 0.0001 (0.0002)	
training:	Epoch: [93][105/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [93][106/408]	Loss 0.0001 (0.0002)	
training:	Epoch: [93][107/408]	Loss 0.0001 (0.0002)	
training:	Epoch: [93][108/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [93][109/408]	Loss 0.0001 (0.0002)	
training:	Epoch: [93][110/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [93][111/408]	Loss 0.0001 (0.0002)	
training:	Epoch: [93][112/408]	Loss 0.0001 (0.0002)	
training:	Epoch: [93][113/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [93][114/408]	Loss 0.0001 (0.0002)	
training:	Epoch: [93][115/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [93][116/408]	Loss 0.0001 (0.0002)	
training:	Epoch: [93][117/408]	Loss 0.0001 (0.0002)	
training:	Epoch: [93][118/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [93][119/408]	Loss 0.0001 (0.0002)	
training:	Epoch: [93][120/408]	Loss 0.0003 (0.0002)	
training:	Epoch: [93][121/408]	Loss 0.0001 (0.0002)	
training:	Epoch: [93][122/408]	Loss 0.0001 (0.0002)	
training:	Epoch: [93][123/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [93][124/408]	Loss 0.0001 (0.0002)	
training:	Epoch: [93][125/408]	Loss 0.0001 (0.0002)	
training:	Epoch: [93][126/408]	Loss 0.0001 (0.0002)	
training:	Epoch: [93][127/408]	Loss 0.0001 (0.0002)	
training:	Epoch: [93][128/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [93][129/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [93][130/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [93][131/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [93][132/408]	Loss 0.0001 (0.0002)	
training:	Epoch: [93][133/408]	Loss 0.0001 (0.0002)	
training:	Epoch: [93][134/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [93][135/408]	Loss 0.0039 (0.0002)	
training:	Epoch: [93][136/408]	Loss 0.0001 (0.0002)	
training:	Epoch: [93][137/408]	Loss 0.0001 (0.0002)	
training:	Epoch: [93][138/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [93][139/408]	Loss 0.0001 (0.0002)	
training:	Epoch: [93][140/408]	Loss 0.0001 (0.0002)	
training:	Epoch: [93][141/408]	Loss 0.0004 (0.0002)	
training:	Epoch: [93][142/408]	Loss 0.0001 (0.0002)	
training:	Epoch: [93][143/408]	Loss 0.0001 (0.0002)	
training:	Epoch: [93][144/408]	Loss 0.0001 (0.0002)	
training:	Epoch: [93][145/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [93][146/408]	Loss 0.0001 (0.0002)	
training:	Epoch: [93][147/408]	Loss 0.0001 (0.0002)	
training:	Epoch: [93][148/408]	Loss 0.0001 (0.0002)	
training:	Epoch: [93][149/408]	Loss 0.0001 (0.0002)	
training:	Epoch: [93][150/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [93][151/408]	Loss 0.0001 (0.0002)	
training:	Epoch: [93][152/408]	Loss 0.0001 (0.0002)	
training:	Epoch: [93][153/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [93][154/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [93][155/408]	Loss 0.0001 (0.0002)	
training:	Epoch: [93][156/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [93][157/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [93][158/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [93][159/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [93][160/408]	Loss 0.0001 (0.0002)	
training:	Epoch: [93][161/408]	Loss 0.0001 (0.0002)	
training:	Epoch: [93][162/408]	Loss 0.0001 (0.0002)	
training:	Epoch: [93][163/408]	Loss 0.0004 (0.0002)	
training:	Epoch: [93][164/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [93][165/408]	Loss 0.0001 (0.0002)	
training:	Epoch: [93][166/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [93][167/408]	Loss 0.0001 (0.0002)	
training:	Epoch: [93][168/408]	Loss 0.0037 (0.0002)	
training:	Epoch: [93][169/408]	Loss 0.0001 (0.0002)	
training:	Epoch: [93][170/408]	Loss 0.0001 (0.0002)	
training:	Epoch: [93][171/408]	Loss 0.0001 (0.0002)	
training:	Epoch: [93][172/408]	Loss 0.0001 (0.0002)	
training:	Epoch: [93][173/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [93][174/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [93][175/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [93][176/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [93][177/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [93][178/408]	Loss 0.0001 (0.0002)	
training:	Epoch: [93][179/408]	Loss 0.0001 (0.0002)	
training:	Epoch: [93][180/408]	Loss 0.0001 (0.0002)	
training:	Epoch: [93][181/408]	Loss 0.0001 (0.0002)	
training:	Epoch: [93][182/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [93][183/408]	Loss 0.0001 (0.0002)	
training:	Epoch: [93][184/408]	Loss 0.0001 (0.0002)	
training:	Epoch: [93][185/408]	Loss 0.0001 (0.0002)	
training:	Epoch: [93][186/408]	Loss 0.0003 (0.0002)	
training:	Epoch: [93][187/408]	Loss 0.0001 (0.0002)	
training:	Epoch: [93][188/408]	Loss 0.0003 (0.0002)	
training:	Epoch: [93][189/408]	Loss 0.0001 (0.0002)	
training:	Epoch: [93][190/408]	Loss 0.0001 (0.0002)	
training:	Epoch: [93][191/408]	Loss 0.0001 (0.0002)	
training:	Epoch: [93][192/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [93][193/408]	Loss 0.0001 (0.0002)	
training:	Epoch: [93][194/408]	Loss 0.0001 (0.0002)	
training:	Epoch: [93][195/408]	Loss 0.0001 (0.0002)	
training:	Epoch: [93][196/408]	Loss 0.0003 (0.0002)	
training:	Epoch: [93][197/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [93][198/408]	Loss 0.0001 (0.0002)	
training:	Epoch: [93][199/408]	Loss 0.0113 (0.0003)	
training:	Epoch: [93][200/408]	Loss 0.0001 (0.0003)	
training:	Epoch: [93][201/408]	Loss 0.0001 (0.0003)	
training:	Epoch: [93][202/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [93][203/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [93][204/408]	Loss 0.0001 (0.0003)	
training:	Epoch: [93][205/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [93][206/408]	Loss 0.0001 (0.0003)	
training:	Epoch: [93][207/408]	Loss 0.0004 (0.0003)	
training:	Epoch: [93][208/408]	Loss 0.0006 (0.0003)	
training:	Epoch: [93][209/408]	Loss 0.0001 (0.0003)	
training:	Epoch: [93][210/408]	Loss 0.0005 (0.0003)	
training:	Epoch: [93][211/408]	Loss 0.0004 (0.0003)	
training:	Epoch: [93][212/408]	Loss 0.0001 (0.0003)	
training:	Epoch: [93][213/408]	Loss 0.0001 (0.0003)	
training:	Epoch: [93][214/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [93][215/408]	Loss 0.0001 (0.0003)	
training:	Epoch: [93][216/408]	Loss 0.0001 (0.0003)	
training:	Epoch: [93][217/408]	Loss 0.0003 (0.0003)	
training:	Epoch: [93][218/408]	Loss 0.0001 (0.0003)	
training:	Epoch: [93][219/408]	Loss 0.0003 (0.0003)	
training:	Epoch: [93][220/408]	Loss 0.0004 (0.0003)	
training:	Epoch: [93][221/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [93][222/408]	Loss 0.0001 (0.0003)	
training:	Epoch: [93][223/408]	Loss 0.0001 (0.0003)	
training:	Epoch: [93][224/408]	Loss 0.0059 (0.0003)	
training:	Epoch: [93][225/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [93][226/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [93][227/408]	Loss 0.0006 (0.0003)	
training:	Epoch: [93][228/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [93][229/408]	Loss 0.0015 (0.0003)	
training:	Epoch: [93][230/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [93][231/408]	Loss 0.0001 (0.0003)	
training:	Epoch: [93][232/408]	Loss 0.0001 (0.0003)	
training:	Epoch: [93][233/408]	Loss 0.0003 (0.0003)	
training:	Epoch: [93][234/408]	Loss 0.0001 (0.0003)	
training:	Epoch: [93][235/408]	Loss 0.0003 (0.0003)	
training:	Epoch: [93][236/408]	Loss 0.0001 (0.0003)	
training:	Epoch: [93][237/408]	Loss 0.0001 (0.0003)	
training:	Epoch: [93][238/408]	Loss 0.0001 (0.0003)	
training:	Epoch: [93][239/408]	Loss 0.0001 (0.0003)	
training:	Epoch: [93][240/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [93][241/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [93][242/408]	Loss 0.0001 (0.0003)	
training:	Epoch: [93][243/408]	Loss 0.0001 (0.0003)	
training:	Epoch: [93][244/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [93][245/408]	Loss 0.0004 (0.0003)	
training:	Epoch: [93][246/408]	Loss 0.0001 (0.0003)	
training:	Epoch: [93][247/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [93][248/408]	Loss 0.0001 (0.0003)	
training:	Epoch: [93][249/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [93][250/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [93][251/408]	Loss 0.0003 (0.0003)	
training:	Epoch: [93][252/408]	Loss 0.0001 (0.0003)	
training:	Epoch: [93][253/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [93][254/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [93][255/408]	Loss 0.0001 (0.0003)	
training:	Epoch: [93][256/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [93][257/408]	Loss 0.0001 (0.0003)	
training:	Epoch: [93][258/408]	Loss 0.0001 (0.0003)	
training:	Epoch: [93][259/408]	Loss 0.0001 (0.0003)	
training:	Epoch: [93][260/408]	Loss 0.0001 (0.0003)	
training:	Epoch: [93][261/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [93][262/408]	Loss 0.0001 (0.0003)	
training:	Epoch: [93][263/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [93][264/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [93][265/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [93][266/408]	Loss 0.0001 (0.0003)	
training:	Epoch: [93][267/408]	Loss 0.0001 (0.0003)	
training:	Epoch: [93][268/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [93][269/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [93][270/408]	Loss 0.0001 (0.0003)	
training:	Epoch: [93][271/408]	Loss 0.0013 (0.0003)	
training:	Epoch: [93][272/408]	Loss 0.0001 (0.0003)	
training:	Epoch: [93][273/408]	Loss 0.0001 (0.0003)	
training:	Epoch: [93][274/408]	Loss 0.0001 (0.0003)	
training:	Epoch: [93][275/408]	Loss 0.0001 (0.0003)	
training:	Epoch: [93][276/408]	Loss 0.0004 (0.0003)	
training:	Epoch: [93][277/408]	Loss 0.0001 (0.0003)	
training:	Epoch: [93][278/408]	Loss 0.0001 (0.0003)	
training:	Epoch: [93][279/408]	Loss 0.0001 (0.0003)	
training:	Epoch: [93][280/408]	Loss 0.0001 (0.0003)	
training:	Epoch: [93][281/408]	Loss 0.0001 (0.0003)	
training:	Epoch: [93][282/408]	Loss 0.0001 (0.0003)	
training:	Epoch: [93][283/408]	Loss 0.0001 (0.0003)	
training:	Epoch: [93][284/408]	Loss 0.0001 (0.0003)	
training:	Epoch: [93][285/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [93][286/408]	Loss 0.0001 (0.0003)	
training:	Epoch: [93][287/408]	Loss 0.0001 (0.0003)	
training:	Epoch: [93][288/408]	Loss 0.0001 (0.0003)	
training:	Epoch: [93][289/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [93][290/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [93][291/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [93][292/408]	Loss 0.0001 (0.0003)	
training:	Epoch: [93][293/408]	Loss 0.0001 (0.0003)	
training:	Epoch: [93][294/408]	Loss 0.0001 (0.0003)	
training:	Epoch: [93][295/408]	Loss 0.0001 (0.0003)	
training:	Epoch: [93][296/408]	Loss 0.0001 (0.0003)	
training:	Epoch: [93][297/408]	Loss 0.0001 (0.0003)	
training:	Epoch: [93][298/408]	Loss 0.0001 (0.0003)	
training:	Epoch: [93][299/408]	Loss 0.0001 (0.0003)	
training:	Epoch: [93][300/408]	Loss 0.0003 (0.0003)	
training:	Epoch: [93][301/408]	Loss 0.0001 (0.0003)	
training:	Epoch: [93][302/408]	Loss 0.0001 (0.0003)	
training:	Epoch: [93][303/408]	Loss 0.0001 (0.0003)	
training:	Epoch: [93][304/408]	Loss 0.0001 (0.0003)	
training:	Epoch: [93][305/408]	Loss 0.0001 (0.0003)	
training:	Epoch: [93][306/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [93][307/408]	Loss 0.0005 (0.0003)	
training:	Epoch: [93][308/408]	Loss 0.0001 (0.0003)	
training:	Epoch: [93][309/408]	Loss 0.0001 (0.0003)	
training:	Epoch: [93][310/408]	Loss 0.0001 (0.0003)	
training:	Epoch: [93][311/408]	Loss 0.0001 (0.0003)	
training:	Epoch: [93][312/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [93][313/408]	Loss 0.0001 (0.0003)	
training:	Epoch: [93][314/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [93][315/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [93][316/408]	Loss 0.0001 (0.0003)	
training:	Epoch: [93][317/408]	Loss 0.0001 (0.0003)	
training:	Epoch: [93][318/408]	Loss 0.0001 (0.0003)	
training:	Epoch: [93][319/408]	Loss 0.0003 (0.0003)	
training:	Epoch: [93][320/408]	Loss 0.0001 (0.0003)	
training:	Epoch: [93][321/408]	Loss 0.0001 (0.0003)	
training:	Epoch: [93][322/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [93][323/408]	Loss 0.0184 (0.0003)	
training:	Epoch: [93][324/408]	Loss 0.0001 (0.0003)	
training:	Epoch: [93][325/408]	Loss 0.0001 (0.0003)	
training:	Epoch: [93][326/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [93][327/408]	Loss 0.0003 (0.0003)	
training:	Epoch: [93][328/408]	Loss 0.0001 (0.0003)	
training:	Epoch: [93][329/408]	Loss 0.0004 (0.0003)	
training:	Epoch: [93][330/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [93][331/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [93][332/408]	Loss 0.0001 (0.0003)	
training:	Epoch: [93][333/408]	Loss 0.0012 (0.0003)	
training:	Epoch: [93][334/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [93][335/408]	Loss 0.0004 (0.0003)	
training:	Epoch: [93][336/408]	Loss 0.0002 (0.0003)	
training:	Epoch: [93][337/408]	Loss 0.0008 (0.0003)	
training:	Epoch: [93][338/408]	Loss 0.2171 (0.0010)	
training:	Epoch: [93][339/408]	Loss 0.0001 (0.0010)	
training:	Epoch: [93][340/408]	Loss 0.0001 (0.0010)	
training:	Epoch: [93][341/408]	Loss 0.0001 (0.0009)	
training:	Epoch: [93][342/408]	Loss 0.0005 (0.0009)	
training:	Epoch: [93][343/408]	Loss 0.0001 (0.0009)	
training:	Epoch: [93][344/408]	Loss 0.0001 (0.0009)	
training:	Epoch: [93][345/408]	Loss 0.0351 (0.0010)	
training:	Epoch: [93][346/408]	Loss 0.0005 (0.0010)	
training:	Epoch: [93][347/408]	Loss 0.0427 (0.0012)	
training:	Epoch: [93][348/408]	Loss 0.2999 (0.0020)	
training:	Epoch: [93][349/408]	Loss 0.0005 (0.0020)	
training:	Epoch: [93][350/408]	Loss 0.0001 (0.0020)	
training:	Epoch: [93][351/408]	Loss 0.0004 (0.0020)	
training:	Epoch: [93][352/408]	Loss 0.0002 (0.0020)	
training:	Epoch: [93][353/408]	Loss 0.0017 (0.0020)	
training:	Epoch: [93][354/408]	Loss 0.0008 (0.0020)	
training:	Epoch: [93][355/408]	Loss 0.0002 (0.0020)	
training:	Epoch: [93][356/408]	Loss 0.1697 (0.0025)	
training:	Epoch: [93][357/408]	Loss 0.4030 (0.0036)	
training:	Epoch: [93][358/408]	Loss 0.0004 (0.0036)	
training:	Epoch: [93][359/408]	Loss 0.0058 (0.0036)	
training:	Epoch: [93][360/408]	Loss 0.0002 (0.0036)	
training:	Epoch: [93][361/408]	Loss 0.0003 (0.0036)	
training:	Epoch: [93][362/408]	Loss 0.0003 (0.0036)	
training:	Epoch: [93][363/408]	Loss 0.0003 (0.0035)	
training:	Epoch: [93][364/408]	Loss 0.0002 (0.0035)	
training:	Epoch: [93][365/408]	Loss 0.0002 (0.0035)	
training:	Epoch: [93][366/408]	Loss 0.0002 (0.0035)	
training:	Epoch: [93][367/408]	Loss 0.1317 (0.0039)	
training:	Epoch: [93][368/408]	Loss 0.0238 (0.0039)	
training:	Epoch: [93][369/408]	Loss 0.0003 (0.0039)	
training:	Epoch: [93][370/408]	Loss 0.0001 (0.0039)	
training:	Epoch: [93][371/408]	Loss 0.0006 (0.0039)	
training:	Epoch: [93][372/408]	Loss 0.0002 (0.0039)	
training:	Epoch: [93][373/408]	Loss 0.0006 (0.0039)	
training:	Epoch: [93][374/408]	Loss 0.0002 (0.0039)	
training:	Epoch: [93][375/408]	Loss 0.0002 (0.0039)	
training:	Epoch: [93][376/408]	Loss 0.0002 (0.0038)	
training:	Epoch: [93][377/408]	Loss 0.0006 (0.0038)	
training:	Epoch: [93][378/408]	Loss 0.0040 (0.0038)	
training:	Epoch: [93][379/408]	Loss 0.0002 (0.0038)	
training:	Epoch: [93][380/408]	Loss 0.0004 (0.0038)	
training:	Epoch: [93][381/408]	Loss 0.1252 (0.0041)	
training:	Epoch: [93][382/408]	Loss 0.0003 (0.0041)	
training:	Epoch: [93][383/408]	Loss 0.0003 (0.0041)	
training:	Epoch: [93][384/408]	Loss 0.0002 (0.0041)	
training:	Epoch: [93][385/408]	Loss 0.0004 (0.0041)	
training:	Epoch: [93][386/408]	Loss 0.0002 (0.0041)	
training:	Epoch: [93][387/408]	Loss 0.6973 (0.0059)	
training:	Epoch: [93][388/408]	Loss 0.0007 (0.0059)	
training:	Epoch: [93][389/408]	Loss 0.0003 (0.0058)	
training:	Epoch: [93][390/408]	Loss 0.0002 (0.0058)	
training:	Epoch: [93][391/408]	Loss 0.0336 (0.0059)	
training:	Epoch: [93][392/408]	Loss 0.0002 (0.0059)	
training:	Epoch: [93][393/408]	Loss 0.0002 (0.0059)	
training:	Epoch: [93][394/408]	Loss 0.0006 (0.0059)	
training:	Epoch: [93][395/408]	Loss 0.0063 (0.0059)	
training:	Epoch: [93][396/408]	Loss 0.0003 (0.0058)	
training:	Epoch: [93][397/408]	Loss 0.0002 (0.0058)	
training:	Epoch: [93][398/408]	Loss 0.0003 (0.0058)	
training:	Epoch: [93][399/408]	Loss 0.0002 (0.0058)	
training:	Epoch: [93][400/408]	Loss 0.0213 (0.0058)	
training:	Epoch: [93][401/408]	Loss 0.0730 (0.0060)	
training:	Epoch: [93][402/408]	Loss 0.0002 (0.0060)	
training:	Epoch: [93][403/408]	Loss 0.1322 (0.0063)	
training:	Epoch: [93][404/408]	Loss 0.0002 (0.0063)	
training:	Epoch: [93][405/408]	Loss 0.0021 (0.0063)	
training:	Epoch: [93][406/408]	Loss 0.3942 (0.0072)	
training:	Epoch: [93][407/408]	Loss 0.0002 (0.0072)	
training:	Epoch: [93][408/408]	Loss 0.0002 (0.0072)	
Training:	 Loss: 0.0072

Training:	 ACC: 0.9990 0.9989 0.9982 0.9997
Validation:	 ACC: 0.7797 0.7796 0.7758 0.7836
Validation:	 Best_BACC: 0.8226 0.8250 0.8772 0.7679
Validation:	 Loss: 1.4280
Pretraining:	Epoch 94/200
----------
training:	Epoch: [94][1/408]	Loss 0.0003 (0.0003)	
training:	Epoch: [94][2/408]	Loss 0.0002 (0.0002)	
training:	Epoch: [94][3/408]	Loss 0.0003 (0.0003)	
training:	Epoch: [94][4/408]	Loss 0.0008 (0.0004)	
training:	Epoch: [94][5/408]	Loss 0.0002 (0.0004)	
training:	Epoch: [94][6/408]	Loss 0.0279 (0.0050)	
training:	Epoch: [94][7/408]	Loss 0.0002 (0.0043)	
training:	Epoch: [94][8/408]	Loss 0.0002 (0.0038)	
training:	Epoch: [94][9/408]	Loss 0.0100 (0.0045)	
training:	Epoch: [94][10/408]	Loss 0.0019 (0.0042)	
training:	Epoch: [94][11/408]	Loss 0.0076 (0.0045)	
training:	Epoch: [94][12/408]	Loss 0.0004 (0.0042)	
training:	Epoch: [94][13/408]	Loss 0.0003 (0.0039)	
training:	Epoch: [94][14/408]	Loss 0.0015 (0.0037)	
training:	Epoch: [94][15/408]	Loss 0.0013 (0.0035)	
training:	Epoch: [94][16/408]	Loss 0.0010 (0.0034)	
training:	Epoch: [94][17/408]	Loss 0.0003 (0.0032)	
training:	Epoch: [94][18/408]	Loss 0.0004 (0.0030)	
training:	Epoch: [94][19/408]	Loss 0.0003 (0.0029)	
training:	Epoch: [94][20/408]	Loss 0.0002 (0.0028)	
training:	Epoch: [94][21/408]	Loss 0.0064 (0.0029)	
training:	Epoch: [94][22/408]	Loss 0.0080 (0.0032)	
training:	Epoch: [94][23/408]	Loss 0.0003 (0.0030)	
training:	Epoch: [94][24/408]	Loss 0.0062 (0.0032)	
training:	Epoch: [94][25/408]	Loss 0.0003 (0.0031)	
training:	Epoch: [94][26/408]	Loss 0.0003 (0.0029)	
training:	Epoch: [94][27/408]	Loss 0.0002 (0.0028)	
training:	Epoch: [94][28/408]	Loss 0.0020 (0.0028)	
training:	Epoch: [94][29/408]	Loss 0.2200 (0.0103)	
training:	Epoch: [94][30/408]	Loss 0.0002 (0.0100)	
training:	Epoch: [94][31/408]	Loss 0.0005 (0.0097)	
training:	Epoch: [94][32/408]	Loss 0.0003 (0.0094)	
training:	Epoch: [94][33/408]	Loss 0.0003 (0.0091)	
training:	Epoch: [94][34/408]	Loss 0.0011 (0.0089)	
training:	Epoch: [94][35/408]	Loss 0.0002 (0.0086)	
training:	Epoch: [94][36/408]	Loss 0.0002 (0.0084)	
training:	Epoch: [94][37/408]	Loss 0.0011 (0.0082)	
training:	Epoch: [94][38/408]	Loss 0.0193 (0.0085)	
training:	Epoch: [94][39/408]	Loss 0.0003 (0.0083)	
training:	Epoch: [94][40/408]	Loss 0.0003 (0.0081)	
training:	Epoch: [94][41/408]	Loss 0.0049 (0.0080)	
training:	Epoch: [94][42/408]	Loss 0.0520 (0.0090)	
training:	Epoch: [94][43/408]	Loss 0.0003 (0.0088)	
training:	Epoch: [94][44/408]	Loss 0.0002 (0.0086)	
training:	Epoch: [94][45/408]	Loss 0.0140 (0.0087)	
training:	Epoch: [94][46/408]	Loss 0.0477 (0.0096)	
training:	Epoch: [94][47/408]	Loss 0.0002 (0.0094)	
training:	Epoch: [94][48/408]	Loss 0.0002 (0.0092)	
training:	Epoch: [94][49/408]	Loss 0.0003 (0.0090)	
training:	Epoch: [94][50/408]	Loss 0.0002 (0.0088)	
training:	Epoch: [94][51/408]	Loss 0.0005 (0.0087)	
training:	Epoch: [94][52/408]	Loss 0.0002 (0.0085)	
training:	Epoch: [94][53/408]	Loss 0.0002 (0.0084)	
training:	Epoch: [94][54/408]	Loss 0.0002 (0.0082)	
training:	Epoch: [94][55/408]	Loss 0.0006 (0.0081)	
training:	Epoch: [94][56/408]	Loss 0.0015 (0.0080)	
training:	Epoch: [94][57/408]	Loss 0.0002 (0.0078)	
training:	Epoch: [94][58/408]	Loss 0.0003 (0.0077)	
training:	Epoch: [94][59/408]	Loss 0.0882 (0.0091)	
training:	Epoch: [94][60/408]	Loss 0.0253 (0.0093)	
training:	Epoch: [94][61/408]	Loss 0.0008 (0.0092)	
training:	Epoch: [94][62/408]	Loss 0.0002 (0.0090)	
training:	Epoch: [94][63/408]	Loss 0.0010 (0.0089)	
training:	Epoch: [94][64/408]	Loss 0.0002 (0.0088)	
training:	Epoch: [94][65/408]	Loss 0.0006 (0.0087)	
training:	Epoch: [94][66/408]	Loss 0.0002 (0.0085)	
training:	Epoch: [94][67/408]	Loss 0.0005 (0.0084)	
training:	Epoch: [94][68/408]	Loss 0.0013 (0.0083)	
training:	Epoch: [94][69/408]	Loss 0.0002 (0.0082)	
training:	Epoch: [94][70/408]	Loss 0.0003 (0.0081)	
training:	Epoch: [94][71/408]	Loss 0.0037 (0.0080)	
training:	Epoch: [94][72/408]	Loss 0.0010 (0.0079)	
training:	Epoch: [94][73/408]	Loss 0.0002 (0.0078)	
training:	Epoch: [94][74/408]	Loss 0.0004 (0.0077)	
training:	Epoch: [94][75/408]	Loss 0.0003 (0.0076)	
training:	Epoch: [94][76/408]	Loss 0.0005 (0.0075)	
training:	Epoch: [94][77/408]	Loss 0.0003 (0.0074)	
training:	Epoch: [94][78/408]	Loss 0.0002 (0.0073)	
training:	Epoch: [94][79/408]	Loss 0.0002 (0.0072)	
training:	Epoch: [94][80/408]	Loss 0.0002 (0.0071)	
training:	Epoch: [94][81/408]	Loss 0.0002 (0.0071)	
training:	Epoch: [94][82/408]	Loss 0.0002 (0.0070)	
training:	Epoch: [94][83/408]	Loss 0.0002 (0.0069)	
training:	Epoch: [94][84/408]	Loss 0.0002 (0.0068)	
training:	Epoch: [94][85/408]	Loss 0.0021 (0.0068)	
training:	Epoch: [94][86/408]	Loss 0.0004 (0.0067)	
training:	Epoch: [94][87/408]	Loss 0.0079 (0.0067)	
training:	Epoch: [94][88/408]	Loss 0.0233 (0.0069)	
training:	Epoch: [94][89/408]	Loss 0.0008 (0.0068)	
training:	Epoch: [94][90/408]	Loss 0.0002 (0.0067)	
training:	Epoch: [94][91/408]	Loss 0.0003 (0.0067)	
training:	Epoch: [94][92/408]	Loss 0.0006 (0.0066)	
training:	Epoch: [94][93/408]	Loss 0.0026 (0.0066)	
training:	Epoch: [94][94/408]	Loss 0.0005 (0.0065)	
training:	Epoch: [94][95/408]	Loss 0.0001 (0.0064)	
training:	Epoch: [94][96/408]	Loss 0.0014 (0.0064)	
training:	Epoch: [94][97/408]	Loss 0.0002 (0.0063)	
training:	Epoch: [94][98/408]	Loss 0.0002 (0.0063)	
training:	Epoch: [94][99/408]	Loss 0.0002 (0.0062)	
training:	Epoch: [94][100/408]	Loss 0.0003 (0.0061)	
training:	Epoch: [94][101/408]	Loss 0.0003 (0.0061)	
training:	Epoch: [94][102/408]	Loss 0.0003 (0.0060)	
training:	Epoch: [94][103/408]	Loss 0.0002 (0.0060)	
training:	Epoch: [94][104/408]	Loss 0.0002 (0.0059)	
training:	Epoch: [94][105/408]	Loss 0.0059 (0.0059)	
training:	Epoch: [94][106/408]	Loss 0.0002 (0.0059)	
training:	Epoch: [94][107/408]	Loss 0.0002 (0.0058)	
training:	Epoch: [94][108/408]	Loss 0.0004 (0.0058)	
training:	Epoch: [94][109/408]	Loss 0.0002 (0.0057)	
training:	Epoch: [94][110/408]	Loss 0.0003 (0.0057)	
training:	Epoch: [94][111/408]	Loss 0.0003 (0.0056)	
training:	Epoch: [94][112/408]	Loss 0.0546 (0.0060)	
training:	Epoch: [94][113/408]	Loss 0.0003 (0.0060)	
training:	Epoch: [94][114/408]	Loss 0.0004 (0.0059)	
training:	Epoch: [94][115/408]	Loss 0.0006 (0.0059)	
training:	Epoch: [94][116/408]	Loss 0.0002 (0.0058)	
training:	Epoch: [94][117/408]	Loss 0.0002 (0.0058)	
training:	Epoch: [94][118/408]	Loss 0.0002 (0.0057)	
training:	Epoch: [94][119/408]	Loss 0.0010 (0.0057)	
training:	Epoch: [94][120/408]	Loss 0.0002 (0.0057)	
training:	Epoch: [94][121/408]	Loss 0.0002 (0.0056)	
training:	Epoch: [94][122/408]	Loss 0.0002 (0.0056)	
training:	Epoch: [94][123/408]	Loss 0.0361 (0.0058)	
training:	Epoch: [94][124/408]	Loss 0.0003 (0.0058)	
training:	Epoch: [94][125/408]	Loss 0.0002 (0.0057)	
training:	Epoch: [94][126/408]	Loss 0.0008 (0.0057)	
training:	Epoch: [94][127/408]	Loss 0.0003 (0.0057)	
training:	Epoch: [94][128/408]	Loss 0.0002 (0.0056)	
training:	Epoch: [94][129/408]	Loss 0.0002 (0.0056)	
training:	Epoch: [94][130/408]	Loss 0.0002 (0.0055)	
training:	Epoch: [94][131/408]	Loss 0.0023 (0.0055)	
training:	Epoch: [94][132/408]	Loss 0.0005 (0.0055)	
training:	Epoch: [94][133/408]	Loss 0.0002 (0.0054)	
training:	Epoch: [94][134/408]	Loss 0.0008 (0.0054)	
training:	Epoch: [94][135/408]	Loss 0.2821 (0.0074)	
training:	Epoch: [94][136/408]	Loss 0.0013 (0.0074)	
training:	Epoch: [94][137/408]	Loss 0.0004 (0.0073)	
training:	Epoch: [94][138/408]	Loss 0.0002 (0.0073)	
training:	Epoch: [94][139/408]	Loss 0.0163 (0.0074)	
training:	Epoch: [94][140/408]	Loss 0.0002 (0.0073)	
training:	Epoch: [94][141/408]	Loss 0.0003 (0.0073)	
training:	Epoch: [94][142/408]	Loss 0.0003 (0.0072)	
training:	Epoch: [94][143/408]	Loss 0.0878 (0.0078)	
training:	Epoch: [94][144/408]	Loss 0.0004 (0.0077)	
training:	Epoch: [94][145/408]	Loss 0.0003 (0.0077)	
training:	Epoch: [94][146/408]	Loss 0.0008 (0.0076)	
training:	Epoch: [94][147/408]	Loss 0.0004 (0.0076)	
training:	Epoch: [94][148/408]	Loss 0.0003 (0.0075)	
training:	Epoch: [94][149/408]	Loss 0.0006 (0.0075)	
training:	Epoch: [94][150/408]	Loss 0.0008 (0.0074)	
training:	Epoch: [94][151/408]	Loss 0.0020 (0.0074)	
training:	Epoch: [94][152/408]	Loss 0.0005 (0.0073)	
training:	Epoch: [94][153/408]	Loss 0.0003 (0.0073)	
training:	Epoch: [94][154/408]	Loss 0.0002 (0.0073)	
training:	Epoch: [94][155/408]	Loss 0.0005 (0.0072)	
training:	Epoch: [94][156/408]	Loss 0.0002 (0.0072)	
training:	Epoch: [94][157/408]	Loss 0.0005 (0.0071)	
training:	Epoch: [94][158/408]	Loss 0.0004 (0.0071)	
training:	Epoch: [94][159/408]	Loss 0.0003 (0.0070)	
training:	Epoch: [94][160/408]	Loss 0.0009 (0.0070)	
training:	Epoch: [94][161/408]	Loss 0.0003 (0.0070)	
training:	Epoch: [94][162/408]	Loss 0.0021 (0.0069)	
training:	Epoch: [94][163/408]	Loss 0.0003 (0.0069)	
training:	Epoch: [94][164/408]	Loss 0.0005 (0.0068)	
training:	Epoch: [94][165/408]	Loss 0.0002 (0.0068)	
training:	Epoch: [94][166/408]	Loss 0.0002 (0.0068)	
training:	Epoch: [94][167/408]	Loss 0.0005 (0.0067)	
training:	Epoch: [94][168/408]	Loss 0.0004 (0.0067)	
training:	Epoch: [94][169/408]	Loss 0.0007 (0.0067)	
training:	Epoch: [94][170/408]	Loss 0.0004 (0.0066)	
training:	Epoch: [94][171/408]	Loss 0.0003 (0.0066)	
training:	Epoch: [94][172/408]	Loss 0.0004 (0.0065)	
training:	Epoch: [94][173/408]	Loss 0.0002 (0.0065)	
training:	Epoch: [94][174/408]	Loss 0.0022 (0.0065)	
training:	Epoch: [94][175/408]	Loss 0.0004 (0.0065)	
training:	Epoch: [94][176/408]	Loss 0.0002 (0.0064)	
training:	Epoch: [94][177/408]	Loss 0.0002 (0.0064)	
training:	Epoch: [94][178/408]	Loss 0.0004 (0.0063)	
training:	Epoch: [94][179/408]	Loss 0.0002 (0.0063)	
training:	Epoch: [94][180/408]	Loss 0.0002 (0.0063)	
training:	Epoch: [94][181/408]	Loss 0.0003 (0.0062)	
training:	Epoch: [94][182/408]	Loss 0.0003 (0.0062)	
training:	Epoch: [94][183/408]	Loss 0.0004 (0.0062)	
training:	Epoch: [94][184/408]	Loss 0.0003 (0.0062)	
training:	Epoch: [94][185/408]	Loss 0.0007 (0.0061)	
training:	Epoch: [94][186/408]	Loss 0.0011 (0.0061)	
training:	Epoch: [94][187/408]	Loss 0.0004 (0.0061)	
training:	Epoch: [94][188/408]	Loss 0.0002 (0.0060)	
training:	Epoch: [94][189/408]	Loss 0.0007 (0.0060)	
training:	Epoch: [94][190/408]	Loss 0.0007 (0.0060)	
training:	Epoch: [94][191/408]	Loss 0.3455 (0.0078)	
training:	Epoch: [94][192/408]	Loss 0.0002 (0.0077)	
training:	Epoch: [94][193/408]	Loss 0.0016 (0.0077)	
training:	Epoch: [94][194/408]	Loss 0.0002 (0.0076)	
training:	Epoch: [94][195/408]	Loss 0.0005 (0.0076)	
training:	Epoch: [94][196/408]	Loss 0.0002 (0.0076)	
training:	Epoch: [94][197/408]	Loss 0.0002 (0.0075)	
training:	Epoch: [94][198/408]	Loss 0.0004 (0.0075)	
training:	Epoch: [94][199/408]	Loss 0.0065 (0.0075)	
training:	Epoch: [94][200/408]	Loss 0.0008 (0.0075)	
training:	Epoch: [94][201/408]	Loss 0.0017 (0.0074)	
training:	Epoch: [94][202/408]	Loss 0.0002 (0.0074)	
training:	Epoch: [94][203/408]	Loss 0.0003 (0.0074)	
training:	Epoch: [94][204/408]	Loss 0.0019 (0.0073)	
training:	Epoch: [94][205/408]	Loss 0.0004 (0.0073)	
training:	Epoch: [94][206/408]	Loss 0.0002 (0.0073)	
training:	Epoch: [94][207/408]	Loss 0.0003 (0.0072)	
training:	Epoch: [94][208/408]	Loss 0.0002 (0.0072)	
training:	Epoch: [94][209/408]	Loss 0.0080 (0.0072)	
training:	Epoch: [94][210/408]	Loss 0.0060 (0.0072)	
training:	Epoch: [94][211/408]	Loss 0.0002 (0.0072)	
training:	Epoch: [94][212/408]	Loss 0.0007 (0.0071)	
training:	Epoch: [94][213/408]	Loss 0.0023 (0.0071)	
training:	Epoch: [94][214/408]	Loss 0.0004 (0.0071)	
training:	Epoch: [94][215/408]	Loss 0.0003 (0.0070)	
training:	Epoch: [94][216/408]	Loss 0.0006 (0.0070)	
training:	Epoch: [94][217/408]	Loss 0.0002 (0.0070)	
training:	Epoch: [94][218/408]	Loss 0.0695 (0.0073)	
training:	Epoch: [94][219/408]	Loss 0.0002 (0.0072)	
training:	Epoch: [94][220/408]	Loss 0.0002 (0.0072)	
training:	Epoch: [94][221/408]	Loss 0.0010 (0.0072)	
training:	Epoch: [94][222/408]	Loss 0.0008 (0.0071)	
training:	Epoch: [94][223/408]	Loss 0.0039 (0.0071)	
training:	Epoch: [94][224/408]	Loss 0.0003 (0.0071)	
training:	Epoch: [94][225/408]	Loss 0.0036 (0.0071)	
training:	Epoch: [94][226/408]	Loss 0.0002 (0.0071)	
training:	Epoch: [94][227/408]	Loss 0.0003 (0.0070)	
training:	Epoch: [94][228/408]	Loss 0.0004 (0.0070)	
training:	Epoch: [94][229/408]	Loss 0.0002 (0.0070)	
training:	Epoch: [94][230/408]	Loss 0.0006 (0.0069)	
training:	Epoch: [94][231/408]	Loss 0.0002 (0.0069)	
training:	Epoch: [94][232/408]	Loss 0.0005 (0.0069)	
training:	Epoch: [94][233/408]	Loss 0.0007 (0.0069)	
training:	Epoch: [94][234/408]	Loss 0.0135 (0.0069)	
training:	Epoch: [94][235/408]	Loss 0.0010 (0.0069)	
training:	Epoch: [94][236/408]	Loss 0.0006 (0.0068)	
training:	Epoch: [94][237/408]	Loss 0.0004 (0.0068)	
training:	Epoch: [94][238/408]	Loss 0.2964 (0.0080)	
training:	Epoch: [94][239/408]	Loss 0.0024 (0.0080)	
training:	Epoch: [94][240/408]	Loss 0.0002 (0.0080)	
training:	Epoch: [94][241/408]	Loss 0.0002 (0.0079)	
training:	Epoch: [94][242/408]	Loss 0.0009 (0.0079)	
training:	Epoch: [94][243/408]	Loss 0.0007 (0.0079)	
training:	Epoch: [94][244/408]	Loss 0.0007 (0.0078)	
training:	Epoch: [94][245/408]	Loss 0.0003 (0.0078)	
training:	Epoch: [94][246/408]	Loss 0.0008 (0.0078)	
training:	Epoch: [94][247/408]	Loss 0.0004 (0.0078)	
training:	Epoch: [94][248/408]	Loss 0.0005 (0.0077)	
training:	Epoch: [94][249/408]	Loss 0.0004 (0.0077)	
training:	Epoch: [94][250/408]	Loss 0.0004 (0.0077)	
training:	Epoch: [94][251/408]	Loss 0.0003 (0.0076)	
training:	Epoch: [94][252/408]	Loss 0.0003 (0.0076)	
training:	Epoch: [94][253/408]	Loss 0.0432 (0.0078)	
training:	Epoch: [94][254/408]	Loss 0.0002 (0.0077)	
training:	Epoch: [94][255/408]	Loss 0.0008 (0.0077)	
training:	Epoch: [94][256/408]	Loss 0.0002 (0.0077)	
training:	Epoch: [94][257/408]	Loss 0.0004 (0.0076)	
training:	Epoch: [94][258/408]	Loss 0.0326 (0.0077)	
training:	Epoch: [94][259/408]	Loss 0.0009 (0.0077)	
training:	Epoch: [94][260/408]	Loss 0.0004 (0.0077)	
training:	Epoch: [94][261/408]	Loss 0.0002 (0.0077)	
training:	Epoch: [94][262/408]	Loss 0.0004 (0.0076)	
training:	Epoch: [94][263/408]	Loss 0.0003 (0.0076)	
training:	Epoch: [94][264/408]	Loss 0.0002 (0.0076)	
training:	Epoch: [94][265/408]	Loss 0.0004 (0.0075)	
training:	Epoch: [94][266/408]	Loss 0.0007 (0.0075)	
training:	Epoch: [94][267/408]	Loss 0.0003 (0.0075)	
training:	Epoch: [94][268/408]	Loss 0.0003 (0.0075)	
training:	Epoch: [94][269/408]	Loss 0.0021 (0.0074)	
training:	Epoch: [94][270/408]	Loss 0.0003 (0.0074)	
training:	Epoch: [94][271/408]	Loss 0.0003 (0.0074)	
training:	Epoch: [94][272/408]	Loss 0.0009 (0.0074)	
training:	Epoch: [94][273/408]	Loss 0.0003 (0.0073)	
training:	Epoch: [94][274/408]	Loss 0.0002 (0.0073)	
training:	Epoch: [94][275/408]	Loss 0.0012 (0.0073)	
training:	Epoch: [94][276/408]	Loss 0.0002 (0.0073)	
training:	Epoch: [94][277/408]	Loss 0.0003 (0.0072)	
training:	Epoch: [94][278/408]	Loss 0.0003 (0.0072)	
training:	Epoch: [94][279/408]	Loss 0.0002 (0.0072)	
training:	Epoch: [94][280/408]	Loss 0.0003 (0.0072)	
training:	Epoch: [94][281/408]	Loss 0.0006 (0.0071)	
training:	Epoch: [94][282/408]	Loss 0.0007 (0.0071)	
training:	Epoch: [94][283/408]	Loss 0.0007 (0.0071)	
training:	Epoch: [94][284/408]	Loss 0.0141 (0.0071)	
training:	Epoch: [94][285/408]	Loss 0.0002 (0.0071)	
training:	Epoch: [94][286/408]	Loss 0.0005 (0.0071)	
training:	Epoch: [94][287/408]	Loss 0.0003 (0.0071)	
training:	Epoch: [94][288/408]	Loss 0.0003 (0.0070)	
training:	Epoch: [94][289/408]	Loss 0.0005 (0.0070)	
training:	Epoch: [94][290/408]	Loss 0.0008 (0.0070)	
training:	Epoch: [94][291/408]	Loss 0.0187 (0.0070)	
training:	Epoch: [94][292/408]	Loss 0.0003 (0.0070)	
training:	Epoch: [94][293/408]	Loss 0.0012 (0.0070)	
training:	Epoch: [94][294/408]	Loss 0.0002 (0.0070)	
training:	Epoch: [94][295/408]	Loss 0.0002 (0.0069)	
training:	Epoch: [94][296/408]	Loss 0.0004 (0.0069)	
training:	Epoch: [94][297/408]	Loss 0.0002 (0.0069)	
training:	Epoch: [94][298/408]	Loss 0.0004 (0.0069)	
training:	Epoch: [94][299/408]	Loss 0.0002 (0.0068)	
training:	Epoch: [94][300/408]	Loss 0.0003 (0.0068)	
training:	Epoch: [94][301/408]	Loss 0.0006 (0.0068)	
training:	Epoch: [94][302/408]	Loss 0.0005 (0.0068)	
training:	Epoch: [94][303/408]	Loss 0.0042 (0.0068)	
training:	Epoch: [94][304/408]	Loss 0.0002 (0.0068)	
training:	Epoch: [94][305/408]	Loss 0.0002 (0.0067)	
training:	Epoch: [94][306/408]	Loss 0.0002 (0.0067)	
training:	Epoch: [94][307/408]	Loss 0.0003 (0.0067)	
training:	Epoch: [94][308/408]	Loss 0.0002 (0.0067)	
training:	Epoch: [94][309/408]	Loss 0.0006 (0.0066)	
training:	Epoch: [94][310/408]	Loss 0.0002 (0.0066)	
training:	Epoch: [94][311/408]	Loss 0.0005 (0.0066)	
training:	Epoch: [94][312/408]	Loss 0.0002 (0.0066)	
training:	Epoch: [94][313/408]	Loss 0.0002 (0.0066)	
training:	Epoch: [94][314/408]	Loss 0.0002 (0.0065)	
training:	Epoch: [94][315/408]	Loss 0.0003 (0.0065)	
training:	Epoch: [94][316/408]	Loss 0.0006 (0.0065)	
training:	Epoch: [94][317/408]	Loss 0.0060 (0.0065)	
training:	Epoch: [94][318/408]	Loss 0.0004 (0.0065)	
training:	Epoch: [94][319/408]	Loss 0.0201 (0.0065)	
training:	Epoch: [94][320/408]	Loss 0.0013 (0.0065)	
training:	Epoch: [94][321/408]	Loss 0.0005 (0.0065)	
training:	Epoch: [94][322/408]	Loss 0.0003 (0.0065)	
training:	Epoch: [94][323/408]	Loss 0.0002 (0.0065)	
training:	Epoch: [94][324/408]	Loss 0.0011 (0.0064)	
training:	Epoch: [94][325/408]	Loss 0.0002 (0.0064)	
training:	Epoch: [94][326/408]	Loss 0.0002 (0.0064)	
training:	Epoch: [94][327/408]	Loss 0.0002 (0.0064)	
training:	Epoch: [94][328/408]	Loss 0.0023 (0.0064)	
training:	Epoch: [94][329/408]	Loss 0.0009 (0.0064)	
training:	Epoch: [94][330/408]	Loss 0.0032 (0.0063)	
training:	Epoch: [94][331/408]	Loss 0.0003 (0.0063)	
training:	Epoch: [94][332/408]	Loss 0.0002 (0.0063)	
training:	Epoch: [94][333/408]	Loss 0.0148 (0.0063)	
training:	Epoch: [94][334/408]	Loss 0.0009 (0.0063)	
training:	Epoch: [94][335/408]	Loss 0.0068 (0.0063)	
training:	Epoch: [94][336/408]	Loss 0.0909 (0.0066)	
training:	Epoch: [94][337/408]	Loss 0.0007 (0.0066)	
training:	Epoch: [94][338/408]	Loss 0.0005 (0.0065)	
training:	Epoch: [94][339/408]	Loss 0.0002 (0.0065)	
training:	Epoch: [94][340/408]	Loss 0.0002 (0.0065)	
training:	Epoch: [94][341/408]	Loss 0.0019 (0.0065)	
training:	Epoch: [94][342/408]	Loss 0.0017 (0.0065)	
training:	Epoch: [94][343/408]	Loss 0.0008 (0.0065)	
training:	Epoch: [94][344/408]	Loss 0.0002 (0.0064)	
training:	Epoch: [94][345/408]	Loss 0.0934 (0.0067)	
training:	Epoch: [94][346/408]	Loss 0.0003 (0.0067)	
training:	Epoch: [94][347/408]	Loss 0.0102 (0.0067)	
training:	Epoch: [94][348/408]	Loss 0.0906 (0.0069)	
training:	Epoch: [94][349/408]	Loss 0.0003 (0.0069)	
training:	Epoch: [94][350/408]	Loss 0.0007 (0.0069)	
training:	Epoch: [94][351/408]	Loss 0.0019 (0.0069)	
training:	Epoch: [94][352/408]	Loss 0.0002 (0.0068)	
training:	Epoch: [94][353/408]	Loss 0.0002 (0.0068)	
training:	Epoch: [94][354/408]	Loss 0.0116 (0.0068)	
training:	Epoch: [94][355/408]	Loss 0.0025 (0.0068)	
training:	Epoch: [94][356/408]	Loss 0.0002 (0.0068)	
training:	Epoch: [94][357/408]	Loss 0.0002 (0.0068)	
training:	Epoch: [94][358/408]	Loss 0.0002 (0.0068)	
training:	Epoch: [94][359/408]	Loss 0.0005 (0.0068)	
training:	Epoch: [94][360/408]	Loss 0.0008 (0.0067)	
training:	Epoch: [94][361/408]	Loss 0.0012 (0.0067)	
training:	Epoch: [94][362/408]	Loss 0.0234 (0.0068)	
training:	Epoch: [94][363/408]	Loss 0.0004 (0.0068)	
training:	Epoch: [94][364/408]	Loss 0.0002 (0.0067)	
training:	Epoch: [94][365/408]	Loss 0.0002 (0.0067)	
training:	Epoch: [94][366/408]	Loss 0.0002 (0.0067)	
training:	Epoch: [94][367/408]	Loss 0.0002 (0.0067)	
training:	Epoch: [94][368/408]	Loss 0.0002 (0.0067)	
