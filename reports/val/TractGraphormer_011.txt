Namespace(inputDirectory='data', outputDirectory='val', DisFile='data/dis_sort_roi2dis2000.npy', task='sex', type='binary', feature=['Nos', 'FA1'], CUDA_id='0', data_id='0', dataset='Classification', norm=True, channels=1, epochs=200, tensorboard=True, net_architecture='TractGraphormer', batch_size=8, rate=3e-05, weight=0.0, sched_step=300, sched_gamma=0.1, printing_frequency=1, seed=1, alpha=0, remix_kappa=0, remix_tau=0, loss='CE', sigma=0, k=20)
Training the 'TractGraphormer' architecture

The following parameters are used:
Batch size:	8
Number of workers:	0
Learning rate:	3e-05
Weight decay:	0.0
Scheduler steps:	300
Scheduler gamma:	0.1
Number of epochs of training:	200
Number of input channels:	1

Data preparation
Reading data from:	./data
Training set size:	6538
Validation set size:	1869

Performing calculations on:	cuda:0

Pretraining:	Epoch 1/200
----------
training:	Epoch: [1][1/817]	Loss 0.7404 (0.7404)	
training:	Epoch: [1][2/817]	Loss 0.6713 (0.7058)	
training:	Epoch: [1][3/817]	Loss 0.6529 (0.6882)	
training:	Epoch: [1][4/817]	Loss 0.7128 (0.6943)	
training:	Epoch: [1][5/817]	Loss 0.7237 (0.7002)	
training:	Epoch: [1][6/817]	Loss 0.6773 (0.6964)	
training:	Epoch: [1][7/817]	Loss 0.7189 (0.6996)	
training:	Epoch: [1][8/817]	Loss 0.7519 (0.7061)	
training:	Epoch: [1][9/817]	Loss 0.7052 (0.7060)	
training:	Epoch: [1][10/817]	Loss 0.7256 (0.7080)	
training:	Epoch: [1][11/817]	Loss 0.7141 (0.7085)	
training:	Epoch: [1][12/817]	Loss 0.6633 (0.7048)	
training:	Epoch: [1][13/817]	Loss 0.6860 (0.7033)	
training:	Epoch: [1][14/817]	Loss 0.7162 (0.7042)	
training:	Epoch: [1][15/817]	Loss 0.6685 (0.7019)	
training:	Epoch: [1][16/817]	Loss 0.6850 (0.7008)	
training:	Epoch: [1][17/817]	Loss 0.7081 (0.7012)	
training:	Epoch: [1][18/817]	Loss 0.7138 (0.7019)	
training:	Epoch: [1][19/817]	Loss 0.6604 (0.6997)	
training:	Epoch: [1][20/817]	Loss 0.7016 (0.6998)	
training:	Epoch: [1][21/817]	Loss 0.6936 (0.6995)	
training:	Epoch: [1][22/817]	Loss 0.6861 (0.6989)	
training:	Epoch: [1][23/817]	Loss 0.6786 (0.6980)	
training:	Epoch: [1][24/817]	Loss 0.6878 (0.6976)	
training:	Epoch: [1][25/817]	Loss 0.7004 (0.6977)	
training:	Epoch: [1][26/817]	Loss 0.6829 (0.6972)	
training:	Epoch: [1][27/817]	Loss 0.7064 (0.6975)	
training:	Epoch: [1][28/817]	Loss 0.6867 (0.6971)	
training:	Epoch: [1][29/817]	Loss 0.7050 (0.6974)	
training:	Epoch: [1][30/817]	Loss 0.7090 (0.6978)	
training:	Epoch: [1][31/817]	Loss 0.7051 (0.6980)	
training:	Epoch: [1][32/817]	Loss 0.7016 (0.6981)	
training:	Epoch: [1][33/817]	Loss 0.6579 (0.6969)	
training:	Epoch: [1][34/817]	Loss 0.6810 (0.6964)	
training:	Epoch: [1][35/817]	Loss 0.6763 (0.6959)	
training:	Epoch: [1][36/817]	Loss 0.7199 (0.6965)	
training:	Epoch: [1][37/817]	Loss 0.6731 (0.6959)	
training:	Epoch: [1][38/817]	Loss 0.6995 (0.6960)	
training:	Epoch: [1][39/817]	Loss 0.6869 (0.6958)	
training:	Epoch: [1][40/817]	Loss 0.6731 (0.6952)	
training:	Epoch: [1][41/817]	Loss 0.7053 (0.6954)	
training:	Epoch: [1][42/817]	Loss 0.6832 (0.6952)	
training:	Epoch: [1][43/817]	Loss 0.7009 (0.6953)	
training:	Epoch: [1][44/817]	Loss 0.6633 (0.6946)	
training:	Epoch: [1][45/817]	Loss 0.6591 (0.6938)	
training:	Epoch: [1][46/817]	Loss 0.6747 (0.6934)	
training:	Epoch: [1][47/817]	Loss 0.7141 (0.6938)	
training:	Epoch: [1][48/817]	Loss 0.6664 (0.6932)	
training:	Epoch: [1][49/817]	Loss 0.6830 (0.6930)	
training:	Epoch: [1][50/817]	Loss 0.7026 (0.6932)	
training:	Epoch: [1][51/817]	Loss 0.6758 (0.6929)	
training:	Epoch: [1][52/817]	Loss 0.6651 (0.6923)	
training:	Epoch: [1][53/817]	Loss 0.6375 (0.6913)	
training:	Epoch: [1][54/817]	Loss 0.6875 (0.6912)	
training:	Epoch: [1][55/817]	Loss 0.6840 (0.6911)	
training:	Epoch: [1][56/817]	Loss 0.7038 (0.6913)	
training:	Epoch: [1][57/817]	Loss 0.6646 (0.6909)	
training:	Epoch: [1][58/817]	Loss 0.6466 (0.6901)	
training:	Epoch: [1][59/817]	Loss 0.7075 (0.6904)	
training:	Epoch: [1][60/817]	Loss 0.6308 (0.6894)	
training:	Epoch: [1][61/817]	Loss 0.7001 (0.6896)	
training:	Epoch: [1][62/817]	Loss 0.6648 (0.6892)	
training:	Epoch: [1][63/817]	Loss 0.7497 (0.6901)	
training:	Epoch: [1][64/817]	Loss 0.7604 (0.6912)	
training:	Epoch: [1][65/817]	Loss 0.6703 (0.6909)	
training:	Epoch: [1][66/817]	Loss 0.6762 (0.6907)	
training:	Epoch: [1][67/817]	Loss 0.6748 (0.6904)	
training:	Epoch: [1][68/817]	Loss 0.7814 (0.6918)	
training:	Epoch: [1][69/817]	Loss 0.6885 (0.6917)	
training:	Epoch: [1][70/817]	Loss 0.6756 (0.6915)	
training:	Epoch: [1][71/817]	Loss 0.7016 (0.6916)	
training:	Epoch: [1][72/817]	Loss 0.6432 (0.6910)	
training:	Epoch: [1][73/817]	Loss 0.7300 (0.6915)	
training:	Epoch: [1][74/817]	Loss 0.6002 (0.6903)	
training:	Epoch: [1][75/817]	Loss 0.6114 (0.6892)	
training:	Epoch: [1][76/817]	Loss 0.6872 (0.6892)	
training:	Epoch: [1][77/817]	Loss 0.6575 (0.6888)	
training:	Epoch: [1][78/817]	Loss 0.6173 (0.6879)	
training:	Epoch: [1][79/817]	Loss 0.7056 (0.6881)	
training:	Epoch: [1][80/817]	Loss 0.6733 (0.6879)	
training:	Epoch: [1][81/817]	Loss 0.6976 (0.6880)	
training:	Epoch: [1][82/817]	Loss 0.7212 (0.6884)	
training:	Epoch: [1][83/817]	Loss 0.6297 (0.6877)	
training:	Epoch: [1][84/817]	Loss 0.6417 (0.6872)	
training:	Epoch: [1][85/817]	Loss 0.7235 (0.6876)	
training:	Epoch: [1][86/817]	Loss 0.6720 (0.6874)	
training:	Epoch: [1][87/817]	Loss 0.7237 (0.6878)	
training:	Epoch: [1][88/817]	Loss 0.6799 (0.6877)	
training:	Epoch: [1][89/817]	Loss 0.7448 (0.6884)	
training:	Epoch: [1][90/817]	Loss 0.7532 (0.6891)	
training:	Epoch: [1][91/817]	Loss 0.7522 (0.6898)	
training:	Epoch: [1][92/817]	Loss 0.6445 (0.6893)	
training:	Epoch: [1][93/817]	Loss 0.7419 (0.6899)	
training:	Epoch: [1][94/817]	Loss 0.7274 (0.6903)	
training:	Epoch: [1][95/817]	Loss 0.6648 (0.6900)	
training:	Epoch: [1][96/817]	Loss 0.7130 (0.6902)	
training:	Epoch: [1][97/817]	Loss 0.6637 (0.6900)	
training:	Epoch: [1][98/817]	Loss 0.6593 (0.6897)	
training:	Epoch: [1][99/817]	Loss 0.6719 (0.6895)	
training:	Epoch: [1][100/817]	Loss 0.6845 (0.6894)	
training:	Epoch: [1][101/817]	Loss 0.7467 (0.6900)	
training:	Epoch: [1][102/817]	Loss 0.6693 (0.6898)	
training:	Epoch: [1][103/817]	Loss 0.6476 (0.6894)	
training:	Epoch: [1][104/817]	Loss 0.6525 (0.6890)	
training:	Epoch: [1][105/817]	Loss 0.6477 (0.6886)	
training:	Epoch: [1][106/817]	Loss 0.6614 (0.6884)	
training:	Epoch: [1][107/817]	Loss 0.6944 (0.6884)	
training:	Epoch: [1][108/817]	Loss 0.6754 (0.6883)	
training:	Epoch: [1][109/817]	Loss 0.6849 (0.6883)	
training:	Epoch: [1][110/817]	Loss 0.6967 (0.6884)	
training:	Epoch: [1][111/817]	Loss 0.6856 (0.6883)	
training:	Epoch: [1][112/817]	Loss 0.6341 (0.6879)	
training:	Epoch: [1][113/817]	Loss 0.7312 (0.6882)	
training:	Epoch: [1][114/817]	Loss 0.6672 (0.6881)	
training:	Epoch: [1][115/817]	Loss 0.6922 (0.6881)	
training:	Epoch: [1][116/817]	Loss 0.6420 (0.6877)	
training:	Epoch: [1][117/817]	Loss 0.6771 (0.6876)	
training:	Epoch: [1][118/817]	Loss 0.6952 (0.6877)	
training:	Epoch: [1][119/817]	Loss 0.6878 (0.6877)	
training:	Epoch: [1][120/817]	Loss 0.7200 (0.6879)	
training:	Epoch: [1][121/817]	Loss 0.6098 (0.6873)	
training:	Epoch: [1][122/817]	Loss 0.6729 (0.6872)	
training:	Epoch: [1][123/817]	Loss 0.6586 (0.6869)	
training:	Epoch: [1][124/817]	Loss 0.6637 (0.6868)	
training:	Epoch: [1][125/817]	Loss 0.7044 (0.6869)	
training:	Epoch: [1][126/817]	Loss 0.7233 (0.6872)	
training:	Epoch: [1][127/817]	Loss 0.6944 (0.6872)	
training:	Epoch: [1][128/817]	Loss 0.6529 (0.6870)	
training:	Epoch: [1][129/817]	Loss 0.6904 (0.6870)	
training:	Epoch: [1][130/817]	Loss 0.6368 (0.6866)	
training:	Epoch: [1][131/817]	Loss 0.6421 (0.6863)	
training:	Epoch: [1][132/817]	Loss 0.7428 (0.6867)	
training:	Epoch: [1][133/817]	Loss 0.6654 (0.6865)	
training:	Epoch: [1][134/817]	Loss 0.6853 (0.6865)	
training:	Epoch: [1][135/817]	Loss 0.6307 (0.6861)	
training:	Epoch: [1][136/817]	Loss 0.6453 (0.6858)	
training:	Epoch: [1][137/817]	Loss 0.6851 (0.6858)	
training:	Epoch: [1][138/817]	Loss 0.6321 (0.6854)	
training:	Epoch: [1][139/817]	Loss 0.6685 (0.6853)	
training:	Epoch: [1][140/817]	Loss 0.8017 (0.6861)	
training:	Epoch: [1][141/817]	Loss 0.7035 (0.6863)	
training:	Epoch: [1][142/817]	Loss 0.7057 (0.6864)	
training:	Epoch: [1][143/817]	Loss 0.6949 (0.6865)	
training:	Epoch: [1][144/817]	Loss 0.6640 (0.6863)	
training:	Epoch: [1][145/817]	Loss 0.7195 (0.6865)	
training:	Epoch: [1][146/817]	Loss 0.7064 (0.6867)	
training:	Epoch: [1][147/817]	Loss 0.6375 (0.6863)	
training:	Epoch: [1][148/817]	Loss 0.6968 (0.6864)	
training:	Epoch: [1][149/817]	Loss 0.7491 (0.6868)	
training:	Epoch: [1][150/817]	Loss 0.6049 (0.6863)	
training:	Epoch: [1][151/817]	Loss 0.6733 (0.6862)	
training:	Epoch: [1][152/817]	Loss 0.6528 (0.6860)	
training:	Epoch: [1][153/817]	Loss 0.6667 (0.6858)	
training:	Epoch: [1][154/817]	Loss 0.6751 (0.6858)	
training:	Epoch: [1][155/817]	Loss 0.7269 (0.6860)	
training:	Epoch: [1][156/817]	Loss 0.6945 (0.6861)	
training:	Epoch: [1][157/817]	Loss 0.6726 (0.6860)	
training:	Epoch: [1][158/817]	Loss 0.5831 (0.6854)	
training:	Epoch: [1][159/817]	Loss 0.6637 (0.6852)	
training:	Epoch: [1][160/817]	Loss 0.6648 (0.6851)	
training:	Epoch: [1][161/817]	Loss 0.6319 (0.6848)	
training:	Epoch: [1][162/817]	Loss 0.6205 (0.6844)	
training:	Epoch: [1][163/817]	Loss 0.7307 (0.6846)	
training:	Epoch: [1][164/817]	Loss 0.6964 (0.6847)	
training:	Epoch: [1][165/817]	Loss 0.6503 (0.6845)	
training:	Epoch: [1][166/817]	Loss 0.6478 (0.6843)	
training:	Epoch: [1][167/817]	Loss 0.6421 (0.6840)	
training:	Epoch: [1][168/817]	Loss 0.6467 (0.6838)	
training:	Epoch: [1][169/817]	Loss 0.6273 (0.6835)	
training:	Epoch: [1][170/817]	Loss 0.6159 (0.6831)	
training:	Epoch: [1][171/817]	Loss 0.6589 (0.6829)	
training:	Epoch: [1][172/817]	Loss 0.6313 (0.6826)	
training:	Epoch: [1][173/817]	Loss 0.6384 (0.6824)	
training:	Epoch: [1][174/817]	Loss 0.6062 (0.6819)	
training:	Epoch: [1][175/817]	Loss 0.7408 (0.6823)	
training:	Epoch: [1][176/817]	Loss 0.7257 (0.6825)	
training:	Epoch: [1][177/817]	Loss 0.6652 (0.6824)	
training:	Epoch: [1][178/817]	Loss 0.5996 (0.6820)	
training:	Epoch: [1][179/817]	Loss 0.8037 (0.6826)	
training:	Epoch: [1][180/817]	Loss 0.6211 (0.6823)	
training:	Epoch: [1][181/817]	Loss 0.6221 (0.6820)	
training:	Epoch: [1][182/817]	Loss 0.6377 (0.6817)	
training:	Epoch: [1][183/817]	Loss 0.7502 (0.6821)	
training:	Epoch: [1][184/817]	Loss 0.5678 (0.6815)	
training:	Epoch: [1][185/817]	Loss 0.5776 (0.6809)	
training:	Epoch: [1][186/817]	Loss 0.5964 (0.6805)	
training:	Epoch: [1][187/817]	Loss 0.5904 (0.6800)	
training:	Epoch: [1][188/817]	Loss 0.7784 (0.6805)	
training:	Epoch: [1][189/817]	Loss 0.6593 (0.6804)	
training:	Epoch: [1][190/817]	Loss 0.7090 (0.6805)	
training:	Epoch: [1][191/817]	Loss 0.5936 (0.6801)	
training:	Epoch: [1][192/817]	Loss 0.6523 (0.6799)	
training:	Epoch: [1][193/817]	Loss 0.5720 (0.6794)	
training:	Epoch: [1][194/817]	Loss 0.6309 (0.6791)	
training:	Epoch: [1][195/817]	Loss 0.6873 (0.6792)	
training:	Epoch: [1][196/817]	Loss 0.6862 (0.6792)	
training:	Epoch: [1][197/817]	Loss 0.6401 (0.6790)	
training:	Epoch: [1][198/817]	Loss 0.5529 (0.6784)	
training:	Epoch: [1][199/817]	Loss 0.6472 (0.6782)	
training:	Epoch: [1][200/817]	Loss 0.6302 (0.6780)	
training:	Epoch: [1][201/817]	Loss 0.6200 (0.6777)	
training:	Epoch: [1][202/817]	Loss 0.5727 (0.6772)	
training:	Epoch: [1][203/817]	Loss 0.6593 (0.6771)	
training:	Epoch: [1][204/817]	Loss 0.6542 (0.6770)	
training:	Epoch: [1][205/817]	Loss 0.5624 (0.6764)	
training:	Epoch: [1][206/817]	Loss 0.6927 (0.6765)	
training:	Epoch: [1][207/817]	Loss 0.7406 (0.6768)	
training:	Epoch: [1][208/817]	Loss 0.7567 (0.6772)	
training:	Epoch: [1][209/817]	Loss 0.5886 (0.6768)	
training:	Epoch: [1][210/817]	Loss 0.6813 (0.6768)	
training:	Epoch: [1][211/817]	Loss 0.8714 (0.6777)	
training:	Epoch: [1][212/817]	Loss 0.5949 (0.6773)	
training:	Epoch: [1][213/817]	Loss 0.5850 (0.6769)	
training:	Epoch: [1][214/817]	Loss 0.6860 (0.6769)	
training:	Epoch: [1][215/817]	Loss 0.6886 (0.6770)	
training:	Epoch: [1][216/817]	Loss 0.6445 (0.6768)	
training:	Epoch: [1][217/817]	Loss 0.7062 (0.6770)	
training:	Epoch: [1][218/817]	Loss 0.5700 (0.6765)	
training:	Epoch: [1][219/817]	Loss 0.6602 (0.6764)	
training:	Epoch: [1][220/817]	Loss 0.6007 (0.6761)	
training:	Epoch: [1][221/817]	Loss 0.6026 (0.6757)	
training:	Epoch: [1][222/817]	Loss 0.5884 (0.6753)	
training:	Epoch: [1][223/817]	Loss 0.6139 (0.6751)	
training:	Epoch: [1][224/817]	Loss 0.6596 (0.6750)	
training:	Epoch: [1][225/817]	Loss 0.6014 (0.6747)	
training:	Epoch: [1][226/817]	Loss 0.7318 (0.6749)	
training:	Epoch: [1][227/817]	Loss 0.5264 (0.6743)	
training:	Epoch: [1][228/817]	Loss 0.5694 (0.6738)	
training:	Epoch: [1][229/817]	Loss 0.6442 (0.6737)	
training:	Epoch: [1][230/817]	Loss 0.7253 (0.6739)	
training:	Epoch: [1][231/817]	Loss 0.6720 (0.6739)	
training:	Epoch: [1][232/817]	Loss 0.4996 (0.6731)	
training:	Epoch: [1][233/817]	Loss 0.8033 (0.6737)	
training:	Epoch: [1][234/817]	Loss 0.6640 (0.6736)	
training:	Epoch: [1][235/817]	Loss 0.6936 (0.6737)	
training:	Epoch: [1][236/817]	Loss 0.7731 (0.6742)	
training:	Epoch: [1][237/817]	Loss 0.4855 (0.6734)	
training:	Epoch: [1][238/817]	Loss 0.6848 (0.6734)	
training:	Epoch: [1][239/817]	Loss 0.5833 (0.6730)	
training:	Epoch: [1][240/817]	Loss 0.6448 (0.6729)	
training:	Epoch: [1][241/817]	Loss 0.7536 (0.6732)	
training:	Epoch: [1][242/817]	Loss 0.5316 (0.6727)	
training:	Epoch: [1][243/817]	Loss 0.6223 (0.6725)	
training:	Epoch: [1][244/817]	Loss 0.6524 (0.6724)	
training:	Epoch: [1][245/817]	Loss 0.6517 (0.6723)	
training:	Epoch: [1][246/817]	Loss 0.6520 (0.6722)	
training:	Epoch: [1][247/817]	Loss 0.5419 (0.6717)	
training:	Epoch: [1][248/817]	Loss 0.5879 (0.6713)	
training:	Epoch: [1][249/817]	Loss 0.6958 (0.6714)	
training:	Epoch: [1][250/817]	Loss 0.5896 (0.6711)	
training:	Epoch: [1][251/817]	Loss 0.7770 (0.6715)	
training:	Epoch: [1][252/817]	Loss 0.7144 (0.6717)	
training:	Epoch: [1][253/817]	Loss 0.6230 (0.6715)	
training:	Epoch: [1][254/817]	Loss 0.5509 (0.6710)	
training:	Epoch: [1][255/817]	Loss 0.7072 (0.6712)	
training:	Epoch: [1][256/817]	Loss 0.6132 (0.6710)	
training:	Epoch: [1][257/817]	Loss 0.5249 (0.6704)	
training:	Epoch: [1][258/817]	Loss 0.6319 (0.6702)	
training:	Epoch: [1][259/817]	Loss 0.6580 (0.6702)	
training:	Epoch: [1][260/817]	Loss 0.5415 (0.6697)	
training:	Epoch: [1][261/817]	Loss 0.5854 (0.6694)	
training:	Epoch: [1][262/817]	Loss 0.6898 (0.6694)	
training:	Epoch: [1][263/817]	Loss 0.5874 (0.6691)	
training:	Epoch: [1][264/817]	Loss 0.6182 (0.6689)	
training:	Epoch: [1][265/817]	Loss 0.4852 (0.6682)	
training:	Epoch: [1][266/817]	Loss 0.6077 (0.6680)	
training:	Epoch: [1][267/817]	Loss 0.5803 (0.6677)	
training:	Epoch: [1][268/817]	Loss 0.5108 (0.6671)	
training:	Epoch: [1][269/817]	Loss 0.5696 (0.6667)	
training:	Epoch: [1][270/817]	Loss 0.6453 (0.6667)	
training:	Epoch: [1][271/817]	Loss 0.5682 (0.6663)	
training:	Epoch: [1][272/817]	Loss 0.5304 (0.6658)	
training:	Epoch: [1][273/817]	Loss 0.7297 (0.6660)	
training:	Epoch: [1][274/817]	Loss 0.7944 (0.6665)	
training:	Epoch: [1][275/817]	Loss 0.5063 (0.6659)	
training:	Epoch: [1][276/817]	Loss 0.6410 (0.6658)	
training:	Epoch: [1][277/817]	Loss 0.6464 (0.6658)	
training:	Epoch: [1][278/817]	Loss 0.5849 (0.6655)	
training:	Epoch: [1][279/817]	Loss 0.6863 (0.6655)	
training:	Epoch: [1][280/817]	Loss 0.3689 (0.6645)	
training:	Epoch: [1][281/817]	Loss 0.6094 (0.6643)	
training:	Epoch: [1][282/817]	Loss 0.6645 (0.6643)	
training:	Epoch: [1][283/817]	Loss 0.7345 (0.6645)	
training:	Epoch: [1][284/817]	Loss 0.5173 (0.6640)	
training:	Epoch: [1][285/817]	Loss 0.5985 (0.6638)	
training:	Epoch: [1][286/817]	Loss 0.5592 (0.6634)	
training:	Epoch: [1][287/817]	Loss 0.6520 (0.6634)	
training:	Epoch: [1][288/817]	Loss 0.5316 (0.6629)	
training:	Epoch: [1][289/817]	Loss 0.4642 (0.6622)	
training:	Epoch: [1][290/817]	Loss 0.5145 (0.6617)	
training:	Epoch: [1][291/817]	Loss 0.6839 (0.6618)	
training:	Epoch: [1][292/817]	Loss 0.5278 (0.6613)	
training:	Epoch: [1][293/817]	Loss 0.5125 (0.6608)	
training:	Epoch: [1][294/817]	Loss 0.6145 (0.6607)	
training:	Epoch: [1][295/817]	Loss 0.5105 (0.6602)	
training:	Epoch: [1][296/817]	Loss 0.5758 (0.6599)	
training:	Epoch: [1][297/817]	Loss 0.5484 (0.6595)	
training:	Epoch: [1][298/817]	Loss 0.6277 (0.6594)	
training:	Epoch: [1][299/817]	Loss 0.5104 (0.6589)	
training:	Epoch: [1][300/817]	Loss 0.4667 (0.6583)	
training:	Epoch: [1][301/817]	Loss 0.6582 (0.6583)	
training:	Epoch: [1][302/817]	Loss 0.4498 (0.6576)	
training:	Epoch: [1][303/817]	Loss 0.5442 (0.6572)	
training:	Epoch: [1][304/817]	Loss 0.4761 (0.6566)	
training:	Epoch: [1][305/817]	Loss 0.5752 (0.6563)	
training:	Epoch: [1][306/817]	Loss 0.7895 (0.6568)	
training:	Epoch: [1][307/817]	Loss 0.4978 (0.6563)	
training:	Epoch: [1][308/817]	Loss 0.7072 (0.6564)	
training:	Epoch: [1][309/817]	Loss 0.7528 (0.6567)	
training:	Epoch: [1][310/817]	Loss 0.7306 (0.6570)	
training:	Epoch: [1][311/817]	Loss 0.7409 (0.6572)	
training:	Epoch: [1][312/817]	Loss 0.5442 (0.6569)	
training:	Epoch: [1][313/817]	Loss 0.6148 (0.6567)	
training:	Epoch: [1][314/817]	Loss 0.5212 (0.6563)	
training:	Epoch: [1][315/817]	Loss 0.5131 (0.6559)	
training:	Epoch: [1][316/817]	Loss 0.5430 (0.6555)	
training:	Epoch: [1][317/817]	Loss 0.5429 (0.6551)	
training:	Epoch: [1][318/817]	Loss 0.8274 (0.6557)	
training:	Epoch: [1][319/817]	Loss 0.3887 (0.6549)	
training:	Epoch: [1][320/817]	Loss 0.6676 (0.6549)	
training:	Epoch: [1][321/817]	Loss 0.5189 (0.6545)	
training:	Epoch: [1][322/817]	Loss 0.5643 (0.6542)	
training:	Epoch: [1][323/817]	Loss 0.5648 (0.6539)	
training:	Epoch: [1][324/817]	Loss 0.5318 (0.6535)	
training:	Epoch: [1][325/817]	Loss 0.5288 (0.6531)	
training:	Epoch: [1][326/817]	Loss 0.6686 (0.6532)	
training:	Epoch: [1][327/817]	Loss 0.5730 (0.6530)	
training:	Epoch: [1][328/817]	Loss 0.6913 (0.6531)	
training:	Epoch: [1][329/817]	Loss 0.7346 (0.6533)	
training:	Epoch: [1][330/817]	Loss 0.4287 (0.6526)	
training:	Epoch: [1][331/817]	Loss 0.5615 (0.6524)	
training:	Epoch: [1][332/817]	Loss 0.5088 (0.6519)	
training:	Epoch: [1][333/817]	Loss 0.4372 (0.6513)	
training:	Epoch: [1][334/817]	Loss 0.7780 (0.6517)	
training:	Epoch: [1][335/817]	Loss 0.4537 (0.6511)	
training:	Epoch: [1][336/817]	Loss 0.6811 (0.6512)	
training:	Epoch: [1][337/817]	Loss 0.5895 (0.6510)	
training:	Epoch: [1][338/817]	Loss 0.7760 (0.6513)	
training:	Epoch: [1][339/817]	Loss 0.4381 (0.6507)	
training:	Epoch: [1][340/817]	Loss 0.4774 (0.6502)	
training:	Epoch: [1][341/817]	Loss 0.5917 (0.6500)	
training:	Epoch: [1][342/817]	Loss 0.4332 (0.6494)	
training:	Epoch: [1][343/817]	Loss 0.5877 (0.6492)	
training:	Epoch: [1][344/817]	Loss 0.4430 (0.6486)	
training:	Epoch: [1][345/817]	Loss 0.4160 (0.6480)	
training:	Epoch: [1][346/817]	Loss 0.4753 (0.6475)	
training:	Epoch: [1][347/817]	Loss 0.6192 (0.6474)	
training:	Epoch: [1][348/817]	Loss 0.6932 (0.6475)	
training:	Epoch: [1][349/817]	Loss 0.5815 (0.6473)	
training:	Epoch: [1][350/817]	Loss 0.5796 (0.6471)	
training:	Epoch: [1][351/817]	Loss 0.8556 (0.6477)	
training:	Epoch: [1][352/817]	Loss 0.5150 (0.6473)	
training:	Epoch: [1][353/817]	Loss 0.5464 (0.6471)	
training:	Epoch: [1][354/817]	Loss 0.6265 (0.6470)	
training:	Epoch: [1][355/817]	Loss 0.5797 (0.6468)	
training:	Epoch: [1][356/817]	Loss 0.4810 (0.6463)	
training:	Epoch: [1][357/817]	Loss 0.3901 (0.6456)	
training:	Epoch: [1][358/817]	Loss 0.4599 (0.6451)	
training:	Epoch: [1][359/817]	Loss 0.8949 (0.6458)	
training:	Epoch: [1][360/817]	Loss 0.9149 (0.6465)	
training:	Epoch: [1][361/817]	Loss 0.4892 (0.6461)	
training:	Epoch: [1][362/817]	Loss 0.3115 (0.6452)	
training:	Epoch: [1][363/817]	Loss 0.6603 (0.6452)	
training:	Epoch: [1][364/817]	Loss 0.4907 (0.6448)	
training:	Epoch: [1][365/817]	Loss 0.5591 (0.6446)	
training:	Epoch: [1][366/817]	Loss 0.4695 (0.6441)	
training:	Epoch: [1][367/817]	Loss 0.4260 (0.6435)	
training:	Epoch: [1][368/817]	Loss 0.7640 (0.6438)	
training:	Epoch: [1][369/817]	Loss 0.5134 (0.6435)	
training:	Epoch: [1][370/817]	Loss 0.5371 (0.6432)	
training:	Epoch: [1][371/817]	Loss 0.4340 (0.6426)	
training:	Epoch: [1][372/817]	Loss 0.5900 (0.6425)	
training:	Epoch: [1][373/817]	Loss 0.4516 (0.6420)	
training:	Epoch: [1][374/817]	Loss 0.4652 (0.6415)	
training:	Epoch: [1][375/817]	Loss 0.4327 (0.6409)	
training:	Epoch: [1][376/817]	Loss 0.9041 (0.6416)	
training:	Epoch: [1][377/817]	Loss 0.4465 (0.6411)	
training:	Epoch: [1][378/817]	Loss 0.6079 (0.6410)	
training:	Epoch: [1][379/817]	Loss 0.5740 (0.6409)	
training:	Epoch: [1][380/817]	Loss 0.7825 (0.6412)	
training:	Epoch: [1][381/817]	Loss 0.7750 (0.6416)	
training:	Epoch: [1][382/817]	Loss 0.4093 (0.6410)	
training:	Epoch: [1][383/817]	Loss 0.4895 (0.6406)	
training:	Epoch: [1][384/817]	Loss 0.2912 (0.6397)	
training:	Epoch: [1][385/817]	Loss 0.4913 (0.6393)	
training:	Epoch: [1][386/817]	Loss 0.6289 (0.6392)	
training:	Epoch: [1][387/817]	Loss 0.4405 (0.6387)	
training:	Epoch: [1][388/817]	Loss 0.4758 (0.6383)	
training:	Epoch: [1][389/817]	Loss 0.7150 (0.6385)	
training:	Epoch: [1][390/817]	Loss 0.5741 (0.6383)	
training:	Epoch: [1][391/817]	Loss 0.4501 (0.6379)	
training:	Epoch: [1][392/817]	Loss 0.6597 (0.6379)	
training:	Epoch: [1][393/817]	Loss 0.7594 (0.6382)	
training:	Epoch: [1][394/817]	Loss 0.7549 (0.6385)	
training:	Epoch: [1][395/817]	Loss 0.7033 (0.6387)	
training:	Epoch: [1][396/817]	Loss 0.5561 (0.6385)	
training:	Epoch: [1][397/817]	Loss 0.5984 (0.6384)	
training:	Epoch: [1][398/817]	Loss 0.5459 (0.6381)	
training:	Epoch: [1][399/817]	Loss 0.4811 (0.6378)	
training:	Epoch: [1][400/817]	Loss 0.5716 (0.6376)	
training:	Epoch: [1][401/817]	Loss 0.7956 (0.6380)	
training:	Epoch: [1][402/817]	Loss 0.6060 (0.6379)	
training:	Epoch: [1][403/817]	Loss 0.6263 (0.6379)	
training:	Epoch: [1][404/817]	Loss 0.4687 (0.6375)	
training:	Epoch: [1][405/817]	Loss 0.6166 (0.6374)	
training:	Epoch: [1][406/817]	Loss 0.2904 (0.6366)	
training:	Epoch: [1][407/817]	Loss 0.3990 (0.6360)	
training:	Epoch: [1][408/817]	Loss 0.6270 (0.6359)	
training:	Epoch: [1][409/817]	Loss 0.4075 (0.6354)	
training:	Epoch: [1][410/817]	Loss 0.3409 (0.6347)	
training:	Epoch: [1][411/817]	Loss 0.4768 (0.6343)	
training:	Epoch: [1][412/817]	Loss 0.5742 (0.6341)	
training:	Epoch: [1][413/817]	Loss 0.3432 (0.6334)	
training:	Epoch: [1][414/817]	Loss 0.6416 (0.6335)	
training:	Epoch: [1][415/817]	Loss 0.5011 (0.6331)	
training:	Epoch: [1][416/817]	Loss 0.5680 (0.6330)	
training:	Epoch: [1][417/817]	Loss 0.6762 (0.6331)	
training:	Epoch: [1][418/817]	Loss 0.5246 (0.6328)	
training:	Epoch: [1][419/817]	Loss 0.2667 (0.6319)	
training:	Epoch: [1][420/817]	Loss 0.4169 (0.6314)	
training:	Epoch: [1][421/817]	Loss 0.6163 (0.6314)	
training:	Epoch: [1][422/817]	Loss 0.3425 (0.6307)	
training:	Epoch: [1][423/817]	Loss 0.5703 (0.6306)	
training:	Epoch: [1][424/817]	Loss 0.5778 (0.6304)	
training:	Epoch: [1][425/817]	Loss 0.4332 (0.6300)	
training:	Epoch: [1][426/817]	Loss 0.6492 (0.6300)	
training:	Epoch: [1][427/817]	Loss 0.3432 (0.6294)	
training:	Epoch: [1][428/817]	Loss 0.5384 (0.6291)	
training:	Epoch: [1][429/817]	Loss 0.5704 (0.6290)	
training:	Epoch: [1][430/817]	Loss 0.6510 (0.6291)	
training:	Epoch: [1][431/817]	Loss 0.4240 (0.6286)	
training:	Epoch: [1][432/817]	Loss 0.4320 (0.6281)	
training:	Epoch: [1][433/817]	Loss 0.5322 (0.6279)	
training:	Epoch: [1][434/817]	Loss 0.6060 (0.6279)	
training:	Epoch: [1][435/817]	Loss 0.5760 (0.6277)	
training:	Epoch: [1][436/817]	Loss 0.5443 (0.6275)	
training:	Epoch: [1][437/817]	Loss 0.5513 (0.6274)	
training:	Epoch: [1][438/817]	Loss 0.4588 (0.6270)	
training:	Epoch: [1][439/817]	Loss 0.4240 (0.6265)	
training:	Epoch: [1][440/817]	Loss 0.9627 (0.6273)	
training:	Epoch: [1][441/817]	Loss 0.4053 (0.6268)	
training:	Epoch: [1][442/817]	Loss 0.4378 (0.6264)	
training:	Epoch: [1][443/817]	Loss 0.8842 (0.6269)	
training:	Epoch: [1][444/817]	Loss 0.6657 (0.6270)	
training:	Epoch: [1][445/817]	Loss 0.4413 (0.6266)	
training:	Epoch: [1][446/817]	Loss 0.5196 (0.6264)	
training:	Epoch: [1][447/817]	Loss 0.6147 (0.6263)	
training:	Epoch: [1][448/817]	Loss 0.7917 (0.6267)	
training:	Epoch: [1][449/817]	Loss 0.4694 (0.6264)	
training:	Epoch: [1][450/817]	Loss 0.8505 (0.6269)	
training:	Epoch: [1][451/817]	Loss 0.5799 (0.6268)	
training:	Epoch: [1][452/817]	Loss 0.4254 (0.6263)	
training:	Epoch: [1][453/817]	Loss 0.3999 (0.6258)	
training:	Epoch: [1][454/817]	Loss 0.6755 (0.6259)	
training:	Epoch: [1][455/817]	Loss 0.7639 (0.6262)	
training:	Epoch: [1][456/817]	Loss 0.4690 (0.6259)	
training:	Epoch: [1][457/817]	Loss 0.6263 (0.6259)	
training:	Epoch: [1][458/817]	Loss 0.6699 (0.6260)	
training:	Epoch: [1][459/817]	Loss 0.6526 (0.6260)	
training:	Epoch: [1][460/817]	Loss 0.8316 (0.6265)	
training:	Epoch: [1][461/817]	Loss 0.4710 (0.6261)	
training:	Epoch: [1][462/817]	Loss 0.5348 (0.6259)	
training:	Epoch: [1][463/817]	Loss 0.4170 (0.6255)	
training:	Epoch: [1][464/817]	Loss 0.3894 (0.6250)	
training:	Epoch: [1][465/817]	Loss 0.9915 (0.6258)	
training:	Epoch: [1][466/817]	Loss 0.6464 (0.6258)	
training:	Epoch: [1][467/817]	Loss 0.5253 (0.6256)	
training:	Epoch: [1][468/817]	Loss 0.5268 (0.6254)	
training:	Epoch: [1][469/817]	Loss 0.6606 (0.6255)	
training:	Epoch: [1][470/817]	Loss 0.4695 (0.6251)	
training:	Epoch: [1][471/817]	Loss 0.7224 (0.6253)	
training:	Epoch: [1][472/817]	Loss 0.3710 (0.6248)	
training:	Epoch: [1][473/817]	Loss 0.8564 (0.6253)	
training:	Epoch: [1][474/817]	Loss 0.5692 (0.6252)	
training:	Epoch: [1][475/817]	Loss 0.3190 (0.6245)	
training:	Epoch: [1][476/817]	Loss 0.6797 (0.6246)	
training:	Epoch: [1][477/817]	Loss 0.5595 (0.6245)	
training:	Epoch: [1][478/817]	Loss 0.4005 (0.6240)	
training:	Epoch: [1][479/817]	Loss 0.3051 (0.6234)	
training:	Epoch: [1][480/817]	Loss 0.7695 (0.6237)	
training:	Epoch: [1][481/817]	Loss 0.6035 (0.6236)	
training:	Epoch: [1][482/817]	Loss 0.6339 (0.6237)	
training:	Epoch: [1][483/817]	Loss 0.7632 (0.6239)	
training:	Epoch: [1][484/817]	Loss 0.5990 (0.6239)	
training:	Epoch: [1][485/817]	Loss 0.5960 (0.6238)	
training:	Epoch: [1][486/817]	Loss 0.3253 (0.6232)	
training:	Epoch: [1][487/817]	Loss 0.4199 (0.6228)	
training:	Epoch: [1][488/817]	Loss 0.4079 (0.6224)	
training:	Epoch: [1][489/817]	Loss 0.6335 (0.6224)	
training:	Epoch: [1][490/817]	Loss 0.5011 (0.6221)	
training:	Epoch: [1][491/817]	Loss 0.4960 (0.6219)	
training:	Epoch: [1][492/817]	Loss 0.8046 (0.6223)	
training:	Epoch: [1][493/817]	Loss 0.4759 (0.6220)	
training:	Epoch: [1][494/817]	Loss 0.3067 (0.6213)	
training:	Epoch: [1][495/817]	Loss 0.5952 (0.6213)	
training:	Epoch: [1][496/817]	Loss 0.3483 (0.6207)	
training:	Epoch: [1][497/817]	Loss 0.6558 (0.6208)	
training:	Epoch: [1][498/817]	Loss 0.6145 (0.6208)	
training:	Epoch: [1][499/817]	Loss 0.6881 (0.6209)	
training:	Epoch: [1][500/817]	Loss 0.4198 (0.6205)	
training:	Epoch: [1][501/817]	Loss 0.5384 (0.6203)	
training:	Epoch: [1][502/817]	Loss 0.5034 (0.6201)	
training:	Epoch: [1][503/817]	Loss 0.3804 (0.6196)	
training:	Epoch: [1][504/817]	Loss 0.3993 (0.6192)	
training:	Epoch: [1][505/817]	Loss 0.4417 (0.6188)	
training:	Epoch: [1][506/817]	Loss 0.5150 (0.6186)	
training:	Epoch: [1][507/817]	Loss 0.4777 (0.6184)	
training:	Epoch: [1][508/817]	Loss 0.3751 (0.6179)	
training:	Epoch: [1][509/817]	Loss 0.5220 (0.6177)	
training:	Epoch: [1][510/817]	Loss 0.4259 (0.6173)	
training:	Epoch: [1][511/817]	Loss 0.4439 (0.6170)	
training:	Epoch: [1][512/817]	Loss 0.5352 (0.6168)	
training:	Epoch: [1][513/817]	Loss 0.6879 (0.6170)	
training:	Epoch: [1][514/817]	Loss 0.7343 (0.6172)	
training:	Epoch: [1][515/817]	Loss 0.6648 (0.6173)	
training:	Epoch: [1][516/817]	Loss 0.5428 (0.6171)	
training:	Epoch: [1][517/817]	Loss 0.4492 (0.6168)	
training:	Epoch: [1][518/817]	Loss 0.4569 (0.6165)	
training:	Epoch: [1][519/817]	Loss 0.4697 (0.6162)	
training:	Epoch: [1][520/817]	Loss 0.3665 (0.6157)	
training:	Epoch: [1][521/817]	Loss 0.7673 (0.6160)	
training:	Epoch: [1][522/817]	Loss 0.7873 (0.6164)	
training:	Epoch: [1][523/817]	Loss 0.5863 (0.6163)	
training:	Epoch: [1][524/817]	Loss 0.8994 (0.6168)	
training:	Epoch: [1][525/817]	Loss 0.5050 (0.6166)	
training:	Epoch: [1][526/817]	Loss 0.6816 (0.6168)	
training:	Epoch: [1][527/817]	Loss 0.5695 (0.6167)	
training:	Epoch: [1][528/817]	Loss 0.4247 (0.6163)	
training:	Epoch: [1][529/817]	Loss 0.3856 (0.6159)	
training:	Epoch: [1][530/817]	Loss 0.4819 (0.6156)	
training:	Epoch: [1][531/817]	Loss 0.5137 (0.6154)	
training:	Epoch: [1][532/817]	Loss 0.4090 (0.6150)	
training:	Epoch: [1][533/817]	Loss 0.5507 (0.6149)	
training:	Epoch: [1][534/817]	Loss 0.3738 (0.6145)	
training:	Epoch: [1][535/817]	Loss 0.3434 (0.6140)	
training:	Epoch: [1][536/817]	Loss 0.2767 (0.6133)	
training:	Epoch: [1][537/817]	Loss 0.4491 (0.6130)	
training:	Epoch: [1][538/817]	Loss 0.2824 (0.6124)	
training:	Epoch: [1][539/817]	Loss 0.3042 (0.6118)	
training:	Epoch: [1][540/817]	Loss 0.5025 (0.6116)	
training:	Epoch: [1][541/817]	Loss 0.4831 (0.6114)	
training:	Epoch: [1][542/817]	Loss 0.4904 (0.6112)	
training:	Epoch: [1][543/817]	Loss 0.5360 (0.6110)	
training:	Epoch: [1][544/817]	Loss 0.4691 (0.6108)	
training:	Epoch: [1][545/817]	Loss 0.8396 (0.6112)	
training:	Epoch: [1][546/817]	Loss 0.4072 (0.6108)	
training:	Epoch: [1][547/817]	Loss 0.4055 (0.6104)	
training:	Epoch: [1][548/817]	Loss 0.7563 (0.6107)	
training:	Epoch: [1][549/817]	Loss 0.4057 (0.6103)	
training:	Epoch: [1][550/817]	Loss 0.6456 (0.6104)	
training:	Epoch: [1][551/817]	Loss 0.4593 (0.6101)	
training:	Epoch: [1][552/817]	Loss 0.5663 (0.6100)	
training:	Epoch: [1][553/817]	Loss 0.6954 (0.6102)	
training:	Epoch: [1][554/817]	Loss 0.2314 (0.6095)	
training:	Epoch: [1][555/817]	Loss 0.3179 (0.6090)	
training:	Epoch: [1][556/817]	Loss 0.3606 (0.6085)	
training:	Epoch: [1][557/817]	Loss 0.4315 (0.6082)	
training:	Epoch: [1][558/817]	Loss 0.4871 (0.6080)	
training:	Epoch: [1][559/817]	Loss 0.6801 (0.6081)	
training:	Epoch: [1][560/817]	Loss 0.4915 (0.6079)	
training:	Epoch: [1][561/817]	Loss 0.2825 (0.6073)	
training:	Epoch: [1][562/817]	Loss 0.3520 (0.6069)	
training:	Epoch: [1][563/817]	Loss 0.5430 (0.6068)	
training:	Epoch: [1][564/817]	Loss 0.4761 (0.6065)	
training:	Epoch: [1][565/817]	Loss 0.4982 (0.6064)	
training:	Epoch: [1][566/817]	Loss 0.5005 (0.6062)	
training:	Epoch: [1][567/817]	Loss 0.4779 (0.6059)	
training:	Epoch: [1][568/817]	Loss 0.4587 (0.6057)	
training:	Epoch: [1][569/817]	Loss 0.3819 (0.6053)	
training:	Epoch: [1][570/817]	Loss 0.4446 (0.6050)	
training:	Epoch: [1][571/817]	Loss 0.3459 (0.6046)	
training:	Epoch: [1][572/817]	Loss 0.3371 (0.6041)	
training:	Epoch: [1][573/817]	Loss 0.5613 (0.6040)	
training:	Epoch: [1][574/817]	Loss 0.6146 (0.6040)	
training:	Epoch: [1][575/817]	Loss 0.7987 (0.6044)	
training:	Epoch: [1][576/817]	Loss 0.4606 (0.6041)	
training:	Epoch: [1][577/817]	Loss 0.4258 (0.6038)	
training:	Epoch: [1][578/817]	Loss 0.5577 (0.6037)	
training:	Epoch: [1][579/817]	Loss 0.5842 (0.6037)	
training:	Epoch: [1][580/817]	Loss 0.3500 (0.6033)	
training:	Epoch: [1][581/817]	Loss 0.7572 (0.6035)	
training:	Epoch: [1][582/817]	Loss 0.6907 (0.6037)	
training:	Epoch: [1][583/817]	Loss 0.3689 (0.6033)	
training:	Epoch: [1][584/817]	Loss 0.6884 (0.6034)	
training:	Epoch: [1][585/817]	Loss 0.7224 (0.6036)	
training:	Epoch: [1][586/817]	Loss 0.3587 (0.6032)	
training:	Epoch: [1][587/817]	Loss 0.5388 (0.6031)	
training:	Epoch: [1][588/817]	Loss 0.5601 (0.6030)	
training:	Epoch: [1][589/817]	Loss 0.4551 (0.6028)	
training:	Epoch: [1][590/817]	Loss 0.5055 (0.6026)	
training:	Epoch: [1][591/817]	Loss 0.3149 (0.6021)	
training:	Epoch: [1][592/817]	Loss 0.4404 (0.6018)	
training:	Epoch: [1][593/817]	Loss 0.5609 (0.6018)	
training:	Epoch: [1][594/817]	Loss 0.4494 (0.6015)	
training:	Epoch: [1][595/817]	Loss 0.5274 (0.6014)	
training:	Epoch: [1][596/817]	Loss 0.3934 (0.6010)	
training:	Epoch: [1][597/817]	Loss 0.8553 (0.6015)	
training:	Epoch: [1][598/817]	Loss 0.3174 (0.6010)	
training:	Epoch: [1][599/817]	Loss 0.6839 (0.6011)	
training:	Epoch: [1][600/817]	Loss 0.6519 (0.6012)	
training:	Epoch: [1][601/817]	Loss 0.3579 (0.6008)	
training:	Epoch: [1][602/817]	Loss 0.6243 (0.6009)	
training:	Epoch: [1][603/817]	Loss 0.4728 (0.6006)	
training:	Epoch: [1][604/817]	Loss 0.2539 (0.6001)	
training:	Epoch: [1][605/817]	Loss 0.3808 (0.5997)	
training:	Epoch: [1][606/817]	Loss 0.3989 (0.5994)	
training:	Epoch: [1][607/817]	Loss 0.4311 (0.5991)	
training:	Epoch: [1][608/817]	Loss 0.5473 (0.5990)	
training:	Epoch: [1][609/817]	Loss 0.5879 (0.5990)	
training:	Epoch: [1][610/817]	Loss 0.3515 (0.5986)	
training:	Epoch: [1][611/817]	Loss 0.6545 (0.5987)	
training:	Epoch: [1][612/817]	Loss 0.3851 (0.5983)	
training:	Epoch: [1][613/817]	Loss 0.4824 (0.5981)	
training:	Epoch: [1][614/817]	Loss 0.3213 (0.5977)	
training:	Epoch: [1][615/817]	Loss 0.3400 (0.5973)	
training:	Epoch: [1][616/817]	Loss 0.6628 (0.5974)	
training:	Epoch: [1][617/817]	Loss 0.6563 (0.5975)	
training:	Epoch: [1][618/817]	Loss 0.5084 (0.5973)	
training:	Epoch: [1][619/817]	Loss 0.4183 (0.5970)	
training:	Epoch: [1][620/817]	Loss 0.6394 (0.5971)	
training:	Epoch: [1][621/817]	Loss 0.3662 (0.5967)	
training:	Epoch: [1][622/817]	Loss 0.5281 (0.5966)	
training:	Epoch: [1][623/817]	Loss 0.6494 (0.5967)	
training:	Epoch: [1][624/817]	Loss 0.8306 (0.5971)	
training:	Epoch: [1][625/817]	Loss 0.3228 (0.5966)	
training:	Epoch: [1][626/817]	Loss 0.3169 (0.5962)	
training:	Epoch: [1][627/817]	Loss 0.3080 (0.5957)	
training:	Epoch: [1][628/817]	Loss 0.5402 (0.5956)	
training:	Epoch: [1][629/817]	Loss 0.6726 (0.5958)	
training:	Epoch: [1][630/817]	Loss 0.3047 (0.5953)	
training:	Epoch: [1][631/817]	Loss 0.5365 (0.5952)	
training:	Epoch: [1][632/817]	Loss 0.5347 (0.5951)	
training:	Epoch: [1][633/817]	Loss 0.3671 (0.5948)	
training:	Epoch: [1][634/817]	Loss 0.7146 (0.5949)	
training:	Epoch: [1][635/817]	Loss 0.5071 (0.5948)	
training:	Epoch: [1][636/817]	Loss 0.8969 (0.5953)	
training:	Epoch: [1][637/817]	Loss 0.6864 (0.5954)	
training:	Epoch: [1][638/817]	Loss 0.2193 (0.5948)	
training:	Epoch: [1][639/817]	Loss 0.2589 (0.5943)	
training:	Epoch: [1][640/817]	Loss 0.7100 (0.5945)	
training:	Epoch: [1][641/817]	Loss 0.3232 (0.5941)	
training:	Epoch: [1][642/817]	Loss 0.2252 (0.5935)	
training:	Epoch: [1][643/817]	Loss 0.1920 (0.5929)	
training:	Epoch: [1][644/817]	Loss 0.2843 (0.5924)	
training:	Epoch: [1][645/817]	Loss 0.4925 (0.5922)	
training:	Epoch: [1][646/817]	Loss 0.6841 (0.5924)	
training:	Epoch: [1][647/817]	Loss 0.4571 (0.5922)	
training:	Epoch: [1][648/817]	Loss 0.5569 (0.5921)	
training:	Epoch: [1][649/817]	Loss 0.4893 (0.5920)	
training:	Epoch: [1][650/817]	Loss 0.4635 (0.5918)	
training:	Epoch: [1][651/817]	Loss 0.4502 (0.5915)	
training:	Epoch: [1][652/817]	Loss 0.4469 (0.5913)	
training:	Epoch: [1][653/817]	Loss 0.5765 (0.5913)	
training:	Epoch: [1][654/817]	Loss 0.5089 (0.5912)	
training:	Epoch: [1][655/817]	Loss 0.2388 (0.5906)	
training:	Epoch: [1][656/817]	Loss 0.4527 (0.5904)	
training:	Epoch: [1][657/817]	Loss 0.5082 (0.5903)	
training:	Epoch: [1][658/817]	Loss 0.6765 (0.5904)	
training:	Epoch: [1][659/817]	Loss 0.3182 (0.5900)	
training:	Epoch: [1][660/817]	Loss 0.4815 (0.5899)	
training:	Epoch: [1][661/817]	Loss 0.1990 (0.5893)	
training:	Epoch: [1][662/817]	Loss 0.5215 (0.5892)	
training:	Epoch: [1][663/817]	Loss 0.4839 (0.5890)	
training:	Epoch: [1][664/817]	Loss 0.2497 (0.5885)	
training:	Epoch: [1][665/817]	Loss 0.3382 (0.5881)	
training:	Epoch: [1][666/817]	Loss 0.6983 (0.5883)	
training:	Epoch: [1][667/817]	Loss 0.3637 (0.5879)	
training:	Epoch: [1][668/817]	Loss 0.6135 (0.5880)	
training:	Epoch: [1][669/817]	Loss 0.5875 (0.5880)	
training:	Epoch: [1][670/817]	Loss 0.5665 (0.5879)	
training:	Epoch: [1][671/817]	Loss 0.5289 (0.5879)	
training:	Epoch: [1][672/817]	Loss 0.8607 (0.5883)	
training:	Epoch: [1][673/817]	Loss 0.4730 (0.5881)	
training:	Epoch: [1][674/817]	Loss 0.7465 (0.5883)	
training:	Epoch: [1][675/817]	Loss 0.5856 (0.5883)	
training:	Epoch: [1][676/817]	Loss 0.2904 (0.5879)	
training:	Epoch: [1][677/817]	Loss 0.4708 (0.5877)	
training:	Epoch: [1][678/817]	Loss 0.3527 (0.5874)	
training:	Epoch: [1][679/817]	Loss 0.4145 (0.5871)	
training:	Epoch: [1][680/817]	Loss 0.4606 (0.5869)	
training:	Epoch: [1][681/817]	Loss 0.2410 (0.5864)	
training:	Epoch: [1][682/817]	Loss 0.4102 (0.5862)	
training:	Epoch: [1][683/817]	Loss 0.3731 (0.5858)	
training:	Epoch: [1][684/817]	Loss 0.3721 (0.5855)	
training:	Epoch: [1][685/817]	Loss 0.6606 (0.5856)	
training:	Epoch: [1][686/817]	Loss 0.5339 (0.5856)	
training:	Epoch: [1][687/817]	Loss 0.4372 (0.5853)	
training:	Epoch: [1][688/817]	Loss 0.4161 (0.5851)	
training:	Epoch: [1][689/817]	Loss 0.6439 (0.5852)	
training:	Epoch: [1][690/817]	Loss 0.2951 (0.5848)	
training:	Epoch: [1][691/817]	Loss 0.2916 (0.5843)	
training:	Epoch: [1][692/817]	Loss 0.3239 (0.5840)	
training:	Epoch: [1][693/817]	Loss 0.2616 (0.5835)	
training:	Epoch: [1][694/817]	Loss 0.3846 (0.5832)	
training:	Epoch: [1][695/817]	Loss 0.2583 (0.5827)	
training:	Epoch: [1][696/817]	Loss 0.1524 (0.5821)	
training:	Epoch: [1][697/817]	Loss 0.2526 (0.5817)	
training:	Epoch: [1][698/817]	Loss 0.4870 (0.5815)	
training:	Epoch: [1][699/817]	Loss 0.2464 (0.5810)	
training:	Epoch: [1][700/817]	Loss 0.3414 (0.5807)	
training:	Epoch: [1][701/817]	Loss 0.6019 (0.5807)	
training:	Epoch: [1][702/817]	Loss 0.4061 (0.5805)	
training:	Epoch: [1][703/817]	Loss 0.2559 (0.5800)	
training:	Epoch: [1][704/817]	Loss 0.2109 (0.5795)	
training:	Epoch: [1][705/817]	Loss 0.5496 (0.5795)	
training:	Epoch: [1][706/817]	Loss 0.4662 (0.5793)	
training:	Epoch: [1][707/817]	Loss 0.5673 (0.5793)	
training:	Epoch: [1][708/817]	Loss 0.3846 (0.5790)	
training:	Epoch: [1][709/817]	Loss 0.3785 (0.5787)	
training:	Epoch: [1][710/817]	Loss 0.5960 (0.5787)	
training:	Epoch: [1][711/817]	Loss 0.4885 (0.5786)	
training:	Epoch: [1][712/817]	Loss 0.1593 (0.5780)	
training:	Epoch: [1][713/817]	Loss 0.7889 (0.5783)	
training:	Epoch: [1][714/817]	Loss 0.6560 (0.5784)	
training:	Epoch: [1][715/817]	Loss 0.6094 (0.5785)	
training:	Epoch: [1][716/817]	Loss 0.7883 (0.5788)	
training:	Epoch: [1][717/817]	Loss 0.3458 (0.5784)	
training:	Epoch: [1][718/817]	Loss 0.1726 (0.5779)	
training:	Epoch: [1][719/817]	Loss 0.4376 (0.5777)	
training:	Epoch: [1][720/817]	Loss 0.3540 (0.5774)	
training:	Epoch: [1][721/817]	Loss 0.4711 (0.5772)	
training:	Epoch: [1][722/817]	Loss 0.6230 (0.5773)	
training:	Epoch: [1][723/817]	Loss 0.3184 (0.5769)	
training:	Epoch: [1][724/817]	Loss 0.8849 (0.5774)	
training:	Epoch: [1][725/817]	Loss 0.5465 (0.5773)	
training:	Epoch: [1][726/817]	Loss 0.7346 (0.5775)	
training:	Epoch: [1][727/817]	Loss 0.5987 (0.5776)	
training:	Epoch: [1][728/817]	Loss 0.5367 (0.5775)	
training:	Epoch: [1][729/817]	Loss 0.5151 (0.5774)	
training:	Epoch: [1][730/817]	Loss 0.5583 (0.5774)	
training:	Epoch: [1][731/817]	Loss 0.5241 (0.5773)	
training:	Epoch: [1][732/817]	Loss 0.5882 (0.5773)	
training:	Epoch: [1][733/817]	Loss 0.5187 (0.5773)	
training:	Epoch: [1][734/817]	Loss 0.6718 (0.5774)	
training:	Epoch: [1][735/817]	Loss 0.3179 (0.5770)	
training:	Epoch: [1][736/817]	Loss 0.4502 (0.5769)	
training:	Epoch: [1][737/817]	Loss 0.2386 (0.5764)	
training:	Epoch: [1][738/817]	Loss 0.6849 (0.5765)	
training:	Epoch: [1][739/817]	Loss 0.5870 (0.5766)	
training:	Epoch: [1][740/817]	Loss 0.7071 (0.5767)	
training:	Epoch: [1][741/817]	Loss 0.1848 (0.5762)	
training:	Epoch: [1][742/817]	Loss 0.3997 (0.5760)	
training:	Epoch: [1][743/817]	Loss 0.3751 (0.5757)	
training:	Epoch: [1][744/817]	Loss 0.3563 (0.5754)	
training:	Epoch: [1][745/817]	Loss 0.5250 (0.5753)	
training:	Epoch: [1][746/817]	Loss 0.4353 (0.5751)	
training:	Epoch: [1][747/817]	Loss 0.3480 (0.5748)	
training:	Epoch: [1][748/817]	Loss 0.3523 (0.5745)	
training:	Epoch: [1][749/817]	Loss 0.7236 (0.5747)	
training:	Epoch: [1][750/817]	Loss 0.4103 (0.5745)	
training:	Epoch: [1][751/817]	Loss 0.4974 (0.5744)	
training:	Epoch: [1][752/817]	Loss 0.9309 (0.5749)	
training:	Epoch: [1][753/817]	Loss 0.3298 (0.5746)	
training:	Epoch: [1][754/817]	Loss 0.4437 (0.5744)	
training:	Epoch: [1][755/817]	Loss 0.4176 (0.5742)	
training:	Epoch: [1][756/817]	Loss 0.3112 (0.5738)	
training:	Epoch: [1][757/817]	Loss 0.2912 (0.5735)	
training:	Epoch: [1][758/817]	Loss 0.4456 (0.5733)	
training:	Epoch: [1][759/817]	Loss 0.5983 (0.5733)	
training:	Epoch: [1][760/817]	Loss 0.6942 (0.5735)	
training:	Epoch: [1][761/817]	Loss 0.3965 (0.5733)	
training:	Epoch: [1][762/817]	Loss 0.5906 (0.5733)	
training:	Epoch: [1][763/817]	Loss 0.4472 (0.5731)	
training:	Epoch: [1][764/817]	Loss 0.3913 (0.5729)	
training:	Epoch: [1][765/817]	Loss 0.5276 (0.5728)	
training:	Epoch: [1][766/817]	Loss 0.4537 (0.5727)	
training:	Epoch: [1][767/817]	Loss 0.3064 (0.5723)	
training:	Epoch: [1][768/817]	Loss 0.3695 (0.5721)	
training:	Epoch: [1][769/817]	Loss 0.3949 (0.5718)	
training:	Epoch: [1][770/817]	Loss 0.5522 (0.5718)	
training:	Epoch: [1][771/817]	Loss 0.5667 (0.5718)	
training:	Epoch: [1][772/817]	Loss 0.5009 (0.5717)	
training:	Epoch: [1][773/817]	Loss 0.4415 (0.5715)	
training:	Epoch: [1][774/817]	Loss 0.2407 (0.5711)	
training:	Epoch: [1][775/817]	Loss 0.6582 (0.5712)	
training:	Epoch: [1][776/817]	Loss 0.4158 (0.5710)	
training:	Epoch: [1][777/817]	Loss 0.5110 (0.5709)	
training:	Epoch: [1][778/817]	Loss 0.4254 (0.5708)	
training:	Epoch: [1][779/817]	Loss 0.3592 (0.5705)	
training:	Epoch: [1][780/817]	Loss 0.4013 (0.5703)	
training:	Epoch: [1][781/817]	Loss 0.3699 (0.5700)	
training:	Epoch: [1][782/817]	Loss 0.3674 (0.5697)	
training:	Epoch: [1][783/817]	Loss 0.6174 (0.5698)	
training:	Epoch: [1][784/817]	Loss 0.5937 (0.5698)	
training:	Epoch: [1][785/817]	Loss 0.4281 (0.5697)	
training:	Epoch: [1][786/817]	Loss 0.5066 (0.5696)	
training:	Epoch: [1][787/817]	Loss 0.4130 (0.5694)	
training:	Epoch: [1][788/817]	Loss 0.6829 (0.5695)	
training:	Epoch: [1][789/817]	Loss 0.8644 (0.5699)	
training:	Epoch: [1][790/817]	Loss 0.2080 (0.5694)	
training:	Epoch: [1][791/817]	Loss 0.3935 (0.5692)	
training:	Epoch: [1][792/817]	Loss 0.3971 (0.5690)	
training:	Epoch: [1][793/817]	Loss 0.5367 (0.5690)	
training:	Epoch: [1][794/817]	Loss 0.4883 (0.5689)	
training:	Epoch: [1][795/817]	Loss 0.5338 (0.5688)	
training:	Epoch: [1][796/817]	Loss 1.0748 (0.5694)	
training:	Epoch: [1][797/817]	Loss 0.4090 (0.5692)	
training:	Epoch: [1][798/817]	Loss 0.3091 (0.5689)	
training:	Epoch: [1][799/817]	Loss 0.3994 (0.5687)	
training:	Epoch: [1][800/817]	Loss 0.8004 (0.5690)	
training:	Epoch: [1][801/817]	Loss 0.2631 (0.5686)	
training:	Epoch: [1][802/817]	Loss 0.1810 (0.5681)	
training:	Epoch: [1][803/817]	Loss 0.2559 (0.5677)	
training:	Epoch: [1][804/817]	Loss 0.2922 (0.5674)	
training:	Epoch: [1][805/817]	Loss 0.2114 (0.5670)	
training:	Epoch: [1][806/817]	Loss 0.4146 (0.5668)	
training:	Epoch: [1][807/817]	Loss 0.2864 (0.5664)	
training:	Epoch: [1][808/817]	Loss 0.6865 (0.5666)	
training:	Epoch: [1][809/817]	Loss 0.7750 (0.5668)	
training:	Epoch: [1][810/817]	Loss 0.4555 (0.5667)	
training:	Epoch: [1][811/817]	Loss 0.5887 (0.5667)	
training:	Epoch: [1][812/817]	Loss 0.4473 (0.5666)	
training:	Epoch: [1][813/817]	Loss 0.4770 (0.5665)	
training:	Epoch: [1][814/817]	Loss 0.4908 (0.5664)	
training:	Epoch: [1][815/817]	Loss 0.2101 (0.5659)	
training:	Epoch: [1][816/817]	Loss 0.2531 (0.5655)	
training:	Epoch: [1][817/817]	Loss 0.2262 (0.5651)	
Training:	 Loss: 0.5650

Training:	 ACC: 0.8152 0.8145 0.7966 0.8339
Validation:	 ACC: 0.7955 0.7945 0.7738 0.8173
Validation:	 Best_BACC: 0.7955 0.7945 0.7738 0.8173
Validation:	 Loss: 0.4539
Pretraining:	Epoch 2/200
----------
training:	Epoch: [2][1/817]	Loss 0.3827 (0.3827)	
training:	Epoch: [2][2/817]	Loss 0.3325 (0.3576)	
training:	Epoch: [2][3/817]	Loss 0.3831 (0.3661)	
training:	Epoch: [2][4/817]	Loss 0.4222 (0.3801)	
training:	Epoch: [2][5/817]	Loss 0.3745 (0.3790)	
training:	Epoch: [2][6/817]	Loss 0.5517 (0.4078)	
training:	Epoch: [2][7/817]	Loss 0.2164 (0.3804)	
training:	Epoch: [2][8/817]	Loss 0.4808 (0.3930)	
training:	Epoch: [2][9/817]	Loss 0.4782 (0.4024)	
training:	Epoch: [2][10/817]	Loss 0.4688 (0.4091)	
training:	Epoch: [2][11/817]	Loss 0.4526 (0.4130)	
training:	Epoch: [2][12/817]	Loss 0.5163 (0.4216)	
training:	Epoch: [2][13/817]	Loss 0.5589 (0.4322)	
training:	Epoch: [2][14/817]	Loss 0.6761 (0.4496)	
training:	Epoch: [2][15/817]	Loss 0.2496 (0.4363)	
training:	Epoch: [2][16/817]	Loss 0.2769 (0.4263)	
training:	Epoch: [2][17/817]	Loss 0.2430 (0.4155)	
training:	Epoch: [2][18/817]	Loss 0.2937 (0.4088)	
training:	Epoch: [2][19/817]	Loss 0.5872 (0.4182)	
training:	Epoch: [2][20/817]	Loss 0.3102 (0.4128)	
training:	Epoch: [2][21/817]	Loss 0.6784 (0.4254)	
training:	Epoch: [2][22/817]	Loss 0.2331 (0.4167)	
training:	Epoch: [2][23/817]	Loss 0.3371 (0.4132)	
training:	Epoch: [2][24/817]	Loss 0.5236 (0.4178)	
training:	Epoch: [2][25/817]	Loss 0.3277 (0.4142)	
training:	Epoch: [2][26/817]	Loss 0.4101 (0.4140)	
training:	Epoch: [2][27/817]	Loss 0.5065 (0.4175)	
training:	Epoch: [2][28/817]	Loss 0.3250 (0.4142)	
training:	Epoch: [2][29/817]	Loss 0.5576 (0.4191)	
training:	Epoch: [2][30/817]	Loss 0.2158 (0.4123)	
training:	Epoch: [2][31/817]	Loss 0.5440 (0.4166)	
training:	Epoch: [2][32/817]	Loss 0.5090 (0.4195)	
training:	Epoch: [2][33/817]	Loss 0.1653 (0.4118)	
training:	Epoch: [2][34/817]	Loss 0.5752 (0.4166)	
training:	Epoch: [2][35/817]	Loss 0.6713 (0.4238)	
training:	Epoch: [2][36/817]	Loss 0.1687 (0.4168)	
training:	Epoch: [2][37/817]	Loss 0.8511 (0.4285)	
training:	Epoch: [2][38/817]	Loss 0.5517 (0.4317)	
training:	Epoch: [2][39/817]	Loss 0.4399 (0.4319)	
training:	Epoch: [2][40/817]	Loss 0.6659 (0.4378)	
training:	Epoch: [2][41/817]	Loss 0.4486 (0.4381)	
training:	Epoch: [2][42/817]	Loss 0.2631 (0.4339)	
training:	Epoch: [2][43/817]	Loss 0.4892 (0.4352)	
training:	Epoch: [2][44/817]	Loss 0.4464 (0.4354)	
training:	Epoch: [2][45/817]	Loss 0.3392 (0.4333)	
training:	Epoch: [2][46/817]	Loss 0.4625 (0.4339)	
training:	Epoch: [2][47/817]	Loss 0.3462 (0.4321)	
training:	Epoch: [2][48/817]	Loss 0.2835 (0.4290)	
training:	Epoch: [2][49/817]	Loss 0.5775 (0.4320)	
training:	Epoch: [2][50/817]	Loss 0.4312 (0.4320)	
training:	Epoch: [2][51/817]	Loss 0.4517 (0.4324)	
training:	Epoch: [2][52/817]	Loss 0.4205 (0.4321)	
training:	Epoch: [2][53/817]	Loss 0.7237 (0.4376)	
training:	Epoch: [2][54/817]	Loss 0.3818 (0.4366)	
training:	Epoch: [2][55/817]	Loss 0.1546 (0.4315)	
training:	Epoch: [2][56/817]	Loss 0.3797 (0.4306)	
training:	Epoch: [2][57/817]	Loss 0.3295 (0.4288)	
training:	Epoch: [2][58/817]	Loss 0.2945 (0.4265)	
training:	Epoch: [2][59/817]	Loss 0.7142 (0.4313)	
training:	Epoch: [2][60/817]	Loss 0.4415 (0.4315)	
training:	Epoch: [2][61/817]	Loss 0.1747 (0.4273)	
training:	Epoch: [2][62/817]	Loss 0.4876 (0.4283)	
training:	Epoch: [2][63/817]	Loss 0.5816 (0.4307)	
training:	Epoch: [2][64/817]	Loss 0.3033 (0.4287)	
training:	Epoch: [2][65/817]	Loss 0.2077 (0.4253)	
training:	Epoch: [2][66/817]	Loss 0.2260 (0.4223)	
training:	Epoch: [2][67/817]	Loss 0.2997 (0.4205)	
training:	Epoch: [2][68/817]	Loss 0.3194 (0.4190)	
training:	Epoch: [2][69/817]	Loss 0.3540 (0.4180)	
training:	Epoch: [2][70/817]	Loss 0.3220 (0.4167)	
training:	Epoch: [2][71/817]	Loss 0.2915 (0.4149)	
training:	Epoch: [2][72/817]	Loss 0.5859 (0.4173)	
training:	Epoch: [2][73/817]	Loss 0.2433 (0.4149)	
training:	Epoch: [2][74/817]	Loss 0.6812 (0.4185)	
training:	Epoch: [2][75/817]	Loss 0.5164 (0.4198)	
training:	Epoch: [2][76/817]	Loss 0.4283 (0.4199)	
training:	Epoch: [2][77/817]	Loss 0.6883 (0.4234)	
training:	Epoch: [2][78/817]	Loss 0.6998 (0.4269)	
training:	Epoch: [2][79/817]	Loss 0.3965 (0.4266)	
training:	Epoch: [2][80/817]	Loss 0.6089 (0.4288)	
training:	Epoch: [2][81/817]	Loss 0.3900 (0.4284)	
training:	Epoch: [2][82/817]	Loss 0.4107 (0.4281)	
training:	Epoch: [2][83/817]	Loss 0.8226 (0.4329)	
training:	Epoch: [2][84/817]	Loss 0.3628 (0.4321)	
training:	Epoch: [2][85/817]	Loss 0.5591 (0.4336)	
training:	Epoch: [2][86/817]	Loss 0.4480 (0.4337)	
training:	Epoch: [2][87/817]	Loss 0.5277 (0.4348)	
training:	Epoch: [2][88/817]	Loss 0.1363 (0.4314)	
training:	Epoch: [2][89/817]	Loss 0.1732 (0.4285)	
training:	Epoch: [2][90/817]	Loss 0.6126 (0.4306)	
training:	Epoch: [2][91/817]	Loss 0.1641 (0.4276)	
training:	Epoch: [2][92/817]	Loss 0.4381 (0.4277)	
training:	Epoch: [2][93/817]	Loss 0.6738 (0.4304)	
training:	Epoch: [2][94/817]	Loss 0.4116 (0.4302)	
training:	Epoch: [2][95/817]	Loss 0.4091 (0.4300)	
training:	Epoch: [2][96/817]	Loss 0.4014 (0.4297)	
training:	Epoch: [2][97/817]	Loss 0.6168 (0.4316)	
training:	Epoch: [2][98/817]	Loss 0.5364 (0.4327)	
training:	Epoch: [2][99/817]	Loss 0.4973 (0.4333)	
training:	Epoch: [2][100/817]	Loss 0.2849 (0.4318)	
training:	Epoch: [2][101/817]	Loss 0.5313 (0.4328)	
training:	Epoch: [2][102/817]	Loss 0.3172 (0.4317)	
training:	Epoch: [2][103/817]	Loss 0.3544 (0.4309)	
training:	Epoch: [2][104/817]	Loss 0.3194 (0.4299)	
training:	Epoch: [2][105/817]	Loss 0.4771 (0.4303)	
training:	Epoch: [2][106/817]	Loss 0.3168 (0.4292)	
training:	Epoch: [2][107/817]	Loss 0.3885 (0.4289)	
training:	Epoch: [2][108/817]	Loss 0.5159 (0.4297)	
training:	Epoch: [2][109/817]	Loss 0.2583 (0.4281)	
training:	Epoch: [2][110/817]	Loss 0.2420 (0.4264)	
training:	Epoch: [2][111/817]	Loss 0.3928 (0.4261)	
training:	Epoch: [2][112/817]	Loss 0.2926 (0.4249)	
training:	Epoch: [2][113/817]	Loss 0.4114 (0.4248)	
training:	Epoch: [2][114/817]	Loss 0.5608 (0.4260)	
training:	Epoch: [2][115/817]	Loss 0.2485 (0.4244)	
training:	Epoch: [2][116/817]	Loss 0.5305 (0.4254)	
training:	Epoch: [2][117/817]	Loss 0.7216 (0.4279)	
training:	Epoch: [2][118/817]	Loss 0.2946 (0.4268)	
training:	Epoch: [2][119/817]	Loss 0.3428 (0.4261)	
training:	Epoch: [2][120/817]	Loss 0.5186 (0.4268)	
training:	Epoch: [2][121/817]	Loss 0.2870 (0.4257)	
training:	Epoch: [2][122/817]	Loss 0.2841 (0.4245)	
training:	Epoch: [2][123/817]	Loss 0.4819 (0.4250)	
training:	Epoch: [2][124/817]	Loss 0.2796 (0.4238)	
training:	Epoch: [2][125/817]	Loss 0.2734 (0.4226)	
training:	Epoch: [2][126/817]	Loss 0.2832 (0.4215)	
training:	Epoch: [2][127/817]	Loss 0.7420 (0.4240)	
training:	Epoch: [2][128/817]	Loss 0.4961 (0.4246)	
training:	Epoch: [2][129/817]	Loss 0.3173 (0.4237)	
training:	Epoch: [2][130/817]	Loss 0.5225 (0.4245)	
training:	Epoch: [2][131/817]	Loss 0.4079 (0.4244)	
training:	Epoch: [2][132/817]	Loss 0.2990 (0.4234)	
training:	Epoch: [2][133/817]	Loss 0.4679 (0.4238)	
training:	Epoch: [2][134/817]	Loss 0.3408 (0.4231)	
training:	Epoch: [2][135/817]	Loss 0.4210 (0.4231)	
training:	Epoch: [2][136/817]	Loss 0.5028 (0.4237)	
training:	Epoch: [2][137/817]	Loss 0.6235 (0.4252)	
training:	Epoch: [2][138/817]	Loss 0.2200 (0.4237)	
training:	Epoch: [2][139/817]	Loss 0.3099 (0.4229)	
training:	Epoch: [2][140/817]	Loss 0.4988 (0.4234)	
training:	Epoch: [2][141/817]	Loss 0.5667 (0.4244)	
training:	Epoch: [2][142/817]	Loss 0.6558 (0.4261)	
training:	Epoch: [2][143/817]	Loss 0.2709 (0.4250)	
training:	Epoch: [2][144/817]	Loss 0.2029 (0.4234)	
training:	Epoch: [2][145/817]	Loss 0.1616 (0.4216)	
training:	Epoch: [2][146/817]	Loss 0.5163 (0.4223)	
training:	Epoch: [2][147/817]	Loss 0.3675 (0.4219)	
training:	Epoch: [2][148/817]	Loss 0.3446 (0.4214)	
training:	Epoch: [2][149/817]	Loss 0.6603 (0.4230)	
training:	Epoch: [2][150/817]	Loss 0.4142 (0.4229)	
training:	Epoch: [2][151/817]	Loss 0.1400 (0.4210)	
training:	Epoch: [2][152/817]	Loss 0.1227 (0.4191)	
training:	Epoch: [2][153/817]	Loss 0.3814 (0.4188)	
training:	Epoch: [2][154/817]	Loss 0.5508 (0.4197)	
training:	Epoch: [2][155/817]	Loss 0.4551 (0.4199)	
training:	Epoch: [2][156/817]	Loss 0.3596 (0.4195)	
training:	Epoch: [2][157/817]	Loss 0.3246 (0.4189)	
training:	Epoch: [2][158/817]	Loss 0.3440 (0.4185)	
training:	Epoch: [2][159/817]	Loss 0.4686 (0.4188)	
training:	Epoch: [2][160/817]	Loss 0.1179 (0.4169)	
training:	Epoch: [2][161/817]	Loss 0.1259 (0.4151)	
training:	Epoch: [2][162/817]	Loss 0.4636 (0.4154)	
training:	Epoch: [2][163/817]	Loss 0.3115 (0.4147)	
training:	Epoch: [2][164/817]	Loss 0.4733 (0.4151)	
training:	Epoch: [2][165/817]	Loss 0.4305 (0.4152)	
training:	Epoch: [2][166/817]	Loss 0.3560 (0.4148)	
training:	Epoch: [2][167/817]	Loss 0.2706 (0.4140)	
training:	Epoch: [2][168/817]	Loss 0.2564 (0.4130)	
training:	Epoch: [2][169/817]	Loss 0.1752 (0.4116)	
training:	Epoch: [2][170/817]	Loss 0.3377 (0.4112)	
training:	Epoch: [2][171/817]	Loss 0.3866 (0.4111)	
training:	Epoch: [2][172/817]	Loss 0.3754 (0.4108)	
training:	Epoch: [2][173/817]	Loss 0.5288 (0.4115)	
training:	Epoch: [2][174/817]	Loss 0.3722 (0.4113)	
training:	Epoch: [2][175/817]	Loss 0.2013 (0.4101)	
training:	Epoch: [2][176/817]	Loss 0.3822 (0.4099)	
training:	Epoch: [2][177/817]	Loss 0.4999 (0.4104)	
training:	Epoch: [2][178/817]	Loss 0.5283 (0.4111)	
training:	Epoch: [2][179/817]	Loss 0.4856 (0.4115)	
training:	Epoch: [2][180/817]	Loss 0.2667 (0.4107)	
training:	Epoch: [2][181/817]	Loss 0.5478 (0.4115)	
training:	Epoch: [2][182/817]	Loss 0.1634 (0.4101)	
training:	Epoch: [2][183/817]	Loss 0.6093 (0.4112)	
training:	Epoch: [2][184/817]	Loss 0.7601 (0.4131)	
training:	Epoch: [2][185/817]	Loss 0.2421 (0.4122)	
training:	Epoch: [2][186/817]	Loss 0.4851 (0.4126)	
training:	Epoch: [2][187/817]	Loss 0.6594 (0.4139)	
training:	Epoch: [2][188/817]	Loss 0.2479 (0.4130)	
training:	Epoch: [2][189/817]	Loss 0.4916 (0.4134)	
training:	Epoch: [2][190/817]	Loss 0.4602 (0.4137)	
training:	Epoch: [2][191/817]	Loss 0.8783 (0.4161)	
training:	Epoch: [2][192/817]	Loss 0.6243 (0.4172)	
training:	Epoch: [2][193/817]	Loss 0.4717 (0.4175)	
training:	Epoch: [2][194/817]	Loss 0.3852 (0.4173)	
training:	Epoch: [2][195/817]	Loss 0.4421 (0.4174)	
training:	Epoch: [2][196/817]	Loss 0.4432 (0.4176)	
training:	Epoch: [2][197/817]	Loss 0.2621 (0.4168)	
training:	Epoch: [2][198/817]	Loss 0.7032 (0.4182)	
training:	Epoch: [2][199/817]	Loss 0.6875 (0.4196)	
training:	Epoch: [2][200/817]	Loss 0.3464 (0.4192)	
training:	Epoch: [2][201/817]	Loss 0.3075 (0.4186)	
training:	Epoch: [2][202/817]	Loss 0.4256 (0.4187)	
training:	Epoch: [2][203/817]	Loss 0.3482 (0.4183)	
training:	Epoch: [2][204/817]	Loss 0.5771 (0.4191)	
training:	Epoch: [2][205/817]	Loss 0.6188 (0.4201)	
training:	Epoch: [2][206/817]	Loss 0.6689 (0.4213)	
training:	Epoch: [2][207/817]	Loss 0.0950 (0.4197)	
training:	Epoch: [2][208/817]	Loss 0.1310 (0.4183)	
training:	Epoch: [2][209/817]	Loss 0.2403 (0.4175)	
training:	Epoch: [2][210/817]	Loss 0.5118 (0.4179)	
training:	Epoch: [2][211/817]	Loss 0.4419 (0.4180)	
training:	Epoch: [2][212/817]	Loss 0.8534 (0.4201)	
training:	Epoch: [2][213/817]	Loss 0.6152 (0.4210)	
training:	Epoch: [2][214/817]	Loss 0.6908 (0.4223)	
training:	Epoch: [2][215/817]	Loss 0.4136 (0.4222)	
training:	Epoch: [2][216/817]	Loss 0.2057 (0.4212)	
training:	Epoch: [2][217/817]	Loss 0.2947 (0.4206)	
training:	Epoch: [2][218/817]	Loss 0.5129 (0.4211)	
training:	Epoch: [2][219/817]	Loss 0.5237 (0.4215)	
training:	Epoch: [2][220/817]	Loss 0.5024 (0.4219)	
training:	Epoch: [2][221/817]	Loss 0.1277 (0.4206)	
training:	Epoch: [2][222/817]	Loss 0.1023 (0.4191)	
training:	Epoch: [2][223/817]	Loss 0.3420 (0.4188)	
training:	Epoch: [2][224/817]	Loss 0.6573 (0.4199)	
training:	Epoch: [2][225/817]	Loss 0.6900 (0.4211)	
training:	Epoch: [2][226/817]	Loss 0.4533 (0.4212)	
training:	Epoch: [2][227/817]	Loss 0.1874 (0.4202)	
training:	Epoch: [2][228/817]	Loss 0.6828 (0.4213)	
training:	Epoch: [2][229/817]	Loss 0.1756 (0.4203)	
training:	Epoch: [2][230/817]	Loss 0.2368 (0.4195)	
training:	Epoch: [2][231/817]	Loss 0.3782 (0.4193)	
training:	Epoch: [2][232/817]	Loss 0.3364 (0.4189)	
training:	Epoch: [2][233/817]	Loss 0.1696 (0.4179)	
training:	Epoch: [2][234/817]	Loss 0.1579 (0.4167)	
training:	Epoch: [2][235/817]	Loss 0.3751 (0.4166)	
training:	Epoch: [2][236/817]	Loss 0.5213 (0.4170)	
training:	Epoch: [2][237/817]	Loss 0.4676 (0.4172)	
training:	Epoch: [2][238/817]	Loss 0.3362 (0.4169)	
training:	Epoch: [2][239/817]	Loss 0.4823 (0.4172)	
training:	Epoch: [2][240/817]	Loss 0.3320 (0.4168)	
training:	Epoch: [2][241/817]	Loss 0.4940 (0.4171)	
training:	Epoch: [2][242/817]	Loss 0.4316 (0.4172)	
training:	Epoch: [2][243/817]	Loss 0.5373 (0.4177)	
training:	Epoch: [2][244/817]	Loss 0.3472 (0.4174)	
training:	Epoch: [2][245/817]	Loss 0.3132 (0.4170)	
training:	Epoch: [2][246/817]	Loss 0.3236 (0.4166)	
training:	Epoch: [2][247/817]	Loss 0.2287 (0.4158)	
training:	Epoch: [2][248/817]	Loss 0.1310 (0.4147)	
training:	Epoch: [2][249/817]	Loss 0.1653 (0.4137)	
training:	Epoch: [2][250/817]	Loss 0.2596 (0.4131)	
training:	Epoch: [2][251/817]	Loss 0.2483 (0.4124)	
training:	Epoch: [2][252/817]	Loss 0.1810 (0.4115)	
training:	Epoch: [2][253/817]	Loss 0.2777 (0.4109)	
training:	Epoch: [2][254/817]	Loss 0.4973 (0.4113)	
training:	Epoch: [2][255/817]	Loss 0.7343 (0.4126)	
training:	Epoch: [2][256/817]	Loss 0.4588 (0.4127)	
training:	Epoch: [2][257/817]	Loss 0.4871 (0.4130)	
training:	Epoch: [2][258/817]	Loss 0.2209 (0.4123)	
training:	Epoch: [2][259/817]	Loss 0.2512 (0.4117)	
training:	Epoch: [2][260/817]	Loss 1.0831 (0.4142)	
training:	Epoch: [2][261/817]	Loss 0.2934 (0.4138)	
training:	Epoch: [2][262/817]	Loss 0.2214 (0.4130)	
training:	Epoch: [2][263/817]	Loss 0.2287 (0.4123)	
training:	Epoch: [2][264/817]	Loss 0.6388 (0.4132)	
training:	Epoch: [2][265/817]	Loss 0.1215 (0.4121)	
training:	Epoch: [2][266/817]	Loss 0.5183 (0.4125)	
training:	Epoch: [2][267/817]	Loss 0.2332 (0.4118)	
training:	Epoch: [2][268/817]	Loss 0.1623 (0.4109)	
training:	Epoch: [2][269/817]	Loss 0.4140 (0.4109)	
training:	Epoch: [2][270/817]	Loss 0.2238 (0.4102)	
training:	Epoch: [2][271/817]	Loss 0.8121 (0.4117)	
training:	Epoch: [2][272/817]	Loss 0.6341 (0.4125)	
training:	Epoch: [2][273/817]	Loss 0.3990 (0.4125)	
training:	Epoch: [2][274/817]	Loss 0.8065 (0.4139)	
training:	Epoch: [2][275/817]	Loss 0.1400 (0.4129)	
training:	Epoch: [2][276/817]	Loss 0.2499 (0.4123)	
training:	Epoch: [2][277/817]	Loss 0.3314 (0.4120)	
training:	Epoch: [2][278/817]	Loss 0.3176 (0.4117)	
training:	Epoch: [2][279/817]	Loss 0.5019 (0.4120)	
training:	Epoch: [2][280/817]	Loss 0.1235 (0.4110)	
training:	Epoch: [2][281/817]	Loss 0.4047 (0.4110)	
training:	Epoch: [2][282/817]	Loss 0.4328 (0.4110)	
training:	Epoch: [2][283/817]	Loss 0.5084 (0.4114)	
training:	Epoch: [2][284/817]	Loss 0.1965 (0.4106)	
training:	Epoch: [2][285/817]	Loss 0.3793 (0.4105)	
training:	Epoch: [2][286/817]	Loss 0.8459 (0.4120)	
training:	Epoch: [2][287/817]	Loss 0.5879 (0.4126)	
training:	Epoch: [2][288/817]	Loss 0.1434 (0.4117)	
training:	Epoch: [2][289/817]	Loss 0.7307 (0.4128)	
training:	Epoch: [2][290/817]	Loss 0.3805 (0.4127)	
training:	Epoch: [2][291/817]	Loss 0.4234 (0.4127)	
training:	Epoch: [2][292/817]	Loss 0.5242 (0.4131)	
training:	Epoch: [2][293/817]	Loss 0.3995 (0.4131)	
training:	Epoch: [2][294/817]	Loss 0.4187 (0.4131)	
training:	Epoch: [2][295/817]	Loss 0.4353 (0.4132)	
training:	Epoch: [2][296/817]	Loss 0.6559 (0.4140)	
training:	Epoch: [2][297/817]	Loss 0.1957 (0.4133)	
training:	Epoch: [2][298/817]	Loss 0.4969 (0.4135)	
training:	Epoch: [2][299/817]	Loss 0.3752 (0.4134)	
training:	Epoch: [2][300/817]	Loss 0.6816 (0.4143)	
training:	Epoch: [2][301/817]	Loss 0.3364 (0.4140)	
training:	Epoch: [2][302/817]	Loss 0.4502 (0.4142)	
training:	Epoch: [2][303/817]	Loss 0.3850 (0.4141)	
training:	Epoch: [2][304/817]	Loss 0.1727 (0.4133)	
training:	Epoch: [2][305/817]	Loss 0.4498 (0.4134)	
training:	Epoch: [2][306/817]	Loss 0.2732 (0.4129)	
training:	Epoch: [2][307/817]	Loss 0.3900 (0.4129)	
training:	Epoch: [2][308/817]	Loss 0.6558 (0.4136)	
training:	Epoch: [2][309/817]	Loss 0.1883 (0.4129)	
training:	Epoch: [2][310/817]	Loss 0.1623 (0.4121)	
training:	Epoch: [2][311/817]	Loss 0.3629 (0.4120)	
training:	Epoch: [2][312/817]	Loss 0.5032 (0.4122)	
training:	Epoch: [2][313/817]	Loss 0.5040 (0.4125)	
training:	Epoch: [2][314/817]	Loss 0.2738 (0.4121)	
training:	Epoch: [2][315/817]	Loss 0.5381 (0.4125)	
training:	Epoch: [2][316/817]	Loss 0.2897 (0.4121)	
training:	Epoch: [2][317/817]	Loss 0.7570 (0.4132)	
training:	Epoch: [2][318/817]	Loss 0.4010 (0.4132)	
training:	Epoch: [2][319/817]	Loss 0.8312 (0.4145)	
training:	Epoch: [2][320/817]	Loss 0.2024 (0.4138)	
training:	Epoch: [2][321/817]	Loss 0.4407 (0.4139)	
training:	Epoch: [2][322/817]	Loss 0.3359 (0.4136)	
training:	Epoch: [2][323/817]	Loss 0.7373 (0.4146)	
training:	Epoch: [2][324/817]	Loss 0.4715 (0.4148)	
training:	Epoch: [2][325/817]	Loss 0.4547 (0.4149)	
training:	Epoch: [2][326/817]	Loss 0.2734 (0.4145)	
training:	Epoch: [2][327/817]	Loss 0.6934 (0.4154)	
training:	Epoch: [2][328/817]	Loss 0.5862 (0.4159)	
training:	Epoch: [2][329/817]	Loss 0.1821 (0.4152)	
training:	Epoch: [2][330/817]	Loss 0.4000 (0.4151)	
training:	Epoch: [2][331/817]	Loss 0.1784 (0.4144)	
training:	Epoch: [2][332/817]	Loss 0.4835 (0.4146)	
training:	Epoch: [2][333/817]	Loss 0.2796 (0.4142)	
training:	Epoch: [2][334/817]	Loss 0.3589 (0.4141)	
training:	Epoch: [2][335/817]	Loss 0.1678 (0.4133)	
training:	Epoch: [2][336/817]	Loss 0.4137 (0.4133)	
training:	Epoch: [2][337/817]	Loss 0.4891 (0.4135)	
training:	Epoch: [2][338/817]	Loss 0.1791 (0.4128)	
training:	Epoch: [2][339/817]	Loss 0.5373 (0.4132)	
training:	Epoch: [2][340/817]	Loss 0.3085 (0.4129)	
training:	Epoch: [2][341/817]	Loss 0.5850 (0.4134)	
training:	Epoch: [2][342/817]	Loss 0.1610 (0.4127)	
training:	Epoch: [2][343/817]	Loss 0.7453 (0.4136)	
training:	Epoch: [2][344/817]	Loss 0.5906 (0.4142)	
training:	Epoch: [2][345/817]	Loss 0.1980 (0.4135)	
training:	Epoch: [2][346/817]	Loss 0.3244 (0.4133)	
training:	Epoch: [2][347/817]	Loss 0.0994 (0.4124)	
training:	Epoch: [2][348/817]	Loss 0.5560 (0.4128)	
training:	Epoch: [2][349/817]	Loss 0.3795 (0.4127)	
training:	Epoch: [2][350/817]	Loss 0.2064 (0.4121)	
training:	Epoch: [2][351/817]	Loss 0.4860 (0.4123)	
training:	Epoch: [2][352/817]	Loss 0.5635 (0.4127)	
training:	Epoch: [2][353/817]	Loss 0.7289 (0.4136)	
training:	Epoch: [2][354/817]	Loss 0.3840 (0.4136)	
training:	Epoch: [2][355/817]	Loss 0.8739 (0.4148)	
training:	Epoch: [2][356/817]	Loss 0.5058 (0.4151)	
training:	Epoch: [2][357/817]	Loss 0.6241 (0.4157)	
training:	Epoch: [2][358/817]	Loss 0.1901 (0.4151)	
training:	Epoch: [2][359/817]	Loss 0.3602 (0.4149)	
training:	Epoch: [2][360/817]	Loss 0.7452 (0.4158)	
training:	Epoch: [2][361/817]	Loss 0.4300 (0.4159)	
training:	Epoch: [2][362/817]	Loss 0.2869 (0.4155)	
training:	Epoch: [2][363/817]	Loss 0.1351 (0.4147)	
training:	Epoch: [2][364/817]	Loss 0.9056 (0.4161)	
training:	Epoch: [2][365/817]	Loss 0.9164 (0.4175)	
training:	Epoch: [2][366/817]	Loss 0.2772 (0.4171)	
training:	Epoch: [2][367/817]	Loss 0.3421 (0.4169)	
training:	Epoch: [2][368/817]	Loss 0.1114 (0.4160)	
training:	Epoch: [2][369/817]	Loss 0.4248 (0.4161)	
training:	Epoch: [2][370/817]	Loss 0.4860 (0.4162)	
training:	Epoch: [2][371/817]	Loss 0.4795 (0.4164)	
training:	Epoch: [2][372/817]	Loss 0.6639 (0.4171)	
training:	Epoch: [2][373/817]	Loss 0.3665 (0.4169)	
training:	Epoch: [2][374/817]	Loss 0.2622 (0.4165)	
training:	Epoch: [2][375/817]	Loss 0.3388 (0.4163)	
training:	Epoch: [2][376/817]	Loss 0.2845 (0.4160)	
training:	Epoch: [2][377/817]	Loss 0.5041 (0.4162)	
training:	Epoch: [2][378/817]	Loss 0.4892 (0.4164)	
training:	Epoch: [2][379/817]	Loss 0.5890 (0.4169)	
training:	Epoch: [2][380/817]	Loss 0.7349 (0.4177)	
training:	Epoch: [2][381/817]	Loss 0.5243 (0.4180)	
training:	Epoch: [2][382/817]	Loss 0.4490 (0.4181)	
training:	Epoch: [2][383/817]	Loss 0.5587 (0.4184)	
training:	Epoch: [2][384/817]	Loss 0.3970 (0.4184)	
training:	Epoch: [2][385/817]	Loss 0.2916 (0.4180)	
training:	Epoch: [2][386/817]	Loss 0.5375 (0.4183)	
training:	Epoch: [2][387/817]	Loss 0.6669 (0.4190)	
training:	Epoch: [2][388/817]	Loss 0.3892 (0.4189)	
training:	Epoch: [2][389/817]	Loss 0.2977 (0.4186)	
training:	Epoch: [2][390/817]	Loss 0.1977 (0.4180)	
training:	Epoch: [2][391/817]	Loss 0.2189 (0.4175)	
training:	Epoch: [2][392/817]	Loss 0.2682 (0.4171)	
training:	Epoch: [2][393/817]	Loss 0.4092 (0.4171)	
training:	Epoch: [2][394/817]	Loss 0.7062 (0.4179)	
training:	Epoch: [2][395/817]	Loss 0.4470 (0.4179)	
training:	Epoch: [2][396/817]	Loss 0.1653 (0.4173)	
training:	Epoch: [2][397/817]	Loss 0.4637 (0.4174)	
training:	Epoch: [2][398/817]	Loss 0.1943 (0.4168)	
training:	Epoch: [2][399/817]	Loss 0.4901 (0.4170)	
training:	Epoch: [2][400/817]	Loss 0.4117 (0.4170)	
training:	Epoch: [2][401/817]	Loss 0.9408 (0.4183)	
training:	Epoch: [2][402/817]	Loss 0.3260 (0.4181)	
training:	Epoch: [2][403/817]	Loss 0.2487 (0.4177)	
training:	Epoch: [2][404/817]	Loss 1.0261 (0.4192)	
training:	Epoch: [2][405/817]	Loss 0.1910 (0.4186)	
training:	Epoch: [2][406/817]	Loss 0.5598 (0.4190)	
training:	Epoch: [2][407/817]	Loss 0.1566 (0.4183)	
training:	Epoch: [2][408/817]	Loss 0.2326 (0.4179)	
training:	Epoch: [2][409/817]	Loss 0.4948 (0.4181)	
training:	Epoch: [2][410/817]	Loss 0.4333 (0.4181)	
training:	Epoch: [2][411/817]	Loss 1.2697 (0.4202)	
training:	Epoch: [2][412/817]	Loss 0.3692 (0.4200)	
training:	Epoch: [2][413/817]	Loss 0.2330 (0.4196)	
training:	Epoch: [2][414/817]	Loss 0.3200 (0.4193)	
training:	Epoch: [2][415/817]	Loss 0.7027 (0.4200)	
training:	Epoch: [2][416/817]	Loss 0.3492 (0.4199)	
training:	Epoch: [2][417/817]	Loss 0.3602 (0.4197)	
training:	Epoch: [2][418/817]	Loss 0.2086 (0.4192)	
training:	Epoch: [2][419/817]	Loss 0.4980 (0.4194)	
training:	Epoch: [2][420/817]	Loss 0.2581 (0.4190)	
training:	Epoch: [2][421/817]	Loss 0.6612 (0.4196)	
training:	Epoch: [2][422/817]	Loss 0.3049 (0.4193)	
training:	Epoch: [2][423/817]	Loss 0.4173 (0.4193)	
training:	Epoch: [2][424/817]	Loss 0.5947 (0.4197)	
training:	Epoch: [2][425/817]	Loss 0.5176 (0.4200)	
training:	Epoch: [2][426/817]	Loss 0.3218 (0.4197)	
training:	Epoch: [2][427/817]	Loss 0.5095 (0.4199)	
training:	Epoch: [2][428/817]	Loss 0.7522 (0.4207)	
training:	Epoch: [2][429/817]	Loss 0.6699 (0.4213)	
training:	Epoch: [2][430/817]	Loss 0.5093 (0.4215)	
training:	Epoch: [2][431/817]	Loss 0.3814 (0.4214)	
training:	Epoch: [2][432/817]	Loss 0.4180 (0.4214)	
training:	Epoch: [2][433/817]	Loss 0.5286 (0.4216)	
training:	Epoch: [2][434/817]	Loss 0.3307 (0.4214)	
training:	Epoch: [2][435/817]	Loss 0.2336 (0.4210)	
training:	Epoch: [2][436/817]	Loss 0.5393 (0.4213)	
training:	Epoch: [2][437/817]	Loss 0.2435 (0.4209)	
training:	Epoch: [2][438/817]	Loss 0.5508 (0.4212)	
training:	Epoch: [2][439/817]	Loss 0.1827 (0.4206)	
training:	Epoch: [2][440/817]	Loss 0.2762 (0.4203)	
training:	Epoch: [2][441/817]	Loss 0.2422 (0.4199)	
training:	Epoch: [2][442/817]	Loss 0.2898 (0.4196)	
training:	Epoch: [2][443/817]	Loss 0.5996 (0.4200)	
training:	Epoch: [2][444/817]	Loss 0.5201 (0.4202)	
training:	Epoch: [2][445/817]	Loss 0.3171 (0.4200)	
training:	Epoch: [2][446/817]	Loss 0.9351 (0.4212)	
training:	Epoch: [2][447/817]	Loss 0.7806 (0.4220)	
training:	Epoch: [2][448/817]	Loss 0.5350 (0.4222)	
training:	Epoch: [2][449/817]	Loss 0.2062 (0.4217)	
training:	Epoch: [2][450/817]	Loss 0.2543 (0.4214)	
training:	Epoch: [2][451/817]	Loss 0.6265 (0.4218)	
training:	Epoch: [2][452/817]	Loss 0.7039 (0.4224)	
training:	Epoch: [2][453/817]	Loss 0.4059 (0.4224)	
training:	Epoch: [2][454/817]	Loss 0.3782 (0.4223)	
training:	Epoch: [2][455/817]	Loss 0.3984 (0.4222)	
training:	Epoch: [2][456/817]	Loss 0.5303 (0.4225)	
training:	Epoch: [2][457/817]	Loss 0.1845 (0.4220)	
training:	Epoch: [2][458/817]	Loss 0.6210 (0.4224)	
training:	Epoch: [2][459/817]	Loss 0.9210 (0.4235)	
training:	Epoch: [2][460/817]	Loss 0.5478 (0.4238)	
training:	Epoch: [2][461/817]	Loss 0.4219 (0.4238)	
training:	Epoch: [2][462/817]	Loss 0.6853 (0.4243)	
training:	Epoch: [2][463/817]	Loss 0.6317 (0.4248)	
training:	Epoch: [2][464/817]	Loss 0.3480 (0.4246)	
training:	Epoch: [2][465/817]	Loss 0.8572 (0.4255)	
training:	Epoch: [2][466/817]	Loss 0.6951 (0.4261)	
training:	Epoch: [2][467/817]	Loss 0.5054 (0.4263)	
training:	Epoch: [2][468/817]	Loss 0.6432 (0.4267)	
training:	Epoch: [2][469/817]	Loss 0.4054 (0.4267)	
training:	Epoch: [2][470/817]	Loss 0.3090 (0.4264)	
training:	Epoch: [2][471/817]	Loss 0.7765 (0.4272)	
training:	Epoch: [2][472/817]	Loss 0.3621 (0.4271)	
training:	Epoch: [2][473/817]	Loss 0.3602 (0.4269)	
training:	Epoch: [2][474/817]	Loss 1.0891 (0.4283)	
training:	Epoch: [2][475/817]	Loss 0.5655 (0.4286)	
training:	Epoch: [2][476/817]	Loss 0.5263 (0.4288)	
training:	Epoch: [2][477/817]	Loss 0.2414 (0.4284)	
training:	Epoch: [2][478/817]	Loss 0.3791 (0.4283)	
training:	Epoch: [2][479/817]	Loss 0.2184 (0.4279)	
training:	Epoch: [2][480/817]	Loss 0.5108 (0.4280)	
training:	Epoch: [2][481/817]	Loss 0.3140 (0.4278)	
training:	Epoch: [2][482/817]	Loss 0.6321 (0.4282)	
training:	Epoch: [2][483/817]	Loss 0.5240 (0.4284)	
training:	Epoch: [2][484/817]	Loss 0.4732 (0.4285)	
training:	Epoch: [2][485/817]	Loss 0.5664 (0.4288)	
training:	Epoch: [2][486/817]	Loss 0.2754 (0.4285)	
training:	Epoch: [2][487/817]	Loss 0.4736 (0.4286)	
training:	Epoch: [2][488/817]	Loss 0.2842 (0.4283)	
training:	Epoch: [2][489/817]	Loss 0.1583 (0.4277)	
training:	Epoch: [2][490/817]	Loss 0.4046 (0.4277)	
training:	Epoch: [2][491/817]	Loss 0.2681 (0.4274)	
training:	Epoch: [2][492/817]	Loss 0.3366 (0.4272)	
training:	Epoch: [2][493/817]	Loss 0.2964 (0.4269)	
training:	Epoch: [2][494/817]	Loss 0.2995 (0.4266)	
training:	Epoch: [2][495/817]	Loss 0.4469 (0.4267)	
training:	Epoch: [2][496/817]	Loss 0.3723 (0.4266)	
training:	Epoch: [2][497/817]	Loss 0.5067 (0.4267)	
training:	Epoch: [2][498/817]	Loss 0.4615 (0.4268)	
training:	Epoch: [2][499/817]	Loss 0.2752 (0.4265)	
training:	Epoch: [2][500/817]	Loss 0.6591 (0.4270)	
training:	Epoch: [2][501/817]	Loss 0.4016 (0.4269)	
training:	Epoch: [2][502/817]	Loss 0.7082 (0.4275)	
training:	Epoch: [2][503/817]	Loss 0.2010 (0.4270)	
training:	Epoch: [2][504/817]	Loss 0.3336 (0.4268)	
training:	Epoch: [2][505/817]	Loss 0.5653 (0.4271)	
training:	Epoch: [2][506/817]	Loss 0.1362 (0.4265)	
training:	Epoch: [2][507/817]	Loss 0.3192 (0.4263)	
training:	Epoch: [2][508/817]	Loss 0.4478 (0.4264)	
training:	Epoch: [2][509/817]	Loss 0.2519 (0.4260)	
training:	Epoch: [2][510/817]	Loss 0.2458 (0.4257)	
training:	Epoch: [2][511/817]	Loss 0.6676 (0.4262)	
training:	Epoch: [2][512/817]	Loss 0.1324 (0.4256)	
training:	Epoch: [2][513/817]	Loss 0.4069 (0.4255)	
training:	Epoch: [2][514/817]	Loss 0.5130 (0.4257)	
training:	Epoch: [2][515/817]	Loss 0.2973 (0.4255)	
training:	Epoch: [2][516/817]	Loss 0.3302 (0.4253)	
training:	Epoch: [2][517/817]	Loss 0.2777 (0.4250)	
training:	Epoch: [2][518/817]	Loss 0.5545 (0.4252)	
training:	Epoch: [2][519/817]	Loss 0.6831 (0.4257)	
training:	Epoch: [2][520/817]	Loss 0.5083 (0.4259)	
training:	Epoch: [2][521/817]	Loss 0.4488 (0.4259)	
training:	Epoch: [2][522/817]	Loss 0.2297 (0.4256)	
training:	Epoch: [2][523/817]	Loss 0.6309 (0.4260)	
training:	Epoch: [2][524/817]	Loss 0.2648 (0.4257)	
training:	Epoch: [2][525/817]	Loss 0.2466 (0.4253)	
training:	Epoch: [2][526/817]	Loss 0.3792 (0.4252)	
training:	Epoch: [2][527/817]	Loss 0.5563 (0.4255)	
training:	Epoch: [2][528/817]	Loss 0.3251 (0.4253)	
training:	Epoch: [2][529/817]	Loss 0.8836 (0.4261)	
training:	Epoch: [2][530/817]	Loss 0.2308 (0.4258)	
training:	Epoch: [2][531/817]	Loss 0.3222 (0.4256)	
training:	Epoch: [2][532/817]	Loss 0.3641 (0.4255)	
training:	Epoch: [2][533/817]	Loss 0.4783 (0.4256)	
training:	Epoch: [2][534/817]	Loss 0.4148 (0.4255)	
training:	Epoch: [2][535/817]	Loss 0.5404 (0.4258)	
training:	Epoch: [2][536/817]	Loss 0.3618 (0.4256)	
training:	Epoch: [2][537/817]	Loss 0.1085 (0.4251)	
training:	Epoch: [2][538/817]	Loss 0.4294 (0.4251)	
training:	Epoch: [2][539/817]	Loss 0.4232 (0.4251)	
training:	Epoch: [2][540/817]	Loss 0.5639 (0.4253)	
training:	Epoch: [2][541/817]	Loss 0.1200 (0.4248)	
training:	Epoch: [2][542/817]	Loss 0.4933 (0.4249)	
training:	Epoch: [2][543/817]	Loss 0.2782 (0.4246)	
training:	Epoch: [2][544/817]	Loss 0.2308 (0.4243)	
training:	Epoch: [2][545/817]	Loss 0.7999 (0.4249)	
training:	Epoch: [2][546/817]	Loss 0.3894 (0.4249)	
training:	Epoch: [2][547/817]	Loss 0.2636 (0.4246)	
training:	Epoch: [2][548/817]	Loss 0.2043 (0.4242)	
training:	Epoch: [2][549/817]	Loss 0.3996 (0.4241)	
training:	Epoch: [2][550/817]	Loss 0.3548 (0.4240)	
training:	Epoch: [2][551/817]	Loss 0.3047 (0.4238)	
training:	Epoch: [2][552/817]	Loss 0.1929 (0.4234)	
training:	Epoch: [2][553/817]	Loss 0.3721 (0.4233)	
training:	Epoch: [2][554/817]	Loss 0.3688 (0.4232)	
training:	Epoch: [2][555/817]	Loss 0.2642 (0.4229)	
training:	Epoch: [2][556/817]	Loss 0.5035 (0.4230)	
training:	Epoch: [2][557/817]	Loss 0.2747 (0.4228)	
training:	Epoch: [2][558/817]	Loss 0.5938 (0.4231)	
training:	Epoch: [2][559/817]	Loss 0.2209 (0.4227)	
training:	Epoch: [2][560/817]	Loss 0.4412 (0.4228)	
training:	Epoch: [2][561/817]	Loss 0.5013 (0.4229)	
training:	Epoch: [2][562/817]	Loss 0.2532 (0.4226)	
training:	Epoch: [2][563/817]	Loss 0.2163 (0.4222)	
training:	Epoch: [2][564/817]	Loss 0.1261 (0.4217)	
training:	Epoch: [2][565/817]	Loss 0.2479 (0.4214)	
training:	Epoch: [2][566/817]	Loss 0.2624 (0.4211)	
training:	Epoch: [2][567/817]	Loss 0.4596 (0.4212)	
training:	Epoch: [2][568/817]	Loss 0.8197 (0.4219)	
training:	Epoch: [2][569/817]	Loss 0.4605 (0.4219)	
training:	Epoch: [2][570/817]	Loss 0.2887 (0.4217)	
training:	Epoch: [2][571/817]	Loss 0.3001 (0.4215)	
training:	Epoch: [2][572/817]	Loss 0.4356 (0.4215)	
training:	Epoch: [2][573/817]	Loss 0.2226 (0.4212)	
training:	Epoch: [2][574/817]	Loss 0.3890 (0.4211)	
training:	Epoch: [2][575/817]	Loss 0.4902 (0.4212)	
training:	Epoch: [2][576/817]	Loss 0.4894 (0.4214)	
training:	Epoch: [2][577/817]	Loss 0.1512 (0.4209)	
training:	Epoch: [2][578/817]	Loss 0.5808 (0.4212)	
training:	Epoch: [2][579/817]	Loss 0.6298 (0.4215)	
training:	Epoch: [2][580/817]	Loss 0.1285 (0.4210)	
training:	Epoch: [2][581/817]	Loss 0.6577 (0.4214)	
training:	Epoch: [2][582/817]	Loss 0.5727 (0.4217)	
training:	Epoch: [2][583/817]	Loss 0.4159 (0.4217)	
training:	Epoch: [2][584/817]	Loss 0.3541 (0.4216)	
training:	Epoch: [2][585/817]	Loss 0.3150 (0.4214)	
training:	Epoch: [2][586/817]	Loss 0.1054 (0.4208)	
training:	Epoch: [2][587/817]	Loss 0.5707 (0.4211)	
training:	Epoch: [2][588/817]	Loss 0.6834 (0.4215)	
training:	Epoch: [2][589/817]	Loss 0.8639 (0.4223)	
training:	Epoch: [2][590/817]	Loss 0.2699 (0.4220)	
training:	Epoch: [2][591/817]	Loss 0.2425 (0.4217)	
training:	Epoch: [2][592/817]	Loss 0.4455 (0.4218)	
training:	Epoch: [2][593/817]	Loss 0.3586 (0.4217)	
training:	Epoch: [2][594/817]	Loss 0.9428 (0.4225)	
training:	Epoch: [2][595/817]	Loss 0.4118 (0.4225)	
training:	Epoch: [2][596/817]	Loss 0.2162 (0.4222)	
training:	Epoch: [2][597/817]	Loss 0.7521 (0.4227)	
training:	Epoch: [2][598/817]	Loss 0.2978 (0.4225)	
training:	Epoch: [2][599/817]	Loss 0.4360 (0.4225)	
training:	Epoch: [2][600/817]	Loss 0.4827 (0.4226)	
training:	Epoch: [2][601/817]	Loss 0.3824 (0.4226)	
training:	Epoch: [2][602/817]	Loss 0.2117 (0.4222)	
training:	Epoch: [2][603/817]	Loss 0.2578 (0.4220)	
training:	Epoch: [2][604/817]	Loss 0.4054 (0.4219)	
training:	Epoch: [2][605/817]	Loss 0.3565 (0.4218)	
training:	Epoch: [2][606/817]	Loss 0.2202 (0.4215)	
training:	Epoch: [2][607/817]	Loss 0.3406 (0.4214)	
training:	Epoch: [2][608/817]	Loss 0.4091 (0.4213)	
training:	Epoch: [2][609/817]	Loss 0.2985 (0.4211)	
training:	Epoch: [2][610/817]	Loss 0.3038 (0.4209)	
training:	Epoch: [2][611/817]	Loss 0.4002 (0.4209)	
training:	Epoch: [2][612/817]	Loss 0.2224 (0.4206)	
training:	Epoch: [2][613/817]	Loss 0.7140 (0.4211)	
training:	Epoch: [2][614/817]	Loss 0.2381 (0.4208)	
training:	Epoch: [2][615/817]	Loss 0.2322 (0.4205)	
training:	Epoch: [2][616/817]	Loss 0.1943 (0.4201)	
training:	Epoch: [2][617/817]	Loss 0.2616 (0.4198)	
training:	Epoch: [2][618/817]	Loss 0.1731 (0.4194)	
training:	Epoch: [2][619/817]	Loss 0.4252 (0.4194)	
training:	Epoch: [2][620/817]	Loss 0.2463 (0.4192)	
training:	Epoch: [2][621/817]	Loss 0.1815 (0.4188)	
training:	Epoch: [2][622/817]	Loss 0.3804 (0.4187)	
training:	Epoch: [2][623/817]	Loss 0.2899 (0.4185)	
training:	Epoch: [2][624/817]	Loss 0.1299 (0.4181)	
training:	Epoch: [2][625/817]	Loss 0.4753 (0.4181)	
training:	Epoch: [2][626/817]	Loss 0.2015 (0.4178)	
training:	Epoch: [2][627/817]	Loss 0.2429 (0.4175)	
training:	Epoch: [2][628/817]	Loss 0.4245 (0.4175)	
training:	Epoch: [2][629/817]	Loss 0.4626 (0.4176)	
training:	Epoch: [2][630/817]	Loss 0.3463 (0.4175)	
training:	Epoch: [2][631/817]	Loss 0.4898 (0.4176)	
training:	Epoch: [2][632/817]	Loss 0.1431 (0.4172)	
training:	Epoch: [2][633/817]	Loss 0.4169 (0.4172)	
training:	Epoch: [2][634/817]	Loss 0.3991 (0.4171)	
training:	Epoch: [2][635/817]	Loss 0.3906 (0.4171)	
training:	Epoch: [2][636/817]	Loss 0.2645 (0.4169)	
training:	Epoch: [2][637/817]	Loss 0.6959 (0.4173)	
training:	Epoch: [2][638/817]	Loss 0.5181 (0.4175)	
training:	Epoch: [2][639/817]	Loss 0.2180 (0.4171)	
training:	Epoch: [2][640/817]	Loss 0.1444 (0.4167)	
training:	Epoch: [2][641/817]	Loss 0.1326 (0.4163)	
training:	Epoch: [2][642/817]	Loss 0.2651 (0.4160)	
training:	Epoch: [2][643/817]	Loss 0.2965 (0.4158)	
training:	Epoch: [2][644/817]	Loss 0.2732 (0.4156)	
training:	Epoch: [2][645/817]	Loss 0.3612 (0.4155)	
training:	Epoch: [2][646/817]	Loss 0.4649 (0.4156)	
training:	Epoch: [2][647/817]	Loss 0.5998 (0.4159)	
training:	Epoch: [2][648/817]	Loss 0.4839 (0.4160)	
training:	Epoch: [2][649/817]	Loss 0.6665 (0.4164)	
training:	Epoch: [2][650/817]	Loss 0.3063 (0.4162)	
training:	Epoch: [2][651/817]	Loss 0.5791 (0.4165)	
training:	Epoch: [2][652/817]	Loss 0.1007 (0.4160)	
training:	Epoch: [2][653/817]	Loss 0.0805 (0.4155)	
training:	Epoch: [2][654/817]	Loss 0.6892 (0.4159)	
training:	Epoch: [2][655/817]	Loss 0.3010 (0.4157)	
training:	Epoch: [2][656/817]	Loss 0.1954 (0.4154)	
training:	Epoch: [2][657/817]	Loss 0.2310 (0.4151)	
training:	Epoch: [2][658/817]	Loss 0.2660 (0.4149)	
training:	Epoch: [2][659/817]	Loss 0.1009 (0.4144)	
training:	Epoch: [2][660/817]	Loss 0.4986 (0.4145)	
training:	Epoch: [2][661/817]	Loss 0.4693 (0.4146)	
training:	Epoch: [2][662/817]	Loss 0.2856 (0.4144)	
training:	Epoch: [2][663/817]	Loss 0.1615 (0.4140)	
training:	Epoch: [2][664/817]	Loss 0.6395 (0.4144)	
training:	Epoch: [2][665/817]	Loss 0.3287 (0.4142)	
training:	Epoch: [2][666/817]	Loss 0.6255 (0.4146)	
training:	Epoch: [2][667/817]	Loss 0.2356 (0.4143)	
training:	Epoch: [2][668/817]	Loss 0.3370 (0.4142)	
training:	Epoch: [2][669/817]	Loss 0.3739 (0.4141)	
training:	Epoch: [2][670/817]	Loss 0.1272 (0.4137)	
training:	Epoch: [2][671/817]	Loss 0.2765 (0.4135)	
training:	Epoch: [2][672/817]	Loss 0.1370 (0.4131)	
training:	Epoch: [2][673/817]	Loss 0.1055 (0.4126)	
training:	Epoch: [2][674/817]	Loss 0.4570 (0.4127)	
training:	Epoch: [2][675/817]	Loss 0.1892 (0.4124)	
training:	Epoch: [2][676/817]	Loss 0.3012 (0.4122)	
training:	Epoch: [2][677/817]	Loss 0.4736 (0.4123)	
training:	Epoch: [2][678/817]	Loss 0.4947 (0.4124)	
training:	Epoch: [2][679/817]	Loss 0.5761 (0.4126)	
training:	Epoch: [2][680/817]	Loss 0.3449 (0.4125)	
training:	Epoch: [2][681/817]	Loss 0.1101 (0.4121)	
training:	Epoch: [2][682/817]	Loss 0.4999 (0.4122)	
training:	Epoch: [2][683/817]	Loss 0.5948 (0.4125)	
training:	Epoch: [2][684/817]	Loss 0.6954 (0.4129)	
training:	Epoch: [2][685/817]	Loss 0.4666 (0.4130)	
training:	Epoch: [2][686/817]	Loss 0.5438 (0.4132)	
training:	Epoch: [2][687/817]	Loss 0.9115 (0.4139)	
training:	Epoch: [2][688/817]	Loss 0.3784 (0.4139)	
training:	Epoch: [2][689/817]	Loss 0.1879 (0.4135)	
training:	Epoch: [2][690/817]	Loss 0.8025 (0.4141)	
training:	Epoch: [2][691/817]	Loss 0.7252 (0.4145)	
training:	Epoch: [2][692/817]	Loss 0.3072 (0.4144)	
training:	Epoch: [2][693/817]	Loss 0.7717 (0.4149)	
training:	Epoch: [2][694/817]	Loss 0.5406 (0.4151)	
training:	Epoch: [2][695/817]	Loss 0.2470 (0.4148)	
training:	Epoch: [2][696/817]	Loss 0.2616 (0.4146)	
training:	Epoch: [2][697/817]	Loss 0.3210 (0.4145)	
training:	Epoch: [2][698/817]	Loss 0.5265 (0.4146)	
training:	Epoch: [2][699/817]	Loss 0.2092 (0.4143)	
training:	Epoch: [2][700/817]	Loss 0.4174 (0.4144)	
training:	Epoch: [2][701/817]	Loss 0.1598 (0.4140)	
training:	Epoch: [2][702/817]	Loss 0.3453 (0.4139)	
training:	Epoch: [2][703/817]	Loss 0.2946 (0.4137)	
training:	Epoch: [2][704/817]	Loss 0.4714 (0.4138)	
training:	Epoch: [2][705/817]	Loss 0.2385 (0.4136)	
training:	Epoch: [2][706/817]	Loss 0.2530 (0.4133)	
training:	Epoch: [2][707/817]	Loss 0.2530 (0.4131)	
training:	Epoch: [2][708/817]	Loss 0.1790 (0.4128)	
training:	Epoch: [2][709/817]	Loss 0.2391 (0.4125)	
training:	Epoch: [2][710/817]	Loss 0.6173 (0.4128)	
training:	Epoch: [2][711/817]	Loss 0.4532 (0.4129)	
training:	Epoch: [2][712/817]	Loss 0.1502 (0.4125)	
training:	Epoch: [2][713/817]	Loss 0.1704 (0.4122)	
training:	Epoch: [2][714/817]	Loss 0.3912 (0.4121)	
training:	Epoch: [2][715/817]	Loss 0.1909 (0.4118)	
training:	Epoch: [2][716/817]	Loss 0.1372 (0.4114)	
training:	Epoch: [2][717/817]	Loss 0.3293 (0.4113)	
training:	Epoch: [2][718/817]	Loss 0.1514 (0.4110)	
training:	Epoch: [2][719/817]	Loss 0.3577 (0.4109)	
training:	Epoch: [2][720/817]	Loss 0.1982 (0.4106)	
training:	Epoch: [2][721/817]	Loss 0.4817 (0.4107)	
training:	Epoch: [2][722/817]	Loss 0.3550 (0.4106)	
training:	Epoch: [2][723/817]	Loss 0.1512 (0.4103)	
training:	Epoch: [2][724/817]	Loss 0.5590 (0.4105)	
training:	Epoch: [2][725/817]	Loss 0.3047 (0.4103)	
training:	Epoch: [2][726/817]	Loss 0.3655 (0.4103)	
training:	Epoch: [2][727/817]	Loss 0.1599 (0.4099)	
training:	Epoch: [2][728/817]	Loss 0.3597 (0.4098)	
training:	Epoch: [2][729/817]	Loss 0.3841 (0.4098)	
training:	Epoch: [2][730/817]	Loss 0.1352 (0.4094)	
training:	Epoch: [2][731/817]	Loss 0.1510 (0.4091)	
training:	Epoch: [2][732/817]	Loss 0.4500 (0.4091)	
training:	Epoch: [2][733/817]	Loss 0.3802 (0.4091)	
training:	Epoch: [2][734/817]	Loss 0.1911 (0.4088)	
training:	Epoch: [2][735/817]	Loss 0.4561 (0.4089)	
training:	Epoch: [2][736/817]	Loss 0.6097 (0.4091)	
training:	Epoch: [2][737/817]	Loss 0.1656 (0.4088)	
training:	Epoch: [2][738/817]	Loss 0.6439 (0.4091)	
training:	Epoch: [2][739/817]	Loss 0.6777 (0.4095)	
training:	Epoch: [2][740/817]	Loss 0.0983 (0.4091)	
training:	Epoch: [2][741/817]	Loss 0.3937 (0.4090)	
training:	Epoch: [2][742/817]	Loss 0.4667 (0.4091)	
training:	Epoch: [2][743/817]	Loss 0.4307 (0.4092)	
training:	Epoch: [2][744/817]	Loss 0.2970 (0.4090)	
training:	Epoch: [2][745/817]	Loss 0.5570 (0.4092)	
training:	Epoch: [2][746/817]	Loss 0.1355 (0.4088)	
training:	Epoch: [2][747/817]	Loss 0.6088 (0.4091)	
training:	Epoch: [2][748/817]	Loss 0.6298 (0.4094)	
training:	Epoch: [2][749/817]	Loss 0.0988 (0.4090)	
training:	Epoch: [2][750/817]	Loss 0.6658 (0.4093)	
training:	Epoch: [2][751/817]	Loss 0.3793 (0.4093)	
training:	Epoch: [2][752/817]	Loss 0.4207 (0.4093)	
training:	Epoch: [2][753/817]	Loss 0.2709 (0.4091)	
training:	Epoch: [2][754/817]	Loss 0.5962 (0.4094)	
training:	Epoch: [2][755/817]	Loss 0.7211 (0.4098)	
training:	Epoch: [2][756/817]	Loss 0.2961 (0.4096)	
training:	Epoch: [2][757/817]	Loss 0.8146 (0.4102)	
training:	Epoch: [2][758/817]	Loss 0.5051 (0.4103)	
training:	Epoch: [2][759/817]	Loss 0.2989 (0.4101)	
training:	Epoch: [2][760/817]	Loss 0.1563 (0.4098)	
training:	Epoch: [2][761/817]	Loss 0.1157 (0.4094)	
training:	Epoch: [2][762/817]	Loss 0.6200 (0.4097)	
training:	Epoch: [2][763/817]	Loss 0.1834 (0.4094)	
training:	Epoch: [2][764/817]	Loss 0.5770 (0.4096)	
training:	Epoch: [2][765/817]	Loss 0.5432 (0.4098)	
training:	Epoch: [2][766/817]	Loss 0.4513 (0.4098)	
training:	Epoch: [2][767/817]	Loss 0.2593 (0.4096)	
training:	Epoch: [2][768/817]	Loss 0.4712 (0.4097)	
training:	Epoch: [2][769/817]	Loss 0.2223 (0.4095)	
training:	Epoch: [2][770/817]	Loss 0.3910 (0.4095)	
training:	Epoch: [2][771/817]	Loss 0.5949 (0.4097)	
training:	Epoch: [2][772/817]	Loss 0.4802 (0.4098)	
training:	Epoch: [2][773/817]	Loss 0.4257 (0.4098)	
training:	Epoch: [2][774/817]	Loss 0.3965 (0.4098)	
training:	Epoch: [2][775/817]	Loss 0.7389 (0.4102)	
training:	Epoch: [2][776/817]	Loss 0.2074 (0.4100)	
training:	Epoch: [2][777/817]	Loss 0.5837 (0.4102)	
training:	Epoch: [2][778/817]	Loss 0.2726 (0.4100)	
training:	Epoch: [2][779/817]	Loss 0.6075 (0.4103)	
training:	Epoch: [2][780/817]	Loss 0.8566 (0.4108)	
training:	Epoch: [2][781/817]	Loss 0.8602 (0.4114)	
training:	Epoch: [2][782/817]	Loss 0.7008 (0.4118)	
training:	Epoch: [2][783/817]	Loss 0.4871 (0.4119)	
training:	Epoch: [2][784/817]	Loss 0.2112 (0.4116)	
training:	Epoch: [2][785/817]	Loss 0.3694 (0.4116)	
training:	Epoch: [2][786/817]	Loss 0.5339 (0.4117)	
training:	Epoch: [2][787/817]	Loss 0.6707 (0.4120)	
training:	Epoch: [2][788/817]	Loss 0.4195 (0.4121)	
training:	Epoch: [2][789/817]	Loss 0.2242 (0.4118)	
training:	Epoch: [2][790/817]	Loss 0.4497 (0.4119)	
training:	Epoch: [2][791/817]	Loss 0.1241 (0.4115)	
training:	Epoch: [2][792/817]	Loss 0.5141 (0.4116)	
training:	Epoch: [2][793/817]	Loss 0.4149 (0.4116)	
training:	Epoch: [2][794/817]	Loss 0.6993 (0.4120)	
training:	Epoch: [2][795/817]	Loss 0.1874 (0.4117)	
training:	Epoch: [2][796/817]	Loss 0.5345 (0.4119)	
training:	Epoch: [2][797/817]	Loss 0.3975 (0.4119)	
training:	Epoch: [2][798/817]	Loss 0.3637 (0.4118)	
training:	Epoch: [2][799/817]	Loss 0.4559 (0.4118)	
training:	Epoch: [2][800/817]	Loss 0.3666 (0.4118)	
training:	Epoch: [2][801/817]	Loss 0.8195 (0.4123)	
training:	Epoch: [2][802/817]	Loss 0.7006 (0.4127)	
training:	Epoch: [2][803/817]	Loss 0.3037 (0.4125)	
training:	Epoch: [2][804/817]	Loss 0.2665 (0.4123)	
training:	Epoch: [2][805/817]	Loss 0.6350 (0.4126)	
training:	Epoch: [2][806/817]	Loss 0.3681 (0.4126)	
training:	Epoch: [2][807/817]	Loss 0.4924 (0.4127)	
training:	Epoch: [2][808/817]	Loss 0.5386 (0.4128)	
training:	Epoch: [2][809/817]	Loss 0.4760 (0.4129)	
training:	Epoch: [2][810/817]	Loss 0.0985 (0.4125)	
training:	Epoch: [2][811/817]	Loss 0.5368 (0.4127)	
training:	Epoch: [2][812/817]	Loss 0.3924 (0.4126)	
training:	Epoch: [2][813/817]	Loss 0.2441 (0.4124)	
training:	Epoch: [2][814/817]	Loss 0.1653 (0.4121)	
training:	Epoch: [2][815/817]	Loss 0.2641 (0.4119)	
training:	Epoch: [2][816/817]	Loss 0.3081 (0.4118)	
training:	Epoch: [2][817/817]	Loss 0.3333 (0.4117)	
Training:	 Loss: 0.4116

Training:	 ACC: 0.8519 0.8533 0.8874 0.8163
Validation:	 ACC: 0.8126 0.8143 0.8506 0.7747
Validation:	 Best_BACC: 0.8126 0.8143 0.8506 0.7747
Validation:	 Loss: 0.4173
Pretraining:	Epoch 3/200
----------
training:	Epoch: [3][1/817]	Loss 0.1373 (0.1373)	
training:	Epoch: [3][2/817]	Loss 0.1874 (0.1624)	
training:	Epoch: [3][3/817]	Loss 0.2960 (0.2069)	
training:	Epoch: [3][4/817]	Loss 0.1892 (0.2025)	
training:	Epoch: [3][5/817]	Loss 0.3113 (0.2242)	
training:	Epoch: [3][6/817]	Loss 0.3246 (0.2410)	
training:	Epoch: [3][7/817]	Loss 0.4279 (0.2677)	
training:	Epoch: [3][8/817]	Loss 0.2414 (0.2644)	
training:	Epoch: [3][9/817]	Loss 0.3646 (0.2755)	
training:	Epoch: [3][10/817]	Loss 0.4279 (0.2908)	
training:	Epoch: [3][11/817]	Loss 0.0930 (0.2728)	
training:	Epoch: [3][12/817]	Loss 1.0043 (0.3337)	
training:	Epoch: [3][13/817]	Loss 0.2269 (0.3255)	
training:	Epoch: [3][14/817]	Loss 0.1155 (0.3105)	
training:	Epoch: [3][15/817]	Loss 0.2848 (0.3088)	
training:	Epoch: [3][16/817]	Loss 0.5699 (0.3251)	
training:	Epoch: [3][17/817]	Loss 0.1377 (0.3141)	
training:	Epoch: [3][18/817]	Loss 0.2616 (0.3112)	
training:	Epoch: [3][19/817]	Loss 0.2873 (0.3099)	
training:	Epoch: [3][20/817]	Loss 0.2334 (0.3061)	
training:	Epoch: [3][21/817]	Loss 0.3243 (0.3070)	
training:	Epoch: [3][22/817]	Loss 0.4948 (0.3155)	
training:	Epoch: [3][23/817]	Loss 0.5204 (0.3244)	
training:	Epoch: [3][24/817]	Loss 0.2657 (0.3220)	
training:	Epoch: [3][25/817]	Loss 0.5270 (0.3302)	
training:	Epoch: [3][26/817]	Loss 0.3496 (0.3309)	
training:	Epoch: [3][27/817]	Loss 0.3458 (0.3315)	
training:	Epoch: [3][28/817]	Loss 0.3684 (0.3328)	
training:	Epoch: [3][29/817]	Loss 0.5691 (0.3409)	
training:	Epoch: [3][30/817]	Loss 0.5067 (0.3465)	
training:	Epoch: [3][31/817]	Loss 0.2574 (0.3436)	
training:	Epoch: [3][32/817]	Loss 0.2961 (0.3421)	
training:	Epoch: [3][33/817]	Loss 0.1040 (0.3349)	
training:	Epoch: [3][34/817]	Loss 0.1527 (0.3295)	
training:	Epoch: [3][35/817]	Loss 0.1528 (0.3245)	
training:	Epoch: [3][36/817]	Loss 0.2566 (0.3226)	
training:	Epoch: [3][37/817]	Loss 0.2197 (0.3198)	
training:	Epoch: [3][38/817]	Loss 0.2393 (0.3177)	
training:	Epoch: [3][39/817]	Loss 0.3393 (0.3182)	
training:	Epoch: [3][40/817]	Loss 0.1268 (0.3135)	
training:	Epoch: [3][41/817]	Loss 0.3259 (0.3138)	
training:	Epoch: [3][42/817]	Loss 0.4651 (0.3174)	
training:	Epoch: [3][43/817]	Loss 0.1920 (0.3145)	
training:	Epoch: [3][44/817]	Loss 0.3764 (0.3159)	
training:	Epoch: [3][45/817]	Loss 0.5935 (0.3220)	
training:	Epoch: [3][46/817]	Loss 0.5785 (0.3276)	
training:	Epoch: [3][47/817]	Loss 0.1777 (0.3244)	
training:	Epoch: [3][48/817]	Loss 0.1405 (0.3206)	
training:	Epoch: [3][49/817]	Loss 0.1871 (0.3179)	
training:	Epoch: [3][50/817]	Loss 0.2336 (0.3162)	
training:	Epoch: [3][51/817]	Loss 0.2697 (0.3153)	
training:	Epoch: [3][52/817]	Loss 0.2789 (0.3146)	
training:	Epoch: [3][53/817]	Loss 0.3338 (0.3149)	
training:	Epoch: [3][54/817]	Loss 0.1994 (0.3128)	
training:	Epoch: [3][55/817]	Loss 0.4678 (0.3156)	
training:	Epoch: [3][56/817]	Loss 0.3273 (0.3158)	
training:	Epoch: [3][57/817]	Loss 0.1786 (0.3134)	
training:	Epoch: [3][58/817]	Loss 0.1621 (0.3108)	
training:	Epoch: [3][59/817]	Loss 0.0877 (0.3070)	
training:	Epoch: [3][60/817]	Loss 0.1880 (0.3050)	
training:	Epoch: [3][61/817]	Loss 0.1836 (0.3030)	
training:	Epoch: [3][62/817]	Loss 0.6310 (0.3083)	
training:	Epoch: [3][63/817]	Loss 0.1567 (0.3059)	
training:	Epoch: [3][64/817]	Loss 0.5272 (0.3094)	
training:	Epoch: [3][65/817]	Loss 0.2812 (0.3089)	
training:	Epoch: [3][66/817]	Loss 0.5169 (0.3121)	
training:	Epoch: [3][67/817]	Loss 0.1057 (0.3090)	
training:	Epoch: [3][68/817]	Loss 0.2100 (0.3076)	
training:	Epoch: [3][69/817]	Loss 0.5804 (0.3115)	
training:	Epoch: [3][70/817]	Loss 0.6134 (0.3158)	
training:	Epoch: [3][71/817]	Loss 0.4652 (0.3179)	
training:	Epoch: [3][72/817]	Loss 0.3640 (0.3186)	
training:	Epoch: [3][73/817]	Loss 0.4967 (0.3210)	
training:	Epoch: [3][74/817]	Loss 0.0960 (0.3180)	
training:	Epoch: [3][75/817]	Loss 0.6955 (0.3230)	
training:	Epoch: [3][76/817]	Loss 0.2755 (0.3224)	
training:	Epoch: [3][77/817]	Loss 0.3379 (0.3226)	
training:	Epoch: [3][78/817]	Loss 0.3085 (0.3224)	
training:	Epoch: [3][79/817]	Loss 0.1673 (0.3204)	
training:	Epoch: [3][80/817]	Loss 0.2930 (0.3201)	
training:	Epoch: [3][81/817]	Loss 0.2466 (0.3192)	
training:	Epoch: [3][82/817]	Loss 0.2782 (0.3187)	
training:	Epoch: [3][83/817]	Loss 0.3500 (0.3191)	
training:	Epoch: [3][84/817]	Loss 0.1720 (0.3173)	
training:	Epoch: [3][85/817]	Loss 0.2482 (0.3165)	
training:	Epoch: [3][86/817]	Loss 0.1617 (0.3147)	
training:	Epoch: [3][87/817]	Loss 0.3998 (0.3157)	
training:	Epoch: [3][88/817]	Loss 0.5714 (0.3186)	
training:	Epoch: [3][89/817]	Loss 0.3834 (0.3193)	
training:	Epoch: [3][90/817]	Loss 0.1680 (0.3176)	
training:	Epoch: [3][91/817]	Loss 0.1465 (0.3158)	
training:	Epoch: [3][92/817]	Loss 0.1054 (0.3135)	
training:	Epoch: [3][93/817]	Loss 0.0916 (0.3111)	
training:	Epoch: [3][94/817]	Loss 0.4834 (0.3129)	
training:	Epoch: [3][95/817]	Loss 0.6741 (0.3167)	
training:	Epoch: [3][96/817]	Loss 0.5315 (0.3190)	
training:	Epoch: [3][97/817]	Loss 0.4087 (0.3199)	
training:	Epoch: [3][98/817]	Loss 0.4165 (0.3209)	
training:	Epoch: [3][99/817]	Loss 0.2560 (0.3202)	
training:	Epoch: [3][100/817]	Loss 0.0978 (0.3180)	
training:	Epoch: [3][101/817]	Loss 0.2051 (0.3169)	
training:	Epoch: [3][102/817]	Loss 0.7893 (0.3215)	
training:	Epoch: [3][103/817]	Loss 0.4057 (0.3223)	
training:	Epoch: [3][104/817]	Loss 0.5892 (0.3249)	
training:	Epoch: [3][105/817]	Loss 0.7103 (0.3286)	
training:	Epoch: [3][106/817]	Loss 0.3403 (0.3287)	
training:	Epoch: [3][107/817]	Loss 0.4405 (0.3297)	
training:	Epoch: [3][108/817]	Loss 0.3778 (0.3302)	
training:	Epoch: [3][109/817]	Loss 0.4025 (0.3308)	
training:	Epoch: [3][110/817]	Loss 0.1782 (0.3294)	
training:	Epoch: [3][111/817]	Loss 0.7395 (0.3331)	
training:	Epoch: [3][112/817]	Loss 0.5905 (0.3354)	
training:	Epoch: [3][113/817]	Loss 0.2562 (0.3347)	
training:	Epoch: [3][114/817]	Loss 0.6308 (0.3373)	
training:	Epoch: [3][115/817]	Loss 0.6099 (0.3397)	
training:	Epoch: [3][116/817]	Loss 0.2025 (0.3385)	
training:	Epoch: [3][117/817]	Loss 0.1474 (0.3369)	
training:	Epoch: [3][118/817]	Loss 0.6495 (0.3395)	
training:	Epoch: [3][119/817]	Loss 0.0747 (0.3373)	
training:	Epoch: [3][120/817]	Loss 0.4133 (0.3379)	
training:	Epoch: [3][121/817]	Loss 0.2170 (0.3369)	
training:	Epoch: [3][122/817]	Loss 0.7216 (0.3401)	
training:	Epoch: [3][123/817]	Loss 0.1232 (0.3383)	
training:	Epoch: [3][124/817]	Loss 0.4787 (0.3395)	
training:	Epoch: [3][125/817]	Loss 0.7717 (0.3429)	
training:	Epoch: [3][126/817]	Loss 0.2314 (0.3420)	
training:	Epoch: [3][127/817]	Loss 0.2061 (0.3410)	
training:	Epoch: [3][128/817]	Loss 0.4001 (0.3414)	
training:	Epoch: [3][129/817]	Loss 0.2548 (0.3407)	
training:	Epoch: [3][130/817]	Loss 0.4265 (0.3414)	
training:	Epoch: [3][131/817]	Loss 0.2909 (0.3410)	
training:	Epoch: [3][132/817]	Loss 0.3233 (0.3409)	
training:	Epoch: [3][133/817]	Loss 1.1137 (0.3467)	
training:	Epoch: [3][134/817]	Loss 0.1784 (0.3454)	
training:	Epoch: [3][135/817]	Loss 0.3806 (0.3457)	
training:	Epoch: [3][136/817]	Loss 0.1561 (0.3443)	
training:	Epoch: [3][137/817]	Loss 0.5600 (0.3459)	
training:	Epoch: [3][138/817]	Loss 0.1960 (0.3448)	
training:	Epoch: [3][139/817]	Loss 0.1320 (0.3433)	
training:	Epoch: [3][140/817]	Loss 0.3195 (0.3431)	
training:	Epoch: [3][141/817]	Loss 0.1591 (0.3418)	
training:	Epoch: [3][142/817]	Loss 0.5640 (0.3434)	
training:	Epoch: [3][143/817]	Loss 0.7320 (0.3461)	
training:	Epoch: [3][144/817]	Loss 0.1753 (0.3449)	
training:	Epoch: [3][145/817]	Loss 0.4608 (0.3457)	
training:	Epoch: [3][146/817]	Loss 0.2199 (0.3448)	
training:	Epoch: [3][147/817]	Loss 0.4035 (0.3452)	
training:	Epoch: [3][148/817]	Loss 0.4698 (0.3461)	
training:	Epoch: [3][149/817]	Loss 0.1547 (0.3448)	
training:	Epoch: [3][150/817]	Loss 0.2596 (0.3442)	
training:	Epoch: [3][151/817]	Loss 0.3854 (0.3445)	
training:	Epoch: [3][152/817]	Loss 0.2179 (0.3437)	
training:	Epoch: [3][153/817]	Loss 0.1373 (0.3423)	
training:	Epoch: [3][154/817]	Loss 0.3833 (0.3426)	
training:	Epoch: [3][155/817]	Loss 0.2153 (0.3417)	
training:	Epoch: [3][156/817]	Loss 0.5177 (0.3429)	
training:	Epoch: [3][157/817]	Loss 0.1805 (0.3418)	
training:	Epoch: [3][158/817]	Loss 0.8791 (0.3452)	
training:	Epoch: [3][159/817]	Loss 0.1407 (0.3440)	
training:	Epoch: [3][160/817]	Loss 0.8477 (0.3471)	
training:	Epoch: [3][161/817]	Loss 0.9036 (0.3506)	
training:	Epoch: [3][162/817]	Loss 0.1715 (0.3495)	
training:	Epoch: [3][163/817]	Loss 0.5383 (0.3506)	
training:	Epoch: [3][164/817]	Loss 0.0939 (0.3490)	
training:	Epoch: [3][165/817]	Loss 0.1059 (0.3476)	
training:	Epoch: [3][166/817]	Loss 0.5176 (0.3486)	
training:	Epoch: [3][167/817]	Loss 0.3886 (0.3488)	
training:	Epoch: [3][168/817]	Loss 0.3578 (0.3489)	
training:	Epoch: [3][169/817]	Loss 0.2742 (0.3485)	
training:	Epoch: [3][170/817]	Loss 0.7906 (0.3511)	
training:	Epoch: [3][171/817]	Loss 0.1539 (0.3499)	
training:	Epoch: [3][172/817]	Loss 0.4804 (0.3507)	
training:	Epoch: [3][173/817]	Loss 0.5000 (0.3515)	
training:	Epoch: [3][174/817]	Loss 0.1599 (0.3504)	
training:	Epoch: [3][175/817]	Loss 0.2992 (0.3501)	
training:	Epoch: [3][176/817]	Loss 0.2260 (0.3494)	
training:	Epoch: [3][177/817]	Loss 0.5697 (0.3507)	
training:	Epoch: [3][178/817]	Loss 0.3137 (0.3505)	
training:	Epoch: [3][179/817]	Loss 0.5084 (0.3513)	
training:	Epoch: [3][180/817]	Loss 0.1129 (0.3500)	
training:	Epoch: [3][181/817]	Loss 0.3600 (0.3501)	
training:	Epoch: [3][182/817]	Loss 0.2884 (0.3497)	
training:	Epoch: [3][183/817]	Loss 0.5293 (0.3507)	
training:	Epoch: [3][184/817]	Loss 0.4592 (0.3513)	
training:	Epoch: [3][185/817]	Loss 0.4508 (0.3518)	
training:	Epoch: [3][186/817]	Loss 0.3695 (0.3519)	
training:	Epoch: [3][187/817]	Loss 0.2621 (0.3515)	
training:	Epoch: [3][188/817]	Loss 0.3849 (0.3516)	
training:	Epoch: [3][189/817]	Loss 0.3087 (0.3514)	
training:	Epoch: [3][190/817]	Loss 0.2365 (0.3508)	
training:	Epoch: [3][191/817]	Loss 0.4338 (0.3512)	
training:	Epoch: [3][192/817]	Loss 0.1504 (0.3502)	
training:	Epoch: [3][193/817]	Loss 0.1091 (0.3489)	
training:	Epoch: [3][194/817]	Loss 1.0102 (0.3523)	
training:	Epoch: [3][195/817]	Loss 0.3893 (0.3525)	
training:	Epoch: [3][196/817]	Loss 0.3439 (0.3525)	
training:	Epoch: [3][197/817]	Loss 0.4437 (0.3530)	
training:	Epoch: [3][198/817]	Loss 0.2829 (0.3526)	
training:	Epoch: [3][199/817]	Loss 0.2866 (0.3523)	
training:	Epoch: [3][200/817]	Loss 0.4985 (0.3530)	
training:	Epoch: [3][201/817]	Loss 0.3962 (0.3532)	
training:	Epoch: [3][202/817]	Loss 0.3198 (0.3531)	
training:	Epoch: [3][203/817]	Loss 0.5010 (0.3538)	
training:	Epoch: [3][204/817]	Loss 0.4169 (0.3541)	
training:	Epoch: [3][205/817]	Loss 0.3576 (0.3541)	
training:	Epoch: [3][206/817]	Loss 0.4597 (0.3546)	
training:	Epoch: [3][207/817]	Loss 0.2885 (0.3543)	
training:	Epoch: [3][208/817]	Loss 0.5013 (0.3550)	
training:	Epoch: [3][209/817]	Loss 0.1246 (0.3539)	
training:	Epoch: [3][210/817]	Loss 0.3830 (0.3540)	
training:	Epoch: [3][211/817]	Loss 0.1343 (0.3530)	
training:	Epoch: [3][212/817]	Loss 0.2005 (0.3523)	
training:	Epoch: [3][213/817]	Loss 0.2286 (0.3517)	
training:	Epoch: [3][214/817]	Loss 0.4672 (0.3522)	
training:	Epoch: [3][215/817]	Loss 0.4556 (0.3527)	
training:	Epoch: [3][216/817]	Loss 0.1863 (0.3520)	
training:	Epoch: [3][217/817]	Loss 0.2921 (0.3517)	
training:	Epoch: [3][218/817]	Loss 0.6119 (0.3529)	
training:	Epoch: [3][219/817]	Loss 0.3743 (0.3530)	
training:	Epoch: [3][220/817]	Loss 0.3132 (0.3528)	
training:	Epoch: [3][221/817]	Loss 0.1168 (0.3517)	
training:	Epoch: [3][222/817]	Loss 0.2174 (0.3511)	
training:	Epoch: [3][223/817]	Loss 0.1488 (0.3502)	
training:	Epoch: [3][224/817]	Loss 0.6755 (0.3517)	
training:	Epoch: [3][225/817]	Loss 0.3754 (0.3518)	
training:	Epoch: [3][226/817]	Loss 0.4081 (0.3520)	
training:	Epoch: [3][227/817]	Loss 0.4596 (0.3525)	
training:	Epoch: [3][228/817]	Loss 0.3631 (0.3525)	
training:	Epoch: [3][229/817]	Loss 0.1733 (0.3518)	
training:	Epoch: [3][230/817]	Loss 0.2730 (0.3514)	
training:	Epoch: [3][231/817]	Loss 0.2130 (0.3508)	
training:	Epoch: [3][232/817]	Loss 0.5385 (0.3516)	
training:	Epoch: [3][233/817]	Loss 0.1227 (0.3506)	
training:	Epoch: [3][234/817]	Loss 0.5470 (0.3515)	
training:	Epoch: [3][235/817]	Loss 0.2157 (0.3509)	
training:	Epoch: [3][236/817]	Loss 0.0760 (0.3497)	
training:	Epoch: [3][237/817]	Loss 0.2262 (0.3492)	
training:	Epoch: [3][238/817]	Loss 0.0629 (0.3480)	
training:	Epoch: [3][239/817]	Loss 0.3752 (0.3481)	
training:	Epoch: [3][240/817]	Loss 0.4016 (0.3483)	
training:	Epoch: [3][241/817]	Loss 0.3302 (0.3483)	
training:	Epoch: [3][242/817]	Loss 0.4355 (0.3486)	
training:	Epoch: [3][243/817]	Loss 0.3381 (0.3486)	
training:	Epoch: [3][244/817]	Loss 0.5515 (0.3494)	
training:	Epoch: [3][245/817]	Loss 0.1180 (0.3485)	
training:	Epoch: [3][246/817]	Loss 0.9703 (0.3510)	
training:	Epoch: [3][247/817]	Loss 0.1782 (0.3503)	
training:	Epoch: [3][248/817]	Loss 0.1785 (0.3496)	
training:	Epoch: [3][249/817]	Loss 0.4401 (0.3500)	
training:	Epoch: [3][250/817]	Loss 0.5397 (0.3507)	
training:	Epoch: [3][251/817]	Loss 0.7027 (0.3521)	
training:	Epoch: [3][252/817]	Loss 0.1204 (0.3512)	
training:	Epoch: [3][253/817]	Loss 0.3743 (0.3513)	
training:	Epoch: [3][254/817]	Loss 0.1798 (0.3506)	
training:	Epoch: [3][255/817]	Loss 0.7506 (0.3522)	
training:	Epoch: [3][256/817]	Loss 0.2602 (0.3518)	
training:	Epoch: [3][257/817]	Loss 0.1596 (0.3511)	
training:	Epoch: [3][258/817]	Loss 0.3813 (0.3512)	
training:	Epoch: [3][259/817]	Loss 0.1213 (0.3503)	
training:	Epoch: [3][260/817]	Loss 0.3419 (0.3503)	
training:	Epoch: [3][261/817]	Loss 0.2928 (0.3501)	
training:	Epoch: [3][262/817]	Loss 0.2588 (0.3497)	
training:	Epoch: [3][263/817]	Loss 0.1996 (0.3492)	
training:	Epoch: [3][264/817]	Loss 0.5284 (0.3498)	
training:	Epoch: [3][265/817]	Loss 0.2723 (0.3495)	
training:	Epoch: [3][266/817]	Loss 0.2679 (0.3492)	
training:	Epoch: [3][267/817]	Loss 0.5514 (0.3500)	
training:	Epoch: [3][268/817]	Loss 0.0971 (0.3490)	
training:	Epoch: [3][269/817]	Loss 0.2594 (0.3487)	
training:	Epoch: [3][270/817]	Loss 0.3378 (0.3487)	
training:	Epoch: [3][271/817]	Loss 0.2105 (0.3482)	
training:	Epoch: [3][272/817]	Loss 0.4216 (0.3484)	
training:	Epoch: [3][273/817]	Loss 0.4490 (0.3488)	
training:	Epoch: [3][274/817]	Loss 0.4596 (0.3492)	
training:	Epoch: [3][275/817]	Loss 0.1689 (0.3485)	
training:	Epoch: [3][276/817]	Loss 0.1983 (0.3480)	
training:	Epoch: [3][277/817]	Loss 0.3895 (0.3482)	
training:	Epoch: [3][278/817]	Loss 0.4397 (0.3485)	
training:	Epoch: [3][279/817]	Loss 0.3181 (0.3484)	
training:	Epoch: [3][280/817]	Loss 0.2870 (0.3482)	
training:	Epoch: [3][281/817]	Loss 0.5740 (0.3490)	
training:	Epoch: [3][282/817]	Loss 0.5571 (0.3497)	
training:	Epoch: [3][283/817]	Loss 0.3976 (0.3499)	
training:	Epoch: [3][284/817]	Loss 0.1840 (0.3493)	
training:	Epoch: [3][285/817]	Loss 0.4332 (0.3496)	
training:	Epoch: [3][286/817]	Loss 1.1027 (0.3522)	
training:	Epoch: [3][287/817]	Loss 0.3975 (0.3524)	
training:	Epoch: [3][288/817]	Loss 0.7382 (0.3537)	
training:	Epoch: [3][289/817]	Loss 0.1355 (0.3530)	
training:	Epoch: [3][290/817]	Loss 0.3805 (0.3530)	
training:	Epoch: [3][291/817]	Loss 0.1302 (0.3523)	
training:	Epoch: [3][292/817]	Loss 0.4178 (0.3525)	
training:	Epoch: [3][293/817]	Loss 0.4777 (0.3529)	
training:	Epoch: [3][294/817]	Loss 0.6124 (0.3538)	
training:	Epoch: [3][295/817]	Loss 0.6266 (0.3547)	
training:	Epoch: [3][296/817]	Loss 0.1695 (0.3541)	
training:	Epoch: [3][297/817]	Loss 0.5839 (0.3549)	
training:	Epoch: [3][298/817]	Loss 0.1362 (0.3542)	
training:	Epoch: [3][299/817]	Loss 0.4419 (0.3544)	
training:	Epoch: [3][300/817]	Loss 0.2624 (0.3541)	
training:	Epoch: [3][301/817]	Loss 0.2455 (0.3538)	
training:	Epoch: [3][302/817]	Loss 0.2222 (0.3533)	
training:	Epoch: [3][303/817]	Loss 0.2721 (0.3531)	
training:	Epoch: [3][304/817]	Loss 0.7599 (0.3544)	
training:	Epoch: [3][305/817]	Loss 0.7064 (0.3556)	
training:	Epoch: [3][306/817]	Loss 0.2509 (0.3552)	
training:	Epoch: [3][307/817]	Loss 0.1757 (0.3546)	
training:	Epoch: [3][308/817]	Loss 0.5151 (0.3552)	
training:	Epoch: [3][309/817]	Loss 0.3748 (0.3552)	
training:	Epoch: [3][310/817]	Loss 0.8985 (0.3570)	
training:	Epoch: [3][311/817]	Loss 0.4126 (0.3572)	
training:	Epoch: [3][312/817]	Loss 0.4818 (0.3576)	
training:	Epoch: [3][313/817]	Loss 0.3128 (0.3574)	
training:	Epoch: [3][314/817]	Loss 0.4257 (0.3576)	
training:	Epoch: [3][315/817]	Loss 0.5514 (0.3582)	
training:	Epoch: [3][316/817]	Loss 0.2763 (0.3580)	
training:	Epoch: [3][317/817]	Loss 0.2005 (0.3575)	
training:	Epoch: [3][318/817]	Loss 0.3461 (0.3575)	
training:	Epoch: [3][319/817]	Loss 0.4935 (0.3579)	
training:	Epoch: [3][320/817]	Loss 0.4154 (0.3581)	
training:	Epoch: [3][321/817]	Loss 0.2588 (0.3578)	
training:	Epoch: [3][322/817]	Loss 0.7087 (0.3588)	
training:	Epoch: [3][323/817]	Loss 0.3029 (0.3587)	
training:	Epoch: [3][324/817]	Loss 0.3841 (0.3587)	
training:	Epoch: [3][325/817]	Loss 0.3381 (0.3587)	
training:	Epoch: [3][326/817]	Loss 0.6445 (0.3596)	
training:	Epoch: [3][327/817]	Loss 0.2506 (0.3592)	
training:	Epoch: [3][328/817]	Loss 0.1715 (0.3587)	
training:	Epoch: [3][329/817]	Loss 0.2078 (0.3582)	
training:	Epoch: [3][330/817]	Loss 0.1984 (0.3577)	
training:	Epoch: [3][331/817]	Loss 0.4330 (0.3579)	
training:	Epoch: [3][332/817]	Loss 0.3159 (0.3578)	
training:	Epoch: [3][333/817]	Loss 0.3106 (0.3577)	
training:	Epoch: [3][334/817]	Loss 0.4956 (0.3581)	
training:	Epoch: [3][335/817]	Loss 0.3687 (0.3581)	
training:	Epoch: [3][336/817]	Loss 0.2191 (0.3577)	
training:	Epoch: [3][337/817]	Loss 0.6774 (0.3587)	
training:	Epoch: [3][338/817]	Loss 0.1464 (0.3580)	
training:	Epoch: [3][339/817]	Loss 0.4161 (0.3582)	
training:	Epoch: [3][340/817]	Loss 0.1764 (0.3577)	
training:	Epoch: [3][341/817]	Loss 0.6343 (0.3585)	
training:	Epoch: [3][342/817]	Loss 0.2534 (0.3582)	
training:	Epoch: [3][343/817]	Loss 0.4826 (0.3585)	
training:	Epoch: [3][344/817]	Loss 0.7363 (0.3596)	
training:	Epoch: [3][345/817]	Loss 0.3956 (0.3597)	
training:	Epoch: [3][346/817]	Loss 0.1050 (0.3590)	
training:	Epoch: [3][347/817]	Loss 0.2903 (0.3588)	
training:	Epoch: [3][348/817]	Loss 0.2984 (0.3586)	
training:	Epoch: [3][349/817]	Loss 0.4067 (0.3588)	
training:	Epoch: [3][350/817]	Loss 0.3340 (0.3587)	
training:	Epoch: [3][351/817]	Loss 0.7815 (0.3599)	
training:	Epoch: [3][352/817]	Loss 0.4262 (0.3601)	
training:	Epoch: [3][353/817]	Loss 0.2630 (0.3598)	
training:	Epoch: [3][354/817]	Loss 0.8768 (0.3613)	
training:	Epoch: [3][355/817]	Loss 0.7662 (0.3624)	
training:	Epoch: [3][356/817]	Loss 0.2096 (0.3620)	
training:	Epoch: [3][357/817]	Loss 0.2503 (0.3617)	
training:	Epoch: [3][358/817]	Loss 0.2016 (0.3612)	
training:	Epoch: [3][359/817]	Loss 0.4535 (0.3615)	
training:	Epoch: [3][360/817]	Loss 0.4220 (0.3616)	
training:	Epoch: [3][361/817]	Loss 0.6140 (0.3623)	
training:	Epoch: [3][362/817]	Loss 0.7608 (0.3634)	
training:	Epoch: [3][363/817]	Loss 0.2883 (0.3632)	
training:	Epoch: [3][364/817]	Loss 0.7187 (0.3642)	
training:	Epoch: [3][365/817]	Loss 0.0949 (0.3635)	
training:	Epoch: [3][366/817]	Loss 0.2543 (0.3632)	
training:	Epoch: [3][367/817]	Loss 0.3351 (0.3631)	
training:	Epoch: [3][368/817]	Loss 0.1128 (0.3624)	
training:	Epoch: [3][369/817]	Loss 0.1361 (0.3618)	
training:	Epoch: [3][370/817]	Loss 0.1684 (0.3613)	
training:	Epoch: [3][371/817]	Loss 0.3804 (0.3613)	
training:	Epoch: [3][372/817]	Loss 0.2179 (0.3609)	
training:	Epoch: [3][373/817]	Loss 0.4765 (0.3613)	
training:	Epoch: [3][374/817]	Loss 0.0815 (0.3605)	
training:	Epoch: [3][375/817]	Loss 0.1824 (0.3600)	
training:	Epoch: [3][376/817]	Loss 0.3999 (0.3601)	
training:	Epoch: [3][377/817]	Loss 0.5853 (0.3607)	
training:	Epoch: [3][378/817]	Loss 0.3924 (0.3608)	
training:	Epoch: [3][379/817]	Loss 0.4224 (0.3610)	
training:	Epoch: [3][380/817]	Loss 0.5292 (0.3614)	
training:	Epoch: [3][381/817]	Loss 0.2500 (0.3611)	
training:	Epoch: [3][382/817]	Loss 0.3731 (0.3612)	
training:	Epoch: [3][383/817]	Loss 0.7794 (0.3623)	
training:	Epoch: [3][384/817]	Loss 0.6164 (0.3629)	
training:	Epoch: [3][385/817]	Loss 0.2865 (0.3627)	
training:	Epoch: [3][386/817]	Loss 0.1729 (0.3622)	
training:	Epoch: [3][387/817]	Loss 0.3987 (0.3623)	
training:	Epoch: [3][388/817]	Loss 0.4672 (0.3626)	
training:	Epoch: [3][389/817]	Loss 0.3626 (0.3626)	
training:	Epoch: [3][390/817]	Loss 0.8536 (0.3639)	
training:	Epoch: [3][391/817]	Loss 0.3861 (0.3639)	
training:	Epoch: [3][392/817]	Loss 0.6179 (0.3646)	
training:	Epoch: [3][393/817]	Loss 0.3261 (0.3645)	
training:	Epoch: [3][394/817]	Loss 0.4702 (0.3647)	
training:	Epoch: [3][395/817]	Loss 0.3399 (0.3647)	
training:	Epoch: [3][396/817]	Loss 0.3495 (0.3646)	
training:	Epoch: [3][397/817]	Loss 0.1216 (0.3640)	
training:	Epoch: [3][398/817]	Loss 0.4285 (0.3642)	
training:	Epoch: [3][399/817]	Loss 0.5085 (0.3645)	
training:	Epoch: [3][400/817]	Loss 0.1754 (0.3641)	
training:	Epoch: [3][401/817]	Loss 0.1249 (0.3635)	
training:	Epoch: [3][402/817]	Loss 0.4825 (0.3638)	
training:	Epoch: [3][403/817]	Loss 0.4042 (0.3639)	
training:	Epoch: [3][404/817]	Loss 0.4846 (0.3642)	
training:	Epoch: [3][405/817]	Loss 0.2707 (0.3639)	
training:	Epoch: [3][406/817]	Loss 0.3669 (0.3639)	
training:	Epoch: [3][407/817]	Loss 0.1189 (0.3633)	
training:	Epoch: [3][408/817]	Loss 0.2547 (0.3631)	
training:	Epoch: [3][409/817]	Loss 0.4768 (0.3634)	
training:	Epoch: [3][410/817]	Loss 0.6772 (0.3641)	
training:	Epoch: [3][411/817]	Loss 0.1308 (0.3635)	
training:	Epoch: [3][412/817]	Loss 0.4600 (0.3638)	
training:	Epoch: [3][413/817]	Loss 0.4376 (0.3640)	
training:	Epoch: [3][414/817]	Loss 0.1210 (0.3634)	
training:	Epoch: [3][415/817]	Loss 0.3055 (0.3632)	
training:	Epoch: [3][416/817]	Loss 0.1482 (0.3627)	
training:	Epoch: [3][417/817]	Loss 0.3450 (0.3627)	
training:	Epoch: [3][418/817]	Loss 0.6711 (0.3634)	
training:	Epoch: [3][419/817]	Loss 0.3107 (0.3633)	
training:	Epoch: [3][420/817]	Loss 0.2122 (0.3629)	
training:	Epoch: [3][421/817]	Loss 0.9650 (0.3644)	
training:	Epoch: [3][422/817]	Loss 0.4824 (0.3646)	
training:	Epoch: [3][423/817]	Loss 0.2271 (0.3643)	
training:	Epoch: [3][424/817]	Loss 0.2041 (0.3639)	
training:	Epoch: [3][425/817]	Loss 0.6621 (0.3646)	
training:	Epoch: [3][426/817]	Loss 0.2971 (0.3645)	
training:	Epoch: [3][427/817]	Loss 0.1125 (0.3639)	
training:	Epoch: [3][428/817]	Loss 1.0158 (0.3654)	
training:	Epoch: [3][429/817]	Loss 0.2497 (0.3651)	
training:	Epoch: [3][430/817]	Loss 0.3700 (0.3652)	
training:	Epoch: [3][431/817]	Loss 0.1636 (0.3647)	
training:	Epoch: [3][432/817]	Loss 0.6626 (0.3654)	
training:	Epoch: [3][433/817]	Loss 0.3331 (0.3653)	
training:	Epoch: [3][434/817]	Loss 0.4486 (0.3655)	
training:	Epoch: [3][435/817]	Loss 0.3747 (0.3655)	
training:	Epoch: [3][436/817]	Loss 0.4423 (0.3657)	
training:	Epoch: [3][437/817]	Loss 0.4286 (0.3658)	
training:	Epoch: [3][438/817]	Loss 0.2871 (0.3657)	
training:	Epoch: [3][439/817]	Loss 0.2057 (0.3653)	
training:	Epoch: [3][440/817]	Loss 0.2617 (0.3651)	
training:	Epoch: [3][441/817]	Loss 0.6062 (0.3656)	
training:	Epoch: [3][442/817]	Loss 0.4510 (0.3658)	
training:	Epoch: [3][443/817]	Loss 0.2454 (0.3655)	
training:	Epoch: [3][444/817]	Loss 0.4004 (0.3656)	
training:	Epoch: [3][445/817]	Loss 0.4188 (0.3657)	
training:	Epoch: [3][446/817]	Loss 0.4758 (0.3660)	
training:	Epoch: [3][447/817]	Loss 0.3516 (0.3659)	
training:	Epoch: [3][448/817]	Loss 0.5728 (0.3664)	
training:	Epoch: [3][449/817]	Loss 0.3414 (0.3663)	
training:	Epoch: [3][450/817]	Loss 0.4290 (0.3665)	
training:	Epoch: [3][451/817]	Loss 0.3330 (0.3664)	
training:	Epoch: [3][452/817]	Loss 0.6949 (0.3671)	
training:	Epoch: [3][453/817]	Loss 0.2132 (0.3668)	
training:	Epoch: [3][454/817]	Loss 0.2175 (0.3665)	
training:	Epoch: [3][455/817]	Loss 0.2346 (0.3662)	
training:	Epoch: [3][456/817]	Loss 0.3151 (0.3661)	
training:	Epoch: [3][457/817]	Loss 0.4677 (0.3663)	
training:	Epoch: [3][458/817]	Loss 0.2788 (0.3661)	
training:	Epoch: [3][459/817]	Loss 0.2309 (0.3658)	
training:	Epoch: [3][460/817]	Loss 0.2133 (0.3655)	
training:	Epoch: [3][461/817]	Loss 0.2333 (0.3652)	
training:	Epoch: [3][462/817]	Loss 0.3745 (0.3652)	
training:	Epoch: [3][463/817]	Loss 0.3637 (0.3652)	
training:	Epoch: [3][464/817]	Loss 0.1638 (0.3648)	
training:	Epoch: [3][465/817]	Loss 0.2134 (0.3644)	
training:	Epoch: [3][466/817]	Loss 0.1173 (0.3639)	
training:	Epoch: [3][467/817]	Loss 0.2101 (0.3636)	
training:	Epoch: [3][468/817]	Loss 0.1139 (0.3630)	
training:	Epoch: [3][469/817]	Loss 0.4197 (0.3632)	
training:	Epoch: [3][470/817]	Loss 0.4109 (0.3633)	
training:	Epoch: [3][471/817]	Loss 0.4811 (0.3635)	
training:	Epoch: [3][472/817]	Loss 0.6588 (0.3641)	
training:	Epoch: [3][473/817]	Loss 0.1967 (0.3638)	
training:	Epoch: [3][474/817]	Loss 0.1409 (0.3633)	
training:	Epoch: [3][475/817]	Loss 0.7642 (0.3642)	
training:	Epoch: [3][476/817]	Loss 0.1434 (0.3637)	
training:	Epoch: [3][477/817]	Loss 0.0980 (0.3631)	
training:	Epoch: [3][478/817]	Loss 0.2903 (0.3630)	
training:	Epoch: [3][479/817]	Loss 0.4672 (0.3632)	
training:	Epoch: [3][480/817]	Loss 0.1182 (0.3627)	
training:	Epoch: [3][481/817]	Loss 0.2231 (0.3624)	
training:	Epoch: [3][482/817]	Loss 0.6418 (0.3630)	
training:	Epoch: [3][483/817]	Loss 0.1482 (0.3625)	
training:	Epoch: [3][484/817]	Loss 0.6341 (0.3631)	
training:	Epoch: [3][485/817]	Loss 0.1379 (0.3626)	
training:	Epoch: [3][486/817]	Loss 0.3827 (0.3627)	
training:	Epoch: [3][487/817]	Loss 0.7398 (0.3635)	
training:	Epoch: [3][488/817]	Loss 0.1134 (0.3629)	
training:	Epoch: [3][489/817]	Loss 0.2962 (0.3628)	
training:	Epoch: [3][490/817]	Loss 0.2270 (0.3625)	
training:	Epoch: [3][491/817]	Loss 0.4135 (0.3626)	
training:	Epoch: [3][492/817]	Loss 0.2911 (0.3625)	
training:	Epoch: [3][493/817]	Loss 0.6640 (0.3631)	
training:	Epoch: [3][494/817]	Loss 0.3244 (0.3630)	
training:	Epoch: [3][495/817]	Loss 0.3482 (0.3630)	
training:	Epoch: [3][496/817]	Loss 0.3692 (0.3630)	
training:	Epoch: [3][497/817]	Loss 0.1122 (0.3625)	
training:	Epoch: [3][498/817]	Loss 0.3622 (0.3625)	
training:	Epoch: [3][499/817]	Loss 0.0878 (0.3619)	
training:	Epoch: [3][500/817]	Loss 0.5863 (0.3624)	
training:	Epoch: [3][501/817]	Loss 0.1723 (0.3620)	
training:	Epoch: [3][502/817]	Loss 0.3775 (0.3620)	
training:	Epoch: [3][503/817]	Loss 0.3856 (0.3621)	
training:	Epoch: [3][504/817]	Loss 0.4826 (0.3623)	
training:	Epoch: [3][505/817]	Loss 0.4271 (0.3625)	
training:	Epoch: [3][506/817]	Loss 0.6138 (0.3630)	
training:	Epoch: [3][507/817]	Loss 0.5689 (0.3634)	
training:	Epoch: [3][508/817]	Loss 0.1639 (0.3630)	
training:	Epoch: [3][509/817]	Loss 0.9260 (0.3641)	
training:	Epoch: [3][510/817]	Loss 0.4398 (0.3642)	
training:	Epoch: [3][511/817]	Loss 0.2297 (0.3640)	
training:	Epoch: [3][512/817]	Loss 0.2370 (0.3637)	
training:	Epoch: [3][513/817]	Loss 0.6787 (0.3643)	
training:	Epoch: [3][514/817]	Loss 0.1470 (0.3639)	
training:	Epoch: [3][515/817]	Loss 0.1399 (0.3635)	
training:	Epoch: [3][516/817]	Loss 0.3341 (0.3634)	
training:	Epoch: [3][517/817]	Loss 0.1795 (0.3631)	
training:	Epoch: [3][518/817]	Loss 0.6001 (0.3635)	
training:	Epoch: [3][519/817]	Loss 0.1493 (0.3631)	
training:	Epoch: [3][520/817]	Loss 0.2294 (0.3628)	
training:	Epoch: [3][521/817]	Loss 0.3617 (0.3628)	
training:	Epoch: [3][522/817]	Loss 0.2167 (0.3626)	
training:	Epoch: [3][523/817]	Loss 0.4303 (0.3627)	
training:	Epoch: [3][524/817]	Loss 0.2066 (0.3624)	
training:	Epoch: [3][525/817]	Loss 0.4250 (0.3625)	
training:	Epoch: [3][526/817]	Loss 0.1343 (0.3621)	
training:	Epoch: [3][527/817]	Loss 0.2384 (0.3618)	
training:	Epoch: [3][528/817]	Loss 0.1786 (0.3615)	
training:	Epoch: [3][529/817]	Loss 0.3808 (0.3615)	
training:	Epoch: [3][530/817]	Loss 0.9004 (0.3626)	
training:	Epoch: [3][531/817]	Loss 0.3148 (0.3625)	
training:	Epoch: [3][532/817]	Loss 1.1182 (0.3639)	
training:	Epoch: [3][533/817]	Loss 0.1424 (0.3635)	
training:	Epoch: [3][534/817]	Loss 0.5081 (0.3637)	
training:	Epoch: [3][535/817]	Loss 0.2385 (0.3635)	
training:	Epoch: [3][536/817]	Loss 0.5203 (0.3638)	
training:	Epoch: [3][537/817]	Loss 0.1816 (0.3635)	
training:	Epoch: [3][538/817]	Loss 0.4939 (0.3637)	
training:	Epoch: [3][539/817]	Loss 0.1367 (0.3633)	
training:	Epoch: [3][540/817]	Loss 0.1393 (0.3629)	
training:	Epoch: [3][541/817]	Loss 0.2822 (0.3627)	
training:	Epoch: [3][542/817]	Loss 0.1405 (0.3623)	
training:	Epoch: [3][543/817]	Loss 0.3150 (0.3622)	
training:	Epoch: [3][544/817]	Loss 0.2562 (0.3620)	
training:	Epoch: [3][545/817]	Loss 0.3779 (0.3621)	
training:	Epoch: [3][546/817]	Loss 0.3525 (0.3620)	
training:	Epoch: [3][547/817]	Loss 0.3036 (0.3619)	
training:	Epoch: [3][548/817]	Loss 0.4293 (0.3621)	
training:	Epoch: [3][549/817]	Loss 0.1955 (0.3617)	
training:	Epoch: [3][550/817]	Loss 0.6424 (0.3623)	
training:	Epoch: [3][551/817]	Loss 0.0894 (0.3618)	
training:	Epoch: [3][552/817]	Loss 0.2221 (0.3615)	
training:	Epoch: [3][553/817]	Loss 0.5309 (0.3618)	
training:	Epoch: [3][554/817]	Loss 0.2041 (0.3615)	
training:	Epoch: [3][555/817]	Loss 0.2151 (0.3613)	
training:	Epoch: [3][556/817]	Loss 0.6159 (0.3617)	
training:	Epoch: [3][557/817]	Loss 0.5566 (0.3621)	
training:	Epoch: [3][558/817]	Loss 0.5629 (0.3624)	
training:	Epoch: [3][559/817]	Loss 0.2214 (0.3622)	
training:	Epoch: [3][560/817]	Loss 0.6732 (0.3627)	
training:	Epoch: [3][561/817]	Loss 0.3028 (0.3626)	
training:	Epoch: [3][562/817]	Loss 0.3724 (0.3626)	
training:	Epoch: [3][563/817]	Loss 0.3777 (0.3627)	
training:	Epoch: [3][564/817]	Loss 0.1141 (0.3622)	
training:	Epoch: [3][565/817]	Loss 0.3757 (0.3623)	
training:	Epoch: [3][566/817]	Loss 0.2634 (0.3621)	
training:	Epoch: [3][567/817]	Loss 0.5762 (0.3625)	
training:	Epoch: [3][568/817]	Loss 0.4874 (0.3627)	
training:	Epoch: [3][569/817]	Loss 0.2295 (0.3624)	
training:	Epoch: [3][570/817]	Loss 0.3220 (0.3624)	
training:	Epoch: [3][571/817]	Loss 0.2306 (0.3621)	
training:	Epoch: [3][572/817]	Loss 0.5917 (0.3625)	
training:	Epoch: [3][573/817]	Loss 0.2652 (0.3624)	
training:	Epoch: [3][574/817]	Loss 0.2319 (0.3621)	
training:	Epoch: [3][575/817]	Loss 0.5602 (0.3625)	
training:	Epoch: [3][576/817]	Loss 0.0988 (0.3620)	
training:	Epoch: [3][577/817]	Loss 0.4730 (0.3622)	
training:	Epoch: [3][578/817]	Loss 0.5748 (0.3626)	
training:	Epoch: [3][579/817]	Loss 0.4415 (0.3627)	
training:	Epoch: [3][580/817]	Loss 0.3405 (0.3627)	
training:	Epoch: [3][581/817]	Loss 0.4703 (0.3629)	
training:	Epoch: [3][582/817]	Loss 0.2076 (0.3626)	
training:	Epoch: [3][583/817]	Loss 0.2115 (0.3624)	
training:	Epoch: [3][584/817]	Loss 0.1353 (0.3620)	
training:	Epoch: [3][585/817]	Loss 0.4902 (0.3622)	
training:	Epoch: [3][586/817]	Loss 0.3395 (0.3621)	
training:	Epoch: [3][587/817]	Loss 0.2799 (0.3620)	
training:	Epoch: [3][588/817]	Loss 0.1167 (0.3616)	
training:	Epoch: [3][589/817]	Loss 0.3736 (0.3616)	
training:	Epoch: [3][590/817]	Loss 0.1389 (0.3612)	
training:	Epoch: [3][591/817]	Loss 0.1059 (0.3608)	
training:	Epoch: [3][592/817]	Loss 0.2300 (0.3606)	
training:	Epoch: [3][593/817]	Loss 0.3390 (0.3605)	
training:	Epoch: [3][594/817]	Loss 0.6479 (0.3610)	
training:	Epoch: [3][595/817]	Loss 0.2949 (0.3609)	
training:	Epoch: [3][596/817]	Loss 0.3980 (0.3610)	
training:	Epoch: [3][597/817]	Loss 0.2651 (0.3608)	
training:	Epoch: [3][598/817]	Loss 0.2939 (0.3607)	
training:	Epoch: [3][599/817]	Loss 0.3764 (0.3607)	
training:	Epoch: [3][600/817]	Loss 0.8229 (0.3615)	
training:	Epoch: [3][601/817]	Loss 0.7161 (0.3621)	
training:	Epoch: [3][602/817]	Loss 0.2707 (0.3619)	
training:	Epoch: [3][603/817]	Loss 0.4542 (0.3621)	
training:	Epoch: [3][604/817]	Loss 0.3486 (0.3621)	
training:	Epoch: [3][605/817]	Loss 0.1487 (0.3617)	
training:	Epoch: [3][606/817]	Loss 0.1583 (0.3614)	
training:	Epoch: [3][607/817]	Loss 0.3027 (0.3613)	
training:	Epoch: [3][608/817]	Loss 0.3187 (0.3612)	
training:	Epoch: [3][609/817]	Loss 0.4936 (0.3614)	
training:	Epoch: [3][610/817]	Loss 0.2216 (0.3612)	
training:	Epoch: [3][611/817]	Loss 0.1862 (0.3609)	
training:	Epoch: [3][612/817]	Loss 0.4345 (0.3610)	
training:	Epoch: [3][613/817]	Loss 0.2123 (0.3608)	
training:	Epoch: [3][614/817]	Loss 0.1245 (0.3604)	
training:	Epoch: [3][615/817]	Loss 0.8563 (0.3612)	
training:	Epoch: [3][616/817]	Loss 0.2660 (0.3611)	
training:	Epoch: [3][617/817]	Loss 0.8990 (0.3619)	
training:	Epoch: [3][618/817]	Loss 0.5055 (0.3622)	
training:	Epoch: [3][619/817]	Loss 0.0905 (0.3617)	
training:	Epoch: [3][620/817]	Loss 0.6095 (0.3621)	
training:	Epoch: [3][621/817]	Loss 0.1301 (0.3618)	
training:	Epoch: [3][622/817]	Loss 0.1257 (0.3614)	
training:	Epoch: [3][623/817]	Loss 0.1643 (0.3611)	
training:	Epoch: [3][624/817]	Loss 0.1888 (0.3608)	
training:	Epoch: [3][625/817]	Loss 0.4833 (0.3610)	
training:	Epoch: [3][626/817]	Loss 0.2294 (0.3608)	
training:	Epoch: [3][627/817]	Loss 0.3225 (0.3607)	
training:	Epoch: [3][628/817]	Loss 0.0797 (0.3603)	
training:	Epoch: [3][629/817]	Loss 0.3039 (0.3602)	
training:	Epoch: [3][630/817]	Loss 0.2872 (0.3601)	
training:	Epoch: [3][631/817]	Loss 0.6979 (0.3606)	
training:	Epoch: [3][632/817]	Loss 0.4736 (0.3608)	
training:	Epoch: [3][633/817]	Loss 0.3909 (0.3608)	
training:	Epoch: [3][634/817]	Loss 0.6363 (0.3612)	
training:	Epoch: [3][635/817]	Loss 0.4656 (0.3614)	
training:	Epoch: [3][636/817]	Loss 0.3796 (0.3614)	
training:	Epoch: [3][637/817]	Loss 0.4064 (0.3615)	
training:	Epoch: [3][638/817]	Loss 0.5923 (0.3619)	
training:	Epoch: [3][639/817]	Loss 0.2081 (0.3616)	
training:	Epoch: [3][640/817]	Loss 0.3039 (0.3615)	
training:	Epoch: [3][641/817]	Loss 0.3743 (0.3616)	
training:	Epoch: [3][642/817]	Loss 0.6039 (0.3619)	
training:	Epoch: [3][643/817]	Loss 0.3673 (0.3619)	
training:	Epoch: [3][644/817]	Loss 0.1583 (0.3616)	
training:	Epoch: [3][645/817]	Loss 0.8562 (0.3624)	
training:	Epoch: [3][646/817]	Loss 0.4462 (0.3625)	
training:	Epoch: [3][647/817]	Loss 0.4535 (0.3627)	
training:	Epoch: [3][648/817]	Loss 0.3371 (0.3626)	
training:	Epoch: [3][649/817]	Loss 0.2871 (0.3625)	
training:	Epoch: [3][650/817]	Loss 0.5616 (0.3628)	
training:	Epoch: [3][651/817]	Loss 0.3337 (0.3628)	
training:	Epoch: [3][652/817]	Loss 0.4307 (0.3629)	
training:	Epoch: [3][653/817]	Loss 0.2374 (0.3627)	
training:	Epoch: [3][654/817]	Loss 0.1998 (0.3624)	
training:	Epoch: [3][655/817]	Loss 0.1822 (0.3622)	
training:	Epoch: [3][656/817]	Loss 0.2844 (0.3620)	
training:	Epoch: [3][657/817]	Loss 0.3271 (0.3620)	
training:	Epoch: [3][658/817]	Loss 0.2465 (0.3618)	
training:	Epoch: [3][659/817]	Loss 0.5124 (0.3620)	
training:	Epoch: [3][660/817]	Loss 0.1414 (0.3617)	
training:	Epoch: [3][661/817]	Loss 0.6483 (0.3621)	
training:	Epoch: [3][662/817]	Loss 0.4639 (0.3623)	
training:	Epoch: [3][663/817]	Loss 0.5014 (0.3625)	
training:	Epoch: [3][664/817]	Loss 0.4592 (0.3627)	
training:	Epoch: [3][665/817]	Loss 0.3315 (0.3626)	
training:	Epoch: [3][666/817]	Loss 0.1412 (0.3623)	
training:	Epoch: [3][667/817]	Loss 0.1744 (0.3620)	
training:	Epoch: [3][668/817]	Loss 0.5418 (0.3623)	
training:	Epoch: [3][669/817]	Loss 0.4509 (0.3624)	
training:	Epoch: [3][670/817]	Loss 0.2124 (0.3622)	
training:	Epoch: [3][671/817]	Loss 0.1367 (0.3618)	
training:	Epoch: [3][672/817]	Loss 0.7120 (0.3624)	
training:	Epoch: [3][673/817]	Loss 0.1552 (0.3620)	
training:	Epoch: [3][674/817]	Loss 0.1462 (0.3617)	
training:	Epoch: [3][675/817]	Loss 0.4490 (0.3619)	
training:	Epoch: [3][676/817]	Loss 0.6900 (0.3623)	
training:	Epoch: [3][677/817]	Loss 0.1756 (0.3621)	
training:	Epoch: [3][678/817]	Loss 0.2619 (0.3619)	
training:	Epoch: [3][679/817]	Loss 0.3192 (0.3619)	
training:	Epoch: [3][680/817]	Loss 0.3597 (0.3619)	
training:	Epoch: [3][681/817]	Loss 0.4523 (0.3620)	
training:	Epoch: [3][682/817]	Loss 0.5809 (0.3623)	
training:	Epoch: [3][683/817]	Loss 0.5542 (0.3626)	
training:	Epoch: [3][684/817]	Loss 0.3333 (0.3625)	
training:	Epoch: [3][685/817]	Loss 0.0936 (0.3622)	
training:	Epoch: [3][686/817]	Loss 0.6199 (0.3625)	
training:	Epoch: [3][687/817]	Loss 0.1524 (0.3622)	
training:	Epoch: [3][688/817]	Loss 0.1659 (0.3619)	
training:	Epoch: [3][689/817]	Loss 0.4290 (0.3620)	
training:	Epoch: [3][690/817]	Loss 0.1606 (0.3617)	
training:	Epoch: [3][691/817]	Loss 0.6168 (0.3621)	
training:	Epoch: [3][692/817]	Loss 0.4681 (0.3623)	
training:	Epoch: [3][693/817]	Loss 0.2650 (0.3621)	
training:	Epoch: [3][694/817]	Loss 0.4730 (0.3623)	
training:	Epoch: [3][695/817]	Loss 0.4253 (0.3624)	
training:	Epoch: [3][696/817]	Loss 0.8590 (0.3631)	
training:	Epoch: [3][697/817]	Loss 0.3929 (0.3631)	
training:	Epoch: [3][698/817]	Loss 0.6443 (0.3635)	
training:	Epoch: [3][699/817]	Loss 0.1494 (0.3632)	
training:	Epoch: [3][700/817]	Loss 0.5337 (0.3635)	
training:	Epoch: [3][701/817]	Loss 0.1391 (0.3631)	
training:	Epoch: [3][702/817]	Loss 0.2857 (0.3630)	
training:	Epoch: [3][703/817]	Loss 0.5146 (0.3633)	
training:	Epoch: [3][704/817]	Loss 0.0744 (0.3628)	
training:	Epoch: [3][705/817]	Loss 0.3798 (0.3629)	
training:	Epoch: [3][706/817]	Loss 0.1742 (0.3626)	
training:	Epoch: [3][707/817]	Loss 0.3582 (0.3626)	
training:	Epoch: [3][708/817]	Loss 0.5146 (0.3628)	
training:	Epoch: [3][709/817]	Loss 0.2035 (0.3626)	
training:	Epoch: [3][710/817]	Loss 0.2047 (0.3624)	
training:	Epoch: [3][711/817]	Loss 0.5378 (0.3626)	
training:	Epoch: [3][712/817]	Loss 0.6706 (0.3630)	
training:	Epoch: [3][713/817]	Loss 0.3612 (0.3630)	
training:	Epoch: [3][714/817]	Loss 0.1476 (0.3627)	
training:	Epoch: [3][715/817]	Loss 0.1751 (0.3625)	
training:	Epoch: [3][716/817]	Loss 0.4443 (0.3626)	
training:	Epoch: [3][717/817]	Loss 0.3271 (0.3625)	
training:	Epoch: [3][718/817]	Loss 0.3500 (0.3625)	
training:	Epoch: [3][719/817]	Loss 0.3647 (0.3625)	
training:	Epoch: [3][720/817]	Loss 0.3494 (0.3625)	
training:	Epoch: [3][721/817]	Loss 0.2128 (0.3623)	
training:	Epoch: [3][722/817]	Loss 0.3763 (0.3623)	
training:	Epoch: [3][723/817]	Loss 0.8864 (0.3630)	
training:	Epoch: [3][724/817]	Loss 0.3545 (0.3630)	
training:	Epoch: [3][725/817]	Loss 0.1056 (0.3627)	
training:	Epoch: [3][726/817]	Loss 0.1469 (0.3624)	
training:	Epoch: [3][727/817]	Loss 0.5020 (0.3626)	
training:	Epoch: [3][728/817]	Loss 0.7294 (0.3631)	
training:	Epoch: [3][729/817]	Loss 0.3148 (0.3630)	
training:	Epoch: [3][730/817]	Loss 0.1911 (0.3628)	
training:	Epoch: [3][731/817]	Loss 0.1373 (0.3625)	
training:	Epoch: [3][732/817]	Loss 0.0896 (0.3621)	
training:	Epoch: [3][733/817]	Loss 0.2648 (0.3620)	
training:	Epoch: [3][734/817]	Loss 0.9896 (0.3628)	
training:	Epoch: [3][735/817]	Loss 0.1560 (0.3625)	
training:	Epoch: [3][736/817]	Loss 0.1554 (0.3623)	
training:	Epoch: [3][737/817]	Loss 0.4628 (0.3624)	
training:	Epoch: [3][738/817]	Loss 0.5836 (0.3627)	
training:	Epoch: [3][739/817]	Loss 0.2859 (0.3626)	
training:	Epoch: [3][740/817]	Loss 0.1243 (0.3623)	
training:	Epoch: [3][741/817]	Loss 0.4267 (0.3623)	
training:	Epoch: [3][742/817]	Loss 0.2810 (0.3622)	
training:	Epoch: [3][743/817]	Loss 0.7073 (0.3627)	
training:	Epoch: [3][744/817]	Loss 0.4676 (0.3628)	
training:	Epoch: [3][745/817]	Loss 0.7994 (0.3634)	
training:	Epoch: [3][746/817]	Loss 0.3378 (0.3634)	
training:	Epoch: [3][747/817]	Loss 0.3689 (0.3634)	
training:	Epoch: [3][748/817]	Loss 0.5694 (0.3637)	
training:	Epoch: [3][749/817]	Loss 0.6315 (0.3640)	
training:	Epoch: [3][750/817]	Loss 0.2936 (0.3639)	
training:	Epoch: [3][751/817]	Loss 0.5374 (0.3642)	
training:	Epoch: [3][752/817]	Loss 0.2469 (0.3640)	
training:	Epoch: [3][753/817]	Loss 0.6261 (0.3644)	
training:	Epoch: [3][754/817]	Loss 0.1313 (0.3641)	
training:	Epoch: [3][755/817]	Loss 0.1566 (0.3638)	
training:	Epoch: [3][756/817]	Loss 0.1178 (0.3635)	
training:	Epoch: [3][757/817]	Loss 0.2608 (0.3633)	
training:	Epoch: [3][758/817]	Loss 0.3076 (0.3632)	
training:	Epoch: [3][759/817]	Loss 0.5823 (0.3635)	
training:	Epoch: [3][760/817]	Loss 0.3000 (0.3635)	
training:	Epoch: [3][761/817]	Loss 0.8796 (0.3641)	
training:	Epoch: [3][762/817]	Loss 0.1901 (0.3639)	
training:	Epoch: [3][763/817]	Loss 0.2614 (0.3638)	
training:	Epoch: [3][764/817]	Loss 0.4856 (0.3639)	
training:	Epoch: [3][765/817]	Loss 0.1838 (0.3637)	
training:	Epoch: [3][766/817]	Loss 0.3975 (0.3637)	
training:	Epoch: [3][767/817]	Loss 0.2924 (0.3636)	
training:	Epoch: [3][768/817]	Loss 0.7570 (0.3642)	
training:	Epoch: [3][769/817]	Loss 0.3065 (0.3641)	
training:	Epoch: [3][770/817]	Loss 0.4752 (0.3642)	
training:	Epoch: [3][771/817]	Loss 0.5539 (0.3645)	
training:	Epoch: [3][772/817]	Loss 0.5550 (0.3647)	
training:	Epoch: [3][773/817]	Loss 0.5479 (0.3650)	
training:	Epoch: [3][774/817]	Loss 0.6539 (0.3653)	
training:	Epoch: [3][775/817]	Loss 0.3756 (0.3653)	
training:	Epoch: [3][776/817]	Loss 0.3778 (0.3654)	
training:	Epoch: [3][777/817]	Loss 0.6122 (0.3657)	
training:	Epoch: [3][778/817]	Loss 0.3350 (0.3656)	
training:	Epoch: [3][779/817]	Loss 0.3235 (0.3656)	
training:	Epoch: [3][780/817]	Loss 0.6030 (0.3659)	
training:	Epoch: [3][781/817]	Loss 0.4530 (0.3660)	
training:	Epoch: [3][782/817]	Loss 0.3447 (0.3660)	
training:	Epoch: [3][783/817]	Loss 0.1969 (0.3658)	
training:	Epoch: [3][784/817]	Loss 0.1640 (0.3655)	
training:	Epoch: [3][785/817]	Loss 0.1749 (0.3653)	
training:	Epoch: [3][786/817]	Loss 0.2210 (0.3651)	
training:	Epoch: [3][787/817]	Loss 0.1369 (0.3648)	
training:	Epoch: [3][788/817]	Loss 0.2758 (0.3647)	
training:	Epoch: [3][789/817]	Loss 0.4242 (0.3647)	
training:	Epoch: [3][790/817]	Loss 0.4312 (0.3648)	
training:	Epoch: [3][791/817]	Loss 0.6616 (0.3652)	
training:	Epoch: [3][792/817]	Loss 0.5524 (0.3654)	
training:	Epoch: [3][793/817]	Loss 0.1147 (0.3651)	
training:	Epoch: [3][794/817]	Loss 0.5738 (0.3654)	
training:	Epoch: [3][795/817]	Loss 0.2887 (0.3653)	
training:	Epoch: [3][796/817]	Loss 0.1718 (0.3650)	
training:	Epoch: [3][797/817]	Loss 0.3558 (0.3650)	
training:	Epoch: [3][798/817]	Loss 0.3555 (0.3650)	
training:	Epoch: [3][799/817]	Loss 0.2972 (0.3649)	
training:	Epoch: [3][800/817]	Loss 0.3433 (0.3649)	
training:	Epoch: [3][801/817]	Loss 0.0873 (0.3646)	
training:	Epoch: [3][802/817]	Loss 0.2570 (0.3644)	
training:	Epoch: [3][803/817]	Loss 0.0966 (0.3641)	
training:	Epoch: [3][804/817]	Loss 0.2622 (0.3640)	
training:	Epoch: [3][805/817]	Loss 0.1382 (0.3637)	
training:	Epoch: [3][806/817]	Loss 0.7173 (0.3641)	
training:	Epoch: [3][807/817]	Loss 0.3813 (0.3641)	
training:	Epoch: [3][808/817]	Loss 0.3415 (0.3641)	
training:	Epoch: [3][809/817]	Loss 0.4181 (0.3642)	
training:	Epoch: [3][810/817]	Loss 0.2821 (0.3641)	
training:	Epoch: [3][811/817]	Loss 0.0936 (0.3638)	
training:	Epoch: [3][812/817]	Loss 0.3245 (0.3637)	
training:	Epoch: [3][813/817]	Loss 0.1222 (0.3634)	
training:	Epoch: [3][814/817]	Loss 0.3331 (0.3634)	
training:	Epoch: [3][815/817]	Loss 0.6585 (0.3637)	
training:	Epoch: [3][816/817]	Loss 0.1608 (0.3635)	
training:	Epoch: [3][817/817]	Loss 0.2165 (0.3633)	
Training:	 Loss: 0.3632

Training:	 ACC: 0.8526 0.8490 0.7646 0.9407
Validation:	 ACC: 0.7975 0.7929 0.6970 0.8980
Validation:	 Best_BACC: 0.8126 0.8143 0.8506 0.7747
Validation:	 Loss: 0.4369
Pretraining:	Epoch 4/200
----------
training:	Epoch: [4][1/817]	Loss 0.4712 (0.4712)	
training:	Epoch: [4][2/817]	Loss 0.3516 (0.4114)	
training:	Epoch: [4][3/817]	Loss 0.2251 (0.3493)	
training:	Epoch: [4][4/817]	Loss 0.1927 (0.3102)	
training:	Epoch: [4][5/817]	Loss 0.1456 (0.2773)	
training:	Epoch: [4][6/817]	Loss 0.2287 (0.2692)	
training:	Epoch: [4][7/817]	Loss 0.2291 (0.2634)	
training:	Epoch: [4][8/817]	Loss 0.1791 (0.2529)	
training:	Epoch: [4][9/817]	Loss 0.2540 (0.2530)	
training:	Epoch: [4][10/817]	Loss 0.1934 (0.2471)	
training:	Epoch: [4][11/817]	Loss 0.1719 (0.2402)	
training:	Epoch: [4][12/817]	Loss 0.1414 (0.2320)	
training:	Epoch: [4][13/817]	Loss 0.4064 (0.2454)	
training:	Epoch: [4][14/817]	Loss 0.0606 (0.2322)	
training:	Epoch: [4][15/817]	Loss 0.1787 (0.2286)	
training:	Epoch: [4][16/817]	Loss 0.1834 (0.2258)	
training:	Epoch: [4][17/817]	Loss 0.4242 (0.2375)	
training:	Epoch: [4][18/817]	Loss 0.4518 (0.2494)	
training:	Epoch: [4][19/817]	Loss 0.0892 (0.2410)	
training:	Epoch: [4][20/817]	Loss 0.0484 (0.2313)	
training:	Epoch: [4][21/817]	Loss 0.4634 (0.2424)	
training:	Epoch: [4][22/817]	Loss 0.1915 (0.2401)	
training:	Epoch: [4][23/817]	Loss 0.2000 (0.2383)	
training:	Epoch: [4][24/817]	Loss 0.2490 (0.2388)	
training:	Epoch: [4][25/817]	Loss 0.1640 (0.2358)	
training:	Epoch: [4][26/817]	Loss 0.6736 (0.2526)	
training:	Epoch: [4][27/817]	Loss 0.2448 (0.2523)	
training:	Epoch: [4][28/817]	Loss 0.3376 (0.2554)	
training:	Epoch: [4][29/817]	Loss 0.3165 (0.2575)	
training:	Epoch: [4][30/817]	Loss 0.2663 (0.2578)	
training:	Epoch: [4][31/817]	Loss 0.2275 (0.2568)	
training:	Epoch: [4][32/817]	Loss 0.2621 (0.2570)	
training:	Epoch: [4][33/817]	Loss 0.2203 (0.2559)	
training:	Epoch: [4][34/817]	Loss 0.1156 (0.2517)	
training:	Epoch: [4][35/817]	Loss 0.1361 (0.2484)	
training:	Epoch: [4][36/817]	Loss 0.0647 (0.2433)	
training:	Epoch: [4][37/817]	Loss 0.2814 (0.2444)	
training:	Epoch: [4][38/817]	Loss 0.6291 (0.2545)	
training:	Epoch: [4][39/817]	Loss 0.3214 (0.2562)	
training:	Epoch: [4][40/817]	Loss 0.0626 (0.2513)	
training:	Epoch: [4][41/817]	Loss 0.4123 (0.2553)	
training:	Epoch: [4][42/817]	Loss 0.2176 (0.2544)	
training:	Epoch: [4][43/817]	Loss 0.4343 (0.2586)	
training:	Epoch: [4][44/817]	Loss 0.0660 (0.2542)	
training:	Epoch: [4][45/817]	Loss 0.6523 (0.2630)	
training:	Epoch: [4][46/817]	Loss 0.5597 (0.2695)	
training:	Epoch: [4][47/817]	Loss 0.1366 (0.2667)	
training:	Epoch: [4][48/817]	Loss 0.5293 (0.2721)	
training:	Epoch: [4][49/817]	Loss 0.0804 (0.2682)	
training:	Epoch: [4][50/817]	Loss 0.2005 (0.2669)	
training:	Epoch: [4][51/817]	Loss 0.3473 (0.2684)	
training:	Epoch: [4][52/817]	Loss 0.3917 (0.2708)	
training:	Epoch: [4][53/817]	Loss 0.1243 (0.2680)	
training:	Epoch: [4][54/817]	Loss 0.1468 (0.2658)	
training:	Epoch: [4][55/817]	Loss 0.5510 (0.2710)	
training:	Epoch: [4][56/817]	Loss 0.3357 (0.2721)	
training:	Epoch: [4][57/817]	Loss 0.3669 (0.2738)	
training:	Epoch: [4][58/817]	Loss 0.4963 (0.2776)	
training:	Epoch: [4][59/817]	Loss 0.4984 (0.2814)	
training:	Epoch: [4][60/817]	Loss 0.7822 (0.2897)	
training:	Epoch: [4][61/817]	Loss 0.8203 (0.2984)	
training:	Epoch: [4][62/817]	Loss 0.4502 (0.3009)	
training:	Epoch: [4][63/817]	Loss 0.6770 (0.3068)	
training:	Epoch: [4][64/817]	Loss 0.3130 (0.3069)	
training:	Epoch: [4][65/817]	Loss 0.3022 (0.3069)	
training:	Epoch: [4][66/817]	Loss 0.2171 (0.3055)	
training:	Epoch: [4][67/817]	Loss 0.1493 (0.3032)	
training:	Epoch: [4][68/817]	Loss 0.4499 (0.3053)	
training:	Epoch: [4][69/817]	Loss 0.5656 (0.3091)	
training:	Epoch: [4][70/817]	Loss 0.0658 (0.3056)	
training:	Epoch: [4][71/817]	Loss 0.9412 (0.3146)	
training:	Epoch: [4][72/817]	Loss 0.8348 (0.3218)	
training:	Epoch: [4][73/817]	Loss 0.5305 (0.3247)	
training:	Epoch: [4][74/817]	Loss 0.0875 (0.3215)	
training:	Epoch: [4][75/817]	Loss 0.4817 (0.3236)	
training:	Epoch: [4][76/817]	Loss 0.3132 (0.3235)	
training:	Epoch: [4][77/817]	Loss 0.6191 (0.3273)	
training:	Epoch: [4][78/817]	Loss 0.1853 (0.3255)	
training:	Epoch: [4][79/817]	Loss 0.1633 (0.3234)	
training:	Epoch: [4][80/817]	Loss 0.4429 (0.3249)	
training:	Epoch: [4][81/817]	Loss 0.2506 (0.3240)	
training:	Epoch: [4][82/817]	Loss 0.2044 (0.3225)	
training:	Epoch: [4][83/817]	Loss 0.6455 (0.3264)	
training:	Epoch: [4][84/817]	Loss 0.1504 (0.3243)	
training:	Epoch: [4][85/817]	Loss 0.6041 (0.3276)	
training:	Epoch: [4][86/817]	Loss 0.3084 (0.3274)	
training:	Epoch: [4][87/817]	Loss 0.3764 (0.3280)	
training:	Epoch: [4][88/817]	Loss 0.6040 (0.3311)	
training:	Epoch: [4][89/817]	Loss 0.3572 (0.3314)	
training:	Epoch: [4][90/817]	Loss 0.1518 (0.3294)	
training:	Epoch: [4][91/817]	Loss 0.1879 (0.3279)	
training:	Epoch: [4][92/817]	Loss 0.6138 (0.3310)	
training:	Epoch: [4][93/817]	Loss 0.0887 (0.3284)	
training:	Epoch: [4][94/817]	Loss 0.2678 (0.3277)	
training:	Epoch: [4][95/817]	Loss 0.3130 (0.3276)	
training:	Epoch: [4][96/817]	Loss 0.1861 (0.3261)	
training:	Epoch: [4][97/817]	Loss 0.6068 (0.3290)	
training:	Epoch: [4][98/817]	Loss 0.2926 (0.3286)	
training:	Epoch: [4][99/817]	Loss 0.2451 (0.3278)	
training:	Epoch: [4][100/817]	Loss 0.1490 (0.3260)	
training:	Epoch: [4][101/817]	Loss 0.3237 (0.3259)	
training:	Epoch: [4][102/817]	Loss 0.4842 (0.3275)	
training:	Epoch: [4][103/817]	Loss 0.1840 (0.3261)	
training:	Epoch: [4][104/817]	Loss 0.0769 (0.3237)	
training:	Epoch: [4][105/817]	Loss 0.1780 (0.3223)	
training:	Epoch: [4][106/817]	Loss 0.0807 (0.3200)	
training:	Epoch: [4][107/817]	Loss 0.2732 (0.3196)	
training:	Epoch: [4][108/817]	Loss 0.6306 (0.3225)	
training:	Epoch: [4][109/817]	Loss 0.4543 (0.3237)	
training:	Epoch: [4][110/817]	Loss 0.2443 (0.3230)	
training:	Epoch: [4][111/817]	Loss 0.4046 (0.3237)	
training:	Epoch: [4][112/817]	Loss 0.5108 (0.3254)	
training:	Epoch: [4][113/817]	Loss 0.5844 (0.3277)	
training:	Epoch: [4][114/817]	Loss 0.6161 (0.3302)	
training:	Epoch: [4][115/817]	Loss 0.4971 (0.3317)	
training:	Epoch: [4][116/817]	Loss 0.1756 (0.3303)	
training:	Epoch: [4][117/817]	Loss 0.0791 (0.3282)	
training:	Epoch: [4][118/817]	Loss 0.5690 (0.3302)	
training:	Epoch: [4][119/817]	Loss 0.1340 (0.3286)	
training:	Epoch: [4][120/817]	Loss 0.3461 (0.3287)	
training:	Epoch: [4][121/817]	Loss 0.4144 (0.3294)	
training:	Epoch: [4][122/817]	Loss 0.3921 (0.3299)	
training:	Epoch: [4][123/817]	Loss 0.1718 (0.3286)	
training:	Epoch: [4][124/817]	Loss 0.3460 (0.3288)	
training:	Epoch: [4][125/817]	Loss 0.8525 (0.3330)	
training:	Epoch: [4][126/817]	Loss 0.6112 (0.3352)	
training:	Epoch: [4][127/817]	Loss 0.1139 (0.3334)	
training:	Epoch: [4][128/817]	Loss 0.4746 (0.3345)	
training:	Epoch: [4][129/817]	Loss 0.4463 (0.3354)	
training:	Epoch: [4][130/817]	Loss 0.0963 (0.3336)	
training:	Epoch: [4][131/817]	Loss 0.2855 (0.3332)	
training:	Epoch: [4][132/817]	Loss 0.2640 (0.3327)	
training:	Epoch: [4][133/817]	Loss 0.1102 (0.3310)	
training:	Epoch: [4][134/817]	Loss 0.1331 (0.3295)	
training:	Epoch: [4][135/817]	Loss 0.4319 (0.3303)	
training:	Epoch: [4][136/817]	Loss 0.1809 (0.3292)	
training:	Epoch: [4][137/817]	Loss 0.5617 (0.3309)	
training:	Epoch: [4][138/817]	Loss 0.2308 (0.3302)	
training:	Epoch: [4][139/817]	Loss 0.5343 (0.3316)	
training:	Epoch: [4][140/817]	Loss 0.2885 (0.3313)	
training:	Epoch: [4][141/817]	Loss 0.2325 (0.3306)	
training:	Epoch: [4][142/817]	Loss 0.2606 (0.3301)	
training:	Epoch: [4][143/817]	Loss 0.5063 (0.3314)	
training:	Epoch: [4][144/817]	Loss 0.2634 (0.3309)	
training:	Epoch: [4][145/817]	Loss 0.1134 (0.3294)	
training:	Epoch: [4][146/817]	Loss 0.3027 (0.3292)	
training:	Epoch: [4][147/817]	Loss 0.3387 (0.3293)	
training:	Epoch: [4][148/817]	Loss 0.6900 (0.3317)	
training:	Epoch: [4][149/817]	Loss 0.1833 (0.3307)	
training:	Epoch: [4][150/817]	Loss 0.4184 (0.3313)	
training:	Epoch: [4][151/817]	Loss 0.3724 (0.3316)	
training:	Epoch: [4][152/817]	Loss 0.1822 (0.3306)	
training:	Epoch: [4][153/817]	Loss 0.5430 (0.3320)	
training:	Epoch: [4][154/817]	Loss 0.6101 (0.3338)	
training:	Epoch: [4][155/817]	Loss 0.4218 (0.3343)	
training:	Epoch: [4][156/817]	Loss 0.1555 (0.3332)	
training:	Epoch: [4][157/817]	Loss 0.3307 (0.3332)	
training:	Epoch: [4][158/817]	Loss 0.1538 (0.3320)	
training:	Epoch: [4][159/817]	Loss 0.3814 (0.3324)	
training:	Epoch: [4][160/817]	Loss 0.2875 (0.3321)	
training:	Epoch: [4][161/817]	Loss 0.2863 (0.3318)	
training:	Epoch: [4][162/817]	Loss 0.1986 (0.3310)	
training:	Epoch: [4][163/817]	Loss 0.3213 (0.3309)	
training:	Epoch: [4][164/817]	Loss 0.4419 (0.3316)	
training:	Epoch: [4][165/817]	Loss 0.1766 (0.3306)	
training:	Epoch: [4][166/817]	Loss 0.1272 (0.3294)	
training:	Epoch: [4][167/817]	Loss 0.2608 (0.3290)	
training:	Epoch: [4][168/817]	Loss 0.4284 (0.3296)	
training:	Epoch: [4][169/817]	Loss 0.3588 (0.3298)	
training:	Epoch: [4][170/817]	Loss 0.3613 (0.3300)	
training:	Epoch: [4][171/817]	Loss 0.2608 (0.3296)	
training:	Epoch: [4][172/817]	Loss 0.4244 (0.3301)	
training:	Epoch: [4][173/817]	Loss 0.4817 (0.3310)	
training:	Epoch: [4][174/817]	Loss 0.2151 (0.3303)	
training:	Epoch: [4][175/817]	Loss 0.1277 (0.3292)	
training:	Epoch: [4][176/817]	Loss 0.2721 (0.3288)	
training:	Epoch: [4][177/817]	Loss 0.1101 (0.3276)	
training:	Epoch: [4][178/817]	Loss 0.2465 (0.3271)	
training:	Epoch: [4][179/817]	Loss 0.3285 (0.3271)	
training:	Epoch: [4][180/817]	Loss 0.4589 (0.3279)	
training:	Epoch: [4][181/817]	Loss 0.1255 (0.3268)	
training:	Epoch: [4][182/817]	Loss 0.1805 (0.3260)	
training:	Epoch: [4][183/817]	Loss 0.2287 (0.3254)	
training:	Epoch: [4][184/817]	Loss 0.2218 (0.3249)	
training:	Epoch: [4][185/817]	Loss 0.8690 (0.3278)	
training:	Epoch: [4][186/817]	Loss 0.2503 (0.3274)	
training:	Epoch: [4][187/817]	Loss 0.3086 (0.3273)	
training:	Epoch: [4][188/817]	Loss 0.3232 (0.3273)	
training:	Epoch: [4][189/817]	Loss 0.6002 (0.3287)	
training:	Epoch: [4][190/817]	Loss 0.2540 (0.3283)	
training:	Epoch: [4][191/817]	Loss 0.1043 (0.3271)	
training:	Epoch: [4][192/817]	Loss 0.1056 (0.3260)	
training:	Epoch: [4][193/817]	Loss 0.2437 (0.3256)	
training:	Epoch: [4][194/817]	Loss 0.5316 (0.3266)	
training:	Epoch: [4][195/817]	Loss 0.6022 (0.3280)	
training:	Epoch: [4][196/817]	Loss 0.2135 (0.3275)	
training:	Epoch: [4][197/817]	Loss 0.4242 (0.3279)	
training:	Epoch: [4][198/817]	Loss 0.4595 (0.3286)	
training:	Epoch: [4][199/817]	Loss 0.7290 (0.3306)	
training:	Epoch: [4][200/817]	Loss 0.3507 (0.3307)	
training:	Epoch: [4][201/817]	Loss 0.1256 (0.3297)	
training:	Epoch: [4][202/817]	Loss 0.1661 (0.3289)	
training:	Epoch: [4][203/817]	Loss 0.3446 (0.3290)	
training:	Epoch: [4][204/817]	Loss 0.2404 (0.3285)	
training:	Epoch: [4][205/817]	Loss 0.1723 (0.3278)	
training:	Epoch: [4][206/817]	Loss 0.6839 (0.3295)	
training:	Epoch: [4][207/817]	Loss 0.2884 (0.3293)	
training:	Epoch: [4][208/817]	Loss 0.1330 (0.3284)	
training:	Epoch: [4][209/817]	Loss 0.1772 (0.3276)	
training:	Epoch: [4][210/817]	Loss 0.4654 (0.3283)	
training:	Epoch: [4][211/817]	Loss 0.1542 (0.3275)	
training:	Epoch: [4][212/817]	Loss 0.3686 (0.3277)	
training:	Epoch: [4][213/817]	Loss 0.3822 (0.3279)	
training:	Epoch: [4][214/817]	Loss 0.5772 (0.3291)	
training:	Epoch: [4][215/817]	Loss 0.7752 (0.3312)	
training:	Epoch: [4][216/817]	Loss 0.1879 (0.3305)	
training:	Epoch: [4][217/817]	Loss 0.2398 (0.3301)	
training:	Epoch: [4][218/817]	Loss 0.3111 (0.3300)	
training:	Epoch: [4][219/817]	Loss 0.5369 (0.3309)	
training:	Epoch: [4][220/817]	Loss 0.1832 (0.3303)	
training:	Epoch: [4][221/817]	Loss 0.4699 (0.3309)	
training:	Epoch: [4][222/817]	Loss 0.4448 (0.3314)	
training:	Epoch: [4][223/817]	Loss 0.0953 (0.3303)	
training:	Epoch: [4][224/817]	Loss 0.1567 (0.3296)	
training:	Epoch: [4][225/817]	Loss 0.4633 (0.3302)	
training:	Epoch: [4][226/817]	Loss 0.7771 (0.3321)	
training:	Epoch: [4][227/817]	Loss 0.1962 (0.3315)	
training:	Epoch: [4][228/817]	Loss 0.4326 (0.3320)	
training:	Epoch: [4][229/817]	Loss 0.2483 (0.3316)	
training:	Epoch: [4][230/817]	Loss 0.3582 (0.3317)	
training:	Epoch: [4][231/817]	Loss 0.5257 (0.3326)	
training:	Epoch: [4][232/817]	Loss 0.5240 (0.3334)	
training:	Epoch: [4][233/817]	Loss 0.4747 (0.3340)	
training:	Epoch: [4][234/817]	Loss 0.5920 (0.3351)	
training:	Epoch: [4][235/817]	Loss 0.1317 (0.3342)	
training:	Epoch: [4][236/817]	Loss 0.1127 (0.3333)	
training:	Epoch: [4][237/817]	Loss 0.2620 (0.3330)	
training:	Epoch: [4][238/817]	Loss 0.4938 (0.3337)	
training:	Epoch: [4][239/817]	Loss 0.1508 (0.3329)	
training:	Epoch: [4][240/817]	Loss 0.2933 (0.3328)	
training:	Epoch: [4][241/817]	Loss 0.1431 (0.3320)	
training:	Epoch: [4][242/817]	Loss 0.3459 (0.3320)	
training:	Epoch: [4][243/817]	Loss 0.2260 (0.3316)	
training:	Epoch: [4][244/817]	Loss 0.4404 (0.3320)	
training:	Epoch: [4][245/817]	Loss 0.0851 (0.3310)	
training:	Epoch: [4][246/817]	Loss 0.2178 (0.3306)	
training:	Epoch: [4][247/817]	Loss 0.3358 (0.3306)	
training:	Epoch: [4][248/817]	Loss 0.5401 (0.3314)	
training:	Epoch: [4][249/817]	Loss 0.1681 (0.3308)	
training:	Epoch: [4][250/817]	Loss 0.0870 (0.3298)	
training:	Epoch: [4][251/817]	Loss 0.1745 (0.3292)	
training:	Epoch: [4][252/817]	Loss 0.1473 (0.3285)	
training:	Epoch: [4][253/817]	Loss 0.1736 (0.3278)	
training:	Epoch: [4][254/817]	Loss 0.4984 (0.3285)	
training:	Epoch: [4][255/817]	Loss 0.0585 (0.3275)	
training:	Epoch: [4][256/817]	Loss 0.2509 (0.3272)	
training:	Epoch: [4][257/817]	Loss 0.1412 (0.3264)	
training:	Epoch: [4][258/817]	Loss 0.1639 (0.3258)	
training:	Epoch: [4][259/817]	Loss 0.7333 (0.3274)	
training:	Epoch: [4][260/817]	Loss 0.5488 (0.3282)	
training:	Epoch: [4][261/817]	Loss 0.5204 (0.3290)	
training:	Epoch: [4][262/817]	Loss 0.1313 (0.3282)	
training:	Epoch: [4][263/817]	Loss 0.6486 (0.3294)	
training:	Epoch: [4][264/817]	Loss 0.2011 (0.3289)	
training:	Epoch: [4][265/817]	Loss 0.1669 (0.3283)	
training:	Epoch: [4][266/817]	Loss 0.4448 (0.3288)	
training:	Epoch: [4][267/817]	Loss 0.3570 (0.3289)	
training:	Epoch: [4][268/817]	Loss 0.5116 (0.3296)	
training:	Epoch: [4][269/817]	Loss 0.1431 (0.3289)	
training:	Epoch: [4][270/817]	Loss 0.1728 (0.3283)	
training:	Epoch: [4][271/817]	Loss 0.2421 (0.3280)	
training:	Epoch: [4][272/817]	Loss 0.6930 (0.3293)	
training:	Epoch: [4][273/817]	Loss 0.2590 (0.3291)	
training:	Epoch: [4][274/817]	Loss 0.2293 (0.3287)	
training:	Epoch: [4][275/817]	Loss 0.3959 (0.3289)	
training:	Epoch: [4][276/817]	Loss 1.1025 (0.3317)	
training:	Epoch: [4][277/817]	Loss 0.3327 (0.3317)	
training:	Epoch: [4][278/817]	Loss 0.5606 (0.3326)	
training:	Epoch: [4][279/817]	Loss 0.1367 (0.3319)	
training:	Epoch: [4][280/817]	Loss 0.1172 (0.3311)	
training:	Epoch: [4][281/817]	Loss 0.4331 (0.3315)	
training:	Epoch: [4][282/817]	Loss 0.5064 (0.3321)	
training:	Epoch: [4][283/817]	Loss 0.2435 (0.3318)	
training:	Epoch: [4][284/817]	Loss 0.2938 (0.3316)	
training:	Epoch: [4][285/817]	Loss 0.2473 (0.3313)	
training:	Epoch: [4][286/817]	Loss 0.2020 (0.3309)	
training:	Epoch: [4][287/817]	Loss 0.0728 (0.3300)	
training:	Epoch: [4][288/817]	Loss 0.2357 (0.3297)	
training:	Epoch: [4][289/817]	Loss 0.3266 (0.3297)	
training:	Epoch: [4][290/817]	Loss 0.4630 (0.3301)	
training:	Epoch: [4][291/817]	Loss 0.2889 (0.3300)	
training:	Epoch: [4][292/817]	Loss 0.1324 (0.3293)	
training:	Epoch: [4][293/817]	Loss 0.1113 (0.3285)	
training:	Epoch: [4][294/817]	Loss 0.1708 (0.3280)	
training:	Epoch: [4][295/817]	Loss 0.7142 (0.3293)	
training:	Epoch: [4][296/817]	Loss 0.4085 (0.3296)	
training:	Epoch: [4][297/817]	Loss 0.4363 (0.3299)	
training:	Epoch: [4][298/817]	Loss 0.3539 (0.3300)	
training:	Epoch: [4][299/817]	Loss 0.5467 (0.3308)	
training:	Epoch: [4][300/817]	Loss 0.2506 (0.3305)	
training:	Epoch: [4][301/817]	Loss 0.7897 (0.3320)	
training:	Epoch: [4][302/817]	Loss 0.4494 (0.3324)	
training:	Epoch: [4][303/817]	Loss 0.2675 (0.3322)	
training:	Epoch: [4][304/817]	Loss 0.1972 (0.3317)	
training:	Epoch: [4][305/817]	Loss 0.1430 (0.3311)	
training:	Epoch: [4][306/817]	Loss 0.1648 (0.3306)	
training:	Epoch: [4][307/817]	Loss 0.1364 (0.3299)	
training:	Epoch: [4][308/817]	Loss 0.3683 (0.3301)	
training:	Epoch: [4][309/817]	Loss 0.2339 (0.3298)	
training:	Epoch: [4][310/817]	Loss 0.1958 (0.3293)	
training:	Epoch: [4][311/817]	Loss 0.1425 (0.3287)	
training:	Epoch: [4][312/817]	Loss 0.2003 (0.3283)	
training:	Epoch: [4][313/817]	Loss 0.1319 (0.3277)	
training:	Epoch: [4][314/817]	Loss 0.1530 (0.3271)	
training:	Epoch: [4][315/817]	Loss 0.3610 (0.3272)	
training:	Epoch: [4][316/817]	Loss 0.1814 (0.3268)	
training:	Epoch: [4][317/817]	Loss 0.4680 (0.3272)	
training:	Epoch: [4][318/817]	Loss 0.1110 (0.3265)	
training:	Epoch: [4][319/817]	Loss 0.3196 (0.3265)	
training:	Epoch: [4][320/817]	Loss 0.0610 (0.3257)	
training:	Epoch: [4][321/817]	Loss 0.0675 (0.3249)	
training:	Epoch: [4][322/817]	Loss 0.2077 (0.3245)	
training:	Epoch: [4][323/817]	Loss 0.4725 (0.3250)	
training:	Epoch: [4][324/817]	Loss 0.3869 (0.3252)	
training:	Epoch: [4][325/817]	Loss 0.8210 (0.3267)	
training:	Epoch: [4][326/817]	Loss 0.2935 (0.3266)	
training:	Epoch: [4][327/817]	Loss 0.6203 (0.3275)	
training:	Epoch: [4][328/817]	Loss 0.1088 (0.3268)	
training:	Epoch: [4][329/817]	Loss 0.0767 (0.3261)	
training:	Epoch: [4][330/817]	Loss 0.1820 (0.3256)	
training:	Epoch: [4][331/817]	Loss 0.1918 (0.3252)	
training:	Epoch: [4][332/817]	Loss 0.5699 (0.3260)	
training:	Epoch: [4][333/817]	Loss 0.4435 (0.3263)	
training:	Epoch: [4][334/817]	Loss 0.0779 (0.3256)	
training:	Epoch: [4][335/817]	Loss 0.1820 (0.3251)	
training:	Epoch: [4][336/817]	Loss 0.4592 (0.3255)	
training:	Epoch: [4][337/817]	Loss 0.4679 (0.3260)	
training:	Epoch: [4][338/817]	Loss 0.3250 (0.3260)	
training:	Epoch: [4][339/817]	Loss 0.4691 (0.3264)	
training:	Epoch: [4][340/817]	Loss 0.3321 (0.3264)	
training:	Epoch: [4][341/817]	Loss 0.2132 (0.3261)	
training:	Epoch: [4][342/817]	Loss 0.1590 (0.3256)	
training:	Epoch: [4][343/817]	Loss 0.2055 (0.3252)	
training:	Epoch: [4][344/817]	Loss 0.5079 (0.3258)	
training:	Epoch: [4][345/817]	Loss 0.1820 (0.3253)	
training:	Epoch: [4][346/817]	Loss 0.2188 (0.3250)	
training:	Epoch: [4][347/817]	Loss 0.4109 (0.3253)	
training:	Epoch: [4][348/817]	Loss 0.2891 (0.3252)	
training:	Epoch: [4][349/817]	Loss 1.1450 (0.3275)	
training:	Epoch: [4][350/817]	Loss 0.3807 (0.3277)	
training:	Epoch: [4][351/817]	Loss 0.3784 (0.3278)	
training:	Epoch: [4][352/817]	Loss 0.6109 (0.3286)	
training:	Epoch: [4][353/817]	Loss 0.3003 (0.3286)	
training:	Epoch: [4][354/817]	Loss 0.4285 (0.3288)	
training:	Epoch: [4][355/817]	Loss 0.6134 (0.3296)	
training:	Epoch: [4][356/817]	Loss 0.0977 (0.3290)	
training:	Epoch: [4][357/817]	Loss 0.1097 (0.3284)	
training:	Epoch: [4][358/817]	Loss 0.1679 (0.3279)	
training:	Epoch: [4][359/817]	Loss 0.1338 (0.3274)	
training:	Epoch: [4][360/817]	Loss 0.4191 (0.3276)	
training:	Epoch: [4][361/817]	Loss 0.2816 (0.3275)	
training:	Epoch: [4][362/817]	Loss 0.2585 (0.3273)	
training:	Epoch: [4][363/817]	Loss 0.1915 (0.3269)	
training:	Epoch: [4][364/817]	Loss 0.2278 (0.3267)	
training:	Epoch: [4][365/817]	Loss 0.0804 (0.3260)	
training:	Epoch: [4][366/817]	Loss 0.3252 (0.3260)	
training:	Epoch: [4][367/817]	Loss 0.5278 (0.3265)	
training:	Epoch: [4][368/817]	Loss 0.3483 (0.3266)	
training:	Epoch: [4][369/817]	Loss 0.4051 (0.3268)	
training:	Epoch: [4][370/817]	Loss 0.2483 (0.3266)	
training:	Epoch: [4][371/817]	Loss 0.3748 (0.3267)	
training:	Epoch: [4][372/817]	Loss 0.0542 (0.3260)	
training:	Epoch: [4][373/817]	Loss 0.7878 (0.3272)	
training:	Epoch: [4][374/817]	Loss 0.1049 (0.3266)	
training:	Epoch: [4][375/817]	Loss 0.7182 (0.3277)	
training:	Epoch: [4][376/817]	Loss 0.3777 (0.3278)	
training:	Epoch: [4][377/817]	Loss 0.1405 (0.3273)	
training:	Epoch: [4][378/817]	Loss 0.2217 (0.3270)	
training:	Epoch: [4][379/817]	Loss 0.5051 (0.3275)	
training:	Epoch: [4][380/817]	Loss 0.6157 (0.3283)	
training:	Epoch: [4][381/817]	Loss 0.4467 (0.3286)	
training:	Epoch: [4][382/817]	Loss 0.5122 (0.3291)	
training:	Epoch: [4][383/817]	Loss 0.1189 (0.3285)	
training:	Epoch: [4][384/817]	Loss 0.3182 (0.3285)	
training:	Epoch: [4][385/817]	Loss 0.8346 (0.3298)	
training:	Epoch: [4][386/817]	Loss 0.5776 (0.3304)	
training:	Epoch: [4][387/817]	Loss 0.4932 (0.3309)	
training:	Epoch: [4][388/817]	Loss 0.2031 (0.3305)	
training:	Epoch: [4][389/817]	Loss 0.2629 (0.3304)	
training:	Epoch: [4][390/817]	Loss 0.2255 (0.3301)	
training:	Epoch: [4][391/817]	Loss 0.3620 (0.3302)	
training:	Epoch: [4][392/817]	Loss 0.1919 (0.3298)	
training:	Epoch: [4][393/817]	Loss 0.2248 (0.3296)	
training:	Epoch: [4][394/817]	Loss 0.2284 (0.3293)	
training:	Epoch: [4][395/817]	Loss 0.1028 (0.3287)	
training:	Epoch: [4][396/817]	Loss 0.6257 (0.3295)	
training:	Epoch: [4][397/817]	Loss 0.3410 (0.3295)	
training:	Epoch: [4][398/817]	Loss 0.2755 (0.3294)	
training:	Epoch: [4][399/817]	Loss 0.2013 (0.3290)	
training:	Epoch: [4][400/817]	Loss 0.4366 (0.3293)	
training:	Epoch: [4][401/817]	Loss 0.1143 (0.3288)	
training:	Epoch: [4][402/817]	Loss 0.1158 (0.3282)	
training:	Epoch: [4][403/817]	Loss 0.1810 (0.3279)	
training:	Epoch: [4][404/817]	Loss 0.2778 (0.3278)	
training:	Epoch: [4][405/817]	Loss 0.2996 (0.3277)	
training:	Epoch: [4][406/817]	Loss 0.3541 (0.3278)	
training:	Epoch: [4][407/817]	Loss 0.1415 (0.3273)	
training:	Epoch: [4][408/817]	Loss 0.2302 (0.3271)	
training:	Epoch: [4][409/817]	Loss 0.1587 (0.3266)	
training:	Epoch: [4][410/817]	Loss 0.6241 (0.3274)	
training:	Epoch: [4][411/817]	Loss 0.1309 (0.3269)	
training:	Epoch: [4][412/817]	Loss 0.1782 (0.3265)	
training:	Epoch: [4][413/817]	Loss 1.1402 (0.3285)	
training:	Epoch: [4][414/817]	Loss 0.2649 (0.3284)	
training:	Epoch: [4][415/817]	Loss 0.4772 (0.3287)	
training:	Epoch: [4][416/817]	Loss 0.1770 (0.3283)	
training:	Epoch: [4][417/817]	Loss 0.5242 (0.3288)	
training:	Epoch: [4][418/817]	Loss 0.5586 (0.3294)	
training:	Epoch: [4][419/817]	Loss 0.1921 (0.3290)	
training:	Epoch: [4][420/817]	Loss 0.4184 (0.3292)	
training:	Epoch: [4][421/817]	Loss 1.1611 (0.3312)	
training:	Epoch: [4][422/817]	Loss 0.0808 (0.3306)	
training:	Epoch: [4][423/817]	Loss 0.2563 (0.3305)	
training:	Epoch: [4][424/817]	Loss 0.4860 (0.3308)	
training:	Epoch: [4][425/817]	Loss 0.1882 (0.3305)	
training:	Epoch: [4][426/817]	Loss 0.7105 (0.3314)	
training:	Epoch: [4][427/817]	Loss 0.3016 (0.3313)	
training:	Epoch: [4][428/817]	Loss 0.0940 (0.3308)	
training:	Epoch: [4][429/817]	Loss 0.1883 (0.3304)	
training:	Epoch: [4][430/817]	Loss 0.1256 (0.3299)	
training:	Epoch: [4][431/817]	Loss 0.3814 (0.3301)	
training:	Epoch: [4][432/817]	Loss 0.3975 (0.3302)	
training:	Epoch: [4][433/817]	Loss 0.1469 (0.3298)	
training:	Epoch: [4][434/817]	Loss 0.2182 (0.3295)	
training:	Epoch: [4][435/817]	Loss 0.4193 (0.3297)	
training:	Epoch: [4][436/817]	Loss 0.0724 (0.3292)	
training:	Epoch: [4][437/817]	Loss 0.1764 (0.3288)	
training:	Epoch: [4][438/817]	Loss 0.1600 (0.3284)	
training:	Epoch: [4][439/817]	Loss 0.3001 (0.3284)	
training:	Epoch: [4][440/817]	Loss 0.2701 (0.3282)	
training:	Epoch: [4][441/817]	Loss 0.1980 (0.3279)	
training:	Epoch: [4][442/817]	Loss 0.1240 (0.3275)	
training:	Epoch: [4][443/817]	Loss 0.2892 (0.3274)	
training:	Epoch: [4][444/817]	Loss 0.7736 (0.3284)	
training:	Epoch: [4][445/817]	Loss 0.5225 (0.3288)	
training:	Epoch: [4][446/817]	Loss 0.4738 (0.3291)	
training:	Epoch: [4][447/817]	Loss 0.3688 (0.3292)	
training:	Epoch: [4][448/817]	Loss 0.2707 (0.3291)	
training:	Epoch: [4][449/817]	Loss 0.4635 (0.3294)	
training:	Epoch: [4][450/817]	Loss 0.1420 (0.3290)	
training:	Epoch: [4][451/817]	Loss 0.4040 (0.3292)	
training:	Epoch: [4][452/817]	Loss 0.1637 (0.3288)	
training:	Epoch: [4][453/817]	Loss 0.1760 (0.3285)	
training:	Epoch: [4][454/817]	Loss 0.5920 (0.3290)	
training:	Epoch: [4][455/817]	Loss 0.0999 (0.3285)	
training:	Epoch: [4][456/817]	Loss 0.2189 (0.3283)	
training:	Epoch: [4][457/817]	Loss 0.1607 (0.3279)	
training:	Epoch: [4][458/817]	Loss 0.1934 (0.3276)	
training:	Epoch: [4][459/817]	Loss 0.5926 (0.3282)	
training:	Epoch: [4][460/817]	Loss 0.1153 (0.3277)	
training:	Epoch: [4][461/817]	Loss 0.1374 (0.3273)	
training:	Epoch: [4][462/817]	Loss 0.3460 (0.3274)	
training:	Epoch: [4][463/817]	Loss 0.4050 (0.3275)	
training:	Epoch: [4][464/817]	Loss 0.4988 (0.3279)	
training:	Epoch: [4][465/817]	Loss 0.1426 (0.3275)	
training:	Epoch: [4][466/817]	Loss 0.1581 (0.3271)	
training:	Epoch: [4][467/817]	Loss 0.1263 (0.3267)	
training:	Epoch: [4][468/817]	Loss 0.2425 (0.3265)	
training:	Epoch: [4][469/817]	Loss 0.3041 (0.3265)	
training:	Epoch: [4][470/817]	Loss 0.7646 (0.3274)	
training:	Epoch: [4][471/817]	Loss 0.3103 (0.3274)	
training:	Epoch: [4][472/817]	Loss 0.1730 (0.3271)	
training:	Epoch: [4][473/817]	Loss 0.1403 (0.3267)	
training:	Epoch: [4][474/817]	Loss 0.3310 (0.3267)	
training:	Epoch: [4][475/817]	Loss 0.8006 (0.3277)	
training:	Epoch: [4][476/817]	Loss 0.5608 (0.3282)	
training:	Epoch: [4][477/817]	Loss 0.3717 (0.3283)	
training:	Epoch: [4][478/817]	Loss 0.8816 (0.3294)	
training:	Epoch: [4][479/817]	Loss 0.2960 (0.3293)	
training:	Epoch: [4][480/817]	Loss 0.3624 (0.3294)	
training:	Epoch: [4][481/817]	Loss 0.0864 (0.3289)	
training:	Epoch: [4][482/817]	Loss 0.3594 (0.3290)	
training:	Epoch: [4][483/817]	Loss 0.3197 (0.3289)	
training:	Epoch: [4][484/817]	Loss 0.7501 (0.3298)	
training:	Epoch: [4][485/817]	Loss 0.0487 (0.3292)	
training:	Epoch: [4][486/817]	Loss 0.1405 (0.3288)	
training:	Epoch: [4][487/817]	Loss 0.1612 (0.3285)	
training:	Epoch: [4][488/817]	Loss 0.0949 (0.3280)	
training:	Epoch: [4][489/817]	Loss 0.5317 (0.3284)	
training:	Epoch: [4][490/817]	Loss 0.3713 (0.3285)	
training:	Epoch: [4][491/817]	Loss 0.1601 (0.3282)	
training:	Epoch: [4][492/817]	Loss 0.1410 (0.3278)	
training:	Epoch: [4][493/817]	Loss 0.0734 (0.3273)	
training:	Epoch: [4][494/817]	Loss 0.6252 (0.3279)	
training:	Epoch: [4][495/817]	Loss 0.2305 (0.3277)	
training:	Epoch: [4][496/817]	Loss 0.7346 (0.3285)	
training:	Epoch: [4][497/817]	Loss 0.1102 (0.3281)	
training:	Epoch: [4][498/817]	Loss 0.4000 (0.3282)	
training:	Epoch: [4][499/817]	Loss 0.4325 (0.3284)	
training:	Epoch: [4][500/817]	Loss 0.1575 (0.3281)	
training:	Epoch: [4][501/817]	Loss 0.0974 (0.3276)	
training:	Epoch: [4][502/817]	Loss 0.1160 (0.3272)	
training:	Epoch: [4][503/817]	Loss 0.4744 (0.3275)	
training:	Epoch: [4][504/817]	Loss 0.3740 (0.3276)	
training:	Epoch: [4][505/817]	Loss 0.3284 (0.3276)	
training:	Epoch: [4][506/817]	Loss 0.1810 (0.3273)	
training:	Epoch: [4][507/817]	Loss 0.4983 (0.3276)	
training:	Epoch: [4][508/817]	Loss 0.5082 (0.3280)	
training:	Epoch: [4][509/817]	Loss 0.1851 (0.3277)	
training:	Epoch: [4][510/817]	Loss 0.1068 (0.3273)	
training:	Epoch: [4][511/817]	Loss 0.1625 (0.3270)	
training:	Epoch: [4][512/817]	Loss 0.7160 (0.3277)	
training:	Epoch: [4][513/817]	Loss 0.2632 (0.3276)	
training:	Epoch: [4][514/817]	Loss 0.1762 (0.3273)	
training:	Epoch: [4][515/817]	Loss 0.3814 (0.3274)	
training:	Epoch: [4][516/817]	Loss 0.1059 (0.3270)	
training:	Epoch: [4][517/817]	Loss 0.4808 (0.3273)	
training:	Epoch: [4][518/817]	Loss 0.1354 (0.3269)	
training:	Epoch: [4][519/817]	Loss 0.6676 (0.3276)	
training:	Epoch: [4][520/817]	Loss 0.0865 (0.3271)	
training:	Epoch: [4][521/817]	Loss 0.2531 (0.3270)	
training:	Epoch: [4][522/817]	Loss 0.4552 (0.3272)	
training:	Epoch: [4][523/817]	Loss 0.3116 (0.3272)	
training:	Epoch: [4][524/817]	Loss 0.4924 (0.3275)	
training:	Epoch: [4][525/817]	Loss 0.2833 (0.3274)	
training:	Epoch: [4][526/817]	Loss 0.6102 (0.3279)	
training:	Epoch: [4][527/817]	Loss 0.1459 (0.3276)	
training:	Epoch: [4][528/817]	Loss 0.1055 (0.3272)	
training:	Epoch: [4][529/817]	Loss 0.0864 (0.3267)	
training:	Epoch: [4][530/817]	Loss 0.4478 (0.3269)	
training:	Epoch: [4][531/817]	Loss 1.0285 (0.3283)	
training:	Epoch: [4][532/817]	Loss 0.1825 (0.3280)	
training:	Epoch: [4][533/817]	Loss 0.5679 (0.3284)	
training:	Epoch: [4][534/817]	Loss 0.1509 (0.3281)	
training:	Epoch: [4][535/817]	Loss 0.1744 (0.3278)	
training:	Epoch: [4][536/817]	Loss 0.2502 (0.3277)	
training:	Epoch: [4][537/817]	Loss 0.4131 (0.3278)	
training:	Epoch: [4][538/817]	Loss 0.0934 (0.3274)	
training:	Epoch: [4][539/817]	Loss 0.1990 (0.3272)	
training:	Epoch: [4][540/817]	Loss 0.4678 (0.3274)	
training:	Epoch: [4][541/817]	Loss 0.5242 (0.3278)	
training:	Epoch: [4][542/817]	Loss 0.4911 (0.3281)	
training:	Epoch: [4][543/817]	Loss 0.2133 (0.3279)	
training:	Epoch: [4][544/817]	Loss 0.5588 (0.3283)	
training:	Epoch: [4][545/817]	Loss 0.1777 (0.3280)	
training:	Epoch: [4][546/817]	Loss 0.7821 (0.3289)	
training:	Epoch: [4][547/817]	Loss 0.1846 (0.3286)	
training:	Epoch: [4][548/817]	Loss 0.3701 (0.3287)	
training:	Epoch: [4][549/817]	Loss 0.1117 (0.3283)	
training:	Epoch: [4][550/817]	Loss 0.3151 (0.3282)	
training:	Epoch: [4][551/817]	Loss 0.1364 (0.3279)	
training:	Epoch: [4][552/817]	Loss 0.2090 (0.3277)	
training:	Epoch: [4][553/817]	Loss 0.2070 (0.3275)	
training:	Epoch: [4][554/817]	Loss 0.2440 (0.3273)	
training:	Epoch: [4][555/817]	Loss 0.2513 (0.3272)	
training:	Epoch: [4][556/817]	Loss 0.5600 (0.3276)	
training:	Epoch: [4][557/817]	Loss 0.0983 (0.3272)	
training:	Epoch: [4][558/817]	Loss 0.1514 (0.3269)	
training:	Epoch: [4][559/817]	Loss 0.1824 (0.3266)	
training:	Epoch: [4][560/817]	Loss 0.4101 (0.3268)	
training:	Epoch: [4][561/817]	Loss 0.5203 (0.3271)	
training:	Epoch: [4][562/817]	Loss 0.3859 (0.3272)	
training:	Epoch: [4][563/817]	Loss 1.0235 (0.3284)	
training:	Epoch: [4][564/817]	Loss 0.4756 (0.3287)	
training:	Epoch: [4][565/817]	Loss 0.9316 (0.3298)	
training:	Epoch: [4][566/817]	Loss 0.2455 (0.3296)	
training:	Epoch: [4][567/817]	Loss 0.3041 (0.3296)	
training:	Epoch: [4][568/817]	Loss 0.1201 (0.3292)	
training:	Epoch: [4][569/817]	Loss 0.2361 (0.3291)	
training:	Epoch: [4][570/817]	Loss 0.4574 (0.3293)	
training:	Epoch: [4][571/817]	Loss 0.8696 (0.3302)	
training:	Epoch: [4][572/817]	Loss 0.3395 (0.3302)	
training:	Epoch: [4][573/817]	Loss 0.1927 (0.3300)	
training:	Epoch: [4][574/817]	Loss 0.6770 (0.3306)	
training:	Epoch: [4][575/817]	Loss 0.3810 (0.3307)	
training:	Epoch: [4][576/817]	Loss 0.2244 (0.3305)	
training:	Epoch: [4][577/817]	Loss 0.1261 (0.3302)	
training:	Epoch: [4][578/817]	Loss 0.4618 (0.3304)	
training:	Epoch: [4][579/817]	Loss 0.2236 (0.3302)	
training:	Epoch: [4][580/817]	Loss 0.5139 (0.3305)	
training:	Epoch: [4][581/817]	Loss 0.0663 (0.3301)	
training:	Epoch: [4][582/817]	Loss 0.1998 (0.3298)	
training:	Epoch: [4][583/817]	Loss 0.2544 (0.3297)	
training:	Epoch: [4][584/817]	Loss 1.0606 (0.3310)	
training:	Epoch: [4][585/817]	Loss 0.0982 (0.3306)	
training:	Epoch: [4][586/817]	Loss 0.5905 (0.3310)	
training:	Epoch: [4][587/817]	Loss 0.3003 (0.3309)	
training:	Epoch: [4][588/817]	Loss 0.3357 (0.3310)	
training:	Epoch: [4][589/817]	Loss 0.1493 (0.3306)	
training:	Epoch: [4][590/817]	Loss 0.6441 (0.3312)	
training:	Epoch: [4][591/817]	Loss 0.2864 (0.3311)	
training:	Epoch: [4][592/817]	Loss 0.3907 (0.3312)	
training:	Epoch: [4][593/817]	Loss 0.1515 (0.3309)	
training:	Epoch: [4][594/817]	Loss 0.2174 (0.3307)	
training:	Epoch: [4][595/817]	Loss 0.3596 (0.3308)	
training:	Epoch: [4][596/817]	Loss 0.3502 (0.3308)	
training:	Epoch: [4][597/817]	Loss 0.0659 (0.3303)	
training:	Epoch: [4][598/817]	Loss 0.2607 (0.3302)	
training:	Epoch: [4][599/817]	Loss 0.6016 (0.3307)	
training:	Epoch: [4][600/817]	Loss 0.5308 (0.3310)	
training:	Epoch: [4][601/817]	Loss 0.4973 (0.3313)	
training:	Epoch: [4][602/817]	Loss 0.5331 (0.3316)	
training:	Epoch: [4][603/817]	Loss 0.7097 (0.3323)	
training:	Epoch: [4][604/817]	Loss 0.1024 (0.3319)	
training:	Epoch: [4][605/817]	Loss 0.9654 (0.3329)	
training:	Epoch: [4][606/817]	Loss 0.2954 (0.3329)	
training:	Epoch: [4][607/817]	Loss 0.3761 (0.3329)	
training:	Epoch: [4][608/817]	Loss 0.5055 (0.3332)	
training:	Epoch: [4][609/817]	Loss 0.1618 (0.3329)	
training:	Epoch: [4][610/817]	Loss 0.1355 (0.3326)	
training:	Epoch: [4][611/817]	Loss 0.2439 (0.3325)	
training:	Epoch: [4][612/817]	Loss 0.4840 (0.3327)	
training:	Epoch: [4][613/817]	Loss 0.2870 (0.3326)	
training:	Epoch: [4][614/817]	Loss 0.1295 (0.3323)	
training:	Epoch: [4][615/817]	Loss 0.3076 (0.3323)	
training:	Epoch: [4][616/817]	Loss 0.4753 (0.3325)	
training:	Epoch: [4][617/817]	Loss 0.1974 (0.3323)	
training:	Epoch: [4][618/817]	Loss 0.3858 (0.3324)	
training:	Epoch: [4][619/817]	Loss 0.2227 (0.3322)	
training:	Epoch: [4][620/817]	Loss 0.3250 (0.3322)	
training:	Epoch: [4][621/817]	Loss 0.1075 (0.3318)	
training:	Epoch: [4][622/817]	Loss 0.3146 (0.3318)	
training:	Epoch: [4][623/817]	Loss 0.3287 (0.3318)	
training:	Epoch: [4][624/817]	Loss 0.7083 (0.3324)	
training:	Epoch: [4][625/817]	Loss 0.3196 (0.3324)	
training:	Epoch: [4][626/817]	Loss 0.1752 (0.3321)	
training:	Epoch: [4][627/817]	Loss 0.1381 (0.3318)	
training:	Epoch: [4][628/817]	Loss 0.3180 (0.3318)	
training:	Epoch: [4][629/817]	Loss 0.3522 (0.3318)	
training:	Epoch: [4][630/817]	Loss 0.2345 (0.3317)	
training:	Epoch: [4][631/817]	Loss 0.2191 (0.3315)	
training:	Epoch: [4][632/817]	Loss 0.6778 (0.3320)	
training:	Epoch: [4][633/817]	Loss 0.2411 (0.3319)	
training:	Epoch: [4][634/817]	Loss 0.2315 (0.3317)	
training:	Epoch: [4][635/817]	Loss 0.1114 (0.3314)	
training:	Epoch: [4][636/817]	Loss 0.2356 (0.3312)	
training:	Epoch: [4][637/817]	Loss 0.3866 (0.3313)	
training:	Epoch: [4][638/817]	Loss 0.5336 (0.3316)	
training:	Epoch: [4][639/817]	Loss 0.1090 (0.3313)	
training:	Epoch: [4][640/817]	Loss 0.2546 (0.3312)	
training:	Epoch: [4][641/817]	Loss 0.3597 (0.3312)	
training:	Epoch: [4][642/817]	Loss 0.2685 (0.3311)	
training:	Epoch: [4][643/817]	Loss 0.6331 (0.3316)	
training:	Epoch: [4][644/817]	Loss 0.1859 (0.3314)	
training:	Epoch: [4][645/817]	Loss 0.1006 (0.3310)	
training:	Epoch: [4][646/817]	Loss 0.1915 (0.3308)	
training:	Epoch: [4][647/817]	Loss 0.2087 (0.3306)	
training:	Epoch: [4][648/817]	Loss 0.6481 (0.3311)	
training:	Epoch: [4][649/817]	Loss 0.4483 (0.3313)	
training:	Epoch: [4][650/817]	Loss 0.3517 (0.3313)	
training:	Epoch: [4][651/817]	Loss 0.3275 (0.3313)	
training:	Epoch: [4][652/817]	Loss 0.2348 (0.3311)	
training:	Epoch: [4][653/817]	Loss 0.4171 (0.3313)	
training:	Epoch: [4][654/817]	Loss 0.4158 (0.3314)	
training:	Epoch: [4][655/817]	Loss 0.1830 (0.3312)	
training:	Epoch: [4][656/817]	Loss 0.1814 (0.3310)	
training:	Epoch: [4][657/817]	Loss 0.1215 (0.3306)	
training:	Epoch: [4][658/817]	Loss 0.3222 (0.3306)	
training:	Epoch: [4][659/817]	Loss 0.4225 (0.3308)	
training:	Epoch: [4][660/817]	Loss 0.0832 (0.3304)	
training:	Epoch: [4][661/817]	Loss 0.6024 (0.3308)	
training:	Epoch: [4][662/817]	Loss 0.5575 (0.3311)	
training:	Epoch: [4][663/817]	Loss 0.2451 (0.3310)	
training:	Epoch: [4][664/817]	Loss 0.5110 (0.3313)	
training:	Epoch: [4][665/817]	Loss 1.0427 (0.3323)	
training:	Epoch: [4][666/817]	Loss 0.2538 (0.3322)	
training:	Epoch: [4][667/817]	Loss 0.5402 (0.3325)	
training:	Epoch: [4][668/817]	Loss 0.1498 (0.3323)	
training:	Epoch: [4][669/817]	Loss 0.5796 (0.3326)	
training:	Epoch: [4][670/817]	Loss 0.3797 (0.3327)	
training:	Epoch: [4][671/817]	Loss 0.5125 (0.3330)	
training:	Epoch: [4][672/817]	Loss 0.0786 (0.3326)	
training:	Epoch: [4][673/817]	Loss 0.6911 (0.3331)	
training:	Epoch: [4][674/817]	Loss 0.2293 (0.3330)	
training:	Epoch: [4][675/817]	Loss 0.8018 (0.3337)	
training:	Epoch: [4][676/817]	Loss 0.3124 (0.3336)	
training:	Epoch: [4][677/817]	Loss 0.2030 (0.3334)	
training:	Epoch: [4][678/817]	Loss 0.6383 (0.3339)	
training:	Epoch: [4][679/817]	Loss 0.2051 (0.3337)	
training:	Epoch: [4][680/817]	Loss 0.1681 (0.3335)	
training:	Epoch: [4][681/817]	Loss 0.1175 (0.3331)	
training:	Epoch: [4][682/817]	Loss 0.5497 (0.3335)	
training:	Epoch: [4][683/817]	Loss 0.7453 (0.3341)	
training:	Epoch: [4][684/817]	Loss 0.1478 (0.3338)	
training:	Epoch: [4][685/817]	Loss 0.3111 (0.3338)	
training:	Epoch: [4][686/817]	Loss 0.7804 (0.3344)	
training:	Epoch: [4][687/817]	Loss 0.4217 (0.3345)	
training:	Epoch: [4][688/817]	Loss 0.2777 (0.3345)	
training:	Epoch: [4][689/817]	Loss 0.8231 (0.3352)	
training:	Epoch: [4][690/817]	Loss 0.6824 (0.3357)	
training:	Epoch: [4][691/817]	Loss 0.1589 (0.3354)	
training:	Epoch: [4][692/817]	Loss 0.3886 (0.3355)	
training:	Epoch: [4][693/817]	Loss 0.2539 (0.3354)	
training:	Epoch: [4][694/817]	Loss 0.2167 (0.3352)	
training:	Epoch: [4][695/817]	Loss 0.2949 (0.3351)	
training:	Epoch: [4][696/817]	Loss 0.6590 (0.3356)	
training:	Epoch: [4][697/817]	Loss 0.1846 (0.3354)	
training:	Epoch: [4][698/817]	Loss 0.3640 (0.3354)	
training:	Epoch: [4][699/817]	Loss 0.1043 (0.3351)	
training:	Epoch: [4][700/817]	Loss 0.1610 (0.3349)	
training:	Epoch: [4][701/817]	Loss 0.4598 (0.3350)	
training:	Epoch: [4][702/817]	Loss 0.1433 (0.3348)	
training:	Epoch: [4][703/817]	Loss 0.6162 (0.3352)	
training:	Epoch: [4][704/817]	Loss 0.6430 (0.3356)	
training:	Epoch: [4][705/817]	Loss 0.1986 (0.3354)	
training:	Epoch: [4][706/817]	Loss 0.0794 (0.3350)	
training:	Epoch: [4][707/817]	Loss 0.4318 (0.3352)	
training:	Epoch: [4][708/817]	Loss 0.6033 (0.3356)	
training:	Epoch: [4][709/817]	Loss 0.1779 (0.3353)	
training:	Epoch: [4][710/817]	Loss 0.5955 (0.3357)	
training:	Epoch: [4][711/817]	Loss 0.1609 (0.3355)	
training:	Epoch: [4][712/817]	Loss 0.2658 (0.3354)	
training:	Epoch: [4][713/817]	Loss 0.1521 (0.3351)	
training:	Epoch: [4][714/817]	Loss 0.4566 (0.3353)	
training:	Epoch: [4][715/817]	Loss 0.0981 (0.3349)	
training:	Epoch: [4][716/817]	Loss 0.1896 (0.3347)	
training:	Epoch: [4][717/817]	Loss 0.2953 (0.3347)	
training:	Epoch: [4][718/817]	Loss 0.3077 (0.3346)	
training:	Epoch: [4][719/817]	Loss 0.3104 (0.3346)	
training:	Epoch: [4][720/817]	Loss 0.3533 (0.3346)	
training:	Epoch: [4][721/817]	Loss 0.0981 (0.3343)	
training:	Epoch: [4][722/817]	Loss 0.1778 (0.3341)	
training:	Epoch: [4][723/817]	Loss 0.3678 (0.3341)	
training:	Epoch: [4][724/817]	Loss 0.2947 (0.3341)	
training:	Epoch: [4][725/817]	Loss 0.3155 (0.3341)	
training:	Epoch: [4][726/817]	Loss 0.7540 (0.3346)	
training:	Epoch: [4][727/817]	Loss 0.2605 (0.3345)	
training:	Epoch: [4][728/817]	Loss 0.1636 (0.3343)	
training:	Epoch: [4][729/817]	Loss 1.0128 (0.3352)	
training:	Epoch: [4][730/817]	Loss 0.1741 (0.3350)	
training:	Epoch: [4][731/817]	Loss 0.3275 (0.3350)	
training:	Epoch: [4][732/817]	Loss 0.6323 (0.3354)	
training:	Epoch: [4][733/817]	Loss 0.2407 (0.3353)	
training:	Epoch: [4][734/817]	Loss 0.0777 (0.3349)	
training:	Epoch: [4][735/817]	Loss 0.3305 (0.3349)	
training:	Epoch: [4][736/817]	Loss 0.3645 (0.3350)	
training:	Epoch: [4][737/817]	Loss 0.5285 (0.3352)	
training:	Epoch: [4][738/817]	Loss 0.2086 (0.3350)	
training:	Epoch: [4][739/817]	Loss 0.2228 (0.3349)	
training:	Epoch: [4][740/817]	Loss 0.4613 (0.3351)	
training:	Epoch: [4][741/817]	Loss 0.4821 (0.3353)	
training:	Epoch: [4][742/817]	Loss 0.7999 (0.3359)	
training:	Epoch: [4][743/817]	Loss 0.5960 (0.3362)	
training:	Epoch: [4][744/817]	Loss 0.6192 (0.3366)	
training:	Epoch: [4][745/817]	Loss 0.2589 (0.3365)	
training:	Epoch: [4][746/817]	Loss 0.1559 (0.3363)	
training:	Epoch: [4][747/817]	Loss 0.2574 (0.3362)	
training:	Epoch: [4][748/817]	Loss 0.1136 (0.3359)	
training:	Epoch: [4][749/817]	Loss 0.3870 (0.3359)	
training:	Epoch: [4][750/817]	Loss 0.2116 (0.3358)	
training:	Epoch: [4][751/817]	Loss 0.3566 (0.3358)	
training:	Epoch: [4][752/817]	Loss 0.1174 (0.3355)	
training:	Epoch: [4][753/817]	Loss 0.0643 (0.3352)	
training:	Epoch: [4][754/817]	Loss 0.0920 (0.3348)	
training:	Epoch: [4][755/817]	Loss 0.0805 (0.3345)	
training:	Epoch: [4][756/817]	Loss 0.2696 (0.3344)	
training:	Epoch: [4][757/817]	Loss 0.1954 (0.3342)	
training:	Epoch: [4][758/817]	Loss 0.4169 (0.3343)	
training:	Epoch: [4][759/817]	Loss 0.3184 (0.3343)	
training:	Epoch: [4][760/817]	Loss 0.3841 (0.3344)	
training:	Epoch: [4][761/817]	Loss 0.1495 (0.3341)	
training:	Epoch: [4][762/817]	Loss 0.4265 (0.3343)	
training:	Epoch: [4][763/817]	Loss 0.5162 (0.3345)	
training:	Epoch: [4][764/817]	Loss 0.5898 (0.3348)	
training:	Epoch: [4][765/817]	Loss 0.3269 (0.3348)	
training:	Epoch: [4][766/817]	Loss 0.3044 (0.3348)	
training:	Epoch: [4][767/817]	Loss 0.1836 (0.3346)	
training:	Epoch: [4][768/817]	Loss 0.4041 (0.3347)	
training:	Epoch: [4][769/817]	Loss 0.1865 (0.3345)	
training:	Epoch: [4][770/817]	Loss 0.1730 (0.3343)	
training:	Epoch: [4][771/817]	Loss 0.3377 (0.3343)	
training:	Epoch: [4][772/817]	Loss 0.3067 (0.3342)	
training:	Epoch: [4][773/817]	Loss 0.6573 (0.3347)	
training:	Epoch: [4][774/817]	Loss 0.3525 (0.3347)	
training:	Epoch: [4][775/817]	Loss 0.3186 (0.3347)	
training:	Epoch: [4][776/817]	Loss 0.3857 (0.3347)	
training:	Epoch: [4][777/817]	Loss 0.9873 (0.3356)	
training:	Epoch: [4][778/817]	Loss 0.4294 (0.3357)	
training:	Epoch: [4][779/817]	Loss 0.1970 (0.3355)	
training:	Epoch: [4][780/817]	Loss 0.3330 (0.3355)	
training:	Epoch: [4][781/817]	Loss 0.5329 (0.3358)	
training:	Epoch: [4][782/817]	Loss 0.4143 (0.3359)	
training:	Epoch: [4][783/817]	Loss 0.4153 (0.3360)	
training:	Epoch: [4][784/817]	Loss 0.8011 (0.3366)	
training:	Epoch: [4][785/817]	Loss 0.5904 (0.3369)	
training:	Epoch: [4][786/817]	Loss 0.3803 (0.3369)	
training:	Epoch: [4][787/817]	Loss 0.2566 (0.3368)	
training:	Epoch: [4][788/817]	Loss 0.4440 (0.3370)	
training:	Epoch: [4][789/817]	Loss 0.3343 (0.3370)	
training:	Epoch: [4][790/817]	Loss 0.3732 (0.3370)	
training:	Epoch: [4][791/817]	Loss 0.2293 (0.3369)	
training:	Epoch: [4][792/817]	Loss 0.4544 (0.3370)	
training:	Epoch: [4][793/817]	Loss 0.0711 (0.3367)	
training:	Epoch: [4][794/817]	Loss 0.5008 (0.3369)	
training:	Epoch: [4][795/817]	Loss 0.0610 (0.3365)	
training:	Epoch: [4][796/817]	Loss 0.2410 (0.3364)	
training:	Epoch: [4][797/817]	Loss 0.1436 (0.3362)	
training:	Epoch: [4][798/817]	Loss 0.0916 (0.3359)	
training:	Epoch: [4][799/817]	Loss 0.4127 (0.3360)	
training:	Epoch: [4][800/817]	Loss 0.6615 (0.3364)	
training:	Epoch: [4][801/817]	Loss 0.2380 (0.3363)	
training:	Epoch: [4][802/817]	Loss 0.5557 (0.3365)	
training:	Epoch: [4][803/817]	Loss 0.3941 (0.3366)	
training:	Epoch: [4][804/817]	Loss 0.2093 (0.3364)	
training:	Epoch: [4][805/817]	Loss 0.1088 (0.3362)	
training:	Epoch: [4][806/817]	Loss 0.2277 (0.3360)	
training:	Epoch: [4][807/817]	Loss 0.3323 (0.3360)	
training:	Epoch: [4][808/817]	Loss 0.1990 (0.3359)	
training:	Epoch: [4][809/817]	Loss 0.1376 (0.3356)	
training:	Epoch: [4][810/817]	Loss 0.1000 (0.3353)	
training:	Epoch: [4][811/817]	Loss 0.1149 (0.3350)	
training:	Epoch: [4][812/817]	Loss 0.1883 (0.3349)	
training:	Epoch: [4][813/817]	Loss 0.1959 (0.3347)	
training:	Epoch: [4][814/817]	Loss 0.2964 (0.3346)	
training:	Epoch: [4][815/817]	Loss 0.3139 (0.3346)	
training:	Epoch: [4][816/817]	Loss 0.1002 (0.3343)	
training:	Epoch: [4][817/817]	Loss 0.1026 (0.3340)	
Training:	 Loss: 0.3339

Training:	 ACC: 0.8909 0.8905 0.8807 0.9011
Validation:	 ACC: 0.8223 0.8218 0.8127 0.8318
Validation:	 Best_BACC: 0.8223 0.8218 0.8127 0.8318
Validation:	 Loss: 0.3931
Pretraining:	Epoch 5/200
----------
training:	Epoch: [5][1/817]	Loss 0.1124 (0.1124)	
training:	Epoch: [5][2/817]	Loss 0.2687 (0.1905)	
training:	Epoch: [5][3/817]	Loss 0.4058 (0.2623)	
training:	Epoch: [5][4/817]	Loss 0.2725 (0.2648)	
training:	Epoch: [5][5/817]	Loss 0.2530 (0.2625)	
training:	Epoch: [5][6/817]	Loss 0.2504 (0.2605)	
training:	Epoch: [5][7/817]	Loss 0.1385 (0.2430)	
training:	Epoch: [5][8/817]	Loss 0.1387 (0.2300)	
training:	Epoch: [5][9/817]	Loss 0.4687 (0.2565)	
training:	Epoch: [5][10/817]	Loss 0.3576 (0.2666)	
training:	Epoch: [5][11/817]	Loss 0.3139 (0.2709)	
training:	Epoch: [5][12/817]	Loss 0.2101 (0.2658)	
training:	Epoch: [5][13/817]	Loss 0.2305 (0.2631)	
training:	Epoch: [5][14/817]	Loss 0.2624 (0.2631)	
training:	Epoch: [5][15/817]	Loss 0.0835 (0.2511)	
training:	Epoch: [5][16/817]	Loss 0.1714 (0.2461)	
training:	Epoch: [5][17/817]	Loss 0.9869 (0.2897)	
training:	Epoch: [5][18/817]	Loss 0.3792 (0.2947)	
training:	Epoch: [5][19/817]	Loss 0.3776 (0.2990)	
training:	Epoch: [5][20/817]	Loss 0.1034 (0.2893)	
training:	Epoch: [5][21/817]	Loss 0.3063 (0.2901)	
training:	Epoch: [5][22/817]	Loss 0.4946 (0.2994)	
training:	Epoch: [5][23/817]	Loss 0.1061 (0.2910)	
training:	Epoch: [5][24/817]	Loss 0.3945 (0.2953)	
training:	Epoch: [5][25/817]	Loss 0.2898 (0.2951)	
training:	Epoch: [5][26/817]	Loss 0.3170 (0.2959)	
training:	Epoch: [5][27/817]	Loss 0.0661 (0.2874)	
training:	Epoch: [5][28/817]	Loss 0.1244 (0.2816)	
training:	Epoch: [5][29/817]	Loss 0.3534 (0.2841)	
training:	Epoch: [5][30/817]	Loss 0.2459 (0.2828)	
training:	Epoch: [5][31/817]	Loss 0.0805 (0.2763)	
training:	Epoch: [5][32/817]	Loss 0.1657 (0.2728)	
training:	Epoch: [5][33/817]	Loss 0.1451 (0.2689)	
training:	Epoch: [5][34/817]	Loss 0.1221 (0.2646)	
training:	Epoch: [5][35/817]	Loss 0.1900 (0.2625)	
training:	Epoch: [5][36/817]	Loss 0.5785 (0.2713)	
training:	Epoch: [5][37/817]	Loss 0.1577 (0.2682)	
training:	Epoch: [5][38/817]	Loss 0.2437 (0.2675)	
training:	Epoch: [5][39/817]	Loss 0.2699 (0.2676)	
training:	Epoch: [5][40/817]	Loss 0.2370 (0.2668)	
training:	Epoch: [5][41/817]	Loss 0.4086 (0.2703)	
training:	Epoch: [5][42/817]	Loss 0.1635 (0.2677)	
training:	Epoch: [5][43/817]	Loss 0.3336 (0.2693)	
training:	Epoch: [5][44/817]	Loss 0.6864 (0.2788)	
training:	Epoch: [5][45/817]	Loss 0.1509 (0.2759)	
training:	Epoch: [5][46/817]	Loss 0.2200 (0.2747)	
training:	Epoch: [5][47/817]	Loss 0.5593 (0.2808)	
training:	Epoch: [5][48/817]	Loss 0.3427 (0.2820)	
training:	Epoch: [5][49/817]	Loss 0.0845 (0.2780)	
training:	Epoch: [5][50/817]	Loss 0.0836 (0.2741)	
training:	Epoch: [5][51/817]	Loss 0.1856 (0.2724)	
training:	Epoch: [5][52/817]	Loss 0.3272 (0.2734)	
training:	Epoch: [5][53/817]	Loss 0.6605 (0.2808)	
training:	Epoch: [5][54/817]	Loss 0.6426 (0.2875)	
training:	Epoch: [5][55/817]	Loss 0.3418 (0.2884)	
training:	Epoch: [5][56/817]	Loss 0.0976 (0.2850)	
training:	Epoch: [5][57/817]	Loss 0.6881 (0.2921)	
training:	Epoch: [5][58/817]	Loss 0.2643 (0.2916)	
training:	Epoch: [5][59/817]	Loss 0.4639 (0.2945)	
training:	Epoch: [5][60/817]	Loss 0.5002 (0.2980)	
training:	Epoch: [5][61/817]	Loss 0.2449 (0.2971)	
training:	Epoch: [5][62/817]	Loss 0.5746 (0.3016)	
training:	Epoch: [5][63/817]	Loss 0.0917 (0.2982)	
training:	Epoch: [5][64/817]	Loss 0.1477 (0.2959)	
training:	Epoch: [5][65/817]	Loss 0.4208 (0.2978)	
training:	Epoch: [5][66/817]	Loss 0.1799 (0.2960)	
training:	Epoch: [5][67/817]	Loss 0.5739 (0.3002)	
training:	Epoch: [5][68/817]	Loss 0.2832 (0.2999)	
training:	Epoch: [5][69/817]	Loss 0.1956 (0.2984)	
training:	Epoch: [5][70/817]	Loss 0.3254 (0.2988)	
training:	Epoch: [5][71/817]	Loss 0.1462 (0.2967)	
training:	Epoch: [5][72/817]	Loss 0.3869 (0.2979)	
training:	Epoch: [5][73/817]	Loss 0.2383 (0.2971)	
training:	Epoch: [5][74/817]	Loss 0.3662 (0.2980)	
training:	Epoch: [5][75/817]	Loss 0.8180 (0.3050)	
training:	Epoch: [5][76/817]	Loss 0.2241 (0.3039)	
training:	Epoch: [5][77/817]	Loss 0.7096 (0.3092)	
training:	Epoch: [5][78/817]	Loss 0.4015 (0.3103)	
training:	Epoch: [5][79/817]	Loss 0.0668 (0.3073)	
training:	Epoch: [5][80/817]	Loss 0.4310 (0.3088)	
training:	Epoch: [5][81/817]	Loss 0.1883 (0.3073)	
training:	Epoch: [5][82/817]	Loss 0.5299 (0.3100)	
training:	Epoch: [5][83/817]	Loss 0.1309 (0.3079)	
training:	Epoch: [5][84/817]	Loss 0.1821 (0.3064)	
training:	Epoch: [5][85/817]	Loss 0.3105 (0.3064)	
training:	Epoch: [5][86/817]	Loss 0.3477 (0.3069)	
training:	Epoch: [5][87/817]	Loss 0.5877 (0.3101)	
training:	Epoch: [5][88/817]	Loss 0.4003 (0.3112)	
training:	Epoch: [5][89/817]	Loss 0.3056 (0.3111)	
training:	Epoch: [5][90/817]	Loss 0.1158 (0.3089)	
training:	Epoch: [5][91/817]	Loss 0.3819 (0.3097)	
training:	Epoch: [5][92/817]	Loss 0.2935 (0.3096)	
training:	Epoch: [5][93/817]	Loss 0.2113 (0.3085)	
training:	Epoch: [5][94/817]	Loss 0.4512 (0.3100)	
training:	Epoch: [5][95/817]	Loss 0.0481 (0.3073)	
training:	Epoch: [5][96/817]	Loss 0.1535 (0.3057)	
training:	Epoch: [5][97/817]	Loss 0.3331 (0.3059)	
training:	Epoch: [5][98/817]	Loss 0.0824 (0.3037)	
training:	Epoch: [5][99/817]	Loss 0.2784 (0.3034)	
training:	Epoch: [5][100/817]	Loss 0.6049 (0.3064)	
training:	Epoch: [5][101/817]	Loss 0.3672 (0.3070)	
training:	Epoch: [5][102/817]	Loss 0.1594 (0.3056)	
training:	Epoch: [5][103/817]	Loss 0.5102 (0.3076)	
training:	Epoch: [5][104/817]	Loss 0.4197 (0.3086)	
training:	Epoch: [5][105/817]	Loss 0.5381 (0.3108)	
training:	Epoch: [5][106/817]	Loss 0.5903 (0.3135)	
training:	Epoch: [5][107/817]	Loss 0.7598 (0.3176)	
training:	Epoch: [5][108/817]	Loss 0.1548 (0.3161)	
training:	Epoch: [5][109/817]	Loss 0.2662 (0.3157)	
training:	Epoch: [5][110/817]	Loss 0.0827 (0.3135)	
training:	Epoch: [5][111/817]	Loss 0.4697 (0.3150)	
training:	Epoch: [5][112/817]	Loss 0.2167 (0.3141)	
training:	Epoch: [5][113/817]	Loss 0.2662 (0.3137)	
training:	Epoch: [5][114/817]	Loss 0.0961 (0.3117)	
training:	Epoch: [5][115/817]	Loss 0.1521 (0.3104)	
training:	Epoch: [5][116/817]	Loss 0.2300 (0.3097)	
training:	Epoch: [5][117/817]	Loss 0.0920 (0.3078)	
training:	Epoch: [5][118/817]	Loss 0.2140 (0.3070)	
training:	Epoch: [5][119/817]	Loss 0.1984 (0.3061)	
training:	Epoch: [5][120/817]	Loss 0.2261 (0.3054)	
training:	Epoch: [5][121/817]	Loss 0.7493 (0.3091)	
training:	Epoch: [5][122/817]	Loss 0.0525 (0.3070)	
training:	Epoch: [5][123/817]	Loss 0.5082 (0.3086)	
training:	Epoch: [5][124/817]	Loss 0.1035 (0.3070)	
training:	Epoch: [5][125/817]	Loss 0.5089 (0.3086)	
training:	Epoch: [5][126/817]	Loss 0.6196 (0.3111)	
training:	Epoch: [5][127/817]	Loss 0.1524 (0.3098)	
training:	Epoch: [5][128/817]	Loss 0.1524 (0.3086)	
training:	Epoch: [5][129/817]	Loss 0.0679 (0.3067)	
training:	Epoch: [5][130/817]	Loss 0.0949 (0.3051)	
training:	Epoch: [5][131/817]	Loss 0.1331 (0.3038)	
training:	Epoch: [5][132/817]	Loss 0.2490 (0.3034)	
training:	Epoch: [5][133/817]	Loss 0.0880 (0.3017)	
training:	Epoch: [5][134/817]	Loss 0.4593 (0.3029)	
training:	Epoch: [5][135/817]	Loss 0.2396 (0.3024)	
training:	Epoch: [5][136/817]	Loss 0.4235 (0.3033)	
training:	Epoch: [5][137/817]	Loss 0.1174 (0.3020)	
training:	Epoch: [5][138/817]	Loss 0.2109 (0.3013)	
training:	Epoch: [5][139/817]	Loss 0.6181 (0.3036)	
training:	Epoch: [5][140/817]	Loss 0.1494 (0.3025)	
training:	Epoch: [5][141/817]	Loss 0.2260 (0.3020)	
training:	Epoch: [5][142/817]	Loss 0.2602 (0.3017)	
training:	Epoch: [5][143/817]	Loss 0.4768 (0.3029)	
training:	Epoch: [5][144/817]	Loss 0.5830 (0.3048)	
training:	Epoch: [5][145/817]	Loss 0.5568 (0.3066)	
training:	Epoch: [5][146/817]	Loss 0.5358 (0.3081)	
training:	Epoch: [5][147/817]	Loss 0.1336 (0.3069)	
training:	Epoch: [5][148/817]	Loss 0.2619 (0.3066)	
training:	Epoch: [5][149/817]	Loss 1.1310 (0.3122)	
training:	Epoch: [5][150/817]	Loss 0.1028 (0.3108)	
training:	Epoch: [5][151/817]	Loss 0.0736 (0.3092)	
training:	Epoch: [5][152/817]	Loss 0.6151 (0.3112)	
training:	Epoch: [5][153/817]	Loss 0.0915 (0.3098)	
training:	Epoch: [5][154/817]	Loss 0.3166 (0.3098)	
training:	Epoch: [5][155/817]	Loss 0.1658 (0.3089)	
training:	Epoch: [5][156/817]	Loss 0.7455 (0.3117)	
training:	Epoch: [5][157/817]	Loss 0.0790 (0.3102)	
training:	Epoch: [5][158/817]	Loss 0.3371 (0.3104)	
training:	Epoch: [5][159/817]	Loss 0.6573 (0.3126)	
training:	Epoch: [5][160/817]	Loss 0.2029 (0.3119)	
training:	Epoch: [5][161/817]	Loss 0.3359 (0.3120)	
training:	Epoch: [5][162/817]	Loss 0.2432 (0.3116)	
training:	Epoch: [5][163/817]	Loss 0.1339 (0.3105)	
training:	Epoch: [5][164/817]	Loss 0.1815 (0.3097)	
training:	Epoch: [5][165/817]	Loss 0.4603 (0.3106)	
training:	Epoch: [5][166/817]	Loss 0.0876 (0.3093)	
training:	Epoch: [5][167/817]	Loss 0.0961 (0.3080)	
training:	Epoch: [5][168/817]	Loss 0.3016 (0.3080)	
training:	Epoch: [5][169/817]	Loss 0.7268 (0.3105)	
training:	Epoch: [5][170/817]	Loss 0.1338 (0.3094)	
training:	Epoch: [5][171/817]	Loss 0.4617 (0.3103)	
training:	Epoch: [5][172/817]	Loss 0.5133 (0.3115)	
training:	Epoch: [5][173/817]	Loss 0.5071 (0.3126)	
training:	Epoch: [5][174/817]	Loss 0.4427 (0.3134)	
training:	Epoch: [5][175/817]	Loss 0.4238 (0.3140)	
training:	Epoch: [5][176/817]	Loss 0.6296 (0.3158)	
training:	Epoch: [5][177/817]	Loss 0.0945 (0.3145)	
training:	Epoch: [5][178/817]	Loss 0.2729 (0.3143)	
training:	Epoch: [5][179/817]	Loss 0.3571 (0.3146)	
training:	Epoch: [5][180/817]	Loss 0.0667 (0.3132)	
training:	Epoch: [5][181/817]	Loss 0.3115 (0.3132)	
training:	Epoch: [5][182/817]	Loss 0.1416 (0.3122)	
training:	Epoch: [5][183/817]	Loss 0.5249 (0.3134)	
training:	Epoch: [5][184/817]	Loss 0.5174 (0.3145)	
training:	Epoch: [5][185/817]	Loss 0.2758 (0.3143)	
training:	Epoch: [5][186/817]	Loss 0.2398 (0.3139)	
training:	Epoch: [5][187/817]	Loss 0.1175 (0.3128)	
training:	Epoch: [5][188/817]	Loss 0.2099 (0.3123)	
training:	Epoch: [5][189/817]	Loss 0.4254 (0.3129)	
training:	Epoch: [5][190/817]	Loss 0.2450 (0.3125)	
training:	Epoch: [5][191/817]	Loss 0.3973 (0.3130)	
training:	Epoch: [5][192/817]	Loss 0.7197 (0.3151)	
training:	Epoch: [5][193/817]	Loss 0.2789 (0.3149)	
training:	Epoch: [5][194/817]	Loss 0.5864 (0.3163)	
training:	Epoch: [5][195/817]	Loss 0.1491 (0.3154)	
training:	Epoch: [5][196/817]	Loss 0.2726 (0.3152)	
training:	Epoch: [5][197/817]	Loss 0.5023 (0.3162)	
training:	Epoch: [5][198/817]	Loss 0.5864 (0.3175)	
training:	Epoch: [5][199/817]	Loss 0.6118 (0.3190)	
training:	Epoch: [5][200/817]	Loss 0.3356 (0.3191)	
training:	Epoch: [5][201/817]	Loss 0.1358 (0.3182)	
training:	Epoch: [5][202/817]	Loss 0.4153 (0.3187)	
training:	Epoch: [5][203/817]	Loss 0.1670 (0.3179)	
training:	Epoch: [5][204/817]	Loss 0.0814 (0.3168)	
training:	Epoch: [5][205/817]	Loss 0.0859 (0.3156)	
training:	Epoch: [5][206/817]	Loss 0.3371 (0.3157)	
training:	Epoch: [5][207/817]	Loss 0.4690 (0.3165)	
training:	Epoch: [5][208/817]	Loss 0.6792 (0.3182)	
training:	Epoch: [5][209/817]	Loss 0.4174 (0.3187)	
training:	Epoch: [5][210/817]	Loss 0.1438 (0.3179)	
training:	Epoch: [5][211/817]	Loss 0.4840 (0.3187)	
training:	Epoch: [5][212/817]	Loss 0.1259 (0.3177)	
training:	Epoch: [5][213/817]	Loss 0.4193 (0.3182)	
training:	Epoch: [5][214/817]	Loss 0.1507 (0.3174)	
training:	Epoch: [5][215/817]	Loss 0.1112 (0.3165)	
training:	Epoch: [5][216/817]	Loss 0.0826 (0.3154)	
training:	Epoch: [5][217/817]	Loss 0.3942 (0.3158)	
training:	Epoch: [5][218/817]	Loss 0.2642 (0.3155)	
training:	Epoch: [5][219/817]	Loss 0.1218 (0.3146)	
training:	Epoch: [5][220/817]	Loss 0.3594 (0.3148)	
training:	Epoch: [5][221/817]	Loss 0.3820 (0.3151)	
training:	Epoch: [5][222/817]	Loss 0.3630 (0.3154)	
training:	Epoch: [5][223/817]	Loss 0.0706 (0.3143)	
training:	Epoch: [5][224/817]	Loss 0.7340 (0.3161)	
training:	Epoch: [5][225/817]	Loss 0.1188 (0.3153)	
training:	Epoch: [5][226/817]	Loss 0.1583 (0.3146)	
training:	Epoch: [5][227/817]	Loss 0.8483 (0.3169)	
training:	Epoch: [5][228/817]	Loss 0.2839 (0.3168)	
training:	Epoch: [5][229/817]	Loss 0.3282 (0.3168)	
training:	Epoch: [5][230/817]	Loss 0.2088 (0.3164)	
training:	Epoch: [5][231/817]	Loss 0.3353 (0.3164)	
training:	Epoch: [5][232/817]	Loss 0.5569 (0.3175)	
training:	Epoch: [5][233/817]	Loss 0.1083 (0.3166)	
training:	Epoch: [5][234/817]	Loss 0.2138 (0.3161)	
training:	Epoch: [5][235/817]	Loss 0.6402 (0.3175)	
training:	Epoch: [5][236/817]	Loss 0.0701 (0.3165)	
training:	Epoch: [5][237/817]	Loss 0.1631 (0.3158)	
training:	Epoch: [5][238/817]	Loss 0.1631 (0.3152)	
training:	Epoch: [5][239/817]	Loss 0.2458 (0.3149)	
training:	Epoch: [5][240/817]	Loss 0.1039 (0.3140)	
training:	Epoch: [5][241/817]	Loss 0.5491 (0.3150)	
training:	Epoch: [5][242/817]	Loss 0.7521 (0.3168)	
training:	Epoch: [5][243/817]	Loss 0.2726 (0.3166)	
training:	Epoch: [5][244/817]	Loss 0.2871 (0.3165)	
training:	Epoch: [5][245/817]	Loss 0.1549 (0.3158)	
training:	Epoch: [5][246/817]	Loss 0.2109 (0.3154)	
training:	Epoch: [5][247/817]	Loss 0.1317 (0.3147)	
training:	Epoch: [5][248/817]	Loss 0.4946 (0.3154)	
training:	Epoch: [5][249/817]	Loss 0.1162 (0.3146)	
training:	Epoch: [5][250/817]	Loss 0.1063 (0.3137)	
training:	Epoch: [5][251/817]	Loss 0.1931 (0.3133)	
training:	Epoch: [5][252/817]	Loss 0.1084 (0.3125)	
training:	Epoch: [5][253/817]	Loss 0.1465 (0.3118)	
training:	Epoch: [5][254/817]	Loss 0.0974 (0.3110)	
training:	Epoch: [5][255/817]	Loss 0.4514 (0.3115)	
training:	Epoch: [5][256/817]	Loss 0.0818 (0.3106)	
training:	Epoch: [5][257/817]	Loss 0.5701 (0.3116)	
training:	Epoch: [5][258/817]	Loss 0.7699 (0.3134)	
training:	Epoch: [5][259/817]	Loss 0.2241 (0.3131)	
training:	Epoch: [5][260/817]	Loss 0.2418 (0.3128)	
training:	Epoch: [5][261/817]	Loss 0.4679 (0.3134)	
training:	Epoch: [5][262/817]	Loss 0.3111 (0.3134)	
training:	Epoch: [5][263/817]	Loss 0.1186 (0.3126)	
training:	Epoch: [5][264/817]	Loss 0.4576 (0.3132)	
training:	Epoch: [5][265/817]	Loss 0.2009 (0.3127)	
training:	Epoch: [5][266/817]	Loss 0.2405 (0.3125)	
training:	Epoch: [5][267/817]	Loss 0.1892 (0.3120)	
training:	Epoch: [5][268/817]	Loss 0.2806 (0.3119)	
training:	Epoch: [5][269/817]	Loss 0.3077 (0.3119)	
training:	Epoch: [5][270/817]	Loss 0.2413 (0.3116)	
training:	Epoch: [5][271/817]	Loss 0.2973 (0.3116)	
training:	Epoch: [5][272/817]	Loss 0.3437 (0.3117)	
training:	Epoch: [5][273/817]	Loss 0.1162 (0.3110)	
training:	Epoch: [5][274/817]	Loss 0.2616 (0.3108)	
training:	Epoch: [5][275/817]	Loss 0.3152 (0.3108)	
training:	Epoch: [5][276/817]	Loss 0.2062 (0.3104)	
training:	Epoch: [5][277/817]	Loss 0.0688 (0.3096)	
training:	Epoch: [5][278/817]	Loss 0.0792 (0.3087)	
training:	Epoch: [5][279/817]	Loss 0.2603 (0.3086)	
training:	Epoch: [5][280/817]	Loss 0.3240 (0.3086)	
training:	Epoch: [5][281/817]	Loss 0.1637 (0.3081)	
training:	Epoch: [5][282/817]	Loss 0.3090 (0.3081)	
training:	Epoch: [5][283/817]	Loss 0.2239 (0.3078)	
training:	Epoch: [5][284/817]	Loss 0.1486 (0.3072)	
training:	Epoch: [5][285/817]	Loss 0.4990 (0.3079)	
training:	Epoch: [5][286/817]	Loss 0.5319 (0.3087)	
training:	Epoch: [5][287/817]	Loss 0.1602 (0.3082)	
training:	Epoch: [5][288/817]	Loss 0.2481 (0.3080)	
training:	Epoch: [5][289/817]	Loss 0.2972 (0.3079)	
training:	Epoch: [5][290/817]	Loss 0.1019 (0.3072)	
training:	Epoch: [5][291/817]	Loss 0.4916 (0.3079)	
training:	Epoch: [5][292/817]	Loss 0.6569 (0.3090)	
training:	Epoch: [5][293/817]	Loss 0.3167 (0.3091)	
training:	Epoch: [5][294/817]	Loss 0.1544 (0.3085)	
training:	Epoch: [5][295/817]	Loss 0.3228 (0.3086)	
training:	Epoch: [5][296/817]	Loss 0.1678 (0.3081)	
training:	Epoch: [5][297/817]	Loss 0.4680 (0.3087)	
training:	Epoch: [5][298/817]	Loss 0.7355 (0.3101)	
training:	Epoch: [5][299/817]	Loss 0.1504 (0.3096)	
training:	Epoch: [5][300/817]	Loss 0.4656 (0.3101)	
training:	Epoch: [5][301/817]	Loss 0.2281 (0.3098)	
training:	Epoch: [5][302/817]	Loss 0.1413 (0.3092)	
training:	Epoch: [5][303/817]	Loss 0.2921 (0.3092)	
training:	Epoch: [5][304/817]	Loss 0.0664 (0.3084)	
training:	Epoch: [5][305/817]	Loss 0.2431 (0.3082)	
training:	Epoch: [5][306/817]	Loss 0.1502 (0.3077)	
training:	Epoch: [5][307/817]	Loss 0.4407 (0.3081)	
training:	Epoch: [5][308/817]	Loss 0.1605 (0.3076)	
training:	Epoch: [5][309/817]	Loss 0.5612 (0.3084)	
training:	Epoch: [5][310/817]	Loss 0.2243 (0.3082)	
training:	Epoch: [5][311/817]	Loss 0.3437 (0.3083)	
training:	Epoch: [5][312/817]	Loss 0.3969 (0.3086)	
training:	Epoch: [5][313/817]	Loss 0.2966 (0.3085)	
training:	Epoch: [5][314/817]	Loss 0.5318 (0.3092)	
training:	Epoch: [5][315/817]	Loss 0.3902 (0.3095)	
training:	Epoch: [5][316/817]	Loss 0.3050 (0.3095)	
training:	Epoch: [5][317/817]	Loss 0.5481 (0.3102)	
training:	Epoch: [5][318/817]	Loss 0.1656 (0.3098)	
training:	Epoch: [5][319/817]	Loss 0.5725 (0.3106)	
training:	Epoch: [5][320/817]	Loss 0.5886 (0.3115)	
training:	Epoch: [5][321/817]	Loss 0.1219 (0.3109)	
training:	Epoch: [5][322/817]	Loss 0.2297 (0.3106)	
training:	Epoch: [5][323/817]	Loss 0.4158 (0.3110)	
training:	Epoch: [5][324/817]	Loss 0.0836 (0.3103)	
training:	Epoch: [5][325/817]	Loss 0.1340 (0.3097)	
training:	Epoch: [5][326/817]	Loss 0.1194 (0.3091)	
training:	Epoch: [5][327/817]	Loss 0.2428 (0.3089)	
training:	Epoch: [5][328/817]	Loss 0.3419 (0.3090)	
training:	Epoch: [5][329/817]	Loss 0.5556 (0.3098)	
training:	Epoch: [5][330/817]	Loss 0.2602 (0.3096)	
training:	Epoch: [5][331/817]	Loss 0.2777 (0.3095)	
training:	Epoch: [5][332/817]	Loss 0.1688 (0.3091)	
training:	Epoch: [5][333/817]	Loss 0.4549 (0.3095)	
training:	Epoch: [5][334/817]	Loss 0.2380 (0.3093)	
training:	Epoch: [5][335/817]	Loss 0.2242 (0.3091)	
training:	Epoch: [5][336/817]	Loss 0.1118 (0.3085)	
training:	Epoch: [5][337/817]	Loss 0.0944 (0.3078)	
training:	Epoch: [5][338/817]	Loss 0.5602 (0.3086)	
training:	Epoch: [5][339/817]	Loss 0.5135 (0.3092)	
training:	Epoch: [5][340/817]	Loss 0.0998 (0.3086)	
training:	Epoch: [5][341/817]	Loss 0.3019 (0.3086)	
training:	Epoch: [5][342/817]	Loss 0.1746 (0.3082)	
training:	Epoch: [5][343/817]	Loss 0.0797 (0.3075)	
training:	Epoch: [5][344/817]	Loss 0.1207 (0.3070)	
training:	Epoch: [5][345/817]	Loss 0.0928 (0.3063)	
training:	Epoch: [5][346/817]	Loss 0.3318 (0.3064)	
training:	Epoch: [5][347/817]	Loss 0.2558 (0.3063)	
training:	Epoch: [5][348/817]	Loss 0.6506 (0.3073)	
training:	Epoch: [5][349/817]	Loss 0.2190 (0.3070)	
training:	Epoch: [5][350/817]	Loss 0.3000 (0.3070)	
training:	Epoch: [5][351/817]	Loss 0.3241 (0.3070)	
training:	Epoch: [5][352/817]	Loss 0.2047 (0.3067)	
training:	Epoch: [5][353/817]	Loss 0.4112 (0.3070)	
training:	Epoch: [5][354/817]	Loss 0.1814 (0.3067)	
training:	Epoch: [5][355/817]	Loss 0.4257 (0.3070)	
training:	Epoch: [5][356/817]	Loss 0.3097 (0.3070)	
training:	Epoch: [5][357/817]	Loss 0.1521 (0.3066)	
training:	Epoch: [5][358/817]	Loss 0.6043 (0.3074)	
training:	Epoch: [5][359/817]	Loss 0.5280 (0.3080)	
training:	Epoch: [5][360/817]	Loss 0.2159 (0.3078)	
training:	Epoch: [5][361/817]	Loss 0.1837 (0.3074)	
training:	Epoch: [5][362/817]	Loss 0.2068 (0.3072)	
training:	Epoch: [5][363/817]	Loss 0.3952 (0.3074)	
training:	Epoch: [5][364/817]	Loss 0.8638 (0.3089)	
training:	Epoch: [5][365/817]	Loss 0.3400 (0.3090)	
training:	Epoch: [5][366/817]	Loss 0.5438 (0.3097)	
training:	Epoch: [5][367/817]	Loss 0.6555 (0.3106)	
training:	Epoch: [5][368/817]	Loss 0.2274 (0.3104)	
training:	Epoch: [5][369/817]	Loss 0.2481 (0.3102)	
training:	Epoch: [5][370/817]	Loss 0.4470 (0.3106)	
training:	Epoch: [5][371/817]	Loss 0.2235 (0.3103)	
training:	Epoch: [5][372/817]	Loss 0.1001 (0.3098)	
training:	Epoch: [5][373/817]	Loss 0.1506 (0.3094)	
training:	Epoch: [5][374/817]	Loss 0.0712 (0.3087)	
training:	Epoch: [5][375/817]	Loss 0.7405 (0.3099)	
training:	Epoch: [5][376/817]	Loss 0.0760 (0.3092)	
training:	Epoch: [5][377/817]	Loss 0.1176 (0.3087)	
training:	Epoch: [5][378/817]	Loss 0.1684 (0.3084)	
training:	Epoch: [5][379/817]	Loss 0.0864 (0.3078)	
training:	Epoch: [5][380/817]	Loss 0.6604 (0.3087)	
training:	Epoch: [5][381/817]	Loss 0.4021 (0.3090)	
training:	Epoch: [5][382/817]	Loss 0.3802 (0.3091)	
training:	Epoch: [5][383/817]	Loss 0.2323 (0.3089)	
training:	Epoch: [5][384/817]	Loss 0.4446 (0.3093)	
training:	Epoch: [5][385/817]	Loss 0.0911 (0.3087)	
training:	Epoch: [5][386/817]	Loss 0.5546 (0.3094)	
training:	Epoch: [5][387/817]	Loss 0.3878 (0.3096)	
training:	Epoch: [5][388/817]	Loss 0.1011 (0.3090)	
training:	Epoch: [5][389/817]	Loss 0.0966 (0.3085)	
training:	Epoch: [5][390/817]	Loss 0.0656 (0.3079)	
training:	Epoch: [5][391/817]	Loss 0.0628 (0.3072)	
training:	Epoch: [5][392/817]	Loss 0.1923 (0.3069)	
training:	Epoch: [5][393/817]	Loss 0.4679 (0.3073)	
training:	Epoch: [5][394/817]	Loss 0.2070 (0.3071)	
training:	Epoch: [5][395/817]	Loss 0.1042 (0.3066)	
training:	Epoch: [5][396/817]	Loss 0.0611 (0.3060)	
training:	Epoch: [5][397/817]	Loss 0.2960 (0.3059)	
training:	Epoch: [5][398/817]	Loss 0.4708 (0.3063)	
training:	Epoch: [5][399/817]	Loss 0.3785 (0.3065)	
training:	Epoch: [5][400/817]	Loss 0.1122 (0.3060)	
training:	Epoch: [5][401/817]	Loss 0.6824 (0.3070)	
training:	Epoch: [5][402/817]	Loss 0.1502 (0.3066)	
training:	Epoch: [5][403/817]	Loss 0.7054 (0.3076)	
training:	Epoch: [5][404/817]	Loss 0.0805 (0.3070)	
training:	Epoch: [5][405/817]	Loss 0.4822 (0.3075)	
training:	Epoch: [5][406/817]	Loss 0.0812 (0.3069)	
training:	Epoch: [5][407/817]	Loss 0.1114 (0.3064)	
training:	Epoch: [5][408/817]	Loss 0.8704 (0.3078)	
training:	Epoch: [5][409/817]	Loss 0.2972 (0.3078)	
training:	Epoch: [5][410/817]	Loss 0.2184 (0.3076)	
training:	Epoch: [5][411/817]	Loss 0.1421 (0.3071)	
training:	Epoch: [5][412/817]	Loss 0.2214 (0.3069)	
training:	Epoch: [5][413/817]	Loss 0.2748 (0.3069)	
training:	Epoch: [5][414/817]	Loss 0.8797 (0.3082)	
training:	Epoch: [5][415/817]	Loss 0.1943 (0.3080)	
training:	Epoch: [5][416/817]	Loss 0.2656 (0.3079)	
training:	Epoch: [5][417/817]	Loss 0.2073 (0.3076)	
training:	Epoch: [5][418/817]	Loss 0.1428 (0.3072)	
training:	Epoch: [5][419/817]	Loss 0.6107 (0.3080)	
training:	Epoch: [5][420/817]	Loss 0.1010 (0.3075)	
training:	Epoch: [5][421/817]	Loss 0.1494 (0.3071)	
training:	Epoch: [5][422/817]	Loss 0.3665 (0.3072)	
training:	Epoch: [5][423/817]	Loss 0.1305 (0.3068)	
training:	Epoch: [5][424/817]	Loss 0.3772 (0.3070)	
training:	Epoch: [5][425/817]	Loss 0.1455 (0.3066)	
training:	Epoch: [5][426/817]	Loss 0.3417 (0.3067)	
training:	Epoch: [5][427/817]	Loss 0.6897 (0.3076)	
training:	Epoch: [5][428/817]	Loss 0.1390 (0.3072)	
training:	Epoch: [5][429/817]	Loss 0.0866 (0.3067)	
training:	Epoch: [5][430/817]	Loss 0.5835 (0.3073)	
training:	Epoch: [5][431/817]	Loss 0.2270 (0.3071)	
training:	Epoch: [5][432/817]	Loss 0.1720 (0.3068)	
training:	Epoch: [5][433/817]	Loss 0.4327 (0.3071)	
training:	Epoch: [5][434/817]	Loss 0.4987 (0.3075)	
training:	Epoch: [5][435/817]	Loss 0.0830 (0.3070)	
training:	Epoch: [5][436/817]	Loss 0.2026 (0.3068)	
training:	Epoch: [5][437/817]	Loss 0.0788 (0.3063)	
training:	Epoch: [5][438/817]	Loss 0.0737 (0.3057)	
training:	Epoch: [5][439/817]	Loss 0.0547 (0.3052)	
training:	Epoch: [5][440/817]	Loss 0.1634 (0.3048)	
training:	Epoch: [5][441/817]	Loss 0.2848 (0.3048)	
training:	Epoch: [5][442/817]	Loss 0.3459 (0.3049)	
training:	Epoch: [5][443/817]	Loss 0.5909 (0.3055)	
training:	Epoch: [5][444/817]	Loss 0.0606 (0.3050)	
training:	Epoch: [5][445/817]	Loss 0.1561 (0.3047)	
training:	Epoch: [5][446/817]	Loss 0.5613 (0.3052)	
training:	Epoch: [5][447/817]	Loss 0.0557 (0.3047)	
training:	Epoch: [5][448/817]	Loss 0.0715 (0.3041)	
training:	Epoch: [5][449/817]	Loss 0.0814 (0.3037)	
training:	Epoch: [5][450/817]	Loss 0.1926 (0.3034)	
training:	Epoch: [5][451/817]	Loss 0.3485 (0.3035)	
training:	Epoch: [5][452/817]	Loss 0.5692 (0.3041)	
training:	Epoch: [5][453/817]	Loss 0.1708 (0.3038)	
training:	Epoch: [5][454/817]	Loss 0.3343 (0.3039)	
training:	Epoch: [5][455/817]	Loss 0.1070 (0.3034)	
training:	Epoch: [5][456/817]	Loss 0.1228 (0.3030)	
training:	Epoch: [5][457/817]	Loss 0.1263 (0.3027)	
training:	Epoch: [5][458/817]	Loss 0.1794 (0.3024)	
training:	Epoch: [5][459/817]	Loss 0.1107 (0.3020)	
training:	Epoch: [5][460/817]	Loss 0.5418 (0.3025)	
training:	Epoch: [5][461/817]	Loss 0.1492 (0.3022)	
training:	Epoch: [5][462/817]	Loss 0.4954 (0.3026)	
training:	Epoch: [5][463/817]	Loss 0.1962 (0.3023)	
training:	Epoch: [5][464/817]	Loss 0.2020 (0.3021)	
training:	Epoch: [5][465/817]	Loss 0.2787 (0.3021)	
training:	Epoch: [5][466/817]	Loss 0.2766 (0.3020)	
training:	Epoch: [5][467/817]	Loss 0.6081 (0.3027)	
training:	Epoch: [5][468/817]	Loss 0.0999 (0.3022)	
training:	Epoch: [5][469/817]	Loss 0.1113 (0.3018)	
training:	Epoch: [5][470/817]	Loss 0.0558 (0.3013)	
training:	Epoch: [5][471/817]	Loss 0.6310 (0.3020)	
training:	Epoch: [5][472/817]	Loss 0.1273 (0.3016)	
training:	Epoch: [5][473/817]	Loss 0.5094 (0.3021)	
training:	Epoch: [5][474/817]	Loss 0.1749 (0.3018)	
training:	Epoch: [5][475/817]	Loss 0.2225 (0.3016)	
training:	Epoch: [5][476/817]	Loss 0.4311 (0.3019)	
training:	Epoch: [5][477/817]	Loss 0.1256 (0.3015)	
training:	Epoch: [5][478/817]	Loss 0.0507 (0.3010)	
training:	Epoch: [5][479/817]	Loss 0.4566 (0.3013)	
training:	Epoch: [5][480/817]	Loss 0.9639 (0.3027)	
training:	Epoch: [5][481/817]	Loss 0.4009 (0.3029)	
training:	Epoch: [5][482/817]	Loss 0.7813 (0.3039)	
training:	Epoch: [5][483/817]	Loss 0.2618 (0.3038)	
training:	Epoch: [5][484/817]	Loss 0.6266 (0.3045)	
training:	Epoch: [5][485/817]	Loss 0.3026 (0.3045)	
training:	Epoch: [5][486/817]	Loss 0.9976 (0.3059)	
training:	Epoch: [5][487/817]	Loss 0.2240 (0.3058)	
training:	Epoch: [5][488/817]	Loss 0.3528 (0.3059)	
training:	Epoch: [5][489/817]	Loss 0.1125 (0.3055)	
training:	Epoch: [5][490/817]	Loss 0.5172 (0.3059)	
training:	Epoch: [5][491/817]	Loss 0.7762 (0.3068)	
training:	Epoch: [5][492/817]	Loss 0.1232 (0.3065)	
training:	Epoch: [5][493/817]	Loss 0.2044 (0.3063)	
training:	Epoch: [5][494/817]	Loss 0.1326 (0.3059)	
training:	Epoch: [5][495/817]	Loss 0.2615 (0.3058)	
training:	Epoch: [5][496/817]	Loss 0.3125 (0.3058)	
training:	Epoch: [5][497/817]	Loss 0.0733 (0.3054)	
training:	Epoch: [5][498/817]	Loss 0.3661 (0.3055)	
training:	Epoch: [5][499/817]	Loss 0.7376 (0.3064)	
training:	Epoch: [5][500/817]	Loss 0.3197 (0.3064)	
training:	Epoch: [5][501/817]	Loss 0.4896 (0.3068)	
training:	Epoch: [5][502/817]	Loss 0.5182 (0.3072)	
training:	Epoch: [5][503/817]	Loss 0.1779 (0.3069)	
training:	Epoch: [5][504/817]	Loss 0.0945 (0.3065)	
training:	Epoch: [5][505/817]	Loss 0.2258 (0.3063)	
training:	Epoch: [5][506/817]	Loss 0.1056 (0.3059)	
training:	Epoch: [5][507/817]	Loss 0.1014 (0.3055)	
training:	Epoch: [5][508/817]	Loss 0.1414 (0.3052)	
training:	Epoch: [5][509/817]	Loss 0.2843 (0.3052)	
training:	Epoch: [5][510/817]	Loss 0.0613 (0.3047)	
training:	Epoch: [5][511/817]	Loss 0.1311 (0.3044)	
training:	Epoch: [5][512/817]	Loss 0.4018 (0.3045)	
training:	Epoch: [5][513/817]	Loss 0.2232 (0.3044)	
training:	Epoch: [5][514/817]	Loss 0.0946 (0.3040)	
training:	Epoch: [5][515/817]	Loss 0.1697 (0.3037)	
training:	Epoch: [5][516/817]	Loss 0.2972 (0.3037)	
training:	Epoch: [5][517/817]	Loss 0.3828 (0.3039)	
training:	Epoch: [5][518/817]	Loss 0.6623 (0.3045)	
training:	Epoch: [5][519/817]	Loss 0.1072 (0.3042)	
training:	Epoch: [5][520/817]	Loss 0.1715 (0.3039)	
training:	Epoch: [5][521/817]	Loss 0.0587 (0.3034)	
training:	Epoch: [5][522/817]	Loss 0.5681 (0.3040)	
training:	Epoch: [5][523/817]	Loss 0.1535 (0.3037)	
training:	Epoch: [5][524/817]	Loss 0.0679 (0.3032)	
training:	Epoch: [5][525/817]	Loss 0.3882 (0.3034)	
training:	Epoch: [5][526/817]	Loss 0.2597 (0.3033)	
training:	Epoch: [5][527/817]	Loss 0.0703 (0.3028)	
training:	Epoch: [5][528/817]	Loss 0.4012 (0.3030)	
training:	Epoch: [5][529/817]	Loss 0.2747 (0.3030)	
training:	Epoch: [5][530/817]	Loss 0.5574 (0.3035)	
training:	Epoch: [5][531/817]	Loss 0.0946 (0.3031)	
training:	Epoch: [5][532/817]	Loss 0.2763 (0.3030)	
training:	Epoch: [5][533/817]	Loss 0.1526 (0.3027)	
training:	Epoch: [5][534/817]	Loss 0.4805 (0.3031)	
training:	Epoch: [5][535/817]	Loss 0.5728 (0.3036)	
training:	Epoch: [5][536/817]	Loss 0.0670 (0.3031)	
training:	Epoch: [5][537/817]	Loss 0.2043 (0.3029)	
training:	Epoch: [5][538/817]	Loss 0.5512 (0.3034)	
training:	Epoch: [5][539/817]	Loss 0.0648 (0.3030)	
training:	Epoch: [5][540/817]	Loss 0.3984 (0.3031)	
training:	Epoch: [5][541/817]	Loss 0.5181 (0.3035)	
training:	Epoch: [5][542/817]	Loss 0.2360 (0.3034)	
training:	Epoch: [5][543/817]	Loss 0.1457 (0.3031)	
training:	Epoch: [5][544/817]	Loss 0.9396 (0.3043)	
training:	Epoch: [5][545/817]	Loss 0.3769 (0.3044)	
training:	Epoch: [5][546/817]	Loss 0.1620 (0.3042)	
training:	Epoch: [5][547/817]	Loss 0.5086 (0.3045)	
training:	Epoch: [5][548/817]	Loss 0.5768 (0.3050)	
training:	Epoch: [5][549/817]	Loss 0.0682 (0.3046)	
training:	Epoch: [5][550/817]	Loss 0.2298 (0.3045)	
training:	Epoch: [5][551/817]	Loss 0.4651 (0.3048)	
training:	Epoch: [5][552/817]	Loss 0.3769 (0.3049)	
training:	Epoch: [5][553/817]	Loss 0.2094 (0.3047)	
training:	Epoch: [5][554/817]	Loss 0.2969 (0.3047)	
training:	Epoch: [5][555/817]	Loss 0.1144 (0.3044)	
training:	Epoch: [5][556/817]	Loss 0.2327 (0.3042)	
training:	Epoch: [5][557/817]	Loss 0.2688 (0.3042)	
training:	Epoch: [5][558/817]	Loss 0.6235 (0.3047)	
training:	Epoch: [5][559/817]	Loss 0.2573 (0.3047)	
training:	Epoch: [5][560/817]	Loss 0.2275 (0.3045)	
training:	Epoch: [5][561/817]	Loss 0.2285 (0.3044)	
training:	Epoch: [5][562/817]	Loss 0.4218 (0.3046)	
training:	Epoch: [5][563/817]	Loss 0.1376 (0.3043)	
training:	Epoch: [5][564/817]	Loss 0.3507 (0.3044)	
training:	Epoch: [5][565/817]	Loss 0.4432 (0.3046)	
training:	Epoch: [5][566/817]	Loss 0.4133 (0.3048)	
training:	Epoch: [5][567/817]	Loss 0.3588 (0.3049)	
training:	Epoch: [5][568/817]	Loss 0.1219 (0.3046)	
training:	Epoch: [5][569/817]	Loss 0.2391 (0.3045)	
training:	Epoch: [5][570/817]	Loss 0.7091 (0.3052)	
training:	Epoch: [5][571/817]	Loss 0.3478 (0.3053)	
training:	Epoch: [5][572/817]	Loss 0.1330 (0.3050)	
training:	Epoch: [5][573/817]	Loss 0.0844 (0.3046)	
training:	Epoch: [5][574/817]	Loss 0.1639 (0.3043)	
training:	Epoch: [5][575/817]	Loss 0.4231 (0.3045)	
training:	Epoch: [5][576/817]	Loss 0.1596 (0.3043)	
training:	Epoch: [5][577/817]	Loss 0.4324 (0.3045)	
training:	Epoch: [5][578/817]	Loss 0.2093 (0.3043)	
training:	Epoch: [5][579/817]	Loss 0.2433 (0.3042)	
training:	Epoch: [5][580/817]	Loss 0.6143 (0.3048)	
training:	Epoch: [5][581/817]	Loss 0.5184 (0.3051)	
training:	Epoch: [5][582/817]	Loss 0.3345 (0.3052)	
training:	Epoch: [5][583/817]	Loss 0.3217 (0.3052)	
training:	Epoch: [5][584/817]	Loss 0.0623 (0.3048)	
training:	Epoch: [5][585/817]	Loss 0.5671 (0.3052)	
training:	Epoch: [5][586/817]	Loss 0.1847 (0.3050)	
training:	Epoch: [5][587/817]	Loss 0.0871 (0.3047)	
training:	Epoch: [5][588/817]	Loss 0.2426 (0.3046)	
training:	Epoch: [5][589/817]	Loss 0.0913 (0.3042)	
training:	Epoch: [5][590/817]	Loss 0.4830 (0.3045)	
training:	Epoch: [5][591/817]	Loss 0.3679 (0.3046)	
training:	Epoch: [5][592/817]	Loss 0.4893 (0.3049)	
training:	Epoch: [5][593/817]	Loss 0.9336 (0.3060)	
training:	Epoch: [5][594/817]	Loss 0.4930 (0.3063)	
training:	Epoch: [5][595/817]	Loss 0.4077 (0.3065)	
training:	Epoch: [5][596/817]	Loss 0.1531 (0.3062)	
training:	Epoch: [5][597/817]	Loss 0.3860 (0.3063)	
training:	Epoch: [5][598/817]	Loss 0.1425 (0.3061)	
training:	Epoch: [5][599/817]	Loss 0.1127 (0.3058)	
training:	Epoch: [5][600/817]	Loss 0.2711 (0.3057)	
training:	Epoch: [5][601/817]	Loss 0.1107 (0.3054)	
training:	Epoch: [5][602/817]	Loss 0.2224 (0.3052)	
training:	Epoch: [5][603/817]	Loss 0.4939 (0.3055)	
training:	Epoch: [5][604/817]	Loss 0.3089 (0.3056)	
training:	Epoch: [5][605/817]	Loss 0.3638 (0.3056)	
training:	Epoch: [5][606/817]	Loss 0.1342 (0.3054)	
training:	Epoch: [5][607/817]	Loss 0.5108 (0.3057)	
training:	Epoch: [5][608/817]	Loss 0.4479 (0.3059)	
training:	Epoch: [5][609/817]	Loss 0.4720 (0.3062)	
training:	Epoch: [5][610/817]	Loss 0.5261 (0.3066)	
training:	Epoch: [5][611/817]	Loss 0.2996 (0.3066)	
training:	Epoch: [5][612/817]	Loss 0.3255 (0.3066)	
training:	Epoch: [5][613/817]	Loss 0.3297 (0.3066)	
training:	Epoch: [5][614/817]	Loss 0.8869 (0.3076)	
training:	Epoch: [5][615/817]	Loss 0.8655 (0.3085)	
training:	Epoch: [5][616/817]	Loss 0.2489 (0.3084)	
training:	Epoch: [5][617/817]	Loss 0.2013 (0.3082)	
training:	Epoch: [5][618/817]	Loss 0.5356 (0.3086)	
training:	Epoch: [5][619/817]	Loss 0.1196 (0.3083)	
training:	Epoch: [5][620/817]	Loss 0.4324 (0.3085)	
training:	Epoch: [5][621/817]	Loss 0.1119 (0.3082)	
training:	Epoch: [5][622/817]	Loss 0.2175 (0.3080)	
training:	Epoch: [5][623/817]	Loss 0.1981 (0.3078)	
training:	Epoch: [5][624/817]	Loss 0.2532 (0.3077)	
training:	Epoch: [5][625/817]	Loss 0.4106 (0.3079)	
training:	Epoch: [5][626/817]	Loss 0.1844 (0.3077)	
training:	Epoch: [5][627/817]	Loss 0.5008 (0.3080)	
training:	Epoch: [5][628/817]	Loss 0.1598 (0.3078)	
training:	Epoch: [5][629/817]	Loss 0.1991 (0.3076)	
training:	Epoch: [5][630/817]	Loss 0.3076 (0.3076)	
training:	Epoch: [5][631/817]	Loss 0.1707 (0.3074)	
training:	Epoch: [5][632/817]	Loss 0.1344 (0.3071)	
training:	Epoch: [5][633/817]	Loss 0.3472 (0.3072)	
training:	Epoch: [5][634/817]	Loss 0.1813 (0.3070)	
training:	Epoch: [5][635/817]	Loss 0.3315 (0.3070)	
training:	Epoch: [5][636/817]	Loss 0.2662 (0.3070)	
training:	Epoch: [5][637/817]	Loss 0.7524 (0.3077)	
training:	Epoch: [5][638/817]	Loss 0.3249 (0.3077)	
training:	Epoch: [5][639/817]	Loss 0.3940 (0.3078)	
training:	Epoch: [5][640/817]	Loss 0.2622 (0.3078)	
training:	Epoch: [5][641/817]	Loss 0.1570 (0.3075)	
training:	Epoch: [5][642/817]	Loss 0.2791 (0.3075)	
training:	Epoch: [5][643/817]	Loss 0.0980 (0.3071)	
training:	Epoch: [5][644/817]	Loss 0.5444 (0.3075)	
training:	Epoch: [5][645/817]	Loss 0.1046 (0.3072)	
training:	Epoch: [5][646/817]	Loss 0.6111 (0.3077)	
training:	Epoch: [5][647/817]	Loss 0.2621 (0.3076)	
training:	Epoch: [5][648/817]	Loss 0.3156 (0.3076)	
training:	Epoch: [5][649/817]	Loss 0.0785 (0.3073)	
training:	Epoch: [5][650/817]	Loss 0.3478 (0.3073)	
training:	Epoch: [5][651/817]	Loss 0.6074 (0.3078)	
training:	Epoch: [5][652/817]	Loss 0.3090 (0.3078)	
training:	Epoch: [5][653/817]	Loss 0.2181 (0.3076)	
training:	Epoch: [5][654/817]	Loss 0.4909 (0.3079)	
training:	Epoch: [5][655/817]	Loss 0.1623 (0.3077)	
training:	Epoch: [5][656/817]	Loss 0.3086 (0.3077)	
training:	Epoch: [5][657/817]	Loss 0.3364 (0.3077)	
training:	Epoch: [5][658/817]	Loss 0.1559 (0.3075)	
training:	Epoch: [5][659/817]	Loss 0.4455 (0.3077)	
training:	Epoch: [5][660/817]	Loss 0.1195 (0.3074)	
training:	Epoch: [5][661/817]	Loss 0.5491 (0.3078)	
training:	Epoch: [5][662/817]	Loss 0.3654 (0.3079)	
training:	Epoch: [5][663/817]	Loss 0.1427 (0.3076)	
training:	Epoch: [5][664/817]	Loss 0.1170 (0.3074)	
training:	Epoch: [5][665/817]	Loss 0.5604 (0.3077)	
training:	Epoch: [5][666/817]	Loss 0.1765 (0.3075)	
training:	Epoch: [5][667/817]	Loss 0.3157 (0.3076)	
training:	Epoch: [5][668/817]	Loss 0.1129 (0.3073)	
training:	Epoch: [5][669/817]	Loss 0.3477 (0.3073)	
training:	Epoch: [5][670/817]	Loss 0.1249 (0.3071)	
training:	Epoch: [5][671/817]	Loss 0.1294 (0.3068)	
training:	Epoch: [5][672/817]	Loss 0.3707 (0.3069)	
training:	Epoch: [5][673/817]	Loss 0.1960 (0.3067)	
training:	Epoch: [5][674/817]	Loss 0.2821 (0.3067)	
training:	Epoch: [5][675/817]	Loss 0.3745 (0.3068)	
training:	Epoch: [5][676/817]	Loss 0.4269 (0.3070)	
training:	Epoch: [5][677/817]	Loss 0.5297 (0.3073)	
training:	Epoch: [5][678/817]	Loss 0.1607 (0.3071)	
training:	Epoch: [5][679/817]	Loss 0.2817 (0.3070)	
training:	Epoch: [5][680/817]	Loss 0.1169 (0.3068)	
training:	Epoch: [5][681/817]	Loss 0.2540 (0.3067)	
training:	Epoch: [5][682/817]	Loss 0.7391 (0.3073)	
training:	Epoch: [5][683/817]	Loss 0.4025 (0.3075)	
training:	Epoch: [5][684/817]	Loss 0.3824 (0.3076)	
training:	Epoch: [5][685/817]	Loss 0.2282 (0.3074)	
training:	Epoch: [5][686/817]	Loss 0.3907 (0.3076)	
training:	Epoch: [5][687/817]	Loss 0.4507 (0.3078)	
training:	Epoch: [5][688/817]	Loss 0.2713 (0.3077)	
training:	Epoch: [5][689/817]	Loss 0.1262 (0.3075)	
training:	Epoch: [5][690/817]	Loss 0.0889 (0.3071)	
training:	Epoch: [5][691/817]	Loss 0.3849 (0.3073)	
training:	Epoch: [5][692/817]	Loss 0.4581 (0.3075)	
training:	Epoch: [5][693/817]	Loss 0.1795 (0.3073)	
training:	Epoch: [5][694/817]	Loss 0.1979 (0.3071)	
training:	Epoch: [5][695/817]	Loss 0.1393 (0.3069)	
training:	Epoch: [5][696/817]	Loss 0.4339 (0.3071)	
training:	Epoch: [5][697/817]	Loss 0.2653 (0.3070)	
training:	Epoch: [5][698/817]	Loss 0.3441 (0.3071)	
training:	Epoch: [5][699/817]	Loss 0.4684 (0.3073)	
training:	Epoch: [5][700/817]	Loss 0.1403 (0.3071)	
training:	Epoch: [5][701/817]	Loss 0.3427 (0.3071)	
training:	Epoch: [5][702/817]	Loss 0.3116 (0.3071)	
training:	Epoch: [5][703/817]	Loss 0.1928 (0.3069)	
training:	Epoch: [5][704/817]	Loss 0.2446 (0.3069)	
training:	Epoch: [5][705/817]	Loss 0.2623 (0.3068)	
training:	Epoch: [5][706/817]	Loss 0.5487 (0.3071)	
training:	Epoch: [5][707/817]	Loss 0.3120 (0.3071)	
training:	Epoch: [5][708/817]	Loss 0.1220 (0.3069)	
training:	Epoch: [5][709/817]	Loss 0.1506 (0.3067)	
training:	Epoch: [5][710/817]	Loss 0.1835 (0.3065)	
training:	Epoch: [5][711/817]	Loss 0.1954 (0.3063)	
training:	Epoch: [5][712/817]	Loss 0.3196 (0.3064)	
training:	Epoch: [5][713/817]	Loss 0.3363 (0.3064)	
training:	Epoch: [5][714/817]	Loss 0.1739 (0.3062)	
training:	Epoch: [5][715/817]	Loss 0.5605 (0.3066)	
training:	Epoch: [5][716/817]	Loss 0.1381 (0.3063)	
training:	Epoch: [5][717/817]	Loss 0.4714 (0.3066)	
training:	Epoch: [5][718/817]	Loss 0.1159 (0.3063)	
training:	Epoch: [5][719/817]	Loss 0.4508 (0.3065)	
training:	Epoch: [5][720/817]	Loss 0.1194 (0.3062)	
training:	Epoch: [5][721/817]	Loss 0.2504 (0.3062)	
training:	Epoch: [5][722/817]	Loss 0.0799 (0.3058)	
training:	Epoch: [5][723/817]	Loss 0.3371 (0.3059)	
training:	Epoch: [5][724/817]	Loss 0.4733 (0.3061)	
training:	Epoch: [5][725/817]	Loss 0.1417 (0.3059)	
training:	Epoch: [5][726/817]	Loss 0.1336 (0.3057)	
training:	Epoch: [5][727/817]	Loss 0.2857 (0.3056)	
training:	Epoch: [5][728/817]	Loss 0.5766 (0.3060)	
training:	Epoch: [5][729/817]	Loss 0.1843 (0.3058)	
training:	Epoch: [5][730/817]	Loss 0.3344 (0.3059)	
training:	Epoch: [5][731/817]	Loss 0.6607 (0.3064)	
training:	Epoch: [5][732/817]	Loss 0.1381 (0.3061)	
training:	Epoch: [5][733/817]	Loss 0.0816 (0.3058)	
training:	Epoch: [5][734/817]	Loss 0.6844 (0.3063)	
training:	Epoch: [5][735/817]	Loss 0.2103 (0.3062)	
training:	Epoch: [5][736/817]	Loss 0.5170 (0.3065)	
training:	Epoch: [5][737/817]	Loss 0.1697 (0.3063)	
training:	Epoch: [5][738/817]	Loss 0.5230 (0.3066)	
training:	Epoch: [5][739/817]	Loss 0.0767 (0.3063)	
training:	Epoch: [5][740/817]	Loss 0.2307 (0.3062)	
training:	Epoch: [5][741/817]	Loss 0.4127 (0.3063)	
training:	Epoch: [5][742/817]	Loss 0.1931 (0.3062)	
training:	Epoch: [5][743/817]	Loss 0.2285 (0.3061)	
training:	Epoch: [5][744/817]	Loss 0.1649 (0.3059)	
training:	Epoch: [5][745/817]	Loss 0.1731 (0.3057)	
training:	Epoch: [5][746/817]	Loss 0.3554 (0.3058)	
training:	Epoch: [5][747/817]	Loss 0.2978 (0.3058)	
training:	Epoch: [5][748/817]	Loss 0.2997 (0.3058)	
training:	Epoch: [5][749/817]	Loss 0.4268 (0.3059)	
training:	Epoch: [5][750/817]	Loss 0.5106 (0.3062)	
training:	Epoch: [5][751/817]	Loss 0.3074 (0.3062)	
training:	Epoch: [5][752/817]	Loss 0.0833 (0.3059)	
training:	Epoch: [5][753/817]	Loss 0.7664 (0.3065)	
training:	Epoch: [5][754/817]	Loss 0.2193 (0.3064)	
training:	Epoch: [5][755/817]	Loss 0.6697 (0.3069)	
training:	Epoch: [5][756/817]	Loss 0.1367 (0.3066)	
training:	Epoch: [5][757/817]	Loss 0.2349 (0.3066)	
training:	Epoch: [5][758/817]	Loss 0.0961 (0.3063)	
training:	Epoch: [5][759/817]	Loss 0.5593 (0.3066)	
training:	Epoch: [5][760/817]	Loss 0.1218 (0.3064)	
training:	Epoch: [5][761/817]	Loss 0.0986 (0.3061)	
training:	Epoch: [5][762/817]	Loss 0.3677 (0.3062)	
training:	Epoch: [5][763/817]	Loss 0.6825 (0.3067)	
training:	Epoch: [5][764/817]	Loss 0.5354 (0.3070)	
training:	Epoch: [5][765/817]	Loss 0.3253 (0.3070)	
training:	Epoch: [5][766/817]	Loss 0.1709 (0.3068)	
training:	Epoch: [5][767/817]	Loss 0.2874 (0.3068)	
training:	Epoch: [5][768/817]	Loss 0.0786 (0.3065)	
training:	Epoch: [5][769/817]	Loss 0.3140 (0.3065)	
training:	Epoch: [5][770/817]	Loss 0.5260 (0.3068)	
training:	Epoch: [5][771/817]	Loss 0.4226 (0.3069)	
training:	Epoch: [5][772/817]	Loss 0.3278 (0.3070)	
training:	Epoch: [5][773/817]	Loss 0.0866 (0.3067)	
training:	Epoch: [5][774/817]	Loss 0.0518 (0.3063)	
training:	Epoch: [5][775/817]	Loss 0.6359 (0.3068)	
training:	Epoch: [5][776/817]	Loss 0.2605 (0.3067)	
training:	Epoch: [5][777/817]	Loss 0.5961 (0.3071)	
training:	Epoch: [5][778/817]	Loss 0.1843 (0.3069)	
training:	Epoch: [5][779/817]	Loss 0.1382 (0.3067)	
training:	Epoch: [5][780/817]	Loss 0.1635 (0.3065)	
training:	Epoch: [5][781/817]	Loss 0.0896 (0.3062)	
training:	Epoch: [5][782/817]	Loss 0.3979 (0.3064)	
training:	Epoch: [5][783/817]	Loss 0.1702 (0.3062)	
training:	Epoch: [5][784/817]	Loss 0.2210 (0.3061)	
training:	Epoch: [5][785/817]	Loss 0.4932 (0.3063)	
training:	Epoch: [5][786/817]	Loss 0.2990 (0.3063)	
training:	Epoch: [5][787/817]	Loss 0.7203 (0.3068)	
training:	Epoch: [5][788/817]	Loss 0.7524 (0.3074)	
training:	Epoch: [5][789/817]	Loss 0.5037 (0.3077)	
training:	Epoch: [5][790/817]	Loss 0.3347 (0.3077)	
training:	Epoch: [5][791/817]	Loss 0.0638 (0.3074)	
training:	Epoch: [5][792/817]	Loss 0.4445 (0.3076)	
training:	Epoch: [5][793/817]	Loss 0.2223 (0.3074)	
training:	Epoch: [5][794/817]	Loss 0.2022 (0.3073)	
training:	Epoch: [5][795/817]	Loss 0.1686 (0.3071)	
training:	Epoch: [5][796/817]	Loss 0.1520 (0.3069)	
training:	Epoch: [5][797/817]	Loss 0.3556 (0.3070)	
training:	Epoch: [5][798/817]	Loss 0.0698 (0.3067)	
training:	Epoch: [5][799/817]	Loss 0.2305 (0.3066)	
training:	Epoch: [5][800/817]	Loss 0.5244 (0.3069)	
training:	Epoch: [5][801/817]	Loss 0.1292 (0.3067)	
training:	Epoch: [5][802/817]	Loss 0.0456 (0.3063)	
training:	Epoch: [5][803/817]	Loss 0.2514 (0.3063)	
training:	Epoch: [5][804/817]	Loss 0.0870 (0.3060)	
training:	Epoch: [5][805/817]	Loss 0.1029 (0.3057)	
training:	Epoch: [5][806/817]	Loss 0.5006 (0.3060)	
training:	Epoch: [5][807/817]	Loss 0.2228 (0.3059)	
training:	Epoch: [5][808/817]	Loss 0.5861 (0.3062)	
training:	Epoch: [5][809/817]	Loss 0.4744 (0.3064)	
training:	Epoch: [5][810/817]	Loss 0.0678 (0.3061)	
training:	Epoch: [5][811/817]	Loss 0.3102 (0.3061)	
training:	Epoch: [5][812/817]	Loss 0.2313 (0.3061)	
training:	Epoch: [5][813/817]	Loss 0.3741 (0.3061)	
training:	Epoch: [5][814/817]	Loss 0.0973 (0.3059)	
training:	Epoch: [5][815/817]	Loss 0.1112 (0.3056)	
training:	Epoch: [5][816/817]	Loss 0.1515 (0.3055)	
training:	Epoch: [5][817/817]	Loss 0.0798 (0.3052)	
Training:	 Loss: 0.3051

Training:	 ACC: 0.9044 0.9050 0.9203 0.8884
Validation:	 ACC: 0.8179 0.8192 0.8465 0.7892
Validation:	 Best_BACC: 0.8223 0.8218 0.8127 0.8318
Validation:	 Loss: 0.4127
Pretraining:	Epoch 6/200
----------
training:	Epoch: [6][1/817]	Loss 0.1505 (0.1505)	
training:	Epoch: [6][2/817]	Loss 0.2494 (0.2000)	
training:	Epoch: [6][3/817]	Loss 0.1047 (0.1682)	
training:	Epoch: [6][4/817]	Loss 0.0886 (0.1483)	
training:	Epoch: [6][5/817]	Loss 0.3134 (0.1813)	
training:	Epoch: [6][6/817]	Loss 0.7206 (0.2712)	
training:	Epoch: [6][7/817]	Loss 0.1336 (0.2515)	
training:	Epoch: [6][8/817]	Loss 0.0987 (0.2324)	
training:	Epoch: [6][9/817]	Loss 0.3202 (0.2422)	
training:	Epoch: [6][10/817]	Loss 0.4543 (0.2634)	
training:	Epoch: [6][11/817]	Loss 0.0963 (0.2482)	
training:	Epoch: [6][12/817]	Loss 0.7567 (0.2906)	
training:	Epoch: [6][13/817]	Loss 0.1132 (0.2769)	
training:	Epoch: [6][14/817]	Loss 0.4766 (0.2912)	
training:	Epoch: [6][15/817]	Loss 0.0556 (0.2755)	
training:	Epoch: [6][16/817]	Loss 0.5047 (0.2898)	
training:	Epoch: [6][17/817]	Loss 0.3228 (0.2918)	
training:	Epoch: [6][18/817]	Loss 0.2019 (0.2868)	
training:	Epoch: [6][19/817]	Loss 0.2142 (0.2829)	
training:	Epoch: [6][20/817]	Loss 0.2359 (0.2806)	
training:	Epoch: [6][21/817]	Loss 0.3493 (0.2839)	
training:	Epoch: [6][22/817]	Loss 0.0542 (0.2734)	
training:	Epoch: [6][23/817]	Loss 0.1573 (0.2684)	
training:	Epoch: [6][24/817]	Loss 0.1664 (0.2641)	
training:	Epoch: [6][25/817]	Loss 0.1805 (0.2608)	
training:	Epoch: [6][26/817]	Loss 0.2896 (0.2619)	
training:	Epoch: [6][27/817]	Loss 0.2732 (0.2623)	
training:	Epoch: [6][28/817]	Loss 0.1962 (0.2600)	
training:	Epoch: [6][29/817]	Loss 0.4716 (0.2673)	
training:	Epoch: [6][30/817]	Loss 0.6617 (0.2804)	
training:	Epoch: [6][31/817]	Loss 0.1541 (0.2763)	
training:	Epoch: [6][32/817]	Loss 0.2417 (0.2752)	
training:	Epoch: [6][33/817]	Loss 0.2687 (0.2750)	
training:	Epoch: [6][34/817]	Loss 0.5498 (0.2831)	
training:	Epoch: [6][35/817]	Loss 0.1242 (0.2786)	
training:	Epoch: [6][36/817]	Loss 0.3288 (0.2800)	
training:	Epoch: [6][37/817]	Loss 0.0603 (0.2740)	
training:	Epoch: [6][38/817]	Loss 0.0757 (0.2688)	
training:	Epoch: [6][39/817]	Loss 0.4223 (0.2728)	
training:	Epoch: [6][40/817]	Loss 0.2509 (0.2722)	
training:	Epoch: [6][41/817]	Loss 0.4208 (0.2758)	
training:	Epoch: [6][42/817]	Loss 0.0623 (0.2708)	
training:	Epoch: [6][43/817]	Loss 0.3128 (0.2717)	
training:	Epoch: [6][44/817]	Loss 0.0777 (0.2673)	
training:	Epoch: [6][45/817]	Loss 0.5350 (0.2733)	
training:	Epoch: [6][46/817]	Loss 0.4727 (0.2776)	
training:	Epoch: [6][47/817]	Loss 0.4655 (0.2816)	
training:	Epoch: [6][48/817]	Loss 0.6853 (0.2900)	
training:	Epoch: [6][49/817]	Loss 0.0739 (0.2856)	
training:	Epoch: [6][50/817]	Loss 0.1817 (0.2835)	
training:	Epoch: [6][51/817]	Loss 0.4154 (0.2861)	
training:	Epoch: [6][52/817]	Loss 0.3035 (0.2864)	
training:	Epoch: [6][53/817]	Loss 0.2066 (0.2849)	
training:	Epoch: [6][54/817]	Loss 0.2255 (0.2838)	
training:	Epoch: [6][55/817]	Loss 0.3983 (0.2859)	
training:	Epoch: [6][56/817]	Loss 0.1959 (0.2843)	
training:	Epoch: [6][57/817]	Loss 0.1811 (0.2825)	
training:	Epoch: [6][58/817]	Loss 0.1526 (0.2803)	
training:	Epoch: [6][59/817]	Loss 0.2014 (0.2789)	
training:	Epoch: [6][60/817]	Loss 0.0690 (0.2754)	
training:	Epoch: [6][61/817]	Loss 0.0925 (0.2724)	
training:	Epoch: [6][62/817]	Loss 0.1460 (0.2704)	
training:	Epoch: [6][63/817]	Loss 0.4135 (0.2727)	
training:	Epoch: [6][64/817]	Loss 0.2855 (0.2729)	
training:	Epoch: [6][65/817]	Loss 0.1779 (0.2714)	
training:	Epoch: [6][66/817]	Loss 0.4746 (0.2745)	
training:	Epoch: [6][67/817]	Loss 0.0760 (0.2715)	
training:	Epoch: [6][68/817]	Loss 0.8977 (0.2807)	
training:	Epoch: [6][69/817]	Loss 0.1319 (0.2786)	
training:	Epoch: [6][70/817]	Loss 0.2879 (0.2787)	
training:	Epoch: [6][71/817]	Loss 0.4529 (0.2812)	
training:	Epoch: [6][72/817]	Loss 0.0893 (0.2785)	
training:	Epoch: [6][73/817]	Loss 0.5541 (0.2823)	
training:	Epoch: [6][74/817]	Loss 0.4619 (0.2847)	
training:	Epoch: [6][75/817]	Loss 0.2436 (0.2841)	
training:	Epoch: [6][76/817]	Loss 0.2717 (0.2840)	
training:	Epoch: [6][77/817]	Loss 0.8195 (0.2909)	
training:	Epoch: [6][78/817]	Loss 0.2838 (0.2908)	
training:	Epoch: [6][79/817]	Loss 0.4558 (0.2929)	
training:	Epoch: [6][80/817]	Loss 0.1348 (0.2910)	
training:	Epoch: [6][81/817]	Loss 0.2454 (0.2904)	
training:	Epoch: [6][82/817]	Loss 0.5106 (0.2931)	
training:	Epoch: [6][83/817]	Loss 0.0643 (0.2903)	
training:	Epoch: [6][84/817]	Loss 0.0490 (0.2874)	
training:	Epoch: [6][85/817]	Loss 0.0805 (0.2850)	
training:	Epoch: [6][86/817]	Loss 0.2756 (0.2849)	
training:	Epoch: [6][87/817]	Loss 0.1869 (0.2838)	
training:	Epoch: [6][88/817]	Loss 0.2098 (0.2829)	
training:	Epoch: [6][89/817]	Loss 0.1011 (0.2809)	
training:	Epoch: [6][90/817]	Loss 0.1105 (0.2790)	
training:	Epoch: [6][91/817]	Loss 0.5300 (0.2818)	
training:	Epoch: [6][92/817]	Loss 0.1537 (0.2804)	
training:	Epoch: [6][93/817]	Loss 0.1193 (0.2786)	
training:	Epoch: [6][94/817]	Loss 0.2594 (0.2784)	
training:	Epoch: [6][95/817]	Loss 0.3717 (0.2794)	
training:	Epoch: [6][96/817]	Loss 0.0821 (0.2774)	
training:	Epoch: [6][97/817]	Loss 0.2406 (0.2770)	
training:	Epoch: [6][98/817]	Loss 0.0548 (0.2747)	
training:	Epoch: [6][99/817]	Loss 0.2421 (0.2744)	
training:	Epoch: [6][100/817]	Loss 0.0611 (0.2722)	
training:	Epoch: [6][101/817]	Loss 0.3699 (0.2732)	
training:	Epoch: [6][102/817]	Loss 0.0458 (0.2710)	
training:	Epoch: [6][103/817]	Loss 0.2585 (0.2709)	
training:	Epoch: [6][104/817]	Loss 0.2412 (0.2706)	
training:	Epoch: [6][105/817]	Loss 0.2572 (0.2705)	
training:	Epoch: [6][106/817]	Loss 0.2166 (0.2699)	
training:	Epoch: [6][107/817]	Loss 0.0585 (0.2680)	
training:	Epoch: [6][108/817]	Loss 0.3594 (0.2688)	
training:	Epoch: [6][109/817]	Loss 0.1209 (0.2675)	
training:	Epoch: [6][110/817]	Loss 0.4686 (0.2693)	
training:	Epoch: [6][111/817]	Loss 0.1282 (0.2680)	
training:	Epoch: [6][112/817]	Loss 0.0751 (0.2663)	
training:	Epoch: [6][113/817]	Loss 0.5252 (0.2686)	
training:	Epoch: [6][114/817]	Loss 0.0397 (0.2666)	
training:	Epoch: [6][115/817]	Loss 0.2061 (0.2660)	
training:	Epoch: [6][116/817]	Loss 0.0798 (0.2644)	
training:	Epoch: [6][117/817]	Loss 0.2856 (0.2646)	
training:	Epoch: [6][118/817]	Loss 0.6248 (0.2677)	
training:	Epoch: [6][119/817]	Loss 0.2558 (0.2676)	
training:	Epoch: [6][120/817]	Loss 0.4410 (0.2690)	
training:	Epoch: [6][121/817]	Loss 0.8105 (0.2735)	
training:	Epoch: [6][122/817]	Loss 0.1392 (0.2724)	
training:	Epoch: [6][123/817]	Loss 0.4906 (0.2742)	
training:	Epoch: [6][124/817]	Loss 0.0995 (0.2728)	
training:	Epoch: [6][125/817]	Loss 0.0397 (0.2709)	
training:	Epoch: [6][126/817]	Loss 0.0846 (0.2694)	
training:	Epoch: [6][127/817]	Loss 0.3496 (0.2700)	
training:	Epoch: [6][128/817]	Loss 0.1223 (0.2689)	
training:	Epoch: [6][129/817]	Loss 0.1535 (0.2680)	
training:	Epoch: [6][130/817]	Loss 0.2261 (0.2677)	
training:	Epoch: [6][131/817]	Loss 0.0753 (0.2662)	
training:	Epoch: [6][132/817]	Loss 0.1892 (0.2656)	
training:	Epoch: [6][133/817]	Loss 0.0650 (0.2641)	
training:	Epoch: [6][134/817]	Loss 0.5119 (0.2660)	
training:	Epoch: [6][135/817]	Loss 0.0628 (0.2645)	
training:	Epoch: [6][136/817]	Loss 0.3271 (0.2649)	
training:	Epoch: [6][137/817]	Loss 0.3345 (0.2654)	
training:	Epoch: [6][138/817]	Loss 0.2680 (0.2654)	
training:	Epoch: [6][139/817]	Loss 0.2039 (0.2650)	
training:	Epoch: [6][140/817]	Loss 0.1834 (0.2644)	
training:	Epoch: [6][141/817]	Loss 0.2944 (0.2646)	
training:	Epoch: [6][142/817]	Loss 0.2761 (0.2647)	
training:	Epoch: [6][143/817]	Loss 0.0848 (0.2635)	
training:	Epoch: [6][144/817]	Loss 0.5383 (0.2654)	
training:	Epoch: [6][145/817]	Loss 0.1041 (0.2643)	
training:	Epoch: [6][146/817]	Loss 0.1710 (0.2636)	
training:	Epoch: [6][147/817]	Loss 0.2188 (0.2633)	
training:	Epoch: [6][148/817]	Loss 0.0724 (0.2620)	
training:	Epoch: [6][149/817]	Loss 0.5625 (0.2640)	
training:	Epoch: [6][150/817]	Loss 0.4477 (0.2653)	
training:	Epoch: [6][151/817]	Loss 0.7041 (0.2682)	
training:	Epoch: [6][152/817]	Loss 0.0800 (0.2669)	
training:	Epoch: [6][153/817]	Loss 0.4103 (0.2679)	
training:	Epoch: [6][154/817]	Loss 0.1504 (0.2671)	
training:	Epoch: [6][155/817]	Loss 0.1109 (0.2661)	
training:	Epoch: [6][156/817]	Loss 0.1486 (0.2653)	
training:	Epoch: [6][157/817]	Loss 0.0467 (0.2639)	
training:	Epoch: [6][158/817]	Loss 0.3415 (0.2644)	
training:	Epoch: [6][159/817]	Loss 0.2312 (0.2642)	
training:	Epoch: [6][160/817]	Loss 0.0486 (0.2629)	
training:	Epoch: [6][161/817]	Loss 0.2183 (0.2626)	
training:	Epoch: [6][162/817]	Loss 0.2733 (0.2627)	
training:	Epoch: [6][163/817]	Loss 0.0805 (0.2616)	
training:	Epoch: [6][164/817]	Loss 0.0987 (0.2606)	
training:	Epoch: [6][165/817]	Loss 0.0390 (0.2592)	
training:	Epoch: [6][166/817]	Loss 0.0932 (0.2582)	
training:	Epoch: [6][167/817]	Loss 0.0398 (0.2569)	
training:	Epoch: [6][168/817]	Loss 0.1698 (0.2564)	
training:	Epoch: [6][169/817]	Loss 0.3732 (0.2571)	
training:	Epoch: [6][170/817]	Loss 0.3411 (0.2576)	
training:	Epoch: [6][171/817]	Loss 0.0952 (0.2566)	
training:	Epoch: [6][172/817]	Loss 0.2689 (0.2567)	
training:	Epoch: [6][173/817]	Loss 0.1804 (0.2563)	
training:	Epoch: [6][174/817]	Loss 0.2243 (0.2561)	
training:	Epoch: [6][175/817]	Loss 0.3859 (0.2568)	
training:	Epoch: [6][176/817]	Loss 0.7274 (0.2595)	
training:	Epoch: [6][177/817]	Loss 0.1521 (0.2589)	
training:	Epoch: [6][178/817]	Loss 0.1594 (0.2583)	
training:	Epoch: [6][179/817]	Loss 0.3841 (0.2590)	
training:	Epoch: [6][180/817]	Loss 0.0313 (0.2578)	
training:	Epoch: [6][181/817]	Loss 0.0562 (0.2566)	
training:	Epoch: [6][182/817]	Loss 0.2243 (0.2565)	
training:	Epoch: [6][183/817]	Loss 0.1547 (0.2559)	
training:	Epoch: [6][184/817]	Loss 0.2160 (0.2557)	
training:	Epoch: [6][185/817]	Loss 0.3160 (0.2560)	
training:	Epoch: [6][186/817]	Loss 0.2042 (0.2557)	
training:	Epoch: [6][187/817]	Loss 0.0701 (0.2548)	
training:	Epoch: [6][188/817]	Loss 0.2382 (0.2547)	
training:	Epoch: [6][189/817]	Loss 0.1293 (0.2540)	
training:	Epoch: [6][190/817]	Loss 0.2326 (0.2539)	
training:	Epoch: [6][191/817]	Loss 0.0893 (0.2530)	
training:	Epoch: [6][192/817]	Loss 0.1339 (0.2524)	
training:	Epoch: [6][193/817]	Loss 0.4584 (0.2535)	
training:	Epoch: [6][194/817]	Loss 0.2863 (0.2536)	
training:	Epoch: [6][195/817]	Loss 0.1251 (0.2530)	
training:	Epoch: [6][196/817]	Loss 0.4054 (0.2538)	
training:	Epoch: [6][197/817]	Loss 0.1051 (0.2530)	
training:	Epoch: [6][198/817]	Loss 0.6618 (0.2551)	
training:	Epoch: [6][199/817]	Loss 0.1117 (0.2544)	
training:	Epoch: [6][200/817]	Loss 0.0759 (0.2535)	
training:	Epoch: [6][201/817]	Loss 0.9777 (0.2571)	
training:	Epoch: [6][202/817]	Loss 1.0996 (0.2612)	
training:	Epoch: [6][203/817]	Loss 0.0493 (0.2602)	
training:	Epoch: [6][204/817]	Loss 0.1026 (0.2594)	
training:	Epoch: [6][205/817]	Loss 0.1409 (0.2588)	
training:	Epoch: [6][206/817]	Loss 0.2068 (0.2586)	
training:	Epoch: [6][207/817]	Loss 0.2304 (0.2584)	
training:	Epoch: [6][208/817]	Loss 0.1700 (0.2580)	
training:	Epoch: [6][209/817]	Loss 0.2342 (0.2579)	
training:	Epoch: [6][210/817]	Loss 0.0702 (0.2570)	
training:	Epoch: [6][211/817]	Loss 0.1251 (0.2564)	
training:	Epoch: [6][212/817]	Loss 0.6037 (0.2580)	
training:	Epoch: [6][213/817]	Loss 0.0755 (0.2572)	
training:	Epoch: [6][214/817]	Loss 0.0353 (0.2561)	
training:	Epoch: [6][215/817]	Loss 0.2390 (0.2561)	
training:	Epoch: [6][216/817]	Loss 0.4559 (0.2570)	
training:	Epoch: [6][217/817]	Loss 0.1505 (0.2565)	
training:	Epoch: [6][218/817]	Loss 0.2843 (0.2566)	
training:	Epoch: [6][219/817]	Loss 0.2250 (0.2565)	
training:	Epoch: [6][220/817]	Loss 0.1488 (0.2560)	
training:	Epoch: [6][221/817]	Loss 0.0771 (0.2552)	
training:	Epoch: [6][222/817]	Loss 0.3890 (0.2558)	
training:	Epoch: [6][223/817]	Loss 0.1360 (0.2552)	
training:	Epoch: [6][224/817]	Loss 0.4407 (0.2561)	
training:	Epoch: [6][225/817]	Loss 0.3332 (0.2564)	
training:	Epoch: [6][226/817]	Loss 0.1619 (0.2560)	
training:	Epoch: [6][227/817]	Loss 0.3638 (0.2565)	
training:	Epoch: [6][228/817]	Loss 0.1560 (0.2560)	
training:	Epoch: [6][229/817]	Loss 0.0425 (0.2551)	
training:	Epoch: [6][230/817]	Loss 0.0990 (0.2544)	
training:	Epoch: [6][231/817]	Loss 0.4138 (0.2551)	
training:	Epoch: [6][232/817]	Loss 0.4350 (0.2559)	
training:	Epoch: [6][233/817]	Loss 0.1699 (0.2555)	
training:	Epoch: [6][234/817]	Loss 0.1048 (0.2549)	
training:	Epoch: [6][235/817]	Loss 0.3133 (0.2551)	
training:	Epoch: [6][236/817]	Loss 0.1180 (0.2545)	
training:	Epoch: [6][237/817]	Loss 0.3212 (0.2548)	
training:	Epoch: [6][238/817]	Loss 0.8339 (0.2573)	
training:	Epoch: [6][239/817]	Loss 0.6142 (0.2587)	
training:	Epoch: [6][240/817]	Loss 0.1973 (0.2585)	
training:	Epoch: [6][241/817]	Loss 0.4856 (0.2594)	
training:	Epoch: [6][242/817]	Loss 0.0578 (0.2586)	
training:	Epoch: [6][243/817]	Loss 0.1401 (0.2581)	
training:	Epoch: [6][244/817]	Loss 0.5967 (0.2595)	
training:	Epoch: [6][245/817]	Loss 0.1521 (0.2591)	
training:	Epoch: [6][246/817]	Loss 0.2873 (0.2592)	
training:	Epoch: [6][247/817]	Loss 0.1412 (0.2587)	
training:	Epoch: [6][248/817]	Loss 0.1659 (0.2583)	
training:	Epoch: [6][249/817]	Loss 0.1819 (0.2580)	
training:	Epoch: [6][250/817]	Loss 0.0589 (0.2572)	
training:	Epoch: [6][251/817]	Loss 0.7162 (0.2590)	
training:	Epoch: [6][252/817]	Loss 0.3381 (0.2594)	
training:	Epoch: [6][253/817]	Loss 0.0830 (0.2587)	
training:	Epoch: [6][254/817]	Loss 0.6007 (0.2600)	
training:	Epoch: [6][255/817]	Loss 0.2624 (0.2600)	
training:	Epoch: [6][256/817]	Loss 0.1847 (0.2597)	
training:	Epoch: [6][257/817]	Loss 0.1788 (0.2594)	
training:	Epoch: [6][258/817]	Loss 0.0944 (0.2588)	
training:	Epoch: [6][259/817]	Loss 0.1495 (0.2583)	
training:	Epoch: [6][260/817]	Loss 0.4429 (0.2591)	
training:	Epoch: [6][261/817]	Loss 0.9090 (0.2615)	
training:	Epoch: [6][262/817]	Loss 0.3281 (0.2618)	
training:	Epoch: [6][263/817]	Loss 0.0873 (0.2611)	
training:	Epoch: [6][264/817]	Loss 0.3399 (0.2614)	
training:	Epoch: [6][265/817]	Loss 0.5235 (0.2624)	
training:	Epoch: [6][266/817]	Loss 0.4063 (0.2630)	
training:	Epoch: [6][267/817]	Loss 0.3533 (0.2633)	
training:	Epoch: [6][268/817]	Loss 0.0644 (0.2626)	
training:	Epoch: [6][269/817]	Loss 0.2656 (0.2626)	
training:	Epoch: [6][270/817]	Loss 0.1671 (0.2622)	
training:	Epoch: [6][271/817]	Loss 0.2776 (0.2623)	
training:	Epoch: [6][272/817]	Loss 0.3192 (0.2625)	
training:	Epoch: [6][273/817]	Loss 0.0658 (0.2618)	
training:	Epoch: [6][274/817]	Loss 0.3479 (0.2621)	
training:	Epoch: [6][275/817]	Loss 0.3652 (0.2625)	
training:	Epoch: [6][276/817]	Loss 0.5051 (0.2633)	
training:	Epoch: [6][277/817]	Loss 0.2671 (0.2633)	
training:	Epoch: [6][278/817]	Loss 0.2471 (0.2633)	
training:	Epoch: [6][279/817]	Loss 0.4684 (0.2640)	
training:	Epoch: [6][280/817]	Loss 0.5092 (0.2649)	
training:	Epoch: [6][281/817]	Loss 0.4315 (0.2655)	
training:	Epoch: [6][282/817]	Loss 0.0556 (0.2647)	
training:	Epoch: [6][283/817]	Loss 0.2645 (0.2647)	
training:	Epoch: [6][284/817]	Loss 0.4131 (0.2653)	
training:	Epoch: [6][285/817]	Loss 0.0605 (0.2646)	
training:	Epoch: [6][286/817]	Loss 0.1658 (0.2642)	
training:	Epoch: [6][287/817]	Loss 0.0688 (0.2635)	
training:	Epoch: [6][288/817]	Loss 0.6426 (0.2648)	
training:	Epoch: [6][289/817]	Loss 0.3302 (0.2651)	
training:	Epoch: [6][290/817]	Loss 0.4207 (0.2656)	
training:	Epoch: [6][291/817]	Loss 0.0510 (0.2649)	
training:	Epoch: [6][292/817]	Loss 0.2387 (0.2648)	
training:	Epoch: [6][293/817]	Loss 0.3074 (0.2649)	
training:	Epoch: [6][294/817]	Loss 0.1941 (0.2647)	
training:	Epoch: [6][295/817]	Loss 0.1750 (0.2644)	
training:	Epoch: [6][296/817]	Loss 0.0856 (0.2638)	
training:	Epoch: [6][297/817]	Loss 0.1512 (0.2634)	
training:	Epoch: [6][298/817]	Loss 0.1070 (0.2629)	
training:	Epoch: [6][299/817]	Loss 0.1351 (0.2624)	
training:	Epoch: [6][300/817]	Loss 0.1408 (0.2620)	
training:	Epoch: [6][301/817]	Loss 0.2616 (0.2620)	
training:	Epoch: [6][302/817]	Loss 0.1766 (0.2618)	
training:	Epoch: [6][303/817]	Loss 0.6139 (0.2629)	
training:	Epoch: [6][304/817]	Loss 0.2214 (0.2628)	
training:	Epoch: [6][305/817]	Loss 0.2157 (0.2626)	
training:	Epoch: [6][306/817]	Loss 0.0670 (0.2620)	
training:	Epoch: [6][307/817]	Loss 0.0923 (0.2614)	
training:	Epoch: [6][308/817]	Loss 0.0683 (0.2608)	
training:	Epoch: [6][309/817]	Loss 0.2034 (0.2606)	
training:	Epoch: [6][310/817]	Loss 0.0547 (0.2600)	
training:	Epoch: [6][311/817]	Loss 0.1488 (0.2596)	
training:	Epoch: [6][312/817]	Loss 0.4339 (0.2602)	
training:	Epoch: [6][313/817]	Loss 0.6372 (0.2614)	
training:	Epoch: [6][314/817]	Loss 0.1222 (0.2609)	
training:	Epoch: [6][315/817]	Loss 0.1834 (0.2607)	
training:	Epoch: [6][316/817]	Loss 0.3274 (0.2609)	
training:	Epoch: [6][317/817]	Loss 0.1433 (0.2605)	
training:	Epoch: [6][318/817]	Loss 0.4402 (0.2611)	
training:	Epoch: [6][319/817]	Loss 0.1187 (0.2606)	
training:	Epoch: [6][320/817]	Loss 0.1662 (0.2603)	
training:	Epoch: [6][321/817]	Loss 0.1507 (0.2600)	
training:	Epoch: [6][322/817]	Loss 0.4675 (0.2606)	
training:	Epoch: [6][323/817]	Loss 0.4210 (0.2611)	
training:	Epoch: [6][324/817]	Loss 0.1754 (0.2609)	
training:	Epoch: [6][325/817]	Loss 0.4705 (0.2615)	
training:	Epoch: [6][326/817]	Loss 0.1262 (0.2611)	
training:	Epoch: [6][327/817]	Loss 0.0590 (0.2605)	
training:	Epoch: [6][328/817]	Loss 0.3184 (0.2607)	
training:	Epoch: [6][329/817]	Loss 0.0350 (0.2600)	
training:	Epoch: [6][330/817]	Loss 0.1512 (0.2596)	
training:	Epoch: [6][331/817]	Loss 0.3458 (0.2599)	
training:	Epoch: [6][332/817]	Loss 0.4517 (0.2605)	
training:	Epoch: [6][333/817]	Loss 0.1014 (0.2600)	
training:	Epoch: [6][334/817]	Loss 0.3423 (0.2603)	
training:	Epoch: [6][335/817]	Loss 0.2868 (0.2603)	
training:	Epoch: [6][336/817]	Loss 0.3296 (0.2605)	
training:	Epoch: [6][337/817]	Loss 0.4425 (0.2611)	
training:	Epoch: [6][338/817]	Loss 0.3935 (0.2615)	
training:	Epoch: [6][339/817]	Loss 0.3657 (0.2618)	
training:	Epoch: [6][340/817]	Loss 0.5273 (0.2626)	
training:	Epoch: [6][341/817]	Loss 0.5759 (0.2635)	
training:	Epoch: [6][342/817]	Loss 0.5485 (0.2643)	
training:	Epoch: [6][343/817]	Loss 0.1289 (0.2639)	
training:	Epoch: [6][344/817]	Loss 0.2222 (0.2638)	
training:	Epoch: [6][345/817]	Loss 0.4836 (0.2644)	
training:	Epoch: [6][346/817]	Loss 0.4479 (0.2650)	
training:	Epoch: [6][347/817]	Loss 0.2107 (0.2648)	
training:	Epoch: [6][348/817]	Loss 0.0565 (0.2642)	
training:	Epoch: [6][349/817]	Loss 0.2006 (0.2640)	
training:	Epoch: [6][350/817]	Loss 0.0775 (0.2635)	
training:	Epoch: [6][351/817]	Loss 0.4509 (0.2640)	
training:	Epoch: [6][352/817]	Loss 0.2124 (0.2639)	
training:	Epoch: [6][353/817]	Loss 0.1728 (0.2636)	
training:	Epoch: [6][354/817]	Loss 0.0676 (0.2631)	
training:	Epoch: [6][355/817]	Loss 0.3406 (0.2633)	
training:	Epoch: [6][356/817]	Loss 0.1915 (0.2631)	
training:	Epoch: [6][357/817]	Loss 0.1617 (0.2628)	
training:	Epoch: [6][358/817]	Loss 0.7469 (0.2642)	
training:	Epoch: [6][359/817]	Loss 0.5598 (0.2650)	
training:	Epoch: [6][360/817]	Loss 0.4833 (0.2656)	
training:	Epoch: [6][361/817]	Loss 0.3303 (0.2658)	
training:	Epoch: [6][362/817]	Loss 0.8122 (0.2673)	
training:	Epoch: [6][363/817]	Loss 0.1746 (0.2670)	
training:	Epoch: [6][364/817]	Loss 0.0559 (0.2664)	
training:	Epoch: [6][365/817]	Loss 0.5692 (0.2673)	
training:	Epoch: [6][366/817]	Loss 0.4597 (0.2678)	
training:	Epoch: [6][367/817]	Loss 0.1541 (0.2675)	
training:	Epoch: [6][368/817]	Loss 0.4674 (0.2680)	
training:	Epoch: [6][369/817]	Loss 0.7517 (0.2693)	
training:	Epoch: [6][370/817]	Loss 0.1740 (0.2691)	
training:	Epoch: [6][371/817]	Loss 0.1173 (0.2687)	
training:	Epoch: [6][372/817]	Loss 0.1937 (0.2685)	
training:	Epoch: [6][373/817]	Loss 0.1150 (0.2681)	
training:	Epoch: [6][374/817]	Loss 0.1547 (0.2678)	
training:	Epoch: [6][375/817]	Loss 0.2478 (0.2677)	
training:	Epoch: [6][376/817]	Loss 0.2751 (0.2677)	
training:	Epoch: [6][377/817]	Loss 0.1325 (0.2674)	
training:	Epoch: [6][378/817]	Loss 0.3855 (0.2677)	
training:	Epoch: [6][379/817]	Loss 0.0890 (0.2672)	
training:	Epoch: [6][380/817]	Loss 0.2202 (0.2671)	
training:	Epoch: [6][381/817]	Loss 0.2732 (0.2671)	
training:	Epoch: [6][382/817]	Loss 0.0648 (0.2666)	
training:	Epoch: [6][383/817]	Loss 0.3751 (0.2668)	
training:	Epoch: [6][384/817]	Loss 0.2268 (0.2667)	
training:	Epoch: [6][385/817]	Loss 0.2623 (0.2667)	
training:	Epoch: [6][386/817]	Loss 0.8338 (0.2682)	
training:	Epoch: [6][387/817]	Loss 0.9387 (0.2699)	
training:	Epoch: [6][388/817]	Loss 0.1767 (0.2697)	
training:	Epoch: [6][389/817]	Loss 0.1123 (0.2693)	
training:	Epoch: [6][390/817]	Loss 0.1115 (0.2689)	
training:	Epoch: [6][391/817]	Loss 0.3564 (0.2691)	
training:	Epoch: [6][392/817]	Loss 0.0772 (0.2686)	
training:	Epoch: [6][393/817]	Loss 0.1393 (0.2683)	
training:	Epoch: [6][394/817]	Loss 0.1817 (0.2681)	
training:	Epoch: [6][395/817]	Loss 0.0970 (0.2676)	
training:	Epoch: [6][396/817]	Loss 0.3656 (0.2679)	
training:	Epoch: [6][397/817]	Loss 0.0966 (0.2675)	
training:	Epoch: [6][398/817]	Loss 0.5119 (0.2681)	
training:	Epoch: [6][399/817]	Loss 0.8986 (0.2696)	
training:	Epoch: [6][400/817]	Loss 0.1479 (0.2693)	
training:	Epoch: [6][401/817]	Loss 0.2102 (0.2692)	
training:	Epoch: [6][402/817]	Loss 0.1175 (0.2688)	
training:	Epoch: [6][403/817]	Loss 0.2096 (0.2687)	
training:	Epoch: [6][404/817]	Loss 0.1831 (0.2685)	
training:	Epoch: [6][405/817]	Loss 0.1532 (0.2682)	
training:	Epoch: [6][406/817]	Loss 0.5271 (0.2688)	
training:	Epoch: [6][407/817]	Loss 0.1245 (0.2685)	
training:	Epoch: [6][408/817]	Loss 0.8125 (0.2698)	
training:	Epoch: [6][409/817]	Loss 0.2721 (0.2698)	
training:	Epoch: [6][410/817]	Loss 0.1846 (0.2696)	
training:	Epoch: [6][411/817]	Loss 0.4523 (0.2700)	
training:	Epoch: [6][412/817]	Loss 0.4504 (0.2705)	
training:	Epoch: [6][413/817]	Loss 0.1641 (0.2702)	
training:	Epoch: [6][414/817]	Loss 0.4602 (0.2707)	
training:	Epoch: [6][415/817]	Loss 0.4791 (0.2712)	
training:	Epoch: [6][416/817]	Loss 0.2876 (0.2712)	
training:	Epoch: [6][417/817]	Loss 0.1942 (0.2710)	
training:	Epoch: [6][418/817]	Loss 0.2524 (0.2710)	
training:	Epoch: [6][419/817]	Loss 0.0586 (0.2705)	
training:	Epoch: [6][420/817]	Loss 0.2100 (0.2703)	
training:	Epoch: [6][421/817]	Loss 0.0690 (0.2699)	
training:	Epoch: [6][422/817]	Loss 0.0532 (0.2693)	
training:	Epoch: [6][423/817]	Loss 0.4174 (0.2697)	
training:	Epoch: [6][424/817]	Loss 0.5311 (0.2703)	
training:	Epoch: [6][425/817]	Loss 0.1466 (0.2700)	
training:	Epoch: [6][426/817]	Loss 0.1735 (0.2698)	
training:	Epoch: [6][427/817]	Loss 0.0852 (0.2694)	
training:	Epoch: [6][428/817]	Loss 0.5390 (0.2700)	
training:	Epoch: [6][429/817]	Loss 0.5409 (0.2706)	
training:	Epoch: [6][430/817]	Loss 0.4616 (0.2711)	
training:	Epoch: [6][431/817]	Loss 0.2076 (0.2709)	
training:	Epoch: [6][432/817]	Loss 0.2226 (0.2708)	
training:	Epoch: [6][433/817]	Loss 0.2468 (0.2707)	
training:	Epoch: [6][434/817]	Loss 0.1727 (0.2705)	
training:	Epoch: [6][435/817]	Loss 0.7701 (0.2717)	
training:	Epoch: [6][436/817]	Loss 0.2293 (0.2716)	
training:	Epoch: [6][437/817]	Loss 0.0675 (0.2711)	
training:	Epoch: [6][438/817]	Loss 0.3883 (0.2714)	
training:	Epoch: [6][439/817]	Loss 0.0677 (0.2709)	
training:	Epoch: [6][440/817]	Loss 0.1339 (0.2706)	
training:	Epoch: [6][441/817]	Loss 0.1973 (0.2704)	
training:	Epoch: [6][442/817]	Loss 0.2028 (0.2703)	
training:	Epoch: [6][443/817]	Loss 0.4831 (0.2708)	
training:	Epoch: [6][444/817]	Loss 0.4826 (0.2712)	
training:	Epoch: [6][445/817]	Loss 0.3530 (0.2714)	
training:	Epoch: [6][446/817]	Loss 0.2710 (0.2714)	
training:	Epoch: [6][447/817]	Loss 0.0723 (0.2710)	
training:	Epoch: [6][448/817]	Loss 0.1067 (0.2706)	
training:	Epoch: [6][449/817]	Loss 0.5322 (0.2712)	
training:	Epoch: [6][450/817]	Loss 0.3975 (0.2715)	
training:	Epoch: [6][451/817]	Loss 0.2508 (0.2714)	
training:	Epoch: [6][452/817]	Loss 0.1052 (0.2711)	
training:	Epoch: [6][453/817]	Loss 0.1527 (0.2708)	
training:	Epoch: [6][454/817]	Loss 0.2698 (0.2708)	
training:	Epoch: [6][455/817]	Loss 0.0505 (0.2703)	
training:	Epoch: [6][456/817]	Loss 0.1831 (0.2701)	
training:	Epoch: [6][457/817]	Loss 0.4723 (0.2706)	
training:	Epoch: [6][458/817]	Loss 0.2127 (0.2704)	
training:	Epoch: [6][459/817]	Loss 0.0926 (0.2700)	
training:	Epoch: [6][460/817]	Loss 0.1332 (0.2698)	
training:	Epoch: [6][461/817]	Loss 0.1354 (0.2695)	
training:	Epoch: [6][462/817]	Loss 0.1335 (0.2692)	
training:	Epoch: [6][463/817]	Loss 0.2456 (0.2691)	
training:	Epoch: [6][464/817]	Loss 0.2896 (0.2692)	
training:	Epoch: [6][465/817]	Loss 0.1332 (0.2689)	
training:	Epoch: [6][466/817]	Loss 0.0539 (0.2684)	
training:	Epoch: [6][467/817]	Loss 0.2211 (0.2683)	
training:	Epoch: [6][468/817]	Loss 0.0944 (0.2679)	
training:	Epoch: [6][469/817]	Loss 0.0624 (0.2675)	
training:	Epoch: [6][470/817]	Loss 0.6048 (0.2682)	
training:	Epoch: [6][471/817]	Loss 0.0900 (0.2678)	
training:	Epoch: [6][472/817]	Loss 0.1654 (0.2676)	
training:	Epoch: [6][473/817]	Loss 0.5061 (0.2681)	
training:	Epoch: [6][474/817]	Loss 0.1368 (0.2678)	
training:	Epoch: [6][475/817]	Loss 0.6727 (0.2687)	
training:	Epoch: [6][476/817]	Loss 0.1316 (0.2684)	
training:	Epoch: [6][477/817]	Loss 0.3012 (0.2685)	
training:	Epoch: [6][478/817]	Loss 0.3337 (0.2686)	
training:	Epoch: [6][479/817]	Loss 0.3285 (0.2687)	
training:	Epoch: [6][480/817]	Loss 0.0823 (0.2683)	
training:	Epoch: [6][481/817]	Loss 0.0749 (0.2679)	
training:	Epoch: [6][482/817]	Loss 0.4989 (0.2684)	
training:	Epoch: [6][483/817]	Loss 0.1229 (0.2681)	
training:	Epoch: [6][484/817]	Loss 0.1387 (0.2679)	
training:	Epoch: [6][485/817]	Loss 0.2035 (0.2677)	
training:	Epoch: [6][486/817]	Loss 0.0426 (0.2673)	
training:	Epoch: [6][487/817]	Loss 0.0407 (0.2668)	
training:	Epoch: [6][488/817]	Loss 1.0389 (0.2684)	
training:	Epoch: [6][489/817]	Loss 0.5663 (0.2690)	
training:	Epoch: [6][490/817]	Loss 0.0804 (0.2686)	
training:	Epoch: [6][491/817]	Loss 0.1351 (0.2683)	
training:	Epoch: [6][492/817]	Loss 0.2161 (0.2682)	
training:	Epoch: [6][493/817]	Loss 0.0778 (0.2678)	
training:	Epoch: [6][494/817]	Loss 0.0891 (0.2675)	
training:	Epoch: [6][495/817]	Loss 0.6512 (0.2683)	
training:	Epoch: [6][496/817]	Loss 0.0726 (0.2679)	
training:	Epoch: [6][497/817]	Loss 0.1548 (0.2676)	
training:	Epoch: [6][498/817]	Loss 0.1027 (0.2673)	
training:	Epoch: [6][499/817]	Loss 0.3205 (0.2674)	
training:	Epoch: [6][500/817]	Loss 0.1386 (0.2671)	
training:	Epoch: [6][501/817]	Loss 0.2495 (0.2671)	
training:	Epoch: [6][502/817]	Loss 0.1772 (0.2669)	
training:	Epoch: [6][503/817]	Loss 0.1616 (0.2667)	
training:	Epoch: [6][504/817]	Loss 0.3343 (0.2669)	
training:	Epoch: [6][505/817]	Loss 0.1372 (0.2666)	
training:	Epoch: [6][506/817]	Loss 0.0427 (0.2662)	
training:	Epoch: [6][507/817]	Loss 0.5209 (0.2667)	
training:	Epoch: [6][508/817]	Loss 0.2146 (0.2666)	
training:	Epoch: [6][509/817]	Loss 0.5298 (0.2671)	
training:	Epoch: [6][510/817]	Loss 0.2954 (0.2671)	
training:	Epoch: [6][511/817]	Loss 0.0830 (0.2668)	
training:	Epoch: [6][512/817]	Loss 0.3779 (0.2670)	
training:	Epoch: [6][513/817]	Loss 0.0640 (0.2666)	
training:	Epoch: [6][514/817]	Loss 0.0540 (0.2662)	
training:	Epoch: [6][515/817]	Loss 0.6041 (0.2668)	
training:	Epoch: [6][516/817]	Loss 0.2235 (0.2668)	
training:	Epoch: [6][517/817]	Loss 0.1173 (0.2665)	
training:	Epoch: [6][518/817]	Loss 0.0935 (0.2661)	
training:	Epoch: [6][519/817]	Loss 0.1794 (0.2660)	
training:	Epoch: [6][520/817]	Loss 0.2798 (0.2660)	
training:	Epoch: [6][521/817]	Loss 0.0477 (0.2656)	
training:	Epoch: [6][522/817]	Loss 0.4330 (0.2659)	
training:	Epoch: [6][523/817]	Loss 0.4029 (0.2662)	
training:	Epoch: [6][524/817]	Loss 0.1860 (0.2660)	
training:	Epoch: [6][525/817]	Loss 0.2436 (0.2660)	
training:	Epoch: [6][526/817]	Loss 0.1033 (0.2656)	
training:	Epoch: [6][527/817]	Loss 0.0414 (0.2652)	
training:	Epoch: [6][528/817]	Loss 0.4396 (0.2656)	
training:	Epoch: [6][529/817]	Loss 0.1678 (0.2654)	
training:	Epoch: [6][530/817]	Loss 0.1684 (0.2652)	
training:	Epoch: [6][531/817]	Loss 0.2770 (0.2652)	
training:	Epoch: [6][532/817]	Loss 0.1680 (0.2650)	
training:	Epoch: [6][533/817]	Loss 0.1291 (0.2648)	
training:	Epoch: [6][534/817]	Loss 0.1765 (0.2646)	
training:	Epoch: [6][535/817]	Loss 0.1796 (0.2644)	
training:	Epoch: [6][536/817]	Loss 0.0478 (0.2640)	
training:	Epoch: [6][537/817]	Loss 0.8232 (0.2651)	
training:	Epoch: [6][538/817]	Loss 0.3000 (0.2651)	
training:	Epoch: [6][539/817]	Loss 0.4005 (0.2654)	
training:	Epoch: [6][540/817]	Loss 0.1441 (0.2652)	
training:	Epoch: [6][541/817]	Loss 0.0553 (0.2648)	
training:	Epoch: [6][542/817]	Loss 0.0650 (0.2644)	
training:	Epoch: [6][543/817]	Loss 0.0848 (0.2641)	
training:	Epoch: [6][544/817]	Loss 0.0771 (0.2637)	
training:	Epoch: [6][545/817]	Loss 0.5966 (0.2644)	
training:	Epoch: [6][546/817]	Loss 0.2575 (0.2643)	
training:	Epoch: [6][547/817]	Loss 0.0987 (0.2640)	
training:	Epoch: [6][548/817]	Loss 0.2345 (0.2640)	
training:	Epoch: [6][549/817]	Loss 0.2060 (0.2639)	
training:	Epoch: [6][550/817]	Loss 0.2931 (0.2639)	
training:	Epoch: [6][551/817]	Loss 0.4504 (0.2643)	
training:	Epoch: [6][552/817]	Loss 0.0657 (0.2639)	
training:	Epoch: [6][553/817]	Loss 0.4807 (0.2643)	
training:	Epoch: [6][554/817]	Loss 0.3756 (0.2645)	
training:	Epoch: [6][555/817]	Loss 0.7632 (0.2654)	
training:	Epoch: [6][556/817]	Loss 0.0382 (0.2650)	
training:	Epoch: [6][557/817]	Loss 0.0377 (0.2646)	
training:	Epoch: [6][558/817]	Loss 0.5032 (0.2650)	
training:	Epoch: [6][559/817]	Loss 0.1945 (0.2649)	
training:	Epoch: [6][560/817]	Loss 0.0495 (0.2645)	
training:	Epoch: [6][561/817]	Loss 0.6278 (0.2651)	
training:	Epoch: [6][562/817]	Loss 0.6296 (0.2658)	
training:	Epoch: [6][563/817]	Loss 0.1330 (0.2656)	
training:	Epoch: [6][564/817]	Loss 0.0829 (0.2652)	
training:	Epoch: [6][565/817]	Loss 0.2455 (0.2652)	
training:	Epoch: [6][566/817]	Loss 0.1520 (0.2650)	
training:	Epoch: [6][567/817]	Loss 0.2631 (0.2650)	
training:	Epoch: [6][568/817]	Loss 0.4731 (0.2654)	
training:	Epoch: [6][569/817]	Loss 0.0894 (0.2651)	
training:	Epoch: [6][570/817]	Loss 0.0707 (0.2647)	
training:	Epoch: [6][571/817]	Loss 0.0976 (0.2644)	
training:	Epoch: [6][572/817]	Loss 0.5523 (0.2649)	
training:	Epoch: [6][573/817]	Loss 0.1150 (0.2647)	
training:	Epoch: [6][574/817]	Loss 0.4777 (0.2650)	
training:	Epoch: [6][575/817]	Loss 0.3099 (0.2651)	
training:	Epoch: [6][576/817]	Loss 0.1258 (0.2649)	
training:	Epoch: [6][577/817]	Loss 0.3592 (0.2650)	
training:	Epoch: [6][578/817]	Loss 0.1577 (0.2648)	
training:	Epoch: [6][579/817]	Loss 0.0604 (0.2645)	
training:	Epoch: [6][580/817]	Loss 0.7437 (0.2653)	
training:	Epoch: [6][581/817]	Loss 0.2199 (0.2652)	
training:	Epoch: [6][582/817]	Loss 0.0956 (0.2650)	
training:	Epoch: [6][583/817]	Loss 0.3980 (0.2652)	
training:	Epoch: [6][584/817]	Loss 0.2851 (0.2652)	
training:	Epoch: [6][585/817]	Loss 0.0881 (0.2649)	
training:	Epoch: [6][586/817]	Loss 0.4810 (0.2653)	
training:	Epoch: [6][587/817]	Loss 0.5117 (0.2657)	
training:	Epoch: [6][588/817]	Loss 0.2306 (0.2656)	
training:	Epoch: [6][589/817]	Loss 0.0987 (0.2654)	
training:	Epoch: [6][590/817]	Loss 0.8084 (0.2663)	
training:	Epoch: [6][591/817]	Loss 0.3349 (0.2664)	
training:	Epoch: [6][592/817]	Loss 0.0684 (0.2661)	
training:	Epoch: [6][593/817]	Loss 0.2063 (0.2660)	
training:	Epoch: [6][594/817]	Loss 0.3371 (0.2661)	
training:	Epoch: [6][595/817]	Loss 0.2791 (0.2661)	
training:	Epoch: [6][596/817]	Loss 0.9916 (0.2673)	
training:	Epoch: [6][597/817]	Loss 0.4666 (0.2677)	
training:	Epoch: [6][598/817]	Loss 0.2676 (0.2677)	
training:	Epoch: [6][599/817]	Loss 0.3840 (0.2678)	
training:	Epoch: [6][600/817]	Loss 0.2565 (0.2678)	
training:	Epoch: [6][601/817]	Loss 0.1077 (0.2676)	
training:	Epoch: [6][602/817]	Loss 0.2871 (0.2676)	
training:	Epoch: [6][603/817]	Loss 0.1769 (0.2674)	
training:	Epoch: [6][604/817]	Loss 0.0460 (0.2671)	
training:	Epoch: [6][605/817]	Loss 0.0447 (0.2667)	
training:	Epoch: [6][606/817]	Loss 0.1265 (0.2665)	
training:	Epoch: [6][607/817]	Loss 0.1274 (0.2662)	
training:	Epoch: [6][608/817]	Loss 0.6667 (0.2669)	
training:	Epoch: [6][609/817]	Loss 0.0356 (0.2665)	
training:	Epoch: [6][610/817]	Loss 0.2560 (0.2665)	
training:	Epoch: [6][611/817]	Loss 0.1964 (0.2664)	
training:	Epoch: [6][612/817]	Loss 0.5111 (0.2668)	
training:	Epoch: [6][613/817]	Loss 0.4213 (0.2670)	
training:	Epoch: [6][614/817]	Loss 0.4915 (0.2674)	
training:	Epoch: [6][615/817]	Loss 0.4426 (0.2677)	
training:	Epoch: [6][616/817]	Loss 0.2303 (0.2676)	
training:	Epoch: [6][617/817]	Loss 0.7046 (0.2683)	
training:	Epoch: [6][618/817]	Loss 0.3119 (0.2684)	
training:	Epoch: [6][619/817]	Loss 0.5797 (0.2689)	
training:	Epoch: [6][620/817]	Loss 0.1947 (0.2688)	
training:	Epoch: [6][621/817]	Loss 0.4689 (0.2691)	
training:	Epoch: [6][622/817]	Loss 0.4494 (0.2694)	
training:	Epoch: [6][623/817]	Loss 0.2367 (0.2694)	
training:	Epoch: [6][624/817]	Loss 0.1289 (0.2691)	
training:	Epoch: [6][625/817]	Loss 0.1132 (0.2689)	
training:	Epoch: [6][626/817]	Loss 0.2298 (0.2688)	
training:	Epoch: [6][627/817]	Loss 0.0972 (0.2685)	
training:	Epoch: [6][628/817]	Loss 0.0871 (0.2683)	
training:	Epoch: [6][629/817]	Loss 0.1796 (0.2681)	
training:	Epoch: [6][630/817]	Loss 0.2821 (0.2681)	
training:	Epoch: [6][631/817]	Loss 0.4722 (0.2685)	
training:	Epoch: [6][632/817]	Loss 0.1823 (0.2683)	
training:	Epoch: [6][633/817]	Loss 0.6415 (0.2689)	
training:	Epoch: [6][634/817]	Loss 0.6036 (0.2694)	
training:	Epoch: [6][635/817]	Loss 0.1535 (0.2693)	
training:	Epoch: [6][636/817]	Loss 0.1503 (0.2691)	
training:	Epoch: [6][637/817]	Loss 0.1189 (0.2688)	
training:	Epoch: [6][638/817]	Loss 0.6707 (0.2695)	
training:	Epoch: [6][639/817]	Loss 0.1516 (0.2693)	
training:	Epoch: [6][640/817]	Loss 0.1604 (0.2691)	
training:	Epoch: [6][641/817]	Loss 0.3245 (0.2692)	
training:	Epoch: [6][642/817]	Loss 0.1433 (0.2690)	
training:	Epoch: [6][643/817]	Loss 0.2569 (0.2690)	
training:	Epoch: [6][644/817]	Loss 0.3753 (0.2692)	
training:	Epoch: [6][645/817]	Loss 0.0623 (0.2688)	
training:	Epoch: [6][646/817]	Loss 0.6326 (0.2694)	
training:	Epoch: [6][647/817]	Loss 0.3883 (0.2696)	
training:	Epoch: [6][648/817]	Loss 0.0605 (0.2693)	
training:	Epoch: [6][649/817]	Loss 0.3350 (0.2694)	
training:	Epoch: [6][650/817]	Loss 0.4063 (0.2696)	
training:	Epoch: [6][651/817]	Loss 0.1505 (0.2694)	
training:	Epoch: [6][652/817]	Loss 0.4253 (0.2696)	
training:	Epoch: [6][653/817]	Loss 0.1914 (0.2695)	
training:	Epoch: [6][654/817]	Loss 0.1637 (0.2693)	
training:	Epoch: [6][655/817]	Loss 0.2309 (0.2693)	
training:	Epoch: [6][656/817]	Loss 0.2065 (0.2692)	
training:	Epoch: [6][657/817]	Loss 0.1389 (0.2690)	
training:	Epoch: [6][658/817]	Loss 0.0503 (0.2687)	
training:	Epoch: [6][659/817]	Loss 0.4167 (0.2689)	
training:	Epoch: [6][660/817]	Loss 0.2232 (0.2688)	
training:	Epoch: [6][661/817]	Loss 0.4782 (0.2691)	
training:	Epoch: [6][662/817]	Loss 0.2321 (0.2691)	
training:	Epoch: [6][663/817]	Loss 0.0870 (0.2688)	
training:	Epoch: [6][664/817]	Loss 0.3515 (0.2689)	
training:	Epoch: [6][665/817]	Loss 0.5067 (0.2693)	
training:	Epoch: [6][666/817]	Loss 0.0503 (0.2690)	
training:	Epoch: [6][667/817]	Loss 0.0550 (0.2686)	
training:	Epoch: [6][668/817]	Loss 0.7232 (0.2693)	
training:	Epoch: [6][669/817]	Loss 0.1167 (0.2691)	
training:	Epoch: [6][670/817]	Loss 0.1822 (0.2690)	
training:	Epoch: [6][671/817]	Loss 0.5166 (0.2693)	
training:	Epoch: [6][672/817]	Loss 0.1057 (0.2691)	
training:	Epoch: [6][673/817]	Loss 0.0530 (0.2688)	
training:	Epoch: [6][674/817]	Loss 0.1654 (0.2686)	
training:	Epoch: [6][675/817]	Loss 0.0360 (0.2683)	
training:	Epoch: [6][676/817]	Loss 0.0485 (0.2679)	
training:	Epoch: [6][677/817]	Loss 0.1065 (0.2677)	
training:	Epoch: [6][678/817]	Loss 0.1158 (0.2675)	
training:	Epoch: [6][679/817]	Loss 0.6174 (0.2680)	
training:	Epoch: [6][680/817]	Loss 0.0823 (0.2677)	
training:	Epoch: [6][681/817]	Loss 0.3553 (0.2678)	
training:	Epoch: [6][682/817]	Loss 0.4930 (0.2682)	
training:	Epoch: [6][683/817]	Loss 0.2683 (0.2682)	
training:	Epoch: [6][684/817]	Loss 0.6449 (0.2687)	
training:	Epoch: [6][685/817]	Loss 0.0682 (0.2684)	
training:	Epoch: [6][686/817]	Loss 0.3915 (0.2686)	
training:	Epoch: [6][687/817]	Loss 0.0537 (0.2683)	
training:	Epoch: [6][688/817]	Loss 0.4848 (0.2686)	
training:	Epoch: [6][689/817]	Loss 0.0982 (0.2684)	
training:	Epoch: [6][690/817]	Loss 0.1393 (0.2682)	
training:	Epoch: [6][691/817]	Loss 0.2473 (0.2681)	
training:	Epoch: [6][692/817]	Loss 0.1873 (0.2680)	
training:	Epoch: [6][693/817]	Loss 0.5847 (0.2685)	
training:	Epoch: [6][694/817]	Loss 0.0409 (0.2682)	
training:	Epoch: [6][695/817]	Loss 0.1049 (0.2679)	
training:	Epoch: [6][696/817]	Loss 0.6655 (0.2685)	
training:	Epoch: [6][697/817]	Loss 0.5766 (0.2689)	
training:	Epoch: [6][698/817]	Loss 0.2567 (0.2689)	
training:	Epoch: [6][699/817]	Loss 0.4987 (0.2692)	
training:	Epoch: [6][700/817]	Loss 0.6714 (0.2698)	
training:	Epoch: [6][701/817]	Loss 0.3239 (0.2699)	
training:	Epoch: [6][702/817]	Loss 0.4722 (0.2702)	
training:	Epoch: [6][703/817]	Loss 0.4477 (0.2704)	
training:	Epoch: [6][704/817]	Loss 0.2693 (0.2704)	
training:	Epoch: [6][705/817]	Loss 0.1098 (0.2702)	
training:	Epoch: [6][706/817]	Loss 0.1247 (0.2700)	
training:	Epoch: [6][707/817]	Loss 0.2325 (0.2700)	
training:	Epoch: [6][708/817]	Loss 0.7809 (0.2707)	
training:	Epoch: [6][709/817]	Loss 0.1351 (0.2705)	
training:	Epoch: [6][710/817]	Loss 0.2840 (0.2705)	
training:	Epoch: [6][711/817]	Loss 0.1174 (0.2703)	
training:	Epoch: [6][712/817]	Loss 0.3241 (0.2704)	
training:	Epoch: [6][713/817]	Loss 0.0868 (0.2701)	
training:	Epoch: [6][714/817]	Loss 0.2904 (0.2701)	
training:	Epoch: [6][715/817]	Loss 0.2346 (0.2701)	
training:	Epoch: [6][716/817]	Loss 0.2651 (0.2701)	
training:	Epoch: [6][717/817]	Loss 0.3277 (0.2702)	
training:	Epoch: [6][718/817]	Loss 0.4558 (0.2704)	
training:	Epoch: [6][719/817]	Loss 0.4520 (0.2707)	
training:	Epoch: [6][720/817]	Loss 0.1444 (0.2705)	
training:	Epoch: [6][721/817]	Loss 0.0912 (0.2702)	
training:	Epoch: [6][722/817]	Loss 0.1523 (0.2701)	
training:	Epoch: [6][723/817]	Loss 0.3534 (0.2702)	
training:	Epoch: [6][724/817]	Loss 0.4782 (0.2705)	
training:	Epoch: [6][725/817]	Loss 0.4907 (0.2708)	
training:	Epoch: [6][726/817]	Loss 0.2751 (0.2708)	
training:	Epoch: [6][727/817]	Loss 0.4309 (0.2710)	
training:	Epoch: [6][728/817]	Loss 0.0453 (0.2707)	
training:	Epoch: [6][729/817]	Loss 0.2884 (0.2707)	
training:	Epoch: [6][730/817]	Loss 0.0779 (0.2705)	
training:	Epoch: [6][731/817]	Loss 0.0720 (0.2702)	
training:	Epoch: [6][732/817]	Loss 0.1106 (0.2700)	
training:	Epoch: [6][733/817]	Loss 0.4821 (0.2703)	
training:	Epoch: [6][734/817]	Loss 0.1853 (0.2701)	
training:	Epoch: [6][735/817]	Loss 0.2135 (0.2701)	
training:	Epoch: [6][736/817]	Loss 0.1441 (0.2699)	
training:	Epoch: [6][737/817]	Loss 0.1199 (0.2697)	
training:	Epoch: [6][738/817]	Loss 0.1967 (0.2696)	
training:	Epoch: [6][739/817]	Loss 0.4156 (0.2698)	
training:	Epoch: [6][740/817]	Loss 0.3279 (0.2699)	
training:	Epoch: [6][741/817]	Loss 0.2988 (0.2699)	
training:	Epoch: [6][742/817]	Loss 0.3250 (0.2700)	
training:	Epoch: [6][743/817]	Loss 0.0863 (0.2697)	
training:	Epoch: [6][744/817]	Loss 0.2674 (0.2697)	
training:	Epoch: [6][745/817]	Loss 0.3175 (0.2698)	
training:	Epoch: [6][746/817]	Loss 0.2882 (0.2698)	
training:	Epoch: [6][747/817]	Loss 0.1022 (0.2696)	
training:	Epoch: [6][748/817]	Loss 0.1831 (0.2695)	
training:	Epoch: [6][749/817]	Loss 0.2249 (0.2694)	
training:	Epoch: [6][750/817]	Loss 0.0988 (0.2692)	
training:	Epoch: [6][751/817]	Loss 0.1373 (0.2690)	
training:	Epoch: [6][752/817]	Loss 0.5498 (0.2694)	
training:	Epoch: [6][753/817]	Loss 0.2243 (0.2693)	
training:	Epoch: [6][754/817]	Loss 0.2886 (0.2694)	
training:	Epoch: [6][755/817]	Loss 0.7919 (0.2701)	
training:	Epoch: [6][756/817]	Loss 0.0721 (0.2698)	
training:	Epoch: [6][757/817]	Loss 0.3162 (0.2699)	
training:	Epoch: [6][758/817]	Loss 0.1148 (0.2696)	
training:	Epoch: [6][759/817]	Loss 0.0878 (0.2694)	
training:	Epoch: [6][760/817]	Loss 0.1909 (0.2693)	
training:	Epoch: [6][761/817]	Loss 0.2565 (0.2693)	
training:	Epoch: [6][762/817]	Loss 0.1695 (0.2692)	
training:	Epoch: [6][763/817]	Loss 0.5338 (0.2695)	
training:	Epoch: [6][764/817]	Loss 0.1768 (0.2694)	
training:	Epoch: [6][765/817]	Loss 0.1226 (0.2692)	
training:	Epoch: [6][766/817]	Loss 0.7726 (0.2698)	
training:	Epoch: [6][767/817]	Loss 0.2455 (0.2698)	
training:	Epoch: [6][768/817]	Loss 0.1225 (0.2696)	
training:	Epoch: [6][769/817]	Loss 0.1518 (0.2695)	
training:	Epoch: [6][770/817]	Loss 0.3919 (0.2696)	
training:	Epoch: [6][771/817]	Loss 0.2080 (0.2696)	
training:	Epoch: [6][772/817]	Loss 0.5097 (0.2699)	
training:	Epoch: [6][773/817]	Loss 0.2162 (0.2698)	
training:	Epoch: [6][774/817]	Loss 0.3472 (0.2699)	
training:	Epoch: [6][775/817]	Loss 0.2850 (0.2699)	
training:	Epoch: [6][776/817]	Loss 0.2375 (0.2699)	
training:	Epoch: [6][777/817]	Loss 0.1084 (0.2697)	
training:	Epoch: [6][778/817]	Loss 0.4849 (0.2699)	
training:	Epoch: [6][779/817]	Loss 0.3496 (0.2700)	
training:	Epoch: [6][780/817]	Loss 0.6660 (0.2705)	
training:	Epoch: [6][781/817]	Loss 0.1111 (0.2703)	
training:	Epoch: [6][782/817]	Loss 0.3598 (0.2705)	
training:	Epoch: [6][783/817]	Loss 0.3373 (0.2705)	
training:	Epoch: [6][784/817]	Loss 0.4583 (0.2708)	
training:	Epoch: [6][785/817]	Loss 0.4413 (0.2710)	
training:	Epoch: [6][786/817]	Loss 0.2017 (0.2709)	
training:	Epoch: [6][787/817]	Loss 0.0810 (0.2707)	
training:	Epoch: [6][788/817]	Loss 0.3003 (0.2707)	
training:	Epoch: [6][789/817]	Loss 0.0978 (0.2705)	
training:	Epoch: [6][790/817]	Loss 0.1356 (0.2703)	
training:	Epoch: [6][791/817]	Loss 0.1229 (0.2701)	
training:	Epoch: [6][792/817]	Loss 0.0812 (0.2699)	
training:	Epoch: [6][793/817]	Loss 0.2304 (0.2698)	
training:	Epoch: [6][794/817]	Loss 0.0622 (0.2696)	
training:	Epoch: [6][795/817]	Loss 0.1742 (0.2695)	
training:	Epoch: [6][796/817]	Loss 0.0615 (0.2692)	
training:	Epoch: [6][797/817]	Loss 0.1027 (0.2690)	
training:	Epoch: [6][798/817]	Loss 0.4445 (0.2692)	
training:	Epoch: [6][799/817]	Loss 0.3440 (0.2693)	
training:	Epoch: [6][800/817]	Loss 0.0630 (0.2690)	
training:	Epoch: [6][801/817]	Loss 0.0535 (0.2688)	
training:	Epoch: [6][802/817]	Loss 0.2940 (0.2688)	
training:	Epoch: [6][803/817]	Loss 0.1515 (0.2687)	
training:	Epoch: [6][804/817]	Loss 0.2128 (0.2686)	
training:	Epoch: [6][805/817]	Loss 0.4095 (0.2688)	
training:	Epoch: [6][806/817]	Loss 0.2840 (0.2688)	
training:	Epoch: [6][807/817]	Loss 0.4431 (0.2690)	
training:	Epoch: [6][808/817]	Loss 0.1975 (0.2689)	
training:	Epoch: [6][809/817]	Loss 0.7933 (0.2696)	
training:	Epoch: [6][810/817]	Loss 0.1235 (0.2694)	
training:	Epoch: [6][811/817]	Loss 0.8328 (0.2701)	
training:	Epoch: [6][812/817]	Loss 0.4197 (0.2703)	
training:	Epoch: [6][813/817]	Loss 0.3103 (0.2703)	
training:	Epoch: [6][814/817]	Loss 0.0993 (0.2701)	
training:	Epoch: [6][815/817]	Loss 0.2447 (0.2701)	
training:	Epoch: [6][816/817]	Loss 0.4344 (0.2703)	
training:	Epoch: [6][817/817]	Loss 0.6880 (0.2708)	
Training:	 Loss: 0.2707

Training:	 ACC: 0.9323 0.9321 0.9271 0.9375
Validation:	 ACC: 0.8161 0.8165 0.8250 0.8072
Validation:	 Best_BACC: 0.8223 0.8218 0.8127 0.8318
Validation:	 Loss: 0.4261
Pretraining:	Epoch 7/200
----------
training:	Epoch: [7][1/817]	Loss 0.3538 (0.3538)	
training:	Epoch: [7][2/817]	Loss 0.1050 (0.2294)	
training:	Epoch: [7][3/817]	Loss 0.2051 (0.2213)	
training:	Epoch: [7][4/817]	Loss 0.1031 (0.1918)	
training:	Epoch: [7][5/817]	Loss 0.1974 (0.1929)	
training:	Epoch: [7][6/817]	Loss 0.1095 (0.1790)	
training:	Epoch: [7][7/817]	Loss 0.0768 (0.1644)	
training:	Epoch: [7][8/817]	Loss 0.3838 (0.1918)	
training:	Epoch: [7][9/817]	Loss 0.1318 (0.1852)	
training:	Epoch: [7][10/817]	Loss 0.3675 (0.2034)	
training:	Epoch: [7][11/817]	Loss 0.3658 (0.2182)	
training:	Epoch: [7][12/817]	Loss 0.1568 (0.2130)	
training:	Epoch: [7][13/817]	Loss 0.1028 (0.2046)	
training:	Epoch: [7][14/817]	Loss 0.1098 (0.1978)	
training:	Epoch: [7][15/817]	Loss 0.1923 (0.1974)	
training:	Epoch: [7][16/817]	Loss 0.3334 (0.2059)	
training:	Epoch: [7][17/817]	Loss 0.3876 (0.2166)	
training:	Epoch: [7][18/817]	Loss 0.0913 (0.2096)	
training:	Epoch: [7][19/817]	Loss 0.1153 (0.2047)	
training:	Epoch: [7][20/817]	Loss 0.0999 (0.1994)	
training:	Epoch: [7][21/817]	Loss 0.2833 (0.2034)	
training:	Epoch: [7][22/817]	Loss 0.4630 (0.2152)	
training:	Epoch: [7][23/817]	Loss 0.0476 (0.2079)	
training:	Epoch: [7][24/817]	Loss 0.2041 (0.2078)	
training:	Epoch: [7][25/817]	Loss 0.1211 (0.2043)	
training:	Epoch: [7][26/817]	Loss 0.1598 (0.2026)	
training:	Epoch: [7][27/817]	Loss 0.0956 (0.1986)	
training:	Epoch: [7][28/817]	Loss 0.4088 (0.2061)	
training:	Epoch: [7][29/817]	Loss 0.4974 (0.2162)	
training:	Epoch: [7][30/817]	Loss 0.1035 (0.2124)	
training:	Epoch: [7][31/817]	Loss 0.0569 (0.2074)	
training:	Epoch: [7][32/817]	Loss 0.0545 (0.2026)	
training:	Epoch: [7][33/817]	Loss 0.4618 (0.2105)	
training:	Epoch: [7][34/817]	Loss 0.2812 (0.2126)	
training:	Epoch: [7][35/817]	Loss 0.1393 (0.2105)	
training:	Epoch: [7][36/817]	Loss 0.4223 (0.2164)	
training:	Epoch: [7][37/817]	Loss 0.0804 (0.2127)	
training:	Epoch: [7][38/817]	Loss 0.1244 (0.2104)	
training:	Epoch: [7][39/817]	Loss 0.1105 (0.2078)	
training:	Epoch: [7][40/817]	Loss 0.1184 (0.2056)	
training:	Epoch: [7][41/817]	Loss 0.3605 (0.2094)	
training:	Epoch: [7][42/817]	Loss 0.4138 (0.2142)	
training:	Epoch: [7][43/817]	Loss 0.3570 (0.2175)	
training:	Epoch: [7][44/817]	Loss 0.1975 (0.2171)	
training:	Epoch: [7][45/817]	Loss 0.1859 (0.2164)	
training:	Epoch: [7][46/817]	Loss 0.0965 (0.2138)	
training:	Epoch: [7][47/817]	Loss 0.0500 (0.2103)	
training:	Epoch: [7][48/817]	Loss 0.1945 (0.2100)	
training:	Epoch: [7][49/817]	Loss 0.1505 (0.2088)	
training:	Epoch: [7][50/817]	Loss 0.0737 (0.2061)	
training:	Epoch: [7][51/817]	Loss 0.4366 (0.2106)	
training:	Epoch: [7][52/817]	Loss 0.5594 (0.2173)	
training:	Epoch: [7][53/817]	Loss 0.3038 (0.2189)	
training:	Epoch: [7][54/817]	Loss 0.0865 (0.2165)	
training:	Epoch: [7][55/817]	Loss 0.0502 (0.2134)	
training:	Epoch: [7][56/817]	Loss 0.0883 (0.2112)	
training:	Epoch: [7][57/817]	Loss 0.0360 (0.2081)	
training:	Epoch: [7][58/817]	Loss 0.0548 (0.2055)	
training:	Epoch: [7][59/817]	Loss 0.1199 (0.2040)	
training:	Epoch: [7][60/817]	Loss 0.1472 (0.2031)	
training:	Epoch: [7][61/817]	Loss 0.1966 (0.2030)	
training:	Epoch: [7][62/817]	Loss 0.1155 (0.2016)	
training:	Epoch: [7][63/817]	Loss 0.2347 (0.2021)	
training:	Epoch: [7][64/817]	Loss 0.1595 (0.2014)	
training:	Epoch: [7][65/817]	Loss 0.4229 (0.2048)	
training:	Epoch: [7][66/817]	Loss 0.1073 (0.2034)	
training:	Epoch: [7][67/817]	Loss 0.0624 (0.2013)	
training:	Epoch: [7][68/817]	Loss 0.3451 (0.2034)	
training:	Epoch: [7][69/817]	Loss 0.0945 (0.2018)	
training:	Epoch: [7][70/817]	Loss 0.2083 (0.2019)	
training:	Epoch: [7][71/817]	Loss 0.0908 (0.2003)	
training:	Epoch: [7][72/817]	Loss 0.0736 (0.1986)	
training:	Epoch: [7][73/817]	Loss 0.4586 (0.2021)	
training:	Epoch: [7][74/817]	Loss 0.1498 (0.2014)	
training:	Epoch: [7][75/817]	Loss 0.0618 (0.1996)	
training:	Epoch: [7][76/817]	Loss 0.0572 (0.1977)	
training:	Epoch: [7][77/817]	Loss 0.5947 (0.2028)	
training:	Epoch: [7][78/817]	Loss 0.5877 (0.2078)	
training:	Epoch: [7][79/817]	Loss 1.1105 (0.2192)	
training:	Epoch: [7][80/817]	Loss 0.3994 (0.2215)	
training:	Epoch: [7][81/817]	Loss 0.1264 (0.2203)	
training:	Epoch: [7][82/817]	Loss 0.2033 (0.2201)	
training:	Epoch: [7][83/817]	Loss 0.0528 (0.2181)	
training:	Epoch: [7][84/817]	Loss 0.0899 (0.2165)	
training:	Epoch: [7][85/817]	Loss 0.5978 (0.2210)	
training:	Epoch: [7][86/817]	Loss 0.0555 (0.2191)	
training:	Epoch: [7][87/817]	Loss 0.3233 (0.2203)	
training:	Epoch: [7][88/817]	Loss 0.0841 (0.2187)	
training:	Epoch: [7][89/817]	Loss 0.4930 (0.2218)	
training:	Epoch: [7][90/817]	Loss 0.0866 (0.2203)	
training:	Epoch: [7][91/817]	Loss 0.1955 (0.2201)	
training:	Epoch: [7][92/817]	Loss 0.2851 (0.2208)	
training:	Epoch: [7][93/817]	Loss 0.1356 (0.2198)	
training:	Epoch: [7][94/817]	Loss 0.1398 (0.2190)	
training:	Epoch: [7][95/817]	Loss 0.0631 (0.2174)	
training:	Epoch: [7][96/817]	Loss 0.8699 (0.2241)	
training:	Epoch: [7][97/817]	Loss 0.0905 (0.2228)	
training:	Epoch: [7][98/817]	Loss 0.3563 (0.2241)	
training:	Epoch: [7][99/817]	Loss 0.2940 (0.2248)	
training:	Epoch: [7][100/817]	Loss 0.2087 (0.2247)	
training:	Epoch: [7][101/817]	Loss 0.4186 (0.2266)	
training:	Epoch: [7][102/817]	Loss 0.1842 (0.2262)	
training:	Epoch: [7][103/817]	Loss 0.2501 (0.2264)	
training:	Epoch: [7][104/817]	Loss 0.1274 (0.2255)	
training:	Epoch: [7][105/817]	Loss 0.2001 (0.2252)	
training:	Epoch: [7][106/817]	Loss 0.4149 (0.2270)	
training:	Epoch: [7][107/817]	Loss 0.2580 (0.2273)	
training:	Epoch: [7][108/817]	Loss 0.0768 (0.2259)	
training:	Epoch: [7][109/817]	Loss 0.0790 (0.2246)	
training:	Epoch: [7][110/817]	Loss 0.1137 (0.2236)	
training:	Epoch: [7][111/817]	Loss 0.2707 (0.2240)	
training:	Epoch: [7][112/817]	Loss 0.2812 (0.2245)	
training:	Epoch: [7][113/817]	Loss 0.1337 (0.2237)	
training:	Epoch: [7][114/817]	Loss 0.4125 (0.2253)	
training:	Epoch: [7][115/817]	Loss 0.0695 (0.2240)	
training:	Epoch: [7][116/817]	Loss 0.2636 (0.2243)	
training:	Epoch: [7][117/817]	Loss 0.5036 (0.2267)	
training:	Epoch: [7][118/817]	Loss 0.4277 (0.2284)	
training:	Epoch: [7][119/817]	Loss 0.0639 (0.2270)	
training:	Epoch: [7][120/817]	Loss 0.3849 (0.2283)	
training:	Epoch: [7][121/817]	Loss 0.1278 (0.2275)	
training:	Epoch: [7][122/817]	Loss 0.3144 (0.2282)	
training:	Epoch: [7][123/817]	Loss 0.0497 (0.2268)	
training:	Epoch: [7][124/817]	Loss 0.1021 (0.2258)	
training:	Epoch: [7][125/817]	Loss 0.1880 (0.2255)	
training:	Epoch: [7][126/817]	Loss 0.0663 (0.2242)	
training:	Epoch: [7][127/817]	Loss 0.5450 (0.2267)	
training:	Epoch: [7][128/817]	Loss 0.2077 (0.2266)	
training:	Epoch: [7][129/817]	Loss 0.0446 (0.2252)	
training:	Epoch: [7][130/817]	Loss 0.0866 (0.2241)	
training:	Epoch: [7][131/817]	Loss 0.0868 (0.2231)	
training:	Epoch: [7][132/817]	Loss 0.2193 (0.2230)	
training:	Epoch: [7][133/817]	Loss 0.0781 (0.2219)	
training:	Epoch: [7][134/817]	Loss 0.7788 (0.2261)	
training:	Epoch: [7][135/817]	Loss 0.3849 (0.2273)	
training:	Epoch: [7][136/817]	Loss 0.2604 (0.2275)	
training:	Epoch: [7][137/817]	Loss 0.2935 (0.2280)	
training:	Epoch: [7][138/817]	Loss 0.2076 (0.2279)	
training:	Epoch: [7][139/817]	Loss 0.0907 (0.2269)	
training:	Epoch: [7][140/817]	Loss 0.0493 (0.2256)	
training:	Epoch: [7][141/817]	Loss 0.1486 (0.2251)	
training:	Epoch: [7][142/817]	Loss 0.0586 (0.2239)	
training:	Epoch: [7][143/817]	Loss 0.0879 (0.2229)	
training:	Epoch: [7][144/817]	Loss 0.6423 (0.2258)	
training:	Epoch: [7][145/817]	Loss 0.0582 (0.2247)	
training:	Epoch: [7][146/817]	Loss 0.2095 (0.2246)	
training:	Epoch: [7][147/817]	Loss 0.1736 (0.2242)	
training:	Epoch: [7][148/817]	Loss 0.4454 (0.2257)	
training:	Epoch: [7][149/817]	Loss 0.0603 (0.2246)	
training:	Epoch: [7][150/817]	Loss 0.4534 (0.2261)	
training:	Epoch: [7][151/817]	Loss 0.2010 (0.2260)	
training:	Epoch: [7][152/817]	Loss 0.1353 (0.2254)	
training:	Epoch: [7][153/817]	Loss 0.0438 (0.2242)	
training:	Epoch: [7][154/817]	Loss 0.2829 (0.2246)	
training:	Epoch: [7][155/817]	Loss 0.0710 (0.2236)	
training:	Epoch: [7][156/817]	Loss 0.1545 (0.2231)	
training:	Epoch: [7][157/817]	Loss 0.0790 (0.2222)	
training:	Epoch: [7][158/817]	Loss 0.3777 (0.2232)	
training:	Epoch: [7][159/817]	Loss 0.2689 (0.2235)	
training:	Epoch: [7][160/817]	Loss 0.1769 (0.2232)	
training:	Epoch: [7][161/817]	Loss 0.0564 (0.2222)	
training:	Epoch: [7][162/817]	Loss 0.1391 (0.2217)	
training:	Epoch: [7][163/817]	Loss 0.2762 (0.2220)	
training:	Epoch: [7][164/817]	Loss 0.0447 (0.2209)	
training:	Epoch: [7][165/817]	Loss 0.2077 (0.2208)	
training:	Epoch: [7][166/817]	Loss 0.0558 (0.2198)	
training:	Epoch: [7][167/817]	Loss 0.0532 (0.2188)	
training:	Epoch: [7][168/817]	Loss 0.3602 (0.2197)	
training:	Epoch: [7][169/817]	Loss 0.1890 (0.2195)	
training:	Epoch: [7][170/817]	Loss 0.2831 (0.2199)	
training:	Epoch: [7][171/817]	Loss 0.1009 (0.2192)	
training:	Epoch: [7][172/817]	Loss 0.0982 (0.2185)	
training:	Epoch: [7][173/817]	Loss 0.4424 (0.2198)	
training:	Epoch: [7][174/817]	Loss 0.1211 (0.2192)	
training:	Epoch: [7][175/817]	Loss 0.1517 (0.2188)	
training:	Epoch: [7][176/817]	Loss 0.0547 (0.2179)	
training:	Epoch: [7][177/817]	Loss 0.0479 (0.2169)	
training:	Epoch: [7][178/817]	Loss 0.2071 (0.2169)	
training:	Epoch: [7][179/817]	Loss 0.0616 (0.2160)	
training:	Epoch: [7][180/817]	Loss 0.1797 (0.2158)	
training:	Epoch: [7][181/817]	Loss 0.1622 (0.2155)	
training:	Epoch: [7][182/817]	Loss 0.3528 (0.2163)	
training:	Epoch: [7][183/817]	Loss 0.0848 (0.2155)	
training:	Epoch: [7][184/817]	Loss 0.4936 (0.2170)	
training:	Epoch: [7][185/817]	Loss 0.0433 (0.2161)	
training:	Epoch: [7][186/817]	Loss 0.1515 (0.2158)	
training:	Epoch: [7][187/817]	Loss 0.0453 (0.2148)	
training:	Epoch: [7][188/817]	Loss 0.0960 (0.2142)	
training:	Epoch: [7][189/817]	Loss 0.1647 (0.2140)	
training:	Epoch: [7][190/817]	Loss 0.0871 (0.2133)	
training:	Epoch: [7][191/817]	Loss 0.3475 (0.2140)	
training:	Epoch: [7][192/817]	Loss 0.0707 (0.2132)	
training:	Epoch: [7][193/817]	Loss 0.0460 (0.2124)	
training:	Epoch: [7][194/817]	Loss 0.1008 (0.2118)	
training:	Epoch: [7][195/817]	Loss 0.7619 (0.2146)	
training:	Epoch: [7][196/817]	Loss 0.2576 (0.2148)	
training:	Epoch: [7][197/817]	Loss 0.0480 (0.2140)	
training:	Epoch: [7][198/817]	Loss 0.3488 (0.2147)	
training:	Epoch: [7][199/817]	Loss 0.0543 (0.2139)	
training:	Epoch: [7][200/817]	Loss 0.2760 (0.2142)	
training:	Epoch: [7][201/817]	Loss 0.1240 (0.2137)	
training:	Epoch: [7][202/817]	Loss 0.0375 (0.2129)	
training:	Epoch: [7][203/817]	Loss 0.3868 (0.2137)	
training:	Epoch: [7][204/817]	Loss 0.0496 (0.2129)	
training:	Epoch: [7][205/817]	Loss 0.0918 (0.2123)	
training:	Epoch: [7][206/817]	Loss 0.0652 (0.2116)	
training:	Epoch: [7][207/817]	Loss 0.0387 (0.2108)	
training:	Epoch: [7][208/817]	Loss 0.0722 (0.2101)	
training:	Epoch: [7][209/817]	Loss 0.1633 (0.2099)	
training:	Epoch: [7][210/817]	Loss 0.0612 (0.2092)	
training:	Epoch: [7][211/817]	Loss 0.0687 (0.2085)	
training:	Epoch: [7][212/817]	Loss 0.0714 (0.2079)	
training:	Epoch: [7][213/817]	Loss 0.0632 (0.2072)	
training:	Epoch: [7][214/817]	Loss 0.5872 (0.2090)	
training:	Epoch: [7][215/817]	Loss 0.4530 (0.2101)	
training:	Epoch: [7][216/817]	Loss 0.8721 (0.2132)	
training:	Epoch: [7][217/817]	Loss 0.5059 (0.2145)	
training:	Epoch: [7][218/817]	Loss 0.6232 (0.2164)	
training:	Epoch: [7][219/817]	Loss 0.0917 (0.2158)	
training:	Epoch: [7][220/817]	Loss 0.4138 (0.2167)	
training:	Epoch: [7][221/817]	Loss 0.3500 (0.2173)	
training:	Epoch: [7][222/817]	Loss 0.0529 (0.2166)	
training:	Epoch: [7][223/817]	Loss 0.0279 (0.2157)	
training:	Epoch: [7][224/817]	Loss 0.0361 (0.2149)	
training:	Epoch: [7][225/817]	Loss 0.5129 (0.2162)	
training:	Epoch: [7][226/817]	Loss 0.0910 (0.2157)	
training:	Epoch: [7][227/817]	Loss 0.2040 (0.2156)	
training:	Epoch: [7][228/817]	Loss 0.2676 (0.2159)	
training:	Epoch: [7][229/817]	Loss 0.4587 (0.2169)	
training:	Epoch: [7][230/817]	Loss 0.3881 (0.2177)	
training:	Epoch: [7][231/817]	Loss 0.2696 (0.2179)	
training:	Epoch: [7][232/817]	Loss 0.3023 (0.2183)	
training:	Epoch: [7][233/817]	Loss 0.2573 (0.2184)	
training:	Epoch: [7][234/817]	Loss 0.0809 (0.2178)	
training:	Epoch: [7][235/817]	Loss 0.1415 (0.2175)	
training:	Epoch: [7][236/817]	Loss 0.1600 (0.2173)	
training:	Epoch: [7][237/817]	Loss 0.0578 (0.2166)	
training:	Epoch: [7][238/817]	Loss 0.2699 (0.2168)	
training:	Epoch: [7][239/817]	Loss 0.0786 (0.2162)	
training:	Epoch: [7][240/817]	Loss 0.1677 (0.2160)	
training:	Epoch: [7][241/817]	Loss 0.5217 (0.2173)	
training:	Epoch: [7][242/817]	Loss 0.0805 (0.2167)	
training:	Epoch: [7][243/817]	Loss 0.5758 (0.2182)	
training:	Epoch: [7][244/817]	Loss 0.4374 (0.2191)	
training:	Epoch: [7][245/817]	Loss 0.3068 (0.2195)	
training:	Epoch: [7][246/817]	Loss 0.1538 (0.2192)	
training:	Epoch: [7][247/817]	Loss 0.0888 (0.2187)	
training:	Epoch: [7][248/817]	Loss 0.0432 (0.2180)	
training:	Epoch: [7][249/817]	Loss 0.3388 (0.2185)	
training:	Epoch: [7][250/817]	Loss 0.0535 (0.2178)	
training:	Epoch: [7][251/817]	Loss 0.1263 (0.2174)	
training:	Epoch: [7][252/817]	Loss 0.0391 (0.2167)	
training:	Epoch: [7][253/817]	Loss 0.3275 (0.2172)	
training:	Epoch: [7][254/817]	Loss 0.0678 (0.2166)	
training:	Epoch: [7][255/817]	Loss 0.0811 (0.2161)	
training:	Epoch: [7][256/817]	Loss 0.1682 (0.2159)	
training:	Epoch: [7][257/817]	Loss 0.0857 (0.2154)	
training:	Epoch: [7][258/817]	Loss 0.4149 (0.2161)	
training:	Epoch: [7][259/817]	Loss 0.1410 (0.2158)	
training:	Epoch: [7][260/817]	Loss 0.1042 (0.2154)	
training:	Epoch: [7][261/817]	Loss 0.1222 (0.2151)	
training:	Epoch: [7][262/817]	Loss 0.1337 (0.2147)	
training:	Epoch: [7][263/817]	Loss 0.0959 (0.2143)	
training:	Epoch: [7][264/817]	Loss 0.7971 (0.2165)	
training:	Epoch: [7][265/817]	Loss 0.1105 (0.2161)	
training:	Epoch: [7][266/817]	Loss 0.3235 (0.2165)	
training:	Epoch: [7][267/817]	Loss 0.0528 (0.2159)	
training:	Epoch: [7][268/817]	Loss 0.1469 (0.2156)	
training:	Epoch: [7][269/817]	Loss 0.2323 (0.2157)	
training:	Epoch: [7][270/817]	Loss 0.6589 (0.2173)	
training:	Epoch: [7][271/817]	Loss 0.1317 (0.2170)	
training:	Epoch: [7][272/817]	Loss 0.0815 (0.2165)	
training:	Epoch: [7][273/817]	Loss 0.0360 (0.2159)	
training:	Epoch: [7][274/817]	Loss 0.4100 (0.2166)	
training:	Epoch: [7][275/817]	Loss 0.3112 (0.2169)	
training:	Epoch: [7][276/817]	Loss 0.2036 (0.2169)	
training:	Epoch: [7][277/817]	Loss 0.1324 (0.2166)	
training:	Epoch: [7][278/817]	Loss 0.1595 (0.2164)	
training:	Epoch: [7][279/817]	Loss 0.1937 (0.2163)	
training:	Epoch: [7][280/817]	Loss 0.1121 (0.2159)	
training:	Epoch: [7][281/817]	Loss 0.0324 (0.2152)	
training:	Epoch: [7][282/817]	Loss 0.1104 (0.2149)	
training:	Epoch: [7][283/817]	Loss 0.0467 (0.2143)	
training:	Epoch: [7][284/817]	Loss 0.0454 (0.2137)	
training:	Epoch: [7][285/817]	Loss 0.0361 (0.2131)	
training:	Epoch: [7][286/817]	Loss 0.3155 (0.2134)	
training:	Epoch: [7][287/817]	Loss 0.0532 (0.2129)	
training:	Epoch: [7][288/817]	Loss 0.2184 (0.2129)	
training:	Epoch: [7][289/817]	Loss 0.1231 (0.2126)	
training:	Epoch: [7][290/817]	Loss 0.0926 (0.2122)	
training:	Epoch: [7][291/817]	Loss 0.0638 (0.2117)	
training:	Epoch: [7][292/817]	Loss 0.4628 (0.2125)	
training:	Epoch: [7][293/817]	Loss 0.1010 (0.2121)	
training:	Epoch: [7][294/817]	Loss 0.3697 (0.2127)	
training:	Epoch: [7][295/817]	Loss 0.1341 (0.2124)	
training:	Epoch: [7][296/817]	Loss 0.0380 (0.2118)	
training:	Epoch: [7][297/817]	Loss 0.0555 (0.2113)	
training:	Epoch: [7][298/817]	Loss 0.0606 (0.2108)	
training:	Epoch: [7][299/817]	Loss 0.2521 (0.2109)	
training:	Epoch: [7][300/817]	Loss 0.1710 (0.2108)	
training:	Epoch: [7][301/817]	Loss 0.0707 (0.2103)	
training:	Epoch: [7][302/817]	Loss 0.0407 (0.2098)	
training:	Epoch: [7][303/817]	Loss 0.0565 (0.2093)	
training:	Epoch: [7][304/817]	Loss 0.1544 (0.2091)	
training:	Epoch: [7][305/817]	Loss 0.2120 (0.2091)	
training:	Epoch: [7][306/817]	Loss 0.3495 (0.2095)	
training:	Epoch: [7][307/817]	Loss 0.1667 (0.2094)	
training:	Epoch: [7][308/817]	Loss 0.6603 (0.2109)	
training:	Epoch: [7][309/817]	Loss 0.1221 (0.2106)	
training:	Epoch: [7][310/817]	Loss 0.0491 (0.2101)	
training:	Epoch: [7][311/817]	Loss 0.0591 (0.2096)	
training:	Epoch: [7][312/817]	Loss 0.6214 (0.2109)	
training:	Epoch: [7][313/817]	Loss 0.1408 (0.2107)	
training:	Epoch: [7][314/817]	Loss 0.6224 (0.2120)	
training:	Epoch: [7][315/817]	Loss 0.0369 (0.2114)	
training:	Epoch: [7][316/817]	Loss 0.1244 (0.2111)	
training:	Epoch: [7][317/817]	Loss 0.5729 (0.2123)	
training:	Epoch: [7][318/817]	Loss 0.0975 (0.2119)	
training:	Epoch: [7][319/817]	Loss 0.1024 (0.2116)	
training:	Epoch: [7][320/817]	Loss 0.1211 (0.2113)	
training:	Epoch: [7][321/817]	Loss 0.5041 (0.2122)	
training:	Epoch: [7][322/817]	Loss 0.0552 (0.2117)	
training:	Epoch: [7][323/817]	Loss 0.1177 (0.2114)	
training:	Epoch: [7][324/817]	Loss 0.0996 (0.2111)	
training:	Epoch: [7][325/817]	Loss 0.0568 (0.2106)	
training:	Epoch: [7][326/817]	Loss 0.2798 (0.2108)	
training:	Epoch: [7][327/817]	Loss 0.4882 (0.2117)	
training:	Epoch: [7][328/817]	Loss 0.4942 (0.2125)	
training:	Epoch: [7][329/817]	Loss 0.3176 (0.2129)	
training:	Epoch: [7][330/817]	Loss 0.3359 (0.2132)	
training:	Epoch: [7][331/817]	Loss 0.4128 (0.2138)	
training:	Epoch: [7][332/817]	Loss 0.0642 (0.2134)	
training:	Epoch: [7][333/817]	Loss 0.2664 (0.2135)	
training:	Epoch: [7][334/817]	Loss 0.1585 (0.2134)	
training:	Epoch: [7][335/817]	Loss 0.0324 (0.2128)	
training:	Epoch: [7][336/817]	Loss 0.0731 (0.2124)	
training:	Epoch: [7][337/817]	Loss 0.0756 (0.2120)	
training:	Epoch: [7][338/817]	Loss 0.1363 (0.2118)	
training:	Epoch: [7][339/817]	Loss 0.6317 (0.2130)	
training:	Epoch: [7][340/817]	Loss 0.0521 (0.2126)	
training:	Epoch: [7][341/817]	Loss 0.1676 (0.2124)	
training:	Epoch: [7][342/817]	Loss 0.0410 (0.2119)	
training:	Epoch: [7][343/817]	Loss 0.0385 (0.2114)	
training:	Epoch: [7][344/817]	Loss 0.0966 (0.2111)	
training:	Epoch: [7][345/817]	Loss 0.0437 (0.2106)	
training:	Epoch: [7][346/817]	Loss 0.3997 (0.2111)	
training:	Epoch: [7][347/817]	Loss 0.0530 (0.2107)	
training:	Epoch: [7][348/817]	Loss 0.3641 (0.2111)	
training:	Epoch: [7][349/817]	Loss 0.2780 (0.2113)	
training:	Epoch: [7][350/817]	Loss 0.0371 (0.2108)	
training:	Epoch: [7][351/817]	Loss 0.5280 (0.2117)	
training:	Epoch: [7][352/817]	Loss 0.2431 (0.2118)	
training:	Epoch: [7][353/817]	Loss 0.4025 (0.2124)	
training:	Epoch: [7][354/817]	Loss 0.1200 (0.2121)	
training:	Epoch: [7][355/817]	Loss 0.0457 (0.2116)	
training:	Epoch: [7][356/817]	Loss 0.1760 (0.2115)	
training:	Epoch: [7][357/817]	Loss 0.7073 (0.2129)	
training:	Epoch: [7][358/817]	Loss 0.4890 (0.2137)	
training:	Epoch: [7][359/817]	Loss 0.0842 (0.2133)	
training:	Epoch: [7][360/817]	Loss 0.0573 (0.2129)	
training:	Epoch: [7][361/817]	Loss 0.0762 (0.2125)	
training:	Epoch: [7][362/817]	Loss 0.6242 (0.2136)	
training:	Epoch: [7][363/817]	Loss 0.0917 (0.2133)	
training:	Epoch: [7][364/817]	Loss 0.0495 (0.2129)	
training:	Epoch: [7][365/817]	Loss 0.0554 (0.2124)	
training:	Epoch: [7][366/817]	Loss 0.4937 (0.2132)	
training:	Epoch: [7][367/817]	Loss 0.0678 (0.2128)	
training:	Epoch: [7][368/817]	Loss 0.0353 (0.2123)	
training:	Epoch: [7][369/817]	Loss 0.0447 (0.2119)	
training:	Epoch: [7][370/817]	Loss 0.0656 (0.2115)	
training:	Epoch: [7][371/817]	Loss 0.0611 (0.2111)	
training:	Epoch: [7][372/817]	Loss 0.0982 (0.2108)	
training:	Epoch: [7][373/817]	Loss 0.1487 (0.2106)	
training:	Epoch: [7][374/817]	Loss 0.3258 (0.2109)	
training:	Epoch: [7][375/817]	Loss 0.5568 (0.2118)	
training:	Epoch: [7][376/817]	Loss 0.1049 (0.2115)	
training:	Epoch: [7][377/817]	Loss 0.0682 (0.2112)	
training:	Epoch: [7][378/817]	Loss 0.1471 (0.2110)	
training:	Epoch: [7][379/817]	Loss 0.5535 (0.2119)	
training:	Epoch: [7][380/817]	Loss 0.0419 (0.2114)	
training:	Epoch: [7][381/817]	Loss 0.0645 (0.2111)	
training:	Epoch: [7][382/817]	Loss 0.0473 (0.2106)	
training:	Epoch: [7][383/817]	Loss 0.2003 (0.2106)	
training:	Epoch: [7][384/817]	Loss 0.5826 (0.2116)	
training:	Epoch: [7][385/817]	Loss 0.1302 (0.2114)	
training:	Epoch: [7][386/817]	Loss 0.0845 (0.2110)	
training:	Epoch: [7][387/817]	Loss 0.3403 (0.2114)	
training:	Epoch: [7][388/817]	Loss 0.0717 (0.2110)	
training:	Epoch: [7][389/817]	Loss 0.0507 (0.2106)	
training:	Epoch: [7][390/817]	Loss 0.2418 (0.2107)	
training:	Epoch: [7][391/817]	Loss 0.1036 (0.2104)	
training:	Epoch: [7][392/817]	Loss 0.0781 (0.2101)	
training:	Epoch: [7][393/817]	Loss 0.2637 (0.2102)	
training:	Epoch: [7][394/817]	Loss 0.4034 (0.2107)	
training:	Epoch: [7][395/817]	Loss 0.8222 (0.2122)	
training:	Epoch: [7][396/817]	Loss 0.0564 (0.2118)	
training:	Epoch: [7][397/817]	Loss 0.0958 (0.2116)	
training:	Epoch: [7][398/817]	Loss 0.5724 (0.2125)	
training:	Epoch: [7][399/817]	Loss 0.0354 (0.2120)	
training:	Epoch: [7][400/817]	Loss 0.1192 (0.2118)	
training:	Epoch: [7][401/817]	Loss 0.1543 (0.2116)	
training:	Epoch: [7][402/817]	Loss 0.3634 (0.2120)	
training:	Epoch: [7][403/817]	Loss 0.0629 (0.2117)	
training:	Epoch: [7][404/817]	Loss 0.0932 (0.2114)	
training:	Epoch: [7][405/817]	Loss 0.0304 (0.2109)	
training:	Epoch: [7][406/817]	Loss 0.2881 (0.2111)	
training:	Epoch: [7][407/817]	Loss 0.0490 (0.2107)	
training:	Epoch: [7][408/817]	Loss 0.1224 (0.2105)	
training:	Epoch: [7][409/817]	Loss 0.5914 (0.2114)	
training:	Epoch: [7][410/817]	Loss 0.1441 (0.2113)	
training:	Epoch: [7][411/817]	Loss 0.3763 (0.2117)	
training:	Epoch: [7][412/817]	Loss 0.3686 (0.2120)	
training:	Epoch: [7][413/817]	Loss 0.2116 (0.2120)	
training:	Epoch: [7][414/817]	Loss 0.0311 (0.2116)	
training:	Epoch: [7][415/817]	Loss 0.0733 (0.2113)	
training:	Epoch: [7][416/817]	Loss 0.3395 (0.2116)	
training:	Epoch: [7][417/817]	Loss 0.1277 (0.2114)	
training:	Epoch: [7][418/817]	Loss 0.1090 (0.2111)	
training:	Epoch: [7][419/817]	Loss 0.0384 (0.2107)	
training:	Epoch: [7][420/817]	Loss 0.0603 (0.2104)	
training:	Epoch: [7][421/817]	Loss 0.3858 (0.2108)	
training:	Epoch: [7][422/817]	Loss 0.4739 (0.2114)	
training:	Epoch: [7][423/817]	Loss 0.0799 (0.2111)	
training:	Epoch: [7][424/817]	Loss 0.0449 (0.2107)	
training:	Epoch: [7][425/817]	Loss 0.3065 (0.2109)	
training:	Epoch: [7][426/817]	Loss 0.0641 (0.2106)	
training:	Epoch: [7][427/817]	Loss 0.0533 (0.2102)	
training:	Epoch: [7][428/817]	Loss 0.5670 (0.2110)	
training:	Epoch: [7][429/817]	Loss 0.0420 (0.2106)	
training:	Epoch: [7][430/817]	Loss 0.1150 (0.2104)	
training:	Epoch: [7][431/817]	Loss 0.4348 (0.2109)	
training:	Epoch: [7][432/817]	Loss 0.0493 (0.2106)	
training:	Epoch: [7][433/817]	Loss 0.0403 (0.2102)	
training:	Epoch: [7][434/817]	Loss 0.1191 (0.2100)	
training:	Epoch: [7][435/817]	Loss 0.5249 (0.2107)	
training:	Epoch: [7][436/817]	Loss 0.8429 (0.2121)	
training:	Epoch: [7][437/817]	Loss 0.2539 (0.2122)	
training:	Epoch: [7][438/817]	Loss 0.1222 (0.2120)	
training:	Epoch: [7][439/817]	Loss 0.0826 (0.2117)	
training:	Epoch: [7][440/817]	Loss 0.2861 (0.2119)	
training:	Epoch: [7][441/817]	Loss 0.0331 (0.2115)	
training:	Epoch: [7][442/817]	Loss 0.1828 (0.2114)	
training:	Epoch: [7][443/817]	Loss 0.4399 (0.2120)	
training:	Epoch: [7][444/817]	Loss 0.2166 (0.2120)	
training:	Epoch: [7][445/817]	Loss 0.2722 (0.2121)	
training:	Epoch: [7][446/817]	Loss 0.0686 (0.2118)	
training:	Epoch: [7][447/817]	Loss 0.1567 (0.2117)	
training:	Epoch: [7][448/817]	Loss 0.1522 (0.2115)	
training:	Epoch: [7][449/817]	Loss 0.0413 (0.2111)	
training:	Epoch: [7][450/817]	Loss 0.3121 (0.2114)	
training:	Epoch: [7][451/817]	Loss 0.1993 (0.2113)	
training:	Epoch: [7][452/817]	Loss 0.0364 (0.2110)	
training:	Epoch: [7][453/817]	Loss 0.0693 (0.2106)	
training:	Epoch: [7][454/817]	Loss 0.0779 (0.2103)	
training:	Epoch: [7][455/817]	Loss 0.4790 (0.2109)	
training:	Epoch: [7][456/817]	Loss 0.0300 (0.2105)	
training:	Epoch: [7][457/817]	Loss 0.1535 (0.2104)	
training:	Epoch: [7][458/817]	Loss 0.2444 (0.2105)	
training:	Epoch: [7][459/817]	Loss 0.1170 (0.2103)	
training:	Epoch: [7][460/817]	Loss 0.0564 (0.2100)	
training:	Epoch: [7][461/817]	Loss 0.0531 (0.2096)	
training:	Epoch: [7][462/817]	Loss 0.6478 (0.2106)	
training:	Epoch: [7][463/817]	Loss 0.3558 (0.2109)	
training:	Epoch: [7][464/817]	Loss 0.1517 (0.2107)	
training:	Epoch: [7][465/817]	Loss 0.2446 (0.2108)	
training:	Epoch: [7][466/817]	Loss 0.0638 (0.2105)	
training:	Epoch: [7][467/817]	Loss 0.5439 (0.2112)	
training:	Epoch: [7][468/817]	Loss 0.5152 (0.2119)	
training:	Epoch: [7][469/817]	Loss 0.2804 (0.2120)	
training:	Epoch: [7][470/817]	Loss 0.0367 (0.2116)	
training:	Epoch: [7][471/817]	Loss 0.6225 (0.2125)	
training:	Epoch: [7][472/817]	Loss 0.6418 (0.2134)	
training:	Epoch: [7][473/817]	Loss 0.1439 (0.2133)	
training:	Epoch: [7][474/817]	Loss 0.1249 (0.2131)	
training:	Epoch: [7][475/817]	Loss 0.2211 (0.2131)	
training:	Epoch: [7][476/817]	Loss 0.0336 (0.2127)	
training:	Epoch: [7][477/817]	Loss 0.1856 (0.2127)	
training:	Epoch: [7][478/817]	Loss 0.1074 (0.2125)	
training:	Epoch: [7][479/817]	Loss 0.1137 (0.2122)	
training:	Epoch: [7][480/817]	Loss 0.1758 (0.2122)	
training:	Epoch: [7][481/817]	Loss 0.0663 (0.2119)	
training:	Epoch: [7][482/817]	Loss 0.2147 (0.2119)	
training:	Epoch: [7][483/817]	Loss 0.5603 (0.2126)	
training:	Epoch: [7][484/817]	Loss 0.2673 (0.2127)	
training:	Epoch: [7][485/817]	Loss 0.4865 (0.2133)	
training:	Epoch: [7][486/817]	Loss 0.2842 (0.2134)	
training:	Epoch: [7][487/817]	Loss 0.0995 (0.2132)	
training:	Epoch: [7][488/817]	Loss 0.0674 (0.2129)	
training:	Epoch: [7][489/817]	Loss 0.0463 (0.2125)	
training:	Epoch: [7][490/817]	Loss 0.1246 (0.2124)	
training:	Epoch: [7][491/817]	Loss 0.1903 (0.2123)	
training:	Epoch: [7][492/817]	Loss 0.1228 (0.2121)	
training:	Epoch: [7][493/817]	Loss 0.3961 (0.2125)	
training:	Epoch: [7][494/817]	Loss 0.0794 (0.2122)	
training:	Epoch: [7][495/817]	Loss 0.8155 (0.2135)	
training:	Epoch: [7][496/817]	Loss 0.1336 (0.2133)	
training:	Epoch: [7][497/817]	Loss 0.3189 (0.2135)	
training:	Epoch: [7][498/817]	Loss 0.1344 (0.2134)	
training:	Epoch: [7][499/817]	Loss 0.4719 (0.2139)	
training:	Epoch: [7][500/817]	Loss 0.1028 (0.2136)	
training:	Epoch: [7][501/817]	Loss 0.2001 (0.2136)	
training:	Epoch: [7][502/817]	Loss 0.2366 (0.2137)	
training:	Epoch: [7][503/817]	Loss 0.2656 (0.2138)	
training:	Epoch: [7][504/817]	Loss 0.3418 (0.2140)	
training:	Epoch: [7][505/817]	Loss 0.7035 (0.2150)	
training:	Epoch: [7][506/817]	Loss 0.1181 (0.2148)	
training:	Epoch: [7][507/817]	Loss 1.0599 (0.2165)	
training:	Epoch: [7][508/817]	Loss 0.1030 (0.2162)	
training:	Epoch: [7][509/817]	Loss 0.0795 (0.2160)	
training:	Epoch: [7][510/817]	Loss 0.0694 (0.2157)	
training:	Epoch: [7][511/817]	Loss 0.4166 (0.2161)	
training:	Epoch: [7][512/817]	Loss 0.0516 (0.2158)	
training:	Epoch: [7][513/817]	Loss 0.2134 (0.2158)	
training:	Epoch: [7][514/817]	Loss 0.0422 (0.2154)	
training:	Epoch: [7][515/817]	Loss 0.7188 (0.2164)	
training:	Epoch: [7][516/817]	Loss 0.0949 (0.2162)	
training:	Epoch: [7][517/817]	Loss 0.0954 (0.2159)	
training:	Epoch: [7][518/817]	Loss 0.1207 (0.2157)	
training:	Epoch: [7][519/817]	Loss 0.3026 (0.2159)	
training:	Epoch: [7][520/817]	Loss 0.3133 (0.2161)	
training:	Epoch: [7][521/817]	Loss 0.4014 (0.2165)	
training:	Epoch: [7][522/817]	Loss 0.9506 (0.2179)	
training:	Epoch: [7][523/817]	Loss 0.2960 (0.2180)	
training:	Epoch: [7][524/817]	Loss 0.2928 (0.2182)	
training:	Epoch: [7][525/817]	Loss 0.4752 (0.2186)	
training:	Epoch: [7][526/817]	Loss 0.1374 (0.2185)	
training:	Epoch: [7][527/817]	Loss 0.0368 (0.2181)	
training:	Epoch: [7][528/817]	Loss 0.0937 (0.2179)	
training:	Epoch: [7][529/817]	Loss 0.0479 (0.2176)	
training:	Epoch: [7][530/817]	Loss 0.3692 (0.2179)	
training:	Epoch: [7][531/817]	Loss 0.0464 (0.2175)	
training:	Epoch: [7][532/817]	Loss 0.5344 (0.2181)	
training:	Epoch: [7][533/817]	Loss 0.0503 (0.2178)	
training:	Epoch: [7][534/817]	Loss 0.3736 (0.2181)	
training:	Epoch: [7][535/817]	Loss 0.0486 (0.2178)	
training:	Epoch: [7][536/817]	Loss 0.5009 (0.2183)	
training:	Epoch: [7][537/817]	Loss 0.2704 (0.2184)	
training:	Epoch: [7][538/817]	Loss 0.1555 (0.2183)	
training:	Epoch: [7][539/817]	Loss 0.4909 (0.2188)	
training:	Epoch: [7][540/817]	Loss 1.0680 (0.2204)	
training:	Epoch: [7][541/817]	Loss 0.0934 (0.2202)	
training:	Epoch: [7][542/817]	Loss 0.1286 (0.2200)	
training:	Epoch: [7][543/817]	Loss 0.0878 (0.2197)	
training:	Epoch: [7][544/817]	Loss 0.5189 (0.2203)	
training:	Epoch: [7][545/817]	Loss 0.3680 (0.2206)	
training:	Epoch: [7][546/817]	Loss 0.1421 (0.2204)	
training:	Epoch: [7][547/817]	Loss 0.0633 (0.2201)	
training:	Epoch: [7][548/817]	Loss 0.1317 (0.2200)	
training:	Epoch: [7][549/817]	Loss 0.0598 (0.2197)	
training:	Epoch: [7][550/817]	Loss 0.2126 (0.2197)	
training:	Epoch: [7][551/817]	Loss 0.0792 (0.2194)	
training:	Epoch: [7][552/817]	Loss 0.4025 (0.2197)	
training:	Epoch: [7][553/817]	Loss 0.1240 (0.2196)	
training:	Epoch: [7][554/817]	Loss 0.7282 (0.2205)	
training:	Epoch: [7][555/817]	Loss 0.1072 (0.2203)	
training:	Epoch: [7][556/817]	Loss 0.0787 (0.2200)	
training:	Epoch: [7][557/817]	Loss 0.2004 (0.2200)	
training:	Epoch: [7][558/817]	Loss 0.3353 (0.2202)	
training:	Epoch: [7][559/817]	Loss 0.0765 (0.2199)	
training:	Epoch: [7][560/817]	Loss 0.5842 (0.2206)	
training:	Epoch: [7][561/817]	Loss 0.1938 (0.2205)	
training:	Epoch: [7][562/817]	Loss 0.0450 (0.2202)	
training:	Epoch: [7][563/817]	Loss 0.2784 (0.2203)	
training:	Epoch: [7][564/817]	Loss 0.1578 (0.2202)	
training:	Epoch: [7][565/817]	Loss 0.1044 (0.2200)	
training:	Epoch: [7][566/817]	Loss 0.1239 (0.2199)	
training:	Epoch: [7][567/817]	Loss 0.2650 (0.2199)	
training:	Epoch: [7][568/817]	Loss 0.2989 (0.2201)	
training:	Epoch: [7][569/817]	Loss 0.0947 (0.2199)	
training:	Epoch: [7][570/817]	Loss 0.2527 (0.2199)	
training:	Epoch: [7][571/817]	Loss 0.1463 (0.2198)	
training:	Epoch: [7][572/817]	Loss 0.1623 (0.2197)	
training:	Epoch: [7][573/817]	Loss 0.3928 (0.2200)	
training:	Epoch: [7][574/817]	Loss 0.5413 (0.2205)	
training:	Epoch: [7][575/817]	Loss 0.0538 (0.2203)	
training:	Epoch: [7][576/817]	Loss 0.0330 (0.2199)	
training:	Epoch: [7][577/817]	Loss 0.0640 (0.2197)	
training:	Epoch: [7][578/817]	Loss 0.5821 (0.2203)	
training:	Epoch: [7][579/817]	Loss 0.2575 (0.2203)	
training:	Epoch: [7][580/817]	Loss 0.4804 (0.2208)	
training:	Epoch: [7][581/817]	Loss 0.2117 (0.2208)	
training:	Epoch: [7][582/817]	Loss 0.1339 (0.2206)	
training:	Epoch: [7][583/817]	Loss 0.5513 (0.2212)	
training:	Epoch: [7][584/817]	Loss 0.1751 (0.2211)	
training:	Epoch: [7][585/817]	Loss 0.1027 (0.2209)	
training:	Epoch: [7][586/817]	Loss 0.1131 (0.2207)	
training:	Epoch: [7][587/817]	Loss 0.0988 (0.2205)	
training:	Epoch: [7][588/817]	Loss 0.3649 (0.2208)	
training:	Epoch: [7][589/817]	Loss 0.5510 (0.2213)	
training:	Epoch: [7][590/817]	Loss 0.1984 (0.2213)	
training:	Epoch: [7][591/817]	Loss 0.1915 (0.2212)	
training:	Epoch: [7][592/817]	Loss 0.2691 (0.2213)	
training:	Epoch: [7][593/817]	Loss 0.0794 (0.2211)	
training:	Epoch: [7][594/817]	Loss 0.0709 (0.2208)	
training:	Epoch: [7][595/817]	Loss 0.0836 (0.2206)	
training:	Epoch: [7][596/817]	Loss 0.2146 (0.2206)	
training:	Epoch: [7][597/817]	Loss 0.6855 (0.2214)	
training:	Epoch: [7][598/817]	Loss 0.0808 (0.2211)	
training:	Epoch: [7][599/817]	Loss 0.7062 (0.2219)	
training:	Epoch: [7][600/817]	Loss 0.0273 (0.2216)	
training:	Epoch: [7][601/817]	Loss 0.1443 (0.2215)	
training:	Epoch: [7][602/817]	Loss 0.0836 (0.2213)	
training:	Epoch: [7][603/817]	Loss 0.1087 (0.2211)	
training:	Epoch: [7][604/817]	Loss 0.4174 (0.2214)	
training:	Epoch: [7][605/817]	Loss 0.1400 (0.2213)	
training:	Epoch: [7][606/817]	Loss 0.1203 (0.2211)	
training:	Epoch: [7][607/817]	Loss 0.0920 (0.2209)	
training:	Epoch: [7][608/817]	Loss 0.4547 (0.2213)	
training:	Epoch: [7][609/817]	Loss 0.2059 (0.2212)	
training:	Epoch: [7][610/817]	Loss 0.0804 (0.2210)	
training:	Epoch: [7][611/817]	Loss 0.1106 (0.2208)	
training:	Epoch: [7][612/817]	Loss 0.1594 (0.2207)	
training:	Epoch: [7][613/817]	Loss 0.4580 (0.2211)	
training:	Epoch: [7][614/817]	Loss 0.1718 (0.2210)	
training:	Epoch: [7][615/817]	Loss 0.0833 (0.2208)	
training:	Epoch: [7][616/817]	Loss 0.1634 (0.2207)	
training:	Epoch: [7][617/817]	Loss 0.0390 (0.2204)	
training:	Epoch: [7][618/817]	Loss 0.0974 (0.2202)	
training:	Epoch: [7][619/817]	Loss 0.1120 (0.2201)	
training:	Epoch: [7][620/817]	Loss 0.2574 (0.2201)	
training:	Epoch: [7][621/817]	Loss 0.3826 (0.2204)	
training:	Epoch: [7][622/817]	Loss 0.0404 (0.2201)	
training:	Epoch: [7][623/817]	Loss 0.7310 (0.2209)	
training:	Epoch: [7][624/817]	Loss 0.0855 (0.2207)	
training:	Epoch: [7][625/817]	Loss 0.0773 (0.2205)	
training:	Epoch: [7][626/817]	Loss 0.1015 (0.2203)	
training:	Epoch: [7][627/817]	Loss 0.2383 (0.2203)	
training:	Epoch: [7][628/817]	Loss 0.3972 (0.2206)	
training:	Epoch: [7][629/817]	Loss 0.3557 (0.2208)	
training:	Epoch: [7][630/817]	Loss 0.2738 (0.2209)	
training:	Epoch: [7][631/817]	Loss 0.1048 (0.2207)	
training:	Epoch: [7][632/817]	Loss 0.3154 (0.2208)	
training:	Epoch: [7][633/817]	Loss 0.2942 (0.2210)	
training:	Epoch: [7][634/817]	Loss 0.6481 (0.2216)	
training:	Epoch: [7][635/817]	Loss 0.1408 (0.2215)	
training:	Epoch: [7][636/817]	Loss 0.0678 (0.2213)	
training:	Epoch: [7][637/817]	Loss 0.1321 (0.2211)	
training:	Epoch: [7][638/817]	Loss 0.1611 (0.2210)	
training:	Epoch: [7][639/817]	Loss 0.4516 (0.2214)	
training:	Epoch: [7][640/817]	Loss 0.1871 (0.2213)	
training:	Epoch: [7][641/817]	Loss 0.1200 (0.2212)	
training:	Epoch: [7][642/817]	Loss 0.1520 (0.2211)	
training:	Epoch: [7][643/817]	Loss 0.1391 (0.2209)	
training:	Epoch: [7][644/817]	Loss 0.0757 (0.2207)	
training:	Epoch: [7][645/817]	Loss 0.0544 (0.2205)	
training:	Epoch: [7][646/817]	Loss 0.3016 (0.2206)	
training:	Epoch: [7][647/817]	Loss 0.0662 (0.2203)	
training:	Epoch: [7][648/817]	Loss 0.0826 (0.2201)	
training:	Epoch: [7][649/817]	Loss 0.1865 (0.2201)	
training:	Epoch: [7][650/817]	Loss 0.2112 (0.2201)	
training:	Epoch: [7][651/817]	Loss 0.1741 (0.2200)	
training:	Epoch: [7][652/817]	Loss 0.0481 (0.2197)	
training:	Epoch: [7][653/817]	Loss 0.1386 (0.2196)	
training:	Epoch: [7][654/817]	Loss 0.2060 (0.2196)	
training:	Epoch: [7][655/817]	Loss 0.0955 (0.2194)	
training:	Epoch: [7][656/817]	Loss 0.1361 (0.2193)	
training:	Epoch: [7][657/817]	Loss 0.0741 (0.2191)	
training:	Epoch: [7][658/817]	Loss 0.1873 (0.2190)	
training:	Epoch: [7][659/817]	Loss 0.0437 (0.2187)	
training:	Epoch: [7][660/817]	Loss 0.1218 (0.2186)	
training:	Epoch: [7][661/817]	Loss 0.2159 (0.2186)	
training:	Epoch: [7][662/817]	Loss 0.0536 (0.2183)	
training:	Epoch: [7][663/817]	Loss 0.2406 (0.2184)	
training:	Epoch: [7][664/817]	Loss 0.0447 (0.2181)	
training:	Epoch: [7][665/817]	Loss 0.1230 (0.2180)	
training:	Epoch: [7][666/817]	Loss 0.2025 (0.2179)	
training:	Epoch: [7][667/817]	Loss 0.0660 (0.2177)	
training:	Epoch: [7][668/817]	Loss 0.3291 (0.2179)	
training:	Epoch: [7][669/817]	Loss 0.3340 (0.2181)	
training:	Epoch: [7][670/817]	Loss 0.0831 (0.2179)	
training:	Epoch: [7][671/817]	Loss 0.6151 (0.2184)	
training:	Epoch: [7][672/817]	Loss 0.5410 (0.2189)	
training:	Epoch: [7][673/817]	Loss 0.0253 (0.2186)	
training:	Epoch: [7][674/817]	Loss 0.0353 (0.2184)	
training:	Epoch: [7][675/817]	Loss 0.0344 (0.2181)	
training:	Epoch: [7][676/817]	Loss 0.0371 (0.2178)	
training:	Epoch: [7][677/817]	Loss 0.0540 (0.2176)	
training:	Epoch: [7][678/817]	Loss 0.0978 (0.2174)	
training:	Epoch: [7][679/817]	Loss 0.1965 (0.2174)	
training:	Epoch: [7][680/817]	Loss 0.3828 (0.2176)	
training:	Epoch: [7][681/817]	Loss 0.0369 (0.2174)	
training:	Epoch: [7][682/817]	Loss 1.0313 (0.2185)	
training:	Epoch: [7][683/817]	Loss 0.1517 (0.2185)	
training:	Epoch: [7][684/817]	Loss 0.0819 (0.2183)	
training:	Epoch: [7][685/817]	Loss 0.2865 (0.2184)	
training:	Epoch: [7][686/817]	Loss 0.0491 (0.2181)	
training:	Epoch: [7][687/817]	Loss 0.0562 (0.2179)	
training:	Epoch: [7][688/817]	Loss 0.0788 (0.2177)	
training:	Epoch: [7][689/817]	Loss 0.1079 (0.2175)	
training:	Epoch: [7][690/817]	Loss 0.4527 (0.2178)	
training:	Epoch: [7][691/817]	Loss 0.0568 (0.2176)	
training:	Epoch: [7][692/817]	Loss 0.0267 (0.2173)	
training:	Epoch: [7][693/817]	Loss 0.5626 (0.2178)	
training:	Epoch: [7][694/817]	Loss 0.1911 (0.2178)	
training:	Epoch: [7][695/817]	Loss 0.2332 (0.2178)	
training:	Epoch: [7][696/817]	Loss 0.0333 (0.2176)	
training:	Epoch: [7][697/817]	Loss 0.0671 (0.2173)	
training:	Epoch: [7][698/817]	Loss 0.1983 (0.2173)	
training:	Epoch: [7][699/817]	Loss 0.3495 (0.2175)	
training:	Epoch: [7][700/817]	Loss 0.2742 (0.2176)	
training:	Epoch: [7][701/817]	Loss 0.1384 (0.2175)	
training:	Epoch: [7][702/817]	Loss 0.3943 (0.2177)	
training:	Epoch: [7][703/817]	Loss 0.0537 (0.2175)	
training:	Epoch: [7][704/817]	Loss 0.0795 (0.2173)	
training:	Epoch: [7][705/817]	Loss 0.9474 (0.2183)	
training:	Epoch: [7][706/817]	Loss 0.0694 (0.2181)	
training:	Epoch: [7][707/817]	Loss 0.5198 (0.2185)	
training:	Epoch: [7][708/817]	Loss 0.1811 (0.2185)	
training:	Epoch: [7][709/817]	Loss 0.0906 (0.2183)	
training:	Epoch: [7][710/817]	Loss 0.2552 (0.2184)	
training:	Epoch: [7][711/817]	Loss 0.5208 (0.2188)	
training:	Epoch: [7][712/817]	Loss 0.3067 (0.2189)	
training:	Epoch: [7][713/817]	Loss 0.0309 (0.2186)	
training:	Epoch: [7][714/817]	Loss 0.1853 (0.2186)	
training:	Epoch: [7][715/817]	Loss 0.1946 (0.2186)	
training:	Epoch: [7][716/817]	Loss 0.0440 (0.2183)	
training:	Epoch: [7][717/817]	Loss 0.2550 (0.2184)	
training:	Epoch: [7][718/817]	Loss 0.0529 (0.2181)	
training:	Epoch: [7][719/817]	Loss 0.5946 (0.2187)	
training:	Epoch: [7][720/817]	Loss 0.7118 (0.2194)	
training:	Epoch: [7][721/817]	Loss 0.1074 (0.2192)	
training:	Epoch: [7][722/817]	Loss 0.0492 (0.2190)	
training:	Epoch: [7][723/817]	Loss 1.0686 (0.2201)	
training:	Epoch: [7][724/817]	Loss 0.3660 (0.2203)	
training:	Epoch: [7][725/817]	Loss 0.0653 (0.2201)	
training:	Epoch: [7][726/817]	Loss 0.4865 (0.2205)	
training:	Epoch: [7][727/817]	Loss 0.0539 (0.2203)	
training:	Epoch: [7][728/817]	Loss 0.2937 (0.2204)	
training:	Epoch: [7][729/817]	Loss 0.4429 (0.2207)	
training:	Epoch: [7][730/817]	Loss 0.0853 (0.2205)	
training:	Epoch: [7][731/817]	Loss 0.4521 (0.2208)	
training:	Epoch: [7][732/817]	Loss 0.5567 (0.2213)	
training:	Epoch: [7][733/817]	Loss 0.0541 (0.2210)	
training:	Epoch: [7][734/817]	Loss 0.6748 (0.2217)	
training:	Epoch: [7][735/817]	Loss 0.0974 (0.2215)	
training:	Epoch: [7][736/817]	Loss 0.0342 (0.2212)	
training:	Epoch: [7][737/817]	Loss 0.2053 (0.2212)	
training:	Epoch: [7][738/817]	Loss 0.0690 (0.2210)	
training:	Epoch: [7][739/817]	Loss 1.5435 (0.2228)	
training:	Epoch: [7][740/817]	Loss 0.1112 (0.2226)	
training:	Epoch: [7][741/817]	Loss 0.1517 (0.2225)	
training:	Epoch: [7][742/817]	Loss 0.0523 (0.2223)	
training:	Epoch: [7][743/817]	Loss 0.7264 (0.2230)	
training:	Epoch: [7][744/817]	Loss 0.1477 (0.2229)	
training:	Epoch: [7][745/817]	Loss 0.0620 (0.2227)	
training:	Epoch: [7][746/817]	Loss 0.4773 (0.2230)	
training:	Epoch: [7][747/817]	Loss 0.1288 (0.2229)	
training:	Epoch: [7][748/817]	Loss 0.0599 (0.2227)	
training:	Epoch: [7][749/817]	Loss 0.1100 (0.2225)	
training:	Epoch: [7][750/817]	Loss 0.4199 (0.2228)	
training:	Epoch: [7][751/817]	Loss 0.1353 (0.2227)	
training:	Epoch: [7][752/817]	Loss 0.0669 (0.2225)	
training:	Epoch: [7][753/817]	Loss 0.0939 (0.2223)	
training:	Epoch: [7][754/817]	Loss 0.0474 (0.2221)	
training:	Epoch: [7][755/817]	Loss 0.6142 (0.2226)	
training:	Epoch: [7][756/817]	Loss 0.0879 (0.2224)	
training:	Epoch: [7][757/817]	Loss 0.4399 (0.2227)	
training:	Epoch: [7][758/817]	Loss 0.2077 (0.2227)	
training:	Epoch: [7][759/817]	Loss 0.2134 (0.2227)	
training:	Epoch: [7][760/817]	Loss 0.0625 (0.2224)	
training:	Epoch: [7][761/817]	Loss 0.0570 (0.2222)	
training:	Epoch: [7][762/817]	Loss 0.2634 (0.2223)	
training:	Epoch: [7][763/817]	Loss 0.4546 (0.2226)	
training:	Epoch: [7][764/817]	Loss 0.5201 (0.2230)	
training:	Epoch: [7][765/817]	Loss 0.0583 (0.2228)	
training:	Epoch: [7][766/817]	Loss 0.0398 (0.2225)	
training:	Epoch: [7][767/817]	Loss 0.6025 (0.2230)	
training:	Epoch: [7][768/817]	Loss 0.0937 (0.2228)	
training:	Epoch: [7][769/817]	Loss 0.2405 (0.2229)	
training:	Epoch: [7][770/817]	Loss 0.0559 (0.2227)	
training:	Epoch: [7][771/817]	Loss 0.0513 (0.2224)	
training:	Epoch: [7][772/817]	Loss 0.0629 (0.2222)	
training:	Epoch: [7][773/817]	Loss 0.1006 (0.2221)	
training:	Epoch: [7][774/817]	Loss 0.3980 (0.2223)	
training:	Epoch: [7][775/817]	Loss 0.3155 (0.2224)	
training:	Epoch: [7][776/817]	Loss 0.0364 (0.2222)	
training:	Epoch: [7][777/817]	Loss 0.5153 (0.2226)	
training:	Epoch: [7][778/817]	Loss 0.2774 (0.2226)	
training:	Epoch: [7][779/817]	Loss 0.0447 (0.2224)	
training:	Epoch: [7][780/817]	Loss 0.0623 (0.2222)	
training:	Epoch: [7][781/817]	Loss 0.1935 (0.2222)	
training:	Epoch: [7][782/817]	Loss 0.2222 (0.2222)	
training:	Epoch: [7][783/817]	Loss 0.6416 (0.2227)	
training:	Epoch: [7][784/817]	Loss 0.0509 (0.2225)	
training:	Epoch: [7][785/817]	Loss 0.0610 (0.2223)	
training:	Epoch: [7][786/817]	Loss 0.3768 (0.2225)	
training:	Epoch: [7][787/817]	Loss 0.4486 (0.2227)	
training:	Epoch: [7][788/817]	Loss 0.0943 (0.2226)	
training:	Epoch: [7][789/817]	Loss 0.3893 (0.2228)	
training:	Epoch: [7][790/817]	Loss 0.1911 (0.2228)	
training:	Epoch: [7][791/817]	Loss 0.0481 (0.2225)	
training:	Epoch: [7][792/817]	Loss 0.0406 (0.2223)	
training:	Epoch: [7][793/817]	Loss 0.2194 (0.2223)	
training:	Epoch: [7][794/817]	Loss 0.0471 (0.2221)	
training:	Epoch: [7][795/817]	Loss 0.0828 (0.2219)	
training:	Epoch: [7][796/817]	Loss 0.1040 (0.2218)	
training:	Epoch: [7][797/817]	Loss 0.3500 (0.2219)	
training:	Epoch: [7][798/817]	Loss 0.0811 (0.2217)	
training:	Epoch: [7][799/817]	Loss 0.2305 (0.2218)	
training:	Epoch: [7][800/817]	Loss 0.0794 (0.2216)	
training:	Epoch: [7][801/817]	Loss 0.7042 (0.2222)	
training:	Epoch: [7][802/817]	Loss 0.0826 (0.2220)	
training:	Epoch: [7][803/817]	Loss 0.0366 (0.2218)	
training:	Epoch: [7][804/817]	Loss 0.1961 (0.2217)	
training:	Epoch: [7][805/817]	Loss 0.4148 (0.2220)	
training:	Epoch: [7][806/817]	Loss 0.3172 (0.2221)	
training:	Epoch: [7][807/817]	Loss 0.0548 (0.2219)	
training:	Epoch: [7][808/817]	Loss 0.1181 (0.2218)	
training:	Epoch: [7][809/817]	Loss 0.0728 (0.2216)	
training:	Epoch: [7][810/817]	Loss 0.6718 (0.2221)	
training:	Epoch: [7][811/817]	Loss 0.3846 (0.2223)	
training:	Epoch: [7][812/817]	Loss 0.0880 (0.2222)	
training:	Epoch: [7][813/817]	Loss 0.0306 (0.2219)	
training:	Epoch: [7][814/817]	Loss 0.0508 (0.2217)	
training:	Epoch: [7][815/817]	Loss 0.0888 (0.2216)	
training:	Epoch: [7][816/817]	Loss 0.1043 (0.2214)	
training:	Epoch: [7][817/817]	Loss 0.5131 (0.2218)	
Training:	 Loss: 0.2217

Training:	 ACC: 0.9538 0.9530 0.9342 0.9735
Validation:	 ACC: 0.8142 0.8138 0.8045 0.8240
Validation:	 Best_BACC: 0.8223 0.8218 0.8127 0.8318
Validation:	 Loss: 0.4787
Pretraining:	Epoch 8/200
----------
training:	Epoch: [8][1/817]	Loss 0.1482 (0.1482)	
training:	Epoch: [8][2/817]	Loss 0.0321 (0.0901)	
training:	Epoch: [8][3/817]	Loss 0.0805 (0.0869)	
training:	Epoch: [8][4/817]	Loss 0.0525 (0.0783)	
training:	Epoch: [8][5/817]	Loss 0.0368 (0.0700)	
training:	Epoch: [8][6/817]	Loss 0.0552 (0.0675)	
training:	Epoch: [8][7/817]	Loss 0.0526 (0.0654)	
training:	Epoch: [8][8/817]	Loss 0.0537 (0.0640)	
training:	Epoch: [8][9/817]	Loss 0.5183 (0.1144)	
training:	Epoch: [8][10/817]	Loss 0.1162 (0.1146)	
training:	Epoch: [8][11/817]	Loss 0.1048 (0.1137)	
training:	Epoch: [8][12/817]	Loss 0.4587 (0.1425)	
training:	Epoch: [8][13/817]	Loss 0.1697 (0.1446)	
training:	Epoch: [8][14/817]	Loss 0.0335 (0.1366)	
training:	Epoch: [8][15/817]	Loss 0.1409 (0.1369)	
training:	Epoch: [8][16/817]	Loss 0.0740 (0.1330)	
training:	Epoch: [8][17/817]	Loss 0.1037 (0.1313)	
training:	Epoch: [8][18/817]	Loss 0.1248 (0.1309)	
training:	Epoch: [8][19/817]	Loss 0.0409 (0.1262)	
training:	Epoch: [8][20/817]	Loss 0.2020 (0.1300)	
training:	Epoch: [8][21/817]	Loss 0.4799 (0.1466)	
training:	Epoch: [8][22/817]	Loss 0.0317 (0.1414)	
training:	Epoch: [8][23/817]	Loss 0.0594 (0.1378)	
training:	Epoch: [8][24/817]	Loss 0.0442 (0.1339)	
training:	Epoch: [8][25/817]	Loss 0.0676 (0.1313)	
training:	Epoch: [8][26/817]	Loss 0.0655 (0.1288)	
training:	Epoch: [8][27/817]	Loss 0.5063 (0.1427)	
training:	Epoch: [8][28/817]	Loss 0.2837 (0.1478)	
training:	Epoch: [8][29/817]	Loss 0.0406 (0.1441)	
training:	Epoch: [8][30/817]	Loss 0.0397 (0.1406)	
training:	Epoch: [8][31/817]	Loss 0.4917 (0.1519)	
training:	Epoch: [8][32/817]	Loss 0.0944 (0.1501)	
training:	Epoch: [8][33/817]	Loss 0.0439 (0.1469)	
training:	Epoch: [8][34/817]	Loss 0.0547 (0.1442)	
training:	Epoch: [8][35/817]	Loss 0.0314 (0.1410)	
training:	Epoch: [8][36/817]	Loss 0.8712 (0.1613)	
training:	Epoch: [8][37/817]	Loss 0.1926 (0.1621)	
training:	Epoch: [8][38/817]	Loss 0.1244 (0.1611)	
training:	Epoch: [8][39/817]	Loss 0.0313 (0.1578)	
training:	Epoch: [8][40/817]	Loss 0.0591 (0.1553)	
training:	Epoch: [8][41/817]	Loss 0.4393 (0.1622)	
training:	Epoch: [8][42/817]	Loss 0.1567 (0.1621)	
training:	Epoch: [8][43/817]	Loss 0.0793 (0.1602)	
training:	Epoch: [8][44/817]	Loss 0.0838 (0.1584)	
training:	Epoch: [8][45/817]	Loss 0.0361 (0.1557)	
training:	Epoch: [8][46/817]	Loss 0.5613 (0.1645)	
training:	Epoch: [8][47/817]	Loss 0.1209 (0.1636)	
training:	Epoch: [8][48/817]	Loss 0.0544 (0.1613)	
training:	Epoch: [8][49/817]	Loss 0.1284 (0.1607)	
training:	Epoch: [8][50/817]	Loss 0.0326 (0.1581)	
training:	Epoch: [8][51/817]	Loss 0.0346 (0.1557)	
training:	Epoch: [8][52/817]	Loss 0.3521 (0.1595)	
training:	Epoch: [8][53/817]	Loss 0.0732 (0.1578)	
training:	Epoch: [8][54/817]	Loss 0.0536 (0.1559)	
training:	Epoch: [8][55/817]	Loss 0.3562 (0.1595)	
training:	Epoch: [8][56/817]	Loss 0.0668 (0.1579)	
training:	Epoch: [8][57/817]	Loss 0.1409 (0.1576)	
training:	Epoch: [8][58/817]	Loss 0.0650 (0.1560)	
training:	Epoch: [8][59/817]	Loss 0.0402 (0.1540)	
training:	Epoch: [8][60/817]	Loss 0.1992 (0.1548)	
training:	Epoch: [8][61/817]	Loss 0.0388 (0.1529)	
training:	Epoch: [8][62/817]	Loss 0.2412 (0.1543)	
training:	Epoch: [8][63/817]	Loss 0.0795 (0.1531)	
training:	Epoch: [8][64/817]	Loss 0.2040 (0.1539)	
training:	Epoch: [8][65/817]	Loss 0.0403 (0.1522)	
training:	Epoch: [8][66/817]	Loss 0.0489 (0.1506)	
training:	Epoch: [8][67/817]	Loss 0.1324 (0.1503)	
training:	Epoch: [8][68/817]	Loss 0.5224 (0.1558)	
training:	Epoch: [8][69/817]	Loss 0.2035 (0.1565)	
training:	Epoch: [8][70/817]	Loss 0.1136 (0.1559)	
training:	Epoch: [8][71/817]	Loss 0.0778 (0.1548)	
training:	Epoch: [8][72/817]	Loss 0.1907 (0.1553)	
training:	Epoch: [8][73/817]	Loss 0.0782 (0.1542)	
training:	Epoch: [8][74/817]	Loss 0.1608 (0.1543)	
training:	Epoch: [8][75/817]	Loss 0.1140 (0.1538)	
training:	Epoch: [8][76/817]	Loss 0.0841 (0.1529)	
training:	Epoch: [8][77/817]	Loss 0.0324 (0.1513)	
training:	Epoch: [8][78/817]	Loss 0.3019 (0.1532)	
training:	Epoch: [8][79/817]	Loss 0.4575 (0.1571)	
training:	Epoch: [8][80/817]	Loss 0.4110 (0.1602)	
training:	Epoch: [8][81/817]	Loss 0.1329 (0.1599)	
training:	Epoch: [8][82/817]	Loss 0.0648 (0.1587)	
training:	Epoch: [8][83/817]	Loss 0.0745 (0.1577)	
training:	Epoch: [8][84/817]	Loss 0.2451 (0.1588)	
training:	Epoch: [8][85/817]	Loss 0.0347 (0.1573)	
training:	Epoch: [8][86/817]	Loss 0.0474 (0.1560)	
training:	Epoch: [8][87/817]	Loss 0.4704 (0.1596)	
training:	Epoch: [8][88/817]	Loss 0.0968 (0.1589)	
training:	Epoch: [8][89/817]	Loss 0.5845 (0.1637)	
training:	Epoch: [8][90/817]	Loss 0.0433 (0.1624)	
training:	Epoch: [8][91/817]	Loss 0.2804 (0.1637)	
training:	Epoch: [8][92/817]	Loss 0.0970 (0.1630)	
training:	Epoch: [8][93/817]	Loss 0.3016 (0.1644)	
training:	Epoch: [8][94/817]	Loss 0.0615 (0.1633)	
training:	Epoch: [8][95/817]	Loss 0.0470 (0.1621)	
training:	Epoch: [8][96/817]	Loss 0.4928 (0.1656)	
training:	Epoch: [8][97/817]	Loss 0.1078 (0.1650)	
training:	Epoch: [8][98/817]	Loss 0.0827 (0.1641)	
training:	Epoch: [8][99/817]	Loss 0.3430 (0.1659)	
training:	Epoch: [8][100/817]	Loss 0.0568 (0.1648)	
training:	Epoch: [8][101/817]	Loss 0.5833 (0.1690)	
training:	Epoch: [8][102/817]	Loss 0.1182 (0.1685)	
training:	Epoch: [8][103/817]	Loss 0.0477 (0.1673)	
training:	Epoch: [8][104/817]	Loss 0.2133 (0.1678)	
training:	Epoch: [8][105/817]	Loss 0.1428 (0.1675)	
training:	Epoch: [8][106/817]	Loss 0.0540 (0.1665)	
training:	Epoch: [8][107/817]	Loss 0.0361 (0.1652)	
training:	Epoch: [8][108/817]	Loss 0.0931 (0.1646)	
training:	Epoch: [8][109/817]	Loss 0.0340 (0.1634)	
training:	Epoch: [8][110/817]	Loss 0.0287 (0.1621)	
training:	Epoch: [8][111/817]	Loss 0.3204 (0.1636)	
training:	Epoch: [8][112/817]	Loss 0.4675 (0.1663)	
training:	Epoch: [8][113/817]	Loss 0.0822 (0.1655)	
training:	Epoch: [8][114/817]	Loss 0.2553 (0.1663)	
training:	Epoch: [8][115/817]	Loss 0.0420 (0.1652)	
training:	Epoch: [8][116/817]	Loss 0.1206 (0.1649)	
training:	Epoch: [8][117/817]	Loss 0.0789 (0.1641)	
training:	Epoch: [8][118/817]	Loss 0.5974 (0.1678)	
training:	Epoch: [8][119/817]	Loss 0.8173 (0.1733)	
training:	Epoch: [8][120/817]	Loss 0.0611 (0.1723)	
training:	Epoch: [8][121/817]	Loss 0.0350 (0.1712)	
training:	Epoch: [8][122/817]	Loss 0.0390 (0.1701)	
training:	Epoch: [8][123/817]	Loss 0.0338 (0.1690)	
training:	Epoch: [8][124/817]	Loss 0.1138 (0.1685)	
training:	Epoch: [8][125/817]	Loss 0.0687 (0.1678)	
training:	Epoch: [8][126/817]	Loss 0.2113 (0.1681)	
training:	Epoch: [8][127/817]	Loss 0.0654 (0.1673)	
training:	Epoch: [8][128/817]	Loss 0.0440 (0.1663)	
training:	Epoch: [8][129/817]	Loss 0.1119 (0.1659)	
training:	Epoch: [8][130/817]	Loss 0.0463 (0.1650)	
training:	Epoch: [8][131/817]	Loss 0.4759 (0.1674)	
training:	Epoch: [8][132/817]	Loss 0.0601 (0.1665)	
training:	Epoch: [8][133/817]	Loss 0.3116 (0.1676)	
training:	Epoch: [8][134/817]	Loss 0.0581 (0.1668)	
training:	Epoch: [8][135/817]	Loss 0.1356 (0.1666)	
training:	Epoch: [8][136/817]	Loss 0.2606 (0.1673)	
training:	Epoch: [8][137/817]	Loss 0.0972 (0.1668)	
training:	Epoch: [8][138/817]	Loss 0.0399 (0.1658)	
training:	Epoch: [8][139/817]	Loss 0.0732 (0.1652)	
training:	Epoch: [8][140/817]	Loss 0.0442 (0.1643)	
training:	Epoch: [8][141/817]	Loss 0.0770 (0.1637)	
training:	Epoch: [8][142/817]	Loss 0.1119 (0.1633)	
training:	Epoch: [8][143/817]	Loss 0.6124 (0.1665)	
training:	Epoch: [8][144/817]	Loss 0.0803 (0.1659)	
training:	Epoch: [8][145/817]	Loss 0.4466 (0.1678)	
training:	Epoch: [8][146/817]	Loss 0.0474 (0.1670)	
training:	Epoch: [8][147/817]	Loss 0.0290 (0.1660)	
training:	Epoch: [8][148/817]	Loss 0.2245 (0.1664)	
training:	Epoch: [8][149/817]	Loss 0.0491 (0.1657)	
training:	Epoch: [8][150/817]	Loss 0.0332 (0.1648)	
training:	Epoch: [8][151/817]	Loss 0.2007 (0.1650)	
training:	Epoch: [8][152/817]	Loss 0.0323 (0.1641)	
training:	Epoch: [8][153/817]	Loss 0.0340 (0.1633)	
training:	Epoch: [8][154/817]	Loss 0.2335 (0.1637)	
training:	Epoch: [8][155/817]	Loss 0.0702 (0.1631)	
training:	Epoch: [8][156/817]	Loss 0.1390 (0.1630)	
training:	Epoch: [8][157/817]	Loss 0.0524 (0.1623)	
training:	Epoch: [8][158/817]	Loss 0.1798 (0.1624)	
training:	Epoch: [8][159/817]	Loss 0.0363 (0.1616)	
training:	Epoch: [8][160/817]	Loss 0.0707 (0.1610)	
training:	Epoch: [8][161/817]	Loss 0.0900 (0.1606)	
training:	Epoch: [8][162/817]	Loss 0.2591 (0.1612)	
training:	Epoch: [8][163/817]	Loss 0.2320 (0.1616)	
training:	Epoch: [8][164/817]	Loss 0.2504 (0.1622)	
training:	Epoch: [8][165/817]	Loss 0.2215 (0.1625)	
training:	Epoch: [8][166/817]	Loss 0.3006 (0.1634)	
training:	Epoch: [8][167/817]	Loss 0.0629 (0.1628)	
training:	Epoch: [8][168/817]	Loss 0.0308 (0.1620)	
training:	Epoch: [8][169/817]	Loss 0.0508 (0.1613)	
training:	Epoch: [8][170/817]	Loss 0.0530 (0.1607)	
training:	Epoch: [8][171/817]	Loss 0.5057 (0.1627)	
training:	Epoch: [8][172/817]	Loss 0.1712 (0.1627)	
training:	Epoch: [8][173/817]	Loss 0.0454 (0.1621)	
training:	Epoch: [8][174/817]	Loss 0.1426 (0.1620)	
training:	Epoch: [8][175/817]	Loss 0.0386 (0.1613)	
training:	Epoch: [8][176/817]	Loss 0.1322 (0.1611)	
training:	Epoch: [8][177/817]	Loss 0.0333 (0.1604)	
training:	Epoch: [8][178/817]	Loss 0.0329 (0.1596)	
training:	Epoch: [8][179/817]	Loss 0.0504 (0.1590)	
training:	Epoch: [8][180/817]	Loss 0.1161 (0.1588)	
training:	Epoch: [8][181/817]	Loss 0.0326 (0.1581)	
training:	Epoch: [8][182/817]	Loss 0.3127 (0.1590)	
training:	Epoch: [8][183/817]	Loss 0.0459 (0.1583)	
training:	Epoch: [8][184/817]	Loss 0.0912 (0.1580)	
training:	Epoch: [8][185/817]	Loss 0.0854 (0.1576)	
training:	Epoch: [8][186/817]	Loss 0.0617 (0.1571)	
training:	Epoch: [8][187/817]	Loss 0.0276 (0.1564)	
training:	Epoch: [8][188/817]	Loss 0.4533 (0.1579)	
training:	Epoch: [8][189/817]	Loss 0.1427 (0.1579)	
training:	Epoch: [8][190/817]	Loss 0.0551 (0.1573)	
training:	Epoch: [8][191/817]	Loss 0.3375 (0.1583)	
training:	Epoch: [8][192/817]	Loss 0.0285 (0.1576)	
training:	Epoch: [8][193/817]	Loss 0.0798 (0.1572)	
training:	Epoch: [8][194/817]	Loss 0.0536 (0.1567)	
training:	Epoch: [8][195/817]	Loss 0.4880 (0.1584)	
training:	Epoch: [8][196/817]	Loss 0.0387 (0.1577)	
training:	Epoch: [8][197/817]	Loss 0.0307 (0.1571)	
training:	Epoch: [8][198/817]	Loss 0.0529 (0.1566)	
training:	Epoch: [8][199/817]	Loss 0.0325 (0.1560)	
training:	Epoch: [8][200/817]	Loss 0.1284 (0.1558)	
training:	Epoch: [8][201/817]	Loss 0.0296 (0.1552)	
training:	Epoch: [8][202/817]	Loss 0.3868 (0.1563)	
training:	Epoch: [8][203/817]	Loss 0.0536 (0.1558)	
training:	Epoch: [8][204/817]	Loss 0.1328 (0.1557)	
training:	Epoch: [8][205/817]	Loss 0.8243 (0.1590)	
training:	Epoch: [8][206/817]	Loss 0.0391 (0.1584)	
training:	Epoch: [8][207/817]	Loss 0.3820 (0.1595)	
training:	Epoch: [8][208/817]	Loss 0.0438 (0.1589)	
training:	Epoch: [8][209/817]	Loss 0.1855 (0.1590)	
training:	Epoch: [8][210/817]	Loss 0.4293 (0.1603)	
training:	Epoch: [8][211/817]	Loss 0.0524 (0.1598)	
training:	Epoch: [8][212/817]	Loss 0.0622 (0.1594)	
training:	Epoch: [8][213/817]	Loss 0.0497 (0.1588)	
training:	Epoch: [8][214/817]	Loss 0.3107 (0.1596)	
training:	Epoch: [8][215/817]	Loss 0.1987 (0.1597)	
training:	Epoch: [8][216/817]	Loss 0.0350 (0.1592)	
training:	Epoch: [8][217/817]	Loss 0.3159 (0.1599)	
training:	Epoch: [8][218/817]	Loss 0.1077 (0.1596)	
training:	Epoch: [8][219/817]	Loss 0.0644 (0.1592)	
training:	Epoch: [8][220/817]	Loss 0.0613 (0.1588)	
training:	Epoch: [8][221/817]	Loss 0.1309 (0.1586)	
training:	Epoch: [8][222/817]	Loss 0.1017 (0.1584)	
training:	Epoch: [8][223/817]	Loss 0.3560 (0.1593)	
training:	Epoch: [8][224/817]	Loss 0.0289 (0.1587)	
training:	Epoch: [8][225/817]	Loss 0.6907 (0.1610)	
training:	Epoch: [8][226/817]	Loss 0.1186 (0.1609)	
training:	Epoch: [8][227/817]	Loss 0.5056 (0.1624)	
training:	Epoch: [8][228/817]	Loss 0.5021 (0.1639)	
training:	Epoch: [8][229/817]	Loss 0.0763 (0.1635)	
training:	Epoch: [8][230/817]	Loss 0.0762 (0.1631)	
training:	Epoch: [8][231/817]	Loss 0.0372 (0.1626)	
training:	Epoch: [8][232/817]	Loss 0.0774 (0.1622)	
training:	Epoch: [8][233/817]	Loss 0.0422 (0.1617)	
training:	Epoch: [8][234/817]	Loss 0.0593 (0.1612)	
training:	Epoch: [8][235/817]	Loss 0.2017 (0.1614)	
training:	Epoch: [8][236/817]	Loss 0.4470 (0.1626)	
training:	Epoch: [8][237/817]	Loss 0.0744 (0.1623)	
training:	Epoch: [8][238/817]	Loss 0.0390 (0.1617)	
training:	Epoch: [8][239/817]	Loss 0.0508 (0.1613)	
training:	Epoch: [8][240/817]	Loss 0.0350 (0.1607)	
training:	Epoch: [8][241/817]	Loss 0.5637 (0.1624)	
training:	Epoch: [8][242/817]	Loss 0.0313 (0.1619)	
training:	Epoch: [8][243/817]	Loss 0.2345 (0.1622)	
training:	Epoch: [8][244/817]	Loss 0.5768 (0.1639)	
training:	Epoch: [8][245/817]	Loss 0.5452 (0.1654)	
training:	Epoch: [8][246/817]	Loss 0.6249 (0.1673)	
training:	Epoch: [8][247/817]	Loss 0.1896 (0.1674)	
training:	Epoch: [8][248/817]	Loss 0.2457 (0.1677)	
training:	Epoch: [8][249/817]	Loss 0.3871 (0.1686)	
training:	Epoch: [8][250/817]	Loss 0.0411 (0.1681)	
training:	Epoch: [8][251/817]	Loss 0.0750 (0.1677)	
training:	Epoch: [8][252/817]	Loss 0.0938 (0.1674)	
training:	Epoch: [8][253/817]	Loss 0.0582 (0.1670)	
training:	Epoch: [8][254/817]	Loss 0.3908 (0.1679)	
training:	Epoch: [8][255/817]	Loss 0.1671 (0.1679)	
training:	Epoch: [8][256/817]	Loss 0.2728 (0.1683)	
training:	Epoch: [8][257/817]	Loss 0.5791 (0.1699)	
training:	Epoch: [8][258/817]	Loss 0.0741 (0.1695)	
training:	Epoch: [8][259/817]	Loss 0.0403 (0.1690)	
training:	Epoch: [8][260/817]	Loss 0.3298 (0.1696)	
training:	Epoch: [8][261/817]	Loss 0.1018 (0.1694)	
training:	Epoch: [8][262/817]	Loss 0.1816 (0.1694)	
training:	Epoch: [8][263/817]	Loss 0.0346 (0.1689)	
training:	Epoch: [8][264/817]	Loss 0.1267 (0.1687)	
training:	Epoch: [8][265/817]	Loss 0.0333 (0.1682)	
training:	Epoch: [8][266/817]	Loss 0.2497 (0.1685)	
training:	Epoch: [8][267/817]	Loss 0.1810 (0.1686)	
training:	Epoch: [8][268/817]	Loss 0.0543 (0.1681)	
training:	Epoch: [8][269/817]	Loss 0.0734 (0.1678)	
training:	Epoch: [8][270/817]	Loss 0.2412 (0.1681)	
training:	Epoch: [8][271/817]	Loss 0.0330 (0.1676)	
training:	Epoch: [8][272/817]	Loss 0.3289 (0.1682)	
training:	Epoch: [8][273/817]	Loss 0.0880 (0.1679)	
training:	Epoch: [8][274/817]	Loss 0.0747 (0.1675)	
training:	Epoch: [8][275/817]	Loss 0.3782 (0.1683)	
training:	Epoch: [8][276/817]	Loss 0.6364 (0.1700)	
training:	Epoch: [8][277/817]	Loss 0.4273 (0.1709)	
training:	Epoch: [8][278/817]	Loss 0.1922 (0.1710)	
training:	Epoch: [8][279/817]	Loss 0.0735 (0.1706)	
training:	Epoch: [8][280/817]	Loss 0.1304 (0.1705)	
training:	Epoch: [8][281/817]	Loss 0.0588 (0.1701)	
training:	Epoch: [8][282/817]	Loss 0.0401 (0.1696)	
training:	Epoch: [8][283/817]	Loss 0.0505 (0.1692)	
training:	Epoch: [8][284/817]	Loss 0.1998 (0.1693)	
training:	Epoch: [8][285/817]	Loss 0.5329 (0.1706)	
training:	Epoch: [8][286/817]	Loss 0.0306 (0.1701)	
training:	Epoch: [8][287/817]	Loss 0.1114 (0.1699)	
training:	Epoch: [8][288/817]	Loss 0.8037 (0.1721)	
training:	Epoch: [8][289/817]	Loss 0.5207 (0.1733)	
training:	Epoch: [8][290/817]	Loss 0.0333 (0.1728)	
training:	Epoch: [8][291/817]	Loss 0.4571 (0.1738)	
training:	Epoch: [8][292/817]	Loss 0.0527 (0.1734)	
training:	Epoch: [8][293/817]	Loss 0.0358 (0.1729)	
training:	Epoch: [8][294/817]	Loss 0.0400 (0.1725)	
training:	Epoch: [8][295/817]	Loss 0.0980 (0.1722)	
training:	Epoch: [8][296/817]	Loss 0.4862 (0.1733)	
training:	Epoch: [8][297/817]	Loss 0.0401 (0.1728)	
training:	Epoch: [8][298/817]	Loss 0.9133 (0.1753)	
training:	Epoch: [8][299/817]	Loss 0.4273 (0.1762)	
training:	Epoch: [8][300/817]	Loss 0.6488 (0.1777)	
training:	Epoch: [8][301/817]	Loss 0.1813 (0.1777)	
training:	Epoch: [8][302/817]	Loss 0.0906 (0.1775)	
training:	Epoch: [8][303/817]	Loss 0.0313 (0.1770)	
training:	Epoch: [8][304/817]	Loss 0.2446 (0.1772)	
training:	Epoch: [8][305/817]	Loss 0.1148 (0.1770)	
training:	Epoch: [8][306/817]	Loss 0.0272 (0.1765)	
training:	Epoch: [8][307/817]	Loss 0.0290 (0.1760)	
training:	Epoch: [8][308/817]	Loss 0.5787 (0.1773)	
training:	Epoch: [8][309/817]	Loss 0.2420 (0.1775)	
training:	Epoch: [8][310/817]	Loss 0.0519 (0.1771)	
training:	Epoch: [8][311/817]	Loss 0.0663 (0.1768)	
training:	Epoch: [8][312/817]	Loss 0.1984 (0.1768)	
training:	Epoch: [8][313/817]	Loss 0.0774 (0.1765)	
training:	Epoch: [8][314/817]	Loss 0.0396 (0.1761)	
training:	Epoch: [8][315/817]	Loss 0.5282 (0.1772)	
training:	Epoch: [8][316/817]	Loss 0.0910 (0.1769)	
training:	Epoch: [8][317/817]	Loss 0.0290 (0.1765)	
training:	Epoch: [8][318/817]	Loss 0.0821 (0.1762)	
training:	Epoch: [8][319/817]	Loss 0.0452 (0.1758)	
training:	Epoch: [8][320/817]	Loss 0.0598 (0.1754)	
training:	Epoch: [8][321/817]	Loss 0.0551 (0.1750)	
training:	Epoch: [8][322/817]	Loss 0.0424 (0.1746)	
training:	Epoch: [8][323/817]	Loss 0.2269 (0.1748)	
training:	Epoch: [8][324/817]	Loss 0.0321 (0.1743)	
training:	Epoch: [8][325/817]	Loss 0.0962 (0.1741)	
training:	Epoch: [8][326/817]	Loss 0.0529 (0.1737)	
training:	Epoch: [8][327/817]	Loss 0.0939 (0.1735)	
training:	Epoch: [8][328/817]	Loss 0.0530 (0.1731)	
training:	Epoch: [8][329/817]	Loss 0.0376 (0.1727)	
training:	Epoch: [8][330/817]	Loss 0.0463 (0.1723)	
training:	Epoch: [8][331/817]	Loss 0.0786 (0.1720)	
training:	Epoch: [8][332/817]	Loss 0.0570 (0.1717)	
training:	Epoch: [8][333/817]	Loss 0.3185 (0.1721)	
training:	Epoch: [8][334/817]	Loss 0.1137 (0.1720)	
training:	Epoch: [8][335/817]	Loss 0.0310 (0.1715)	
training:	Epoch: [8][336/817]	Loss 0.0801 (0.1713)	
training:	Epoch: [8][337/817]	Loss 0.1257 (0.1711)	
training:	Epoch: [8][338/817]	Loss 0.1362 (0.1710)	
training:	Epoch: [8][339/817]	Loss 0.4196 (0.1718)	
training:	Epoch: [8][340/817]	Loss 0.0902 (0.1715)	
training:	Epoch: [8][341/817]	Loss 0.0739 (0.1712)	
training:	Epoch: [8][342/817]	Loss 0.4757 (0.1721)	
training:	Epoch: [8][343/817]	Loss 0.0332 (0.1717)	
training:	Epoch: [8][344/817]	Loss 0.0385 (0.1713)	
training:	Epoch: [8][345/817]	Loss 0.0298 (0.1709)	
training:	Epoch: [8][346/817]	Loss 0.0791 (0.1707)	
training:	Epoch: [8][347/817]	Loss 0.0346 (0.1703)	
training:	Epoch: [8][348/817]	Loss 0.0312 (0.1699)	
training:	Epoch: [8][349/817]	Loss 0.0434 (0.1695)	
training:	Epoch: [8][350/817]	Loss 0.0366 (0.1691)	
training:	Epoch: [8][351/817]	Loss 0.0561 (0.1688)	
training:	Epoch: [8][352/817]	Loss 0.3961 (0.1694)	
training:	Epoch: [8][353/817]	Loss 0.0300 (0.1691)	
training:	Epoch: [8][354/817]	Loss 0.0270 (0.1686)	
training:	Epoch: [8][355/817]	Loss 0.2139 (0.1688)	
training:	Epoch: [8][356/817]	Loss 0.2063 (0.1689)	
training:	Epoch: [8][357/817]	Loss 0.0267 (0.1685)	
training:	Epoch: [8][358/817]	Loss 0.1136 (0.1683)	
training:	Epoch: [8][359/817]	Loss 0.0375 (0.1680)	
training:	Epoch: [8][360/817]	Loss 0.0816 (0.1677)	
training:	Epoch: [8][361/817]	Loss 0.0897 (0.1675)	
training:	Epoch: [8][362/817]	Loss 0.0978 (0.1673)	
training:	Epoch: [8][363/817]	Loss 0.1411 (0.1672)	
training:	Epoch: [8][364/817]	Loss 0.0354 (0.1669)	
training:	Epoch: [8][365/817]	Loss 0.2762 (0.1672)	
training:	Epoch: [8][366/817]	Loss 0.1079 (0.1670)	
training:	Epoch: [8][367/817]	Loss 0.0324 (0.1667)	
training:	Epoch: [8][368/817]	Loss 0.4619 (0.1675)	
training:	Epoch: [8][369/817]	Loss 0.0335 (0.1671)	
training:	Epoch: [8][370/817]	Loss 0.0472 (0.1668)	
training:	Epoch: [8][371/817]	Loss 0.0348 (0.1664)	
training:	Epoch: [8][372/817]	Loss 0.0267 (0.1660)	
training:	Epoch: [8][373/817]	Loss 0.4833 (0.1669)	
training:	Epoch: [8][374/817]	Loss 0.0667 (0.1666)	
training:	Epoch: [8][375/817]	Loss 0.0500 (0.1663)	
training:	Epoch: [8][376/817]	Loss 0.0855 (0.1661)	
training:	Epoch: [8][377/817]	Loss 0.0957 (0.1659)	
training:	Epoch: [8][378/817]	Loss 0.0645 (0.1656)	
training:	Epoch: [8][379/817]	Loss 0.0437 (0.1653)	
training:	Epoch: [8][380/817]	Loss 0.1574 (0.1653)	
training:	Epoch: [8][381/817]	Loss 0.1201 (0.1652)	
training:	Epoch: [8][382/817]	Loss 0.1990 (0.1653)	
training:	Epoch: [8][383/817]	Loss 0.0370 (0.1649)	
training:	Epoch: [8][384/817]	Loss 0.4516 (0.1657)	
training:	Epoch: [8][385/817]	Loss 0.0618 (0.1654)	
training:	Epoch: [8][386/817]	Loss 0.0679 (0.1652)	
training:	Epoch: [8][387/817]	Loss 0.0862 (0.1650)	
training:	Epoch: [8][388/817]	Loss 0.0751 (0.1647)	
training:	Epoch: [8][389/817]	Loss 0.0373 (0.1644)	
training:	Epoch: [8][390/817]	Loss 0.3053 (0.1648)	
training:	Epoch: [8][391/817]	Loss 0.0404 (0.1644)	
training:	Epoch: [8][392/817]	Loss 0.7820 (0.1660)	
training:	Epoch: [8][393/817]	Loss 0.8195 (0.1677)	
training:	Epoch: [8][394/817]	Loss 0.1172 (0.1675)	
training:	Epoch: [8][395/817]	Loss 0.0531 (0.1673)	
training:	Epoch: [8][396/817]	Loss 0.0380 (0.1669)	
training:	Epoch: [8][397/817]	Loss 0.0298 (0.1666)	
training:	Epoch: [8][398/817]	Loss 0.1270 (0.1665)	
training:	Epoch: [8][399/817]	Loss 0.1696 (0.1665)	
training:	Epoch: [8][400/817]	Loss 0.0418 (0.1662)	
training:	Epoch: [8][401/817]	Loss 0.0441 (0.1659)	
training:	Epoch: [8][402/817]	Loss 0.2154 (0.1660)	
training:	Epoch: [8][403/817]	Loss 0.0974 (0.1658)	
training:	Epoch: [8][404/817]	Loss 0.5096 (0.1667)	
training:	Epoch: [8][405/817]	Loss 0.4569 (0.1674)	
training:	Epoch: [8][406/817]	Loss 0.7121 (0.1687)	
training:	Epoch: [8][407/817]	Loss 0.6807 (0.1700)	
training:	Epoch: [8][408/817]	Loss 0.1695 (0.1700)	
training:	Epoch: [8][409/817]	Loss 0.5366 (0.1709)	
training:	Epoch: [8][410/817]	Loss 0.2742 (0.1711)	
training:	Epoch: [8][411/817]	Loss 0.0853 (0.1709)	
training:	Epoch: [8][412/817]	Loss 0.1105 (0.1708)	
training:	Epoch: [8][413/817]	Loss 0.0320 (0.1705)	
training:	Epoch: [8][414/817]	Loss 0.0355 (0.1701)	
training:	Epoch: [8][415/817]	Loss 0.3259 (0.1705)	
training:	Epoch: [8][416/817]	Loss 0.2667 (0.1707)	
training:	Epoch: [8][417/817]	Loss 0.0905 (0.1705)	
training:	Epoch: [8][418/817]	Loss 0.1247 (0.1704)	
training:	Epoch: [8][419/817]	Loss 0.5804 (0.1714)	
training:	Epoch: [8][420/817]	Loss 0.0541 (0.1711)	
training:	Epoch: [8][421/817]	Loss 0.0306 (0.1708)	
training:	Epoch: [8][422/817]	Loss 0.4562 (0.1715)	
training:	Epoch: [8][423/817]	Loss 0.4340 (0.1721)	
training:	Epoch: [8][424/817]	Loss 0.0627 (0.1718)	
training:	Epoch: [8][425/817]	Loss 0.0535 (0.1716)	
training:	Epoch: [8][426/817]	Loss 0.5868 (0.1725)	
training:	Epoch: [8][427/817]	Loss 0.1148 (0.1724)	
training:	Epoch: [8][428/817]	Loss 0.0939 (0.1722)	
training:	Epoch: [8][429/817]	Loss 0.0355 (0.1719)	
training:	Epoch: [8][430/817]	Loss 0.0686 (0.1717)	
training:	Epoch: [8][431/817]	Loss 0.0610 (0.1714)	
training:	Epoch: [8][432/817]	Loss 0.1379 (0.1713)	
training:	Epoch: [8][433/817]	Loss 0.0729 (0.1711)	
training:	Epoch: [8][434/817]	Loss 0.2392 (0.1713)	
training:	Epoch: [8][435/817]	Loss 0.0671 (0.1710)	
training:	Epoch: [8][436/817]	Loss 0.1817 (0.1710)	
training:	Epoch: [8][437/817]	Loss 0.0401 (0.1707)	
training:	Epoch: [8][438/817]	Loss 0.0423 (0.1704)	
training:	Epoch: [8][439/817]	Loss 0.0451 (0.1702)	
training:	Epoch: [8][440/817]	Loss 0.0611 (0.1699)	
training:	Epoch: [8][441/817]	Loss 0.0400 (0.1696)	
training:	Epoch: [8][442/817]	Loss 0.0381 (0.1693)	
training:	Epoch: [8][443/817]	Loss 0.1207 (0.1692)	
training:	Epoch: [8][444/817]	Loss 0.4520 (0.1698)	
training:	Epoch: [8][445/817]	Loss 0.3975 (0.1704)	
training:	Epoch: [8][446/817]	Loss 0.0550 (0.1701)	
training:	Epoch: [8][447/817]	Loss 0.3623 (0.1705)	
training:	Epoch: [8][448/817]	Loss 0.7179 (0.1717)	
training:	Epoch: [8][449/817]	Loss 0.0420 (0.1715)	
training:	Epoch: [8][450/817]	Loss 0.2153 (0.1716)	
training:	Epoch: [8][451/817]	Loss 0.2464 (0.1717)	
training:	Epoch: [8][452/817]	Loss 0.2065 (0.1718)	
training:	Epoch: [8][453/817]	Loss 0.0679 (0.1716)	
training:	Epoch: [8][454/817]	Loss 0.0509 (0.1713)	
training:	Epoch: [8][455/817]	Loss 0.4358 (0.1719)	
training:	Epoch: [8][456/817]	Loss 0.1284 (0.1718)	
training:	Epoch: [8][457/817]	Loss 0.3821 (0.1723)	
training:	Epoch: [8][458/817]	Loss 0.0812 (0.1721)	
training:	Epoch: [8][459/817]	Loss 0.8246 (0.1735)	
training:	Epoch: [8][460/817]	Loss 0.1238 (0.1734)	
training:	Epoch: [8][461/817]	Loss 0.1563 (0.1733)	
training:	Epoch: [8][462/817]	Loss 0.0726 (0.1731)	
training:	Epoch: [8][463/817]	Loss 0.1517 (0.1731)	
training:	Epoch: [8][464/817]	Loss 0.1374 (0.1730)	
training:	Epoch: [8][465/817]	Loss 0.0627 (0.1728)	
training:	Epoch: [8][466/817]	Loss 0.2094 (0.1728)	
training:	Epoch: [8][467/817]	Loss 0.5237 (0.1736)	
training:	Epoch: [8][468/817]	Loss 0.1911 (0.1736)	
training:	Epoch: [8][469/817]	Loss 0.3522 (0.1740)	
training:	Epoch: [8][470/817]	Loss 0.0492 (0.1737)	
training:	Epoch: [8][471/817]	Loss 0.0689 (0.1735)	
training:	Epoch: [8][472/817]	Loss 0.1281 (0.1734)	
training:	Epoch: [8][473/817]	Loss 0.0793 (0.1732)	
training:	Epoch: [8][474/817]	Loss 0.5349 (0.1740)	
training:	Epoch: [8][475/817]	Loss 0.2106 (0.1741)	
training:	Epoch: [8][476/817]	Loss 0.0416 (0.1738)	
training:	Epoch: [8][477/817]	Loss 0.2440 (0.1739)	
training:	Epoch: [8][478/817]	Loss 0.1049 (0.1738)	
training:	Epoch: [8][479/817]	Loss 0.0357 (0.1735)	
training:	Epoch: [8][480/817]	Loss 0.4739 (0.1741)	
training:	Epoch: [8][481/817]	Loss 0.0966 (0.1740)	
training:	Epoch: [8][482/817]	Loss 0.0897 (0.1738)	
training:	Epoch: [8][483/817]	Loss 0.0346 (0.1735)	
training:	Epoch: [8][484/817]	Loss 0.0422 (0.1732)	
training:	Epoch: [8][485/817]	Loss 0.0559 (0.1730)	
training:	Epoch: [8][486/817]	Loss 0.1077 (0.1728)	
training:	Epoch: [8][487/817]	Loss 0.1163 (0.1727)	
training:	Epoch: [8][488/817]	Loss 0.2529 (0.1729)	
training:	Epoch: [8][489/817]	Loss 0.0685 (0.1727)	
training:	Epoch: [8][490/817]	Loss 0.3801 (0.1731)	
training:	Epoch: [8][491/817]	Loss 0.0390 (0.1728)	
training:	Epoch: [8][492/817]	Loss 0.0277 (0.1725)	
training:	Epoch: [8][493/817]	Loss 0.1060 (0.1724)	
training:	Epoch: [8][494/817]	Loss 0.2413 (0.1725)	
training:	Epoch: [8][495/817]	Loss 0.0314 (0.1723)	
training:	Epoch: [8][496/817]	Loss 0.5091 (0.1729)	
training:	Epoch: [8][497/817]	Loss 0.0423 (0.1727)	
training:	Epoch: [8][498/817]	Loss 0.0285 (0.1724)	
training:	Epoch: [8][499/817]	Loss 0.0536 (0.1721)	
training:	Epoch: [8][500/817]	Loss 0.1708 (0.1721)	
training:	Epoch: [8][501/817]	Loss 0.0819 (0.1720)	
training:	Epoch: [8][502/817]	Loss 0.1246 (0.1719)	
training:	Epoch: [8][503/817]	Loss 0.1384 (0.1718)	
training:	Epoch: [8][504/817]	Loss 0.0343 (0.1715)	
training:	Epoch: [8][505/817]	Loss 0.3429 (0.1719)	
training:	Epoch: [8][506/817]	Loss 0.0307 (0.1716)	
training:	Epoch: [8][507/817]	Loss 0.3247 (0.1719)	
training:	Epoch: [8][508/817]	Loss 0.1721 (0.1719)	
training:	Epoch: [8][509/817]	Loss 0.0538 (0.1717)	
training:	Epoch: [8][510/817]	Loss 0.1292 (0.1716)	
training:	Epoch: [8][511/817]	Loss 0.0959 (0.1714)	
training:	Epoch: [8][512/817]	Loss 0.0428 (0.1712)	
training:	Epoch: [8][513/817]	Loss 0.0665 (0.1710)	
training:	Epoch: [8][514/817]	Loss 0.0302 (0.1707)	
training:	Epoch: [8][515/817]	Loss 0.3828 (0.1711)	
training:	Epoch: [8][516/817]	Loss 0.4199 (0.1716)	
training:	Epoch: [8][517/817]	Loss 0.0335 (0.1713)	
training:	Epoch: [8][518/817]	Loss 0.0332 (0.1711)	
training:	Epoch: [8][519/817]	Loss 0.6495 (0.1720)	
training:	Epoch: [8][520/817]	Loss 0.3479 (0.1723)	
training:	Epoch: [8][521/817]	Loss 0.0459 (0.1721)	
training:	Epoch: [8][522/817]	Loss 0.5563 (0.1728)	
training:	Epoch: [8][523/817]	Loss 0.4805 (0.1734)	
training:	Epoch: [8][524/817]	Loss 0.1488 (0.1734)	
training:	Epoch: [8][525/817]	Loss 0.0375 (0.1731)	
training:	Epoch: [8][526/817]	Loss 0.0482 (0.1729)	
training:	Epoch: [8][527/817]	Loss 0.1520 (0.1728)	
training:	Epoch: [8][528/817]	Loss 0.0890 (0.1727)	
training:	Epoch: [8][529/817]	Loss 0.0570 (0.1724)	
training:	Epoch: [8][530/817]	Loss 0.2887 (0.1727)	
training:	Epoch: [8][531/817]	Loss 0.0385 (0.1724)	
training:	Epoch: [8][532/817]	Loss 0.0565 (0.1722)	
training:	Epoch: [8][533/817]	Loss 0.1393 (0.1721)	
training:	Epoch: [8][534/817]	Loss 0.0489 (0.1719)	
training:	Epoch: [8][535/817]	Loss 0.1226 (0.1718)	
training:	Epoch: [8][536/817]	Loss 0.0983 (0.1717)	
training:	Epoch: [8][537/817]	Loss 0.0589 (0.1715)	
training:	Epoch: [8][538/817]	Loss 0.1292 (0.1714)	
training:	Epoch: [8][539/817]	Loss 0.0330 (0.1711)	
training:	Epoch: [8][540/817]	Loss 0.1191 (0.1710)	
training:	Epoch: [8][541/817]	Loss 0.5266 (0.1717)	
training:	Epoch: [8][542/817]	Loss 0.4223 (0.1721)	
training:	Epoch: [8][543/817]	Loss 0.4431 (0.1726)	
training:	Epoch: [8][544/817]	Loss 0.0267 (0.1724)	
training:	Epoch: [8][545/817]	Loss 0.2479 (0.1725)	
training:	Epoch: [8][546/817]	Loss 0.7603 (0.1736)	
training:	Epoch: [8][547/817]	Loss 0.0312 (0.1733)	
training:	Epoch: [8][548/817]	Loss 0.0501 (0.1731)	
training:	Epoch: [8][549/817]	Loss 0.1028 (0.1730)	
training:	Epoch: [8][550/817]	Loss 0.0448 (0.1727)	
training:	Epoch: [8][551/817]	Loss 0.1110 (0.1726)	
training:	Epoch: [8][552/817]	Loss 0.1811 (0.1726)	
training:	Epoch: [8][553/817]	Loss 0.0818 (0.1725)	
training:	Epoch: [8][554/817]	Loss 0.7168 (0.1735)	
training:	Epoch: [8][555/817]	Loss 0.0626 (0.1733)	
training:	Epoch: [8][556/817]	Loss 0.0700 (0.1731)	
training:	Epoch: [8][557/817]	Loss 0.0834 (0.1729)	
training:	Epoch: [8][558/817]	Loss 0.2325 (0.1730)	
training:	Epoch: [8][559/817]	Loss 0.1689 (0.1730)	
training:	Epoch: [8][560/817]	Loss 0.0608 (0.1728)	
training:	Epoch: [8][561/817]	Loss 0.0544 (0.1726)	
training:	Epoch: [8][562/817]	Loss 0.5353 (0.1733)	
training:	Epoch: [8][563/817]	Loss 0.0739 (0.1731)	
training:	Epoch: [8][564/817]	Loss 0.0736 (0.1729)	
training:	Epoch: [8][565/817]	Loss 0.4198 (0.1733)	
training:	Epoch: [8][566/817]	Loss 0.5334 (0.1740)	
training:	Epoch: [8][567/817]	Loss 0.0374 (0.1737)	
training:	Epoch: [8][568/817]	Loss 0.0931 (0.1736)	
training:	Epoch: [8][569/817]	Loss 0.0390 (0.1734)	
training:	Epoch: [8][570/817]	Loss 0.0352 (0.1731)	
training:	Epoch: [8][571/817]	Loss 0.3377 (0.1734)	
training:	Epoch: [8][572/817]	Loss 0.1494 (0.1734)	
training:	Epoch: [8][573/817]	Loss 0.0467 (0.1731)	
training:	Epoch: [8][574/817]	Loss 0.0491 (0.1729)	
training:	Epoch: [8][575/817]	Loss 0.4413 (0.1734)	
training:	Epoch: [8][576/817]	Loss 0.8122 (0.1745)	
training:	Epoch: [8][577/817]	Loss 0.1366 (0.1744)	
training:	Epoch: [8][578/817]	Loss 0.6383 (0.1752)	
training:	Epoch: [8][579/817]	Loss 0.5298 (0.1758)	
training:	Epoch: [8][580/817]	Loss 0.0582 (0.1756)	
training:	Epoch: [8][581/817]	Loss 0.2064 (0.1757)	
training:	Epoch: [8][582/817]	Loss 0.2049 (0.1757)	
training:	Epoch: [8][583/817]	Loss 0.1076 (0.1756)	
training:	Epoch: [8][584/817]	Loss 0.0324 (0.1754)	
training:	Epoch: [8][585/817]	Loss 0.8878 (0.1766)	
training:	Epoch: [8][586/817]	Loss 0.5945 (0.1773)	
training:	Epoch: [8][587/817]	Loss 0.0518 (0.1771)	
training:	Epoch: [8][588/817]	Loss 0.0864 (0.1769)	
training:	Epoch: [8][589/817]	Loss 0.0769 (0.1768)	
training:	Epoch: [8][590/817]	Loss 0.0298 (0.1765)	
training:	Epoch: [8][591/817]	Loss 0.6343 (0.1773)	
training:	Epoch: [8][592/817]	Loss 0.0521 (0.1771)	
training:	Epoch: [8][593/817]	Loss 0.1507 (0.1770)	
training:	Epoch: [8][594/817]	Loss 0.0318 (0.1768)	
training:	Epoch: [8][595/817]	Loss 0.1599 (0.1768)	
training:	Epoch: [8][596/817]	Loss 0.5549 (0.1774)	
training:	Epoch: [8][597/817]	Loss 0.1743 (0.1774)	
training:	Epoch: [8][598/817]	Loss 0.4953 (0.1779)	
training:	Epoch: [8][599/817]	Loss 0.0416 (0.1777)	
training:	Epoch: [8][600/817]	Loss 0.1289 (0.1776)	
training:	Epoch: [8][601/817]	Loss 0.0625 (0.1774)	
training:	Epoch: [8][602/817]	Loss 0.0368 (0.1772)	
training:	Epoch: [8][603/817]	Loss 0.0374 (0.1770)	
training:	Epoch: [8][604/817]	Loss 0.0888 (0.1768)	
training:	Epoch: [8][605/817]	Loss 0.5046 (0.1774)	
training:	Epoch: [8][606/817]	Loss 0.0439 (0.1771)	
training:	Epoch: [8][607/817]	Loss 0.0408 (0.1769)	
training:	Epoch: [8][608/817]	Loss 0.2188 (0.1770)	
training:	Epoch: [8][609/817]	Loss 0.2839 (0.1772)	
training:	Epoch: [8][610/817]	Loss 0.0525 (0.1770)	
training:	Epoch: [8][611/817]	Loss 0.0937 (0.1768)	
training:	Epoch: [8][612/817]	Loss 0.0485 (0.1766)	
training:	Epoch: [8][613/817]	Loss 0.1342 (0.1765)	
training:	Epoch: [8][614/817]	Loss 0.1785 (0.1765)	
training:	Epoch: [8][615/817]	Loss 0.1215 (0.1765)	
training:	Epoch: [8][616/817]	Loss 0.0708 (0.1763)	
training:	Epoch: [8][617/817]	Loss 0.1174 (0.1762)	
training:	Epoch: [8][618/817]	Loss 0.2532 (0.1763)	
training:	Epoch: [8][619/817]	Loss 0.0943 (0.1762)	
training:	Epoch: [8][620/817]	Loss 0.2851 (0.1764)	
training:	Epoch: [8][621/817]	Loss 0.4155 (0.1767)	
training:	Epoch: [8][622/817]	Loss 0.2480 (0.1769)	
training:	Epoch: [8][623/817]	Loss 0.0356 (0.1766)	
training:	Epoch: [8][624/817]	Loss 0.0316 (0.1764)	
training:	Epoch: [8][625/817]	Loss 0.2521 (0.1765)	
training:	Epoch: [8][626/817]	Loss 0.2300 (0.1766)	
training:	Epoch: [8][627/817]	Loss 0.0611 (0.1764)	
training:	Epoch: [8][628/817]	Loss 0.0352 (0.1762)	
training:	Epoch: [8][629/817]	Loss 0.2725 (0.1764)	
training:	Epoch: [8][630/817]	Loss 0.1698 (0.1763)	
training:	Epoch: [8][631/817]	Loss 0.0358 (0.1761)	
training:	Epoch: [8][632/817]	Loss 0.4241 (0.1765)	
training:	Epoch: [8][633/817]	Loss 0.0315 (0.1763)	
training:	Epoch: [8][634/817]	Loss 0.1381 (0.1762)	
training:	Epoch: [8][635/817]	Loss 0.0401 (0.1760)	
training:	Epoch: [8][636/817]	Loss 0.1599 (0.1760)	
training:	Epoch: [8][637/817]	Loss 0.1626 (0.1760)	
training:	Epoch: [8][638/817]	Loss 0.1046 (0.1758)	
training:	Epoch: [8][639/817]	Loss 0.6097 (0.1765)	
training:	Epoch: [8][640/817]	Loss 0.3285 (0.1768)	
training:	Epoch: [8][641/817]	Loss 0.6562 (0.1775)	
training:	Epoch: [8][642/817]	Loss 0.1829 (0.1775)	
training:	Epoch: [8][643/817]	Loss 0.6363 (0.1782)	
training:	Epoch: [8][644/817]	Loss 0.5514 (0.1788)	
training:	Epoch: [8][645/817]	Loss 0.0402 (0.1786)	
training:	Epoch: [8][646/817]	Loss 0.0725 (0.1784)	
training:	Epoch: [8][647/817]	Loss 0.0709 (0.1783)	
training:	Epoch: [8][648/817]	Loss 0.0617 (0.1781)	
training:	Epoch: [8][649/817]	Loss 0.1933 (0.1781)	
training:	Epoch: [8][650/817]	Loss 0.1495 (0.1781)	
training:	Epoch: [8][651/817]	Loss 0.1555 (0.1780)	
training:	Epoch: [8][652/817]	Loss 0.0324 (0.1778)	
training:	Epoch: [8][653/817]	Loss 0.0307 (0.1776)	
training:	Epoch: [8][654/817]	Loss 0.1739 (0.1776)	
training:	Epoch: [8][655/817]	Loss 0.0344 (0.1774)	
training:	Epoch: [8][656/817]	Loss 0.0338 (0.1771)	
training:	Epoch: [8][657/817]	Loss 0.0847 (0.1770)	
training:	Epoch: [8][658/817]	Loss 0.0562 (0.1768)	
training:	Epoch: [8][659/817]	Loss 0.0707 (0.1767)	
training:	Epoch: [8][660/817]	Loss 0.0822 (0.1765)	
training:	Epoch: [8][661/817]	Loss 0.1159 (0.1764)	
training:	Epoch: [8][662/817]	Loss 0.0616 (0.1762)	
training:	Epoch: [8][663/817]	Loss 0.4734 (0.1767)	
training:	Epoch: [8][664/817]	Loss 0.0481 (0.1765)	
training:	Epoch: [8][665/817]	Loss 0.4714 (0.1769)	
training:	Epoch: [8][666/817]	Loss 0.0609 (0.1768)	
training:	Epoch: [8][667/817]	Loss 0.0653 (0.1766)	
training:	Epoch: [8][668/817]	Loss 0.2262 (0.1767)	
training:	Epoch: [8][669/817]	Loss 0.0361 (0.1765)	
training:	Epoch: [8][670/817]	Loss 0.0340 (0.1763)	
training:	Epoch: [8][671/817]	Loss 0.0257 (0.1760)	
training:	Epoch: [8][672/817]	Loss 0.0799 (0.1759)	
training:	Epoch: [8][673/817]	Loss 0.5921 (0.1765)	
training:	Epoch: [8][674/817]	Loss 0.0543 (0.1763)	
training:	Epoch: [8][675/817]	Loss 0.0545 (0.1761)	
training:	Epoch: [8][676/817]	Loss 0.2300 (0.1762)	
training:	Epoch: [8][677/817]	Loss 0.0375 (0.1760)	
training:	Epoch: [8][678/817]	Loss 0.1399 (0.1760)	
training:	Epoch: [8][679/817]	Loss 0.0297 (0.1758)	
training:	Epoch: [8][680/817]	Loss 0.0346 (0.1755)	
training:	Epoch: [8][681/817]	Loss 0.4586 (0.1760)	
training:	Epoch: [8][682/817]	Loss 0.0282 (0.1757)	
training:	Epoch: [8][683/817]	Loss 0.0500 (0.1756)	
training:	Epoch: [8][684/817]	Loss 0.0368 (0.1754)	
training:	Epoch: [8][685/817]	Loss 0.2420 (0.1755)	
training:	Epoch: [8][686/817]	Loss 0.0571 (0.1753)	
training:	Epoch: [8][687/817]	Loss 0.1571 (0.1753)	
training:	Epoch: [8][688/817]	Loss 0.1526 (0.1752)	
training:	Epoch: [8][689/817]	Loss 0.0652 (0.1751)	
training:	Epoch: [8][690/817]	Loss 0.2256 (0.1751)	
training:	Epoch: [8][691/817]	Loss 0.2692 (0.1753)	
training:	Epoch: [8][692/817]	Loss 0.1247 (0.1752)	
training:	Epoch: [8][693/817]	Loss 0.3473 (0.1754)	
training:	Epoch: [8][694/817]	Loss 0.0387 (0.1752)	
training:	Epoch: [8][695/817]	Loss 0.0777 (0.1751)	
training:	Epoch: [8][696/817]	Loss 0.0316 (0.1749)	
training:	Epoch: [8][697/817]	Loss 0.2886 (0.1751)	
training:	Epoch: [8][698/817]	Loss 0.1277 (0.1750)	
training:	Epoch: [8][699/817]	Loss 0.2849 (0.1752)	
training:	Epoch: [8][700/817]	Loss 0.4253 (0.1755)	
training:	Epoch: [8][701/817]	Loss 0.0326 (0.1753)	
training:	Epoch: [8][702/817]	Loss 0.4959 (0.1758)	
training:	Epoch: [8][703/817]	Loss 0.4226 (0.1761)	
training:	Epoch: [8][704/817]	Loss 0.0725 (0.1760)	
training:	Epoch: [8][705/817]	Loss 0.3262 (0.1762)	
training:	Epoch: [8][706/817]	Loss 0.4173 (0.1765)	
training:	Epoch: [8][707/817]	Loss 0.5644 (0.1771)	
training:	Epoch: [8][708/817]	Loss 0.0459 (0.1769)	
training:	Epoch: [8][709/817]	Loss 0.1230 (0.1768)	
training:	Epoch: [8][710/817]	Loss 0.0384 (0.1766)	
training:	Epoch: [8][711/817]	Loss 0.1572 (0.1766)	
training:	Epoch: [8][712/817]	Loss 0.3077 (0.1768)	
training:	Epoch: [8][713/817]	Loss 0.1294 (0.1767)	
training:	Epoch: [8][714/817]	Loss 0.4599 (0.1771)	
training:	Epoch: [8][715/817]	Loss 0.1121 (0.1770)	
training:	Epoch: [8][716/817]	Loss 0.0274 (0.1768)	
training:	Epoch: [8][717/817]	Loss 0.3888 (0.1771)	
training:	Epoch: [8][718/817]	Loss 0.2087 (0.1771)	
training:	Epoch: [8][719/817]	Loss 0.1293 (0.1771)	
training:	Epoch: [8][720/817]	Loss 0.5099 (0.1775)	
training:	Epoch: [8][721/817]	Loss 0.0549 (0.1774)	
training:	Epoch: [8][722/817]	Loss 0.0942 (0.1773)	
training:	Epoch: [8][723/817]	Loss 0.0282 (0.1770)	
training:	Epoch: [8][724/817]	Loss 0.0273 (0.1768)	
training:	Epoch: [8][725/817]	Loss 0.0338 (0.1766)	
training:	Epoch: [8][726/817]	Loss 0.2415 (0.1767)	
training:	Epoch: [8][727/817]	Loss 0.5335 (0.1772)	
training:	Epoch: [8][728/817]	Loss 0.0983 (0.1771)	
training:	Epoch: [8][729/817]	Loss 0.0656 (0.1770)	
training:	Epoch: [8][730/817]	Loss 0.0320 (0.1768)	
training:	Epoch: [8][731/817]	Loss 0.0982 (0.1767)	
training:	Epoch: [8][732/817]	Loss 0.0968 (0.1765)	
training:	Epoch: [8][733/817]	Loss 0.3010 (0.1767)	
training:	Epoch: [8][734/817]	Loss 0.0269 (0.1765)	
training:	Epoch: [8][735/817]	Loss 0.0434 (0.1763)	
training:	Epoch: [8][736/817]	Loss 0.0806 (0.1762)	
training:	Epoch: [8][737/817]	Loss 0.0429 (0.1760)	
training:	Epoch: [8][738/817]	Loss 0.0546 (0.1759)	
training:	Epoch: [8][739/817]	Loss 0.0576 (0.1757)	
training:	Epoch: [8][740/817]	Loss 0.5645 (0.1762)	
training:	Epoch: [8][741/817]	Loss 0.0350 (0.1760)	
training:	Epoch: [8][742/817]	Loss 0.0476 (0.1759)	
training:	Epoch: [8][743/817]	Loss 0.0520 (0.1757)	
training:	Epoch: [8][744/817]	Loss 0.0304 (0.1755)	
training:	Epoch: [8][745/817]	Loss 0.5444 (0.1760)	
training:	Epoch: [8][746/817]	Loss 0.1116 (0.1759)	
training:	Epoch: [8][747/817]	Loss 0.0295 (0.1757)	
training:	Epoch: [8][748/817]	Loss 0.0407 (0.1755)	
training:	Epoch: [8][749/817]	Loss 0.0256 (0.1753)	
training:	Epoch: [8][750/817]	Loss 0.4406 (0.1757)	
training:	Epoch: [8][751/817]	Loss 0.0258 (0.1755)	
training:	Epoch: [8][752/817]	Loss 0.0275 (0.1753)	
training:	Epoch: [8][753/817]	Loss 0.0323 (0.1751)	
training:	Epoch: [8][754/817]	Loss 0.1710 (0.1751)	
training:	Epoch: [8][755/817]	Loss 0.1355 (0.1750)	
training:	Epoch: [8][756/817]	Loss 0.0316 (0.1748)	
training:	Epoch: [8][757/817]	Loss 0.0285 (0.1747)	
training:	Epoch: [8][758/817]	Loss 0.5133 (0.1751)	
training:	Epoch: [8][759/817]	Loss 0.3986 (0.1754)	
training:	Epoch: [8][760/817]	Loss 0.6005 (0.1760)	
training:	Epoch: [8][761/817]	Loss 0.4479 (0.1763)	
training:	Epoch: [8][762/817]	Loss 0.2179 (0.1764)	
training:	Epoch: [8][763/817]	Loss 0.1175 (0.1763)	
training:	Epoch: [8][764/817]	Loss 0.2046 (0.1763)	
training:	Epoch: [8][765/817]	Loss 0.0472 (0.1762)	
training:	Epoch: [8][766/817]	Loss 0.0709 (0.1760)	
training:	Epoch: [8][767/817]	Loss 0.0340 (0.1758)	
training:	Epoch: [8][768/817]	Loss 0.0733 (0.1757)	
training:	Epoch: [8][769/817]	Loss 0.0302 (0.1755)	
training:	Epoch: [8][770/817]	Loss 0.0433 (0.1753)	
training:	Epoch: [8][771/817]	Loss 0.4004 (0.1756)	
training:	Epoch: [8][772/817]	Loss 0.0404 (0.1755)	
training:	Epoch: [8][773/817]	Loss 0.8654 (0.1764)	
training:	Epoch: [8][774/817]	Loss 0.1032 (0.1763)	
training:	Epoch: [8][775/817]	Loss 0.0739 (0.1761)	
training:	Epoch: [8][776/817]	Loss 0.0246 (0.1759)	
training:	Epoch: [8][777/817]	Loss 0.0289 (0.1757)	
training:	Epoch: [8][778/817]	Loss 0.0523 (0.1756)	
training:	Epoch: [8][779/817]	Loss 0.0859 (0.1755)	
training:	Epoch: [8][780/817]	Loss 0.9082 (0.1764)	
training:	Epoch: [8][781/817]	Loss 0.0337 (0.1762)	
training:	Epoch: [8][782/817]	Loss 0.0799 (0.1761)	
training:	Epoch: [8][783/817]	Loss 0.0540 (0.1759)	
training:	Epoch: [8][784/817]	Loss 0.0715 (0.1758)	
training:	Epoch: [8][785/817]	Loss 0.3940 (0.1761)	
training:	Epoch: [8][786/817]	Loss 0.0358 (0.1759)	
training:	Epoch: [8][787/817]	Loss 0.0451 (0.1757)	
training:	Epoch: [8][788/817]	Loss 0.0380 (0.1756)	
training:	Epoch: [8][789/817]	Loss 0.0318 (0.1754)	
training:	Epoch: [8][790/817]	Loss 0.1726 (0.1754)	
training:	Epoch: [8][791/817]	Loss 0.1407 (0.1753)	
training:	Epoch: [8][792/817]	Loss 0.0543 (0.1752)	
training:	Epoch: [8][793/817]	Loss 0.0573 (0.1750)	
training:	Epoch: [8][794/817]	Loss 0.0846 (0.1749)	
training:	Epoch: [8][795/817]	Loss 0.4960 (0.1753)	
training:	Epoch: [8][796/817]	Loss 0.0356 (0.1752)	
training:	Epoch: [8][797/817]	Loss 0.0262 (0.1750)	
training:	Epoch: [8][798/817]	Loss 0.1643 (0.1750)	
training:	Epoch: [8][799/817]	Loss 0.0299 (0.1748)	
training:	Epoch: [8][800/817]	Loss 0.3402 (0.1750)	
training:	Epoch: [8][801/817]	Loss 0.0251 (0.1748)	
training:	Epoch: [8][802/817]	Loss 0.1171 (0.1747)	
training:	Epoch: [8][803/817]	Loss 0.5002 (0.1751)	
training:	Epoch: [8][804/817]	Loss 0.0645 (0.1750)	
training:	Epoch: [8][805/817]	Loss 0.1059 (0.1749)	
training:	Epoch: [8][806/817]	Loss 0.0938 (0.1748)	
training:	Epoch: [8][807/817]	Loss 0.0473 (0.1746)	
training:	Epoch: [8][808/817]	Loss 0.0822 (0.1745)	
training:	Epoch: [8][809/817]	Loss 0.2449 (0.1746)	
training:	Epoch: [8][810/817]	Loss 0.2968 (0.1748)	
training:	Epoch: [8][811/817]	Loss 0.0563 (0.1746)	
training:	Epoch: [8][812/817]	Loss 0.0277 (0.1744)	
training:	Epoch: [8][813/817]	Loss 0.5193 (0.1749)	
training:	Epoch: [8][814/817]	Loss 0.0247 (0.1747)	
training:	Epoch: [8][815/817]	Loss 0.4139 (0.1750)	
training:	Epoch: [8][816/817]	Loss 0.0299 (0.1748)	
training:	Epoch: [8][817/817]	Loss 0.0270 (0.1746)	
Training:	 Loss: 0.1746

Training:	 ACC: 0.9645 0.9636 0.9418 0.9872
Validation:	 ACC: 0.7950 0.7935 0.7615 0.8285
Validation:	 Best_BACC: 0.8223 0.8218 0.8127 0.8318
Validation:	 Loss: 0.5572
Pretraining:	Epoch 9/200
----------
training:	Epoch: [9][1/817]	Loss 0.0335 (0.0335)	
training:	Epoch: [9][2/817]	Loss 0.0302 (0.0318)	
training:	Epoch: [9][3/817]	Loss 0.5288 (0.1975)	
training:	Epoch: [9][4/817]	Loss 0.0508 (0.1608)	
training:	Epoch: [9][5/817]	Loss 0.0699 (0.1426)	
training:	Epoch: [9][6/817]	Loss 0.6474 (0.2268)	
training:	Epoch: [9][7/817]	Loss 0.0371 (0.1997)	
training:	Epoch: [9][8/817]	Loss 0.0567 (0.1818)	
training:	Epoch: [9][9/817]	Loss 0.0425 (0.1663)	
training:	Epoch: [9][10/817]	Loss 0.6639 (0.2161)	
training:	Epoch: [9][11/817]	Loss 0.0409 (0.2001)	
training:	Epoch: [9][12/817]	Loss 0.0495 (0.1876)	
training:	Epoch: [9][13/817]	Loss 0.0304 (0.1755)	
training:	Epoch: [9][14/817]	Loss 0.0272 (0.1649)	
training:	Epoch: [9][15/817]	Loss 0.0320 (0.1560)	
training:	Epoch: [9][16/817]	Loss 0.0894 (0.1519)	
training:	Epoch: [9][17/817]	Loss 0.0480 (0.1458)	
training:	Epoch: [9][18/817]	Loss 0.0333 (0.1395)	
training:	Epoch: [9][19/817]	Loss 0.0494 (0.1348)	
training:	Epoch: [9][20/817]	Loss 0.0774 (0.1319)	
training:	Epoch: [9][21/817]	Loss 0.0295 (0.1270)	
training:	Epoch: [9][22/817]	Loss 0.0264 (0.1225)	
training:	Epoch: [9][23/817]	Loss 0.0582 (0.1197)	
training:	Epoch: [9][24/817]	Loss 0.0422 (0.1164)	
training:	Epoch: [9][25/817]	Loss 0.0849 (0.1152)	
training:	Epoch: [9][26/817]	Loss 0.0676 (0.1133)	
training:	Epoch: [9][27/817]	Loss 0.0487 (0.1110)	
training:	Epoch: [9][28/817]	Loss 0.0306 (0.1081)	
training:	Epoch: [9][29/817]	Loss 0.5312 (0.1227)	
training:	Epoch: [9][30/817]	Loss 0.2137 (0.1257)	
training:	Epoch: [9][31/817]	Loss 0.0312 (0.1227)	
training:	Epoch: [9][32/817]	Loss 0.0404 (0.1201)	
training:	Epoch: [9][33/817]	Loss 0.1174 (0.1200)	
training:	Epoch: [9][34/817]	Loss 0.0909 (0.1191)	
training:	Epoch: [9][35/817]	Loss 0.5472 (0.1314)	
training:	Epoch: [9][36/817]	Loss 0.0347 (0.1287)	
training:	Epoch: [9][37/817]	Loss 0.0415 (0.1263)	
training:	Epoch: [9][38/817]	Loss 0.5418 (0.1373)	
training:	Epoch: [9][39/817]	Loss 0.0780 (0.1357)	
training:	Epoch: [9][40/817]	Loss 0.5307 (0.1456)	
training:	Epoch: [9][41/817]	Loss 0.0880 (0.1442)	
training:	Epoch: [9][42/817]	Loss 0.1554 (0.1445)	
training:	Epoch: [9][43/817]	Loss 0.0848 (0.1431)	
training:	Epoch: [9][44/817]	Loss 0.0274 (0.1405)	
training:	Epoch: [9][45/817]	Loss 0.0353 (0.1381)	
training:	Epoch: [9][46/817]	Loss 0.0305 (0.1358)	
training:	Epoch: [9][47/817]	Loss 0.0580 (0.1341)	
training:	Epoch: [9][48/817]	Loss 0.0817 (0.1330)	
training:	Epoch: [9][49/817]	Loss 0.0306 (0.1310)	
training:	Epoch: [9][50/817]	Loss 0.4106 (0.1365)	
training:	Epoch: [9][51/817]	Loss 0.0287 (0.1344)	
training:	Epoch: [9][52/817]	Loss 0.1425 (0.1346)	
training:	Epoch: [9][53/817]	Loss 0.0559 (0.1331)	
training:	Epoch: [9][54/817]	Loss 0.1177 (0.1328)	
training:	Epoch: [9][55/817]	Loss 0.1273 (0.1327)	
training:	Epoch: [9][56/817]	Loss 0.0679 (0.1316)	
training:	Epoch: [9][57/817]	Loss 0.0412 (0.1300)	
training:	Epoch: [9][58/817]	Loss 0.0340 (0.1283)	
training:	Epoch: [9][59/817]	Loss 0.0510 (0.1270)	
training:	Epoch: [9][60/817]	Loss 0.0265 (0.1253)	
training:	Epoch: [9][61/817]	Loss 0.1016 (0.1249)	
training:	Epoch: [9][62/817]	Loss 0.0271 (0.1234)	
training:	Epoch: [9][63/817]	Loss 0.4171 (0.1280)	
training:	Epoch: [9][64/817]	Loss 0.0545 (0.1269)	
training:	Epoch: [9][65/817]	Loss 0.0274 (0.1253)	
training:	Epoch: [9][66/817]	Loss 0.0447 (0.1241)	
training:	Epoch: [9][67/817]	Loss 0.0577 (0.1231)	
training:	Epoch: [9][68/817]	Loss 0.0710 (0.1224)	
training:	Epoch: [9][69/817]	Loss 0.0275 (0.1210)	
training:	Epoch: [9][70/817]	Loss 0.0412 (0.1199)	
training:	Epoch: [9][71/817]	Loss 0.1171 (0.1198)	
training:	Epoch: [9][72/817]	Loss 0.0320 (0.1186)	
training:	Epoch: [9][73/817]	Loss 0.4522 (0.1232)	
training:	Epoch: [9][74/817]	Loss 0.0997 (0.1228)	
training:	Epoch: [9][75/817]	Loss 0.2725 (0.1248)	
training:	Epoch: [9][76/817]	Loss 0.0425 (0.1238)	
training:	Epoch: [9][77/817]	Loss 0.2586 (0.1255)	
training:	Epoch: [9][78/817]	Loss 0.0518 (0.1246)	
training:	Epoch: [9][79/817]	Loss 0.0250 (0.1233)	
training:	Epoch: [9][80/817]	Loss 0.0331 (0.1222)	
training:	Epoch: [9][81/817]	Loss 0.0820 (0.1217)	
training:	Epoch: [9][82/817]	Loss 0.0981 (0.1214)	
training:	Epoch: [9][83/817]	Loss 0.0566 (0.1206)	
training:	Epoch: [9][84/817]	Loss 0.1104 (0.1205)	
training:	Epoch: [9][85/817]	Loss 0.0876 (0.1201)	
training:	Epoch: [9][86/817]	Loss 0.1843 (0.1209)	
training:	Epoch: [9][87/817]	Loss 0.4883 (0.1251)	
training:	Epoch: [9][88/817]	Loss 0.0803 (0.1246)	
training:	Epoch: [9][89/817]	Loss 0.0436 (0.1237)	
training:	Epoch: [9][90/817]	Loss 0.1342 (0.1238)	
training:	Epoch: [9][91/817]	Loss 0.0584 (0.1231)	
training:	Epoch: [9][92/817]	Loss 0.0288 (0.1220)	
training:	Epoch: [9][93/817]	Loss 0.5210 (0.1263)	
training:	Epoch: [9][94/817]	Loss 0.4013 (0.1292)	
training:	Epoch: [9][95/817]	Loss 0.0592 (0.1285)	
training:	Epoch: [9][96/817]	Loss 0.0436 (0.1276)	
training:	Epoch: [9][97/817]	Loss 0.0417 (0.1267)	
training:	Epoch: [9][98/817]	Loss 0.0746 (0.1262)	
training:	Epoch: [9][99/817]	Loss 0.0560 (0.1255)	
training:	Epoch: [9][100/817]	Loss 0.0276 (0.1245)	
training:	Epoch: [9][101/817]	Loss 0.0593 (0.1239)	
training:	Epoch: [9][102/817]	Loss 0.0964 (0.1236)	
training:	Epoch: [9][103/817]	Loss 0.0448 (0.1228)	
training:	Epoch: [9][104/817]	Loss 0.0605 (0.1222)	
training:	Epoch: [9][105/817]	Loss 0.0338 (0.1214)	
training:	Epoch: [9][106/817]	Loss 0.3226 (0.1233)	
training:	Epoch: [9][107/817]	Loss 0.2979 (0.1249)	
training:	Epoch: [9][108/817]	Loss 0.0261 (0.1240)	
training:	Epoch: [9][109/817]	Loss 0.6157 (0.1285)	
training:	Epoch: [9][110/817]	Loss 0.4174 (0.1312)	
training:	Epoch: [9][111/817]	Loss 0.0266 (0.1302)	
training:	Epoch: [9][112/817]	Loss 0.1343 (0.1302)	
training:	Epoch: [9][113/817]	Loss 0.4705 (0.1333)	
training:	Epoch: [9][114/817]	Loss 0.0420 (0.1325)	
training:	Epoch: [9][115/817]	Loss 0.4571 (0.1353)	
training:	Epoch: [9][116/817]	Loss 0.5035 (0.1385)	
training:	Epoch: [9][117/817]	Loss 0.1093 (0.1382)	
training:	Epoch: [9][118/817]	Loss 0.5979 (0.1421)	
training:	Epoch: [9][119/817]	Loss 0.1741 (0.1424)	
training:	Epoch: [9][120/817]	Loss 0.2617 (0.1434)	
training:	Epoch: [9][121/817]	Loss 0.0334 (0.1425)	
training:	Epoch: [9][122/817]	Loss 0.0750 (0.1419)	
training:	Epoch: [9][123/817]	Loss 0.0329 (0.1410)	
training:	Epoch: [9][124/817]	Loss 0.0275 (0.1401)	
training:	Epoch: [9][125/817]	Loss 0.0291 (0.1392)	
training:	Epoch: [9][126/817]	Loss 0.0802 (0.1387)	
training:	Epoch: [9][127/817]	Loss 0.0695 (0.1382)	
training:	Epoch: [9][128/817]	Loss 0.0407 (0.1374)	
training:	Epoch: [9][129/817]	Loss 0.5247 (0.1404)	
training:	Epoch: [9][130/817]	Loss 0.0766 (0.1399)	
training:	Epoch: [9][131/817]	Loss 0.0323 (0.1391)	
training:	Epoch: [9][132/817]	Loss 0.5299 (0.1421)	
training:	Epoch: [9][133/817]	Loss 0.1192 (0.1419)	
training:	Epoch: [9][134/817]	Loss 0.5623 (0.1451)	
training:	Epoch: [9][135/817]	Loss 0.0524 (0.1444)	
training:	Epoch: [9][136/817]	Loss 0.2415 (0.1451)	
training:	Epoch: [9][137/817]	Loss 0.5838 (0.1483)	
training:	Epoch: [9][138/817]	Loss 0.5742 (0.1514)	
training:	Epoch: [9][139/817]	Loss 0.0246 (0.1505)	
training:	Epoch: [9][140/817]	Loss 0.0270 (0.1496)	
training:	Epoch: [9][141/817]	Loss 0.0491 (0.1489)	
training:	Epoch: [9][142/817]	Loss 0.0378 (0.1481)	
training:	Epoch: [9][143/817]	Loss 0.3339 (0.1494)	
training:	Epoch: [9][144/817]	Loss 0.0370 (0.1486)	
training:	Epoch: [9][145/817]	Loss 0.0424 (0.1479)	
training:	Epoch: [9][146/817]	Loss 0.0413 (0.1471)	
training:	Epoch: [9][147/817]	Loss 0.0242 (0.1463)	
training:	Epoch: [9][148/817]	Loss 0.0380 (0.1456)	
training:	Epoch: [9][149/817]	Loss 0.1624 (0.1457)	
training:	Epoch: [9][150/817]	Loss 0.3785 (0.1472)	
training:	Epoch: [9][151/817]	Loss 0.0291 (0.1465)	
training:	Epoch: [9][152/817]	Loss 0.0310 (0.1457)	
training:	Epoch: [9][153/817]	Loss 0.0446 (0.1450)	
training:	Epoch: [9][154/817]	Loss 0.1602 (0.1451)	
training:	Epoch: [9][155/817]	Loss 0.1237 (0.1450)	
training:	Epoch: [9][156/817]	Loss 0.0695 (0.1445)	
training:	Epoch: [9][157/817]	Loss 0.0665 (0.1440)	
training:	Epoch: [9][158/817]	Loss 0.0311 (0.1433)	
training:	Epoch: [9][159/817]	Loss 0.0547 (0.1427)	
training:	Epoch: [9][160/817]	Loss 0.0433 (0.1421)	
training:	Epoch: [9][161/817]	Loss 0.1109 (0.1419)	
training:	Epoch: [9][162/817]	Loss 0.0841 (0.1416)	
training:	Epoch: [9][163/817]	Loss 0.0594 (0.1411)	
training:	Epoch: [9][164/817]	Loss 0.4643 (0.1430)	
training:	Epoch: [9][165/817]	Loss 0.0228 (0.1423)	
training:	Epoch: [9][166/817]	Loss 0.0330 (0.1416)	
training:	Epoch: [9][167/817]	Loss 0.3769 (0.1431)	
training:	Epoch: [9][168/817]	Loss 0.5800 (0.1457)	
training:	Epoch: [9][169/817]	Loss 0.4320 (0.1473)	
training:	Epoch: [9][170/817]	Loss 0.0275 (0.1466)	
training:	Epoch: [9][171/817]	Loss 0.2309 (0.1471)	
training:	Epoch: [9][172/817]	Loss 0.0596 (0.1466)	
training:	Epoch: [9][173/817]	Loss 0.0259 (0.1459)	
training:	Epoch: [9][174/817]	Loss 0.5331 (0.1482)	
training:	Epoch: [9][175/817]	Loss 0.4866 (0.1501)	
training:	Epoch: [9][176/817]	Loss 0.3820 (0.1514)	
training:	Epoch: [9][177/817]	Loss 0.0475 (0.1508)	
training:	Epoch: [9][178/817]	Loss 0.0439 (0.1502)	
training:	Epoch: [9][179/817]	Loss 0.1378 (0.1502)	
training:	Epoch: [9][180/817]	Loss 0.0319 (0.1495)	
training:	Epoch: [9][181/817]	Loss 0.0570 (0.1490)	
training:	Epoch: [9][182/817]	Loss 0.1836 (0.1492)	
training:	Epoch: [9][183/817]	Loss 0.2160 (0.1495)	
training:	Epoch: [9][184/817]	Loss 0.1145 (0.1493)	
training:	Epoch: [9][185/817]	Loss 0.0278 (0.1487)	
training:	Epoch: [9][186/817]	Loss 0.0524 (0.1482)	
training:	Epoch: [9][187/817]	Loss 0.0430 (0.1476)	
training:	Epoch: [9][188/817]	Loss 0.0600 (0.1471)	
training:	Epoch: [9][189/817]	Loss 0.0400 (0.1466)	
training:	Epoch: [9][190/817]	Loss 0.0272 (0.1459)	
training:	Epoch: [9][191/817]	Loss 0.0258 (0.1453)	
training:	Epoch: [9][192/817]	Loss 0.1807 (0.1455)	
training:	Epoch: [9][193/817]	Loss 0.1857 (0.1457)	
training:	Epoch: [9][194/817]	Loss 0.0438 (0.1452)	
training:	Epoch: [9][195/817]	Loss 0.0487 (0.1447)	
training:	Epoch: [9][196/817]	Loss 0.0317 (0.1441)	
training:	Epoch: [9][197/817]	Loss 0.0266 (0.1435)	
training:	Epoch: [9][198/817]	Loss 0.0467 (0.1430)	
training:	Epoch: [9][199/817]	Loss 0.5263 (0.1450)	
training:	Epoch: [9][200/817]	Loss 0.0455 (0.1445)	
training:	Epoch: [9][201/817]	Loss 0.0450 (0.1440)	
training:	Epoch: [9][202/817]	Loss 0.0730 (0.1436)	
training:	Epoch: [9][203/817]	Loss 0.0371 (0.1431)	
training:	Epoch: [9][204/817]	Loss 0.0757 (0.1428)	
training:	Epoch: [9][205/817]	Loss 0.0329 (0.1422)	
training:	Epoch: [9][206/817]	Loss 0.0256 (0.1417)	
training:	Epoch: [9][207/817]	Loss 0.4533 (0.1432)	
training:	Epoch: [9][208/817]	Loss 0.0267 (0.1426)	
training:	Epoch: [9][209/817]	Loss 0.0286 (0.1421)	
training:	Epoch: [9][210/817]	Loss 0.0434 (0.1416)	
training:	Epoch: [9][211/817]	Loss 0.0273 (0.1410)	
training:	Epoch: [9][212/817]	Loss 0.4532 (0.1425)	
training:	Epoch: [9][213/817]	Loss 0.1016 (0.1423)	
training:	Epoch: [9][214/817]	Loss 0.0356 (0.1418)	
training:	Epoch: [9][215/817]	Loss 0.1455 (0.1418)	
training:	Epoch: [9][216/817]	Loss 0.0415 (0.1414)	
training:	Epoch: [9][217/817]	Loss 0.0320 (0.1409)	
training:	Epoch: [9][218/817]	Loss 0.0289 (0.1404)	
training:	Epoch: [9][219/817]	Loss 0.0258 (0.1398)	
training:	Epoch: [9][220/817]	Loss 0.0532 (0.1394)	
training:	Epoch: [9][221/817]	Loss 0.0291 (0.1389)	
training:	Epoch: [9][222/817]	Loss 0.1044 (0.1388)	
training:	Epoch: [9][223/817]	Loss 0.0429 (0.1384)	
training:	Epoch: [9][224/817]	Loss 0.0540 (0.1380)	
training:	Epoch: [9][225/817]	Loss 0.0525 (0.1376)	
training:	Epoch: [9][226/817]	Loss 0.6305 (0.1398)	
training:	Epoch: [9][227/817]	Loss 0.0458 (0.1394)	
training:	Epoch: [9][228/817]	Loss 0.0412 (0.1389)	
training:	Epoch: [9][229/817]	Loss 0.2992 (0.1396)	
training:	Epoch: [9][230/817]	Loss 0.4802 (0.1411)	
training:	Epoch: [9][231/817]	Loss 0.4034 (0.1423)	
training:	Epoch: [9][232/817]	Loss 0.0593 (0.1419)	
training:	Epoch: [9][233/817]	Loss 0.0293 (0.1414)	
training:	Epoch: [9][234/817]	Loss 0.0278 (0.1409)	
training:	Epoch: [9][235/817]	Loss 0.3904 (0.1420)	
training:	Epoch: [9][236/817]	Loss 0.0350 (0.1415)	
training:	Epoch: [9][237/817]	Loss 0.0323 (0.1411)	
training:	Epoch: [9][238/817]	Loss 0.0457 (0.1407)	
training:	Epoch: [9][239/817]	Loss 0.0571 (0.1403)	
training:	Epoch: [9][240/817]	Loss 0.0517 (0.1400)	
training:	Epoch: [9][241/817]	Loss 0.5517 (0.1417)	
training:	Epoch: [9][242/817]	Loss 0.0229 (0.1412)	
training:	Epoch: [9][243/817]	Loss 0.8811 (0.1442)	
training:	Epoch: [9][244/817]	Loss 0.1035 (0.1441)	
training:	Epoch: [9][245/817]	Loss 0.0234 (0.1436)	
training:	Epoch: [9][246/817]	Loss 0.0273 (0.1431)	
training:	Epoch: [9][247/817]	Loss 0.1553 (0.1431)	
training:	Epoch: [9][248/817]	Loss 0.0252 (0.1427)	
training:	Epoch: [9][249/817]	Loss 0.0406 (0.1423)	
training:	Epoch: [9][250/817]	Loss 0.2609 (0.1427)	
training:	Epoch: [9][251/817]	Loss 0.0335 (0.1423)	
training:	Epoch: [9][252/817]	Loss 0.0291 (0.1418)	
training:	Epoch: [9][253/817]	Loss 0.0393 (0.1414)	
training:	Epoch: [9][254/817]	Loss 0.0621 (0.1411)	
training:	Epoch: [9][255/817]	Loss 0.0341 (0.1407)	
training:	Epoch: [9][256/817]	Loss 0.0395 (0.1403)	
training:	Epoch: [9][257/817]	Loss 0.2036 (0.1406)	
training:	Epoch: [9][258/817]	Loss 0.0255 (0.1401)	
training:	Epoch: [9][259/817]	Loss 0.0269 (0.1397)	
training:	Epoch: [9][260/817]	Loss 0.0259 (0.1392)	
training:	Epoch: [9][261/817]	Loss 0.0842 (0.1390)	
training:	Epoch: [9][262/817]	Loss 0.0347 (0.1386)	
training:	Epoch: [9][263/817]	Loss 0.0328 (0.1382)	
training:	Epoch: [9][264/817]	Loss 0.1489 (0.1383)	
training:	Epoch: [9][265/817]	Loss 0.0349 (0.1379)	
training:	Epoch: [9][266/817]	Loss 0.0397 (0.1375)	
training:	Epoch: [9][267/817]	Loss 0.0338 (0.1371)	
training:	Epoch: [9][268/817]	Loss 0.0479 (0.1368)	
training:	Epoch: [9][269/817]	Loss 0.0258 (0.1364)	
training:	Epoch: [9][270/817]	Loss 0.5057 (0.1377)	
training:	Epoch: [9][271/817]	Loss 0.0308 (0.1373)	
training:	Epoch: [9][272/817]	Loss 0.5244 (0.1388)	
training:	Epoch: [9][273/817]	Loss 0.4616 (0.1399)	
training:	Epoch: [9][274/817]	Loss 0.0368 (0.1396)	
training:	Epoch: [9][275/817]	Loss 0.0323 (0.1392)	
training:	Epoch: [9][276/817]	Loss 0.0210 (0.1388)	
training:	Epoch: [9][277/817]	Loss 0.1623 (0.1388)	
training:	Epoch: [9][278/817]	Loss 0.0490 (0.1385)	
training:	Epoch: [9][279/817]	Loss 0.0280 (0.1381)	
training:	Epoch: [9][280/817]	Loss 0.0423 (0.1378)	
training:	Epoch: [9][281/817]	Loss 0.0668 (0.1375)	
training:	Epoch: [9][282/817]	Loss 0.0734 (0.1373)	
training:	Epoch: [9][283/817]	Loss 0.0256 (0.1369)	
training:	Epoch: [9][284/817]	Loss 0.0323 (0.1365)	
training:	Epoch: [9][285/817]	Loss 0.4936 (0.1378)	
training:	Epoch: [9][286/817]	Loss 0.0381 (0.1374)	
training:	Epoch: [9][287/817]	Loss 0.0421 (0.1371)	
training:	Epoch: [9][288/817]	Loss 0.0392 (0.1368)	
training:	Epoch: [9][289/817]	Loss 0.0297 (0.1364)	
training:	Epoch: [9][290/817]	Loss 0.0352 (0.1360)	
training:	Epoch: [9][291/817]	Loss 0.0385 (0.1357)	
training:	Epoch: [9][292/817]	Loss 0.3842 (0.1366)	
training:	Epoch: [9][293/817]	Loss 0.4814 (0.1377)	
training:	Epoch: [9][294/817]	Loss 0.0395 (0.1374)	
training:	Epoch: [9][295/817]	Loss 0.0227 (0.1370)	
training:	Epoch: [9][296/817]	Loss 0.1919 (0.1372)	
training:	Epoch: [9][297/817]	Loss 0.0750 (0.1370)	
training:	Epoch: [9][298/817]	Loss 0.5344 (0.1383)	
training:	Epoch: [9][299/817]	Loss 0.2082 (0.1386)	
training:	Epoch: [9][300/817]	Loss 0.0902 (0.1384)	
training:	Epoch: [9][301/817]	Loss 0.0316 (0.1380)	
training:	Epoch: [9][302/817]	Loss 0.0800 (0.1379)	
training:	Epoch: [9][303/817]	Loss 0.0612 (0.1376)	
training:	Epoch: [9][304/817]	Loss 0.0226 (0.1372)	
training:	Epoch: [9][305/817]	Loss 0.1456 (0.1372)	
training:	Epoch: [9][306/817]	Loss 0.0524 (0.1370)	
training:	Epoch: [9][307/817]	Loss 0.0293 (0.1366)	
training:	Epoch: [9][308/817]	Loss 0.0647 (0.1364)	
training:	Epoch: [9][309/817]	Loss 0.0379 (0.1361)	
training:	Epoch: [9][310/817]	Loss 0.0239 (0.1357)	
training:	Epoch: [9][311/817]	Loss 0.4177 (0.1366)	
training:	Epoch: [9][312/817]	Loss 0.4545 (0.1376)	
training:	Epoch: [9][313/817]	Loss 0.0247 (0.1373)	
training:	Epoch: [9][314/817]	Loss 0.1861 (0.1374)	
training:	Epoch: [9][315/817]	Loss 0.0417 (0.1371)	
training:	Epoch: [9][316/817]	Loss 0.0303 (0.1368)	
training:	Epoch: [9][317/817]	Loss 0.4635 (0.1378)	
training:	Epoch: [9][318/817]	Loss 0.0440 (0.1375)	
training:	Epoch: [9][319/817]	Loss 0.0789 (0.1373)	
training:	Epoch: [9][320/817]	Loss 0.6367 (0.1389)	
training:	Epoch: [9][321/817]	Loss 0.0396 (0.1386)	
training:	Epoch: [9][322/817]	Loss 0.0569 (0.1383)	
training:	Epoch: [9][323/817]	Loss 0.0281 (0.1380)	
training:	Epoch: [9][324/817]	Loss 0.0353 (0.1377)	
training:	Epoch: [9][325/817]	Loss 0.0427 (0.1374)	
training:	Epoch: [9][326/817]	Loss 0.0342 (0.1371)	
training:	Epoch: [9][327/817]	Loss 0.4377 (0.1380)	
training:	Epoch: [9][328/817]	Loss 0.0767 (0.1378)	
training:	Epoch: [9][329/817]	Loss 0.0243 (0.1375)	
training:	Epoch: [9][330/817]	Loss 0.0533 (0.1372)	
training:	Epoch: [9][331/817]	Loss 0.0563 (0.1370)	
training:	Epoch: [9][332/817]	Loss 0.0843 (0.1368)	
training:	Epoch: [9][333/817]	Loss 0.0219 (0.1365)	
training:	Epoch: [9][334/817]	Loss 0.0350 (0.1361)	
training:	Epoch: [9][335/817]	Loss 0.0943 (0.1360)	
training:	Epoch: [9][336/817]	Loss 0.0322 (0.1357)	
training:	Epoch: [9][337/817]	Loss 0.0351 (0.1354)	
training:	Epoch: [9][338/817]	Loss 0.0291 (0.1351)	
training:	Epoch: [9][339/817]	Loss 0.1150 (0.1350)	
training:	Epoch: [9][340/817]	Loss 0.1793 (0.1352)	
training:	Epoch: [9][341/817]	Loss 0.0303 (0.1349)	
training:	Epoch: [9][342/817]	Loss 0.2810 (0.1353)	
training:	Epoch: [9][343/817]	Loss 0.0401 (0.1350)	
training:	Epoch: [9][344/817]	Loss 0.0283 (0.1347)	
training:	Epoch: [9][345/817]	Loss 0.3670 (0.1354)	
training:	Epoch: [9][346/817]	Loss 0.5039 (0.1364)	
training:	Epoch: [9][347/817]	Loss 0.0365 (0.1362)	
training:	Epoch: [9][348/817]	Loss 0.0301 (0.1358)	
training:	Epoch: [9][349/817]	Loss 0.0277 (0.1355)	
training:	Epoch: [9][350/817]	Loss 0.0249 (0.1352)	
training:	Epoch: [9][351/817]	Loss 0.0243 (0.1349)	
training:	Epoch: [9][352/817]	Loss 0.0274 (0.1346)	
training:	Epoch: [9][353/817]	Loss 0.5486 (0.1358)	
training:	Epoch: [9][354/817]	Loss 0.3520 (0.1364)	
training:	Epoch: [9][355/817]	Loss 0.0478 (0.1361)	
training:	Epoch: [9][356/817]	Loss 0.0218 (0.1358)	
training:	Epoch: [9][357/817]	Loss 0.1539 (0.1359)	
training:	Epoch: [9][358/817]	Loss 0.0245 (0.1356)	
training:	Epoch: [9][359/817]	Loss 0.6388 (0.1370)	
training:	Epoch: [9][360/817]	Loss 0.0288 (0.1367)	
training:	Epoch: [9][361/817]	Loss 0.0332 (0.1364)	
training:	Epoch: [9][362/817]	Loss 0.0317 (0.1361)	
training:	Epoch: [9][363/817]	Loss 0.0528 (0.1359)	
training:	Epoch: [9][364/817]	Loss 0.2153 (0.1361)	
training:	Epoch: [9][365/817]	Loss 0.0502 (0.1358)	
training:	Epoch: [9][366/817]	Loss 0.0530 (0.1356)	
training:	Epoch: [9][367/817]	Loss 0.0398 (0.1353)	
training:	Epoch: [9][368/817]	Loss 0.0422 (0.1351)	
training:	Epoch: [9][369/817]	Loss 0.0354 (0.1348)	
training:	Epoch: [9][370/817]	Loss 0.1538 (0.1349)	
training:	Epoch: [9][371/817]	Loss 0.0714 (0.1347)	
training:	Epoch: [9][372/817]	Loss 0.0585 (0.1345)	
training:	Epoch: [9][373/817]	Loss 0.0498 (0.1343)	
training:	Epoch: [9][374/817]	Loss 0.8012 (0.1361)	
training:	Epoch: [9][375/817]	Loss 0.0375 (0.1358)	
training:	Epoch: [9][376/817]	Loss 0.0386 (0.1355)	
training:	Epoch: [9][377/817]	Loss 0.0431 (0.1353)	
training:	Epoch: [9][378/817]	Loss 0.0235 (0.1350)	
training:	Epoch: [9][379/817]	Loss 0.0433 (0.1348)	
training:	Epoch: [9][380/817]	Loss 0.0264 (0.1345)	
training:	Epoch: [9][381/817]	Loss 0.0293 (0.1342)	
training:	Epoch: [9][382/817]	Loss 0.0250 (0.1339)	
training:	Epoch: [9][383/817]	Loss 0.0274 (0.1336)	
training:	Epoch: [9][384/817]	Loss 0.0781 (0.1335)	
training:	Epoch: [9][385/817]	Loss 0.0213 (0.1332)	
training:	Epoch: [9][386/817]	Loss 0.0444 (0.1330)	
training:	Epoch: [9][387/817]	Loss 0.3532 (0.1335)	
training:	Epoch: [9][388/817]	Loss 0.5117 (0.1345)	
training:	Epoch: [9][389/817]	Loss 0.3295 (0.1350)	
training:	Epoch: [9][390/817]	Loss 0.3520 (0.1356)	
training:	Epoch: [9][391/817]	Loss 0.0521 (0.1353)	
training:	Epoch: [9][392/817]	Loss 0.0222 (0.1351)	
training:	Epoch: [9][393/817]	Loss 0.4870 (0.1360)	
training:	Epoch: [9][394/817]	Loss 0.5008 (0.1369)	
training:	Epoch: [9][395/817]	Loss 0.0248 (0.1366)	
training:	Epoch: [9][396/817]	Loss 0.0428 (0.1364)	
training:	Epoch: [9][397/817]	Loss 0.0664 (0.1362)	
training:	Epoch: [9][398/817]	Loss 0.0394 (0.1359)	
training:	Epoch: [9][399/817]	Loss 0.1056 (0.1359)	
training:	Epoch: [9][400/817]	Loss 0.1094 (0.1358)	
training:	Epoch: [9][401/817]	Loss 0.0334 (0.1355)	
training:	Epoch: [9][402/817]	Loss 0.0263 (0.1353)	
training:	Epoch: [9][403/817]	Loss 0.2354 (0.1355)	
training:	Epoch: [9][404/817]	Loss 0.0564 (0.1353)	
training:	Epoch: [9][405/817]	Loss 0.1096 (0.1353)	
training:	Epoch: [9][406/817]	Loss 0.0687 (0.1351)	
training:	Epoch: [9][407/817]	Loss 0.0453 (0.1349)	
training:	Epoch: [9][408/817]	Loss 0.0355 (0.1346)	
training:	Epoch: [9][409/817]	Loss 0.0939 (0.1345)	
training:	Epoch: [9][410/817]	Loss 0.3289 (0.1350)	
training:	Epoch: [9][411/817]	Loss 0.0568 (0.1348)	
training:	Epoch: [9][412/817]	Loss 0.0336 (0.1346)	
training:	Epoch: [9][413/817]	Loss 0.2714 (0.1349)	
training:	Epoch: [9][414/817]	Loss 0.0514 (0.1347)	
training:	Epoch: [9][415/817]	Loss 0.0296 (0.1344)	
training:	Epoch: [9][416/817]	Loss 0.0289 (0.1342)	
training:	Epoch: [9][417/817]	Loss 0.0474 (0.1340)	
training:	Epoch: [9][418/817]	Loss 0.0336 (0.1337)	
training:	Epoch: [9][419/817]	Loss 0.0232 (0.1335)	
training:	Epoch: [9][420/817]	Loss 0.0270 (0.1332)	
training:	Epoch: [9][421/817]	Loss 0.0317 (0.1330)	
training:	Epoch: [9][422/817]	Loss 0.0238 (0.1327)	
training:	Epoch: [9][423/817]	Loss 0.0294 (0.1325)	
training:	Epoch: [9][424/817]	Loss 0.0209 (0.1322)	
training:	Epoch: [9][425/817]	Loss 0.0537 (0.1320)	
training:	Epoch: [9][426/817]	Loss 0.0910 (0.1319)	
training:	Epoch: [9][427/817]	Loss 0.5605 (0.1329)	
training:	Epoch: [9][428/817]	Loss 0.0311 (0.1327)	
training:	Epoch: [9][429/817]	Loss 0.0354 (0.1325)	
training:	Epoch: [9][430/817]	Loss 0.0282 (0.1322)	
training:	Epoch: [9][431/817]	Loss 0.3284 (0.1327)	
training:	Epoch: [9][432/817]	Loss 0.0279 (0.1324)	
training:	Epoch: [9][433/817]	Loss 0.1863 (0.1326)	
training:	Epoch: [9][434/817]	Loss 0.0428 (0.1324)	
training:	Epoch: [9][435/817]	Loss 0.0360 (0.1321)	
training:	Epoch: [9][436/817]	Loss 0.4582 (0.1329)	
training:	Epoch: [9][437/817]	Loss 0.0549 (0.1327)	
training:	Epoch: [9][438/817]	Loss 0.0484 (0.1325)	
training:	Epoch: [9][439/817]	Loss 0.0279 (0.1323)	
training:	Epoch: [9][440/817]	Loss 0.1399 (0.1323)	
training:	Epoch: [9][441/817]	Loss 0.5525 (0.1333)	
training:	Epoch: [9][442/817]	Loss 0.0659 (0.1331)	
training:	Epoch: [9][443/817]	Loss 0.0245 (0.1329)	
training:	Epoch: [9][444/817]	Loss 0.0307 (0.1326)	
training:	Epoch: [9][445/817]	Loss 0.1262 (0.1326)	
training:	Epoch: [9][446/817]	Loss 0.3606 (0.1331)	
training:	Epoch: [9][447/817]	Loss 0.4201 (0.1338)	
training:	Epoch: [9][448/817]	Loss 0.0322 (0.1335)	
training:	Epoch: [9][449/817]	Loss 0.5044 (0.1344)	
training:	Epoch: [9][450/817]	Loss 0.4042 (0.1350)	
training:	Epoch: [9][451/817]	Loss 0.0463 (0.1348)	
training:	Epoch: [9][452/817]	Loss 0.0256 (0.1345)	
training:	Epoch: [9][453/817]	Loss 0.0212 (0.1343)	
training:	Epoch: [9][454/817]	Loss 0.0301 (0.1340)	
training:	Epoch: [9][455/817]	Loss 0.0273 (0.1338)	
training:	Epoch: [9][456/817]	Loss 0.0297 (0.1336)	
training:	Epoch: [9][457/817]	Loss 0.4346 (0.1342)	
training:	Epoch: [9][458/817]	Loss 0.0745 (0.1341)	
training:	Epoch: [9][459/817]	Loss 0.2251 (0.1343)	
training:	Epoch: [9][460/817]	Loss 0.0554 (0.1341)	
training:	Epoch: [9][461/817]	Loss 0.0807 (0.1340)	
training:	Epoch: [9][462/817]	Loss 0.0386 (0.1338)	
training:	Epoch: [9][463/817]	Loss 0.1033 (0.1337)	
training:	Epoch: [9][464/817]	Loss 0.0294 (0.1335)	
training:	Epoch: [9][465/817]	Loss 0.0281 (0.1333)	
training:	Epoch: [9][466/817]	Loss 0.0807 (0.1332)	
training:	Epoch: [9][467/817]	Loss 0.0443 (0.1330)	
training:	Epoch: [9][468/817]	Loss 0.0315 (0.1328)	
training:	Epoch: [9][469/817]	Loss 0.2009 (0.1329)	
training:	Epoch: [9][470/817]	Loss 0.0989 (0.1328)	
training:	Epoch: [9][471/817]	Loss 0.0261 (0.1326)	
training:	Epoch: [9][472/817]	Loss 0.0277 (0.1324)	
training:	Epoch: [9][473/817]	Loss 0.0697 (0.1323)	
training:	Epoch: [9][474/817]	Loss 0.0270 (0.1320)	
training:	Epoch: [9][475/817]	Loss 0.0771 (0.1319)	
training:	Epoch: [9][476/817]	Loss 0.0451 (0.1317)	
training:	Epoch: [9][477/817]	Loss 0.0415 (0.1316)	
training:	Epoch: [9][478/817]	Loss 0.0288 (0.1313)	
training:	Epoch: [9][479/817]	Loss 0.0553 (0.1312)	
training:	Epoch: [9][480/817]	Loss 0.0252 (0.1310)	
training:	Epoch: [9][481/817]	Loss 0.2139 (0.1311)	
training:	Epoch: [9][482/817]	Loss 0.0672 (0.1310)	
training:	Epoch: [9][483/817]	Loss 0.4259 (0.1316)	
training:	Epoch: [9][484/817]	Loss 0.0355 (0.1314)	
training:	Epoch: [9][485/817]	Loss 0.0810 (0.1313)	
training:	Epoch: [9][486/817]	Loss 0.0837 (0.1312)	
training:	Epoch: [9][487/817]	Loss 0.0261 (0.1310)	
training:	Epoch: [9][488/817]	Loss 0.2742 (0.1313)	
training:	Epoch: [9][489/817]	Loss 0.0267 (0.1311)	
training:	Epoch: [9][490/817]	Loss 0.0726 (0.1310)	
training:	Epoch: [9][491/817]	Loss 0.0583 (0.1308)	
training:	Epoch: [9][492/817]	Loss 0.0266 (0.1306)	
training:	Epoch: [9][493/817]	Loss 0.0957 (0.1305)	
training:	Epoch: [9][494/817]	Loss 0.0346 (0.1303)	
training:	Epoch: [9][495/817]	Loss 0.2520 (0.1306)	
training:	Epoch: [9][496/817]	Loss 0.4528 (0.1312)	
training:	Epoch: [9][497/817]	Loss 0.0258 (0.1310)	
training:	Epoch: [9][498/817]	Loss 0.1387 (0.1310)	
training:	Epoch: [9][499/817]	Loss 0.0214 (0.1308)	
training:	Epoch: [9][500/817]	Loss 0.1153 (0.1308)	
training:	Epoch: [9][501/817]	Loss 0.0308 (0.1306)	
training:	Epoch: [9][502/817]	Loss 0.0421 (0.1304)	
training:	Epoch: [9][503/817]	Loss 0.0399 (0.1302)	
training:	Epoch: [9][504/817]	Loss 0.0271 (0.1300)	
training:	Epoch: [9][505/817]	Loss 0.0237 (0.1298)	
training:	Epoch: [9][506/817]	Loss 0.0216 (0.1296)	
training:	Epoch: [9][507/817]	Loss 0.0393 (0.1294)	
training:	Epoch: [9][508/817]	Loss 0.2172 (0.1296)	
training:	Epoch: [9][509/817]	Loss 0.1390 (0.1296)	
training:	Epoch: [9][510/817]	Loss 0.1220 (0.1296)	
training:	Epoch: [9][511/817]	Loss 0.1243 (0.1296)	
training:	Epoch: [9][512/817]	Loss 0.0235 (0.1294)	
training:	Epoch: [9][513/817]	Loss 0.0385 (0.1292)	
training:	Epoch: [9][514/817]	Loss 0.2634 (0.1295)	
training:	Epoch: [9][515/817]	Loss 0.1031 (0.1294)	
training:	Epoch: [9][516/817]	Loss 0.0833 (0.1293)	
training:	Epoch: [9][517/817]	Loss 0.0540 (0.1292)	
training:	Epoch: [9][518/817]	Loss 0.0453 (0.1290)	
training:	Epoch: [9][519/817]	Loss 0.1084 (0.1290)	
training:	Epoch: [9][520/817]	Loss 0.1523 (0.1290)	
training:	Epoch: [9][521/817]	Loss 0.0440 (0.1289)	
training:	Epoch: [9][522/817]	Loss 0.0406 (0.1287)	
training:	Epoch: [9][523/817]	Loss 0.2764 (0.1290)	
training:	Epoch: [9][524/817]	Loss 0.0239 (0.1288)	
training:	Epoch: [9][525/817]	Loss 0.1611 (0.1288)	
training:	Epoch: [9][526/817]	Loss 0.0202 (0.1286)	
training:	Epoch: [9][527/817]	Loss 0.0394 (0.1285)	
training:	Epoch: [9][528/817]	Loss 0.3856 (0.1289)	
training:	Epoch: [9][529/817]	Loss 0.0244 (0.1287)	
training:	Epoch: [9][530/817]	Loss 0.0259 (0.1285)	
training:	Epoch: [9][531/817]	Loss 0.0516 (0.1284)	
training:	Epoch: [9][532/817]	Loss 0.0341 (0.1282)	
training:	Epoch: [9][533/817]	Loss 0.0868 (0.1281)	
training:	Epoch: [9][534/817]	Loss 0.0356 (0.1280)	
training:	Epoch: [9][535/817]	Loss 0.0361 (0.1278)	
training:	Epoch: [9][536/817]	Loss 0.0369 (0.1276)	
training:	Epoch: [9][537/817]	Loss 0.0618 (0.1275)	
training:	Epoch: [9][538/817]	Loss 0.0280 (0.1273)	
training:	Epoch: [9][539/817]	Loss 0.0251 (0.1271)	
training:	Epoch: [9][540/817]	Loss 0.0716 (0.1270)	
training:	Epoch: [9][541/817]	Loss 0.3322 (0.1274)	
training:	Epoch: [9][542/817]	Loss 0.0477 (0.1273)	
training:	Epoch: [9][543/817]	Loss 0.0799 (0.1272)	
training:	Epoch: [9][544/817]	Loss 0.0846 (0.1271)	
training:	Epoch: [9][545/817]	Loss 0.0311 (0.1269)	
training:	Epoch: [9][546/817]	Loss 0.0594 (0.1268)	
training:	Epoch: [9][547/817]	Loss 0.1282 (0.1268)	
training:	Epoch: [9][548/817]	Loss 0.0283 (0.1266)	
training:	Epoch: [9][549/817]	Loss 0.0200 (0.1264)	
training:	Epoch: [9][550/817]	Loss 0.0302 (0.1263)	
training:	Epoch: [9][551/817]	Loss 0.3880 (0.1267)	
training:	Epoch: [9][552/817]	Loss 0.0451 (0.1266)	
training:	Epoch: [9][553/817]	Loss 0.0619 (0.1265)	
training:	Epoch: [9][554/817]	Loss 0.2405 (0.1267)	
training:	Epoch: [9][555/817]	Loss 0.0248 (0.1265)	
training:	Epoch: [9][556/817]	Loss 0.4659 (0.1271)	
training:	Epoch: [9][557/817]	Loss 0.2295 (0.1273)	
training:	Epoch: [9][558/817]	Loss 0.0204 (0.1271)	
training:	Epoch: [9][559/817]	Loss 0.1786 (0.1272)	
training:	Epoch: [9][560/817]	Loss 0.0648 (0.1271)	
training:	Epoch: [9][561/817]	Loss 0.4947 (0.1277)	
training:	Epoch: [9][562/817]	Loss 0.1231 (0.1277)	
training:	Epoch: [9][563/817]	Loss 0.4887 (0.1284)	
training:	Epoch: [9][564/817]	Loss 0.2322 (0.1285)	
training:	Epoch: [9][565/817]	Loss 0.1910 (0.1287)	
training:	Epoch: [9][566/817]	Loss 0.3295 (0.1290)	
training:	Epoch: [9][567/817]	Loss 0.0644 (0.1289)	
training:	Epoch: [9][568/817]	Loss 0.3230 (0.1292)	
training:	Epoch: [9][569/817]	Loss 0.0280 (0.1291)	
training:	Epoch: [9][570/817]	Loss 0.0384 (0.1289)	
training:	Epoch: [9][571/817]	Loss 0.1964 (0.1290)	
training:	Epoch: [9][572/817]	Loss 0.0305 (0.1288)	
training:	Epoch: [9][573/817]	Loss 0.0392 (0.1287)	
training:	Epoch: [9][574/817]	Loss 0.0200 (0.1285)	
training:	Epoch: [9][575/817]	Loss 0.5179 (0.1292)	
training:	Epoch: [9][576/817]	Loss 0.4593 (0.1297)	
training:	Epoch: [9][577/817]	Loss 0.0278 (0.1296)	
training:	Epoch: [9][578/817]	Loss 0.0854 (0.1295)	
training:	Epoch: [9][579/817]	Loss 0.0314 (0.1293)	
training:	Epoch: [9][580/817]	Loss 0.0292 (0.1292)	
training:	Epoch: [9][581/817]	Loss 0.4403 (0.1297)	
training:	Epoch: [9][582/817]	Loss 0.0257 (0.1295)	
training:	Epoch: [9][583/817]	Loss 0.1686 (0.1296)	
training:	Epoch: [9][584/817]	Loss 0.4866 (0.1302)	
training:	Epoch: [9][585/817]	Loss 0.5473 (0.1309)	
training:	Epoch: [9][586/817]	Loss 0.0259 (0.1307)	
training:	Epoch: [9][587/817]	Loss 0.0304 (0.1306)	
training:	Epoch: [9][588/817]	Loss 0.0265 (0.1304)	
training:	Epoch: [9][589/817]	Loss 0.1441 (0.1304)	
training:	Epoch: [9][590/817]	Loss 0.0716 (0.1303)	
training:	Epoch: [9][591/817]	Loss 0.0351 (0.1301)	
training:	Epoch: [9][592/817]	Loss 0.0339 (0.1300)	
training:	Epoch: [9][593/817]	Loss 0.0634 (0.1299)	
training:	Epoch: [9][594/817]	Loss 0.0214 (0.1297)	
training:	Epoch: [9][595/817]	Loss 0.0383 (0.1295)	
training:	Epoch: [9][596/817]	Loss 0.0240 (0.1294)	
training:	Epoch: [9][597/817]	Loss 0.0306 (0.1292)	
training:	Epoch: [9][598/817]	Loss 0.5632 (0.1299)	
training:	Epoch: [9][599/817]	Loss 0.0502 (0.1298)	
training:	Epoch: [9][600/817]	Loss 0.0259 (0.1296)	
training:	Epoch: [9][601/817]	Loss 0.1156 (0.1296)	
training:	Epoch: [9][602/817]	Loss 0.0349 (0.1294)	
training:	Epoch: [9][603/817]	Loss 0.2586 (0.1296)	
training:	Epoch: [9][604/817]	Loss 0.1882 (0.1297)	
training:	Epoch: [9][605/817]	Loss 0.0501 (0.1296)	
training:	Epoch: [9][606/817]	Loss 0.0551 (0.1295)	
training:	Epoch: [9][607/817]	Loss 0.1235 (0.1295)	
training:	Epoch: [9][608/817]	Loss 0.3913 (0.1299)	
training:	Epoch: [9][609/817]	Loss 0.2034 (0.1300)	
training:	Epoch: [9][610/817]	Loss 0.0569 (0.1299)	
training:	Epoch: [9][611/817]	Loss 0.0660 (0.1298)	
training:	Epoch: [9][612/817]	Loss 0.0235 (0.1296)	
training:	Epoch: [9][613/817]	Loss 0.0708 (0.1295)	
training:	Epoch: [9][614/817]	Loss 0.0347 (0.1294)	
training:	Epoch: [9][615/817]	Loss 0.2050 (0.1295)	
training:	Epoch: [9][616/817]	Loss 0.5796 (0.1302)	
training:	Epoch: [9][617/817]	Loss 0.0313 (0.1301)	
training:	Epoch: [9][618/817]	Loss 0.1111 (0.1300)	
training:	Epoch: [9][619/817]	Loss 0.0318 (0.1299)	
training:	Epoch: [9][620/817]	Loss 0.0238 (0.1297)	
training:	Epoch: [9][621/817]	Loss 0.0309 (0.1295)	
training:	Epoch: [9][622/817]	Loss 0.5812 (0.1303)	
training:	Epoch: [9][623/817]	Loss 0.1548 (0.1303)	
training:	Epoch: [9][624/817]	Loss 0.2757 (0.1305)	
training:	Epoch: [9][625/817]	Loss 0.2239 (0.1307)	
training:	Epoch: [9][626/817]	Loss 0.4764 (0.1312)	
training:	Epoch: [9][627/817]	Loss 0.4991 (0.1318)	
training:	Epoch: [9][628/817]	Loss 0.0323 (0.1317)	
training:	Epoch: [9][629/817]	Loss 0.0397 (0.1315)	
training:	Epoch: [9][630/817]	Loss 0.0226 (0.1314)	
training:	Epoch: [9][631/817]	Loss 0.5040 (0.1319)	
training:	Epoch: [9][632/817]	Loss 0.0188 (0.1318)	
training:	Epoch: [9][633/817]	Loss 0.2301 (0.1319)	
training:	Epoch: [9][634/817]	Loss 0.1205 (0.1319)	
training:	Epoch: [9][635/817]	Loss 0.1658 (0.1320)	
training:	Epoch: [9][636/817]	Loss 0.0280 (0.1318)	
training:	Epoch: [9][637/817]	Loss 0.0453 (0.1317)	
training:	Epoch: [9][638/817]	Loss 0.0363 (0.1315)	
training:	Epoch: [9][639/817]	Loss 0.0989 (0.1315)	
training:	Epoch: [9][640/817]	Loss 0.0321 (0.1313)	
training:	Epoch: [9][641/817]	Loss 0.1379 (0.1313)	
training:	Epoch: [9][642/817]	Loss 0.4069 (0.1317)	
training:	Epoch: [9][643/817]	Loss 0.5133 (0.1323)	
training:	Epoch: [9][644/817]	Loss 0.1203 (0.1323)	
training:	Epoch: [9][645/817]	Loss 0.2481 (0.1325)	
training:	Epoch: [9][646/817]	Loss 0.4715 (0.1330)	
training:	Epoch: [9][647/817]	Loss 0.3743 (0.1334)	
training:	Epoch: [9][648/817]	Loss 0.0232 (0.1332)	
training:	Epoch: [9][649/817]	Loss 0.0298 (0.1331)	
training:	Epoch: [9][650/817]	Loss 0.0289 (0.1329)	
training:	Epoch: [9][651/817]	Loss 0.1859 (0.1330)	
training:	Epoch: [9][652/817]	Loss 0.0428 (0.1328)	
training:	Epoch: [9][653/817]	Loss 0.0342 (0.1327)	
training:	Epoch: [9][654/817]	Loss 0.1441 (0.1327)	
training:	Epoch: [9][655/817]	Loss 0.1875 (0.1328)	
training:	Epoch: [9][656/817]	Loss 0.1490 (0.1328)	
training:	Epoch: [9][657/817]	Loss 0.2230 (0.1330)	
training:	Epoch: [9][658/817]	Loss 0.0208 (0.1328)	
training:	Epoch: [9][659/817]	Loss 0.0341 (0.1326)	
training:	Epoch: [9][660/817]	Loss 0.0439 (0.1325)	
training:	Epoch: [9][661/817]	Loss 0.0329 (0.1324)	
training:	Epoch: [9][662/817]	Loss 0.2313 (0.1325)	
training:	Epoch: [9][663/817]	Loss 0.5708 (0.1332)	
training:	Epoch: [9][664/817]	Loss 0.0317 (0.1330)	
training:	Epoch: [9][665/817]	Loss 0.2594 (0.1332)	
training:	Epoch: [9][666/817]	Loss 0.4844 (0.1337)	
training:	Epoch: [9][667/817]	Loss 0.3998 (0.1341)	
training:	Epoch: [9][668/817]	Loss 0.0403 (0.1340)	
training:	Epoch: [9][669/817]	Loss 0.0233 (0.1338)	
training:	Epoch: [9][670/817]	Loss 0.1587 (0.1339)	
training:	Epoch: [9][671/817]	Loss 0.0508 (0.1337)	
training:	Epoch: [9][672/817]	Loss 0.6410 (0.1345)	
training:	Epoch: [9][673/817]	Loss 0.0551 (0.1344)	
training:	Epoch: [9][674/817]	Loss 0.5300 (0.1350)	
training:	Epoch: [9][675/817]	Loss 0.0242 (0.1348)	
training:	Epoch: [9][676/817]	Loss 0.0310 (0.1346)	
training:	Epoch: [9][677/817]	Loss 0.5429 (0.1352)	
training:	Epoch: [9][678/817]	Loss 0.0320 (0.1351)	
training:	Epoch: [9][679/817]	Loss 0.0369 (0.1349)	
training:	Epoch: [9][680/817]	Loss 0.0424 (0.1348)	
training:	Epoch: [9][681/817]	Loss 0.0648 (0.1347)	
training:	Epoch: [9][682/817]	Loss 0.0963 (0.1347)	
training:	Epoch: [9][683/817]	Loss 0.0592 (0.1345)	
training:	Epoch: [9][684/817]	Loss 0.0589 (0.1344)	
training:	Epoch: [9][685/817]	Loss 0.0276 (0.1343)	
training:	Epoch: [9][686/817]	Loss 0.3658 (0.1346)	
training:	Epoch: [9][687/817]	Loss 0.2674 (0.1348)	
training:	Epoch: [9][688/817]	Loss 0.0409 (0.1347)	
training:	Epoch: [9][689/817]	Loss 0.0387 (0.1345)	
training:	Epoch: [9][690/817]	Loss 0.0236 (0.1344)	
training:	Epoch: [9][691/817]	Loss 0.0372 (0.1342)	
training:	Epoch: [9][692/817]	Loss 0.0269 (0.1341)	
training:	Epoch: [9][693/817]	Loss 0.0303 (0.1339)	
training:	Epoch: [9][694/817]	Loss 0.0302 (0.1338)	
training:	Epoch: [9][695/817]	Loss 0.0282 (0.1336)	
training:	Epoch: [9][696/817]	Loss 0.0606 (0.1335)	
training:	Epoch: [9][697/817]	Loss 0.0236 (0.1334)	
training:	Epoch: [9][698/817]	Loss 0.0414 (0.1332)	
training:	Epoch: [9][699/817]	Loss 0.0337 (0.1331)	
training:	Epoch: [9][700/817]	Loss 0.3252 (0.1334)	
training:	Epoch: [9][701/817]	Loss 0.2253 (0.1335)	
training:	Epoch: [9][702/817]	Loss 0.0191 (0.1333)	
training:	Epoch: [9][703/817]	Loss 0.0829 (0.1333)	
training:	Epoch: [9][704/817]	Loss 0.0731 (0.1332)	
training:	Epoch: [9][705/817]	Loss 0.0334 (0.1330)	
training:	Epoch: [9][706/817]	Loss 0.0884 (0.1330)	
training:	Epoch: [9][707/817]	Loss 0.0320 (0.1328)	
training:	Epoch: [9][708/817]	Loss 0.0314 (0.1327)	
training:	Epoch: [9][709/817]	Loss 0.3771 (0.1330)	
training:	Epoch: [9][710/817]	Loss 0.1293 (0.1330)	
training:	Epoch: [9][711/817]	Loss 0.0552 (0.1329)	
training:	Epoch: [9][712/817]	Loss 0.0510 (0.1328)	
training:	Epoch: [9][713/817]	Loss 0.2125 (0.1329)	
training:	Epoch: [9][714/817]	Loss 0.0209 (0.1328)	
training:	Epoch: [9][715/817]	Loss 0.0517 (0.1326)	
training:	Epoch: [9][716/817]	Loss 0.0295 (0.1325)	
training:	Epoch: [9][717/817]	Loss 0.4228 (0.1329)	
training:	Epoch: [9][718/817]	Loss 0.4203 (0.1333)	
training:	Epoch: [9][719/817]	Loss 0.0404 (0.1332)	
training:	Epoch: [9][720/817]	Loss 0.1707 (0.1332)	
training:	Epoch: [9][721/817]	Loss 0.0354 (0.1331)	
training:	Epoch: [9][722/817]	Loss 0.0328 (0.1329)	
training:	Epoch: [9][723/817]	Loss 0.0383 (0.1328)	
training:	Epoch: [9][724/817]	Loss 0.0222 (0.1327)	
training:	Epoch: [9][725/817]	Loss 0.4716 (0.1331)	
training:	Epoch: [9][726/817]	Loss 0.0939 (0.1331)	
training:	Epoch: [9][727/817]	Loss 0.3185 (0.1333)	
training:	Epoch: [9][728/817]	Loss 0.1799 (0.1334)	
training:	Epoch: [9][729/817]	Loss 0.2173 (0.1335)	
training:	Epoch: [9][730/817]	Loss 0.0847 (0.1334)	
training:	Epoch: [9][731/817]	Loss 0.5939 (0.1341)	
training:	Epoch: [9][732/817]	Loss 0.4838 (0.1346)	
training:	Epoch: [9][733/817]	Loss 0.0315 (0.1344)	
training:	Epoch: [9][734/817]	Loss 0.0311 (0.1343)	
training:	Epoch: [9][735/817]	Loss 0.1827 (0.1343)	
training:	Epoch: [9][736/817]	Loss 0.0283 (0.1342)	
training:	Epoch: [9][737/817]	Loss 0.0728 (0.1341)	
training:	Epoch: [9][738/817]	Loss 0.0370 (0.1340)	
training:	Epoch: [9][739/817]	Loss 0.0376 (0.1338)	
training:	Epoch: [9][740/817]	Loss 0.0535 (0.1337)	
training:	Epoch: [9][741/817]	Loss 0.0327 (0.1336)	
training:	Epoch: [9][742/817]	Loss 0.0285 (0.1335)	
training:	Epoch: [9][743/817]	Loss 0.4344 (0.1339)	
training:	Epoch: [9][744/817]	Loss 0.0297 (0.1337)	
training:	Epoch: [9][745/817]	Loss 0.0882 (0.1337)	
training:	Epoch: [9][746/817]	Loss 0.1531 (0.1337)	
training:	Epoch: [9][747/817]	Loss 0.0369 (0.1336)	
training:	Epoch: [9][748/817]	Loss 0.5029 (0.1341)	
training:	Epoch: [9][749/817]	Loss 0.0323 (0.1339)	
training:	Epoch: [9][750/817]	Loss 0.0286 (0.1338)	
training:	Epoch: [9][751/817]	Loss 0.0741 (0.1337)	
training:	Epoch: [9][752/817]	Loss 0.0325 (0.1336)	
training:	Epoch: [9][753/817]	Loss 0.0446 (0.1334)	
training:	Epoch: [9][754/817]	Loss 0.3705 (0.1338)	
training:	Epoch: [9][755/817]	Loss 0.0478 (0.1336)	
training:	Epoch: [9][756/817]	Loss 0.0649 (0.1336)	
training:	Epoch: [9][757/817]	Loss 0.0263 (0.1334)	
training:	Epoch: [9][758/817]	Loss 0.0280 (0.1333)	
training:	Epoch: [9][759/817]	Loss 0.0488 (0.1332)	
training:	Epoch: [9][760/817]	Loss 0.0466 (0.1330)	
training:	Epoch: [9][761/817]	Loss 0.0716 (0.1330)	
training:	Epoch: [9][762/817]	Loss 0.1792 (0.1330)	
training:	Epoch: [9][763/817]	Loss 0.0224 (0.1329)	
training:	Epoch: [9][764/817]	Loss 0.0657 (0.1328)	
training:	Epoch: [9][765/817]	Loss 0.1503 (0.1328)	
training:	Epoch: [9][766/817]	Loss 0.0264 (0.1327)	
training:	Epoch: [9][767/817]	Loss 0.0286 (0.1325)	
training:	Epoch: [9][768/817]	Loss 0.0419 (0.1324)	
training:	Epoch: [9][769/817]	Loss 0.0699 (0.1323)	
training:	Epoch: [9][770/817]	Loss 0.0471 (0.1322)	
training:	Epoch: [9][771/817]	Loss 0.0263 (0.1321)	
training:	Epoch: [9][772/817]	Loss 0.0454 (0.1320)	
training:	Epoch: [9][773/817]	Loss 0.4866 (0.1324)	
training:	Epoch: [9][774/817]	Loss 0.1372 (0.1325)	
training:	Epoch: [9][775/817]	Loss 0.1017 (0.1324)	
training:	Epoch: [9][776/817]	Loss 0.0207 (0.1323)	
training:	Epoch: [9][777/817]	Loss 0.0289 (0.1321)	
training:	Epoch: [9][778/817]	Loss 0.1033 (0.1321)	
training:	Epoch: [9][779/817]	Loss 0.0320 (0.1320)	
training:	Epoch: [9][780/817]	Loss 0.3596 (0.1323)	
training:	Epoch: [9][781/817]	Loss 0.0547 (0.1322)	
training:	Epoch: [9][782/817]	Loss 0.0522 (0.1321)	
training:	Epoch: [9][783/817]	Loss 0.0895 (0.1320)	
training:	Epoch: [9][784/817]	Loss 0.3731 (0.1323)	
training:	Epoch: [9][785/817]	Loss 0.0213 (0.1322)	
training:	Epoch: [9][786/817]	Loss 0.5282 (0.1327)	
training:	Epoch: [9][787/817]	Loss 0.0289 (0.1325)	
training:	Epoch: [9][788/817]	Loss 0.0247 (0.1324)	
training:	Epoch: [9][789/817]	Loss 0.5647 (0.1330)	
training:	Epoch: [9][790/817]	Loss 0.0600 (0.1329)	
training:	Epoch: [9][791/817]	Loss 0.0412 (0.1327)	
training:	Epoch: [9][792/817]	Loss 0.0237 (0.1326)	
training:	Epoch: [9][793/817]	Loss 0.0254 (0.1325)	
training:	Epoch: [9][794/817]	Loss 0.0984 (0.1324)	
training:	Epoch: [9][795/817]	Loss 0.1530 (0.1325)	
training:	Epoch: [9][796/817]	Loss 0.4072 (0.1328)	
training:	Epoch: [9][797/817]	Loss 0.0356 (0.1327)	
training:	Epoch: [9][798/817]	Loss 0.1192 (0.1327)	
training:	Epoch: [9][799/817]	Loss 0.5406 (0.1332)	
training:	Epoch: [9][800/817]	Loss 0.0341 (0.1330)	
training:	Epoch: [9][801/817]	Loss 0.0787 (0.1330)	
training:	Epoch: [9][802/817]	Loss 0.1588 (0.1330)	
training:	Epoch: [9][803/817]	Loss 0.0392 (0.1329)	
training:	Epoch: [9][804/817]	Loss 0.1498 (0.1329)	
training:	Epoch: [9][805/817]	Loss 0.0261 (0.1328)	
training:	Epoch: [9][806/817]	Loss 0.4635 (0.1332)	
training:	Epoch: [9][807/817]	Loss 0.4527 (0.1336)	
training:	Epoch: [9][808/817]	Loss 0.5372 (0.1341)	
training:	Epoch: [9][809/817]	Loss 0.0340 (0.1340)	
training:	Epoch: [9][810/817]	Loss 0.0571 (0.1339)	
training:	Epoch: [9][811/817]	Loss 0.0757 (0.1338)	
training:	Epoch: [9][812/817]	Loss 0.2952 (0.1340)	
training:	Epoch: [9][813/817]	Loss 0.0257 (0.1339)	
training:	Epoch: [9][814/817]	Loss 0.5536 (0.1344)	
training:	Epoch: [9][815/817]	Loss 0.1029 (0.1343)	
training:	Epoch: [9][816/817]	Loss 0.1393 (0.1343)	
training:	Epoch: [9][817/817]	Loss 0.0269 (0.1342)	
Training:	 Loss: 0.1342

Training:	 ACC: 0.9733 0.9726 0.9562 0.9904
Validation:	 ACC: 0.8019 0.8004 0.7697 0.8341
Validation:	 Best_BACC: 0.8223 0.8218 0.8127 0.8318
Validation:	 Loss: 0.5726
Pretraining:	Epoch 10/200
----------
training:	Epoch: [10][1/817]	Loss 0.4671 (0.4671)	
training:	Epoch: [10][2/817]	Loss 0.5014 (0.4842)	
training:	Epoch: [10][3/817]	Loss 0.0313 (0.3333)	
training:	Epoch: [10][4/817]	Loss 0.1258 (0.2814)	
training:	Epoch: [10][5/817]	Loss 0.0392 (0.2329)	
training:	Epoch: [10][6/817]	Loss 0.0407 (0.2009)	
training:	Epoch: [10][7/817]	Loss 0.0377 (0.1776)	
training:	Epoch: [10][8/817]	Loss 0.0668 (0.1637)	
training:	Epoch: [10][9/817]	Loss 0.0527 (0.1514)	
training:	Epoch: [10][10/817]	Loss 0.0437 (0.1406)	
training:	Epoch: [10][11/817]	Loss 0.0316 (0.1307)	
training:	Epoch: [10][12/817]	Loss 0.0253 (0.1219)	
training:	Epoch: [10][13/817]	Loss 0.0927 (0.1197)	
training:	Epoch: [10][14/817]	Loss 0.2798 (0.1311)	
training:	Epoch: [10][15/817]	Loss 0.4090 (0.1496)	
training:	Epoch: [10][16/817]	Loss 0.0295 (0.1421)	
training:	Epoch: [10][17/817]	Loss 0.0619 (0.1374)	
training:	Epoch: [10][18/817]	Loss 0.0395 (0.1320)	
training:	Epoch: [10][19/817]	Loss 0.0327 (0.1268)	
training:	Epoch: [10][20/817]	Loss 0.0297 (0.1219)	
training:	Epoch: [10][21/817]	Loss 0.0273 (0.1174)	
training:	Epoch: [10][22/817]	Loss 0.0570 (0.1147)	
training:	Epoch: [10][23/817]	Loss 0.1488 (0.1161)	
training:	Epoch: [10][24/817]	Loss 0.0194 (0.1121)	
training:	Epoch: [10][25/817]	Loss 0.0530 (0.1097)	
training:	Epoch: [10][26/817]	Loss 0.0317 (0.1067)	
training:	Epoch: [10][27/817]	Loss 0.0324 (0.1040)	
training:	Epoch: [10][28/817]	Loss 0.0305 (0.1014)	
training:	Epoch: [10][29/817]	Loss 0.0821 (0.1007)	
training:	Epoch: [10][30/817]	Loss 0.0329 (0.0984)	
training:	Epoch: [10][31/817]	Loss 0.0397 (0.0965)	
training:	Epoch: [10][32/817]	Loss 0.0219 (0.0942)	
training:	Epoch: [10][33/817]	Loss 0.0315 (0.0923)	
training:	Epoch: [10][34/817]	Loss 0.0302 (0.0905)	
training:	Epoch: [10][35/817]	Loss 0.0200 (0.0885)	
training:	Epoch: [10][36/817]	Loss 0.0324 (0.0869)	
training:	Epoch: [10][37/817]	Loss 0.0312 (0.0854)	
training:	Epoch: [10][38/817]	Loss 0.0520 (0.0845)	
training:	Epoch: [10][39/817]	Loss 0.0763 (0.0843)	
training:	Epoch: [10][40/817]	Loss 0.0612 (0.0837)	
training:	Epoch: [10][41/817]	Loss 0.0273 (0.0824)	
training:	Epoch: [10][42/817]	Loss 0.0459 (0.0815)	
training:	Epoch: [10][43/817]	Loss 0.2713 (0.0859)	
training:	Epoch: [10][44/817]	Loss 0.0482 (0.0850)	
training:	Epoch: [10][45/817]	Loss 0.0414 (0.0841)	
training:	Epoch: [10][46/817]	Loss 0.4966 (0.0930)	
training:	Epoch: [10][47/817]	Loss 0.0216 (0.0915)	
training:	Epoch: [10][48/817]	Loss 0.0359 (0.0904)	
training:	Epoch: [10][49/817]	Loss 0.0296 (0.0891)	
training:	Epoch: [10][50/817]	Loss 0.8977 (0.1053)	
training:	Epoch: [10][51/817]	Loss 0.0279 (0.1038)	
training:	Epoch: [10][52/817]	Loss 0.0379 (0.1025)	
training:	Epoch: [10][53/817]	Loss 0.4378 (0.1088)	
training:	Epoch: [10][54/817]	Loss 0.0310 (0.1074)	
training:	Epoch: [10][55/817]	Loss 0.0194 (0.1058)	
training:	Epoch: [10][56/817]	Loss 0.1122 (0.1059)	
training:	Epoch: [10][57/817]	Loss 0.0565 (0.1050)	
training:	Epoch: [10][58/817]	Loss 0.0189 (0.1036)	
training:	Epoch: [10][59/817]	Loss 0.0413 (0.1025)	
training:	Epoch: [10][60/817]	Loss 0.0380 (0.1014)	
training:	Epoch: [10][61/817]	Loss 0.0196 (0.1001)	
training:	Epoch: [10][62/817]	Loss 0.0678 (0.0996)	
training:	Epoch: [10][63/817]	Loss 0.0238 (0.0984)	
training:	Epoch: [10][64/817]	Loss 0.0308 (0.0973)	
training:	Epoch: [10][65/817]	Loss 0.0218 (0.0961)	
training:	Epoch: [10][66/817]	Loss 0.0426 (0.0953)	
training:	Epoch: [10][67/817]	Loss 0.0367 (0.0945)	
training:	Epoch: [10][68/817]	Loss 0.0296 (0.0935)	
training:	Epoch: [10][69/817]	Loss 0.0567 (0.0930)	
training:	Epoch: [10][70/817]	Loss 0.0284 (0.0921)	
training:	Epoch: [10][71/817]	Loss 0.0251 (0.0911)	
training:	Epoch: [10][72/817]	Loss 0.0569 (0.0906)	
training:	Epoch: [10][73/817]	Loss 0.0995 (0.0908)	
training:	Epoch: [10][74/817]	Loss 0.0299 (0.0899)	
training:	Epoch: [10][75/817]	Loss 0.0907 (0.0899)	
training:	Epoch: [10][76/817]	Loss 0.0259 (0.0891)	
training:	Epoch: [10][77/817]	Loss 0.0214 (0.0882)	
training:	Epoch: [10][78/817]	Loss 0.6532 (0.0955)	
training:	Epoch: [10][79/817]	Loss 0.0195 (0.0945)	
training:	Epoch: [10][80/817]	Loss 0.0260 (0.0936)	
training:	Epoch: [10][81/817]	Loss 0.0321 (0.0929)	
training:	Epoch: [10][82/817]	Loss 0.0449 (0.0923)	
training:	Epoch: [10][83/817]	Loss 0.0552 (0.0919)	
training:	Epoch: [10][84/817]	Loss 0.0495 (0.0913)	
training:	Epoch: [10][85/817]	Loss 0.0454 (0.0908)	
training:	Epoch: [10][86/817]	Loss 0.0217 (0.0900)	
training:	Epoch: [10][87/817]	Loss 0.4578 (0.0942)	
training:	Epoch: [10][88/817]	Loss 0.0215 (0.0934)	
training:	Epoch: [10][89/817]	Loss 0.3460 (0.0962)	
training:	Epoch: [10][90/817]	Loss 0.0554 (0.0958)	
training:	Epoch: [10][91/817]	Loss 0.0202 (0.0950)	
training:	Epoch: [10][92/817]	Loss 0.2831 (0.0970)	
training:	Epoch: [10][93/817]	Loss 0.0284 (0.0963)	
training:	Epoch: [10][94/817]	Loss 0.0268 (0.0955)	
training:	Epoch: [10][95/817]	Loss 0.3134 (0.0978)	
training:	Epoch: [10][96/817]	Loss 0.0205 (0.0970)	
training:	Epoch: [10][97/817]	Loss 0.0386 (0.0964)	
training:	Epoch: [10][98/817]	Loss 0.0413 (0.0958)	
training:	Epoch: [10][99/817]	Loss 0.1185 (0.0961)	
training:	Epoch: [10][100/817]	Loss 0.1213 (0.0963)	
training:	Epoch: [10][101/817]	Loss 0.0236 (0.0956)	
training:	Epoch: [10][102/817]	Loss 0.0221 (0.0949)	
training:	Epoch: [10][103/817]	Loss 0.3370 (0.0972)	
training:	Epoch: [10][104/817]	Loss 0.0579 (0.0969)	
training:	Epoch: [10][105/817]	Loss 0.0256 (0.0962)	
training:	Epoch: [10][106/817]	Loss 0.0184 (0.0954)	
training:	Epoch: [10][107/817]	Loss 0.0240 (0.0948)	
training:	Epoch: [10][108/817]	Loss 0.1085 (0.0949)	
training:	Epoch: [10][109/817]	Loss 0.3884 (0.0976)	
training:	Epoch: [10][110/817]	Loss 0.0676 (0.0973)	
training:	Epoch: [10][111/817]	Loss 0.0488 (0.0969)	
training:	Epoch: [10][112/817]	Loss 0.0482 (0.0965)	
training:	Epoch: [10][113/817]	Loss 0.0234 (0.0958)	
training:	Epoch: [10][114/817]	Loss 0.0685 (0.0956)	
training:	Epoch: [10][115/817]	Loss 0.0249 (0.0950)	
training:	Epoch: [10][116/817]	Loss 0.1414 (0.0954)	
training:	Epoch: [10][117/817]	Loss 0.0200 (0.0947)	
training:	Epoch: [10][118/817]	Loss 0.2015 (0.0956)	
training:	Epoch: [10][119/817]	Loss 0.2466 (0.0969)	
training:	Epoch: [10][120/817]	Loss 0.0229 (0.0963)	
training:	Epoch: [10][121/817]	Loss 0.0552 (0.0959)	
training:	Epoch: [10][122/817]	Loss 0.1045 (0.0960)	
training:	Epoch: [10][123/817]	Loss 0.0262 (0.0954)	
training:	Epoch: [10][124/817]	Loss 0.0641 (0.0952)	
training:	Epoch: [10][125/817]	Loss 0.0172 (0.0946)	
training:	Epoch: [10][126/817]	Loss 0.0262 (0.0940)	
training:	Epoch: [10][127/817]	Loss 0.0291 (0.0935)	
training:	Epoch: [10][128/817]	Loss 0.0260 (0.0930)	
training:	Epoch: [10][129/817]	Loss 0.2279 (0.0940)	
training:	Epoch: [10][130/817]	Loss 0.0196 (0.0935)	
training:	Epoch: [10][131/817]	Loss 0.0231 (0.0929)	
training:	Epoch: [10][132/817]	Loss 0.5645 (0.0965)	
training:	Epoch: [10][133/817]	Loss 0.1191 (0.0967)	
training:	Epoch: [10][134/817]	Loss 0.0418 (0.0962)	
training:	Epoch: [10][135/817]	Loss 0.0360 (0.0958)	
training:	Epoch: [10][136/817]	Loss 0.0751 (0.0956)	
training:	Epoch: [10][137/817]	Loss 0.0316 (0.0952)	
training:	Epoch: [10][138/817]	Loss 0.1455 (0.0955)	
training:	Epoch: [10][139/817]	Loss 0.0528 (0.0952)	
training:	Epoch: [10][140/817]	Loss 0.0361 (0.0948)	
training:	Epoch: [10][141/817]	Loss 0.0333 (0.0944)	
training:	Epoch: [10][142/817]	Loss 0.1540 (0.0948)	
training:	Epoch: [10][143/817]	Loss 0.4689 (0.0974)	
training:	Epoch: [10][144/817]	Loss 0.1897 (0.0981)	
training:	Epoch: [10][145/817]	Loss 0.0463 (0.0977)	
training:	Epoch: [10][146/817]	Loss 0.0281 (0.0972)	
training:	Epoch: [10][147/817]	Loss 0.0615 (0.0970)	
training:	Epoch: [10][148/817]	Loss 0.0254 (0.0965)	
training:	Epoch: [10][149/817]	Loss 0.0185 (0.0960)	
training:	Epoch: [10][150/817]	Loss 0.0207 (0.0955)	
training:	Epoch: [10][151/817]	Loss 0.0307 (0.0950)	
training:	Epoch: [10][152/817]	Loss 0.3183 (0.0965)	
training:	Epoch: [10][153/817]	Loss 0.0278 (0.0961)	
training:	Epoch: [10][154/817]	Loss 0.0630 (0.0958)	
training:	Epoch: [10][155/817]	Loss 0.0427 (0.0955)	
training:	Epoch: [10][156/817]	Loss 0.0328 (0.0951)	
training:	Epoch: [10][157/817]	Loss 0.0243 (0.0946)	
training:	Epoch: [10][158/817]	Loss 0.0371 (0.0943)	
training:	Epoch: [10][159/817]	Loss 0.0262 (0.0939)	
training:	Epoch: [10][160/817]	Loss 0.0420 (0.0935)	
training:	Epoch: [10][161/817]	Loss 0.0226 (0.0931)	
training:	Epoch: [10][162/817]	Loss 0.0872 (0.0931)	
training:	Epoch: [10][163/817]	Loss 0.0682 (0.0929)	
training:	Epoch: [10][164/817]	Loss 0.0198 (0.0925)	
training:	Epoch: [10][165/817]	Loss 0.0242 (0.0920)	
training:	Epoch: [10][166/817]	Loss 0.0178 (0.0916)	
training:	Epoch: [10][167/817]	Loss 0.0220 (0.0912)	
training:	Epoch: [10][168/817]	Loss 0.0445 (0.0909)	
training:	Epoch: [10][169/817]	Loss 0.0216 (0.0905)	
training:	Epoch: [10][170/817]	Loss 0.0465 (0.0902)	
training:	Epoch: [10][171/817]	Loss 0.0501 (0.0900)	
training:	Epoch: [10][172/817]	Loss 0.0272 (0.0896)	
training:	Epoch: [10][173/817]	Loss 0.0189 (0.0892)	
training:	Epoch: [10][174/817]	Loss 0.0738 (0.0891)	
training:	Epoch: [10][175/817]	Loss 0.0529 (0.0889)	
training:	Epoch: [10][176/817]	Loss 0.0273 (0.0886)	
training:	Epoch: [10][177/817]	Loss 0.0233 (0.0882)	
training:	Epoch: [10][178/817]	Loss 0.0224 (0.0878)	
training:	Epoch: [10][179/817]	Loss 0.0259 (0.0875)	
training:	Epoch: [10][180/817]	Loss 0.0488 (0.0873)	
training:	Epoch: [10][181/817]	Loss 0.5861 (0.0900)	
training:	Epoch: [10][182/817]	Loss 0.1250 (0.0902)	
training:	Epoch: [10][183/817]	Loss 0.0955 (0.0903)	
training:	Epoch: [10][184/817]	Loss 0.0206 (0.0899)	
training:	Epoch: [10][185/817]	Loss 0.2624 (0.0908)	
training:	Epoch: [10][186/817]	Loss 0.0252 (0.0905)	
training:	Epoch: [10][187/817]	Loss 0.0193 (0.0901)	
training:	Epoch: [10][188/817]	Loss 0.0470 (0.0898)	
training:	Epoch: [10][189/817]	Loss 0.0427 (0.0896)	
training:	Epoch: [10][190/817]	Loss 0.0428 (0.0894)	
training:	Epoch: [10][191/817]	Loss 0.0320 (0.0891)	
training:	Epoch: [10][192/817]	Loss 0.0516 (0.0889)	
training:	Epoch: [10][193/817]	Loss 0.1097 (0.0890)	
training:	Epoch: [10][194/817]	Loss 0.0212 (0.0886)	
training:	Epoch: [10][195/817]	Loss 0.0196 (0.0883)	
training:	Epoch: [10][196/817]	Loss 0.0264 (0.0879)	
training:	Epoch: [10][197/817]	Loss 0.0185 (0.0876)	
training:	Epoch: [10][198/817]	Loss 0.0240 (0.0873)	
training:	Epoch: [10][199/817]	Loss 0.0267 (0.0870)	
training:	Epoch: [10][200/817]	Loss 0.0215 (0.0866)	
training:	Epoch: [10][201/817]	Loss 0.0186 (0.0863)	
training:	Epoch: [10][202/817]	Loss 0.0331 (0.0860)	
training:	Epoch: [10][203/817]	Loss 0.2044 (0.0866)	
training:	Epoch: [10][204/817]	Loss 0.0212 (0.0863)	
training:	Epoch: [10][205/817]	Loss 0.0203 (0.0860)	
training:	Epoch: [10][206/817]	Loss 0.0248 (0.0857)	
training:	Epoch: [10][207/817]	Loss 0.0349 (0.0854)	
training:	Epoch: [10][208/817]	Loss 0.0430 (0.0852)	
training:	Epoch: [10][209/817]	Loss 0.0180 (0.0849)	
training:	Epoch: [10][210/817]	Loss 0.6304 (0.0875)	
training:	Epoch: [10][211/817]	Loss 0.0190 (0.0872)	
training:	Epoch: [10][212/817]	Loss 0.5232 (0.0892)	
training:	Epoch: [10][213/817]	Loss 0.0259 (0.0889)	
training:	Epoch: [10][214/817]	Loss 0.0227 (0.0886)	
training:	Epoch: [10][215/817]	Loss 0.0192 (0.0883)	
training:	Epoch: [10][216/817]	Loss 0.0269 (0.0880)	
training:	Epoch: [10][217/817]	Loss 0.0252 (0.0877)	
training:	Epoch: [10][218/817]	Loss 0.0236 (0.0874)	
training:	Epoch: [10][219/817]	Loss 0.0789 (0.0874)	
training:	Epoch: [10][220/817]	Loss 0.0202 (0.0871)	
training:	Epoch: [10][221/817]	Loss 0.0219 (0.0868)	
training:	Epoch: [10][222/817]	Loss 0.0225 (0.0865)	
training:	Epoch: [10][223/817]	Loss 0.1459 (0.0868)	
training:	Epoch: [10][224/817]	Loss 0.0241 (0.0865)	
training:	Epoch: [10][225/817]	Loss 0.5421 (0.0885)	
training:	Epoch: [10][226/817]	Loss 0.0173 (0.0882)	
training:	Epoch: [10][227/817]	Loss 0.2036 (0.0887)	
training:	Epoch: [10][228/817]	Loss 0.5017 (0.0905)	
training:	Epoch: [10][229/817]	Loss 0.0431 (0.0903)	
training:	Epoch: [10][230/817]	Loss 0.0202 (0.0900)	
training:	Epoch: [10][231/817]	Loss 0.0237 (0.0897)	
training:	Epoch: [10][232/817]	Loss 0.0566 (0.0896)	
training:	Epoch: [10][233/817]	Loss 0.2191 (0.0901)	
training:	Epoch: [10][234/817]	Loss 0.0210 (0.0898)	
training:	Epoch: [10][235/817]	Loss 0.0178 (0.0895)	
training:	Epoch: [10][236/817]	Loss 0.0213 (0.0892)	
training:	Epoch: [10][237/817]	Loss 0.0193 (0.0890)	
training:	Epoch: [10][238/817]	Loss 0.0316 (0.0887)	
training:	Epoch: [10][239/817]	Loss 0.0425 (0.0885)	
training:	Epoch: [10][240/817]	Loss 0.0201 (0.0882)	
training:	Epoch: [10][241/817]	Loss 0.0213 (0.0880)	
training:	Epoch: [10][242/817]	Loss 0.0365 (0.0877)	
training:	Epoch: [10][243/817]	Loss 0.4699 (0.0893)	
training:	Epoch: [10][244/817]	Loss 0.6159 (0.0915)	
training:	Epoch: [10][245/817]	Loss 0.0759 (0.0914)	
training:	Epoch: [10][246/817]	Loss 0.0436 (0.0912)	
training:	Epoch: [10][247/817]	Loss 0.0352 (0.0910)	
training:	Epoch: [10][248/817]	Loss 0.0281 (0.0907)	
training:	Epoch: [10][249/817]	Loss 0.0281 (0.0905)	
training:	Epoch: [10][250/817]	Loss 0.0475 (0.0903)	
training:	Epoch: [10][251/817]	Loss 0.4953 (0.0919)	
training:	Epoch: [10][252/817]	Loss 0.0247 (0.0917)	
training:	Epoch: [10][253/817]	Loss 0.6444 (0.0938)	
training:	Epoch: [10][254/817]	Loss 0.0231 (0.0936)	
training:	Epoch: [10][255/817]	Loss 0.5691 (0.0954)	
training:	Epoch: [10][256/817]	Loss 0.0337 (0.0952)	
training:	Epoch: [10][257/817]	Loss 0.0248 (0.0949)	
training:	Epoch: [10][258/817]	Loss 0.3293 (0.0958)	
training:	Epoch: [10][259/817]	Loss 0.0312 (0.0956)	
training:	Epoch: [10][260/817]	Loss 0.1153 (0.0957)	
training:	Epoch: [10][261/817]	Loss 0.1740 (0.0960)	
training:	Epoch: [10][262/817]	Loss 0.4497 (0.0973)	
training:	Epoch: [10][263/817]	Loss 0.0193 (0.0970)	
training:	Epoch: [10][264/817]	Loss 0.0194 (0.0967)	
training:	Epoch: [10][265/817]	Loss 0.0208 (0.0964)	
training:	Epoch: [10][266/817]	Loss 0.0192 (0.0961)	
training:	Epoch: [10][267/817]	Loss 0.0573 (0.0960)	
training:	Epoch: [10][268/817]	Loss 0.0659 (0.0959)	
training:	Epoch: [10][269/817]	Loss 0.0510 (0.0957)	
training:	Epoch: [10][270/817]	Loss 0.0345 (0.0955)	
training:	Epoch: [10][271/817]	Loss 0.0284 (0.0952)	
training:	Epoch: [10][272/817]	Loss 0.0279 (0.0950)	
training:	Epoch: [10][273/817]	Loss 0.0434 (0.0948)	
training:	Epoch: [10][274/817]	Loss 0.5516 (0.0965)	
training:	Epoch: [10][275/817]	Loss 0.5559 (0.0981)	
training:	Epoch: [10][276/817]	Loss 0.0249 (0.0979)	
training:	Epoch: [10][277/817]	Loss 0.0196 (0.0976)	
training:	Epoch: [10][278/817]	Loss 0.4911 (0.0990)	
training:	Epoch: [10][279/817]	Loss 0.0711 (0.0989)	
training:	Epoch: [10][280/817]	Loss 0.0624 (0.0988)	
training:	Epoch: [10][281/817]	Loss 0.0502 (0.0986)	
training:	Epoch: [10][282/817]	Loss 0.0407 (0.0984)	
training:	Epoch: [10][283/817]	Loss 0.0262 (0.0981)	
training:	Epoch: [10][284/817]	Loss 0.0200 (0.0979)	
training:	Epoch: [10][285/817]	Loss 0.0813 (0.0978)	
training:	Epoch: [10][286/817]	Loss 0.0399 (0.0976)	
training:	Epoch: [10][287/817]	Loss 0.0216 (0.0973)	
training:	Epoch: [10][288/817]	Loss 0.0200 (0.0971)	
training:	Epoch: [10][289/817]	Loss 0.1406 (0.0972)	
training:	Epoch: [10][290/817]	Loss 0.0250 (0.0970)	
training:	Epoch: [10][291/817]	Loss 0.5085 (0.0984)	
training:	Epoch: [10][292/817]	Loss 0.0403 (0.0982)	
training:	Epoch: [10][293/817]	Loss 0.0212 (0.0979)	
training:	Epoch: [10][294/817]	Loss 1.3559 (0.1022)	
training:	Epoch: [10][295/817]	Loss 0.0337 (0.1020)	
training:	Epoch: [10][296/817]	Loss 0.0975 (0.1020)	
training:	Epoch: [10][297/817]	Loss 0.0187 (0.1017)	
training:	Epoch: [10][298/817]	Loss 0.0249 (0.1014)	
training:	Epoch: [10][299/817]	Loss 0.0177 (0.1011)	
training:	Epoch: [10][300/817]	Loss 0.0169 (0.1009)	
training:	Epoch: [10][301/817]	Loss 0.5142 (0.1022)	
training:	Epoch: [10][302/817]	Loss 0.0199 (0.1020)	
training:	Epoch: [10][303/817]	Loss 0.0634 (0.1018)	
training:	Epoch: [10][304/817]	Loss 0.0566 (0.1017)	
training:	Epoch: [10][305/817]	Loss 0.0267 (0.1014)	
training:	Epoch: [10][306/817]	Loss 0.0182 (0.1012)	
training:	Epoch: [10][307/817]	Loss 0.0166 (0.1009)	
training:	Epoch: [10][308/817]	Loss 0.0186 (0.1006)	
training:	Epoch: [10][309/817]	Loss 0.0434 (0.1004)	
training:	Epoch: [10][310/817]	Loss 0.0180 (0.1002)	
training:	Epoch: [10][311/817]	Loss 0.0822 (0.1001)	
training:	Epoch: [10][312/817]	Loss 0.4110 (0.1011)	
training:	Epoch: [10][313/817]	Loss 0.1057 (0.1011)	
training:	Epoch: [10][314/817]	Loss 0.0927 (0.1011)	
training:	Epoch: [10][315/817]	Loss 0.0379 (0.1009)	
training:	Epoch: [10][316/817]	Loss 0.5073 (0.1022)	
training:	Epoch: [10][317/817]	Loss 0.0271 (0.1019)	
training:	Epoch: [10][318/817]	Loss 0.0200 (0.1017)	
training:	Epoch: [10][319/817]	Loss 0.1838 (0.1019)	
training:	Epoch: [10][320/817]	Loss 0.4912 (0.1032)	
training:	Epoch: [10][321/817]	Loss 0.0198 (0.1029)	
training:	Epoch: [10][322/817]	Loss 0.0197 (0.1026)	
training:	Epoch: [10][323/817]	Loss 0.0217 (0.1024)	
training:	Epoch: [10][324/817]	Loss 0.0365 (0.1022)	
training:	Epoch: [10][325/817]	Loss 0.0212 (0.1019)	
training:	Epoch: [10][326/817]	Loss 0.0366 (0.1017)	
training:	Epoch: [10][327/817]	Loss 0.0235 (0.1015)	
training:	Epoch: [10][328/817]	Loss 0.0427 (0.1013)	
training:	Epoch: [10][329/817]	Loss 0.0182 (0.1011)	
training:	Epoch: [10][330/817]	Loss 0.0216 (0.1008)	
training:	Epoch: [10][331/817]	Loss 0.0317 (0.1006)	
training:	Epoch: [10][332/817]	Loss 0.0215 (0.1004)	
training:	Epoch: [10][333/817]	Loss 0.0206 (0.1001)	
training:	Epoch: [10][334/817]	Loss 0.5547 (0.1015)	
training:	Epoch: [10][335/817]	Loss 0.0262 (0.1013)	
training:	Epoch: [10][336/817]	Loss 0.0226 (0.1010)	
training:	Epoch: [10][337/817]	Loss 0.5063 (0.1022)	
training:	Epoch: [10][338/817]	Loss 0.0250 (0.1020)	
training:	Epoch: [10][339/817]	Loss 0.0236 (0.1018)	
training:	Epoch: [10][340/817]	Loss 0.0228 (0.1016)	
training:	Epoch: [10][341/817]	Loss 0.0552 (0.1014)	
training:	Epoch: [10][342/817]	Loss 0.0181 (0.1012)	
training:	Epoch: [10][343/817]	Loss 0.0243 (0.1010)	
training:	Epoch: [10][344/817]	Loss 0.0227 (0.1007)	
training:	Epoch: [10][345/817]	Loss 0.1202 (0.1008)	
training:	Epoch: [10][346/817]	Loss 0.0495 (0.1006)	
training:	Epoch: [10][347/817]	Loss 0.0232 (0.1004)	
training:	Epoch: [10][348/817]	Loss 0.0697 (0.1003)	
training:	Epoch: [10][349/817]	Loss 0.0266 (0.1001)	
training:	Epoch: [10][350/817]	Loss 0.0302 (0.0999)	
training:	Epoch: [10][351/817]	Loss 0.0242 (0.0997)	
training:	Epoch: [10][352/817]	Loss 0.4941 (0.1008)	
training:	Epoch: [10][353/817]	Loss 0.0185 (0.1006)	
training:	Epoch: [10][354/817]	Loss 0.4378 (0.1015)	
training:	Epoch: [10][355/817]	Loss 0.0182 (0.1013)	
training:	Epoch: [10][356/817]	Loss 0.0473 (0.1011)	
training:	Epoch: [10][357/817]	Loss 0.0217 (0.1009)	
training:	Epoch: [10][358/817]	Loss 0.5528 (0.1022)	
training:	Epoch: [10][359/817]	Loss 0.0270 (0.1020)	
training:	Epoch: [10][360/817]	Loss 0.0214 (0.1018)	
training:	Epoch: [10][361/817]	Loss 0.0419 (0.1016)	
training:	Epoch: [10][362/817]	Loss 0.5543 (0.1028)	
training:	Epoch: [10][363/817]	Loss 0.0669 (0.1027)	
training:	Epoch: [10][364/817]	Loss 0.0819 (0.1027)	
training:	Epoch: [10][365/817]	Loss 0.0261 (0.1025)	
training:	Epoch: [10][366/817]	Loss 0.0257 (0.1023)	
training:	Epoch: [10][367/817]	Loss 0.0876 (0.1022)	
training:	Epoch: [10][368/817]	Loss 0.0756 (0.1022)	
training:	Epoch: [10][369/817]	Loss 0.0563 (0.1020)	
training:	Epoch: [10][370/817]	Loss 0.0237 (0.1018)	
training:	Epoch: [10][371/817]	Loss 0.0280 (0.1016)	
training:	Epoch: [10][372/817]	Loss 0.0353 (0.1014)	
training:	Epoch: [10][373/817]	Loss 0.0327 (0.1013)	
training:	Epoch: [10][374/817]	Loss 0.0231 (0.1010)	
training:	Epoch: [10][375/817]	Loss 0.0590 (0.1009)	
training:	Epoch: [10][376/817]	Loss 0.0184 (0.1007)	
training:	Epoch: [10][377/817]	Loss 0.0264 (0.1005)	
training:	Epoch: [10][378/817]	Loss 0.0224 (0.1003)	
training:	Epoch: [10][379/817]	Loss 0.0212 (0.1001)	
training:	Epoch: [10][380/817]	Loss 0.0200 (0.0999)	
training:	Epoch: [10][381/817]	Loss 0.4468 (0.1008)	
training:	Epoch: [10][382/817]	Loss 0.0355 (0.1006)	
training:	Epoch: [10][383/817]	Loss 0.0214 (0.1004)	
training:	Epoch: [10][384/817]	Loss 0.0221 (0.1002)	
training:	Epoch: [10][385/817]	Loss 0.0268 (0.1000)	
training:	Epoch: [10][386/817]	Loss 0.0253 (0.0998)	
training:	Epoch: [10][387/817]	Loss 0.0218 (0.0996)	
training:	Epoch: [10][388/817]	Loss 0.0234 (0.0994)	
training:	Epoch: [10][389/817]	Loss 0.0231 (0.0992)	
training:	Epoch: [10][390/817]	Loss 0.0184 (0.0990)	
training:	Epoch: [10][391/817]	Loss 0.0178 (0.0988)	
training:	Epoch: [10][392/817]	Loss 0.0310 (0.0987)	
training:	Epoch: [10][393/817]	Loss 0.0221 (0.0985)	
training:	Epoch: [10][394/817]	Loss 0.0188 (0.0983)	
training:	Epoch: [10][395/817]	Loss 0.0410 (0.0981)	
training:	Epoch: [10][396/817]	Loss 0.0474 (0.0980)	
training:	Epoch: [10][397/817]	Loss 0.4885 (0.0990)	
training:	Epoch: [10][398/817]	Loss 0.0224 (0.0988)	
training:	Epoch: [10][399/817]	Loss 0.0243 (0.0986)	
training:	Epoch: [10][400/817]	Loss 0.0384 (0.0984)	
training:	Epoch: [10][401/817]	Loss 0.0224 (0.0982)	
training:	Epoch: [10][402/817]	Loss 0.0350 (0.0981)	
training:	Epoch: [10][403/817]	Loss 0.1252 (0.0982)	
training:	Epoch: [10][404/817]	Loss 0.0264 (0.0980)	
training:	Epoch: [10][405/817]	Loss 0.0249 (0.0978)	
training:	Epoch: [10][406/817]	Loss 0.0205 (0.0976)	
training:	Epoch: [10][407/817]	Loss 0.1296 (0.0977)	
training:	Epoch: [10][408/817]	Loss 0.0248 (0.0975)	
training:	Epoch: [10][409/817]	Loss 0.1624 (0.0977)	
training:	Epoch: [10][410/817]	Loss 0.1931 (0.0979)	
training:	Epoch: [10][411/817]	Loss 0.0208 (0.0977)	
training:	Epoch: [10][412/817]	Loss 0.5074 (0.0987)	
training:	Epoch: [10][413/817]	Loss 0.4608 (0.0996)	
training:	Epoch: [10][414/817]	Loss 0.0844 (0.0995)	
training:	Epoch: [10][415/817]	Loss 0.0201 (0.0994)	
training:	Epoch: [10][416/817]	Loss 0.0197 (0.0992)	
training:	Epoch: [10][417/817]	Loss 0.0239 (0.0990)	
training:	Epoch: [10][418/817]	Loss 0.0211 (0.0988)	
training:	Epoch: [10][419/817]	Loss 0.0453 (0.0987)	
training:	Epoch: [10][420/817]	Loss 0.0198 (0.0985)	
training:	Epoch: [10][421/817]	Loss 0.1458 (0.0986)	
training:	Epoch: [10][422/817]	Loss 0.5611 (0.0997)	
training:	Epoch: [10][423/817]	Loss 0.5461 (0.1007)	
training:	Epoch: [10][424/817]	Loss 0.0232 (0.1006)	
training:	Epoch: [10][425/817]	Loss 0.0372 (0.1004)	
training:	Epoch: [10][426/817]	Loss 0.0226 (0.1002)	
training:	Epoch: [10][427/817]	Loss 0.0213 (0.1000)	
training:	Epoch: [10][428/817]	Loss 0.0190 (0.0999)	
training:	Epoch: [10][429/817]	Loss 0.6955 (0.1012)	
training:	Epoch: [10][430/817]	Loss 0.0225 (0.1011)	
training:	Epoch: [10][431/817]	Loss 0.0401 (0.1009)	
training:	Epoch: [10][432/817]	Loss 0.0286 (0.1008)	
training:	Epoch: [10][433/817]	Loss 0.0251 (0.1006)	
training:	Epoch: [10][434/817]	Loss 0.4623 (0.1014)	
training:	Epoch: [10][435/817]	Loss 0.0199 (0.1012)	
training:	Epoch: [10][436/817]	Loss 0.0775 (0.1012)	
training:	Epoch: [10][437/817]	Loss 0.0174 (0.1010)	
training:	Epoch: [10][438/817]	Loss 0.3544 (0.1016)	
training:	Epoch: [10][439/817]	Loss 0.0293 (0.1014)	
training:	Epoch: [10][440/817]	Loss 0.0504 (0.1013)	
training:	Epoch: [10][441/817]	Loss 0.0551 (0.1012)	
training:	Epoch: [10][442/817]	Loss 0.0212 (0.1010)	
training:	Epoch: [10][443/817]	Loss 0.0850 (0.1010)	
training:	Epoch: [10][444/817]	Loss 0.0276 (0.1008)	
training:	Epoch: [10][445/817]	Loss 0.1064 (0.1008)	
training:	Epoch: [10][446/817]	Loss 0.0313 (0.1006)	
training:	Epoch: [10][447/817]	Loss 0.0394 (0.1005)	
training:	Epoch: [10][448/817]	Loss 0.0191 (0.1003)	
training:	Epoch: [10][449/817]	Loss 0.0398 (0.1002)	
training:	Epoch: [10][450/817]	Loss 0.2270 (0.1005)	
training:	Epoch: [10][451/817]	Loss 0.0325 (0.1003)	
training:	Epoch: [10][452/817]	Loss 0.0396 (0.1002)	
training:	Epoch: [10][453/817]	Loss 0.5324 (0.1011)	
training:	Epoch: [10][454/817]	Loss 0.5075 (0.1020)	
training:	Epoch: [10][455/817]	Loss 0.0216 (0.1019)	
training:	Epoch: [10][456/817]	Loss 0.0264 (0.1017)	
training:	Epoch: [10][457/817]	Loss 0.0436 (0.1016)	
training:	Epoch: [10][458/817]	Loss 0.0787 (0.1015)	
training:	Epoch: [10][459/817]	Loss 0.0380 (0.1014)	
training:	Epoch: [10][460/817]	Loss 0.0206 (0.1012)	
training:	Epoch: [10][461/817]	Loss 0.0208 (0.1010)	
training:	Epoch: [10][462/817]	Loss 0.0508 (0.1009)	
training:	Epoch: [10][463/817]	Loss 0.0833 (0.1009)	
training:	Epoch: [10][464/817]	Loss 0.0502 (0.1008)	
training:	Epoch: [10][465/817]	Loss 0.0201 (0.1006)	
training:	Epoch: [10][466/817]	Loss 0.0176 (0.1004)	
training:	Epoch: [10][467/817]	Loss 0.0310 (0.1003)	
training:	Epoch: [10][468/817]	Loss 0.5428 (0.1012)	
training:	Epoch: [10][469/817]	Loss 0.0175 (0.1010)	
training:	Epoch: [10][470/817]	Loss 0.0246 (0.1009)	
training:	Epoch: [10][471/817]	Loss 0.0203 (0.1007)	
training:	Epoch: [10][472/817]	Loss 0.0205 (0.1005)	
training:	Epoch: [10][473/817]	Loss 0.0381 (0.1004)	
training:	Epoch: [10][474/817]	Loss 0.0399 (0.1003)	
training:	Epoch: [10][475/817]	Loss 0.0412 (0.1002)	
training:	Epoch: [10][476/817]	Loss 0.5618 (0.1011)	
training:	Epoch: [10][477/817]	Loss 0.0221 (0.1010)	
training:	Epoch: [10][478/817]	Loss 0.0814 (0.1009)	
training:	Epoch: [10][479/817]	Loss 0.4950 (0.1017)	
training:	Epoch: [10][480/817]	Loss 0.0876 (0.1017)	
training:	Epoch: [10][481/817]	Loss 0.0264 (0.1016)	
training:	Epoch: [10][482/817]	Loss 0.0224 (0.1014)	
training:	Epoch: [10][483/817]	Loss 0.0203 (0.1012)	
training:	Epoch: [10][484/817]	Loss 0.0182 (0.1010)	
training:	Epoch: [10][485/817]	Loss 0.4914 (0.1019)	
training:	Epoch: [10][486/817]	Loss 0.0356 (0.1017)	
training:	Epoch: [10][487/817]	Loss 0.0228 (0.1016)	
training:	Epoch: [10][488/817]	Loss 0.0748 (0.1015)	
training:	Epoch: [10][489/817]	Loss 0.0228 (0.1013)	
training:	Epoch: [10][490/817]	Loss 0.0227 (0.1012)	
training:	Epoch: [10][491/817]	Loss 0.0332 (0.1010)	
training:	Epoch: [10][492/817]	Loss 0.0193 (0.1009)	
training:	Epoch: [10][493/817]	Loss 0.0196 (0.1007)	
training:	Epoch: [10][494/817]	Loss 0.0461 (0.1006)	
training:	Epoch: [10][495/817]	Loss 0.0247 (0.1004)	
training:	Epoch: [10][496/817]	Loss 0.0247 (0.1003)	
training:	Epoch: [10][497/817]	Loss 0.0984 (0.1003)	
training:	Epoch: [10][498/817]	Loss 0.4451 (0.1010)	
training:	Epoch: [10][499/817]	Loss 0.0474 (0.1009)	
training:	Epoch: [10][500/817]	Loss 0.5637 (0.1018)	
training:	Epoch: [10][501/817]	Loss 0.0240 (0.1016)	
training:	Epoch: [10][502/817]	Loss 0.0426 (0.1015)	
training:	Epoch: [10][503/817]	Loss 0.0171 (0.1014)	
training:	Epoch: [10][504/817]	Loss 0.0366 (0.1012)	
training:	Epoch: [10][505/817]	Loss 0.3753 (0.1018)	
training:	Epoch: [10][506/817]	Loss 0.0226 (0.1016)	
training:	Epoch: [10][507/817]	Loss 0.0196 (0.1015)	
training:	Epoch: [10][508/817]	Loss 0.0215 (0.1013)	
training:	Epoch: [10][509/817]	Loss 0.5093 (0.1021)	
training:	Epoch: [10][510/817]	Loss 0.0270 (0.1020)	
training:	Epoch: [10][511/817]	Loss 0.0253 (0.1018)	
training:	Epoch: [10][512/817]	Loss 0.0213 (0.1016)	
training:	Epoch: [10][513/817]	Loss 0.0923 (0.1016)	
training:	Epoch: [10][514/817]	Loss 0.0239 (0.1015)	
training:	Epoch: [10][515/817]	Loss 0.5552 (0.1024)	
training:	Epoch: [10][516/817]	Loss 0.0210 (0.1022)	
training:	Epoch: [10][517/817]	Loss 0.0188 (0.1020)	
training:	Epoch: [10][518/817]	Loss 0.1816 (0.1022)	
training:	Epoch: [10][519/817]	Loss 0.0235 (0.1020)	
training:	Epoch: [10][520/817]	Loss 0.0347 (0.1019)	
training:	Epoch: [10][521/817]	Loss 0.0180 (0.1018)	
training:	Epoch: [10][522/817]	Loss 0.0220 (0.1016)	
training:	Epoch: [10][523/817]	Loss 0.0281 (0.1015)	
training:	Epoch: [10][524/817]	Loss 0.0166 (0.1013)	
training:	Epoch: [10][525/817]	Loss 0.2809 (0.1016)	
training:	Epoch: [10][526/817]	Loss 0.2329 (0.1019)	
training:	Epoch: [10][527/817]	Loss 0.0196 (0.1017)	
training:	Epoch: [10][528/817]	Loss 0.0209 (0.1016)	
training:	Epoch: [10][529/817]	Loss 0.2444 (0.1018)	
training:	Epoch: [10][530/817]	Loss 0.0531 (0.1018)	
training:	Epoch: [10][531/817]	Loss 0.0325 (0.1016)	
training:	Epoch: [10][532/817]	Loss 0.3329 (0.1021)	
training:	Epoch: [10][533/817]	Loss 0.0221 (0.1019)	
training:	Epoch: [10][534/817]	Loss 0.2631 (0.1022)	
training:	Epoch: [10][535/817]	Loss 0.0209 (0.1021)	
training:	Epoch: [10][536/817]	Loss 0.0325 (0.1019)	
training:	Epoch: [10][537/817]	Loss 0.3020 (0.1023)	
training:	Epoch: [10][538/817]	Loss 0.2650 (0.1026)	
training:	Epoch: [10][539/817]	Loss 0.0187 (0.1024)	
training:	Epoch: [10][540/817]	Loss 0.2188 (0.1027)	
training:	Epoch: [10][541/817]	Loss 0.0208 (0.1025)	
training:	Epoch: [10][542/817]	Loss 0.4672 (0.1032)	
training:	Epoch: [10][543/817]	Loss 0.0256 (0.1030)	
training:	Epoch: [10][544/817]	Loss 0.0324 (0.1029)	
training:	Epoch: [10][545/817]	Loss 0.0198 (0.1028)	
training:	Epoch: [10][546/817]	Loss 0.0254 (0.1026)	
training:	Epoch: [10][547/817]	Loss 0.0542 (0.1025)	
training:	Epoch: [10][548/817]	Loss 0.0511 (0.1024)	
training:	Epoch: [10][549/817]	Loss 0.1874 (0.1026)	
training:	Epoch: [10][550/817]	Loss 0.5469 (0.1034)	
training:	Epoch: [10][551/817]	Loss 0.0806 (0.1034)	
training:	Epoch: [10][552/817]	Loss 0.0237 (0.1032)	
training:	Epoch: [10][553/817]	Loss 0.0206 (0.1031)	
training:	Epoch: [10][554/817]	Loss 0.0305 (0.1029)	
training:	Epoch: [10][555/817]	Loss 0.0507 (0.1028)	
training:	Epoch: [10][556/817]	Loss 0.1871 (0.1030)	
training:	Epoch: [10][557/817]	Loss 0.0412 (0.1029)	
training:	Epoch: [10][558/817]	Loss 0.0928 (0.1029)	
training:	Epoch: [10][559/817]	Loss 0.0197 (0.1027)	
training:	Epoch: [10][560/817]	Loss 0.0649 (0.1026)	
training:	Epoch: [10][561/817]	Loss 0.1045 (0.1026)	
training:	Epoch: [10][562/817]	Loss 0.0187 (0.1025)	
training:	Epoch: [10][563/817]	Loss 0.3184 (0.1029)	
training:	Epoch: [10][564/817]	Loss 0.0329 (0.1028)	
training:	Epoch: [10][565/817]	Loss 0.3013 (0.1031)	
training:	Epoch: [10][566/817]	Loss 0.2730 (0.1034)	
training:	Epoch: [10][567/817]	Loss 0.0208 (0.1033)	
training:	Epoch: [10][568/817]	Loss 0.0182 (0.1031)	
training:	Epoch: [10][569/817]	Loss 0.4940 (0.1038)	
training:	Epoch: [10][570/817]	Loss 0.0214 (0.1037)	
training:	Epoch: [10][571/817]	Loss 0.0291 (0.1035)	
training:	Epoch: [10][572/817]	Loss 0.0197 (0.1034)	
training:	Epoch: [10][573/817]	Loss 0.0274 (0.1032)	
training:	Epoch: [10][574/817]	Loss 0.0827 (0.1032)	
training:	Epoch: [10][575/817]	Loss 0.0376 (0.1031)	
training:	Epoch: [10][576/817]	Loss 0.0226 (0.1030)	
training:	Epoch: [10][577/817]	Loss 0.0231 (0.1028)	
training:	Epoch: [10][578/817]	Loss 0.0942 (0.1028)	
training:	Epoch: [10][579/817]	Loss 0.0226 (0.1027)	
training:	Epoch: [10][580/817]	Loss 0.0231 (0.1025)	
training:	Epoch: [10][581/817]	Loss 0.0339 (0.1024)	
training:	Epoch: [10][582/817]	Loss 0.0477 (0.1023)	
training:	Epoch: [10][583/817]	Loss 0.2997 (0.1027)	
training:	Epoch: [10][584/817]	Loss 0.2590 (0.1029)	
training:	Epoch: [10][585/817]	Loss 0.0215 (0.1028)	
training:	Epoch: [10][586/817]	Loss 0.0186 (0.1026)	
training:	Epoch: [10][587/817]	Loss 0.0224 (0.1025)	
training:	Epoch: [10][588/817]	Loss 0.0337 (0.1024)	
training:	Epoch: [10][589/817]	Loss 0.3788 (0.1029)	
training:	Epoch: [10][590/817]	Loss 0.0533 (0.1028)	
training:	Epoch: [10][591/817]	Loss 0.0229 (0.1026)	
training:	Epoch: [10][592/817]	Loss 0.0586 (0.1026)	
training:	Epoch: [10][593/817]	Loss 0.0212 (0.1024)	
training:	Epoch: [10][594/817]	Loss 0.0178 (0.1023)	
training:	Epoch: [10][595/817]	Loss 0.0648 (0.1022)	
training:	Epoch: [10][596/817]	Loss 0.4962 (0.1029)	
training:	Epoch: [10][597/817]	Loss 0.1656 (0.1030)	
training:	Epoch: [10][598/817]	Loss 0.0343 (0.1029)	
training:	Epoch: [10][599/817]	Loss 0.0256 (0.1027)	
training:	Epoch: [10][600/817]	Loss 0.0338 (0.1026)	
training:	Epoch: [10][601/817]	Loss 0.1721 (0.1027)	
training:	Epoch: [10][602/817]	Loss 0.4015 (0.1032)	
training:	Epoch: [10][603/817]	Loss 0.0219 (0.1031)	
training:	Epoch: [10][604/817]	Loss 0.0371 (0.1030)	
training:	Epoch: [10][605/817]	Loss 0.0388 (0.1029)	
training:	Epoch: [10][606/817]	Loss 0.4479 (0.1035)	
training:	Epoch: [10][607/817]	Loss 0.0199 (0.1033)	
training:	Epoch: [10][608/817]	Loss 0.2185 (0.1035)	
training:	Epoch: [10][609/817]	Loss 0.1368 (0.1036)	
training:	Epoch: [10][610/817]	Loss 0.4300 (0.1041)	
training:	Epoch: [10][611/817]	Loss 0.3554 (0.1045)	
training:	Epoch: [10][612/817]	Loss 0.0792 (0.1045)	
training:	Epoch: [10][613/817]	Loss 0.1715 (0.1046)	
training:	Epoch: [10][614/817]	Loss 0.1868 (0.1047)	
training:	Epoch: [10][615/817]	Loss 0.0182 (0.1046)	
training:	Epoch: [10][616/817]	Loss 0.0996 (0.1046)	
training:	Epoch: [10][617/817]	Loss 0.0559 (0.1045)	
training:	Epoch: [10][618/817]	Loss 0.1781 (0.1046)	
training:	Epoch: [10][619/817]	Loss 0.0762 (0.1046)	
training:	Epoch: [10][620/817]	Loss 0.6076 (0.1054)	
training:	Epoch: [10][621/817]	Loss 0.1401 (0.1054)	
training:	Epoch: [10][622/817]	Loss 0.0278 (0.1053)	
training:	Epoch: [10][623/817]	Loss 0.0254 (0.1052)	
training:	Epoch: [10][624/817]	Loss 0.0491 (0.1051)	
training:	Epoch: [10][625/817]	Loss 0.0420 (0.1050)	
training:	Epoch: [10][626/817]	Loss 0.0253 (0.1049)	
training:	Epoch: [10][627/817]	Loss 0.0271 (0.1047)	
training:	Epoch: [10][628/817]	Loss 0.0308 (0.1046)	
training:	Epoch: [10][629/817]	Loss 0.0244 (0.1045)	
training:	Epoch: [10][630/817]	Loss 0.0226 (0.1044)	
training:	Epoch: [10][631/817]	Loss 0.0282 (0.1042)	
training:	Epoch: [10][632/817]	Loss 0.0594 (0.1042)	
training:	Epoch: [10][633/817]	Loss 0.0445 (0.1041)	
training:	Epoch: [10][634/817]	Loss 0.0247 (0.1039)	
training:	Epoch: [10][635/817]	Loss 0.0389 (0.1038)	
training:	Epoch: [10][636/817]	Loss 0.0195 (0.1037)	
training:	Epoch: [10][637/817]	Loss 0.0582 (0.1036)	
training:	Epoch: [10][638/817]	Loss 0.1176 (0.1037)	
training:	Epoch: [10][639/817]	Loss 0.1146 (0.1037)	
training:	Epoch: [10][640/817]	Loss 0.2035 (0.1038)	
training:	Epoch: [10][641/817]	Loss 0.4886 (0.1044)	
training:	Epoch: [10][642/817]	Loss 0.0224 (0.1043)	
training:	Epoch: [10][643/817]	Loss 0.1163 (0.1043)	
training:	Epoch: [10][644/817]	Loss 0.0362 (0.1042)	
training:	Epoch: [10][645/817]	Loss 0.0423 (0.1041)	
training:	Epoch: [10][646/817]	Loss 0.1250 (0.1042)	
training:	Epoch: [10][647/817]	Loss 0.0633 (0.1041)	
training:	Epoch: [10][648/817]	Loss 0.0362 (0.1040)	
training:	Epoch: [10][649/817]	Loss 0.0850 (0.1040)	
training:	Epoch: [10][650/817]	Loss 0.0157 (0.1038)	
training:	Epoch: [10][651/817]	Loss 0.1354 (0.1039)	
training:	Epoch: [10][652/817]	Loss 0.0183 (0.1037)	
training:	Epoch: [10][653/817]	Loss 0.0252 (0.1036)	
training:	Epoch: [10][654/817]	Loss 0.0667 (0.1036)	
training:	Epoch: [10][655/817]	Loss 0.6367 (0.1044)	
training:	Epoch: [10][656/817]	Loss 0.0264 (0.1043)	
training:	Epoch: [10][657/817]	Loss 0.0202 (0.1041)	
training:	Epoch: [10][658/817]	Loss 0.0240 (0.1040)	
training:	Epoch: [10][659/817]	Loss 0.0416 (0.1039)	
training:	Epoch: [10][660/817]	Loss 0.0479 (0.1038)	
training:	Epoch: [10][661/817]	Loss 0.0434 (0.1037)	
training:	Epoch: [10][662/817]	Loss 0.4005 (0.1042)	
training:	Epoch: [10][663/817]	Loss 0.0288 (0.1041)	
training:	Epoch: [10][664/817]	Loss 0.0218 (0.1039)	
training:	Epoch: [10][665/817]	Loss 0.0612 (0.1039)	
training:	Epoch: [10][666/817]	Loss 0.0191 (0.1038)	
training:	Epoch: [10][667/817]	Loss 0.0221 (0.1036)	
training:	Epoch: [10][668/817]	Loss 0.0419 (0.1035)	
training:	Epoch: [10][669/817]	Loss 0.3574 (0.1039)	
training:	Epoch: [10][670/817]	Loss 0.0226 (0.1038)	
training:	Epoch: [10][671/817]	Loss 0.5480 (0.1045)	
training:	Epoch: [10][672/817]	Loss 0.2590 (0.1047)	
training:	Epoch: [10][673/817]	Loss 0.0216 (0.1046)	
training:	Epoch: [10][674/817]	Loss 0.0214 (0.1044)	
training:	Epoch: [10][675/817]	Loss 0.0190 (0.1043)	
training:	Epoch: [10][676/817]	Loss 0.0492 (0.1042)	
training:	Epoch: [10][677/817]	Loss 0.0301 (0.1041)	
training:	Epoch: [10][678/817]	Loss 0.6032 (0.1049)	
training:	Epoch: [10][679/817]	Loss 0.0440 (0.1048)	
training:	Epoch: [10][680/817]	Loss 0.0258 (0.1047)	
training:	Epoch: [10][681/817]	Loss 0.0216 (0.1045)	
training:	Epoch: [10][682/817]	Loss 0.0154 (0.1044)	
training:	Epoch: [10][683/817]	Loss 0.1069 (0.1044)	
training:	Epoch: [10][684/817]	Loss 0.0199 (0.1043)	
training:	Epoch: [10][685/817]	Loss 0.0161 (0.1042)	
training:	Epoch: [10][686/817]	Loss 0.0174 (0.1040)	
training:	Epoch: [10][687/817]	Loss 0.0347 (0.1039)	
training:	Epoch: [10][688/817]	Loss 0.0307 (0.1038)	
training:	Epoch: [10][689/817]	Loss 0.0506 (0.1037)	
training:	Epoch: [10][690/817]	Loss 0.0281 (0.1036)	
training:	Epoch: [10][691/817]	Loss 0.0191 (0.1035)	
training:	Epoch: [10][692/817]	Loss 0.2686 (0.1038)	
training:	Epoch: [10][693/817]	Loss 0.5426 (0.1044)	
training:	Epoch: [10][694/817]	Loss 0.0448 (0.1043)	
training:	Epoch: [10][695/817]	Loss 0.0176 (0.1042)	
training:	Epoch: [10][696/817]	Loss 0.0315 (0.1041)	
training:	Epoch: [10][697/817]	Loss 0.4713 (0.1046)	
training:	Epoch: [10][698/817]	Loss 0.0164 (0.1045)	
training:	Epoch: [10][699/817]	Loss 0.1724 (0.1046)	
training:	Epoch: [10][700/817]	Loss 0.0209 (0.1044)	
training:	Epoch: [10][701/817]	Loss 0.1421 (0.1045)	
training:	Epoch: [10][702/817]	Loss 0.0331 (0.1044)	
training:	Epoch: [10][703/817]	Loss 0.1274 (0.1044)	
training:	Epoch: [10][704/817]	Loss 0.5511 (0.1051)	
training:	Epoch: [10][705/817]	Loss 0.0196 (0.1049)	
training:	Epoch: [10][706/817]	Loss 0.0249 (0.1048)	
training:	Epoch: [10][707/817]	Loss 0.0242 (0.1047)	
training:	Epoch: [10][708/817]	Loss 0.5606 (0.1054)	
training:	Epoch: [10][709/817]	Loss 0.0191 (0.1052)	
training:	Epoch: [10][710/817]	Loss 0.0193 (0.1051)	
training:	Epoch: [10][711/817]	Loss 0.0207 (0.1050)	
training:	Epoch: [10][712/817]	Loss 0.0198 (0.1049)	
training:	Epoch: [10][713/817]	Loss 0.0173 (0.1048)	
training:	Epoch: [10][714/817]	Loss 0.0253 (0.1046)	
training:	Epoch: [10][715/817]	Loss 0.0267 (0.1045)	
training:	Epoch: [10][716/817]	Loss 0.0203 (0.1044)	
training:	Epoch: [10][717/817]	Loss 0.0843 (0.1044)	
training:	Epoch: [10][718/817]	Loss 0.0186 (0.1043)	
training:	Epoch: [10][719/817]	Loss 0.5610 (0.1049)	
training:	Epoch: [10][720/817]	Loss 0.0187 (0.1048)	
training:	Epoch: [10][721/817]	Loss 0.3300 (0.1051)	
training:	Epoch: [10][722/817]	Loss 0.0233 (0.1050)	
training:	Epoch: [10][723/817]	Loss 0.0269 (0.1049)	
training:	Epoch: [10][724/817]	Loss 0.0191 (0.1048)	
training:	Epoch: [10][725/817]	Loss 0.0200 (0.1046)	
training:	Epoch: [10][726/817]	Loss 0.0206 (0.1045)	
training:	Epoch: [10][727/817]	Loss 0.0257 (0.1044)	
training:	Epoch: [10][728/817]	Loss 0.0325 (0.1043)	
training:	Epoch: [10][729/817]	Loss 0.0201 (0.1042)	
training:	Epoch: [10][730/817]	Loss 0.0230 (0.1041)	
training:	Epoch: [10][731/817]	Loss 0.0693 (0.1040)	
training:	Epoch: [10][732/817]	Loss 0.0239 (0.1039)	
training:	Epoch: [10][733/817]	Loss 0.0252 (0.1038)	
training:	Epoch: [10][734/817]	Loss 0.0278 (0.1037)	
training:	Epoch: [10][735/817]	Loss 0.0168 (0.1036)	
training:	Epoch: [10][736/817]	Loss 0.0525 (0.1035)	
training:	Epoch: [10][737/817]	Loss 0.0288 (0.1034)	
training:	Epoch: [10][738/817]	Loss 0.1308 (0.1035)	
training:	Epoch: [10][739/817]	Loss 0.4528 (0.1039)	
training:	Epoch: [10][740/817]	Loss 0.0297 (0.1038)	
training:	Epoch: [10][741/817]	Loss 0.0240 (0.1037)	
training:	Epoch: [10][742/817]	Loss 0.2711 (0.1040)	
training:	Epoch: [10][743/817]	Loss 0.0194 (0.1038)	
training:	Epoch: [10][744/817]	Loss 0.0305 (0.1038)	
training:	Epoch: [10][745/817]	Loss 0.0215 (0.1036)	
training:	Epoch: [10][746/817]	Loss 0.1252 (0.1037)	
training:	Epoch: [10][747/817]	Loss 0.0270 (0.1036)	
training:	Epoch: [10][748/817]	Loss 0.5305 (0.1041)	
training:	Epoch: [10][749/817]	Loss 0.0193 (0.1040)	
training:	Epoch: [10][750/817]	Loss 0.0446 (0.1039)	
training:	Epoch: [10][751/817]	Loss 0.0897 (0.1039)	
training:	Epoch: [10][752/817]	Loss 0.1333 (0.1040)	
training:	Epoch: [10][753/817]	Loss 0.0179 (0.1039)	
training:	Epoch: [10][754/817]	Loss 0.0372 (0.1038)	
training:	Epoch: [10][755/817]	Loss 0.0373 (0.1037)	
training:	Epoch: [10][756/817]	Loss 0.0456 (0.1036)	
training:	Epoch: [10][757/817]	Loss 0.0224 (0.1035)	
training:	Epoch: [10][758/817]	Loss 0.1957 (0.1036)	
training:	Epoch: [10][759/817]	Loss 0.4356 (0.1040)	
training:	Epoch: [10][760/817]	Loss 0.0272 (0.1039)	
training:	Epoch: [10][761/817]	Loss 0.0197 (0.1038)	
training:	Epoch: [10][762/817]	Loss 0.0209 (0.1037)	
training:	Epoch: [10][763/817]	Loss 0.0700 (0.1037)	
training:	Epoch: [10][764/817]	Loss 0.0179 (0.1036)	
training:	Epoch: [10][765/817]	Loss 0.5834 (0.1042)	
training:	Epoch: [10][766/817]	Loss 0.0283 (0.1041)	
training:	Epoch: [10][767/817]	Loss 0.2256 (0.1043)	
training:	Epoch: [10][768/817]	Loss 0.1218 (0.1043)	
training:	Epoch: [10][769/817]	Loss 0.0616 (0.1042)	
training:	Epoch: [10][770/817]	Loss 0.0284 (0.1041)	
training:	Epoch: [10][771/817]	Loss 0.6076 (0.1048)	
training:	Epoch: [10][772/817]	Loss 0.0322 (0.1047)	
training:	Epoch: [10][773/817]	Loss 0.0314 (0.1046)	
training:	Epoch: [10][774/817]	Loss 0.0223 (0.1045)	
training:	Epoch: [10][775/817]	Loss 0.0200 (0.1044)	
training:	Epoch: [10][776/817]	Loss 0.0201 (0.1043)	
training:	Epoch: [10][777/817]	Loss 0.0182 (0.1042)	
training:	Epoch: [10][778/817]	Loss 0.0243 (0.1041)	
training:	Epoch: [10][779/817]	Loss 0.0300 (0.1040)	
training:	Epoch: [10][780/817]	Loss 0.0251 (0.1039)	
training:	Epoch: [10][781/817]	Loss 0.0889 (0.1038)	
training:	Epoch: [10][782/817]	Loss 0.0956 (0.1038)	
training:	Epoch: [10][783/817]	Loss 0.0956 (0.1038)	
training:	Epoch: [10][784/817]	Loss 0.5459 (0.1044)	
training:	Epoch: [10][785/817]	Loss 0.0178 (0.1043)	
training:	Epoch: [10][786/817]	Loss 0.0648 (0.1042)	
training:	Epoch: [10][787/817]	Loss 0.0205 (0.1041)	
training:	Epoch: [10][788/817]	Loss 0.0324 (0.1040)	
training:	Epoch: [10][789/817]	Loss 0.0283 (0.1039)	
training:	Epoch: [10][790/817]	Loss 0.0261 (0.1038)	
training:	Epoch: [10][791/817]	Loss 0.0360 (0.1037)	
training:	Epoch: [10][792/817]	Loss 0.0521 (0.1037)	
training:	Epoch: [10][793/817]	Loss 0.0317 (0.1036)	
training:	Epoch: [10][794/817]	Loss 0.0734 (0.1036)	
training:	Epoch: [10][795/817]	Loss 0.0225 (0.1034)	
training:	Epoch: [10][796/817]	Loss 0.0654 (0.1034)	
training:	Epoch: [10][797/817]	Loss 0.0208 (0.1033)	
training:	Epoch: [10][798/817]	Loss 0.0163 (0.1032)	
training:	Epoch: [10][799/817]	Loss 0.0201 (0.1031)	
training:	Epoch: [10][800/817]	Loss 0.0533 (0.1030)	
training:	Epoch: [10][801/817]	Loss 0.0194 (0.1029)	
training:	Epoch: [10][802/817]	Loss 0.0560 (0.1029)	
training:	Epoch: [10][803/817]	Loss 0.0268 (0.1028)	
training:	Epoch: [10][804/817]	Loss 0.0182 (0.1027)	
training:	Epoch: [10][805/817]	Loss 0.0177 (0.1026)	
training:	Epoch: [10][806/817]	Loss 0.0174 (0.1024)	
training:	Epoch: [10][807/817]	Loss 0.0328 (0.1024)	
training:	Epoch: [10][808/817]	Loss 0.5504 (0.1029)	
training:	Epoch: [10][809/817]	Loss 0.1550 (0.1030)	
training:	Epoch: [10][810/817]	Loss 0.0221 (0.1029)	
training:	Epoch: [10][811/817]	Loss 0.4695 (0.1033)	
training:	Epoch: [10][812/817]	Loss 0.0184 (0.1032)	
training:	Epoch: [10][813/817]	Loss 0.0185 (0.1031)	
training:	Epoch: [10][814/817]	Loss 0.0187 (0.1030)	
training:	Epoch: [10][815/817]	Loss 0.0332 (0.1029)	
training:	Epoch: [10][816/817]	Loss 0.0220 (0.1028)	
training:	Epoch: [10][817/817]	Loss 0.0195 (0.1027)	
Training:	 Loss: 0.1027

Training:	 ACC: 0.9847 0.9846 0.9821 0.9872
Validation:	 ACC: 0.7956 0.7972 0.8311 0.7601
Validation:	 Best_BACC: 0.8223 0.8218 0.8127 0.8318
Validation:	 Loss: 0.6895
Pretraining:	Epoch 11/200
----------
training:	Epoch: [11][1/817]	Loss 0.0171 (0.0171)	
training:	Epoch: [11][2/817]	Loss 0.0362 (0.0266)	
training:	Epoch: [11][3/817]	Loss 0.0184 (0.0239)	
training:	Epoch: [11][4/817]	Loss 0.0186 (0.0225)	
training:	Epoch: [11][5/817]	Loss 0.0389 (0.0258)	
training:	Epoch: [11][6/817]	Loss 0.5153 (0.1074)	
training:	Epoch: [11][7/817]	Loss 0.0206 (0.0950)	
training:	Epoch: [11][8/817]	Loss 0.0169 (0.0852)	
training:	Epoch: [11][9/817]	Loss 0.0204 (0.0780)	
training:	Epoch: [11][10/817]	Loss 0.0546 (0.0757)	
training:	Epoch: [11][11/817]	Loss 0.6714 (0.1298)	
training:	Epoch: [11][12/817]	Loss 0.0270 (0.1213)	
training:	Epoch: [11][13/817]	Loss 0.1714 (0.1251)	
training:	Epoch: [11][14/817]	Loss 0.0388 (0.1190)	
training:	Epoch: [11][15/817]	Loss 0.0262 (0.1128)	
training:	Epoch: [11][16/817]	Loss 0.0180 (0.1069)	
training:	Epoch: [11][17/817]	Loss 0.0215 (0.1018)	
training:	Epoch: [11][18/817]	Loss 0.0171 (0.0971)	
training:	Epoch: [11][19/817]	Loss 0.0526 (0.0948)	
training:	Epoch: [11][20/817]	Loss 0.0725 (0.0937)	
training:	Epoch: [11][21/817]	Loss 0.0208 (0.0902)	
training:	Epoch: [11][22/817]	Loss 0.0187 (0.0869)	
training:	Epoch: [11][23/817]	Loss 0.0172 (0.0839)	
training:	Epoch: [11][24/817]	Loss 0.0180 (0.0812)	
training:	Epoch: [11][25/817]	Loss 0.5445 (0.0997)	
training:	Epoch: [11][26/817]	Loss 0.0559 (0.0980)	
training:	Epoch: [11][27/817]	Loss 0.0196 (0.0951)	
training:	Epoch: [11][28/817]	Loss 0.5101 (0.1099)	
training:	Epoch: [11][29/817]	Loss 0.0196 (0.1068)	
training:	Epoch: [11][30/817]	Loss 0.0199 (0.1039)	
training:	Epoch: [11][31/817]	Loss 0.0141 (0.1010)	
training:	Epoch: [11][32/817]	Loss 0.0200 (0.0985)	
training:	Epoch: [11][33/817]	Loss 0.0339 (0.0965)	
training:	Epoch: [11][34/817]	Loss 0.0177 (0.0942)	
training:	Epoch: [11][35/817]	Loss 0.0211 (0.0921)	
training:	Epoch: [11][36/817]	Loss 0.0178 (0.0901)	
training:	Epoch: [11][37/817]	Loss 0.0187 (0.0881)	
training:	Epoch: [11][38/817]	Loss 0.0178 (0.0863)	
training:	Epoch: [11][39/817]	Loss 0.0200 (0.0846)	
training:	Epoch: [11][40/817]	Loss 0.0174 (0.0829)	
training:	Epoch: [11][41/817]	Loss 0.0230 (0.0814)	
training:	Epoch: [11][42/817]	Loss 0.4205 (0.0895)	
training:	Epoch: [11][43/817]	Loss 0.5127 (0.0994)	
training:	Epoch: [11][44/817]	Loss 0.4252 (0.1068)	
training:	Epoch: [11][45/817]	Loss 0.0291 (0.1050)	
training:	Epoch: [11][46/817]	Loss 0.2326 (0.1078)	
training:	Epoch: [11][47/817]	Loss 0.0204 (0.1059)	
training:	Epoch: [11][48/817]	Loss 0.0245 (0.1043)	
training:	Epoch: [11][49/817]	Loss 0.0229 (0.1026)	
training:	Epoch: [11][50/817]	Loss 0.0163 (0.1009)	
training:	Epoch: [11][51/817]	Loss 0.2336 (0.1035)	
training:	Epoch: [11][52/817]	Loss 0.0379 (0.1022)	
training:	Epoch: [11][53/817]	Loss 0.3529 (0.1069)	
training:	Epoch: [11][54/817]	Loss 0.0195 (0.1053)	
training:	Epoch: [11][55/817]	Loss 0.0174 (0.1037)	
training:	Epoch: [11][56/817]	Loss 0.5020 (0.1108)	
training:	Epoch: [11][57/817]	Loss 0.0237 (0.1093)	
training:	Epoch: [11][58/817]	Loss 0.1468 (0.1099)	
training:	Epoch: [11][59/817]	Loss 0.1031 (0.1098)	
training:	Epoch: [11][60/817]	Loss 0.0175 (0.1083)	
training:	Epoch: [11][61/817]	Loss 0.0212 (0.1069)	
training:	Epoch: [11][62/817]	Loss 0.0375 (0.1057)	
training:	Epoch: [11][63/817]	Loss 0.0170 (0.1043)	
training:	Epoch: [11][64/817]	Loss 0.0652 (0.1037)	
training:	Epoch: [11][65/817]	Loss 0.0197 (0.1024)	
training:	Epoch: [11][66/817]	Loss 0.0268 (0.1013)	
training:	Epoch: [11][67/817]	Loss 0.0184 (0.1001)	
training:	Epoch: [11][68/817]	Loss 0.5256 (0.1063)	
training:	Epoch: [11][69/817]	Loss 0.0160 (0.1050)	
training:	Epoch: [11][70/817]	Loss 0.0269 (0.1039)	
training:	Epoch: [11][71/817]	Loss 0.0163 (0.1027)	
training:	Epoch: [11][72/817]	Loss 0.0562 (0.1020)	
training:	Epoch: [11][73/817]	Loss 0.0274 (0.1010)	
training:	Epoch: [11][74/817]	Loss 0.0155 (0.0998)	
training:	Epoch: [11][75/817]	Loss 0.1123 (0.1000)	
training:	Epoch: [11][76/817]	Loss 0.0179 (0.0989)	
training:	Epoch: [11][77/817]	Loss 0.1935 (0.1001)	
training:	Epoch: [11][78/817]	Loss 0.0182 (0.0991)	
training:	Epoch: [11][79/817]	Loss 0.4775 (0.1039)	
training:	Epoch: [11][80/817]	Loss 0.0234 (0.1029)	
training:	Epoch: [11][81/817]	Loss 0.1611 (0.1036)	
training:	Epoch: [11][82/817]	Loss 0.0238 (0.1026)	
training:	Epoch: [11][83/817]	Loss 0.0168 (0.1016)	
training:	Epoch: [11][84/817]	Loss 0.0205 (0.1006)	
training:	Epoch: [11][85/817]	Loss 0.0233 (0.0997)	
training:	Epoch: [11][86/817]	Loss 0.2522 (0.1015)	
training:	Epoch: [11][87/817]	Loss 0.0233 (0.1006)	
training:	Epoch: [11][88/817]	Loss 0.5005 (0.1051)	
training:	Epoch: [11][89/817]	Loss 0.2533 (0.1068)	
training:	Epoch: [11][90/817]	Loss 0.0180 (0.1058)	
training:	Epoch: [11][91/817]	Loss 0.0160 (0.1048)	
training:	Epoch: [11][92/817]	Loss 0.0324 (0.1040)	
training:	Epoch: [11][93/817]	Loss 0.0154 (0.1031)	
training:	Epoch: [11][94/817]	Loss 0.0375 (0.1024)	
training:	Epoch: [11][95/817]	Loss 0.0177 (0.1015)	
training:	Epoch: [11][96/817]	Loss 0.0220 (0.1007)	
training:	Epoch: [11][97/817]	Loss 0.0200 (0.0998)	
training:	Epoch: [11][98/817]	Loss 0.0655 (0.0995)	
training:	Epoch: [11][99/817]	Loss 0.0187 (0.0987)	
training:	Epoch: [11][100/817]	Loss 0.0174 (0.0979)	
training:	Epoch: [11][101/817]	Loss 0.0154 (0.0970)	
training:	Epoch: [11][102/817]	Loss 0.1055 (0.0971)	
training:	Epoch: [11][103/817]	Loss 0.0203 (0.0964)	
training:	Epoch: [11][104/817]	Loss 0.0176 (0.0956)	
training:	Epoch: [11][105/817]	Loss 0.0232 (0.0949)	
training:	Epoch: [11][106/817]	Loss 0.0232 (0.0943)	
training:	Epoch: [11][107/817]	Loss 0.0181 (0.0935)	
training:	Epoch: [11][108/817]	Loss 0.7483 (0.0996)	
training:	Epoch: [11][109/817]	Loss 0.0200 (0.0989)	
training:	Epoch: [11][110/817]	Loss 0.0177 (0.0981)	
training:	Epoch: [11][111/817]	Loss 0.0193 (0.0974)	
training:	Epoch: [11][112/817]	Loss 0.0458 (0.0970)	
training:	Epoch: [11][113/817]	Loss 0.2279 (0.0981)	
training:	Epoch: [11][114/817]	Loss 0.1906 (0.0989)	
training:	Epoch: [11][115/817]	Loss 0.0178 (0.0982)	
training:	Epoch: [11][116/817]	Loss 0.5762 (0.1023)	
training:	Epoch: [11][117/817]	Loss 0.0213 (0.1017)	
training:	Epoch: [11][118/817]	Loss 0.0182 (0.1009)	
training:	Epoch: [11][119/817]	Loss 0.0156 (0.1002)	
training:	Epoch: [11][120/817]	Loss 0.0308 (0.0997)	
training:	Epoch: [11][121/817]	Loss 0.0934 (0.0996)	
training:	Epoch: [11][122/817]	Loss 0.0171 (0.0989)	
training:	Epoch: [11][123/817]	Loss 0.0202 (0.0983)	
training:	Epoch: [11][124/817]	Loss 0.0730 (0.0981)	
training:	Epoch: [11][125/817]	Loss 0.0205 (0.0975)	
training:	Epoch: [11][126/817]	Loss 0.0263 (0.0969)	
training:	Epoch: [11][127/817]	Loss 0.0228 (0.0963)	
training:	Epoch: [11][128/817]	Loss 0.2868 (0.0978)	
training:	Epoch: [11][129/817]	Loss 0.0832 (0.0977)	
training:	Epoch: [11][130/817]	Loss 0.0207 (0.0971)	
training:	Epoch: [11][131/817]	Loss 0.0162 (0.0965)	
training:	Epoch: [11][132/817]	Loss 0.0250 (0.0959)	
training:	Epoch: [11][133/817]	Loss 0.0145 (0.0953)	
training:	Epoch: [11][134/817]	Loss 0.0395 (0.0949)	
training:	Epoch: [11][135/817]	Loss 0.0860 (0.0948)	
training:	Epoch: [11][136/817]	Loss 0.5642 (0.0983)	
training:	Epoch: [11][137/817]	Loss 0.0288 (0.0978)	
training:	Epoch: [11][138/817]	Loss 0.3050 (0.0993)	
training:	Epoch: [11][139/817]	Loss 0.0460 (0.0989)	
training:	Epoch: [11][140/817]	Loss 0.0218 (0.0984)	
training:	Epoch: [11][141/817]	Loss 0.0310 (0.0979)	
training:	Epoch: [11][142/817]	Loss 0.0331 (0.0974)	
training:	Epoch: [11][143/817]	Loss 0.0282 (0.0969)	
training:	Epoch: [11][144/817]	Loss 0.0294 (0.0965)	
training:	Epoch: [11][145/817]	Loss 0.0204 (0.0959)	
training:	Epoch: [11][146/817]	Loss 0.0241 (0.0954)	
training:	Epoch: [11][147/817]	Loss 0.0180 (0.0949)	
training:	Epoch: [11][148/817]	Loss 0.0166 (0.0944)	
training:	Epoch: [11][149/817]	Loss 0.0209 (0.0939)	
training:	Epoch: [11][150/817]	Loss 0.0298 (0.0935)	
training:	Epoch: [11][151/817]	Loss 0.0326 (0.0931)	
training:	Epoch: [11][152/817]	Loss 0.0166 (0.0926)	
training:	Epoch: [11][153/817]	Loss 0.0172 (0.0921)	
training:	Epoch: [11][154/817]	Loss 0.0176 (0.0916)	
training:	Epoch: [11][155/817]	Loss 0.0240 (0.0912)	
training:	Epoch: [11][156/817]	Loss 0.5127 (0.0939)	
training:	Epoch: [11][157/817]	Loss 0.0168 (0.0934)	
training:	Epoch: [11][158/817]	Loss 0.0216 (0.0929)	
training:	Epoch: [11][159/817]	Loss 0.0221 (0.0925)	
training:	Epoch: [11][160/817]	Loss 0.0218 (0.0920)	
training:	Epoch: [11][161/817]	Loss 0.0191 (0.0916)	
training:	Epoch: [11][162/817]	Loss 0.4604 (0.0938)	
training:	Epoch: [11][163/817]	Loss 0.1147 (0.0940)	
training:	Epoch: [11][164/817]	Loss 0.0196 (0.0935)	
training:	Epoch: [11][165/817]	Loss 0.0231 (0.0931)	
training:	Epoch: [11][166/817]	Loss 0.0199 (0.0927)	
training:	Epoch: [11][167/817]	Loss 0.0224 (0.0922)	
training:	Epoch: [11][168/817]	Loss 0.0190 (0.0918)	
training:	Epoch: [11][169/817]	Loss 0.5057 (0.0942)	
training:	Epoch: [11][170/817]	Loss 0.0192 (0.0938)	
training:	Epoch: [11][171/817]	Loss 0.0189 (0.0934)	
training:	Epoch: [11][172/817]	Loss 0.2743 (0.0944)	
training:	Epoch: [11][173/817]	Loss 0.8740 (0.0989)	
training:	Epoch: [11][174/817]	Loss 0.0176 (0.0985)	
training:	Epoch: [11][175/817]	Loss 0.4585 (0.1005)	
training:	Epoch: [11][176/817]	Loss 0.0164 (0.1000)	
training:	Epoch: [11][177/817]	Loss 0.0170 (0.0996)	
training:	Epoch: [11][178/817]	Loss 0.0513 (0.0993)	
training:	Epoch: [11][179/817]	Loss 0.1481 (0.0996)	
training:	Epoch: [11][180/817]	Loss 0.0257 (0.0992)	
training:	Epoch: [11][181/817]	Loss 0.0209 (0.0987)	
training:	Epoch: [11][182/817]	Loss 0.0560 (0.0985)	
training:	Epoch: [11][183/817]	Loss 0.0211 (0.0981)	
training:	Epoch: [11][184/817]	Loss 0.0221 (0.0977)	
training:	Epoch: [11][185/817]	Loss 0.3374 (0.0990)	
training:	Epoch: [11][186/817]	Loss 0.0183 (0.0985)	
training:	Epoch: [11][187/817]	Loss 0.0200 (0.0981)	
training:	Epoch: [11][188/817]	Loss 0.0218 (0.0977)	
training:	Epoch: [11][189/817]	Loss 0.0203 (0.0973)	
training:	Epoch: [11][190/817]	Loss 0.0213 (0.0969)	
training:	Epoch: [11][191/817]	Loss 0.0222 (0.0965)	
training:	Epoch: [11][192/817]	Loss 0.1568 (0.0968)	
training:	Epoch: [11][193/817]	Loss 0.0189 (0.0964)	
training:	Epoch: [11][194/817]	Loss 0.0442 (0.0961)	
training:	Epoch: [11][195/817]	Loss 0.0178 (0.0957)	
training:	Epoch: [11][196/817]	Loss 0.0363 (0.0954)	
training:	Epoch: [11][197/817]	Loss 0.0299 (0.0951)	
training:	Epoch: [11][198/817]	Loss 0.0200 (0.0947)	
training:	Epoch: [11][199/817]	Loss 0.0318 (0.0944)	
training:	Epoch: [11][200/817]	Loss 0.0158 (0.0940)	
training:	Epoch: [11][201/817]	Loss 0.4960 (0.0960)	
training:	Epoch: [11][202/817]	Loss 0.2643 (0.0968)	
training:	Epoch: [11][203/817]	Loss 0.0240 (0.0965)	
training:	Epoch: [11][204/817]	Loss 0.0221 (0.0961)	
training:	Epoch: [11][205/817]	Loss 0.0154 (0.0957)	
training:	Epoch: [11][206/817]	Loss 0.0306 (0.0954)	
training:	Epoch: [11][207/817]	Loss 0.0289 (0.0951)	
training:	Epoch: [11][208/817]	Loss 0.0189 (0.0947)	
training:	Epoch: [11][209/817]	Loss 0.0232 (0.0944)	
training:	Epoch: [11][210/817]	Loss 0.0298 (0.0941)	
training:	Epoch: [11][211/817]	Loss 0.0168 (0.0937)	
training:	Epoch: [11][212/817]	Loss 0.0514 (0.0935)	
training:	Epoch: [11][213/817]	Loss 0.0195 (0.0932)	
training:	Epoch: [11][214/817]	Loss 0.0164 (0.0928)	
training:	Epoch: [11][215/817]	Loss 0.5014 (0.0947)	
training:	Epoch: [11][216/817]	Loss 0.0219 (0.0944)	
training:	Epoch: [11][217/817]	Loss 0.0329 (0.0941)	
training:	Epoch: [11][218/817]	Loss 0.0227 (0.0938)	
training:	Epoch: [11][219/817]	Loss 0.0259 (0.0934)	
training:	Epoch: [11][220/817]	Loss 0.0260 (0.0931)	
training:	Epoch: [11][221/817]	Loss 0.1331 (0.0933)	
training:	Epoch: [11][222/817]	Loss 0.0203 (0.0930)	
training:	Epoch: [11][223/817]	Loss 0.0153 (0.0926)	
training:	Epoch: [11][224/817]	Loss 0.0216 (0.0923)	
training:	Epoch: [11][225/817]	Loss 0.0136 (0.0920)	
training:	Epoch: [11][226/817]	Loss 0.0167 (0.0916)	
training:	Epoch: [11][227/817]	Loss 0.0213 (0.0913)	
training:	Epoch: [11][228/817]	Loss 0.0185 (0.0910)	
training:	Epoch: [11][229/817]	Loss 0.7540 (0.0939)	
training:	Epoch: [11][230/817]	Loss 0.0201 (0.0936)	
training:	Epoch: [11][231/817]	Loss 0.0174 (0.0933)	
training:	Epoch: [11][232/817]	Loss 0.0218 (0.0929)	
training:	Epoch: [11][233/817]	Loss 0.0194 (0.0926)	
training:	Epoch: [11][234/817]	Loss 0.0226 (0.0923)	
training:	Epoch: [11][235/817]	Loss 0.0234 (0.0920)	
training:	Epoch: [11][236/817]	Loss 0.0165 (0.0917)	
training:	Epoch: [11][237/817]	Loss 0.0261 (0.0914)	
training:	Epoch: [11][238/817]	Loss 0.0160 (0.0911)	
training:	Epoch: [11][239/817]	Loss 0.0194 (0.0908)	
training:	Epoch: [11][240/817]	Loss 0.0173 (0.0905)	
training:	Epoch: [11][241/817]	Loss 0.0234 (0.0902)	
training:	Epoch: [11][242/817]	Loss 0.4747 (0.0918)	
training:	Epoch: [11][243/817]	Loss 0.5612 (0.0938)	
training:	Epoch: [11][244/817]	Loss 0.0420 (0.0935)	
training:	Epoch: [11][245/817]	Loss 0.0188 (0.0932)	
training:	Epoch: [11][246/817]	Loss 0.0239 (0.0930)	
training:	Epoch: [11][247/817]	Loss 0.4997 (0.0946)	
training:	Epoch: [11][248/817]	Loss 0.0224 (0.0943)	
training:	Epoch: [11][249/817]	Loss 0.4792 (0.0959)	
training:	Epoch: [11][250/817]	Loss 0.5189 (0.0976)	
training:	Epoch: [11][251/817]	Loss 0.1242 (0.0977)	
training:	Epoch: [11][252/817]	Loss 0.0331 (0.0974)	
training:	Epoch: [11][253/817]	Loss 0.0285 (0.0971)	
training:	Epoch: [11][254/817]	Loss 0.0764 (0.0970)	
training:	Epoch: [11][255/817]	Loss 0.0798 (0.0970)	
training:	Epoch: [11][256/817]	Loss 0.0197 (0.0967)	
training:	Epoch: [11][257/817]	Loss 0.0223 (0.0964)	
training:	Epoch: [11][258/817]	Loss 0.0178 (0.0961)	
training:	Epoch: [11][259/817]	Loss 0.0213 (0.0958)	
training:	Epoch: [11][260/817]	Loss 0.0162 (0.0955)	
training:	Epoch: [11][261/817]	Loss 0.5741 (0.0973)	
training:	Epoch: [11][262/817]	Loss 0.0139 (0.0970)	
training:	Epoch: [11][263/817]	Loss 0.0165 (0.0967)	
training:	Epoch: [11][264/817]	Loss 0.0348 (0.0965)	
training:	Epoch: [11][265/817]	Loss 0.0194 (0.0962)	
training:	Epoch: [11][266/817]	Loss 0.0397 (0.0960)	
training:	Epoch: [11][267/817]	Loss 0.0211 (0.0957)	
training:	Epoch: [11][268/817]	Loss 0.0174 (0.0954)	
training:	Epoch: [11][269/817]	Loss 0.0197 (0.0951)	
training:	Epoch: [11][270/817]	Loss 0.0194 (0.0948)	
training:	Epoch: [11][271/817]	Loss 0.0181 (0.0945)	
training:	Epoch: [11][272/817]	Loss 0.0190 (0.0943)	
training:	Epoch: [11][273/817]	Loss 0.0367 (0.0941)	
training:	Epoch: [11][274/817]	Loss 0.0210 (0.0938)	
training:	Epoch: [11][275/817]	Loss 0.0780 (0.0937)	
training:	Epoch: [11][276/817]	Loss 0.0318 (0.0935)	
training:	Epoch: [11][277/817]	Loss 0.0200 (0.0932)	
training:	Epoch: [11][278/817]	Loss 0.0679 (0.0932)	
training:	Epoch: [11][279/817]	Loss 0.0151 (0.0929)	
training:	Epoch: [11][280/817]	Loss 0.0161 (0.0926)	
training:	Epoch: [11][281/817]	Loss 0.0166 (0.0923)	
training:	Epoch: [11][282/817]	Loss 0.4896 (0.0937)	
training:	Epoch: [11][283/817]	Loss 0.0325 (0.0935)	
training:	Epoch: [11][284/817]	Loss 0.4742 (0.0949)	
training:	Epoch: [11][285/817]	Loss 0.0210 (0.0946)	
training:	Epoch: [11][286/817]	Loss 0.0174 (0.0943)	
training:	Epoch: [11][287/817]	Loss 0.0311 (0.0941)	
training:	Epoch: [11][288/817]	Loss 0.0138 (0.0938)	
training:	Epoch: [11][289/817]	Loss 0.0209 (0.0936)	
training:	Epoch: [11][290/817]	Loss 0.0157 (0.0933)	
training:	Epoch: [11][291/817]	Loss 0.0239 (0.0931)	
training:	Epoch: [11][292/817]	Loss 0.0184 (0.0928)	
training:	Epoch: [11][293/817]	Loss 0.4996 (0.0942)	
training:	Epoch: [11][294/817]	Loss 0.5279 (0.0957)	
training:	Epoch: [11][295/817]	Loss 0.0227 (0.0954)	
training:	Epoch: [11][296/817]	Loss 0.0315 (0.0952)	
training:	Epoch: [11][297/817]	Loss 0.0152 (0.0949)	
training:	Epoch: [11][298/817]	Loss 0.4981 (0.0963)	
training:	Epoch: [11][299/817]	Loss 0.4986 (0.0976)	
training:	Epoch: [11][300/817]	Loss 0.3828 (0.0986)	
training:	Epoch: [11][301/817]	Loss 0.1229 (0.0987)	
training:	Epoch: [11][302/817]	Loss 0.0271 (0.0984)	
training:	Epoch: [11][303/817]	Loss 0.0227 (0.0982)	
training:	Epoch: [11][304/817]	Loss 0.0255 (0.0979)	
training:	Epoch: [11][305/817]	Loss 0.0240 (0.0977)	
training:	Epoch: [11][306/817]	Loss 0.2826 (0.0983)	
training:	Epoch: [11][307/817]	Loss 0.0279 (0.0981)	
training:	Epoch: [11][308/817]	Loss 0.0315 (0.0979)	
training:	Epoch: [11][309/817]	Loss 0.0230 (0.0976)	
training:	Epoch: [11][310/817]	Loss 0.0191 (0.0974)	
training:	Epoch: [11][311/817]	Loss 0.0609 (0.0973)	
training:	Epoch: [11][312/817]	Loss 0.0260 (0.0970)	
training:	Epoch: [11][313/817]	Loss 0.0768 (0.0970)	
training:	Epoch: [11][314/817]	Loss 0.0196 (0.0967)	
training:	Epoch: [11][315/817]	Loss 0.6168 (0.0984)	
training:	Epoch: [11][316/817]	Loss 0.0214 (0.0981)	
training:	Epoch: [11][317/817]	Loss 0.0282 (0.0979)	
training:	Epoch: [11][318/817]	Loss 0.0200 (0.0977)	
training:	Epoch: [11][319/817]	Loss 0.0207 (0.0974)	
training:	Epoch: [11][320/817]	Loss 0.0232 (0.0972)	
training:	Epoch: [11][321/817]	Loss 0.0273 (0.0970)	
training:	Epoch: [11][322/817]	Loss 0.0452 (0.0968)	
training:	Epoch: [11][323/817]	Loss 0.5570 (0.0982)	
training:	Epoch: [11][324/817]	Loss 0.0159 (0.0980)	
training:	Epoch: [11][325/817]	Loss 0.0346 (0.0978)	
training:	Epoch: [11][326/817]	Loss 0.0222 (0.0975)	
training:	Epoch: [11][327/817]	Loss 0.0179 (0.0973)	
training:	Epoch: [11][328/817]	Loss 0.0245 (0.0971)	
training:	Epoch: [11][329/817]	Loss 0.0784 (0.0970)	
training:	Epoch: [11][330/817]	Loss 0.0250 (0.0968)	
training:	Epoch: [11][331/817]	Loss 0.0247 (0.0966)	
training:	Epoch: [11][332/817]	Loss 0.0550 (0.0965)	
training:	Epoch: [11][333/817]	Loss 0.0174 (0.0962)	
training:	Epoch: [11][334/817]	Loss 0.0552 (0.0961)	
training:	Epoch: [11][335/817]	Loss 0.1476 (0.0963)	
training:	Epoch: [11][336/817]	Loss 0.0286 (0.0961)	
training:	Epoch: [11][337/817]	Loss 0.0178 (0.0958)	
training:	Epoch: [11][338/817]	Loss 0.0244 (0.0956)	
training:	Epoch: [11][339/817]	Loss 0.0615 (0.0955)	
training:	Epoch: [11][340/817]	Loss 0.0174 (0.0953)	
training:	Epoch: [11][341/817]	Loss 0.0164 (0.0951)	
training:	Epoch: [11][342/817]	Loss 0.0947 (0.0950)	
training:	Epoch: [11][343/817]	Loss 0.0224 (0.0948)	
training:	Epoch: [11][344/817]	Loss 0.0192 (0.0946)	
training:	Epoch: [11][345/817]	Loss 0.0156 (0.0944)	
training:	Epoch: [11][346/817]	Loss 0.0183 (0.0942)	
training:	Epoch: [11][347/817]	Loss 0.0181 (0.0939)	
training:	Epoch: [11][348/817]	Loss 0.0215 (0.0937)	
training:	Epoch: [11][349/817]	Loss 0.0181 (0.0935)	
training:	Epoch: [11][350/817]	Loss 0.6151 (0.0950)	
training:	Epoch: [11][351/817]	Loss 0.0293 (0.0948)	
training:	Epoch: [11][352/817]	Loss 0.0229 (0.0946)	
training:	Epoch: [11][353/817]	Loss 0.0307 (0.0944)	
training:	Epoch: [11][354/817]	Loss 0.0196 (0.0942)	
training:	Epoch: [11][355/817]	Loss 0.2860 (0.0948)	
training:	Epoch: [11][356/817]	Loss 0.0193 (0.0946)	
training:	Epoch: [11][357/817]	Loss 0.0176 (0.0943)	
training:	Epoch: [11][358/817]	Loss 0.0208 (0.0941)	
training:	Epoch: [11][359/817]	Loss 0.0317 (0.0940)	
training:	Epoch: [11][360/817]	Loss 0.0269 (0.0938)	
training:	Epoch: [11][361/817]	Loss 0.0160 (0.0936)	
training:	Epoch: [11][362/817]	Loss 0.0179 (0.0934)	
training:	Epoch: [11][363/817]	Loss 0.0327 (0.0932)	
training:	Epoch: [11][364/817]	Loss 0.0213 (0.0930)	
training:	Epoch: [11][365/817]	Loss 0.4807 (0.0941)	
training:	Epoch: [11][366/817]	Loss 0.0840 (0.0940)	
training:	Epoch: [11][367/817]	Loss 0.0194 (0.0938)	
training:	Epoch: [11][368/817]	Loss 0.0169 (0.0936)	
training:	Epoch: [11][369/817]	Loss 0.0177 (0.0934)	
training:	Epoch: [11][370/817]	Loss 0.0198 (0.0932)	
training:	Epoch: [11][371/817]	Loss 0.0189 (0.0930)	
training:	Epoch: [11][372/817]	Loss 0.0163 (0.0928)	
training:	Epoch: [11][373/817]	Loss 0.0142 (0.0926)	
training:	Epoch: [11][374/817]	Loss 0.0240 (0.0924)	
training:	Epoch: [11][375/817]	Loss 0.5648 (0.0937)	
training:	Epoch: [11][376/817]	Loss 0.0184 (0.0935)	
training:	Epoch: [11][377/817]	Loss 0.0187 (0.0933)	
training:	Epoch: [11][378/817]	Loss 0.0190 (0.0931)	
training:	Epoch: [11][379/817]	Loss 0.0189 (0.0929)	
training:	Epoch: [11][380/817]	Loss 0.0271 (0.0927)	
training:	Epoch: [11][381/817]	Loss 0.0182 (0.0925)	
training:	Epoch: [11][382/817]	Loss 0.0204 (0.0923)	
training:	Epoch: [11][383/817]	Loss 0.0150 (0.0921)	
training:	Epoch: [11][384/817]	Loss 0.0318 (0.0920)	
training:	Epoch: [11][385/817]	Loss 0.1089 (0.0920)	
training:	Epoch: [11][386/817]	Loss 0.0181 (0.0918)	
training:	Epoch: [11][387/817]	Loss 0.0242 (0.0916)	
training:	Epoch: [11][388/817]	Loss 0.0323 (0.0915)	
training:	Epoch: [11][389/817]	Loss 0.0213 (0.0913)	
training:	Epoch: [11][390/817]	Loss 0.0237 (0.0911)	
training:	Epoch: [11][391/817]	Loss 0.5516 (0.0923)	
training:	Epoch: [11][392/817]	Loss 0.0340 (0.0922)	
training:	Epoch: [11][393/817]	Loss 0.0314 (0.0920)	
training:	Epoch: [11][394/817]	Loss 0.5708 (0.0932)	
training:	Epoch: [11][395/817]	Loss 0.0253 (0.0930)	
training:	Epoch: [11][396/817]	Loss 0.5564 (0.0942)	
training:	Epoch: [11][397/817]	Loss 0.4984 (0.0952)	
training:	Epoch: [11][398/817]	Loss 0.0173 (0.0950)	
training:	Epoch: [11][399/817]	Loss 0.0170 (0.0948)	
training:	Epoch: [11][400/817]	Loss 0.0218 (0.0947)	
training:	Epoch: [11][401/817]	Loss 0.5461 (0.0958)	
training:	Epoch: [11][402/817]	Loss 0.0192 (0.0956)	
training:	Epoch: [11][403/817]	Loss 0.0182 (0.0954)	
training:	Epoch: [11][404/817]	Loss 0.0178 (0.0952)	
training:	Epoch: [11][405/817]	Loss 0.0186 (0.0950)	
training:	Epoch: [11][406/817]	Loss 0.0185 (0.0948)	
training:	Epoch: [11][407/817]	Loss 0.0161 (0.0946)	
training:	Epoch: [11][408/817]	Loss 0.1574 (0.0948)	
training:	Epoch: [11][409/817]	Loss 0.0215 (0.0946)	
training:	Epoch: [11][410/817]	Loss 0.0523 (0.0945)	
training:	Epoch: [11][411/817]	Loss 0.0190 (0.0943)	
training:	Epoch: [11][412/817]	Loss 0.0154 (0.0941)	
training:	Epoch: [11][413/817]	Loss 0.4788 (0.0951)	
training:	Epoch: [11][414/817]	Loss 0.0210 (0.0949)	
training:	Epoch: [11][415/817]	Loss 0.0201 (0.0947)	
training:	Epoch: [11][416/817]	Loss 0.3599 (0.0953)	
training:	Epoch: [11][417/817]	Loss 0.0209 (0.0952)	
training:	Epoch: [11][418/817]	Loss 0.0212 (0.0950)	
training:	Epoch: [11][419/817]	Loss 0.0251 (0.0948)	
training:	Epoch: [11][420/817]	Loss 0.0218 (0.0947)	
training:	Epoch: [11][421/817]	Loss 0.0242 (0.0945)	
training:	Epoch: [11][422/817]	Loss 0.0232 (0.0943)	
training:	Epoch: [11][423/817]	Loss 0.0161 (0.0941)	
training:	Epoch: [11][424/817]	Loss 0.0164 (0.0939)	
training:	Epoch: [11][425/817]	Loss 0.0186 (0.0938)	
training:	Epoch: [11][426/817]	Loss 0.0202 (0.0936)	
training:	Epoch: [11][427/817]	Loss 0.0186 (0.0934)	
training:	Epoch: [11][428/817]	Loss 0.0142 (0.0932)	
training:	Epoch: [11][429/817]	Loss 0.0171 (0.0931)	
training:	Epoch: [11][430/817]	Loss 0.0212 (0.0929)	
training:	Epoch: [11][431/817]	Loss 0.0170 (0.0927)	
training:	Epoch: [11][432/817]	Loss 0.0158 (0.0925)	
training:	Epoch: [11][433/817]	Loss 0.0153 (0.0924)	
training:	Epoch: [11][434/817]	Loss 0.0201 (0.0922)	
training:	Epoch: [11][435/817]	Loss 0.0196 (0.0920)	
training:	Epoch: [11][436/817]	Loss 0.0183 (0.0919)	
training:	Epoch: [11][437/817]	Loss 0.0713 (0.0918)	
training:	Epoch: [11][438/817]	Loss 0.0182 (0.0916)	
training:	Epoch: [11][439/817]	Loss 0.0164 (0.0915)	
training:	Epoch: [11][440/817]	Loss 0.0219 (0.0913)	
training:	Epoch: [11][441/817]	Loss 0.0497 (0.0912)	
training:	Epoch: [11][442/817]	Loss 0.0158 (0.0910)	
training:	Epoch: [11][443/817]	Loss 0.0269 (0.0909)	
training:	Epoch: [11][444/817]	Loss 0.0219 (0.0907)	
training:	Epoch: [11][445/817]	Loss 0.0175 (0.0906)	
training:	Epoch: [11][446/817]	Loss 0.0164 (0.0904)	
training:	Epoch: [11][447/817]	Loss 0.3964 (0.0911)	
training:	Epoch: [11][448/817]	Loss 0.0331 (0.0910)	
training:	Epoch: [11][449/817]	Loss 0.0157 (0.0908)	
training:	Epoch: [11][450/817]	Loss 0.0184 (0.0906)	
training:	Epoch: [11][451/817]	Loss 0.0290 (0.0905)	
training:	Epoch: [11][452/817]	Loss 0.0181 (0.0903)	
training:	Epoch: [11][453/817]	Loss 0.0211 (0.0902)	
training:	Epoch: [11][454/817]	Loss 0.0924 (0.0902)	
training:	Epoch: [11][455/817]	Loss 0.0182 (0.0900)	
training:	Epoch: [11][456/817]	Loss 0.0323 (0.0899)	
training:	Epoch: [11][457/817]	Loss 0.0212 (0.0898)	
training:	Epoch: [11][458/817]	Loss 0.0170 (0.0896)	
training:	Epoch: [11][459/817]	Loss 0.0351 (0.0895)	
training:	Epoch: [11][460/817]	Loss 0.0161 (0.0893)	
training:	Epoch: [11][461/817]	Loss 0.0187 (0.0892)	
training:	Epoch: [11][462/817]	Loss 0.0176 (0.0890)	
training:	Epoch: [11][463/817]	Loss 0.0171 (0.0889)	
training:	Epoch: [11][464/817]	Loss 0.0322 (0.0887)	
training:	Epoch: [11][465/817]	Loss 0.0171 (0.0886)	
training:	Epoch: [11][466/817]	Loss 0.0234 (0.0884)	
training:	Epoch: [11][467/817]	Loss 0.5527 (0.0894)	
training:	Epoch: [11][468/817]	Loss 0.0172 (0.0893)	
training:	Epoch: [11][469/817]	Loss 0.0319 (0.0892)	
training:	Epoch: [11][470/817]	Loss 0.0188 (0.0890)	
training:	Epoch: [11][471/817]	Loss 0.0220 (0.0889)	
training:	Epoch: [11][472/817]	Loss 0.0208 (0.0887)	
training:	Epoch: [11][473/817]	Loss 0.5018 (0.0896)	
training:	Epoch: [11][474/817]	Loss 0.0276 (0.0895)	
training:	Epoch: [11][475/817]	Loss 0.0173 (0.0893)	
training:	Epoch: [11][476/817]	Loss 0.0223 (0.0892)	
training:	Epoch: [11][477/817]	Loss 0.0180 (0.0890)	
training:	Epoch: [11][478/817]	Loss 0.0165 (0.0889)	
training:	Epoch: [11][479/817]	Loss 0.1259 (0.0890)	
training:	Epoch: [11][480/817]	Loss 0.0270 (0.0888)	
training:	Epoch: [11][481/817]	Loss 0.0166 (0.0887)	
training:	Epoch: [11][482/817]	Loss 0.0947 (0.0887)	
training:	Epoch: [11][483/817]	Loss 0.5679 (0.0897)	
training:	Epoch: [11][484/817]	Loss 0.0159 (0.0895)	
training:	Epoch: [11][485/817]	Loss 0.5531 (0.0905)	
training:	Epoch: [11][486/817]	Loss 0.0158 (0.0903)	
training:	Epoch: [11][487/817]	Loss 0.0171 (0.0902)	
training:	Epoch: [11][488/817]	Loss 0.0211 (0.0900)	
training:	Epoch: [11][489/817]	Loss 0.0196 (0.0899)	
training:	Epoch: [11][490/817]	Loss 0.5605 (0.0909)	
training:	Epoch: [11][491/817]	Loss 0.1094 (0.0909)	
training:	Epoch: [11][492/817]	Loss 0.0151 (0.0907)	
training:	Epoch: [11][493/817]	Loss 0.0228 (0.0906)	
training:	Epoch: [11][494/817]	Loss 0.0258 (0.0905)	
training:	Epoch: [11][495/817]	Loss 0.0327 (0.0903)	
training:	Epoch: [11][496/817]	Loss 1.0974 (0.0924)	
training:	Epoch: [11][497/817]	Loss 0.0196 (0.0922)	
training:	Epoch: [11][498/817]	Loss 0.3541 (0.0928)	
training:	Epoch: [11][499/817]	Loss 0.0227 (0.0926)	
training:	Epoch: [11][500/817]	Loss 0.0242 (0.0925)	
training:	Epoch: [11][501/817]	Loss 0.0178 (0.0923)	
training:	Epoch: [11][502/817]	Loss 0.0153 (0.0922)	
training:	Epoch: [11][503/817]	Loss 0.0244 (0.0920)	
training:	Epoch: [11][504/817]	Loss 0.0157 (0.0919)	
training:	Epoch: [11][505/817]	Loss 0.0342 (0.0918)	
training:	Epoch: [11][506/817]	Loss 0.0450 (0.0917)	
training:	Epoch: [11][507/817]	Loss 0.0176 (0.0915)	
training:	Epoch: [11][508/817]	Loss 0.0160 (0.0914)	
training:	Epoch: [11][509/817]	Loss 0.0192 (0.0912)	
training:	Epoch: [11][510/817]	Loss 0.0209 (0.0911)	
training:	Epoch: [11][511/817]	Loss 0.0954 (0.0911)	
training:	Epoch: [11][512/817]	Loss 0.0159 (0.0910)	
training:	Epoch: [11][513/817]	Loss 0.0283 (0.0909)	
training:	Epoch: [11][514/817]	Loss 0.0180 (0.0907)	
training:	Epoch: [11][515/817]	Loss 0.0332 (0.0906)	
training:	Epoch: [11][516/817]	Loss 0.0153 (0.0905)	
training:	Epoch: [11][517/817]	Loss 0.2521 (0.0908)	
training:	Epoch: [11][518/817]	Loss 0.5355 (0.0916)	
training:	Epoch: [11][519/817]	Loss 0.0180 (0.0915)	
training:	Epoch: [11][520/817]	Loss 0.0191 (0.0913)	
training:	Epoch: [11][521/817]	Loss 0.0206 (0.0912)	
training:	Epoch: [11][522/817]	Loss 0.0148 (0.0911)	
training:	Epoch: [11][523/817]	Loss 0.0262 (0.0909)	
training:	Epoch: [11][524/817]	Loss 0.0176 (0.0908)	
training:	Epoch: [11][525/817]	Loss 0.0171 (0.0907)	
training:	Epoch: [11][526/817]	Loss 0.0174 (0.0905)	
training:	Epoch: [11][527/817]	Loss 0.0184 (0.0904)	
training:	Epoch: [11][528/817]	Loss 0.0188 (0.0902)	
training:	Epoch: [11][529/817]	Loss 0.0184 (0.0901)	
training:	Epoch: [11][530/817]	Loss 0.0267 (0.0900)	
training:	Epoch: [11][531/817]	Loss 0.4515 (0.0907)	
training:	Epoch: [11][532/817]	Loss 0.0155 (0.0905)	
training:	Epoch: [11][533/817]	Loss 0.0211 (0.0904)	
training:	Epoch: [11][534/817]	Loss 0.0388 (0.0903)	
training:	Epoch: [11][535/817]	Loss 0.0200 (0.0902)	
training:	Epoch: [11][536/817]	Loss 0.0205 (0.0900)	
training:	Epoch: [11][537/817]	Loss 0.0190 (0.0899)	
training:	Epoch: [11][538/817]	Loss 0.0280 (0.0898)	
training:	Epoch: [11][539/817]	Loss 0.0674 (0.0898)	
training:	Epoch: [11][540/817]	Loss 0.4922 (0.0905)	
training:	Epoch: [11][541/817]	Loss 0.9715 (0.0921)	
training:	Epoch: [11][542/817]	Loss 0.0175 (0.0920)	
training:	Epoch: [11][543/817]	Loss 0.0270 (0.0919)	
training:	Epoch: [11][544/817]	Loss 0.0198 (0.0917)	
training:	Epoch: [11][545/817]	Loss 0.0167 (0.0916)	
training:	Epoch: [11][546/817]	Loss 0.0240 (0.0915)	
training:	Epoch: [11][547/817]	Loss 0.5647 (0.0923)	
training:	Epoch: [11][548/817]	Loss 0.0173 (0.0922)	
training:	Epoch: [11][549/817]	Loss 0.0254 (0.0921)	
training:	Epoch: [11][550/817]	Loss 0.5164 (0.0929)	
training:	Epoch: [11][551/817]	Loss 0.0153 (0.0927)	
training:	Epoch: [11][552/817]	Loss 0.5143 (0.0935)	
training:	Epoch: [11][553/817]	Loss 0.0161 (0.0933)	
training:	Epoch: [11][554/817]	Loss 0.0186 (0.0932)	
training:	Epoch: [11][555/817]	Loss 0.0211 (0.0931)	
training:	Epoch: [11][556/817]	Loss 0.0209 (0.0929)	
training:	Epoch: [11][557/817]	Loss 0.0157 (0.0928)	
training:	Epoch: [11][558/817]	Loss 0.2325 (0.0931)	
training:	Epoch: [11][559/817]	Loss 0.3858 (0.0936)	
training:	Epoch: [11][560/817]	Loss 0.0207 (0.0934)	
training:	Epoch: [11][561/817]	Loss 0.5460 (0.0943)	
training:	Epoch: [11][562/817]	Loss 0.0217 (0.0941)	
training:	Epoch: [11][563/817]	Loss 0.0264 (0.0940)	
training:	Epoch: [11][564/817]	Loss 0.0175 (0.0939)	
training:	Epoch: [11][565/817]	Loss 0.0188 (0.0937)	
training:	Epoch: [11][566/817]	Loss 0.0215 (0.0936)	
training:	Epoch: [11][567/817]	Loss 0.0244 (0.0935)	
training:	Epoch: [11][568/817]	Loss 0.0234 (0.0934)	
training:	Epoch: [11][569/817]	Loss 0.0421 (0.0933)	
training:	Epoch: [11][570/817]	Loss 0.0162 (0.0931)	
training:	Epoch: [11][571/817]	Loss 0.0209 (0.0930)	
training:	Epoch: [11][572/817]	Loss 0.0972 (0.0930)	
training:	Epoch: [11][573/817]	Loss 0.0275 (0.0929)	
training:	Epoch: [11][574/817]	Loss 0.0503 (0.0928)	
training:	Epoch: [11][575/817]	Loss 0.0196 (0.0927)	
training:	Epoch: [11][576/817]	Loss 0.0217 (0.0926)	
training:	Epoch: [11][577/817]	Loss 0.1131 (0.0926)	
training:	Epoch: [11][578/817]	Loss 0.0172 (0.0925)	
training:	Epoch: [11][579/817]	Loss 0.0276 (0.0924)	
training:	Epoch: [11][580/817]	Loss 0.0184 (0.0922)	
training:	Epoch: [11][581/817]	Loss 0.0154 (0.0921)	
training:	Epoch: [11][582/817]	Loss 0.0212 (0.0920)	
training:	Epoch: [11][583/817]	Loss 0.0163 (0.0919)	
training:	Epoch: [11][584/817]	Loss 0.0178 (0.0917)	
training:	Epoch: [11][585/817]	Loss 0.0182 (0.0916)	
training:	Epoch: [11][586/817]	Loss 0.1242 (0.0917)	
training:	Epoch: [11][587/817]	Loss 0.0152 (0.0915)	
training:	Epoch: [11][588/817]	Loss 0.0176 (0.0914)	
training:	Epoch: [11][589/817]	Loss 0.0880 (0.0914)	
training:	Epoch: [11][590/817]	Loss 0.2141 (0.0916)	
training:	Epoch: [11][591/817]	Loss 0.0177 (0.0915)	
training:	Epoch: [11][592/817]	Loss 0.1376 (0.0916)	
training:	Epoch: [11][593/817]	Loss 0.0205 (0.0914)	
training:	Epoch: [11][594/817]	Loss 0.2559 (0.0917)	
training:	Epoch: [11][595/817]	Loss 0.0320 (0.0916)	
training:	Epoch: [11][596/817]	Loss 0.0171 (0.0915)	
training:	Epoch: [11][597/817]	Loss 0.0221 (0.0914)	
training:	Epoch: [11][598/817]	Loss 0.0298 (0.0913)	
training:	Epoch: [11][599/817]	Loss 0.6091 (0.0921)	
training:	Epoch: [11][600/817]	Loss 0.0442 (0.0921)	
training:	Epoch: [11][601/817]	Loss 0.1052 (0.0921)	
training:	Epoch: [11][602/817]	Loss 0.0362 (0.0920)	
training:	Epoch: [11][603/817]	Loss 0.0392 (0.0919)	
training:	Epoch: [11][604/817]	Loss 0.5469 (0.0927)	
training:	Epoch: [11][605/817]	Loss 0.0149 (0.0925)	
training:	Epoch: [11][606/817]	Loss 0.0171 (0.0924)	
training:	Epoch: [11][607/817]	Loss 0.0192 (0.0923)	
training:	Epoch: [11][608/817]	Loss 0.0159 (0.0922)	
training:	Epoch: [11][609/817]	Loss 0.0167 (0.0920)	
training:	Epoch: [11][610/817]	Loss 0.1853 (0.0922)	
training:	Epoch: [11][611/817]	Loss 0.0201 (0.0921)	
training:	Epoch: [11][612/817]	Loss 0.0348 (0.0920)	
training:	Epoch: [11][613/817]	Loss 0.0332 (0.0919)	
training:	Epoch: [11][614/817]	Loss 0.0190 (0.0918)	
training:	Epoch: [11][615/817]	Loss 0.0340 (0.0917)	
training:	Epoch: [11][616/817]	Loss 0.0198 (0.0915)	
training:	Epoch: [11][617/817]	Loss 0.0170 (0.0914)	
training:	Epoch: [11][618/817]	Loss 0.0325 (0.0913)	
training:	Epoch: [11][619/817]	Loss 0.0146 (0.0912)	
training:	Epoch: [11][620/817]	Loss 0.0147 (0.0911)	
training:	Epoch: [11][621/817]	Loss 0.0143 (0.0910)	
training:	Epoch: [11][622/817]	Loss 0.0233 (0.0909)	
training:	Epoch: [11][623/817]	Loss 0.0391 (0.0908)	
training:	Epoch: [11][624/817]	Loss 0.0168 (0.0906)	
training:	Epoch: [11][625/817]	Loss 0.0192 (0.0905)	
training:	Epoch: [11][626/817]	Loss 0.0978 (0.0905)	
training:	Epoch: [11][627/817]	Loss 0.0472 (0.0905)	
training:	Epoch: [11][628/817]	Loss 0.0153 (0.0904)	
training:	Epoch: [11][629/817]	Loss 0.0196 (0.0902)	
training:	Epoch: [11][630/817]	Loss 0.0142 (0.0901)	
training:	Epoch: [11][631/817]	Loss 0.7528 (0.0912)	
training:	Epoch: [11][632/817]	Loss 0.0195 (0.0911)	
training:	Epoch: [11][633/817]	Loss 0.0209 (0.0910)	
training:	Epoch: [11][634/817]	Loss 0.0173 (0.0908)	
training:	Epoch: [11][635/817]	Loss 0.0681 (0.0908)	
training:	Epoch: [11][636/817]	Loss 0.0324 (0.0907)	
training:	Epoch: [11][637/817]	Loss 0.0253 (0.0906)	
training:	Epoch: [11][638/817]	Loss 0.0451 (0.0905)	
training:	Epoch: [11][639/817]	Loss 0.0133 (0.0904)	
training:	Epoch: [11][640/817]	Loss 0.1668 (0.0905)	
training:	Epoch: [11][641/817]	Loss 0.0223 (0.0904)	
training:	Epoch: [11][642/817]	Loss 0.0215 (0.0903)	
training:	Epoch: [11][643/817]	Loss 0.0181 (0.0902)	
training:	Epoch: [11][644/817]	Loss 0.0201 (0.0901)	
training:	Epoch: [11][645/817]	Loss 0.0170 (0.0900)	
training:	Epoch: [11][646/817]	Loss 0.0171 (0.0899)	
training:	Epoch: [11][647/817]	Loss 0.0156 (0.0898)	
training:	Epoch: [11][648/817]	Loss 0.0162 (0.0896)	
training:	Epoch: [11][649/817]	Loss 0.0549 (0.0896)	
training:	Epoch: [11][650/817]	Loss 0.5013 (0.0902)	
training:	Epoch: [11][651/817]	Loss 0.0163 (0.0901)	
training:	Epoch: [11][652/817]	Loss 0.0595 (0.0901)	
training:	Epoch: [11][653/817]	Loss 0.0198 (0.0900)	
training:	Epoch: [11][654/817]	Loss 0.0235 (0.0899)	
training:	Epoch: [11][655/817]	Loss 0.0312 (0.0898)	
training:	Epoch: [11][656/817]	Loss 0.1121 (0.0898)	
training:	Epoch: [11][657/817]	Loss 0.0214 (0.0897)	
training:	Epoch: [11][658/817]	Loss 0.0168 (0.0896)	
training:	Epoch: [11][659/817]	Loss 0.0165 (0.0895)	
training:	Epoch: [11][660/817]	Loss 0.0185 (0.0894)	
training:	Epoch: [11][661/817]	Loss 0.0529 (0.0893)	
training:	Epoch: [11][662/817]	Loss 0.0225 (0.0892)	
training:	Epoch: [11][663/817]	Loss 0.0200 (0.0891)	
training:	Epoch: [11][664/817]	Loss 0.0226 (0.0890)	
training:	Epoch: [11][665/817]	Loss 0.0389 (0.0889)	
training:	Epoch: [11][666/817]	Loss 0.0154 (0.0888)	
training:	Epoch: [11][667/817]	Loss 0.3324 (0.0892)	
training:	Epoch: [11][668/817]	Loss 0.0337 (0.0891)	
training:	Epoch: [11][669/817]	Loss 0.0371 (0.0890)	
training:	Epoch: [11][670/817]	Loss 0.0229 (0.0889)	
training:	Epoch: [11][671/817]	Loss 0.0181 (0.0888)	
training:	Epoch: [11][672/817]	Loss 0.0139 (0.0887)	
training:	Epoch: [11][673/817]	Loss 0.5170 (0.0893)	
training:	Epoch: [11][674/817]	Loss 0.0148 (0.0892)	
training:	Epoch: [11][675/817]	Loss 0.0165 (0.0891)	
training:	Epoch: [11][676/817]	Loss 0.0154 (0.0890)	
training:	Epoch: [11][677/817]	Loss 0.1055 (0.0890)	
training:	Epoch: [11][678/817]	Loss 0.1024 (0.0891)	
training:	Epoch: [11][679/817]	Loss 0.0169 (0.0890)	
training:	Epoch: [11][680/817]	Loss 0.0607 (0.0889)	
training:	Epoch: [11][681/817]	Loss 0.0249 (0.0888)	
training:	Epoch: [11][682/817]	Loss 0.0166 (0.0887)	
training:	Epoch: [11][683/817]	Loss 0.2003 (0.0889)	
training:	Epoch: [11][684/817]	Loss 0.0163 (0.0888)	
training:	Epoch: [11][685/817]	Loss 0.0276 (0.0887)	
training:	Epoch: [11][686/817]	Loss 0.0256 (0.0886)	
training:	Epoch: [11][687/817]	Loss 0.0329 (0.0885)	
training:	Epoch: [11][688/817]	Loss 0.0440 (0.0884)	
training:	Epoch: [11][689/817]	Loss 0.0198 (0.0883)	
training:	Epoch: [11][690/817]	Loss 0.0225 (0.0882)	
training:	Epoch: [11][691/817]	Loss 0.0432 (0.0882)	
training:	Epoch: [11][692/817]	Loss 0.0230 (0.0881)	
training:	Epoch: [11][693/817]	Loss 0.0187 (0.0880)	
training:	Epoch: [11][694/817]	Loss 0.0163 (0.0879)	
training:	Epoch: [11][695/817]	Loss 0.1353 (0.0880)	
training:	Epoch: [11][696/817]	Loss 0.0182 (0.0879)	
training:	Epoch: [11][697/817]	Loss 0.0302 (0.0878)	
training:	Epoch: [11][698/817]	Loss 0.1169 (0.0878)	
training:	Epoch: [11][699/817]	Loss 0.0164 (0.0877)	
training:	Epoch: [11][700/817]	Loss 0.0308 (0.0876)	
training:	Epoch: [11][701/817]	Loss 0.0163 (0.0875)	
training:	Epoch: [11][702/817]	Loss 0.0160 (0.0874)	
training:	Epoch: [11][703/817]	Loss 0.0199 (0.0873)	
training:	Epoch: [11][704/817]	Loss 0.0147 (0.0872)	
training:	Epoch: [11][705/817]	Loss 0.0224 (0.0871)	
training:	Epoch: [11][706/817]	Loss 0.0335 (0.0871)	
training:	Epoch: [11][707/817]	Loss 0.0180 (0.0870)	
training:	Epoch: [11][708/817]	Loss 0.0263 (0.0869)	
training:	Epoch: [11][709/817]	Loss 0.0251 (0.0868)	
training:	Epoch: [11][710/817]	Loss 0.0153 (0.0867)	
training:	Epoch: [11][711/817]	Loss 0.3299 (0.0870)	
training:	Epoch: [11][712/817]	Loss 0.0167 (0.0869)	
training:	Epoch: [11][713/817]	Loss 0.0398 (0.0869)	
training:	Epoch: [11][714/817]	Loss 0.0246 (0.0868)	
training:	Epoch: [11][715/817]	Loss 0.0175 (0.0867)	
training:	Epoch: [11][716/817]	Loss 0.5757 (0.0874)	
training:	Epoch: [11][717/817]	Loss 0.3986 (0.0878)	
training:	Epoch: [11][718/817]	Loss 0.0186 (0.0877)	
training:	Epoch: [11][719/817]	Loss 0.0178 (0.0876)	
training:	Epoch: [11][720/817]	Loss 0.0218 (0.0875)	
training:	Epoch: [11][721/817]	Loss 0.0215 (0.0874)	
training:	Epoch: [11][722/817]	Loss 0.0911 (0.0874)	
training:	Epoch: [11][723/817]	Loss 0.0192 (0.0873)	
training:	Epoch: [11][724/817]	Loss 0.0187 (0.0872)	
training:	Epoch: [11][725/817]	Loss 0.0160 (0.0871)	
training:	Epoch: [11][726/817]	Loss 0.4742 (0.0877)	
training:	Epoch: [11][727/817]	Loss 0.5056 (0.0882)	
training:	Epoch: [11][728/817]	Loss 0.0140 (0.0881)	
training:	Epoch: [11][729/817]	Loss 0.5600 (0.0888)	
training:	Epoch: [11][730/817]	Loss 0.0389 (0.0887)	
training:	Epoch: [11][731/817]	Loss 0.0535 (0.0887)	
training:	Epoch: [11][732/817]	Loss 0.0237 (0.0886)	
training:	Epoch: [11][733/817]	Loss 0.0321 (0.0885)	
training:	Epoch: [11][734/817]	Loss 0.0167 (0.0884)	
training:	Epoch: [11][735/817]	Loss 0.3789 (0.0888)	
training:	Epoch: [11][736/817]	Loss 0.0356 (0.0887)	
training:	Epoch: [11][737/817]	Loss 0.0210 (0.0886)	
training:	Epoch: [11][738/817]	Loss 0.0205 (0.0885)	
training:	Epoch: [11][739/817]	Loss 0.0713 (0.0885)	
training:	Epoch: [11][740/817]	Loss 0.0149 (0.0884)	
training:	Epoch: [11][741/817]	Loss 0.0217 (0.0883)	
training:	Epoch: [11][742/817]	Loss 0.0248 (0.0882)	
training:	Epoch: [11][743/817]	Loss 0.0168 (0.0882)	
training:	Epoch: [11][744/817]	Loss 0.0148 (0.0881)	
training:	Epoch: [11][745/817]	Loss 0.0539 (0.0880)	
training:	Epoch: [11][746/817]	Loss 0.0690 (0.0880)	
training:	Epoch: [11][747/817]	Loss 0.0237 (0.0879)	
training:	Epoch: [11][748/817]	Loss 0.4296 (0.0884)	
training:	Epoch: [11][749/817]	Loss 0.0184 (0.0883)	
training:	Epoch: [11][750/817]	Loss 0.0226 (0.0882)	
training:	Epoch: [11][751/817]	Loss 0.0218 (0.0881)	
training:	Epoch: [11][752/817]	Loss 0.1427 (0.0882)	
training:	Epoch: [11][753/817]	Loss 0.0418 (0.0881)	
training:	Epoch: [11][754/817]	Loss 0.0149 (0.0880)	
training:	Epoch: [11][755/817]	Loss 0.0263 (0.0879)	
training:	Epoch: [11][756/817]	Loss 0.0194 (0.0878)	
training:	Epoch: [11][757/817]	Loss 0.0264 (0.0877)	
training:	Epoch: [11][758/817]	Loss 0.0341 (0.0877)	
training:	Epoch: [11][759/817]	Loss 0.0316 (0.0876)	
training:	Epoch: [11][760/817]	Loss 0.0538 (0.0876)	
training:	Epoch: [11][761/817]	Loss 0.4043 (0.0880)	
training:	Epoch: [11][762/817]	Loss 0.0164 (0.0879)	
training:	Epoch: [11][763/817]	Loss 0.0182 (0.0878)	
training:	Epoch: [11][764/817]	Loss 0.0311 (0.0877)	
training:	Epoch: [11][765/817]	Loss 0.0480 (0.0877)	
training:	Epoch: [11][766/817]	Loss 0.0403 (0.0876)	
training:	Epoch: [11][767/817]	Loss 0.0174 (0.0875)	
training:	Epoch: [11][768/817]	Loss 0.0143 (0.0874)	
training:	Epoch: [11][769/817]	Loss 0.1005 (0.0874)	
training:	Epoch: [11][770/817]	Loss 0.0303 (0.0874)	
training:	Epoch: [11][771/817]	Loss 0.0156 (0.0873)	
training:	Epoch: [11][772/817]	Loss 0.0164 (0.0872)	
training:	Epoch: [11][773/817]	Loss 0.1035 (0.0872)	
training:	Epoch: [11][774/817]	Loss 0.0130 (0.0871)	
training:	Epoch: [11][775/817]	Loss 0.0183 (0.0870)	
training:	Epoch: [11][776/817]	Loss 0.1421 (0.0871)	
training:	Epoch: [11][777/817]	Loss 0.0184 (0.0870)	
training:	Epoch: [11][778/817]	Loss 0.0191 (0.0869)	
training:	Epoch: [11][779/817]	Loss 0.0255 (0.0868)	
training:	Epoch: [11][780/817]	Loss 0.0190 (0.0867)	
training:	Epoch: [11][781/817]	Loss 0.0189 (0.0866)	
training:	Epoch: [11][782/817]	Loss 0.0145 (0.0866)	
training:	Epoch: [11][783/817]	Loss 0.0310 (0.0865)	
training:	Epoch: [11][784/817]	Loss 0.1784 (0.0866)	
training:	Epoch: [11][785/817]	Loss 0.0190 (0.0865)	
training:	Epoch: [11][786/817]	Loss 0.0164 (0.0864)	
training:	Epoch: [11][787/817]	Loss 0.0168 (0.0863)	
training:	Epoch: [11][788/817]	Loss 0.0158 (0.0863)	
training:	Epoch: [11][789/817]	Loss 0.0159 (0.0862)	
training:	Epoch: [11][790/817]	Loss 0.0227 (0.0861)	
training:	Epoch: [11][791/817]	Loss 0.0535 (0.0860)	
training:	Epoch: [11][792/817]	Loss 0.5215 (0.0866)	
training:	Epoch: [11][793/817]	Loss 0.5690 (0.0872)	
training:	Epoch: [11][794/817]	Loss 0.0233 (0.0871)	
training:	Epoch: [11][795/817]	Loss 0.0172 (0.0870)	
training:	Epoch: [11][796/817]	Loss 0.6892 (0.0878)	
training:	Epoch: [11][797/817]	Loss 0.0271 (0.0877)	
training:	Epoch: [11][798/817]	Loss 0.0176 (0.0876)	
training:	Epoch: [11][799/817]	Loss 0.0156 (0.0875)	
training:	Epoch: [11][800/817]	Loss 0.0164 (0.0874)	
training:	Epoch: [11][801/817]	Loss 0.0167 (0.0874)	
training:	Epoch: [11][802/817]	Loss 0.0179 (0.0873)	
training:	Epoch: [11][803/817]	Loss 0.0753 (0.0873)	
training:	Epoch: [11][804/817]	Loss 0.0148 (0.0872)	
training:	Epoch: [11][805/817]	Loss 0.1136 (0.0872)	
training:	Epoch: [11][806/817]	Loss 0.0274 (0.0871)	
training:	Epoch: [11][807/817]	Loss 0.0147 (0.0870)	
training:	Epoch: [11][808/817]	Loss 0.0268 (0.0870)	
training:	Epoch: [11][809/817]	Loss 0.0150 (0.0869)	
training:	Epoch: [11][810/817]	Loss 0.0145 (0.0868)	
training:	Epoch: [11][811/817]	Loss 0.0171 (0.0867)	
training:	Epoch: [11][812/817]	Loss 0.0189 (0.0866)	
training:	Epoch: [11][813/817]	Loss 0.0429 (0.0866)	
training:	Epoch: [11][814/817]	Loss 0.0162 (0.0865)	
training:	Epoch: [11][815/817]	Loss 0.0148 (0.0864)	
training:	Epoch: [11][816/817]	Loss 0.0202 (0.0863)	
training:	Epoch: [11][817/817]	Loss 0.0162 (0.0862)	
Training:	 Loss: 0.0862

Training:	 ACC: 0.9861 0.9861 0.9847 0.9876
Validation:	 ACC: 0.7833 0.7844 0.8076 0.7590
Validation:	 Best_BACC: 0.8223 0.8218 0.8127 0.8318
Validation:	 Loss: 0.7560
Pretraining:	Epoch 12/200
----------
training:	Epoch: [12][1/817]	Loss 0.0151 (0.0151)	
training:	Epoch: [12][2/817]	Loss 0.1835 (0.0993)	
training:	Epoch: [12][3/817]	Loss 0.5066 (0.2351)	
training:	Epoch: [12][4/817]	Loss 0.0185 (0.1809)	
training:	Epoch: [12][5/817]	Loss 0.5154 (0.2478)	
training:	Epoch: [12][6/817]	Loss 0.0177 (0.2095)	
training:	Epoch: [12][7/817]	Loss 0.0163 (0.1819)	
training:	Epoch: [12][8/817]	Loss 0.1216 (0.1743)	
training:	Epoch: [12][9/817]	Loss 0.0223 (0.1575)	
training:	Epoch: [12][10/817]	Loss 0.0182 (0.1435)	
training:	Epoch: [12][11/817]	Loss 0.0364 (0.1338)	
training:	Epoch: [12][12/817]	Loss 0.0164 (0.1240)	
training:	Epoch: [12][13/817]	Loss 0.0306 (0.1168)	
training:	Epoch: [12][14/817]	Loss 0.0128 (0.1094)	
training:	Epoch: [12][15/817]	Loss 0.0179 (0.1033)	
training:	Epoch: [12][16/817]	Loss 0.0972 (0.1029)	
training:	Epoch: [12][17/817]	Loss 0.0535 (0.1000)	
training:	Epoch: [12][18/817]	Loss 0.0177 (0.0954)	
training:	Epoch: [12][19/817]	Loss 0.0369 (0.0924)	
training:	Epoch: [12][20/817]	Loss 0.0476 (0.0901)	
training:	Epoch: [12][21/817]	Loss 0.0130 (0.0864)	
training:	Epoch: [12][22/817]	Loss 0.0159 (0.0832)	
training:	Epoch: [12][23/817]	Loss 0.0187 (0.0804)	
training:	Epoch: [12][24/817]	Loss 0.5171 (0.0986)	
training:	Epoch: [12][25/817]	Loss 0.0154 (0.0953)	
training:	Epoch: [12][26/817]	Loss 0.0144 (0.0922)	
training:	Epoch: [12][27/817]	Loss 0.0619 (0.0911)	
training:	Epoch: [12][28/817]	Loss 0.0144 (0.0883)	
training:	Epoch: [12][29/817]	Loss 0.0157 (0.0858)	
training:	Epoch: [12][30/817]	Loss 0.5024 (0.0997)	
training:	Epoch: [12][31/817]	Loss 0.0204 (0.0972)	
training:	Epoch: [12][32/817]	Loss 0.0147 (0.0946)	
training:	Epoch: [12][33/817]	Loss 0.4894 (0.1065)	
training:	Epoch: [12][34/817]	Loss 0.0169 (0.1039)	
training:	Epoch: [12][35/817]	Loss 0.0194 (0.1015)	
training:	Epoch: [12][36/817]	Loss 0.0179 (0.0992)	
training:	Epoch: [12][37/817]	Loss 0.0197 (0.0970)	
training:	Epoch: [12][38/817]	Loss 0.0155 (0.0949)	
training:	Epoch: [12][39/817]	Loss 0.0152 (0.0928)	
training:	Epoch: [12][40/817]	Loss 0.0151 (0.0909)	
training:	Epoch: [12][41/817]	Loss 0.0175 (0.0891)	
training:	Epoch: [12][42/817]	Loss 0.0234 (0.0875)	
training:	Epoch: [12][43/817]	Loss 0.0200 (0.0860)	
training:	Epoch: [12][44/817]	Loss 0.0179 (0.0844)	
training:	Epoch: [12][45/817]	Loss 1.0671 (0.1063)	
training:	Epoch: [12][46/817]	Loss 0.0157 (0.1043)	
training:	Epoch: [12][47/817]	Loss 0.0200 (0.1025)	
training:	Epoch: [12][48/817]	Loss 0.0144 (0.1007)	
training:	Epoch: [12][49/817]	Loss 0.0167 (0.0989)	
training:	Epoch: [12][50/817]	Loss 0.0196 (0.0974)	
training:	Epoch: [12][51/817]	Loss 0.0181 (0.0958)	
training:	Epoch: [12][52/817]	Loss 0.0156 (0.0943)	
training:	Epoch: [12][53/817]	Loss 0.0213 (0.0929)	
training:	Epoch: [12][54/817]	Loss 0.0160 (0.0915)	
training:	Epoch: [12][55/817]	Loss 0.0175 (0.0901)	
training:	Epoch: [12][56/817]	Loss 0.0170 (0.0888)	
training:	Epoch: [12][57/817]	Loss 0.0200 (0.0876)	
training:	Epoch: [12][58/817]	Loss 0.3057 (0.0914)	
training:	Epoch: [12][59/817]	Loss 0.0177 (0.0901)	
training:	Epoch: [12][60/817]	Loss 0.0148 (0.0889)	
training:	Epoch: [12][61/817]	Loss 0.0164 (0.0877)	
training:	Epoch: [12][62/817]	Loss 0.0248 (0.0867)	
training:	Epoch: [12][63/817]	Loss 0.0153 (0.0855)	
training:	Epoch: [12][64/817]	Loss 0.0162 (0.0844)	
training:	Epoch: [12][65/817]	Loss 0.0173 (0.0834)	
training:	Epoch: [12][66/817]	Loss 0.0254 (0.0825)	
training:	Epoch: [12][67/817]	Loss 0.0152 (0.0815)	
training:	Epoch: [12][68/817]	Loss 0.0685 (0.0813)	
training:	Epoch: [12][69/817]	Loss 0.4618 (0.0868)	
training:	Epoch: [12][70/817]	Loss 0.0138 (0.0858)	
training:	Epoch: [12][71/817]	Loss 0.0139 (0.0848)	
training:	Epoch: [12][72/817]	Loss 0.0335 (0.0841)	
training:	Epoch: [12][73/817]	Loss 0.0527 (0.0836)	
training:	Epoch: [12][74/817]	Loss 0.0763 (0.0835)	
training:	Epoch: [12][75/817]	Loss 0.0172 (0.0827)	
training:	Epoch: [12][76/817]	Loss 0.0730 (0.0825)	
training:	Epoch: [12][77/817]	Loss 0.0245 (0.0818)	
training:	Epoch: [12][78/817]	Loss 0.0327 (0.0812)	
training:	Epoch: [12][79/817]	Loss 0.0169 (0.0803)	
training:	Epoch: [12][80/817]	Loss 0.0130 (0.0795)	
training:	Epoch: [12][81/817]	Loss 0.0168 (0.0787)	
training:	Epoch: [12][82/817]	Loss 0.0154 (0.0779)	
training:	Epoch: [12][83/817]	Loss 0.0183 (0.0772)	
training:	Epoch: [12][84/817]	Loss 0.0170 (0.0765)	
training:	Epoch: [12][85/817]	Loss 0.0182 (0.0758)	
training:	Epoch: [12][86/817]	Loss 0.0141 (0.0751)	
training:	Epoch: [12][87/817]	Loss 0.0580 (0.0749)	
training:	Epoch: [12][88/817]	Loss 0.0153 (0.0742)	
training:	Epoch: [12][89/817]	Loss 0.2157 (0.0758)	
training:	Epoch: [12][90/817]	Loss 0.0173 (0.0752)	
training:	Epoch: [12][91/817]	Loss 0.0177 (0.0745)	
training:	Epoch: [12][92/817]	Loss 0.0167 (0.0739)	
training:	Epoch: [12][93/817]	Loss 0.0217 (0.0734)	
training:	Epoch: [12][94/817]	Loss 0.0178 (0.0728)	
training:	Epoch: [12][95/817]	Loss 0.0194 (0.0722)	
training:	Epoch: [12][96/817]	Loss 0.0162 (0.0716)	
training:	Epoch: [12][97/817]	Loss 0.0154 (0.0710)	
training:	Epoch: [12][98/817]	Loss 0.0277 (0.0706)	
training:	Epoch: [12][99/817]	Loss 0.0146 (0.0700)	
training:	Epoch: [12][100/817]	Loss 0.0172 (0.0695)	
training:	Epoch: [12][101/817]	Loss 0.0799 (0.0696)	
training:	Epoch: [12][102/817]	Loss 0.0175 (0.0691)	
training:	Epoch: [12][103/817]	Loss 0.0164 (0.0686)	
training:	Epoch: [12][104/817]	Loss 0.0147 (0.0681)	
training:	Epoch: [12][105/817]	Loss 0.0946 (0.0683)	
training:	Epoch: [12][106/817]	Loss 0.0161 (0.0678)	
training:	Epoch: [12][107/817]	Loss 0.0160 (0.0673)	
training:	Epoch: [12][108/817]	Loss 0.5809 (0.0721)	
training:	Epoch: [12][109/817]	Loss 0.0916 (0.0723)	
training:	Epoch: [12][110/817]	Loss 0.5780 (0.0769)	
training:	Epoch: [12][111/817]	Loss 0.0135 (0.0763)	
training:	Epoch: [12][112/817]	Loss 0.0165 (0.0758)	
training:	Epoch: [12][113/817]	Loss 0.0166 (0.0752)	
training:	Epoch: [12][114/817]	Loss 0.0296 (0.0748)	
training:	Epoch: [12][115/817]	Loss 0.0154 (0.0743)	
training:	Epoch: [12][116/817]	Loss 0.0129 (0.0738)	
training:	Epoch: [12][117/817]	Loss 0.0139 (0.0733)	
training:	Epoch: [12][118/817]	Loss 0.0213 (0.0728)	
training:	Epoch: [12][119/817]	Loss 0.0171 (0.0724)	
training:	Epoch: [12][120/817]	Loss 0.0144 (0.0719)	
training:	Epoch: [12][121/817]	Loss 0.0238 (0.0715)	
training:	Epoch: [12][122/817]	Loss 0.0216 (0.0711)	
training:	Epoch: [12][123/817]	Loss 0.0136 (0.0706)	
training:	Epoch: [12][124/817]	Loss 0.0190 (0.0702)	
training:	Epoch: [12][125/817]	Loss 0.0172 (0.0698)	
training:	Epoch: [12][126/817]	Loss 0.0178 (0.0694)	
training:	Epoch: [12][127/817]	Loss 0.5622 (0.0732)	
training:	Epoch: [12][128/817]	Loss 0.0152 (0.0728)	
training:	Epoch: [12][129/817]	Loss 0.0324 (0.0725)	
training:	Epoch: [12][130/817]	Loss 0.0225 (0.0721)	
training:	Epoch: [12][131/817]	Loss 1.0741 (0.0797)	
training:	Epoch: [12][132/817]	Loss 0.0180 (0.0793)	
training:	Epoch: [12][133/817]	Loss 0.0191 (0.0788)	
training:	Epoch: [12][134/817]	Loss 0.0174 (0.0784)	
training:	Epoch: [12][135/817]	Loss 0.1276 (0.0787)	
training:	Epoch: [12][136/817]	Loss 0.0196 (0.0783)	
training:	Epoch: [12][137/817]	Loss 0.0337 (0.0780)	
training:	Epoch: [12][138/817]	Loss 0.0165 (0.0775)	
training:	Epoch: [12][139/817]	Loss 0.0154 (0.0771)	
training:	Epoch: [12][140/817]	Loss 0.0137 (0.0766)	
training:	Epoch: [12][141/817]	Loss 0.0189 (0.0762)	
training:	Epoch: [12][142/817]	Loss 0.0154 (0.0758)	
training:	Epoch: [12][143/817]	Loss 0.0215 (0.0754)	
training:	Epoch: [12][144/817]	Loss 0.0159 (0.0750)	
training:	Epoch: [12][145/817]	Loss 0.0184 (0.0746)	
training:	Epoch: [12][146/817]	Loss 0.0246 (0.0743)	
training:	Epoch: [12][147/817]	Loss 0.0185 (0.0739)	
training:	Epoch: [12][148/817]	Loss 0.0142 (0.0735)	
training:	Epoch: [12][149/817]	Loss 0.0154 (0.0731)	
training:	Epoch: [12][150/817]	Loss 0.0159 (0.0727)	
training:	Epoch: [12][151/817]	Loss 0.0153 (0.0723)	
training:	Epoch: [12][152/817]	Loss 0.0254 (0.0720)	
training:	Epoch: [12][153/817]	Loss 0.0159 (0.0717)	
training:	Epoch: [12][154/817]	Loss 0.0176 (0.0713)	
training:	Epoch: [12][155/817]	Loss 0.5750 (0.0745)	
training:	Epoch: [12][156/817]	Loss 0.6846 (0.0785)	
training:	Epoch: [12][157/817]	Loss 0.0161 (0.0781)	
training:	Epoch: [12][158/817]	Loss 0.0217 (0.0777)	
training:	Epoch: [12][159/817]	Loss 0.0167 (0.0773)	
training:	Epoch: [12][160/817]	Loss 0.5612 (0.0803)	
training:	Epoch: [12][161/817]	Loss 0.0172 (0.0800)	
training:	Epoch: [12][162/817]	Loss 0.0209 (0.0796)	
training:	Epoch: [12][163/817]	Loss 0.0181 (0.0792)	
training:	Epoch: [12][164/817]	Loss 0.0154 (0.0788)	
training:	Epoch: [12][165/817]	Loss 0.0185 (0.0785)	
training:	Epoch: [12][166/817]	Loss 0.0670 (0.0784)	
training:	Epoch: [12][167/817]	Loss 0.0161 (0.0780)	
training:	Epoch: [12][168/817]	Loss 0.0154 (0.0776)	
training:	Epoch: [12][169/817]	Loss 0.0236 (0.0773)	
training:	Epoch: [12][170/817]	Loss 0.0340 (0.0771)	
training:	Epoch: [12][171/817]	Loss 0.0221 (0.0767)	
training:	Epoch: [12][172/817]	Loss 0.0171 (0.0764)	
training:	Epoch: [12][173/817]	Loss 0.0150 (0.0760)	
training:	Epoch: [12][174/817]	Loss 0.0185 (0.0757)	
training:	Epoch: [12][175/817]	Loss 0.0147 (0.0754)	
training:	Epoch: [12][176/817]	Loss 0.0167 (0.0750)	
training:	Epoch: [12][177/817]	Loss 0.0177 (0.0747)	
training:	Epoch: [12][178/817]	Loss 0.0182 (0.0744)	
training:	Epoch: [12][179/817]	Loss 0.0142 (0.0741)	
training:	Epoch: [12][180/817]	Loss 0.5319 (0.0766)	
training:	Epoch: [12][181/817]	Loss 0.0161 (0.0763)	
training:	Epoch: [12][182/817]	Loss 0.0167 (0.0759)	
training:	Epoch: [12][183/817]	Loss 0.0148 (0.0756)	
training:	Epoch: [12][184/817]	Loss 0.0152 (0.0753)	
training:	Epoch: [12][185/817]	Loss 0.0201 (0.0750)	
training:	Epoch: [12][186/817]	Loss 0.0194 (0.0747)	
training:	Epoch: [12][187/817]	Loss 0.1677 (0.0752)	
training:	Epoch: [12][188/817]	Loss 0.0136 (0.0748)	
training:	Epoch: [12][189/817]	Loss 0.0160 (0.0745)	
training:	Epoch: [12][190/817]	Loss 0.0137 (0.0742)	
training:	Epoch: [12][191/817]	Loss 0.0156 (0.0739)	
training:	Epoch: [12][192/817]	Loss 0.0146 (0.0736)	
training:	Epoch: [12][193/817]	Loss 0.0148 (0.0733)	
training:	Epoch: [12][194/817]	Loss 0.0159 (0.0730)	
training:	Epoch: [12][195/817]	Loss 0.0212 (0.0727)	
training:	Epoch: [12][196/817]	Loss 0.0172 (0.0725)	
training:	Epoch: [12][197/817]	Loss 0.0169 (0.0722)	
training:	Epoch: [12][198/817]	Loss 0.0146 (0.0719)	
training:	Epoch: [12][199/817]	Loss 0.0430 (0.0717)	
training:	Epoch: [12][200/817]	Loss 0.0524 (0.0716)	
training:	Epoch: [12][201/817]	Loss 0.0195 (0.0714)	
training:	Epoch: [12][202/817]	Loss 0.0234 (0.0711)	
training:	Epoch: [12][203/817]	Loss 0.4000 (0.0728)	
training:	Epoch: [12][204/817]	Loss 0.0292 (0.0725)	
training:	Epoch: [12][205/817]	Loss 0.0160 (0.0723)	
training:	Epoch: [12][206/817]	Loss 0.0154 (0.0720)	
training:	Epoch: [12][207/817]	Loss 0.0395 (0.0718)	
training:	Epoch: [12][208/817]	Loss 0.0150 (0.0716)	
training:	Epoch: [12][209/817]	Loss 0.0161 (0.0713)	
training:	Epoch: [12][210/817]	Loss 0.5398 (0.0735)	
training:	Epoch: [12][211/817]	Loss 0.0150 (0.0733)	
training:	Epoch: [12][212/817]	Loss 0.0170 (0.0730)	
training:	Epoch: [12][213/817]	Loss 0.5646 (0.0753)	
training:	Epoch: [12][214/817]	Loss 0.0150 (0.0750)	
training:	Epoch: [12][215/817]	Loss 0.0189 (0.0748)	
training:	Epoch: [12][216/817]	Loss 0.0192 (0.0745)	
training:	Epoch: [12][217/817]	Loss 0.0154 (0.0742)	
training:	Epoch: [12][218/817]	Loss 0.0218 (0.0740)	
training:	Epoch: [12][219/817]	Loss 0.0212 (0.0737)	
training:	Epoch: [12][220/817]	Loss 0.0380 (0.0736)	
training:	Epoch: [12][221/817]	Loss 0.0264 (0.0734)	
training:	Epoch: [12][222/817]	Loss 0.0181 (0.0731)	
training:	Epoch: [12][223/817]	Loss 0.5336 (0.0752)	
training:	Epoch: [12][224/817]	Loss 0.0135 (0.0749)	
training:	Epoch: [12][225/817]	Loss 0.0287 (0.0747)	
training:	Epoch: [12][226/817]	Loss 0.0586 (0.0746)	
training:	Epoch: [12][227/817]	Loss 0.0154 (0.0744)	
training:	Epoch: [12][228/817]	Loss 0.0442 (0.0742)	
training:	Epoch: [12][229/817]	Loss 0.0177 (0.0740)	
training:	Epoch: [12][230/817]	Loss 0.0194 (0.0738)	
training:	Epoch: [12][231/817]	Loss 0.0144 (0.0735)	
training:	Epoch: [12][232/817]	Loss 0.0209 (0.0733)	
training:	Epoch: [12][233/817]	Loss 0.0155 (0.0730)	
training:	Epoch: [12][234/817]	Loss 0.0133 (0.0728)	
training:	Epoch: [12][235/817]	Loss 0.0414 (0.0726)	
training:	Epoch: [12][236/817]	Loss 0.0157 (0.0724)	
training:	Epoch: [12][237/817]	Loss 0.0622 (0.0723)	
training:	Epoch: [12][238/817]	Loss 0.0153 (0.0721)	
training:	Epoch: [12][239/817]	Loss 0.0140 (0.0719)	
training:	Epoch: [12][240/817]	Loss 0.0199 (0.0716)	
training:	Epoch: [12][241/817]	Loss 0.0252 (0.0715)	
training:	Epoch: [12][242/817]	Loss 0.0156 (0.0712)	
training:	Epoch: [12][243/817]	Loss 0.0173 (0.0710)	
training:	Epoch: [12][244/817]	Loss 0.0456 (0.0709)	
training:	Epoch: [12][245/817]	Loss 0.0179 (0.0707)	
training:	Epoch: [12][246/817]	Loss 0.0154 (0.0705)	
training:	Epoch: [12][247/817]	Loss 0.0143 (0.0702)	
training:	Epoch: [12][248/817]	Loss 0.0149 (0.0700)	
training:	Epoch: [12][249/817]	Loss 0.0137 (0.0698)	
training:	Epoch: [12][250/817]	Loss 0.1977 (0.0703)	
training:	Epoch: [12][251/817]	Loss 0.0153 (0.0701)	
training:	Epoch: [12][252/817]	Loss 0.0125 (0.0698)	
training:	Epoch: [12][253/817]	Loss 0.0163 (0.0696)	
training:	Epoch: [12][254/817]	Loss 0.0321 (0.0695)	
training:	Epoch: [12][255/817]	Loss 0.0190 (0.0693)	
training:	Epoch: [12][256/817]	Loss 0.0196 (0.0691)	
training:	Epoch: [12][257/817]	Loss 0.0153 (0.0689)	
training:	Epoch: [12][258/817]	Loss 0.0198 (0.0687)	
training:	Epoch: [12][259/817]	Loss 0.0489 (0.0686)	
training:	Epoch: [12][260/817]	Loss 0.0141 (0.0684)	
training:	Epoch: [12][261/817]	Loss 0.0140 (0.0682)	
training:	Epoch: [12][262/817]	Loss 0.0125 (0.0680)	
training:	Epoch: [12][263/817]	Loss 0.0157 (0.0678)	
training:	Epoch: [12][264/817]	Loss 0.0151 (0.0676)	
training:	Epoch: [12][265/817]	Loss 0.0180 (0.0674)	
training:	Epoch: [12][266/817]	Loss 0.0134 (0.0672)	
training:	Epoch: [12][267/817]	Loss 0.0159 (0.0670)	
training:	Epoch: [12][268/817]	Loss 0.6704 (0.0693)	
training:	Epoch: [12][269/817]	Loss 0.0131 (0.0691)	
training:	Epoch: [12][270/817]	Loss 0.0171 (0.0689)	
training:	Epoch: [12][271/817]	Loss 0.4551 (0.0703)	
training:	Epoch: [12][272/817]	Loss 0.0177 (0.0701)	
training:	Epoch: [12][273/817]	Loss 0.0167 (0.0699)	
training:	Epoch: [12][274/817]	Loss 0.0157 (0.0697)	
training:	Epoch: [12][275/817]	Loss 0.1965 (0.0702)	
training:	Epoch: [12][276/817]	Loss 0.0136 (0.0700)	
training:	Epoch: [12][277/817]	Loss 0.0148 (0.0698)	
training:	Epoch: [12][278/817]	Loss 0.0152 (0.0696)	
training:	Epoch: [12][279/817]	Loss 0.0183 (0.0694)	
training:	Epoch: [12][280/817]	Loss 0.0147 (0.0692)	
training:	Epoch: [12][281/817]	Loss 0.1271 (0.0694)	
training:	Epoch: [12][282/817]	Loss 0.0152 (0.0692)	
training:	Epoch: [12][283/817]	Loss 0.4678 (0.0706)	
training:	Epoch: [12][284/817]	Loss 0.0194 (0.0704)	
training:	Epoch: [12][285/817]	Loss 0.2082 (0.0709)	
training:	Epoch: [12][286/817]	Loss 0.0184 (0.0707)	
training:	Epoch: [12][287/817]	Loss 0.4179 (0.0719)	
training:	Epoch: [12][288/817]	Loss 0.0167 (0.0717)	
training:	Epoch: [12][289/817]	Loss 0.0139 (0.0715)	
training:	Epoch: [12][290/817]	Loss 0.0159 (0.0713)	
training:	Epoch: [12][291/817]	Loss 0.0148 (0.0712)	
training:	Epoch: [12][292/817]	Loss 0.0160 (0.0710)	
training:	Epoch: [12][293/817]	Loss 0.0204 (0.0708)	
training:	Epoch: [12][294/817]	Loss 0.5736 (0.0725)	
training:	Epoch: [12][295/817]	Loss 0.0131 (0.0723)	
training:	Epoch: [12][296/817]	Loss 0.0201 (0.0721)	
training:	Epoch: [12][297/817]	Loss 0.0142 (0.0719)	
training:	Epoch: [12][298/817]	Loss 0.0152 (0.0717)	
training:	Epoch: [12][299/817]	Loss 0.0819 (0.0718)	
training:	Epoch: [12][300/817]	Loss 0.0130 (0.0716)	
training:	Epoch: [12][301/817]	Loss 0.0132 (0.0714)	
training:	Epoch: [12][302/817]	Loss 0.0136 (0.0712)	
training:	Epoch: [12][303/817]	Loss 0.0589 (0.0711)	
training:	Epoch: [12][304/817]	Loss 0.0160 (0.0710)	
training:	Epoch: [12][305/817]	Loss 0.2709 (0.0716)	
training:	Epoch: [12][306/817]	Loss 0.0142 (0.0714)	
training:	Epoch: [12][307/817]	Loss 0.0296 (0.0713)	
training:	Epoch: [12][308/817]	Loss 0.0551 (0.0712)	
training:	Epoch: [12][309/817]	Loss 0.0238 (0.0711)	
training:	Epoch: [12][310/817]	Loss 0.0156 (0.0709)	
training:	Epoch: [12][311/817]	Loss 0.0150 (0.0707)	
training:	Epoch: [12][312/817]	Loss 0.0173 (0.0706)	
training:	Epoch: [12][313/817]	Loss 0.0172 (0.0704)	
training:	Epoch: [12][314/817]	Loss 1.1573 (0.0739)	
training:	Epoch: [12][315/817]	Loss 0.0139 (0.0737)	
training:	Epoch: [12][316/817]	Loss 0.6775 (0.0756)	
training:	Epoch: [12][317/817]	Loss 0.0204 (0.0754)	
training:	Epoch: [12][318/817]	Loss 0.0134 (0.0752)	
training:	Epoch: [12][319/817]	Loss 0.0148 (0.0750)	
training:	Epoch: [12][320/817]	Loss 0.0547 (0.0750)	
training:	Epoch: [12][321/817]	Loss 0.0247 (0.0748)	
training:	Epoch: [12][322/817]	Loss 0.2340 (0.0753)	
training:	Epoch: [12][323/817]	Loss 0.0394 (0.0752)	
training:	Epoch: [12][324/817]	Loss 0.0168 (0.0750)	
training:	Epoch: [12][325/817]	Loss 0.0186 (0.0748)	
training:	Epoch: [12][326/817]	Loss 0.0242 (0.0747)	
training:	Epoch: [12][327/817]	Loss 0.0158 (0.0745)	
training:	Epoch: [12][328/817]	Loss 0.0146 (0.0743)	
training:	Epoch: [12][329/817]	Loss 0.0187 (0.0741)	
training:	Epoch: [12][330/817]	Loss 0.5343 (0.0755)	
training:	Epoch: [12][331/817]	Loss 0.0167 (0.0754)	
training:	Epoch: [12][332/817]	Loss 0.2367 (0.0758)	
training:	Epoch: [12][333/817]	Loss 0.0634 (0.0758)	
training:	Epoch: [12][334/817]	Loss 0.1387 (0.0760)	
training:	Epoch: [12][335/817]	Loss 0.0408 (0.0759)	
training:	Epoch: [12][336/817]	Loss 0.0135 (0.0757)	
training:	Epoch: [12][337/817]	Loss 0.0182 (0.0755)	
training:	Epoch: [12][338/817]	Loss 0.0175 (0.0754)	
training:	Epoch: [12][339/817]	Loss 0.0151 (0.0752)	
training:	Epoch: [12][340/817]	Loss 0.0387 (0.0751)	
training:	Epoch: [12][341/817]	Loss 0.0182 (0.0749)	
training:	Epoch: [12][342/817]	Loss 0.0148 (0.0747)	
training:	Epoch: [12][343/817]	Loss 0.0179 (0.0746)	
training:	Epoch: [12][344/817]	Loss 0.0156 (0.0744)	
training:	Epoch: [12][345/817]	Loss 0.0190 (0.0742)	
training:	Epoch: [12][346/817]	Loss 0.0252 (0.0741)	
training:	Epoch: [12][347/817]	Loss 0.0142 (0.0739)	
training:	Epoch: [12][348/817]	Loss 0.0190 (0.0738)	
training:	Epoch: [12][349/817]	Loss 0.0148 (0.0736)	
training:	Epoch: [12][350/817]	Loss 0.0171 (0.0734)	
training:	Epoch: [12][351/817]	Loss 0.0141 (0.0733)	
training:	Epoch: [12][352/817]	Loss 0.0160 (0.0731)	
training:	Epoch: [12][353/817]	Loss 0.5795 (0.0745)	
training:	Epoch: [12][354/817]	Loss 0.0417 (0.0744)	
training:	Epoch: [12][355/817]	Loss 0.4965 (0.0756)	
training:	Epoch: [12][356/817]	Loss 0.0228 (0.0755)	
training:	Epoch: [12][357/817]	Loss 0.4765 (0.0766)	
training:	Epoch: [12][358/817]	Loss 0.0210 (0.0765)	
training:	Epoch: [12][359/817]	Loss 0.0136 (0.0763)	
training:	Epoch: [12][360/817]	Loss 0.0151 (0.0761)	
training:	Epoch: [12][361/817]	Loss 0.0182 (0.0759)	
training:	Epoch: [12][362/817]	Loss 0.0760 (0.0759)	
training:	Epoch: [12][363/817]	Loss 0.0136 (0.0758)	
training:	Epoch: [12][364/817]	Loss 0.0543 (0.0757)	
training:	Epoch: [12][365/817]	Loss 0.0162 (0.0756)	
training:	Epoch: [12][366/817]	Loss 0.0172 (0.0754)	
training:	Epoch: [12][367/817]	Loss 0.0515 (0.0753)	
training:	Epoch: [12][368/817]	Loss 0.0181 (0.0752)	
training:	Epoch: [12][369/817]	Loss 0.0183 (0.0750)	
training:	Epoch: [12][370/817]	Loss 0.1791 (0.0753)	
training:	Epoch: [12][371/817]	Loss 0.0139 (0.0751)	
training:	Epoch: [12][372/817]	Loss 0.0152 (0.0750)	
training:	Epoch: [12][373/817]	Loss 0.0159 (0.0748)	
training:	Epoch: [12][374/817]	Loss 0.0154 (0.0747)	
training:	Epoch: [12][375/817]	Loss 0.0165 (0.0745)	
training:	Epoch: [12][376/817]	Loss 0.0156 (0.0743)	
training:	Epoch: [12][377/817]	Loss 0.0150 (0.0742)	
training:	Epoch: [12][378/817]	Loss 0.0271 (0.0741)	
training:	Epoch: [12][379/817]	Loss 0.0160 (0.0739)	
training:	Epoch: [12][380/817]	Loss 0.0232 (0.0738)	
training:	Epoch: [12][381/817]	Loss 0.5905 (0.0751)	
training:	Epoch: [12][382/817]	Loss 0.0148 (0.0750)	
training:	Epoch: [12][383/817]	Loss 0.0152 (0.0748)	
training:	Epoch: [12][384/817]	Loss 0.0165 (0.0747)	
training:	Epoch: [12][385/817]	Loss 0.0137 (0.0745)	
training:	Epoch: [12][386/817]	Loss 0.0187 (0.0744)	
training:	Epoch: [12][387/817]	Loss 0.1021 (0.0744)	
training:	Epoch: [12][388/817]	Loss 0.2104 (0.0748)	
training:	Epoch: [12][389/817]	Loss 0.0177 (0.0746)	
training:	Epoch: [12][390/817]	Loss 0.0148 (0.0745)	
training:	Epoch: [12][391/817]	Loss 0.0145 (0.0743)	
training:	Epoch: [12][392/817]	Loss 0.0168 (0.0742)	
training:	Epoch: [12][393/817]	Loss 0.0295 (0.0741)	
training:	Epoch: [12][394/817]	Loss 0.0169 (0.0739)	
training:	Epoch: [12][395/817]	Loss 0.0147 (0.0738)	
training:	Epoch: [12][396/817]	Loss 0.0138 (0.0736)	
training:	Epoch: [12][397/817]	Loss 0.0178 (0.0735)	
training:	Epoch: [12][398/817]	Loss 0.0156 (0.0733)	
training:	Epoch: [12][399/817]	Loss 0.0137 (0.0732)	
training:	Epoch: [12][400/817]	Loss 0.0157 (0.0730)	
training:	Epoch: [12][401/817]	Loss 0.5025 (0.0741)	
training:	Epoch: [12][402/817]	Loss 0.0204 (0.0740)	
training:	Epoch: [12][403/817]	Loss 0.6724 (0.0755)	
training:	Epoch: [12][404/817]	Loss 0.4858 (0.0765)	
training:	Epoch: [12][405/817]	Loss 0.5710 (0.0777)	
training:	Epoch: [12][406/817]	Loss 0.0118 (0.0775)	
training:	Epoch: [12][407/817]	Loss 0.0234 (0.0774)	
training:	Epoch: [12][408/817]	Loss 0.0134 (0.0773)	
training:	Epoch: [12][409/817]	Loss 0.0136 (0.0771)	
training:	Epoch: [12][410/817]	Loss 0.0126 (0.0769)	
training:	Epoch: [12][411/817]	Loss 0.0164 (0.0768)	
training:	Epoch: [12][412/817]	Loss 0.3518 (0.0775)	
training:	Epoch: [12][413/817]	Loss 0.0307 (0.0773)	
training:	Epoch: [12][414/817]	Loss 0.0185 (0.0772)	
training:	Epoch: [12][415/817]	Loss 0.0165 (0.0771)	
training:	Epoch: [12][416/817]	Loss 0.0141 (0.0769)	
training:	Epoch: [12][417/817]	Loss 0.3504 (0.0776)	
training:	Epoch: [12][418/817]	Loss 0.0135 (0.0774)	
training:	Epoch: [12][419/817]	Loss 0.0254 (0.0773)	
training:	Epoch: [12][420/817]	Loss 0.0367 (0.0772)	
training:	Epoch: [12][421/817]	Loss 0.6598 (0.0786)	
training:	Epoch: [12][422/817]	Loss 0.0198 (0.0784)	
training:	Epoch: [12][423/817]	Loss 0.0140 (0.0783)	
training:	Epoch: [12][424/817]	Loss 0.0168 (0.0781)	
training:	Epoch: [12][425/817]	Loss 0.0194 (0.0780)	
training:	Epoch: [12][426/817]	Loss 0.0162 (0.0779)	
training:	Epoch: [12][427/817]	Loss 0.0145 (0.0777)	
training:	Epoch: [12][428/817]	Loss 0.0236 (0.0776)	
training:	Epoch: [12][429/817]	Loss 0.0204 (0.0774)	
training:	Epoch: [12][430/817]	Loss 0.1127 (0.0775)	
training:	Epoch: [12][431/817]	Loss 0.5814 (0.0787)	
training:	Epoch: [12][432/817]	Loss 0.0146 (0.0785)	
training:	Epoch: [12][433/817]	Loss 0.0233 (0.0784)	
training:	Epoch: [12][434/817]	Loss 0.0157 (0.0783)	
training:	Epoch: [12][435/817]	Loss 0.0206 (0.0781)	
training:	Epoch: [12][436/817]	Loss 0.3385 (0.0787)	
training:	Epoch: [12][437/817]	Loss 0.5323 (0.0798)	
training:	Epoch: [12][438/817]	Loss 0.0122 (0.0796)	
training:	Epoch: [12][439/817]	Loss 0.0241 (0.0795)	
training:	Epoch: [12][440/817]	Loss 0.0161 (0.0794)	
training:	Epoch: [12][441/817]	Loss 0.0156 (0.0792)	
training:	Epoch: [12][442/817]	Loss 0.0147 (0.0791)	
training:	Epoch: [12][443/817]	Loss 0.0293 (0.0789)	
training:	Epoch: [12][444/817]	Loss 0.0138 (0.0788)	
training:	Epoch: [12][445/817]	Loss 0.0181 (0.0787)	
training:	Epoch: [12][446/817]	Loss 0.0144 (0.0785)	
training:	Epoch: [12][447/817]	Loss 0.0219 (0.0784)	
training:	Epoch: [12][448/817]	Loss 0.0150 (0.0783)	
training:	Epoch: [12][449/817]	Loss 0.0136 (0.0781)	
training:	Epoch: [12][450/817]	Loss 0.0188 (0.0780)	
training:	Epoch: [12][451/817]	Loss 0.7206 (0.0794)	
training:	Epoch: [12][452/817]	Loss 0.0145 (0.0793)	
training:	Epoch: [12][453/817]	Loss 0.0158 (0.0791)	
training:	Epoch: [12][454/817]	Loss 0.0125 (0.0790)	
training:	Epoch: [12][455/817]	Loss 0.0288 (0.0789)	
training:	Epoch: [12][456/817]	Loss 0.0287 (0.0788)	
training:	Epoch: [12][457/817]	Loss 0.0287 (0.0786)	
training:	Epoch: [12][458/817]	Loss 0.0144 (0.0785)	
training:	Epoch: [12][459/817]	Loss 0.0165 (0.0784)	
training:	Epoch: [12][460/817]	Loss 0.0317 (0.0783)	
training:	Epoch: [12][461/817]	Loss 0.0152 (0.0781)	
training:	Epoch: [12][462/817]	Loss 0.0135 (0.0780)	
training:	Epoch: [12][463/817]	Loss 0.5284 (0.0790)	
training:	Epoch: [12][464/817]	Loss 0.5530 (0.0800)	
training:	Epoch: [12][465/817]	Loss 0.0176 (0.0798)	
training:	Epoch: [12][466/817]	Loss 0.0143 (0.0797)	
training:	Epoch: [12][467/817]	Loss 0.0182 (0.0796)	
training:	Epoch: [12][468/817]	Loss 0.0173 (0.0794)	
training:	Epoch: [12][469/817]	Loss 0.0149 (0.0793)	
training:	Epoch: [12][470/817]	Loss 0.5330 (0.0803)	
training:	Epoch: [12][471/817]	Loss 0.1061 (0.0803)	
training:	Epoch: [12][472/817]	Loss 0.0134 (0.0802)	
training:	Epoch: [12][473/817]	Loss 0.0348 (0.0801)	
training:	Epoch: [12][474/817]	Loss 0.0201 (0.0800)	
training:	Epoch: [12][475/817]	Loss 0.0161 (0.0798)	
training:	Epoch: [12][476/817]	Loss 0.0226 (0.0797)	
training:	Epoch: [12][477/817]	Loss 0.0160 (0.0796)	
training:	Epoch: [12][478/817]	Loss 0.0162 (0.0794)	
training:	Epoch: [12][479/817]	Loss 0.0169 (0.0793)	
training:	Epoch: [12][480/817]	Loss 0.0184 (0.0792)	
training:	Epoch: [12][481/817]	Loss 0.1440 (0.0793)	
training:	Epoch: [12][482/817]	Loss 0.0138 (0.0792)	
training:	Epoch: [12][483/817]	Loss 0.0174 (0.0791)	
training:	Epoch: [12][484/817]	Loss 0.0170 (0.0789)	
training:	Epoch: [12][485/817]	Loss 0.0167 (0.0788)	
training:	Epoch: [12][486/817]	Loss 0.0188 (0.0787)	
training:	Epoch: [12][487/817]	Loss 0.0132 (0.0785)	
training:	Epoch: [12][488/817]	Loss 0.0151 (0.0784)	
training:	Epoch: [12][489/817]	Loss 0.0206 (0.0783)	
training:	Epoch: [12][490/817]	Loss 0.3725 (0.0789)	
training:	Epoch: [12][491/817]	Loss 0.0315 (0.0788)	
training:	Epoch: [12][492/817]	Loss 0.0149 (0.0787)	
training:	Epoch: [12][493/817]	Loss 0.0146 (0.0785)	
training:	Epoch: [12][494/817]	Loss 0.1749 (0.0787)	
training:	Epoch: [12][495/817]	Loss 0.0230 (0.0786)	
training:	Epoch: [12][496/817]	Loss 0.5104 (0.0795)	
training:	Epoch: [12][497/817]	Loss 0.0416 (0.0794)	
training:	Epoch: [12][498/817]	Loss 0.3501 (0.0800)	
training:	Epoch: [12][499/817]	Loss 0.6528 (0.0811)	
training:	Epoch: [12][500/817]	Loss 0.0148 (0.0810)	
training:	Epoch: [12][501/817]	Loss 0.0133 (0.0808)	
training:	Epoch: [12][502/817]	Loss 0.1051 (0.0809)	
training:	Epoch: [12][503/817]	Loss 0.0166 (0.0808)	
training:	Epoch: [12][504/817]	Loss 0.0172 (0.0806)	
training:	Epoch: [12][505/817]	Loss 0.0137 (0.0805)	
training:	Epoch: [12][506/817]	Loss 0.0129 (0.0804)	
training:	Epoch: [12][507/817]	Loss 0.0227 (0.0803)	
training:	Epoch: [12][508/817]	Loss 0.5373 (0.0812)	
training:	Epoch: [12][509/817]	Loss 0.5068 (0.0820)	
training:	Epoch: [12][510/817]	Loss 0.0145 (0.0819)	
training:	Epoch: [12][511/817]	Loss 0.0193 (0.0817)	
training:	Epoch: [12][512/817]	Loss 0.5114 (0.0826)	
training:	Epoch: [12][513/817]	Loss 0.5047 (0.0834)	
training:	Epoch: [12][514/817]	Loss 0.0230 (0.0833)	
training:	Epoch: [12][515/817]	Loss 0.5563 (0.0842)	
training:	Epoch: [12][516/817]	Loss 0.0277 (0.0841)	
training:	Epoch: [12][517/817]	Loss 0.0348 (0.0840)	
training:	Epoch: [12][518/817]	Loss 0.0387 (0.0839)	
training:	Epoch: [12][519/817]	Loss 0.0243 (0.0838)	
training:	Epoch: [12][520/817]	Loss 0.0141 (0.0837)	
training:	Epoch: [12][521/817]	Loss 0.0223 (0.0835)	
training:	Epoch: [12][522/817]	Loss 0.0177 (0.0834)	
training:	Epoch: [12][523/817]	Loss 0.0773 (0.0834)	
training:	Epoch: [12][524/817]	Loss 0.4709 (0.0841)	
training:	Epoch: [12][525/817]	Loss 0.0601 (0.0841)	
training:	Epoch: [12][526/817]	Loss 0.0168 (0.0840)	
training:	Epoch: [12][527/817]	Loss 0.0152 (0.0838)	
training:	Epoch: [12][528/817]	Loss 0.0244 (0.0837)	
training:	Epoch: [12][529/817]	Loss 0.0157 (0.0836)	
training:	Epoch: [12][530/817]	Loss 0.0139 (0.0835)	
training:	Epoch: [12][531/817]	Loss 0.0151 (0.0833)	
training:	Epoch: [12][532/817]	Loss 0.0159 (0.0832)	
training:	Epoch: [12][533/817]	Loss 0.0365 (0.0831)	
training:	Epoch: [12][534/817]	Loss 0.0163 (0.0830)	
training:	Epoch: [12][535/817]	Loss 0.7345 (0.0842)	
training:	Epoch: [12][536/817]	Loss 0.0147 (0.0841)	
training:	Epoch: [12][537/817]	Loss 0.0168 (0.0840)	
training:	Epoch: [12][538/817]	Loss 0.0193 (0.0838)	
training:	Epoch: [12][539/817]	Loss 0.0194 (0.0837)	
training:	Epoch: [12][540/817]	Loss 0.0155 (0.0836)	
training:	Epoch: [12][541/817]	Loss 0.0170 (0.0835)	
training:	Epoch: [12][542/817]	Loss 0.0132 (0.0833)	
training:	Epoch: [12][543/817]	Loss 0.0404 (0.0833)	
training:	Epoch: [12][544/817]	Loss 0.0185 (0.0831)	
training:	Epoch: [12][545/817]	Loss 0.0141 (0.0830)	
training:	Epoch: [12][546/817]	Loss 0.0192 (0.0829)	
training:	Epoch: [12][547/817]	Loss 0.0558 (0.0828)	
training:	Epoch: [12][548/817]	Loss 0.0621 (0.0828)	
training:	Epoch: [12][549/817]	Loss 0.3444 (0.0833)	
training:	Epoch: [12][550/817]	Loss 0.0157 (0.0832)	
training:	Epoch: [12][551/817]	Loss 0.0170 (0.0830)	
training:	Epoch: [12][552/817]	Loss 0.0155 (0.0829)	
training:	Epoch: [12][553/817]	Loss 0.2096 (0.0831)	
training:	Epoch: [12][554/817]	Loss 0.0197 (0.0830)	
training:	Epoch: [12][555/817]	Loss 0.0316 (0.0829)	
training:	Epoch: [12][556/817]	Loss 0.0184 (0.0828)	
training:	Epoch: [12][557/817]	Loss 0.0167 (0.0827)	
training:	Epoch: [12][558/817]	Loss 0.0201 (0.0826)	
training:	Epoch: [12][559/817]	Loss 0.0183 (0.0825)	
training:	Epoch: [12][560/817]	Loss 0.0309 (0.0824)	
training:	Epoch: [12][561/817]	Loss 0.0145 (0.0823)	
training:	Epoch: [12][562/817]	Loss 0.0154 (0.0821)	
training:	Epoch: [12][563/817]	Loss 0.0222 (0.0820)	
training:	Epoch: [12][564/817]	Loss 0.0180 (0.0819)	
training:	Epoch: [12][565/817]	Loss 0.0128 (0.0818)	
training:	Epoch: [12][566/817]	Loss 0.0281 (0.0817)	
training:	Epoch: [12][567/817]	Loss 0.0203 (0.0816)	
training:	Epoch: [12][568/817]	Loss 0.0170 (0.0815)	
training:	Epoch: [12][569/817]	Loss 0.0145 (0.0814)	
training:	Epoch: [12][570/817]	Loss 0.0163 (0.0813)	
training:	Epoch: [12][571/817]	Loss 0.0256 (0.0812)	
training:	Epoch: [12][572/817]	Loss 0.0159 (0.0810)	
training:	Epoch: [12][573/817]	Loss 0.0144 (0.0809)	
training:	Epoch: [12][574/817]	Loss 0.0152 (0.0808)	
training:	Epoch: [12][575/817]	Loss 0.0167 (0.0807)	
training:	Epoch: [12][576/817]	Loss 0.0448 (0.0806)	
training:	Epoch: [12][577/817]	Loss 0.0155 (0.0805)	
training:	Epoch: [12][578/817]	Loss 0.0134 (0.0804)	
training:	Epoch: [12][579/817]	Loss 0.0163 (0.0803)	
training:	Epoch: [12][580/817]	Loss 0.0183 (0.0802)	
training:	Epoch: [12][581/817]	Loss 0.1829 (0.0804)	
training:	Epoch: [12][582/817]	Loss 0.0188 (0.0803)	
training:	Epoch: [12][583/817]	Loss 0.0501 (0.0802)	
training:	Epoch: [12][584/817]	Loss 0.5301 (0.0810)	
training:	Epoch: [12][585/817]	Loss 0.5907 (0.0819)	
training:	Epoch: [12][586/817]	Loss 0.0174 (0.0817)	
training:	Epoch: [12][587/817]	Loss 0.0145 (0.0816)	
training:	Epoch: [12][588/817]	Loss 0.0199 (0.0815)	
training:	Epoch: [12][589/817]	Loss 0.0143 (0.0814)	
training:	Epoch: [12][590/817]	Loss 0.0167 (0.0813)	
training:	Epoch: [12][591/817]	Loss 0.0164 (0.0812)	
training:	Epoch: [12][592/817]	Loss 0.0244 (0.0811)	
training:	Epoch: [12][593/817]	Loss 0.0175 (0.0810)	
training:	Epoch: [12][594/817]	Loss 0.0346 (0.0809)	
training:	Epoch: [12][595/817]	Loss 0.2250 (0.0812)	
training:	Epoch: [12][596/817]	Loss 0.0209 (0.0811)	
training:	Epoch: [12][597/817]	Loss 0.0147 (0.0809)	
training:	Epoch: [12][598/817]	Loss 0.0175 (0.0808)	
training:	Epoch: [12][599/817]	Loss 0.0252 (0.0807)	
training:	Epoch: [12][600/817]	Loss 0.0212 (0.0806)	
training:	Epoch: [12][601/817]	Loss 0.0177 (0.0805)	
training:	Epoch: [12][602/817]	Loss 0.0166 (0.0804)	
training:	Epoch: [12][603/817]	Loss 0.0271 (0.0803)	
training:	Epoch: [12][604/817]	Loss 0.3682 (0.0808)	
training:	Epoch: [12][605/817]	Loss 0.0277 (0.0807)	
training:	Epoch: [12][606/817]	Loss 0.5652 (0.0815)	
training:	Epoch: [12][607/817]	Loss 0.0193 (0.0814)	
training:	Epoch: [12][608/817]	Loss 0.0130 (0.0813)	
training:	Epoch: [12][609/817]	Loss 0.0150 (0.0812)	
training:	Epoch: [12][610/817]	Loss 0.0238 (0.0811)	
training:	Epoch: [12][611/817]	Loss 0.0151 (0.0810)	
training:	Epoch: [12][612/817]	Loss 0.0226 (0.0809)	
training:	Epoch: [12][613/817]	Loss 0.2351 (0.0812)	
training:	Epoch: [12][614/817]	Loss 0.0226 (0.0811)	
training:	Epoch: [12][615/817]	Loss 0.0179 (0.0810)	
training:	Epoch: [12][616/817]	Loss 0.0296 (0.0809)	
training:	Epoch: [12][617/817]	Loss 0.0141 (0.0808)	
training:	Epoch: [12][618/817]	Loss 0.0133 (0.0807)	
training:	Epoch: [12][619/817]	Loss 0.0168 (0.0806)	
training:	Epoch: [12][620/817]	Loss 0.0151 (0.0805)	
training:	Epoch: [12][621/817]	Loss 0.0160 (0.0803)	
training:	Epoch: [12][622/817]	Loss 0.0113 (0.0802)	
training:	Epoch: [12][623/817]	Loss 0.0160 (0.0801)	
training:	Epoch: [12][624/817]	Loss 0.0164 (0.0800)	
training:	Epoch: [12][625/817]	Loss 0.0199 (0.0799)	
training:	Epoch: [12][626/817]	Loss 0.0206 (0.0798)	
training:	Epoch: [12][627/817]	Loss 0.0454 (0.0798)	
training:	Epoch: [12][628/817]	Loss 0.0321 (0.0797)	
training:	Epoch: [12][629/817]	Loss 0.0145 (0.0796)	
training:	Epoch: [12][630/817]	Loss 0.0241 (0.0795)	
training:	Epoch: [12][631/817]	Loss 0.0138 (0.0794)	
training:	Epoch: [12][632/817]	Loss 0.0145 (0.0793)	
training:	Epoch: [12][633/817]	Loss 0.0248 (0.0792)	
training:	Epoch: [12][634/817]	Loss 0.0138 (0.0791)	
training:	Epoch: [12][635/817]	Loss 0.0140 (0.0790)	
training:	Epoch: [12][636/817]	Loss 0.0139 (0.0789)	
training:	Epoch: [12][637/817]	Loss 0.0160 (0.0788)	
training:	Epoch: [12][638/817]	Loss 0.0557 (0.0788)	
training:	Epoch: [12][639/817]	Loss 0.0240 (0.0787)	
training:	Epoch: [12][640/817]	Loss 0.0635 (0.0787)	
training:	Epoch: [12][641/817]	Loss 0.0215 (0.0786)	
training:	Epoch: [12][642/817]	Loss 0.0178 (0.0785)	
training:	Epoch: [12][643/817]	Loss 0.0173 (0.0784)	
training:	Epoch: [12][644/817]	Loss 0.0130 (0.0783)	
training:	Epoch: [12][645/817]	Loss 0.0169 (0.0782)	
training:	Epoch: [12][646/817]	Loss 0.0131 (0.0781)	
training:	Epoch: [12][647/817]	Loss 0.6402 (0.0790)	
training:	Epoch: [12][648/817]	Loss 0.0193 (0.0789)	
training:	Epoch: [12][649/817]	Loss 0.0144 (0.0788)	
training:	Epoch: [12][650/817]	Loss 0.0704 (0.0788)	
training:	Epoch: [12][651/817]	Loss 0.5364 (0.0795)	
training:	Epoch: [12][652/817]	Loss 0.0221 (0.0794)	
training:	Epoch: [12][653/817]	Loss 0.0177 (0.0793)	
training:	Epoch: [12][654/817]	Loss 0.0179 (0.0792)	
training:	Epoch: [12][655/817]	Loss 0.1294 (0.0793)	
training:	Epoch: [12][656/817]	Loss 0.0199 (0.0792)	
training:	Epoch: [12][657/817]	Loss 0.0170 (0.0791)	
training:	Epoch: [12][658/817]	Loss 0.0134 (0.0790)	
training:	Epoch: [12][659/817]	Loss 0.5157 (0.0796)	
training:	Epoch: [12][660/817]	Loss 0.0168 (0.0795)	
training:	Epoch: [12][661/817]	Loss 0.0339 (0.0795)	
training:	Epoch: [12][662/817]	Loss 0.0160 (0.0794)	
training:	Epoch: [12][663/817]	Loss 0.0149 (0.0793)	
training:	Epoch: [12][664/817]	Loss 0.0152 (0.0792)	
training:	Epoch: [12][665/817]	Loss 0.0136 (0.0791)	
training:	Epoch: [12][666/817]	Loss 0.0145 (0.0790)	
training:	Epoch: [12][667/817]	Loss 0.0171 (0.0789)	
training:	Epoch: [12][668/817]	Loss 0.0218 (0.0788)	
training:	Epoch: [12][669/817]	Loss 0.0456 (0.0788)	
training:	Epoch: [12][670/817]	Loss 0.0261 (0.0787)	
training:	Epoch: [12][671/817]	Loss 0.0568 (0.0787)	
training:	Epoch: [12][672/817]	Loss 0.0142 (0.0786)	
training:	Epoch: [12][673/817]	Loss 0.0169 (0.0785)	
training:	Epoch: [12][674/817]	Loss 0.0137 (0.0784)	
training:	Epoch: [12][675/817]	Loss 0.0155 (0.0783)	
training:	Epoch: [12][676/817]	Loss 0.5108 (0.0789)	
training:	Epoch: [12][677/817]	Loss 0.0139 (0.0788)	
training:	Epoch: [12][678/817]	Loss 0.0199 (0.0787)	
training:	Epoch: [12][679/817]	Loss 0.0209 (0.0786)	
training:	Epoch: [12][680/817]	Loss 0.0236 (0.0786)	
training:	Epoch: [12][681/817]	Loss 0.0148 (0.0785)	
training:	Epoch: [12][682/817]	Loss 0.0200 (0.0784)	
training:	Epoch: [12][683/817]	Loss 0.0146 (0.0783)	
training:	Epoch: [12][684/817]	Loss 0.0235 (0.0782)	
training:	Epoch: [12][685/817]	Loss 0.0174 (0.0781)	
training:	Epoch: [12][686/817]	Loss 0.5170 (0.0788)	
training:	Epoch: [12][687/817]	Loss 0.6517 (0.0796)	
training:	Epoch: [12][688/817]	Loss 0.0287 (0.0795)	
training:	Epoch: [12][689/817]	Loss 0.6792 (0.0804)	
training:	Epoch: [12][690/817]	Loss 0.0153 (0.0803)	
training:	Epoch: [12][691/817]	Loss 0.0223 (0.0802)	
training:	Epoch: [12][692/817]	Loss 0.0152 (0.0801)	
training:	Epoch: [12][693/817]	Loss 0.0212 (0.0800)	
training:	Epoch: [12][694/817]	Loss 0.1699 (0.0802)	
training:	Epoch: [12][695/817]	Loss 0.0194 (0.0801)	
training:	Epoch: [12][696/817]	Loss 0.0169 (0.0800)	
training:	Epoch: [12][697/817]	Loss 0.0212 (0.0799)	
training:	Epoch: [12][698/817]	Loss 0.0171 (0.0798)	
training:	Epoch: [12][699/817]	Loss 0.0150 (0.0797)	
training:	Epoch: [12][700/817]	Loss 0.0145 (0.0796)	
training:	Epoch: [12][701/817]	Loss 0.0142 (0.0795)	
training:	Epoch: [12][702/817]	Loss 0.0252 (0.0795)	
training:	Epoch: [12][703/817]	Loss 0.5106 (0.0801)	
training:	Epoch: [12][704/817]	Loss 0.0151 (0.0800)	
training:	Epoch: [12][705/817]	Loss 0.0153 (0.0799)	
training:	Epoch: [12][706/817]	Loss 0.0166 (0.0798)	
training:	Epoch: [12][707/817]	Loss 0.0269 (0.0797)	
training:	Epoch: [12][708/817]	Loss 0.0151 (0.0796)	
training:	Epoch: [12][709/817]	Loss 0.0158 (0.0795)	
training:	Epoch: [12][710/817]	Loss 0.0137 (0.0795)	
training:	Epoch: [12][711/817]	Loss 0.0301 (0.0794)	
training:	Epoch: [12][712/817]	Loss 0.2567 (0.0796)	
training:	Epoch: [12][713/817]	Loss 0.0131 (0.0795)	
training:	Epoch: [12][714/817]	Loss 0.0162 (0.0794)	
training:	Epoch: [12][715/817]	Loss 0.1782 (0.0796)	
training:	Epoch: [12][716/817]	Loss 0.0693 (0.0796)	
training:	Epoch: [12][717/817]	Loss 0.0206 (0.0795)	
training:	Epoch: [12][718/817]	Loss 0.0215 (0.0794)	
training:	Epoch: [12][719/817]	Loss 0.0645 (0.0794)	
training:	Epoch: [12][720/817]	Loss 0.0225 (0.0793)	
training:	Epoch: [12][721/817]	Loss 0.0168 (0.0792)	
training:	Epoch: [12][722/817]	Loss 0.0141 (0.0791)	
training:	Epoch: [12][723/817]	Loss 0.0179 (0.0790)	
training:	Epoch: [12][724/817]	Loss 0.2021 (0.0792)	
training:	Epoch: [12][725/817]	Loss 0.0136 (0.0791)	
training:	Epoch: [12][726/817]	Loss 0.0155 (0.0790)	
training:	Epoch: [12][727/817]	Loss 0.0136 (0.0790)	
training:	Epoch: [12][728/817]	Loss 0.0170 (0.0789)	
training:	Epoch: [12][729/817]	Loss 0.0163 (0.0788)	
training:	Epoch: [12][730/817]	Loss 0.4221 (0.0792)	
training:	Epoch: [12][731/817]	Loss 0.0154 (0.0792)	
training:	Epoch: [12][732/817]	Loss 0.0142 (0.0791)	
training:	Epoch: [12][733/817]	Loss 0.0140 (0.0790)	
training:	Epoch: [12][734/817]	Loss 0.0147 (0.0789)	
training:	Epoch: [12][735/817]	Loss 0.0142 (0.0788)	
training:	Epoch: [12][736/817]	Loss 0.0218 (0.0787)	
training:	Epoch: [12][737/817]	Loss 0.0115 (0.0786)	
training:	Epoch: [12][738/817]	Loss 0.0148 (0.0786)	
training:	Epoch: [12][739/817]	Loss 0.0254 (0.0785)	
training:	Epoch: [12][740/817]	Loss 0.0144 (0.0784)	
training:	Epoch: [12][741/817]	Loss 0.0453 (0.0784)	
training:	Epoch: [12][742/817]	Loss 0.0181 (0.0783)	
training:	Epoch: [12][743/817]	Loss 0.0179 (0.0782)	
training:	Epoch: [12][744/817]	Loss 0.0174 (0.0781)	
training:	Epoch: [12][745/817]	Loss 0.0136 (0.0780)	
training:	Epoch: [12][746/817]	Loss 0.0204 (0.0779)	
training:	Epoch: [12][747/817]	Loss 0.0230 (0.0779)	
training:	Epoch: [12][748/817]	Loss 0.5770 (0.0785)	
training:	Epoch: [12][749/817]	Loss 0.0145 (0.0785)	
training:	Epoch: [12][750/817]	Loss 0.0151 (0.0784)	
training:	Epoch: [12][751/817]	Loss 0.0151 (0.0783)	
training:	Epoch: [12][752/817]	Loss 0.0132 (0.0782)	
training:	Epoch: [12][753/817]	Loss 0.0180 (0.0781)	
training:	Epoch: [12][754/817]	Loss 0.0339 (0.0781)	
training:	Epoch: [12][755/817]	Loss 0.0151 (0.0780)	
training:	Epoch: [12][756/817]	Loss 0.0171 (0.0779)	
training:	Epoch: [12][757/817]	Loss 0.0156 (0.0778)	
training:	Epoch: [12][758/817]	Loss 0.0153 (0.0777)	
training:	Epoch: [12][759/817]	Loss 0.0135 (0.0776)	
training:	Epoch: [12][760/817]	Loss 0.0167 (0.0776)	
training:	Epoch: [12][761/817]	Loss 0.0152 (0.0775)	
training:	Epoch: [12][762/817]	Loss 0.0155 (0.0774)	
training:	Epoch: [12][763/817]	Loss 0.0148 (0.0773)	
training:	Epoch: [12][764/817]	Loss 0.0163 (0.0772)	
training:	Epoch: [12][765/817]	Loss 0.0143 (0.0772)	
training:	Epoch: [12][766/817]	Loss 0.0149 (0.0771)	
training:	Epoch: [12][767/817]	Loss 0.0155 (0.0770)	
training:	Epoch: [12][768/817]	Loss 0.0176 (0.0769)	
training:	Epoch: [12][769/817]	Loss 0.0208 (0.0768)	
training:	Epoch: [12][770/817]	Loss 0.1074 (0.0769)	
training:	Epoch: [12][771/817]	Loss 0.0161 (0.0768)	
training:	Epoch: [12][772/817]	Loss 0.0148 (0.0767)	
training:	Epoch: [12][773/817]	Loss 0.0148 (0.0766)	
training:	Epoch: [12][774/817]	Loss 0.0130 (0.0766)	
training:	Epoch: [12][775/817]	Loss 0.0352 (0.0765)	
training:	Epoch: [12][776/817]	Loss 0.0163 (0.0764)	
training:	Epoch: [12][777/817]	Loss 0.2643 (0.0767)	
training:	Epoch: [12][778/817]	Loss 0.0132 (0.0766)	
training:	Epoch: [12][779/817]	Loss 0.0173 (0.0765)	
training:	Epoch: [12][780/817]	Loss 0.0181 (0.0764)	
training:	Epoch: [12][781/817]	Loss 0.0134 (0.0764)	
training:	Epoch: [12][782/817]	Loss 0.0149 (0.0763)	
training:	Epoch: [12][783/817]	Loss 0.0161 (0.0762)	
training:	Epoch: [12][784/817]	Loss 0.0243 (0.0761)	
training:	Epoch: [12][785/817]	Loss 0.0451 (0.0761)	
training:	Epoch: [12][786/817]	Loss 0.0179 (0.0760)	
training:	Epoch: [12][787/817]	Loss 0.1160 (0.0761)	
training:	Epoch: [12][788/817]	Loss 0.0128 (0.0760)	
training:	Epoch: [12][789/817]	Loss 0.1888 (0.0761)	
training:	Epoch: [12][790/817]	Loss 0.0208 (0.0761)	
training:	Epoch: [12][791/817]	Loss 0.3635 (0.0764)	
training:	Epoch: [12][792/817]	Loss 0.0166 (0.0764)	
training:	Epoch: [12][793/817]	Loss 0.0140 (0.0763)	
training:	Epoch: [12][794/817]	Loss 0.0202 (0.0762)	
training:	Epoch: [12][795/817]	Loss 0.9796 (0.0773)	
training:	Epoch: [12][796/817]	Loss 0.0321 (0.0773)	
training:	Epoch: [12][797/817]	Loss 0.0156 (0.0772)	
training:	Epoch: [12][798/817]	Loss 0.0137 (0.0771)	
training:	Epoch: [12][799/817]	Loss 0.0170 (0.0771)	
training:	Epoch: [12][800/817]	Loss 0.0168 (0.0770)	
training:	Epoch: [12][801/817]	Loss 0.0230 (0.0769)	
training:	Epoch: [12][802/817]	Loss 0.0155 (0.0768)	
training:	Epoch: [12][803/817]	Loss 0.0153 (0.0768)	
training:	Epoch: [12][804/817]	Loss 0.0152 (0.0767)	
training:	Epoch: [12][805/817]	Loss 0.0157 (0.0766)	
training:	Epoch: [12][806/817]	Loss 0.0156 (0.0765)	
training:	Epoch: [12][807/817]	Loss 0.0194 (0.0765)	
training:	Epoch: [12][808/817]	Loss 0.0140 (0.0764)	
training:	Epoch: [12][809/817]	Loss 0.0138 (0.0763)	
training:	Epoch: [12][810/817]	Loss 0.0223 (0.0762)	
training:	Epoch: [12][811/817]	Loss 0.0301 (0.0762)	
training:	Epoch: [12][812/817]	Loss 0.0191 (0.0761)	
training:	Epoch: [12][813/817]	Loss 0.0140 (0.0760)	
training:	Epoch: [12][814/817]	Loss 0.0149 (0.0760)	
training:	Epoch: [12][815/817]	Loss 0.4386 (0.0764)	
training:	Epoch: [12][816/817]	Loss 0.6264 (0.0771)	
training:	Epoch: [12][817/817]	Loss 0.0128 (0.0770)	
Training:	 Loss: 0.0770

Training:	 ACC: 0.9837 0.9833 0.9753 0.9920
Validation:	 ACC: 0.7888 0.7871 0.7513 0.8262
Validation:	 Best_BACC: 0.8223 0.8218 0.8127 0.8318
Validation:	 Loss: 0.7479
Pretraining:	Epoch 13/200
----------
training:	Epoch: [13][1/817]	Loss 0.0494 (0.0494)	
training:	Epoch: [13][2/817]	Loss 0.5769 (0.3132)	
training:	Epoch: [13][3/817]	Loss 0.5908 (0.4057)	
training:	Epoch: [13][4/817]	Loss 0.0432 (0.3151)	
training:	Epoch: [13][5/817]	Loss 0.0215 (0.2564)	
training:	Epoch: [13][6/817]	Loss 0.0173 (0.2165)	
training:	Epoch: [13][7/817]	Loss 0.0150 (0.1877)	
training:	Epoch: [13][8/817]	Loss 0.0148 (0.1661)	
training:	Epoch: [13][9/817]	Loss 0.0176 (0.1496)	
training:	Epoch: [13][10/817]	Loss 0.0125 (0.1359)	
training:	Epoch: [13][11/817]	Loss 0.0242 (0.1257)	
training:	Epoch: [13][12/817]	Loss 0.0146 (0.1165)	
training:	Epoch: [13][13/817]	Loss 0.0157 (0.1087)	
training:	Epoch: [13][14/817]	Loss 0.0168 (0.1022)	
training:	Epoch: [13][15/817]	Loss 0.0373 (0.0978)	
training:	Epoch: [13][16/817]	Loss 0.0177 (0.0928)	
training:	Epoch: [13][17/817]	Loss 0.0142 (0.0882)	
training:	Epoch: [13][18/817]	Loss 0.0133 (0.0840)	
training:	Epoch: [13][19/817]	Loss 0.0154 (0.0804)	
training:	Epoch: [13][20/817]	Loss 0.5382 (0.1033)	
training:	Epoch: [13][21/817]	Loss 0.1623 (0.1061)	
training:	Epoch: [13][22/817]	Loss 0.0133 (0.1019)	
training:	Epoch: [13][23/817]	Loss 0.0141 (0.0981)	
training:	Epoch: [13][24/817]	Loss 0.0247 (0.0950)	
training:	Epoch: [13][25/817]	Loss 0.0138 (0.0918)	
training:	Epoch: [13][26/817]	Loss 0.0146 (0.0888)	
training:	Epoch: [13][27/817]	Loss 0.0143 (0.0861)	
training:	Epoch: [13][28/817]	Loss 0.0149 (0.0835)	
training:	Epoch: [13][29/817]	Loss 0.5397 (0.0992)	
training:	Epoch: [13][30/817]	Loss 0.0149 (0.0964)	
training:	Epoch: [13][31/817]	Loss 0.0162 (0.0938)	
training:	Epoch: [13][32/817]	Loss 0.5850 (0.1092)	
training:	Epoch: [13][33/817]	Loss 0.0152 (0.1063)	
training:	Epoch: [13][34/817]	Loss 0.5520 (0.1195)	
training:	Epoch: [13][35/817]	Loss 0.0169 (0.1165)	
training:	Epoch: [13][36/817]	Loss 0.0174 (0.1138)	
training:	Epoch: [13][37/817]	Loss 0.0172 (0.1112)	
training:	Epoch: [13][38/817]	Loss 0.0591 (0.1098)	
training:	Epoch: [13][39/817]	Loss 0.0170 (0.1074)	
training:	Epoch: [13][40/817]	Loss 0.0156 (0.1051)	
training:	Epoch: [13][41/817]	Loss 0.0153 (0.1029)	
training:	Epoch: [13][42/817]	Loss 0.2227 (0.1058)	
training:	Epoch: [13][43/817]	Loss 0.0140 (0.1036)	
training:	Epoch: [13][44/817]	Loss 0.0149 (0.1016)	
training:	Epoch: [13][45/817]	Loss 0.0142 (0.0997)	
training:	Epoch: [13][46/817]	Loss 0.0187 (0.0979)	
training:	Epoch: [13][47/817]	Loss 0.0148 (0.0962)	
training:	Epoch: [13][48/817]	Loss 0.0133 (0.0944)	
training:	Epoch: [13][49/817]	Loss 0.0154 (0.0928)	
training:	Epoch: [13][50/817]	Loss 0.0226 (0.0914)	
training:	Epoch: [13][51/817]	Loss 0.0192 (0.0900)	
training:	Epoch: [13][52/817]	Loss 0.4993 (0.0979)	
training:	Epoch: [13][53/817]	Loss 0.0163 (0.0963)	
training:	Epoch: [13][54/817]	Loss 0.0148 (0.0948)	
training:	Epoch: [13][55/817]	Loss 0.0765 (0.0945)	
training:	Epoch: [13][56/817]	Loss 0.5175 (0.1020)	
training:	Epoch: [13][57/817]	Loss 0.0148 (0.1005)	
training:	Epoch: [13][58/817]	Loss 0.0159 (0.0991)	
training:	Epoch: [13][59/817]	Loss 0.0218 (0.0977)	
training:	Epoch: [13][60/817]	Loss 0.0204 (0.0965)	
training:	Epoch: [13][61/817]	Loss 0.0284 (0.0953)	
training:	Epoch: [13][62/817]	Loss 0.0383 (0.0944)	
training:	Epoch: [13][63/817]	Loss 0.0175 (0.0932)	
training:	Epoch: [13][64/817]	Loss 0.0148 (0.0920)	
training:	Epoch: [13][65/817]	Loss 0.5776 (0.0994)	
training:	Epoch: [13][66/817]	Loss 0.0148 (0.0982)	
training:	Epoch: [13][67/817]	Loss 0.0136 (0.0969)	
training:	Epoch: [13][68/817]	Loss 0.5191 (0.1031)	
training:	Epoch: [13][69/817]	Loss 0.0146 (0.1018)	
training:	Epoch: [13][70/817]	Loss 0.5624 (0.1084)	
training:	Epoch: [13][71/817]	Loss 0.1195 (0.1086)	
training:	Epoch: [13][72/817]	Loss 0.1280 (0.1088)	
training:	Epoch: [13][73/817]	Loss 0.0803 (0.1084)	
training:	Epoch: [13][74/817]	Loss 0.5378 (0.1142)	
training:	Epoch: [13][75/817]	Loss 0.0146 (0.1129)	
training:	Epoch: [13][76/817]	Loss 0.0136 (0.1116)	
training:	Epoch: [13][77/817]	Loss 0.0209 (0.1104)	
training:	Epoch: [13][78/817]	Loss 0.0517 (0.1097)	
training:	Epoch: [13][79/817]	Loss 0.0127 (0.1085)	
training:	Epoch: [13][80/817]	Loss 0.0167 (0.1073)	
training:	Epoch: [13][81/817]	Loss 0.0261 (0.1063)	
training:	Epoch: [13][82/817]	Loss 0.0184 (0.1052)	
training:	Epoch: [13][83/817]	Loss 0.0317 (0.1043)	
training:	Epoch: [13][84/817]	Loss 0.0355 (0.1035)	
training:	Epoch: [13][85/817]	Loss 0.0280 (0.1026)	
training:	Epoch: [13][86/817]	Loss 0.0158 (0.1016)	
training:	Epoch: [13][87/817]	Loss 0.0130 (0.1006)	
training:	Epoch: [13][88/817]	Loss 0.0166 (0.0997)	
training:	Epoch: [13][89/817]	Loss 0.0277 (0.0988)	
training:	Epoch: [13][90/817]	Loss 0.3543 (0.1017)	
training:	Epoch: [13][91/817]	Loss 0.0147 (0.1007)	
training:	Epoch: [13][92/817]	Loss 0.0159 (0.0998)	
training:	Epoch: [13][93/817]	Loss 0.0135 (0.0989)	
training:	Epoch: [13][94/817]	Loss 0.0141 (0.0980)	
training:	Epoch: [13][95/817]	Loss 0.5746 (0.1030)	
training:	Epoch: [13][96/817]	Loss 0.0260 (0.1022)	
training:	Epoch: [13][97/817]	Loss 0.2504 (0.1037)	
training:	Epoch: [13][98/817]	Loss 0.0160 (0.1028)	
training:	Epoch: [13][99/817]	Loss 0.0125 (0.1019)	
training:	Epoch: [13][100/817]	Loss 0.0138 (0.1010)	
training:	Epoch: [13][101/817]	Loss 0.0123 (0.1002)	
training:	Epoch: [13][102/817]	Loss 0.0146 (0.0993)	
training:	Epoch: [13][103/817]	Loss 0.0139 (0.0985)	
training:	Epoch: [13][104/817]	Loss 0.0305 (0.0978)	
training:	Epoch: [13][105/817]	Loss 0.0193 (0.0971)	
training:	Epoch: [13][106/817]	Loss 0.0504 (0.0966)	
training:	Epoch: [13][107/817]	Loss 0.0134 (0.0959)	
training:	Epoch: [13][108/817]	Loss 0.0115 (0.0951)	
training:	Epoch: [13][109/817]	Loss 0.0376 (0.0946)	
training:	Epoch: [13][110/817]	Loss 0.0167 (0.0938)	
training:	Epoch: [13][111/817]	Loss 0.0162 (0.0931)	
training:	Epoch: [13][112/817]	Loss 0.5145 (0.0969)	
training:	Epoch: [13][113/817]	Loss 0.0192 (0.0962)	
training:	Epoch: [13][114/817]	Loss 0.0162 (0.0955)	
training:	Epoch: [13][115/817]	Loss 0.0152 (0.0948)	
training:	Epoch: [13][116/817]	Loss 0.0157 (0.0941)	
training:	Epoch: [13][117/817]	Loss 0.0123 (0.0934)	
training:	Epoch: [13][118/817]	Loss 0.0151 (0.0928)	
training:	Epoch: [13][119/817]	Loss 0.1137 (0.0930)	
training:	Epoch: [13][120/817]	Loss 0.0421 (0.0925)	
training:	Epoch: [13][121/817]	Loss 0.0196 (0.0919)	
training:	Epoch: [13][122/817]	Loss 0.5851 (0.0960)	
training:	Epoch: [13][123/817]	Loss 0.1708 (0.0966)	
training:	Epoch: [13][124/817]	Loss 0.0132 (0.0959)	
training:	Epoch: [13][125/817]	Loss 0.0116 (0.0952)	
training:	Epoch: [13][126/817]	Loss 0.2541 (0.0965)	
training:	Epoch: [13][127/817]	Loss 0.0161 (0.0959)	
training:	Epoch: [13][128/817]	Loss 0.0174 (0.0952)	
training:	Epoch: [13][129/817]	Loss 0.0268 (0.0947)	
training:	Epoch: [13][130/817]	Loss 0.0833 (0.0946)	
training:	Epoch: [13][131/817]	Loss 0.0120 (0.0940)	
training:	Epoch: [13][132/817]	Loss 0.0134 (0.0934)	
training:	Epoch: [13][133/817]	Loss 1.1393 (0.1013)	
training:	Epoch: [13][134/817]	Loss 0.0138 (0.1006)	
training:	Epoch: [13][135/817]	Loss 0.0166 (0.1000)	
training:	Epoch: [13][136/817]	Loss 0.0146 (0.0993)	
training:	Epoch: [13][137/817]	Loss 0.0173 (0.0987)	
training:	Epoch: [13][138/817]	Loss 0.0243 (0.0982)	
training:	Epoch: [13][139/817]	Loss 0.0349 (0.0978)	
training:	Epoch: [13][140/817]	Loss 0.0212 (0.0972)	
training:	Epoch: [13][141/817]	Loss 0.0380 (0.0968)	
training:	Epoch: [13][142/817]	Loss 0.0145 (0.0962)	
training:	Epoch: [13][143/817]	Loss 0.5365 (0.0993)	
training:	Epoch: [13][144/817]	Loss 0.0177 (0.0987)	
training:	Epoch: [13][145/817]	Loss 0.0185 (0.0982)	
training:	Epoch: [13][146/817]	Loss 0.0155 (0.0976)	
training:	Epoch: [13][147/817]	Loss 0.0138 (0.0970)	
training:	Epoch: [13][148/817]	Loss 0.0121 (0.0965)	
training:	Epoch: [13][149/817]	Loss 0.0119 (0.0959)	
training:	Epoch: [13][150/817]	Loss 0.0161 (0.0954)	
training:	Epoch: [13][151/817]	Loss 0.0155 (0.0948)	
training:	Epoch: [13][152/817]	Loss 0.0168 (0.0943)	
training:	Epoch: [13][153/817]	Loss 0.0153 (0.0938)	
training:	Epoch: [13][154/817]	Loss 0.0145 (0.0933)	
training:	Epoch: [13][155/817]	Loss 0.0141 (0.0928)	
training:	Epoch: [13][156/817]	Loss 0.0166 (0.0923)	
training:	Epoch: [13][157/817]	Loss 0.0133 (0.0918)	
training:	Epoch: [13][158/817]	Loss 0.0145 (0.0913)	
training:	Epoch: [13][159/817]	Loss 0.0145 (0.0908)	
training:	Epoch: [13][160/817]	Loss 0.0190 (0.0904)	
training:	Epoch: [13][161/817]	Loss 0.0150 (0.0899)	
training:	Epoch: [13][162/817]	Loss 0.0132 (0.0894)	
training:	Epoch: [13][163/817]	Loss 0.0233 (0.0890)	
training:	Epoch: [13][164/817]	Loss 0.0347 (0.0887)	
training:	Epoch: [13][165/817]	Loss 0.0149 (0.0882)	
training:	Epoch: [13][166/817]	Loss 0.1184 (0.0884)	
training:	Epoch: [13][167/817]	Loss 0.0194 (0.0880)	
training:	Epoch: [13][168/817]	Loss 0.0140 (0.0876)	
training:	Epoch: [13][169/817]	Loss 0.0209 (0.0872)	
training:	Epoch: [13][170/817]	Loss 0.0161 (0.0868)	
training:	Epoch: [13][171/817]	Loss 0.0137 (0.0863)	
training:	Epoch: [13][172/817]	Loss 0.0141 (0.0859)	
training:	Epoch: [13][173/817]	Loss 0.0192 (0.0855)	
training:	Epoch: [13][174/817]	Loss 0.0141 (0.0851)	
training:	Epoch: [13][175/817]	Loss 0.0190 (0.0847)	
training:	Epoch: [13][176/817]	Loss 0.0213 (0.0844)	
training:	Epoch: [13][177/817]	Loss 0.5210 (0.0868)	
training:	Epoch: [13][178/817]	Loss 0.0143 (0.0864)	
training:	Epoch: [13][179/817]	Loss 0.0135 (0.0860)	
training:	Epoch: [13][180/817]	Loss 0.0134 (0.0856)	
training:	Epoch: [13][181/817]	Loss 0.0127 (0.0852)	
training:	Epoch: [13][182/817]	Loss 0.0136 (0.0848)	
training:	Epoch: [13][183/817]	Loss 0.2411 (0.0857)	
training:	Epoch: [13][184/817]	Loss 0.0163 (0.0853)	
training:	Epoch: [13][185/817]	Loss 0.0157 (0.0849)	
training:	Epoch: [13][186/817]	Loss 0.1644 (0.0854)	
training:	Epoch: [13][187/817]	Loss 0.5741 (0.0880)	
training:	Epoch: [13][188/817]	Loss 0.0137 (0.0876)	
training:	Epoch: [13][189/817]	Loss 0.0128 (0.0872)	
training:	Epoch: [13][190/817]	Loss 0.0141 (0.0868)	
training:	Epoch: [13][191/817]	Loss 0.0138 (0.0864)	
training:	Epoch: [13][192/817]	Loss 0.0153 (0.0860)	
training:	Epoch: [13][193/817]	Loss 0.0146 (0.0857)	
training:	Epoch: [13][194/817]	Loss 0.0166 (0.0853)	
training:	Epoch: [13][195/817]	Loss 0.0134 (0.0849)	
training:	Epoch: [13][196/817]	Loss 0.0147 (0.0846)	
training:	Epoch: [13][197/817]	Loss 0.0502 (0.0844)	
training:	Epoch: [13][198/817]	Loss 0.0149 (0.0841)	
training:	Epoch: [13][199/817]	Loss 0.0151 (0.0837)	
training:	Epoch: [13][200/817]	Loss 0.0150 (0.0834)	
training:	Epoch: [13][201/817]	Loss 0.0243 (0.0831)	
training:	Epoch: [13][202/817]	Loss 0.0155 (0.0827)	
training:	Epoch: [13][203/817]	Loss 0.0154 (0.0824)	
training:	Epoch: [13][204/817]	Loss 0.0169 (0.0821)	
training:	Epoch: [13][205/817]	Loss 0.0126 (0.0817)	
training:	Epoch: [13][206/817]	Loss 0.0163 (0.0814)	
training:	Epoch: [13][207/817]	Loss 0.0179 (0.0811)	
training:	Epoch: [13][208/817]	Loss 0.0128 (0.0808)	
training:	Epoch: [13][209/817]	Loss 0.0156 (0.0805)	
training:	Epoch: [13][210/817]	Loss 0.1834 (0.0810)	
training:	Epoch: [13][211/817]	Loss 0.5808 (0.0833)	
training:	Epoch: [13][212/817]	Loss 0.0121 (0.0830)	
training:	Epoch: [13][213/817]	Loss 0.0230 (0.0827)	
training:	Epoch: [13][214/817]	Loss 0.0140 (0.0824)	
training:	Epoch: [13][215/817]	Loss 0.0231 (0.0821)	
training:	Epoch: [13][216/817]	Loss 0.0156 (0.0818)	
training:	Epoch: [13][217/817]	Loss 0.0162 (0.0815)	
training:	Epoch: [13][218/817]	Loss 0.5658 (0.0837)	
training:	Epoch: [13][219/817]	Loss 0.0171 (0.0834)	
training:	Epoch: [13][220/817]	Loss 0.0167 (0.0831)	
training:	Epoch: [13][221/817]	Loss 0.0139 (0.0828)	
training:	Epoch: [13][222/817]	Loss 0.0122 (0.0825)	
training:	Epoch: [13][223/817]	Loss 0.0156 (0.0822)	
training:	Epoch: [13][224/817]	Loss 0.0133 (0.0819)	
training:	Epoch: [13][225/817]	Loss 0.0146 (0.0816)	
training:	Epoch: [13][226/817]	Loss 0.0183 (0.0813)	
training:	Epoch: [13][227/817]	Loss 0.0261 (0.0811)	
training:	Epoch: [13][228/817]	Loss 0.0128 (0.0808)	
training:	Epoch: [13][229/817]	Loss 0.5612 (0.0829)	
training:	Epoch: [13][230/817]	Loss 0.0165 (0.0826)	
training:	Epoch: [13][231/817]	Loss 0.0172 (0.0823)	
training:	Epoch: [13][232/817]	Loss 0.5704 (0.0844)	
training:	Epoch: [13][233/817]	Loss 0.0138 (0.0841)	
training:	Epoch: [13][234/817]	Loss 0.0157 (0.0838)	
training:	Epoch: [13][235/817]	Loss 0.0136 (0.0835)	
training:	Epoch: [13][236/817]	Loss 0.0160 (0.0832)	
training:	Epoch: [13][237/817]	Loss 0.0229 (0.0830)	
training:	Epoch: [13][238/817]	Loss 0.0211 (0.0827)	
training:	Epoch: [13][239/817]	Loss 0.0126 (0.0824)	
training:	Epoch: [13][240/817]	Loss 0.0170 (0.0821)	
training:	Epoch: [13][241/817]	Loss 0.0219 (0.0819)	
training:	Epoch: [13][242/817]	Loss 0.5398 (0.0838)	
training:	Epoch: [13][243/817]	Loss 0.0180 (0.0835)	
training:	Epoch: [13][244/817]	Loss 0.0148 (0.0832)	
training:	Epoch: [13][245/817]	Loss 0.0155 (0.0830)	
training:	Epoch: [13][246/817]	Loss 0.5253 (0.0848)	
training:	Epoch: [13][247/817]	Loss 0.0155 (0.0845)	
training:	Epoch: [13][248/817]	Loss 0.0147 (0.0842)	
training:	Epoch: [13][249/817]	Loss 0.0137 (0.0839)	
training:	Epoch: [13][250/817]	Loss 0.0192 (0.0836)	
training:	Epoch: [13][251/817]	Loss 0.0154 (0.0834)	
training:	Epoch: [13][252/817]	Loss 0.0124 (0.0831)	
training:	Epoch: [13][253/817]	Loss 0.0185 (0.0828)	
training:	Epoch: [13][254/817]	Loss 0.0163 (0.0826)	
training:	Epoch: [13][255/817]	Loss 0.0510 (0.0825)	
training:	Epoch: [13][256/817]	Loss 0.0157 (0.0822)	
training:	Epoch: [13][257/817]	Loss 0.0218 (0.0820)	
training:	Epoch: [13][258/817]	Loss 0.0169 (0.0817)	
training:	Epoch: [13][259/817]	Loss 0.0426 (0.0816)	
training:	Epoch: [13][260/817]	Loss 0.3784 (0.0827)	
training:	Epoch: [13][261/817]	Loss 0.0391 (0.0825)	
training:	Epoch: [13][262/817]	Loss 0.0134 (0.0823)	
training:	Epoch: [13][263/817]	Loss 0.0138 (0.0820)	
training:	Epoch: [13][264/817]	Loss 0.0164 (0.0818)	
training:	Epoch: [13][265/817]	Loss 0.0136 (0.0815)	
training:	Epoch: [13][266/817]	Loss 0.0141 (0.0812)	
training:	Epoch: [13][267/817]	Loss 0.0117 (0.0810)	
training:	Epoch: [13][268/817]	Loss 1.1118 (0.0848)	
training:	Epoch: [13][269/817]	Loss 0.0122 (0.0846)	
training:	Epoch: [13][270/817]	Loss 0.0137 (0.0843)	
training:	Epoch: [13][271/817]	Loss 0.0133 (0.0840)	
training:	Epoch: [13][272/817]	Loss 0.0173 (0.0838)	
training:	Epoch: [13][273/817]	Loss 0.0131 (0.0835)	
training:	Epoch: [13][274/817]	Loss 0.0563 (0.0834)	
training:	Epoch: [13][275/817]	Loss 0.5188 (0.0850)	
training:	Epoch: [13][276/817]	Loss 0.0135 (0.0848)	
training:	Epoch: [13][277/817]	Loss 0.0141 (0.0845)	
training:	Epoch: [13][278/817]	Loss 0.5692 (0.0862)	
training:	Epoch: [13][279/817]	Loss 0.2659 (0.0869)	
training:	Epoch: [13][280/817]	Loss 0.0135 (0.0866)	
training:	Epoch: [13][281/817]	Loss 0.0208 (0.0864)	
training:	Epoch: [13][282/817]	Loss 0.5434 (0.0880)	
training:	Epoch: [13][283/817]	Loss 0.0136 (0.0878)	
training:	Epoch: [13][284/817]	Loss 0.0196 (0.0875)	
training:	Epoch: [13][285/817]	Loss 0.0126 (0.0872)	
training:	Epoch: [13][286/817]	Loss 0.0125 (0.0870)	
training:	Epoch: [13][287/817]	Loss 0.0154 (0.0867)	
training:	Epoch: [13][288/817]	Loss 0.0169 (0.0865)	
training:	Epoch: [13][289/817]	Loss 0.0213 (0.0863)	
training:	Epoch: [13][290/817]	Loss 0.0149 (0.0860)	
training:	Epoch: [13][291/817]	Loss 0.0221 (0.0858)	
training:	Epoch: [13][292/817]	Loss 0.0155 (0.0856)	
training:	Epoch: [13][293/817]	Loss 0.0519 (0.0854)	
training:	Epoch: [13][294/817]	Loss 0.0200 (0.0852)	
training:	Epoch: [13][295/817]	Loss 0.0170 (0.0850)	
training:	Epoch: [13][296/817]	Loss 0.0143 (0.0848)	
training:	Epoch: [13][297/817]	Loss 0.0183 (0.0845)	
training:	Epoch: [13][298/817]	Loss 0.0146 (0.0843)	
training:	Epoch: [13][299/817]	Loss 0.0266 (0.0841)	
training:	Epoch: [13][300/817]	Loss 0.0144 (0.0839)	
training:	Epoch: [13][301/817]	Loss 0.0211 (0.0837)	
training:	Epoch: [13][302/817]	Loss 0.0143 (0.0834)	
training:	Epoch: [13][303/817]	Loss 0.0177 (0.0832)	
training:	Epoch: [13][304/817]	Loss 0.0146 (0.0830)	
training:	Epoch: [13][305/817]	Loss 0.0138 (0.0828)	
training:	Epoch: [13][306/817]	Loss 0.0160 (0.0825)	
training:	Epoch: [13][307/817]	Loss 0.0152 (0.0823)	
training:	Epoch: [13][308/817]	Loss 0.0155 (0.0821)	
training:	Epoch: [13][309/817]	Loss 0.0142 (0.0819)	
training:	Epoch: [13][310/817]	Loss 0.0230 (0.0817)	
training:	Epoch: [13][311/817]	Loss 0.0199 (0.0815)	
training:	Epoch: [13][312/817]	Loss 0.0137 (0.0813)	
training:	Epoch: [13][313/817]	Loss 0.0298 (0.0811)	
training:	Epoch: [13][314/817]	Loss 0.0907 (0.0812)	
training:	Epoch: [13][315/817]	Loss 0.0164 (0.0809)	
training:	Epoch: [13][316/817]	Loss 0.0204 (0.0808)	
training:	Epoch: [13][317/817]	Loss 0.0138 (0.0805)	
training:	Epoch: [13][318/817]	Loss 0.0141 (0.0803)	
training:	Epoch: [13][319/817]	Loss 0.0150 (0.0801)	
training:	Epoch: [13][320/817]	Loss 0.0134 (0.0799)	
training:	Epoch: [13][321/817]	Loss 0.3782 (0.0808)	
training:	Epoch: [13][322/817]	Loss 0.0138 (0.0806)	
training:	Epoch: [13][323/817]	Loss 0.0139 (0.0804)	
training:	Epoch: [13][324/817]	Loss 0.0145 (0.0802)	
training:	Epoch: [13][325/817]	Loss 0.0145 (0.0800)	
training:	Epoch: [13][326/817]	Loss 0.0143 (0.0798)	
training:	Epoch: [13][327/817]	Loss 0.0128 (0.0796)	
training:	Epoch: [13][328/817]	Loss 0.0174 (0.0794)	
training:	Epoch: [13][329/817]	Loss 0.0146 (0.0792)	
training:	Epoch: [13][330/817]	Loss 0.0138 (0.0790)	
training:	Epoch: [13][331/817]	Loss 0.0117 (0.0788)	
training:	Epoch: [13][332/817]	Loss 0.0140 (0.0786)	
training:	Epoch: [13][333/817]	Loss 0.0181 (0.0785)	
training:	Epoch: [13][334/817]	Loss 0.0140 (0.0783)	
training:	Epoch: [13][335/817]	Loss 0.2597 (0.0788)	
training:	Epoch: [13][336/817]	Loss 0.0147 (0.0786)	
training:	Epoch: [13][337/817]	Loss 0.0170 (0.0784)	
training:	Epoch: [13][338/817]	Loss 0.0153 (0.0782)	
training:	Epoch: [13][339/817]	Loss 0.0177 (0.0781)	
training:	Epoch: [13][340/817]	Loss 0.0136 (0.0779)	
training:	Epoch: [13][341/817]	Loss 0.0387 (0.0778)	
training:	Epoch: [13][342/817]	Loss 0.4841 (0.0789)	
training:	Epoch: [13][343/817]	Loss 0.0134 (0.0788)	
training:	Epoch: [13][344/817]	Loss 0.0129 (0.0786)	
training:	Epoch: [13][345/817]	Loss 0.0146 (0.0784)	
training:	Epoch: [13][346/817]	Loss 0.0137 (0.0782)	
training:	Epoch: [13][347/817]	Loss 1.0924 (0.0811)	
training:	Epoch: [13][348/817]	Loss 0.0139 (0.0809)	
training:	Epoch: [13][349/817]	Loss 0.0132 (0.0807)	
training:	Epoch: [13][350/817]	Loss 0.0133 (0.0805)	
training:	Epoch: [13][351/817]	Loss 0.0134 (0.0803)	
training:	Epoch: [13][352/817]	Loss 0.0598 (0.0803)	
training:	Epoch: [13][353/817]	Loss 0.0156 (0.0801)	
training:	Epoch: [13][354/817]	Loss 0.0138 (0.0799)	
training:	Epoch: [13][355/817]	Loss 0.0143 (0.0797)	
training:	Epoch: [13][356/817]	Loss 0.0142 (0.0795)	
training:	Epoch: [13][357/817]	Loss 0.0165 (0.0794)	
training:	Epoch: [13][358/817]	Loss 0.0173 (0.0792)	
training:	Epoch: [13][359/817]	Loss 0.0143 (0.0790)	
training:	Epoch: [13][360/817]	Loss 0.0157 (0.0788)	
training:	Epoch: [13][361/817]	Loss 0.0306 (0.0787)	
training:	Epoch: [13][362/817]	Loss 0.2770 (0.0793)	
training:	Epoch: [13][363/817]	Loss 0.0141 (0.0791)	
training:	Epoch: [13][364/817]	Loss 0.0126 (0.0789)	
training:	Epoch: [13][365/817]	Loss 0.0189 (0.0787)	
training:	Epoch: [13][366/817]	Loss 0.1101 (0.0788)	
training:	Epoch: [13][367/817]	Loss 0.0180 (0.0786)	
training:	Epoch: [13][368/817]	Loss 0.0184 (0.0785)	
training:	Epoch: [13][369/817]	Loss 0.0143 (0.0783)	
training:	Epoch: [13][370/817]	Loss 0.0150 (0.0781)	
training:	Epoch: [13][371/817]	Loss 0.0135 (0.0780)	
training:	Epoch: [13][372/817]	Loss 0.0143 (0.0778)	
training:	Epoch: [13][373/817]	Loss 0.0203 (0.0776)	
training:	Epoch: [13][374/817]	Loss 0.0144 (0.0775)	
training:	Epoch: [13][375/817]	Loss 0.0134 (0.0773)	
training:	Epoch: [13][376/817]	Loss 0.0283 (0.0772)	
training:	Epoch: [13][377/817]	Loss 0.0140 (0.0770)	
training:	Epoch: [13][378/817]	Loss 0.0146 (0.0768)	
training:	Epoch: [13][379/817]	Loss 0.0135 (0.0767)	
training:	Epoch: [13][380/817]	Loss 0.0140 (0.0765)	
training:	Epoch: [13][381/817]	Loss 0.0134 (0.0763)	
training:	Epoch: [13][382/817]	Loss 0.0139 (0.0762)	
training:	Epoch: [13][383/817]	Loss 0.0210 (0.0760)	
training:	Epoch: [13][384/817]	Loss 0.0132 (0.0759)	
training:	Epoch: [13][385/817]	Loss 0.0138 (0.0757)	
training:	Epoch: [13][386/817]	Loss 0.0216 (0.0756)	
training:	Epoch: [13][387/817]	Loss 0.5427 (0.0768)	
training:	Epoch: [13][388/817]	Loss 0.0692 (0.0768)	
training:	Epoch: [13][389/817]	Loss 0.0133 (0.0766)	
training:	Epoch: [13][390/817]	Loss 0.0180 (0.0764)	
training:	Epoch: [13][391/817]	Loss 0.0151 (0.0763)	
training:	Epoch: [13][392/817]	Loss 0.0142 (0.0761)	
training:	Epoch: [13][393/817]	Loss 0.0187 (0.0760)	
training:	Epoch: [13][394/817]	Loss 0.0203 (0.0758)	
training:	Epoch: [13][395/817]	Loss 0.0130 (0.0757)	
training:	Epoch: [13][396/817]	Loss 0.0136 (0.0755)	
training:	Epoch: [13][397/817]	Loss 0.0134 (0.0754)	
training:	Epoch: [13][398/817]	Loss 0.0161 (0.0752)	
training:	Epoch: [13][399/817]	Loss 0.0148 (0.0751)	
training:	Epoch: [13][400/817]	Loss 0.0126 (0.0749)	
training:	Epoch: [13][401/817]	Loss 0.0168 (0.0748)	
training:	Epoch: [13][402/817]	Loss 0.0139 (0.0746)	
training:	Epoch: [13][403/817]	Loss 0.0131 (0.0745)	
training:	Epoch: [13][404/817]	Loss 0.0124 (0.0743)	
training:	Epoch: [13][405/817]	Loss 0.5606 (0.0755)	
training:	Epoch: [13][406/817]	Loss 0.0136 (0.0754)	
training:	Epoch: [13][407/817]	Loss 0.0381 (0.0753)	
training:	Epoch: [13][408/817]	Loss 0.0177 (0.0751)	
training:	Epoch: [13][409/817]	Loss 0.0136 (0.0750)	
training:	Epoch: [13][410/817]	Loss 0.0131 (0.0748)	
training:	Epoch: [13][411/817]	Loss 0.0144 (0.0747)	
training:	Epoch: [13][412/817]	Loss 0.0130 (0.0745)	
training:	Epoch: [13][413/817]	Loss 0.0369 (0.0744)	
training:	Epoch: [13][414/817]	Loss 0.0219 (0.0743)	
training:	Epoch: [13][415/817]	Loss 1.0954 (0.0768)	
training:	Epoch: [13][416/817]	Loss 0.0221 (0.0766)	
training:	Epoch: [13][417/817]	Loss 0.5768 (0.0778)	
training:	Epoch: [13][418/817]	Loss 0.0168 (0.0777)	
training:	Epoch: [13][419/817]	Loss 0.0154 (0.0775)	
training:	Epoch: [13][420/817]	Loss 0.0129 (0.0774)	
training:	Epoch: [13][421/817]	Loss 0.0135 (0.0772)	
training:	Epoch: [13][422/817]	Loss 0.0155 (0.0771)	
training:	Epoch: [13][423/817]	Loss 0.0145 (0.0769)	
training:	Epoch: [13][424/817]	Loss 0.4623 (0.0779)	
training:	Epoch: [13][425/817]	Loss 0.0166 (0.0777)	
training:	Epoch: [13][426/817]	Loss 0.2951 (0.0782)	
training:	Epoch: [13][427/817]	Loss 0.1296 (0.0783)	
training:	Epoch: [13][428/817]	Loss 0.0146 (0.0782)	
training:	Epoch: [13][429/817]	Loss 0.0158 (0.0780)	
training:	Epoch: [13][430/817]	Loss 0.0146 (0.0779)	
training:	Epoch: [13][431/817]	Loss 0.0134 (0.0777)	
training:	Epoch: [13][432/817]	Loss 0.2973 (0.0783)	
training:	Epoch: [13][433/817]	Loss 0.0143 (0.0781)	
training:	Epoch: [13][434/817]	Loss 0.0121 (0.0780)	
training:	Epoch: [13][435/817]	Loss 0.0143 (0.0778)	
training:	Epoch: [13][436/817]	Loss 0.0130 (0.0777)	
training:	Epoch: [13][437/817]	Loss 0.0158 (0.0775)	
training:	Epoch: [13][438/817]	Loss 0.0385 (0.0774)	
training:	Epoch: [13][439/817]	Loss 0.0231 (0.0773)	
training:	Epoch: [13][440/817]	Loss 0.0157 (0.0772)	
training:	Epoch: [13][441/817]	Loss 0.0148 (0.0770)	
training:	Epoch: [13][442/817]	Loss 0.0580 (0.0770)	
training:	Epoch: [13][443/817]	Loss 0.0128 (0.0768)	
training:	Epoch: [13][444/817]	Loss 0.0154 (0.0767)	
training:	Epoch: [13][445/817]	Loss 0.0148 (0.0766)	
training:	Epoch: [13][446/817]	Loss 1.0672 (0.0788)	
training:	Epoch: [13][447/817]	Loss 0.1216 (0.0789)	
training:	Epoch: [13][448/817]	Loss 0.0120 (0.0787)	
training:	Epoch: [13][449/817]	Loss 0.0181 (0.0786)	
training:	Epoch: [13][450/817]	Loss 0.1613 (0.0788)	
training:	Epoch: [13][451/817]	Loss 0.5354 (0.0798)	
training:	Epoch: [13][452/817]	Loss 0.0136 (0.0796)	
training:	Epoch: [13][453/817]	Loss 0.0153 (0.0795)	
training:	Epoch: [13][454/817]	Loss 0.0179 (0.0794)	
training:	Epoch: [13][455/817]	Loss 0.1484 (0.0795)	
training:	Epoch: [13][456/817]	Loss 0.0130 (0.0794)	
training:	Epoch: [13][457/817]	Loss 0.0140 (0.0792)	
training:	Epoch: [13][458/817]	Loss 0.1494 (0.0794)	
training:	Epoch: [13][459/817]	Loss 0.0126 (0.0792)	
training:	Epoch: [13][460/817]	Loss 0.0126 (0.0791)	
training:	Epoch: [13][461/817]	Loss 0.0172 (0.0790)	
training:	Epoch: [13][462/817]	Loss 0.0162 (0.0788)	
training:	Epoch: [13][463/817]	Loss 0.0132 (0.0787)	
training:	Epoch: [13][464/817]	Loss 0.0133 (0.0785)	
training:	Epoch: [13][465/817]	Loss 0.0161 (0.0784)	
training:	Epoch: [13][466/817]	Loss 0.0135 (0.0783)	
training:	Epoch: [13][467/817]	Loss 0.0124 (0.0781)	
training:	Epoch: [13][468/817]	Loss 0.0262 (0.0780)	
training:	Epoch: [13][469/817]	Loss 0.0208 (0.0779)	
training:	Epoch: [13][470/817]	Loss 0.0213 (0.0778)	
training:	Epoch: [13][471/817]	Loss 0.0116 (0.0776)	
training:	Epoch: [13][472/817]	Loss 0.0154 (0.0775)	
training:	Epoch: [13][473/817]	Loss 0.1869 (0.0777)	
training:	Epoch: [13][474/817]	Loss 0.0180 (0.0776)	
training:	Epoch: [13][475/817]	Loss 0.0278 (0.0775)	
training:	Epoch: [13][476/817]	Loss 0.0343 (0.0774)	
training:	Epoch: [13][477/817]	Loss 0.0140 (0.0773)	
training:	Epoch: [13][478/817]	Loss 0.0288 (0.0772)	
training:	Epoch: [13][479/817]	Loss 0.0116 (0.0770)	
training:	Epoch: [13][480/817]	Loss 0.0558 (0.0770)	
training:	Epoch: [13][481/817]	Loss 0.0142 (0.0769)	
training:	Epoch: [13][482/817]	Loss 0.0126 (0.0767)	
training:	Epoch: [13][483/817]	Loss 0.0130 (0.0766)	
training:	Epoch: [13][484/817]	Loss 0.2501 (0.0770)	
training:	Epoch: [13][485/817]	Loss 0.0142 (0.0768)	
training:	Epoch: [13][486/817]	Loss 0.0157 (0.0767)	
training:	Epoch: [13][487/817]	Loss 0.4962 (0.0776)	
training:	Epoch: [13][488/817]	Loss 0.0160 (0.0774)	
training:	Epoch: [13][489/817]	Loss 0.0642 (0.0774)	
training:	Epoch: [13][490/817]	Loss 0.0142 (0.0773)	
training:	Epoch: [13][491/817]	Loss 0.0143 (0.0771)	
training:	Epoch: [13][492/817]	Loss 0.0141 (0.0770)	
training:	Epoch: [13][493/817]	Loss 0.0176 (0.0769)	
training:	Epoch: [13][494/817]	Loss 0.0150 (0.0768)	
training:	Epoch: [13][495/817]	Loss 0.0213 (0.0767)	
training:	Epoch: [13][496/817]	Loss 0.0403 (0.0766)	
training:	Epoch: [13][497/817]	Loss 0.0220 (0.0765)	
training:	Epoch: [13][498/817]	Loss 0.0146 (0.0764)	
training:	Epoch: [13][499/817]	Loss 0.0146 (0.0762)	
training:	Epoch: [13][500/817]	Loss 0.0145 (0.0761)	
training:	Epoch: [13][501/817]	Loss 0.1056 (0.0762)	
training:	Epoch: [13][502/817]	Loss 0.5349 (0.0771)	
training:	Epoch: [13][503/817]	Loss 0.1367 (0.0772)	
training:	Epoch: [13][504/817]	Loss 0.0190 (0.0771)	
training:	Epoch: [13][505/817]	Loss 0.0179 (0.0770)	
training:	Epoch: [13][506/817]	Loss 0.2354 (0.0773)	
training:	Epoch: [13][507/817]	Loss 0.0227 (0.0772)	
training:	Epoch: [13][508/817]	Loss 0.0175 (0.0771)	
training:	Epoch: [13][509/817]	Loss 0.0150 (0.0769)	
training:	Epoch: [13][510/817]	Loss 0.1234 (0.0770)	
training:	Epoch: [13][511/817]	Loss 0.0149 (0.0769)	
training:	Epoch: [13][512/817]	Loss 0.0166 (0.0768)	
training:	Epoch: [13][513/817]	Loss 0.0133 (0.0767)	
training:	Epoch: [13][514/817]	Loss 0.1030 (0.0767)	
training:	Epoch: [13][515/817]	Loss 0.0186 (0.0766)	
training:	Epoch: [13][516/817]	Loss 0.0191 (0.0765)	
training:	Epoch: [13][517/817]	Loss 0.0117 (0.0764)	
training:	Epoch: [13][518/817]	Loss 0.0131 (0.0762)	
training:	Epoch: [13][519/817]	Loss 0.0160 (0.0761)	
training:	Epoch: [13][520/817]	Loss 0.0183 (0.0760)	
training:	Epoch: [13][521/817]	Loss 0.3370 (0.0765)	
training:	Epoch: [13][522/817]	Loss 0.0144 (0.0764)	
training:	Epoch: [13][523/817]	Loss 0.2922 (0.0768)	
training:	Epoch: [13][524/817]	Loss 0.0130 (0.0767)	
training:	Epoch: [13][525/817]	Loss 0.5087 (0.0775)	
training:	Epoch: [13][526/817]	Loss 0.0163 (0.0774)	
training:	Epoch: [13][527/817]	Loss 0.0130 (0.0773)	
training:	Epoch: [13][528/817]	Loss 0.1496 (0.0774)	
training:	Epoch: [13][529/817]	Loss 0.0135 (0.0773)	
training:	Epoch: [13][530/817]	Loss 0.5301 (0.0781)	
training:	Epoch: [13][531/817]	Loss 0.0201 (0.0780)	
training:	Epoch: [13][532/817]	Loss 0.0266 (0.0779)	
training:	Epoch: [13][533/817]	Loss 0.0142 (0.0778)	
training:	Epoch: [13][534/817]	Loss 0.0145 (0.0777)	
training:	Epoch: [13][535/817]	Loss 0.5023 (0.0785)	
training:	Epoch: [13][536/817]	Loss 0.0142 (0.0784)	
training:	Epoch: [13][537/817]	Loss 0.0458 (0.0783)	
training:	Epoch: [13][538/817]	Loss 0.0121 (0.0782)	
training:	Epoch: [13][539/817]	Loss 0.0157 (0.0781)	
training:	Epoch: [13][540/817]	Loss 0.3610 (0.0786)	
training:	Epoch: [13][541/817]	Loss 0.0221 (0.0785)	
training:	Epoch: [13][542/817]	Loss 0.0157 (0.0784)	
training:	Epoch: [13][543/817]	Loss 0.0141 (0.0783)	
training:	Epoch: [13][544/817]	Loss 0.0157 (0.0781)	
training:	Epoch: [13][545/817]	Loss 0.0156 (0.0780)	
training:	Epoch: [13][546/817]	Loss 0.0627 (0.0780)	
training:	Epoch: [13][547/817]	Loss 0.3116 (0.0784)	
training:	Epoch: [13][548/817]	Loss 0.0246 (0.0783)	
training:	Epoch: [13][549/817]	Loss 0.0322 (0.0782)	
training:	Epoch: [13][550/817]	Loss 0.1705 (0.0784)	
training:	Epoch: [13][551/817]	Loss 0.0144 (0.0783)	
training:	Epoch: [13][552/817]	Loss 0.0127 (0.0782)	
training:	Epoch: [13][553/817]	Loss 0.0172 (0.0781)	
training:	Epoch: [13][554/817]	Loss 0.0189 (0.0780)	
training:	Epoch: [13][555/817]	Loss 0.0199 (0.0779)	
training:	Epoch: [13][556/817]	Loss 0.0169 (0.0777)	
training:	Epoch: [13][557/817]	Loss 0.0134 (0.0776)	
training:	Epoch: [13][558/817]	Loss 0.0196 (0.0775)	
training:	Epoch: [13][559/817]	Loss 0.0241 (0.0774)	
training:	Epoch: [13][560/817]	Loss 0.0195 (0.0773)	
training:	Epoch: [13][561/817]	Loss 0.0337 (0.0772)	
training:	Epoch: [13][562/817]	Loss 0.0319 (0.0772)	
training:	Epoch: [13][563/817]	Loss 0.0111 (0.0771)	
training:	Epoch: [13][564/817]	Loss 0.0135 (0.0769)	
training:	Epoch: [13][565/817]	Loss 0.0128 (0.0768)	
training:	Epoch: [13][566/817]	Loss 0.0117 (0.0767)	
training:	Epoch: [13][567/817]	Loss 0.0129 (0.0766)	
training:	Epoch: [13][568/817]	Loss 0.5796 (0.0775)	
training:	Epoch: [13][569/817]	Loss 0.0147 (0.0774)	
training:	Epoch: [13][570/817]	Loss 0.0139 (0.0773)	
training:	Epoch: [13][571/817]	Loss 0.0157 (0.0772)	
training:	Epoch: [13][572/817]	Loss 0.0141 (0.0770)	
training:	Epoch: [13][573/817]	Loss 0.0167 (0.0769)	
training:	Epoch: [13][574/817]	Loss 0.0136 (0.0768)	
training:	Epoch: [13][575/817]	Loss 0.0135 (0.0767)	
training:	Epoch: [13][576/817]	Loss 0.0147 (0.0766)	
training:	Epoch: [13][577/817]	Loss 0.0128 (0.0765)	
training:	Epoch: [13][578/817]	Loss 0.0127 (0.0764)	
training:	Epoch: [13][579/817]	Loss 0.0279 (0.0763)	
training:	Epoch: [13][580/817]	Loss 0.0151 (0.0762)	
training:	Epoch: [13][581/817]	Loss 0.0215 (0.0761)	
training:	Epoch: [13][582/817]	Loss 0.1410 (0.0762)	
training:	Epoch: [13][583/817]	Loss 0.4040 (0.0768)	
training:	Epoch: [13][584/817]	Loss 0.3855 (0.0773)	
training:	Epoch: [13][585/817]	Loss 0.0141 (0.0772)	
training:	Epoch: [13][586/817]	Loss 0.0344 (0.0771)	
training:	Epoch: [13][587/817]	Loss 0.0158 (0.0770)	
training:	Epoch: [13][588/817]	Loss 0.0172 (0.0769)	
training:	Epoch: [13][589/817]	Loss 0.0149 (0.0768)	
training:	Epoch: [13][590/817]	Loss 0.0490 (0.0768)	
training:	Epoch: [13][591/817]	Loss 0.0120 (0.0767)	
training:	Epoch: [13][592/817]	Loss 0.0137 (0.0766)	
training:	Epoch: [13][593/817]	Loss 0.0769 (0.0766)	
training:	Epoch: [13][594/817]	Loss 0.0172 (0.0765)	
training:	Epoch: [13][595/817]	Loss 0.4451 (0.0771)	
training:	Epoch: [13][596/817]	Loss 0.0185 (0.0770)	
training:	Epoch: [13][597/817]	Loss 0.0187 (0.0769)	
training:	Epoch: [13][598/817]	Loss 0.0117 (0.0768)	
training:	Epoch: [13][599/817]	Loss 0.0268 (0.0767)	
training:	Epoch: [13][600/817]	Loss 0.5966 (0.0776)	
training:	Epoch: [13][601/817]	Loss 0.0178 (0.0775)	
training:	Epoch: [13][602/817]	Loss 0.0218 (0.0774)	
training:	Epoch: [13][603/817]	Loss 0.0147 (0.0773)	
training:	Epoch: [13][604/817]	Loss 0.0173 (0.0772)	
training:	Epoch: [13][605/817]	Loss 0.0142 (0.0771)	
training:	Epoch: [13][606/817]	Loss 0.0163 (0.0770)	
training:	Epoch: [13][607/817]	Loss 0.0208 (0.0769)	
training:	Epoch: [13][608/817]	Loss 0.0197 (0.0768)	
training:	Epoch: [13][609/817]	Loss 0.0176 (0.0767)	
training:	Epoch: [13][610/817]	Loss 0.0136 (0.0766)	
training:	Epoch: [13][611/817]	Loss 0.0135 (0.0765)	
training:	Epoch: [13][612/817]	Loss 0.0469 (0.0764)	
training:	Epoch: [13][613/817]	Loss 0.0168 (0.0763)	
training:	Epoch: [13][614/817]	Loss 0.0140 (0.0762)	
training:	Epoch: [13][615/817]	Loss 0.0215 (0.0761)	
training:	Epoch: [13][616/817]	Loss 0.0138 (0.0760)	
training:	Epoch: [13][617/817]	Loss 0.0164 (0.0759)	
training:	Epoch: [13][618/817]	Loss 0.0242 (0.0758)	
training:	Epoch: [13][619/817]	Loss 0.0904 (0.0759)	
training:	Epoch: [13][620/817]	Loss 0.0138 (0.0758)	
training:	Epoch: [13][621/817]	Loss 0.0213 (0.0757)	
training:	Epoch: [13][622/817]	Loss 0.0132 (0.0756)	
training:	Epoch: [13][623/817]	Loss 0.0129 (0.0755)	
training:	Epoch: [13][624/817]	Loss 0.0170 (0.0754)	
training:	Epoch: [13][625/817]	Loss 0.0140 (0.0753)	
training:	Epoch: [13][626/817]	Loss 0.0124 (0.0752)	
training:	Epoch: [13][627/817]	Loss 0.0129 (0.0751)	
training:	Epoch: [13][628/817]	Loss 0.0377 (0.0750)	
training:	Epoch: [13][629/817]	Loss 0.0154 (0.0749)	
training:	Epoch: [13][630/817]	Loss 0.0139 (0.0748)	
training:	Epoch: [13][631/817]	Loss 0.1536 (0.0750)	
training:	Epoch: [13][632/817]	Loss 0.0160 (0.0749)	
training:	Epoch: [13][633/817]	Loss 0.0155 (0.0748)	
training:	Epoch: [13][634/817]	Loss 0.0130 (0.0747)	
training:	Epoch: [13][635/817]	Loss 0.0157 (0.0746)	
training:	Epoch: [13][636/817]	Loss 0.3988 (0.0751)	
training:	Epoch: [13][637/817]	Loss 0.0121 (0.0750)	
training:	Epoch: [13][638/817]	Loss 0.0254 (0.0749)	
training:	Epoch: [13][639/817]	Loss 0.0200 (0.0748)	
training:	Epoch: [13][640/817]	Loss 0.0112 (0.0747)	
training:	Epoch: [13][641/817]	Loss 0.0174 (0.0746)	
training:	Epoch: [13][642/817]	Loss 0.0177 (0.0746)	
training:	Epoch: [13][643/817]	Loss 0.0121 (0.0745)	
training:	Epoch: [13][644/817]	Loss 0.0165 (0.0744)	
training:	Epoch: [13][645/817]	Loss 0.0133 (0.0743)	
training:	Epoch: [13][646/817]	Loss 0.0128 (0.0742)	
training:	Epoch: [13][647/817]	Loss 0.0115 (0.0741)	
training:	Epoch: [13][648/817]	Loss 0.0136 (0.0740)	
training:	Epoch: [13][649/817]	Loss 0.0332 (0.0739)	
training:	Epoch: [13][650/817]	Loss 0.0139 (0.0738)	
training:	Epoch: [13][651/817]	Loss 0.0119 (0.0737)	
training:	Epoch: [13][652/817]	Loss 0.0146 (0.0736)	
training:	Epoch: [13][653/817]	Loss 0.4982 (0.0743)	
training:	Epoch: [13][654/817]	Loss 0.4259 (0.0748)	
training:	Epoch: [13][655/817]	Loss 0.0120 (0.0747)	
training:	Epoch: [13][656/817]	Loss 0.0172 (0.0746)	
training:	Epoch: [13][657/817]	Loss 0.0172 (0.0746)	
training:	Epoch: [13][658/817]	Loss 0.0194 (0.0745)	
training:	Epoch: [13][659/817]	Loss 0.0159 (0.0744)	
training:	Epoch: [13][660/817]	Loss 0.0433 (0.0743)	
training:	Epoch: [13][661/817]	Loss 0.0246 (0.0743)	
training:	Epoch: [13][662/817]	Loss 0.0793 (0.0743)	
training:	Epoch: [13][663/817]	Loss 0.0224 (0.0742)	
training:	Epoch: [13][664/817]	Loss 0.1016 (0.0742)	
training:	Epoch: [13][665/817]	Loss 0.0206 (0.0742)	
training:	Epoch: [13][666/817]	Loss 0.0331 (0.0741)	
training:	Epoch: [13][667/817]	Loss 0.0144 (0.0740)	
training:	Epoch: [13][668/817]	Loss 0.0472 (0.0740)	
training:	Epoch: [13][669/817]	Loss 0.0148 (0.0739)	
training:	Epoch: [13][670/817]	Loss 0.0210 (0.0738)	
training:	Epoch: [13][671/817]	Loss 0.0350 (0.0737)	
training:	Epoch: [13][672/817]	Loss 0.0147 (0.0737)	
training:	Epoch: [13][673/817]	Loss 0.4522 (0.0742)	
training:	Epoch: [13][674/817]	Loss 0.0125 (0.0741)	
training:	Epoch: [13][675/817]	Loss 0.5292 (0.0748)	
training:	Epoch: [13][676/817]	Loss 0.0128 (0.0747)	
training:	Epoch: [13][677/817]	Loss 0.0153 (0.0746)	
training:	Epoch: [13][678/817]	Loss 0.0177 (0.0745)	
training:	Epoch: [13][679/817]	Loss 0.0140 (0.0744)	
training:	Epoch: [13][680/817]	Loss 0.0144 (0.0744)	
training:	Epoch: [13][681/817]	Loss 0.0125 (0.0743)	
training:	Epoch: [13][682/817]	Loss 0.0217 (0.0742)	
training:	Epoch: [13][683/817]	Loss 0.0117 (0.0741)	
training:	Epoch: [13][684/817]	Loss 0.0175 (0.0740)	
training:	Epoch: [13][685/817]	Loss 0.5706 (0.0747)	
training:	Epoch: [13][686/817]	Loss 0.0122 (0.0746)	
training:	Epoch: [13][687/817]	Loss 0.0131 (0.0746)	
training:	Epoch: [13][688/817]	Loss 0.0203 (0.0745)	
training:	Epoch: [13][689/817]	Loss 0.0122 (0.0744)	
training:	Epoch: [13][690/817]	Loss 0.0131 (0.0743)	
training:	Epoch: [13][691/817]	Loss 0.0268 (0.0742)	
training:	Epoch: [13][692/817]	Loss 0.0144 (0.0741)	
training:	Epoch: [13][693/817]	Loss 0.0119 (0.0741)	
training:	Epoch: [13][694/817]	Loss 0.0145 (0.0740)	
training:	Epoch: [13][695/817]	Loss 0.0127 (0.0739)	
training:	Epoch: [13][696/817]	Loss 0.0148 (0.0738)	
training:	Epoch: [13][697/817]	Loss 0.5227 (0.0744)	
training:	Epoch: [13][698/817]	Loss 0.5292 (0.0751)	
training:	Epoch: [13][699/817]	Loss 0.0106 (0.0750)	
training:	Epoch: [13][700/817]	Loss 0.0157 (0.0749)	
training:	Epoch: [13][701/817]	Loss 0.0164 (0.0748)	
training:	Epoch: [13][702/817]	Loss 0.0141 (0.0747)	
training:	Epoch: [13][703/817]	Loss 0.0121 (0.0747)	
training:	Epoch: [13][704/817]	Loss 0.0142 (0.0746)	
training:	Epoch: [13][705/817]	Loss 0.0127 (0.0745)	
training:	Epoch: [13][706/817]	Loss 0.0185 (0.0744)	
training:	Epoch: [13][707/817]	Loss 0.1604 (0.0745)	
training:	Epoch: [13][708/817]	Loss 0.0180 (0.0744)	
training:	Epoch: [13][709/817]	Loss 0.0113 (0.0744)	
training:	Epoch: [13][710/817]	Loss 0.0181 (0.0743)	
training:	Epoch: [13][711/817]	Loss 0.0129 (0.0742)	
training:	Epoch: [13][712/817]	Loss 0.5840 (0.0749)	
training:	Epoch: [13][713/817]	Loss 0.0252 (0.0748)	
training:	Epoch: [13][714/817]	Loss 0.0223 (0.0748)	
training:	Epoch: [13][715/817]	Loss 0.0430 (0.0747)	
training:	Epoch: [13][716/817]	Loss 0.0142 (0.0746)	
training:	Epoch: [13][717/817]	Loss 0.0116 (0.0745)	
training:	Epoch: [13][718/817]	Loss 0.0134 (0.0745)	
training:	Epoch: [13][719/817]	Loss 0.0136 (0.0744)	
training:	Epoch: [13][720/817]	Loss 0.0217 (0.0743)	
training:	Epoch: [13][721/817]	Loss 0.0190 (0.0742)	
training:	Epoch: [13][722/817]	Loss 0.0227 (0.0742)	
training:	Epoch: [13][723/817]	Loss 0.0123 (0.0741)	
training:	Epoch: [13][724/817]	Loss 0.0126 (0.0740)	
training:	Epoch: [13][725/817]	Loss 0.0161 (0.0739)	
training:	Epoch: [13][726/817]	Loss 0.0136 (0.0738)	
training:	Epoch: [13][727/817]	Loss 0.0143 (0.0737)	
training:	Epoch: [13][728/817]	Loss 0.0133 (0.0737)	
training:	Epoch: [13][729/817]	Loss 0.0138 (0.0736)	
training:	Epoch: [13][730/817]	Loss 0.0138 (0.0735)	
training:	Epoch: [13][731/817]	Loss 0.0191 (0.0734)	
training:	Epoch: [13][732/817]	Loss 0.0133 (0.0733)	
training:	Epoch: [13][733/817]	Loss 0.0168 (0.0733)	
training:	Epoch: [13][734/817]	Loss 0.0140 (0.0732)	
training:	Epoch: [13][735/817]	Loss 0.0161 (0.0731)	
training:	Epoch: [13][736/817]	Loss 0.0136 (0.0730)	
training:	Epoch: [13][737/817]	Loss 0.0243 (0.0730)	
training:	Epoch: [13][738/817]	Loss 0.0145 (0.0729)	
training:	Epoch: [13][739/817]	Loss 0.0141 (0.0728)	
training:	Epoch: [13][740/817]	Loss 0.0125 (0.0727)	
training:	Epoch: [13][741/817]	Loss 0.0195 (0.0726)	
training:	Epoch: [13][742/817]	Loss 0.0147 (0.0726)	
training:	Epoch: [13][743/817]	Loss 0.0144 (0.0725)	
training:	Epoch: [13][744/817]	Loss 0.0157 (0.0724)	
training:	Epoch: [13][745/817]	Loss 0.0130 (0.0723)	
training:	Epoch: [13][746/817]	Loss 0.5238 (0.0729)	
training:	Epoch: [13][747/817]	Loss 0.1067 (0.0730)	
training:	Epoch: [13][748/817]	Loss 0.0152 (0.0729)	
training:	Epoch: [13][749/817]	Loss 0.0111 (0.0728)	
training:	Epoch: [13][750/817]	Loss 0.0128 (0.0727)	
training:	Epoch: [13][751/817]	Loss 0.0137 (0.0727)	
training:	Epoch: [13][752/817]	Loss 0.5586 (0.0733)	
training:	Epoch: [13][753/817]	Loss 0.0201 (0.0732)	
training:	Epoch: [13][754/817]	Loss 0.0206 (0.0732)	
training:	Epoch: [13][755/817]	Loss 0.0141 (0.0731)	
training:	Epoch: [13][756/817]	Loss 0.0304 (0.0730)	
training:	Epoch: [13][757/817]	Loss 0.0612 (0.0730)	
training:	Epoch: [13][758/817]	Loss 0.0130 (0.0729)	
training:	Epoch: [13][759/817]	Loss 0.0121 (0.0729)	
training:	Epoch: [13][760/817]	Loss 0.0103 (0.0728)	
training:	Epoch: [13][761/817]	Loss 0.0127 (0.0727)	
training:	Epoch: [13][762/817]	Loss 0.0119 (0.0726)	
training:	Epoch: [13][763/817]	Loss 0.0174 (0.0725)	
training:	Epoch: [13][764/817]	Loss 0.0186 (0.0725)	
training:	Epoch: [13][765/817]	Loss 0.0123 (0.0724)	
training:	Epoch: [13][766/817]	Loss 0.0147 (0.0723)	
training:	Epoch: [13][767/817]	Loss 0.0139 (0.0722)	
training:	Epoch: [13][768/817]	Loss 0.0115 (0.0722)	
training:	Epoch: [13][769/817]	Loss 0.0112 (0.0721)	
training:	Epoch: [13][770/817]	Loss 0.0151 (0.0720)	
training:	Epoch: [13][771/817]	Loss 0.0132 (0.0719)	
training:	Epoch: [13][772/817]	Loss 0.0495 (0.0719)	
training:	Epoch: [13][773/817]	Loss 0.0126 (0.0718)	
training:	Epoch: [13][774/817]	Loss 0.0198 (0.0718)	
training:	Epoch: [13][775/817]	Loss 0.0137 (0.0717)	
training:	Epoch: [13][776/817]	Loss 0.0130 (0.0716)	
training:	Epoch: [13][777/817]	Loss 0.0121 (0.0715)	
training:	Epoch: [13][778/817]	Loss 0.1334 (0.0716)	
training:	Epoch: [13][779/817]	Loss 0.0197 (0.0715)	
training:	Epoch: [13][780/817]	Loss 0.0225 (0.0715)	
training:	Epoch: [13][781/817]	Loss 0.0786 (0.0715)	
training:	Epoch: [13][782/817]	Loss 0.0153 (0.0714)	
training:	Epoch: [13][783/817]	Loss 0.0149 (0.0713)	
training:	Epoch: [13][784/817]	Loss 0.0132 (0.0713)	
training:	Epoch: [13][785/817]	Loss 0.0146 (0.0712)	
training:	Epoch: [13][786/817]	Loss 0.0105 (0.0711)	
training:	Epoch: [13][787/817]	Loss 0.0159 (0.0711)	
training:	Epoch: [13][788/817]	Loss 0.0144 (0.0710)	
training:	Epoch: [13][789/817]	Loss 0.5932 (0.0716)	
training:	Epoch: [13][790/817]	Loss 0.0166 (0.0716)	
training:	Epoch: [13][791/817]	Loss 0.0114 (0.0715)	
training:	Epoch: [13][792/817]	Loss 0.0127 (0.0714)	
training:	Epoch: [13][793/817]	Loss 0.0105 (0.0713)	
training:	Epoch: [13][794/817]	Loss 0.0132 (0.0713)	
training:	Epoch: [13][795/817]	Loss 0.0115 (0.0712)	
training:	Epoch: [13][796/817]	Loss 0.0144 (0.0711)	
training:	Epoch: [13][797/817]	Loss 0.0142 (0.0711)	
training:	Epoch: [13][798/817]	Loss 0.0180 (0.0710)	
training:	Epoch: [13][799/817]	Loss 0.0131 (0.0709)	
training:	Epoch: [13][800/817]	Loss 0.0129 (0.0708)	
training:	Epoch: [13][801/817]	Loss 0.0364 (0.0708)	
training:	Epoch: [13][802/817]	Loss 0.0127 (0.0707)	
training:	Epoch: [13][803/817]	Loss 0.0150 (0.0707)	
training:	Epoch: [13][804/817]	Loss 0.0212 (0.0706)	
training:	Epoch: [13][805/817]	Loss 0.0792 (0.0706)	
training:	Epoch: [13][806/817]	Loss 0.0784 (0.0706)	
training:	Epoch: [13][807/817]	Loss 0.0548 (0.0706)	
training:	Epoch: [13][808/817]	Loss 0.0115 (0.0705)	
training:	Epoch: [13][809/817]	Loss 0.0163 (0.0705)	
training:	Epoch: [13][810/817]	Loss 0.0141 (0.0704)	
training:	Epoch: [13][811/817]	Loss 0.0144 (0.0703)	
training:	Epoch: [13][812/817]	Loss 0.0157 (0.0703)	
training:	Epoch: [13][813/817]	Loss 0.0143 (0.0702)	
training:	Epoch: [13][814/817]	Loss 0.0120 (0.0701)	
training:	Epoch: [13][815/817]	Loss 0.0126 (0.0700)	
training:	Epoch: [13][816/817]	Loss 0.0130 (0.0700)	
training:	Epoch: [13][817/817]	Loss 0.0175 (0.0699)	
Training:	 Loss: 0.0699

Training:	 ACC: 0.9898 0.9898 0.9891 0.9904
Validation:	 ACC: 0.7885 0.7897 0.8158 0.7612
Validation:	 Best_BACC: 0.8223 0.8218 0.8127 0.8318
Validation:	 Loss: 0.7785
Pretraining:	Epoch 14/200
----------
training:	Epoch: [14][1/817]	Loss 0.0124 (0.0124)	
training:	Epoch: [14][2/817]	Loss 0.0121 (0.0123)	
training:	Epoch: [14][3/817]	Loss 0.0161 (0.0135)	
training:	Epoch: [14][4/817]	Loss 0.0763 (0.0292)	
training:	Epoch: [14][5/817]	Loss 0.0228 (0.0279)	
training:	Epoch: [14][6/817]	Loss 0.0112 (0.0252)	
training:	Epoch: [14][7/817]	Loss 0.0971 (0.0354)	
training:	Epoch: [14][8/817]	Loss 0.0211 (0.0336)	
training:	Epoch: [14][9/817]	Loss 0.0136 (0.0314)	
training:	Epoch: [14][10/817]	Loss 0.0241 (0.0307)	
training:	Epoch: [14][11/817]	Loss 0.0117 (0.0289)	
training:	Epoch: [14][12/817]	Loss 0.0131 (0.0276)	
training:	Epoch: [14][13/817]	Loss 0.0653 (0.0305)	
training:	Epoch: [14][14/817]	Loss 0.0114 (0.0292)	
training:	Epoch: [14][15/817]	Loss 0.0118 (0.0280)	
training:	Epoch: [14][16/817]	Loss 0.0140 (0.0271)	
training:	Epoch: [14][17/817]	Loss 0.0115 (0.0262)	
training:	Epoch: [14][18/817]	Loss 0.0121 (0.0254)	
training:	Epoch: [14][19/817]	Loss 0.0127 (0.0248)	
training:	Epoch: [14][20/817]	Loss 0.5398 (0.0505)	
training:	Epoch: [14][21/817]	Loss 0.0132 (0.0487)	
training:	Epoch: [14][22/817]	Loss 0.0468 (0.0486)	
training:	Epoch: [14][23/817]	Loss 0.6014 (0.0727)	
training:	Epoch: [14][24/817]	Loss 0.0121 (0.0701)	
training:	Epoch: [14][25/817]	Loss 0.0127 (0.0678)	
training:	Epoch: [14][26/817]	Loss 0.0108 (0.0657)	
training:	Epoch: [14][27/817]	Loss 0.0130 (0.0637)	
training:	Epoch: [14][28/817]	Loss 0.0140 (0.0619)	
training:	Epoch: [14][29/817]	Loss 0.0121 (0.0602)	
training:	Epoch: [14][30/817]	Loss 0.0155 (0.0587)	
training:	Epoch: [14][31/817]	Loss 0.5006 (0.0730)	
training:	Epoch: [14][32/817]	Loss 0.0136 (0.0711)	
training:	Epoch: [14][33/817]	Loss 0.0107 (0.0693)	
training:	Epoch: [14][34/817]	Loss 0.0142 (0.0677)	
training:	Epoch: [14][35/817]	Loss 0.0156 (0.0662)	
training:	Epoch: [14][36/817]	Loss 0.0122 (0.0647)	
training:	Epoch: [14][37/817]	Loss 0.0249 (0.0636)	
training:	Epoch: [14][38/817]	Loss 0.0128 (0.0623)	
training:	Epoch: [14][39/817]	Loss 0.0174 (0.0611)	
training:	Epoch: [14][40/817]	Loss 0.0159 (0.0600)	
training:	Epoch: [14][41/817]	Loss 0.0103 (0.0588)	
training:	Epoch: [14][42/817]	Loss 0.0119 (0.0577)	
training:	Epoch: [14][43/817]	Loss 0.0131 (0.0566)	
training:	Epoch: [14][44/817]	Loss 0.0113 (0.0556)	
training:	Epoch: [14][45/817]	Loss 0.0123 (0.0546)	
training:	Epoch: [14][46/817]	Loss 0.0123 (0.0537)	
training:	Epoch: [14][47/817]	Loss 0.0127 (0.0528)	
training:	Epoch: [14][48/817]	Loss 0.0131 (0.0520)	
training:	Epoch: [14][49/817]	Loss 0.0339 (0.0516)	
training:	Epoch: [14][50/817]	Loss 0.0119 (0.0508)	
training:	Epoch: [14][51/817]	Loss 0.5511 (0.0607)	
training:	Epoch: [14][52/817]	Loss 0.5866 (0.0708)	
training:	Epoch: [14][53/817]	Loss 0.0119 (0.0697)	
training:	Epoch: [14][54/817]	Loss 0.0124 (0.0686)	
training:	Epoch: [14][55/817]	Loss 0.0122 (0.0676)	
training:	Epoch: [14][56/817]	Loss 0.0116 (0.0666)	
training:	Epoch: [14][57/817]	Loss 0.0167 (0.0657)	
training:	Epoch: [14][58/817]	Loss 0.0128 (0.0648)	
training:	Epoch: [14][59/817]	Loss 0.0125 (0.0639)	
training:	Epoch: [14][60/817]	Loss 0.0123 (0.0630)	
training:	Epoch: [14][61/817]	Loss 0.0125 (0.0622)	
training:	Epoch: [14][62/817]	Loss 0.0122 (0.0614)	
training:	Epoch: [14][63/817]	Loss 0.0117 (0.0606)	
training:	Epoch: [14][64/817]	Loss 0.0109 (0.0598)	
training:	Epoch: [14][65/817]	Loss 0.0111 (0.0591)	
training:	Epoch: [14][66/817]	Loss 0.0100 (0.0583)	
training:	Epoch: [14][67/817]	Loss 0.0140 (0.0577)	
training:	Epoch: [14][68/817]	Loss 0.0132 (0.0570)	
training:	Epoch: [14][69/817]	Loss 0.0128 (0.0564)	
training:	Epoch: [14][70/817]	Loss 0.0141 (0.0558)	
training:	Epoch: [14][71/817]	Loss 0.0188 (0.0553)	
training:	Epoch: [14][72/817]	Loss 0.0180 (0.0547)	
training:	Epoch: [14][73/817]	Loss 0.0121 (0.0542)	
training:	Epoch: [14][74/817]	Loss 0.5271 (0.0606)	
training:	Epoch: [14][75/817]	Loss 0.0129 (0.0599)	
training:	Epoch: [14][76/817]	Loss 0.0127 (0.0593)	
training:	Epoch: [14][77/817]	Loss 0.0145 (0.0587)	
training:	Epoch: [14][78/817]	Loss 0.0125 (0.0581)	
training:	Epoch: [14][79/817]	Loss 0.0108 (0.0575)	
training:	Epoch: [14][80/817]	Loss 0.0162 (0.0570)	
training:	Epoch: [14][81/817]	Loss 0.0131 (0.0565)	
training:	Epoch: [14][82/817]	Loss 0.0109 (0.0559)	
training:	Epoch: [14][83/817]	Loss 0.0142 (0.0554)	
training:	Epoch: [14][84/817]	Loss 0.0125 (0.0549)	
training:	Epoch: [14][85/817]	Loss 0.0126 (0.0544)	
training:	Epoch: [14][86/817]	Loss 0.0330 (0.0541)	
training:	Epoch: [14][87/817]	Loss 0.0153 (0.0537)	
training:	Epoch: [14][88/817]	Loss 0.0137 (0.0532)	
training:	Epoch: [14][89/817]	Loss 0.0139 (0.0528)	
training:	Epoch: [14][90/817]	Loss 0.0166 (0.0524)	
training:	Epoch: [14][91/817]	Loss 0.0160 (0.0520)	
training:	Epoch: [14][92/817]	Loss 0.5952 (0.0579)	
training:	Epoch: [14][93/817]	Loss 0.0180 (0.0575)	
training:	Epoch: [14][94/817]	Loss 0.0144 (0.0570)	
training:	Epoch: [14][95/817]	Loss 0.0115 (0.0565)	
training:	Epoch: [14][96/817]	Loss 0.0110 (0.0561)	
training:	Epoch: [14][97/817]	Loss 0.0120 (0.0556)	
training:	Epoch: [14][98/817]	Loss 0.0142 (0.0552)	
training:	Epoch: [14][99/817]	Loss 0.0118 (0.0548)	
training:	Epoch: [14][100/817]	Loss 0.0110 (0.0543)	
training:	Epoch: [14][101/817]	Loss 0.0124 (0.0539)	
training:	Epoch: [14][102/817]	Loss 0.0124 (0.0535)	
training:	Epoch: [14][103/817]	Loss 0.0181 (0.0531)	
training:	Epoch: [14][104/817]	Loss 0.0124 (0.0528)	
training:	Epoch: [14][105/817]	Loss 0.0096 (0.0523)	
training:	Epoch: [14][106/817]	Loss 0.0145 (0.0520)	
training:	Epoch: [14][107/817]	Loss 0.0114 (0.0516)	
training:	Epoch: [14][108/817]	Loss 0.5480 (0.0562)	
training:	Epoch: [14][109/817]	Loss 0.0171 (0.0558)	
training:	Epoch: [14][110/817]	Loss 0.0136 (0.0555)	
training:	Epoch: [14][111/817]	Loss 0.0132 (0.0551)	
training:	Epoch: [14][112/817]	Loss 0.0126 (0.0547)	
training:	Epoch: [14][113/817]	Loss 0.4813 (0.0585)	
training:	Epoch: [14][114/817]	Loss 0.0129 (0.0581)	
training:	Epoch: [14][115/817]	Loss 0.0106 (0.0577)	
training:	Epoch: [14][116/817]	Loss 0.0218 (0.0574)	
training:	Epoch: [14][117/817]	Loss 0.0124 (0.0570)	
training:	Epoch: [14][118/817]	Loss 0.0112 (0.0566)	
training:	Epoch: [14][119/817]	Loss 0.0255 (0.0563)	
training:	Epoch: [14][120/817]	Loss 0.0127 (0.0560)	
training:	Epoch: [14][121/817]	Loss 0.0126 (0.0556)	
training:	Epoch: [14][122/817]	Loss 0.0131 (0.0553)	
training:	Epoch: [14][123/817]	Loss 0.4950 (0.0588)	
training:	Epoch: [14][124/817]	Loss 0.0183 (0.0585)	
training:	Epoch: [14][125/817]	Loss 0.0129 (0.0581)	
training:	Epoch: [14][126/817]	Loss 0.0166 (0.0578)	
training:	Epoch: [14][127/817]	Loss 0.0215 (0.0575)	
training:	Epoch: [14][128/817]	Loss 0.0111 (0.0572)	
training:	Epoch: [14][129/817]	Loss 0.0127 (0.0568)	
training:	Epoch: [14][130/817]	Loss 0.0134 (0.0565)	
training:	Epoch: [14][131/817]	Loss 0.0120 (0.0561)	
training:	Epoch: [14][132/817]	Loss 0.2583 (0.0577)	
training:	Epoch: [14][133/817]	Loss 0.0112 (0.0573)	
training:	Epoch: [14][134/817]	Loss 0.0116 (0.0570)	
training:	Epoch: [14][135/817]	Loss 0.0273 (0.0568)	
training:	Epoch: [14][136/817]	Loss 0.0119 (0.0564)	
training:	Epoch: [14][137/817]	Loss 0.0118 (0.0561)	
training:	Epoch: [14][138/817]	Loss 0.0123 (0.0558)	
training:	Epoch: [14][139/817]	Loss 0.0155 (0.0555)	
training:	Epoch: [14][140/817]	Loss 0.0159 (0.0552)	
training:	Epoch: [14][141/817]	Loss 0.0118 (0.0549)	
training:	Epoch: [14][142/817]	Loss 0.0147 (0.0546)	
training:	Epoch: [14][143/817]	Loss 0.0145 (0.0543)	
training:	Epoch: [14][144/817]	Loss 0.4272 (0.0569)	
training:	Epoch: [14][145/817]	Loss 0.0128 (0.0566)	
training:	Epoch: [14][146/817]	Loss 0.0143 (0.0563)	
training:	Epoch: [14][147/817]	Loss 0.0145 (0.0561)	
training:	Epoch: [14][148/817]	Loss 0.0114 (0.0558)	
training:	Epoch: [14][149/817]	Loss 0.0159 (0.0555)	
training:	Epoch: [14][150/817]	Loss 0.0129 (0.0552)	
training:	Epoch: [14][151/817]	Loss 0.0113 (0.0549)	
training:	Epoch: [14][152/817]	Loss 0.0139 (0.0546)	
training:	Epoch: [14][153/817]	Loss 0.0135 (0.0544)	
training:	Epoch: [14][154/817]	Loss 0.0124 (0.0541)	
training:	Epoch: [14][155/817]	Loss 0.0112 (0.0538)	
training:	Epoch: [14][156/817]	Loss 0.0152 (0.0536)	
training:	Epoch: [14][157/817]	Loss 0.0123 (0.0533)	
training:	Epoch: [14][158/817]	Loss 0.0094 (0.0530)	
training:	Epoch: [14][159/817]	Loss 0.0116 (0.0528)	
training:	Epoch: [14][160/817]	Loss 0.0224 (0.0526)	
training:	Epoch: [14][161/817]	Loss 0.1814 (0.0534)	
training:	Epoch: [14][162/817]	Loss 0.0202 (0.0532)	
training:	Epoch: [14][163/817]	Loss 0.0105 (0.0529)	
training:	Epoch: [14][164/817]	Loss 0.0113 (0.0527)	
training:	Epoch: [14][165/817]	Loss 0.0113 (0.0524)	
training:	Epoch: [14][166/817]	Loss 0.5217 (0.0552)	
training:	Epoch: [14][167/817]	Loss 0.5265 (0.0581)	
training:	Epoch: [14][168/817]	Loss 0.0117 (0.0578)	
training:	Epoch: [14][169/817]	Loss 0.0203 (0.0576)	
training:	Epoch: [14][170/817]	Loss 0.0129 (0.0573)	
training:	Epoch: [14][171/817]	Loss 0.0143 (0.0570)	
training:	Epoch: [14][172/817]	Loss 0.0101 (0.0568)	
training:	Epoch: [14][173/817]	Loss 0.0100 (0.0565)	
training:	Epoch: [14][174/817]	Loss 0.5574 (0.0594)	
training:	Epoch: [14][175/817]	Loss 0.0148 (0.0591)	
training:	Epoch: [14][176/817]	Loss 0.5849 (0.0621)	
training:	Epoch: [14][177/817]	Loss 0.0161 (0.0619)	
training:	Epoch: [14][178/817]	Loss 0.0132 (0.0616)	
training:	Epoch: [14][179/817]	Loss 0.0144 (0.0613)	
training:	Epoch: [14][180/817]	Loss 0.0119 (0.0610)	
training:	Epoch: [14][181/817]	Loss 0.0114 (0.0608)	
training:	Epoch: [14][182/817]	Loss 0.0127 (0.0605)	
training:	Epoch: [14][183/817]	Loss 0.0145 (0.0603)	
training:	Epoch: [14][184/817]	Loss 0.0108 (0.0600)	
training:	Epoch: [14][185/817]	Loss 0.0115 (0.0597)	
training:	Epoch: [14][186/817]	Loss 0.0124 (0.0595)	
training:	Epoch: [14][187/817]	Loss 0.0149 (0.0592)	
training:	Epoch: [14][188/817]	Loss 1.0944 (0.0647)	
training:	Epoch: [14][189/817]	Loss 0.0120 (0.0645)	
training:	Epoch: [14][190/817]	Loss 0.0167 (0.0642)	
training:	Epoch: [14][191/817]	Loss 0.0118 (0.0639)	
training:	Epoch: [14][192/817]	Loss 0.0123 (0.0637)	
training:	Epoch: [14][193/817]	Loss 0.0174 (0.0634)	
training:	Epoch: [14][194/817]	Loss 0.0154 (0.0632)	
training:	Epoch: [14][195/817]	Loss 0.0116 (0.0629)	
training:	Epoch: [14][196/817]	Loss 0.0181 (0.0627)	
training:	Epoch: [14][197/817]	Loss 0.0136 (0.0624)	
training:	Epoch: [14][198/817]	Loss 0.0137 (0.0622)	
training:	Epoch: [14][199/817]	Loss 0.5487 (0.0646)	
training:	Epoch: [14][200/817]	Loss 0.0170 (0.0644)	
training:	Epoch: [14][201/817]	Loss 0.0123 (0.0641)	
training:	Epoch: [14][202/817]	Loss 0.0158 (0.0639)	
training:	Epoch: [14][203/817]	Loss 0.0139 (0.0636)	
training:	Epoch: [14][204/817]	Loss 0.0138 (0.0634)	
training:	Epoch: [14][205/817]	Loss 0.0132 (0.0632)	
training:	Epoch: [14][206/817]	Loss 0.0349 (0.0630)	
training:	Epoch: [14][207/817]	Loss 0.0116 (0.0628)	
training:	Epoch: [14][208/817]	Loss 0.0138 (0.0625)	
training:	Epoch: [14][209/817]	Loss 1.0299 (0.0672)	
training:	Epoch: [14][210/817]	Loss 0.0124 (0.0669)	
training:	Epoch: [14][211/817]	Loss 0.0156 (0.0667)	
training:	Epoch: [14][212/817]	Loss 0.0161 (0.0664)	
training:	Epoch: [14][213/817]	Loss 0.1045 (0.0666)	
training:	Epoch: [14][214/817]	Loss 0.0133 (0.0664)	
training:	Epoch: [14][215/817]	Loss 0.0099 (0.0661)	
training:	Epoch: [14][216/817]	Loss 0.0369 (0.0660)	
training:	Epoch: [14][217/817]	Loss 0.0983 (0.0661)	
training:	Epoch: [14][218/817]	Loss 0.0133 (0.0659)	
training:	Epoch: [14][219/817]	Loss 0.0120 (0.0656)	
training:	Epoch: [14][220/817]	Loss 0.0133 (0.0654)	
training:	Epoch: [14][221/817]	Loss 0.0163 (0.0652)	
training:	Epoch: [14][222/817]	Loss 0.0171 (0.0649)	
training:	Epoch: [14][223/817]	Loss 0.5930 (0.0673)	
training:	Epoch: [14][224/817]	Loss 0.0115 (0.0671)	
training:	Epoch: [14][225/817]	Loss 0.0226 (0.0669)	
training:	Epoch: [14][226/817]	Loss 0.0144 (0.0666)	
training:	Epoch: [14][227/817]	Loss 0.0147 (0.0664)	
training:	Epoch: [14][228/817]	Loss 0.0167 (0.0662)	
training:	Epoch: [14][229/817]	Loss 0.0123 (0.0659)	
training:	Epoch: [14][230/817]	Loss 0.0130 (0.0657)	
training:	Epoch: [14][231/817]	Loss 0.0242 (0.0655)	
training:	Epoch: [14][232/817]	Loss 0.0199 (0.0653)	
training:	Epoch: [14][233/817]	Loss 0.0228 (0.0652)	
training:	Epoch: [14][234/817]	Loss 0.0148 (0.0649)	
training:	Epoch: [14][235/817]	Loss 0.0117 (0.0647)	
training:	Epoch: [14][236/817]	Loss 0.0137 (0.0645)	
training:	Epoch: [14][237/817]	Loss 0.0113 (0.0643)	
training:	Epoch: [14][238/817]	Loss 0.0139 (0.0641)	
training:	Epoch: [14][239/817]	Loss 0.0150 (0.0639)	
training:	Epoch: [14][240/817]	Loss 0.0510 (0.0638)	
training:	Epoch: [14][241/817]	Loss 0.0182 (0.0636)	
training:	Epoch: [14][242/817]	Loss 0.0159 (0.0634)	
training:	Epoch: [14][243/817]	Loss 0.0125 (0.0632)	
training:	Epoch: [14][244/817]	Loss 0.0123 (0.0630)	
training:	Epoch: [14][245/817]	Loss 0.0175 (0.0628)	
training:	Epoch: [14][246/817]	Loss 0.0130 (0.0626)	
training:	Epoch: [14][247/817]	Loss 0.0124 (0.0624)	
training:	Epoch: [14][248/817]	Loss 0.0148 (0.0622)	
training:	Epoch: [14][249/817]	Loss 0.0125 (0.0620)	
training:	Epoch: [14][250/817]	Loss 0.0119 (0.0618)	
training:	Epoch: [14][251/817]	Loss 0.8823 (0.0651)	
training:	Epoch: [14][252/817]	Loss 0.0129 (0.0649)	
training:	Epoch: [14][253/817]	Loss 0.0119 (0.0647)	
training:	Epoch: [14][254/817]	Loss 0.0111 (0.0645)	
training:	Epoch: [14][255/817]	Loss 0.0348 (0.0643)	
training:	Epoch: [14][256/817]	Loss 0.5937 (0.0664)	
training:	Epoch: [14][257/817]	Loss 0.0140 (0.0662)	
training:	Epoch: [14][258/817]	Loss 0.0176 (0.0660)	
training:	Epoch: [14][259/817]	Loss 0.0176 (0.0658)	
training:	Epoch: [14][260/817]	Loss 0.0192 (0.0657)	
training:	Epoch: [14][261/817]	Loss 0.0126 (0.0655)	
training:	Epoch: [14][262/817]	Loss 0.0145 (0.0653)	
training:	Epoch: [14][263/817]	Loss 0.0102 (0.0650)	
training:	Epoch: [14][264/817]	Loss 0.0178 (0.0649)	
training:	Epoch: [14][265/817]	Loss 0.0120 (0.0647)	
training:	Epoch: [14][266/817]	Loss 0.0166 (0.0645)	
training:	Epoch: [14][267/817]	Loss 0.0183 (0.0643)	
training:	Epoch: [14][268/817]	Loss 0.0124 (0.0641)	
training:	Epoch: [14][269/817]	Loss 0.0204 (0.0640)	
training:	Epoch: [14][270/817]	Loss 0.0123 (0.0638)	
training:	Epoch: [14][271/817]	Loss 0.0120 (0.0636)	
training:	Epoch: [14][272/817]	Loss 0.0128 (0.0634)	
training:	Epoch: [14][273/817]	Loss 0.0162 (0.0632)	
training:	Epoch: [14][274/817]	Loss 0.0126 (0.0630)	
training:	Epoch: [14][275/817]	Loss 0.0134 (0.0629)	
training:	Epoch: [14][276/817]	Loss 0.0114 (0.0627)	
training:	Epoch: [14][277/817]	Loss 0.0142 (0.0625)	
training:	Epoch: [14][278/817]	Loss 0.0136 (0.0623)	
training:	Epoch: [14][279/817]	Loss 0.0118 (0.0621)	
training:	Epoch: [14][280/817]	Loss 0.0187 (0.0620)	
training:	Epoch: [14][281/817]	Loss 0.0124 (0.0618)	
training:	Epoch: [14][282/817]	Loss 0.5487 (0.0635)	
training:	Epoch: [14][283/817]	Loss 0.0158 (0.0634)	
training:	Epoch: [14][284/817]	Loss 0.5932 (0.0652)	
training:	Epoch: [14][285/817]	Loss 0.0217 (0.0651)	
training:	Epoch: [14][286/817]	Loss 0.0124 (0.0649)	
training:	Epoch: [14][287/817]	Loss 0.0130 (0.0647)	
training:	Epoch: [14][288/817]	Loss 0.5937 (0.0665)	
training:	Epoch: [14][289/817]	Loss 0.0122 (0.0664)	
training:	Epoch: [14][290/817]	Loss 0.5926 (0.0682)	
training:	Epoch: [14][291/817]	Loss 0.0133 (0.0680)	
training:	Epoch: [14][292/817]	Loss 0.0158 (0.0678)	
training:	Epoch: [14][293/817]	Loss 0.0126 (0.0676)	
training:	Epoch: [14][294/817]	Loss 0.0125 (0.0674)	
training:	Epoch: [14][295/817]	Loss 0.0134 (0.0672)	
training:	Epoch: [14][296/817]	Loss 0.0126 (0.0671)	
training:	Epoch: [14][297/817]	Loss 0.0132 (0.0669)	
training:	Epoch: [14][298/817]	Loss 0.0158 (0.0667)	
training:	Epoch: [14][299/817]	Loss 0.0159 (0.0665)	
training:	Epoch: [14][300/817]	Loss 0.0142 (0.0664)	
training:	Epoch: [14][301/817]	Loss 0.0114 (0.0662)	
training:	Epoch: [14][302/817]	Loss 0.0160 (0.0660)	
training:	Epoch: [14][303/817]	Loss 0.0153 (0.0658)	
training:	Epoch: [14][304/817]	Loss 0.0263 (0.0657)	
training:	Epoch: [14][305/817]	Loss 0.5994 (0.0675)	
training:	Epoch: [14][306/817]	Loss 0.0112 (0.0673)	
training:	Epoch: [14][307/817]	Loss 0.0120 (0.0671)	
training:	Epoch: [14][308/817]	Loss 0.0157 (0.0669)	
training:	Epoch: [14][309/817]	Loss 0.0117 (0.0668)	
training:	Epoch: [14][310/817]	Loss 0.0127 (0.0666)	
training:	Epoch: [14][311/817]	Loss 0.0332 (0.0665)	
training:	Epoch: [14][312/817]	Loss 0.0122 (0.0663)	
training:	Epoch: [14][313/817]	Loss 0.0157 (0.0661)	
training:	Epoch: [14][314/817]	Loss 0.5867 (0.0678)	
training:	Epoch: [14][315/817]	Loss 0.0124 (0.0676)	
training:	Epoch: [14][316/817]	Loss 0.0113 (0.0674)	
training:	Epoch: [14][317/817]	Loss 0.0119 (0.0673)	
training:	Epoch: [14][318/817]	Loss 0.0103 (0.0671)	
training:	Epoch: [14][319/817]	Loss 0.0206 (0.0669)	
training:	Epoch: [14][320/817]	Loss 0.0116 (0.0668)	
training:	Epoch: [14][321/817]	Loss 0.0132 (0.0666)	
training:	Epoch: [14][322/817]	Loss 0.0119 (0.0664)	
training:	Epoch: [14][323/817]	Loss 0.0136 (0.0663)	
training:	Epoch: [14][324/817]	Loss 0.0112 (0.0661)	
training:	Epoch: [14][325/817]	Loss 0.0116 (0.0659)	
training:	Epoch: [14][326/817]	Loss 0.0169 (0.0658)	
training:	Epoch: [14][327/817]	Loss 0.0177 (0.0656)	
training:	Epoch: [14][328/817]	Loss 0.0163 (0.0655)	
training:	Epoch: [14][329/817]	Loss 0.0166 (0.0653)	
training:	Epoch: [14][330/817]	Loss 0.0127 (0.0652)	
training:	Epoch: [14][331/817]	Loss 0.0510 (0.0651)	
training:	Epoch: [14][332/817]	Loss 0.0139 (0.0650)	
training:	Epoch: [14][333/817]	Loss 0.0211 (0.0648)	
training:	Epoch: [14][334/817]	Loss 0.0236 (0.0647)	
training:	Epoch: [14][335/817]	Loss 0.1462 (0.0650)	
training:	Epoch: [14][336/817]	Loss 0.0129 (0.0648)	
training:	Epoch: [14][337/817]	Loss 0.0614 (0.0648)	
training:	Epoch: [14][338/817]	Loss 0.0126 (0.0646)	
training:	Epoch: [14][339/817]	Loss 0.0113 (0.0645)	
training:	Epoch: [14][340/817]	Loss 0.0136 (0.0643)	
training:	Epoch: [14][341/817]	Loss 0.0469 (0.0643)	
training:	Epoch: [14][342/817]	Loss 0.0158 (0.0641)	
training:	Epoch: [14][343/817]	Loss 0.0135 (0.0640)	
training:	Epoch: [14][344/817]	Loss 0.0122 (0.0639)	
training:	Epoch: [14][345/817]	Loss 0.0130 (0.0637)	
training:	Epoch: [14][346/817]	Loss 0.0129 (0.0636)	
training:	Epoch: [14][347/817]	Loss 0.0117 (0.0634)	
training:	Epoch: [14][348/817]	Loss 0.0107 (0.0633)	
training:	Epoch: [14][349/817]	Loss 0.0235 (0.0631)	
training:	Epoch: [14][350/817]	Loss 0.0146 (0.0630)	
training:	Epoch: [14][351/817]	Loss 0.0143 (0.0629)	
training:	Epoch: [14][352/817]	Loss 0.0163 (0.0627)	
training:	Epoch: [14][353/817]	Loss 0.0147 (0.0626)	
training:	Epoch: [14][354/817]	Loss 0.0126 (0.0625)	
training:	Epoch: [14][355/817]	Loss 0.0131 (0.0623)	
training:	Epoch: [14][356/817]	Loss 0.0141 (0.0622)	
training:	Epoch: [14][357/817]	Loss 0.1184 (0.0623)	
training:	Epoch: [14][358/817]	Loss 0.0170 (0.0622)	
training:	Epoch: [14][359/817]	Loss 0.1615 (0.0625)	
training:	Epoch: [14][360/817]	Loss 0.0108 (0.0623)	
training:	Epoch: [14][361/817]	Loss 0.0121 (0.0622)	
training:	Epoch: [14][362/817]	Loss 0.0127 (0.0621)	
training:	Epoch: [14][363/817]	Loss 0.0110 (0.0619)	
training:	Epoch: [14][364/817]	Loss 0.0213 (0.0618)	
training:	Epoch: [14][365/817]	Loss 0.0161 (0.0617)	
training:	Epoch: [14][366/817]	Loss 0.0170 (0.0616)	
training:	Epoch: [14][367/817]	Loss 0.0155 (0.0614)	
training:	Epoch: [14][368/817]	Loss 0.0104 (0.0613)	
training:	Epoch: [14][369/817]	Loss 0.0129 (0.0612)	
training:	Epoch: [14][370/817]	Loss 0.0154 (0.0610)	
training:	Epoch: [14][371/817]	Loss 0.0142 (0.0609)	
training:	Epoch: [14][372/817]	Loss 0.0106 (0.0608)	
training:	Epoch: [14][373/817]	Loss 0.0251 (0.0607)	
training:	Epoch: [14][374/817]	Loss 0.5531 (0.0620)	
training:	Epoch: [14][375/817]	Loss 0.1997 (0.0624)	
training:	Epoch: [14][376/817]	Loss 0.0450 (0.0623)	
training:	Epoch: [14][377/817]	Loss 0.0150 (0.0622)	
training:	Epoch: [14][378/817]	Loss 0.0183 (0.0621)	
training:	Epoch: [14][379/817]	Loss 0.8252 (0.0641)	
training:	Epoch: [14][380/817]	Loss 0.0354 (0.0640)	
training:	Epoch: [14][381/817]	Loss 0.0140 (0.0639)	
training:	Epoch: [14][382/817]	Loss 0.0180 (0.0638)	
training:	Epoch: [14][383/817]	Loss 0.0128 (0.0636)	
training:	Epoch: [14][384/817]	Loss 0.0115 (0.0635)	
training:	Epoch: [14][385/817]	Loss 0.0144 (0.0634)	
training:	Epoch: [14][386/817]	Loss 0.0155 (0.0633)	
training:	Epoch: [14][387/817]	Loss 0.0119 (0.0631)	
training:	Epoch: [14][388/817]	Loss 0.0142 (0.0630)	
training:	Epoch: [14][389/817]	Loss 0.2041 (0.0634)	
training:	Epoch: [14][390/817]	Loss 0.0160 (0.0632)	
training:	Epoch: [14][391/817]	Loss 0.0135 (0.0631)	
training:	Epoch: [14][392/817]	Loss 0.0105 (0.0630)	
training:	Epoch: [14][393/817]	Loss 0.0107 (0.0628)	
training:	Epoch: [14][394/817]	Loss 0.0138 (0.0627)	
training:	Epoch: [14][395/817]	Loss 0.5941 (0.0641)	
training:	Epoch: [14][396/817]	Loss 0.0118 (0.0639)	
training:	Epoch: [14][397/817]	Loss 0.0114 (0.0638)	
training:	Epoch: [14][398/817]	Loss 0.0184 (0.0637)	
training:	Epoch: [14][399/817]	Loss 0.0149 (0.0636)	
training:	Epoch: [14][400/817]	Loss 0.0309 (0.0635)	
training:	Epoch: [14][401/817]	Loss 0.0109 (0.0634)	
training:	Epoch: [14][402/817]	Loss 0.0315 (0.0633)	
training:	Epoch: [14][403/817]	Loss 0.0165 (0.0632)	
training:	Epoch: [14][404/817]	Loss 0.0137 (0.0630)	
training:	Epoch: [14][405/817]	Loss 0.5507 (0.0642)	
training:	Epoch: [14][406/817]	Loss 0.5990 (0.0656)	
training:	Epoch: [14][407/817]	Loss 0.0171 (0.0654)	
training:	Epoch: [14][408/817]	Loss 0.0216 (0.0653)	
training:	Epoch: [14][409/817]	Loss 0.0121 (0.0652)	
training:	Epoch: [14][410/817]	Loss 0.0104 (0.0651)	
training:	Epoch: [14][411/817]	Loss 0.0199 (0.0650)	
training:	Epoch: [14][412/817]	Loss 0.0123 (0.0648)	
training:	Epoch: [14][413/817]	Loss 0.0111 (0.0647)	
training:	Epoch: [14][414/817]	Loss 0.2509 (0.0651)	
training:	Epoch: [14][415/817]	Loss 0.0108 (0.0650)	
training:	Epoch: [14][416/817]	Loss 0.0122 (0.0649)	
training:	Epoch: [14][417/817]	Loss 0.0138 (0.0648)	
training:	Epoch: [14][418/817]	Loss 0.0113 (0.0646)	
training:	Epoch: [14][419/817]	Loss 0.0137 (0.0645)	
training:	Epoch: [14][420/817]	Loss 0.0115 (0.0644)	
training:	Epoch: [14][421/817]	Loss 0.0183 (0.0643)	
training:	Epoch: [14][422/817]	Loss 0.0155 (0.0642)	
training:	Epoch: [14][423/817]	Loss 0.0118 (0.0640)	
training:	Epoch: [14][424/817]	Loss 0.0101 (0.0639)	
training:	Epoch: [14][425/817]	Loss 0.0149 (0.0638)	
training:	Epoch: [14][426/817]	Loss 0.0111 (0.0637)	
training:	Epoch: [14][427/817]	Loss 0.0109 (0.0636)	
training:	Epoch: [14][428/817]	Loss 0.0205 (0.0635)	
training:	Epoch: [14][429/817]	Loss 0.0111 (0.0633)	
training:	Epoch: [14][430/817]	Loss 0.0151 (0.0632)	
training:	Epoch: [14][431/817]	Loss 0.0130 (0.0631)	
training:	Epoch: [14][432/817]	Loss 0.0108 (0.0630)	
training:	Epoch: [14][433/817]	Loss 0.0133 (0.0629)	
training:	Epoch: [14][434/817]	Loss 0.0112 (0.0627)	
training:	Epoch: [14][435/817]	Loss 0.0155 (0.0626)	
training:	Epoch: [14][436/817]	Loss 0.0116 (0.0625)	
training:	Epoch: [14][437/817]	Loss 0.0148 (0.0624)	
training:	Epoch: [14][438/817]	Loss 0.0106 (0.0623)	
training:	Epoch: [14][439/817]	Loss 0.0203 (0.0622)	
training:	Epoch: [14][440/817]	Loss 0.0161 (0.0621)	
training:	Epoch: [14][441/817]	Loss 0.5497 (0.0632)	
training:	Epoch: [14][442/817]	Loss 0.0141 (0.0631)	
training:	Epoch: [14][443/817]	Loss 0.0135 (0.0630)	
training:	Epoch: [14][444/817]	Loss 0.0120 (0.0629)	
training:	Epoch: [14][445/817]	Loss 0.0115 (0.0627)	
training:	Epoch: [14][446/817]	Loss 0.0123 (0.0626)	
training:	Epoch: [14][447/817]	Loss 0.0157 (0.0625)	
training:	Epoch: [14][448/817]	Loss 0.0121 (0.0624)	
training:	Epoch: [14][449/817]	Loss 0.0125 (0.0623)	
training:	Epoch: [14][450/817]	Loss 0.0114 (0.0622)	
training:	Epoch: [14][451/817]	Loss 0.0398 (0.0621)	
training:	Epoch: [14][452/817]	Loss 0.0115 (0.0620)	
training:	Epoch: [14][453/817]	Loss 0.0254 (0.0619)	
training:	Epoch: [14][454/817]	Loss 0.0136 (0.0618)	
training:	Epoch: [14][455/817]	Loss 0.0125 (0.0617)	
training:	Epoch: [14][456/817]	Loss 0.0181 (0.0616)	
training:	Epoch: [14][457/817]	Loss 0.0157 (0.0615)	
training:	Epoch: [14][458/817]	Loss 0.0103 (0.0614)	
training:	Epoch: [14][459/817]	Loss 0.0116 (0.0613)	
training:	Epoch: [14][460/817]	Loss 0.5417 (0.0624)	
training:	Epoch: [14][461/817]	Loss 0.0127 (0.0623)	
training:	Epoch: [14][462/817]	Loss 0.0109 (0.0621)	
training:	Epoch: [14][463/817]	Loss 0.0143 (0.0620)	
training:	Epoch: [14][464/817]	Loss 0.0105 (0.0619)	
training:	Epoch: [14][465/817]	Loss 0.0116 (0.0618)	
training:	Epoch: [14][466/817]	Loss 0.0181 (0.0617)	
training:	Epoch: [14][467/817]	Loss 0.0128 (0.0616)	
training:	Epoch: [14][468/817]	Loss 0.0142 (0.0615)	
training:	Epoch: [14][469/817]	Loss 0.5548 (0.0626)	
training:	Epoch: [14][470/817]	Loss 0.0130 (0.0625)	
training:	Epoch: [14][471/817]	Loss 0.0111 (0.0624)	
training:	Epoch: [14][472/817]	Loss 0.0114 (0.0622)	
training:	Epoch: [14][473/817]	Loss 0.0164 (0.0622)	
training:	Epoch: [14][474/817]	Loss 0.0107 (0.0620)	
training:	Epoch: [14][475/817]	Loss 0.0103 (0.0619)	
training:	Epoch: [14][476/817]	Loss 0.0368 (0.0619)	
training:	Epoch: [14][477/817]	Loss 0.0172 (0.0618)	
training:	Epoch: [14][478/817]	Loss 0.0158 (0.0617)	
training:	Epoch: [14][479/817]	Loss 0.0221 (0.0616)	
training:	Epoch: [14][480/817]	Loss 0.0503 (0.0616)	
training:	Epoch: [14][481/817]	Loss 0.0118 (0.0615)	
training:	Epoch: [14][482/817]	Loss 0.0180 (0.0614)	
training:	Epoch: [14][483/817]	Loss 0.0202 (0.0613)	
training:	Epoch: [14][484/817]	Loss 0.0247 (0.0612)	
training:	Epoch: [14][485/817]	Loss 0.0121 (0.0611)	
training:	Epoch: [14][486/817]	Loss 0.0140 (0.0610)	
training:	Epoch: [14][487/817]	Loss 0.6867 (0.0623)	
training:	Epoch: [14][488/817]	Loss 0.0131 (0.0622)	
training:	Epoch: [14][489/817]	Loss 0.0117 (0.0621)	
training:	Epoch: [14][490/817]	Loss 0.0134 (0.0620)	
training:	Epoch: [14][491/817]	Loss 0.0111 (0.0619)	
training:	Epoch: [14][492/817]	Loss 0.0108 (0.0618)	
training:	Epoch: [14][493/817]	Loss 0.0113 (0.0617)	
training:	Epoch: [14][494/817]	Loss 0.0193 (0.0616)	
training:	Epoch: [14][495/817]	Loss 0.0101 (0.0615)	
training:	Epoch: [14][496/817]	Loss 0.0192 (0.0614)	
training:	Epoch: [14][497/817]	Loss 0.0145 (0.0613)	
training:	Epoch: [14][498/817]	Loss 0.0115 (0.0612)	
training:	Epoch: [14][499/817]	Loss 0.0166 (0.0611)	
training:	Epoch: [14][500/817]	Loss 0.0128 (0.0610)	
training:	Epoch: [14][501/817]	Loss 0.0141 (0.0610)	
training:	Epoch: [14][502/817]	Loss 0.0117 (0.0609)	
training:	Epoch: [14][503/817]	Loss 0.0129 (0.0608)	
training:	Epoch: [14][504/817]	Loss 0.0133 (0.0607)	
training:	Epoch: [14][505/817]	Loss 0.0129 (0.0606)	
training:	Epoch: [14][506/817]	Loss 0.4883 (0.0614)	
training:	Epoch: [14][507/817]	Loss 0.0108 (0.0613)	
training:	Epoch: [14][508/817]	Loss 0.0256 (0.0612)	
training:	Epoch: [14][509/817]	Loss 0.0113 (0.0611)	
training:	Epoch: [14][510/817]	Loss 0.0146 (0.0611)	
training:	Epoch: [14][511/817]	Loss 0.0189 (0.0610)	
training:	Epoch: [14][512/817]	Loss 0.0109 (0.0609)	
training:	Epoch: [14][513/817]	Loss 0.0099 (0.0608)	
training:	Epoch: [14][514/817]	Loss 0.0119 (0.0607)	
training:	Epoch: [14][515/817]	Loss 0.0147 (0.0606)	
training:	Epoch: [14][516/817]	Loss 0.0160 (0.0605)	
training:	Epoch: [14][517/817]	Loss 0.0115 (0.0604)	
training:	Epoch: [14][518/817]	Loss 0.0147 (0.0603)	
training:	Epoch: [14][519/817]	Loss 0.0109 (0.0602)	
training:	Epoch: [14][520/817]	Loss 0.5437 (0.0612)	
training:	Epoch: [14][521/817]	Loss 0.0213 (0.0611)	
training:	Epoch: [14][522/817]	Loss 0.0125 (0.0610)	
training:	Epoch: [14][523/817]	Loss 0.0206 (0.0609)	
training:	Epoch: [14][524/817]	Loss 0.0615 (0.0609)	
training:	Epoch: [14][525/817]	Loss 0.0167 (0.0608)	
training:	Epoch: [14][526/817]	Loss 0.0102 (0.0607)	
training:	Epoch: [14][527/817]	Loss 0.0136 (0.0606)	
training:	Epoch: [14][528/817]	Loss 0.0117 (0.0605)	
training:	Epoch: [14][529/817]	Loss 0.0811 (0.0606)	
training:	Epoch: [14][530/817]	Loss 0.0102 (0.0605)	
training:	Epoch: [14][531/817]	Loss 0.0510 (0.0605)	
training:	Epoch: [14][532/817]	Loss 0.0152 (0.0604)	
training:	Epoch: [14][533/817]	Loss 0.0110 (0.0603)	
training:	Epoch: [14][534/817]	Loss 0.0105 (0.0602)	
training:	Epoch: [14][535/817]	Loss 0.0126 (0.0601)	
training:	Epoch: [14][536/817]	Loss 0.0108 (0.0600)	
training:	Epoch: [14][537/817]	Loss 0.5784 (0.0610)	
training:	Epoch: [14][538/817]	Loss 0.0112 (0.0609)	
training:	Epoch: [14][539/817]	Loss 0.0103 (0.0608)	
training:	Epoch: [14][540/817]	Loss 0.0125 (0.0607)	
training:	Epoch: [14][541/817]	Loss 0.5991 (0.0617)	
training:	Epoch: [14][542/817]	Loss 0.0129 (0.0616)	
training:	Epoch: [14][543/817]	Loss 0.0136 (0.0615)	
training:	Epoch: [14][544/817]	Loss 0.0108 (0.0614)	
training:	Epoch: [14][545/817]	Loss 0.0217 (0.0614)	
training:	Epoch: [14][546/817]	Loss 0.0122 (0.0613)	
training:	Epoch: [14][547/817]	Loss 0.0108 (0.0612)	
training:	Epoch: [14][548/817]	Loss 0.0101 (0.0611)	
training:	Epoch: [14][549/817]	Loss 0.0118 (0.0610)	
training:	Epoch: [14][550/817]	Loss 0.0113 (0.0609)	
training:	Epoch: [14][551/817]	Loss 0.0122 (0.0608)	
training:	Epoch: [14][552/817]	Loss 0.0162 (0.0607)	
training:	Epoch: [14][553/817]	Loss 0.0125 (0.0607)	
training:	Epoch: [14][554/817]	Loss 0.0123 (0.0606)	
training:	Epoch: [14][555/817]	Loss 0.0113 (0.0605)	
training:	Epoch: [14][556/817]	Loss 0.0108 (0.0604)	
training:	Epoch: [14][557/817]	Loss 0.0126 (0.0603)	
training:	Epoch: [14][558/817]	Loss 0.0106 (0.0602)	
training:	Epoch: [14][559/817]	Loss 0.0153 (0.0601)	
training:	Epoch: [14][560/817]	Loss 0.0119 (0.0600)	
training:	Epoch: [14][561/817]	Loss 0.0110 (0.0600)	
training:	Epoch: [14][562/817]	Loss 0.0121 (0.0599)	
training:	Epoch: [14][563/817]	Loss 0.0113 (0.0598)	
training:	Epoch: [14][564/817]	Loss 0.2177 (0.0601)	
training:	Epoch: [14][565/817]	Loss 0.0120 (0.0600)	
training:	Epoch: [14][566/817]	Loss 0.0122 (0.0599)	
training:	Epoch: [14][567/817]	Loss 0.0122 (0.0598)	
training:	Epoch: [14][568/817]	Loss 0.0113 (0.0597)	
training:	Epoch: [14][569/817]	Loss 0.0108 (0.0596)	
training:	Epoch: [14][570/817]	Loss 0.0158 (0.0596)	
training:	Epoch: [14][571/817]	Loss 0.0195 (0.0595)	
training:	Epoch: [14][572/817]	Loss 0.0134 (0.0594)	
training:	Epoch: [14][573/817]	Loss 0.0112 (0.0593)	
training:	Epoch: [14][574/817]	Loss 0.0165 (0.0593)	
training:	Epoch: [14][575/817]	Loss 0.0223 (0.0592)	
training:	Epoch: [14][576/817]	Loss 0.0145 (0.0591)	
training:	Epoch: [14][577/817]	Loss 0.0126 (0.0590)	
training:	Epoch: [14][578/817]	Loss 0.0102 (0.0589)	
training:	Epoch: [14][579/817]	Loss 0.0166 (0.0589)	
training:	Epoch: [14][580/817]	Loss 0.0125 (0.0588)	
training:	Epoch: [14][581/817]	Loss 0.0129 (0.0587)	
training:	Epoch: [14][582/817]	Loss 0.0132 (0.0586)	
training:	Epoch: [14][583/817]	Loss 0.0103 (0.0586)	
training:	Epoch: [14][584/817]	Loss 0.0104 (0.0585)	
training:	Epoch: [14][585/817]	Loss 0.0128 (0.0584)	
training:	Epoch: [14][586/817]	Loss 0.2601 (0.0587)	
training:	Epoch: [14][587/817]	Loss 0.0130 (0.0587)	
training:	Epoch: [14][588/817]	Loss 0.0114 (0.0586)	
training:	Epoch: [14][589/817]	Loss 0.5963 (0.0595)	
training:	Epoch: [14][590/817]	Loss 0.1787 (0.0597)	
training:	Epoch: [14][591/817]	Loss 0.0104 (0.0596)	
training:	Epoch: [14][592/817]	Loss 0.0127 (0.0595)	
training:	Epoch: [14][593/817]	Loss 0.0107 (0.0594)	
training:	Epoch: [14][594/817]	Loss 0.0118 (0.0594)	
training:	Epoch: [14][595/817]	Loss 0.0134 (0.0593)	
training:	Epoch: [14][596/817]	Loss 0.0098 (0.0592)	
training:	Epoch: [14][597/817]	Loss 0.0098 (0.0591)	
training:	Epoch: [14][598/817]	Loss 0.0128 (0.0590)	
training:	Epoch: [14][599/817]	Loss 0.0098 (0.0590)	
training:	Epoch: [14][600/817]	Loss 0.0099 (0.0589)	
training:	Epoch: [14][601/817]	Loss 0.0671 (0.0589)	
training:	Epoch: [14][602/817]	Loss 0.0190 (0.0588)	
training:	Epoch: [14][603/817]	Loss 0.0182 (0.0588)	
training:	Epoch: [14][604/817]	Loss 0.0348 (0.0587)	
training:	Epoch: [14][605/817]	Loss 0.6170 (0.0596)	
training:	Epoch: [14][606/817]	Loss 0.0118 (0.0596)	
training:	Epoch: [14][607/817]	Loss 0.0279 (0.0595)	
training:	Epoch: [14][608/817]	Loss 0.0416 (0.0595)	
training:	Epoch: [14][609/817]	Loss 0.1927 (0.0597)	
training:	Epoch: [14][610/817]	Loss 0.0122 (0.0596)	
training:	Epoch: [14][611/817]	Loss 0.5344 (0.0604)	
training:	Epoch: [14][612/817]	Loss 0.0117 (0.0603)	
training:	Epoch: [14][613/817]	Loss 0.0110 (0.0602)	
training:	Epoch: [14][614/817]	Loss 0.0136 (0.0602)	
training:	Epoch: [14][615/817]	Loss 0.0175 (0.0601)	
training:	Epoch: [14][616/817]	Loss 0.0117 (0.0600)	
training:	Epoch: [14][617/817]	Loss 0.0150 (0.0599)	
training:	Epoch: [14][618/817]	Loss 0.0144 (0.0599)	
training:	Epoch: [14][619/817]	Loss 0.0146 (0.0598)	
training:	Epoch: [14][620/817]	Loss 0.6030 (0.0607)	
training:	Epoch: [14][621/817]	Loss 0.0110 (0.0606)	
training:	Epoch: [14][622/817]	Loss 0.0144 (0.0605)	
training:	Epoch: [14][623/817]	Loss 0.0104 (0.0604)	
training:	Epoch: [14][624/817]	Loss 0.0121 (0.0604)	
training:	Epoch: [14][625/817]	Loss 0.0113 (0.0603)	
training:	Epoch: [14][626/817]	Loss 0.0167 (0.0602)	
training:	Epoch: [14][627/817]	Loss 0.0118 (0.0601)	
training:	Epoch: [14][628/817]	Loss 0.0147 (0.0601)	
training:	Epoch: [14][629/817]	Loss 0.0176 (0.0600)	
training:	Epoch: [14][630/817]	Loss 0.0100 (0.0599)	
training:	Epoch: [14][631/817]	Loss 0.0154 (0.0598)	
training:	Epoch: [14][632/817]	Loss 0.0112 (0.0598)	
training:	Epoch: [14][633/817]	Loss 0.0112 (0.0597)	
training:	Epoch: [14][634/817]	Loss 0.0115 (0.0596)	
training:	Epoch: [14][635/817]	Loss 0.0125 (0.0595)	
training:	Epoch: [14][636/817]	Loss 0.0126 (0.0595)	
training:	Epoch: [14][637/817]	Loss 0.5640 (0.0603)	
training:	Epoch: [14][638/817]	Loss 0.0130 (0.0602)	
training:	Epoch: [14][639/817]	Loss 0.0113 (0.0601)	
training:	Epoch: [14][640/817]	Loss 0.0221 (0.0601)	
training:	Epoch: [14][641/817]	Loss 0.0108 (0.0600)	
training:	Epoch: [14][642/817]	Loss 0.0215 (0.0599)	
training:	Epoch: [14][643/817]	Loss 0.0141 (0.0598)	
training:	Epoch: [14][644/817]	Loss 0.4258 (0.0604)	
training:	Epoch: [14][645/817]	Loss 0.0120 (0.0603)	
training:	Epoch: [14][646/817]	Loss 0.0183 (0.0603)	
training:	Epoch: [14][647/817]	Loss 0.0172 (0.0602)	
training:	Epoch: [14][648/817]	Loss 0.0114 (0.0601)	
training:	Epoch: [14][649/817]	Loss 0.0358 (0.0601)	
training:	Epoch: [14][650/817]	Loss 0.0164 (0.0600)	
training:	Epoch: [14][651/817]	Loss 0.6150 (0.0609)	
training:	Epoch: [14][652/817]	Loss 0.0184 (0.0608)	
training:	Epoch: [14][653/817]	Loss 0.3561 (0.0613)	
training:	Epoch: [14][654/817]	Loss 0.0103 (0.0612)	
training:	Epoch: [14][655/817]	Loss 0.0140 (0.0611)	
training:	Epoch: [14][656/817]	Loss 0.0180 (0.0611)	
training:	Epoch: [14][657/817]	Loss 0.0746 (0.0611)	
training:	Epoch: [14][658/817]	Loss 0.0158 (0.0610)	
training:	Epoch: [14][659/817]	Loss 0.0131 (0.0609)	
training:	Epoch: [14][660/817]	Loss 0.0118 (0.0609)	
training:	Epoch: [14][661/817]	Loss 0.0117 (0.0608)	
training:	Epoch: [14][662/817]	Loss 0.0114 (0.0607)	
training:	Epoch: [14][663/817]	Loss 0.0103 (0.0606)	
training:	Epoch: [14][664/817]	Loss 0.0128 (0.0606)	
training:	Epoch: [14][665/817]	Loss 0.0356 (0.0605)	
training:	Epoch: [14][666/817]	Loss 0.0126 (0.0604)	
training:	Epoch: [14][667/817]	Loss 0.0122 (0.0604)	
training:	Epoch: [14][668/817]	Loss 0.0173 (0.0603)	
training:	Epoch: [14][669/817]	Loss 0.0116 (0.0602)	
training:	Epoch: [14][670/817]	Loss 0.0149 (0.0602)	
training:	Epoch: [14][671/817]	Loss 0.0126 (0.0601)	
training:	Epoch: [14][672/817]	Loss 0.0125 (0.0600)	
training:	Epoch: [14][673/817]	Loss 0.0112 (0.0600)	
training:	Epoch: [14][674/817]	Loss 0.0184 (0.0599)	
training:	Epoch: [14][675/817]	Loss 0.0110 (0.0598)	
training:	Epoch: [14][676/817]	Loss 0.0115 (0.0598)	
training:	Epoch: [14][677/817]	Loss 0.0112 (0.0597)	
training:	Epoch: [14][678/817]	Loss 0.0149 (0.0596)	
training:	Epoch: [14][679/817]	Loss 0.0114 (0.0595)	
training:	Epoch: [14][680/817]	Loss 0.0131 (0.0595)	
training:	Epoch: [14][681/817]	Loss 0.6007 (0.0603)	
training:	Epoch: [14][682/817]	Loss 0.0290 (0.0602)	
training:	Epoch: [14][683/817]	Loss 0.0113 (0.0602)	
training:	Epoch: [14][684/817]	Loss 1.0848 (0.0617)	
training:	Epoch: [14][685/817]	Loss 0.0110 (0.0616)	
training:	Epoch: [14][686/817]	Loss 0.0115 (0.0615)	
training:	Epoch: [14][687/817]	Loss 0.5505 (0.0622)	
training:	Epoch: [14][688/817]	Loss 0.0113 (0.0621)	
training:	Epoch: [14][689/817]	Loss 0.0146 (0.0621)	
training:	Epoch: [14][690/817]	Loss 0.0106 (0.0620)	
training:	Epoch: [14][691/817]	Loss 0.0106 (0.0619)	
training:	Epoch: [14][692/817]	Loss 0.0488 (0.0619)	
training:	Epoch: [14][693/817]	Loss 0.5824 (0.0627)	
training:	Epoch: [14][694/817]	Loss 0.0131 (0.0626)	
training:	Epoch: [14][695/817]	Loss 0.0106 (0.0625)	
training:	Epoch: [14][696/817]	Loss 0.0111 (0.0624)	
training:	Epoch: [14][697/817]	Loss 0.0122 (0.0624)	
training:	Epoch: [14][698/817]	Loss 0.0131 (0.0623)	
training:	Epoch: [14][699/817]	Loss 0.0117 (0.0622)	
training:	Epoch: [14][700/817]	Loss 0.0138 (0.0622)	
training:	Epoch: [14][701/817]	Loss 0.0131 (0.0621)	
training:	Epoch: [14][702/817]	Loss 0.0110 (0.0620)	
training:	Epoch: [14][703/817]	Loss 0.0106 (0.0619)	
training:	Epoch: [14][704/817]	Loss 0.0132 (0.0619)	
training:	Epoch: [14][705/817]	Loss 0.0140 (0.0618)	
training:	Epoch: [14][706/817]	Loss 0.0126 (0.0617)	
training:	Epoch: [14][707/817]	Loss 0.0124 (0.0617)	
training:	Epoch: [14][708/817]	Loss 0.5437 (0.0623)	
training:	Epoch: [14][709/817]	Loss 0.0147 (0.0623)	
training:	Epoch: [14][710/817]	Loss 0.0207 (0.0622)	
training:	Epoch: [14][711/817]	Loss 0.0118 (0.0621)	
training:	Epoch: [14][712/817]	Loss 0.0118 (0.0621)	
training:	Epoch: [14][713/817]	Loss 0.0103 (0.0620)	
training:	Epoch: [14][714/817]	Loss 0.0112 (0.0619)	
training:	Epoch: [14][715/817]	Loss 0.0135 (0.0619)	
training:	Epoch: [14][716/817]	Loss 0.0123 (0.0618)	
training:	Epoch: [14][717/817]	Loss 0.0121 (0.0617)	
training:	Epoch: [14][718/817]	Loss 0.0102 (0.0617)	
training:	Epoch: [14][719/817]	Loss 0.0112 (0.0616)	
training:	Epoch: [14][720/817]	Loss 0.0112 (0.0615)	
training:	Epoch: [14][721/817]	Loss 0.0256 (0.0615)	
training:	Epoch: [14][722/817]	Loss 0.0159 (0.0614)	
training:	Epoch: [14][723/817]	Loss 0.0114 (0.0613)	
training:	Epoch: [14][724/817]	Loss 0.0128 (0.0613)	
training:	Epoch: [14][725/817]	Loss 0.0129 (0.0612)	
training:	Epoch: [14][726/817]	Loss 0.0149 (0.0611)	
training:	Epoch: [14][727/817]	Loss 0.0145 (0.0611)	
training:	Epoch: [14][728/817]	Loss 0.0114 (0.0610)	
training:	Epoch: [14][729/817]	Loss 0.0127 (0.0609)	
training:	Epoch: [14][730/817]	Loss 0.0105 (0.0609)	
training:	Epoch: [14][731/817]	Loss 0.0104 (0.0608)	
training:	Epoch: [14][732/817]	Loss 0.0473 (0.0608)	
training:	Epoch: [14][733/817]	Loss 0.1312 (0.0609)	
training:	Epoch: [14][734/817]	Loss 0.0222 (0.0608)	
training:	Epoch: [14][735/817]	Loss 0.0137 (0.0608)	
training:	Epoch: [14][736/817]	Loss 0.1284 (0.0608)	
training:	Epoch: [14][737/817]	Loss 0.0111 (0.0608)	
training:	Epoch: [14][738/817]	Loss 0.0112 (0.0607)	
training:	Epoch: [14][739/817]	Loss 0.0114 (0.0606)	
training:	Epoch: [14][740/817]	Loss 0.0219 (0.0606)	
training:	Epoch: [14][741/817]	Loss 0.0205 (0.0605)	
training:	Epoch: [14][742/817]	Loss 0.0175 (0.0605)	
training:	Epoch: [14][743/817]	Loss 0.0115 (0.0604)	
training:	Epoch: [14][744/817]	Loss 0.0112 (0.0603)	
training:	Epoch: [14][745/817]	Loss 0.0267 (0.0603)	
training:	Epoch: [14][746/817]	Loss 0.0105 (0.0602)	
training:	Epoch: [14][747/817]	Loss 0.0113 (0.0602)	
training:	Epoch: [14][748/817]	Loss 0.0112 (0.0601)	
training:	Epoch: [14][749/817]	Loss 0.0145 (0.0600)	
training:	Epoch: [14][750/817]	Loss 0.0107 (0.0600)	
training:	Epoch: [14][751/817]	Loss 0.0118 (0.0599)	
training:	Epoch: [14][752/817]	Loss 0.0108 (0.0598)	
training:	Epoch: [14][753/817]	Loss 0.0152 (0.0598)	
training:	Epoch: [14][754/817]	Loss 0.0106 (0.0597)	
training:	Epoch: [14][755/817]	Loss 0.0124 (0.0597)	
training:	Epoch: [14][756/817]	Loss 0.0139 (0.0596)	
training:	Epoch: [14][757/817]	Loss 0.0118 (0.0595)	
training:	Epoch: [14][758/817]	Loss 0.0106 (0.0595)	
training:	Epoch: [14][759/817]	Loss 0.0121 (0.0594)	
training:	Epoch: [14][760/817]	Loss 0.0147 (0.0594)	
training:	Epoch: [14][761/817]	Loss 0.6053 (0.0601)	
training:	Epoch: [14][762/817]	Loss 0.0195 (0.0600)	
training:	Epoch: [14][763/817]	Loss 0.0114 (0.0600)	
training:	Epoch: [14][764/817]	Loss 0.0101 (0.0599)	
training:	Epoch: [14][765/817]	Loss 0.0130 (0.0598)	
training:	Epoch: [14][766/817]	Loss 0.0122 (0.0598)	
training:	Epoch: [14][767/817]	Loss 0.0155 (0.0597)	
training:	Epoch: [14][768/817]	Loss 0.0112 (0.0596)	
training:	Epoch: [14][769/817]	Loss 0.0123 (0.0596)	
training:	Epoch: [14][770/817]	Loss 0.0104 (0.0595)	
training:	Epoch: [14][771/817]	Loss 0.0161 (0.0595)	
training:	Epoch: [14][772/817]	Loss 0.0149 (0.0594)	
training:	Epoch: [14][773/817]	Loss 0.0113 (0.0593)	
training:	Epoch: [14][774/817]	Loss 0.0150 (0.0593)	
training:	Epoch: [14][775/817]	Loss 0.0119 (0.0592)	
training:	Epoch: [14][776/817]	Loss 0.0156 (0.0592)	
training:	Epoch: [14][777/817]	Loss 0.0126 (0.0591)	
training:	Epoch: [14][778/817]	Loss 0.0115 (0.0590)	
training:	Epoch: [14][779/817]	Loss 0.0109 (0.0590)	
training:	Epoch: [14][780/817]	Loss 0.0102 (0.0589)	
training:	Epoch: [14][781/817]	Loss 0.0127 (0.0589)	
training:	Epoch: [14][782/817]	Loss 0.0110 (0.0588)	
training:	Epoch: [14][783/817]	Loss 0.0107 (0.0587)	
training:	Epoch: [14][784/817]	Loss 0.0128 (0.0587)	
training:	Epoch: [14][785/817]	Loss 0.0102 (0.0586)	
training:	Epoch: [14][786/817]	Loss 0.0113 (0.0586)	
training:	Epoch: [14][787/817]	Loss 0.5665 (0.0592)	
training:	Epoch: [14][788/817]	Loss 0.0117 (0.0591)	
training:	Epoch: [14][789/817]	Loss 0.0114 (0.0591)	
training:	Epoch: [14][790/817]	Loss 0.0117 (0.0590)	
training:	Epoch: [14][791/817]	Loss 0.0117 (0.0590)	
training:	Epoch: [14][792/817]	Loss 0.0129 (0.0589)	
training:	Epoch: [14][793/817]	Loss 0.5664 (0.0595)	
training:	Epoch: [14][794/817]	Loss 0.9327 (0.0606)	
training:	Epoch: [14][795/817]	Loss 0.5806 (0.0613)	
training:	Epoch: [14][796/817]	Loss 0.0111 (0.0612)	
training:	Epoch: [14][797/817]	Loss 0.0113 (0.0612)	
training:	Epoch: [14][798/817]	Loss 0.0251 (0.0611)	
training:	Epoch: [14][799/817]	Loss 0.0231 (0.0611)	
training:	Epoch: [14][800/817]	Loss 0.6042 (0.0618)	
training:	Epoch: [14][801/817]	Loss 0.0126 (0.0617)	
training:	Epoch: [14][802/817]	Loss 0.0105 (0.0616)	
training:	Epoch: [14][803/817]	Loss 0.0126 (0.0616)	
training:	Epoch: [14][804/817]	Loss 0.0106 (0.0615)	
training:	Epoch: [14][805/817]	Loss 0.0132 (0.0615)	
training:	Epoch: [14][806/817]	Loss 0.0119 (0.0614)	
training:	Epoch: [14][807/817]	Loss 0.0114 (0.0613)	
training:	Epoch: [14][808/817]	Loss 0.0166 (0.0613)	
training:	Epoch: [14][809/817]	Loss 0.0129 (0.0612)	
training:	Epoch: [14][810/817]	Loss 0.0103 (0.0611)	
training:	Epoch: [14][811/817]	Loss 0.1541 (0.0613)	
training:	Epoch: [14][812/817]	Loss 0.0260 (0.0612)	
training:	Epoch: [14][813/817]	Loss 0.0109 (0.0612)	
training:	Epoch: [14][814/817]	Loss 0.0113 (0.0611)	
training:	Epoch: [14][815/817]	Loss 0.0349 (0.0611)	
training:	Epoch: [14][816/817]	Loss 0.0132 (0.0610)	
training:	Epoch: [14][817/817]	Loss 0.0127 (0.0609)	
Training:	 Loss: 0.0609

Training:	 ACC: 0.9901 0.9901 0.9879 0.9923
Validation:	 ACC: 0.7845 0.7838 0.7707 0.7982
Validation:	 Best_BACC: 0.8223 0.8218 0.8127 0.8318
Validation:	 Loss: 0.8261
Pretraining:	Epoch 15/200
----------
training:	Epoch: [15][1/817]	Loss 0.0224 (0.0224)	
training:	Epoch: [15][2/817]	Loss 0.0114 (0.0169)	
training:	Epoch: [15][3/817]	Loss 0.0109 (0.0149)	
training:	Epoch: [15][4/817]	Loss 0.0098 (0.0136)	
training:	Epoch: [15][5/817]	Loss 0.0116 (0.0132)	
training:	Epoch: [15][6/817]	Loss 0.2544 (0.0534)	
training:	Epoch: [15][7/817]	Loss 0.0123 (0.0476)	
training:	Epoch: [15][8/817]	Loss 0.0100 (0.0429)	
training:	Epoch: [15][9/817]	Loss 0.0133 (0.0396)	
training:	Epoch: [15][10/817]	Loss 0.0114 (0.0368)	
training:	Epoch: [15][11/817]	Loss 0.0117 (0.0345)	
training:	Epoch: [15][12/817]	Loss 0.0110 (0.0325)	
training:	Epoch: [15][13/817]	Loss 0.0253 (0.0320)	
training:	Epoch: [15][14/817]	Loss 0.0119 (0.0305)	
training:	Epoch: [15][15/817]	Loss 0.0108 (0.0292)	
training:	Epoch: [15][16/817]	Loss 0.0114 (0.0281)	
training:	Epoch: [15][17/817]	Loss 0.0117 (0.0271)	
training:	Epoch: [15][18/817]	Loss 0.0118 (0.0263)	
training:	Epoch: [15][19/817]	Loss 0.0147 (0.0257)	
training:	Epoch: [15][20/817]	Loss 0.0113 (0.0250)	
training:	Epoch: [15][21/817]	Loss 0.0111 (0.0243)	
training:	Epoch: [15][22/817]	Loss 0.1401 (0.0296)	
training:	Epoch: [15][23/817]	Loss 0.0125 (0.0288)	
training:	Epoch: [15][24/817]	Loss 0.0110 (0.0281)	
training:	Epoch: [15][25/817]	Loss 0.0127 (0.0275)	
training:	Epoch: [15][26/817]	Loss 0.0132 (0.0269)	
training:	Epoch: [15][27/817]	Loss 0.0120 (0.0264)	
training:	Epoch: [15][28/817]	Loss 0.0126 (0.0259)	
training:	Epoch: [15][29/817]	Loss 0.0106 (0.0253)	
training:	Epoch: [15][30/817]	Loss 0.0112 (0.0249)	
training:	Epoch: [15][31/817]	Loss 0.0195 (0.0247)	
training:	Epoch: [15][32/817]	Loss 0.0118 (0.0243)	
training:	Epoch: [15][33/817]	Loss 0.0115 (0.0239)	
training:	Epoch: [15][34/817]	Loss 0.0121 (0.0236)	
training:	Epoch: [15][35/817]	Loss 0.0119 (0.0232)	
training:	Epoch: [15][36/817]	Loss 0.0112 (0.0229)	
training:	Epoch: [15][37/817]	Loss 0.0234 (0.0229)	
training:	Epoch: [15][38/817]	Loss 0.0168 (0.0227)	
training:	Epoch: [15][39/817]	Loss 0.0111 (0.0224)	
training:	Epoch: [15][40/817]	Loss 0.0113 (0.0222)	
training:	Epoch: [15][41/817]	Loss 0.0101 (0.0219)	
training:	Epoch: [15][42/817]	Loss 0.0128 (0.0217)	
training:	Epoch: [15][43/817]	Loss 0.0125 (0.0214)	
training:	Epoch: [15][44/817]	Loss 0.0407 (0.0219)	
training:	Epoch: [15][45/817]	Loss 0.0113 (0.0216)	
training:	Epoch: [15][46/817]	Loss 0.0149 (0.0215)	
training:	Epoch: [15][47/817]	Loss 0.0112 (0.0213)	
training:	Epoch: [15][48/817]	Loss 0.0106 (0.0211)	
training:	Epoch: [15][49/817]	Loss 0.0116 (0.0209)	
training:	Epoch: [15][50/817]	Loss 0.0109 (0.0207)	
training:	Epoch: [15][51/817]	Loss 0.0295 (0.0208)	
training:	Epoch: [15][52/817]	Loss 0.5578 (0.0312)	
training:	Epoch: [15][53/817]	Loss 0.0124 (0.0308)	
training:	Epoch: [15][54/817]	Loss 0.4424 (0.0384)	
training:	Epoch: [15][55/817]	Loss 0.0104 (0.0379)	
training:	Epoch: [15][56/817]	Loss 0.0108 (0.0374)	
training:	Epoch: [15][57/817]	Loss 0.0112 (0.0370)	
training:	Epoch: [15][58/817]	Loss 0.0128 (0.0366)	
training:	Epoch: [15][59/817]	Loss 0.0110 (0.0361)	
training:	Epoch: [15][60/817]	Loss 0.0124 (0.0357)	
training:	Epoch: [15][61/817]	Loss 0.6008 (0.0450)	
training:	Epoch: [15][62/817]	Loss 0.0128 (0.0445)	
training:	Epoch: [15][63/817]	Loss 0.0095 (0.0439)	
training:	Epoch: [15][64/817]	Loss 0.0103 (0.0434)	
training:	Epoch: [15][65/817]	Loss 0.0111 (0.0429)	
training:	Epoch: [15][66/817]	Loss 0.5500 (0.0506)	
training:	Epoch: [15][67/817]	Loss 0.0101 (0.0500)	
training:	Epoch: [15][68/817]	Loss 0.0111 (0.0494)	
training:	Epoch: [15][69/817]	Loss 0.0103 (0.0488)	
training:	Epoch: [15][70/817]	Loss 0.0116 (0.0483)	
training:	Epoch: [15][71/817]	Loss 0.0129 (0.0478)	
training:	Epoch: [15][72/817]	Loss 0.0103 (0.0473)	
training:	Epoch: [15][73/817]	Loss 0.0107 (0.0468)	
training:	Epoch: [15][74/817]	Loss 0.0109 (0.0463)	
training:	Epoch: [15][75/817]	Loss 0.0109 (0.0458)	
training:	Epoch: [15][76/817]	Loss 0.0366 (0.0457)	
training:	Epoch: [15][77/817]	Loss 0.0231 (0.0454)	
training:	Epoch: [15][78/817]	Loss 0.0123 (0.0450)	
training:	Epoch: [15][79/817]	Loss 0.0094 (0.0445)	
training:	Epoch: [15][80/817]	Loss 0.5490 (0.0508)	
training:	Epoch: [15][81/817]	Loss 0.0148 (0.0504)	
training:	Epoch: [15][82/817]	Loss 0.0136 (0.0499)	
training:	Epoch: [15][83/817]	Loss 0.0111 (0.0495)	
training:	Epoch: [15][84/817]	Loss 0.0107 (0.0490)	
training:	Epoch: [15][85/817]	Loss 0.0105 (0.0486)	
training:	Epoch: [15][86/817]	Loss 0.0111 (0.0481)	
training:	Epoch: [15][87/817]	Loss 0.0105 (0.0477)	
training:	Epoch: [15][88/817]	Loss 0.0115 (0.0473)	
training:	Epoch: [15][89/817]	Loss 0.0118 (0.0469)	
training:	Epoch: [15][90/817]	Loss 0.0130 (0.0465)	
training:	Epoch: [15][91/817]	Loss 0.0114 (0.0461)	
training:	Epoch: [15][92/817]	Loss 0.0106 (0.0457)	
training:	Epoch: [15][93/817]	Loss 0.0162 (0.0454)	
training:	Epoch: [15][94/817]	Loss 0.0128 (0.0451)	
training:	Epoch: [15][95/817]	Loss 0.0115 (0.0447)	
training:	Epoch: [15][96/817]	Loss 0.0103 (0.0444)	
training:	Epoch: [15][97/817]	Loss 0.6106 (0.0502)	
training:	Epoch: [15][98/817]	Loss 0.0102 (0.0498)	
training:	Epoch: [15][99/817]	Loss 0.0135 (0.0494)	
training:	Epoch: [15][100/817]	Loss 0.0114 (0.0490)	
training:	Epoch: [15][101/817]	Loss 0.0116 (0.0487)	
training:	Epoch: [15][102/817]	Loss 0.0119 (0.0483)	
training:	Epoch: [15][103/817]	Loss 0.0099 (0.0479)	
training:	Epoch: [15][104/817]	Loss 0.0097 (0.0476)	
training:	Epoch: [15][105/817]	Loss 0.0106 (0.0472)	
training:	Epoch: [15][106/817]	Loss 0.0156 (0.0469)	
training:	Epoch: [15][107/817]	Loss 0.0141 (0.0466)	
training:	Epoch: [15][108/817]	Loss 0.0118 (0.0463)	
training:	Epoch: [15][109/817]	Loss 0.0175 (0.0460)	
training:	Epoch: [15][110/817]	Loss 0.5848 (0.0509)	
training:	Epoch: [15][111/817]	Loss 0.0259 (0.0507)	
training:	Epoch: [15][112/817]	Loss 0.0125 (0.0504)	
training:	Epoch: [15][113/817]	Loss 0.0113 (0.0500)	
training:	Epoch: [15][114/817]	Loss 0.0107 (0.0497)	
training:	Epoch: [15][115/817]	Loss 0.0139 (0.0494)	
training:	Epoch: [15][116/817]	Loss 0.0122 (0.0490)	
training:	Epoch: [15][117/817]	Loss 0.0101 (0.0487)	
training:	Epoch: [15][118/817]	Loss 0.0115 (0.0484)	
training:	Epoch: [15][119/817]	Loss 0.0114 (0.0481)	
training:	Epoch: [15][120/817]	Loss 0.0108 (0.0478)	
training:	Epoch: [15][121/817]	Loss 0.0702 (0.0480)	
training:	Epoch: [15][122/817]	Loss 0.0140 (0.0477)	
training:	Epoch: [15][123/817]	Loss 0.0110 (0.0474)	
training:	Epoch: [15][124/817]	Loss 0.0104 (0.0471)	
training:	Epoch: [15][125/817]	Loss 0.0121 (0.0468)	
training:	Epoch: [15][126/817]	Loss 0.0103 (0.0465)	
training:	Epoch: [15][127/817]	Loss 0.0142 (0.0463)	
training:	Epoch: [15][128/817]	Loss 0.0111 (0.0460)	
training:	Epoch: [15][129/817]	Loss 0.0098 (0.0457)	
training:	Epoch: [15][130/817]	Loss 0.0100 (0.0454)	
training:	Epoch: [15][131/817]	Loss 0.0115 (0.0452)	
training:	Epoch: [15][132/817]	Loss 0.0109 (0.0449)	
training:	Epoch: [15][133/817]	Loss 0.0125 (0.0447)	
training:	Epoch: [15][134/817]	Loss 0.0126 (0.0444)	
training:	Epoch: [15][135/817]	Loss 0.0120 (0.0442)	
training:	Epoch: [15][136/817]	Loss 0.0113 (0.0439)	
training:	Epoch: [15][137/817]	Loss 0.0103 (0.0437)	
training:	Epoch: [15][138/817]	Loss 0.5944 (0.0477)	
training:	Epoch: [15][139/817]	Loss 0.0127 (0.0474)	
training:	Epoch: [15][140/817]	Loss 0.0204 (0.0472)	
training:	Epoch: [15][141/817]	Loss 0.0136 (0.0470)	
training:	Epoch: [15][142/817]	Loss 0.0116 (0.0468)	
training:	Epoch: [15][143/817]	Loss 0.0107 (0.0465)	
training:	Epoch: [15][144/817]	Loss 0.0107 (0.0463)	
training:	Epoch: [15][145/817]	Loss 0.0126 (0.0460)	
training:	Epoch: [15][146/817]	Loss 0.0102 (0.0458)	
training:	Epoch: [15][147/817]	Loss 0.0125 (0.0455)	
training:	Epoch: [15][148/817]	Loss 0.0115 (0.0453)	
training:	Epoch: [15][149/817]	Loss 0.0125 (0.0451)	
training:	Epoch: [15][150/817]	Loss 0.0113 (0.0449)	
training:	Epoch: [15][151/817]	Loss 0.5282 (0.0481)	
training:	Epoch: [15][152/817]	Loss 0.0124 (0.0478)	
training:	Epoch: [15][153/817]	Loss 0.0098 (0.0476)	
training:	Epoch: [15][154/817]	Loss 0.5642 (0.0509)	
training:	Epoch: [15][155/817]	Loss 0.0119 (0.0507)	
training:	Epoch: [15][156/817]	Loss 0.0121 (0.0504)	
training:	Epoch: [15][157/817]	Loss 0.0123 (0.0502)	
training:	Epoch: [15][158/817]	Loss 0.2705 (0.0516)	
training:	Epoch: [15][159/817]	Loss 0.0113 (0.0513)	
training:	Epoch: [15][160/817]	Loss 0.0115 (0.0511)	
training:	Epoch: [15][161/817]	Loss 0.0132 (0.0509)	
training:	Epoch: [15][162/817]	Loss 0.0102 (0.0506)	
training:	Epoch: [15][163/817]	Loss 0.0105 (0.0504)	
training:	Epoch: [15][164/817]	Loss 0.3467 (0.0522)	
training:	Epoch: [15][165/817]	Loss 0.0108 (0.0519)	
training:	Epoch: [15][166/817]	Loss 0.0098 (0.0517)	
training:	Epoch: [15][167/817]	Loss 0.0122 (0.0514)	
training:	Epoch: [15][168/817]	Loss 0.0103 (0.0512)	
training:	Epoch: [15][169/817]	Loss 0.0155 (0.0510)	
training:	Epoch: [15][170/817]	Loss 0.0592 (0.0510)	
training:	Epoch: [15][171/817]	Loss 0.5954 (0.0542)	
training:	Epoch: [15][172/817]	Loss 0.0115 (0.0540)	
training:	Epoch: [15][173/817]	Loss 0.0129 (0.0537)	
training:	Epoch: [15][174/817]	Loss 0.0097 (0.0535)	
training:	Epoch: [15][175/817]	Loss 0.0114 (0.0532)	
training:	Epoch: [15][176/817]	Loss 0.0119 (0.0530)	
training:	Epoch: [15][177/817]	Loss 0.0107 (0.0528)	
training:	Epoch: [15][178/817]	Loss 0.0526 (0.0528)	
training:	Epoch: [15][179/817]	Loss 0.0115 (0.0525)	
training:	Epoch: [15][180/817]	Loss 0.2062 (0.0534)	
training:	Epoch: [15][181/817]	Loss 0.0127 (0.0531)	
training:	Epoch: [15][182/817]	Loss 0.0099 (0.0529)	
training:	Epoch: [15][183/817]	Loss 0.0107 (0.0527)	
training:	Epoch: [15][184/817]	Loss 0.0103 (0.0525)	
training:	Epoch: [15][185/817]	Loss 0.0106 (0.0522)	
training:	Epoch: [15][186/817]	Loss 0.0110 (0.0520)	
training:	Epoch: [15][187/817]	Loss 0.0112 (0.0518)	
training:	Epoch: [15][188/817]	Loss 0.0104 (0.0516)	
training:	Epoch: [15][189/817]	Loss 0.0123 (0.0514)	
training:	Epoch: [15][190/817]	Loss 0.0103 (0.0511)	
training:	Epoch: [15][191/817]	Loss 0.0128 (0.0509)	
training:	Epoch: [15][192/817]	Loss 0.0165 (0.0508)	
training:	Epoch: [15][193/817]	Loss 0.0112 (0.0506)	
training:	Epoch: [15][194/817]	Loss 0.0106 (0.0503)	
training:	Epoch: [15][195/817]	Loss 0.0096 (0.0501)	
training:	Epoch: [15][196/817]	Loss 0.0118 (0.0499)	
training:	Epoch: [15][197/817]	Loss 0.0092 (0.0497)	
training:	Epoch: [15][198/817]	Loss 0.0121 (0.0495)	
training:	Epoch: [15][199/817]	Loss 0.0098 (0.0493)	
training:	Epoch: [15][200/817]	Loss 0.0099 (0.0492)	
training:	Epoch: [15][201/817]	Loss 0.0130 (0.0490)	
training:	Epoch: [15][202/817]	Loss 0.0112 (0.0488)	
training:	Epoch: [15][203/817]	Loss 0.5584 (0.0513)	
training:	Epoch: [15][204/817]	Loss 0.0185 (0.0511)	
training:	Epoch: [15][205/817]	Loss 0.0156 (0.0510)	
training:	Epoch: [15][206/817]	Loss 0.0106 (0.0508)	
training:	Epoch: [15][207/817]	Loss 0.0709 (0.0509)	
training:	Epoch: [15][208/817]	Loss 0.0111 (0.0507)	
training:	Epoch: [15][209/817]	Loss 0.1239 (0.0510)	
training:	Epoch: [15][210/817]	Loss 0.0125 (0.0508)	
training:	Epoch: [15][211/817]	Loss 0.0108 (0.0506)	
training:	Epoch: [15][212/817]	Loss 0.0104 (0.0505)	
training:	Epoch: [15][213/817]	Loss 0.0105 (0.0503)	
training:	Epoch: [15][214/817]	Loss 0.0121 (0.0501)	
training:	Epoch: [15][215/817]	Loss 0.0110 (0.0499)	
training:	Epoch: [15][216/817]	Loss 0.0128 (0.0497)	
training:	Epoch: [15][217/817]	Loss 0.0115 (0.0496)	
training:	Epoch: [15][218/817]	Loss 0.0149 (0.0494)	
training:	Epoch: [15][219/817]	Loss 0.0134 (0.0492)	
training:	Epoch: [15][220/817]	Loss 0.0110 (0.0491)	
training:	Epoch: [15][221/817]	Loss 0.0242 (0.0490)	
training:	Epoch: [15][222/817]	Loss 0.0103 (0.0488)	
training:	Epoch: [15][223/817]	Loss 0.0316 (0.0487)	
training:	Epoch: [15][224/817]	Loss 0.0220 (0.0486)	
training:	Epoch: [15][225/817]	Loss 0.0162 (0.0484)	
training:	Epoch: [15][226/817]	Loss 0.0493 (0.0484)	
training:	Epoch: [15][227/817]	Loss 0.0108 (0.0483)	
training:	Epoch: [15][228/817]	Loss 0.0125 (0.0481)	
training:	Epoch: [15][229/817]	Loss 0.0101 (0.0480)	
training:	Epoch: [15][230/817]	Loss 0.0143 (0.0478)	
training:	Epoch: [15][231/817]	Loss 0.0171 (0.0477)	
training:	Epoch: [15][232/817]	Loss 0.2395 (0.0485)	
training:	Epoch: [15][233/817]	Loss 0.0109 (0.0483)	
training:	Epoch: [15][234/817]	Loss 0.0172 (0.0482)	
training:	Epoch: [15][235/817]	Loss 0.0123 (0.0481)	
training:	Epoch: [15][236/817]	Loss 0.0249 (0.0480)	
training:	Epoch: [15][237/817]	Loss 0.0184 (0.0478)	
training:	Epoch: [15][238/817]	Loss 0.0106 (0.0477)	
training:	Epoch: [15][239/817]	Loss 0.6149 (0.0500)	
training:	Epoch: [15][240/817]	Loss 0.0088 (0.0499)	
training:	Epoch: [15][241/817]	Loss 0.0117 (0.0497)	
training:	Epoch: [15][242/817]	Loss 0.0162 (0.0496)	
training:	Epoch: [15][243/817]	Loss 0.0490 (0.0496)	
training:	Epoch: [15][244/817]	Loss 0.6006 (0.0518)	
training:	Epoch: [15][245/817]	Loss 0.0106 (0.0517)	
training:	Epoch: [15][246/817]	Loss 0.0116 (0.0515)	
training:	Epoch: [15][247/817]	Loss 0.0204 (0.0514)	
training:	Epoch: [15][248/817]	Loss 0.0147 (0.0512)	
training:	Epoch: [15][249/817]	Loss 0.0132 (0.0511)	
training:	Epoch: [15][250/817]	Loss 0.0430 (0.0510)	
training:	Epoch: [15][251/817]	Loss 0.0112 (0.0509)	
training:	Epoch: [15][252/817]	Loss 0.0106 (0.0507)	
training:	Epoch: [15][253/817]	Loss 0.0103 (0.0506)	
training:	Epoch: [15][254/817]	Loss 0.0107 (0.0504)	
training:	Epoch: [15][255/817]	Loss 0.0110 (0.0503)	
training:	Epoch: [15][256/817]	Loss 0.2556 (0.0511)	
training:	Epoch: [15][257/817]	Loss 0.0114 (0.0509)	
training:	Epoch: [15][258/817]	Loss 0.0104 (0.0507)	
training:	Epoch: [15][259/817]	Loss 0.0125 (0.0506)	
training:	Epoch: [15][260/817]	Loss 0.0098 (0.0504)	
training:	Epoch: [15][261/817]	Loss 0.0099 (0.0503)	
training:	Epoch: [15][262/817]	Loss 0.0299 (0.0502)	
training:	Epoch: [15][263/817]	Loss 0.0164 (0.0501)	
training:	Epoch: [15][264/817]	Loss 0.0102 (0.0499)	
training:	Epoch: [15][265/817]	Loss 0.0117 (0.0498)	
training:	Epoch: [15][266/817]	Loss 0.0231 (0.0497)	
training:	Epoch: [15][267/817]	Loss 0.0126 (0.0495)	
training:	Epoch: [15][268/817]	Loss 0.0113 (0.0494)	
training:	Epoch: [15][269/817]	Loss 0.0106 (0.0493)	
training:	Epoch: [15][270/817]	Loss 0.0340 (0.0492)	
training:	Epoch: [15][271/817]	Loss 0.0116 (0.0491)	
training:	Epoch: [15][272/817]	Loss 0.0157 (0.0489)	
training:	Epoch: [15][273/817]	Loss 0.1145 (0.0492)	
training:	Epoch: [15][274/817]	Loss 0.0252 (0.0491)	
training:	Epoch: [15][275/817]	Loss 0.0188 (0.0490)	
training:	Epoch: [15][276/817]	Loss 0.0133 (0.0489)	
training:	Epoch: [15][277/817]	Loss 0.0110 (0.0487)	
training:	Epoch: [15][278/817]	Loss 0.0091 (0.0486)	
training:	Epoch: [15][279/817]	Loss 0.0118 (0.0484)	
training:	Epoch: [15][280/817]	Loss 0.0093 (0.0483)	
training:	Epoch: [15][281/817]	Loss 0.0103 (0.0482)	
training:	Epoch: [15][282/817]	Loss 0.0282 (0.0481)	
training:	Epoch: [15][283/817]	Loss 0.0129 (0.0480)	
training:	Epoch: [15][284/817]	Loss 0.3209 (0.0489)	
training:	Epoch: [15][285/817]	Loss 0.0104 (0.0488)	
training:	Epoch: [15][286/817]	Loss 0.0137 (0.0487)	
training:	Epoch: [15][287/817]	Loss 0.0122 (0.0485)	
training:	Epoch: [15][288/817]	Loss 0.0096 (0.0484)	
training:	Epoch: [15][289/817]	Loss 0.0101 (0.0483)	
training:	Epoch: [15][290/817]	Loss 0.0120 (0.0482)	
training:	Epoch: [15][291/817]	Loss 0.0120 (0.0480)	
training:	Epoch: [15][292/817]	Loss 0.0255 (0.0480)	
training:	Epoch: [15][293/817]	Loss 0.0100 (0.0478)	
training:	Epoch: [15][294/817]	Loss 0.0113 (0.0477)	
training:	Epoch: [15][295/817]	Loss 0.0626 (0.0477)	
training:	Epoch: [15][296/817]	Loss 0.0096 (0.0476)	
training:	Epoch: [15][297/817]	Loss 0.0088 (0.0475)	
training:	Epoch: [15][298/817]	Loss 0.0127 (0.0474)	
training:	Epoch: [15][299/817]	Loss 0.0104 (0.0472)	
training:	Epoch: [15][300/817]	Loss 0.5928 (0.0491)	
training:	Epoch: [15][301/817]	Loss 0.0121 (0.0489)	
training:	Epoch: [15][302/817]	Loss 0.0117 (0.0488)	
training:	Epoch: [15][303/817]	Loss 0.0124 (0.0487)	
training:	Epoch: [15][304/817]	Loss 0.0110 (0.0486)	
training:	Epoch: [15][305/817]	Loss 0.0115 (0.0485)	
training:	Epoch: [15][306/817]	Loss 0.0110 (0.0483)	
training:	Epoch: [15][307/817]	Loss 0.0115 (0.0482)	
training:	Epoch: [15][308/817]	Loss 0.0096 (0.0481)	
training:	Epoch: [15][309/817]	Loss 0.0152 (0.0480)	
training:	Epoch: [15][310/817]	Loss 0.0096 (0.0479)	
training:	Epoch: [15][311/817]	Loss 0.0102 (0.0477)	
training:	Epoch: [15][312/817]	Loss 0.0235 (0.0477)	
training:	Epoch: [15][313/817]	Loss 0.0104 (0.0475)	
training:	Epoch: [15][314/817]	Loss 0.5509 (0.0491)	
training:	Epoch: [15][315/817]	Loss 0.0209 (0.0491)	
training:	Epoch: [15][316/817]	Loss 0.0097 (0.0489)	
training:	Epoch: [15][317/817]	Loss 0.0101 (0.0488)	
training:	Epoch: [15][318/817]	Loss 0.0205 (0.0487)	
training:	Epoch: [15][319/817]	Loss 0.0111 (0.0486)	
training:	Epoch: [15][320/817]	Loss 0.0284 (0.0485)	
training:	Epoch: [15][321/817]	Loss 0.0103 (0.0484)	
training:	Epoch: [15][322/817]	Loss 0.0371 (0.0484)	
training:	Epoch: [15][323/817]	Loss 0.0107 (0.0483)	
training:	Epoch: [15][324/817]	Loss 0.0108 (0.0481)	
training:	Epoch: [15][325/817]	Loss 0.0101 (0.0480)	
training:	Epoch: [15][326/817]	Loss 0.0176 (0.0479)	
training:	Epoch: [15][327/817]	Loss 0.0094 (0.0478)	
training:	Epoch: [15][328/817]	Loss 0.0113 (0.0477)	
training:	Epoch: [15][329/817]	Loss 0.0110 (0.0476)	
training:	Epoch: [15][330/817]	Loss 0.6200 (0.0493)	
training:	Epoch: [15][331/817]	Loss 0.0101 (0.0492)	
training:	Epoch: [15][332/817]	Loss 0.0105 (0.0491)	
training:	Epoch: [15][333/817]	Loss 0.0127 (0.0490)	
training:	Epoch: [15][334/817]	Loss 0.0149 (0.0489)	
training:	Epoch: [15][335/817]	Loss 0.0107 (0.0488)	
training:	Epoch: [15][336/817]	Loss 0.0090 (0.0487)	
training:	Epoch: [15][337/817]	Loss 0.0111 (0.0485)	
training:	Epoch: [15][338/817]	Loss 0.0228 (0.0485)	
training:	Epoch: [15][339/817]	Loss 0.0120 (0.0484)	
training:	Epoch: [15][340/817]	Loss 0.0248 (0.0483)	
training:	Epoch: [15][341/817]	Loss 0.0148 (0.0482)	
training:	Epoch: [15][342/817]	Loss 0.0104 (0.0481)	
training:	Epoch: [15][343/817]	Loss 0.0112 (0.0480)	
training:	Epoch: [15][344/817]	Loss 0.0160 (0.0479)	
training:	Epoch: [15][345/817]	Loss 0.0582 (0.0479)	
training:	Epoch: [15][346/817]	Loss 0.0105 (0.0478)	
training:	Epoch: [15][347/817]	Loss 0.0156 (0.0477)	
training:	Epoch: [15][348/817]	Loss 0.0114 (0.0476)	
training:	Epoch: [15][349/817]	Loss 0.0140 (0.0475)	
training:	Epoch: [15][350/817]	Loss 0.0120 (0.0474)	
training:	Epoch: [15][351/817]	Loss 0.0141 (0.0473)	
training:	Epoch: [15][352/817]	Loss 0.5644 (0.0488)	
training:	Epoch: [15][353/817]	Loss 0.0299 (0.0487)	
training:	Epoch: [15][354/817]	Loss 0.0117 (0.0486)	
training:	Epoch: [15][355/817]	Loss 0.0135 (0.0485)	
training:	Epoch: [15][356/817]	Loss 0.0103 (0.0484)	
training:	Epoch: [15][357/817]	Loss 0.0122 (0.0483)	
training:	Epoch: [15][358/817]	Loss 0.0117 (0.0482)	
training:	Epoch: [15][359/817]	Loss 0.0108 (0.0481)	
training:	Epoch: [15][360/817]	Loss 0.0270 (0.0481)	
training:	Epoch: [15][361/817]	Loss 0.0106 (0.0479)	
training:	Epoch: [15][362/817]	Loss 0.0090 (0.0478)	
training:	Epoch: [15][363/817]	Loss 0.0107 (0.0477)	
training:	Epoch: [15][364/817]	Loss 0.0110 (0.0476)	
training:	Epoch: [15][365/817]	Loss 0.0101 (0.0475)	
training:	Epoch: [15][366/817]	Loss 0.0100 (0.0474)	
training:	Epoch: [15][367/817]	Loss 0.0112 (0.0473)	
training:	Epoch: [15][368/817]	Loss 0.0107 (0.0472)	
training:	Epoch: [15][369/817]	Loss 0.0099 (0.0471)	
training:	Epoch: [15][370/817]	Loss 0.0150 (0.0470)	
training:	Epoch: [15][371/817]	Loss 0.0106 (0.0469)	
training:	Epoch: [15][372/817]	Loss 0.0128 (0.0469)	
training:	Epoch: [15][373/817]	Loss 0.0109 (0.0468)	
training:	Epoch: [15][374/817]	Loss 0.0106 (0.0467)	
training:	Epoch: [15][375/817]	Loss 0.0099 (0.0466)	
training:	Epoch: [15][376/817]	Loss 0.0163 (0.0465)	
training:	Epoch: [15][377/817]	Loss 0.0098 (0.0464)	
training:	Epoch: [15][378/817]	Loss 0.0122 (0.0463)	
training:	Epoch: [15][379/817]	Loss 0.0110 (0.0462)	
training:	Epoch: [15][380/817]	Loss 0.0112 (0.0461)	
training:	Epoch: [15][381/817]	Loss 0.0098 (0.0460)	
training:	Epoch: [15][382/817]	Loss 0.0099 (0.0459)	
training:	Epoch: [15][383/817]	Loss 0.0128 (0.0458)	
training:	Epoch: [15][384/817]	Loss 0.0117 (0.0457)	
training:	Epoch: [15][385/817]	Loss 0.0097 (0.0457)	
training:	Epoch: [15][386/817]	Loss 0.0105 (0.0456)	
training:	Epoch: [15][387/817]	Loss 0.6064 (0.0470)	
training:	Epoch: [15][388/817]	Loss 0.0248 (0.0470)	
training:	Epoch: [15][389/817]	Loss 0.0105 (0.0469)	
training:	Epoch: [15][390/817]	Loss 0.0112 (0.0468)	
training:	Epoch: [15][391/817]	Loss 0.0110 (0.0467)	
training:	Epoch: [15][392/817]	Loss 0.0123 (0.0466)	
training:	Epoch: [15][393/817]	Loss 0.0102 (0.0465)	
training:	Epoch: [15][394/817]	Loss 0.0170 (0.0464)	
training:	Epoch: [15][395/817]	Loss 0.0097 (0.0463)	
training:	Epoch: [15][396/817]	Loss 0.0128 (0.0462)	
training:	Epoch: [15][397/817]	Loss 0.0103 (0.0462)	
training:	Epoch: [15][398/817]	Loss 0.0111 (0.0461)	
training:	Epoch: [15][399/817]	Loss 0.0106 (0.0460)	
training:	Epoch: [15][400/817]	Loss 0.0101 (0.0459)	
training:	Epoch: [15][401/817]	Loss 0.0101 (0.0458)	
training:	Epoch: [15][402/817]	Loss 0.0099 (0.0457)	
training:	Epoch: [15][403/817]	Loss 0.0098 (0.0456)	
training:	Epoch: [15][404/817]	Loss 0.0110 (0.0455)	
training:	Epoch: [15][405/817]	Loss 0.0103 (0.0454)	
training:	Epoch: [15][406/817]	Loss 0.0098 (0.0454)	
training:	Epoch: [15][407/817]	Loss 0.0173 (0.0453)	
training:	Epoch: [15][408/817]	Loss 0.0117 (0.0452)	
training:	Epoch: [15][409/817]	Loss 0.0101 (0.0451)	
training:	Epoch: [15][410/817]	Loss 0.0098 (0.0450)	
training:	Epoch: [15][411/817]	Loss 0.0103 (0.0449)	
training:	Epoch: [15][412/817]	Loss 0.0085 (0.0449)	
training:	Epoch: [15][413/817]	Loss 0.0112 (0.0448)	
training:	Epoch: [15][414/817]	Loss 0.0118 (0.0447)	
training:	Epoch: [15][415/817]	Loss 0.0113 (0.0446)	
training:	Epoch: [15][416/817]	Loss 0.0100 (0.0445)	
training:	Epoch: [15][417/817]	Loss 0.0099 (0.0445)	
training:	Epoch: [15][418/817]	Loss 0.0178 (0.0444)	
training:	Epoch: [15][419/817]	Loss 0.0099 (0.0443)	
training:	Epoch: [15][420/817]	Loss 0.0100 (0.0442)	
training:	Epoch: [15][421/817]	Loss 0.0104 (0.0441)	
training:	Epoch: [15][422/817]	Loss 0.6120 (0.0455)	
training:	Epoch: [15][423/817]	Loss 0.0103 (0.0454)	
training:	Epoch: [15][424/817]	Loss 0.6132 (0.0467)	
training:	Epoch: [15][425/817]	Loss 0.0098 (0.0467)	
training:	Epoch: [15][426/817]	Loss 0.0161 (0.0466)	
training:	Epoch: [15][427/817]	Loss 0.0098 (0.0465)	
training:	Epoch: [15][428/817]	Loss 0.1124 (0.0467)	
training:	Epoch: [15][429/817]	Loss 0.0096 (0.0466)	
training:	Epoch: [15][430/817]	Loss 0.0100 (0.0465)	
training:	Epoch: [15][431/817]	Loss 0.0101 (0.0464)	
training:	Epoch: [15][432/817]	Loss 0.0123 (0.0463)	
training:	Epoch: [15][433/817]	Loss 0.0105 (0.0462)	
training:	Epoch: [15][434/817]	Loss 0.5663 (0.0474)	
training:	Epoch: [15][435/817]	Loss 0.1954 (0.0478)	
training:	Epoch: [15][436/817]	Loss 0.0103 (0.0477)	
training:	Epoch: [15][437/817]	Loss 0.5603 (0.0489)	
training:	Epoch: [15][438/817]	Loss 0.0095 (0.0488)	
training:	Epoch: [15][439/817]	Loss 0.0104 (0.0487)	
training:	Epoch: [15][440/817]	Loss 0.0118 (0.0486)	
training:	Epoch: [15][441/817]	Loss 0.0108 (0.0485)	
training:	Epoch: [15][442/817]	Loss 0.0097 (0.0484)	
training:	Epoch: [15][443/817]	Loss 0.0093 (0.0483)	
training:	Epoch: [15][444/817]	Loss 0.6021 (0.0496)	
training:	Epoch: [15][445/817]	Loss 0.0104 (0.0495)	
training:	Epoch: [15][446/817]	Loss 0.5713 (0.0507)	
training:	Epoch: [15][447/817]	Loss 0.0104 (0.0506)	
training:	Epoch: [15][448/817]	Loss 0.0136 (0.0505)	
training:	Epoch: [15][449/817]	Loss 0.0147 (0.0504)	
training:	Epoch: [15][450/817]	Loss 0.0111 (0.0503)	
training:	Epoch: [15][451/817]	Loss 0.0120 (0.0502)	
training:	Epoch: [15][452/817]	Loss 0.0109 (0.0502)	
training:	Epoch: [15][453/817]	Loss 1.1366 (0.0526)	
training:	Epoch: [15][454/817]	Loss 0.0097 (0.0525)	
training:	Epoch: [15][455/817]	Loss 0.0102 (0.0524)	
training:	Epoch: [15][456/817]	Loss 0.4991 (0.0533)	
training:	Epoch: [15][457/817]	Loss 0.2164 (0.0537)	
training:	Epoch: [15][458/817]	Loss 0.0175 (0.0536)	
training:	Epoch: [15][459/817]	Loss 0.0139 (0.0535)	
training:	Epoch: [15][460/817]	Loss 0.0128 (0.0535)	
training:	Epoch: [15][461/817]	Loss 0.0107 (0.0534)	
training:	Epoch: [15][462/817]	Loss 0.0111 (0.0533)	
training:	Epoch: [15][463/817]	Loss 0.3963 (0.0540)	
training:	Epoch: [15][464/817]	Loss 0.0115 (0.0539)	
training:	Epoch: [15][465/817]	Loss 0.6065 (0.0551)	
training:	Epoch: [15][466/817]	Loss 0.0105 (0.0550)	
training:	Epoch: [15][467/817]	Loss 0.0120 (0.0549)	
training:	Epoch: [15][468/817]	Loss 0.0116 (0.0548)	
training:	Epoch: [15][469/817]	Loss 0.0096 (0.0547)	
training:	Epoch: [15][470/817]	Loss 0.0119 (0.0546)	
training:	Epoch: [15][471/817]	Loss 0.0112 (0.0545)	
training:	Epoch: [15][472/817]	Loss 0.0137 (0.0545)	
training:	Epoch: [15][473/817]	Loss 0.0102 (0.0544)	
training:	Epoch: [15][474/817]	Loss 0.0106 (0.0543)	
training:	Epoch: [15][475/817]	Loss 0.0107 (0.0542)	
training:	Epoch: [15][476/817]	Loss 0.0103 (0.0541)	
training:	Epoch: [15][477/817]	Loss 0.0115 (0.0540)	
training:	Epoch: [15][478/817]	Loss 0.0115 (0.0539)	
training:	Epoch: [15][479/817]	Loss 0.0361 (0.0539)	
training:	Epoch: [15][480/817]	Loss 0.0103 (0.0538)	
training:	Epoch: [15][481/817]	Loss 0.0092 (0.0537)	
training:	Epoch: [15][482/817]	Loss 0.0102 (0.0536)	
training:	Epoch: [15][483/817]	Loss 0.0111 (0.0535)	
training:	Epoch: [15][484/817]	Loss 0.0107 (0.0534)	
training:	Epoch: [15][485/817]	Loss 0.0108 (0.0533)	
training:	Epoch: [15][486/817]	Loss 0.0107 (0.0532)	
training:	Epoch: [15][487/817]	Loss 0.0115 (0.0532)	
training:	Epoch: [15][488/817]	Loss 0.0117 (0.0531)	
training:	Epoch: [15][489/817]	Loss 0.0117 (0.0530)	
training:	Epoch: [15][490/817]	Loss 0.0097 (0.0529)	
training:	Epoch: [15][491/817]	Loss 0.0115 (0.0528)	
training:	Epoch: [15][492/817]	Loss 0.0127 (0.0527)	
training:	Epoch: [15][493/817]	Loss 0.0103 (0.0527)	
training:	Epoch: [15][494/817]	Loss 0.0768 (0.0527)	
training:	Epoch: [15][495/817]	Loss 0.0094 (0.0526)	
training:	Epoch: [15][496/817]	Loss 0.0106 (0.0525)	
training:	Epoch: [15][497/817]	Loss 0.0153 (0.0525)	
training:	Epoch: [15][498/817]	Loss 0.0099 (0.0524)	
training:	Epoch: [15][499/817]	Loss 0.0112 (0.0523)	
training:	Epoch: [15][500/817]	Loss 0.0106 (0.0522)	
training:	Epoch: [15][501/817]	Loss 0.0108 (0.0521)	
training:	Epoch: [15][502/817]	Loss 0.0100 (0.0520)	
training:	Epoch: [15][503/817]	Loss 0.0109 (0.0520)	
training:	Epoch: [15][504/817]	Loss 0.0407 (0.0519)	
training:	Epoch: [15][505/817]	Loss 0.0105 (0.0518)	
training:	Epoch: [15][506/817]	Loss 0.0115 (0.0518)	
training:	Epoch: [15][507/817]	Loss 0.0124 (0.0517)	
training:	Epoch: [15][508/817]	Loss 0.0098 (0.0516)	
training:	Epoch: [15][509/817]	Loss 0.0115 (0.0515)	
training:	Epoch: [15][510/817]	Loss 0.0092 (0.0514)	
training:	Epoch: [15][511/817]	Loss 0.0148 (0.0514)	
training:	Epoch: [15][512/817]	Loss 0.0137 (0.0513)	
training:	Epoch: [15][513/817]	Loss 0.0102 (0.0512)	
training:	Epoch: [15][514/817]	Loss 0.2454 (0.0516)	
training:	Epoch: [15][515/817]	Loss 0.0190 (0.0515)	
training:	Epoch: [15][516/817]	Loss 0.5675 (0.0525)	
training:	Epoch: [15][517/817]	Loss 0.0125 (0.0525)	
training:	Epoch: [15][518/817]	Loss 0.0118 (0.0524)	
training:	Epoch: [15][519/817]	Loss 0.0111 (0.0523)	
training:	Epoch: [15][520/817]	Loss 0.0115 (0.0522)	
training:	Epoch: [15][521/817]	Loss 0.0354 (0.0522)	
training:	Epoch: [15][522/817]	Loss 0.2569 (0.0526)	
training:	Epoch: [15][523/817]	Loss 0.0101 (0.0525)	
training:	Epoch: [15][524/817]	Loss 0.0108 (0.0524)	
training:	Epoch: [15][525/817]	Loss 0.0101 (0.0523)	
training:	Epoch: [15][526/817]	Loss 0.0098 (0.0523)	
training:	Epoch: [15][527/817]	Loss 0.0157 (0.0522)	
training:	Epoch: [15][528/817]	Loss 0.0116 (0.0521)	
training:	Epoch: [15][529/817]	Loss 0.0105 (0.0520)	
training:	Epoch: [15][530/817]	Loss 0.0123 (0.0520)	
training:	Epoch: [15][531/817]	Loss 0.0107 (0.0519)	
training:	Epoch: [15][532/817]	Loss 0.0157 (0.0518)	
training:	Epoch: [15][533/817]	Loss 0.0091 (0.0517)	
training:	Epoch: [15][534/817]	Loss 0.0100 (0.0517)	
training:	Epoch: [15][535/817]	Loss 0.5627 (0.0526)	
training:	Epoch: [15][536/817]	Loss 0.0111 (0.0525)	
training:	Epoch: [15][537/817]	Loss 0.0118 (0.0525)	
training:	Epoch: [15][538/817]	Loss 0.0115 (0.0524)	
training:	Epoch: [15][539/817]	Loss 0.0099 (0.0523)	
training:	Epoch: [15][540/817]	Loss 0.0087 (0.0522)	
training:	Epoch: [15][541/817]	Loss 0.0100 (0.0521)	
training:	Epoch: [15][542/817]	Loss 0.0174 (0.0521)	
training:	Epoch: [15][543/817]	Loss 0.0099 (0.0520)	
training:	Epoch: [15][544/817]	Loss 0.5459 (0.0529)	
training:	Epoch: [15][545/817]	Loss 0.0096 (0.0528)	
training:	Epoch: [15][546/817]	Loss 0.0102 (0.0528)	
training:	Epoch: [15][547/817]	Loss 0.0534 (0.0528)	
training:	Epoch: [15][548/817]	Loss 0.0102 (0.0527)	
training:	Epoch: [15][549/817]	Loss 0.0101 (0.0526)	
training:	Epoch: [15][550/817]	Loss 0.0106 (0.0525)	
training:	Epoch: [15][551/817]	Loss 0.0101 (0.0524)	
training:	Epoch: [15][552/817]	Loss 0.0109 (0.0524)	
training:	Epoch: [15][553/817]	Loss 0.0097 (0.0523)	
training:	Epoch: [15][554/817]	Loss 0.0106 (0.0522)	
training:	Epoch: [15][555/817]	Loss 0.0102 (0.0521)	
training:	Epoch: [15][556/817]	Loss 0.0115 (0.0521)	
training:	Epoch: [15][557/817]	Loss 0.0107 (0.0520)	
training:	Epoch: [15][558/817]	Loss 0.0107 (0.0519)	
training:	Epoch: [15][559/817]	Loss 0.6082 (0.0529)	
training:	Epoch: [15][560/817]	Loss 0.0105 (0.0528)	
training:	Epoch: [15][561/817]	Loss 0.6550 (0.0539)	
training:	Epoch: [15][562/817]	Loss 0.0104 (0.0538)	
training:	Epoch: [15][563/817]	Loss 0.0106 (0.0538)	
training:	Epoch: [15][564/817]	Loss 0.0122 (0.0537)	
training:	Epoch: [15][565/817]	Loss 0.0113 (0.0536)	
training:	Epoch: [15][566/817]	Loss 0.0125 (0.0535)	
training:	Epoch: [15][567/817]	Loss 0.0132 (0.0535)	
training:	Epoch: [15][568/817]	Loss 0.5464 (0.0543)	
training:	Epoch: [15][569/817]	Loss 0.0104 (0.0543)	
training:	Epoch: [15][570/817]	Loss 0.0158 (0.0542)	
training:	Epoch: [15][571/817]	Loss 0.0098 (0.0541)	
training:	Epoch: [15][572/817]	Loss 0.0098 (0.0540)	
training:	Epoch: [15][573/817]	Loss 0.0096 (0.0540)	
training:	Epoch: [15][574/817]	Loss 0.0175 (0.0539)	
training:	Epoch: [15][575/817]	Loss 0.0095 (0.0538)	
training:	Epoch: [15][576/817]	Loss 0.0109 (0.0537)	
training:	Epoch: [15][577/817]	Loss 0.0109 (0.0537)	
training:	Epoch: [15][578/817]	Loss 0.0358 (0.0536)	
training:	Epoch: [15][579/817]	Loss 0.0112 (0.0536)	
training:	Epoch: [15][580/817]	Loss 0.0142 (0.0535)	
training:	Epoch: [15][581/817]	Loss 0.0110 (0.0534)	
training:	Epoch: [15][582/817]	Loss 0.0101 (0.0533)	
training:	Epoch: [15][583/817]	Loss 0.5736 (0.0542)	
training:	Epoch: [15][584/817]	Loss 0.0176 (0.0542)	
training:	Epoch: [15][585/817]	Loss 0.0171 (0.0541)	
training:	Epoch: [15][586/817]	Loss 0.5722 (0.0550)	
training:	Epoch: [15][587/817]	Loss 0.4232 (0.0556)	
training:	Epoch: [15][588/817]	Loss 0.0101 (0.0555)	
training:	Epoch: [15][589/817]	Loss 0.0111 (0.0555)	
training:	Epoch: [15][590/817]	Loss 0.0185 (0.0554)	
training:	Epoch: [15][591/817]	Loss 0.0109 (0.0553)	
training:	Epoch: [15][592/817]	Loss 0.0110 (0.0553)	
training:	Epoch: [15][593/817]	Loss 0.0095 (0.0552)	
training:	Epoch: [15][594/817]	Loss 0.0112 (0.0551)	
training:	Epoch: [15][595/817]	Loss 0.0148 (0.0550)	
training:	Epoch: [15][596/817]	Loss 0.0114 (0.0550)	
training:	Epoch: [15][597/817]	Loss 0.0101 (0.0549)	
training:	Epoch: [15][598/817]	Loss 0.6116 (0.0558)	
training:	Epoch: [15][599/817]	Loss 0.0084 (0.0557)	
training:	Epoch: [15][600/817]	Loss 0.0098 (0.0557)	
training:	Epoch: [15][601/817]	Loss 0.0116 (0.0556)	
training:	Epoch: [15][602/817]	Loss 0.0253 (0.0555)	
training:	Epoch: [15][603/817]	Loss 0.0086 (0.0555)	
training:	Epoch: [15][604/817]	Loss 0.3785 (0.0560)	
training:	Epoch: [15][605/817]	Loss 0.3675 (0.0565)	
training:	Epoch: [15][606/817]	Loss 0.4198 (0.0571)	
training:	Epoch: [15][607/817]	Loss 0.5509 (0.0579)	
training:	Epoch: [15][608/817]	Loss 0.0118 (0.0579)	
training:	Epoch: [15][609/817]	Loss 0.0101 (0.0578)	
training:	Epoch: [15][610/817]	Loss 0.0105 (0.0577)	
training:	Epoch: [15][611/817]	Loss 0.0104 (0.0576)	
training:	Epoch: [15][612/817]	Loss 0.0105 (0.0575)	
training:	Epoch: [15][613/817]	Loss 0.0130 (0.0575)	
training:	Epoch: [15][614/817]	Loss 0.0108 (0.0574)	
training:	Epoch: [15][615/817]	Loss 0.0094 (0.0573)	
training:	Epoch: [15][616/817]	Loss 0.0124 (0.0572)	
training:	Epoch: [15][617/817]	Loss 0.0108 (0.0572)	
training:	Epoch: [15][618/817]	Loss 0.0110 (0.0571)	
training:	Epoch: [15][619/817]	Loss 0.0107 (0.0570)	
training:	Epoch: [15][620/817]	Loss 0.4229 (0.0576)	
training:	Epoch: [15][621/817]	Loss 0.0137 (0.0575)	
training:	Epoch: [15][622/817]	Loss 0.0157 (0.0575)	
training:	Epoch: [15][623/817]	Loss 0.1217 (0.0576)	
training:	Epoch: [15][624/817]	Loss 0.0097 (0.0575)	
training:	Epoch: [15][625/817]	Loss 0.0121 (0.0574)	
training:	Epoch: [15][626/817]	Loss 0.0108 (0.0573)	
training:	Epoch: [15][627/817]	Loss 0.0109 (0.0573)	
training:	Epoch: [15][628/817]	Loss 0.0110 (0.0572)	
training:	Epoch: [15][629/817]	Loss 0.0104 (0.0571)	
training:	Epoch: [15][630/817]	Loss 0.0167 (0.0571)	
training:	Epoch: [15][631/817]	Loss 0.0150 (0.0570)	
training:	Epoch: [15][632/817]	Loss 0.0103 (0.0569)	
training:	Epoch: [15][633/817]	Loss 0.0095 (0.0568)	
training:	Epoch: [15][634/817]	Loss 0.0736 (0.0569)	
training:	Epoch: [15][635/817]	Loss 0.0092 (0.0568)	
training:	Epoch: [15][636/817]	Loss 0.0095 (0.0567)	
training:	Epoch: [15][637/817]	Loss 0.0100 (0.0567)	
training:	Epoch: [15][638/817]	Loss 0.0366 (0.0566)	
training:	Epoch: [15][639/817]	Loss 0.0491 (0.0566)	
training:	Epoch: [15][640/817]	Loss 0.0083 (0.0565)	
training:	Epoch: [15][641/817]	Loss 0.0174 (0.0565)	
training:	Epoch: [15][642/817]	Loss 0.1819 (0.0567)	
training:	Epoch: [15][643/817]	Loss 0.2658 (0.0570)	
training:	Epoch: [15][644/817]	Loss 0.0109 (0.0569)	
training:	Epoch: [15][645/817]	Loss 0.0107 (0.0568)	
training:	Epoch: [15][646/817]	Loss 0.0089 (0.0568)	
training:	Epoch: [15][647/817]	Loss 0.0112 (0.0567)	
training:	Epoch: [15][648/817]	Loss 0.0280 (0.0567)	
training:	Epoch: [15][649/817]	Loss 0.0121 (0.0566)	
training:	Epoch: [15][650/817]	Loss 0.6092 (0.0574)	
training:	Epoch: [15][651/817]	Loss 0.0525 (0.0574)	
training:	Epoch: [15][652/817]	Loss 0.0102 (0.0574)	
training:	Epoch: [15][653/817]	Loss 0.0103 (0.0573)	
training:	Epoch: [15][654/817]	Loss 0.0314 (0.0572)	
training:	Epoch: [15][655/817]	Loss 0.0741 (0.0573)	
training:	Epoch: [15][656/817]	Loss 0.0553 (0.0573)	
training:	Epoch: [15][657/817]	Loss 0.0104 (0.0572)	
training:	Epoch: [15][658/817]	Loss 0.0106 (0.0571)	
training:	Epoch: [15][659/817]	Loss 0.0101 (0.0571)	
training:	Epoch: [15][660/817]	Loss 0.0101 (0.0570)	
training:	Epoch: [15][661/817]	Loss 0.0096 (0.0569)	
training:	Epoch: [15][662/817]	Loss 0.0108 (0.0568)	
training:	Epoch: [15][663/817]	Loss 0.0097 (0.0568)	
training:	Epoch: [15][664/817]	Loss 0.0104 (0.0567)	
training:	Epoch: [15][665/817]	Loss 0.6085 (0.0575)	
training:	Epoch: [15][666/817]	Loss 0.0332 (0.0575)	
training:	Epoch: [15][667/817]	Loss 0.1480 (0.0576)	
training:	Epoch: [15][668/817]	Loss 0.0125 (0.0576)	
training:	Epoch: [15][669/817]	Loss 0.0106 (0.0575)	
training:	Epoch: [15][670/817]	Loss 0.0098 (0.0574)	
training:	Epoch: [15][671/817]	Loss 0.0095 (0.0574)	
training:	Epoch: [15][672/817]	Loss 1.7336 (0.0598)	
training:	Epoch: [15][673/817]	Loss 0.0127 (0.0598)	
training:	Epoch: [15][674/817]	Loss 0.0110 (0.0597)	
training:	Epoch: [15][675/817]	Loss 0.5554 (0.0604)	
training:	Epoch: [15][676/817]	Loss 0.0104 (0.0604)	
training:	Epoch: [15][677/817]	Loss 0.2254 (0.0606)	
training:	Epoch: [15][678/817]	Loss 0.0111 (0.0605)	
training:	Epoch: [15][679/817]	Loss 0.0177 (0.0605)	
training:	Epoch: [15][680/817]	Loss 0.0986 (0.0605)	
training:	Epoch: [15][681/817]	Loss 0.0135 (0.0605)	
training:	Epoch: [15][682/817]	Loss 0.0128 (0.0604)	
training:	Epoch: [15][683/817]	Loss 0.0150 (0.0603)	
training:	Epoch: [15][684/817]	Loss 0.0120 (0.0603)	
training:	Epoch: [15][685/817]	Loss 0.0101 (0.0602)	
training:	Epoch: [15][686/817]	Loss 0.0114 (0.0601)	
training:	Epoch: [15][687/817]	Loss 0.0097 (0.0600)	
training:	Epoch: [15][688/817]	Loss 0.0601 (0.0600)	
training:	Epoch: [15][689/817]	Loss 0.0110 (0.0600)	
training:	Epoch: [15][690/817]	Loss 0.0128 (0.0599)	
training:	Epoch: [15][691/817]	Loss 0.0110 (0.0598)	
training:	Epoch: [15][692/817]	Loss 0.0120 (0.0598)	
training:	Epoch: [15][693/817]	Loss 0.0150 (0.0597)	
training:	Epoch: [15][694/817]	Loss 0.0126 (0.0596)	
training:	Epoch: [15][695/817]	Loss 0.0114 (0.0596)	
training:	Epoch: [15][696/817]	Loss 0.0109 (0.0595)	
training:	Epoch: [15][697/817]	Loss 0.0143 (0.0594)	
training:	Epoch: [15][698/817]	Loss 0.0102 (0.0593)	
training:	Epoch: [15][699/817]	Loss 0.0095 (0.0593)	
training:	Epoch: [15][700/817]	Loss 0.0246 (0.0592)	
training:	Epoch: [15][701/817]	Loss 0.0116 (0.0592)	
training:	Epoch: [15][702/817]	Loss 0.0094 (0.0591)	
training:	Epoch: [15][703/817]	Loss 0.0107 (0.0590)	
training:	Epoch: [15][704/817]	Loss 0.0099 (0.0590)	
training:	Epoch: [15][705/817]	Loss 0.0334 (0.0589)	
training:	Epoch: [15][706/817]	Loss 0.0130 (0.0589)	
training:	Epoch: [15][707/817]	Loss 0.0144 (0.0588)	
training:	Epoch: [15][708/817]	Loss 0.0102 (0.0587)	
training:	Epoch: [15][709/817]	Loss 0.0098 (0.0586)	
training:	Epoch: [15][710/817]	Loss 0.0106 (0.0586)	
training:	Epoch: [15][711/817]	Loss 0.0097 (0.0585)	
training:	Epoch: [15][712/817]	Loss 0.0104 (0.0584)	
training:	Epoch: [15][713/817]	Loss 0.0105 (0.0584)	
training:	Epoch: [15][714/817]	Loss 0.0104 (0.0583)	
training:	Epoch: [15][715/817]	Loss 0.0125 (0.0582)	
training:	Epoch: [15][716/817]	Loss 0.0099 (0.0582)	
training:	Epoch: [15][717/817]	Loss 0.0096 (0.0581)	
training:	Epoch: [15][718/817]	Loss 0.0101 (0.0580)	
training:	Epoch: [15][719/817]	Loss 0.0098 (0.0580)	
training:	Epoch: [15][720/817]	Loss 0.0105 (0.0579)	
training:	Epoch: [15][721/817]	Loss 0.0117 (0.0578)	
training:	Epoch: [15][722/817]	Loss 0.0102 (0.0578)	
training:	Epoch: [15][723/817]	Loss 0.0097 (0.0577)	
training:	Epoch: [15][724/817]	Loss 0.0107 (0.0577)	
training:	Epoch: [15][725/817]	Loss 0.0128 (0.0576)	
training:	Epoch: [15][726/817]	Loss 0.0117 (0.0575)	
training:	Epoch: [15][727/817]	Loss 0.0281 (0.0575)	
training:	Epoch: [15][728/817]	Loss 0.0192 (0.0574)	
training:	Epoch: [15][729/817]	Loss 0.0106 (0.0574)	
training:	Epoch: [15][730/817]	Loss 0.0099 (0.0573)	
training:	Epoch: [15][731/817]	Loss 0.0099 (0.0572)	
training:	Epoch: [15][732/817]	Loss 0.5730 (0.0579)	
training:	Epoch: [15][733/817]	Loss 0.0114 (0.0579)	
training:	Epoch: [15][734/817]	Loss 0.0106 (0.0578)	
training:	Epoch: [15][735/817]	Loss 0.0099 (0.0577)	
training:	Epoch: [15][736/817]	Loss 0.0111 (0.0577)	
training:	Epoch: [15][737/817]	Loss 0.6169 (0.0584)	
training:	Epoch: [15][738/817]	Loss 0.0106 (0.0584)	
training:	Epoch: [15][739/817]	Loss 0.4960 (0.0590)	
training:	Epoch: [15][740/817]	Loss 0.0109 (0.0589)	
training:	Epoch: [15][741/817]	Loss 0.0132 (0.0588)	
training:	Epoch: [15][742/817]	Loss 0.0108 (0.0588)	
training:	Epoch: [15][743/817]	Loss 0.0103 (0.0587)	
training:	Epoch: [15][744/817]	Loss 0.3075 (0.0590)	
training:	Epoch: [15][745/817]	Loss 0.0102 (0.0590)	
training:	Epoch: [15][746/817]	Loss 0.5882 (0.0597)	
training:	Epoch: [15][747/817]	Loss 0.0133 (0.0596)	
training:	Epoch: [15][748/817]	Loss 0.0109 (0.0596)	
training:	Epoch: [15][749/817]	Loss 0.0230 (0.0595)	
training:	Epoch: [15][750/817]	Loss 0.0143 (0.0595)	
training:	Epoch: [15][751/817]	Loss 0.0110 (0.0594)	
training:	Epoch: [15][752/817]	Loss 0.0121 (0.0593)	
training:	Epoch: [15][753/817]	Loss 0.0163 (0.0593)	
training:	Epoch: [15][754/817]	Loss 0.6092 (0.0600)	
training:	Epoch: [15][755/817]	Loss 0.0118 (0.0599)	
training:	Epoch: [15][756/817]	Loss 0.0106 (0.0599)	
training:	Epoch: [15][757/817]	Loss 0.0215 (0.0598)	
training:	Epoch: [15][758/817]	Loss 0.0113 (0.0598)	
training:	Epoch: [15][759/817]	Loss 0.0109 (0.0597)	
training:	Epoch: [15][760/817]	Loss 0.5362 (0.0603)	
training:	Epoch: [15][761/817]	Loss 0.0099 (0.0603)	
training:	Epoch: [15][762/817]	Loss 0.0244 (0.0602)	
training:	Epoch: [15][763/817]	Loss 0.6632 (0.0610)	
training:	Epoch: [15][764/817]	Loss 0.1718 (0.0611)	
training:	Epoch: [15][765/817]	Loss 0.5653 (0.0618)	
training:	Epoch: [15][766/817]	Loss 0.0116 (0.0617)	
training:	Epoch: [15][767/817]	Loss 0.0102 (0.0617)	
training:	Epoch: [15][768/817]	Loss 0.0122 (0.0616)	
training:	Epoch: [15][769/817]	Loss 0.0161 (0.0615)	
training:	Epoch: [15][770/817]	Loss 0.0102 (0.0615)	
training:	Epoch: [15][771/817]	Loss 0.0106 (0.0614)	
training:	Epoch: [15][772/817]	Loss 0.0109 (0.0613)	
training:	Epoch: [15][773/817]	Loss 0.0113 (0.0613)	
training:	Epoch: [15][774/817]	Loss 0.0118 (0.0612)	
training:	Epoch: [15][775/817]	Loss 0.0252 (0.0612)	
training:	Epoch: [15][776/817]	Loss 0.0302 (0.0611)	
training:	Epoch: [15][777/817]	Loss 0.0109 (0.0611)	
training:	Epoch: [15][778/817]	Loss 0.0301 (0.0610)	
training:	Epoch: [15][779/817]	Loss 0.0393 (0.0610)	
training:	Epoch: [15][780/817]	Loss 0.0101 (0.0609)	
training:	Epoch: [15][781/817]	Loss 0.0132 (0.0609)	
training:	Epoch: [15][782/817]	Loss 0.1714 (0.0610)	
training:	Epoch: [15][783/817]	Loss 0.3393 (0.0614)	
training:	Epoch: [15][784/817]	Loss 0.5126 (0.0619)	
training:	Epoch: [15][785/817]	Loss 0.5291 (0.0625)	
training:	Epoch: [15][786/817]	Loss 0.0100 (0.0625)	
training:	Epoch: [15][787/817]	Loss 0.0113 (0.0624)	
training:	Epoch: [15][788/817]	Loss 0.0095 (0.0623)	
training:	Epoch: [15][789/817]	Loss 0.0107 (0.0623)	
training:	Epoch: [15][790/817]	Loss 0.0474 (0.0623)	
training:	Epoch: [15][791/817]	Loss 0.3277 (0.0626)	
training:	Epoch: [15][792/817]	Loss 0.4472 (0.0631)	
training:	Epoch: [15][793/817]	Loss 0.0096 (0.0630)	
training:	Epoch: [15][794/817]	Loss 0.0218 (0.0630)	
training:	Epoch: [15][795/817]	Loss 0.0127 (0.0629)	
training:	Epoch: [15][796/817]	Loss 0.5421 (0.0635)	
training:	Epoch: [15][797/817]	Loss 0.0119 (0.0634)	
training:	Epoch: [15][798/817]	Loss 0.2754 (0.0637)	
training:	Epoch: [15][799/817]	Loss 0.6052 (0.0644)	
training:	Epoch: [15][800/817]	Loss 0.0479 (0.0644)	
training:	Epoch: [15][801/817]	Loss 0.0096 (0.0643)	
training:	Epoch: [15][802/817]	Loss 0.0131 (0.0642)	
training:	Epoch: [15][803/817]	Loss 0.0159 (0.0642)	
training:	Epoch: [15][804/817]	Loss 0.0104 (0.0641)	
training:	Epoch: [15][805/817]	Loss 0.0119 (0.0640)	
training:	Epoch: [15][806/817]	Loss 0.0244 (0.0640)	
training:	Epoch: [15][807/817]	Loss 0.0124 (0.0639)	
training:	Epoch: [15][808/817]	Loss 0.0131 (0.0639)	
training:	Epoch: [15][809/817]	Loss 0.0107 (0.0638)	
training:	Epoch: [15][810/817]	Loss 0.0124 (0.0637)	
training:	Epoch: [15][811/817]	Loss 0.0122 (0.0637)	
training:	Epoch: [15][812/817]	Loss 0.0100 (0.0636)	
training:	Epoch: [15][813/817]	Loss 0.0189 (0.0635)	
training:	Epoch: [15][814/817]	Loss 0.0114 (0.0635)	
training:	Epoch: [15][815/817]	Loss 0.0102 (0.0634)	
training:	Epoch: [15][816/817]	Loss 0.5553 (0.0640)	
training:	Epoch: [15][817/817]	Loss 0.0129 (0.0640)	
Training:	 Loss: 0.0639

Training:	 ACC: 0.9890 0.9890 0.9894 0.9885
Validation:	 ACC: 0.7876 0.7897 0.8342 0.7410
Validation:	 Best_BACC: 0.8223 0.8218 0.8127 0.8318
Validation:	 Loss: 0.7892
Pretraining:	Epoch 16/200
----------
training:	Epoch: [16][1/817]	Loss 0.0663 (0.0663)	
training:	Epoch: [16][2/817]	Loss 0.0125 (0.0394)	
training:	Epoch: [16][3/817]	Loss 0.0132 (0.0307)	
training:	Epoch: [16][4/817]	Loss 0.0132 (0.0263)	
training:	Epoch: [16][5/817]	Loss 0.0122 (0.0235)	
training:	Epoch: [16][6/817]	Loss 0.0318 (0.0249)	
training:	Epoch: [16][7/817]	Loss 0.0128 (0.0231)	
training:	Epoch: [16][8/817]	Loss 0.0110 (0.0216)	
training:	Epoch: [16][9/817]	Loss 0.1002 (0.0303)	
training:	Epoch: [16][10/817]	Loss 0.6068 (0.0880)	
training:	Epoch: [16][11/817]	Loss 0.0134 (0.0812)	
training:	Epoch: [16][12/817]	Loss 0.6149 (0.1257)	
training:	Epoch: [16][13/817]	Loss 0.1693 (0.1290)	
training:	Epoch: [16][14/817]	Loss 0.0136 (0.1208)	
training:	Epoch: [16][15/817]	Loss 0.0128 (0.1136)	
training:	Epoch: [16][16/817]	Loss 0.0120 (0.1072)	
training:	Epoch: [16][17/817]	Loss 0.0123 (0.1016)	
training:	Epoch: [16][18/817]	Loss 0.0105 (0.0966)	
training:	Epoch: [16][19/817]	Loss 0.0147 (0.0923)	
training:	Epoch: [16][20/817]	Loss 0.0111 (0.0882)	
training:	Epoch: [16][21/817]	Loss 0.0810 (0.0879)	
training:	Epoch: [16][22/817]	Loss 0.0110 (0.0844)	
training:	Epoch: [16][23/817]	Loss 0.0111 (0.0812)	
training:	Epoch: [16][24/817]	Loss 0.0115 (0.0783)	
training:	Epoch: [16][25/817]	Loss 0.0165 (0.0758)	
training:	Epoch: [16][26/817]	Loss 0.0115 (0.0733)	
training:	Epoch: [16][27/817]	Loss 0.0126 (0.0711)	
training:	Epoch: [16][28/817]	Loss 0.5953 (0.0898)	
training:	Epoch: [16][29/817]	Loss 0.0131 (0.0872)	
training:	Epoch: [16][30/817]	Loss 0.0119 (0.0847)	
training:	Epoch: [16][31/817]	Loss 0.0113 (0.0823)	
training:	Epoch: [16][32/817]	Loss 0.0172 (0.0803)	
training:	Epoch: [16][33/817]	Loss 0.0111 (0.0782)	
training:	Epoch: [16][34/817]	Loss 0.0552 (0.0775)	
training:	Epoch: [16][35/817]	Loss 0.0121 (0.0756)	
training:	Epoch: [16][36/817]	Loss 0.0109 (0.0738)	
training:	Epoch: [16][37/817]	Loss 0.0133 (0.0722)	
training:	Epoch: [16][38/817]	Loss 0.0114 (0.0706)	
training:	Epoch: [16][39/817]	Loss 0.0118 (0.0691)	
training:	Epoch: [16][40/817]	Loss 0.0100 (0.0676)	
training:	Epoch: [16][41/817]	Loss 0.0124 (0.0663)	
training:	Epoch: [16][42/817]	Loss 0.0210 (0.0652)	
training:	Epoch: [16][43/817]	Loss 0.0164 (0.0640)	
training:	Epoch: [16][44/817]	Loss 0.0130 (0.0629)	
training:	Epoch: [16][45/817]	Loss 0.0133 (0.0618)	
training:	Epoch: [16][46/817]	Loss 0.0120 (0.0607)	
training:	Epoch: [16][47/817]	Loss 0.0120 (0.0597)	
training:	Epoch: [16][48/817]	Loss 0.0125 (0.0587)	
training:	Epoch: [16][49/817]	Loss 0.3237 (0.0641)	
training:	Epoch: [16][50/817]	Loss 0.0121 (0.0630)	
training:	Epoch: [16][51/817]	Loss 0.0140 (0.0621)	
training:	Epoch: [16][52/817]	Loss 0.0281 (0.0614)	
training:	Epoch: [16][53/817]	Loss 0.0119 (0.0605)	
training:	Epoch: [16][54/817]	Loss 0.0119 (0.0596)	
training:	Epoch: [16][55/817]	Loss 0.0101 (0.0587)	
training:	Epoch: [16][56/817]	Loss 0.0112 (0.0578)	
training:	Epoch: [16][57/817]	Loss 0.0137 (0.0571)	
training:	Epoch: [16][58/817]	Loss 0.0105 (0.0563)	
training:	Epoch: [16][59/817]	Loss 0.0102 (0.0555)	
training:	Epoch: [16][60/817]	Loss 0.0113 (0.0548)	
training:	Epoch: [16][61/817]	Loss 0.0125 (0.0541)	
training:	Epoch: [16][62/817]	Loss 0.0104 (0.0534)	
training:	Epoch: [16][63/817]	Loss 0.0106 (0.0527)	
training:	Epoch: [16][64/817]	Loss 0.0132 (0.0521)	
training:	Epoch: [16][65/817]	Loss 0.0120 (0.0514)	
training:	Epoch: [16][66/817]	Loss 0.0119 (0.0508)	
training:	Epoch: [16][67/817]	Loss 0.0103 (0.0502)	
training:	Epoch: [16][68/817]	Loss 0.0138 (0.0497)	
training:	Epoch: [16][69/817]	Loss 0.6034 (0.0577)	
training:	Epoch: [16][70/817]	Loss 0.0111 (0.0571)	
training:	Epoch: [16][71/817]	Loss 0.0114 (0.0564)	
training:	Epoch: [16][72/817]	Loss 0.0123 (0.0558)	
training:	Epoch: [16][73/817]	Loss 0.0298 (0.0554)	
training:	Epoch: [16][74/817]	Loss 0.0108 (0.0548)	
training:	Epoch: [16][75/817]	Loss 0.5940 (0.0620)	
training:	Epoch: [16][76/817]	Loss 0.0105 (0.0614)	
training:	Epoch: [16][77/817]	Loss 0.0107 (0.0607)	
training:	Epoch: [16][78/817]	Loss 0.0114 (0.0601)	
training:	Epoch: [16][79/817]	Loss 0.0109 (0.0594)	
training:	Epoch: [16][80/817]	Loss 0.0113 (0.0588)	
training:	Epoch: [16][81/817]	Loss 0.0126 (0.0583)	
training:	Epoch: [16][82/817]	Loss 0.0105 (0.0577)	
training:	Epoch: [16][83/817]	Loss 0.0115 (0.0571)	
training:	Epoch: [16][84/817]	Loss 0.5268 (0.0627)	
training:	Epoch: [16][85/817]	Loss 0.0141 (0.0622)	
training:	Epoch: [16][86/817]	Loss 0.0110 (0.0616)	
training:	Epoch: [16][87/817]	Loss 0.0115 (0.0610)	
training:	Epoch: [16][88/817]	Loss 0.3948 (0.0648)	
training:	Epoch: [16][89/817]	Loss 0.0111 (0.0642)	
training:	Epoch: [16][90/817]	Loss 0.0099 (0.0636)	
training:	Epoch: [16][91/817]	Loss 0.0121 (0.0630)	
training:	Epoch: [16][92/817]	Loss 0.0122 (0.0625)	
training:	Epoch: [16][93/817]	Loss 0.8211 (0.0706)	
training:	Epoch: [16][94/817]	Loss 0.0175 (0.0700)	
training:	Epoch: [16][95/817]	Loss 0.5461 (0.0751)	
training:	Epoch: [16][96/817]	Loss 0.0197 (0.0745)	
training:	Epoch: [16][97/817]	Loss 0.0108 (0.0738)	
training:	Epoch: [16][98/817]	Loss 0.0095 (0.0732)	
training:	Epoch: [16][99/817]	Loss 0.0123 (0.0726)	
training:	Epoch: [16][100/817]	Loss 0.0399 (0.0722)	
training:	Epoch: [16][101/817]	Loss 0.0122 (0.0716)	
training:	Epoch: [16][102/817]	Loss 0.0117 (0.0710)	
training:	Epoch: [16][103/817]	Loss 0.0101 (0.0705)	
training:	Epoch: [16][104/817]	Loss 0.0124 (0.0699)	
training:	Epoch: [16][105/817]	Loss 0.0110 (0.0693)	
training:	Epoch: [16][106/817]	Loss 0.0112 (0.0688)	
training:	Epoch: [16][107/817]	Loss 0.5551 (0.0733)	
training:	Epoch: [16][108/817]	Loss 0.0105 (0.0727)	
training:	Epoch: [16][109/817]	Loss 0.0156 (0.0722)	
training:	Epoch: [16][110/817]	Loss 0.0256 (0.0718)	
training:	Epoch: [16][111/817]	Loss 0.5918 (0.0765)	
training:	Epoch: [16][112/817]	Loss 0.0135 (0.0759)	
training:	Epoch: [16][113/817]	Loss 0.0221 (0.0754)	
training:	Epoch: [16][114/817]	Loss 0.0104 (0.0749)	
training:	Epoch: [16][115/817]	Loss 0.3893 (0.0776)	
training:	Epoch: [16][116/817]	Loss 0.0125 (0.0770)	
training:	Epoch: [16][117/817]	Loss 0.0170 (0.0765)	
training:	Epoch: [16][118/817]	Loss 0.0113 (0.0760)	
training:	Epoch: [16][119/817]	Loss 0.0099 (0.0754)	
training:	Epoch: [16][120/817]	Loss 0.0112 (0.0749)	
training:	Epoch: [16][121/817]	Loss 0.0283 (0.0745)	
training:	Epoch: [16][122/817]	Loss 0.5643 (0.0785)	
training:	Epoch: [16][123/817]	Loss 0.0108 (0.0780)	
training:	Epoch: [16][124/817]	Loss 0.0113 (0.0774)	
training:	Epoch: [16][125/817]	Loss 0.0107 (0.0769)	
training:	Epoch: [16][126/817]	Loss 0.2355 (0.0782)	
training:	Epoch: [16][127/817]	Loss 0.0142 (0.0777)	
training:	Epoch: [16][128/817]	Loss 0.0121 (0.0771)	
training:	Epoch: [16][129/817]	Loss 0.0131 (0.0766)	
training:	Epoch: [16][130/817]	Loss 0.0125 (0.0762)	
training:	Epoch: [16][131/817]	Loss 0.0109 (0.0757)	
training:	Epoch: [16][132/817]	Loss 0.0127 (0.0752)	
training:	Epoch: [16][133/817]	Loss 0.0113 (0.0747)	
training:	Epoch: [16][134/817]	Loss 0.0091 (0.0742)	
training:	Epoch: [16][135/817]	Loss 0.0102 (0.0737)	
training:	Epoch: [16][136/817]	Loss 0.0506 (0.0736)	
training:	Epoch: [16][137/817]	Loss 0.0157 (0.0731)	
training:	Epoch: [16][138/817]	Loss 0.0554 (0.0730)	
training:	Epoch: [16][139/817]	Loss 0.0122 (0.0726)	
training:	Epoch: [16][140/817]	Loss 0.0113 (0.0721)	
training:	Epoch: [16][141/817]	Loss 0.0130 (0.0717)	
training:	Epoch: [16][142/817]	Loss 0.0156 (0.0713)	
training:	Epoch: [16][143/817]	Loss 0.0110 (0.0709)	
training:	Epoch: [16][144/817]	Loss 0.0140 (0.0705)	
training:	Epoch: [16][145/817]	Loss 0.1936 (0.0714)	
training:	Epoch: [16][146/817]	Loss 0.0321 (0.0711)	
training:	Epoch: [16][147/817]	Loss 0.0117 (0.0707)	
training:	Epoch: [16][148/817]	Loss 0.0179 (0.0703)	
training:	Epoch: [16][149/817]	Loss 0.0330 (0.0701)	
training:	Epoch: [16][150/817]	Loss 0.0120 (0.0697)	
training:	Epoch: [16][151/817]	Loss 0.0203 (0.0694)	
training:	Epoch: [16][152/817]	Loss 0.0106 (0.0690)	
training:	Epoch: [16][153/817]	Loss 0.0117 (0.0686)	
training:	Epoch: [16][154/817]	Loss 0.0096 (0.0682)	
training:	Epoch: [16][155/817]	Loss 0.3217 (0.0698)	
training:	Epoch: [16][156/817]	Loss 0.0446 (0.0697)	
training:	Epoch: [16][157/817]	Loss 0.0427 (0.0695)	
training:	Epoch: [16][158/817]	Loss 0.0168 (0.0692)	
training:	Epoch: [16][159/817]	Loss 0.0116 (0.0688)	
training:	Epoch: [16][160/817]	Loss 0.0127 (0.0685)	
training:	Epoch: [16][161/817]	Loss 0.0148 (0.0681)	
training:	Epoch: [16][162/817]	Loss 0.0123 (0.0678)	
training:	Epoch: [16][163/817]	Loss 0.0134 (0.0675)	
training:	Epoch: [16][164/817]	Loss 0.0134 (0.0671)	
training:	Epoch: [16][165/817]	Loss 0.0117 (0.0668)	
training:	Epoch: [16][166/817]	Loss 0.0229 (0.0665)	
training:	Epoch: [16][167/817]	Loss 0.0155 (0.0662)	
training:	Epoch: [16][168/817]	Loss 0.0105 (0.0659)	
training:	Epoch: [16][169/817]	Loss 0.0132 (0.0656)	
training:	Epoch: [16][170/817]	Loss 0.0125 (0.0653)	
training:	Epoch: [16][171/817]	Loss 0.0161 (0.0650)	
training:	Epoch: [16][172/817]	Loss 0.0140 (0.0647)	
training:	Epoch: [16][173/817]	Loss 0.0124 (0.0644)	
training:	Epoch: [16][174/817]	Loss 0.0248 (0.0642)	
training:	Epoch: [16][175/817]	Loss 0.0155 (0.0639)	
training:	Epoch: [16][176/817]	Loss 0.0147 (0.0636)	
training:	Epoch: [16][177/817]	Loss 0.0165 (0.0633)	
training:	Epoch: [16][178/817]	Loss 0.5295 (0.0659)	
training:	Epoch: [16][179/817]	Loss 0.0124 (0.0656)	
training:	Epoch: [16][180/817]	Loss 0.0152 (0.0654)	
training:	Epoch: [16][181/817]	Loss 0.0394 (0.0652)	
training:	Epoch: [16][182/817]	Loss 0.0867 (0.0653)	
training:	Epoch: [16][183/817]	Loss 0.0195 (0.0651)	
training:	Epoch: [16][184/817]	Loss 0.0109 (0.0648)	
training:	Epoch: [16][185/817]	Loss 0.5451 (0.0674)	
training:	Epoch: [16][186/817]	Loss 0.0204 (0.0671)	
training:	Epoch: [16][187/817]	Loss 0.0154 (0.0669)	
training:	Epoch: [16][188/817]	Loss 0.0106 (0.0666)	
training:	Epoch: [16][189/817]	Loss 0.0130 (0.0663)	
training:	Epoch: [16][190/817]	Loss 0.0177 (0.0660)	
training:	Epoch: [16][191/817]	Loss 0.0136 (0.0658)	
training:	Epoch: [16][192/817]	Loss 0.0095 (0.0655)	
training:	Epoch: [16][193/817]	Loss 0.1064 (0.0657)	
training:	Epoch: [16][194/817]	Loss 0.0120 (0.0654)	
training:	Epoch: [16][195/817]	Loss 0.0145 (0.0651)	
training:	Epoch: [16][196/817]	Loss 0.0161 (0.0649)	
training:	Epoch: [16][197/817]	Loss 0.0111 (0.0646)	
training:	Epoch: [16][198/817]	Loss 0.0139 (0.0644)	
training:	Epoch: [16][199/817]	Loss 0.0155 (0.0641)	
training:	Epoch: [16][200/817]	Loss 0.0148 (0.0639)	
training:	Epoch: [16][201/817]	Loss 0.0196 (0.0636)	
training:	Epoch: [16][202/817]	Loss 0.0134 (0.0634)	
training:	Epoch: [16][203/817]	Loss 0.0139 (0.0631)	
training:	Epoch: [16][204/817]	Loss 0.0109 (0.0629)	
training:	Epoch: [16][205/817]	Loss 0.0241 (0.0627)	
training:	Epoch: [16][206/817]	Loss 0.0148 (0.0625)	
training:	Epoch: [16][207/817]	Loss 0.0091 (0.0622)	
training:	Epoch: [16][208/817]	Loss 0.0113 (0.0620)	
training:	Epoch: [16][209/817]	Loss 0.0719 (0.0620)	
training:	Epoch: [16][210/817]	Loss 0.0136 (0.0618)	
training:	Epoch: [16][211/817]	Loss 0.0120 (0.0615)	
training:	Epoch: [16][212/817]	Loss 0.0108 (0.0613)	
training:	Epoch: [16][213/817]	Loss 0.0109 (0.0611)	
training:	Epoch: [16][214/817]	Loss 0.5108 (0.0632)	
training:	Epoch: [16][215/817]	Loss 0.0116 (0.0629)	
training:	Epoch: [16][216/817]	Loss 0.0108 (0.0627)	
training:	Epoch: [16][217/817]	Loss 0.0137 (0.0625)	
training:	Epoch: [16][218/817]	Loss 0.0108 (0.0622)	
training:	Epoch: [16][219/817]	Loss 0.0124 (0.0620)	
training:	Epoch: [16][220/817]	Loss 0.0130 (0.0618)	
training:	Epoch: [16][221/817]	Loss 0.0293 (0.0616)	
training:	Epoch: [16][222/817]	Loss 0.0131 (0.0614)	
training:	Epoch: [16][223/817]	Loss 0.0181 (0.0612)	
training:	Epoch: [16][224/817]	Loss 0.0248 (0.0611)	
training:	Epoch: [16][225/817]	Loss 0.0114 (0.0608)	
training:	Epoch: [16][226/817]	Loss 0.0174 (0.0606)	
training:	Epoch: [16][227/817]	Loss 0.0196 (0.0605)	
training:	Epoch: [16][228/817]	Loss 0.0116 (0.0602)	
training:	Epoch: [16][229/817]	Loss 0.6118 (0.0627)	
training:	Epoch: [16][230/817]	Loss 0.0102 (0.0624)	
training:	Epoch: [16][231/817]	Loss 0.0115 (0.0622)	
training:	Epoch: [16][232/817]	Loss 0.0127 (0.0620)	
training:	Epoch: [16][233/817]	Loss 0.0106 (0.0618)	
training:	Epoch: [16][234/817]	Loss 0.0114 (0.0616)	
training:	Epoch: [16][235/817]	Loss 0.0108 (0.0613)	
training:	Epoch: [16][236/817]	Loss 0.0135 (0.0611)	
training:	Epoch: [16][237/817]	Loss 0.0121 (0.0609)	
training:	Epoch: [16][238/817]	Loss 0.0143 (0.0607)	
training:	Epoch: [16][239/817]	Loss 0.0122 (0.0605)	
training:	Epoch: [16][240/817]	Loss 0.0155 (0.0603)	
training:	Epoch: [16][241/817]	Loss 0.0119 (0.0601)	
training:	Epoch: [16][242/817]	Loss 0.0114 (0.0599)	
training:	Epoch: [16][243/817]	Loss 0.0170 (0.0598)	
training:	Epoch: [16][244/817]	Loss 0.0097 (0.0596)	
training:	Epoch: [16][245/817]	Loss 0.6023 (0.0618)	
training:	Epoch: [16][246/817]	Loss 0.0097 (0.0616)	
training:	Epoch: [16][247/817]	Loss 0.0092 (0.0614)	
training:	Epoch: [16][248/817]	Loss 0.0138 (0.0612)	
training:	Epoch: [16][249/817]	Loss 0.0111 (0.0610)	
training:	Epoch: [16][250/817]	Loss 0.0111 (0.0608)	
training:	Epoch: [16][251/817]	Loss 0.0140 (0.0606)	
training:	Epoch: [16][252/817]	Loss 0.4405 (0.0621)	
training:	Epoch: [16][253/817]	Loss 0.0101 (0.0619)	
training:	Epoch: [16][254/817]	Loss 0.6154 (0.0641)	
training:	Epoch: [16][255/817]	Loss 0.6116 (0.0662)	
training:	Epoch: [16][256/817]	Loss 0.0128 (0.0660)	
training:	Epoch: [16][257/817]	Loss 0.0129 (0.0658)	
training:	Epoch: [16][258/817]	Loss 0.0098 (0.0656)	
training:	Epoch: [16][259/817]	Loss 0.0123 (0.0654)	
training:	Epoch: [16][260/817]	Loss 0.0118 (0.0652)	
training:	Epoch: [16][261/817]	Loss 0.0126 (0.0650)	
training:	Epoch: [16][262/817]	Loss 0.0114 (0.0648)	
training:	Epoch: [16][263/817]	Loss 0.0124 (0.0646)	
training:	Epoch: [16][264/817]	Loss 0.0102 (0.0644)	
training:	Epoch: [16][265/817]	Loss 0.0104 (0.0641)	
training:	Epoch: [16][266/817]	Loss 0.0122 (0.0640)	
training:	Epoch: [16][267/817]	Loss 0.0143 (0.0638)	
training:	Epoch: [16][268/817]	Loss 0.0316 (0.0636)	
training:	Epoch: [16][269/817]	Loss 0.0609 (0.0636)	
training:	Epoch: [16][270/817]	Loss 0.0116 (0.0634)	
training:	Epoch: [16][271/817]	Loss 0.0150 (0.0633)	
training:	Epoch: [16][272/817]	Loss 0.0154 (0.0631)	
training:	Epoch: [16][273/817]	Loss 0.0126 (0.0629)	
training:	Epoch: [16][274/817]	Loss 0.0174 (0.0627)	
training:	Epoch: [16][275/817]	Loss 0.0106 (0.0625)	
training:	Epoch: [16][276/817]	Loss 0.0115 (0.0624)	
training:	Epoch: [16][277/817]	Loss 0.0131 (0.0622)	
training:	Epoch: [16][278/817]	Loss 0.0094 (0.0620)	
training:	Epoch: [16][279/817]	Loss 0.0187 (0.0618)	
training:	Epoch: [16][280/817]	Loss 0.0125 (0.0617)	
training:	Epoch: [16][281/817]	Loss 0.0106 (0.0615)	
training:	Epoch: [16][282/817]	Loss 0.0110 (0.0613)	
training:	Epoch: [16][283/817]	Loss 0.0136 (0.0611)	
training:	Epoch: [16][284/817]	Loss 0.0162 (0.0610)	
training:	Epoch: [16][285/817]	Loss 0.0109 (0.0608)	
training:	Epoch: [16][286/817]	Loss 0.0128 (0.0606)	
training:	Epoch: [16][287/817]	Loss 0.0102 (0.0605)	
training:	Epoch: [16][288/817]	Loss 0.0108 (0.0603)	
training:	Epoch: [16][289/817]	Loss 0.0149 (0.0601)	
training:	Epoch: [16][290/817]	Loss 0.0192 (0.0600)	
training:	Epoch: [16][291/817]	Loss 0.0112 (0.0598)	
training:	Epoch: [16][292/817]	Loss 0.0111 (0.0597)	
training:	Epoch: [16][293/817]	Loss 0.0107 (0.0595)	
training:	Epoch: [16][294/817]	Loss 0.0114 (0.0593)	
training:	Epoch: [16][295/817]	Loss 0.0113 (0.0592)	
training:	Epoch: [16][296/817]	Loss 0.5476 (0.0608)	
training:	Epoch: [16][297/817]	Loss 0.0112 (0.0606)	
training:	Epoch: [16][298/817]	Loss 0.0121 (0.0605)	
training:	Epoch: [16][299/817]	Loss 0.0104 (0.0603)	
training:	Epoch: [16][300/817]	Loss 0.0120 (0.0602)	
training:	Epoch: [16][301/817]	Loss 0.0135 (0.0600)	
training:	Epoch: [16][302/817]	Loss 0.0116 (0.0598)	
training:	Epoch: [16][303/817]	Loss 0.0111 (0.0597)	
training:	Epoch: [16][304/817]	Loss 0.5756 (0.0614)	
training:	Epoch: [16][305/817]	Loss 0.0131 (0.0612)	
training:	Epoch: [16][306/817]	Loss 0.2433 (0.0618)	
training:	Epoch: [16][307/817]	Loss 0.0197 (0.0617)	
training:	Epoch: [16][308/817]	Loss 0.0112 (0.0615)	
training:	Epoch: [16][309/817]	Loss 0.0193 (0.0614)	
training:	Epoch: [16][310/817]	Loss 0.5647 (0.0630)	
training:	Epoch: [16][311/817]	Loss 0.0105 (0.0628)	
training:	Epoch: [16][312/817]	Loss 0.0105 (0.0627)	
training:	Epoch: [16][313/817]	Loss 0.0106 (0.0625)	
training:	Epoch: [16][314/817]	Loss 0.0111 (0.0623)	
training:	Epoch: [16][315/817]	Loss 0.0124 (0.0622)	
training:	Epoch: [16][316/817]	Loss 0.0097 (0.0620)	
training:	Epoch: [16][317/817]	Loss 0.0123 (0.0618)	
training:	Epoch: [16][318/817]	Loss 0.0113 (0.0617)	
training:	Epoch: [16][319/817]	Loss 0.0125 (0.0615)	
training:	Epoch: [16][320/817]	Loss 0.0153 (0.0614)	
training:	Epoch: [16][321/817]	Loss 0.5502 (0.0629)	
training:	Epoch: [16][322/817]	Loss 0.0108 (0.0628)	
training:	Epoch: [16][323/817]	Loss 0.0109 (0.0626)	
training:	Epoch: [16][324/817]	Loss 0.0099 (0.0624)	
training:	Epoch: [16][325/817]	Loss 0.0110 (0.0623)	
training:	Epoch: [16][326/817]	Loss 0.0138 (0.0621)	
training:	Epoch: [16][327/817]	Loss 0.5636 (0.0637)	
training:	Epoch: [16][328/817]	Loss 0.0110 (0.0635)	
training:	Epoch: [16][329/817]	Loss 0.0113 (0.0633)	
training:	Epoch: [16][330/817]	Loss 0.0302 (0.0632)	
training:	Epoch: [16][331/817]	Loss 0.7656 (0.0654)	
training:	Epoch: [16][332/817]	Loss 0.0109 (0.0652)	
training:	Epoch: [16][333/817]	Loss 0.3997 (0.0662)	
training:	Epoch: [16][334/817]	Loss 0.5360 (0.0676)	
training:	Epoch: [16][335/817]	Loss 0.0113 (0.0674)	
training:	Epoch: [16][336/817]	Loss 0.0099 (0.0673)	
training:	Epoch: [16][337/817]	Loss 0.6085 (0.0689)	
training:	Epoch: [16][338/817]	Loss 0.0142 (0.0687)	
training:	Epoch: [16][339/817]	Loss 0.0106 (0.0685)	
training:	Epoch: [16][340/817]	Loss 0.0105 (0.0684)	
training:	Epoch: [16][341/817]	Loss 0.0103 (0.0682)	
training:	Epoch: [16][342/817]	Loss 0.0781 (0.0682)	
training:	Epoch: [16][343/817]	Loss 0.0147 (0.0681)	
training:	Epoch: [16][344/817]	Loss 0.0125 (0.0679)	
training:	Epoch: [16][345/817]	Loss 0.0155 (0.0678)	
training:	Epoch: [16][346/817]	Loss 0.0148 (0.0676)	
training:	Epoch: [16][347/817]	Loss 0.0221 (0.0675)	
training:	Epoch: [16][348/817]	Loss 0.0130 (0.0673)	
training:	Epoch: [16][349/817]	Loss 0.4056 (0.0683)	
training:	Epoch: [16][350/817]	Loss 0.0117 (0.0681)	
training:	Epoch: [16][351/817]	Loss 0.0126 (0.0680)	
training:	Epoch: [16][352/817]	Loss 0.0096 (0.0678)	
training:	Epoch: [16][353/817]	Loss 0.0194 (0.0677)	
training:	Epoch: [16][354/817]	Loss 0.0165 (0.0675)	
training:	Epoch: [16][355/817]	Loss 0.0159 (0.0674)	
training:	Epoch: [16][356/817]	Loss 0.0172 (0.0672)	
training:	Epoch: [16][357/817]	Loss 0.0123 (0.0671)	
training:	Epoch: [16][358/817]	Loss 0.0108 (0.0669)	
training:	Epoch: [16][359/817]	Loss 0.0127 (0.0668)	
training:	Epoch: [16][360/817]	Loss 0.0349 (0.0667)	
training:	Epoch: [16][361/817]	Loss 0.0112 (0.0665)	
training:	Epoch: [16][362/817]	Loss 0.0168 (0.0664)	
training:	Epoch: [16][363/817]	Loss 0.0101 (0.0662)	
training:	Epoch: [16][364/817]	Loss 0.0174 (0.0661)	
training:	Epoch: [16][365/817]	Loss 0.0112 (0.0659)	
training:	Epoch: [16][366/817]	Loss 0.0102 (0.0658)	
training:	Epoch: [16][367/817]	Loss 0.0125 (0.0657)	
training:	Epoch: [16][368/817]	Loss 0.0111 (0.0655)	
training:	Epoch: [16][369/817]	Loss 0.0131 (0.0654)	
training:	Epoch: [16][370/817]	Loss 0.0140 (0.0652)	
training:	Epoch: [16][371/817]	Loss 0.0107 (0.0651)	
training:	Epoch: [16][372/817]	Loss 0.0119 (0.0649)	
training:	Epoch: [16][373/817]	Loss 0.0135 (0.0648)	
training:	Epoch: [16][374/817]	Loss 0.0167 (0.0647)	
training:	Epoch: [16][375/817]	Loss 0.0144 (0.0645)	
training:	Epoch: [16][376/817]	Loss 0.0163 (0.0644)	
training:	Epoch: [16][377/817]	Loss 0.0124 (0.0643)	
training:	Epoch: [16][378/817]	Loss 0.0103 (0.0641)	
training:	Epoch: [16][379/817]	Loss 0.0131 (0.0640)	
training:	Epoch: [16][380/817]	Loss 0.0102 (0.0638)	
training:	Epoch: [16][381/817]	Loss 0.0108 (0.0637)	
training:	Epoch: [16][382/817]	Loss 0.0130 (0.0636)	
training:	Epoch: [16][383/817]	Loss 0.0106 (0.0634)	
training:	Epoch: [16][384/817]	Loss 0.0114 (0.0633)	
training:	Epoch: [16][385/817]	Loss 0.0337 (0.0632)	
training:	Epoch: [16][386/817]	Loss 0.0100 (0.0631)	
training:	Epoch: [16][387/817]	Loss 0.0194 (0.0630)	
training:	Epoch: [16][388/817]	Loss 0.0097 (0.0628)	
training:	Epoch: [16][389/817]	Loss 0.0117 (0.0627)	
training:	Epoch: [16][390/817]	Loss 0.0121 (0.0626)	
training:	Epoch: [16][391/817]	Loss 0.0126 (0.0624)	
training:	Epoch: [16][392/817]	Loss 0.0106 (0.0623)	
training:	Epoch: [16][393/817]	Loss 0.0108 (0.0622)	
training:	Epoch: [16][394/817]	Loss 0.0099 (0.0621)	
training:	Epoch: [16][395/817]	Loss 0.0136 (0.0619)	
training:	Epoch: [16][396/817]	Loss 0.0143 (0.0618)	
training:	Epoch: [16][397/817]	Loss 0.0141 (0.0617)	
training:	Epoch: [16][398/817]	Loss 0.0129 (0.0616)	
training:	Epoch: [16][399/817]	Loss 0.0106 (0.0614)	
training:	Epoch: [16][400/817]	Loss 0.0117 (0.0613)	
training:	Epoch: [16][401/817]	Loss 0.0101 (0.0612)	
training:	Epoch: [16][402/817]	Loss 0.0110 (0.0611)	
training:	Epoch: [16][403/817]	Loss 0.0097 (0.0609)	
training:	Epoch: [16][404/817]	Loss 0.0097 (0.0608)	
training:	Epoch: [16][405/817]	Loss 0.0138 (0.0607)	
training:	Epoch: [16][406/817]	Loss 0.0106 (0.0606)	
training:	Epoch: [16][407/817]	Loss 0.5586 (0.0618)	
training:	Epoch: [16][408/817]	Loss 0.0142 (0.0617)	
training:	Epoch: [16][409/817]	Loss 0.0130 (0.0616)	
training:	Epoch: [16][410/817]	Loss 0.0146 (0.0614)	
training:	Epoch: [16][411/817]	Loss 0.0123 (0.0613)	
training:	Epoch: [16][412/817]	Loss 0.0105 (0.0612)	
training:	Epoch: [16][413/817]	Loss 0.0098 (0.0611)	
training:	Epoch: [16][414/817]	Loss 0.0123 (0.0610)	
training:	Epoch: [16][415/817]	Loss 0.0133 (0.0608)	
training:	Epoch: [16][416/817]	Loss 0.0126 (0.0607)	
training:	Epoch: [16][417/817]	Loss 0.0135 (0.0606)	
training:	Epoch: [16][418/817]	Loss 0.0125 (0.0605)	
training:	Epoch: [16][419/817]	Loss 0.0092 (0.0604)	
training:	Epoch: [16][420/817]	Loss 0.0145 (0.0603)	
training:	Epoch: [16][421/817]	Loss 0.0105 (0.0601)	
training:	Epoch: [16][422/817]	Loss 0.0137 (0.0600)	
training:	Epoch: [16][423/817]	Loss 0.5418 (0.0612)	
training:	Epoch: [16][424/817]	Loss 0.0146 (0.0611)	
training:	Epoch: [16][425/817]	Loss 0.0131 (0.0610)	
training:	Epoch: [16][426/817]	Loss 0.0160 (0.0608)	
training:	Epoch: [16][427/817]	Loss 0.0126 (0.0607)	
training:	Epoch: [16][428/817]	Loss 0.0097 (0.0606)	
training:	Epoch: [16][429/817]	Loss 0.6115 (0.0619)	
training:	Epoch: [16][430/817]	Loss 0.0099 (0.0618)	
training:	Epoch: [16][431/817]	Loss 0.0091 (0.0617)	
training:	Epoch: [16][432/817]	Loss 0.6097 (0.0629)	
training:	Epoch: [16][433/817]	Loss 0.0185 (0.0628)	
training:	Epoch: [16][434/817]	Loss 0.0109 (0.0627)	
training:	Epoch: [16][435/817]	Loss 0.0211 (0.0626)	
training:	Epoch: [16][436/817]	Loss 0.0378 (0.0625)	
training:	Epoch: [16][437/817]	Loss 0.0097 (0.0624)	
training:	Epoch: [16][438/817]	Loss 0.0127 (0.0623)	
training:	Epoch: [16][439/817]	Loss 0.0188 (0.0622)	
training:	Epoch: [16][440/817]	Loss 0.0122 (0.0621)	
training:	Epoch: [16][441/817]	Loss 0.0101 (0.0620)	
training:	Epoch: [16][442/817]	Loss 0.0116 (0.0619)	
training:	Epoch: [16][443/817]	Loss 0.6115 (0.0631)	
training:	Epoch: [16][444/817]	Loss 0.5704 (0.0643)	
training:	Epoch: [16][445/817]	Loss 0.0102 (0.0641)	
training:	Epoch: [16][446/817]	Loss 0.0096 (0.0640)	
training:	Epoch: [16][447/817]	Loss 0.0198 (0.0639)	
training:	Epoch: [16][448/817]	Loss 0.0102 (0.0638)	
training:	Epoch: [16][449/817]	Loss 0.0099 (0.0637)	
training:	Epoch: [16][450/817]	Loss 0.0335 (0.0636)	
training:	Epoch: [16][451/817]	Loss 0.0136 (0.0635)	
training:	Epoch: [16][452/817]	Loss 0.0121 (0.0634)	
training:	Epoch: [16][453/817]	Loss 0.0112 (0.0633)	
training:	Epoch: [16][454/817]	Loss 0.0103 (0.0631)	
training:	Epoch: [16][455/817]	Loss 0.0095 (0.0630)	
training:	Epoch: [16][456/817]	Loss 0.5768 (0.0642)	
training:	Epoch: [16][457/817]	Loss 0.5637 (0.0652)	
training:	Epoch: [16][458/817]	Loss 0.0103 (0.0651)	
training:	Epoch: [16][459/817]	Loss 0.0160 (0.0650)	
training:	Epoch: [16][460/817]	Loss 0.0107 (0.0649)	
training:	Epoch: [16][461/817]	Loss 0.0186 (0.0648)	
training:	Epoch: [16][462/817]	Loss 0.0098 (0.0647)	
training:	Epoch: [16][463/817]	Loss 0.0273 (0.0646)	
training:	Epoch: [16][464/817]	Loss 0.0113 (0.0645)	
training:	Epoch: [16][465/817]	Loss 0.0104 (0.0644)	
training:	Epoch: [16][466/817]	Loss 0.0533 (0.0643)	
training:	Epoch: [16][467/817]	Loss 0.0113 (0.0642)	
training:	Epoch: [16][468/817]	Loss 0.0135 (0.0641)	
training:	Epoch: [16][469/817]	Loss 0.5778 (0.0652)	
training:	Epoch: [16][470/817]	Loss 0.0108 (0.0651)	
training:	Epoch: [16][471/817]	Loss 0.0139 (0.0650)	
training:	Epoch: [16][472/817]	Loss 0.0099 (0.0649)	
training:	Epoch: [16][473/817]	Loss 0.0102 (0.0648)	
training:	Epoch: [16][474/817]	Loss 0.0276 (0.0647)	
training:	Epoch: [16][475/817]	Loss 0.0095 (0.0646)	
training:	Epoch: [16][476/817]	Loss 0.0091 (0.0645)	
training:	Epoch: [16][477/817]	Loss 0.0162 (0.0644)	
training:	Epoch: [16][478/817]	Loss 0.0122 (0.0642)	
training:	Epoch: [16][479/817]	Loss 0.0108 (0.0641)	
training:	Epoch: [16][480/817]	Loss 0.0101 (0.0640)	
training:	Epoch: [16][481/817]	Loss 0.0119 (0.0639)	
training:	Epoch: [16][482/817]	Loss 0.0145 (0.0638)	
training:	Epoch: [16][483/817]	Loss 0.0122 (0.0637)	
training:	Epoch: [16][484/817]	Loss 0.0105 (0.0636)	
training:	Epoch: [16][485/817]	Loss 0.0125 (0.0635)	
training:	Epoch: [16][486/817]	Loss 0.0131 (0.0634)	
training:	Epoch: [16][487/817]	Loss 0.0106 (0.0633)	
training:	Epoch: [16][488/817]	Loss 0.0096 (0.0632)	
training:	Epoch: [16][489/817]	Loss 0.0108 (0.0631)	
training:	Epoch: [16][490/817]	Loss 0.1463 (0.0632)	
training:	Epoch: [16][491/817]	Loss 0.0098 (0.0631)	
training:	Epoch: [16][492/817]	Loss 0.5694 (0.0641)	
training:	Epoch: [16][493/817]	Loss 0.6109 (0.0653)	
training:	Epoch: [16][494/817]	Loss 0.0107 (0.0651)	
training:	Epoch: [16][495/817]	Loss 0.0113 (0.0650)	
training:	Epoch: [16][496/817]	Loss 0.0111 (0.0649)	
training:	Epoch: [16][497/817]	Loss 0.0145 (0.0648)	
training:	Epoch: [16][498/817]	Loss 0.0141 (0.0647)	
training:	Epoch: [16][499/817]	Loss 0.0116 (0.0646)	
training:	Epoch: [16][500/817]	Loss 0.0956 (0.0647)	
training:	Epoch: [16][501/817]	Loss 0.0103 (0.0646)	
training:	Epoch: [16][502/817]	Loss 0.3857 (0.0652)	
training:	Epoch: [16][503/817]	Loss 0.0682 (0.0652)	
training:	Epoch: [16][504/817]	Loss 0.1356 (0.0654)	
training:	Epoch: [16][505/817]	Loss 0.1013 (0.0654)	
training:	Epoch: [16][506/817]	Loss 0.0105 (0.0653)	
training:	Epoch: [16][507/817]	Loss 0.0123 (0.0652)	
training:	Epoch: [16][508/817]	Loss 0.0105 (0.0651)	
training:	Epoch: [16][509/817]	Loss 0.0096 (0.0650)	
training:	Epoch: [16][510/817]	Loss 0.0285 (0.0649)	
training:	Epoch: [16][511/817]	Loss 0.0106 (0.0648)	
training:	Epoch: [16][512/817]	Loss 0.0109 (0.0647)	
training:	Epoch: [16][513/817]	Loss 0.0259 (0.0646)	
training:	Epoch: [16][514/817]	Loss 0.0116 (0.0645)	
training:	Epoch: [16][515/817]	Loss 0.2322 (0.0649)	
training:	Epoch: [16][516/817]	Loss 0.4538 (0.0656)	
training:	Epoch: [16][517/817]	Loss 0.2935 (0.0661)	
training:	Epoch: [16][518/817]	Loss 0.0126 (0.0660)	
training:	Epoch: [16][519/817]	Loss 0.3305 (0.0665)	
training:	Epoch: [16][520/817]	Loss 0.0115 (0.0664)	
training:	Epoch: [16][521/817]	Loss 0.0451 (0.0663)	
training:	Epoch: [16][522/817]	Loss 0.0097 (0.0662)	
training:	Epoch: [16][523/817]	Loss 0.0099 (0.0661)	
training:	Epoch: [16][524/817]	Loss 0.0106 (0.0660)	
training:	Epoch: [16][525/817]	Loss 0.0092 (0.0659)	
training:	Epoch: [16][526/817]	Loss 0.0111 (0.0658)	
training:	Epoch: [16][527/817]	Loss 0.0098 (0.0657)	
training:	Epoch: [16][528/817]	Loss 0.4987 (0.0665)	
training:	Epoch: [16][529/817]	Loss 0.0141 (0.0664)	
training:	Epoch: [16][530/817]	Loss 0.0148 (0.0663)	
training:	Epoch: [16][531/817]	Loss 0.0140 (0.0662)	
training:	Epoch: [16][532/817]	Loss 0.0182 (0.0661)	
training:	Epoch: [16][533/817]	Loss 0.0692 (0.0661)	
training:	Epoch: [16][534/817]	Loss 0.0110 (0.0660)	
training:	Epoch: [16][535/817]	Loss 0.0132 (0.0659)	
training:	Epoch: [16][536/817]	Loss 0.2080 (0.0662)	
training:	Epoch: [16][537/817]	Loss 0.0386 (0.0661)	
training:	Epoch: [16][538/817]	Loss 0.0096 (0.0660)	
training:	Epoch: [16][539/817]	Loss 0.0162 (0.0659)	
training:	Epoch: [16][540/817]	Loss 0.0125 (0.0658)	
training:	Epoch: [16][541/817]	Loss 0.0221 (0.0658)	
training:	Epoch: [16][542/817]	Loss 0.0135 (0.0657)	
training:	Epoch: [16][543/817]	Loss 0.0107 (0.0656)	
training:	Epoch: [16][544/817]	Loss 0.5643 (0.0665)	
training:	Epoch: [16][545/817]	Loss 0.0108 (0.0664)	
training:	Epoch: [16][546/817]	Loss 0.0679 (0.0664)	
training:	Epoch: [16][547/817]	Loss 0.0140 (0.0663)	
training:	Epoch: [16][548/817]	Loss 0.0111 (0.0662)	
training:	Epoch: [16][549/817]	Loss 0.0265 (0.0661)	
training:	Epoch: [16][550/817]	Loss 0.0152 (0.0660)	
training:	Epoch: [16][551/817]	Loss 0.0112 (0.0659)	
training:	Epoch: [16][552/817]	Loss 0.0100 (0.0658)	
training:	Epoch: [16][553/817]	Loss 0.0098 (0.0657)	
training:	Epoch: [16][554/817]	Loss 0.0093 (0.0656)	
training:	Epoch: [16][555/817]	Loss 0.0109 (0.0655)	
training:	Epoch: [16][556/817]	Loss 0.0121 (0.0654)	
training:	Epoch: [16][557/817]	Loss 0.1245 (0.0655)	
training:	Epoch: [16][558/817]	Loss 0.0100 (0.0654)	
training:	Epoch: [16][559/817]	Loss 0.0109 (0.0653)	
training:	Epoch: [16][560/817]	Loss 0.0099 (0.0652)	
training:	Epoch: [16][561/817]	Loss 0.0504 (0.0652)	
training:	Epoch: [16][562/817]	Loss 0.0109 (0.0651)	
training:	Epoch: [16][563/817]	Loss 0.3653 (0.0656)	
training:	Epoch: [16][564/817]	Loss 0.0102 (0.0655)	
training:	Epoch: [16][565/817]	Loss 0.0123 (0.0654)	
training:	Epoch: [16][566/817]	Loss 0.0526 (0.0654)	
training:	Epoch: [16][567/817]	Loss 0.0096 (0.0653)	
training:	Epoch: [16][568/817]	Loss 0.0133 (0.0652)	
training:	Epoch: [16][569/817]	Loss 0.0113 (0.0651)	
training:	Epoch: [16][570/817]	Loss 0.0130 (0.0650)	
training:	Epoch: [16][571/817]	Loss 0.0101 (0.0649)	
training:	Epoch: [16][572/817]	Loss 0.0101 (0.0648)	
training:	Epoch: [16][573/817]	Loss 0.0123 (0.0648)	
training:	Epoch: [16][574/817]	Loss 0.0378 (0.0647)	
training:	Epoch: [16][575/817]	Loss 0.0249 (0.0646)	
training:	Epoch: [16][576/817]	Loss 0.0097 (0.0645)	
training:	Epoch: [16][577/817]	Loss 0.0102 (0.0645)	
training:	Epoch: [16][578/817]	Loss 0.0116 (0.0644)	
training:	Epoch: [16][579/817]	Loss 0.0202 (0.0643)	
training:	Epoch: [16][580/817]	Loss 0.0108 (0.0642)	
training:	Epoch: [16][581/817]	Loss 0.0340 (0.0641)	
training:	Epoch: [16][582/817]	Loss 0.0105 (0.0640)	
training:	Epoch: [16][583/817]	Loss 0.0105 (0.0640)	
training:	Epoch: [16][584/817]	Loss 0.0112 (0.0639)	
training:	Epoch: [16][585/817]	Loss 0.5569 (0.0647)	
training:	Epoch: [16][586/817]	Loss 0.0103 (0.0646)	
training:	Epoch: [16][587/817]	Loss 0.5321 (0.0654)	
training:	Epoch: [16][588/817]	Loss 0.0125 (0.0653)	
training:	Epoch: [16][589/817]	Loss 0.0103 (0.0652)	
training:	Epoch: [16][590/817]	Loss 0.0157 (0.0651)	
training:	Epoch: [16][591/817]	Loss 0.0106 (0.0651)	
training:	Epoch: [16][592/817]	Loss 0.0115 (0.0650)	
training:	Epoch: [16][593/817]	Loss 0.0135 (0.0649)	
training:	Epoch: [16][594/817]	Loss 0.1831 (0.0651)	
training:	Epoch: [16][595/817]	Loss 0.0106 (0.0650)	
training:	Epoch: [16][596/817]	Loss 0.0336 (0.0649)	
training:	Epoch: [16][597/817]	Loss 0.0104 (0.0648)	
training:	Epoch: [16][598/817]	Loss 0.0109 (0.0647)	
training:	Epoch: [16][599/817]	Loss 0.0169 (0.0647)	
training:	Epoch: [16][600/817]	Loss 0.0119 (0.0646)	
training:	Epoch: [16][601/817]	Loss 0.0114 (0.0645)	
training:	Epoch: [16][602/817]	Loss 0.0179 (0.0644)	
training:	Epoch: [16][603/817]	Loss 0.1071 (0.0645)	
training:	Epoch: [16][604/817]	Loss 0.0090 (0.0644)	
training:	Epoch: [16][605/817]	Loss 0.0103 (0.0643)	
training:	Epoch: [16][606/817]	Loss 0.5906 (0.0652)	
training:	Epoch: [16][607/817]	Loss 0.0114 (0.0651)	
training:	Epoch: [16][608/817]	Loss 0.0154 (0.0650)	
training:	Epoch: [16][609/817]	Loss 0.0106 (0.0649)	
training:	Epoch: [16][610/817]	Loss 0.0110 (0.0648)	
training:	Epoch: [16][611/817]	Loss 0.0103 (0.0647)	
training:	Epoch: [16][612/817]	Loss 0.0113 (0.0646)	
training:	Epoch: [16][613/817]	Loss 0.0169 (0.0646)	
training:	Epoch: [16][614/817]	Loss 0.0130 (0.0645)	
training:	Epoch: [16][615/817]	Loss 0.6073 (0.0654)	
training:	Epoch: [16][616/817]	Loss 0.0179 (0.0653)	
training:	Epoch: [16][617/817]	Loss 0.0103 (0.0652)	
training:	Epoch: [16][618/817]	Loss 0.0103 (0.0651)	
training:	Epoch: [16][619/817]	Loss 0.6117 (0.0660)	
training:	Epoch: [16][620/817]	Loss 0.0118 (0.0659)	
training:	Epoch: [16][621/817]	Loss 0.0100 (0.0658)	
training:	Epoch: [16][622/817]	Loss 0.0122 (0.0657)	
training:	Epoch: [16][623/817]	Loss 0.0102 (0.0656)	
training:	Epoch: [16][624/817]	Loss 0.0113 (0.0656)	
training:	Epoch: [16][625/817]	Loss 0.0153 (0.0655)	
training:	Epoch: [16][626/817]	Loss 0.0161 (0.0654)	
training:	Epoch: [16][627/817]	Loss 0.0116 (0.0653)	
training:	Epoch: [16][628/817]	Loss 0.0100 (0.0652)	
training:	Epoch: [16][629/817]	Loss 0.0430 (0.0652)	
training:	Epoch: [16][630/817]	Loss 0.0092 (0.0651)	
training:	Epoch: [16][631/817]	Loss 0.0118 (0.0650)	
training:	Epoch: [16][632/817]	Loss 0.0093 (0.0649)	
training:	Epoch: [16][633/817]	Loss 0.0109 (0.0648)	
training:	Epoch: [16][634/817]	Loss 0.0111 (0.0648)	
training:	Epoch: [16][635/817]	Loss 0.0107 (0.0647)	
training:	Epoch: [16][636/817]	Loss 0.0116 (0.0646)	
training:	Epoch: [16][637/817]	Loss 0.0125 (0.0645)	
training:	Epoch: [16][638/817]	Loss 0.0116 (0.0644)	
training:	Epoch: [16][639/817]	Loss 0.0091 (0.0643)	
training:	Epoch: [16][640/817]	Loss 0.0136 (0.0643)	
training:	Epoch: [16][641/817]	Loss 0.0095 (0.0642)	
training:	Epoch: [16][642/817]	Loss 0.0104 (0.0641)	
training:	Epoch: [16][643/817]	Loss 0.0105 (0.0640)	
training:	Epoch: [16][644/817]	Loss 0.0120 (0.0639)	
training:	Epoch: [16][645/817]	Loss 0.0095 (0.0638)	
training:	Epoch: [16][646/817]	Loss 0.0096 (0.0638)	
training:	Epoch: [16][647/817]	Loss 0.0100 (0.0637)	
training:	Epoch: [16][648/817]	Loss 0.0877 (0.0637)	
training:	Epoch: [16][649/817]	Loss 0.0106 (0.0636)	
training:	Epoch: [16][650/817]	Loss 0.0110 (0.0635)	
training:	Epoch: [16][651/817]	Loss 0.0101 (0.0635)	
training:	Epoch: [16][652/817]	Loss 0.0106 (0.0634)	
training:	Epoch: [16][653/817]	Loss 0.0138 (0.0633)	
training:	Epoch: [16][654/817]	Loss 0.0287 (0.0633)	
training:	Epoch: [16][655/817]	Loss 0.0111 (0.0632)	
training:	Epoch: [16][656/817]	Loss 0.0114 (0.0631)	
training:	Epoch: [16][657/817]	Loss 0.0098 (0.0630)	
training:	Epoch: [16][658/817]	Loss 0.0107 (0.0629)	
training:	Epoch: [16][659/817]	Loss 0.0475 (0.0629)	
training:	Epoch: [16][660/817]	Loss 0.0110 (0.0628)	
training:	Epoch: [16][661/817]	Loss 0.0106 (0.0628)	
training:	Epoch: [16][662/817]	Loss 0.0102 (0.0627)	
training:	Epoch: [16][663/817]	Loss 0.0102 (0.0626)	
training:	Epoch: [16][664/817]	Loss 0.0097 (0.0625)	
training:	Epoch: [16][665/817]	Loss 0.0099 (0.0624)	
training:	Epoch: [16][666/817]	Loss 0.2463 (0.0627)	
training:	Epoch: [16][667/817]	Loss 0.0122 (0.0626)	
training:	Epoch: [16][668/817]	Loss 0.0104 (0.0626)	
training:	Epoch: [16][669/817]	Loss 0.0451 (0.0625)	
training:	Epoch: [16][670/817]	Loss 0.4830 (0.0632)	
training:	Epoch: [16][671/817]	Loss 0.0106 (0.0631)	
training:	Epoch: [16][672/817]	Loss 0.0102 (0.0630)	
training:	Epoch: [16][673/817]	Loss 0.0689 (0.0630)	
training:	Epoch: [16][674/817]	Loss 0.0107 (0.0629)	
training:	Epoch: [16][675/817]	Loss 0.0096 (0.0629)	
training:	Epoch: [16][676/817]	Loss 0.0113 (0.0628)	
training:	Epoch: [16][677/817]	Loss 0.0124 (0.0627)	
training:	Epoch: [16][678/817]	Loss 0.0149 (0.0626)	
training:	Epoch: [16][679/817]	Loss 0.0210 (0.0626)	
training:	Epoch: [16][680/817]	Loss 0.0101 (0.0625)	
training:	Epoch: [16][681/817]	Loss 0.0096 (0.0624)	
training:	Epoch: [16][682/817]	Loss 0.0099 (0.0623)	
training:	Epoch: [16][683/817]	Loss 0.0097 (0.0623)	
training:	Epoch: [16][684/817]	Loss 0.0138 (0.0622)	
training:	Epoch: [16][685/817]	Loss 0.0126 (0.0621)	
training:	Epoch: [16][686/817]	Loss 0.0098 (0.0620)	
training:	Epoch: [16][687/817]	Loss 0.5585 (0.0628)	
training:	Epoch: [16][688/817]	Loss 0.0096 (0.0627)	
training:	Epoch: [16][689/817]	Loss 0.5545 (0.0634)	
training:	Epoch: [16][690/817]	Loss 0.5619 (0.0641)	
training:	Epoch: [16][691/817]	Loss 0.0093 (0.0640)	
training:	Epoch: [16][692/817]	Loss 0.0091 (0.0640)	
training:	Epoch: [16][693/817]	Loss 0.6194 (0.0648)	
training:	Epoch: [16][694/817]	Loss 0.0364 (0.0647)	
training:	Epoch: [16][695/817]	Loss 0.0139 (0.0647)	
training:	Epoch: [16][696/817]	Loss 0.0104 (0.0646)	
training:	Epoch: [16][697/817]	Loss 0.0094 (0.0645)	
training:	Epoch: [16][698/817]	Loss 0.0346 (0.0645)	
training:	Epoch: [16][699/817]	Loss 0.0112 (0.0644)	
training:	Epoch: [16][700/817]	Loss 0.0109 (0.0643)	
training:	Epoch: [16][701/817]	Loss 0.0205 (0.0642)	
training:	Epoch: [16][702/817]	Loss 0.0091 (0.0642)	
training:	Epoch: [16][703/817]	Loss 0.0130 (0.0641)	
training:	Epoch: [16][704/817]	Loss 0.0134 (0.0640)	
training:	Epoch: [16][705/817]	Loss 0.0119 (0.0639)	
training:	Epoch: [16][706/817]	Loss 0.0114 (0.0639)	
training:	Epoch: [16][707/817]	Loss 0.0089 (0.0638)	
training:	Epoch: [16][708/817]	Loss 0.0107 (0.0637)	
training:	Epoch: [16][709/817]	Loss 0.0110 (0.0636)	
training:	Epoch: [16][710/817]	Loss 0.5718 (0.0644)	
training:	Epoch: [16][711/817]	Loss 0.0107 (0.0643)	
training:	Epoch: [16][712/817]	Loss 0.0106 (0.0642)	
training:	Epoch: [16][713/817]	Loss 0.0201 (0.0641)	
training:	Epoch: [16][714/817]	Loss 0.0090 (0.0641)	
training:	Epoch: [16][715/817]	Loss 0.0101 (0.0640)	
training:	Epoch: [16][716/817]	Loss 0.0106 (0.0639)	
training:	Epoch: [16][717/817]	Loss 0.0097 (0.0638)	
training:	Epoch: [16][718/817]	Loss 0.0109 (0.0638)	
training:	Epoch: [16][719/817]	Loss 0.0120 (0.0637)	
training:	Epoch: [16][720/817]	Loss 0.0095 (0.0636)	
training:	Epoch: [16][721/817]	Loss 0.0303 (0.0636)	
training:	Epoch: [16][722/817]	Loss 0.0107 (0.0635)	
training:	Epoch: [16][723/817]	Loss 0.0095 (0.0634)	
training:	Epoch: [16][724/817]	Loss 0.0103 (0.0634)	
training:	Epoch: [16][725/817]	Loss 0.0138 (0.0633)	
training:	Epoch: [16][726/817]	Loss 0.0099 (0.0632)	
training:	Epoch: [16][727/817]	Loss 0.0171 (0.0631)	
training:	Epoch: [16][728/817]	Loss 0.0107 (0.0631)	
training:	Epoch: [16][729/817]	Loss 0.0125 (0.0630)	
training:	Epoch: [16][730/817]	Loss 0.0293 (0.0630)	
training:	Epoch: [16][731/817]	Loss 0.0109 (0.0629)	
training:	Epoch: [16][732/817]	Loss 0.0362 (0.0629)	
training:	Epoch: [16][733/817]	Loss 0.0106 (0.0628)	
training:	Epoch: [16][734/817]	Loss 0.0273 (0.0627)	
training:	Epoch: [16][735/817]	Loss 0.0126 (0.0627)	
training:	Epoch: [16][736/817]	Loss 0.0146 (0.0626)	
training:	Epoch: [16][737/817]	Loss 0.0104 (0.0625)	
training:	Epoch: [16][738/817]	Loss 0.0105 (0.0625)	
training:	Epoch: [16][739/817]	Loss 0.0107 (0.0624)	
training:	Epoch: [16][740/817]	Loss 0.0104 (0.0623)	
training:	Epoch: [16][741/817]	Loss 0.0109 (0.0622)	
training:	Epoch: [16][742/817]	Loss 0.0104 (0.0622)	
training:	Epoch: [16][743/817]	Loss 0.0100 (0.0621)	
training:	Epoch: [16][744/817]	Loss 0.0116 (0.0620)	
training:	Epoch: [16][745/817]	Loss 0.0104 (0.0620)	
training:	Epoch: [16][746/817]	Loss 0.0117 (0.0619)	
training:	Epoch: [16][747/817]	Loss 0.0105 (0.0618)	
training:	Epoch: [16][748/817]	Loss 0.0108 (0.0618)	
training:	Epoch: [16][749/817]	Loss 0.0106 (0.0617)	
training:	Epoch: [16][750/817]	Loss 0.0090 (0.0616)	
training:	Epoch: [16][751/817]	Loss 0.0117 (0.0616)	
training:	Epoch: [16][752/817]	Loss 0.0124 (0.0615)	
training:	Epoch: [16][753/817]	Loss 0.0110 (0.0614)	
training:	Epoch: [16][754/817]	Loss 0.0096 (0.0614)	
training:	Epoch: [16][755/817]	Loss 0.1146 (0.0614)	
training:	Epoch: [16][756/817]	Loss 0.0122 (0.0614)	
training:	Epoch: [16][757/817]	Loss 0.0114 (0.0613)	
training:	Epoch: [16][758/817]	Loss 0.1881 (0.0615)	
training:	Epoch: [16][759/817]	Loss 0.0090 (0.0614)	
training:	Epoch: [16][760/817]	Loss 0.0091 (0.0613)	
training:	Epoch: [16][761/817]	Loss 0.0099 (0.0613)	
training:	Epoch: [16][762/817]	Loss 0.0172 (0.0612)	
training:	Epoch: [16][763/817]	Loss 0.0178 (0.0611)	
training:	Epoch: [16][764/817]	Loss 0.0128 (0.0611)	
training:	Epoch: [16][765/817]	Loss 0.0097 (0.0610)	
training:	Epoch: [16][766/817]	Loss 0.0103 (0.0610)	
training:	Epoch: [16][767/817]	Loss 0.0460 (0.0609)	
training:	Epoch: [16][768/817]	Loss 0.0107 (0.0609)	
training:	Epoch: [16][769/817]	Loss 0.0103 (0.0608)	
training:	Epoch: [16][770/817]	Loss 0.0091 (0.0607)	
training:	Epoch: [16][771/817]	Loss 0.0121 (0.0607)	
training:	Epoch: [16][772/817]	Loss 0.5478 (0.0613)	
training:	Epoch: [16][773/817]	Loss 0.0107 (0.0612)	
training:	Epoch: [16][774/817]	Loss 0.0126 (0.0612)	
training:	Epoch: [16][775/817]	Loss 0.1280 (0.0613)	
training:	Epoch: [16][776/817]	Loss 0.0139 (0.0612)	
training:	Epoch: [16][777/817]	Loss 0.0104 (0.0611)	
training:	Epoch: [16][778/817]	Loss 0.0106 (0.0611)	
training:	Epoch: [16][779/817]	Loss 0.0104 (0.0610)	
training:	Epoch: [16][780/817]	Loss 0.0090 (0.0609)	
training:	Epoch: [16][781/817]	Loss 0.0105 (0.0609)	
training:	Epoch: [16][782/817]	Loss 0.0137 (0.0608)	
training:	Epoch: [16][783/817]	Loss 0.0092 (0.0607)	
training:	Epoch: [16][784/817]	Loss 0.0089 (0.0607)	
training:	Epoch: [16][785/817]	Loss 0.0116 (0.0606)	
training:	Epoch: [16][786/817]	Loss 0.0107 (0.0606)	
training:	Epoch: [16][787/817]	Loss 0.0090 (0.0605)	
training:	Epoch: [16][788/817]	Loss 0.0118 (0.0604)	
training:	Epoch: [16][789/817]	Loss 0.0091 (0.0604)	
training:	Epoch: [16][790/817]	Loss 0.0127 (0.0603)	
training:	Epoch: [16][791/817]	Loss 0.0096 (0.0602)	
training:	Epoch: [16][792/817]	Loss 0.0117 (0.0602)	
training:	Epoch: [16][793/817]	Loss 0.0093 (0.0601)	
training:	Epoch: [16][794/817]	Loss 0.6095 (0.0608)	
training:	Epoch: [16][795/817]	Loss 0.0104 (0.0607)	
training:	Epoch: [16][796/817]	Loss 0.0117 (0.0607)	
training:	Epoch: [16][797/817]	Loss 0.0149 (0.0606)	
training:	Epoch: [16][798/817]	Loss 0.5705 (0.0613)	
training:	Epoch: [16][799/817]	Loss 0.1251 (0.0613)	
training:	Epoch: [16][800/817]	Loss 0.0113 (0.0613)	
training:	Epoch: [16][801/817]	Loss 0.0113 (0.0612)	
training:	Epoch: [16][802/817]	Loss 0.0111 (0.0612)	
training:	Epoch: [16][803/817]	Loss 0.0116 (0.0611)	
training:	Epoch: [16][804/817]	Loss 0.0103 (0.0610)	
training:	Epoch: [16][805/817]	Loss 0.0099 (0.0610)	
training:	Epoch: [16][806/817]	Loss 0.0096 (0.0609)	
training:	Epoch: [16][807/817]	Loss 0.0103 (0.0608)	
training:	Epoch: [16][808/817]	Loss 0.1652 (0.0610)	
training:	Epoch: [16][809/817]	Loss 0.0102 (0.0609)	
training:	Epoch: [16][810/817]	Loss 0.0092 (0.0608)	
training:	Epoch: [16][811/817]	Loss 0.0103 (0.0608)	
training:	Epoch: [16][812/817]	Loss 0.0102 (0.0607)	
training:	Epoch: [16][813/817]	Loss 0.0140 (0.0607)	
training:	Epoch: [16][814/817]	Loss 0.0113 (0.0606)	
training:	Epoch: [16][815/817]	Loss 0.0101 (0.0605)	
training:	Epoch: [16][816/817]	Loss 0.2677 (0.0608)	
training:	Epoch: [16][817/817]	Loss 0.0377 (0.0608)	
Training:	 Loss: 0.0607

Training:	 ACC: 0.9882 0.9881 0.9844 0.9920
Validation:	 ACC: 0.7799 0.7790 0.7615 0.7982
Validation:	 Best_BACC: 0.8223 0.8218 0.8127 0.8318
Validation:	 Loss: 0.8379
Pretraining:	Epoch 17/200
----------
training:	Epoch: [17][1/817]	Loss 0.0098 (0.0098)	
training:	Epoch: [17][2/817]	Loss 0.0096 (0.0097)	
training:	Epoch: [17][3/817]	Loss 0.0093 (0.0096)	
training:	Epoch: [17][4/817]	Loss 0.2101 (0.0597)	
training:	Epoch: [17][5/817]	Loss 0.0088 (0.0495)	
training:	Epoch: [17][6/817]	Loss 0.0102 (0.0430)	
training:	Epoch: [17][7/817]	Loss 0.0113 (0.0384)	
training:	Epoch: [17][8/817]	Loss 0.0104 (0.0349)	
training:	Epoch: [17][9/817]	Loss 0.0090 (0.0321)	
training:	Epoch: [17][10/817]	Loss 0.0100 (0.0298)	
training:	Epoch: [17][11/817]	Loss 0.0423 (0.0310)	
training:	Epoch: [17][12/817]	Loss 0.6231 (0.0803)	
training:	Epoch: [17][13/817]	Loss 0.0104 (0.0749)	
training:	Epoch: [17][14/817]	Loss 0.0100 (0.0703)	
training:	Epoch: [17][15/817]	Loss 0.0097 (0.0663)	
training:	Epoch: [17][16/817]	Loss 0.0754 (0.0668)	
training:	Epoch: [17][17/817]	Loss 0.0120 (0.0636)	
training:	Epoch: [17][18/817]	Loss 0.0139 (0.0608)	
training:	Epoch: [17][19/817]	Loss 0.0100 (0.0582)	
training:	Epoch: [17][20/817]	Loss 0.0097 (0.0557)	
training:	Epoch: [17][21/817]	Loss 0.0105 (0.0536)	
training:	Epoch: [17][22/817]	Loss 0.0138 (0.0518)	
training:	Epoch: [17][23/817]	Loss 0.0092 (0.0499)	
training:	Epoch: [17][24/817]	Loss 0.6088 (0.0732)	
training:	Epoch: [17][25/817]	Loss 0.0157 (0.0709)	
training:	Epoch: [17][26/817]	Loss 0.0102 (0.0686)	
training:	Epoch: [17][27/817]	Loss 0.0106 (0.0664)	
training:	Epoch: [17][28/817]	Loss 0.0091 (0.0644)	
training:	Epoch: [17][29/817]	Loss 0.0202 (0.0629)	
training:	Epoch: [17][30/817]	Loss 0.0098 (0.0611)	
training:	Epoch: [17][31/817]	Loss 0.0097 (0.0594)	
training:	Epoch: [17][32/817]	Loss 0.0114 (0.0579)	
training:	Epoch: [17][33/817]	Loss 0.0119 (0.0565)	
training:	Epoch: [17][34/817]	Loss 0.0105 (0.0552)	
training:	Epoch: [17][35/817]	Loss 0.0109 (0.0539)	
training:	Epoch: [17][36/817]	Loss 0.0114 (0.0527)	
training:	Epoch: [17][37/817]	Loss 0.0118 (0.0516)	
training:	Epoch: [17][38/817]	Loss 0.0101 (0.0505)	
training:	Epoch: [17][39/817]	Loss 0.0096 (0.0495)	
training:	Epoch: [17][40/817]	Loss 0.0130 (0.0486)	
training:	Epoch: [17][41/817]	Loss 0.0093 (0.0476)	
training:	Epoch: [17][42/817]	Loss 0.0114 (0.0468)	
training:	Epoch: [17][43/817]	Loss 0.0094 (0.0459)	
training:	Epoch: [17][44/817]	Loss 0.0334 (0.0456)	
training:	Epoch: [17][45/817]	Loss 0.0306 (0.0453)	
training:	Epoch: [17][46/817]	Loss 0.0108 (0.0445)	
training:	Epoch: [17][47/817]	Loss 0.0092 (0.0438)	
training:	Epoch: [17][48/817]	Loss 0.0120 (0.0431)	
training:	Epoch: [17][49/817]	Loss 0.0119 (0.0425)	
training:	Epoch: [17][50/817]	Loss 0.0087 (0.0418)	
training:	Epoch: [17][51/817]	Loss 0.0170 (0.0413)	
training:	Epoch: [17][52/817]	Loss 0.1282 (0.0430)	
training:	Epoch: [17][53/817]	Loss 0.0631 (0.0434)	
training:	Epoch: [17][54/817]	Loss 0.0251 (0.0430)	
training:	Epoch: [17][55/817]	Loss 0.0116 (0.0424)	
training:	Epoch: [17][56/817]	Loss 0.0104 (0.0419)	
training:	Epoch: [17][57/817]	Loss 0.0097 (0.0413)	
training:	Epoch: [17][58/817]	Loss 0.0101 (0.0408)	
training:	Epoch: [17][59/817]	Loss 0.0097 (0.0402)	
training:	Epoch: [17][60/817]	Loss 0.0100 (0.0397)	
training:	Epoch: [17][61/817]	Loss 0.0099 (0.0393)	
training:	Epoch: [17][62/817]	Loss 0.0231 (0.0390)	
training:	Epoch: [17][63/817]	Loss 0.0105 (0.0385)	
training:	Epoch: [17][64/817]	Loss 0.0130 (0.0381)	
training:	Epoch: [17][65/817]	Loss 0.6215 (0.0471)	
training:	Epoch: [17][66/817]	Loss 0.1159 (0.0482)	
training:	Epoch: [17][67/817]	Loss 0.0097 (0.0476)	
training:	Epoch: [17][68/817]	Loss 0.0118 (0.0471)	
training:	Epoch: [17][69/817]	Loss 0.0109 (0.0465)	
training:	Epoch: [17][70/817]	Loss 0.0107 (0.0460)	
training:	Epoch: [17][71/817]	Loss 0.0100 (0.0455)	
training:	Epoch: [17][72/817]	Loss 0.0091 (0.0450)	
training:	Epoch: [17][73/817]	Loss 0.0192 (0.0447)	
training:	Epoch: [17][74/817]	Loss 0.0096 (0.0442)	
training:	Epoch: [17][75/817]	Loss 0.0109 (0.0437)	
training:	Epoch: [17][76/817]	Loss 0.0247 (0.0435)	
training:	Epoch: [17][77/817]	Loss 0.0095 (0.0430)	
training:	Epoch: [17][78/817]	Loss 0.0098 (0.0426)	
training:	Epoch: [17][79/817]	Loss 0.0372 (0.0426)	
training:	Epoch: [17][80/817]	Loss 0.0100 (0.0421)	
training:	Epoch: [17][81/817]	Loss 0.0102 (0.0417)	
training:	Epoch: [17][82/817]	Loss 0.0096 (0.0414)	
training:	Epoch: [17][83/817]	Loss 0.0092 (0.0410)	
training:	Epoch: [17][84/817]	Loss 0.0100 (0.0406)	
training:	Epoch: [17][85/817]	Loss 0.0092 (0.0402)	
training:	Epoch: [17][86/817]	Loss 0.0090 (0.0399)	
training:	Epoch: [17][87/817]	Loss 0.0099 (0.0395)	
training:	Epoch: [17][88/817]	Loss 0.0097 (0.0392)	
training:	Epoch: [17][89/817]	Loss 0.0101 (0.0389)	
training:	Epoch: [17][90/817]	Loss 0.1063 (0.0396)	
training:	Epoch: [17][91/817]	Loss 0.0121 (0.0393)	
training:	Epoch: [17][92/817]	Loss 0.0099 (0.0390)	
training:	Epoch: [17][93/817]	Loss 0.0125 (0.0387)	
training:	Epoch: [17][94/817]	Loss 0.5658 (0.0443)	
training:	Epoch: [17][95/817]	Loss 0.5360 (0.0495)	
training:	Epoch: [17][96/817]	Loss 0.0099 (0.0491)	
training:	Epoch: [17][97/817]	Loss 0.0100 (0.0487)	
training:	Epoch: [17][98/817]	Loss 0.0100 (0.0483)	
training:	Epoch: [17][99/817]	Loss 0.0105 (0.0479)	
training:	Epoch: [17][100/817]	Loss 0.5700 (0.0531)	
training:	Epoch: [17][101/817]	Loss 0.0094 (0.0527)	
training:	Epoch: [17][102/817]	Loss 0.0108 (0.0523)	
training:	Epoch: [17][103/817]	Loss 0.0109 (0.0519)	
training:	Epoch: [17][104/817]	Loss 0.0091 (0.0515)	
training:	Epoch: [17][105/817]	Loss 0.0104 (0.0511)	
training:	Epoch: [17][106/817]	Loss 0.0092 (0.0507)	
training:	Epoch: [17][107/817]	Loss 0.0100 (0.0503)	
training:	Epoch: [17][108/817]	Loss 0.0104 (0.0499)	
training:	Epoch: [17][109/817]	Loss 0.0207 (0.0497)	
training:	Epoch: [17][110/817]	Loss 0.0101 (0.0493)	
training:	Epoch: [17][111/817]	Loss 0.0127 (0.0490)	
training:	Epoch: [17][112/817]	Loss 0.0094 (0.0486)	
training:	Epoch: [17][113/817]	Loss 0.0095 (0.0483)	
training:	Epoch: [17][114/817]	Loss 0.0092 (0.0479)	
training:	Epoch: [17][115/817]	Loss 0.0098 (0.0476)	
training:	Epoch: [17][116/817]	Loss 0.5581 (0.0520)	
training:	Epoch: [17][117/817]	Loss 0.0096 (0.0516)	
training:	Epoch: [17][118/817]	Loss 0.0100 (0.0513)	
training:	Epoch: [17][119/817]	Loss 0.0101 (0.0509)	
training:	Epoch: [17][120/817]	Loss 0.6033 (0.0555)	
training:	Epoch: [17][121/817]	Loss 0.0125 (0.0552)	
training:	Epoch: [17][122/817]	Loss 0.0108 (0.0548)	
training:	Epoch: [17][123/817]	Loss 0.0099 (0.0544)	
training:	Epoch: [17][124/817]	Loss 0.0280 (0.0542)	
training:	Epoch: [17][125/817]	Loss 0.0091 (0.0539)	
training:	Epoch: [17][126/817]	Loss 0.0098 (0.0535)	
training:	Epoch: [17][127/817]	Loss 0.0092 (0.0532)	
training:	Epoch: [17][128/817]	Loss 0.0086 (0.0528)	
training:	Epoch: [17][129/817]	Loss 0.0135 (0.0525)	
training:	Epoch: [17][130/817]	Loss 0.0105 (0.0522)	
training:	Epoch: [17][131/817]	Loss 0.0115 (0.0519)	
training:	Epoch: [17][132/817]	Loss 0.0094 (0.0516)	
training:	Epoch: [17][133/817]	Loss 0.0090 (0.0512)	
training:	Epoch: [17][134/817]	Loss 0.0343 (0.0511)	
training:	Epoch: [17][135/817]	Loss 0.0104 (0.0508)	
training:	Epoch: [17][136/817]	Loss 0.0101 (0.0505)	
training:	Epoch: [17][137/817]	Loss 0.0118 (0.0502)	
training:	Epoch: [17][138/817]	Loss 0.0097 (0.0499)	
training:	Epoch: [17][139/817]	Loss 0.0101 (0.0497)	
training:	Epoch: [17][140/817]	Loss 0.0092 (0.0494)	
training:	Epoch: [17][141/817]	Loss 0.0125 (0.0491)	
training:	Epoch: [17][142/817]	Loss 0.0098 (0.0488)	
training:	Epoch: [17][143/817]	Loss 0.0109 (0.0486)	
training:	Epoch: [17][144/817]	Loss 0.0120 (0.0483)	
training:	Epoch: [17][145/817]	Loss 0.0119 (0.0481)	
training:	Epoch: [17][146/817]	Loss 0.5179 (0.0513)	
training:	Epoch: [17][147/817]	Loss 0.0095 (0.0510)	
training:	Epoch: [17][148/817]	Loss 0.0106 (0.0507)	
training:	Epoch: [17][149/817]	Loss 0.0111 (0.0505)	
training:	Epoch: [17][150/817]	Loss 0.0110 (0.0502)	
training:	Epoch: [17][151/817]	Loss 0.0109 (0.0499)	
training:	Epoch: [17][152/817]	Loss 0.0080 (0.0497)	
training:	Epoch: [17][153/817]	Loss 0.0214 (0.0495)	
training:	Epoch: [17][154/817]	Loss 0.0134 (0.0492)	
training:	Epoch: [17][155/817]	Loss 0.0148 (0.0490)	
training:	Epoch: [17][156/817]	Loss 0.0087 (0.0488)	
training:	Epoch: [17][157/817]	Loss 0.5570 (0.0520)	
training:	Epoch: [17][158/817]	Loss 0.0119 (0.0517)	
training:	Epoch: [17][159/817]	Loss 0.0110 (0.0515)	
training:	Epoch: [17][160/817]	Loss 0.0097 (0.0512)	
training:	Epoch: [17][161/817]	Loss 0.0110 (0.0510)	
training:	Epoch: [17][162/817]	Loss 0.0098 (0.0507)	
training:	Epoch: [17][163/817]	Loss 0.0124 (0.0505)	
training:	Epoch: [17][164/817]	Loss 0.0130 (0.0503)	
training:	Epoch: [17][165/817]	Loss 0.0111 (0.0500)	
training:	Epoch: [17][166/817]	Loss 0.0101 (0.0498)	
training:	Epoch: [17][167/817]	Loss 0.0087 (0.0495)	
training:	Epoch: [17][168/817]	Loss 0.6085 (0.0529)	
training:	Epoch: [17][169/817]	Loss 0.0106 (0.0526)	
training:	Epoch: [17][170/817]	Loss 0.0100 (0.0524)	
training:	Epoch: [17][171/817]	Loss 0.0099 (0.0521)	
training:	Epoch: [17][172/817]	Loss 0.0087 (0.0519)	
training:	Epoch: [17][173/817]	Loss 0.0104 (0.0516)	
training:	Epoch: [17][174/817]	Loss 0.6225 (0.0549)	
training:	Epoch: [17][175/817]	Loss 0.0102 (0.0546)	
training:	Epoch: [17][176/817]	Loss 0.0101 (0.0544)	
training:	Epoch: [17][177/817]	Loss 0.0172 (0.0542)	
training:	Epoch: [17][178/817]	Loss 0.0116 (0.0539)	
training:	Epoch: [17][179/817]	Loss 0.0232 (0.0538)	
training:	Epoch: [17][180/817]	Loss 0.0106 (0.0535)	
training:	Epoch: [17][181/817]	Loss 0.0116 (0.0533)	
training:	Epoch: [17][182/817]	Loss 0.0090 (0.0531)	
training:	Epoch: [17][183/817]	Loss 0.0103 (0.0528)	
training:	Epoch: [17][184/817]	Loss 0.0093 (0.0526)	
training:	Epoch: [17][185/817]	Loss 0.0097 (0.0524)	
training:	Epoch: [17][186/817]	Loss 0.0121 (0.0521)	
training:	Epoch: [17][187/817]	Loss 0.0108 (0.0519)	
training:	Epoch: [17][188/817]	Loss 0.0108 (0.0517)	
training:	Epoch: [17][189/817]	Loss 0.0099 (0.0515)	
training:	Epoch: [17][190/817]	Loss 0.0105 (0.0513)	
training:	Epoch: [17][191/817]	Loss 0.0103 (0.0510)	
training:	Epoch: [17][192/817]	Loss 0.0126 (0.0508)	
training:	Epoch: [17][193/817]	Loss 0.0093 (0.0506)	
training:	Epoch: [17][194/817]	Loss 0.0093 (0.0504)	
training:	Epoch: [17][195/817]	Loss 0.0100 (0.0502)	
training:	Epoch: [17][196/817]	Loss 0.0145 (0.0500)	
training:	Epoch: [17][197/817]	Loss 0.0111 (0.0498)	
training:	Epoch: [17][198/817]	Loss 0.0113 (0.0496)	
training:	Epoch: [17][199/817]	Loss 0.0104 (0.0494)	
training:	Epoch: [17][200/817]	Loss 0.0104 (0.0492)	
training:	Epoch: [17][201/817]	Loss 0.0149 (0.0491)	
training:	Epoch: [17][202/817]	Loss 0.0090 (0.0489)	
training:	Epoch: [17][203/817]	Loss 0.0104 (0.0487)	
training:	Epoch: [17][204/817]	Loss 0.0405 (0.0486)	
training:	Epoch: [17][205/817]	Loss 0.0094 (0.0485)	
training:	Epoch: [17][206/817]	Loss 0.0104 (0.0483)	
training:	Epoch: [17][207/817]	Loss 0.5572 (0.0507)	
training:	Epoch: [17][208/817]	Loss 0.0082 (0.0505)	
training:	Epoch: [17][209/817]	Loss 0.0155 (0.0504)	
training:	Epoch: [17][210/817]	Loss 0.0104 (0.0502)	
training:	Epoch: [17][211/817]	Loss 0.0091 (0.0500)	
training:	Epoch: [17][212/817]	Loss 0.0111 (0.0498)	
training:	Epoch: [17][213/817]	Loss 0.0175 (0.0496)	
training:	Epoch: [17][214/817]	Loss 0.0099 (0.0494)	
training:	Epoch: [17][215/817]	Loss 0.0150 (0.0493)	
training:	Epoch: [17][216/817]	Loss 0.0126 (0.0491)	
training:	Epoch: [17][217/817]	Loss 0.0106 (0.0489)	
training:	Epoch: [17][218/817]	Loss 0.0100 (0.0488)	
training:	Epoch: [17][219/817]	Loss 0.0092 (0.0486)	
training:	Epoch: [17][220/817]	Loss 0.0129 (0.0484)	
training:	Epoch: [17][221/817]	Loss 0.0177 (0.0483)	
training:	Epoch: [17][222/817]	Loss 0.0104 (0.0481)	
training:	Epoch: [17][223/817]	Loss 0.0355 (0.0481)	
training:	Epoch: [17][224/817]	Loss 0.0109 (0.0479)	
training:	Epoch: [17][225/817]	Loss 0.0087 (0.0477)	
training:	Epoch: [17][226/817]	Loss 0.0097 (0.0475)	
training:	Epoch: [17][227/817]	Loss 0.0105 (0.0474)	
training:	Epoch: [17][228/817]	Loss 0.0105 (0.0472)	
training:	Epoch: [17][229/817]	Loss 0.5273 (0.0493)	
training:	Epoch: [17][230/817]	Loss 0.6156 (0.0518)	
training:	Epoch: [17][231/817]	Loss 0.0101 (0.0516)	
training:	Epoch: [17][232/817]	Loss 0.0094 (0.0514)	
training:	Epoch: [17][233/817]	Loss 0.0107 (0.0512)	
training:	Epoch: [17][234/817]	Loss 0.0118 (0.0511)	
training:	Epoch: [17][235/817]	Loss 0.0103 (0.0509)	
training:	Epoch: [17][236/817]	Loss 0.0099 (0.0507)	
training:	Epoch: [17][237/817]	Loss 0.0112 (0.0506)	
training:	Epoch: [17][238/817]	Loss 0.0102 (0.0504)	
training:	Epoch: [17][239/817]	Loss 0.0088 (0.0502)	
training:	Epoch: [17][240/817]	Loss 0.0966 (0.0504)	
training:	Epoch: [17][241/817]	Loss 0.0104 (0.0502)	
training:	Epoch: [17][242/817]	Loss 0.0087 (0.0501)	
training:	Epoch: [17][243/817]	Loss 0.0100 (0.0499)	
training:	Epoch: [17][244/817]	Loss 0.0105 (0.0497)	
training:	Epoch: [17][245/817]	Loss 0.0121 (0.0496)	
training:	Epoch: [17][246/817]	Loss 0.0097 (0.0494)	
training:	Epoch: [17][247/817]	Loss 0.0116 (0.0493)	
training:	Epoch: [17][248/817]	Loss 0.0096 (0.0491)	
training:	Epoch: [17][249/817]	Loss 0.0097 (0.0490)	
training:	Epoch: [17][250/817]	Loss 0.0088 (0.0488)	
training:	Epoch: [17][251/817]	Loss 0.0102 (0.0486)	
training:	Epoch: [17][252/817]	Loss 0.0112 (0.0485)	
training:	Epoch: [17][253/817]	Loss 0.0111 (0.0483)	
training:	Epoch: [17][254/817]	Loss 0.0114 (0.0482)	
training:	Epoch: [17][255/817]	Loss 0.0105 (0.0481)	
training:	Epoch: [17][256/817]	Loss 0.0110 (0.0479)	
training:	Epoch: [17][257/817]	Loss 0.0119 (0.0478)	
training:	Epoch: [17][258/817]	Loss 0.0082 (0.0476)	
training:	Epoch: [17][259/817]	Loss 0.0119 (0.0475)	
training:	Epoch: [17][260/817]	Loss 0.0092 (0.0473)	
training:	Epoch: [17][261/817]	Loss 0.0113 (0.0472)	
training:	Epoch: [17][262/817]	Loss 0.0122 (0.0471)	
training:	Epoch: [17][263/817]	Loss 0.0087 (0.0469)	
training:	Epoch: [17][264/817]	Loss 0.0089 (0.0468)	
training:	Epoch: [17][265/817]	Loss 0.6187 (0.0489)	
training:	Epoch: [17][266/817]	Loss 0.0085 (0.0488)	
training:	Epoch: [17][267/817]	Loss 0.0089 (0.0486)	
training:	Epoch: [17][268/817]	Loss 0.0093 (0.0485)	
training:	Epoch: [17][269/817]	Loss 0.0093 (0.0483)	
training:	Epoch: [17][270/817]	Loss 0.0107 (0.0482)	
training:	Epoch: [17][271/817]	Loss 0.1562 (0.0486)	
training:	Epoch: [17][272/817]	Loss 0.0100 (0.0484)	
training:	Epoch: [17][273/817]	Loss 0.0094 (0.0483)	
training:	Epoch: [17][274/817]	Loss 0.0105 (0.0482)	
training:	Epoch: [17][275/817]	Loss 0.0096 (0.0480)	
training:	Epoch: [17][276/817]	Loss 0.0115 (0.0479)	
training:	Epoch: [17][277/817]	Loss 0.8878 (0.0509)	
training:	Epoch: [17][278/817]	Loss 0.0105 (0.0508)	
training:	Epoch: [17][279/817]	Loss 0.0108 (0.0506)	
training:	Epoch: [17][280/817]	Loss 0.0089 (0.0505)	
training:	Epoch: [17][281/817]	Loss 0.0087 (0.0503)	
training:	Epoch: [17][282/817]	Loss 0.0105 (0.0502)	
training:	Epoch: [17][283/817]	Loss 0.0128 (0.0501)	
training:	Epoch: [17][284/817]	Loss 0.0111 (0.0499)	
training:	Epoch: [17][285/817]	Loss 0.0095 (0.0498)	
training:	Epoch: [17][286/817]	Loss 0.6162 (0.0518)	
training:	Epoch: [17][287/817]	Loss 0.0102 (0.0516)	
training:	Epoch: [17][288/817]	Loss 0.0098 (0.0515)	
training:	Epoch: [17][289/817]	Loss 0.0104 (0.0513)	
training:	Epoch: [17][290/817]	Loss 0.0095 (0.0512)	
training:	Epoch: [17][291/817]	Loss 0.0096 (0.0510)	
training:	Epoch: [17][292/817]	Loss 0.6022 (0.0529)	
training:	Epoch: [17][293/817]	Loss 0.5578 (0.0547)	
training:	Epoch: [17][294/817]	Loss 0.0114 (0.0545)	
training:	Epoch: [17][295/817]	Loss 0.0102 (0.0544)	
training:	Epoch: [17][296/817]	Loss 0.0105 (0.0542)	
training:	Epoch: [17][297/817]	Loss 0.0099 (0.0541)	
training:	Epoch: [17][298/817]	Loss 0.0104 (0.0539)	
training:	Epoch: [17][299/817]	Loss 0.0105 (0.0538)	
training:	Epoch: [17][300/817]	Loss 0.0173 (0.0537)	
training:	Epoch: [17][301/817]	Loss 0.0100 (0.0535)	
training:	Epoch: [17][302/817]	Loss 0.0485 (0.0535)	
training:	Epoch: [17][303/817]	Loss 0.0118 (0.0534)	
training:	Epoch: [17][304/817]	Loss 0.0108 (0.0532)	
training:	Epoch: [17][305/817]	Loss 0.0091 (0.0531)	
training:	Epoch: [17][306/817]	Loss 0.0122 (0.0529)	
training:	Epoch: [17][307/817]	Loss 0.0088 (0.0528)	
training:	Epoch: [17][308/817]	Loss 0.0110 (0.0527)	
training:	Epoch: [17][309/817]	Loss 0.0104 (0.0525)	
training:	Epoch: [17][310/817]	Loss 0.0275 (0.0524)	
training:	Epoch: [17][311/817]	Loss 0.0103 (0.0523)	
training:	Epoch: [17][312/817]	Loss 0.0103 (0.0522)	
training:	Epoch: [17][313/817]	Loss 0.0108 (0.0520)	
training:	Epoch: [17][314/817]	Loss 0.5644 (0.0537)	
training:	Epoch: [17][315/817]	Loss 0.0183 (0.0536)	
training:	Epoch: [17][316/817]	Loss 0.0104 (0.0534)	
training:	Epoch: [17][317/817]	Loss 0.0106 (0.0533)	
training:	Epoch: [17][318/817]	Loss 0.0109 (0.0531)	
training:	Epoch: [17][319/817]	Loss 0.0108 (0.0530)	
training:	Epoch: [17][320/817]	Loss 0.0104 (0.0529)	
training:	Epoch: [17][321/817]	Loss 0.0442 (0.0529)	
training:	Epoch: [17][322/817]	Loss 0.0104 (0.0527)	
training:	Epoch: [17][323/817]	Loss 0.0113 (0.0526)	
training:	Epoch: [17][324/817]	Loss 0.3960 (0.0537)	
training:	Epoch: [17][325/817]	Loss 1.2204 (0.0572)	
training:	Epoch: [17][326/817]	Loss 0.0125 (0.0571)	
training:	Epoch: [17][327/817]	Loss 0.6204 (0.0588)	
training:	Epoch: [17][328/817]	Loss 0.0115 (0.0587)	
training:	Epoch: [17][329/817]	Loss 0.0107 (0.0585)	
training:	Epoch: [17][330/817]	Loss 0.0100 (0.0584)	
training:	Epoch: [17][331/817]	Loss 0.0107 (0.0583)	
training:	Epoch: [17][332/817]	Loss 0.0092 (0.0581)	
training:	Epoch: [17][333/817]	Loss 0.0091 (0.0580)	
training:	Epoch: [17][334/817]	Loss 0.5674 (0.0595)	
training:	Epoch: [17][335/817]	Loss 0.0140 (0.0593)	
training:	Epoch: [17][336/817]	Loss 0.0096 (0.0592)	
training:	Epoch: [17][337/817]	Loss 0.0096 (0.0591)	
training:	Epoch: [17][338/817]	Loss 0.0137 (0.0589)	
training:	Epoch: [17][339/817]	Loss 0.0091 (0.0588)	
training:	Epoch: [17][340/817]	Loss 0.5295 (0.0602)	
training:	Epoch: [17][341/817]	Loss 0.0096 (0.0600)	
training:	Epoch: [17][342/817]	Loss 0.0208 (0.0599)	
training:	Epoch: [17][343/817]	Loss 0.0104 (0.0597)	
training:	Epoch: [17][344/817]	Loss 0.6102 (0.0613)	
training:	Epoch: [17][345/817]	Loss 0.0132 (0.0612)	
training:	Epoch: [17][346/817]	Loss 0.0099 (0.0611)	
training:	Epoch: [17][347/817]	Loss 0.0101 (0.0609)	
training:	Epoch: [17][348/817]	Loss 0.0126 (0.0608)	
training:	Epoch: [17][349/817]	Loss 0.0106 (0.0606)	
training:	Epoch: [17][350/817]	Loss 0.0102 (0.0605)	
training:	Epoch: [17][351/817]	Loss 0.0170 (0.0604)	
training:	Epoch: [17][352/817]	Loss 0.0111 (0.0602)	
training:	Epoch: [17][353/817]	Loss 0.0109 (0.0601)	
training:	Epoch: [17][354/817]	Loss 0.0103 (0.0599)	
training:	Epoch: [17][355/817]	Loss 0.0484 (0.0599)	
training:	Epoch: [17][356/817]	Loss 0.0122 (0.0598)	
training:	Epoch: [17][357/817]	Loss 0.0129 (0.0596)	
training:	Epoch: [17][358/817]	Loss 0.0116 (0.0595)	
training:	Epoch: [17][359/817]	Loss 0.0098 (0.0594)	
training:	Epoch: [17][360/817]	Loss 0.0104 (0.0592)	
training:	Epoch: [17][361/817]	Loss 0.0123 (0.0591)	
training:	Epoch: [17][362/817]	Loss 0.0097 (0.0590)	
training:	Epoch: [17][363/817]	Loss 0.0110 (0.0588)	
training:	Epoch: [17][364/817]	Loss 0.0103 (0.0587)	
training:	Epoch: [17][365/817]	Loss 0.0104 (0.0586)	
training:	Epoch: [17][366/817]	Loss 0.0100 (0.0584)	
training:	Epoch: [17][367/817]	Loss 0.0132 (0.0583)	
training:	Epoch: [17][368/817]	Loss 0.0106 (0.0582)	
training:	Epoch: [17][369/817]	Loss 0.0285 (0.0581)	
training:	Epoch: [17][370/817]	Loss 0.0114 (0.0580)	
training:	Epoch: [17][371/817]	Loss 0.5724 (0.0594)	
training:	Epoch: [17][372/817]	Loss 0.0119 (0.0592)	
training:	Epoch: [17][373/817]	Loss 0.0097 (0.0591)	
training:	Epoch: [17][374/817]	Loss 0.0116 (0.0590)	
training:	Epoch: [17][375/817]	Loss 0.0105 (0.0588)	
training:	Epoch: [17][376/817]	Loss 0.0122 (0.0587)	
training:	Epoch: [17][377/817]	Loss 0.0093 (0.0586)	
training:	Epoch: [17][378/817]	Loss 0.1221 (0.0588)	
training:	Epoch: [17][379/817]	Loss 0.0090 (0.0586)	
training:	Epoch: [17][380/817]	Loss 0.0094 (0.0585)	
training:	Epoch: [17][381/817]	Loss 0.0121 (0.0584)	
training:	Epoch: [17][382/817]	Loss 0.0100 (0.0583)	
training:	Epoch: [17][383/817]	Loss 0.0094 (0.0581)	
training:	Epoch: [17][384/817]	Loss 0.0130 (0.0580)	
training:	Epoch: [17][385/817]	Loss 0.0128 (0.0579)	
training:	Epoch: [17][386/817]	Loss 0.0105 (0.0578)	
training:	Epoch: [17][387/817]	Loss 0.0094 (0.0576)	
training:	Epoch: [17][388/817]	Loss 0.0137 (0.0575)	
training:	Epoch: [17][389/817]	Loss 0.0119 (0.0574)	
training:	Epoch: [17][390/817]	Loss 0.0092 (0.0573)	
training:	Epoch: [17][391/817]	Loss 0.0101 (0.0572)	
training:	Epoch: [17][392/817]	Loss 0.0098 (0.0570)	
training:	Epoch: [17][393/817]	Loss 0.0141 (0.0569)	
training:	Epoch: [17][394/817]	Loss 0.0378 (0.0569)	
training:	Epoch: [17][395/817]	Loss 0.0097 (0.0568)	
training:	Epoch: [17][396/817]	Loss 0.0116 (0.0567)	
training:	Epoch: [17][397/817]	Loss 0.0153 (0.0566)	
training:	Epoch: [17][398/817]	Loss 0.2311 (0.0570)	
training:	Epoch: [17][399/817]	Loss 0.0576 (0.0570)	
training:	Epoch: [17][400/817]	Loss 0.0106 (0.0569)	
training:	Epoch: [17][401/817]	Loss 0.0129 (0.0568)	
training:	Epoch: [17][402/817]	Loss 0.0091 (0.0566)	
training:	Epoch: [17][403/817]	Loss 0.0166 (0.0565)	
training:	Epoch: [17][404/817]	Loss 0.0097 (0.0564)	
training:	Epoch: [17][405/817]	Loss 0.0098 (0.0563)	
training:	Epoch: [17][406/817]	Loss 0.0111 (0.0562)	
training:	Epoch: [17][407/817]	Loss 0.0100 (0.0561)	
training:	Epoch: [17][408/817]	Loss 0.1630 (0.0564)	
training:	Epoch: [17][409/817]	Loss 0.0109 (0.0562)	
training:	Epoch: [17][410/817]	Loss 0.0111 (0.0561)	
training:	Epoch: [17][411/817]	Loss 0.0100 (0.0560)	
training:	Epoch: [17][412/817]	Loss 0.0082 (0.0559)	
training:	Epoch: [17][413/817]	Loss 0.0090 (0.0558)	
training:	Epoch: [17][414/817]	Loss 0.0098 (0.0557)	
training:	Epoch: [17][415/817]	Loss 0.0527 (0.0557)	
training:	Epoch: [17][416/817]	Loss 0.0102 (0.0556)	
training:	Epoch: [17][417/817]	Loss 0.0139 (0.0555)	
training:	Epoch: [17][418/817]	Loss 0.0088 (0.0554)	
training:	Epoch: [17][419/817]	Loss 0.5538 (0.0565)	
training:	Epoch: [17][420/817]	Loss 0.1035 (0.0567)	
training:	Epoch: [17][421/817]	Loss 0.0113 (0.0565)	
training:	Epoch: [17][422/817]	Loss 0.0101 (0.0564)	
training:	Epoch: [17][423/817]	Loss 0.0106 (0.0563)	
training:	Epoch: [17][424/817]	Loss 0.0101 (0.0562)	
training:	Epoch: [17][425/817]	Loss 0.5749 (0.0574)	
training:	Epoch: [17][426/817]	Loss 0.0120 (0.0573)	
training:	Epoch: [17][427/817]	Loss 0.0115 (0.0572)	
training:	Epoch: [17][428/817]	Loss 0.0104 (0.0571)	
training:	Epoch: [17][429/817]	Loss 0.0108 (0.0570)	
training:	Epoch: [17][430/817]	Loss 0.2099 (0.0574)	
training:	Epoch: [17][431/817]	Loss 0.0839 (0.0574)	
training:	Epoch: [17][432/817]	Loss 0.0115 (0.0573)	
training:	Epoch: [17][433/817]	Loss 0.0095 (0.0572)	
training:	Epoch: [17][434/817]	Loss 0.0164 (0.0571)	
training:	Epoch: [17][435/817]	Loss 0.0090 (0.0570)	
training:	Epoch: [17][436/817]	Loss 0.0116 (0.0569)	
training:	Epoch: [17][437/817]	Loss 0.5656 (0.0581)	
training:	Epoch: [17][438/817]	Loss 0.0485 (0.0580)	
training:	Epoch: [17][439/817]	Loss 0.0113 (0.0579)	
training:	Epoch: [17][440/817]	Loss 0.0109 (0.0578)	
training:	Epoch: [17][441/817]	Loss 0.0105 (0.0577)	
training:	Epoch: [17][442/817]	Loss 0.0103 (0.0576)	
training:	Epoch: [17][443/817]	Loss 0.0107 (0.0575)	
training:	Epoch: [17][444/817]	Loss 0.0106 (0.0574)	
training:	Epoch: [17][445/817]	Loss 0.0180 (0.0573)	
training:	Epoch: [17][446/817]	Loss 0.0121 (0.0572)	
training:	Epoch: [17][447/817]	Loss 0.0129 (0.0571)	
training:	Epoch: [17][448/817]	Loss 0.0088 (0.0570)	
training:	Epoch: [17][449/817]	Loss 0.0089 (0.0569)	
training:	Epoch: [17][450/817]	Loss 0.0082 (0.0568)	
training:	Epoch: [17][451/817]	Loss 0.0159 (0.0567)	
training:	Epoch: [17][452/817]	Loss 0.0114 (0.0566)	
training:	Epoch: [17][453/817]	Loss 0.0084 (0.0565)	
training:	Epoch: [17][454/817]	Loss 0.0120 (0.0564)	
training:	Epoch: [17][455/817]	Loss 0.0183 (0.0563)	
training:	Epoch: [17][456/817]	Loss 0.0158 (0.0562)	
training:	Epoch: [17][457/817]	Loss 0.0103 (0.0561)	
training:	Epoch: [17][458/817]	Loss 0.0179 (0.0560)	
training:	Epoch: [17][459/817]	Loss 0.0099 (0.0559)	
training:	Epoch: [17][460/817]	Loss 0.0139 (0.0558)	
training:	Epoch: [17][461/817]	Loss 0.0078 (0.0557)	
training:	Epoch: [17][462/817]	Loss 0.1933 (0.0560)	
training:	Epoch: [17][463/817]	Loss 0.0692 (0.0561)	
training:	Epoch: [17][464/817]	Loss 0.0139 (0.0560)	
training:	Epoch: [17][465/817]	Loss 0.0113 (0.0559)	
training:	Epoch: [17][466/817]	Loss 0.0097 (0.0558)	
training:	Epoch: [17][467/817]	Loss 0.0100 (0.0557)	
training:	Epoch: [17][468/817]	Loss 0.0230 (0.0556)	
training:	Epoch: [17][469/817]	Loss 0.0101 (0.0555)	
training:	Epoch: [17][470/817]	Loss 0.0097 (0.0554)	
training:	Epoch: [17][471/817]	Loss 0.0114 (0.0553)	
training:	Epoch: [17][472/817]	Loss 0.0108 (0.0552)	
training:	Epoch: [17][473/817]	Loss 0.0095 (0.0551)	
training:	Epoch: [17][474/817]	Loss 0.0195 (0.0551)	
training:	Epoch: [17][475/817]	Loss 0.0095 (0.0550)	
training:	Epoch: [17][476/817]	Loss 0.0149 (0.0549)	
training:	Epoch: [17][477/817]	Loss 0.0108 (0.0548)	
training:	Epoch: [17][478/817]	Loss 0.0103 (0.0547)	
training:	Epoch: [17][479/817]	Loss 0.0101 (0.0546)	
training:	Epoch: [17][480/817]	Loss 0.0084 (0.0545)	
training:	Epoch: [17][481/817]	Loss 0.0103 (0.0544)	
training:	Epoch: [17][482/817]	Loss 0.0105 (0.0543)	
training:	Epoch: [17][483/817]	Loss 0.0126 (0.0542)	
training:	Epoch: [17][484/817]	Loss 0.0125 (0.0541)	
training:	Epoch: [17][485/817]	Loss 0.0145 (0.0541)	
training:	Epoch: [17][486/817]	Loss 0.0095 (0.0540)	
training:	Epoch: [17][487/817]	Loss 0.0097 (0.0539)	
training:	Epoch: [17][488/817]	Loss 0.5876 (0.0550)	
training:	Epoch: [17][489/817]	Loss 0.0111 (0.0549)	
training:	Epoch: [17][490/817]	Loss 0.0148 (0.0548)	
training:	Epoch: [17][491/817]	Loss 0.0255 (0.0547)	
training:	Epoch: [17][492/817]	Loss 0.0081 (0.0547)	
training:	Epoch: [17][493/817]	Loss 0.0098 (0.0546)	
training:	Epoch: [17][494/817]	Loss 0.0090 (0.0545)	
training:	Epoch: [17][495/817]	Loss 0.0110 (0.0544)	
training:	Epoch: [17][496/817]	Loss 0.6316 (0.0555)	
training:	Epoch: [17][497/817]	Loss 0.0090 (0.0555)	
training:	Epoch: [17][498/817]	Loss 0.0092 (0.0554)	
training:	Epoch: [17][499/817]	Loss 0.0097 (0.0553)	
training:	Epoch: [17][500/817]	Loss 0.0107 (0.0552)	
training:	Epoch: [17][501/817]	Loss 0.5623 (0.0562)	
training:	Epoch: [17][502/817]	Loss 0.0107 (0.0561)	
training:	Epoch: [17][503/817]	Loss 0.5317 (0.0570)	
training:	Epoch: [17][504/817]	Loss 0.3002 (0.0575)	
training:	Epoch: [17][505/817]	Loss 0.0113 (0.0574)	
training:	Epoch: [17][506/817]	Loss 0.0105 (0.0573)	
training:	Epoch: [17][507/817]	Loss 0.0101 (0.0572)	
training:	Epoch: [17][508/817]	Loss 0.0112 (0.0572)	
training:	Epoch: [17][509/817]	Loss 0.0089 (0.0571)	
training:	Epoch: [17][510/817]	Loss 0.0109 (0.0570)	
training:	Epoch: [17][511/817]	Loss 0.0087 (0.0569)	
training:	Epoch: [17][512/817]	Loss 0.0221 (0.0568)	
training:	Epoch: [17][513/817]	Loss 0.0124 (0.0567)	
training:	Epoch: [17][514/817]	Loss 0.0102 (0.0566)	
training:	Epoch: [17][515/817]	Loss 0.0106 (0.0565)	
training:	Epoch: [17][516/817]	Loss 0.0117 (0.0565)	
training:	Epoch: [17][517/817]	Loss 0.1953 (0.0567)	
training:	Epoch: [17][518/817]	Loss 0.0102 (0.0566)	
training:	Epoch: [17][519/817]	Loss 0.0104 (0.0565)	
training:	Epoch: [17][520/817]	Loss 0.0430 (0.0565)	
training:	Epoch: [17][521/817]	Loss 0.0108 (0.0564)	
training:	Epoch: [17][522/817]	Loss 0.5787 (0.0574)	
training:	Epoch: [17][523/817]	Loss 0.0087 (0.0573)	
training:	Epoch: [17][524/817]	Loss 0.0236 (0.0573)	
training:	Epoch: [17][525/817]	Loss 0.0176 (0.0572)	
training:	Epoch: [17][526/817]	Loss 0.0101 (0.0571)	
training:	Epoch: [17][527/817]	Loss 0.0101 (0.0570)	
training:	Epoch: [17][528/817]	Loss 0.0092 (0.0569)	
training:	Epoch: [17][529/817]	Loss 0.6241 (0.0580)	
training:	Epoch: [17][530/817]	Loss 0.0129 (0.0579)	
training:	Epoch: [17][531/817]	Loss 0.0098 (0.0578)	
training:	Epoch: [17][532/817]	Loss 0.0295 (0.0578)	
training:	Epoch: [17][533/817]	Loss 0.0120 (0.0577)	
training:	Epoch: [17][534/817]	Loss 0.0095 (0.0576)	
training:	Epoch: [17][535/817]	Loss 0.0095 (0.0575)	
training:	Epoch: [17][536/817]	Loss 0.0423 (0.0575)	
training:	Epoch: [17][537/817]	Loss 0.0085 (0.0574)	
training:	Epoch: [17][538/817]	Loss 0.0104 (0.0573)	
training:	Epoch: [17][539/817]	Loss 0.5675 (0.0582)	
training:	Epoch: [17][540/817]	Loss 0.0106 (0.0582)	
training:	Epoch: [17][541/817]	Loss 0.0106 (0.0581)	
training:	Epoch: [17][542/817]	Loss 0.0142 (0.0580)	
training:	Epoch: [17][543/817]	Loss 0.5688 (0.0589)	
training:	Epoch: [17][544/817]	Loss 0.0086 (0.0588)	
training:	Epoch: [17][545/817]	Loss 0.0099 (0.0588)	
training:	Epoch: [17][546/817]	Loss 0.0088 (0.0587)	
training:	Epoch: [17][547/817]	Loss 0.0150 (0.0586)	
training:	Epoch: [17][548/817]	Loss 0.0508 (0.0586)	
training:	Epoch: [17][549/817]	Loss 0.0168 (0.0585)	
training:	Epoch: [17][550/817]	Loss 0.0094 (0.0584)	
training:	Epoch: [17][551/817]	Loss 0.0113 (0.0583)	
training:	Epoch: [17][552/817]	Loss 0.0091 (0.0582)	
training:	Epoch: [17][553/817]	Loss 0.0091 (0.0581)	
training:	Epoch: [17][554/817]	Loss 0.0134 (0.0581)	
training:	Epoch: [17][555/817]	Loss 0.0129 (0.0580)	
training:	Epoch: [17][556/817]	Loss 0.0091 (0.0579)	
training:	Epoch: [17][557/817]	Loss 0.0100 (0.0578)	
training:	Epoch: [17][558/817]	Loss 0.0095 (0.0577)	
training:	Epoch: [17][559/817]	Loss 0.0109 (0.0576)	
training:	Epoch: [17][560/817]	Loss 0.0098 (0.0575)	
training:	Epoch: [17][561/817]	Loss 0.0101 (0.0575)	
training:	Epoch: [17][562/817]	Loss 0.6144 (0.0585)	
training:	Epoch: [17][563/817]	Loss 0.0107 (0.0584)	
training:	Epoch: [17][564/817]	Loss 0.0098 (0.0583)	
training:	Epoch: [17][565/817]	Loss 0.0144 (0.0582)	
training:	Epoch: [17][566/817]	Loss 0.0109 (0.0581)	
training:	Epoch: [17][567/817]	Loss 0.0103 (0.0580)	
training:	Epoch: [17][568/817]	Loss 0.0277 (0.0580)	
training:	Epoch: [17][569/817]	Loss 0.0095 (0.0579)	
training:	Epoch: [17][570/817]	Loss 0.0105 (0.0578)	
training:	Epoch: [17][571/817]	Loss 0.0112 (0.0577)	
training:	Epoch: [17][572/817]	Loss 0.0109 (0.0576)	
training:	Epoch: [17][573/817]	Loss 0.0099 (0.0576)	
training:	Epoch: [17][574/817]	Loss 0.0093 (0.0575)	
training:	Epoch: [17][575/817]	Loss 0.0104 (0.0574)	
training:	Epoch: [17][576/817]	Loss 0.0107 (0.0573)	
training:	Epoch: [17][577/817]	Loss 0.0115 (0.0572)	
training:	Epoch: [17][578/817]	Loss 0.0091 (0.0572)	
training:	Epoch: [17][579/817]	Loss 0.0106 (0.0571)	
training:	Epoch: [17][580/817]	Loss 0.0135 (0.0570)	
training:	Epoch: [17][581/817]	Loss 0.0102 (0.0569)	
training:	Epoch: [17][582/817]	Loss 0.6141 (0.0579)	
training:	Epoch: [17][583/817]	Loss 0.0127 (0.0578)	
training:	Epoch: [17][584/817]	Loss 0.0133 (0.0577)	
training:	Epoch: [17][585/817]	Loss 0.0121 (0.0576)	
training:	Epoch: [17][586/817]	Loss 0.0098 (0.0576)	
training:	Epoch: [17][587/817]	Loss 0.0101 (0.0575)	
training:	Epoch: [17][588/817]	Loss 0.0475 (0.0575)	
training:	Epoch: [17][589/817]	Loss 0.0100 (0.0574)	
training:	Epoch: [17][590/817]	Loss 0.0110 (0.0573)	
training:	Epoch: [17][591/817]	Loss 0.0090 (0.0572)	
training:	Epoch: [17][592/817]	Loss 0.0123 (0.0571)	
training:	Epoch: [17][593/817]	Loss 0.0106 (0.0571)	
training:	Epoch: [17][594/817]	Loss 0.3754 (0.0576)	
training:	Epoch: [17][595/817]	Loss 0.0116 (0.0575)	
training:	Epoch: [17][596/817]	Loss 0.0096 (0.0574)	
training:	Epoch: [17][597/817]	Loss 0.0159 (0.0574)	
training:	Epoch: [17][598/817]	Loss 0.0108 (0.0573)	
training:	Epoch: [17][599/817]	Loss 0.0089 (0.0572)	
training:	Epoch: [17][600/817]	Loss 0.0118 (0.0571)	
training:	Epoch: [17][601/817]	Loss 0.0122 (0.0571)	
training:	Epoch: [17][602/817]	Loss 0.0105 (0.0570)	
training:	Epoch: [17][603/817]	Loss 0.0098 (0.0569)	
training:	Epoch: [17][604/817]	Loss 0.0093 (0.0568)	
training:	Epoch: [17][605/817]	Loss 0.0110 (0.0568)	
training:	Epoch: [17][606/817]	Loss 0.0105 (0.0567)	
training:	Epoch: [17][607/817]	Loss 0.0093 (0.0566)	
training:	Epoch: [17][608/817]	Loss 0.0092 (0.0565)	
training:	Epoch: [17][609/817]	Loss 0.0114 (0.0565)	
training:	Epoch: [17][610/817]	Loss 0.0095 (0.0564)	
training:	Epoch: [17][611/817]	Loss 0.0087 (0.0563)	
training:	Epoch: [17][612/817]	Loss 0.0089 (0.0562)	
training:	Epoch: [17][613/817]	Loss 0.0101 (0.0561)	
training:	Epoch: [17][614/817]	Loss 0.0115 (0.0561)	
training:	Epoch: [17][615/817]	Loss 0.0094 (0.0560)	
training:	Epoch: [17][616/817]	Loss 0.0086 (0.0559)	
training:	Epoch: [17][617/817]	Loss 0.0093 (0.0558)	
training:	Epoch: [17][618/817]	Loss 0.0102 (0.0558)	
training:	Epoch: [17][619/817]	Loss 0.0099 (0.0557)	
training:	Epoch: [17][620/817]	Loss 0.0118 (0.0556)	
training:	Epoch: [17][621/817]	Loss 0.0111 (0.0556)	
training:	Epoch: [17][622/817]	Loss 0.0111 (0.0555)	
training:	Epoch: [17][623/817]	Loss 0.0160 (0.0554)	
training:	Epoch: [17][624/817]	Loss 0.0084 (0.0553)	
training:	Epoch: [17][625/817]	Loss 0.0117 (0.0553)	
training:	Epoch: [17][626/817]	Loss 0.0099 (0.0552)	
training:	Epoch: [17][627/817]	Loss 0.0091 (0.0551)	
training:	Epoch: [17][628/817]	Loss 0.0332 (0.0551)	
training:	Epoch: [17][629/817]	Loss 0.0109 (0.0550)	
training:	Epoch: [17][630/817]	Loss 0.0113 (0.0550)	
training:	Epoch: [17][631/817]	Loss 0.0115 (0.0549)	
training:	Epoch: [17][632/817]	Loss 0.0089 (0.0548)	
training:	Epoch: [17][633/817]	Loss 0.0093 (0.0547)	
training:	Epoch: [17][634/817]	Loss 0.0090 (0.0547)	
training:	Epoch: [17][635/817]	Loss 0.0090 (0.0546)	
training:	Epoch: [17][636/817]	Loss 0.0090 (0.0545)	
training:	Epoch: [17][637/817]	Loss 0.0099 (0.0545)	
training:	Epoch: [17][638/817]	Loss 0.0092 (0.0544)	
training:	Epoch: [17][639/817]	Loss 0.0103 (0.0543)	
training:	Epoch: [17][640/817]	Loss 0.0102 (0.0542)	
training:	Epoch: [17][641/817]	Loss 0.0098 (0.0542)	
training:	Epoch: [17][642/817]	Loss 0.0123 (0.0541)	
training:	Epoch: [17][643/817]	Loss 0.0093 (0.0540)	
training:	Epoch: [17][644/817]	Loss 0.0109 (0.0540)	
training:	Epoch: [17][645/817]	Loss 0.6043 (0.0548)	
training:	Epoch: [17][646/817]	Loss 0.0100 (0.0548)	
training:	Epoch: [17][647/817]	Loss 0.0086 (0.0547)	
training:	Epoch: [17][648/817]	Loss 0.0099 (0.0546)	
training:	Epoch: [17][649/817]	Loss 0.0087 (0.0545)	
training:	Epoch: [17][650/817]	Loss 0.0111 (0.0545)	
training:	Epoch: [17][651/817]	Loss 0.0126 (0.0544)	
training:	Epoch: [17][652/817]	Loss 0.0212 (0.0544)	
training:	Epoch: [17][653/817]	Loss 0.0112 (0.0543)	
training:	Epoch: [17][654/817]	Loss 0.5521 (0.0551)	
training:	Epoch: [17][655/817]	Loss 0.0102 (0.0550)	
training:	Epoch: [17][656/817]	Loss 0.0100 (0.0549)	
training:	Epoch: [17][657/817]	Loss 0.0091 (0.0549)	
training:	Epoch: [17][658/817]	Loss 0.0095 (0.0548)	
training:	Epoch: [17][659/817]	Loss 0.0095 (0.0547)	
training:	Epoch: [17][660/817]	Loss 0.0114 (0.0547)	
training:	Epoch: [17][661/817]	Loss 0.0104 (0.0546)	
training:	Epoch: [17][662/817]	Loss 0.0102 (0.0545)	
training:	Epoch: [17][663/817]	Loss 0.0112 (0.0545)	
training:	Epoch: [17][664/817]	Loss 0.0113 (0.0544)	
training:	Epoch: [17][665/817]	Loss 0.0143 (0.0543)	
training:	Epoch: [17][666/817]	Loss 0.0102 (0.0543)	
training:	Epoch: [17][667/817]	Loss 0.0103 (0.0542)	
training:	Epoch: [17][668/817]	Loss 0.0139 (0.0541)	
training:	Epoch: [17][669/817]	Loss 0.0104 (0.0541)	
training:	Epoch: [17][670/817]	Loss 0.0102 (0.0540)	
training:	Epoch: [17][671/817]	Loss 0.0101 (0.0539)	
training:	Epoch: [17][672/817]	Loss 0.2518 (0.0542)	
training:	Epoch: [17][673/817]	Loss 0.0096 (0.0542)	
training:	Epoch: [17][674/817]	Loss 0.0100 (0.0541)	
training:	Epoch: [17][675/817]	Loss 0.0101 (0.0540)	
training:	Epoch: [17][676/817]	Loss 0.0085 (0.0540)	
training:	Epoch: [17][677/817]	Loss 0.0094 (0.0539)	
training:	Epoch: [17][678/817]	Loss 0.0096 (0.0538)	
training:	Epoch: [17][679/817]	Loss 0.0105 (0.0538)	
training:	Epoch: [17][680/817]	Loss 0.0123 (0.0537)	
training:	Epoch: [17][681/817]	Loss 0.0113 (0.0536)	
training:	Epoch: [17][682/817]	Loss 0.0114 (0.0536)	
training:	Epoch: [17][683/817]	Loss 0.0079 (0.0535)	
training:	Epoch: [17][684/817]	Loss 0.0084 (0.0535)	
training:	Epoch: [17][685/817]	Loss 0.0132 (0.0534)	
training:	Epoch: [17][686/817]	Loss 0.0273 (0.0534)	
training:	Epoch: [17][687/817]	Loss 0.0085 (0.0533)	
training:	Epoch: [17][688/817]	Loss 0.0094 (0.0532)	
training:	Epoch: [17][689/817]	Loss 0.0112 (0.0532)	
training:	Epoch: [17][690/817]	Loss 0.0088 (0.0531)	
training:	Epoch: [17][691/817]	Loss 0.0093 (0.0530)	
training:	Epoch: [17][692/817]	Loss 0.1947 (0.0532)	
training:	Epoch: [17][693/817]	Loss 0.7052 (0.0542)	
training:	Epoch: [17][694/817]	Loss 0.0096 (0.0541)	
training:	Epoch: [17][695/817]	Loss 0.0442 (0.0541)	
training:	Epoch: [17][696/817]	Loss 0.0109 (0.0540)	
training:	Epoch: [17][697/817]	Loss 0.0094 (0.0540)	
training:	Epoch: [17][698/817]	Loss 0.0114 (0.0539)	
training:	Epoch: [17][699/817]	Loss 0.0103 (0.0539)	
training:	Epoch: [17][700/817]	Loss 0.0133 (0.0538)	
training:	Epoch: [17][701/817]	Loss 0.0104 (0.0537)	
training:	Epoch: [17][702/817]	Loss 0.0118 (0.0537)	
training:	Epoch: [17][703/817]	Loss 0.6133 (0.0545)	
training:	Epoch: [17][704/817]	Loss 0.0256 (0.0544)	
training:	Epoch: [17][705/817]	Loss 0.0092 (0.0544)	
training:	Epoch: [17][706/817]	Loss 0.0373 (0.0543)	
training:	Epoch: [17][707/817]	Loss 0.0099 (0.0543)	
training:	Epoch: [17][708/817]	Loss 0.0107 (0.0542)	
training:	Epoch: [17][709/817]	Loss 0.0089 (0.0542)	
training:	Epoch: [17][710/817]	Loss 0.0089 (0.0541)	
training:	Epoch: [17][711/817]	Loss 0.0333 (0.0541)	
training:	Epoch: [17][712/817]	Loss 0.0093 (0.0540)	
training:	Epoch: [17][713/817]	Loss 0.0092 (0.0539)	
training:	Epoch: [17][714/817]	Loss 0.0095 (0.0539)	
training:	Epoch: [17][715/817]	Loss 0.0293 (0.0538)	
training:	Epoch: [17][716/817]	Loss 0.0094 (0.0538)	
training:	Epoch: [17][717/817]	Loss 0.0151 (0.0537)	
training:	Epoch: [17][718/817]	Loss 0.0136 (0.0537)	
training:	Epoch: [17][719/817]	Loss 0.0099 (0.0536)	
training:	Epoch: [17][720/817]	Loss 0.0097 (0.0535)	
training:	Epoch: [17][721/817]	Loss 0.0098 (0.0535)	
training:	Epoch: [17][722/817]	Loss 0.0102 (0.0534)	
training:	Epoch: [17][723/817]	Loss 0.0083 (0.0534)	
training:	Epoch: [17][724/817]	Loss 0.0271 (0.0533)	
training:	Epoch: [17][725/817]	Loss 0.2650 (0.0536)	
training:	Epoch: [17][726/817]	Loss 0.8702 (0.0547)	
training:	Epoch: [17][727/817]	Loss 0.1929 (0.0549)	
training:	Epoch: [17][728/817]	Loss 0.0105 (0.0549)	
training:	Epoch: [17][729/817]	Loss 0.1910 (0.0551)	
training:	Epoch: [17][730/817]	Loss 0.0101 (0.0550)	
training:	Epoch: [17][731/817]	Loss 0.0126 (0.0549)	
training:	Epoch: [17][732/817]	Loss 0.0110 (0.0549)	
training:	Epoch: [17][733/817]	Loss 0.4931 (0.0555)	
training:	Epoch: [17][734/817]	Loss 0.0094 (0.0554)	
training:	Epoch: [17][735/817]	Loss 0.1462 (0.0555)	
training:	Epoch: [17][736/817]	Loss 0.0091 (0.0555)	
training:	Epoch: [17][737/817]	Loss 0.1101 (0.0556)	
training:	Epoch: [17][738/817]	Loss 0.4710 (0.0561)	
training:	Epoch: [17][739/817]	Loss 0.1783 (0.0563)	
training:	Epoch: [17][740/817]	Loss 0.0090 (0.0562)	
training:	Epoch: [17][741/817]	Loss 0.0132 (0.0562)	
training:	Epoch: [17][742/817]	Loss 0.0100 (0.0561)	
training:	Epoch: [17][743/817]	Loss 0.0089 (0.0560)	
training:	Epoch: [17][744/817]	Loss 0.0097 (0.0560)	
training:	Epoch: [17][745/817]	Loss 0.0089 (0.0559)	
training:	Epoch: [17][746/817]	Loss 0.0349 (0.0559)	
training:	Epoch: [17][747/817]	Loss 0.0091 (0.0558)	
training:	Epoch: [17][748/817]	Loss 0.6136 (0.0566)	
training:	Epoch: [17][749/817]	Loss 0.0090 (0.0565)	
training:	Epoch: [17][750/817]	Loss 0.0124 (0.0564)	
training:	Epoch: [17][751/817]	Loss 0.0211 (0.0564)	
training:	Epoch: [17][752/817]	Loss 0.0091 (0.0563)	
training:	Epoch: [17][753/817]	Loss 0.0092 (0.0563)	
training:	Epoch: [17][754/817]	Loss 0.5737 (0.0570)	
training:	Epoch: [17][755/817]	Loss 0.0097 (0.0569)	
training:	Epoch: [17][756/817]	Loss 0.1446 (0.0570)	
training:	Epoch: [17][757/817]	Loss 0.0089 (0.0569)	
training:	Epoch: [17][758/817]	Loss 0.0097 (0.0569)	
training:	Epoch: [17][759/817]	Loss 0.0082 (0.0568)	
training:	Epoch: [17][760/817]	Loss 0.0109 (0.0568)	
training:	Epoch: [17][761/817]	Loss 0.0093 (0.0567)	
training:	Epoch: [17][762/817]	Loss 0.0298 (0.0567)	
training:	Epoch: [17][763/817]	Loss 0.0087 (0.0566)	
training:	Epoch: [17][764/817]	Loss 0.0097 (0.0565)	
training:	Epoch: [17][765/817]	Loss 0.0148 (0.0565)	
training:	Epoch: [17][766/817]	Loss 0.0108 (0.0564)	
training:	Epoch: [17][767/817]	Loss 0.0120 (0.0564)	
training:	Epoch: [17][768/817]	Loss 0.0102 (0.0563)	
training:	Epoch: [17][769/817]	Loss 0.0094 (0.0562)	
training:	Epoch: [17][770/817]	Loss 0.0099 (0.0562)	
training:	Epoch: [17][771/817]	Loss 0.0103 (0.0561)	
training:	Epoch: [17][772/817]	Loss 0.0116 (0.0561)	
training:	Epoch: [17][773/817]	Loss 0.0092 (0.0560)	
training:	Epoch: [17][774/817]	Loss 0.0111 (0.0559)	
training:	Epoch: [17][775/817]	Loss 0.0225 (0.0559)	
training:	Epoch: [17][776/817]	Loss 0.0099 (0.0558)	
training:	Epoch: [17][777/817]	Loss 0.0157 (0.0558)	
training:	Epoch: [17][778/817]	Loss 0.0106 (0.0557)	
training:	Epoch: [17][779/817]	Loss 0.5366 (0.0563)	
training:	Epoch: [17][780/817]	Loss 0.0105 (0.0563)	
training:	Epoch: [17][781/817]	Loss 0.0106 (0.0562)	
training:	Epoch: [17][782/817]	Loss 0.0105 (0.0562)	
training:	Epoch: [17][783/817]	Loss 0.0103 (0.0561)	
training:	Epoch: [17][784/817]	Loss 0.0095 (0.0561)	
training:	Epoch: [17][785/817]	Loss 0.6140 (0.0568)	
training:	Epoch: [17][786/817]	Loss 0.3207 (0.0571)	
training:	Epoch: [17][787/817]	Loss 0.0100 (0.0570)	
training:	Epoch: [17][788/817]	Loss 0.0088 (0.0570)	
training:	Epoch: [17][789/817]	Loss 0.0932 (0.0570)	
training:	Epoch: [17][790/817]	Loss 0.0103 (0.0570)	
training:	Epoch: [17][791/817]	Loss 0.5563 (0.0576)	
training:	Epoch: [17][792/817]	Loss 0.0100 (0.0575)	
training:	Epoch: [17][793/817]	Loss 0.0090 (0.0575)	
training:	Epoch: [17][794/817]	Loss 0.0106 (0.0574)	
training:	Epoch: [17][795/817]	Loss 0.0444 (0.0574)	
training:	Epoch: [17][796/817]	Loss 0.0098 (0.0573)	
training:	Epoch: [17][797/817]	Loss 0.6403 (0.0581)	
training:	Epoch: [17][798/817]	Loss 0.0130 (0.0580)	
training:	Epoch: [17][799/817]	Loss 0.0164 (0.0580)	
training:	Epoch: [17][800/817]	Loss 0.0095 (0.0579)	
training:	Epoch: [17][801/817]	Loss 0.0173 (0.0579)	
training:	Epoch: [17][802/817]	Loss 0.5651 (0.0585)	
training:	Epoch: [17][803/817]	Loss 0.0101 (0.0584)	
training:	Epoch: [17][804/817]	Loss 0.0484 (0.0584)	
training:	Epoch: [17][805/817]	Loss 0.6174 (0.0591)	
training:	Epoch: [17][806/817]	Loss 0.0107 (0.0590)	
training:	Epoch: [17][807/817]	Loss 0.0120 (0.0590)	
training:	Epoch: [17][808/817]	Loss 0.0097 (0.0589)	
training:	Epoch: [17][809/817]	Loss 0.0113 (0.0589)	
training:	Epoch: [17][810/817]	Loss 0.0165 (0.0588)	
training:	Epoch: [17][811/817]	Loss 0.0181 (0.0588)	
training:	Epoch: [17][812/817]	Loss 0.0107 (0.0587)	
training:	Epoch: [17][813/817]	Loss 0.0360 (0.0587)	
training:	Epoch: [17][814/817]	Loss 0.0105 (0.0586)	
training:	Epoch: [17][815/817]	Loss 0.2056 (0.0588)	
training:	Epoch: [17][816/817]	Loss 0.1427 (0.0589)	
training:	Epoch: [17][817/817]	Loss 0.0107 (0.0588)	
Training:	 Loss: 0.0588

Training:	 ACC: 0.9891 0.9890 0.9856 0.9927
Validation:	 ACC: 0.7892 0.7881 0.7646 0.8139
Validation:	 Best_BACC: 0.8223 0.8218 0.8127 0.8318
Validation:	 Loss: 0.8459
Pretraining:	Epoch 18/200
----------
training:	Epoch: [18][1/817]	Loss 0.0162 (0.0162)	
training:	Epoch: [18][2/817]	Loss 0.0095 (0.0128)	
training:	Epoch: [18][3/817]	Loss 0.0108 (0.0122)	
training:	Epoch: [18][4/817]	Loss 0.0103 (0.0117)	
training:	Epoch: [18][5/817]	Loss 0.0097 (0.0113)	
training:	Epoch: [18][6/817]	Loss 0.0106 (0.0112)	
training:	Epoch: [18][7/817]	Loss 0.6147 (0.0974)	
training:	Epoch: [18][8/817]	Loss 0.0115 (0.0866)	
training:	Epoch: [18][9/817]	Loss 0.0144 (0.0786)	
training:	Epoch: [18][10/817]	Loss 0.0092 (0.0717)	
training:	Epoch: [18][11/817]	Loss 0.0096 (0.0660)	
training:	Epoch: [18][12/817]	Loss 0.0097 (0.0613)	
training:	Epoch: [18][13/817]	Loss 0.0101 (0.0574)	
training:	Epoch: [18][14/817]	Loss 0.0231 (0.0550)	
training:	Epoch: [18][15/817]	Loss 0.0435 (0.0542)	
training:	Epoch: [18][16/817]	Loss 0.0327 (0.0528)	
training:	Epoch: [18][17/817]	Loss 0.0100 (0.0503)	
training:	Epoch: [18][18/817]	Loss 0.0109 (0.0481)	
training:	Epoch: [18][19/817]	Loss 0.0108 (0.0462)	
training:	Epoch: [18][20/817]	Loss 0.0098 (0.0444)	
training:	Epoch: [18][21/817]	Loss 0.0143 (0.0429)	
training:	Epoch: [18][22/817]	Loss 0.0097 (0.0414)	
training:	Epoch: [18][23/817]	Loss 0.0107 (0.0401)	
training:	Epoch: [18][24/817]	Loss 0.0100 (0.0388)	
training:	Epoch: [18][25/817]	Loss 0.0091 (0.0376)	
training:	Epoch: [18][26/817]	Loss 0.2225 (0.0447)	
training:	Epoch: [18][27/817]	Loss 0.0102 (0.0435)	
training:	Epoch: [18][28/817]	Loss 0.0434 (0.0435)	
training:	Epoch: [18][29/817]	Loss 0.0195 (0.0426)	
training:	Epoch: [18][30/817]	Loss 0.0123 (0.0416)	
training:	Epoch: [18][31/817]	Loss 0.0104 (0.0406)	
training:	Epoch: [18][32/817]	Loss 0.0096 (0.0396)	
training:	Epoch: [18][33/817]	Loss 0.0103 (0.0388)	
training:	Epoch: [18][34/817]	Loss 0.0111 (0.0379)	
training:	Epoch: [18][35/817]	Loss 0.0102 (0.0371)	
training:	Epoch: [18][36/817]	Loss 0.0091 (0.0364)	
training:	Epoch: [18][37/817]	Loss 0.0091 (0.0356)	
training:	Epoch: [18][38/817]	Loss 0.0091 (0.0349)	
training:	Epoch: [18][39/817]	Loss 0.1022 (0.0367)	
training:	Epoch: [18][40/817]	Loss 0.6068 (0.0509)	
training:	Epoch: [18][41/817]	Loss 0.1483 (0.0533)	
training:	Epoch: [18][42/817]	Loss 0.0098 (0.0523)	
training:	Epoch: [18][43/817]	Loss 0.5318 (0.0634)	
training:	Epoch: [18][44/817]	Loss 0.0123 (0.0622)	
training:	Epoch: [18][45/817]	Loss 0.0096 (0.0611)	
training:	Epoch: [18][46/817]	Loss 0.0108 (0.0600)	
training:	Epoch: [18][47/817]	Loss 0.0088 (0.0589)	
training:	Epoch: [18][48/817]	Loss 0.0095 (0.0579)	
training:	Epoch: [18][49/817]	Loss 0.0107 (0.0569)	
training:	Epoch: [18][50/817]	Loss 0.0096 (0.0560)	
training:	Epoch: [18][51/817]	Loss 0.0149 (0.0552)	
training:	Epoch: [18][52/817]	Loss 0.0100 (0.0543)	
training:	Epoch: [18][53/817]	Loss 0.0084 (0.0534)	
training:	Epoch: [18][54/817]	Loss 0.0106 (0.0526)	
training:	Epoch: [18][55/817]	Loss 0.0182 (0.0520)	
training:	Epoch: [18][56/817]	Loss 0.0091 (0.0512)	
training:	Epoch: [18][57/817]	Loss 0.0151 (0.0506)	
training:	Epoch: [18][58/817]	Loss 0.1195 (0.0518)	
training:	Epoch: [18][59/817]	Loss 0.0094 (0.0511)	
training:	Epoch: [18][60/817]	Loss 0.0106 (0.0504)	
training:	Epoch: [18][61/817]	Loss 0.6919 (0.0609)	
training:	Epoch: [18][62/817]	Loss 0.0094 (0.0601)	
training:	Epoch: [18][63/817]	Loss 0.0093 (0.0593)	
training:	Epoch: [18][64/817]	Loss 0.0112 (0.0585)	
training:	Epoch: [18][65/817]	Loss 0.0099 (0.0578)	
training:	Epoch: [18][66/817]	Loss 0.0093 (0.0570)	
training:	Epoch: [18][67/817]	Loss 0.0115 (0.0564)	
training:	Epoch: [18][68/817]	Loss 0.0085 (0.0557)	
training:	Epoch: [18][69/817]	Loss 0.0190 (0.0551)	
training:	Epoch: [18][70/817]	Loss 0.5596 (0.0623)	
training:	Epoch: [18][71/817]	Loss 0.0118 (0.0616)	
training:	Epoch: [18][72/817]	Loss 0.0096 (0.0609)	
training:	Epoch: [18][73/817]	Loss 0.0090 (0.0602)	
training:	Epoch: [18][74/817]	Loss 0.7300 (0.0692)	
training:	Epoch: [18][75/817]	Loss 0.0110 (0.0685)	
training:	Epoch: [18][76/817]	Loss 0.0087 (0.0677)	
training:	Epoch: [18][77/817]	Loss 0.0106 (0.0669)	
training:	Epoch: [18][78/817]	Loss 0.0092 (0.0662)	
training:	Epoch: [18][79/817]	Loss 0.0153 (0.0655)	
training:	Epoch: [18][80/817]	Loss 0.0095 (0.0648)	
training:	Epoch: [18][81/817]	Loss 0.0105 (0.0642)	
training:	Epoch: [18][82/817]	Loss 0.0081 (0.0635)	
training:	Epoch: [18][83/817]	Loss 0.0094 (0.0628)	
training:	Epoch: [18][84/817]	Loss 0.0121 (0.0622)	
training:	Epoch: [18][85/817]	Loss 0.0212 (0.0618)	
training:	Epoch: [18][86/817]	Loss 0.0087 (0.0611)	
training:	Epoch: [18][87/817]	Loss 0.0103 (0.0606)	
training:	Epoch: [18][88/817]	Loss 0.0097 (0.0600)	
training:	Epoch: [18][89/817]	Loss 0.0378 (0.0597)	
training:	Epoch: [18][90/817]	Loss 0.1139 (0.0603)	
training:	Epoch: [18][91/817]	Loss 0.0356 (0.0601)	
training:	Epoch: [18][92/817]	Loss 0.0168 (0.0596)	
training:	Epoch: [18][93/817]	Loss 0.0098 (0.0591)	
training:	Epoch: [18][94/817]	Loss 0.0088 (0.0585)	
training:	Epoch: [18][95/817]	Loss 0.0090 (0.0580)	
training:	Epoch: [18][96/817]	Loss 0.0102 (0.0575)	
training:	Epoch: [18][97/817]	Loss 0.0201 (0.0571)	
training:	Epoch: [18][98/817]	Loss 0.0195 (0.0567)	
training:	Epoch: [18][99/817]	Loss 0.0400 (0.0566)	
training:	Epoch: [18][100/817]	Loss 0.0101 (0.0561)	
training:	Epoch: [18][101/817]	Loss 0.0113 (0.0557)	
training:	Epoch: [18][102/817]	Loss 0.0092 (0.0552)	
training:	Epoch: [18][103/817]	Loss 0.0155 (0.0548)	
training:	Epoch: [18][104/817]	Loss 0.0120 (0.0544)	
training:	Epoch: [18][105/817]	Loss 0.0126 (0.0540)	
training:	Epoch: [18][106/817]	Loss 0.0093 (0.0536)	
training:	Epoch: [18][107/817]	Loss 0.0105 (0.0532)	
training:	Epoch: [18][108/817]	Loss 0.0110 (0.0528)	
training:	Epoch: [18][109/817]	Loss 0.0130 (0.0524)	
training:	Epoch: [18][110/817]	Loss 0.0105 (0.0520)	
training:	Epoch: [18][111/817]	Loss 0.6030 (0.0570)	
training:	Epoch: [18][112/817]	Loss 0.0091 (0.0566)	
training:	Epoch: [18][113/817]	Loss 0.0122 (0.0562)	
training:	Epoch: [18][114/817]	Loss 0.0103 (0.0558)	
training:	Epoch: [18][115/817]	Loss 0.0098 (0.0554)	
training:	Epoch: [18][116/817]	Loss 0.0084 (0.0550)	
training:	Epoch: [18][117/817]	Loss 0.0113 (0.0546)	
training:	Epoch: [18][118/817]	Loss 0.0097 (0.0542)	
training:	Epoch: [18][119/817]	Loss 0.0115 (0.0539)	
training:	Epoch: [18][120/817]	Loss 0.0106 (0.0535)	
training:	Epoch: [18][121/817]	Loss 0.0102 (0.0531)	
training:	Epoch: [18][122/817]	Loss 0.0093 (0.0528)	
training:	Epoch: [18][123/817]	Loss 0.0093 (0.0524)	
training:	Epoch: [18][124/817]	Loss 0.0101 (0.0521)	
training:	Epoch: [18][125/817]	Loss 0.0087 (0.0517)	
training:	Epoch: [18][126/817]	Loss 0.0090 (0.0514)	
training:	Epoch: [18][127/817]	Loss 0.0112 (0.0511)	
training:	Epoch: [18][128/817]	Loss 0.0148 (0.0508)	
training:	Epoch: [18][129/817]	Loss 0.0131 (0.0505)	
training:	Epoch: [18][130/817]	Loss 0.0113 (0.0502)	
training:	Epoch: [18][131/817]	Loss 0.0091 (0.0499)	
training:	Epoch: [18][132/817]	Loss 0.0102 (0.0496)	
training:	Epoch: [18][133/817]	Loss 0.0094 (0.0493)	
training:	Epoch: [18][134/817]	Loss 0.0125 (0.0490)	
training:	Epoch: [18][135/817]	Loss 0.0088 (0.0487)	
training:	Epoch: [18][136/817]	Loss 0.0107 (0.0484)	
training:	Epoch: [18][137/817]	Loss 0.0137 (0.0482)	
training:	Epoch: [18][138/817]	Loss 0.5893 (0.0521)	
training:	Epoch: [18][139/817]	Loss 0.0106 (0.0518)	
training:	Epoch: [18][140/817]	Loss 0.0098 (0.0515)	
training:	Epoch: [18][141/817]	Loss 0.0101 (0.0512)	
training:	Epoch: [18][142/817]	Loss 0.0102 (0.0509)	
training:	Epoch: [18][143/817]	Loss 0.0104 (0.0506)	
training:	Epoch: [18][144/817]	Loss 0.6074 (0.0545)	
training:	Epoch: [18][145/817]	Loss 0.1077 (0.0549)	
training:	Epoch: [18][146/817]	Loss 0.0085 (0.0546)	
training:	Epoch: [18][147/817]	Loss 0.0086 (0.0542)	
training:	Epoch: [18][148/817]	Loss 0.0769 (0.0544)	
training:	Epoch: [18][149/817]	Loss 0.0099 (0.0541)	
training:	Epoch: [18][150/817]	Loss 0.0143 (0.0538)	
training:	Epoch: [18][151/817]	Loss 0.0084 (0.0535)	
training:	Epoch: [18][152/817]	Loss 0.0094 (0.0532)	
training:	Epoch: [18][153/817]	Loss 0.0124 (0.0530)	
training:	Epoch: [18][154/817]	Loss 0.0108 (0.0527)	
training:	Epoch: [18][155/817]	Loss 0.0088 (0.0524)	
training:	Epoch: [18][156/817]	Loss 0.0080 (0.0521)	
training:	Epoch: [18][157/817]	Loss 0.0109 (0.0519)	
training:	Epoch: [18][158/817]	Loss 0.0795 (0.0520)	
training:	Epoch: [18][159/817]	Loss 0.4521 (0.0546)	
training:	Epoch: [18][160/817]	Loss 0.0100 (0.0543)	
training:	Epoch: [18][161/817]	Loss 0.0112 (0.0540)	
training:	Epoch: [18][162/817]	Loss 0.0088 (0.0537)	
training:	Epoch: [18][163/817]	Loss 0.0192 (0.0535)	
training:	Epoch: [18][164/817]	Loss 0.0092 (0.0533)	
training:	Epoch: [18][165/817]	Loss 0.0092 (0.0530)	
training:	Epoch: [18][166/817]	Loss 0.0100 (0.0527)	
training:	Epoch: [18][167/817]	Loss 0.0411 (0.0527)	
training:	Epoch: [18][168/817]	Loss 0.0156 (0.0524)	
training:	Epoch: [18][169/817]	Loss 0.0102 (0.0522)	
training:	Epoch: [18][170/817]	Loss 0.0128 (0.0520)	
training:	Epoch: [18][171/817]	Loss 0.0125 (0.0517)	
training:	Epoch: [18][172/817]	Loss 0.0086 (0.0515)	
training:	Epoch: [18][173/817]	Loss 0.0096 (0.0512)	
training:	Epoch: [18][174/817]	Loss 0.5350 (0.0540)	
training:	Epoch: [18][175/817]	Loss 0.0085 (0.0538)	
training:	Epoch: [18][176/817]	Loss 0.0100 (0.0535)	
training:	Epoch: [18][177/817]	Loss 0.0101 (0.0533)	
training:	Epoch: [18][178/817]	Loss 0.0089 (0.0530)	
training:	Epoch: [18][179/817]	Loss 0.0096 (0.0528)	
training:	Epoch: [18][180/817]	Loss 0.0088 (0.0525)	
training:	Epoch: [18][181/817]	Loss 0.0562 (0.0525)	
training:	Epoch: [18][182/817]	Loss 0.0092 (0.0523)	
training:	Epoch: [18][183/817]	Loss 0.0098 (0.0521)	
training:	Epoch: [18][184/817]	Loss 0.0095 (0.0518)	
training:	Epoch: [18][185/817]	Loss 0.0088 (0.0516)	
training:	Epoch: [18][186/817]	Loss 0.0101 (0.0514)	
training:	Epoch: [18][187/817]	Loss 0.0101 (0.0512)	
training:	Epoch: [18][188/817]	Loss 0.0082 (0.0509)	
training:	Epoch: [18][189/817]	Loss 0.4662 (0.0531)	
training:	Epoch: [18][190/817]	Loss 0.0258 (0.0530)	
training:	Epoch: [18][191/817]	Loss 0.0126 (0.0528)	
training:	Epoch: [18][192/817]	Loss 0.0082 (0.0525)	
training:	Epoch: [18][193/817]	Loss 0.0112 (0.0523)	
training:	Epoch: [18][194/817]	Loss 0.0096 (0.0521)	
training:	Epoch: [18][195/817]	Loss 0.6214 (0.0550)	
training:	Epoch: [18][196/817]	Loss 0.0094 (0.0548)	
training:	Epoch: [18][197/817]	Loss 0.0206 (0.0546)	
training:	Epoch: [18][198/817]	Loss 0.0083 (0.0544)	
training:	Epoch: [18][199/817]	Loss 0.6139 (0.0572)	
training:	Epoch: [18][200/817]	Loss 0.0292 (0.0571)	
training:	Epoch: [18][201/817]	Loss 0.0120 (0.0568)	
training:	Epoch: [18][202/817]	Loss 0.0082 (0.0566)	
training:	Epoch: [18][203/817]	Loss 0.0085 (0.0564)	
training:	Epoch: [18][204/817]	Loss 0.7010 (0.0595)	
training:	Epoch: [18][205/817]	Loss 0.0099 (0.0593)	
training:	Epoch: [18][206/817]	Loss 0.0113 (0.0590)	
training:	Epoch: [18][207/817]	Loss 0.0106 (0.0588)	
training:	Epoch: [18][208/817]	Loss 0.0124 (0.0586)	
training:	Epoch: [18][209/817]	Loss 0.0091 (0.0584)	
training:	Epoch: [18][210/817]	Loss 0.0108 (0.0581)	
training:	Epoch: [18][211/817]	Loss 0.0098 (0.0579)	
training:	Epoch: [18][212/817]	Loss 0.0093 (0.0577)	
training:	Epoch: [18][213/817]	Loss 0.0088 (0.0574)	
training:	Epoch: [18][214/817]	Loss 0.0098 (0.0572)	
training:	Epoch: [18][215/817]	Loss 0.0103 (0.0570)	
training:	Epoch: [18][216/817]	Loss 0.0100 (0.0568)	
training:	Epoch: [18][217/817]	Loss 0.0084 (0.0566)	
training:	Epoch: [18][218/817]	Loss 0.0102 (0.0563)	
training:	Epoch: [18][219/817]	Loss 0.0097 (0.0561)	
training:	Epoch: [18][220/817]	Loss 0.0116 (0.0559)	
training:	Epoch: [18][221/817]	Loss 0.0117 (0.0557)	
training:	Epoch: [18][222/817]	Loss 0.0095 (0.0555)	
training:	Epoch: [18][223/817]	Loss 0.0141 (0.0553)	
training:	Epoch: [18][224/817]	Loss 0.0179 (0.0552)	
training:	Epoch: [18][225/817]	Loss 0.0116 (0.0550)	
training:	Epoch: [18][226/817]	Loss 0.0125 (0.0548)	
training:	Epoch: [18][227/817]	Loss 0.0147 (0.0546)	
training:	Epoch: [18][228/817]	Loss 0.0098 (0.0544)	
training:	Epoch: [18][229/817]	Loss 0.0125 (0.0542)	
training:	Epoch: [18][230/817]	Loss 0.0091 (0.0540)	
training:	Epoch: [18][231/817]	Loss 0.0092 (0.0538)	
training:	Epoch: [18][232/817]	Loss 0.0098 (0.0537)	
training:	Epoch: [18][233/817]	Loss 0.0093 (0.0535)	
training:	Epoch: [18][234/817]	Loss 0.0101 (0.0533)	
training:	Epoch: [18][235/817]	Loss 0.0089 (0.0531)	
training:	Epoch: [18][236/817]	Loss 0.5852 (0.0553)	
training:	Epoch: [18][237/817]	Loss 0.0096 (0.0551)	
training:	Epoch: [18][238/817]	Loss 0.0089 (0.0550)	
training:	Epoch: [18][239/817]	Loss 0.0183 (0.0548)	
training:	Epoch: [18][240/817]	Loss 0.0123 (0.0546)	
training:	Epoch: [18][241/817]	Loss 0.5763 (0.0568)	
training:	Epoch: [18][242/817]	Loss 0.0104 (0.0566)	
training:	Epoch: [18][243/817]	Loss 0.0089 (0.0564)	
training:	Epoch: [18][244/817]	Loss 0.0088 (0.0562)	
training:	Epoch: [18][245/817]	Loss 0.5736 (0.0583)	
training:	Epoch: [18][246/817]	Loss 0.0097 (0.0581)	
training:	Epoch: [18][247/817]	Loss 0.0094 (0.0579)	
training:	Epoch: [18][248/817]	Loss 0.0098 (0.0577)	
training:	Epoch: [18][249/817]	Loss 0.0092 (0.0575)	
training:	Epoch: [18][250/817]	Loss 0.0103 (0.0573)	
training:	Epoch: [18][251/817]	Loss 0.0093 (0.0572)	
training:	Epoch: [18][252/817]	Loss 0.0104 (0.0570)	
training:	Epoch: [18][253/817]	Loss 0.0095 (0.0568)	
training:	Epoch: [18][254/817]	Loss 0.0115 (0.0566)	
training:	Epoch: [18][255/817]	Loss 0.0095 (0.0564)	
training:	Epoch: [18][256/817]	Loss 0.0187 (0.0563)	
training:	Epoch: [18][257/817]	Loss 1.1501 (0.0605)	
training:	Epoch: [18][258/817]	Loss 0.0085 (0.0603)	
training:	Epoch: [18][259/817]	Loss 0.0115 (0.0601)	
training:	Epoch: [18][260/817]	Loss 0.0102 (0.0599)	
training:	Epoch: [18][261/817]	Loss 0.0193 (0.0598)	
training:	Epoch: [18][262/817]	Loss 0.0099 (0.0596)	
training:	Epoch: [18][263/817]	Loss 0.0118 (0.0594)	
training:	Epoch: [18][264/817]	Loss 0.0101 (0.0592)	
training:	Epoch: [18][265/817]	Loss 0.0095 (0.0590)	
training:	Epoch: [18][266/817]	Loss 0.0094 (0.0589)	
training:	Epoch: [18][267/817]	Loss 0.7195 (0.0613)	
training:	Epoch: [18][268/817]	Loss 0.0088 (0.0611)	
training:	Epoch: [18][269/817]	Loss 0.4560 (0.0626)	
training:	Epoch: [18][270/817]	Loss 0.0119 (0.0624)	
training:	Epoch: [18][271/817]	Loss 0.0124 (0.0622)	
training:	Epoch: [18][272/817]	Loss 0.0092 (0.0620)	
training:	Epoch: [18][273/817]	Loss 0.0099 (0.0618)	
training:	Epoch: [18][274/817]	Loss 0.0095 (0.0617)	
training:	Epoch: [18][275/817]	Loss 0.0088 (0.0615)	
training:	Epoch: [18][276/817]	Loss 0.0102 (0.0613)	
training:	Epoch: [18][277/817]	Loss 0.0159 (0.0611)	
training:	Epoch: [18][278/817]	Loss 0.0105 (0.0609)	
training:	Epoch: [18][279/817]	Loss 0.0132 (0.0608)	
training:	Epoch: [18][280/817]	Loss 0.0124 (0.0606)	
training:	Epoch: [18][281/817]	Loss 0.0091 (0.0604)	
training:	Epoch: [18][282/817]	Loss 0.0098 (0.0602)	
training:	Epoch: [18][283/817]	Loss 0.0111 (0.0600)	
training:	Epoch: [18][284/817]	Loss 0.0133 (0.0599)	
training:	Epoch: [18][285/817]	Loss 0.0083 (0.0597)	
training:	Epoch: [18][286/817]	Loss 0.0097 (0.0595)	
training:	Epoch: [18][287/817]	Loss 0.0095 (0.0594)	
training:	Epoch: [18][288/817]	Loss 0.0091 (0.0592)	
training:	Epoch: [18][289/817]	Loss 0.0776 (0.0592)	
training:	Epoch: [18][290/817]	Loss 0.0076 (0.0591)	
training:	Epoch: [18][291/817]	Loss 0.0121 (0.0589)	
training:	Epoch: [18][292/817]	Loss 0.5086 (0.0604)	
training:	Epoch: [18][293/817]	Loss 0.0099 (0.0603)	
training:	Epoch: [18][294/817]	Loss 0.0102 (0.0601)	
training:	Epoch: [18][295/817]	Loss 0.0091 (0.0599)	
training:	Epoch: [18][296/817]	Loss 0.0093 (0.0598)	
training:	Epoch: [18][297/817]	Loss 0.0107 (0.0596)	
training:	Epoch: [18][298/817]	Loss 0.0098 (0.0594)	
training:	Epoch: [18][299/817]	Loss 0.0097 (0.0593)	
training:	Epoch: [18][300/817]	Loss 0.0465 (0.0592)	
training:	Epoch: [18][301/817]	Loss 0.0085 (0.0590)	
training:	Epoch: [18][302/817]	Loss 0.0096 (0.0589)	
training:	Epoch: [18][303/817]	Loss 0.0096 (0.0587)	
training:	Epoch: [18][304/817]	Loss 0.0100 (0.0586)	
training:	Epoch: [18][305/817]	Loss 0.0098 (0.0584)	
training:	Epoch: [18][306/817]	Loss 0.0131 (0.0583)	
training:	Epoch: [18][307/817]	Loss 0.0101 (0.0581)	
training:	Epoch: [18][308/817]	Loss 0.0137 (0.0579)	
training:	Epoch: [18][309/817]	Loss 0.0093 (0.0578)	
training:	Epoch: [18][310/817]	Loss 0.0090 (0.0576)	
training:	Epoch: [18][311/817]	Loss 0.0096 (0.0575)	
training:	Epoch: [18][312/817]	Loss 0.5765 (0.0591)	
training:	Epoch: [18][313/817]	Loss 0.0115 (0.0590)	
training:	Epoch: [18][314/817]	Loss 0.0108 (0.0588)	
training:	Epoch: [18][315/817]	Loss 0.0119 (0.0587)	
training:	Epoch: [18][316/817]	Loss 0.0095 (0.0585)	
training:	Epoch: [18][317/817]	Loss 0.0112 (0.0584)	
training:	Epoch: [18][318/817]	Loss 0.0100 (0.0582)	
training:	Epoch: [18][319/817]	Loss 0.0091 (0.0581)	
training:	Epoch: [18][320/817]	Loss 0.0089 (0.0579)	
training:	Epoch: [18][321/817]	Loss 0.0089 (0.0578)	
training:	Epoch: [18][322/817]	Loss 0.0119 (0.0576)	
training:	Epoch: [18][323/817]	Loss 0.6220 (0.0594)	
training:	Epoch: [18][324/817]	Loss 0.0171 (0.0592)	
training:	Epoch: [18][325/817]	Loss 0.0112 (0.0591)	
training:	Epoch: [18][326/817]	Loss 0.0131 (0.0590)	
training:	Epoch: [18][327/817]	Loss 0.6173 (0.0607)	
training:	Epoch: [18][328/817]	Loss 0.0113 (0.0605)	
training:	Epoch: [18][329/817]	Loss 0.0125 (0.0604)	
training:	Epoch: [18][330/817]	Loss 0.0105 (0.0602)	
training:	Epoch: [18][331/817]	Loss 0.0121 (0.0601)	
training:	Epoch: [18][332/817]	Loss 0.0799 (0.0601)	
training:	Epoch: [18][333/817]	Loss 0.0104 (0.0600)	
training:	Epoch: [18][334/817]	Loss 0.0116 (0.0598)	
training:	Epoch: [18][335/817]	Loss 0.0155 (0.0597)	
training:	Epoch: [18][336/817]	Loss 0.0111 (0.0596)	
training:	Epoch: [18][337/817]	Loss 0.0092 (0.0594)	
training:	Epoch: [18][338/817]	Loss 0.0252 (0.0593)	
training:	Epoch: [18][339/817]	Loss 0.0098 (0.0592)	
training:	Epoch: [18][340/817]	Loss 0.0087 (0.0590)	
training:	Epoch: [18][341/817]	Loss 0.0089 (0.0589)	
training:	Epoch: [18][342/817]	Loss 0.0083 (0.0587)	
training:	Epoch: [18][343/817]	Loss 0.0341 (0.0586)	
training:	Epoch: [18][344/817]	Loss 0.0116 (0.0585)	
training:	Epoch: [18][345/817]	Loss 0.5635 (0.0600)	
training:	Epoch: [18][346/817]	Loss 0.0107 (0.0598)	
training:	Epoch: [18][347/817]	Loss 0.0130 (0.0597)	
training:	Epoch: [18][348/817]	Loss 0.0109 (0.0596)	
training:	Epoch: [18][349/817]	Loss 0.0350 (0.0595)	
training:	Epoch: [18][350/817]	Loss 0.0097 (0.0593)	
training:	Epoch: [18][351/817]	Loss 0.0101 (0.0592)	
training:	Epoch: [18][352/817]	Loss 0.0102 (0.0591)	
training:	Epoch: [18][353/817]	Loss 0.0112 (0.0589)	
training:	Epoch: [18][354/817]	Loss 0.5228 (0.0602)	
training:	Epoch: [18][355/817]	Loss 0.0089 (0.0601)	
training:	Epoch: [18][356/817]	Loss 0.0223 (0.0600)	
training:	Epoch: [18][357/817]	Loss 0.0092 (0.0598)	
training:	Epoch: [18][358/817]	Loss 0.0095 (0.0597)	
training:	Epoch: [18][359/817]	Loss 0.0264 (0.0596)	
training:	Epoch: [18][360/817]	Loss 0.0093 (0.0595)	
training:	Epoch: [18][361/817]	Loss 0.0096 (0.0593)	
training:	Epoch: [18][362/817]	Loss 0.0111 (0.0592)	
training:	Epoch: [18][363/817]	Loss 0.0164 (0.0591)	
training:	Epoch: [18][364/817]	Loss 0.0875 (0.0592)	
training:	Epoch: [18][365/817]	Loss 0.0156 (0.0590)	
training:	Epoch: [18][366/817]	Loss 0.0132 (0.0589)	
training:	Epoch: [18][367/817]	Loss 0.0099 (0.0588)	
training:	Epoch: [18][368/817]	Loss 0.0107 (0.0587)	
training:	Epoch: [18][369/817]	Loss 0.0116 (0.0585)	
training:	Epoch: [18][370/817]	Loss 0.0088 (0.0584)	
training:	Epoch: [18][371/817]	Loss 0.0094 (0.0583)	
training:	Epoch: [18][372/817]	Loss 0.0091 (0.0581)	
training:	Epoch: [18][373/817]	Loss 0.0113 (0.0580)	
training:	Epoch: [18][374/817]	Loss 0.0133 (0.0579)	
training:	Epoch: [18][375/817]	Loss 0.0092 (0.0578)	
training:	Epoch: [18][376/817]	Loss 0.0092 (0.0576)	
training:	Epoch: [18][377/817]	Loss 0.0095 (0.0575)	
training:	Epoch: [18][378/817]	Loss 0.0095 (0.0574)	
training:	Epoch: [18][379/817]	Loss 0.0102 (0.0572)	
training:	Epoch: [18][380/817]	Loss 0.0115 (0.0571)	
training:	Epoch: [18][381/817]	Loss 0.0094 (0.0570)	
training:	Epoch: [18][382/817]	Loss 0.0158 (0.0569)	
training:	Epoch: [18][383/817]	Loss 0.0098 (0.0568)	
training:	Epoch: [18][384/817]	Loss 0.0096 (0.0566)	
training:	Epoch: [18][385/817]	Loss 0.0102 (0.0565)	
training:	Epoch: [18][386/817]	Loss 0.0103 (0.0564)	
training:	Epoch: [18][387/817]	Loss 0.0088 (0.0563)	
training:	Epoch: [18][388/817]	Loss 0.0109 (0.0562)	
training:	Epoch: [18][389/817]	Loss 0.0101 (0.0560)	
training:	Epoch: [18][390/817]	Loss 0.0091 (0.0559)	
training:	Epoch: [18][391/817]	Loss 0.0112 (0.0558)	
training:	Epoch: [18][392/817]	Loss 0.0113 (0.0557)	
training:	Epoch: [18][393/817]	Loss 0.0103 (0.0556)	
training:	Epoch: [18][394/817]	Loss 0.0187 (0.0555)	
training:	Epoch: [18][395/817]	Loss 0.0082 (0.0554)	
training:	Epoch: [18][396/817]	Loss 0.0160 (0.0553)	
training:	Epoch: [18][397/817]	Loss 0.0090 (0.0552)	
training:	Epoch: [18][398/817]	Loss 0.0112 (0.0550)	
training:	Epoch: [18][399/817]	Loss 0.0094 (0.0549)	
training:	Epoch: [18][400/817]	Loss 0.0090 (0.0548)	
training:	Epoch: [18][401/817]	Loss 0.0188 (0.0547)	
training:	Epoch: [18][402/817]	Loss 0.0088 (0.0546)	
training:	Epoch: [18][403/817]	Loss 0.0126 (0.0545)	
training:	Epoch: [18][404/817]	Loss 0.0107 (0.0544)	
training:	Epoch: [18][405/817]	Loss 0.5654 (0.0557)	
training:	Epoch: [18][406/817]	Loss 0.0883 (0.0557)	
training:	Epoch: [18][407/817]	Loss 0.0091 (0.0556)	
training:	Epoch: [18][408/817]	Loss 0.0091 (0.0555)	
training:	Epoch: [18][409/817]	Loss 0.0089 (0.0554)	
training:	Epoch: [18][410/817]	Loss 0.0103 (0.0553)	
training:	Epoch: [18][411/817]	Loss 0.0113 (0.0552)	
training:	Epoch: [18][412/817]	Loss 0.0097 (0.0551)	
training:	Epoch: [18][413/817]	Loss 0.0099 (0.0550)	
training:	Epoch: [18][414/817]	Loss 0.0095 (0.0549)	
training:	Epoch: [18][415/817]	Loss 0.0081 (0.0547)	
training:	Epoch: [18][416/817]	Loss 0.0090 (0.0546)	
training:	Epoch: [18][417/817]	Loss 0.0096 (0.0545)	
training:	Epoch: [18][418/817]	Loss 0.0087 (0.0544)	
training:	Epoch: [18][419/817]	Loss 0.0118 (0.0543)	
training:	Epoch: [18][420/817]	Loss 0.0550 (0.0543)	
training:	Epoch: [18][421/817]	Loss 0.0090 (0.0542)	
training:	Epoch: [18][422/817]	Loss 0.0092 (0.0541)	
training:	Epoch: [18][423/817]	Loss 0.0089 (0.0540)	
training:	Epoch: [18][424/817]	Loss 0.0095 (0.0539)	
training:	Epoch: [18][425/817]	Loss 0.0083 (0.0538)	
training:	Epoch: [18][426/817]	Loss 0.6322 (0.0551)	
training:	Epoch: [18][427/817]	Loss 0.0360 (0.0551)	
training:	Epoch: [18][428/817]	Loss 0.0086 (0.0550)	
training:	Epoch: [18][429/817]	Loss 0.0093 (0.0549)	
training:	Epoch: [18][430/817]	Loss 0.6084 (0.0562)	
training:	Epoch: [18][431/817]	Loss 0.0097 (0.0561)	
training:	Epoch: [18][432/817]	Loss 0.0093 (0.0559)	
training:	Epoch: [18][433/817]	Loss 0.0650 (0.0560)	
training:	Epoch: [18][434/817]	Loss 0.0138 (0.0559)	
training:	Epoch: [18][435/817]	Loss 0.0105 (0.0558)	
training:	Epoch: [18][436/817]	Loss 0.0099 (0.0557)	
training:	Epoch: [18][437/817]	Loss 0.0112 (0.0556)	
training:	Epoch: [18][438/817]	Loss 0.0101 (0.0555)	
training:	Epoch: [18][439/817]	Loss 0.0112 (0.0554)	
training:	Epoch: [18][440/817]	Loss 0.0092 (0.0552)	
training:	Epoch: [18][441/817]	Loss 0.0090 (0.0551)	
training:	Epoch: [18][442/817]	Loss 0.0094 (0.0550)	
training:	Epoch: [18][443/817]	Loss 0.0095 (0.0549)	
training:	Epoch: [18][444/817]	Loss 0.0098 (0.0548)	
training:	Epoch: [18][445/817]	Loss 0.0085 (0.0547)	
training:	Epoch: [18][446/817]	Loss 0.0100 (0.0546)	
training:	Epoch: [18][447/817]	Loss 0.0106 (0.0545)	
training:	Epoch: [18][448/817]	Loss 0.0085 (0.0544)	
training:	Epoch: [18][449/817]	Loss 0.0104 (0.0543)	
training:	Epoch: [18][450/817]	Loss 0.0502 (0.0543)	
training:	Epoch: [18][451/817]	Loss 0.0084 (0.0542)	
training:	Epoch: [18][452/817]	Loss 0.6004 (0.0554)	
training:	Epoch: [18][453/817]	Loss 0.0109 (0.0553)	
training:	Epoch: [18][454/817]	Loss 0.0239 (0.0553)	
training:	Epoch: [18][455/817]	Loss 0.0097 (0.0552)	
training:	Epoch: [18][456/817]	Loss 0.0089 (0.0551)	
training:	Epoch: [18][457/817]	Loss 0.0102 (0.0550)	
training:	Epoch: [18][458/817]	Loss 0.0087 (0.0549)	
training:	Epoch: [18][459/817]	Loss 0.0096 (0.0548)	
training:	Epoch: [18][460/817]	Loss 0.0093 (0.0547)	
training:	Epoch: [18][461/817]	Loss 0.0100 (0.0546)	
training:	Epoch: [18][462/817]	Loss 0.0108 (0.0545)	
training:	Epoch: [18][463/817]	Loss 0.0104 (0.0544)	
training:	Epoch: [18][464/817]	Loss 0.0091 (0.0543)	
training:	Epoch: [18][465/817]	Loss 0.0089 (0.0542)	
training:	Epoch: [18][466/817]	Loss 0.6135 (0.0554)	
training:	Epoch: [18][467/817]	Loss 0.0090 (0.0553)	
training:	Epoch: [18][468/817]	Loss 0.0096 (0.0552)	
training:	Epoch: [18][469/817]	Loss 0.5290 (0.0562)	
training:	Epoch: [18][470/817]	Loss 0.0098 (0.0561)	
training:	Epoch: [18][471/817]	Loss 0.0087 (0.0560)	
training:	Epoch: [18][472/817]	Loss 0.0090 (0.0559)	
training:	Epoch: [18][473/817]	Loss 0.0388 (0.0559)	
training:	Epoch: [18][474/817]	Loss 0.0091 (0.0558)	
training:	Epoch: [18][475/817]	Loss 0.0081 (0.0557)	
training:	Epoch: [18][476/817]	Loss 0.0095 (0.0556)	
training:	Epoch: [18][477/817]	Loss 0.0126 (0.0555)	
training:	Epoch: [18][478/817]	Loss 0.0105 (0.0554)	
training:	Epoch: [18][479/817]	Loss 0.0094 (0.0553)	
training:	Epoch: [18][480/817]	Loss 0.0104 (0.0552)	
training:	Epoch: [18][481/817]	Loss 0.6254 (0.0564)	
training:	Epoch: [18][482/817]	Loss 0.0099 (0.0563)	
training:	Epoch: [18][483/817]	Loss 0.0091 (0.0562)	
training:	Epoch: [18][484/817]	Loss 0.0130 (0.0561)	
training:	Epoch: [18][485/817]	Loss 0.6076 (0.0572)	
training:	Epoch: [18][486/817]	Loss 0.0104 (0.0571)	
training:	Epoch: [18][487/817]	Loss 0.0092 (0.0570)	
training:	Epoch: [18][488/817]	Loss 0.0099 (0.0569)	
training:	Epoch: [18][489/817]	Loss 0.0180 (0.0569)	
training:	Epoch: [18][490/817]	Loss 0.0091 (0.0568)	
training:	Epoch: [18][491/817]	Loss 0.0091 (0.0567)	
training:	Epoch: [18][492/817]	Loss 0.0094 (0.0566)	
training:	Epoch: [18][493/817]	Loss 0.0091 (0.0565)	
training:	Epoch: [18][494/817]	Loss 0.0202 (0.0564)	
training:	Epoch: [18][495/817]	Loss 0.0098 (0.0563)	
training:	Epoch: [18][496/817]	Loss 0.0095 (0.0562)	
training:	Epoch: [18][497/817]	Loss 0.0092 (0.0561)	
training:	Epoch: [18][498/817]	Loss 0.0094 (0.0560)	
training:	Epoch: [18][499/817]	Loss 0.0093 (0.0559)	
training:	Epoch: [18][500/817]	Loss 0.0095 (0.0558)	
training:	Epoch: [18][501/817]	Loss 0.0093 (0.0557)	
training:	Epoch: [18][502/817]	Loss 0.0098 (0.0557)	
training:	Epoch: [18][503/817]	Loss 0.0164 (0.0556)	
training:	Epoch: [18][504/817]	Loss 0.0111 (0.0555)	
training:	Epoch: [18][505/817]	Loss 0.0098 (0.0554)	
training:	Epoch: [18][506/817]	Loss 0.0100 (0.0553)	
training:	Epoch: [18][507/817]	Loss 0.0086 (0.0552)	
training:	Epoch: [18][508/817]	Loss 0.0141 (0.0551)	
training:	Epoch: [18][509/817]	Loss 0.0142 (0.0551)	
training:	Epoch: [18][510/817]	Loss 0.0097 (0.0550)	
training:	Epoch: [18][511/817]	Loss 0.0080 (0.0549)	
training:	Epoch: [18][512/817]	Loss 0.0096 (0.0548)	
training:	Epoch: [18][513/817]	Loss 0.0111 (0.0547)	
training:	Epoch: [18][514/817]	Loss 0.0089 (0.0546)	
training:	Epoch: [18][515/817]	Loss 0.0089 (0.0545)	
training:	Epoch: [18][516/817]	Loss 0.0095 (0.0544)	
training:	Epoch: [18][517/817]	Loss 0.5439 (0.0554)	
training:	Epoch: [18][518/817]	Loss 0.0104 (0.0553)	
training:	Epoch: [18][519/817]	Loss 0.0168 (0.0552)	
training:	Epoch: [18][520/817]	Loss 0.0096 (0.0551)	
training:	Epoch: [18][521/817]	Loss 0.5092 (0.0560)	
training:	Epoch: [18][522/817]	Loss 0.0134 (0.0559)	
training:	Epoch: [18][523/817]	Loss 0.0085 (0.0558)	
training:	Epoch: [18][524/817]	Loss 0.0151 (0.0558)	
training:	Epoch: [18][525/817]	Loss 0.1769 (0.0560)	
training:	Epoch: [18][526/817]	Loss 0.0088 (0.0559)	
training:	Epoch: [18][527/817]	Loss 0.0096 (0.0558)	
training:	Epoch: [18][528/817]	Loss 0.0090 (0.0557)	
training:	Epoch: [18][529/817]	Loss 0.0095 (0.0556)	
training:	Epoch: [18][530/817]	Loss 0.0096 (0.0555)	
training:	Epoch: [18][531/817]	Loss 0.0097 (0.0555)	
training:	Epoch: [18][532/817]	Loss 0.0097 (0.0554)	
training:	Epoch: [18][533/817]	Loss 0.0093 (0.0553)	
training:	Epoch: [18][534/817]	Loss 0.0091 (0.0552)	
training:	Epoch: [18][535/817]	Loss 0.0101 (0.0551)	
training:	Epoch: [18][536/817]	Loss 0.0108 (0.0550)	
training:	Epoch: [18][537/817]	Loss 0.0089 (0.0549)	
training:	Epoch: [18][538/817]	Loss 0.0104 (0.0549)	
training:	Epoch: [18][539/817]	Loss 0.5727 (0.0558)	
training:	Epoch: [18][540/817]	Loss 0.0093 (0.0557)	
training:	Epoch: [18][541/817]	Loss 0.0117 (0.0557)	
training:	Epoch: [18][542/817]	Loss 0.0095 (0.0556)	
training:	Epoch: [18][543/817]	Loss 0.0329 (0.0555)	
training:	Epoch: [18][544/817]	Loss 0.0091 (0.0554)	
training:	Epoch: [18][545/817]	Loss 0.1485 (0.0556)	
training:	Epoch: [18][546/817]	Loss 0.0294 (0.0556)	
training:	Epoch: [18][547/817]	Loss 0.0105 (0.0555)	
training:	Epoch: [18][548/817]	Loss 0.0161 (0.0554)	
training:	Epoch: [18][549/817]	Loss 0.0092 (0.0553)	
training:	Epoch: [18][550/817]	Loss 0.0470 (0.0553)	
training:	Epoch: [18][551/817]	Loss 0.2970 (0.0557)	
training:	Epoch: [18][552/817]	Loss 0.0126 (0.0557)	
training:	Epoch: [18][553/817]	Loss 0.0093 (0.0556)	
training:	Epoch: [18][554/817]	Loss 0.0094 (0.0555)	
training:	Epoch: [18][555/817]	Loss 0.0094 (0.0554)	
training:	Epoch: [18][556/817]	Loss 0.0095 (0.0553)	
training:	Epoch: [18][557/817]	Loss 0.0135 (0.0553)	
training:	Epoch: [18][558/817]	Loss 0.0099 (0.0552)	
training:	Epoch: [18][559/817]	Loss 0.0109 (0.0551)	
training:	Epoch: [18][560/817]	Loss 0.0128 (0.0550)	
training:	Epoch: [18][561/817]	Loss 0.5652 (0.0559)	
training:	Epoch: [18][562/817]	Loss 0.0091 (0.0559)	
training:	Epoch: [18][563/817]	Loss 0.0103 (0.0558)	
training:	Epoch: [18][564/817]	Loss 0.0087 (0.0557)	
training:	Epoch: [18][565/817]	Loss 0.0094 (0.0556)	
training:	Epoch: [18][566/817]	Loss 0.5639 (0.0565)	
training:	Epoch: [18][567/817]	Loss 0.0135 (0.0564)	
training:	Epoch: [18][568/817]	Loss 0.0088 (0.0563)	
training:	Epoch: [18][569/817]	Loss 0.0088 (0.0563)	
training:	Epoch: [18][570/817]	Loss 0.5719 (0.0572)	
training:	Epoch: [18][571/817]	Loss 0.0115 (0.0571)	
training:	Epoch: [18][572/817]	Loss 0.0091 (0.0570)	
training:	Epoch: [18][573/817]	Loss 0.0091 (0.0569)	
training:	Epoch: [18][574/817]	Loss 0.0096 (0.0568)	
training:	Epoch: [18][575/817]	Loss 0.0091 (0.0568)	
training:	Epoch: [18][576/817]	Loss 0.0108 (0.0567)	
training:	Epoch: [18][577/817]	Loss 0.0097 (0.0566)	
training:	Epoch: [18][578/817]	Loss 0.0082 (0.0565)	
training:	Epoch: [18][579/817]	Loss 0.0096 (0.0564)	
training:	Epoch: [18][580/817]	Loss 0.0099 (0.0563)	
training:	Epoch: [18][581/817]	Loss 0.0940 (0.0564)	
training:	Epoch: [18][582/817]	Loss 0.5601 (0.0573)	
training:	Epoch: [18][583/817]	Loss 0.0094 (0.0572)	
training:	Epoch: [18][584/817]	Loss 0.0156 (0.0571)	
training:	Epoch: [18][585/817]	Loss 0.0126 (0.0570)	
training:	Epoch: [18][586/817]	Loss 0.0100 (0.0570)	
training:	Epoch: [18][587/817]	Loss 0.0095 (0.0569)	
training:	Epoch: [18][588/817]	Loss 0.0085 (0.0568)	
training:	Epoch: [18][589/817]	Loss 0.0133 (0.0567)	
training:	Epoch: [18][590/817]	Loss 0.0086 (0.0566)	
training:	Epoch: [18][591/817]	Loss 0.0089 (0.0566)	
training:	Epoch: [18][592/817]	Loss 0.0122 (0.0565)	
training:	Epoch: [18][593/817]	Loss 0.6092 (0.0574)	
training:	Epoch: [18][594/817]	Loss 0.0108 (0.0573)	
training:	Epoch: [18][595/817]	Loss 0.0112 (0.0573)	
training:	Epoch: [18][596/817]	Loss 0.0361 (0.0572)	
training:	Epoch: [18][597/817]	Loss 0.0092 (0.0572)	
training:	Epoch: [18][598/817]	Loss 0.0102 (0.0571)	
training:	Epoch: [18][599/817]	Loss 0.0100 (0.0570)	
training:	Epoch: [18][600/817]	Loss 0.0105 (0.0569)	
training:	Epoch: [18][601/817]	Loss 0.6165 (0.0578)	
training:	Epoch: [18][602/817]	Loss 0.0095 (0.0578)	
training:	Epoch: [18][603/817]	Loss 0.0090 (0.0577)	
training:	Epoch: [18][604/817]	Loss 0.0101 (0.0576)	
training:	Epoch: [18][605/817]	Loss 0.1908 (0.0578)	
training:	Epoch: [18][606/817]	Loss 0.0101 (0.0578)	
training:	Epoch: [18][607/817]	Loss 0.0104 (0.0577)	
training:	Epoch: [18][608/817]	Loss 0.0240 (0.0576)	
training:	Epoch: [18][609/817]	Loss 0.0082 (0.0575)	
training:	Epoch: [18][610/817]	Loss 0.0095 (0.0575)	
training:	Epoch: [18][611/817]	Loss 0.5173 (0.0582)	
training:	Epoch: [18][612/817]	Loss 0.0091 (0.0581)	
training:	Epoch: [18][613/817]	Loss 0.0095 (0.0581)	
training:	Epoch: [18][614/817]	Loss 0.0129 (0.0580)	
training:	Epoch: [18][615/817]	Loss 0.0090 (0.0579)	
training:	Epoch: [18][616/817]	Loss 0.0128 (0.0578)	
training:	Epoch: [18][617/817]	Loss 0.0088 (0.0577)	
training:	Epoch: [18][618/817]	Loss 0.0094 (0.0577)	
training:	Epoch: [18][619/817]	Loss 0.6120 (0.0586)	
training:	Epoch: [18][620/817]	Loss 0.0092 (0.0585)	
training:	Epoch: [18][621/817]	Loss 0.0135 (0.0584)	
training:	Epoch: [18][622/817]	Loss 0.0094 (0.0583)	
training:	Epoch: [18][623/817]	Loss 0.0098 (0.0583)	
training:	Epoch: [18][624/817]	Loss 0.0079 (0.0582)	
training:	Epoch: [18][625/817]	Loss 0.6133 (0.0591)	
training:	Epoch: [18][626/817]	Loss 0.0103 (0.0590)	
training:	Epoch: [18][627/817]	Loss 0.0093 (0.0589)	
training:	Epoch: [18][628/817]	Loss 0.0180 (0.0588)	
training:	Epoch: [18][629/817]	Loss 0.0099 (0.0588)	
training:	Epoch: [18][630/817]	Loss 0.0098 (0.0587)	
training:	Epoch: [18][631/817]	Loss 0.0088 (0.0586)	
training:	Epoch: [18][632/817]	Loss 0.0123 (0.0585)	
training:	Epoch: [18][633/817]	Loss 0.0109 (0.0585)	
training:	Epoch: [18][634/817]	Loss 0.0123 (0.0584)	
training:	Epoch: [18][635/817]	Loss 0.0260 (0.0583)	
training:	Epoch: [18][636/817]	Loss 0.0105 (0.0583)	
training:	Epoch: [18][637/817]	Loss 0.6114 (0.0591)	
training:	Epoch: [18][638/817]	Loss 0.0509 (0.0591)	
training:	Epoch: [18][639/817]	Loss 0.0089 (0.0590)	
training:	Epoch: [18][640/817]	Loss 0.0108 (0.0590)	
training:	Epoch: [18][641/817]	Loss 0.0102 (0.0589)	
training:	Epoch: [18][642/817]	Loss 0.0110 (0.0588)	
training:	Epoch: [18][643/817]	Loss 0.0100 (0.0587)	
training:	Epoch: [18][644/817]	Loss 0.0135 (0.0587)	
training:	Epoch: [18][645/817]	Loss 0.0511 (0.0586)	
training:	Epoch: [18][646/817]	Loss 0.0245 (0.0586)	
training:	Epoch: [18][647/817]	Loss 0.0095 (0.0585)	
training:	Epoch: [18][648/817]	Loss 0.0095 (0.0584)	
training:	Epoch: [18][649/817]	Loss 0.0107 (0.0584)	
training:	Epoch: [18][650/817]	Loss 0.0106 (0.0583)	
training:	Epoch: [18][651/817]	Loss 0.0107 (0.0582)	
training:	Epoch: [18][652/817]	Loss 0.0095 (0.0582)	
training:	Epoch: [18][653/817]	Loss 0.0091 (0.0581)	
training:	Epoch: [18][654/817]	Loss 0.0102 (0.0580)	
training:	Epoch: [18][655/817]	Loss 0.0140 (0.0579)	
training:	Epoch: [18][656/817]	Loss 0.0096 (0.0579)	
training:	Epoch: [18][657/817]	Loss 0.0111 (0.0578)	
training:	Epoch: [18][658/817]	Loss 0.0097 (0.0577)	
training:	Epoch: [18][659/817]	Loss 0.0117 (0.0576)	
training:	Epoch: [18][660/817]	Loss 0.0096 (0.0576)	
training:	Epoch: [18][661/817]	Loss 0.0112 (0.0575)	
training:	Epoch: [18][662/817]	Loss 0.0100 (0.0574)	
training:	Epoch: [18][663/817]	Loss 0.5840 (0.0582)	
training:	Epoch: [18][664/817]	Loss 0.0406 (0.0582)	
training:	Epoch: [18][665/817]	Loss 0.0092 (0.0581)	
training:	Epoch: [18][666/817]	Loss 0.0094 (0.0581)	
training:	Epoch: [18][667/817]	Loss 0.0165 (0.0580)	
training:	Epoch: [18][668/817]	Loss 0.0098 (0.0579)	
training:	Epoch: [18][669/817]	Loss 0.5804 (0.0587)	
training:	Epoch: [18][670/817]	Loss 0.0107 (0.0586)	
training:	Epoch: [18][671/817]	Loss 0.0101 (0.0586)	
training:	Epoch: [18][672/817]	Loss 0.0166 (0.0585)	
training:	Epoch: [18][673/817]	Loss 0.0093 (0.0584)	
training:	Epoch: [18][674/817]	Loss 0.0114 (0.0584)	
training:	Epoch: [18][675/817]	Loss 0.0090 (0.0583)	
training:	Epoch: [18][676/817]	Loss 0.0090 (0.0582)	
training:	Epoch: [18][677/817]	Loss 0.0096 (0.0581)	
training:	Epoch: [18][678/817]	Loss 0.0098 (0.0581)	
training:	Epoch: [18][679/817]	Loss 0.0104 (0.0580)	
training:	Epoch: [18][680/817]	Loss 0.0108 (0.0579)	
training:	Epoch: [18][681/817]	Loss 0.0109 (0.0579)	
training:	Epoch: [18][682/817]	Loss 0.0096 (0.0578)	
training:	Epoch: [18][683/817]	Loss 0.0123 (0.0577)	
training:	Epoch: [18][684/817]	Loss 0.0200 (0.0577)	
training:	Epoch: [18][685/817]	Loss 0.0102 (0.0576)	
training:	Epoch: [18][686/817]	Loss 0.0084 (0.0575)	
training:	Epoch: [18][687/817]	Loss 0.0100 (0.0575)	
training:	Epoch: [18][688/817]	Loss 0.0092 (0.0574)	
training:	Epoch: [18][689/817]	Loss 0.0125 (0.0573)	
training:	Epoch: [18][690/817]	Loss 0.0092 (0.0572)	
training:	Epoch: [18][691/817]	Loss 0.0762 (0.0573)	
training:	Epoch: [18][692/817]	Loss 0.0094 (0.0572)	
training:	Epoch: [18][693/817]	Loss 0.0101 (0.0571)	
training:	Epoch: [18][694/817]	Loss 0.0089 (0.0571)	
training:	Epoch: [18][695/817]	Loss 0.0090 (0.0570)	
training:	Epoch: [18][696/817]	Loss 0.0093 (0.0569)	
training:	Epoch: [18][697/817]	Loss 0.0098 (0.0569)	
training:	Epoch: [18][698/817]	Loss 0.0103 (0.0568)	
training:	Epoch: [18][699/817]	Loss 0.0090 (0.0567)	
training:	Epoch: [18][700/817]	Loss 0.0104 (0.0567)	
training:	Epoch: [18][701/817]	Loss 0.0100 (0.0566)	
training:	Epoch: [18][702/817]	Loss 0.0096 (0.0565)	
training:	Epoch: [18][703/817]	Loss 0.0124 (0.0565)	
training:	Epoch: [18][704/817]	Loss 0.0113 (0.0564)	
training:	Epoch: [18][705/817]	Loss 0.0103 (0.0563)	
training:	Epoch: [18][706/817]	Loss 0.0091 (0.0563)	
training:	Epoch: [18][707/817]	Loss 0.0162 (0.0562)	
training:	Epoch: [18][708/817]	Loss 0.0088 (0.0561)	
training:	Epoch: [18][709/817]	Loss 0.0090 (0.0561)	
training:	Epoch: [18][710/817]	Loss 0.0141 (0.0560)	
training:	Epoch: [18][711/817]	Loss 0.0109 (0.0560)	
training:	Epoch: [18][712/817]	Loss 0.0093 (0.0559)	
training:	Epoch: [18][713/817]	Loss 0.0093 (0.0558)	
training:	Epoch: [18][714/817]	Loss 0.0102 (0.0558)	
training:	Epoch: [18][715/817]	Loss 0.0112 (0.0557)	
training:	Epoch: [18][716/817]	Loss 0.0113 (0.0556)	
training:	Epoch: [18][717/817]	Loss 0.0096 (0.0556)	
training:	Epoch: [18][718/817]	Loss 0.0096 (0.0555)	
training:	Epoch: [18][719/817]	Loss 0.0095 (0.0554)	
training:	Epoch: [18][720/817]	Loss 0.0139 (0.0554)	
training:	Epoch: [18][721/817]	Loss 0.0089 (0.0553)	
training:	Epoch: [18][722/817]	Loss 0.0117 (0.0553)	
training:	Epoch: [18][723/817]	Loss 0.0089 (0.0552)	
training:	Epoch: [18][724/817]	Loss 0.6190 (0.0560)	
training:	Epoch: [18][725/817]	Loss 0.0118 (0.0559)	
training:	Epoch: [18][726/817]	Loss 0.0192 (0.0559)	
training:	Epoch: [18][727/817]	Loss 0.0120 (0.0558)	
training:	Epoch: [18][728/817]	Loss 0.0089 (0.0557)	
training:	Epoch: [18][729/817]	Loss 0.0098 (0.0557)	
training:	Epoch: [18][730/817]	Loss 0.0095 (0.0556)	
training:	Epoch: [18][731/817]	Loss 0.0100 (0.0555)	
training:	Epoch: [18][732/817]	Loss 0.0098 (0.0555)	
training:	Epoch: [18][733/817]	Loss 0.0093 (0.0554)	
training:	Epoch: [18][734/817]	Loss 0.0971 (0.0555)	
training:	Epoch: [18][735/817]	Loss 0.5688 (0.0562)	
training:	Epoch: [18][736/817]	Loss 0.0122 (0.0561)	
training:	Epoch: [18][737/817]	Loss 0.0095 (0.0561)	
training:	Epoch: [18][738/817]	Loss 0.0164 (0.0560)	
training:	Epoch: [18][739/817]	Loss 0.0093 (0.0559)	
training:	Epoch: [18][740/817]	Loss 0.0090 (0.0559)	
training:	Epoch: [18][741/817]	Loss 0.0094 (0.0558)	
training:	Epoch: [18][742/817]	Loss 0.0100 (0.0558)	
training:	Epoch: [18][743/817]	Loss 0.0115 (0.0557)	
training:	Epoch: [18][744/817]	Loss 0.0100 (0.0556)	
training:	Epoch: [18][745/817]	Loss 0.0109 (0.0556)	
training:	Epoch: [18][746/817]	Loss 0.0095 (0.0555)	
training:	Epoch: [18][747/817]	Loss 0.0097 (0.0554)	
training:	Epoch: [18][748/817]	Loss 0.0102 (0.0554)	
training:	Epoch: [18][749/817]	Loss 0.0596 (0.0554)	
training:	Epoch: [18][750/817]	Loss 0.0092 (0.0553)	
training:	Epoch: [18][751/817]	Loss 0.0085 (0.0553)	
training:	Epoch: [18][752/817]	Loss 0.0104 (0.0552)	
training:	Epoch: [18][753/817]	Loss 0.0180 (0.0552)	
training:	Epoch: [18][754/817]	Loss 0.0088 (0.0551)	
training:	Epoch: [18][755/817]	Loss 0.0090 (0.0550)	
training:	Epoch: [18][756/817]	Loss 0.0102 (0.0550)	
training:	Epoch: [18][757/817]	Loss 0.0098 (0.0549)	
training:	Epoch: [18][758/817]	Loss 0.0090 (0.0549)	
training:	Epoch: [18][759/817]	Loss 0.0089 (0.0548)	
training:	Epoch: [18][760/817]	Loss 0.0099 (0.0547)	
training:	Epoch: [18][761/817]	Loss 0.0211 (0.0547)	
training:	Epoch: [18][762/817]	Loss 0.0095 (0.0546)	
training:	Epoch: [18][763/817]	Loss 0.0105 (0.0546)	
training:	Epoch: [18][764/817]	Loss 0.0096 (0.0545)	
training:	Epoch: [18][765/817]	Loss 0.0091 (0.0545)	
training:	Epoch: [18][766/817]	Loss 0.0098 (0.0544)	
training:	Epoch: [18][767/817]	Loss 0.0107 (0.0543)	
training:	Epoch: [18][768/817]	Loss 0.0095 (0.0543)	
training:	Epoch: [18][769/817]	Loss 0.0129 (0.0542)	
training:	Epoch: [18][770/817]	Loss 0.0235 (0.0542)	
training:	Epoch: [18][771/817]	Loss 0.0152 (0.0541)	
training:	Epoch: [18][772/817]	Loss 0.0115 (0.0541)	
training:	Epoch: [18][773/817]	Loss 0.0090 (0.0540)	
training:	Epoch: [18][774/817]	Loss 0.0083 (0.0540)	
training:	Epoch: [18][775/817]	Loss 0.0083 (0.0539)	
training:	Epoch: [18][776/817]	Loss 0.0498 (0.0539)	
training:	Epoch: [18][777/817]	Loss 0.0086 (0.0538)	
training:	Epoch: [18][778/817]	Loss 0.0092 (0.0538)	
training:	Epoch: [18][779/817]	Loss 0.0083 (0.0537)	
training:	Epoch: [18][780/817]	Loss 0.0091 (0.0537)	
training:	Epoch: [18][781/817]	Loss 0.0101 (0.0536)	
training:	Epoch: [18][782/817]	Loss 0.0127 (0.0536)	
training:	Epoch: [18][783/817]	Loss 0.0089 (0.0535)	
training:	Epoch: [18][784/817]	Loss 0.0086 (0.0534)	
training:	Epoch: [18][785/817]	Loss 0.0088 (0.0534)	
training:	Epoch: [18][786/817]	Loss 0.0373 (0.0534)	
training:	Epoch: [18][787/817]	Loss 0.0094 (0.0533)	
training:	Epoch: [18][788/817]	Loss 0.5606 (0.0540)	
training:	Epoch: [18][789/817]	Loss 0.0086 (0.0539)	
training:	Epoch: [18][790/817]	Loss 0.6087 (0.0546)	
training:	Epoch: [18][791/817]	Loss 0.0092 (0.0545)	
training:	Epoch: [18][792/817]	Loss 0.0123 (0.0545)	
training:	Epoch: [18][793/817]	Loss 0.0093 (0.0544)	
training:	Epoch: [18][794/817]	Loss 0.0110 (0.0544)	
training:	Epoch: [18][795/817]	Loss 0.0099 (0.0543)	
training:	Epoch: [18][796/817]	Loss 0.0099 (0.0543)	
training:	Epoch: [18][797/817]	Loss 0.0089 (0.0542)	
training:	Epoch: [18][798/817]	Loss 0.0092 (0.0542)	
training:	Epoch: [18][799/817]	Loss 0.0172 (0.0541)	
training:	Epoch: [18][800/817]	Loss 0.0105 (0.0541)	
training:	Epoch: [18][801/817]	Loss 0.0102 (0.0540)	
training:	Epoch: [18][802/817]	Loss 0.0102 (0.0539)	
training:	Epoch: [18][803/817]	Loss 0.0089 (0.0539)	
training:	Epoch: [18][804/817]	Loss 0.0106 (0.0538)	
training:	Epoch: [18][805/817]	Loss 0.0104 (0.0538)	
training:	Epoch: [18][806/817]	Loss 0.0079 (0.0537)	
training:	Epoch: [18][807/817]	Loss 0.0139 (0.0537)	
training:	Epoch: [18][808/817]	Loss 0.0092 (0.0536)	
training:	Epoch: [18][809/817]	Loss 0.0090 (0.0536)	
training:	Epoch: [18][810/817]	Loss 0.0102 (0.0535)	
training:	Epoch: [18][811/817]	Loss 0.0324 (0.0535)	
training:	Epoch: [18][812/817]	Loss 0.0084 (0.0534)	
training:	Epoch: [18][813/817]	Loss 0.0081 (0.0534)	
training:	Epoch: [18][814/817]	Loss 0.0092 (0.0533)	
training:	Epoch: [18][815/817]	Loss 0.0092 (0.0533)	
training:	Epoch: [18][816/817]	Loss 0.0086 (0.0532)	
training:	Epoch: [18][817/817]	Loss 0.0086 (0.0532)	
Training:	 Loss: 0.0531

Training:	 ACC: 0.9922 0.9922 0.9918 0.9927
Validation:	 ACC: 0.7836 0.7844 0.8014 0.7657
Validation:	 Best_BACC: 0.8223 0.8218 0.8127 0.8318
Validation:	 Loss: 0.8827
Pretraining:	Epoch 19/200
----------
training:	Epoch: [19][1/817]	Loss 0.0089 (0.0089)	
training:	Epoch: [19][2/817]	Loss 0.0106 (0.0098)	
training:	Epoch: [19][3/817]	Loss 0.0089 (0.0095)	
training:	Epoch: [19][4/817]	Loss 0.0097 (0.0096)	
training:	Epoch: [19][5/817]	Loss 0.0110 (0.0099)	
training:	Epoch: [19][6/817]	Loss 0.0093 (0.0098)	
training:	Epoch: [19][7/817]	Loss 0.0096 (0.0097)	
training:	Epoch: [19][8/817]	Loss 0.0110 (0.0099)	
training:	Epoch: [19][9/817]	Loss 0.0085 (0.0098)	
training:	Epoch: [19][10/817]	Loss 0.0095 (0.0097)	
training:	Epoch: [19][11/817]	Loss 0.0101 (0.0098)	
training:	Epoch: [19][12/817]	Loss 0.0104 (0.0098)	
training:	Epoch: [19][13/817]	Loss 0.0100 (0.0098)	
training:	Epoch: [19][14/817]	Loss 0.0090 (0.0098)	
training:	Epoch: [19][15/817]	Loss 0.0095 (0.0097)	
training:	Epoch: [19][16/817]	Loss 0.0099 (0.0098)	
training:	Epoch: [19][17/817]	Loss 0.0099 (0.0098)	
training:	Epoch: [19][18/817]	Loss 0.0091 (0.0097)	
training:	Epoch: [19][19/817]	Loss 0.0795 (0.0134)	
training:	Epoch: [19][20/817]	Loss 0.0091 (0.0132)	
training:	Epoch: [19][21/817]	Loss 0.0098 (0.0130)	
training:	Epoch: [19][22/817]	Loss 0.0115 (0.0129)	
training:	Epoch: [19][23/817]	Loss 0.0095 (0.0128)	
training:	Epoch: [19][24/817]	Loss 0.5636 (0.0357)	
training:	Epoch: [19][25/817]	Loss 0.0094 (0.0347)	
training:	Epoch: [19][26/817]	Loss 0.0093 (0.0337)	
training:	Epoch: [19][27/817]	Loss 0.0088 (0.0328)	
training:	Epoch: [19][28/817]	Loss 0.0083 (0.0319)	
training:	Epoch: [19][29/817]	Loss 0.0090 (0.0311)	
training:	Epoch: [19][30/817]	Loss 0.0098 (0.0304)	
training:	Epoch: [19][31/817]	Loss 0.0086 (0.0297)	
training:	Epoch: [19][32/817]	Loss 0.0088 (0.0291)	
training:	Epoch: [19][33/817]	Loss 0.0084 (0.0284)	
training:	Epoch: [19][34/817]	Loss 0.0088 (0.0279)	
training:	Epoch: [19][35/817]	Loss 0.0148 (0.0275)	
training:	Epoch: [19][36/817]	Loss 0.0112 (0.0270)	
training:	Epoch: [19][37/817]	Loss 0.0090 (0.0265)	
training:	Epoch: [19][38/817]	Loss 0.0087 (0.0261)	
training:	Epoch: [19][39/817]	Loss 0.5232 (0.0388)	
training:	Epoch: [19][40/817]	Loss 0.0103 (0.0381)	
training:	Epoch: [19][41/817]	Loss 0.0128 (0.0375)	
training:	Epoch: [19][42/817]	Loss 0.0103 (0.0368)	
training:	Epoch: [19][43/817]	Loss 0.0122 (0.0363)	
training:	Epoch: [19][44/817]	Loss 0.0085 (0.0356)	
training:	Epoch: [19][45/817]	Loss 0.5716 (0.0475)	
training:	Epoch: [19][46/817]	Loss 0.0082 (0.0467)	
training:	Epoch: [19][47/817]	Loss 0.0447 (0.0466)	
training:	Epoch: [19][48/817]	Loss 0.0137 (0.0460)	
training:	Epoch: [19][49/817]	Loss 0.0088 (0.0452)	
training:	Epoch: [19][50/817]	Loss 0.0102 (0.0445)	
training:	Epoch: [19][51/817]	Loss 0.0102 (0.0438)	
training:	Epoch: [19][52/817]	Loss 0.0093 (0.0432)	
training:	Epoch: [19][53/817]	Loss 0.0266 (0.0429)	
training:	Epoch: [19][54/817]	Loss 0.0089 (0.0422)	
training:	Epoch: [19][55/817]	Loss 0.0092 (0.0416)	
training:	Epoch: [19][56/817]	Loss 0.0098 (0.0411)	
training:	Epoch: [19][57/817]	Loss 0.0129 (0.0406)	
training:	Epoch: [19][58/817]	Loss 0.0118 (0.0401)	
training:	Epoch: [19][59/817]	Loss 0.0091 (0.0395)	
training:	Epoch: [19][60/817]	Loss 0.0079 (0.0390)	
training:	Epoch: [19][61/817]	Loss 0.0098 (0.0385)	
training:	Epoch: [19][62/817]	Loss 0.0094 (0.0381)	
training:	Epoch: [19][63/817]	Loss 0.0083 (0.0376)	
training:	Epoch: [19][64/817]	Loss 0.0128 (0.0372)	
training:	Epoch: [19][65/817]	Loss 0.0095 (0.0368)	
training:	Epoch: [19][66/817]	Loss 0.0110 (0.0364)	
training:	Epoch: [19][67/817]	Loss 0.0088 (0.0360)	
training:	Epoch: [19][68/817]	Loss 0.0146 (0.0357)	
training:	Epoch: [19][69/817]	Loss 0.0119 (0.0353)	
training:	Epoch: [19][70/817]	Loss 0.0079 (0.0349)	
training:	Epoch: [19][71/817]	Loss 0.0080 (0.0345)	
training:	Epoch: [19][72/817]	Loss 0.0082 (0.0342)	
training:	Epoch: [19][73/817]	Loss 0.0085 (0.0338)	
training:	Epoch: [19][74/817]	Loss 0.0090 (0.0335)	
training:	Epoch: [19][75/817]	Loss 0.0097 (0.0332)	
training:	Epoch: [19][76/817]	Loss 0.0092 (0.0329)	
training:	Epoch: [19][77/817]	Loss 0.0109 (0.0326)	
training:	Epoch: [19][78/817]	Loss 0.6147 (0.0400)	
training:	Epoch: [19][79/817]	Loss 0.0111 (0.0397)	
training:	Epoch: [19][80/817]	Loss 0.0085 (0.0393)	
training:	Epoch: [19][81/817]	Loss 0.0084 (0.0389)	
training:	Epoch: [19][82/817]	Loss 0.0108 (0.0386)	
training:	Epoch: [19][83/817]	Loss 0.0093 (0.0382)	
training:	Epoch: [19][84/817]	Loss 0.0115 (0.0379)	
training:	Epoch: [19][85/817]	Loss 0.0086 (0.0375)	
training:	Epoch: [19][86/817]	Loss 0.0095 (0.0372)	
training:	Epoch: [19][87/817]	Loss 0.0103 (0.0369)	
training:	Epoch: [19][88/817]	Loss 0.0090 (0.0366)	
training:	Epoch: [19][89/817]	Loss 0.0098 (0.0363)	
training:	Epoch: [19][90/817]	Loss 0.0084 (0.0360)	
training:	Epoch: [19][91/817]	Loss 0.0093 (0.0357)	
training:	Epoch: [19][92/817]	Loss 0.0118 (0.0354)	
training:	Epoch: [19][93/817]	Loss 0.0103 (0.0352)	
training:	Epoch: [19][94/817]	Loss 0.0092 (0.0349)	
training:	Epoch: [19][95/817]	Loss 0.0096 (0.0346)	
training:	Epoch: [19][96/817]	Loss 0.0087 (0.0343)	
training:	Epoch: [19][97/817]	Loss 0.0086 (0.0341)	
training:	Epoch: [19][98/817]	Loss 0.0087 (0.0338)	
training:	Epoch: [19][99/817]	Loss 0.0094 (0.0336)	
training:	Epoch: [19][100/817]	Loss 0.6193 (0.0394)	
training:	Epoch: [19][101/817]	Loss 0.0088 (0.0391)	
training:	Epoch: [19][102/817]	Loss 0.0099 (0.0388)	
training:	Epoch: [19][103/817]	Loss 0.0093 (0.0386)	
training:	Epoch: [19][104/817]	Loss 0.0103 (0.0383)	
training:	Epoch: [19][105/817]	Loss 0.0237 (0.0381)	
training:	Epoch: [19][106/817]	Loss 0.0093 (0.0379)	
training:	Epoch: [19][107/817]	Loss 0.0096 (0.0376)	
training:	Epoch: [19][108/817]	Loss 0.0088 (0.0373)	
training:	Epoch: [19][109/817]	Loss 0.0098 (0.0371)	
training:	Epoch: [19][110/817]	Loss 0.0102 (0.0368)	
training:	Epoch: [19][111/817]	Loss 0.0097 (0.0366)	
training:	Epoch: [19][112/817]	Loss 0.0092 (0.0364)	
training:	Epoch: [19][113/817]	Loss 0.0101 (0.0361)	
training:	Epoch: [19][114/817]	Loss 0.0094 (0.0359)	
training:	Epoch: [19][115/817]	Loss 0.0108 (0.0357)	
training:	Epoch: [19][116/817]	Loss 0.0084 (0.0354)	
training:	Epoch: [19][117/817]	Loss 0.0091 (0.0352)	
training:	Epoch: [19][118/817]	Loss 0.0083 (0.0350)	
training:	Epoch: [19][119/817]	Loss 0.0085 (0.0348)	
training:	Epoch: [19][120/817]	Loss 0.0110 (0.0346)	
training:	Epoch: [19][121/817]	Loss 0.0083 (0.0343)	
training:	Epoch: [19][122/817]	Loss 0.0076 (0.0341)	
training:	Epoch: [19][123/817]	Loss 0.0099 (0.0339)	
training:	Epoch: [19][124/817]	Loss 0.0097 (0.0337)	
training:	Epoch: [19][125/817]	Loss 0.6221 (0.0384)	
training:	Epoch: [19][126/817]	Loss 0.0086 (0.0382)	
training:	Epoch: [19][127/817]	Loss 0.0147 (0.0380)	
training:	Epoch: [19][128/817]	Loss 0.0109 (0.0378)	
training:	Epoch: [19][129/817]	Loss 0.0107 (0.0376)	
training:	Epoch: [19][130/817]	Loss 0.0152 (0.0374)	
training:	Epoch: [19][131/817]	Loss 0.0103 (0.0372)	
training:	Epoch: [19][132/817]	Loss 0.0088 (0.0370)	
training:	Epoch: [19][133/817]	Loss 0.0087 (0.0368)	
training:	Epoch: [19][134/817]	Loss 0.0089 (0.0366)	
training:	Epoch: [19][135/817]	Loss 0.0095 (0.0364)	
training:	Epoch: [19][136/817]	Loss 0.0094 (0.0362)	
training:	Epoch: [19][137/817]	Loss 0.0088 (0.0360)	
training:	Epoch: [19][138/817]	Loss 0.0092 (0.0358)	
training:	Epoch: [19][139/817]	Loss 0.0091 (0.0356)	
training:	Epoch: [19][140/817]	Loss 0.0084 (0.0354)	
training:	Epoch: [19][141/817]	Loss 0.0101 (0.0352)	
training:	Epoch: [19][142/817]	Loss 0.0096 (0.0350)	
training:	Epoch: [19][143/817]	Loss 0.0102 (0.0349)	
training:	Epoch: [19][144/817]	Loss 0.0090 (0.0347)	
training:	Epoch: [19][145/817]	Loss 0.0092 (0.0345)	
training:	Epoch: [19][146/817]	Loss 0.0090 (0.0343)	
training:	Epoch: [19][147/817]	Loss 0.0090 (0.0342)	
training:	Epoch: [19][148/817]	Loss 0.0106 (0.0340)	
training:	Epoch: [19][149/817]	Loss 0.0079 (0.0338)	
training:	Epoch: [19][150/817]	Loss 0.0101 (0.0337)	
training:	Epoch: [19][151/817]	Loss 0.0076 (0.0335)	
training:	Epoch: [19][152/817]	Loss 0.0094 (0.0333)	
training:	Epoch: [19][153/817]	Loss 0.0084 (0.0332)	
training:	Epoch: [19][154/817]	Loss 0.6250 (0.0370)	
training:	Epoch: [19][155/817]	Loss 0.0090 (0.0368)	
training:	Epoch: [19][156/817]	Loss 0.0088 (0.0367)	
training:	Epoch: [19][157/817]	Loss 0.0088 (0.0365)	
training:	Epoch: [19][158/817]	Loss 0.0092 (0.0363)	
training:	Epoch: [19][159/817]	Loss 0.0096 (0.0361)	
training:	Epoch: [19][160/817]	Loss 0.0076 (0.0360)	
training:	Epoch: [19][161/817]	Loss 0.0090 (0.0358)	
training:	Epoch: [19][162/817]	Loss 0.0084 (0.0356)	
training:	Epoch: [19][163/817]	Loss 0.0112 (0.0355)	
training:	Epoch: [19][164/817]	Loss 0.0084 (0.0353)	
training:	Epoch: [19][165/817]	Loss 0.0092 (0.0352)	
training:	Epoch: [19][166/817]	Loss 0.0081 (0.0350)	
training:	Epoch: [19][167/817]	Loss 0.0111 (0.0349)	
training:	Epoch: [19][168/817]	Loss 0.0106 (0.0347)	
training:	Epoch: [19][169/817]	Loss 0.0096 (0.0346)	
training:	Epoch: [19][170/817]	Loss 0.0092 (0.0344)	
training:	Epoch: [19][171/817]	Loss 0.0089 (0.0343)	
training:	Epoch: [19][172/817]	Loss 0.0091 (0.0341)	
training:	Epoch: [19][173/817]	Loss 0.0086 (0.0340)	
training:	Epoch: [19][174/817]	Loss 0.0087 (0.0338)	
training:	Epoch: [19][175/817]	Loss 0.0089 (0.0337)	
training:	Epoch: [19][176/817]	Loss 0.0087 (0.0335)	
training:	Epoch: [19][177/817]	Loss 0.0092 (0.0334)	
training:	Epoch: [19][178/817]	Loss 0.0086 (0.0333)	
training:	Epoch: [19][179/817]	Loss 0.0082 (0.0331)	
training:	Epoch: [19][180/817]	Loss 0.0090 (0.0330)	
training:	Epoch: [19][181/817]	Loss 0.0083 (0.0328)	
training:	Epoch: [19][182/817]	Loss 0.0092 (0.0327)	
training:	Epoch: [19][183/817]	Loss 0.0914 (0.0330)	
training:	Epoch: [19][184/817]	Loss 0.0084 (0.0329)	
training:	Epoch: [19][185/817]	Loss 0.0089 (0.0328)	
training:	Epoch: [19][186/817]	Loss 0.0088 (0.0326)	
training:	Epoch: [19][187/817]	Loss 0.0099 (0.0325)	
training:	Epoch: [19][188/817]	Loss 0.0085 (0.0324)	
training:	Epoch: [19][189/817]	Loss 0.0090 (0.0323)	
training:	Epoch: [19][190/817]	Loss 0.0098 (0.0322)	
training:	Epoch: [19][191/817]	Loss 0.0085 (0.0320)	
training:	Epoch: [19][192/817]	Loss 0.0087 (0.0319)	
training:	Epoch: [19][193/817]	Loss 0.0134 (0.0318)	
training:	Epoch: [19][194/817]	Loss 0.0088 (0.0317)	
training:	Epoch: [19][195/817]	Loss 0.0086 (0.0316)	
training:	Epoch: [19][196/817]	Loss 0.0098 (0.0315)	
training:	Epoch: [19][197/817]	Loss 0.6244 (0.0345)	
training:	Epoch: [19][198/817]	Loss 0.0092 (0.0343)	
training:	Epoch: [19][199/817]	Loss 0.0089 (0.0342)	
training:	Epoch: [19][200/817]	Loss 0.0103 (0.0341)	
training:	Epoch: [19][201/817]	Loss 0.0098 (0.0340)	
training:	Epoch: [19][202/817]	Loss 0.0106 (0.0339)	
training:	Epoch: [19][203/817]	Loss 0.0133 (0.0338)	
training:	Epoch: [19][204/817]	Loss 0.0276 (0.0337)	
training:	Epoch: [19][205/817]	Loss 0.0087 (0.0336)	
training:	Epoch: [19][206/817]	Loss 0.5798 (0.0363)	
training:	Epoch: [19][207/817]	Loss 0.0089 (0.0361)	
training:	Epoch: [19][208/817]	Loss 0.0076 (0.0360)	
training:	Epoch: [19][209/817]	Loss 0.0087 (0.0359)	
training:	Epoch: [19][210/817]	Loss 0.0091 (0.0357)	
training:	Epoch: [19][211/817]	Loss 0.0084 (0.0356)	
training:	Epoch: [19][212/817]	Loss 0.0082 (0.0355)	
training:	Epoch: [19][213/817]	Loss 0.0095 (0.0354)	
training:	Epoch: [19][214/817]	Loss 0.0111 (0.0352)	
training:	Epoch: [19][215/817]	Loss 0.0087 (0.0351)	
training:	Epoch: [19][216/817]	Loss 0.0084 (0.0350)	
training:	Epoch: [19][217/817]	Loss 0.0084 (0.0349)	
training:	Epoch: [19][218/817]	Loss 0.0087 (0.0348)	
training:	Epoch: [19][219/817]	Loss 0.0145 (0.0347)	
training:	Epoch: [19][220/817]	Loss 0.0098 (0.0345)	
training:	Epoch: [19][221/817]	Loss 0.1423 (0.0350)	
training:	Epoch: [19][222/817]	Loss 0.0084 (0.0349)	
training:	Epoch: [19][223/817]	Loss 0.5962 (0.0374)	
training:	Epoch: [19][224/817]	Loss 0.0086 (0.0373)	
training:	Epoch: [19][225/817]	Loss 0.0090 (0.0372)	
training:	Epoch: [19][226/817]	Loss 0.0083 (0.0370)	
training:	Epoch: [19][227/817]	Loss 0.0085 (0.0369)	
training:	Epoch: [19][228/817]	Loss 0.6230 (0.0395)	
training:	Epoch: [19][229/817]	Loss 0.0096 (0.0394)	
training:	Epoch: [19][230/817]	Loss 0.0085 (0.0392)	
training:	Epoch: [19][231/817]	Loss 0.4876 (0.0412)	
training:	Epoch: [19][232/817]	Loss 0.0090 (0.0410)	
training:	Epoch: [19][233/817]	Loss 0.0606 (0.0411)	
training:	Epoch: [19][234/817]	Loss 0.0084 (0.0410)	
training:	Epoch: [19][235/817]	Loss 0.0084 (0.0408)	
training:	Epoch: [19][236/817]	Loss 0.0091 (0.0407)	
training:	Epoch: [19][237/817]	Loss 0.0094 (0.0406)	
training:	Epoch: [19][238/817]	Loss 0.0089 (0.0404)	
training:	Epoch: [19][239/817]	Loss 0.2458 (0.0413)	
training:	Epoch: [19][240/817]	Loss 0.0090 (0.0412)	
training:	Epoch: [19][241/817]	Loss 0.0089 (0.0410)	
training:	Epoch: [19][242/817]	Loss 0.0135 (0.0409)	
training:	Epoch: [19][243/817]	Loss 0.0088 (0.0408)	
training:	Epoch: [19][244/817]	Loss 0.0091 (0.0407)	
training:	Epoch: [19][245/817]	Loss 0.0100 (0.0405)	
training:	Epoch: [19][246/817]	Loss 0.0086 (0.0404)	
training:	Epoch: [19][247/817]	Loss 0.0088 (0.0403)	
training:	Epoch: [19][248/817]	Loss 0.0083 (0.0401)	
training:	Epoch: [19][249/817]	Loss 0.0086 (0.0400)	
training:	Epoch: [19][250/817]	Loss 0.0086 (0.0399)	
training:	Epoch: [19][251/817]	Loss 0.0082 (0.0398)	
training:	Epoch: [19][252/817]	Loss 0.0088 (0.0396)	
training:	Epoch: [19][253/817]	Loss 0.0096 (0.0395)	
training:	Epoch: [19][254/817]	Loss 0.0087 (0.0394)	
training:	Epoch: [19][255/817]	Loss 0.4180 (0.0409)	
training:	Epoch: [19][256/817]	Loss 0.0088 (0.0408)	
training:	Epoch: [19][257/817]	Loss 0.5671 (0.0428)	
training:	Epoch: [19][258/817]	Loss 0.0149 (0.0427)	
training:	Epoch: [19][259/817]	Loss 0.0087 (0.0426)	
training:	Epoch: [19][260/817]	Loss 0.0088 (0.0424)	
training:	Epoch: [19][261/817]	Loss 0.5925 (0.0445)	
training:	Epoch: [19][262/817]	Loss 0.2414 (0.0453)	
training:	Epoch: [19][263/817]	Loss 0.0087 (0.0452)	
training:	Epoch: [19][264/817]	Loss 0.0185 (0.0451)	
training:	Epoch: [19][265/817]	Loss 0.0131 (0.0449)	
training:	Epoch: [19][266/817]	Loss 0.0078 (0.0448)	
training:	Epoch: [19][267/817]	Loss 0.0084 (0.0447)	
training:	Epoch: [19][268/817]	Loss 0.0081 (0.0445)	
training:	Epoch: [19][269/817]	Loss 0.0086 (0.0444)	
training:	Epoch: [19][270/817]	Loss 0.1160 (0.0447)	
training:	Epoch: [19][271/817]	Loss 0.0083 (0.0445)	
training:	Epoch: [19][272/817]	Loss 0.5816 (0.0465)	
training:	Epoch: [19][273/817]	Loss 0.5117 (0.0482)	
training:	Epoch: [19][274/817]	Loss 0.1141 (0.0484)	
training:	Epoch: [19][275/817]	Loss 0.0084 (0.0483)	
training:	Epoch: [19][276/817]	Loss 0.0099 (0.0482)	
training:	Epoch: [19][277/817]	Loss 0.0155 (0.0480)	
training:	Epoch: [19][278/817]	Loss 0.0103 (0.0479)	
training:	Epoch: [19][279/817]	Loss 0.0085 (0.0478)	
training:	Epoch: [19][280/817]	Loss 0.0091 (0.0476)	
training:	Epoch: [19][281/817]	Loss 0.0099 (0.0475)	
training:	Epoch: [19][282/817]	Loss 0.0103 (0.0474)	
training:	Epoch: [19][283/817]	Loss 0.0093 (0.0472)	
training:	Epoch: [19][284/817]	Loss 0.4748 (0.0487)	
training:	Epoch: [19][285/817]	Loss 0.0088 (0.0486)	
training:	Epoch: [19][286/817]	Loss 0.0091 (0.0484)	
training:	Epoch: [19][287/817]	Loss 0.0087 (0.0483)	
training:	Epoch: [19][288/817]	Loss 0.0081 (0.0482)	
training:	Epoch: [19][289/817]	Loss 0.0139 (0.0481)	
training:	Epoch: [19][290/817]	Loss 0.0094 (0.0479)	
training:	Epoch: [19][291/817]	Loss 0.0084 (0.0478)	
training:	Epoch: [19][292/817]	Loss 0.0102 (0.0477)	
training:	Epoch: [19][293/817]	Loss 0.0091 (0.0475)	
training:	Epoch: [19][294/817]	Loss 0.0092 (0.0474)	
training:	Epoch: [19][295/817]	Loss 0.0097 (0.0473)	
training:	Epoch: [19][296/817]	Loss 0.0104 (0.0471)	
training:	Epoch: [19][297/817]	Loss 0.0091 (0.0470)	
training:	Epoch: [19][298/817]	Loss 0.0100 (0.0469)	
training:	Epoch: [19][299/817]	Loss 0.0090 (0.0468)	
training:	Epoch: [19][300/817]	Loss 0.0085 (0.0466)	
training:	Epoch: [19][301/817]	Loss 0.0088 (0.0465)	
training:	Epoch: [19][302/817]	Loss 0.0108 (0.0464)	
training:	Epoch: [19][303/817]	Loss 0.0121 (0.0463)	
training:	Epoch: [19][304/817]	Loss 0.0089 (0.0462)	
training:	Epoch: [19][305/817]	Loss 0.0094 (0.0460)	
training:	Epoch: [19][306/817]	Loss 0.0129 (0.0459)	
training:	Epoch: [19][307/817]	Loss 0.0094 (0.0458)	
training:	Epoch: [19][308/817]	Loss 0.0082 (0.0457)	
training:	Epoch: [19][309/817]	Loss 0.0206 (0.0456)	
training:	Epoch: [19][310/817]	Loss 0.0163 (0.0455)	
training:	Epoch: [19][311/817]	Loss 0.0096 (0.0454)	
training:	Epoch: [19][312/817]	Loss 0.0097 (0.0453)	
training:	Epoch: [19][313/817]	Loss 0.0089 (0.0452)	
training:	Epoch: [19][314/817]	Loss 0.0094 (0.0450)	
training:	Epoch: [19][315/817]	Loss 0.0111 (0.0449)	
training:	Epoch: [19][316/817]	Loss 0.0091 (0.0448)	
training:	Epoch: [19][317/817]	Loss 0.0094 (0.0447)	
training:	Epoch: [19][318/817]	Loss 0.0103 (0.0446)	
training:	Epoch: [19][319/817]	Loss 0.0119 (0.0445)	
training:	Epoch: [19][320/817]	Loss 0.0136 (0.0444)	
training:	Epoch: [19][321/817]	Loss 0.2900 (0.0452)	
training:	Epoch: [19][322/817]	Loss 0.0232 (0.0451)	
training:	Epoch: [19][323/817]	Loss 0.0092 (0.0450)	
training:	Epoch: [19][324/817]	Loss 0.0100 (0.0449)	
training:	Epoch: [19][325/817]	Loss 0.0097 (0.0448)	
training:	Epoch: [19][326/817]	Loss 0.2307 (0.0453)	
training:	Epoch: [19][327/817]	Loss 0.0098 (0.0452)	
training:	Epoch: [19][328/817]	Loss 0.0095 (0.0451)	
training:	Epoch: [19][329/817]	Loss 0.0093 (0.0450)	
training:	Epoch: [19][330/817]	Loss 0.0100 (0.0449)	
training:	Epoch: [19][331/817]	Loss 0.0093 (0.0448)	
training:	Epoch: [19][332/817]	Loss 0.0089 (0.0447)	
training:	Epoch: [19][333/817]	Loss 0.0099 (0.0446)	
training:	Epoch: [19][334/817]	Loss 0.0096 (0.0445)	
training:	Epoch: [19][335/817]	Loss 0.0098 (0.0444)	
training:	Epoch: [19][336/817]	Loss 0.5819 (0.0460)	
training:	Epoch: [19][337/817]	Loss 0.0083 (0.0459)	
training:	Epoch: [19][338/817]	Loss 0.0086 (0.0458)	
training:	Epoch: [19][339/817]	Loss 0.0094 (0.0457)	
training:	Epoch: [19][340/817]	Loss 0.0720 (0.0457)	
training:	Epoch: [19][341/817]	Loss 0.0107 (0.0456)	
training:	Epoch: [19][342/817]	Loss 0.0096 (0.0455)	
training:	Epoch: [19][343/817]	Loss 0.0087 (0.0454)	
training:	Epoch: [19][344/817]	Loss 0.0149 (0.0453)	
training:	Epoch: [19][345/817]	Loss 0.6152 (0.0470)	
training:	Epoch: [19][346/817]	Loss 0.0081 (0.0469)	
training:	Epoch: [19][347/817]	Loss 0.6172 (0.0485)	
training:	Epoch: [19][348/817]	Loss 0.0102 (0.0484)	
training:	Epoch: [19][349/817]	Loss 0.0111 (0.0483)	
training:	Epoch: [19][350/817]	Loss 0.0112 (0.0482)	
training:	Epoch: [19][351/817]	Loss 0.0086 (0.0481)	
training:	Epoch: [19][352/817]	Loss 0.0100 (0.0480)	
training:	Epoch: [19][353/817]	Loss 1.1735 (0.0512)	
training:	Epoch: [19][354/817]	Loss 0.0088 (0.0510)	
training:	Epoch: [19][355/817]	Loss 0.2289 (0.0515)	
training:	Epoch: [19][356/817]	Loss 0.0089 (0.0514)	
training:	Epoch: [19][357/817]	Loss 0.0188 (0.0513)	
training:	Epoch: [19][358/817]	Loss 0.0104 (0.0512)	
training:	Epoch: [19][359/817]	Loss 0.0205 (0.0511)	
training:	Epoch: [19][360/817]	Loss 0.0098 (0.0510)	
training:	Epoch: [19][361/817]	Loss 0.0089 (0.0509)	
training:	Epoch: [19][362/817]	Loss 0.0077 (0.0508)	
training:	Epoch: [19][363/817]	Loss 0.0081 (0.0507)	
training:	Epoch: [19][364/817]	Loss 0.0102 (0.0505)	
training:	Epoch: [19][365/817]	Loss 0.0218 (0.0505)	
training:	Epoch: [19][366/817]	Loss 0.0110 (0.0504)	
training:	Epoch: [19][367/817]	Loss 0.0093 (0.0503)	
training:	Epoch: [19][368/817]	Loss 0.0168 (0.0502)	
training:	Epoch: [19][369/817]	Loss 0.0100 (0.0501)	
training:	Epoch: [19][370/817]	Loss 0.0201 (0.0500)	
training:	Epoch: [19][371/817]	Loss 0.0112 (0.0499)	
training:	Epoch: [19][372/817]	Loss 0.0105 (0.0498)	
training:	Epoch: [19][373/817]	Loss 0.5801 (0.0512)	
training:	Epoch: [19][374/817]	Loss 0.0199 (0.0511)	
training:	Epoch: [19][375/817]	Loss 0.0086 (0.0510)	
training:	Epoch: [19][376/817]	Loss 0.0102 (0.0509)	
training:	Epoch: [19][377/817]	Loss 0.0083 (0.0508)	
training:	Epoch: [19][378/817]	Loss 0.0094 (0.0507)	
training:	Epoch: [19][379/817]	Loss 0.0091 (0.0505)	
training:	Epoch: [19][380/817]	Loss 0.0126 (0.0504)	
training:	Epoch: [19][381/817]	Loss 0.0085 (0.0503)	
training:	Epoch: [19][382/817]	Loss 0.0092 (0.0502)	
training:	Epoch: [19][383/817]	Loss 0.0087 (0.0501)	
training:	Epoch: [19][384/817]	Loss 0.0159 (0.0500)	
training:	Epoch: [19][385/817]	Loss 0.5380 (0.0513)	
training:	Epoch: [19][386/817]	Loss 0.6358 (0.0528)	
training:	Epoch: [19][387/817]	Loss 0.0595 (0.0528)	
training:	Epoch: [19][388/817]	Loss 0.0083 (0.0527)	
training:	Epoch: [19][389/817]	Loss 0.0098 (0.0526)	
training:	Epoch: [19][390/817]	Loss 0.0117 (0.0525)	
training:	Epoch: [19][391/817]	Loss 0.0081 (0.0524)	
training:	Epoch: [19][392/817]	Loss 0.0097 (0.0523)	
training:	Epoch: [19][393/817]	Loss 0.0184 (0.0522)	
training:	Epoch: [19][394/817]	Loss 0.0086 (0.0521)	
training:	Epoch: [19][395/817]	Loss 0.0131 (0.0520)	
training:	Epoch: [19][396/817]	Loss 0.0094 (0.0519)	
training:	Epoch: [19][397/817]	Loss 0.0088 (0.0518)	
training:	Epoch: [19][398/817]	Loss 0.0143 (0.0517)	
training:	Epoch: [19][399/817]	Loss 0.0335 (0.0516)	
training:	Epoch: [19][400/817]	Loss 0.0094 (0.0515)	
training:	Epoch: [19][401/817]	Loss 0.5778 (0.0528)	
training:	Epoch: [19][402/817]	Loss 0.0111 (0.0527)	
training:	Epoch: [19][403/817]	Loss 0.0105 (0.0526)	
training:	Epoch: [19][404/817]	Loss 0.0107 (0.0525)	
training:	Epoch: [19][405/817]	Loss 0.0417 (0.0525)	
training:	Epoch: [19][406/817]	Loss 0.0087 (0.0524)	
training:	Epoch: [19][407/817]	Loss 0.6303 (0.0538)	
training:	Epoch: [19][408/817]	Loss 0.0084 (0.0537)	
training:	Epoch: [19][409/817]	Loss 0.0093 (0.0536)	
training:	Epoch: [19][410/817]	Loss 0.0098 (0.0535)	
training:	Epoch: [19][411/817]	Loss 0.0081 (0.0534)	
training:	Epoch: [19][412/817]	Loss 0.0081 (0.0533)	
training:	Epoch: [19][413/817]	Loss 0.0089 (0.0531)	
training:	Epoch: [19][414/817]	Loss 0.0101 (0.0530)	
training:	Epoch: [19][415/817]	Loss 0.0219 (0.0530)	
training:	Epoch: [19][416/817]	Loss 0.0168 (0.0529)	
training:	Epoch: [19][417/817]	Loss 0.0095 (0.0528)	
training:	Epoch: [19][418/817]	Loss 0.0116 (0.0527)	
training:	Epoch: [19][419/817]	Loss 0.0097 (0.0526)	
training:	Epoch: [19][420/817]	Loss 0.0085 (0.0525)	
training:	Epoch: [19][421/817]	Loss 0.0125 (0.0524)	
training:	Epoch: [19][422/817]	Loss 0.0142 (0.0523)	
training:	Epoch: [19][423/817]	Loss 0.0085 (0.0522)	
training:	Epoch: [19][424/817]	Loss 0.0098 (0.0521)	
training:	Epoch: [19][425/817]	Loss 0.0097 (0.0520)	
training:	Epoch: [19][426/817]	Loss 0.0095 (0.0519)	
training:	Epoch: [19][427/817]	Loss 0.0086 (0.0518)	
training:	Epoch: [19][428/817]	Loss 0.0117 (0.0517)	
training:	Epoch: [19][429/817]	Loss 0.0136 (0.0516)	
training:	Epoch: [19][430/817]	Loss 0.0087 (0.0515)	
training:	Epoch: [19][431/817]	Loss 0.0079 (0.0514)	
training:	Epoch: [19][432/817]	Loss 0.0085 (0.0513)	
training:	Epoch: [19][433/817]	Loss 0.0096 (0.0512)	
training:	Epoch: [19][434/817]	Loss 0.0104 (0.0511)	
training:	Epoch: [19][435/817]	Loss 0.0097 (0.0510)	
training:	Epoch: [19][436/817]	Loss 0.0090 (0.0509)	
training:	Epoch: [19][437/817]	Loss 0.0096 (0.0508)	
training:	Epoch: [19][438/817]	Loss 0.0088 (0.0507)	
training:	Epoch: [19][439/817]	Loss 0.6086 (0.0520)	
training:	Epoch: [19][440/817]	Loss 0.0085 (0.0519)	
training:	Epoch: [19][441/817]	Loss 0.6000 (0.0531)	
training:	Epoch: [19][442/817]	Loss 0.0096 (0.0530)	
training:	Epoch: [19][443/817]	Loss 0.0085 (0.0529)	
training:	Epoch: [19][444/817]	Loss 0.0085 (0.0528)	
training:	Epoch: [19][445/817]	Loss 0.0097 (0.0527)	
training:	Epoch: [19][446/817]	Loss 0.0085 (0.0526)	
training:	Epoch: [19][447/817]	Loss 0.0092 (0.0525)	
training:	Epoch: [19][448/817]	Loss 0.0095 (0.0525)	
training:	Epoch: [19][449/817]	Loss 0.0082 (0.0524)	
training:	Epoch: [19][450/817]	Loss 0.0094 (0.0523)	
training:	Epoch: [19][451/817]	Loss 0.0089 (0.0522)	
training:	Epoch: [19][452/817]	Loss 0.0091 (0.0521)	
training:	Epoch: [19][453/817]	Loss 0.0085 (0.0520)	
training:	Epoch: [19][454/817]	Loss 0.0110 (0.0519)	
training:	Epoch: [19][455/817]	Loss 0.0155 (0.0518)	
training:	Epoch: [19][456/817]	Loss 0.0080 (0.0517)	
training:	Epoch: [19][457/817]	Loss 0.0114 (0.0516)	
training:	Epoch: [19][458/817]	Loss 0.0088 (0.0515)	
training:	Epoch: [19][459/817]	Loss 0.5926 (0.0527)	
training:	Epoch: [19][460/817]	Loss 0.6017 (0.0539)	
training:	Epoch: [19][461/817]	Loss 0.0095 (0.0538)	
training:	Epoch: [19][462/817]	Loss 0.0086 (0.0537)	
training:	Epoch: [19][463/817]	Loss 0.0091 (0.0536)	
training:	Epoch: [19][464/817]	Loss 0.0092 (0.0535)	
training:	Epoch: [19][465/817]	Loss 0.0082 (0.0534)	
training:	Epoch: [19][466/817]	Loss 0.0086 (0.0533)	
training:	Epoch: [19][467/817]	Loss 0.0092 (0.0532)	
training:	Epoch: [19][468/817]	Loss 0.0135 (0.0531)	
training:	Epoch: [19][469/817]	Loss 0.0087 (0.0530)	
training:	Epoch: [19][470/817]	Loss 0.6106 (0.0542)	
training:	Epoch: [19][471/817]	Loss 0.0091 (0.0541)	
training:	Epoch: [19][472/817]	Loss 0.0102 (0.0540)	
training:	Epoch: [19][473/817]	Loss 0.0083 (0.0539)	
training:	Epoch: [19][474/817]	Loss 0.0081 (0.0538)	
training:	Epoch: [19][475/817]	Loss 0.0087 (0.0538)	
training:	Epoch: [19][476/817]	Loss 0.0092 (0.0537)	
training:	Epoch: [19][477/817]	Loss 0.0092 (0.0536)	
training:	Epoch: [19][478/817]	Loss 0.0111 (0.0535)	
training:	Epoch: [19][479/817]	Loss 0.0091 (0.0534)	
training:	Epoch: [19][480/817]	Loss 0.0093 (0.0533)	
training:	Epoch: [19][481/817]	Loss 0.0091 (0.0532)	
training:	Epoch: [19][482/817]	Loss 0.0100 (0.0531)	
training:	Epoch: [19][483/817]	Loss 0.0091 (0.0530)	
training:	Epoch: [19][484/817]	Loss 0.0086 (0.0529)	
training:	Epoch: [19][485/817]	Loss 0.0085 (0.0528)	
training:	Epoch: [19][486/817]	Loss 0.0102 (0.0527)	
training:	Epoch: [19][487/817]	Loss 0.0093 (0.0527)	
training:	Epoch: [19][488/817]	Loss 0.0117 (0.0526)	
training:	Epoch: [19][489/817]	Loss 0.0088 (0.0525)	
training:	Epoch: [19][490/817]	Loss 0.0129 (0.0524)	
training:	Epoch: [19][491/817]	Loss 0.0092 (0.0523)	
training:	Epoch: [19][492/817]	Loss 0.0102 (0.0522)	
training:	Epoch: [19][493/817]	Loss 0.0085 (0.0521)	
training:	Epoch: [19][494/817]	Loss 0.0085 (0.0521)	
training:	Epoch: [19][495/817]	Loss 1.1507 (0.0543)	
training:	Epoch: [19][496/817]	Loss 0.0089 (0.0542)	
training:	Epoch: [19][497/817]	Loss 0.0112 (0.0541)	
training:	Epoch: [19][498/817]	Loss 0.0083 (0.0540)	
training:	Epoch: [19][499/817]	Loss 0.0094 (0.0539)	
training:	Epoch: [19][500/817]	Loss 0.0093 (0.0538)	
training:	Epoch: [19][501/817]	Loss 0.0096 (0.0537)	
training:	Epoch: [19][502/817]	Loss 0.0090 (0.0536)	
training:	Epoch: [19][503/817]	Loss 0.0102 (0.0536)	
training:	Epoch: [19][504/817]	Loss 0.0082 (0.0535)	
training:	Epoch: [19][505/817]	Loss 0.0094 (0.0534)	
training:	Epoch: [19][506/817]	Loss 0.0085 (0.0533)	
training:	Epoch: [19][507/817]	Loss 0.5284 (0.0542)	
training:	Epoch: [19][508/817]	Loss 0.0100 (0.0541)	
training:	Epoch: [19][509/817]	Loss 0.0106 (0.0541)	
training:	Epoch: [19][510/817]	Loss 0.0102 (0.0540)	
training:	Epoch: [19][511/817]	Loss 0.0101 (0.0539)	
training:	Epoch: [19][512/817]	Loss 0.0092 (0.0538)	
training:	Epoch: [19][513/817]	Loss 0.0104 (0.0537)	
training:	Epoch: [19][514/817]	Loss 0.0155 (0.0536)	
training:	Epoch: [19][515/817]	Loss 0.0086 (0.0536)	
training:	Epoch: [19][516/817]	Loss 0.0102 (0.0535)	
training:	Epoch: [19][517/817]	Loss 0.0094 (0.0534)	
training:	Epoch: [19][518/817]	Loss 0.0086 (0.0533)	
training:	Epoch: [19][519/817]	Loss 0.0097 (0.0532)	
training:	Epoch: [19][520/817]	Loss 0.0090 (0.0531)	
training:	Epoch: [19][521/817]	Loss 0.0094 (0.0530)	
training:	Epoch: [19][522/817]	Loss 0.0095 (0.0530)	
training:	Epoch: [19][523/817]	Loss 0.0086 (0.0529)	
training:	Epoch: [19][524/817]	Loss 0.0087 (0.0528)	
training:	Epoch: [19][525/817]	Loss 0.0101 (0.0527)	
training:	Epoch: [19][526/817]	Loss 0.5302 (0.0536)	
training:	Epoch: [19][527/817]	Loss 0.0095 (0.0535)	
training:	Epoch: [19][528/817]	Loss 0.0095 (0.0535)	
training:	Epoch: [19][529/817]	Loss 0.0096 (0.0534)	
training:	Epoch: [19][530/817]	Loss 0.0087 (0.0533)	
training:	Epoch: [19][531/817]	Loss 0.0087 (0.0532)	
training:	Epoch: [19][532/817]	Loss 0.0090 (0.0531)	
training:	Epoch: [19][533/817]	Loss 0.0269 (0.0531)	
training:	Epoch: [19][534/817]	Loss 0.0174 (0.0530)	
training:	Epoch: [19][535/817]	Loss 0.0102 (0.0529)	
training:	Epoch: [19][536/817]	Loss 0.0090 (0.0528)	
training:	Epoch: [19][537/817]	Loss 0.0117 (0.0528)	
training:	Epoch: [19][538/817]	Loss 0.0159 (0.0527)	
training:	Epoch: [19][539/817]	Loss 0.0084 (0.0526)	
training:	Epoch: [19][540/817]	Loss 0.0089 (0.0525)	
training:	Epoch: [19][541/817]	Loss 0.0106 (0.0525)	
training:	Epoch: [19][542/817]	Loss 0.0091 (0.0524)	
training:	Epoch: [19][543/817]	Loss 0.0115 (0.0523)	
training:	Epoch: [19][544/817]	Loss 0.0082 (0.0522)	
training:	Epoch: [19][545/817]	Loss 0.0089 (0.0521)	
training:	Epoch: [19][546/817]	Loss 0.0091 (0.0521)	
training:	Epoch: [19][547/817]	Loss 0.0086 (0.0520)	
training:	Epoch: [19][548/817]	Loss 0.0095 (0.0519)	
training:	Epoch: [19][549/817]	Loss 0.0093 (0.0518)	
training:	Epoch: [19][550/817]	Loss 0.0097 (0.0517)	
training:	Epoch: [19][551/817]	Loss 0.0097 (0.0517)	
training:	Epoch: [19][552/817]	Loss 0.0088 (0.0516)	
training:	Epoch: [19][553/817]	Loss 0.0088 (0.0515)	
training:	Epoch: [19][554/817]	Loss 0.0084 (0.0514)	
training:	Epoch: [19][555/817]	Loss 0.0351 (0.0514)	
training:	Epoch: [19][556/817]	Loss 0.0096 (0.0513)	
training:	Epoch: [19][557/817]	Loss 0.0105 (0.0513)	
training:	Epoch: [19][558/817]	Loss 0.0088 (0.0512)	
training:	Epoch: [19][559/817]	Loss 0.0085 (0.0511)	
training:	Epoch: [19][560/817]	Loss 0.0084 (0.0510)	
training:	Epoch: [19][561/817]	Loss 0.0090 (0.0510)	
training:	Epoch: [19][562/817]	Loss 0.0206 (0.0509)	
training:	Epoch: [19][563/817]	Loss 0.0096 (0.0508)	
training:	Epoch: [19][564/817]	Loss 0.0090 (0.0508)	
training:	Epoch: [19][565/817]	Loss 0.0113 (0.0507)	
training:	Epoch: [19][566/817]	Loss 0.0087 (0.0506)	
training:	Epoch: [19][567/817]	Loss 0.0087 (0.0505)	
training:	Epoch: [19][568/817]	Loss 0.0082 (0.0505)	
training:	Epoch: [19][569/817]	Loss 0.0082 (0.0504)	
training:	Epoch: [19][570/817]	Loss 0.0103 (0.0503)	
training:	Epoch: [19][571/817]	Loss 0.0093 (0.0502)	
training:	Epoch: [19][572/817]	Loss 0.0107 (0.0502)	
training:	Epoch: [19][573/817]	Loss 0.0087 (0.0501)	
training:	Epoch: [19][574/817]	Loss 0.0096 (0.0500)	
training:	Epoch: [19][575/817]	Loss 0.0084 (0.0500)	
training:	Epoch: [19][576/817]	Loss 0.0094 (0.0499)	
training:	Epoch: [19][577/817]	Loss 0.0081 (0.0498)	
training:	Epoch: [19][578/817]	Loss 0.0085 (0.0497)	
training:	Epoch: [19][579/817]	Loss 0.0108 (0.0497)	
training:	Epoch: [19][580/817]	Loss 0.0076 (0.0496)	
training:	Epoch: [19][581/817]	Loss 1.2032 (0.0516)	
training:	Epoch: [19][582/817]	Loss 0.0088 (0.0515)	
training:	Epoch: [19][583/817]	Loss 0.0096 (0.0514)	
training:	Epoch: [19][584/817]	Loss 0.0079 (0.0514)	
training:	Epoch: [19][585/817]	Loss 0.0089 (0.0513)	
training:	Epoch: [19][586/817]	Loss 0.0082 (0.0512)	
training:	Epoch: [19][587/817]	Loss 0.0093 (0.0512)	
training:	Epoch: [19][588/817]	Loss 0.0089 (0.0511)	
training:	Epoch: [19][589/817]	Loss 0.0116 (0.0510)	
training:	Epoch: [19][590/817]	Loss 0.0086 (0.0509)	
training:	Epoch: [19][591/817]	Loss 0.0086 (0.0509)	
training:	Epoch: [19][592/817]	Loss 0.0174 (0.0508)	
training:	Epoch: [19][593/817]	Loss 0.0090 (0.0507)	
training:	Epoch: [19][594/817]	Loss 0.0081 (0.0507)	
training:	Epoch: [19][595/817]	Loss 0.0096 (0.0506)	
training:	Epoch: [19][596/817]	Loss 0.0083 (0.0505)	
training:	Epoch: [19][597/817]	Loss 0.0130 (0.0505)	
training:	Epoch: [19][598/817]	Loss 0.0116 (0.0504)	
training:	Epoch: [19][599/817]	Loss 0.0133 (0.0503)	
training:	Epoch: [19][600/817]	Loss 0.0092 (0.0503)	
training:	Epoch: [19][601/817]	Loss 0.0094 (0.0502)	
training:	Epoch: [19][602/817]	Loss 0.0086 (0.0501)	
training:	Epoch: [19][603/817]	Loss 0.0085 (0.0501)	
training:	Epoch: [19][604/817]	Loss 0.0094 (0.0500)	
training:	Epoch: [19][605/817]	Loss 0.0081 (0.0499)	
training:	Epoch: [19][606/817]	Loss 0.0079 (0.0499)	
training:	Epoch: [19][607/817]	Loss 0.0099 (0.0498)	
training:	Epoch: [19][608/817]	Loss 0.0100 (0.0497)	
training:	Epoch: [19][609/817]	Loss 0.0098 (0.0497)	
training:	Epoch: [19][610/817]	Loss 0.6133 (0.0506)	
training:	Epoch: [19][611/817]	Loss 0.0090 (0.0505)	
training:	Epoch: [19][612/817]	Loss 0.0104 (0.0505)	
training:	Epoch: [19][613/817]	Loss 0.0082 (0.0504)	
training:	Epoch: [19][614/817]	Loss 0.0090 (0.0503)	
training:	Epoch: [19][615/817]	Loss 0.0096 (0.0503)	
training:	Epoch: [19][616/817]	Loss 0.0086 (0.0502)	
training:	Epoch: [19][617/817]	Loss 0.0087 (0.0501)	
training:	Epoch: [19][618/817]	Loss 0.5904 (0.0510)	
training:	Epoch: [19][619/817]	Loss 0.0096 (0.0509)	
training:	Epoch: [19][620/817]	Loss 0.0082 (0.0509)	
training:	Epoch: [19][621/817]	Loss 0.0085 (0.0508)	
training:	Epoch: [19][622/817]	Loss 0.0085 (0.0507)	
training:	Epoch: [19][623/817]	Loss 0.0096 (0.0507)	
training:	Epoch: [19][624/817]	Loss 0.0098 (0.0506)	
training:	Epoch: [19][625/817]	Loss 0.0094 (0.0505)	
training:	Epoch: [19][626/817]	Loss 0.0083 (0.0505)	
training:	Epoch: [19][627/817]	Loss 0.0081 (0.0504)	
training:	Epoch: [19][628/817]	Loss 0.0897 (0.0505)	
training:	Epoch: [19][629/817]	Loss 0.0081 (0.0504)	
training:	Epoch: [19][630/817]	Loss 0.0097 (0.0503)	
training:	Epoch: [19][631/817]	Loss 0.0102 (0.0503)	
training:	Epoch: [19][632/817]	Loss 0.0099 (0.0502)	
training:	Epoch: [19][633/817]	Loss 0.0087 (0.0501)	
training:	Epoch: [19][634/817]	Loss 0.0093 (0.0501)	
training:	Epoch: [19][635/817]	Loss 0.0123 (0.0500)	
training:	Epoch: [19][636/817]	Loss 0.0101 (0.0499)	
training:	Epoch: [19][637/817]	Loss 0.0099 (0.0499)	
training:	Epoch: [19][638/817]	Loss 0.0088 (0.0498)	
training:	Epoch: [19][639/817]	Loss 0.0094 (0.0498)	
training:	Epoch: [19][640/817]	Loss 0.0096 (0.0497)	
training:	Epoch: [19][641/817]	Loss 0.0088 (0.0496)	
training:	Epoch: [19][642/817]	Loss 0.0110 (0.0496)	
training:	Epoch: [19][643/817]	Loss 0.0106 (0.0495)	
training:	Epoch: [19][644/817]	Loss 0.0098 (0.0494)	
training:	Epoch: [19][645/817]	Loss 0.0091 (0.0494)	
training:	Epoch: [19][646/817]	Loss 0.0086 (0.0493)	
training:	Epoch: [19][647/817]	Loss 0.0080 (0.0493)	
training:	Epoch: [19][648/817]	Loss 0.6138 (0.0501)	
training:	Epoch: [19][649/817]	Loss 0.0094 (0.0501)	
training:	Epoch: [19][650/817]	Loss 0.0106 (0.0500)	
training:	Epoch: [19][651/817]	Loss 0.0466 (0.0500)	
training:	Epoch: [19][652/817]	Loss 0.0106 (0.0499)	
training:	Epoch: [19][653/817]	Loss 0.0094 (0.0499)	
training:	Epoch: [19][654/817]	Loss 0.0088 (0.0498)	
training:	Epoch: [19][655/817]	Loss 0.0096 (0.0497)	
training:	Epoch: [19][656/817]	Loss 0.0097 (0.0497)	
training:	Epoch: [19][657/817]	Loss 0.0085 (0.0496)	
training:	Epoch: [19][658/817]	Loss 0.0082 (0.0496)	
training:	Epoch: [19][659/817]	Loss 0.0089 (0.0495)	
training:	Epoch: [19][660/817]	Loss 0.0090 (0.0494)	
training:	Epoch: [19][661/817]	Loss 0.6155 (0.0503)	
training:	Epoch: [19][662/817]	Loss 0.0105 (0.0502)	
training:	Epoch: [19][663/817]	Loss 0.0082 (0.0502)	
training:	Epoch: [19][664/817]	Loss 0.0080 (0.0501)	
training:	Epoch: [19][665/817]	Loss 0.0088 (0.0500)	
training:	Epoch: [19][666/817]	Loss 0.0085 (0.0500)	
training:	Epoch: [19][667/817]	Loss 0.0087 (0.0499)	
training:	Epoch: [19][668/817]	Loss 0.0117 (0.0499)	
training:	Epoch: [19][669/817]	Loss 0.0109 (0.0498)	
training:	Epoch: [19][670/817]	Loss 0.0088 (0.0497)	
training:	Epoch: [19][671/817]	Loss 1.1519 (0.0514)	
training:	Epoch: [19][672/817]	Loss 0.0091 (0.0513)	
training:	Epoch: [19][673/817]	Loss 0.0090 (0.0513)	
training:	Epoch: [19][674/817]	Loss 0.0087 (0.0512)	
training:	Epoch: [19][675/817]	Loss 0.5913 (0.0520)	
training:	Epoch: [19][676/817]	Loss 0.0273 (0.0520)	
training:	Epoch: [19][677/817]	Loss 0.0544 (0.0520)	
training:	Epoch: [19][678/817]	Loss 0.0092 (0.0519)	
training:	Epoch: [19][679/817]	Loss 0.0096 (0.0518)	
training:	Epoch: [19][680/817]	Loss 0.0106 (0.0518)	
training:	Epoch: [19][681/817]	Loss 0.0092 (0.0517)	
training:	Epoch: [19][682/817]	Loss 0.0099 (0.0517)	
training:	Epoch: [19][683/817]	Loss 0.0090 (0.0516)	
training:	Epoch: [19][684/817]	Loss 0.0092 (0.0515)	
training:	Epoch: [19][685/817]	Loss 0.0090 (0.0515)	
training:	Epoch: [19][686/817]	Loss 0.0095 (0.0514)	
training:	Epoch: [19][687/817]	Loss 0.0084 (0.0513)	
training:	Epoch: [19][688/817]	Loss 0.0097 (0.0513)	
training:	Epoch: [19][689/817]	Loss 0.0091 (0.0512)	
training:	Epoch: [19][690/817]	Loss 0.0219 (0.0512)	
training:	Epoch: [19][691/817]	Loss 0.0093 (0.0511)	
training:	Epoch: [19][692/817]	Loss 0.6063 (0.0519)	
training:	Epoch: [19][693/817]	Loss 0.0144 (0.0519)	
training:	Epoch: [19][694/817]	Loss 0.0094 (0.0518)	
training:	Epoch: [19][695/817]	Loss 0.0140 (0.0518)	
training:	Epoch: [19][696/817]	Loss 0.0088 (0.0517)	
training:	Epoch: [19][697/817]	Loss 0.0087 (0.0516)	
training:	Epoch: [19][698/817]	Loss 0.0094 (0.0516)	
training:	Epoch: [19][699/817]	Loss 0.0078 (0.0515)	
training:	Epoch: [19][700/817]	Loss 0.0085 (0.0514)	
training:	Epoch: [19][701/817]	Loss 0.0094 (0.0514)	
training:	Epoch: [19][702/817]	Loss 0.6138 (0.0522)	
training:	Epoch: [19][703/817]	Loss 0.0089 (0.0521)	
training:	Epoch: [19][704/817]	Loss 0.0086 (0.0521)	
training:	Epoch: [19][705/817]	Loss 0.0089 (0.0520)	
training:	Epoch: [19][706/817]	Loss 0.0083 (0.0519)	
training:	Epoch: [19][707/817]	Loss 0.0084 (0.0519)	
training:	Epoch: [19][708/817]	Loss 0.2011 (0.0521)	
training:	Epoch: [19][709/817]	Loss 0.0085 (0.0520)	
training:	Epoch: [19][710/817]	Loss 0.0083 (0.0520)	
training:	Epoch: [19][711/817]	Loss 0.0083 (0.0519)	
training:	Epoch: [19][712/817]	Loss 0.0085 (0.0518)	
training:	Epoch: [19][713/817]	Loss 0.0094 (0.0518)	
training:	Epoch: [19][714/817]	Loss 0.0095 (0.0517)	
training:	Epoch: [19][715/817]	Loss 0.0100 (0.0517)	
training:	Epoch: [19][716/817]	Loss 0.0085 (0.0516)	
training:	Epoch: [19][717/817]	Loss 0.0103 (0.0515)	
training:	Epoch: [19][718/817]	Loss 0.0084 (0.0515)	
training:	Epoch: [19][719/817]	Loss 0.0089 (0.0514)	
training:	Epoch: [19][720/817]	Loss 0.1614 (0.0516)	
training:	Epoch: [19][721/817]	Loss 0.0170 (0.0515)	
training:	Epoch: [19][722/817]	Loss 0.0097 (0.0515)	
training:	Epoch: [19][723/817]	Loss 0.0087 (0.0514)	
training:	Epoch: [19][724/817]	Loss 0.0085 (0.0514)	
training:	Epoch: [19][725/817]	Loss 0.6218 (0.0521)	
training:	Epoch: [19][726/817]	Loss 0.0086 (0.0521)	
training:	Epoch: [19][727/817]	Loss 0.0091 (0.0520)	
training:	Epoch: [19][728/817]	Loss 0.0085 (0.0520)	
training:	Epoch: [19][729/817]	Loss 0.0100 (0.0519)	
training:	Epoch: [19][730/817]	Loss 0.0092 (0.0518)	
training:	Epoch: [19][731/817]	Loss 0.5465 (0.0525)	
training:	Epoch: [19][732/817]	Loss 0.0278 (0.0525)	
training:	Epoch: [19][733/817]	Loss 0.0085 (0.0524)	
training:	Epoch: [19][734/817]	Loss 0.0104 (0.0524)	
training:	Epoch: [19][735/817]	Loss 0.0084 (0.0523)	
training:	Epoch: [19][736/817]	Loss 0.0099 (0.0523)	
training:	Epoch: [19][737/817]	Loss 0.0087 (0.0522)	
training:	Epoch: [19][738/817]	Loss 0.0097 (0.0521)	
training:	Epoch: [19][739/817]	Loss 0.0096 (0.0521)	
training:	Epoch: [19][740/817]	Loss 0.0085 (0.0520)	
training:	Epoch: [19][741/817]	Loss 0.0095 (0.0520)	
training:	Epoch: [19][742/817]	Loss 0.0090 (0.0519)	
training:	Epoch: [19][743/817]	Loss 0.0084 (0.0519)	
training:	Epoch: [19][744/817]	Loss 0.0107 (0.0518)	
training:	Epoch: [19][745/817]	Loss 0.0087 (0.0517)	
training:	Epoch: [19][746/817]	Loss 0.0088 (0.0517)	
training:	Epoch: [19][747/817]	Loss 0.0107 (0.0516)	
training:	Epoch: [19][748/817]	Loss 0.0089 (0.0516)	
training:	Epoch: [19][749/817]	Loss 0.0177 (0.0515)	
training:	Epoch: [19][750/817]	Loss 0.0116 (0.0515)	
training:	Epoch: [19][751/817]	Loss 0.0095 (0.0514)	
training:	Epoch: [19][752/817]	Loss 0.0087 (0.0514)	
training:	Epoch: [19][753/817]	Loss 0.0092 (0.0513)	
training:	Epoch: [19][754/817]	Loss 0.0094 (0.0512)	
training:	Epoch: [19][755/817]	Loss 0.0082 (0.0512)	
training:	Epoch: [19][756/817]	Loss 0.0100 (0.0511)	
training:	Epoch: [19][757/817]	Loss 0.6489 (0.0519)	
training:	Epoch: [19][758/817]	Loss 0.0099 (0.0519)	
training:	Epoch: [19][759/817]	Loss 0.0084 (0.0518)	
training:	Epoch: [19][760/817]	Loss 0.0095 (0.0518)	
training:	Epoch: [19][761/817]	Loss 0.0083 (0.0517)	
training:	Epoch: [19][762/817]	Loss 0.0087 (0.0516)	
training:	Epoch: [19][763/817]	Loss 0.0087 (0.0516)	
training:	Epoch: [19][764/817]	Loss 0.0097 (0.0515)	
training:	Epoch: [19][765/817]	Loss 0.0094 (0.0515)	
training:	Epoch: [19][766/817]	Loss 0.0087 (0.0514)	
training:	Epoch: [19][767/817]	Loss 0.0084 (0.0514)	
training:	Epoch: [19][768/817]	Loss 0.0090 (0.0513)	
training:	Epoch: [19][769/817]	Loss 0.0089 (0.0513)	
training:	Epoch: [19][770/817]	Loss 0.0078 (0.0512)	
training:	Epoch: [19][771/817]	Loss 0.0091 (0.0511)	
training:	Epoch: [19][772/817]	Loss 0.0089 (0.0511)	
training:	Epoch: [19][773/817]	Loss 0.0106 (0.0510)	
training:	Epoch: [19][774/817]	Loss 0.0086 (0.0510)	
training:	Epoch: [19][775/817]	Loss 0.0089 (0.0509)	
training:	Epoch: [19][776/817]	Loss 0.0097 (0.0509)	
training:	Epoch: [19][777/817]	Loss 0.0098 (0.0508)	
training:	Epoch: [19][778/817]	Loss 0.0090 (0.0508)	
training:	Epoch: [19][779/817]	Loss 0.0093 (0.0507)	
training:	Epoch: [19][780/817]	Loss 0.0088 (0.0507)	
training:	Epoch: [19][781/817]	Loss 0.0105 (0.0506)	
training:	Epoch: [19][782/817]	Loss 0.0108 (0.0506)	
training:	Epoch: [19][783/817]	Loss 0.0086 (0.0505)	
training:	Epoch: [19][784/817]	Loss 0.5897 (0.0512)	
training:	Epoch: [19][785/817]	Loss 0.0088 (0.0511)	
training:	Epoch: [19][786/817]	Loss 0.0086 (0.0511)	
training:	Epoch: [19][787/817]	Loss 0.0083 (0.0510)	
training:	Epoch: [19][788/817]	Loss 0.5745 (0.0517)	
training:	Epoch: [19][789/817]	Loss 0.0089 (0.0516)	
training:	Epoch: [19][790/817]	Loss 0.0079 (0.0516)	
training:	Epoch: [19][791/817]	Loss 0.0088 (0.0515)	
training:	Epoch: [19][792/817]	Loss 0.0085 (0.0515)	
training:	Epoch: [19][793/817]	Loss 0.0090 (0.0514)	
training:	Epoch: [19][794/817]	Loss 0.0086 (0.0514)	
training:	Epoch: [19][795/817]	Loss 0.0085 (0.0513)	
training:	Epoch: [19][796/817]	Loss 0.1348 (0.0514)	
training:	Epoch: [19][797/817]	Loss 0.5870 (0.0521)	
training:	Epoch: [19][798/817]	Loss 0.3659 (0.0525)	
training:	Epoch: [19][799/817]	Loss 0.0116 (0.0524)	
training:	Epoch: [19][800/817]	Loss 0.0091 (0.0524)	
training:	Epoch: [19][801/817]	Loss 0.0081 (0.0523)	
training:	Epoch: [19][802/817]	Loss 0.0086 (0.0523)	
training:	Epoch: [19][803/817]	Loss 0.0096 (0.0522)	
training:	Epoch: [19][804/817]	Loss 0.0083 (0.0522)	
training:	Epoch: [19][805/817]	Loss 0.0093 (0.0521)	
training:	Epoch: [19][806/817]	Loss 0.0095 (0.0521)	
training:	Epoch: [19][807/817]	Loss 0.0092 (0.0520)	
training:	Epoch: [19][808/817]	Loss 0.0094 (0.0519)	
training:	Epoch: [19][809/817]	Loss 0.0086 (0.0519)	
training:	Epoch: [19][810/817]	Loss 0.0092 (0.0518)	
training:	Epoch: [19][811/817]	Loss 0.3104 (0.0522)	
training:	Epoch: [19][812/817]	Loss 0.0086 (0.0521)	
training:	Epoch: [19][813/817]	Loss 0.0378 (0.0521)	
training:	Epoch: [19][814/817]	Loss 0.0096 (0.0520)	
training:	Epoch: [19][815/817]	Loss 0.0090 (0.0520)	
training:	Epoch: [19][816/817]	Loss 0.0105 (0.0519)	
training:	Epoch: [19][817/817]	Loss 0.0097 (0.0519)	
Training:	 Loss: 0.0519

Training:	 ACC: 0.9922 0.9922 0.9924 0.9920
Validation:	 ACC: 0.7856 0.7865 0.8066 0.7646
Validation:	 Best_BACC: 0.8223 0.8218 0.8127 0.8318
Validation:	 Loss: 0.8603
Pretraining:	Epoch 20/200
----------
training:	Epoch: [20][1/817]	Loss 0.0088 (0.0088)	
training:	Epoch: [20][2/817]	Loss 0.5751 (0.2919)	
training:	Epoch: [20][3/817]	Loss 0.0083 (0.1974)	
training:	Epoch: [20][4/817]	Loss 0.0091 (0.1503)	
training:	Epoch: [20][5/817]	Loss 0.0092 (0.1221)	
training:	Epoch: [20][6/817]	Loss 0.0104 (0.1035)	
training:	Epoch: [20][7/817]	Loss 0.0094 (0.0900)	
training:	Epoch: [20][8/817]	Loss 0.0109 (0.0802)	
training:	Epoch: [20][9/817]	Loss 0.0107 (0.0724)	
training:	Epoch: [20][10/817]	Loss 0.0085 (0.0660)	
training:	Epoch: [20][11/817]	Loss 0.5419 (0.1093)	
training:	Epoch: [20][12/817]	Loss 0.0099 (0.1010)	
training:	Epoch: [20][13/817]	Loss 0.0084 (0.0939)	
training:	Epoch: [20][14/817]	Loss 0.1122 (0.0952)	
training:	Epoch: [20][15/817]	Loss 0.6259 (0.1306)	
training:	Epoch: [20][16/817]	Loss 0.0220 (0.1238)	
training:	Epoch: [20][17/817]	Loss 0.0093 (0.1171)	
training:	Epoch: [20][18/817]	Loss 0.0105 (0.1111)	
training:	Epoch: [20][19/817]	Loss 0.0119 (0.1059)	
training:	Epoch: [20][20/817]	Loss 0.2619 (0.1137)	
training:	Epoch: [20][21/817]	Loss 0.0102 (0.1088)	
training:	Epoch: [20][22/817]	Loss 0.0087 (0.1042)	
training:	Epoch: [20][23/817]	Loss 0.0102 (0.1001)	
training:	Epoch: [20][24/817]	Loss 0.4715 (0.1156)	
training:	Epoch: [20][25/817]	Loss 0.0116 (0.1115)	
training:	Epoch: [20][26/817]	Loss 0.0104 (0.1076)	
training:	Epoch: [20][27/817]	Loss 0.0087 (0.1039)	
training:	Epoch: [20][28/817]	Loss 0.0112 (0.1006)	
training:	Epoch: [20][29/817]	Loss 0.0092 (0.0974)	
training:	Epoch: [20][30/817]	Loss 0.0117 (0.0946)	
training:	Epoch: [20][31/817]	Loss 0.0093 (0.0918)	
training:	Epoch: [20][32/817]	Loss 0.0120 (0.0893)	
training:	Epoch: [20][33/817]	Loss 0.0091 (0.0869)	
training:	Epoch: [20][34/817]	Loss 0.1306 (0.0882)	
training:	Epoch: [20][35/817]	Loss 0.0094 (0.0859)	
training:	Epoch: [20][36/817]	Loss 0.0113 (0.0839)	
training:	Epoch: [20][37/817]	Loss 0.0091 (0.0818)	
training:	Epoch: [20][38/817]	Loss 0.0091 (0.0799)	
training:	Epoch: [20][39/817]	Loss 0.0082 (0.0781)	
training:	Epoch: [20][40/817]	Loss 0.0126 (0.0765)	
training:	Epoch: [20][41/817]	Loss 0.0089 (0.0748)	
training:	Epoch: [20][42/817]	Loss 0.0164 (0.0734)	
training:	Epoch: [20][43/817]	Loss 0.0465 (0.0728)	
training:	Epoch: [20][44/817]	Loss 0.0092 (0.0713)	
training:	Epoch: [20][45/817]	Loss 0.2615 (0.0756)	
training:	Epoch: [20][46/817]	Loss 0.5140 (0.0851)	
training:	Epoch: [20][47/817]	Loss 0.0109 (0.0835)	
training:	Epoch: [20][48/817]	Loss 0.0097 (0.0820)	
training:	Epoch: [20][49/817]	Loss 0.5111 (0.0907)	
training:	Epoch: [20][50/817]	Loss 0.0109 (0.0891)	
training:	Epoch: [20][51/817]	Loss 0.0098 (0.0876)	
training:	Epoch: [20][52/817]	Loss 0.0099 (0.0861)	
training:	Epoch: [20][53/817]	Loss 0.0792 (0.0860)	
training:	Epoch: [20][54/817]	Loss 0.5384 (0.0943)	
training:	Epoch: [20][55/817]	Loss 0.1862 (0.0960)	
training:	Epoch: [20][56/817]	Loss 0.0098 (0.0945)	
training:	Epoch: [20][57/817]	Loss 0.0151 (0.0931)	
training:	Epoch: [20][58/817]	Loss 0.6009 (0.1018)	
training:	Epoch: [20][59/817]	Loss 0.0159 (0.1004)	
training:	Epoch: [20][60/817]	Loss 0.0170 (0.0990)	
training:	Epoch: [20][61/817]	Loss 0.0128 (0.0976)	
training:	Epoch: [20][62/817]	Loss 0.0100 (0.0962)	
training:	Epoch: [20][63/817]	Loss 0.0112 (0.0948)	
training:	Epoch: [20][64/817]	Loss 0.0094 (0.0935)	
training:	Epoch: [20][65/817]	Loss 0.0092 (0.0922)	
training:	Epoch: [20][66/817]	Loss 0.0148 (0.0910)	
training:	Epoch: [20][67/817]	Loss 0.0539 (0.0905)	
training:	Epoch: [20][68/817]	Loss 0.0118 (0.0893)	
training:	Epoch: [20][69/817]	Loss 0.0102 (0.0882)	
training:	Epoch: [20][70/817]	Loss 0.0104 (0.0871)	
training:	Epoch: [20][71/817]	Loss 0.0144 (0.0860)	
training:	Epoch: [20][72/817]	Loss 0.0102 (0.0850)	
training:	Epoch: [20][73/817]	Loss 0.0098 (0.0839)	
training:	Epoch: [20][74/817]	Loss 0.0333 (0.0833)	
training:	Epoch: [20][75/817]	Loss 0.0086 (0.0823)	
training:	Epoch: [20][76/817]	Loss 0.0107 (0.0813)	
training:	Epoch: [20][77/817]	Loss 0.0372 (0.0807)	
training:	Epoch: [20][78/817]	Loss 0.0122 (0.0799)	
training:	Epoch: [20][79/817]	Loss 0.0120 (0.0790)	
training:	Epoch: [20][80/817]	Loss 0.0097 (0.0781)	
training:	Epoch: [20][81/817]	Loss 0.0099 (0.0773)	
training:	Epoch: [20][82/817]	Loss 0.0150 (0.0765)	
training:	Epoch: [20][83/817]	Loss 0.0459 (0.0762)	
training:	Epoch: [20][84/817]	Loss 0.0094 (0.0754)	
training:	Epoch: [20][85/817]	Loss 0.0092 (0.0746)	
training:	Epoch: [20][86/817]	Loss 0.0221 (0.0740)	
training:	Epoch: [20][87/817]	Loss 0.0130 (0.0733)	
training:	Epoch: [20][88/817]	Loss 0.0087 (0.0726)	
training:	Epoch: [20][89/817]	Loss 0.0117 (0.0719)	
training:	Epoch: [20][90/817]	Loss 0.0090 (0.0712)	
training:	Epoch: [20][91/817]	Loss 0.0100 (0.0705)	
training:	Epoch: [20][92/817]	Loss 0.0132 (0.0699)	
training:	Epoch: [20][93/817]	Loss 0.0112 (0.0692)	
training:	Epoch: [20][94/817]	Loss 0.0092 (0.0686)	
training:	Epoch: [20][95/817]	Loss 0.0144 (0.0680)	
training:	Epoch: [20][96/817]	Loss 0.0090 (0.0674)	
training:	Epoch: [20][97/817]	Loss 0.0100 (0.0668)	
training:	Epoch: [20][98/817]	Loss 0.6177 (0.0725)	
training:	Epoch: [20][99/817]	Loss 0.0094 (0.0718)	
training:	Epoch: [20][100/817]	Loss 0.0149 (0.0712)	
training:	Epoch: [20][101/817]	Loss 0.0104 (0.0706)	
training:	Epoch: [20][102/817]	Loss 0.0149 (0.0701)	
training:	Epoch: [20][103/817]	Loss 0.0098 (0.0695)	
training:	Epoch: [20][104/817]	Loss 0.3740 (0.0724)	
training:	Epoch: [20][105/817]	Loss 0.0603 (0.0723)	
training:	Epoch: [20][106/817]	Loss 0.0138 (0.0718)	
training:	Epoch: [20][107/817]	Loss 0.0092 (0.0712)	
training:	Epoch: [20][108/817]	Loss 0.0101 (0.0706)	
training:	Epoch: [20][109/817]	Loss 0.0093 (0.0701)	
training:	Epoch: [20][110/817]	Loss 0.0102 (0.0695)	
training:	Epoch: [20][111/817]	Loss 0.0092 (0.0690)	
training:	Epoch: [20][112/817]	Loss 0.0098 (0.0684)	
training:	Epoch: [20][113/817]	Loss 0.0151 (0.0680)	
training:	Epoch: [20][114/817]	Loss 0.0103 (0.0675)	
training:	Epoch: [20][115/817]	Loss 0.0094 (0.0670)	
training:	Epoch: [20][116/817]	Loss 0.0096 (0.0665)	
training:	Epoch: [20][117/817]	Loss 0.0188 (0.0661)	
training:	Epoch: [20][118/817]	Loss 0.0126 (0.0656)	
training:	Epoch: [20][119/817]	Loss 0.0098 (0.0651)	
training:	Epoch: [20][120/817]	Loss 0.5801 (0.0694)	
training:	Epoch: [20][121/817]	Loss 0.0085 (0.0689)	
training:	Epoch: [20][122/817]	Loss 0.0102 (0.0684)	
training:	Epoch: [20][123/817]	Loss 0.0090 (0.0680)	
training:	Epoch: [20][124/817]	Loss 0.5024 (0.0715)	
training:	Epoch: [20][125/817]	Loss 0.0094 (0.0710)	
training:	Epoch: [20][126/817]	Loss 0.0097 (0.0705)	
training:	Epoch: [20][127/817]	Loss 0.0110 (0.0700)	
training:	Epoch: [20][128/817]	Loss 0.0098 (0.0695)	
training:	Epoch: [20][129/817]	Loss 0.0150 (0.0691)	
training:	Epoch: [20][130/817]	Loss 0.6070 (0.0733)	
training:	Epoch: [20][131/817]	Loss 0.0101 (0.0728)	
training:	Epoch: [20][132/817]	Loss 0.0128 (0.0723)	
training:	Epoch: [20][133/817]	Loss 0.0083 (0.0718)	
training:	Epoch: [20][134/817]	Loss 0.0150 (0.0714)	
training:	Epoch: [20][135/817]	Loss 0.0209 (0.0710)	
training:	Epoch: [20][136/817]	Loss 0.0101 (0.0706)	
training:	Epoch: [20][137/817]	Loss 0.0095 (0.0701)	
training:	Epoch: [20][138/817]	Loss 0.0104 (0.0697)	
training:	Epoch: [20][139/817]	Loss 0.0098 (0.0693)	
training:	Epoch: [20][140/817]	Loss 0.0100 (0.0689)	
training:	Epoch: [20][141/817]	Loss 0.0098 (0.0684)	
training:	Epoch: [20][142/817]	Loss 0.0114 (0.0680)	
training:	Epoch: [20][143/817]	Loss 0.0140 (0.0677)	
training:	Epoch: [20][144/817]	Loss 0.0120 (0.0673)	
training:	Epoch: [20][145/817]	Loss 0.0112 (0.0669)	
training:	Epoch: [20][146/817]	Loss 0.5683 (0.0703)	
training:	Epoch: [20][147/817]	Loss 0.0089 (0.0699)	
training:	Epoch: [20][148/817]	Loss 0.0092 (0.0695)	
training:	Epoch: [20][149/817]	Loss 0.0092 (0.0691)	
training:	Epoch: [20][150/817]	Loss 0.0087 (0.0687)	
training:	Epoch: [20][151/817]	Loss 0.0091 (0.0683)	
training:	Epoch: [20][152/817]	Loss 0.0083 (0.0679)	
training:	Epoch: [20][153/817]	Loss 0.0091 (0.0675)	
training:	Epoch: [20][154/817]	Loss 0.0090 (0.0671)	
training:	Epoch: [20][155/817]	Loss 0.0084 (0.0668)	
training:	Epoch: [20][156/817]	Loss 0.0117 (0.0664)	
training:	Epoch: [20][157/817]	Loss 0.0442 (0.0663)	
training:	Epoch: [20][158/817]	Loss 0.0125 (0.0659)	
training:	Epoch: [20][159/817]	Loss 0.0113 (0.0656)	
training:	Epoch: [20][160/817]	Loss 0.0093 (0.0652)	
training:	Epoch: [20][161/817]	Loss 0.1793 (0.0659)	
training:	Epoch: [20][162/817]	Loss 0.0160 (0.0656)	
training:	Epoch: [20][163/817]	Loss 0.0127 (0.0653)	
training:	Epoch: [20][164/817]	Loss 0.0101 (0.0650)	
training:	Epoch: [20][165/817]	Loss 0.0100 (0.0646)	
training:	Epoch: [20][166/817]	Loss 0.0120 (0.0643)	
training:	Epoch: [20][167/817]	Loss 0.0110 (0.0640)	
training:	Epoch: [20][168/817]	Loss 0.0079 (0.0637)	
training:	Epoch: [20][169/817]	Loss 0.0134 (0.0634)	
training:	Epoch: [20][170/817]	Loss 0.0092 (0.0630)	
training:	Epoch: [20][171/817]	Loss 0.0133 (0.0628)	
training:	Epoch: [20][172/817]	Loss 0.0106 (0.0624)	
training:	Epoch: [20][173/817]	Loss 0.0601 (0.0624)	
training:	Epoch: [20][174/817]	Loss 0.0197 (0.0622)	
training:	Epoch: [20][175/817]	Loss 0.0081 (0.0619)	
training:	Epoch: [20][176/817]	Loss 0.0089 (0.0616)	
training:	Epoch: [20][177/817]	Loss 0.0186 (0.0613)	
training:	Epoch: [20][178/817]	Loss 0.0081 (0.0610)	
training:	Epoch: [20][179/817]	Loss 0.0170 (0.0608)	
training:	Epoch: [20][180/817]	Loss 0.0086 (0.0605)	
training:	Epoch: [20][181/817]	Loss 0.0110 (0.0602)	
training:	Epoch: [20][182/817]	Loss 0.0102 (0.0600)	
training:	Epoch: [20][183/817]	Loss 0.0092 (0.0597)	
training:	Epoch: [20][184/817]	Loss 0.6118 (0.0627)	
training:	Epoch: [20][185/817]	Loss 0.5602 (0.0654)	
training:	Epoch: [20][186/817]	Loss 0.0113 (0.0651)	
training:	Epoch: [20][187/817]	Loss 0.5647 (0.0677)	
training:	Epoch: [20][188/817]	Loss 0.0102 (0.0674)	
training:	Epoch: [20][189/817]	Loss 0.0092 (0.0671)	
training:	Epoch: [20][190/817]	Loss 0.0127 (0.0668)	
training:	Epoch: [20][191/817]	Loss 0.0105 (0.0666)	
training:	Epoch: [20][192/817]	Loss 0.5666 (0.0692)	
training:	Epoch: [20][193/817]	Loss 0.0104 (0.0689)	
training:	Epoch: [20][194/817]	Loss 0.6100 (0.0716)	
training:	Epoch: [20][195/817]	Loss 0.0100 (0.0713)	
training:	Epoch: [20][196/817]	Loss 0.0093 (0.0710)	
training:	Epoch: [20][197/817]	Loss 0.0091 (0.0707)	
training:	Epoch: [20][198/817]	Loss 0.0099 (0.0704)	
training:	Epoch: [20][199/817]	Loss 0.0141 (0.0701)	
training:	Epoch: [20][200/817]	Loss 0.0082 (0.0698)	
training:	Epoch: [20][201/817]	Loss 0.0122 (0.0695)	
training:	Epoch: [20][202/817]	Loss 0.0089 (0.0692)	
training:	Epoch: [20][203/817]	Loss 0.0088 (0.0689)	
training:	Epoch: [20][204/817]	Loss 0.0102 (0.0686)	
training:	Epoch: [20][205/817]	Loss 0.0107 (0.0683)	
training:	Epoch: [20][206/817]	Loss 0.0084 (0.0680)	
training:	Epoch: [20][207/817]	Loss 0.0096 (0.0678)	
training:	Epoch: [20][208/817]	Loss 0.0088 (0.0675)	
training:	Epoch: [20][209/817]	Loss 0.0096 (0.0672)	
training:	Epoch: [20][210/817]	Loss 0.0146 (0.0670)	
training:	Epoch: [20][211/817]	Loss 0.0102 (0.0667)	
training:	Epoch: [20][212/817]	Loss 0.0084 (0.0664)	
training:	Epoch: [20][213/817]	Loss 0.0663 (0.0664)	
training:	Epoch: [20][214/817]	Loss 0.0117 (0.0662)	
training:	Epoch: [20][215/817]	Loss 0.0087 (0.0659)	
training:	Epoch: [20][216/817]	Loss 0.0104 (0.0656)	
training:	Epoch: [20][217/817]	Loss 0.4835 (0.0676)	
training:	Epoch: [20][218/817]	Loss 0.5658 (0.0698)	
training:	Epoch: [20][219/817]	Loss 0.0117 (0.0696)	
training:	Epoch: [20][220/817]	Loss 0.0114 (0.0693)	
training:	Epoch: [20][221/817]	Loss 0.0091 (0.0690)	
training:	Epoch: [20][222/817]	Loss 0.0085 (0.0688)	
training:	Epoch: [20][223/817]	Loss 0.0103 (0.0685)	
training:	Epoch: [20][224/817]	Loss 0.0111 (0.0682)	
training:	Epoch: [20][225/817]	Loss 0.0095 (0.0680)	
training:	Epoch: [20][226/817]	Loss 0.0103 (0.0677)	
training:	Epoch: [20][227/817]	Loss 0.0094 (0.0675)	
training:	Epoch: [20][228/817]	Loss 0.0100 (0.0672)	
training:	Epoch: [20][229/817]	Loss 0.0100 (0.0670)	
training:	Epoch: [20][230/817]	Loss 0.0097 (0.0667)	
training:	Epoch: [20][231/817]	Loss 0.0110 (0.0665)	
training:	Epoch: [20][232/817]	Loss 0.0127 (0.0663)	
training:	Epoch: [20][233/817]	Loss 0.3190 (0.0673)	
training:	Epoch: [20][234/817]	Loss 0.0107 (0.0671)	
training:	Epoch: [20][235/817]	Loss 0.0148 (0.0669)	
training:	Epoch: [20][236/817]	Loss 0.0100 (0.0666)	
training:	Epoch: [20][237/817]	Loss 0.0078 (0.0664)	
training:	Epoch: [20][238/817]	Loss 0.0108 (0.0661)	
training:	Epoch: [20][239/817]	Loss 0.0107 (0.0659)	
training:	Epoch: [20][240/817]	Loss 0.0089 (0.0657)	
training:	Epoch: [20][241/817]	Loss 0.0109 (0.0655)	
training:	Epoch: [20][242/817]	Loss 0.0099 (0.0652)	
training:	Epoch: [20][243/817]	Loss 0.0227 (0.0650)	
training:	Epoch: [20][244/817]	Loss 0.0096 (0.0648)	
training:	Epoch: [20][245/817]	Loss 0.0137 (0.0646)	
training:	Epoch: [20][246/817]	Loss 0.0096 (0.0644)	
training:	Epoch: [20][247/817]	Loss 0.0098 (0.0642)	
training:	Epoch: [20][248/817]	Loss 0.6278 (0.0664)	
training:	Epoch: [20][249/817]	Loss 0.0096 (0.0662)	
training:	Epoch: [20][250/817]	Loss 0.0097 (0.0660)	
training:	Epoch: [20][251/817]	Loss 0.0091 (0.0658)	
training:	Epoch: [20][252/817]	Loss 0.0127 (0.0655)	
training:	Epoch: [20][253/817]	Loss 0.0105 (0.0653)	
training:	Epoch: [20][254/817]	Loss 0.0111 (0.0651)	
training:	Epoch: [20][255/817]	Loss 0.0096 (0.0649)	
training:	Epoch: [20][256/817]	Loss 0.0094 (0.0647)	
training:	Epoch: [20][257/817]	Loss 0.0094 (0.0645)	
training:	Epoch: [20][258/817]	Loss 0.0098 (0.0643)	
training:	Epoch: [20][259/817]	Loss 0.0117 (0.0641)	
training:	Epoch: [20][260/817]	Loss 0.0098 (0.0638)	
training:	Epoch: [20][261/817]	Loss 0.0165 (0.0637)	
training:	Epoch: [20][262/817]	Loss 0.0081 (0.0635)	
training:	Epoch: [20][263/817]	Loss 0.0136 (0.0633)	
training:	Epoch: [20][264/817]	Loss 0.5681 (0.0652)	
training:	Epoch: [20][265/817]	Loss 0.0095 (0.0650)	
training:	Epoch: [20][266/817]	Loss 0.0110 (0.0648)	
training:	Epoch: [20][267/817]	Loss 0.0094 (0.0646)	
training:	Epoch: [20][268/817]	Loss 0.0108 (0.0644)	
training:	Epoch: [20][269/817]	Loss 0.0087 (0.0641)	
training:	Epoch: [20][270/817]	Loss 0.0106 (0.0639)	
training:	Epoch: [20][271/817]	Loss 0.0087 (0.0637)	
training:	Epoch: [20][272/817]	Loss 0.0095 (0.0635)	
training:	Epoch: [20][273/817]	Loss 0.0091 (0.0633)	
training:	Epoch: [20][274/817]	Loss 0.0097 (0.0631)	
training:	Epoch: [20][275/817]	Loss 0.0097 (0.0630)	
training:	Epoch: [20][276/817]	Loss 0.0100 (0.0628)	
training:	Epoch: [20][277/817]	Loss 0.0094 (0.0626)	
training:	Epoch: [20][278/817]	Loss 0.0090 (0.0624)	
training:	Epoch: [20][279/817]	Loss 0.0095 (0.0622)	
training:	Epoch: [20][280/817]	Loss 0.0113 (0.0620)	
training:	Epoch: [20][281/817]	Loss 0.0098 (0.0618)	
training:	Epoch: [20][282/817]	Loss 0.0091 (0.0616)	
training:	Epoch: [20][283/817]	Loss 0.0105 (0.0615)	
training:	Epoch: [20][284/817]	Loss 0.0088 (0.0613)	
training:	Epoch: [20][285/817]	Loss 0.5719 (0.0631)	
training:	Epoch: [20][286/817]	Loss 0.0109 (0.0629)	
training:	Epoch: [20][287/817]	Loss 0.0081 (0.0627)	
training:	Epoch: [20][288/817]	Loss 0.0092 (0.0625)	
training:	Epoch: [20][289/817]	Loss 0.0089 (0.0623)	
training:	Epoch: [20][290/817]	Loss 0.0091 (0.0621)	
training:	Epoch: [20][291/817]	Loss 0.0108 (0.0620)	
training:	Epoch: [20][292/817]	Loss 0.0100 (0.0618)	
training:	Epoch: [20][293/817]	Loss 0.0093 (0.0616)	
training:	Epoch: [20][294/817]	Loss 0.0098 (0.0614)	
training:	Epoch: [20][295/817]	Loss 0.0081 (0.0612)	
training:	Epoch: [20][296/817]	Loss 0.0101 (0.0611)	
training:	Epoch: [20][297/817]	Loss 0.0084 (0.0609)	
training:	Epoch: [20][298/817]	Loss 0.0089 (0.0607)	
training:	Epoch: [20][299/817]	Loss 0.0117 (0.0606)	
training:	Epoch: [20][300/817]	Loss 0.0097 (0.0604)	
training:	Epoch: [20][301/817]	Loss 0.0087 (0.0602)	
training:	Epoch: [20][302/817]	Loss 0.0226 (0.0601)	
training:	Epoch: [20][303/817]	Loss 0.0082 (0.0599)	
training:	Epoch: [20][304/817]	Loss 0.0084 (0.0597)	
training:	Epoch: [20][305/817]	Loss 0.0084 (0.0596)	
training:	Epoch: [20][306/817]	Loss 0.0134 (0.0594)	
training:	Epoch: [20][307/817]	Loss 0.0105 (0.0593)	
training:	Epoch: [20][308/817]	Loss 0.0111 (0.0591)	
training:	Epoch: [20][309/817]	Loss 0.0283 (0.0590)	
training:	Epoch: [20][310/817]	Loss 0.0084 (0.0588)	
training:	Epoch: [20][311/817]	Loss 0.0092 (0.0587)	
training:	Epoch: [20][312/817]	Loss 0.0122 (0.0585)	
training:	Epoch: [20][313/817]	Loss 0.0108 (0.0584)	
training:	Epoch: [20][314/817]	Loss 0.0102 (0.0582)	
training:	Epoch: [20][315/817]	Loss 0.0215 (0.0581)	
training:	Epoch: [20][316/817]	Loss 0.6311 (0.0599)	
training:	Epoch: [20][317/817]	Loss 0.0092 (0.0598)	
training:	Epoch: [20][318/817]	Loss 0.0092 (0.0596)	
training:	Epoch: [20][319/817]	Loss 0.5790 (0.0612)	
training:	Epoch: [20][320/817]	Loss 0.0084 (0.0611)	
training:	Epoch: [20][321/817]	Loss 0.0111 (0.0609)	
training:	Epoch: [20][322/817]	Loss 0.0088 (0.0608)	
training:	Epoch: [20][323/817]	Loss 0.0106 (0.0606)	
training:	Epoch: [20][324/817]	Loss 0.0094 (0.0604)	
training:	Epoch: [20][325/817]	Loss 0.0105 (0.0603)	
training:	Epoch: [20][326/817]	Loss 0.0104 (0.0601)	
training:	Epoch: [20][327/817]	Loss 0.0109 (0.0600)	
training:	Epoch: [20][328/817]	Loss 0.0138 (0.0598)	
training:	Epoch: [20][329/817]	Loss 0.0095 (0.0597)	
training:	Epoch: [20][330/817]	Loss 0.0098 (0.0595)	
training:	Epoch: [20][331/817]	Loss 0.5668 (0.0611)	
training:	Epoch: [20][332/817]	Loss 0.0102 (0.0609)	
training:	Epoch: [20][333/817]	Loss 0.0093 (0.0608)	
training:	Epoch: [20][334/817]	Loss 0.0081 (0.0606)	
training:	Epoch: [20][335/817]	Loss 0.0105 (0.0605)	
training:	Epoch: [20][336/817]	Loss 0.0112 (0.0603)	
training:	Epoch: [20][337/817]	Loss 0.0085 (0.0602)	
training:	Epoch: [20][338/817]	Loss 0.0091 (0.0600)	
training:	Epoch: [20][339/817]	Loss 0.0104 (0.0599)	
training:	Epoch: [20][340/817]	Loss 0.0088 (0.0597)	
training:	Epoch: [20][341/817]	Loss 0.0103 (0.0596)	
training:	Epoch: [20][342/817]	Loss 0.0095 (0.0594)	
training:	Epoch: [20][343/817]	Loss 0.0092 (0.0593)	
training:	Epoch: [20][344/817]	Loss 0.0098 (0.0591)	
training:	Epoch: [20][345/817]	Loss 0.0205 (0.0590)	
training:	Epoch: [20][346/817]	Loss 0.0077 (0.0589)	
training:	Epoch: [20][347/817]	Loss 0.0114 (0.0587)	
training:	Epoch: [20][348/817]	Loss 0.0095 (0.0586)	
training:	Epoch: [20][349/817]	Loss 0.0101 (0.0585)	
training:	Epoch: [20][350/817]	Loss 0.0662 (0.0585)	
training:	Epoch: [20][351/817]	Loss 0.0092 (0.0583)	
training:	Epoch: [20][352/817]	Loss 0.0097 (0.0582)	
training:	Epoch: [20][353/817]	Loss 0.0103 (0.0581)	
training:	Epoch: [20][354/817]	Loss 0.0088 (0.0579)	
training:	Epoch: [20][355/817]	Loss 0.0098 (0.0578)	
training:	Epoch: [20][356/817]	Loss 0.0224 (0.0577)	
training:	Epoch: [20][357/817]	Loss 0.9236 (0.0601)	
training:	Epoch: [20][358/817]	Loss 0.2623 (0.0607)	
training:	Epoch: [20][359/817]	Loss 0.0085 (0.0605)	
training:	Epoch: [20][360/817]	Loss 0.0097 (0.0604)	
training:	Epoch: [20][361/817]	Loss 0.0102 (0.0602)	
training:	Epoch: [20][362/817]	Loss 0.0118 (0.0601)	
training:	Epoch: [20][363/817]	Loss 0.0143 (0.0600)	
training:	Epoch: [20][364/817]	Loss 0.0116 (0.0599)	
training:	Epoch: [20][365/817]	Loss 0.0103 (0.0597)	
training:	Epoch: [20][366/817]	Loss 0.0108 (0.0596)	
training:	Epoch: [20][367/817]	Loss 0.0180 (0.0595)	
training:	Epoch: [20][368/817]	Loss 0.0222 (0.0594)	
training:	Epoch: [20][369/817]	Loss 0.0092 (0.0592)	
training:	Epoch: [20][370/817]	Loss 0.0115 (0.0591)	
training:	Epoch: [20][371/817]	Loss 0.0094 (0.0590)	
training:	Epoch: [20][372/817]	Loss 0.0087 (0.0588)	
training:	Epoch: [20][373/817]	Loss 0.0088 (0.0587)	
training:	Epoch: [20][374/817]	Loss 0.0087 (0.0586)	
training:	Epoch: [20][375/817]	Loss 0.0119 (0.0584)	
training:	Epoch: [20][376/817]	Loss 0.0233 (0.0584)	
training:	Epoch: [20][377/817]	Loss 0.0099 (0.0582)	
training:	Epoch: [20][378/817]	Loss 0.0300 (0.0581)	
training:	Epoch: [20][379/817]	Loss 0.0089 (0.0580)	
training:	Epoch: [20][380/817]	Loss 0.5906 (0.0594)	
training:	Epoch: [20][381/817]	Loss 0.6197 (0.0609)	
training:	Epoch: [20][382/817]	Loss 0.6139 (0.0623)	
training:	Epoch: [20][383/817]	Loss 0.0100 (0.0622)	
training:	Epoch: [20][384/817]	Loss 0.0098 (0.0621)	
training:	Epoch: [20][385/817]	Loss 0.0097 (0.0619)	
training:	Epoch: [20][386/817]	Loss 0.0095 (0.0618)	
training:	Epoch: [20][387/817]	Loss 0.0087 (0.0617)	
training:	Epoch: [20][388/817]	Loss 0.0133 (0.0615)	
training:	Epoch: [20][389/817]	Loss 0.0183 (0.0614)	
training:	Epoch: [20][390/817]	Loss 0.0110 (0.0613)	
training:	Epoch: [20][391/817]	Loss 0.0089 (0.0612)	
training:	Epoch: [20][392/817]	Loss 0.0091 (0.0610)	
training:	Epoch: [20][393/817]	Loss 0.0087 (0.0609)	
training:	Epoch: [20][394/817]	Loss 0.0085 (0.0608)	
training:	Epoch: [20][395/817]	Loss 0.0563 (0.0607)	
training:	Epoch: [20][396/817]	Loss 0.0339 (0.0607)	
training:	Epoch: [20][397/817]	Loss 0.0329 (0.0606)	
training:	Epoch: [20][398/817]	Loss 0.0166 (0.0605)	
training:	Epoch: [20][399/817]	Loss 0.0106 (0.0604)	
training:	Epoch: [20][400/817]	Loss 0.0116 (0.0603)	
training:	Epoch: [20][401/817]	Loss 0.0091 (0.0601)	
training:	Epoch: [20][402/817]	Loss 0.0084 (0.0600)	
training:	Epoch: [20][403/817]	Loss 0.0087 (0.0599)	
training:	Epoch: [20][404/817]	Loss 0.0085 (0.0597)	
training:	Epoch: [20][405/817]	Loss 0.0079 (0.0596)	
training:	Epoch: [20][406/817]	Loss 0.0096 (0.0595)	
training:	Epoch: [20][407/817]	Loss 0.0108 (0.0594)	
training:	Epoch: [20][408/817]	Loss 0.1230 (0.0595)	
training:	Epoch: [20][409/817]	Loss 0.0100 (0.0594)	
training:	Epoch: [20][410/817]	Loss 0.0086 (0.0593)	
training:	Epoch: [20][411/817]	Loss 0.0095 (0.0592)	
training:	Epoch: [20][412/817]	Loss 0.6094 (0.0605)	
training:	Epoch: [20][413/817]	Loss 0.0114 (0.0604)	
training:	Epoch: [20][414/817]	Loss 0.0088 (0.0603)	
training:	Epoch: [20][415/817]	Loss 0.0092 (0.0601)	
training:	Epoch: [20][416/817]	Loss 0.0111 (0.0600)	
training:	Epoch: [20][417/817]	Loss 0.0114 (0.0599)	
training:	Epoch: [20][418/817]	Loss 0.6057 (0.0612)	
training:	Epoch: [20][419/817]	Loss 0.0094 (0.0611)	
training:	Epoch: [20][420/817]	Loss 0.5603 (0.0623)	
training:	Epoch: [20][421/817]	Loss 0.0176 (0.0622)	
training:	Epoch: [20][422/817]	Loss 0.0127 (0.0620)	
training:	Epoch: [20][423/817]	Loss 0.0106 (0.0619)	
training:	Epoch: [20][424/817]	Loss 0.0141 (0.0618)	
training:	Epoch: [20][425/817]	Loss 0.0102 (0.0617)	
training:	Epoch: [20][426/817]	Loss 0.0097 (0.0616)	
training:	Epoch: [20][427/817]	Loss 0.0096 (0.0614)	
training:	Epoch: [20][428/817]	Loss 0.0115 (0.0613)	
training:	Epoch: [20][429/817]	Loss 0.0101 (0.0612)	
training:	Epoch: [20][430/817]	Loss 0.0146 (0.0611)	
training:	Epoch: [20][431/817]	Loss 0.0103 (0.0610)	
training:	Epoch: [20][432/817]	Loss 0.0245 (0.0609)	
training:	Epoch: [20][433/817]	Loss 0.0087 (0.0608)	
training:	Epoch: [20][434/817]	Loss 0.0103 (0.0607)	
training:	Epoch: [20][435/817]	Loss 0.0118 (0.0605)	
training:	Epoch: [20][436/817]	Loss 0.0100 (0.0604)	
training:	Epoch: [20][437/817]	Loss 0.0710 (0.0605)	
training:	Epoch: [20][438/817]	Loss 0.6058 (0.0617)	
training:	Epoch: [20][439/817]	Loss 0.0108 (0.0616)	
training:	Epoch: [20][440/817]	Loss 0.0099 (0.0615)	
training:	Epoch: [20][441/817]	Loss 0.0087 (0.0613)	
training:	Epoch: [20][442/817]	Loss 0.0087 (0.0612)	
training:	Epoch: [20][443/817]	Loss 0.0094 (0.0611)	
training:	Epoch: [20][444/817]	Loss 0.0090 (0.0610)	
training:	Epoch: [20][445/817]	Loss 0.0087 (0.0609)	
training:	Epoch: [20][446/817]	Loss 0.0099 (0.0608)	
training:	Epoch: [20][447/817]	Loss 0.0089 (0.0606)	
training:	Epoch: [20][448/817]	Loss 0.0102 (0.0605)	
training:	Epoch: [20][449/817]	Loss 0.0105 (0.0604)	
training:	Epoch: [20][450/817]	Loss 0.0238 (0.0603)	
training:	Epoch: [20][451/817]	Loss 0.0086 (0.0602)	
training:	Epoch: [20][452/817]	Loss 0.0124 (0.0601)	
training:	Epoch: [20][453/817]	Loss 0.0108 (0.0600)	
training:	Epoch: [20][454/817]	Loss 0.0334 (0.0600)	
training:	Epoch: [20][455/817]	Loss 0.0091 (0.0598)	
training:	Epoch: [20][456/817]	Loss 0.0092 (0.0597)	
training:	Epoch: [20][457/817]	Loss 0.0162 (0.0596)	
training:	Epoch: [20][458/817]	Loss 0.0091 (0.0595)	
training:	Epoch: [20][459/817]	Loss 0.0099 (0.0594)	
training:	Epoch: [20][460/817]	Loss 0.0097 (0.0593)	
training:	Epoch: [20][461/817]	Loss 0.0100 (0.0592)	
training:	Epoch: [20][462/817]	Loss 0.0178 (0.0591)	
training:	Epoch: [20][463/817]	Loss 0.0094 (0.0590)	
training:	Epoch: [20][464/817]	Loss 0.0091 (0.0589)	
training:	Epoch: [20][465/817]	Loss 0.0096 (0.0588)	
training:	Epoch: [20][466/817]	Loss 0.0097 (0.0587)	
training:	Epoch: [20][467/817]	Loss 0.0093 (0.0586)	
training:	Epoch: [20][468/817]	Loss 0.0090 (0.0585)	
training:	Epoch: [20][469/817]	Loss 0.0096 (0.0584)	
training:	Epoch: [20][470/817]	Loss 0.0086 (0.0583)	
training:	Epoch: [20][471/817]	Loss 0.0091 (0.0582)	
training:	Epoch: [20][472/817]	Loss 0.0089 (0.0581)	
training:	Epoch: [20][473/817]	Loss 0.0102 (0.0580)	
training:	Epoch: [20][474/817]	Loss 0.0091 (0.0579)	
training:	Epoch: [20][475/817]	Loss 0.5685 (0.0589)	
training:	Epoch: [20][476/817]	Loss 0.0091 (0.0588)	
training:	Epoch: [20][477/817]	Loss 0.0100 (0.0587)	
training:	Epoch: [20][478/817]	Loss 0.0088 (0.0586)	
training:	Epoch: [20][479/817]	Loss 0.2059 (0.0589)	
training:	Epoch: [20][480/817]	Loss 0.0108 (0.0588)	
training:	Epoch: [20][481/817]	Loss 0.6189 (0.0600)	
training:	Epoch: [20][482/817]	Loss 0.0087 (0.0599)	
training:	Epoch: [20][483/817]	Loss 0.0091 (0.0598)	
training:	Epoch: [20][484/817]	Loss 0.0119 (0.0597)	
training:	Epoch: [20][485/817]	Loss 0.0170 (0.0596)	
training:	Epoch: [20][486/817]	Loss 0.0085 (0.0595)	
training:	Epoch: [20][487/817]	Loss 0.0093 (0.0594)	
training:	Epoch: [20][488/817]	Loss 0.0086 (0.0593)	
training:	Epoch: [20][489/817]	Loss 0.0091 (0.0592)	
training:	Epoch: [20][490/817]	Loss 0.0115 (0.0591)	
training:	Epoch: [20][491/817]	Loss 0.0087 (0.0590)	
training:	Epoch: [20][492/817]	Loss 0.0084 (0.0589)	
training:	Epoch: [20][493/817]	Loss 0.0097 (0.0588)	
training:	Epoch: [20][494/817]	Loss 0.0090 (0.0587)	
training:	Epoch: [20][495/817]	Loss 0.0092 (0.0586)	
training:	Epoch: [20][496/817]	Loss 0.0091 (0.0585)	
training:	Epoch: [20][497/817]	Loss 0.0256 (0.0584)	
training:	Epoch: [20][498/817]	Loss 0.0088 (0.0583)	
training:	Epoch: [20][499/817]	Loss 0.0106 (0.0582)	
training:	Epoch: [20][500/817]	Loss 0.6028 (0.0593)	
training:	Epoch: [20][501/817]	Loss 0.0086 (0.0592)	
training:	Epoch: [20][502/817]	Loss 0.0097 (0.0591)	
training:	Epoch: [20][503/817]	Loss 0.0144 (0.0590)	
training:	Epoch: [20][504/817]	Loss 0.0182 (0.0589)	
training:	Epoch: [20][505/817]	Loss 0.0120 (0.0588)	
training:	Epoch: [20][506/817]	Loss 0.0137 (0.0587)	
training:	Epoch: [20][507/817]	Loss 0.0090 (0.0586)	
training:	Epoch: [20][508/817]	Loss 0.0086 (0.0585)	
training:	Epoch: [20][509/817]	Loss 0.0086 (0.0585)	
training:	Epoch: [20][510/817]	Loss 0.0103 (0.0584)	
training:	Epoch: [20][511/817]	Loss 0.0083 (0.0583)	
training:	Epoch: [20][512/817]	Loss 0.0093 (0.0582)	
training:	Epoch: [20][513/817]	Loss 0.0096 (0.0581)	
training:	Epoch: [20][514/817]	Loss 0.0096 (0.0580)	
training:	Epoch: [20][515/817]	Loss 0.0138 (0.0579)	
training:	Epoch: [20][516/817]	Loss 0.0084 (0.0578)	
training:	Epoch: [20][517/817]	Loss 0.0160 (0.0577)	
training:	Epoch: [20][518/817]	Loss 0.0089 (0.0576)	
training:	Epoch: [20][519/817]	Loss 0.0091 (0.0575)	
training:	Epoch: [20][520/817]	Loss 0.0091 (0.0574)	
training:	Epoch: [20][521/817]	Loss 0.0101 (0.0573)	
training:	Epoch: [20][522/817]	Loss 0.0102 (0.0573)	
training:	Epoch: [20][523/817]	Loss 0.0093 (0.0572)	
training:	Epoch: [20][524/817]	Loss 0.0163 (0.0571)	
training:	Epoch: [20][525/817]	Loss 0.0083 (0.0570)	
training:	Epoch: [20][526/817]	Loss 0.0124 (0.0569)	
training:	Epoch: [20][527/817]	Loss 0.0090 (0.0568)	
training:	Epoch: [20][528/817]	Loss 0.0084 (0.0567)	
training:	Epoch: [20][529/817]	Loss 0.0087 (0.0566)	
training:	Epoch: [20][530/817]	Loss 0.0087 (0.0565)	
training:	Epoch: [20][531/817]	Loss 0.0101 (0.0565)	
training:	Epoch: [20][532/817]	Loss 0.0099 (0.0564)	
training:	Epoch: [20][533/817]	Loss 0.0095 (0.0563)	
training:	Epoch: [20][534/817]	Loss 0.0087 (0.0562)	
training:	Epoch: [20][535/817]	Loss 0.0091 (0.0561)	
training:	Epoch: [20][536/817]	Loss 0.0090 (0.0560)	
training:	Epoch: [20][537/817]	Loss 0.0088 (0.0559)	
training:	Epoch: [20][538/817]	Loss 0.0100 (0.0558)	
training:	Epoch: [20][539/817]	Loss 0.6088 (0.0569)	
training:	Epoch: [20][540/817]	Loss 0.0096 (0.0568)	
training:	Epoch: [20][541/817]	Loss 0.0086 (0.0567)	
training:	Epoch: [20][542/817]	Loss 0.0077 (0.0566)	
training:	Epoch: [20][543/817]	Loss 0.0085 (0.0565)	
training:	Epoch: [20][544/817]	Loss 0.0089 (0.0564)	
training:	Epoch: [20][545/817]	Loss 0.0093 (0.0563)	
training:	Epoch: [20][546/817]	Loss 0.0108 (0.0563)	
training:	Epoch: [20][547/817]	Loss 0.0235 (0.0562)	
training:	Epoch: [20][548/817]	Loss 0.0116 (0.0561)	
training:	Epoch: [20][549/817]	Loss 0.0098 (0.0560)	
training:	Epoch: [20][550/817]	Loss 0.0103 (0.0559)	
training:	Epoch: [20][551/817]	Loss 0.0084 (0.0559)	
training:	Epoch: [20][552/817]	Loss 0.0088 (0.0558)	
training:	Epoch: [20][553/817]	Loss 0.0088 (0.0557)	
training:	Epoch: [20][554/817]	Loss 0.0099 (0.0556)	
training:	Epoch: [20][555/817]	Loss 0.0091 (0.0555)	
training:	Epoch: [20][556/817]	Loss 0.0093 (0.0554)	
training:	Epoch: [20][557/817]	Loss 0.0109 (0.0554)	
training:	Epoch: [20][558/817]	Loss 0.1390 (0.0555)	
training:	Epoch: [20][559/817]	Loss 0.0084 (0.0554)	
training:	Epoch: [20][560/817]	Loss 0.0088 (0.0553)	
training:	Epoch: [20][561/817]	Loss 1.1571 (0.0573)	
training:	Epoch: [20][562/817]	Loss 0.0083 (0.0572)	
training:	Epoch: [20][563/817]	Loss 0.0094 (0.0571)	
training:	Epoch: [20][564/817]	Loss 0.0116 (0.0571)	
training:	Epoch: [20][565/817]	Loss 0.0088 (0.0570)	
training:	Epoch: [20][566/817]	Loss 0.0120 (0.0569)	
training:	Epoch: [20][567/817]	Loss 0.0096 (0.0568)	
training:	Epoch: [20][568/817]	Loss 0.0090 (0.0567)	
training:	Epoch: [20][569/817]	Loss 0.0089 (0.0566)	
training:	Epoch: [20][570/817]	Loss 0.5809 (0.0576)	
training:	Epoch: [20][571/817]	Loss 0.0085 (0.0575)	
training:	Epoch: [20][572/817]	Loss 0.0093 (0.0574)	
training:	Epoch: [20][573/817]	Loss 0.0091 (0.0573)	
training:	Epoch: [20][574/817]	Loss 0.0090 (0.0572)	
training:	Epoch: [20][575/817]	Loss 0.0091 (0.0571)	
training:	Epoch: [20][576/817]	Loss 0.0089 (0.0570)	
training:	Epoch: [20][577/817]	Loss 0.0107 (0.0570)	
training:	Epoch: [20][578/817]	Loss 0.0093 (0.0569)	
training:	Epoch: [20][579/817]	Loss 0.0091 (0.0568)	
training:	Epoch: [20][580/817]	Loss 0.0089 (0.0567)	
training:	Epoch: [20][581/817]	Loss 0.0096 (0.0566)	
training:	Epoch: [20][582/817]	Loss 0.0123 (0.0566)	
training:	Epoch: [20][583/817]	Loss 0.0115 (0.0565)	
training:	Epoch: [20][584/817]	Loss 0.0088 (0.0564)	
training:	Epoch: [20][585/817]	Loss 0.0093 (0.0563)	
training:	Epoch: [20][586/817]	Loss 0.0126 (0.0562)	
training:	Epoch: [20][587/817]	Loss 0.0090 (0.0562)	
training:	Epoch: [20][588/817]	Loss 0.0083 (0.0561)	
training:	Epoch: [20][589/817]	Loss 0.0085 (0.0560)	
training:	Epoch: [20][590/817]	Loss 0.0085 (0.0559)	
training:	Epoch: [20][591/817]	Loss 0.0087 (0.0558)	
training:	Epoch: [20][592/817]	Loss 0.0087 (0.0558)	
training:	Epoch: [20][593/817]	Loss 0.0098 (0.0557)	
training:	Epoch: [20][594/817]	Loss 0.0094 (0.0556)	
training:	Epoch: [20][595/817]	Loss 0.0086 (0.0555)	
training:	Epoch: [20][596/817]	Loss 0.0092 (0.0555)	
training:	Epoch: [20][597/817]	Loss 0.0103 (0.0554)	
training:	Epoch: [20][598/817]	Loss 0.0098 (0.0553)	
training:	Epoch: [20][599/817]	Loss 0.0106 (0.0552)	
training:	Epoch: [20][600/817]	Loss 0.0092 (0.0552)	
training:	Epoch: [20][601/817]	Loss 0.5892 (0.0560)	
training:	Epoch: [20][602/817]	Loss 0.0115 (0.0560)	
training:	Epoch: [20][603/817]	Loss 0.0094 (0.0559)	
training:	Epoch: [20][604/817]	Loss 0.0099 (0.0558)	
training:	Epoch: [20][605/817]	Loss 0.0085 (0.0557)	
training:	Epoch: [20][606/817]	Loss 0.0093 (0.0557)	
training:	Epoch: [20][607/817]	Loss 0.0094 (0.0556)	
training:	Epoch: [20][608/817]	Loss 0.0091 (0.0555)	
training:	Epoch: [20][609/817]	Loss 0.0089 (0.0554)	
training:	Epoch: [20][610/817]	Loss 0.0094 (0.0554)	
training:	Epoch: [20][611/817]	Loss 0.0095 (0.0553)	
training:	Epoch: [20][612/817]	Loss 0.0097 (0.0552)	
training:	Epoch: [20][613/817]	Loss 0.0392 (0.0552)	
training:	Epoch: [20][614/817]	Loss 0.0089 (0.0551)	
training:	Epoch: [20][615/817]	Loss 0.0090 (0.0550)	
training:	Epoch: [20][616/817]	Loss 0.0092 (0.0550)	
training:	Epoch: [20][617/817]	Loss 0.0090 (0.0549)	
training:	Epoch: [20][618/817]	Loss 0.0088 (0.0548)	
training:	Epoch: [20][619/817]	Loss 0.0098 (0.0547)	
training:	Epoch: [20][620/817]	Loss 0.0088 (0.0547)	
training:	Epoch: [20][621/817]	Loss 0.0104 (0.0546)	
training:	Epoch: [20][622/817]	Loss 0.0085 (0.0545)	
training:	Epoch: [20][623/817]	Loss 0.0093 (0.0544)	
training:	Epoch: [20][624/817]	Loss 0.1334 (0.0546)	
training:	Epoch: [20][625/817]	Loss 0.0095 (0.0545)	
training:	Epoch: [20][626/817]	Loss 0.0092 (0.0544)	
training:	Epoch: [20][627/817]	Loss 0.0090 (0.0543)	
training:	Epoch: [20][628/817]	Loss 0.0090 (0.0543)	
training:	Epoch: [20][629/817]	Loss 0.0084 (0.0542)	
training:	Epoch: [20][630/817]	Loss 0.0187 (0.0541)	
training:	Epoch: [20][631/817]	Loss 0.0091 (0.0541)	
training:	Epoch: [20][632/817]	Loss 0.0203 (0.0540)	
training:	Epoch: [20][633/817]	Loss 0.0094 (0.0540)	
training:	Epoch: [20][634/817]	Loss 0.0101 (0.0539)	
training:	Epoch: [20][635/817]	Loss 0.0099 (0.0538)	
training:	Epoch: [20][636/817]	Loss 0.0082 (0.0537)	
training:	Epoch: [20][637/817]	Loss 0.0114 (0.0537)	
training:	Epoch: [20][638/817]	Loss 0.0256 (0.0536)	
training:	Epoch: [20][639/817]	Loss 0.0087 (0.0536)	
training:	Epoch: [20][640/817]	Loss 0.0086 (0.0535)	
training:	Epoch: [20][641/817]	Loss 0.4653 (0.0541)	
training:	Epoch: [20][642/817]	Loss 0.0088 (0.0541)	
training:	Epoch: [20][643/817]	Loss 0.0111 (0.0540)	
training:	Epoch: [20][644/817]	Loss 0.0082 (0.0539)	
training:	Epoch: [20][645/817]	Loss 0.0107 (0.0539)	
training:	Epoch: [20][646/817]	Loss 0.0090 (0.0538)	
training:	Epoch: [20][647/817]	Loss 0.0089 (0.0537)	
training:	Epoch: [20][648/817]	Loss 0.0085 (0.0536)	
training:	Epoch: [20][649/817]	Loss 0.0093 (0.0536)	
training:	Epoch: [20][650/817]	Loss 0.0098 (0.0535)	
training:	Epoch: [20][651/817]	Loss 1.1849 (0.0553)	
training:	Epoch: [20][652/817]	Loss 0.0086 (0.0552)	
training:	Epoch: [20][653/817]	Loss 0.0083 (0.0551)	
training:	Epoch: [20][654/817]	Loss 0.0092 (0.0550)	
training:	Epoch: [20][655/817]	Loss 0.0086 (0.0550)	
training:	Epoch: [20][656/817]	Loss 0.0084 (0.0549)	
training:	Epoch: [20][657/817]	Loss 0.0094 (0.0548)	
training:	Epoch: [20][658/817]	Loss 0.0092 (0.0548)	
training:	Epoch: [20][659/817]	Loss 0.0091 (0.0547)	
training:	Epoch: [20][660/817]	Loss 0.0088 (0.0546)	
training:	Epoch: [20][661/817]	Loss 0.0103 (0.0546)	
training:	Epoch: [20][662/817]	Loss 0.0097 (0.0545)	
training:	Epoch: [20][663/817]	Loss 0.0138 (0.0544)	
training:	Epoch: [20][664/817]	Loss 0.0084 (0.0544)	
training:	Epoch: [20][665/817]	Loss 0.0087 (0.0543)	
training:	Epoch: [20][666/817]	Loss 0.0086 (0.0542)	
training:	Epoch: [20][667/817]	Loss 0.0118 (0.0542)	
training:	Epoch: [20][668/817]	Loss 0.0088 (0.0541)	
training:	Epoch: [20][669/817]	Loss 0.0094 (0.0540)	
training:	Epoch: [20][670/817]	Loss 0.0087 (0.0539)	
training:	Epoch: [20][671/817]	Loss 0.0144 (0.0539)	
training:	Epoch: [20][672/817]	Loss 0.0083 (0.0538)	
training:	Epoch: [20][673/817]	Loss 0.0097 (0.0538)	
training:	Epoch: [20][674/817]	Loss 0.0100 (0.0537)	
training:	Epoch: [20][675/817]	Loss 0.0090 (0.0536)	
training:	Epoch: [20][676/817]	Loss 0.0104 (0.0536)	
training:	Epoch: [20][677/817]	Loss 0.0091 (0.0535)	
training:	Epoch: [20][678/817]	Loss 0.3071 (0.0539)	
training:	Epoch: [20][679/817]	Loss 0.0085 (0.0538)	
training:	Epoch: [20][680/817]	Loss 0.0084 (0.0537)	
training:	Epoch: [20][681/817]	Loss 0.0135 (0.0537)	
training:	Epoch: [20][682/817]	Loss 0.0086 (0.0536)	
training:	Epoch: [20][683/817]	Loss 0.0088 (0.0535)	
training:	Epoch: [20][684/817]	Loss 0.0535 (0.0535)	
training:	Epoch: [20][685/817]	Loss 0.0101 (0.0535)	
training:	Epoch: [20][686/817]	Loss 0.0089 (0.0534)	
training:	Epoch: [20][687/817]	Loss 0.0090 (0.0534)	
training:	Epoch: [20][688/817]	Loss 0.0079 (0.0533)	
training:	Epoch: [20][689/817]	Loss 0.0107 (0.0532)	
training:	Epoch: [20][690/817]	Loss 0.0089 (0.0532)	
training:	Epoch: [20][691/817]	Loss 0.0089 (0.0531)	
training:	Epoch: [20][692/817]	Loss 0.0110 (0.0530)	
training:	Epoch: [20][693/817]	Loss 0.0081 (0.0530)	
training:	Epoch: [20][694/817]	Loss 0.0089 (0.0529)	
training:	Epoch: [20][695/817]	Loss 0.0089 (0.0528)	
training:	Epoch: [20][696/817]	Loss 0.0082 (0.0528)	
training:	Epoch: [20][697/817]	Loss 0.0086 (0.0527)	
training:	Epoch: [20][698/817]	Loss 0.0085 (0.0527)	
training:	Epoch: [20][699/817]	Loss 0.3590 (0.0531)	
training:	Epoch: [20][700/817]	Loss 0.0092 (0.0530)	
training:	Epoch: [20][701/817]	Loss 0.0182 (0.0530)	
training:	Epoch: [20][702/817]	Loss 0.0097 (0.0529)	
training:	Epoch: [20][703/817]	Loss 0.0083 (0.0529)	
training:	Epoch: [20][704/817]	Loss 0.0092 (0.0528)	
training:	Epoch: [20][705/817]	Loss 0.5964 (0.0536)	
training:	Epoch: [20][706/817]	Loss 0.0086 (0.0535)	
training:	Epoch: [20][707/817]	Loss 0.5974 (0.0543)	
training:	Epoch: [20][708/817]	Loss 0.0098 (0.0542)	
training:	Epoch: [20][709/817]	Loss 0.0086 (0.0541)	
training:	Epoch: [20][710/817]	Loss 0.0085 (0.0541)	
training:	Epoch: [20][711/817]	Loss 0.0095 (0.0540)	
training:	Epoch: [20][712/817]	Loss 0.0105 (0.0540)	
training:	Epoch: [20][713/817]	Loss 0.0084 (0.0539)	
training:	Epoch: [20][714/817]	Loss 0.0095 (0.0538)	
training:	Epoch: [20][715/817]	Loss 0.0086 (0.0538)	
training:	Epoch: [20][716/817]	Loss 0.0084 (0.0537)	
training:	Epoch: [20][717/817]	Loss 0.0094 (0.0536)	
training:	Epoch: [20][718/817]	Loss 0.0087 (0.0536)	
training:	Epoch: [20][719/817]	Loss 0.0089 (0.0535)	
training:	Epoch: [20][720/817]	Loss 0.0094 (0.0535)	
training:	Epoch: [20][721/817]	Loss 0.3538 (0.0539)	
training:	Epoch: [20][722/817]	Loss 0.0156 (0.0538)	
training:	Epoch: [20][723/817]	Loss 0.0086 (0.0538)	
training:	Epoch: [20][724/817]	Loss 0.5508 (0.0544)	
training:	Epoch: [20][725/817]	Loss 0.0097 (0.0544)	
training:	Epoch: [20][726/817]	Loss 0.0094 (0.0543)	
training:	Epoch: [20][727/817]	Loss 0.0089 (0.0543)	
training:	Epoch: [20][728/817]	Loss 0.0094 (0.0542)	
training:	Epoch: [20][729/817]	Loss 0.0084 (0.0541)	
training:	Epoch: [20][730/817]	Loss 0.0086 (0.0541)	
training:	Epoch: [20][731/817]	Loss 0.0081 (0.0540)	
training:	Epoch: [20][732/817]	Loss 0.0083 (0.0539)	
training:	Epoch: [20][733/817]	Loss 0.0092 (0.0539)	
training:	Epoch: [20][734/817]	Loss 0.0086 (0.0538)	
training:	Epoch: [20][735/817]	Loss 0.0080 (0.0538)	
training:	Epoch: [20][736/817]	Loss 0.0196 (0.0537)	
training:	Epoch: [20][737/817]	Loss 0.0111 (0.0537)	
training:	Epoch: [20][738/817]	Loss 0.0086 (0.0536)	
training:	Epoch: [20][739/817]	Loss 0.0637 (0.0536)	
training:	Epoch: [20][740/817]	Loss 0.0188 (0.0536)	
training:	Epoch: [20][741/817]	Loss 0.0379 (0.0535)	
training:	Epoch: [20][742/817]	Loss 0.0084 (0.0535)	
training:	Epoch: [20][743/817]	Loss 0.0110 (0.0534)	
training:	Epoch: [20][744/817]	Loss 0.0084 (0.0534)	
training:	Epoch: [20][745/817]	Loss 0.0395 (0.0533)	
training:	Epoch: [20][746/817]	Loss 0.0089 (0.0533)	
training:	Epoch: [20][747/817]	Loss 0.0084 (0.0532)	
training:	Epoch: [20][748/817]	Loss 0.0086 (0.0532)	
training:	Epoch: [20][749/817]	Loss 0.0094 (0.0531)	
training:	Epoch: [20][750/817]	Loss 0.0090 (0.0530)	
training:	Epoch: [20][751/817]	Loss 0.0087 (0.0530)	
training:	Epoch: [20][752/817]	Loss 0.0087 (0.0529)	
training:	Epoch: [20][753/817]	Loss 0.0085 (0.0529)	
training:	Epoch: [20][754/817]	Loss 0.0093 (0.0528)	
training:	Epoch: [20][755/817]	Loss 0.0087 (0.0528)	
training:	Epoch: [20][756/817]	Loss 0.0092 (0.0527)	
training:	Epoch: [20][757/817]	Loss 0.0111 (0.0526)	
training:	Epoch: [20][758/817]	Loss 0.0086 (0.0526)	
training:	Epoch: [20][759/817]	Loss 0.0085 (0.0525)	
training:	Epoch: [20][760/817]	Loss 0.0087 (0.0525)	
training:	Epoch: [20][761/817]	Loss 0.0100 (0.0524)	
training:	Epoch: [20][762/817]	Loss 0.0127 (0.0524)	
training:	Epoch: [20][763/817]	Loss 0.0147 (0.0523)	
training:	Epoch: [20][764/817]	Loss 0.0079 (0.0522)	
training:	Epoch: [20][765/817]	Loss 0.6021 (0.0530)	
training:	Epoch: [20][766/817]	Loss 0.0110 (0.0529)	
training:	Epoch: [20][767/817]	Loss 0.0093 (0.0529)	
training:	Epoch: [20][768/817]	Loss 0.0089 (0.0528)	
training:	Epoch: [20][769/817]	Loss 0.5931 (0.0535)	
training:	Epoch: [20][770/817]	Loss 0.0085 (0.0534)	
training:	Epoch: [20][771/817]	Loss 0.0087 (0.0534)	
training:	Epoch: [20][772/817]	Loss 0.0115 (0.0533)	
training:	Epoch: [20][773/817]	Loss 0.0086 (0.0533)	
training:	Epoch: [20][774/817]	Loss 0.0204 (0.0532)	
training:	Epoch: [20][775/817]	Loss 0.0087 (0.0532)	
training:	Epoch: [20][776/817]	Loss 0.0103 (0.0531)	
training:	Epoch: [20][777/817]	Loss 0.0087 (0.0531)	
training:	Epoch: [20][778/817]	Loss 0.0093 (0.0530)	
training:	Epoch: [20][779/817]	Loss 0.0084 (0.0529)	
training:	Epoch: [20][780/817]	Loss 0.0080 (0.0529)	
training:	Epoch: [20][781/817]	Loss 0.0083 (0.0528)	
training:	Epoch: [20][782/817]	Loss 0.6075 (0.0535)	
training:	Epoch: [20][783/817]	Loss 0.0085 (0.0535)	
training:	Epoch: [20][784/817]	Loss 0.0086 (0.0534)	
training:	Epoch: [20][785/817]	Loss 0.0084 (0.0534)	
training:	Epoch: [20][786/817]	Loss 0.0088 (0.0533)	
training:	Epoch: [20][787/817]	Loss 0.0336 (0.0533)	
training:	Epoch: [20][788/817]	Loss 0.0085 (0.0532)	
training:	Epoch: [20][789/817]	Loss 0.0097 (0.0532)	
training:	Epoch: [20][790/817]	Loss 0.0087 (0.0531)	
training:	Epoch: [20][791/817]	Loss 0.0088 (0.0531)	
training:	Epoch: [20][792/817]	Loss 0.0094 (0.0530)	
training:	Epoch: [20][793/817]	Loss 0.0082 (0.0530)	
training:	Epoch: [20][794/817]	Loss 0.0095 (0.0529)	
training:	Epoch: [20][795/817]	Loss 0.0105 (0.0528)	
training:	Epoch: [20][796/817]	Loss 0.0097 (0.0528)	
training:	Epoch: [20][797/817]	Loss 0.0087 (0.0527)	
training:	Epoch: [20][798/817]	Loss 0.0088 (0.0527)	
training:	Epoch: [20][799/817]	Loss 0.0098 (0.0526)	
training:	Epoch: [20][800/817]	Loss 0.0088 (0.0526)	
training:	Epoch: [20][801/817]	Loss 0.0085 (0.0525)	
training:	Epoch: [20][802/817]	Loss 0.0090 (0.0525)	
training:	Epoch: [20][803/817]	Loss 0.0086 (0.0524)	
training:	Epoch: [20][804/817]	Loss 0.0083 (0.0524)	
training:	Epoch: [20][805/817]	Loss 0.0093 (0.0523)	
training:	Epoch: [20][806/817]	Loss 0.0085 (0.0522)	
training:	Epoch: [20][807/817]	Loss 0.0086 (0.0522)	
training:	Epoch: [20][808/817]	Loss 0.0085 (0.0521)	
training:	Epoch: [20][809/817]	Loss 0.0097 (0.0521)	
training:	Epoch: [20][810/817]	Loss 0.0085 (0.0520)	
training:	Epoch: [20][811/817]	Loss 0.0083 (0.0520)	
training:	Epoch: [20][812/817]	Loss 0.0090 (0.0519)	
training:	Epoch: [20][813/817]	Loss 0.0090 (0.0519)	
training:	Epoch: [20][814/817]	Loss 0.0084 (0.0518)	
training:	Epoch: [20][815/817]	Loss 0.0089 (0.0518)	
training:	Epoch: [20][816/817]	Loss 0.0084 (0.0517)	
training:	Epoch: [20][817/817]	Loss 0.0088 (0.0517)	
Training:	 Loss: 0.0516

Training:	 ACC: 0.9920 0.9920 0.9921 0.9920
Validation:	 ACC: 0.7868 0.7871 0.7932 0.7803
Validation:	 Best_BACC: 0.8223 0.8218 0.8127 0.8318
Validation:	 Loss: 0.8710
Pretraining:	Epoch 21/200
----------
training:	Epoch: [21][1/817]	Loss 0.0084 (0.0084)	
training:	Epoch: [21][2/817]	Loss 0.0090 (0.0087)	
training:	Epoch: [21][3/817]	Loss 0.0092 (0.0089)	
training:	Epoch: [21][4/817]	Loss 0.0085 (0.0088)	
training:	Epoch: [21][5/817]	Loss 0.0083 (0.0087)	
training:	Epoch: [21][6/817]	Loss 0.0084 (0.0086)	
training:	Epoch: [21][7/817]	Loss 0.0078 (0.0085)	
training:	Epoch: [21][8/817]	Loss 0.0092 (0.0086)	
training:	Epoch: [21][9/817]	Loss 0.0086 (0.0086)	
training:	Epoch: [21][10/817]	Loss 0.0096 (0.0087)	
training:	Epoch: [21][11/817]	Loss 0.0081 (0.0086)	
training:	Epoch: [21][12/817]	Loss 0.0088 (0.0087)	
training:	Epoch: [21][13/817]	Loss 0.0081 (0.0086)	
training:	Epoch: [21][14/817]	Loss 0.0611 (0.0124)	
training:	Epoch: [21][15/817]	Loss 0.0091 (0.0122)	
training:	Epoch: [21][16/817]	Loss 0.0081 (0.0119)	
training:	Epoch: [21][17/817]	Loss 0.1308 (0.0189)	
training:	Epoch: [21][18/817]	Loss 0.6130 (0.0519)	
training:	Epoch: [21][19/817]	Loss 0.0096 (0.0497)	
training:	Epoch: [21][20/817]	Loss 0.0089 (0.0476)	
training:	Epoch: [21][21/817]	Loss 0.0083 (0.0458)	
training:	Epoch: [21][22/817]	Loss 0.0086 (0.0441)	
training:	Epoch: [21][23/817]	Loss 0.0084 (0.0425)	
training:	Epoch: [21][24/817]	Loss 0.0094 (0.0411)	
training:	Epoch: [21][25/817]	Loss 0.0078 (0.0398)	
training:	Epoch: [21][26/817]	Loss 0.0098 (0.0387)	
training:	Epoch: [21][27/817]	Loss 0.0126 (0.0377)	
training:	Epoch: [21][28/817]	Loss 0.0088 (0.0367)	
training:	Epoch: [21][29/817]	Loss 0.0085 (0.0357)	
training:	Epoch: [21][30/817]	Loss 0.0083 (0.0348)	
training:	Epoch: [21][31/817]	Loss 0.0128 (0.0341)	
training:	Epoch: [21][32/817]	Loss 0.0087 (0.0333)	
training:	Epoch: [21][33/817]	Loss 0.0090 (0.0325)	
training:	Epoch: [21][34/817]	Loss 0.0093 (0.0319)	
training:	Epoch: [21][35/817]	Loss 0.0087 (0.0312)	
training:	Epoch: [21][36/817]	Loss 0.0083 (0.0306)	
training:	Epoch: [21][37/817]	Loss 0.0079 (0.0299)	
training:	Epoch: [21][38/817]	Loss 0.0083 (0.0294)	
training:	Epoch: [21][39/817]	Loss 0.0088 (0.0288)	
training:	Epoch: [21][40/817]	Loss 0.0116 (0.0284)	
training:	Epoch: [21][41/817]	Loss 0.0953 (0.0300)	
training:	Epoch: [21][42/817]	Loss 0.0093 (0.0296)	
training:	Epoch: [21][43/817]	Loss 0.5963 (0.0427)	
training:	Epoch: [21][44/817]	Loss 0.0086 (0.0420)	
training:	Epoch: [21][45/817]	Loss 0.0503 (0.0421)	
training:	Epoch: [21][46/817]	Loss 0.0109 (0.0415)	
training:	Epoch: [21][47/817]	Loss 0.0078 (0.0408)	
training:	Epoch: [21][48/817]	Loss 0.0085 (0.0401)	
training:	Epoch: [21][49/817]	Loss 0.0086 (0.0394)	
training:	Epoch: [21][50/817]	Loss 0.6031 (0.0507)	
training:	Epoch: [21][51/817]	Loss 0.0081 (0.0499)	
training:	Epoch: [21][52/817]	Loss 0.0080 (0.0491)	
training:	Epoch: [21][53/817]	Loss 0.0098 (0.0483)	
training:	Epoch: [21][54/817]	Loss 0.0143 (0.0477)	
training:	Epoch: [21][55/817]	Loss 0.0099 (0.0470)	
training:	Epoch: [21][56/817]	Loss 0.0087 (0.0463)	
training:	Epoch: [21][57/817]	Loss 0.0086 (0.0457)	
training:	Epoch: [21][58/817]	Loss 0.0080 (0.0450)	
training:	Epoch: [21][59/817]	Loss 0.0083 (0.0444)	
training:	Epoch: [21][60/817]	Loss 0.0084 (0.0438)	
training:	Epoch: [21][61/817]	Loss 0.0109 (0.0433)	
training:	Epoch: [21][62/817]	Loss 0.0081 (0.0427)	
training:	Epoch: [21][63/817]	Loss 0.0080 (0.0421)	
training:	Epoch: [21][64/817]	Loss 0.2164 (0.0449)	
training:	Epoch: [21][65/817]	Loss 0.0076 (0.0443)	
training:	Epoch: [21][66/817]	Loss 0.0087 (0.0437)	
training:	Epoch: [21][67/817]	Loss 0.0078 (0.0432)	
training:	Epoch: [21][68/817]	Loss 0.0081 (0.0427)	
training:	Epoch: [21][69/817]	Loss 0.0082 (0.0422)	
training:	Epoch: [21][70/817]	Loss 0.0275 (0.0420)	
training:	Epoch: [21][71/817]	Loss 0.1994 (0.0442)	
training:	Epoch: [21][72/817]	Loss 0.0083 (0.0437)	
training:	Epoch: [21][73/817]	Loss 0.0088 (0.0432)	
training:	Epoch: [21][74/817]	Loss 0.0083 (0.0428)	
training:	Epoch: [21][75/817]	Loss 0.0089 (0.0423)	
training:	Epoch: [21][76/817]	Loss 0.0085 (0.0419)	
training:	Epoch: [21][77/817]	Loss 0.0096 (0.0414)	
training:	Epoch: [21][78/817]	Loss 0.0088 (0.0410)	
training:	Epoch: [21][79/817]	Loss 0.0083 (0.0406)	
training:	Epoch: [21][80/817]	Loss 0.0078 (0.0402)	
training:	Epoch: [21][81/817]	Loss 0.0099 (0.0398)	
training:	Epoch: [21][82/817]	Loss 0.0091 (0.0394)	
training:	Epoch: [21][83/817]	Loss 0.0083 (0.0391)	
training:	Epoch: [21][84/817]	Loss 0.0401 (0.0391)	
training:	Epoch: [21][85/817]	Loss 0.0085 (0.0387)	
training:	Epoch: [21][86/817]	Loss 0.0081 (0.0384)	
training:	Epoch: [21][87/817]	Loss 0.0089 (0.0380)	
training:	Epoch: [21][88/817]	Loss 0.0090 (0.0377)	
training:	Epoch: [21][89/817]	Loss 0.0085 (0.0374)	
training:	Epoch: [21][90/817]	Loss 0.0079 (0.0370)	
training:	Epoch: [21][91/817]	Loss 0.0082 (0.0367)	
training:	Epoch: [21][92/817]	Loss 0.0080 (0.0364)	
training:	Epoch: [21][93/817]	Loss 0.0082 (0.0361)	
training:	Epoch: [21][94/817]	Loss 0.0084 (0.0358)	
training:	Epoch: [21][95/817]	Loss 0.0085 (0.0355)	
training:	Epoch: [21][96/817]	Loss 0.0081 (0.0352)	
training:	Epoch: [21][97/817]	Loss 0.0100 (0.0350)	
training:	Epoch: [21][98/817]	Loss 0.1349 (0.0360)	
training:	Epoch: [21][99/817]	Loss 0.0085 (0.0357)	
training:	Epoch: [21][100/817]	Loss 0.0079 (0.0354)	
training:	Epoch: [21][101/817]	Loss 0.0084 (0.0352)	
training:	Epoch: [21][102/817]	Loss 0.0081 (0.0349)	
training:	Epoch: [21][103/817]	Loss 0.0099 (0.0347)	
training:	Epoch: [21][104/817]	Loss 0.0087 (0.0344)	
training:	Epoch: [21][105/817]	Loss 0.0099 (0.0342)	
training:	Epoch: [21][106/817]	Loss 0.0076 (0.0339)	
training:	Epoch: [21][107/817]	Loss 0.0085 (0.0337)	
training:	Epoch: [21][108/817]	Loss 0.0669 (0.0340)	
training:	Epoch: [21][109/817]	Loss 0.0077 (0.0338)	
training:	Epoch: [21][110/817]	Loss 0.0089 (0.0335)	
training:	Epoch: [21][111/817]	Loss 0.6241 (0.0389)	
training:	Epoch: [21][112/817]	Loss 0.0077 (0.0386)	
training:	Epoch: [21][113/817]	Loss 0.0096 (0.0383)	
training:	Epoch: [21][114/817]	Loss 0.0080 (0.0381)	
training:	Epoch: [21][115/817]	Loss 0.0092 (0.0378)	
training:	Epoch: [21][116/817]	Loss 0.0116 (0.0376)	
training:	Epoch: [21][117/817]	Loss 0.0181 (0.0374)	
training:	Epoch: [21][118/817]	Loss 0.0397 (0.0374)	
training:	Epoch: [21][119/817]	Loss 0.0084 (0.0372)	
training:	Epoch: [21][120/817]	Loss 0.0083 (0.0369)	
training:	Epoch: [21][121/817]	Loss 0.0085 (0.0367)	
training:	Epoch: [21][122/817]	Loss 0.0085 (0.0365)	
training:	Epoch: [21][123/817]	Loss 0.0076 (0.0362)	
training:	Epoch: [21][124/817]	Loss 0.6042 (0.0408)	
training:	Epoch: [21][125/817]	Loss 0.0081 (0.0406)	
training:	Epoch: [21][126/817]	Loss 0.0082 (0.0403)	
training:	Epoch: [21][127/817]	Loss 0.0086 (0.0401)	
training:	Epoch: [21][128/817]	Loss 0.0084 (0.0398)	
training:	Epoch: [21][129/817]	Loss 0.0084 (0.0396)	
training:	Epoch: [21][130/817]	Loss 0.0085 (0.0393)	
training:	Epoch: [21][131/817]	Loss 0.0081 (0.0391)	
training:	Epoch: [21][132/817]	Loss 0.0086 (0.0389)	
training:	Epoch: [21][133/817]	Loss 0.0409 (0.0389)	
training:	Epoch: [21][134/817]	Loss 0.0123 (0.0387)	
training:	Epoch: [21][135/817]	Loss 0.6225 (0.0430)	
training:	Epoch: [21][136/817]	Loss 0.6148 (0.0472)	
training:	Epoch: [21][137/817]	Loss 0.0081 (0.0469)	
training:	Epoch: [21][138/817]	Loss 0.0078 (0.0466)	
training:	Epoch: [21][139/817]	Loss 0.0078 (0.0464)	
training:	Epoch: [21][140/817]	Loss 0.0086 (0.0461)	
training:	Epoch: [21][141/817]	Loss 0.0104 (0.0458)	
training:	Epoch: [21][142/817]	Loss 0.0087 (0.0456)	
training:	Epoch: [21][143/817]	Loss 0.0079 (0.0453)	
training:	Epoch: [21][144/817]	Loss 0.0081 (0.0450)	
training:	Epoch: [21][145/817]	Loss 0.0078 (0.0448)	
training:	Epoch: [21][146/817]	Loss 0.0077 (0.0445)	
training:	Epoch: [21][147/817]	Loss 0.0080 (0.0443)	
training:	Epoch: [21][148/817]	Loss 0.0137 (0.0441)	
training:	Epoch: [21][149/817]	Loss 0.0087 (0.0438)	
training:	Epoch: [21][150/817]	Loss 0.0079 (0.0436)	
training:	Epoch: [21][151/817]	Loss 0.0079 (0.0434)	
training:	Epoch: [21][152/817]	Loss 0.0163 (0.0432)	
training:	Epoch: [21][153/817]	Loss 0.0088 (0.0430)	
training:	Epoch: [21][154/817]	Loss 0.0086 (0.0427)	
training:	Epoch: [21][155/817]	Loss 0.0082 (0.0425)	
training:	Epoch: [21][156/817]	Loss 0.4830 (0.0453)	
training:	Epoch: [21][157/817]	Loss 0.0093 (0.0451)	
training:	Epoch: [21][158/817]	Loss 0.0082 (0.0449)	
training:	Epoch: [21][159/817]	Loss 0.0085 (0.0447)	
training:	Epoch: [21][160/817]	Loss 0.0096 (0.0444)	
training:	Epoch: [21][161/817]	Loss 0.0078 (0.0442)	
training:	Epoch: [21][162/817]	Loss 0.0083 (0.0440)	
training:	Epoch: [21][163/817]	Loss 0.0088 (0.0438)	
training:	Epoch: [21][164/817]	Loss 0.0085 (0.0436)	
training:	Epoch: [21][165/817]	Loss 0.0086 (0.0433)	
training:	Epoch: [21][166/817]	Loss 0.0077 (0.0431)	
training:	Epoch: [21][167/817]	Loss 0.0077 (0.0429)	
training:	Epoch: [21][168/817]	Loss 0.5916 (0.0462)	
training:	Epoch: [21][169/817]	Loss 0.0078 (0.0460)	
training:	Epoch: [21][170/817]	Loss 0.0079 (0.0457)	
training:	Epoch: [21][171/817]	Loss 0.0096 (0.0455)	
training:	Epoch: [21][172/817]	Loss 0.0114 (0.0453)	
training:	Epoch: [21][173/817]	Loss 0.0080 (0.0451)	
training:	Epoch: [21][174/817]	Loss 0.0087 (0.0449)	
training:	Epoch: [21][175/817]	Loss 0.0191 (0.0447)	
training:	Epoch: [21][176/817]	Loss 0.0084 (0.0445)	
training:	Epoch: [21][177/817]	Loss 0.0089 (0.0443)	
training:	Epoch: [21][178/817]	Loss 0.0302 (0.0443)	
training:	Epoch: [21][179/817]	Loss 0.0136 (0.0441)	
training:	Epoch: [21][180/817]	Loss 0.0115 (0.0439)	
training:	Epoch: [21][181/817]	Loss 0.0085 (0.0437)	
training:	Epoch: [21][182/817]	Loss 0.0108 (0.0435)	
training:	Epoch: [21][183/817]	Loss 1.2249 (0.0500)	
training:	Epoch: [21][184/817]	Loss 0.0101 (0.0498)	
training:	Epoch: [21][185/817]	Loss 0.0082 (0.0495)	
training:	Epoch: [21][186/817]	Loss 0.0091 (0.0493)	
training:	Epoch: [21][187/817]	Loss 0.0082 (0.0491)	
training:	Epoch: [21][188/817]	Loss 0.0083 (0.0489)	
training:	Epoch: [21][189/817]	Loss 0.0081 (0.0487)	
training:	Epoch: [21][190/817]	Loss 0.0082 (0.0485)	
training:	Epoch: [21][191/817]	Loss 0.0088 (0.0483)	
training:	Epoch: [21][192/817]	Loss 0.0085 (0.0480)	
training:	Epoch: [21][193/817]	Loss 0.0087 (0.0478)	
training:	Epoch: [21][194/817]	Loss 0.0087 (0.0476)	
training:	Epoch: [21][195/817]	Loss 0.0087 (0.0474)	
training:	Epoch: [21][196/817]	Loss 0.0088 (0.0472)	
training:	Epoch: [21][197/817]	Loss 0.0090 (0.0471)	
training:	Epoch: [21][198/817]	Loss 0.3175 (0.0484)	
training:	Epoch: [21][199/817]	Loss 0.0088 (0.0482)	
training:	Epoch: [21][200/817]	Loss 0.0211 (0.0481)	
training:	Epoch: [21][201/817]	Loss 0.0087 (0.0479)	
training:	Epoch: [21][202/817]	Loss 0.0090 (0.0477)	
training:	Epoch: [21][203/817]	Loss 0.0101 (0.0475)	
training:	Epoch: [21][204/817]	Loss 0.0162 (0.0474)	
training:	Epoch: [21][205/817]	Loss 0.0087 (0.0472)	
training:	Epoch: [21][206/817]	Loss 0.0079 (0.0470)	
training:	Epoch: [21][207/817]	Loss 0.0088 (0.0468)	
training:	Epoch: [21][208/817]	Loss 0.0088 (0.0466)	
training:	Epoch: [21][209/817]	Loss 0.0078 (0.0464)	
training:	Epoch: [21][210/817]	Loss 0.0085 (0.0462)	
training:	Epoch: [21][211/817]	Loss 0.0090 (0.0461)	
training:	Epoch: [21][212/817]	Loss 0.0088 (0.0459)	
training:	Epoch: [21][213/817]	Loss 0.0085 (0.0457)	
training:	Epoch: [21][214/817]	Loss 0.0080 (0.0455)	
training:	Epoch: [21][215/817]	Loss 0.0160 (0.0454)	
training:	Epoch: [21][216/817]	Loss 0.0277 (0.0453)	
training:	Epoch: [21][217/817]	Loss 0.0080 (0.0451)	
training:	Epoch: [21][218/817]	Loss 0.0085 (0.0450)	
training:	Epoch: [21][219/817]	Loss 0.0089 (0.0448)	
training:	Epoch: [21][220/817]	Loss 0.0076 (0.0446)	
training:	Epoch: [21][221/817]	Loss 0.0080 (0.0445)	
training:	Epoch: [21][222/817]	Loss 0.5271 (0.0467)	
training:	Epoch: [21][223/817]	Loss 0.0087 (0.0465)	
training:	Epoch: [21][224/817]	Loss 0.0082 (0.0463)	
training:	Epoch: [21][225/817]	Loss 0.0088 (0.0461)	
training:	Epoch: [21][226/817]	Loss 0.5808 (0.0485)	
training:	Epoch: [21][227/817]	Loss 0.0078 (0.0483)	
training:	Epoch: [21][228/817]	Loss 0.5826 (0.0507)	
training:	Epoch: [21][229/817]	Loss 0.0083 (0.0505)	
training:	Epoch: [21][230/817]	Loss 0.0082 (0.0503)	
training:	Epoch: [21][231/817]	Loss 0.0089 (0.0501)	
training:	Epoch: [21][232/817]	Loss 0.0966 (0.0503)	
training:	Epoch: [21][233/817]	Loss 0.0086 (0.0501)	
training:	Epoch: [21][234/817]	Loss 0.0084 (0.0500)	
training:	Epoch: [21][235/817]	Loss 0.0120 (0.0498)	
training:	Epoch: [21][236/817]	Loss 0.0085 (0.0496)	
training:	Epoch: [21][237/817]	Loss 0.0091 (0.0495)	
training:	Epoch: [21][238/817]	Loss 0.0085 (0.0493)	
training:	Epoch: [21][239/817]	Loss 0.0090 (0.0491)	
training:	Epoch: [21][240/817]	Loss 0.0087 (0.0490)	
training:	Epoch: [21][241/817]	Loss 0.0155 (0.0488)	
training:	Epoch: [21][242/817]	Loss 0.0080 (0.0486)	
training:	Epoch: [21][243/817]	Loss 0.0087 (0.0485)	
training:	Epoch: [21][244/817]	Loss 0.0088 (0.0483)	
training:	Epoch: [21][245/817]	Loss 0.0079 (0.0482)	
training:	Epoch: [21][246/817]	Loss 0.0088 (0.0480)	
training:	Epoch: [21][247/817]	Loss 0.0206 (0.0479)	
training:	Epoch: [21][248/817]	Loss 0.6135 (0.0502)	
training:	Epoch: [21][249/817]	Loss 0.0083 (0.0500)	
training:	Epoch: [21][250/817]	Loss 0.0087 (0.0498)	
training:	Epoch: [21][251/817]	Loss 0.0080 (0.0497)	
training:	Epoch: [21][252/817]	Loss 0.0089 (0.0495)	
training:	Epoch: [21][253/817]	Loss 0.0089 (0.0493)	
training:	Epoch: [21][254/817]	Loss 0.0168 (0.0492)	
training:	Epoch: [21][255/817]	Loss 0.0092 (0.0491)	
training:	Epoch: [21][256/817]	Loss 0.0084 (0.0489)	
training:	Epoch: [21][257/817]	Loss 0.0098 (0.0487)	
training:	Epoch: [21][258/817]	Loss 0.0379 (0.0487)	
training:	Epoch: [21][259/817]	Loss 0.0087 (0.0485)	
training:	Epoch: [21][260/817]	Loss 0.0108 (0.0484)	
training:	Epoch: [21][261/817]	Loss 0.0078 (0.0482)	
training:	Epoch: [21][262/817]	Loss 0.0107 (0.0481)	
training:	Epoch: [21][263/817]	Loss 0.0096 (0.0480)	
training:	Epoch: [21][264/817]	Loss 0.0111 (0.0478)	
training:	Epoch: [21][265/817]	Loss 0.0089 (0.0477)	
training:	Epoch: [21][266/817]	Loss 0.0081 (0.0475)	
training:	Epoch: [21][267/817]	Loss 0.0087 (0.0474)	
training:	Epoch: [21][268/817]	Loss 0.0078 (0.0472)	
training:	Epoch: [21][269/817]	Loss 0.0085 (0.0471)	
training:	Epoch: [21][270/817]	Loss 0.0089 (0.0469)	
training:	Epoch: [21][271/817]	Loss 0.0088 (0.0468)	
training:	Epoch: [21][272/817]	Loss 0.0091 (0.0467)	
training:	Epoch: [21][273/817]	Loss 0.0130 (0.0465)	
training:	Epoch: [21][274/817]	Loss 0.0081 (0.0464)	
training:	Epoch: [21][275/817]	Loss 0.0081 (0.0463)	
training:	Epoch: [21][276/817]	Loss 0.0096 (0.0461)	
training:	Epoch: [21][277/817]	Loss 0.0084 (0.0460)	
training:	Epoch: [21][278/817]	Loss 0.0077 (0.0459)	
training:	Epoch: [21][279/817]	Loss 0.0099 (0.0457)	
training:	Epoch: [21][280/817]	Loss 0.0081 (0.0456)	
training:	Epoch: [21][281/817]	Loss 0.0103 (0.0455)	
training:	Epoch: [21][282/817]	Loss 0.0082 (0.0453)	
training:	Epoch: [21][283/817]	Loss 0.0091 (0.0452)	
training:	Epoch: [21][284/817]	Loss 0.0267 (0.0451)	
training:	Epoch: [21][285/817]	Loss 0.0674 (0.0452)	
training:	Epoch: [21][286/817]	Loss 0.0079 (0.0451)	
training:	Epoch: [21][287/817]	Loss 0.6061 (0.0470)	
training:	Epoch: [21][288/817]	Loss 0.0138 (0.0469)	
training:	Epoch: [21][289/817]	Loss 0.0085 (0.0468)	
training:	Epoch: [21][290/817]	Loss 0.0086 (0.0467)	
training:	Epoch: [21][291/817]	Loss 0.0083 (0.0465)	
training:	Epoch: [21][292/817]	Loss 0.0084 (0.0464)	
training:	Epoch: [21][293/817]	Loss 0.0083 (0.0463)	
training:	Epoch: [21][294/817]	Loss 0.6160 (0.0482)	
training:	Epoch: [21][295/817]	Loss 0.0080 (0.0481)	
training:	Epoch: [21][296/817]	Loss 0.0081 (0.0479)	
training:	Epoch: [21][297/817]	Loss 0.0084 (0.0478)	
training:	Epoch: [21][298/817]	Loss 0.0118 (0.0477)	
training:	Epoch: [21][299/817]	Loss 0.0083 (0.0476)	
training:	Epoch: [21][300/817]	Loss 0.0084 (0.0474)	
training:	Epoch: [21][301/817]	Loss 0.0078 (0.0473)	
training:	Epoch: [21][302/817]	Loss 0.0094 (0.0472)	
training:	Epoch: [21][303/817]	Loss 0.1398 (0.0475)	
training:	Epoch: [21][304/817]	Loss 0.0085 (0.0473)	
training:	Epoch: [21][305/817]	Loss 0.0108 (0.0472)	
training:	Epoch: [21][306/817]	Loss 0.0081 (0.0471)	
training:	Epoch: [21][307/817]	Loss 0.0093 (0.0470)	
training:	Epoch: [21][308/817]	Loss 0.0086 (0.0468)	
training:	Epoch: [21][309/817]	Loss 0.0093 (0.0467)	
training:	Epoch: [21][310/817]	Loss 0.0084 (0.0466)	
training:	Epoch: [21][311/817]	Loss 0.0086 (0.0465)	
training:	Epoch: [21][312/817]	Loss 0.0080 (0.0464)	
training:	Epoch: [21][313/817]	Loss 0.0080 (0.0462)	
training:	Epoch: [21][314/817]	Loss 0.0083 (0.0461)	
training:	Epoch: [21][315/817]	Loss 0.0080 (0.0460)	
training:	Epoch: [21][316/817]	Loss 0.0080 (0.0459)	
training:	Epoch: [21][317/817]	Loss 0.0081 (0.0458)	
training:	Epoch: [21][318/817]	Loss 0.0081 (0.0456)	
training:	Epoch: [21][319/817]	Loss 0.0104 (0.0455)	
training:	Epoch: [21][320/817]	Loss 0.0082 (0.0454)	
training:	Epoch: [21][321/817]	Loss 0.0089 (0.0453)	
training:	Epoch: [21][322/817]	Loss 0.1893 (0.0457)	
training:	Epoch: [21][323/817]	Loss 0.0086 (0.0456)	
training:	Epoch: [21][324/817]	Loss 0.0093 (0.0455)	
training:	Epoch: [21][325/817]	Loss 0.1049 (0.0457)	
training:	Epoch: [21][326/817]	Loss 0.6086 (0.0474)	
training:	Epoch: [21][327/817]	Loss 0.0085 (0.0473)	
training:	Epoch: [21][328/817]	Loss 0.0081 (0.0472)	
training:	Epoch: [21][329/817]	Loss 0.0080 (0.0471)	
training:	Epoch: [21][330/817]	Loss 0.0079 (0.0469)	
training:	Epoch: [21][331/817]	Loss 0.0083 (0.0468)	
training:	Epoch: [21][332/817]	Loss 0.0082 (0.0467)	
training:	Epoch: [21][333/817]	Loss 0.0082 (0.0466)	
training:	Epoch: [21][334/817]	Loss 0.0109 (0.0465)	
training:	Epoch: [21][335/817]	Loss 0.0107 (0.0464)	
training:	Epoch: [21][336/817]	Loss 0.0082 (0.0463)	
training:	Epoch: [21][337/817]	Loss 0.0085 (0.0462)	
training:	Epoch: [21][338/817]	Loss 0.0111 (0.0461)	
training:	Epoch: [21][339/817]	Loss 0.0086 (0.0459)	
training:	Epoch: [21][340/817]	Loss 0.0077 (0.0458)	
training:	Epoch: [21][341/817]	Loss 0.0081 (0.0457)	
training:	Epoch: [21][342/817]	Loss 0.5939 (0.0473)	
training:	Epoch: [21][343/817]	Loss 0.0079 (0.0472)	
training:	Epoch: [21][344/817]	Loss 0.0090 (0.0471)	
training:	Epoch: [21][345/817]	Loss 0.0085 (0.0470)	
training:	Epoch: [21][346/817]	Loss 0.0084 (0.0469)	
training:	Epoch: [21][347/817]	Loss 0.6020 (0.0485)	
training:	Epoch: [21][348/817]	Loss 0.0388 (0.0484)	
training:	Epoch: [21][349/817]	Loss 0.0082 (0.0483)	
training:	Epoch: [21][350/817]	Loss 0.0078 (0.0482)	
training:	Epoch: [21][351/817]	Loss 0.0084 (0.0481)	
training:	Epoch: [21][352/817]	Loss 0.0147 (0.0480)	
training:	Epoch: [21][353/817]	Loss 0.0083 (0.0479)	
training:	Epoch: [21][354/817]	Loss 0.0084 (0.0478)	
training:	Epoch: [21][355/817]	Loss 0.0082 (0.0477)	
training:	Epoch: [21][356/817]	Loss 0.0085 (0.0476)	
training:	Epoch: [21][357/817]	Loss 0.0189 (0.0475)	
training:	Epoch: [21][358/817]	Loss 0.0165 (0.0474)	
training:	Epoch: [21][359/817]	Loss 0.0086 (0.0473)	
training:	Epoch: [21][360/817]	Loss 0.0089 (0.0472)	
training:	Epoch: [21][361/817]	Loss 0.0081 (0.0471)	
training:	Epoch: [21][362/817]	Loss 0.0081 (0.0470)	
training:	Epoch: [21][363/817]	Loss 0.0082 (0.0469)	
training:	Epoch: [21][364/817]	Loss 0.0203 (0.0468)	
training:	Epoch: [21][365/817]	Loss 0.0081 (0.0467)	
training:	Epoch: [21][366/817]	Loss 0.0085 (0.0466)	
training:	Epoch: [21][367/817]	Loss 0.0087 (0.0465)	
training:	Epoch: [21][368/817]	Loss 0.0079 (0.0464)	
training:	Epoch: [21][369/817]	Loss 0.0088 (0.0463)	
training:	Epoch: [21][370/817]	Loss 0.0079 (0.0462)	
training:	Epoch: [21][371/817]	Loss 0.0081 (0.0461)	
training:	Epoch: [21][372/817]	Loss 0.0086 (0.0460)	
training:	Epoch: [21][373/817]	Loss 0.0111 (0.0459)	
training:	Epoch: [21][374/817]	Loss 0.0092 (0.0458)	
training:	Epoch: [21][375/817]	Loss 0.0163 (0.0457)	
training:	Epoch: [21][376/817]	Loss 0.0077 (0.0456)	
training:	Epoch: [21][377/817]	Loss 0.0086 (0.0455)	
training:	Epoch: [21][378/817]	Loss 0.0141 (0.0454)	
training:	Epoch: [21][379/817]	Loss 0.0079 (0.0453)	
training:	Epoch: [21][380/817]	Loss 0.5741 (0.0467)	
training:	Epoch: [21][381/817]	Loss 0.0082 (0.0466)	
training:	Epoch: [21][382/817]	Loss 0.0294 (0.0466)	
training:	Epoch: [21][383/817]	Loss 0.0083 (0.0465)	
training:	Epoch: [21][384/817]	Loss 0.0127 (0.0464)	
training:	Epoch: [21][385/817]	Loss 0.0081 (0.0463)	
training:	Epoch: [21][386/817]	Loss 0.0091 (0.0462)	
training:	Epoch: [21][387/817]	Loss 0.0079 (0.0461)	
training:	Epoch: [21][388/817]	Loss 0.0088 (0.0460)	
training:	Epoch: [21][389/817]	Loss 0.0098 (0.0459)	
training:	Epoch: [21][390/817]	Loss 0.0088 (0.0458)	
training:	Epoch: [21][391/817]	Loss 0.0087 (0.0457)	
training:	Epoch: [21][392/817]	Loss 0.0078 (0.0456)	
training:	Epoch: [21][393/817]	Loss 0.0081 (0.0455)	
training:	Epoch: [21][394/817]	Loss 0.0359 (0.0455)	
training:	Epoch: [21][395/817]	Loss 0.0083 (0.0454)	
training:	Epoch: [21][396/817]	Loss 0.0090 (0.0453)	
training:	Epoch: [21][397/817]	Loss 0.0086 (0.0452)	
training:	Epoch: [21][398/817]	Loss 0.0082 (0.0451)	
training:	Epoch: [21][399/817]	Loss 0.0087 (0.0450)	
training:	Epoch: [21][400/817]	Loss 0.0084 (0.0449)	
training:	Epoch: [21][401/817]	Loss 0.5221 (0.0461)	
training:	Epoch: [21][402/817]	Loss 0.0082 (0.0460)	
training:	Epoch: [21][403/817]	Loss 0.0081 (0.0459)	
training:	Epoch: [21][404/817]	Loss 0.0076 (0.0458)	
training:	Epoch: [21][405/817]	Loss 0.2099 (0.0462)	
training:	Epoch: [21][406/817]	Loss 0.0087 (0.0461)	
training:	Epoch: [21][407/817]	Loss 0.0081 (0.0460)	
training:	Epoch: [21][408/817]	Loss 0.0085 (0.0460)	
training:	Epoch: [21][409/817]	Loss 0.0086 (0.0459)	
training:	Epoch: [21][410/817]	Loss 0.0087 (0.0458)	
training:	Epoch: [21][411/817]	Loss 0.0157 (0.0457)	
training:	Epoch: [21][412/817]	Loss 0.0086 (0.0456)	
training:	Epoch: [21][413/817]	Loss 0.0078 (0.0455)	
training:	Epoch: [21][414/817]	Loss 0.0088 (0.0454)	
training:	Epoch: [21][415/817]	Loss 0.0083 (0.0453)	
training:	Epoch: [21][416/817]	Loss 0.0098 (0.0453)	
training:	Epoch: [21][417/817]	Loss 0.0097 (0.0452)	
training:	Epoch: [21][418/817]	Loss 0.0084 (0.0451)	
training:	Epoch: [21][419/817]	Loss 0.0078 (0.0450)	
training:	Epoch: [21][420/817]	Loss 0.0088 (0.0449)	
training:	Epoch: [21][421/817]	Loss 0.0083 (0.0448)	
training:	Epoch: [21][422/817]	Loss 0.0081 (0.0447)	
training:	Epoch: [21][423/817]	Loss 0.0078 (0.0446)	
training:	Epoch: [21][424/817]	Loss 0.0214 (0.0446)	
training:	Epoch: [21][425/817]	Loss 0.0103 (0.0445)	
training:	Epoch: [21][426/817]	Loss 0.0196 (0.0445)	
training:	Epoch: [21][427/817]	Loss 0.0088 (0.0444)	
training:	Epoch: [21][428/817]	Loss 0.0083 (0.0443)	
training:	Epoch: [21][429/817]	Loss 0.0082 (0.0442)	
training:	Epoch: [21][430/817]	Loss 0.0088 (0.0441)	
training:	Epoch: [21][431/817]	Loss 0.0077 (0.0440)	
training:	Epoch: [21][432/817]	Loss 0.6278 (0.0454)	
training:	Epoch: [21][433/817]	Loss 0.0079 (0.0453)	
training:	Epoch: [21][434/817]	Loss 0.0079 (0.0452)	
training:	Epoch: [21][435/817]	Loss 0.0091 (0.0451)	
training:	Epoch: [21][436/817]	Loss 0.0078 (0.0450)	
training:	Epoch: [21][437/817]	Loss 0.0215 (0.0450)	
training:	Epoch: [21][438/817]	Loss 0.0078 (0.0449)	
training:	Epoch: [21][439/817]	Loss 0.0089 (0.0448)	
training:	Epoch: [21][440/817]	Loss 0.0080 (0.0447)	
training:	Epoch: [21][441/817]	Loss 0.0086 (0.0447)	
training:	Epoch: [21][442/817]	Loss 0.0082 (0.0446)	
training:	Epoch: [21][443/817]	Loss 0.0086 (0.0445)	
training:	Epoch: [21][444/817]	Loss 0.0082 (0.0444)	
training:	Epoch: [21][445/817]	Loss 0.0085 (0.0443)	
training:	Epoch: [21][446/817]	Loss 0.0167 (0.0443)	
training:	Epoch: [21][447/817]	Loss 0.0080 (0.0442)	
training:	Epoch: [21][448/817]	Loss 0.1853 (0.0445)	
training:	Epoch: [21][449/817]	Loss 0.5942 (0.0457)	
training:	Epoch: [21][450/817]	Loss 0.0145 (0.0457)	
training:	Epoch: [21][451/817]	Loss 0.0079 (0.0456)	
training:	Epoch: [21][452/817]	Loss 0.0096 (0.0455)	
training:	Epoch: [21][453/817]	Loss 0.0081 (0.0454)	
training:	Epoch: [21][454/817]	Loss 0.0080 (0.0453)	
training:	Epoch: [21][455/817]	Loss 0.0084 (0.0452)	
training:	Epoch: [21][456/817]	Loss 0.0125 (0.0452)	
training:	Epoch: [21][457/817]	Loss 0.0087 (0.0451)	
training:	Epoch: [21][458/817]	Loss 0.6273 (0.0464)	
training:	Epoch: [21][459/817]	Loss 0.0081 (0.0463)	
training:	Epoch: [21][460/817]	Loss 0.0081 (0.0462)	
training:	Epoch: [21][461/817]	Loss 0.5923 (0.0474)	
training:	Epoch: [21][462/817]	Loss 0.0517 (0.0474)	
training:	Epoch: [21][463/817]	Loss 0.0149 (0.0473)	
training:	Epoch: [21][464/817]	Loss 0.0081 (0.0472)	
training:	Epoch: [21][465/817]	Loss 0.0190 (0.0472)	
training:	Epoch: [21][466/817]	Loss 0.0080 (0.0471)	
training:	Epoch: [21][467/817]	Loss 0.0087 (0.0470)	
training:	Epoch: [21][468/817]	Loss 0.2199 (0.0474)	
training:	Epoch: [21][469/817]	Loss 0.6066 (0.0486)	
training:	Epoch: [21][470/817]	Loss 0.0098 (0.0485)	
training:	Epoch: [21][471/817]	Loss 0.0079 (0.0484)	
training:	Epoch: [21][472/817]	Loss 0.0108 (0.0483)	
training:	Epoch: [21][473/817]	Loss 0.0079 (0.0482)	
training:	Epoch: [21][474/817]	Loss 0.0083 (0.0482)	
training:	Epoch: [21][475/817]	Loss 0.0082 (0.0481)	
training:	Epoch: [21][476/817]	Loss 0.0081 (0.0480)	
training:	Epoch: [21][477/817]	Loss 0.5572 (0.0491)	
training:	Epoch: [21][478/817]	Loss 0.0080 (0.0490)	
training:	Epoch: [21][479/817]	Loss 0.0087 (0.0489)	
training:	Epoch: [21][480/817]	Loss 0.0089 (0.0488)	
training:	Epoch: [21][481/817]	Loss 0.0102 (0.0487)	
training:	Epoch: [21][482/817]	Loss 0.0087 (0.0486)	
training:	Epoch: [21][483/817]	Loss 0.0087 (0.0486)	
training:	Epoch: [21][484/817]	Loss 0.0086 (0.0485)	
training:	Epoch: [21][485/817]	Loss 0.5913 (0.0496)	
training:	Epoch: [21][486/817]	Loss 0.0101 (0.0495)	
training:	Epoch: [21][487/817]	Loss 0.0171 (0.0494)	
training:	Epoch: [21][488/817]	Loss 0.5911 (0.0506)	
training:	Epoch: [21][489/817]	Loss 0.0134 (0.0505)	
training:	Epoch: [21][490/817]	Loss 0.0092 (0.0504)	
training:	Epoch: [21][491/817]	Loss 0.0082 (0.0503)	
training:	Epoch: [21][492/817]	Loss 0.0100 (0.0502)	
training:	Epoch: [21][493/817]	Loss 0.0129 (0.0502)	
training:	Epoch: [21][494/817]	Loss 0.0080 (0.0501)	
training:	Epoch: [21][495/817]	Loss 0.0082 (0.0500)	
training:	Epoch: [21][496/817]	Loss 0.0092 (0.0499)	
training:	Epoch: [21][497/817]	Loss 0.0109 (0.0498)	
training:	Epoch: [21][498/817]	Loss 0.0076 (0.0497)	
training:	Epoch: [21][499/817]	Loss 0.0091 (0.0497)	
training:	Epoch: [21][500/817]	Loss 0.0082 (0.0496)	
training:	Epoch: [21][501/817]	Loss 0.0082 (0.0495)	
training:	Epoch: [21][502/817]	Loss 0.0091 (0.0494)	
training:	Epoch: [21][503/817]	Loss 0.0114 (0.0493)	
training:	Epoch: [21][504/817]	Loss 0.0096 (0.0493)	
training:	Epoch: [21][505/817]	Loss 0.0086 (0.0492)	
training:	Epoch: [21][506/817]	Loss 0.0094 (0.0491)	
training:	Epoch: [21][507/817]	Loss 0.0242 (0.0490)	
training:	Epoch: [21][508/817]	Loss 0.0093 (0.0490)	
training:	Epoch: [21][509/817]	Loss 0.0083 (0.0489)	
training:	Epoch: [21][510/817]	Loss 0.0094 (0.0488)	
training:	Epoch: [21][511/817]	Loss 0.0104 (0.0487)	
training:	Epoch: [21][512/817]	Loss 0.0080 (0.0487)	
training:	Epoch: [21][513/817]	Loss 0.0087 (0.0486)	
training:	Epoch: [21][514/817]	Loss 0.0106 (0.0485)	
training:	Epoch: [21][515/817]	Loss 0.0113 (0.0484)	
training:	Epoch: [21][516/817]	Loss 0.0095 (0.0484)	
training:	Epoch: [21][517/817]	Loss 0.0093 (0.0483)	
training:	Epoch: [21][518/817]	Loss 0.0084 (0.0482)	
training:	Epoch: [21][519/817]	Loss 0.0101 (0.0481)	
training:	Epoch: [21][520/817]	Loss 0.0092 (0.0481)	
training:	Epoch: [21][521/817]	Loss 0.0081 (0.0480)	
training:	Epoch: [21][522/817]	Loss 0.0083 (0.0479)	
training:	Epoch: [21][523/817]	Loss 0.0467 (0.0479)	
training:	Epoch: [21][524/817]	Loss 0.0089 (0.0478)	
training:	Epoch: [21][525/817]	Loss 0.0181 (0.0478)	
training:	Epoch: [21][526/817]	Loss 0.0097 (0.0477)	
training:	Epoch: [21][527/817]	Loss 0.0133 (0.0476)	
training:	Epoch: [21][528/817]	Loss 0.5893 (0.0487)	
training:	Epoch: [21][529/817]	Loss 0.0078 (0.0486)	
training:	Epoch: [21][530/817]	Loss 0.0078 (0.0485)	
training:	Epoch: [21][531/817]	Loss 0.5648 (0.0495)	
training:	Epoch: [21][532/817]	Loss 0.0090 (0.0494)	
training:	Epoch: [21][533/817]	Loss 0.3854 (0.0500)	
training:	Epoch: [21][534/817]	Loss 0.0088 (0.0500)	
training:	Epoch: [21][535/817]	Loss 0.0081 (0.0499)	
training:	Epoch: [21][536/817]	Loss 0.0174 (0.0498)	
training:	Epoch: [21][537/817]	Loss 0.0087 (0.0497)	
training:	Epoch: [21][538/817]	Loss 0.0087 (0.0497)	
training:	Epoch: [21][539/817]	Loss 0.0085 (0.0496)	
training:	Epoch: [21][540/817]	Loss 0.0086 (0.0495)	
training:	Epoch: [21][541/817]	Loss 0.0076 (0.0494)	
training:	Epoch: [21][542/817]	Loss 0.0078 (0.0494)	
training:	Epoch: [21][543/817]	Loss 0.5965 (0.0504)	
training:	Epoch: [21][544/817]	Loss 0.0103 (0.0503)	
training:	Epoch: [21][545/817]	Loss 0.0118 (0.0502)	
training:	Epoch: [21][546/817]	Loss 0.0079 (0.0501)	
training:	Epoch: [21][547/817]	Loss 0.0099 (0.0501)	
training:	Epoch: [21][548/817]	Loss 0.0112 (0.0500)	
training:	Epoch: [21][549/817]	Loss 0.0079 (0.0499)	
training:	Epoch: [21][550/817]	Loss 0.0098 (0.0498)	
training:	Epoch: [21][551/817]	Loss 0.0396 (0.0498)	
training:	Epoch: [21][552/817]	Loss 0.0082 (0.0498)	
training:	Epoch: [21][553/817]	Loss 0.0090 (0.0497)	
training:	Epoch: [21][554/817]	Loss 0.0096 (0.0496)	
training:	Epoch: [21][555/817]	Loss 0.0085 (0.0495)	
training:	Epoch: [21][556/817]	Loss 0.0082 (0.0495)	
training:	Epoch: [21][557/817]	Loss 0.0082 (0.0494)	
training:	Epoch: [21][558/817]	Loss 0.0100 (0.0493)	
training:	Epoch: [21][559/817]	Loss 0.0087 (0.0492)	
training:	Epoch: [21][560/817]	Loss 0.0088 (0.0492)	
training:	Epoch: [21][561/817]	Loss 0.0238 (0.0491)	
training:	Epoch: [21][562/817]	Loss 0.0086 (0.0490)	
training:	Epoch: [21][563/817]	Loss 0.0090 (0.0490)	
training:	Epoch: [21][564/817]	Loss 0.0093 (0.0489)	
training:	Epoch: [21][565/817]	Loss 0.0094 (0.0488)	
training:	Epoch: [21][566/817]	Loss 0.0077 (0.0488)	
training:	Epoch: [21][567/817]	Loss 0.0086 (0.0487)	
training:	Epoch: [21][568/817]	Loss 0.0081 (0.0486)	
training:	Epoch: [21][569/817]	Loss 0.5513 (0.0495)	
training:	Epoch: [21][570/817]	Loss 0.0093 (0.0494)	
training:	Epoch: [21][571/817]	Loss 0.0086 (0.0494)	
training:	Epoch: [21][572/817]	Loss 0.0176 (0.0493)	
training:	Epoch: [21][573/817]	Loss 0.0081 (0.0492)	
training:	Epoch: [21][574/817]	Loss 0.0080 (0.0492)	
training:	Epoch: [21][575/817]	Loss 0.0083 (0.0491)	
training:	Epoch: [21][576/817]	Loss 0.0089 (0.0490)	
training:	Epoch: [21][577/817]	Loss 0.0098 (0.0490)	
training:	Epoch: [21][578/817]	Loss 0.0095 (0.0489)	
training:	Epoch: [21][579/817]	Loss 0.0089 (0.0488)	
training:	Epoch: [21][580/817]	Loss 0.0088 (0.0488)	
training:	Epoch: [21][581/817]	Loss 0.0103 (0.0487)	
training:	Epoch: [21][582/817]	Loss 0.0091 (0.0486)	
training:	Epoch: [21][583/817]	Loss 0.0097 (0.0485)	
training:	Epoch: [21][584/817]	Loss 0.0072 (0.0485)	
training:	Epoch: [21][585/817]	Loss 0.5783 (0.0494)	
training:	Epoch: [21][586/817]	Loss 0.0082 (0.0493)	
training:	Epoch: [21][587/817]	Loss 0.0088 (0.0492)	
training:	Epoch: [21][588/817]	Loss 0.0108 (0.0492)	
training:	Epoch: [21][589/817]	Loss 0.0086 (0.0491)	
training:	Epoch: [21][590/817]	Loss 0.0461 (0.0491)	
training:	Epoch: [21][591/817]	Loss 0.0103 (0.0490)	
training:	Epoch: [21][592/817]	Loss 0.0162 (0.0490)	
training:	Epoch: [21][593/817]	Loss 0.6270 (0.0500)	
training:	Epoch: [21][594/817]	Loss 0.0123 (0.0499)	
training:	Epoch: [21][595/817]	Loss 0.0078 (0.0498)	
training:	Epoch: [21][596/817]	Loss 0.0089 (0.0498)	
training:	Epoch: [21][597/817]	Loss 0.0093 (0.0497)	
training:	Epoch: [21][598/817]	Loss 0.0139 (0.0496)	
training:	Epoch: [21][599/817]	Loss 0.0087 (0.0496)	
training:	Epoch: [21][600/817]	Loss 0.0080 (0.0495)	
training:	Epoch: [21][601/817]	Loss 0.0081 (0.0494)	
training:	Epoch: [21][602/817]	Loss 0.0079 (0.0494)	
training:	Epoch: [21][603/817]	Loss 0.0101 (0.0493)	
training:	Epoch: [21][604/817]	Loss 0.0080 (0.0492)	
training:	Epoch: [21][605/817]	Loss 0.0094 (0.0492)	
training:	Epoch: [21][606/817]	Loss 0.0080 (0.0491)	
training:	Epoch: [21][607/817]	Loss 0.0087 (0.0490)	
training:	Epoch: [21][608/817]	Loss 0.0107 (0.0490)	
training:	Epoch: [21][609/817]	Loss 0.0087 (0.0489)	
training:	Epoch: [21][610/817]	Loss 0.0210 (0.0488)	
training:	Epoch: [21][611/817]	Loss 0.0091 (0.0488)	
training:	Epoch: [21][612/817]	Loss 0.6198 (0.0497)	
training:	Epoch: [21][613/817]	Loss 0.0115 (0.0497)	
training:	Epoch: [21][614/817]	Loss 0.0090 (0.0496)	
training:	Epoch: [21][615/817]	Loss 0.0090 (0.0495)	
training:	Epoch: [21][616/817]	Loss 0.0097 (0.0495)	
training:	Epoch: [21][617/817]	Loss 0.0085 (0.0494)	
training:	Epoch: [21][618/817]	Loss 0.1218 (0.0495)	
training:	Epoch: [21][619/817]	Loss 0.0089 (0.0494)	
training:	Epoch: [21][620/817]	Loss 0.0085 (0.0494)	
training:	Epoch: [21][621/817]	Loss 0.0098 (0.0493)	
training:	Epoch: [21][622/817]	Loss 0.0111 (0.0492)	
training:	Epoch: [21][623/817]	Loss 0.0105 (0.0492)	
training:	Epoch: [21][624/817]	Loss 0.0091 (0.0491)	
training:	Epoch: [21][625/817]	Loss 0.0113 (0.0491)	
training:	Epoch: [21][626/817]	Loss 0.0081 (0.0490)	
training:	Epoch: [21][627/817]	Loss 0.0075 (0.0489)	
training:	Epoch: [21][628/817]	Loss 0.5810 (0.0498)	
training:	Epoch: [21][629/817]	Loss 0.0085 (0.0497)	
training:	Epoch: [21][630/817]	Loss 0.0092 (0.0496)	
training:	Epoch: [21][631/817]	Loss 0.0087 (0.0496)	
training:	Epoch: [21][632/817]	Loss 0.6215 (0.0505)	
training:	Epoch: [21][633/817]	Loss 0.0094 (0.0504)	
training:	Epoch: [21][634/817]	Loss 0.0089 (0.0504)	
training:	Epoch: [21][635/817]	Loss 0.0075 (0.0503)	
training:	Epoch: [21][636/817]	Loss 0.0103 (0.0502)	
training:	Epoch: [21][637/817]	Loss 0.0089 (0.0502)	
training:	Epoch: [21][638/817]	Loss 0.2335 (0.0504)	
training:	Epoch: [21][639/817]	Loss 0.0718 (0.0505)	
training:	Epoch: [21][640/817]	Loss 0.3964 (0.0510)	
training:	Epoch: [21][641/817]	Loss 0.0091 (0.0510)	
training:	Epoch: [21][642/817]	Loss 0.0092 (0.0509)	
training:	Epoch: [21][643/817]	Loss 0.0083 (0.0508)	
training:	Epoch: [21][644/817]	Loss 0.0131 (0.0508)	
training:	Epoch: [21][645/817]	Loss 0.0079 (0.0507)	
training:	Epoch: [21][646/817]	Loss 0.0080 (0.0506)	
training:	Epoch: [21][647/817]	Loss 0.0238 (0.0506)	
training:	Epoch: [21][648/817]	Loss 0.0082 (0.0505)	
training:	Epoch: [21][649/817]	Loss 0.0084 (0.0505)	
training:	Epoch: [21][650/817]	Loss 0.0097 (0.0504)	
training:	Epoch: [21][651/817]	Loss 0.3095 (0.0508)	
training:	Epoch: [21][652/817]	Loss 0.0483 (0.0508)	
training:	Epoch: [21][653/817]	Loss 0.0282 (0.0508)	
training:	Epoch: [21][654/817]	Loss 0.0080 (0.0507)	
training:	Epoch: [21][655/817]	Loss 0.6136 (0.0516)	
training:	Epoch: [21][656/817]	Loss 0.0085 (0.0515)	
training:	Epoch: [21][657/817]	Loss 0.5789 (0.0523)	
training:	Epoch: [21][658/817]	Loss 0.0082 (0.0522)	
training:	Epoch: [21][659/817]	Loss 0.0421 (0.0522)	
training:	Epoch: [21][660/817]	Loss 0.0085 (0.0521)	
training:	Epoch: [21][661/817]	Loss 0.0107 (0.0521)	
training:	Epoch: [21][662/817]	Loss 0.0101 (0.0520)	
training:	Epoch: [21][663/817]	Loss 0.0098 (0.0520)	
training:	Epoch: [21][664/817]	Loss 0.0079 (0.0519)	
training:	Epoch: [21][665/817]	Loss 0.0081 (0.0518)	
training:	Epoch: [21][666/817]	Loss 0.0090 (0.0518)	
training:	Epoch: [21][667/817]	Loss 0.0082 (0.0517)	
training:	Epoch: [21][668/817]	Loss 0.0076 (0.0516)	
training:	Epoch: [21][669/817]	Loss 0.0087 (0.0516)	
training:	Epoch: [21][670/817]	Loss 0.0076 (0.0515)	
training:	Epoch: [21][671/817]	Loss 0.0078 (0.0514)	
training:	Epoch: [21][672/817]	Loss 0.0274 (0.0514)	
training:	Epoch: [21][673/817]	Loss 0.0089 (0.0513)	
training:	Epoch: [21][674/817]	Loss 0.0086 (0.0513)	
training:	Epoch: [21][675/817]	Loss 0.0087 (0.0512)	
training:	Epoch: [21][676/817]	Loss 0.0086 (0.0511)	
training:	Epoch: [21][677/817]	Loss 0.0102 (0.0511)	
training:	Epoch: [21][678/817]	Loss 0.0089 (0.0510)	
training:	Epoch: [21][679/817]	Loss 0.8485 (0.0522)	
training:	Epoch: [21][680/817]	Loss 0.0101 (0.0521)	
training:	Epoch: [21][681/817]	Loss 0.0125 (0.0521)	
training:	Epoch: [21][682/817]	Loss 0.0083 (0.0520)	
training:	Epoch: [21][683/817]	Loss 0.0083 (0.0519)	
training:	Epoch: [21][684/817]	Loss 0.0107 (0.0519)	
training:	Epoch: [21][685/817]	Loss 0.0088 (0.0518)	
training:	Epoch: [21][686/817]	Loss 0.0125 (0.0518)	
training:	Epoch: [21][687/817]	Loss 0.0087 (0.0517)	
training:	Epoch: [21][688/817]	Loss 0.0204 (0.0517)	
training:	Epoch: [21][689/817]	Loss 0.0079 (0.0516)	
training:	Epoch: [21][690/817]	Loss 0.0108 (0.0515)	
training:	Epoch: [21][691/817]	Loss 0.0081 (0.0515)	
training:	Epoch: [21][692/817]	Loss 0.1806 (0.0517)	
training:	Epoch: [21][693/817]	Loss 0.0092 (0.0516)	
training:	Epoch: [21][694/817]	Loss 0.0088 (0.0515)	
training:	Epoch: [21][695/817]	Loss 0.0097 (0.0515)	
training:	Epoch: [21][696/817]	Loss 0.0090 (0.0514)	
training:	Epoch: [21][697/817]	Loss 0.0084 (0.0514)	
training:	Epoch: [21][698/817]	Loss 0.0084 (0.0513)	
training:	Epoch: [21][699/817]	Loss 0.0094 (0.0512)	
training:	Epoch: [21][700/817]	Loss 0.1153 (0.0513)	
training:	Epoch: [21][701/817]	Loss 0.0087 (0.0513)	
training:	Epoch: [21][702/817]	Loss 0.0088 (0.0512)	
training:	Epoch: [21][703/817]	Loss 0.5570 (0.0519)	
training:	Epoch: [21][704/817]	Loss 0.0080 (0.0519)	
training:	Epoch: [21][705/817]	Loss 0.0084 (0.0518)	
training:	Epoch: [21][706/817]	Loss 0.0088 (0.0517)	
training:	Epoch: [21][707/817]	Loss 0.0137 (0.0517)	
training:	Epoch: [21][708/817]	Loss 0.0089 (0.0516)	
training:	Epoch: [21][709/817]	Loss 0.1136 (0.0517)	
training:	Epoch: [21][710/817]	Loss 0.0102 (0.0517)	
training:	Epoch: [21][711/817]	Loss 0.0083 (0.0516)	
training:	Epoch: [21][712/817]	Loss 0.0090 (0.0515)	
training:	Epoch: [21][713/817]	Loss 0.2948 (0.0519)	
training:	Epoch: [21][714/817]	Loss 0.1611 (0.0520)	
training:	Epoch: [21][715/817]	Loss 0.0081 (0.0520)	
training:	Epoch: [21][716/817]	Loss 0.0147 (0.0519)	
training:	Epoch: [21][717/817]	Loss 0.0100 (0.0519)	
training:	Epoch: [21][718/817]	Loss 0.0082 (0.0518)	
training:	Epoch: [21][719/817]	Loss 0.0079 (0.0517)	
training:	Epoch: [21][720/817]	Loss 0.0081 (0.0517)	
training:	Epoch: [21][721/817]	Loss 0.0112 (0.0516)	
training:	Epoch: [21][722/817]	Loss 0.0083 (0.0516)	
training:	Epoch: [21][723/817]	Loss 0.0086 (0.0515)	
training:	Epoch: [21][724/817]	Loss 0.0090 (0.0514)	
training:	Epoch: [21][725/817]	Loss 0.0257 (0.0514)	
training:	Epoch: [21][726/817]	Loss 0.0093 (0.0513)	
training:	Epoch: [21][727/817]	Loss 0.4391 (0.0519)	
training:	Epoch: [21][728/817]	Loss 0.0083 (0.0518)	
training:	Epoch: [21][729/817]	Loss 0.0643 (0.0518)	
training:	Epoch: [21][730/817]	Loss 0.0857 (0.0519)	
training:	Epoch: [21][731/817]	Loss 0.0086 (0.0518)	
training:	Epoch: [21][732/817]	Loss 0.0087 (0.0518)	
training:	Epoch: [21][733/817]	Loss 0.6003 (0.0525)	
training:	Epoch: [21][734/817]	Loss 0.0327 (0.0525)	
training:	Epoch: [21][735/817]	Loss 0.0088 (0.0524)	
training:	Epoch: [21][736/817]	Loss 0.0086 (0.0524)	
training:	Epoch: [21][737/817]	Loss 0.0090 (0.0523)	
training:	Epoch: [21][738/817]	Loss 0.6296 (0.0531)	
training:	Epoch: [21][739/817]	Loss 0.0098 (0.0530)	
training:	Epoch: [21][740/817]	Loss 0.0076 (0.0530)	
training:	Epoch: [21][741/817]	Loss 0.0082 (0.0529)	
training:	Epoch: [21][742/817]	Loss 0.0108 (0.0528)	
training:	Epoch: [21][743/817]	Loss 0.0086 (0.0528)	
training:	Epoch: [21][744/817]	Loss 0.5791 (0.0535)	
training:	Epoch: [21][745/817]	Loss 0.0103 (0.0534)	
training:	Epoch: [21][746/817]	Loss 0.1194 (0.0535)	
training:	Epoch: [21][747/817]	Loss 0.0086 (0.0535)	
training:	Epoch: [21][748/817]	Loss 0.0097 (0.0534)	
training:	Epoch: [21][749/817]	Loss 0.0107 (0.0534)	
training:	Epoch: [21][750/817]	Loss 0.0084 (0.0533)	
training:	Epoch: [21][751/817]	Loss 0.0082 (0.0532)	
training:	Epoch: [21][752/817]	Loss 0.0105 (0.0532)	
training:	Epoch: [21][753/817]	Loss 0.0091 (0.0531)	
training:	Epoch: [21][754/817]	Loss 0.0087 (0.0531)	
training:	Epoch: [21][755/817]	Loss 0.0079 (0.0530)	
training:	Epoch: [21][756/817]	Loss 0.0095 (0.0529)	
training:	Epoch: [21][757/817]	Loss 0.0088 (0.0529)	
training:	Epoch: [21][758/817]	Loss 0.0082 (0.0528)	
training:	Epoch: [21][759/817]	Loss 0.0078 (0.0528)	
training:	Epoch: [21][760/817]	Loss 0.0115 (0.0527)	
training:	Epoch: [21][761/817]	Loss 0.0088 (0.0527)	
training:	Epoch: [21][762/817]	Loss 0.6071 (0.0534)	
training:	Epoch: [21][763/817]	Loss 0.0093 (0.0533)	
training:	Epoch: [21][764/817]	Loss 0.0081 (0.0533)	
training:	Epoch: [21][765/817]	Loss 0.0088 (0.0532)	
training:	Epoch: [21][766/817]	Loss 0.0094 (0.0531)	
training:	Epoch: [21][767/817]	Loss 0.0349 (0.0531)	
training:	Epoch: [21][768/817]	Loss 0.0161 (0.0531)	
training:	Epoch: [21][769/817]	Loss 0.0098 (0.0530)	
training:	Epoch: [21][770/817]	Loss 0.6110 (0.0537)	
training:	Epoch: [21][771/817]	Loss 0.5921 (0.0544)	
training:	Epoch: [21][772/817]	Loss 0.0094 (0.0544)	
training:	Epoch: [21][773/817]	Loss 0.0079 (0.0543)	
training:	Epoch: [21][774/817]	Loss 0.0086 (0.0543)	
training:	Epoch: [21][775/817]	Loss 0.0125 (0.0542)	
training:	Epoch: [21][776/817]	Loss 0.0105 (0.0542)	
training:	Epoch: [21][777/817]	Loss 0.0079 (0.0541)	
training:	Epoch: [21][778/817]	Loss 0.0094 (0.0540)	
training:	Epoch: [21][779/817]	Loss 0.5417 (0.0547)	
training:	Epoch: [21][780/817]	Loss 0.0094 (0.0546)	
training:	Epoch: [21][781/817]	Loss 0.0082 (0.0545)	
training:	Epoch: [21][782/817]	Loss 0.0088 (0.0545)	
training:	Epoch: [21][783/817]	Loss 0.0092 (0.0544)	
training:	Epoch: [21][784/817]	Loss 0.0084 (0.0544)	
training:	Epoch: [21][785/817]	Loss 0.0139 (0.0543)	
training:	Epoch: [21][786/817]	Loss 0.0109 (0.0543)	
training:	Epoch: [21][787/817]	Loss 0.0082 (0.0542)	
training:	Epoch: [21][788/817]	Loss 0.0081 (0.0541)	
training:	Epoch: [21][789/817]	Loss 0.0078 (0.0541)	
training:	Epoch: [21][790/817]	Loss 0.0094 (0.0540)	
training:	Epoch: [21][791/817]	Loss 0.0095 (0.0540)	
training:	Epoch: [21][792/817]	Loss 0.0079 (0.0539)	
training:	Epoch: [21][793/817]	Loss 0.0088 (0.0539)	
training:	Epoch: [21][794/817]	Loss 0.0095 (0.0538)	
training:	Epoch: [21][795/817]	Loss 0.0089 (0.0537)	
training:	Epoch: [21][796/817]	Loss 0.0081 (0.0537)	
training:	Epoch: [21][797/817]	Loss 0.0084 (0.0536)	
training:	Epoch: [21][798/817]	Loss 0.0098 (0.0536)	
training:	Epoch: [21][799/817]	Loss 0.0092 (0.0535)	
training:	Epoch: [21][800/817]	Loss 0.0090 (0.0535)	
training:	Epoch: [21][801/817]	Loss 0.0363 (0.0534)	
training:	Epoch: [21][802/817]	Loss 0.0085 (0.0534)	
training:	Epoch: [21][803/817]	Loss 0.0083 (0.0533)	
training:	Epoch: [21][804/817]	Loss 0.0089 (0.0533)	
training:	Epoch: [21][805/817]	Loss 0.0081 (0.0532)	
training:	Epoch: [21][806/817]	Loss 0.0159 (0.0532)	
training:	Epoch: [21][807/817]	Loss 0.0084 (0.0531)	
training:	Epoch: [21][808/817]	Loss 0.0084 (0.0531)	
training:	Epoch: [21][809/817]	Loss 0.0084 (0.0530)	
training:	Epoch: [21][810/817]	Loss 0.0105 (0.0530)	
training:	Epoch: [21][811/817]	Loss 0.0085 (0.0529)	
training:	Epoch: [21][812/817]	Loss 0.0175 (0.0529)	
training:	Epoch: [21][813/817]	Loss 0.0082 (0.0528)	
training:	Epoch: [21][814/817]	Loss 0.0086 (0.0527)	
training:	Epoch: [21][815/817]	Loss 0.0088 (0.0527)	
training:	Epoch: [21][816/817]	Loss 0.0084 (0.0526)	
training:	Epoch: [21][817/817]	Loss 0.0085 (0.0526)	
Training:	 Loss: 0.0526

Training:	 ACC: 0.9914 0.9914 0.9918 0.9911
Validation:	 ACC: 0.7934 0.7945 0.8188 0.7679
Validation:	 Best_BACC: 0.8223 0.8218 0.8127 0.8318
Validation:	 Loss: 0.8522
Pretraining:	Epoch 22/200
----------
training:	Epoch: [22][1/817]	Loss 0.0089 (0.0089)	
training:	Epoch: [22][2/817]	Loss 0.0088 (0.0089)	
training:	Epoch: [22][3/817]	Loss 0.0089 (0.0089)	
training:	Epoch: [22][4/817]	Loss 0.0079 (0.0086)	
training:	Epoch: [22][5/817]	Loss 0.0094 (0.0088)	
training:	Epoch: [22][6/817]	Loss 0.0087 (0.0088)	
training:	Epoch: [22][7/817]	Loss 0.5082 (0.0801)	
training:	Epoch: [22][8/817]	Loss 0.0087 (0.0712)	
training:	Epoch: [22][9/817]	Loss 0.0103 (0.0644)	
training:	Epoch: [22][10/817]	Loss 0.5884 (0.1168)	
training:	Epoch: [22][11/817]	Loss 0.0081 (0.1069)	
training:	Epoch: [22][12/817]	Loss 0.0087 (0.0987)	
training:	Epoch: [22][13/817]	Loss 0.0084 (0.0918)	
training:	Epoch: [22][14/817]	Loss 0.0091 (0.0859)	
training:	Epoch: [22][15/817]	Loss 0.0090 (0.0808)	
training:	Epoch: [22][16/817]	Loss 0.0105 (0.0764)	
training:	Epoch: [22][17/817]	Loss 0.0139 (0.0727)	
training:	Epoch: [22][18/817]	Loss 0.0099 (0.0692)	
training:	Epoch: [22][19/817]	Loss 0.0238 (0.0668)	
training:	Epoch: [22][20/817]	Loss 0.0123 (0.0641)	
training:	Epoch: [22][21/817]	Loss 0.0098 (0.0615)	
training:	Epoch: [22][22/817]	Loss 0.0092 (0.0591)	
training:	Epoch: [22][23/817]	Loss 0.0081 (0.0569)	
training:	Epoch: [22][24/817]	Loss 0.0108 (0.0550)	
training:	Epoch: [22][25/817]	Loss 0.0089 (0.0531)	
training:	Epoch: [22][26/817]	Loss 0.0080 (0.0514)	
training:	Epoch: [22][27/817]	Loss 0.0089 (0.0498)	
training:	Epoch: [22][28/817]	Loss 0.0084 (0.0484)	
training:	Epoch: [22][29/817]	Loss 0.0092 (0.0470)	
training:	Epoch: [22][30/817]	Loss 0.0081 (0.0457)	
training:	Epoch: [22][31/817]	Loss 0.0082 (0.0445)	
training:	Epoch: [22][32/817]	Loss 0.0097 (0.0434)	
training:	Epoch: [22][33/817]	Loss 0.0081 (0.0423)	
training:	Epoch: [22][34/817]	Loss 0.0082 (0.0413)	
training:	Epoch: [22][35/817]	Loss 0.0086 (0.0404)	
training:	Epoch: [22][36/817]	Loss 0.0085 (0.0395)	
training:	Epoch: [22][37/817]	Loss 0.0089 (0.0387)	
training:	Epoch: [22][38/817]	Loss 0.0079 (0.0379)	
training:	Epoch: [22][39/817]	Loss 0.0087 (0.0371)	
training:	Epoch: [22][40/817]	Loss 0.6193 (0.0517)	
training:	Epoch: [22][41/817]	Loss 0.0087 (0.0506)	
training:	Epoch: [22][42/817]	Loss 0.0086 (0.0496)	
training:	Epoch: [22][43/817]	Loss 0.0074 (0.0487)	
training:	Epoch: [22][44/817]	Loss 0.0099 (0.0478)	
training:	Epoch: [22][45/817]	Loss 0.5570 (0.0591)	
training:	Epoch: [22][46/817]	Loss 0.0094 (0.0580)	
training:	Epoch: [22][47/817]	Loss 0.0098 (0.0570)	
training:	Epoch: [22][48/817]	Loss 0.0080 (0.0560)	
training:	Epoch: [22][49/817]	Loss 0.0085 (0.0550)	
training:	Epoch: [22][50/817]	Loss 0.0085 (0.0541)	
training:	Epoch: [22][51/817]	Loss 0.0077 (0.0532)	
training:	Epoch: [22][52/817]	Loss 0.0091 (0.0523)	
training:	Epoch: [22][53/817]	Loss 0.0084 (0.0515)	
training:	Epoch: [22][54/817]	Loss 0.0089 (0.0507)	
training:	Epoch: [22][55/817]	Loss 0.0080 (0.0499)	
training:	Epoch: [22][56/817]	Loss 0.0084 (0.0492)	
training:	Epoch: [22][57/817]	Loss 0.0078 (0.0484)	
training:	Epoch: [22][58/817]	Loss 0.0076 (0.0477)	
training:	Epoch: [22][59/817]	Loss 0.0097 (0.0471)	
training:	Epoch: [22][60/817]	Loss 0.0086 (0.0465)	
training:	Epoch: [22][61/817]	Loss 0.0101 (0.0459)	
training:	Epoch: [22][62/817]	Loss 0.0084 (0.0453)	
training:	Epoch: [22][63/817]	Loss 0.0089 (0.0447)	
training:	Epoch: [22][64/817]	Loss 0.0272 (0.0444)	
training:	Epoch: [22][65/817]	Loss 0.0089 (0.0439)	
training:	Epoch: [22][66/817]	Loss 0.0106 (0.0434)	
training:	Epoch: [22][67/817]	Loss 0.0086 (0.0428)	
training:	Epoch: [22][68/817]	Loss 0.0079 (0.0423)	
training:	Epoch: [22][69/817]	Loss 0.0102 (0.0419)	
training:	Epoch: [22][70/817]	Loss 0.0094 (0.0414)	
training:	Epoch: [22][71/817]	Loss 0.0081 (0.0409)	
training:	Epoch: [22][72/817]	Loss 0.0095 (0.0405)	
training:	Epoch: [22][73/817]	Loss 0.0100 (0.0401)	
training:	Epoch: [22][74/817]	Loss 0.0084 (0.0396)	
training:	Epoch: [22][75/817]	Loss 0.0090 (0.0392)	
training:	Epoch: [22][76/817]	Loss 0.0085 (0.0388)	
training:	Epoch: [22][77/817]	Loss 0.0080 (0.0384)	
training:	Epoch: [22][78/817]	Loss 0.0096 (0.0381)	
training:	Epoch: [22][79/817]	Loss 0.0092 (0.0377)	
training:	Epoch: [22][80/817]	Loss 0.0097 (0.0373)	
training:	Epoch: [22][81/817]	Loss 0.0086 (0.0370)	
training:	Epoch: [22][82/817]	Loss 0.0082 (0.0366)	
training:	Epoch: [22][83/817]	Loss 0.0235 (0.0365)	
training:	Epoch: [22][84/817]	Loss 0.0087 (0.0362)	
training:	Epoch: [22][85/817]	Loss 0.0141 (0.0359)	
training:	Epoch: [22][86/817]	Loss 0.0111 (0.0356)	
training:	Epoch: [22][87/817]	Loss 0.0115 (0.0353)	
training:	Epoch: [22][88/817]	Loss 0.0124 (0.0351)	
training:	Epoch: [22][89/817]	Loss 0.0091 (0.0348)	
training:	Epoch: [22][90/817]	Loss 0.0079 (0.0345)	
training:	Epoch: [22][91/817]	Loss 0.0098 (0.0342)	
training:	Epoch: [22][92/817]	Loss 0.0071 (0.0339)	
training:	Epoch: [22][93/817]	Loss 0.0086 (0.0336)	
training:	Epoch: [22][94/817]	Loss 0.0073 (0.0334)	
training:	Epoch: [22][95/817]	Loss 0.0984 (0.0340)	
training:	Epoch: [22][96/817]	Loss 0.0079 (0.0338)	
training:	Epoch: [22][97/817]	Loss 0.0283 (0.0337)	
training:	Epoch: [22][98/817]	Loss 0.0087 (0.0335)	
training:	Epoch: [22][99/817]	Loss 0.0087 (0.0332)	
training:	Epoch: [22][100/817]	Loss 0.0086 (0.0330)	
training:	Epoch: [22][101/817]	Loss 0.0089 (0.0327)	
training:	Epoch: [22][102/817]	Loss 0.0087 (0.0325)	
training:	Epoch: [22][103/817]	Loss 0.0131 (0.0323)	
training:	Epoch: [22][104/817]	Loss 0.0096 (0.0321)	
training:	Epoch: [22][105/817]	Loss 0.0087 (0.0319)	
training:	Epoch: [22][106/817]	Loss 0.0095 (0.0316)	
training:	Epoch: [22][107/817]	Loss 0.0081 (0.0314)	
training:	Epoch: [22][108/817]	Loss 0.0195 (0.0313)	
training:	Epoch: [22][109/817]	Loss 0.0092 (0.0311)	
training:	Epoch: [22][110/817]	Loss 0.0078 (0.0309)	
training:	Epoch: [22][111/817]	Loss 0.0074 (0.0307)	
training:	Epoch: [22][112/817]	Loss 0.0083 (0.0305)	
training:	Epoch: [22][113/817]	Loss 0.0075 (0.0303)	
training:	Epoch: [22][114/817]	Loss 0.0166 (0.0302)	
training:	Epoch: [22][115/817]	Loss 0.0105 (0.0300)	
training:	Epoch: [22][116/817]	Loss 0.0083 (0.0298)	
training:	Epoch: [22][117/817]	Loss 0.0084 (0.0296)	
training:	Epoch: [22][118/817]	Loss 0.0088 (0.0294)	
training:	Epoch: [22][119/817]	Loss 0.0099 (0.0293)	
training:	Epoch: [22][120/817]	Loss 0.0086 (0.0291)	
training:	Epoch: [22][121/817]	Loss 0.0088 (0.0289)	
training:	Epoch: [22][122/817]	Loss 0.0096 (0.0288)	
training:	Epoch: [22][123/817]	Loss 0.0086 (0.0286)	
training:	Epoch: [22][124/817]	Loss 0.0090 (0.0285)	
training:	Epoch: [22][125/817]	Loss 0.0104 (0.0283)	
training:	Epoch: [22][126/817]	Loss 0.0095 (0.0282)	
training:	Epoch: [22][127/817]	Loss 0.0076 (0.0280)	
training:	Epoch: [22][128/817]	Loss 0.0085 (0.0279)	
training:	Epoch: [22][129/817]	Loss 0.0091 (0.0277)	
training:	Epoch: [22][130/817]	Loss 0.0083 (0.0276)	
training:	Epoch: [22][131/817]	Loss 0.0084 (0.0274)	
training:	Epoch: [22][132/817]	Loss 0.0091 (0.0273)	
training:	Epoch: [22][133/817]	Loss 0.0083 (0.0271)	
training:	Epoch: [22][134/817]	Loss 0.0225 (0.0271)	
training:	Epoch: [22][135/817]	Loss 0.0079 (0.0270)	
training:	Epoch: [22][136/817]	Loss 0.0076 (0.0268)	
training:	Epoch: [22][137/817]	Loss 0.5754 (0.0308)	
training:	Epoch: [22][138/817]	Loss 0.0948 (0.0313)	
training:	Epoch: [22][139/817]	Loss 0.0087 (0.0311)	
training:	Epoch: [22][140/817]	Loss 0.0900 (0.0315)	
training:	Epoch: [22][141/817]	Loss 0.0080 (0.0314)	
training:	Epoch: [22][142/817]	Loss 0.0089 (0.0312)	
training:	Epoch: [22][143/817]	Loss 0.0159 (0.0311)	
training:	Epoch: [22][144/817]	Loss 0.0082 (0.0309)	
training:	Epoch: [22][145/817]	Loss 0.0078 (0.0308)	
training:	Epoch: [22][146/817]	Loss 0.6232 (0.0348)	
training:	Epoch: [22][147/817]	Loss 0.0076 (0.0347)	
training:	Epoch: [22][148/817]	Loss 0.0082 (0.0345)	
training:	Epoch: [22][149/817]	Loss 0.0079 (0.0343)	
training:	Epoch: [22][150/817]	Loss 0.6375 (0.0383)	
training:	Epoch: [22][151/817]	Loss 0.0082 (0.0381)	
training:	Epoch: [22][152/817]	Loss 0.0083 (0.0379)	
training:	Epoch: [22][153/817]	Loss 0.0090 (0.0377)	
training:	Epoch: [22][154/817]	Loss 0.2409 (0.0391)	
training:	Epoch: [22][155/817]	Loss 0.0154 (0.0389)	
training:	Epoch: [22][156/817]	Loss 0.0082 (0.0387)	
training:	Epoch: [22][157/817]	Loss 0.0077 (0.0385)	
training:	Epoch: [22][158/817]	Loss 0.0858 (0.0388)	
training:	Epoch: [22][159/817]	Loss 0.0091 (0.0386)	
training:	Epoch: [22][160/817]	Loss 0.0075 (0.0384)	
training:	Epoch: [22][161/817]	Loss 0.0084 (0.0382)	
training:	Epoch: [22][162/817]	Loss 0.0096 (0.0381)	
training:	Epoch: [22][163/817]	Loss 0.5537 (0.0412)	
training:	Epoch: [22][164/817]	Loss 0.0084 (0.0410)	
training:	Epoch: [22][165/817]	Loss 0.0080 (0.0408)	
training:	Epoch: [22][166/817]	Loss 0.0075 (0.0406)	
training:	Epoch: [22][167/817]	Loss 0.6740 (0.0444)	
training:	Epoch: [22][168/817]	Loss 0.6180 (0.0478)	
training:	Epoch: [22][169/817]	Loss 0.0080 (0.0476)	
training:	Epoch: [22][170/817]	Loss 0.0084 (0.0474)	
training:	Epoch: [22][171/817]	Loss 0.0080 (0.0471)	
training:	Epoch: [22][172/817]	Loss 0.0218 (0.0470)	
training:	Epoch: [22][173/817]	Loss 0.0114 (0.0468)	
training:	Epoch: [22][174/817]	Loss 0.0210 (0.0466)	
training:	Epoch: [22][175/817]	Loss 0.0080 (0.0464)	
training:	Epoch: [22][176/817]	Loss 0.0131 (0.0462)	
training:	Epoch: [22][177/817]	Loss 0.0083 (0.0460)	
training:	Epoch: [22][178/817]	Loss 0.4092 (0.0481)	
training:	Epoch: [22][179/817]	Loss 0.0096 (0.0478)	
training:	Epoch: [22][180/817]	Loss 0.0079 (0.0476)	
training:	Epoch: [22][181/817]	Loss 0.0089 (0.0474)	
training:	Epoch: [22][182/817]	Loss 0.0091 (0.0472)	
training:	Epoch: [22][183/817]	Loss 0.0086 (0.0470)	
training:	Epoch: [22][184/817]	Loss 0.0084 (0.0468)	
training:	Epoch: [22][185/817]	Loss 0.0085 (0.0466)	
training:	Epoch: [22][186/817]	Loss 0.0139 (0.0464)	
training:	Epoch: [22][187/817]	Loss 0.0088 (0.0462)	
training:	Epoch: [22][188/817]	Loss 0.0091 (0.0460)	
training:	Epoch: [22][189/817]	Loss 0.3202 (0.0474)	
training:	Epoch: [22][190/817]	Loss 0.0177 (0.0473)	
training:	Epoch: [22][191/817]	Loss 0.0115 (0.0471)	
training:	Epoch: [22][192/817]	Loss 0.0135 (0.0469)	
training:	Epoch: [22][193/817]	Loss 0.0111 (0.0467)	
training:	Epoch: [22][194/817]	Loss 0.0102 (0.0466)	
training:	Epoch: [22][195/817]	Loss 0.0113 (0.0464)	
training:	Epoch: [22][196/817]	Loss 0.0083 (0.0462)	
training:	Epoch: [22][197/817]	Loss 0.0084 (0.0460)	
training:	Epoch: [22][198/817]	Loss 0.0104 (0.0458)	
training:	Epoch: [22][199/817]	Loss 0.0081 (0.0456)	
training:	Epoch: [22][200/817]	Loss 0.0101 (0.0454)	
training:	Epoch: [22][201/817]	Loss 0.0156 (0.0453)	
training:	Epoch: [22][202/817]	Loss 0.0098 (0.0451)	
training:	Epoch: [22][203/817]	Loss 0.0093 (0.0449)	
training:	Epoch: [22][204/817]	Loss 0.0099 (0.0448)	
training:	Epoch: [22][205/817]	Loss 0.0080 (0.0446)	
training:	Epoch: [22][206/817]	Loss 0.0077 (0.0444)	
training:	Epoch: [22][207/817]	Loss 0.0946 (0.0446)	
training:	Epoch: [22][208/817]	Loss 0.0370 (0.0446)	
training:	Epoch: [22][209/817]	Loss 0.0097 (0.0444)	
training:	Epoch: [22][210/817]	Loss 0.0097 (0.0443)	
training:	Epoch: [22][211/817]	Loss 0.0088 (0.0441)	
training:	Epoch: [22][212/817]	Loss 0.0139 (0.0440)	
training:	Epoch: [22][213/817]	Loss 0.0138 (0.0438)	
training:	Epoch: [22][214/817]	Loss 0.0171 (0.0437)	
training:	Epoch: [22][215/817]	Loss 0.0090 (0.0435)	
training:	Epoch: [22][216/817]	Loss 0.0096 (0.0434)	
training:	Epoch: [22][217/817]	Loss 0.0087 (0.0432)	
training:	Epoch: [22][218/817]	Loss 0.0074 (0.0431)	
training:	Epoch: [22][219/817]	Loss 0.0082 (0.0429)	
training:	Epoch: [22][220/817]	Loss 0.0086 (0.0427)	
training:	Epoch: [22][221/817]	Loss 0.0083 (0.0426)	
training:	Epoch: [22][222/817]	Loss 0.0076 (0.0424)	
training:	Epoch: [22][223/817]	Loss 0.0171 (0.0423)	
training:	Epoch: [22][224/817]	Loss 0.0090 (0.0422)	
training:	Epoch: [22][225/817]	Loss 0.0099 (0.0420)	
training:	Epoch: [22][226/817]	Loss 0.0093 (0.0419)	
training:	Epoch: [22][227/817]	Loss 0.0089 (0.0417)	
training:	Epoch: [22][228/817]	Loss 0.1239 (0.0421)	
training:	Epoch: [22][229/817]	Loss 0.0098 (0.0420)	
training:	Epoch: [22][230/817]	Loss 0.0291 (0.0419)	
training:	Epoch: [22][231/817]	Loss 0.0087 (0.0418)	
training:	Epoch: [22][232/817]	Loss 0.0090 (0.0416)	
training:	Epoch: [22][233/817]	Loss 0.0089 (0.0415)	
training:	Epoch: [22][234/817]	Loss 0.0084 (0.0413)	
training:	Epoch: [22][235/817]	Loss 0.0079 (0.0412)	
training:	Epoch: [22][236/817]	Loss 0.0085 (0.0411)	
training:	Epoch: [22][237/817]	Loss 0.0093 (0.0409)	
training:	Epoch: [22][238/817]	Loss 0.5512 (0.0431)	
training:	Epoch: [22][239/817]	Loss 0.0079 (0.0429)	
training:	Epoch: [22][240/817]	Loss 0.0096 (0.0428)	
training:	Epoch: [22][241/817]	Loss 0.0108 (0.0426)	
training:	Epoch: [22][242/817]	Loss 0.0096 (0.0425)	
training:	Epoch: [22][243/817]	Loss 0.0077 (0.0424)	
training:	Epoch: [22][244/817]	Loss 0.0084 (0.0422)	
training:	Epoch: [22][245/817]	Loss 0.0090 (0.0421)	
training:	Epoch: [22][246/817]	Loss 0.0149 (0.0420)	
training:	Epoch: [22][247/817]	Loss 0.0082 (0.0418)	
training:	Epoch: [22][248/817]	Loss 0.0076 (0.0417)	
training:	Epoch: [22][249/817]	Loss 0.0155 (0.0416)	
training:	Epoch: [22][250/817]	Loss 0.0083 (0.0415)	
training:	Epoch: [22][251/817]	Loss 0.0076 (0.0413)	
training:	Epoch: [22][252/817]	Loss 0.0283 (0.0413)	
training:	Epoch: [22][253/817]	Loss 0.0086 (0.0411)	
training:	Epoch: [22][254/817]	Loss 0.0803 (0.0413)	
training:	Epoch: [22][255/817]	Loss 0.0081 (0.0412)	
training:	Epoch: [22][256/817]	Loss 0.0082 (0.0410)	
training:	Epoch: [22][257/817]	Loss 0.0115 (0.0409)	
training:	Epoch: [22][258/817]	Loss 0.0091 (0.0408)	
training:	Epoch: [22][259/817]	Loss 0.0091 (0.0407)	
training:	Epoch: [22][260/817]	Loss 0.0095 (0.0406)	
training:	Epoch: [22][261/817]	Loss 0.0081 (0.0404)	
training:	Epoch: [22][262/817]	Loss 0.0086 (0.0403)	
training:	Epoch: [22][263/817]	Loss 0.0073 (0.0402)	
training:	Epoch: [22][264/817]	Loss 0.0073 (0.0401)	
training:	Epoch: [22][265/817]	Loss 0.0097 (0.0400)	
training:	Epoch: [22][266/817]	Loss 0.0086 (0.0398)	
training:	Epoch: [22][267/817]	Loss 0.0084 (0.0397)	
training:	Epoch: [22][268/817]	Loss 0.0136 (0.0396)	
training:	Epoch: [22][269/817]	Loss 0.0089 (0.0395)	
training:	Epoch: [22][270/817]	Loss 0.0075 (0.0394)	
training:	Epoch: [22][271/817]	Loss 0.0085 (0.0393)	
training:	Epoch: [22][272/817]	Loss 0.0119 (0.0392)	
training:	Epoch: [22][273/817]	Loss 0.0080 (0.0391)	
training:	Epoch: [22][274/817]	Loss 0.0078 (0.0389)	
training:	Epoch: [22][275/817]	Loss 0.0232 (0.0389)	
training:	Epoch: [22][276/817]	Loss 0.0096 (0.0388)	
training:	Epoch: [22][277/817]	Loss 0.0082 (0.0387)	
training:	Epoch: [22][278/817]	Loss 0.0087 (0.0386)	
training:	Epoch: [22][279/817]	Loss 0.0101 (0.0385)	
training:	Epoch: [22][280/817]	Loss 0.0084 (0.0384)	
training:	Epoch: [22][281/817]	Loss 0.0086 (0.0382)	
training:	Epoch: [22][282/817]	Loss 0.0104 (0.0381)	
training:	Epoch: [22][283/817]	Loss 0.0077 (0.0380)	
training:	Epoch: [22][284/817]	Loss 0.0079 (0.0379)	
training:	Epoch: [22][285/817]	Loss 0.0134 (0.0378)	
training:	Epoch: [22][286/817]	Loss 0.0080 (0.0377)	
training:	Epoch: [22][287/817]	Loss 0.4781 (0.0393)	
training:	Epoch: [22][288/817]	Loss 0.0164 (0.0392)	
training:	Epoch: [22][289/817]	Loss 0.0112 (0.0391)	
training:	Epoch: [22][290/817]	Loss 0.0068 (0.0390)	
training:	Epoch: [22][291/817]	Loss 0.4603 (0.0404)	
training:	Epoch: [22][292/817]	Loss 0.0084 (0.0403)	
training:	Epoch: [22][293/817]	Loss 0.0107 (0.0402)	
training:	Epoch: [22][294/817]	Loss 0.0090 (0.0401)	
training:	Epoch: [22][295/817]	Loss 0.0108 (0.0400)	
training:	Epoch: [22][296/817]	Loss 0.0080 (0.0399)	
training:	Epoch: [22][297/817]	Loss 0.0077 (0.0398)	
training:	Epoch: [22][298/817]	Loss 0.0158 (0.0397)	
training:	Epoch: [22][299/817]	Loss 0.0092 (0.0396)	
training:	Epoch: [22][300/817]	Loss 0.0082 (0.0395)	
training:	Epoch: [22][301/817]	Loss 0.0092 (0.0394)	
training:	Epoch: [22][302/817]	Loss 0.0090 (0.0393)	
training:	Epoch: [22][303/817]	Loss 0.0085 (0.0392)	
training:	Epoch: [22][304/817]	Loss 0.0082 (0.0391)	
training:	Epoch: [22][305/817]	Loss 0.0147 (0.0390)	
training:	Epoch: [22][306/817]	Loss 0.0100 (0.0389)	
training:	Epoch: [22][307/817]	Loss 0.5888 (0.0407)	
training:	Epoch: [22][308/817]	Loss 0.0130 (0.0406)	
training:	Epoch: [22][309/817]	Loss 0.0096 (0.0405)	
training:	Epoch: [22][310/817]	Loss 0.0082 (0.0404)	
training:	Epoch: [22][311/817]	Loss 0.0098 (0.0403)	
training:	Epoch: [22][312/817]	Loss 0.0087 (0.0402)	
training:	Epoch: [22][313/817]	Loss 0.5764 (0.0419)	
training:	Epoch: [22][314/817]	Loss 0.0075 (0.0418)	
training:	Epoch: [22][315/817]	Loss 0.6152 (0.0437)	
training:	Epoch: [22][316/817]	Loss 0.0078 (0.0435)	
training:	Epoch: [22][317/817]	Loss 0.0083 (0.0434)	
training:	Epoch: [22][318/817]	Loss 0.0090 (0.0433)	
training:	Epoch: [22][319/817]	Loss 0.0080 (0.0432)	
training:	Epoch: [22][320/817]	Loss 0.0291 (0.0432)	
training:	Epoch: [22][321/817]	Loss 0.0089 (0.0431)	
training:	Epoch: [22][322/817]	Loss 0.0076 (0.0430)	
training:	Epoch: [22][323/817]	Loss 0.0071 (0.0428)	
training:	Epoch: [22][324/817]	Loss 0.0128 (0.0427)	
training:	Epoch: [22][325/817]	Loss 0.0080 (0.0426)	
training:	Epoch: [22][326/817]	Loss 0.6191 (0.0444)	
training:	Epoch: [22][327/817]	Loss 0.0455 (0.0444)	
training:	Epoch: [22][328/817]	Loss 0.0101 (0.0443)	
training:	Epoch: [22][329/817]	Loss 0.5698 (0.0459)	
training:	Epoch: [22][330/817]	Loss 0.0092 (0.0458)	
training:	Epoch: [22][331/817]	Loss 0.0082 (0.0457)	
training:	Epoch: [22][332/817]	Loss 0.0090 (0.0456)	
training:	Epoch: [22][333/817]	Loss 0.5826 (0.0472)	
training:	Epoch: [22][334/817]	Loss 0.0074 (0.0471)	
training:	Epoch: [22][335/817]	Loss 0.5726 (0.0486)	
training:	Epoch: [22][336/817]	Loss 0.0071 (0.0485)	
training:	Epoch: [22][337/817]	Loss 0.0074 (0.0484)	
training:	Epoch: [22][338/817]	Loss 0.0107 (0.0483)	
training:	Epoch: [22][339/817]	Loss 0.0072 (0.0482)	
training:	Epoch: [22][340/817]	Loss 0.0083 (0.0480)	
training:	Epoch: [22][341/817]	Loss 0.0076 (0.0479)	
training:	Epoch: [22][342/817]	Loss 0.0078 (0.0478)	
training:	Epoch: [22][343/817]	Loss 0.0099 (0.0477)	
training:	Epoch: [22][344/817]	Loss 0.0081 (0.0476)	
training:	Epoch: [22][345/817]	Loss 0.0616 (0.0476)	
training:	Epoch: [22][346/817]	Loss 0.0075 (0.0475)	
training:	Epoch: [22][347/817]	Loss 0.0087 (0.0474)	
training:	Epoch: [22][348/817]	Loss 0.0088 (0.0473)	
training:	Epoch: [22][349/817]	Loss 0.0089 (0.0472)	
training:	Epoch: [22][350/817]	Loss 0.6239 (0.0488)	
training:	Epoch: [22][351/817]	Loss 0.0091 (0.0487)	
training:	Epoch: [22][352/817]	Loss 0.0245 (0.0486)	
training:	Epoch: [22][353/817]	Loss 0.0760 (0.0487)	
training:	Epoch: [22][354/817]	Loss 0.0087 (0.0486)	
training:	Epoch: [22][355/817]	Loss 0.0098 (0.0485)	
training:	Epoch: [22][356/817]	Loss 0.0095 (0.0484)	
training:	Epoch: [22][357/817]	Loss 0.0116 (0.0483)	
training:	Epoch: [22][358/817]	Loss 0.0085 (0.0482)	
training:	Epoch: [22][359/817]	Loss 0.0077 (0.0481)	
training:	Epoch: [22][360/817]	Loss 0.0093 (0.0479)	
training:	Epoch: [22][361/817]	Loss 0.0095 (0.0478)	
training:	Epoch: [22][362/817]	Loss 0.0103 (0.0477)	
training:	Epoch: [22][363/817]	Loss 0.0141 (0.0476)	
training:	Epoch: [22][364/817]	Loss 0.0090 (0.0475)	
training:	Epoch: [22][365/817]	Loss 0.0088 (0.0474)	
training:	Epoch: [22][366/817]	Loss 0.0094 (0.0473)	
training:	Epoch: [22][367/817]	Loss 0.0085 (0.0472)	
training:	Epoch: [22][368/817]	Loss 0.0099 (0.0471)	
training:	Epoch: [22][369/817]	Loss 0.0120 (0.0470)	
training:	Epoch: [22][370/817]	Loss 0.3449 (0.0478)	
training:	Epoch: [22][371/817]	Loss 0.0086 (0.0477)	
training:	Epoch: [22][372/817]	Loss 0.5466 (0.0491)	
training:	Epoch: [22][373/817]	Loss 0.0089 (0.0490)	
training:	Epoch: [22][374/817]	Loss 0.0120 (0.0489)	
training:	Epoch: [22][375/817]	Loss 0.0108 (0.0488)	
training:	Epoch: [22][376/817]	Loss 0.0107 (0.0487)	
training:	Epoch: [22][377/817]	Loss 0.0578 (0.0487)	
training:	Epoch: [22][378/817]	Loss 0.0084 (0.0486)	
training:	Epoch: [22][379/817]	Loss 0.0084 (0.0485)	
training:	Epoch: [22][380/817]	Loss 0.0091 (0.0484)	
training:	Epoch: [22][381/817]	Loss 0.0106 (0.0483)	
training:	Epoch: [22][382/817]	Loss 0.0105 (0.0482)	
training:	Epoch: [22][383/817]	Loss 0.0149 (0.0481)	
training:	Epoch: [22][384/817]	Loss 0.0085 (0.0480)	
training:	Epoch: [22][385/817]	Loss 0.0132 (0.0479)	
training:	Epoch: [22][386/817]	Loss 0.0112 (0.0478)	
training:	Epoch: [22][387/817]	Loss 0.0094 (0.0477)	
training:	Epoch: [22][388/817]	Loss 0.0099 (0.0476)	
training:	Epoch: [22][389/817]	Loss 0.0108 (0.0475)	
training:	Epoch: [22][390/817]	Loss 0.0084 (0.0474)	
training:	Epoch: [22][391/817]	Loss 0.5743 (0.0487)	
training:	Epoch: [22][392/817]	Loss 0.0105 (0.0486)	
training:	Epoch: [22][393/817]	Loss 0.0082 (0.0485)	
training:	Epoch: [22][394/817]	Loss 0.0101 (0.0484)	
training:	Epoch: [22][395/817]	Loss 0.0113 (0.0484)	
training:	Epoch: [22][396/817]	Loss 0.0091 (0.0483)	
training:	Epoch: [22][397/817]	Loss 0.0131 (0.0482)	
training:	Epoch: [22][398/817]	Loss 0.0099 (0.0481)	
training:	Epoch: [22][399/817]	Loss 0.0084 (0.0480)	
training:	Epoch: [22][400/817]	Loss 0.0075 (0.0479)	
training:	Epoch: [22][401/817]	Loss 0.0112 (0.0478)	
training:	Epoch: [22][402/817]	Loss 0.0092 (0.0477)	
training:	Epoch: [22][403/817]	Loss 0.0091 (0.0476)	
training:	Epoch: [22][404/817]	Loss 0.0086 (0.0475)	
training:	Epoch: [22][405/817]	Loss 0.0084 (0.0474)	
training:	Epoch: [22][406/817]	Loss 0.0096 (0.0473)	
training:	Epoch: [22][407/817]	Loss 0.0139 (0.0472)	
training:	Epoch: [22][408/817]	Loss 0.0100 (0.0471)	
training:	Epoch: [22][409/817]	Loss 0.0122 (0.0470)	
training:	Epoch: [22][410/817]	Loss 0.4784 (0.0481)	
training:	Epoch: [22][411/817]	Loss 0.0108 (0.0480)	
training:	Epoch: [22][412/817]	Loss 0.0082 (0.0479)	
training:	Epoch: [22][413/817]	Loss 0.0102 (0.0478)	
training:	Epoch: [22][414/817]	Loss 0.0090 (0.0477)	
training:	Epoch: [22][415/817]	Loss 0.0098 (0.0476)	
training:	Epoch: [22][416/817]	Loss 0.0079 (0.0475)	
training:	Epoch: [22][417/817]	Loss 0.0097 (0.0474)	
training:	Epoch: [22][418/817]	Loss 0.0102 (0.0474)	
training:	Epoch: [22][419/817]	Loss 0.0096 (0.0473)	
training:	Epoch: [22][420/817]	Loss 0.6262 (0.0486)	
training:	Epoch: [22][421/817]	Loss 0.0085 (0.0485)	
training:	Epoch: [22][422/817]	Loss 0.0106 (0.0485)	
training:	Epoch: [22][423/817]	Loss 0.0419 (0.0484)	
training:	Epoch: [22][424/817]	Loss 0.0116 (0.0484)	
training:	Epoch: [22][425/817]	Loss 0.1662 (0.0486)	
training:	Epoch: [22][426/817]	Loss 0.0108 (0.0485)	
training:	Epoch: [22][427/817]	Loss 0.0111 (0.0485)	
training:	Epoch: [22][428/817]	Loss 0.0101 (0.0484)	
training:	Epoch: [22][429/817]	Loss 0.0089 (0.0483)	
training:	Epoch: [22][430/817]	Loss 0.0134 (0.0482)	
training:	Epoch: [22][431/817]	Loss 0.0271 (0.0481)	
training:	Epoch: [22][432/817]	Loss 0.0090 (0.0481)	
training:	Epoch: [22][433/817]	Loss 0.0100 (0.0480)	
training:	Epoch: [22][434/817]	Loss 0.0117 (0.0479)	
training:	Epoch: [22][435/817]	Loss 0.0262 (0.0478)	
training:	Epoch: [22][436/817]	Loss 0.0081 (0.0477)	
training:	Epoch: [22][437/817]	Loss 0.0149 (0.0477)	
training:	Epoch: [22][438/817]	Loss 0.0090 (0.0476)	
training:	Epoch: [22][439/817]	Loss 0.4995 (0.0486)	
training:	Epoch: [22][440/817]	Loss 0.0089 (0.0485)	
training:	Epoch: [22][441/817]	Loss 0.0119 (0.0484)	
training:	Epoch: [22][442/817]	Loss 0.1546 (0.0487)	
training:	Epoch: [22][443/817]	Loss 0.0510 (0.0487)	
training:	Epoch: [22][444/817]	Loss 0.0104 (0.0486)	
training:	Epoch: [22][445/817]	Loss 0.0088 (0.0485)	
training:	Epoch: [22][446/817]	Loss 0.0077 (0.0484)	
training:	Epoch: [22][447/817]	Loss 0.0105 (0.0483)	
training:	Epoch: [22][448/817]	Loss 0.0091 (0.0482)	
training:	Epoch: [22][449/817]	Loss 0.0102 (0.0482)	
training:	Epoch: [22][450/817]	Loss 0.0092 (0.0481)	
training:	Epoch: [22][451/817]	Loss 0.0474 (0.0481)	
training:	Epoch: [22][452/817]	Loss 0.0099 (0.0480)	
training:	Epoch: [22][453/817]	Loss 0.0087 (0.0479)	
training:	Epoch: [22][454/817]	Loss 0.0095 (0.0478)	
training:	Epoch: [22][455/817]	Loss 0.0077 (0.0477)	
training:	Epoch: [22][456/817]	Loss 0.5648 (0.0489)	
training:	Epoch: [22][457/817]	Loss 0.0093 (0.0488)	
training:	Epoch: [22][458/817]	Loss 0.0072 (0.0487)	
training:	Epoch: [22][459/817]	Loss 0.0111 (0.0486)	
training:	Epoch: [22][460/817]	Loss 0.0118 (0.0485)	
training:	Epoch: [22][461/817]	Loss 0.0537 (0.0485)	
training:	Epoch: [22][462/817]	Loss 0.0128 (0.0485)	
training:	Epoch: [22][463/817]	Loss 0.0088 (0.0484)	
training:	Epoch: [22][464/817]	Loss 0.0083 (0.0483)	
training:	Epoch: [22][465/817]	Loss 0.0071 (0.0482)	
training:	Epoch: [22][466/817]	Loss 0.2376 (0.0486)	
training:	Epoch: [22][467/817]	Loss 0.0086 (0.0485)	
training:	Epoch: [22][468/817]	Loss 0.0110 (0.0484)	
training:	Epoch: [22][469/817]	Loss 0.0092 (0.0483)	
training:	Epoch: [22][470/817]	Loss 0.0251 (0.0483)	
training:	Epoch: [22][471/817]	Loss 0.0797 (0.0484)	
training:	Epoch: [22][472/817]	Loss 0.0088 (0.0483)	
training:	Epoch: [22][473/817]	Loss 0.0107 (0.0482)	
training:	Epoch: [22][474/817]	Loss 0.0123 (0.0481)	
training:	Epoch: [22][475/817]	Loss 0.0078 (0.0480)	
training:	Epoch: [22][476/817]	Loss 0.0093 (0.0480)	
training:	Epoch: [22][477/817]	Loss 0.0079 (0.0479)	
training:	Epoch: [22][478/817]	Loss 0.0097 (0.0478)	
training:	Epoch: [22][479/817]	Loss 0.0082 (0.0477)	
training:	Epoch: [22][480/817]	Loss 0.0106 (0.0476)	
training:	Epoch: [22][481/817]	Loss 0.0097 (0.0476)	
training:	Epoch: [22][482/817]	Loss 0.0126 (0.0475)	
training:	Epoch: [22][483/817]	Loss 0.0132 (0.0474)	
training:	Epoch: [22][484/817]	Loss 0.0079 (0.0473)	
training:	Epoch: [22][485/817]	Loss 0.0106 (0.0473)	
training:	Epoch: [22][486/817]	Loss 0.0084 (0.0472)	
training:	Epoch: [22][487/817]	Loss 0.0107 (0.0471)	
training:	Epoch: [22][488/817]	Loss 0.0126 (0.0470)	
training:	Epoch: [22][489/817]	Loss 0.0360 (0.0470)	
training:	Epoch: [22][490/817]	Loss 0.0164 (0.0469)	
training:	Epoch: [22][491/817]	Loss 0.0093 (0.0469)	
training:	Epoch: [22][492/817]	Loss 0.0134 (0.0468)	
training:	Epoch: [22][493/817]	Loss 0.0081 (0.0467)	
training:	Epoch: [22][494/817]	Loss 0.0099 (0.0466)	
training:	Epoch: [22][495/817]	Loss 0.0113 (0.0466)	
training:	Epoch: [22][496/817]	Loss 0.0241 (0.0465)	
training:	Epoch: [22][497/817]	Loss 0.0074 (0.0465)	
training:	Epoch: [22][498/817]	Loss 0.0096 (0.0464)	
training:	Epoch: [22][499/817]	Loss 0.0075 (0.0463)	
training:	Epoch: [22][500/817]	Loss 0.0075 (0.0462)	
training:	Epoch: [22][501/817]	Loss 0.0079 (0.0461)	
training:	Epoch: [22][502/817]	Loss 0.0077 (0.0461)	
training:	Epoch: [22][503/817]	Loss 0.0109 (0.0460)	
training:	Epoch: [22][504/817]	Loss 0.0087 (0.0459)	
training:	Epoch: [22][505/817]	Loss 0.0081 (0.0459)	
training:	Epoch: [22][506/817]	Loss 0.0083 (0.0458)	
training:	Epoch: [22][507/817]	Loss 0.0094 (0.0457)	
training:	Epoch: [22][508/817]	Loss 0.0076 (0.0456)	
training:	Epoch: [22][509/817]	Loss 0.0080 (0.0456)	
training:	Epoch: [22][510/817]	Loss 0.0094 (0.0455)	
training:	Epoch: [22][511/817]	Loss 0.0096 (0.0454)	
training:	Epoch: [22][512/817]	Loss 0.5496 (0.0464)	
training:	Epoch: [22][513/817]	Loss 0.0075 (0.0463)	
training:	Epoch: [22][514/817]	Loss 0.0088 (0.0463)	
training:	Epoch: [22][515/817]	Loss 0.0119 (0.0462)	
training:	Epoch: [22][516/817]	Loss 0.0092 (0.0461)	
training:	Epoch: [22][517/817]	Loss 0.0096 (0.0460)	
training:	Epoch: [22][518/817]	Loss 0.0090 (0.0460)	
training:	Epoch: [22][519/817]	Loss 0.0082 (0.0459)	
training:	Epoch: [22][520/817]	Loss 0.0086 (0.0458)	
training:	Epoch: [22][521/817]	Loss 0.0078 (0.0458)	
training:	Epoch: [22][522/817]	Loss 0.0085 (0.0457)	
training:	Epoch: [22][523/817]	Loss 0.0116 (0.0456)	
training:	Epoch: [22][524/817]	Loss 0.0098 (0.0455)	
training:	Epoch: [22][525/817]	Loss 0.0100 (0.0455)	
training:	Epoch: [22][526/817]	Loss 0.0088 (0.0454)	
training:	Epoch: [22][527/817]	Loss 0.0418 (0.0454)	
training:	Epoch: [22][528/817]	Loss 0.0079 (0.0453)	
training:	Epoch: [22][529/817]	Loss 0.0082 (0.0453)	
training:	Epoch: [22][530/817]	Loss 0.0069 (0.0452)	
training:	Epoch: [22][531/817]	Loss 0.6458 (0.0463)	
training:	Epoch: [22][532/817]	Loss 0.0079 (0.0462)	
training:	Epoch: [22][533/817]	Loss 0.0081 (0.0462)	
training:	Epoch: [22][534/817]	Loss 0.0074 (0.0461)	
training:	Epoch: [22][535/817]	Loss 0.0083 (0.0460)	
training:	Epoch: [22][536/817]	Loss 0.0083 (0.0460)	
training:	Epoch: [22][537/817]	Loss 0.0071 (0.0459)	
training:	Epoch: [22][538/817]	Loss 0.0103 (0.0458)	
training:	Epoch: [22][539/817]	Loss 0.0082 (0.0458)	
training:	Epoch: [22][540/817]	Loss 0.0083 (0.0457)	
training:	Epoch: [22][541/817]	Loss 0.0072 (0.0456)	
training:	Epoch: [22][542/817]	Loss 0.0081 (0.0455)	
training:	Epoch: [22][543/817]	Loss 0.0084 (0.0455)	
training:	Epoch: [22][544/817]	Loss 0.0084 (0.0454)	
training:	Epoch: [22][545/817]	Loss 0.0081 (0.0453)	
training:	Epoch: [22][546/817]	Loss 0.0075 (0.0453)	
training:	Epoch: [22][547/817]	Loss 0.0079 (0.0452)	
training:	Epoch: [22][548/817]	Loss 0.0082 (0.0451)	
training:	Epoch: [22][549/817]	Loss 0.0084 (0.0451)	
training:	Epoch: [22][550/817]	Loss 0.0096 (0.0450)	
training:	Epoch: [22][551/817]	Loss 0.0087 (0.0449)	
training:	Epoch: [22][552/817]	Loss 0.0084 (0.0449)	
training:	Epoch: [22][553/817]	Loss 0.0092 (0.0448)	
training:	Epoch: [22][554/817]	Loss 0.6409 (0.0459)	
training:	Epoch: [22][555/817]	Loss 0.0076 (0.0458)	
training:	Epoch: [22][556/817]	Loss 0.0078 (0.0457)	
training:	Epoch: [22][557/817]	Loss 0.0084 (0.0457)	
training:	Epoch: [22][558/817]	Loss 0.0099 (0.0456)	
training:	Epoch: [22][559/817]	Loss 0.0127 (0.0456)	
training:	Epoch: [22][560/817]	Loss 0.0074 (0.0455)	
training:	Epoch: [22][561/817]	Loss 0.0103 (0.0454)	
training:	Epoch: [22][562/817]	Loss 0.0127 (0.0454)	
training:	Epoch: [22][563/817]	Loss 0.0125 (0.0453)	
training:	Epoch: [22][564/817]	Loss 0.0089 (0.0452)	
training:	Epoch: [22][565/817]	Loss 0.0097 (0.0452)	
training:	Epoch: [22][566/817]	Loss 0.0089 (0.0451)	
training:	Epoch: [22][567/817]	Loss 0.0087 (0.0451)	
training:	Epoch: [22][568/817]	Loss 0.0083 (0.0450)	
training:	Epoch: [22][569/817]	Loss 0.0108 (0.0449)	
training:	Epoch: [22][570/817]	Loss 0.0090 (0.0449)	
training:	Epoch: [22][571/817]	Loss 0.0081 (0.0448)	
training:	Epoch: [22][572/817]	Loss 0.0090 (0.0447)	
training:	Epoch: [22][573/817]	Loss 0.0082 (0.0447)	
training:	Epoch: [22][574/817]	Loss 0.0081 (0.0446)	
training:	Epoch: [22][575/817]	Loss 0.0104 (0.0446)	
training:	Epoch: [22][576/817]	Loss 0.0086 (0.0445)	
training:	Epoch: [22][577/817]	Loss 0.0123 (0.0444)	
training:	Epoch: [22][578/817]	Loss 0.0091 (0.0444)	
training:	Epoch: [22][579/817]	Loss 0.0126 (0.0443)	
training:	Epoch: [22][580/817]	Loss 0.0098 (0.0443)	
training:	Epoch: [22][581/817]	Loss 0.6432 (0.0453)	
training:	Epoch: [22][582/817]	Loss 0.0083 (0.0452)	
training:	Epoch: [22][583/817]	Loss 0.0077 (0.0452)	
training:	Epoch: [22][584/817]	Loss 0.6173 (0.0461)	
training:	Epoch: [22][585/817]	Loss 0.0093 (0.0461)	
training:	Epoch: [22][586/817]	Loss 0.0089 (0.0460)	
training:	Epoch: [22][587/817]	Loss 0.0207 (0.0460)	
training:	Epoch: [22][588/817]	Loss 0.0081 (0.0459)	
training:	Epoch: [22][589/817]	Loss 0.0085 (0.0458)	
training:	Epoch: [22][590/817]	Loss 0.0080 (0.0458)	
training:	Epoch: [22][591/817]	Loss 0.0086 (0.0457)	
training:	Epoch: [22][592/817]	Loss 0.0078 (0.0457)	
training:	Epoch: [22][593/817]	Loss 0.0084 (0.0456)	
training:	Epoch: [22][594/817]	Loss 0.0225 (0.0455)	
training:	Epoch: [22][595/817]	Loss 0.0121 (0.0455)	
training:	Epoch: [22][596/817]	Loss 0.0075 (0.0454)	
training:	Epoch: [22][597/817]	Loss 0.0102 (0.0454)	
training:	Epoch: [22][598/817]	Loss 0.0071 (0.0453)	
training:	Epoch: [22][599/817]	Loss 0.0088 (0.0452)	
training:	Epoch: [22][600/817]	Loss 0.0078 (0.0452)	
training:	Epoch: [22][601/817]	Loss 0.0077 (0.0451)	
training:	Epoch: [22][602/817]	Loss 0.0076 (0.0451)	
training:	Epoch: [22][603/817]	Loss 0.0080 (0.0450)	
training:	Epoch: [22][604/817]	Loss 0.0083 (0.0449)	
training:	Epoch: [22][605/817]	Loss 0.0185 (0.0449)	
training:	Epoch: [22][606/817]	Loss 0.0091 (0.0448)	
training:	Epoch: [22][607/817]	Loss 0.0091 (0.0448)	
training:	Epoch: [22][608/817]	Loss 0.0074 (0.0447)	
training:	Epoch: [22][609/817]	Loss 0.0094 (0.0447)	
training:	Epoch: [22][610/817]	Loss 0.5737 (0.0455)	
training:	Epoch: [22][611/817]	Loss 0.0093 (0.0455)	
training:	Epoch: [22][612/817]	Loss 0.0092 (0.0454)	
training:	Epoch: [22][613/817]	Loss 0.0086 (0.0453)	
training:	Epoch: [22][614/817]	Loss 0.0089 (0.0453)	
training:	Epoch: [22][615/817]	Loss 0.0080 (0.0452)	
training:	Epoch: [22][616/817]	Loss 0.0080 (0.0452)	
training:	Epoch: [22][617/817]	Loss 0.0073 (0.0451)	
training:	Epoch: [22][618/817]	Loss 0.0106 (0.0450)	
training:	Epoch: [22][619/817]	Loss 0.0100 (0.0450)	
training:	Epoch: [22][620/817]	Loss 0.0082 (0.0449)	
training:	Epoch: [22][621/817]	Loss 0.5457 (0.0457)	
training:	Epoch: [22][622/817]	Loss 0.0089 (0.0457)	
training:	Epoch: [22][623/817]	Loss 0.0081 (0.0456)	
training:	Epoch: [22][624/817]	Loss 0.6304 (0.0466)	
training:	Epoch: [22][625/817]	Loss 0.0106 (0.0465)	
training:	Epoch: [22][626/817]	Loss 0.6023 (0.0474)	
training:	Epoch: [22][627/817]	Loss 0.0071 (0.0473)	
training:	Epoch: [22][628/817]	Loss 0.0071 (0.0473)	
training:	Epoch: [22][629/817]	Loss 0.0088 (0.0472)	
training:	Epoch: [22][630/817]	Loss 0.0093 (0.0471)	
training:	Epoch: [22][631/817]	Loss 0.0084 (0.0471)	
training:	Epoch: [22][632/817]	Loss 0.0087 (0.0470)	
training:	Epoch: [22][633/817]	Loss 0.0088 (0.0470)	
training:	Epoch: [22][634/817]	Loss 0.0094 (0.0469)	
training:	Epoch: [22][635/817]	Loss 0.0092 (0.0468)	
training:	Epoch: [22][636/817]	Loss 0.0079 (0.0468)	
training:	Epoch: [22][637/817]	Loss 0.6261 (0.0477)	
training:	Epoch: [22][638/817]	Loss 0.0080 (0.0476)	
training:	Epoch: [22][639/817]	Loss 0.0087 (0.0476)	
training:	Epoch: [22][640/817]	Loss 0.0088 (0.0475)	
training:	Epoch: [22][641/817]	Loss 0.0082 (0.0474)	
training:	Epoch: [22][642/817]	Loss 0.0109 (0.0474)	
training:	Epoch: [22][643/817]	Loss 0.0089 (0.0473)	
training:	Epoch: [22][644/817]	Loss 0.0298 (0.0473)	
training:	Epoch: [22][645/817]	Loss 0.0088 (0.0472)	
training:	Epoch: [22][646/817]	Loss 0.0076 (0.0472)	
training:	Epoch: [22][647/817]	Loss 0.0084 (0.0471)	
training:	Epoch: [22][648/817]	Loss 0.0543 (0.0471)	
training:	Epoch: [22][649/817]	Loss 0.0089 (0.0471)	
training:	Epoch: [22][650/817]	Loss 0.0087 (0.0470)	
training:	Epoch: [22][651/817]	Loss 0.0079 (0.0469)	
training:	Epoch: [22][652/817]	Loss 0.0088 (0.0469)	
training:	Epoch: [22][653/817]	Loss 0.0094 (0.0468)	
training:	Epoch: [22][654/817]	Loss 0.0079 (0.0468)	
training:	Epoch: [22][655/817]	Loss 0.0073 (0.0467)	
training:	Epoch: [22][656/817]	Loss 0.0156 (0.0467)	
training:	Epoch: [22][657/817]	Loss 0.0091 (0.0466)	
training:	Epoch: [22][658/817]	Loss 0.0151 (0.0466)	
training:	Epoch: [22][659/817]	Loss 0.0083 (0.0465)	
training:	Epoch: [22][660/817]	Loss 0.0077 (0.0464)	
training:	Epoch: [22][661/817]	Loss 0.0103 (0.0464)	
training:	Epoch: [22][662/817]	Loss 0.0134 (0.0463)	
training:	Epoch: [22][663/817]	Loss 0.0148 (0.0463)	
training:	Epoch: [22][664/817]	Loss 0.0077 (0.0462)	
training:	Epoch: [22][665/817]	Loss 0.0074 (0.0462)	
training:	Epoch: [22][666/817]	Loss 0.0094 (0.0461)	
training:	Epoch: [22][667/817]	Loss 0.0089 (0.0461)	
training:	Epoch: [22][668/817]	Loss 0.0095 (0.0460)	
training:	Epoch: [22][669/817]	Loss 0.0312 (0.0460)	
training:	Epoch: [22][670/817]	Loss 0.0084 (0.0459)	
training:	Epoch: [22][671/817]	Loss 0.0079 (0.0459)	
training:	Epoch: [22][672/817]	Loss 0.0091 (0.0458)	
training:	Epoch: [22][673/817]	Loss 0.0090 (0.0458)	
training:	Epoch: [22][674/817]	Loss 0.0089 (0.0457)	
training:	Epoch: [22][675/817]	Loss 0.6072 (0.0465)	
training:	Epoch: [22][676/817]	Loss 0.0082 (0.0465)	
training:	Epoch: [22][677/817]	Loss 0.0097 (0.0464)	
training:	Epoch: [22][678/817]	Loss 0.0080 (0.0464)	
training:	Epoch: [22][679/817]	Loss 0.0084 (0.0463)	
training:	Epoch: [22][680/817]	Loss 0.0079 (0.0463)	
training:	Epoch: [22][681/817]	Loss 0.0118 (0.0462)	
training:	Epoch: [22][682/817]	Loss 0.0094 (0.0462)	
training:	Epoch: [22][683/817]	Loss 0.0111 (0.0461)	
training:	Epoch: [22][684/817]	Loss 0.2638 (0.0464)	
training:	Epoch: [22][685/817]	Loss 0.0080 (0.0464)	
training:	Epoch: [22][686/817]	Loss 0.0094 (0.0463)	
training:	Epoch: [22][687/817]	Loss 0.0086 (0.0463)	
training:	Epoch: [22][688/817]	Loss 0.6226 (0.0471)	
training:	Epoch: [22][689/817]	Loss 0.0090 (0.0470)	
training:	Epoch: [22][690/817]	Loss 0.0098 (0.0470)	
training:	Epoch: [22][691/817]	Loss 0.0119 (0.0469)	
training:	Epoch: [22][692/817]	Loss 0.0089 (0.0469)	
training:	Epoch: [22][693/817]	Loss 0.0083 (0.0468)	
training:	Epoch: [22][694/817]	Loss 0.0083 (0.0468)	
training:	Epoch: [22][695/817]	Loss 0.0080 (0.0467)	
training:	Epoch: [22][696/817]	Loss 0.0132 (0.0467)	
training:	Epoch: [22][697/817]	Loss 0.0103 (0.0466)	
training:	Epoch: [22][698/817]	Loss 0.0087 (0.0466)	
training:	Epoch: [22][699/817]	Loss 0.0089 (0.0465)	
training:	Epoch: [22][700/817]	Loss 0.0102 (0.0465)	
training:	Epoch: [22][701/817]	Loss 0.0098 (0.0464)	
training:	Epoch: [22][702/817]	Loss 0.0079 (0.0463)	
training:	Epoch: [22][703/817]	Loss 0.0082 (0.0463)	
training:	Epoch: [22][704/817]	Loss 0.0085 (0.0462)	
training:	Epoch: [22][705/817]	Loss 0.0092 (0.0462)	
training:	Epoch: [22][706/817]	Loss 1.1395 (0.0477)	
training:	Epoch: [22][707/817]	Loss 0.0078 (0.0477)	
training:	Epoch: [22][708/817]	Loss 0.0096 (0.0476)	
training:	Epoch: [22][709/817]	Loss 0.0088 (0.0476)	
training:	Epoch: [22][710/817]	Loss 0.0090 (0.0475)	
training:	Epoch: [22][711/817]	Loss 0.0086 (0.0475)	
training:	Epoch: [22][712/817]	Loss 0.0091 (0.0474)	
training:	Epoch: [22][713/817]	Loss 0.0089 (0.0474)	
training:	Epoch: [22][714/817]	Loss 0.0091 (0.0473)	
training:	Epoch: [22][715/817]	Loss 0.0098 (0.0472)	
training:	Epoch: [22][716/817]	Loss 0.0090 (0.0472)	
training:	Epoch: [22][717/817]	Loss 0.0097 (0.0471)	
training:	Epoch: [22][718/817]	Loss 0.0113 (0.0471)	
training:	Epoch: [22][719/817]	Loss 0.0080 (0.0470)	
training:	Epoch: [22][720/817]	Loss 0.0095 (0.0470)	
training:	Epoch: [22][721/817]	Loss 0.0075 (0.0469)	
training:	Epoch: [22][722/817]	Loss 0.0099 (0.0469)	
training:	Epoch: [22][723/817]	Loss 0.0084 (0.0468)	
training:	Epoch: [22][724/817]	Loss 0.0092 (0.0468)	
training:	Epoch: [22][725/817]	Loss 0.0094 (0.0467)	
training:	Epoch: [22][726/817]	Loss 0.0090 (0.0467)	
training:	Epoch: [22][727/817]	Loss 0.0095 (0.0466)	
training:	Epoch: [22][728/817]	Loss 0.0255 (0.0466)	
training:	Epoch: [22][729/817]	Loss 0.0085 (0.0465)	
training:	Epoch: [22][730/817]	Loss 0.0079 (0.0465)	
training:	Epoch: [22][731/817]	Loss 0.0088 (0.0464)	
training:	Epoch: [22][732/817]	Loss 0.0135 (0.0464)	
training:	Epoch: [22][733/817]	Loss 0.0090 (0.0463)	
training:	Epoch: [22][734/817]	Loss 0.0088 (0.0463)	
training:	Epoch: [22][735/817]	Loss 0.0136 (0.0462)	
training:	Epoch: [22][736/817]	Loss 0.0117 (0.0462)	
training:	Epoch: [22][737/817]	Loss 0.0095 (0.0461)	
training:	Epoch: [22][738/817]	Loss 0.0086 (0.0461)	
training:	Epoch: [22][739/817]	Loss 0.0077 (0.0460)	
training:	Epoch: [22][740/817]	Loss 0.5958 (0.0468)	
training:	Epoch: [22][741/817]	Loss 0.0085 (0.0467)	
training:	Epoch: [22][742/817]	Loss 0.0083 (0.0467)	
training:	Epoch: [22][743/817]	Loss 0.0143 (0.0466)	
training:	Epoch: [22][744/817]	Loss 0.6227 (0.0474)	
training:	Epoch: [22][745/817]	Loss 0.6267 (0.0482)	
training:	Epoch: [22][746/817]	Loss 0.0092 (0.0481)	
training:	Epoch: [22][747/817]	Loss 0.0085 (0.0481)	
training:	Epoch: [22][748/817]	Loss 0.0108 (0.0480)	
training:	Epoch: [22][749/817]	Loss 0.0078 (0.0480)	
training:	Epoch: [22][750/817]	Loss 0.0093 (0.0479)	
training:	Epoch: [22][751/817]	Loss 0.0082 (0.0479)	
training:	Epoch: [22][752/817]	Loss 0.0099 (0.0478)	
training:	Epoch: [22][753/817]	Loss 0.0148 (0.0478)	
training:	Epoch: [22][754/817]	Loss 0.0084 (0.0477)	
training:	Epoch: [22][755/817]	Loss 0.0082 (0.0477)	
training:	Epoch: [22][756/817]	Loss 0.0109 (0.0476)	
training:	Epoch: [22][757/817]	Loss 0.0109 (0.0476)	
training:	Epoch: [22][758/817]	Loss 0.0076 (0.0475)	
training:	Epoch: [22][759/817]	Loss 0.0091 (0.0475)	
training:	Epoch: [22][760/817]	Loss 0.0196 (0.0474)	
training:	Epoch: [22][761/817]	Loss 0.0102 (0.0474)	
training:	Epoch: [22][762/817]	Loss 0.0080 (0.0473)	
training:	Epoch: [22][763/817]	Loss 0.0078 (0.0473)	
training:	Epoch: [22][764/817]	Loss 0.0080 (0.0472)	
training:	Epoch: [22][765/817]	Loss 0.0076 (0.0472)	
training:	Epoch: [22][766/817]	Loss 0.0088 (0.0471)	
training:	Epoch: [22][767/817]	Loss 0.0083 (0.0471)	
training:	Epoch: [22][768/817]	Loss 0.0080 (0.0470)	
training:	Epoch: [22][769/817]	Loss 0.0086 (0.0470)	
training:	Epoch: [22][770/817]	Loss 0.0078 (0.0469)	
training:	Epoch: [22][771/817]	Loss 0.0161 (0.0469)	
training:	Epoch: [22][772/817]	Loss 0.0086 (0.0468)	
training:	Epoch: [22][773/817]	Loss 0.0137 (0.0468)	
training:	Epoch: [22][774/817]	Loss 0.0085 (0.0467)	
training:	Epoch: [22][775/817]	Loss 0.5894 (0.0474)	
training:	Epoch: [22][776/817]	Loss 0.0081 (0.0474)	
training:	Epoch: [22][777/817]	Loss 0.0077 (0.0473)	
training:	Epoch: [22][778/817]	Loss 0.0085 (0.0473)	
training:	Epoch: [22][779/817]	Loss 0.0114 (0.0473)	
training:	Epoch: [22][780/817]	Loss 0.0079 (0.0472)	
training:	Epoch: [22][781/817]	Loss 0.0099 (0.0472)	
training:	Epoch: [22][782/817]	Loss 0.0090 (0.0471)	
training:	Epoch: [22][783/817]	Loss 0.0076 (0.0471)	
training:	Epoch: [22][784/817]	Loss 0.0085 (0.0470)	
training:	Epoch: [22][785/817]	Loss 0.5738 (0.0477)	
training:	Epoch: [22][786/817]	Loss 0.0086 (0.0476)	
training:	Epoch: [22][787/817]	Loss 0.0076 (0.0476)	
training:	Epoch: [22][788/817]	Loss 0.0082 (0.0475)	
training:	Epoch: [22][789/817]	Loss 0.6114 (0.0482)	
training:	Epoch: [22][790/817]	Loss 0.0076 (0.0482)	
training:	Epoch: [22][791/817]	Loss 0.0076 (0.0481)	
training:	Epoch: [22][792/817]	Loss 0.0091 (0.0481)	
training:	Epoch: [22][793/817]	Loss 0.0078 (0.0480)	
training:	Epoch: [22][794/817]	Loss 0.0091 (0.0480)	
training:	Epoch: [22][795/817]	Loss 0.0087 (0.0479)	
training:	Epoch: [22][796/817]	Loss 0.0076 (0.0479)	
training:	Epoch: [22][797/817]	Loss 0.0085 (0.0478)	
training:	Epoch: [22][798/817]	Loss 0.0078 (0.0478)	
training:	Epoch: [22][799/817]	Loss 0.0085 (0.0477)	
training:	Epoch: [22][800/817]	Loss 0.0082 (0.0477)	
training:	Epoch: [22][801/817]	Loss 0.0083 (0.0476)	
training:	Epoch: [22][802/817]	Loss 0.0077 (0.0476)	
training:	Epoch: [22][803/817]	Loss 0.0089 (0.0475)	
training:	Epoch: [22][804/817]	Loss 0.0099 (0.0475)	
training:	Epoch: [22][805/817]	Loss 0.0091 (0.0474)	
training:	Epoch: [22][806/817]	Loss 0.0076 (0.0474)	
training:	Epoch: [22][807/817]	Loss 0.0095 (0.0474)	
training:	Epoch: [22][808/817]	Loss 0.0091 (0.0473)	
training:	Epoch: [22][809/817]	Loss 0.0086 (0.0473)	
training:	Epoch: [22][810/817]	Loss 0.0128 (0.0472)	
training:	Epoch: [22][811/817]	Loss 0.0105 (0.0472)	
training:	Epoch: [22][812/817]	Loss 0.0231 (0.0471)	
training:	Epoch: [22][813/817]	Loss 0.0094 (0.0471)	
training:	Epoch: [22][814/817]	Loss 0.0095 (0.0470)	
training:	Epoch: [22][815/817]	Loss 0.0473 (0.0470)	
training:	Epoch: [22][816/817]	Loss 0.0083 (0.0470)	
training:	Epoch: [22][817/817]	Loss 0.0122 (0.0470)	
Training:	 Loss: 0.0469

Training:	 ACC: 0.9926 0.9927 0.9929 0.9923
Validation:	 ACC: 0.7871 0.7881 0.8086 0.7657
Validation:	 Best_BACC: 0.8223 0.8218 0.8127 0.8318
Validation:	 Loss: 0.8907
Pretraining:	Epoch 23/200
----------
training:	Epoch: [23][1/817]	Loss 0.0074 (0.0074)	
training:	Epoch: [23][2/817]	Loss 0.0104 (0.0089)	
training:	Epoch: [23][3/817]	Loss 0.0083 (0.0087)	
training:	Epoch: [23][4/817]	Loss 0.0084 (0.0086)	
training:	Epoch: [23][5/817]	Loss 0.0083 (0.0086)	
training:	Epoch: [23][6/817]	Loss 0.0083 (0.0085)	
training:	Epoch: [23][7/817]	Loss 0.0083 (0.0085)	
training:	Epoch: [23][8/817]	Loss 0.0099 (0.0087)	
training:	Epoch: [23][9/817]	Loss 0.0086 (0.0086)	
training:	Epoch: [23][10/817]	Loss 0.0085 (0.0086)	
training:	Epoch: [23][11/817]	Loss 0.0086 (0.0086)	
training:	Epoch: [23][12/817]	Loss 0.1390 (0.0195)	
training:	Epoch: [23][13/817]	Loss 0.0090 (0.0187)	
training:	Epoch: [23][14/817]	Loss 0.0091 (0.0180)	
training:	Epoch: [23][15/817]	Loss 0.5982 (0.0567)	
training:	Epoch: [23][16/817]	Loss 0.0080 (0.0536)	
training:	Epoch: [23][17/817]	Loss 0.0100 (0.0511)	
training:	Epoch: [23][18/817]	Loss 0.0079 (0.0487)	
training:	Epoch: [23][19/817]	Loss 0.0082 (0.0465)	
training:	Epoch: [23][20/817]	Loss 0.0082 (0.0446)	
training:	Epoch: [23][21/817]	Loss 0.0079 (0.0429)	
training:	Epoch: [23][22/817]	Loss 0.0198 (0.0418)	
training:	Epoch: [23][23/817]	Loss 0.0091 (0.0404)	
training:	Epoch: [23][24/817]	Loss 0.0092 (0.0391)	
training:	Epoch: [23][25/817]	Loss 0.0163 (0.0382)	
training:	Epoch: [23][26/817]	Loss 0.0097 (0.0371)	
training:	Epoch: [23][27/817]	Loss 0.0076 (0.0360)	
training:	Epoch: [23][28/817]	Loss 0.0094 (0.0351)	
training:	Epoch: [23][29/817]	Loss 0.0082 (0.0341)	
training:	Epoch: [23][30/817]	Loss 0.0100 (0.0333)	
training:	Epoch: [23][31/817]	Loss 0.0080 (0.0325)	
training:	Epoch: [23][32/817]	Loss 0.0082 (0.0318)	
training:	Epoch: [23][33/817]	Loss 0.0083 (0.0310)	
training:	Epoch: [23][34/817]	Loss 0.1574 (0.0348)	
training:	Epoch: [23][35/817]	Loss 0.0080 (0.0340)	
training:	Epoch: [23][36/817]	Loss 0.0091 (0.0333)	
training:	Epoch: [23][37/817]	Loss 0.0087 (0.0326)	
training:	Epoch: [23][38/817]	Loss 0.0072 (0.0320)	
training:	Epoch: [23][39/817]	Loss 0.0077 (0.0313)	
training:	Epoch: [23][40/817]	Loss 0.0095 (0.0308)	
training:	Epoch: [23][41/817]	Loss 0.0085 (0.0303)	
training:	Epoch: [23][42/817]	Loss 0.0077 (0.0297)	
training:	Epoch: [23][43/817]	Loss 0.5797 (0.0425)	
training:	Epoch: [23][44/817]	Loss 0.0078 (0.0417)	
training:	Epoch: [23][45/817]	Loss 0.0134 (0.0411)	
training:	Epoch: [23][46/817]	Loss 0.0997 (0.0424)	
training:	Epoch: [23][47/817]	Loss 0.0087 (0.0416)	
training:	Epoch: [23][48/817]	Loss 0.0081 (0.0409)	
training:	Epoch: [23][49/817]	Loss 0.0076 (0.0403)	
training:	Epoch: [23][50/817]	Loss 0.0084 (0.0396)	
training:	Epoch: [23][51/817]	Loss 0.0099 (0.0390)	
training:	Epoch: [23][52/817]	Loss 0.0084 (0.0385)	
training:	Epoch: [23][53/817]	Loss 0.0118 (0.0380)	
training:	Epoch: [23][54/817]	Loss 0.0077 (0.0374)	
training:	Epoch: [23][55/817]	Loss 0.0090 (0.0369)	
training:	Epoch: [23][56/817]	Loss 0.0078 (0.0364)	
training:	Epoch: [23][57/817]	Loss 0.0079 (0.0359)	
training:	Epoch: [23][58/817]	Loss 0.0079 (0.0354)	
training:	Epoch: [23][59/817]	Loss 0.0084 (0.0349)	
training:	Epoch: [23][60/817]	Loss 0.0076 (0.0345)	
training:	Epoch: [23][61/817]	Loss 0.0078 (0.0340)	
training:	Epoch: [23][62/817]	Loss 0.0092 (0.0336)	
training:	Epoch: [23][63/817]	Loss 0.0079 (0.0332)	
training:	Epoch: [23][64/817]	Loss 0.0101 (0.0329)	
training:	Epoch: [23][65/817]	Loss 0.0087 (0.0325)	
training:	Epoch: [23][66/817]	Loss 0.0109 (0.0322)	
training:	Epoch: [23][67/817]	Loss 0.0080 (0.0318)	
training:	Epoch: [23][68/817]	Loss 0.0097 (0.0315)	
training:	Epoch: [23][69/817]	Loss 0.0730 (0.0321)	
training:	Epoch: [23][70/817]	Loss 0.0079 (0.0317)	
training:	Epoch: [23][71/817]	Loss 0.0085 (0.0314)	
training:	Epoch: [23][72/817]	Loss 0.0079 (0.0311)	
training:	Epoch: [23][73/817]	Loss 0.0079 (0.0308)	
training:	Epoch: [23][74/817]	Loss 0.0105 (0.0305)	
training:	Epoch: [23][75/817]	Loss 0.0072 (0.0302)	
training:	Epoch: [23][76/817]	Loss 0.0822 (0.0309)	
training:	Epoch: [23][77/817]	Loss 0.0115 (0.0306)	
training:	Epoch: [23][78/817]	Loss 0.0101 (0.0303)	
training:	Epoch: [23][79/817]	Loss 0.0083 (0.0301)	
training:	Epoch: [23][80/817]	Loss 0.0081 (0.0298)	
training:	Epoch: [23][81/817]	Loss 0.0083 (0.0295)	
training:	Epoch: [23][82/817]	Loss 0.5671 (0.0361)	
training:	Epoch: [23][83/817]	Loss 0.0117 (0.0358)	
training:	Epoch: [23][84/817]	Loss 0.0077 (0.0355)	
training:	Epoch: [23][85/817]	Loss 0.0075 (0.0351)	
training:	Epoch: [23][86/817]	Loss 0.0093 (0.0348)	
training:	Epoch: [23][87/817]	Loss 0.0079 (0.0345)	
training:	Epoch: [23][88/817]	Loss 0.0082 (0.0342)	
training:	Epoch: [23][89/817]	Loss 0.0082 (0.0339)	
training:	Epoch: [23][90/817]	Loss 0.0124 (0.0337)	
training:	Epoch: [23][91/817]	Loss 0.6098 (0.0400)	
training:	Epoch: [23][92/817]	Loss 0.0093 (0.0397)	
training:	Epoch: [23][93/817]	Loss 0.0075 (0.0393)	
training:	Epoch: [23][94/817]	Loss 0.0077 (0.0390)	
training:	Epoch: [23][95/817]	Loss 0.0085 (0.0387)	
training:	Epoch: [23][96/817]	Loss 1.1651 (0.0504)	
training:	Epoch: [23][97/817]	Loss 0.0085 (0.0500)	
training:	Epoch: [23][98/817]	Loss 0.0185 (0.0497)	
training:	Epoch: [23][99/817]	Loss 0.0106 (0.0493)	
training:	Epoch: [23][100/817]	Loss 0.0075 (0.0488)	
training:	Epoch: [23][101/817]	Loss 0.0087 (0.0484)	
training:	Epoch: [23][102/817]	Loss 0.0094 (0.0481)	
training:	Epoch: [23][103/817]	Loss 0.0071 (0.0477)	
training:	Epoch: [23][104/817]	Loss 0.0088 (0.0473)	
training:	Epoch: [23][105/817]	Loss 0.0090 (0.0469)	
training:	Epoch: [23][106/817]	Loss 0.0083 (0.0466)	
training:	Epoch: [23][107/817]	Loss 0.0083 (0.0462)	
training:	Epoch: [23][108/817]	Loss 0.0091 (0.0459)	
training:	Epoch: [23][109/817]	Loss 0.0077 (0.0455)	
training:	Epoch: [23][110/817]	Loss 0.0080 (0.0452)	
training:	Epoch: [23][111/817]	Loss 0.0088 (0.0448)	
training:	Epoch: [23][112/817]	Loss 0.0080 (0.0445)	
training:	Epoch: [23][113/817]	Loss 0.0093 (0.0442)	
training:	Epoch: [23][114/817]	Loss 0.0082 (0.0439)	
training:	Epoch: [23][115/817]	Loss 0.0082 (0.0436)	
training:	Epoch: [23][116/817]	Loss 0.0080 (0.0433)	
training:	Epoch: [23][117/817]	Loss 0.0079 (0.0430)	
training:	Epoch: [23][118/817]	Loss 0.0084 (0.0427)	
training:	Epoch: [23][119/817]	Loss 0.0082 (0.0424)	
training:	Epoch: [23][120/817]	Loss 0.5802 (0.0469)	
training:	Epoch: [23][121/817]	Loss 0.0080 (0.0465)	
training:	Epoch: [23][122/817]	Loss 0.0076 (0.0462)	
training:	Epoch: [23][123/817]	Loss 0.0081 (0.0459)	
training:	Epoch: [23][124/817]	Loss 0.0087 (0.0456)	
training:	Epoch: [23][125/817]	Loss 0.0086 (0.0453)	
training:	Epoch: [23][126/817]	Loss 0.0092 (0.0450)	
training:	Epoch: [23][127/817]	Loss 0.0090 (0.0448)	
training:	Epoch: [23][128/817]	Loss 0.0088 (0.0445)	
training:	Epoch: [23][129/817]	Loss 0.0083 (0.0442)	
training:	Epoch: [23][130/817]	Loss 0.0088 (0.0439)	
training:	Epoch: [23][131/817]	Loss 0.0082 (0.0436)	
training:	Epoch: [23][132/817]	Loss 0.0374 (0.0436)	
training:	Epoch: [23][133/817]	Loss 0.0083 (0.0433)	
training:	Epoch: [23][134/817]	Loss 0.0090 (0.0431)	
training:	Epoch: [23][135/817]	Loss 0.0083 (0.0428)	
training:	Epoch: [23][136/817]	Loss 0.0082 (0.0426)	
training:	Epoch: [23][137/817]	Loss 0.0079 (0.0423)	
training:	Epoch: [23][138/817]	Loss 0.0081 (0.0421)	
training:	Epoch: [23][139/817]	Loss 0.0092 (0.0418)	
training:	Epoch: [23][140/817]	Loss 0.0087 (0.0416)	
training:	Epoch: [23][141/817]	Loss 0.0087 (0.0414)	
training:	Epoch: [23][142/817]	Loss 0.0092 (0.0411)	
training:	Epoch: [23][143/817]	Loss 0.0087 (0.0409)	
training:	Epoch: [23][144/817]	Loss 0.0106 (0.0407)	
training:	Epoch: [23][145/817]	Loss 0.0085 (0.0405)	
training:	Epoch: [23][146/817]	Loss 0.0083 (0.0402)	
training:	Epoch: [23][147/817]	Loss 0.0076 (0.0400)	
training:	Epoch: [23][148/817]	Loss 0.0099 (0.0398)	
training:	Epoch: [23][149/817]	Loss 0.0083 (0.0396)	
training:	Epoch: [23][150/817]	Loss 0.0201 (0.0395)	
training:	Epoch: [23][151/817]	Loss 0.0089 (0.0393)	
training:	Epoch: [23][152/817]	Loss 0.0113 (0.0391)	
training:	Epoch: [23][153/817]	Loss 0.0083 (0.0389)	
training:	Epoch: [23][154/817]	Loss 0.0084 (0.0387)	
training:	Epoch: [23][155/817]	Loss 0.0118 (0.0385)	
training:	Epoch: [23][156/817]	Loss 0.0089 (0.0383)	
training:	Epoch: [23][157/817]	Loss 0.5948 (0.0419)	
training:	Epoch: [23][158/817]	Loss 0.0077 (0.0417)	
training:	Epoch: [23][159/817]	Loss 0.0076 (0.0414)	
training:	Epoch: [23][160/817]	Loss 0.0089 (0.0412)	
training:	Epoch: [23][161/817]	Loss 0.0090 (0.0410)	
training:	Epoch: [23][162/817]	Loss 0.0082 (0.0408)	
training:	Epoch: [23][163/817]	Loss 0.0094 (0.0406)	
training:	Epoch: [23][164/817]	Loss 0.0090 (0.0405)	
training:	Epoch: [23][165/817]	Loss 0.0085 (0.0403)	
training:	Epoch: [23][166/817]	Loss 0.0079 (0.0401)	
training:	Epoch: [23][167/817]	Loss 0.0152 (0.0399)	
training:	Epoch: [23][168/817]	Loss 0.0080 (0.0397)	
training:	Epoch: [23][169/817]	Loss 0.0083 (0.0395)	
training:	Epoch: [23][170/817]	Loss 0.0086 (0.0394)	
training:	Epoch: [23][171/817]	Loss 0.0081 (0.0392)	
training:	Epoch: [23][172/817]	Loss 0.0090 (0.0390)	
training:	Epoch: [23][173/817]	Loss 0.0091 (0.0388)	
training:	Epoch: [23][174/817]	Loss 0.0075 (0.0386)	
training:	Epoch: [23][175/817]	Loss 0.0083 (0.0385)	
training:	Epoch: [23][176/817]	Loss 0.0075 (0.0383)	
training:	Epoch: [23][177/817]	Loss 0.0087 (0.0381)	
training:	Epoch: [23][178/817]	Loss 0.0177 (0.0380)	
training:	Epoch: [23][179/817]	Loss 0.0091 (0.0379)	
training:	Epoch: [23][180/817]	Loss 0.0094 (0.0377)	
training:	Epoch: [23][181/817]	Loss 0.0117 (0.0376)	
training:	Epoch: [23][182/817]	Loss 0.0156 (0.0374)	
training:	Epoch: [23][183/817]	Loss 0.0088 (0.0373)	
training:	Epoch: [23][184/817]	Loss 0.0081 (0.0371)	
training:	Epoch: [23][185/817]	Loss 0.0080 (0.0370)	
training:	Epoch: [23][186/817]	Loss 0.0072 (0.0368)	
training:	Epoch: [23][187/817]	Loss 0.0072 (0.0366)	
training:	Epoch: [23][188/817]	Loss 0.0086 (0.0365)	
training:	Epoch: [23][189/817]	Loss 0.0081 (0.0363)	
training:	Epoch: [23][190/817]	Loss 0.6141 (0.0394)	
training:	Epoch: [23][191/817]	Loss 0.0102 (0.0392)	
training:	Epoch: [23][192/817]	Loss 0.0086 (0.0391)	
training:	Epoch: [23][193/817]	Loss 0.0074 (0.0389)	
training:	Epoch: [23][194/817]	Loss 0.0088 (0.0388)	
training:	Epoch: [23][195/817]	Loss 0.6298 (0.0418)	
training:	Epoch: [23][196/817]	Loss 0.0078 (0.0416)	
training:	Epoch: [23][197/817]	Loss 0.0081 (0.0414)	
training:	Epoch: [23][198/817]	Loss 0.0080 (0.0413)	
training:	Epoch: [23][199/817]	Loss 0.0083 (0.0411)	
training:	Epoch: [23][200/817]	Loss 0.0289 (0.0410)	
training:	Epoch: [23][201/817]	Loss 0.0078 (0.0409)	
training:	Epoch: [23][202/817]	Loss 0.0073 (0.0407)	
training:	Epoch: [23][203/817]	Loss 0.0086 (0.0406)	
training:	Epoch: [23][204/817]	Loss 0.0081 (0.0404)	
training:	Epoch: [23][205/817]	Loss 0.0097 (0.0402)	
training:	Epoch: [23][206/817]	Loss 0.0075 (0.0401)	
training:	Epoch: [23][207/817]	Loss 0.0255 (0.0400)	
training:	Epoch: [23][208/817]	Loss 0.0091 (0.0399)	
training:	Epoch: [23][209/817]	Loss 0.0075 (0.0397)	
training:	Epoch: [23][210/817]	Loss 0.0075 (0.0396)	
training:	Epoch: [23][211/817]	Loss 0.0079 (0.0394)	
training:	Epoch: [23][212/817]	Loss 0.0087 (0.0393)	
training:	Epoch: [23][213/817]	Loss 0.6276 (0.0420)	
training:	Epoch: [23][214/817]	Loss 0.0085 (0.0419)	
training:	Epoch: [23][215/817]	Loss 0.0088 (0.0417)	
training:	Epoch: [23][216/817]	Loss 0.0085 (0.0416)	
training:	Epoch: [23][217/817]	Loss 0.0077 (0.0414)	
training:	Epoch: [23][218/817]	Loss 0.0080 (0.0413)	
training:	Epoch: [23][219/817]	Loss 0.0081 (0.0411)	
training:	Epoch: [23][220/817]	Loss 0.0079 (0.0410)	
training:	Epoch: [23][221/817]	Loss 0.0082 (0.0408)	
training:	Epoch: [23][222/817]	Loss 0.0078 (0.0407)	
training:	Epoch: [23][223/817]	Loss 0.0098 (0.0405)	
training:	Epoch: [23][224/817]	Loss 0.0076 (0.0404)	
training:	Epoch: [23][225/817]	Loss 0.0080 (0.0402)	
training:	Epoch: [23][226/817]	Loss 0.0074 (0.0401)	
training:	Epoch: [23][227/817]	Loss 0.0088 (0.0399)	
training:	Epoch: [23][228/817]	Loss 0.0085 (0.0398)	
training:	Epoch: [23][229/817]	Loss 0.0084 (0.0397)	
training:	Epoch: [23][230/817]	Loss 0.0090 (0.0395)	
training:	Epoch: [23][231/817]	Loss 0.0080 (0.0394)	
training:	Epoch: [23][232/817]	Loss 0.0076 (0.0393)	
training:	Epoch: [23][233/817]	Loss 0.5029 (0.0413)	
training:	Epoch: [23][234/817]	Loss 0.0077 (0.0411)	
training:	Epoch: [23][235/817]	Loss 0.0079 (0.0410)	
training:	Epoch: [23][236/817]	Loss 0.0087 (0.0408)	
training:	Epoch: [23][237/817]	Loss 0.0079 (0.0407)	
training:	Epoch: [23][238/817]	Loss 0.0133 (0.0406)	
training:	Epoch: [23][239/817]	Loss 0.0075 (0.0404)	
training:	Epoch: [23][240/817]	Loss 0.0088 (0.0403)	
training:	Epoch: [23][241/817]	Loss 0.0078 (0.0402)	
training:	Epoch: [23][242/817]	Loss 0.0122 (0.0401)	
training:	Epoch: [23][243/817]	Loss 0.0077 (0.0399)	
training:	Epoch: [23][244/817]	Loss 0.0079 (0.0398)	
training:	Epoch: [23][245/817]	Loss 0.0138 (0.0397)	
training:	Epoch: [23][246/817]	Loss 0.0079 (0.0396)	
training:	Epoch: [23][247/817]	Loss 0.0080 (0.0394)	
training:	Epoch: [23][248/817]	Loss 0.0085 (0.0393)	
training:	Epoch: [23][249/817]	Loss 0.2565 (0.0402)	
training:	Epoch: [23][250/817]	Loss 0.0078 (0.0400)	
training:	Epoch: [23][251/817]	Loss 0.0077 (0.0399)	
training:	Epoch: [23][252/817]	Loss 0.0083 (0.0398)	
training:	Epoch: [23][253/817]	Loss 0.0080 (0.0397)	
training:	Epoch: [23][254/817]	Loss 0.0086 (0.0395)	
training:	Epoch: [23][255/817]	Loss 0.0093 (0.0394)	
training:	Epoch: [23][256/817]	Loss 0.0088 (0.0393)	
training:	Epoch: [23][257/817]	Loss 0.0077 (0.0392)	
training:	Epoch: [23][258/817]	Loss 0.0212 (0.0391)	
training:	Epoch: [23][259/817]	Loss 0.0082 (0.0390)	
training:	Epoch: [23][260/817]	Loss 0.0086 (0.0389)	
training:	Epoch: [23][261/817]	Loss 0.0084 (0.0388)	
training:	Epoch: [23][262/817]	Loss 0.0079 (0.0386)	
training:	Epoch: [23][263/817]	Loss 0.0082 (0.0385)	
training:	Epoch: [23][264/817]	Loss 0.0081 (0.0384)	
training:	Epoch: [23][265/817]	Loss 0.0079 (0.0383)	
training:	Epoch: [23][266/817]	Loss 0.0141 (0.0382)	
training:	Epoch: [23][267/817]	Loss 0.0093 (0.0381)	
training:	Epoch: [23][268/817]	Loss 0.0079 (0.0380)	
training:	Epoch: [23][269/817]	Loss 0.0084 (0.0379)	
training:	Epoch: [23][270/817]	Loss 0.5316 (0.0397)	
training:	Epoch: [23][271/817]	Loss 0.0084 (0.0396)	
training:	Epoch: [23][272/817]	Loss 0.0085 (0.0395)	
training:	Epoch: [23][273/817]	Loss 0.0076 (0.0394)	
training:	Epoch: [23][274/817]	Loss 0.2535 (0.0401)	
training:	Epoch: [23][275/817]	Loss 0.0076 (0.0400)	
training:	Epoch: [23][276/817]	Loss 0.0131 (0.0399)	
training:	Epoch: [23][277/817]	Loss 0.0077 (0.0398)	
training:	Epoch: [23][278/817]	Loss 0.0582 (0.0399)	
training:	Epoch: [23][279/817]	Loss 0.0081 (0.0398)	
training:	Epoch: [23][280/817]	Loss 0.0085 (0.0396)	
training:	Epoch: [23][281/817]	Loss 0.0084 (0.0395)	
training:	Epoch: [23][282/817]	Loss 0.0077 (0.0394)	
training:	Epoch: [23][283/817]	Loss 0.0094 (0.0393)	
training:	Epoch: [23][284/817]	Loss 0.0076 (0.0392)	
training:	Epoch: [23][285/817]	Loss 0.0126 (0.0391)	
training:	Epoch: [23][286/817]	Loss 0.5883 (0.0410)	
training:	Epoch: [23][287/817]	Loss 0.5926 (0.0430)	
training:	Epoch: [23][288/817]	Loss 0.0082 (0.0428)	
training:	Epoch: [23][289/817]	Loss 0.0079 (0.0427)	
training:	Epoch: [23][290/817]	Loss 0.0087 (0.0426)	
training:	Epoch: [23][291/817]	Loss 0.0118 (0.0425)	
training:	Epoch: [23][292/817]	Loss 0.0079 (0.0424)	
training:	Epoch: [23][293/817]	Loss 0.0093 (0.0423)	
training:	Epoch: [23][294/817]	Loss 0.0088 (0.0421)	
training:	Epoch: [23][295/817]	Loss 0.0085 (0.0420)	
training:	Epoch: [23][296/817]	Loss 0.0092 (0.0419)	
training:	Epoch: [23][297/817]	Loss 0.5997 (0.0438)	
training:	Epoch: [23][298/817]	Loss 0.0081 (0.0437)	
training:	Epoch: [23][299/817]	Loss 0.5945 (0.0455)	
training:	Epoch: [23][300/817]	Loss 0.0086 (0.0454)	
training:	Epoch: [23][301/817]	Loss 0.0083 (0.0453)	
training:	Epoch: [23][302/817]	Loss 0.0250 (0.0452)	
training:	Epoch: [23][303/817]	Loss 0.0155 (0.0451)	
training:	Epoch: [23][304/817]	Loss 0.0086 (0.0450)	
training:	Epoch: [23][305/817]	Loss 0.0152 (0.0449)	
training:	Epoch: [23][306/817]	Loss 0.0086 (0.0448)	
training:	Epoch: [23][307/817]	Loss 0.0131 (0.0447)	
training:	Epoch: [23][308/817]	Loss 0.0089 (0.0446)	
training:	Epoch: [23][309/817]	Loss 0.0089 (0.0444)	
training:	Epoch: [23][310/817]	Loss 0.0160 (0.0443)	
training:	Epoch: [23][311/817]	Loss 0.0189 (0.0443)	
training:	Epoch: [23][312/817]	Loss 0.0080 (0.0441)	
training:	Epoch: [23][313/817]	Loss 0.0078 (0.0440)	
training:	Epoch: [23][314/817]	Loss 0.0089 (0.0439)	
training:	Epoch: [23][315/817]	Loss 0.0080 (0.0438)	
training:	Epoch: [23][316/817]	Loss 0.0080 (0.0437)	
training:	Epoch: [23][317/817]	Loss 0.5714 (0.0454)	
training:	Epoch: [23][318/817]	Loss 0.0089 (0.0452)	
training:	Epoch: [23][319/817]	Loss 0.0084 (0.0451)	
training:	Epoch: [23][320/817]	Loss 0.0083 (0.0450)	
training:	Epoch: [23][321/817]	Loss 0.0079 (0.0449)	
training:	Epoch: [23][322/817]	Loss 0.0083 (0.0448)	
training:	Epoch: [23][323/817]	Loss 0.0101 (0.0447)	
training:	Epoch: [23][324/817]	Loss 0.0364 (0.0446)	
training:	Epoch: [23][325/817]	Loss 0.0088 (0.0445)	
training:	Epoch: [23][326/817]	Loss 0.0075 (0.0444)	
training:	Epoch: [23][327/817]	Loss 0.0078 (0.0443)	
training:	Epoch: [23][328/817]	Loss 0.0084 (0.0442)	
training:	Epoch: [23][329/817]	Loss 0.0085 (0.0441)	
training:	Epoch: [23][330/817]	Loss 0.0209 (0.0440)	
training:	Epoch: [23][331/817]	Loss 0.0119 (0.0439)	
training:	Epoch: [23][332/817]	Loss 0.0080 (0.0438)	
training:	Epoch: [23][333/817]	Loss 0.0093 (0.0437)	
training:	Epoch: [23][334/817]	Loss 0.0082 (0.0436)	
training:	Epoch: [23][335/817]	Loss 0.0077 (0.0435)	
training:	Epoch: [23][336/817]	Loss 0.0119 (0.0434)	
training:	Epoch: [23][337/817]	Loss 0.0088 (0.0433)	
training:	Epoch: [23][338/817]	Loss 0.0084 (0.0432)	
training:	Epoch: [23][339/817]	Loss 0.0082 (0.0431)	
training:	Epoch: [23][340/817]	Loss 0.0087 (0.0430)	
training:	Epoch: [23][341/817]	Loss 0.0088 (0.0429)	
training:	Epoch: [23][342/817]	Loss 0.0091 (0.0428)	
training:	Epoch: [23][343/817]	Loss 0.0085 (0.0427)	
training:	Epoch: [23][344/817]	Loss 0.0081 (0.0426)	
training:	Epoch: [23][345/817]	Loss 0.6047 (0.0442)	
training:	Epoch: [23][346/817]	Loss 0.0081 (0.0441)	
training:	Epoch: [23][347/817]	Loss 0.0083 (0.0440)	
training:	Epoch: [23][348/817]	Loss 0.0086 (0.0439)	
training:	Epoch: [23][349/817]	Loss 0.0083 (0.0438)	
training:	Epoch: [23][350/817]	Loss 0.0081 (0.0437)	
training:	Epoch: [23][351/817]	Loss 0.0084 (0.0436)	
training:	Epoch: [23][352/817]	Loss 0.0092 (0.0435)	
training:	Epoch: [23][353/817]	Loss 0.0113 (0.0434)	
training:	Epoch: [23][354/817]	Loss 0.0090 (0.0433)	
training:	Epoch: [23][355/817]	Loss 0.0079 (0.0432)	
training:	Epoch: [23][356/817]	Loss 0.0082 (0.0431)	
training:	Epoch: [23][357/817]	Loss 0.0141 (0.0430)	
training:	Epoch: [23][358/817]	Loss 0.0091 (0.0430)	
training:	Epoch: [23][359/817]	Loss 0.0084 (0.0429)	
training:	Epoch: [23][360/817]	Loss 0.0080 (0.0428)	
training:	Epoch: [23][361/817]	Loss 0.0080 (0.0427)	
training:	Epoch: [23][362/817]	Loss 0.0076 (0.0426)	
training:	Epoch: [23][363/817]	Loss 0.0376 (0.0425)	
training:	Epoch: [23][364/817]	Loss 0.0089 (0.0425)	
training:	Epoch: [23][365/817]	Loss 0.0081 (0.0424)	
training:	Epoch: [23][366/817]	Loss 0.0086 (0.0423)	
training:	Epoch: [23][367/817]	Loss 0.0076 (0.0422)	
training:	Epoch: [23][368/817]	Loss 0.0085 (0.0421)	
training:	Epoch: [23][369/817]	Loss 0.0088 (0.0420)	
training:	Epoch: [23][370/817]	Loss 0.6064 (0.0435)	
training:	Epoch: [23][371/817]	Loss 0.0078 (0.0434)	
training:	Epoch: [23][372/817]	Loss 0.0077 (0.0433)	
training:	Epoch: [23][373/817]	Loss 0.0096 (0.0432)	
training:	Epoch: [23][374/817]	Loss 0.0079 (0.0431)	
training:	Epoch: [23][375/817]	Loss 0.0082 (0.0430)	
training:	Epoch: [23][376/817]	Loss 0.0089 (0.0430)	
training:	Epoch: [23][377/817]	Loss 0.0086 (0.0429)	
training:	Epoch: [23][378/817]	Loss 0.0077 (0.0428)	
training:	Epoch: [23][379/817]	Loss 0.0143 (0.0427)	
training:	Epoch: [23][380/817]	Loss 0.0084 (0.0426)	
training:	Epoch: [23][381/817]	Loss 0.0086 (0.0425)	
training:	Epoch: [23][382/817]	Loss 0.0086 (0.0424)	
training:	Epoch: [23][383/817]	Loss 0.0738 (0.0425)	
training:	Epoch: [23][384/817]	Loss 0.0121 (0.0424)	
training:	Epoch: [23][385/817]	Loss 0.0083 (0.0423)	
training:	Epoch: [23][386/817]	Loss 0.0076 (0.0423)	
training:	Epoch: [23][387/817]	Loss 0.0079 (0.0422)	
training:	Epoch: [23][388/817]	Loss 0.0077 (0.0421)	
training:	Epoch: [23][389/817]	Loss 0.4784 (0.0432)	
training:	Epoch: [23][390/817]	Loss 0.0081 (0.0431)	
training:	Epoch: [23][391/817]	Loss 0.0080 (0.0430)	
training:	Epoch: [23][392/817]	Loss 0.0081 (0.0429)	
training:	Epoch: [23][393/817]	Loss 0.0091 (0.0428)	
training:	Epoch: [23][394/817]	Loss 0.0111 (0.0428)	
training:	Epoch: [23][395/817]	Loss 0.0092 (0.0427)	
training:	Epoch: [23][396/817]	Loss 0.0087 (0.0426)	
training:	Epoch: [23][397/817]	Loss 0.0156 (0.0425)	
training:	Epoch: [23][398/817]	Loss 0.0086 (0.0424)	
training:	Epoch: [23][399/817]	Loss 0.0082 (0.0424)	
training:	Epoch: [23][400/817]	Loss 0.0092 (0.0423)	
training:	Epoch: [23][401/817]	Loss 0.0081 (0.0422)	
training:	Epoch: [23][402/817]	Loss 0.0108 (0.0421)	
training:	Epoch: [23][403/817]	Loss 0.0102 (0.0420)	
training:	Epoch: [23][404/817]	Loss 0.0174 (0.0420)	
training:	Epoch: [23][405/817]	Loss 0.0085 (0.0419)	
training:	Epoch: [23][406/817]	Loss 0.0080 (0.0418)	
training:	Epoch: [23][407/817]	Loss 0.0082 (0.0417)	
training:	Epoch: [23][408/817]	Loss 0.0088 (0.0416)	
training:	Epoch: [23][409/817]	Loss 0.0144 (0.0416)	
training:	Epoch: [23][410/817]	Loss 0.0090 (0.0415)	
training:	Epoch: [23][411/817]	Loss 0.0076 (0.0414)	
training:	Epoch: [23][412/817]	Loss 0.0083 (0.0413)	
training:	Epoch: [23][413/817]	Loss 0.0086 (0.0413)	
training:	Epoch: [23][414/817]	Loss 0.0079 (0.0412)	
training:	Epoch: [23][415/817]	Loss 0.5424 (0.0424)	
training:	Epoch: [23][416/817]	Loss 0.0100 (0.0423)	
training:	Epoch: [23][417/817]	Loss 0.0091 (0.0422)	
training:	Epoch: [23][418/817]	Loss 0.0093 (0.0421)	
training:	Epoch: [23][419/817]	Loss 0.0117 (0.0421)	
training:	Epoch: [23][420/817]	Loss 0.6240 (0.0435)	
training:	Epoch: [23][421/817]	Loss 0.3522 (0.0442)	
training:	Epoch: [23][422/817]	Loss 0.5931 (0.0455)	
training:	Epoch: [23][423/817]	Loss 0.0080 (0.0454)	
training:	Epoch: [23][424/817]	Loss 0.0084 (0.0453)	
training:	Epoch: [23][425/817]	Loss 0.0085 (0.0452)	
training:	Epoch: [23][426/817]	Loss 0.0084 (0.0451)	
training:	Epoch: [23][427/817]	Loss 0.0078 (0.0451)	
training:	Epoch: [23][428/817]	Loss 0.0077 (0.0450)	
training:	Epoch: [23][429/817]	Loss 0.0109 (0.0449)	
training:	Epoch: [23][430/817]	Loss 0.0082 (0.0448)	
training:	Epoch: [23][431/817]	Loss 0.0086 (0.0447)	
training:	Epoch: [23][432/817]	Loss 0.0090 (0.0446)	
training:	Epoch: [23][433/817]	Loss 0.0079 (0.0445)	
training:	Epoch: [23][434/817]	Loss 0.0128 (0.0445)	
training:	Epoch: [23][435/817]	Loss 0.0124 (0.0444)	
training:	Epoch: [23][436/817]	Loss 0.0083 (0.0443)	
training:	Epoch: [23][437/817]	Loss 0.0087 (0.0442)	
training:	Epoch: [23][438/817]	Loss 0.0088 (0.0442)	
training:	Epoch: [23][439/817]	Loss 0.0086 (0.0441)	
training:	Epoch: [23][440/817]	Loss 0.0075 (0.0440)	
training:	Epoch: [23][441/817]	Loss 0.0211 (0.0439)	
training:	Epoch: [23][442/817]	Loss 0.0079 (0.0439)	
training:	Epoch: [23][443/817]	Loss 0.0085 (0.0438)	
training:	Epoch: [23][444/817]	Loss 0.6029 (0.0450)	
training:	Epoch: [23][445/817]	Loss 0.0104 (0.0450)	
training:	Epoch: [23][446/817]	Loss 0.0079 (0.0449)	
training:	Epoch: [23][447/817]	Loss 0.0119 (0.0448)	
training:	Epoch: [23][448/817]	Loss 0.0087 (0.0447)	
training:	Epoch: [23][449/817]	Loss 0.0077 (0.0446)	
training:	Epoch: [23][450/817]	Loss 0.0104 (0.0446)	
training:	Epoch: [23][451/817]	Loss 0.0101 (0.0445)	
training:	Epoch: [23][452/817]	Loss 0.0095 (0.0444)	
training:	Epoch: [23][453/817]	Loss 0.0171 (0.0444)	
training:	Epoch: [23][454/817]	Loss 0.0116 (0.0443)	
training:	Epoch: [23][455/817]	Loss 0.0099 (0.0442)	
training:	Epoch: [23][456/817]	Loss 0.0123 (0.0441)	
training:	Epoch: [23][457/817]	Loss 0.0078 (0.0441)	
training:	Epoch: [23][458/817]	Loss 0.0467 (0.0441)	
training:	Epoch: [23][459/817]	Loss 0.0098 (0.0440)	
training:	Epoch: [23][460/817]	Loss 0.0115 (0.0439)	
training:	Epoch: [23][461/817]	Loss 0.0085 (0.0438)	
training:	Epoch: [23][462/817]	Loss 0.0107 (0.0438)	
training:	Epoch: [23][463/817]	Loss 0.0101 (0.0437)	
training:	Epoch: [23][464/817]	Loss 0.0097 (0.0436)	
training:	Epoch: [23][465/817]	Loss 0.0082 (0.0435)	
training:	Epoch: [23][466/817]	Loss 0.6248 (0.0448)	
training:	Epoch: [23][467/817]	Loss 0.0081 (0.0447)	
training:	Epoch: [23][468/817]	Loss 0.0148 (0.0446)	
training:	Epoch: [23][469/817]	Loss 0.0082 (0.0446)	
training:	Epoch: [23][470/817]	Loss 0.0080 (0.0445)	
training:	Epoch: [23][471/817]	Loss 0.0078 (0.0444)	
training:	Epoch: [23][472/817]	Loss 0.0079 (0.0443)	
training:	Epoch: [23][473/817]	Loss 0.5994 (0.0455)	
training:	Epoch: [23][474/817]	Loss 0.0079 (0.0454)	
training:	Epoch: [23][475/817]	Loss 0.0116 (0.0454)	
training:	Epoch: [23][476/817]	Loss 0.0459 (0.0454)	
training:	Epoch: [23][477/817]	Loss 0.0080 (0.0453)	
training:	Epoch: [23][478/817]	Loss 0.0103 (0.0452)	
training:	Epoch: [23][479/817]	Loss 0.0091 (0.0451)	
training:	Epoch: [23][480/817]	Loss 0.0079 (0.0451)	
training:	Epoch: [23][481/817]	Loss 0.0080 (0.0450)	
training:	Epoch: [23][482/817]	Loss 0.0092 (0.0449)	
training:	Epoch: [23][483/817]	Loss 0.0092 (0.0448)	
training:	Epoch: [23][484/817]	Loss 0.0090 (0.0448)	
training:	Epoch: [23][485/817]	Loss 0.0094 (0.0447)	
training:	Epoch: [23][486/817]	Loss 0.0128 (0.0446)	
training:	Epoch: [23][487/817]	Loss 0.0103 (0.0445)	
training:	Epoch: [23][488/817]	Loss 0.0083 (0.0445)	
training:	Epoch: [23][489/817]	Loss 0.0108 (0.0444)	
training:	Epoch: [23][490/817]	Loss 0.0104 (0.0443)	
training:	Epoch: [23][491/817]	Loss 0.0082 (0.0443)	
training:	Epoch: [23][492/817]	Loss 0.0086 (0.0442)	
training:	Epoch: [23][493/817]	Loss 0.0087 (0.0441)	
training:	Epoch: [23][494/817]	Loss 0.0106 (0.0441)	
training:	Epoch: [23][495/817]	Loss 0.0095 (0.0440)	
training:	Epoch: [23][496/817]	Loss 0.0103 (0.0439)	
training:	Epoch: [23][497/817]	Loss 0.0089 (0.0438)	
training:	Epoch: [23][498/817]	Loss 0.0092 (0.0438)	
training:	Epoch: [23][499/817]	Loss 0.0088 (0.0437)	
training:	Epoch: [23][500/817]	Loss 0.0090 (0.0436)	
training:	Epoch: [23][501/817]	Loss 0.0080 (0.0436)	
training:	Epoch: [23][502/817]	Loss 0.0124 (0.0435)	
training:	Epoch: [23][503/817]	Loss 0.0075 (0.0434)	
training:	Epoch: [23][504/817]	Loss 0.0082 (0.0434)	
training:	Epoch: [23][505/817]	Loss 0.0110 (0.0433)	
training:	Epoch: [23][506/817]	Loss 0.0088 (0.0432)	
training:	Epoch: [23][507/817]	Loss 0.0092 (0.0432)	
training:	Epoch: [23][508/817]	Loss 0.0083 (0.0431)	
training:	Epoch: [23][509/817]	Loss 0.0149 (0.0430)	
training:	Epoch: [23][510/817]	Loss 0.0107 (0.0430)	
training:	Epoch: [23][511/817]	Loss 0.0100 (0.0429)	
training:	Epoch: [23][512/817]	Loss 0.0093 (0.0428)	
training:	Epoch: [23][513/817]	Loss 0.0095 (0.0428)	
training:	Epoch: [23][514/817]	Loss 0.0085 (0.0427)	
training:	Epoch: [23][515/817]	Loss 0.0125 (0.0427)	
training:	Epoch: [23][516/817]	Loss 0.0088 (0.0426)	
training:	Epoch: [23][517/817]	Loss 0.0114 (0.0425)	
training:	Epoch: [23][518/817]	Loss 0.0082 (0.0425)	
training:	Epoch: [23][519/817]	Loss 0.0083 (0.0424)	
training:	Epoch: [23][520/817]	Loss 0.0093 (0.0423)	
training:	Epoch: [23][521/817]	Loss 0.0083 (0.0423)	
training:	Epoch: [23][522/817]	Loss 0.0087 (0.0422)	
training:	Epoch: [23][523/817]	Loss 0.6212 (0.0433)	
training:	Epoch: [23][524/817]	Loss 0.0070 (0.0432)	
training:	Epoch: [23][525/817]	Loss 0.0074 (0.0432)	
training:	Epoch: [23][526/817]	Loss 0.0073 (0.0431)	
training:	Epoch: [23][527/817]	Loss 0.0076 (0.0430)	
training:	Epoch: [23][528/817]	Loss 0.0080 (0.0430)	
training:	Epoch: [23][529/817]	Loss 0.0081 (0.0429)	
training:	Epoch: [23][530/817]	Loss 0.0078 (0.0428)	
training:	Epoch: [23][531/817]	Loss 0.0086 (0.0428)	
training:	Epoch: [23][532/817]	Loss 0.0079 (0.0427)	
training:	Epoch: [23][533/817]	Loss 0.0094 (0.0426)	
training:	Epoch: [23][534/817]	Loss 0.0082 (0.0426)	
training:	Epoch: [23][535/817]	Loss 0.0090 (0.0425)	
training:	Epoch: [23][536/817]	Loss 0.0091 (0.0425)	
training:	Epoch: [23][537/817]	Loss 0.0084 (0.0424)	
training:	Epoch: [23][538/817]	Loss 0.0087 (0.0423)	
training:	Epoch: [23][539/817]	Loss 0.0244 (0.0423)	
training:	Epoch: [23][540/817]	Loss 0.0080 (0.0422)	
training:	Epoch: [23][541/817]	Loss 0.0079 (0.0422)	
training:	Epoch: [23][542/817]	Loss 0.0083 (0.0421)	
training:	Epoch: [23][543/817]	Loss 0.0076 (0.0420)	
training:	Epoch: [23][544/817]	Loss 0.0082 (0.0420)	
training:	Epoch: [23][545/817]	Loss 0.0085 (0.0419)	
training:	Epoch: [23][546/817]	Loss 0.0096 (0.0419)	
training:	Epoch: [23][547/817]	Loss 0.0091 (0.0418)	
training:	Epoch: [23][548/817]	Loss 0.0094 (0.0417)	
training:	Epoch: [23][549/817]	Loss 0.0072 (0.0417)	
training:	Epoch: [23][550/817]	Loss 0.0080 (0.0416)	
training:	Epoch: [23][551/817]	Loss 0.0076 (0.0416)	
training:	Epoch: [23][552/817]	Loss 0.0090 (0.0415)	
training:	Epoch: [23][553/817]	Loss 0.0084 (0.0414)	
training:	Epoch: [23][554/817]	Loss 0.0074 (0.0414)	
training:	Epoch: [23][555/817]	Loss 0.0121 (0.0413)	
training:	Epoch: [23][556/817]	Loss 0.0075 (0.0413)	
training:	Epoch: [23][557/817]	Loss 0.0074 (0.0412)	
training:	Epoch: [23][558/817]	Loss 0.0077 (0.0411)	
training:	Epoch: [23][559/817]	Loss 0.0085 (0.0411)	
training:	Epoch: [23][560/817]	Loss 0.0112 (0.0410)	
training:	Epoch: [23][561/817]	Loss 0.0079 (0.0410)	
training:	Epoch: [23][562/817]	Loss 0.0086 (0.0409)	
training:	Epoch: [23][563/817]	Loss 0.0084 (0.0409)	
training:	Epoch: [23][564/817]	Loss 0.0091 (0.0408)	
training:	Epoch: [23][565/817]	Loss 0.0097 (0.0407)	
training:	Epoch: [23][566/817]	Loss 0.0074 (0.0407)	
training:	Epoch: [23][567/817]	Loss 0.0237 (0.0407)	
training:	Epoch: [23][568/817]	Loss 0.0085 (0.0406)	
training:	Epoch: [23][569/817]	Loss 0.0075 (0.0405)	
training:	Epoch: [23][570/817]	Loss 0.0081 (0.0405)	
training:	Epoch: [23][571/817]	Loss 0.0078 (0.0404)	
training:	Epoch: [23][572/817]	Loss 0.0070 (0.0404)	
training:	Epoch: [23][573/817]	Loss 0.0114 (0.0403)	
training:	Epoch: [23][574/817]	Loss 0.0069 (0.0403)	
training:	Epoch: [23][575/817]	Loss 0.0076 (0.0402)	
training:	Epoch: [23][576/817]	Loss 0.0075 (0.0401)	
training:	Epoch: [23][577/817]	Loss 0.0079 (0.0401)	
training:	Epoch: [23][578/817]	Loss 0.0093 (0.0400)	
training:	Epoch: [23][579/817]	Loss 0.0088 (0.0400)	
training:	Epoch: [23][580/817]	Loss 0.0071 (0.0399)	
training:	Epoch: [23][581/817]	Loss 0.0110 (0.0399)	
training:	Epoch: [23][582/817]	Loss 0.0078 (0.0398)	
training:	Epoch: [23][583/817]	Loss 0.0080 (0.0398)	
training:	Epoch: [23][584/817]	Loss 0.0077 (0.0397)	
training:	Epoch: [23][585/817]	Loss 0.0077 (0.0397)	
training:	Epoch: [23][586/817]	Loss 0.0107 (0.0396)	
training:	Epoch: [23][587/817]	Loss 0.0079 (0.0396)	
training:	Epoch: [23][588/817]	Loss 0.0084 (0.0395)	
training:	Epoch: [23][589/817]	Loss 0.0074 (0.0394)	
training:	Epoch: [23][590/817]	Loss 0.0078 (0.0394)	
training:	Epoch: [23][591/817]	Loss 0.0073 (0.0393)	
training:	Epoch: [23][592/817]	Loss 0.0073 (0.0393)	
training:	Epoch: [23][593/817]	Loss 0.0075 (0.0392)	
training:	Epoch: [23][594/817]	Loss 0.0079 (0.0392)	
training:	Epoch: [23][595/817]	Loss 0.0075 (0.0391)	
training:	Epoch: [23][596/817]	Loss 0.0073 (0.0391)	
training:	Epoch: [23][597/817]	Loss 0.0081 (0.0390)	
training:	Epoch: [23][598/817]	Loss 0.0073 (0.0390)	
training:	Epoch: [23][599/817]	Loss 0.0087 (0.0389)	
training:	Epoch: [23][600/817]	Loss 0.0080 (0.0389)	
training:	Epoch: [23][601/817]	Loss 0.0082 (0.0388)	
training:	Epoch: [23][602/817]	Loss 0.0072 (0.0388)	
training:	Epoch: [23][603/817]	Loss 0.0542 (0.0388)	
training:	Epoch: [23][604/817]	Loss 0.0075 (0.0387)	
training:	Epoch: [23][605/817]	Loss 0.0074 (0.0387)	
training:	Epoch: [23][606/817]	Loss 0.0077 (0.0386)	
training:	Epoch: [23][607/817]	Loss 0.0091 (0.0386)	
training:	Epoch: [23][608/817]	Loss 0.0078 (0.0385)	
training:	Epoch: [23][609/817]	Loss 0.0072 (0.0385)	
training:	Epoch: [23][610/817]	Loss 0.0074 (0.0384)	
training:	Epoch: [23][611/817]	Loss 0.0073 (0.0384)	
training:	Epoch: [23][612/817]	Loss 0.0089 (0.0383)	
training:	Epoch: [23][613/817]	Loss 0.0078 (0.0383)	
training:	Epoch: [23][614/817]	Loss 0.0082 (0.0382)	
training:	Epoch: [23][615/817]	Loss 0.0071 (0.0382)	
training:	Epoch: [23][616/817]	Loss 0.0108 (0.0381)	
training:	Epoch: [23][617/817]	Loss 0.0082 (0.0381)	
training:	Epoch: [23][618/817]	Loss 0.0080 (0.0380)	
training:	Epoch: [23][619/817]	Loss 0.0079 (0.0380)	
training:	Epoch: [23][620/817]	Loss 0.0073 (0.0379)	
training:	Epoch: [23][621/817]	Loss 0.0075 (0.0379)	
training:	Epoch: [23][622/817]	Loss 0.0079 (0.0378)	
training:	Epoch: [23][623/817]	Loss 0.0090 (0.0378)	
training:	Epoch: [23][624/817]	Loss 0.0080 (0.0377)	
training:	Epoch: [23][625/817]	Loss 0.0080 (0.0377)	
training:	Epoch: [23][626/817]	Loss 0.0084 (0.0377)	
training:	Epoch: [23][627/817]	Loss 0.0073 (0.0376)	
training:	Epoch: [23][628/817]	Loss 0.0723 (0.0377)	
training:	Epoch: [23][629/817]	Loss 0.0110 (0.0376)	
training:	Epoch: [23][630/817]	Loss 0.0071 (0.0376)	
training:	Epoch: [23][631/817]	Loss 0.0075 (0.0375)	
training:	Epoch: [23][632/817]	Loss 0.0071 (0.0375)	
training:	Epoch: [23][633/817]	Loss 0.0076 (0.0374)	
training:	Epoch: [23][634/817]	Loss 0.0070 (0.0374)	
training:	Epoch: [23][635/817]	Loss 0.0075 (0.0373)	
training:	Epoch: [23][636/817]	Loss 0.2198 (0.0376)	
training:	Epoch: [23][637/817]	Loss 0.0081 (0.0376)	
training:	Epoch: [23][638/817]	Loss 0.0173 (0.0375)	
training:	Epoch: [23][639/817]	Loss 0.0078 (0.0375)	
training:	Epoch: [23][640/817]	Loss 0.0072 (0.0374)	
training:	Epoch: [23][641/817]	Loss 0.0073 (0.0374)	
training:	Epoch: [23][642/817]	Loss 0.0075 (0.0374)	
training:	Epoch: [23][643/817]	Loss 0.0086 (0.0373)	
training:	Epoch: [23][644/817]	Loss 0.0068 (0.0373)	
training:	Epoch: [23][645/817]	Loss 0.0067 (0.0372)	
training:	Epoch: [23][646/817]	Loss 0.6099 (0.0381)	
training:	Epoch: [23][647/817]	Loss 0.6322 (0.0390)	
training:	Epoch: [23][648/817]	Loss 0.0079 (0.0390)	
training:	Epoch: [23][649/817]	Loss 0.0073 (0.0389)	
training:	Epoch: [23][650/817]	Loss 0.6133 (0.0398)	
training:	Epoch: [23][651/817]	Loss 0.6177 (0.0407)	
training:	Epoch: [23][652/817]	Loss 0.0076 (0.0406)	
training:	Epoch: [23][653/817]	Loss 0.0080 (0.0406)	
training:	Epoch: [23][654/817]	Loss 0.0075 (0.0405)	
training:	Epoch: [23][655/817]	Loss 0.0085 (0.0405)	
training:	Epoch: [23][656/817]	Loss 0.0078 (0.0404)	
training:	Epoch: [23][657/817]	Loss 0.0075 (0.0404)	
training:	Epoch: [23][658/817]	Loss 0.0078 (0.0403)	
training:	Epoch: [23][659/817]	Loss 0.0096 (0.0403)	
training:	Epoch: [23][660/817]	Loss 0.0082 (0.0402)	
training:	Epoch: [23][661/817]	Loss 0.0078 (0.0402)	
training:	Epoch: [23][662/817]	Loss 1.2267 (0.0420)	
training:	Epoch: [23][663/817]	Loss 0.0081 (0.0419)	
training:	Epoch: [23][664/817]	Loss 0.0075 (0.0419)	
training:	Epoch: [23][665/817]	Loss 0.0078 (0.0418)	
training:	Epoch: [23][666/817]	Loss 0.0114 (0.0418)	
training:	Epoch: [23][667/817]	Loss 0.0070 (0.0417)	
training:	Epoch: [23][668/817]	Loss 0.5794 (0.0425)	
training:	Epoch: [23][669/817]	Loss 0.0076 (0.0425)	
training:	Epoch: [23][670/817]	Loss 0.0081 (0.0424)	
training:	Epoch: [23][671/817]	Loss 0.0085 (0.0424)	
training:	Epoch: [23][672/817]	Loss 0.0072 (0.0423)	
training:	Epoch: [23][673/817]	Loss 0.1301 (0.0425)	
training:	Epoch: [23][674/817]	Loss 0.0079 (0.0424)	
training:	Epoch: [23][675/817]	Loss 0.0087 (0.0424)	
training:	Epoch: [23][676/817]	Loss 0.0082 (0.0423)	
training:	Epoch: [23][677/817]	Loss 0.0182 (0.0423)	
training:	Epoch: [23][678/817]	Loss 0.0085 (0.0422)	
training:	Epoch: [23][679/817]	Loss 0.0076 (0.0422)	
training:	Epoch: [23][680/817]	Loss 0.0077 (0.0421)	
training:	Epoch: [23][681/817]	Loss 0.0087 (0.0421)	
training:	Epoch: [23][682/817]	Loss 0.0076 (0.0420)	
training:	Epoch: [23][683/817]	Loss 0.5943 (0.0428)	
training:	Epoch: [23][684/817]	Loss 0.6016 (0.0437)	
training:	Epoch: [23][685/817]	Loss 0.0071 (0.0436)	
training:	Epoch: [23][686/817]	Loss 0.0079 (0.0435)	
training:	Epoch: [23][687/817]	Loss 0.0081 (0.0435)	
training:	Epoch: [23][688/817]	Loss 0.0074 (0.0434)	
training:	Epoch: [23][689/817]	Loss 0.6222 (0.0443)	
training:	Epoch: [23][690/817]	Loss 0.0081 (0.0442)	
training:	Epoch: [23][691/817]	Loss 0.0076 (0.0442)	
training:	Epoch: [23][692/817]	Loss 0.0078 (0.0441)	
training:	Epoch: [23][693/817]	Loss 0.0103 (0.0441)	
training:	Epoch: [23][694/817]	Loss 0.0077 (0.0440)	
training:	Epoch: [23][695/817]	Loss 0.0085 (0.0440)	
training:	Epoch: [23][696/817]	Loss 0.0099 (0.0439)	
training:	Epoch: [23][697/817]	Loss 0.0079 (0.0439)	
training:	Epoch: [23][698/817]	Loss 0.0084 (0.0438)	
training:	Epoch: [23][699/817]	Loss 0.0081 (0.0438)	
training:	Epoch: [23][700/817]	Loss 0.0114 (0.0437)	
training:	Epoch: [23][701/817]	Loss 0.0102 (0.0437)	
training:	Epoch: [23][702/817]	Loss 0.0112 (0.0436)	
training:	Epoch: [23][703/817]	Loss 0.5251 (0.0443)	
training:	Epoch: [23][704/817]	Loss 0.0098 (0.0443)	
training:	Epoch: [23][705/817]	Loss 0.0091 (0.0442)	
training:	Epoch: [23][706/817]	Loss 0.0084 (0.0442)	
training:	Epoch: [23][707/817]	Loss 0.0081 (0.0441)	
training:	Epoch: [23][708/817]	Loss 0.0083 (0.0441)	
training:	Epoch: [23][709/817]	Loss 0.0079 (0.0440)	
training:	Epoch: [23][710/817]	Loss 0.0074 (0.0440)	
training:	Epoch: [23][711/817]	Loss 0.0075 (0.0439)	
training:	Epoch: [23][712/817]	Loss 0.0098 (0.0439)	
training:	Epoch: [23][713/817]	Loss 0.0084 (0.0438)	
training:	Epoch: [23][714/817]	Loss 0.0078 (0.0438)	
training:	Epoch: [23][715/817]	Loss 0.0088 (0.0437)	
training:	Epoch: [23][716/817]	Loss 0.6166 (0.0445)	
training:	Epoch: [23][717/817]	Loss 0.0088 (0.0445)	
training:	Epoch: [23][718/817]	Loss 0.0090 (0.0444)	
training:	Epoch: [23][719/817]	Loss 0.0084 (0.0444)	
training:	Epoch: [23][720/817]	Loss 0.0083 (0.0443)	
training:	Epoch: [23][721/817]	Loss 0.0094 (0.0443)	
training:	Epoch: [23][722/817]	Loss 0.0076 (0.0442)	
training:	Epoch: [23][723/817]	Loss 0.0135 (0.0442)	
training:	Epoch: [23][724/817]	Loss 0.0086 (0.0441)	
training:	Epoch: [23][725/817]	Loss 0.0081 (0.0441)	
training:	Epoch: [23][726/817]	Loss 0.0086 (0.0440)	
training:	Epoch: [23][727/817]	Loss 0.0090 (0.0440)	
training:	Epoch: [23][728/817]	Loss 0.0084 (0.0439)	
training:	Epoch: [23][729/817]	Loss 0.0087 (0.0439)	
training:	Epoch: [23][730/817]	Loss 0.0076 (0.0438)	
training:	Epoch: [23][731/817]	Loss 0.0084 (0.0438)	
training:	Epoch: [23][732/817]	Loss 0.0084 (0.0437)	
training:	Epoch: [23][733/817]	Loss 0.0094 (0.0437)	
training:	Epoch: [23][734/817]	Loss 0.0077 (0.0436)	
training:	Epoch: [23][735/817]	Loss 0.5218 (0.0443)	
training:	Epoch: [23][736/817]	Loss 1.1858 (0.0458)	
training:	Epoch: [23][737/817]	Loss 0.0100 (0.0458)	
training:	Epoch: [23][738/817]	Loss 0.0095 (0.0457)	
training:	Epoch: [23][739/817]	Loss 0.1757 (0.0459)	
training:	Epoch: [23][740/817]	Loss 0.0079 (0.0459)	
training:	Epoch: [23][741/817]	Loss 0.0078 (0.0458)	
training:	Epoch: [23][742/817]	Loss 0.0091 (0.0458)	
training:	Epoch: [23][743/817]	Loss 0.0087 (0.0457)	
training:	Epoch: [23][744/817]	Loss 0.0093 (0.0457)	
training:	Epoch: [23][745/817]	Loss 0.0115 (0.0456)	
training:	Epoch: [23][746/817]	Loss 0.0101 (0.0456)	
training:	Epoch: [23][747/817]	Loss 0.4910 (0.0462)	
training:	Epoch: [23][748/817]	Loss 0.0081 (0.0461)	
training:	Epoch: [23][749/817]	Loss 0.0082 (0.0461)	
training:	Epoch: [23][750/817]	Loss 0.6030 (0.0468)	
training:	Epoch: [23][751/817]	Loss 0.0085 (0.0468)	
training:	Epoch: [23][752/817]	Loss 0.0083 (0.0467)	
training:	Epoch: [23][753/817]	Loss 0.0080 (0.0467)	
training:	Epoch: [23][754/817]	Loss 0.0087 (0.0466)	
training:	Epoch: [23][755/817]	Loss 0.0134 (0.0466)	
training:	Epoch: [23][756/817]	Loss 0.0086 (0.0465)	
training:	Epoch: [23][757/817]	Loss 0.0084 (0.0465)	
training:	Epoch: [23][758/817]	Loss 0.5804 (0.0472)	
training:	Epoch: [23][759/817]	Loss 0.0093 (0.0471)	
training:	Epoch: [23][760/817]	Loss 0.0105 (0.0471)	
training:	Epoch: [23][761/817]	Loss 0.0082 (0.0470)	
training:	Epoch: [23][762/817]	Loss 0.0077 (0.0470)	
training:	Epoch: [23][763/817]	Loss 0.0315 (0.0469)	
training:	Epoch: [23][764/817]	Loss 0.0082 (0.0469)	
training:	Epoch: [23][765/817]	Loss 0.0075 (0.0468)	
training:	Epoch: [23][766/817]	Loss 0.1003 (0.0469)	
training:	Epoch: [23][767/817]	Loss 0.0088 (0.0469)	
training:	Epoch: [23][768/817]	Loss 0.0288 (0.0468)	
training:	Epoch: [23][769/817]	Loss 0.0091 (0.0468)	
training:	Epoch: [23][770/817]	Loss 0.0087 (0.0467)	
training:	Epoch: [23][771/817]	Loss 0.0080 (0.0467)	
training:	Epoch: [23][772/817]	Loss 0.0078 (0.0466)	
training:	Epoch: [23][773/817]	Loss 0.0091 (0.0466)	
training:	Epoch: [23][774/817]	Loss 0.4667 (0.0471)	
training:	Epoch: [23][775/817]	Loss 0.0100 (0.0471)	
training:	Epoch: [23][776/817]	Loss 0.0085 (0.0470)	
training:	Epoch: [23][777/817]	Loss 0.0083 (0.0470)	
training:	Epoch: [23][778/817]	Loss 0.0103 (0.0469)	
training:	Epoch: [23][779/817]	Loss 0.0097 (0.0469)	
training:	Epoch: [23][780/817]	Loss 0.0087 (0.0468)	
training:	Epoch: [23][781/817]	Loss 0.0078 (0.0468)	
training:	Epoch: [23][782/817]	Loss 0.0095 (0.0467)	
training:	Epoch: [23][783/817]	Loss 0.0078 (0.0467)	
training:	Epoch: [23][784/817]	Loss 0.0680 (0.0467)	
training:	Epoch: [23][785/817]	Loss 0.5685 (0.0474)	
training:	Epoch: [23][786/817]	Loss 0.0081 (0.0473)	
training:	Epoch: [23][787/817]	Loss 0.0084 (0.0473)	
training:	Epoch: [23][788/817]	Loss 0.0087 (0.0472)	
training:	Epoch: [23][789/817]	Loss 0.0086 (0.0472)	
training:	Epoch: [23][790/817]	Loss 0.0089 (0.0471)	
training:	Epoch: [23][791/817]	Loss 0.0095 (0.0471)	
training:	Epoch: [23][792/817]	Loss 0.0088 (0.0470)	
training:	Epoch: [23][793/817]	Loss 0.0108 (0.0470)	
training:	Epoch: [23][794/817]	Loss 0.0084 (0.0470)	
training:	Epoch: [23][795/817]	Loss 0.0084 (0.0469)	
training:	Epoch: [23][796/817]	Loss 0.0096 (0.0469)	
training:	Epoch: [23][797/817]	Loss 0.0092 (0.0468)	
training:	Epoch: [23][798/817]	Loss 0.0089 (0.0468)	
training:	Epoch: [23][799/817]	Loss 0.0090 (0.0467)	
training:	Epoch: [23][800/817]	Loss 0.0085 (0.0467)	
training:	Epoch: [23][801/817]	Loss 0.0096 (0.0466)	
training:	Epoch: [23][802/817]	Loss 0.0081 (0.0466)	
training:	Epoch: [23][803/817]	Loss 0.0078 (0.0465)	
training:	Epoch: [23][804/817]	Loss 0.0087 (0.0465)	
training:	Epoch: [23][805/817]	Loss 0.0084 (0.0464)	
training:	Epoch: [23][806/817]	Loss 0.0086 (0.0464)	
training:	Epoch: [23][807/817]	Loss 0.0091 (0.0463)	
training:	Epoch: [23][808/817]	Loss 0.0313 (0.0463)	
training:	Epoch: [23][809/817]	Loss 0.0090 (0.0463)	
training:	Epoch: [23][810/817]	Loss 0.0077 (0.0462)	
training:	Epoch: [23][811/817]	Loss 0.0083 (0.0462)	
training:	Epoch: [23][812/817]	Loss 0.0600 (0.0462)	
training:	Epoch: [23][813/817]	Loss 0.6067 (0.0469)	
training:	Epoch: [23][814/817]	Loss 0.0079 (0.0468)	
training:	Epoch: [23][815/817]	Loss 0.6358 (0.0476)	
training:	Epoch: [23][816/817]	Loss 0.0075 (0.0475)	
training:	Epoch: [23][817/817]	Loss 0.0081 (0.0475)	
Training:	 Loss: 0.0474

Training:	 ACC: 0.9911 0.9911 0.9921 0.9901
Validation:	 ACC: 0.7839 0.7844 0.7943 0.7735
Validation:	 Best_BACC: 0.8223 0.8218 0.8127 0.8318
Validation:	 Loss: 0.9146
Pretraining:	Epoch 24/200
----------
training:	Epoch: [24][1/817]	Loss 0.0087 (0.0087)	
training:	Epoch: [24][2/817]	Loss 0.0082 (0.0084)	
training:	Epoch: [24][3/817]	Loss 0.0122 (0.0097)	
training:	Epoch: [24][4/817]	Loss 0.6246 (0.1634)	
training:	Epoch: [24][5/817]	Loss 0.0087 (0.1325)	
training:	Epoch: [24][6/817]	Loss 0.0082 (0.1118)	
training:	Epoch: [24][7/817]	Loss 0.0082 (0.0970)	
training:	Epoch: [24][8/817]	Loss 0.0083 (0.0859)	
training:	Epoch: [24][9/817]	Loss 0.0092 (0.0774)	
training:	Epoch: [24][10/817]	Loss 0.0088 (0.0705)	
training:	Epoch: [24][11/817]	Loss 0.0191 (0.0658)	
training:	Epoch: [24][12/817]	Loss 0.0082 (0.0610)	
training:	Epoch: [24][13/817]	Loss 0.0101 (0.0571)	
training:	Epoch: [24][14/817]	Loss 0.0122 (0.0539)	
training:	Epoch: [24][15/817]	Loss 0.0082 (0.0509)	
training:	Epoch: [24][16/817]	Loss 0.0098 (0.0483)	
training:	Epoch: [24][17/817]	Loss 0.0082 (0.0459)	
training:	Epoch: [24][18/817]	Loss 0.0077 (0.0438)	
training:	Epoch: [24][19/817]	Loss 0.0090 (0.0420)	
training:	Epoch: [24][20/817]	Loss 0.0083 (0.0403)	
training:	Epoch: [24][21/817]	Loss 0.0139 (0.0390)	
training:	Epoch: [24][22/817]	Loss 0.0076 (0.0376)	
training:	Epoch: [24][23/817]	Loss 0.0091 (0.0364)	
training:	Epoch: [24][24/817]	Loss 0.0083 (0.0352)	
training:	Epoch: [24][25/817]	Loss 0.0142 (0.0344)	
training:	Epoch: [24][26/817]	Loss 0.0087 (0.0334)	
training:	Epoch: [24][27/817]	Loss 0.0095 (0.0325)	
training:	Epoch: [24][28/817]	Loss 0.0078 (0.0316)	
training:	Epoch: [24][29/817]	Loss 0.0097 (0.0309)	
training:	Epoch: [24][30/817]	Loss 0.0091 (0.0301)	
training:	Epoch: [24][31/817]	Loss 0.0090 (0.0294)	
training:	Epoch: [24][32/817]	Loss 0.0085 (0.0288)	
training:	Epoch: [24][33/817]	Loss 0.0089 (0.0282)	
training:	Epoch: [24][34/817]	Loss 0.0104 (0.0277)	
training:	Epoch: [24][35/817]	Loss 0.0276 (0.0277)	
training:	Epoch: [24][36/817]	Loss 0.0080 (0.0271)	
training:	Epoch: [24][37/817]	Loss 0.0094 (0.0266)	
training:	Epoch: [24][38/817]	Loss 0.0085 (0.0262)	
training:	Epoch: [24][39/817]	Loss 0.0093 (0.0257)	
training:	Epoch: [24][40/817]	Loss 0.0087 (0.0253)	
training:	Epoch: [24][41/817]	Loss 0.0075 (0.0249)	
training:	Epoch: [24][42/817]	Loss 0.0085 (0.0245)	
training:	Epoch: [24][43/817]	Loss 0.0255 (0.0245)	
training:	Epoch: [24][44/817]	Loss 0.0084 (0.0241)	
training:	Epoch: [24][45/817]	Loss 0.0089 (0.0238)	
training:	Epoch: [24][46/817]	Loss 0.0083 (0.0235)	
training:	Epoch: [24][47/817]	Loss 0.0095 (0.0232)	
training:	Epoch: [24][48/817]	Loss 0.0094 (0.0229)	
training:	Epoch: [24][49/817]	Loss 0.0080 (0.0226)	
training:	Epoch: [24][50/817]	Loss 0.0096 (0.0223)	
training:	Epoch: [24][51/817]	Loss 0.0088 (0.0220)	
training:	Epoch: [24][52/817]	Loss 0.0115 (0.0218)	
training:	Epoch: [24][53/817]	Loss 0.6004 (0.0328)	
training:	Epoch: [24][54/817]	Loss 0.0124 (0.0324)	
training:	Epoch: [24][55/817]	Loss 0.0078 (0.0319)	
training:	Epoch: [24][56/817]	Loss 0.0083 (0.0315)	
training:	Epoch: [24][57/817]	Loss 0.0081 (0.0311)	
training:	Epoch: [24][58/817]	Loss 0.0086 (0.0307)	
training:	Epoch: [24][59/817]	Loss 0.0082 (0.0303)	
training:	Epoch: [24][60/817]	Loss 0.0086 (0.0300)	
training:	Epoch: [24][61/817]	Loss 0.0097 (0.0296)	
training:	Epoch: [24][62/817]	Loss 0.0101 (0.0293)	
training:	Epoch: [24][63/817]	Loss 0.0097 (0.0290)	
training:	Epoch: [24][64/817]	Loss 0.0092 (0.0287)	
training:	Epoch: [24][65/817]	Loss 0.0079 (0.0284)	
training:	Epoch: [24][66/817]	Loss 0.0080 (0.0281)	
training:	Epoch: [24][67/817]	Loss 0.5785 (0.0363)	
training:	Epoch: [24][68/817]	Loss 0.0087 (0.0359)	
training:	Epoch: [24][69/817]	Loss 0.6132 (0.0442)	
training:	Epoch: [24][70/817]	Loss 0.0078 (0.0437)	
training:	Epoch: [24][71/817]	Loss 0.0105 (0.0433)	
training:	Epoch: [24][72/817]	Loss 0.0075 (0.0428)	
training:	Epoch: [24][73/817]	Loss 0.0102 (0.0423)	
training:	Epoch: [24][74/817]	Loss 0.0082 (0.0419)	
training:	Epoch: [24][75/817]	Loss 0.0082 (0.0414)	
training:	Epoch: [24][76/817]	Loss 0.0082 (0.0410)	
training:	Epoch: [24][77/817]	Loss 0.0076 (0.0405)	
training:	Epoch: [24][78/817]	Loss 0.0094 (0.0401)	
training:	Epoch: [24][79/817]	Loss 0.0106 (0.0398)	
training:	Epoch: [24][80/817]	Loss 0.0087 (0.0394)	
training:	Epoch: [24][81/817]	Loss 0.0082 (0.0390)	
training:	Epoch: [24][82/817]	Loss 0.0104 (0.0386)	
training:	Epoch: [24][83/817]	Loss 0.0095 (0.0383)	
training:	Epoch: [24][84/817]	Loss 0.0100 (0.0380)	
training:	Epoch: [24][85/817]	Loss 0.0077 (0.0376)	
training:	Epoch: [24][86/817]	Loss 0.0077 (0.0373)	
training:	Epoch: [24][87/817]	Loss 0.0086 (0.0369)	
training:	Epoch: [24][88/817]	Loss 0.0087 (0.0366)	
training:	Epoch: [24][89/817]	Loss 0.0080 (0.0363)	
training:	Epoch: [24][90/817]	Loss 0.0078 (0.0360)	
training:	Epoch: [24][91/817]	Loss 0.0082 (0.0357)	
training:	Epoch: [24][92/817]	Loss 0.0081 (0.0354)	
training:	Epoch: [24][93/817]	Loss 0.0089 (0.0351)	
training:	Epoch: [24][94/817]	Loss 0.0082 (0.0348)	
training:	Epoch: [24][95/817]	Loss 0.0081 (0.0345)	
training:	Epoch: [24][96/817]	Loss 0.0084 (0.0342)	
training:	Epoch: [24][97/817]	Loss 0.0080 (0.0340)	
training:	Epoch: [24][98/817]	Loss 0.0085 (0.0337)	
training:	Epoch: [24][99/817]	Loss 0.0089 (0.0335)	
training:	Epoch: [24][100/817]	Loss 0.0087 (0.0332)	
training:	Epoch: [24][101/817]	Loss 0.0077 (0.0330)	
training:	Epoch: [24][102/817]	Loss 0.0119 (0.0327)	
training:	Epoch: [24][103/817]	Loss 0.5882 (0.0381)	
training:	Epoch: [24][104/817]	Loss 0.7370 (0.0449)	
training:	Epoch: [24][105/817]	Loss 0.0086 (0.0445)	
training:	Epoch: [24][106/817]	Loss 0.6280 (0.0500)	
training:	Epoch: [24][107/817]	Loss 0.0088 (0.0496)	
training:	Epoch: [24][108/817]	Loss 0.0092 (0.0493)	
training:	Epoch: [24][109/817]	Loss 0.0076 (0.0489)	
training:	Epoch: [24][110/817]	Loss 0.0199 (0.0486)	
training:	Epoch: [24][111/817]	Loss 0.0093 (0.0483)	
training:	Epoch: [24][112/817]	Loss 0.0214 (0.0480)	
training:	Epoch: [24][113/817]	Loss 0.5907 (0.0528)	
training:	Epoch: [24][114/817]	Loss 0.0095 (0.0524)	
training:	Epoch: [24][115/817]	Loss 0.0102 (0.0521)	
training:	Epoch: [24][116/817]	Loss 0.0093 (0.0517)	
training:	Epoch: [24][117/817]	Loss 0.0091 (0.0513)	
training:	Epoch: [24][118/817]	Loss 0.0078 (0.0510)	
training:	Epoch: [24][119/817]	Loss 0.0084 (0.0506)	
training:	Epoch: [24][120/817]	Loss 0.0401 (0.0505)	
training:	Epoch: [24][121/817]	Loss 0.0090 (0.0502)	
training:	Epoch: [24][122/817]	Loss 0.0093 (0.0499)	
training:	Epoch: [24][123/817]	Loss 0.0103 (0.0495)	
training:	Epoch: [24][124/817]	Loss 0.0148 (0.0492)	
training:	Epoch: [24][125/817]	Loss 0.0137 (0.0490)	
training:	Epoch: [24][126/817]	Loss 0.6251 (0.0535)	
training:	Epoch: [24][127/817]	Loss 0.0079 (0.0532)	
training:	Epoch: [24][128/817]	Loss 0.0078 (0.0528)	
training:	Epoch: [24][129/817]	Loss 0.0078 (0.0525)	
training:	Epoch: [24][130/817]	Loss 0.0087 (0.0521)	
training:	Epoch: [24][131/817]	Loss 0.0095 (0.0518)	
training:	Epoch: [24][132/817]	Loss 0.0091 (0.0515)	
training:	Epoch: [24][133/817]	Loss 0.0082 (0.0512)	
training:	Epoch: [24][134/817]	Loss 0.0102 (0.0509)	
training:	Epoch: [24][135/817]	Loss 0.2873 (0.0526)	
training:	Epoch: [24][136/817]	Loss 0.0081 (0.0523)	
training:	Epoch: [24][137/817]	Loss 0.0092 (0.0520)	
training:	Epoch: [24][138/817]	Loss 0.0087 (0.0517)	
training:	Epoch: [24][139/817]	Loss 0.0080 (0.0513)	
training:	Epoch: [24][140/817]	Loss 0.0082 (0.0510)	
training:	Epoch: [24][141/817]	Loss 0.0077 (0.0507)	
training:	Epoch: [24][142/817]	Loss 0.0112 (0.0504)	
training:	Epoch: [24][143/817]	Loss 0.0081 (0.0502)	
training:	Epoch: [24][144/817]	Loss 0.0096 (0.0499)	
training:	Epoch: [24][145/817]	Loss 0.0089 (0.0496)	
training:	Epoch: [24][146/817]	Loss 0.0082 (0.0493)	
training:	Epoch: [24][147/817]	Loss 0.0097 (0.0490)	
training:	Epoch: [24][148/817]	Loss 0.1987 (0.0500)	
training:	Epoch: [24][149/817]	Loss 0.0084 (0.0498)	
training:	Epoch: [24][150/817]	Loss 0.0072 (0.0495)	
training:	Epoch: [24][151/817]	Loss 0.0089 (0.0492)	
training:	Epoch: [24][152/817]	Loss 0.0080 (0.0489)	
training:	Epoch: [24][153/817]	Loss 0.0104 (0.0487)	
training:	Epoch: [24][154/817]	Loss 0.0083 (0.0484)	
training:	Epoch: [24][155/817]	Loss 0.0090 (0.0482)	
training:	Epoch: [24][156/817]	Loss 0.0091 (0.0479)	
training:	Epoch: [24][157/817]	Loss 0.0081 (0.0477)	
training:	Epoch: [24][158/817]	Loss 0.0096 (0.0474)	
training:	Epoch: [24][159/817]	Loss 0.0084 (0.0472)	
training:	Epoch: [24][160/817]	Loss 0.0110 (0.0470)	
training:	Epoch: [24][161/817]	Loss 0.0114 (0.0467)	
training:	Epoch: [24][162/817]	Loss 0.0085 (0.0465)	
training:	Epoch: [24][163/817]	Loss 0.0086 (0.0463)	
training:	Epoch: [24][164/817]	Loss 0.0082 (0.0460)	
training:	Epoch: [24][165/817]	Loss 0.0086 (0.0458)	
training:	Epoch: [24][166/817]	Loss 0.0094 (0.0456)	
training:	Epoch: [24][167/817]	Loss 0.0146 (0.0454)	
training:	Epoch: [24][168/817]	Loss 0.0084 (0.0452)	
training:	Epoch: [24][169/817]	Loss 0.0085 (0.0450)	
training:	Epoch: [24][170/817]	Loss 0.0082 (0.0447)	
training:	Epoch: [24][171/817]	Loss 0.0084 (0.0445)	
training:	Epoch: [24][172/817]	Loss 0.0082 (0.0443)	
training:	Epoch: [24][173/817]	Loss 0.0099 (0.0441)	
training:	Epoch: [24][174/817]	Loss 0.0097 (0.0439)	
training:	Epoch: [24][175/817]	Loss 0.0087 (0.0437)	
training:	Epoch: [24][176/817]	Loss 0.0082 (0.0435)	
training:	Epoch: [24][177/817]	Loss 0.0075 (0.0433)	
training:	Epoch: [24][178/817]	Loss 0.0078 (0.0431)	
training:	Epoch: [24][179/817]	Loss 0.0091 (0.0429)	
training:	Epoch: [24][180/817]	Loss 0.0094 (0.0427)	
training:	Epoch: [24][181/817]	Loss 0.0082 (0.0426)	
training:	Epoch: [24][182/817]	Loss 0.0099 (0.0424)	
training:	Epoch: [24][183/817]	Loss 0.0079 (0.0422)	
training:	Epoch: [24][184/817]	Loss 0.0084 (0.0420)	
training:	Epoch: [24][185/817]	Loss 0.0080 (0.0418)	
training:	Epoch: [24][186/817]	Loss 0.0076 (0.0416)	
training:	Epoch: [24][187/817]	Loss 0.0096 (0.0415)	
training:	Epoch: [24][188/817]	Loss 0.0094 (0.0413)	
training:	Epoch: [24][189/817]	Loss 0.6066 (0.0443)	
training:	Epoch: [24][190/817]	Loss 0.6035 (0.0472)	
training:	Epoch: [24][191/817]	Loss 0.0288 (0.0471)	
training:	Epoch: [24][192/817]	Loss 0.0102 (0.0469)	
training:	Epoch: [24][193/817]	Loss 0.0112 (0.0468)	
training:	Epoch: [24][194/817]	Loss 0.0079 (0.0466)	
training:	Epoch: [24][195/817]	Loss 0.0085 (0.0464)	
training:	Epoch: [24][196/817]	Loss 0.0075 (0.0462)	
training:	Epoch: [24][197/817]	Loss 0.0084 (0.0460)	
training:	Epoch: [24][198/817]	Loss 0.0087 (0.0458)	
training:	Epoch: [24][199/817]	Loss 0.0189 (0.0456)	
training:	Epoch: [24][200/817]	Loss 0.5437 (0.0481)	
training:	Epoch: [24][201/817]	Loss 0.0078 (0.0479)	
training:	Epoch: [24][202/817]	Loss 0.0091 (0.0477)	
training:	Epoch: [24][203/817]	Loss 0.0078 (0.0475)	
training:	Epoch: [24][204/817]	Loss 0.0086 (0.0474)	
training:	Epoch: [24][205/817]	Loss 0.0077 (0.0472)	
training:	Epoch: [24][206/817]	Loss 0.6490 (0.0501)	
training:	Epoch: [24][207/817]	Loss 0.0076 (0.0499)	
training:	Epoch: [24][208/817]	Loss 0.0081 (0.0497)	
training:	Epoch: [24][209/817]	Loss 0.6190 (0.0524)	
training:	Epoch: [24][210/817]	Loss 0.0085 (0.0522)	
training:	Epoch: [24][211/817]	Loss 0.0081 (0.0520)	
training:	Epoch: [24][212/817]	Loss 0.0084 (0.0518)	
training:	Epoch: [24][213/817]	Loss 0.1164 (0.0521)	
training:	Epoch: [24][214/817]	Loss 0.0173 (0.0519)	
training:	Epoch: [24][215/817]	Loss 0.0113 (0.0517)	
training:	Epoch: [24][216/817]	Loss 0.0090 (0.0515)	
training:	Epoch: [24][217/817]	Loss 0.0145 (0.0514)	
training:	Epoch: [24][218/817]	Loss 0.2334 (0.0522)	
training:	Epoch: [24][219/817]	Loss 0.0099 (0.0520)	
training:	Epoch: [24][220/817]	Loss 0.0101 (0.0518)	
training:	Epoch: [24][221/817]	Loss 0.0082 (0.0516)	
training:	Epoch: [24][222/817]	Loss 0.0080 (0.0514)	
training:	Epoch: [24][223/817]	Loss 0.0086 (0.0512)	
training:	Epoch: [24][224/817]	Loss 0.1120 (0.0515)	
training:	Epoch: [24][225/817]	Loss 0.0084 (0.0513)	
training:	Epoch: [24][226/817]	Loss 0.0076 (0.0511)	
training:	Epoch: [24][227/817]	Loss 0.5031 (0.0531)	
training:	Epoch: [24][228/817]	Loss 0.0080 (0.0529)	
training:	Epoch: [24][229/817]	Loss 0.2711 (0.0539)	
training:	Epoch: [24][230/817]	Loss 0.0085 (0.0537)	
training:	Epoch: [24][231/817]	Loss 0.0119 (0.0535)	
training:	Epoch: [24][232/817]	Loss 0.6101 (0.0559)	
training:	Epoch: [24][233/817]	Loss 0.0082 (0.0557)	
training:	Epoch: [24][234/817]	Loss 0.0119 (0.0555)	
training:	Epoch: [24][235/817]	Loss 0.0083 (0.0553)	
training:	Epoch: [24][236/817]	Loss 0.0085 (0.0551)	
training:	Epoch: [24][237/817]	Loss 0.0083 (0.0549)	
training:	Epoch: [24][238/817]	Loss 0.0080 (0.0547)	
training:	Epoch: [24][239/817]	Loss 0.0076 (0.0545)	
training:	Epoch: [24][240/817]	Loss 0.0081 (0.0543)	
training:	Epoch: [24][241/817]	Loss 0.0501 (0.0543)	
training:	Epoch: [24][242/817]	Loss 0.6153 (0.0566)	
training:	Epoch: [24][243/817]	Loss 0.0082 (0.0564)	
training:	Epoch: [24][244/817]	Loss 0.0079 (0.0562)	
training:	Epoch: [24][245/817]	Loss 0.0084 (0.0560)	
training:	Epoch: [24][246/817]	Loss 0.0221 (0.0559)	
training:	Epoch: [24][247/817]	Loss 0.5268 (0.0578)	
training:	Epoch: [24][248/817]	Loss 0.0085 (0.0576)	
training:	Epoch: [24][249/817]	Loss 0.0080 (0.0574)	
training:	Epoch: [24][250/817]	Loss 0.0346 (0.0573)	
training:	Epoch: [24][251/817]	Loss 0.0085 (0.0571)	
training:	Epoch: [24][252/817]	Loss 0.0084 (0.0569)	
training:	Epoch: [24][253/817]	Loss 0.0355 (0.0568)	
training:	Epoch: [24][254/817]	Loss 0.0083 (0.0566)	
training:	Epoch: [24][255/817]	Loss 0.0094 (0.0564)	
training:	Epoch: [24][256/817]	Loss 0.0078 (0.0563)	
training:	Epoch: [24][257/817]	Loss 0.0081 (0.0561)	
training:	Epoch: [24][258/817]	Loss 0.3467 (0.0572)	
training:	Epoch: [24][259/817]	Loss 0.0082 (0.0570)	
training:	Epoch: [24][260/817]	Loss 0.0096 (0.0568)	
training:	Epoch: [24][261/817]	Loss 0.0150 (0.0567)	
training:	Epoch: [24][262/817]	Loss 0.5900 (0.0587)	
training:	Epoch: [24][263/817]	Loss 0.0075 (0.0585)	
training:	Epoch: [24][264/817]	Loss 0.0083 (0.0583)	
training:	Epoch: [24][265/817]	Loss 0.0084 (0.0581)	
training:	Epoch: [24][266/817]	Loss 0.0080 (0.0579)	
training:	Epoch: [24][267/817]	Loss 0.0093 (0.0578)	
training:	Epoch: [24][268/817]	Loss 0.0102 (0.0576)	
training:	Epoch: [24][269/817]	Loss 0.0108 (0.0574)	
training:	Epoch: [24][270/817]	Loss 0.0080 (0.0572)	
training:	Epoch: [24][271/817]	Loss 0.0079 (0.0570)	
training:	Epoch: [24][272/817]	Loss 0.0097 (0.0569)	
training:	Epoch: [24][273/817]	Loss 0.3888 (0.0581)	
training:	Epoch: [24][274/817]	Loss 0.0218 (0.0579)	
training:	Epoch: [24][275/817]	Loss 0.0103 (0.0578)	
training:	Epoch: [24][276/817]	Loss 0.0090 (0.0576)	
training:	Epoch: [24][277/817]	Loss 0.0079 (0.0574)	
training:	Epoch: [24][278/817]	Loss 0.0543 (0.0574)	
training:	Epoch: [24][279/817]	Loss 0.0088 (0.0572)	
training:	Epoch: [24][280/817]	Loss 0.0085 (0.0571)	
training:	Epoch: [24][281/817]	Loss 0.0083 (0.0569)	
training:	Epoch: [24][282/817]	Loss 0.0084 (0.0567)	
training:	Epoch: [24][283/817]	Loss 0.0083 (0.0565)	
training:	Epoch: [24][284/817]	Loss 0.0076 (0.0564)	
training:	Epoch: [24][285/817]	Loss 0.0083 (0.0562)	
training:	Epoch: [24][286/817]	Loss 0.0080 (0.0560)	
training:	Epoch: [24][287/817]	Loss 0.0090 (0.0559)	
training:	Epoch: [24][288/817]	Loss 0.0075 (0.0557)	
training:	Epoch: [24][289/817]	Loss 0.0087 (0.0555)	
training:	Epoch: [24][290/817]	Loss 0.1633 (0.0559)	
training:	Epoch: [24][291/817]	Loss 0.0078 (0.0557)	
training:	Epoch: [24][292/817]	Loss 0.0080 (0.0556)	
training:	Epoch: [24][293/817]	Loss 0.0084 (0.0554)	
training:	Epoch: [24][294/817]	Loss 0.0084 (0.0553)	
training:	Epoch: [24][295/817]	Loss 0.0078 (0.0551)	
training:	Epoch: [24][296/817]	Loss 0.0094 (0.0549)	
training:	Epoch: [24][297/817]	Loss 0.0099 (0.0548)	
training:	Epoch: [24][298/817]	Loss 0.0090 (0.0546)	
training:	Epoch: [24][299/817]	Loss 0.0101 (0.0545)	
training:	Epoch: [24][300/817]	Loss 0.0082 (0.0543)	
training:	Epoch: [24][301/817]	Loss 0.0092 (0.0542)	
training:	Epoch: [24][302/817]	Loss 0.0092 (0.0540)	
training:	Epoch: [24][303/817]	Loss 0.0088 (0.0539)	
training:	Epoch: [24][304/817]	Loss 0.0107 (0.0537)	
training:	Epoch: [24][305/817]	Loss 0.0081 (0.0536)	
training:	Epoch: [24][306/817]	Loss 0.0075 (0.0534)	
training:	Epoch: [24][307/817]	Loss 0.0077 (0.0533)	
training:	Epoch: [24][308/817]	Loss 0.0084 (0.0532)	
training:	Epoch: [24][309/817]	Loss 0.0093 (0.0530)	
training:	Epoch: [24][310/817]	Loss 0.0899 (0.0531)	
training:	Epoch: [24][311/817]	Loss 0.0076 (0.0530)	
training:	Epoch: [24][312/817]	Loss 0.0086 (0.0528)	
training:	Epoch: [24][313/817]	Loss 0.0084 (0.0527)	
training:	Epoch: [24][314/817]	Loss 0.0082 (0.0526)	
training:	Epoch: [24][315/817]	Loss 0.0077 (0.0524)	
training:	Epoch: [24][316/817]	Loss 0.0074 (0.0523)	
training:	Epoch: [24][317/817]	Loss 0.0627 (0.0523)	
training:	Epoch: [24][318/817]	Loss 0.0099 (0.0522)	
training:	Epoch: [24][319/817]	Loss 0.5858 (0.0538)	
training:	Epoch: [24][320/817]	Loss 0.0089 (0.0537)	
training:	Epoch: [24][321/817]	Loss 0.0088 (0.0536)	
training:	Epoch: [24][322/817]	Loss 1.1847 (0.0571)	
training:	Epoch: [24][323/817]	Loss 0.1659 (0.0574)	
training:	Epoch: [24][324/817]	Loss 0.0077 (0.0573)	
training:	Epoch: [24][325/817]	Loss 0.0085 (0.0571)	
training:	Epoch: [24][326/817]	Loss 0.0085 (0.0570)	
training:	Epoch: [24][327/817]	Loss 0.0135 (0.0568)	
training:	Epoch: [24][328/817]	Loss 0.0077 (0.0567)	
training:	Epoch: [24][329/817]	Loss 0.0085 (0.0565)	
training:	Epoch: [24][330/817]	Loss 0.0079 (0.0564)	
training:	Epoch: [24][331/817]	Loss 0.0076 (0.0562)	
training:	Epoch: [24][332/817]	Loss 0.0082 (0.0561)	
training:	Epoch: [24][333/817]	Loss 0.1535 (0.0564)	
training:	Epoch: [24][334/817]	Loss 0.0083 (0.0562)	
training:	Epoch: [24][335/817]	Loss 0.0600 (0.0563)	
training:	Epoch: [24][336/817]	Loss 0.0075 (0.0561)	
training:	Epoch: [24][337/817]	Loss 0.0101 (0.0560)	
training:	Epoch: [24][338/817]	Loss 0.0090 (0.0558)	
training:	Epoch: [24][339/817]	Loss 0.1348 (0.0561)	
training:	Epoch: [24][340/817]	Loss 0.0088 (0.0559)	
training:	Epoch: [24][341/817]	Loss 0.5586 (0.0574)	
training:	Epoch: [24][342/817]	Loss 0.0079 (0.0573)	
training:	Epoch: [24][343/817]	Loss 0.0112 (0.0571)	
training:	Epoch: [24][344/817]	Loss 0.0246 (0.0570)	
training:	Epoch: [24][345/817]	Loss 0.0118 (0.0569)	
training:	Epoch: [24][346/817]	Loss 0.0084 (0.0568)	
training:	Epoch: [24][347/817]	Loss 0.0083 (0.0566)	
training:	Epoch: [24][348/817]	Loss 0.0096 (0.0565)	
training:	Epoch: [24][349/817]	Loss 0.0077 (0.0563)	
training:	Epoch: [24][350/817]	Loss 0.0075 (0.0562)	
training:	Epoch: [24][351/817]	Loss 0.0086 (0.0561)	
training:	Epoch: [24][352/817]	Loss 0.0091 (0.0559)	
training:	Epoch: [24][353/817]	Loss 0.0086 (0.0558)	
training:	Epoch: [24][354/817]	Loss 0.0087 (0.0557)	
training:	Epoch: [24][355/817]	Loss 0.0080 (0.0555)	
training:	Epoch: [24][356/817]	Loss 0.0079 (0.0554)	
training:	Epoch: [24][357/817]	Loss 0.0086 (0.0553)	
training:	Epoch: [24][358/817]	Loss 0.0089 (0.0551)	
training:	Epoch: [24][359/817]	Loss 0.0101 (0.0550)	
training:	Epoch: [24][360/817]	Loss 0.0088 (0.0549)	
training:	Epoch: [24][361/817]	Loss 0.0095 (0.0548)	
training:	Epoch: [24][362/817]	Loss 0.0460 (0.0547)	
training:	Epoch: [24][363/817]	Loss 0.0089 (0.0546)	
training:	Epoch: [24][364/817]	Loss 0.0082 (0.0545)	
training:	Epoch: [24][365/817]	Loss 0.0078 (0.0544)	
training:	Epoch: [24][366/817]	Loss 0.6166 (0.0559)	
training:	Epoch: [24][367/817]	Loss 0.0090 (0.0558)	
training:	Epoch: [24][368/817]	Loss 0.0097 (0.0556)	
training:	Epoch: [24][369/817]	Loss 0.0075 (0.0555)	
training:	Epoch: [24][370/817]	Loss 0.0090 (0.0554)	
training:	Epoch: [24][371/817]	Loss 0.0083 (0.0553)	
training:	Epoch: [24][372/817]	Loss 0.1463 (0.0555)	
training:	Epoch: [24][373/817]	Loss 0.0085 (0.0554)	
training:	Epoch: [24][374/817]	Loss 0.0139 (0.0553)	
training:	Epoch: [24][375/817]	Loss 0.0088 (0.0551)	
training:	Epoch: [24][376/817]	Loss 0.0098 (0.0550)	
training:	Epoch: [24][377/817]	Loss 0.6020 (0.0565)	
training:	Epoch: [24][378/817]	Loss 0.0079 (0.0563)	
training:	Epoch: [24][379/817]	Loss 0.0084 (0.0562)	
training:	Epoch: [24][380/817]	Loss 0.0078 (0.0561)	
training:	Epoch: [24][381/817]	Loss 0.0101 (0.0560)	
training:	Epoch: [24][382/817]	Loss 0.0080 (0.0558)	
training:	Epoch: [24][383/817]	Loss 0.0093 (0.0557)	
training:	Epoch: [24][384/817]	Loss 0.0084 (0.0556)	
training:	Epoch: [24][385/817]	Loss 0.0080 (0.0555)	
training:	Epoch: [24][386/817]	Loss 0.0195 (0.0554)	
training:	Epoch: [24][387/817]	Loss 0.0087 (0.0553)	
training:	Epoch: [24][388/817]	Loss 0.0083 (0.0551)	
training:	Epoch: [24][389/817]	Loss 0.0083 (0.0550)	
training:	Epoch: [24][390/817]	Loss 0.0094 (0.0549)	
training:	Epoch: [24][391/817]	Loss 0.3862 (0.0557)	
training:	Epoch: [24][392/817]	Loss 0.0078 (0.0556)	
training:	Epoch: [24][393/817]	Loss 0.0090 (0.0555)	
training:	Epoch: [24][394/817]	Loss 0.0080 (0.0554)	
training:	Epoch: [24][395/817]	Loss 0.0127 (0.0553)	
training:	Epoch: [24][396/817]	Loss 0.0078 (0.0552)	
training:	Epoch: [24][397/817]	Loss 0.0077 (0.0550)	
training:	Epoch: [24][398/817]	Loss 0.0079 (0.0549)	
training:	Epoch: [24][399/817]	Loss 0.0085 (0.0548)	
training:	Epoch: [24][400/817]	Loss 0.0077 (0.0547)	
training:	Epoch: [24][401/817]	Loss 0.6155 (0.0561)	
training:	Epoch: [24][402/817]	Loss 0.0079 (0.0560)	
training:	Epoch: [24][403/817]	Loss 0.0080 (0.0558)	
training:	Epoch: [24][404/817]	Loss 0.0115 (0.0557)	
training:	Epoch: [24][405/817]	Loss 0.0083 (0.0556)	
training:	Epoch: [24][406/817]	Loss 0.0072 (0.0555)	
training:	Epoch: [24][407/817]	Loss 0.0112 (0.0554)	
training:	Epoch: [24][408/817]	Loss 0.0093 (0.0553)	
training:	Epoch: [24][409/817]	Loss 0.0098 (0.0552)	
training:	Epoch: [24][410/817]	Loss 0.0080 (0.0550)	
training:	Epoch: [24][411/817]	Loss 0.0088 (0.0549)	
training:	Epoch: [24][412/817]	Loss 0.0077 (0.0548)	
training:	Epoch: [24][413/817]	Loss 0.0081 (0.0547)	
training:	Epoch: [24][414/817]	Loss 0.0087 (0.0546)	
training:	Epoch: [24][415/817]	Loss 0.0119 (0.0545)	
training:	Epoch: [24][416/817]	Loss 0.0087 (0.0544)	
training:	Epoch: [24][417/817]	Loss 0.0078 (0.0543)	
training:	Epoch: [24][418/817]	Loss 0.0123 (0.0542)	
training:	Epoch: [24][419/817]	Loss 0.0361 (0.0541)	
training:	Epoch: [24][420/817]	Loss 0.0075 (0.0540)	
training:	Epoch: [24][421/817]	Loss 0.0076 (0.0539)	
training:	Epoch: [24][422/817]	Loss 0.0074 (0.0538)	
training:	Epoch: [24][423/817]	Loss 0.0082 (0.0537)	
training:	Epoch: [24][424/817]	Loss 0.0077 (0.0536)	
training:	Epoch: [24][425/817]	Loss 0.0082 (0.0535)	
training:	Epoch: [24][426/817]	Loss 0.0082 (0.0534)	
training:	Epoch: [24][427/817]	Loss 0.0085 (0.0533)	
training:	Epoch: [24][428/817]	Loss 0.0075 (0.0532)	
training:	Epoch: [24][429/817]	Loss 0.0078 (0.0530)	
training:	Epoch: [24][430/817]	Loss 0.0072 (0.0529)	
training:	Epoch: [24][431/817]	Loss 0.0072 (0.0528)	
training:	Epoch: [24][432/817]	Loss 0.0078 (0.0527)	
training:	Epoch: [24][433/817]	Loss 0.0175 (0.0527)	
training:	Epoch: [24][434/817]	Loss 0.0080 (0.0525)	
training:	Epoch: [24][435/817]	Loss 0.0356 (0.0525)	
training:	Epoch: [24][436/817]	Loss 0.0083 (0.0524)	
training:	Epoch: [24][437/817]	Loss 0.2918 (0.0530)	
training:	Epoch: [24][438/817]	Loss 0.0075 (0.0529)	
training:	Epoch: [24][439/817]	Loss 0.0092 (0.0528)	
training:	Epoch: [24][440/817]	Loss 0.0150 (0.0527)	
training:	Epoch: [24][441/817]	Loss 0.0211 (0.0526)	
training:	Epoch: [24][442/817]	Loss 0.0076 (0.0525)	
training:	Epoch: [24][443/817]	Loss 0.6118 (0.0538)	
training:	Epoch: [24][444/817]	Loss 0.1057 (0.0539)	
training:	Epoch: [24][445/817]	Loss 0.0232 (0.0538)	
training:	Epoch: [24][446/817]	Loss 0.0084 (0.0537)	
training:	Epoch: [24][447/817]	Loss 0.0080 (0.0536)	
training:	Epoch: [24][448/817]	Loss 0.0085 (0.0535)	
training:	Epoch: [24][449/817]	Loss 0.0086 (0.0534)	
training:	Epoch: [24][450/817]	Loss 0.0080 (0.0533)	
training:	Epoch: [24][451/817]	Loss 0.0071 (0.0532)	
training:	Epoch: [24][452/817]	Loss 0.0081 (0.0531)	
training:	Epoch: [24][453/817]	Loss 0.0101 (0.0530)	
training:	Epoch: [24][454/817]	Loss 0.0079 (0.0529)	
training:	Epoch: [24][455/817]	Loss 0.0072 (0.0528)	
training:	Epoch: [24][456/817]	Loss 0.0129 (0.0527)	
training:	Epoch: [24][457/817]	Loss 0.0078 (0.0526)	
training:	Epoch: [24][458/817]	Loss 0.0459 (0.0526)	
training:	Epoch: [24][459/817]	Loss 0.0077 (0.0525)	
training:	Epoch: [24][460/817]	Loss 0.0096 (0.0524)	
training:	Epoch: [24][461/817]	Loss 0.0077 (0.0523)	
training:	Epoch: [24][462/817]	Loss 0.0088 (0.0522)	
training:	Epoch: [24][463/817]	Loss 0.0084 (0.0521)	
training:	Epoch: [24][464/817]	Loss 0.0082 (0.0520)	
training:	Epoch: [24][465/817]	Loss 0.0075 (0.0519)	
training:	Epoch: [24][466/817]	Loss 0.4778 (0.0528)	
training:	Epoch: [24][467/817]	Loss 0.5298 (0.0539)	
training:	Epoch: [24][468/817]	Loss 0.0077 (0.0538)	
training:	Epoch: [24][469/817]	Loss 0.0091 (0.0537)	
training:	Epoch: [24][470/817]	Loss 0.0098 (0.0536)	
training:	Epoch: [24][471/817]	Loss 0.0083 (0.0535)	
training:	Epoch: [24][472/817]	Loss 0.1184 (0.0536)	
training:	Epoch: [24][473/817]	Loss 0.0076 (0.0535)	
training:	Epoch: [24][474/817]	Loss 0.0090 (0.0534)	
training:	Epoch: [24][475/817]	Loss 0.0080 (0.0533)	
training:	Epoch: [24][476/817]	Loss 0.0111 (0.0532)	
training:	Epoch: [24][477/817]	Loss 0.0211 (0.0532)	
training:	Epoch: [24][478/817]	Loss 0.0093 (0.0531)	
training:	Epoch: [24][479/817]	Loss 0.0071 (0.0530)	
training:	Epoch: [24][480/817]	Loss 0.0253 (0.0529)	
training:	Epoch: [24][481/817]	Loss 0.0074 (0.0528)	
training:	Epoch: [24][482/817]	Loss 0.0080 (0.0527)	
training:	Epoch: [24][483/817]	Loss 0.0075 (0.0527)	
training:	Epoch: [24][484/817]	Loss 0.0070 (0.0526)	
training:	Epoch: [24][485/817]	Loss 0.0074 (0.0525)	
training:	Epoch: [24][486/817]	Loss 0.0075 (0.0524)	
training:	Epoch: [24][487/817]	Loss 0.0085 (0.0523)	
training:	Epoch: [24][488/817]	Loss 0.0077 (0.0522)	
training:	Epoch: [24][489/817]	Loss 0.0077 (0.0521)	
training:	Epoch: [24][490/817]	Loss 0.0081 (0.0520)	
training:	Epoch: [24][491/817]	Loss 0.0298 (0.0520)	
training:	Epoch: [24][492/817]	Loss 0.0084 (0.0519)	
training:	Epoch: [24][493/817]	Loss 0.0077 (0.0518)	
training:	Epoch: [24][494/817]	Loss 0.0089 (0.0517)	
training:	Epoch: [24][495/817]	Loss 0.0074 (0.0516)	
training:	Epoch: [24][496/817]	Loss 0.0074 (0.0515)	
training:	Epoch: [24][497/817]	Loss 0.1754 (0.0518)	
training:	Epoch: [24][498/817]	Loss 0.0074 (0.0517)	
training:	Epoch: [24][499/817]	Loss 0.0072 (0.0516)	
training:	Epoch: [24][500/817]	Loss 0.0077 (0.0515)	
training:	Epoch: [24][501/817]	Loss 0.0083 (0.0514)	
training:	Epoch: [24][502/817]	Loss 0.0076 (0.0513)	
training:	Epoch: [24][503/817]	Loss 0.0071 (0.0512)	
training:	Epoch: [24][504/817]	Loss 0.0079 (0.0512)	
training:	Epoch: [24][505/817]	Loss 0.0075 (0.0511)	
training:	Epoch: [24][506/817]	Loss 0.0558 (0.0511)	
training:	Epoch: [24][507/817]	Loss 0.0087 (0.0510)	
training:	Epoch: [24][508/817]	Loss 0.0069 (0.0509)	
training:	Epoch: [24][509/817]	Loss 0.0074 (0.0508)	
training:	Epoch: [24][510/817]	Loss 0.0080 (0.0507)	
training:	Epoch: [24][511/817]	Loss 0.0070 (0.0507)	
training:	Epoch: [24][512/817]	Loss 0.0109 (0.0506)	
training:	Epoch: [24][513/817]	Loss 0.0071 (0.0505)	
training:	Epoch: [24][514/817]	Loss 0.0096 (0.0504)	
training:	Epoch: [24][515/817]	Loss 0.0073 (0.0503)	
training:	Epoch: [24][516/817]	Loss 0.0088 (0.0502)	
training:	Epoch: [24][517/817]	Loss 0.0091 (0.0502)	
training:	Epoch: [24][518/817]	Loss 0.0147 (0.0501)	
training:	Epoch: [24][519/817]	Loss 0.0085 (0.0500)	
training:	Epoch: [24][520/817]	Loss 0.0109 (0.0499)	
training:	Epoch: [24][521/817]	Loss 0.0150 (0.0499)	
training:	Epoch: [24][522/817]	Loss 0.0077 (0.0498)	
training:	Epoch: [24][523/817]	Loss 0.0086 (0.0497)	
training:	Epoch: [24][524/817]	Loss 0.0076 (0.0496)	
training:	Epoch: [24][525/817]	Loss 0.0079 (0.0496)	
training:	Epoch: [24][526/817]	Loss 0.0071 (0.0495)	
training:	Epoch: [24][527/817]	Loss 0.0106 (0.0494)	
training:	Epoch: [24][528/817]	Loss 0.0070 (0.0493)	
training:	Epoch: [24][529/817]	Loss 0.0107 (0.0493)	
training:	Epoch: [24][530/817]	Loss 0.0095 (0.0492)	
training:	Epoch: [24][531/817]	Loss 0.0077 (0.0491)	
training:	Epoch: [24][532/817]	Loss 0.0098 (0.0490)	
training:	Epoch: [24][533/817]	Loss 0.0072 (0.0489)	
training:	Epoch: [24][534/817]	Loss 0.0342 (0.0489)	
training:	Epoch: [24][535/817]	Loss 0.0127 (0.0489)	
training:	Epoch: [24][536/817]	Loss 0.0089 (0.0488)	
training:	Epoch: [24][537/817]	Loss 0.0081 (0.0487)	
training:	Epoch: [24][538/817]	Loss 0.0078 (0.0486)	
training:	Epoch: [24][539/817]	Loss 0.0095 (0.0486)	
training:	Epoch: [24][540/817]	Loss 0.0075 (0.0485)	
training:	Epoch: [24][541/817]	Loss 0.0074 (0.0484)	
training:	Epoch: [24][542/817]	Loss 0.6228 (0.0495)	
training:	Epoch: [24][543/817]	Loss 0.0283 (0.0494)	
training:	Epoch: [24][544/817]	Loss 0.0089 (0.0493)	
training:	Epoch: [24][545/817]	Loss 0.0079 (0.0493)	
training:	Epoch: [24][546/817]	Loss 0.0074 (0.0492)	
training:	Epoch: [24][547/817]	Loss 0.5858 (0.0502)	
training:	Epoch: [24][548/817]	Loss 0.0095 (0.0501)	
training:	Epoch: [24][549/817]	Loss 0.0078 (0.0500)	
training:	Epoch: [24][550/817]	Loss 0.5908 (0.0510)	
training:	Epoch: [24][551/817]	Loss 0.0071 (0.0509)	
training:	Epoch: [24][552/817]	Loss 0.0090 (0.0509)	
training:	Epoch: [24][553/817]	Loss 0.6173 (0.0519)	
training:	Epoch: [24][554/817]	Loss 0.0195 (0.0518)	
training:	Epoch: [24][555/817]	Loss 0.0075 (0.0517)	
training:	Epoch: [24][556/817]	Loss 0.0074 (0.0517)	
training:	Epoch: [24][557/817]	Loss 0.0082 (0.0516)	
training:	Epoch: [24][558/817]	Loss 0.0084 (0.0515)	
training:	Epoch: [24][559/817]	Loss 0.0088 (0.0514)	
training:	Epoch: [24][560/817]	Loss 0.0083 (0.0513)	
training:	Epoch: [24][561/817]	Loss 0.0073 (0.0513)	
training:	Epoch: [24][562/817]	Loss 0.5761 (0.0522)	
training:	Epoch: [24][563/817]	Loss 0.0078 (0.0521)	
training:	Epoch: [24][564/817]	Loss 0.0150 (0.0521)	
training:	Epoch: [24][565/817]	Loss 0.0078 (0.0520)	
training:	Epoch: [24][566/817]	Loss 0.0095 (0.0519)	
training:	Epoch: [24][567/817]	Loss 0.0077 (0.0518)	
training:	Epoch: [24][568/817]	Loss 0.0071 (0.0517)	
training:	Epoch: [24][569/817]	Loss 0.0081 (0.0517)	
training:	Epoch: [24][570/817]	Loss 0.1301 (0.0518)	
training:	Epoch: [24][571/817]	Loss 0.0257 (0.0518)	
training:	Epoch: [24][572/817]	Loss 0.0083 (0.0517)	
training:	Epoch: [24][573/817]	Loss 0.0091 (0.0516)	
training:	Epoch: [24][574/817]	Loss 0.0163 (0.0516)	
training:	Epoch: [24][575/817]	Loss 0.0096 (0.0515)	
training:	Epoch: [24][576/817]	Loss 0.0084 (0.0514)	
training:	Epoch: [24][577/817]	Loss 0.0076 (0.0513)	
training:	Epoch: [24][578/817]	Loss 0.0077 (0.0513)	
training:	Epoch: [24][579/817]	Loss 0.0073 (0.0512)	
training:	Epoch: [24][580/817]	Loss 0.0104 (0.0511)	
training:	Epoch: [24][581/817]	Loss 0.0076 (0.0510)	
training:	Epoch: [24][582/817]	Loss 0.0071 (0.0510)	
training:	Epoch: [24][583/817]	Loss 0.0106 (0.0509)	
training:	Epoch: [24][584/817]	Loss 0.0071 (0.0508)	
training:	Epoch: [24][585/817]	Loss 0.0096 (0.0507)	
training:	Epoch: [24][586/817]	Loss 0.0079 (0.0507)	
training:	Epoch: [24][587/817]	Loss 0.0092 (0.0506)	
training:	Epoch: [24][588/817]	Loss 0.0090 (0.0505)	
training:	Epoch: [24][589/817]	Loss 0.0081 (0.0505)	
training:	Epoch: [24][590/817]	Loss 0.0077 (0.0504)	
training:	Epoch: [24][591/817]	Loss 0.0085 (0.0503)	
training:	Epoch: [24][592/817]	Loss 0.0075 (0.0502)	
training:	Epoch: [24][593/817]	Loss 0.0087 (0.0502)	
training:	Epoch: [24][594/817]	Loss 0.0078 (0.0501)	
training:	Epoch: [24][595/817]	Loss 0.0077 (0.0500)	
training:	Epoch: [24][596/817]	Loss 0.0079 (0.0500)	
training:	Epoch: [24][597/817]	Loss 0.0083 (0.0499)	
training:	Epoch: [24][598/817]	Loss 0.0093 (0.0498)	
training:	Epoch: [24][599/817]	Loss 0.0070 (0.0497)	
training:	Epoch: [24][600/817]	Loss 0.0084 (0.0497)	
training:	Epoch: [24][601/817]	Loss 0.0085 (0.0496)	
training:	Epoch: [24][602/817]	Loss 0.0085 (0.0495)	
training:	Epoch: [24][603/817]	Loss 0.0095 (0.0495)	
training:	Epoch: [24][604/817]	Loss 0.0075 (0.0494)	
training:	Epoch: [24][605/817]	Loss 0.0132 (0.0493)	
training:	Epoch: [24][606/817]	Loss 0.0075 (0.0493)	
training:	Epoch: [24][607/817]	Loss 0.0075 (0.0492)	
training:	Epoch: [24][608/817]	Loss 0.0090 (0.0491)	
training:	Epoch: [24][609/817]	Loss 0.0081 (0.0491)	
training:	Epoch: [24][610/817]	Loss 0.6275 (0.0500)	
training:	Epoch: [24][611/817]	Loss 0.0083 (0.0500)	
training:	Epoch: [24][612/817]	Loss 0.0080 (0.0499)	
training:	Epoch: [24][613/817]	Loss 0.0097 (0.0498)	
training:	Epoch: [24][614/817]	Loss 0.0075 (0.0498)	
training:	Epoch: [24][615/817]	Loss 0.0076 (0.0497)	
training:	Epoch: [24][616/817]	Loss 0.0099 (0.0496)	
training:	Epoch: [24][617/817]	Loss 0.0087 (0.0496)	
training:	Epoch: [24][618/817]	Loss 0.0098 (0.0495)	
training:	Epoch: [24][619/817]	Loss 0.0078 (0.0494)	
training:	Epoch: [24][620/817]	Loss 0.0079 (0.0494)	
training:	Epoch: [24][621/817]	Loss 0.0084 (0.0493)	
training:	Epoch: [24][622/817]	Loss 0.0077 (0.0492)	
training:	Epoch: [24][623/817]	Loss 0.0067 (0.0492)	
training:	Epoch: [24][624/817]	Loss 0.0082 (0.0491)	
training:	Epoch: [24][625/817]	Loss 0.0073 (0.0490)	
training:	Epoch: [24][626/817]	Loss 0.0109 (0.0490)	
training:	Epoch: [24][627/817]	Loss 0.0074 (0.0489)	
training:	Epoch: [24][628/817]	Loss 0.0071 (0.0488)	
training:	Epoch: [24][629/817]	Loss 0.0077 (0.0488)	
training:	Epoch: [24][630/817]	Loss 0.0076 (0.0487)	
training:	Epoch: [24][631/817]	Loss 0.0074 (0.0486)	
training:	Epoch: [24][632/817]	Loss 0.0085 (0.0486)	
training:	Epoch: [24][633/817]	Loss 0.0081 (0.0485)	
training:	Epoch: [24][634/817]	Loss 0.0083 (0.0484)	
training:	Epoch: [24][635/817]	Loss 0.0090 (0.0484)	
training:	Epoch: [24][636/817]	Loss 0.0073 (0.0483)	
training:	Epoch: [24][637/817]	Loss 0.0397 (0.0483)	
training:	Epoch: [24][638/817]	Loss 0.0088 (0.0482)	
training:	Epoch: [24][639/817]	Loss 0.0075 (0.0482)	
training:	Epoch: [24][640/817]	Loss 0.0090 (0.0481)	
training:	Epoch: [24][641/817]	Loss 0.0096 (0.0481)	
training:	Epoch: [24][642/817]	Loss 0.0089 (0.0480)	
training:	Epoch: [24][643/817]	Loss 0.0081 (0.0479)	
training:	Epoch: [24][644/817]	Loss 0.0085 (0.0479)	
training:	Epoch: [24][645/817]	Loss 0.0081 (0.0478)	
training:	Epoch: [24][646/817]	Loss 0.0089 (0.0477)	
training:	Epoch: [24][647/817]	Loss 0.0075 (0.0477)	
training:	Epoch: [24][648/817]	Loss 0.0083 (0.0476)	
training:	Epoch: [24][649/817]	Loss 0.0078 (0.0476)	
training:	Epoch: [24][650/817]	Loss 0.0076 (0.0475)	
training:	Epoch: [24][651/817]	Loss 0.0080 (0.0474)	
training:	Epoch: [24][652/817]	Loss 0.0069 (0.0474)	
training:	Epoch: [24][653/817]	Loss 0.0116 (0.0473)	
training:	Epoch: [24][654/817]	Loss 0.0076 (0.0473)	
training:	Epoch: [24][655/817]	Loss 0.0110 (0.0472)	
training:	Epoch: [24][656/817]	Loss 0.0081 (0.0471)	
training:	Epoch: [24][657/817]	Loss 0.0080 (0.0471)	
training:	Epoch: [24][658/817]	Loss 0.0073 (0.0470)	
training:	Epoch: [24][659/817]	Loss 0.0236 (0.0470)	
training:	Epoch: [24][660/817]	Loss 0.0079 (0.0469)	
training:	Epoch: [24][661/817]	Loss 0.6229 (0.0478)	
training:	Epoch: [24][662/817]	Loss 0.0099 (0.0477)	
training:	Epoch: [24][663/817]	Loss 0.0075 (0.0477)	
training:	Epoch: [24][664/817]	Loss 0.0079 (0.0476)	
training:	Epoch: [24][665/817]	Loss 0.0067 (0.0476)	
training:	Epoch: [24][666/817]	Loss 0.0079 (0.0475)	
training:	Epoch: [24][667/817]	Loss 0.0071 (0.0474)	
training:	Epoch: [24][668/817]	Loss 0.0105 (0.0474)	
training:	Epoch: [24][669/817]	Loss 0.0075 (0.0473)	
training:	Epoch: [24][670/817]	Loss 0.0075 (0.0473)	
training:	Epoch: [24][671/817]	Loss 0.0161 (0.0472)	
training:	Epoch: [24][672/817]	Loss 0.0079 (0.0472)	
training:	Epoch: [24][673/817]	Loss 0.0073 (0.0471)	
training:	Epoch: [24][674/817]	Loss 0.0075 (0.0470)	
training:	Epoch: [24][675/817]	Loss 0.0068 (0.0470)	
training:	Epoch: [24][676/817]	Loss 0.0079 (0.0469)	
training:	Epoch: [24][677/817]	Loss 0.6042 (0.0478)	
training:	Epoch: [24][678/817]	Loss 0.0079 (0.0477)	
training:	Epoch: [24][679/817]	Loss 0.0081 (0.0476)	
training:	Epoch: [24][680/817]	Loss 0.0077 (0.0476)	
training:	Epoch: [24][681/817]	Loss 0.0080 (0.0475)	
training:	Epoch: [24][682/817]	Loss 0.0072 (0.0475)	
training:	Epoch: [24][683/817]	Loss 0.0065 (0.0474)	
training:	Epoch: [24][684/817]	Loss 0.0080 (0.0473)	
training:	Epoch: [24][685/817]	Loss 0.0074 (0.0473)	
training:	Epoch: [24][686/817]	Loss 0.0169 (0.0472)	
training:	Epoch: [24][687/817]	Loss 0.0111 (0.0472)	
training:	Epoch: [24][688/817]	Loss 0.0076 (0.0471)	
training:	Epoch: [24][689/817]	Loss 0.0070 (0.0471)	
training:	Epoch: [24][690/817]	Loss 0.0072 (0.0470)	
training:	Epoch: [24][691/817]	Loss 0.0078 (0.0470)	
training:	Epoch: [24][692/817]	Loss 0.0079 (0.0469)	
training:	Epoch: [24][693/817]	Loss 0.0076 (0.0468)	
training:	Epoch: [24][694/817]	Loss 0.0078 (0.0468)	
training:	Epoch: [24][695/817]	Loss 0.0078 (0.0467)	
training:	Epoch: [24][696/817]	Loss 0.0070 (0.0467)	
training:	Epoch: [24][697/817]	Loss 0.0113 (0.0466)	
training:	Epoch: [24][698/817]	Loss 0.0081 (0.0466)	
training:	Epoch: [24][699/817]	Loss 0.0200 (0.0465)	
training:	Epoch: [24][700/817]	Loss 0.0174 (0.0465)	
training:	Epoch: [24][701/817]	Loss 0.0073 (0.0464)	
training:	Epoch: [24][702/817]	Loss 0.0089 (0.0464)	
training:	Epoch: [24][703/817]	Loss 0.5183 (0.0470)	
training:	Epoch: [24][704/817]	Loss 0.0085 (0.0470)	
training:	Epoch: [24][705/817]	Loss 0.0070 (0.0469)	
training:	Epoch: [24][706/817]	Loss 0.0083 (0.0469)	
training:	Epoch: [24][707/817]	Loss 0.0082 (0.0468)	
training:	Epoch: [24][708/817]	Loss 0.0072 (0.0468)	
training:	Epoch: [24][709/817]	Loss 0.0081 (0.0467)	
training:	Epoch: [24][710/817]	Loss 0.0080 (0.0467)	
training:	Epoch: [24][711/817]	Loss 0.0080 (0.0466)	
training:	Epoch: [24][712/817]	Loss 0.0083 (0.0466)	
training:	Epoch: [24][713/817]	Loss 0.0097 (0.0465)	
training:	Epoch: [24][714/817]	Loss 0.0083 (0.0464)	
training:	Epoch: [24][715/817]	Loss 0.0074 (0.0464)	
training:	Epoch: [24][716/817]	Loss 0.0088 (0.0463)	
training:	Epoch: [24][717/817]	Loss 0.0075 (0.0463)	
training:	Epoch: [24][718/817]	Loss 0.0075 (0.0462)	
training:	Epoch: [24][719/817]	Loss 0.0078 (0.0462)	
training:	Epoch: [24][720/817]	Loss 0.0083 (0.0461)	
training:	Epoch: [24][721/817]	Loss 0.0075 (0.0461)	
training:	Epoch: [24][722/817]	Loss 0.0073 (0.0460)	
training:	Epoch: [24][723/817]	Loss 0.0920 (0.0461)	
training:	Epoch: [24][724/817]	Loss 0.0079 (0.0460)	
training:	Epoch: [24][725/817]	Loss 0.0085 (0.0460)	
training:	Epoch: [24][726/817]	Loss 0.6278 (0.0468)	
training:	Epoch: [24][727/817]	Loss 0.6144 (0.0476)	
training:	Epoch: [24][728/817]	Loss 0.0201 (0.0475)	
training:	Epoch: [24][729/817]	Loss 0.0090 (0.0475)	
training:	Epoch: [24][730/817]	Loss 0.5947 (0.0482)	
training:	Epoch: [24][731/817]	Loss 0.0080 (0.0482)	
training:	Epoch: [24][732/817]	Loss 0.0072 (0.0481)	
training:	Epoch: [24][733/817]	Loss 0.0081 (0.0481)	
training:	Epoch: [24][734/817]	Loss 0.0076 (0.0480)	
training:	Epoch: [24][735/817]	Loss 0.0081 (0.0479)	
training:	Epoch: [24][736/817]	Loss 0.0096 (0.0479)	
training:	Epoch: [24][737/817]	Loss 0.0076 (0.0478)	
training:	Epoch: [24][738/817]	Loss 0.0085 (0.0478)	
training:	Epoch: [24][739/817]	Loss 0.6335 (0.0486)	
training:	Epoch: [24][740/817]	Loss 0.0084 (0.0485)	
training:	Epoch: [24][741/817]	Loss 0.0080 (0.0485)	
training:	Epoch: [24][742/817]	Loss 0.0090 (0.0484)	
training:	Epoch: [24][743/817]	Loss 0.0069 (0.0484)	
training:	Epoch: [24][744/817]	Loss 0.0107 (0.0483)	
training:	Epoch: [24][745/817]	Loss 0.0073 (0.0483)	
training:	Epoch: [24][746/817]	Loss 0.0073 (0.0482)	
training:	Epoch: [24][747/817]	Loss 0.0084 (0.0481)	
training:	Epoch: [24][748/817]	Loss 0.0082 (0.0481)	
training:	Epoch: [24][749/817]	Loss 0.0077 (0.0480)	
training:	Epoch: [24][750/817]	Loss 0.0072 (0.0480)	
training:	Epoch: [24][751/817]	Loss 0.0080 (0.0479)	
training:	Epoch: [24][752/817]	Loss 0.0074 (0.0479)	
training:	Epoch: [24][753/817]	Loss 0.0072 (0.0478)	
training:	Epoch: [24][754/817]	Loss 0.0094 (0.0478)	
training:	Epoch: [24][755/817]	Loss 0.0075 (0.0477)	
training:	Epoch: [24][756/817]	Loss 0.0088 (0.0477)	
training:	Epoch: [24][757/817]	Loss 0.0067 (0.0476)	
training:	Epoch: [24][758/817]	Loss 0.0072 (0.0476)	
training:	Epoch: [24][759/817]	Loss 0.0085 (0.0475)	
training:	Epoch: [24][760/817]	Loss 0.0676 (0.0475)	
training:	Epoch: [24][761/817]	Loss 0.6317 (0.0483)	
training:	Epoch: [24][762/817]	Loss 0.0085 (0.0483)	
training:	Epoch: [24][763/817]	Loss 0.5801 (0.0489)	
training:	Epoch: [24][764/817]	Loss 0.0084 (0.0489)	
training:	Epoch: [24][765/817]	Loss 0.1363 (0.0490)	
training:	Epoch: [24][766/817]	Loss 0.0099 (0.0490)	
training:	Epoch: [24][767/817]	Loss 0.0086 (0.0489)	
training:	Epoch: [24][768/817]	Loss 0.0080 (0.0489)	
training:	Epoch: [24][769/817]	Loss 0.0084 (0.0488)	
training:	Epoch: [24][770/817]	Loss 0.0089 (0.0487)	
training:	Epoch: [24][771/817]	Loss 0.0078 (0.0487)	
training:	Epoch: [24][772/817]	Loss 0.0084 (0.0486)	
training:	Epoch: [24][773/817]	Loss 0.0482 (0.0486)	
training:	Epoch: [24][774/817]	Loss 0.0133 (0.0486)	
training:	Epoch: [24][775/817]	Loss 0.0086 (0.0485)	
training:	Epoch: [24][776/817]	Loss 0.0097 (0.0485)	
training:	Epoch: [24][777/817]	Loss 0.0072 (0.0484)	
training:	Epoch: [24][778/817]	Loss 0.0077 (0.0484)	
training:	Epoch: [24][779/817]	Loss 0.0522 (0.0484)	
training:	Epoch: [24][780/817]	Loss 0.0083 (0.0483)	
training:	Epoch: [24][781/817]	Loss 0.0091 (0.0483)	
training:	Epoch: [24][782/817]	Loss 0.5501 (0.0489)	
training:	Epoch: [24][783/817]	Loss 0.0084 (0.0489)	
training:	Epoch: [24][784/817]	Loss 0.0084 (0.0488)	
training:	Epoch: [24][785/817]	Loss 0.0072 (0.0488)	
training:	Epoch: [24][786/817]	Loss 0.0080 (0.0487)	
training:	Epoch: [24][787/817]	Loss 0.5311 (0.0493)	
training:	Epoch: [24][788/817]	Loss 0.0080 (0.0493)	
training:	Epoch: [24][789/817]	Loss 0.0102 (0.0492)	
training:	Epoch: [24][790/817]	Loss 0.0122 (0.0492)	
training:	Epoch: [24][791/817]	Loss 0.0076 (0.0491)	
training:	Epoch: [24][792/817]	Loss 0.0114 (0.0491)	
training:	Epoch: [24][793/817]	Loss 0.0082 (0.0490)	
training:	Epoch: [24][794/817]	Loss 0.0111 (0.0490)	
training:	Epoch: [24][795/817]	Loss 0.0083 (0.0489)	
training:	Epoch: [24][796/817]	Loss 0.0075 (0.0489)	
training:	Epoch: [24][797/817]	Loss 0.0080 (0.0488)	
training:	Epoch: [24][798/817]	Loss 0.0110 (0.0488)	
training:	Epoch: [24][799/817]	Loss 0.6077 (0.0495)	
training:	Epoch: [24][800/817]	Loss 0.0079 (0.0494)	
training:	Epoch: [24][801/817]	Loss 0.0084 (0.0494)	
training:	Epoch: [24][802/817]	Loss 0.0105 (0.0493)	
training:	Epoch: [24][803/817]	Loss 0.0079 (0.0493)	
training:	Epoch: [24][804/817]	Loss 0.0095 (0.0492)	
training:	Epoch: [24][805/817]	Loss 0.0092 (0.0492)	
training:	Epoch: [24][806/817]	Loss 0.0105 (0.0491)	
training:	Epoch: [24][807/817]	Loss 0.0076 (0.0491)	
training:	Epoch: [24][808/817]	Loss 0.0074 (0.0490)	
training:	Epoch: [24][809/817]	Loss 0.0089 (0.0490)	
training:	Epoch: [24][810/817]	Loss 0.0080 (0.0489)	
training:	Epoch: [24][811/817]	Loss 0.0097 (0.0489)	
training:	Epoch: [24][812/817]	Loss 0.6319 (0.0496)	
training:	Epoch: [24][813/817]	Loss 0.0069 (0.0496)	
training:	Epoch: [24][814/817]	Loss 0.0079 (0.0495)	
training:	Epoch: [24][815/817]	Loss 0.0080 (0.0494)	
training:	Epoch: [24][816/817]	Loss 0.0083 (0.0494)	
training:	Epoch: [24][817/817]	Loss 0.0110 (0.0494)	
Training:	 Loss: 0.0493

Training:	 ACC: 0.9930 0.9930 0.9932 0.9927
Validation:	 ACC: 0.7935 0.7945 0.8168 0.7702
Validation:	 Best_BACC: 0.8223 0.8218 0.8127 0.8318
Validation:	 Loss: 0.8672
Pretraining:	Epoch 25/200
----------
training:	Epoch: [25][1/817]	Loss 0.0080 (0.0080)	
training:	Epoch: [25][2/817]	Loss 0.0075 (0.0077)	
training:	Epoch: [25][3/817]	Loss 0.0088 (0.0081)	
training:	Epoch: [25][4/817]	Loss 0.0076 (0.0080)	
training:	Epoch: [25][5/817]	Loss 0.0079 (0.0079)	
training:	Epoch: [25][6/817]	Loss 0.0081 (0.0080)	
training:	Epoch: [25][7/817]	Loss 0.0089 (0.0081)	
training:	Epoch: [25][8/817]	Loss 0.0089 (0.0082)	
training:	Epoch: [25][9/817]	Loss 0.1592 (0.0250)	
training:	Epoch: [25][10/817]	Loss 0.0095 (0.0234)	
training:	Epoch: [25][11/817]	Loss 0.0087 (0.0221)	
training:	Epoch: [25][12/817]	Loss 0.0091 (0.0210)	
training:	Epoch: [25][13/817]	Loss 0.0073 (0.0200)	
training:	Epoch: [25][14/817]	Loss 0.0071 (0.0191)	
training:	Epoch: [25][15/817]	Loss 0.0099 (0.0184)	
training:	Epoch: [25][16/817]	Loss 0.0079 (0.0178)	
training:	Epoch: [25][17/817]	Loss 0.0069 (0.0171)	
training:	Epoch: [25][18/817]	Loss 0.6157 (0.0504)	
training:	Epoch: [25][19/817]	Loss 0.0084 (0.0482)	
training:	Epoch: [25][20/817]	Loss 0.6040 (0.0760)	
training:	Epoch: [25][21/817]	Loss 0.0081 (0.0727)	
training:	Epoch: [25][22/817]	Loss 0.0084 (0.0698)	
training:	Epoch: [25][23/817]	Loss 0.0152 (0.0674)	
training:	Epoch: [25][24/817]	Loss 0.0078 (0.0650)	
training:	Epoch: [25][25/817]	Loss 0.0077 (0.0627)	
training:	Epoch: [25][26/817]	Loss 0.0088 (0.0606)	
training:	Epoch: [25][27/817]	Loss 0.0101 (0.0587)	
training:	Epoch: [25][28/817]	Loss 0.0086 (0.0569)	
training:	Epoch: [25][29/817]	Loss 0.0084 (0.0553)	
training:	Epoch: [25][30/817]	Loss 0.0171 (0.0540)	
training:	Epoch: [25][31/817]	Loss 0.0085 (0.0525)	
training:	Epoch: [25][32/817]	Loss 0.0169 (0.0514)	
training:	Epoch: [25][33/817]	Loss 0.0082 (0.0501)	
training:	Epoch: [25][34/817]	Loss 0.0090 (0.0489)	
training:	Epoch: [25][35/817]	Loss 0.0081 (0.0477)	
training:	Epoch: [25][36/817]	Loss 0.0078 (0.0466)	
training:	Epoch: [25][37/817]	Loss 0.0076 (0.0456)	
training:	Epoch: [25][38/817]	Loss 0.0084 (0.0446)	
training:	Epoch: [25][39/817]	Loss 0.0070 (0.0436)	
training:	Epoch: [25][40/817]	Loss 0.0075 (0.0427)	
training:	Epoch: [25][41/817]	Loss 0.0081 (0.0419)	
training:	Epoch: [25][42/817]	Loss 0.0083 (0.0411)	
training:	Epoch: [25][43/817]	Loss 0.5875 (0.0538)	
training:	Epoch: [25][44/817]	Loss 0.0084 (0.0528)	
training:	Epoch: [25][45/817]	Loss 0.0086 (0.0518)	
training:	Epoch: [25][46/817]	Loss 0.0086 (0.0508)	
training:	Epoch: [25][47/817]	Loss 0.0086 (0.0499)	
training:	Epoch: [25][48/817]	Loss 0.0081 (0.0491)	
training:	Epoch: [25][49/817]	Loss 0.0093 (0.0483)	
training:	Epoch: [25][50/817]	Loss 0.0094 (0.0475)	
training:	Epoch: [25][51/817]	Loss 0.0111 (0.0468)	
training:	Epoch: [25][52/817]	Loss 0.0088 (0.0460)	
training:	Epoch: [25][53/817]	Loss 0.0081 (0.0453)	
training:	Epoch: [25][54/817]	Loss 0.0072 (0.0446)	
training:	Epoch: [25][55/817]	Loss 0.0105 (0.0440)	
training:	Epoch: [25][56/817]	Loss 0.0101 (0.0434)	
training:	Epoch: [25][57/817]	Loss 0.0076 (0.0428)	
training:	Epoch: [25][58/817]	Loss 0.0078 (0.0422)	
training:	Epoch: [25][59/817]	Loss 0.0080 (0.0416)	
training:	Epoch: [25][60/817]	Loss 0.0077 (0.0410)	
training:	Epoch: [25][61/817]	Loss 0.0081 (0.0405)	
training:	Epoch: [25][62/817]	Loss 0.0099 (0.0400)	
training:	Epoch: [25][63/817]	Loss 0.0082 (0.0395)	
training:	Epoch: [25][64/817]	Loss 0.0083 (0.0390)	
training:	Epoch: [25][65/817]	Loss 0.0086 (0.0385)	
training:	Epoch: [25][66/817]	Loss 0.0079 (0.0381)	
training:	Epoch: [25][67/817]	Loss 0.0074 (0.0376)	
training:	Epoch: [25][68/817]	Loss 0.6218 (0.0462)	
training:	Epoch: [25][69/817]	Loss 0.0078 (0.0456)	
training:	Epoch: [25][70/817]	Loss 0.0075 (0.0451)	
training:	Epoch: [25][71/817]	Loss 0.0080 (0.0446)	
training:	Epoch: [25][72/817]	Loss 0.0075 (0.0441)	
training:	Epoch: [25][73/817]	Loss 0.0076 (0.0436)	
training:	Epoch: [25][74/817]	Loss 0.0082 (0.0431)	
training:	Epoch: [25][75/817]	Loss 0.0094 (0.0426)	
training:	Epoch: [25][76/817]	Loss 0.0072 (0.0422)	
training:	Epoch: [25][77/817]	Loss 0.0087 (0.0417)	
training:	Epoch: [25][78/817]	Loss 0.0083 (0.0413)	
training:	Epoch: [25][79/817]	Loss 0.5712 (0.0480)	
training:	Epoch: [25][80/817]	Loss 0.0086 (0.0475)	
training:	Epoch: [25][81/817]	Loss 0.0079 (0.0470)	
training:	Epoch: [25][82/817]	Loss 0.0083 (0.0465)	
training:	Epoch: [25][83/817]	Loss 0.0082 (0.0461)	
training:	Epoch: [25][84/817]	Loss 0.0087 (0.0456)	
training:	Epoch: [25][85/817]	Loss 0.0455 (0.0456)	
training:	Epoch: [25][86/817]	Loss 0.5928 (0.0520)	
training:	Epoch: [25][87/817]	Loss 0.0111 (0.0515)	
training:	Epoch: [25][88/817]	Loss 0.0082 (0.0510)	
training:	Epoch: [25][89/817]	Loss 0.0083 (0.0506)	
training:	Epoch: [25][90/817]	Loss 0.0077 (0.0501)	
training:	Epoch: [25][91/817]	Loss 0.0214 (0.0498)	
training:	Epoch: [25][92/817]	Loss 0.0079 (0.0493)	
training:	Epoch: [25][93/817]	Loss 0.0086 (0.0489)	
training:	Epoch: [25][94/817]	Loss 0.0083 (0.0484)	
training:	Epoch: [25][95/817]	Loss 0.0096 (0.0480)	
training:	Epoch: [25][96/817]	Loss 0.0091 (0.0476)	
training:	Epoch: [25][97/817]	Loss 0.0079 (0.0472)	
training:	Epoch: [25][98/817]	Loss 0.0252 (0.0470)	
training:	Epoch: [25][99/817]	Loss 0.0091 (0.0466)	
training:	Epoch: [25][100/817]	Loss 0.0086 (0.0462)	
training:	Epoch: [25][101/817]	Loss 0.0081 (0.0459)	
training:	Epoch: [25][102/817]	Loss 0.0086 (0.0455)	
training:	Epoch: [25][103/817]	Loss 0.0077 (0.0451)	
training:	Epoch: [25][104/817]	Loss 0.0090 (0.0448)	
training:	Epoch: [25][105/817]	Loss 0.0081 (0.0444)	
training:	Epoch: [25][106/817]	Loss 0.0091 (0.0441)	
training:	Epoch: [25][107/817]	Loss 0.0081 (0.0438)	
training:	Epoch: [25][108/817]	Loss 0.0112 (0.0435)	
training:	Epoch: [25][109/817]	Loss 0.0080 (0.0431)	
training:	Epoch: [25][110/817]	Loss 0.0087 (0.0428)	
training:	Epoch: [25][111/817]	Loss 0.0077 (0.0425)	
training:	Epoch: [25][112/817]	Loss 0.0136 (0.0422)	
training:	Epoch: [25][113/817]	Loss 0.0070 (0.0419)	
training:	Epoch: [25][114/817]	Loss 0.0081 (0.0416)	
training:	Epoch: [25][115/817]	Loss 0.0092 (0.0414)	
training:	Epoch: [25][116/817]	Loss 0.0077 (0.0411)	
training:	Epoch: [25][117/817]	Loss 0.0087 (0.0408)	
training:	Epoch: [25][118/817]	Loss 0.0080 (0.0405)	
training:	Epoch: [25][119/817]	Loss 0.0108 (0.0403)	
training:	Epoch: [25][120/817]	Loss 0.0094 (0.0400)	
training:	Epoch: [25][121/817]	Loss 0.0078 (0.0397)	
training:	Epoch: [25][122/817]	Loss 0.0085 (0.0395)	
training:	Epoch: [25][123/817]	Loss 0.0131 (0.0393)	
training:	Epoch: [25][124/817]	Loss 0.0084 (0.0390)	
training:	Epoch: [25][125/817]	Loss 0.0085 (0.0388)	
training:	Epoch: [25][126/817]	Loss 0.0083 (0.0385)	
training:	Epoch: [25][127/817]	Loss 0.0073 (0.0383)	
training:	Epoch: [25][128/817]	Loss 0.0079 (0.0380)	
training:	Epoch: [25][129/817]	Loss 0.0081 (0.0378)	
training:	Epoch: [25][130/817]	Loss 0.0081 (0.0376)	
training:	Epoch: [25][131/817]	Loss 0.0071 (0.0374)	
training:	Epoch: [25][132/817]	Loss 0.0330 (0.0373)	
training:	Epoch: [25][133/817]	Loss 0.0081 (0.0371)	
training:	Epoch: [25][134/817]	Loss 0.0083 (0.0369)	
training:	Epoch: [25][135/817]	Loss 0.0075 (0.0367)	
training:	Epoch: [25][136/817]	Loss 0.0075 (0.0365)	
training:	Epoch: [25][137/817]	Loss 0.0080 (0.0362)	
training:	Epoch: [25][138/817]	Loss 0.6156 (0.0404)	
training:	Epoch: [25][139/817]	Loss 0.0075 (0.0402)	
training:	Epoch: [25][140/817]	Loss 0.0079 (0.0400)	
training:	Epoch: [25][141/817]	Loss 0.0081 (0.0397)	
training:	Epoch: [25][142/817]	Loss 0.5886 (0.0436)	
training:	Epoch: [25][143/817]	Loss 0.0079 (0.0434)	
training:	Epoch: [25][144/817]	Loss 0.0087 (0.0431)	
training:	Epoch: [25][145/817]	Loss 0.0087 (0.0429)	
training:	Epoch: [25][146/817]	Loss 0.0081 (0.0426)	
training:	Epoch: [25][147/817]	Loss 0.0080 (0.0424)	
training:	Epoch: [25][148/817]	Loss 0.0087 (0.0422)	
training:	Epoch: [25][149/817]	Loss 0.0072 (0.0419)	
training:	Epoch: [25][150/817]	Loss 0.0087 (0.0417)	
training:	Epoch: [25][151/817]	Loss 0.0080 (0.0415)	
training:	Epoch: [25][152/817]	Loss 0.0083 (0.0413)	
training:	Epoch: [25][153/817]	Loss 0.0135 (0.0411)	
training:	Epoch: [25][154/817]	Loss 0.0078 (0.0409)	
training:	Epoch: [25][155/817]	Loss 0.0076 (0.0407)	
training:	Epoch: [25][156/817]	Loss 0.0096 (0.0405)	
training:	Epoch: [25][157/817]	Loss 0.0086 (0.0403)	
training:	Epoch: [25][158/817]	Loss 0.0088 (0.0401)	
training:	Epoch: [25][159/817]	Loss 0.0093 (0.0399)	
training:	Epoch: [25][160/817]	Loss 0.0088 (0.0397)	
training:	Epoch: [25][161/817]	Loss 0.0076 (0.0395)	
training:	Epoch: [25][162/817]	Loss 0.0084 (0.0393)	
training:	Epoch: [25][163/817]	Loss 0.0088 (0.0391)	
training:	Epoch: [25][164/817]	Loss 0.0083 (0.0389)	
training:	Epoch: [25][165/817]	Loss 0.0067 (0.0387)	
training:	Epoch: [25][166/817]	Loss 0.0076 (0.0385)	
training:	Epoch: [25][167/817]	Loss 0.0077 (0.0384)	
training:	Epoch: [25][168/817]	Loss 0.0077 (0.0382)	
training:	Epoch: [25][169/817]	Loss 0.0153 (0.0380)	
training:	Epoch: [25][170/817]	Loss 0.5888 (0.0413)	
training:	Epoch: [25][171/817]	Loss 0.5865 (0.0445)	
training:	Epoch: [25][172/817]	Loss 0.0078 (0.0442)	
training:	Epoch: [25][173/817]	Loss 0.0089 (0.0440)	
training:	Epoch: [25][174/817]	Loss 0.0071 (0.0438)	
training:	Epoch: [25][175/817]	Loss 0.0076 (0.0436)	
training:	Epoch: [25][176/817]	Loss 0.0076 (0.0434)	
training:	Epoch: [25][177/817]	Loss 0.0121 (0.0432)	
training:	Epoch: [25][178/817]	Loss 0.0083 (0.0430)	
training:	Epoch: [25][179/817]	Loss 0.0084 (0.0429)	
training:	Epoch: [25][180/817]	Loss 0.0085 (0.0427)	
training:	Epoch: [25][181/817]	Loss 0.0088 (0.0425)	
training:	Epoch: [25][182/817]	Loss 0.0080 (0.0423)	
training:	Epoch: [25][183/817]	Loss 0.0082 (0.0421)	
training:	Epoch: [25][184/817]	Loss 0.0074 (0.0419)	
training:	Epoch: [25][185/817]	Loss 0.0077 (0.0417)	
training:	Epoch: [25][186/817]	Loss 0.0077 (0.0415)	
training:	Epoch: [25][187/817]	Loss 0.0087 (0.0414)	
training:	Epoch: [25][188/817]	Loss 0.0075 (0.0412)	
training:	Epoch: [25][189/817]	Loss 0.0078 (0.0410)	
training:	Epoch: [25][190/817]	Loss 0.0096 (0.0408)	
training:	Epoch: [25][191/817]	Loss 0.0084 (0.0407)	
training:	Epoch: [25][192/817]	Loss 0.0084 (0.0405)	
training:	Epoch: [25][193/817]	Loss 0.0110 (0.0404)	
training:	Epoch: [25][194/817]	Loss 0.0086 (0.0402)	
training:	Epoch: [25][195/817]	Loss 0.0089 (0.0400)	
training:	Epoch: [25][196/817]	Loss 0.0079 (0.0399)	
training:	Epoch: [25][197/817]	Loss 0.0080 (0.0397)	
training:	Epoch: [25][198/817]	Loss 0.0110 (0.0396)	
training:	Epoch: [25][199/817]	Loss 0.0079 (0.0394)	
training:	Epoch: [25][200/817]	Loss 0.0083 (0.0392)	
training:	Epoch: [25][201/817]	Loss 0.0076 (0.0391)	
training:	Epoch: [25][202/817]	Loss 0.0081 (0.0389)	
training:	Epoch: [25][203/817]	Loss 0.0085 (0.0388)	
training:	Epoch: [25][204/817]	Loss 0.0076 (0.0386)	
training:	Epoch: [25][205/817]	Loss 0.0086 (0.0385)	
training:	Epoch: [25][206/817]	Loss 0.0075 (0.0383)	
training:	Epoch: [25][207/817]	Loss 0.0081 (0.0382)	
training:	Epoch: [25][208/817]	Loss 0.0076 (0.0380)	
training:	Epoch: [25][209/817]	Loss 0.0089 (0.0379)	
training:	Epoch: [25][210/817]	Loss 0.0086 (0.0378)	
training:	Epoch: [25][211/817]	Loss 0.0088 (0.0376)	
training:	Epoch: [25][212/817]	Loss 0.0078 (0.0375)	
training:	Epoch: [25][213/817]	Loss 0.0074 (0.0373)	
training:	Epoch: [25][214/817]	Loss 0.0076 (0.0372)	
training:	Epoch: [25][215/817]	Loss 0.0071 (0.0371)	
training:	Epoch: [25][216/817]	Loss 0.6157 (0.0397)	
training:	Epoch: [25][217/817]	Loss 0.0080 (0.0396)	
training:	Epoch: [25][218/817]	Loss 0.0082 (0.0395)	
training:	Epoch: [25][219/817]	Loss 0.0070 (0.0393)	
training:	Epoch: [25][220/817]	Loss 0.0075 (0.0392)	
training:	Epoch: [25][221/817]	Loss 0.0081 (0.0390)	
training:	Epoch: [25][222/817]	Loss 0.0074 (0.0389)	
training:	Epoch: [25][223/817]	Loss 0.0092 (0.0387)	
training:	Epoch: [25][224/817]	Loss 0.0072 (0.0386)	
training:	Epoch: [25][225/817]	Loss 0.0088 (0.0385)	
training:	Epoch: [25][226/817]	Loss 0.0100 (0.0383)	
training:	Epoch: [25][227/817]	Loss 0.0082 (0.0382)	
training:	Epoch: [25][228/817]	Loss 0.0076 (0.0381)	
training:	Epoch: [25][229/817]	Loss 0.0080 (0.0379)	
training:	Epoch: [25][230/817]	Loss 0.0079 (0.0378)	
training:	Epoch: [25][231/817]	Loss 0.0077 (0.0377)	
training:	Epoch: [25][232/817]	Loss 0.0083 (0.0376)	
training:	Epoch: [25][233/817]	Loss 0.0077 (0.0374)	
training:	Epoch: [25][234/817]	Loss 0.0071 (0.0373)	
training:	Epoch: [25][235/817]	Loss 0.6035 (0.0397)	
training:	Epoch: [25][236/817]	Loss 0.0071 (0.0396)	
training:	Epoch: [25][237/817]	Loss 0.0079 (0.0394)	
training:	Epoch: [25][238/817]	Loss 0.0086 (0.0393)	
training:	Epoch: [25][239/817]	Loss 0.0079 (0.0392)	
training:	Epoch: [25][240/817]	Loss 0.0086 (0.0390)	
training:	Epoch: [25][241/817]	Loss 0.0072 (0.0389)	
training:	Epoch: [25][242/817]	Loss 0.0072 (0.0388)	
training:	Epoch: [25][243/817]	Loss 0.0084 (0.0387)	
training:	Epoch: [25][244/817]	Loss 0.0076 (0.0385)	
training:	Epoch: [25][245/817]	Loss 0.0077 (0.0384)	
training:	Epoch: [25][246/817]	Loss 0.0076 (0.0383)	
training:	Epoch: [25][247/817]	Loss 0.0076 (0.0382)	
training:	Epoch: [25][248/817]	Loss 0.0075 (0.0380)	
training:	Epoch: [25][249/817]	Loss 0.0098 (0.0379)	
training:	Epoch: [25][250/817]	Loss 0.0081 (0.0378)	
training:	Epoch: [25][251/817]	Loss 0.0082 (0.0377)	
training:	Epoch: [25][252/817]	Loss 0.0100 (0.0376)	
training:	Epoch: [25][253/817]	Loss 0.0076 (0.0375)	
training:	Epoch: [25][254/817]	Loss 0.0098 (0.0373)	
training:	Epoch: [25][255/817]	Loss 0.6208 (0.0396)	
training:	Epoch: [25][256/817]	Loss 0.0083 (0.0395)	
training:	Epoch: [25][257/817]	Loss 0.0083 (0.0394)	
training:	Epoch: [25][258/817]	Loss 0.0079 (0.0393)	
training:	Epoch: [25][259/817]	Loss 0.0078 (0.0391)	
training:	Epoch: [25][260/817]	Loss 0.0086 (0.0390)	
training:	Epoch: [25][261/817]	Loss 0.0071 (0.0389)	
training:	Epoch: [25][262/817]	Loss 0.0080 (0.0388)	
training:	Epoch: [25][263/817]	Loss 0.0080 (0.0387)	
training:	Epoch: [25][264/817]	Loss 0.0073 (0.0386)	
training:	Epoch: [25][265/817]	Loss 0.0085 (0.0384)	
training:	Epoch: [25][266/817]	Loss 0.0078 (0.0383)	
training:	Epoch: [25][267/817]	Loss 0.0081 (0.0382)	
training:	Epoch: [25][268/817]	Loss 0.0080 (0.0381)	
training:	Epoch: [25][269/817]	Loss 0.0076 (0.0380)	
training:	Epoch: [25][270/817]	Loss 0.0071 (0.0379)	
training:	Epoch: [25][271/817]	Loss 0.0083 (0.0378)	
training:	Epoch: [25][272/817]	Loss 0.0073 (0.0377)	
training:	Epoch: [25][273/817]	Loss 0.0069 (0.0375)	
training:	Epoch: [25][274/817]	Loss 0.0079 (0.0374)	
training:	Epoch: [25][275/817]	Loss 0.0079 (0.0373)	
training:	Epoch: [25][276/817]	Loss 0.0080 (0.0372)	
training:	Epoch: [25][277/817]	Loss 0.0068 (0.0371)	
training:	Epoch: [25][278/817]	Loss 0.0085 (0.0370)	
training:	Epoch: [25][279/817]	Loss 0.0083 (0.0369)	
training:	Epoch: [25][280/817]	Loss 0.0078 (0.0368)	
training:	Epoch: [25][281/817]	Loss 0.0076 (0.0367)	
training:	Epoch: [25][282/817]	Loss 0.0086 (0.0366)	
training:	Epoch: [25][283/817]	Loss 0.0077 (0.0365)	
training:	Epoch: [25][284/817]	Loss 0.0068 (0.0364)	
training:	Epoch: [25][285/817]	Loss 0.0083 (0.0363)	
training:	Epoch: [25][286/817]	Loss 0.0084 (0.0362)	
training:	Epoch: [25][287/817]	Loss 0.0084 (0.0361)	
training:	Epoch: [25][288/817]	Loss 0.0071 (0.0360)	
training:	Epoch: [25][289/817]	Loss 0.0082 (0.0359)	
training:	Epoch: [25][290/817]	Loss 0.0074 (0.0358)	
training:	Epoch: [25][291/817]	Loss 0.0074 (0.0357)	
training:	Epoch: [25][292/817]	Loss 0.4959 (0.0373)	
training:	Epoch: [25][293/817]	Loss 0.0073 (0.0372)	
training:	Epoch: [25][294/817]	Loss 0.0074 (0.0371)	
training:	Epoch: [25][295/817]	Loss 0.0070 (0.0370)	
training:	Epoch: [25][296/817]	Loss 0.0078 (0.0369)	
training:	Epoch: [25][297/817]	Loss 0.0090 (0.0368)	
training:	Epoch: [25][298/817]	Loss 0.0074 (0.0367)	
training:	Epoch: [25][299/817]	Loss 0.0076 (0.0366)	
training:	Epoch: [25][300/817]	Loss 0.0172 (0.0365)	
training:	Epoch: [25][301/817]	Loss 0.0075 (0.0364)	
training:	Epoch: [25][302/817]	Loss 0.0075 (0.0363)	
training:	Epoch: [25][303/817]	Loss 0.0075 (0.0362)	
training:	Epoch: [25][304/817]	Loss 0.0074 (0.0361)	
training:	Epoch: [25][305/817]	Loss 0.0078 (0.0360)	
training:	Epoch: [25][306/817]	Loss 0.0071 (0.0359)	
training:	Epoch: [25][307/817]	Loss 0.0077 (0.0359)	
training:	Epoch: [25][308/817]	Loss 0.0080 (0.0358)	
training:	Epoch: [25][309/817]	Loss 0.0073 (0.0357)	
training:	Epoch: [25][310/817]	Loss 0.0081 (0.0356)	
training:	Epoch: [25][311/817]	Loss 0.0121 (0.0355)	
training:	Epoch: [25][312/817]	Loss 0.0076 (0.0354)	
training:	Epoch: [25][313/817]	Loss 0.0072 (0.0353)	
training:	Epoch: [25][314/817]	Loss 0.0081 (0.0352)	
training:	Epoch: [25][315/817]	Loss 0.0084 (0.0352)	
training:	Epoch: [25][316/817]	Loss 0.0073 (0.0351)	
training:	Epoch: [25][317/817]	Loss 0.0071 (0.0350)	
training:	Epoch: [25][318/817]	Loss 0.0069 (0.0349)	
training:	Epoch: [25][319/817]	Loss 0.0071 (0.0348)	
training:	Epoch: [25][320/817]	Loss 0.0075 (0.0347)	
training:	Epoch: [25][321/817]	Loss 0.0282 (0.0347)	
training:	Epoch: [25][322/817]	Loss 0.0079 (0.0346)	
training:	Epoch: [25][323/817]	Loss 0.0073 (0.0345)	
training:	Epoch: [25][324/817]	Loss 0.0072 (0.0344)	
training:	Epoch: [25][325/817]	Loss 0.0080 (0.0344)	
training:	Epoch: [25][326/817]	Loss 0.6366 (0.0362)	
training:	Epoch: [25][327/817]	Loss 0.0082 (0.0361)	
training:	Epoch: [25][328/817]	Loss 0.0078 (0.0360)	
training:	Epoch: [25][329/817]	Loss 0.0082 (0.0360)	
training:	Epoch: [25][330/817]	Loss 0.0071 (0.0359)	
training:	Epoch: [25][331/817]	Loss 0.0077 (0.0358)	
training:	Epoch: [25][332/817]	Loss 0.0076 (0.0357)	
training:	Epoch: [25][333/817]	Loss 0.0080 (0.0356)	
training:	Epoch: [25][334/817]	Loss 0.0083 (0.0355)	
training:	Epoch: [25][335/817]	Loss 0.0068 (0.0354)	
training:	Epoch: [25][336/817]	Loss 0.0074 (0.0354)	
training:	Epoch: [25][337/817]	Loss 0.0094 (0.0353)	
training:	Epoch: [25][338/817]	Loss 0.0069 (0.0352)	
training:	Epoch: [25][339/817]	Loss 0.0078 (0.0351)	
training:	Epoch: [25][340/817]	Loss 0.0070 (0.0350)	
training:	Epoch: [25][341/817]	Loss 0.0066 (0.0350)	
training:	Epoch: [25][342/817]	Loss 0.0068 (0.0349)	
training:	Epoch: [25][343/817]	Loss 0.0073 (0.0348)	
training:	Epoch: [25][344/817]	Loss 0.0069 (0.0347)	
training:	Epoch: [25][345/817]	Loss 0.0160 (0.0347)	
training:	Epoch: [25][346/817]	Loss 0.0083 (0.0346)	
training:	Epoch: [25][347/817]	Loss 0.0072 (0.0345)	
training:	Epoch: [25][348/817]	Loss 0.0072 (0.0344)	
training:	Epoch: [25][349/817]	Loss 0.0071 (0.0343)	
training:	Epoch: [25][350/817]	Loss 0.0076 (0.0343)	
training:	Epoch: [25][351/817]	Loss 0.0069 (0.0342)	
training:	Epoch: [25][352/817]	Loss 0.0091 (0.0341)	
training:	Epoch: [25][353/817]	Loss 0.0070 (0.0340)	
training:	Epoch: [25][354/817]	Loss 0.0079 (0.0340)	
training:	Epoch: [25][355/817]	Loss 0.0071 (0.0339)	
training:	Epoch: [25][356/817]	Loss 0.0080 (0.0338)	
training:	Epoch: [25][357/817]	Loss 0.0079 (0.0338)	
training:	Epoch: [25][358/817]	Loss 0.0072 (0.0337)	
training:	Epoch: [25][359/817]	Loss 0.0071 (0.0336)	
training:	Epoch: [25][360/817]	Loss 0.0069 (0.0335)	
training:	Epoch: [25][361/817]	Loss 0.0069 (0.0335)	
training:	Epoch: [25][362/817]	Loss 0.0079 (0.0334)	
training:	Epoch: [25][363/817]	Loss 0.0071 (0.0333)	
training:	Epoch: [25][364/817]	Loss 0.0075 (0.0332)	
training:	Epoch: [25][365/817]	Loss 0.0072 (0.0332)	
training:	Epoch: [25][366/817]	Loss 0.0070 (0.0331)	
training:	Epoch: [25][367/817]	Loss 0.0093 (0.0330)	
training:	Epoch: [25][368/817]	Loss 0.0075 (0.0330)	
training:	Epoch: [25][369/817]	Loss 0.0071 (0.0329)	
training:	Epoch: [25][370/817]	Loss 0.0071 (0.0328)	
training:	Epoch: [25][371/817]	Loss 0.6160 (0.0344)	
training:	Epoch: [25][372/817]	Loss 0.0071 (0.0343)	
training:	Epoch: [25][373/817]	Loss 0.0079 (0.0343)	
training:	Epoch: [25][374/817]	Loss 0.0070 (0.0342)	
training:	Epoch: [25][375/817]	Loss 0.0069 (0.0341)	
training:	Epoch: [25][376/817]	Loss 0.0074 (0.0340)	
training:	Epoch: [25][377/817]	Loss 0.0073 (0.0340)	
training:	Epoch: [25][378/817]	Loss 0.0072 (0.0339)	
training:	Epoch: [25][379/817]	Loss 0.0071 (0.0338)	
training:	Epoch: [25][380/817]	Loss 0.0092 (0.0338)	
training:	Epoch: [25][381/817]	Loss 0.0076 (0.0337)	
training:	Epoch: [25][382/817]	Loss 0.0063 (0.0336)	
training:	Epoch: [25][383/817]	Loss 0.0078 (0.0336)	
training:	Epoch: [25][384/817]	Loss 0.0069 (0.0335)	
training:	Epoch: [25][385/817]	Loss 0.0069 (0.0334)	
training:	Epoch: [25][386/817]	Loss 0.0078 (0.0333)	
training:	Epoch: [25][387/817]	Loss 0.0073 (0.0333)	
training:	Epoch: [25][388/817]	Loss 0.0077 (0.0332)	
training:	Epoch: [25][389/817]	Loss 0.0073 (0.0331)	
training:	Epoch: [25][390/817]	Loss 0.0073 (0.0331)	
training:	Epoch: [25][391/817]	Loss 0.0068 (0.0330)	
training:	Epoch: [25][392/817]	Loss 0.0071 (0.0329)	
training:	Epoch: [25][393/817]	Loss 0.0075 (0.0329)	
training:	Epoch: [25][394/817]	Loss 0.0074 (0.0328)	
training:	Epoch: [25][395/817]	Loss 0.0072 (0.0328)	
training:	Epoch: [25][396/817]	Loss 0.0078 (0.0327)	
training:	Epoch: [25][397/817]	Loss 0.0071 (0.0326)	
training:	Epoch: [25][398/817]	Loss 0.0075 (0.0326)	
training:	Epoch: [25][399/817]	Loss 0.0067 (0.0325)	
training:	Epoch: [25][400/817]	Loss 0.0082 (0.0324)	
training:	Epoch: [25][401/817]	Loss 0.0080 (0.0324)	
training:	Epoch: [25][402/817]	Loss 0.0080 (0.0323)	
training:	Epoch: [25][403/817]	Loss 0.0080 (0.0323)	
training:	Epoch: [25][404/817]	Loss 0.0068 (0.0322)	
training:	Epoch: [25][405/817]	Loss 0.0072 (0.0321)	
training:	Epoch: [25][406/817]	Loss 0.0070 (0.0321)	
training:	Epoch: [25][407/817]	Loss 0.5985 (0.0335)	
training:	Epoch: [25][408/817]	Loss 0.0073 (0.0334)	
training:	Epoch: [25][409/817]	Loss 0.0073 (0.0333)	
training:	Epoch: [25][410/817]	Loss 0.0082 (0.0333)	
training:	Epoch: [25][411/817]	Loss 0.0071 (0.0332)	
training:	Epoch: [25][412/817]	Loss 0.0072 (0.0331)	
training:	Epoch: [25][413/817]	Loss 0.5882 (0.0345)	
training:	Epoch: [25][414/817]	Loss 0.0069 (0.0344)	
training:	Epoch: [25][415/817]	Loss 0.0067 (0.0344)	
training:	Epoch: [25][416/817]	Loss 0.0067 (0.0343)	
training:	Epoch: [25][417/817]	Loss 0.0072 (0.0342)	
training:	Epoch: [25][418/817]	Loss 0.0074 (0.0342)	
training:	Epoch: [25][419/817]	Loss 0.0084 (0.0341)	
training:	Epoch: [25][420/817]	Loss 0.0066 (0.0340)	
training:	Epoch: [25][421/817]	Loss 0.6300 (0.0354)	
training:	Epoch: [25][422/817]	Loss 0.0081 (0.0354)	
training:	Epoch: [25][423/817]	Loss 0.0083 (0.0353)	
training:	Epoch: [25][424/817]	Loss 0.0070 (0.0353)	
training:	Epoch: [25][425/817]	Loss 0.0075 (0.0352)	
training:	Epoch: [25][426/817]	Loss 0.0072 (0.0351)	
training:	Epoch: [25][427/817]	Loss 0.0077 (0.0351)	
training:	Epoch: [25][428/817]	Loss 0.0076 (0.0350)	
training:	Epoch: [25][429/817]	Loss 0.0070 (0.0349)	
training:	Epoch: [25][430/817]	Loss 0.0073 (0.0349)	
training:	Epoch: [25][431/817]	Loss 0.0086 (0.0348)	
training:	Epoch: [25][432/817]	Loss 0.0073 (0.0347)	
training:	Epoch: [25][433/817]	Loss 0.0071 (0.0347)	
training:	Epoch: [25][434/817]	Loss 0.0077 (0.0346)	
training:	Epoch: [25][435/817]	Loss 0.0082 (0.0345)	
training:	Epoch: [25][436/817]	Loss 0.0077 (0.0345)	
training:	Epoch: [25][437/817]	Loss 0.0070 (0.0344)	
training:	Epoch: [25][438/817]	Loss 0.0079 (0.0344)	
training:	Epoch: [25][439/817]	Loss 0.0063 (0.0343)	
training:	Epoch: [25][440/817]	Loss 0.0070 (0.0342)	
training:	Epoch: [25][441/817]	Loss 0.0072 (0.0342)	
training:	Epoch: [25][442/817]	Loss 0.6097 (0.0355)	
training:	Epoch: [25][443/817]	Loss 0.0081 (0.0354)	
training:	Epoch: [25][444/817]	Loss 0.0080 (0.0354)	
training:	Epoch: [25][445/817]	Loss 0.0077 (0.0353)	
training:	Epoch: [25][446/817]	Loss 0.0084 (0.0352)	
training:	Epoch: [25][447/817]	Loss 0.0077 (0.0352)	
training:	Epoch: [25][448/817]	Loss 0.0063 (0.0351)	
training:	Epoch: [25][449/817]	Loss 0.0071 (0.0350)	
training:	Epoch: [25][450/817]	Loss 0.0067 (0.0350)	
training:	Epoch: [25][451/817]	Loss 0.0084 (0.0349)	
training:	Epoch: [25][452/817]	Loss 0.0083 (0.0349)	
training:	Epoch: [25][453/817]	Loss 0.0081 (0.0348)	
training:	Epoch: [25][454/817]	Loss 0.0083 (0.0347)	
training:	Epoch: [25][455/817]	Loss 0.0076 (0.0347)	
training:	Epoch: [25][456/817]	Loss 0.0085 (0.0346)	
training:	Epoch: [25][457/817]	Loss 0.0070 (0.0346)	
training:	Epoch: [25][458/817]	Loss 0.0071 (0.0345)	
training:	Epoch: [25][459/817]	Loss 0.0080 (0.0345)	
training:	Epoch: [25][460/817]	Loss 0.0071 (0.0344)	
training:	Epoch: [25][461/817]	Loss 0.0085 (0.0343)	
training:	Epoch: [25][462/817]	Loss 0.0081 (0.0343)	
training:	Epoch: [25][463/817]	Loss 0.0073 (0.0342)	
training:	Epoch: [25][464/817]	Loss 0.0071 (0.0342)	
training:	Epoch: [25][465/817]	Loss 0.0066 (0.0341)	
training:	Epoch: [25][466/817]	Loss 0.0070 (0.0340)	
training:	Epoch: [25][467/817]	Loss 0.0076 (0.0340)	
training:	Epoch: [25][468/817]	Loss 0.0076 (0.0339)	
training:	Epoch: [25][469/817]	Loss 0.0074 (0.0339)	
training:	Epoch: [25][470/817]	Loss 0.0072 (0.0338)	
training:	Epoch: [25][471/817]	Loss 0.0068 (0.0338)	
training:	Epoch: [25][472/817]	Loss 0.5769 (0.0349)	
training:	Epoch: [25][473/817]	Loss 0.0074 (0.0349)	
training:	Epoch: [25][474/817]	Loss 0.0066 (0.0348)	
training:	Epoch: [25][475/817]	Loss 0.0077 (0.0347)	
training:	Epoch: [25][476/817]	Loss 0.0074 (0.0347)	
training:	Epoch: [25][477/817]	Loss 0.5963 (0.0359)	
training:	Epoch: [25][478/817]	Loss 0.0071 (0.0358)	
training:	Epoch: [25][479/817]	Loss 0.0078 (0.0357)	
training:	Epoch: [25][480/817]	Loss 0.0069 (0.0357)	
training:	Epoch: [25][481/817]	Loss 0.0087 (0.0356)	
training:	Epoch: [25][482/817]	Loss 0.0070 (0.0356)	
training:	Epoch: [25][483/817]	Loss 0.0067 (0.0355)	
training:	Epoch: [25][484/817]	Loss 0.0083 (0.0354)	
training:	Epoch: [25][485/817]	Loss 0.0292 (0.0354)	
training:	Epoch: [25][486/817]	Loss 0.0100 (0.0354)	
training:	Epoch: [25][487/817]	Loss 0.0074 (0.0353)	
training:	Epoch: [25][488/817]	Loss 0.6008 (0.0365)	
training:	Epoch: [25][489/817]	Loss 0.0083 (0.0364)	
training:	Epoch: [25][490/817]	Loss 0.0085 (0.0364)	
training:	Epoch: [25][491/817]	Loss 0.0086 (0.0363)	
training:	Epoch: [25][492/817]	Loss 0.6302 (0.0375)	
training:	Epoch: [25][493/817]	Loss 0.0076 (0.0375)	
training:	Epoch: [25][494/817]	Loss 0.0076 (0.0374)	
training:	Epoch: [25][495/817]	Loss 0.0071 (0.0373)	
training:	Epoch: [25][496/817]	Loss 0.0076 (0.0373)	
training:	Epoch: [25][497/817]	Loss 0.0083 (0.0372)	
training:	Epoch: [25][498/817]	Loss 0.0069 (0.0372)	
training:	Epoch: [25][499/817]	Loss 0.0085 (0.0371)	
training:	Epoch: [25][500/817]	Loss 0.0073 (0.0370)	
training:	Epoch: [25][501/817]	Loss 0.0081 (0.0370)	
training:	Epoch: [25][502/817]	Loss 0.0077 (0.0369)	
training:	Epoch: [25][503/817]	Loss 0.0065 (0.0369)	
training:	Epoch: [25][504/817]	Loss 0.0068 (0.0368)	
training:	Epoch: [25][505/817]	Loss 0.0082 (0.0367)	
training:	Epoch: [25][506/817]	Loss 0.0077 (0.0367)	
training:	Epoch: [25][507/817]	Loss 0.5057 (0.0376)	
training:	Epoch: [25][508/817]	Loss 0.0073 (0.0376)	
training:	Epoch: [25][509/817]	Loss 0.0076 (0.0375)	
training:	Epoch: [25][510/817]	Loss 0.0079 (0.0374)	
training:	Epoch: [25][511/817]	Loss 0.0076 (0.0374)	
training:	Epoch: [25][512/817]	Loss 0.0938 (0.0375)	
training:	Epoch: [25][513/817]	Loss 0.0078 (0.0374)	
training:	Epoch: [25][514/817]	Loss 0.0077 (0.0374)	
training:	Epoch: [25][515/817]	Loss 0.0078 (0.0373)	
training:	Epoch: [25][516/817]	Loss 0.0898 (0.0374)	
training:	Epoch: [25][517/817]	Loss 0.0079 (0.0374)	
training:	Epoch: [25][518/817]	Loss 0.0070 (0.0373)	
training:	Epoch: [25][519/817]	Loss 0.0072 (0.0372)	
training:	Epoch: [25][520/817]	Loss 0.0100 (0.0372)	
training:	Epoch: [25][521/817]	Loss 0.0073 (0.0371)	
training:	Epoch: [25][522/817]	Loss 0.0069 (0.0371)	
training:	Epoch: [25][523/817]	Loss 0.0082 (0.0370)	
training:	Epoch: [25][524/817]	Loss 0.0077 (0.0370)	
training:	Epoch: [25][525/817]	Loss 0.0072 (0.0369)	
training:	Epoch: [25][526/817]	Loss 0.0070 (0.0369)	
training:	Epoch: [25][527/817]	Loss 0.6220 (0.0380)	
training:	Epoch: [25][528/817]	Loss 0.0073 (0.0379)	
training:	Epoch: [25][529/817]	Loss 0.0073 (0.0378)	
training:	Epoch: [25][530/817]	Loss 0.0078 (0.0378)	
training:	Epoch: [25][531/817]	Loss 0.0071 (0.0377)	
training:	Epoch: [25][532/817]	Loss 0.0083 (0.0377)	
training:	Epoch: [25][533/817]	Loss 0.0100 (0.0376)	
training:	Epoch: [25][534/817]	Loss 0.0075 (0.0376)	
training:	Epoch: [25][535/817]	Loss 0.0074 (0.0375)	
training:	Epoch: [25][536/817]	Loss 0.0068 (0.0375)	
training:	Epoch: [25][537/817]	Loss 0.0077 (0.0374)	
training:	Epoch: [25][538/817]	Loss 0.0085 (0.0373)	
training:	Epoch: [25][539/817]	Loss 0.0080 (0.0373)	
training:	Epoch: [25][540/817]	Loss 0.0068 (0.0372)	
training:	Epoch: [25][541/817]	Loss 0.2178 (0.0376)	
training:	Epoch: [25][542/817]	Loss 0.0077 (0.0375)	
training:	Epoch: [25][543/817]	Loss 0.0071 (0.0375)	
training:	Epoch: [25][544/817]	Loss 0.0077 (0.0374)	
training:	Epoch: [25][545/817]	Loss 0.0072 (0.0373)	
training:	Epoch: [25][546/817]	Loss 0.0078 (0.0373)	
training:	Epoch: [25][547/817]	Loss 0.0092 (0.0372)	
training:	Epoch: [25][548/817]	Loss 0.0079 (0.0372)	
training:	Epoch: [25][549/817]	Loss 0.0086 (0.0371)	
training:	Epoch: [25][550/817]	Loss 0.0076 (0.0371)	
training:	Epoch: [25][551/817]	Loss 0.0084 (0.0370)	
training:	Epoch: [25][552/817]	Loss 0.0073 (0.0370)	
training:	Epoch: [25][553/817]	Loss 0.1048 (0.0371)	
training:	Epoch: [25][554/817]	Loss 0.0150 (0.0371)	
training:	Epoch: [25][555/817]	Loss 0.0075 (0.0370)	
training:	Epoch: [25][556/817]	Loss 0.0068 (0.0370)	
training:	Epoch: [25][557/817]	Loss 0.0066 (0.0369)	
training:	Epoch: [25][558/817]	Loss 0.0074 (0.0368)	
training:	Epoch: [25][559/817]	Loss 0.0085 (0.0368)	
training:	Epoch: [25][560/817]	Loss 0.0072 (0.0367)	
training:	Epoch: [25][561/817]	Loss 0.5859 (0.0377)	
training:	Epoch: [25][562/817]	Loss 0.0074 (0.0377)	
training:	Epoch: [25][563/817]	Loss 0.0089 (0.0376)	
training:	Epoch: [25][564/817]	Loss 0.0071 (0.0376)	
training:	Epoch: [25][565/817]	Loss 0.0203 (0.0375)	
training:	Epoch: [25][566/817]	Loss 0.0079 (0.0375)	
training:	Epoch: [25][567/817]	Loss 0.0077 (0.0374)	
training:	Epoch: [25][568/817]	Loss 0.0152 (0.0374)	
training:	Epoch: [25][569/817]	Loss 0.0073 (0.0373)	
training:	Epoch: [25][570/817]	Loss 0.0074 (0.0373)	
training:	Epoch: [25][571/817]	Loss 0.0074 (0.0372)	
training:	Epoch: [25][572/817]	Loss 0.0071 (0.0372)	
training:	Epoch: [25][573/817]	Loss 0.3165 (0.0377)	
training:	Epoch: [25][574/817]	Loss 0.0111 (0.0376)	
training:	Epoch: [25][575/817]	Loss 0.0066 (0.0376)	
training:	Epoch: [25][576/817]	Loss 0.0068 (0.0375)	
training:	Epoch: [25][577/817]	Loss 0.0086 (0.0375)	
training:	Epoch: [25][578/817]	Loss 0.5961 (0.0384)	
training:	Epoch: [25][579/817]	Loss 0.0071 (0.0384)	
training:	Epoch: [25][580/817]	Loss 0.0069 (0.0383)	
training:	Epoch: [25][581/817]	Loss 0.0071 (0.0383)	
training:	Epoch: [25][582/817]	Loss 0.0072 (0.0382)	
training:	Epoch: [25][583/817]	Loss 0.0085 (0.0382)	
training:	Epoch: [25][584/817]	Loss 0.0080 (0.0381)	
training:	Epoch: [25][585/817]	Loss 0.0094 (0.0381)	
training:	Epoch: [25][586/817]	Loss 0.0074 (0.0380)	
training:	Epoch: [25][587/817]	Loss 0.0074 (0.0380)	
training:	Epoch: [25][588/817]	Loss 0.0069 (0.0379)	
training:	Epoch: [25][589/817]	Loss 0.0077 (0.0379)	
training:	Epoch: [25][590/817]	Loss 0.0171 (0.0378)	
training:	Epoch: [25][591/817]	Loss 0.0074 (0.0378)	
training:	Epoch: [25][592/817]	Loss 0.0076 (0.0377)	
training:	Epoch: [25][593/817]	Loss 0.0067 (0.0377)	
training:	Epoch: [25][594/817]	Loss 0.6041 (0.0386)	
training:	Epoch: [25][595/817]	Loss 0.0070 (0.0386)	
training:	Epoch: [25][596/817]	Loss 0.0095 (0.0385)	
training:	Epoch: [25][597/817]	Loss 0.0065 (0.0385)	
training:	Epoch: [25][598/817]	Loss 0.0072 (0.0384)	
training:	Epoch: [25][599/817]	Loss 0.0071 (0.0384)	
training:	Epoch: [25][600/817]	Loss 0.0074 (0.0383)	
training:	Epoch: [25][601/817]	Loss 0.0068 (0.0382)	
training:	Epoch: [25][602/817]	Loss 0.0075 (0.0382)	
training:	Epoch: [25][603/817]	Loss 0.1821 (0.0384)	
training:	Epoch: [25][604/817]	Loss 0.6259 (0.0394)	
training:	Epoch: [25][605/817]	Loss 0.0160 (0.0394)	
training:	Epoch: [25][606/817]	Loss 0.0640 (0.0394)	
training:	Epoch: [25][607/817]	Loss 0.0075 (0.0394)	
training:	Epoch: [25][608/817]	Loss 0.0075 (0.0393)	
training:	Epoch: [25][609/817]	Loss 0.0077 (0.0393)	
training:	Epoch: [25][610/817]	Loss 0.0071 (0.0392)	
training:	Epoch: [25][611/817]	Loss 0.0078 (0.0392)	
training:	Epoch: [25][612/817]	Loss 0.0069 (0.0391)	
training:	Epoch: [25][613/817]	Loss 0.0074 (0.0390)	
training:	Epoch: [25][614/817]	Loss 0.0066 (0.0390)	
training:	Epoch: [25][615/817]	Loss 0.0088 (0.0389)	
training:	Epoch: [25][616/817]	Loss 0.0077 (0.0389)	
training:	Epoch: [25][617/817]	Loss 0.0071 (0.0388)	
training:	Epoch: [25][618/817]	Loss 0.0082 (0.0388)	
training:	Epoch: [25][619/817]	Loss 0.0082 (0.0387)	
training:	Epoch: [25][620/817]	Loss 0.0117 (0.0387)	
training:	Epoch: [25][621/817]	Loss 0.0081 (0.0387)	
training:	Epoch: [25][622/817]	Loss 0.0076 (0.0386)	
training:	Epoch: [25][623/817]	Loss 0.0105 (0.0386)	
training:	Epoch: [25][624/817]	Loss 0.0073 (0.0385)	
training:	Epoch: [25][625/817]	Loss 0.0106 (0.0385)	
training:	Epoch: [25][626/817]	Loss 0.0073 (0.0384)	
training:	Epoch: [25][627/817]	Loss 0.2046 (0.0387)	
training:	Epoch: [25][628/817]	Loss 0.0073 (0.0386)	
training:	Epoch: [25][629/817]	Loss 0.0098 (0.0386)	
training:	Epoch: [25][630/817]	Loss 0.0080 (0.0385)	
training:	Epoch: [25][631/817]	Loss 0.0080 (0.0385)	
training:	Epoch: [25][632/817]	Loss 0.0073 (0.0384)	
training:	Epoch: [25][633/817]	Loss 0.0073 (0.0384)	
training:	Epoch: [25][634/817]	Loss 0.0072 (0.0383)	
training:	Epoch: [25][635/817]	Loss 0.0097 (0.0383)	
training:	Epoch: [25][636/817]	Loss 0.0075 (0.0382)	
training:	Epoch: [25][637/817]	Loss 0.0075 (0.0382)	
training:	Epoch: [25][638/817]	Loss 0.0074 (0.0381)	
training:	Epoch: [25][639/817]	Loss 0.0084 (0.0381)	
training:	Epoch: [25][640/817]	Loss 0.0083 (0.0381)	
training:	Epoch: [25][641/817]	Loss 0.0073 (0.0380)	
training:	Epoch: [25][642/817]	Loss 0.0080 (0.0380)	
training:	Epoch: [25][643/817]	Loss 0.0074 (0.0379)	
training:	Epoch: [25][644/817]	Loss 0.0076 (0.0379)	
training:	Epoch: [25][645/817]	Loss 0.0074 (0.0378)	
training:	Epoch: [25][646/817]	Loss 0.0078 (0.0378)	
training:	Epoch: [25][647/817]	Loss 0.0068 (0.0377)	
training:	Epoch: [25][648/817]	Loss 0.0078 (0.0377)	
training:	Epoch: [25][649/817]	Loss 0.0086 (0.0376)	
training:	Epoch: [25][650/817]	Loss 0.0089 (0.0376)	
training:	Epoch: [25][651/817]	Loss 0.0078 (0.0375)	
training:	Epoch: [25][652/817]	Loss 0.0159 (0.0375)	
training:	Epoch: [25][653/817]	Loss 0.0070 (0.0375)	
training:	Epoch: [25][654/817]	Loss 0.0066 (0.0374)	
training:	Epoch: [25][655/817]	Loss 0.0070 (0.0374)	
training:	Epoch: [25][656/817]	Loss 0.0078 (0.0373)	
training:	Epoch: [25][657/817]	Loss 0.0094 (0.0373)	
training:	Epoch: [25][658/817]	Loss 0.0075 (0.0372)	
training:	Epoch: [25][659/817]	Loss 0.0078 (0.0372)	
training:	Epoch: [25][660/817]	Loss 0.0076 (0.0371)	
training:	Epoch: [25][661/817]	Loss 0.0080 (0.0371)	
training:	Epoch: [25][662/817]	Loss 0.0085 (0.0371)	
training:	Epoch: [25][663/817]	Loss 0.0132 (0.0370)	
training:	Epoch: [25][664/817]	Loss 0.0074 (0.0370)	
training:	Epoch: [25][665/817]	Loss 0.0074 (0.0369)	
training:	Epoch: [25][666/817]	Loss 0.3509 (0.0374)	
training:	Epoch: [25][667/817]	Loss 0.0072 (0.0374)	
training:	Epoch: [25][668/817]	Loss 0.0071 (0.0373)	
training:	Epoch: [25][669/817]	Loss 0.0102 (0.0373)	
training:	Epoch: [25][670/817]	Loss 0.5921 (0.0381)	
training:	Epoch: [25][671/817]	Loss 0.6261 (0.0390)	
training:	Epoch: [25][672/817]	Loss 0.5337 (0.0397)	
training:	Epoch: [25][673/817]	Loss 0.0071 (0.0397)	
training:	Epoch: [25][674/817]	Loss 0.0073 (0.0396)	
training:	Epoch: [25][675/817]	Loss 0.2549 (0.0399)	
training:	Epoch: [25][676/817]	Loss 0.0086 (0.0399)	
training:	Epoch: [25][677/817]	Loss 0.0074 (0.0398)	
training:	Epoch: [25][678/817]	Loss 0.0082 (0.0398)	
training:	Epoch: [25][679/817]	Loss 0.0085 (0.0397)	
training:	Epoch: [25][680/817]	Loss 0.0141 (0.0397)	
training:	Epoch: [25][681/817]	Loss 0.0640 (0.0397)	
training:	Epoch: [25][682/817]	Loss 0.0083 (0.0397)	
training:	Epoch: [25][683/817]	Loss 0.0080 (0.0397)	
training:	Epoch: [25][684/817]	Loss 0.5713 (0.0404)	
training:	Epoch: [25][685/817]	Loss 0.5943 (0.0412)	
training:	Epoch: [25][686/817]	Loss 0.0071 (0.0412)	
training:	Epoch: [25][687/817]	Loss 0.0076 (0.0411)	
training:	Epoch: [25][688/817]	Loss 0.0090 (0.0411)	
training:	Epoch: [25][689/817]	Loss 0.3619 (0.0416)	
training:	Epoch: [25][690/817]	Loss 0.0076 (0.0415)	
training:	Epoch: [25][691/817]	Loss 0.0077 (0.0415)	
training:	Epoch: [25][692/817]	Loss 0.0105 (0.0414)	
training:	Epoch: [25][693/817]	Loss 0.0084 (0.0414)	
training:	Epoch: [25][694/817]	Loss 0.0094 (0.0413)	
training:	Epoch: [25][695/817]	Loss 0.2045 (0.0416)	
training:	Epoch: [25][696/817]	Loss 0.0076 (0.0415)	
training:	Epoch: [25][697/817]	Loss 0.0143 (0.0415)	
training:	Epoch: [25][698/817]	Loss 0.0134 (0.0414)	
training:	Epoch: [25][699/817]	Loss 0.0074 (0.0414)	
training:	Epoch: [25][700/817]	Loss 0.0086 (0.0413)	
training:	Epoch: [25][701/817]	Loss 0.0078 (0.0413)	
training:	Epoch: [25][702/817]	Loss 0.0081 (0.0412)	
training:	Epoch: [25][703/817]	Loss 0.5987 (0.0420)	
training:	Epoch: [25][704/817]	Loss 0.0077 (0.0420)	
training:	Epoch: [25][705/817]	Loss 0.0079 (0.0419)	
training:	Epoch: [25][706/817]	Loss 0.0074 (0.0419)	
training:	Epoch: [25][707/817]	Loss 0.0090 (0.0418)	
training:	Epoch: [25][708/817]	Loss 0.0881 (0.0419)	
training:	Epoch: [25][709/817]	Loss 0.0187 (0.0419)	
training:	Epoch: [25][710/817]	Loss 0.0099 (0.0418)	
training:	Epoch: [25][711/817]	Loss 0.0073 (0.0418)	
training:	Epoch: [25][712/817]	Loss 0.0074 (0.0417)	
training:	Epoch: [25][713/817]	Loss 0.0072 (0.0417)	
training:	Epoch: [25][714/817]	Loss 0.0078 (0.0416)	
training:	Epoch: [25][715/817]	Loss 0.0072 (0.0416)	
training:	Epoch: [25][716/817]	Loss 0.6234 (0.0424)	
training:	Epoch: [25][717/817]	Loss 0.0079 (0.0424)	
training:	Epoch: [25][718/817]	Loss 0.0165 (0.0423)	
training:	Epoch: [25][719/817]	Loss 0.0077 (0.0423)	
training:	Epoch: [25][720/817]	Loss 0.0105 (0.0422)	
training:	Epoch: [25][721/817]	Loss 0.0071 (0.0422)	
training:	Epoch: [25][722/817]	Loss 0.5965 (0.0429)	
training:	Epoch: [25][723/817]	Loss 0.0090 (0.0429)	
training:	Epoch: [25][724/817]	Loss 0.0106 (0.0429)	
training:	Epoch: [25][725/817]	Loss 0.0088 (0.0428)	
training:	Epoch: [25][726/817]	Loss 0.0075 (0.0428)	
training:	Epoch: [25][727/817]	Loss 0.0081 (0.0427)	
training:	Epoch: [25][728/817]	Loss 0.0072 (0.0427)	
training:	Epoch: [25][729/817]	Loss 0.6107 (0.0434)	
training:	Epoch: [25][730/817]	Loss 0.0092 (0.0434)	
training:	Epoch: [25][731/817]	Loss 0.0087 (0.0433)	
training:	Epoch: [25][732/817]	Loss 0.6312 (0.0441)	
training:	Epoch: [25][733/817]	Loss 0.1153 (0.0442)	
training:	Epoch: [25][734/817]	Loss 0.0596 (0.0443)	
training:	Epoch: [25][735/817]	Loss 0.0087 (0.0442)	
training:	Epoch: [25][736/817]	Loss 0.0088 (0.0442)	
training:	Epoch: [25][737/817]	Loss 0.0079 (0.0441)	
training:	Epoch: [25][738/817]	Loss 0.0072 (0.0441)	
training:	Epoch: [25][739/817]	Loss 0.0075 (0.0440)	
training:	Epoch: [25][740/817]	Loss 0.0088 (0.0440)	
training:	Epoch: [25][741/817]	Loss 0.0095 (0.0439)	
training:	Epoch: [25][742/817]	Loss 0.6018 (0.0447)	
training:	Epoch: [25][743/817]	Loss 0.0080 (0.0446)	
training:	Epoch: [25][744/817]	Loss 0.1764 (0.0448)	
training:	Epoch: [25][745/817]	Loss 0.0095 (0.0448)	
training:	Epoch: [25][746/817]	Loss 0.0084 (0.0447)	
training:	Epoch: [25][747/817]	Loss 0.0089 (0.0447)	
training:	Epoch: [25][748/817]	Loss 0.0084 (0.0446)	
training:	Epoch: [25][749/817]	Loss 0.0798 (0.0447)	
training:	Epoch: [25][750/817]	Loss 0.0181 (0.0446)	
training:	Epoch: [25][751/817]	Loss 0.0078 (0.0446)	
training:	Epoch: [25][752/817]	Loss 0.1004 (0.0446)	
training:	Epoch: [25][753/817]	Loss 0.0074 (0.0446)	
training:	Epoch: [25][754/817]	Loss 0.0091 (0.0446)	
training:	Epoch: [25][755/817]	Loss 0.0095 (0.0445)	
training:	Epoch: [25][756/817]	Loss 0.0083 (0.0445)	
training:	Epoch: [25][757/817]	Loss 0.0083 (0.0444)	
training:	Epoch: [25][758/817]	Loss 0.0075 (0.0444)	
training:	Epoch: [25][759/817]	Loss 0.0077 (0.0443)	
training:	Epoch: [25][760/817]	Loss 0.0086 (0.0443)	
training:	Epoch: [25][761/817]	Loss 0.0316 (0.0442)	
training:	Epoch: [25][762/817]	Loss 0.0106 (0.0442)	
training:	Epoch: [25][763/817]	Loss 0.0409 (0.0442)	
training:	Epoch: [25][764/817]	Loss 0.0088 (0.0442)	
training:	Epoch: [25][765/817]	Loss 0.0081 (0.0441)	
training:	Epoch: [25][766/817]	Loss 0.0077 (0.0441)	
training:	Epoch: [25][767/817]	Loss 0.0679 (0.0441)	
training:	Epoch: [25][768/817]	Loss 0.0191 (0.0441)	
training:	Epoch: [25][769/817]	Loss 0.0080 (0.0440)	
training:	Epoch: [25][770/817]	Loss 0.0081 (0.0440)	
training:	Epoch: [25][771/817]	Loss 0.0077 (0.0439)	
training:	Epoch: [25][772/817]	Loss 0.0070 (0.0439)	
training:	Epoch: [25][773/817]	Loss 0.0076 (0.0438)	
training:	Epoch: [25][774/817]	Loss 0.0088 (0.0438)	
training:	Epoch: [25][775/817]	Loss 0.0090 (0.0437)	
training:	Epoch: [25][776/817]	Loss 0.5666 (0.0444)	
training:	Epoch: [25][777/817]	Loss 0.0085 (0.0444)	
training:	Epoch: [25][778/817]	Loss 0.0086 (0.0443)	
training:	Epoch: [25][779/817]	Loss 0.0097 (0.0443)	
training:	Epoch: [25][780/817]	Loss 0.0086 (0.0442)	
training:	Epoch: [25][781/817]	Loss 0.0076 (0.0442)	
training:	Epoch: [25][782/817]	Loss 0.0078 (0.0441)	
training:	Epoch: [25][783/817]	Loss 0.0077 (0.0441)	
training:	Epoch: [25][784/817]	Loss 0.0076 (0.0440)	
training:	Epoch: [25][785/817]	Loss 0.0074 (0.0440)	
training:	Epoch: [25][786/817]	Loss 0.0076 (0.0439)	
training:	Epoch: [25][787/817]	Loss 0.0139 (0.0439)	
training:	Epoch: [25][788/817]	Loss 0.0079 (0.0439)	
training:	Epoch: [25][789/817]	Loss 0.0154 (0.0438)	
training:	Epoch: [25][790/817]	Loss 0.0108 (0.0438)	
training:	Epoch: [25][791/817]	Loss 0.0085 (0.0437)	
training:	Epoch: [25][792/817]	Loss 0.0094 (0.0437)	
training:	Epoch: [25][793/817]	Loss 0.0073 (0.0436)	
training:	Epoch: [25][794/817]	Loss 0.0075 (0.0436)	
training:	Epoch: [25][795/817]	Loss 0.0076 (0.0436)	
training:	Epoch: [25][796/817]	Loss 0.0085 (0.0435)	
training:	Epoch: [25][797/817]	Loss 0.5460 (0.0441)	
training:	Epoch: [25][798/817]	Loss 0.0080 (0.0441)	
training:	Epoch: [25][799/817]	Loss 0.0081 (0.0441)	
training:	Epoch: [25][800/817]	Loss 0.0079 (0.0440)	
training:	Epoch: [25][801/817]	Loss 0.0125 (0.0440)	
training:	Epoch: [25][802/817]	Loss 0.0081 (0.0439)	
training:	Epoch: [25][803/817]	Loss 0.0093 (0.0439)	
training:	Epoch: [25][804/817]	Loss 0.0085 (0.0438)	
training:	Epoch: [25][805/817]	Loss 0.0092 (0.0438)	
training:	Epoch: [25][806/817]	Loss 0.0091 (0.0438)	
training:	Epoch: [25][807/817]	Loss 0.0080 (0.0437)	
training:	Epoch: [25][808/817]	Loss 0.5340 (0.0443)	
training:	Epoch: [25][809/817]	Loss 0.0077 (0.0443)	
training:	Epoch: [25][810/817]	Loss 0.0080 (0.0442)	
training:	Epoch: [25][811/817]	Loss 0.0091 (0.0442)	
training:	Epoch: [25][812/817]	Loss 0.0085 (0.0441)	
training:	Epoch: [25][813/817]	Loss 0.0091 (0.0441)	
training:	Epoch: [25][814/817]	Loss 0.0076 (0.0440)	
training:	Epoch: [25][815/817]	Loss 0.0076 (0.0440)	
training:	Epoch: [25][816/817]	Loss 0.0080 (0.0440)	
training:	Epoch: [25][817/817]	Loss 0.0088 (0.0439)	
Training:	 Loss: 0.0439

Training:	 ACC: 0.9933 0.9933 0.9932 0.9933
Validation:	 ACC: 0.7915 0.7919 0.7994 0.7836
Validation:	 Best_BACC: 0.8223 0.8218 0.8127 0.8318
Validation:	 Loss: 0.8640
Pretraining:	Epoch 26/200
----------
training:	Epoch: [26][1/817]	Loss 0.0084 (0.0084)	
training:	Epoch: [26][2/817]	Loss 0.0101 (0.0092)	
training:	Epoch: [26][3/817]	Loss 0.0072 (0.0086)	
training:	Epoch: [26][4/817]	Loss 0.0084 (0.0085)	
training:	Epoch: [26][5/817]	Loss 0.0077 (0.0084)	
training:	Epoch: [26][6/817]	Loss 0.0089 (0.0084)	
training:	Epoch: [26][7/817]	Loss 0.0087 (0.0085)	
training:	Epoch: [26][8/817]	Loss 0.0081 (0.0084)	
training:	Epoch: [26][9/817]	Loss 0.0092 (0.0085)	
training:	Epoch: [26][10/817]	Loss 0.0711 (0.0148)	
training:	Epoch: [26][11/817]	Loss 0.0086 (0.0142)	
training:	Epoch: [26][12/817]	Loss 0.0082 (0.0137)	
training:	Epoch: [26][13/817]	Loss 0.0086 (0.0133)	
training:	Epoch: [26][14/817]	Loss 0.0089 (0.0130)	
training:	Epoch: [26][15/817]	Loss 0.0069 (0.0126)	
training:	Epoch: [26][16/817]	Loss 0.0080 (0.0123)	
training:	Epoch: [26][17/817]	Loss 0.0083 (0.0121)	
training:	Epoch: [26][18/817]	Loss 0.0075 (0.0118)	
training:	Epoch: [26][19/817]	Loss 0.0073 (0.0116)	
training:	Epoch: [26][20/817]	Loss 0.0100 (0.0115)	
training:	Epoch: [26][21/817]	Loss 0.0081 (0.0113)	
training:	Epoch: [26][22/817]	Loss 0.0087 (0.0112)	
training:	Epoch: [26][23/817]	Loss 0.0086 (0.0111)	
training:	Epoch: [26][24/817]	Loss 0.0079 (0.0110)	
training:	Epoch: [26][25/817]	Loss 0.0077 (0.0108)	
training:	Epoch: [26][26/817]	Loss 0.0081 (0.0107)	
training:	Epoch: [26][27/817]	Loss 0.0092 (0.0107)	
training:	Epoch: [26][28/817]	Loss 0.0084 (0.0106)	
training:	Epoch: [26][29/817]	Loss 0.0078 (0.0105)	
training:	Epoch: [26][30/817]	Loss 0.0174 (0.0107)	
training:	Epoch: [26][31/817]	Loss 0.0081 (0.0106)	
training:	Epoch: [26][32/817]	Loss 0.0085 (0.0106)	
training:	Epoch: [26][33/817]	Loss 0.0086 (0.0105)	
training:	Epoch: [26][34/817]	Loss 0.0710 (0.0123)	
training:	Epoch: [26][35/817]	Loss 0.0086 (0.0122)	
training:	Epoch: [26][36/817]	Loss 0.5848 (0.0281)	
training:	Epoch: [26][37/817]	Loss 0.0107 (0.0276)	
training:	Epoch: [26][38/817]	Loss 0.0081 (0.0271)	
training:	Epoch: [26][39/817]	Loss 0.5985 (0.0418)	
training:	Epoch: [26][40/817]	Loss 0.0091 (0.0409)	
training:	Epoch: [26][41/817]	Loss 0.0086 (0.0402)	
training:	Epoch: [26][42/817]	Loss 0.0070 (0.0394)	
training:	Epoch: [26][43/817]	Loss 0.0085 (0.0386)	
training:	Epoch: [26][44/817]	Loss 0.0087 (0.0380)	
training:	Epoch: [26][45/817]	Loss 0.0087 (0.0373)	
training:	Epoch: [26][46/817]	Loss 0.0088 (0.0367)	
training:	Epoch: [26][47/817]	Loss 0.0069 (0.0361)	
training:	Epoch: [26][48/817]	Loss 0.0081 (0.0355)	
training:	Epoch: [26][49/817]	Loss 0.0081 (0.0349)	
training:	Epoch: [26][50/817]	Loss 0.0087 (0.0344)	
training:	Epoch: [26][51/817]	Loss 0.5815 (0.0451)	
training:	Epoch: [26][52/817]	Loss 0.0078 (0.0444)	
training:	Epoch: [26][53/817]	Loss 0.0073 (0.0437)	
training:	Epoch: [26][54/817]	Loss 0.0083 (0.0431)	
training:	Epoch: [26][55/817]	Loss 0.0079 (0.0424)	
training:	Epoch: [26][56/817]	Loss 0.0087 (0.0418)	
training:	Epoch: [26][57/817]	Loss 0.0078 (0.0412)	
training:	Epoch: [26][58/817]	Loss 0.0080 (0.0406)	
training:	Epoch: [26][59/817]	Loss 0.0094 (0.0401)	
training:	Epoch: [26][60/817]	Loss 0.0083 (0.0396)	
training:	Epoch: [26][61/817]	Loss 0.0090 (0.0391)	
training:	Epoch: [26][62/817]	Loss 0.0076 (0.0386)	
training:	Epoch: [26][63/817]	Loss 0.0122 (0.0382)	
training:	Epoch: [26][64/817]	Loss 0.0090 (0.0377)	
training:	Epoch: [26][65/817]	Loss 0.5590 (0.0457)	
training:	Epoch: [26][66/817]	Loss 0.0159 (0.0453)	
training:	Epoch: [26][67/817]	Loss 0.0079 (0.0447)	
training:	Epoch: [26][68/817]	Loss 0.0082 (0.0442)	
training:	Epoch: [26][69/817]	Loss 0.0080 (0.0436)	
training:	Epoch: [26][70/817]	Loss 0.0078 (0.0431)	
training:	Epoch: [26][71/817]	Loss 0.0086 (0.0427)	
training:	Epoch: [26][72/817]	Loss 0.0088 (0.0422)	
training:	Epoch: [26][73/817]	Loss 0.0091 (0.0417)	
training:	Epoch: [26][74/817]	Loss 0.0076 (0.0413)	
training:	Epoch: [26][75/817]	Loss 0.0081 (0.0408)	
training:	Epoch: [26][76/817]	Loss 0.0090 (0.0404)	
training:	Epoch: [26][77/817]	Loss 0.0076 (0.0400)	
training:	Epoch: [26][78/817]	Loss 0.0081 (0.0396)	
training:	Epoch: [26][79/817]	Loss 0.0090 (0.0392)	
training:	Epoch: [26][80/817]	Loss 0.0105 (0.0388)	
training:	Epoch: [26][81/817]	Loss 0.0076 (0.0384)	
training:	Epoch: [26][82/817]	Loss 0.0087 (0.0381)	
training:	Epoch: [26][83/817]	Loss 0.0099 (0.0377)	
training:	Epoch: [26][84/817]	Loss 0.0088 (0.0374)	
training:	Epoch: [26][85/817]	Loss 0.0081 (0.0370)	
training:	Epoch: [26][86/817]	Loss 0.0081 (0.0367)	
training:	Epoch: [26][87/817]	Loss 0.6150 (0.0434)	
training:	Epoch: [26][88/817]	Loss 0.0077 (0.0430)	
training:	Epoch: [26][89/817]	Loss 0.0085 (0.0426)	
training:	Epoch: [26][90/817]	Loss 0.0088 (0.0422)	
training:	Epoch: [26][91/817]	Loss 0.0074 (0.0418)	
training:	Epoch: [26][92/817]	Loss 0.0083 (0.0414)	
training:	Epoch: [26][93/817]	Loss 0.5757 (0.0472)	
training:	Epoch: [26][94/817]	Loss 0.0089 (0.0468)	
training:	Epoch: [26][95/817]	Loss 0.0077 (0.0464)	
training:	Epoch: [26][96/817]	Loss 0.0086 (0.0460)	
training:	Epoch: [26][97/817]	Loss 0.0076 (0.0456)	
training:	Epoch: [26][98/817]	Loss 0.0077 (0.0452)	
training:	Epoch: [26][99/817]	Loss 0.0082 (0.0448)	
training:	Epoch: [26][100/817]	Loss 0.0088 (0.0445)	
training:	Epoch: [26][101/817]	Loss 0.0075 (0.0441)	
training:	Epoch: [26][102/817]	Loss 0.0077 (0.0437)	
training:	Epoch: [26][103/817]	Loss 0.0089 (0.0434)	
training:	Epoch: [26][104/817]	Loss 0.0097 (0.0431)	
training:	Epoch: [26][105/817]	Loss 0.0083 (0.0427)	
training:	Epoch: [26][106/817]	Loss 0.0074 (0.0424)	
training:	Epoch: [26][107/817]	Loss 0.0096 (0.0421)	
training:	Epoch: [26][108/817]	Loss 0.0078 (0.0418)	
training:	Epoch: [26][109/817]	Loss 0.0081 (0.0415)	
training:	Epoch: [26][110/817]	Loss 0.0093 (0.0412)	
training:	Epoch: [26][111/817]	Loss 0.0078 (0.0409)	
training:	Epoch: [26][112/817]	Loss 0.0170 (0.0407)	
training:	Epoch: [26][113/817]	Loss 0.0077 (0.0404)	
training:	Epoch: [26][114/817]	Loss 0.0091 (0.0401)	
training:	Epoch: [26][115/817]	Loss 0.0077 (0.0398)	
training:	Epoch: [26][116/817]	Loss 0.0086 (0.0396)	
training:	Epoch: [26][117/817]	Loss 0.0081 (0.0393)	
training:	Epoch: [26][118/817]	Loss 0.0087 (0.0390)	
training:	Epoch: [26][119/817]	Loss 0.0077 (0.0388)	
training:	Epoch: [26][120/817]	Loss 0.0073 (0.0385)	
training:	Epoch: [26][121/817]	Loss 0.0087 (0.0383)	
training:	Epoch: [26][122/817]	Loss 0.0089 (0.0380)	
training:	Epoch: [26][123/817]	Loss 0.0074 (0.0378)	
training:	Epoch: [26][124/817]	Loss 0.0088 (0.0375)	
training:	Epoch: [26][125/817]	Loss 0.5976 (0.0420)	
training:	Epoch: [26][126/817]	Loss 0.0084 (0.0417)	
training:	Epoch: [26][127/817]	Loss 0.0080 (0.0415)	
training:	Epoch: [26][128/817]	Loss 0.0082 (0.0412)	
training:	Epoch: [26][129/817]	Loss 0.0067 (0.0410)	
training:	Epoch: [26][130/817]	Loss 0.0080 (0.0407)	
training:	Epoch: [26][131/817]	Loss 0.0084 (0.0405)	
training:	Epoch: [26][132/817]	Loss 0.0083 (0.0402)	
training:	Epoch: [26][133/817]	Loss 0.0073 (0.0400)	
training:	Epoch: [26][134/817]	Loss 0.0069 (0.0397)	
training:	Epoch: [26][135/817]	Loss 0.0100 (0.0395)	
training:	Epoch: [26][136/817]	Loss 0.0082 (0.0393)	
training:	Epoch: [26][137/817]	Loss 0.0083 (0.0390)	
training:	Epoch: [26][138/817]	Loss 0.0079 (0.0388)	
training:	Epoch: [26][139/817]	Loss 0.0078 (0.0386)	
training:	Epoch: [26][140/817]	Loss 0.0075 (0.0384)	
training:	Epoch: [26][141/817]	Loss 0.0082 (0.0382)	
training:	Epoch: [26][142/817]	Loss 0.0083 (0.0379)	
training:	Epoch: [26][143/817]	Loss 0.0071 (0.0377)	
training:	Epoch: [26][144/817]	Loss 0.0083 (0.0375)	
training:	Epoch: [26][145/817]	Loss 0.0083 (0.0373)	
training:	Epoch: [26][146/817]	Loss 0.0067 (0.0371)	
training:	Epoch: [26][147/817]	Loss 0.0090 (0.0369)	
training:	Epoch: [26][148/817]	Loss 0.0096 (0.0367)	
training:	Epoch: [26][149/817]	Loss 0.5719 (0.0403)	
training:	Epoch: [26][150/817]	Loss 0.0071 (0.0401)	
training:	Epoch: [26][151/817]	Loss 0.0082 (0.0399)	
training:	Epoch: [26][152/817]	Loss 0.0074 (0.0397)	
training:	Epoch: [26][153/817]	Loss 0.0084 (0.0395)	
training:	Epoch: [26][154/817]	Loss 0.0077 (0.0393)	
training:	Epoch: [26][155/817]	Loss 0.0082 (0.0391)	
training:	Epoch: [26][156/817]	Loss 0.0091 (0.0389)	
training:	Epoch: [26][157/817]	Loss 0.0075 (0.0387)	
training:	Epoch: [26][158/817]	Loss 0.0078 (0.0385)	
training:	Epoch: [26][159/817]	Loss 0.0075 (0.0383)	
training:	Epoch: [26][160/817]	Loss 0.0075 (0.0381)	
training:	Epoch: [26][161/817]	Loss 0.0069 (0.0379)	
training:	Epoch: [26][162/817]	Loss 0.0082 (0.0377)	
training:	Epoch: [26][163/817]	Loss 0.0110 (0.0376)	
training:	Epoch: [26][164/817]	Loss 0.0074 (0.0374)	
training:	Epoch: [26][165/817]	Loss 0.0086 (0.0372)	
training:	Epoch: [26][166/817]	Loss 0.0075 (0.0370)	
training:	Epoch: [26][167/817]	Loss 0.0075 (0.0368)	
training:	Epoch: [26][168/817]	Loss 0.0078 (0.0367)	
training:	Epoch: [26][169/817]	Loss 0.0088 (0.0365)	
training:	Epoch: [26][170/817]	Loss 0.6021 (0.0398)	
training:	Epoch: [26][171/817]	Loss 0.5891 (0.0430)	
training:	Epoch: [26][172/817]	Loss 0.0082 (0.0428)	
training:	Epoch: [26][173/817]	Loss 0.0086 (0.0426)	
training:	Epoch: [26][174/817]	Loss 0.0097 (0.0425)	
training:	Epoch: [26][175/817]	Loss 0.0385 (0.0424)	
training:	Epoch: [26][176/817]	Loss 0.0084 (0.0422)	
training:	Epoch: [26][177/817]	Loss 0.0087 (0.0420)	
training:	Epoch: [26][178/817]	Loss 0.0095 (0.0419)	
training:	Epoch: [26][179/817]	Loss 0.0074 (0.0417)	
training:	Epoch: [26][180/817]	Loss 0.0084 (0.0415)	
training:	Epoch: [26][181/817]	Loss 0.0081 (0.0413)	
training:	Epoch: [26][182/817]	Loss 0.0083 (0.0411)	
training:	Epoch: [26][183/817]	Loss 0.5824 (0.0441)	
training:	Epoch: [26][184/817]	Loss 0.0079 (0.0439)	
training:	Epoch: [26][185/817]	Loss 0.0083 (0.0437)	
training:	Epoch: [26][186/817]	Loss 0.3779 (0.0455)	
training:	Epoch: [26][187/817]	Loss 0.5811 (0.0484)	
training:	Epoch: [26][188/817]	Loss 0.0075 (0.0481)	
training:	Epoch: [26][189/817]	Loss 0.0090 (0.0479)	
training:	Epoch: [26][190/817]	Loss 0.0083 (0.0477)	
training:	Epoch: [26][191/817]	Loss 0.0082 (0.0475)	
training:	Epoch: [26][192/817]	Loss 0.0074 (0.0473)	
training:	Epoch: [26][193/817]	Loss 0.6108 (0.0502)	
training:	Epoch: [26][194/817]	Loss 0.0085 (0.0500)	
training:	Epoch: [26][195/817]	Loss 0.0087 (0.0498)	
training:	Epoch: [26][196/817]	Loss 0.0097 (0.0496)	
training:	Epoch: [26][197/817]	Loss 0.0108 (0.0494)	
training:	Epoch: [26][198/817]	Loss 0.0087 (0.0492)	
training:	Epoch: [26][199/817]	Loss 0.0072 (0.0490)	
training:	Epoch: [26][200/817]	Loss 0.0089 (0.0488)	
training:	Epoch: [26][201/817]	Loss 0.0088 (0.0486)	
training:	Epoch: [26][202/817]	Loss 0.0090 (0.0484)	
training:	Epoch: [26][203/817]	Loss 0.0137 (0.0482)	
training:	Epoch: [26][204/817]	Loss 0.0144 (0.0480)	
training:	Epoch: [26][205/817]	Loss 0.0094 (0.0479)	
training:	Epoch: [26][206/817]	Loss 0.0231 (0.0477)	
training:	Epoch: [26][207/817]	Loss 0.0095 (0.0476)	
training:	Epoch: [26][208/817]	Loss 0.0083 (0.0474)	
training:	Epoch: [26][209/817]	Loss 0.0094 (0.0472)	
training:	Epoch: [26][210/817]	Loss 0.1444 (0.0476)	
training:	Epoch: [26][211/817]	Loss 0.0085 (0.0475)	
training:	Epoch: [26][212/817]	Loss 0.0093 (0.0473)	
training:	Epoch: [26][213/817]	Loss 0.0083 (0.0471)	
training:	Epoch: [26][214/817]	Loss 0.0082 (0.0469)	
training:	Epoch: [26][215/817]	Loss 0.0086 (0.0467)	
training:	Epoch: [26][216/817]	Loss 0.0086 (0.0466)	
training:	Epoch: [26][217/817]	Loss 0.0085 (0.0464)	
training:	Epoch: [26][218/817]	Loss 0.0078 (0.0462)	
training:	Epoch: [26][219/817]	Loss 0.0072 (0.0460)	
training:	Epoch: [26][220/817]	Loss 0.0098 (0.0459)	
training:	Epoch: [26][221/817]	Loss 0.0072 (0.0457)	
training:	Epoch: [26][222/817]	Loss 0.0073 (0.0455)	
training:	Epoch: [26][223/817]	Loss 0.0101 (0.0454)	
training:	Epoch: [26][224/817]	Loss 0.5621 (0.0477)	
training:	Epoch: [26][225/817]	Loss 0.0082 (0.0475)	
training:	Epoch: [26][226/817]	Loss 0.0078 (0.0473)	
training:	Epoch: [26][227/817]	Loss 0.0096 (0.0471)	
training:	Epoch: [26][228/817]	Loss 0.0084 (0.0470)	
training:	Epoch: [26][229/817]	Loss 0.0080 (0.0468)	
training:	Epoch: [26][230/817]	Loss 0.0087 (0.0466)	
training:	Epoch: [26][231/817]	Loss 0.0100 (0.0465)	
training:	Epoch: [26][232/817]	Loss 0.0103 (0.0463)	
training:	Epoch: [26][233/817]	Loss 0.0103 (0.0462)	
training:	Epoch: [26][234/817]	Loss 0.0081 (0.0460)	
training:	Epoch: [26][235/817]	Loss 0.0078 (0.0458)	
training:	Epoch: [26][236/817]	Loss 0.0155 (0.0457)	
training:	Epoch: [26][237/817]	Loss 0.0076 (0.0456)	
training:	Epoch: [26][238/817]	Loss 0.0081 (0.0454)	
training:	Epoch: [26][239/817]	Loss 0.0090 (0.0452)	
training:	Epoch: [26][240/817]	Loss 0.0103 (0.0451)	
training:	Epoch: [26][241/817]	Loss 0.0082 (0.0450)	
training:	Epoch: [26][242/817]	Loss 0.0092 (0.0448)	
training:	Epoch: [26][243/817]	Loss 0.0081 (0.0447)	
training:	Epoch: [26][244/817]	Loss 0.0091 (0.0445)	
training:	Epoch: [26][245/817]	Loss 0.0086 (0.0444)	
training:	Epoch: [26][246/817]	Loss 0.4951 (0.0462)	
training:	Epoch: [26][247/817]	Loss 0.0082 (0.0460)	
training:	Epoch: [26][248/817]	Loss 0.0073 (0.0459)	
training:	Epoch: [26][249/817]	Loss 0.0076 (0.0457)	
training:	Epoch: [26][250/817]	Loss 0.0091 (0.0456)	
training:	Epoch: [26][251/817]	Loss 0.0131 (0.0455)	
training:	Epoch: [26][252/817]	Loss 0.0093 (0.0453)	
training:	Epoch: [26][253/817]	Loss 0.8575 (0.0485)	
training:	Epoch: [26][254/817]	Loss 0.0080 (0.0484)	
training:	Epoch: [26][255/817]	Loss 0.0088 (0.0482)	
training:	Epoch: [26][256/817]	Loss 0.0103 (0.0481)	
training:	Epoch: [26][257/817]	Loss 0.0087 (0.0479)	
training:	Epoch: [26][258/817]	Loss 0.0097 (0.0478)	
training:	Epoch: [26][259/817]	Loss 0.0103 (0.0476)	
training:	Epoch: [26][260/817]	Loss 0.0101 (0.0475)	
training:	Epoch: [26][261/817]	Loss 0.0095 (0.0473)	
training:	Epoch: [26][262/817]	Loss 0.0093 (0.0472)	
training:	Epoch: [26][263/817]	Loss 0.0083 (0.0470)	
training:	Epoch: [26][264/817]	Loss 0.0092 (0.0469)	
training:	Epoch: [26][265/817]	Loss 0.0068 (0.0467)	
training:	Epoch: [26][266/817]	Loss 0.0100 (0.0466)	
training:	Epoch: [26][267/817]	Loss 0.0101 (0.0465)	
training:	Epoch: [26][268/817]	Loss 0.0070 (0.0463)	
training:	Epoch: [26][269/817]	Loss 0.0097 (0.0462)	
training:	Epoch: [26][270/817]	Loss 0.0111 (0.0460)	
training:	Epoch: [26][271/817]	Loss 0.0125 (0.0459)	
training:	Epoch: [26][272/817]	Loss 0.0069 (0.0458)	
training:	Epoch: [26][273/817]	Loss 0.0107 (0.0456)	
training:	Epoch: [26][274/817]	Loss 0.0086 (0.0455)	
training:	Epoch: [26][275/817]	Loss 0.0083 (0.0454)	
training:	Epoch: [26][276/817]	Loss 0.0108 (0.0453)	
training:	Epoch: [26][277/817]	Loss 0.0081 (0.0451)	
training:	Epoch: [26][278/817]	Loss 0.0070 (0.0450)	
training:	Epoch: [26][279/817]	Loss 0.0082 (0.0449)	
training:	Epoch: [26][280/817]	Loss 0.0102 (0.0447)	
training:	Epoch: [26][281/817]	Loss 0.0077 (0.0446)	
training:	Epoch: [26][282/817]	Loss 0.0088 (0.0445)	
training:	Epoch: [26][283/817]	Loss 0.0100 (0.0443)	
training:	Epoch: [26][284/817]	Loss 0.0099 (0.0442)	
training:	Epoch: [26][285/817]	Loss 0.6371 (0.0463)	
training:	Epoch: [26][286/817]	Loss 0.0076 (0.0462)	
training:	Epoch: [26][287/817]	Loss 0.0080 (0.0460)	
training:	Epoch: [26][288/817]	Loss 0.0082 (0.0459)	
training:	Epoch: [26][289/817]	Loss 0.5433 (0.0476)	
training:	Epoch: [26][290/817]	Loss 0.0085 (0.0475)	
training:	Epoch: [26][291/817]	Loss 0.0085 (0.0474)	
training:	Epoch: [26][292/817]	Loss 0.0103 (0.0472)	
training:	Epoch: [26][293/817]	Loss 0.0090 (0.0471)	
training:	Epoch: [26][294/817]	Loss 0.0088 (0.0470)	
training:	Epoch: [26][295/817]	Loss 0.6132 (0.0489)	
training:	Epoch: [26][296/817]	Loss 0.0078 (0.0488)	
training:	Epoch: [26][297/817]	Loss 0.0085 (0.0486)	
training:	Epoch: [26][298/817]	Loss 0.0103 (0.0485)	
training:	Epoch: [26][299/817]	Loss 0.0087 (0.0484)	
training:	Epoch: [26][300/817]	Loss 0.0084 (0.0482)	
training:	Epoch: [26][301/817]	Loss 0.0082 (0.0481)	
training:	Epoch: [26][302/817]	Loss 0.0089 (0.0480)	
training:	Epoch: [26][303/817]	Loss 0.0086 (0.0478)	
training:	Epoch: [26][304/817]	Loss 0.0086 (0.0477)	
training:	Epoch: [26][305/817]	Loss 0.0078 (0.0476)	
training:	Epoch: [26][306/817]	Loss 0.0228 (0.0475)	
training:	Epoch: [26][307/817]	Loss 0.0081 (0.0474)	
training:	Epoch: [26][308/817]	Loss 0.0084 (0.0472)	
training:	Epoch: [26][309/817]	Loss 0.0085 (0.0471)	
training:	Epoch: [26][310/817]	Loss 0.0080 (0.0470)	
training:	Epoch: [26][311/817]	Loss 0.0103 (0.0469)	
training:	Epoch: [26][312/817]	Loss 0.0098 (0.0467)	
training:	Epoch: [26][313/817]	Loss 0.0083 (0.0466)	
training:	Epoch: [26][314/817]	Loss 0.0121 (0.0465)	
training:	Epoch: [26][315/817]	Loss 0.0089 (0.0464)	
training:	Epoch: [26][316/817]	Loss 0.0091 (0.0463)	
training:	Epoch: [26][317/817]	Loss 0.5541 (0.0479)	
training:	Epoch: [26][318/817]	Loss 0.0068 (0.0477)	
training:	Epoch: [26][319/817]	Loss 0.0089 (0.0476)	
training:	Epoch: [26][320/817]	Loss 0.0088 (0.0475)	
training:	Epoch: [26][321/817]	Loss 0.0094 (0.0474)	
training:	Epoch: [26][322/817]	Loss 0.0087 (0.0473)	
training:	Epoch: [26][323/817]	Loss 0.0105 (0.0472)	
training:	Epoch: [26][324/817]	Loss 0.2231 (0.0477)	
training:	Epoch: [26][325/817]	Loss 0.0080 (0.0476)	
training:	Epoch: [26][326/817]	Loss 0.0081 (0.0475)	
training:	Epoch: [26][327/817]	Loss 0.0070 (0.0473)	
training:	Epoch: [26][328/817]	Loss 0.0075 (0.0472)	
training:	Epoch: [26][329/817]	Loss 0.0100 (0.0471)	
training:	Epoch: [26][330/817]	Loss 0.0089 (0.0470)	
training:	Epoch: [26][331/817]	Loss 0.0085 (0.0469)	
training:	Epoch: [26][332/817]	Loss 0.0091 (0.0467)	
training:	Epoch: [26][333/817]	Loss 0.0088 (0.0466)	
training:	Epoch: [26][334/817]	Loss 0.0076 (0.0465)	
training:	Epoch: [26][335/817]	Loss 0.0184 (0.0464)	
training:	Epoch: [26][336/817]	Loss 0.0094 (0.0463)	
training:	Epoch: [26][337/817]	Loss 0.0101 (0.0462)	
training:	Epoch: [26][338/817]	Loss 0.0134 (0.0461)	
training:	Epoch: [26][339/817]	Loss 0.0103 (0.0460)	
training:	Epoch: [26][340/817]	Loss 0.0090 (0.0459)	
training:	Epoch: [26][341/817]	Loss 0.0081 (0.0458)	
training:	Epoch: [26][342/817]	Loss 0.0090 (0.0457)	
training:	Epoch: [26][343/817]	Loss 0.0131 (0.0456)	
training:	Epoch: [26][344/817]	Loss 0.2348 (0.0461)	
training:	Epoch: [26][345/817]	Loss 0.0071 (0.0460)	
training:	Epoch: [26][346/817]	Loss 0.1505 (0.0463)	
training:	Epoch: [26][347/817]	Loss 0.0104 (0.0462)	
training:	Epoch: [26][348/817]	Loss 0.0085 (0.0461)	
training:	Epoch: [26][349/817]	Loss 0.0084 (0.0460)	
training:	Epoch: [26][350/817]	Loss 0.0091 (0.0459)	
training:	Epoch: [26][351/817]	Loss 0.0082 (0.0458)	
training:	Epoch: [26][352/817]	Loss 0.0090 (0.0457)	
training:	Epoch: [26][353/817]	Loss 0.0115 (0.0456)	
training:	Epoch: [26][354/817]	Loss 0.0081 (0.0455)	
training:	Epoch: [26][355/817]	Loss 0.0075 (0.0454)	
training:	Epoch: [26][356/817]	Loss 0.0087 (0.0453)	
training:	Epoch: [26][357/817]	Loss 0.0124 (0.0452)	
training:	Epoch: [26][358/817]	Loss 0.0103 (0.0451)	
training:	Epoch: [26][359/817]	Loss 0.0070 (0.0450)	
training:	Epoch: [26][360/817]	Loss 0.0278 (0.0449)	
training:	Epoch: [26][361/817]	Loss 0.0100 (0.0448)	
training:	Epoch: [26][362/817]	Loss 0.5345 (0.0462)	
training:	Epoch: [26][363/817]	Loss 0.0075 (0.0461)	
training:	Epoch: [26][364/817]	Loss 0.0075 (0.0460)	
training:	Epoch: [26][365/817]	Loss 0.0068 (0.0459)	
training:	Epoch: [26][366/817]	Loss 0.0145 (0.0458)	
training:	Epoch: [26][367/817]	Loss 0.0675 (0.0458)	
training:	Epoch: [26][368/817]	Loss 0.0092 (0.0457)	
training:	Epoch: [26][369/817]	Loss 0.0623 (0.0458)	
training:	Epoch: [26][370/817]	Loss 0.0125 (0.0457)	
training:	Epoch: [26][371/817]	Loss 0.0078 (0.0456)	
training:	Epoch: [26][372/817]	Loss 0.0073 (0.0455)	
training:	Epoch: [26][373/817]	Loss 0.0083 (0.0454)	
training:	Epoch: [26][374/817]	Loss 0.0291 (0.0454)	
training:	Epoch: [26][375/817]	Loss 0.0076 (0.0453)	
training:	Epoch: [26][376/817]	Loss 0.0111 (0.0452)	
training:	Epoch: [26][377/817]	Loss 0.0082 (0.0451)	
training:	Epoch: [26][378/817]	Loss 0.0081 (0.0450)	
training:	Epoch: [26][379/817]	Loss 0.0087 (0.0449)	
training:	Epoch: [26][380/817]	Loss 0.0075 (0.0448)	
training:	Epoch: [26][381/817]	Loss 0.0103 (0.0447)	
training:	Epoch: [26][382/817]	Loss 0.0088 (0.0446)	
training:	Epoch: [26][383/817]	Loss 0.0087 (0.0445)	
training:	Epoch: [26][384/817]	Loss 0.0076 (0.0444)	
training:	Epoch: [26][385/817]	Loss 0.0074 (0.0443)	
training:	Epoch: [26][386/817]	Loss 0.0073 (0.0442)	
training:	Epoch: [26][387/817]	Loss 0.0085 (0.0441)	
training:	Epoch: [26][388/817]	Loss 0.0101 (0.0440)	
training:	Epoch: [26][389/817]	Loss 0.0082 (0.0439)	
training:	Epoch: [26][390/817]	Loss 0.0086 (0.0438)	
training:	Epoch: [26][391/817]	Loss 0.0192 (0.0438)	
training:	Epoch: [26][392/817]	Loss 0.0084 (0.0437)	
training:	Epoch: [26][393/817]	Loss 0.0081 (0.0436)	
training:	Epoch: [26][394/817]	Loss 0.0079 (0.0435)	
training:	Epoch: [26][395/817]	Loss 0.0105 (0.0434)	
training:	Epoch: [26][396/817]	Loss 0.0118 (0.0433)	
training:	Epoch: [26][397/817]	Loss 0.0094 (0.0433)	
training:	Epoch: [26][398/817]	Loss 0.0098 (0.0432)	
training:	Epoch: [26][399/817]	Loss 0.0089 (0.0431)	
training:	Epoch: [26][400/817]	Loss 0.0081 (0.0430)	
training:	Epoch: [26][401/817]	Loss 0.0070 (0.0429)	
training:	Epoch: [26][402/817]	Loss 0.0084 (0.0428)	
training:	Epoch: [26][403/817]	Loss 0.0102 (0.0427)	
training:	Epoch: [26][404/817]	Loss 0.0078 (0.0427)	
training:	Epoch: [26][405/817]	Loss 0.0083 (0.0426)	
training:	Epoch: [26][406/817]	Loss 0.4229 (0.0435)	
training:	Epoch: [26][407/817]	Loss 0.0094 (0.0434)	
training:	Epoch: [26][408/817]	Loss 0.0082 (0.0433)	
training:	Epoch: [26][409/817]	Loss 0.4716 (0.0444)	
training:	Epoch: [26][410/817]	Loss 0.0094 (0.0443)	
training:	Epoch: [26][411/817]	Loss 0.0114 (0.0442)	
training:	Epoch: [26][412/817]	Loss 0.0089 (0.0441)	
training:	Epoch: [26][413/817]	Loss 0.0103 (0.0441)	
training:	Epoch: [26][414/817]	Loss 0.0088 (0.0440)	
training:	Epoch: [26][415/817]	Loss 0.3686 (0.0447)	
training:	Epoch: [26][416/817]	Loss 0.0079 (0.0447)	
training:	Epoch: [26][417/817]	Loss 0.0076 (0.0446)	
training:	Epoch: [26][418/817]	Loss 0.0087 (0.0445)	
training:	Epoch: [26][419/817]	Loss 0.6286 (0.0459)	
training:	Epoch: [26][420/817]	Loss 0.0091 (0.0458)	
training:	Epoch: [26][421/817]	Loss 0.2165 (0.0462)	
training:	Epoch: [26][422/817]	Loss 0.0079 (0.0461)	
training:	Epoch: [26][423/817]	Loss 0.0083 (0.0460)	
training:	Epoch: [26][424/817]	Loss 0.0093 (0.0459)	
training:	Epoch: [26][425/817]	Loss 0.0088 (0.0458)	
training:	Epoch: [26][426/817]	Loss 0.0075 (0.0458)	
training:	Epoch: [26][427/817]	Loss 0.0075 (0.0457)	
training:	Epoch: [26][428/817]	Loss 0.0102 (0.0456)	
training:	Epoch: [26][429/817]	Loss 0.5631 (0.0468)	
training:	Epoch: [26][430/817]	Loss 0.0287 (0.0467)	
training:	Epoch: [26][431/817]	Loss 0.0235 (0.0467)	
training:	Epoch: [26][432/817]	Loss 0.6293 (0.0480)	
training:	Epoch: [26][433/817]	Loss 0.0104 (0.0480)	
training:	Epoch: [26][434/817]	Loss 0.0114 (0.0479)	
training:	Epoch: [26][435/817]	Loss 0.0108 (0.0478)	
training:	Epoch: [26][436/817]	Loss 0.0130 (0.0477)	
training:	Epoch: [26][437/817]	Loss 0.0099 (0.0476)	
training:	Epoch: [26][438/817]	Loss 0.0082 (0.0475)	
training:	Epoch: [26][439/817]	Loss 0.0093 (0.0474)	
training:	Epoch: [26][440/817]	Loss 0.0074 (0.0474)	
training:	Epoch: [26][441/817]	Loss 0.0082 (0.0473)	
training:	Epoch: [26][442/817]	Loss 0.0066 (0.0472)	
training:	Epoch: [26][443/817]	Loss 0.0273 (0.0471)	
training:	Epoch: [26][444/817]	Loss 0.0108 (0.0470)	
training:	Epoch: [26][445/817]	Loss 0.0105 (0.0470)	
training:	Epoch: [26][446/817]	Loss 0.0112 (0.0469)	
training:	Epoch: [26][447/817]	Loss 0.0083 (0.0468)	
training:	Epoch: [26][448/817]	Loss 0.0077 (0.0467)	
training:	Epoch: [26][449/817]	Loss 0.6262 (0.0480)	
training:	Epoch: [26][450/817]	Loss 0.0080 (0.0479)	
training:	Epoch: [26][451/817]	Loss 0.0112 (0.0478)	
training:	Epoch: [26][452/817]	Loss 0.0108 (0.0477)	
training:	Epoch: [26][453/817]	Loss 0.0081 (0.0477)	
training:	Epoch: [26][454/817]	Loss 0.0088 (0.0476)	
training:	Epoch: [26][455/817]	Loss 0.0093 (0.0475)	
training:	Epoch: [26][456/817]	Loss 0.0163 (0.0474)	
training:	Epoch: [26][457/817]	Loss 0.0115 (0.0473)	
training:	Epoch: [26][458/817]	Loss 0.0072 (0.0473)	
training:	Epoch: [26][459/817]	Loss 0.0091 (0.0472)	
training:	Epoch: [26][460/817]	Loss 0.0123 (0.0471)	
training:	Epoch: [26][461/817]	Loss 0.0089 (0.0470)	
training:	Epoch: [26][462/817]	Loss 0.0078 (0.0469)	
training:	Epoch: [26][463/817]	Loss 0.0219 (0.0469)	
training:	Epoch: [26][464/817]	Loss 0.0103 (0.0468)	
training:	Epoch: [26][465/817]	Loss 0.0075 (0.0467)	
training:	Epoch: [26][466/817]	Loss 0.0091 (0.0466)	
training:	Epoch: [26][467/817]	Loss 0.0111 (0.0466)	
training:	Epoch: [26][468/817]	Loss 0.0112 (0.0465)	
training:	Epoch: [26][469/817]	Loss 0.0065 (0.0464)	
training:	Epoch: [26][470/817]	Loss 0.0084 (0.0463)	
training:	Epoch: [26][471/817]	Loss 0.0091 (0.0462)	
training:	Epoch: [26][472/817]	Loss 0.0153 (0.0462)	
training:	Epoch: [26][473/817]	Loss 0.0080 (0.0461)	
training:	Epoch: [26][474/817]	Loss 0.0091 (0.0460)	
training:	Epoch: [26][475/817]	Loss 0.0079 (0.0459)	
training:	Epoch: [26][476/817]	Loss 0.1130 (0.0461)	
training:	Epoch: [26][477/817]	Loss 0.0082 (0.0460)	
training:	Epoch: [26][478/817]	Loss 0.0091 (0.0459)	
training:	Epoch: [26][479/817]	Loss 0.0094 (0.0458)	
training:	Epoch: [26][480/817]	Loss 0.0091 (0.0458)	
training:	Epoch: [26][481/817]	Loss 0.0284 (0.0457)	
training:	Epoch: [26][482/817]	Loss 0.0088 (0.0456)	
training:	Epoch: [26][483/817]	Loss 0.0088 (0.0456)	
training:	Epoch: [26][484/817]	Loss 0.0094 (0.0455)	
training:	Epoch: [26][485/817]	Loss 0.0078 (0.0454)	
training:	Epoch: [26][486/817]	Loss 0.0084 (0.0453)	
training:	Epoch: [26][487/817]	Loss 0.0086 (0.0453)	
training:	Epoch: [26][488/817]	Loss 0.0087 (0.0452)	
training:	Epoch: [26][489/817]	Loss 0.0116 (0.0451)	
training:	Epoch: [26][490/817]	Loss 0.0167 (0.0451)	
training:	Epoch: [26][491/817]	Loss 0.0081 (0.0450)	
training:	Epoch: [26][492/817]	Loss 0.0073 (0.0449)	
training:	Epoch: [26][493/817]	Loss 0.0134 (0.0448)	
training:	Epoch: [26][494/817]	Loss 0.0070 (0.0448)	
training:	Epoch: [26][495/817]	Loss 0.0091 (0.0447)	
training:	Epoch: [26][496/817]	Loss 0.0540 (0.0447)	
training:	Epoch: [26][497/817]	Loss 0.0088 (0.0446)	
training:	Epoch: [26][498/817]	Loss 0.0084 (0.0446)	
training:	Epoch: [26][499/817]	Loss 0.4724 (0.0454)	
training:	Epoch: [26][500/817]	Loss 0.0083 (0.0454)	
training:	Epoch: [26][501/817]	Loss 0.0134 (0.0453)	
training:	Epoch: [26][502/817]	Loss 0.0364 (0.0453)	
training:	Epoch: [26][503/817]	Loss 0.5583 (0.0463)	
training:	Epoch: [26][504/817]	Loss 0.0098 (0.0462)	
training:	Epoch: [26][505/817]	Loss 0.0071 (0.0461)	
training:	Epoch: [26][506/817]	Loss 0.0083 (0.0461)	
training:	Epoch: [26][507/817]	Loss 0.0095 (0.0460)	
training:	Epoch: [26][508/817]	Loss 0.0090 (0.0459)	
training:	Epoch: [26][509/817]	Loss 0.0081 (0.0459)	
training:	Epoch: [26][510/817]	Loss 0.0097 (0.0458)	
training:	Epoch: [26][511/817]	Loss 0.0082 (0.0457)	
training:	Epoch: [26][512/817]	Loss 0.0083 (0.0456)	
training:	Epoch: [26][513/817]	Loss 0.0087 (0.0456)	
training:	Epoch: [26][514/817]	Loss 0.0096 (0.0455)	
training:	Epoch: [26][515/817]	Loss 0.0090 (0.0454)	
training:	Epoch: [26][516/817]	Loss 0.0083 (0.0453)	
training:	Epoch: [26][517/817]	Loss 0.0091 (0.0453)	
training:	Epoch: [26][518/817]	Loss 0.0082 (0.0452)	
training:	Epoch: [26][519/817]	Loss 0.0081 (0.0451)	
training:	Epoch: [26][520/817]	Loss 0.0105 (0.0451)	
training:	Epoch: [26][521/817]	Loss 0.0113 (0.0450)	
training:	Epoch: [26][522/817]	Loss 0.0083 (0.0449)	
training:	Epoch: [26][523/817]	Loss 0.0087 (0.0449)	
training:	Epoch: [26][524/817]	Loss 0.0078 (0.0448)	
training:	Epoch: [26][525/817]	Loss 0.0120 (0.0447)	
training:	Epoch: [26][526/817]	Loss 0.0282 (0.0447)	
training:	Epoch: [26][527/817]	Loss 0.0095 (0.0446)	
training:	Epoch: [26][528/817]	Loss 0.0093 (0.0446)	
training:	Epoch: [26][529/817]	Loss 0.0085 (0.0445)	
training:	Epoch: [26][530/817]	Loss 0.0116 (0.0444)	
training:	Epoch: [26][531/817]	Loss 0.0083 (0.0444)	
training:	Epoch: [26][532/817]	Loss 0.6481 (0.0455)	
training:	Epoch: [26][533/817]	Loss 0.4307 (0.0462)	
training:	Epoch: [26][534/817]	Loss 0.0100 (0.0462)	
training:	Epoch: [26][535/817]	Loss 0.0079 (0.0461)	
training:	Epoch: [26][536/817]	Loss 0.0084 (0.0460)	
training:	Epoch: [26][537/817]	Loss 0.0087 (0.0459)	
training:	Epoch: [26][538/817]	Loss 0.0080 (0.0459)	
training:	Epoch: [26][539/817]	Loss 0.0069 (0.0458)	
training:	Epoch: [26][540/817]	Loss 0.0084 (0.0457)	
training:	Epoch: [26][541/817]	Loss 0.0076 (0.0457)	
training:	Epoch: [26][542/817]	Loss 0.0077 (0.0456)	
training:	Epoch: [26][543/817]	Loss 0.5221 (0.0465)	
training:	Epoch: [26][544/817]	Loss 0.1096 (0.0466)	
training:	Epoch: [26][545/817]	Loss 0.6340 (0.0477)	
training:	Epoch: [26][546/817]	Loss 0.0079 (0.0476)	
training:	Epoch: [26][547/817]	Loss 0.0080 (0.0475)	
training:	Epoch: [26][548/817]	Loss 0.6138 (0.0486)	
training:	Epoch: [26][549/817]	Loss 0.0085 (0.0485)	
training:	Epoch: [26][550/817]	Loss 0.0076 (0.0484)	
training:	Epoch: [26][551/817]	Loss 0.0132 (0.0483)	
training:	Epoch: [26][552/817]	Loss 0.0099 (0.0483)	
training:	Epoch: [26][553/817]	Loss 0.0087 (0.0482)	
training:	Epoch: [26][554/817]	Loss 0.0090 (0.0481)	
training:	Epoch: [26][555/817]	Loss 0.0635 (0.0482)	
training:	Epoch: [26][556/817]	Loss 0.0086 (0.0481)	
training:	Epoch: [26][557/817]	Loss 0.0217 (0.0480)	
training:	Epoch: [26][558/817]	Loss 0.5633 (0.0490)	
training:	Epoch: [26][559/817]	Loss 0.0081 (0.0489)	
training:	Epoch: [26][560/817]	Loss 0.0071 (0.0488)	
training:	Epoch: [26][561/817]	Loss 0.0227 (0.0488)	
training:	Epoch: [26][562/817]	Loss 0.0073 (0.0487)	
training:	Epoch: [26][563/817]	Loss 0.0092 (0.0486)	
training:	Epoch: [26][564/817]	Loss 0.0097 (0.0486)	
training:	Epoch: [26][565/817]	Loss 0.0097 (0.0485)	
training:	Epoch: [26][566/817]	Loss 0.6060 (0.0495)	
training:	Epoch: [26][567/817]	Loss 0.0094 (0.0494)	
training:	Epoch: [26][568/817]	Loss 0.0100 (0.0493)	
training:	Epoch: [26][569/817]	Loss 0.0086 (0.0493)	
training:	Epoch: [26][570/817]	Loss 0.0087 (0.0492)	
training:	Epoch: [26][571/817]	Loss 0.0070 (0.0491)	
training:	Epoch: [26][572/817]	Loss 0.0080 (0.0490)	
training:	Epoch: [26][573/817]	Loss 0.0089 (0.0490)	
training:	Epoch: [26][574/817]	Loss 0.0097 (0.0489)	
training:	Epoch: [26][575/817]	Loss 0.0077 (0.0488)	
training:	Epoch: [26][576/817]	Loss 0.2631 (0.0492)	
training:	Epoch: [26][577/817]	Loss 0.0077 (0.0491)	
training:	Epoch: [26][578/817]	Loss 0.0078 (0.0491)	
training:	Epoch: [26][579/817]	Loss 0.0098 (0.0490)	
training:	Epoch: [26][580/817]	Loss 0.0075 (0.0489)	
training:	Epoch: [26][581/817]	Loss 0.0084 (0.0489)	
training:	Epoch: [26][582/817]	Loss 0.0084 (0.0488)	
training:	Epoch: [26][583/817]	Loss 0.0091 (0.0487)	
training:	Epoch: [26][584/817]	Loss 0.0086 (0.0486)	
training:	Epoch: [26][585/817]	Loss 0.0070 (0.0486)	
training:	Epoch: [26][586/817]	Loss 0.0102 (0.0485)	
training:	Epoch: [26][587/817]	Loss 0.0084 (0.0484)	
training:	Epoch: [26][588/817]	Loss 0.0083 (0.0484)	
training:	Epoch: [26][589/817]	Loss 0.0083 (0.0483)	
training:	Epoch: [26][590/817]	Loss 0.0082 (0.0482)	
training:	Epoch: [26][591/817]	Loss 0.0152 (0.0482)	
training:	Epoch: [26][592/817]	Loss 0.0078 (0.0481)	
training:	Epoch: [26][593/817]	Loss 0.0131 (0.0481)	
training:	Epoch: [26][594/817]	Loss 0.0087 (0.0480)	
training:	Epoch: [26][595/817]	Loss 0.0095 (0.0479)	
training:	Epoch: [26][596/817]	Loss 0.0084 (0.0479)	
training:	Epoch: [26][597/817]	Loss 0.2212 (0.0481)	
training:	Epoch: [26][598/817]	Loss 0.0082 (0.0481)	
training:	Epoch: [26][599/817]	Loss 0.0077 (0.0480)	
training:	Epoch: [26][600/817]	Loss 0.0108 (0.0480)	
training:	Epoch: [26][601/817]	Loss 0.0111 (0.0479)	
training:	Epoch: [26][602/817]	Loss 0.0072 (0.0478)	
training:	Epoch: [26][603/817]	Loss 0.0087 (0.0478)	
training:	Epoch: [26][604/817]	Loss 0.0080 (0.0477)	
training:	Epoch: [26][605/817]	Loss 0.0082 (0.0476)	
training:	Epoch: [26][606/817]	Loss 0.0191 (0.0476)	
training:	Epoch: [26][607/817]	Loss 0.0093 (0.0475)	
training:	Epoch: [26][608/817]	Loss 0.0083 (0.0475)	
training:	Epoch: [26][609/817]	Loss 0.0080 (0.0474)	
training:	Epoch: [26][610/817]	Loss 0.0116 (0.0473)	
training:	Epoch: [26][611/817]	Loss 0.0097 (0.0473)	
training:	Epoch: [26][612/817]	Loss 0.0075 (0.0472)	
training:	Epoch: [26][613/817]	Loss 0.0076 (0.0471)	
training:	Epoch: [26][614/817]	Loss 0.0073 (0.0471)	
training:	Epoch: [26][615/817]	Loss 0.3207 (0.0475)	
training:	Epoch: [26][616/817]	Loss 0.0082 (0.0475)	
training:	Epoch: [26][617/817]	Loss 0.0066 (0.0474)	
training:	Epoch: [26][618/817]	Loss 0.0064 (0.0473)	
training:	Epoch: [26][619/817]	Loss 0.0090 (0.0473)	
training:	Epoch: [26][620/817]	Loss 0.0072 (0.0472)	
training:	Epoch: [26][621/817]	Loss 0.0072 (0.0471)	
training:	Epoch: [26][622/817]	Loss 0.0070 (0.0471)	
training:	Epoch: [26][623/817]	Loss 0.0084 (0.0470)	
training:	Epoch: [26][624/817]	Loss 0.6186 (0.0479)	
training:	Epoch: [26][625/817]	Loss 0.0073 (0.0479)	
training:	Epoch: [26][626/817]	Loss 0.0095 (0.0478)	
training:	Epoch: [26][627/817]	Loss 0.0091 (0.0477)	
training:	Epoch: [26][628/817]	Loss 0.0081 (0.0477)	
training:	Epoch: [26][629/817]	Loss 0.0093 (0.0476)	
training:	Epoch: [26][630/817]	Loss 0.0065 (0.0475)	
training:	Epoch: [26][631/817]	Loss 0.0102 (0.0475)	
training:	Epoch: [26][632/817]	Loss 0.0096 (0.0474)	
training:	Epoch: [26][633/817]	Loss 0.0085 (0.0474)	
training:	Epoch: [26][634/817]	Loss 0.0078 (0.0473)	
training:	Epoch: [26][635/817]	Loss 0.0083 (0.0472)	
training:	Epoch: [26][636/817]	Loss 0.0082 (0.0472)	
training:	Epoch: [26][637/817]	Loss 0.1159 (0.0473)	
training:	Epoch: [26][638/817]	Loss 0.0090 (0.0472)	
training:	Epoch: [26][639/817]	Loss 0.0088 (0.0472)	
training:	Epoch: [26][640/817]	Loss 0.0081 (0.0471)	
training:	Epoch: [26][641/817]	Loss 0.0071 (0.0470)	
training:	Epoch: [26][642/817]	Loss 0.0090 (0.0470)	
training:	Epoch: [26][643/817]	Loss 0.0136 (0.0469)	
training:	Epoch: [26][644/817]	Loss 0.0086 (0.0469)	
training:	Epoch: [26][645/817]	Loss 0.0204 (0.0468)	
training:	Epoch: [26][646/817]	Loss 0.0102 (0.0468)	
training:	Epoch: [26][647/817]	Loss 0.0072 (0.0467)	
training:	Epoch: [26][648/817]	Loss 0.0077 (0.0467)	
training:	Epoch: [26][649/817]	Loss 0.0082 (0.0466)	
training:	Epoch: [26][650/817]	Loss 0.0106 (0.0465)	
training:	Epoch: [26][651/817]	Loss 0.0089 (0.0465)	
training:	Epoch: [26][652/817]	Loss 0.0082 (0.0464)	
training:	Epoch: [26][653/817]	Loss 0.0102 (0.0464)	
training:	Epoch: [26][654/817]	Loss 0.0090 (0.0463)	
training:	Epoch: [26][655/817]	Loss 0.0081 (0.0462)	
training:	Epoch: [26][656/817]	Loss 0.0074 (0.0462)	
training:	Epoch: [26][657/817]	Loss 0.0090 (0.0461)	
training:	Epoch: [26][658/817]	Loss 0.0087 (0.0461)	
training:	Epoch: [26][659/817]	Loss 0.0093 (0.0460)	
training:	Epoch: [26][660/817]	Loss 0.0089 (0.0460)	
training:	Epoch: [26][661/817]	Loss 0.0071 (0.0459)	
training:	Epoch: [26][662/817]	Loss 0.0067 (0.0458)	
training:	Epoch: [26][663/817]	Loss 0.0072 (0.0458)	
training:	Epoch: [26][664/817]	Loss 0.0069 (0.0457)	
training:	Epoch: [26][665/817]	Loss 0.0084 (0.0457)	
training:	Epoch: [26][666/817]	Loss 0.0076 (0.0456)	
training:	Epoch: [26][667/817]	Loss 0.0102 (0.0456)	
training:	Epoch: [26][668/817]	Loss 0.0071 (0.0455)	
training:	Epoch: [26][669/817]	Loss 0.0071 (0.0454)	
training:	Epoch: [26][670/817]	Loss 0.0102 (0.0454)	
training:	Epoch: [26][671/817]	Loss 0.0073 (0.0453)	
training:	Epoch: [26][672/817]	Loss 0.0078 (0.0453)	
training:	Epoch: [26][673/817]	Loss 0.0394 (0.0453)	
training:	Epoch: [26][674/817]	Loss 0.0115 (0.0452)	
training:	Epoch: [26][675/817]	Loss 0.0078 (0.0452)	
training:	Epoch: [26][676/817]	Loss 0.0150 (0.0451)	
training:	Epoch: [26][677/817]	Loss 0.0086 (0.0451)	
training:	Epoch: [26][678/817]	Loss 0.0091 (0.0450)	
training:	Epoch: [26][679/817]	Loss 0.0072 (0.0450)	
training:	Epoch: [26][680/817]	Loss 0.0083 (0.0449)	
training:	Epoch: [26][681/817]	Loss 0.0139 (0.0449)	
training:	Epoch: [26][682/817]	Loss 0.0082 (0.0448)	
training:	Epoch: [26][683/817]	Loss 0.0080 (0.0448)	
training:	Epoch: [26][684/817]	Loss 0.0278 (0.0447)	
training:	Epoch: [26][685/817]	Loss 0.0079 (0.0447)	
training:	Epoch: [26][686/817]	Loss 0.0081 (0.0446)	
training:	Epoch: [26][687/817]	Loss 0.0171 (0.0446)	
training:	Epoch: [26][688/817]	Loss 0.0091 (0.0445)	
training:	Epoch: [26][689/817]	Loss 0.0076 (0.0445)	
training:	Epoch: [26][690/817]	Loss 0.2573 (0.0448)	
training:	Epoch: [26][691/817]	Loss 0.0076 (0.0447)	
training:	Epoch: [26][692/817]	Loss 0.0103 (0.0447)	
training:	Epoch: [26][693/817]	Loss 0.0070 (0.0446)	
training:	Epoch: [26][694/817]	Loss 0.0134 (0.0446)	
training:	Epoch: [26][695/817]	Loss 0.0077 (0.0445)	
training:	Epoch: [26][696/817]	Loss 0.0090 (0.0445)	
training:	Epoch: [26][697/817]	Loss 0.0073 (0.0444)	
training:	Epoch: [26][698/817]	Loss 0.0066 (0.0444)	
training:	Epoch: [26][699/817]	Loss 0.0075 (0.0443)	
training:	Epoch: [26][700/817]	Loss 0.0074 (0.0443)	
training:	Epoch: [26][701/817]	Loss 0.0069 (0.0442)	
training:	Epoch: [26][702/817]	Loss 0.6242 (0.0450)	
training:	Epoch: [26][703/817]	Loss 0.0079 (0.0450)	
training:	Epoch: [26][704/817]	Loss 0.0083 (0.0449)	
training:	Epoch: [26][705/817]	Loss 0.0452 (0.0449)	
training:	Epoch: [26][706/817]	Loss 0.0084 (0.0449)	
training:	Epoch: [26][707/817]	Loss 0.0080 (0.0448)	
training:	Epoch: [26][708/817]	Loss 0.0079 (0.0448)	
training:	Epoch: [26][709/817]	Loss 0.2340 (0.0450)	
training:	Epoch: [26][710/817]	Loss 0.0105 (0.0450)	
training:	Epoch: [26][711/817]	Loss 0.0085 (0.0449)	
training:	Epoch: [26][712/817]	Loss 0.0075 (0.0449)	
training:	Epoch: [26][713/817]	Loss 0.0081 (0.0448)	
training:	Epoch: [26][714/817]	Loss 0.0089 (0.0448)	
training:	Epoch: [26][715/817]	Loss 0.0086 (0.0447)	
training:	Epoch: [26][716/817]	Loss 0.0074 (0.0447)	
training:	Epoch: [26][717/817]	Loss 0.0073 (0.0446)	
training:	Epoch: [26][718/817]	Loss 0.0095 (0.0446)	
training:	Epoch: [26][719/817]	Loss 0.2065 (0.0448)	
training:	Epoch: [26][720/817]	Loss 0.0097 (0.0448)	
training:	Epoch: [26][721/817]	Loss 0.0062 (0.0447)	
training:	Epoch: [26][722/817]	Loss 0.0088 (0.0447)	
training:	Epoch: [26][723/817]	Loss 0.0123 (0.0446)	
training:	Epoch: [26][724/817]	Loss 0.0179 (0.0446)	
training:	Epoch: [26][725/817]	Loss 0.0083 (0.0445)	
training:	Epoch: [26][726/817]	Loss 0.0114 (0.0445)	
training:	Epoch: [26][727/817]	Loss 0.0068 (0.0444)	
training:	Epoch: [26][728/817]	Loss 0.0071 (0.0444)	
training:	Epoch: [26][729/817]	Loss 0.0103 (0.0443)	
training:	Epoch: [26][730/817]	Loss 0.0069 (0.0443)	
training:	Epoch: [26][731/817]	Loss 0.0071 (0.0442)	
training:	Epoch: [26][732/817]	Loss 0.6358 (0.0450)	
training:	Epoch: [26][733/817]	Loss 0.0119 (0.0450)	
training:	Epoch: [26][734/817]	Loss 0.0080 (0.0449)	
training:	Epoch: [26][735/817]	Loss 0.0089 (0.0449)	
training:	Epoch: [26][736/817]	Loss 0.0062 (0.0448)	
training:	Epoch: [26][737/817]	Loss 0.0086 (0.0448)	
training:	Epoch: [26][738/817]	Loss 0.0069 (0.0447)	
training:	Epoch: [26][739/817]	Loss 0.0066 (0.0447)	
training:	Epoch: [26][740/817]	Loss 0.0075 (0.0446)	
training:	Epoch: [26][741/817]	Loss 0.0090 (0.0446)	
training:	Epoch: [26][742/817]	Loss 0.0120 (0.0445)	
training:	Epoch: [26][743/817]	Loss 0.0132 (0.0445)	
training:	Epoch: [26][744/817]	Loss 0.0088 (0.0445)	
training:	Epoch: [26][745/817]	Loss 0.0110 (0.0444)	
training:	Epoch: [26][746/817]	Loss 0.0076 (0.0444)	
training:	Epoch: [26][747/817]	Loss 0.0067 (0.0443)	
training:	Epoch: [26][748/817]	Loss 0.6215 (0.0451)	
training:	Epoch: [26][749/817]	Loss 0.0073 (0.0450)	
training:	Epoch: [26][750/817]	Loss 0.0142 (0.0450)	
training:	Epoch: [26][751/817]	Loss 0.0073 (0.0449)	
training:	Epoch: [26][752/817]	Loss 0.1724 (0.0451)	
training:	Epoch: [26][753/817]	Loss 0.0079 (0.0451)	
training:	Epoch: [26][754/817]	Loss 0.0074 (0.0450)	
training:	Epoch: [26][755/817]	Loss 0.0098 (0.0450)	
training:	Epoch: [26][756/817]	Loss 0.0078 (0.0449)	
training:	Epoch: [26][757/817]	Loss 0.0068 (0.0449)	
training:	Epoch: [26][758/817]	Loss 0.0071 (0.0448)	
training:	Epoch: [26][759/817]	Loss 0.0075 (0.0448)	
training:	Epoch: [26][760/817]	Loss 0.0090 (0.0447)	
training:	Epoch: [26][761/817]	Loss 0.0074 (0.0447)	
training:	Epoch: [26][762/817]	Loss 0.0076 (0.0446)	
training:	Epoch: [26][763/817]	Loss 0.6457 (0.0454)	
training:	Epoch: [26][764/817]	Loss 0.0099 (0.0454)	
training:	Epoch: [26][765/817]	Loss 0.0071 (0.0453)	
training:	Epoch: [26][766/817]	Loss 0.0071 (0.0453)	
training:	Epoch: [26][767/817]	Loss 0.0083 (0.0452)	
training:	Epoch: [26][768/817]	Loss 0.0093 (0.0452)	
training:	Epoch: [26][769/817]	Loss 0.0080 (0.0451)	
training:	Epoch: [26][770/817]	Loss 0.1109 (0.0452)	
training:	Epoch: [26][771/817]	Loss 0.0068 (0.0452)	
training:	Epoch: [26][772/817]	Loss 0.0078 (0.0451)	
training:	Epoch: [26][773/817]	Loss 0.0072 (0.0451)	
training:	Epoch: [26][774/817]	Loss 0.0074 (0.0450)	
training:	Epoch: [26][775/817]	Loss 0.0068 (0.0450)	
training:	Epoch: [26][776/817]	Loss 0.0072 (0.0449)	
training:	Epoch: [26][777/817]	Loss 0.0075 (0.0449)	
training:	Epoch: [26][778/817]	Loss 0.0072 (0.0448)	
training:	Epoch: [26][779/817]	Loss 0.0362 (0.0448)	
training:	Epoch: [26][780/817]	Loss 0.0079 (0.0448)	
training:	Epoch: [26][781/817]	Loss 0.0069 (0.0447)	
training:	Epoch: [26][782/817]	Loss 0.0070 (0.0447)	
training:	Epoch: [26][783/817]	Loss 0.0094 (0.0446)	
training:	Epoch: [26][784/817]	Loss 0.0171 (0.0446)	
training:	Epoch: [26][785/817]	Loss 0.0070 (0.0445)	
training:	Epoch: [26][786/817]	Loss 0.0128 (0.0445)	
training:	Epoch: [26][787/817]	Loss 0.0077 (0.0444)	
training:	Epoch: [26][788/817]	Loss 0.6022 (0.0452)	
training:	Epoch: [26][789/817]	Loss 0.0075 (0.0451)	
training:	Epoch: [26][790/817]	Loss 0.4933 (0.0457)	
training:	Epoch: [26][791/817]	Loss 0.6028 (0.0464)	
training:	Epoch: [26][792/817]	Loss 0.0065 (0.0463)	
training:	Epoch: [26][793/817]	Loss 0.0077 (0.0463)	
training:	Epoch: [26][794/817]	Loss 0.0077 (0.0462)	
training:	Epoch: [26][795/817]	Loss 0.0066 (0.0462)	
training:	Epoch: [26][796/817]	Loss 0.0084 (0.0461)	
training:	Epoch: [26][797/817]	Loss 0.1008 (0.0462)	
training:	Epoch: [26][798/817]	Loss 0.0078 (0.0462)	
training:	Epoch: [26][799/817]	Loss 0.0071 (0.0461)	
training:	Epoch: [26][800/817]	Loss 0.0079 (0.0461)	
training:	Epoch: [26][801/817]	Loss 0.0083 (0.0460)	
training:	Epoch: [26][802/817]	Loss 0.0080 (0.0460)	
training:	Epoch: [26][803/817]	Loss 0.0070 (0.0459)	
training:	Epoch: [26][804/817]	Loss 0.0085 (0.0459)	
training:	Epoch: [26][805/817]	Loss 0.0092 (0.0458)	
training:	Epoch: [26][806/817]	Loss 0.0077 (0.0458)	
training:	Epoch: [26][807/817]	Loss 0.0128 (0.0457)	
training:	Epoch: [26][808/817]	Loss 0.0069 (0.0457)	
training:	Epoch: [26][809/817]	Loss 0.0072 (0.0456)	
training:	Epoch: [26][810/817]	Loss 0.0087 (0.0456)	
training:	Epoch: [26][811/817]	Loss 0.1262 (0.0457)	
training:	Epoch: [26][812/817]	Loss 0.0087 (0.0456)	
training:	Epoch: [26][813/817]	Loss 0.0075 (0.0456)	
training:	Epoch: [26][814/817]	Loss 0.0066 (0.0455)	
training:	Epoch: [26][815/817]	Loss 0.0086 (0.0455)	
training:	Epoch: [26][816/817]	Loss 0.0077 (0.0455)	
training:	Epoch: [26][817/817]	Loss 0.0079 (0.0454)	
Training:	 Loss: 0.0454

Training:	 ACC: 0.9920 0.9920 0.9932 0.9908
Validation:	 ACC: 0.7919 0.7935 0.8270 0.7567
Validation:	 Best_BACC: 0.8223 0.8218 0.8127 0.8318
Validation:	 Loss: 0.8809
Pretraining:	Epoch 27/200
----------
training:	Epoch: [27][1/817]	Loss 0.6154 (0.6154)	
training:	Epoch: [27][2/817]	Loss 0.0079 (0.3116)	
training:	Epoch: [27][3/817]	Loss 0.0269 (0.2167)	
training:	Epoch: [27][4/817]	Loss 0.0065 (0.1642)	
training:	Epoch: [27][5/817]	Loss 0.0096 (0.1332)	
training:	Epoch: [27][6/817]	Loss 0.0079 (0.1123)	
training:	Epoch: [27][7/817]	Loss 0.0082 (0.0975)	
training:	Epoch: [27][8/817]	Loss 0.0871 (0.0962)	
training:	Epoch: [27][9/817]	Loss 0.0260 (0.0884)	
training:	Epoch: [27][10/817]	Loss 0.0081 (0.0803)	
training:	Epoch: [27][11/817]	Loss 0.0072 (0.0737)	
training:	Epoch: [27][12/817]	Loss 0.0085 (0.0683)	
training:	Epoch: [27][13/817]	Loss 0.0078 (0.0636)	
training:	Epoch: [27][14/817]	Loss 0.0067 (0.0596)	
training:	Epoch: [27][15/817]	Loss 0.0098 (0.0562)	
training:	Epoch: [27][16/817]	Loss 0.0073 (0.0532)	
training:	Epoch: [27][17/817]	Loss 0.0084 (0.0505)	
training:	Epoch: [27][18/817]	Loss 0.0071 (0.0481)	
training:	Epoch: [27][19/817]	Loss 0.0074 (0.0460)	
training:	Epoch: [27][20/817]	Loss 0.0183 (0.0446)	
training:	Epoch: [27][21/817]	Loss 0.0076 (0.0428)	
training:	Epoch: [27][22/817]	Loss 0.0074 (0.0412)	
training:	Epoch: [27][23/817]	Loss 0.0085 (0.0398)	
training:	Epoch: [27][24/817]	Loss 0.0069 (0.0384)	
training:	Epoch: [27][25/817]	Loss 0.0071 (0.0372)	
training:	Epoch: [27][26/817]	Loss 0.0069 (0.0360)	
training:	Epoch: [27][27/817]	Loss 0.0438 (0.0363)	
training:	Epoch: [27][28/817]	Loss 0.0095 (0.0353)	
training:	Epoch: [27][29/817]	Loss 0.0133 (0.0346)	
training:	Epoch: [27][30/817]	Loss 0.0162 (0.0340)	
training:	Epoch: [27][31/817]	Loss 0.0070 (0.0331)	
training:	Epoch: [27][32/817]	Loss 0.0076 (0.0323)	
training:	Epoch: [27][33/817]	Loss 0.0076 (0.0316)	
training:	Epoch: [27][34/817]	Loss 0.0073 (0.0308)	
training:	Epoch: [27][35/817]	Loss 0.0074 (0.0302)	
training:	Epoch: [27][36/817]	Loss 0.5990 (0.0460)	
training:	Epoch: [27][37/817]	Loss 0.0082 (0.0450)	
training:	Epoch: [27][38/817]	Loss 0.0061 (0.0439)	
training:	Epoch: [27][39/817]	Loss 0.0108 (0.0431)	
training:	Epoch: [27][40/817]	Loss 0.0074 (0.0422)	
training:	Epoch: [27][41/817]	Loss 0.0089 (0.0414)	
training:	Epoch: [27][42/817]	Loss 0.0075 (0.0406)	
training:	Epoch: [27][43/817]	Loss 0.0073 (0.0398)	
training:	Epoch: [27][44/817]	Loss 0.0069 (0.0391)	
training:	Epoch: [27][45/817]	Loss 0.0077 (0.0384)	
training:	Epoch: [27][46/817]	Loss 0.0085 (0.0377)	
training:	Epoch: [27][47/817]	Loss 0.0076 (0.0371)	
training:	Epoch: [27][48/817]	Loss 0.0069 (0.0364)	
training:	Epoch: [27][49/817]	Loss 0.2497 (0.0408)	
training:	Epoch: [27][50/817]	Loss 0.0071 (0.0401)	
training:	Epoch: [27][51/817]	Loss 0.0082 (0.0395)	
training:	Epoch: [27][52/817]	Loss 0.0066 (0.0389)	
training:	Epoch: [27][53/817]	Loss 0.0070 (0.0383)	
training:	Epoch: [27][54/817]	Loss 0.0067 (0.0377)	
training:	Epoch: [27][55/817]	Loss 0.0061 (0.0371)	
training:	Epoch: [27][56/817]	Loss 0.0089 (0.0366)	
training:	Epoch: [27][57/817]	Loss 0.0080 (0.0361)	
training:	Epoch: [27][58/817]	Loss 0.0086 (0.0356)	
training:	Epoch: [27][59/817]	Loss 0.0077 (0.0351)	
training:	Epoch: [27][60/817]	Loss 0.0996 (0.0362)	
training:	Epoch: [27][61/817]	Loss 0.0071 (0.0357)	
training:	Epoch: [27][62/817]	Loss 0.0143 (0.0354)	
training:	Epoch: [27][63/817]	Loss 0.0073 (0.0350)	
training:	Epoch: [27][64/817]	Loss 0.0080 (0.0345)	
training:	Epoch: [27][65/817]	Loss 0.0069 (0.0341)	
training:	Epoch: [27][66/817]	Loss 0.0067 (0.0337)	
training:	Epoch: [27][67/817]	Loss 0.0380 (0.0338)	
training:	Epoch: [27][68/817]	Loss 0.0092 (0.0334)	
training:	Epoch: [27][69/817]	Loss 0.0079 (0.0330)	
training:	Epoch: [27][70/817]	Loss 0.0077 (0.0327)	
training:	Epoch: [27][71/817]	Loss 0.0067 (0.0323)	
training:	Epoch: [27][72/817]	Loss 0.0073 (0.0319)	
training:	Epoch: [27][73/817]	Loss 0.0083 (0.0316)	
training:	Epoch: [27][74/817]	Loss 0.0087 (0.0313)	
training:	Epoch: [27][75/817]	Loss 0.0077 (0.0310)	
training:	Epoch: [27][76/817]	Loss 0.6103 (0.0386)	
training:	Epoch: [27][77/817]	Loss 0.0070 (0.0382)	
training:	Epoch: [27][78/817]	Loss 0.0075 (0.0378)	
training:	Epoch: [27][79/817]	Loss 0.0083 (0.0374)	
training:	Epoch: [27][80/817]	Loss 0.0078 (0.0371)	
training:	Epoch: [27][81/817]	Loss 0.0074 (0.0367)	
training:	Epoch: [27][82/817]	Loss 0.0075 (0.0363)	
training:	Epoch: [27][83/817]	Loss 0.0078 (0.0360)	
training:	Epoch: [27][84/817]	Loss 0.0080 (0.0357)	
training:	Epoch: [27][85/817]	Loss 0.0070 (0.0353)	
training:	Epoch: [27][86/817]	Loss 0.0064 (0.0350)	
training:	Epoch: [27][87/817]	Loss 0.0073 (0.0347)	
training:	Epoch: [27][88/817]	Loss 0.0085 (0.0344)	
training:	Epoch: [27][89/817]	Loss 0.0068 (0.0341)	
training:	Epoch: [27][90/817]	Loss 0.0064 (0.0338)	
training:	Epoch: [27][91/817]	Loss 0.0072 (0.0335)	
training:	Epoch: [27][92/817]	Loss 0.0077 (0.0332)	
training:	Epoch: [27][93/817]	Loss 0.0160 (0.0330)	
training:	Epoch: [27][94/817]	Loss 0.0072 (0.0327)	
training:	Epoch: [27][95/817]	Loss 0.0068 (0.0325)	
training:	Epoch: [27][96/817]	Loss 0.0069 (0.0322)	
training:	Epoch: [27][97/817]	Loss 0.0075 (0.0319)	
training:	Epoch: [27][98/817]	Loss 1.1959 (0.0438)	
training:	Epoch: [27][99/817]	Loss 0.0077 (0.0435)	
training:	Epoch: [27][100/817]	Loss 0.0075 (0.0431)	
training:	Epoch: [27][101/817]	Loss 0.0081 (0.0427)	
training:	Epoch: [27][102/817]	Loss 0.0126 (0.0425)	
training:	Epoch: [27][103/817]	Loss 0.0081 (0.0421)	
training:	Epoch: [27][104/817]	Loss 0.0077 (0.0418)	
training:	Epoch: [27][105/817]	Loss 0.0074 (0.0415)	
training:	Epoch: [27][106/817]	Loss 0.0078 (0.0411)	
training:	Epoch: [27][107/817]	Loss 0.0089 (0.0408)	
training:	Epoch: [27][108/817]	Loss 0.0074 (0.0405)	
training:	Epoch: [27][109/817]	Loss 0.0073 (0.0402)	
training:	Epoch: [27][110/817]	Loss 0.0075 (0.0399)	
training:	Epoch: [27][111/817]	Loss 0.0071 (0.0396)	
training:	Epoch: [27][112/817]	Loss 0.0079 (0.0393)	
training:	Epoch: [27][113/817]	Loss 0.0080 (0.0391)	
training:	Epoch: [27][114/817]	Loss 0.0071 (0.0388)	
training:	Epoch: [27][115/817]	Loss 0.0106 (0.0385)	
training:	Epoch: [27][116/817]	Loss 0.0072 (0.0383)	
training:	Epoch: [27][117/817]	Loss 0.0076 (0.0380)	
training:	Epoch: [27][118/817]	Loss 0.0075 (0.0378)	
training:	Epoch: [27][119/817]	Loss 0.5939 (0.0424)	
training:	Epoch: [27][120/817]	Loss 0.0078 (0.0421)	
training:	Epoch: [27][121/817]	Loss 0.0073 (0.0419)	
training:	Epoch: [27][122/817]	Loss 0.0072 (0.0416)	
training:	Epoch: [27][123/817]	Loss 0.0081 (0.0413)	
training:	Epoch: [27][124/817]	Loss 0.0066 (0.0410)	
training:	Epoch: [27][125/817]	Loss 0.0075 (0.0407)	
training:	Epoch: [27][126/817]	Loss 0.0069 (0.0405)	
training:	Epoch: [27][127/817]	Loss 0.0107 (0.0402)	
training:	Epoch: [27][128/817]	Loss 0.0079 (0.0400)	
training:	Epoch: [27][129/817]	Loss 0.0081 (0.0397)	
training:	Epoch: [27][130/817]	Loss 0.0066 (0.0395)	
training:	Epoch: [27][131/817]	Loss 0.0080 (0.0392)	
training:	Epoch: [27][132/817]	Loss 0.0100 (0.0390)	
training:	Epoch: [27][133/817]	Loss 0.0077 (0.0388)	
training:	Epoch: [27][134/817]	Loss 0.0083 (0.0386)	
training:	Epoch: [27][135/817]	Loss 0.0080 (0.0383)	
training:	Epoch: [27][136/817]	Loss 0.0069 (0.0381)	
training:	Epoch: [27][137/817]	Loss 0.0075 (0.0379)	
training:	Epoch: [27][138/817]	Loss 0.0075 (0.0377)	
training:	Epoch: [27][139/817]	Loss 0.0067 (0.0374)	
training:	Epoch: [27][140/817]	Loss 0.0075 (0.0372)	
training:	Epoch: [27][141/817]	Loss 0.5908 (0.0412)	
training:	Epoch: [27][142/817]	Loss 0.0079 (0.0409)	
training:	Epoch: [27][143/817]	Loss 0.0092 (0.0407)	
training:	Epoch: [27][144/817]	Loss 0.0066 (0.0405)	
training:	Epoch: [27][145/817]	Loss 0.0068 (0.0402)	
training:	Epoch: [27][146/817]	Loss 0.0074 (0.0400)	
training:	Epoch: [27][147/817]	Loss 0.0094 (0.0398)	
training:	Epoch: [27][148/817]	Loss 0.0079 (0.0396)	
training:	Epoch: [27][149/817]	Loss 0.0087 (0.0394)	
training:	Epoch: [27][150/817]	Loss 0.0079 (0.0392)	
training:	Epoch: [27][151/817]	Loss 0.0080 (0.0390)	
training:	Epoch: [27][152/817]	Loss 0.0150 (0.0388)	
training:	Epoch: [27][153/817]	Loss 0.0064 (0.0386)	
training:	Epoch: [27][154/817]	Loss 0.4257 (0.0411)	
training:	Epoch: [27][155/817]	Loss 0.0070 (0.0409)	
training:	Epoch: [27][156/817]	Loss 0.0064 (0.0407)	
training:	Epoch: [27][157/817]	Loss 0.0073 (0.0404)	
training:	Epoch: [27][158/817]	Loss 0.0063 (0.0402)	
training:	Epoch: [27][159/817]	Loss 0.0074 (0.0400)	
training:	Epoch: [27][160/817]	Loss 0.0077 (0.0398)	
training:	Epoch: [27][161/817]	Loss 0.0077 (0.0396)	
training:	Epoch: [27][162/817]	Loss 0.0072 (0.0394)	
training:	Epoch: [27][163/817]	Loss 0.0093 (0.0392)	
training:	Epoch: [27][164/817]	Loss 0.0076 (0.0390)	
training:	Epoch: [27][165/817]	Loss 0.0075 (0.0389)	
training:	Epoch: [27][166/817]	Loss 0.0072 (0.0387)	
training:	Epoch: [27][167/817]	Loss 0.0139 (0.0385)	
training:	Epoch: [27][168/817]	Loss 0.0075 (0.0383)	
training:	Epoch: [27][169/817]	Loss 0.0089 (0.0382)	
training:	Epoch: [27][170/817]	Loss 0.0077 (0.0380)	
training:	Epoch: [27][171/817]	Loss 0.0118 (0.0378)	
training:	Epoch: [27][172/817]	Loss 0.0067 (0.0376)	
training:	Epoch: [27][173/817]	Loss 0.0145 (0.0375)	
training:	Epoch: [27][174/817]	Loss 0.0070 (0.0373)	
training:	Epoch: [27][175/817]	Loss 0.0079 (0.0372)	
training:	Epoch: [27][176/817]	Loss 0.0065 (0.0370)	
training:	Epoch: [27][177/817]	Loss 0.0081 (0.0368)	
training:	Epoch: [27][178/817]	Loss 0.0076 (0.0367)	
training:	Epoch: [27][179/817]	Loss 0.0087 (0.0365)	
training:	Epoch: [27][180/817]	Loss 0.0082 (0.0364)	
training:	Epoch: [27][181/817]	Loss 0.0072 (0.0362)	
training:	Epoch: [27][182/817]	Loss 0.0071 (0.0360)	
training:	Epoch: [27][183/817]	Loss 0.0088 (0.0359)	
training:	Epoch: [27][184/817]	Loss 0.0071 (0.0357)	
training:	Epoch: [27][185/817]	Loss 0.0081 (0.0356)	
training:	Epoch: [27][186/817]	Loss 0.0079 (0.0354)	
training:	Epoch: [27][187/817]	Loss 0.0074 (0.0353)	
training:	Epoch: [27][188/817]	Loss 0.0074 (0.0351)	
training:	Epoch: [27][189/817]	Loss 0.0081 (0.0350)	
training:	Epoch: [27][190/817]	Loss 0.0101 (0.0349)	
training:	Epoch: [27][191/817]	Loss 0.5886 (0.0378)	
training:	Epoch: [27][192/817]	Loss 0.0070 (0.0376)	
training:	Epoch: [27][193/817]	Loss 0.0069 (0.0374)	
training:	Epoch: [27][194/817]	Loss 0.0094 (0.0373)	
training:	Epoch: [27][195/817]	Loss 0.0073 (0.0371)	
training:	Epoch: [27][196/817]	Loss 0.0081 (0.0370)	
training:	Epoch: [27][197/817]	Loss 0.0088 (0.0368)	
training:	Epoch: [27][198/817]	Loss 0.0648 (0.0370)	
training:	Epoch: [27][199/817]	Loss 0.0080 (0.0368)	
training:	Epoch: [27][200/817]	Loss 0.0067 (0.0367)	
training:	Epoch: [27][201/817]	Loss 0.0509 (0.0368)	
training:	Epoch: [27][202/817]	Loss 0.0065 (0.0366)	
training:	Epoch: [27][203/817]	Loss 0.0078 (0.0365)	
training:	Epoch: [27][204/817]	Loss 0.2961 (0.0377)	
training:	Epoch: [27][205/817]	Loss 0.0069 (0.0376)	
training:	Epoch: [27][206/817]	Loss 0.0086 (0.0375)	
training:	Epoch: [27][207/817]	Loss 0.0082 (0.0373)	
training:	Epoch: [27][208/817]	Loss 0.0074 (0.0372)	
training:	Epoch: [27][209/817]	Loss 0.0080 (0.0370)	
training:	Epoch: [27][210/817]	Loss 0.0072 (0.0369)	
training:	Epoch: [27][211/817]	Loss 0.0087 (0.0368)	
training:	Epoch: [27][212/817]	Loss 0.0077 (0.0366)	
training:	Epoch: [27][213/817]	Loss 0.0063 (0.0365)	
training:	Epoch: [27][214/817]	Loss 0.0065 (0.0363)	
training:	Epoch: [27][215/817]	Loss 0.0094 (0.0362)	
training:	Epoch: [27][216/817]	Loss 0.0329 (0.0362)	
training:	Epoch: [27][217/817]	Loss 0.5755 (0.0387)	
training:	Epoch: [27][218/817]	Loss 0.0081 (0.0385)	
training:	Epoch: [27][219/817]	Loss 0.0074 (0.0384)	
training:	Epoch: [27][220/817]	Loss 0.0070 (0.0383)	
training:	Epoch: [27][221/817]	Loss 0.0092 (0.0381)	
training:	Epoch: [27][222/817]	Loss 0.0072 (0.0380)	
training:	Epoch: [27][223/817]	Loss 0.0080 (0.0378)	
training:	Epoch: [27][224/817]	Loss 0.0091 (0.0377)	
training:	Epoch: [27][225/817]	Loss 0.0073 (0.0376)	
training:	Epoch: [27][226/817]	Loss 0.0079 (0.0375)	
training:	Epoch: [27][227/817]	Loss 0.0089 (0.0373)	
training:	Epoch: [27][228/817]	Loss 0.0080 (0.0372)	
training:	Epoch: [27][229/817]	Loss 0.0070 (0.0371)	
training:	Epoch: [27][230/817]	Loss 0.0074 (0.0369)	
training:	Epoch: [27][231/817]	Loss 0.0071 (0.0368)	
training:	Epoch: [27][232/817]	Loss 0.0072 (0.0367)	
training:	Epoch: [27][233/817]	Loss 0.0253 (0.0366)	
training:	Epoch: [27][234/817]	Loss 0.0077 (0.0365)	
training:	Epoch: [27][235/817]	Loss 0.0081 (0.0364)	
training:	Epoch: [27][236/817]	Loss 0.0078 (0.0363)	
training:	Epoch: [27][237/817]	Loss 0.0074 (0.0361)	
training:	Epoch: [27][238/817]	Loss 0.0093 (0.0360)	
training:	Epoch: [27][239/817]	Loss 0.0095 (0.0359)	
training:	Epoch: [27][240/817]	Loss 0.0076 (0.0358)	
training:	Epoch: [27][241/817]	Loss 0.0094 (0.0357)	
training:	Epoch: [27][242/817]	Loss 0.0068 (0.0356)	
training:	Epoch: [27][243/817]	Loss 0.0068 (0.0355)	
training:	Epoch: [27][244/817]	Loss 0.0084 (0.0353)	
training:	Epoch: [27][245/817]	Loss 0.0071 (0.0352)	
training:	Epoch: [27][246/817]	Loss 0.0074 (0.0351)	
training:	Epoch: [27][247/817]	Loss 0.0068 (0.0350)	
training:	Epoch: [27][248/817]	Loss 0.0078 (0.0349)	
training:	Epoch: [27][249/817]	Loss 0.0093 (0.0348)	
training:	Epoch: [27][250/817]	Loss 0.0061 (0.0347)	
training:	Epoch: [27][251/817]	Loss 0.0066 (0.0346)	
training:	Epoch: [27][252/817]	Loss 0.0065 (0.0344)	
training:	Epoch: [27][253/817]	Loss 0.0080 (0.0343)	
training:	Epoch: [27][254/817]	Loss 0.6304 (0.0367)	
training:	Epoch: [27][255/817]	Loss 0.0077 (0.0366)	
training:	Epoch: [27][256/817]	Loss 0.0073 (0.0365)	
training:	Epoch: [27][257/817]	Loss 0.0075 (0.0364)	
training:	Epoch: [27][258/817]	Loss 0.0623 (0.0365)	
training:	Epoch: [27][259/817]	Loss 0.0087 (0.0363)	
training:	Epoch: [27][260/817]	Loss 0.0068 (0.0362)	
training:	Epoch: [27][261/817]	Loss 0.0099 (0.0361)	
training:	Epoch: [27][262/817]	Loss 0.0072 (0.0360)	
training:	Epoch: [27][263/817]	Loss 0.0087 (0.0359)	
training:	Epoch: [27][264/817]	Loss 0.0070 (0.0358)	
training:	Epoch: [27][265/817]	Loss 0.0068 (0.0357)	
training:	Epoch: [27][266/817]	Loss 0.0088 (0.0356)	
training:	Epoch: [27][267/817]	Loss 0.0063 (0.0355)	
training:	Epoch: [27][268/817]	Loss 0.0087 (0.0354)	
training:	Epoch: [27][269/817]	Loss 0.0068 (0.0353)	
training:	Epoch: [27][270/817]	Loss 0.0075 (0.0352)	
training:	Epoch: [27][271/817]	Loss 0.0072 (0.0351)	
training:	Epoch: [27][272/817]	Loss 0.6305 (0.0373)	
training:	Epoch: [27][273/817]	Loss 0.0083 (0.0372)	
training:	Epoch: [27][274/817]	Loss 0.0100 (0.0371)	
training:	Epoch: [27][275/817]	Loss 0.0062 (0.0369)	
training:	Epoch: [27][276/817]	Loss 0.0076 (0.0368)	
training:	Epoch: [27][277/817]	Loss 0.0067 (0.0367)	
training:	Epoch: [27][278/817]	Loss 0.0071 (0.0366)	
training:	Epoch: [27][279/817]	Loss 0.0073 (0.0365)	
training:	Epoch: [27][280/817]	Loss 0.0069 (0.0364)	
training:	Epoch: [27][281/817]	Loss 0.0079 (0.0363)	
training:	Epoch: [27][282/817]	Loss 0.0068 (0.0362)	
training:	Epoch: [27][283/817]	Loss 0.0075 (0.0361)	
training:	Epoch: [27][284/817]	Loss 0.0080 (0.0360)	
training:	Epoch: [27][285/817]	Loss 0.0074 (0.0359)	
training:	Epoch: [27][286/817]	Loss 0.0085 (0.0358)	
training:	Epoch: [27][287/817]	Loss 0.0069 (0.0357)	
training:	Epoch: [27][288/817]	Loss 0.6323 (0.0378)	
training:	Epoch: [27][289/817]	Loss 0.0073 (0.0377)	
training:	Epoch: [27][290/817]	Loss 0.0084 (0.0376)	
training:	Epoch: [27][291/817]	Loss 0.0169 (0.0375)	
training:	Epoch: [27][292/817]	Loss 0.0066 (0.0374)	
training:	Epoch: [27][293/817]	Loss 0.0075 (0.0373)	
training:	Epoch: [27][294/817]	Loss 0.0075 (0.0372)	
training:	Epoch: [27][295/817]	Loss 0.0075 (0.0371)	
training:	Epoch: [27][296/817]	Loss 0.0083 (0.0370)	
training:	Epoch: [27][297/817]	Loss 0.0069 (0.0369)	
training:	Epoch: [27][298/817]	Loss 0.0071 (0.0368)	
training:	Epoch: [27][299/817]	Loss 0.0076 (0.0367)	
training:	Epoch: [27][300/817]	Loss 0.0067 (0.0366)	
training:	Epoch: [27][301/817]	Loss 0.0062 (0.0365)	
training:	Epoch: [27][302/817]	Loss 0.0071 (0.0364)	
training:	Epoch: [27][303/817]	Loss 0.0077 (0.0363)	
training:	Epoch: [27][304/817]	Loss 0.0084 (0.0362)	
training:	Epoch: [27][305/817]	Loss 0.0079 (0.0361)	
training:	Epoch: [27][306/817]	Loss 0.0068 (0.0360)	
training:	Epoch: [27][307/817]	Loss 0.0088 (0.0359)	
training:	Epoch: [27][308/817]	Loss 0.0075 (0.0358)	
training:	Epoch: [27][309/817]	Loss 0.0069 (0.0357)	
training:	Epoch: [27][310/817]	Loss 0.0075 (0.0357)	
training:	Epoch: [27][311/817]	Loss 0.0067 (0.0356)	
training:	Epoch: [27][312/817]	Loss 0.0097 (0.0355)	
training:	Epoch: [27][313/817]	Loss 0.0159 (0.0354)	
training:	Epoch: [27][314/817]	Loss 0.0072 (0.0353)	
training:	Epoch: [27][315/817]	Loss 0.0064 (0.0352)	
training:	Epoch: [27][316/817]	Loss 0.5922 (0.0370)	
training:	Epoch: [27][317/817]	Loss 0.0079 (0.0369)	
training:	Epoch: [27][318/817]	Loss 0.0076 (0.0368)	
training:	Epoch: [27][319/817]	Loss 0.0075 (0.0367)	
training:	Epoch: [27][320/817]	Loss 0.0072 (0.0366)	
training:	Epoch: [27][321/817]	Loss 0.0087 (0.0365)	
training:	Epoch: [27][322/817]	Loss 0.0075 (0.0365)	
training:	Epoch: [27][323/817]	Loss 0.5547 (0.0381)	
training:	Epoch: [27][324/817]	Loss 0.0067 (0.0380)	
training:	Epoch: [27][325/817]	Loss 0.5914 (0.0397)	
training:	Epoch: [27][326/817]	Loss 0.0063 (0.0396)	
training:	Epoch: [27][327/817]	Loss 0.5942 (0.0413)	
training:	Epoch: [27][328/817]	Loss 0.0077 (0.0412)	
training:	Epoch: [27][329/817]	Loss 0.0119 (0.0411)	
training:	Epoch: [27][330/817]	Loss 0.0210 (0.0410)	
training:	Epoch: [27][331/817]	Loss 0.0073 (0.0409)	
training:	Epoch: [27][332/817]	Loss 0.0065 (0.0408)	
training:	Epoch: [27][333/817]	Loss 0.0071 (0.0407)	
training:	Epoch: [27][334/817]	Loss 0.0072 (0.0406)	
training:	Epoch: [27][335/817]	Loss 0.0082 (0.0405)	
training:	Epoch: [27][336/817]	Loss 0.0085 (0.0404)	
training:	Epoch: [27][337/817]	Loss 0.0080 (0.0403)	
training:	Epoch: [27][338/817]	Loss 0.0085 (0.0402)	
training:	Epoch: [27][339/817]	Loss 0.0069 (0.0401)	
training:	Epoch: [27][340/817]	Loss 0.0087 (0.0400)	
training:	Epoch: [27][341/817]	Loss 0.0322 (0.0400)	
training:	Epoch: [27][342/817]	Loss 0.0092 (0.0399)	
training:	Epoch: [27][343/817]	Loss 0.0072 (0.0398)	
training:	Epoch: [27][344/817]	Loss 0.0086 (0.0397)	
training:	Epoch: [27][345/817]	Loss 0.0083 (0.0396)	
training:	Epoch: [27][346/817]	Loss 0.0080 (0.0395)	
training:	Epoch: [27][347/817]	Loss 0.0083 (0.0395)	
training:	Epoch: [27][348/817]	Loss 0.0074 (0.0394)	
training:	Epoch: [27][349/817]	Loss 0.0078 (0.0393)	
training:	Epoch: [27][350/817]	Loss 0.0077 (0.0392)	
training:	Epoch: [27][351/817]	Loss 0.0071 (0.0391)	
training:	Epoch: [27][352/817]	Loss 0.0067 (0.0390)	
training:	Epoch: [27][353/817]	Loss 0.0081 (0.0389)	
training:	Epoch: [27][354/817]	Loss 0.0078 (0.0388)	
training:	Epoch: [27][355/817]	Loss 0.0071 (0.0387)	
training:	Epoch: [27][356/817]	Loss 0.0060 (0.0386)	
training:	Epoch: [27][357/817]	Loss 0.0065 (0.0386)	
training:	Epoch: [27][358/817]	Loss 0.0076 (0.0385)	
training:	Epoch: [27][359/817]	Loss 0.0069 (0.0384)	
training:	Epoch: [27][360/817]	Loss 0.0088 (0.0383)	
training:	Epoch: [27][361/817]	Loss 0.0086 (0.0382)	
training:	Epoch: [27][362/817]	Loss 0.0085 (0.0381)	
training:	Epoch: [27][363/817]	Loss 0.0071 (0.0380)	
training:	Epoch: [27][364/817]	Loss 0.2560 (0.0386)	
training:	Epoch: [27][365/817]	Loss 0.0074 (0.0386)	
training:	Epoch: [27][366/817]	Loss 0.0077 (0.0385)	
training:	Epoch: [27][367/817]	Loss 0.0065 (0.0384)	
training:	Epoch: [27][368/817]	Loss 0.0070 (0.0383)	
training:	Epoch: [27][369/817]	Loss 0.0089 (0.0382)	
training:	Epoch: [27][370/817]	Loss 0.0093 (0.0381)	
training:	Epoch: [27][371/817]	Loss 0.6448 (0.0398)	
training:	Epoch: [27][372/817]	Loss 0.0098 (0.0397)	
training:	Epoch: [27][373/817]	Loss 0.0098 (0.0396)	
training:	Epoch: [27][374/817]	Loss 0.0088 (0.0395)	
training:	Epoch: [27][375/817]	Loss 0.0075 (0.0395)	
training:	Epoch: [27][376/817]	Loss 0.0079 (0.0394)	
training:	Epoch: [27][377/817]	Loss 0.0205 (0.0393)	
training:	Epoch: [27][378/817]	Loss 0.1375 (0.0396)	
training:	Epoch: [27][379/817]	Loss 0.0068 (0.0395)	
training:	Epoch: [27][380/817]	Loss 0.0081 (0.0394)	
training:	Epoch: [27][381/817]	Loss 0.0075 (0.0393)	
training:	Epoch: [27][382/817]	Loss 0.0072 (0.0392)	
training:	Epoch: [27][383/817]	Loss 0.0067 (0.0392)	
training:	Epoch: [27][384/817]	Loss 0.0081 (0.0391)	
training:	Epoch: [27][385/817]	Loss 0.0069 (0.0390)	
training:	Epoch: [27][386/817]	Loss 0.0080 (0.0389)	
training:	Epoch: [27][387/817]	Loss 0.0109 (0.0388)	
training:	Epoch: [27][388/817]	Loss 0.0069 (0.0388)	
training:	Epoch: [27][389/817]	Loss 0.0071 (0.0387)	
training:	Epoch: [27][390/817]	Loss 0.0075 (0.0386)	
training:	Epoch: [27][391/817]	Loss 0.0081 (0.0385)	
training:	Epoch: [27][392/817]	Loss 0.0068 (0.0384)	
training:	Epoch: [27][393/817]	Loss 0.0077 (0.0384)	
training:	Epoch: [27][394/817]	Loss 0.0078 (0.0383)	
training:	Epoch: [27][395/817]	Loss 0.0124 (0.0382)	
training:	Epoch: [27][396/817]	Loss 0.0066 (0.0381)	
training:	Epoch: [27][397/817]	Loss 0.0074 (0.0381)	
training:	Epoch: [27][398/817]	Loss 0.0076 (0.0380)	
training:	Epoch: [27][399/817]	Loss 0.0084 (0.0379)	
training:	Epoch: [27][400/817]	Loss 0.0066 (0.0378)	
training:	Epoch: [27][401/817]	Loss 0.0084 (0.0378)	
training:	Epoch: [27][402/817]	Loss 0.0065 (0.0377)	
training:	Epoch: [27][403/817]	Loss 0.0078 (0.0376)	
training:	Epoch: [27][404/817]	Loss 0.0080 (0.0375)	
training:	Epoch: [27][405/817]	Loss 0.0063 (0.0375)	
training:	Epoch: [27][406/817]	Loss 0.0066 (0.0374)	
training:	Epoch: [27][407/817]	Loss 0.0065 (0.0373)	
training:	Epoch: [27][408/817]	Loss 0.0074 (0.0372)	
training:	Epoch: [27][409/817]	Loss 0.0068 (0.0372)	
training:	Epoch: [27][410/817]	Loss 0.0076 (0.0371)	
training:	Epoch: [27][411/817]	Loss 0.0065 (0.0370)	
training:	Epoch: [27][412/817]	Loss 0.0081 (0.0369)	
training:	Epoch: [27][413/817]	Loss 0.0067 (0.0369)	
training:	Epoch: [27][414/817]	Loss 0.0067 (0.0368)	
training:	Epoch: [27][415/817]	Loss 0.0069 (0.0367)	
training:	Epoch: [27][416/817]	Loss 0.0070 (0.0366)	
training:	Epoch: [27][417/817]	Loss 0.0090 (0.0366)	
training:	Epoch: [27][418/817]	Loss 0.0075 (0.0365)	
training:	Epoch: [27][419/817]	Loss 0.0076 (0.0364)	
training:	Epoch: [27][420/817]	Loss 0.0071 (0.0364)	
training:	Epoch: [27][421/817]	Loss 0.0063 (0.0363)	
training:	Epoch: [27][422/817]	Loss 0.0079 (0.0362)	
training:	Epoch: [27][423/817]	Loss 0.6325 (0.0376)	
training:	Epoch: [27][424/817]	Loss 0.0062 (0.0376)	
training:	Epoch: [27][425/817]	Loss 0.0065 (0.0375)	
training:	Epoch: [27][426/817]	Loss 0.6009 (0.0388)	
training:	Epoch: [27][427/817]	Loss 0.0073 (0.0387)	
training:	Epoch: [27][428/817]	Loss 0.0073 (0.0387)	
training:	Epoch: [27][429/817]	Loss 0.0076 (0.0386)	
training:	Epoch: [27][430/817]	Loss 0.0077 (0.0385)	
training:	Epoch: [27][431/817]	Loss 0.0083 (0.0385)	
training:	Epoch: [27][432/817]	Loss 0.0071 (0.0384)	
training:	Epoch: [27][433/817]	Loss 0.0071 (0.0383)	
training:	Epoch: [27][434/817]	Loss 0.0072 (0.0382)	
training:	Epoch: [27][435/817]	Loss 0.0080 (0.0382)	
training:	Epoch: [27][436/817]	Loss 0.0065 (0.0381)	
training:	Epoch: [27][437/817]	Loss 0.0075 (0.0380)	
training:	Epoch: [27][438/817]	Loss 0.0069 (0.0380)	
training:	Epoch: [27][439/817]	Loss 0.0073 (0.0379)	
training:	Epoch: [27][440/817]	Loss 0.0288 (0.0379)	
training:	Epoch: [27][441/817]	Loss 0.5853 (0.0391)	
training:	Epoch: [27][442/817]	Loss 0.0992 (0.0392)	
training:	Epoch: [27][443/817]	Loss 0.0066 (0.0392)	
training:	Epoch: [27][444/817]	Loss 0.0070 (0.0391)	
training:	Epoch: [27][445/817]	Loss 0.0088 (0.0390)	
training:	Epoch: [27][446/817]	Loss 0.0075 (0.0390)	
training:	Epoch: [27][447/817]	Loss 0.0241 (0.0389)	
training:	Epoch: [27][448/817]	Loss 0.0091 (0.0389)	
training:	Epoch: [27][449/817]	Loss 0.0066 (0.0388)	
training:	Epoch: [27][450/817]	Loss 0.0073 (0.0387)	
training:	Epoch: [27][451/817]	Loss 0.0072 (0.0386)	
training:	Epoch: [27][452/817]	Loss 0.6345 (0.0400)	
training:	Epoch: [27][453/817]	Loss 0.0098 (0.0399)	
training:	Epoch: [27][454/817]	Loss 0.0071 (0.0398)	
training:	Epoch: [27][455/817]	Loss 0.0078 (0.0398)	
training:	Epoch: [27][456/817]	Loss 0.4810 (0.0407)	
training:	Epoch: [27][457/817]	Loss 0.0069 (0.0407)	
training:	Epoch: [27][458/817]	Loss 0.0074 (0.0406)	
training:	Epoch: [27][459/817]	Loss 0.0073 (0.0405)	
training:	Epoch: [27][460/817]	Loss 0.0076 (0.0404)	
training:	Epoch: [27][461/817]	Loss 0.0062 (0.0404)	
training:	Epoch: [27][462/817]	Loss 0.0069 (0.0403)	
training:	Epoch: [27][463/817]	Loss 0.0090 (0.0402)	
training:	Epoch: [27][464/817]	Loss 0.0137 (0.0402)	
training:	Epoch: [27][465/817]	Loss 0.0080 (0.0401)	
training:	Epoch: [27][466/817]	Loss 0.0070 (0.0400)	
training:	Epoch: [27][467/817]	Loss 0.0071 (0.0400)	
training:	Epoch: [27][468/817]	Loss 0.0097 (0.0399)	
training:	Epoch: [27][469/817]	Loss 0.0072 (0.0398)	
training:	Epoch: [27][470/817]	Loss 0.0084 (0.0398)	
training:	Epoch: [27][471/817]	Loss 0.0087 (0.0397)	
training:	Epoch: [27][472/817]	Loss 0.0093 (0.0396)	
training:	Epoch: [27][473/817]	Loss 0.0088 (0.0396)	
training:	Epoch: [27][474/817]	Loss 0.0974 (0.0397)	
training:	Epoch: [27][475/817]	Loss 0.0079 (0.0396)	
training:	Epoch: [27][476/817]	Loss 0.0078 (0.0395)	
training:	Epoch: [27][477/817]	Loss 0.0068 (0.0395)	
training:	Epoch: [27][478/817]	Loss 0.0125 (0.0394)	
training:	Epoch: [27][479/817]	Loss 0.0082 (0.0394)	
training:	Epoch: [27][480/817]	Loss 0.0083 (0.0393)	
training:	Epoch: [27][481/817]	Loss 0.0073 (0.0392)	
training:	Epoch: [27][482/817]	Loss 0.0076 (0.0392)	
training:	Epoch: [27][483/817]	Loss 0.0077 (0.0391)	
training:	Epoch: [27][484/817]	Loss 0.0071 (0.0390)	
training:	Epoch: [27][485/817]	Loss 0.0122 (0.0390)	
training:	Epoch: [27][486/817]	Loss 0.0099 (0.0389)	
training:	Epoch: [27][487/817]	Loss 0.0066 (0.0388)	
training:	Epoch: [27][488/817]	Loss 0.0071 (0.0388)	
training:	Epoch: [27][489/817]	Loss 0.0066 (0.0387)	
training:	Epoch: [27][490/817]	Loss 0.2499 (0.0391)	
training:	Epoch: [27][491/817]	Loss 0.0570 (0.0392)	
training:	Epoch: [27][492/817]	Loss 0.0066 (0.0391)	
training:	Epoch: [27][493/817]	Loss 0.0076 (0.0391)	
training:	Epoch: [27][494/817]	Loss 0.0060 (0.0390)	
training:	Epoch: [27][495/817]	Loss 0.0085 (0.0389)	
training:	Epoch: [27][496/817]	Loss 0.6444 (0.0401)	
training:	Epoch: [27][497/817]	Loss 0.3244 (0.0407)	
training:	Epoch: [27][498/817]	Loss 0.0087 (0.0407)	
training:	Epoch: [27][499/817]	Loss 0.0063 (0.0406)	
training:	Epoch: [27][500/817]	Loss 0.0099 (0.0405)	
training:	Epoch: [27][501/817]	Loss 0.0081 (0.0405)	
training:	Epoch: [27][502/817]	Loss 0.0067 (0.0404)	
training:	Epoch: [27][503/817]	Loss 0.0114 (0.0403)	
training:	Epoch: [27][504/817]	Loss 0.0082 (0.0403)	
training:	Epoch: [27][505/817]	Loss 0.0090 (0.0402)	
training:	Epoch: [27][506/817]	Loss 0.0075 (0.0401)	
training:	Epoch: [27][507/817]	Loss 0.5468 (0.0411)	
training:	Epoch: [27][508/817]	Loss 0.0065 (0.0411)	
training:	Epoch: [27][509/817]	Loss 0.0065 (0.0410)	
training:	Epoch: [27][510/817]	Loss 0.0109 (0.0409)	
training:	Epoch: [27][511/817]	Loss 0.0074 (0.0409)	
training:	Epoch: [27][512/817]	Loss 0.0091 (0.0408)	
training:	Epoch: [27][513/817]	Loss 0.0075 (0.0408)	
training:	Epoch: [27][514/817]	Loss 0.0091 (0.0407)	
training:	Epoch: [27][515/817]	Loss 0.0091 (0.0406)	
training:	Epoch: [27][516/817]	Loss 0.1090 (0.0408)	
training:	Epoch: [27][517/817]	Loss 0.0140 (0.0407)	
training:	Epoch: [27][518/817]	Loss 0.1679 (0.0410)	
training:	Epoch: [27][519/817]	Loss 0.0079 (0.0409)	
training:	Epoch: [27][520/817]	Loss 0.0071 (0.0408)	
training:	Epoch: [27][521/817]	Loss 0.0084 (0.0408)	
training:	Epoch: [27][522/817]	Loss 0.0090 (0.0407)	
training:	Epoch: [27][523/817]	Loss 0.1717 (0.0410)	
training:	Epoch: [27][524/817]	Loss 0.1317 (0.0411)	
training:	Epoch: [27][525/817]	Loss 0.6485 (0.0423)	
training:	Epoch: [27][526/817]	Loss 0.0103 (0.0422)	
training:	Epoch: [27][527/817]	Loss 0.0079 (0.0422)	
training:	Epoch: [27][528/817]	Loss 0.0077 (0.0421)	
training:	Epoch: [27][529/817]	Loss 0.6143 (0.0432)	
training:	Epoch: [27][530/817]	Loss 0.0083 (0.0431)	
training:	Epoch: [27][531/817]	Loss 0.0328 (0.0431)	
training:	Epoch: [27][532/817]	Loss 0.5104 (0.0440)	
training:	Epoch: [27][533/817]	Loss 0.0070 (0.0439)	
training:	Epoch: [27][534/817]	Loss 0.0077 (0.0438)	
training:	Epoch: [27][535/817]	Loss 0.0072 (0.0438)	
training:	Epoch: [27][536/817]	Loss 0.0075 (0.0437)	
training:	Epoch: [27][537/817]	Loss 0.0096 (0.0436)	
training:	Epoch: [27][538/817]	Loss 0.0079 (0.0436)	
training:	Epoch: [27][539/817]	Loss 0.0077 (0.0435)	
training:	Epoch: [27][540/817]	Loss 0.0088 (0.0434)	
training:	Epoch: [27][541/817]	Loss 0.0075 (0.0434)	
training:	Epoch: [27][542/817]	Loss 0.0093 (0.0433)	
training:	Epoch: [27][543/817]	Loss 0.0438 (0.0433)	
training:	Epoch: [27][544/817]	Loss 0.0079 (0.0432)	
training:	Epoch: [27][545/817]	Loss 0.0082 (0.0432)	
training:	Epoch: [27][546/817]	Loss 0.0085 (0.0431)	
training:	Epoch: [27][547/817]	Loss 0.0079 (0.0430)	
training:	Epoch: [27][548/817]	Loss 0.0079 (0.0430)	
training:	Epoch: [27][549/817]	Loss 0.0066 (0.0429)	
training:	Epoch: [27][550/817]	Loss 0.0087 (0.0429)	
training:	Epoch: [27][551/817]	Loss 0.4929 (0.0437)	
training:	Epoch: [27][552/817]	Loss 0.0083 (0.0436)	
training:	Epoch: [27][553/817]	Loss 0.0080 (0.0435)	
training:	Epoch: [27][554/817]	Loss 0.0106 (0.0435)	
training:	Epoch: [27][555/817]	Loss 0.4823 (0.0443)	
training:	Epoch: [27][556/817]	Loss 0.0076 (0.0442)	
training:	Epoch: [27][557/817]	Loss 0.0086 (0.0441)	
training:	Epoch: [27][558/817]	Loss 0.0104 (0.0441)	
training:	Epoch: [27][559/817]	Loss 0.0132 (0.0440)	
training:	Epoch: [27][560/817]	Loss 0.0093 (0.0440)	
training:	Epoch: [27][561/817]	Loss 0.0084 (0.0439)	
training:	Epoch: [27][562/817]	Loss 0.0083 (0.0438)	
training:	Epoch: [27][563/817]	Loss 0.0080 (0.0438)	
training:	Epoch: [27][564/817]	Loss 0.0119 (0.0437)	
training:	Epoch: [27][565/817]	Loss 0.0112 (0.0437)	
training:	Epoch: [27][566/817]	Loss 0.0113 (0.0436)	
training:	Epoch: [27][567/817]	Loss 0.0091 (0.0435)	
training:	Epoch: [27][568/817]	Loss 0.0102 (0.0435)	
training:	Epoch: [27][569/817]	Loss 0.0080 (0.0434)	
training:	Epoch: [27][570/817]	Loss 0.0080 (0.0434)	
training:	Epoch: [27][571/817]	Loss 0.0074 (0.0433)	
training:	Epoch: [27][572/817]	Loss 0.0068 (0.0432)	
training:	Epoch: [27][573/817]	Loss 0.0082 (0.0432)	
training:	Epoch: [27][574/817]	Loss 0.0081 (0.0431)	
training:	Epoch: [27][575/817]	Loss 0.0075 (0.0431)	
training:	Epoch: [27][576/817]	Loss 0.0107 (0.0430)	
training:	Epoch: [27][577/817]	Loss 0.0116 (0.0429)	
training:	Epoch: [27][578/817]	Loss 0.0092 (0.0429)	
training:	Epoch: [27][579/817]	Loss 0.0109 (0.0428)	
training:	Epoch: [27][580/817]	Loss 0.0103 (0.0428)	
training:	Epoch: [27][581/817]	Loss 0.0081 (0.0427)	
training:	Epoch: [27][582/817]	Loss 0.0103 (0.0427)	
training:	Epoch: [27][583/817]	Loss 0.0079 (0.0426)	
training:	Epoch: [27][584/817]	Loss 0.0266 (0.0426)	
training:	Epoch: [27][585/817]	Loss 0.0073 (0.0425)	
training:	Epoch: [27][586/817]	Loss 0.0112 (0.0425)	
training:	Epoch: [27][587/817]	Loss 0.0093 (0.0424)	
training:	Epoch: [27][588/817]	Loss 0.0106 (0.0423)	
training:	Epoch: [27][589/817]	Loss 0.0076 (0.0423)	
training:	Epoch: [27][590/817]	Loss 0.0081 (0.0422)	
training:	Epoch: [27][591/817]	Loss 0.0125 (0.0422)	
training:	Epoch: [27][592/817]	Loss 0.0070 (0.0421)	
training:	Epoch: [27][593/817]	Loss 0.5957 (0.0431)	
training:	Epoch: [27][594/817]	Loss 0.0099 (0.0430)	
training:	Epoch: [27][595/817]	Loss 0.0093 (0.0429)	
training:	Epoch: [27][596/817]	Loss 0.0079 (0.0429)	
training:	Epoch: [27][597/817]	Loss 0.0086 (0.0428)	
training:	Epoch: [27][598/817]	Loss 0.0301 (0.0428)	
training:	Epoch: [27][599/817]	Loss 0.0063 (0.0427)	
training:	Epoch: [27][600/817]	Loss 0.0100 (0.0427)	
training:	Epoch: [27][601/817]	Loss 0.0098 (0.0426)	
training:	Epoch: [27][602/817]	Loss 0.0092 (0.0426)	
training:	Epoch: [27][603/817]	Loss 0.0123 (0.0425)	
training:	Epoch: [27][604/817]	Loss 0.0092 (0.0425)	
training:	Epoch: [27][605/817]	Loss 0.0104 (0.0424)	
training:	Epoch: [27][606/817]	Loss 0.0112 (0.0424)	
training:	Epoch: [27][607/817]	Loss 0.0069 (0.0423)	
training:	Epoch: [27][608/817]	Loss 0.0074 (0.0422)	
training:	Epoch: [27][609/817]	Loss 0.0085 (0.0422)	
training:	Epoch: [27][610/817]	Loss 0.0094 (0.0421)	
training:	Epoch: [27][611/817]	Loss 0.0074 (0.0421)	
training:	Epoch: [27][612/817]	Loss 0.0141 (0.0420)	
training:	Epoch: [27][613/817]	Loss 0.0075 (0.0420)	
training:	Epoch: [27][614/817]	Loss 0.0070 (0.0419)	
training:	Epoch: [27][615/817]	Loss 0.0085 (0.0419)	
training:	Epoch: [27][616/817]	Loss 0.0063 (0.0418)	
training:	Epoch: [27][617/817]	Loss 0.0068 (0.0418)	
training:	Epoch: [27][618/817]	Loss 0.0086 (0.0417)	
training:	Epoch: [27][619/817]	Loss 0.0090 (0.0416)	
training:	Epoch: [27][620/817]	Loss 0.0100 (0.0416)	
training:	Epoch: [27][621/817]	Loss 0.0079 (0.0415)	
training:	Epoch: [27][622/817]	Loss 0.0232 (0.0415)	
training:	Epoch: [27][623/817]	Loss 0.0066 (0.0415)	
training:	Epoch: [27][624/817]	Loss 0.0071 (0.0414)	
training:	Epoch: [27][625/817]	Loss 0.5596 (0.0422)	
training:	Epoch: [27][626/817]	Loss 0.0089 (0.0422)	
training:	Epoch: [27][627/817]	Loss 0.0081 (0.0421)	
training:	Epoch: [27][628/817]	Loss 0.0088 (0.0421)	
training:	Epoch: [27][629/817]	Loss 0.0068 (0.0420)	
training:	Epoch: [27][630/817]	Loss 0.0092 (0.0420)	
training:	Epoch: [27][631/817]	Loss 0.0071 (0.0419)	
training:	Epoch: [27][632/817]	Loss 0.0120 (0.0419)	
training:	Epoch: [27][633/817]	Loss 0.0079 (0.0418)	
training:	Epoch: [27][634/817]	Loss 0.0115 (0.0418)	
training:	Epoch: [27][635/817]	Loss 0.0079 (0.0417)	
training:	Epoch: [27][636/817]	Loss 0.0078 (0.0417)	
training:	Epoch: [27][637/817]	Loss 0.0115 (0.0416)	
training:	Epoch: [27][638/817]	Loss 0.0073 (0.0416)	
training:	Epoch: [27][639/817]	Loss 0.0071 (0.0415)	
training:	Epoch: [27][640/817]	Loss 0.0064 (0.0414)	
training:	Epoch: [27][641/817]	Loss 0.0086 (0.0414)	
training:	Epoch: [27][642/817]	Loss 0.0081 (0.0413)	
training:	Epoch: [27][643/817]	Loss 0.0073 (0.0413)	
training:	Epoch: [27][644/817]	Loss 0.0071 (0.0412)	
training:	Epoch: [27][645/817]	Loss 0.0073 (0.0412)	
training:	Epoch: [27][646/817]	Loss 0.0082 (0.0411)	
training:	Epoch: [27][647/817]	Loss 0.0063 (0.0411)	
training:	Epoch: [27][648/817]	Loss 0.0079 (0.0410)	
training:	Epoch: [27][649/817]	Loss 0.0151 (0.0410)	
training:	Epoch: [27][650/817]	Loss 0.0070 (0.0409)	
training:	Epoch: [27][651/817]	Loss 0.0084 (0.0409)	
training:	Epoch: [27][652/817]	Loss 0.0077 (0.0408)	
training:	Epoch: [27][653/817]	Loss 0.0070 (0.0408)	
training:	Epoch: [27][654/817]	Loss 0.0063 (0.0407)	
training:	Epoch: [27][655/817]	Loss 0.0070 (0.0407)	
training:	Epoch: [27][656/817]	Loss 0.0080 (0.0406)	
training:	Epoch: [27][657/817]	Loss 0.0071 (0.0406)	
training:	Epoch: [27][658/817]	Loss 0.0086 (0.0405)	
training:	Epoch: [27][659/817]	Loss 0.0078 (0.0405)	
training:	Epoch: [27][660/817]	Loss 0.0067 (0.0404)	
training:	Epoch: [27][661/817]	Loss 0.0076 (0.0404)	
training:	Epoch: [27][662/817]	Loss 0.0095 (0.0403)	
training:	Epoch: [27][663/817]	Loss 0.0065 (0.0403)	
training:	Epoch: [27][664/817]	Loss 0.0083 (0.0402)	
training:	Epoch: [27][665/817]	Loss 0.0179 (0.0402)	
training:	Epoch: [27][666/817]	Loss 0.0071 (0.0401)	
training:	Epoch: [27][667/817]	Loss 0.0072 (0.0401)	
training:	Epoch: [27][668/817]	Loss 0.3723 (0.0406)	
training:	Epoch: [27][669/817]	Loss 0.0191 (0.0406)	
training:	Epoch: [27][670/817]	Loss 0.6413 (0.0415)	
training:	Epoch: [27][671/817]	Loss 0.0071 (0.0414)	
training:	Epoch: [27][672/817]	Loss 0.0068 (0.0414)	
training:	Epoch: [27][673/817]	Loss 0.0069 (0.0413)	
training:	Epoch: [27][674/817]	Loss 0.0126 (0.0413)	
training:	Epoch: [27][675/817]	Loss 0.0066 (0.0412)	
training:	Epoch: [27][676/817]	Loss 0.0067 (0.0412)	
training:	Epoch: [27][677/817]	Loss 0.0073 (0.0411)	
training:	Epoch: [27][678/817]	Loss 0.0060 (0.0411)	
training:	Epoch: [27][679/817]	Loss 0.0086 (0.0410)	
training:	Epoch: [27][680/817]	Loss 0.0091 (0.0410)	
training:	Epoch: [27][681/817]	Loss 0.6455 (0.0419)	
training:	Epoch: [27][682/817]	Loss 0.0066 (0.0418)	
training:	Epoch: [27][683/817]	Loss 0.0069 (0.0417)	
training:	Epoch: [27][684/817]	Loss 0.4595 (0.0424)	
training:	Epoch: [27][685/817]	Loss 0.6353 (0.0432)	
training:	Epoch: [27][686/817]	Loss 0.0088 (0.0432)	
training:	Epoch: [27][687/817]	Loss 0.0064 (0.0431)	
training:	Epoch: [27][688/817]	Loss 0.0143 (0.0431)	
training:	Epoch: [27][689/817]	Loss 0.0075 (0.0430)	
training:	Epoch: [27][690/817]	Loss 0.0099 (0.0430)	
training:	Epoch: [27][691/817]	Loss 0.0069 (0.0429)	
training:	Epoch: [27][692/817]	Loss 0.0074 (0.0429)	
training:	Epoch: [27][693/817]	Loss 0.0064 (0.0428)	
training:	Epoch: [27][694/817]	Loss 0.0087 (0.0428)	
training:	Epoch: [27][695/817]	Loss 0.0088 (0.0427)	
training:	Epoch: [27][696/817]	Loss 0.0075 (0.0427)	
training:	Epoch: [27][697/817]	Loss 0.0066 (0.0426)	
training:	Epoch: [27][698/817]	Loss 0.0065 (0.0426)	
training:	Epoch: [27][699/817]	Loss 0.0066 (0.0425)	
training:	Epoch: [27][700/817]	Loss 0.6437 (0.0434)	
training:	Epoch: [27][701/817]	Loss 0.0103 (0.0433)	
training:	Epoch: [27][702/817]	Loss 0.0064 (0.0433)	
training:	Epoch: [27][703/817]	Loss 0.0481 (0.0433)	
training:	Epoch: [27][704/817]	Loss 0.0082 (0.0432)	
training:	Epoch: [27][705/817]	Loss 0.0080 (0.0432)	
training:	Epoch: [27][706/817]	Loss 0.0078 (0.0431)	
training:	Epoch: [27][707/817]	Loss 0.0090 (0.0431)	
training:	Epoch: [27][708/817]	Loss 0.0070 (0.0430)	
training:	Epoch: [27][709/817]	Loss 0.0083 (0.0430)	
training:	Epoch: [27][710/817]	Loss 0.0073 (0.0429)	
training:	Epoch: [27][711/817]	Loss 0.0074 (0.0429)	
training:	Epoch: [27][712/817]	Loss 0.0087 (0.0428)	
training:	Epoch: [27][713/817]	Loss 0.0080 (0.0428)	
training:	Epoch: [27][714/817]	Loss 0.0068 (0.0427)	
training:	Epoch: [27][715/817]	Loss 0.0077 (0.0427)	
training:	Epoch: [27][716/817]	Loss 0.0083 (0.0426)	
training:	Epoch: [27][717/817]	Loss 0.0079 (0.0426)	
training:	Epoch: [27][718/817]	Loss 0.0070 (0.0425)	
training:	Epoch: [27][719/817]	Loss 0.0082 (0.0425)	
training:	Epoch: [27][720/817]	Loss 0.0076 (0.0424)	
training:	Epoch: [27][721/817]	Loss 0.0078 (0.0424)	
training:	Epoch: [27][722/817]	Loss 0.0080 (0.0424)	
training:	Epoch: [27][723/817]	Loss 0.0081 (0.0423)	
training:	Epoch: [27][724/817]	Loss 0.6327 (0.0431)	
training:	Epoch: [27][725/817]	Loss 0.0078 (0.0431)	
training:	Epoch: [27][726/817]	Loss 0.0067 (0.0430)	
training:	Epoch: [27][727/817]	Loss 0.0081 (0.0430)	
training:	Epoch: [27][728/817]	Loss 0.0758 (0.0430)	
training:	Epoch: [27][729/817]	Loss 0.0078 (0.0430)	
training:	Epoch: [27][730/817]	Loss 0.0075 (0.0429)	
training:	Epoch: [27][731/817]	Loss 0.2591 (0.0432)	
training:	Epoch: [27][732/817]	Loss 0.6140 (0.0440)	
training:	Epoch: [27][733/817]	Loss 0.0062 (0.0439)	
training:	Epoch: [27][734/817]	Loss 0.0072 (0.0439)	
training:	Epoch: [27][735/817]	Loss 0.0078 (0.0438)	
training:	Epoch: [27][736/817]	Loss 0.6044 (0.0446)	
training:	Epoch: [27][737/817]	Loss 0.0072 (0.0446)	
training:	Epoch: [27][738/817]	Loss 0.0085 (0.0445)	
training:	Epoch: [27][739/817]	Loss 0.0106 (0.0445)	
training:	Epoch: [27][740/817]	Loss 0.0074 (0.0444)	
training:	Epoch: [27][741/817]	Loss 0.0077 (0.0444)	
training:	Epoch: [27][742/817]	Loss 0.0173 (0.0443)	
training:	Epoch: [27][743/817]	Loss 0.0079 (0.0443)	
training:	Epoch: [27][744/817]	Loss 0.0212 (0.0442)	
training:	Epoch: [27][745/817]	Loss 0.0094 (0.0442)	
training:	Epoch: [27][746/817]	Loss 0.0347 (0.0442)	
training:	Epoch: [27][747/817]	Loss 0.0086 (0.0441)	
training:	Epoch: [27][748/817]	Loss 0.1914 (0.0443)	
training:	Epoch: [27][749/817]	Loss 0.0153 (0.0443)	
training:	Epoch: [27][750/817]	Loss 0.0092 (0.0443)	
training:	Epoch: [27][751/817]	Loss 0.0178 (0.0442)	
training:	Epoch: [27][752/817]	Loss 0.0087 (0.0442)	
training:	Epoch: [27][753/817]	Loss 0.0072 (0.0441)	
training:	Epoch: [27][754/817]	Loss 0.0079 (0.0441)	
training:	Epoch: [27][755/817]	Loss 0.0077 (0.0440)	
training:	Epoch: [27][756/817]	Loss 0.0076 (0.0440)	
training:	Epoch: [27][757/817]	Loss 0.0076 (0.0439)	
training:	Epoch: [27][758/817]	Loss 0.5842 (0.0446)	
training:	Epoch: [27][759/817]	Loss 0.0078 (0.0446)	
training:	Epoch: [27][760/817]	Loss 0.0079 (0.0445)	
training:	Epoch: [27][761/817]	Loss 0.0098 (0.0445)	
training:	Epoch: [27][762/817]	Loss 0.0080 (0.0444)	
training:	Epoch: [27][763/817]	Loss 0.0091 (0.0444)	
training:	Epoch: [27][764/817]	Loss 0.0080 (0.0444)	
training:	Epoch: [27][765/817]	Loss 0.0091 (0.0443)	
training:	Epoch: [27][766/817]	Loss 0.0122 (0.0443)	
training:	Epoch: [27][767/817]	Loss 0.0081 (0.0442)	
training:	Epoch: [27][768/817]	Loss 0.2690 (0.0445)	
training:	Epoch: [27][769/817]	Loss 0.0085 (0.0445)	
training:	Epoch: [27][770/817]	Loss 0.0072 (0.0444)	
training:	Epoch: [27][771/817]	Loss 0.0077 (0.0444)	
training:	Epoch: [27][772/817]	Loss 0.0088 (0.0443)	
training:	Epoch: [27][773/817]	Loss 0.0081 (0.0443)	
training:	Epoch: [27][774/817]	Loss 0.0082 (0.0442)	
training:	Epoch: [27][775/817]	Loss 0.0075 (0.0442)	
training:	Epoch: [27][776/817]	Loss 0.0085 (0.0441)	
training:	Epoch: [27][777/817]	Loss 0.0079 (0.0441)	
training:	Epoch: [27][778/817]	Loss 0.0084 (0.0440)	
training:	Epoch: [27][779/817]	Loss 0.6437 (0.0448)	
training:	Epoch: [27][780/817]	Loss 0.0083 (0.0448)	
training:	Epoch: [27][781/817]	Loss 0.2171 (0.0450)	
training:	Epoch: [27][782/817]	Loss 0.0061 (0.0449)	
training:	Epoch: [27][783/817]	Loss 0.6205 (0.0457)	
training:	Epoch: [27][784/817]	Loss 0.0462 (0.0457)	
training:	Epoch: [27][785/817]	Loss 0.0086 (0.0456)	
training:	Epoch: [27][786/817]	Loss 0.0097 (0.0456)	
training:	Epoch: [27][787/817]	Loss 0.0087 (0.0455)	
training:	Epoch: [27][788/817]	Loss 0.0082 (0.0455)	
training:	Epoch: [27][789/817]	Loss 0.0076 (0.0454)	
training:	Epoch: [27][790/817]	Loss 0.0070 (0.0454)	
training:	Epoch: [27][791/817]	Loss 0.0226 (0.0454)	
training:	Epoch: [27][792/817]	Loss 0.0104 (0.0453)	
training:	Epoch: [27][793/817]	Loss 0.0075 (0.0453)	
training:	Epoch: [27][794/817]	Loss 0.0074 (0.0452)	
training:	Epoch: [27][795/817]	Loss 0.0076 (0.0452)	
training:	Epoch: [27][796/817]	Loss 0.0083 (0.0451)	
training:	Epoch: [27][797/817]	Loss 0.0081 (0.0451)	
training:	Epoch: [27][798/817]	Loss 0.0080 (0.0450)	
training:	Epoch: [27][799/817]	Loss 0.0072 (0.0450)	
training:	Epoch: [27][800/817]	Loss 0.0247 (0.0450)	
training:	Epoch: [27][801/817]	Loss 0.0150 (0.0449)	
training:	Epoch: [27][802/817]	Loss 0.0103 (0.0449)	
training:	Epoch: [27][803/817]	Loss 0.0086 (0.0448)	
training:	Epoch: [27][804/817]	Loss 0.0076 (0.0448)	
training:	Epoch: [27][805/817]	Loss 0.0144 (0.0448)	
training:	Epoch: [27][806/817]	Loss 0.0091 (0.0447)	
training:	Epoch: [27][807/817]	Loss 0.0105 (0.0447)	
training:	Epoch: [27][808/817]	Loss 0.0089 (0.0446)	
training:	Epoch: [27][809/817]	Loss 0.0080 (0.0446)	
training:	Epoch: [27][810/817]	Loss 0.0082 (0.0445)	
training:	Epoch: [27][811/817]	Loss 0.0076 (0.0445)	
training:	Epoch: [27][812/817]	Loss 0.0078 (0.0444)	
training:	Epoch: [27][813/817]	Loss 0.0087 (0.0444)	
training:	Epoch: [27][814/817]	Loss 0.0135 (0.0444)	
training:	Epoch: [27][815/817]	Loss 0.0071 (0.0443)	
training:	Epoch: [27][816/817]	Loss 0.0074 (0.0443)	
training:	Epoch: [27][817/817]	Loss 0.0116 (0.0442)	
Training:	 Loss: 0.0442

Training:	 ACC: 0.9923 0.9922 0.9909 0.9936
Validation:	 ACC: 0.7910 0.7903 0.7748 0.8072
Validation:	 Best_BACC: 0.8223 0.8218 0.8127 0.8318
Validation:	 Loss: 0.9005
Pretraining:	Epoch 28/200
----------
training:	Epoch: [28][1/817]	Loss 0.0080 (0.0080)	
training:	Epoch: [28][2/817]	Loss 0.0129 (0.0105)	
training:	Epoch: [28][3/817]	Loss 0.0064 (0.0091)	
training:	Epoch: [28][4/817]	Loss 0.0069 (0.0086)	
training:	Epoch: [28][5/817]	Loss 0.0093 (0.0087)	
training:	Epoch: [28][6/817]	Loss 0.0077 (0.0085)	
training:	Epoch: [28][7/817]	Loss 0.0173 (0.0098)	
training:	Epoch: [28][8/817]	Loss 0.0082 (0.0096)	
training:	Epoch: [28][9/817]	Loss 0.0075 (0.0094)	
training:	Epoch: [28][10/817]	Loss 0.0079 (0.0092)	
training:	Epoch: [28][11/817]	Loss 0.0076 (0.0091)	
training:	Epoch: [28][12/817]	Loss 0.0074 (0.0089)	
training:	Epoch: [28][13/817]	Loss 0.0078 (0.0088)	
training:	Epoch: [28][14/817]	Loss 0.0078 (0.0088)	
training:	Epoch: [28][15/817]	Loss 0.0070 (0.0087)	
training:	Epoch: [28][16/817]	Loss 0.0078 (0.0086)	
training:	Epoch: [28][17/817]	Loss 0.0086 (0.0086)	
training:	Epoch: [28][18/817]	Loss 0.0068 (0.0085)	
training:	Epoch: [28][19/817]	Loss 0.0104 (0.0086)	
training:	Epoch: [28][20/817]	Loss 0.0080 (0.0086)	
training:	Epoch: [28][21/817]	Loss 0.0076 (0.0085)	
training:	Epoch: [28][22/817]	Loss 0.0084 (0.0085)	
training:	Epoch: [28][23/817]	Loss 0.0116 (0.0087)	
training:	Epoch: [28][24/817]	Loss 0.0080 (0.0086)	
training:	Epoch: [28][25/817]	Loss 0.0110 (0.0087)	
training:	Epoch: [28][26/817]	Loss 0.0082 (0.0087)	
training:	Epoch: [28][27/817]	Loss 0.0109 (0.0088)	
training:	Epoch: [28][28/817]	Loss 0.0084 (0.0088)	
training:	Epoch: [28][29/817]	Loss 0.0241 (0.0093)	
training:	Epoch: [28][30/817]	Loss 0.0068 (0.0092)	
training:	Epoch: [28][31/817]	Loss 0.0074 (0.0092)	
training:	Epoch: [28][32/817]	Loss 0.0091 (0.0092)	
training:	Epoch: [28][33/817]	Loss 0.0089 (0.0091)	
training:	Epoch: [28][34/817]	Loss 0.0081 (0.0091)	
training:	Epoch: [28][35/817]	Loss 0.0078 (0.0091)	
training:	Epoch: [28][36/817]	Loss 0.0078 (0.0090)	
training:	Epoch: [28][37/817]	Loss 0.0068 (0.0090)	
training:	Epoch: [28][38/817]	Loss 0.0069 (0.0089)	
training:	Epoch: [28][39/817]	Loss 0.4786 (0.0210)	
training:	Epoch: [28][40/817]	Loss 0.0066 (0.0206)	
training:	Epoch: [28][41/817]	Loss 0.6398 (0.0357)	
training:	Epoch: [28][42/817]	Loss 0.0120 (0.0352)	
training:	Epoch: [28][43/817]	Loss 0.0074 (0.0345)	
training:	Epoch: [28][44/817]	Loss 0.0101 (0.0340)	
training:	Epoch: [28][45/817]	Loss 0.5889 (0.0463)	
training:	Epoch: [28][46/817]	Loss 0.0071 (0.0454)	
training:	Epoch: [28][47/817]	Loss 0.0080 (0.0446)	
training:	Epoch: [28][48/817]	Loss 0.0098 (0.0439)	
training:	Epoch: [28][49/817]	Loss 0.0077 (0.0432)	
training:	Epoch: [28][50/817]	Loss 0.0070 (0.0424)	
training:	Epoch: [28][51/817]	Loss 0.0067 (0.0417)	
training:	Epoch: [28][52/817]	Loss 0.0083 (0.0411)	
training:	Epoch: [28][53/817]	Loss 0.0087 (0.0405)	
training:	Epoch: [28][54/817]	Loss 0.0071 (0.0399)	
training:	Epoch: [28][55/817]	Loss 0.0124 (0.0394)	
training:	Epoch: [28][56/817]	Loss 0.0093 (0.0388)	
training:	Epoch: [28][57/817]	Loss 0.0070 (0.0383)	
training:	Epoch: [28][58/817]	Loss 0.0080 (0.0378)	
training:	Epoch: [28][59/817]	Loss 0.0075 (0.0372)	
training:	Epoch: [28][60/817]	Loss 0.0098 (0.0368)	
training:	Epoch: [28][61/817]	Loss 0.0073 (0.0363)	
training:	Epoch: [28][62/817]	Loss 0.0075 (0.0358)	
training:	Epoch: [28][63/817]	Loss 0.0070 (0.0354)	
training:	Epoch: [28][64/817]	Loss 0.0065 (0.0349)	
training:	Epoch: [28][65/817]	Loss 0.0072 (0.0345)	
training:	Epoch: [28][66/817]	Loss 0.0112 (0.0342)	
training:	Epoch: [28][67/817]	Loss 0.0107 (0.0338)	
training:	Epoch: [28][68/817]	Loss 0.0354 (0.0338)	
training:	Epoch: [28][69/817]	Loss 0.0082 (0.0335)	
training:	Epoch: [28][70/817]	Loss 0.5834 (0.0413)	
training:	Epoch: [28][71/817]	Loss 0.0066 (0.0408)	
training:	Epoch: [28][72/817]	Loss 0.5343 (0.0477)	
training:	Epoch: [28][73/817]	Loss 0.0085 (0.0471)	
training:	Epoch: [28][74/817]	Loss 0.0071 (0.0466)	
training:	Epoch: [28][75/817]	Loss 0.0072 (0.0461)	
training:	Epoch: [28][76/817]	Loss 0.0090 (0.0456)	
training:	Epoch: [28][77/817]	Loss 0.0079 (0.0451)	
training:	Epoch: [28][78/817]	Loss 0.0061 (0.0446)	
training:	Epoch: [28][79/817]	Loss 0.0080 (0.0441)	
training:	Epoch: [28][80/817]	Loss 0.0071 (0.0437)	
training:	Epoch: [28][81/817]	Loss 0.0109 (0.0433)	
training:	Epoch: [28][82/817]	Loss 0.0083 (0.0428)	
training:	Epoch: [28][83/817]	Loss 0.0101 (0.0424)	
training:	Epoch: [28][84/817]	Loss 0.0076 (0.0420)	
training:	Epoch: [28][85/817]	Loss 0.0080 (0.0416)	
training:	Epoch: [28][86/817]	Loss 0.5459 (0.0475)	
training:	Epoch: [28][87/817]	Loss 0.0091 (0.0471)	
training:	Epoch: [28][88/817]	Loss 0.0082 (0.0466)	
training:	Epoch: [28][89/817]	Loss 0.0069 (0.0462)	
training:	Epoch: [28][90/817]	Loss 0.0071 (0.0457)	
training:	Epoch: [28][91/817]	Loss 0.0093 (0.0453)	
training:	Epoch: [28][92/817]	Loss 0.0094 (0.0449)	
training:	Epoch: [28][93/817]	Loss 0.0080 (0.0445)	
training:	Epoch: [28][94/817]	Loss 0.0082 (0.0442)	
training:	Epoch: [28][95/817]	Loss 0.0088 (0.0438)	
training:	Epoch: [28][96/817]	Loss 0.0087 (0.0434)	
training:	Epoch: [28][97/817]	Loss 0.0081 (0.0431)	
training:	Epoch: [28][98/817]	Loss 0.0085 (0.0427)	
training:	Epoch: [28][99/817]	Loss 0.0097 (0.0424)	
training:	Epoch: [28][100/817]	Loss 0.0107 (0.0420)	
training:	Epoch: [28][101/817]	Loss 0.0085 (0.0417)	
training:	Epoch: [28][102/817]	Loss 0.0135 (0.0414)	
training:	Epoch: [28][103/817]	Loss 0.0087 (0.0411)	
training:	Epoch: [28][104/817]	Loss 0.0103 (0.0408)	
training:	Epoch: [28][105/817]	Loss 0.0070 (0.0405)	
training:	Epoch: [28][106/817]	Loss 0.0061 (0.0402)	
training:	Epoch: [28][107/817]	Loss 0.0078 (0.0399)	
training:	Epoch: [28][108/817]	Loss 0.6393 (0.0454)	
training:	Epoch: [28][109/817]	Loss 0.0094 (0.0451)	
training:	Epoch: [28][110/817]	Loss 0.0080 (0.0448)	
training:	Epoch: [28][111/817]	Loss 0.0126 (0.0445)	
training:	Epoch: [28][112/817]	Loss 0.0094 (0.0442)	
training:	Epoch: [28][113/817]	Loss 0.0148 (0.0439)	
training:	Epoch: [28][114/817]	Loss 0.0077 (0.0436)	
training:	Epoch: [28][115/817]	Loss 0.0075 (0.0433)	
training:	Epoch: [28][116/817]	Loss 0.0097 (0.0430)	
training:	Epoch: [28][117/817]	Loss 0.0075 (0.0427)	
training:	Epoch: [28][118/817]	Loss 0.6278 (0.0476)	
training:	Epoch: [28][119/817]	Loss 0.0069 (0.0473)	
training:	Epoch: [28][120/817]	Loss 0.0083 (0.0470)	
training:	Epoch: [28][121/817]	Loss 0.0093 (0.0467)	
training:	Epoch: [28][122/817]	Loss 0.0074 (0.0463)	
training:	Epoch: [28][123/817]	Loss 0.0085 (0.0460)	
training:	Epoch: [28][124/817]	Loss 0.0079 (0.0457)	
training:	Epoch: [28][125/817]	Loss 0.0085 (0.0454)	
training:	Epoch: [28][126/817]	Loss 0.0081 (0.0451)	
training:	Epoch: [28][127/817]	Loss 0.0076 (0.0448)	
training:	Epoch: [28][128/817]	Loss 0.0084 (0.0445)	
training:	Epoch: [28][129/817]	Loss 0.0092 (0.0443)	
training:	Epoch: [28][130/817]	Loss 0.0076 (0.0440)	
training:	Epoch: [28][131/817]	Loss 0.0080 (0.0437)	
training:	Epoch: [28][132/817]	Loss 0.0068 (0.0434)	
training:	Epoch: [28][133/817]	Loss 0.0070 (0.0432)	
training:	Epoch: [28][134/817]	Loss 0.0071 (0.0429)	
training:	Epoch: [28][135/817]	Loss 0.0088 (0.0426)	
training:	Epoch: [28][136/817]	Loss 0.0083 (0.0424)	
training:	Epoch: [28][137/817]	Loss 0.0069 (0.0421)	
training:	Epoch: [28][138/817]	Loss 0.0082 (0.0419)	
training:	Epoch: [28][139/817]	Loss 0.0073 (0.0416)	
training:	Epoch: [28][140/817]	Loss 0.5518 (0.0453)	
training:	Epoch: [28][141/817]	Loss 0.0105 (0.0450)	
training:	Epoch: [28][142/817]	Loss 0.0076 (0.0448)	
training:	Epoch: [28][143/817]	Loss 0.0081 (0.0445)	
training:	Epoch: [28][144/817]	Loss 0.0069 (0.0442)	
training:	Epoch: [28][145/817]	Loss 0.6149 (0.0482)	
training:	Epoch: [28][146/817]	Loss 0.0106 (0.0479)	
training:	Epoch: [28][147/817]	Loss 0.0071 (0.0476)	
training:	Epoch: [28][148/817]	Loss 0.0086 (0.0474)	
training:	Epoch: [28][149/817]	Loss 0.0063 (0.0471)	
training:	Epoch: [28][150/817]	Loss 0.0086 (0.0468)	
training:	Epoch: [28][151/817]	Loss 0.0088 (0.0466)	
training:	Epoch: [28][152/817]	Loss 0.0071 (0.0463)	
training:	Epoch: [28][153/817]	Loss 0.0075 (0.0461)	
training:	Epoch: [28][154/817]	Loss 0.0115 (0.0459)	
training:	Epoch: [28][155/817]	Loss 0.0070 (0.0456)	
training:	Epoch: [28][156/817]	Loss 0.0125 (0.0454)	
training:	Epoch: [28][157/817]	Loss 0.0078 (0.0452)	
training:	Epoch: [28][158/817]	Loss 0.0076 (0.0449)	
training:	Epoch: [28][159/817]	Loss 0.0092 (0.0447)	
training:	Epoch: [28][160/817]	Loss 0.0065 (0.0445)	
training:	Epoch: [28][161/817]	Loss 0.0094 (0.0442)	
training:	Epoch: [28][162/817]	Loss 0.0073 (0.0440)	
training:	Epoch: [28][163/817]	Loss 0.0074 (0.0438)	
training:	Epoch: [28][164/817]	Loss 0.0137 (0.0436)	
training:	Epoch: [28][165/817]	Loss 0.0082 (0.0434)	
training:	Epoch: [28][166/817]	Loss 0.0073 (0.0432)	
training:	Epoch: [28][167/817]	Loss 0.0078 (0.0430)	
training:	Epoch: [28][168/817]	Loss 0.0063 (0.0427)	
training:	Epoch: [28][169/817]	Loss 0.0081 (0.0425)	
training:	Epoch: [28][170/817]	Loss 0.0073 (0.0423)	
training:	Epoch: [28][171/817]	Loss 0.0073 (0.0421)	
training:	Epoch: [28][172/817]	Loss 0.0072 (0.0419)	
training:	Epoch: [28][173/817]	Loss 0.0104 (0.0417)	
training:	Epoch: [28][174/817]	Loss 0.0079 (0.0415)	
training:	Epoch: [28][175/817]	Loss 0.0068 (0.0413)	
training:	Epoch: [28][176/817]	Loss 0.0078 (0.0412)	
training:	Epoch: [28][177/817]	Loss 0.0071 (0.0410)	
training:	Epoch: [28][178/817]	Loss 0.0071 (0.0408)	
training:	Epoch: [28][179/817]	Loss 0.0083 (0.0406)	
training:	Epoch: [28][180/817]	Loss 0.0074 (0.0404)	
training:	Epoch: [28][181/817]	Loss 0.0072 (0.0402)	
training:	Epoch: [28][182/817]	Loss 0.0099 (0.0401)	
training:	Epoch: [28][183/817]	Loss 0.0068 (0.0399)	
training:	Epoch: [28][184/817]	Loss 0.0081 (0.0397)	
training:	Epoch: [28][185/817]	Loss 0.0076 (0.0395)	
training:	Epoch: [28][186/817]	Loss 0.0087 (0.0394)	
training:	Epoch: [28][187/817]	Loss 0.0069 (0.0392)	
training:	Epoch: [28][188/817]	Loss 0.0085 (0.0390)	
training:	Epoch: [28][189/817]	Loss 0.0083 (0.0389)	
training:	Epoch: [28][190/817]	Loss 0.0072 (0.0387)	
training:	Epoch: [28][191/817]	Loss 0.0078 (0.0385)	
training:	Epoch: [28][192/817]	Loss 0.0058 (0.0384)	
training:	Epoch: [28][193/817]	Loss 0.0085 (0.0382)	
training:	Epoch: [28][194/817]	Loss 0.0072 (0.0381)	
training:	Epoch: [28][195/817]	Loss 0.6497 (0.0412)	
training:	Epoch: [28][196/817]	Loss 0.0128 (0.0410)	
training:	Epoch: [28][197/817]	Loss 0.0079 (0.0409)	
training:	Epoch: [28][198/817]	Loss 0.5401 (0.0434)	
training:	Epoch: [28][199/817]	Loss 0.0074 (0.0432)	
training:	Epoch: [28][200/817]	Loss 0.0060 (0.0430)	
training:	Epoch: [28][201/817]	Loss 0.0072 (0.0429)	
training:	Epoch: [28][202/817]	Loss 0.0109 (0.0427)	
training:	Epoch: [28][203/817]	Loss 0.0085 (0.0425)	
training:	Epoch: [28][204/817]	Loss 0.0076 (0.0424)	
training:	Epoch: [28][205/817]	Loss 0.0069 (0.0422)	
training:	Epoch: [28][206/817]	Loss 0.0064 (0.0420)	
training:	Epoch: [28][207/817]	Loss 0.0107 (0.0419)	
training:	Epoch: [28][208/817]	Loss 0.0159 (0.0417)	
training:	Epoch: [28][209/817]	Loss 0.0145 (0.0416)	
training:	Epoch: [28][210/817]	Loss 0.0085 (0.0414)	
training:	Epoch: [28][211/817]	Loss 0.0068 (0.0413)	
training:	Epoch: [28][212/817]	Loss 0.0071 (0.0411)	
training:	Epoch: [28][213/817]	Loss 0.0087 (0.0410)	
training:	Epoch: [28][214/817]	Loss 0.0076 (0.0408)	
training:	Epoch: [28][215/817]	Loss 0.0078 (0.0407)	
training:	Epoch: [28][216/817]	Loss 0.0066 (0.0405)	
training:	Epoch: [28][217/817]	Loss 0.0077 (0.0403)	
training:	Epoch: [28][218/817]	Loss 0.0100 (0.0402)	
training:	Epoch: [28][219/817]	Loss 0.0079 (0.0401)	
training:	Epoch: [28][220/817]	Loss 0.0072 (0.0399)	
training:	Epoch: [28][221/817]	Loss 0.0123 (0.0398)	
training:	Epoch: [28][222/817]	Loss 0.0070 (0.0396)	
training:	Epoch: [28][223/817]	Loss 0.0079 (0.0395)	
training:	Epoch: [28][224/817]	Loss 0.0066 (0.0393)	
training:	Epoch: [28][225/817]	Loss 0.0080 (0.0392)	
training:	Epoch: [28][226/817]	Loss 0.0090 (0.0391)	
training:	Epoch: [28][227/817]	Loss 0.0084 (0.0389)	
training:	Epoch: [28][228/817]	Loss 0.0097 (0.0388)	
training:	Epoch: [28][229/817]	Loss 0.0074 (0.0387)	
training:	Epoch: [28][230/817]	Loss 0.0064 (0.0385)	
training:	Epoch: [28][231/817]	Loss 0.0077 (0.0384)	
training:	Epoch: [28][232/817]	Loss 0.0063 (0.0383)	
training:	Epoch: [28][233/817]	Loss 0.1150 (0.0386)	
training:	Epoch: [28][234/817]	Loss 0.0081 (0.0385)	
training:	Epoch: [28][235/817]	Loss 0.0081 (0.0383)	
training:	Epoch: [28][236/817]	Loss 0.0085 (0.0382)	
training:	Epoch: [28][237/817]	Loss 0.0066 (0.0381)	
training:	Epoch: [28][238/817]	Loss 0.0075 (0.0379)	
training:	Epoch: [28][239/817]	Loss 0.0083 (0.0378)	
training:	Epoch: [28][240/817]	Loss 0.4065 (0.0394)	
training:	Epoch: [28][241/817]	Loss 0.0072 (0.0392)	
training:	Epoch: [28][242/817]	Loss 0.0101 (0.0391)	
training:	Epoch: [28][243/817]	Loss 0.0081 (0.0390)	
training:	Epoch: [28][244/817]	Loss 0.0077 (0.0388)	
training:	Epoch: [28][245/817]	Loss 0.0067 (0.0387)	
training:	Epoch: [28][246/817]	Loss 0.0066 (0.0386)	
training:	Epoch: [28][247/817]	Loss 0.0079 (0.0385)	
training:	Epoch: [28][248/817]	Loss 0.0075 (0.0383)	
training:	Epoch: [28][249/817]	Loss 0.0068 (0.0382)	
training:	Epoch: [28][250/817]	Loss 0.0096 (0.0381)	
training:	Epoch: [28][251/817]	Loss 0.0074 (0.0380)	
training:	Epoch: [28][252/817]	Loss 0.0069 (0.0378)	
training:	Epoch: [28][253/817]	Loss 0.0091 (0.0377)	
training:	Epoch: [28][254/817]	Loss 0.0114 (0.0376)	
training:	Epoch: [28][255/817]	Loss 0.0066 (0.0375)	
training:	Epoch: [28][256/817]	Loss 0.0068 (0.0374)	
training:	Epoch: [28][257/817]	Loss 0.0084 (0.0373)	
training:	Epoch: [28][258/817]	Loss 0.0155 (0.0372)	
training:	Epoch: [28][259/817]	Loss 0.0328 (0.0372)	
training:	Epoch: [28][260/817]	Loss 0.0117 (0.0371)	
training:	Epoch: [28][261/817]	Loss 0.0072 (0.0370)	
training:	Epoch: [28][262/817]	Loss 0.0095 (0.0369)	
training:	Epoch: [28][263/817]	Loss 0.4741 (0.0385)	
training:	Epoch: [28][264/817]	Loss 0.6512 (0.0408)	
training:	Epoch: [28][265/817]	Loss 0.0083 (0.0407)	
training:	Epoch: [28][266/817]	Loss 0.0096 (0.0406)	
training:	Epoch: [28][267/817]	Loss 0.0071 (0.0405)	
training:	Epoch: [28][268/817]	Loss 0.0062 (0.0403)	
training:	Epoch: [28][269/817]	Loss 0.0076 (0.0402)	
training:	Epoch: [28][270/817]	Loss 0.0076 (0.0401)	
training:	Epoch: [28][271/817]	Loss 0.0069 (0.0400)	
training:	Epoch: [28][272/817]	Loss 0.0104 (0.0399)	
training:	Epoch: [28][273/817]	Loss 0.0088 (0.0398)	
training:	Epoch: [28][274/817]	Loss 0.0079 (0.0396)	
training:	Epoch: [28][275/817]	Loss 0.0077 (0.0395)	
training:	Epoch: [28][276/817]	Loss 0.0067 (0.0394)	
training:	Epoch: [28][277/817]	Loss 0.0071 (0.0393)	
training:	Epoch: [28][278/817]	Loss 0.0088 (0.0392)	
training:	Epoch: [28][279/817]	Loss 0.0067 (0.0391)	
training:	Epoch: [28][280/817]	Loss 0.0078 (0.0390)	
training:	Epoch: [28][281/817]	Loss 0.0073 (0.0388)	
training:	Epoch: [28][282/817]	Loss 0.0080 (0.0387)	
training:	Epoch: [28][283/817]	Loss 0.0072 (0.0386)	
training:	Epoch: [28][284/817]	Loss 0.0079 (0.0385)	
training:	Epoch: [28][285/817]	Loss 0.0109 (0.0384)	
training:	Epoch: [28][286/817]	Loss 0.0085 (0.0383)	
training:	Epoch: [28][287/817]	Loss 0.0102 (0.0382)	
training:	Epoch: [28][288/817]	Loss 1.1096 (0.0419)	
training:	Epoch: [28][289/817]	Loss 0.0087 (0.0418)	
training:	Epoch: [28][290/817]	Loss 0.0074 (0.0417)	
training:	Epoch: [28][291/817]	Loss 0.0073 (0.0416)	
training:	Epoch: [28][292/817]	Loss 0.0091 (0.0415)	
training:	Epoch: [28][293/817]	Loss 0.0079 (0.0414)	
training:	Epoch: [28][294/817]	Loss 0.0078 (0.0412)	
training:	Epoch: [28][295/817]	Loss 0.0065 (0.0411)	
training:	Epoch: [28][296/817]	Loss 0.0086 (0.0410)	
training:	Epoch: [28][297/817]	Loss 0.0079 (0.0409)	
training:	Epoch: [28][298/817]	Loss 0.0064 (0.0408)	
training:	Epoch: [28][299/817]	Loss 0.0069 (0.0407)	
training:	Epoch: [28][300/817]	Loss 0.0079 (0.0406)	
training:	Epoch: [28][301/817]	Loss 0.0127 (0.0405)	
training:	Epoch: [28][302/817]	Loss 0.0086 (0.0404)	
training:	Epoch: [28][303/817]	Loss 0.0081 (0.0403)	
training:	Epoch: [28][304/817]	Loss 0.0088 (0.0402)	
training:	Epoch: [28][305/817]	Loss 0.0070 (0.0400)	
training:	Epoch: [28][306/817]	Loss 0.0080 (0.0399)	
training:	Epoch: [28][307/817]	Loss 0.0064 (0.0398)	
training:	Epoch: [28][308/817]	Loss 0.0103 (0.0397)	
training:	Epoch: [28][309/817]	Loss 0.0083 (0.0396)	
training:	Epoch: [28][310/817]	Loss 0.0067 (0.0395)	
training:	Epoch: [28][311/817]	Loss 0.0087 (0.0394)	
training:	Epoch: [28][312/817]	Loss 0.0078 (0.0393)	
training:	Epoch: [28][313/817]	Loss 0.0069 (0.0392)	
training:	Epoch: [28][314/817]	Loss 0.0064 (0.0391)	
training:	Epoch: [28][315/817]	Loss 0.0092 (0.0390)	
training:	Epoch: [28][316/817]	Loss 0.0086 (0.0389)	
training:	Epoch: [28][317/817]	Loss 0.0079 (0.0388)	
training:	Epoch: [28][318/817]	Loss 0.0073 (0.0387)	
training:	Epoch: [28][319/817]	Loss 0.0082 (0.0386)	
training:	Epoch: [28][320/817]	Loss 0.0076 (0.0385)	
training:	Epoch: [28][321/817]	Loss 0.0087 (0.0384)	
training:	Epoch: [28][322/817]	Loss 0.0090 (0.0384)	
training:	Epoch: [28][323/817]	Loss 0.0076 (0.0383)	
training:	Epoch: [28][324/817]	Loss 0.0081 (0.0382)	
training:	Epoch: [28][325/817]	Loss 0.0079 (0.0381)	
training:	Epoch: [28][326/817]	Loss 0.0081 (0.0380)	
training:	Epoch: [28][327/817]	Loss 0.0083 (0.0379)	
training:	Epoch: [28][328/817]	Loss 0.0070 (0.0378)	
training:	Epoch: [28][329/817]	Loss 0.0072 (0.0377)	
training:	Epoch: [28][330/817]	Loss 0.0068 (0.0376)	
training:	Epoch: [28][331/817]	Loss 0.0077 (0.0375)	
training:	Epoch: [28][332/817]	Loss 0.0081 (0.0374)	
training:	Epoch: [28][333/817]	Loss 0.0079 (0.0373)	
training:	Epoch: [28][334/817]	Loss 0.0074 (0.0373)	
training:	Epoch: [28][335/817]	Loss 0.0084 (0.0372)	
training:	Epoch: [28][336/817]	Loss 0.0061 (0.0371)	
training:	Epoch: [28][337/817]	Loss 0.0078 (0.0370)	
training:	Epoch: [28][338/817]	Loss 0.5622 (0.0385)	
training:	Epoch: [28][339/817]	Loss 0.0069 (0.0384)	
training:	Epoch: [28][340/817]	Loss 0.0067 (0.0384)	
training:	Epoch: [28][341/817]	Loss 0.0085 (0.0383)	
training:	Epoch: [28][342/817]	Loss 0.0071 (0.0382)	
training:	Epoch: [28][343/817]	Loss 0.0084 (0.0381)	
training:	Epoch: [28][344/817]	Loss 0.0080 (0.0380)	
training:	Epoch: [28][345/817]	Loss 0.5175 (0.0394)	
training:	Epoch: [28][346/817]	Loss 0.0069 (0.0393)	
training:	Epoch: [28][347/817]	Loss 0.0090 (0.0392)	
training:	Epoch: [28][348/817]	Loss 0.0082 (0.0391)	
training:	Epoch: [28][349/817]	Loss 0.0078 (0.0390)	
training:	Epoch: [28][350/817]	Loss 0.0073 (0.0389)	
training:	Epoch: [28][351/817]	Loss 0.0080 (0.0389)	
training:	Epoch: [28][352/817]	Loss 0.0069 (0.0388)	
training:	Epoch: [28][353/817]	Loss 0.0083 (0.0387)	
training:	Epoch: [28][354/817]	Loss 0.6159 (0.0403)	
training:	Epoch: [28][355/817]	Loss 0.6391 (0.0420)	
training:	Epoch: [28][356/817]	Loss 0.0090 (0.0419)	
training:	Epoch: [28][357/817]	Loss 0.0074 (0.0418)	
training:	Epoch: [28][358/817]	Loss 0.0090 (0.0417)	
training:	Epoch: [28][359/817]	Loss 0.0075 (0.0416)	
training:	Epoch: [28][360/817]	Loss 0.0084 (0.0415)	
training:	Epoch: [28][361/817]	Loss 0.0153 (0.0415)	
training:	Epoch: [28][362/817]	Loss 0.0087 (0.0414)	
training:	Epoch: [28][363/817]	Loss 0.0069 (0.0413)	
training:	Epoch: [28][364/817]	Loss 0.0080 (0.0412)	
training:	Epoch: [28][365/817]	Loss 0.0065 (0.0411)	
training:	Epoch: [28][366/817]	Loss 0.0084 (0.0410)	
training:	Epoch: [28][367/817]	Loss 0.0064 (0.0409)	
training:	Epoch: [28][368/817]	Loss 0.0076 (0.0408)	
training:	Epoch: [28][369/817]	Loss 0.0076 (0.0407)	
training:	Epoch: [28][370/817]	Loss 0.0082 (0.0406)	
training:	Epoch: [28][371/817]	Loss 0.0087 (0.0405)	
training:	Epoch: [28][372/817]	Loss 0.0086 (0.0405)	
training:	Epoch: [28][373/817]	Loss 0.0066 (0.0404)	
training:	Epoch: [28][374/817]	Loss 0.0063 (0.0403)	
training:	Epoch: [28][375/817]	Loss 0.0071 (0.0402)	
training:	Epoch: [28][376/817]	Loss 0.0070 (0.0401)	
training:	Epoch: [28][377/817]	Loss 0.0062 (0.0400)	
training:	Epoch: [28][378/817]	Loss 0.0097 (0.0399)	
training:	Epoch: [28][379/817]	Loss 0.0081 (0.0398)	
training:	Epoch: [28][380/817]	Loss 0.6268 (0.0414)	
training:	Epoch: [28][381/817]	Loss 0.0144 (0.0413)	
training:	Epoch: [28][382/817]	Loss 0.0091 (0.0412)	
training:	Epoch: [28][383/817]	Loss 0.0082 (0.0411)	
training:	Epoch: [28][384/817]	Loss 0.0085 (0.0411)	
training:	Epoch: [28][385/817]	Loss 0.0107 (0.0410)	
training:	Epoch: [28][386/817]	Loss 0.0084 (0.0409)	
training:	Epoch: [28][387/817]	Loss 0.0100 (0.0408)	
training:	Epoch: [28][388/817]	Loss 0.0077 (0.0407)	
training:	Epoch: [28][389/817]	Loss 0.0093 (0.0407)	
training:	Epoch: [28][390/817]	Loss 0.0071 (0.0406)	
training:	Epoch: [28][391/817]	Loss 0.0071 (0.0405)	
training:	Epoch: [28][392/817]	Loss 0.0073 (0.0404)	
training:	Epoch: [28][393/817]	Loss 0.0071 (0.0403)	
training:	Epoch: [28][394/817]	Loss 0.0089 (0.0402)	
training:	Epoch: [28][395/817]	Loss 0.0082 (0.0402)	
training:	Epoch: [28][396/817]	Loss 0.0064 (0.0401)	
training:	Epoch: [28][397/817]	Loss 0.0080 (0.0400)	
training:	Epoch: [28][398/817]	Loss 0.0070 (0.0399)	
training:	Epoch: [28][399/817]	Loss 0.5991 (0.0413)	
training:	Epoch: [28][400/817]	Loss 0.0069 (0.0412)	
training:	Epoch: [28][401/817]	Loss 0.0072 (0.0411)	
training:	Epoch: [28][402/817]	Loss 0.0080 (0.0411)	
training:	Epoch: [28][403/817]	Loss 0.0092 (0.0410)	
training:	Epoch: [28][404/817]	Loss 0.0071 (0.0409)	
training:	Epoch: [28][405/817]	Loss 0.0064 (0.0408)	
training:	Epoch: [28][406/817]	Loss 0.0085 (0.0407)	
training:	Epoch: [28][407/817]	Loss 0.0072 (0.0406)	
training:	Epoch: [28][408/817]	Loss 0.0068 (0.0406)	
training:	Epoch: [28][409/817]	Loss 0.0063 (0.0405)	
training:	Epoch: [28][410/817]	Loss 0.0076 (0.0404)	
training:	Epoch: [28][411/817]	Loss 0.0072 (0.0403)	
training:	Epoch: [28][412/817]	Loss 0.0088 (0.0402)	
training:	Epoch: [28][413/817]	Loss 0.0062 (0.0402)	
training:	Epoch: [28][414/817]	Loss 0.0086 (0.0401)	
training:	Epoch: [28][415/817]	Loss 0.0087 (0.0400)	
training:	Epoch: [28][416/817]	Loss 0.0075 (0.0399)	
training:	Epoch: [28][417/817]	Loss 0.0064 (0.0398)	
training:	Epoch: [28][418/817]	Loss 0.0091 (0.0398)	
training:	Epoch: [28][419/817]	Loss 0.0072 (0.0397)	
training:	Epoch: [28][420/817]	Loss 0.0074 (0.0396)	
training:	Epoch: [28][421/817]	Loss 0.0075 (0.0395)	
training:	Epoch: [28][422/817]	Loss 0.0069 (0.0395)	
training:	Epoch: [28][423/817]	Loss 0.0069 (0.0394)	
training:	Epoch: [28][424/817]	Loss 0.0071 (0.0393)	
training:	Epoch: [28][425/817]	Loss 0.0123 (0.0392)	
training:	Epoch: [28][426/817]	Loss 0.0061 (0.0392)	
training:	Epoch: [28][427/817]	Loss 0.0075 (0.0391)	
training:	Epoch: [28][428/817]	Loss 0.0062 (0.0390)	
training:	Epoch: [28][429/817]	Loss 0.0085 (0.0389)	
training:	Epoch: [28][430/817]	Loss 0.0070 (0.0389)	
training:	Epoch: [28][431/817]	Loss 0.0074 (0.0388)	
training:	Epoch: [28][432/817]	Loss 0.0070 (0.0387)	
training:	Epoch: [28][433/817]	Loss 0.0078 (0.0387)	
training:	Epoch: [28][434/817]	Loss 0.0080 (0.0386)	
training:	Epoch: [28][435/817]	Loss 0.0070 (0.0385)	
training:	Epoch: [28][436/817]	Loss 0.0067 (0.0384)	
training:	Epoch: [28][437/817]	Loss 0.0076 (0.0384)	
training:	Epoch: [28][438/817]	Loss 0.0082 (0.0383)	
training:	Epoch: [28][439/817]	Loss 0.0077 (0.0382)	
training:	Epoch: [28][440/817]	Loss 0.0066 (0.0382)	
training:	Epoch: [28][441/817]	Loss 0.0058 (0.0381)	
training:	Epoch: [28][442/817]	Loss 0.0078 (0.0380)	
training:	Epoch: [28][443/817]	Loss 0.0094 (0.0379)	
training:	Epoch: [28][444/817]	Loss 0.0079 (0.0379)	
training:	Epoch: [28][445/817]	Loss 0.0064 (0.0378)	
training:	Epoch: [28][446/817]	Loss 0.0079 (0.0377)	
training:	Epoch: [28][447/817]	Loss 0.0077 (0.0377)	
training:	Epoch: [28][448/817]	Loss 0.0094 (0.0376)	
training:	Epoch: [28][449/817]	Loss 0.0079 (0.0375)	
training:	Epoch: [28][450/817]	Loss 0.0076 (0.0375)	
training:	Epoch: [28][451/817]	Loss 0.0093 (0.0374)	
training:	Epoch: [28][452/817]	Loss 0.0102 (0.0374)	
training:	Epoch: [28][453/817]	Loss 0.0136 (0.0373)	
training:	Epoch: [28][454/817]	Loss 0.0065 (0.0372)	
training:	Epoch: [28][455/817]	Loss 0.0081 (0.0372)	
training:	Epoch: [28][456/817]	Loss 0.0078 (0.0371)	
training:	Epoch: [28][457/817]	Loss 0.0072 (0.0370)	
training:	Epoch: [28][458/817]	Loss 0.0068 (0.0370)	
training:	Epoch: [28][459/817]	Loss 0.0075 (0.0369)	
training:	Epoch: [28][460/817]	Loss 0.0065 (0.0368)	
training:	Epoch: [28][461/817]	Loss 0.0085 (0.0368)	
training:	Epoch: [28][462/817]	Loss 0.5761 (0.0380)	
training:	Epoch: [28][463/817]	Loss 0.0104 (0.0379)	
training:	Epoch: [28][464/817]	Loss 0.0068 (0.0378)	
training:	Epoch: [28][465/817]	Loss 0.0065 (0.0378)	
training:	Epoch: [28][466/817]	Loss 0.0069 (0.0377)	
training:	Epoch: [28][467/817]	Loss 0.0080 (0.0376)	
training:	Epoch: [28][468/817]	Loss 0.0072 (0.0376)	
training:	Epoch: [28][469/817]	Loss 0.0083 (0.0375)	
training:	Epoch: [28][470/817]	Loss 0.0066 (0.0374)	
training:	Epoch: [28][471/817]	Loss 0.6223 (0.0387)	
training:	Epoch: [28][472/817]	Loss 0.0074 (0.0386)	
training:	Epoch: [28][473/817]	Loss 0.0068 (0.0385)	
training:	Epoch: [28][474/817]	Loss 0.0079 (0.0385)	
training:	Epoch: [28][475/817]	Loss 0.0077 (0.0384)	
training:	Epoch: [28][476/817]	Loss 0.0077 (0.0384)	
training:	Epoch: [28][477/817]	Loss 0.0070 (0.0383)	
training:	Epoch: [28][478/817]	Loss 0.0115 (0.0382)	
training:	Epoch: [28][479/817]	Loss 0.0068 (0.0382)	
training:	Epoch: [28][480/817]	Loss 0.0067 (0.0381)	
training:	Epoch: [28][481/817]	Loss 0.0067 (0.0380)	
training:	Epoch: [28][482/817]	Loss 0.0072 (0.0380)	
training:	Epoch: [28][483/817]	Loss 0.0071 (0.0379)	
training:	Epoch: [28][484/817]	Loss 0.0072 (0.0378)	
training:	Epoch: [28][485/817]	Loss 0.0084 (0.0378)	
training:	Epoch: [28][486/817]	Loss 0.0075 (0.0377)	
training:	Epoch: [28][487/817]	Loss 0.0069 (0.0377)	
training:	Epoch: [28][488/817]	Loss 0.0061 (0.0376)	
training:	Epoch: [28][489/817]	Loss 0.0069 (0.0375)	
training:	Epoch: [28][490/817]	Loss 0.0067 (0.0375)	
training:	Epoch: [28][491/817]	Loss 0.0071 (0.0374)	
training:	Epoch: [28][492/817]	Loss 0.0070 (0.0373)	
training:	Epoch: [28][493/817]	Loss 0.0077 (0.0373)	
training:	Epoch: [28][494/817]	Loss 0.0069 (0.0372)	
training:	Epoch: [28][495/817]	Loss 0.0070 (0.0372)	
training:	Epoch: [28][496/817]	Loss 0.0072 (0.0371)	
training:	Epoch: [28][497/817]	Loss 0.0069 (0.0370)	
training:	Epoch: [28][498/817]	Loss 0.0077 (0.0370)	
training:	Epoch: [28][499/817]	Loss 0.0064 (0.0369)	
training:	Epoch: [28][500/817]	Loss 0.0067 (0.0369)	
training:	Epoch: [28][501/817]	Loss 0.0070 (0.0368)	
training:	Epoch: [28][502/817]	Loss 0.0073 (0.0367)	
training:	Epoch: [28][503/817]	Loss 0.0076 (0.0367)	
training:	Epoch: [28][504/817]	Loss 0.0078 (0.0366)	
training:	Epoch: [28][505/817]	Loss 0.0073 (0.0366)	
training:	Epoch: [28][506/817]	Loss 0.0084 (0.0365)	
training:	Epoch: [28][507/817]	Loss 0.0087 (0.0365)	
training:	Epoch: [28][508/817]	Loss 0.0094 (0.0364)	
training:	Epoch: [28][509/817]	Loss 0.0070 (0.0363)	
training:	Epoch: [28][510/817]	Loss 0.0079 (0.0363)	
training:	Epoch: [28][511/817]	Loss 0.0072 (0.0362)	
training:	Epoch: [28][512/817]	Loss 0.0096 (0.0362)	
training:	Epoch: [28][513/817]	Loss 0.0084 (0.0361)	
training:	Epoch: [28][514/817]	Loss 0.0085 (0.0361)	
training:	Epoch: [28][515/817]	Loss 0.0075 (0.0360)	
training:	Epoch: [28][516/817]	Loss 0.0083 (0.0360)	
training:	Epoch: [28][517/817]	Loss 0.0067 (0.0359)	
training:	Epoch: [28][518/817]	Loss 0.0073 (0.0358)	
training:	Epoch: [28][519/817]	Loss 0.0086 (0.0358)	
training:	Epoch: [28][520/817]	Loss 0.0065 (0.0357)	
training:	Epoch: [28][521/817]	Loss 0.0064 (0.0357)	
training:	Epoch: [28][522/817]	Loss 0.0072 (0.0356)	
training:	Epoch: [28][523/817]	Loss 0.0065 (0.0356)	
training:	Epoch: [28][524/817]	Loss 0.0066 (0.0355)	
training:	Epoch: [28][525/817]	Loss 0.0077 (0.0355)	
training:	Epoch: [28][526/817]	Loss 0.0081 (0.0354)	
training:	Epoch: [28][527/817]	Loss 0.0085 (0.0354)	
training:	Epoch: [28][528/817]	Loss 0.0102 (0.0353)	
training:	Epoch: [28][529/817]	Loss 0.0066 (0.0353)	
training:	Epoch: [28][530/817]	Loss 0.0059 (0.0352)	
training:	Epoch: [28][531/817]	Loss 0.0089 (0.0352)	
training:	Epoch: [28][532/817]	Loss 0.0102 (0.0351)	
training:	Epoch: [28][533/817]	Loss 0.0071 (0.0351)	
training:	Epoch: [28][534/817]	Loss 0.0068 (0.0350)	
training:	Epoch: [28][535/817]	Loss 0.0074 (0.0350)	
training:	Epoch: [28][536/817]	Loss 0.0061 (0.0349)	
training:	Epoch: [28][537/817]	Loss 0.0067 (0.0348)	
training:	Epoch: [28][538/817]	Loss 0.0069 (0.0348)	
training:	Epoch: [28][539/817]	Loss 0.0074 (0.0347)	
training:	Epoch: [28][540/817]	Loss 0.0071 (0.0347)	
training:	Epoch: [28][541/817]	Loss 0.0070 (0.0346)	
training:	Epoch: [28][542/817]	Loss 0.0068 (0.0346)	
training:	Epoch: [28][543/817]	Loss 0.0083 (0.0345)	
training:	Epoch: [28][544/817]	Loss 0.0060 (0.0345)	
training:	Epoch: [28][545/817]	Loss 0.0069 (0.0344)	
training:	Epoch: [28][546/817]	Loss 0.0065 (0.0344)	
training:	Epoch: [28][547/817]	Loss 0.5956 (0.0354)	
training:	Epoch: [28][548/817]	Loss 0.0064 (0.0354)	
training:	Epoch: [28][549/817]	Loss 0.0065 (0.0353)	
training:	Epoch: [28][550/817]	Loss 0.0068 (0.0353)	
training:	Epoch: [28][551/817]	Loss 0.0061 (0.0352)	
training:	Epoch: [28][552/817]	Loss 0.0074 (0.0352)	
training:	Epoch: [28][553/817]	Loss 0.0080 (0.0351)	
training:	Epoch: [28][554/817]	Loss 0.0057 (0.0350)	
training:	Epoch: [28][555/817]	Loss 0.0073 (0.0350)	
training:	Epoch: [28][556/817]	Loss 0.0068 (0.0349)	
training:	Epoch: [28][557/817]	Loss 0.0065 (0.0349)	
training:	Epoch: [28][558/817]	Loss 0.0074 (0.0348)	
training:	Epoch: [28][559/817]	Loss 0.0076 (0.0348)	
training:	Epoch: [28][560/817]	Loss 0.0076 (0.0348)	
training:	Epoch: [28][561/817]	Loss 0.0072 (0.0347)	
training:	Epoch: [28][562/817]	Loss 0.0070 (0.0347)	
training:	Epoch: [28][563/817]	Loss 0.0064 (0.0346)	
training:	Epoch: [28][564/817]	Loss 0.0071 (0.0346)	
training:	Epoch: [28][565/817]	Loss 0.0064 (0.0345)	
training:	Epoch: [28][566/817]	Loss 0.0069 (0.0345)	
training:	Epoch: [28][567/817]	Loss 0.0066 (0.0344)	
training:	Epoch: [28][568/817]	Loss 0.0063 (0.0344)	
training:	Epoch: [28][569/817]	Loss 0.0071 (0.0343)	
training:	Epoch: [28][570/817]	Loss 0.0063 (0.0343)	
training:	Epoch: [28][571/817]	Loss 0.6311 (0.0353)	
training:	Epoch: [28][572/817]	Loss 0.0072 (0.0353)	
training:	Epoch: [28][573/817]	Loss 0.0075 (0.0352)	
training:	Epoch: [28][574/817]	Loss 0.0060 (0.0352)	
training:	Epoch: [28][575/817]	Loss 0.0071 (0.0351)	
training:	Epoch: [28][576/817]	Loss 0.0068 (0.0351)	
training:	Epoch: [28][577/817]	Loss 0.0071 (0.0350)	
training:	Epoch: [28][578/817]	Loss 0.0062 (0.0350)	
training:	Epoch: [28][579/817]	Loss 0.5988 (0.0359)	
training:	Epoch: [28][580/817]	Loss 0.0160 (0.0359)	
training:	Epoch: [28][581/817]	Loss 0.0072 (0.0359)	
training:	Epoch: [28][582/817]	Loss 0.6319 (0.0369)	
training:	Epoch: [28][583/817]	Loss 0.0065 (0.0368)	
training:	Epoch: [28][584/817]	Loss 0.0082 (0.0368)	
training:	Epoch: [28][585/817]	Loss 0.0065 (0.0367)	
training:	Epoch: [28][586/817]	Loss 0.0060 (0.0367)	
training:	Epoch: [28][587/817]	Loss 0.0060 (0.0366)	
training:	Epoch: [28][588/817]	Loss 0.6205 (0.0376)	
training:	Epoch: [28][589/817]	Loss 0.0081 (0.0376)	
training:	Epoch: [28][590/817]	Loss 0.0066 (0.0375)	
training:	Epoch: [28][591/817]	Loss 0.0068 (0.0375)	
training:	Epoch: [28][592/817]	Loss 0.0071 (0.0374)	
training:	Epoch: [28][593/817]	Loss 0.0066 (0.0374)	
training:	Epoch: [28][594/817]	Loss 0.6343 (0.0384)	
training:	Epoch: [28][595/817]	Loss 0.0073 (0.0383)	
training:	Epoch: [28][596/817]	Loss 0.0066 (0.0383)	
training:	Epoch: [28][597/817]	Loss 0.0078 (0.0382)	
training:	Epoch: [28][598/817]	Loss 0.0057 (0.0381)	
training:	Epoch: [28][599/817]	Loss 0.0075 (0.0381)	
training:	Epoch: [28][600/817]	Loss 0.0069 (0.0380)	
training:	Epoch: [28][601/817]	Loss 0.0071 (0.0380)	
training:	Epoch: [28][602/817]	Loss 0.0062 (0.0379)	
training:	Epoch: [28][603/817]	Loss 0.0089 (0.0379)	
training:	Epoch: [28][604/817]	Loss 0.0064 (0.0378)	
training:	Epoch: [28][605/817]	Loss 0.0064 (0.0378)	
training:	Epoch: [28][606/817]	Loss 0.0074 (0.0377)	
training:	Epoch: [28][607/817]	Loss 0.0076 (0.0377)	
training:	Epoch: [28][608/817]	Loss 0.0072 (0.0376)	
training:	Epoch: [28][609/817]	Loss 0.0076 (0.0376)	
training:	Epoch: [28][610/817]	Loss 0.0070 (0.0375)	
training:	Epoch: [28][611/817]	Loss 0.0078 (0.0375)	
training:	Epoch: [28][612/817]	Loss 0.5967 (0.0384)	
training:	Epoch: [28][613/817]	Loss 0.0083 (0.0384)	
training:	Epoch: [28][614/817]	Loss 0.0066 (0.0383)	
training:	Epoch: [28][615/817]	Loss 0.0068 (0.0383)	
training:	Epoch: [28][616/817]	Loss 0.0064 (0.0382)	
training:	Epoch: [28][617/817]	Loss 0.0076 (0.0381)	
training:	Epoch: [28][618/817]	Loss 0.0081 (0.0381)	
training:	Epoch: [28][619/817]	Loss 0.0088 (0.0381)	
training:	Epoch: [28][620/817]	Loss 0.0066 (0.0380)	
training:	Epoch: [28][621/817]	Loss 0.0068 (0.0380)	
training:	Epoch: [28][622/817]	Loss 0.0078 (0.0379)	
training:	Epoch: [28][623/817]	Loss 0.0074 (0.0379)	
training:	Epoch: [28][624/817]	Loss 0.0066 (0.0378)	
training:	Epoch: [28][625/817]	Loss 0.0064 (0.0378)	
training:	Epoch: [28][626/817]	Loss 0.0092 (0.0377)	
training:	Epoch: [28][627/817]	Loss 0.0109 (0.0377)	
training:	Epoch: [28][628/817]	Loss 0.0070 (0.0376)	
training:	Epoch: [28][629/817]	Loss 0.0081 (0.0376)	
training:	Epoch: [28][630/817]	Loss 0.0090 (0.0375)	
training:	Epoch: [28][631/817]	Loss 0.0069 (0.0375)	
training:	Epoch: [28][632/817]	Loss 0.0068 (0.0374)	
training:	Epoch: [28][633/817]	Loss 0.0062 (0.0374)	
training:	Epoch: [28][634/817]	Loss 0.0075 (0.0373)	
training:	Epoch: [28][635/817]	Loss 0.0068 (0.0373)	
training:	Epoch: [28][636/817]	Loss 0.0078 (0.0372)	
training:	Epoch: [28][637/817]	Loss 0.0062 (0.0372)	
training:	Epoch: [28][638/817]	Loss 0.0065 (0.0371)	
training:	Epoch: [28][639/817]	Loss 0.0161 (0.0371)	
training:	Epoch: [28][640/817]	Loss 0.0417 (0.0371)	
training:	Epoch: [28][641/817]	Loss 0.0075 (0.0371)	
training:	Epoch: [28][642/817]	Loss 0.0085 (0.0370)	
training:	Epoch: [28][643/817]	Loss 0.0070 (0.0370)	
training:	Epoch: [28][644/817]	Loss 0.0072 (0.0369)	
training:	Epoch: [28][645/817]	Loss 0.0057 (0.0369)	
training:	Epoch: [28][646/817]	Loss 0.0076 (0.0368)	
training:	Epoch: [28][647/817]	Loss 0.0065 (0.0368)	
training:	Epoch: [28][648/817]	Loss 0.0074 (0.0367)	
training:	Epoch: [28][649/817]	Loss 0.5907 (0.0376)	
training:	Epoch: [28][650/817]	Loss 0.0064 (0.0376)	
training:	Epoch: [28][651/817]	Loss 0.0086 (0.0375)	
training:	Epoch: [28][652/817]	Loss 0.0066 (0.0375)	
training:	Epoch: [28][653/817]	Loss 0.0075 (0.0374)	
training:	Epoch: [28][654/817]	Loss 0.5827 (0.0382)	
training:	Epoch: [28][655/817]	Loss 0.0080 (0.0382)	
training:	Epoch: [28][656/817]	Loss 0.0069 (0.0382)	
training:	Epoch: [28][657/817]	Loss 0.0075 (0.0381)	
training:	Epoch: [28][658/817]	Loss 0.0083 (0.0381)	
training:	Epoch: [28][659/817]	Loss 0.0071 (0.0380)	
training:	Epoch: [28][660/817]	Loss 0.0067 (0.0380)	
training:	Epoch: [28][661/817]	Loss 0.0068 (0.0379)	
training:	Epoch: [28][662/817]	Loss 0.0937 (0.0380)	
training:	Epoch: [28][663/817]	Loss 0.0160 (0.0380)	
training:	Epoch: [28][664/817]	Loss 0.0082 (0.0379)	
training:	Epoch: [28][665/817]	Loss 0.0074 (0.0379)	
training:	Epoch: [28][666/817]	Loss 0.0073 (0.0378)	
training:	Epoch: [28][667/817]	Loss 0.0074 (0.0378)	
training:	Epoch: [28][668/817]	Loss 0.0092 (0.0377)	
training:	Epoch: [28][669/817]	Loss 0.0067 (0.0377)	
training:	Epoch: [28][670/817]	Loss 0.0071 (0.0377)	
training:	Epoch: [28][671/817]	Loss 0.0071 (0.0376)	
training:	Epoch: [28][672/817]	Loss 0.0077 (0.0376)	
training:	Epoch: [28][673/817]	Loss 0.0077 (0.0375)	
training:	Epoch: [28][674/817]	Loss 0.0076 (0.0375)	
training:	Epoch: [28][675/817]	Loss 0.0092 (0.0374)	
training:	Epoch: [28][676/817]	Loss 0.0075 (0.0374)	
training:	Epoch: [28][677/817]	Loss 0.0072 (0.0373)	
training:	Epoch: [28][678/817]	Loss 0.0076 (0.0373)	
training:	Epoch: [28][679/817]	Loss 0.0062 (0.0373)	
training:	Epoch: [28][680/817]	Loss 0.0090 (0.0372)	
training:	Epoch: [28][681/817]	Loss 0.0114 (0.0372)	
training:	Epoch: [28][682/817]	Loss 0.0068 (0.0371)	
training:	Epoch: [28][683/817]	Loss 0.0086 (0.0371)	
training:	Epoch: [28][684/817]	Loss 0.0080 (0.0370)	
training:	Epoch: [28][685/817]	Loss 0.0074 (0.0370)	
training:	Epoch: [28][686/817]	Loss 0.0085 (0.0370)	
training:	Epoch: [28][687/817]	Loss 0.0066 (0.0369)	
training:	Epoch: [28][688/817]	Loss 0.0076 (0.0369)	
training:	Epoch: [28][689/817]	Loss 0.0060 (0.0368)	
training:	Epoch: [28][690/817]	Loss 0.6554 (0.0377)	
training:	Epoch: [28][691/817]	Loss 0.0069 (0.0377)	
training:	Epoch: [28][692/817]	Loss 0.0095 (0.0376)	
training:	Epoch: [28][693/817]	Loss 0.0084 (0.0376)	
training:	Epoch: [28][694/817]	Loss 0.0071 (0.0376)	
training:	Epoch: [28][695/817]	Loss 0.0073 (0.0375)	
training:	Epoch: [28][696/817]	Loss 0.0080 (0.0375)	
training:	Epoch: [28][697/817]	Loss 0.0068 (0.0374)	
training:	Epoch: [28][698/817]	Loss 0.0076 (0.0374)	
training:	Epoch: [28][699/817]	Loss 0.0068 (0.0373)	
training:	Epoch: [28][700/817]	Loss 0.0081 (0.0373)	
training:	Epoch: [28][701/817]	Loss 0.0075 (0.0373)	
training:	Epoch: [28][702/817]	Loss 0.0075 (0.0372)	
training:	Epoch: [28][703/817]	Loss 0.0064 (0.0372)	
training:	Epoch: [28][704/817]	Loss 0.0081 (0.0371)	
training:	Epoch: [28][705/817]	Loss 0.0076 (0.0371)	
training:	Epoch: [28][706/817]	Loss 0.0087 (0.0370)	
training:	Epoch: [28][707/817]	Loss 0.0173 (0.0370)	
training:	Epoch: [28][708/817]	Loss 0.0816 (0.0371)	
training:	Epoch: [28][709/817]	Loss 0.0076 (0.0370)	
training:	Epoch: [28][710/817]	Loss 0.0067 (0.0370)	
training:	Epoch: [28][711/817]	Loss 0.0063 (0.0370)	
training:	Epoch: [28][712/817]	Loss 0.0074 (0.0369)	
training:	Epoch: [28][713/817]	Loss 0.0079 (0.0369)	
training:	Epoch: [28][714/817]	Loss 0.0083 (0.0368)	
training:	Epoch: [28][715/817]	Loss 0.0075 (0.0368)	
training:	Epoch: [28][716/817]	Loss 0.0081 (0.0367)	
training:	Epoch: [28][717/817]	Loss 0.0082 (0.0367)	
training:	Epoch: [28][718/817]	Loss 0.0073 (0.0367)	
training:	Epoch: [28][719/817]	Loss 0.0066 (0.0366)	
training:	Epoch: [28][720/817]	Loss 0.0073 (0.0366)	
training:	Epoch: [28][721/817]	Loss 0.0067 (0.0365)	
training:	Epoch: [28][722/817]	Loss 0.0062 (0.0365)	
training:	Epoch: [28][723/817]	Loss 0.0346 (0.0365)	
training:	Epoch: [28][724/817]	Loss 0.0068 (0.0365)	
training:	Epoch: [28][725/817]	Loss 0.0066 (0.0364)	
training:	Epoch: [28][726/817]	Loss 0.0072 (0.0364)	
training:	Epoch: [28][727/817]	Loss 0.0097 (0.0363)	
training:	Epoch: [28][728/817]	Loss 0.0091 (0.0363)	
training:	Epoch: [28][729/817]	Loss 0.0067 (0.0363)	
training:	Epoch: [28][730/817]	Loss 0.0069 (0.0362)	
training:	Epoch: [28][731/817]	Loss 0.0071 (0.0362)	
training:	Epoch: [28][732/817]	Loss 0.0087 (0.0361)	
training:	Epoch: [28][733/817]	Loss 0.0070 (0.0361)	
training:	Epoch: [28][734/817]	Loss 0.0080 (0.0361)	
training:	Epoch: [28][735/817]	Loss 0.0095 (0.0360)	
training:	Epoch: [28][736/817]	Loss 0.0080 (0.0360)	
training:	Epoch: [28][737/817]	Loss 0.0074 (0.0360)	
training:	Epoch: [28][738/817]	Loss 0.6162 (0.0367)	
training:	Epoch: [28][739/817]	Loss 0.0078 (0.0367)	
training:	Epoch: [28][740/817]	Loss 0.0078 (0.0367)	
training:	Epoch: [28][741/817]	Loss 0.0115 (0.0366)	
training:	Epoch: [28][742/817]	Loss 0.0069 (0.0366)	
training:	Epoch: [28][743/817]	Loss 0.0073 (0.0365)	
training:	Epoch: [28][744/817]	Loss 0.0063 (0.0365)	
training:	Epoch: [28][745/817]	Loss 0.0073 (0.0365)	
training:	Epoch: [28][746/817]	Loss 0.0074 (0.0364)	
training:	Epoch: [28][747/817]	Loss 0.0079 (0.0364)	
training:	Epoch: [28][748/817]	Loss 0.0076 (0.0364)	
training:	Epoch: [28][749/817]	Loss 0.1951 (0.0366)	
training:	Epoch: [28][750/817]	Loss 0.0067 (0.0365)	
training:	Epoch: [28][751/817]	Loss 0.0128 (0.0365)	
training:	Epoch: [28][752/817]	Loss 0.0076 (0.0365)	
training:	Epoch: [28][753/817]	Loss 0.6048 (0.0372)	
training:	Epoch: [28][754/817]	Loss 0.0072 (0.0372)	
training:	Epoch: [28][755/817]	Loss 0.0068 (0.0371)	
training:	Epoch: [28][756/817]	Loss 0.0087 (0.0371)	
training:	Epoch: [28][757/817]	Loss 0.0080 (0.0371)	
training:	Epoch: [28][758/817]	Loss 0.0061 (0.0370)	
training:	Epoch: [28][759/817]	Loss 0.0058 (0.0370)	
training:	Epoch: [28][760/817]	Loss 0.0083 (0.0369)	
training:	Epoch: [28][761/817]	Loss 0.0078 (0.0369)	
training:	Epoch: [28][762/817]	Loss 0.0070 (0.0369)	
training:	Epoch: [28][763/817]	Loss 0.0112 (0.0368)	
training:	Epoch: [28][764/817]	Loss 0.0073 (0.0368)	
training:	Epoch: [28][765/817]	Loss 0.0073 (0.0367)	
training:	Epoch: [28][766/817]	Loss 0.0062 (0.0367)	
training:	Epoch: [28][767/817]	Loss 0.6237 (0.0375)	
training:	Epoch: [28][768/817]	Loss 0.0069 (0.0374)	
training:	Epoch: [28][769/817]	Loss 0.0074 (0.0374)	
training:	Epoch: [28][770/817]	Loss 0.0060 (0.0374)	
training:	Epoch: [28][771/817]	Loss 0.0079 (0.0373)	
training:	Epoch: [28][772/817]	Loss 0.0076 (0.0373)	
training:	Epoch: [28][773/817]	Loss 0.0066 (0.0372)	
training:	Epoch: [28][774/817]	Loss 0.0067 (0.0372)	
training:	Epoch: [28][775/817]	Loss 0.0061 (0.0372)	
training:	Epoch: [28][776/817]	Loss 0.0072 (0.0371)	
training:	Epoch: [28][777/817]	Loss 0.0117 (0.0371)	
training:	Epoch: [28][778/817]	Loss 0.0074 (0.0370)	
training:	Epoch: [28][779/817]	Loss 0.0085 (0.0370)	
training:	Epoch: [28][780/817]	Loss 0.0069 (0.0370)	
training:	Epoch: [28][781/817]	Loss 0.6009 (0.0377)	
training:	Epoch: [28][782/817]	Loss 0.0065 (0.0377)	
training:	Epoch: [28][783/817]	Loss 0.0073 (0.0376)	
training:	Epoch: [28][784/817]	Loss 0.0084 (0.0376)	
training:	Epoch: [28][785/817]	Loss 0.0065 (0.0375)	
training:	Epoch: [28][786/817]	Loss 0.0068 (0.0375)	
training:	Epoch: [28][787/817]	Loss 0.0071 (0.0375)	
training:	Epoch: [28][788/817]	Loss 0.0070 (0.0374)	
training:	Epoch: [28][789/817]	Loss 0.0089 (0.0374)	
training:	Epoch: [28][790/817]	Loss 0.0074 (0.0373)	
training:	Epoch: [28][791/817]	Loss 0.0079 (0.0373)	
training:	Epoch: [28][792/817]	Loss 0.0074 (0.0373)	
training:	Epoch: [28][793/817]	Loss 0.5933 (0.0380)	
training:	Epoch: [28][794/817]	Loss 0.0082 (0.0379)	
training:	Epoch: [28][795/817]	Loss 0.0073 (0.0379)	
training:	Epoch: [28][796/817]	Loss 0.0078 (0.0379)	
training:	Epoch: [28][797/817]	Loss 0.0092 (0.0378)	
training:	Epoch: [28][798/817]	Loss 0.0103 (0.0378)	
training:	Epoch: [28][799/817]	Loss 0.0070 (0.0377)	
training:	Epoch: [28][800/817]	Loss 0.0070 (0.0377)	
training:	Epoch: [28][801/817]	Loss 0.0078 (0.0377)	
training:	Epoch: [28][802/817]	Loss 0.0065 (0.0376)	
training:	Epoch: [28][803/817]	Loss 0.0080 (0.0376)	
training:	Epoch: [28][804/817]	Loss 0.0072 (0.0376)	
training:	Epoch: [28][805/817]	Loss 0.0072 (0.0375)	
training:	Epoch: [28][806/817]	Loss 0.0070 (0.0375)	
training:	Epoch: [28][807/817]	Loss 0.0068 (0.0374)	
training:	Epoch: [28][808/817]	Loss 0.0072 (0.0374)	
training:	Epoch: [28][809/817]	Loss 0.0075 (0.0374)	
training:	Epoch: [28][810/817]	Loss 0.0071 (0.0373)	
training:	Epoch: [28][811/817]	Loss 0.6048 (0.0380)	
training:	Epoch: [28][812/817]	Loss 0.0066 (0.0380)	
training:	Epoch: [28][813/817]	Loss 0.0071 (0.0380)	
training:	Epoch: [28][814/817]	Loss 0.4900 (0.0385)	
training:	Epoch: [28][815/817]	Loss 0.0079 (0.0385)	
training:	Epoch: [28][816/817]	Loss 0.0066 (0.0384)	
training:	Epoch: [28][817/817]	Loss 0.0073 (0.0384)	
Training:	 Loss: 0.0384

Training:	 ACC: 0.9931 0.9931 0.9935 0.9927
Validation:	 ACC: 0.7843 0.7865 0.8332 0.7354
Validation:	 Best_BACC: 0.8223 0.8218 0.8127 0.8318
Validation:	 Loss: 0.9391
Pretraining:	Epoch 29/200
----------
training:	Epoch: [29][1/817]	Loss 0.0190 (0.0190)	
training:	Epoch: [29][2/817]	Loss 0.0083 (0.0136)	
training:	Epoch: [29][3/817]	Loss 0.0079 (0.0117)	
training:	Epoch: [29][4/817]	Loss 0.0064 (0.0104)	
training:	Epoch: [29][5/817]	Loss 0.0089 (0.0101)	
training:	Epoch: [29][6/817]	Loss 0.0072 (0.0096)	
training:	Epoch: [29][7/817]	Loss 0.0073 (0.0093)	
training:	Epoch: [29][8/817]	Loss 0.0074 (0.0091)	
training:	Epoch: [29][9/817]	Loss 0.0073 (0.0089)	
training:	Epoch: [29][10/817]	Loss 0.0080 (0.0088)	
training:	Epoch: [29][11/817]	Loss 0.0084 (0.0087)	
training:	Epoch: [29][12/817]	Loss 0.0091 (0.0088)	
training:	Epoch: [29][13/817]	Loss 0.0070 (0.0086)	
training:	Epoch: [29][14/817]	Loss 0.0086 (0.0086)	
training:	Epoch: [29][15/817]	Loss 0.0079 (0.0086)	
training:	Epoch: [29][16/817]	Loss 0.0071 (0.0085)	
training:	Epoch: [29][17/817]	Loss 0.0142 (0.0088)	
training:	Epoch: [29][18/817]	Loss 0.0097 (0.0089)	
training:	Epoch: [29][19/817]	Loss 0.0102 (0.0090)	
training:	Epoch: [29][20/817]	Loss 0.0085 (0.0089)	
training:	Epoch: [29][21/817]	Loss 0.0388 (0.0104)	
training:	Epoch: [29][22/817]	Loss 0.0080 (0.0102)	
training:	Epoch: [29][23/817]	Loss 0.0065 (0.0101)	
training:	Epoch: [29][24/817]	Loss 0.0082 (0.0100)	
training:	Epoch: [29][25/817]	Loss 0.0205 (0.0104)	
training:	Epoch: [29][26/817]	Loss 0.0073 (0.0103)	
training:	Epoch: [29][27/817]	Loss 0.0067 (0.0102)	
training:	Epoch: [29][28/817]	Loss 0.0084 (0.0101)	
training:	Epoch: [29][29/817]	Loss 0.0078 (0.0100)	
training:	Epoch: [29][30/817]	Loss 0.0072 (0.0099)	
training:	Epoch: [29][31/817]	Loss 0.0061 (0.0098)	
training:	Epoch: [29][32/817]	Loss 0.6172 (0.0288)	
training:	Epoch: [29][33/817]	Loss 0.0069 (0.0281)	
training:	Epoch: [29][34/817]	Loss 0.0071 (0.0275)	
training:	Epoch: [29][35/817]	Loss 0.0096 (0.0270)	
training:	Epoch: [29][36/817]	Loss 0.0068 (0.0264)	
training:	Epoch: [29][37/817]	Loss 0.0073 (0.0259)	
training:	Epoch: [29][38/817]	Loss 0.0077 (0.0254)	
training:	Epoch: [29][39/817]	Loss 0.0100 (0.0250)	
training:	Epoch: [29][40/817]	Loss 0.0074 (0.0246)	
training:	Epoch: [29][41/817]	Loss 0.0214 (0.0245)	
training:	Epoch: [29][42/817]	Loss 0.0068 (0.0241)	
training:	Epoch: [29][43/817]	Loss 0.0084 (0.0237)	
training:	Epoch: [29][44/817]	Loss 0.0072 (0.0234)	
training:	Epoch: [29][45/817]	Loss 0.0072 (0.0230)	
training:	Epoch: [29][46/817]	Loss 0.0080 (0.0227)	
training:	Epoch: [29][47/817]	Loss 0.0060 (0.0223)	
training:	Epoch: [29][48/817]	Loss 0.0082 (0.0220)	
training:	Epoch: [29][49/817]	Loss 0.2996 (0.0277)	
training:	Epoch: [29][50/817]	Loss 0.0082 (0.0273)	
training:	Epoch: [29][51/817]	Loss 0.0088 (0.0269)	
training:	Epoch: [29][52/817]	Loss 0.5478 (0.0370)	
training:	Epoch: [29][53/817]	Loss 0.0085 (0.0364)	
training:	Epoch: [29][54/817]	Loss 0.0070 (0.0359)	
training:	Epoch: [29][55/817]	Loss 0.0065 (0.0353)	
training:	Epoch: [29][56/817]	Loss 0.0087 (0.0349)	
training:	Epoch: [29][57/817]	Loss 0.0093 (0.0344)	
training:	Epoch: [29][58/817]	Loss 0.0080 (0.0340)	
training:	Epoch: [29][59/817]	Loss 0.0063 (0.0335)	
training:	Epoch: [29][60/817]	Loss 0.0062 (0.0330)	
training:	Epoch: [29][61/817]	Loss 0.0283 (0.0330)	
training:	Epoch: [29][62/817]	Loss 0.0075 (0.0326)	
training:	Epoch: [29][63/817]	Loss 0.0066 (0.0321)	
training:	Epoch: [29][64/817]	Loss 0.2056 (0.0348)	
training:	Epoch: [29][65/817]	Loss 0.0065 (0.0344)	
training:	Epoch: [29][66/817]	Loss 0.5420 (0.0421)	
training:	Epoch: [29][67/817]	Loss 0.0063 (0.0416)	
training:	Epoch: [29][68/817]	Loss 0.0076 (0.0411)	
training:	Epoch: [29][69/817]	Loss 0.0065 (0.0406)	
training:	Epoch: [29][70/817]	Loss 0.0075 (0.0401)	
training:	Epoch: [29][71/817]	Loss 0.0069 (0.0396)	
training:	Epoch: [29][72/817]	Loss 0.0075 (0.0392)	
training:	Epoch: [29][73/817]	Loss 0.3107 (0.0429)	
training:	Epoch: [29][74/817]	Loss 0.0080 (0.0424)	
training:	Epoch: [29][75/817]	Loss 0.0081 (0.0420)	
training:	Epoch: [29][76/817]	Loss 0.0080 (0.0415)	
training:	Epoch: [29][77/817]	Loss 0.0100 (0.0411)	
training:	Epoch: [29][78/817]	Loss 0.0085 (0.0407)	
training:	Epoch: [29][79/817]	Loss 0.0094 (0.0403)	
training:	Epoch: [29][80/817]	Loss 0.0106 (0.0399)	
training:	Epoch: [29][81/817]	Loss 0.0076 (0.0395)	
training:	Epoch: [29][82/817]	Loss 0.0080 (0.0391)	
training:	Epoch: [29][83/817]	Loss 0.0086 (0.0388)	
training:	Epoch: [29][84/817]	Loss 0.5541 (0.0449)	
training:	Epoch: [29][85/817]	Loss 0.0070 (0.0445)	
training:	Epoch: [29][86/817]	Loss 0.0091 (0.0441)	
training:	Epoch: [29][87/817]	Loss 0.0068 (0.0436)	
training:	Epoch: [29][88/817]	Loss 0.0074 (0.0432)	
training:	Epoch: [29][89/817]	Loss 0.0074 (0.0428)	
training:	Epoch: [29][90/817]	Loss 0.0109 (0.0425)	
training:	Epoch: [29][91/817]	Loss 0.0094 (0.0421)	
training:	Epoch: [29][92/817]	Loss 0.0075 (0.0417)	
training:	Epoch: [29][93/817]	Loss 0.0104 (0.0414)	
training:	Epoch: [29][94/817]	Loss 0.0081 (0.0410)	
training:	Epoch: [29][95/817]	Loss 0.0064 (0.0407)	
training:	Epoch: [29][96/817]	Loss 0.0098 (0.0403)	
training:	Epoch: [29][97/817]	Loss 0.0137 (0.0401)	
training:	Epoch: [29][98/817]	Loss 0.0077 (0.0397)	
training:	Epoch: [29][99/817]	Loss 0.0204 (0.0395)	
training:	Epoch: [29][100/817]	Loss 0.0135 (0.0393)	
training:	Epoch: [29][101/817]	Loss 0.0066 (0.0390)	
training:	Epoch: [29][102/817]	Loss 0.0120 (0.0387)	
training:	Epoch: [29][103/817]	Loss 0.0074 (0.0384)	
training:	Epoch: [29][104/817]	Loss 0.0172 (0.0382)	
training:	Epoch: [29][105/817]	Loss 0.0060 (0.0379)	
training:	Epoch: [29][106/817]	Loss 0.0115 (0.0376)	
training:	Epoch: [29][107/817]	Loss 0.0140 (0.0374)	
training:	Epoch: [29][108/817]	Loss 0.0081 (0.0371)	
training:	Epoch: [29][109/817]	Loss 0.0082 (0.0369)	
training:	Epoch: [29][110/817]	Loss 0.0081 (0.0366)	
training:	Epoch: [29][111/817]	Loss 0.0084 (0.0364)	
training:	Epoch: [29][112/817]	Loss 0.0072 (0.0361)	
training:	Epoch: [29][113/817]	Loss 0.0061 (0.0358)	
training:	Epoch: [29][114/817]	Loss 0.0095 (0.0356)	
training:	Epoch: [29][115/817]	Loss 0.0065 (0.0353)	
training:	Epoch: [29][116/817]	Loss 0.0092 (0.0351)	
training:	Epoch: [29][117/817]	Loss 0.0092 (0.0349)	
training:	Epoch: [29][118/817]	Loss 0.0108 (0.0347)	
training:	Epoch: [29][119/817]	Loss 0.0086 (0.0345)	
training:	Epoch: [29][120/817]	Loss 0.0078 (0.0343)	
training:	Epoch: [29][121/817]	Loss 0.0091 (0.0340)	
training:	Epoch: [29][122/817]	Loss 0.0067 (0.0338)	
training:	Epoch: [29][123/817]	Loss 0.0071 (0.0336)	
training:	Epoch: [29][124/817]	Loss 0.0083 (0.0334)	
training:	Epoch: [29][125/817]	Loss 0.0076 (0.0332)	
training:	Epoch: [29][126/817]	Loss 0.0078 (0.0330)	
training:	Epoch: [29][127/817]	Loss 0.0115 (0.0328)	
training:	Epoch: [29][128/817]	Loss 0.0074 (0.0326)	
training:	Epoch: [29][129/817]	Loss 0.0081 (0.0324)	
training:	Epoch: [29][130/817]	Loss 0.0066 (0.0322)	
training:	Epoch: [29][131/817]	Loss 0.0095 (0.0321)	
training:	Epoch: [29][132/817]	Loss 0.5738 (0.0362)	
training:	Epoch: [29][133/817]	Loss 0.0100 (0.0360)	
training:	Epoch: [29][134/817]	Loss 0.0096 (0.0358)	
training:	Epoch: [29][135/817]	Loss 0.0076 (0.0356)	
training:	Epoch: [29][136/817]	Loss 0.0067 (0.0354)	
training:	Epoch: [29][137/817]	Loss 0.0110 (0.0352)	
training:	Epoch: [29][138/817]	Loss 0.0086 (0.0350)	
training:	Epoch: [29][139/817]	Loss 0.0270 (0.0349)	
training:	Epoch: [29][140/817]	Loss 0.0105 (0.0348)	
training:	Epoch: [29][141/817]	Loss 0.0277 (0.0347)	
training:	Epoch: [29][142/817]	Loss 0.0093 (0.0345)	
training:	Epoch: [29][143/817]	Loss 0.0149 (0.0344)	
training:	Epoch: [29][144/817]	Loss 0.0121 (0.0342)	
training:	Epoch: [29][145/817]	Loss 0.0074 (0.0340)	
training:	Epoch: [29][146/817]	Loss 0.0088 (0.0339)	
training:	Epoch: [29][147/817]	Loss 0.0069 (0.0337)	
training:	Epoch: [29][148/817]	Loss 0.5578 (0.0372)	
training:	Epoch: [29][149/817]	Loss 0.0079 (0.0370)	
training:	Epoch: [29][150/817]	Loss 0.0262 (0.0370)	
training:	Epoch: [29][151/817]	Loss 0.0076 (0.0368)	
training:	Epoch: [29][152/817]	Loss 0.0077 (0.0366)	
training:	Epoch: [29][153/817]	Loss 0.0079 (0.0364)	
training:	Epoch: [29][154/817]	Loss 0.0084 (0.0362)	
training:	Epoch: [29][155/817]	Loss 0.6467 (0.0401)	
training:	Epoch: [29][156/817]	Loss 0.0088 (0.0399)	
training:	Epoch: [29][157/817]	Loss 0.0072 (0.0397)	
training:	Epoch: [29][158/817]	Loss 0.0167 (0.0396)	
training:	Epoch: [29][159/817]	Loss 0.0084 (0.0394)	
training:	Epoch: [29][160/817]	Loss 0.0092 (0.0392)	
training:	Epoch: [29][161/817]	Loss 0.0083 (0.0390)	
training:	Epoch: [29][162/817]	Loss 0.0090 (0.0388)	
training:	Epoch: [29][163/817]	Loss 0.0107 (0.0387)	
training:	Epoch: [29][164/817]	Loss 0.0069 (0.0385)	
training:	Epoch: [29][165/817]	Loss 0.0070 (0.0383)	
training:	Epoch: [29][166/817]	Loss 0.6513 (0.0420)	
training:	Epoch: [29][167/817]	Loss 0.0078 (0.0418)	
training:	Epoch: [29][168/817]	Loss 0.0089 (0.0416)	
training:	Epoch: [29][169/817]	Loss 0.0069 (0.0414)	
training:	Epoch: [29][170/817]	Loss 0.0069 (0.0412)	
training:	Epoch: [29][171/817]	Loss 0.0082 (0.0410)	
training:	Epoch: [29][172/817]	Loss 0.0072 (0.0408)	
training:	Epoch: [29][173/817]	Loss 0.0079 (0.0406)	
training:	Epoch: [29][174/817]	Loss 0.0081 (0.0404)	
training:	Epoch: [29][175/817]	Loss 0.0080 (0.0402)	
training:	Epoch: [29][176/817]	Loss 0.0063 (0.0400)	
training:	Epoch: [29][177/817]	Loss 0.0112 (0.0398)	
training:	Epoch: [29][178/817]	Loss 0.0109 (0.0397)	
training:	Epoch: [29][179/817]	Loss 0.0068 (0.0395)	
training:	Epoch: [29][180/817]	Loss 0.0106 (0.0393)	
training:	Epoch: [29][181/817]	Loss 0.0062 (0.0392)	
training:	Epoch: [29][182/817]	Loss 0.0185 (0.0390)	
training:	Epoch: [29][183/817]	Loss 0.0072 (0.0389)	
training:	Epoch: [29][184/817]	Loss 0.0071 (0.0387)	
training:	Epoch: [29][185/817]	Loss 0.0068 (0.0385)	
training:	Epoch: [29][186/817]	Loss 0.0077 (0.0384)	
training:	Epoch: [29][187/817]	Loss 0.0071 (0.0382)	
training:	Epoch: [29][188/817]	Loss 0.0081 (0.0380)	
training:	Epoch: [29][189/817]	Loss 0.0078 (0.0379)	
training:	Epoch: [29][190/817]	Loss 0.0104 (0.0377)	
training:	Epoch: [29][191/817]	Loss 0.0074 (0.0376)	
training:	Epoch: [29][192/817]	Loss 0.5860 (0.0404)	
training:	Epoch: [29][193/817]	Loss 0.0070 (0.0403)	
training:	Epoch: [29][194/817]	Loss 0.0067 (0.0401)	
training:	Epoch: [29][195/817]	Loss 0.0071 (0.0399)	
training:	Epoch: [29][196/817]	Loss 0.0081 (0.0397)	
training:	Epoch: [29][197/817]	Loss 0.6672 (0.0429)	
training:	Epoch: [29][198/817]	Loss 0.0064 (0.0428)	
training:	Epoch: [29][199/817]	Loss 0.0069 (0.0426)	
training:	Epoch: [29][200/817]	Loss 0.0064 (0.0424)	
training:	Epoch: [29][201/817]	Loss 0.0072 (0.0422)	
training:	Epoch: [29][202/817]	Loss 0.0058 (0.0420)	
training:	Epoch: [29][203/817]	Loss 0.0069 (0.0419)	
training:	Epoch: [29][204/817]	Loss 0.0076 (0.0417)	
training:	Epoch: [29][205/817]	Loss 0.0080 (0.0415)	
training:	Epoch: [29][206/817]	Loss 0.0073 (0.0414)	
training:	Epoch: [29][207/817]	Loss 0.0068 (0.0412)	
training:	Epoch: [29][208/817]	Loss 0.0062 (0.0410)	
training:	Epoch: [29][209/817]	Loss 0.0069 (0.0409)	
training:	Epoch: [29][210/817]	Loss 0.0063 (0.0407)	
training:	Epoch: [29][211/817]	Loss 0.0081 (0.0405)	
training:	Epoch: [29][212/817]	Loss 0.0072 (0.0404)	
training:	Epoch: [29][213/817]	Loss 0.0124 (0.0403)	
training:	Epoch: [29][214/817]	Loss 0.0073 (0.0401)	
training:	Epoch: [29][215/817]	Loss 0.0069 (0.0399)	
training:	Epoch: [29][216/817]	Loss 0.0077 (0.0398)	
training:	Epoch: [29][217/817]	Loss 0.0075 (0.0397)	
training:	Epoch: [29][218/817]	Loss 0.0065 (0.0395)	
training:	Epoch: [29][219/817]	Loss 0.3042 (0.0407)	
training:	Epoch: [29][220/817]	Loss 0.6106 (0.0433)	
training:	Epoch: [29][221/817]	Loss 0.0112 (0.0432)	
training:	Epoch: [29][222/817]	Loss 0.0091 (0.0430)	
training:	Epoch: [29][223/817]	Loss 0.0068 (0.0428)	
training:	Epoch: [29][224/817]	Loss 0.6317 (0.0455)	
training:	Epoch: [29][225/817]	Loss 0.0070 (0.0453)	
training:	Epoch: [29][226/817]	Loss 0.0082 (0.0451)	
training:	Epoch: [29][227/817]	Loss 0.5876 (0.0475)	
training:	Epoch: [29][228/817]	Loss 0.0080 (0.0473)	
training:	Epoch: [29][229/817]	Loss 0.0071 (0.0472)	
training:	Epoch: [29][230/817]	Loss 0.0070 (0.0470)	
training:	Epoch: [29][231/817]	Loss 0.0082 (0.0468)	
training:	Epoch: [29][232/817]	Loss 0.0118 (0.0467)	
training:	Epoch: [29][233/817]	Loss 0.0067 (0.0465)	
training:	Epoch: [29][234/817]	Loss 0.0150 (0.0464)	
training:	Epoch: [29][235/817]	Loss 0.3890 (0.0478)	
training:	Epoch: [29][236/817]	Loss 0.0090 (0.0477)	
training:	Epoch: [29][237/817]	Loss 0.0082 (0.0475)	
training:	Epoch: [29][238/817]	Loss 0.0085 (0.0473)	
training:	Epoch: [29][239/817]	Loss 0.0103 (0.0472)	
training:	Epoch: [29][240/817]	Loss 0.0082 (0.0470)	
training:	Epoch: [29][241/817]	Loss 0.0079 (0.0469)	
training:	Epoch: [29][242/817]	Loss 0.0086 (0.0467)	
training:	Epoch: [29][243/817]	Loss 0.0065 (0.0465)	
training:	Epoch: [29][244/817]	Loss 0.0084 (0.0464)	
training:	Epoch: [29][245/817]	Loss 0.0075 (0.0462)	
training:	Epoch: [29][246/817]	Loss 0.0091 (0.0461)	
training:	Epoch: [29][247/817]	Loss 0.0079 (0.0459)	
training:	Epoch: [29][248/817]	Loss 0.0082 (0.0458)	
training:	Epoch: [29][249/817]	Loss 0.6155 (0.0480)	
training:	Epoch: [29][250/817]	Loss 0.0080 (0.0479)	
training:	Epoch: [29][251/817]	Loss 0.0069 (0.0477)	
training:	Epoch: [29][252/817]	Loss 0.0091 (0.0476)	
training:	Epoch: [29][253/817]	Loss 0.0125 (0.0474)	
training:	Epoch: [29][254/817]	Loss 0.0087 (0.0473)	
training:	Epoch: [29][255/817]	Loss 0.0076 (0.0471)	
training:	Epoch: [29][256/817]	Loss 0.0085 (0.0470)	
training:	Epoch: [29][257/817]	Loss 0.0071 (0.0468)	
training:	Epoch: [29][258/817]	Loss 0.0599 (0.0469)	
training:	Epoch: [29][259/817]	Loss 0.0083 (0.0467)	
training:	Epoch: [29][260/817]	Loss 0.0079 (0.0466)	
training:	Epoch: [29][261/817]	Loss 0.0070 (0.0464)	
training:	Epoch: [29][262/817]	Loss 0.0082 (0.0463)	
training:	Epoch: [29][263/817]	Loss 0.0087 (0.0461)	
training:	Epoch: [29][264/817]	Loss 0.0082 (0.0460)	
training:	Epoch: [29][265/817]	Loss 0.0066 (0.0458)	
training:	Epoch: [29][266/817]	Loss 0.1448 (0.0462)	
training:	Epoch: [29][267/817]	Loss 0.0078 (0.0461)	
training:	Epoch: [29][268/817]	Loss 0.0084 (0.0459)	
training:	Epoch: [29][269/817]	Loss 0.0111 (0.0458)	
training:	Epoch: [29][270/817]	Loss 0.0088 (0.0457)	
training:	Epoch: [29][271/817]	Loss 0.0070 (0.0455)	
training:	Epoch: [29][272/817]	Loss 0.0074 (0.0454)	
training:	Epoch: [29][273/817]	Loss 0.0083 (0.0452)	
training:	Epoch: [29][274/817]	Loss 0.2833 (0.0461)	
training:	Epoch: [29][275/817]	Loss 0.0081 (0.0460)	
training:	Epoch: [29][276/817]	Loss 0.0108 (0.0458)	
training:	Epoch: [29][277/817]	Loss 0.0104 (0.0457)	
training:	Epoch: [29][278/817]	Loss 0.0082 (0.0456)	
training:	Epoch: [29][279/817]	Loss 0.0068 (0.0454)	
training:	Epoch: [29][280/817]	Loss 0.0118 (0.0453)	
training:	Epoch: [29][281/817]	Loss 0.0078 (0.0452)	
training:	Epoch: [29][282/817]	Loss 0.6106 (0.0472)	
training:	Epoch: [29][283/817]	Loss 0.0090 (0.0471)	
training:	Epoch: [29][284/817]	Loss 0.4986 (0.0486)	
training:	Epoch: [29][285/817]	Loss 0.0137 (0.0485)	
training:	Epoch: [29][286/817]	Loss 0.0077 (0.0484)	
training:	Epoch: [29][287/817]	Loss 0.0148 (0.0483)	
training:	Epoch: [29][288/817]	Loss 0.0083 (0.0481)	
training:	Epoch: [29][289/817]	Loss 0.4566 (0.0495)	
training:	Epoch: [29][290/817]	Loss 0.0061 (0.0494)	
training:	Epoch: [29][291/817]	Loss 0.0089 (0.0492)	
training:	Epoch: [29][292/817]	Loss 0.0067 (0.0491)	
training:	Epoch: [29][293/817]	Loss 0.0063 (0.0490)	
training:	Epoch: [29][294/817]	Loss 0.0071 (0.0488)	
training:	Epoch: [29][295/817]	Loss 0.5995 (0.0507)	
training:	Epoch: [29][296/817]	Loss 0.0080 (0.0505)	
training:	Epoch: [29][297/817]	Loss 0.0101 (0.0504)	
training:	Epoch: [29][298/817]	Loss 0.0103 (0.0503)	
training:	Epoch: [29][299/817]	Loss 0.0065 (0.0501)	
training:	Epoch: [29][300/817]	Loss 0.0092 (0.0500)	
training:	Epoch: [29][301/817]	Loss 0.0089 (0.0498)	
training:	Epoch: [29][302/817]	Loss 0.0075 (0.0497)	
training:	Epoch: [29][303/817]	Loss 0.0112 (0.0496)	
training:	Epoch: [29][304/817]	Loss 0.0085 (0.0494)	
training:	Epoch: [29][305/817]	Loss 0.0075 (0.0493)	
training:	Epoch: [29][306/817]	Loss 0.0075 (0.0492)	
training:	Epoch: [29][307/817]	Loss 0.0081 (0.0490)	
training:	Epoch: [29][308/817]	Loss 0.0080 (0.0489)	
training:	Epoch: [29][309/817]	Loss 0.0095 (0.0488)	
training:	Epoch: [29][310/817]	Loss 0.0154 (0.0487)	
training:	Epoch: [29][311/817]	Loss 0.0076 (0.0485)	
training:	Epoch: [29][312/817]	Loss 0.0086 (0.0484)	
training:	Epoch: [29][313/817]	Loss 0.0081 (0.0483)	
training:	Epoch: [29][314/817]	Loss 0.0093 (0.0482)	
training:	Epoch: [29][315/817]	Loss 0.0103 (0.0480)	
training:	Epoch: [29][316/817]	Loss 0.0091 (0.0479)	
training:	Epoch: [29][317/817]	Loss 0.0198 (0.0478)	
training:	Epoch: [29][318/817]	Loss 0.0060 (0.0477)	
training:	Epoch: [29][319/817]	Loss 0.0086 (0.0476)	
training:	Epoch: [29][320/817]	Loss 0.0096 (0.0475)	
training:	Epoch: [29][321/817]	Loss 0.0110 (0.0473)	
training:	Epoch: [29][322/817]	Loss 0.0073 (0.0472)	
training:	Epoch: [29][323/817]	Loss 0.0204 (0.0471)	
training:	Epoch: [29][324/817]	Loss 0.0082 (0.0470)	
training:	Epoch: [29][325/817]	Loss 0.0071 (0.0469)	
training:	Epoch: [29][326/817]	Loss 0.0074 (0.0468)	
training:	Epoch: [29][327/817]	Loss 0.3343 (0.0476)	
training:	Epoch: [29][328/817]	Loss 0.0083 (0.0475)	
training:	Epoch: [29][329/817]	Loss 0.0101 (0.0474)	
training:	Epoch: [29][330/817]	Loss 0.0078 (0.0473)	
training:	Epoch: [29][331/817]	Loss 0.0085 (0.0472)	
training:	Epoch: [29][332/817]	Loss 0.0071 (0.0471)	
training:	Epoch: [29][333/817]	Loss 0.0079 (0.0469)	
training:	Epoch: [29][334/817]	Loss 0.0133 (0.0468)	
training:	Epoch: [29][335/817]	Loss 0.0088 (0.0467)	
training:	Epoch: [29][336/817]	Loss 0.0070 (0.0466)	
training:	Epoch: [29][337/817]	Loss 0.0062 (0.0465)	
training:	Epoch: [29][338/817]	Loss 0.0081 (0.0464)	
training:	Epoch: [29][339/817]	Loss 0.0080 (0.0463)	
training:	Epoch: [29][340/817]	Loss 0.0073 (0.0461)	
training:	Epoch: [29][341/817]	Loss 0.0089 (0.0460)	
training:	Epoch: [29][342/817]	Loss 0.0084 (0.0459)	
training:	Epoch: [29][343/817]	Loss 0.0068 (0.0458)	
training:	Epoch: [29][344/817]	Loss 0.0075 (0.0457)	
training:	Epoch: [29][345/817]	Loss 0.6982 (0.0476)	
training:	Epoch: [29][346/817]	Loss 0.0069 (0.0475)	
training:	Epoch: [29][347/817]	Loss 0.0090 (0.0474)	
training:	Epoch: [29][348/817]	Loss 0.0087 (0.0473)	
training:	Epoch: [29][349/817]	Loss 0.0067 (0.0471)	
training:	Epoch: [29][350/817]	Loss 0.0073 (0.0470)	
training:	Epoch: [29][351/817]	Loss 0.0107 (0.0469)	
training:	Epoch: [29][352/817]	Loss 0.0078 (0.0468)	
training:	Epoch: [29][353/817]	Loss 0.0072 (0.0467)	
training:	Epoch: [29][354/817]	Loss 0.0103 (0.0466)	
training:	Epoch: [29][355/817]	Loss 0.0103 (0.0465)	
training:	Epoch: [29][356/817]	Loss 0.0075 (0.0464)	
training:	Epoch: [29][357/817]	Loss 0.0084 (0.0463)	
training:	Epoch: [29][358/817]	Loss 0.0082 (0.0462)	
training:	Epoch: [29][359/817]	Loss 0.0082 (0.0461)	
training:	Epoch: [29][360/817]	Loss 0.0096 (0.0460)	
training:	Epoch: [29][361/817]	Loss 0.0104 (0.0459)	
training:	Epoch: [29][362/817]	Loss 0.0093 (0.0458)	
training:	Epoch: [29][363/817]	Loss 0.0084 (0.0457)	
training:	Epoch: [29][364/817]	Loss 0.0072 (0.0456)	
training:	Epoch: [29][365/817]	Loss 0.0070 (0.0454)	
training:	Epoch: [29][366/817]	Loss 0.0084 (0.0453)	
training:	Epoch: [29][367/817]	Loss 0.0069 (0.0452)	
training:	Epoch: [29][368/817]	Loss 0.0163 (0.0452)	
training:	Epoch: [29][369/817]	Loss 0.0077 (0.0451)	
training:	Epoch: [29][370/817]	Loss 0.1234 (0.0453)	
training:	Epoch: [29][371/817]	Loss 0.0064 (0.0452)	
training:	Epoch: [29][372/817]	Loss 0.0064 (0.0451)	
training:	Epoch: [29][373/817]	Loss 0.0076 (0.0450)	
training:	Epoch: [29][374/817]	Loss 0.0058 (0.0449)	
training:	Epoch: [29][375/817]	Loss 0.0101 (0.0448)	
training:	Epoch: [29][376/817]	Loss 0.0058 (0.0447)	
training:	Epoch: [29][377/817]	Loss 0.0072 (0.0446)	
training:	Epoch: [29][378/817]	Loss 0.0087 (0.0445)	
training:	Epoch: [29][379/817]	Loss 0.0065 (0.0444)	
training:	Epoch: [29][380/817]	Loss 0.0066 (0.0443)	
training:	Epoch: [29][381/817]	Loss 0.0102 (0.0442)	
training:	Epoch: [29][382/817]	Loss 0.6063 (0.0456)	
training:	Epoch: [29][383/817]	Loss 0.0078 (0.0456)	
training:	Epoch: [29][384/817]	Loss 0.0070 (0.0455)	
training:	Epoch: [29][385/817]	Loss 0.0073 (0.0454)	
training:	Epoch: [29][386/817]	Loss 0.6354 (0.0469)	
training:	Epoch: [29][387/817]	Loss 0.0113 (0.0468)	
training:	Epoch: [29][388/817]	Loss 0.0087 (0.0467)	
training:	Epoch: [29][389/817]	Loss 0.0076 (0.0466)	
training:	Epoch: [29][390/817]	Loss 0.0070 (0.0465)	
training:	Epoch: [29][391/817]	Loss 0.0094 (0.0464)	
training:	Epoch: [29][392/817]	Loss 0.0082 (0.0463)	
training:	Epoch: [29][393/817]	Loss 0.0087 (0.0462)	
training:	Epoch: [29][394/817]	Loss 0.0090 (0.0461)	
training:	Epoch: [29][395/817]	Loss 0.0106 (0.0460)	
training:	Epoch: [29][396/817]	Loss 0.0071 (0.0459)	
training:	Epoch: [29][397/817]	Loss 0.0073 (0.0458)	
training:	Epoch: [29][398/817]	Loss 0.0077 (0.0457)	
training:	Epoch: [29][399/817]	Loss 0.0088 (0.0456)	
training:	Epoch: [29][400/817]	Loss 0.0083 (0.0455)	
training:	Epoch: [29][401/817]	Loss 0.0105 (0.0455)	
training:	Epoch: [29][402/817]	Loss 0.0085 (0.0454)	
training:	Epoch: [29][403/817]	Loss 0.0059 (0.0453)	
training:	Epoch: [29][404/817]	Loss 0.0080 (0.0452)	
training:	Epoch: [29][405/817]	Loss 0.0093 (0.0451)	
training:	Epoch: [29][406/817]	Loss 0.0085 (0.0450)	
training:	Epoch: [29][407/817]	Loss 0.0709 (0.0451)	
training:	Epoch: [29][408/817]	Loss 0.0080 (0.0450)	
training:	Epoch: [29][409/817]	Loss 0.0104 (0.0449)	
training:	Epoch: [29][410/817]	Loss 0.0066 (0.0448)	
training:	Epoch: [29][411/817]	Loss 0.0082 (0.0447)	
training:	Epoch: [29][412/817]	Loss 0.0070 (0.0446)	
training:	Epoch: [29][413/817]	Loss 0.0073 (0.0445)	
training:	Epoch: [29][414/817]	Loss 0.0078 (0.0444)	
training:	Epoch: [29][415/817]	Loss 0.0078 (0.0443)	
training:	Epoch: [29][416/817]	Loss 0.0409 (0.0443)	
training:	Epoch: [29][417/817]	Loss 0.0085 (0.0442)	
training:	Epoch: [29][418/817]	Loss 0.0068 (0.0442)	
training:	Epoch: [29][419/817]	Loss 0.1152 (0.0443)	
training:	Epoch: [29][420/817]	Loss 0.0071 (0.0442)	
training:	Epoch: [29][421/817]	Loss 0.0081 (0.0441)	
training:	Epoch: [29][422/817]	Loss 0.0068 (0.0441)	
training:	Epoch: [29][423/817]	Loss 0.6023 (0.0454)	
training:	Epoch: [29][424/817]	Loss 0.0107 (0.0453)	
training:	Epoch: [29][425/817]	Loss 0.0076 (0.0452)	
training:	Epoch: [29][426/817]	Loss 0.0069 (0.0451)	
training:	Epoch: [29][427/817]	Loss 0.0078 (0.0450)	
training:	Epoch: [29][428/817]	Loss 0.0065 (0.0449)	
training:	Epoch: [29][429/817]	Loss 0.0067 (0.0449)	
training:	Epoch: [29][430/817]	Loss 0.0088 (0.0448)	
training:	Epoch: [29][431/817]	Loss 0.4794 (0.0458)	
training:	Epoch: [29][432/817]	Loss 0.0061 (0.0457)	
training:	Epoch: [29][433/817]	Loss 0.0078 (0.0456)	
training:	Epoch: [29][434/817]	Loss 0.0073 (0.0455)	
training:	Epoch: [29][435/817]	Loss 0.0062 (0.0454)	
training:	Epoch: [29][436/817]	Loss 0.0076 (0.0453)	
training:	Epoch: [29][437/817]	Loss 0.0071 (0.0452)	
training:	Epoch: [29][438/817]	Loss 0.0074 (0.0452)	
training:	Epoch: [29][439/817]	Loss 0.0180 (0.0451)	
training:	Epoch: [29][440/817]	Loss 0.0101 (0.0450)	
training:	Epoch: [29][441/817]	Loss 0.0071 (0.0449)	
training:	Epoch: [29][442/817]	Loss 0.0090 (0.0449)	
training:	Epoch: [29][443/817]	Loss 0.0080 (0.0448)	
training:	Epoch: [29][444/817]	Loss 0.0068 (0.0447)	
training:	Epoch: [29][445/817]	Loss 0.0065 (0.0446)	
training:	Epoch: [29][446/817]	Loss 0.0102 (0.0445)	
training:	Epoch: [29][447/817]	Loss 0.0076 (0.0444)	
training:	Epoch: [29][448/817]	Loss 0.0070 (0.0444)	
training:	Epoch: [29][449/817]	Loss 0.0077 (0.0443)	
training:	Epoch: [29][450/817]	Loss 0.0065 (0.0442)	
training:	Epoch: [29][451/817]	Loss 0.0071 (0.0441)	
training:	Epoch: [29][452/817]	Loss 0.7356 (0.0456)	
training:	Epoch: [29][453/817]	Loss 0.0071 (0.0455)	
training:	Epoch: [29][454/817]	Loss 0.0083 (0.0455)	
training:	Epoch: [29][455/817]	Loss 0.0073 (0.0454)	
training:	Epoch: [29][456/817]	Loss 0.0082 (0.0453)	
training:	Epoch: [29][457/817]	Loss 0.6232 (0.0466)	
training:	Epoch: [29][458/817]	Loss 0.1569 (0.0468)	
training:	Epoch: [29][459/817]	Loss 0.0080 (0.0467)	
training:	Epoch: [29][460/817]	Loss 0.0095 (0.0466)	
training:	Epoch: [29][461/817]	Loss 0.0092 (0.0466)	
training:	Epoch: [29][462/817]	Loss 0.0086 (0.0465)	
training:	Epoch: [29][463/817]	Loss 0.0087 (0.0464)	
training:	Epoch: [29][464/817]	Loss 0.0076 (0.0463)	
training:	Epoch: [29][465/817]	Loss 0.0076 (0.0462)	
training:	Epoch: [29][466/817]	Loss 0.0073 (0.0461)	
training:	Epoch: [29][467/817]	Loss 0.0079 (0.0461)	
training:	Epoch: [29][468/817]	Loss 0.0085 (0.0460)	
training:	Epoch: [29][469/817]	Loss 0.0078 (0.0459)	
training:	Epoch: [29][470/817]	Loss 0.0137 (0.0458)	
training:	Epoch: [29][471/817]	Loss 0.0078 (0.0458)	
training:	Epoch: [29][472/817]	Loss 0.0066 (0.0457)	
training:	Epoch: [29][473/817]	Loss 0.0099 (0.0456)	
training:	Epoch: [29][474/817]	Loss 0.2553 (0.0460)	
training:	Epoch: [29][475/817]	Loss 0.0080 (0.0460)	
training:	Epoch: [29][476/817]	Loss 0.0069 (0.0459)	
training:	Epoch: [29][477/817]	Loss 0.0076 (0.0458)	
training:	Epoch: [29][478/817]	Loss 0.0066 (0.0457)	
training:	Epoch: [29][479/817]	Loss 0.0079 (0.0456)	
training:	Epoch: [29][480/817]	Loss 0.0072 (0.0456)	
training:	Epoch: [29][481/817]	Loss 0.0076 (0.0455)	
training:	Epoch: [29][482/817]	Loss 0.0066 (0.0454)	
training:	Epoch: [29][483/817]	Loss 0.0062 (0.0453)	
training:	Epoch: [29][484/817]	Loss 0.0543 (0.0453)	
training:	Epoch: [29][485/817]	Loss 0.0071 (0.0453)	
training:	Epoch: [29][486/817]	Loss 0.5877 (0.0464)	
training:	Epoch: [29][487/817]	Loss 0.5921 (0.0475)	
training:	Epoch: [29][488/817]	Loss 0.0069 (0.0474)	
training:	Epoch: [29][489/817]	Loss 0.0110 (0.0473)	
training:	Epoch: [29][490/817]	Loss 0.0084 (0.0473)	
training:	Epoch: [29][491/817]	Loss 0.0085 (0.0472)	
training:	Epoch: [29][492/817]	Loss 0.0070 (0.0471)	
training:	Epoch: [29][493/817]	Loss 0.0073 (0.0470)	
training:	Epoch: [29][494/817]	Loss 0.0064 (0.0469)	
training:	Epoch: [29][495/817]	Loss 0.0084 (0.0469)	
training:	Epoch: [29][496/817]	Loss 0.0064 (0.0468)	
training:	Epoch: [29][497/817]	Loss 0.0069 (0.0467)	
training:	Epoch: [29][498/817]	Loss 0.0073 (0.0466)	
training:	Epoch: [29][499/817]	Loss 0.0075 (0.0465)	
training:	Epoch: [29][500/817]	Loss 0.0082 (0.0465)	
training:	Epoch: [29][501/817]	Loss 0.0079 (0.0464)	
training:	Epoch: [29][502/817]	Loss 0.0076 (0.0463)	
training:	Epoch: [29][503/817]	Loss 0.0066 (0.0462)	
training:	Epoch: [29][504/817]	Loss 0.0076 (0.0461)	
training:	Epoch: [29][505/817]	Loss 0.0077 (0.0461)	
training:	Epoch: [29][506/817]	Loss 0.2917 (0.0466)	
training:	Epoch: [29][507/817]	Loss 0.0073 (0.0465)	
training:	Epoch: [29][508/817]	Loss 0.0098 (0.0464)	
training:	Epoch: [29][509/817]	Loss 0.0128 (0.0463)	
training:	Epoch: [29][510/817]	Loss 0.0079 (0.0463)	
training:	Epoch: [29][511/817]	Loss 0.6147 (0.0474)	
training:	Epoch: [29][512/817]	Loss 0.0080 (0.0473)	
training:	Epoch: [29][513/817]	Loss 0.0073 (0.0472)	
training:	Epoch: [29][514/817]	Loss 0.0079 (0.0471)	
training:	Epoch: [29][515/817]	Loss 0.0071 (0.0471)	
training:	Epoch: [29][516/817]	Loss 0.0061 (0.0470)	
training:	Epoch: [29][517/817]	Loss 0.0077 (0.0469)	
training:	Epoch: [29][518/817]	Loss 0.0071 (0.0468)	
training:	Epoch: [29][519/817]	Loss 0.0065 (0.0468)	
training:	Epoch: [29][520/817]	Loss 0.0064 (0.0467)	
training:	Epoch: [29][521/817]	Loss 0.0073 (0.0466)	
training:	Epoch: [29][522/817]	Loss 0.0081 (0.0465)	
training:	Epoch: [29][523/817]	Loss 0.6573 (0.0477)	
training:	Epoch: [29][524/817]	Loss 0.0081 (0.0476)	
training:	Epoch: [29][525/817]	Loss 0.0125 (0.0476)	
training:	Epoch: [29][526/817]	Loss 0.0074 (0.0475)	
training:	Epoch: [29][527/817]	Loss 0.0091 (0.0474)	
training:	Epoch: [29][528/817]	Loss 0.0075 (0.0473)	
training:	Epoch: [29][529/817]	Loss 0.0078 (0.0473)	
training:	Epoch: [29][530/817]	Loss 0.0069 (0.0472)	
training:	Epoch: [29][531/817]	Loss 0.0079 (0.0471)	
training:	Epoch: [29][532/817]	Loss 0.0088 (0.0470)	
training:	Epoch: [29][533/817]	Loss 0.0069 (0.0470)	
training:	Epoch: [29][534/817]	Loss 0.0071 (0.0469)	
training:	Epoch: [29][535/817]	Loss 0.0088 (0.0468)	
training:	Epoch: [29][536/817]	Loss 0.0059 (0.0467)	
training:	Epoch: [29][537/817]	Loss 0.0078 (0.0467)	
training:	Epoch: [29][538/817]	Loss 0.0077 (0.0466)	
training:	Epoch: [29][539/817]	Loss 0.0076 (0.0465)	
training:	Epoch: [29][540/817]	Loss 0.0080 (0.0464)	
training:	Epoch: [29][541/817]	Loss 0.0083 (0.0464)	
training:	Epoch: [29][542/817]	Loss 0.0070 (0.0463)	
training:	Epoch: [29][543/817]	Loss 0.1094 (0.0464)	
training:	Epoch: [29][544/817]	Loss 0.0079 (0.0463)	
training:	Epoch: [29][545/817]	Loss 0.0073 (0.0463)	
training:	Epoch: [29][546/817]	Loss 0.0079 (0.0462)	
training:	Epoch: [29][547/817]	Loss 0.0076 (0.0461)	
training:	Epoch: [29][548/817]	Loss 0.0086 (0.0461)	
training:	Epoch: [29][549/817]	Loss 0.0071 (0.0460)	
training:	Epoch: [29][550/817]	Loss 0.0060 (0.0459)	
training:	Epoch: [29][551/817]	Loss 0.0807 (0.0460)	
training:	Epoch: [29][552/817]	Loss 0.0080 (0.0459)	
training:	Epoch: [29][553/817]	Loss 0.0500 (0.0459)	
training:	Epoch: [29][554/817]	Loss 0.0062 (0.0459)	
training:	Epoch: [29][555/817]	Loss 0.0066 (0.0458)	
training:	Epoch: [29][556/817]	Loss 0.0075 (0.0457)	
training:	Epoch: [29][557/817]	Loss 0.0081 (0.0456)	
training:	Epoch: [29][558/817]	Loss 0.0062 (0.0456)	
training:	Epoch: [29][559/817]	Loss 0.0082 (0.0455)	
training:	Epoch: [29][560/817]	Loss 0.0067 (0.0454)	
training:	Epoch: [29][561/817]	Loss 0.0104 (0.0454)	
training:	Epoch: [29][562/817]	Loss 0.0072 (0.0453)	
training:	Epoch: [29][563/817]	Loss 0.0093 (0.0452)	
training:	Epoch: [29][564/817]	Loss 0.0079 (0.0452)	
training:	Epoch: [29][565/817]	Loss 0.0060 (0.0451)	
training:	Epoch: [29][566/817]	Loss 0.5003 (0.0459)	
training:	Epoch: [29][567/817]	Loss 0.0076 (0.0458)	
training:	Epoch: [29][568/817]	Loss 0.0087 (0.0458)	
training:	Epoch: [29][569/817]	Loss 0.6342 (0.0468)	
training:	Epoch: [29][570/817]	Loss 0.5481 (0.0477)	
training:	Epoch: [29][571/817]	Loss 0.0063 (0.0476)	
training:	Epoch: [29][572/817]	Loss 0.0063 (0.0476)	
training:	Epoch: [29][573/817]	Loss 0.0075 (0.0475)	
training:	Epoch: [29][574/817]	Loss 0.0065 (0.0474)	
training:	Epoch: [29][575/817]	Loss 0.0065 (0.0473)	
training:	Epoch: [29][576/817]	Loss 0.0069 (0.0473)	
training:	Epoch: [29][577/817]	Loss 0.0067 (0.0472)	
training:	Epoch: [29][578/817]	Loss 0.0062 (0.0471)	
training:	Epoch: [29][579/817]	Loss 0.0080 (0.0471)	
training:	Epoch: [29][580/817]	Loss 0.0078 (0.0470)	
training:	Epoch: [29][581/817]	Loss 0.0065 (0.0469)	
training:	Epoch: [29][582/817]	Loss 0.0066 (0.0469)	
training:	Epoch: [29][583/817]	Loss 0.0159 (0.0468)	
training:	Epoch: [29][584/817]	Loss 0.0062 (0.0467)	
training:	Epoch: [29][585/817]	Loss 0.0195 (0.0467)	
training:	Epoch: [29][586/817]	Loss 0.0108 (0.0466)	
training:	Epoch: [29][587/817]	Loss 0.0086 (0.0466)	
training:	Epoch: [29][588/817]	Loss 0.0085 (0.0465)	
training:	Epoch: [29][589/817]	Loss 0.0445 (0.0465)	
training:	Epoch: [29][590/817]	Loss 0.0054 (0.0464)	
training:	Epoch: [29][591/817]	Loss 0.0391 (0.0464)	
training:	Epoch: [29][592/817]	Loss 0.0101 (0.0463)	
training:	Epoch: [29][593/817]	Loss 0.0099 (0.0463)	
training:	Epoch: [29][594/817]	Loss 0.0097 (0.0462)	
training:	Epoch: [29][595/817]	Loss 0.0064 (0.0462)	
training:	Epoch: [29][596/817]	Loss 0.0062 (0.0461)	
training:	Epoch: [29][597/817]	Loss 0.0086 (0.0460)	
training:	Epoch: [29][598/817]	Loss 0.0103 (0.0460)	
training:	Epoch: [29][599/817]	Loss 0.0099 (0.0459)	
training:	Epoch: [29][600/817]	Loss 0.0068 (0.0458)	
training:	Epoch: [29][601/817]	Loss 0.0073 (0.0458)	
training:	Epoch: [29][602/817]	Loss 0.0086 (0.0457)	
training:	Epoch: [29][603/817]	Loss 0.0064 (0.0457)	
training:	Epoch: [29][604/817]	Loss 0.0175 (0.0456)	
training:	Epoch: [29][605/817]	Loss 0.0727 (0.0456)	
training:	Epoch: [29][606/817]	Loss 0.0071 (0.0456)	
training:	Epoch: [29][607/817]	Loss 0.0094 (0.0455)	
training:	Epoch: [29][608/817]	Loss 0.0221 (0.0455)	
training:	Epoch: [29][609/817]	Loss 0.0068 (0.0454)	
training:	Epoch: [29][610/817]	Loss 0.0072 (0.0454)	
training:	Epoch: [29][611/817]	Loss 0.0122 (0.0453)	
training:	Epoch: [29][612/817]	Loss 0.0090 (0.0452)	
training:	Epoch: [29][613/817]	Loss 0.0064 (0.0452)	
training:	Epoch: [29][614/817]	Loss 1.2654 (0.0472)	
training:	Epoch: [29][615/817]	Loss 0.0078 (0.0471)	
training:	Epoch: [29][616/817]	Loss 0.0070 (0.0470)	
training:	Epoch: [29][617/817]	Loss 0.0659 (0.0471)	
training:	Epoch: [29][618/817]	Loss 0.0070 (0.0470)	
training:	Epoch: [29][619/817]	Loss 0.0078 (0.0469)	
training:	Epoch: [29][620/817]	Loss 0.0088 (0.0469)	
training:	Epoch: [29][621/817]	Loss 0.0109 (0.0468)	
training:	Epoch: [29][622/817]	Loss 0.1234 (0.0469)	
training:	Epoch: [29][623/817]	Loss 0.0078 (0.0469)	
training:	Epoch: [29][624/817]	Loss 0.0071 (0.0468)	
training:	Epoch: [29][625/817]	Loss 0.5804 (0.0477)	
training:	Epoch: [29][626/817]	Loss 0.0068 (0.0476)	
training:	Epoch: [29][627/817]	Loss 0.0084 (0.0475)	
training:	Epoch: [29][628/817]	Loss 0.0069 (0.0475)	
training:	Epoch: [29][629/817]	Loss 0.0082 (0.0474)	
training:	Epoch: [29][630/817]	Loss 0.0151 (0.0474)	
training:	Epoch: [29][631/817]	Loss 0.0081 (0.0473)	
training:	Epoch: [29][632/817]	Loss 0.0057 (0.0472)	
training:	Epoch: [29][633/817]	Loss 0.0073 (0.0472)	
training:	Epoch: [29][634/817]	Loss 0.0149 (0.0471)	
training:	Epoch: [29][635/817]	Loss 0.0106 (0.0471)	
training:	Epoch: [29][636/817]	Loss 0.0086 (0.0470)	
training:	Epoch: [29][637/817]	Loss 0.0073 (0.0469)	
training:	Epoch: [29][638/817]	Loss 0.0082 (0.0469)	
training:	Epoch: [29][639/817]	Loss 0.0073 (0.0468)	
training:	Epoch: [29][640/817]	Loss 0.0066 (0.0468)	
training:	Epoch: [29][641/817]	Loss 0.0068 (0.0467)	
training:	Epoch: [29][642/817]	Loss 0.0084 (0.0466)	
training:	Epoch: [29][643/817]	Loss 0.0066 (0.0466)	
training:	Epoch: [29][644/817]	Loss 0.0076 (0.0465)	
training:	Epoch: [29][645/817]	Loss 0.0083 (0.0465)	
training:	Epoch: [29][646/817]	Loss 0.0081 (0.0464)	
training:	Epoch: [29][647/817]	Loss 0.0072 (0.0463)	
training:	Epoch: [29][648/817]	Loss 0.1232 (0.0465)	
training:	Epoch: [29][649/817]	Loss 0.0059 (0.0464)	
training:	Epoch: [29][650/817]	Loss 0.0094 (0.0463)	
training:	Epoch: [29][651/817]	Loss 0.0069 (0.0463)	
training:	Epoch: [29][652/817]	Loss 0.0069 (0.0462)	
training:	Epoch: [29][653/817]	Loss 0.0084 (0.0462)	
training:	Epoch: [29][654/817]	Loss 0.0095 (0.0461)	
training:	Epoch: [29][655/817]	Loss 0.0063 (0.0460)	
training:	Epoch: [29][656/817]	Loss 0.1305 (0.0462)	
training:	Epoch: [29][657/817]	Loss 0.0084 (0.0461)	
training:	Epoch: [29][658/817]	Loss 0.0122 (0.0461)	
training:	Epoch: [29][659/817]	Loss 0.0076 (0.0460)	
training:	Epoch: [29][660/817]	Loss 0.0067 (0.0459)	
training:	Epoch: [29][661/817]	Loss 0.0059 (0.0459)	
training:	Epoch: [29][662/817]	Loss 0.0297 (0.0459)	
training:	Epoch: [29][663/817]	Loss 0.0075 (0.0458)	
training:	Epoch: [29][664/817]	Loss 0.0063 (0.0457)	
training:	Epoch: [29][665/817]	Loss 0.0064 (0.0457)	
training:	Epoch: [29][666/817]	Loss 0.0072 (0.0456)	
training:	Epoch: [29][667/817]	Loss 0.0077 (0.0456)	
training:	Epoch: [29][668/817]	Loss 0.0087 (0.0455)	
training:	Epoch: [29][669/817]	Loss 0.0085 (0.0455)	
training:	Epoch: [29][670/817]	Loss 0.0073 (0.0454)	
training:	Epoch: [29][671/817]	Loss 0.0073 (0.0453)	
training:	Epoch: [29][672/817]	Loss 0.0066 (0.0453)	
training:	Epoch: [29][673/817]	Loss 0.0064 (0.0452)	
training:	Epoch: [29][674/817]	Loss 0.0068 (0.0452)	
training:	Epoch: [29][675/817]	Loss 0.0079 (0.0451)	
training:	Epoch: [29][676/817]	Loss 0.0076 (0.0451)	
training:	Epoch: [29][677/817]	Loss 0.6451 (0.0459)	
training:	Epoch: [29][678/817]	Loss 0.0088 (0.0459)	
training:	Epoch: [29][679/817]	Loss 0.0073 (0.0458)	
training:	Epoch: [29][680/817]	Loss 0.0076 (0.0458)	
training:	Epoch: [29][681/817]	Loss 0.0155 (0.0457)	
training:	Epoch: [29][682/817]	Loss 0.0073 (0.0457)	
training:	Epoch: [29][683/817]	Loss 0.0061 (0.0456)	
training:	Epoch: [29][684/817]	Loss 0.0070 (0.0456)	
training:	Epoch: [29][685/817]	Loss 0.0069 (0.0455)	
training:	Epoch: [29][686/817]	Loss 0.0068 (0.0454)	
training:	Epoch: [29][687/817]	Loss 0.0065 (0.0454)	
training:	Epoch: [29][688/817]	Loss 0.0061 (0.0453)	
training:	Epoch: [29][689/817]	Loss 0.0076 (0.0453)	
training:	Epoch: [29][690/817]	Loss 0.0057 (0.0452)	
training:	Epoch: [29][691/817]	Loss 0.0072 (0.0452)	
training:	Epoch: [29][692/817]	Loss 0.0082 (0.0451)	
training:	Epoch: [29][693/817]	Loss 0.0072 (0.0451)	
training:	Epoch: [29][694/817]	Loss 0.0069 (0.0450)	
training:	Epoch: [29][695/817]	Loss 0.0067 (0.0450)	
training:	Epoch: [29][696/817]	Loss 0.1001 (0.0450)	
training:	Epoch: [29][697/817]	Loss 0.0071 (0.0450)	
training:	Epoch: [29][698/817]	Loss 0.0067 (0.0449)	
training:	Epoch: [29][699/817]	Loss 0.0064 (0.0449)	
training:	Epoch: [29][700/817]	Loss 0.6114 (0.0457)	
training:	Epoch: [29][701/817]	Loss 0.0229 (0.0456)	
training:	Epoch: [29][702/817]	Loss 0.0089 (0.0456)	
training:	Epoch: [29][703/817]	Loss 0.0069 (0.0455)	
training:	Epoch: [29][704/817]	Loss 0.0088 (0.0455)	
training:	Epoch: [29][705/817]	Loss 0.0066 (0.0454)	
training:	Epoch: [29][706/817]	Loss 0.0074 (0.0454)	
training:	Epoch: [29][707/817]	Loss 0.0073 (0.0453)	
training:	Epoch: [29][708/817]	Loss 0.0071 (0.0453)	
training:	Epoch: [29][709/817]	Loss 0.0066 (0.0452)	
training:	Epoch: [29][710/817]	Loss 0.0059 (0.0452)	
training:	Epoch: [29][711/817]	Loss 0.0064 (0.0451)	
training:	Epoch: [29][712/817]	Loss 0.0069 (0.0450)	
training:	Epoch: [29][713/817]	Loss 0.0291 (0.0450)	
training:	Epoch: [29][714/817]	Loss 0.0081 (0.0450)	
training:	Epoch: [29][715/817]	Loss 0.0081 (0.0449)	
training:	Epoch: [29][716/817]	Loss 0.5361 (0.0456)	
training:	Epoch: [29][717/817]	Loss 0.0071 (0.0456)	
training:	Epoch: [29][718/817]	Loss 0.0065 (0.0455)	
training:	Epoch: [29][719/817]	Loss 0.0067 (0.0454)	
training:	Epoch: [29][720/817]	Loss 0.0074 (0.0454)	
training:	Epoch: [29][721/817]	Loss 0.0064 (0.0453)	
training:	Epoch: [29][722/817]	Loss 0.0071 (0.0453)	
training:	Epoch: [29][723/817]	Loss 0.0065 (0.0452)	
training:	Epoch: [29][724/817]	Loss 0.0068 (0.0452)	
training:	Epoch: [29][725/817]	Loss 0.0076 (0.0451)	
training:	Epoch: [29][726/817]	Loss 0.0081 (0.0451)	
training:	Epoch: [29][727/817]	Loss 0.0069 (0.0450)	
training:	Epoch: [29][728/817]	Loss 0.0062 (0.0450)	
training:	Epoch: [29][729/817]	Loss 0.0099 (0.0449)	
training:	Epoch: [29][730/817]	Loss 0.0075 (0.0449)	
training:	Epoch: [29][731/817]	Loss 0.0069 (0.0448)	
training:	Epoch: [29][732/817]	Loss 0.0066 (0.0448)	
training:	Epoch: [29][733/817]	Loss 0.0069 (0.0447)	
training:	Epoch: [29][734/817]	Loss 0.0067 (0.0447)	
training:	Epoch: [29][735/817]	Loss 0.0070 (0.0446)	
training:	Epoch: [29][736/817]	Loss 0.5981 (0.0454)	
training:	Epoch: [29][737/817]	Loss 0.0069 (0.0453)	
training:	Epoch: [29][738/817]	Loss 0.0110 (0.0453)	
training:	Epoch: [29][739/817]	Loss 0.0070 (0.0452)	
training:	Epoch: [29][740/817]	Loss 0.0061 (0.0452)	
training:	Epoch: [29][741/817]	Loss 0.0058 (0.0451)	
training:	Epoch: [29][742/817]	Loss 0.0070 (0.0451)	
training:	Epoch: [29][743/817]	Loss 0.0068 (0.0450)	
training:	Epoch: [29][744/817]	Loss 0.0067 (0.0450)	
training:	Epoch: [29][745/817]	Loss 0.0069 (0.0449)	
training:	Epoch: [29][746/817]	Loss 0.0098 (0.0449)	
training:	Epoch: [29][747/817]	Loss 0.0074 (0.0448)	
training:	Epoch: [29][748/817]	Loss 0.0090 (0.0448)	
training:	Epoch: [29][749/817]	Loss 0.0065 (0.0447)	
training:	Epoch: [29][750/817]	Loss 0.0069 (0.0447)	
training:	Epoch: [29][751/817]	Loss 0.5096 (0.0453)	
training:	Epoch: [29][752/817]	Loss 0.0058 (0.0452)	
training:	Epoch: [29][753/817]	Loss 0.0063 (0.0452)	
training:	Epoch: [29][754/817]	Loss 0.0073 (0.0451)	
training:	Epoch: [29][755/817]	Loss 0.0075 (0.0451)	
training:	Epoch: [29][756/817]	Loss 0.0162 (0.0450)	
training:	Epoch: [29][757/817]	Loss 0.0221 (0.0450)	
training:	Epoch: [29][758/817]	Loss 0.0070 (0.0450)	
training:	Epoch: [29][759/817]	Loss 0.0064 (0.0449)	
training:	Epoch: [29][760/817]	Loss 0.0154 (0.0449)	
training:	Epoch: [29][761/817]	Loss 0.0076 (0.0448)	
training:	Epoch: [29][762/817]	Loss 0.2169 (0.0450)	
training:	Epoch: [29][763/817]	Loss 0.0141 (0.0450)	
training:	Epoch: [29][764/817]	Loss 0.0072 (0.0449)	
training:	Epoch: [29][765/817]	Loss 0.0056 (0.0449)	
training:	Epoch: [29][766/817]	Loss 0.0077 (0.0448)	
training:	Epoch: [29][767/817]	Loss 0.0065 (0.0448)	
training:	Epoch: [29][768/817]	Loss 0.0108 (0.0448)	
training:	Epoch: [29][769/817]	Loss 0.6819 (0.0456)	
training:	Epoch: [29][770/817]	Loss 0.0060 (0.0455)	
training:	Epoch: [29][771/817]	Loss 0.0060 (0.0455)	
training:	Epoch: [29][772/817]	Loss 0.0062 (0.0454)	
training:	Epoch: [29][773/817]	Loss 0.0073 (0.0454)	
training:	Epoch: [29][774/817]	Loss 0.0098 (0.0453)	
training:	Epoch: [29][775/817]	Loss 0.0060 (0.0453)	
training:	Epoch: [29][776/817]	Loss 0.0060 (0.0452)	
training:	Epoch: [29][777/817]	Loss 0.0132 (0.0452)	
training:	Epoch: [29][778/817]	Loss 0.0081 (0.0451)	
training:	Epoch: [29][779/817]	Loss 0.0067 (0.0451)	
training:	Epoch: [29][780/817]	Loss 0.0696 (0.0451)	
training:	Epoch: [29][781/817]	Loss 0.0077 (0.0451)	
training:	Epoch: [29][782/817]	Loss 0.0073 (0.0450)	
training:	Epoch: [29][783/817]	Loss 0.0061 (0.0450)	
training:	Epoch: [29][784/817]	Loss 0.0103 (0.0449)	
training:	Epoch: [29][785/817]	Loss 0.0063 (0.0449)	
training:	Epoch: [29][786/817]	Loss 0.0071 (0.0448)	
training:	Epoch: [29][787/817]	Loss 0.0085 (0.0448)	
training:	Epoch: [29][788/817]	Loss 0.0080 (0.0447)	
training:	Epoch: [29][789/817]	Loss 0.0062 (0.0447)	
training:	Epoch: [29][790/817]	Loss 0.0064 (0.0446)	
training:	Epoch: [29][791/817]	Loss 0.0069 (0.0446)	
training:	Epoch: [29][792/817]	Loss 0.0066 (0.0446)	
training:	Epoch: [29][793/817]	Loss 0.0060 (0.0445)	
training:	Epoch: [29][794/817]	Loss 0.0077 (0.0445)	
training:	Epoch: [29][795/817]	Loss 0.0084 (0.0444)	
training:	Epoch: [29][796/817]	Loss 0.0066 (0.0444)	
training:	Epoch: [29][797/817]	Loss 0.0071 (0.0443)	
training:	Epoch: [29][798/817]	Loss 0.0068 (0.0443)	
training:	Epoch: [29][799/817]	Loss 0.0074 (0.0442)	
training:	Epoch: [29][800/817]	Loss 0.0077 (0.0442)	
training:	Epoch: [29][801/817]	Loss 0.0208 (0.0442)	
training:	Epoch: [29][802/817]	Loss 0.0074 (0.0441)	
training:	Epoch: [29][803/817]	Loss 0.0126 (0.0441)	
training:	Epoch: [29][804/817]	Loss 0.0066 (0.0440)	
training:	Epoch: [29][805/817]	Loss 0.6166 (0.0447)	
training:	Epoch: [29][806/817]	Loss 0.5784 (0.0454)	
training:	Epoch: [29][807/817]	Loss 0.0079 (0.0453)	
training:	Epoch: [29][808/817]	Loss 0.0065 (0.0453)	
training:	Epoch: [29][809/817]	Loss 0.6130 (0.0460)	
training:	Epoch: [29][810/817]	Loss 0.0067 (0.0460)	
training:	Epoch: [29][811/817]	Loss 0.0066 (0.0459)	
training:	Epoch: [29][812/817]	Loss 0.0073 (0.0459)	
training:	Epoch: [29][813/817]	Loss 0.0079 (0.0458)	
training:	Epoch: [29][814/817]	Loss 0.0128 (0.0458)	
training:	Epoch: [29][815/817]	Loss 0.0071 (0.0457)	
training:	Epoch: [29][816/817]	Loss 0.0077 (0.0457)	
training:	Epoch: [29][817/817]	Loss 0.0070 (0.0456)	
Training:	 Loss: 0.0456

Training:	 ACC: 0.9920 0.9920 0.9924 0.9917
Validation:	 ACC: 0.7917 0.7924 0.8066 0.7769
Validation:	 Best_BACC: 0.8223 0.8218 0.8127 0.8318
Validation:	 Loss: 0.9201
Pretraining:	Epoch 30/200
----------
training:	Epoch: [30][1/817]	Loss 0.0059 (0.0059)	
training:	Epoch: [30][2/817]	Loss 0.0090 (0.0075)	
training:	Epoch: [30][3/817]	Loss 0.0066 (0.0072)	
training:	Epoch: [30][4/817]	Loss 0.0117 (0.0083)	
training:	Epoch: [30][5/817]	Loss 0.0064 (0.0079)	
training:	Epoch: [30][6/817]	Loss 0.0060 (0.0076)	
training:	Epoch: [30][7/817]	Loss 0.2842 (0.0471)	
training:	Epoch: [30][8/817]	Loss 0.0095 (0.0424)	
training:	Epoch: [30][9/817]	Loss 0.0272 (0.0407)	
training:	Epoch: [30][10/817]	Loss 0.4768 (0.0843)	
training:	Epoch: [30][11/817]	Loss 0.0067 (0.0773)	
training:	Epoch: [30][12/817]	Loss 0.0073 (0.0714)	
training:	Epoch: [30][13/817]	Loss 0.0099 (0.0667)	
training:	Epoch: [30][14/817]	Loss 0.0079 (0.0625)	
training:	Epoch: [30][15/817]	Loss 0.0084 (0.0589)	
training:	Epoch: [30][16/817]	Loss 0.0124 (0.0560)	
training:	Epoch: [30][17/817]	Loss 0.0119 (0.0534)	
training:	Epoch: [30][18/817]	Loss 0.0364 (0.0525)	
training:	Epoch: [30][19/817]	Loss 0.0078 (0.0501)	
training:	Epoch: [30][20/817]	Loss 0.0079 (0.0480)	
training:	Epoch: [30][21/817]	Loss 0.0075 (0.0461)	
training:	Epoch: [30][22/817]	Loss 0.0069 (0.0443)	
training:	Epoch: [30][23/817]	Loss 0.0063 (0.0426)	
training:	Epoch: [30][24/817]	Loss 0.0995 (0.0450)	
training:	Epoch: [30][25/817]	Loss 0.0072 (0.0435)	
training:	Epoch: [30][26/817]	Loss 0.0095 (0.0422)	
training:	Epoch: [30][27/817]	Loss 0.0082 (0.0409)	
training:	Epoch: [30][28/817]	Loss 0.0064 (0.0397)	
training:	Epoch: [30][29/817]	Loss 0.0067 (0.0385)	
training:	Epoch: [30][30/817]	Loss 0.0081 (0.0375)	
training:	Epoch: [30][31/817]	Loss 0.0060 (0.0365)	
training:	Epoch: [30][32/817]	Loss 0.0063 (0.0356)	
training:	Epoch: [30][33/817]	Loss 0.0078 (0.0347)	
training:	Epoch: [30][34/817]	Loss 0.0075 (0.0339)	
training:	Epoch: [30][35/817]	Loss 0.0060 (0.0331)	
training:	Epoch: [30][36/817]	Loss 0.0062 (0.0324)	
training:	Epoch: [30][37/817]	Loss 0.0068 (0.0317)	
training:	Epoch: [30][38/817]	Loss 0.0077 (0.0311)	
training:	Epoch: [30][39/817]	Loss 0.0099 (0.0305)	
training:	Epoch: [30][40/817]	Loss 0.0079 (0.0300)	
training:	Epoch: [30][41/817]	Loss 0.0097 (0.0295)	
training:	Epoch: [30][42/817]	Loss 0.0067 (0.0289)	
training:	Epoch: [30][43/817]	Loss 0.0070 (0.0284)	
training:	Epoch: [30][44/817]	Loss 0.0068 (0.0279)	
training:	Epoch: [30][45/817]	Loss 0.0071 (0.0275)	
training:	Epoch: [30][46/817]	Loss 0.5805 (0.0395)	
training:	Epoch: [30][47/817]	Loss 0.0086 (0.0388)	
training:	Epoch: [30][48/817]	Loss 0.0071 (0.0382)	
training:	Epoch: [30][49/817]	Loss 0.0057 (0.0375)	
training:	Epoch: [30][50/817]	Loss 0.0066 (0.0369)	
training:	Epoch: [30][51/817]	Loss 0.0066 (0.0363)	
training:	Epoch: [30][52/817]	Loss 0.0056 (0.0357)	
training:	Epoch: [30][53/817]	Loss 0.0088 (0.0352)	
training:	Epoch: [30][54/817]	Loss 0.0070 (0.0347)	
training:	Epoch: [30][55/817]	Loss 0.0057 (0.0341)	
training:	Epoch: [30][56/817]	Loss 0.0074 (0.0337)	
training:	Epoch: [30][57/817]	Loss 0.0075 (0.0332)	
training:	Epoch: [30][58/817]	Loss 0.0085 (0.0328)	
training:	Epoch: [30][59/817]	Loss 0.0068 (0.0323)	
training:	Epoch: [30][60/817]	Loss 0.0071 (0.0319)	
training:	Epoch: [30][61/817]	Loss 0.0103 (0.0316)	
training:	Epoch: [30][62/817]	Loss 0.0073 (0.0312)	
training:	Epoch: [30][63/817]	Loss 0.0085 (0.0308)	
training:	Epoch: [30][64/817]	Loss 0.0089 (0.0305)	
training:	Epoch: [30][65/817]	Loss 0.0126 (0.0302)	
training:	Epoch: [30][66/817]	Loss 0.0072 (0.0298)	
training:	Epoch: [30][67/817]	Loss 0.0083 (0.0295)	
training:	Epoch: [30][68/817]	Loss 0.0088 (0.0292)	
training:	Epoch: [30][69/817]	Loss 0.0070 (0.0289)	
training:	Epoch: [30][70/817]	Loss 0.6287 (0.0375)	
training:	Epoch: [30][71/817]	Loss 0.0074 (0.0370)	
training:	Epoch: [30][72/817]	Loss 0.0080 (0.0366)	
training:	Epoch: [30][73/817]	Loss 0.0085 (0.0363)	
training:	Epoch: [30][74/817]	Loss 0.0093 (0.0359)	
training:	Epoch: [30][75/817]	Loss 0.0086 (0.0355)	
training:	Epoch: [30][76/817]	Loss 0.0064 (0.0351)	
training:	Epoch: [30][77/817]	Loss 0.0073 (0.0348)	
training:	Epoch: [30][78/817]	Loss 0.0071 (0.0344)	
training:	Epoch: [30][79/817]	Loss 0.0076 (0.0341)	
training:	Epoch: [30][80/817]	Loss 0.0067 (0.0337)	
training:	Epoch: [30][81/817]	Loss 0.0073 (0.0334)	
training:	Epoch: [30][82/817]	Loss 0.0062 (0.0331)	
training:	Epoch: [30][83/817]	Loss 0.0059 (0.0328)	
training:	Epoch: [30][84/817]	Loss 0.0666 (0.0332)	
training:	Epoch: [30][85/817]	Loss 0.0065 (0.0328)	
training:	Epoch: [30][86/817]	Loss 0.0063 (0.0325)	
training:	Epoch: [30][87/817]	Loss 0.0075 (0.0322)	
training:	Epoch: [30][88/817]	Loss 0.0069 (0.0320)	
training:	Epoch: [30][89/817]	Loss 0.5628 (0.0379)	
training:	Epoch: [30][90/817]	Loss 0.0056 (0.0376)	
training:	Epoch: [30][91/817]	Loss 0.0066 (0.0372)	
training:	Epoch: [30][92/817]	Loss 0.0077 (0.0369)	
training:	Epoch: [30][93/817]	Loss 0.0071 (0.0366)	
training:	Epoch: [30][94/817]	Loss 0.0072 (0.0363)	
training:	Epoch: [30][95/817]	Loss 0.0056 (0.0359)	
training:	Epoch: [30][96/817]	Loss 0.0057 (0.0356)	
training:	Epoch: [30][97/817]	Loss 0.0064 (0.0353)	
training:	Epoch: [30][98/817]	Loss 0.0097 (0.0351)	
training:	Epoch: [30][99/817]	Loss 0.0151 (0.0349)	
training:	Epoch: [30][100/817]	Loss 0.0059 (0.0346)	
training:	Epoch: [30][101/817]	Loss 0.0073 (0.0343)	
training:	Epoch: [30][102/817]	Loss 0.0066 (0.0340)	
training:	Epoch: [30][103/817]	Loss 0.0070 (0.0338)	
training:	Epoch: [30][104/817]	Loss 0.0064 (0.0335)	
training:	Epoch: [30][105/817]	Loss 0.0060 (0.0333)	
training:	Epoch: [30][106/817]	Loss 0.6548 (0.0391)	
training:	Epoch: [30][107/817]	Loss 0.0090 (0.0388)	
training:	Epoch: [30][108/817]	Loss 0.0101 (0.0386)	
training:	Epoch: [30][109/817]	Loss 0.0078 (0.0383)	
training:	Epoch: [30][110/817]	Loss 0.0080 (0.0380)	
training:	Epoch: [30][111/817]	Loss 0.0101 (0.0378)	
training:	Epoch: [30][112/817]	Loss 0.0087 (0.0375)	
training:	Epoch: [30][113/817]	Loss 0.0058 (0.0372)	
training:	Epoch: [30][114/817]	Loss 0.0083 (0.0370)	
training:	Epoch: [30][115/817]	Loss 0.0110 (0.0367)	
training:	Epoch: [30][116/817]	Loss 0.0065 (0.0365)	
training:	Epoch: [30][117/817]	Loss 0.0062 (0.0362)	
training:	Epoch: [30][118/817]	Loss 0.0084 (0.0360)	
training:	Epoch: [30][119/817]	Loss 0.0076 (0.0357)	
training:	Epoch: [30][120/817]	Loss 0.0070 (0.0355)	
training:	Epoch: [30][121/817]	Loss 0.0074 (0.0353)	
training:	Epoch: [30][122/817]	Loss 0.0074 (0.0350)	
training:	Epoch: [30][123/817]	Loss 0.0066 (0.0348)	
training:	Epoch: [30][124/817]	Loss 0.0091 (0.0346)	
training:	Epoch: [30][125/817]	Loss 0.0074 (0.0344)	
training:	Epoch: [30][126/817]	Loss 0.0074 (0.0342)	
training:	Epoch: [30][127/817]	Loss 0.0130 (0.0340)	
training:	Epoch: [30][128/817]	Loss 0.0058 (0.0338)	
training:	Epoch: [30][129/817]	Loss 0.0090 (0.0336)	
training:	Epoch: [30][130/817]	Loss 0.0152 (0.0335)	
training:	Epoch: [30][131/817]	Loss 0.0078 (0.0333)	
training:	Epoch: [30][132/817]	Loss 0.0061 (0.0331)	
training:	Epoch: [30][133/817]	Loss 0.5413 (0.0369)	
training:	Epoch: [30][134/817]	Loss 0.0060 (0.0366)	
training:	Epoch: [30][135/817]	Loss 0.0070 (0.0364)	
training:	Epoch: [30][136/817]	Loss 0.0093 (0.0362)	
training:	Epoch: [30][137/817]	Loss 0.0101 (0.0360)	
training:	Epoch: [30][138/817]	Loss 0.0064 (0.0358)	
training:	Epoch: [30][139/817]	Loss 0.0082 (0.0356)	
training:	Epoch: [30][140/817]	Loss 0.0089 (0.0354)	
training:	Epoch: [30][141/817]	Loss 0.6793 (0.0400)	
training:	Epoch: [30][142/817]	Loss 0.0079 (0.0398)	
training:	Epoch: [30][143/817]	Loss 0.0073 (0.0395)	
training:	Epoch: [30][144/817]	Loss 0.0059 (0.0393)	
training:	Epoch: [30][145/817]	Loss 0.0076 (0.0391)	
training:	Epoch: [30][146/817]	Loss 0.0088 (0.0389)	
training:	Epoch: [30][147/817]	Loss 0.0065 (0.0387)	
training:	Epoch: [30][148/817]	Loss 0.0064 (0.0384)	
training:	Epoch: [30][149/817]	Loss 0.0063 (0.0382)	
training:	Epoch: [30][150/817]	Loss 0.0077 (0.0380)	
training:	Epoch: [30][151/817]	Loss 0.0069 (0.0378)	
training:	Epoch: [30][152/817]	Loss 0.0066 (0.0376)	
training:	Epoch: [30][153/817]	Loss 0.0072 (0.0374)	
training:	Epoch: [30][154/817]	Loss 0.0063 (0.0372)	
training:	Epoch: [30][155/817]	Loss 0.0061 (0.0370)	
training:	Epoch: [30][156/817]	Loss 0.0066 (0.0368)	
training:	Epoch: [30][157/817]	Loss 0.5639 (0.0402)	
training:	Epoch: [30][158/817]	Loss 0.0072 (0.0400)	
training:	Epoch: [30][159/817]	Loss 0.0077 (0.0398)	
training:	Epoch: [30][160/817]	Loss 0.0079 (0.0396)	
training:	Epoch: [30][161/817]	Loss 0.0089 (0.0394)	
training:	Epoch: [30][162/817]	Loss 0.0075 (0.0392)	
training:	Epoch: [30][163/817]	Loss 0.6347 (0.0428)	
training:	Epoch: [30][164/817]	Loss 0.0070 (0.0426)	
training:	Epoch: [30][165/817]	Loss 0.0739 (0.0428)	
training:	Epoch: [30][166/817]	Loss 0.0069 (0.0426)	
training:	Epoch: [30][167/817]	Loss 0.0092 (0.0424)	
training:	Epoch: [30][168/817]	Loss 0.0085 (0.0422)	
training:	Epoch: [30][169/817]	Loss 0.0061 (0.0420)	
training:	Epoch: [30][170/817]	Loss 0.0077 (0.0418)	
training:	Epoch: [30][171/817]	Loss 0.0088 (0.0416)	
training:	Epoch: [30][172/817]	Loss 0.0071 (0.0414)	
training:	Epoch: [30][173/817]	Loss 0.0103 (0.0412)	
training:	Epoch: [30][174/817]	Loss 0.0079 (0.0410)	
training:	Epoch: [30][175/817]	Loss 0.0070 (0.0408)	
training:	Epoch: [30][176/817]	Loss 0.0080 (0.0406)	
training:	Epoch: [30][177/817]	Loss 0.0068 (0.0404)	
training:	Epoch: [30][178/817]	Loss 0.0062 (0.0402)	
training:	Epoch: [30][179/817]	Loss 0.0077 (0.0401)	
training:	Epoch: [30][180/817]	Loss 0.0082 (0.0399)	
training:	Epoch: [30][181/817]	Loss 0.0075 (0.0397)	
training:	Epoch: [30][182/817]	Loss 0.0080 (0.0395)	
training:	Epoch: [30][183/817]	Loss 0.0080 (0.0394)	
training:	Epoch: [30][184/817]	Loss 0.6103 (0.0425)	
training:	Epoch: [30][185/817]	Loss 0.0076 (0.0423)	
training:	Epoch: [30][186/817]	Loss 0.0069 (0.0421)	
training:	Epoch: [30][187/817]	Loss 0.6412 (0.0453)	
training:	Epoch: [30][188/817]	Loss 0.0079 (0.0451)	
training:	Epoch: [30][189/817]	Loss 0.0060 (0.0449)	
training:	Epoch: [30][190/817]	Loss 0.0082 (0.0447)	
training:	Epoch: [30][191/817]	Loss 0.0074 (0.0445)	
training:	Epoch: [30][192/817]	Loss 0.0078 (0.0443)	
training:	Epoch: [30][193/817]	Loss 0.0068 (0.0441)	
training:	Epoch: [30][194/817]	Loss 0.0090 (0.0439)	
training:	Epoch: [30][195/817]	Loss 0.0074 (0.0437)	
training:	Epoch: [30][196/817]	Loss 0.6056 (0.0466)	
training:	Epoch: [30][197/817]	Loss 0.0106 (0.0464)	
training:	Epoch: [30][198/817]	Loss 0.0071 (0.0462)	
training:	Epoch: [30][199/817]	Loss 0.0066 (0.0460)	
training:	Epoch: [30][200/817]	Loss 0.0085 (0.0458)	
training:	Epoch: [30][201/817]	Loss 0.0074 (0.0456)	
training:	Epoch: [30][202/817]	Loss 0.0077 (0.0455)	
training:	Epoch: [30][203/817]	Loss 0.0080 (0.0453)	
training:	Epoch: [30][204/817]	Loss 0.0100 (0.0451)	
training:	Epoch: [30][205/817]	Loss 0.0085 (0.0449)	
training:	Epoch: [30][206/817]	Loss 0.0057 (0.0447)	
training:	Epoch: [30][207/817]	Loss 0.0071 (0.0445)	
training:	Epoch: [30][208/817]	Loss 0.0081 (0.0444)	
training:	Epoch: [30][209/817]	Loss 0.5676 (0.0469)	
training:	Epoch: [30][210/817]	Loss 0.0096 (0.0467)	
training:	Epoch: [30][211/817]	Loss 0.0086 (0.0465)	
training:	Epoch: [30][212/817]	Loss 0.0074 (0.0463)	
training:	Epoch: [30][213/817]	Loss 0.0074 (0.0461)	
training:	Epoch: [30][214/817]	Loss 0.0057 (0.0460)	
training:	Epoch: [30][215/817]	Loss 0.5937 (0.0485)	
training:	Epoch: [30][216/817]	Loss 0.0097 (0.0483)	
training:	Epoch: [30][217/817]	Loss 0.0073 (0.0481)	
training:	Epoch: [30][218/817]	Loss 0.0065 (0.0479)	
training:	Epoch: [30][219/817]	Loss 0.6150 (0.0505)	
training:	Epoch: [30][220/817]	Loss 0.5548 (0.0528)	
training:	Epoch: [30][221/817]	Loss 0.0081 (0.0526)	
training:	Epoch: [30][222/817]	Loss 0.0084 (0.0524)	
training:	Epoch: [30][223/817]	Loss 0.0073 (0.0522)	
training:	Epoch: [30][224/817]	Loss 0.0066 (0.0520)	
training:	Epoch: [30][225/817]	Loss 0.3007 (0.0531)	
training:	Epoch: [30][226/817]	Loss 0.0083 (0.0529)	
training:	Epoch: [30][227/817]	Loss 0.0093 (0.0527)	
training:	Epoch: [30][228/817]	Loss 0.0063 (0.0525)	
training:	Epoch: [30][229/817]	Loss 0.0070 (0.0523)	
training:	Epoch: [30][230/817]	Loss 0.0082 (0.0521)	
training:	Epoch: [30][231/817]	Loss 0.0098 (0.0520)	
training:	Epoch: [30][232/817]	Loss 0.0095 (0.0518)	
training:	Epoch: [30][233/817]	Loss 0.0086 (0.0516)	
training:	Epoch: [30][234/817]	Loss 0.0078 (0.0514)	
training:	Epoch: [30][235/817]	Loss 0.0077 (0.0512)	
training:	Epoch: [30][236/817]	Loss 0.0070 (0.0510)	
training:	Epoch: [30][237/817]	Loss 0.0101 (0.0509)	
training:	Epoch: [30][238/817]	Loss 0.0070 (0.0507)	
training:	Epoch: [30][239/817]	Loss 0.0085 (0.0505)	
training:	Epoch: [30][240/817]	Loss 0.0078 (0.0503)	
training:	Epoch: [30][241/817]	Loss 0.0095 (0.0501)	
training:	Epoch: [30][242/817]	Loss 0.0074 (0.0500)	
training:	Epoch: [30][243/817]	Loss 0.0074 (0.0498)	
training:	Epoch: [30][244/817]	Loss 0.0075 (0.0496)	
training:	Epoch: [30][245/817]	Loss 0.0084 (0.0495)	
training:	Epoch: [30][246/817]	Loss 0.0076 (0.0493)	
training:	Epoch: [30][247/817]	Loss 0.0099 (0.0491)	
training:	Epoch: [30][248/817]	Loss 0.0102 (0.0490)	
training:	Epoch: [30][249/817]	Loss 0.0094 (0.0488)	
training:	Epoch: [30][250/817]	Loss 0.0085 (0.0486)	
training:	Epoch: [30][251/817]	Loss 0.0086 (0.0485)	
training:	Epoch: [30][252/817]	Loss 0.5932 (0.0507)	
training:	Epoch: [30][253/817]	Loss 0.0101 (0.0505)	
training:	Epoch: [30][254/817]	Loss 0.5521 (0.0525)	
training:	Epoch: [30][255/817]	Loss 0.0091 (0.0523)	
training:	Epoch: [30][256/817]	Loss 0.0093 (0.0521)	
training:	Epoch: [30][257/817]	Loss 0.0070 (0.0520)	
training:	Epoch: [30][258/817]	Loss 0.0110 (0.0518)	
training:	Epoch: [30][259/817]	Loss 0.5972 (0.0539)	
training:	Epoch: [30][260/817]	Loss 0.0083 (0.0537)	
training:	Epoch: [30][261/817]	Loss 0.6323 (0.0559)	
training:	Epoch: [30][262/817]	Loss 0.0082 (0.0558)	
training:	Epoch: [30][263/817]	Loss 0.5789 (0.0577)	
training:	Epoch: [30][264/817]	Loss 0.0119 (0.0576)	
training:	Epoch: [30][265/817]	Loss 0.0090 (0.0574)	
training:	Epoch: [30][266/817]	Loss 0.0086 (0.0572)	
training:	Epoch: [30][267/817]	Loss 0.0111 (0.0570)	
training:	Epoch: [30][268/817]	Loss 0.0082 (0.0569)	
training:	Epoch: [30][269/817]	Loss 0.0077 (0.0567)	
training:	Epoch: [30][270/817]	Loss 0.0082 (0.0565)	
training:	Epoch: [30][271/817]	Loss 0.0077 (0.0563)	
training:	Epoch: [30][272/817]	Loss 0.0108 (0.0561)	
training:	Epoch: [30][273/817]	Loss 0.0116 (0.0560)	
training:	Epoch: [30][274/817]	Loss 0.0090 (0.0558)	
training:	Epoch: [30][275/817]	Loss 0.0099 (0.0556)	
training:	Epoch: [30][276/817]	Loss 0.0098 (0.0555)	
training:	Epoch: [30][277/817]	Loss 0.0113 (0.0553)	
training:	Epoch: [30][278/817]	Loss 0.0075 (0.0551)	
training:	Epoch: [30][279/817]	Loss 0.0103 (0.0550)	
training:	Epoch: [30][280/817]	Loss 0.0093 (0.0548)	
training:	Epoch: [30][281/817]	Loss 0.0093 (0.0547)	
training:	Epoch: [30][282/817]	Loss 0.0104 (0.0545)	
training:	Epoch: [30][283/817]	Loss 0.0105 (0.0543)	
training:	Epoch: [30][284/817]	Loss 0.0081 (0.0542)	
training:	Epoch: [30][285/817]	Loss 0.0076 (0.0540)	
training:	Epoch: [30][286/817]	Loss 0.0171 (0.0539)	
training:	Epoch: [30][287/817]	Loss 0.0090 (0.0537)	
training:	Epoch: [30][288/817]	Loss 0.0076 (0.0536)	
training:	Epoch: [30][289/817]	Loss 0.0077 (0.0534)	
training:	Epoch: [30][290/817]	Loss 0.0095 (0.0533)	
training:	Epoch: [30][291/817]	Loss 0.0085 (0.0531)	
training:	Epoch: [30][292/817]	Loss 0.0198 (0.0530)	
training:	Epoch: [30][293/817]	Loss 0.0097 (0.0528)	
training:	Epoch: [30][294/817]	Loss 0.0122 (0.0527)	
training:	Epoch: [30][295/817]	Loss 0.3755 (0.0538)	
training:	Epoch: [30][296/817]	Loss 0.0085 (0.0536)	
training:	Epoch: [30][297/817]	Loss 0.0089 (0.0535)	
training:	Epoch: [30][298/817]	Loss 0.0107 (0.0534)	
training:	Epoch: [30][299/817]	Loss 0.0090 (0.0532)	
training:	Epoch: [30][300/817]	Loss 0.0086 (0.0531)	
training:	Epoch: [30][301/817]	Loss 0.0080 (0.0529)	
training:	Epoch: [30][302/817]	Loss 0.0093 (0.0528)	
training:	Epoch: [30][303/817]	Loss 0.0082 (0.0526)	
training:	Epoch: [30][304/817]	Loss 0.0082 (0.0525)	
training:	Epoch: [30][305/817]	Loss 0.0082 (0.0523)	
training:	Epoch: [30][306/817]	Loss 0.0074 (0.0522)	
training:	Epoch: [30][307/817]	Loss 0.0079 (0.0520)	
training:	Epoch: [30][308/817]	Loss 0.0150 (0.0519)	
training:	Epoch: [30][309/817]	Loss 0.0089 (0.0518)	
training:	Epoch: [30][310/817]	Loss 0.0075 (0.0516)	
training:	Epoch: [30][311/817]	Loss 0.0094 (0.0515)	
training:	Epoch: [30][312/817]	Loss 0.0090 (0.0514)	
training:	Epoch: [30][313/817]	Loss 0.0076 (0.0512)	
training:	Epoch: [30][314/817]	Loss 0.0136 (0.0511)	
training:	Epoch: [30][315/817]	Loss 0.0091 (0.0510)	
training:	Epoch: [30][316/817]	Loss 0.0159 (0.0509)	
training:	Epoch: [30][317/817]	Loss 0.0148 (0.0507)	
training:	Epoch: [30][318/817]	Loss 0.0075 (0.0506)	
training:	Epoch: [30][319/817]	Loss 0.0137 (0.0505)	
training:	Epoch: [30][320/817]	Loss 0.0078 (0.0504)	
training:	Epoch: [30][321/817]	Loss 0.0087 (0.0502)	
training:	Epoch: [30][322/817]	Loss 0.0088 (0.0501)	
training:	Epoch: [30][323/817]	Loss 0.0072 (0.0500)	
training:	Epoch: [30][324/817]	Loss 0.0098 (0.0498)	
training:	Epoch: [30][325/817]	Loss 0.0095 (0.0497)	
training:	Epoch: [30][326/817]	Loss 0.0094 (0.0496)	
training:	Epoch: [30][327/817]	Loss 0.0073 (0.0495)	
training:	Epoch: [30][328/817]	Loss 0.0062 (0.0493)	
training:	Epoch: [30][329/817]	Loss 0.0080 (0.0492)	
training:	Epoch: [30][330/817]	Loss 0.0065 (0.0491)	
training:	Epoch: [30][331/817]	Loss 0.0078 (0.0490)	
training:	Epoch: [30][332/817]	Loss 0.0085 (0.0488)	
training:	Epoch: [30][333/817]	Loss 0.0080 (0.0487)	
training:	Epoch: [30][334/817]	Loss 0.0101 (0.0486)	
training:	Epoch: [30][335/817]	Loss 0.0077 (0.0485)	
training:	Epoch: [30][336/817]	Loss 0.0090 (0.0484)	
training:	Epoch: [30][337/817]	Loss 0.0100 (0.0482)	
training:	Epoch: [30][338/817]	Loss 0.0120 (0.0481)	
training:	Epoch: [30][339/817]	Loss 0.0087 (0.0480)	
training:	Epoch: [30][340/817]	Loss 0.1128 (0.0482)	
training:	Epoch: [30][341/817]	Loss 0.0063 (0.0481)	
training:	Epoch: [30][342/817]	Loss 0.0072 (0.0480)	
training:	Epoch: [30][343/817]	Loss 0.0080 (0.0478)	
training:	Epoch: [30][344/817]	Loss 0.0074 (0.0477)	
training:	Epoch: [30][345/817]	Loss 0.0125 (0.0476)	
training:	Epoch: [30][346/817]	Loss 0.0081 (0.0475)	
training:	Epoch: [30][347/817]	Loss 0.0087 (0.0474)	
training:	Epoch: [30][348/817]	Loss 0.0075 (0.0473)	
training:	Epoch: [30][349/817]	Loss 0.0078 (0.0472)	
training:	Epoch: [30][350/817]	Loss 0.0067 (0.0471)	
training:	Epoch: [30][351/817]	Loss 0.0105 (0.0470)	
training:	Epoch: [30][352/817]	Loss 0.0090 (0.0468)	
training:	Epoch: [30][353/817]	Loss 0.0059 (0.0467)	
training:	Epoch: [30][354/817]	Loss 0.0078 (0.0466)	
training:	Epoch: [30][355/817]	Loss 0.0067 (0.0465)	
training:	Epoch: [30][356/817]	Loss 0.0087 (0.0464)	
training:	Epoch: [30][357/817]	Loss 0.0082 (0.0463)	
training:	Epoch: [30][358/817]	Loss 0.0069 (0.0462)	
training:	Epoch: [30][359/817]	Loss 0.0065 (0.0461)	
training:	Epoch: [30][360/817]	Loss 0.0058 (0.0460)	
training:	Epoch: [30][361/817]	Loss 0.0081 (0.0459)	
training:	Epoch: [30][362/817]	Loss 0.0071 (0.0458)	
training:	Epoch: [30][363/817]	Loss 0.0082 (0.0456)	
training:	Epoch: [30][364/817]	Loss 0.0085 (0.0455)	
training:	Epoch: [30][365/817]	Loss 0.0170 (0.0455)	
training:	Epoch: [30][366/817]	Loss 0.0068 (0.0454)	
training:	Epoch: [30][367/817]	Loss 0.0063 (0.0453)	
training:	Epoch: [30][368/817]	Loss 0.0069 (0.0452)	
training:	Epoch: [30][369/817]	Loss 0.1227 (0.0454)	
training:	Epoch: [30][370/817]	Loss 0.0071 (0.0453)	
training:	Epoch: [30][371/817]	Loss 0.0083 (0.0452)	
training:	Epoch: [30][372/817]	Loss 0.0080 (0.0451)	
training:	Epoch: [30][373/817]	Loss 0.0073 (0.0450)	
training:	Epoch: [30][374/817]	Loss 0.0084 (0.0449)	
training:	Epoch: [30][375/817]	Loss 0.0068 (0.0448)	
training:	Epoch: [30][376/817]	Loss 0.0080 (0.0447)	
training:	Epoch: [30][377/817]	Loss 0.0062 (0.0446)	
training:	Epoch: [30][378/817]	Loss 0.0067 (0.0445)	
training:	Epoch: [30][379/817]	Loss 0.0072 (0.0444)	
training:	Epoch: [30][380/817]	Loss 0.0074 (0.0443)	
training:	Epoch: [30][381/817]	Loss 0.6063 (0.0457)	
training:	Epoch: [30][382/817]	Loss 0.0080 (0.0456)	
training:	Epoch: [30][383/817]	Loss 0.0059 (0.0455)	
training:	Epoch: [30][384/817]	Loss 0.0193 (0.0455)	
training:	Epoch: [30][385/817]	Loss 0.0104 (0.0454)	
training:	Epoch: [30][386/817]	Loss 0.1072 (0.0455)	
training:	Epoch: [30][387/817]	Loss 0.0062 (0.0454)	
training:	Epoch: [30][388/817]	Loss 0.6424 (0.0470)	
training:	Epoch: [30][389/817]	Loss 0.0064 (0.0469)	
training:	Epoch: [30][390/817]	Loss 0.0060 (0.0468)	
training:	Epoch: [30][391/817]	Loss 0.0072 (0.0467)	
training:	Epoch: [30][392/817]	Loss 0.0077 (0.0466)	
training:	Epoch: [30][393/817]	Loss 0.0078 (0.0465)	
training:	Epoch: [30][394/817]	Loss 0.0062 (0.0464)	
training:	Epoch: [30][395/817]	Loss 0.0091 (0.0463)	
training:	Epoch: [30][396/817]	Loss 0.0090 (0.0462)	
training:	Epoch: [30][397/817]	Loss 0.0087 (0.0461)	
training:	Epoch: [30][398/817]	Loss 0.0071 (0.0460)	
training:	Epoch: [30][399/817]	Loss 0.0093 (0.0459)	
training:	Epoch: [30][400/817]	Loss 0.0181 (0.0458)	
training:	Epoch: [30][401/817]	Loss 0.0176 (0.0457)	
training:	Epoch: [30][402/817]	Loss 0.0099 (0.0457)	
training:	Epoch: [30][403/817]	Loss 0.0109 (0.0456)	
training:	Epoch: [30][404/817]	Loss 0.0089 (0.0455)	
training:	Epoch: [30][405/817]	Loss 0.0077 (0.0454)	
training:	Epoch: [30][406/817]	Loss 0.5785 (0.0467)	
training:	Epoch: [30][407/817]	Loss 0.0071 (0.0466)	
training:	Epoch: [30][408/817]	Loss 0.0080 (0.0465)	
training:	Epoch: [30][409/817]	Loss 0.0364 (0.0465)	
training:	Epoch: [30][410/817]	Loss 0.0075 (0.0464)	
training:	Epoch: [30][411/817]	Loss 0.0081 (0.0463)	
training:	Epoch: [30][412/817]	Loss 0.0073 (0.0462)	
training:	Epoch: [30][413/817]	Loss 0.0076 (0.0461)	
training:	Epoch: [30][414/817]	Loss 0.0080 (0.0460)	
training:	Epoch: [30][415/817]	Loss 0.0069 (0.0459)	
training:	Epoch: [30][416/817]	Loss 0.0076 (0.0458)	
training:	Epoch: [30][417/817]	Loss 0.0064 (0.0457)	
training:	Epoch: [30][418/817]	Loss 0.0073 (0.0456)	
training:	Epoch: [30][419/817]	Loss 0.0074 (0.0456)	
training:	Epoch: [30][420/817]	Loss 0.0070 (0.0455)	
training:	Epoch: [30][421/817]	Loss 0.0065 (0.0454)	
training:	Epoch: [30][422/817]	Loss 0.0082 (0.0453)	
training:	Epoch: [30][423/817]	Loss 0.0077 (0.0452)	
training:	Epoch: [30][424/817]	Loss 0.0088 (0.0451)	
training:	Epoch: [30][425/817]	Loss 0.0500 (0.0451)	
training:	Epoch: [30][426/817]	Loss 0.0076 (0.0450)	
training:	Epoch: [30][427/817]	Loss 0.0091 (0.0449)	
training:	Epoch: [30][428/817]	Loss 0.0084 (0.0449)	
training:	Epoch: [30][429/817]	Loss 0.0078 (0.0448)	
training:	Epoch: [30][430/817]	Loss 0.0090 (0.0447)	
training:	Epoch: [30][431/817]	Loss 0.0065 (0.0446)	
training:	Epoch: [30][432/817]	Loss 0.0079 (0.0445)	
training:	Epoch: [30][433/817]	Loss 0.0084 (0.0444)	
training:	Epoch: [30][434/817]	Loss 0.0103 (0.0444)	
training:	Epoch: [30][435/817]	Loss 0.0080 (0.0443)	
training:	Epoch: [30][436/817]	Loss 0.0080 (0.0442)	
training:	Epoch: [30][437/817]	Loss 0.0088 (0.0441)	
training:	Epoch: [30][438/817]	Loss 0.0063 (0.0440)	
training:	Epoch: [30][439/817]	Loss 0.0088 (0.0439)	
training:	Epoch: [30][440/817]	Loss 0.0075 (0.0439)	
training:	Epoch: [30][441/817]	Loss 0.0063 (0.0438)	
training:	Epoch: [30][442/817]	Loss 0.0083 (0.0437)	
training:	Epoch: [30][443/817]	Loss 0.0095 (0.0436)	
training:	Epoch: [30][444/817]	Loss 0.0062 (0.0435)	
training:	Epoch: [30][445/817]	Loss 0.0072 (0.0435)	
training:	Epoch: [30][446/817]	Loss 0.0080 (0.0434)	
training:	Epoch: [30][447/817]	Loss 0.0089 (0.0433)	
training:	Epoch: [30][448/817]	Loss 0.0070 (0.0432)	
training:	Epoch: [30][449/817]	Loss 0.0063 (0.0431)	
training:	Epoch: [30][450/817]	Loss 0.0068 (0.0431)	
training:	Epoch: [30][451/817]	Loss 0.5582 (0.0442)	
training:	Epoch: [30][452/817]	Loss 0.0079 (0.0441)	
training:	Epoch: [30][453/817]	Loss 0.0073 (0.0440)	
training:	Epoch: [30][454/817]	Loss 0.0120 (0.0440)	
training:	Epoch: [30][455/817]	Loss 0.0071 (0.0439)	
training:	Epoch: [30][456/817]	Loss 0.0064 (0.0438)	
training:	Epoch: [30][457/817]	Loss 0.0072 (0.0437)	
training:	Epoch: [30][458/817]	Loss 0.0065 (0.0436)	
training:	Epoch: [30][459/817]	Loss 0.0079 (0.0436)	
training:	Epoch: [30][460/817]	Loss 0.0056 (0.0435)	
training:	Epoch: [30][461/817]	Loss 0.0104 (0.0434)	
training:	Epoch: [30][462/817]	Loss 0.0083 (0.0433)	
training:	Epoch: [30][463/817]	Loss 0.0072 (0.0432)	
training:	Epoch: [30][464/817]	Loss 0.5211 (0.0443)	
training:	Epoch: [30][465/817]	Loss 0.0056 (0.0442)	
training:	Epoch: [30][466/817]	Loss 0.0089 (0.0441)	
training:	Epoch: [30][467/817]	Loss 0.0095 (0.0440)	
training:	Epoch: [30][468/817]	Loss 0.0068 (0.0440)	
training:	Epoch: [30][469/817]	Loss 0.0069 (0.0439)	
training:	Epoch: [30][470/817]	Loss 0.0073 (0.0438)	
training:	Epoch: [30][471/817]	Loss 0.0070 (0.0437)	
training:	Epoch: [30][472/817]	Loss 0.0070 (0.0437)	
training:	Epoch: [30][473/817]	Loss 0.0078 (0.0436)	
training:	Epoch: [30][474/817]	Loss 0.0090 (0.0435)	
training:	Epoch: [30][475/817]	Loss 0.0087 (0.0434)	
training:	Epoch: [30][476/817]	Loss 0.0080 (0.0434)	
training:	Epoch: [30][477/817]	Loss 0.0083 (0.0433)	
training:	Epoch: [30][478/817]	Loss 0.0077 (0.0432)	
training:	Epoch: [30][479/817]	Loss 0.0633 (0.0433)	
training:	Epoch: [30][480/817]	Loss 0.0735 (0.0433)	
training:	Epoch: [30][481/817]	Loss 0.0070 (0.0432)	
training:	Epoch: [30][482/817]	Loss 0.0083 (0.0432)	
training:	Epoch: [30][483/817]	Loss 0.0073 (0.0431)	
training:	Epoch: [30][484/817]	Loss 0.0079 (0.0430)	
training:	Epoch: [30][485/817]	Loss 0.0075 (0.0429)	
training:	Epoch: [30][486/817]	Loss 0.0063 (0.0429)	
training:	Epoch: [30][487/817]	Loss 0.0087 (0.0428)	
training:	Epoch: [30][488/817]	Loss 0.0086 (0.0427)	
training:	Epoch: [30][489/817]	Loss 0.0073 (0.0427)	
training:	Epoch: [30][490/817]	Loss 0.0070 (0.0426)	
training:	Epoch: [30][491/817]	Loss 0.0064 (0.0425)	
training:	Epoch: [30][492/817]	Loss 0.0089 (0.0424)	
training:	Epoch: [30][493/817]	Loss 0.0065 (0.0424)	
training:	Epoch: [30][494/817]	Loss 0.0068 (0.0423)	
training:	Epoch: [30][495/817]	Loss 0.0063 (0.0422)	
training:	Epoch: [30][496/817]	Loss 0.0085 (0.0422)	
training:	Epoch: [30][497/817]	Loss 0.0075 (0.0421)	
training:	Epoch: [30][498/817]	Loss 0.0132 (0.0420)	
training:	Epoch: [30][499/817]	Loss 0.0090 (0.0420)	
training:	Epoch: [30][500/817]	Loss 0.0506 (0.0420)	
training:	Epoch: [30][501/817]	Loss 0.0077 (0.0419)	
training:	Epoch: [30][502/817]	Loss 0.0067 (0.0418)	
training:	Epoch: [30][503/817]	Loss 0.3206 (0.0424)	
training:	Epoch: [30][504/817]	Loss 0.0062 (0.0423)	
training:	Epoch: [30][505/817]	Loss 0.0079 (0.0423)	
training:	Epoch: [30][506/817]	Loss 0.0064 (0.0422)	
training:	Epoch: [30][507/817]	Loss 0.0064 (0.0421)	
training:	Epoch: [30][508/817]	Loss 0.0070 (0.0420)	
training:	Epoch: [30][509/817]	Loss 0.0067 (0.0420)	
training:	Epoch: [30][510/817]	Loss 0.0086 (0.0419)	
training:	Epoch: [30][511/817]	Loss 0.0068 (0.0418)	
training:	Epoch: [30][512/817]	Loss 0.0072 (0.0418)	
training:	Epoch: [30][513/817]	Loss 0.0070 (0.0417)	
training:	Epoch: [30][514/817]	Loss 0.0069 (0.0416)	
training:	Epoch: [30][515/817]	Loss 0.0062 (0.0416)	
training:	Epoch: [30][516/817]	Loss 0.0065 (0.0415)	
training:	Epoch: [30][517/817]	Loss 0.0120 (0.0414)	
training:	Epoch: [30][518/817]	Loss 0.4145 (0.0422)	
training:	Epoch: [30][519/817]	Loss 0.0065 (0.0421)	
training:	Epoch: [30][520/817]	Loss 0.0087 (0.0420)	
training:	Epoch: [30][521/817]	Loss 0.0080 (0.0420)	
training:	Epoch: [30][522/817]	Loss 0.0084 (0.0419)	
training:	Epoch: [30][523/817]	Loss 0.0064 (0.0418)	
training:	Epoch: [30][524/817]	Loss 0.0078 (0.0418)	
training:	Epoch: [30][525/817]	Loss 0.0077 (0.0417)	
training:	Epoch: [30][526/817]	Loss 0.0080 (0.0416)	
training:	Epoch: [30][527/817]	Loss 0.0058 (0.0416)	
training:	Epoch: [30][528/817]	Loss 0.0066 (0.0415)	
training:	Epoch: [30][529/817]	Loss 0.0069 (0.0414)	
training:	Epoch: [30][530/817]	Loss 0.0073 (0.0414)	
training:	Epoch: [30][531/817]	Loss 0.0063 (0.0413)	
training:	Epoch: [30][532/817]	Loss 0.0125 (0.0413)	
training:	Epoch: [30][533/817]	Loss 0.0089 (0.0412)	
training:	Epoch: [30][534/817]	Loss 0.0085 (0.0411)	
training:	Epoch: [30][535/817]	Loss 0.0075 (0.0411)	
training:	Epoch: [30][536/817]	Loss 0.0079 (0.0410)	
training:	Epoch: [30][537/817]	Loss 0.1111 (0.0411)	
training:	Epoch: [30][538/817]	Loss 0.0062 (0.0411)	
training:	Epoch: [30][539/817]	Loss 0.0080 (0.0410)	
training:	Epoch: [30][540/817]	Loss 0.0081 (0.0410)	
training:	Epoch: [30][541/817]	Loss 0.0064 (0.0409)	
training:	Epoch: [30][542/817]	Loss 0.0086 (0.0408)	
training:	Epoch: [30][543/817]	Loss 0.0083 (0.0408)	
training:	Epoch: [30][544/817]	Loss 0.0051 (0.0407)	
training:	Epoch: [30][545/817]	Loss 0.0067 (0.0406)	
training:	Epoch: [30][546/817]	Loss 0.0072 (0.0406)	
training:	Epoch: [30][547/817]	Loss 0.0068 (0.0405)	
training:	Epoch: [30][548/817]	Loss 0.1575 (0.0407)	
training:	Epoch: [30][549/817]	Loss 0.0067 (0.0407)	
training:	Epoch: [30][550/817]	Loss 0.0071 (0.0406)	
training:	Epoch: [30][551/817]	Loss 0.0076 (0.0405)	
training:	Epoch: [30][552/817]	Loss 0.0054 (0.0405)	
training:	Epoch: [30][553/817]	Loss 0.0077 (0.0404)	
training:	Epoch: [30][554/817]	Loss 0.0079 (0.0404)	
training:	Epoch: [30][555/817]	Loss 0.0084 (0.0403)	
training:	Epoch: [30][556/817]	Loss 0.0069 (0.0402)	
training:	Epoch: [30][557/817]	Loss 0.0061 (0.0402)	
training:	Epoch: [30][558/817]	Loss 0.0053 (0.0401)	
training:	Epoch: [30][559/817]	Loss 0.0071 (0.0401)	
training:	Epoch: [30][560/817]	Loss 0.0064 (0.0400)	
training:	Epoch: [30][561/817]	Loss 0.0065 (0.0399)	
training:	Epoch: [30][562/817]	Loss 0.0070 (0.0399)	
training:	Epoch: [30][563/817]	Loss 0.0065 (0.0398)	
training:	Epoch: [30][564/817]	Loss 0.1321 (0.0400)	
training:	Epoch: [30][565/817]	Loss 0.0063 (0.0399)	
training:	Epoch: [30][566/817]	Loss 0.0067 (0.0399)	
training:	Epoch: [30][567/817]	Loss 0.0074 (0.0398)	
training:	Epoch: [30][568/817]	Loss 0.0062 (0.0398)	
training:	Epoch: [30][569/817]	Loss 0.0087 (0.0397)	
training:	Epoch: [30][570/817]	Loss 0.0069 (0.0396)	
training:	Epoch: [30][571/817]	Loss 0.0082 (0.0396)	
training:	Epoch: [30][572/817]	Loss 0.0067 (0.0395)	
training:	Epoch: [30][573/817]	Loss 0.6036 (0.0405)	
training:	Epoch: [30][574/817]	Loss 0.0071 (0.0405)	
training:	Epoch: [30][575/817]	Loss 0.2395 (0.0408)	
training:	Epoch: [30][576/817]	Loss 0.0073 (0.0407)	
training:	Epoch: [30][577/817]	Loss 0.0053 (0.0407)	
training:	Epoch: [30][578/817]	Loss 0.0087 (0.0406)	
training:	Epoch: [30][579/817]	Loss 0.0073 (0.0406)	
training:	Epoch: [30][580/817]	Loss 0.0061 (0.0405)	
training:	Epoch: [30][581/817]	Loss 0.0066 (0.0405)	
training:	Epoch: [30][582/817]	Loss 0.0094 (0.0404)	
training:	Epoch: [30][583/817]	Loss 0.0063 (0.0403)	
training:	Epoch: [30][584/817]	Loss 0.0069 (0.0403)	
training:	Epoch: [30][585/817]	Loss 0.6518 (0.0413)	
training:	Epoch: [30][586/817]	Loss 0.0066 (0.0413)	
training:	Epoch: [30][587/817]	Loss 0.0065 (0.0412)	
training:	Epoch: [30][588/817]	Loss 0.0077 (0.0412)	
training:	Epoch: [30][589/817]	Loss 0.0064 (0.0411)	
training:	Epoch: [30][590/817]	Loss 0.0066 (0.0410)	
training:	Epoch: [30][591/817]	Loss 0.0058 (0.0410)	
training:	Epoch: [30][592/817]	Loss 0.0074 (0.0409)	
training:	Epoch: [30][593/817]	Loss 0.0065 (0.0409)	
training:	Epoch: [30][594/817]	Loss 0.0082 (0.0408)	
training:	Epoch: [30][595/817]	Loss 0.0062 (0.0408)	
training:	Epoch: [30][596/817]	Loss 0.0086 (0.0407)	
training:	Epoch: [30][597/817]	Loss 0.0068 (0.0406)	
training:	Epoch: [30][598/817]	Loss 0.0060 (0.0406)	
training:	Epoch: [30][599/817]	Loss 0.0080 (0.0405)	
training:	Epoch: [30][600/817]	Loss 0.0085 (0.0405)	
training:	Epoch: [30][601/817]	Loss 0.0085 (0.0404)	
training:	Epoch: [30][602/817]	Loss 0.0085 (0.0404)	
training:	Epoch: [30][603/817]	Loss 0.0061 (0.0403)	
training:	Epoch: [30][604/817]	Loss 0.0069 (0.0403)	
training:	Epoch: [30][605/817]	Loss 0.0081 (0.0402)	
training:	Epoch: [30][606/817]	Loss 0.0062 (0.0401)	
training:	Epoch: [30][607/817]	Loss 0.0073 (0.0401)	
training:	Epoch: [30][608/817]	Loss 0.0063 (0.0400)	
training:	Epoch: [30][609/817]	Loss 0.0059 (0.0400)	
training:	Epoch: [30][610/817]	Loss 0.3390 (0.0405)	
training:	Epoch: [30][611/817]	Loss 0.0068 (0.0404)	
training:	Epoch: [30][612/817]	Loss 0.0058 (0.0404)	
training:	Epoch: [30][613/817]	Loss 0.0067 (0.0403)	
training:	Epoch: [30][614/817]	Loss 0.0080 (0.0403)	
training:	Epoch: [30][615/817]	Loss 0.0290 (0.0402)	
training:	Epoch: [30][616/817]	Loss 0.0062 (0.0402)	
training:	Epoch: [30][617/817]	Loss 0.0060 (0.0401)	
training:	Epoch: [30][618/817]	Loss 0.0064 (0.0401)	
training:	Epoch: [30][619/817]	Loss 0.0079 (0.0400)	
training:	Epoch: [30][620/817]	Loss 0.0062 (0.0400)	
training:	Epoch: [30][621/817]	Loss 0.0081 (0.0399)	
training:	Epoch: [30][622/817]	Loss 0.0063 (0.0399)	
training:	Epoch: [30][623/817]	Loss 0.2772 (0.0402)	
training:	Epoch: [30][624/817]	Loss 0.0649 (0.0403)	
training:	Epoch: [30][625/817]	Loss 0.0058 (0.0402)	
training:	Epoch: [30][626/817]	Loss 0.0068 (0.0402)	
training:	Epoch: [30][627/817]	Loss 0.0139 (0.0401)	
training:	Epoch: [30][628/817]	Loss 0.0055 (0.0401)	
training:	Epoch: [30][629/817]	Loss 0.0059 (0.0400)	
training:	Epoch: [30][630/817]	Loss 0.1396 (0.0402)	
training:	Epoch: [30][631/817]	Loss 0.0061 (0.0401)	
training:	Epoch: [30][632/817]	Loss 0.0077 (0.0401)	
training:	Epoch: [30][633/817]	Loss 0.0060 (0.0400)	
training:	Epoch: [30][634/817]	Loss 0.0105 (0.0400)	
training:	Epoch: [30][635/817]	Loss 0.0074 (0.0399)	
training:	Epoch: [30][636/817]	Loss 0.0061 (0.0399)	
training:	Epoch: [30][637/817]	Loss 0.0080 (0.0398)	
training:	Epoch: [30][638/817]	Loss 0.2098 (0.0401)	
training:	Epoch: [30][639/817]	Loss 0.6114 (0.0410)	
training:	Epoch: [30][640/817]	Loss 0.0097 (0.0409)	
training:	Epoch: [30][641/817]	Loss 0.0378 (0.0409)	
training:	Epoch: [30][642/817]	Loss 0.0072 (0.0409)	
training:	Epoch: [30][643/817]	Loss 0.0066 (0.0408)	
training:	Epoch: [30][644/817]	Loss 0.6283 (0.0417)	
training:	Epoch: [30][645/817]	Loss 0.0066 (0.0417)	
training:	Epoch: [30][646/817]	Loss 0.1283 (0.0418)	
training:	Epoch: [30][647/817]	Loss 0.0057 (0.0418)	
training:	Epoch: [30][648/817]	Loss 0.0074 (0.0417)	
training:	Epoch: [30][649/817]	Loss 0.0130 (0.0417)	
training:	Epoch: [30][650/817]	Loss 0.6172 (0.0425)	
training:	Epoch: [30][651/817]	Loss 0.0075 (0.0425)	
training:	Epoch: [30][652/817]	Loss 0.0071 (0.0424)	
training:	Epoch: [30][653/817]	Loss 0.0060 (0.0424)	
training:	Epoch: [30][654/817]	Loss 0.0073 (0.0423)	
training:	Epoch: [30][655/817]	Loss 0.0063 (0.0423)	
training:	Epoch: [30][656/817]	Loss 0.0074 (0.0422)	
training:	Epoch: [30][657/817]	Loss 0.0314 (0.0422)	
training:	Epoch: [30][658/817]	Loss 0.0067 (0.0421)	
training:	Epoch: [30][659/817]	Loss 0.0074 (0.0421)	
training:	Epoch: [30][660/817]	Loss 0.0065 (0.0420)	
training:	Epoch: [30][661/817]	Loss 0.0109 (0.0420)	
training:	Epoch: [30][662/817]	Loss 0.0068 (0.0419)	
training:	Epoch: [30][663/817]	Loss 0.0062 (0.0419)	
training:	Epoch: [30][664/817]	Loss 0.0071 (0.0418)	
training:	Epoch: [30][665/817]	Loss 0.0073 (0.0418)	
training:	Epoch: [30][666/817]	Loss 0.0118 (0.0417)	
training:	Epoch: [30][667/817]	Loss 0.5756 (0.0425)	
training:	Epoch: [30][668/817]	Loss 0.0071 (0.0425)	
training:	Epoch: [30][669/817]	Loss 0.0075 (0.0424)	
training:	Epoch: [30][670/817]	Loss 0.0064 (0.0424)	
training:	Epoch: [30][671/817]	Loss 0.0066 (0.0423)	
training:	Epoch: [30][672/817]	Loss 0.0076 (0.0423)	
training:	Epoch: [30][673/817]	Loss 0.0066 (0.0422)	
training:	Epoch: [30][674/817]	Loss 0.0060 (0.0422)	
training:	Epoch: [30][675/817]	Loss 0.5304 (0.0429)	
training:	Epoch: [30][676/817]	Loss 0.0083 (0.0428)	
training:	Epoch: [30][677/817]	Loss 0.0073 (0.0428)	
training:	Epoch: [30][678/817]	Loss 0.0068 (0.0427)	
training:	Epoch: [30][679/817]	Loss 0.0071 (0.0427)	
training:	Epoch: [30][680/817]	Loss 0.0070 (0.0426)	
training:	Epoch: [30][681/817]	Loss 0.0118 (0.0426)	
training:	Epoch: [30][682/817]	Loss 0.0071 (0.0425)	
training:	Epoch: [30][683/817]	Loss 0.0072 (0.0425)	
training:	Epoch: [30][684/817]	Loss 0.0075 (0.0424)	
training:	Epoch: [30][685/817]	Loss 0.0071 (0.0424)	
training:	Epoch: [30][686/817]	Loss 0.0075 (0.0423)	
training:	Epoch: [30][687/817]	Loss 0.0061 (0.0423)	
training:	Epoch: [30][688/817]	Loss 0.0052 (0.0422)	
training:	Epoch: [30][689/817]	Loss 0.0073 (0.0422)	
training:	Epoch: [30][690/817]	Loss 0.0064 (0.0421)	
training:	Epoch: [30][691/817]	Loss 0.0086 (0.0421)	
training:	Epoch: [30][692/817]	Loss 0.0113 (0.0420)	
training:	Epoch: [30][693/817]	Loss 0.0063 (0.0420)	
training:	Epoch: [30][694/817]	Loss 0.0065 (0.0419)	
training:	Epoch: [30][695/817]	Loss 0.0083 (0.0419)	
training:	Epoch: [30][696/817]	Loss 0.0072 (0.0418)	
training:	Epoch: [30][697/817]	Loss 0.0061 (0.0418)	
training:	Epoch: [30][698/817]	Loss 0.0071 (0.0417)	
training:	Epoch: [30][699/817]	Loss 0.0090 (0.0417)	
training:	Epoch: [30][700/817]	Loss 0.0069 (0.0416)	
training:	Epoch: [30][701/817]	Loss 0.0061 (0.0416)	
training:	Epoch: [30][702/817]	Loss 0.5662 (0.0423)	
training:	Epoch: [30][703/817]	Loss 0.0074 (0.0423)	
training:	Epoch: [30][704/817]	Loss 0.0070 (0.0422)	
training:	Epoch: [30][705/817]	Loss 0.0058 (0.0422)	
training:	Epoch: [30][706/817]	Loss 0.0066 (0.0421)	
training:	Epoch: [30][707/817]	Loss 0.0075 (0.0421)	
training:	Epoch: [30][708/817]	Loss 0.0060 (0.0420)	
training:	Epoch: [30][709/817]	Loss 0.0087 (0.0420)	
training:	Epoch: [30][710/817]	Loss 0.0084 (0.0419)	
training:	Epoch: [30][711/817]	Loss 0.0081 (0.0419)	
training:	Epoch: [30][712/817]	Loss 0.0148 (0.0418)	
training:	Epoch: [30][713/817]	Loss 0.0069 (0.0418)	
training:	Epoch: [30][714/817]	Loss 0.0070 (0.0417)	
training:	Epoch: [30][715/817]	Loss 0.0073 (0.0417)	
training:	Epoch: [30][716/817]	Loss 0.0082 (0.0416)	
training:	Epoch: [30][717/817]	Loss 0.0062 (0.0416)	
training:	Epoch: [30][718/817]	Loss 0.0085 (0.0415)	
training:	Epoch: [30][719/817]	Loss 0.0064 (0.0415)	
training:	Epoch: [30][720/817]	Loss 0.0073 (0.0415)	
training:	Epoch: [30][721/817]	Loss 0.0108 (0.0414)	
training:	Epoch: [30][722/817]	Loss 0.0062 (0.0414)	
training:	Epoch: [30][723/817]	Loss 0.0083 (0.0413)	
training:	Epoch: [30][724/817]	Loss 0.0073 (0.0413)	
training:	Epoch: [30][725/817]	Loss 0.0074 (0.0412)	
training:	Epoch: [30][726/817]	Loss 0.0073 (0.0412)	
training:	Epoch: [30][727/817]	Loss 0.0080 (0.0411)	
training:	Epoch: [30][728/817]	Loss 0.0061 (0.0411)	
training:	Epoch: [30][729/817]	Loss 0.0070 (0.0410)	
training:	Epoch: [30][730/817]	Loss 0.0065 (0.0410)	
training:	Epoch: [30][731/817]	Loss 0.0074 (0.0409)	
training:	Epoch: [30][732/817]	Loss 0.0068 (0.0409)	
training:	Epoch: [30][733/817]	Loss 0.0069 (0.0408)	
training:	Epoch: [30][734/817]	Loss 0.0079 (0.0408)	
training:	Epoch: [30][735/817]	Loss 0.0063 (0.0408)	
training:	Epoch: [30][736/817]	Loss 0.0057 (0.0407)	
training:	Epoch: [30][737/817]	Loss 0.0075 (0.0407)	
training:	Epoch: [30][738/817]	Loss 0.0076 (0.0406)	
training:	Epoch: [30][739/817]	Loss 0.6156 (0.0414)	
training:	Epoch: [30][740/817]	Loss 0.0078 (0.0414)	
training:	Epoch: [30][741/817]	Loss 0.0054 (0.0413)	
training:	Epoch: [30][742/817]	Loss 0.0063 (0.0413)	
training:	Epoch: [30][743/817]	Loss 0.0084 (0.0412)	
training:	Epoch: [30][744/817]	Loss 0.0071 (0.0412)	
training:	Epoch: [30][745/817]	Loss 0.0052 (0.0411)	
training:	Epoch: [30][746/817]	Loss 0.0075 (0.0411)	
training:	Epoch: [30][747/817]	Loss 0.0070 (0.0410)	
training:	Epoch: [30][748/817]	Loss 0.0068 (0.0410)	
training:	Epoch: [30][749/817]	Loss 0.0060 (0.0409)	
training:	Epoch: [30][750/817]	Loss 0.0058 (0.0409)	
training:	Epoch: [30][751/817]	Loss 0.0079 (0.0408)	
training:	Epoch: [30][752/817]	Loss 0.0068 (0.0408)	
training:	Epoch: [30][753/817]	Loss 0.0118 (0.0408)	
training:	Epoch: [30][754/817]	Loss 0.0056 (0.0407)	
training:	Epoch: [30][755/817]	Loss 0.0075 (0.0407)	
training:	Epoch: [30][756/817]	Loss 0.0068 (0.0406)	
training:	Epoch: [30][757/817]	Loss 0.0059 (0.0406)	
training:	Epoch: [30][758/817]	Loss 0.0088 (0.0405)	
training:	Epoch: [30][759/817]	Loss 0.0056 (0.0405)	
training:	Epoch: [30][760/817]	Loss 0.6585 (0.0413)	
training:	Epoch: [30][761/817]	Loss 0.0065 (0.0413)	
training:	Epoch: [30][762/817]	Loss 0.0071 (0.0412)	
training:	Epoch: [30][763/817]	Loss 0.0097 (0.0412)	
training:	Epoch: [30][764/817]	Loss 0.0068 (0.0411)	
training:	Epoch: [30][765/817]	Loss 0.0071 (0.0411)	
training:	Epoch: [30][766/817]	Loss 0.0077 (0.0410)	
training:	Epoch: [30][767/817]	Loss 0.0072 (0.0410)	
training:	Epoch: [30][768/817]	Loss 0.0074 (0.0410)	
training:	Epoch: [30][769/817]	Loss 0.0090 (0.0409)	
training:	Epoch: [30][770/817]	Loss 0.0066 (0.0409)	
training:	Epoch: [30][771/817]	Loss 0.0084 (0.0408)	
training:	Epoch: [30][772/817]	Loss 0.0075 (0.0408)	
training:	Epoch: [30][773/817]	Loss 0.0062 (0.0407)	
training:	Epoch: [30][774/817]	Loss 0.0079 (0.0407)	
training:	Epoch: [30][775/817]	Loss 0.0079 (0.0407)	
training:	Epoch: [30][776/817]	Loss 0.0094 (0.0406)	
training:	Epoch: [30][777/817]	Loss 0.0057 (0.0406)	
training:	Epoch: [30][778/817]	Loss 0.0065 (0.0405)	
training:	Epoch: [30][779/817]	Loss 0.0075 (0.0405)	
training:	Epoch: [30][780/817]	Loss 0.0073 (0.0404)	
training:	Epoch: [30][781/817]	Loss 0.0064 (0.0404)	
training:	Epoch: [30][782/817]	Loss 0.0063 (0.0403)	
training:	Epoch: [30][783/817]	Loss 0.0066 (0.0403)	
training:	Epoch: [30][784/817]	Loss 0.0075 (0.0403)	
training:	Epoch: [30][785/817]	Loss 0.0054 (0.0402)	
training:	Epoch: [30][786/817]	Loss 0.0068 (0.0402)	
training:	Epoch: [30][787/817]	Loss 0.4224 (0.0407)	
training:	Epoch: [30][788/817]	Loss 0.0067 (0.0406)	
training:	Epoch: [30][789/817]	Loss 0.0071 (0.0406)	
training:	Epoch: [30][790/817]	Loss 0.0075 (0.0405)	
training:	Epoch: [30][791/817]	Loss 0.0080 (0.0405)	
training:	Epoch: [30][792/817]	Loss 0.0081 (0.0405)	
training:	Epoch: [30][793/817]	Loss 0.0059 (0.0404)	
training:	Epoch: [30][794/817]	Loss 0.0066 (0.0404)	
training:	Epoch: [30][795/817]	Loss 0.5420 (0.0410)	
training:	Epoch: [30][796/817]	Loss 0.0073 (0.0410)	
training:	Epoch: [30][797/817]	Loss 0.0080 (0.0409)	
training:	Epoch: [30][798/817]	Loss 0.0083 (0.0409)	
training:	Epoch: [30][799/817]	Loss 0.0086 (0.0408)	
training:	Epoch: [30][800/817]	Loss 0.0475 (0.0408)	
training:	Epoch: [30][801/817]	Loss 0.0079 (0.0408)	
training:	Epoch: [30][802/817]	Loss 0.0080 (0.0408)	
training:	Epoch: [30][803/817]	Loss 0.0062 (0.0407)	
training:	Epoch: [30][804/817]	Loss 0.0096 (0.0407)	
training:	Epoch: [30][805/817]	Loss 0.0078 (0.0406)	
training:	Epoch: [30][806/817]	Loss 0.5717 (0.0413)	
training:	Epoch: [30][807/817]	Loss 0.0118 (0.0413)	
training:	Epoch: [30][808/817]	Loss 0.0071 (0.0412)	
training:	Epoch: [30][809/817]	Loss 0.0081 (0.0412)	
training:	Epoch: [30][810/817]	Loss 0.0069 (0.0411)	
training:	Epoch: [30][811/817]	Loss 0.0128 (0.0411)	
training:	Epoch: [30][812/817]	Loss 0.0067 (0.0411)	
training:	Epoch: [30][813/817]	Loss 0.0089 (0.0410)	
training:	Epoch: [30][814/817]	Loss 0.0093 (0.0410)	
training:	Epoch: [30][815/817]	Loss 0.0074 (0.0409)	
training:	Epoch: [30][816/817]	Loss 0.0066 (0.0409)	
training:	Epoch: [30][817/817]	Loss 0.0116 (0.0409)	
Training:	 Loss: 0.0408

Training:	 ACC: 0.9936 0.9936 0.9941 0.9930
Validation:	 ACC: 0.7865 0.7887 0.8332 0.7399
Validation:	 Best_BACC: 0.8223 0.8218 0.8127 0.8318
Validation:	 Loss: 0.9375
Pretraining:	Epoch 31/200
----------
training:	Epoch: [31][1/817]	Loss 0.0069 (0.0069)	
training:	Epoch: [31][2/817]	Loss 0.0060 (0.0064)	
training:	Epoch: [31][3/817]	Loss 0.0064 (0.0064)	
training:	Epoch: [31][4/817]	Loss 0.0074 (0.0067)	
training:	Epoch: [31][5/817]	Loss 0.0691 (0.0192)	
training:	Epoch: [31][6/817]	Loss 0.0065 (0.0170)	
training:	Epoch: [31][7/817]	Loss 0.0077 (0.0157)	
training:	Epoch: [31][8/817]	Loss 0.0086 (0.0148)	
training:	Epoch: [31][9/817]	Loss 0.0063 (0.0139)	
training:	Epoch: [31][10/817]	Loss 0.0061 (0.0131)	
training:	Epoch: [31][11/817]	Loss 0.0080 (0.0126)	
training:	Epoch: [31][12/817]	Loss 0.0059 (0.0121)	
training:	Epoch: [31][13/817]	Loss 0.0065 (0.0116)	
training:	Epoch: [31][14/817]	Loss 0.0086 (0.0114)	
training:	Epoch: [31][15/817]	Loss 0.0074 (0.0112)	
training:	Epoch: [31][16/817]	Loss 0.0107 (0.0111)	
training:	Epoch: [31][17/817]	Loss 0.0063 (0.0108)	
training:	Epoch: [31][18/817]	Loss 0.0061 (0.0106)	
training:	Epoch: [31][19/817]	Loss 0.0079 (0.0104)	
training:	Epoch: [31][20/817]	Loss 0.0069 (0.0103)	
training:	Epoch: [31][21/817]	Loss 0.0063 (0.0101)	
training:	Epoch: [31][22/817]	Loss 0.0079 (0.0100)	
training:	Epoch: [31][23/817]	Loss 0.0068 (0.0098)	
training:	Epoch: [31][24/817]	Loss 0.0069 (0.0097)	
training:	Epoch: [31][25/817]	Loss 0.0067 (0.0096)	
training:	Epoch: [31][26/817]	Loss 0.0072 (0.0095)	
training:	Epoch: [31][27/817]	Loss 0.0074 (0.0094)	
training:	Epoch: [31][28/817]	Loss 0.6522 (0.0324)	
training:	Epoch: [31][29/817]	Loss 0.0065 (0.0315)	
training:	Epoch: [31][30/817]	Loss 0.0065 (0.0307)	
training:	Epoch: [31][31/817]	Loss 0.0065 (0.0299)	
training:	Epoch: [31][32/817]	Loss 0.0098 (0.0293)	
training:	Epoch: [31][33/817]	Loss 0.0077 (0.0286)	
training:	Epoch: [31][34/817]	Loss 0.0067 (0.0280)	
training:	Epoch: [31][35/817]	Loss 0.0066 (0.0274)	
training:	Epoch: [31][36/817]	Loss 0.0065 (0.0268)	
training:	Epoch: [31][37/817]	Loss 0.0065 (0.0262)	
training:	Epoch: [31][38/817]	Loss 0.0067 (0.0257)	
training:	Epoch: [31][39/817]	Loss 0.0073 (0.0252)	
training:	Epoch: [31][40/817]	Loss 0.0076 (0.0248)	
training:	Epoch: [31][41/817]	Loss 0.0063 (0.0243)	
training:	Epoch: [31][42/817]	Loss 0.0077 (0.0239)	
training:	Epoch: [31][43/817]	Loss 0.0072 (0.0236)	
training:	Epoch: [31][44/817]	Loss 0.0082 (0.0232)	
training:	Epoch: [31][45/817]	Loss 0.0067 (0.0228)	
training:	Epoch: [31][46/817]	Loss 0.0078 (0.0225)	
training:	Epoch: [31][47/817]	Loss 0.0107 (0.0223)	
training:	Epoch: [31][48/817]	Loss 0.0157 (0.0221)	
training:	Epoch: [31][49/817]	Loss 0.0076 (0.0218)	
training:	Epoch: [31][50/817]	Loss 0.0066 (0.0215)	
training:	Epoch: [31][51/817]	Loss 0.0059 (0.0212)	
training:	Epoch: [31][52/817]	Loss 0.0056 (0.0209)	
training:	Epoch: [31][53/817]	Loss 0.0060 (0.0206)	
training:	Epoch: [31][54/817]	Loss 0.0059 (0.0204)	
training:	Epoch: [31][55/817]	Loss 0.0063 (0.0201)	
training:	Epoch: [31][56/817]	Loss 0.0071 (0.0199)	
training:	Epoch: [31][57/817]	Loss 0.0078 (0.0197)	
training:	Epoch: [31][58/817]	Loss 0.0068 (0.0194)	
training:	Epoch: [31][59/817]	Loss 0.6343 (0.0299)	
training:	Epoch: [31][60/817]	Loss 0.0060 (0.0295)	
training:	Epoch: [31][61/817]	Loss 0.0084 (0.0291)	
training:	Epoch: [31][62/817]	Loss 0.0060 (0.0287)	
training:	Epoch: [31][63/817]	Loss 0.0073 (0.0284)	
training:	Epoch: [31][64/817]	Loss 0.0060 (0.0281)	
training:	Epoch: [31][65/817]	Loss 0.0065 (0.0277)	
training:	Epoch: [31][66/817]	Loss 0.0085 (0.0274)	
training:	Epoch: [31][67/817]	Loss 0.0057 (0.0271)	
training:	Epoch: [31][68/817]	Loss 0.0065 (0.0268)	
training:	Epoch: [31][69/817]	Loss 0.0078 (0.0265)	
training:	Epoch: [31][70/817]	Loss 0.5975 (0.0347)	
training:	Epoch: [31][71/817]	Loss 0.0081 (0.0343)	
training:	Epoch: [31][72/817]	Loss 0.0071 (0.0339)	
training:	Epoch: [31][73/817]	Loss 0.0225 (0.0338)	
training:	Epoch: [31][74/817]	Loss 0.0075 (0.0334)	
training:	Epoch: [31][75/817]	Loss 0.0066 (0.0331)	
training:	Epoch: [31][76/817]	Loss 0.0057 (0.0327)	
training:	Epoch: [31][77/817]	Loss 0.0072 (0.0324)	
training:	Epoch: [31][78/817]	Loss 0.0076 (0.0321)	
training:	Epoch: [31][79/817]	Loss 0.0064 (0.0317)	
training:	Epoch: [31][80/817]	Loss 0.0074 (0.0314)	
training:	Epoch: [31][81/817]	Loss 0.0054 (0.0311)	
training:	Epoch: [31][82/817]	Loss 0.0067 (0.0308)	
training:	Epoch: [31][83/817]	Loss 0.0059 (0.0305)	
training:	Epoch: [31][84/817]	Loss 0.0081 (0.0302)	
training:	Epoch: [31][85/817]	Loss 0.0079 (0.0300)	
training:	Epoch: [31][86/817]	Loss 0.0059 (0.0297)	
training:	Epoch: [31][87/817]	Loss 0.5237 (0.0354)	
training:	Epoch: [31][88/817]	Loss 0.0073 (0.0351)	
training:	Epoch: [31][89/817]	Loss 0.0070 (0.0347)	
training:	Epoch: [31][90/817]	Loss 0.0074 (0.0344)	
training:	Epoch: [31][91/817]	Loss 0.0050 (0.0341)	
training:	Epoch: [31][92/817]	Loss 0.0069 (0.0338)	
training:	Epoch: [31][93/817]	Loss 0.0055 (0.0335)	
training:	Epoch: [31][94/817]	Loss 0.0050 (0.0332)	
training:	Epoch: [31][95/817]	Loss 0.0068 (0.0329)	
training:	Epoch: [31][96/817]	Loss 0.0060 (0.0327)	
training:	Epoch: [31][97/817]	Loss 0.0086 (0.0324)	
training:	Epoch: [31][98/817]	Loss 0.0063 (0.0321)	
training:	Epoch: [31][99/817]	Loss 0.0072 (0.0319)	
training:	Epoch: [31][100/817]	Loss 0.5861 (0.0374)	
training:	Epoch: [31][101/817]	Loss 0.0075 (0.0371)	
training:	Epoch: [31][102/817]	Loss 0.0060 (0.0368)	
training:	Epoch: [31][103/817]	Loss 0.0069 (0.0365)	
training:	Epoch: [31][104/817]	Loss 0.0060 (0.0362)	
training:	Epoch: [31][105/817]	Loss 0.0070 (0.0360)	
training:	Epoch: [31][106/817]	Loss 0.0080 (0.0357)	
training:	Epoch: [31][107/817]	Loss 0.0063 (0.0354)	
training:	Epoch: [31][108/817]	Loss 0.0062 (0.0352)	
training:	Epoch: [31][109/817]	Loss 0.5821 (0.0402)	
training:	Epoch: [31][110/817]	Loss 0.0074 (0.0399)	
training:	Epoch: [31][111/817]	Loss 0.6355 (0.0452)	
training:	Epoch: [31][112/817]	Loss 0.0063 (0.0449)	
training:	Epoch: [31][113/817]	Loss 0.0062 (0.0446)	
training:	Epoch: [31][114/817]	Loss 0.0061 (0.0442)	
training:	Epoch: [31][115/817]	Loss 0.0081 (0.0439)	
training:	Epoch: [31][116/817]	Loss 0.0086 (0.0436)	
training:	Epoch: [31][117/817]	Loss 0.0074 (0.0433)	
training:	Epoch: [31][118/817]	Loss 0.0066 (0.0430)	
training:	Epoch: [31][119/817]	Loss 0.0060 (0.0427)	
training:	Epoch: [31][120/817]	Loss 0.0075 (0.0424)	
training:	Epoch: [31][121/817]	Loss 0.0115 (0.0421)	
training:	Epoch: [31][122/817]	Loss 0.0069 (0.0418)	
training:	Epoch: [31][123/817]	Loss 0.0068 (0.0415)	
training:	Epoch: [31][124/817]	Loss 0.0691 (0.0418)	
training:	Epoch: [31][125/817]	Loss 0.0070 (0.0415)	
training:	Epoch: [31][126/817]	Loss 0.0064 (0.0412)	
training:	Epoch: [31][127/817]	Loss 0.0056 (0.0409)	
training:	Epoch: [31][128/817]	Loss 0.0073 (0.0407)	
training:	Epoch: [31][129/817]	Loss 0.6027 (0.0450)	
training:	Epoch: [31][130/817]	Loss 0.0095 (0.0447)	
training:	Epoch: [31][131/817]	Loss 0.0060 (0.0445)	
training:	Epoch: [31][132/817]	Loss 0.0059 (0.0442)	
training:	Epoch: [31][133/817]	Loss 0.0061 (0.0439)	
training:	Epoch: [31][134/817]	Loss 0.0070 (0.0436)	
training:	Epoch: [31][135/817]	Loss 0.0085 (0.0433)	
training:	Epoch: [31][136/817]	Loss 0.0071 (0.0431)	
training:	Epoch: [31][137/817]	Loss 0.0069 (0.0428)	
training:	Epoch: [31][138/817]	Loss 0.0065 (0.0425)	
training:	Epoch: [31][139/817]	Loss 0.0073 (0.0423)	
training:	Epoch: [31][140/817]	Loss 0.0076 (0.0420)	
training:	Epoch: [31][141/817]	Loss 0.0066 (0.0418)	
training:	Epoch: [31][142/817]	Loss 0.0080 (0.0416)	
training:	Epoch: [31][143/817]	Loss 0.0085 (0.0413)	
training:	Epoch: [31][144/817]	Loss 0.6289 (0.0454)	
training:	Epoch: [31][145/817]	Loss 0.0070 (0.0451)	
training:	Epoch: [31][146/817]	Loss 0.0061 (0.0449)	
training:	Epoch: [31][147/817]	Loss 0.0069 (0.0446)	
training:	Epoch: [31][148/817]	Loss 0.0072 (0.0444)	
training:	Epoch: [31][149/817]	Loss 0.0086 (0.0441)	
training:	Epoch: [31][150/817]	Loss 0.0071 (0.0439)	
training:	Epoch: [31][151/817]	Loss 0.0075 (0.0436)	
training:	Epoch: [31][152/817]	Loss 0.0088 (0.0434)	
training:	Epoch: [31][153/817]	Loss 0.0082 (0.0432)	
training:	Epoch: [31][154/817]	Loss 0.0072 (0.0429)	
training:	Epoch: [31][155/817]	Loss 0.0064 (0.0427)	
training:	Epoch: [31][156/817]	Loss 0.6044 (0.0463)	
training:	Epoch: [31][157/817]	Loss 0.0375 (0.0463)	
training:	Epoch: [31][158/817]	Loss 0.0054 (0.0460)	
training:	Epoch: [31][159/817]	Loss 0.0068 (0.0457)	
training:	Epoch: [31][160/817]	Loss 0.0064 (0.0455)	
training:	Epoch: [31][161/817]	Loss 0.0083 (0.0453)	
training:	Epoch: [31][162/817]	Loss 0.0063 (0.0450)	
training:	Epoch: [31][163/817]	Loss 0.0062 (0.0448)	
training:	Epoch: [31][164/817]	Loss 0.0068 (0.0446)	
training:	Epoch: [31][165/817]	Loss 0.0060 (0.0443)	
training:	Epoch: [31][166/817]	Loss 0.0065 (0.0441)	
training:	Epoch: [31][167/817]	Loss 0.0069 (0.0439)	
training:	Epoch: [31][168/817]	Loss 0.0061 (0.0436)	
training:	Epoch: [31][169/817]	Loss 0.0069 (0.0434)	
training:	Epoch: [31][170/817]	Loss 0.0084 (0.0432)	
training:	Epoch: [31][171/817]	Loss 0.0069 (0.0430)	
training:	Epoch: [31][172/817]	Loss 0.1084 (0.0434)	
training:	Epoch: [31][173/817]	Loss 0.0056 (0.0432)	
training:	Epoch: [31][174/817]	Loss 0.0078 (0.0430)	
training:	Epoch: [31][175/817]	Loss 0.5781 (0.0460)	
training:	Epoch: [31][176/817]	Loss 0.0053 (0.0458)	
training:	Epoch: [31][177/817]	Loss 0.0078 (0.0456)	
training:	Epoch: [31][178/817]	Loss 0.6611 (0.0490)	
training:	Epoch: [31][179/817]	Loss 0.0107 (0.0488)	
training:	Epoch: [31][180/817]	Loss 0.0062 (0.0486)	
training:	Epoch: [31][181/817]	Loss 0.0069 (0.0484)	
training:	Epoch: [31][182/817]	Loss 0.0124 (0.0482)	
training:	Epoch: [31][183/817]	Loss 0.0121 (0.0480)	
training:	Epoch: [31][184/817]	Loss 0.0065 (0.0477)	
training:	Epoch: [31][185/817]	Loss 0.0068 (0.0475)	
training:	Epoch: [31][186/817]	Loss 0.0097 (0.0473)	
training:	Epoch: [31][187/817]	Loss 0.0061 (0.0471)	
training:	Epoch: [31][188/817]	Loss 0.2333 (0.0481)	
training:	Epoch: [31][189/817]	Loss 0.0063 (0.0479)	
training:	Epoch: [31][190/817]	Loss 0.0082 (0.0477)	
training:	Epoch: [31][191/817]	Loss 0.0077 (0.0474)	
training:	Epoch: [31][192/817]	Loss 0.0074 (0.0472)	
training:	Epoch: [31][193/817]	Loss 0.0063 (0.0470)	
training:	Epoch: [31][194/817]	Loss 0.0076 (0.0468)	
training:	Epoch: [31][195/817]	Loss 0.0080 (0.0466)	
training:	Epoch: [31][196/817]	Loss 0.0071 (0.0464)	
training:	Epoch: [31][197/817]	Loss 0.0064 (0.0462)	
training:	Epoch: [31][198/817]	Loss 0.0076 (0.0460)	
training:	Epoch: [31][199/817]	Loss 0.0072 (0.0458)	
training:	Epoch: [31][200/817]	Loss 0.0066 (0.0456)	
training:	Epoch: [31][201/817]	Loss 0.0077 (0.0454)	
training:	Epoch: [31][202/817]	Loss 0.0077 (0.0453)	
training:	Epoch: [31][203/817]	Loss 0.0080 (0.0451)	
training:	Epoch: [31][204/817]	Loss 0.0079 (0.0449)	
training:	Epoch: [31][205/817]	Loss 0.0097 (0.0447)	
training:	Epoch: [31][206/817]	Loss 0.5800 (0.0473)	
training:	Epoch: [31][207/817]	Loss 0.0084 (0.0471)	
training:	Epoch: [31][208/817]	Loss 0.0083 (0.0469)	
training:	Epoch: [31][209/817]	Loss 0.0068 (0.0468)	
training:	Epoch: [31][210/817]	Loss 0.0070 (0.0466)	
training:	Epoch: [31][211/817]	Loss 0.0081 (0.0464)	
training:	Epoch: [31][212/817]	Loss 0.0065 (0.0462)	
training:	Epoch: [31][213/817]	Loss 0.0127 (0.0460)	
training:	Epoch: [31][214/817]	Loss 0.0078 (0.0459)	
training:	Epoch: [31][215/817]	Loss 0.0604 (0.0459)	
training:	Epoch: [31][216/817]	Loss 0.1579 (0.0464)	
training:	Epoch: [31][217/817]	Loss 0.0079 (0.0463)	
training:	Epoch: [31][218/817]	Loss 0.0069 (0.0461)	
training:	Epoch: [31][219/817]	Loss 0.0064 (0.0459)	
training:	Epoch: [31][220/817]	Loss 0.0083 (0.0457)	
training:	Epoch: [31][221/817]	Loss 0.0066 (0.0456)	
training:	Epoch: [31][222/817]	Loss 0.0062 (0.0454)	
training:	Epoch: [31][223/817]	Loss 0.0084 (0.0452)	
training:	Epoch: [31][224/817]	Loss 0.0073 (0.0450)	
training:	Epoch: [31][225/817]	Loss 0.0073 (0.0449)	
training:	Epoch: [31][226/817]	Loss 0.0119 (0.0447)	
training:	Epoch: [31][227/817]	Loss 0.0073 (0.0446)	
training:	Epoch: [31][228/817]	Loss 0.0066 (0.0444)	
training:	Epoch: [31][229/817]	Loss 0.0075 (0.0442)	
training:	Epoch: [31][230/817]	Loss 0.0080 (0.0441)	
training:	Epoch: [31][231/817]	Loss 0.0068 (0.0439)	
training:	Epoch: [31][232/817]	Loss 0.0072 (0.0438)	
training:	Epoch: [31][233/817]	Loss 0.0065 (0.0436)	
training:	Epoch: [31][234/817]	Loss 0.0107 (0.0435)	
training:	Epoch: [31][235/817]	Loss 0.5922 (0.0458)	
training:	Epoch: [31][236/817]	Loss 0.0063 (0.0456)	
training:	Epoch: [31][237/817]	Loss 0.0069 (0.0455)	
training:	Epoch: [31][238/817]	Loss 0.0077 (0.0453)	
training:	Epoch: [31][239/817]	Loss 0.0075 (0.0451)	
training:	Epoch: [31][240/817]	Loss 0.0083 (0.0450)	
training:	Epoch: [31][241/817]	Loss 0.0066 (0.0448)	
training:	Epoch: [31][242/817]	Loss 0.0068 (0.0447)	
training:	Epoch: [31][243/817]	Loss 0.0070 (0.0445)	
training:	Epoch: [31][244/817]	Loss 0.0056 (0.0444)	
training:	Epoch: [31][245/817]	Loss 0.0078 (0.0442)	
training:	Epoch: [31][246/817]	Loss 0.0064 (0.0441)	
training:	Epoch: [31][247/817]	Loss 0.0066 (0.0439)	
training:	Epoch: [31][248/817]	Loss 0.0078 (0.0438)	
training:	Epoch: [31][249/817]	Loss 0.0073 (0.0436)	
training:	Epoch: [31][250/817]	Loss 0.0100 (0.0435)	
training:	Epoch: [31][251/817]	Loss 0.0070 (0.0433)	
training:	Epoch: [31][252/817]	Loss 0.0093 (0.0432)	
training:	Epoch: [31][253/817]	Loss 0.0067 (0.0431)	
training:	Epoch: [31][254/817]	Loss 0.0105 (0.0429)	
training:	Epoch: [31][255/817]	Loss 0.2568 (0.0438)	
training:	Epoch: [31][256/817]	Loss 0.0057 (0.0436)	
training:	Epoch: [31][257/817]	Loss 0.0068 (0.0435)	
training:	Epoch: [31][258/817]	Loss 0.0124 (0.0434)	
training:	Epoch: [31][259/817]	Loss 0.0069 (0.0432)	
training:	Epoch: [31][260/817]	Loss 0.0065 (0.0431)	
training:	Epoch: [31][261/817]	Loss 0.0093 (0.0429)	
training:	Epoch: [31][262/817]	Loss 0.0067 (0.0428)	
training:	Epoch: [31][263/817]	Loss 0.0071 (0.0427)	
training:	Epoch: [31][264/817]	Loss 0.0076 (0.0425)	
training:	Epoch: [31][265/817]	Loss 0.5739 (0.0445)	
training:	Epoch: [31][266/817]	Loss 0.0084 (0.0444)	
training:	Epoch: [31][267/817]	Loss 0.0065 (0.0443)	
training:	Epoch: [31][268/817]	Loss 0.0059 (0.0441)	
training:	Epoch: [31][269/817]	Loss 0.0061 (0.0440)	
training:	Epoch: [31][270/817]	Loss 0.0070 (0.0438)	
training:	Epoch: [31][271/817]	Loss 0.0068 (0.0437)	
training:	Epoch: [31][272/817]	Loss 0.0073 (0.0436)	
training:	Epoch: [31][273/817]	Loss 0.0082 (0.0434)	
training:	Epoch: [31][274/817]	Loss 0.0067 (0.0433)	
training:	Epoch: [31][275/817]	Loss 0.0077 (0.0432)	
training:	Epoch: [31][276/817]	Loss 0.0081 (0.0430)	
training:	Epoch: [31][277/817]	Loss 0.0071 (0.0429)	
training:	Epoch: [31][278/817]	Loss 0.0060 (0.0428)	
training:	Epoch: [31][279/817]	Loss 0.0071 (0.0427)	
training:	Epoch: [31][280/817]	Loss 0.6027 (0.0447)	
training:	Epoch: [31][281/817]	Loss 0.5463 (0.0464)	
training:	Epoch: [31][282/817]	Loss 0.0071 (0.0463)	
training:	Epoch: [31][283/817]	Loss 0.0074 (0.0462)	
training:	Epoch: [31][284/817]	Loss 0.0057 (0.0460)	
training:	Epoch: [31][285/817]	Loss 0.0077 (0.0459)	
training:	Epoch: [31][286/817]	Loss 0.0067 (0.0458)	
training:	Epoch: [31][287/817]	Loss 0.0072 (0.0456)	
training:	Epoch: [31][288/817]	Loss 0.0068 (0.0455)	
training:	Epoch: [31][289/817]	Loss 0.0083 (0.0454)	
training:	Epoch: [31][290/817]	Loss 0.0090 (0.0452)	
training:	Epoch: [31][291/817]	Loss 0.0079 (0.0451)	
training:	Epoch: [31][292/817]	Loss 0.0063 (0.0450)	
training:	Epoch: [31][293/817]	Loss 0.0072 (0.0448)	
training:	Epoch: [31][294/817]	Loss 0.0078 (0.0447)	
training:	Epoch: [31][295/817]	Loss 0.0074 (0.0446)	
training:	Epoch: [31][296/817]	Loss 0.0085 (0.0445)	
training:	Epoch: [31][297/817]	Loss 0.0299 (0.0444)	
training:	Epoch: [31][298/817]	Loss 0.0059 (0.0443)	
training:	Epoch: [31][299/817]	Loss 0.0058 (0.0442)	
training:	Epoch: [31][300/817]	Loss 0.0088 (0.0440)	
training:	Epoch: [31][301/817]	Loss 0.0071 (0.0439)	
training:	Epoch: [31][302/817]	Loss 0.0073 (0.0438)	
training:	Epoch: [31][303/817]	Loss 0.0081 (0.0437)	
training:	Epoch: [31][304/817]	Loss 0.0083 (0.0436)	
training:	Epoch: [31][305/817]	Loss 0.0063 (0.0434)	
training:	Epoch: [31][306/817]	Loss 0.1836 (0.0439)	
training:	Epoch: [31][307/817]	Loss 0.0079 (0.0438)	
training:	Epoch: [31][308/817]	Loss 0.0057 (0.0437)	
training:	Epoch: [31][309/817]	Loss 0.0071 (0.0435)	
training:	Epoch: [31][310/817]	Loss 0.0068 (0.0434)	
training:	Epoch: [31][311/817]	Loss 0.0075 (0.0433)	
training:	Epoch: [31][312/817]	Loss 0.0092 (0.0432)	
training:	Epoch: [31][313/817]	Loss 0.0076 (0.0431)	
training:	Epoch: [31][314/817]	Loss 0.0086 (0.0430)	
training:	Epoch: [31][315/817]	Loss 0.0077 (0.0429)	
training:	Epoch: [31][316/817]	Loss 0.0068 (0.0427)	
training:	Epoch: [31][317/817]	Loss 0.0075 (0.0426)	
training:	Epoch: [31][318/817]	Loss 0.0082 (0.0425)	
training:	Epoch: [31][319/817]	Loss 0.0069 (0.0424)	
training:	Epoch: [31][320/817]	Loss 0.0075 (0.0423)	
training:	Epoch: [31][321/817]	Loss 0.0063 (0.0422)	
training:	Epoch: [31][322/817]	Loss 0.0061 (0.0421)	
training:	Epoch: [31][323/817]	Loss 0.0073 (0.0420)	
training:	Epoch: [31][324/817]	Loss 0.4913 (0.0434)	
training:	Epoch: [31][325/817]	Loss 0.0073 (0.0432)	
training:	Epoch: [31][326/817]	Loss 0.0067 (0.0431)	
training:	Epoch: [31][327/817]	Loss 0.0061 (0.0430)	
training:	Epoch: [31][328/817]	Loss 0.0088 (0.0429)	
training:	Epoch: [31][329/817]	Loss 0.1307 (0.0432)	
training:	Epoch: [31][330/817]	Loss 0.0076 (0.0431)	
training:	Epoch: [31][331/817]	Loss 0.0073 (0.0430)	
training:	Epoch: [31][332/817]	Loss 0.0063 (0.0429)	
training:	Epoch: [31][333/817]	Loss 0.0069 (0.0428)	
training:	Epoch: [31][334/817]	Loss 0.0059 (0.0426)	
training:	Epoch: [31][335/817]	Loss 0.0061 (0.0425)	
training:	Epoch: [31][336/817]	Loss 0.6299 (0.0443)	
training:	Epoch: [31][337/817]	Loss 0.0070 (0.0442)	
training:	Epoch: [31][338/817]	Loss 0.0084 (0.0441)	
training:	Epoch: [31][339/817]	Loss 0.0078 (0.0440)	
training:	Epoch: [31][340/817]	Loss 0.0145 (0.0439)	
training:	Epoch: [31][341/817]	Loss 0.0071 (0.0438)	
training:	Epoch: [31][342/817]	Loss 0.0093 (0.0437)	
training:	Epoch: [31][343/817]	Loss 0.0596 (0.0437)	
training:	Epoch: [31][344/817]	Loss 0.0081 (0.0436)	
training:	Epoch: [31][345/817]	Loss 0.0086 (0.0435)	
training:	Epoch: [31][346/817]	Loss 0.0073 (0.0434)	
training:	Epoch: [31][347/817]	Loss 0.0073 (0.0433)	
training:	Epoch: [31][348/817]	Loss 0.0074 (0.0432)	
training:	Epoch: [31][349/817]	Loss 0.0097 (0.0431)	
training:	Epoch: [31][350/817]	Loss 0.0079 (0.0430)	
training:	Epoch: [31][351/817]	Loss 0.0080 (0.0429)	
training:	Epoch: [31][352/817]	Loss 0.0057 (0.0428)	
training:	Epoch: [31][353/817]	Loss 0.0060 (0.0427)	
training:	Epoch: [31][354/817]	Loss 0.0081 (0.0426)	
training:	Epoch: [31][355/817]	Loss 0.0068 (0.0425)	
training:	Epoch: [31][356/817]	Loss 0.0053 (0.0424)	
training:	Epoch: [31][357/817]	Loss 0.0076 (0.0423)	
training:	Epoch: [31][358/817]	Loss 0.0081 (0.0422)	
training:	Epoch: [31][359/817]	Loss 0.0116 (0.0421)	
training:	Epoch: [31][360/817]	Loss 0.0063 (0.0420)	
training:	Epoch: [31][361/817]	Loss 0.0297 (0.0420)	
training:	Epoch: [31][362/817]	Loss 0.0066 (0.0419)	
training:	Epoch: [31][363/817]	Loss 0.0066 (0.0418)	
training:	Epoch: [31][364/817]	Loss 0.0067 (0.0417)	
training:	Epoch: [31][365/817]	Loss 0.0061 (0.0416)	
training:	Epoch: [31][366/817]	Loss 0.0089 (0.0415)	
training:	Epoch: [31][367/817]	Loss 0.0109 (0.0414)	
training:	Epoch: [31][368/817]	Loss 0.0073 (0.0413)	
training:	Epoch: [31][369/817]	Loss 0.0070 (0.0412)	
training:	Epoch: [31][370/817]	Loss 0.0065 (0.0411)	
training:	Epoch: [31][371/817]	Loss 0.0050 (0.0410)	
training:	Epoch: [31][372/817]	Loss 0.0054 (0.0409)	
training:	Epoch: [31][373/817]	Loss 0.2817 (0.0416)	
training:	Epoch: [31][374/817]	Loss 0.0242 (0.0415)	
training:	Epoch: [31][375/817]	Loss 0.0079 (0.0414)	
training:	Epoch: [31][376/817]	Loss 0.0071 (0.0414)	
training:	Epoch: [31][377/817]	Loss 0.0096 (0.0413)	
training:	Epoch: [31][378/817]	Loss 0.0086 (0.0412)	
training:	Epoch: [31][379/817]	Loss 0.0078 (0.0411)	
training:	Epoch: [31][380/817]	Loss 0.0056 (0.0410)	
training:	Epoch: [31][381/817]	Loss 0.0070 (0.0409)	
training:	Epoch: [31][382/817]	Loss 0.0062 (0.0408)	
training:	Epoch: [31][383/817]	Loss 0.0067 (0.0407)	
training:	Epoch: [31][384/817]	Loss 0.0074 (0.0406)	
training:	Epoch: [31][385/817]	Loss 0.0503 (0.0407)	
training:	Epoch: [31][386/817]	Loss 0.0136 (0.0406)	
training:	Epoch: [31][387/817]	Loss 0.0052 (0.0405)	
training:	Epoch: [31][388/817]	Loss 0.0091 (0.0404)	
training:	Epoch: [31][389/817]	Loss 0.0071 (0.0403)	
training:	Epoch: [31][390/817]	Loss 0.0122 (0.0403)	
training:	Epoch: [31][391/817]	Loss 0.0067 (0.0402)	
training:	Epoch: [31][392/817]	Loss 0.0060 (0.0401)	
training:	Epoch: [31][393/817]	Loss 0.0096 (0.0400)	
training:	Epoch: [31][394/817]	Loss 0.0076 (0.0399)	
training:	Epoch: [31][395/817]	Loss 0.0088 (0.0399)	
training:	Epoch: [31][396/817]	Loss 0.0071 (0.0398)	
training:	Epoch: [31][397/817]	Loss 0.0063 (0.0397)	
training:	Epoch: [31][398/817]	Loss 0.0116 (0.0396)	
training:	Epoch: [31][399/817]	Loss 0.0058 (0.0395)	
training:	Epoch: [31][400/817]	Loss 0.1508 (0.0398)	
training:	Epoch: [31][401/817]	Loss 0.0079 (0.0397)	
training:	Epoch: [31][402/817]	Loss 0.0228 (0.0397)	
training:	Epoch: [31][403/817]	Loss 0.0064 (0.0396)	
training:	Epoch: [31][404/817]	Loss 0.0085 (0.0395)	
training:	Epoch: [31][405/817]	Loss 0.0076 (0.0395)	
training:	Epoch: [31][406/817]	Loss 0.4736 (0.0405)	
training:	Epoch: [31][407/817]	Loss 0.0091 (0.0404)	
training:	Epoch: [31][408/817]	Loss 0.0080 (0.0404)	
training:	Epoch: [31][409/817]	Loss 0.0070 (0.0403)	
training:	Epoch: [31][410/817]	Loss 0.0058 (0.0402)	
training:	Epoch: [31][411/817]	Loss 0.0080 (0.0401)	
training:	Epoch: [31][412/817]	Loss 0.0083 (0.0400)	
training:	Epoch: [31][413/817]	Loss 0.0072 (0.0400)	
training:	Epoch: [31][414/817]	Loss 0.0065 (0.0399)	
training:	Epoch: [31][415/817]	Loss 0.0078 (0.0398)	
training:	Epoch: [31][416/817]	Loss 0.0065 (0.0397)	
training:	Epoch: [31][417/817]	Loss 0.0067 (0.0396)	
training:	Epoch: [31][418/817]	Loss 0.0068 (0.0396)	
training:	Epoch: [31][419/817]	Loss 0.0062 (0.0395)	
training:	Epoch: [31][420/817]	Loss 0.0064 (0.0394)	
training:	Epoch: [31][421/817]	Loss 0.0057 (0.0393)	
training:	Epoch: [31][422/817]	Loss 0.0075 (0.0393)	
training:	Epoch: [31][423/817]	Loss 0.0071 (0.0392)	
training:	Epoch: [31][424/817]	Loss 0.0121 (0.0391)	
training:	Epoch: [31][425/817]	Loss 0.0069 (0.0390)	
training:	Epoch: [31][426/817]	Loss 0.0057 (0.0390)	
training:	Epoch: [31][427/817]	Loss 0.0062 (0.0389)	
training:	Epoch: [31][428/817]	Loss 0.0061 (0.0388)	
training:	Epoch: [31][429/817]	Loss 0.0072 (0.0387)	
training:	Epoch: [31][430/817]	Loss 0.0080 (0.0387)	
training:	Epoch: [31][431/817]	Loss 0.5765 (0.0399)	
training:	Epoch: [31][432/817]	Loss 0.0074 (0.0398)	
training:	Epoch: [31][433/817]	Loss 0.0067 (0.0398)	
training:	Epoch: [31][434/817]	Loss 0.0058 (0.0397)	
training:	Epoch: [31][435/817]	Loss 0.0066 (0.0396)	
training:	Epoch: [31][436/817]	Loss 0.0060 (0.0395)	
training:	Epoch: [31][437/817]	Loss 0.0060 (0.0395)	
training:	Epoch: [31][438/817]	Loss 0.0082 (0.0394)	
training:	Epoch: [31][439/817]	Loss 0.0077 (0.0393)	
training:	Epoch: [31][440/817]	Loss 0.0056 (0.0392)	
training:	Epoch: [31][441/817]	Loss 0.0094 (0.0392)	
training:	Epoch: [31][442/817]	Loss 0.0068 (0.0391)	
training:	Epoch: [31][443/817]	Loss 0.0068 (0.0390)	
training:	Epoch: [31][444/817]	Loss 0.0225 (0.0390)	
training:	Epoch: [31][445/817]	Loss 0.0078 (0.0389)	
training:	Epoch: [31][446/817]	Loss 0.0055 (0.0388)	
training:	Epoch: [31][447/817]	Loss 0.0078 (0.0388)	
training:	Epoch: [31][448/817]	Loss 0.0066 (0.0387)	
training:	Epoch: [31][449/817]	Loss 0.0060 (0.0386)	
training:	Epoch: [31][450/817]	Loss 0.0075 (0.0386)	
training:	Epoch: [31][451/817]	Loss 0.0063 (0.0385)	
training:	Epoch: [31][452/817]	Loss 0.0057 (0.0384)	
training:	Epoch: [31][453/817]	Loss 0.0067 (0.0383)	
training:	Epoch: [31][454/817]	Loss 0.0078 (0.0383)	
training:	Epoch: [31][455/817]	Loss 0.0067 (0.0382)	
training:	Epoch: [31][456/817]	Loss 0.0073 (0.0381)	
training:	Epoch: [31][457/817]	Loss 0.5728 (0.0393)	
training:	Epoch: [31][458/817]	Loss 0.0069 (0.0392)	
training:	Epoch: [31][459/817]	Loss 0.0070 (0.0392)	
training:	Epoch: [31][460/817]	Loss 0.0081 (0.0391)	
training:	Epoch: [31][461/817]	Loss 0.0065 (0.0390)	
training:	Epoch: [31][462/817]	Loss 0.0070 (0.0390)	
training:	Epoch: [31][463/817]	Loss 0.0078 (0.0389)	
training:	Epoch: [31][464/817]	Loss 0.0060 (0.0388)	
training:	Epoch: [31][465/817]	Loss 0.0067 (0.0387)	
training:	Epoch: [31][466/817]	Loss 0.0088 (0.0387)	
training:	Epoch: [31][467/817]	Loss 0.0059 (0.0386)	
training:	Epoch: [31][468/817]	Loss 0.0067 (0.0385)	
training:	Epoch: [31][469/817]	Loss 0.0082 (0.0385)	
training:	Epoch: [31][470/817]	Loss 0.0071 (0.0384)	
training:	Epoch: [31][471/817]	Loss 0.0068 (0.0383)	
training:	Epoch: [31][472/817]	Loss 0.0059 (0.0383)	
training:	Epoch: [31][473/817]	Loss 0.0059 (0.0382)	
training:	Epoch: [31][474/817]	Loss 0.0066 (0.0381)	
training:	Epoch: [31][475/817]	Loss 0.0078 (0.0381)	
training:	Epoch: [31][476/817]	Loss 0.0066 (0.0380)	
training:	Epoch: [31][477/817]	Loss 0.0059 (0.0379)	
training:	Epoch: [31][478/817]	Loss 0.0082 (0.0379)	
training:	Epoch: [31][479/817]	Loss 0.0084 (0.0378)	
training:	Epoch: [31][480/817]	Loss 0.0064 (0.0378)	
training:	Epoch: [31][481/817]	Loss 0.0066 (0.0377)	
training:	Epoch: [31][482/817]	Loss 0.0074 (0.0376)	
training:	Epoch: [31][483/817]	Loss 0.0088 (0.0376)	
training:	Epoch: [31][484/817]	Loss 0.0062 (0.0375)	
training:	Epoch: [31][485/817]	Loss 0.0074 (0.0374)	
training:	Epoch: [31][486/817]	Loss 0.0070 (0.0374)	
training:	Epoch: [31][487/817]	Loss 0.0058 (0.0373)	
training:	Epoch: [31][488/817]	Loss 0.0068 (0.0373)	
training:	Epoch: [31][489/817]	Loss 0.0059 (0.0372)	
training:	Epoch: [31][490/817]	Loss 0.0407 (0.0372)	
training:	Epoch: [31][491/817]	Loss 0.5997 (0.0383)	
training:	Epoch: [31][492/817]	Loss 0.0053 (0.0383)	
training:	Epoch: [31][493/817]	Loss 0.0069 (0.0382)	
training:	Epoch: [31][494/817]	Loss 0.0066 (0.0381)	
training:	Epoch: [31][495/817]	Loss 0.0063 (0.0381)	
training:	Epoch: [31][496/817]	Loss 0.0060 (0.0380)	
training:	Epoch: [31][497/817]	Loss 0.0128 (0.0380)	
training:	Epoch: [31][498/817]	Loss 0.0089 (0.0379)	
training:	Epoch: [31][499/817]	Loss 0.0067 (0.0378)	
training:	Epoch: [31][500/817]	Loss 0.0072 (0.0378)	
training:	Epoch: [31][501/817]	Loss 0.0069 (0.0377)	
training:	Epoch: [31][502/817]	Loss 0.0054 (0.0377)	
training:	Epoch: [31][503/817]	Loss 0.0077 (0.0376)	
training:	Epoch: [31][504/817]	Loss 0.0091 (0.0375)	
training:	Epoch: [31][505/817]	Loss 0.6022 (0.0387)	
training:	Epoch: [31][506/817]	Loss 0.0063 (0.0386)	
training:	Epoch: [31][507/817]	Loss 0.0065 (0.0385)	
training:	Epoch: [31][508/817]	Loss 0.0078 (0.0385)	
training:	Epoch: [31][509/817]	Loss 0.0067 (0.0384)	
training:	Epoch: [31][510/817]	Loss 0.0088 (0.0384)	
training:	Epoch: [31][511/817]	Loss 0.0060 (0.0383)	
training:	Epoch: [31][512/817]	Loss 0.0052 (0.0382)	
training:	Epoch: [31][513/817]	Loss 0.0086 (0.0382)	
training:	Epoch: [31][514/817]	Loss 0.0066 (0.0381)	
training:	Epoch: [31][515/817]	Loss 0.0073 (0.0380)	
training:	Epoch: [31][516/817]	Loss 0.0070 (0.0380)	
training:	Epoch: [31][517/817]	Loss 0.0063 (0.0379)	
training:	Epoch: [31][518/817]	Loss 0.0065 (0.0379)	
training:	Epoch: [31][519/817]	Loss 0.0069 (0.0378)	
training:	Epoch: [31][520/817]	Loss 0.0088 (0.0377)	
training:	Epoch: [31][521/817]	Loss 0.0069 (0.0377)	
training:	Epoch: [31][522/817]	Loss 0.0388 (0.0377)	
training:	Epoch: [31][523/817]	Loss 0.5836 (0.0387)	
training:	Epoch: [31][524/817]	Loss 0.0098 (0.0387)	
training:	Epoch: [31][525/817]	Loss 0.0071 (0.0386)	
training:	Epoch: [31][526/817]	Loss 0.0072 (0.0386)	
training:	Epoch: [31][527/817]	Loss 0.0067 (0.0385)	
training:	Epoch: [31][528/817]	Loss 0.4533 (0.0393)	
training:	Epoch: [31][529/817]	Loss 0.4842 (0.0401)	
training:	Epoch: [31][530/817]	Loss 0.0068 (0.0401)	
training:	Epoch: [31][531/817]	Loss 0.0068 (0.0400)	
training:	Epoch: [31][532/817]	Loss 0.0110 (0.0399)	
training:	Epoch: [31][533/817]	Loss 0.0098 (0.0399)	
training:	Epoch: [31][534/817]	Loss 0.0074 (0.0398)	
training:	Epoch: [31][535/817]	Loss 0.0071 (0.0398)	
training:	Epoch: [31][536/817]	Loss 0.0159 (0.0397)	
training:	Epoch: [31][537/817]	Loss 0.0060 (0.0397)	
training:	Epoch: [31][538/817]	Loss 0.0059 (0.0396)	
training:	Epoch: [31][539/817]	Loss 0.5256 (0.0405)	
training:	Epoch: [31][540/817]	Loss 0.0119 (0.0404)	
training:	Epoch: [31][541/817]	Loss 0.0090 (0.0404)	
training:	Epoch: [31][542/817]	Loss 0.0087 (0.0403)	
training:	Epoch: [31][543/817]	Loss 0.0170 (0.0403)	
training:	Epoch: [31][544/817]	Loss 0.0846 (0.0404)	
training:	Epoch: [31][545/817]	Loss 0.0077 (0.0403)	
training:	Epoch: [31][546/817]	Loss 0.0069 (0.0402)	
training:	Epoch: [31][547/817]	Loss 0.0104 (0.0402)	
training:	Epoch: [31][548/817]	Loss 0.0107 (0.0401)	
training:	Epoch: [31][549/817]	Loss 0.5738 (0.0411)	
training:	Epoch: [31][550/817]	Loss 0.0089 (0.0411)	
training:	Epoch: [31][551/817]	Loss 0.0105 (0.0410)	
training:	Epoch: [31][552/817]	Loss 0.0087 (0.0409)	
training:	Epoch: [31][553/817]	Loss 0.0071 (0.0409)	
training:	Epoch: [31][554/817]	Loss 0.0115 (0.0408)	
training:	Epoch: [31][555/817]	Loss 0.0111 (0.0408)	
training:	Epoch: [31][556/817]	Loss 0.0075 (0.0407)	
training:	Epoch: [31][557/817]	Loss 0.0085 (0.0407)	
training:	Epoch: [31][558/817]	Loss 0.0125 (0.0406)	
training:	Epoch: [31][559/817]	Loss 0.0158 (0.0406)	
training:	Epoch: [31][560/817]	Loss 0.0100 (0.0405)	
training:	Epoch: [31][561/817]	Loss 0.0086 (0.0404)	
training:	Epoch: [31][562/817]	Loss 0.0116 (0.0404)	
training:	Epoch: [31][563/817]	Loss 0.0084 (0.0403)	
training:	Epoch: [31][564/817]	Loss 0.0072 (0.0403)	
training:	Epoch: [31][565/817]	Loss 0.0104 (0.0402)	
training:	Epoch: [31][566/817]	Loss 0.1544 (0.0404)	
training:	Epoch: [31][567/817]	Loss 0.0070 (0.0404)	
training:	Epoch: [31][568/817]	Loss 0.0108 (0.0403)	
training:	Epoch: [31][569/817]	Loss 0.0092 (0.0403)	
training:	Epoch: [31][570/817]	Loss 0.0098 (0.0402)	
training:	Epoch: [31][571/817]	Loss 0.0103 (0.0402)	
training:	Epoch: [31][572/817]	Loss 0.0067 (0.0401)	
training:	Epoch: [31][573/817]	Loss 0.0082 (0.0400)	
training:	Epoch: [31][574/817]	Loss 0.0079 (0.0400)	
training:	Epoch: [31][575/817]	Loss 0.1328 (0.0401)	
training:	Epoch: [31][576/817]	Loss 0.0083 (0.0401)	
training:	Epoch: [31][577/817]	Loss 0.0216 (0.0401)	
training:	Epoch: [31][578/817]	Loss 0.0077 (0.0400)	
training:	Epoch: [31][579/817]	Loss 0.5508 (0.0409)	
training:	Epoch: [31][580/817]	Loss 0.0085 (0.0408)	
training:	Epoch: [31][581/817]	Loss 0.0126 (0.0408)	
training:	Epoch: [31][582/817]	Loss 0.0070 (0.0407)	
training:	Epoch: [31][583/817]	Loss 0.0078 (0.0407)	
training:	Epoch: [31][584/817]	Loss 0.0063 (0.0406)	
training:	Epoch: [31][585/817]	Loss 0.0055 (0.0405)	
training:	Epoch: [31][586/817]	Loss 0.0068 (0.0405)	
training:	Epoch: [31][587/817]	Loss 0.0074 (0.0404)	
training:	Epoch: [31][588/817]	Loss 0.0091 (0.0404)	
training:	Epoch: [31][589/817]	Loss 0.0090 (0.0403)	
training:	Epoch: [31][590/817]	Loss 0.0070 (0.0403)	
training:	Epoch: [31][591/817]	Loss 0.5472 (0.0411)	
training:	Epoch: [31][592/817]	Loss 0.0078 (0.0411)	
training:	Epoch: [31][593/817]	Loss 0.0137 (0.0410)	
training:	Epoch: [31][594/817]	Loss 0.0073 (0.0410)	
training:	Epoch: [31][595/817]	Loss 0.0079 (0.0409)	
training:	Epoch: [31][596/817]	Loss 0.0069 (0.0409)	
training:	Epoch: [31][597/817]	Loss 0.0077 (0.0408)	
training:	Epoch: [31][598/817]	Loss 0.0066 (0.0407)	
training:	Epoch: [31][599/817]	Loss 0.0104 (0.0407)	
training:	Epoch: [31][600/817]	Loss 0.0068 (0.0406)	
training:	Epoch: [31][601/817]	Loss 0.0088 (0.0406)	
training:	Epoch: [31][602/817]	Loss 0.0066 (0.0405)	
training:	Epoch: [31][603/817]	Loss 0.0073 (0.0405)	
training:	Epoch: [31][604/817]	Loss 0.0107 (0.0404)	
training:	Epoch: [31][605/817]	Loss 0.0073 (0.0404)	
training:	Epoch: [31][606/817]	Loss 0.0124 (0.0403)	
training:	Epoch: [31][607/817]	Loss 0.0088 (0.0403)	
training:	Epoch: [31][608/817]	Loss 0.0062 (0.0402)	
training:	Epoch: [31][609/817]	Loss 0.0103 (0.0402)	
training:	Epoch: [31][610/817]	Loss 0.0062 (0.0401)	
training:	Epoch: [31][611/817]	Loss 0.0078 (0.0401)	
training:	Epoch: [31][612/817]	Loss 0.0075 (0.0400)	
training:	Epoch: [31][613/817]	Loss 0.0063 (0.0399)	
training:	Epoch: [31][614/817]	Loss 0.0097 (0.0399)	
training:	Epoch: [31][615/817]	Loss 0.0067 (0.0398)	
training:	Epoch: [31][616/817]	Loss 0.0087 (0.0398)	
training:	Epoch: [31][617/817]	Loss 0.0063 (0.0397)	
training:	Epoch: [31][618/817]	Loss 0.0091 (0.0397)	
training:	Epoch: [31][619/817]	Loss 0.5978 (0.0406)	
training:	Epoch: [31][620/817]	Loss 0.0074 (0.0405)	
training:	Epoch: [31][621/817]	Loss 0.0069 (0.0405)	
training:	Epoch: [31][622/817]	Loss 0.0069 (0.0404)	
training:	Epoch: [31][623/817]	Loss 0.0068 (0.0404)	
training:	Epoch: [31][624/817]	Loss 0.0143 (0.0403)	
training:	Epoch: [31][625/817]	Loss 0.0100 (0.0403)	
training:	Epoch: [31][626/817]	Loss 0.0802 (0.0404)	
training:	Epoch: [31][627/817]	Loss 0.0102 (0.0403)	
training:	Epoch: [31][628/817]	Loss 0.0058 (0.0402)	
training:	Epoch: [31][629/817]	Loss 0.0085 (0.0402)	
training:	Epoch: [31][630/817]	Loss 0.0094 (0.0401)	
training:	Epoch: [31][631/817]	Loss 0.0067 (0.0401)	
training:	Epoch: [31][632/817]	Loss 0.0088 (0.0400)	
training:	Epoch: [31][633/817]	Loss 0.0087 (0.0400)	
training:	Epoch: [31][634/817]	Loss 0.0067 (0.0399)	
training:	Epoch: [31][635/817]	Loss 0.0100 (0.0399)	
training:	Epoch: [31][636/817]	Loss 0.0076 (0.0398)	
training:	Epoch: [31][637/817]	Loss 0.0085 (0.0398)	
training:	Epoch: [31][638/817]	Loss 0.1203 (0.0399)	
training:	Epoch: [31][639/817]	Loss 0.0074 (0.0399)	
training:	Epoch: [31][640/817]	Loss 0.0064 (0.0398)	
training:	Epoch: [31][641/817]	Loss 0.0083 (0.0398)	
training:	Epoch: [31][642/817]	Loss 0.0105 (0.0397)	
training:	Epoch: [31][643/817]	Loss 0.0072 (0.0397)	
training:	Epoch: [31][644/817]	Loss 0.0098 (0.0396)	
training:	Epoch: [31][645/817]	Loss 0.0061 (0.0396)	
training:	Epoch: [31][646/817]	Loss 0.0080 (0.0395)	
training:	Epoch: [31][647/817]	Loss 0.0055 (0.0395)	
training:	Epoch: [31][648/817]	Loss 0.0073 (0.0394)	
training:	Epoch: [31][649/817]	Loss 0.0090 (0.0394)	
training:	Epoch: [31][650/817]	Loss 0.0091 (0.0393)	
training:	Epoch: [31][651/817]	Loss 0.0060 (0.0393)	
training:	Epoch: [31][652/817]	Loss 0.0066 (0.0392)	
training:	Epoch: [31][653/817]	Loss 0.0124 (0.0392)	
training:	Epoch: [31][654/817]	Loss 0.0141 (0.0391)	
training:	Epoch: [31][655/817]	Loss 0.0073 (0.0391)	
training:	Epoch: [31][656/817]	Loss 0.0107 (0.0391)	
training:	Epoch: [31][657/817]	Loss 0.0075 (0.0390)	
training:	Epoch: [31][658/817]	Loss 0.0080 (0.0390)	
training:	Epoch: [31][659/817]	Loss 0.0070 (0.0389)	
training:	Epoch: [31][660/817]	Loss 0.0065 (0.0389)	
training:	Epoch: [31][661/817]	Loss 0.0073 (0.0388)	
training:	Epoch: [31][662/817]	Loss 0.0054 (0.0388)	
training:	Epoch: [31][663/817]	Loss 0.0063 (0.0387)	
training:	Epoch: [31][664/817]	Loss 0.0059 (0.0387)	
training:	Epoch: [31][665/817]	Loss 0.0068 (0.0386)	
training:	Epoch: [31][666/817]	Loss 0.0068 (0.0386)	
training:	Epoch: [31][667/817]	Loss 0.0068 (0.0385)	
training:	Epoch: [31][668/817]	Loss 0.0061 (0.0385)	
training:	Epoch: [31][669/817]	Loss 0.0079 (0.0384)	
training:	Epoch: [31][670/817]	Loss 0.0061 (0.0384)	
training:	Epoch: [31][671/817]	Loss 0.0078 (0.0383)	
training:	Epoch: [31][672/817]	Loss 0.0077 (0.0383)	
training:	Epoch: [31][673/817]	Loss 0.0074 (0.0382)	
training:	Epoch: [31][674/817]	Loss 0.0082 (0.0382)	
training:	Epoch: [31][675/817]	Loss 0.0071 (0.0382)	
training:	Epoch: [31][676/817]	Loss 0.0066 (0.0381)	
training:	Epoch: [31][677/817]	Loss 0.5359 (0.0388)	
training:	Epoch: [31][678/817]	Loss 0.0063 (0.0388)	
training:	Epoch: [31][679/817]	Loss 0.0060 (0.0387)	
training:	Epoch: [31][680/817]	Loss 0.0066 (0.0387)	
training:	Epoch: [31][681/817]	Loss 0.0068 (0.0387)	
training:	Epoch: [31][682/817]	Loss 0.0077 (0.0386)	
training:	Epoch: [31][683/817]	Loss 0.0065 (0.0386)	
training:	Epoch: [31][684/817]	Loss 0.0068 (0.0385)	
training:	Epoch: [31][685/817]	Loss 0.0075 (0.0385)	
training:	Epoch: [31][686/817]	Loss 0.0076 (0.0384)	
training:	Epoch: [31][687/817]	Loss 0.0058 (0.0384)	
training:	Epoch: [31][688/817]	Loss 0.0066 (0.0383)	
training:	Epoch: [31][689/817]	Loss 0.0061 (0.0383)	
training:	Epoch: [31][690/817]	Loss 0.0078 (0.0382)	
training:	Epoch: [31][691/817]	Loss 0.0071 (0.0382)	
training:	Epoch: [31][692/817]	Loss 0.0107 (0.0382)	
training:	Epoch: [31][693/817]	Loss 0.0066 (0.0381)	
training:	Epoch: [31][694/817]	Loss 0.5907 (0.0389)	
training:	Epoch: [31][695/817]	Loss 0.0061 (0.0389)	
training:	Epoch: [31][696/817]	Loss 0.6222 (0.0397)	
training:	Epoch: [31][697/817]	Loss 0.0096 (0.0397)	
training:	Epoch: [31][698/817]	Loss 0.0054 (0.0396)	
training:	Epoch: [31][699/817]	Loss 0.6187 (0.0404)	
training:	Epoch: [31][700/817]	Loss 0.0088 (0.0404)	
training:	Epoch: [31][701/817]	Loss 0.0059 (0.0403)	
training:	Epoch: [31][702/817]	Loss 0.0073 (0.0403)	
training:	Epoch: [31][703/817]	Loss 0.0066 (0.0402)	
training:	Epoch: [31][704/817]	Loss 0.0073 (0.0402)	
training:	Epoch: [31][705/817]	Loss 0.0077 (0.0402)	
training:	Epoch: [31][706/817]	Loss 0.5804 (0.0409)	
training:	Epoch: [31][707/817]	Loss 0.0053 (0.0409)	
training:	Epoch: [31][708/817]	Loss 0.0058 (0.0408)	
training:	Epoch: [31][709/817]	Loss 0.0086 (0.0408)	
training:	Epoch: [31][710/817]	Loss 0.0067 (0.0407)	
training:	Epoch: [31][711/817]	Loss 0.0065 (0.0407)	
training:	Epoch: [31][712/817]	Loss 0.0082 (0.0406)	
training:	Epoch: [31][713/817]	Loss 0.0070 (0.0406)	
training:	Epoch: [31][714/817]	Loss 0.0063 (0.0405)	
training:	Epoch: [31][715/817]	Loss 0.0085 (0.0405)	
training:	Epoch: [31][716/817]	Loss 0.0069 (0.0404)	
training:	Epoch: [31][717/817]	Loss 0.0072 (0.0404)	
training:	Epoch: [31][718/817]	Loss 0.0090 (0.0404)	
training:	Epoch: [31][719/817]	Loss 0.0079 (0.0403)	
training:	Epoch: [31][720/817]	Loss 0.0084 (0.0403)	
training:	Epoch: [31][721/817]	Loss 0.0088 (0.0402)	
training:	Epoch: [31][722/817]	Loss 0.0057 (0.0402)	
training:	Epoch: [31][723/817]	Loss 0.0094 (0.0401)	
training:	Epoch: [31][724/817]	Loss 0.1236 (0.0402)	
training:	Epoch: [31][725/817]	Loss 0.0095 (0.0402)	
training:	Epoch: [31][726/817]	Loss 0.0088 (0.0402)	
training:	Epoch: [31][727/817]	Loss 0.0121 (0.0401)	
training:	Epoch: [31][728/817]	Loss 0.0070 (0.0401)	
training:	Epoch: [31][729/817]	Loss 0.0069 (0.0400)	
training:	Epoch: [31][730/817]	Loss 0.0077 (0.0400)	
training:	Epoch: [31][731/817]	Loss 0.0090 (0.0399)	
training:	Epoch: [31][732/817]	Loss 0.0065 (0.0399)	
training:	Epoch: [31][733/817]	Loss 0.0073 (0.0399)	
training:	Epoch: [31][734/817]	Loss 0.0070 (0.0398)	
training:	Epoch: [31][735/817]	Loss 0.0076 (0.0398)	
training:	Epoch: [31][736/817]	Loss 0.0076 (0.0397)	
training:	Epoch: [31][737/817]	Loss 0.0071 (0.0397)	
training:	Epoch: [31][738/817]	Loss 0.5819 (0.0404)	
training:	Epoch: [31][739/817]	Loss 0.0059 (0.0404)	
training:	Epoch: [31][740/817]	Loss 0.0074 (0.0403)	
training:	Epoch: [31][741/817]	Loss 0.0079 (0.0403)	
training:	Epoch: [31][742/817]	Loss 0.0061 (0.0402)	
training:	Epoch: [31][743/817]	Loss 0.0079 (0.0402)	
training:	Epoch: [31][744/817]	Loss 0.0074 (0.0401)	
training:	Epoch: [31][745/817]	Loss 0.0058 (0.0401)	
training:	Epoch: [31][746/817]	Loss 0.0104 (0.0401)	
training:	Epoch: [31][747/817]	Loss 0.0079 (0.0400)	
training:	Epoch: [31][748/817]	Loss 0.0078 (0.0400)	
training:	Epoch: [31][749/817]	Loss 0.0070 (0.0399)	
training:	Epoch: [31][750/817]	Loss 0.0066 (0.0399)	
training:	Epoch: [31][751/817]	Loss 0.0079 (0.0398)	
training:	Epoch: [31][752/817]	Loss 0.0073 (0.0398)	
training:	Epoch: [31][753/817]	Loss 0.0069 (0.0398)	
training:	Epoch: [31][754/817]	Loss 0.0315 (0.0397)	
training:	Epoch: [31][755/817]	Loss 0.0084 (0.0397)	
training:	Epoch: [31][756/817]	Loss 0.0067 (0.0397)	
training:	Epoch: [31][757/817]	Loss 0.0090 (0.0396)	
training:	Epoch: [31][758/817]	Loss 0.0077 (0.0396)	
training:	Epoch: [31][759/817]	Loss 0.0066 (0.0395)	
training:	Epoch: [31][760/817]	Loss 0.0071 (0.0395)	
training:	Epoch: [31][761/817]	Loss 0.0079 (0.0394)	
training:	Epoch: [31][762/817]	Loss 0.0071 (0.0394)	
training:	Epoch: [31][763/817]	Loss 0.0076 (0.0394)	
training:	Epoch: [31][764/817]	Loss 0.0084 (0.0393)	
training:	Epoch: [31][765/817]	Loss 0.0077 (0.0393)	
training:	Epoch: [31][766/817]	Loss 0.0217 (0.0393)	
training:	Epoch: [31][767/817]	Loss 0.0065 (0.0392)	
training:	Epoch: [31][768/817]	Loss 0.0078 (0.0392)	
training:	Epoch: [31][769/817]	Loss 0.0075 (0.0391)	
training:	Epoch: [31][770/817]	Loss 0.0080 (0.0391)	
training:	Epoch: [31][771/817]	Loss 0.0061 (0.0390)	
training:	Epoch: [31][772/817]	Loss 0.0062 (0.0390)	
training:	Epoch: [31][773/817]	Loss 0.0091 (0.0390)	
training:	Epoch: [31][774/817]	Loss 0.0080 (0.0389)	
training:	Epoch: [31][775/817]	Loss 0.0093 (0.0389)	
training:	Epoch: [31][776/817]	Loss 0.0065 (0.0388)	
training:	Epoch: [31][777/817]	Loss 0.0110 (0.0388)	
training:	Epoch: [31][778/817]	Loss 0.0094 (0.0388)	
training:	Epoch: [31][779/817]	Loss 0.0089 (0.0387)	
training:	Epoch: [31][780/817]	Loss 0.0074 (0.0387)	
training:	Epoch: [31][781/817]	Loss 0.0084 (0.0387)	
training:	Epoch: [31][782/817]	Loss 0.0249 (0.0386)	
training:	Epoch: [31][783/817]	Loss 0.0072 (0.0386)	
training:	Epoch: [31][784/817]	Loss 0.0061 (0.0386)	
training:	Epoch: [31][785/817]	Loss 0.0072 (0.0385)	
training:	Epoch: [31][786/817]	Loss 0.0079 (0.0385)	
training:	Epoch: [31][787/817]	Loss 0.2408 (0.0387)	
training:	Epoch: [31][788/817]	Loss 0.0064 (0.0387)	
training:	Epoch: [31][789/817]	Loss 0.0067 (0.0387)	
training:	Epoch: [31][790/817]	Loss 0.0098 (0.0386)	
training:	Epoch: [31][791/817]	Loss 0.0068 (0.0386)	
training:	Epoch: [31][792/817]	Loss 0.0084 (0.0385)	
training:	Epoch: [31][793/817]	Loss 0.0059 (0.0385)	
training:	Epoch: [31][794/817]	Loss 0.0078 (0.0385)	
training:	Epoch: [31][795/817]	Loss 0.0058 (0.0384)	
training:	Epoch: [31][796/817]	Loss 0.0101 (0.0384)	
training:	Epoch: [31][797/817]	Loss 0.0057 (0.0383)	
training:	Epoch: [31][798/817]	Loss 0.0073 (0.0383)	
training:	Epoch: [31][799/817]	Loss 0.0061 (0.0383)	
training:	Epoch: [31][800/817]	Loss 0.0068 (0.0382)	
training:	Epoch: [31][801/817]	Loss 0.0064 (0.0382)	
training:	Epoch: [31][802/817]	Loss 0.5706 (0.0388)	
training:	Epoch: [31][803/817]	Loss 0.0089 (0.0388)	
training:	Epoch: [31][804/817]	Loss 0.1046 (0.0389)	
training:	Epoch: [31][805/817]	Loss 0.0082 (0.0389)	
training:	Epoch: [31][806/817]	Loss 0.0060 (0.0388)	
training:	Epoch: [31][807/817]	Loss 0.0085 (0.0388)	
training:	Epoch: [31][808/817]	Loss 0.0090 (0.0387)	
training:	Epoch: [31][809/817]	Loss 0.0082 (0.0387)	
training:	Epoch: [31][810/817]	Loss 0.0081 (0.0387)	
training:	Epoch: [31][811/817]	Loss 0.6821 (0.0395)	
training:	Epoch: [31][812/817]	Loss 0.0075 (0.0394)	
training:	Epoch: [31][813/817]	Loss 0.0132 (0.0394)	
training:	Epoch: [31][814/817]	Loss 0.0087 (0.0393)	
training:	Epoch: [31][815/817]	Loss 0.0053 (0.0393)	
training:	Epoch: [31][816/817]	Loss 0.0052 (0.0393)	
training:	Epoch: [31][817/817]	Loss 0.0072 (0.0392)	
Training:	 Loss: 0.0392

Training:	 ACC: 0.9934 0.9934 0.9932 0.9936
Validation:	 ACC: 0.7879 0.7871 0.7687 0.8072
Validation:	 Best_BACC: 0.8223 0.8218 0.8127 0.8318
Validation:	 Loss: 0.9352
Pretraining:	Epoch 32/200
----------
training:	Epoch: [32][1/817]	Loss 0.0084 (0.0084)	
training:	Epoch: [32][2/817]	Loss 0.0051 (0.0067)	
training:	Epoch: [32][3/817]	Loss 0.0095 (0.0077)	
training:	Epoch: [32][4/817]	Loss 0.0069 (0.0075)	
training:	Epoch: [32][5/817]	Loss 0.0076 (0.0075)	
training:	Epoch: [32][6/817]	Loss 0.0076 (0.0075)	
training:	Epoch: [32][7/817]	Loss 0.0090 (0.0077)	
training:	Epoch: [32][8/817]	Loss 0.0081 (0.0078)	
training:	Epoch: [32][9/817]	Loss 0.0079 (0.0078)	
training:	Epoch: [32][10/817]	Loss 0.0379 (0.0108)	
training:	Epoch: [32][11/817]	Loss 0.0083 (0.0106)	
training:	Epoch: [32][12/817]	Loss 0.0059 (0.0102)	
training:	Epoch: [32][13/817]	Loss 0.0092 (0.0101)	
training:	Epoch: [32][14/817]	Loss 0.0070 (0.0099)	
training:	Epoch: [32][15/817]	Loss 0.0095 (0.0099)	
training:	Epoch: [32][16/817]	Loss 0.0073 (0.0097)	
training:	Epoch: [32][17/817]	Loss 0.0061 (0.0095)	
training:	Epoch: [32][18/817]	Loss 0.0072 (0.0094)	
training:	Epoch: [32][19/817]	Loss 0.0060 (0.0092)	
training:	Epoch: [32][20/817]	Loss 0.0084 (0.0092)	
training:	Epoch: [32][21/817]	Loss 0.0068 (0.0090)	
training:	Epoch: [32][22/817]	Loss 0.0069 (0.0089)	
training:	Epoch: [32][23/817]	Loss 0.0098 (0.0090)	
training:	Epoch: [32][24/817]	Loss 0.0073 (0.0089)	
training:	Epoch: [32][25/817]	Loss 0.0078 (0.0089)	
training:	Epoch: [32][26/817]	Loss 0.0049 (0.0087)	
training:	Epoch: [32][27/817]	Loss 0.0087 (0.0087)	
training:	Epoch: [32][28/817]	Loss 0.0086 (0.0087)	
training:	Epoch: [32][29/817]	Loss 0.0076 (0.0087)	
training:	Epoch: [32][30/817]	Loss 0.0075 (0.0086)	
training:	Epoch: [32][31/817]	Loss 0.0064 (0.0086)	
training:	Epoch: [32][32/817]	Loss 0.0051 (0.0085)	
training:	Epoch: [32][33/817]	Loss 0.0087 (0.0085)	
training:	Epoch: [32][34/817]	Loss 0.0084 (0.0085)	
training:	Epoch: [32][35/817]	Loss 0.0061 (0.0084)	
training:	Epoch: [32][36/817]	Loss 0.0081 (0.0084)	
training:	Epoch: [32][37/817]	Loss 0.0061 (0.0083)	
training:	Epoch: [32][38/817]	Loss 0.0069 (0.0083)	
training:	Epoch: [32][39/817]	Loss 0.0072 (0.0083)	
training:	Epoch: [32][40/817]	Loss 0.0051 (0.0082)	
training:	Epoch: [32][41/817]	Loss 0.0081 (0.0082)	
training:	Epoch: [32][42/817]	Loss 0.0064 (0.0081)	
training:	Epoch: [32][43/817]	Loss 0.0060 (0.0081)	
training:	Epoch: [32][44/817]	Loss 0.0065 (0.0080)	
training:	Epoch: [32][45/817]	Loss 0.0062 (0.0080)	
training:	Epoch: [32][46/817]	Loss 0.0076 (0.0080)	
training:	Epoch: [32][47/817]	Loss 0.0090 (0.0080)	
training:	Epoch: [32][48/817]	Loss 0.0075 (0.0080)	
training:	Epoch: [32][49/817]	Loss 0.0068 (0.0080)	
training:	Epoch: [32][50/817]	Loss 0.0082 (0.0080)	
training:	Epoch: [32][51/817]	Loss 0.0059 (0.0079)	
training:	Epoch: [32][52/817]	Loss 0.0079 (0.0079)	
training:	Epoch: [32][53/817]	Loss 0.0074 (0.0079)	
training:	Epoch: [32][54/817]	Loss 0.0062 (0.0079)	
training:	Epoch: [32][55/817]	Loss 0.0067 (0.0079)	
training:	Epoch: [32][56/817]	Loss 0.0071 (0.0079)	
training:	Epoch: [32][57/817]	Loss 0.0067 (0.0078)	
training:	Epoch: [32][58/817]	Loss 0.0049 (0.0078)	
training:	Epoch: [32][59/817]	Loss 0.0069 (0.0078)	
training:	Epoch: [32][60/817]	Loss 0.0445 (0.0084)	
training:	Epoch: [32][61/817]	Loss 0.0052 (0.0083)	
training:	Epoch: [32][62/817]	Loss 0.0060 (0.0083)	
training:	Epoch: [32][63/817]	Loss 0.0066 (0.0083)	
training:	Epoch: [32][64/817]	Loss 0.0061 (0.0082)	
training:	Epoch: [32][65/817]	Loss 0.0063 (0.0082)	
training:	Epoch: [32][66/817]	Loss 0.0063 (0.0082)	
training:	Epoch: [32][67/817]	Loss 0.0061 (0.0082)	
training:	Epoch: [32][68/817]	Loss 0.0067 (0.0081)	
training:	Epoch: [32][69/817]	Loss 0.0050 (0.0081)	
training:	Epoch: [32][70/817]	Loss 0.0063 (0.0081)	
training:	Epoch: [32][71/817]	Loss 0.0073 (0.0080)	
training:	Epoch: [32][72/817]	Loss 0.0065 (0.0080)	
training:	Epoch: [32][73/817]	Loss 0.0072 (0.0080)	
training:	Epoch: [32][74/817]	Loss 0.0051 (0.0080)	
training:	Epoch: [32][75/817]	Loss 0.0054 (0.0079)	
training:	Epoch: [32][76/817]	Loss 0.5706 (0.0153)	
training:	Epoch: [32][77/817]	Loss 0.0059 (0.0152)	
training:	Epoch: [32][78/817]	Loss 0.0085 (0.0151)	
training:	Epoch: [32][79/817]	Loss 0.0062 (0.0150)	
training:	Epoch: [32][80/817]	Loss 0.4628 (0.0206)	
training:	Epoch: [32][81/817]	Loss 0.0071 (0.0205)	
training:	Epoch: [32][82/817]	Loss 0.2726 (0.0235)	
training:	Epoch: [32][83/817]	Loss 0.0060 (0.0233)	
training:	Epoch: [32][84/817]	Loss 0.0073 (0.0231)	
training:	Epoch: [32][85/817]	Loss 0.5647 (0.0295)	
training:	Epoch: [32][86/817]	Loss 0.0127 (0.0293)	
training:	Epoch: [32][87/817]	Loss 0.0073 (0.0290)	
training:	Epoch: [32][88/817]	Loss 0.0073 (0.0288)	
training:	Epoch: [32][89/817]	Loss 0.0090 (0.0286)	
training:	Epoch: [32][90/817]	Loss 0.0075 (0.0283)	
training:	Epoch: [32][91/817]	Loss 0.0092 (0.0281)	
training:	Epoch: [32][92/817]	Loss 0.0069 (0.0279)	
training:	Epoch: [32][93/817]	Loss 0.0098 (0.0277)	
training:	Epoch: [32][94/817]	Loss 0.0065 (0.0275)	
training:	Epoch: [32][95/817]	Loss 0.0050 (0.0272)	
training:	Epoch: [32][96/817]	Loss 0.0060 (0.0270)	
training:	Epoch: [32][97/817]	Loss 0.0082 (0.0268)	
training:	Epoch: [32][98/817]	Loss 0.0058 (0.0266)	
training:	Epoch: [32][99/817]	Loss 0.0066 (0.0264)	
training:	Epoch: [32][100/817]	Loss 0.0192 (0.0263)	
training:	Epoch: [32][101/817]	Loss 0.0070 (0.0262)	
training:	Epoch: [32][102/817]	Loss 0.0051 (0.0259)	
training:	Epoch: [32][103/817]	Loss 0.0067 (0.0258)	
training:	Epoch: [32][104/817]	Loss 0.0086 (0.0256)	
training:	Epoch: [32][105/817]	Loss 0.0097 (0.0254)	
training:	Epoch: [32][106/817]	Loss 0.0065 (0.0253)	
training:	Epoch: [32][107/817]	Loss 0.0072 (0.0251)	
training:	Epoch: [32][108/817]	Loss 0.0070 (0.0249)	
training:	Epoch: [32][109/817]	Loss 0.0062 (0.0248)	
training:	Epoch: [32][110/817]	Loss 0.0069 (0.0246)	
training:	Epoch: [32][111/817]	Loss 0.0052 (0.0244)	
training:	Epoch: [32][112/817]	Loss 0.0066 (0.0243)	
training:	Epoch: [32][113/817]	Loss 0.0067 (0.0241)	
training:	Epoch: [32][114/817]	Loss 0.0066 (0.0240)	
training:	Epoch: [32][115/817]	Loss 0.0074 (0.0238)	
training:	Epoch: [32][116/817]	Loss 0.0069 (0.0237)	
training:	Epoch: [32][117/817]	Loss 0.0066 (0.0235)	
training:	Epoch: [32][118/817]	Loss 0.0068 (0.0234)	
training:	Epoch: [32][119/817]	Loss 0.0074 (0.0232)	
training:	Epoch: [32][120/817]	Loss 0.6396 (0.0284)	
training:	Epoch: [32][121/817]	Loss 0.0065 (0.0282)	
training:	Epoch: [32][122/817]	Loss 0.0120 (0.0281)	
training:	Epoch: [32][123/817]	Loss 0.0094 (0.0279)	
training:	Epoch: [32][124/817]	Loss 0.0066 (0.0277)	
training:	Epoch: [32][125/817]	Loss 0.0074 (0.0276)	
training:	Epoch: [32][126/817]	Loss 0.0061 (0.0274)	
training:	Epoch: [32][127/817]	Loss 0.0052 (0.0272)	
training:	Epoch: [32][128/817]	Loss 0.0066 (0.0271)	
training:	Epoch: [32][129/817]	Loss 0.0086 (0.0269)	
training:	Epoch: [32][130/817]	Loss 0.0093 (0.0268)	
training:	Epoch: [32][131/817]	Loss 0.0072 (0.0266)	
training:	Epoch: [32][132/817]	Loss 0.0078 (0.0265)	
training:	Epoch: [32][133/817]	Loss 0.6177 (0.0309)	
training:	Epoch: [32][134/817]	Loss 0.0074 (0.0308)	
training:	Epoch: [32][135/817]	Loss 0.0058 (0.0306)	
training:	Epoch: [32][136/817]	Loss 0.0079 (0.0304)	
training:	Epoch: [32][137/817]	Loss 0.0081 (0.0303)	
training:	Epoch: [32][138/817]	Loss 0.0073 (0.0301)	
training:	Epoch: [32][139/817]	Loss 0.0093 (0.0299)	
training:	Epoch: [32][140/817]	Loss 0.0062 (0.0298)	
training:	Epoch: [32][141/817]	Loss 0.0054 (0.0296)	
training:	Epoch: [32][142/817]	Loss 0.0060 (0.0294)	
training:	Epoch: [32][143/817]	Loss 0.0080 (0.0293)	
training:	Epoch: [32][144/817]	Loss 0.0081 (0.0291)	
training:	Epoch: [32][145/817]	Loss 0.0134 (0.0290)	
training:	Epoch: [32][146/817]	Loss 0.0088 (0.0289)	
training:	Epoch: [32][147/817]	Loss 0.0079 (0.0287)	
training:	Epoch: [32][148/817]	Loss 0.0076 (0.0286)	
training:	Epoch: [32][149/817]	Loss 0.0110 (0.0285)	
training:	Epoch: [32][150/817]	Loss 0.0074 (0.0283)	
training:	Epoch: [32][151/817]	Loss 0.0064 (0.0282)	
training:	Epoch: [32][152/817]	Loss 0.0086 (0.0281)	
training:	Epoch: [32][153/817]	Loss 0.0118 (0.0280)	
training:	Epoch: [32][154/817]	Loss 0.0063 (0.0278)	
training:	Epoch: [32][155/817]	Loss 0.0100 (0.0277)	
training:	Epoch: [32][156/817]	Loss 0.0078 (0.0276)	
training:	Epoch: [32][157/817]	Loss 0.0064 (0.0274)	
training:	Epoch: [32][158/817]	Loss 0.0050 (0.0273)	
training:	Epoch: [32][159/817]	Loss 0.0061 (0.0272)	
training:	Epoch: [32][160/817]	Loss 0.0057 (0.0270)	
training:	Epoch: [32][161/817]	Loss 0.0055 (0.0269)	
training:	Epoch: [32][162/817]	Loss 0.0064 (0.0268)	
training:	Epoch: [32][163/817]	Loss 0.0057 (0.0266)	
training:	Epoch: [32][164/817]	Loss 0.0066 (0.0265)	
training:	Epoch: [32][165/817]	Loss 0.0070 (0.0264)	
training:	Epoch: [32][166/817]	Loss 0.0070 (0.0263)	
training:	Epoch: [32][167/817]	Loss 0.0081 (0.0262)	
training:	Epoch: [32][168/817]	Loss 0.0088 (0.0261)	
training:	Epoch: [32][169/817]	Loss 0.0060 (0.0260)	
training:	Epoch: [32][170/817]	Loss 0.0064 (0.0258)	
training:	Epoch: [32][171/817]	Loss 0.0075 (0.0257)	
training:	Epoch: [32][172/817]	Loss 0.0066 (0.0256)	
training:	Epoch: [32][173/817]	Loss 0.0055 (0.0255)	
training:	Epoch: [32][174/817]	Loss 0.0065 (0.0254)	
training:	Epoch: [32][175/817]	Loss 0.0071 (0.0253)	
training:	Epoch: [32][176/817]	Loss 0.0059 (0.0252)	
training:	Epoch: [32][177/817]	Loss 0.0107 (0.0251)	
training:	Epoch: [32][178/817]	Loss 0.0055 (0.0250)	
training:	Epoch: [32][179/817]	Loss 0.0067 (0.0249)	
training:	Epoch: [32][180/817]	Loss 0.0053 (0.0248)	
training:	Epoch: [32][181/817]	Loss 0.0091 (0.0247)	
training:	Epoch: [32][182/817]	Loss 0.0065 (0.0246)	
training:	Epoch: [32][183/817]	Loss 0.0065 (0.0245)	
training:	Epoch: [32][184/817]	Loss 0.0076 (0.0244)	
training:	Epoch: [32][185/817]	Loss 0.0072 (0.0243)	
training:	Epoch: [32][186/817]	Loss 0.0076 (0.0242)	
training:	Epoch: [32][187/817]	Loss 0.0055 (0.0241)	
training:	Epoch: [32][188/817]	Loss 0.0076 (0.0240)	
training:	Epoch: [32][189/817]	Loss 0.0061 (0.0239)	
training:	Epoch: [32][190/817]	Loss 0.0071 (0.0238)	
training:	Epoch: [32][191/817]	Loss 0.0056 (0.0238)	
training:	Epoch: [32][192/817]	Loss 0.0058 (0.0237)	
training:	Epoch: [32][193/817]	Loss 0.0065 (0.0236)	
training:	Epoch: [32][194/817]	Loss 0.0057 (0.0235)	
training:	Epoch: [32][195/817]	Loss 0.0058 (0.0234)	
training:	Epoch: [32][196/817]	Loss 0.0056 (0.0233)	
training:	Epoch: [32][197/817]	Loss 0.0072 (0.0232)	
training:	Epoch: [32][198/817]	Loss 0.0057 (0.0231)	
training:	Epoch: [32][199/817]	Loss 0.5711 (0.0259)	
training:	Epoch: [32][200/817]	Loss 0.0057 (0.0258)	
training:	Epoch: [32][201/817]	Loss 0.0073 (0.0257)	
training:	Epoch: [32][202/817]	Loss 0.0058 (0.0256)	
training:	Epoch: [32][203/817]	Loss 0.0078 (0.0255)	
training:	Epoch: [32][204/817]	Loss 0.0087 (0.0254)	
training:	Epoch: [32][205/817]	Loss 0.0062 (0.0253)	
training:	Epoch: [32][206/817]	Loss 0.0064 (0.0252)	
training:	Epoch: [32][207/817]	Loss 0.0073 (0.0251)	
training:	Epoch: [32][208/817]	Loss 0.0126 (0.0251)	
training:	Epoch: [32][209/817]	Loss 0.0074 (0.0250)	
training:	Epoch: [32][210/817]	Loss 0.0063 (0.0249)	
training:	Epoch: [32][211/817]	Loss 0.0051 (0.0248)	
training:	Epoch: [32][212/817]	Loss 0.0092 (0.0247)	
training:	Epoch: [32][213/817]	Loss 0.0071 (0.0247)	
training:	Epoch: [32][214/817]	Loss 0.0084 (0.0246)	
training:	Epoch: [32][215/817]	Loss 0.0060 (0.0245)	
training:	Epoch: [32][216/817]	Loss 0.0109 (0.0244)	
training:	Epoch: [32][217/817]	Loss 0.0055 (0.0243)	
training:	Epoch: [32][218/817]	Loss 0.0058 (0.0243)	
training:	Epoch: [32][219/817]	Loss 0.5425 (0.0266)	
training:	Epoch: [32][220/817]	Loss 0.0072 (0.0265)	
training:	Epoch: [32][221/817]	Loss 0.0071 (0.0265)	
training:	Epoch: [32][222/817]	Loss 0.0073 (0.0264)	
training:	Epoch: [32][223/817]	Loss 0.0067 (0.0263)	
training:	Epoch: [32][224/817]	Loss 0.0080 (0.0262)	
training:	Epoch: [32][225/817]	Loss 0.6376 (0.0289)	
training:	Epoch: [32][226/817]	Loss 0.0086 (0.0288)	
training:	Epoch: [32][227/817]	Loss 0.0046 (0.0287)	
training:	Epoch: [32][228/817]	Loss 0.0059 (0.0286)	
training:	Epoch: [32][229/817]	Loss 0.0069 (0.0285)	
training:	Epoch: [32][230/817]	Loss 0.0062 (0.0284)	
training:	Epoch: [32][231/817]	Loss 0.0058 (0.0283)	
training:	Epoch: [32][232/817]	Loss 0.0072 (0.0282)	
training:	Epoch: [32][233/817]	Loss 0.0082 (0.0282)	
training:	Epoch: [32][234/817]	Loss 0.0051 (0.0281)	
training:	Epoch: [32][235/817]	Loss 0.0049 (0.0280)	
training:	Epoch: [32][236/817]	Loss 0.0086 (0.0279)	
training:	Epoch: [32][237/817]	Loss 0.0073 (0.0278)	
training:	Epoch: [32][238/817]	Loss 0.0073 (0.0277)	
training:	Epoch: [32][239/817]	Loss 0.0092 (0.0276)	
training:	Epoch: [32][240/817]	Loss 0.0082 (0.0275)	
training:	Epoch: [32][241/817]	Loss 1.3076 (0.0329)	
training:	Epoch: [32][242/817]	Loss 0.0055 (0.0327)	
training:	Epoch: [32][243/817]	Loss 0.0079 (0.0326)	
training:	Epoch: [32][244/817]	Loss 0.0067 (0.0325)	
training:	Epoch: [32][245/817]	Loss 0.0067 (0.0324)	
training:	Epoch: [32][246/817]	Loss 0.0210 (0.0324)	
training:	Epoch: [32][247/817]	Loss 0.0087 (0.0323)	
training:	Epoch: [32][248/817]	Loss 0.0052 (0.0322)	
training:	Epoch: [32][249/817]	Loss 0.0062 (0.0321)	
training:	Epoch: [32][250/817]	Loss 0.0061 (0.0320)	
training:	Epoch: [32][251/817]	Loss 0.0056 (0.0319)	
training:	Epoch: [32][252/817]	Loss 0.0082 (0.0318)	
training:	Epoch: [32][253/817]	Loss 0.0084 (0.0317)	
training:	Epoch: [32][254/817]	Loss 0.0068 (0.0316)	
training:	Epoch: [32][255/817]	Loss 0.0072 (0.0315)	
training:	Epoch: [32][256/817]	Loss 0.0065 (0.0314)	
training:	Epoch: [32][257/817]	Loss 0.0070 (0.0313)	
training:	Epoch: [32][258/817]	Loss 0.0066 (0.0312)	
training:	Epoch: [32][259/817]	Loss 0.0084 (0.0311)	
training:	Epoch: [32][260/817]	Loss 0.0055 (0.0310)	
training:	Epoch: [32][261/817]	Loss 0.0082 (0.0309)	
training:	Epoch: [32][262/817]	Loss 0.0059 (0.0308)	
training:	Epoch: [32][263/817]	Loss 0.0049 (0.0307)	
training:	Epoch: [32][264/817]	Loss 0.0104 (0.0306)	
training:	Epoch: [32][265/817]	Loss 0.0077 (0.0306)	
training:	Epoch: [32][266/817]	Loss 0.0058 (0.0305)	
training:	Epoch: [32][267/817]	Loss 0.0052 (0.0304)	
training:	Epoch: [32][268/817]	Loss 0.6042 (0.0325)	
training:	Epoch: [32][269/817]	Loss 0.0065 (0.0324)	
training:	Epoch: [32][270/817]	Loss 0.0064 (0.0323)	
training:	Epoch: [32][271/817]	Loss 0.0064 (0.0322)	
training:	Epoch: [32][272/817]	Loss 0.0071 (0.0321)	
training:	Epoch: [32][273/817]	Loss 0.0056 (0.0320)	
training:	Epoch: [32][274/817]	Loss 0.0084 (0.0320)	
training:	Epoch: [32][275/817]	Loss 0.0054 (0.0319)	
training:	Epoch: [32][276/817]	Loss 0.6094 (0.0339)	
training:	Epoch: [32][277/817]	Loss 0.0132 (0.0339)	
training:	Epoch: [32][278/817]	Loss 0.0080 (0.0338)	
training:	Epoch: [32][279/817]	Loss 0.5641 (0.0357)	
training:	Epoch: [32][280/817]	Loss 0.0061 (0.0356)	
training:	Epoch: [32][281/817]	Loss 0.0071 (0.0355)	
training:	Epoch: [32][282/817]	Loss 0.0062 (0.0354)	
training:	Epoch: [32][283/817]	Loss 0.0083 (0.0353)	
training:	Epoch: [32][284/817]	Loss 0.0083 (0.0352)	
training:	Epoch: [32][285/817]	Loss 0.0071 (0.0351)	
training:	Epoch: [32][286/817]	Loss 0.0067 (0.0350)	
training:	Epoch: [32][287/817]	Loss 0.0082 (0.0349)	
training:	Epoch: [32][288/817]	Loss 0.0057 (0.0348)	
training:	Epoch: [32][289/817]	Loss 0.0077 (0.0347)	
training:	Epoch: [32][290/817]	Loss 0.0098 (0.0346)	
training:	Epoch: [32][291/817]	Loss 0.0062 (0.0345)	
training:	Epoch: [32][292/817]	Loss 0.0074 (0.0344)	
training:	Epoch: [32][293/817]	Loss 0.0057 (0.0343)	
training:	Epoch: [32][294/817]	Loss 0.0056 (0.0342)	
training:	Epoch: [32][295/817]	Loss 0.0075 (0.0341)	
training:	Epoch: [32][296/817]	Loss 0.0065 (0.0340)	
training:	Epoch: [32][297/817]	Loss 0.0066 (0.0339)	
training:	Epoch: [32][298/817]	Loss 0.0075 (0.0339)	
training:	Epoch: [32][299/817]	Loss 0.0419 (0.0339)	
training:	Epoch: [32][300/817]	Loss 0.0055 (0.0338)	
training:	Epoch: [32][301/817]	Loss 0.0087 (0.0337)	
training:	Epoch: [32][302/817]	Loss 0.0079 (0.0336)	
training:	Epoch: [32][303/817]	Loss 0.0075 (0.0335)	
training:	Epoch: [32][304/817]	Loss 0.0055 (0.0334)	
training:	Epoch: [32][305/817]	Loss 0.0074 (0.0334)	
training:	Epoch: [32][306/817]	Loss 0.0076 (0.0333)	
training:	Epoch: [32][307/817]	Loss 0.0077 (0.0332)	
training:	Epoch: [32][308/817]	Loss 0.0055 (0.0331)	
training:	Epoch: [32][309/817]	Loss 0.0599 (0.0332)	
training:	Epoch: [32][310/817]	Loss 0.0086 (0.0331)	
training:	Epoch: [32][311/817]	Loss 0.0062 (0.0330)	
training:	Epoch: [32][312/817]	Loss 0.0089 (0.0329)	
training:	Epoch: [32][313/817]	Loss 0.0081 (0.0329)	
training:	Epoch: [32][314/817]	Loss 0.0077 (0.0328)	
training:	Epoch: [32][315/817]	Loss 0.0068 (0.0327)	
training:	Epoch: [32][316/817]	Loss 0.0080 (0.0326)	
training:	Epoch: [32][317/817]	Loss 0.6516 (0.0346)	
training:	Epoch: [32][318/817]	Loss 0.0073 (0.0345)	
training:	Epoch: [32][319/817]	Loss 0.0067 (0.0344)	
training:	Epoch: [32][320/817]	Loss 0.0065 (0.0343)	
training:	Epoch: [32][321/817]	Loss 0.0078 (0.0342)	
training:	Epoch: [32][322/817]	Loss 0.0058 (0.0341)	
training:	Epoch: [32][323/817]	Loss 0.0428 (0.0342)	
training:	Epoch: [32][324/817]	Loss 0.0088 (0.0341)	
training:	Epoch: [32][325/817]	Loss 0.0055 (0.0340)	
training:	Epoch: [32][326/817]	Loss 0.0079 (0.0339)	
training:	Epoch: [32][327/817]	Loss 0.0086 (0.0338)	
training:	Epoch: [32][328/817]	Loss 0.0075 (0.0338)	
training:	Epoch: [32][329/817]	Loss 0.0074 (0.0337)	
training:	Epoch: [32][330/817]	Loss 0.0060 (0.0336)	
training:	Epoch: [32][331/817]	Loss 0.0072 (0.0335)	
training:	Epoch: [32][332/817]	Loss 0.0063 (0.0334)	
training:	Epoch: [32][333/817]	Loss 0.0071 (0.0334)	
training:	Epoch: [32][334/817]	Loss 0.0070 (0.0333)	
training:	Epoch: [32][335/817]	Loss 0.0070 (0.0332)	
training:	Epoch: [32][336/817]	Loss 0.0050 (0.0331)	
training:	Epoch: [32][337/817]	Loss 0.0069 (0.0330)	
training:	Epoch: [32][338/817]	Loss 0.0059 (0.0330)	
training:	Epoch: [32][339/817]	Loss 0.0059 (0.0329)	
training:	Epoch: [32][340/817]	Loss 0.0064 (0.0328)	
training:	Epoch: [32][341/817]	Loss 0.0089 (0.0327)	
training:	Epoch: [32][342/817]	Loss 0.0069 (0.0327)	
training:	Epoch: [32][343/817]	Loss 0.0056 (0.0326)	
training:	Epoch: [32][344/817]	Loss 0.0067 (0.0325)	
training:	Epoch: [32][345/817]	Loss 0.0063 (0.0324)	
training:	Epoch: [32][346/817]	Loss 0.0076 (0.0324)	
training:	Epoch: [32][347/817]	Loss 0.0061 (0.0323)	
training:	Epoch: [32][348/817]	Loss 0.0070 (0.0322)	
training:	Epoch: [32][349/817]	Loss 0.0068 (0.0321)	
training:	Epoch: [32][350/817]	Loss 0.0077 (0.0321)	
training:	Epoch: [32][351/817]	Loss 0.5570 (0.0336)	
training:	Epoch: [32][352/817]	Loss 0.0076 (0.0335)	
training:	Epoch: [32][353/817]	Loss 0.0072 (0.0334)	
training:	Epoch: [32][354/817]	Loss 0.0067 (0.0333)	
training:	Epoch: [32][355/817]	Loss 0.0087 (0.0333)	
training:	Epoch: [32][356/817]	Loss 0.0069 (0.0332)	
training:	Epoch: [32][357/817]	Loss 0.0063 (0.0331)	
training:	Epoch: [32][358/817]	Loss 0.0058 (0.0330)	
training:	Epoch: [32][359/817]	Loss 0.0071 (0.0330)	
training:	Epoch: [32][360/817]	Loss 0.0109 (0.0329)	
training:	Epoch: [32][361/817]	Loss 0.0078 (0.0328)	
training:	Epoch: [32][362/817]	Loss 0.0072 (0.0328)	
training:	Epoch: [32][363/817]	Loss 0.5544 (0.0342)	
training:	Epoch: [32][364/817]	Loss 0.0053 (0.0341)	
training:	Epoch: [32][365/817]	Loss 0.0060 (0.0340)	
training:	Epoch: [32][366/817]	Loss 0.0056 (0.0340)	
training:	Epoch: [32][367/817]	Loss 0.0074 (0.0339)	
training:	Epoch: [32][368/817]	Loss 0.0068 (0.0338)	
training:	Epoch: [32][369/817]	Loss 0.0053 (0.0337)	
training:	Epoch: [32][370/817]	Loss 0.0120 (0.0337)	
training:	Epoch: [32][371/817]	Loss 0.0072 (0.0336)	
training:	Epoch: [32][372/817]	Loss 0.0060 (0.0335)	
training:	Epoch: [32][373/817]	Loss 0.0063 (0.0335)	
training:	Epoch: [32][374/817]	Loss 0.0065 (0.0334)	
training:	Epoch: [32][375/817]	Loss 0.0061 (0.0333)	
training:	Epoch: [32][376/817]	Loss 0.0068 (0.0333)	
training:	Epoch: [32][377/817]	Loss 0.0074 (0.0332)	
training:	Epoch: [32][378/817]	Loss 0.0082 (0.0331)	
training:	Epoch: [32][379/817]	Loss 0.0067 (0.0330)	
training:	Epoch: [32][380/817]	Loss 0.0075 (0.0330)	
training:	Epoch: [32][381/817]	Loss 0.0052 (0.0329)	
training:	Epoch: [32][382/817]	Loss 0.0063 (0.0328)	
training:	Epoch: [32][383/817]	Loss 0.0107 (0.0328)	
training:	Epoch: [32][384/817]	Loss 0.0066 (0.0327)	
training:	Epoch: [32][385/817]	Loss 0.0092 (0.0327)	
training:	Epoch: [32][386/817]	Loss 0.0058 (0.0326)	
training:	Epoch: [32][387/817]	Loss 0.0053 (0.0325)	
training:	Epoch: [32][388/817]	Loss 0.0069 (0.0324)	
training:	Epoch: [32][389/817]	Loss 0.0073 (0.0324)	
training:	Epoch: [32][390/817]	Loss 0.0062 (0.0323)	
training:	Epoch: [32][391/817]	Loss 0.0058 (0.0322)	
training:	Epoch: [32][392/817]	Loss 0.0076 (0.0322)	
training:	Epoch: [32][393/817]	Loss 0.0077 (0.0321)	
training:	Epoch: [32][394/817]	Loss 0.0069 (0.0321)	
training:	Epoch: [32][395/817]	Loss 0.0063 (0.0320)	
training:	Epoch: [32][396/817]	Loss 0.0056 (0.0319)	
training:	Epoch: [32][397/817]	Loss 0.0052 (0.0319)	
training:	Epoch: [32][398/817]	Loss 0.0074 (0.0318)	
training:	Epoch: [32][399/817]	Loss 0.0073 (0.0317)	
training:	Epoch: [32][400/817]	Loss 0.0061 (0.0317)	
training:	Epoch: [32][401/817]	Loss 0.0086 (0.0316)	
training:	Epoch: [32][402/817]	Loss 0.0065 (0.0316)	
training:	Epoch: [32][403/817]	Loss 0.0070 (0.0315)	
training:	Epoch: [32][404/817]	Loss 0.0071 (0.0314)	
training:	Epoch: [32][405/817]	Loss 0.0049 (0.0314)	
training:	Epoch: [32][406/817]	Loss 0.0068 (0.0313)	
training:	Epoch: [32][407/817]	Loss 0.0062 (0.0312)	
training:	Epoch: [32][408/817]	Loss 0.0061 (0.0312)	
training:	Epoch: [32][409/817]	Loss 0.1211 (0.0314)	
training:	Epoch: [32][410/817]	Loss 0.0062 (0.0313)	
training:	Epoch: [32][411/817]	Loss 0.0073 (0.0313)	
training:	Epoch: [32][412/817]	Loss 0.0057 (0.0312)	
training:	Epoch: [32][413/817]	Loss 0.0117 (0.0312)	
training:	Epoch: [32][414/817]	Loss 0.0068 (0.0311)	
training:	Epoch: [32][415/817]	Loss 0.0064 (0.0311)	
training:	Epoch: [32][416/817]	Loss 0.0075 (0.0310)	
training:	Epoch: [32][417/817]	Loss 0.0055 (0.0309)	
training:	Epoch: [32][418/817]	Loss 0.0066 (0.0309)	
training:	Epoch: [32][419/817]	Loss 0.0102 (0.0308)	
training:	Epoch: [32][420/817]	Loss 0.0061 (0.0308)	
training:	Epoch: [32][421/817]	Loss 0.0069 (0.0307)	
training:	Epoch: [32][422/817]	Loss 0.3728 (0.0315)	
training:	Epoch: [32][423/817]	Loss 0.0849 (0.0316)	
training:	Epoch: [32][424/817]	Loss 0.0061 (0.0316)	
training:	Epoch: [32][425/817]	Loss 0.0067 (0.0315)	
training:	Epoch: [32][426/817]	Loss 0.5889 (0.0328)	
training:	Epoch: [32][427/817]	Loss 0.0304 (0.0328)	
training:	Epoch: [32][428/817]	Loss 0.0067 (0.0328)	
training:	Epoch: [32][429/817]	Loss 0.4674 (0.0338)	
training:	Epoch: [32][430/817]	Loss 0.0084 (0.0337)	
training:	Epoch: [32][431/817]	Loss 0.0064 (0.0337)	
training:	Epoch: [32][432/817]	Loss 0.0078 (0.0336)	
training:	Epoch: [32][433/817]	Loss 0.0123 (0.0336)	
training:	Epoch: [32][434/817]	Loss 0.0059 (0.0335)	
training:	Epoch: [32][435/817]	Loss 0.0083 (0.0334)	
training:	Epoch: [32][436/817]	Loss 0.0083 (0.0334)	
training:	Epoch: [32][437/817]	Loss 0.0061 (0.0333)	
training:	Epoch: [32][438/817]	Loss 0.0064 (0.0333)	
training:	Epoch: [32][439/817]	Loss 0.0079 (0.0332)	
training:	Epoch: [32][440/817]	Loss 0.0084 (0.0331)	
training:	Epoch: [32][441/817]	Loss 0.0062 (0.0331)	
training:	Epoch: [32][442/817]	Loss 0.0071 (0.0330)	
training:	Epoch: [32][443/817]	Loss 0.3977 (0.0338)	
training:	Epoch: [32][444/817]	Loss 0.0065 (0.0338)	
training:	Epoch: [32][445/817]	Loss 0.0071 (0.0337)	
training:	Epoch: [32][446/817]	Loss 0.0077 (0.0337)	
training:	Epoch: [32][447/817]	Loss 0.0083 (0.0336)	
training:	Epoch: [32][448/817]	Loss 0.0054 (0.0335)	
training:	Epoch: [32][449/817]	Loss 0.0066 (0.0335)	
training:	Epoch: [32][450/817]	Loss 0.0056 (0.0334)	
training:	Epoch: [32][451/817]	Loss 0.0082 (0.0334)	
training:	Epoch: [32][452/817]	Loss 0.0062 (0.0333)	
training:	Epoch: [32][453/817]	Loss 0.4159 (0.0341)	
training:	Epoch: [32][454/817]	Loss 0.0057 (0.0341)	
training:	Epoch: [32][455/817]	Loss 0.0089 (0.0340)	
training:	Epoch: [32][456/817]	Loss 0.0080 (0.0340)	
training:	Epoch: [32][457/817]	Loss 0.0063 (0.0339)	
training:	Epoch: [32][458/817]	Loss 0.6513 (0.0353)	
training:	Epoch: [32][459/817]	Loss 0.0056 (0.0352)	
training:	Epoch: [32][460/817]	Loss 0.0378 (0.0352)	
training:	Epoch: [32][461/817]	Loss 0.0059 (0.0351)	
training:	Epoch: [32][462/817]	Loss 0.0073 (0.0351)	
training:	Epoch: [32][463/817]	Loss 0.0082 (0.0350)	
training:	Epoch: [32][464/817]	Loss 0.6215 (0.0363)	
training:	Epoch: [32][465/817]	Loss 0.0064 (0.0362)	
training:	Epoch: [32][466/817]	Loss 0.0090 (0.0362)	
training:	Epoch: [32][467/817]	Loss 0.0298 (0.0361)	
training:	Epoch: [32][468/817]	Loss 0.0092 (0.0361)	
training:	Epoch: [32][469/817]	Loss 0.0106 (0.0360)	
training:	Epoch: [32][470/817]	Loss 0.0107 (0.0360)	
training:	Epoch: [32][471/817]	Loss 0.0082 (0.0359)	
training:	Epoch: [32][472/817]	Loss 0.5475 (0.0370)	
training:	Epoch: [32][473/817]	Loss 0.5552 (0.0381)	
training:	Epoch: [32][474/817]	Loss 0.0071 (0.0380)	
training:	Epoch: [32][475/817]	Loss 0.0073 (0.0380)	
training:	Epoch: [32][476/817]	Loss 0.0077 (0.0379)	
training:	Epoch: [32][477/817]	Loss 0.0055 (0.0378)	
training:	Epoch: [32][478/817]	Loss 0.0068 (0.0378)	
training:	Epoch: [32][479/817]	Loss 0.0055 (0.0377)	
training:	Epoch: [32][480/817]	Loss 0.0070 (0.0376)	
training:	Epoch: [32][481/817]	Loss 0.0109 (0.0376)	
training:	Epoch: [32][482/817]	Loss 0.0076 (0.0375)	
training:	Epoch: [32][483/817]	Loss 0.0082 (0.0375)	
training:	Epoch: [32][484/817]	Loss 0.0119 (0.0374)	
training:	Epoch: [32][485/817]	Loss 0.0061 (0.0373)	
training:	Epoch: [32][486/817]	Loss 0.0199 (0.0373)	
training:	Epoch: [32][487/817]	Loss 0.0059 (0.0372)	
training:	Epoch: [32][488/817]	Loss 0.0075 (0.0372)	
training:	Epoch: [32][489/817]	Loss 0.0077 (0.0371)	
training:	Epoch: [32][490/817]	Loss 0.0749 (0.0372)	
training:	Epoch: [32][491/817]	Loss 0.0081 (0.0371)	
training:	Epoch: [32][492/817]	Loss 0.1239 (0.0373)	
training:	Epoch: [32][493/817]	Loss 0.0289 (0.0373)	
training:	Epoch: [32][494/817]	Loss 0.0073 (0.0372)	
training:	Epoch: [32][495/817]	Loss 0.0068 (0.0372)	
training:	Epoch: [32][496/817]	Loss 0.0054 (0.0371)	
training:	Epoch: [32][497/817]	Loss 0.0081 (0.0371)	
training:	Epoch: [32][498/817]	Loss 0.0074 (0.0370)	
training:	Epoch: [32][499/817]	Loss 0.0182 (0.0370)	
training:	Epoch: [32][500/817]	Loss 0.0095 (0.0369)	
training:	Epoch: [32][501/817]	Loss 0.0076 (0.0368)	
training:	Epoch: [32][502/817]	Loss 0.5107 (0.0378)	
training:	Epoch: [32][503/817]	Loss 0.0086 (0.0377)	
training:	Epoch: [32][504/817]	Loss 0.0062 (0.0377)	
training:	Epoch: [32][505/817]	Loss 0.0066 (0.0376)	
training:	Epoch: [32][506/817]	Loss 0.0075 (0.0375)	
training:	Epoch: [32][507/817]	Loss 0.0082 (0.0375)	
training:	Epoch: [32][508/817]	Loss 0.0082 (0.0374)	
training:	Epoch: [32][509/817]	Loss 0.2433 (0.0378)	
training:	Epoch: [32][510/817]	Loss 0.2350 (0.0382)	
training:	Epoch: [32][511/817]	Loss 0.6379 (0.0394)	
training:	Epoch: [32][512/817]	Loss 0.0058 (0.0393)	
training:	Epoch: [32][513/817]	Loss 0.0058 (0.0393)	
training:	Epoch: [32][514/817]	Loss 0.0069 (0.0392)	
training:	Epoch: [32][515/817]	Loss 0.0095 (0.0391)	
training:	Epoch: [32][516/817]	Loss 0.0059 (0.0391)	
training:	Epoch: [32][517/817]	Loss 0.0070 (0.0390)	
training:	Epoch: [32][518/817]	Loss 0.0068 (0.0390)	
training:	Epoch: [32][519/817]	Loss 0.0078 (0.0389)	
training:	Epoch: [32][520/817]	Loss 0.0073 (0.0388)	
training:	Epoch: [32][521/817]	Loss 0.0068 (0.0388)	
training:	Epoch: [32][522/817]	Loss 0.0076 (0.0387)	
training:	Epoch: [32][523/817]	Loss 0.0092 (0.0387)	
training:	Epoch: [32][524/817]	Loss 0.0143 (0.0386)	
training:	Epoch: [32][525/817]	Loss 0.0466 (0.0386)	
training:	Epoch: [32][526/817]	Loss 0.0061 (0.0386)	
training:	Epoch: [32][527/817]	Loss 0.0086 (0.0385)	
training:	Epoch: [32][528/817]	Loss 0.0071 (0.0384)	
training:	Epoch: [32][529/817]	Loss 0.0087 (0.0384)	
training:	Epoch: [32][530/817]	Loss 0.0058 (0.0383)	
training:	Epoch: [32][531/817]	Loss 0.0076 (0.0383)	
training:	Epoch: [32][532/817]	Loss 0.0096 (0.0382)	
training:	Epoch: [32][533/817]	Loss 0.0065 (0.0382)	
training:	Epoch: [32][534/817]	Loss 0.0081 (0.0381)	
training:	Epoch: [32][535/817]	Loss 0.0087 (0.0380)	
training:	Epoch: [32][536/817]	Loss 0.0054 (0.0380)	
training:	Epoch: [32][537/817]	Loss 0.0073 (0.0379)	
training:	Epoch: [32][538/817]	Loss 0.0073 (0.0379)	
training:	Epoch: [32][539/817]	Loss 0.5860 (0.0389)	
training:	Epoch: [32][540/817]	Loss 0.0070 (0.0388)	
training:	Epoch: [32][541/817]	Loss 0.0077 (0.0388)	
training:	Epoch: [32][542/817]	Loss 0.0428 (0.0388)	
training:	Epoch: [32][543/817]	Loss 0.0117 (0.0387)	
training:	Epoch: [32][544/817]	Loss 0.0073 (0.0387)	
training:	Epoch: [32][545/817]	Loss 0.0073 (0.0386)	
training:	Epoch: [32][546/817]	Loss 0.0078 (0.0386)	
training:	Epoch: [32][547/817]	Loss 0.0062 (0.0385)	
training:	Epoch: [32][548/817]	Loss 0.0063 (0.0384)	
training:	Epoch: [32][549/817]	Loss 0.0084 (0.0384)	
training:	Epoch: [32][550/817]	Loss 0.0074 (0.0383)	
training:	Epoch: [32][551/817]	Loss 0.0057 (0.0383)	
training:	Epoch: [32][552/817]	Loss 0.0089 (0.0382)	
training:	Epoch: [32][553/817]	Loss 0.0051 (0.0382)	
training:	Epoch: [32][554/817]	Loss 0.0073 (0.0381)	
training:	Epoch: [32][555/817]	Loss 0.0061 (0.0380)	
training:	Epoch: [32][556/817]	Loss 0.0067 (0.0380)	
training:	Epoch: [32][557/817]	Loss 0.0069 (0.0379)	
training:	Epoch: [32][558/817]	Loss 0.0078 (0.0379)	
training:	Epoch: [32][559/817]	Loss 0.0072 (0.0378)	
training:	Epoch: [32][560/817]	Loss 0.0083 (0.0378)	
training:	Epoch: [32][561/817]	Loss 0.0079 (0.0377)	
training:	Epoch: [32][562/817]	Loss 0.0118 (0.0377)	
training:	Epoch: [32][563/817]	Loss 0.5614 (0.0386)	
training:	Epoch: [32][564/817]	Loss 0.0061 (0.0385)	
training:	Epoch: [32][565/817]	Loss 0.0084 (0.0385)	
training:	Epoch: [32][566/817]	Loss 0.0063 (0.0384)	
training:	Epoch: [32][567/817]	Loss 0.0054 (0.0384)	
training:	Epoch: [32][568/817]	Loss 0.0082 (0.0383)	
training:	Epoch: [32][569/817]	Loss 0.0065 (0.0383)	
training:	Epoch: [32][570/817]	Loss 0.0097 (0.0382)	
training:	Epoch: [32][571/817]	Loss 0.0067 (0.0382)	
training:	Epoch: [32][572/817]	Loss 0.0073 (0.0381)	
training:	Epoch: [32][573/817]	Loss 0.0072 (0.0381)	
training:	Epoch: [32][574/817]	Loss 0.0082 (0.0380)	
training:	Epoch: [32][575/817]	Loss 0.0072 (0.0379)	
training:	Epoch: [32][576/817]	Loss 0.6190 (0.0390)	
training:	Epoch: [32][577/817]	Loss 0.0100 (0.0389)	
training:	Epoch: [32][578/817]	Loss 0.0054 (0.0388)	
training:	Epoch: [32][579/817]	Loss 0.0137 (0.0388)	
training:	Epoch: [32][580/817]	Loss 0.0067 (0.0388)	
training:	Epoch: [32][581/817]	Loss 0.0071 (0.0387)	
training:	Epoch: [32][582/817]	Loss 0.0066 (0.0386)	
training:	Epoch: [32][583/817]	Loss 0.0102 (0.0386)	
training:	Epoch: [32][584/817]	Loss 0.0069 (0.0385)	
training:	Epoch: [32][585/817]	Loss 0.5501 (0.0394)	
training:	Epoch: [32][586/817]	Loss 0.0071 (0.0394)	
training:	Epoch: [32][587/817]	Loss 0.0112 (0.0393)	
training:	Epoch: [32][588/817]	Loss 0.5380 (0.0402)	
training:	Epoch: [32][589/817]	Loss 0.0073 (0.0401)	
training:	Epoch: [32][590/817]	Loss 0.0079 (0.0400)	
training:	Epoch: [32][591/817]	Loss 0.0070 (0.0400)	
training:	Epoch: [32][592/817]	Loss 0.0097 (0.0399)	
training:	Epoch: [32][593/817]	Loss 0.6376 (0.0409)	
training:	Epoch: [32][594/817]	Loss 0.0091 (0.0409)	
training:	Epoch: [32][595/817]	Loss 0.5249 (0.0417)	
training:	Epoch: [32][596/817]	Loss 0.0102 (0.0417)	
training:	Epoch: [32][597/817]	Loss 0.0057 (0.0416)	
training:	Epoch: [32][598/817]	Loss 0.0100 (0.0415)	
training:	Epoch: [32][599/817]	Loss 0.0077 (0.0415)	
training:	Epoch: [32][600/817]	Loss 0.0185 (0.0414)	
training:	Epoch: [32][601/817]	Loss 0.0066 (0.0414)	
training:	Epoch: [32][602/817]	Loss 0.0078 (0.0413)	
training:	Epoch: [32][603/817]	Loss 0.0072 (0.0413)	
training:	Epoch: [32][604/817]	Loss 0.0081 (0.0412)	
training:	Epoch: [32][605/817]	Loss 0.0067 (0.0412)	
training:	Epoch: [32][606/817]	Loss 0.0079 (0.0411)	
training:	Epoch: [32][607/817]	Loss 0.0076 (0.0411)	
training:	Epoch: [32][608/817]	Loss 0.0551 (0.0411)	
training:	Epoch: [32][609/817]	Loss 0.1837 (0.0413)	
training:	Epoch: [32][610/817]	Loss 0.0308 (0.0413)	
training:	Epoch: [32][611/817]	Loss 0.0082 (0.0412)	
training:	Epoch: [32][612/817]	Loss 0.0063 (0.0412)	
training:	Epoch: [32][613/817]	Loss 0.4963 (0.0419)	
training:	Epoch: [32][614/817]	Loss 0.0107 (0.0419)	
training:	Epoch: [32][615/817]	Loss 0.0082 (0.0418)	
training:	Epoch: [32][616/817]	Loss 0.0109 (0.0418)	
training:	Epoch: [32][617/817]	Loss 0.0072 (0.0417)	
training:	Epoch: [32][618/817]	Loss 0.0270 (0.0417)	
training:	Epoch: [32][619/817]	Loss 0.0065 (0.0416)	
training:	Epoch: [32][620/817]	Loss 0.0134 (0.0416)	
training:	Epoch: [32][621/817]	Loss 0.0055 (0.0415)	
training:	Epoch: [32][622/817]	Loss 0.0102 (0.0415)	
training:	Epoch: [32][623/817]	Loss 0.0090 (0.0414)	
training:	Epoch: [32][624/817]	Loss 0.0099 (0.0414)	
training:	Epoch: [32][625/817]	Loss 0.5660 (0.0422)	
training:	Epoch: [32][626/817]	Loss 0.0096 (0.0422)	
training:	Epoch: [32][627/817]	Loss 0.0089 (0.0421)	
training:	Epoch: [32][628/817]	Loss 0.0113 (0.0421)	
training:	Epoch: [32][629/817]	Loss 0.0091 (0.0420)	
training:	Epoch: [32][630/817]	Loss 0.0100 (0.0420)	
training:	Epoch: [32][631/817]	Loss 0.0083 (0.0419)	
training:	Epoch: [32][632/817]	Loss 0.0072 (0.0418)	
training:	Epoch: [32][633/817]	Loss 0.0084 (0.0418)	
training:	Epoch: [32][634/817]	Loss 0.0078 (0.0417)	
training:	Epoch: [32][635/817]	Loss 0.0118 (0.0417)	
training:	Epoch: [32][636/817]	Loss 0.0065 (0.0416)	
training:	Epoch: [32][637/817]	Loss 0.0073 (0.0416)	
training:	Epoch: [32][638/817]	Loss 0.0075 (0.0415)	
training:	Epoch: [32][639/817]	Loss 0.0074 (0.0415)	
training:	Epoch: [32][640/817]	Loss 0.0060 (0.0414)	
training:	Epoch: [32][641/817]	Loss 0.0068 (0.0414)	
training:	Epoch: [32][642/817]	Loss 0.0076 (0.0413)	
training:	Epoch: [32][643/817]	Loss 0.0061 (0.0413)	
training:	Epoch: [32][644/817]	Loss 0.0068 (0.0412)	
training:	Epoch: [32][645/817]	Loss 0.0064 (0.0412)	
training:	Epoch: [32][646/817]	Loss 0.0094 (0.0411)	
training:	Epoch: [32][647/817]	Loss 0.6339 (0.0420)	
training:	Epoch: [32][648/817]	Loss 0.0109 (0.0420)	
training:	Epoch: [32][649/817]	Loss 0.0097 (0.0419)	
training:	Epoch: [32][650/817]	Loss 0.0064 (0.0419)	
training:	Epoch: [32][651/817]	Loss 0.0175 (0.0418)	
training:	Epoch: [32][652/817]	Loss 0.0076 (0.0418)	
training:	Epoch: [32][653/817]	Loss 0.0086 (0.0417)	
training:	Epoch: [32][654/817]	Loss 0.0071 (0.0417)	
training:	Epoch: [32][655/817]	Loss 0.0079 (0.0416)	
training:	Epoch: [32][656/817]	Loss 0.0078 (0.0416)	
training:	Epoch: [32][657/817]	Loss 0.0063 (0.0415)	
training:	Epoch: [32][658/817]	Loss 0.0079 (0.0415)	
training:	Epoch: [32][659/817]	Loss 0.0099 (0.0414)	
training:	Epoch: [32][660/817]	Loss 0.0064 (0.0414)	
training:	Epoch: [32][661/817]	Loss 0.0110 (0.0413)	
training:	Epoch: [32][662/817]	Loss 0.0092 (0.0413)	
training:	Epoch: [32][663/817]	Loss 0.0077 (0.0412)	
training:	Epoch: [32][664/817]	Loss 0.0066 (0.0412)	
training:	Epoch: [32][665/817]	Loss 0.0057 (0.0411)	
training:	Epoch: [32][666/817]	Loss 0.0083 (0.0411)	
training:	Epoch: [32][667/817]	Loss 0.0066 (0.0410)	
training:	Epoch: [32][668/817]	Loss 0.0049 (0.0410)	
training:	Epoch: [32][669/817]	Loss 0.0066 (0.0409)	
training:	Epoch: [32][670/817]	Loss 0.0109 (0.0409)	
training:	Epoch: [32][671/817]	Loss 0.0091 (0.0408)	
training:	Epoch: [32][672/817]	Loss 0.0080 (0.0408)	
training:	Epoch: [32][673/817]	Loss 0.0085 (0.0407)	
training:	Epoch: [32][674/817]	Loss 0.0098 (0.0407)	
training:	Epoch: [32][675/817]	Loss 0.0074 (0.0406)	
training:	Epoch: [32][676/817]	Loss 0.6035 (0.0415)	
training:	Epoch: [32][677/817]	Loss 0.0085 (0.0414)	
training:	Epoch: [32][678/817]	Loss 0.0072 (0.0414)	
training:	Epoch: [32][679/817]	Loss 0.0063 (0.0413)	
training:	Epoch: [32][680/817]	Loss 0.0076 (0.0413)	
training:	Epoch: [32][681/817]	Loss 0.0102 (0.0412)	
training:	Epoch: [32][682/817]	Loss 0.0088 (0.0412)	
training:	Epoch: [32][683/817]	Loss 0.0076 (0.0411)	
training:	Epoch: [32][684/817]	Loss 0.0090 (0.0411)	
training:	Epoch: [32][685/817]	Loss 0.1022 (0.0412)	
training:	Epoch: [32][686/817]	Loss 0.0072 (0.0411)	
training:	Epoch: [32][687/817]	Loss 0.0076 (0.0411)	
training:	Epoch: [32][688/817]	Loss 0.0066 (0.0410)	
training:	Epoch: [32][689/817]	Loss 0.0093 (0.0410)	
training:	Epoch: [32][690/817]	Loss 0.0084 (0.0409)	
training:	Epoch: [32][691/817]	Loss 0.0072 (0.0409)	
training:	Epoch: [32][692/817]	Loss 0.0114 (0.0408)	
training:	Epoch: [32][693/817]	Loss 0.0056 (0.0408)	
training:	Epoch: [32][694/817]	Loss 0.0090 (0.0407)	
training:	Epoch: [32][695/817]	Loss 0.0067 (0.0407)	
training:	Epoch: [32][696/817]	Loss 0.0119 (0.0406)	
training:	Epoch: [32][697/817]	Loss 0.6286 (0.0415)	
training:	Epoch: [32][698/817]	Loss 0.0096 (0.0414)	
training:	Epoch: [32][699/817]	Loss 0.0120 (0.0414)	
training:	Epoch: [32][700/817]	Loss 0.0094 (0.0413)	
training:	Epoch: [32][701/817]	Loss 0.0072 (0.0413)	
training:	Epoch: [32][702/817]	Loss 0.0125 (0.0413)	
training:	Epoch: [32][703/817]	Loss 0.4792 (0.0419)	
training:	Epoch: [32][704/817]	Loss 0.0083 (0.0418)	
training:	Epoch: [32][705/817]	Loss 0.0083 (0.0418)	
training:	Epoch: [32][706/817]	Loss 0.0085 (0.0417)	
training:	Epoch: [32][707/817]	Loss 0.0082 (0.0417)	
training:	Epoch: [32][708/817]	Loss 0.0064 (0.0416)	
training:	Epoch: [32][709/817]	Loss 0.0069 (0.0416)	
training:	Epoch: [32][710/817]	Loss 0.0098 (0.0415)	
training:	Epoch: [32][711/817]	Loss 0.0106 (0.0415)	
training:	Epoch: [32][712/817]	Loss 0.0070 (0.0415)	
training:	Epoch: [32][713/817]	Loss 0.0076 (0.0414)	
training:	Epoch: [32][714/817]	Loss 0.0072 (0.0414)	
training:	Epoch: [32][715/817]	Loss 0.0069 (0.0413)	
training:	Epoch: [32][716/817]	Loss 0.0062 (0.0413)	
training:	Epoch: [32][717/817]	Loss 0.0100 (0.0412)	
training:	Epoch: [32][718/817]	Loss 0.0069 (0.0412)	
training:	Epoch: [32][719/817]	Loss 0.0082 (0.0411)	
training:	Epoch: [32][720/817]	Loss 0.0074 (0.0411)	
training:	Epoch: [32][721/817]	Loss 0.0077 (0.0410)	
training:	Epoch: [32][722/817]	Loss 0.0080 (0.0410)	
training:	Epoch: [32][723/817]	Loss 0.0061 (0.0409)	
training:	Epoch: [32][724/817]	Loss 0.0079 (0.0409)	
training:	Epoch: [32][725/817]	Loss 0.0108 (0.0409)	
training:	Epoch: [32][726/817]	Loss 0.0077 (0.0408)	
training:	Epoch: [32][727/817]	Loss 0.0068 (0.0408)	
training:	Epoch: [32][728/817]	Loss 0.0058 (0.0407)	
training:	Epoch: [32][729/817]	Loss 0.0085 (0.0407)	
training:	Epoch: [32][730/817]	Loss 0.0092 (0.0406)	
training:	Epoch: [32][731/817]	Loss 0.0085 (0.0406)	
training:	Epoch: [32][732/817]	Loss 0.0089 (0.0405)	
training:	Epoch: [32][733/817]	Loss 0.0091 (0.0405)	
training:	Epoch: [32][734/817]	Loss 0.0080 (0.0405)	
training:	Epoch: [32][735/817]	Loss 0.0082 (0.0404)	
training:	Epoch: [32][736/817]	Loss 0.0062 (0.0404)	
training:	Epoch: [32][737/817]	Loss 0.0072 (0.0403)	
training:	Epoch: [32][738/817]	Loss 0.0081 (0.0403)	
training:	Epoch: [32][739/817]	Loss 0.0055 (0.0402)	
training:	Epoch: [32][740/817]	Loss 0.0066 (0.0402)	
training:	Epoch: [32][741/817]	Loss 0.0107 (0.0401)	
training:	Epoch: [32][742/817]	Loss 0.0064 (0.0401)	
training:	Epoch: [32][743/817]	Loss 0.0088 (0.0401)	
training:	Epoch: [32][744/817]	Loss 0.0067 (0.0400)	
training:	Epoch: [32][745/817]	Loss 0.0058 (0.0400)	
training:	Epoch: [32][746/817]	Loss 0.9273 (0.0412)	
training:	Epoch: [32][747/817]	Loss 0.0071 (0.0411)	
training:	Epoch: [32][748/817]	Loss 0.0055 (0.0411)	
training:	Epoch: [32][749/817]	Loss 0.0057 (0.0410)	
training:	Epoch: [32][750/817]	Loss 0.0059 (0.0410)	
training:	Epoch: [32][751/817]	Loss 0.0057 (0.0409)	
training:	Epoch: [32][752/817]	Loss 0.0059 (0.0409)	
training:	Epoch: [32][753/817]	Loss 0.0081 (0.0408)	
training:	Epoch: [32][754/817]	Loss 0.0075 (0.0408)	
training:	Epoch: [32][755/817]	Loss 0.0072 (0.0407)	
training:	Epoch: [32][756/817]	Loss 0.0055 (0.0407)	
training:	Epoch: [32][757/817]	Loss 0.0104 (0.0407)	
training:	Epoch: [32][758/817]	Loss 0.0085 (0.0406)	
training:	Epoch: [32][759/817]	Loss 0.0078 (0.0406)	
training:	Epoch: [32][760/817]	Loss 0.0086 (0.0405)	
training:	Epoch: [32][761/817]	Loss 0.0116 (0.0405)	
training:	Epoch: [32][762/817]	Loss 0.5799 (0.0412)	
training:	Epoch: [32][763/817]	Loss 0.0079 (0.0411)	
training:	Epoch: [32][764/817]	Loss 0.0061 (0.0411)	
training:	Epoch: [32][765/817]	Loss 0.6364 (0.0419)	
training:	Epoch: [32][766/817]	Loss 0.0069 (0.0418)	
training:	Epoch: [32][767/817]	Loss 0.0075 (0.0418)	
training:	Epoch: [32][768/817]	Loss 0.0066 (0.0417)	
training:	Epoch: [32][769/817]	Loss 0.0071 (0.0417)	
training:	Epoch: [32][770/817]	Loss 0.0050 (0.0417)	
training:	Epoch: [32][771/817]	Loss 0.0093 (0.0416)	
training:	Epoch: [32][772/817]	Loss 0.0077 (0.0416)	
training:	Epoch: [32][773/817]	Loss 0.0064 (0.0415)	
training:	Epoch: [32][774/817]	Loss 0.0182 (0.0415)	
training:	Epoch: [32][775/817]	Loss 0.0083 (0.0414)	
training:	Epoch: [32][776/817]	Loss 0.0362 (0.0414)	
training:	Epoch: [32][777/817]	Loss 0.0086 (0.0414)	
training:	Epoch: [32][778/817]	Loss 0.0061 (0.0414)	
training:	Epoch: [32][779/817]	Loss 0.0071 (0.0413)	
training:	Epoch: [32][780/817]	Loss 0.0076 (0.0413)	
training:	Epoch: [32][781/817]	Loss 0.0089 (0.0412)	
training:	Epoch: [32][782/817]	Loss 0.0084 (0.0412)	
training:	Epoch: [32][783/817]	Loss 0.0081 (0.0411)	
training:	Epoch: [32][784/817]	Loss 0.1073 (0.0412)	
training:	Epoch: [32][785/817]	Loss 0.0741 (0.0413)	
training:	Epoch: [32][786/817]	Loss 0.0081 (0.0412)	
training:	Epoch: [32][787/817]	Loss 0.0068 (0.0412)	
training:	Epoch: [32][788/817]	Loss 0.0091 (0.0411)	
training:	Epoch: [32][789/817]	Loss 0.0093 (0.0411)	
training:	Epoch: [32][790/817]	Loss 0.3527 (0.0415)	
training:	Epoch: [32][791/817]	Loss 0.0071 (0.0415)	
training:	Epoch: [32][792/817]	Loss 0.0074 (0.0414)	
training:	Epoch: [32][793/817]	Loss 0.1306 (0.0415)	
training:	Epoch: [32][794/817]	Loss 0.0089 (0.0415)	
training:	Epoch: [32][795/817]	Loss 0.0083 (0.0414)	
training:	Epoch: [32][796/817]	Loss 0.0098 (0.0414)	
training:	Epoch: [32][797/817]	Loss 0.0078 (0.0414)	
training:	Epoch: [32][798/817]	Loss 0.0077 (0.0413)	
training:	Epoch: [32][799/817]	Loss 0.0090 (0.0413)	
training:	Epoch: [32][800/817]	Loss 0.0074 (0.0412)	
training:	Epoch: [32][801/817]	Loss 0.0247 (0.0412)	
training:	Epoch: [32][802/817]	Loss 0.0067 (0.0412)	
training:	Epoch: [32][803/817]	Loss 0.0110 (0.0411)	
training:	Epoch: [32][804/817]	Loss 0.0139 (0.0411)	
training:	Epoch: [32][805/817]	Loss 0.0088 (0.0411)	
training:	Epoch: [32][806/817]	Loss 0.0087 (0.0410)	
training:	Epoch: [32][807/817]	Loss 0.0100 (0.0410)	
training:	Epoch: [32][808/817]	Loss 0.0094 (0.0409)	
training:	Epoch: [32][809/817]	Loss 0.0090 (0.0409)	
training:	Epoch: [32][810/817]	Loss 0.0738 (0.0409)	
training:	Epoch: [32][811/817]	Loss 0.0058 (0.0409)	
training:	Epoch: [32][812/817]	Loss 0.0065 (0.0409)	
training:	Epoch: [32][813/817]	Loss 0.0104 (0.0408)	
training:	Epoch: [32][814/817]	Loss 0.0089 (0.0408)	
training:	Epoch: [32][815/817]	Loss 0.0079 (0.0407)	
training:	Epoch: [32][816/817]	Loss 0.0043 (0.0407)	
training:	Epoch: [32][817/817]	Loss 0.0078 (0.0407)	
Training:	 Loss: 0.0406

Training:	 ACC: 0.9916 0.9917 0.9944 0.9888
Validation:	 ACC: 0.7808 0.7838 0.8485 0.7130
Validation:	 Best_BACC: 0.8223 0.8218 0.8127 0.8318
Validation:	 Loss: 0.9543
Pretraining:	Epoch 33/200
----------
training:	Epoch: [33][1/817]	Loss 0.0094 (0.0094)	
training:	Epoch: [33][2/817]	Loss 0.0075 (0.0085)	
training:	Epoch: [33][3/817]	Loss 0.0126 (0.0099)	
training:	Epoch: [33][4/817]	Loss 0.0076 (0.0093)	
training:	Epoch: [33][5/817]	Loss 0.0068 (0.0088)	
training:	Epoch: [33][6/817]	Loss 0.0213 (0.0109)	
training:	Epoch: [33][7/817]	Loss 0.5562 (0.0888)	
training:	Epoch: [33][8/817]	Loss 0.0110 (0.0791)	
training:	Epoch: [33][9/817]	Loss 0.0059 (0.0709)	
training:	Epoch: [33][10/817]	Loss 0.0074 (0.0646)	
training:	Epoch: [33][11/817]	Loss 0.0057 (0.0592)	
training:	Epoch: [33][12/817]	Loss 0.0068 (0.0549)	
training:	Epoch: [33][13/817]	Loss 0.0064 (0.0511)	
training:	Epoch: [33][14/817]	Loss 0.0060 (0.0479)	
training:	Epoch: [33][15/817]	Loss 0.0074 (0.0452)	
training:	Epoch: [33][16/817]	Loss 0.0083 (0.0429)	
training:	Epoch: [33][17/817]	Loss 0.0064 (0.0407)	
training:	Epoch: [33][18/817]	Loss 0.0085 (0.0390)	
training:	Epoch: [33][19/817]	Loss 0.0064 (0.0372)	
training:	Epoch: [33][20/817]	Loss 0.0065 (0.0357)	
training:	Epoch: [33][21/817]	Loss 0.0062 (0.0343)	
training:	Epoch: [33][22/817]	Loss 0.0067 (0.0330)	
training:	Epoch: [33][23/817]	Loss 0.0068 (0.0319)	
training:	Epoch: [33][24/817]	Loss 0.0062 (0.0308)	
training:	Epoch: [33][25/817]	Loss 0.0079 (0.0299)	
training:	Epoch: [33][26/817]	Loss 0.0089 (0.0291)	
training:	Epoch: [33][27/817]	Loss 0.5540 (0.0486)	
training:	Epoch: [33][28/817]	Loss 0.0075 (0.0471)	
training:	Epoch: [33][29/817]	Loss 0.0057 (0.0457)	
training:	Epoch: [33][30/817]	Loss 0.0076 (0.0444)	
training:	Epoch: [33][31/817]	Loss 0.0053 (0.0431)	
training:	Epoch: [33][32/817]	Loss 0.0058 (0.0420)	
training:	Epoch: [33][33/817]	Loss 0.0070 (0.0409)	
training:	Epoch: [33][34/817]	Loss 0.0077 (0.0399)	
training:	Epoch: [33][35/817]	Loss 0.0065 (0.0390)	
training:	Epoch: [33][36/817]	Loss 0.0063 (0.0381)	
training:	Epoch: [33][37/817]	Loss 0.0069 (0.0372)	
training:	Epoch: [33][38/817]	Loss 0.0060 (0.0364)	
training:	Epoch: [33][39/817]	Loss 0.0063 (0.0356)	
training:	Epoch: [33][40/817]	Loss 0.0067 (0.0349)	
training:	Epoch: [33][41/817]	Loss 0.0062 (0.0342)	
training:	Epoch: [33][42/817]	Loss 0.4861 (0.0450)	
training:	Epoch: [33][43/817]	Loss 0.0070 (0.0441)	
training:	Epoch: [33][44/817]	Loss 0.5075 (0.0546)	
training:	Epoch: [33][45/817]	Loss 0.0069 (0.0535)	
training:	Epoch: [33][46/817]	Loss 0.0059 (0.0525)	
training:	Epoch: [33][47/817]	Loss 0.0077 (0.0516)	
training:	Epoch: [33][48/817]	Loss 0.0074 (0.0506)	
training:	Epoch: [33][49/817]	Loss 0.0072 (0.0498)	
training:	Epoch: [33][50/817]	Loss 0.0068 (0.0489)	
training:	Epoch: [33][51/817]	Loss 0.0074 (0.0481)	
training:	Epoch: [33][52/817]	Loss 0.5389 (0.0575)	
training:	Epoch: [33][53/817]	Loss 0.0079 (0.0566)	
training:	Epoch: [33][54/817]	Loss 0.0067 (0.0557)	
training:	Epoch: [33][55/817]	Loss 0.0067 (0.0548)	
training:	Epoch: [33][56/817]	Loss 0.0143 (0.0541)	
training:	Epoch: [33][57/817]	Loss 0.0083 (0.0532)	
training:	Epoch: [33][58/817]	Loss 0.0083 (0.0525)	
training:	Epoch: [33][59/817]	Loss 0.0059 (0.0517)	
training:	Epoch: [33][60/817]	Loss 0.0237 (0.0512)	
training:	Epoch: [33][61/817]	Loss 0.0079 (0.0505)	
training:	Epoch: [33][62/817]	Loss 0.5852 (0.0591)	
training:	Epoch: [33][63/817]	Loss 0.0070 (0.0583)	
training:	Epoch: [33][64/817]	Loss 0.0078 (0.0575)	
training:	Epoch: [33][65/817]	Loss 0.5544 (0.0652)	
training:	Epoch: [33][66/817]	Loss 0.0098 (0.0643)	
training:	Epoch: [33][67/817]	Loss 0.0084 (0.0635)	
training:	Epoch: [33][68/817]	Loss 0.0174 (0.0628)	
training:	Epoch: [33][69/817]	Loss 0.0084 (0.0620)	
training:	Epoch: [33][70/817]	Loss 0.0114 (0.0613)	
training:	Epoch: [33][71/817]	Loss 0.0100 (0.0606)	
training:	Epoch: [33][72/817]	Loss 0.0069 (0.0598)	
training:	Epoch: [33][73/817]	Loss 0.0073 (0.0591)	
training:	Epoch: [33][74/817]	Loss 0.0075 (0.0584)	
training:	Epoch: [33][75/817]	Loss 0.0080 (0.0577)	
training:	Epoch: [33][76/817]	Loss 0.0131 (0.0572)	
training:	Epoch: [33][77/817]	Loss 0.0086 (0.0565)	
training:	Epoch: [33][78/817]	Loss 0.0108 (0.0559)	
training:	Epoch: [33][79/817]	Loss 0.0057 (0.0553)	
training:	Epoch: [33][80/817]	Loss 0.0100 (0.0547)	
training:	Epoch: [33][81/817]	Loss 0.0073 (0.0541)	
training:	Epoch: [33][82/817]	Loss 0.0079 (0.0536)	
training:	Epoch: [33][83/817]	Loss 0.0117 (0.0531)	
training:	Epoch: [33][84/817]	Loss 0.0080 (0.0525)	
training:	Epoch: [33][85/817]	Loss 0.0093 (0.0520)	
training:	Epoch: [33][86/817]	Loss 0.0076 (0.0515)	
training:	Epoch: [33][87/817]	Loss 0.0101 (0.0510)	
training:	Epoch: [33][88/817]	Loss 0.0089 (0.0506)	
training:	Epoch: [33][89/817]	Loss 0.0087 (0.0501)	
training:	Epoch: [33][90/817]	Loss 0.0074 (0.0496)	
training:	Epoch: [33][91/817]	Loss 0.0061 (0.0491)	
training:	Epoch: [33][92/817]	Loss 0.0064 (0.0487)	
training:	Epoch: [33][93/817]	Loss 0.0076 (0.0482)	
training:	Epoch: [33][94/817]	Loss 0.4856 (0.0529)	
training:	Epoch: [33][95/817]	Loss 0.0081 (0.0524)	
training:	Epoch: [33][96/817]	Loss 0.0113 (0.0520)	
training:	Epoch: [33][97/817]	Loss 0.0084 (0.0515)	
training:	Epoch: [33][98/817]	Loss 0.0067 (0.0511)	
training:	Epoch: [33][99/817]	Loss 0.0079 (0.0506)	
training:	Epoch: [33][100/817]	Loss 0.0078 (0.0502)	
training:	Epoch: [33][101/817]	Loss 0.0090 (0.0498)	
training:	Epoch: [33][102/817]	Loss 0.0109 (0.0494)	
training:	Epoch: [33][103/817]	Loss 0.5718 (0.0545)	
training:	Epoch: [33][104/817]	Loss 0.0084 (0.0541)	
training:	Epoch: [33][105/817]	Loss 0.0082 (0.0536)	
training:	Epoch: [33][106/817]	Loss 0.0081 (0.0532)	
training:	Epoch: [33][107/817]	Loss 0.0098 (0.0528)	
training:	Epoch: [33][108/817]	Loss 0.0069 (0.0524)	
training:	Epoch: [33][109/817]	Loss 0.0066 (0.0519)	
training:	Epoch: [33][110/817]	Loss 0.0091 (0.0515)	
training:	Epoch: [33][111/817]	Loss 0.0064 (0.0511)	
training:	Epoch: [33][112/817]	Loss 0.0126 (0.0508)	
training:	Epoch: [33][113/817]	Loss 0.0078 (0.0504)	
training:	Epoch: [33][114/817]	Loss 0.0074 (0.0500)	
training:	Epoch: [33][115/817]	Loss 0.0063 (0.0497)	
training:	Epoch: [33][116/817]	Loss 0.0085 (0.0493)	
training:	Epoch: [33][117/817]	Loss 0.6242 (0.0542)	
training:	Epoch: [33][118/817]	Loss 0.0087 (0.0538)	
training:	Epoch: [33][119/817]	Loss 0.0079 (0.0534)	
training:	Epoch: [33][120/817]	Loss 0.0068 (0.0531)	
training:	Epoch: [33][121/817]	Loss 0.0082 (0.0527)	
training:	Epoch: [33][122/817]	Loss 0.0076 (0.0523)	
training:	Epoch: [33][123/817]	Loss 0.0083 (0.0520)	
training:	Epoch: [33][124/817]	Loss 0.0084 (0.0516)	
training:	Epoch: [33][125/817]	Loss 0.0098 (0.0513)	
training:	Epoch: [33][126/817]	Loss 0.0071 (0.0509)	
training:	Epoch: [33][127/817]	Loss 0.0090 (0.0506)	
training:	Epoch: [33][128/817]	Loss 0.0057 (0.0502)	
training:	Epoch: [33][129/817]	Loss 0.0095 (0.0499)	
training:	Epoch: [33][130/817]	Loss 0.0118 (0.0496)	
training:	Epoch: [33][131/817]	Loss 0.0085 (0.0493)	
training:	Epoch: [33][132/817]	Loss 0.0079 (0.0490)	
training:	Epoch: [33][133/817]	Loss 0.0119 (0.0487)	
training:	Epoch: [33][134/817]	Loss 0.0070 (0.0484)	
training:	Epoch: [33][135/817]	Loss 0.0059 (0.0481)	
training:	Epoch: [33][136/817]	Loss 0.0053 (0.0478)	
training:	Epoch: [33][137/817]	Loss 0.0092 (0.0475)	
training:	Epoch: [33][138/817]	Loss 0.0069 (0.0472)	
training:	Epoch: [33][139/817]	Loss 0.0083 (0.0469)	
training:	Epoch: [33][140/817]	Loss 0.0076 (0.0467)	
training:	Epoch: [33][141/817]	Loss 0.0097 (0.0464)	
training:	Epoch: [33][142/817]	Loss 0.0087 (0.0461)	
training:	Epoch: [33][143/817]	Loss 0.0113 (0.0459)	
training:	Epoch: [33][144/817]	Loss 0.0076 (0.0456)	
training:	Epoch: [33][145/817]	Loss 0.0084 (0.0454)	
training:	Epoch: [33][146/817]	Loss 0.0069 (0.0451)	
training:	Epoch: [33][147/817]	Loss 0.0074 (0.0448)	
training:	Epoch: [33][148/817]	Loss 0.0565 (0.0449)	
training:	Epoch: [33][149/817]	Loss 0.0075 (0.0447)	
training:	Epoch: [33][150/817]	Loss 0.0081 (0.0444)	
training:	Epoch: [33][151/817]	Loss 0.0081 (0.0442)	
training:	Epoch: [33][152/817]	Loss 0.0057 (0.0439)	
training:	Epoch: [33][153/817]	Loss 0.0096 (0.0437)	
training:	Epoch: [33][154/817]	Loss 0.0075 (0.0435)	
training:	Epoch: [33][155/817]	Loss 0.0075 (0.0432)	
training:	Epoch: [33][156/817]	Loss 0.0075 (0.0430)	
training:	Epoch: [33][157/817]	Loss 0.0072 (0.0428)	
training:	Epoch: [33][158/817]	Loss 0.0205 (0.0426)	
training:	Epoch: [33][159/817]	Loss 0.6336 (0.0464)	
training:	Epoch: [33][160/817]	Loss 0.0096 (0.0461)	
training:	Epoch: [33][161/817]	Loss 0.0064 (0.0459)	
training:	Epoch: [33][162/817]	Loss 0.0354 (0.0458)	
training:	Epoch: [33][163/817]	Loss 0.0079 (0.0456)	
training:	Epoch: [33][164/817]	Loss 0.0090 (0.0454)	
training:	Epoch: [33][165/817]	Loss 0.0069 (0.0451)	
training:	Epoch: [33][166/817]	Loss 0.0093 (0.0449)	
training:	Epoch: [33][167/817]	Loss 0.0080 (0.0447)	
training:	Epoch: [33][168/817]	Loss 0.0062 (0.0445)	
training:	Epoch: [33][169/817]	Loss 0.0082 (0.0442)	
training:	Epoch: [33][170/817]	Loss 0.0196 (0.0441)	
training:	Epoch: [33][171/817]	Loss 0.0063 (0.0439)	
training:	Epoch: [33][172/817]	Loss 0.0098 (0.0437)	
training:	Epoch: [33][173/817]	Loss 0.0085 (0.0435)	
training:	Epoch: [33][174/817]	Loss 0.0097 (0.0433)	
training:	Epoch: [33][175/817]	Loss 0.4657 (0.0457)	
training:	Epoch: [33][176/817]	Loss 0.0097 (0.0455)	
training:	Epoch: [33][177/817]	Loss 0.0052 (0.0453)	
training:	Epoch: [33][178/817]	Loss 0.0073 (0.0451)	
training:	Epoch: [33][179/817]	Loss 0.0075 (0.0448)	
training:	Epoch: [33][180/817]	Loss 0.0069 (0.0446)	
training:	Epoch: [33][181/817]	Loss 0.0062 (0.0444)	
training:	Epoch: [33][182/817]	Loss 0.0070 (0.0442)	
training:	Epoch: [33][183/817]	Loss 0.0067 (0.0440)	
training:	Epoch: [33][184/817]	Loss 0.0088 (0.0438)	
training:	Epoch: [33][185/817]	Loss 0.0090 (0.0436)	
training:	Epoch: [33][186/817]	Loss 0.3930 (0.0455)	
training:	Epoch: [33][187/817]	Loss 0.0074 (0.0453)	
training:	Epoch: [33][188/817]	Loss 0.0069 (0.0451)	
training:	Epoch: [33][189/817]	Loss 0.0083 (0.0449)	
training:	Epoch: [33][190/817]	Loss 0.0092 (0.0447)	
training:	Epoch: [33][191/817]	Loss 0.0088 (0.0445)	
training:	Epoch: [33][192/817]	Loss 0.0114 (0.0444)	
training:	Epoch: [33][193/817]	Loss 0.0107 (0.0442)	
training:	Epoch: [33][194/817]	Loss 0.0080 (0.0440)	
training:	Epoch: [33][195/817]	Loss 0.0070 (0.0438)	
training:	Epoch: [33][196/817]	Loss 0.0138 (0.0437)	
training:	Epoch: [33][197/817]	Loss 0.0074 (0.0435)	
training:	Epoch: [33][198/817]	Loss 0.0080 (0.0433)	
training:	Epoch: [33][199/817]	Loss 0.0121 (0.0431)	
training:	Epoch: [33][200/817]	Loss 0.0069 (0.0430)	
training:	Epoch: [33][201/817]	Loss 0.0085 (0.0428)	
training:	Epoch: [33][202/817]	Loss 0.0081 (0.0426)	
training:	Epoch: [33][203/817]	Loss 0.0084 (0.0424)	
training:	Epoch: [33][204/817]	Loss 0.0085 (0.0423)	
training:	Epoch: [33][205/817]	Loss 0.0086 (0.0421)	
training:	Epoch: [33][206/817]	Loss 0.0077 (0.0419)	
training:	Epoch: [33][207/817]	Loss 0.0101 (0.0418)	
training:	Epoch: [33][208/817]	Loss 0.5983 (0.0445)	
training:	Epoch: [33][209/817]	Loss 0.0129 (0.0443)	
training:	Epoch: [33][210/817]	Loss 0.0069 (0.0441)	
training:	Epoch: [33][211/817]	Loss 0.0073 (0.0440)	
training:	Epoch: [33][212/817]	Loss 0.0063 (0.0438)	
training:	Epoch: [33][213/817]	Loss 0.0064 (0.0436)	
training:	Epoch: [33][214/817]	Loss 0.0063 (0.0434)	
training:	Epoch: [33][215/817]	Loss 0.0052 (0.0433)	
training:	Epoch: [33][216/817]	Loss 0.0110 (0.0431)	
training:	Epoch: [33][217/817]	Loss 0.0065 (0.0429)	
training:	Epoch: [33][218/817]	Loss 0.0100 (0.0428)	
training:	Epoch: [33][219/817]	Loss 0.0070 (0.0426)	
training:	Epoch: [33][220/817]	Loss 0.0064 (0.0425)	
training:	Epoch: [33][221/817]	Loss 0.0101 (0.0423)	
training:	Epoch: [33][222/817]	Loss 0.0061 (0.0422)	
training:	Epoch: [33][223/817]	Loss 0.0122 (0.0420)	
training:	Epoch: [33][224/817]	Loss 0.0112 (0.0419)	
training:	Epoch: [33][225/817]	Loss 0.0084 (0.0417)	
training:	Epoch: [33][226/817]	Loss 0.0080 (0.0416)	
training:	Epoch: [33][227/817]	Loss 0.0077 (0.0414)	
training:	Epoch: [33][228/817]	Loss 0.0082 (0.0413)	
training:	Epoch: [33][229/817]	Loss 0.0073 (0.0411)	
training:	Epoch: [33][230/817]	Loss 0.0068 (0.0410)	
training:	Epoch: [33][231/817]	Loss 0.0103 (0.0409)	
training:	Epoch: [33][232/817]	Loss 0.5547 (0.0431)	
training:	Epoch: [33][233/817]	Loss 0.0069 (0.0429)	
training:	Epoch: [33][234/817]	Loss 0.0090 (0.0428)	
training:	Epoch: [33][235/817]	Loss 0.0057 (0.0426)	
training:	Epoch: [33][236/817]	Loss 0.0075 (0.0425)	
training:	Epoch: [33][237/817]	Loss 0.0064 (0.0423)	
training:	Epoch: [33][238/817]	Loss 0.0052 (0.0422)	
training:	Epoch: [33][239/817]	Loss 0.0161 (0.0420)	
training:	Epoch: [33][240/817]	Loss 0.0113 (0.0419)	
training:	Epoch: [33][241/817]	Loss 0.0091 (0.0418)	
training:	Epoch: [33][242/817]	Loss 0.3594 (0.0431)	
training:	Epoch: [33][243/817]	Loss 0.0056 (0.0429)	
training:	Epoch: [33][244/817]	Loss 0.0094 (0.0428)	
training:	Epoch: [33][245/817]	Loss 0.0108 (0.0427)	
training:	Epoch: [33][246/817]	Loss 0.0075 (0.0425)	
training:	Epoch: [33][247/817]	Loss 0.0090 (0.0424)	
training:	Epoch: [33][248/817]	Loss 0.0070 (0.0422)	
training:	Epoch: [33][249/817]	Loss 0.0118 (0.0421)	
training:	Epoch: [33][250/817]	Loss 0.5348 (0.0441)	
training:	Epoch: [33][251/817]	Loss 0.0735 (0.0442)	
training:	Epoch: [33][252/817]	Loss 0.0079 (0.0441)	
training:	Epoch: [33][253/817]	Loss 0.0049 (0.0439)	
training:	Epoch: [33][254/817]	Loss 0.0079 (0.0438)	
training:	Epoch: [33][255/817]	Loss 0.0069 (0.0436)	
training:	Epoch: [33][256/817]	Loss 0.0102 (0.0435)	
training:	Epoch: [33][257/817]	Loss 0.0089 (0.0434)	
training:	Epoch: [33][258/817]	Loss 0.0113 (0.0432)	
training:	Epoch: [33][259/817]	Loss 0.0096 (0.0431)	
training:	Epoch: [33][260/817]	Loss 0.0109 (0.0430)	
training:	Epoch: [33][261/817]	Loss 0.0087 (0.0429)	
training:	Epoch: [33][262/817]	Loss 0.0089 (0.0427)	
training:	Epoch: [33][263/817]	Loss 0.0120 (0.0426)	
training:	Epoch: [33][264/817]	Loss 0.0085 (0.0425)	
training:	Epoch: [33][265/817]	Loss 0.0049 (0.0423)	
training:	Epoch: [33][266/817]	Loss 0.0072 (0.0422)	
training:	Epoch: [33][267/817]	Loss 0.0076 (0.0421)	
training:	Epoch: [33][268/817]	Loss 0.0120 (0.0420)	
training:	Epoch: [33][269/817]	Loss 0.0104 (0.0418)	
training:	Epoch: [33][270/817]	Loss 0.0065 (0.0417)	
training:	Epoch: [33][271/817]	Loss 0.0051 (0.0416)	
training:	Epoch: [33][272/817]	Loss 0.0056 (0.0414)	
training:	Epoch: [33][273/817]	Loss 0.0080 (0.0413)	
training:	Epoch: [33][274/817]	Loss 0.0110 (0.0412)	
training:	Epoch: [33][275/817]	Loss 0.0080 (0.0411)	
training:	Epoch: [33][276/817]	Loss 0.0060 (0.0410)	
training:	Epoch: [33][277/817]	Loss 0.0110 (0.0409)	
training:	Epoch: [33][278/817]	Loss 0.0076 (0.0407)	
training:	Epoch: [33][279/817]	Loss 0.0080 (0.0406)	
training:	Epoch: [33][280/817]	Loss 0.0115 (0.0405)	
training:	Epoch: [33][281/817]	Loss 0.0078 (0.0404)	
training:	Epoch: [33][282/817]	Loss 0.0100 (0.0403)	
training:	Epoch: [33][283/817]	Loss 0.0085 (0.0402)	
training:	Epoch: [33][284/817]	Loss 0.0058 (0.0401)	
training:	Epoch: [33][285/817]	Loss 0.0063 (0.0399)	
training:	Epoch: [33][286/817]	Loss 0.0057 (0.0398)	
training:	Epoch: [33][287/817]	Loss 0.0067 (0.0397)	
training:	Epoch: [33][288/817]	Loss 0.0045 (0.0396)	
training:	Epoch: [33][289/817]	Loss 0.0082 (0.0395)	
training:	Epoch: [33][290/817]	Loss 0.0065 (0.0394)	
training:	Epoch: [33][291/817]	Loss 0.0068 (0.0392)	
training:	Epoch: [33][292/817]	Loss 0.0099 (0.0391)	
training:	Epoch: [33][293/817]	Loss 0.0108 (0.0391)	
training:	Epoch: [33][294/817]	Loss 0.0119 (0.0390)	
training:	Epoch: [33][295/817]	Loss 0.0073 (0.0389)	
training:	Epoch: [33][296/817]	Loss 0.0077 (0.0387)	
training:	Epoch: [33][297/817]	Loss 0.0093 (0.0386)	
training:	Epoch: [33][298/817]	Loss 0.0159 (0.0386)	
training:	Epoch: [33][299/817]	Loss 0.0105 (0.0385)	
training:	Epoch: [33][300/817]	Loss 0.0079 (0.0384)	
training:	Epoch: [33][301/817]	Loss 0.0077 (0.0383)	
training:	Epoch: [33][302/817]	Loss 0.0062 (0.0382)	
training:	Epoch: [33][303/817]	Loss 0.0074 (0.0381)	
training:	Epoch: [33][304/817]	Loss 0.0087 (0.0380)	
training:	Epoch: [33][305/817]	Loss 0.0087 (0.0379)	
training:	Epoch: [33][306/817]	Loss 0.0082 (0.0378)	
training:	Epoch: [33][307/817]	Loss 0.0069 (0.0377)	
training:	Epoch: [33][308/817]	Loss 0.0128 (0.0376)	
training:	Epoch: [33][309/817]	Loss 0.0101 (0.0375)	
training:	Epoch: [33][310/817]	Loss 0.0063 (0.0374)	
training:	Epoch: [33][311/817]	Loss 0.0098 (0.0373)	
training:	Epoch: [33][312/817]	Loss 0.0068 (0.0372)	
training:	Epoch: [33][313/817]	Loss 0.0050 (0.0371)	
training:	Epoch: [33][314/817]	Loss 0.0064 (0.0370)	
training:	Epoch: [33][315/817]	Loss 0.0080 (0.0369)	
training:	Epoch: [33][316/817]	Loss 0.5971 (0.0387)	
training:	Epoch: [33][317/817]	Loss 0.0080 (0.0386)	
training:	Epoch: [33][318/817]	Loss 0.0064 (0.0385)	
training:	Epoch: [33][319/817]	Loss 0.0058 (0.0384)	
training:	Epoch: [33][320/817]	Loss 0.0063 (0.0383)	
training:	Epoch: [33][321/817]	Loss 0.0073 (0.0382)	
training:	Epoch: [33][322/817]	Loss 0.0050 (0.0381)	
training:	Epoch: [33][323/817]	Loss 0.0056 (0.0380)	
training:	Epoch: [33][324/817]	Loss 0.0061 (0.0379)	
training:	Epoch: [33][325/817]	Loss 0.0064 (0.0378)	
training:	Epoch: [33][326/817]	Loss 0.0059 (0.0377)	
training:	Epoch: [33][327/817]	Loss 0.0068 (0.0376)	
training:	Epoch: [33][328/817]	Loss 0.0075 (0.0375)	
training:	Epoch: [33][329/817]	Loss 0.0061 (0.0374)	
training:	Epoch: [33][330/817]	Loss 0.0076 (0.0373)	
training:	Epoch: [33][331/817]	Loss 0.0066 (0.0372)	
training:	Epoch: [33][332/817]	Loss 0.0055 (0.0371)	
training:	Epoch: [33][333/817]	Loss 0.0061 (0.0371)	
training:	Epoch: [33][334/817]	Loss 0.0058 (0.0370)	
training:	Epoch: [33][335/817]	Loss 0.0079 (0.0369)	
training:	Epoch: [33][336/817]	Loss 0.0060 (0.0368)	
training:	Epoch: [33][337/817]	Loss 0.0066 (0.0367)	
training:	Epoch: [33][338/817]	Loss 0.0163 (0.0366)	
training:	Epoch: [33][339/817]	Loss 0.0076 (0.0365)	
training:	Epoch: [33][340/817]	Loss 0.0087 (0.0365)	
training:	Epoch: [33][341/817]	Loss 0.0087 (0.0364)	
training:	Epoch: [33][342/817]	Loss 0.0352 (0.0364)	
training:	Epoch: [33][343/817]	Loss 0.0294 (0.0364)	
training:	Epoch: [33][344/817]	Loss 0.0069 (0.0363)	
training:	Epoch: [33][345/817]	Loss 0.0070 (0.0362)	
training:	Epoch: [33][346/817]	Loss 0.5491 (0.0377)	
training:	Epoch: [33][347/817]	Loss 0.0085 (0.0376)	
training:	Epoch: [33][348/817]	Loss 0.0087 (0.0375)	
training:	Epoch: [33][349/817]	Loss 0.0056 (0.0374)	
training:	Epoch: [33][350/817]	Loss 0.0062 (0.0373)	
training:	Epoch: [33][351/817]	Loss 0.0066 (0.0372)	
training:	Epoch: [33][352/817]	Loss 0.0055 (0.0371)	
training:	Epoch: [33][353/817]	Loss 0.0052 (0.0371)	
training:	Epoch: [33][354/817]	Loss 0.0062 (0.0370)	
training:	Epoch: [33][355/817]	Loss 0.0141 (0.0369)	
training:	Epoch: [33][356/817]	Loss 0.0057 (0.0368)	
training:	Epoch: [33][357/817]	Loss 0.0065 (0.0367)	
training:	Epoch: [33][358/817]	Loss 0.0070 (0.0366)	
training:	Epoch: [33][359/817]	Loss 0.0067 (0.0366)	
training:	Epoch: [33][360/817]	Loss 0.0102 (0.0365)	
training:	Epoch: [33][361/817]	Loss 0.0067 (0.0364)	
training:	Epoch: [33][362/817]	Loss 0.0063 (0.0363)	
training:	Epoch: [33][363/817]	Loss 0.0095 (0.0362)	
training:	Epoch: [33][364/817]	Loss 0.0051 (0.0362)	
training:	Epoch: [33][365/817]	Loss 0.0075 (0.0361)	
training:	Epoch: [33][366/817]	Loss 0.0075 (0.0360)	
training:	Epoch: [33][367/817]	Loss 0.0050 (0.0359)	
training:	Epoch: [33][368/817]	Loss 0.0093 (0.0358)	
training:	Epoch: [33][369/817]	Loss 0.0104 (0.0358)	
training:	Epoch: [33][370/817]	Loss 0.0107 (0.0357)	
training:	Epoch: [33][371/817]	Loss 0.0064 (0.0356)	
training:	Epoch: [33][372/817]	Loss 0.0063 (0.0356)	
training:	Epoch: [33][373/817]	Loss 0.0076 (0.0355)	
training:	Epoch: [33][374/817]	Loss 0.0055 (0.0354)	
training:	Epoch: [33][375/817]	Loss 0.0073 (0.0353)	
training:	Epoch: [33][376/817]	Loss 0.0053 (0.0352)	
training:	Epoch: [33][377/817]	Loss 0.0067 (0.0352)	
training:	Epoch: [33][378/817]	Loss 0.0079 (0.0351)	
training:	Epoch: [33][379/817]	Loss 0.0111 (0.0350)	
training:	Epoch: [33][380/817]	Loss 0.0142 (0.0350)	
training:	Epoch: [33][381/817]	Loss 0.0059 (0.0349)	
training:	Epoch: [33][382/817]	Loss 0.0070 (0.0348)	
training:	Epoch: [33][383/817]	Loss 0.0591 (0.0349)	
training:	Epoch: [33][384/817]	Loss 0.0061 (0.0348)	
training:	Epoch: [33][385/817]	Loss 0.0064 (0.0347)	
training:	Epoch: [33][386/817]	Loss 0.0090 (0.0347)	
training:	Epoch: [33][387/817]	Loss 0.0061 (0.0346)	
training:	Epoch: [33][388/817]	Loss 0.0059 (0.0345)	
training:	Epoch: [33][389/817]	Loss 0.6378 (0.0361)	
training:	Epoch: [33][390/817]	Loss 0.0059 (0.0360)	
training:	Epoch: [33][391/817]	Loss 0.0086 (0.0359)	
training:	Epoch: [33][392/817]	Loss 0.0063 (0.0359)	
training:	Epoch: [33][393/817]	Loss 0.0057 (0.0358)	
training:	Epoch: [33][394/817]	Loss 0.0069 (0.0357)	
training:	Epoch: [33][395/817]	Loss 0.0086 (0.0356)	
training:	Epoch: [33][396/817]	Loss 0.0065 (0.0356)	
training:	Epoch: [33][397/817]	Loss 0.0076 (0.0355)	
training:	Epoch: [33][398/817]	Loss 0.0060 (0.0354)	
training:	Epoch: [33][399/817]	Loss 0.0058 (0.0353)	
training:	Epoch: [33][400/817]	Loss 0.0071 (0.0353)	
training:	Epoch: [33][401/817]	Loss 0.0072 (0.0352)	
training:	Epoch: [33][402/817]	Loss 0.0071 (0.0351)	
training:	Epoch: [33][403/817]	Loss 0.0087 (0.0351)	
training:	Epoch: [33][404/817]	Loss 0.0088 (0.0350)	
training:	Epoch: [33][405/817]	Loss 0.0082 (0.0349)	
training:	Epoch: [33][406/817]	Loss 0.0073 (0.0349)	
training:	Epoch: [33][407/817]	Loss 0.0076 (0.0348)	
training:	Epoch: [33][408/817]	Loss 0.0082 (0.0347)	
training:	Epoch: [33][409/817]	Loss 0.0057 (0.0347)	
training:	Epoch: [33][410/817]	Loss 0.0081 (0.0346)	
training:	Epoch: [33][411/817]	Loss 0.0307 (0.0346)	
training:	Epoch: [33][412/817]	Loss 0.0065 (0.0345)	
training:	Epoch: [33][413/817]	Loss 0.0076 (0.0345)	
training:	Epoch: [33][414/817]	Loss 0.0069 (0.0344)	
training:	Epoch: [33][415/817]	Loss 0.0062 (0.0343)	
training:	Epoch: [33][416/817]	Loss 0.0070 (0.0343)	
training:	Epoch: [33][417/817]	Loss 0.0074 (0.0342)	
training:	Epoch: [33][418/817]	Loss 0.0061 (0.0341)	
training:	Epoch: [33][419/817]	Loss 0.0057 (0.0341)	
training:	Epoch: [33][420/817]	Loss 0.0073 (0.0340)	
training:	Epoch: [33][421/817]	Loss 0.6768 (0.0355)	
training:	Epoch: [33][422/817]	Loss 0.0062 (0.0355)	
training:	Epoch: [33][423/817]	Loss 0.0059 (0.0354)	
training:	Epoch: [33][424/817]	Loss 0.0073 (0.0353)	
training:	Epoch: [33][425/817]	Loss 0.0093 (0.0353)	
training:	Epoch: [33][426/817]	Loss 0.0059 (0.0352)	
training:	Epoch: [33][427/817]	Loss 0.0066 (0.0351)	
training:	Epoch: [33][428/817]	Loss 0.0081 (0.0351)	
training:	Epoch: [33][429/817]	Loss 0.0074 (0.0350)	
training:	Epoch: [33][430/817]	Loss 0.0062 (0.0349)	
training:	Epoch: [33][431/817]	Loss 0.0060 (0.0349)	
training:	Epoch: [33][432/817]	Loss 0.0046 (0.0348)	
training:	Epoch: [33][433/817]	Loss 0.0058 (0.0347)	
training:	Epoch: [33][434/817]	Loss 0.0058 (0.0347)	
training:	Epoch: [33][435/817]	Loss 0.0077 (0.0346)	
training:	Epoch: [33][436/817]	Loss 0.0058 (0.0345)	
training:	Epoch: [33][437/817]	Loss 0.0074 (0.0345)	
training:	Epoch: [33][438/817]	Loss 0.0052 (0.0344)	
training:	Epoch: [33][439/817]	Loss 0.0047 (0.0343)	
training:	Epoch: [33][440/817]	Loss 0.0073 (0.0343)	
training:	Epoch: [33][441/817]	Loss 0.0092 (0.0342)	
training:	Epoch: [33][442/817]	Loss 0.0072 (0.0342)	
training:	Epoch: [33][443/817]	Loss 0.0067 (0.0341)	
training:	Epoch: [33][444/817]	Loss 0.0058 (0.0340)	
training:	Epoch: [33][445/817]	Loss 0.0063 (0.0340)	
training:	Epoch: [33][446/817]	Loss 0.0066 (0.0339)	
training:	Epoch: [33][447/817]	Loss 0.0050 (0.0338)	
training:	Epoch: [33][448/817]	Loss 0.0062 (0.0338)	
training:	Epoch: [33][449/817]	Loss 0.0071 (0.0337)	
training:	Epoch: [33][450/817]	Loss 0.0048 (0.0337)	
training:	Epoch: [33][451/817]	Loss 0.4201 (0.0345)	
training:	Epoch: [33][452/817]	Loss 0.1576 (0.0348)	
training:	Epoch: [33][453/817]	Loss 0.0065 (0.0347)	
training:	Epoch: [33][454/817]	Loss 0.0065 (0.0347)	
training:	Epoch: [33][455/817]	Loss 0.0115 (0.0346)	
training:	Epoch: [33][456/817]	Loss 0.0062 (0.0345)	
training:	Epoch: [33][457/817]	Loss 0.0056 (0.0345)	
training:	Epoch: [33][458/817]	Loss 0.0079 (0.0344)	
training:	Epoch: [33][459/817]	Loss 0.0055 (0.0344)	
training:	Epoch: [33][460/817]	Loss 0.0059 (0.0343)	
training:	Epoch: [33][461/817]	Loss 0.0084 (0.0342)	
training:	Epoch: [33][462/817]	Loss 0.0073 (0.0342)	
training:	Epoch: [33][463/817]	Loss 0.0067 (0.0341)	
training:	Epoch: [33][464/817]	Loss 0.0061 (0.0341)	
training:	Epoch: [33][465/817]	Loss 0.0062 (0.0340)	
training:	Epoch: [33][466/817]	Loss 0.0056 (0.0339)	
training:	Epoch: [33][467/817]	Loss 0.0073 (0.0339)	
training:	Epoch: [33][468/817]	Loss 0.0060 (0.0338)	
training:	Epoch: [33][469/817]	Loss 0.0061 (0.0338)	
training:	Epoch: [33][470/817]	Loss 0.0054 (0.0337)	
training:	Epoch: [33][471/817]	Loss 0.0063 (0.0336)	
training:	Epoch: [33][472/817]	Loss 0.0060 (0.0336)	
training:	Epoch: [33][473/817]	Loss 0.0055 (0.0335)	
training:	Epoch: [33][474/817]	Loss 0.0060 (0.0335)	
training:	Epoch: [33][475/817]	Loss 0.3697 (0.0342)	
training:	Epoch: [33][476/817]	Loss 0.0045 (0.0341)	
training:	Epoch: [33][477/817]	Loss 0.0061 (0.0341)	
training:	Epoch: [33][478/817]	Loss 0.0057 (0.0340)	
training:	Epoch: [33][479/817]	Loss 0.0062 (0.0339)	
training:	Epoch: [33][480/817]	Loss 0.0083 (0.0339)	
training:	Epoch: [33][481/817]	Loss 0.3056 (0.0345)	
training:	Epoch: [33][482/817]	Loss 0.0064 (0.0344)	
training:	Epoch: [33][483/817]	Loss 0.0048 (0.0343)	
training:	Epoch: [33][484/817]	Loss 0.0059 (0.0343)	
training:	Epoch: [33][485/817]	Loss 0.0099 (0.0342)	
training:	Epoch: [33][486/817]	Loss 0.0069 (0.0342)	
training:	Epoch: [33][487/817]	Loss 0.0072 (0.0341)	
training:	Epoch: [33][488/817]	Loss 0.5893 (0.0353)	
training:	Epoch: [33][489/817]	Loss 0.0058 (0.0352)	
training:	Epoch: [33][490/817]	Loss 0.6291 (0.0364)	
training:	Epoch: [33][491/817]	Loss 0.0074 (0.0363)	
training:	Epoch: [33][492/817]	Loss 0.0060 (0.0363)	
training:	Epoch: [33][493/817]	Loss 0.0059 (0.0362)	
training:	Epoch: [33][494/817]	Loss 0.0064 (0.0362)	
training:	Epoch: [33][495/817]	Loss 0.0074 (0.0361)	
training:	Epoch: [33][496/817]	Loss 0.0054 (0.0360)	
training:	Epoch: [33][497/817]	Loss 0.0065 (0.0360)	
training:	Epoch: [33][498/817]	Loss 0.0069 (0.0359)	
training:	Epoch: [33][499/817]	Loss 0.0077 (0.0359)	
training:	Epoch: [33][500/817]	Loss 0.0235 (0.0358)	
training:	Epoch: [33][501/817]	Loss 0.0065 (0.0358)	
training:	Epoch: [33][502/817]	Loss 0.0072 (0.0357)	
training:	Epoch: [33][503/817]	Loss 0.0053 (0.0357)	
training:	Epoch: [33][504/817]	Loss 0.0054 (0.0356)	
training:	Epoch: [33][505/817]	Loss 0.0080 (0.0356)	
training:	Epoch: [33][506/817]	Loss 0.0065 (0.0355)	
training:	Epoch: [33][507/817]	Loss 0.0060 (0.0354)	
training:	Epoch: [33][508/817]	Loss 0.0078 (0.0354)	
training:	Epoch: [33][509/817]	Loss 0.0133 (0.0353)	
training:	Epoch: [33][510/817]	Loss 0.0081 (0.0353)	
training:	Epoch: [33][511/817]	Loss 0.0066 (0.0352)	
training:	Epoch: [33][512/817]	Loss 0.0070 (0.0352)	
training:	Epoch: [33][513/817]	Loss 0.0053 (0.0351)	
training:	Epoch: [33][514/817]	Loss 0.0076 (0.0351)	
training:	Epoch: [33][515/817]	Loss 0.0062 (0.0350)	
training:	Epoch: [33][516/817]	Loss 0.0054 (0.0349)	
training:	Epoch: [33][517/817]	Loss 0.0064 (0.0349)	
training:	Epoch: [33][518/817]	Loss 0.0075 (0.0348)	
training:	Epoch: [33][519/817]	Loss 0.0184 (0.0348)	
training:	Epoch: [33][520/817]	Loss 0.0058 (0.0348)	
training:	Epoch: [33][521/817]	Loss 0.0055 (0.0347)	
training:	Epoch: [33][522/817]	Loss 0.0073 (0.0346)	
training:	Epoch: [33][523/817]	Loss 0.0883 (0.0347)	
training:	Epoch: [33][524/817]	Loss 0.0050 (0.0347)	
training:	Epoch: [33][525/817]	Loss 0.0055 (0.0346)	
training:	Epoch: [33][526/817]	Loss 0.0058 (0.0346)	
training:	Epoch: [33][527/817]	Loss 0.0048 (0.0345)	
training:	Epoch: [33][528/817]	Loss 0.0060 (0.0345)	
training:	Epoch: [33][529/817]	Loss 0.0047 (0.0344)	
training:	Epoch: [33][530/817]	Loss 0.0084 (0.0344)	
training:	Epoch: [33][531/817]	Loss 0.0056 (0.0343)	
training:	Epoch: [33][532/817]	Loss 0.0078 (0.0343)	
training:	Epoch: [33][533/817]	Loss 0.0067 (0.0342)	
training:	Epoch: [33][534/817]	Loss 0.0056 (0.0342)	
training:	Epoch: [33][535/817]	Loss 0.0054 (0.0341)	
training:	Epoch: [33][536/817]	Loss 0.0055 (0.0340)	
training:	Epoch: [33][537/817]	Loss 0.0086 (0.0340)	
training:	Epoch: [33][538/817]	Loss 0.0069 (0.0339)	
training:	Epoch: [33][539/817]	Loss 0.0107 (0.0339)	
training:	Epoch: [33][540/817]	Loss 0.0082 (0.0339)	
training:	Epoch: [33][541/817]	Loss 0.0065 (0.0338)	
training:	Epoch: [33][542/817]	Loss 0.6229 (0.0349)	
training:	Epoch: [33][543/817]	Loss 0.0048 (0.0348)	
training:	Epoch: [33][544/817]	Loss 0.0215 (0.0348)	
training:	Epoch: [33][545/817]	Loss 0.0086 (0.0348)	
training:	Epoch: [33][546/817]	Loss 0.0056 (0.0347)	
training:	Epoch: [33][547/817]	Loss 0.0078 (0.0347)	
training:	Epoch: [33][548/817]	Loss 0.0061 (0.0346)	
training:	Epoch: [33][549/817]	Loss 0.0068 (0.0346)	
training:	Epoch: [33][550/817]	Loss 0.0064 (0.0345)	
training:	Epoch: [33][551/817]	Loss 0.0069 (0.0345)	
training:	Epoch: [33][552/817]	Loss 0.0076 (0.0344)	
training:	Epoch: [33][553/817]	Loss 0.0076 (0.0344)	
training:	Epoch: [33][554/817]	Loss 0.0113 (0.0343)	
training:	Epoch: [33][555/817]	Loss 0.0059 (0.0343)	
training:	Epoch: [33][556/817]	Loss 0.0069 (0.0342)	
training:	Epoch: [33][557/817]	Loss 0.0067 (0.0342)	
training:	Epoch: [33][558/817]	Loss 0.0058 (0.0341)	
training:	Epoch: [33][559/817]	Loss 0.0061 (0.0341)	
training:	Epoch: [33][560/817]	Loss 0.0057 (0.0340)	
training:	Epoch: [33][561/817]	Loss 0.0088 (0.0340)	
training:	Epoch: [33][562/817]	Loss 0.0060 (0.0339)	
training:	Epoch: [33][563/817]	Loss 0.0051 (0.0339)	
training:	Epoch: [33][564/817]	Loss 0.5847 (0.0348)	
training:	Epoch: [33][565/817]	Loss 0.0073 (0.0348)	
training:	Epoch: [33][566/817]	Loss 0.0063 (0.0348)	
training:	Epoch: [33][567/817]	Loss 0.0054 (0.0347)	
training:	Epoch: [33][568/817]	Loss 0.0047 (0.0346)	
training:	Epoch: [33][569/817]	Loss 0.0060 (0.0346)	
training:	Epoch: [33][570/817]	Loss 0.0059 (0.0345)	
training:	Epoch: [33][571/817]	Loss 0.0064 (0.0345)	
training:	Epoch: [33][572/817]	Loss 0.0053 (0.0344)	
training:	Epoch: [33][573/817]	Loss 0.0063 (0.0344)	
training:	Epoch: [33][574/817]	Loss 0.0055 (0.0343)	
training:	Epoch: [33][575/817]	Loss 0.0087 (0.0343)	
training:	Epoch: [33][576/817]	Loss 0.0061 (0.0343)	
training:	Epoch: [33][577/817]	Loss 0.0052 (0.0342)	
training:	Epoch: [33][578/817]	Loss 0.0065 (0.0342)	
training:	Epoch: [33][579/817]	Loss 0.0071 (0.0341)	
training:	Epoch: [33][580/817]	Loss 0.0069 (0.0341)	
training:	Epoch: [33][581/817]	Loss 0.0078 (0.0340)	
training:	Epoch: [33][582/817]	Loss 0.0080 (0.0340)	
training:	Epoch: [33][583/817]	Loss 0.0055 (0.0339)	
training:	Epoch: [33][584/817]	Loss 0.0072 (0.0339)	
training:	Epoch: [33][585/817]	Loss 0.0062 (0.0338)	
training:	Epoch: [33][586/817]	Loss 0.0063 (0.0338)	
training:	Epoch: [33][587/817]	Loss 0.0074 (0.0337)	
training:	Epoch: [33][588/817]	Loss 0.0047 (0.0337)	
training:	Epoch: [33][589/817]	Loss 0.0047 (0.0336)	
training:	Epoch: [33][590/817]	Loss 0.0059 (0.0336)	
training:	Epoch: [33][591/817]	Loss 0.0061 (0.0335)	
training:	Epoch: [33][592/817]	Loss 0.0069 (0.0335)	
training:	Epoch: [33][593/817]	Loss 0.0060 (0.0335)	
training:	Epoch: [33][594/817]	Loss 0.0071 (0.0334)	
training:	Epoch: [33][595/817]	Loss 0.0061 (0.0334)	
training:	Epoch: [33][596/817]	Loss 0.0065 (0.0333)	
training:	Epoch: [33][597/817]	Loss 0.6887 (0.0344)	
training:	Epoch: [33][598/817]	Loss 0.0062 (0.0344)	
training:	Epoch: [33][599/817]	Loss 0.0068 (0.0343)	
training:	Epoch: [33][600/817]	Loss 0.0072 (0.0343)	
training:	Epoch: [33][601/817]	Loss 0.0077 (0.0342)	
training:	Epoch: [33][602/817]	Loss 0.0071 (0.0342)	
training:	Epoch: [33][603/817]	Loss 0.0070 (0.0341)	
training:	Epoch: [33][604/817]	Loss 0.2673 (0.0345)	
training:	Epoch: [33][605/817]	Loss 0.0065 (0.0345)	
training:	Epoch: [33][606/817]	Loss 0.0054 (0.0344)	
training:	Epoch: [33][607/817]	Loss 0.0071 (0.0344)	
training:	Epoch: [33][608/817]	Loss 0.0059 (0.0343)	
training:	Epoch: [33][609/817]	Loss 0.0066 (0.0343)	
training:	Epoch: [33][610/817]	Loss 0.0057 (0.0342)	
training:	Epoch: [33][611/817]	Loss 0.0065 (0.0342)	
training:	Epoch: [33][612/817]	Loss 0.0065 (0.0342)	
training:	Epoch: [33][613/817]	Loss 0.0058 (0.0341)	
training:	Epoch: [33][614/817]	Loss 0.0060 (0.0341)	
training:	Epoch: [33][615/817]	Loss 0.0049 (0.0340)	
training:	Epoch: [33][616/817]	Loss 0.0053 (0.0340)	
training:	Epoch: [33][617/817]	Loss 0.0051 (0.0339)	
training:	Epoch: [33][618/817]	Loss 0.0049 (0.0339)	
training:	Epoch: [33][619/817]	Loss 0.0058 (0.0338)	
training:	Epoch: [33][620/817]	Loss 1.1982 (0.0357)	
training:	Epoch: [33][621/817]	Loss 0.0199 (0.0357)	
training:	Epoch: [33][622/817]	Loss 0.0055 (0.0356)	
training:	Epoch: [33][623/817]	Loss 0.0082 (0.0356)	
training:	Epoch: [33][624/817]	Loss 0.0054 (0.0355)	
training:	Epoch: [33][625/817]	Loss 0.0084 (0.0355)	
training:	Epoch: [33][626/817]	Loss 0.0062 (0.0355)	
training:	Epoch: [33][627/817]	Loss 0.5103 (0.0362)	
training:	Epoch: [33][628/817]	Loss 0.0070 (0.0362)	
training:	Epoch: [33][629/817]	Loss 0.0083 (0.0361)	
training:	Epoch: [33][630/817]	Loss 0.0061 (0.0361)	
training:	Epoch: [33][631/817]	Loss 0.0088 (0.0360)	
training:	Epoch: [33][632/817]	Loss 0.0070 (0.0360)	
training:	Epoch: [33][633/817]	Loss 0.0059 (0.0359)	
training:	Epoch: [33][634/817]	Loss 0.0069 (0.0359)	
training:	Epoch: [33][635/817]	Loss 0.0213 (0.0359)	
training:	Epoch: [33][636/817]	Loss 0.0059 (0.0358)	
training:	Epoch: [33][637/817]	Loss 0.0070 (0.0358)	
training:	Epoch: [33][638/817]	Loss 0.0077 (0.0357)	
training:	Epoch: [33][639/817]	Loss 0.0058 (0.0357)	
training:	Epoch: [33][640/817]	Loss 0.0080 (0.0356)	
training:	Epoch: [33][641/817]	Loss 0.0051 (0.0356)	
training:	Epoch: [33][642/817]	Loss 0.0053 (0.0355)	
training:	Epoch: [33][643/817]	Loss 0.6291 (0.0365)	
training:	Epoch: [33][644/817]	Loss 0.0061 (0.0364)	
training:	Epoch: [33][645/817]	Loss 0.0062 (0.0364)	
training:	Epoch: [33][646/817]	Loss 0.0046 (0.0363)	
training:	Epoch: [33][647/817]	Loss 0.6050 (0.0372)	
training:	Epoch: [33][648/817]	Loss 0.0056 (0.0372)	
training:	Epoch: [33][649/817]	Loss 0.0052 (0.0371)	
training:	Epoch: [33][650/817]	Loss 0.0076 (0.0371)	
training:	Epoch: [33][651/817]	Loss 0.5732 (0.0379)	
training:	Epoch: [33][652/817]	Loss 0.0068 (0.0378)	
training:	Epoch: [33][653/817]	Loss 0.0070 (0.0378)	
training:	Epoch: [33][654/817]	Loss 0.0069 (0.0377)	
training:	Epoch: [33][655/817]	Loss 0.0061 (0.0377)	
training:	Epoch: [33][656/817]	Loss 0.0108 (0.0377)	
training:	Epoch: [33][657/817]	Loss 0.0080 (0.0376)	
training:	Epoch: [33][658/817]	Loss 0.0056 (0.0376)	
training:	Epoch: [33][659/817]	Loss 0.0070 (0.0375)	
training:	Epoch: [33][660/817]	Loss 0.0066 (0.0375)	
training:	Epoch: [33][661/817]	Loss 0.0061 (0.0374)	
training:	Epoch: [33][662/817]	Loss 0.0067 (0.0374)	
training:	Epoch: [33][663/817]	Loss 0.0064 (0.0373)	
training:	Epoch: [33][664/817]	Loss 0.0058 (0.0373)	
training:	Epoch: [33][665/817]	Loss 0.0090 (0.0372)	
training:	Epoch: [33][666/817]	Loss 0.0094 (0.0372)	
training:	Epoch: [33][667/817]	Loss 0.0076 (0.0372)	
training:	Epoch: [33][668/817]	Loss 0.0067 (0.0371)	
training:	Epoch: [33][669/817]	Loss 0.0067 (0.0371)	
training:	Epoch: [33][670/817]	Loss 0.0076 (0.0370)	
training:	Epoch: [33][671/817]	Loss 0.0064 (0.0370)	
training:	Epoch: [33][672/817]	Loss 0.0085 (0.0369)	
training:	Epoch: [33][673/817]	Loss 0.0134 (0.0369)	
training:	Epoch: [33][674/817]	Loss 0.0061 (0.0368)	
training:	Epoch: [33][675/817]	Loss 0.0063 (0.0368)	
training:	Epoch: [33][676/817]	Loss 0.0091 (0.0368)	
training:	Epoch: [33][677/817]	Loss 0.0118 (0.0367)	
training:	Epoch: [33][678/817]	Loss 0.0070 (0.0367)	
training:	Epoch: [33][679/817]	Loss 0.0046 (0.0366)	
training:	Epoch: [33][680/817]	Loss 0.6111 (0.0375)	
training:	Epoch: [33][681/817]	Loss 0.0066 (0.0374)	
training:	Epoch: [33][682/817]	Loss 0.0062 (0.0374)	
training:	Epoch: [33][683/817]	Loss 0.0148 (0.0374)	
training:	Epoch: [33][684/817]	Loss 0.0061 (0.0373)	
training:	Epoch: [33][685/817]	Loss 0.0072 (0.0373)	
training:	Epoch: [33][686/817]	Loss 0.0065 (0.0372)	
training:	Epoch: [33][687/817]	Loss 0.0080 (0.0372)	
training:	Epoch: [33][688/817]	Loss 0.0070 (0.0371)	
training:	Epoch: [33][689/817]	Loss 0.0056 (0.0371)	
training:	Epoch: [33][690/817]	Loss 0.0059 (0.0370)	
training:	Epoch: [33][691/817]	Loss 0.0070 (0.0370)	
training:	Epoch: [33][692/817]	Loss 0.0066 (0.0370)	
training:	Epoch: [33][693/817]	Loss 0.0067 (0.0369)	
training:	Epoch: [33][694/817]	Loss 0.0098 (0.0369)	
training:	Epoch: [33][695/817]	Loss 0.0092 (0.0368)	
training:	Epoch: [33][696/817]	Loss 0.0070 (0.0368)	
training:	Epoch: [33][697/817]	Loss 0.0079 (0.0367)	
training:	Epoch: [33][698/817]	Loss 0.5711 (0.0375)	
training:	Epoch: [33][699/817]	Loss 0.0072 (0.0375)	
training:	Epoch: [33][700/817]	Loss 0.0074 (0.0374)	
training:	Epoch: [33][701/817]	Loss 0.0053 (0.0374)	
training:	Epoch: [33][702/817]	Loss 0.0062 (0.0373)	
training:	Epoch: [33][703/817]	Loss 0.0057 (0.0373)	
training:	Epoch: [33][704/817]	Loss 0.0087 (0.0373)	
training:	Epoch: [33][705/817]	Loss 0.0057 (0.0372)	
training:	Epoch: [33][706/817]	Loss 0.0081 (0.0372)	
training:	Epoch: [33][707/817]	Loss 0.0071 (0.0371)	
training:	Epoch: [33][708/817]	Loss 0.0090 (0.0371)	
training:	Epoch: [33][709/817]	Loss 0.0054 (0.0370)	
training:	Epoch: [33][710/817]	Loss 0.0105 (0.0370)	
training:	Epoch: [33][711/817]	Loss 0.0059 (0.0370)	
training:	Epoch: [33][712/817]	Loss 0.0051 (0.0369)	
training:	Epoch: [33][713/817]	Loss 0.0074 (0.0369)	
training:	Epoch: [33][714/817]	Loss 0.0088 (0.0368)	
training:	Epoch: [33][715/817]	Loss 0.0077 (0.0368)	
training:	Epoch: [33][716/817]	Loss 0.0076 (0.0368)	
training:	Epoch: [33][717/817]	Loss 0.0070 (0.0367)	
training:	Epoch: [33][718/817]	Loss 0.0062 (0.0367)	
training:	Epoch: [33][719/817]	Loss 0.0057 (0.0366)	
training:	Epoch: [33][720/817]	Loss 0.0053 (0.0366)	
training:	Epoch: [33][721/817]	Loss 0.0092 (0.0365)	
training:	Epoch: [33][722/817]	Loss 0.0087 (0.0365)	
training:	Epoch: [33][723/817]	Loss 0.5511 (0.0372)	
training:	Epoch: [33][724/817]	Loss 0.0102 (0.0372)	
training:	Epoch: [33][725/817]	Loss 0.0064 (0.0371)	
training:	Epoch: [33][726/817]	Loss 0.0070 (0.0371)	
training:	Epoch: [33][727/817]	Loss 0.0067 (0.0371)	
training:	Epoch: [33][728/817]	Loss 0.0080 (0.0370)	
training:	Epoch: [33][729/817]	Loss 0.0066 (0.0370)	
training:	Epoch: [33][730/817]	Loss 0.0060 (0.0369)	
training:	Epoch: [33][731/817]	Loss 0.0088 (0.0369)	
training:	Epoch: [33][732/817]	Loss 0.0075 (0.0368)	
training:	Epoch: [33][733/817]	Loss 0.5385 (0.0375)	
training:	Epoch: [33][734/817]	Loss 0.0058 (0.0375)	
training:	Epoch: [33][735/817]	Loss 0.0085 (0.0375)	
training:	Epoch: [33][736/817]	Loss 0.0074 (0.0374)	
training:	Epoch: [33][737/817]	Loss 0.0058 (0.0374)	
training:	Epoch: [33][738/817]	Loss 0.6426 (0.0382)	
training:	Epoch: [33][739/817]	Loss 0.0060 (0.0381)	
training:	Epoch: [33][740/817]	Loss 0.0056 (0.0381)	
training:	Epoch: [33][741/817]	Loss 0.0055 (0.0381)	
training:	Epoch: [33][742/817]	Loss 0.0083 (0.0380)	
training:	Epoch: [33][743/817]	Loss 0.0054 (0.0380)	
training:	Epoch: [33][744/817]	Loss 0.0053 (0.0379)	
training:	Epoch: [33][745/817]	Loss 0.0063 (0.0379)	
training:	Epoch: [33][746/817]	Loss 0.5112 (0.0385)	
training:	Epoch: [33][747/817]	Loss 0.0067 (0.0385)	
training:	Epoch: [33][748/817]	Loss 0.0091 (0.0384)	
training:	Epoch: [33][749/817]	Loss 0.0110 (0.0384)	
training:	Epoch: [33][750/817]	Loss 0.0093 (0.0384)	
training:	Epoch: [33][751/817]	Loss 0.0065 (0.0383)	
training:	Epoch: [33][752/817]	Loss 0.0073 (0.0383)	
training:	Epoch: [33][753/817]	Loss 0.0077 (0.0382)	
training:	Epoch: [33][754/817]	Loss 0.0060 (0.0382)	
training:	Epoch: [33][755/817]	Loss 0.0097 (0.0382)	
training:	Epoch: [33][756/817]	Loss 0.0110 (0.0381)	
training:	Epoch: [33][757/817]	Loss 0.0089 (0.0381)	
training:	Epoch: [33][758/817]	Loss 0.0047 (0.0380)	
training:	Epoch: [33][759/817]	Loss 0.0078 (0.0380)	
training:	Epoch: [33][760/817]	Loss 0.0090 (0.0380)	
training:	Epoch: [33][761/817]	Loss 0.0108 (0.0379)	
training:	Epoch: [33][762/817]	Loss 0.0046 (0.0379)	
training:	Epoch: [33][763/817]	Loss 0.0083 (0.0378)	
training:	Epoch: [33][764/817]	Loss 0.0071 (0.0378)	
training:	Epoch: [33][765/817]	Loss 0.0080 (0.0378)	
training:	Epoch: [33][766/817]	Loss 0.0099 (0.0377)	
training:	Epoch: [33][767/817]	Loss 0.0075 (0.0377)	
training:	Epoch: [33][768/817]	Loss 0.0080 (0.0376)	
training:	Epoch: [33][769/817]	Loss 0.0067 (0.0376)	
training:	Epoch: [33][770/817]	Loss 0.0132 (0.0376)	
training:	Epoch: [33][771/817]	Loss 0.0091 (0.0375)	
training:	Epoch: [33][772/817]	Loss 0.0077 (0.0375)	
training:	Epoch: [33][773/817]	Loss 0.0064 (0.0375)	
training:	Epoch: [33][774/817]	Loss 0.0082 (0.0374)	
training:	Epoch: [33][775/817]	Loss 0.0089 (0.0374)	
training:	Epoch: [33][776/817]	Loss 0.0064 (0.0373)	
training:	Epoch: [33][777/817]	Loss 0.0107 (0.0373)	
training:	Epoch: [33][778/817]	Loss 0.0080 (0.0373)	
training:	Epoch: [33][779/817]	Loss 0.0088 (0.0372)	
training:	Epoch: [33][780/817]	Loss 0.0072 (0.0372)	
training:	Epoch: [33][781/817]	Loss 0.0086 (0.0372)	
training:	Epoch: [33][782/817]	Loss 0.0053 (0.0371)	
training:	Epoch: [33][783/817]	Loss 0.0068 (0.0371)	
training:	Epoch: [33][784/817]	Loss 0.0077 (0.0370)	
training:	Epoch: [33][785/817]	Loss 0.0072 (0.0370)	
training:	Epoch: [33][786/817]	Loss 0.0085 (0.0370)	
training:	Epoch: [33][787/817]	Loss 0.0077 (0.0369)	
training:	Epoch: [33][788/817]	Loss 0.0066 (0.0369)	
training:	Epoch: [33][789/817]	Loss 0.0065 (0.0369)	
training:	Epoch: [33][790/817]	Loss 0.0085 (0.0368)	
training:	Epoch: [33][791/817]	Loss 0.6011 (0.0375)	
training:	Epoch: [33][792/817]	Loss 0.0059 (0.0375)	
training:	Epoch: [33][793/817]	Loss 0.0085 (0.0375)	
training:	Epoch: [33][794/817]	Loss 0.0074 (0.0374)	
training:	Epoch: [33][795/817]	Loss 0.0063 (0.0374)	
training:	Epoch: [33][796/817]	Loss 0.0100 (0.0373)	
training:	Epoch: [33][797/817]	Loss 0.0064 (0.0373)	
training:	Epoch: [33][798/817]	Loss 0.0127 (0.0373)	
training:	Epoch: [33][799/817]	Loss 0.0081 (0.0372)	
training:	Epoch: [33][800/817]	Loss 0.0077 (0.0372)	
training:	Epoch: [33][801/817]	Loss 0.0049 (0.0372)	
training:	Epoch: [33][802/817]	Loss 0.0105 (0.0371)	
training:	Epoch: [33][803/817]	Loss 0.0152 (0.0371)	
training:	Epoch: [33][804/817]	Loss 0.0054 (0.0371)	
training:	Epoch: [33][805/817]	Loss 0.0067 (0.0370)	
training:	Epoch: [33][806/817]	Loss 0.0072 (0.0370)	
training:	Epoch: [33][807/817]	Loss 0.0074 (0.0370)	
training:	Epoch: [33][808/817]	Loss 0.0071 (0.0369)	
training:	Epoch: [33][809/817]	Loss 0.0066 (0.0369)	
training:	Epoch: [33][810/817]	Loss 0.0071 (0.0368)	
training:	Epoch: [33][811/817]	Loss 0.0059 (0.0368)	
training:	Epoch: [33][812/817]	Loss 0.0054 (0.0368)	
training:	Epoch: [33][813/817]	Loss 0.0076 (0.0367)	
training:	Epoch: [33][814/817]	Loss 0.0084 (0.0367)	
training:	Epoch: [33][815/817]	Loss 0.0078 (0.0367)	
training:	Epoch: [33][816/817]	Loss 0.0069 (0.0366)	
training:	Epoch: [33][817/817]	Loss 0.0048 (0.0366)	
Training:	 Loss: 0.0366

Training:	 ACC: 0.9940 0.9940 0.9944 0.9936
Validation:	 ACC: 0.7845 0.7854 0.8045 0.7646
Validation:	 Best_BACC: 0.8223 0.8218 0.8127 0.8318
Validation:	 Loss: 0.9498
Pretraining:	Epoch 34/200
----------
training:	Epoch: [34][1/817]	Loss 0.0084 (0.0084)	
training:	Epoch: [34][2/817]	Loss 0.0047 (0.0066)	
training:	Epoch: [34][3/817]	Loss 0.0086 (0.0072)	
training:	Epoch: [34][4/817]	Loss 0.0054 (0.0068)	
training:	Epoch: [34][5/817]	Loss 0.0044 (0.0063)	
training:	Epoch: [34][6/817]	Loss 0.0076 (0.0065)	
training:	Epoch: [34][7/817]	Loss 0.0088 (0.0069)	
training:	Epoch: [34][8/817]	Loss 0.0057 (0.0067)	
training:	Epoch: [34][9/817]	Loss 0.0057 (0.0066)	
training:	Epoch: [34][10/817]	Loss 0.0055 (0.0065)	
training:	Epoch: [34][11/817]	Loss 0.0081 (0.0066)	
training:	Epoch: [34][12/817]	Loss 0.0068 (0.0067)	
training:	Epoch: [34][13/817]	Loss 0.0079 (0.0068)	
training:	Epoch: [34][14/817]	Loss 0.0072 (0.0068)	
training:	Epoch: [34][15/817]	Loss 0.0112 (0.0071)	
training:	Epoch: [34][16/817]	Loss 0.0066 (0.0070)	
training:	Epoch: [34][17/817]	Loss 0.0066 (0.0070)	
training:	Epoch: [34][18/817]	Loss 0.0072 (0.0070)	
training:	Epoch: [34][19/817]	Loss 0.0063 (0.0070)	
training:	Epoch: [34][20/817]	Loss 0.0060 (0.0069)	
training:	Epoch: [34][21/817]	Loss 0.0065 (0.0069)	
training:	Epoch: [34][22/817]	Loss 0.5337 (0.0309)	
training:	Epoch: [34][23/817]	Loss 0.0095 (0.0299)	
training:	Epoch: [34][24/817]	Loss 0.0072 (0.0290)	
training:	Epoch: [34][25/817]	Loss 0.0096 (0.0282)	
training:	Epoch: [34][26/817]	Loss 0.0063 (0.0274)	
training:	Epoch: [34][27/817]	Loss 0.0060 (0.0266)	
training:	Epoch: [34][28/817]	Loss 0.0060 (0.0258)	
training:	Epoch: [34][29/817]	Loss 0.0060 (0.0252)	
training:	Epoch: [34][30/817]	Loss 0.0061 (0.0245)	
training:	Epoch: [34][31/817]	Loss 0.0093 (0.0240)	
training:	Epoch: [34][32/817]	Loss 0.0100 (0.0236)	
training:	Epoch: [34][33/817]	Loss 0.0056 (0.0231)	
training:	Epoch: [34][34/817]	Loss 0.0081 (0.0226)	
training:	Epoch: [34][35/817]	Loss 0.0061 (0.0221)	
training:	Epoch: [34][36/817]	Loss 0.0082 (0.0218)	
training:	Epoch: [34][37/817]	Loss 0.0056 (0.0213)	
training:	Epoch: [34][38/817]	Loss 0.0060 (0.0209)	
training:	Epoch: [34][39/817]	Loss 0.0096 (0.0206)	
training:	Epoch: [34][40/817]	Loss 0.0055 (0.0203)	
training:	Epoch: [34][41/817]	Loss 0.0061 (0.0199)	
training:	Epoch: [34][42/817]	Loss 0.0080 (0.0196)	
training:	Epoch: [34][43/817]	Loss 0.0055 (0.0193)	
training:	Epoch: [34][44/817]	Loss 0.0049 (0.0190)	
training:	Epoch: [34][45/817]	Loss 0.0065 (0.0187)	
training:	Epoch: [34][46/817]	Loss 0.0071 (0.0184)	
training:	Epoch: [34][47/817]	Loss 0.0075 (0.0182)	
training:	Epoch: [34][48/817]	Loss 0.0068 (0.0180)	
training:	Epoch: [34][49/817]	Loss 0.0068 (0.0177)	
training:	Epoch: [34][50/817]	Loss 0.0070 (0.0175)	
training:	Epoch: [34][51/817]	Loss 0.0061 (0.0173)	
training:	Epoch: [34][52/817]	Loss 0.0074 (0.0171)	
training:	Epoch: [34][53/817]	Loss 0.0073 (0.0169)	
training:	Epoch: [34][54/817]	Loss 0.0078 (0.0168)	
training:	Epoch: [34][55/817]	Loss 0.0047 (0.0165)	
training:	Epoch: [34][56/817]	Loss 0.0056 (0.0163)	
training:	Epoch: [34][57/817]	Loss 0.0057 (0.0162)	
training:	Epoch: [34][58/817]	Loss 0.0068 (0.0160)	
training:	Epoch: [34][59/817]	Loss 0.0052 (0.0158)	
training:	Epoch: [34][60/817]	Loss 0.0063 (0.0157)	
training:	Epoch: [34][61/817]	Loss 0.0071 (0.0155)	
training:	Epoch: [34][62/817]	Loss 0.0082 (0.0154)	
training:	Epoch: [34][63/817]	Loss 0.0079 (0.0153)	
training:	Epoch: [34][64/817]	Loss 0.0057 (0.0151)	
training:	Epoch: [34][65/817]	Loss 0.0064 (0.0150)	
training:	Epoch: [34][66/817]	Loss 0.0056 (0.0148)	
training:	Epoch: [34][67/817]	Loss 0.0070 (0.0147)	
training:	Epoch: [34][68/817]	Loss 0.0066 (0.0146)	
training:	Epoch: [34][69/817]	Loss 0.0064 (0.0145)	
training:	Epoch: [34][70/817]	Loss 0.0071 (0.0144)	
training:	Epoch: [34][71/817]	Loss 0.0060 (0.0143)	
training:	Epoch: [34][72/817]	Loss 0.0062 (0.0142)	
training:	Epoch: [34][73/817]	Loss 0.0074 (0.0141)	
training:	Epoch: [34][74/817]	Loss 0.0074 (0.0140)	
training:	Epoch: [34][75/817]	Loss 0.0050 (0.0139)	
training:	Epoch: [34][76/817]	Loss 0.0051 (0.0137)	
training:	Epoch: [34][77/817]	Loss 0.0052 (0.0136)	
training:	Epoch: [34][78/817]	Loss 0.0066 (0.0135)	
training:	Epoch: [34][79/817]	Loss 0.0075 (0.0135)	
training:	Epoch: [34][80/817]	Loss 0.0065 (0.0134)	
training:	Epoch: [34][81/817]	Loss 0.0059 (0.0133)	
training:	Epoch: [34][82/817]	Loss 0.0062 (0.0132)	
training:	Epoch: [34][83/817]	Loss 0.5995 (0.0203)	
training:	Epoch: [34][84/817]	Loss 0.0072 (0.0201)	
training:	Epoch: [34][85/817]	Loss 0.0055 (0.0199)	
training:	Epoch: [34][86/817]	Loss 0.0064 (0.0198)	
training:	Epoch: [34][87/817]	Loss 0.0051 (0.0196)	
training:	Epoch: [34][88/817]	Loss 0.0242 (0.0197)	
training:	Epoch: [34][89/817]	Loss 0.0048 (0.0195)	
training:	Epoch: [34][90/817]	Loss 0.0050 (0.0193)	
training:	Epoch: [34][91/817]	Loss 0.0060 (0.0192)	
training:	Epoch: [34][92/817]	Loss 0.0087 (0.0191)	
training:	Epoch: [34][93/817]	Loss 0.0064 (0.0189)	
training:	Epoch: [34][94/817]	Loss 0.0070 (0.0188)	
training:	Epoch: [34][95/817]	Loss 0.0078 (0.0187)	
training:	Epoch: [34][96/817]	Loss 0.0049 (0.0185)	
training:	Epoch: [34][97/817]	Loss 0.0083 (0.0184)	
training:	Epoch: [34][98/817]	Loss 0.0124 (0.0184)	
training:	Epoch: [34][99/817]	Loss 0.0062 (0.0183)	
training:	Epoch: [34][100/817]	Loss 0.0070 (0.0181)	
training:	Epoch: [34][101/817]	Loss 0.5553 (0.0235)	
training:	Epoch: [34][102/817]	Loss 0.0056 (0.0233)	
training:	Epoch: [34][103/817]	Loss 0.0056 (0.0231)	
training:	Epoch: [34][104/817]	Loss 0.0080 (0.0230)	
training:	Epoch: [34][105/817]	Loss 0.0073 (0.0228)	
training:	Epoch: [34][106/817]	Loss 0.0059 (0.0227)	
training:	Epoch: [34][107/817]	Loss 0.0067 (0.0225)	
training:	Epoch: [34][108/817]	Loss 0.0056 (0.0224)	
training:	Epoch: [34][109/817]	Loss 0.0069 (0.0222)	
training:	Epoch: [34][110/817]	Loss 0.0053 (0.0221)	
training:	Epoch: [34][111/817]	Loss 0.0065 (0.0219)	
training:	Epoch: [34][112/817]	Loss 0.0066 (0.0218)	
training:	Epoch: [34][113/817]	Loss 0.0065 (0.0216)	
training:	Epoch: [34][114/817]	Loss 0.0059 (0.0215)	
training:	Epoch: [34][115/817]	Loss 0.0060 (0.0214)	
training:	Epoch: [34][116/817]	Loss 0.0066 (0.0212)	
training:	Epoch: [34][117/817]	Loss 0.0058 (0.0211)	
training:	Epoch: [34][118/817]	Loss 0.0068 (0.0210)	
training:	Epoch: [34][119/817]	Loss 0.0071 (0.0209)	
training:	Epoch: [34][120/817]	Loss 0.0063 (0.0208)	
training:	Epoch: [34][121/817]	Loss 0.0081 (0.0206)	
training:	Epoch: [34][122/817]	Loss 0.0074 (0.0205)	
training:	Epoch: [34][123/817]	Loss 0.0067 (0.0204)	
training:	Epoch: [34][124/817]	Loss 0.0051 (0.0203)	
training:	Epoch: [34][125/817]	Loss 0.0064 (0.0202)	
training:	Epoch: [34][126/817]	Loss 0.0068 (0.0201)	
training:	Epoch: [34][127/817]	Loss 0.0046 (0.0200)	
training:	Epoch: [34][128/817]	Loss 0.0066 (0.0199)	
training:	Epoch: [34][129/817]	Loss 0.0056 (0.0198)	
training:	Epoch: [34][130/817]	Loss 0.4743 (0.0232)	
training:	Epoch: [34][131/817]	Loss 0.0063 (0.0231)	
training:	Epoch: [34][132/817]	Loss 0.0057 (0.0230)	
training:	Epoch: [34][133/817]	Loss 0.0069 (0.0229)	
training:	Epoch: [34][134/817]	Loss 0.0052 (0.0227)	
training:	Epoch: [34][135/817]	Loss 0.0062 (0.0226)	
training:	Epoch: [34][136/817]	Loss 0.0082 (0.0225)	
training:	Epoch: [34][137/817]	Loss 0.0061 (0.0224)	
training:	Epoch: [34][138/817]	Loss 0.0074 (0.0223)	
training:	Epoch: [34][139/817]	Loss 0.0055 (0.0222)	
training:	Epoch: [34][140/817]	Loss 0.0053 (0.0220)	
training:	Epoch: [34][141/817]	Loss 0.0063 (0.0219)	
training:	Epoch: [34][142/817]	Loss 0.0053 (0.0218)	
training:	Epoch: [34][143/817]	Loss 0.0057 (0.0217)	
training:	Epoch: [34][144/817]	Loss 0.0081 (0.0216)	
training:	Epoch: [34][145/817]	Loss 0.0073 (0.0215)	
training:	Epoch: [34][146/817]	Loss 0.0058 (0.0214)	
training:	Epoch: [34][147/817]	Loss 0.0076 (0.0213)	
training:	Epoch: [34][148/817]	Loss 0.0065 (0.0212)	
training:	Epoch: [34][149/817]	Loss 0.0067 (0.0211)	
training:	Epoch: [34][150/817]	Loss 0.0058 (0.0210)	
training:	Epoch: [34][151/817]	Loss 0.0062 (0.0209)	
training:	Epoch: [34][152/817]	Loss 0.0060 (0.0208)	
training:	Epoch: [34][153/817]	Loss 0.0061 (0.0207)	
training:	Epoch: [34][154/817]	Loss 0.0080 (0.0206)	
training:	Epoch: [34][155/817]	Loss 0.0059 (0.0205)	
training:	Epoch: [34][156/817]	Loss 0.0078 (0.0204)	
training:	Epoch: [34][157/817]	Loss 0.0048 (0.0203)	
training:	Epoch: [34][158/817]	Loss 0.0062 (0.0203)	
training:	Epoch: [34][159/817]	Loss 0.0052 (0.0202)	
training:	Epoch: [34][160/817]	Loss 0.0060 (0.0201)	
training:	Epoch: [34][161/817]	Loss 0.0078 (0.0200)	
training:	Epoch: [34][162/817]	Loss 0.0048 (0.0199)	
training:	Epoch: [34][163/817]	Loss 0.0059 (0.0198)	
training:	Epoch: [34][164/817]	Loss 0.0089 (0.0198)	
training:	Epoch: [34][165/817]	Loss 0.0064 (0.0197)	
training:	Epoch: [34][166/817]	Loss 0.0053 (0.0196)	
training:	Epoch: [34][167/817]	Loss 0.0072 (0.0195)	
training:	Epoch: [34][168/817]	Loss 0.0058 (0.0194)	
training:	Epoch: [34][169/817]	Loss 0.0057 (0.0194)	
training:	Epoch: [34][170/817]	Loss 0.0061 (0.0193)	
training:	Epoch: [34][171/817]	Loss 0.0054 (0.0192)	
training:	Epoch: [34][172/817]	Loss 0.0078 (0.0191)	
training:	Epoch: [34][173/817]	Loss 0.0054 (0.0190)	
training:	Epoch: [34][174/817]	Loss 0.0054 (0.0190)	
training:	Epoch: [34][175/817]	Loss 0.0048 (0.0189)	
training:	Epoch: [34][176/817]	Loss 0.0051 (0.0188)	
training:	Epoch: [34][177/817]	Loss 0.0056 (0.0187)	
training:	Epoch: [34][178/817]	Loss 0.0059 (0.0187)	
training:	Epoch: [34][179/817]	Loss 0.0060 (0.0186)	
training:	Epoch: [34][180/817]	Loss 0.0061 (0.0185)	
training:	Epoch: [34][181/817]	Loss 0.0068 (0.0185)	
training:	Epoch: [34][182/817]	Loss 0.0049 (0.0184)	
training:	Epoch: [34][183/817]	Loss 0.0048 (0.0183)	
training:	Epoch: [34][184/817]	Loss 0.0053 (0.0182)	
training:	Epoch: [34][185/817]	Loss 0.0051 (0.0182)	
training:	Epoch: [34][186/817]	Loss 0.0064 (0.0181)	
training:	Epoch: [34][187/817]	Loss 0.0911 (0.0185)	
training:	Epoch: [34][188/817]	Loss 0.0046 (0.0184)	
training:	Epoch: [34][189/817]	Loss 0.0039 (0.0183)	
training:	Epoch: [34][190/817]	Loss 0.0056 (0.0183)	
training:	Epoch: [34][191/817]	Loss 0.0058 (0.0182)	
training:	Epoch: [34][192/817]	Loss 0.0054 (0.0181)	
training:	Epoch: [34][193/817]	Loss 0.0071 (0.0181)	
training:	Epoch: [34][194/817]	Loss 0.0045 (0.0180)	
training:	Epoch: [34][195/817]	Loss 0.0048 (0.0179)	
training:	Epoch: [34][196/817]	Loss 0.0076 (0.0179)	
training:	Epoch: [34][197/817]	Loss 0.5928 (0.0208)	
training:	Epoch: [34][198/817]	Loss 0.0055 (0.0207)	
training:	Epoch: [34][199/817]	Loss 0.0053 (0.0207)	
training:	Epoch: [34][200/817]	Loss 0.0055 (0.0206)	
training:	Epoch: [34][201/817]	Loss 0.0054 (0.0205)	
training:	Epoch: [34][202/817]	Loss 0.0063 (0.0204)	
training:	Epoch: [34][203/817]	Loss 0.0072 (0.0204)	
training:	Epoch: [34][204/817]	Loss 0.0041 (0.0203)	
training:	Epoch: [34][205/817]	Loss 0.0074 (0.0202)	
training:	Epoch: [34][206/817]	Loss 0.0048 (0.0202)	
training:	Epoch: [34][207/817]	Loss 0.0385 (0.0202)	
training:	Epoch: [34][208/817]	Loss 0.0045 (0.0202)	
training:	Epoch: [34][209/817]	Loss 0.0077 (0.0201)	
training:	Epoch: [34][210/817]	Loss 0.0055 (0.0200)	
training:	Epoch: [34][211/817]	Loss 0.0050 (0.0200)	
training:	Epoch: [34][212/817]	Loss 0.0058 (0.0199)	
training:	Epoch: [34][213/817]	Loss 0.0065 (0.0198)	
training:	Epoch: [34][214/817]	Loss 0.0081 (0.0198)	
training:	Epoch: [34][215/817]	Loss 0.0062 (0.0197)	
training:	Epoch: [34][216/817]	Loss 0.0051 (0.0197)	
training:	Epoch: [34][217/817]	Loss 0.0067 (0.0196)	
training:	Epoch: [34][218/817]	Loss 0.0044 (0.0195)	
training:	Epoch: [34][219/817]	Loss 0.0056 (0.0195)	
training:	Epoch: [34][220/817]	Loss 0.0061 (0.0194)	
training:	Epoch: [34][221/817]	Loss 0.0061 (0.0193)	
training:	Epoch: [34][222/817]	Loss 0.0060 (0.0193)	
training:	Epoch: [34][223/817]	Loss 0.0061 (0.0192)	
training:	Epoch: [34][224/817]	Loss 0.0044 (0.0192)	
training:	Epoch: [34][225/817]	Loss 0.0058 (0.0191)	
training:	Epoch: [34][226/817]	Loss 0.0050 (0.0190)	
training:	Epoch: [34][227/817]	Loss 0.0058 (0.0190)	
training:	Epoch: [34][228/817]	Loss 0.0074 (0.0189)	
training:	Epoch: [34][229/817]	Loss 0.0052 (0.0189)	
training:	Epoch: [34][230/817]	Loss 0.5964 (0.0214)	
training:	Epoch: [34][231/817]	Loss 0.0054 (0.0213)	
training:	Epoch: [34][232/817]	Loss 0.0061 (0.0212)	
training:	Epoch: [34][233/817]	Loss 0.6427 (0.0239)	
training:	Epoch: [34][234/817]	Loss 0.0086 (0.0238)	
training:	Epoch: [34][235/817]	Loss 0.0079 (0.0238)	
training:	Epoch: [34][236/817]	Loss 0.0058 (0.0237)	
training:	Epoch: [34][237/817]	Loss 0.0055 (0.0236)	
training:	Epoch: [34][238/817]	Loss 0.0050 (0.0235)	
training:	Epoch: [34][239/817]	Loss 0.0076 (0.0235)	
training:	Epoch: [34][240/817]	Loss 0.0054 (0.0234)	
training:	Epoch: [34][241/817]	Loss 0.0053 (0.0233)	
training:	Epoch: [34][242/817]	Loss 0.0056 (0.0233)	
training:	Epoch: [34][243/817]	Loss 0.0055 (0.0232)	
training:	Epoch: [34][244/817]	Loss 0.0055 (0.0231)	
training:	Epoch: [34][245/817]	Loss 0.0065 (0.0230)	
training:	Epoch: [34][246/817]	Loss 0.0064 (0.0230)	
training:	Epoch: [34][247/817]	Loss 0.0062 (0.0229)	
training:	Epoch: [34][248/817]	Loss 0.0045 (0.0228)	
training:	Epoch: [34][249/817]	Loss 0.6123 (0.0252)	
training:	Epoch: [34][250/817]	Loss 0.0047 (0.0251)	
training:	Epoch: [34][251/817]	Loss 0.0059 (0.0250)	
training:	Epoch: [34][252/817]	Loss 0.0066 (0.0250)	
training:	Epoch: [34][253/817]	Loss 0.0073 (0.0249)	
training:	Epoch: [34][254/817]	Loss 0.0052 (0.0248)	
training:	Epoch: [34][255/817]	Loss 0.0043 (0.0247)	
training:	Epoch: [34][256/817]	Loss 0.5473 (0.0268)	
training:	Epoch: [34][257/817]	Loss 0.0073 (0.0267)	
training:	Epoch: [34][258/817]	Loss 0.0072 (0.0266)	
training:	Epoch: [34][259/817]	Loss 0.0050 (0.0265)	
training:	Epoch: [34][260/817]	Loss 0.0043 (0.0265)	
training:	Epoch: [34][261/817]	Loss 0.0070 (0.0264)	
training:	Epoch: [34][262/817]	Loss 0.0059 (0.0263)	
training:	Epoch: [34][263/817]	Loss 0.0077 (0.0262)	
training:	Epoch: [34][264/817]	Loss 0.0060 (0.0262)	
training:	Epoch: [34][265/817]	Loss 0.0049 (0.0261)	
training:	Epoch: [34][266/817]	Loss 0.0073 (0.0260)	
training:	Epoch: [34][267/817]	Loss 0.0052 (0.0259)	
training:	Epoch: [34][268/817]	Loss 0.0050 (0.0259)	
training:	Epoch: [34][269/817]	Loss 0.0062 (0.0258)	
training:	Epoch: [34][270/817]	Loss 0.0060 (0.0257)	
training:	Epoch: [34][271/817]	Loss 0.0060 (0.0256)	
training:	Epoch: [34][272/817]	Loss 0.0047 (0.0256)	
training:	Epoch: [34][273/817]	Loss 0.0071 (0.0255)	
training:	Epoch: [34][274/817]	Loss 0.4872 (0.0272)	
training:	Epoch: [34][275/817]	Loss 0.0087 (0.0271)	
training:	Epoch: [34][276/817]	Loss 0.0068 (0.0270)	
training:	Epoch: [34][277/817]	Loss 0.0056 (0.0270)	
training:	Epoch: [34][278/817]	Loss 0.0053 (0.0269)	
training:	Epoch: [34][279/817]	Loss 0.0069 (0.0268)	
training:	Epoch: [34][280/817]	Loss 0.5498 (0.0287)	
training:	Epoch: [34][281/817]	Loss 0.0041 (0.0286)	
training:	Epoch: [34][282/817]	Loss 0.0056 (0.0285)	
training:	Epoch: [34][283/817]	Loss 0.0073 (0.0284)	
training:	Epoch: [34][284/817]	Loss 0.0072 (0.0284)	
training:	Epoch: [34][285/817]	Loss 0.1391 (0.0287)	
training:	Epoch: [34][286/817]	Loss 0.0064 (0.0287)	
training:	Epoch: [34][287/817]	Loss 0.0062 (0.0286)	
training:	Epoch: [34][288/817]	Loss 0.0081 (0.0285)	
training:	Epoch: [34][289/817]	Loss 0.0075 (0.0284)	
training:	Epoch: [34][290/817]	Loss 0.0074 (0.0284)	
training:	Epoch: [34][291/817]	Loss 0.0069 (0.0283)	
training:	Epoch: [34][292/817]	Loss 0.0051 (0.0282)	
training:	Epoch: [34][293/817]	Loss 0.0063 (0.0281)	
training:	Epoch: [34][294/817]	Loss 0.0098 (0.0281)	
training:	Epoch: [34][295/817]	Loss 0.0055 (0.0280)	
training:	Epoch: [34][296/817]	Loss 0.0083 (0.0279)	
training:	Epoch: [34][297/817]	Loss 0.0057 (0.0279)	
training:	Epoch: [34][298/817]	Loss 0.0090 (0.0278)	
training:	Epoch: [34][299/817]	Loss 0.0089 (0.0277)	
training:	Epoch: [34][300/817]	Loss 0.0097 (0.0277)	
training:	Epoch: [34][301/817]	Loss 0.2179 (0.0283)	
training:	Epoch: [34][302/817]	Loss 0.0099 (0.0282)	
training:	Epoch: [34][303/817]	Loss 0.0060 (0.0282)	
training:	Epoch: [34][304/817]	Loss 0.0076 (0.0281)	
training:	Epoch: [34][305/817]	Loss 0.0065 (0.0280)	
training:	Epoch: [34][306/817]	Loss 0.0052 (0.0280)	
training:	Epoch: [34][307/817]	Loss 0.0058 (0.0279)	
training:	Epoch: [34][308/817]	Loss 0.0051 (0.0278)	
training:	Epoch: [34][309/817]	Loss 0.0053 (0.0277)	
training:	Epoch: [34][310/817]	Loss 0.0172 (0.0277)	
training:	Epoch: [34][311/817]	Loss 0.0070 (0.0276)	
training:	Epoch: [34][312/817]	Loss 0.0062 (0.0276)	
training:	Epoch: [34][313/817]	Loss 0.0066 (0.0275)	
training:	Epoch: [34][314/817]	Loss 0.0070 (0.0274)	
training:	Epoch: [34][315/817]	Loss 0.0080 (0.0274)	
training:	Epoch: [34][316/817]	Loss 1.0806 (0.0307)	
training:	Epoch: [34][317/817]	Loss 0.0074 (0.0306)	
training:	Epoch: [34][318/817]	Loss 0.0396 (0.0307)	
training:	Epoch: [34][319/817]	Loss 0.0095 (0.0306)	
training:	Epoch: [34][320/817]	Loss 0.0440 (0.0306)	
training:	Epoch: [34][321/817]	Loss 0.0073 (0.0306)	
training:	Epoch: [34][322/817]	Loss 0.0072 (0.0305)	
training:	Epoch: [34][323/817]	Loss 0.0064 (0.0304)	
training:	Epoch: [34][324/817]	Loss 0.0062 (0.0303)	
training:	Epoch: [34][325/817]	Loss 0.0083 (0.0303)	
training:	Epoch: [34][326/817]	Loss 0.0070 (0.0302)	
training:	Epoch: [34][327/817]	Loss 0.0071 (0.0301)	
training:	Epoch: [34][328/817]	Loss 0.0062 (0.0301)	
training:	Epoch: [34][329/817]	Loss 0.0061 (0.0300)	
training:	Epoch: [34][330/817]	Loss 0.0125 (0.0299)	
training:	Epoch: [34][331/817]	Loss 0.0060 (0.0299)	
training:	Epoch: [34][332/817]	Loss 0.0057 (0.0298)	
training:	Epoch: [34][333/817]	Loss 0.0060 (0.0297)	
training:	Epoch: [34][334/817]	Loss 0.0071 (0.0297)	
training:	Epoch: [34][335/817]	Loss 0.0117 (0.0296)	
training:	Epoch: [34][336/817]	Loss 0.0055 (0.0295)	
training:	Epoch: [34][337/817]	Loss 0.0062 (0.0295)	
training:	Epoch: [34][338/817]	Loss 0.0049 (0.0294)	
training:	Epoch: [34][339/817]	Loss 0.0103 (0.0293)	
training:	Epoch: [34][340/817]	Loss 0.0138 (0.0293)	
training:	Epoch: [34][341/817]	Loss 0.0074 (0.0292)	
training:	Epoch: [34][342/817]	Loss 0.0101 (0.0292)	
training:	Epoch: [34][343/817]	Loss 0.0048 (0.0291)	
training:	Epoch: [34][344/817]	Loss 0.0052 (0.0290)	
training:	Epoch: [34][345/817]	Loss 0.0061 (0.0290)	
training:	Epoch: [34][346/817]	Loss 1.3486 (0.0328)	
training:	Epoch: [34][347/817]	Loss 0.0070 (0.0327)	
training:	Epoch: [34][348/817]	Loss 0.0069 (0.0326)	
training:	Epoch: [34][349/817]	Loss 0.0069 (0.0325)	
training:	Epoch: [34][350/817]	Loss 0.0064 (0.0325)	
training:	Epoch: [34][351/817]	Loss 0.0049 (0.0324)	
training:	Epoch: [34][352/817]	Loss 0.0089 (0.0323)	
training:	Epoch: [34][353/817]	Loss 0.0080 (0.0323)	
training:	Epoch: [34][354/817]	Loss 0.0089 (0.0322)	
training:	Epoch: [34][355/817]	Loss 0.0064 (0.0321)	
training:	Epoch: [34][356/817]	Loss 0.0052 (0.0320)	
training:	Epoch: [34][357/817]	Loss 0.0072 (0.0320)	
training:	Epoch: [34][358/817]	Loss 0.0056 (0.0319)	
training:	Epoch: [34][359/817]	Loss 0.0079 (0.0318)	
training:	Epoch: [34][360/817]	Loss 0.0089 (0.0318)	
training:	Epoch: [34][361/817]	Loss 0.0088 (0.0317)	
training:	Epoch: [34][362/817]	Loss 0.0066 (0.0316)	
training:	Epoch: [34][363/817]	Loss 0.0081 (0.0316)	
training:	Epoch: [34][364/817]	Loss 0.0056 (0.0315)	
training:	Epoch: [34][365/817]	Loss 0.0060 (0.0314)	
training:	Epoch: [34][366/817]	Loss 0.0094 (0.0314)	
training:	Epoch: [34][367/817]	Loss 0.0071 (0.0313)	
training:	Epoch: [34][368/817]	Loss 0.0083 (0.0312)	
training:	Epoch: [34][369/817]	Loss 0.0075 (0.0312)	
training:	Epoch: [34][370/817]	Loss 0.0061 (0.0311)	
training:	Epoch: [34][371/817]	Loss 0.0148 (0.0311)	
training:	Epoch: [34][372/817]	Loss 0.0077 (0.0310)	
training:	Epoch: [34][373/817]	Loss 0.0069 (0.0309)	
training:	Epoch: [34][374/817]	Loss 0.0083 (0.0309)	
training:	Epoch: [34][375/817]	Loss 0.0091 (0.0308)	
training:	Epoch: [34][376/817]	Loss 0.0747 (0.0309)	
training:	Epoch: [34][377/817]	Loss 0.0069 (0.0309)	
training:	Epoch: [34][378/817]	Loss 0.0075 (0.0308)	
training:	Epoch: [34][379/817]	Loss 0.0060 (0.0307)	
training:	Epoch: [34][380/817]	Loss 0.5712 (0.0322)	
training:	Epoch: [34][381/817]	Loss 0.0076 (0.0321)	
training:	Epoch: [34][382/817]	Loss 0.0082 (0.0320)	
training:	Epoch: [34][383/817]	Loss 0.0069 (0.0320)	
training:	Epoch: [34][384/817]	Loss 0.0064 (0.0319)	
training:	Epoch: [34][385/817]	Loss 0.0080 (0.0318)	
training:	Epoch: [34][386/817]	Loss 0.0080 (0.0318)	
training:	Epoch: [34][387/817]	Loss 0.0092 (0.0317)	
training:	Epoch: [34][388/817]	Loss 0.0060 (0.0317)	
training:	Epoch: [34][389/817]	Loss 0.0058 (0.0316)	
training:	Epoch: [34][390/817]	Loss 0.0064 (0.0315)	
training:	Epoch: [34][391/817]	Loss 0.0081 (0.0315)	
training:	Epoch: [34][392/817]	Loss 0.0085 (0.0314)	
training:	Epoch: [34][393/817]	Loss 0.0062 (0.0313)	
training:	Epoch: [34][394/817]	Loss 0.0048 (0.0313)	
training:	Epoch: [34][395/817]	Loss 0.0066 (0.0312)	
training:	Epoch: [34][396/817]	Loss 0.0527 (0.0313)	
training:	Epoch: [34][397/817]	Loss 0.0059 (0.0312)	
training:	Epoch: [34][398/817]	Loss 0.0076 (0.0311)	
training:	Epoch: [34][399/817]	Loss 0.0090 (0.0311)	
training:	Epoch: [34][400/817]	Loss 0.0079 (0.0310)	
training:	Epoch: [34][401/817]	Loss 0.0071 (0.0310)	
training:	Epoch: [34][402/817]	Loss 0.0079 (0.0309)	
training:	Epoch: [34][403/817]	Loss 0.0073 (0.0309)	
training:	Epoch: [34][404/817]	Loss 0.0065 (0.0308)	
training:	Epoch: [34][405/817]	Loss 0.0062 (0.0307)	
training:	Epoch: [34][406/817]	Loss 0.0067 (0.0307)	
training:	Epoch: [34][407/817]	Loss 0.0115 (0.0306)	
training:	Epoch: [34][408/817]	Loss 0.5568 (0.0319)	
training:	Epoch: [34][409/817]	Loss 0.0052 (0.0319)	
training:	Epoch: [34][410/817]	Loss 0.0076 (0.0318)	
training:	Epoch: [34][411/817]	Loss 0.0075 (0.0317)	
training:	Epoch: [34][412/817]	Loss 0.0079 (0.0317)	
training:	Epoch: [34][413/817]	Loss 0.0058 (0.0316)	
training:	Epoch: [34][414/817]	Loss 0.0055 (0.0316)	
training:	Epoch: [34][415/817]	Loss 0.0058 (0.0315)	
training:	Epoch: [34][416/817]	Loss 0.0057 (0.0314)	
training:	Epoch: [34][417/817]	Loss 0.0406 (0.0315)	
training:	Epoch: [34][418/817]	Loss 0.0052 (0.0314)	
training:	Epoch: [34][419/817]	Loss 0.0155 (0.0314)	
training:	Epoch: [34][420/817]	Loss 0.0055 (0.0313)	
training:	Epoch: [34][421/817]	Loss 0.0054 (0.0312)	
training:	Epoch: [34][422/817]	Loss 0.0088 (0.0312)	
training:	Epoch: [34][423/817]	Loss 0.0053 (0.0311)	
training:	Epoch: [34][424/817]	Loss 0.5584 (0.0324)	
training:	Epoch: [34][425/817]	Loss 0.0091 (0.0323)	
training:	Epoch: [34][426/817]	Loss 0.0062 (0.0322)	
training:	Epoch: [34][427/817]	Loss 0.0056 (0.0322)	
training:	Epoch: [34][428/817]	Loss 0.0064 (0.0321)	
training:	Epoch: [34][429/817]	Loss 0.0064 (0.0321)	
training:	Epoch: [34][430/817]	Loss 0.0088 (0.0320)	
training:	Epoch: [34][431/817]	Loss 0.0059 (0.0319)	
training:	Epoch: [34][432/817]	Loss 0.0052 (0.0319)	
training:	Epoch: [34][433/817]	Loss 0.0051 (0.0318)	
training:	Epoch: [34][434/817]	Loss 0.0052 (0.0318)	
training:	Epoch: [34][435/817]	Loss 0.0071 (0.0317)	
training:	Epoch: [34][436/817]	Loss 0.0078 (0.0316)	
training:	Epoch: [34][437/817]	Loss 0.0068 (0.0316)	
training:	Epoch: [34][438/817]	Loss 0.0088 (0.0315)	
training:	Epoch: [34][439/817]	Loss 0.0050 (0.0315)	
training:	Epoch: [34][440/817]	Loss 0.0119 (0.0314)	
training:	Epoch: [34][441/817]	Loss 0.0063 (0.0314)	
training:	Epoch: [34][442/817]	Loss 0.0072 (0.0313)	
training:	Epoch: [34][443/817]	Loss 0.0054 (0.0313)	
training:	Epoch: [34][444/817]	Loss 0.0182 (0.0312)	
training:	Epoch: [34][445/817]	Loss 0.0067 (0.0312)	
training:	Epoch: [34][446/817]	Loss 0.0072 (0.0311)	
training:	Epoch: [34][447/817]	Loss 0.0102 (0.0311)	
training:	Epoch: [34][448/817]	Loss 0.0053 (0.0310)	
training:	Epoch: [34][449/817]	Loss 0.0079 (0.0310)	
training:	Epoch: [34][450/817]	Loss 0.0053 (0.0309)	
training:	Epoch: [34][451/817]	Loss 0.0059 (0.0309)	
training:	Epoch: [34][452/817]	Loss 0.0068 (0.0308)	
training:	Epoch: [34][453/817]	Loss 0.0112 (0.0308)	
training:	Epoch: [34][454/817]	Loss 0.0098 (0.0307)	
training:	Epoch: [34][455/817]	Loss 0.0126 (0.0307)	
training:	Epoch: [34][456/817]	Loss 0.0043 (0.0306)	
training:	Epoch: [34][457/817]	Loss 0.0056 (0.0306)	
training:	Epoch: [34][458/817]	Loss 0.0054 (0.0305)	
training:	Epoch: [34][459/817]	Loss 0.0047 (0.0305)	
training:	Epoch: [34][460/817]	Loss 0.0088 (0.0304)	
training:	Epoch: [34][461/817]	Loss 0.0073 (0.0304)	
training:	Epoch: [34][462/817]	Loss 0.0081 (0.0303)	
training:	Epoch: [34][463/817]	Loss 0.0060 (0.0303)	
training:	Epoch: [34][464/817]	Loss 0.0050 (0.0302)	
training:	Epoch: [34][465/817]	Loss 0.0057 (0.0301)	
training:	Epoch: [34][466/817]	Loss 0.0095 (0.0301)	
training:	Epoch: [34][467/817]	Loss 0.0060 (0.0300)	
training:	Epoch: [34][468/817]	Loss 0.0076 (0.0300)	
training:	Epoch: [34][469/817]	Loss 0.6204 (0.0313)	
training:	Epoch: [34][470/817]	Loss 0.0081 (0.0312)	
training:	Epoch: [34][471/817]	Loss 0.0064 (0.0312)	
training:	Epoch: [34][472/817]	Loss 0.0061 (0.0311)	
training:	Epoch: [34][473/817]	Loss 0.0055 (0.0311)	
training:	Epoch: [34][474/817]	Loss 0.0064 (0.0310)	
training:	Epoch: [34][475/817]	Loss 0.0056 (0.0309)	
training:	Epoch: [34][476/817]	Loss 0.0059 (0.0309)	
training:	Epoch: [34][477/817]	Loss 0.0068 (0.0308)	
training:	Epoch: [34][478/817]	Loss 0.0054 (0.0308)	
training:	Epoch: [34][479/817]	Loss 0.0070 (0.0307)	
training:	Epoch: [34][480/817]	Loss 0.0063 (0.0307)	
training:	Epoch: [34][481/817]	Loss 0.0463 (0.0307)	
training:	Epoch: [34][482/817]	Loss 0.0073 (0.0307)	
training:	Epoch: [34][483/817]	Loss 0.0966 (0.0308)	
training:	Epoch: [34][484/817]	Loss 0.0098 (0.0308)	
training:	Epoch: [34][485/817]	Loss 0.0078 (0.0307)	
training:	Epoch: [34][486/817]	Loss 0.0116 (0.0307)	
training:	Epoch: [34][487/817]	Loss 0.0085 (0.0306)	
training:	Epoch: [34][488/817]	Loss 0.0067 (0.0306)	
training:	Epoch: [34][489/817]	Loss 0.0076 (0.0305)	
training:	Epoch: [34][490/817]	Loss 0.0054 (0.0305)	
training:	Epoch: [34][491/817]	Loss 0.0059 (0.0304)	
training:	Epoch: [34][492/817]	Loss 0.0546 (0.0305)	
training:	Epoch: [34][493/817]	Loss 0.0073 (0.0304)	
training:	Epoch: [34][494/817]	Loss 0.2006 (0.0308)	
training:	Epoch: [34][495/817]	Loss 0.0104 (0.0307)	
training:	Epoch: [34][496/817]	Loss 0.0074 (0.0307)	
training:	Epoch: [34][497/817]	Loss 0.0080 (0.0306)	
training:	Epoch: [34][498/817]	Loss 0.0059 (0.0306)	
training:	Epoch: [34][499/817]	Loss 0.0066 (0.0306)	
training:	Epoch: [34][500/817]	Loss 0.0062 (0.0305)	
training:	Epoch: [34][501/817]	Loss 0.0071 (0.0305)	
training:	Epoch: [34][502/817]	Loss 0.0071 (0.0304)	
training:	Epoch: [34][503/817]	Loss 0.0059 (0.0304)	
training:	Epoch: [34][504/817]	Loss 0.0048 (0.0303)	
training:	Epoch: [34][505/817]	Loss 0.0051 (0.0303)	
training:	Epoch: [34][506/817]	Loss 0.0049 (0.0302)	
training:	Epoch: [34][507/817]	Loss 0.0051 (0.0302)	
training:	Epoch: [34][508/817]	Loss 0.0089 (0.0301)	
training:	Epoch: [34][509/817]	Loss 0.0059 (0.0301)	
training:	Epoch: [34][510/817]	Loss 0.0110 (0.0300)	
training:	Epoch: [34][511/817]	Loss 0.0065 (0.0300)	
training:	Epoch: [34][512/817]	Loss 0.0065 (0.0299)	
training:	Epoch: [34][513/817]	Loss 0.0060 (0.0299)	
training:	Epoch: [34][514/817]	Loss 0.0048 (0.0298)	
training:	Epoch: [34][515/817]	Loss 0.0048 (0.0298)	
training:	Epoch: [34][516/817]	Loss 0.0049 (0.0297)	
training:	Epoch: [34][517/817]	Loss 0.0057 (0.0297)	
training:	Epoch: [34][518/817]	Loss 0.5808 (0.0308)	
training:	Epoch: [34][519/817]	Loss 0.0060 (0.0307)	
training:	Epoch: [34][520/817]	Loss 0.0062 (0.0307)	
training:	Epoch: [34][521/817]	Loss 0.0078 (0.0306)	
training:	Epoch: [34][522/817]	Loss 0.0711 (0.0307)	
training:	Epoch: [34][523/817]	Loss 0.0055 (0.0307)	
training:	Epoch: [34][524/817]	Loss 0.0043 (0.0306)	
training:	Epoch: [34][525/817]	Loss 0.0052 (0.0306)	
training:	Epoch: [34][526/817]	Loss 0.0057 (0.0305)	
training:	Epoch: [34][527/817]	Loss 0.3808 (0.0312)	
training:	Epoch: [34][528/817]	Loss 0.0052 (0.0311)	
training:	Epoch: [34][529/817]	Loss 0.0057 (0.0311)	
training:	Epoch: [34][530/817]	Loss 0.0547 (0.0311)	
training:	Epoch: [34][531/817]	Loss 0.0099 (0.0311)	
training:	Epoch: [34][532/817]	Loss 0.0077 (0.0310)	
training:	Epoch: [34][533/817]	Loss 0.0051 (0.0310)	
training:	Epoch: [34][534/817]	Loss 0.5938 (0.0320)	
training:	Epoch: [34][535/817]	Loss 0.0060 (0.0320)	
training:	Epoch: [34][536/817]	Loss 0.0064 (0.0319)	
training:	Epoch: [34][537/817]	Loss 0.0059 (0.0319)	
training:	Epoch: [34][538/817]	Loss 0.1534 (0.0321)	
training:	Epoch: [34][539/817]	Loss 0.0068 (0.0321)	
training:	Epoch: [34][540/817]	Loss 0.0063 (0.0320)	
training:	Epoch: [34][541/817]	Loss 0.0101 (0.0320)	
training:	Epoch: [34][542/817]	Loss 0.0072 (0.0319)	
training:	Epoch: [34][543/817]	Loss 0.0066 (0.0319)	
training:	Epoch: [34][544/817]	Loss 0.4767 (0.0327)	
training:	Epoch: [34][545/817]	Loss 0.0079 (0.0327)	
training:	Epoch: [34][546/817]	Loss 0.0112 (0.0326)	
training:	Epoch: [34][547/817]	Loss 0.0080 (0.0326)	
training:	Epoch: [34][548/817]	Loss 0.0072 (0.0325)	
training:	Epoch: [34][549/817]	Loss 0.0415 (0.0326)	
training:	Epoch: [34][550/817]	Loss 0.0085 (0.0325)	
training:	Epoch: [34][551/817]	Loss 0.0326 (0.0325)	
training:	Epoch: [34][552/817]	Loss 0.1581 (0.0327)	
training:	Epoch: [34][553/817]	Loss 0.5105 (0.0336)	
training:	Epoch: [34][554/817]	Loss 0.2454 (0.0340)	
training:	Epoch: [34][555/817]	Loss 0.0068 (0.0339)	
training:	Epoch: [34][556/817]	Loss 0.0068 (0.0339)	
training:	Epoch: [34][557/817]	Loss 0.0062 (0.0338)	
training:	Epoch: [34][558/817]	Loss 0.0082 (0.0338)	
training:	Epoch: [34][559/817]	Loss 0.0055 (0.0337)	
training:	Epoch: [34][560/817]	Loss 0.0092 (0.0337)	
training:	Epoch: [34][561/817]	Loss 0.0080 (0.0337)	
training:	Epoch: [34][562/817]	Loss 0.0109 (0.0336)	
training:	Epoch: [34][563/817]	Loss 0.1892 (0.0339)	
training:	Epoch: [34][564/817]	Loss 0.0114 (0.0338)	
training:	Epoch: [34][565/817]	Loss 0.0054 (0.0338)	
training:	Epoch: [34][566/817]	Loss 0.0074 (0.0338)	
training:	Epoch: [34][567/817]	Loss 0.0057 (0.0337)	
training:	Epoch: [34][568/817]	Loss 0.0050 (0.0337)	
training:	Epoch: [34][569/817]	Loss 0.0113 (0.0336)	
training:	Epoch: [34][570/817]	Loss 0.0122 (0.0336)	
training:	Epoch: [34][571/817]	Loss 0.0138 (0.0335)	
training:	Epoch: [34][572/817]	Loss 0.0102 (0.0335)	
training:	Epoch: [34][573/817]	Loss 0.0074 (0.0335)	
training:	Epoch: [34][574/817]	Loss 0.0165 (0.0334)	
training:	Epoch: [34][575/817]	Loss 0.0069 (0.0334)	
training:	Epoch: [34][576/817]	Loss 0.0056 (0.0333)	
training:	Epoch: [34][577/817]	Loss 0.0056 (0.0333)	
training:	Epoch: [34][578/817]	Loss 0.0157 (0.0333)	
training:	Epoch: [34][579/817]	Loss 0.0071 (0.0332)	
training:	Epoch: [34][580/817]	Loss 0.0490 (0.0332)	
training:	Epoch: [34][581/817]	Loss 0.0067 (0.0332)	
training:	Epoch: [34][582/817]	Loss 0.0063 (0.0331)	
training:	Epoch: [34][583/817]	Loss 0.0101 (0.0331)	
training:	Epoch: [34][584/817]	Loss 0.5993 (0.0341)	
training:	Epoch: [34][585/817]	Loss 0.0106 (0.0340)	
training:	Epoch: [34][586/817]	Loss 0.0115 (0.0340)	
training:	Epoch: [34][587/817]	Loss 0.0089 (0.0340)	
training:	Epoch: [34][588/817]	Loss 0.5662 (0.0349)	
training:	Epoch: [34][589/817]	Loss 0.0090 (0.0348)	
training:	Epoch: [34][590/817]	Loss 0.0055 (0.0348)	
training:	Epoch: [34][591/817]	Loss 0.0093 (0.0347)	
training:	Epoch: [34][592/817]	Loss 0.0061 (0.0347)	
training:	Epoch: [34][593/817]	Loss 0.0168 (0.0346)	
training:	Epoch: [34][594/817]	Loss 0.0087 (0.0346)	
training:	Epoch: [34][595/817]	Loss 0.0071 (0.0346)	
training:	Epoch: [34][596/817]	Loss 0.0086 (0.0345)	
training:	Epoch: [34][597/817]	Loss 0.0113 (0.0345)	
training:	Epoch: [34][598/817]	Loss 0.0067 (0.0344)	
training:	Epoch: [34][599/817]	Loss 0.6229 (0.0354)	
training:	Epoch: [34][600/817]	Loss 0.5819 (0.0363)	
training:	Epoch: [34][601/817]	Loss 0.1556 (0.0365)	
training:	Epoch: [34][602/817]	Loss 0.0063 (0.0365)	
training:	Epoch: [34][603/817]	Loss 0.0094 (0.0364)	
training:	Epoch: [34][604/817]	Loss 0.0120 (0.0364)	
training:	Epoch: [34][605/817]	Loss 0.0095 (0.0363)	
training:	Epoch: [34][606/817]	Loss 0.0086 (0.0363)	
training:	Epoch: [34][607/817]	Loss 0.0068 (0.0362)	
training:	Epoch: [34][608/817]	Loss 0.0061 (0.0362)	
training:	Epoch: [34][609/817]	Loss 0.0066 (0.0361)	
training:	Epoch: [34][610/817]	Loss 0.0070 (0.0361)	
training:	Epoch: [34][611/817]	Loss 0.0072 (0.0360)	
training:	Epoch: [34][612/817]	Loss 0.0064 (0.0360)	
training:	Epoch: [34][613/817]	Loss 0.0071 (0.0360)	
training:	Epoch: [34][614/817]	Loss 0.0077 (0.0359)	
training:	Epoch: [34][615/817]	Loss 0.0109 (0.0359)	
training:	Epoch: [34][616/817]	Loss 0.0086 (0.0358)	
training:	Epoch: [34][617/817]	Loss 0.0075 (0.0358)	
training:	Epoch: [34][618/817]	Loss 0.0058 (0.0357)	
training:	Epoch: [34][619/817]	Loss 0.0069 (0.0357)	
training:	Epoch: [34][620/817]	Loss 0.0084 (0.0356)	
training:	Epoch: [34][621/817]	Loss 0.0067 (0.0356)	
training:	Epoch: [34][622/817]	Loss 0.0065 (0.0355)	
training:	Epoch: [34][623/817]	Loss 0.0060 (0.0355)	
training:	Epoch: [34][624/817]	Loss 0.0060 (0.0354)	
training:	Epoch: [34][625/817]	Loss 0.0084 (0.0354)	
training:	Epoch: [34][626/817]	Loss 0.0097 (0.0354)	
training:	Epoch: [34][627/817]	Loss 0.0056 (0.0353)	
training:	Epoch: [34][628/817]	Loss 0.0074 (0.0353)	
training:	Epoch: [34][629/817]	Loss 0.6320 (0.0362)	
training:	Epoch: [34][630/817]	Loss 0.0055 (0.0362)	
training:	Epoch: [34][631/817]	Loss 0.0097 (0.0361)	
training:	Epoch: [34][632/817]	Loss 0.0146 (0.0361)	
training:	Epoch: [34][633/817]	Loss 0.0072 (0.0360)	
training:	Epoch: [34][634/817]	Loss 0.0125 (0.0360)	
training:	Epoch: [34][635/817]	Loss 0.0222 (0.0360)	
training:	Epoch: [34][636/817]	Loss 0.0075 (0.0359)	
training:	Epoch: [34][637/817]	Loss 0.0220 (0.0359)	
training:	Epoch: [34][638/817]	Loss 0.6285 (0.0369)	
training:	Epoch: [34][639/817]	Loss 0.0070 (0.0368)	
training:	Epoch: [34][640/817]	Loss 0.0094 (0.0368)	
training:	Epoch: [34][641/817]	Loss 0.0056 (0.0367)	
training:	Epoch: [34][642/817]	Loss 0.0054 (0.0367)	
training:	Epoch: [34][643/817]	Loss 0.0060 (0.0366)	
training:	Epoch: [34][644/817]	Loss 0.0086 (0.0366)	
training:	Epoch: [34][645/817]	Loss 0.0054 (0.0365)	
training:	Epoch: [34][646/817]	Loss 0.0848 (0.0366)	
training:	Epoch: [34][647/817]	Loss 0.0129 (0.0366)	
training:	Epoch: [34][648/817]	Loss 0.0069 (0.0365)	
training:	Epoch: [34][649/817]	Loss 0.0125 (0.0365)	
training:	Epoch: [34][650/817]	Loss 0.0060 (0.0364)	
training:	Epoch: [34][651/817]	Loss 0.0159 (0.0364)	
training:	Epoch: [34][652/817]	Loss 0.1085 (0.0365)	
training:	Epoch: [34][653/817]	Loss 0.0086 (0.0365)	
training:	Epoch: [34][654/817]	Loss 0.0139 (0.0364)	
training:	Epoch: [34][655/817]	Loss 0.0086 (0.0364)	
training:	Epoch: [34][656/817]	Loss 0.0593 (0.0364)	
training:	Epoch: [34][657/817]	Loss 0.5015 (0.0371)	
training:	Epoch: [34][658/817]	Loss 0.0074 (0.0371)	
training:	Epoch: [34][659/817]	Loss 0.0058 (0.0370)	
training:	Epoch: [34][660/817]	Loss 0.0056 (0.0370)	
training:	Epoch: [34][661/817]	Loss 0.0078 (0.0370)	
training:	Epoch: [34][662/817]	Loss 0.0110 (0.0369)	
training:	Epoch: [34][663/817]	Loss 0.0114 (0.0369)	
training:	Epoch: [34][664/817]	Loss 0.6384 (0.0378)	
training:	Epoch: [34][665/817]	Loss 0.0058 (0.0377)	
training:	Epoch: [34][666/817]	Loss 0.5440 (0.0385)	
training:	Epoch: [34][667/817]	Loss 0.0060 (0.0384)	
training:	Epoch: [34][668/817]	Loss 0.0091 (0.0384)	
training:	Epoch: [34][669/817]	Loss 0.0090 (0.0384)	
training:	Epoch: [34][670/817]	Loss 0.0071 (0.0383)	
training:	Epoch: [34][671/817]	Loss 0.4498 (0.0389)	
training:	Epoch: [34][672/817]	Loss 0.0107 (0.0389)	
training:	Epoch: [34][673/817]	Loss 0.0078 (0.0388)	
training:	Epoch: [34][674/817]	Loss 0.2226 (0.0391)	
training:	Epoch: [34][675/817]	Loss 0.0131 (0.0391)	
training:	Epoch: [34][676/817]	Loss 0.0058 (0.0390)	
training:	Epoch: [34][677/817]	Loss 0.0232 (0.0390)	
training:	Epoch: [34][678/817]	Loss 0.6649 (0.0399)	
training:	Epoch: [34][679/817]	Loss 0.0068 (0.0399)	
training:	Epoch: [34][680/817]	Loss 0.0088 (0.0398)	
training:	Epoch: [34][681/817]	Loss 0.0088 (0.0398)	
training:	Epoch: [34][682/817]	Loss 0.0111 (0.0397)	
training:	Epoch: [34][683/817]	Loss 0.0076 (0.0397)	
training:	Epoch: [34][684/817]	Loss 0.0070 (0.0396)	
training:	Epoch: [34][685/817]	Loss 0.0085 (0.0396)	
training:	Epoch: [34][686/817]	Loss 0.5008 (0.0403)	
training:	Epoch: [34][687/817]	Loss 0.0076 (0.0402)	
training:	Epoch: [34][688/817]	Loss 0.0140 (0.0402)	
training:	Epoch: [34][689/817]	Loss 0.0079 (0.0401)	
training:	Epoch: [34][690/817]	Loss 0.0167 (0.0401)	
training:	Epoch: [34][691/817]	Loss 0.0082 (0.0401)	
training:	Epoch: [34][692/817]	Loss 0.0105 (0.0400)	
training:	Epoch: [34][693/817]	Loss 0.0067 (0.0400)	
training:	Epoch: [34][694/817]	Loss 0.0220 (0.0399)	
training:	Epoch: [34][695/817]	Loss 0.0104 (0.0399)	
training:	Epoch: [34][696/817]	Loss 0.0064 (0.0398)	
training:	Epoch: [34][697/817]	Loss 0.0078 (0.0398)	
training:	Epoch: [34][698/817]	Loss 0.0066 (0.0398)	
training:	Epoch: [34][699/817]	Loss 0.0071 (0.0397)	
training:	Epoch: [34][700/817]	Loss 0.0078 (0.0397)	
training:	Epoch: [34][701/817]	Loss 0.0090 (0.0396)	
training:	Epoch: [34][702/817]	Loss 0.0133 (0.0396)	
training:	Epoch: [34][703/817]	Loss 0.0081 (0.0395)	
training:	Epoch: [34][704/817]	Loss 0.0080 (0.0395)	
training:	Epoch: [34][705/817]	Loss 0.1115 (0.0396)	
training:	Epoch: [34][706/817]	Loss 0.0064 (0.0395)	
training:	Epoch: [34][707/817]	Loss 0.0112 (0.0395)	
training:	Epoch: [34][708/817]	Loss 0.0167 (0.0395)	
training:	Epoch: [34][709/817]	Loss 0.0059 (0.0394)	
training:	Epoch: [34][710/817]	Loss 0.0068 (0.0394)	
training:	Epoch: [34][711/817]	Loss 0.0069 (0.0393)	
training:	Epoch: [34][712/817]	Loss 0.0096 (0.0393)	
training:	Epoch: [34][713/817]	Loss 0.0051 (0.0392)	
training:	Epoch: [34][714/817]	Loss 0.0062 (0.0392)	
training:	Epoch: [34][715/817]	Loss 0.0067 (0.0392)	
training:	Epoch: [34][716/817]	Loss 0.0085 (0.0391)	
training:	Epoch: [34][717/817]	Loss 0.0063 (0.0391)	
training:	Epoch: [34][718/817]	Loss 0.0066 (0.0390)	
training:	Epoch: [34][719/817]	Loss 0.0070 (0.0390)	
training:	Epoch: [34][720/817]	Loss 0.0081 (0.0389)	
training:	Epoch: [34][721/817]	Loss 0.0043 (0.0389)	
training:	Epoch: [34][722/817]	Loss 0.0058 (0.0388)	
training:	Epoch: [34][723/817]	Loss 0.0050 (0.0388)	
training:	Epoch: [34][724/817]	Loss 0.0097 (0.0388)	
training:	Epoch: [34][725/817]	Loss 0.0215 (0.0387)	
training:	Epoch: [34][726/817]	Loss 0.0059 (0.0387)	
training:	Epoch: [34][727/817]	Loss 0.0086 (0.0386)	
training:	Epoch: [34][728/817]	Loss 0.0062 (0.0386)	
training:	Epoch: [34][729/817]	Loss 0.0059 (0.0386)	
training:	Epoch: [34][730/817]	Loss 0.0069 (0.0385)	
training:	Epoch: [34][731/817]	Loss 0.0062 (0.0385)	
training:	Epoch: [34][732/817]	Loss 0.0072 (0.0384)	
training:	Epoch: [34][733/817]	Loss 0.0064 (0.0384)	
training:	Epoch: [34][734/817]	Loss 0.0098 (0.0383)	
training:	Epoch: [34][735/817]	Loss 0.0070 (0.0383)	
training:	Epoch: [34][736/817]	Loss 0.0082 (0.0383)	
training:	Epoch: [34][737/817]	Loss 0.0074 (0.0382)	
training:	Epoch: [34][738/817]	Loss 0.0054 (0.0382)	
training:	Epoch: [34][739/817]	Loss 0.0070 (0.0381)	
training:	Epoch: [34][740/817]	Loss 0.0058 (0.0381)	
training:	Epoch: [34][741/817]	Loss 0.2183 (0.0383)	
training:	Epoch: [34][742/817]	Loss 0.0277 (0.0383)	
training:	Epoch: [34][743/817]	Loss 0.0051 (0.0383)	
training:	Epoch: [34][744/817]	Loss 0.0076 (0.0382)	
training:	Epoch: [34][745/817]	Loss 0.0070 (0.0382)	
training:	Epoch: [34][746/817]	Loss 0.0079 (0.0381)	
training:	Epoch: [34][747/817]	Loss 0.0068 (0.0381)	
training:	Epoch: [34][748/817]	Loss 0.0089 (0.0381)	
training:	Epoch: [34][749/817]	Loss 0.0063 (0.0380)	
training:	Epoch: [34][750/817]	Loss 0.0085 (0.0380)	
training:	Epoch: [34][751/817]	Loss 0.0064 (0.0379)	
training:	Epoch: [34][752/817]	Loss 0.0073 (0.0379)	
training:	Epoch: [34][753/817]	Loss 0.0067 (0.0379)	
training:	Epoch: [34][754/817]	Loss 0.0082 (0.0378)	
training:	Epoch: [34][755/817]	Loss 0.0069 (0.0378)	
training:	Epoch: [34][756/817]	Loss 0.0053 (0.0377)	
training:	Epoch: [34][757/817]	Loss 0.0080 (0.0377)	
training:	Epoch: [34][758/817]	Loss 0.6292 (0.0385)	
training:	Epoch: [34][759/817]	Loss 0.0096 (0.0384)	
training:	Epoch: [34][760/817]	Loss 0.5286 (0.0391)	
training:	Epoch: [34][761/817]	Loss 0.0080 (0.0390)	
training:	Epoch: [34][762/817]	Loss 0.0090 (0.0390)	
training:	Epoch: [34][763/817]	Loss 0.0069 (0.0390)	
training:	Epoch: [34][764/817]	Loss 0.0077 (0.0389)	
training:	Epoch: [34][765/817]	Loss 0.0058 (0.0389)	
training:	Epoch: [34][766/817]	Loss 0.0374 (0.0389)	
training:	Epoch: [34][767/817]	Loss 0.0068 (0.0388)	
training:	Epoch: [34][768/817]	Loss 0.0059 (0.0388)	
training:	Epoch: [34][769/817]	Loss 0.0060 (0.0387)	
training:	Epoch: [34][770/817]	Loss 0.0062 (0.0387)	
training:	Epoch: [34][771/817]	Loss 0.0111 (0.0387)	
training:	Epoch: [34][772/817]	Loss 0.0069 (0.0386)	
training:	Epoch: [34][773/817]	Loss 0.0099 (0.0386)	
training:	Epoch: [34][774/817]	Loss 0.0087 (0.0386)	
training:	Epoch: [34][775/817]	Loss 0.2538 (0.0388)	
training:	Epoch: [34][776/817]	Loss 0.0060 (0.0388)	
training:	Epoch: [34][777/817]	Loss 0.0077 (0.0387)	
training:	Epoch: [34][778/817]	Loss 0.0130 (0.0387)	
training:	Epoch: [34][779/817]	Loss 0.0058 (0.0387)	
training:	Epoch: [34][780/817]	Loss 0.0094 (0.0386)	
training:	Epoch: [34][781/817]	Loss 0.0058 (0.0386)	
training:	Epoch: [34][782/817]	Loss 0.0082 (0.0386)	
training:	Epoch: [34][783/817]	Loss 0.0058 (0.0385)	
training:	Epoch: [34][784/817]	Loss 0.0074 (0.0385)	
training:	Epoch: [34][785/817]	Loss 0.0070 (0.0384)	
training:	Epoch: [34][786/817]	Loss 0.0055 (0.0384)	
training:	Epoch: [34][787/817]	Loss 0.0083 (0.0384)	
training:	Epoch: [34][788/817]	Loss 0.0057 (0.0383)	
training:	Epoch: [34][789/817]	Loss 0.0071 (0.0383)	
training:	Epoch: [34][790/817]	Loss 0.0059 (0.0382)	
training:	Epoch: [34][791/817]	Loss 0.0080 (0.0382)	
training:	Epoch: [34][792/817]	Loss 0.0172 (0.0382)	
training:	Epoch: [34][793/817]	Loss 0.0081 (0.0381)	
training:	Epoch: [34][794/817]	Loss 0.0068 (0.0381)	
training:	Epoch: [34][795/817]	Loss 0.0060 (0.0380)	
training:	Epoch: [34][796/817]	Loss 0.0060 (0.0380)	
training:	Epoch: [34][797/817]	Loss 0.0101 (0.0380)	
training:	Epoch: [34][798/817]	Loss 0.0060 (0.0379)	
training:	Epoch: [34][799/817]	Loss 0.6483 (0.0387)	
training:	Epoch: [34][800/817]	Loss 0.0075 (0.0387)	
training:	Epoch: [34][801/817]	Loss 0.0063 (0.0386)	
training:	Epoch: [34][802/817]	Loss 0.0070 (0.0386)	
training:	Epoch: [34][803/817]	Loss 0.0061 (0.0385)	
training:	Epoch: [34][804/817]	Loss 0.0061 (0.0385)	
training:	Epoch: [34][805/817]	Loss 0.5221 (0.0391)	
training:	Epoch: [34][806/817]	Loss 0.0122 (0.0391)	
training:	Epoch: [34][807/817]	Loss 0.0084 (0.0390)	
training:	Epoch: [34][808/817]	Loss 0.0061 (0.0390)	
training:	Epoch: [34][809/817]	Loss 0.0070 (0.0389)	
training:	Epoch: [34][810/817]	Loss 0.0195 (0.0389)	
training:	Epoch: [34][811/817]	Loss 0.0063 (0.0389)	
training:	Epoch: [34][812/817]	Loss 0.0173 (0.0389)	
training:	Epoch: [34][813/817]	Loss 0.0072 (0.0388)	
training:	Epoch: [34][814/817]	Loss 0.0097 (0.0388)	
training:	Epoch: [34][815/817]	Loss 0.0093 (0.0387)	
training:	Epoch: [34][816/817]	Loss 0.0092 (0.0387)	
training:	Epoch: [34][817/817]	Loss 0.0074 (0.0387)	
Training:	 Loss: 0.0387

Training:	 ACC: 0.9902 0.9901 0.9868 0.9936
Validation:	 ACC: 0.7867 0.7844 0.7349 0.8386
Validation:	 Best_BACC: 0.8223 0.8218 0.8127 0.8318
Validation:	 Loss: 0.9657
Pretraining:	Epoch 35/200
----------
training:	Epoch: [35][1/817]	Loss 0.0058 (0.0058)	
training:	Epoch: [35][2/817]	Loss 0.0054 (0.0056)	
training:	Epoch: [35][3/817]	Loss 0.0076 (0.0063)	
training:	Epoch: [35][4/817]	Loss 0.0095 (0.0071)	
training:	Epoch: [35][5/817]	Loss 0.0054 (0.0068)	
training:	Epoch: [35][6/817]	Loss 0.3186 (0.0587)	
training:	Epoch: [35][7/817]	Loss 0.0057 (0.0512)	
training:	Epoch: [35][8/817]	Loss 0.0067 (0.0456)	
training:	Epoch: [35][9/817]	Loss 0.0159 (0.0423)	
training:	Epoch: [35][10/817]	Loss 0.0057 (0.0386)	
training:	Epoch: [35][11/817]	Loss 0.5332 (0.0836)	
training:	Epoch: [35][12/817]	Loss 0.0070 (0.0772)	
training:	Epoch: [35][13/817]	Loss 0.0073 (0.0718)	
training:	Epoch: [35][14/817]	Loss 0.0075 (0.0672)	
training:	Epoch: [35][15/817]	Loss 0.0066 (0.0632)	
training:	Epoch: [35][16/817]	Loss 0.0058 (0.0596)	
training:	Epoch: [35][17/817]	Loss 0.0062 (0.0565)	
training:	Epoch: [35][18/817]	Loss 0.0075 (0.0537)	
training:	Epoch: [35][19/817]	Loss 0.5048 (0.0775)	
training:	Epoch: [35][20/817]	Loss 0.0085 (0.0740)	
training:	Epoch: [35][21/817]	Loss 0.0105 (0.0710)	
training:	Epoch: [35][22/817]	Loss 0.0064 (0.0681)	
training:	Epoch: [35][23/817]	Loss 0.0052 (0.0653)	
training:	Epoch: [35][24/817]	Loss 0.0058 (0.0629)	
training:	Epoch: [35][25/817]	Loss 0.0854 (0.0638)	
training:	Epoch: [35][26/817]	Loss 0.0097 (0.0617)	
training:	Epoch: [35][27/817]	Loss 0.0107 (0.0598)	
training:	Epoch: [35][28/817]	Loss 0.0061 (0.0579)	
training:	Epoch: [35][29/817]	Loss 0.5309 (0.0742)	
training:	Epoch: [35][30/817]	Loss 0.0082 (0.0720)	
training:	Epoch: [35][31/817]	Loss 0.0178 (0.0702)	
training:	Epoch: [35][32/817]	Loss 0.0069 (0.0683)	
training:	Epoch: [35][33/817]	Loss 0.5050 (0.0815)	
training:	Epoch: [35][34/817]	Loss 0.0062 (0.0793)	
training:	Epoch: [35][35/817]	Loss 0.0059 (0.0772)	
training:	Epoch: [35][36/817]	Loss 0.0054 (0.0752)	
training:	Epoch: [35][37/817]	Loss 0.0057 (0.0733)	
training:	Epoch: [35][38/817]	Loss 0.0215 (0.0719)	
training:	Epoch: [35][39/817]	Loss 0.0079 (0.0703)	
training:	Epoch: [35][40/817]	Loss 0.0090 (0.0688)	
training:	Epoch: [35][41/817]	Loss 0.0068 (0.0673)	
training:	Epoch: [35][42/817]	Loss 0.0072 (0.0658)	
training:	Epoch: [35][43/817]	Loss 0.0096 (0.0645)	
training:	Epoch: [35][44/817]	Loss 0.0071 (0.0632)	
training:	Epoch: [35][45/817]	Loss 0.0066 (0.0620)	
training:	Epoch: [35][46/817]	Loss 0.0073 (0.0608)	
training:	Epoch: [35][47/817]	Loss 0.0213 (0.0599)	
training:	Epoch: [35][48/817]	Loss 0.0310 (0.0593)	
training:	Epoch: [35][49/817]	Loss 0.0064 (0.0582)	
training:	Epoch: [35][50/817]	Loss 0.0106 (0.0573)	
training:	Epoch: [35][51/817]	Loss 0.0109 (0.0564)	
training:	Epoch: [35][52/817]	Loss 0.0097 (0.0555)	
training:	Epoch: [35][53/817]	Loss 0.0338 (0.0551)	
training:	Epoch: [35][54/817]	Loss 0.0085 (0.0542)	
training:	Epoch: [35][55/817]	Loss 0.0138 (0.0535)	
training:	Epoch: [35][56/817]	Loss 0.0081 (0.0527)	
training:	Epoch: [35][57/817]	Loss 0.0113 (0.0519)	
training:	Epoch: [35][58/817]	Loss 0.0065 (0.0512)	
training:	Epoch: [35][59/817]	Loss 0.0071 (0.0504)	
training:	Epoch: [35][60/817]	Loss 0.0077 (0.0497)	
training:	Epoch: [35][61/817]	Loss 0.0065 (0.0490)	
training:	Epoch: [35][62/817]	Loss 0.0104 (0.0484)	
training:	Epoch: [35][63/817]	Loss 0.0126 (0.0478)	
training:	Epoch: [35][64/817]	Loss 0.0076 (0.0472)	
training:	Epoch: [35][65/817]	Loss 0.0126 (0.0466)	
training:	Epoch: [35][66/817]	Loss 0.0062 (0.0460)	
training:	Epoch: [35][67/817]	Loss 0.0052 (0.0454)	
training:	Epoch: [35][68/817]	Loss 0.0058 (0.0448)	
training:	Epoch: [35][69/817]	Loss 0.0120 (0.0444)	
training:	Epoch: [35][70/817]	Loss 0.0093 (0.0439)	
training:	Epoch: [35][71/817]	Loss 0.0074 (0.0433)	
training:	Epoch: [35][72/817]	Loss 0.0060 (0.0428)	
training:	Epoch: [35][73/817]	Loss 0.0062 (0.0423)	
training:	Epoch: [35][74/817]	Loss 0.0076 (0.0419)	
training:	Epoch: [35][75/817]	Loss 0.0076 (0.0414)	
training:	Epoch: [35][76/817]	Loss 0.0085 (0.0410)	
training:	Epoch: [35][77/817]	Loss 0.0092 (0.0406)	
training:	Epoch: [35][78/817]	Loss 0.0060 (0.0401)	
training:	Epoch: [35][79/817]	Loss 0.6421 (0.0477)	
training:	Epoch: [35][80/817]	Loss 0.0141 (0.0473)	
training:	Epoch: [35][81/817]	Loss 0.0092 (0.0468)	
training:	Epoch: [35][82/817]	Loss 0.0098 (0.0464)	
training:	Epoch: [35][83/817]	Loss 0.0078 (0.0459)	
training:	Epoch: [35][84/817]	Loss 0.0184 (0.0456)	
training:	Epoch: [35][85/817]	Loss 0.0066 (0.0451)	
training:	Epoch: [35][86/817]	Loss 0.0109 (0.0447)	
training:	Epoch: [35][87/817]	Loss 0.0095 (0.0443)	
training:	Epoch: [35][88/817]	Loss 0.0082 (0.0439)	
training:	Epoch: [35][89/817]	Loss 0.0075 (0.0435)	
training:	Epoch: [35][90/817]	Loss 0.0078 (0.0431)	
training:	Epoch: [35][91/817]	Loss 0.0062 (0.0427)	
training:	Epoch: [35][92/817]	Loss 0.0047 (0.0423)	
training:	Epoch: [35][93/817]	Loss 0.0059 (0.0419)	
training:	Epoch: [35][94/817]	Loss 0.0082 (0.0416)	
training:	Epoch: [35][95/817]	Loss 0.0066 (0.0412)	
training:	Epoch: [35][96/817]	Loss 0.0080 (0.0408)	
training:	Epoch: [35][97/817]	Loss 0.0072 (0.0405)	
training:	Epoch: [35][98/817]	Loss 0.0058 (0.0401)	
training:	Epoch: [35][99/817]	Loss 0.0052 (0.0398)	
training:	Epoch: [35][100/817]	Loss 0.0052 (0.0394)	
training:	Epoch: [35][101/817]	Loss 0.0058 (0.0391)	
training:	Epoch: [35][102/817]	Loss 0.0086 (0.0388)	
training:	Epoch: [35][103/817]	Loss 0.0073 (0.0385)	
training:	Epoch: [35][104/817]	Loss 0.0054 (0.0382)	
training:	Epoch: [35][105/817]	Loss 0.0067 (0.0379)	
training:	Epoch: [35][106/817]	Loss 0.0074 (0.0376)	
training:	Epoch: [35][107/817]	Loss 0.0087 (0.0373)	
training:	Epoch: [35][108/817]	Loss 0.0070 (0.0370)	
training:	Epoch: [35][109/817]	Loss 0.0075 (0.0368)	
training:	Epoch: [35][110/817]	Loss 0.0070 (0.0365)	
training:	Epoch: [35][111/817]	Loss 0.0066 (0.0362)	
training:	Epoch: [35][112/817]	Loss 0.0078 (0.0360)	
training:	Epoch: [35][113/817]	Loss 0.0055 (0.0357)	
training:	Epoch: [35][114/817]	Loss 0.0070 (0.0355)	
training:	Epoch: [35][115/817]	Loss 0.0074 (0.0352)	
training:	Epoch: [35][116/817]	Loss 0.0073 (0.0350)	
training:	Epoch: [35][117/817]	Loss 0.0064 (0.0347)	
training:	Epoch: [35][118/817]	Loss 0.0113 (0.0345)	
training:	Epoch: [35][119/817]	Loss 0.0078 (0.0343)	
training:	Epoch: [35][120/817]	Loss 0.0078 (0.0341)	
training:	Epoch: [35][121/817]	Loss 0.0048 (0.0338)	
training:	Epoch: [35][122/817]	Loss 0.0073 (0.0336)	
training:	Epoch: [35][123/817]	Loss 0.0073 (0.0334)	
training:	Epoch: [35][124/817]	Loss 0.0072 (0.0332)	
training:	Epoch: [35][125/817]	Loss 0.0050 (0.0330)	
training:	Epoch: [35][126/817]	Loss 0.0064 (0.0328)	
training:	Epoch: [35][127/817]	Loss 0.0076 (0.0326)	
training:	Epoch: [35][128/817]	Loss 0.6283 (0.0372)	
training:	Epoch: [35][129/817]	Loss 0.0057 (0.0370)	
training:	Epoch: [35][130/817]	Loss 0.0473 (0.0371)	
training:	Epoch: [35][131/817]	Loss 0.0055 (0.0368)	
training:	Epoch: [35][132/817]	Loss 0.0052 (0.0366)	
training:	Epoch: [35][133/817]	Loss 0.0078 (0.0364)	
training:	Epoch: [35][134/817]	Loss 0.0057 (0.0361)	
training:	Epoch: [35][135/817]	Loss 0.0065 (0.0359)	
training:	Epoch: [35][136/817]	Loss 0.0078 (0.0357)	
training:	Epoch: [35][137/817]	Loss 0.0057 (0.0355)	
training:	Epoch: [35][138/817]	Loss 0.6269 (0.0398)	
training:	Epoch: [35][139/817]	Loss 0.0799 (0.0401)	
training:	Epoch: [35][140/817]	Loss 0.0078 (0.0398)	
training:	Epoch: [35][141/817]	Loss 0.0047 (0.0396)	
training:	Epoch: [35][142/817]	Loss 0.4732 (0.0426)	
training:	Epoch: [35][143/817]	Loss 0.4841 (0.0457)	
training:	Epoch: [35][144/817]	Loss 0.0051 (0.0454)	
training:	Epoch: [35][145/817]	Loss 0.0065 (0.0452)	
training:	Epoch: [35][146/817]	Loss 0.0081 (0.0449)	
training:	Epoch: [35][147/817]	Loss 0.0086 (0.0447)	
training:	Epoch: [35][148/817]	Loss 0.0075 (0.0444)	
training:	Epoch: [35][149/817]	Loss 0.0077 (0.0442)	
training:	Epoch: [35][150/817]	Loss 0.0072 (0.0439)	
training:	Epoch: [35][151/817]	Loss 0.5497 (0.0473)	
training:	Epoch: [35][152/817]	Loss 0.0081 (0.0470)	
training:	Epoch: [35][153/817]	Loss 0.0071 (0.0468)	
training:	Epoch: [35][154/817]	Loss 0.0071 (0.0465)	
training:	Epoch: [35][155/817]	Loss 0.0065 (0.0462)	
training:	Epoch: [35][156/817]	Loss 0.0084 (0.0460)	
training:	Epoch: [35][157/817]	Loss 0.0066 (0.0457)	
training:	Epoch: [35][158/817]	Loss 0.0066 (0.0455)	
training:	Epoch: [35][159/817]	Loss 0.0062 (0.0452)	
training:	Epoch: [35][160/817]	Loss 0.0079 (0.0450)	
training:	Epoch: [35][161/817]	Loss 0.0077 (0.0448)	
training:	Epoch: [35][162/817]	Loss 0.0054 (0.0445)	
training:	Epoch: [35][163/817]	Loss 0.5244 (0.0475)	
training:	Epoch: [35][164/817]	Loss 0.0078 (0.0472)	
training:	Epoch: [35][165/817]	Loss 0.0079 (0.0470)	
training:	Epoch: [35][166/817]	Loss 0.0049 (0.0468)	
training:	Epoch: [35][167/817]	Loss 0.0063 (0.0465)	
training:	Epoch: [35][168/817]	Loss 0.0076 (0.0463)	
training:	Epoch: [35][169/817]	Loss 0.0072 (0.0460)	
training:	Epoch: [35][170/817]	Loss 0.0118 (0.0458)	
training:	Epoch: [35][171/817]	Loss 0.0186 (0.0457)	
training:	Epoch: [35][172/817]	Loss 0.0083 (0.0455)	
training:	Epoch: [35][173/817]	Loss 0.0093 (0.0453)	
training:	Epoch: [35][174/817]	Loss 0.0081 (0.0450)	
training:	Epoch: [35][175/817]	Loss 0.0088 (0.0448)	
training:	Epoch: [35][176/817]	Loss 0.0085 (0.0446)	
training:	Epoch: [35][177/817]	Loss 0.0085 (0.0444)	
training:	Epoch: [35][178/817]	Loss 0.0084 (0.0442)	
training:	Epoch: [35][179/817]	Loss 0.0057 (0.0440)	
training:	Epoch: [35][180/817]	Loss 0.0082 (0.0438)	
training:	Epoch: [35][181/817]	Loss 0.0082 (0.0436)	
training:	Epoch: [35][182/817]	Loss 0.0097 (0.0434)	
training:	Epoch: [35][183/817]	Loss 0.0098 (0.0432)	
training:	Epoch: [35][184/817]	Loss 0.0042 (0.0430)	
training:	Epoch: [35][185/817]	Loss 0.0075 (0.0428)	
training:	Epoch: [35][186/817]	Loss 0.0095 (0.0427)	
training:	Epoch: [35][187/817]	Loss 0.0051 (0.0425)	
training:	Epoch: [35][188/817]	Loss 0.0063 (0.0423)	
training:	Epoch: [35][189/817]	Loss 0.0320 (0.0422)	
training:	Epoch: [35][190/817]	Loss 0.0086 (0.0420)	
training:	Epoch: [35][191/817]	Loss 0.0064 (0.0418)	
training:	Epoch: [35][192/817]	Loss 0.0059 (0.0417)	
training:	Epoch: [35][193/817]	Loss 0.0043 (0.0415)	
training:	Epoch: [35][194/817]	Loss 0.0080 (0.0413)	
training:	Epoch: [35][195/817]	Loss 0.0068 (0.0411)	
training:	Epoch: [35][196/817]	Loss 0.0058 (0.0409)	
training:	Epoch: [35][197/817]	Loss 0.0111 (0.0408)	
training:	Epoch: [35][198/817]	Loss 0.0062 (0.0406)	
training:	Epoch: [35][199/817]	Loss 0.0071 (0.0404)	
training:	Epoch: [35][200/817]	Loss 0.0074 (0.0403)	
training:	Epoch: [35][201/817]	Loss 0.0081 (0.0401)	
training:	Epoch: [35][202/817]	Loss 0.0102 (0.0400)	
training:	Epoch: [35][203/817]	Loss 0.0072 (0.0398)	
training:	Epoch: [35][204/817]	Loss 0.0082 (0.0397)	
training:	Epoch: [35][205/817]	Loss 0.0125 (0.0395)	
training:	Epoch: [35][206/817]	Loss 0.4797 (0.0417)	
training:	Epoch: [35][207/817]	Loss 0.0053 (0.0415)	
training:	Epoch: [35][208/817]	Loss 0.0332 (0.0414)	
training:	Epoch: [35][209/817]	Loss 0.5680 (0.0440)	
training:	Epoch: [35][210/817]	Loss 0.0056 (0.0438)	
training:	Epoch: [35][211/817]	Loss 0.0101 (0.0436)	
training:	Epoch: [35][212/817]	Loss 0.0097 (0.0435)	
training:	Epoch: [35][213/817]	Loss 0.0069 (0.0433)	
training:	Epoch: [35][214/817]	Loss 0.0075 (0.0431)	
training:	Epoch: [35][215/817]	Loss 0.0043 (0.0429)	
training:	Epoch: [35][216/817]	Loss 0.0102 (0.0428)	
training:	Epoch: [35][217/817]	Loss 0.0067 (0.0426)	
training:	Epoch: [35][218/817]	Loss 0.0091 (0.0425)	
training:	Epoch: [35][219/817]	Loss 0.0093 (0.0423)	
training:	Epoch: [35][220/817]	Loss 0.0067 (0.0422)	
training:	Epoch: [35][221/817]	Loss 0.0083 (0.0420)	
training:	Epoch: [35][222/817]	Loss 0.0110 (0.0419)	
training:	Epoch: [35][223/817]	Loss 0.0054 (0.0417)	
training:	Epoch: [35][224/817]	Loss 0.0050 (0.0415)	
training:	Epoch: [35][225/817]	Loss 0.0088 (0.0414)	
training:	Epoch: [35][226/817]	Loss 0.0188 (0.0413)	
training:	Epoch: [35][227/817]	Loss 0.0057 (0.0411)	
training:	Epoch: [35][228/817]	Loss 0.0093 (0.0410)	
training:	Epoch: [35][229/817]	Loss 0.0079 (0.0408)	
training:	Epoch: [35][230/817]	Loss 0.0047 (0.0407)	
training:	Epoch: [35][231/817]	Loss 0.0073 (0.0405)	
training:	Epoch: [35][232/817]	Loss 0.0066 (0.0404)	
training:	Epoch: [35][233/817]	Loss 0.0087 (0.0403)	
training:	Epoch: [35][234/817]	Loss 0.0050 (0.0401)	
training:	Epoch: [35][235/817]	Loss 0.0080 (0.0400)	
training:	Epoch: [35][236/817]	Loss 0.0050 (0.0398)	
training:	Epoch: [35][237/817]	Loss 0.0074 (0.0397)	
training:	Epoch: [35][238/817]	Loss 0.0053 (0.0395)	
training:	Epoch: [35][239/817]	Loss 0.0069 (0.0394)	
training:	Epoch: [35][240/817]	Loss 0.0065 (0.0393)	
training:	Epoch: [35][241/817]	Loss 0.5364 (0.0413)	
training:	Epoch: [35][242/817]	Loss 0.0080 (0.0412)	
training:	Epoch: [35][243/817]	Loss 0.0086 (0.0411)	
training:	Epoch: [35][244/817]	Loss 0.0084 (0.0409)	
training:	Epoch: [35][245/817]	Loss 0.0089 (0.0408)	
training:	Epoch: [35][246/817]	Loss 0.0087 (0.0407)	
training:	Epoch: [35][247/817]	Loss 0.0065 (0.0405)	
training:	Epoch: [35][248/817]	Loss 0.0058 (0.0404)	
training:	Epoch: [35][249/817]	Loss 0.0157 (0.0403)	
training:	Epoch: [35][250/817]	Loss 0.0052 (0.0402)	
training:	Epoch: [35][251/817]	Loss 0.0050 (0.0400)	
training:	Epoch: [35][252/817]	Loss 0.0108 (0.0399)	
training:	Epoch: [35][253/817]	Loss 0.0076 (0.0398)	
training:	Epoch: [35][254/817]	Loss 0.0075 (0.0396)	
training:	Epoch: [35][255/817]	Loss 0.0073 (0.0395)	
training:	Epoch: [35][256/817]	Loss 0.0114 (0.0394)	
training:	Epoch: [35][257/817]	Loss 0.0083 (0.0393)	
training:	Epoch: [35][258/817]	Loss 0.0044 (0.0391)	
training:	Epoch: [35][259/817]	Loss 0.0058 (0.0390)	
training:	Epoch: [35][260/817]	Loss 0.0102 (0.0389)	
training:	Epoch: [35][261/817]	Loss 0.0067 (0.0388)	
training:	Epoch: [35][262/817]	Loss 0.0065 (0.0387)	
training:	Epoch: [35][263/817]	Loss 0.0096 (0.0386)	
training:	Epoch: [35][264/817]	Loss 0.0058 (0.0384)	
training:	Epoch: [35][265/817]	Loss 0.0051 (0.0383)	
training:	Epoch: [35][266/817]	Loss 0.0054 (0.0382)	
training:	Epoch: [35][267/817]	Loss 0.0095 (0.0381)	
training:	Epoch: [35][268/817]	Loss 0.0052 (0.0379)	
training:	Epoch: [35][269/817]	Loss 0.6430 (0.0402)	
training:	Epoch: [35][270/817]	Loss 0.0132 (0.0401)	
training:	Epoch: [35][271/817]	Loss 0.0075 (0.0400)	
training:	Epoch: [35][272/817]	Loss 0.0057 (0.0398)	
training:	Epoch: [35][273/817]	Loss 0.0081 (0.0397)	
training:	Epoch: [35][274/817]	Loss 0.0062 (0.0396)	
training:	Epoch: [35][275/817]	Loss 0.0058 (0.0395)	
training:	Epoch: [35][276/817]	Loss 0.0090 (0.0394)	
training:	Epoch: [35][277/817]	Loss 0.0065 (0.0393)	
training:	Epoch: [35][278/817]	Loss 0.0080 (0.0391)	
training:	Epoch: [35][279/817]	Loss 0.0110 (0.0390)	
training:	Epoch: [35][280/817]	Loss 0.0079 (0.0389)	
training:	Epoch: [35][281/817]	Loss 0.0083 (0.0388)	
training:	Epoch: [35][282/817]	Loss 0.0091 (0.0387)	
training:	Epoch: [35][283/817]	Loss 0.0063 (0.0386)	
training:	Epoch: [35][284/817]	Loss 0.0055 (0.0385)	
training:	Epoch: [35][285/817]	Loss 0.0166 (0.0384)	
training:	Epoch: [35][286/817]	Loss 0.0075 (0.0383)	
training:	Epoch: [35][287/817]	Loss 0.0036 (0.0382)	
training:	Epoch: [35][288/817]	Loss 0.0068 (0.0381)	
training:	Epoch: [35][289/817]	Loss 0.0060 (0.0380)	
training:	Epoch: [35][290/817]	Loss 0.0059 (0.0379)	
training:	Epoch: [35][291/817]	Loss 0.0091 (0.0378)	
training:	Epoch: [35][292/817]	Loss 0.0077 (0.0377)	
training:	Epoch: [35][293/817]	Loss 0.0065 (0.0375)	
training:	Epoch: [35][294/817]	Loss 0.0079 (0.0374)	
training:	Epoch: [35][295/817]	Loss 0.0054 (0.0373)	
training:	Epoch: [35][296/817]	Loss 0.0054 (0.0372)	
training:	Epoch: [35][297/817]	Loss 0.0100 (0.0371)	
training:	Epoch: [35][298/817]	Loss 0.0052 (0.0370)	
training:	Epoch: [35][299/817]	Loss 0.0054 (0.0369)	
training:	Epoch: [35][300/817]	Loss 0.5866 (0.0388)	
training:	Epoch: [35][301/817]	Loss 0.0055 (0.0386)	
training:	Epoch: [35][302/817]	Loss 0.0052 (0.0385)	
training:	Epoch: [35][303/817]	Loss 0.0097 (0.0384)	
training:	Epoch: [35][304/817]	Loss 0.0055 (0.0383)	
training:	Epoch: [35][305/817]	Loss 0.0060 (0.0382)	
training:	Epoch: [35][306/817]	Loss 0.0053 (0.0381)	
training:	Epoch: [35][307/817]	Loss 0.0085 (0.0380)	
training:	Epoch: [35][308/817]	Loss 0.5776 (0.0398)	
training:	Epoch: [35][309/817]	Loss 0.0062 (0.0397)	
training:	Epoch: [35][310/817]	Loss 0.0100 (0.0396)	
training:	Epoch: [35][311/817]	Loss 0.0060 (0.0395)	
training:	Epoch: [35][312/817]	Loss 0.0047 (0.0393)	
training:	Epoch: [35][313/817]	Loss 0.0105 (0.0393)	
training:	Epoch: [35][314/817]	Loss 0.0297 (0.0392)	
training:	Epoch: [35][315/817]	Loss 0.0079 (0.0391)	
training:	Epoch: [35][316/817]	Loss 0.0068 (0.0390)	
training:	Epoch: [35][317/817]	Loss 0.0058 (0.0389)	
training:	Epoch: [35][318/817]	Loss 0.0079 (0.0388)	
training:	Epoch: [35][319/817]	Loss 0.0058 (0.0387)	
training:	Epoch: [35][320/817]	Loss 0.0085 (0.0386)	
training:	Epoch: [35][321/817]	Loss 0.0067 (0.0385)	
training:	Epoch: [35][322/817]	Loss 0.0067 (0.0384)	
training:	Epoch: [35][323/817]	Loss 0.0236 (0.0384)	
training:	Epoch: [35][324/817]	Loss 0.0067 (0.0383)	
training:	Epoch: [35][325/817]	Loss 0.0079 (0.0382)	
training:	Epoch: [35][326/817]	Loss 0.0086 (0.0381)	
training:	Epoch: [35][327/817]	Loss 0.0051 (0.0380)	
training:	Epoch: [35][328/817]	Loss 0.0066 (0.0379)	
training:	Epoch: [35][329/817]	Loss 0.0062 (0.0378)	
training:	Epoch: [35][330/817]	Loss 0.0083 (0.0377)	
training:	Epoch: [35][331/817]	Loss 0.0088 (0.0376)	
training:	Epoch: [35][332/817]	Loss 0.0069 (0.0375)	
training:	Epoch: [35][333/817]	Loss 0.0066 (0.0374)	
training:	Epoch: [35][334/817]	Loss 0.0082 (0.0374)	
training:	Epoch: [35][335/817]	Loss 0.5922 (0.0390)	
training:	Epoch: [35][336/817]	Loss 0.0072 (0.0389)	
training:	Epoch: [35][337/817]	Loss 0.0092 (0.0388)	
training:	Epoch: [35][338/817]	Loss 0.0071 (0.0387)	
training:	Epoch: [35][339/817]	Loss 0.0080 (0.0386)	
training:	Epoch: [35][340/817]	Loss 0.0072 (0.0386)	
training:	Epoch: [35][341/817]	Loss 0.0073 (0.0385)	
training:	Epoch: [35][342/817]	Loss 0.5854 (0.0401)	
training:	Epoch: [35][343/817]	Loss 0.0055 (0.0400)	
training:	Epoch: [35][344/817]	Loss 0.0060 (0.0399)	
training:	Epoch: [35][345/817]	Loss 0.0084 (0.0398)	
training:	Epoch: [35][346/817]	Loss 0.0111 (0.0397)	
training:	Epoch: [35][347/817]	Loss 0.0104 (0.0396)	
training:	Epoch: [35][348/817]	Loss 0.0068 (0.0395)	
training:	Epoch: [35][349/817]	Loss 0.0077 (0.0394)	
training:	Epoch: [35][350/817]	Loss 0.0065 (0.0393)	
training:	Epoch: [35][351/817]	Loss 0.0094 (0.0392)	
training:	Epoch: [35][352/817]	Loss 0.0072 (0.0391)	
training:	Epoch: [35][353/817]	Loss 0.0079 (0.0391)	
training:	Epoch: [35][354/817]	Loss 0.0093 (0.0390)	
training:	Epoch: [35][355/817]	Loss 0.0069 (0.0389)	
training:	Epoch: [35][356/817]	Loss 0.0069 (0.0388)	
training:	Epoch: [35][357/817]	Loss 0.0049 (0.0387)	
training:	Epoch: [35][358/817]	Loss 0.0116 (0.0386)	
training:	Epoch: [35][359/817]	Loss 0.0060 (0.0385)	
training:	Epoch: [35][360/817]	Loss 0.0066 (0.0384)	
training:	Epoch: [35][361/817]	Loss 0.0063 (0.0384)	
training:	Epoch: [35][362/817]	Loss 0.0073 (0.0383)	
training:	Epoch: [35][363/817]	Loss 0.0055 (0.0382)	
training:	Epoch: [35][364/817]	Loss 0.0068 (0.0381)	
training:	Epoch: [35][365/817]	Loss 0.0082 (0.0380)	
training:	Epoch: [35][366/817]	Loss 0.0085 (0.0379)	
training:	Epoch: [35][367/817]	Loss 0.0053 (0.0378)	
training:	Epoch: [35][368/817]	Loss 0.0072 (0.0378)	
training:	Epoch: [35][369/817]	Loss 0.0119 (0.0377)	
training:	Epoch: [35][370/817]	Loss 0.0056 (0.0376)	
training:	Epoch: [35][371/817]	Loss 0.0090 (0.0375)	
training:	Epoch: [35][372/817]	Loss 0.0064 (0.0374)	
training:	Epoch: [35][373/817]	Loss 0.0072 (0.0374)	
training:	Epoch: [35][374/817]	Loss 0.0053 (0.0373)	
training:	Epoch: [35][375/817]	Loss 0.0109 (0.0372)	
training:	Epoch: [35][376/817]	Loss 0.0098 (0.0371)	
training:	Epoch: [35][377/817]	Loss 0.0070 (0.0370)	
training:	Epoch: [35][378/817]	Loss 0.0063 (0.0370)	
training:	Epoch: [35][379/817]	Loss 0.0068 (0.0369)	
training:	Epoch: [35][380/817]	Loss 0.0066 (0.0368)	
training:	Epoch: [35][381/817]	Loss 0.0061 (0.0367)	
training:	Epoch: [35][382/817]	Loss 0.0056 (0.0366)	
training:	Epoch: [35][383/817]	Loss 0.0058 (0.0366)	
training:	Epoch: [35][384/817]	Loss 0.0060 (0.0365)	
training:	Epoch: [35][385/817]	Loss 0.0068 (0.0364)	
training:	Epoch: [35][386/817]	Loss 0.0071 (0.0363)	
training:	Epoch: [35][387/817]	Loss 0.0091 (0.0363)	
training:	Epoch: [35][388/817]	Loss 0.2066 (0.0367)	
training:	Epoch: [35][389/817]	Loss 0.0075 (0.0366)	
training:	Epoch: [35][390/817]	Loss 0.0091 (0.0366)	
training:	Epoch: [35][391/817]	Loss 0.0053 (0.0365)	
training:	Epoch: [35][392/817]	Loss 0.0086 (0.0364)	
training:	Epoch: [35][393/817]	Loss 0.0063 (0.0363)	
training:	Epoch: [35][394/817]	Loss 0.0058 (0.0363)	
training:	Epoch: [35][395/817]	Loss 0.0070 (0.0362)	
training:	Epoch: [35][396/817]	Loss 0.0234 (0.0361)	
training:	Epoch: [35][397/817]	Loss 0.0088 (0.0361)	
training:	Epoch: [35][398/817]	Loss 0.0074 (0.0360)	
training:	Epoch: [35][399/817]	Loss 0.0094 (0.0359)	
training:	Epoch: [35][400/817]	Loss 0.5374 (0.0372)	
training:	Epoch: [35][401/817]	Loss 0.0081 (0.0371)	
training:	Epoch: [35][402/817]	Loss 0.4911 (0.0382)	
training:	Epoch: [35][403/817]	Loss 0.1479 (0.0385)	
training:	Epoch: [35][404/817]	Loss 0.0065 (0.0384)	
training:	Epoch: [35][405/817]	Loss 0.0067 (0.0384)	
training:	Epoch: [35][406/817]	Loss 0.0047 (0.0383)	
training:	Epoch: [35][407/817]	Loss 0.0053 (0.0382)	
training:	Epoch: [35][408/817]	Loss 0.0064 (0.0381)	
training:	Epoch: [35][409/817]	Loss 0.0096 (0.0380)	
training:	Epoch: [35][410/817]	Loss 0.0056 (0.0380)	
training:	Epoch: [35][411/817]	Loss 0.0080 (0.0379)	
training:	Epoch: [35][412/817]	Loss 0.0057 (0.0378)	
training:	Epoch: [35][413/817]	Loss 0.0081 (0.0377)	
training:	Epoch: [35][414/817]	Loss 0.0056 (0.0377)	
training:	Epoch: [35][415/817]	Loss 0.0061 (0.0376)	
training:	Epoch: [35][416/817]	Loss 0.0064 (0.0375)	
training:	Epoch: [35][417/817]	Loss 0.0051 (0.0374)	
training:	Epoch: [35][418/817]	Loss 0.0081 (0.0374)	
training:	Epoch: [35][419/817]	Loss 0.0080 (0.0373)	
training:	Epoch: [35][420/817]	Loss 0.5686 (0.0386)	
training:	Epoch: [35][421/817]	Loss 0.0069 (0.0385)	
training:	Epoch: [35][422/817]	Loss 0.0064 (0.0384)	
training:	Epoch: [35][423/817]	Loss 0.0066 (0.0383)	
training:	Epoch: [35][424/817]	Loss 0.0067 (0.0383)	
training:	Epoch: [35][425/817]	Loss 0.0112 (0.0382)	
training:	Epoch: [35][426/817]	Loss 0.0080 (0.0381)	
training:	Epoch: [35][427/817]	Loss 0.0056 (0.0381)	
training:	Epoch: [35][428/817]	Loss 0.0099 (0.0380)	
training:	Epoch: [35][429/817]	Loss 0.0069 (0.0379)	
training:	Epoch: [35][430/817]	Loss 0.0061 (0.0378)	
training:	Epoch: [35][431/817]	Loss 0.0076 (0.0378)	
training:	Epoch: [35][432/817]	Loss 0.0051 (0.0377)	
training:	Epoch: [35][433/817]	Loss 0.0059 (0.0376)	
training:	Epoch: [35][434/817]	Loss 0.0051 (0.0375)	
training:	Epoch: [35][435/817]	Loss 0.0273 (0.0375)	
training:	Epoch: [35][436/817]	Loss 0.0074 (0.0375)	
training:	Epoch: [35][437/817]	Loss 0.0069 (0.0374)	
training:	Epoch: [35][438/817]	Loss 0.0112 (0.0373)	
training:	Epoch: [35][439/817]	Loss 0.0081 (0.0373)	
training:	Epoch: [35][440/817]	Loss 0.0078 (0.0372)	
training:	Epoch: [35][441/817]	Loss 0.0076 (0.0371)	
training:	Epoch: [35][442/817]	Loss 0.0085 (0.0371)	
training:	Epoch: [35][443/817]	Loss 0.0070 (0.0370)	
training:	Epoch: [35][444/817]	Loss 0.0050 (0.0369)	
training:	Epoch: [35][445/817]	Loss 0.0073 (0.0369)	
training:	Epoch: [35][446/817]	Loss 0.0064 (0.0368)	
training:	Epoch: [35][447/817]	Loss 0.0107 (0.0367)	
training:	Epoch: [35][448/817]	Loss 0.0054 (0.0367)	
training:	Epoch: [35][449/817]	Loss 0.0067 (0.0366)	
training:	Epoch: [35][450/817]	Loss 0.0089 (0.0365)	
training:	Epoch: [35][451/817]	Loss 0.0096 (0.0365)	
training:	Epoch: [35][452/817]	Loss 0.0063 (0.0364)	
training:	Epoch: [35][453/817]	Loss 0.0060 (0.0363)	
training:	Epoch: [35][454/817]	Loss 0.0066 (0.0363)	
training:	Epoch: [35][455/817]	Loss 0.0053 (0.0362)	
training:	Epoch: [35][456/817]	Loss 0.0070 (0.0361)	
training:	Epoch: [35][457/817]	Loss 0.0071 (0.0361)	
training:	Epoch: [35][458/817]	Loss 0.0094 (0.0360)	
training:	Epoch: [35][459/817]	Loss 0.0115 (0.0360)	
training:	Epoch: [35][460/817]	Loss 0.0054 (0.0359)	
training:	Epoch: [35][461/817]	Loss 0.0065 (0.0358)	
training:	Epoch: [35][462/817]	Loss 0.0083 (0.0358)	
training:	Epoch: [35][463/817]	Loss 0.0055 (0.0357)	
training:	Epoch: [35][464/817]	Loss 0.0056 (0.0356)	
training:	Epoch: [35][465/817]	Loss 0.0083 (0.0356)	
training:	Epoch: [35][466/817]	Loss 0.0071 (0.0355)	
training:	Epoch: [35][467/817]	Loss 0.0053 (0.0355)	
training:	Epoch: [35][468/817]	Loss 0.0053 (0.0354)	
training:	Epoch: [35][469/817]	Loss 0.0078 (0.0353)	
training:	Epoch: [35][470/817]	Loss 0.0063 (0.0353)	
training:	Epoch: [35][471/817]	Loss 0.0071 (0.0352)	
training:	Epoch: [35][472/817]	Loss 0.0071 (0.0352)	
training:	Epoch: [35][473/817]	Loss 0.0086 (0.0351)	
training:	Epoch: [35][474/817]	Loss 0.0056 (0.0350)	
training:	Epoch: [35][475/817]	Loss 0.0069 (0.0350)	
training:	Epoch: [35][476/817]	Loss 0.0269 (0.0350)	
training:	Epoch: [35][477/817]	Loss 0.4978 (0.0359)	
training:	Epoch: [35][478/817]	Loss 0.0064 (0.0359)	
training:	Epoch: [35][479/817]	Loss 0.0083 (0.0358)	
training:	Epoch: [35][480/817]	Loss 0.0067 (0.0357)	
training:	Epoch: [35][481/817]	Loss 0.0076 (0.0357)	
training:	Epoch: [35][482/817]	Loss 0.0114 (0.0356)	
training:	Epoch: [35][483/817]	Loss 0.0070 (0.0356)	
training:	Epoch: [35][484/817]	Loss 0.0083 (0.0355)	
training:	Epoch: [35][485/817]	Loss 0.5308 (0.0365)	
training:	Epoch: [35][486/817]	Loss 0.0066 (0.0365)	
training:	Epoch: [35][487/817]	Loss 0.0090 (0.0364)	
training:	Epoch: [35][488/817]	Loss 0.0068 (0.0364)	
training:	Epoch: [35][489/817]	Loss 0.0056 (0.0363)	
training:	Epoch: [35][490/817]	Loss 0.0081 (0.0362)	
training:	Epoch: [35][491/817]	Loss 0.0049 (0.0362)	
training:	Epoch: [35][492/817]	Loss 0.0075 (0.0361)	
training:	Epoch: [35][493/817]	Loss 0.0060 (0.0361)	
training:	Epoch: [35][494/817]	Loss 0.0047 (0.0360)	
training:	Epoch: [35][495/817]	Loss 0.0084 (0.0359)	
training:	Epoch: [35][496/817]	Loss 0.0082 (0.0359)	
training:	Epoch: [35][497/817]	Loss 0.0059 (0.0358)	
training:	Epoch: [35][498/817]	Loss 0.0050 (0.0358)	
training:	Epoch: [35][499/817]	Loss 0.0070 (0.0357)	
training:	Epoch: [35][500/817]	Loss 0.0088 (0.0357)	
training:	Epoch: [35][501/817]	Loss 0.0091 (0.0356)	
training:	Epoch: [35][502/817]	Loss 0.0085 (0.0355)	
training:	Epoch: [35][503/817]	Loss 0.0048 (0.0355)	
training:	Epoch: [35][504/817]	Loss 0.1005 (0.0356)	
training:	Epoch: [35][505/817]	Loss 0.0073 (0.0356)	
training:	Epoch: [35][506/817]	Loss 0.0099 (0.0355)	
training:	Epoch: [35][507/817]	Loss 0.0075 (0.0355)	
training:	Epoch: [35][508/817]	Loss 0.0069 (0.0354)	
training:	Epoch: [35][509/817]	Loss 0.0053 (0.0353)	
training:	Epoch: [35][510/817]	Loss 0.3677 (0.0360)	
training:	Epoch: [35][511/817]	Loss 0.0054 (0.0359)	
training:	Epoch: [35][512/817]	Loss 0.0061 (0.0359)	
training:	Epoch: [35][513/817]	Loss 0.0049 (0.0358)	
training:	Epoch: [35][514/817]	Loss 0.0047 (0.0358)	
training:	Epoch: [35][515/817]	Loss 0.0061 (0.0357)	
training:	Epoch: [35][516/817]	Loss 0.5868 (0.0368)	
training:	Epoch: [35][517/817]	Loss 0.0202 (0.0367)	
training:	Epoch: [35][518/817]	Loss 0.0078 (0.0367)	
training:	Epoch: [35][519/817]	Loss 0.0076 (0.0366)	
training:	Epoch: [35][520/817]	Loss 0.0065 (0.0366)	
training:	Epoch: [35][521/817]	Loss 0.0091 (0.0365)	
training:	Epoch: [35][522/817]	Loss 0.0103 (0.0365)	
training:	Epoch: [35][523/817]	Loss 0.0109 (0.0364)	
training:	Epoch: [35][524/817]	Loss 0.0055 (0.0363)	
training:	Epoch: [35][525/817]	Loss 0.0092 (0.0363)	
training:	Epoch: [35][526/817]	Loss 0.0112 (0.0362)	
training:	Epoch: [35][527/817]	Loss 0.0107 (0.0362)	
training:	Epoch: [35][528/817]	Loss 0.0063 (0.0361)	
training:	Epoch: [35][529/817]	Loss 0.0095 (0.0361)	
training:	Epoch: [35][530/817]	Loss 0.0090 (0.0360)	
training:	Epoch: [35][531/817]	Loss 0.0072 (0.0360)	
training:	Epoch: [35][532/817]	Loss 0.0064 (0.0359)	
training:	Epoch: [35][533/817]	Loss 0.0060 (0.0359)	
training:	Epoch: [35][534/817]	Loss 0.0083 (0.0358)	
training:	Epoch: [35][535/817]	Loss 0.0065 (0.0358)	
training:	Epoch: [35][536/817]	Loss 0.0064 (0.0357)	
training:	Epoch: [35][537/817]	Loss 0.0058 (0.0357)	
training:	Epoch: [35][538/817]	Loss 0.0096 (0.0356)	
training:	Epoch: [35][539/817]	Loss 0.0059 (0.0356)	
training:	Epoch: [35][540/817]	Loss 0.0070 (0.0355)	
training:	Epoch: [35][541/817]	Loss 0.0046 (0.0354)	
training:	Epoch: [35][542/817]	Loss 0.0087 (0.0354)	
training:	Epoch: [35][543/817]	Loss 0.0060 (0.0353)	
training:	Epoch: [35][544/817]	Loss 0.0122 (0.0353)	
training:	Epoch: [35][545/817]	Loss 0.0062 (0.0352)	
training:	Epoch: [35][546/817]	Loss 0.0058 (0.0352)	
training:	Epoch: [35][547/817]	Loss 0.0082 (0.0351)	
training:	Epoch: [35][548/817]	Loss 0.0089 (0.0351)	
training:	Epoch: [35][549/817]	Loss 0.0072 (0.0350)	
training:	Epoch: [35][550/817]	Loss 0.0055 (0.0350)	
training:	Epoch: [35][551/817]	Loss 0.0125 (0.0350)	
training:	Epoch: [35][552/817]	Loss 0.0108 (0.0349)	
training:	Epoch: [35][553/817]	Loss 0.0091 (0.0349)	
training:	Epoch: [35][554/817]	Loss 0.1023 (0.0350)	
training:	Epoch: [35][555/817]	Loss 0.0075 (0.0349)	
training:	Epoch: [35][556/817]	Loss 0.0044 (0.0349)	
training:	Epoch: [35][557/817]	Loss 0.0065 (0.0348)	
training:	Epoch: [35][558/817]	Loss 0.0088 (0.0348)	
training:	Epoch: [35][559/817]	Loss 0.0080 (0.0347)	
training:	Epoch: [35][560/817]	Loss 0.0076 (0.0347)	
training:	Epoch: [35][561/817]	Loss 0.0077 (0.0346)	
training:	Epoch: [35][562/817]	Loss 0.0069 (0.0346)	
training:	Epoch: [35][563/817]	Loss 0.0121 (0.0345)	
training:	Epoch: [35][564/817]	Loss 0.0081 (0.0345)	
training:	Epoch: [35][565/817]	Loss 0.0066 (0.0344)	
training:	Epoch: [35][566/817]	Loss 0.0057 (0.0344)	
training:	Epoch: [35][567/817]	Loss 0.0081 (0.0344)	
training:	Epoch: [35][568/817]	Loss 0.0078 (0.0343)	
training:	Epoch: [35][569/817]	Loss 0.0073 (0.0343)	
training:	Epoch: [35][570/817]	Loss 0.0076 (0.0342)	
training:	Epoch: [35][571/817]	Loss 0.0079 (0.0342)	
training:	Epoch: [35][572/817]	Loss 0.0058 (0.0341)	
training:	Epoch: [35][573/817]	Loss 0.0096 (0.0341)	
training:	Epoch: [35][574/817]	Loss 0.0064 (0.0340)	
training:	Epoch: [35][575/817]	Loss 0.0073 (0.0340)	
training:	Epoch: [35][576/817]	Loss 0.0068 (0.0339)	
training:	Epoch: [35][577/817]	Loss 0.0068 (0.0339)	
training:	Epoch: [35][578/817]	Loss 0.0055 (0.0338)	
training:	Epoch: [35][579/817]	Loss 0.0134 (0.0338)	
training:	Epoch: [35][580/817]	Loss 0.0100 (0.0338)	
training:	Epoch: [35][581/817]	Loss 0.0075 (0.0337)	
training:	Epoch: [35][582/817]	Loss 0.0051 (0.0337)	
training:	Epoch: [35][583/817]	Loss 0.0088 (0.0336)	
training:	Epoch: [35][584/817]	Loss 0.0067 (0.0336)	
training:	Epoch: [35][585/817]	Loss 0.0053 (0.0335)	
training:	Epoch: [35][586/817]	Loss 0.0062 (0.0335)	
training:	Epoch: [35][587/817]	Loss 0.0056 (0.0334)	
training:	Epoch: [35][588/817]	Loss 0.0064 (0.0334)	
training:	Epoch: [35][589/817]	Loss 0.0097 (0.0333)	
training:	Epoch: [35][590/817]	Loss 0.0089 (0.0333)	
training:	Epoch: [35][591/817]	Loss 0.0068 (0.0333)	
training:	Epoch: [35][592/817]	Loss 0.0092 (0.0332)	
training:	Epoch: [35][593/817]	Loss 0.0071 (0.0332)	
training:	Epoch: [35][594/817]	Loss 0.0052 (0.0331)	
training:	Epoch: [35][595/817]	Loss 0.0062 (0.0331)	
training:	Epoch: [35][596/817]	Loss 0.0056 (0.0330)	
training:	Epoch: [35][597/817]	Loss 0.0048 (0.0330)	
training:	Epoch: [35][598/817]	Loss 0.0049 (0.0329)	
training:	Epoch: [35][599/817]	Loss 0.0072 (0.0329)	
training:	Epoch: [35][600/817]	Loss 0.0056 (0.0329)	
training:	Epoch: [35][601/817]	Loss 0.0045 (0.0328)	
training:	Epoch: [35][602/817]	Loss 0.0055 (0.0328)	
training:	Epoch: [35][603/817]	Loss 0.0065 (0.0327)	
training:	Epoch: [35][604/817]	Loss 0.0076 (0.0327)	
training:	Epoch: [35][605/817]	Loss 0.0053 (0.0326)	
training:	Epoch: [35][606/817]	Loss 0.0089 (0.0326)	
training:	Epoch: [35][607/817]	Loss 0.0069 (0.0326)	
training:	Epoch: [35][608/817]	Loss 0.0065 (0.0325)	
training:	Epoch: [35][609/817]	Loss 0.0089 (0.0325)	
training:	Epoch: [35][610/817]	Loss 0.0057 (0.0324)	
training:	Epoch: [35][611/817]	Loss 0.0036 (0.0324)	
training:	Epoch: [35][612/817]	Loss 0.0077 (0.0323)	
training:	Epoch: [35][613/817]	Loss 0.1128 (0.0325)	
training:	Epoch: [35][614/817]	Loss 0.0079 (0.0324)	
training:	Epoch: [35][615/817]	Loss 0.0044 (0.0324)	
training:	Epoch: [35][616/817]	Loss 0.0079 (0.0323)	
training:	Epoch: [35][617/817]	Loss 0.0056 (0.0323)	
training:	Epoch: [35][618/817]	Loss 0.0069 (0.0323)	
training:	Epoch: [35][619/817]	Loss 0.0083 (0.0322)	
training:	Epoch: [35][620/817]	Loss 0.0052 (0.0322)	
training:	Epoch: [35][621/817]	Loss 0.0050 (0.0321)	
training:	Epoch: [35][622/817]	Loss 0.0049 (0.0321)	
training:	Epoch: [35][623/817]	Loss 0.0079 (0.0321)	
training:	Epoch: [35][624/817]	Loss 0.0058 (0.0320)	
training:	Epoch: [35][625/817]	Loss 0.0071 (0.0320)	
training:	Epoch: [35][626/817]	Loss 0.0052 (0.0319)	
training:	Epoch: [35][627/817]	Loss 0.0041 (0.0319)	
training:	Epoch: [35][628/817]	Loss 0.0053 (0.0318)	
training:	Epoch: [35][629/817]	Loss 0.0053 (0.0318)	
training:	Epoch: [35][630/817]	Loss 0.0066 (0.0318)	
training:	Epoch: [35][631/817]	Loss 0.0068 (0.0317)	
training:	Epoch: [35][632/817]	Loss 0.0061 (0.0317)	
training:	Epoch: [35][633/817]	Loss 0.0052 (0.0316)	
training:	Epoch: [35][634/817]	Loss 0.0049 (0.0316)	
training:	Epoch: [35][635/817]	Loss 0.0155 (0.0316)	
training:	Epoch: [35][636/817]	Loss 0.0044 (0.0315)	
training:	Epoch: [35][637/817]	Loss 0.0050 (0.0315)	
training:	Epoch: [35][638/817]	Loss 0.0088 (0.0314)	
training:	Epoch: [35][639/817]	Loss 0.0096 (0.0314)	
training:	Epoch: [35][640/817]	Loss 0.0050 (0.0314)	
training:	Epoch: [35][641/817]	Loss 0.0053 (0.0313)	
training:	Epoch: [35][642/817]	Loss 0.0055 (0.0313)	
training:	Epoch: [35][643/817]	Loss 0.5504 (0.0321)	
training:	Epoch: [35][644/817]	Loss 0.0048 (0.0321)	
training:	Epoch: [35][645/817]	Loss 0.0046 (0.0320)	
training:	Epoch: [35][646/817]	Loss 0.0094 (0.0320)	
training:	Epoch: [35][647/817]	Loss 0.0054 (0.0319)	
training:	Epoch: [35][648/817]	Loss 0.0081 (0.0319)	
training:	Epoch: [35][649/817]	Loss 0.0052 (0.0319)	
training:	Epoch: [35][650/817]	Loss 0.0061 (0.0318)	
training:	Epoch: [35][651/817]	Loss 0.0052 (0.0318)	
training:	Epoch: [35][652/817]	Loss 0.0053 (0.0317)	
training:	Epoch: [35][653/817]	Loss 0.0044 (0.0317)	
training:	Epoch: [35][654/817]	Loss 0.0054 (0.0317)	
training:	Epoch: [35][655/817]	Loss 0.0050 (0.0316)	
training:	Epoch: [35][656/817]	Loss 0.0072 (0.0316)	
training:	Epoch: [35][657/817]	Loss 0.0035 (0.0315)	
training:	Epoch: [35][658/817]	Loss 0.0046 (0.0315)	
training:	Epoch: [35][659/817]	Loss 0.0066 (0.0315)	
training:	Epoch: [35][660/817]	Loss 0.0065 (0.0314)	
training:	Epoch: [35][661/817]	Loss 0.0074 (0.0314)	
training:	Epoch: [35][662/817]	Loss 0.0049 (0.0313)	
training:	Epoch: [35][663/817]	Loss 0.4055 (0.0319)	
training:	Epoch: [35][664/817]	Loss 0.0049 (0.0319)	
training:	Epoch: [35][665/817]	Loss 0.0052 (0.0318)	
training:	Epoch: [35][666/817]	Loss 0.0060 (0.0318)	
training:	Epoch: [35][667/817]	Loss 0.0051 (0.0317)	
training:	Epoch: [35][668/817]	Loss 0.0055 (0.0317)	
training:	Epoch: [35][669/817]	Loss 0.0052 (0.0317)	
training:	Epoch: [35][670/817]	Loss 0.0346 (0.0317)	
training:	Epoch: [35][671/817]	Loss 0.0164 (0.0317)	
training:	Epoch: [35][672/817]	Loss 0.0083 (0.0316)	
training:	Epoch: [35][673/817]	Loss 0.0438 (0.0316)	
training:	Epoch: [35][674/817]	Loss 0.0043 (0.0316)	
training:	Epoch: [35][675/817]	Loss 0.0079 (0.0316)	
training:	Epoch: [35][676/817]	Loss 0.0983 (0.0317)	
training:	Epoch: [35][677/817]	Loss 0.0069 (0.0316)	
training:	Epoch: [35][678/817]	Loss 0.0044 (0.0316)	
training:	Epoch: [35][679/817]	Loss 0.0062 (0.0315)	
training:	Epoch: [35][680/817]	Loss 0.0054 (0.0315)	
training:	Epoch: [35][681/817]	Loss 0.0057 (0.0315)	
training:	Epoch: [35][682/817]	Loss 0.0060 (0.0314)	
training:	Epoch: [35][683/817]	Loss 0.0052 (0.0314)	
training:	Epoch: [35][684/817]	Loss 0.0058 (0.0314)	
training:	Epoch: [35][685/817]	Loss 0.0040 (0.0313)	
training:	Epoch: [35][686/817]	Loss 0.0049 (0.0313)	
training:	Epoch: [35][687/817]	Loss 0.0055 (0.0312)	
training:	Epoch: [35][688/817]	Loss 0.0162 (0.0312)	
training:	Epoch: [35][689/817]	Loss 0.0071 (0.0312)	
training:	Epoch: [35][690/817]	Loss 0.0052 (0.0311)	
training:	Epoch: [35][691/817]	Loss 0.0059 (0.0311)	
training:	Epoch: [35][692/817]	Loss 0.0059 (0.0311)	
training:	Epoch: [35][693/817]	Loss 0.0042 (0.0310)	
training:	Epoch: [35][694/817]	Loss 0.0056 (0.0310)	
training:	Epoch: [35][695/817]	Loss 0.0064 (0.0310)	
training:	Epoch: [35][696/817]	Loss 0.0038 (0.0309)	
training:	Epoch: [35][697/817]	Loss 0.0061 (0.0309)	
training:	Epoch: [35][698/817]	Loss 0.0063 (0.0308)	
training:	Epoch: [35][699/817]	Loss 0.0070 (0.0308)	
training:	Epoch: [35][700/817]	Loss 0.0105 (0.0308)	
training:	Epoch: [35][701/817]	Loss 0.0057 (0.0307)	
training:	Epoch: [35][702/817]	Loss 0.0040 (0.0307)	
training:	Epoch: [35][703/817]	Loss 0.0044 (0.0307)	
training:	Epoch: [35][704/817]	Loss 1.2408 (0.0324)	
training:	Epoch: [35][705/817]	Loss 0.0046 (0.0324)	
training:	Epoch: [35][706/817]	Loss 0.0048 (0.0323)	
training:	Epoch: [35][707/817]	Loss 0.0049 (0.0323)	
training:	Epoch: [35][708/817]	Loss 0.0046 (0.0322)	
training:	Epoch: [35][709/817]	Loss 0.0046 (0.0322)	
training:	Epoch: [35][710/817]	Loss 0.0062 (0.0322)	
training:	Epoch: [35][711/817]	Loss 0.0051 (0.0321)	
training:	Epoch: [35][712/817]	Loss 0.0085 (0.0321)	
training:	Epoch: [35][713/817]	Loss 0.0057 (0.0321)	
training:	Epoch: [35][714/817]	Loss 0.0049 (0.0320)	
training:	Epoch: [35][715/817]	Loss 0.0071 (0.0320)	
training:	Epoch: [35][716/817]	Loss 0.0055 (0.0319)	
training:	Epoch: [35][717/817]	Loss 0.0067 (0.0319)	
training:	Epoch: [35][718/817]	Loss 0.0060 (0.0319)	
training:	Epoch: [35][719/817]	Loss 0.0067 (0.0318)	
training:	Epoch: [35][720/817]	Loss 0.0057 (0.0318)	
training:	Epoch: [35][721/817]	Loss 0.0090 (0.0318)	
training:	Epoch: [35][722/817]	Loss 0.0064 (0.0317)	
training:	Epoch: [35][723/817]	Loss 0.6147 (0.0325)	
training:	Epoch: [35][724/817]	Loss 0.0383 (0.0325)	
training:	Epoch: [35][725/817]	Loss 0.0060 (0.0325)	
training:	Epoch: [35][726/817]	Loss 0.0055 (0.0325)	
training:	Epoch: [35][727/817]	Loss 0.0056 (0.0324)	
training:	Epoch: [35][728/817]	Loss 0.0043 (0.0324)	
training:	Epoch: [35][729/817]	Loss 0.0070 (0.0324)	
training:	Epoch: [35][730/817]	Loss 0.0094 (0.0323)	
training:	Epoch: [35][731/817]	Loss 0.0104 (0.0323)	
training:	Epoch: [35][732/817]	Loss 0.0059 (0.0323)	
training:	Epoch: [35][733/817]	Loss 0.0051 (0.0322)	
training:	Epoch: [35][734/817]	Loss 0.0281 (0.0322)	
training:	Epoch: [35][735/817]	Loss 0.0075 (0.0322)	
training:	Epoch: [35][736/817]	Loss 0.0052 (0.0322)	
training:	Epoch: [35][737/817]	Loss 0.0071 (0.0321)	
training:	Epoch: [35][738/817]	Loss 0.6913 (0.0330)	
training:	Epoch: [35][739/817]	Loss 0.0066 (0.0330)	
training:	Epoch: [35][740/817]	Loss 0.0082 (0.0329)	
training:	Epoch: [35][741/817]	Loss 0.0052 (0.0329)	
training:	Epoch: [35][742/817]	Loss 0.0063 (0.0329)	
training:	Epoch: [35][743/817]	Loss 0.0062 (0.0328)	
training:	Epoch: [35][744/817]	Loss 0.0083 (0.0328)	
training:	Epoch: [35][745/817]	Loss 0.0063 (0.0328)	
training:	Epoch: [35][746/817]	Loss 0.0118 (0.0327)	
training:	Epoch: [35][747/817]	Loss 0.0087 (0.0327)	
training:	Epoch: [35][748/817]	Loss 0.0057 (0.0327)	
training:	Epoch: [35][749/817]	Loss 0.0057 (0.0326)	
training:	Epoch: [35][750/817]	Loss 0.0043 (0.0326)	
training:	Epoch: [35][751/817]	Loss 0.0053 (0.0326)	
training:	Epoch: [35][752/817]	Loss 0.0036 (0.0325)	
training:	Epoch: [35][753/817]	Loss 0.0044 (0.0325)	
training:	Epoch: [35][754/817]	Loss 0.5094 (0.0331)	
training:	Epoch: [35][755/817]	Loss 0.0050 (0.0331)	
training:	Epoch: [35][756/817]	Loss 0.0090 (0.0330)	
training:	Epoch: [35][757/817]	Loss 0.0061 (0.0330)	
training:	Epoch: [35][758/817]	Loss 0.0068 (0.0330)	
training:	Epoch: [35][759/817]	Loss 0.0084 (0.0329)	
training:	Epoch: [35][760/817]	Loss 0.0060 (0.0329)	
training:	Epoch: [35][761/817]	Loss 0.0069 (0.0329)	
training:	Epoch: [35][762/817]	Loss 0.0072 (0.0328)	
training:	Epoch: [35][763/817]	Loss 0.0073 (0.0328)	
training:	Epoch: [35][764/817]	Loss 0.0063 (0.0328)	
training:	Epoch: [35][765/817]	Loss 0.0060 (0.0327)	
training:	Epoch: [35][766/817]	Loss 0.0054 (0.0327)	
training:	Epoch: [35][767/817]	Loss 1.0605 (0.0340)	
training:	Epoch: [35][768/817]	Loss 0.0058 (0.0340)	
training:	Epoch: [35][769/817]	Loss 0.0051 (0.0340)	
training:	Epoch: [35][770/817]	Loss 0.0060 (0.0339)	
training:	Epoch: [35][771/817]	Loss 0.0048 (0.0339)	
training:	Epoch: [35][772/817]	Loss 0.0061 (0.0339)	
training:	Epoch: [35][773/817]	Loss 0.0076 (0.0338)	
training:	Epoch: [35][774/817]	Loss 0.0085 (0.0338)	
training:	Epoch: [35][775/817]	Loss 0.0053 (0.0338)	
training:	Epoch: [35][776/817]	Loss 0.0064 (0.0337)	
training:	Epoch: [35][777/817]	Loss 0.0061 (0.0337)	
training:	Epoch: [35][778/817]	Loss 0.0106 (0.0337)	
training:	Epoch: [35][779/817]	Loss 0.0058 (0.0336)	
training:	Epoch: [35][780/817]	Loss 0.0043 (0.0336)	
training:	Epoch: [35][781/817]	Loss 0.0079 (0.0335)	
training:	Epoch: [35][782/817]	Loss 0.0060 (0.0335)	
training:	Epoch: [35][783/817]	Loss 0.0070 (0.0335)	
training:	Epoch: [35][784/817]	Loss 0.0059 (0.0334)	
training:	Epoch: [35][785/817]	Loss 0.0124 (0.0334)	
training:	Epoch: [35][786/817]	Loss 0.0056 (0.0334)	
training:	Epoch: [35][787/817]	Loss 0.0063 (0.0333)	
training:	Epoch: [35][788/817]	Loss 0.0333 (0.0333)	
training:	Epoch: [35][789/817]	Loss 0.0144 (0.0333)	
training:	Epoch: [35][790/817]	Loss 0.0072 (0.0333)	
training:	Epoch: [35][791/817]	Loss 0.0073 (0.0333)	
training:	Epoch: [35][792/817]	Loss 0.0066 (0.0332)	
training:	Epoch: [35][793/817]	Loss 0.0183 (0.0332)	
training:	Epoch: [35][794/817]	Loss 0.0053 (0.0332)	
training:	Epoch: [35][795/817]	Loss 0.0097 (0.0331)	
training:	Epoch: [35][796/817]	Loss 0.0079 (0.0331)	
training:	Epoch: [35][797/817]	Loss 0.0061 (0.0331)	
training:	Epoch: [35][798/817]	Loss 0.0043 (0.0330)	
training:	Epoch: [35][799/817]	Loss 0.0060 (0.0330)	
training:	Epoch: [35][800/817]	Loss 0.0057 (0.0330)	
training:	Epoch: [35][801/817]	Loss 0.0125 (0.0329)	
training:	Epoch: [35][802/817]	Loss 0.0052 (0.0329)	
training:	Epoch: [35][803/817]	Loss 0.0080 (0.0329)	
training:	Epoch: [35][804/817]	Loss 0.0080 (0.0328)	
training:	Epoch: [35][805/817]	Loss 0.0154 (0.0328)	
training:	Epoch: [35][806/817]	Loss 0.0048 (0.0328)	
training:	Epoch: [35][807/817]	Loss 0.0058 (0.0328)	
training:	Epoch: [35][808/817]	Loss 0.0087 (0.0327)	
training:	Epoch: [35][809/817]	Loss 0.0081 (0.0327)	
training:	Epoch: [35][810/817]	Loss 0.0045 (0.0327)	
training:	Epoch: [35][811/817]	Loss 0.0072 (0.0326)	
training:	Epoch: [35][812/817]	Loss 0.6821 (0.0334)	
training:	Epoch: [35][813/817]	Loss 0.0053 (0.0334)	
training:	Epoch: [35][814/817]	Loss 0.0158 (0.0334)	
training:	Epoch: [35][815/817]	Loss 0.4628 (0.0339)	
training:	Epoch: [35][816/817]	Loss 0.0050 (0.0339)	
training:	Epoch: [35][817/817]	Loss 0.0068 (0.0338)	
Training:	 Loss: 0.0338

Training:	 ACC: 0.9945 0.9945 0.9953 0.9936
Validation:	 ACC: 0.7835 0.7838 0.7912 0.7758
Validation:	 Best_BACC: 0.8223 0.8218 0.8127 0.8318
Validation:	 Loss: 1.0112
Pretraining:	Epoch 36/200
----------
training:	Epoch: [36][1/817]	Loss 0.0060 (0.0060)	
training:	Epoch: [36][2/817]	Loss 0.0062 (0.0061)	
training:	Epoch: [36][3/817]	Loss 0.0067 (0.0063)	
training:	Epoch: [36][4/817]	Loss 0.0056 (0.0061)	
training:	Epoch: [36][5/817]	Loss 0.0094 (0.0068)	
training:	Epoch: [36][6/817]	Loss 0.0048 (0.0065)	
training:	Epoch: [36][7/817]	Loss 0.0060 (0.0064)	
training:	Epoch: [36][8/817]	Loss 0.0063 (0.0064)	
training:	Epoch: [36][9/817]	Loss 0.0065 (0.0064)	
training:	Epoch: [36][10/817]	Loss 0.0070 (0.0065)	
training:	Epoch: [36][11/817]	Loss 0.0065 (0.0065)	
training:	Epoch: [36][12/817]	Loss 0.0052 (0.0064)	
training:	Epoch: [36][13/817]	Loss 0.0080 (0.0065)	
training:	Epoch: [36][14/817]	Loss 0.0072 (0.0065)	
training:	Epoch: [36][15/817]	Loss 0.0053 (0.0065)	
training:	Epoch: [36][16/817]	Loss 0.0069 (0.0065)	
training:	Epoch: [36][17/817]	Loss 0.0058 (0.0064)	
training:	Epoch: [36][18/817]	Loss 0.0076 (0.0065)	
training:	Epoch: [36][19/817]	Loss 0.0109 (0.0067)	
training:	Epoch: [36][20/817]	Loss 0.0059 (0.0067)	
training:	Epoch: [36][21/817]	Loss 0.0129 (0.0070)	
training:	Epoch: [36][22/817]	Loss 0.0071 (0.0070)	
training:	Epoch: [36][23/817]	Loss 0.0065 (0.0070)	
training:	Epoch: [36][24/817]	Loss 0.0058 (0.0069)	
training:	Epoch: [36][25/817]	Loss 0.0085 (0.0070)	
training:	Epoch: [36][26/817]	Loss 0.0063 (0.0070)	
training:	Epoch: [36][27/817]	Loss 0.0067 (0.0070)	
training:	Epoch: [36][28/817]	Loss 0.0057 (0.0069)	
training:	Epoch: [36][29/817]	Loss 0.0091 (0.0070)	
training:	Epoch: [36][30/817]	Loss 0.0093 (0.0071)	
training:	Epoch: [36][31/817]	Loss 0.0055 (0.0070)	
training:	Epoch: [36][32/817]	Loss 0.0056 (0.0070)	
training:	Epoch: [36][33/817]	Loss 0.0060 (0.0069)	
training:	Epoch: [36][34/817]	Loss 0.0045 (0.0069)	
training:	Epoch: [36][35/817]	Loss 0.0059 (0.0068)	
training:	Epoch: [36][36/817]	Loss 0.5733 (0.0226)	
training:	Epoch: [36][37/817]	Loss 0.5940 (0.0380)	
training:	Epoch: [36][38/817]	Loss 0.0089 (0.0372)	
training:	Epoch: [36][39/817]	Loss 0.0048 (0.0364)	
training:	Epoch: [36][40/817]	Loss 0.0078 (0.0357)	
training:	Epoch: [36][41/817]	Loss 0.0086 (0.0350)	
training:	Epoch: [36][42/817]	Loss 0.0078 (0.0344)	
training:	Epoch: [36][43/817]	Loss 0.0051 (0.0337)	
training:	Epoch: [36][44/817]	Loss 0.0055 (0.0331)	
training:	Epoch: [36][45/817]	Loss 0.0083 (0.0325)	
training:	Epoch: [36][46/817]	Loss 0.0055 (0.0319)	
training:	Epoch: [36][47/817]	Loss 0.0063 (0.0314)	
training:	Epoch: [36][48/817]	Loss 0.5947 (0.0431)	
training:	Epoch: [36][49/817]	Loss 0.0127 (0.0425)	
training:	Epoch: [36][50/817]	Loss 0.0102 (0.0419)	
training:	Epoch: [36][51/817]	Loss 0.0041 (0.0411)	
training:	Epoch: [36][52/817]	Loss 0.0055 (0.0404)	
training:	Epoch: [36][53/817]	Loss 0.0072 (0.0398)	
training:	Epoch: [36][54/817]	Loss 0.6161 (0.0505)	
training:	Epoch: [36][55/817]	Loss 0.0061 (0.0497)	
training:	Epoch: [36][56/817]	Loss 0.0062 (0.0489)	
training:	Epoch: [36][57/817]	Loss 0.0076 (0.0482)	
training:	Epoch: [36][58/817]	Loss 0.0069 (0.0475)	
training:	Epoch: [36][59/817]	Loss 0.0104 (0.0468)	
training:	Epoch: [36][60/817]	Loss 0.0080 (0.0462)	
training:	Epoch: [36][61/817]	Loss 0.0060 (0.0455)	
training:	Epoch: [36][62/817]	Loss 0.0083 (0.0449)	
training:	Epoch: [36][63/817]	Loss 0.0061 (0.0443)	
training:	Epoch: [36][64/817]	Loss 0.0187 (0.0439)	
training:	Epoch: [36][65/817]	Loss 0.0078 (0.0434)	
training:	Epoch: [36][66/817]	Loss 0.0081 (0.0428)	
training:	Epoch: [36][67/817]	Loss 0.0091 (0.0423)	
training:	Epoch: [36][68/817]	Loss 0.0058 (0.0418)	
training:	Epoch: [36][69/817]	Loss 0.0062 (0.0413)	
training:	Epoch: [36][70/817]	Loss 0.0067 (0.0408)	
training:	Epoch: [36][71/817]	Loss 0.0065 (0.0403)	
training:	Epoch: [36][72/817]	Loss 0.0055 (0.0398)	
training:	Epoch: [36][73/817]	Loss 0.0059 (0.0393)	
training:	Epoch: [36][74/817]	Loss 0.0085 (0.0389)	
training:	Epoch: [36][75/817]	Loss 0.0055 (0.0385)	
training:	Epoch: [36][76/817]	Loss 0.0050 (0.0380)	
training:	Epoch: [36][77/817]	Loss 0.0079 (0.0376)	
training:	Epoch: [36][78/817]	Loss 0.5725 (0.0445)	
training:	Epoch: [36][79/817]	Loss 0.5601 (0.0510)	
training:	Epoch: [36][80/817]	Loss 0.0077 (0.0505)	
training:	Epoch: [36][81/817]	Loss 0.0050 (0.0499)	
training:	Epoch: [36][82/817]	Loss 0.0070 (0.0494)	
training:	Epoch: [36][83/817]	Loss 0.0069 (0.0489)	
training:	Epoch: [36][84/817]	Loss 0.0068 (0.0484)	
training:	Epoch: [36][85/817]	Loss 0.0079 (0.0479)	
training:	Epoch: [36][86/817]	Loss 0.0070 (0.0474)	
training:	Epoch: [36][87/817]	Loss 0.0064 (0.0470)	
training:	Epoch: [36][88/817]	Loss 0.0054 (0.0465)	
training:	Epoch: [36][89/817]	Loss 0.0068 (0.0460)	
training:	Epoch: [36][90/817]	Loss 0.0081 (0.0456)	
training:	Epoch: [36][91/817]	Loss 0.0077 (0.0452)	
training:	Epoch: [36][92/817]	Loss 0.0118 (0.0448)	
training:	Epoch: [36][93/817]	Loss 0.0096 (0.0445)	
training:	Epoch: [36][94/817]	Loss 0.0080 (0.0441)	
training:	Epoch: [36][95/817]	Loss 0.0075 (0.0437)	
training:	Epoch: [36][96/817]	Loss 0.0060 (0.0433)	
training:	Epoch: [36][97/817]	Loss 0.0088 (0.0429)	
training:	Epoch: [36][98/817]	Loss 0.0068 (0.0426)	
training:	Epoch: [36][99/817]	Loss 0.0066 (0.0422)	
training:	Epoch: [36][100/817]	Loss 0.0065 (0.0419)	
training:	Epoch: [36][101/817]	Loss 0.0079 (0.0415)	
training:	Epoch: [36][102/817]	Loss 0.0081 (0.0412)	
training:	Epoch: [36][103/817]	Loss 0.0162 (0.0410)	
training:	Epoch: [36][104/817]	Loss 0.0067 (0.0406)	
training:	Epoch: [36][105/817]	Loss 0.0186 (0.0404)	
training:	Epoch: [36][106/817]	Loss 0.0054 (0.0401)	
training:	Epoch: [36][107/817]	Loss 0.0053 (0.0398)	
training:	Epoch: [36][108/817]	Loss 0.0063 (0.0394)	
training:	Epoch: [36][109/817]	Loss 0.0061 (0.0391)	
training:	Epoch: [36][110/817]	Loss 0.0095 (0.0389)	
training:	Epoch: [36][111/817]	Loss 0.0091 (0.0386)	
training:	Epoch: [36][112/817]	Loss 0.0073 (0.0383)	
training:	Epoch: [36][113/817]	Loss 0.0050 (0.0380)	
training:	Epoch: [36][114/817]	Loss 0.0053 (0.0377)	
training:	Epoch: [36][115/817]	Loss 0.0071 (0.0375)	
training:	Epoch: [36][116/817]	Loss 0.0060 (0.0372)	
training:	Epoch: [36][117/817]	Loss 0.0078 (0.0370)	
training:	Epoch: [36][118/817]	Loss 0.0064 (0.0367)	
training:	Epoch: [36][119/817]	Loss 0.0083 (0.0365)	
training:	Epoch: [36][120/817]	Loss 0.0071 (0.0362)	
training:	Epoch: [36][121/817]	Loss 0.0065 (0.0360)	
training:	Epoch: [36][122/817]	Loss 0.0080 (0.0357)	
training:	Epoch: [36][123/817]	Loss 0.0045 (0.0355)	
training:	Epoch: [36][124/817]	Loss 0.0063 (0.0352)	
training:	Epoch: [36][125/817]	Loss 0.0167 (0.0351)	
training:	Epoch: [36][126/817]	Loss 0.0069 (0.0349)	
training:	Epoch: [36][127/817]	Loss 0.0066 (0.0347)	
training:	Epoch: [36][128/817]	Loss 0.0089 (0.0345)	
training:	Epoch: [36][129/817]	Loss 0.0043 (0.0342)	
training:	Epoch: [36][130/817]	Loss 0.0057 (0.0340)	
training:	Epoch: [36][131/817]	Loss 0.0057 (0.0338)	
training:	Epoch: [36][132/817]	Loss 0.0057 (0.0336)	
training:	Epoch: [36][133/817]	Loss 0.0066 (0.0334)	
training:	Epoch: [36][134/817]	Loss 0.0052 (0.0332)	
training:	Epoch: [36][135/817]	Loss 0.0048 (0.0329)	
training:	Epoch: [36][136/817]	Loss 0.0054 (0.0327)	
training:	Epoch: [36][137/817]	Loss 0.0070 (0.0326)	
training:	Epoch: [36][138/817]	Loss 0.0070 (0.0324)	
training:	Epoch: [36][139/817]	Loss 0.0045 (0.0322)	
training:	Epoch: [36][140/817]	Loss 0.0074 (0.0320)	
training:	Epoch: [36][141/817]	Loss 0.0050 (0.0318)	
training:	Epoch: [36][142/817]	Loss 0.0064 (0.0316)	
training:	Epoch: [36][143/817]	Loss 0.0082 (0.0315)	
training:	Epoch: [36][144/817]	Loss 0.0080 (0.0313)	
training:	Epoch: [36][145/817]	Loss 0.0068 (0.0311)	
training:	Epoch: [36][146/817]	Loss 0.0060 (0.0310)	
training:	Epoch: [36][147/817]	Loss 0.0064 (0.0308)	
training:	Epoch: [36][148/817]	Loss 0.0045 (0.0306)	
training:	Epoch: [36][149/817]	Loss 0.0065 (0.0304)	
training:	Epoch: [36][150/817]	Loss 0.0043 (0.0303)	
training:	Epoch: [36][151/817]	Loss 0.0053 (0.0301)	
training:	Epoch: [36][152/817]	Loss 0.0055 (0.0299)	
training:	Epoch: [36][153/817]	Loss 0.0059 (0.0298)	
training:	Epoch: [36][154/817]	Loss 0.0054 (0.0296)	
training:	Epoch: [36][155/817]	Loss 0.0037 (0.0295)	
training:	Epoch: [36][156/817]	Loss 0.0059 (0.0293)	
training:	Epoch: [36][157/817]	Loss 0.0056 (0.0292)	
training:	Epoch: [36][158/817]	Loss 0.0045 (0.0290)	
training:	Epoch: [36][159/817]	Loss 0.0077 (0.0289)	
training:	Epoch: [36][160/817]	Loss 0.0076 (0.0287)	
training:	Epoch: [36][161/817]	Loss 0.0050 (0.0286)	
training:	Epoch: [36][162/817]	Loss 0.0071 (0.0285)	
training:	Epoch: [36][163/817]	Loss 0.0053 (0.0283)	
training:	Epoch: [36][164/817]	Loss 0.0102 (0.0282)	
training:	Epoch: [36][165/817]	Loss 0.0043 (0.0281)	
training:	Epoch: [36][166/817]	Loss 0.0046 (0.0279)	
training:	Epoch: [36][167/817]	Loss 0.0060 (0.0278)	
training:	Epoch: [36][168/817]	Loss 0.0063 (0.0277)	
training:	Epoch: [36][169/817]	Loss 0.0085 (0.0275)	
training:	Epoch: [36][170/817]	Loss 0.0044 (0.0274)	
training:	Epoch: [36][171/817]	Loss 0.6012 (0.0308)	
training:	Epoch: [36][172/817]	Loss 0.0053 (0.0306)	
training:	Epoch: [36][173/817]	Loss 0.0079 (0.0305)	
training:	Epoch: [36][174/817]	Loss 0.0047 (0.0303)	
training:	Epoch: [36][175/817]	Loss 0.0043 (0.0302)	
training:	Epoch: [36][176/817]	Loss 0.0052 (0.0300)	
training:	Epoch: [36][177/817]	Loss 0.0047 (0.0299)	
training:	Epoch: [36][178/817]	Loss 0.0082 (0.0298)	
training:	Epoch: [36][179/817]	Loss 0.0125 (0.0297)	
training:	Epoch: [36][180/817]	Loss 0.0057 (0.0296)	
training:	Epoch: [36][181/817]	Loss 0.0051 (0.0294)	
training:	Epoch: [36][182/817]	Loss 0.0069 (0.0293)	
training:	Epoch: [36][183/817]	Loss 0.0054 (0.0292)	
training:	Epoch: [36][184/817]	Loss 0.0079 (0.0290)	
training:	Epoch: [36][185/817]	Loss 0.0058 (0.0289)	
training:	Epoch: [36][186/817]	Loss 0.0050 (0.0288)	
training:	Epoch: [36][187/817]	Loss 0.0076 (0.0287)	
training:	Epoch: [36][188/817]	Loss 0.0061 (0.0286)	
training:	Epoch: [36][189/817]	Loss 0.0041 (0.0284)	
training:	Epoch: [36][190/817]	Loss 0.0058 (0.0283)	
training:	Epoch: [36][191/817]	Loss 0.0047 (0.0282)	
training:	Epoch: [36][192/817]	Loss 0.0058 (0.0281)	
training:	Epoch: [36][193/817]	Loss 0.0070 (0.0280)	
training:	Epoch: [36][194/817]	Loss 0.0050 (0.0278)	
training:	Epoch: [36][195/817]	Loss 0.0058 (0.0277)	
training:	Epoch: [36][196/817]	Loss 0.0059 (0.0276)	
training:	Epoch: [36][197/817]	Loss 0.0055 (0.0275)	
training:	Epoch: [36][198/817]	Loss 0.0049 (0.0274)	
training:	Epoch: [36][199/817]	Loss 0.0066 (0.0273)	
training:	Epoch: [36][200/817]	Loss 0.0068 (0.0272)	
training:	Epoch: [36][201/817]	Loss 0.0045 (0.0271)	
training:	Epoch: [36][202/817]	Loss 0.0061 (0.0270)	
training:	Epoch: [36][203/817]	Loss 0.0055 (0.0269)	
training:	Epoch: [36][204/817]	Loss 0.0063 (0.0268)	
training:	Epoch: [36][205/817]	Loss 0.0054 (0.0267)	
training:	Epoch: [36][206/817]	Loss 0.0078 (0.0266)	
training:	Epoch: [36][207/817]	Loss 0.0048 (0.0265)	
training:	Epoch: [36][208/817]	Loss 0.0062 (0.0264)	
training:	Epoch: [36][209/817]	Loss 0.0060 (0.0263)	
training:	Epoch: [36][210/817]	Loss 0.0048 (0.0262)	
training:	Epoch: [36][211/817]	Loss 0.0079 (0.0261)	
training:	Epoch: [36][212/817]	Loss 0.0052 (0.0260)	
training:	Epoch: [36][213/817]	Loss 0.0040 (0.0259)	
training:	Epoch: [36][214/817]	Loss 0.0061 (0.0258)	
training:	Epoch: [36][215/817]	Loss 0.0040 (0.0257)	
training:	Epoch: [36][216/817]	Loss 0.0055 (0.0256)	
training:	Epoch: [36][217/817]	Loss 0.0067 (0.0255)	
training:	Epoch: [36][218/817]	Loss 0.0069 (0.0254)	
training:	Epoch: [36][219/817]	Loss 0.0049 (0.0253)	
training:	Epoch: [36][220/817]	Loss 0.0044 (0.0252)	
training:	Epoch: [36][221/817]	Loss 0.0043 (0.0251)	
training:	Epoch: [36][222/817]	Loss 0.0079 (0.0251)	
training:	Epoch: [36][223/817]	Loss 0.0052 (0.0250)	
training:	Epoch: [36][224/817]	Loss 0.0045 (0.0249)	
training:	Epoch: [36][225/817]	Loss 0.0068 (0.0248)	
training:	Epoch: [36][226/817]	Loss 0.0049 (0.0247)	
training:	Epoch: [36][227/817]	Loss 0.0078 (0.0246)	
training:	Epoch: [36][228/817]	Loss 0.0050 (0.0245)	
training:	Epoch: [36][229/817]	Loss 0.0064 (0.0245)	
training:	Epoch: [36][230/817]	Loss 0.0056 (0.0244)	
training:	Epoch: [36][231/817]	Loss 0.0062 (0.0243)	
training:	Epoch: [36][232/817]	Loss 0.0178 (0.0243)	
training:	Epoch: [36][233/817]	Loss 0.0063 (0.0242)	
training:	Epoch: [36][234/817]	Loss 0.0050 (0.0241)	
training:	Epoch: [36][235/817]	Loss 0.0051 (0.0240)	
training:	Epoch: [36][236/817]	Loss 0.0075 (0.0240)	
training:	Epoch: [36][237/817]	Loss 0.0056 (0.0239)	
training:	Epoch: [36][238/817]	Loss 0.0056 (0.0238)	
training:	Epoch: [36][239/817]	Loss 0.0053 (0.0237)	
training:	Epoch: [36][240/817]	Loss 0.0057 (0.0237)	
training:	Epoch: [36][241/817]	Loss 0.0069 (0.0236)	
training:	Epoch: [36][242/817]	Loss 0.0044 (0.0235)	
training:	Epoch: [36][243/817]	Loss 0.0064 (0.0234)	
training:	Epoch: [36][244/817]	Loss 0.0051 (0.0234)	
training:	Epoch: [36][245/817]	Loss 0.0064 (0.0233)	
training:	Epoch: [36][246/817]	Loss 0.0051 (0.0232)	
training:	Epoch: [36][247/817]	Loss 0.0045 (0.0231)	
training:	Epoch: [36][248/817]	Loss 0.0058 (0.0231)	
training:	Epoch: [36][249/817]	Loss 0.0050 (0.0230)	
training:	Epoch: [36][250/817]	Loss 0.0034 (0.0229)	
training:	Epoch: [36][251/817]	Loss 0.0053 (0.0229)	
training:	Epoch: [36][252/817]	Loss 0.0054 (0.0228)	
training:	Epoch: [36][253/817]	Loss 0.0057 (0.0227)	
training:	Epoch: [36][254/817]	Loss 0.0052 (0.0227)	
training:	Epoch: [36][255/817]	Loss 0.0048 (0.0226)	
training:	Epoch: [36][256/817]	Loss 0.0048 (0.0225)	
training:	Epoch: [36][257/817]	Loss 0.0047 (0.0224)	
training:	Epoch: [36][258/817]	Loss 0.0066 (0.0224)	
training:	Epoch: [36][259/817]	Loss 0.0083 (0.0223)	
training:	Epoch: [36][260/817]	Loss 0.0046 (0.0223)	
training:	Epoch: [36][261/817]	Loss 0.0100 (0.0222)	
training:	Epoch: [36][262/817]	Loss 0.0063 (0.0222)	
training:	Epoch: [36][263/817]	Loss 0.0044 (0.0221)	
training:	Epoch: [36][264/817]	Loss 0.0066 (0.0220)	
training:	Epoch: [36][265/817]	Loss 0.0057 (0.0220)	
training:	Epoch: [36][266/817]	Loss 0.0052 (0.0219)	
training:	Epoch: [36][267/817]	Loss 0.0038 (0.0218)	
training:	Epoch: [36][268/817]	Loss 0.0056 (0.0218)	
training:	Epoch: [36][269/817]	Loss 0.0053 (0.0217)	
training:	Epoch: [36][270/817]	Loss 0.0068 (0.0217)	
training:	Epoch: [36][271/817]	Loss 0.0042 (0.0216)	
training:	Epoch: [36][272/817]	Loss 0.0404 (0.0217)	
training:	Epoch: [36][273/817]	Loss 0.0051 (0.0216)	
training:	Epoch: [36][274/817]	Loss 0.0062 (0.0215)	
training:	Epoch: [36][275/817]	Loss 0.0059 (0.0215)	
training:	Epoch: [36][276/817]	Loss 0.0044 (0.0214)	
training:	Epoch: [36][277/817]	Loss 0.0040 (0.0214)	
training:	Epoch: [36][278/817]	Loss 0.0045 (0.0213)	
training:	Epoch: [36][279/817]	Loss 0.0060 (0.0212)	
training:	Epoch: [36][280/817]	Loss 0.0060 (0.0212)	
training:	Epoch: [36][281/817]	Loss 0.0039 (0.0211)	
training:	Epoch: [36][282/817]	Loss 0.0050 (0.0211)	
training:	Epoch: [36][283/817]	Loss 0.0048 (0.0210)	
training:	Epoch: [36][284/817]	Loss 0.0049 (0.0210)	
training:	Epoch: [36][285/817]	Loss 0.0086 (0.0209)	
training:	Epoch: [36][286/817]	Loss 0.0047 (0.0209)	
training:	Epoch: [36][287/817]	Loss 0.0043 (0.0208)	
training:	Epoch: [36][288/817]	Loss 0.0049 (0.0207)	
training:	Epoch: [36][289/817]	Loss 0.0051 (0.0207)	
training:	Epoch: [36][290/817]	Loss 0.0047 (0.0206)	
training:	Epoch: [36][291/817]	Loss 0.0054 (0.0206)	
training:	Epoch: [36][292/817]	Loss 0.0038 (0.0205)	
training:	Epoch: [36][293/817]	Loss 0.5066 (0.0222)	
training:	Epoch: [36][294/817]	Loss 0.0053 (0.0221)	
training:	Epoch: [36][295/817]	Loss 0.0057 (0.0221)	
training:	Epoch: [36][296/817]	Loss 0.0053 (0.0220)	
training:	Epoch: [36][297/817]	Loss 0.0051 (0.0220)	
training:	Epoch: [36][298/817]	Loss 0.0039 (0.0219)	
training:	Epoch: [36][299/817]	Loss 0.0054 (0.0218)	
training:	Epoch: [36][300/817]	Loss 0.0051 (0.0218)	
training:	Epoch: [36][301/817]	Loss 0.6111 (0.0237)	
training:	Epoch: [36][302/817]	Loss 0.0066 (0.0237)	
training:	Epoch: [36][303/817]	Loss 0.0042 (0.0236)	
training:	Epoch: [36][304/817]	Loss 0.0061 (0.0236)	
training:	Epoch: [36][305/817]	Loss 0.0053 (0.0235)	
training:	Epoch: [36][306/817]	Loss 0.0043 (0.0234)	
training:	Epoch: [36][307/817]	Loss 0.5033 (0.0250)	
training:	Epoch: [36][308/817]	Loss 0.0038 (0.0249)	
training:	Epoch: [36][309/817]	Loss 0.0070 (0.0249)	
training:	Epoch: [36][310/817]	Loss 0.0069 (0.0248)	
training:	Epoch: [36][311/817]	Loss 0.0062 (0.0248)	
training:	Epoch: [36][312/817]	Loss 0.0060 (0.0247)	
training:	Epoch: [36][313/817]	Loss 0.0043 (0.0246)	
training:	Epoch: [36][314/817]	Loss 0.0057 (0.0246)	
training:	Epoch: [36][315/817]	Loss 0.0054 (0.0245)	
training:	Epoch: [36][316/817]	Loss 0.0049 (0.0245)	
training:	Epoch: [36][317/817]	Loss 0.0045 (0.0244)	
training:	Epoch: [36][318/817]	Loss 0.0056 (0.0243)	
training:	Epoch: [36][319/817]	Loss 0.0068 (0.0243)	
training:	Epoch: [36][320/817]	Loss 0.0052 (0.0242)	
training:	Epoch: [36][321/817]	Loss 0.0064 (0.0242)	
training:	Epoch: [36][322/817]	Loss 0.0062 (0.0241)	
training:	Epoch: [36][323/817]	Loss 0.0064 (0.0240)	
training:	Epoch: [36][324/817]	Loss 0.0049 (0.0240)	
training:	Epoch: [36][325/817]	Loss 0.0076 (0.0239)	
training:	Epoch: [36][326/817]	Loss 0.0068 (0.0239)	
training:	Epoch: [36][327/817]	Loss 0.0057 (0.0238)	
training:	Epoch: [36][328/817]	Loss 0.0064 (0.0238)	
training:	Epoch: [36][329/817]	Loss 0.0051 (0.0237)	
training:	Epoch: [36][330/817]	Loss 0.0057 (0.0237)	
training:	Epoch: [36][331/817]	Loss 0.0059 (0.0236)	
training:	Epoch: [36][332/817]	Loss 0.0040 (0.0236)	
training:	Epoch: [36][333/817]	Loss 0.0050 (0.0235)	
training:	Epoch: [36][334/817]	Loss 0.0061 (0.0234)	
training:	Epoch: [36][335/817]	Loss 0.0057 (0.0234)	
training:	Epoch: [36][336/817]	Loss 0.0050 (0.0233)	
training:	Epoch: [36][337/817]	Loss 0.0055 (0.0233)	
training:	Epoch: [36][338/817]	Loss 0.0044 (0.0232)	
training:	Epoch: [36][339/817]	Loss 0.0050 (0.0232)	
training:	Epoch: [36][340/817]	Loss 0.0051 (0.0231)	
training:	Epoch: [36][341/817]	Loss 0.0081 (0.0231)	
training:	Epoch: [36][342/817]	Loss 0.0045 (0.0230)	
training:	Epoch: [36][343/817]	Loss 0.0057 (0.0230)	
training:	Epoch: [36][344/817]	Loss 0.0060 (0.0229)	
training:	Epoch: [36][345/817]	Loss 0.0054 (0.0229)	
training:	Epoch: [36][346/817]	Loss 0.0084 (0.0228)	
training:	Epoch: [36][347/817]	Loss 0.0045 (0.0228)	
training:	Epoch: [36][348/817]	Loss 0.0041 (0.0227)	
training:	Epoch: [36][349/817]	Loss 0.0056 (0.0227)	
training:	Epoch: [36][350/817]	Loss 0.0046 (0.0226)	
training:	Epoch: [36][351/817]	Loss 0.0063 (0.0226)	
training:	Epoch: [36][352/817]	Loss 0.0056 (0.0225)	
training:	Epoch: [36][353/817]	Loss 0.0045 (0.0225)	
training:	Epoch: [36][354/817]	Loss 0.0055 (0.0224)	
training:	Epoch: [36][355/817]	Loss 0.0047 (0.0224)	
training:	Epoch: [36][356/817]	Loss 0.0054 (0.0223)	
training:	Epoch: [36][357/817]	Loss 0.0037 (0.0223)	
training:	Epoch: [36][358/817]	Loss 0.0063 (0.0222)	
training:	Epoch: [36][359/817]	Loss 0.0054 (0.0222)	
training:	Epoch: [36][360/817]	Loss 0.0078 (0.0221)	
training:	Epoch: [36][361/817]	Loss 0.0080 (0.0221)	
training:	Epoch: [36][362/817]	Loss 0.0041 (0.0221)	
training:	Epoch: [36][363/817]	Loss 0.0047 (0.0220)	
training:	Epoch: [36][364/817]	Loss 0.6145 (0.0236)	
training:	Epoch: [36][365/817]	Loss 0.0044 (0.0236)	
training:	Epoch: [36][366/817]	Loss 0.0051 (0.0235)	
training:	Epoch: [36][367/817]	Loss 0.0049 (0.0235)	
training:	Epoch: [36][368/817]	Loss 0.0044 (0.0234)	
training:	Epoch: [36][369/817]	Loss 0.0055 (0.0234)	
training:	Epoch: [36][370/817]	Loss 0.0040 (0.0233)	
training:	Epoch: [36][371/817]	Loss 0.0059 (0.0233)	
training:	Epoch: [36][372/817]	Loss 0.0036 (0.0232)	
training:	Epoch: [36][373/817]	Loss 0.0050 (0.0232)	
training:	Epoch: [36][374/817]	Loss 0.0052 (0.0231)	
training:	Epoch: [36][375/817]	Loss 0.0047 (0.0231)	
training:	Epoch: [36][376/817]	Loss 0.0047 (0.0230)	
training:	Epoch: [36][377/817]	Loss 0.0048 (0.0230)	
training:	Epoch: [36][378/817]	Loss 0.0051 (0.0229)	
training:	Epoch: [36][379/817]	Loss 0.0066 (0.0229)	
training:	Epoch: [36][380/817]	Loss 0.0124 (0.0229)	
training:	Epoch: [36][381/817]	Loss 0.0045 (0.0228)	
training:	Epoch: [36][382/817]	Loss 0.0046 (0.0228)	
training:	Epoch: [36][383/817]	Loss 0.0067 (0.0227)	
training:	Epoch: [36][384/817]	Loss 0.0057 (0.0227)	
training:	Epoch: [36][385/817]	Loss 0.0051 (0.0226)	
training:	Epoch: [36][386/817]	Loss 0.0052 (0.0226)	
training:	Epoch: [36][387/817]	Loss 0.0057 (0.0226)	
training:	Epoch: [36][388/817]	Loss 0.0047 (0.0225)	
training:	Epoch: [36][389/817]	Loss 0.0057 (0.0225)	
training:	Epoch: [36][390/817]	Loss 0.0060 (0.0224)	
training:	Epoch: [36][391/817]	Loss 0.0046 (0.0224)	
training:	Epoch: [36][392/817]	Loss 0.0040 (0.0223)	
training:	Epoch: [36][393/817]	Loss 0.5851 (0.0238)	
training:	Epoch: [36][394/817]	Loss 0.0053 (0.0237)	
training:	Epoch: [36][395/817]	Loss 0.0060 (0.0237)	
training:	Epoch: [36][396/817]	Loss 0.0054 (0.0236)	
training:	Epoch: [36][397/817]	Loss 0.0067 (0.0236)	
training:	Epoch: [36][398/817]	Loss 0.0054 (0.0235)	
training:	Epoch: [36][399/817]	Loss 0.0045 (0.0235)	
training:	Epoch: [36][400/817]	Loss 0.5581 (0.0248)	
training:	Epoch: [36][401/817]	Loss 0.0051 (0.0248)	
training:	Epoch: [36][402/817]	Loss 0.0053 (0.0247)	
training:	Epoch: [36][403/817]	Loss 0.0062 (0.0247)	
training:	Epoch: [36][404/817]	Loss 0.0070 (0.0246)	
training:	Epoch: [36][405/817]	Loss 0.0058 (0.0246)	
training:	Epoch: [36][406/817]	Loss 0.0061 (0.0245)	
training:	Epoch: [36][407/817]	Loss 0.0053 (0.0245)	
training:	Epoch: [36][408/817]	Loss 0.0061 (0.0245)	
training:	Epoch: [36][409/817]	Loss 0.0089 (0.0244)	
training:	Epoch: [36][410/817]	Loss 0.5957 (0.0258)	
training:	Epoch: [36][411/817]	Loss 0.0150 (0.0258)	
training:	Epoch: [36][412/817]	Loss 0.0056 (0.0257)	
training:	Epoch: [36][413/817]	Loss 0.0047 (0.0257)	
training:	Epoch: [36][414/817]	Loss 0.0042 (0.0256)	
training:	Epoch: [36][415/817]	Loss 0.0046 (0.0256)	
training:	Epoch: [36][416/817]	Loss 0.0046 (0.0255)	
training:	Epoch: [36][417/817]	Loss 0.0050 (0.0255)	
training:	Epoch: [36][418/817]	Loss 0.0052 (0.0254)	
training:	Epoch: [36][419/817]	Loss 0.4747 (0.0265)	
training:	Epoch: [36][420/817]	Loss 0.0067 (0.0265)	
training:	Epoch: [36][421/817]	Loss 0.1063 (0.0266)	
training:	Epoch: [36][422/817]	Loss 0.0050 (0.0266)	
training:	Epoch: [36][423/817]	Loss 0.0065 (0.0265)	
training:	Epoch: [36][424/817]	Loss 0.0062 (0.0265)	
training:	Epoch: [36][425/817]	Loss 0.0054 (0.0265)	
training:	Epoch: [36][426/817]	Loss 0.0055 (0.0264)	
training:	Epoch: [36][427/817]	Loss 0.0057 (0.0264)	
training:	Epoch: [36][428/817]	Loss 0.0099 (0.0263)	
training:	Epoch: [36][429/817]	Loss 0.0046 (0.0263)	
training:	Epoch: [36][430/817]	Loss 0.0162 (0.0262)	
training:	Epoch: [36][431/817]	Loss 0.0070 (0.0262)	
training:	Epoch: [36][432/817]	Loss 0.0046 (0.0261)	
training:	Epoch: [36][433/817]	Loss 0.0055 (0.0261)	
training:	Epoch: [36][434/817]	Loss 0.0065 (0.0261)	
training:	Epoch: [36][435/817]	Loss 0.0051 (0.0260)	
training:	Epoch: [36][436/817]	Loss 0.0092 (0.0260)	
training:	Epoch: [36][437/817]	Loss 0.0044 (0.0259)	
training:	Epoch: [36][438/817]	Loss 0.0057 (0.0259)	
training:	Epoch: [36][439/817]	Loss 0.0063 (0.0258)	
training:	Epoch: [36][440/817]	Loss 0.0085 (0.0258)	
training:	Epoch: [36][441/817]	Loss 0.0055 (0.0257)	
training:	Epoch: [36][442/817]	Loss 0.5298 (0.0269)	
training:	Epoch: [36][443/817]	Loss 0.0059 (0.0268)	
training:	Epoch: [36][444/817]	Loss 0.0061 (0.0268)	
training:	Epoch: [36][445/817]	Loss 0.6064 (0.0281)	
training:	Epoch: [36][446/817]	Loss 0.0044 (0.0280)	
training:	Epoch: [36][447/817]	Loss 0.0076 (0.0280)	
training:	Epoch: [36][448/817]	Loss 0.0065 (0.0279)	
training:	Epoch: [36][449/817]	Loss 0.0062 (0.0279)	
training:	Epoch: [36][450/817]	Loss 0.0057 (0.0278)	
training:	Epoch: [36][451/817]	Loss 0.0063 (0.0278)	
training:	Epoch: [36][452/817]	Loss 0.0040 (0.0277)	
training:	Epoch: [36][453/817]	Loss 0.0058 (0.0277)	
training:	Epoch: [36][454/817]	Loss 0.0086 (0.0277)	
training:	Epoch: [36][455/817]	Loss 0.0071 (0.0276)	
training:	Epoch: [36][456/817]	Loss 0.0061 (0.0276)	
training:	Epoch: [36][457/817]	Loss 0.0055 (0.0275)	
training:	Epoch: [36][458/817]	Loss 0.0043 (0.0275)	
training:	Epoch: [36][459/817]	Loss 0.0039 (0.0274)	
training:	Epoch: [36][460/817]	Loss 0.0063 (0.0274)	
training:	Epoch: [36][461/817]	Loss 0.0070 (0.0273)	
training:	Epoch: [36][462/817]	Loss 0.0052 (0.0273)	
training:	Epoch: [36][463/817]	Loss 0.0069 (0.0272)	
training:	Epoch: [36][464/817]	Loss 0.0060 (0.0272)	
training:	Epoch: [36][465/817]	Loss 0.0074 (0.0271)	
training:	Epoch: [36][466/817]	Loss 0.0058 (0.0271)	
training:	Epoch: [36][467/817]	Loss 0.0062 (0.0271)	
training:	Epoch: [36][468/817]	Loss 0.0075 (0.0270)	
training:	Epoch: [36][469/817]	Loss 0.0061 (0.0270)	
training:	Epoch: [36][470/817]	Loss 0.0050 (0.0269)	
training:	Epoch: [36][471/817]	Loss 0.0045 (0.0269)	
training:	Epoch: [36][472/817]	Loss 0.0103 (0.0268)	
training:	Epoch: [36][473/817]	Loss 0.0044 (0.0268)	
training:	Epoch: [36][474/817]	Loss 0.6121 (0.0280)	
training:	Epoch: [36][475/817]	Loss 0.0076 (0.0280)	
training:	Epoch: [36][476/817]	Loss 0.0044 (0.0279)	
training:	Epoch: [36][477/817]	Loss 0.0068 (0.0279)	
training:	Epoch: [36][478/817]	Loss 0.0080 (0.0278)	
training:	Epoch: [36][479/817]	Loss 0.0074 (0.0278)	
training:	Epoch: [36][480/817]	Loss 0.5717 (0.0289)	
training:	Epoch: [36][481/817]	Loss 0.0112 (0.0289)	
training:	Epoch: [36][482/817]	Loss 0.0088 (0.0289)	
training:	Epoch: [36][483/817]	Loss 0.0057 (0.0288)	
training:	Epoch: [36][484/817]	Loss 0.0067 (0.0288)	
training:	Epoch: [36][485/817]	Loss 0.0055 (0.0287)	
training:	Epoch: [36][486/817]	Loss 0.0045 (0.0287)	
training:	Epoch: [36][487/817]	Loss 0.0062 (0.0286)	
training:	Epoch: [36][488/817]	Loss 0.0052 (0.0286)	
training:	Epoch: [36][489/817]	Loss 0.0073 (0.0285)	
training:	Epoch: [36][490/817]	Loss 0.0073 (0.0285)	
training:	Epoch: [36][491/817]	Loss 0.0064 (0.0284)	
training:	Epoch: [36][492/817]	Loss 0.0073 (0.0284)	
training:	Epoch: [36][493/817]	Loss 0.0053 (0.0284)	
training:	Epoch: [36][494/817]	Loss 0.0051 (0.0283)	
training:	Epoch: [36][495/817]	Loss 0.0065 (0.0283)	
training:	Epoch: [36][496/817]	Loss 0.0053 (0.0282)	
training:	Epoch: [36][497/817]	Loss 0.0053 (0.0282)	
training:	Epoch: [36][498/817]	Loss 0.0068 (0.0281)	
training:	Epoch: [36][499/817]	Loss 0.0050 (0.0281)	
training:	Epoch: [36][500/817]	Loss 0.0073 (0.0280)	
training:	Epoch: [36][501/817]	Loss 0.0057 (0.0280)	
training:	Epoch: [36][502/817]	Loss 0.0064 (0.0279)	
training:	Epoch: [36][503/817]	Loss 0.0044 (0.0279)	
training:	Epoch: [36][504/817]	Loss 0.0064 (0.0279)	
training:	Epoch: [36][505/817]	Loss 0.0050 (0.0278)	
training:	Epoch: [36][506/817]	Loss 0.0086 (0.0278)	
training:	Epoch: [36][507/817]	Loss 0.5621 (0.0288)	
training:	Epoch: [36][508/817]	Loss 0.0150 (0.0288)	
training:	Epoch: [36][509/817]	Loss 0.0053 (0.0288)	
training:	Epoch: [36][510/817]	Loss 0.0050 (0.0287)	
training:	Epoch: [36][511/817]	Loss 0.0068 (0.0287)	
training:	Epoch: [36][512/817]	Loss 0.0059 (0.0286)	
training:	Epoch: [36][513/817]	Loss 0.0049 (0.0286)	
training:	Epoch: [36][514/817]	Loss 0.0059 (0.0285)	
training:	Epoch: [36][515/817]	Loss 0.0074 (0.0285)	
training:	Epoch: [36][516/817]	Loss 0.0049 (0.0284)	
training:	Epoch: [36][517/817]	Loss 0.0048 (0.0284)	
training:	Epoch: [36][518/817]	Loss 0.0062 (0.0284)	
training:	Epoch: [36][519/817]	Loss 0.0041 (0.0283)	
training:	Epoch: [36][520/817]	Loss 0.0050 (0.0283)	
training:	Epoch: [36][521/817]	Loss 0.0066 (0.0282)	
training:	Epoch: [36][522/817]	Loss 0.0076 (0.0282)	
training:	Epoch: [36][523/817]	Loss 0.0066 (0.0281)	
training:	Epoch: [36][524/817]	Loss 0.0070 (0.0281)	
training:	Epoch: [36][525/817]	Loss 0.0170 (0.0281)	
training:	Epoch: [36][526/817]	Loss 0.0058 (0.0280)	
training:	Epoch: [36][527/817]	Loss 0.0051 (0.0280)	
training:	Epoch: [36][528/817]	Loss 0.0048 (0.0280)	
training:	Epoch: [36][529/817]	Loss 0.0069 (0.0279)	
training:	Epoch: [36][530/817]	Loss 0.0055 (0.0279)	
training:	Epoch: [36][531/817]	Loss 0.0067 (0.0278)	
training:	Epoch: [36][532/817]	Loss 0.0092 (0.0278)	
training:	Epoch: [36][533/817]	Loss 0.0069 (0.0278)	
training:	Epoch: [36][534/817]	Loss 0.0048 (0.0277)	
training:	Epoch: [36][535/817]	Loss 0.0067 (0.0277)	
training:	Epoch: [36][536/817]	Loss 0.0066 (0.0276)	
training:	Epoch: [36][537/817]	Loss 0.0067 (0.0276)	
training:	Epoch: [36][538/817]	Loss 0.0053 (0.0276)	
training:	Epoch: [36][539/817]	Loss 0.0073 (0.0275)	
training:	Epoch: [36][540/817]	Loss 0.0062 (0.0275)	
training:	Epoch: [36][541/817]	Loss 0.0078 (0.0274)	
training:	Epoch: [36][542/817]	Loss 0.0060 (0.0274)	
training:	Epoch: [36][543/817]	Loss 0.5579 (0.0284)	
training:	Epoch: [36][544/817]	Loss 0.0110 (0.0283)	
training:	Epoch: [36][545/817]	Loss 0.4671 (0.0292)	
training:	Epoch: [36][546/817]	Loss 0.0065 (0.0291)	
training:	Epoch: [36][547/817]	Loss 0.5787 (0.0301)	
training:	Epoch: [36][548/817]	Loss 0.0091 (0.0301)	
training:	Epoch: [36][549/817]	Loss 0.0052 (0.0300)	
training:	Epoch: [36][550/817]	Loss 0.0054 (0.0300)	
training:	Epoch: [36][551/817]	Loss 0.0098 (0.0299)	
training:	Epoch: [36][552/817]	Loss 0.0097 (0.0299)	
training:	Epoch: [36][553/817]	Loss 0.0077 (0.0299)	
training:	Epoch: [36][554/817]	Loss 0.0093 (0.0298)	
training:	Epoch: [36][555/817]	Loss 0.0066 (0.0298)	
training:	Epoch: [36][556/817]	Loss 0.0070 (0.0298)	
training:	Epoch: [36][557/817]	Loss 0.0043 (0.0297)	
training:	Epoch: [36][558/817]	Loss 0.0080 (0.0297)	
training:	Epoch: [36][559/817]	Loss 0.0057 (0.0296)	
training:	Epoch: [36][560/817]	Loss 0.0071 (0.0296)	
training:	Epoch: [36][561/817]	Loss 0.0065 (0.0295)	
training:	Epoch: [36][562/817]	Loss 0.0105 (0.0295)	
training:	Epoch: [36][563/817]	Loss 0.0111 (0.0295)	
training:	Epoch: [36][564/817]	Loss 0.0070 (0.0294)	
training:	Epoch: [36][565/817]	Loss 0.0076 (0.0294)	
training:	Epoch: [36][566/817]	Loss 0.0075 (0.0294)	
training:	Epoch: [36][567/817]	Loss 0.0065 (0.0293)	
training:	Epoch: [36][568/817]	Loss 0.0048 (0.0293)	
training:	Epoch: [36][569/817]	Loss 0.0072 (0.0292)	
training:	Epoch: [36][570/817]	Loss 0.0064 (0.0292)	
training:	Epoch: [36][571/817]	Loss 0.5021 (0.0300)	
training:	Epoch: [36][572/817]	Loss 0.0075 (0.0300)	
training:	Epoch: [36][573/817]	Loss 0.0045 (0.0299)	
training:	Epoch: [36][574/817]	Loss 0.0051 (0.0299)	
training:	Epoch: [36][575/817]	Loss 0.0080 (0.0299)	
training:	Epoch: [36][576/817]	Loss 0.0076 (0.0298)	
training:	Epoch: [36][577/817]	Loss 0.0076 (0.0298)	
training:	Epoch: [36][578/817]	Loss 0.0064 (0.0297)	
training:	Epoch: [36][579/817]	Loss 0.0073 (0.0297)	
training:	Epoch: [36][580/817]	Loss 0.0091 (0.0297)	
training:	Epoch: [36][581/817]	Loss 0.0089 (0.0296)	
training:	Epoch: [36][582/817]	Loss 0.0053 (0.0296)	
training:	Epoch: [36][583/817]	Loss 0.0059 (0.0295)	
training:	Epoch: [36][584/817]	Loss 0.0061 (0.0295)	
training:	Epoch: [36][585/817]	Loss 0.0053 (0.0295)	
training:	Epoch: [36][586/817]	Loss 0.0064 (0.0294)	
training:	Epoch: [36][587/817]	Loss 0.0130 (0.0294)	
training:	Epoch: [36][588/817]	Loss 0.0080 (0.0294)	
training:	Epoch: [36][589/817]	Loss 0.0132 (0.0293)	
training:	Epoch: [36][590/817]	Loss 0.0079 (0.0293)	
training:	Epoch: [36][591/817]	Loss 0.0078 (0.0293)	
training:	Epoch: [36][592/817]	Loss 0.0062 (0.0292)	
training:	Epoch: [36][593/817]	Loss 0.0140 (0.0292)	
training:	Epoch: [36][594/817]	Loss 0.0069 (0.0292)	
training:	Epoch: [36][595/817]	Loss 0.4967 (0.0299)	
training:	Epoch: [36][596/817]	Loss 0.0074 (0.0299)	
training:	Epoch: [36][597/817]	Loss 0.0061 (0.0299)	
training:	Epoch: [36][598/817]	Loss 0.0057 (0.0298)	
training:	Epoch: [36][599/817]	Loss 0.0042 (0.0298)	
training:	Epoch: [36][600/817]	Loss 0.0106 (0.0298)	
training:	Epoch: [36][601/817]	Loss 0.0065 (0.0297)	
training:	Epoch: [36][602/817]	Loss 0.0093 (0.0297)	
training:	Epoch: [36][603/817]	Loss 0.0110 (0.0297)	
training:	Epoch: [36][604/817]	Loss 0.0080 (0.0296)	
training:	Epoch: [36][605/817]	Loss 0.0085 (0.0296)	
training:	Epoch: [36][606/817]	Loss 0.0060 (0.0295)	
training:	Epoch: [36][607/817]	Loss 0.0067 (0.0295)	
training:	Epoch: [36][608/817]	Loss 0.0070 (0.0295)	
training:	Epoch: [36][609/817]	Loss 0.0063 (0.0294)	
training:	Epoch: [36][610/817]	Loss 0.5690 (0.0303)	
training:	Epoch: [36][611/817]	Loss 0.0052 (0.0303)	
training:	Epoch: [36][612/817]	Loss 0.0080 (0.0302)	
training:	Epoch: [36][613/817]	Loss 0.0110 (0.0302)	
training:	Epoch: [36][614/817]	Loss 0.0056 (0.0302)	
training:	Epoch: [36][615/817]	Loss 0.0099 (0.0301)	
training:	Epoch: [36][616/817]	Loss 0.0073 (0.0301)	
training:	Epoch: [36][617/817]	Loss 0.0046 (0.0301)	
training:	Epoch: [36][618/817]	Loss 0.0061 (0.0300)	
training:	Epoch: [36][619/817]	Loss 0.0054 (0.0300)	
training:	Epoch: [36][620/817]	Loss 0.0091 (0.0299)	
training:	Epoch: [36][621/817]	Loss 0.0070 (0.0299)	
training:	Epoch: [36][622/817]	Loss 0.0107 (0.0299)	
training:	Epoch: [36][623/817]	Loss 0.0066 (0.0298)	
training:	Epoch: [36][624/817]	Loss 0.0052 (0.0298)	
training:	Epoch: [36][625/817]	Loss 0.0314 (0.0298)	
training:	Epoch: [36][626/817]	Loss 0.0073 (0.0298)	
training:	Epoch: [36][627/817]	Loss 0.0084 (0.0297)	
training:	Epoch: [36][628/817]	Loss 0.0079 (0.0297)	
training:	Epoch: [36][629/817]	Loss 0.0080 (0.0297)	
training:	Epoch: [36][630/817]	Loss 0.4008 (0.0302)	
training:	Epoch: [36][631/817]	Loss 0.0048 (0.0302)	
training:	Epoch: [36][632/817]	Loss 0.0060 (0.0302)	
training:	Epoch: [36][633/817]	Loss 0.0104 (0.0301)	
training:	Epoch: [36][634/817]	Loss 0.0059 (0.0301)	
training:	Epoch: [36][635/817]	Loss 0.0089 (0.0301)	
training:	Epoch: [36][636/817]	Loss 0.0123 (0.0300)	
training:	Epoch: [36][637/817]	Loss 0.0092 (0.0300)	
training:	Epoch: [36][638/817]	Loss 0.0061 (0.0300)	
training:	Epoch: [36][639/817]	Loss 0.0064 (0.0299)	
training:	Epoch: [36][640/817]	Loss 0.0082 (0.0299)	
training:	Epoch: [36][641/817]	Loss 0.0160 (0.0299)	
training:	Epoch: [36][642/817]	Loss 0.5529 (0.0307)	
training:	Epoch: [36][643/817]	Loss 0.0082 (0.0307)	
training:	Epoch: [36][644/817]	Loss 0.0138 (0.0306)	
training:	Epoch: [36][645/817]	Loss 0.0047 (0.0306)	
training:	Epoch: [36][646/817]	Loss 0.0074 (0.0306)	
training:	Epoch: [36][647/817]	Loss 0.0072 (0.0305)	
training:	Epoch: [36][648/817]	Loss 0.0087 (0.0305)	
training:	Epoch: [36][649/817]	Loss 0.0056 (0.0304)	
training:	Epoch: [36][650/817]	Loss 0.0098 (0.0304)	
training:	Epoch: [36][651/817]	Loss 0.0088 (0.0304)	
training:	Epoch: [36][652/817]	Loss 0.0100 (0.0303)	
training:	Epoch: [36][653/817]	Loss 0.0050 (0.0303)	
training:	Epoch: [36][654/817]	Loss 0.0070 (0.0303)	
training:	Epoch: [36][655/817]	Loss 0.0042 (0.0302)	
training:	Epoch: [36][656/817]	Loss 0.0098 (0.0302)	
training:	Epoch: [36][657/817]	Loss 0.0113 (0.0302)	
training:	Epoch: [36][658/817]	Loss 0.0092 (0.0301)	
training:	Epoch: [36][659/817]	Loss 0.0053 (0.0301)	
training:	Epoch: [36][660/817]	Loss 0.0060 (0.0301)	
training:	Epoch: [36][661/817]	Loss 0.0068 (0.0300)	
training:	Epoch: [36][662/817]	Loss 0.0080 (0.0300)	
training:	Epoch: [36][663/817]	Loss 0.0116 (0.0300)	
training:	Epoch: [36][664/817]	Loss 0.0120 (0.0299)	
training:	Epoch: [36][665/817]	Loss 0.0064 (0.0299)	
training:	Epoch: [36][666/817]	Loss 0.0075 (0.0299)	
training:	Epoch: [36][667/817]	Loss 0.0085 (0.0298)	
training:	Epoch: [36][668/817]	Loss 0.0061 (0.0298)	
training:	Epoch: [36][669/817]	Loss 0.0093 (0.0298)	
training:	Epoch: [36][670/817]	Loss 0.0103 (0.0297)	
training:	Epoch: [36][671/817]	Loss 0.0111 (0.0297)	
training:	Epoch: [36][672/817]	Loss 0.0148 (0.0297)	
training:	Epoch: [36][673/817]	Loss 0.0040 (0.0297)	
training:	Epoch: [36][674/817]	Loss 0.0114 (0.0296)	
training:	Epoch: [36][675/817]	Loss 0.0075 (0.0296)	
training:	Epoch: [36][676/817]	Loss 0.0074 (0.0296)	
training:	Epoch: [36][677/817]	Loss 0.5952 (0.0304)	
training:	Epoch: [36][678/817]	Loss 0.0097 (0.0304)	
training:	Epoch: [36][679/817]	Loss 0.0055 (0.0303)	
training:	Epoch: [36][680/817]	Loss 0.0092 (0.0303)	
training:	Epoch: [36][681/817]	Loss 0.0067 (0.0303)	
training:	Epoch: [36][682/817]	Loss 0.0062 (0.0302)	
training:	Epoch: [36][683/817]	Loss 0.0054 (0.0302)	
training:	Epoch: [36][684/817]	Loss 0.0082 (0.0302)	
training:	Epoch: [36][685/817]	Loss 0.0077 (0.0301)	
training:	Epoch: [36][686/817]	Loss 0.0051 (0.0301)	
training:	Epoch: [36][687/817]	Loss 0.0123 (0.0301)	
training:	Epoch: [36][688/817]	Loss 0.0073 (0.0300)	
training:	Epoch: [36][689/817]	Loss 0.0082 (0.0300)	
training:	Epoch: [36][690/817]	Loss 0.0055 (0.0300)	
training:	Epoch: [36][691/817]	Loss 0.0065 (0.0299)	
training:	Epoch: [36][692/817]	Loss 0.0042 (0.0299)	
training:	Epoch: [36][693/817]	Loss 0.0066 (0.0299)	
training:	Epoch: [36][694/817]	Loss 0.0072 (0.0298)	
training:	Epoch: [36][695/817]	Loss 0.0067 (0.0298)	
training:	Epoch: [36][696/817]	Loss 0.0061 (0.0298)	
training:	Epoch: [36][697/817]	Loss 0.0115 (0.0297)	
training:	Epoch: [36][698/817]	Loss 0.0090 (0.0297)	
training:	Epoch: [36][699/817]	Loss 0.0062 (0.0297)	
training:	Epoch: [36][700/817]	Loss 0.0088 (0.0296)	
training:	Epoch: [36][701/817]	Loss 0.0059 (0.0296)	
training:	Epoch: [36][702/817]	Loss 0.0107 (0.0296)	
training:	Epoch: [36][703/817]	Loss 0.0062 (0.0296)	
training:	Epoch: [36][704/817]	Loss 0.0076 (0.0295)	
training:	Epoch: [36][705/817]	Loss 0.0077 (0.0295)	
training:	Epoch: [36][706/817]	Loss 0.0063 (0.0295)	
training:	Epoch: [36][707/817]	Loss 0.0102 (0.0294)	
training:	Epoch: [36][708/817]	Loss 0.4710 (0.0301)	
training:	Epoch: [36][709/817]	Loss 0.0080 (0.0300)	
training:	Epoch: [36][710/817]	Loss 0.0087 (0.0300)	
training:	Epoch: [36][711/817]	Loss 0.0080 (0.0300)	
training:	Epoch: [36][712/817]	Loss 0.0062 (0.0299)	
training:	Epoch: [36][713/817]	Loss 0.0094 (0.0299)	
training:	Epoch: [36][714/817]	Loss 0.0071 (0.0299)	
training:	Epoch: [36][715/817]	Loss 0.0050 (0.0298)	
training:	Epoch: [36][716/817]	Loss 0.0073 (0.0298)	
training:	Epoch: [36][717/817]	Loss 0.4684 (0.0304)	
training:	Epoch: [36][718/817]	Loss 0.0083 (0.0304)	
training:	Epoch: [36][719/817]	Loss 0.0061 (0.0303)	
training:	Epoch: [36][720/817]	Loss 0.0119 (0.0303)	
training:	Epoch: [36][721/817]	Loss 0.0080 (0.0303)	
training:	Epoch: [36][722/817]	Loss 0.0066 (0.0303)	
training:	Epoch: [36][723/817]	Loss 0.0070 (0.0302)	
training:	Epoch: [36][724/817]	Loss 0.0043 (0.0302)	
training:	Epoch: [36][725/817]	Loss 0.0064 (0.0302)	
training:	Epoch: [36][726/817]	Loss 0.0082 (0.0301)	
training:	Epoch: [36][727/817]	Loss 0.4981 (0.0308)	
training:	Epoch: [36][728/817]	Loss 0.0110 (0.0307)	
training:	Epoch: [36][729/817]	Loss 0.4678 (0.0313)	
training:	Epoch: [36][730/817]	Loss 0.0106 (0.0313)	
training:	Epoch: [36][731/817]	Loss 0.0093 (0.0313)	
training:	Epoch: [36][732/817]	Loss 0.0073 (0.0313)	
training:	Epoch: [36][733/817]	Loss 0.0078 (0.0312)	
training:	Epoch: [36][734/817]	Loss 0.0069 (0.0312)	
training:	Epoch: [36][735/817]	Loss 0.0067 (0.0312)	
training:	Epoch: [36][736/817]	Loss 0.0067 (0.0311)	
training:	Epoch: [36][737/817]	Loss 0.0116 (0.0311)	
training:	Epoch: [36][738/817]	Loss 0.0074 (0.0311)	
training:	Epoch: [36][739/817]	Loss 0.0065 (0.0310)	
training:	Epoch: [36][740/817]	Loss 0.0058 (0.0310)	
training:	Epoch: [36][741/817]	Loss 0.0098 (0.0310)	
training:	Epoch: [36][742/817]	Loss 0.0123 (0.0309)	
training:	Epoch: [36][743/817]	Loss 0.0142 (0.0309)	
training:	Epoch: [36][744/817]	Loss 0.0078 (0.0309)	
training:	Epoch: [36][745/817]	Loss 0.0079 (0.0309)	
training:	Epoch: [36][746/817]	Loss 0.0080 (0.0308)	
training:	Epoch: [36][747/817]	Loss 0.0080 (0.0308)	
training:	Epoch: [36][748/817]	Loss 0.0107 (0.0308)	
training:	Epoch: [36][749/817]	Loss 0.0098 (0.0307)	
training:	Epoch: [36][750/817]	Loss 0.0104 (0.0307)	
training:	Epoch: [36][751/817]	Loss 0.0046 (0.0307)	
training:	Epoch: [36][752/817]	Loss 0.0072 (0.0306)	
training:	Epoch: [36][753/817]	Loss 0.0067 (0.0306)	
training:	Epoch: [36][754/817]	Loss 0.0126 (0.0306)	
training:	Epoch: [36][755/817]	Loss 0.0069 (0.0306)	
training:	Epoch: [36][756/817]	Loss 0.0082 (0.0305)	
training:	Epoch: [36][757/817]	Loss 0.0090 (0.0305)	
training:	Epoch: [36][758/817]	Loss 0.0068 (0.0305)	
training:	Epoch: [36][759/817]	Loss 0.0064 (0.0304)	
training:	Epoch: [36][760/817]	Loss 0.4264 (0.0310)	
training:	Epoch: [36][761/817]	Loss 0.6657 (0.0318)	
training:	Epoch: [36][762/817]	Loss 0.0067 (0.0318)	
training:	Epoch: [36][763/817]	Loss 0.0063 (0.0317)	
training:	Epoch: [36][764/817]	Loss 0.0048 (0.0317)	
training:	Epoch: [36][765/817]	Loss 0.0088 (0.0317)	
training:	Epoch: [36][766/817]	Loss 0.0099 (0.0316)	
training:	Epoch: [36][767/817]	Loss 0.0086 (0.0316)	
training:	Epoch: [36][768/817]	Loss 0.0062 (0.0316)	
training:	Epoch: [36][769/817]	Loss 0.0071 (0.0315)	
training:	Epoch: [36][770/817]	Loss 0.0051 (0.0315)	
training:	Epoch: [36][771/817]	Loss 0.0060 (0.0315)	
training:	Epoch: [36][772/817]	Loss 0.0074 (0.0314)	
training:	Epoch: [36][773/817]	Loss 0.0088 (0.0314)	
training:	Epoch: [36][774/817]	Loss 0.0043 (0.0314)	
training:	Epoch: [36][775/817]	Loss 0.0065 (0.0313)	
training:	Epoch: [36][776/817]	Loss 0.0089 (0.0313)	
training:	Epoch: [36][777/817]	Loss 0.0057 (0.0313)	
training:	Epoch: [36][778/817]	Loss 0.0124 (0.0313)	
training:	Epoch: [36][779/817]	Loss 0.0050 (0.0312)	
training:	Epoch: [36][780/817]	Loss 0.0068 (0.0312)	
training:	Epoch: [36][781/817]	Loss 0.4364 (0.0317)	
training:	Epoch: [36][782/817]	Loss 0.0053 (0.0317)	
training:	Epoch: [36][783/817]	Loss 0.0057 (0.0316)	
training:	Epoch: [36][784/817]	Loss 0.0064 (0.0316)	
training:	Epoch: [36][785/817]	Loss 0.0078 (0.0316)	
training:	Epoch: [36][786/817]	Loss 0.0046 (0.0315)	
training:	Epoch: [36][787/817]	Loss 0.0125 (0.0315)	
training:	Epoch: [36][788/817]	Loss 0.0107 (0.0315)	
training:	Epoch: [36][789/817]	Loss 0.0115 (0.0315)	
training:	Epoch: [36][790/817]	Loss 0.0079 (0.0314)	
training:	Epoch: [36][791/817]	Loss 0.0044 (0.0314)	
training:	Epoch: [36][792/817]	Loss 0.0084 (0.0314)	
training:	Epoch: [36][793/817]	Loss 0.0066 (0.0313)	
training:	Epoch: [36][794/817]	Loss 0.0069 (0.0313)	
training:	Epoch: [36][795/817]	Loss 0.0063 (0.0313)	
training:	Epoch: [36][796/817]	Loss 0.0098 (0.0313)	
training:	Epoch: [36][797/817]	Loss 0.0055 (0.0312)	
training:	Epoch: [36][798/817]	Loss 0.0057 (0.0312)	
training:	Epoch: [36][799/817]	Loss 0.0078 (0.0312)	
training:	Epoch: [36][800/817]	Loss 0.0066 (0.0311)	
training:	Epoch: [36][801/817]	Loss 0.0101 (0.0311)	
training:	Epoch: [36][802/817]	Loss 0.0080 (0.0311)	
training:	Epoch: [36][803/817]	Loss 0.0063 (0.0310)	
training:	Epoch: [36][804/817]	Loss 0.0059 (0.0310)	
training:	Epoch: [36][805/817]	Loss 0.0081 (0.0310)	
training:	Epoch: [36][806/817]	Loss 0.0085 (0.0310)	
training:	Epoch: [36][807/817]	Loss 0.0042 (0.0309)	
training:	Epoch: [36][808/817]	Loss 0.0044 (0.0309)	
training:	Epoch: [36][809/817]	Loss 0.0091 (0.0309)	
training:	Epoch: [36][810/817]	Loss 0.0047 (0.0308)	
training:	Epoch: [36][811/817]	Loss 0.0061 (0.0308)	
training:	Epoch: [36][812/817]	Loss 0.0066 (0.0308)	
training:	Epoch: [36][813/817]	Loss 0.0101 (0.0308)	
training:	Epoch: [36][814/817]	Loss 0.0047 (0.0307)	
training:	Epoch: [36][815/817]	Loss 0.0077 (0.0307)	
training:	Epoch: [36][816/817]	Loss 0.0126 (0.0307)	
training:	Epoch: [36][817/817]	Loss 0.0051 (0.0306)	
Training:	 Loss: 0.0306

Training:	 ACC: 0.9945 0.9945 0.9953 0.9936
Validation:	 ACC: 0.7799 0.7812 0.8076 0.7522
Validation:	 Best_BACC: 0.8223 0.8218 0.8127 0.8318
Validation:	 Loss: 1.0189
Pretraining:	Epoch 37/200
----------
training:	Epoch: [37][1/817]	Loss 0.0073 (0.0073)	
training:	Epoch: [37][2/817]	Loss 0.0051 (0.0062)	
training:	Epoch: [37][3/817]	Loss 0.0099 (0.0074)	
training:	Epoch: [37][4/817]	Loss 0.0076 (0.0075)	
training:	Epoch: [37][5/817]	Loss 0.0082 (0.0076)	
training:	Epoch: [37][6/817]	Loss 0.0075 (0.0076)	
training:	Epoch: [37][7/817]	Loss 0.0109 (0.0081)	
training:	Epoch: [37][8/817]	Loss 0.0282 (0.0106)	
training:	Epoch: [37][9/817]	Loss 0.0059 (0.0101)	
training:	Epoch: [37][10/817]	Loss 0.0049 (0.0095)	
training:	Epoch: [37][11/817]	Loss 0.0057 (0.0092)	
training:	Epoch: [37][12/817]	Loss 0.0070 (0.0090)	
training:	Epoch: [37][13/817]	Loss 0.0139 (0.0094)	
training:	Epoch: [37][14/817]	Loss 0.0054 (0.0091)	
training:	Epoch: [37][15/817]	Loss 0.0045 (0.0088)	
training:	Epoch: [37][16/817]	Loss 0.0090 (0.0088)	
training:	Epoch: [37][17/817]	Loss 0.0063 (0.0087)	
training:	Epoch: [37][18/817]	Loss 0.0040 (0.0084)	
training:	Epoch: [37][19/817]	Loss 0.0064 (0.0083)	
training:	Epoch: [37][20/817]	Loss 0.0068 (0.0082)	
training:	Epoch: [37][21/817]	Loss 0.0064 (0.0081)	
training:	Epoch: [37][22/817]	Loss 0.0070 (0.0081)	
training:	Epoch: [37][23/817]	Loss 0.0072 (0.0080)	
training:	Epoch: [37][24/817]	Loss 0.0057 (0.0079)	
training:	Epoch: [37][25/817]	Loss 0.0033 (0.0078)	
training:	Epoch: [37][26/817]	Loss 0.0057 (0.0077)	
training:	Epoch: [37][27/817]	Loss 0.0047 (0.0076)	
training:	Epoch: [37][28/817]	Loss 0.0068 (0.0075)	
training:	Epoch: [37][29/817]	Loss 0.0063 (0.0075)	
training:	Epoch: [37][30/817]	Loss 0.0040 (0.0074)	
training:	Epoch: [37][31/817]	Loss 0.0056 (0.0073)	
training:	Epoch: [37][32/817]	Loss 0.0039 (0.0072)	
training:	Epoch: [37][33/817]	Loss 0.0056 (0.0072)	
training:	Epoch: [37][34/817]	Loss 0.0068 (0.0072)	
training:	Epoch: [37][35/817]	Loss 0.0077 (0.0072)	
training:	Epoch: [37][36/817]	Loss 0.0079 (0.0072)	
training:	Epoch: [37][37/817]	Loss 0.0035 (0.0071)	
training:	Epoch: [37][38/817]	Loss 0.0085 (0.0071)	
training:	Epoch: [37][39/817]	Loss 0.0063 (0.0071)	
training:	Epoch: [37][40/817]	Loss 0.0081 (0.0071)	
training:	Epoch: [37][41/817]	Loss 0.0048 (0.0071)	
training:	Epoch: [37][42/817]	Loss 0.6614 (0.0227)	
training:	Epoch: [37][43/817]	Loss 0.0053 (0.0223)	
training:	Epoch: [37][44/817]	Loss 0.0043 (0.0218)	
training:	Epoch: [37][45/817]	Loss 0.0072 (0.0215)	
training:	Epoch: [37][46/817]	Loss 0.0079 (0.0212)	
training:	Epoch: [37][47/817]	Loss 0.0042 (0.0209)	
training:	Epoch: [37][48/817]	Loss 0.0079 (0.0206)	
training:	Epoch: [37][49/817]	Loss 0.0052 (0.0203)	
training:	Epoch: [37][50/817]	Loss 0.0076 (0.0200)	
training:	Epoch: [37][51/817]	Loss 0.0054 (0.0197)	
training:	Epoch: [37][52/817]	Loss 0.0042 (0.0194)	
training:	Epoch: [37][53/817]	Loss 0.0058 (0.0192)	
training:	Epoch: [37][54/817]	Loss 0.0046 (0.0189)	
training:	Epoch: [37][55/817]	Loss 0.0053 (0.0187)	
training:	Epoch: [37][56/817]	Loss 0.0069 (0.0185)	
training:	Epoch: [37][57/817]	Loss 0.0051 (0.0182)	
training:	Epoch: [37][58/817]	Loss 0.0053 (0.0180)	
training:	Epoch: [37][59/817]	Loss 0.0049 (0.0178)	
training:	Epoch: [37][60/817]	Loss 0.0046 (0.0176)	
training:	Epoch: [37][61/817]	Loss 0.0054 (0.0174)	
training:	Epoch: [37][62/817]	Loss 0.0052 (0.0172)	
training:	Epoch: [37][63/817]	Loss 0.0062 (0.0170)	
training:	Epoch: [37][64/817]	Loss 0.0056 (0.0168)	
training:	Epoch: [37][65/817]	Loss 0.0063 (0.0166)	
training:	Epoch: [37][66/817]	Loss 0.0038 (0.0165)	
training:	Epoch: [37][67/817]	Loss 0.0057 (0.0163)	
training:	Epoch: [37][68/817]	Loss 0.0053 (0.0161)	
training:	Epoch: [37][69/817]	Loss 0.0044 (0.0160)	
training:	Epoch: [37][70/817]	Loss 0.0053 (0.0158)	
training:	Epoch: [37][71/817]	Loss 0.0047 (0.0157)	
training:	Epoch: [37][72/817]	Loss 0.0078 (0.0155)	
training:	Epoch: [37][73/817]	Loss 0.0078 (0.0154)	
training:	Epoch: [37][74/817]	Loss 0.0045 (0.0153)	
training:	Epoch: [37][75/817]	Loss 0.0058 (0.0152)	
training:	Epoch: [37][76/817]	Loss 0.0068 (0.0151)	
training:	Epoch: [37][77/817]	Loss 0.0037 (0.0149)	
training:	Epoch: [37][78/817]	Loss 0.0046 (0.0148)	
training:	Epoch: [37][79/817]	Loss 0.0053 (0.0147)	
training:	Epoch: [37][80/817]	Loss 0.0063 (0.0145)	
training:	Epoch: [37][81/817]	Loss 0.6244 (0.0221)	
training:	Epoch: [37][82/817]	Loss 0.0060 (0.0219)	
training:	Epoch: [37][83/817]	Loss 0.0041 (0.0217)	
training:	Epoch: [37][84/817]	Loss 0.4438 (0.0267)	
training:	Epoch: [37][85/817]	Loss 0.0083 (0.0265)	
training:	Epoch: [37][86/817]	Loss 0.0091 (0.0263)	
training:	Epoch: [37][87/817]	Loss 0.0035 (0.0260)	
training:	Epoch: [37][88/817]	Loss 0.0061 (0.0258)	
training:	Epoch: [37][89/817]	Loss 0.0058 (0.0256)	
training:	Epoch: [37][90/817]	Loss 0.0071 (0.0254)	
training:	Epoch: [37][91/817]	Loss 0.0044 (0.0251)	
training:	Epoch: [37][92/817]	Loss 0.0052 (0.0249)	
training:	Epoch: [37][93/817]	Loss 0.0046 (0.0247)	
training:	Epoch: [37][94/817]	Loss 0.0052 (0.0245)	
training:	Epoch: [37][95/817]	Loss 0.0083 (0.0243)	
training:	Epoch: [37][96/817]	Loss 0.0043 (0.0241)	
training:	Epoch: [37][97/817]	Loss 0.0116 (0.0240)	
training:	Epoch: [37][98/817]	Loss 0.0053 (0.0238)	
training:	Epoch: [37][99/817]	Loss 0.0058 (0.0236)	
training:	Epoch: [37][100/817]	Loss 0.0094 (0.0235)	
training:	Epoch: [37][101/817]	Loss 0.0055 (0.0233)	
training:	Epoch: [37][102/817]	Loss 0.0073 (0.0231)	
training:	Epoch: [37][103/817]	Loss 0.0041 (0.0229)	
training:	Epoch: [37][104/817]	Loss 0.0053 (0.0228)	
training:	Epoch: [37][105/817]	Loss 0.0053 (0.0226)	
training:	Epoch: [37][106/817]	Loss 0.0061 (0.0225)	
training:	Epoch: [37][107/817]	Loss 0.0049 (0.0223)	
training:	Epoch: [37][108/817]	Loss 0.0061 (0.0221)	
training:	Epoch: [37][109/817]	Loss 0.0066 (0.0220)	
training:	Epoch: [37][110/817]	Loss 0.0064 (0.0219)	
training:	Epoch: [37][111/817]	Loss 0.0053 (0.0217)	
training:	Epoch: [37][112/817]	Loss 0.0053 (0.0216)	
training:	Epoch: [37][113/817]	Loss 0.0082 (0.0214)	
training:	Epoch: [37][114/817]	Loss 0.0055 (0.0213)	
training:	Epoch: [37][115/817]	Loss 0.0035 (0.0211)	
training:	Epoch: [37][116/817]	Loss 0.0057 (0.0210)	
training:	Epoch: [37][117/817]	Loss 0.0053 (0.0209)	
training:	Epoch: [37][118/817]	Loss 0.0059 (0.0207)	
training:	Epoch: [37][119/817]	Loss 0.0042 (0.0206)	
training:	Epoch: [37][120/817]	Loss 0.0090 (0.0205)	
training:	Epoch: [37][121/817]	Loss 0.0046 (0.0204)	
training:	Epoch: [37][122/817]	Loss 0.0046 (0.0203)	
training:	Epoch: [37][123/817]	Loss 0.0061 (0.0201)	
training:	Epoch: [37][124/817]	Loss 0.0054 (0.0200)	
training:	Epoch: [37][125/817]	Loss 0.0058 (0.0199)	
training:	Epoch: [37][126/817]	Loss 0.0035 (0.0198)	
training:	Epoch: [37][127/817]	Loss 0.0074 (0.0197)	
training:	Epoch: [37][128/817]	Loss 0.0039 (0.0196)	
training:	Epoch: [37][129/817]	Loss 0.0055 (0.0194)	
training:	Epoch: [37][130/817]	Loss 0.0056 (0.0193)	
training:	Epoch: [37][131/817]	Loss 0.0056 (0.0192)	
training:	Epoch: [37][132/817]	Loss 0.0055 (0.0191)	
training:	Epoch: [37][133/817]	Loss 0.0037 (0.0190)	
training:	Epoch: [37][134/817]	Loss 0.0063 (0.0189)	
training:	Epoch: [37][135/817]	Loss 0.0068 (0.0188)	
training:	Epoch: [37][136/817]	Loss 0.3836 (0.0215)	
training:	Epoch: [37][137/817]	Loss 0.0062 (0.0214)	
training:	Epoch: [37][138/817]	Loss 0.0048 (0.0213)	
training:	Epoch: [37][139/817]	Loss 0.0071 (0.0212)	
training:	Epoch: [37][140/817]	Loss 0.0038 (0.0211)	
training:	Epoch: [37][141/817]	Loss 0.0042 (0.0209)	
training:	Epoch: [37][142/817]	Loss 0.6248 (0.0252)	
training:	Epoch: [37][143/817]	Loss 0.0063 (0.0251)	
training:	Epoch: [37][144/817]	Loss 0.0097 (0.0249)	
training:	Epoch: [37][145/817]	Loss 0.6327 (0.0291)	
training:	Epoch: [37][146/817]	Loss 0.0040 (0.0290)	
training:	Epoch: [37][147/817]	Loss 0.0160 (0.0289)	
training:	Epoch: [37][148/817]	Loss 0.0073 (0.0287)	
training:	Epoch: [37][149/817]	Loss 0.0068 (0.0286)	
training:	Epoch: [37][150/817]	Loss 0.0054 (0.0284)	
training:	Epoch: [37][151/817]	Loss 0.0063 (0.0283)	
training:	Epoch: [37][152/817]	Loss 0.3678 (0.0305)	
training:	Epoch: [37][153/817]	Loss 0.0088 (0.0304)	
training:	Epoch: [37][154/817]	Loss 0.0084 (0.0302)	
training:	Epoch: [37][155/817]	Loss 0.0041 (0.0301)	
training:	Epoch: [37][156/817]	Loss 0.0061 (0.0299)	
training:	Epoch: [37][157/817]	Loss 0.0068 (0.0298)	
training:	Epoch: [37][158/817]	Loss 0.0050 (0.0296)	
training:	Epoch: [37][159/817]	Loss 0.0065 (0.0295)	
training:	Epoch: [37][160/817]	Loss 0.0073 (0.0293)	
training:	Epoch: [37][161/817]	Loss 0.0077 (0.0292)	
training:	Epoch: [37][162/817]	Loss 0.0050 (0.0290)	
training:	Epoch: [37][163/817]	Loss 0.0042 (0.0289)	
training:	Epoch: [37][164/817]	Loss 0.0114 (0.0288)	
training:	Epoch: [37][165/817]	Loss 0.0119 (0.0287)	
training:	Epoch: [37][166/817]	Loss 0.0129 (0.0286)	
training:	Epoch: [37][167/817]	Loss 0.0135 (0.0285)	
training:	Epoch: [37][168/817]	Loss 0.0062 (0.0284)	
training:	Epoch: [37][169/817]	Loss 0.0105 (0.0283)	
training:	Epoch: [37][170/817]	Loss 0.0104 (0.0281)	
training:	Epoch: [37][171/817]	Loss 0.0198 (0.0281)	
training:	Epoch: [37][172/817]	Loss 0.0071 (0.0280)	
training:	Epoch: [37][173/817]	Loss 0.0056 (0.0278)	
training:	Epoch: [37][174/817]	Loss 0.0039 (0.0277)	
training:	Epoch: [37][175/817]	Loss 0.0055 (0.0276)	
training:	Epoch: [37][176/817]	Loss 0.0039 (0.0275)	
training:	Epoch: [37][177/817]	Loss 0.0045 (0.0273)	
training:	Epoch: [37][178/817]	Loss 0.0040 (0.0272)	
training:	Epoch: [37][179/817]	Loss 0.0055 (0.0271)	
training:	Epoch: [37][180/817]	Loss 0.0041 (0.0269)	
training:	Epoch: [37][181/817]	Loss 0.0064 (0.0268)	
training:	Epoch: [37][182/817]	Loss 0.0096 (0.0267)	
training:	Epoch: [37][183/817]	Loss 0.0096 (0.0266)	
training:	Epoch: [37][184/817]	Loss 0.0043 (0.0265)	
training:	Epoch: [37][185/817]	Loss 0.0048 (0.0264)	
training:	Epoch: [37][186/817]	Loss 0.0059 (0.0263)	
training:	Epoch: [37][187/817]	Loss 0.0048 (0.0262)	
training:	Epoch: [37][188/817]	Loss 0.0096 (0.0261)	
training:	Epoch: [37][189/817]	Loss 0.0051 (0.0260)	
training:	Epoch: [37][190/817]	Loss 0.4344 (0.0281)	
training:	Epoch: [37][191/817]	Loss 0.0077 (0.0280)	
training:	Epoch: [37][192/817]	Loss 0.0057 (0.0279)	
training:	Epoch: [37][193/817]	Loss 0.0042 (0.0278)	
training:	Epoch: [37][194/817]	Loss 0.0077 (0.0277)	
training:	Epoch: [37][195/817]	Loss 0.0063 (0.0276)	
training:	Epoch: [37][196/817]	Loss 0.0055 (0.0275)	
training:	Epoch: [37][197/817]	Loss 0.0067 (0.0273)	
training:	Epoch: [37][198/817]	Loss 0.0114 (0.0273)	
training:	Epoch: [37][199/817]	Loss 0.0062 (0.0272)	
training:	Epoch: [37][200/817]	Loss 0.0065 (0.0271)	
training:	Epoch: [37][201/817]	Loss 0.0136 (0.0270)	
training:	Epoch: [37][202/817]	Loss 0.0069 (0.0269)	
training:	Epoch: [37][203/817]	Loss 0.4655 (0.0291)	
training:	Epoch: [37][204/817]	Loss 0.0065 (0.0289)	
training:	Epoch: [37][205/817]	Loss 0.0203 (0.0289)	
training:	Epoch: [37][206/817]	Loss 0.0049 (0.0288)	
training:	Epoch: [37][207/817]	Loss 0.0049 (0.0287)	
training:	Epoch: [37][208/817]	Loss 0.0055 (0.0286)	
training:	Epoch: [37][209/817]	Loss 0.0064 (0.0285)	
training:	Epoch: [37][210/817]	Loss 0.0055 (0.0283)	
training:	Epoch: [37][211/817]	Loss 0.0112 (0.0283)	
training:	Epoch: [37][212/817]	Loss 0.0057 (0.0282)	
training:	Epoch: [37][213/817]	Loss 0.0114 (0.0281)	
training:	Epoch: [37][214/817]	Loss 0.0042 (0.0280)	
training:	Epoch: [37][215/817]	Loss 0.5838 (0.0305)	
training:	Epoch: [37][216/817]	Loss 0.0049 (0.0304)	
training:	Epoch: [37][217/817]	Loss 0.0054 (0.0303)	
training:	Epoch: [37][218/817]	Loss 0.0057 (0.0302)	
training:	Epoch: [37][219/817]	Loss 0.0046 (0.0301)	
training:	Epoch: [37][220/817]	Loss 0.0069 (0.0300)	
training:	Epoch: [37][221/817]	Loss 0.0040 (0.0299)	
training:	Epoch: [37][222/817]	Loss 0.5070 (0.0320)	
training:	Epoch: [37][223/817]	Loss 0.0053 (0.0319)	
training:	Epoch: [37][224/817]	Loss 0.0057 (0.0318)	
training:	Epoch: [37][225/817]	Loss 0.0066 (0.0317)	
training:	Epoch: [37][226/817]	Loss 0.0051 (0.0315)	
training:	Epoch: [37][227/817]	Loss 0.0079 (0.0314)	
training:	Epoch: [37][228/817]	Loss 0.0111 (0.0314)	
training:	Epoch: [37][229/817]	Loss 0.0106 (0.0313)	
training:	Epoch: [37][230/817]	Loss 0.0051 (0.0311)	
training:	Epoch: [37][231/817]	Loss 0.0078 (0.0310)	
training:	Epoch: [37][232/817]	Loss 0.0083 (0.0309)	
training:	Epoch: [37][233/817]	Loss 0.0042 (0.0308)	
training:	Epoch: [37][234/817]	Loss 0.0054 (0.0307)	
training:	Epoch: [37][235/817]	Loss 0.0054 (0.0306)	
training:	Epoch: [37][236/817]	Loss 0.0090 (0.0305)	
training:	Epoch: [37][237/817]	Loss 0.0114 (0.0304)	
training:	Epoch: [37][238/817]	Loss 0.0103 (0.0304)	
training:	Epoch: [37][239/817]	Loss 0.0070 (0.0303)	
training:	Epoch: [37][240/817]	Loss 0.0105 (0.0302)	
training:	Epoch: [37][241/817]	Loss 0.0059 (0.0301)	
training:	Epoch: [37][242/817]	Loss 0.0055 (0.0300)	
training:	Epoch: [37][243/817]	Loss 0.0051 (0.0299)	
training:	Epoch: [37][244/817]	Loss 0.0046 (0.0298)	
training:	Epoch: [37][245/817]	Loss 0.0054 (0.0297)	
training:	Epoch: [37][246/817]	Loss 0.0060 (0.0296)	
training:	Epoch: [37][247/817]	Loss 0.0032 (0.0295)	
training:	Epoch: [37][248/817]	Loss 0.5830 (0.0317)	
training:	Epoch: [37][249/817]	Loss 0.0058 (0.0316)	
training:	Epoch: [37][250/817]	Loss 0.0056 (0.0315)	
training:	Epoch: [37][251/817]	Loss 0.0122 (0.0314)	
training:	Epoch: [37][252/817]	Loss 0.0289 (0.0314)	
training:	Epoch: [37][253/817]	Loss 0.0076 (0.0313)	
training:	Epoch: [37][254/817]	Loss 0.0072 (0.0312)	
training:	Epoch: [37][255/817]	Loss 0.0066 (0.0311)	
training:	Epoch: [37][256/817]	Loss 0.0085 (0.0310)	
training:	Epoch: [37][257/817]	Loss 0.0073 (0.0309)	
training:	Epoch: [37][258/817]	Loss 0.0074 (0.0308)	
training:	Epoch: [37][259/817]	Loss 0.4880 (0.0326)	
training:	Epoch: [37][260/817]	Loss 0.0059 (0.0325)	
training:	Epoch: [37][261/817]	Loss 0.0074 (0.0324)	
training:	Epoch: [37][262/817]	Loss 0.4791 (0.0341)	
training:	Epoch: [37][263/817]	Loss 0.0059 (0.0340)	
training:	Epoch: [37][264/817]	Loss 0.0069 (0.0339)	
training:	Epoch: [37][265/817]	Loss 0.0060 (0.0338)	
training:	Epoch: [37][266/817]	Loss 0.5386 (0.0357)	
training:	Epoch: [37][267/817]	Loss 0.0051 (0.0356)	
training:	Epoch: [37][268/817]	Loss 0.0059 (0.0355)	
training:	Epoch: [37][269/817]	Loss 0.0111 (0.0354)	
training:	Epoch: [37][270/817]	Loss 0.0078 (0.0353)	
training:	Epoch: [37][271/817]	Loss 0.0082 (0.0352)	
training:	Epoch: [37][272/817]	Loss 0.0062 (0.0351)	
training:	Epoch: [37][273/817]	Loss 0.0093 (0.0350)	
training:	Epoch: [37][274/817]	Loss 0.5714 (0.0369)	
training:	Epoch: [37][275/817]	Loss 0.0105 (0.0368)	
training:	Epoch: [37][276/817]	Loss 0.0142 (0.0368)	
training:	Epoch: [37][277/817]	Loss 0.0082 (0.0367)	
training:	Epoch: [37][278/817]	Loss 0.0185 (0.0366)	
training:	Epoch: [37][279/817]	Loss 0.0117 (0.0365)	
training:	Epoch: [37][280/817]	Loss 0.0190 (0.0364)	
training:	Epoch: [37][281/817]	Loss 0.0062 (0.0363)	
training:	Epoch: [37][282/817]	Loss 0.0108 (0.0362)	
training:	Epoch: [37][283/817]	Loss 0.0092 (0.0361)	
training:	Epoch: [37][284/817]	Loss 0.5403 (0.0379)	
training:	Epoch: [37][285/817]	Loss 0.0070 (0.0378)	
training:	Epoch: [37][286/817]	Loss 0.0100 (0.0377)	
training:	Epoch: [37][287/817]	Loss 0.0098 (0.0376)	
training:	Epoch: [37][288/817]	Loss 0.0087 (0.0375)	
training:	Epoch: [37][289/817]	Loss 0.0067 (0.0374)	
training:	Epoch: [37][290/817]	Loss 0.0061 (0.0373)	
training:	Epoch: [37][291/817]	Loss 0.0094 (0.0372)	
training:	Epoch: [37][292/817]	Loss 0.0069 (0.0371)	
training:	Epoch: [37][293/817]	Loss 0.0066 (0.0370)	
training:	Epoch: [37][294/817]	Loss 0.0080 (0.0369)	
training:	Epoch: [37][295/817]	Loss 0.2573 (0.0377)	
training:	Epoch: [37][296/817]	Loss 0.0090 (0.0376)	
training:	Epoch: [37][297/817]	Loss 0.0058 (0.0374)	
training:	Epoch: [37][298/817]	Loss 0.0075 (0.0373)	
training:	Epoch: [37][299/817]	Loss 0.0068 (0.0372)	
training:	Epoch: [37][300/817]	Loss 0.0102 (0.0372)	
training:	Epoch: [37][301/817]	Loss 0.0083 (0.0371)	
training:	Epoch: [37][302/817]	Loss 0.0077 (0.0370)	
training:	Epoch: [37][303/817]	Loss 0.0146 (0.0369)	
training:	Epoch: [37][304/817]	Loss 0.0191 (0.0368)	
training:	Epoch: [37][305/817]	Loss 0.0103 (0.0367)	
training:	Epoch: [37][306/817]	Loss 0.0145 (0.0367)	
training:	Epoch: [37][307/817]	Loss 0.0185 (0.0366)	
training:	Epoch: [37][308/817]	Loss 0.0124 (0.0365)	
training:	Epoch: [37][309/817]	Loss 0.0133 (0.0365)	
training:	Epoch: [37][310/817]	Loss 0.0070 (0.0364)	
training:	Epoch: [37][311/817]	Loss 0.0052 (0.0363)	
training:	Epoch: [37][312/817]	Loss 0.0052 (0.0362)	
training:	Epoch: [37][313/817]	Loss 0.0159 (0.0361)	
training:	Epoch: [37][314/817]	Loss 0.0130 (0.0360)	
training:	Epoch: [37][315/817]	Loss 0.0106 (0.0359)	
training:	Epoch: [37][316/817]	Loss 0.0051 (0.0358)	
training:	Epoch: [37][317/817]	Loss 0.0047 (0.0357)	
training:	Epoch: [37][318/817]	Loss 0.0066 (0.0357)	
training:	Epoch: [37][319/817]	Loss 0.0063 (0.0356)	
training:	Epoch: [37][320/817]	Loss 0.4307 (0.0368)	
training:	Epoch: [37][321/817]	Loss 0.0094 (0.0367)	
training:	Epoch: [37][322/817]	Loss 0.0116 (0.0366)	
training:	Epoch: [37][323/817]	Loss 0.0080 (0.0365)	
training:	Epoch: [37][324/817]	Loss 0.0138 (0.0365)	
training:	Epoch: [37][325/817]	Loss 0.0075 (0.0364)	
training:	Epoch: [37][326/817]	Loss 0.0153 (0.0363)	
training:	Epoch: [37][327/817]	Loss 0.0148 (0.0363)	
training:	Epoch: [37][328/817]	Loss 0.0057 (0.0362)	
training:	Epoch: [37][329/817]	Loss 0.0069 (0.0361)	
training:	Epoch: [37][330/817]	Loss 0.0064 (0.0360)	
training:	Epoch: [37][331/817]	Loss 0.0038 (0.0359)	
training:	Epoch: [37][332/817]	Loss 0.0097 (0.0358)	
training:	Epoch: [37][333/817]	Loss 0.0060 (0.0357)	
training:	Epoch: [37][334/817]	Loss 0.0067 (0.0356)	
training:	Epoch: [37][335/817]	Loss 0.0093 (0.0356)	
training:	Epoch: [37][336/817]	Loss 0.0052 (0.0355)	
training:	Epoch: [37][337/817]	Loss 0.0069 (0.0354)	
training:	Epoch: [37][338/817]	Loss 0.0112 (0.0353)	
training:	Epoch: [37][339/817]	Loss 0.0097 (0.0352)	
training:	Epoch: [37][340/817]	Loss 0.0088 (0.0352)	
training:	Epoch: [37][341/817]	Loss 0.0094 (0.0351)	
training:	Epoch: [37][342/817]	Loss 0.0056 (0.0350)	
training:	Epoch: [37][343/817]	Loss 0.0066 (0.0349)	
training:	Epoch: [37][344/817]	Loss 0.0137 (0.0348)	
training:	Epoch: [37][345/817]	Loss 0.0074 (0.0348)	
training:	Epoch: [37][346/817]	Loss 0.0047 (0.0347)	
training:	Epoch: [37][347/817]	Loss 0.0039 (0.0346)	
training:	Epoch: [37][348/817]	Loss 0.0081 (0.0345)	
training:	Epoch: [37][349/817]	Loss 0.0042 (0.0344)	
training:	Epoch: [37][350/817]	Loss 0.0055 (0.0343)	
training:	Epoch: [37][351/817]	Loss 0.0064 (0.0343)	
training:	Epoch: [37][352/817]	Loss 0.0048 (0.0342)	
training:	Epoch: [37][353/817]	Loss 0.0078 (0.0341)	
training:	Epoch: [37][354/817]	Loss 0.0045 (0.0340)	
training:	Epoch: [37][355/817]	Loss 0.0064 (0.0339)	
training:	Epoch: [37][356/817]	Loss 0.0062 (0.0339)	
training:	Epoch: [37][357/817]	Loss 0.0088 (0.0338)	
training:	Epoch: [37][358/817]	Loss 0.0044 (0.0337)	
training:	Epoch: [37][359/817]	Loss 0.0113 (0.0337)	
training:	Epoch: [37][360/817]	Loss 0.0080 (0.0336)	
training:	Epoch: [37][361/817]	Loss 0.0086 (0.0335)	
training:	Epoch: [37][362/817]	Loss 0.0066 (0.0334)	
training:	Epoch: [37][363/817]	Loss 0.0065 (0.0334)	
training:	Epoch: [37][364/817]	Loss 0.0076 (0.0333)	
training:	Epoch: [37][365/817]	Loss 0.0051 (0.0332)	
training:	Epoch: [37][366/817]	Loss 0.0053 (0.0331)	
training:	Epoch: [37][367/817]	Loss 0.0064 (0.0331)	
training:	Epoch: [37][368/817]	Loss 0.0048 (0.0330)	
training:	Epoch: [37][369/817]	Loss 0.0051 (0.0329)	
training:	Epoch: [37][370/817]	Loss 0.0053 (0.0328)	
training:	Epoch: [37][371/817]	Loss 0.0039 (0.0328)	
training:	Epoch: [37][372/817]	Loss 0.0059 (0.0327)	
training:	Epoch: [37][373/817]	Loss 0.0066 (0.0326)	
training:	Epoch: [37][374/817]	Loss 0.0058 (0.0325)	
training:	Epoch: [37][375/817]	Loss 0.0064 (0.0325)	
training:	Epoch: [37][376/817]	Loss 0.0116 (0.0324)	
training:	Epoch: [37][377/817]	Loss 0.0051 (0.0324)	
training:	Epoch: [37][378/817]	Loss 0.0055 (0.0323)	
training:	Epoch: [37][379/817]	Loss 0.0063 (0.0322)	
training:	Epoch: [37][380/817]	Loss 0.0079 (0.0321)	
training:	Epoch: [37][381/817]	Loss 0.6158 (0.0337)	
training:	Epoch: [37][382/817]	Loss 0.0058 (0.0336)	
training:	Epoch: [37][383/817]	Loss 0.0071 (0.0335)	
training:	Epoch: [37][384/817]	Loss 0.0068 (0.0335)	
training:	Epoch: [37][385/817]	Loss 0.0071 (0.0334)	
training:	Epoch: [37][386/817]	Loss 0.0053 (0.0333)	
training:	Epoch: [37][387/817]	Loss 0.0058 (0.0333)	
training:	Epoch: [37][388/817]	Loss 0.0074 (0.0332)	
training:	Epoch: [37][389/817]	Loss 0.0053 (0.0331)	
training:	Epoch: [37][390/817]	Loss 0.0086 (0.0331)	
training:	Epoch: [37][391/817]	Loss 0.0060 (0.0330)	
training:	Epoch: [37][392/817]	Loss 0.0071 (0.0329)	
training:	Epoch: [37][393/817]	Loss 0.0080 (0.0329)	
training:	Epoch: [37][394/817]	Loss 0.0104 (0.0328)	
training:	Epoch: [37][395/817]	Loss 0.0071 (0.0327)	
training:	Epoch: [37][396/817]	Loss 0.0067 (0.0327)	
training:	Epoch: [37][397/817]	Loss 0.0076 (0.0326)	
training:	Epoch: [37][398/817]	Loss 0.0057 (0.0325)	
training:	Epoch: [37][399/817]	Loss 0.0063 (0.0325)	
training:	Epoch: [37][400/817]	Loss 0.0093 (0.0324)	
training:	Epoch: [37][401/817]	Loss 0.0042 (0.0323)	
training:	Epoch: [37][402/817]	Loss 0.0084 (0.0323)	
training:	Epoch: [37][403/817]	Loss 0.0070 (0.0322)	
training:	Epoch: [37][404/817]	Loss 0.0088 (0.0322)	
training:	Epoch: [37][405/817]	Loss 0.0078 (0.0321)	
training:	Epoch: [37][406/817]	Loss 0.0059 (0.0320)	
training:	Epoch: [37][407/817]	Loss 0.0139 (0.0320)	
training:	Epoch: [37][408/817]	Loss 0.0041 (0.0319)	
training:	Epoch: [37][409/817]	Loss 0.0052 (0.0319)	
training:	Epoch: [37][410/817]	Loss 0.0074 (0.0318)	
training:	Epoch: [37][411/817]	Loss 0.0052 (0.0317)	
training:	Epoch: [37][412/817]	Loss 0.0063 (0.0317)	
training:	Epoch: [37][413/817]	Loss 0.0044 (0.0316)	
training:	Epoch: [37][414/817]	Loss 0.0064 (0.0315)	
training:	Epoch: [37][415/817]	Loss 0.0070 (0.0315)	
training:	Epoch: [37][416/817]	Loss 0.0049 (0.0314)	
training:	Epoch: [37][417/817]	Loss 0.0071 (0.0314)	
training:	Epoch: [37][418/817]	Loss 0.0047 (0.0313)	
training:	Epoch: [37][419/817]	Loss 0.0057 (0.0312)	
training:	Epoch: [37][420/817]	Loss 0.0057 (0.0312)	
training:	Epoch: [37][421/817]	Loss 0.4881 (0.0323)	
training:	Epoch: [37][422/817]	Loss 0.0047 (0.0322)	
training:	Epoch: [37][423/817]	Loss 0.0048 (0.0321)	
training:	Epoch: [37][424/817]	Loss 0.0046 (0.0321)	
training:	Epoch: [37][425/817]	Loss 0.0046 (0.0320)	
training:	Epoch: [37][426/817]	Loss 0.0044 (0.0319)	
training:	Epoch: [37][427/817]	Loss 0.0067 (0.0319)	
training:	Epoch: [37][428/817]	Loss 0.0091 (0.0318)	
training:	Epoch: [37][429/817]	Loss 0.0077 (0.0318)	
training:	Epoch: [37][430/817]	Loss 0.0047 (0.0317)	
training:	Epoch: [37][431/817]	Loss 0.0061 (0.0317)	
training:	Epoch: [37][432/817]	Loss 0.0066 (0.0316)	
training:	Epoch: [37][433/817]	Loss 0.0047 (0.0315)	
training:	Epoch: [37][434/817]	Loss 0.0041 (0.0315)	
training:	Epoch: [37][435/817]	Loss 0.0038 (0.0314)	
training:	Epoch: [37][436/817]	Loss 0.0090 (0.0314)	
training:	Epoch: [37][437/817]	Loss 0.0067 (0.0313)	
training:	Epoch: [37][438/817]	Loss 0.0121 (0.0313)	
training:	Epoch: [37][439/817]	Loss 0.0073 (0.0312)	
training:	Epoch: [37][440/817]	Loss 0.0055 (0.0311)	
training:	Epoch: [37][441/817]	Loss 0.0061 (0.0311)	
training:	Epoch: [37][442/817]	Loss 0.0069 (0.0310)	
training:	Epoch: [37][443/817]	Loss 0.0082 (0.0310)	
training:	Epoch: [37][444/817]	Loss 0.0058 (0.0309)	
training:	Epoch: [37][445/817]	Loss 0.0083 (0.0309)	
training:	Epoch: [37][446/817]	Loss 0.0037 (0.0308)	
training:	Epoch: [37][447/817]	Loss 0.0094 (0.0308)	
training:	Epoch: [37][448/817]	Loss 0.0081 (0.0307)	
training:	Epoch: [37][449/817]	Loss 0.0046 (0.0307)	
training:	Epoch: [37][450/817]	Loss 0.0066 (0.0306)	
training:	Epoch: [37][451/817]	Loss 0.0064 (0.0305)	
training:	Epoch: [37][452/817]	Loss 0.0080 (0.0305)	
training:	Epoch: [37][453/817]	Loss 0.0055 (0.0304)	
training:	Epoch: [37][454/817]	Loss 0.0072 (0.0304)	
training:	Epoch: [37][455/817]	Loss 0.0053 (0.0303)	
training:	Epoch: [37][456/817]	Loss 0.0042 (0.0303)	
training:	Epoch: [37][457/817]	Loss 0.0049 (0.0302)	
training:	Epoch: [37][458/817]	Loss 0.0061 (0.0302)	
training:	Epoch: [37][459/817]	Loss 0.0082 (0.0301)	
training:	Epoch: [37][460/817]	Loss 0.0053 (0.0301)	
training:	Epoch: [37][461/817]	Loss 0.0054 (0.0300)	
training:	Epoch: [37][462/817]	Loss 0.0074 (0.0300)	
training:	Epoch: [37][463/817]	Loss 0.0099 (0.0299)	
training:	Epoch: [37][464/817]	Loss 0.0045 (0.0299)	
training:	Epoch: [37][465/817]	Loss 0.0070 (0.0298)	
training:	Epoch: [37][466/817]	Loss 0.0069 (0.0298)	
training:	Epoch: [37][467/817]	Loss 0.0069 (0.0297)	
training:	Epoch: [37][468/817]	Loss 0.5472 (0.0308)	
training:	Epoch: [37][469/817]	Loss 0.0063 (0.0308)	
training:	Epoch: [37][470/817]	Loss 0.0061 (0.0307)	
training:	Epoch: [37][471/817]	Loss 0.0053 (0.0307)	
training:	Epoch: [37][472/817]	Loss 0.0046 (0.0306)	
training:	Epoch: [37][473/817]	Loss 0.0078 (0.0306)	
training:	Epoch: [37][474/817]	Loss 0.0047 (0.0305)	
training:	Epoch: [37][475/817]	Loss 0.0087 (0.0305)	
training:	Epoch: [37][476/817]	Loss 0.0033 (0.0304)	
training:	Epoch: [37][477/817]	Loss 0.0096 (0.0304)	
training:	Epoch: [37][478/817]	Loss 0.0088 (0.0303)	
training:	Epoch: [37][479/817]	Loss 0.0063 (0.0303)	
training:	Epoch: [37][480/817]	Loss 0.0062 (0.0302)	
training:	Epoch: [37][481/817]	Loss 0.0039 (0.0302)	
training:	Epoch: [37][482/817]	Loss 0.0039 (0.0301)	
training:	Epoch: [37][483/817]	Loss 0.0057 (0.0301)	
training:	Epoch: [37][484/817]	Loss 0.0068 (0.0300)	
training:	Epoch: [37][485/817]	Loss 0.0046 (0.0300)	
training:	Epoch: [37][486/817]	Loss 0.0067 (0.0299)	
training:	Epoch: [37][487/817]	Loss 0.0061 (0.0299)	
training:	Epoch: [37][488/817]	Loss 0.0060 (0.0298)	
training:	Epoch: [37][489/817]	Loss 0.0049 (0.0298)	
training:	Epoch: [37][490/817]	Loss 0.0122 (0.0297)	
training:	Epoch: [37][491/817]	Loss 0.0066 (0.0297)	
training:	Epoch: [37][492/817]	Loss 0.0045 (0.0296)	
training:	Epoch: [37][493/817]	Loss 0.0061 (0.0296)	
training:	Epoch: [37][494/817]	Loss 0.0076 (0.0295)	
training:	Epoch: [37][495/817]	Loss 0.0075 (0.0295)	
training:	Epoch: [37][496/817]	Loss 0.0065 (0.0294)	
training:	Epoch: [37][497/817]	Loss 0.0047 (0.0294)	
training:	Epoch: [37][498/817]	Loss 0.0051 (0.0293)	
training:	Epoch: [37][499/817]	Loss 0.0079 (0.0293)	
training:	Epoch: [37][500/817]	Loss 0.0047 (0.0292)	
training:	Epoch: [37][501/817]	Loss 0.0040 (0.0292)	
training:	Epoch: [37][502/817]	Loss 0.0044 (0.0291)	
training:	Epoch: [37][503/817]	Loss 0.0072 (0.0291)	
training:	Epoch: [37][504/817]	Loss 0.0054 (0.0291)	
training:	Epoch: [37][505/817]	Loss 0.0042 (0.0290)	
training:	Epoch: [37][506/817]	Loss 0.0053 (0.0290)	
training:	Epoch: [37][507/817]	Loss 0.0046 (0.0289)	
training:	Epoch: [37][508/817]	Loss 0.0032 (0.0289)	
training:	Epoch: [37][509/817]	Loss 0.0064 (0.0288)	
training:	Epoch: [37][510/817]	Loss 0.0060 (0.0288)	
training:	Epoch: [37][511/817]	Loss 0.0048 (0.0287)	
training:	Epoch: [37][512/817]	Loss 0.0051 (0.0287)	
training:	Epoch: [37][513/817]	Loss 0.0051 (0.0286)	
training:	Epoch: [37][514/817]	Loss 0.0080 (0.0286)	
training:	Epoch: [37][515/817]	Loss 0.0040 (0.0285)	
training:	Epoch: [37][516/817]	Loss 0.0069 (0.0285)	
training:	Epoch: [37][517/817]	Loss 0.0053 (0.0285)	
training:	Epoch: [37][518/817]	Loss 0.0066 (0.0284)	
training:	Epoch: [37][519/817]	Loss 0.0097 (0.0284)	
training:	Epoch: [37][520/817]	Loss 0.0052 (0.0283)	
training:	Epoch: [37][521/817]	Loss 0.0038 (0.0283)	
training:	Epoch: [37][522/817]	Loss 0.0035 (0.0282)	
training:	Epoch: [37][523/817]	Loss 0.0045 (0.0282)	
training:	Epoch: [37][524/817]	Loss 0.0033 (0.0282)	
training:	Epoch: [37][525/817]	Loss 0.0062 (0.0281)	
training:	Epoch: [37][526/817]	Loss 0.0067 (0.0281)	
training:	Epoch: [37][527/817]	Loss 0.0056 (0.0280)	
training:	Epoch: [37][528/817]	Loss 0.0048 (0.0280)	
training:	Epoch: [37][529/817]	Loss 0.0035 (0.0279)	
training:	Epoch: [37][530/817]	Loss 0.0073 (0.0279)	
training:	Epoch: [37][531/817]	Loss 0.0037 (0.0279)	
training:	Epoch: [37][532/817]	Loss 0.0038 (0.0278)	
training:	Epoch: [37][533/817]	Loss 0.0051 (0.0278)	
training:	Epoch: [37][534/817]	Loss 0.0043 (0.0277)	
training:	Epoch: [37][535/817]	Loss 0.0057 (0.0277)	
training:	Epoch: [37][536/817]	Loss 0.0053 (0.0276)	
training:	Epoch: [37][537/817]	Loss 0.0072 (0.0276)	
training:	Epoch: [37][538/817]	Loss 0.0046 (0.0276)	
training:	Epoch: [37][539/817]	Loss 0.0048 (0.0275)	
training:	Epoch: [37][540/817]	Loss 0.0060 (0.0275)	
training:	Epoch: [37][541/817]	Loss 0.0047 (0.0274)	
training:	Epoch: [37][542/817]	Loss 0.0056 (0.0274)	
training:	Epoch: [37][543/817]	Loss 0.0041 (0.0273)	
training:	Epoch: [37][544/817]	Loss 0.0040 (0.0273)	
training:	Epoch: [37][545/817]	Loss 0.0088 (0.0273)	
training:	Epoch: [37][546/817]	Loss 0.0050 (0.0272)	
training:	Epoch: [37][547/817]	Loss 0.0046 (0.0272)	
training:	Epoch: [37][548/817]	Loss 0.0058 (0.0272)	
training:	Epoch: [37][549/817]	Loss 0.0043 (0.0271)	
training:	Epoch: [37][550/817]	Loss 0.0049 (0.0271)	
training:	Epoch: [37][551/817]	Loss 0.0034 (0.0270)	
training:	Epoch: [37][552/817]	Loss 0.0046 (0.0270)	
training:	Epoch: [37][553/817]	Loss 0.0053 (0.0269)	
training:	Epoch: [37][554/817]	Loss 0.0080 (0.0269)	
training:	Epoch: [37][555/817]	Loss 0.0049 (0.0269)	
training:	Epoch: [37][556/817]	Loss 0.0043 (0.0268)	
training:	Epoch: [37][557/817]	Loss 0.0051 (0.0268)	
training:	Epoch: [37][558/817]	Loss 0.0040 (0.0268)	
training:	Epoch: [37][559/817]	Loss 0.0033 (0.0267)	
training:	Epoch: [37][560/817]	Loss 0.0043 (0.0267)	
training:	Epoch: [37][561/817]	Loss 0.0047 (0.0266)	
training:	Epoch: [37][562/817]	Loss 0.0042 (0.0266)	
training:	Epoch: [37][563/817]	Loss 0.0045 (0.0266)	
training:	Epoch: [37][564/817]	Loss 0.0072 (0.0265)	
training:	Epoch: [37][565/817]	Loss 0.0039 (0.0265)	
training:	Epoch: [37][566/817]	Loss 0.0038 (0.0264)	
training:	Epoch: [37][567/817]	Loss 0.0070 (0.0264)	
training:	Epoch: [37][568/817]	Loss 0.0042 (0.0264)	
training:	Epoch: [37][569/817]	Loss 0.0035 (0.0263)	
training:	Epoch: [37][570/817]	Loss 0.0038 (0.0263)	
training:	Epoch: [37][571/817]	Loss 0.0045 (0.0262)	
training:	Epoch: [37][572/817]	Loss 0.0048 (0.0262)	
training:	Epoch: [37][573/817]	Loss 0.0044 (0.0262)	
training:	Epoch: [37][574/817]	Loss 0.0059 (0.0261)	
training:	Epoch: [37][575/817]	Loss 0.4653 (0.0269)	
training:	Epoch: [37][576/817]	Loss 0.0035 (0.0269)	
training:	Epoch: [37][577/817]	Loss 0.0058 (0.0268)	
training:	Epoch: [37][578/817]	Loss 0.0048 (0.0268)	
training:	Epoch: [37][579/817]	Loss 0.0060 (0.0267)	
training:	Epoch: [37][580/817]	Loss 0.0058 (0.0267)	
training:	Epoch: [37][581/817]	Loss 0.0047 (0.0267)	
training:	Epoch: [37][582/817]	Loss 0.0043 (0.0266)	
training:	Epoch: [37][583/817]	Loss 0.4857 (0.0274)	
training:	Epoch: [37][584/817]	Loss 0.0055 (0.0274)	
training:	Epoch: [37][585/817]	Loss 0.0037 (0.0273)	
training:	Epoch: [37][586/817]	Loss 0.0041 (0.0273)	
training:	Epoch: [37][587/817]	Loss 0.0042 (0.0273)	
training:	Epoch: [37][588/817]	Loss 0.0051 (0.0272)	
training:	Epoch: [37][589/817]	Loss 0.0059 (0.0272)	
training:	Epoch: [37][590/817]	Loss 0.0055 (0.0272)	
training:	Epoch: [37][591/817]	Loss 0.0046 (0.0271)	
training:	Epoch: [37][592/817]	Loss 0.3976 (0.0277)	
training:	Epoch: [37][593/817]	Loss 0.0029 (0.0277)	
training:	Epoch: [37][594/817]	Loss 0.0048 (0.0277)	
training:	Epoch: [37][595/817]	Loss 0.0048 (0.0276)	
training:	Epoch: [37][596/817]	Loss 0.0044 (0.0276)	
training:	Epoch: [37][597/817]	Loss 0.0045 (0.0275)	
training:	Epoch: [37][598/817]	Loss 0.0050 (0.0275)	
training:	Epoch: [37][599/817]	Loss 0.0037 (0.0275)	
training:	Epoch: [37][600/817]	Loss 0.0056 (0.0274)	
training:	Epoch: [37][601/817]	Loss 0.0036 (0.0274)	
training:	Epoch: [37][602/817]	Loss 0.0040 (0.0274)	
training:	Epoch: [37][603/817]	Loss 0.0058 (0.0273)	
training:	Epoch: [37][604/817]	Loss 0.0052 (0.0273)	
training:	Epoch: [37][605/817]	Loss 0.0056 (0.0272)	
training:	Epoch: [37][606/817]	Loss 0.0075 (0.0272)	
training:	Epoch: [37][607/817]	Loss 0.0038 (0.0272)	
training:	Epoch: [37][608/817]	Loss 0.0053 (0.0271)	
training:	Epoch: [37][609/817]	Loss 0.0058 (0.0271)	
training:	Epoch: [37][610/817]	Loss 0.0087 (0.0271)	
training:	Epoch: [37][611/817]	Loss 0.0058 (0.0270)	
training:	Epoch: [37][612/817]	Loss 0.0052 (0.0270)	
training:	Epoch: [37][613/817]	Loss 0.0043 (0.0270)	
training:	Epoch: [37][614/817]	Loss 0.0047 (0.0269)	
training:	Epoch: [37][615/817]	Loss 0.0051 (0.0269)	
training:	Epoch: [37][616/817]	Loss 0.0056 (0.0269)	
training:	Epoch: [37][617/817]	Loss 0.0081 (0.0268)	
training:	Epoch: [37][618/817]	Loss 0.0064 (0.0268)	
training:	Epoch: [37][619/817]	Loss 0.0045 (0.0268)	
training:	Epoch: [37][620/817]	Loss 0.0041 (0.0267)	
training:	Epoch: [37][621/817]	Loss 0.0071 (0.0267)	
training:	Epoch: [37][622/817]	Loss 0.0060 (0.0267)	
training:	Epoch: [37][623/817]	Loss 0.0056 (0.0266)	
training:	Epoch: [37][624/817]	Loss 0.0043 (0.0266)	
training:	Epoch: [37][625/817]	Loss 0.0037 (0.0266)	
training:	Epoch: [37][626/817]	Loss 0.0037 (0.0265)	
training:	Epoch: [37][627/817]	Loss 0.4504 (0.0272)	
training:	Epoch: [37][628/817]	Loss 0.0046 (0.0272)	
training:	Epoch: [37][629/817]	Loss 0.0071 (0.0271)	
training:	Epoch: [37][630/817]	Loss 0.0080 (0.0271)	
training:	Epoch: [37][631/817]	Loss 0.0037 (0.0271)	
training:	Epoch: [37][632/817]	Loss 0.0074 (0.0270)	
training:	Epoch: [37][633/817]	Loss 0.0040 (0.0270)	
training:	Epoch: [37][634/817]	Loss 0.0039 (0.0270)	
training:	Epoch: [37][635/817]	Loss 0.0104 (0.0269)	
training:	Epoch: [37][636/817]	Loss 0.0060 (0.0269)	
training:	Epoch: [37][637/817]	Loss 0.0035 (0.0269)	
training:	Epoch: [37][638/817]	Loss 0.0060 (0.0268)	
training:	Epoch: [37][639/817]	Loss 0.0069 (0.0268)	
training:	Epoch: [37][640/817]	Loss 0.0063 (0.0268)	
training:	Epoch: [37][641/817]	Loss 0.0037 (0.0267)	
training:	Epoch: [37][642/817]	Loss 0.0050 (0.0267)	
training:	Epoch: [37][643/817]	Loss 0.0041 (0.0267)	
training:	Epoch: [37][644/817]	Loss 0.0058 (0.0266)	
training:	Epoch: [37][645/817]	Loss 0.0041 (0.0266)	
training:	Epoch: [37][646/817]	Loss 0.0051 (0.0266)	
training:	Epoch: [37][647/817]	Loss 0.0062 (0.0265)	
training:	Epoch: [37][648/817]	Loss 0.0089 (0.0265)	
training:	Epoch: [37][649/817]	Loss 0.0042 (0.0265)	
training:	Epoch: [37][650/817]	Loss 0.0046 (0.0264)	
training:	Epoch: [37][651/817]	Loss 0.5389 (0.0272)	
training:	Epoch: [37][652/817]	Loss 0.0083 (0.0272)	
training:	Epoch: [37][653/817]	Loss 0.5834 (0.0280)	
training:	Epoch: [37][654/817]	Loss 0.0050 (0.0280)	
training:	Epoch: [37][655/817]	Loss 0.0065 (0.0280)	
training:	Epoch: [37][656/817]	Loss 0.0059 (0.0279)	
training:	Epoch: [37][657/817]	Loss 0.0046 (0.0279)	
training:	Epoch: [37][658/817]	Loss 0.0052 (0.0279)	
training:	Epoch: [37][659/817]	Loss 0.0067 (0.0278)	
training:	Epoch: [37][660/817]	Loss 0.0042 (0.0278)	
training:	Epoch: [37][661/817]	Loss 0.0057 (0.0278)	
training:	Epoch: [37][662/817]	Loss 0.0066 (0.0277)	
training:	Epoch: [37][663/817]	Loss 0.0060 (0.0277)	
training:	Epoch: [37][664/817]	Loss 0.0086 (0.0277)	
training:	Epoch: [37][665/817]	Loss 0.0044 (0.0276)	
training:	Epoch: [37][666/817]	Loss 0.0049 (0.0276)	
training:	Epoch: [37][667/817]	Loss 0.0168 (0.0276)	
training:	Epoch: [37][668/817]	Loss 0.5430 (0.0284)	
training:	Epoch: [37][669/817]	Loss 0.0043 (0.0283)	
training:	Epoch: [37][670/817]	Loss 0.0065 (0.0283)	
training:	Epoch: [37][671/817]	Loss 0.0062 (0.0283)	
training:	Epoch: [37][672/817]	Loss 0.0065 (0.0282)	
training:	Epoch: [37][673/817]	Loss 0.0071 (0.0282)	
training:	Epoch: [37][674/817]	Loss 0.0035 (0.0282)	
training:	Epoch: [37][675/817]	Loss 0.0081 (0.0281)	
training:	Epoch: [37][676/817]	Loss 0.0049 (0.0281)	
training:	Epoch: [37][677/817]	Loss 0.0069 (0.0281)	
training:	Epoch: [37][678/817]	Loss 0.0075 (0.0280)	
training:	Epoch: [37][679/817]	Loss 0.0046 (0.0280)	
training:	Epoch: [37][680/817]	Loss 0.0061 (0.0280)	
training:	Epoch: [37][681/817]	Loss 0.0113 (0.0279)	
training:	Epoch: [37][682/817]	Loss 0.0095 (0.0279)	
training:	Epoch: [37][683/817]	Loss 0.0043 (0.0279)	
training:	Epoch: [37][684/817]	Loss 0.0072 (0.0278)	
training:	Epoch: [37][685/817]	Loss 0.0062 (0.0278)	
training:	Epoch: [37][686/817]	Loss 0.0091 (0.0278)	
training:	Epoch: [37][687/817]	Loss 0.0104 (0.0278)	
training:	Epoch: [37][688/817]	Loss 0.0064 (0.0277)	
training:	Epoch: [37][689/817]	Loss 0.0051 (0.0277)	
training:	Epoch: [37][690/817]	Loss 0.0061 (0.0277)	
training:	Epoch: [37][691/817]	Loss 0.4301 (0.0283)	
training:	Epoch: [37][692/817]	Loss 0.0049 (0.0282)	
training:	Epoch: [37][693/817]	Loss 0.0067 (0.0282)	
training:	Epoch: [37][694/817]	Loss 0.0066 (0.0282)	
training:	Epoch: [37][695/817]	Loss 0.0043 (0.0281)	
training:	Epoch: [37][696/817]	Loss 0.0040 (0.0281)	
training:	Epoch: [37][697/817]	Loss 0.0053 (0.0281)	
training:	Epoch: [37][698/817]	Loss 0.0084 (0.0280)	
training:	Epoch: [37][699/817]	Loss 0.0062 (0.0280)	
training:	Epoch: [37][700/817]	Loss 0.0053 (0.0280)	
training:	Epoch: [37][701/817]	Loss 0.0053 (0.0279)	
training:	Epoch: [37][702/817]	Loss 0.0068 (0.0279)	
training:	Epoch: [37][703/817]	Loss 0.0067 (0.0279)	
training:	Epoch: [37][704/817]	Loss 0.0078 (0.0278)	
training:	Epoch: [37][705/817]	Loss 0.0077 (0.0278)	
training:	Epoch: [37][706/817]	Loss 0.0060 (0.0278)	
training:	Epoch: [37][707/817]	Loss 0.0135 (0.0278)	
training:	Epoch: [37][708/817]	Loss 0.0078 (0.0277)	
training:	Epoch: [37][709/817]	Loss 0.0063 (0.0277)	
training:	Epoch: [37][710/817]	Loss 0.0073 (0.0277)	
training:	Epoch: [37][711/817]	Loss 0.0045 (0.0276)	
training:	Epoch: [37][712/817]	Loss 0.0062 (0.0276)	
training:	Epoch: [37][713/817]	Loss 0.0068 (0.0276)	
training:	Epoch: [37][714/817]	Loss 0.0040 (0.0275)	
training:	Epoch: [37][715/817]	Loss 0.0063 (0.0275)	
training:	Epoch: [37][716/817]	Loss 0.0052 (0.0275)	
training:	Epoch: [37][717/817]	Loss 0.0096 (0.0275)	
training:	Epoch: [37][718/817]	Loss 0.0073 (0.0274)	
training:	Epoch: [37][719/817]	Loss 0.0088 (0.0274)	
training:	Epoch: [37][720/817]	Loss 0.0045 (0.0274)	
training:	Epoch: [37][721/817]	Loss 0.0047 (0.0273)	
training:	Epoch: [37][722/817]	Loss 0.0049 (0.0273)	
training:	Epoch: [37][723/817]	Loss 0.0067 (0.0273)	
training:	Epoch: [37][724/817]	Loss 0.0072 (0.0273)	
training:	Epoch: [37][725/817]	Loss 0.0040 (0.0272)	
training:	Epoch: [37][726/817]	Loss 0.0072 (0.0272)	
training:	Epoch: [37][727/817]	Loss 0.0037 (0.0272)	
training:	Epoch: [37][728/817]	Loss 0.0045 (0.0271)	
training:	Epoch: [37][729/817]	Loss 0.0122 (0.0271)	
training:	Epoch: [37][730/817]	Loss 0.0063 (0.0271)	
training:	Epoch: [37][731/817]	Loss 0.0041 (0.0271)	
training:	Epoch: [37][732/817]	Loss 0.0046 (0.0270)	
training:	Epoch: [37][733/817]	Loss 0.0052 (0.0270)	
training:	Epoch: [37][734/817]	Loss 0.0052 (0.0270)	
training:	Epoch: [37][735/817]	Loss 0.0054 (0.0269)	
training:	Epoch: [37][736/817]	Loss 0.0036 (0.0269)	
training:	Epoch: [37][737/817]	Loss 0.0066 (0.0269)	
training:	Epoch: [37][738/817]	Loss 0.0069 (0.0268)	
training:	Epoch: [37][739/817]	Loss 0.0067 (0.0268)	
training:	Epoch: [37][740/817]	Loss 0.0053 (0.0268)	
training:	Epoch: [37][741/817]	Loss 0.0058 (0.0268)	
training:	Epoch: [37][742/817]	Loss 0.0046 (0.0267)	
training:	Epoch: [37][743/817]	Loss 0.0041 (0.0267)	
training:	Epoch: [37][744/817]	Loss 0.0060 (0.0267)	
training:	Epoch: [37][745/817]	Loss 0.0050 (0.0266)	
training:	Epoch: [37][746/817]	Loss 0.0085 (0.0266)	
training:	Epoch: [37][747/817]	Loss 0.0074 (0.0266)	
training:	Epoch: [37][748/817]	Loss 0.0042 (0.0266)	
training:	Epoch: [37][749/817]	Loss 0.0049 (0.0265)	
training:	Epoch: [37][750/817]	Loss 0.6962 (0.0274)	
training:	Epoch: [37][751/817]	Loss 0.0047 (0.0274)	
training:	Epoch: [37][752/817]	Loss 0.0070 (0.0274)	
training:	Epoch: [37][753/817]	Loss 0.0054 (0.0273)	
training:	Epoch: [37][754/817]	Loss 0.0064 (0.0273)	
training:	Epoch: [37][755/817]	Loss 0.0038 (0.0273)	
training:	Epoch: [37][756/817]	Loss 0.0196 (0.0273)	
training:	Epoch: [37][757/817]	Loss 0.4972 (0.0279)	
training:	Epoch: [37][758/817]	Loss 0.0064 (0.0279)	
training:	Epoch: [37][759/817]	Loss 0.0057 (0.0278)	
training:	Epoch: [37][760/817]	Loss 0.0063 (0.0278)	
training:	Epoch: [37][761/817]	Loss 0.0066 (0.0278)	
training:	Epoch: [37][762/817]	Loss 0.0062 (0.0278)	
training:	Epoch: [37][763/817]	Loss 0.0050 (0.0277)	
training:	Epoch: [37][764/817]	Loss 0.0057 (0.0277)	
training:	Epoch: [37][765/817]	Loss 0.0044 (0.0277)	
training:	Epoch: [37][766/817]	Loss 0.0044 (0.0276)	
training:	Epoch: [37][767/817]	Loss 0.0044 (0.0276)	
training:	Epoch: [37][768/817]	Loss 0.0091 (0.0276)	
training:	Epoch: [37][769/817]	Loss 0.4419 (0.0281)	
training:	Epoch: [37][770/817]	Loss 0.0051 (0.0281)	
training:	Epoch: [37][771/817]	Loss 0.0118 (0.0281)	
training:	Epoch: [37][772/817]	Loss 0.6040 (0.0288)	
training:	Epoch: [37][773/817]	Loss 0.0060 (0.0288)	
training:	Epoch: [37][774/817]	Loss 0.0077 (0.0288)	
training:	Epoch: [37][775/817]	Loss 0.0035 (0.0287)	
training:	Epoch: [37][776/817]	Loss 0.0099 (0.0287)	
training:	Epoch: [37][777/817]	Loss 0.5414 (0.0294)	
training:	Epoch: [37][778/817]	Loss 0.0106 (0.0293)	
training:	Epoch: [37][779/817]	Loss 0.0098 (0.0293)	
training:	Epoch: [37][780/817]	Loss 0.0153 (0.0293)	
training:	Epoch: [37][781/817]	Loss 0.5461 (0.0300)	
training:	Epoch: [37][782/817]	Loss 0.0070 (0.0299)	
training:	Epoch: [37][783/817]	Loss 0.0060 (0.0299)	
training:	Epoch: [37][784/817]	Loss 0.0071 (0.0299)	
training:	Epoch: [37][785/817]	Loss 0.0063 (0.0298)	
training:	Epoch: [37][786/817]	Loss 0.0045 (0.0298)	
training:	Epoch: [37][787/817]	Loss 0.0063 (0.0298)	
training:	Epoch: [37][788/817]	Loss 0.0048 (0.0297)	
training:	Epoch: [37][789/817]	Loss 0.0067 (0.0297)	
training:	Epoch: [37][790/817]	Loss 0.0088 (0.0297)	
training:	Epoch: [37][791/817]	Loss 0.0036 (0.0297)	
training:	Epoch: [37][792/817]	Loss 0.0098 (0.0296)	
training:	Epoch: [37][793/817]	Loss 0.0052 (0.0296)	
training:	Epoch: [37][794/817]	Loss 0.0068 (0.0296)	
training:	Epoch: [37][795/817]	Loss 0.0061 (0.0295)	
training:	Epoch: [37][796/817]	Loss 0.0080 (0.0295)	
training:	Epoch: [37][797/817]	Loss 0.0064 (0.0295)	
training:	Epoch: [37][798/817]	Loss 0.0092 (0.0295)	
training:	Epoch: [37][799/817]	Loss 0.0075 (0.0294)	
training:	Epoch: [37][800/817]	Loss 0.0059 (0.0294)	
training:	Epoch: [37][801/817]	Loss 0.0079 (0.0294)	
training:	Epoch: [37][802/817]	Loss 0.0091 (0.0293)	
training:	Epoch: [37][803/817]	Loss 0.0080 (0.0293)	
training:	Epoch: [37][804/817]	Loss 0.0072 (0.0293)	
training:	Epoch: [37][805/817]	Loss 0.0083 (0.0293)	
training:	Epoch: [37][806/817]	Loss 0.0057 (0.0292)	
training:	Epoch: [37][807/817]	Loss 0.0100 (0.0292)	
training:	Epoch: [37][808/817]	Loss 0.0064 (0.0292)	
training:	Epoch: [37][809/817]	Loss 0.0047 (0.0292)	
training:	Epoch: [37][810/817]	Loss 0.0081 (0.0291)	
training:	Epoch: [37][811/817]	Loss 0.0092 (0.0291)	
training:	Epoch: [37][812/817]	Loss 0.0118 (0.0291)	
training:	Epoch: [37][813/817]	Loss 0.0055 (0.0291)	
training:	Epoch: [37][814/817]	Loss 0.0064 (0.0290)	
training:	Epoch: [37][815/817]	Loss 0.0077 (0.0290)	
training:	Epoch: [37][816/817]	Loss 0.0143 (0.0290)	
training:	Epoch: [37][817/817]	Loss 0.0053 (0.0290)	
Training:	 Loss: 0.0289

Training:	 ACC: 0.9945 0.9945 0.9953 0.9936
Validation:	 ACC: 0.7815 0.7828 0.8086 0.7545
Validation:	 Best_BACC: 0.8223 0.8218 0.8127 0.8318
Validation:	 Loss: 1.0106
Pretraining:	Epoch 38/200
----------
training:	Epoch: [38][1/817]	Loss 0.0053 (0.0053)	
training:	Epoch: [38][2/817]	Loss 0.0072 (0.0062)	
training:	Epoch: [38][3/817]	Loss 0.0057 (0.0061)	
training:	Epoch: [38][4/817]	Loss 0.0060 (0.0060)	
training:	Epoch: [38][5/817]	Loss 0.0068 (0.0062)	
training:	Epoch: [38][6/817]	Loss 0.0049 (0.0060)	
training:	Epoch: [38][7/817]	Loss 0.0036 (0.0057)	
training:	Epoch: [38][8/817]	Loss 0.0084 (0.0060)	
training:	Epoch: [38][9/817]	Loss 0.0067 (0.0061)	
training:	Epoch: [38][10/817]	Loss 0.0075 (0.0062)	
training:	Epoch: [38][11/817]	Loss 0.0216 (0.0076)	
training:	Epoch: [38][12/817]	Loss 0.0249 (0.0091)	
training:	Epoch: [38][13/817]	Loss 0.0057 (0.0088)	
training:	Epoch: [38][14/817]	Loss 0.0079 (0.0087)	
training:	Epoch: [38][15/817]	Loss 0.0078 (0.0087)	
training:	Epoch: [38][16/817]	Loss 0.0059 (0.0085)	
training:	Epoch: [38][17/817]	Loss 0.0067 (0.0084)	
training:	Epoch: [38][18/817]	Loss 0.0040 (0.0081)	
training:	Epoch: [38][19/817]	Loss 0.0037 (0.0079)	
training:	Epoch: [38][20/817]	Loss 0.0064 (0.0078)	
training:	Epoch: [38][21/817]	Loss 0.4205 (0.0275)	
training:	Epoch: [38][22/817]	Loss 0.0079 (0.0266)	
training:	Epoch: [38][23/817]	Loss 0.0072 (0.0258)	
training:	Epoch: [38][24/817]	Loss 0.0074 (0.0250)	
training:	Epoch: [38][25/817]	Loss 0.5345 (0.0454)	
training:	Epoch: [38][26/817]	Loss 0.0060 (0.0439)	
training:	Epoch: [38][27/817]	Loss 0.0047 (0.0424)	
training:	Epoch: [38][28/817]	Loss 0.0063 (0.0411)	
training:	Epoch: [38][29/817]	Loss 0.0064 (0.0399)	
training:	Epoch: [38][30/817]	Loss 0.0100 (0.0389)	
training:	Epoch: [38][31/817]	Loss 0.0080 (0.0379)	
training:	Epoch: [38][32/817]	Loss 0.0051 (0.0369)	
training:	Epoch: [38][33/817]	Loss 0.0076 (0.0360)	
training:	Epoch: [38][34/817]	Loss 0.0041 (0.0351)	
training:	Epoch: [38][35/817]	Loss 0.0054 (0.0342)	
training:	Epoch: [38][36/817]	Loss 0.0083 (0.0335)	
training:	Epoch: [38][37/817]	Loss 0.0064 (0.0328)	
training:	Epoch: [38][38/817]	Loss 0.0064 (0.0321)	
training:	Epoch: [38][39/817]	Loss 0.0116 (0.0315)	
training:	Epoch: [38][40/817]	Loss 0.0059 (0.0309)	
training:	Epoch: [38][41/817]	Loss 0.0041 (0.0303)	
training:	Epoch: [38][42/817]	Loss 0.0070 (0.0297)	
training:	Epoch: [38][43/817]	Loss 0.0078 (0.0292)	
training:	Epoch: [38][44/817]	Loss 0.0127 (0.0288)	
training:	Epoch: [38][45/817]	Loss 0.0087 (0.0284)	
training:	Epoch: [38][46/817]	Loss 0.0039 (0.0278)	
training:	Epoch: [38][47/817]	Loss 0.0086 (0.0274)	
training:	Epoch: [38][48/817]	Loss 0.0150 (0.0272)	
training:	Epoch: [38][49/817]	Loss 0.0103 (0.0268)	
training:	Epoch: [38][50/817]	Loss 0.0057 (0.0264)	
training:	Epoch: [38][51/817]	Loss 0.0054 (0.0260)	
training:	Epoch: [38][52/817]	Loss 0.0059 (0.0256)	
training:	Epoch: [38][53/817]	Loss 0.0076 (0.0253)	
training:	Epoch: [38][54/817]	Loss 0.0096 (0.0250)	
training:	Epoch: [38][55/817]	Loss 0.0105 (0.0247)	
training:	Epoch: [38][56/817]	Loss 0.0095 (0.0244)	
training:	Epoch: [38][57/817]	Loss 0.0061 (0.0241)	
training:	Epoch: [38][58/817]	Loss 0.0051 (0.0238)	
training:	Epoch: [38][59/817]	Loss 0.0086 (0.0235)	
training:	Epoch: [38][60/817]	Loss 0.0104 (0.0233)	
training:	Epoch: [38][61/817]	Loss 0.0060 (0.0230)	
training:	Epoch: [38][62/817]	Loss 0.0052 (0.0227)	
training:	Epoch: [38][63/817]	Loss 0.0036 (0.0224)	
training:	Epoch: [38][64/817]	Loss 0.0033 (0.0221)	
training:	Epoch: [38][65/817]	Loss 0.0108 (0.0220)	
training:	Epoch: [38][66/817]	Loss 0.0041 (0.0217)	
training:	Epoch: [38][67/817]	Loss 0.6518 (0.0311)	
training:	Epoch: [38][68/817]	Loss 0.0105 (0.0308)	
training:	Epoch: [38][69/817]	Loss 0.0097 (0.0305)	
training:	Epoch: [38][70/817]	Loss 0.0108 (0.0302)	
training:	Epoch: [38][71/817]	Loss 0.0071 (0.0299)	
training:	Epoch: [38][72/817]	Loss 0.0073 (0.0296)	
training:	Epoch: [38][73/817]	Loss 0.0071 (0.0293)	
training:	Epoch: [38][74/817]	Loss 0.0067 (0.0290)	
training:	Epoch: [38][75/817]	Loss 0.0033 (0.0286)	
training:	Epoch: [38][76/817]	Loss 0.0067 (0.0283)	
training:	Epoch: [38][77/817]	Loss 0.0040 (0.0280)	
training:	Epoch: [38][78/817]	Loss 0.0053 (0.0277)	
training:	Epoch: [38][79/817]	Loss 0.0098 (0.0275)	
training:	Epoch: [38][80/817]	Loss 0.0042 (0.0272)	
training:	Epoch: [38][81/817]	Loss 0.0038 (0.0269)	
training:	Epoch: [38][82/817]	Loss 0.0065 (0.0267)	
training:	Epoch: [38][83/817]	Loss 0.0045 (0.0264)	
training:	Epoch: [38][84/817]	Loss 0.0059 (0.0261)	
training:	Epoch: [38][85/817]	Loss 0.0120 (0.0260)	
training:	Epoch: [38][86/817]	Loss 0.0062 (0.0258)	
training:	Epoch: [38][87/817]	Loss 0.0067 (0.0255)	
training:	Epoch: [38][88/817]	Loss 0.0085 (0.0253)	
training:	Epoch: [38][89/817]	Loss 0.0088 (0.0252)	
training:	Epoch: [38][90/817]	Loss 0.0052 (0.0249)	
training:	Epoch: [38][91/817]	Loss 0.0096 (0.0248)	
training:	Epoch: [38][92/817]	Loss 0.0045 (0.0245)	
training:	Epoch: [38][93/817]	Loss 0.0054 (0.0243)	
training:	Epoch: [38][94/817]	Loss 0.0045 (0.0241)	
training:	Epoch: [38][95/817]	Loss 0.0054 (0.0239)	
training:	Epoch: [38][96/817]	Loss 0.0055 (0.0237)	
training:	Epoch: [38][97/817]	Loss 0.0047 (0.0235)	
training:	Epoch: [38][98/817]	Loss 0.0056 (0.0234)	
training:	Epoch: [38][99/817]	Loss 0.0046 (0.0232)	
training:	Epoch: [38][100/817]	Loss 0.0073 (0.0230)	
training:	Epoch: [38][101/817]	Loss 0.0043 (0.0228)	
training:	Epoch: [38][102/817]	Loss 0.0068 (0.0227)	
training:	Epoch: [38][103/817]	Loss 0.0081 (0.0225)	
training:	Epoch: [38][104/817]	Loss 0.0062 (0.0224)	
training:	Epoch: [38][105/817]	Loss 0.0045 (0.0222)	
training:	Epoch: [38][106/817]	Loss 0.0096 (0.0221)	
training:	Epoch: [38][107/817]	Loss 0.0072 (0.0219)	
training:	Epoch: [38][108/817]	Loss 0.0084 (0.0218)	
training:	Epoch: [38][109/817]	Loss 0.0065 (0.0217)	
training:	Epoch: [38][110/817]	Loss 0.5819 (0.0268)	
training:	Epoch: [38][111/817]	Loss 0.0075 (0.0266)	
training:	Epoch: [38][112/817]	Loss 0.0061 (0.0264)	
training:	Epoch: [38][113/817]	Loss 0.0093 (0.0263)	
training:	Epoch: [38][114/817]	Loss 0.0065 (0.0261)	
training:	Epoch: [38][115/817]	Loss 0.0062 (0.0259)	
training:	Epoch: [38][116/817]	Loss 0.0068 (0.0257)	
training:	Epoch: [38][117/817]	Loss 0.0092 (0.0256)	
training:	Epoch: [38][118/817]	Loss 0.0063 (0.0254)	
training:	Epoch: [38][119/817]	Loss 0.0049 (0.0253)	
training:	Epoch: [38][120/817]	Loss 0.5636 (0.0298)	
training:	Epoch: [38][121/817]	Loss 0.0061 (0.0296)	
training:	Epoch: [38][122/817]	Loss 0.0043 (0.0294)	
training:	Epoch: [38][123/817]	Loss 0.0085 (0.0292)	
training:	Epoch: [38][124/817]	Loss 0.0074 (0.0290)	
training:	Epoch: [38][125/817]	Loss 0.0038 (0.0288)	
training:	Epoch: [38][126/817]	Loss 0.0087 (0.0286)	
training:	Epoch: [38][127/817]	Loss 0.0040 (0.0285)	
training:	Epoch: [38][128/817]	Loss 0.0053 (0.0283)	
training:	Epoch: [38][129/817]	Loss 0.0061 (0.0281)	
training:	Epoch: [38][130/817]	Loss 0.0070 (0.0279)	
training:	Epoch: [38][131/817]	Loss 0.0087 (0.0278)	
training:	Epoch: [38][132/817]	Loss 0.0036 (0.0276)	
training:	Epoch: [38][133/817]	Loss 0.0040 (0.0274)	
training:	Epoch: [38][134/817]	Loss 0.0046 (0.0273)	
training:	Epoch: [38][135/817]	Loss 0.4646 (0.0305)	
training:	Epoch: [38][136/817]	Loss 0.0058 (0.0303)	
training:	Epoch: [38][137/817]	Loss 0.0056 (0.0301)	
training:	Epoch: [38][138/817]	Loss 0.0078 (0.0300)	
training:	Epoch: [38][139/817]	Loss 0.0046 (0.0298)	
training:	Epoch: [38][140/817]	Loss 0.0089 (0.0296)	
training:	Epoch: [38][141/817]	Loss 0.0081 (0.0295)	
training:	Epoch: [38][142/817]	Loss 0.0047 (0.0293)	
training:	Epoch: [38][143/817]	Loss 0.0071 (0.0292)	
training:	Epoch: [38][144/817]	Loss 0.0048 (0.0290)	
training:	Epoch: [38][145/817]	Loss 0.0041 (0.0288)	
training:	Epoch: [38][146/817]	Loss 0.0048 (0.0287)	
training:	Epoch: [38][147/817]	Loss 0.0038 (0.0285)	
training:	Epoch: [38][148/817]	Loss 0.0053 (0.0283)	
training:	Epoch: [38][149/817]	Loss 0.0051 (0.0282)	
training:	Epoch: [38][150/817]	Loss 0.0067 (0.0280)	
training:	Epoch: [38][151/817]	Loss 0.0059 (0.0279)	
training:	Epoch: [38][152/817]	Loss 0.0062 (0.0277)	
training:	Epoch: [38][153/817]	Loss 0.0054 (0.0276)	
training:	Epoch: [38][154/817]	Loss 0.0057 (0.0275)	
training:	Epoch: [38][155/817]	Loss 0.0078 (0.0273)	
training:	Epoch: [38][156/817]	Loss 0.0050 (0.0272)	
training:	Epoch: [38][157/817]	Loss 0.0072 (0.0271)	
training:	Epoch: [38][158/817]	Loss 0.0055 (0.0269)	
training:	Epoch: [38][159/817]	Loss 0.0066 (0.0268)	
training:	Epoch: [38][160/817]	Loss 0.0085 (0.0267)	
training:	Epoch: [38][161/817]	Loss 0.0041 (0.0265)	
training:	Epoch: [38][162/817]	Loss 0.0126 (0.0265)	
training:	Epoch: [38][163/817]	Loss 0.0076 (0.0263)	
training:	Epoch: [38][164/817]	Loss 0.0053 (0.0262)	
training:	Epoch: [38][165/817]	Loss 0.0072 (0.0261)	
training:	Epoch: [38][166/817]	Loss 0.0090 (0.0260)	
training:	Epoch: [38][167/817]	Loss 0.0042 (0.0259)	
training:	Epoch: [38][168/817]	Loss 0.0042 (0.0257)	
training:	Epoch: [38][169/817]	Loss 0.0050 (0.0256)	
training:	Epoch: [38][170/817]	Loss 0.0044 (0.0255)	
training:	Epoch: [38][171/817]	Loss 0.0056 (0.0254)	
training:	Epoch: [38][172/817]	Loss 0.0064 (0.0253)	
training:	Epoch: [38][173/817]	Loss 0.0102 (0.0252)	
training:	Epoch: [38][174/817]	Loss 0.0058 (0.0251)	
training:	Epoch: [38][175/817]	Loss 0.0045 (0.0249)	
training:	Epoch: [38][176/817]	Loss 0.0032 (0.0248)	
training:	Epoch: [38][177/817]	Loss 0.0064 (0.0247)	
training:	Epoch: [38][178/817]	Loss 0.0073 (0.0246)	
training:	Epoch: [38][179/817]	Loss 0.0064 (0.0245)	
training:	Epoch: [38][180/817]	Loss 0.0039 (0.0244)	
training:	Epoch: [38][181/817]	Loss 0.5805 (0.0275)	
training:	Epoch: [38][182/817]	Loss 0.0061 (0.0274)	
training:	Epoch: [38][183/817]	Loss 0.0071 (0.0272)	
training:	Epoch: [38][184/817]	Loss 0.0062 (0.0271)	
training:	Epoch: [38][185/817]	Loss 0.0033 (0.0270)	
training:	Epoch: [38][186/817]	Loss 0.0101 (0.0269)	
training:	Epoch: [38][187/817]	Loss 0.0072 (0.0268)	
training:	Epoch: [38][188/817]	Loss 0.0084 (0.0267)	
training:	Epoch: [38][189/817]	Loss 0.0047 (0.0266)	
training:	Epoch: [38][190/817]	Loss 0.0043 (0.0265)	
training:	Epoch: [38][191/817]	Loss 0.0053 (0.0264)	
training:	Epoch: [38][192/817]	Loss 0.0051 (0.0262)	
training:	Epoch: [38][193/817]	Loss 0.0043 (0.0261)	
training:	Epoch: [38][194/817]	Loss 0.2757 (0.0274)	
training:	Epoch: [38][195/817]	Loss 0.0033 (0.0273)	
training:	Epoch: [38][196/817]	Loss 0.0053 (0.0272)	
training:	Epoch: [38][197/817]	Loss 0.0053 (0.0271)	
training:	Epoch: [38][198/817]	Loss 0.0067 (0.0270)	
training:	Epoch: [38][199/817]	Loss 0.0060 (0.0269)	
training:	Epoch: [38][200/817]	Loss 0.0032 (0.0267)	
training:	Epoch: [38][201/817]	Loss 0.0141 (0.0267)	
training:	Epoch: [38][202/817]	Loss 0.0057 (0.0266)	
training:	Epoch: [38][203/817]	Loss 0.0051 (0.0265)	
training:	Epoch: [38][204/817]	Loss 0.0041 (0.0264)	
training:	Epoch: [38][205/817]	Loss 0.0069 (0.0263)	
training:	Epoch: [38][206/817]	Loss 0.0089 (0.0262)	
training:	Epoch: [38][207/817]	Loss 0.0072 (0.0261)	
training:	Epoch: [38][208/817]	Loss 0.0035 (0.0260)	
training:	Epoch: [38][209/817]	Loss 0.3849 (0.0277)	
training:	Epoch: [38][210/817]	Loss 0.0041 (0.0276)	
training:	Epoch: [38][211/817]	Loss 0.0047 (0.0275)	
training:	Epoch: [38][212/817]	Loss 0.0075 (0.0274)	
training:	Epoch: [38][213/817]	Loss 0.0097 (0.0273)	
training:	Epoch: [38][214/817]	Loss 0.3379 (0.0288)	
training:	Epoch: [38][215/817]	Loss 0.0071 (0.0287)	
training:	Epoch: [38][216/817]	Loss 0.0050 (0.0285)	
training:	Epoch: [38][217/817]	Loss 0.0068 (0.0284)	
training:	Epoch: [38][218/817]	Loss 0.0047 (0.0283)	
training:	Epoch: [38][219/817]	Loss 0.0154 (0.0283)	
training:	Epoch: [38][220/817]	Loss 0.0074 (0.0282)	
training:	Epoch: [38][221/817]	Loss 0.0092 (0.0281)	
training:	Epoch: [38][222/817]	Loss 0.0071 (0.0280)	
training:	Epoch: [38][223/817]	Loss 0.0043 (0.0279)	
training:	Epoch: [38][224/817]	Loss 0.0063 (0.0278)	
training:	Epoch: [38][225/817]	Loss 0.0063 (0.0277)	
training:	Epoch: [38][226/817]	Loss 0.0108 (0.0276)	
training:	Epoch: [38][227/817]	Loss 0.0029 (0.0275)	
training:	Epoch: [38][228/817]	Loss 0.0058 (0.0274)	
training:	Epoch: [38][229/817]	Loss 0.0072 (0.0273)	
training:	Epoch: [38][230/817]	Loss 0.0040 (0.0272)	
training:	Epoch: [38][231/817]	Loss 0.0043 (0.0271)	
training:	Epoch: [38][232/817]	Loss 1.1510 (0.0320)	
training:	Epoch: [38][233/817]	Loss 0.0067 (0.0319)	
training:	Epoch: [38][234/817]	Loss 0.0106 (0.0318)	
training:	Epoch: [38][235/817]	Loss 0.0116 (0.0317)	
training:	Epoch: [38][236/817]	Loss 0.0044 (0.0316)	
training:	Epoch: [38][237/817]	Loss 0.0064 (0.0315)	
training:	Epoch: [38][238/817]	Loss 0.0045 (0.0314)	
training:	Epoch: [38][239/817]	Loss 0.0042 (0.0312)	
training:	Epoch: [38][240/817]	Loss 0.0060 (0.0311)	
training:	Epoch: [38][241/817]	Loss 0.0055 (0.0310)	
training:	Epoch: [38][242/817]	Loss 0.0109 (0.0310)	
training:	Epoch: [38][243/817]	Loss 0.0047 (0.0308)	
training:	Epoch: [38][244/817]	Loss 0.0089 (0.0308)	
training:	Epoch: [38][245/817]	Loss 0.0058 (0.0307)	
training:	Epoch: [38][246/817]	Loss 0.0057 (0.0306)	
training:	Epoch: [38][247/817]	Loss 0.0077 (0.0305)	
training:	Epoch: [38][248/817]	Loss 0.0061 (0.0304)	
training:	Epoch: [38][249/817]	Loss 0.0061 (0.0303)	
training:	Epoch: [38][250/817]	Loss 0.0082 (0.0302)	
training:	Epoch: [38][251/817]	Loss 0.0071 (0.0301)	
training:	Epoch: [38][252/817]	Loss 0.0066 (0.0300)	
training:	Epoch: [38][253/817]	Loss 0.0078 (0.0299)	
training:	Epoch: [38][254/817]	Loss 0.0039 (0.0298)	
training:	Epoch: [38][255/817]	Loss 0.0054 (0.0297)	
training:	Epoch: [38][256/817]	Loss 0.0083 (0.0296)	
training:	Epoch: [38][257/817]	Loss 0.0072 (0.0295)	
training:	Epoch: [38][258/817]	Loss 0.0051 (0.0294)	
training:	Epoch: [38][259/817]	Loss 0.0084 (0.0294)	
training:	Epoch: [38][260/817]	Loss 0.0064 (0.0293)	
training:	Epoch: [38][261/817]	Loss 0.5414 (0.0312)	
training:	Epoch: [38][262/817]	Loss 0.0066 (0.0311)	
training:	Epoch: [38][263/817]	Loss 0.0077 (0.0310)	
training:	Epoch: [38][264/817]	Loss 0.0068 (0.0310)	
training:	Epoch: [38][265/817]	Loss 0.0074 (0.0309)	
training:	Epoch: [38][266/817]	Loss 0.0107 (0.0308)	
training:	Epoch: [38][267/817]	Loss 0.0040 (0.0307)	
training:	Epoch: [38][268/817]	Loss 0.0040 (0.0306)	
training:	Epoch: [38][269/817]	Loss 0.0111 (0.0305)	
training:	Epoch: [38][270/817]	Loss 0.0040 (0.0304)	
training:	Epoch: [38][271/817]	Loss 0.0092 (0.0303)	
training:	Epoch: [38][272/817]	Loss 0.0038 (0.0302)	
training:	Epoch: [38][273/817]	Loss 0.0080 (0.0302)	
training:	Epoch: [38][274/817]	Loss 0.0056 (0.0301)	
training:	Epoch: [38][275/817]	Loss 0.0056 (0.0300)	
training:	Epoch: [38][276/817]	Loss 0.0102 (0.0299)	
training:	Epoch: [38][277/817]	Loss 0.0055 (0.0298)	
training:	Epoch: [38][278/817]	Loss 0.0041 (0.0297)	
training:	Epoch: [38][279/817]	Loss 0.0078 (0.0297)	
training:	Epoch: [38][280/817]	Loss 0.0075 (0.0296)	
training:	Epoch: [38][281/817]	Loss 0.0044 (0.0295)	
training:	Epoch: [38][282/817]	Loss 0.0042 (0.0294)	
training:	Epoch: [38][283/817]	Loss 0.0075 (0.0293)	
training:	Epoch: [38][284/817]	Loss 0.0094 (0.0292)	
training:	Epoch: [38][285/817]	Loss 0.0073 (0.0292)	
training:	Epoch: [38][286/817]	Loss 0.0050 (0.0291)	
training:	Epoch: [38][287/817]	Loss 0.0096 (0.0290)	
training:	Epoch: [38][288/817]	Loss 0.0058 (0.0289)	
training:	Epoch: [38][289/817]	Loss 0.0059 (0.0289)	
training:	Epoch: [38][290/817]	Loss 0.0192 (0.0288)	
training:	Epoch: [38][291/817]	Loss 0.0068 (0.0287)	
training:	Epoch: [38][292/817]	Loss 0.0049 (0.0287)	
training:	Epoch: [38][293/817]	Loss 0.0052 (0.0286)	
training:	Epoch: [38][294/817]	Loss 0.0074 (0.0285)	
training:	Epoch: [38][295/817]	Loss 0.0061 (0.0284)	
training:	Epoch: [38][296/817]	Loss 0.0061 (0.0284)	
training:	Epoch: [38][297/817]	Loss 0.0067 (0.0283)	
training:	Epoch: [38][298/817]	Loss 0.0075 (0.0282)	
training:	Epoch: [38][299/817]	Loss 0.0108 (0.0282)	
training:	Epoch: [38][300/817]	Loss 0.0132 (0.0281)	
training:	Epoch: [38][301/817]	Loss 0.0054 (0.0280)	
training:	Epoch: [38][302/817]	Loss 0.0090 (0.0280)	
training:	Epoch: [38][303/817]	Loss 0.0040 (0.0279)	
training:	Epoch: [38][304/817]	Loss 0.0089 (0.0278)	
training:	Epoch: [38][305/817]	Loss 0.0036 (0.0278)	
training:	Epoch: [38][306/817]	Loss 0.0043 (0.0277)	
training:	Epoch: [38][307/817]	Loss 0.0063 (0.0276)	
training:	Epoch: [38][308/817]	Loss 0.0045 (0.0275)	
training:	Epoch: [38][309/817]	Loss 0.0057 (0.0275)	
training:	Epoch: [38][310/817]	Loss 0.0065 (0.0274)	
training:	Epoch: [38][311/817]	Loss 0.0057 (0.0273)	
training:	Epoch: [38][312/817]	Loss 0.0044 (0.0273)	
training:	Epoch: [38][313/817]	Loss 0.0057 (0.0272)	
training:	Epoch: [38][314/817]	Loss 0.0073 (0.0271)	
training:	Epoch: [38][315/817]	Loss 0.0051 (0.0270)	
training:	Epoch: [38][316/817]	Loss 0.0064 (0.0270)	
training:	Epoch: [38][317/817]	Loss 0.0067 (0.0269)	
training:	Epoch: [38][318/817]	Loss 0.0103 (0.0269)	
training:	Epoch: [38][319/817]	Loss 0.0060 (0.0268)	
training:	Epoch: [38][320/817]	Loss 0.0032 (0.0267)	
training:	Epoch: [38][321/817]	Loss 0.0054 (0.0267)	
training:	Epoch: [38][322/817]	Loss 0.0077 (0.0266)	
training:	Epoch: [38][323/817]	Loss 0.0033 (0.0265)	
training:	Epoch: [38][324/817]	Loss 0.0063 (0.0265)	
training:	Epoch: [38][325/817]	Loss 0.0089 (0.0264)	
training:	Epoch: [38][326/817]	Loss 0.0035 (0.0263)	
training:	Epoch: [38][327/817]	Loss 0.1418 (0.0267)	
training:	Epoch: [38][328/817]	Loss 0.0043 (0.0266)	
training:	Epoch: [38][329/817]	Loss 0.0048 (0.0266)	
training:	Epoch: [38][330/817]	Loss 0.0082 (0.0265)	
training:	Epoch: [38][331/817]	Loss 0.0037 (0.0264)	
training:	Epoch: [38][332/817]	Loss 0.0031 (0.0264)	
training:	Epoch: [38][333/817]	Loss 0.0034 (0.0263)	
training:	Epoch: [38][334/817]	Loss 0.0051 (0.0262)	
training:	Epoch: [38][335/817]	Loss 0.0080 (0.0262)	
training:	Epoch: [38][336/817]	Loss 0.0061 (0.0261)	
training:	Epoch: [38][337/817]	Loss 0.0105 (0.0261)	
training:	Epoch: [38][338/817]	Loss 0.0097 (0.0260)	
training:	Epoch: [38][339/817]	Loss 0.0290 (0.0260)	
training:	Epoch: [38][340/817]	Loss 0.0098 (0.0260)	
training:	Epoch: [38][341/817]	Loss 0.0102 (0.0259)	
training:	Epoch: [38][342/817]	Loss 0.0046 (0.0259)	
training:	Epoch: [38][343/817]	Loss 0.0110 (0.0258)	
training:	Epoch: [38][344/817]	Loss 0.0046 (0.0258)	
training:	Epoch: [38][345/817]	Loss 0.0073 (0.0257)	
training:	Epoch: [38][346/817]	Loss 0.0034 (0.0257)	
training:	Epoch: [38][347/817]	Loss 0.0053 (0.0256)	
training:	Epoch: [38][348/817]	Loss 0.0073 (0.0255)	
training:	Epoch: [38][349/817]	Loss 0.3400 (0.0264)	
training:	Epoch: [38][350/817]	Loss 0.0067 (0.0264)	
training:	Epoch: [38][351/817]	Loss 0.0044 (0.0263)	
training:	Epoch: [38][352/817]	Loss 0.0286 (0.0263)	
training:	Epoch: [38][353/817]	Loss 0.0103 (0.0263)	
training:	Epoch: [38][354/817]	Loss 0.0031 (0.0262)	
training:	Epoch: [38][355/817]	Loss 0.0041 (0.0262)	
training:	Epoch: [38][356/817]	Loss 0.0065 (0.0261)	
training:	Epoch: [38][357/817]	Loss 0.4241 (0.0272)	
training:	Epoch: [38][358/817]	Loss 0.0068 (0.0272)	
training:	Epoch: [38][359/817]	Loss 0.0051 (0.0271)	
training:	Epoch: [38][360/817]	Loss 0.0039 (0.0270)	
training:	Epoch: [38][361/817]	Loss 0.4875 (0.0283)	
training:	Epoch: [38][362/817]	Loss 0.0065 (0.0283)	
training:	Epoch: [38][363/817]	Loss 0.0053 (0.0282)	
training:	Epoch: [38][364/817]	Loss 0.0058 (0.0281)	
training:	Epoch: [38][365/817]	Loss 0.0037 (0.0281)	
training:	Epoch: [38][366/817]	Loss 0.0039 (0.0280)	
training:	Epoch: [38][367/817]	Loss 0.0060 (0.0279)	
training:	Epoch: [38][368/817]	Loss 0.0028 (0.0279)	
training:	Epoch: [38][369/817]	Loss 0.0166 (0.0278)	
training:	Epoch: [38][370/817]	Loss 0.0092 (0.0278)	
training:	Epoch: [38][371/817]	Loss 0.0071 (0.0277)	
training:	Epoch: [38][372/817]	Loss 0.0039 (0.0277)	
training:	Epoch: [38][373/817]	Loss 0.0055 (0.0276)	
training:	Epoch: [38][374/817]	Loss 0.0047 (0.0275)	
training:	Epoch: [38][375/817]	Loss 0.0125 (0.0275)	
training:	Epoch: [38][376/817]	Loss 0.0063 (0.0274)	
training:	Epoch: [38][377/817]	Loss 0.0038 (0.0274)	
training:	Epoch: [38][378/817]	Loss 0.0121 (0.0273)	
training:	Epoch: [38][379/817]	Loss 0.0098 (0.0273)	
training:	Epoch: [38][380/817]	Loss 0.0046 (0.0272)	
training:	Epoch: [38][381/817]	Loss 0.0033 (0.0272)	
training:	Epoch: [38][382/817]	Loss 0.0095 (0.0271)	
training:	Epoch: [38][383/817]	Loss 0.0090 (0.0271)	
training:	Epoch: [38][384/817]	Loss 0.0038 (0.0270)	
training:	Epoch: [38][385/817]	Loss 0.0058 (0.0270)	
training:	Epoch: [38][386/817]	Loss 0.0055 (0.0269)	
training:	Epoch: [38][387/817]	Loss 0.0055 (0.0269)	
training:	Epoch: [38][388/817]	Loss 0.0059 (0.0268)	
training:	Epoch: [38][389/817]	Loss 0.0077 (0.0268)	
training:	Epoch: [38][390/817]	Loss 0.0046 (0.0267)	
training:	Epoch: [38][391/817]	Loss 0.0073 (0.0266)	
training:	Epoch: [38][392/817]	Loss 0.0058 (0.0266)	
training:	Epoch: [38][393/817]	Loss 0.0053 (0.0265)	
training:	Epoch: [38][394/817]	Loss 0.0074 (0.0265)	
training:	Epoch: [38][395/817]	Loss 0.0037 (0.0264)	
training:	Epoch: [38][396/817]	Loss 0.0042 (0.0264)	
training:	Epoch: [38][397/817]	Loss 0.0052 (0.0263)	
training:	Epoch: [38][398/817]	Loss 0.0089 (0.0263)	
training:	Epoch: [38][399/817]	Loss 0.0044 (0.0262)	
training:	Epoch: [38][400/817]	Loss 0.0076 (0.0262)	
training:	Epoch: [38][401/817]	Loss 0.0045 (0.0261)	
training:	Epoch: [38][402/817]	Loss 0.0076 (0.0261)	
training:	Epoch: [38][403/817]	Loss 0.0051 (0.0260)	
training:	Epoch: [38][404/817]	Loss 0.0089 (0.0260)	
training:	Epoch: [38][405/817]	Loss 0.0055 (0.0259)	
training:	Epoch: [38][406/817]	Loss 0.5203 (0.0272)	
training:	Epoch: [38][407/817]	Loss 0.0040 (0.0271)	
training:	Epoch: [38][408/817]	Loss 0.0071 (0.0270)	
training:	Epoch: [38][409/817]	Loss 0.0045 (0.0270)	
training:	Epoch: [38][410/817]	Loss 0.0131 (0.0270)	
training:	Epoch: [38][411/817]	Loss 0.5358 (0.0282)	
training:	Epoch: [38][412/817]	Loss 0.0058 (0.0281)	
training:	Epoch: [38][413/817]	Loss 0.0056 (0.0281)	
training:	Epoch: [38][414/817]	Loss 0.0060 (0.0280)	
training:	Epoch: [38][415/817]	Loss 0.0074 (0.0280)	
training:	Epoch: [38][416/817]	Loss 0.4774 (0.0291)	
training:	Epoch: [38][417/817]	Loss 0.0084 (0.0290)	
training:	Epoch: [38][418/817]	Loss 0.0038 (0.0290)	
training:	Epoch: [38][419/817]	Loss 0.0073 (0.0289)	
training:	Epoch: [38][420/817]	Loss 0.0058 (0.0288)	
training:	Epoch: [38][421/817]	Loss 0.0080 (0.0288)	
training:	Epoch: [38][422/817]	Loss 0.0123 (0.0288)	
training:	Epoch: [38][423/817]	Loss 0.0063 (0.0287)	
training:	Epoch: [38][424/817]	Loss 0.0095 (0.0287)	
training:	Epoch: [38][425/817]	Loss 0.0050 (0.0286)	
training:	Epoch: [38][426/817]	Loss 0.0068 (0.0286)	
training:	Epoch: [38][427/817]	Loss 0.0083 (0.0285)	
training:	Epoch: [38][428/817]	Loss 0.5678 (0.0298)	
training:	Epoch: [38][429/817]	Loss 0.0068 (0.0297)	
training:	Epoch: [38][430/817]	Loss 0.0034 (0.0296)	
training:	Epoch: [38][431/817]	Loss 0.0049 (0.0296)	
training:	Epoch: [38][432/817]	Loss 0.0098 (0.0295)	
training:	Epoch: [38][433/817]	Loss 0.0096 (0.0295)	
training:	Epoch: [38][434/817]	Loss 0.0068 (0.0294)	
training:	Epoch: [38][435/817]	Loss 0.0056 (0.0294)	
training:	Epoch: [38][436/817]	Loss 0.0080 (0.0293)	
training:	Epoch: [38][437/817]	Loss 0.0053 (0.0293)	
training:	Epoch: [38][438/817]	Loss 0.0120 (0.0292)	
training:	Epoch: [38][439/817]	Loss 0.0050 (0.0292)	
training:	Epoch: [38][440/817]	Loss 0.0082 (0.0291)	
training:	Epoch: [38][441/817]	Loss 0.2599 (0.0297)	
training:	Epoch: [38][442/817]	Loss 0.0047 (0.0296)	
training:	Epoch: [38][443/817]	Loss 0.0131 (0.0296)	
training:	Epoch: [38][444/817]	Loss 0.0052 (0.0295)	
training:	Epoch: [38][445/817]	Loss 0.0329 (0.0295)	
training:	Epoch: [38][446/817]	Loss 0.0322 (0.0295)	
training:	Epoch: [38][447/817]	Loss 0.0065 (0.0295)	
training:	Epoch: [38][448/817]	Loss 0.0082 (0.0294)	
training:	Epoch: [38][449/817]	Loss 0.0047 (0.0294)	
training:	Epoch: [38][450/817]	Loss 0.0053 (0.0293)	
training:	Epoch: [38][451/817]	Loss 0.0168 (0.0293)	
training:	Epoch: [38][452/817]	Loss 0.0057 (0.0292)	
training:	Epoch: [38][453/817]	Loss 0.0048 (0.0292)	
training:	Epoch: [38][454/817]	Loss 0.0058 (0.0291)	
training:	Epoch: [38][455/817]	Loss 0.0050 (0.0291)	
training:	Epoch: [38][456/817]	Loss 0.0057 (0.0290)	
training:	Epoch: [38][457/817]	Loss 0.0065 (0.0290)	
training:	Epoch: [38][458/817]	Loss 0.0059 (0.0289)	
training:	Epoch: [38][459/817]	Loss 0.0131 (0.0289)	
training:	Epoch: [38][460/817]	Loss 0.0071 (0.0289)	
training:	Epoch: [38][461/817]	Loss 0.0062 (0.0288)	
training:	Epoch: [38][462/817]	Loss 0.0098 (0.0288)	
training:	Epoch: [38][463/817]	Loss 0.0063 (0.0287)	
training:	Epoch: [38][464/817]	Loss 0.0070 (0.0287)	
training:	Epoch: [38][465/817]	Loss 0.0055 (0.0286)	
training:	Epoch: [38][466/817]	Loss 0.0086 (0.0286)	
training:	Epoch: [38][467/817]	Loss 0.0036 (0.0285)	
training:	Epoch: [38][468/817]	Loss 0.0099 (0.0285)	
training:	Epoch: [38][469/817]	Loss 0.0035 (0.0284)	
training:	Epoch: [38][470/817]	Loss 0.0036 (0.0284)	
training:	Epoch: [38][471/817]	Loss 0.0042 (0.0283)	
training:	Epoch: [38][472/817]	Loss 0.0074 (0.0283)	
training:	Epoch: [38][473/817]	Loss 0.0077 (0.0282)	
training:	Epoch: [38][474/817]	Loss 0.0056 (0.0282)	
training:	Epoch: [38][475/817]	Loss 0.3460 (0.0289)	
training:	Epoch: [38][476/817]	Loss 0.0069 (0.0288)	
training:	Epoch: [38][477/817]	Loss 0.0068 (0.0288)	
training:	Epoch: [38][478/817]	Loss 0.0058 (0.0287)	
training:	Epoch: [38][479/817]	Loss 0.0108 (0.0287)	
training:	Epoch: [38][480/817]	Loss 0.0058 (0.0286)	
training:	Epoch: [38][481/817]	Loss 0.0069 (0.0286)	
training:	Epoch: [38][482/817]	Loss 0.0061 (0.0285)	
training:	Epoch: [38][483/817]	Loss 0.0066 (0.0285)	
training:	Epoch: [38][484/817]	Loss 0.0054 (0.0284)	
training:	Epoch: [38][485/817]	Loss 0.0048 (0.0284)	
training:	Epoch: [38][486/817]	Loss 0.0036 (0.0283)	
training:	Epoch: [38][487/817]	Loss 0.0126 (0.0283)	
training:	Epoch: [38][488/817]	Loss 0.0082 (0.0283)	
training:	Epoch: [38][489/817]	Loss 0.5207 (0.0293)	
training:	Epoch: [38][490/817]	Loss 0.0036 (0.0292)	
training:	Epoch: [38][491/817]	Loss 0.0099 (0.0292)	
training:	Epoch: [38][492/817]	Loss 0.0124 (0.0292)	
training:	Epoch: [38][493/817]	Loss 0.0055 (0.0291)	
training:	Epoch: [38][494/817]	Loss 0.0071 (0.0291)	
training:	Epoch: [38][495/817]	Loss 0.0073 (0.0290)	
training:	Epoch: [38][496/817]	Loss 0.0073 (0.0290)	
training:	Epoch: [38][497/817]	Loss 0.0041 (0.0289)	
training:	Epoch: [38][498/817]	Loss 0.0089 (0.0289)	
training:	Epoch: [38][499/817]	Loss 0.0057 (0.0288)	
training:	Epoch: [38][500/817]	Loss 0.0054 (0.0288)	
training:	Epoch: [38][501/817]	Loss 0.0051 (0.0287)	
training:	Epoch: [38][502/817]	Loss 0.0056 (0.0287)	
training:	Epoch: [38][503/817]	Loss 0.0063 (0.0287)	
training:	Epoch: [38][504/817]	Loss 0.0049 (0.0286)	
training:	Epoch: [38][505/817]	Loss 0.0243 (0.0286)	
training:	Epoch: [38][506/817]	Loss 0.0061 (0.0286)	
training:	Epoch: [38][507/817]	Loss 0.0076 (0.0285)	
training:	Epoch: [38][508/817]	Loss 0.0061 (0.0285)	
training:	Epoch: [38][509/817]	Loss 0.0067 (0.0284)	
training:	Epoch: [38][510/817]	Loss 0.0079 (0.0284)	
training:	Epoch: [38][511/817]	Loss 0.0092 (0.0283)	
training:	Epoch: [38][512/817]	Loss 0.0054 (0.0283)	
training:	Epoch: [38][513/817]	Loss 0.0031 (0.0283)	
training:	Epoch: [38][514/817]	Loss 0.0038 (0.0282)	
training:	Epoch: [38][515/817]	Loss 0.0095 (0.0282)	
training:	Epoch: [38][516/817]	Loss 0.0053 (0.0281)	
training:	Epoch: [38][517/817]	Loss 0.0036 (0.0281)	
training:	Epoch: [38][518/817]	Loss 0.0048 (0.0280)	
training:	Epoch: [38][519/817]	Loss 0.0035 (0.0280)	
training:	Epoch: [38][520/817]	Loss 0.0062 (0.0279)	
training:	Epoch: [38][521/817]	Loss 0.0138 (0.0279)	
training:	Epoch: [38][522/817]	Loss 0.0082 (0.0279)	
training:	Epoch: [38][523/817]	Loss 0.0077 (0.0278)	
training:	Epoch: [38][524/817]	Loss 0.0035 (0.0278)	
training:	Epoch: [38][525/817]	Loss 0.0039 (0.0277)	
training:	Epoch: [38][526/817]	Loss 0.0074 (0.0277)	
training:	Epoch: [38][527/817]	Loss 0.0080 (0.0277)	
training:	Epoch: [38][528/817]	Loss 0.0068 (0.0276)	
training:	Epoch: [38][529/817]	Loss 0.0088 (0.0276)	
training:	Epoch: [38][530/817]	Loss 0.0088 (0.0276)	
training:	Epoch: [38][531/817]	Loss 0.0054 (0.0275)	
training:	Epoch: [38][532/817]	Loss 0.0077 (0.0275)	
training:	Epoch: [38][533/817]	Loss 0.0070 (0.0274)	
training:	Epoch: [38][534/817]	Loss 0.0048 (0.0274)	
training:	Epoch: [38][535/817]	Loss 0.0051 (0.0274)	
training:	Epoch: [38][536/817]	Loss 0.0121 (0.0273)	
training:	Epoch: [38][537/817]	Loss 0.0045 (0.0273)	
training:	Epoch: [38][538/817]	Loss 0.0074 (0.0273)	
training:	Epoch: [38][539/817]	Loss 0.0066 (0.0272)	
training:	Epoch: [38][540/817]	Loss 0.0060 (0.0272)	
training:	Epoch: [38][541/817]	Loss 0.0073 (0.0271)	
training:	Epoch: [38][542/817]	Loss 0.0050 (0.0271)	
training:	Epoch: [38][543/817]	Loss 0.0048 (0.0271)	
training:	Epoch: [38][544/817]	Loss 0.0053 (0.0270)	
training:	Epoch: [38][545/817]	Loss 0.0077 (0.0270)	
training:	Epoch: [38][546/817]	Loss 0.0061 (0.0269)	
training:	Epoch: [38][547/817]	Loss 0.4945 (0.0278)	
training:	Epoch: [38][548/817]	Loss 0.0039 (0.0278)	
training:	Epoch: [38][549/817]	Loss 0.0034 (0.0277)	
training:	Epoch: [38][550/817]	Loss 0.3650 (0.0283)	
training:	Epoch: [38][551/817]	Loss 0.0053 (0.0283)	
training:	Epoch: [38][552/817]	Loss 0.0060 (0.0282)	
training:	Epoch: [38][553/817]	Loss 0.0058 (0.0282)	
training:	Epoch: [38][554/817]	Loss 0.0068 (0.0282)	
training:	Epoch: [38][555/817]	Loss 0.0034 (0.0281)	
training:	Epoch: [38][556/817]	Loss 0.0056 (0.0281)	
training:	Epoch: [38][557/817]	Loss 0.0063 (0.0280)	
training:	Epoch: [38][558/817]	Loss 0.0051 (0.0280)	
training:	Epoch: [38][559/817]	Loss 0.0065 (0.0280)	
training:	Epoch: [38][560/817]	Loss 0.4930 (0.0288)	
training:	Epoch: [38][561/817]	Loss 0.0036 (0.0287)	
training:	Epoch: [38][562/817]	Loss 0.0119 (0.0287)	
training:	Epoch: [38][563/817]	Loss 0.0038 (0.0287)	
training:	Epoch: [38][564/817]	Loss 0.0036 (0.0286)	
training:	Epoch: [38][565/817]	Loss 0.0041 (0.0286)	
training:	Epoch: [38][566/817]	Loss 0.0081 (0.0285)	
training:	Epoch: [38][567/817]	Loss 0.0036 (0.0285)	
training:	Epoch: [38][568/817]	Loss 0.0046 (0.0285)	
training:	Epoch: [38][569/817]	Loss 0.0040 (0.0284)	
training:	Epoch: [38][570/817]	Loss 0.0137 (0.0284)	
training:	Epoch: [38][571/817]	Loss 0.0045 (0.0283)	
training:	Epoch: [38][572/817]	Loss 0.0059 (0.0283)	
training:	Epoch: [38][573/817]	Loss 0.0051 (0.0283)	
training:	Epoch: [38][574/817]	Loss 0.0047 (0.0282)	
training:	Epoch: [38][575/817]	Loss 0.0055 (0.0282)	
training:	Epoch: [38][576/817]	Loss 0.0092 (0.0282)	
training:	Epoch: [38][577/817]	Loss 0.0051 (0.0281)	
training:	Epoch: [38][578/817]	Loss 0.0042 (0.0281)	
training:	Epoch: [38][579/817]	Loss 0.2190 (0.0284)	
training:	Epoch: [38][580/817]	Loss 0.0053 (0.0284)	
training:	Epoch: [38][581/817]	Loss 0.0071 (0.0283)	
training:	Epoch: [38][582/817]	Loss 0.0076 (0.0283)	
training:	Epoch: [38][583/817]	Loss 0.0035 (0.0282)	
training:	Epoch: [38][584/817]	Loss 0.0033 (0.0282)	
training:	Epoch: [38][585/817]	Loss 0.0038 (0.0282)	
training:	Epoch: [38][586/817]	Loss 0.0085 (0.0281)	
training:	Epoch: [38][587/817]	Loss 0.0078 (0.0281)	
training:	Epoch: [38][588/817]	Loss 0.0102 (0.0281)	
training:	Epoch: [38][589/817]	Loss 0.0057 (0.0280)	
training:	Epoch: [38][590/817]	Loss 0.0082 (0.0280)	
training:	Epoch: [38][591/817]	Loss 0.0134 (0.0280)	
training:	Epoch: [38][592/817]	Loss 0.0038 (0.0279)	
training:	Epoch: [38][593/817]	Loss 0.3706 (0.0285)	
training:	Epoch: [38][594/817]	Loss 0.0039 (0.0285)	
training:	Epoch: [38][595/817]	Loss 0.0050 (0.0284)	
training:	Epoch: [38][596/817]	Loss 0.0051 (0.0284)	
training:	Epoch: [38][597/817]	Loss 0.0134 (0.0284)	
training:	Epoch: [38][598/817]	Loss 0.0072 (0.0283)	
training:	Epoch: [38][599/817]	Loss 0.0041 (0.0283)	
training:	Epoch: [38][600/817]	Loss 0.0079 (0.0283)	
training:	Epoch: [38][601/817]	Loss 0.0051 (0.0282)	
training:	Epoch: [38][602/817]	Loss 0.0052 (0.0282)	
training:	Epoch: [38][603/817]	Loss 0.0096 (0.0281)	
training:	Epoch: [38][604/817]	Loss 0.0065 (0.0281)	
training:	Epoch: [38][605/817]	Loss 0.0133 (0.0281)	
training:	Epoch: [38][606/817]	Loss 0.0068 (0.0280)	
training:	Epoch: [38][607/817]	Loss 0.0263 (0.0280)	
training:	Epoch: [38][608/817]	Loss 0.0121 (0.0280)	
training:	Epoch: [38][609/817]	Loss 0.0048 (0.0280)	
training:	Epoch: [38][610/817]	Loss 0.0160 (0.0280)	
training:	Epoch: [38][611/817]	Loss 0.0151 (0.0279)	
training:	Epoch: [38][612/817]	Loss 0.0189 (0.0279)	
training:	Epoch: [38][613/817]	Loss 0.0044 (0.0279)	
training:	Epoch: [38][614/817]	Loss 0.0200 (0.0279)	
training:	Epoch: [38][615/817]	Loss 0.0192 (0.0279)	
training:	Epoch: [38][616/817]	Loss 0.0131 (0.0278)	
training:	Epoch: [38][617/817]	Loss 0.0095 (0.0278)	
training:	Epoch: [38][618/817]	Loss 0.0075 (0.0278)	
training:	Epoch: [38][619/817]	Loss 0.0058 (0.0277)	
training:	Epoch: [38][620/817]	Loss 0.0045 (0.0277)	
training:	Epoch: [38][621/817]	Loss 0.0177 (0.0277)	
training:	Epoch: [38][622/817]	Loss 0.0029 (0.0276)	
training:	Epoch: [38][623/817]	Loss 0.0092 (0.0276)	
training:	Epoch: [38][624/817]	Loss 0.0115 (0.0276)	
training:	Epoch: [38][625/817]	Loss 0.0055 (0.0276)	
training:	Epoch: [38][626/817]	Loss 0.0090 (0.0275)	
training:	Epoch: [38][627/817]	Loss 0.0042 (0.0275)	
training:	Epoch: [38][628/817]	Loss 0.0061 (0.0275)	
training:	Epoch: [38][629/817]	Loss 0.0067 (0.0274)	
training:	Epoch: [38][630/817]	Loss 0.0058 (0.0274)	
training:	Epoch: [38][631/817]	Loss 0.0055 (0.0274)	
training:	Epoch: [38][632/817]	Loss 0.0084 (0.0273)	
training:	Epoch: [38][633/817]	Loss 0.0029 (0.0273)	
training:	Epoch: [38][634/817]	Loss 0.0102 (0.0273)	
training:	Epoch: [38][635/817]	Loss 0.0062 (0.0272)	
training:	Epoch: [38][636/817]	Loss 0.0047 (0.0272)	
training:	Epoch: [38][637/817]	Loss 0.0056 (0.0272)	
training:	Epoch: [38][638/817]	Loss 0.0062 (0.0271)	
training:	Epoch: [38][639/817]	Loss 0.0060 (0.0271)	
training:	Epoch: [38][640/817]	Loss 0.0062 (0.0271)	
training:	Epoch: [38][641/817]	Loss 0.0045 (0.0270)	
training:	Epoch: [38][642/817]	Loss 0.0041 (0.0270)	
training:	Epoch: [38][643/817]	Loss 0.0045 (0.0270)	
training:	Epoch: [38][644/817]	Loss 0.0064 (0.0269)	
training:	Epoch: [38][645/817]	Loss 0.0043 (0.0269)	
training:	Epoch: [38][646/817]	Loss 0.0057 (0.0269)	
training:	Epoch: [38][647/817]	Loss 0.0047 (0.0268)	
training:	Epoch: [38][648/817]	Loss 0.0061 (0.0268)	
training:	Epoch: [38][649/817]	Loss 0.0032 (0.0267)	
training:	Epoch: [38][650/817]	Loss 0.0037 (0.0267)	
training:	Epoch: [38][651/817]	Loss 0.0048 (0.0267)	
training:	Epoch: [38][652/817]	Loss 0.0047 (0.0266)	
training:	Epoch: [38][653/817]	Loss 0.0082 (0.0266)	
training:	Epoch: [38][654/817]	Loss 0.0042 (0.0266)	
training:	Epoch: [38][655/817]	Loss 0.0064 (0.0266)	
training:	Epoch: [38][656/817]	Loss 0.0045 (0.0265)	
training:	Epoch: [38][657/817]	Loss 0.4677 (0.0272)	
training:	Epoch: [38][658/817]	Loss 0.0084 (0.0272)	
training:	Epoch: [38][659/817]	Loss 0.0037 (0.0271)	
training:	Epoch: [38][660/817]	Loss 0.0047 (0.0271)	
training:	Epoch: [38][661/817]	Loss 0.0039 (0.0271)	
training:	Epoch: [38][662/817]	Loss 0.0037 (0.0270)	
training:	Epoch: [38][663/817]	Loss 0.0058 (0.0270)	
training:	Epoch: [38][664/817]	Loss 0.0035 (0.0270)	
training:	Epoch: [38][665/817]	Loss 0.0074 (0.0269)	
training:	Epoch: [38][666/817]	Loss 0.0043 (0.0269)	
training:	Epoch: [38][667/817]	Loss 0.3532 (0.0274)	
training:	Epoch: [38][668/817]	Loss 0.0037 (0.0273)	
training:	Epoch: [38][669/817]	Loss 0.0034 (0.0273)	
training:	Epoch: [38][670/817]	Loss 0.0050 (0.0273)	
training:	Epoch: [38][671/817]	Loss 0.2223 (0.0276)	
training:	Epoch: [38][672/817]	Loss 0.0048 (0.0275)	
training:	Epoch: [38][673/817]	Loss 0.0049 (0.0275)	
training:	Epoch: [38][674/817]	Loss 0.0054 (0.0275)	
training:	Epoch: [38][675/817]	Loss 0.0096 (0.0274)	
training:	Epoch: [38][676/817]	Loss 0.0150 (0.0274)	
training:	Epoch: [38][677/817]	Loss 0.0041 (0.0274)	
training:	Epoch: [38][678/817]	Loss 0.0125 (0.0274)	
training:	Epoch: [38][679/817]	Loss 0.0054 (0.0273)	
training:	Epoch: [38][680/817]	Loss 0.0090 (0.0273)	
training:	Epoch: [38][681/817]	Loss 0.0064 (0.0273)	
training:	Epoch: [38][682/817]	Loss 0.0058 (0.0272)	
training:	Epoch: [38][683/817]	Loss 0.0060 (0.0272)	
training:	Epoch: [38][684/817]	Loss 0.0065 (0.0272)	
training:	Epoch: [38][685/817]	Loss 0.4744 (0.0278)	
training:	Epoch: [38][686/817]	Loss 0.0113 (0.0278)	
training:	Epoch: [38][687/817]	Loss 0.0052 (0.0278)	
training:	Epoch: [38][688/817]	Loss 0.0065 (0.0277)	
training:	Epoch: [38][689/817]	Loss 0.0074 (0.0277)	
training:	Epoch: [38][690/817]	Loss 0.0067 (0.0277)	
training:	Epoch: [38][691/817]	Loss 0.5072 (0.0284)	
training:	Epoch: [38][692/817]	Loss 0.0080 (0.0284)	
training:	Epoch: [38][693/817]	Loss 0.0054 (0.0283)	
training:	Epoch: [38][694/817]	Loss 0.0096 (0.0283)	
training:	Epoch: [38][695/817]	Loss 0.0121 (0.0283)	
training:	Epoch: [38][696/817]	Loss 0.0042 (0.0282)	
training:	Epoch: [38][697/817]	Loss 0.0116 (0.0282)	
training:	Epoch: [38][698/817]	Loss 0.0087 (0.0282)	
training:	Epoch: [38][699/817]	Loss 0.0074 (0.0282)	
training:	Epoch: [38][700/817]	Loss 0.0265 (0.0281)	
training:	Epoch: [38][701/817]	Loss 0.0125 (0.0281)	
training:	Epoch: [38][702/817]	Loss 0.0049 (0.0281)	
training:	Epoch: [38][703/817]	Loss 0.0113 (0.0281)	
training:	Epoch: [38][704/817]	Loss 0.0039 (0.0280)	
training:	Epoch: [38][705/817]	Loss 0.0173 (0.0280)	
training:	Epoch: [38][706/817]	Loss 0.0057 (0.0280)	
training:	Epoch: [38][707/817]	Loss 0.0082 (0.0280)	
training:	Epoch: [38][708/817]	Loss 0.0043 (0.0279)	
training:	Epoch: [38][709/817]	Loss 0.0049 (0.0279)	
training:	Epoch: [38][710/817]	Loss 0.0056 (0.0279)	
training:	Epoch: [38][711/817]	Loss 0.0045 (0.0278)	
training:	Epoch: [38][712/817]	Loss 0.0052 (0.0278)	
training:	Epoch: [38][713/817]	Loss 0.0544 (0.0278)	
training:	Epoch: [38][714/817]	Loss 0.0059 (0.0278)	
training:	Epoch: [38][715/817]	Loss 0.0048 (0.0278)	
training:	Epoch: [38][716/817]	Loss 0.0029 (0.0277)	
training:	Epoch: [38][717/817]	Loss 0.0072 (0.0277)	
training:	Epoch: [38][718/817]	Loss 0.0050 (0.0277)	
training:	Epoch: [38][719/817]	Loss 0.0053 (0.0276)	
training:	Epoch: [38][720/817]	Loss 0.0062 (0.0276)	
training:	Epoch: [38][721/817]	Loss 0.0043 (0.0276)	
training:	Epoch: [38][722/817]	Loss 0.0105 (0.0276)	
training:	Epoch: [38][723/817]	Loss 0.0119 (0.0275)	
training:	Epoch: [38][724/817]	Loss 0.0060 (0.0275)	
training:	Epoch: [38][725/817]	Loss 0.0098 (0.0275)	
training:	Epoch: [38][726/817]	Loss 0.0064 (0.0275)	
training:	Epoch: [38][727/817]	Loss 0.0083 (0.0274)	
training:	Epoch: [38][728/817]	Loss 0.0113 (0.0274)	
training:	Epoch: [38][729/817]	Loss 0.0032 (0.0274)	
training:	Epoch: [38][730/817]	Loss 0.0035 (0.0273)	
training:	Epoch: [38][731/817]	Loss 0.0045 (0.0273)	
training:	Epoch: [38][732/817]	Loss 0.0070 (0.0273)	
training:	Epoch: [38][733/817]	Loss 0.0063 (0.0273)	
training:	Epoch: [38][734/817]	Loss 0.0047 (0.0272)	
training:	Epoch: [38][735/817]	Loss 0.0089 (0.0272)	
training:	Epoch: [38][736/817]	Loss 0.0044 (0.0272)	
training:	Epoch: [38][737/817]	Loss 0.0060 (0.0271)	
training:	Epoch: [38][738/817]	Loss 0.0079 (0.0271)	
training:	Epoch: [38][739/817]	Loss 0.0119 (0.0271)	
training:	Epoch: [38][740/817]	Loss 0.5780 (0.0278)	
training:	Epoch: [38][741/817]	Loss 0.0060 (0.0278)	
training:	Epoch: [38][742/817]	Loss 0.0088 (0.0278)	
training:	Epoch: [38][743/817]	Loss 0.0046 (0.0278)	
training:	Epoch: [38][744/817]	Loss 0.0050 (0.0277)	
training:	Epoch: [38][745/817]	Loss 0.0042 (0.0277)	
training:	Epoch: [38][746/817]	Loss 0.0053 (0.0277)	
training:	Epoch: [38][747/817]	Loss 0.0115 (0.0276)	
training:	Epoch: [38][748/817]	Loss 0.0078 (0.0276)	
training:	Epoch: [38][749/817]	Loss 0.0115 (0.0276)	
training:	Epoch: [38][750/817]	Loss 0.0038 (0.0276)	
training:	Epoch: [38][751/817]	Loss 0.3447 (0.0280)	
training:	Epoch: [38][752/817]	Loss 0.0034 (0.0279)	
training:	Epoch: [38][753/817]	Loss 0.0056 (0.0279)	
training:	Epoch: [38][754/817]	Loss 0.0051 (0.0279)	
training:	Epoch: [38][755/817]	Loss 0.0046 (0.0279)	
training:	Epoch: [38][756/817]	Loss 0.0047 (0.0278)	
training:	Epoch: [38][757/817]	Loss 0.0073 (0.0278)	
training:	Epoch: [38][758/817]	Loss 0.0054 (0.0278)	
training:	Epoch: [38][759/817]	Loss 0.0067 (0.0277)	
training:	Epoch: [38][760/817]	Loss 0.0078 (0.0277)	
training:	Epoch: [38][761/817]	Loss 0.0038 (0.0277)	
training:	Epoch: [38][762/817]	Loss 0.0066 (0.0277)	
training:	Epoch: [38][763/817]	Loss 0.0103 (0.0276)	
training:	Epoch: [38][764/817]	Loss 0.0036 (0.0276)	
training:	Epoch: [38][765/817]	Loss 0.0067 (0.0276)	
training:	Epoch: [38][766/817]	Loss 0.0045 (0.0275)	
training:	Epoch: [38][767/817]	Loss 0.5183 (0.0282)	
training:	Epoch: [38][768/817]	Loss 0.0036 (0.0282)	
training:	Epoch: [38][769/817]	Loss 0.0140 (0.0281)	
training:	Epoch: [38][770/817]	Loss 0.0073 (0.0281)	
training:	Epoch: [38][771/817]	Loss 0.0064 (0.0281)	
training:	Epoch: [38][772/817]	Loss 0.0084 (0.0281)	
training:	Epoch: [38][773/817]	Loss 0.0114 (0.0280)	
training:	Epoch: [38][774/817]	Loss 0.0041 (0.0280)	
training:	Epoch: [38][775/817]	Loss 0.0063 (0.0280)	
training:	Epoch: [38][776/817]	Loss 0.0043 (0.0279)	
training:	Epoch: [38][777/817]	Loss 0.0066 (0.0279)	
training:	Epoch: [38][778/817]	Loss 0.0060 (0.0279)	
training:	Epoch: [38][779/817]	Loss 0.0068 (0.0279)	
training:	Epoch: [38][780/817]	Loss 0.0064 (0.0278)	
training:	Epoch: [38][781/817]	Loss 0.0053 (0.0278)	
training:	Epoch: [38][782/817]	Loss 0.0068 (0.0278)	
training:	Epoch: [38][783/817]	Loss 0.0083 (0.0278)	
training:	Epoch: [38][784/817]	Loss 0.0060 (0.0277)	
training:	Epoch: [38][785/817]	Loss 0.0072 (0.0277)	
training:	Epoch: [38][786/817]	Loss 0.0067 (0.0277)	
training:	Epoch: [38][787/817]	Loss 0.0044 (0.0276)	
training:	Epoch: [38][788/817]	Loss 0.0060 (0.0276)	
training:	Epoch: [38][789/817]	Loss 0.0070 (0.0276)	
training:	Epoch: [38][790/817]	Loss 0.0059 (0.0276)	
training:	Epoch: [38][791/817]	Loss 0.0084 (0.0275)	
training:	Epoch: [38][792/817]	Loss 0.0025 (0.0275)	
training:	Epoch: [38][793/817]	Loss 0.0087 (0.0275)	
training:	Epoch: [38][794/817]	Loss 0.0123 (0.0275)	
training:	Epoch: [38][795/817]	Loss 0.0113 (0.0274)	
training:	Epoch: [38][796/817]	Loss 0.0044 (0.0274)	
training:	Epoch: [38][797/817]	Loss 0.0064 (0.0274)	
training:	Epoch: [38][798/817]	Loss 0.0038 (0.0274)	
training:	Epoch: [38][799/817]	Loss 0.0055 (0.0273)	
training:	Epoch: [38][800/817]	Loss 0.0083 (0.0273)	
training:	Epoch: [38][801/817]	Loss 0.0078 (0.0273)	
training:	Epoch: [38][802/817]	Loss 0.0031 (0.0273)	
training:	Epoch: [38][803/817]	Loss 0.0092 (0.0272)	
training:	Epoch: [38][804/817]	Loss 0.0089 (0.0272)	
training:	Epoch: [38][805/817]	Loss 0.0057 (0.0272)	
training:	Epoch: [38][806/817]	Loss 0.0032 (0.0271)	
training:	Epoch: [38][807/817]	Loss 0.0056 (0.0271)	
training:	Epoch: [38][808/817]	Loss 0.0052 (0.0271)	
training:	Epoch: [38][809/817]	Loss 0.0058 (0.0271)	
training:	Epoch: [38][810/817]	Loss 0.0065 (0.0270)	
training:	Epoch: [38][811/817]	Loss 0.0059 (0.0270)	
training:	Epoch: [38][812/817]	Loss 0.0041 (0.0270)	
training:	Epoch: [38][813/817]	Loss 0.0052 (0.0270)	
training:	Epoch: [38][814/817]	Loss 0.0042 (0.0269)	
training:	Epoch: [38][815/817]	Loss 0.0037 (0.0269)	
training:	Epoch: [38][816/817]	Loss 0.0042 (0.0269)	
training:	Epoch: [38][817/817]	Loss 0.0046 (0.0269)	
Training:	 Loss: 0.0268

Training:	 ACC: 0.9948 0.9948 0.9956 0.9939
Validation:	 ACC: 0.7789 0.7790 0.7820 0.7758
Validation:	 Best_BACC: 0.8223 0.8218 0.8127 0.8318
Validation:	 Loss: 1.0357
Pretraining:	Epoch 39/200
----------
training:	Epoch: [39][1/817]	Loss 0.0047 (0.0047)	
training:	Epoch: [39][2/817]	Loss 0.0043 (0.0045)	
training:	Epoch: [39][3/817]	Loss 0.0080 (0.0056)	
training:	Epoch: [39][4/817]	Loss 0.0039 (0.0052)	
training:	Epoch: [39][5/817]	Loss 0.0032 (0.0048)	
training:	Epoch: [39][6/817]	Loss 0.2891 (0.0522)	
training:	Epoch: [39][7/817]	Loss 0.0061 (0.0456)	
training:	Epoch: [39][8/817]	Loss 0.0054 (0.0406)	
training:	Epoch: [39][9/817]	Loss 0.0038 (0.0365)	
training:	Epoch: [39][10/817]	Loss 0.0066 (0.0335)	
training:	Epoch: [39][11/817]	Loss 0.0049 (0.0309)	
training:	Epoch: [39][12/817]	Loss 0.0084 (0.0290)	
training:	Epoch: [39][13/817]	Loss 0.0057 (0.0272)	
training:	Epoch: [39][14/817]	Loss 0.0040 (0.0256)	
training:	Epoch: [39][15/817]	Loss 0.0067 (0.0243)	
training:	Epoch: [39][16/817]	Loss 0.0118 (0.0235)	
training:	Epoch: [39][17/817]	Loss 0.0067 (0.0226)	
training:	Epoch: [39][18/817]	Loss 0.0050 (0.0216)	
training:	Epoch: [39][19/817]	Loss 0.0122 (0.0211)	
training:	Epoch: [39][20/817]	Loss 0.0041 (0.0202)	
training:	Epoch: [39][21/817]	Loss 0.0032 (0.0194)	
training:	Epoch: [39][22/817]	Loss 0.0089 (0.0189)	
training:	Epoch: [39][23/817]	Loss 0.0053 (0.0184)	
training:	Epoch: [39][24/817]	Loss 0.0083 (0.0179)	
training:	Epoch: [39][25/817]	Loss 0.0028 (0.0173)	
training:	Epoch: [39][26/817]	Loss 0.0046 (0.0168)	
training:	Epoch: [39][27/817]	Loss 0.0170 (0.0168)	
training:	Epoch: [39][28/817]	Loss 0.3848 (0.0300)	
training:	Epoch: [39][29/817]	Loss 0.0036 (0.0291)	
training:	Epoch: [39][30/817]	Loss 0.0066 (0.0283)	
training:	Epoch: [39][31/817]	Loss 0.0053 (0.0276)	
training:	Epoch: [39][32/817]	Loss 0.0055 (0.0269)	
training:	Epoch: [39][33/817]	Loss 0.0075 (0.0263)	
training:	Epoch: [39][34/817]	Loss 0.0074 (0.0258)	
training:	Epoch: [39][35/817]	Loss 0.0091 (0.0253)	
training:	Epoch: [39][36/817]	Loss 0.0044 (0.0247)	
training:	Epoch: [39][37/817]	Loss 0.0063 (0.0242)	
training:	Epoch: [39][38/817]	Loss 0.0051 (0.0237)	
training:	Epoch: [39][39/817]	Loss 0.0035 (0.0232)	
training:	Epoch: [39][40/817]	Loss 0.0038 (0.0227)	
training:	Epoch: [39][41/817]	Loss 0.0128 (0.0225)	
training:	Epoch: [39][42/817]	Loss 0.0056 (0.0220)	
training:	Epoch: [39][43/817]	Loss 0.2840 (0.0281)	
training:	Epoch: [39][44/817]	Loss 0.0082 (0.0277)	
training:	Epoch: [39][45/817]	Loss 0.0076 (0.0272)	
training:	Epoch: [39][46/817]	Loss 0.0035 (0.0267)	
training:	Epoch: [39][47/817]	Loss 0.0110 (0.0264)	
training:	Epoch: [39][48/817]	Loss 0.0063 (0.0260)	
training:	Epoch: [39][49/817]	Loss 0.0047 (0.0255)	
training:	Epoch: [39][50/817]	Loss 0.0068 (0.0252)	
training:	Epoch: [39][51/817]	Loss 0.0046 (0.0248)	
training:	Epoch: [39][52/817]	Loss 0.0038 (0.0244)	
training:	Epoch: [39][53/817]	Loss 0.0113 (0.0241)	
training:	Epoch: [39][54/817]	Loss 0.0064 (0.0238)	
training:	Epoch: [39][55/817]	Loss 0.0052 (0.0234)	
training:	Epoch: [39][56/817]	Loss 0.0032 (0.0231)	
training:	Epoch: [39][57/817]	Loss 0.0068 (0.0228)	
training:	Epoch: [39][58/817]	Loss 0.0045 (0.0225)	
training:	Epoch: [39][59/817]	Loss 0.0085 (0.0222)	
training:	Epoch: [39][60/817]	Loss 0.0068 (0.0220)	
training:	Epoch: [39][61/817]	Loss 0.0044 (0.0217)	
training:	Epoch: [39][62/817]	Loss 0.0034 (0.0214)	
training:	Epoch: [39][63/817]	Loss 0.0073 (0.0212)	
training:	Epoch: [39][64/817]	Loss 0.0057 (0.0209)	
training:	Epoch: [39][65/817]	Loss 0.0046 (0.0207)	
training:	Epoch: [39][66/817]	Loss 0.0052 (0.0204)	
training:	Epoch: [39][67/817]	Loss 0.0105 (0.0203)	
training:	Epoch: [39][68/817]	Loss 0.0054 (0.0201)	
training:	Epoch: [39][69/817]	Loss 0.0066 (0.0199)	
training:	Epoch: [39][70/817]	Loss 0.0068 (0.0197)	
training:	Epoch: [39][71/817]	Loss 0.0062 (0.0195)	
training:	Epoch: [39][72/817]	Loss 0.0070 (0.0193)	
training:	Epoch: [39][73/817]	Loss 0.0047 (0.0191)	
training:	Epoch: [39][74/817]	Loss 0.0074 (0.0190)	
training:	Epoch: [39][75/817]	Loss 0.0044 (0.0188)	
training:	Epoch: [39][76/817]	Loss 0.0111 (0.0187)	
training:	Epoch: [39][77/817]	Loss 0.0044 (0.0185)	
training:	Epoch: [39][78/817]	Loss 0.0054 (0.0183)	
training:	Epoch: [39][79/817]	Loss 0.6879 (0.0268)	
training:	Epoch: [39][80/817]	Loss 0.0062 (0.0265)	
training:	Epoch: [39][81/817]	Loss 0.0043 (0.0263)	
training:	Epoch: [39][82/817]	Loss 0.0055 (0.0260)	
training:	Epoch: [39][83/817]	Loss 0.0050 (0.0258)	
training:	Epoch: [39][84/817]	Loss 0.0049 (0.0255)	
training:	Epoch: [39][85/817]	Loss 0.0042 (0.0253)	
training:	Epoch: [39][86/817]	Loss 0.0040 (0.0250)	
training:	Epoch: [39][87/817]	Loss 0.0066 (0.0248)	
training:	Epoch: [39][88/817]	Loss 0.0048 (0.0246)	
training:	Epoch: [39][89/817]	Loss 0.0075 (0.0244)	
training:	Epoch: [39][90/817]	Loss 0.0060 (0.0242)	
training:	Epoch: [39][91/817]	Loss 0.0066 (0.0240)	
training:	Epoch: [39][92/817]	Loss 0.0038 (0.0238)	
training:	Epoch: [39][93/817]	Loss 0.0032 (0.0236)	
training:	Epoch: [39][94/817]	Loss 0.0061 (0.0234)	
training:	Epoch: [39][95/817]	Loss 0.0055 (0.0232)	
training:	Epoch: [39][96/817]	Loss 0.4848 (0.0280)	
training:	Epoch: [39][97/817]	Loss 0.0054 (0.0278)	
training:	Epoch: [39][98/817]	Loss 0.0063 (0.0275)	
training:	Epoch: [39][99/817]	Loss 0.0104 (0.0274)	
training:	Epoch: [39][100/817]	Loss 0.0046 (0.0271)	
training:	Epoch: [39][101/817]	Loss 0.0047 (0.0269)	
training:	Epoch: [39][102/817]	Loss 0.0049 (0.0267)	
training:	Epoch: [39][103/817]	Loss 0.0049 (0.0265)	
training:	Epoch: [39][104/817]	Loss 0.0032 (0.0263)	
training:	Epoch: [39][105/817]	Loss 0.0182 (0.0262)	
training:	Epoch: [39][106/817]	Loss 0.0062 (0.0260)	
training:	Epoch: [39][107/817]	Loss 0.0046 (0.0258)	
training:	Epoch: [39][108/817]	Loss 0.0049 (0.0256)	
training:	Epoch: [39][109/817]	Loss 0.4871 (0.0298)	
training:	Epoch: [39][110/817]	Loss 0.0046 (0.0296)	
training:	Epoch: [39][111/817]	Loss 0.0025 (0.0294)	
training:	Epoch: [39][112/817]	Loss 0.0030 (0.0291)	
training:	Epoch: [39][113/817]	Loss 0.0055 (0.0289)	
training:	Epoch: [39][114/817]	Loss 0.0046 (0.0287)	
training:	Epoch: [39][115/817]	Loss 0.0056 (0.0285)	
training:	Epoch: [39][116/817]	Loss 0.0060 (0.0283)	
training:	Epoch: [39][117/817]	Loss 0.0057 (0.0281)	
training:	Epoch: [39][118/817]	Loss 0.0081 (0.0279)	
training:	Epoch: [39][119/817]	Loss 0.0047 (0.0278)	
training:	Epoch: [39][120/817]	Loss 0.0050 (0.0276)	
training:	Epoch: [39][121/817]	Loss 0.0036 (0.0274)	
training:	Epoch: [39][122/817]	Loss 0.0056 (0.0272)	
training:	Epoch: [39][123/817]	Loss 0.0058 (0.0270)	
training:	Epoch: [39][124/817]	Loss 0.0051 (0.0268)	
training:	Epoch: [39][125/817]	Loss 0.4406 (0.0301)	
training:	Epoch: [39][126/817]	Loss 0.0053 (0.0299)	
training:	Epoch: [39][127/817]	Loss 0.0055 (0.0298)	
training:	Epoch: [39][128/817]	Loss 0.0071 (0.0296)	
training:	Epoch: [39][129/817]	Loss 0.0056 (0.0294)	
training:	Epoch: [39][130/817]	Loss 0.0081 (0.0292)	
training:	Epoch: [39][131/817]	Loss 0.0063 (0.0291)	
training:	Epoch: [39][132/817]	Loss 0.0043 (0.0289)	
training:	Epoch: [39][133/817]	Loss 0.0065 (0.0287)	
training:	Epoch: [39][134/817]	Loss 0.0063 (0.0285)	
training:	Epoch: [39][135/817]	Loss 0.0081 (0.0284)	
training:	Epoch: [39][136/817]	Loss 0.0062 (0.0282)	
training:	Epoch: [39][137/817]	Loss 0.0097 (0.0281)	
training:	Epoch: [39][138/817]	Loss 0.0042 (0.0279)	
training:	Epoch: [39][139/817]	Loss 0.0033 (0.0277)	
training:	Epoch: [39][140/817]	Loss 0.0039 (0.0276)	
training:	Epoch: [39][141/817]	Loss 0.0057 (0.0274)	
training:	Epoch: [39][142/817]	Loss 0.0101 (0.0273)	
training:	Epoch: [39][143/817]	Loss 0.4915 (0.0305)	
training:	Epoch: [39][144/817]	Loss 0.0057 (0.0304)	
training:	Epoch: [39][145/817]	Loss 0.0063 (0.0302)	
training:	Epoch: [39][146/817]	Loss 0.0103 (0.0301)	
training:	Epoch: [39][147/817]	Loss 0.0038 (0.0299)	
training:	Epoch: [39][148/817]	Loss 0.0066 (0.0297)	
training:	Epoch: [39][149/817]	Loss 0.0060 (0.0296)	
training:	Epoch: [39][150/817]	Loss 0.0110 (0.0294)	
training:	Epoch: [39][151/817]	Loss 0.0079 (0.0293)	
training:	Epoch: [39][152/817]	Loss 0.0145 (0.0292)	
training:	Epoch: [39][153/817]	Loss 0.0104 (0.0291)	
training:	Epoch: [39][154/817]	Loss 0.0065 (0.0289)	
training:	Epoch: [39][155/817]	Loss 0.0071 (0.0288)	
training:	Epoch: [39][156/817]	Loss 0.0040 (0.0286)	
training:	Epoch: [39][157/817]	Loss 0.0039 (0.0285)	
training:	Epoch: [39][158/817]	Loss 0.0055 (0.0283)	
training:	Epoch: [39][159/817]	Loss 0.0030 (0.0282)	
training:	Epoch: [39][160/817]	Loss 0.0054 (0.0280)	
training:	Epoch: [39][161/817]	Loss 0.0083 (0.0279)	
training:	Epoch: [39][162/817]	Loss 0.0095 (0.0278)	
training:	Epoch: [39][163/817]	Loss 0.0060 (0.0277)	
training:	Epoch: [39][164/817]	Loss 0.0031 (0.0275)	
training:	Epoch: [39][165/817]	Loss 0.0041 (0.0274)	
training:	Epoch: [39][166/817]	Loss 0.0076 (0.0272)	
training:	Epoch: [39][167/817]	Loss 0.0079 (0.0271)	
training:	Epoch: [39][168/817]	Loss 0.0055 (0.0270)	
training:	Epoch: [39][169/817]	Loss 0.0036 (0.0269)	
training:	Epoch: [39][170/817]	Loss 0.0050 (0.0267)	
training:	Epoch: [39][171/817]	Loss 0.0094 (0.0266)	
training:	Epoch: [39][172/817]	Loss 0.0068 (0.0265)	
training:	Epoch: [39][173/817]	Loss 0.5247 (0.0294)	
training:	Epoch: [39][174/817]	Loss 0.0064 (0.0293)	
training:	Epoch: [39][175/817]	Loss 0.0099 (0.0291)	
training:	Epoch: [39][176/817]	Loss 0.0038 (0.0290)	
training:	Epoch: [39][177/817]	Loss 0.0058 (0.0289)	
training:	Epoch: [39][178/817]	Loss 0.0071 (0.0288)	
training:	Epoch: [39][179/817]	Loss 0.0048 (0.0286)	
training:	Epoch: [39][180/817]	Loss 0.0079 (0.0285)	
training:	Epoch: [39][181/817]	Loss 0.0082 (0.0284)	
training:	Epoch: [39][182/817]	Loss 0.0043 (0.0283)	
training:	Epoch: [39][183/817]	Loss 0.0074 (0.0281)	
training:	Epoch: [39][184/817]	Loss 0.0029 (0.0280)	
training:	Epoch: [39][185/817]	Loss 0.0037 (0.0279)	
training:	Epoch: [39][186/817]	Loss 0.0057 (0.0278)	
training:	Epoch: [39][187/817]	Loss 0.0106 (0.0277)	
training:	Epoch: [39][188/817]	Loss 0.0075 (0.0276)	
training:	Epoch: [39][189/817]	Loss 0.0055 (0.0274)	
training:	Epoch: [39][190/817]	Loss 0.0539 (0.0276)	
training:	Epoch: [39][191/817]	Loss 0.0027 (0.0274)	
training:	Epoch: [39][192/817]	Loss 0.0063 (0.0273)	
training:	Epoch: [39][193/817]	Loss 0.0036 (0.0272)	
training:	Epoch: [39][194/817]	Loss 0.0198 (0.0272)	
training:	Epoch: [39][195/817]	Loss 0.0050 (0.0271)	
training:	Epoch: [39][196/817]	Loss 0.0080 (0.0270)	
training:	Epoch: [39][197/817]	Loss 0.0058 (0.0269)	
training:	Epoch: [39][198/817]	Loss 0.0065 (0.0268)	
training:	Epoch: [39][199/817]	Loss 0.0979 (0.0271)	
training:	Epoch: [39][200/817]	Loss 0.0036 (0.0270)	
training:	Epoch: [39][201/817]	Loss 0.0087 (0.0269)	
training:	Epoch: [39][202/817]	Loss 0.0049 (0.0268)	
training:	Epoch: [39][203/817]	Loss 0.0060 (0.0267)	
training:	Epoch: [39][204/817]	Loss 0.0036 (0.0266)	
training:	Epoch: [39][205/817]	Loss 0.0058 (0.0265)	
training:	Epoch: [39][206/817]	Loss 0.0027 (0.0264)	
training:	Epoch: [39][207/817]	Loss 0.0046 (0.0263)	
training:	Epoch: [39][208/817]	Loss 0.0038 (0.0262)	
training:	Epoch: [39][209/817]	Loss 0.0056 (0.0261)	
training:	Epoch: [39][210/817]	Loss 0.1158 (0.0265)	
training:	Epoch: [39][211/817]	Loss 0.0059 (0.0264)	
training:	Epoch: [39][212/817]	Loss 0.0063 (0.0263)	
training:	Epoch: [39][213/817]	Loss 0.0039 (0.0262)	
training:	Epoch: [39][214/817]	Loss 0.0091 (0.0261)	
training:	Epoch: [39][215/817]	Loss 0.0044 (0.0260)	
training:	Epoch: [39][216/817]	Loss 0.0040 (0.0259)	
training:	Epoch: [39][217/817]	Loss 0.3673 (0.0275)	
training:	Epoch: [39][218/817]	Loss 0.0042 (0.0274)	
training:	Epoch: [39][219/817]	Loss 0.0048 (0.0273)	
training:	Epoch: [39][220/817]	Loss 0.0056 (0.0272)	
training:	Epoch: [39][221/817]	Loss 0.0032 (0.0271)	
training:	Epoch: [39][222/817]	Loss 0.0042 (0.0270)	
training:	Epoch: [39][223/817]	Loss 0.0113 (0.0269)	
training:	Epoch: [39][224/817]	Loss 0.0066 (0.0268)	
training:	Epoch: [39][225/817]	Loss 0.0051 (0.0267)	
training:	Epoch: [39][226/817]	Loss 0.0029 (0.0266)	
training:	Epoch: [39][227/817]	Loss 0.0059 (0.0265)	
training:	Epoch: [39][228/817]	Loss 0.0117 (0.0264)	
training:	Epoch: [39][229/817]	Loss 0.0043 (0.0263)	
training:	Epoch: [39][230/817]	Loss 0.0089 (0.0263)	
training:	Epoch: [39][231/817]	Loss 0.0034 (0.0262)	
training:	Epoch: [39][232/817]	Loss 0.0046 (0.0261)	
training:	Epoch: [39][233/817]	Loss 0.0050 (0.0260)	
training:	Epoch: [39][234/817]	Loss 0.0071 (0.0259)	
training:	Epoch: [39][235/817]	Loss 0.0091 (0.0258)	
training:	Epoch: [39][236/817]	Loss 0.0067 (0.0257)	
training:	Epoch: [39][237/817]	Loss 0.0036 (0.0257)	
training:	Epoch: [39][238/817]	Loss 0.0055 (0.0256)	
training:	Epoch: [39][239/817]	Loss 0.0059 (0.0255)	
training:	Epoch: [39][240/817]	Loss 0.0089 (0.0254)	
training:	Epoch: [39][241/817]	Loss 0.0051 (0.0253)	
training:	Epoch: [39][242/817]	Loss 0.0042 (0.0252)	
training:	Epoch: [39][243/817]	Loss 0.0073 (0.0252)	
training:	Epoch: [39][244/817]	Loss 0.0052 (0.0251)	
training:	Epoch: [39][245/817]	Loss 0.0045 (0.0250)	
training:	Epoch: [39][246/817]	Loss 0.0072 (0.0249)	
training:	Epoch: [39][247/817]	Loss 0.0058 (0.0249)	
training:	Epoch: [39][248/817]	Loss 0.4373 (0.0265)	
training:	Epoch: [39][249/817]	Loss 0.0072 (0.0264)	
training:	Epoch: [39][250/817]	Loss 0.0035 (0.0264)	
training:	Epoch: [39][251/817]	Loss 0.0041 (0.0263)	
training:	Epoch: [39][252/817]	Loss 0.0056 (0.0262)	
training:	Epoch: [39][253/817]	Loss 0.0064 (0.0261)	
training:	Epoch: [39][254/817]	Loss 0.0065 (0.0260)	
training:	Epoch: [39][255/817]	Loss 0.0054 (0.0259)	
training:	Epoch: [39][256/817]	Loss 0.5785 (0.0281)	
training:	Epoch: [39][257/817]	Loss 0.0086 (0.0280)	
training:	Epoch: [39][258/817]	Loss 0.0032 (0.0279)	
training:	Epoch: [39][259/817]	Loss 0.0065 (0.0278)	
training:	Epoch: [39][260/817]	Loss 0.0040 (0.0278)	
training:	Epoch: [39][261/817]	Loss 0.0336 (0.0278)	
training:	Epoch: [39][262/817]	Loss 0.0056 (0.0277)	
training:	Epoch: [39][263/817]	Loss 0.0064 (0.0276)	
training:	Epoch: [39][264/817]	Loss 0.0066 (0.0275)	
training:	Epoch: [39][265/817]	Loss 0.0274 (0.0275)	
training:	Epoch: [39][266/817]	Loss 0.0069 (0.0275)	
training:	Epoch: [39][267/817]	Loss 0.3090 (0.0285)	
training:	Epoch: [39][268/817]	Loss 0.0050 (0.0284)	
training:	Epoch: [39][269/817]	Loss 0.0036 (0.0283)	
training:	Epoch: [39][270/817]	Loss 0.0065 (0.0282)	
training:	Epoch: [39][271/817]	Loss 0.0084 (0.0282)	
training:	Epoch: [39][272/817]	Loss 0.0042 (0.0281)	
training:	Epoch: [39][273/817]	Loss 0.0909 (0.0283)	
training:	Epoch: [39][274/817]	Loss 0.0104 (0.0283)	
training:	Epoch: [39][275/817]	Loss 0.0064 (0.0282)	
training:	Epoch: [39][276/817]	Loss 0.0060 (0.0281)	
training:	Epoch: [39][277/817]	Loss 0.0037 (0.0280)	
training:	Epoch: [39][278/817]	Loss 0.0155 (0.0280)	
training:	Epoch: [39][279/817]	Loss 0.0186 (0.0279)	
training:	Epoch: [39][280/817]	Loss 0.0056 (0.0278)	
training:	Epoch: [39][281/817]	Loss 0.0058 (0.0278)	
training:	Epoch: [39][282/817]	Loss 0.0044 (0.0277)	
training:	Epoch: [39][283/817]	Loss 0.0157 (0.0276)	
training:	Epoch: [39][284/817]	Loss 0.4736 (0.0292)	
training:	Epoch: [39][285/817]	Loss 0.0076 (0.0291)	
training:	Epoch: [39][286/817]	Loss 0.0113 (0.0291)	
training:	Epoch: [39][287/817]	Loss 0.0033 (0.0290)	
training:	Epoch: [39][288/817]	Loss 0.0105 (0.0289)	
training:	Epoch: [39][289/817]	Loss 0.0086 (0.0289)	
training:	Epoch: [39][290/817]	Loss 0.3545 (0.0300)	
training:	Epoch: [39][291/817]	Loss 0.0183 (0.0299)	
training:	Epoch: [39][292/817]	Loss 0.0309 (0.0299)	
training:	Epoch: [39][293/817]	Loss 0.0059 (0.0299)	
training:	Epoch: [39][294/817]	Loss 0.0069 (0.0298)	
training:	Epoch: [39][295/817]	Loss 0.0041 (0.0297)	
training:	Epoch: [39][296/817]	Loss 0.0082 (0.0296)	
training:	Epoch: [39][297/817]	Loss 0.0057 (0.0295)	
training:	Epoch: [39][298/817]	Loss 0.0047 (0.0295)	
training:	Epoch: [39][299/817]	Loss 0.0078 (0.0294)	
training:	Epoch: [39][300/817]	Loss 0.1019 (0.0296)	
training:	Epoch: [39][301/817]	Loss 0.0033 (0.0295)	
training:	Epoch: [39][302/817]	Loss 0.0092 (0.0295)	
training:	Epoch: [39][303/817]	Loss 0.0061 (0.0294)	
training:	Epoch: [39][304/817]	Loss 0.0099 (0.0293)	
training:	Epoch: [39][305/817]	Loss 0.0054 (0.0292)	
training:	Epoch: [39][306/817]	Loss 0.0101 (0.0292)	
training:	Epoch: [39][307/817]	Loss 0.0056 (0.0291)	
training:	Epoch: [39][308/817]	Loss 0.0050 (0.0290)	
training:	Epoch: [39][309/817]	Loss 0.0171 (0.0290)	
training:	Epoch: [39][310/817]	Loss 0.0038 (0.0289)	
training:	Epoch: [39][311/817]	Loss 0.0525 (0.0290)	
training:	Epoch: [39][312/817]	Loss 0.0080 (0.0289)	
training:	Epoch: [39][313/817]	Loss 0.0040 (0.0288)	
training:	Epoch: [39][314/817]	Loss 0.0118 (0.0288)	
training:	Epoch: [39][315/817]	Loss 0.0055 (0.0287)	
training:	Epoch: [39][316/817]	Loss 0.0051 (0.0286)	
training:	Epoch: [39][317/817]	Loss 0.1641 (0.0291)	
training:	Epoch: [39][318/817]	Loss 0.0048 (0.0290)	
training:	Epoch: [39][319/817]	Loss 0.0059 (0.0289)	
training:	Epoch: [39][320/817]	Loss 0.0032 (0.0288)	
training:	Epoch: [39][321/817]	Loss 0.0076 (0.0288)	
training:	Epoch: [39][322/817]	Loss 0.0022 (0.0287)	
training:	Epoch: [39][323/817]	Loss 0.4230 (0.0299)	
training:	Epoch: [39][324/817]	Loss 0.0102 (0.0298)	
training:	Epoch: [39][325/817]	Loss 0.0169 (0.0298)	
training:	Epoch: [39][326/817]	Loss 0.0631 (0.0299)	
training:	Epoch: [39][327/817]	Loss 0.0039 (0.0298)	
training:	Epoch: [39][328/817]	Loss 0.0080 (0.0298)	
training:	Epoch: [39][329/817]	Loss 0.0097 (0.0297)	
training:	Epoch: [39][330/817]	Loss 0.0136 (0.0297)	
training:	Epoch: [39][331/817]	Loss 0.0099 (0.0296)	
training:	Epoch: [39][332/817]	Loss 0.0107 (0.0295)	
training:	Epoch: [39][333/817]	Loss 0.0043 (0.0295)	
training:	Epoch: [39][334/817]	Loss 0.0071 (0.0294)	
training:	Epoch: [39][335/817]	Loss 0.0054 (0.0293)	
training:	Epoch: [39][336/817]	Loss 0.0136 (0.0293)	
training:	Epoch: [39][337/817]	Loss 0.0186 (0.0292)	
training:	Epoch: [39][338/817]	Loss 0.0064 (0.0292)	
training:	Epoch: [39][339/817]	Loss 0.0146 (0.0291)	
training:	Epoch: [39][340/817]	Loss 0.0144 (0.0291)	
training:	Epoch: [39][341/817]	Loss 0.0118 (0.0290)	
training:	Epoch: [39][342/817]	Loss 0.0111 (0.0290)	
training:	Epoch: [39][343/817]	Loss 0.0140 (0.0289)	
training:	Epoch: [39][344/817]	Loss 0.0132 (0.0289)	
training:	Epoch: [39][345/817]	Loss 0.0374 (0.0289)	
training:	Epoch: [39][346/817]	Loss 0.0061 (0.0289)	
training:	Epoch: [39][347/817]	Loss 0.0040 (0.0288)	
training:	Epoch: [39][348/817]	Loss 0.0060 (0.0287)	
training:	Epoch: [39][349/817]	Loss 0.0064 (0.0287)	
training:	Epoch: [39][350/817]	Loss 0.0039 (0.0286)	
training:	Epoch: [39][351/817]	Loss 0.0069 (0.0285)	
training:	Epoch: [39][352/817]	Loss 0.0574 (0.0286)	
training:	Epoch: [39][353/817]	Loss 0.0030 (0.0285)	
training:	Epoch: [39][354/817]	Loss 0.0075 (0.0285)	
training:	Epoch: [39][355/817]	Loss 0.5798 (0.0300)	
training:	Epoch: [39][356/817]	Loss 0.0055 (0.0300)	
training:	Epoch: [39][357/817]	Loss 0.0041 (0.0299)	
training:	Epoch: [39][358/817]	Loss 0.0038 (0.0298)	
training:	Epoch: [39][359/817]	Loss 0.0031 (0.0297)	
training:	Epoch: [39][360/817]	Loss 0.0053 (0.0297)	
training:	Epoch: [39][361/817]	Loss 0.0137 (0.0296)	
training:	Epoch: [39][362/817]	Loss 0.0073 (0.0296)	
training:	Epoch: [39][363/817]	Loss 0.0097 (0.0295)	
training:	Epoch: [39][364/817]	Loss 0.0039 (0.0294)	
training:	Epoch: [39][365/817]	Loss 0.0041 (0.0294)	
training:	Epoch: [39][366/817]	Loss 0.0091 (0.0293)	
training:	Epoch: [39][367/817]	Loss 0.0050 (0.0292)	
training:	Epoch: [39][368/817]	Loss 0.0036 (0.0292)	
training:	Epoch: [39][369/817]	Loss 0.0048 (0.0291)	
training:	Epoch: [39][370/817]	Loss 0.0034 (0.0290)	
training:	Epoch: [39][371/817]	Loss 0.0038 (0.0290)	
training:	Epoch: [39][372/817]	Loss 0.0064 (0.0289)	
training:	Epoch: [39][373/817]	Loss 0.0043 (0.0288)	
training:	Epoch: [39][374/817]	Loss 0.0197 (0.0288)	
training:	Epoch: [39][375/817]	Loss 0.2539 (0.0294)	
training:	Epoch: [39][376/817]	Loss 0.0086 (0.0294)	
training:	Epoch: [39][377/817]	Loss 0.0041 (0.0293)	
training:	Epoch: [39][378/817]	Loss 0.0041 (0.0292)	
training:	Epoch: [39][379/817]	Loss 0.0046 (0.0292)	
training:	Epoch: [39][380/817]	Loss 0.0063 (0.0291)	
training:	Epoch: [39][381/817]	Loss 0.0037 (0.0290)	
training:	Epoch: [39][382/817]	Loss 0.0133 (0.0290)	
training:	Epoch: [39][383/817]	Loss 0.0155 (0.0290)	
training:	Epoch: [39][384/817]	Loss 0.0051 (0.0289)	
training:	Epoch: [39][385/817]	Loss 0.0124 (0.0289)	
training:	Epoch: [39][386/817]	Loss 0.0048 (0.0288)	
training:	Epoch: [39][387/817]	Loss 0.0128 (0.0288)	
training:	Epoch: [39][388/817]	Loss 0.0048 (0.0287)	
training:	Epoch: [39][389/817]	Loss 0.0035 (0.0286)	
training:	Epoch: [39][390/817]	Loss 0.0044 (0.0286)	
training:	Epoch: [39][391/817]	Loss 0.0036 (0.0285)	
training:	Epoch: [39][392/817]	Loss 0.0026 (0.0284)	
training:	Epoch: [39][393/817]	Loss 0.0182 (0.0284)	
training:	Epoch: [39][394/817]	Loss 0.0038 (0.0284)	
training:	Epoch: [39][395/817]	Loss 0.0060 (0.0283)	
training:	Epoch: [39][396/817]	Loss 0.0047 (0.0282)	
training:	Epoch: [39][397/817]	Loss 0.0038 (0.0282)	
training:	Epoch: [39][398/817]	Loss 0.0044 (0.0281)	
training:	Epoch: [39][399/817]	Loss 0.0042 (0.0281)	
training:	Epoch: [39][400/817]	Loss 0.0035 (0.0280)	
training:	Epoch: [39][401/817]	Loss 0.0049 (0.0279)	
training:	Epoch: [39][402/817]	Loss 0.0083 (0.0279)	
training:	Epoch: [39][403/817]	Loss 0.2572 (0.0285)	
training:	Epoch: [39][404/817]	Loss 0.0060 (0.0284)	
training:	Epoch: [39][405/817]	Loss 0.0040 (0.0283)	
training:	Epoch: [39][406/817]	Loss 0.0082 (0.0283)	
training:	Epoch: [39][407/817]	Loss 0.0048 (0.0282)	
training:	Epoch: [39][408/817]	Loss 0.0035 (0.0282)	
training:	Epoch: [39][409/817]	Loss 0.0056 (0.0281)	
training:	Epoch: [39][410/817]	Loss 0.0070 (0.0281)	
training:	Epoch: [39][411/817]	Loss 0.0073 (0.0280)	
training:	Epoch: [39][412/817]	Loss 0.0037 (0.0280)	
training:	Epoch: [39][413/817]	Loss 0.2433 (0.0285)	
training:	Epoch: [39][414/817]	Loss 0.0029 (0.0284)	
training:	Epoch: [39][415/817]	Loss 0.0026 (0.0284)	
training:	Epoch: [39][416/817]	Loss 0.0032 (0.0283)	
training:	Epoch: [39][417/817]	Loss 0.0039 (0.0282)	
training:	Epoch: [39][418/817]	Loss 0.0062 (0.0282)	
training:	Epoch: [39][419/817]	Loss 0.0045 (0.0281)	
training:	Epoch: [39][420/817]	Loss 0.0036 (0.0281)	
training:	Epoch: [39][421/817]	Loss 0.0276 (0.0281)	
training:	Epoch: [39][422/817]	Loss 0.0030 (0.0280)	
training:	Epoch: [39][423/817]	Loss 0.0032 (0.0279)	
training:	Epoch: [39][424/817]	Loss 0.0047 (0.0279)	
training:	Epoch: [39][425/817]	Loss 0.0028 (0.0278)	
training:	Epoch: [39][426/817]	Loss 0.0083 (0.0278)	
training:	Epoch: [39][427/817]	Loss 0.0084 (0.0277)	
training:	Epoch: [39][428/817]	Loss 0.0090 (0.0277)	
training:	Epoch: [39][429/817]	Loss 0.0032 (0.0276)	
training:	Epoch: [39][430/817]	Loss 0.0067 (0.0276)	
training:	Epoch: [39][431/817]	Loss 0.0030 (0.0275)	
training:	Epoch: [39][432/817]	Loss 0.0038 (0.0275)	
training:	Epoch: [39][433/817]	Loss 0.0065 (0.0274)	
training:	Epoch: [39][434/817]	Loss 0.0059 (0.0274)	
training:	Epoch: [39][435/817]	Loss 0.0037 (0.0273)	
training:	Epoch: [39][436/817]	Loss 0.0021 (0.0273)	
training:	Epoch: [39][437/817]	Loss 0.5232 (0.0284)	
training:	Epoch: [39][438/817]	Loss 0.0311 (0.0284)	
training:	Epoch: [39][439/817]	Loss 0.0162 (0.0284)	
training:	Epoch: [39][440/817]	Loss 0.0028 (0.0283)	
training:	Epoch: [39][441/817]	Loss 0.0146 (0.0283)	
training:	Epoch: [39][442/817]	Loss 0.0070 (0.0282)	
training:	Epoch: [39][443/817]	Loss 0.0055 (0.0282)	
training:	Epoch: [39][444/817]	Loss 0.0040 (0.0281)	
training:	Epoch: [39][445/817]	Loss 0.0123 (0.0281)	
training:	Epoch: [39][446/817]	Loss 0.0057 (0.0281)	
training:	Epoch: [39][447/817]	Loss 0.0039 (0.0280)	
training:	Epoch: [39][448/817]	Loss 0.0254 (0.0280)	
training:	Epoch: [39][449/817]	Loss 0.0035 (0.0279)	
training:	Epoch: [39][450/817]	Loss 0.0030 (0.0279)	
training:	Epoch: [39][451/817]	Loss 0.0049 (0.0278)	
training:	Epoch: [39][452/817]	Loss 0.0090 (0.0278)	
training:	Epoch: [39][453/817]	Loss 0.0177 (0.0278)	
training:	Epoch: [39][454/817]	Loss 0.0081 (0.0277)	
training:	Epoch: [39][455/817]	Loss 0.0035 (0.0277)	
training:	Epoch: [39][456/817]	Loss 0.0071 (0.0276)	
training:	Epoch: [39][457/817]	Loss 0.0121 (0.0276)	
training:	Epoch: [39][458/817]	Loss 0.0046 (0.0275)	
training:	Epoch: [39][459/817]	Loss 0.0058 (0.0275)	
training:	Epoch: [39][460/817]	Loss 0.0062 (0.0274)	
training:	Epoch: [39][461/817]	Loss 0.0069 (0.0274)	
training:	Epoch: [39][462/817]	Loss 0.0127 (0.0274)	
training:	Epoch: [39][463/817]	Loss 0.0031 (0.0273)	
training:	Epoch: [39][464/817]	Loss 0.0044 (0.0273)	
training:	Epoch: [39][465/817]	Loss 0.0086 (0.0272)	
training:	Epoch: [39][466/817]	Loss 0.0050 (0.0272)	
training:	Epoch: [39][467/817]	Loss 0.0061 (0.0271)	
training:	Epoch: [39][468/817]	Loss 0.0040 (0.0271)	
training:	Epoch: [39][469/817]	Loss 0.0064 (0.0270)	
training:	Epoch: [39][470/817]	Loss 0.0057 (0.0270)	
training:	Epoch: [39][471/817]	Loss 0.0066 (0.0270)	
training:	Epoch: [39][472/817]	Loss 0.0049 (0.0269)	
training:	Epoch: [39][473/817]	Loss 0.0069 (0.0269)	
training:	Epoch: [39][474/817]	Loss 0.0038 (0.0268)	
training:	Epoch: [39][475/817]	Loss 0.0041 (0.0268)	
training:	Epoch: [39][476/817]	Loss 0.0053 (0.0267)	
training:	Epoch: [39][477/817]	Loss 0.0040 (0.0267)	
training:	Epoch: [39][478/817]	Loss 0.0049 (0.0266)	
training:	Epoch: [39][479/817]	Loss 0.0057 (0.0266)	
training:	Epoch: [39][480/817]	Loss 0.0031 (0.0265)	
training:	Epoch: [39][481/817]	Loss 0.0505 (0.0266)	
training:	Epoch: [39][482/817]	Loss 0.0050 (0.0265)	
training:	Epoch: [39][483/817]	Loss 0.0073 (0.0265)	
training:	Epoch: [39][484/817]	Loss 0.0038 (0.0265)	
training:	Epoch: [39][485/817]	Loss 0.0065 (0.0264)	
training:	Epoch: [39][486/817]	Loss 0.0051 (0.0264)	
training:	Epoch: [39][487/817]	Loss 0.0040 (0.0263)	
training:	Epoch: [39][488/817]	Loss 0.0037 (0.0263)	
training:	Epoch: [39][489/817]	Loss 0.0031 (0.0262)	
training:	Epoch: [39][490/817]	Loss 0.0036 (0.0262)	
training:	Epoch: [39][491/817]	Loss 0.0046 (0.0261)	
training:	Epoch: [39][492/817]	Loss 0.0071 (0.0261)	
training:	Epoch: [39][493/817]	Loss 0.0033 (0.0261)	
training:	Epoch: [39][494/817]	Loss 0.0062 (0.0260)	
training:	Epoch: [39][495/817]	Loss 0.0043 (0.0260)	
training:	Epoch: [39][496/817]	Loss 0.0090 (0.0259)	
training:	Epoch: [39][497/817]	Loss 0.0308 (0.0259)	
training:	Epoch: [39][498/817]	Loss 0.0350 (0.0260)	
training:	Epoch: [39][499/817]	Loss 0.0061 (0.0259)	
training:	Epoch: [39][500/817]	Loss 0.0045 (0.0259)	
training:	Epoch: [39][501/817]	Loss 0.0030 (0.0258)	
training:	Epoch: [39][502/817]	Loss 0.0033 (0.0258)	
training:	Epoch: [39][503/817]	Loss 0.0039 (0.0257)	
training:	Epoch: [39][504/817]	Loss 0.0031 (0.0257)	
training:	Epoch: [39][505/817]	Loss 0.0044 (0.0257)	
training:	Epoch: [39][506/817]	Loss 0.0035 (0.0256)	
training:	Epoch: [39][507/817]	Loss 0.0081 (0.0256)	
training:	Epoch: [39][508/817]	Loss 0.0034 (0.0255)	
training:	Epoch: [39][509/817]	Loss 0.0062 (0.0255)	
training:	Epoch: [39][510/817]	Loss 0.0034 (0.0255)	
training:	Epoch: [39][511/817]	Loss 0.0034 (0.0254)	
training:	Epoch: [39][512/817]	Loss 0.0043 (0.0254)	
training:	Epoch: [39][513/817]	Loss 0.0115 (0.0253)	
training:	Epoch: [39][514/817]	Loss 0.0060 (0.0253)	
training:	Epoch: [39][515/817]	Loss 0.0038 (0.0253)	
training:	Epoch: [39][516/817]	Loss 0.0043 (0.0252)	
training:	Epoch: [39][517/817]	Loss 0.0055 (0.0252)	
training:	Epoch: [39][518/817]	Loss 0.0031 (0.0251)	
training:	Epoch: [39][519/817]	Loss 0.0036 (0.0251)	
training:	Epoch: [39][520/817]	Loss 0.0160 (0.0251)	
training:	Epoch: [39][521/817]	Loss 0.0040 (0.0250)	
training:	Epoch: [39][522/817]	Loss 0.0091 (0.0250)	
training:	Epoch: [39][523/817]	Loss 0.0050 (0.0250)	
training:	Epoch: [39][524/817]	Loss 0.0045 (0.0249)	
training:	Epoch: [39][525/817]	Loss 0.0036 (0.0249)	
training:	Epoch: [39][526/817]	Loss 0.0055 (0.0249)	
training:	Epoch: [39][527/817]	Loss 0.0029 (0.0248)	
training:	Epoch: [39][528/817]	Loss 0.0039 (0.0248)	
training:	Epoch: [39][529/817]	Loss 0.0051 (0.0247)	
training:	Epoch: [39][530/817]	Loss 0.0091 (0.0247)	
training:	Epoch: [39][531/817]	Loss 0.0031 (0.0247)	
training:	Epoch: [39][532/817]	Loss 0.0043 (0.0246)	
training:	Epoch: [39][533/817]	Loss 0.0035 (0.0246)	
training:	Epoch: [39][534/817]	Loss 0.0024 (0.0246)	
training:	Epoch: [39][535/817]	Loss 0.1205 (0.0247)	
training:	Epoch: [39][536/817]	Loss 0.0025 (0.0247)	
training:	Epoch: [39][537/817]	Loss 0.0028 (0.0246)	
training:	Epoch: [39][538/817]	Loss 0.0061 (0.0246)	
training:	Epoch: [39][539/817]	Loss 0.0033 (0.0246)	
training:	Epoch: [39][540/817]	Loss 0.0034 (0.0245)	
training:	Epoch: [39][541/817]	Loss 0.0073 (0.0245)	
training:	Epoch: [39][542/817]	Loss 0.0061 (0.0245)	
training:	Epoch: [39][543/817]	Loss 0.0033 (0.0244)	
training:	Epoch: [39][544/817]	Loss 0.0057 (0.0244)	
training:	Epoch: [39][545/817]	Loss 0.0030 (0.0244)	
training:	Epoch: [39][546/817]	Loss 0.0039 (0.0243)	
training:	Epoch: [39][547/817]	Loss 0.0070 (0.0243)	
training:	Epoch: [39][548/817]	Loss 0.0937 (0.0244)	
training:	Epoch: [39][549/817]	Loss 0.0033 (0.0244)	
training:	Epoch: [39][550/817]	Loss 0.0042 (0.0243)	
training:	Epoch: [39][551/817]	Loss 0.0045 (0.0243)	
training:	Epoch: [39][552/817]	Loss 0.0036 (0.0243)	
training:	Epoch: [39][553/817]	Loss 0.0033 (0.0242)	
training:	Epoch: [39][554/817]	Loss 0.0044 (0.0242)	
training:	Epoch: [39][555/817]	Loss 0.0037 (0.0242)	
training:	Epoch: [39][556/817]	Loss 0.0038 (0.0241)	
training:	Epoch: [39][557/817]	Loss 0.0047 (0.0241)	
training:	Epoch: [39][558/817]	Loss 0.0040 (0.0240)	
training:	Epoch: [39][559/817]	Loss 0.0032 (0.0240)	
training:	Epoch: [39][560/817]	Loss 0.0113 (0.0240)	
training:	Epoch: [39][561/817]	Loss 0.0066 (0.0240)	
training:	Epoch: [39][562/817]	Loss 0.0080 (0.0239)	
training:	Epoch: [39][563/817]	Loss 0.0026 (0.0239)	
training:	Epoch: [39][564/817]	Loss 0.0051 (0.0239)	
training:	Epoch: [39][565/817]	Loss 0.0033 (0.0238)	
training:	Epoch: [39][566/817]	Loss 0.0029 (0.0238)	
training:	Epoch: [39][567/817]	Loss 0.0034 (0.0237)	
training:	Epoch: [39][568/817]	Loss 0.0027 (0.0237)	
training:	Epoch: [39][569/817]	Loss 0.0046 (0.0237)	
training:	Epoch: [39][570/817]	Loss 0.0030 (0.0236)	
training:	Epoch: [39][571/817]	Loss 0.0153 (0.0236)	
training:	Epoch: [39][572/817]	Loss 0.0035 (0.0236)	
training:	Epoch: [39][573/817]	Loss 0.0036 (0.0236)	
training:	Epoch: [39][574/817]	Loss 0.0025 (0.0235)	
training:	Epoch: [39][575/817]	Loss 0.0094 (0.0235)	
training:	Epoch: [39][576/817]	Loss 0.2417 (0.0239)	
training:	Epoch: [39][577/817]	Loss 0.1137 (0.0240)	
training:	Epoch: [39][578/817]	Loss 0.0022 (0.0240)	
training:	Epoch: [39][579/817]	Loss 0.0031 (0.0240)	
training:	Epoch: [39][580/817]	Loss 0.0030 (0.0239)	
training:	Epoch: [39][581/817]	Loss 0.0027 (0.0239)	
training:	Epoch: [39][582/817]	Loss 0.0038 (0.0238)	
training:	Epoch: [39][583/817]	Loss 0.0032 (0.0238)	
training:	Epoch: [39][584/817]	Loss 0.0028 (0.0238)	
training:	Epoch: [39][585/817]	Loss 0.0037 (0.0237)	
training:	Epoch: [39][586/817]	Loss 0.0038 (0.0237)	
training:	Epoch: [39][587/817]	Loss 0.0031 (0.0237)	
training:	Epoch: [39][588/817]	Loss 0.0061 (0.0236)	
training:	Epoch: [39][589/817]	Loss 0.0052 (0.0236)	
training:	Epoch: [39][590/817]	Loss 0.0029 (0.0236)	
training:	Epoch: [39][591/817]	Loss 0.0078 (0.0236)	
training:	Epoch: [39][592/817]	Loss 0.0059 (0.0235)	
training:	Epoch: [39][593/817]	Loss 0.0034 (0.0235)	
training:	Epoch: [39][594/817]	Loss 0.3356 (0.0240)	
training:	Epoch: [39][595/817]	Loss 0.0029 (0.0240)	
training:	Epoch: [39][596/817]	Loss 0.0040 (0.0239)	
training:	Epoch: [39][597/817]	Loss 0.0030 (0.0239)	
training:	Epoch: [39][598/817]	Loss 0.0027 (0.0239)	
training:	Epoch: [39][599/817]	Loss 0.0033 (0.0238)	
training:	Epoch: [39][600/817]	Loss 0.0032 (0.0238)	
training:	Epoch: [39][601/817]	Loss 0.0119 (0.0238)	
training:	Epoch: [39][602/817]	Loss 0.4347 (0.0245)	
training:	Epoch: [39][603/817]	Loss 0.0028 (0.0244)	
training:	Epoch: [39][604/817]	Loss 0.0097 (0.0244)	
training:	Epoch: [39][605/817]	Loss 0.0028 (0.0244)	
training:	Epoch: [39][606/817]	Loss 0.0028 (0.0243)	
training:	Epoch: [39][607/817]	Loss 0.0115 (0.0243)	
training:	Epoch: [39][608/817]	Loss 0.0047 (0.0243)	
training:	Epoch: [39][609/817]	Loss 0.0028 (0.0242)	
training:	Epoch: [39][610/817]	Loss 0.0106 (0.0242)	
training:	Epoch: [39][611/817]	Loss 0.0034 (0.0242)	
training:	Epoch: [39][612/817]	Loss 0.0055 (0.0242)	
training:	Epoch: [39][613/817]	Loss 0.0584 (0.0242)	
training:	Epoch: [39][614/817]	Loss 0.0081 (0.0242)	
training:	Epoch: [39][615/817]	Loss 0.0050 (0.0242)	
training:	Epoch: [39][616/817]	Loss 0.0074 (0.0241)	
training:	Epoch: [39][617/817]	Loss 0.0026 (0.0241)	
training:	Epoch: [39][618/817]	Loss 0.0060 (0.0241)	
training:	Epoch: [39][619/817]	Loss 0.0026 (0.0240)	
training:	Epoch: [39][620/817]	Loss 0.0053 (0.0240)	
training:	Epoch: [39][621/817]	Loss 0.0026 (0.0240)	
training:	Epoch: [39][622/817]	Loss 0.0028 (0.0239)	
training:	Epoch: [39][623/817]	Loss 0.0044 (0.0239)	
training:	Epoch: [39][624/817]	Loss 0.0029 (0.0239)	
training:	Epoch: [39][625/817]	Loss 0.3411 (0.0244)	
training:	Epoch: [39][626/817]	Loss 0.0043 (0.0243)	
training:	Epoch: [39][627/817]	Loss 0.1324 (0.0245)	
training:	Epoch: [39][628/817]	Loss 0.0037 (0.0245)	
training:	Epoch: [39][629/817]	Loss 0.0023 (0.0244)	
training:	Epoch: [39][630/817]	Loss 0.0026 (0.0244)	
training:	Epoch: [39][631/817]	Loss 0.5116 (0.0252)	
training:	Epoch: [39][632/817]	Loss 0.0031 (0.0252)	
training:	Epoch: [39][633/817]	Loss 0.0032 (0.0251)	
training:	Epoch: [39][634/817]	Loss 0.0039 (0.0251)	
training:	Epoch: [39][635/817]	Loss 0.0039 (0.0250)	
training:	Epoch: [39][636/817]	Loss 0.0048 (0.0250)	
training:	Epoch: [39][637/817]	Loss 0.0204 (0.0250)	
training:	Epoch: [39][638/817]	Loss 0.0146 (0.0250)	
training:	Epoch: [39][639/817]	Loss 0.0035 (0.0250)	
training:	Epoch: [39][640/817]	Loss 0.0119 (0.0249)	
training:	Epoch: [39][641/817]	Loss 0.0038 (0.0249)	
training:	Epoch: [39][642/817]	Loss 0.0026 (0.0249)	
training:	Epoch: [39][643/817]	Loss 0.6070 (0.0258)	
training:	Epoch: [39][644/817]	Loss 0.0033 (0.0257)	
training:	Epoch: [39][645/817]	Loss 0.0041 (0.0257)	
training:	Epoch: [39][646/817]	Loss 0.3688 (0.0262)	
training:	Epoch: [39][647/817]	Loss 0.0044 (0.0262)	
training:	Epoch: [39][648/817]	Loss 0.0050 (0.0262)	
training:	Epoch: [39][649/817]	Loss 0.0068 (0.0261)	
training:	Epoch: [39][650/817]	Loss 0.0053 (0.0261)	
training:	Epoch: [39][651/817]	Loss 0.1278 (0.0263)	
training:	Epoch: [39][652/817]	Loss 0.0061 (0.0262)	
training:	Epoch: [39][653/817]	Loss 0.0033 (0.0262)	
training:	Epoch: [39][654/817]	Loss 0.5416 (0.0270)	
training:	Epoch: [39][655/817]	Loss 0.4071 (0.0276)	
training:	Epoch: [39][656/817]	Loss 0.0055 (0.0275)	
training:	Epoch: [39][657/817]	Loss 0.0157 (0.0275)	
training:	Epoch: [39][658/817]	Loss 0.0110 (0.0275)	
training:	Epoch: [39][659/817]	Loss 0.0071 (0.0275)	
training:	Epoch: [39][660/817]	Loss 0.0039 (0.0274)	
training:	Epoch: [39][661/817]	Loss 0.4732 (0.0281)	
training:	Epoch: [39][662/817]	Loss 0.4561 (0.0287)	
training:	Epoch: [39][663/817]	Loss 0.0078 (0.0287)	
training:	Epoch: [39][664/817]	Loss 0.6631 (0.0297)	
training:	Epoch: [39][665/817]	Loss 0.0089 (0.0296)	
training:	Epoch: [39][666/817]	Loss 0.0045 (0.0296)	
training:	Epoch: [39][667/817]	Loss 0.0076 (0.0296)	
training:	Epoch: [39][668/817]	Loss 0.0082 (0.0295)	
training:	Epoch: [39][669/817]	Loss 0.0046 (0.0295)	
training:	Epoch: [39][670/817]	Loss 0.0539 (0.0295)	
training:	Epoch: [39][671/817]	Loss 0.0035 (0.0295)	
training:	Epoch: [39][672/817]	Loss 0.0083 (0.0295)	
training:	Epoch: [39][673/817]	Loss 0.0190 (0.0295)	
training:	Epoch: [39][674/817]	Loss 0.0197 (0.0294)	
training:	Epoch: [39][675/817]	Loss 0.0102 (0.0294)	
training:	Epoch: [39][676/817]	Loss 0.0093 (0.0294)	
training:	Epoch: [39][677/817]	Loss 0.0071 (0.0293)	
training:	Epoch: [39][678/817]	Loss 0.0675 (0.0294)	
training:	Epoch: [39][679/817]	Loss 0.0179 (0.0294)	
training:	Epoch: [39][680/817]	Loss 0.0343 (0.0294)	
training:	Epoch: [39][681/817]	Loss 0.0059 (0.0294)	
training:	Epoch: [39][682/817]	Loss 0.0110 (0.0293)	
training:	Epoch: [39][683/817]	Loss 0.0116 (0.0293)	
training:	Epoch: [39][684/817]	Loss 0.0175 (0.0293)	
training:	Epoch: [39][685/817]	Loss 0.0058 (0.0293)	
training:	Epoch: [39][686/817]	Loss 0.0393 (0.0293)	
training:	Epoch: [39][687/817]	Loss 0.0051 (0.0292)	
training:	Epoch: [39][688/817]	Loss 0.0036 (0.0292)	
training:	Epoch: [39][689/817]	Loss 0.3314 (0.0296)	
training:	Epoch: [39][690/817]	Loss 0.0496 (0.0297)	
training:	Epoch: [39][691/817]	Loss 0.0073 (0.0296)	
training:	Epoch: [39][692/817]	Loss 0.0090 (0.0296)	
training:	Epoch: [39][693/817]	Loss 0.0059 (0.0296)	
training:	Epoch: [39][694/817]	Loss 0.0065 (0.0295)	
training:	Epoch: [39][695/817]	Loss 0.0027 (0.0295)	
training:	Epoch: [39][696/817]	Loss 0.0240 (0.0295)	
training:	Epoch: [39][697/817]	Loss 0.0245 (0.0295)	
training:	Epoch: [39][698/817]	Loss 0.0053 (0.0294)	
training:	Epoch: [39][699/817]	Loss 0.0057 (0.0294)	
training:	Epoch: [39][700/817]	Loss 0.0072 (0.0294)	
training:	Epoch: [39][701/817]	Loss 0.0031 (0.0293)	
training:	Epoch: [39][702/817]	Loss 0.0042 (0.0293)	
training:	Epoch: [39][703/817]	Loss 0.0028 (0.0293)	
training:	Epoch: [39][704/817]	Loss 0.0073 (0.0292)	
training:	Epoch: [39][705/817]	Loss 0.0084 (0.0292)	
training:	Epoch: [39][706/817]	Loss 0.0100 (0.0292)	
training:	Epoch: [39][707/817]	Loss 0.0124 (0.0292)	
training:	Epoch: [39][708/817]	Loss 0.0108 (0.0291)	
training:	Epoch: [39][709/817]	Loss 0.0081 (0.0291)	
training:	Epoch: [39][710/817]	Loss 0.0048 (0.0291)	
training:	Epoch: [39][711/817]	Loss 0.0077 (0.0290)	
training:	Epoch: [39][712/817]	Loss 0.0038 (0.0290)	
training:	Epoch: [39][713/817]	Loss 0.0054 (0.0290)	
training:	Epoch: [39][714/817]	Loss 0.0676 (0.0290)	
training:	Epoch: [39][715/817]	Loss 0.0060 (0.0290)	
training:	Epoch: [39][716/817]	Loss 0.3332 (0.0294)	
training:	Epoch: [39][717/817]	Loss 0.0101 (0.0294)	
training:	Epoch: [39][718/817]	Loss 0.0031 (0.0294)	
training:	Epoch: [39][719/817]	Loss 0.0136 (0.0293)	
training:	Epoch: [39][720/817]	Loss 0.0059 (0.0293)	
training:	Epoch: [39][721/817]	Loss 0.0079 (0.0293)	
training:	Epoch: [39][722/817]	Loss 0.0087 (0.0292)	
training:	Epoch: [39][723/817]	Loss 0.0033 (0.0292)	
training:	Epoch: [39][724/817]	Loss 0.0878 (0.0293)	
training:	Epoch: [39][725/817]	Loss 0.0091 (0.0293)	
training:	Epoch: [39][726/817]	Loss 0.0037 (0.0292)	
training:	Epoch: [39][727/817]	Loss 0.0115 (0.0292)	
training:	Epoch: [39][728/817]	Loss 0.0074 (0.0292)	
training:	Epoch: [39][729/817]	Loss 0.0046 (0.0291)	
training:	Epoch: [39][730/817]	Loss 0.0038 (0.0291)	
training:	Epoch: [39][731/817]	Loss 0.0145 (0.0291)	
training:	Epoch: [39][732/817]	Loss 0.0071 (0.0290)	
training:	Epoch: [39][733/817]	Loss 0.0113 (0.0290)	
training:	Epoch: [39][734/817]	Loss 0.0165 (0.0290)	
training:	Epoch: [39][735/817]	Loss 0.0109 (0.0290)	
training:	Epoch: [39][736/817]	Loss 0.0057 (0.0290)	
training:	Epoch: [39][737/817]	Loss 0.0074 (0.0289)	
training:	Epoch: [39][738/817]	Loss 0.0133 (0.0289)	
training:	Epoch: [39][739/817]	Loss 0.0065 (0.0289)	
training:	Epoch: [39][740/817]	Loss 0.0037 (0.0288)	
training:	Epoch: [39][741/817]	Loss 0.0114 (0.0288)	
training:	Epoch: [39][742/817]	Loss 0.0108 (0.0288)	
training:	Epoch: [39][743/817]	Loss 0.0038 (0.0288)	
training:	Epoch: [39][744/817]	Loss 0.0070 (0.0287)	
training:	Epoch: [39][745/817]	Loss 0.0070 (0.0287)	
training:	Epoch: [39][746/817]	Loss 0.0049 (0.0287)	
training:	Epoch: [39][747/817]	Loss 0.0075 (0.0286)	
training:	Epoch: [39][748/817]	Loss 0.0041 (0.0286)	
training:	Epoch: [39][749/817]	Loss 0.6856 (0.0295)	
training:	Epoch: [39][750/817]	Loss 0.0076 (0.0295)	
training:	Epoch: [39][751/817]	Loss 0.0350 (0.0295)	
training:	Epoch: [39][752/817]	Loss 0.0083 (0.0294)	
training:	Epoch: [39][753/817]	Loss 0.0158 (0.0294)	
training:	Epoch: [39][754/817]	Loss 0.0301 (0.0294)	
training:	Epoch: [39][755/817]	Loss 0.0032 (0.0294)	
training:	Epoch: [39][756/817]	Loss 0.0053 (0.0293)	
training:	Epoch: [39][757/817]	Loss 0.0035 (0.0293)	
training:	Epoch: [39][758/817]	Loss 0.0041 (0.0293)	
training:	Epoch: [39][759/817]	Loss 0.0055 (0.0292)	
training:	Epoch: [39][760/817]	Loss 0.0037 (0.0292)	
training:	Epoch: [39][761/817]	Loss 0.0093 (0.0292)	
training:	Epoch: [39][762/817]	Loss 0.0063 (0.0292)	
training:	Epoch: [39][763/817]	Loss 0.5309 (0.0298)	
training:	Epoch: [39][764/817]	Loss 0.0054 (0.0298)	
training:	Epoch: [39][765/817]	Loss 0.0032 (0.0297)	
training:	Epoch: [39][766/817]	Loss 0.0034 (0.0297)	
training:	Epoch: [39][767/817]	Loss 0.0120 (0.0297)	
training:	Epoch: [39][768/817]	Loss 0.0064 (0.0297)	
training:	Epoch: [39][769/817]	Loss 0.0059 (0.0296)	
training:	Epoch: [39][770/817]	Loss 0.0082 (0.0296)	
training:	Epoch: [39][771/817]	Loss 0.0059 (0.0296)	
training:	Epoch: [39][772/817]	Loss 0.0075 (0.0295)	
training:	Epoch: [39][773/817]	Loss 0.0056 (0.0295)	
training:	Epoch: [39][774/817]	Loss 0.0036 (0.0295)	
training:	Epoch: [39][775/817]	Loss 0.0068 (0.0294)	
training:	Epoch: [39][776/817]	Loss 0.0246 (0.0294)	
training:	Epoch: [39][777/817]	Loss 0.0072 (0.0294)	
training:	Epoch: [39][778/817]	Loss 0.0112 (0.0294)	
training:	Epoch: [39][779/817]	Loss 0.0048 (0.0294)	
training:	Epoch: [39][780/817]	Loss 0.0249 (0.0294)	
training:	Epoch: [39][781/817]	Loss 0.0057 (0.0293)	
training:	Epoch: [39][782/817]	Loss 0.0167 (0.0293)	
training:	Epoch: [39][783/817]	Loss 0.0054 (0.0293)	
training:	Epoch: [39][784/817]	Loss 0.0070 (0.0292)	
training:	Epoch: [39][785/817]	Loss 0.0098 (0.0292)	
training:	Epoch: [39][786/817]	Loss 0.0047 (0.0292)	
training:	Epoch: [39][787/817]	Loss 0.0027 (0.0292)	
training:	Epoch: [39][788/817]	Loss 0.0153 (0.0291)	
training:	Epoch: [39][789/817]	Loss 0.0038 (0.0291)	
training:	Epoch: [39][790/817]	Loss 0.0026 (0.0291)	
training:	Epoch: [39][791/817]	Loss 0.0044 (0.0290)	
training:	Epoch: [39][792/817]	Loss 0.0031 (0.0290)	
training:	Epoch: [39][793/817]	Loss 0.2535 (0.0293)	
training:	Epoch: [39][794/817]	Loss 0.0107 (0.0293)	
training:	Epoch: [39][795/817]	Loss 0.0074 (0.0292)	
training:	Epoch: [39][796/817]	Loss 0.0030 (0.0292)	
training:	Epoch: [39][797/817]	Loss 0.0052 (0.0292)	
training:	Epoch: [39][798/817]	Loss 0.3910 (0.0296)	
training:	Epoch: [39][799/817]	Loss 0.0057 (0.0296)	
training:	Epoch: [39][800/817]	Loss 0.0061 (0.0296)	
training:	Epoch: [39][801/817]	Loss 0.0037 (0.0295)	
training:	Epoch: [39][802/817]	Loss 0.0096 (0.0295)	
training:	Epoch: [39][803/817]	Loss 0.6316 (0.0303)	
training:	Epoch: [39][804/817]	Loss 0.0060 (0.0302)	
training:	Epoch: [39][805/817]	Loss 0.0085 (0.0302)	
training:	Epoch: [39][806/817]	Loss 0.0082 (0.0302)	
training:	Epoch: [39][807/817]	Loss 0.0430 (0.0302)	
training:	Epoch: [39][808/817]	Loss 0.1014 (0.0303)	
training:	Epoch: [39][809/817]	Loss 0.0059 (0.0303)	
training:	Epoch: [39][810/817]	Loss 0.0048 (0.0302)	
training:	Epoch: [39][811/817]	Loss 0.0056 (0.0302)	
training:	Epoch: [39][812/817]	Loss 0.0120 (0.0302)	
training:	Epoch: [39][813/817]	Loss 0.0057 (0.0301)	
training:	Epoch: [39][814/817]	Loss 0.0119 (0.0301)	
training:	Epoch: [39][815/817]	Loss 0.0055 (0.0301)	
training:	Epoch: [39][816/817]	Loss 0.0056 (0.0301)	
training:	Epoch: [39][817/817]	Loss 0.0057 (0.0300)	
Training:	 Loss: 0.0300

Training:	 ACC: 0.9924 0.9925 0.9959 0.9888
Validation:	 ACC: 0.7813 0.7838 0.8373 0.7253
Validation:	 Best_BACC: 0.8223 0.8218 0.8127 0.8318
Validation:	 Loss: 1.0242
Pretraining:	Epoch 40/200
----------
training:	Epoch: [40][1/817]	Loss 0.0053 (0.0053)	
training:	Epoch: [40][2/817]	Loss 0.0047 (0.0050)	
training:	Epoch: [40][3/817]	Loss 0.0071 (0.0057)	
training:	Epoch: [40][4/817]	Loss 0.0066 (0.0059)	
training:	Epoch: [40][5/817]	Loss 0.0071 (0.0062)	
training:	Epoch: [40][6/817]	Loss 0.0098 (0.0068)	
training:	Epoch: [40][7/817]	Loss 0.0029 (0.0062)	
training:	Epoch: [40][8/817]	Loss 0.0026 (0.0058)	
training:	Epoch: [40][9/817]	Loss 0.0087 (0.0061)	
training:	Epoch: [40][10/817]	Loss 0.0038 (0.0059)	
training:	Epoch: [40][11/817]	Loss 0.0047 (0.0058)	
training:	Epoch: [40][12/817]	Loss 0.0049 (0.0057)	
training:	Epoch: [40][13/817]	Loss 0.0096 (0.0060)	
training:	Epoch: [40][14/817]	Loss 0.0093 (0.0062)	
training:	Epoch: [40][15/817]	Loss 0.0086 (0.0064)	
training:	Epoch: [40][16/817]	Loss 0.0064 (0.0064)	
training:	Epoch: [40][17/817]	Loss 0.0042 (0.0063)	
training:	Epoch: [40][18/817]	Loss 0.0165 (0.0068)	
training:	Epoch: [40][19/817]	Loss 0.0044 (0.0067)	
training:	Epoch: [40][20/817]	Loss 0.0032 (0.0065)	
training:	Epoch: [40][21/817]	Loss 0.0054 (0.0065)	
training:	Epoch: [40][22/817]	Loss 0.0178 (0.0070)	
training:	Epoch: [40][23/817]	Loss 0.0064 (0.0070)	
training:	Epoch: [40][24/817]	Loss 0.0030 (0.0068)	
training:	Epoch: [40][25/817]	Loss 0.0071 (0.0068)	
training:	Epoch: [40][26/817]	Loss 0.0022 (0.0066)	
training:	Epoch: [40][27/817]	Loss 0.0098 (0.0068)	
training:	Epoch: [40][28/817]	Loss 0.0028 (0.0066)	
training:	Epoch: [40][29/817]	Loss 0.0045 (0.0065)	
training:	Epoch: [40][30/817]	Loss 0.0061 (0.0065)	
training:	Epoch: [40][31/817]	Loss 0.0068 (0.0065)	
training:	Epoch: [40][32/817]	Loss 0.0117 (0.0067)	
training:	Epoch: [40][33/817]	Loss 0.0040 (0.0066)	
training:	Epoch: [40][34/817]	Loss 0.0047 (0.0066)	
training:	Epoch: [40][35/817]	Loss 0.0077 (0.0066)	
training:	Epoch: [40][36/817]	Loss 0.0027 (0.0065)	
training:	Epoch: [40][37/817]	Loss 0.0072 (0.0065)	
training:	Epoch: [40][38/817]	Loss 0.0027 (0.0064)	
training:	Epoch: [40][39/817]	Loss 0.0050 (0.0064)	
training:	Epoch: [40][40/817]	Loss 0.0085 (0.0064)	
training:	Epoch: [40][41/817]	Loss 0.0092 (0.0065)	
training:	Epoch: [40][42/817]	Loss 0.0048 (0.0064)	
training:	Epoch: [40][43/817]	Loss 0.0054 (0.0064)	
training:	Epoch: [40][44/817]	Loss 0.0112 (0.0065)	
training:	Epoch: [40][45/817]	Loss 0.0038 (0.0065)	
training:	Epoch: [40][46/817]	Loss 0.0058 (0.0065)	
training:	Epoch: [40][47/817]	Loss 0.0038 (0.0064)	
training:	Epoch: [40][48/817]	Loss 0.0023 (0.0063)	
training:	Epoch: [40][49/817]	Loss 0.0049 (0.0063)	
training:	Epoch: [40][50/817]	Loss 0.0026 (0.0062)	
training:	Epoch: [40][51/817]	Loss 0.0157 (0.0064)	
training:	Epoch: [40][52/817]	Loss 0.0071 (0.0064)	
training:	Epoch: [40][53/817]	Loss 0.0068 (0.0064)	
training:	Epoch: [40][54/817]	Loss 0.0042 (0.0064)	
training:	Epoch: [40][55/817]	Loss 0.0029 (0.0063)	
training:	Epoch: [40][56/817]	Loss 0.0039 (0.0063)	
training:	Epoch: [40][57/817]	Loss 0.0072 (0.0063)	
training:	Epoch: [40][58/817]	Loss 0.0045 (0.0063)	
training:	Epoch: [40][59/817]	Loss 0.0026 (0.0062)	
training:	Epoch: [40][60/817]	Loss 0.0052 (0.0062)	
training:	Epoch: [40][61/817]	Loss 0.0080 (0.0062)	
training:	Epoch: [40][62/817]	Loss 0.0031 (0.0062)	
training:	Epoch: [40][63/817]	Loss 0.0046 (0.0061)	
training:	Epoch: [40][64/817]	Loss 0.0034 (0.0061)	
training:	Epoch: [40][65/817]	Loss 0.0926 (0.0074)	
training:	Epoch: [40][66/817]	Loss 0.0062 (0.0074)	
training:	Epoch: [40][67/817]	Loss 0.4700 (0.0143)	
training:	Epoch: [40][68/817]	Loss 0.0046 (0.0142)	
training:	Epoch: [40][69/817]	Loss 0.0038 (0.0140)	
training:	Epoch: [40][70/817]	Loss 0.0025 (0.0138)	
training:	Epoch: [40][71/817]	Loss 0.4993 (0.0207)	
training:	Epoch: [40][72/817]	Loss 0.2028 (0.0232)	
training:	Epoch: [40][73/817]	Loss 0.0056 (0.0230)	
training:	Epoch: [40][74/817]	Loss 0.0075 (0.0228)	
training:	Epoch: [40][75/817]	Loss 0.0048 (0.0225)	
training:	Epoch: [40][76/817]	Loss 0.0096 (0.0224)	
training:	Epoch: [40][77/817]	Loss 0.0034 (0.0221)	
training:	Epoch: [40][78/817]	Loss 0.0043 (0.0219)	
training:	Epoch: [40][79/817]	Loss 0.0039 (0.0217)	
training:	Epoch: [40][80/817]	Loss 0.0072 (0.0215)	
training:	Epoch: [40][81/817]	Loss 0.0053 (0.0213)	
training:	Epoch: [40][82/817]	Loss 0.0100 (0.0211)	
training:	Epoch: [40][83/817]	Loss 0.0049 (0.0209)	
training:	Epoch: [40][84/817]	Loss 0.0041 (0.0207)	
training:	Epoch: [40][85/817]	Loss 0.0136 (0.0207)	
training:	Epoch: [40][86/817]	Loss 0.0042 (0.0205)	
training:	Epoch: [40][87/817]	Loss 0.0025 (0.0203)	
training:	Epoch: [40][88/817]	Loss 0.4108 (0.0247)	
training:	Epoch: [40][89/817]	Loss 0.0103 (0.0245)	
training:	Epoch: [40][90/817]	Loss 0.0049 (0.0243)	
training:	Epoch: [40][91/817]	Loss 0.0059 (0.0241)	
training:	Epoch: [40][92/817]	Loss 0.0051 (0.0239)	
training:	Epoch: [40][93/817]	Loss 0.0055 (0.0237)	
training:	Epoch: [40][94/817]	Loss 0.0080 (0.0235)	
training:	Epoch: [40][95/817]	Loss 0.4391 (0.0279)	
training:	Epoch: [40][96/817]	Loss 0.0033 (0.0277)	
training:	Epoch: [40][97/817]	Loss 0.0031 (0.0274)	
training:	Epoch: [40][98/817]	Loss 0.0052 (0.0272)	
training:	Epoch: [40][99/817]	Loss 0.0038 (0.0269)	
training:	Epoch: [40][100/817]	Loss 0.0044 (0.0267)	
training:	Epoch: [40][101/817]	Loss 0.3915 (0.0303)	
training:	Epoch: [40][102/817]	Loss 0.0025 (0.0301)	
training:	Epoch: [40][103/817]	Loss 0.0060 (0.0298)	
training:	Epoch: [40][104/817]	Loss 0.0185 (0.0297)	
training:	Epoch: [40][105/817]	Loss 0.0079 (0.0295)	
training:	Epoch: [40][106/817]	Loss 0.0080 (0.0293)	
training:	Epoch: [40][107/817]	Loss 0.0117 (0.0291)	
training:	Epoch: [40][108/817]	Loss 0.0041 (0.0289)	
training:	Epoch: [40][109/817]	Loss 0.0078 (0.0287)	
training:	Epoch: [40][110/817]	Loss 0.0050 (0.0285)	
training:	Epoch: [40][111/817]	Loss 0.0046 (0.0283)	
training:	Epoch: [40][112/817]	Loss 0.0071 (0.0281)	
training:	Epoch: [40][113/817]	Loss 0.0084 (0.0279)	
training:	Epoch: [40][114/817]	Loss 0.0027 (0.0277)	
training:	Epoch: [40][115/817]	Loss 0.1157 (0.0285)	
training:	Epoch: [40][116/817]	Loss 0.0033 (0.0282)	
training:	Epoch: [40][117/817]	Loss 0.2855 (0.0304)	
training:	Epoch: [40][118/817]	Loss 0.0038 (0.0302)	
training:	Epoch: [40][119/817]	Loss 0.0027 (0.0300)	
training:	Epoch: [40][120/817]	Loss 0.0128 (0.0298)	
training:	Epoch: [40][121/817]	Loss 0.0203 (0.0298)	
training:	Epoch: [40][122/817]	Loss 0.0092 (0.0296)	
training:	Epoch: [40][123/817]	Loss 0.0116 (0.0295)	
training:	Epoch: [40][124/817]	Loss 0.0174 (0.0294)	
training:	Epoch: [40][125/817]	Loss 0.0129 (0.0292)	
training:	Epoch: [40][126/817]	Loss 0.0075 (0.0291)	
training:	Epoch: [40][127/817]	Loss 0.0063 (0.0289)	
training:	Epoch: [40][128/817]	Loss 0.0069 (0.0287)	
training:	Epoch: [40][129/817]	Loss 0.1920 (0.0300)	
training:	Epoch: [40][130/817]	Loss 0.0097 (0.0298)	
training:	Epoch: [40][131/817]	Loss 0.0034 (0.0296)	
training:	Epoch: [40][132/817]	Loss 0.0474 (0.0297)	
training:	Epoch: [40][133/817]	Loss 0.0032 (0.0295)	
training:	Epoch: [40][134/817]	Loss 0.0043 (0.0294)	
training:	Epoch: [40][135/817]	Loss 0.0050 (0.0292)	
training:	Epoch: [40][136/817]	Loss 0.0060 (0.0290)	
training:	Epoch: [40][137/817]	Loss 0.3472 (0.0313)	
training:	Epoch: [40][138/817]	Loss 0.0252 (0.0313)	
training:	Epoch: [40][139/817]	Loss 0.0027 (0.0311)	
training:	Epoch: [40][140/817]	Loss 0.0579 (0.0313)	
training:	Epoch: [40][141/817]	Loss 0.0136 (0.0311)	
training:	Epoch: [40][142/817]	Loss 0.1363 (0.0319)	
training:	Epoch: [40][143/817]	Loss 0.0102 (0.0317)	
training:	Epoch: [40][144/817]	Loss 0.0043 (0.0315)	
training:	Epoch: [40][145/817]	Loss 0.0104 (0.0314)	
training:	Epoch: [40][146/817]	Loss 0.0047 (0.0312)	
training:	Epoch: [40][147/817]	Loss 0.0063 (0.0310)	
training:	Epoch: [40][148/817]	Loss 0.0123 (0.0309)	
training:	Epoch: [40][149/817]	Loss 0.0100 (0.0308)	
training:	Epoch: [40][150/817]	Loss 0.0050 (0.0306)	
training:	Epoch: [40][151/817]	Loss 0.0064 (0.0304)	
training:	Epoch: [40][152/817]	Loss 0.0094 (0.0303)	
training:	Epoch: [40][153/817]	Loss 0.0104 (0.0302)	
training:	Epoch: [40][154/817]	Loss 0.0115 (0.0301)	
training:	Epoch: [40][155/817]	Loss 0.2246 (0.0313)	
training:	Epoch: [40][156/817]	Loss 0.0104 (0.0312)	
training:	Epoch: [40][157/817]	Loss 0.0104 (0.0310)	
training:	Epoch: [40][158/817]	Loss 0.0056 (0.0309)	
training:	Epoch: [40][159/817]	Loss 0.0069 (0.0307)	
training:	Epoch: [40][160/817]	Loss 0.1530 (0.0315)	
training:	Epoch: [40][161/817]	Loss 0.0148 (0.0314)	
training:	Epoch: [40][162/817]	Loss 0.0070 (0.0312)	
training:	Epoch: [40][163/817]	Loss 0.0078 (0.0311)	
training:	Epoch: [40][164/817]	Loss 0.0082 (0.0310)	
training:	Epoch: [40][165/817]	Loss 0.0043 (0.0308)	
training:	Epoch: [40][166/817]	Loss 0.0055 (0.0306)	
training:	Epoch: [40][167/817]	Loss 0.0063 (0.0305)	
training:	Epoch: [40][168/817]	Loss 0.0056 (0.0303)	
training:	Epoch: [40][169/817]	Loss 0.7473 (0.0346)	
training:	Epoch: [40][170/817]	Loss 0.0042 (0.0344)	
training:	Epoch: [40][171/817]	Loss 0.0478 (0.0345)	
training:	Epoch: [40][172/817]	Loss 0.0279 (0.0345)	
training:	Epoch: [40][173/817]	Loss 0.0045 (0.0343)	
training:	Epoch: [40][174/817]	Loss 0.0086 (0.0341)	
training:	Epoch: [40][175/817]	Loss 0.0078 (0.0340)	
training:	Epoch: [40][176/817]	Loss 0.0105 (0.0338)	
training:	Epoch: [40][177/817]	Loss 0.0056 (0.0337)	
training:	Epoch: [40][178/817]	Loss 0.0041 (0.0335)	
training:	Epoch: [40][179/817]	Loss 0.0043 (0.0334)	
training:	Epoch: [40][180/817]	Loss 0.0066 (0.0332)	
training:	Epoch: [40][181/817]	Loss 0.0031 (0.0330)	
training:	Epoch: [40][182/817]	Loss 0.0040 (0.0329)	
training:	Epoch: [40][183/817]	Loss 0.0068 (0.0327)	
training:	Epoch: [40][184/817]	Loss 0.0061 (0.0326)	
training:	Epoch: [40][185/817]	Loss 0.0201 (0.0325)	
training:	Epoch: [40][186/817]	Loss 0.0156 (0.0324)	
training:	Epoch: [40][187/817]	Loss 0.0061 (0.0323)	
training:	Epoch: [40][188/817]	Loss 0.0035 (0.0321)	
training:	Epoch: [40][189/817]	Loss 0.0150 (0.0321)	
training:	Epoch: [40][190/817]	Loss 0.0658 (0.0322)	
training:	Epoch: [40][191/817]	Loss 0.0084 (0.0321)	
training:	Epoch: [40][192/817]	Loss 0.0102 (0.0320)	
training:	Epoch: [40][193/817]	Loss 0.0024 (0.0318)	
training:	Epoch: [40][194/817]	Loss 0.0092 (0.0317)	
training:	Epoch: [40][195/817]	Loss 0.0033 (0.0316)	
training:	Epoch: [40][196/817]	Loss 0.0034 (0.0314)	
training:	Epoch: [40][197/817]	Loss 0.0071 (0.0313)	
training:	Epoch: [40][198/817]	Loss 0.0289 (0.0313)	
training:	Epoch: [40][199/817]	Loss 0.0121 (0.0312)	
training:	Epoch: [40][200/817]	Loss 0.0092 (0.0311)	
training:	Epoch: [40][201/817]	Loss 0.0079 (0.0310)	
training:	Epoch: [40][202/817]	Loss 0.0033 (0.0308)	
training:	Epoch: [40][203/817]	Loss 0.0079 (0.0307)	
training:	Epoch: [40][204/817]	Loss 0.0036 (0.0306)	
training:	Epoch: [40][205/817]	Loss 0.0040 (0.0305)	
training:	Epoch: [40][206/817]	Loss 0.0046 (0.0303)	
training:	Epoch: [40][207/817]	Loss 0.0040 (0.0302)	
training:	Epoch: [40][208/817]	Loss 0.0049 (0.0301)	
training:	Epoch: [40][209/817]	Loss 0.0100 (0.0300)	
training:	Epoch: [40][210/817]	Loss 0.0041 (0.0299)	
training:	Epoch: [40][211/817]	Loss 0.0034 (0.0297)	
training:	Epoch: [40][212/817]	Loss 0.0038 (0.0296)	
training:	Epoch: [40][213/817]	Loss 0.0054 (0.0295)	
training:	Epoch: [40][214/817]	Loss 0.0047 (0.0294)	
training:	Epoch: [40][215/817]	Loss 0.0086 (0.0293)	
training:	Epoch: [40][216/817]	Loss 0.0179 (0.0292)	
training:	Epoch: [40][217/817]	Loss 0.0052 (0.0291)	
training:	Epoch: [40][218/817]	Loss 0.0059 (0.0290)	
training:	Epoch: [40][219/817]	Loss 0.6765 (0.0320)	
training:	Epoch: [40][220/817]	Loss 0.0093 (0.0319)	
training:	Epoch: [40][221/817]	Loss 0.0452 (0.0319)	
training:	Epoch: [40][222/817]	Loss 0.0026 (0.0318)	
training:	Epoch: [40][223/817]	Loss 0.0031 (0.0317)	
training:	Epoch: [40][224/817]	Loss 0.0046 (0.0316)	
training:	Epoch: [40][225/817]	Loss 0.0054 (0.0314)	
training:	Epoch: [40][226/817]	Loss 0.0104 (0.0313)	
training:	Epoch: [40][227/817]	Loss 0.0059 (0.0312)	
training:	Epoch: [40][228/817]	Loss 0.0028 (0.0311)	
training:	Epoch: [40][229/817]	Loss 0.0082 (0.0310)	
training:	Epoch: [40][230/817]	Loss 0.0253 (0.0310)	
training:	Epoch: [40][231/817]	Loss 0.5426 (0.0332)	
training:	Epoch: [40][232/817]	Loss 0.0036 (0.0331)	
training:	Epoch: [40][233/817]	Loss 0.0064 (0.0330)	
training:	Epoch: [40][234/817]	Loss 0.0187 (0.0329)	
training:	Epoch: [40][235/817]	Loss 0.2013 (0.0336)	
training:	Epoch: [40][236/817]	Loss 0.0173 (0.0335)	
training:	Epoch: [40][237/817]	Loss 0.0142 (0.0335)	
training:	Epoch: [40][238/817]	Loss 0.5459 (0.0356)	
training:	Epoch: [40][239/817]	Loss 0.0025 (0.0355)	
training:	Epoch: [40][240/817]	Loss 0.0063 (0.0354)	
training:	Epoch: [40][241/817]	Loss 0.0037 (0.0352)	
training:	Epoch: [40][242/817]	Loss 0.0048 (0.0351)	
training:	Epoch: [40][243/817]	Loss 0.0065 (0.0350)	
training:	Epoch: [40][244/817]	Loss 0.0041 (0.0349)	
training:	Epoch: [40][245/817]	Loss 0.0090 (0.0347)	
training:	Epoch: [40][246/817]	Loss 0.0031 (0.0346)	
training:	Epoch: [40][247/817]	Loss 0.0045 (0.0345)	
training:	Epoch: [40][248/817]	Loss 0.0095 (0.0344)	
training:	Epoch: [40][249/817]	Loss 0.0207 (0.0343)	
training:	Epoch: [40][250/817]	Loss 0.0410 (0.0344)	
training:	Epoch: [40][251/817]	Loss 0.0050 (0.0343)	
training:	Epoch: [40][252/817]	Loss 0.0046 (0.0341)	
training:	Epoch: [40][253/817]	Loss 0.0172 (0.0341)	
training:	Epoch: [40][254/817]	Loss 0.0102 (0.0340)	
training:	Epoch: [40][255/817]	Loss 0.0047 (0.0339)	
training:	Epoch: [40][256/817]	Loss 0.0037 (0.0337)	
training:	Epoch: [40][257/817]	Loss 0.0034 (0.0336)	
training:	Epoch: [40][258/817]	Loss 0.0046 (0.0335)	
training:	Epoch: [40][259/817]	Loss 0.0037 (0.0334)	
training:	Epoch: [40][260/817]	Loss 0.1453 (0.0338)	
training:	Epoch: [40][261/817]	Loss 0.0065 (0.0337)	
training:	Epoch: [40][262/817]	Loss 0.2161 (0.0344)	
training:	Epoch: [40][263/817]	Loss 0.1141 (0.0347)	
training:	Epoch: [40][264/817]	Loss 0.0093 (0.0346)	
training:	Epoch: [40][265/817]	Loss 0.0041 (0.0345)	
training:	Epoch: [40][266/817]	Loss 0.0032 (0.0344)	
training:	Epoch: [40][267/817]	Loss 0.0067 (0.0343)	
training:	Epoch: [40][268/817]	Loss 0.1650 (0.0348)	
training:	Epoch: [40][269/817]	Loss 0.0034 (0.0347)	
training:	Epoch: [40][270/817]	Loss 0.0179 (0.0346)	
training:	Epoch: [40][271/817]	Loss 0.0058 (0.0345)	
training:	Epoch: [40][272/817]	Loss 0.2878 (0.0354)	
training:	Epoch: [40][273/817]	Loss 0.0042 (0.0353)	
training:	Epoch: [40][274/817]	Loss 0.0030 (0.0352)	
training:	Epoch: [40][275/817]	Loss 0.0069 (0.0351)	
training:	Epoch: [40][276/817]	Loss 0.0181 (0.0350)	
training:	Epoch: [40][277/817]	Loss 0.0046 (0.0349)	
training:	Epoch: [40][278/817]	Loss 0.0050 (0.0348)	
training:	Epoch: [40][279/817]	Loss 0.0091 (0.0347)	
training:	Epoch: [40][280/817]	Loss 0.0069 (0.0346)	
training:	Epoch: [40][281/817]	Loss 0.0025 (0.0345)	
training:	Epoch: [40][282/817]	Loss 0.0063 (0.0344)	
training:	Epoch: [40][283/817]	Loss 0.0104 (0.0343)	
training:	Epoch: [40][284/817]	Loss 0.0078 (0.0342)	
training:	Epoch: [40][285/817]	Loss 0.4005 (0.0355)	
training:	Epoch: [40][286/817]	Loss 0.0074 (0.0354)	
training:	Epoch: [40][287/817]	Loss 0.0089 (0.0353)	
training:	Epoch: [40][288/817]	Loss 0.0037 (0.0352)	
training:	Epoch: [40][289/817]	Loss 0.0093 (0.0351)	
training:	Epoch: [40][290/817]	Loss 0.0047 (0.0350)	
training:	Epoch: [40][291/817]	Loss 0.6508 (0.0371)	
training:	Epoch: [40][292/817]	Loss 0.0118 (0.0370)	
training:	Epoch: [40][293/817]	Loss 0.0464 (0.0371)	
training:	Epoch: [40][294/817]	Loss 0.0030 (0.0370)	
training:	Epoch: [40][295/817]	Loss 0.0122 (0.0369)	
training:	Epoch: [40][296/817]	Loss 0.0039 (0.0368)	
training:	Epoch: [40][297/817]	Loss 0.0109 (0.0367)	
training:	Epoch: [40][298/817]	Loss 0.0057 (0.0366)	
training:	Epoch: [40][299/817]	Loss 0.0028 (0.0365)	
training:	Epoch: [40][300/817]	Loss 0.0069 (0.0364)	
training:	Epoch: [40][301/817]	Loss 0.0063 (0.0363)	
training:	Epoch: [40][302/817]	Loss 0.0042 (0.0362)	
training:	Epoch: [40][303/817]	Loss 0.0050 (0.0361)	
training:	Epoch: [40][304/817]	Loss 0.0074 (0.0360)	
training:	Epoch: [40][305/817]	Loss 0.0122 (0.0359)	
training:	Epoch: [40][306/817]	Loss 0.0075 (0.0358)	
training:	Epoch: [40][307/817]	Loss 0.0032 (0.0357)	
training:	Epoch: [40][308/817]	Loss 0.0035 (0.0356)	
training:	Epoch: [40][309/817]	Loss 0.0181 (0.0355)	
training:	Epoch: [40][310/817]	Loss 0.0060 (0.0354)	
training:	Epoch: [40][311/817]	Loss 0.0045 (0.0353)	
training:	Epoch: [40][312/817]	Loss 0.0055 (0.0352)	
training:	Epoch: [40][313/817]	Loss 0.0767 (0.0354)	
training:	Epoch: [40][314/817]	Loss 0.0027 (0.0353)	
training:	Epoch: [40][315/817]	Loss 0.0077 (0.0352)	
training:	Epoch: [40][316/817]	Loss 0.0117 (0.0351)	
training:	Epoch: [40][317/817]	Loss 0.0045 (0.0350)	
training:	Epoch: [40][318/817]	Loss 0.0022 (0.0349)	
training:	Epoch: [40][319/817]	Loss 0.0221 (0.0349)	
training:	Epoch: [40][320/817]	Loss 0.0114 (0.0348)	
training:	Epoch: [40][321/817]	Loss 0.0035 (0.0347)	
training:	Epoch: [40][322/817]	Loss 0.0042 (0.0346)	
training:	Epoch: [40][323/817]	Loss 0.0070 (0.0345)	
training:	Epoch: [40][324/817]	Loss 0.0095 (0.0344)	
training:	Epoch: [40][325/817]	Loss 0.0066 (0.0343)	
training:	Epoch: [40][326/817]	Loss 0.0040 (0.0343)	
training:	Epoch: [40][327/817]	Loss 0.0104 (0.0342)	
training:	Epoch: [40][328/817]	Loss 0.0149 (0.0341)	
training:	Epoch: [40][329/817]	Loss 0.0055 (0.0340)	
training:	Epoch: [40][330/817]	Loss 0.0059 (0.0339)	
training:	Epoch: [40][331/817]	Loss 0.0054 (0.0339)	
training:	Epoch: [40][332/817]	Loss 0.0079 (0.0338)	
training:	Epoch: [40][333/817]	Loss 0.0378 (0.0338)	
training:	Epoch: [40][334/817]	Loss 0.0035 (0.0337)	
training:	Epoch: [40][335/817]	Loss 0.0063 (0.0336)	
training:	Epoch: [40][336/817]	Loss 0.0046 (0.0335)	
training:	Epoch: [40][337/817]	Loss 0.0047 (0.0334)	
training:	Epoch: [40][338/817]	Loss 0.0088 (0.0334)	
training:	Epoch: [40][339/817]	Loss 0.0031 (0.0333)	
training:	Epoch: [40][340/817]	Loss 0.0032 (0.0332)	
training:	Epoch: [40][341/817]	Loss 0.0039 (0.0331)	
training:	Epoch: [40][342/817]	Loss 0.7051 (0.0351)	
training:	Epoch: [40][343/817]	Loss 0.1150 (0.0353)	
training:	Epoch: [40][344/817]	Loss 0.1182 (0.0356)	
training:	Epoch: [40][345/817]	Loss 0.0083 (0.0355)	
training:	Epoch: [40][346/817]	Loss 0.0055 (0.0354)	
training:	Epoch: [40][347/817]	Loss 0.0027 (0.0353)	
training:	Epoch: [40][348/817]	Loss 0.0046 (0.0352)	
training:	Epoch: [40][349/817]	Loss 0.0083 (0.0351)	
training:	Epoch: [40][350/817]	Loss 0.0065 (0.0350)	
training:	Epoch: [40][351/817]	Loss 0.0068 (0.0350)	
training:	Epoch: [40][352/817]	Loss 0.0057 (0.0349)	
training:	Epoch: [40][353/817]	Loss 0.0045 (0.0348)	
training:	Epoch: [40][354/817]	Loss 0.0041 (0.0347)	
training:	Epoch: [40][355/817]	Loss 0.0060 (0.0346)	
training:	Epoch: [40][356/817]	Loss 0.0036 (0.0345)	
training:	Epoch: [40][357/817]	Loss 0.0061 (0.0345)	
training:	Epoch: [40][358/817]	Loss 0.0233 (0.0344)	
training:	Epoch: [40][359/817]	Loss 0.0030 (0.0343)	
training:	Epoch: [40][360/817]	Loss 0.0036 (0.0343)	
training:	Epoch: [40][361/817]	Loss 0.0021 (0.0342)	
training:	Epoch: [40][362/817]	Loss 0.0058 (0.0341)	
training:	Epoch: [40][363/817]	Loss 0.0033 (0.0340)	
training:	Epoch: [40][364/817]	Loss 0.0030 (0.0339)	
training:	Epoch: [40][365/817]	Loss 0.0050 (0.0338)	
training:	Epoch: [40][366/817]	Loss 0.0191 (0.0338)	
training:	Epoch: [40][367/817]	Loss 0.0038 (0.0337)	
training:	Epoch: [40][368/817]	Loss 0.0027 (0.0336)	
training:	Epoch: [40][369/817]	Loss 0.0074 (0.0336)	
training:	Epoch: [40][370/817]	Loss 0.0036 (0.0335)	
training:	Epoch: [40][371/817]	Loss 0.0039 (0.0334)	
training:	Epoch: [40][372/817]	Loss 0.0028 (0.0333)	
training:	Epoch: [40][373/817]	Loss 0.0054 (0.0332)	
training:	Epoch: [40][374/817]	Loss 0.0049 (0.0332)	
training:	Epoch: [40][375/817]	Loss 0.0054 (0.0331)	
training:	Epoch: [40][376/817]	Loss 0.0028 (0.0330)	
training:	Epoch: [40][377/817]	Loss 0.0042 (0.0329)	
training:	Epoch: [40][378/817]	Loss 0.0045 (0.0329)	
training:	Epoch: [40][379/817]	Loss 0.0084 (0.0328)	
training:	Epoch: [40][380/817]	Loss 0.0065 (0.0327)	
training:	Epoch: [40][381/817]	Loss 0.0061 (0.0327)	
training:	Epoch: [40][382/817]	Loss 0.0037 (0.0326)	
training:	Epoch: [40][383/817]	Loss 0.0071 (0.0325)	
training:	Epoch: [40][384/817]	Loss 0.0034 (0.0324)	
training:	Epoch: [40][385/817]	Loss 0.0054 (0.0324)	
training:	Epoch: [40][386/817]	Loss 0.0037 (0.0323)	
training:	Epoch: [40][387/817]	Loss 0.0031 (0.0322)	
training:	Epoch: [40][388/817]	Loss 0.0024 (0.0321)	
training:	Epoch: [40][389/817]	Loss 0.0033 (0.0321)	
training:	Epoch: [40][390/817]	Loss 0.0069 (0.0320)	
training:	Epoch: [40][391/817]	Loss 0.0067 (0.0319)	
training:	Epoch: [40][392/817]	Loss 0.4396 (0.0330)	
training:	Epoch: [40][393/817]	Loss 0.0100 (0.0329)	
training:	Epoch: [40][394/817]	Loss 0.0033 (0.0328)	
training:	Epoch: [40][395/817]	Loss 0.0043 (0.0328)	
training:	Epoch: [40][396/817]	Loss 0.0027 (0.0327)	
training:	Epoch: [40][397/817]	Loss 0.0032 (0.0326)	
training:	Epoch: [40][398/817]	Loss 0.0035 (0.0326)	
training:	Epoch: [40][399/817]	Loss 0.0067 (0.0325)	
training:	Epoch: [40][400/817]	Loss 0.0037 (0.0324)	
training:	Epoch: [40][401/817]	Loss 0.0032 (0.0323)	
training:	Epoch: [40][402/817]	Loss 0.0028 (0.0323)	
training:	Epoch: [40][403/817]	Loss 0.0032 (0.0322)	
training:	Epoch: [40][404/817]	Loss 0.0032 (0.0321)	
training:	Epoch: [40][405/817]	Loss 0.0027 (0.0321)	
training:	Epoch: [40][406/817]	Loss 0.0025 (0.0320)	
training:	Epoch: [40][407/817]	Loss 0.0908 (0.0321)	
training:	Epoch: [40][408/817]	Loss 0.0446 (0.0322)	
training:	Epoch: [40][409/817]	Loss 0.0513 (0.0322)	
training:	Epoch: [40][410/817]	Loss 0.0219 (0.0322)	
training:	Epoch: [40][411/817]	Loss 0.0043 (0.0321)	
training:	Epoch: [40][412/817]	Loss 0.0029 (0.0320)	
training:	Epoch: [40][413/817]	Loss 0.0034 (0.0320)	
training:	Epoch: [40][414/817]	Loss 0.0042 (0.0319)	
training:	Epoch: [40][415/817]	Loss 0.1253 (0.0321)	
training:	Epoch: [40][416/817]	Loss 0.0057 (0.0321)	
training:	Epoch: [40][417/817]	Loss 0.0033 (0.0320)	
training:	Epoch: [40][418/817]	Loss 0.0037 (0.0319)	
training:	Epoch: [40][419/817]	Loss 0.0039 (0.0319)	
training:	Epoch: [40][420/817]	Loss 0.1137 (0.0321)	
training:	Epoch: [40][421/817]	Loss 0.1204 (0.0323)	
training:	Epoch: [40][422/817]	Loss 0.0024 (0.0322)	
training:	Epoch: [40][423/817]	Loss 0.0029 (0.0321)	
training:	Epoch: [40][424/817]	Loss 0.0031 (0.0321)	
training:	Epoch: [40][425/817]	Loss 0.0271 (0.0320)	
training:	Epoch: [40][426/817]	Loss 0.0147 (0.0320)	
training:	Epoch: [40][427/817]	Loss 0.0036 (0.0319)	
training:	Epoch: [40][428/817]	Loss 0.0027 (0.0319)	
training:	Epoch: [40][429/817]	Loss 0.0087 (0.0318)	
training:	Epoch: [40][430/817]	Loss 0.0054 (0.0318)	
training:	Epoch: [40][431/817]	Loss 0.0042 (0.0317)	
training:	Epoch: [40][432/817]	Loss 0.0030 (0.0316)	
training:	Epoch: [40][433/817]	Loss 0.0031 (0.0316)	
training:	Epoch: [40][434/817]	Loss 0.0043 (0.0315)	
training:	Epoch: [40][435/817]	Loss 0.4944 (0.0326)	
training:	Epoch: [40][436/817]	Loss 0.1129 (0.0327)	
training:	Epoch: [40][437/817]	Loss 0.0265 (0.0327)	
training:	Epoch: [40][438/817]	Loss 0.0027 (0.0327)	
training:	Epoch: [40][439/817]	Loss 0.0027 (0.0326)	
training:	Epoch: [40][440/817]	Loss 0.0025 (0.0325)	
training:	Epoch: [40][441/817]	Loss 0.0031 (0.0325)	
training:	Epoch: [40][442/817]	Loss 0.0636 (0.0325)	
training:	Epoch: [40][443/817]	Loss 0.0042 (0.0325)	
training:	Epoch: [40][444/817]	Loss 0.2961 (0.0331)	
training:	Epoch: [40][445/817]	Loss 0.0025 (0.0330)	
training:	Epoch: [40][446/817]	Loss 0.0033 (0.0329)	
training:	Epoch: [40][447/817]	Loss 0.0036 (0.0329)	
training:	Epoch: [40][448/817]	Loss 0.0042 (0.0328)	
training:	Epoch: [40][449/817]	Loss 0.0033 (0.0327)	
training:	Epoch: [40][450/817]	Loss 0.0024 (0.0327)	
training:	Epoch: [40][451/817]	Loss 0.0035 (0.0326)	
training:	Epoch: [40][452/817]	Loss 0.0025 (0.0325)	
training:	Epoch: [40][453/817]	Loss 0.0034 (0.0325)	
training:	Epoch: [40][454/817]	Loss 0.0429 (0.0325)	
training:	Epoch: [40][455/817]	Loss 0.0047 (0.0324)	
training:	Epoch: [40][456/817]	Loss 0.0035 (0.0324)	
training:	Epoch: [40][457/817]	Loss 0.0031 (0.0323)	
training:	Epoch: [40][458/817]	Loss 0.0042 (0.0322)	
training:	Epoch: [40][459/817]	Loss 0.0026 (0.0322)	
training:	Epoch: [40][460/817]	Loss 0.0058 (0.0321)	
training:	Epoch: [40][461/817]	Loss 0.0036 (0.0321)	
training:	Epoch: [40][462/817]	Loss 0.0021 (0.0320)	
training:	Epoch: [40][463/817]	Loss 0.0024 (0.0319)	
training:	Epoch: [40][464/817]	Loss 0.0030 (0.0319)	
training:	Epoch: [40][465/817]	Loss 0.0043 (0.0318)	
training:	Epoch: [40][466/817]	Loss 0.0033 (0.0317)	
training:	Epoch: [40][467/817]	Loss 0.0022 (0.0317)	
training:	Epoch: [40][468/817]	Loss 0.0084 (0.0316)	
training:	Epoch: [40][469/817]	Loss 0.0038 (0.0316)	
training:	Epoch: [40][470/817]	Loss 0.6980 (0.0330)	
training:	Epoch: [40][471/817]	Loss 0.0072 (0.0329)	
training:	Epoch: [40][472/817]	Loss 0.0052 (0.0329)	
training:	Epoch: [40][473/817]	Loss 0.0211 (0.0328)	
training:	Epoch: [40][474/817]	Loss 0.0064 (0.0328)	
training:	Epoch: [40][475/817]	Loss 0.0043 (0.0327)	
training:	Epoch: [40][476/817]	Loss 0.0031 (0.0327)	
training:	Epoch: [40][477/817]	Loss 0.0023 (0.0326)	
training:	Epoch: [40][478/817]	Loss 0.0051 (0.0325)	
training:	Epoch: [40][479/817]	Loss 0.0036 (0.0325)	
training:	Epoch: [40][480/817]	Loss 0.0027 (0.0324)	
training:	Epoch: [40][481/817]	Loss 0.0051 (0.0324)	
training:	Epoch: [40][482/817]	Loss 0.0027 (0.0323)	
training:	Epoch: [40][483/817]	Loss 0.0025 (0.0322)	
training:	Epoch: [40][484/817]	Loss 0.0039 (0.0322)	
training:	Epoch: [40][485/817]	Loss 0.0042 (0.0321)	
training:	Epoch: [40][486/817]	Loss 0.0047 (0.0321)	
training:	Epoch: [40][487/817]	Loss 0.0022 (0.0320)	
training:	Epoch: [40][488/817]	Loss 0.0041 (0.0320)	
training:	Epoch: [40][489/817]	Loss 0.0127 (0.0319)	
training:	Epoch: [40][490/817]	Loss 0.0025 (0.0319)	
training:	Epoch: [40][491/817]	Loss 0.0043 (0.0318)	
training:	Epoch: [40][492/817]	Loss 0.0033 (0.0317)	
training:	Epoch: [40][493/817]	Loss 0.0027 (0.0317)	
training:	Epoch: [40][494/817]	Loss 0.0020 (0.0316)	
training:	Epoch: [40][495/817]	Loss 0.0046 (0.0316)	
training:	Epoch: [40][496/817]	Loss 0.5413 (0.0326)	
training:	Epoch: [40][497/817]	Loss 0.0060 (0.0325)	
training:	Epoch: [40][498/817]	Loss 0.0034 (0.0325)	
training:	Epoch: [40][499/817]	Loss 0.0020 (0.0324)	
training:	Epoch: [40][500/817]	Loss 0.0034 (0.0324)	
training:	Epoch: [40][501/817]	Loss 0.0038 (0.0323)	
training:	Epoch: [40][502/817]	Loss 0.0034 (0.0322)	
training:	Epoch: [40][503/817]	Loss 0.0045 (0.0322)	
training:	Epoch: [40][504/817]	Loss 0.0060 (0.0321)	
training:	Epoch: [40][505/817]	Loss 0.0024 (0.0321)	
training:	Epoch: [40][506/817]	Loss 0.0043 (0.0320)	
training:	Epoch: [40][507/817]	Loss 0.0047 (0.0320)	
training:	Epoch: [40][508/817]	Loss 0.0056 (0.0319)	
training:	Epoch: [40][509/817]	Loss 0.0084 (0.0319)	
training:	Epoch: [40][510/817]	Loss 0.0027 (0.0318)	
training:	Epoch: [40][511/817]	Loss 0.0038 (0.0318)	
training:	Epoch: [40][512/817]	Loss 0.0032 (0.0317)	
training:	Epoch: [40][513/817]	Loss 0.0025 (0.0317)	
training:	Epoch: [40][514/817]	Loss 0.0031 (0.0316)	
training:	Epoch: [40][515/817]	Loss 0.0072 (0.0315)	
training:	Epoch: [40][516/817]	Loss 0.3004 (0.0321)	
training:	Epoch: [40][517/817]	Loss 0.0041 (0.0320)	
training:	Epoch: [40][518/817]	Loss 0.0217 (0.0320)	
training:	Epoch: [40][519/817]	Loss 0.0027 (0.0319)	
training:	Epoch: [40][520/817]	Loss 0.0060 (0.0319)	
training:	Epoch: [40][521/817]	Loss 0.0032 (0.0318)	
training:	Epoch: [40][522/817]	Loss 0.0051 (0.0318)	
training:	Epoch: [40][523/817]	Loss 0.0036 (0.0317)	
training:	Epoch: [40][524/817]	Loss 0.0039 (0.0317)	
training:	Epoch: [40][525/817]	Loss 0.0064 (0.0316)	
training:	Epoch: [40][526/817]	Loss 0.0056 (0.0316)	
training:	Epoch: [40][527/817]	Loss 0.0589 (0.0316)	
training:	Epoch: [40][528/817]	Loss 0.0069 (0.0316)	
training:	Epoch: [40][529/817]	Loss 0.0065 (0.0315)	
training:	Epoch: [40][530/817]	Loss 0.0110 (0.0315)	
training:	Epoch: [40][531/817]	Loss 0.0024 (0.0314)	
training:	Epoch: [40][532/817]	Loss 0.0042 (0.0314)	
training:	Epoch: [40][533/817]	Loss 0.0059 (0.0313)	
training:	Epoch: [40][534/817]	Loss 0.0029 (0.0313)	
training:	Epoch: [40][535/817]	Loss 0.0158 (0.0313)	
training:	Epoch: [40][536/817]	Loss 0.0122 (0.0312)	
training:	Epoch: [40][537/817]	Loss 0.0047 (0.0312)	
training:	Epoch: [40][538/817]	Loss 0.0032 (0.0311)	
training:	Epoch: [40][539/817]	Loss 0.0033 (0.0311)	
training:	Epoch: [40][540/817]	Loss 0.0029 (0.0310)	
training:	Epoch: [40][541/817]	Loss 0.0050 (0.0310)	
training:	Epoch: [40][542/817]	Loss 0.0022 (0.0309)	
training:	Epoch: [40][543/817]	Loss 0.0035 (0.0309)	
training:	Epoch: [40][544/817]	Loss 0.0025 (0.0308)	
training:	Epoch: [40][545/817]	Loss 0.0036 (0.0308)	
training:	Epoch: [40][546/817]	Loss 0.0028 (0.0307)	
training:	Epoch: [40][547/817]	Loss 0.0034 (0.0307)	
training:	Epoch: [40][548/817]	Loss 0.0067 (0.0306)	
training:	Epoch: [40][549/817]	Loss 0.0034 (0.0306)	
training:	Epoch: [40][550/817]	Loss 0.0026 (0.0305)	
training:	Epoch: [40][551/817]	Loss 0.0041 (0.0305)	
training:	Epoch: [40][552/817]	Loss 0.0025 (0.0304)	
training:	Epoch: [40][553/817]	Loss 0.0053 (0.0304)	
training:	Epoch: [40][554/817]	Loss 0.0023 (0.0303)	
training:	Epoch: [40][555/817]	Loss 0.0033 (0.0303)	
training:	Epoch: [40][556/817]	Loss 0.0027 (0.0302)	
training:	Epoch: [40][557/817]	Loss 0.0056 (0.0302)	
training:	Epoch: [40][558/817]	Loss 0.0037 (0.0301)	
training:	Epoch: [40][559/817]	Loss 0.0036 (0.0301)	
training:	Epoch: [40][560/817]	Loss 0.0025 (0.0300)	
training:	Epoch: [40][561/817]	Loss 0.0269 (0.0300)	
training:	Epoch: [40][562/817]	Loss 0.0037 (0.0300)	
training:	Epoch: [40][563/817]	Loss 0.0104 (0.0300)	
training:	Epoch: [40][564/817]	Loss 0.0026 (0.0299)	
training:	Epoch: [40][565/817]	Loss 0.0079 (0.0299)	
training:	Epoch: [40][566/817]	Loss 0.0044 (0.0298)	
training:	Epoch: [40][567/817]	Loss 0.0066 (0.0298)	
training:	Epoch: [40][568/817]	Loss 0.0020 (0.0297)	
training:	Epoch: [40][569/817]	Loss 0.0042 (0.0297)	
training:	Epoch: [40][570/817]	Loss 0.0042 (0.0296)	
training:	Epoch: [40][571/817]	Loss 0.0247 (0.0296)	
training:	Epoch: [40][572/817]	Loss 0.0039 (0.0296)	
training:	Epoch: [40][573/817]	Loss 0.0051 (0.0295)	
training:	Epoch: [40][574/817]	Loss 0.0023 (0.0295)	
training:	Epoch: [40][575/817]	Loss 0.0052 (0.0295)	
training:	Epoch: [40][576/817]	Loss 0.0027 (0.0294)	
training:	Epoch: [40][577/817]	Loss 0.0036 (0.0294)	
training:	Epoch: [40][578/817]	Loss 0.0021 (0.0293)	
training:	Epoch: [40][579/817]	Loss 0.0040 (0.0293)	
training:	Epoch: [40][580/817]	Loss 0.3392 (0.0298)	
training:	Epoch: [40][581/817]	Loss 0.0053 (0.0298)	
training:	Epoch: [40][582/817]	Loss 0.0050 (0.0297)	
training:	Epoch: [40][583/817]	Loss 0.0065 (0.0297)	
training:	Epoch: [40][584/817]	Loss 0.0025 (0.0296)	
training:	Epoch: [40][585/817]	Loss 0.0064 (0.0296)	
training:	Epoch: [40][586/817]	Loss 0.0028 (0.0296)	
training:	Epoch: [40][587/817]	Loss 0.0034 (0.0295)	
training:	Epoch: [40][588/817]	Loss 0.0038 (0.0295)	
training:	Epoch: [40][589/817]	Loss 0.0032 (0.0294)	
training:	Epoch: [40][590/817]	Loss 0.0026 (0.0294)	
training:	Epoch: [40][591/817]	Loss 0.0037 (0.0293)	
training:	Epoch: [40][592/817]	Loss 0.5243 (0.0302)	
training:	Epoch: [40][593/817]	Loss 0.0022 (0.0301)	
training:	Epoch: [40][594/817]	Loss 0.0049 (0.0301)	
training:	Epoch: [40][595/817]	Loss 0.0050 (0.0300)	
training:	Epoch: [40][596/817]	Loss 0.0030 (0.0300)	
training:	Epoch: [40][597/817]	Loss 0.0030 (0.0299)	
training:	Epoch: [40][598/817]	Loss 0.0023 (0.0299)	
training:	Epoch: [40][599/817]	Loss 0.0051 (0.0299)	
training:	Epoch: [40][600/817]	Loss 0.0038 (0.0298)	
training:	Epoch: [40][601/817]	Loss 0.0055 (0.0298)	
training:	Epoch: [40][602/817]	Loss 0.0025 (0.0297)	
training:	Epoch: [40][603/817]	Loss 0.0022 (0.0297)	
training:	Epoch: [40][604/817]	Loss 0.0040 (0.0296)	
training:	Epoch: [40][605/817]	Loss 0.0049 (0.0296)	
training:	Epoch: [40][606/817]	Loss 0.0063 (0.0296)	
training:	Epoch: [40][607/817]	Loss 0.0052 (0.0295)	
training:	Epoch: [40][608/817]	Loss 0.0038 (0.0295)	
training:	Epoch: [40][609/817]	Loss 0.0037 (0.0294)	
training:	Epoch: [40][610/817]	Loss 0.0039 (0.0294)	
training:	Epoch: [40][611/817]	Loss 0.0038 (0.0294)	
training:	Epoch: [40][612/817]	Loss 0.0075 (0.0293)	
training:	Epoch: [40][613/817]	Loss 0.0029 (0.0293)	
training:	Epoch: [40][614/817]	Loss 0.0053 (0.0292)	
training:	Epoch: [40][615/817]	Loss 0.0046 (0.0292)	
training:	Epoch: [40][616/817]	Loss 0.0041 (0.0292)	
training:	Epoch: [40][617/817]	Loss 0.0039 (0.0291)	
training:	Epoch: [40][618/817]	Loss 0.0098 (0.0291)	
training:	Epoch: [40][619/817]	Loss 0.0044 (0.0290)	
training:	Epoch: [40][620/817]	Loss 0.0052 (0.0290)	
training:	Epoch: [40][621/817]	Loss 0.0033 (0.0290)	
training:	Epoch: [40][622/817]	Loss 0.0115 (0.0289)	
training:	Epoch: [40][623/817]	Loss 0.0043 (0.0289)	
training:	Epoch: [40][624/817]	Loss 0.0039 (0.0289)	
training:	Epoch: [40][625/817]	Loss 0.0067 (0.0288)	
training:	Epoch: [40][626/817]	Loss 0.0033 (0.0288)	
training:	Epoch: [40][627/817]	Loss 0.0023 (0.0287)	
training:	Epoch: [40][628/817]	Loss 0.0048 (0.0287)	
training:	Epoch: [40][629/817]	Loss 0.0028 (0.0287)	
training:	Epoch: [40][630/817]	Loss 0.0061 (0.0286)	
training:	Epoch: [40][631/817]	Loss 0.0031 (0.0286)	
training:	Epoch: [40][632/817]	Loss 0.0038 (0.0285)	
training:	Epoch: [40][633/817]	Loss 0.0028 (0.0285)	
training:	Epoch: [40][634/817]	Loss 0.0038 (0.0285)	
training:	Epoch: [40][635/817]	Loss 0.0061 (0.0284)	
training:	Epoch: [40][636/817]	Loss 0.0028 (0.0284)	
training:	Epoch: [40][637/817]	Loss 0.0024 (0.0283)	
training:	Epoch: [40][638/817]	Loss 0.0024 (0.0283)	
training:	Epoch: [40][639/817]	Loss 0.0035 (0.0283)	
training:	Epoch: [40][640/817]	Loss 0.0126 (0.0282)	
training:	Epoch: [40][641/817]	Loss 0.0037 (0.0282)	
training:	Epoch: [40][642/817]	Loss 0.0034 (0.0282)	
training:	Epoch: [40][643/817]	Loss 0.0023 (0.0281)	
training:	Epoch: [40][644/817]	Loss 0.0031 (0.0281)	
training:	Epoch: [40][645/817]	Loss 0.0142 (0.0281)	
training:	Epoch: [40][646/817]	Loss 0.0041 (0.0280)	
training:	Epoch: [40][647/817]	Loss 0.0280 (0.0280)	
training:	Epoch: [40][648/817]	Loss 0.0020 (0.0280)	
training:	Epoch: [40][649/817]	Loss 0.0032 (0.0279)	
training:	Epoch: [40][650/817]	Loss 0.0028 (0.0279)	
training:	Epoch: [40][651/817]	Loss 0.0070 (0.0279)	
training:	Epoch: [40][652/817]	Loss 0.0030 (0.0278)	
training:	Epoch: [40][653/817]	Loss 0.0034 (0.0278)	
training:	Epoch: [40][654/817]	Loss 0.0026 (0.0278)	
training:	Epoch: [40][655/817]	Loss 0.0034 (0.0277)	
training:	Epoch: [40][656/817]	Loss 0.0022 (0.0277)	
training:	Epoch: [40][657/817]	Loss 0.0055 (0.0277)	
training:	Epoch: [40][658/817]	Loss 0.4506 (0.0283)	
training:	Epoch: [40][659/817]	Loss 0.0029 (0.0283)	
training:	Epoch: [40][660/817]	Loss 0.0056 (0.0282)	
training:	Epoch: [40][661/817]	Loss 0.0037 (0.0282)	
training:	Epoch: [40][662/817]	Loss 0.0055 (0.0281)	
training:	Epoch: [40][663/817]	Loss 0.0035 (0.0281)	
training:	Epoch: [40][664/817]	Loss 0.0054 (0.0281)	
training:	Epoch: [40][665/817]	Loss 0.3172 (0.0285)	
training:	Epoch: [40][666/817]	Loss 0.3959 (0.0291)	
training:	Epoch: [40][667/817]	Loss 0.0118 (0.0290)	
training:	Epoch: [40][668/817]	Loss 0.0045 (0.0290)	
training:	Epoch: [40][669/817]	Loss 0.0057 (0.0290)	
training:	Epoch: [40][670/817]	Loss 0.0033 (0.0289)	
training:	Epoch: [40][671/817]	Loss 0.0038 (0.0289)	
training:	Epoch: [40][672/817]	Loss 0.0157 (0.0289)	
training:	Epoch: [40][673/817]	Loss 0.0028 (0.0288)	
training:	Epoch: [40][674/817]	Loss 0.0027 (0.0288)	
training:	Epoch: [40][675/817]	Loss 0.0024 (0.0288)	
training:	Epoch: [40][676/817]	Loss 0.0023 (0.0287)	
training:	Epoch: [40][677/817]	Loss 0.0058 (0.0287)	
training:	Epoch: [40][678/817]	Loss 0.0032 (0.0286)	
training:	Epoch: [40][679/817]	Loss 0.0033 (0.0286)	
training:	Epoch: [40][680/817]	Loss 0.0396 (0.0286)	
training:	Epoch: [40][681/817]	Loss 0.0111 (0.0286)	
training:	Epoch: [40][682/817]	Loss 0.0066 (0.0286)	
training:	Epoch: [40][683/817]	Loss 0.0024 (0.0285)	
training:	Epoch: [40][684/817]	Loss 0.0043 (0.0285)	
training:	Epoch: [40][685/817]	Loss 0.5090 (0.0292)	
training:	Epoch: [40][686/817]	Loss 0.0034 (0.0292)	
training:	Epoch: [40][687/817]	Loss 0.0066 (0.0291)	
training:	Epoch: [40][688/817]	Loss 0.0027 (0.0291)	
training:	Epoch: [40][689/817]	Loss 0.0041 (0.0290)	
training:	Epoch: [40][690/817]	Loss 0.0229 (0.0290)	
training:	Epoch: [40][691/817]	Loss 0.0141 (0.0290)	
training:	Epoch: [40][692/817]	Loss 0.0056 (0.0290)	
training:	Epoch: [40][693/817]	Loss 0.0052 (0.0289)	
training:	Epoch: [40][694/817]	Loss 0.0084 (0.0289)	
training:	Epoch: [40][695/817]	Loss 0.0045 (0.0289)	
training:	Epoch: [40][696/817]	Loss 0.0044 (0.0288)	
training:	Epoch: [40][697/817]	Loss 0.0046 (0.0288)	
training:	Epoch: [40][698/817]	Loss 0.0027 (0.0288)	
training:	Epoch: [40][699/817]	Loss 0.0026 (0.0287)	
training:	Epoch: [40][700/817]	Loss 0.0033 (0.0287)	
training:	Epoch: [40][701/817]	Loss 0.0049 (0.0287)	
training:	Epoch: [40][702/817]	Loss 0.0051 (0.0286)	
training:	Epoch: [40][703/817]	Loss 0.0066 (0.0286)	
training:	Epoch: [40][704/817]	Loss 0.0053 (0.0286)	
training:	Epoch: [40][705/817]	Loss 0.0059 (0.0285)	
training:	Epoch: [40][706/817]	Loss 0.0027 (0.0285)	
training:	Epoch: [40][707/817]	Loss 0.0025 (0.0285)	
training:	Epoch: [40][708/817]	Loss 0.0121 (0.0284)	
training:	Epoch: [40][709/817]	Loss 0.0201 (0.0284)	
training:	Epoch: [40][710/817]	Loss 0.2260 (0.0287)	
training:	Epoch: [40][711/817]	Loss 0.0040 (0.0287)	
training:	Epoch: [40][712/817]	Loss 0.0188 (0.0287)	
training:	Epoch: [40][713/817]	Loss 0.0103 (0.0286)	
training:	Epoch: [40][714/817]	Loss 0.0035 (0.0286)	
training:	Epoch: [40][715/817]	Loss 0.0079 (0.0286)	
training:	Epoch: [40][716/817]	Loss 0.0048 (0.0285)	
training:	Epoch: [40][717/817]	Loss 0.0052 (0.0285)	
training:	Epoch: [40][718/817]	Loss 0.0029 (0.0285)	
training:	Epoch: [40][719/817]	Loss 0.1039 (0.0286)	
training:	Epoch: [40][720/817]	Loss 0.0059 (0.0285)	
training:	Epoch: [40][721/817]	Loss 0.0145 (0.0285)	
training:	Epoch: [40][722/817]	Loss 0.0029 (0.0285)	
training:	Epoch: [40][723/817]	Loss 0.0257 (0.0285)	
training:	Epoch: [40][724/817]	Loss 0.0032 (0.0284)	
training:	Epoch: [40][725/817]	Loss 0.0051 (0.0284)	
training:	Epoch: [40][726/817]	Loss 0.0033 (0.0284)	
training:	Epoch: [40][727/817]	Loss 0.0042 (0.0283)	
training:	Epoch: [40][728/817]	Loss 0.0046 (0.0283)	
training:	Epoch: [40][729/817]	Loss 0.0031 (0.0283)	
training:	Epoch: [40][730/817]	Loss 0.0029 (0.0282)	
training:	Epoch: [40][731/817]	Loss 0.0030 (0.0282)	
training:	Epoch: [40][732/817]	Loss 0.0031 (0.0282)	
training:	Epoch: [40][733/817]	Loss 0.0071 (0.0281)	
training:	Epoch: [40][734/817]	Loss 0.0028 (0.0281)	
training:	Epoch: [40][735/817]	Loss 0.0086 (0.0281)	
training:	Epoch: [40][736/817]	Loss 0.0023 (0.0281)	
training:	Epoch: [40][737/817]	Loss 0.0350 (0.0281)	
training:	Epoch: [40][738/817]	Loss 0.0028 (0.0280)	
training:	Epoch: [40][739/817]	Loss 0.0101 (0.0280)	
training:	Epoch: [40][740/817]	Loss 0.0149 (0.0280)	
training:	Epoch: [40][741/817]	Loss 0.0030 (0.0280)	
training:	Epoch: [40][742/817]	Loss 0.0053 (0.0279)	
training:	Epoch: [40][743/817]	Loss 0.0055 (0.0279)	
training:	Epoch: [40][744/817]	Loss 0.0062 (0.0279)	
training:	Epoch: [40][745/817]	Loss 0.6617 (0.0287)	
training:	Epoch: [40][746/817]	Loss 0.0034 (0.0287)	
training:	Epoch: [40][747/817]	Loss 0.0037 (0.0286)	
training:	Epoch: [40][748/817]	Loss 0.0035 (0.0286)	
training:	Epoch: [40][749/817]	Loss 0.0031 (0.0286)	
training:	Epoch: [40][750/817]	Loss 0.0056 (0.0285)	
training:	Epoch: [40][751/817]	Loss 0.0065 (0.0285)	
training:	Epoch: [40][752/817]	Loss 0.1244 (0.0286)	
training:	Epoch: [40][753/817]	Loss 0.0039 (0.0286)	
training:	Epoch: [40][754/817]	Loss 0.0024 (0.0286)	
training:	Epoch: [40][755/817]	Loss 0.0039 (0.0285)	
training:	Epoch: [40][756/817]	Loss 0.0085 (0.0285)	
training:	Epoch: [40][757/817]	Loss 0.0034 (0.0285)	
training:	Epoch: [40][758/817]	Loss 0.0029 (0.0285)	
training:	Epoch: [40][759/817]	Loss 0.0026 (0.0284)	
training:	Epoch: [40][760/817]	Loss 0.0194 (0.0284)	
training:	Epoch: [40][761/817]	Loss 0.0026 (0.0284)	
training:	Epoch: [40][762/817]	Loss 0.0167 (0.0284)	
training:	Epoch: [40][763/817]	Loss 0.0110 (0.0283)	
training:	Epoch: [40][764/817]	Loss 0.4136 (0.0288)	
training:	Epoch: [40][765/817]	Loss 0.0032 (0.0288)	
training:	Epoch: [40][766/817]	Loss 0.0048 (0.0288)	
training:	Epoch: [40][767/817]	Loss 0.0086 (0.0287)	
training:	Epoch: [40][768/817]	Loss 0.0062 (0.0287)	
training:	Epoch: [40][769/817]	Loss 0.0024 (0.0287)	
training:	Epoch: [40][770/817]	Loss 0.0034 (0.0287)	
training:	Epoch: [40][771/817]	Loss 0.4194 (0.0292)	
training:	Epoch: [40][772/817]	Loss 0.0092 (0.0291)	
training:	Epoch: [40][773/817]	Loss 0.0051 (0.0291)	
training:	Epoch: [40][774/817]	Loss 0.0082 (0.0291)	
training:	Epoch: [40][775/817]	Loss 0.0052 (0.0290)	
training:	Epoch: [40][776/817]	Loss 0.0353 (0.0291)	
training:	Epoch: [40][777/817]	Loss 0.0033 (0.0290)	
training:	Epoch: [40][778/817]	Loss 0.0038 (0.0290)	
training:	Epoch: [40][779/817]	Loss 0.0050 (0.0290)	
training:	Epoch: [40][780/817]	Loss 0.0078 (0.0289)	
training:	Epoch: [40][781/817]	Loss 0.0026 (0.0289)	
training:	Epoch: [40][782/817]	Loss 0.0033 (0.0289)	
training:	Epoch: [40][783/817]	Loss 0.0024 (0.0288)	
training:	Epoch: [40][784/817]	Loss 0.0112 (0.0288)	
training:	Epoch: [40][785/817]	Loss 0.0035 (0.0288)	
training:	Epoch: [40][786/817]	Loss 0.0744 (0.0288)	
training:	Epoch: [40][787/817]	Loss 0.0053 (0.0288)	
training:	Epoch: [40][788/817]	Loss 0.0031 (0.0288)	
training:	Epoch: [40][789/817]	Loss 0.0053 (0.0287)	
training:	Epoch: [40][790/817]	Loss 0.0087 (0.0287)	
training:	Epoch: [40][791/817]	Loss 0.0133 (0.0287)	
training:	Epoch: [40][792/817]	Loss 0.2620 (0.0290)	
training:	Epoch: [40][793/817]	Loss 0.1345 (0.0291)	
training:	Epoch: [40][794/817]	Loss 0.0025 (0.0291)	
training:	Epoch: [40][795/817]	Loss 0.0051 (0.0291)	
training:	Epoch: [40][796/817]	Loss 0.0064 (0.0290)	
training:	Epoch: [40][797/817]	Loss 0.0032 (0.0290)	
training:	Epoch: [40][798/817]	Loss 0.0049 (0.0290)	
training:	Epoch: [40][799/817]	Loss 0.0102 (0.0289)	
training:	Epoch: [40][800/817]	Loss 0.0036 (0.0289)	
training:	Epoch: [40][801/817]	Loss 0.0036 (0.0289)	
training:	Epoch: [40][802/817]	Loss 0.0144 (0.0289)	
training:	Epoch: [40][803/817]	Loss 0.0069 (0.0288)	
training:	Epoch: [40][804/817]	Loss 0.0032 (0.0288)	
training:	Epoch: [40][805/817]	Loss 0.0050 (0.0288)	
training:	Epoch: [40][806/817]	Loss 0.0085 (0.0287)	
training:	Epoch: [40][807/817]	Loss 0.0047 (0.0287)	
training:	Epoch: [40][808/817]	Loss 0.0023 (0.0287)	
training:	Epoch: [40][809/817]	Loss 0.0069 (0.0287)	
training:	Epoch: [40][810/817]	Loss 0.0067 (0.0286)	
training:	Epoch: [40][811/817]	Loss 0.0038 (0.0286)	
training:	Epoch: [40][812/817]	Loss 0.0059 (0.0286)	
training:	Epoch: [40][813/817]	Loss 0.1269 (0.0287)	
training:	Epoch: [40][814/817]	Loss 0.0065 (0.0287)	
training:	Epoch: [40][815/817]	Loss 0.0034 (0.0286)	
training:	Epoch: [40][816/817]	Loss 0.0068 (0.0286)	
training:	Epoch: [40][817/817]	Loss 0.0059 (0.0286)	
Training:	 Loss: 0.0286

Training:	 ACC: 0.9948 0.9948 0.9953 0.9943
Validation:	 ACC: 0.7897 0.7892 0.7789 0.8004
Validation:	 Best_BACC: 0.8223 0.8218 0.8127 0.8318
Validation:	 Loss: 1.0011
Pretraining:	Epoch 41/200
----------
training:	Epoch: [41][1/817]	Loss 0.0082 (0.0082)	
training:	Epoch: [41][2/817]	Loss 0.0148 (0.0115)	
training:	Epoch: [41][3/817]	Loss 0.0058 (0.0096)	
training:	Epoch: [41][4/817]	Loss 0.0038 (0.0082)	
training:	Epoch: [41][5/817]	Loss 0.0079 (0.0081)	
training:	Epoch: [41][6/817]	Loss 0.0173 (0.0097)	
training:	Epoch: [41][7/817]	Loss 0.0040 (0.0089)	
training:	Epoch: [41][8/817]	Loss 0.0030 (0.0081)	
training:	Epoch: [41][9/817]	Loss 0.0039 (0.0077)	
training:	Epoch: [41][10/817]	Loss 0.0038 (0.0073)	
training:	Epoch: [41][11/817]	Loss 0.0094 (0.0075)	
training:	Epoch: [41][12/817]	Loss 0.4126 (0.0412)	
training:	Epoch: [41][13/817]	Loss 0.0050 (0.0384)	
training:	Epoch: [41][14/817]	Loss 0.0027 (0.0359)	
training:	Epoch: [41][15/817]	Loss 0.0046 (0.0338)	
training:	Epoch: [41][16/817]	Loss 0.0163 (0.0327)	
training:	Epoch: [41][17/817]	Loss 0.0070 (0.0312)	
training:	Epoch: [41][18/817]	Loss 0.0169 (0.0304)	
training:	Epoch: [41][19/817]	Loss 0.0040 (0.0290)	
training:	Epoch: [41][20/817]	Loss 0.0024 (0.0277)	
training:	Epoch: [41][21/817]	Loss 0.0040 (0.0266)	
training:	Epoch: [41][22/817]	Loss 0.0027 (0.0255)	
training:	Epoch: [41][23/817]	Loss 0.0056 (0.0246)	
training:	Epoch: [41][24/817]	Loss 0.0067 (0.0239)	
training:	Epoch: [41][25/817]	Loss 0.0057 (0.0231)	
training:	Epoch: [41][26/817]	Loss 0.0079 (0.0225)	
training:	Epoch: [41][27/817]	Loss 0.0042 (0.0219)	
training:	Epoch: [41][28/817]	Loss 0.0035 (0.0212)	
training:	Epoch: [41][29/817]	Loss 0.0037 (0.0206)	
training:	Epoch: [41][30/817]	Loss 0.0038 (0.0200)	
training:	Epoch: [41][31/817]	Loss 0.0142 (0.0199)	
training:	Epoch: [41][32/817]	Loss 0.0096 (0.0195)	
training:	Epoch: [41][33/817]	Loss 0.0117 (0.0193)	
training:	Epoch: [41][34/817]	Loss 0.0071 (0.0189)	
training:	Epoch: [41][35/817]	Loss 0.0026 (0.0185)	
training:	Epoch: [41][36/817]	Loss 0.0111 (0.0183)	
training:	Epoch: [41][37/817]	Loss 0.0081 (0.0180)	
training:	Epoch: [41][38/817]	Loss 0.0040 (0.0176)	
training:	Epoch: [41][39/817]	Loss 0.0155 (0.0176)	
training:	Epoch: [41][40/817]	Loss 0.0039 (0.0172)	
training:	Epoch: [41][41/817]	Loss 0.0030 (0.0169)	
training:	Epoch: [41][42/817]	Loss 0.0027 (0.0165)	
training:	Epoch: [41][43/817]	Loss 0.0027 (0.0162)	
training:	Epoch: [41][44/817]	Loss 0.0220 (0.0164)	
training:	Epoch: [41][45/817]	Loss 0.0058 (0.0161)	
training:	Epoch: [41][46/817]	Loss 0.0037 (0.0158)	
training:	Epoch: [41][47/817]	Loss 0.0056 (0.0156)	
training:	Epoch: [41][48/817]	Loss 0.0026 (0.0154)	
training:	Epoch: [41][49/817]	Loss 0.0205 (0.0155)	
training:	Epoch: [41][50/817]	Loss 0.0083 (0.0153)	
training:	Epoch: [41][51/817]	Loss 0.0046 (0.0151)	
training:	Epoch: [41][52/817]	Loss 0.0021 (0.0149)	
training:	Epoch: [41][53/817]	Loss 0.0045 (0.0147)	
training:	Epoch: [41][54/817]	Loss 0.0132 (0.0146)	
training:	Epoch: [41][55/817]	Loss 0.0036 (0.0144)	
training:	Epoch: [41][56/817]	Loss 0.0092 (0.0143)	
training:	Epoch: [41][57/817]	Loss 0.0021 (0.0141)	
training:	Epoch: [41][58/817]	Loss 0.0041 (0.0140)	
training:	Epoch: [41][59/817]	Loss 0.2930 (0.0187)	
training:	Epoch: [41][60/817]	Loss 0.0088 (0.0185)	
training:	Epoch: [41][61/817]	Loss 0.0055 (0.0183)	
training:	Epoch: [41][62/817]	Loss 0.0057 (0.0181)	
training:	Epoch: [41][63/817]	Loss 0.0057 (0.0179)	
training:	Epoch: [41][64/817]	Loss 0.0087 (0.0178)	
training:	Epoch: [41][65/817]	Loss 0.0044 (0.0176)	
training:	Epoch: [41][66/817]	Loss 0.0027 (0.0173)	
training:	Epoch: [41][67/817]	Loss 0.0478 (0.0178)	
training:	Epoch: [41][68/817]	Loss 0.0099 (0.0177)	
training:	Epoch: [41][69/817]	Loss 0.0048 (0.0175)	
training:	Epoch: [41][70/817]	Loss 0.0025 (0.0173)	
training:	Epoch: [41][71/817]	Loss 0.0073 (0.0171)	
training:	Epoch: [41][72/817]	Loss 0.0192 (0.0172)	
training:	Epoch: [41][73/817]	Loss 0.0021 (0.0169)	
training:	Epoch: [41][74/817]	Loss 0.0043 (0.0168)	
training:	Epoch: [41][75/817]	Loss 0.0039 (0.0166)	
training:	Epoch: [41][76/817]	Loss 0.0065 (0.0165)	
training:	Epoch: [41][77/817]	Loss 0.0072 (0.0164)	
training:	Epoch: [41][78/817]	Loss 0.0075 (0.0162)	
training:	Epoch: [41][79/817]	Loss 0.0053 (0.0161)	
training:	Epoch: [41][80/817]	Loss 0.0057 (0.0160)	
training:	Epoch: [41][81/817]	Loss 0.0044 (0.0158)	
training:	Epoch: [41][82/817]	Loss 0.0042 (0.0157)	
training:	Epoch: [41][83/817]	Loss 0.0072 (0.0156)	
training:	Epoch: [41][84/817]	Loss 0.0025 (0.0154)	
training:	Epoch: [41][85/817]	Loss 0.0035 (0.0153)	
training:	Epoch: [41][86/817]	Loss 0.0049 (0.0152)	
training:	Epoch: [41][87/817]	Loss 0.0041 (0.0150)	
training:	Epoch: [41][88/817]	Loss 0.0038 (0.0149)	
training:	Epoch: [41][89/817]	Loss 0.0036 (0.0148)	
training:	Epoch: [41][90/817]	Loss 0.0059 (0.0147)	
training:	Epoch: [41][91/817]	Loss 0.0034 (0.0146)	
training:	Epoch: [41][92/817]	Loss 0.0025 (0.0144)	
training:	Epoch: [41][93/817]	Loss 0.0032 (0.0143)	
training:	Epoch: [41][94/817]	Loss 0.0069 (0.0142)	
training:	Epoch: [41][95/817]	Loss 0.0025 (0.0141)	
training:	Epoch: [41][96/817]	Loss 0.0032 (0.0140)	
training:	Epoch: [41][97/817]	Loss 0.0032 (0.0139)	
training:	Epoch: [41][98/817]	Loss 0.0037 (0.0138)	
training:	Epoch: [41][99/817]	Loss 0.0025 (0.0137)	
training:	Epoch: [41][100/817]	Loss 0.0057 (0.0136)	
training:	Epoch: [41][101/817]	Loss 0.0032 (0.0135)	
training:	Epoch: [41][102/817]	Loss 0.0050 (0.0134)	
training:	Epoch: [41][103/817]	Loss 0.0119 (0.0134)	
training:	Epoch: [41][104/817]	Loss 0.0110 (0.0134)	
training:	Epoch: [41][105/817]	Loss 0.0025 (0.0133)	
training:	Epoch: [41][106/817]	Loss 0.0053 (0.0132)	
training:	Epoch: [41][107/817]	Loss 0.0047 (0.0131)	
training:	Epoch: [41][108/817]	Loss 0.0042 (0.0130)	
training:	Epoch: [41][109/817]	Loss 0.0057 (0.0130)	
training:	Epoch: [41][110/817]	Loss 0.0024 (0.0129)	
training:	Epoch: [41][111/817]	Loss 0.0050 (0.0128)	
training:	Epoch: [41][112/817]	Loss 0.0053 (0.0127)	
training:	Epoch: [41][113/817]	Loss 0.0055 (0.0127)	
training:	Epoch: [41][114/817]	Loss 0.0027 (0.0126)	
training:	Epoch: [41][115/817]	Loss 0.0035 (0.0125)	
training:	Epoch: [41][116/817]	Loss 0.0039 (0.0124)	
training:	Epoch: [41][117/817]	Loss 0.0053 (0.0124)	
training:	Epoch: [41][118/817]	Loss 0.0023 (0.0123)	
training:	Epoch: [41][119/817]	Loss 0.0046 (0.0122)	
training:	Epoch: [41][120/817]	Loss 0.0030 (0.0121)	
training:	Epoch: [41][121/817]	Loss 0.0052 (0.0121)	
training:	Epoch: [41][122/817]	Loss 0.0108 (0.0121)	
training:	Epoch: [41][123/817]	Loss 0.0027 (0.0120)	
training:	Epoch: [41][124/817]	Loss 0.0020 (0.0119)	
training:	Epoch: [41][125/817]	Loss 0.0038 (0.0118)	
training:	Epoch: [41][126/817]	Loss 0.0047 (0.0118)	
training:	Epoch: [41][127/817]	Loss 0.4201 (0.0150)	
training:	Epoch: [41][128/817]	Loss 0.3485 (0.0176)	
training:	Epoch: [41][129/817]	Loss 0.0049 (0.0175)	
training:	Epoch: [41][130/817]	Loss 0.0035 (0.0174)	
training:	Epoch: [41][131/817]	Loss 0.0064 (0.0173)	
training:	Epoch: [41][132/817]	Loss 0.0031 (0.0172)	
training:	Epoch: [41][133/817]	Loss 0.0083 (0.0171)	
training:	Epoch: [41][134/817]	Loss 0.0120 (0.0171)	
training:	Epoch: [41][135/817]	Loss 0.0043 (0.0170)	
training:	Epoch: [41][136/817]	Loss 0.0072 (0.0169)	
training:	Epoch: [41][137/817]	Loss 0.0028 (0.0168)	
training:	Epoch: [41][138/817]	Loss 0.0147 (0.0168)	
training:	Epoch: [41][139/817]	Loss 0.0034 (0.0167)	
training:	Epoch: [41][140/817]	Loss 0.0041 (0.0166)	
training:	Epoch: [41][141/817]	Loss 0.0068 (0.0166)	
training:	Epoch: [41][142/817]	Loss 0.1176 (0.0173)	
training:	Epoch: [41][143/817]	Loss 0.0034 (0.0172)	
training:	Epoch: [41][144/817]	Loss 0.0028 (0.0171)	
training:	Epoch: [41][145/817]	Loss 0.0043 (0.0170)	
training:	Epoch: [41][146/817]	Loss 0.0021 (0.0169)	
training:	Epoch: [41][147/817]	Loss 0.0032 (0.0168)	
training:	Epoch: [41][148/817]	Loss 0.0097 (0.0167)	
training:	Epoch: [41][149/817]	Loss 0.0031 (0.0167)	
training:	Epoch: [41][150/817]	Loss 0.0096 (0.0166)	
training:	Epoch: [41][151/817]	Loss 0.0037 (0.0165)	
training:	Epoch: [41][152/817]	Loss 0.0023 (0.0164)	
training:	Epoch: [41][153/817]	Loss 0.0042 (0.0163)	
training:	Epoch: [41][154/817]	Loss 0.0096 (0.0163)	
training:	Epoch: [41][155/817]	Loss 0.4215 (0.0189)	
training:	Epoch: [41][156/817]	Loss 0.0062 (0.0188)	
training:	Epoch: [41][157/817]	Loss 0.0121 (0.0188)	
training:	Epoch: [41][158/817]	Loss 0.0028 (0.0187)	
training:	Epoch: [41][159/817]	Loss 0.0086 (0.0186)	
training:	Epoch: [41][160/817]	Loss 0.0054 (0.0185)	
training:	Epoch: [41][161/817]	Loss 0.0068 (0.0185)	
training:	Epoch: [41][162/817]	Loss 0.0099 (0.0184)	
training:	Epoch: [41][163/817]	Loss 0.0028 (0.0183)	
training:	Epoch: [41][164/817]	Loss 0.0059 (0.0182)	
training:	Epoch: [41][165/817]	Loss 0.0027 (0.0182)	
training:	Epoch: [41][166/817]	Loss 0.0052 (0.0181)	
training:	Epoch: [41][167/817]	Loss 0.0028 (0.0180)	
training:	Epoch: [41][168/817]	Loss 0.0098 (0.0179)	
training:	Epoch: [41][169/817]	Loss 0.0039 (0.0179)	
training:	Epoch: [41][170/817]	Loss 0.0043 (0.0178)	
training:	Epoch: [41][171/817]	Loss 0.0063 (0.0177)	
training:	Epoch: [41][172/817]	Loss 0.0024 (0.0176)	
training:	Epoch: [41][173/817]	Loss 0.0351 (0.0177)	
training:	Epoch: [41][174/817]	Loss 0.0080 (0.0177)	
training:	Epoch: [41][175/817]	Loss 0.0065 (0.0176)	
training:	Epoch: [41][176/817]	Loss 0.0027 (0.0175)	
training:	Epoch: [41][177/817]	Loss 0.0028 (0.0174)	
training:	Epoch: [41][178/817]	Loss 0.0053 (0.0174)	
training:	Epoch: [41][179/817]	Loss 0.0049 (0.0173)	
training:	Epoch: [41][180/817]	Loss 0.0029 (0.0172)	
training:	Epoch: [41][181/817]	Loss 0.0028 (0.0171)	
training:	Epoch: [41][182/817]	Loss 0.0038 (0.0171)	
training:	Epoch: [41][183/817]	Loss 0.0051 (0.0170)	
training:	Epoch: [41][184/817]	Loss 0.0673 (0.0173)	
training:	Epoch: [41][185/817]	Loss 0.0048 (0.0172)	
training:	Epoch: [41][186/817]	Loss 0.0027 (0.0171)	
training:	Epoch: [41][187/817]	Loss 0.0056 (0.0171)	
training:	Epoch: [41][188/817]	Loss 0.0025 (0.0170)	
training:	Epoch: [41][189/817]	Loss 0.0085 (0.0169)	
training:	Epoch: [41][190/817]	Loss 0.0030 (0.0169)	
training:	Epoch: [41][191/817]	Loss 0.0942 (0.0173)	
training:	Epoch: [41][192/817]	Loss 0.0041 (0.0172)	
training:	Epoch: [41][193/817]	Loss 0.0027 (0.0171)	
training:	Epoch: [41][194/817]	Loss 0.0048 (0.0171)	
training:	Epoch: [41][195/817]	Loss 0.0034 (0.0170)	
training:	Epoch: [41][196/817]	Loss 0.1563 (0.0177)	
training:	Epoch: [41][197/817]	Loss 0.0038 (0.0176)	
training:	Epoch: [41][198/817]	Loss 0.0030 (0.0176)	
training:	Epoch: [41][199/817]	Loss 0.0070 (0.0175)	
training:	Epoch: [41][200/817]	Loss 0.0024 (0.0174)	
training:	Epoch: [41][201/817]	Loss 0.0038 (0.0174)	
training:	Epoch: [41][202/817]	Loss 0.0033 (0.0173)	
training:	Epoch: [41][203/817]	Loss 0.0050 (0.0172)	
training:	Epoch: [41][204/817]	Loss 0.0141 (0.0172)	
training:	Epoch: [41][205/817]	Loss 0.0109 (0.0172)	
training:	Epoch: [41][206/817]	Loss 0.0027 (0.0171)	
training:	Epoch: [41][207/817]	Loss 0.0029 (0.0170)	
training:	Epoch: [41][208/817]	Loss 0.0034 (0.0170)	
training:	Epoch: [41][209/817]	Loss 0.0031 (0.0169)	
training:	Epoch: [41][210/817]	Loss 0.0036 (0.0169)	
training:	Epoch: [41][211/817]	Loss 0.0092 (0.0168)	
training:	Epoch: [41][212/817]	Loss 0.0038 (0.0168)	
training:	Epoch: [41][213/817]	Loss 0.0052 (0.0167)	
training:	Epoch: [41][214/817]	Loss 0.0071 (0.0167)	
training:	Epoch: [41][215/817]	Loss 0.0027 (0.0166)	
training:	Epoch: [41][216/817]	Loss 0.0037 (0.0165)	
training:	Epoch: [41][217/817]	Loss 0.0075 (0.0165)	
training:	Epoch: [41][218/817]	Loss 0.0043 (0.0164)	
training:	Epoch: [41][219/817]	Loss 0.0064 (0.0164)	
training:	Epoch: [41][220/817]	Loss 0.0040 (0.0163)	
training:	Epoch: [41][221/817]	Loss 0.0065 (0.0163)	
training:	Epoch: [41][222/817]	Loss 0.0055 (0.0162)	
training:	Epoch: [41][223/817]	Loss 0.0063 (0.0162)	
training:	Epoch: [41][224/817]	Loss 0.0029 (0.0161)	
training:	Epoch: [41][225/817]	Loss 0.2129 (0.0170)	
training:	Epoch: [41][226/817]	Loss 0.0067 (0.0170)	
training:	Epoch: [41][227/817]	Loss 0.0037 (0.0169)	
training:	Epoch: [41][228/817]	Loss 0.0040 (0.0168)	
training:	Epoch: [41][229/817]	Loss 0.0072 (0.0168)	
training:	Epoch: [41][230/817]	Loss 0.0042 (0.0168)	
training:	Epoch: [41][231/817]	Loss 0.0060 (0.0167)	
training:	Epoch: [41][232/817]	Loss 0.0035 (0.0166)	
training:	Epoch: [41][233/817]	Loss 0.0022 (0.0166)	
training:	Epoch: [41][234/817]	Loss 0.0105 (0.0166)	
training:	Epoch: [41][235/817]	Loss 0.0107 (0.0165)	
training:	Epoch: [41][236/817]	Loss 0.0118 (0.0165)	
training:	Epoch: [41][237/817]	Loss 0.0139 (0.0165)	
training:	Epoch: [41][238/817]	Loss 0.0065 (0.0165)	
training:	Epoch: [41][239/817]	Loss 0.0140 (0.0165)	
training:	Epoch: [41][240/817]	Loss 0.0037 (0.0164)	
training:	Epoch: [41][241/817]	Loss 0.0048 (0.0164)	
training:	Epoch: [41][242/817]	Loss 0.0069 (0.0163)	
training:	Epoch: [41][243/817]	Loss 0.0031 (0.0163)	
training:	Epoch: [41][244/817]	Loss 0.0068 (0.0162)	
training:	Epoch: [41][245/817]	Loss 0.0038 (0.0162)	
training:	Epoch: [41][246/817]	Loss 0.0031 (0.0161)	
training:	Epoch: [41][247/817]	Loss 0.0027 (0.0161)	
training:	Epoch: [41][248/817]	Loss 0.0083 (0.0160)	
training:	Epoch: [41][249/817]	Loss 0.0035 (0.0160)	
training:	Epoch: [41][250/817]	Loss 0.0059 (0.0159)	
training:	Epoch: [41][251/817]	Loss 0.0020 (0.0159)	
training:	Epoch: [41][252/817]	Loss 0.0025 (0.0158)	
training:	Epoch: [41][253/817]	Loss 0.0018 (0.0158)	
training:	Epoch: [41][254/817]	Loss 0.0040 (0.0157)	
training:	Epoch: [41][255/817]	Loss 0.0033 (0.0157)	
training:	Epoch: [41][256/817]	Loss 0.0030 (0.0156)	
training:	Epoch: [41][257/817]	Loss 0.0040 (0.0156)	
training:	Epoch: [41][258/817]	Loss 0.0042 (0.0155)	
training:	Epoch: [41][259/817]	Loss 0.0055 (0.0155)	
training:	Epoch: [41][260/817]	Loss 0.0027 (0.0155)	
training:	Epoch: [41][261/817]	Loss 0.0057 (0.0154)	
training:	Epoch: [41][262/817]	Loss 0.0104 (0.0154)	
training:	Epoch: [41][263/817]	Loss 0.0021 (0.0153)	
training:	Epoch: [41][264/817]	Loss 0.0034 (0.0153)	
training:	Epoch: [41][265/817]	Loss 0.0027 (0.0153)	
training:	Epoch: [41][266/817]	Loss 0.0031 (0.0152)	
training:	Epoch: [41][267/817]	Loss 0.0022 (0.0152)	
training:	Epoch: [41][268/817]	Loss 0.0032 (0.0151)	
training:	Epoch: [41][269/817]	Loss 0.0035 (0.0151)	
training:	Epoch: [41][270/817]	Loss 0.0027 (0.0150)	
training:	Epoch: [41][271/817]	Loss 0.0034 (0.0150)	
training:	Epoch: [41][272/817]	Loss 0.0036 (0.0149)	
training:	Epoch: [41][273/817]	Loss 0.2446 (0.0158)	
training:	Epoch: [41][274/817]	Loss 0.0207 (0.0158)	
training:	Epoch: [41][275/817]	Loss 0.0030 (0.0158)	
training:	Epoch: [41][276/817]	Loss 0.0048 (0.0157)	
training:	Epoch: [41][277/817]	Loss 0.3276 (0.0168)	
training:	Epoch: [41][278/817]	Loss 0.0055 (0.0168)	
training:	Epoch: [41][279/817]	Loss 0.0026 (0.0167)	
training:	Epoch: [41][280/817]	Loss 0.0052 (0.0167)	
training:	Epoch: [41][281/817]	Loss 0.0098 (0.0167)	
training:	Epoch: [41][282/817]	Loss 0.0112 (0.0167)	
training:	Epoch: [41][283/817]	Loss 0.0028 (0.0166)	
training:	Epoch: [41][284/817]	Loss 0.0028 (0.0166)	
training:	Epoch: [41][285/817]	Loss 0.0025 (0.0165)	
training:	Epoch: [41][286/817]	Loss 0.0050 (0.0165)	
training:	Epoch: [41][287/817]	Loss 0.0023 (0.0164)	
training:	Epoch: [41][288/817]	Loss 0.0031 (0.0164)	
training:	Epoch: [41][289/817]	Loss 0.0074 (0.0163)	
training:	Epoch: [41][290/817]	Loss 0.0022 (0.0163)	
training:	Epoch: [41][291/817]	Loss 0.0042 (0.0163)	
training:	Epoch: [41][292/817]	Loss 0.0256 (0.0163)	
training:	Epoch: [41][293/817]	Loss 0.0052 (0.0163)	
training:	Epoch: [41][294/817]	Loss 0.0067 (0.0162)	
training:	Epoch: [41][295/817]	Loss 0.0023 (0.0162)	
training:	Epoch: [41][296/817]	Loss 0.0030 (0.0161)	
training:	Epoch: [41][297/817]	Loss 0.0034 (0.0161)	
training:	Epoch: [41][298/817]	Loss 0.0051 (0.0160)	
training:	Epoch: [41][299/817]	Loss 0.0024 (0.0160)	
training:	Epoch: [41][300/817]	Loss 0.0030 (0.0160)	
training:	Epoch: [41][301/817]	Loss 0.0041 (0.0159)	
training:	Epoch: [41][302/817]	Loss 0.0035 (0.0159)	
training:	Epoch: [41][303/817]	Loss 0.0030 (0.0158)	
training:	Epoch: [41][304/817]	Loss 0.0062 (0.0158)	
training:	Epoch: [41][305/817]	Loss 0.0047 (0.0158)	
training:	Epoch: [41][306/817]	Loss 0.0033 (0.0157)	
training:	Epoch: [41][307/817]	Loss 0.5421 (0.0174)	
training:	Epoch: [41][308/817]	Loss 0.0081 (0.0174)	
training:	Epoch: [41][309/817]	Loss 0.0069 (0.0174)	
training:	Epoch: [41][310/817]	Loss 0.0028 (0.0173)	
training:	Epoch: [41][311/817]	Loss 0.0036 (0.0173)	
training:	Epoch: [41][312/817]	Loss 0.0037 (0.0172)	
training:	Epoch: [41][313/817]	Loss 0.0064 (0.0172)	
training:	Epoch: [41][314/817]	Loss 0.0031 (0.0172)	
training:	Epoch: [41][315/817]	Loss 0.0132 (0.0172)	
training:	Epoch: [41][316/817]	Loss 0.0099 (0.0171)	
training:	Epoch: [41][317/817]	Loss 0.0063 (0.0171)	
training:	Epoch: [41][318/817]	Loss 0.0048 (0.0171)	
training:	Epoch: [41][319/817]	Loss 0.0034 (0.0170)	
training:	Epoch: [41][320/817]	Loss 0.0024 (0.0170)	
training:	Epoch: [41][321/817]	Loss 0.0036 (0.0169)	
training:	Epoch: [41][322/817]	Loss 0.0025 (0.0169)	
training:	Epoch: [41][323/817]	Loss 0.0050 (0.0168)	
training:	Epoch: [41][324/817]	Loss 0.0033 (0.0168)	
training:	Epoch: [41][325/817]	Loss 0.0027 (0.0168)	
training:	Epoch: [41][326/817]	Loss 0.0032 (0.0167)	
training:	Epoch: [41][327/817]	Loss 0.0025 (0.0167)	
training:	Epoch: [41][328/817]	Loss 0.0039 (0.0166)	
training:	Epoch: [41][329/817]	Loss 0.0028 (0.0166)	
training:	Epoch: [41][330/817]	Loss 0.0038 (0.0166)	
training:	Epoch: [41][331/817]	Loss 0.0047 (0.0165)	
training:	Epoch: [41][332/817]	Loss 0.0089 (0.0165)	
training:	Epoch: [41][333/817]	Loss 0.0104 (0.0165)	
training:	Epoch: [41][334/817]	Loss 0.0039 (0.0164)	
training:	Epoch: [41][335/817]	Loss 0.0042 (0.0164)	
training:	Epoch: [41][336/817]	Loss 0.0056 (0.0164)	
training:	Epoch: [41][337/817]	Loss 0.0027 (0.0163)	
training:	Epoch: [41][338/817]	Loss 0.0032 (0.0163)	
training:	Epoch: [41][339/817]	Loss 0.0024 (0.0162)	
training:	Epoch: [41][340/817]	Loss 0.0046 (0.0162)	
training:	Epoch: [41][341/817]	Loss 0.0539 (0.0163)	
training:	Epoch: [41][342/817]	Loss 0.0087 (0.0163)	
training:	Epoch: [41][343/817]	Loss 0.0056 (0.0163)	
training:	Epoch: [41][344/817]	Loss 0.0086 (0.0162)	
training:	Epoch: [41][345/817]	Loss 0.0028 (0.0162)	
training:	Epoch: [41][346/817]	Loss 0.0029 (0.0162)	
training:	Epoch: [41][347/817]	Loss 0.0031 (0.0161)	
training:	Epoch: [41][348/817]	Loss 0.0024 (0.0161)	
training:	Epoch: [41][349/817]	Loss 0.0018 (0.0161)	
training:	Epoch: [41][350/817]	Loss 0.0032 (0.0160)	
training:	Epoch: [41][351/817]	Loss 0.0024 (0.0160)	
training:	Epoch: [41][352/817]	Loss 0.0026 (0.0159)	
training:	Epoch: [41][353/817]	Loss 0.0078 (0.0159)	
training:	Epoch: [41][354/817]	Loss 0.0072 (0.0159)	
training:	Epoch: [41][355/817]	Loss 0.0019 (0.0159)	
training:	Epoch: [41][356/817]	Loss 0.0037 (0.0158)	
training:	Epoch: [41][357/817]	Loss 0.0057 (0.0158)	
training:	Epoch: [41][358/817]	Loss 0.0039 (0.0158)	
training:	Epoch: [41][359/817]	Loss 0.0029 (0.0157)	
training:	Epoch: [41][360/817]	Loss 0.0033 (0.0157)	
training:	Epoch: [41][361/817]	Loss 0.0046 (0.0157)	
training:	Epoch: [41][362/817]	Loss 0.0054 (0.0156)	
training:	Epoch: [41][363/817]	Loss 0.0062 (0.0156)	
training:	Epoch: [41][364/817]	Loss 0.0093 (0.0156)	
training:	Epoch: [41][365/817]	Loss 0.0046 (0.0156)	
training:	Epoch: [41][366/817]	Loss 0.0023 (0.0155)	
training:	Epoch: [41][367/817]	Loss 0.0060 (0.0155)	
training:	Epoch: [41][368/817]	Loss 0.0054 (0.0155)	
training:	Epoch: [41][369/817]	Loss 0.0038 (0.0154)	
training:	Epoch: [41][370/817]	Loss 0.0027 (0.0154)	
training:	Epoch: [41][371/817]	Loss 0.0054 (0.0154)	
training:	Epoch: [41][372/817]	Loss 0.0029 (0.0153)	
training:	Epoch: [41][373/817]	Loss 0.0021 (0.0153)	
training:	Epoch: [41][374/817]	Loss 0.1799 (0.0157)	
training:	Epoch: [41][375/817]	Loss 0.0027 (0.0157)	
training:	Epoch: [41][376/817]	Loss 0.0037 (0.0157)	
training:	Epoch: [41][377/817]	Loss 0.0030 (0.0156)	
training:	Epoch: [41][378/817]	Loss 0.0053 (0.0156)	
training:	Epoch: [41][379/817]	Loss 0.0095 (0.0156)	
training:	Epoch: [41][380/817]	Loss 0.0019 (0.0156)	
training:	Epoch: [41][381/817]	Loss 0.0034 (0.0155)	
training:	Epoch: [41][382/817]	Loss 0.0028 (0.0155)	
training:	Epoch: [41][383/817]	Loss 0.0104 (0.0155)	
training:	Epoch: [41][384/817]	Loss 0.0141 (0.0155)	
training:	Epoch: [41][385/817]	Loss 0.0029 (0.0154)	
training:	Epoch: [41][386/817]	Loss 0.0127 (0.0154)	
training:	Epoch: [41][387/817]	Loss 0.0059 (0.0154)	
training:	Epoch: [41][388/817]	Loss 0.0048 (0.0154)	
training:	Epoch: [41][389/817]	Loss 0.0029 (0.0154)	
training:	Epoch: [41][390/817]	Loss 0.0035 (0.0153)	
training:	Epoch: [41][391/817]	Loss 0.0029 (0.0153)	
training:	Epoch: [41][392/817]	Loss 0.0023 (0.0153)	
training:	Epoch: [41][393/817]	Loss 0.0042 (0.0152)	
training:	Epoch: [41][394/817]	Loss 0.0137 (0.0152)	
training:	Epoch: [41][395/817]	Loss 0.0024 (0.0152)	
training:	Epoch: [41][396/817]	Loss 0.0026 (0.0152)	
training:	Epoch: [41][397/817]	Loss 0.0030 (0.0151)	
training:	Epoch: [41][398/817]	Loss 0.0023 (0.0151)	
training:	Epoch: [41][399/817]	Loss 0.0060 (0.0151)	
training:	Epoch: [41][400/817]	Loss 0.0050 (0.0151)	
training:	Epoch: [41][401/817]	Loss 0.0025 (0.0150)	
training:	Epoch: [41][402/817]	Loss 0.0046 (0.0150)	
training:	Epoch: [41][403/817]	Loss 0.0028 (0.0150)	
training:	Epoch: [41][404/817]	Loss 0.0024 (0.0149)	
training:	Epoch: [41][405/817]	Loss 0.0020 (0.0149)	
training:	Epoch: [41][406/817]	Loss 0.0052 (0.0149)	
training:	Epoch: [41][407/817]	Loss 0.0038 (0.0149)	
training:	Epoch: [41][408/817]	Loss 0.0137 (0.0148)	
training:	Epoch: [41][409/817]	Loss 0.0047 (0.0148)	
training:	Epoch: [41][410/817]	Loss 0.0036 (0.0148)	
training:	Epoch: [41][411/817]	Loss 0.0051 (0.0148)	
training:	Epoch: [41][412/817]	Loss 0.0024 (0.0147)	
training:	Epoch: [41][413/817]	Loss 0.0101 (0.0147)	
training:	Epoch: [41][414/817]	Loss 0.0037 (0.0147)	
training:	Epoch: [41][415/817]	Loss 0.0021 (0.0147)	
training:	Epoch: [41][416/817]	Loss 0.0061 (0.0147)	
training:	Epoch: [41][417/817]	Loss 0.0030 (0.0146)	
training:	Epoch: [41][418/817]	Loss 0.0034 (0.0146)	
training:	Epoch: [41][419/817]	Loss 0.0031 (0.0146)	
training:	Epoch: [41][420/817]	Loss 0.0028 (0.0145)	
training:	Epoch: [41][421/817]	Loss 0.0020 (0.0145)	
training:	Epoch: [41][422/817]	Loss 0.0024 (0.0145)	
training:	Epoch: [41][423/817]	Loss 0.0027 (0.0145)	
training:	Epoch: [41][424/817]	Loss 0.0027 (0.0144)	
training:	Epoch: [41][425/817]	Loss 0.0028 (0.0144)	
training:	Epoch: [41][426/817]	Loss 0.0027 (0.0144)	
training:	Epoch: [41][427/817]	Loss 0.0029 (0.0143)	
training:	Epoch: [41][428/817]	Loss 0.0026 (0.0143)	
training:	Epoch: [41][429/817]	Loss 0.0137 (0.0143)	
training:	Epoch: [41][430/817]	Loss 0.0029 (0.0143)	
training:	Epoch: [41][431/817]	Loss 0.0029 (0.0143)	
training:	Epoch: [41][432/817]	Loss 0.0039 (0.0142)	
training:	Epoch: [41][433/817]	Loss 0.0032 (0.0142)	
training:	Epoch: [41][434/817]	Loss 0.0048 (0.0142)	
training:	Epoch: [41][435/817]	Loss 0.0024 (0.0142)	
training:	Epoch: [41][436/817]	Loss 0.0019 (0.0141)	
training:	Epoch: [41][437/817]	Loss 0.0026 (0.0141)	
training:	Epoch: [41][438/817]	Loss 0.1028 (0.0143)	
training:	Epoch: [41][439/817]	Loss 0.0038 (0.0143)	
training:	Epoch: [41][440/817]	Loss 0.0074 (0.0143)	
training:	Epoch: [41][441/817]	Loss 0.0027 (0.0143)	
training:	Epoch: [41][442/817]	Loss 0.0132 (0.0142)	
training:	Epoch: [41][443/817]	Loss 0.0157 (0.0143)	
training:	Epoch: [41][444/817]	Loss 0.0048 (0.0142)	
training:	Epoch: [41][445/817]	Loss 0.0033 (0.0142)	
training:	Epoch: [41][446/817]	Loss 0.0074 (0.0142)	
training:	Epoch: [41][447/817]	Loss 0.0056 (0.0142)	
training:	Epoch: [41][448/817]	Loss 0.0031 (0.0141)	
training:	Epoch: [41][449/817]	Loss 0.1351 (0.0144)	
training:	Epoch: [41][450/817]	Loss 0.0069 (0.0144)	
training:	Epoch: [41][451/817]	Loss 0.0018 (0.0144)	
training:	Epoch: [41][452/817]	Loss 0.0022 (0.0143)	
training:	Epoch: [41][453/817]	Loss 0.0037 (0.0143)	
training:	Epoch: [41][454/817]	Loss 0.0046 (0.0143)	
training:	Epoch: [41][455/817]	Loss 0.0052 (0.0143)	
training:	Epoch: [41][456/817]	Loss 0.0028 (0.0143)	
training:	Epoch: [41][457/817]	Loss 0.0032 (0.0142)	
training:	Epoch: [41][458/817]	Loss 0.0086 (0.0142)	
training:	Epoch: [41][459/817]	Loss 0.0785 (0.0144)	
training:	Epoch: [41][460/817]	Loss 0.0022 (0.0143)	
training:	Epoch: [41][461/817]	Loss 0.0034 (0.0143)	
training:	Epoch: [41][462/817]	Loss 0.0026 (0.0143)	
training:	Epoch: [41][463/817]	Loss 0.2806 (0.0149)	
training:	Epoch: [41][464/817]	Loss 0.0024 (0.0148)	
training:	Epoch: [41][465/817]	Loss 0.0034 (0.0148)	
training:	Epoch: [41][466/817]	Loss 0.0025 (0.0148)	
training:	Epoch: [41][467/817]	Loss 0.5186 (0.0159)	
training:	Epoch: [41][468/817]	Loss 0.0022 (0.0158)	
training:	Epoch: [41][469/817]	Loss 0.0022 (0.0158)	
training:	Epoch: [41][470/817]	Loss 0.0030 (0.0158)	
training:	Epoch: [41][471/817]	Loss 0.0027 (0.0157)	
training:	Epoch: [41][472/817]	Loss 0.0037 (0.0157)	
training:	Epoch: [41][473/817]	Loss 0.0039 (0.0157)	
training:	Epoch: [41][474/817]	Loss 0.0028 (0.0157)	
training:	Epoch: [41][475/817]	Loss 0.0036 (0.0156)	
training:	Epoch: [41][476/817]	Loss 0.0043 (0.0156)	
training:	Epoch: [41][477/817]	Loss 0.0055 (0.0156)	
training:	Epoch: [41][478/817]	Loss 0.0027 (0.0156)	
training:	Epoch: [41][479/817]	Loss 0.0093 (0.0156)	
training:	Epoch: [41][480/817]	Loss 0.0040 (0.0155)	
training:	Epoch: [41][481/817]	Loss 0.0032 (0.0155)	
training:	Epoch: [41][482/817]	Loss 0.0035 (0.0155)	
training:	Epoch: [41][483/817]	Loss 0.0109 (0.0155)	
training:	Epoch: [41][484/817]	Loss 0.0019 (0.0154)	
training:	Epoch: [41][485/817]	Loss 0.0026 (0.0154)	
training:	Epoch: [41][486/817]	Loss 0.0130 (0.0154)	
training:	Epoch: [41][487/817]	Loss 0.0027 (0.0154)	
training:	Epoch: [41][488/817]	Loss 0.4162 (0.0162)	
training:	Epoch: [41][489/817]	Loss 0.0109 (0.0162)	
training:	Epoch: [41][490/817]	Loss 0.0060 (0.0162)	
training:	Epoch: [41][491/817]	Loss 0.0024 (0.0161)	
training:	Epoch: [41][492/817]	Loss 0.0057 (0.0161)	
training:	Epoch: [41][493/817]	Loss 0.0024 (0.0161)	
training:	Epoch: [41][494/817]	Loss 0.0110 (0.0161)	
training:	Epoch: [41][495/817]	Loss 0.0044 (0.0161)	
training:	Epoch: [41][496/817]	Loss 0.0294 (0.0161)	
training:	Epoch: [41][497/817]	Loss 0.0049 (0.0161)	
training:	Epoch: [41][498/817]	Loss 0.0077 (0.0161)	
training:	Epoch: [41][499/817]	Loss 0.0056 (0.0160)	
training:	Epoch: [41][500/817]	Loss 0.0018 (0.0160)	
training:	Epoch: [41][501/817]	Loss 0.0021 (0.0160)	
training:	Epoch: [41][502/817]	Loss 0.0079 (0.0160)	
training:	Epoch: [41][503/817]	Loss 0.0023 (0.0159)	
training:	Epoch: [41][504/817]	Loss 0.0068 (0.0159)	
training:	Epoch: [41][505/817]	Loss 0.0078 (0.0159)	
training:	Epoch: [41][506/817]	Loss 0.0023 (0.0159)	
training:	Epoch: [41][507/817]	Loss 0.0028 (0.0158)	
training:	Epoch: [41][508/817]	Loss 0.0043 (0.0158)	
training:	Epoch: [41][509/817]	Loss 0.0062 (0.0158)	
training:	Epoch: [41][510/817]	Loss 0.0025 (0.0158)	
training:	Epoch: [41][511/817]	Loss 0.0043 (0.0158)	
training:	Epoch: [41][512/817]	Loss 0.0065 (0.0157)	
training:	Epoch: [41][513/817]	Loss 0.0031 (0.0157)	
training:	Epoch: [41][514/817]	Loss 0.0046 (0.0157)	
training:	Epoch: [41][515/817]	Loss 0.0038 (0.0157)	
training:	Epoch: [41][516/817]	Loss 0.0207 (0.0157)	
training:	Epoch: [41][517/817]	Loss 0.0039 (0.0157)	
training:	Epoch: [41][518/817]	Loss 0.0050 (0.0156)	
training:	Epoch: [41][519/817]	Loss 0.0030 (0.0156)	
training:	Epoch: [41][520/817]	Loss 0.0059 (0.0156)	
training:	Epoch: [41][521/817]	Loss 0.0036 (0.0156)	
training:	Epoch: [41][522/817]	Loss 0.0067 (0.0156)	
training:	Epoch: [41][523/817]	Loss 0.0036 (0.0155)	
training:	Epoch: [41][524/817]	Loss 0.0021 (0.0155)	
training:	Epoch: [41][525/817]	Loss 0.0027 (0.0155)	
training:	Epoch: [41][526/817]	Loss 0.0136 (0.0155)	
training:	Epoch: [41][527/817]	Loss 0.0027 (0.0155)	
training:	Epoch: [41][528/817]	Loss 0.0030 (0.0154)	
training:	Epoch: [41][529/817]	Loss 0.0065 (0.0154)	
training:	Epoch: [41][530/817]	Loss 0.0025 (0.0154)	
training:	Epoch: [41][531/817]	Loss 0.0045 (0.0154)	
training:	Epoch: [41][532/817]	Loss 0.0035 (0.0153)	
training:	Epoch: [41][533/817]	Loss 0.0022 (0.0153)	
training:	Epoch: [41][534/817]	Loss 0.0155 (0.0153)	
training:	Epoch: [41][535/817]	Loss 0.0101 (0.0153)	
training:	Epoch: [41][536/817]	Loss 0.0134 (0.0153)	
training:	Epoch: [41][537/817]	Loss 0.0022 (0.0153)	
training:	Epoch: [41][538/817]	Loss 0.0022 (0.0153)	
training:	Epoch: [41][539/817]	Loss 0.0022 (0.0152)	
training:	Epoch: [41][540/817]	Loss 0.0086 (0.0152)	
training:	Epoch: [41][541/817]	Loss 0.0037 (0.0152)	
training:	Epoch: [41][542/817]	Loss 0.0028 (0.0152)	
training:	Epoch: [41][543/817]	Loss 0.0100 (0.0152)	
training:	Epoch: [41][544/817]	Loss 0.0081 (0.0152)	
training:	Epoch: [41][545/817]	Loss 0.0036 (0.0151)	
training:	Epoch: [41][546/817]	Loss 0.0023 (0.0151)	
training:	Epoch: [41][547/817]	Loss 0.0023 (0.0151)	
training:	Epoch: [41][548/817]	Loss 0.0037 (0.0151)	
training:	Epoch: [41][549/817]	Loss 0.0032 (0.0150)	
training:	Epoch: [41][550/817]	Loss 0.0019 (0.0150)	
training:	Epoch: [41][551/817]	Loss 0.0017 (0.0150)	
training:	Epoch: [41][552/817]	Loss 0.0028 (0.0150)	
training:	Epoch: [41][553/817]	Loss 0.0030 (0.0150)	
training:	Epoch: [41][554/817]	Loss 0.4823 (0.0158)	
training:	Epoch: [41][555/817]	Loss 0.0037 (0.0158)	
training:	Epoch: [41][556/817]	Loss 0.0035 (0.0158)	
training:	Epoch: [41][557/817]	Loss 0.0027 (0.0157)	
training:	Epoch: [41][558/817]	Loss 0.0031 (0.0157)	
training:	Epoch: [41][559/817]	Loss 0.0061 (0.0157)	
training:	Epoch: [41][560/817]	Loss 0.0061 (0.0157)	
training:	Epoch: [41][561/817]	Loss 0.0041 (0.0157)	
training:	Epoch: [41][562/817]	Loss 0.0026 (0.0156)	
training:	Epoch: [41][563/817]	Loss 0.2046 (0.0160)	
training:	Epoch: [41][564/817]	Loss 0.0057 (0.0159)	
training:	Epoch: [41][565/817]	Loss 0.0059 (0.0159)	
training:	Epoch: [41][566/817]	Loss 0.0038 (0.0159)	
training:	Epoch: [41][567/817]	Loss 0.0026 (0.0159)	
training:	Epoch: [41][568/817]	Loss 0.0034 (0.0159)	
training:	Epoch: [41][569/817]	Loss 0.0113 (0.0159)	
training:	Epoch: [41][570/817]	Loss 0.7306 (0.0171)	
training:	Epoch: [41][571/817]	Loss 0.0047 (0.0171)	
training:	Epoch: [41][572/817]	Loss 0.0025 (0.0171)	
training:	Epoch: [41][573/817]	Loss 0.0186 (0.0171)	
training:	Epoch: [41][574/817]	Loss 0.0030 (0.0170)	
training:	Epoch: [41][575/817]	Loss 0.5021 (0.0179)	
training:	Epoch: [41][576/817]	Loss 0.0022 (0.0179)	
training:	Epoch: [41][577/817]	Loss 0.0170 (0.0179)	
training:	Epoch: [41][578/817]	Loss 0.0039 (0.0178)	
training:	Epoch: [41][579/817]	Loss 0.0043 (0.0178)	
training:	Epoch: [41][580/817]	Loss 0.0041 (0.0178)	
training:	Epoch: [41][581/817]	Loss 0.0082 (0.0178)	
training:	Epoch: [41][582/817]	Loss 0.0042 (0.0177)	
training:	Epoch: [41][583/817]	Loss 0.2253 (0.0181)	
training:	Epoch: [41][584/817]	Loss 0.0059 (0.0181)	
training:	Epoch: [41][585/817]	Loss 0.3874 (0.0187)	
training:	Epoch: [41][586/817]	Loss 0.0026 (0.0187)	
training:	Epoch: [41][587/817]	Loss 0.0032 (0.0187)	
training:	Epoch: [41][588/817]	Loss 0.0175 (0.0187)	
training:	Epoch: [41][589/817]	Loss 0.0038 (0.0186)	
training:	Epoch: [41][590/817]	Loss 0.0043 (0.0186)	
training:	Epoch: [41][591/817]	Loss 0.0029 (0.0186)	
training:	Epoch: [41][592/817]	Loss 0.0059 (0.0186)	
training:	Epoch: [41][593/817]	Loss 0.0103 (0.0185)	
training:	Epoch: [41][594/817]	Loss 0.0664 (0.0186)	
training:	Epoch: [41][595/817]	Loss 0.0023 (0.0186)	
training:	Epoch: [41][596/817]	Loss 0.0069 (0.0186)	
training:	Epoch: [41][597/817]	Loss 0.0022 (0.0185)	
training:	Epoch: [41][598/817]	Loss 0.0054 (0.0185)	
training:	Epoch: [41][599/817]	Loss 0.0510 (0.0186)	
training:	Epoch: [41][600/817]	Loss 0.0050 (0.0186)	
training:	Epoch: [41][601/817]	Loss 0.0256 (0.0186)	
training:	Epoch: [41][602/817]	Loss 0.0050 (0.0185)	
training:	Epoch: [41][603/817]	Loss 0.0278 (0.0186)	
training:	Epoch: [41][604/817]	Loss 0.0124 (0.0186)	
training:	Epoch: [41][605/817]	Loss 0.0054 (0.0185)	
training:	Epoch: [41][606/817]	Loss 0.0053 (0.0185)	
training:	Epoch: [41][607/817]	Loss 0.1806 (0.0188)	
training:	Epoch: [41][608/817]	Loss 0.0076 (0.0188)	
training:	Epoch: [41][609/817]	Loss 0.0054 (0.0187)	
training:	Epoch: [41][610/817]	Loss 0.0095 (0.0187)	
training:	Epoch: [41][611/817]	Loss 0.0037 (0.0187)	
training:	Epoch: [41][612/817]	Loss 0.0042 (0.0187)	
training:	Epoch: [41][613/817]	Loss 0.0048 (0.0186)	
training:	Epoch: [41][614/817]	Loss 0.0036 (0.0186)	
training:	Epoch: [41][615/817]	Loss 0.0058 (0.0186)	
training:	Epoch: [41][616/817]	Loss 0.0027 (0.0186)	
training:	Epoch: [41][617/817]	Loss 0.0048 (0.0186)	
training:	Epoch: [41][618/817]	Loss 0.0026 (0.0185)	
training:	Epoch: [41][619/817]	Loss 0.0158 (0.0185)	
training:	Epoch: [41][620/817]	Loss 0.0034 (0.0185)	
training:	Epoch: [41][621/817]	Loss 0.0049 (0.0185)	
training:	Epoch: [41][622/817]	Loss 0.0089 (0.0185)	
training:	Epoch: [41][623/817]	Loss 0.0035 (0.0184)	
training:	Epoch: [41][624/817]	Loss 0.0024 (0.0184)	
training:	Epoch: [41][625/817]	Loss 0.0080 (0.0184)	
training:	Epoch: [41][626/817]	Loss 0.0025 (0.0184)	
training:	Epoch: [41][627/817]	Loss 0.0053 (0.0183)	
training:	Epoch: [41][628/817]	Loss 0.0046 (0.0183)	
training:	Epoch: [41][629/817]	Loss 0.0062 (0.0183)	
training:	Epoch: [41][630/817]	Loss 0.0060 (0.0183)	
training:	Epoch: [41][631/817]	Loss 0.0023 (0.0183)	
training:	Epoch: [41][632/817]	Loss 0.0335 (0.0183)	
training:	Epoch: [41][633/817]	Loss 0.0030 (0.0183)	
training:	Epoch: [41][634/817]	Loss 0.0166 (0.0183)	
training:	Epoch: [41][635/817]	Loss 0.0049 (0.0182)	
training:	Epoch: [41][636/817]	Loss 0.0038 (0.0182)	
training:	Epoch: [41][637/817]	Loss 0.0046 (0.0182)	
training:	Epoch: [41][638/817]	Loss 0.0118 (0.0182)	
training:	Epoch: [41][639/817]	Loss 0.0070 (0.0182)	
training:	Epoch: [41][640/817]	Loss 0.0189 (0.0182)	
training:	Epoch: [41][641/817]	Loss 0.0065 (0.0182)	
training:	Epoch: [41][642/817]	Loss 0.0036 (0.0181)	
training:	Epoch: [41][643/817]	Loss 0.0024 (0.0181)	
training:	Epoch: [41][644/817]	Loss 0.0143 (0.0181)	
training:	Epoch: [41][645/817]	Loss 0.0047 (0.0181)	
training:	Epoch: [41][646/817]	Loss 0.0056 (0.0181)	
training:	Epoch: [41][647/817]	Loss 0.0108 (0.0180)	
training:	Epoch: [41][648/817]	Loss 0.0040 (0.0180)	
training:	Epoch: [41][649/817]	Loss 0.0045 (0.0180)	
training:	Epoch: [41][650/817]	Loss 0.0024 (0.0180)	
training:	Epoch: [41][651/817]	Loss 0.0050 (0.0180)	
training:	Epoch: [41][652/817]	Loss 0.0045 (0.0179)	
training:	Epoch: [41][653/817]	Loss 0.0093 (0.0179)	
training:	Epoch: [41][654/817]	Loss 0.0043 (0.0179)	
training:	Epoch: [41][655/817]	Loss 0.0073 (0.0179)	
training:	Epoch: [41][656/817]	Loss 0.0150 (0.0179)	
training:	Epoch: [41][657/817]	Loss 0.0075 (0.0179)	
training:	Epoch: [41][658/817]	Loss 0.0081 (0.0179)	
training:	Epoch: [41][659/817]	Loss 0.0032 (0.0178)	
training:	Epoch: [41][660/817]	Loss 0.0022 (0.0178)	
training:	Epoch: [41][661/817]	Loss 0.0029 (0.0178)	
training:	Epoch: [41][662/817]	Loss 0.0037 (0.0178)	
training:	Epoch: [41][663/817]	Loss 0.0034 (0.0177)	
training:	Epoch: [41][664/817]	Loss 0.0037 (0.0177)	
training:	Epoch: [41][665/817]	Loss 0.0019 (0.0177)	
training:	Epoch: [41][666/817]	Loss 0.0032 (0.0177)	
training:	Epoch: [41][667/817]	Loss 0.0045 (0.0177)	
training:	Epoch: [41][668/817]	Loss 0.0028 (0.0176)	
training:	Epoch: [41][669/817]	Loss 0.0025 (0.0176)	
training:	Epoch: [41][670/817]	Loss 0.0021 (0.0176)	
training:	Epoch: [41][671/817]	Loss 0.0028 (0.0176)	
training:	Epoch: [41][672/817]	Loss 0.0086 (0.0176)	
training:	Epoch: [41][673/817]	Loss 0.0030 (0.0175)	
training:	Epoch: [41][674/817]	Loss 0.0037 (0.0175)	
training:	Epoch: [41][675/817]	Loss 0.0036 (0.0175)	
training:	Epoch: [41][676/817]	Loss 0.0024 (0.0175)	
training:	Epoch: [41][677/817]	Loss 0.0023 (0.0174)	
training:	Epoch: [41][678/817]	Loss 0.0028 (0.0174)	
training:	Epoch: [41][679/817]	Loss 0.0053 (0.0174)	
training:	Epoch: [41][680/817]	Loss 0.0021 (0.0174)	
training:	Epoch: [41][681/817]	Loss 0.0032 (0.0174)	
training:	Epoch: [41][682/817]	Loss 0.0020 (0.0173)	
training:	Epoch: [41][683/817]	Loss 0.3847 (0.0179)	
training:	Epoch: [41][684/817]	Loss 0.0060 (0.0179)	
training:	Epoch: [41][685/817]	Loss 0.0048 (0.0178)	
training:	Epoch: [41][686/817]	Loss 0.0034 (0.0178)	
training:	Epoch: [41][687/817]	Loss 0.0029 (0.0178)	
training:	Epoch: [41][688/817]	Loss 0.0023 (0.0178)	
training:	Epoch: [41][689/817]	Loss 0.0032 (0.0178)	
training:	Epoch: [41][690/817]	Loss 0.0054 (0.0177)	
training:	Epoch: [41][691/817]	Loss 0.0031 (0.0177)	
training:	Epoch: [41][692/817]	Loss 0.0028 (0.0177)	
training:	Epoch: [41][693/817]	Loss 0.0029 (0.0177)	
training:	Epoch: [41][694/817]	Loss 0.0027 (0.0177)	
training:	Epoch: [41][695/817]	Loss 0.0055 (0.0176)	
training:	Epoch: [41][696/817]	Loss 0.0026 (0.0176)	
training:	Epoch: [41][697/817]	Loss 0.0039 (0.0176)	
training:	Epoch: [41][698/817]	Loss 0.0019 (0.0176)	
training:	Epoch: [41][699/817]	Loss 0.0085 (0.0176)	
training:	Epoch: [41][700/817]	Loss 0.0036 (0.0175)	
training:	Epoch: [41][701/817]	Loss 0.0053 (0.0175)	
training:	Epoch: [41][702/817]	Loss 0.0027 (0.0175)	
training:	Epoch: [41][703/817]	Loss 0.0088 (0.0175)	
training:	Epoch: [41][704/817]	Loss 0.0046 (0.0175)	
training:	Epoch: [41][705/817]	Loss 0.0034 (0.0174)	
training:	Epoch: [41][706/817]	Loss 0.0055 (0.0174)	
training:	Epoch: [41][707/817]	Loss 0.0020 (0.0174)	
training:	Epoch: [41][708/817]	Loss 0.0109 (0.0174)	
training:	Epoch: [41][709/817]	Loss 0.0447 (0.0174)	
training:	Epoch: [41][710/817]	Loss 0.0053 (0.0174)	
training:	Epoch: [41][711/817]	Loss 0.0039 (0.0174)	
training:	Epoch: [41][712/817]	Loss 0.0086 (0.0174)	
training:	Epoch: [41][713/817]	Loss 0.0052 (0.0174)	
training:	Epoch: [41][714/817]	Loss 0.0040 (0.0174)	
training:	Epoch: [41][715/817]	Loss 0.0024 (0.0173)	
training:	Epoch: [41][716/817]	Loss 0.2346 (0.0176)	
training:	Epoch: [41][717/817]	Loss 0.0020 (0.0176)	
training:	Epoch: [41][718/817]	Loss 0.0154 (0.0176)	
training:	Epoch: [41][719/817]	Loss 0.5966 (0.0184)	
training:	Epoch: [41][720/817]	Loss 0.0029 (0.0184)	
training:	Epoch: [41][721/817]	Loss 0.0053 (0.0184)	
training:	Epoch: [41][722/817]	Loss 0.0201 (0.0184)	
training:	Epoch: [41][723/817]	Loss 0.0051 (0.0184)	
training:	Epoch: [41][724/817]	Loss 0.0505 (0.0184)	
training:	Epoch: [41][725/817]	Loss 0.0035 (0.0184)	
training:	Epoch: [41][726/817]	Loss 0.0032 (0.0184)	
training:	Epoch: [41][727/817]	Loss 0.0032 (0.0183)	
training:	Epoch: [41][728/817]	Loss 0.0048 (0.0183)	
training:	Epoch: [41][729/817]	Loss 0.0076 (0.0183)	
training:	Epoch: [41][730/817]	Loss 0.0037 (0.0183)	
training:	Epoch: [41][731/817]	Loss 0.0110 (0.0183)	
training:	Epoch: [41][732/817]	Loss 0.0027 (0.0183)	
training:	Epoch: [41][733/817]	Loss 0.0067 (0.0182)	
training:	Epoch: [41][734/817]	Loss 0.0035 (0.0182)	
training:	Epoch: [41][735/817]	Loss 0.0028 (0.0182)	
training:	Epoch: [41][736/817]	Loss 0.0026 (0.0182)	
training:	Epoch: [41][737/817]	Loss 0.0094 (0.0182)	
training:	Epoch: [41][738/817]	Loss 0.0023 (0.0181)	
training:	Epoch: [41][739/817]	Loss 0.0053 (0.0181)	
training:	Epoch: [41][740/817]	Loss 0.0048 (0.0181)	
training:	Epoch: [41][741/817]	Loss 0.0023 (0.0181)	
training:	Epoch: [41][742/817]	Loss 0.0054 (0.0181)	
training:	Epoch: [41][743/817]	Loss 0.0072 (0.0181)	
training:	Epoch: [41][744/817]	Loss 0.0053 (0.0180)	
training:	Epoch: [41][745/817]	Loss 0.0070 (0.0180)	
training:	Epoch: [41][746/817]	Loss 0.0052 (0.0180)	
training:	Epoch: [41][747/817]	Loss 0.0035 (0.0180)	
training:	Epoch: [41][748/817]	Loss 0.0074 (0.0180)	
training:	Epoch: [41][749/817]	Loss 0.0104 (0.0180)	
training:	Epoch: [41][750/817]	Loss 0.0044 (0.0179)	
training:	Epoch: [41][751/817]	Loss 0.0019 (0.0179)	
training:	Epoch: [41][752/817]	Loss 0.0042 (0.0179)	
training:	Epoch: [41][753/817]	Loss 0.0030 (0.0179)	
training:	Epoch: [41][754/817]	Loss 0.0040 (0.0179)	
training:	Epoch: [41][755/817]	Loss 0.0031 (0.0179)	
training:	Epoch: [41][756/817]	Loss 0.0044 (0.0178)	
training:	Epoch: [41][757/817]	Loss 0.0041 (0.0178)	
training:	Epoch: [41][758/817]	Loss 0.0052 (0.0178)	
training:	Epoch: [41][759/817]	Loss 0.0115 (0.0178)	
training:	Epoch: [41][760/817]	Loss 0.0045 (0.0178)	
training:	Epoch: [41][761/817]	Loss 0.0022 (0.0178)	
training:	Epoch: [41][762/817]	Loss 0.0061 (0.0177)	
training:	Epoch: [41][763/817]	Loss 0.0017 (0.0177)	
training:	Epoch: [41][764/817]	Loss 0.0041 (0.0177)	
training:	Epoch: [41][765/817]	Loss 0.0046 (0.0177)	
training:	Epoch: [41][766/817]	Loss 0.0044 (0.0177)	
training:	Epoch: [41][767/817]	Loss 0.0023 (0.0176)	
training:	Epoch: [41][768/817]	Loss 0.0040 (0.0176)	
training:	Epoch: [41][769/817]	Loss 0.0040 (0.0176)	
training:	Epoch: [41][770/817]	Loss 0.0031 (0.0176)	
training:	Epoch: [41][771/817]	Loss 0.0028 (0.0176)	
training:	Epoch: [41][772/817]	Loss 0.1049 (0.0177)	
training:	Epoch: [41][773/817]	Loss 0.0038 (0.0177)	
training:	Epoch: [41][774/817]	Loss 0.0059 (0.0176)	
training:	Epoch: [41][775/817]	Loss 0.0027 (0.0176)	
training:	Epoch: [41][776/817]	Loss 0.0061 (0.0176)	
training:	Epoch: [41][777/817]	Loss 0.0019 (0.0176)	
training:	Epoch: [41][778/817]	Loss 0.0051 (0.0176)	
training:	Epoch: [41][779/817]	Loss 0.0076 (0.0176)	
training:	Epoch: [41][780/817]	Loss 0.0059 (0.0176)	
training:	Epoch: [41][781/817]	Loss 0.0031 (0.0175)	
training:	Epoch: [41][782/817]	Loss 0.0108 (0.0175)	
training:	Epoch: [41][783/817]	Loss 0.0023 (0.0175)	
training:	Epoch: [41][784/817]	Loss 0.0087 (0.0175)	
training:	Epoch: [41][785/817]	Loss 0.0036 (0.0175)	
training:	Epoch: [41][786/817]	Loss 0.0021 (0.0175)	
training:	Epoch: [41][787/817]	Loss 0.0047 (0.0174)	
training:	Epoch: [41][788/817]	Loss 0.0292 (0.0175)	
training:	Epoch: [41][789/817]	Loss 0.0023 (0.0174)	
training:	Epoch: [41][790/817]	Loss 0.0066 (0.0174)	
training:	Epoch: [41][791/817]	Loss 0.0017 (0.0174)	
training:	Epoch: [41][792/817]	Loss 0.0027 (0.0174)	
training:	Epoch: [41][793/817]	Loss 0.0050 (0.0174)	
training:	Epoch: [41][794/817]	Loss 0.0033 (0.0174)	
training:	Epoch: [41][795/817]	Loss 0.0024 (0.0173)	
training:	Epoch: [41][796/817]	Loss 0.0038 (0.0173)	
training:	Epoch: [41][797/817]	Loss 0.0189 (0.0173)	
training:	Epoch: [41][798/817]	Loss 0.0033 (0.0173)	
training:	Epoch: [41][799/817]	Loss 0.0025 (0.0173)	
training:	Epoch: [41][800/817]	Loss 0.0020 (0.0173)	
training:	Epoch: [41][801/817]	Loss 0.0045 (0.0172)	
training:	Epoch: [41][802/817]	Loss 0.0078 (0.0172)	
training:	Epoch: [41][803/817]	Loss 0.0025 (0.0172)	
training:	Epoch: [41][804/817]	Loss 0.0024 (0.0172)	
training:	Epoch: [41][805/817]	Loss 0.0022 (0.0172)	
training:	Epoch: [41][806/817]	Loss 0.0022 (0.0172)	
training:	Epoch: [41][807/817]	Loss 0.0017 (0.0171)	
training:	Epoch: [41][808/817]	Loss 0.0020 (0.0171)	
training:	Epoch: [41][809/817]	Loss 0.2983 (0.0175)	
training:	Epoch: [41][810/817]	Loss 0.0020 (0.0175)	
training:	Epoch: [41][811/817]	Loss 0.0137 (0.0174)	
training:	Epoch: [41][812/817]	Loss 0.0039 (0.0174)	
training:	Epoch: [41][813/817]	Loss 0.0039 (0.0174)	
training:	Epoch: [41][814/817]	Loss 0.0028 (0.0174)	
training:	Epoch: [41][815/817]	Loss 0.0033 (0.0174)	
training:	Epoch: [41][816/817]	Loss 0.0090 (0.0174)	
training:	Epoch: [41][817/817]	Loss 0.0031 (0.0173)	
Training:	 Loss: 0.0173

Training:	 ACC: 0.9964 0.9965 0.9979 0.9949
Validation:	 ACC: 0.7838 0.7849 0.8076 0.7601
Validation:	 Best_BACC: 0.8223 0.8218 0.8127 0.8318
Validation:	 Loss: 1.0521
Pretraining:	Epoch 42/200
----------
training:	Epoch: [42][1/817]	Loss 0.0022 (0.0022)	
training:	Epoch: [42][2/817]	Loss 0.0022 (0.0022)	
training:	Epoch: [42][3/817]	Loss 0.0044 (0.0029)	
training:	Epoch: [42][4/817]	Loss 0.0067 (0.0039)	
training:	Epoch: [42][5/817]	Loss 0.2642 (0.0560)	
training:	Epoch: [42][6/817]	Loss 0.0070 (0.0478)	
training:	Epoch: [42][7/817]	Loss 0.0064 (0.0419)	
training:	Epoch: [42][8/817]	Loss 0.0027 (0.0370)	
training:	Epoch: [42][9/817]	Loss 0.0023 (0.0331)	
training:	Epoch: [42][10/817]	Loss 0.0039 (0.0302)	
training:	Epoch: [42][11/817]	Loss 0.0071 (0.0281)	
training:	Epoch: [42][12/817]	Loss 0.0038 (0.0261)	
training:	Epoch: [42][13/817]	Loss 0.0035 (0.0243)	
training:	Epoch: [42][14/817]	Loss 0.0025 (0.0228)	
training:	Epoch: [42][15/817]	Loss 0.0053 (0.0216)	
training:	Epoch: [42][16/817]	Loss 0.0028 (0.0204)	
training:	Epoch: [42][17/817]	Loss 0.0136 (0.0200)	
training:	Epoch: [42][18/817]	Loss 0.0046 (0.0192)	
training:	Epoch: [42][19/817]	Loss 0.0031 (0.0183)	
training:	Epoch: [42][20/817]	Loss 0.0029 (0.0176)	
training:	Epoch: [42][21/817]	Loss 0.0026 (0.0168)	
training:	Epoch: [42][22/817]	Loss 0.0047 (0.0163)	
training:	Epoch: [42][23/817]	Loss 0.0059 (0.0158)	
training:	Epoch: [42][24/817]	Loss 0.0321 (0.0165)	
training:	Epoch: [42][25/817]	Loss 0.0049 (0.0161)	
training:	Epoch: [42][26/817]	Loss 0.0022 (0.0155)	
training:	Epoch: [42][27/817]	Loss 0.0023 (0.0150)	
training:	Epoch: [42][28/817]	Loss 0.0034 (0.0146)	
training:	Epoch: [42][29/817]	Loss 0.0064 (0.0143)	
training:	Epoch: [42][30/817]	Loss 0.0031 (0.0140)	
training:	Epoch: [42][31/817]	Loss 0.0061 (0.0137)	
training:	Epoch: [42][32/817]	Loss 0.0017 (0.0133)	
training:	Epoch: [42][33/817]	Loss 0.0017 (0.0130)	
training:	Epoch: [42][34/817]	Loss 0.0476 (0.0140)	
training:	Epoch: [42][35/817]	Loss 0.0029 (0.0137)	
training:	Epoch: [42][36/817]	Loss 0.0035 (0.0134)	
training:	Epoch: [42][37/817]	Loss 0.0049 (0.0132)	
training:	Epoch: [42][38/817]	Loss 0.0040 (0.0129)	
training:	Epoch: [42][39/817]	Loss 0.0022 (0.0126)	
training:	Epoch: [42][40/817]	Loss 0.0063 (0.0125)	
training:	Epoch: [42][41/817]	Loss 0.0073 (0.0124)	
training:	Epoch: [42][42/817]	Loss 0.0020 (0.0121)	
training:	Epoch: [42][43/817]	Loss 0.0024 (0.0119)	
training:	Epoch: [42][44/817]	Loss 0.0099 (0.0118)	
training:	Epoch: [42][45/817]	Loss 0.0026 (0.0116)	
training:	Epoch: [42][46/817]	Loss 0.0122 (0.0117)	
training:	Epoch: [42][47/817]	Loss 0.0034 (0.0115)	
training:	Epoch: [42][48/817]	Loss 0.0027 (0.0113)	
training:	Epoch: [42][49/817]	Loss 0.0034 (0.0111)	
training:	Epoch: [42][50/817]	Loss 0.0026 (0.0110)	
training:	Epoch: [42][51/817]	Loss 0.0025 (0.0108)	
training:	Epoch: [42][52/817]	Loss 0.0031 (0.0106)	
training:	Epoch: [42][53/817]	Loss 0.0052 (0.0105)	
training:	Epoch: [42][54/817]	Loss 0.0020 (0.0104)	
training:	Epoch: [42][55/817]	Loss 0.0023 (0.0102)	
training:	Epoch: [42][56/817]	Loss 0.0058 (0.0102)	
training:	Epoch: [42][57/817]	Loss 0.0024 (0.0100)	
training:	Epoch: [42][58/817]	Loss 0.0100 (0.0100)	
training:	Epoch: [42][59/817]	Loss 0.0025 (0.0099)	
training:	Epoch: [42][60/817]	Loss 0.0022 (0.0098)	
training:	Epoch: [42][61/817]	Loss 0.0020 (0.0096)	
training:	Epoch: [42][62/817]	Loss 0.0019 (0.0095)	
training:	Epoch: [42][63/817]	Loss 0.0191 (0.0097)	
training:	Epoch: [42][64/817]	Loss 0.0052 (0.0096)	
training:	Epoch: [42][65/817]	Loss 0.0022 (0.0095)	
training:	Epoch: [42][66/817]	Loss 0.0718 (0.0104)	
training:	Epoch: [42][67/817]	Loss 0.0095 (0.0104)	
training:	Epoch: [42][68/817]	Loss 0.0038 (0.0103)	
training:	Epoch: [42][69/817]	Loss 0.0055 (0.0102)	
training:	Epoch: [42][70/817]	Loss 0.3574 (0.0152)	
training:	Epoch: [42][71/817]	Loss 0.0029 (0.0150)	
training:	Epoch: [42][72/817]	Loss 0.0041 (0.0149)	
training:	Epoch: [42][73/817]	Loss 0.0025 (0.0147)	
training:	Epoch: [42][74/817]	Loss 0.4538 (0.0206)	
training:	Epoch: [42][75/817]	Loss 0.0027 (0.0204)	
training:	Epoch: [42][76/817]	Loss 0.0019 (0.0202)	
training:	Epoch: [42][77/817]	Loss 0.0043 (0.0200)	
training:	Epoch: [42][78/817]	Loss 0.0050 (0.0198)	
training:	Epoch: [42][79/817]	Loss 0.0028 (0.0196)	
training:	Epoch: [42][80/817]	Loss 0.0030 (0.0193)	
training:	Epoch: [42][81/817]	Loss 0.0078 (0.0192)	
training:	Epoch: [42][82/817]	Loss 0.0059 (0.0190)	
training:	Epoch: [42][83/817]	Loss 0.0025 (0.0188)	
training:	Epoch: [42][84/817]	Loss 0.0122 (0.0188)	
training:	Epoch: [42][85/817]	Loss 0.0071 (0.0186)	
training:	Epoch: [42][86/817]	Loss 0.0024 (0.0184)	
training:	Epoch: [42][87/817]	Loss 0.0064 (0.0183)	
training:	Epoch: [42][88/817]	Loss 0.0023 (0.0181)	
training:	Epoch: [42][89/817]	Loss 0.0030 (0.0179)	
training:	Epoch: [42][90/817]	Loss 0.0025 (0.0178)	
training:	Epoch: [42][91/817]	Loss 0.0090 (0.0177)	
training:	Epoch: [42][92/817]	Loss 0.0024 (0.0175)	
training:	Epoch: [42][93/817]	Loss 0.0023 (0.0173)	
training:	Epoch: [42][94/817]	Loss 0.0203 (0.0174)	
training:	Epoch: [42][95/817]	Loss 0.0037 (0.0172)	
training:	Epoch: [42][96/817]	Loss 0.0090 (0.0172)	
training:	Epoch: [42][97/817]	Loss 0.0032 (0.0170)	
training:	Epoch: [42][98/817]	Loss 0.0032 (0.0169)	
training:	Epoch: [42][99/817]	Loss 0.0061 (0.0168)	
training:	Epoch: [42][100/817]	Loss 0.0186 (0.0168)	
training:	Epoch: [42][101/817]	Loss 0.0048 (0.0167)	
training:	Epoch: [42][102/817]	Loss 0.0027 (0.0165)	
training:	Epoch: [42][103/817]	Loss 0.0034 (0.0164)	
training:	Epoch: [42][104/817]	Loss 0.0027 (0.0163)	
training:	Epoch: [42][105/817]	Loss 0.0028 (0.0161)	
training:	Epoch: [42][106/817]	Loss 0.0028 (0.0160)	
training:	Epoch: [42][107/817]	Loss 0.0026 (0.0159)	
training:	Epoch: [42][108/817]	Loss 0.0024 (0.0158)	
training:	Epoch: [42][109/817]	Loss 0.0018 (0.0156)	
training:	Epoch: [42][110/817]	Loss 0.0044 (0.0155)	
training:	Epoch: [42][111/817]	Loss 0.0041 (0.0154)	
training:	Epoch: [42][112/817]	Loss 0.0043 (0.0153)	
training:	Epoch: [42][113/817]	Loss 0.0019 (0.0152)	
training:	Epoch: [42][114/817]	Loss 0.0033 (0.0151)	
training:	Epoch: [42][115/817]	Loss 0.0051 (0.0150)	
training:	Epoch: [42][116/817]	Loss 0.0085 (0.0150)	
training:	Epoch: [42][117/817]	Loss 0.0022 (0.0149)	
training:	Epoch: [42][118/817]	Loss 0.0126 (0.0148)	
training:	Epoch: [42][119/817]	Loss 0.0028 (0.0147)	
training:	Epoch: [42][120/817]	Loss 0.0039 (0.0146)	
training:	Epoch: [42][121/817]	Loss 0.0213 (0.0147)	
training:	Epoch: [42][122/817]	Loss 0.0017 (0.0146)	
training:	Epoch: [42][123/817]	Loss 0.0063 (0.0145)	
training:	Epoch: [42][124/817]	Loss 0.0021 (0.0144)	
training:	Epoch: [42][125/817]	Loss 0.0152 (0.0144)	
training:	Epoch: [42][126/817]	Loss 0.0028 (0.0143)	
training:	Epoch: [42][127/817]	Loss 0.0072 (0.0143)	
training:	Epoch: [42][128/817]	Loss 0.0108 (0.0143)	
training:	Epoch: [42][129/817]	Loss 0.0019 (0.0142)	
training:	Epoch: [42][130/817]	Loss 0.0118 (0.0141)	
training:	Epoch: [42][131/817]	Loss 0.0093 (0.0141)	
training:	Epoch: [42][132/817]	Loss 0.0059 (0.0140)	
training:	Epoch: [42][133/817]	Loss 0.0017 (0.0139)	
training:	Epoch: [42][134/817]	Loss 0.0021 (0.0139)	
training:	Epoch: [42][135/817]	Loss 0.0312 (0.0140)	
training:	Epoch: [42][136/817]	Loss 0.0046 (0.0139)	
training:	Epoch: [42][137/817]	Loss 0.0018 (0.0138)	
training:	Epoch: [42][138/817]	Loss 0.0026 (0.0137)	
training:	Epoch: [42][139/817]	Loss 0.0024 (0.0137)	
training:	Epoch: [42][140/817]	Loss 0.0056 (0.0136)	
training:	Epoch: [42][141/817]	Loss 0.0043 (0.0135)	
training:	Epoch: [42][142/817]	Loss 0.0032 (0.0135)	
training:	Epoch: [42][143/817]	Loss 0.0044 (0.0134)	
training:	Epoch: [42][144/817]	Loss 0.0028 (0.0133)	
training:	Epoch: [42][145/817]	Loss 0.0025 (0.0133)	
training:	Epoch: [42][146/817]	Loss 0.0024 (0.0132)	
training:	Epoch: [42][147/817]	Loss 0.0023 (0.0131)	
training:	Epoch: [42][148/817]	Loss 0.0025 (0.0130)	
training:	Epoch: [42][149/817]	Loss 0.0079 (0.0130)	
training:	Epoch: [42][150/817]	Loss 0.0021 (0.0129)	
training:	Epoch: [42][151/817]	Loss 0.0037 (0.0129)	
training:	Epoch: [42][152/817]	Loss 0.0046 (0.0128)	
training:	Epoch: [42][153/817]	Loss 0.0029 (0.0128)	
training:	Epoch: [42][154/817]	Loss 0.0083 (0.0127)	
training:	Epoch: [42][155/817]	Loss 0.0021 (0.0127)	
training:	Epoch: [42][156/817]	Loss 0.0027 (0.0126)	
training:	Epoch: [42][157/817]	Loss 0.0031 (0.0125)	
training:	Epoch: [42][158/817]	Loss 0.0069 (0.0125)	
training:	Epoch: [42][159/817]	Loss 0.0027 (0.0124)	
training:	Epoch: [42][160/817]	Loss 0.0021 (0.0124)	
training:	Epoch: [42][161/817]	Loss 0.0018 (0.0123)	
training:	Epoch: [42][162/817]	Loss 0.0022 (0.0122)	
training:	Epoch: [42][163/817]	Loss 0.0036 (0.0122)	
training:	Epoch: [42][164/817]	Loss 0.3475 (0.0142)	
training:	Epoch: [42][165/817]	Loss 0.0038 (0.0142)	
training:	Epoch: [42][166/817]	Loss 0.0032 (0.0141)	
training:	Epoch: [42][167/817]	Loss 0.0020 (0.0140)	
training:	Epoch: [42][168/817]	Loss 0.0029 (0.0140)	
training:	Epoch: [42][169/817]	Loss 0.0024 (0.0139)	
training:	Epoch: [42][170/817]	Loss 0.0044 (0.0138)	
training:	Epoch: [42][171/817]	Loss 0.0119 (0.0138)	
training:	Epoch: [42][172/817]	Loss 0.0041 (0.0138)	
training:	Epoch: [42][173/817]	Loss 0.0027 (0.0137)	
training:	Epoch: [42][174/817]	Loss 0.0045 (0.0137)	
training:	Epoch: [42][175/817]	Loss 0.0018 (0.0136)	
training:	Epoch: [42][176/817]	Loss 0.0031 (0.0135)	
training:	Epoch: [42][177/817]	Loss 0.2482 (0.0149)	
training:	Epoch: [42][178/817]	Loss 0.0020 (0.0148)	
training:	Epoch: [42][179/817]	Loss 0.0030 (0.0147)	
training:	Epoch: [42][180/817]	Loss 0.0024 (0.0146)	
training:	Epoch: [42][181/817]	Loss 0.0050 (0.0146)	
training:	Epoch: [42][182/817]	Loss 0.0036 (0.0145)	
training:	Epoch: [42][183/817]	Loss 0.0031 (0.0145)	
training:	Epoch: [42][184/817]	Loss 0.0024 (0.0144)	
training:	Epoch: [42][185/817]	Loss 0.0037 (0.0143)	
training:	Epoch: [42][186/817]	Loss 0.0047 (0.0143)	
training:	Epoch: [42][187/817]	Loss 0.0113 (0.0143)	
training:	Epoch: [42][188/817]	Loss 0.0039 (0.0142)	
training:	Epoch: [42][189/817]	Loss 0.0071 (0.0142)	
training:	Epoch: [42][190/817]	Loss 0.0032 (0.0141)	
training:	Epoch: [42][191/817]	Loss 0.0037 (0.0141)	
training:	Epoch: [42][192/817]	Loss 0.0105 (0.0141)	
training:	Epoch: [42][193/817]	Loss 0.0035 (0.0140)	
training:	Epoch: [42][194/817]	Loss 0.0072 (0.0140)	
training:	Epoch: [42][195/817]	Loss 0.0042 (0.0139)	
training:	Epoch: [42][196/817]	Loss 0.0021 (0.0139)	
training:	Epoch: [42][197/817]	Loss 0.0031 (0.0138)	
training:	Epoch: [42][198/817]	Loss 0.0061 (0.0138)	
training:	Epoch: [42][199/817]	Loss 0.0034 (0.0137)	
training:	Epoch: [42][200/817]	Loss 0.0021 (0.0136)	
training:	Epoch: [42][201/817]	Loss 0.0062 (0.0136)	
training:	Epoch: [42][202/817]	Loss 0.0047 (0.0136)	
training:	Epoch: [42][203/817]	Loss 0.0104 (0.0136)	
training:	Epoch: [42][204/817]	Loss 0.0135 (0.0136)	
training:	Epoch: [42][205/817]	Loss 0.0015 (0.0135)	
training:	Epoch: [42][206/817]	Loss 0.0021 (0.0134)	
training:	Epoch: [42][207/817]	Loss 0.0037 (0.0134)	
training:	Epoch: [42][208/817]	Loss 0.0033 (0.0133)	
training:	Epoch: [42][209/817]	Loss 0.0028 (0.0133)	
training:	Epoch: [42][210/817]	Loss 0.0028 (0.0132)	
training:	Epoch: [42][211/817]	Loss 0.0037 (0.0132)	
training:	Epoch: [42][212/817]	Loss 0.0022 (0.0131)	
training:	Epoch: [42][213/817]	Loss 0.0034 (0.0131)	
training:	Epoch: [42][214/817]	Loss 0.0073 (0.0131)	
training:	Epoch: [42][215/817]	Loss 0.0028 (0.0130)	
training:	Epoch: [42][216/817]	Loss 0.0035 (0.0130)	
training:	Epoch: [42][217/817]	Loss 0.0035 (0.0129)	
training:	Epoch: [42][218/817]	Loss 0.0038 (0.0129)	
training:	Epoch: [42][219/817]	Loss 0.0021 (0.0128)	
training:	Epoch: [42][220/817]	Loss 0.0042 (0.0128)	
training:	Epoch: [42][221/817]	Loss 0.0076 (0.0128)	
training:	Epoch: [42][222/817]	Loss 0.0046 (0.0127)	
training:	Epoch: [42][223/817]	Loss 0.0263 (0.0128)	
training:	Epoch: [42][224/817]	Loss 0.0018 (0.0128)	
training:	Epoch: [42][225/817]	Loss 0.0027 (0.0127)	
training:	Epoch: [42][226/817]	Loss 0.0026 (0.0127)	
training:	Epoch: [42][227/817]	Loss 0.0040 (0.0126)	
training:	Epoch: [42][228/817]	Loss 0.0038 (0.0126)	
training:	Epoch: [42][229/817]	Loss 0.0021 (0.0125)	
training:	Epoch: [42][230/817]	Loss 0.0021 (0.0125)	
training:	Epoch: [42][231/817]	Loss 0.0019 (0.0125)	
training:	Epoch: [42][232/817]	Loss 0.0026 (0.0124)	
training:	Epoch: [42][233/817]	Loss 0.0019 (0.0124)	
training:	Epoch: [42][234/817]	Loss 0.0039 (0.0123)	
training:	Epoch: [42][235/817]	Loss 0.0031 (0.0123)	
training:	Epoch: [42][236/817]	Loss 0.0019 (0.0122)	
training:	Epoch: [42][237/817]	Loss 0.0039 (0.0122)	
training:	Epoch: [42][238/817]	Loss 0.0043 (0.0122)	
training:	Epoch: [42][239/817]	Loss 0.0058 (0.0122)	
training:	Epoch: [42][240/817]	Loss 0.0021 (0.0121)	
training:	Epoch: [42][241/817]	Loss 0.0019 (0.0121)	
training:	Epoch: [42][242/817]	Loss 0.0022 (0.0120)	
training:	Epoch: [42][243/817]	Loss 0.0214 (0.0121)	
training:	Epoch: [42][244/817]	Loss 0.0030 (0.0120)	
training:	Epoch: [42][245/817]	Loss 0.0094 (0.0120)	
training:	Epoch: [42][246/817]	Loss 0.0028 (0.0120)	
training:	Epoch: [42][247/817]	Loss 0.0023 (0.0119)	
training:	Epoch: [42][248/817]	Loss 0.0024 (0.0119)	
training:	Epoch: [42][249/817]	Loss 0.0023 (0.0119)	
training:	Epoch: [42][250/817]	Loss 0.0017 (0.0118)	
training:	Epoch: [42][251/817]	Loss 0.0017 (0.0118)	
training:	Epoch: [42][252/817]	Loss 0.0020 (0.0117)	
training:	Epoch: [42][253/817]	Loss 0.0018 (0.0117)	
training:	Epoch: [42][254/817]	Loss 0.0049 (0.0117)	
training:	Epoch: [42][255/817]	Loss 0.0019 (0.0116)	
training:	Epoch: [42][256/817]	Loss 0.0017 (0.0116)	
training:	Epoch: [42][257/817]	Loss 0.0018 (0.0116)	
training:	Epoch: [42][258/817]	Loss 0.4302 (0.0132)	
training:	Epoch: [42][259/817]	Loss 0.0023 (0.0131)	
training:	Epoch: [42][260/817]	Loss 0.0031 (0.0131)	
training:	Epoch: [42][261/817]	Loss 0.0028 (0.0131)	
training:	Epoch: [42][262/817]	Loss 0.0030 (0.0130)	
training:	Epoch: [42][263/817]	Loss 0.0047 (0.0130)	
training:	Epoch: [42][264/817]	Loss 0.0069 (0.0130)	
training:	Epoch: [42][265/817]	Loss 0.0063 (0.0129)	
training:	Epoch: [42][266/817]	Loss 0.0055 (0.0129)	
training:	Epoch: [42][267/817]	Loss 0.2686 (0.0139)	
training:	Epoch: [42][268/817]	Loss 0.0025 (0.0138)	
training:	Epoch: [42][269/817]	Loss 0.0030 (0.0138)	
training:	Epoch: [42][270/817]	Loss 0.0035 (0.0138)	
training:	Epoch: [42][271/817]	Loss 0.0055 (0.0137)	
training:	Epoch: [42][272/817]	Loss 0.0034 (0.0137)	
training:	Epoch: [42][273/817]	Loss 0.0039 (0.0136)	
training:	Epoch: [42][274/817]	Loss 0.0023 (0.0136)	
training:	Epoch: [42][275/817]	Loss 0.0036 (0.0136)	
training:	Epoch: [42][276/817]	Loss 0.0290 (0.0136)	
training:	Epoch: [42][277/817]	Loss 0.0017 (0.0136)	
training:	Epoch: [42][278/817]	Loss 0.0106 (0.0136)	
training:	Epoch: [42][279/817]	Loss 0.0035 (0.0135)	
training:	Epoch: [42][280/817]	Loss 0.0023 (0.0135)	
training:	Epoch: [42][281/817]	Loss 0.0076 (0.0135)	
training:	Epoch: [42][282/817]	Loss 0.0017 (0.0134)	
training:	Epoch: [42][283/817]	Loss 0.0050 (0.0134)	
training:	Epoch: [42][284/817]	Loss 0.0031 (0.0134)	
training:	Epoch: [42][285/817]	Loss 0.0023 (0.0133)	
training:	Epoch: [42][286/817]	Loss 0.0034 (0.0133)	
training:	Epoch: [42][287/817]	Loss 0.0029 (0.0133)	
training:	Epoch: [42][288/817]	Loss 0.0042 (0.0132)	
training:	Epoch: [42][289/817]	Loss 0.0032 (0.0132)	
training:	Epoch: [42][290/817]	Loss 0.0030 (0.0132)	
training:	Epoch: [42][291/817]	Loss 0.0081 (0.0131)	
training:	Epoch: [42][292/817]	Loss 0.0042 (0.0131)	
training:	Epoch: [42][293/817]	Loss 0.0025 (0.0131)	
training:	Epoch: [42][294/817]	Loss 0.0088 (0.0131)	
training:	Epoch: [42][295/817]	Loss 0.0024 (0.0130)	
training:	Epoch: [42][296/817]	Loss 0.0295 (0.0131)	
training:	Epoch: [42][297/817]	Loss 0.0024 (0.0130)	
training:	Epoch: [42][298/817]	Loss 0.0046 (0.0130)	
training:	Epoch: [42][299/817]	Loss 0.0055 (0.0130)	
training:	Epoch: [42][300/817]	Loss 0.0023 (0.0130)	
training:	Epoch: [42][301/817]	Loss 0.0018 (0.0129)	
training:	Epoch: [42][302/817]	Loss 0.0037 (0.0129)	
training:	Epoch: [42][303/817]	Loss 0.0079 (0.0129)	
training:	Epoch: [42][304/817]	Loss 0.0024 (0.0128)	
training:	Epoch: [42][305/817]	Loss 0.0021 (0.0128)	
training:	Epoch: [42][306/817]	Loss 0.0038 (0.0128)	
training:	Epoch: [42][307/817]	Loss 0.0043 (0.0127)	
training:	Epoch: [42][308/817]	Loss 0.0046 (0.0127)	
training:	Epoch: [42][309/817]	Loss 0.0034 (0.0127)	
training:	Epoch: [42][310/817]	Loss 0.0065 (0.0127)	
training:	Epoch: [42][311/817]	Loss 0.0021 (0.0126)	
training:	Epoch: [42][312/817]	Loss 0.3334 (0.0137)	
training:	Epoch: [42][313/817]	Loss 0.0024 (0.0136)	
training:	Epoch: [42][314/817]	Loss 0.0022 (0.0136)	
training:	Epoch: [42][315/817]	Loss 0.0020 (0.0136)	
training:	Epoch: [42][316/817]	Loss 0.0079 (0.0135)	
training:	Epoch: [42][317/817]	Loss 0.0052 (0.0135)	
training:	Epoch: [42][318/817]	Loss 0.0045 (0.0135)	
training:	Epoch: [42][319/817]	Loss 0.0033 (0.0134)	
training:	Epoch: [42][320/817]	Loss 0.0023 (0.0134)	
training:	Epoch: [42][321/817]	Loss 0.0043 (0.0134)	
training:	Epoch: [42][322/817]	Loss 0.0021 (0.0133)	
training:	Epoch: [42][323/817]	Loss 0.0035 (0.0133)	
training:	Epoch: [42][324/817]	Loss 0.0024 (0.0133)	
training:	Epoch: [42][325/817]	Loss 0.0023 (0.0133)	
training:	Epoch: [42][326/817]	Loss 0.0246 (0.0133)	
training:	Epoch: [42][327/817]	Loss 0.0052 (0.0133)	
training:	Epoch: [42][328/817]	Loss 0.0029 (0.0132)	
training:	Epoch: [42][329/817]	Loss 0.0047 (0.0132)	
training:	Epoch: [42][330/817]	Loss 0.0023 (0.0132)	
training:	Epoch: [42][331/817]	Loss 0.0049 (0.0131)	
training:	Epoch: [42][332/817]	Loss 0.0019 (0.0131)	
training:	Epoch: [42][333/817]	Loss 0.0066 (0.0131)	
training:	Epoch: [42][334/817]	Loss 0.0038 (0.0131)	
training:	Epoch: [42][335/817]	Loss 0.0035 (0.0130)	
training:	Epoch: [42][336/817]	Loss 0.0026 (0.0130)	
training:	Epoch: [42][337/817]	Loss 0.0058 (0.0130)	
training:	Epoch: [42][338/817]	Loss 0.0019 (0.0130)	
training:	Epoch: [42][339/817]	Loss 0.0040 (0.0129)	
training:	Epoch: [42][340/817]	Loss 0.0627 (0.0131)	
training:	Epoch: [42][341/817]	Loss 0.0025 (0.0130)	
training:	Epoch: [42][342/817]	Loss 0.0015 (0.0130)	
training:	Epoch: [42][343/817]	Loss 0.0074 (0.0130)	
training:	Epoch: [42][344/817]	Loss 0.0040 (0.0130)	
training:	Epoch: [42][345/817]	Loss 0.0018 (0.0129)	
training:	Epoch: [42][346/817]	Loss 0.0024 (0.0129)	
training:	Epoch: [42][347/817]	Loss 0.0647 (0.0130)	
training:	Epoch: [42][348/817]	Loss 0.0017 (0.0130)	
training:	Epoch: [42][349/817]	Loss 0.0076 (0.0130)	
training:	Epoch: [42][350/817]	Loss 0.0021 (0.0130)	
training:	Epoch: [42][351/817]	Loss 0.0035 (0.0129)	
training:	Epoch: [42][352/817]	Loss 0.0066 (0.0129)	
training:	Epoch: [42][353/817]	Loss 0.0021 (0.0129)	
training:	Epoch: [42][354/817]	Loss 0.0082 (0.0129)	
training:	Epoch: [42][355/817]	Loss 0.0017 (0.0129)	
training:	Epoch: [42][356/817]	Loss 0.0022 (0.0128)	
training:	Epoch: [42][357/817]	Loss 0.0049 (0.0128)	
training:	Epoch: [42][358/817]	Loss 0.0021 (0.0128)	
training:	Epoch: [42][359/817]	Loss 0.0790 (0.0130)	
training:	Epoch: [42][360/817]	Loss 0.0020 (0.0129)	
training:	Epoch: [42][361/817]	Loss 0.0017 (0.0129)	
training:	Epoch: [42][362/817]	Loss 0.0023 (0.0129)	
training:	Epoch: [42][363/817]	Loss 0.0023 (0.0128)	
training:	Epoch: [42][364/817]	Loss 0.0046 (0.0128)	
training:	Epoch: [42][365/817]	Loss 0.1095 (0.0131)	
training:	Epoch: [42][366/817]	Loss 0.0037 (0.0130)	
training:	Epoch: [42][367/817]	Loss 0.0016 (0.0130)	
training:	Epoch: [42][368/817]	Loss 0.0020 (0.0130)	
training:	Epoch: [42][369/817]	Loss 0.0018 (0.0130)	
training:	Epoch: [42][370/817]	Loss 0.0017 (0.0129)	
training:	Epoch: [42][371/817]	Loss 0.0035 (0.0129)	
training:	Epoch: [42][372/817]	Loss 0.0031 (0.0129)	
training:	Epoch: [42][373/817]	Loss 0.0029 (0.0128)	
training:	Epoch: [42][374/817]	Loss 0.0022 (0.0128)	
training:	Epoch: [42][375/817]	Loss 0.0021 (0.0128)	
training:	Epoch: [42][376/817]	Loss 0.0024 (0.0128)	
training:	Epoch: [42][377/817]	Loss 0.0089 (0.0128)	
training:	Epoch: [42][378/817]	Loss 0.0112 (0.0128)	
training:	Epoch: [42][379/817]	Loss 0.0394 (0.0128)	
training:	Epoch: [42][380/817]	Loss 0.0033 (0.0128)	
training:	Epoch: [42][381/817]	Loss 0.0025 (0.0128)	
training:	Epoch: [42][382/817]	Loss 0.0029 (0.0127)	
training:	Epoch: [42][383/817]	Loss 0.0024 (0.0127)	
training:	Epoch: [42][384/817]	Loss 0.0017 (0.0127)	
training:	Epoch: [42][385/817]	Loss 0.0023 (0.0127)	
training:	Epoch: [42][386/817]	Loss 0.0021 (0.0126)	
training:	Epoch: [42][387/817]	Loss 0.3342 (0.0135)	
training:	Epoch: [42][388/817]	Loss 0.0033 (0.0134)	
training:	Epoch: [42][389/817]	Loss 0.0040 (0.0134)	
training:	Epoch: [42][390/817]	Loss 0.2715 (0.0141)	
training:	Epoch: [42][391/817]	Loss 0.2345 (0.0146)	
training:	Epoch: [42][392/817]	Loss 0.0017 (0.0146)	
training:	Epoch: [42][393/817]	Loss 0.0020 (0.0146)	
training:	Epoch: [42][394/817]	Loss 0.0206 (0.0146)	
training:	Epoch: [42][395/817]	Loss 0.0042 (0.0146)	
training:	Epoch: [42][396/817]	Loss 0.6758 (0.0162)	
training:	Epoch: [42][397/817]	Loss 0.0021 (0.0162)	
training:	Epoch: [42][398/817]	Loss 0.0085 (0.0162)	
training:	Epoch: [42][399/817]	Loss 0.0045 (0.0161)	
training:	Epoch: [42][400/817]	Loss 0.0310 (0.0162)	
training:	Epoch: [42][401/817]	Loss 0.0021 (0.0162)	
training:	Epoch: [42][402/817]	Loss 0.0029 (0.0161)	
training:	Epoch: [42][403/817]	Loss 0.0020 (0.0161)	
training:	Epoch: [42][404/817]	Loss 0.0033 (0.0161)	
training:	Epoch: [42][405/817]	Loss 0.0071 (0.0160)	
training:	Epoch: [42][406/817]	Loss 0.0022 (0.0160)	
training:	Epoch: [42][407/817]	Loss 0.0021 (0.0160)	
training:	Epoch: [42][408/817]	Loss 0.0601 (0.0161)	
training:	Epoch: [42][409/817]	Loss 0.0159 (0.0161)	
training:	Epoch: [42][410/817]	Loss 0.3451 (0.0169)	
training:	Epoch: [42][411/817]	Loss 0.0247 (0.0169)	
training:	Epoch: [42][412/817]	Loss 0.0032 (0.0169)	
training:	Epoch: [42][413/817]	Loss 0.0022 (0.0168)	
training:	Epoch: [42][414/817]	Loss 0.0182 (0.0168)	
training:	Epoch: [42][415/817]	Loss 0.0026 (0.0168)	
training:	Epoch: [42][416/817]	Loss 0.0053 (0.0168)	
training:	Epoch: [42][417/817]	Loss 0.0029 (0.0167)	
training:	Epoch: [42][418/817]	Loss 0.0169 (0.0167)	
training:	Epoch: [42][419/817]	Loss 0.0016 (0.0167)	
training:	Epoch: [42][420/817]	Loss 0.0224 (0.0167)	
training:	Epoch: [42][421/817]	Loss 0.0169 (0.0167)	
training:	Epoch: [42][422/817]	Loss 0.0075 (0.0167)	
training:	Epoch: [42][423/817]	Loss 0.0054 (0.0167)	
training:	Epoch: [42][424/817]	Loss 0.0080 (0.0166)	
training:	Epoch: [42][425/817]	Loss 0.0092 (0.0166)	
training:	Epoch: [42][426/817]	Loss 0.0150 (0.0166)	
training:	Epoch: [42][427/817]	Loss 0.0021 (0.0166)	
training:	Epoch: [42][428/817]	Loss 0.0131 (0.0166)	
training:	Epoch: [42][429/817]	Loss 0.0063 (0.0166)	
training:	Epoch: [42][430/817]	Loss 0.0037 (0.0165)	
training:	Epoch: [42][431/817]	Loss 0.0068 (0.0165)	
training:	Epoch: [42][432/817]	Loss 0.0028 (0.0165)	
training:	Epoch: [42][433/817]	Loss 0.2803 (0.0171)	
training:	Epoch: [42][434/817]	Loss 0.0094 (0.0171)	
training:	Epoch: [42][435/817]	Loss 0.0021 (0.0170)	
training:	Epoch: [42][436/817]	Loss 0.0017 (0.0170)	
training:	Epoch: [42][437/817]	Loss 0.0051 (0.0170)	
training:	Epoch: [42][438/817]	Loss 0.0017 (0.0169)	
training:	Epoch: [42][439/817]	Loss 0.0026 (0.0169)	
training:	Epoch: [42][440/817]	Loss 0.0056 (0.0169)	
training:	Epoch: [42][441/817]	Loss 0.0075 (0.0168)	
training:	Epoch: [42][442/817]	Loss 0.0165 (0.0168)	
training:	Epoch: [42][443/817]	Loss 0.0040 (0.0168)	
training:	Epoch: [42][444/817]	Loss 0.0025 (0.0168)	
training:	Epoch: [42][445/817]	Loss 0.0047 (0.0168)	
training:	Epoch: [42][446/817]	Loss 0.0016 (0.0167)	
training:	Epoch: [42][447/817]	Loss 0.0037 (0.0167)	
training:	Epoch: [42][448/817]	Loss 0.0083 (0.0167)	
training:	Epoch: [42][449/817]	Loss 0.0048 (0.0167)	
training:	Epoch: [42][450/817]	Loss 0.0025 (0.0166)	
training:	Epoch: [42][451/817]	Loss 0.0022 (0.0166)	
training:	Epoch: [42][452/817]	Loss 0.0020 (0.0166)	
training:	Epoch: [42][453/817]	Loss 0.0027 (0.0165)	
training:	Epoch: [42][454/817]	Loss 0.0145 (0.0165)	
training:	Epoch: [42][455/817]	Loss 0.0034 (0.0165)	
training:	Epoch: [42][456/817]	Loss 0.0040 (0.0165)	
training:	Epoch: [42][457/817]	Loss 0.0026 (0.0164)	
training:	Epoch: [42][458/817]	Loss 0.0203 (0.0164)	
training:	Epoch: [42][459/817]	Loss 0.0047 (0.0164)	
training:	Epoch: [42][460/817]	Loss 0.0021 (0.0164)	
training:	Epoch: [42][461/817]	Loss 0.0023 (0.0164)	
training:	Epoch: [42][462/817]	Loss 0.0022 (0.0163)	
training:	Epoch: [42][463/817]	Loss 0.0301 (0.0164)	
training:	Epoch: [42][464/817]	Loss 0.0058 (0.0163)	
training:	Epoch: [42][465/817]	Loss 0.0024 (0.0163)	
training:	Epoch: [42][466/817]	Loss 0.0048 (0.0163)	
training:	Epoch: [42][467/817]	Loss 0.0042 (0.0163)	
training:	Epoch: [42][468/817]	Loss 0.0020 (0.0162)	
training:	Epoch: [42][469/817]	Loss 0.0036 (0.0162)	
training:	Epoch: [42][470/817]	Loss 0.0022 (0.0162)	
training:	Epoch: [42][471/817]	Loss 0.0106 (0.0162)	
training:	Epoch: [42][472/817]	Loss 0.0042 (0.0161)	
training:	Epoch: [42][473/817]	Loss 0.0019 (0.0161)	
training:	Epoch: [42][474/817]	Loss 0.0026 (0.0161)	
training:	Epoch: [42][475/817]	Loss 0.0030 (0.0160)	
training:	Epoch: [42][476/817]	Loss 0.0038 (0.0160)	
training:	Epoch: [42][477/817]	Loss 0.0123 (0.0160)	
training:	Epoch: [42][478/817]	Loss 0.0030 (0.0160)	
training:	Epoch: [42][479/817]	Loss 0.0024 (0.0160)	
training:	Epoch: [42][480/817]	Loss 0.0021 (0.0159)	
training:	Epoch: [42][481/817]	Loss 0.0020 (0.0159)	
training:	Epoch: [42][482/817]	Loss 0.0031 (0.0159)	
training:	Epoch: [42][483/817]	Loss 0.0043 (0.0158)	
training:	Epoch: [42][484/817]	Loss 0.0037 (0.0158)	
training:	Epoch: [42][485/817]	Loss 0.0054 (0.0158)	
training:	Epoch: [42][486/817]	Loss 0.0022 (0.0158)	
training:	Epoch: [42][487/817]	Loss 0.0017 (0.0157)	
training:	Epoch: [42][488/817]	Loss 0.0017 (0.0157)	
training:	Epoch: [42][489/817]	Loss 0.0045 (0.0157)	
training:	Epoch: [42][490/817]	Loss 0.0025 (0.0157)	
training:	Epoch: [42][491/817]	Loss 0.0052 (0.0156)	
training:	Epoch: [42][492/817]	Loss 0.0049 (0.0156)	
training:	Epoch: [42][493/817]	Loss 0.0025 (0.0156)	
training:	Epoch: [42][494/817]	Loss 0.0026 (0.0156)	
training:	Epoch: [42][495/817]	Loss 0.0024 (0.0155)	
training:	Epoch: [42][496/817]	Loss 0.0031 (0.0155)	
training:	Epoch: [42][497/817]	Loss 0.0019 (0.0155)	
training:	Epoch: [42][498/817]	Loss 0.0035 (0.0155)	
training:	Epoch: [42][499/817]	Loss 0.0108 (0.0155)	
training:	Epoch: [42][500/817]	Loss 0.0139 (0.0154)	
training:	Epoch: [42][501/817]	Loss 0.0024 (0.0154)	
training:	Epoch: [42][502/817]	Loss 0.0031 (0.0154)	
training:	Epoch: [42][503/817]	Loss 0.0061 (0.0154)	
training:	Epoch: [42][504/817]	Loss 0.0028 (0.0154)	
training:	Epoch: [42][505/817]	Loss 0.0031 (0.0153)	
training:	Epoch: [42][506/817]	Loss 0.0032 (0.0153)	
training:	Epoch: [42][507/817]	Loss 0.0096 (0.0153)	
training:	Epoch: [42][508/817]	Loss 0.0022 (0.0153)	
training:	Epoch: [42][509/817]	Loss 0.0076 (0.0153)	
training:	Epoch: [42][510/817]	Loss 0.0020 (0.0152)	
training:	Epoch: [42][511/817]	Loss 0.0024 (0.0152)	
training:	Epoch: [42][512/817]	Loss 0.0057 (0.0152)	
training:	Epoch: [42][513/817]	Loss 0.0022 (0.0152)	
training:	Epoch: [42][514/817]	Loss 0.0042 (0.0151)	
training:	Epoch: [42][515/817]	Loss 0.0024 (0.0151)	
training:	Epoch: [42][516/817]	Loss 0.0031 (0.0151)	
training:	Epoch: [42][517/817]	Loss 0.0019 (0.0151)	
training:	Epoch: [42][518/817]	Loss 0.0020 (0.0150)	
training:	Epoch: [42][519/817]	Loss 0.0028 (0.0150)	
training:	Epoch: [42][520/817]	Loss 0.0037 (0.0150)	
training:	Epoch: [42][521/817]	Loss 0.0031 (0.0150)	
training:	Epoch: [42][522/817]	Loss 0.0032 (0.0149)	
training:	Epoch: [42][523/817]	Loss 0.0020 (0.0149)	
training:	Epoch: [42][524/817]	Loss 0.0043 (0.0149)	
training:	Epoch: [42][525/817]	Loss 0.2320 (0.0153)	
training:	Epoch: [42][526/817]	Loss 0.0027 (0.0153)	
training:	Epoch: [42][527/817]	Loss 0.0020 (0.0153)	
training:	Epoch: [42][528/817]	Loss 0.0025 (0.0152)	
training:	Epoch: [42][529/817]	Loss 0.0035 (0.0152)	
training:	Epoch: [42][530/817]	Loss 0.0806 (0.0153)	
training:	Epoch: [42][531/817]	Loss 0.0044 (0.0153)	
training:	Epoch: [42][532/817]	Loss 0.0021 (0.0153)	
training:	Epoch: [42][533/817]	Loss 0.0037 (0.0153)	
training:	Epoch: [42][534/817]	Loss 0.0018 (0.0153)	
training:	Epoch: [42][535/817]	Loss 0.0023 (0.0152)	
training:	Epoch: [42][536/817]	Loss 0.0052 (0.0152)	
training:	Epoch: [42][537/817]	Loss 0.0073 (0.0152)	
training:	Epoch: [42][538/817]	Loss 0.0040 (0.0152)	
training:	Epoch: [42][539/817]	Loss 0.0029 (0.0152)	
training:	Epoch: [42][540/817]	Loss 0.0025 (0.0151)	
training:	Epoch: [42][541/817]	Loss 0.0051 (0.0151)	
training:	Epoch: [42][542/817]	Loss 0.0107 (0.0151)	
training:	Epoch: [42][543/817]	Loss 0.0120 (0.0151)	
training:	Epoch: [42][544/817]	Loss 0.0026 (0.0151)	
training:	Epoch: [42][545/817]	Loss 0.0031 (0.0151)	
training:	Epoch: [42][546/817]	Loss 0.0021 (0.0150)	
training:	Epoch: [42][547/817]	Loss 0.0021 (0.0150)	
training:	Epoch: [42][548/817]	Loss 0.0020 (0.0150)	
training:	Epoch: [42][549/817]	Loss 0.0034 (0.0150)	
training:	Epoch: [42][550/817]	Loss 0.0017 (0.0149)	
training:	Epoch: [42][551/817]	Loss 0.0019 (0.0149)	
training:	Epoch: [42][552/817]	Loss 0.0058 (0.0149)	
training:	Epoch: [42][553/817]	Loss 0.0048 (0.0149)	
training:	Epoch: [42][554/817]	Loss 0.0022 (0.0149)	
training:	Epoch: [42][555/817]	Loss 0.0020 (0.0148)	
training:	Epoch: [42][556/817]	Loss 0.0020 (0.0148)	
training:	Epoch: [42][557/817]	Loss 0.0047 (0.0148)	
training:	Epoch: [42][558/817]	Loss 0.0034 (0.0148)	
training:	Epoch: [42][559/817]	Loss 0.0019 (0.0147)	
training:	Epoch: [42][560/817]	Loss 0.0018 (0.0147)	
training:	Epoch: [42][561/817]	Loss 0.0039 (0.0147)	
training:	Epoch: [42][562/817]	Loss 0.0045 (0.0147)	
training:	Epoch: [42][563/817]	Loss 0.0017 (0.0147)	
training:	Epoch: [42][564/817]	Loss 0.0042 (0.0146)	
training:	Epoch: [42][565/817]	Loss 0.0019 (0.0146)	
training:	Epoch: [42][566/817]	Loss 0.0014 (0.0146)	
training:	Epoch: [42][567/817]	Loss 0.0022 (0.0146)	
training:	Epoch: [42][568/817]	Loss 0.0034 (0.0146)	
training:	Epoch: [42][569/817]	Loss 0.0060 (0.0145)	
training:	Epoch: [42][570/817]	Loss 0.0028 (0.0145)	
training:	Epoch: [42][571/817]	Loss 0.0017 (0.0145)	
training:	Epoch: [42][572/817]	Loss 0.0035 (0.0145)	
training:	Epoch: [42][573/817]	Loss 0.0014 (0.0145)	
training:	Epoch: [42][574/817]	Loss 0.0022 (0.0144)	
training:	Epoch: [42][575/817]	Loss 0.0067 (0.0144)	
training:	Epoch: [42][576/817]	Loss 0.0142 (0.0144)	
training:	Epoch: [42][577/817]	Loss 0.0020 (0.0144)	
training:	Epoch: [42][578/817]	Loss 0.0046 (0.0144)	
training:	Epoch: [42][579/817]	Loss 0.0409 (0.0144)	
training:	Epoch: [42][580/817]	Loss 0.0126 (0.0144)	
training:	Epoch: [42][581/817]	Loss 0.0022 (0.0144)	
training:	Epoch: [42][582/817]	Loss 0.0060 (0.0144)	
training:	Epoch: [42][583/817]	Loss 0.0021 (0.0144)	
training:	Epoch: [42][584/817]	Loss 0.0022 (0.0143)	
training:	Epoch: [42][585/817]	Loss 0.0037 (0.0143)	
training:	Epoch: [42][586/817]	Loss 0.0017 (0.0143)	
training:	Epoch: [42][587/817]	Loss 0.0022 (0.0143)	
training:	Epoch: [42][588/817]	Loss 0.0027 (0.0143)	
training:	Epoch: [42][589/817]	Loss 0.0029 (0.0142)	
training:	Epoch: [42][590/817]	Loss 0.0023 (0.0142)	
training:	Epoch: [42][591/817]	Loss 0.0023 (0.0142)	
training:	Epoch: [42][592/817]	Loss 0.0018 (0.0142)	
training:	Epoch: [42][593/817]	Loss 0.0015 (0.0142)	
training:	Epoch: [42][594/817]	Loss 0.0041 (0.0141)	
training:	Epoch: [42][595/817]	Loss 0.0031 (0.0141)	
training:	Epoch: [42][596/817]	Loss 0.0032 (0.0141)	
training:	Epoch: [42][597/817]	Loss 0.0018 (0.0141)	
training:	Epoch: [42][598/817]	Loss 0.0021 (0.0141)	
training:	Epoch: [42][599/817]	Loss 0.0021 (0.0141)	
training:	Epoch: [42][600/817]	Loss 0.0018 (0.0140)	
training:	Epoch: [42][601/817]	Loss 0.0022 (0.0140)	
training:	Epoch: [42][602/817]	Loss 0.0031 (0.0140)	
training:	Epoch: [42][603/817]	Loss 0.0019 (0.0140)	
training:	Epoch: [42][604/817]	Loss 0.0022 (0.0140)	
training:	Epoch: [42][605/817]	Loss 0.0020 (0.0139)	
training:	Epoch: [42][606/817]	Loss 0.0018 (0.0139)	
training:	Epoch: [42][607/817]	Loss 0.0135 (0.0139)	
training:	Epoch: [42][608/817]	Loss 0.0023 (0.0139)	
training:	Epoch: [42][609/817]	Loss 0.0018 (0.0139)	
training:	Epoch: [42][610/817]	Loss 0.0016 (0.0139)	
training:	Epoch: [42][611/817]	Loss 0.0022 (0.0138)	
training:	Epoch: [42][612/817]	Loss 0.0021 (0.0138)	
training:	Epoch: [42][613/817]	Loss 0.0017 (0.0138)	
training:	Epoch: [42][614/817]	Loss 0.0029 (0.0138)	
training:	Epoch: [42][615/817]	Loss 0.0019 (0.0138)	
training:	Epoch: [42][616/817]	Loss 0.0032 (0.0137)	
training:	Epoch: [42][617/817]	Loss 0.0020 (0.0137)	
training:	Epoch: [42][618/817]	Loss 0.0023 (0.0137)	
training:	Epoch: [42][619/817]	Loss 0.0025 (0.0137)	
training:	Epoch: [42][620/817]	Loss 0.0032 (0.0137)	
training:	Epoch: [42][621/817]	Loss 0.0016 (0.0136)	
training:	Epoch: [42][622/817]	Loss 0.0071 (0.0136)	
training:	Epoch: [42][623/817]	Loss 0.0019 (0.0136)	
training:	Epoch: [42][624/817]	Loss 0.4360 (0.0143)	
training:	Epoch: [42][625/817]	Loss 0.0017 (0.0143)	
training:	Epoch: [42][626/817]	Loss 0.0031 (0.0143)	
training:	Epoch: [42][627/817]	Loss 0.0014 (0.0142)	
training:	Epoch: [42][628/817]	Loss 0.3021 (0.0147)	
training:	Epoch: [42][629/817]	Loss 0.0481 (0.0148)	
training:	Epoch: [42][630/817]	Loss 0.0025 (0.0147)	
training:	Epoch: [42][631/817]	Loss 0.0029 (0.0147)	
training:	Epoch: [42][632/817]	Loss 0.0027 (0.0147)	
training:	Epoch: [42][633/817]	Loss 0.0013 (0.0147)	
training:	Epoch: [42][634/817]	Loss 0.0070 (0.0147)	
training:	Epoch: [42][635/817]	Loss 0.0020 (0.0146)	
training:	Epoch: [42][636/817]	Loss 0.0024 (0.0146)	
training:	Epoch: [42][637/817]	Loss 0.0017 (0.0146)	
training:	Epoch: [42][638/817]	Loss 0.0017 (0.0146)	
training:	Epoch: [42][639/817]	Loss 0.0100 (0.0146)	
training:	Epoch: [42][640/817]	Loss 0.0026 (0.0146)	
training:	Epoch: [42][641/817]	Loss 0.0181 (0.0146)	
training:	Epoch: [42][642/817]	Loss 0.0081 (0.0146)	
training:	Epoch: [42][643/817]	Loss 0.0022 (0.0145)	
training:	Epoch: [42][644/817]	Loss 0.0019 (0.0145)	
training:	Epoch: [42][645/817]	Loss 0.0038 (0.0145)	
training:	Epoch: [42][646/817]	Loss 0.0058 (0.0145)	
training:	Epoch: [42][647/817]	Loss 0.0025 (0.0145)	
training:	Epoch: [42][648/817]	Loss 0.1433 (0.0147)	
training:	Epoch: [42][649/817]	Loss 0.0017 (0.0146)	
training:	Epoch: [42][650/817]	Loss 0.0020 (0.0146)	
training:	Epoch: [42][651/817]	Loss 0.0027 (0.0146)	
training:	Epoch: [42][652/817]	Loss 0.0021 (0.0146)	
training:	Epoch: [42][653/817]	Loss 0.0050 (0.0146)	
training:	Epoch: [42][654/817]	Loss 0.2642 (0.0150)	
training:	Epoch: [42][655/817]	Loss 0.0024 (0.0149)	
training:	Epoch: [42][656/817]	Loss 0.2450 (0.0153)	
training:	Epoch: [42][657/817]	Loss 0.0112 (0.0153)	
training:	Epoch: [42][658/817]	Loss 0.0036 (0.0153)	
training:	Epoch: [42][659/817]	Loss 0.0040 (0.0152)	
training:	Epoch: [42][660/817]	Loss 0.0028 (0.0152)	
training:	Epoch: [42][661/817]	Loss 0.0041 (0.0152)	
training:	Epoch: [42][662/817]	Loss 0.0119 (0.0152)	
training:	Epoch: [42][663/817]	Loss 0.0448 (0.0152)	
training:	Epoch: [42][664/817]	Loss 0.1257 (0.0154)	
training:	Epoch: [42][665/817]	Loss 0.0973 (0.0155)	
training:	Epoch: [42][666/817]	Loss 0.0030 (0.0155)	
training:	Epoch: [42][667/817]	Loss 0.0279 (0.0155)	
training:	Epoch: [42][668/817]	Loss 0.0023 (0.0155)	
training:	Epoch: [42][669/817]	Loss 0.0034 (0.0155)	
training:	Epoch: [42][670/817]	Loss 0.0380 (0.0155)	
training:	Epoch: [42][671/817]	Loss 0.0135 (0.0155)	
training:	Epoch: [42][672/817]	Loss 0.0052 (0.0155)	
training:	Epoch: [42][673/817]	Loss 0.0029 (0.0155)	
training:	Epoch: [42][674/817]	Loss 0.0100 (0.0155)	
training:	Epoch: [42][675/817]	Loss 0.0028 (0.0155)	
training:	Epoch: [42][676/817]	Loss 0.0042 (0.0154)	
training:	Epoch: [42][677/817]	Loss 0.0102 (0.0154)	
training:	Epoch: [42][678/817]	Loss 0.0029 (0.0154)	
training:	Epoch: [42][679/817]	Loss 0.1544 (0.0156)	
training:	Epoch: [42][680/817]	Loss 0.0033 (0.0156)	
training:	Epoch: [42][681/817]	Loss 0.0031 (0.0156)	
training:	Epoch: [42][682/817]	Loss 0.0293 (0.0156)	
training:	Epoch: [42][683/817]	Loss 0.0132 (0.0156)	
training:	Epoch: [42][684/817]	Loss 0.0102 (0.0156)	
training:	Epoch: [42][685/817]	Loss 0.0032 (0.0156)	
training:	Epoch: [42][686/817]	Loss 0.0168 (0.0156)	
training:	Epoch: [42][687/817]	Loss 0.0045 (0.0156)	
training:	Epoch: [42][688/817]	Loss 0.0383 (0.0156)	
training:	Epoch: [42][689/817]	Loss 0.0041 (0.0156)	
training:	Epoch: [42][690/817]	Loss 0.0089 (0.0156)	
training:	Epoch: [42][691/817]	Loss 0.0015 (0.0156)	
training:	Epoch: [42][692/817]	Loss 0.0254 (0.0156)	
training:	Epoch: [42][693/817]	Loss 0.0087 (0.0156)	
training:	Epoch: [42][694/817]	Loss 0.0024 (0.0155)	
training:	Epoch: [42][695/817]	Loss 0.0034 (0.0155)	
training:	Epoch: [42][696/817]	Loss 0.7076 (0.0165)	
training:	Epoch: [42][697/817]	Loss 0.0042 (0.0165)	
training:	Epoch: [42][698/817]	Loss 0.0014 (0.0165)	
training:	Epoch: [42][699/817]	Loss 0.0117 (0.0165)	
training:	Epoch: [42][700/817]	Loss 0.0029 (0.0165)	
training:	Epoch: [42][701/817]	Loss 0.0021 (0.0164)	
training:	Epoch: [42][702/817]	Loss 0.0040 (0.0164)	
training:	Epoch: [42][703/817]	Loss 0.0017 (0.0164)	
training:	Epoch: [42][704/817]	Loss 0.0343 (0.0164)	
training:	Epoch: [42][705/817]	Loss 0.0017 (0.0164)	
training:	Epoch: [42][706/817]	Loss 0.0019 (0.0164)	
training:	Epoch: [42][707/817]	Loss 0.0025 (0.0164)	
training:	Epoch: [42][708/817]	Loss 0.0017 (0.0163)	
training:	Epoch: [42][709/817]	Loss 0.0033 (0.0163)	
training:	Epoch: [42][710/817]	Loss 0.0302 (0.0163)	
training:	Epoch: [42][711/817]	Loss 0.0032 (0.0163)	
training:	Epoch: [42][712/817]	Loss 0.0053 (0.0163)	
training:	Epoch: [42][713/817]	Loss 0.0027 (0.0163)	
training:	Epoch: [42][714/817]	Loss 0.0032 (0.0163)	
training:	Epoch: [42][715/817]	Loss 0.0071 (0.0163)	
training:	Epoch: [42][716/817]	Loss 0.0035 (0.0162)	
training:	Epoch: [42][717/817]	Loss 0.0021 (0.0162)	
training:	Epoch: [42][718/817]	Loss 0.0018 (0.0162)	
training:	Epoch: [42][719/817]	Loss 0.0083 (0.0162)	
training:	Epoch: [42][720/817]	Loss 0.0032 (0.0162)	
training:	Epoch: [42][721/817]	Loss 0.0119 (0.0162)	
training:	Epoch: [42][722/817]	Loss 0.0044 (0.0161)	
training:	Epoch: [42][723/817]	Loss 0.0057 (0.0161)	
training:	Epoch: [42][724/817]	Loss 0.0087 (0.0161)	
training:	Epoch: [42][725/817]	Loss 0.0017 (0.0161)	
training:	Epoch: [42][726/817]	Loss 0.0246 (0.0161)	
training:	Epoch: [42][727/817]	Loss 0.0123 (0.0161)	
training:	Epoch: [42][728/817]	Loss 0.0042 (0.0161)	
training:	Epoch: [42][729/817]	Loss 0.0017 (0.0161)	
training:	Epoch: [42][730/817]	Loss 0.0182 (0.0161)	
training:	Epoch: [42][731/817]	Loss 0.0223 (0.0161)	
training:	Epoch: [42][732/817]	Loss 0.0021 (0.0161)	
training:	Epoch: [42][733/817]	Loss 0.0023 (0.0160)	
training:	Epoch: [42][734/817]	Loss 0.0022 (0.0160)	
training:	Epoch: [42][735/817]	Loss 0.0026 (0.0160)	
training:	Epoch: [42][736/817]	Loss 0.0022 (0.0160)	
training:	Epoch: [42][737/817]	Loss 0.0055 (0.0160)	
training:	Epoch: [42][738/817]	Loss 0.0075 (0.0160)	
training:	Epoch: [42][739/817]	Loss 0.0029 (0.0159)	
training:	Epoch: [42][740/817]	Loss 0.0027 (0.0159)	
training:	Epoch: [42][741/817]	Loss 0.0022 (0.0159)	
training:	Epoch: [42][742/817]	Loss 0.0083 (0.0159)	
training:	Epoch: [42][743/817]	Loss 0.0038 (0.0159)	
training:	Epoch: [42][744/817]	Loss 0.0025 (0.0159)	
training:	Epoch: [42][745/817]	Loss 0.0021 (0.0158)	
training:	Epoch: [42][746/817]	Loss 0.0115 (0.0158)	
training:	Epoch: [42][747/817]	Loss 0.0019 (0.0158)	
training:	Epoch: [42][748/817]	Loss 0.0018 (0.0158)	
training:	Epoch: [42][749/817]	Loss 0.0038 (0.0158)	
training:	Epoch: [42][750/817]	Loss 0.0018 (0.0158)	
training:	Epoch: [42][751/817]	Loss 0.0025 (0.0157)	
training:	Epoch: [42][752/817]	Loss 0.0063 (0.0157)	
training:	Epoch: [42][753/817]	Loss 0.0080 (0.0157)	
training:	Epoch: [42][754/817]	Loss 0.0017 (0.0157)	
training:	Epoch: [42][755/817]	Loss 0.0019 (0.0157)	
training:	Epoch: [42][756/817]	Loss 0.0025 (0.0157)	
training:	Epoch: [42][757/817]	Loss 0.0020 (0.0157)	
training:	Epoch: [42][758/817]	Loss 0.0017 (0.0156)	
training:	Epoch: [42][759/817]	Loss 0.0024 (0.0156)	
training:	Epoch: [42][760/817]	Loss 0.0032 (0.0156)	
training:	Epoch: [42][761/817]	Loss 0.0017 (0.0156)	
training:	Epoch: [42][762/817]	Loss 0.0043 (0.0156)	
training:	Epoch: [42][763/817]	Loss 0.0015 (0.0156)	
training:	Epoch: [42][764/817]	Loss 0.0070 (0.0155)	
training:	Epoch: [42][765/817]	Loss 0.0037 (0.0155)	
training:	Epoch: [42][766/817]	Loss 0.0023 (0.0155)	
training:	Epoch: [42][767/817]	Loss 0.0023 (0.0155)	
training:	Epoch: [42][768/817]	Loss 0.0022 (0.0155)	
training:	Epoch: [42][769/817]	Loss 0.0023 (0.0155)	
training:	Epoch: [42][770/817]	Loss 0.0026 (0.0154)	
training:	Epoch: [42][771/817]	Loss 0.0013 (0.0154)	
training:	Epoch: [42][772/817]	Loss 0.0033 (0.0154)	
training:	Epoch: [42][773/817]	Loss 0.0052 (0.0154)	
training:	Epoch: [42][774/817]	Loss 0.0015 (0.0154)	
training:	Epoch: [42][775/817]	Loss 0.0018 (0.0154)	
training:	Epoch: [42][776/817]	Loss 0.0020 (0.0153)	
training:	Epoch: [42][777/817]	Loss 0.0051 (0.0153)	
training:	Epoch: [42][778/817]	Loss 0.0562 (0.0154)	
training:	Epoch: [42][779/817]	Loss 0.0020 (0.0154)	
training:	Epoch: [42][780/817]	Loss 0.0044 (0.0153)	
training:	Epoch: [42][781/817]	Loss 0.0026 (0.0153)	
training:	Epoch: [42][782/817]	Loss 0.0241 (0.0153)	
training:	Epoch: [42][783/817]	Loss 0.0024 (0.0153)	
training:	Epoch: [42][784/817]	Loss 0.0023 (0.0153)	
training:	Epoch: [42][785/817]	Loss 0.0030 (0.0153)	
training:	Epoch: [42][786/817]	Loss 0.0024 (0.0153)	
training:	Epoch: [42][787/817]	Loss 0.0030 (0.0153)	
training:	Epoch: [42][788/817]	Loss 0.0015 (0.0152)	
training:	Epoch: [42][789/817]	Loss 0.0018 (0.0152)	
training:	Epoch: [42][790/817]	Loss 0.0018 (0.0152)	
training:	Epoch: [42][791/817]	Loss 0.0140 (0.0152)	
training:	Epoch: [42][792/817]	Loss 0.0034 (0.0152)	
training:	Epoch: [42][793/817]	Loss 0.0034 (0.0152)	
training:	Epoch: [42][794/817]	Loss 0.0019 (0.0152)	
training:	Epoch: [42][795/817]	Loss 0.0019 (0.0151)	
training:	Epoch: [42][796/817]	Loss 0.0028 (0.0151)	
training:	Epoch: [42][797/817]	Loss 0.0015 (0.0151)	
training:	Epoch: [42][798/817]	Loss 0.0025 (0.0151)	
training:	Epoch: [42][799/817]	Loss 0.0028 (0.0151)	
training:	Epoch: [42][800/817]	Loss 0.0024 (0.0151)	
training:	Epoch: [42][801/817]	Loss 0.0014 (0.0150)	
training:	Epoch: [42][802/817]	Loss 0.0047 (0.0150)	
training:	Epoch: [42][803/817]	Loss 0.0019 (0.0150)	
training:	Epoch: [42][804/817]	Loss 0.0054 (0.0150)	
training:	Epoch: [42][805/817]	Loss 0.0019 (0.0150)	
training:	Epoch: [42][806/817]	Loss 0.0037 (0.0150)	
training:	Epoch: [42][807/817]	Loss 0.0088 (0.0150)	
training:	Epoch: [42][808/817]	Loss 0.0026 (0.0150)	
training:	Epoch: [42][809/817]	Loss 0.0033 (0.0149)	
training:	Epoch: [42][810/817]	Loss 0.0015 (0.0149)	
training:	Epoch: [42][811/817]	Loss 0.0029 (0.0149)	
training:	Epoch: [42][812/817]	Loss 0.0016 (0.0149)	
training:	Epoch: [42][813/817]	Loss 0.0016 (0.0149)	
training:	Epoch: [42][814/817]	Loss 0.0018 (0.0149)	
training:	Epoch: [42][815/817]	Loss 0.0014 (0.0148)	
training:	Epoch: [42][816/817]	Loss 0.0016 (0.0148)	
training:	Epoch: [42][817/817]	Loss 0.0016 (0.0148)	
Training:	 Loss: 0.0148

Training:	 ACC: 0.9964 0.9965 0.9979 0.9949
Validation:	 ACC: 0.7827 0.7833 0.7963 0.7691
Validation:	 Best_BACC: 0.8223 0.8218 0.8127 0.8318
Validation:	 Loss: 1.1107
Pretraining:	Epoch 43/200
----------
training:	Epoch: [43][1/817]	Loss 0.0019 (0.0019)	
training:	Epoch: [43][2/817]	Loss 0.2491 (0.1255)	
training:	Epoch: [43][3/817]	Loss 0.0018 (0.0843)	
training:	Epoch: [43][4/817]	Loss 0.0018 (0.0637)	
training:	Epoch: [43][5/817]	Loss 0.0025 (0.0514)	
training:	Epoch: [43][6/817]	Loss 0.0033 (0.0434)	
training:	Epoch: [43][7/817]	Loss 0.0066 (0.0381)	
training:	Epoch: [43][8/817]	Loss 0.0020 (0.0336)	
training:	Epoch: [43][9/817]	Loss 0.0041 (0.0303)	
training:	Epoch: [43][10/817]	Loss 0.0108 (0.0284)	
training:	Epoch: [43][11/817]	Loss 0.0021 (0.0260)	
training:	Epoch: [43][12/817]	Loss 0.0026 (0.0240)	
training:	Epoch: [43][13/817]	Loss 0.0025 (0.0224)	
training:	Epoch: [43][14/817]	Loss 0.0020 (0.0209)	
training:	Epoch: [43][15/817]	Loss 0.0027 (0.0197)	
training:	Epoch: [43][16/817]	Loss 0.0019 (0.0186)	
training:	Epoch: [43][17/817]	Loss 0.0017 (0.0176)	
training:	Epoch: [43][18/817]	Loss 0.0024 (0.0168)	
training:	Epoch: [43][19/817]	Loss 0.0019 (0.0160)	
training:	Epoch: [43][20/817]	Loss 0.0025 (0.0153)	
training:	Epoch: [43][21/817]	Loss 0.0019 (0.0147)	
training:	Epoch: [43][22/817]	Loss 0.0032 (0.0142)	
training:	Epoch: [43][23/817]	Loss 0.0035 (0.0137)	
training:	Epoch: [43][24/817]	Loss 0.0023 (0.0132)	
training:	Epoch: [43][25/817]	Loss 0.0016 (0.0128)	
training:	Epoch: [43][26/817]	Loss 0.0015 (0.0123)	
training:	Epoch: [43][27/817]	Loss 0.0109 (0.0123)	
training:	Epoch: [43][28/817]	Loss 0.0089 (0.0121)	
training:	Epoch: [43][29/817]	Loss 0.0018 (0.0118)	
training:	Epoch: [43][30/817]	Loss 0.0018 (0.0115)	
training:	Epoch: [43][31/817]	Loss 0.0014 (0.0111)	
training:	Epoch: [43][32/817]	Loss 0.0019 (0.0108)	
training:	Epoch: [43][33/817]	Loss 0.0047 (0.0107)	
training:	Epoch: [43][34/817]	Loss 0.0174 (0.0109)	
training:	Epoch: [43][35/817]	Loss 0.0029 (0.0106)	
training:	Epoch: [43][36/817]	Loss 0.0038 (0.0104)	
training:	Epoch: [43][37/817]	Loss 0.0016 (0.0102)	
training:	Epoch: [43][38/817]	Loss 0.0014 (0.0100)	
training:	Epoch: [43][39/817]	Loss 0.0037 (0.0098)	
training:	Epoch: [43][40/817]	Loss 0.0020 (0.0096)	
training:	Epoch: [43][41/817]	Loss 0.0018 (0.0094)	
training:	Epoch: [43][42/817]	Loss 0.0014 (0.0092)	
training:	Epoch: [43][43/817]	Loss 0.0034 (0.0091)	
training:	Epoch: [43][44/817]	Loss 0.0112 (0.0091)	
training:	Epoch: [43][45/817]	Loss 0.0016 (0.0090)	
training:	Epoch: [43][46/817]	Loss 0.0014 (0.0088)	
training:	Epoch: [43][47/817]	Loss 0.1117 (0.0110)	
training:	Epoch: [43][48/817]	Loss 0.0019 (0.0108)	
training:	Epoch: [43][49/817]	Loss 0.0029 (0.0106)	
training:	Epoch: [43][50/817]	Loss 0.0023 (0.0105)	
training:	Epoch: [43][51/817]	Loss 0.0016 (0.0103)	
training:	Epoch: [43][52/817]	Loss 0.0048 (0.0102)	
training:	Epoch: [43][53/817]	Loss 0.0020 (0.0100)	
training:	Epoch: [43][54/817]	Loss 0.0024 (0.0099)	
training:	Epoch: [43][55/817]	Loss 0.0023 (0.0098)	
training:	Epoch: [43][56/817]	Loss 0.0021 (0.0096)	
training:	Epoch: [43][57/817]	Loss 0.0021 (0.0095)	
training:	Epoch: [43][58/817]	Loss 0.0027 (0.0094)	
training:	Epoch: [43][59/817]	Loss 0.0540 (0.0101)	
training:	Epoch: [43][60/817]	Loss 0.0017 (0.0100)	
training:	Epoch: [43][61/817]	Loss 0.0023 (0.0099)	
training:	Epoch: [43][62/817]	Loss 0.0029 (0.0098)	
training:	Epoch: [43][63/817]	Loss 0.0016 (0.0096)	
training:	Epoch: [43][64/817]	Loss 0.0050 (0.0096)	
training:	Epoch: [43][65/817]	Loss 0.0016 (0.0094)	
training:	Epoch: [43][66/817]	Loss 0.0015 (0.0093)	
training:	Epoch: [43][67/817]	Loss 0.0119 (0.0094)	
training:	Epoch: [43][68/817]	Loss 0.0021 (0.0092)	
training:	Epoch: [43][69/817]	Loss 0.0024 (0.0091)	
training:	Epoch: [43][70/817]	Loss 0.0024 (0.0091)	
training:	Epoch: [43][71/817]	Loss 0.0017 (0.0089)	
training:	Epoch: [43][72/817]	Loss 0.0018 (0.0088)	
training:	Epoch: [43][73/817]	Loss 0.0044 (0.0088)	
training:	Epoch: [43][74/817]	Loss 0.0020 (0.0087)	
training:	Epoch: [43][75/817]	Loss 0.0036 (0.0086)	
training:	Epoch: [43][76/817]	Loss 0.0027 (0.0085)	
training:	Epoch: [43][77/817]	Loss 0.0014 (0.0085)	
training:	Epoch: [43][78/817]	Loss 0.0014 (0.0084)	
training:	Epoch: [43][79/817]	Loss 0.0029 (0.0083)	
training:	Epoch: [43][80/817]	Loss 0.0041 (0.0082)	
training:	Epoch: [43][81/817]	Loss 0.0059 (0.0082)	
training:	Epoch: [43][82/817]	Loss 0.0019 (0.0081)	
training:	Epoch: [43][83/817]	Loss 0.0016 (0.0081)	
training:	Epoch: [43][84/817]	Loss 0.0092 (0.0081)	
training:	Epoch: [43][85/817]	Loss 0.0059 (0.0080)	
training:	Epoch: [43][86/817]	Loss 0.0020 (0.0080)	
training:	Epoch: [43][87/817]	Loss 0.0058 (0.0080)	
training:	Epoch: [43][88/817]	Loss 0.0033 (0.0079)	
training:	Epoch: [43][89/817]	Loss 0.0017 (0.0078)	
training:	Epoch: [43][90/817]	Loss 0.0027 (0.0078)	
training:	Epoch: [43][91/817]	Loss 0.0057 (0.0078)	
training:	Epoch: [43][92/817]	Loss 0.0016 (0.0077)	
training:	Epoch: [43][93/817]	Loss 0.0036 (0.0076)	
training:	Epoch: [43][94/817]	Loss 0.0019 (0.0076)	
training:	Epoch: [43][95/817]	Loss 0.0021 (0.0075)	
training:	Epoch: [43][96/817]	Loss 0.0025 (0.0075)	
training:	Epoch: [43][97/817]	Loss 0.0031 (0.0074)	
training:	Epoch: [43][98/817]	Loss 0.0022 (0.0074)	
training:	Epoch: [43][99/817]	Loss 0.0018 (0.0073)	
training:	Epoch: [43][100/817]	Loss 0.0021 (0.0073)	
training:	Epoch: [43][101/817]	Loss 0.0118 (0.0073)	
training:	Epoch: [43][102/817]	Loss 0.0027 (0.0073)	
training:	Epoch: [43][103/817]	Loss 0.0022 (0.0072)	
training:	Epoch: [43][104/817]	Loss 0.0018 (0.0072)	
training:	Epoch: [43][105/817]	Loss 0.0016 (0.0071)	
training:	Epoch: [43][106/817]	Loss 0.0020 (0.0071)	
training:	Epoch: [43][107/817]	Loss 0.0018 (0.0070)	
training:	Epoch: [43][108/817]	Loss 0.0018 (0.0070)	
training:	Epoch: [43][109/817]	Loss 0.0021 (0.0069)	
training:	Epoch: [43][110/817]	Loss 0.0016 (0.0069)	
training:	Epoch: [43][111/817]	Loss 0.0029 (0.0068)	
training:	Epoch: [43][112/817]	Loss 0.0017 (0.0068)	
training:	Epoch: [43][113/817]	Loss 0.0019 (0.0067)	
training:	Epoch: [43][114/817]	Loss 0.0067 (0.0067)	
training:	Epoch: [43][115/817]	Loss 0.1554 (0.0080)	
training:	Epoch: [43][116/817]	Loss 0.0039 (0.0080)	
training:	Epoch: [43][117/817]	Loss 0.0017 (0.0079)	
training:	Epoch: [43][118/817]	Loss 0.0021 (0.0079)	
training:	Epoch: [43][119/817]	Loss 0.0029 (0.0079)	
training:	Epoch: [43][120/817]	Loss 0.0029 (0.0078)	
training:	Epoch: [43][121/817]	Loss 0.0025 (0.0078)	
training:	Epoch: [43][122/817]	Loss 0.0016 (0.0077)	
training:	Epoch: [43][123/817]	Loss 0.0019 (0.0077)	
training:	Epoch: [43][124/817]	Loss 0.3611 (0.0105)	
training:	Epoch: [43][125/817]	Loss 0.0033 (0.0105)	
training:	Epoch: [43][126/817]	Loss 0.0015 (0.0104)	
training:	Epoch: [43][127/817]	Loss 0.0021 (0.0103)	
training:	Epoch: [43][128/817]	Loss 0.0017 (0.0103)	
training:	Epoch: [43][129/817]	Loss 0.0027 (0.0102)	
training:	Epoch: [43][130/817]	Loss 0.0013 (0.0101)	
training:	Epoch: [43][131/817]	Loss 0.0034 (0.0101)	
training:	Epoch: [43][132/817]	Loss 0.0032 (0.0100)	
training:	Epoch: [43][133/817]	Loss 0.0027 (0.0100)	
training:	Epoch: [43][134/817]	Loss 0.0014 (0.0099)	
training:	Epoch: [43][135/817]	Loss 0.0026 (0.0099)	
training:	Epoch: [43][136/817]	Loss 0.0038 (0.0098)	
training:	Epoch: [43][137/817]	Loss 0.0018 (0.0098)	
training:	Epoch: [43][138/817]	Loss 0.0046 (0.0097)	
training:	Epoch: [43][139/817]	Loss 0.0032 (0.0097)	
training:	Epoch: [43][140/817]	Loss 0.0021 (0.0096)	
training:	Epoch: [43][141/817]	Loss 0.0019 (0.0096)	
training:	Epoch: [43][142/817]	Loss 0.0017 (0.0095)	
training:	Epoch: [43][143/817]	Loss 0.0034 (0.0095)	
training:	Epoch: [43][144/817]	Loss 0.0020 (0.0094)	
training:	Epoch: [43][145/817]	Loss 0.0018 (0.0094)	
training:	Epoch: [43][146/817]	Loss 0.0058 (0.0093)	
training:	Epoch: [43][147/817]	Loss 0.0031 (0.0093)	
training:	Epoch: [43][148/817]	Loss 0.0020 (0.0092)	
training:	Epoch: [43][149/817]	Loss 0.0023 (0.0092)	
training:	Epoch: [43][150/817]	Loss 0.0035 (0.0092)	
training:	Epoch: [43][151/817]	Loss 0.0026 (0.0091)	
training:	Epoch: [43][152/817]	Loss 0.0020 (0.0091)	
training:	Epoch: [43][153/817]	Loss 0.0033 (0.0090)	
training:	Epoch: [43][154/817]	Loss 0.0026 (0.0090)	
training:	Epoch: [43][155/817]	Loss 0.0022 (0.0089)	
training:	Epoch: [43][156/817]	Loss 0.0018 (0.0089)	
training:	Epoch: [43][157/817]	Loss 0.0019 (0.0089)	
training:	Epoch: [43][158/817]	Loss 0.0025 (0.0088)	
training:	Epoch: [43][159/817]	Loss 0.0032 (0.0088)	
training:	Epoch: [43][160/817]	Loss 0.0025 (0.0087)	
training:	Epoch: [43][161/817]	Loss 0.0044 (0.0087)	
training:	Epoch: [43][162/817]	Loss 0.0020 (0.0087)	
training:	Epoch: [43][163/817]	Loss 0.0027 (0.0086)	
training:	Epoch: [43][164/817]	Loss 0.0043 (0.0086)	
training:	Epoch: [43][165/817]	Loss 0.0017 (0.0086)	
training:	Epoch: [43][166/817]	Loss 0.0017 (0.0085)	
training:	Epoch: [43][167/817]	Loss 0.0116 (0.0085)	
training:	Epoch: [43][168/817]	Loss 0.0033 (0.0085)	
training:	Epoch: [43][169/817]	Loss 0.0027 (0.0085)	
training:	Epoch: [43][170/817]	Loss 0.0035 (0.0084)	
training:	Epoch: [43][171/817]	Loss 0.0016 (0.0084)	
training:	Epoch: [43][172/817]	Loss 0.2400 (0.0098)	
training:	Epoch: [43][173/817]	Loss 0.0017 (0.0097)	
training:	Epoch: [43][174/817]	Loss 0.0014 (0.0097)	
training:	Epoch: [43][175/817]	Loss 0.0042 (0.0096)	
training:	Epoch: [43][176/817]	Loss 0.0025 (0.0096)	
training:	Epoch: [43][177/817]	Loss 0.0041 (0.0096)	
training:	Epoch: [43][178/817]	Loss 0.0044 (0.0095)	
training:	Epoch: [43][179/817]	Loss 0.0036 (0.0095)	
training:	Epoch: [43][180/817]	Loss 0.0021 (0.0095)	
training:	Epoch: [43][181/817]	Loss 0.0017 (0.0094)	
training:	Epoch: [43][182/817]	Loss 0.0020 (0.0094)	
training:	Epoch: [43][183/817]	Loss 0.0092 (0.0094)	
training:	Epoch: [43][184/817]	Loss 0.0076 (0.0094)	
training:	Epoch: [43][185/817]	Loss 0.0026 (0.0093)	
training:	Epoch: [43][186/817]	Loss 0.0017 (0.0093)	
training:	Epoch: [43][187/817]	Loss 0.0032 (0.0093)	
training:	Epoch: [43][188/817]	Loss 0.0019 (0.0092)	
training:	Epoch: [43][189/817]	Loss 0.0016 (0.0092)	
training:	Epoch: [43][190/817]	Loss 0.0020 (0.0091)	
training:	Epoch: [43][191/817]	Loss 0.0033 (0.0091)	
training:	Epoch: [43][192/817]	Loss 0.0026 (0.0091)	
training:	Epoch: [43][193/817]	Loss 0.0033 (0.0090)	
training:	Epoch: [43][194/817]	Loss 0.0027 (0.0090)	
training:	Epoch: [43][195/817]	Loss 0.0018 (0.0090)	
training:	Epoch: [43][196/817]	Loss 0.0026 (0.0089)	
training:	Epoch: [43][197/817]	Loss 0.0020 (0.0089)	
training:	Epoch: [43][198/817]	Loss 0.0023 (0.0089)	
training:	Epoch: [43][199/817]	Loss 0.0023 (0.0088)	
training:	Epoch: [43][200/817]	Loss 0.0026 (0.0088)	
training:	Epoch: [43][201/817]	Loss 0.0026 (0.0088)	
training:	Epoch: [43][202/817]	Loss 0.0052 (0.0088)	
training:	Epoch: [43][203/817]	Loss 0.0104 (0.0088)	
training:	Epoch: [43][204/817]	Loss 0.0025 (0.0087)	
training:	Epoch: [43][205/817]	Loss 0.0015 (0.0087)	
training:	Epoch: [43][206/817]	Loss 0.0028 (0.0087)	
training:	Epoch: [43][207/817]	Loss 0.0019 (0.0086)	
training:	Epoch: [43][208/817]	Loss 0.0029 (0.0086)	
training:	Epoch: [43][209/817]	Loss 0.0019 (0.0086)	
training:	Epoch: [43][210/817]	Loss 0.0021 (0.0085)	
training:	Epoch: [43][211/817]	Loss 0.0095 (0.0086)	
training:	Epoch: [43][212/817]	Loss 0.0017 (0.0085)	
training:	Epoch: [43][213/817]	Loss 0.0014 (0.0085)	
training:	Epoch: [43][214/817]	Loss 0.0015 (0.0085)	
training:	Epoch: [43][215/817]	Loss 0.0059 (0.0084)	
training:	Epoch: [43][216/817]	Loss 0.0116 (0.0085)	
training:	Epoch: [43][217/817]	Loss 0.0015 (0.0084)	
training:	Epoch: [43][218/817]	Loss 0.0015 (0.0084)	
training:	Epoch: [43][219/817]	Loss 0.0035 (0.0084)	
training:	Epoch: [43][220/817]	Loss 0.0033 (0.0083)	
training:	Epoch: [43][221/817]	Loss 0.0017 (0.0083)	
training:	Epoch: [43][222/817]	Loss 0.0018 (0.0083)	
training:	Epoch: [43][223/817]	Loss 0.0020 (0.0083)	
training:	Epoch: [43][224/817]	Loss 0.0033 (0.0082)	
training:	Epoch: [43][225/817]	Loss 0.0024 (0.0082)	
training:	Epoch: [43][226/817]	Loss 0.0088 (0.0082)	
training:	Epoch: [43][227/817]	Loss 0.0019 (0.0082)	
training:	Epoch: [43][228/817]	Loss 0.0078 (0.0082)	
training:	Epoch: [43][229/817]	Loss 0.0017 (0.0082)	
training:	Epoch: [43][230/817]	Loss 0.0017 (0.0081)	
training:	Epoch: [43][231/817]	Loss 0.0017 (0.0081)	
training:	Epoch: [43][232/817]	Loss 0.0026 (0.0081)	
training:	Epoch: [43][233/817]	Loss 0.0023 (0.0081)	
training:	Epoch: [43][234/817]	Loss 0.0025 (0.0080)	
training:	Epoch: [43][235/817]	Loss 0.0022 (0.0080)	
training:	Epoch: [43][236/817]	Loss 0.0016 (0.0080)	
training:	Epoch: [43][237/817]	Loss 0.0128 (0.0080)	
training:	Epoch: [43][238/817]	Loss 0.0017 (0.0080)	
training:	Epoch: [43][239/817]	Loss 0.1830 (0.0087)	
training:	Epoch: [43][240/817]	Loss 0.0050 (0.0087)	
training:	Epoch: [43][241/817]	Loss 0.0018 (0.0087)	
training:	Epoch: [43][242/817]	Loss 0.0021 (0.0086)	
training:	Epoch: [43][243/817]	Loss 0.0044 (0.0086)	
training:	Epoch: [43][244/817]	Loss 0.0014 (0.0086)	
training:	Epoch: [43][245/817]	Loss 0.0014 (0.0086)	
training:	Epoch: [43][246/817]	Loss 0.0020 (0.0085)	
training:	Epoch: [43][247/817]	Loss 0.0016 (0.0085)	
training:	Epoch: [43][248/817]	Loss 0.0038 (0.0085)	
training:	Epoch: [43][249/817]	Loss 0.0062 (0.0085)	
training:	Epoch: [43][250/817]	Loss 0.0017 (0.0084)	
training:	Epoch: [43][251/817]	Loss 0.0031 (0.0084)	
training:	Epoch: [43][252/817]	Loss 0.0022 (0.0084)	
training:	Epoch: [43][253/817]	Loss 0.0046 (0.0084)	
training:	Epoch: [43][254/817]	Loss 0.0031 (0.0084)	
training:	Epoch: [43][255/817]	Loss 0.0029 (0.0083)	
training:	Epoch: [43][256/817]	Loss 0.0020 (0.0083)	
training:	Epoch: [43][257/817]	Loss 0.0018 (0.0083)	
training:	Epoch: [43][258/817]	Loss 0.0021 (0.0083)	
training:	Epoch: [43][259/817]	Loss 0.0035 (0.0082)	
training:	Epoch: [43][260/817]	Loss 0.0015 (0.0082)	
training:	Epoch: [43][261/817]	Loss 0.0014 (0.0082)	
training:	Epoch: [43][262/817]	Loss 0.0106 (0.0082)	
training:	Epoch: [43][263/817]	Loss 0.0029 (0.0082)	
training:	Epoch: [43][264/817]	Loss 0.0032 (0.0082)	
training:	Epoch: [43][265/817]	Loss 0.0024 (0.0081)	
training:	Epoch: [43][266/817]	Loss 0.0015 (0.0081)	
training:	Epoch: [43][267/817]	Loss 0.3812 (0.0095)	
training:	Epoch: [43][268/817]	Loss 0.0016 (0.0095)	
training:	Epoch: [43][269/817]	Loss 0.0014 (0.0095)	
training:	Epoch: [43][270/817]	Loss 0.0014 (0.0094)	
training:	Epoch: [43][271/817]	Loss 0.0018 (0.0094)	
training:	Epoch: [43][272/817]	Loss 0.0112 (0.0094)	
training:	Epoch: [43][273/817]	Loss 0.0030 (0.0094)	
training:	Epoch: [43][274/817]	Loss 0.0019 (0.0094)	
training:	Epoch: [43][275/817]	Loss 0.0016 (0.0093)	
training:	Epoch: [43][276/817]	Loss 0.0137 (0.0093)	
training:	Epoch: [43][277/817]	Loss 0.0042 (0.0093)	
training:	Epoch: [43][278/817]	Loss 0.0017 (0.0093)	
training:	Epoch: [43][279/817]	Loss 0.0028 (0.0093)	
training:	Epoch: [43][280/817]	Loss 0.0021 (0.0092)	
training:	Epoch: [43][281/817]	Loss 0.0015 (0.0092)	
training:	Epoch: [43][282/817]	Loss 0.0045 (0.0092)	
training:	Epoch: [43][283/817]	Loss 0.0015 (0.0092)	
training:	Epoch: [43][284/817]	Loss 0.0023 (0.0092)	
training:	Epoch: [43][285/817]	Loss 0.0016 (0.0091)	
training:	Epoch: [43][286/817]	Loss 0.0083 (0.0091)	
training:	Epoch: [43][287/817]	Loss 0.0018 (0.0091)	
training:	Epoch: [43][288/817]	Loss 0.0049 (0.0091)	
training:	Epoch: [43][289/817]	Loss 0.0017 (0.0091)	
training:	Epoch: [43][290/817]	Loss 0.0023 (0.0090)	
training:	Epoch: [43][291/817]	Loss 0.0024 (0.0090)	
training:	Epoch: [43][292/817]	Loss 0.0041 (0.0090)	
training:	Epoch: [43][293/817]	Loss 0.0017 (0.0090)	
training:	Epoch: [43][294/817]	Loss 0.0016 (0.0089)	
training:	Epoch: [43][295/817]	Loss 0.0073 (0.0089)	
training:	Epoch: [43][296/817]	Loss 0.0024 (0.0089)	
training:	Epoch: [43][297/817]	Loss 0.0064 (0.0089)	
training:	Epoch: [43][298/817]	Loss 0.0018 (0.0089)	
training:	Epoch: [43][299/817]	Loss 0.0014 (0.0089)	
training:	Epoch: [43][300/817]	Loss 0.0016 (0.0088)	
training:	Epoch: [43][301/817]	Loss 0.0207 (0.0089)	
training:	Epoch: [43][302/817]	Loss 0.0024 (0.0089)	
training:	Epoch: [43][303/817]	Loss 0.0016 (0.0088)	
training:	Epoch: [43][304/817]	Loss 0.0120 (0.0088)	
training:	Epoch: [43][305/817]	Loss 0.0015 (0.0088)	
training:	Epoch: [43][306/817]	Loss 0.0017 (0.0088)	
training:	Epoch: [43][307/817]	Loss 0.0024 (0.0088)	
training:	Epoch: [43][308/817]	Loss 0.0015 (0.0087)	
training:	Epoch: [43][309/817]	Loss 0.0024 (0.0087)	
training:	Epoch: [43][310/817]	Loss 0.0024 (0.0087)	
training:	Epoch: [43][311/817]	Loss 0.0015 (0.0087)	
training:	Epoch: [43][312/817]	Loss 0.0022 (0.0087)	
training:	Epoch: [43][313/817]	Loss 0.0019 (0.0086)	
training:	Epoch: [43][314/817]	Loss 0.0020 (0.0086)	
training:	Epoch: [43][315/817]	Loss 0.0016 (0.0086)	
training:	Epoch: [43][316/817]	Loss 0.0320 (0.0087)	
training:	Epoch: [43][317/817]	Loss 0.0016 (0.0086)	
training:	Epoch: [43][318/817]	Loss 0.0025 (0.0086)	
training:	Epoch: [43][319/817]	Loss 0.0022 (0.0086)	
training:	Epoch: [43][320/817]	Loss 0.0025 (0.0086)	
training:	Epoch: [43][321/817]	Loss 0.0033 (0.0086)	
training:	Epoch: [43][322/817]	Loss 0.0032 (0.0086)	
training:	Epoch: [43][323/817]	Loss 0.0146 (0.0086)	
training:	Epoch: [43][324/817]	Loss 0.0091 (0.0086)	
training:	Epoch: [43][325/817]	Loss 0.0016 (0.0086)	
training:	Epoch: [43][326/817]	Loss 0.0021 (0.0085)	
training:	Epoch: [43][327/817]	Loss 0.0042 (0.0085)	
training:	Epoch: [43][328/817]	Loss 0.0031 (0.0085)	
training:	Epoch: [43][329/817]	Loss 0.0022 (0.0085)	
training:	Epoch: [43][330/817]	Loss 0.0022 (0.0085)	
training:	Epoch: [43][331/817]	Loss 0.0389 (0.0086)	
training:	Epoch: [43][332/817]	Loss 0.0017 (0.0085)	
training:	Epoch: [43][333/817]	Loss 0.0022 (0.0085)	
training:	Epoch: [43][334/817]	Loss 0.0014 (0.0085)	
training:	Epoch: [43][335/817]	Loss 0.0035 (0.0085)	
training:	Epoch: [43][336/817]	Loss 0.0014 (0.0085)	
training:	Epoch: [43][337/817]	Loss 0.0024 (0.0084)	
training:	Epoch: [43][338/817]	Loss 0.0014 (0.0084)	
training:	Epoch: [43][339/817]	Loss 0.0013 (0.0084)	
training:	Epoch: [43][340/817]	Loss 0.0066 (0.0084)	
training:	Epoch: [43][341/817]	Loss 0.0022 (0.0084)	
training:	Epoch: [43][342/817]	Loss 0.0386 (0.0085)	
training:	Epoch: [43][343/817]	Loss 0.0033 (0.0085)	
training:	Epoch: [43][344/817]	Loss 0.0025 (0.0084)	
training:	Epoch: [43][345/817]	Loss 0.0032 (0.0084)	
training:	Epoch: [43][346/817]	Loss 0.0017 (0.0084)	
training:	Epoch: [43][347/817]	Loss 0.0016 (0.0084)	
training:	Epoch: [43][348/817]	Loss 0.0017 (0.0084)	
training:	Epoch: [43][349/817]	Loss 0.0023 (0.0083)	
training:	Epoch: [43][350/817]	Loss 0.0020 (0.0083)	
training:	Epoch: [43][351/817]	Loss 0.0152 (0.0083)	
training:	Epoch: [43][352/817]	Loss 0.0025 (0.0083)	
training:	Epoch: [43][353/817]	Loss 0.0017 (0.0083)	
training:	Epoch: [43][354/817]	Loss 0.0016 (0.0083)	
training:	Epoch: [43][355/817]	Loss 0.0016 (0.0083)	
training:	Epoch: [43][356/817]	Loss 0.0016 (0.0083)	
training:	Epoch: [43][357/817]	Loss 0.0015 (0.0082)	
training:	Epoch: [43][358/817]	Loss 0.0019 (0.0082)	
training:	Epoch: [43][359/817]	Loss 0.0258 (0.0083)	
training:	Epoch: [43][360/817]	Loss 0.0016 (0.0082)	
training:	Epoch: [43][361/817]	Loss 0.0014 (0.0082)	
training:	Epoch: [43][362/817]	Loss 0.0034 (0.0082)	
training:	Epoch: [43][363/817]	Loss 0.0014 (0.0082)	
training:	Epoch: [43][364/817]	Loss 0.0015 (0.0082)	
training:	Epoch: [43][365/817]	Loss 0.0019 (0.0082)	
training:	Epoch: [43][366/817]	Loss 0.0017 (0.0081)	
training:	Epoch: [43][367/817]	Loss 0.0019 (0.0081)	
training:	Epoch: [43][368/817]	Loss 0.0014 (0.0081)	
training:	Epoch: [43][369/817]	Loss 0.0072 (0.0081)	
training:	Epoch: [43][370/817]	Loss 0.0026 (0.0081)	
training:	Epoch: [43][371/817]	Loss 0.0046 (0.0081)	
training:	Epoch: [43][372/817]	Loss 0.0041 (0.0081)	
training:	Epoch: [43][373/817]	Loss 0.0019 (0.0081)	
training:	Epoch: [43][374/817]	Loss 0.0015 (0.0080)	
training:	Epoch: [43][375/817]	Loss 0.0024 (0.0080)	
training:	Epoch: [43][376/817]	Loss 0.0031 (0.0080)	
training:	Epoch: [43][377/817]	Loss 0.0015 (0.0080)	
training:	Epoch: [43][378/817]	Loss 0.0018 (0.0080)	
training:	Epoch: [43][379/817]	Loss 0.0019 (0.0080)	
training:	Epoch: [43][380/817]	Loss 0.0021 (0.0079)	
training:	Epoch: [43][381/817]	Loss 0.0027 (0.0079)	
training:	Epoch: [43][382/817]	Loss 0.0014 (0.0079)	
training:	Epoch: [43][383/817]	Loss 0.0018 (0.0079)	
training:	Epoch: [43][384/817]	Loss 0.0015 (0.0079)	
training:	Epoch: [43][385/817]	Loss 0.0021 (0.0079)	
training:	Epoch: [43][386/817]	Loss 0.0027 (0.0079)	
training:	Epoch: [43][387/817]	Loss 0.0018 (0.0078)	
training:	Epoch: [43][388/817]	Loss 0.0022 (0.0078)	
training:	Epoch: [43][389/817]	Loss 0.0028 (0.0078)	
training:	Epoch: [43][390/817]	Loss 0.0014 (0.0078)	
training:	Epoch: [43][391/817]	Loss 0.0016 (0.0078)	
training:	Epoch: [43][392/817]	Loss 0.0018 (0.0078)	
training:	Epoch: [43][393/817]	Loss 0.0665 (0.0079)	
training:	Epoch: [43][394/817]	Loss 0.0020 (0.0079)	
training:	Epoch: [43][395/817]	Loss 0.0020 (0.0079)	
training:	Epoch: [43][396/817]	Loss 0.0051 (0.0079)	
training:	Epoch: [43][397/817]	Loss 0.0021 (0.0079)	
training:	Epoch: [43][398/817]	Loss 0.0025 (0.0078)	
training:	Epoch: [43][399/817]	Loss 0.0536 (0.0080)	
training:	Epoch: [43][400/817]	Loss 0.5479 (0.0093)	
training:	Epoch: [43][401/817]	Loss 0.0015 (0.0093)	
training:	Epoch: [43][402/817]	Loss 0.0016 (0.0093)	
training:	Epoch: [43][403/817]	Loss 0.0011 (0.0093)	
training:	Epoch: [43][404/817]	Loss 0.0018 (0.0092)	
training:	Epoch: [43][405/817]	Loss 0.0047 (0.0092)	
training:	Epoch: [43][406/817]	Loss 0.0045 (0.0092)	
training:	Epoch: [43][407/817]	Loss 0.0020 (0.0092)	
training:	Epoch: [43][408/817]	Loss 0.0026 (0.0092)	
training:	Epoch: [43][409/817]	Loss 0.0062 (0.0092)	
training:	Epoch: [43][410/817]	Loss 0.0023 (0.0092)	
training:	Epoch: [43][411/817]	Loss 0.0016 (0.0091)	
training:	Epoch: [43][412/817]	Loss 0.0031 (0.0091)	
training:	Epoch: [43][413/817]	Loss 0.0046 (0.0091)	
training:	Epoch: [43][414/817]	Loss 0.0015 (0.0091)	
training:	Epoch: [43][415/817]	Loss 0.0016 (0.0091)	
training:	Epoch: [43][416/817]	Loss 0.0016 (0.0091)	
training:	Epoch: [43][417/817]	Loss 0.0013 (0.0090)	
training:	Epoch: [43][418/817]	Loss 0.0046 (0.0090)	
training:	Epoch: [43][419/817]	Loss 0.0026 (0.0090)	
training:	Epoch: [43][420/817]	Loss 0.0020 (0.0090)	
training:	Epoch: [43][421/817]	Loss 0.0019 (0.0090)	
training:	Epoch: [43][422/817]	Loss 0.0032 (0.0090)	
training:	Epoch: [43][423/817]	Loss 0.0057 (0.0090)	
training:	Epoch: [43][424/817]	Loss 0.0017 (0.0089)	
training:	Epoch: [43][425/817]	Loss 0.0024 (0.0089)	
training:	Epoch: [43][426/817]	Loss 0.0016 (0.0089)	
training:	Epoch: [43][427/817]	Loss 0.0016 (0.0089)	
training:	Epoch: [43][428/817]	Loss 0.0015 (0.0089)	
training:	Epoch: [43][429/817]	Loss 0.0018 (0.0089)	
training:	Epoch: [43][430/817]	Loss 0.0017 (0.0088)	
training:	Epoch: [43][431/817]	Loss 0.0292 (0.0089)	
training:	Epoch: [43][432/817]	Loss 0.0015 (0.0089)	
training:	Epoch: [43][433/817]	Loss 0.0015 (0.0089)	
training:	Epoch: [43][434/817]	Loss 0.0046 (0.0088)	
training:	Epoch: [43][435/817]	Loss 0.0176 (0.0089)	
training:	Epoch: [43][436/817]	Loss 0.0033 (0.0088)	
training:	Epoch: [43][437/817]	Loss 0.0015 (0.0088)	
training:	Epoch: [43][438/817]	Loss 0.0023 (0.0088)	
training:	Epoch: [43][439/817]	Loss 0.0024 (0.0088)	
training:	Epoch: [43][440/817]	Loss 0.0028 (0.0088)	
training:	Epoch: [43][441/817]	Loss 0.0039 (0.0088)	
training:	Epoch: [43][442/817]	Loss 0.3288 (0.0095)	
training:	Epoch: [43][443/817]	Loss 0.0028 (0.0095)	
training:	Epoch: [43][444/817]	Loss 0.0022 (0.0095)	
training:	Epoch: [43][445/817]	Loss 0.0018 (0.0095)	
training:	Epoch: [43][446/817]	Loss 0.0013 (0.0094)	
training:	Epoch: [43][447/817]	Loss 0.0017 (0.0094)	
training:	Epoch: [43][448/817]	Loss 0.2431 (0.0099)	
training:	Epoch: [43][449/817]	Loss 0.0054 (0.0099)	
training:	Epoch: [43][450/817]	Loss 0.0046 (0.0099)	
training:	Epoch: [43][451/817]	Loss 0.0020 (0.0099)	
training:	Epoch: [43][452/817]	Loss 0.0018 (0.0099)	
training:	Epoch: [43][453/817]	Loss 0.0030 (0.0099)	
training:	Epoch: [43][454/817]	Loss 0.0072 (0.0099)	
training:	Epoch: [43][455/817]	Loss 0.0033 (0.0098)	
training:	Epoch: [43][456/817]	Loss 0.0015 (0.0098)	
training:	Epoch: [43][457/817]	Loss 0.0228 (0.0099)	
training:	Epoch: [43][458/817]	Loss 0.0016 (0.0098)	
training:	Epoch: [43][459/817]	Loss 0.0019 (0.0098)	
training:	Epoch: [43][460/817]	Loss 0.0046 (0.0098)	
training:	Epoch: [43][461/817]	Loss 0.0103 (0.0098)	
training:	Epoch: [43][462/817]	Loss 0.0026 (0.0098)	
training:	Epoch: [43][463/817]	Loss 0.0138 (0.0098)	
training:	Epoch: [43][464/817]	Loss 0.0037 (0.0098)	
training:	Epoch: [43][465/817]	Loss 0.0012 (0.0098)	
training:	Epoch: [43][466/817]	Loss 0.0109 (0.0098)	
training:	Epoch: [43][467/817]	Loss 0.0035 (0.0098)	
training:	Epoch: [43][468/817]	Loss 0.0018 (0.0097)	
training:	Epoch: [43][469/817]	Loss 0.0027 (0.0097)	
training:	Epoch: [43][470/817]	Loss 0.0110 (0.0097)	
training:	Epoch: [43][471/817]	Loss 0.0020 (0.0097)	
training:	Epoch: [43][472/817]	Loss 0.0092 (0.0097)	
training:	Epoch: [43][473/817]	Loss 0.0025 (0.0097)	
training:	Epoch: [43][474/817]	Loss 0.0028 (0.0097)	
training:	Epoch: [43][475/817]	Loss 0.0032 (0.0097)	
training:	Epoch: [43][476/817]	Loss 0.0047 (0.0097)	
training:	Epoch: [43][477/817]	Loss 0.0026 (0.0096)	
training:	Epoch: [43][478/817]	Loss 0.0020 (0.0096)	
training:	Epoch: [43][479/817]	Loss 0.0060 (0.0096)	
training:	Epoch: [43][480/817]	Loss 0.0015 (0.0096)	
training:	Epoch: [43][481/817]	Loss 0.0050 (0.0096)	
training:	Epoch: [43][482/817]	Loss 0.0083 (0.0096)	
training:	Epoch: [43][483/817]	Loss 0.0016 (0.0096)	
training:	Epoch: [43][484/817]	Loss 0.0015 (0.0096)	
training:	Epoch: [43][485/817]	Loss 0.0105 (0.0096)	
training:	Epoch: [43][486/817]	Loss 0.2481 (0.0101)	
training:	Epoch: [43][487/817]	Loss 0.0019 (0.0100)	
training:	Epoch: [43][488/817]	Loss 0.0103 (0.0100)	
training:	Epoch: [43][489/817]	Loss 0.0044 (0.0100)	
training:	Epoch: [43][490/817]	Loss 0.0020 (0.0100)	
training:	Epoch: [43][491/817]	Loss 0.0025 (0.0100)	
training:	Epoch: [43][492/817]	Loss 0.0034 (0.0100)	
training:	Epoch: [43][493/817]	Loss 0.0013 (0.0100)	
training:	Epoch: [43][494/817]	Loss 0.0030 (0.0099)	
training:	Epoch: [43][495/817]	Loss 0.0083 (0.0099)	
training:	Epoch: [43][496/817]	Loss 0.0045 (0.0099)	
training:	Epoch: [43][497/817]	Loss 0.0013 (0.0099)	
training:	Epoch: [43][498/817]	Loss 0.0029 (0.0099)	
training:	Epoch: [43][499/817]	Loss 0.0034 (0.0099)	
training:	Epoch: [43][500/817]	Loss 0.0019 (0.0099)	
training:	Epoch: [43][501/817]	Loss 0.0132 (0.0099)	
training:	Epoch: [43][502/817]	Loss 0.0018 (0.0099)	
training:	Epoch: [43][503/817]	Loss 0.0018 (0.0098)	
training:	Epoch: [43][504/817]	Loss 0.0027 (0.0098)	
training:	Epoch: [43][505/817]	Loss 0.0057 (0.0098)	
training:	Epoch: [43][506/817]	Loss 0.0022 (0.0098)	
training:	Epoch: [43][507/817]	Loss 0.0069 (0.0098)	
training:	Epoch: [43][508/817]	Loss 0.0075 (0.0098)	
training:	Epoch: [43][509/817]	Loss 0.0050 (0.0098)	
training:	Epoch: [43][510/817]	Loss 0.0014 (0.0098)	
training:	Epoch: [43][511/817]	Loss 0.0014 (0.0098)	
training:	Epoch: [43][512/817]	Loss 0.0054 (0.0097)	
training:	Epoch: [43][513/817]	Loss 0.0015 (0.0097)	
training:	Epoch: [43][514/817]	Loss 0.0110 (0.0097)	
training:	Epoch: [43][515/817]	Loss 0.0015 (0.0097)	
training:	Epoch: [43][516/817]	Loss 0.0353 (0.0098)	
training:	Epoch: [43][517/817]	Loss 0.0025 (0.0098)	
training:	Epoch: [43][518/817]	Loss 0.0053 (0.0097)	
training:	Epoch: [43][519/817]	Loss 0.0016 (0.0097)	
training:	Epoch: [43][520/817]	Loss 0.0022 (0.0097)	
training:	Epoch: [43][521/817]	Loss 0.0070 (0.0097)	
training:	Epoch: [43][522/817]	Loss 0.0118 (0.0097)	
training:	Epoch: [43][523/817]	Loss 0.0017 (0.0097)	
training:	Epoch: [43][524/817]	Loss 0.0072 (0.0097)	
training:	Epoch: [43][525/817]	Loss 0.0017 (0.0097)	
training:	Epoch: [43][526/817]	Loss 0.0020 (0.0097)	
training:	Epoch: [43][527/817]	Loss 0.0021 (0.0097)	
training:	Epoch: [43][528/817]	Loss 0.0029 (0.0096)	
training:	Epoch: [43][529/817]	Loss 0.0022 (0.0096)	
training:	Epoch: [43][530/817]	Loss 0.2787 (0.0101)	
training:	Epoch: [43][531/817]	Loss 0.0025 (0.0101)	
training:	Epoch: [43][532/817]	Loss 0.0024 (0.0101)	
training:	Epoch: [43][533/817]	Loss 0.0014 (0.0101)	
training:	Epoch: [43][534/817]	Loss 0.0350 (0.0101)	
training:	Epoch: [43][535/817]	Loss 0.0022 (0.0101)	
training:	Epoch: [43][536/817]	Loss 0.0019 (0.0101)	
training:	Epoch: [43][537/817]	Loss 0.0017 (0.0101)	
training:	Epoch: [43][538/817]	Loss 0.0021 (0.0101)	
training:	Epoch: [43][539/817]	Loss 0.1198 (0.0103)	
training:	Epoch: [43][540/817]	Loss 0.0032 (0.0103)	
training:	Epoch: [43][541/817]	Loss 0.0027 (0.0102)	
training:	Epoch: [43][542/817]	Loss 0.0020 (0.0102)	
training:	Epoch: [43][543/817]	Loss 0.0016 (0.0102)	
training:	Epoch: [43][544/817]	Loss 0.0061 (0.0102)	
training:	Epoch: [43][545/817]	Loss 0.0027 (0.0102)	
training:	Epoch: [43][546/817]	Loss 0.0031 (0.0102)	
training:	Epoch: [43][547/817]	Loss 0.0041 (0.0102)	
training:	Epoch: [43][548/817]	Loss 0.0018 (0.0102)	
training:	Epoch: [43][549/817]	Loss 0.0039 (0.0101)	
training:	Epoch: [43][550/817]	Loss 0.0034 (0.0101)	
training:	Epoch: [43][551/817]	Loss 0.0107 (0.0101)	
training:	Epoch: [43][552/817]	Loss 0.0016 (0.0101)	
training:	Epoch: [43][553/817]	Loss 0.0016 (0.0101)	
training:	Epoch: [43][554/817]	Loss 0.0063 (0.0101)	
training:	Epoch: [43][555/817]	Loss 0.0113 (0.0101)	
training:	Epoch: [43][556/817]	Loss 0.0016 (0.0101)	
training:	Epoch: [43][557/817]	Loss 0.0023 (0.0101)	
training:	Epoch: [43][558/817]	Loss 0.0015 (0.0101)	
training:	Epoch: [43][559/817]	Loss 0.0032 (0.0100)	
training:	Epoch: [43][560/817]	Loss 0.0037 (0.0100)	
training:	Epoch: [43][561/817]	Loss 0.0026 (0.0100)	
training:	Epoch: [43][562/817]	Loss 0.0015 (0.0100)	
training:	Epoch: [43][563/817]	Loss 0.0207 (0.0100)	
training:	Epoch: [43][564/817]	Loss 0.0047 (0.0100)	
training:	Epoch: [43][565/817]	Loss 0.0043 (0.0100)	
training:	Epoch: [43][566/817]	Loss 0.0045 (0.0100)	
training:	Epoch: [43][567/817]	Loss 0.0017 (0.0100)	
training:	Epoch: [43][568/817]	Loss 0.0030 (0.0100)	
training:	Epoch: [43][569/817]	Loss 0.0086 (0.0100)	
training:	Epoch: [43][570/817]	Loss 0.0162 (0.0100)	
training:	Epoch: [43][571/817]	Loss 0.0020 (0.0100)	
training:	Epoch: [43][572/817]	Loss 0.0214 (0.0100)	
training:	Epoch: [43][573/817]	Loss 0.0014 (0.0100)	
training:	Epoch: [43][574/817]	Loss 0.0016 (0.0100)	
training:	Epoch: [43][575/817]	Loss 0.0136 (0.0100)	
training:	Epoch: [43][576/817]	Loss 0.0018 (0.0099)	
training:	Epoch: [43][577/817]	Loss 0.0025 (0.0099)	
training:	Epoch: [43][578/817]	Loss 0.0020 (0.0099)	
training:	Epoch: [43][579/817]	Loss 0.0058 (0.0099)	
training:	Epoch: [43][580/817]	Loss 0.0032 (0.0099)	
training:	Epoch: [43][581/817]	Loss 0.0028 (0.0099)	
training:	Epoch: [43][582/817]	Loss 0.0036 (0.0099)	
training:	Epoch: [43][583/817]	Loss 0.0021 (0.0099)	
training:	Epoch: [43][584/817]	Loss 0.0029 (0.0098)	
training:	Epoch: [43][585/817]	Loss 0.0081 (0.0098)	
training:	Epoch: [43][586/817]	Loss 0.0037 (0.0098)	
training:	Epoch: [43][587/817]	Loss 0.0050 (0.0098)	
training:	Epoch: [43][588/817]	Loss 0.0023 (0.0098)	
training:	Epoch: [43][589/817]	Loss 0.0018 (0.0098)	
training:	Epoch: [43][590/817]	Loss 0.0032 (0.0098)	
training:	Epoch: [43][591/817]	Loss 0.0025 (0.0098)	
training:	Epoch: [43][592/817]	Loss 0.0076 (0.0098)	
training:	Epoch: [43][593/817]	Loss 0.1013 (0.0099)	
training:	Epoch: [43][594/817]	Loss 0.0060 (0.0099)	
training:	Epoch: [43][595/817]	Loss 0.0015 (0.0099)	
training:	Epoch: [43][596/817]	Loss 0.0022 (0.0099)	
training:	Epoch: [43][597/817]	Loss 0.0018 (0.0099)	
training:	Epoch: [43][598/817]	Loss 0.0015 (0.0099)	
training:	Epoch: [43][599/817]	Loss 0.0021 (0.0099)	
training:	Epoch: [43][600/817]	Loss 0.0017 (0.0098)	
training:	Epoch: [43][601/817]	Loss 0.0274 (0.0099)	
training:	Epoch: [43][602/817]	Loss 0.0012 (0.0099)	
training:	Epoch: [43][603/817]	Loss 0.0016 (0.0098)	
training:	Epoch: [43][604/817]	Loss 0.0017 (0.0098)	
training:	Epoch: [43][605/817]	Loss 0.0012 (0.0098)	
training:	Epoch: [43][606/817]	Loss 0.0044 (0.0098)	
training:	Epoch: [43][607/817]	Loss 0.0060 (0.0098)	
training:	Epoch: [43][608/817]	Loss 0.0045 (0.0098)	
training:	Epoch: [43][609/817]	Loss 0.0016 (0.0098)	
training:	Epoch: [43][610/817]	Loss 0.0019 (0.0098)	
training:	Epoch: [43][611/817]	Loss 0.0021 (0.0098)	
training:	Epoch: [43][612/817]	Loss 0.0019 (0.0097)	
training:	Epoch: [43][613/817]	Loss 0.0015 (0.0097)	
training:	Epoch: [43][614/817]	Loss 0.0023 (0.0097)	
training:	Epoch: [43][615/817]	Loss 0.0017 (0.0097)	
training:	Epoch: [43][616/817]	Loss 0.0021 (0.0097)	
training:	Epoch: [43][617/817]	Loss 0.0014 (0.0097)	
training:	Epoch: [43][618/817]	Loss 0.0013 (0.0097)	
training:	Epoch: [43][619/817]	Loss 0.0017 (0.0096)	
training:	Epoch: [43][620/817]	Loss 0.0018 (0.0096)	
training:	Epoch: [43][621/817]	Loss 0.0017 (0.0096)	
training:	Epoch: [43][622/817]	Loss 0.0028 (0.0096)	
training:	Epoch: [43][623/817]	Loss 0.0014 (0.0096)	
training:	Epoch: [43][624/817]	Loss 0.0028 (0.0096)	
training:	Epoch: [43][625/817]	Loss 0.0013 (0.0096)	
training:	Epoch: [43][626/817]	Loss 0.0019 (0.0096)	
training:	Epoch: [43][627/817]	Loss 0.0027 (0.0096)	
training:	Epoch: [43][628/817]	Loss 0.0030 (0.0095)	
training:	Epoch: [43][629/817]	Loss 0.0018 (0.0095)	
training:	Epoch: [43][630/817]	Loss 0.0017 (0.0095)	
training:	Epoch: [43][631/817]	Loss 0.0033 (0.0095)	
training:	Epoch: [43][632/817]	Loss 0.0017 (0.0095)	
training:	Epoch: [43][633/817]	Loss 0.0071 (0.0095)	
training:	Epoch: [43][634/817]	Loss 0.0020 (0.0095)	
training:	Epoch: [43][635/817]	Loss 0.0074 (0.0095)	
training:	Epoch: [43][636/817]	Loss 0.0017 (0.0095)	
training:	Epoch: [43][637/817]	Loss 0.0209 (0.0095)	
training:	Epoch: [43][638/817]	Loss 0.0017 (0.0095)	
training:	Epoch: [43][639/817]	Loss 0.0016 (0.0095)	
training:	Epoch: [43][640/817]	Loss 0.0017 (0.0094)	
training:	Epoch: [43][641/817]	Loss 0.0021 (0.0094)	
training:	Epoch: [43][642/817]	Loss 0.0016 (0.0094)	
training:	Epoch: [43][643/817]	Loss 0.0013 (0.0094)	
training:	Epoch: [43][644/817]	Loss 0.0020 (0.0094)	
training:	Epoch: [43][645/817]	Loss 0.0015 (0.0094)	
training:	Epoch: [43][646/817]	Loss 0.0019 (0.0094)	
training:	Epoch: [43][647/817]	Loss 0.0024 (0.0094)	
training:	Epoch: [43][648/817]	Loss 0.0019 (0.0093)	
training:	Epoch: [43][649/817]	Loss 0.0018 (0.0093)	
training:	Epoch: [43][650/817]	Loss 0.0015 (0.0093)	
training:	Epoch: [43][651/817]	Loss 0.0016 (0.0093)	
training:	Epoch: [43][652/817]	Loss 0.0030 (0.0093)	
training:	Epoch: [43][653/817]	Loss 0.0127 (0.0093)	
training:	Epoch: [43][654/817]	Loss 0.0017 (0.0093)	
training:	Epoch: [43][655/817]	Loss 0.0014 (0.0093)	
training:	Epoch: [43][656/817]	Loss 0.1092 (0.0094)	
training:	Epoch: [43][657/817]	Loss 0.0029 (0.0094)	
training:	Epoch: [43][658/817]	Loss 0.0016 (0.0094)	
training:	Epoch: [43][659/817]	Loss 0.0024 (0.0094)	
training:	Epoch: [43][660/817]	Loss 0.0063 (0.0094)	
training:	Epoch: [43][661/817]	Loss 0.0014 (0.0094)	
training:	Epoch: [43][662/817]	Loss 0.0037 (0.0094)	
training:	Epoch: [43][663/817]	Loss 0.0046 (0.0094)	
training:	Epoch: [43][664/817]	Loss 0.0020 (0.0094)	
training:	Epoch: [43][665/817]	Loss 0.0014 (0.0094)	
training:	Epoch: [43][666/817]	Loss 0.0032 (0.0093)	
training:	Epoch: [43][667/817]	Loss 0.0088 (0.0093)	
training:	Epoch: [43][668/817]	Loss 0.0023 (0.0093)	
training:	Epoch: [43][669/817]	Loss 0.0063 (0.0093)	
training:	Epoch: [43][670/817]	Loss 0.0045 (0.0093)	
training:	Epoch: [43][671/817]	Loss 0.0017 (0.0093)	
training:	Epoch: [43][672/817]	Loss 0.0028 (0.0093)	
training:	Epoch: [43][673/817]	Loss 0.0343 (0.0093)	
training:	Epoch: [43][674/817]	Loss 0.0158 (0.0093)	
training:	Epoch: [43][675/817]	Loss 0.0021 (0.0093)	
training:	Epoch: [43][676/817]	Loss 0.0027 (0.0093)	
training:	Epoch: [43][677/817]	Loss 0.0267 (0.0093)	
training:	Epoch: [43][678/817]	Loss 0.0025 (0.0093)	
training:	Epoch: [43][679/817]	Loss 0.0115 (0.0093)	
training:	Epoch: [43][680/817]	Loss 0.0013 (0.0093)	
training:	Epoch: [43][681/817]	Loss 0.0020 (0.0093)	
training:	Epoch: [43][682/817]	Loss 0.0020 (0.0093)	
training:	Epoch: [43][683/817]	Loss 0.0022 (0.0093)	
training:	Epoch: [43][684/817]	Loss 0.0015 (0.0093)	
training:	Epoch: [43][685/817]	Loss 0.0020 (0.0093)	
training:	Epoch: [43][686/817]	Loss 0.0016 (0.0093)	
training:	Epoch: [43][687/817]	Loss 0.0017 (0.0093)	
training:	Epoch: [43][688/817]	Loss 0.0020 (0.0092)	
training:	Epoch: [43][689/817]	Loss 0.2071 (0.0095)	
training:	Epoch: [43][690/817]	Loss 0.0024 (0.0095)	
training:	Epoch: [43][691/817]	Loss 0.0023 (0.0095)	
training:	Epoch: [43][692/817]	Loss 0.0055 (0.0095)	
training:	Epoch: [43][693/817]	Loss 0.0050 (0.0095)	
training:	Epoch: [43][694/817]	Loss 0.0102 (0.0095)	
training:	Epoch: [43][695/817]	Loss 0.0015 (0.0095)	
training:	Epoch: [43][696/817]	Loss 0.0014 (0.0095)	
training:	Epoch: [43][697/817]	Loss 0.0023 (0.0095)	
training:	Epoch: [43][698/817]	Loss 0.0021 (0.0095)	
training:	Epoch: [43][699/817]	Loss 0.0032 (0.0094)	
training:	Epoch: [43][700/817]	Loss 0.0021 (0.0094)	
training:	Epoch: [43][701/817]	Loss 0.0015 (0.0094)	
training:	Epoch: [43][702/817]	Loss 0.0016 (0.0094)	
training:	Epoch: [43][703/817]	Loss 0.0024 (0.0094)	
training:	Epoch: [43][704/817]	Loss 0.0509 (0.0095)	
training:	Epoch: [43][705/817]	Loss 0.0027 (0.0095)	
training:	Epoch: [43][706/817]	Loss 0.0017 (0.0094)	
training:	Epoch: [43][707/817]	Loss 0.0014 (0.0094)	
training:	Epoch: [43][708/817]	Loss 0.0016 (0.0094)	
training:	Epoch: [43][709/817]	Loss 0.0183 (0.0094)	
training:	Epoch: [43][710/817]	Loss 0.0016 (0.0094)	
training:	Epoch: [43][711/817]	Loss 0.0016 (0.0094)	
training:	Epoch: [43][712/817]	Loss 0.0021 (0.0094)	
training:	Epoch: [43][713/817]	Loss 0.0026 (0.0094)	
training:	Epoch: [43][714/817]	Loss 0.0018 (0.0094)	
training:	Epoch: [43][715/817]	Loss 0.0020 (0.0094)	
training:	Epoch: [43][716/817]	Loss 0.0019 (0.0094)	
training:	Epoch: [43][717/817]	Loss 0.0015 (0.0093)	
training:	Epoch: [43][718/817]	Loss 0.0014 (0.0093)	
training:	Epoch: [43][719/817]	Loss 0.0035 (0.0093)	
training:	Epoch: [43][720/817]	Loss 0.0033 (0.0093)	
training:	Epoch: [43][721/817]	Loss 0.0025 (0.0093)	
training:	Epoch: [43][722/817]	Loss 0.0018 (0.0093)	
training:	Epoch: [43][723/817]	Loss 0.0159 (0.0093)	
training:	Epoch: [43][724/817]	Loss 0.0017 (0.0093)	
training:	Epoch: [43][725/817]	Loss 0.0019 (0.0093)	
training:	Epoch: [43][726/817]	Loss 0.0256 (0.0093)	
training:	Epoch: [43][727/817]	Loss 0.0014 (0.0093)	
training:	Epoch: [43][728/817]	Loss 0.0094 (0.0093)	
training:	Epoch: [43][729/817]	Loss 0.0035 (0.0093)	
training:	Epoch: [43][730/817]	Loss 0.0015 (0.0093)	
training:	Epoch: [43][731/817]	Loss 0.0041 (0.0093)	
training:	Epoch: [43][732/817]	Loss 0.0016 (0.0093)	
training:	Epoch: [43][733/817]	Loss 0.0016 (0.0093)	
training:	Epoch: [43][734/817]	Loss 0.0015 (0.0092)	
training:	Epoch: [43][735/817]	Loss 0.0059 (0.0092)	
training:	Epoch: [43][736/817]	Loss 0.0012 (0.0092)	
training:	Epoch: [43][737/817]	Loss 0.0013 (0.0092)	
training:	Epoch: [43][738/817]	Loss 0.0019 (0.0092)	
training:	Epoch: [43][739/817]	Loss 0.0021 (0.0092)	
training:	Epoch: [43][740/817]	Loss 0.0019 (0.0092)	
training:	Epoch: [43][741/817]	Loss 0.0011 (0.0092)	
training:	Epoch: [43][742/817]	Loss 0.0109 (0.0092)	
training:	Epoch: [43][743/817]	Loss 0.0015 (0.0092)	
training:	Epoch: [43][744/817]	Loss 0.0017 (0.0092)	
training:	Epoch: [43][745/817]	Loss 0.4156 (0.0097)	
training:	Epoch: [43][746/817]	Loss 0.0018 (0.0097)	
training:	Epoch: [43][747/817]	Loss 0.0021 (0.0097)	
training:	Epoch: [43][748/817]	Loss 0.0016 (0.0097)	
training:	Epoch: [43][749/817]	Loss 0.0027 (0.0097)	
training:	Epoch: [43][750/817]	Loss 0.0020 (0.0097)	
training:	Epoch: [43][751/817]	Loss 0.0028 (0.0096)	
training:	Epoch: [43][752/817]	Loss 0.0061 (0.0096)	
training:	Epoch: [43][753/817]	Loss 0.0018 (0.0096)	
training:	Epoch: [43][754/817]	Loss 0.0017 (0.0096)	
training:	Epoch: [43][755/817]	Loss 0.5635 (0.0104)	
training:	Epoch: [43][756/817]	Loss 0.0036 (0.0103)	
training:	Epoch: [43][757/817]	Loss 0.0126 (0.0103)	
training:	Epoch: [43][758/817]	Loss 0.0016 (0.0103)	
training:	Epoch: [43][759/817]	Loss 0.0038 (0.0103)	
training:	Epoch: [43][760/817]	Loss 0.0029 (0.0103)	
training:	Epoch: [43][761/817]	Loss 0.0580 (0.0104)	
training:	Epoch: [43][762/817]	Loss 0.0022 (0.0104)	
training:	Epoch: [43][763/817]	Loss 0.0774 (0.0105)	
training:	Epoch: [43][764/817]	Loss 0.0055 (0.0104)	
training:	Epoch: [43][765/817]	Loss 0.0212 (0.0105)	
training:	Epoch: [43][766/817]	Loss 0.0165 (0.0105)	
training:	Epoch: [43][767/817]	Loss 0.0014 (0.0105)	
training:	Epoch: [43][768/817]	Loss 0.0024 (0.0104)	
training:	Epoch: [43][769/817]	Loss 0.0014 (0.0104)	
training:	Epoch: [43][770/817]	Loss 0.0017 (0.0104)	
training:	Epoch: [43][771/817]	Loss 0.0518 (0.0105)	
training:	Epoch: [43][772/817]	Loss 0.0057 (0.0105)	
training:	Epoch: [43][773/817]	Loss 0.0021 (0.0105)	
training:	Epoch: [43][774/817]	Loss 0.0016 (0.0104)	
training:	Epoch: [43][775/817]	Loss 0.0033 (0.0104)	
training:	Epoch: [43][776/817]	Loss 0.0017 (0.0104)	
training:	Epoch: [43][777/817]	Loss 0.0037 (0.0104)	
training:	Epoch: [43][778/817]	Loss 0.0138 (0.0104)	
training:	Epoch: [43][779/817]	Loss 0.0103 (0.0104)	
training:	Epoch: [43][780/817]	Loss 0.0029 (0.0104)	
training:	Epoch: [43][781/817]	Loss 0.0027 (0.0104)	
training:	Epoch: [43][782/817]	Loss 0.0025 (0.0104)	
training:	Epoch: [43][783/817]	Loss 0.0016 (0.0104)	
training:	Epoch: [43][784/817]	Loss 0.0043 (0.0104)	
training:	Epoch: [43][785/817]	Loss 0.0168 (0.0104)	
training:	Epoch: [43][786/817]	Loss 0.0041 (0.0104)	
training:	Epoch: [43][787/817]	Loss 0.0065 (0.0104)	
training:	Epoch: [43][788/817]	Loss 0.0017 (0.0104)	
training:	Epoch: [43][789/817]	Loss 0.0017 (0.0103)	
training:	Epoch: [43][790/817]	Loss 0.0020 (0.0103)	
training:	Epoch: [43][791/817]	Loss 0.0467 (0.0104)	
training:	Epoch: [43][792/817]	Loss 0.1675 (0.0106)	
training:	Epoch: [43][793/817]	Loss 0.0018 (0.0106)	
training:	Epoch: [43][794/817]	Loss 0.0145 (0.0106)	
training:	Epoch: [43][795/817]	Loss 0.0019 (0.0106)	
training:	Epoch: [43][796/817]	Loss 0.0175 (0.0106)	
training:	Epoch: [43][797/817]	Loss 0.0153 (0.0106)	
training:	Epoch: [43][798/817]	Loss 0.0150 (0.0106)	
training:	Epoch: [43][799/817]	Loss 0.5819 (0.0113)	
training:	Epoch: [43][800/817]	Loss 0.0023 (0.0113)	
training:	Epoch: [43][801/817]	Loss 0.6996 (0.0121)	
training:	Epoch: [43][802/817]	Loss 0.0012 (0.0121)	
training:	Epoch: [43][803/817]	Loss 0.1802 (0.0123)	
training:	Epoch: [43][804/817]	Loss 0.0020 (0.0123)	
training:	Epoch: [43][805/817]	Loss 0.0025 (0.0123)	
training:	Epoch: [43][806/817]	Loss 0.0017 (0.0123)	
training:	Epoch: [43][807/817]	Loss 0.0016 (0.0123)	
training:	Epoch: [43][808/817]	Loss 0.0280 (0.0123)	
training:	Epoch: [43][809/817]	Loss 0.0063 (0.0123)	
training:	Epoch: [43][810/817]	Loss 0.0021 (0.0123)	
training:	Epoch: [43][811/817]	Loss 0.0016 (0.0123)	
training:	Epoch: [43][812/817]	Loss 0.0033 (0.0123)	
training:	Epoch: [43][813/817]	Loss 0.0026 (0.0123)	
training:	Epoch: [43][814/817]	Loss 0.0022 (0.0122)	
training:	Epoch: [43][815/817]	Loss 0.0019 (0.0122)	
training:	Epoch: [43][816/817]	Loss 0.0019 (0.0122)	
training:	Epoch: [43][817/817]	Loss 0.0025 (0.0122)	
Training:	 Loss: 0.0122

Training:	 ACC: 0.9983 0.9983 0.9991 0.9974
Validation:	 ACC: 0.7909 0.7919 0.8127 0.7691
Validation:	 Best_BACC: 0.8223 0.8218 0.8127 0.8318
Validation:	 Loss: 1.0895
Pretraining:	Epoch 44/200
----------
training:	Epoch: [44][1/817]	Loss 0.0032 (0.0032)	
training:	Epoch: [44][2/817]	Loss 0.0052 (0.0042)	
training:	Epoch: [44][3/817]	Loss 0.0568 (0.0217)	
training:	Epoch: [44][4/817]	Loss 0.0016 (0.0167)	
training:	Epoch: [44][5/817]	Loss 0.0042 (0.0142)	
training:	Epoch: [44][6/817]	Loss 0.0023 (0.0122)	
training:	Epoch: [44][7/817]	Loss 0.0027 (0.0109)	
training:	Epoch: [44][8/817]	Loss 0.0015 (0.0097)	
training:	Epoch: [44][9/817]	Loss 0.0034 (0.0090)	
training:	Epoch: [44][10/817]	Loss 0.0020 (0.0083)	
training:	Epoch: [44][11/817]	Loss 0.1308 (0.0194)	
training:	Epoch: [44][12/817]	Loss 0.0061 (0.0183)	
training:	Epoch: [44][13/817]	Loss 0.0023 (0.0171)	
training:	Epoch: [44][14/817]	Loss 0.0028 (0.0161)	
training:	Epoch: [44][15/817]	Loss 0.0017 (0.0151)	
training:	Epoch: [44][16/817]	Loss 0.0026 (0.0143)	
training:	Epoch: [44][17/817]	Loss 0.0110 (0.0141)	
training:	Epoch: [44][18/817]	Loss 0.0059 (0.0137)	
training:	Epoch: [44][19/817]	Loss 0.1644 (0.0216)	
training:	Epoch: [44][20/817]	Loss 0.0045 (0.0208)	
training:	Epoch: [44][21/817]	Loss 0.0022 (0.0199)	
training:	Epoch: [44][22/817]	Loss 0.0015 (0.0190)	
training:	Epoch: [44][23/817]	Loss 0.0017 (0.0183)	
training:	Epoch: [44][24/817]	Loss 0.0040 (0.0177)	
training:	Epoch: [44][25/817]	Loss 0.0027 (0.0171)	
training:	Epoch: [44][26/817]	Loss 0.0018 (0.0165)	
training:	Epoch: [44][27/817]	Loss 0.0026 (0.0160)	
training:	Epoch: [44][28/817]	Loss 0.0024 (0.0155)	
training:	Epoch: [44][29/817]	Loss 0.0055 (0.0152)	
training:	Epoch: [44][30/817]	Loss 0.0017 (0.0147)	
training:	Epoch: [44][31/817]	Loss 0.1008 (0.0175)	
training:	Epoch: [44][32/817]	Loss 0.0023 (0.0170)	
training:	Epoch: [44][33/817]	Loss 0.0012 (0.0165)	
training:	Epoch: [44][34/817]	Loss 0.0013 (0.0161)	
training:	Epoch: [44][35/817]	Loss 0.0016 (0.0157)	
training:	Epoch: [44][36/817]	Loss 0.0031 (0.0153)	
training:	Epoch: [44][37/817]	Loss 0.0017 (0.0150)	
training:	Epoch: [44][38/817]	Loss 0.0017 (0.0146)	
training:	Epoch: [44][39/817]	Loss 0.0493 (0.0155)	
training:	Epoch: [44][40/817]	Loss 0.0016 (0.0151)	
training:	Epoch: [44][41/817]	Loss 0.0072 (0.0150)	
training:	Epoch: [44][42/817]	Loss 0.0019 (0.0146)	
training:	Epoch: [44][43/817]	Loss 0.0027 (0.0144)	
training:	Epoch: [44][44/817]	Loss 0.0018 (0.0141)	
training:	Epoch: [44][45/817]	Loss 0.0275 (0.0144)	
training:	Epoch: [44][46/817]	Loss 0.0051 (0.0142)	
training:	Epoch: [44][47/817]	Loss 0.0014 (0.0139)	
training:	Epoch: [44][48/817]	Loss 0.0019 (0.0137)	
training:	Epoch: [44][49/817]	Loss 0.0309 (0.0140)	
training:	Epoch: [44][50/817]	Loss 0.0021 (0.0138)	
training:	Epoch: [44][51/817]	Loss 0.0020 (0.0135)	
training:	Epoch: [44][52/817]	Loss 0.0311 (0.0139)	
training:	Epoch: [44][53/817]	Loss 0.0130 (0.0139)	
training:	Epoch: [44][54/817]	Loss 0.0027 (0.0137)	
training:	Epoch: [44][55/817]	Loss 0.0015 (0.0134)	
training:	Epoch: [44][56/817]	Loss 0.0021 (0.0132)	
training:	Epoch: [44][57/817]	Loss 0.0058 (0.0131)	
training:	Epoch: [44][58/817]	Loss 0.0064 (0.0130)	
training:	Epoch: [44][59/817]	Loss 0.0015 (0.0128)	
training:	Epoch: [44][60/817]	Loss 0.0017 (0.0126)	
training:	Epoch: [44][61/817]	Loss 0.0020 (0.0124)	
training:	Epoch: [44][62/817]	Loss 0.0026 (0.0123)	
training:	Epoch: [44][63/817]	Loss 0.0014 (0.0121)	
training:	Epoch: [44][64/817]	Loss 0.0015 (0.0119)	
training:	Epoch: [44][65/817]	Loss 0.0020 (0.0118)	
training:	Epoch: [44][66/817]	Loss 0.0016 (0.0116)	
training:	Epoch: [44][67/817]	Loss 0.0015 (0.0115)	
training:	Epoch: [44][68/817]	Loss 0.0631 (0.0122)	
training:	Epoch: [44][69/817]	Loss 0.0014 (0.0121)	
training:	Epoch: [44][70/817]	Loss 0.0018 (0.0119)	
training:	Epoch: [44][71/817]	Loss 0.0047 (0.0118)	
training:	Epoch: [44][72/817]	Loss 0.0024 (0.0117)	
training:	Epoch: [44][73/817]	Loss 0.0012 (0.0116)	
training:	Epoch: [44][74/817]	Loss 0.0022 (0.0114)	
training:	Epoch: [44][75/817]	Loss 0.0024 (0.0113)	
training:	Epoch: [44][76/817]	Loss 0.0017 (0.0112)	
training:	Epoch: [44][77/817]	Loss 0.0019 (0.0111)	
training:	Epoch: [44][78/817]	Loss 0.0015 (0.0109)	
training:	Epoch: [44][79/817]	Loss 0.0084 (0.0109)	
training:	Epoch: [44][80/817]	Loss 0.0321 (0.0112)	
training:	Epoch: [44][81/817]	Loss 0.0018 (0.0111)	
training:	Epoch: [44][82/817]	Loss 0.0023 (0.0109)	
training:	Epoch: [44][83/817]	Loss 0.0329 (0.0112)	
training:	Epoch: [44][84/817]	Loss 0.0027 (0.0111)	
training:	Epoch: [44][85/817]	Loss 0.0014 (0.0110)	
training:	Epoch: [44][86/817]	Loss 0.0053 (0.0109)	
training:	Epoch: [44][87/817]	Loss 0.0527 (0.0114)	
training:	Epoch: [44][88/817]	Loss 0.0017 (0.0113)	
training:	Epoch: [44][89/817]	Loss 0.0028 (0.0112)	
training:	Epoch: [44][90/817]	Loss 0.0015 (0.0111)	
training:	Epoch: [44][91/817]	Loss 0.0013 (0.0110)	
training:	Epoch: [44][92/817]	Loss 0.0015 (0.0109)	
training:	Epoch: [44][93/817]	Loss 0.0042 (0.0108)	
training:	Epoch: [44][94/817]	Loss 0.0018 (0.0107)	
training:	Epoch: [44][95/817]	Loss 0.0055 (0.0107)	
training:	Epoch: [44][96/817]	Loss 0.0018 (0.0106)	
training:	Epoch: [44][97/817]	Loss 0.0014 (0.0105)	
training:	Epoch: [44][98/817]	Loss 0.0020 (0.0104)	
training:	Epoch: [44][99/817]	Loss 0.0016 (0.0103)	
training:	Epoch: [44][100/817]	Loss 0.0032 (0.0102)	
training:	Epoch: [44][101/817]	Loss 0.0021 (0.0101)	
training:	Epoch: [44][102/817]	Loss 0.0021 (0.0101)	
training:	Epoch: [44][103/817]	Loss 0.0029 (0.0100)	
training:	Epoch: [44][104/817]	Loss 0.0212 (0.0101)	
training:	Epoch: [44][105/817]	Loss 0.0013 (0.0100)	
training:	Epoch: [44][106/817]	Loss 0.0013 (0.0099)	
training:	Epoch: [44][107/817]	Loss 0.0014 (0.0099)	
training:	Epoch: [44][108/817]	Loss 0.0140 (0.0099)	
training:	Epoch: [44][109/817]	Loss 0.0026 (0.0098)	
training:	Epoch: [44][110/817]	Loss 0.0015 (0.0098)	
training:	Epoch: [44][111/817]	Loss 0.0014 (0.0097)	
training:	Epoch: [44][112/817]	Loss 0.0012 (0.0096)	
training:	Epoch: [44][113/817]	Loss 0.0016 (0.0095)	
training:	Epoch: [44][114/817]	Loss 0.0013 (0.0095)	
training:	Epoch: [44][115/817]	Loss 0.0013 (0.0094)	
training:	Epoch: [44][116/817]	Loss 0.0014 (0.0093)	
training:	Epoch: [44][117/817]	Loss 0.0081 (0.0093)	
training:	Epoch: [44][118/817]	Loss 0.0018 (0.0092)	
training:	Epoch: [44][119/817]	Loss 0.0014 (0.0092)	
training:	Epoch: [44][120/817]	Loss 0.0019 (0.0091)	
training:	Epoch: [44][121/817]	Loss 0.0031 (0.0091)	
training:	Epoch: [44][122/817]	Loss 0.0029 (0.0090)	
training:	Epoch: [44][123/817]	Loss 0.0011 (0.0090)	
training:	Epoch: [44][124/817]	Loss 0.0017 (0.0089)	
training:	Epoch: [44][125/817]	Loss 0.0796 (0.0095)	
training:	Epoch: [44][126/817]	Loss 0.0015 (0.0094)	
training:	Epoch: [44][127/817]	Loss 0.0012 (0.0093)	
training:	Epoch: [44][128/817]	Loss 0.0021 (0.0093)	
training:	Epoch: [44][129/817]	Loss 0.0015 (0.0092)	
training:	Epoch: [44][130/817]	Loss 0.0015 (0.0092)	
training:	Epoch: [44][131/817]	Loss 0.0031 (0.0091)	
training:	Epoch: [44][132/817]	Loss 0.0017 (0.0091)	
training:	Epoch: [44][133/817]	Loss 0.0164 (0.0091)	
training:	Epoch: [44][134/817]	Loss 0.1550 (0.0102)	
training:	Epoch: [44][135/817]	Loss 0.0015 (0.0101)	
training:	Epoch: [44][136/817]	Loss 0.0018 (0.0101)	
training:	Epoch: [44][137/817]	Loss 0.0025 (0.0100)	
training:	Epoch: [44][138/817]	Loss 0.0016 (0.0100)	
training:	Epoch: [44][139/817]	Loss 0.0014 (0.0099)	
training:	Epoch: [44][140/817]	Loss 0.0013 (0.0098)	
training:	Epoch: [44][141/817]	Loss 0.0046 (0.0098)	
training:	Epoch: [44][142/817]	Loss 0.0022 (0.0097)	
training:	Epoch: [44][143/817]	Loss 0.0017 (0.0097)	
training:	Epoch: [44][144/817]	Loss 0.0042 (0.0097)	
training:	Epoch: [44][145/817]	Loss 0.0018 (0.0096)	
training:	Epoch: [44][146/817]	Loss 0.0022 (0.0095)	
training:	Epoch: [44][147/817]	Loss 0.0103 (0.0096)	
training:	Epoch: [44][148/817]	Loss 0.0155 (0.0096)	
training:	Epoch: [44][149/817]	Loss 0.0128 (0.0096)	
training:	Epoch: [44][150/817]	Loss 0.0021 (0.0096)	
training:	Epoch: [44][151/817]	Loss 0.0087 (0.0096)	
training:	Epoch: [44][152/817]	Loss 0.0024 (0.0095)	
training:	Epoch: [44][153/817]	Loss 0.0959 (0.0101)	
training:	Epoch: [44][154/817]	Loss 0.0086 (0.0101)	
training:	Epoch: [44][155/817]	Loss 0.0011 (0.0100)	
training:	Epoch: [44][156/817]	Loss 0.0025 (0.0100)	
training:	Epoch: [44][157/817]	Loss 0.0030 (0.0099)	
training:	Epoch: [44][158/817]	Loss 0.0014 (0.0099)	
training:	Epoch: [44][159/817]	Loss 0.0016 (0.0098)	
training:	Epoch: [44][160/817]	Loss 0.0014 (0.0098)	
training:	Epoch: [44][161/817]	Loss 0.0032 (0.0097)	
training:	Epoch: [44][162/817]	Loss 0.0077 (0.0097)	
training:	Epoch: [44][163/817]	Loss 0.0133 (0.0097)	
training:	Epoch: [44][164/817]	Loss 0.0032 (0.0097)	
training:	Epoch: [44][165/817]	Loss 0.0016 (0.0096)	
training:	Epoch: [44][166/817]	Loss 0.0021 (0.0096)	
training:	Epoch: [44][167/817]	Loss 0.0071 (0.0096)	
training:	Epoch: [44][168/817]	Loss 0.0021 (0.0095)	
training:	Epoch: [44][169/817]	Loss 0.0020 (0.0095)	
training:	Epoch: [44][170/817]	Loss 0.0018 (0.0094)	
training:	Epoch: [44][171/817]	Loss 0.1381 (0.0102)	
training:	Epoch: [44][172/817]	Loss 0.0034 (0.0102)	
training:	Epoch: [44][173/817]	Loss 0.0023 (0.0101)	
training:	Epoch: [44][174/817]	Loss 0.0014 (0.0101)	
training:	Epoch: [44][175/817]	Loss 0.0016 (0.0100)	
training:	Epoch: [44][176/817]	Loss 0.0058 (0.0100)	
training:	Epoch: [44][177/817]	Loss 0.0016 (0.0099)	
training:	Epoch: [44][178/817]	Loss 0.0014 (0.0099)	
training:	Epoch: [44][179/817]	Loss 0.0080 (0.0099)	
training:	Epoch: [44][180/817]	Loss 0.0146 (0.0099)	
training:	Epoch: [44][181/817]	Loss 0.0029 (0.0099)	
training:	Epoch: [44][182/817]	Loss 0.0020 (0.0098)	
training:	Epoch: [44][183/817]	Loss 0.0016 (0.0098)	
training:	Epoch: [44][184/817]	Loss 0.0033 (0.0097)	
training:	Epoch: [44][185/817]	Loss 0.0039 (0.0097)	
training:	Epoch: [44][186/817]	Loss 0.0010 (0.0097)	
training:	Epoch: [44][187/817]	Loss 0.0032 (0.0096)	
training:	Epoch: [44][188/817]	Loss 0.0299 (0.0097)	
training:	Epoch: [44][189/817]	Loss 0.0064 (0.0097)	
training:	Epoch: [44][190/817]	Loss 0.0028 (0.0097)	
training:	Epoch: [44][191/817]	Loss 0.0014 (0.0096)	
training:	Epoch: [44][192/817]	Loss 0.0174 (0.0097)	
training:	Epoch: [44][193/817]	Loss 0.0014 (0.0096)	
training:	Epoch: [44][194/817]	Loss 0.0018 (0.0096)	
training:	Epoch: [44][195/817]	Loss 0.0099 (0.0096)	
training:	Epoch: [44][196/817]	Loss 0.0015 (0.0096)	
training:	Epoch: [44][197/817]	Loss 0.0037 (0.0095)	
training:	Epoch: [44][198/817]	Loss 0.0984 (0.0100)	
training:	Epoch: [44][199/817]	Loss 0.0021 (0.0099)	
training:	Epoch: [44][200/817]	Loss 0.0035 (0.0099)	
training:	Epoch: [44][201/817]	Loss 0.3508 (0.0116)	
training:	Epoch: [44][202/817]	Loss 0.0012 (0.0116)	
training:	Epoch: [44][203/817]	Loss 0.0037 (0.0115)	
training:	Epoch: [44][204/817]	Loss 0.3107 (0.0130)	
training:	Epoch: [44][205/817]	Loss 0.0013 (0.0129)	
training:	Epoch: [44][206/817]	Loss 0.0015 (0.0129)	
training:	Epoch: [44][207/817]	Loss 0.0023 (0.0128)	
training:	Epoch: [44][208/817]	Loss 0.0159 (0.0128)	
training:	Epoch: [44][209/817]	Loss 0.0115 (0.0128)	
training:	Epoch: [44][210/817]	Loss 0.0015 (0.0128)	
training:	Epoch: [44][211/817]	Loss 0.0014 (0.0127)	
training:	Epoch: [44][212/817]	Loss 0.0023 (0.0127)	
training:	Epoch: [44][213/817]	Loss 0.0035 (0.0126)	
training:	Epoch: [44][214/817]	Loss 0.0745 (0.0129)	
training:	Epoch: [44][215/817]	Loss 0.0019 (0.0129)	
training:	Epoch: [44][216/817]	Loss 0.0023 (0.0128)	
training:	Epoch: [44][217/817]	Loss 0.0018 (0.0128)	
training:	Epoch: [44][218/817]	Loss 0.0011 (0.0127)	
training:	Epoch: [44][219/817]	Loss 0.0035 (0.0127)	
training:	Epoch: [44][220/817]	Loss 0.1166 (0.0131)	
training:	Epoch: [44][221/817]	Loss 0.0024 (0.0131)	
training:	Epoch: [44][222/817]	Loss 0.0171 (0.0131)	
training:	Epoch: [44][223/817]	Loss 0.0166 (0.0131)	
training:	Epoch: [44][224/817]	Loss 0.0020 (0.0131)	
training:	Epoch: [44][225/817]	Loss 0.0064 (0.0130)	
training:	Epoch: [44][226/817]	Loss 0.0476 (0.0132)	
training:	Epoch: [44][227/817]	Loss 0.0017 (0.0131)	
training:	Epoch: [44][228/817]	Loss 0.0025 (0.0131)	
training:	Epoch: [44][229/817]	Loss 0.0151 (0.0131)	
training:	Epoch: [44][230/817]	Loss 0.0014 (0.0131)	
training:	Epoch: [44][231/817]	Loss 0.0017 (0.0130)	
training:	Epoch: [44][232/817]	Loss 0.0014 (0.0130)	
training:	Epoch: [44][233/817]	Loss 0.0129 (0.0130)	
training:	Epoch: [44][234/817]	Loss 0.0028 (0.0129)	
training:	Epoch: [44][235/817]	Loss 0.0038 (0.0129)	
training:	Epoch: [44][236/817]	Loss 0.0014 (0.0128)	
training:	Epoch: [44][237/817]	Loss 0.0023 (0.0128)	
training:	Epoch: [44][238/817]	Loss 0.0019 (0.0127)	
training:	Epoch: [44][239/817]	Loss 0.0787 (0.0130)	
training:	Epoch: [44][240/817]	Loss 0.0016 (0.0130)	
training:	Epoch: [44][241/817]	Loss 0.0038 (0.0129)	
training:	Epoch: [44][242/817]	Loss 0.0027 (0.0129)	
training:	Epoch: [44][243/817]	Loss 0.0038 (0.0128)	
training:	Epoch: [44][244/817]	Loss 0.0017 (0.0128)	
training:	Epoch: [44][245/817]	Loss 0.0028 (0.0128)	
training:	Epoch: [44][246/817]	Loss 0.0141 (0.0128)	
training:	Epoch: [44][247/817]	Loss 0.0014 (0.0127)	
training:	Epoch: [44][248/817]	Loss 0.0015 (0.0127)	
training:	Epoch: [44][249/817]	Loss 0.0593 (0.0129)	
training:	Epoch: [44][250/817]	Loss 0.0095 (0.0129)	
training:	Epoch: [44][251/817]	Loss 0.0033 (0.0128)	
training:	Epoch: [44][252/817]	Loss 0.0011 (0.0128)	
training:	Epoch: [44][253/817]	Loss 0.0385 (0.0129)	
training:	Epoch: [44][254/817]	Loss 0.0023 (0.0128)	
training:	Epoch: [44][255/817]	Loss 0.0015 (0.0128)	
training:	Epoch: [44][256/817]	Loss 0.0083 (0.0128)	
training:	Epoch: [44][257/817]	Loss 0.0014 (0.0127)	
training:	Epoch: [44][258/817]	Loss 0.0013 (0.0127)	
training:	Epoch: [44][259/817]	Loss 0.0036 (0.0126)	
training:	Epoch: [44][260/817]	Loss 0.0012 (0.0126)	
training:	Epoch: [44][261/817]	Loss 0.0013 (0.0126)	
training:	Epoch: [44][262/817]	Loss 0.0014 (0.0125)	
training:	Epoch: [44][263/817]	Loss 0.0015 (0.0125)	
training:	Epoch: [44][264/817]	Loss 0.0022 (0.0124)	
training:	Epoch: [44][265/817]	Loss 0.0037 (0.0124)	
training:	Epoch: [44][266/817]	Loss 0.0018 (0.0124)	
training:	Epoch: [44][267/817]	Loss 0.0016 (0.0123)	
training:	Epoch: [44][268/817]	Loss 0.0014 (0.0123)	
training:	Epoch: [44][269/817]	Loss 0.0022 (0.0122)	
training:	Epoch: [44][270/817]	Loss 0.0017 (0.0122)	
training:	Epoch: [44][271/817]	Loss 0.0013 (0.0122)	
training:	Epoch: [44][272/817]	Loss 0.0061 (0.0121)	
training:	Epoch: [44][273/817]	Loss 0.0169 (0.0122)	
training:	Epoch: [44][274/817]	Loss 0.0039 (0.0121)	
training:	Epoch: [44][275/817]	Loss 0.0028 (0.0121)	
training:	Epoch: [44][276/817]	Loss 0.0017 (0.0121)	
training:	Epoch: [44][277/817]	Loss 0.0012 (0.0120)	
training:	Epoch: [44][278/817]	Loss 0.0014 (0.0120)	
training:	Epoch: [44][279/817]	Loss 0.0016 (0.0119)	
training:	Epoch: [44][280/817]	Loss 0.0010 (0.0119)	
training:	Epoch: [44][281/817]	Loss 0.0016 (0.0119)	
training:	Epoch: [44][282/817]	Loss 0.0013 (0.0118)	
training:	Epoch: [44][283/817]	Loss 0.0162 (0.0118)	
training:	Epoch: [44][284/817]	Loss 0.0040 (0.0118)	
training:	Epoch: [44][285/817]	Loss 0.0014 (0.0118)	
training:	Epoch: [44][286/817]	Loss 0.0014 (0.0117)	
training:	Epoch: [44][287/817]	Loss 0.0019 (0.0117)	
training:	Epoch: [44][288/817]	Loss 0.0014 (0.0117)	
training:	Epoch: [44][289/817]	Loss 0.0029 (0.0116)	
training:	Epoch: [44][290/817]	Loss 0.0015 (0.0116)	
training:	Epoch: [44][291/817]	Loss 0.5499 (0.0135)	
training:	Epoch: [44][292/817]	Loss 0.0021 (0.0134)	
training:	Epoch: [44][293/817]	Loss 0.0024 (0.0134)	
training:	Epoch: [44][294/817]	Loss 0.0014 (0.0133)	
training:	Epoch: [44][295/817]	Loss 0.0024 (0.0133)	
training:	Epoch: [44][296/817]	Loss 0.1473 (0.0138)	
training:	Epoch: [44][297/817]	Loss 0.0016 (0.0137)	
training:	Epoch: [44][298/817]	Loss 0.0013 (0.0137)	
training:	Epoch: [44][299/817]	Loss 0.0472 (0.0138)	
training:	Epoch: [44][300/817]	Loss 0.0043 (0.0138)	
training:	Epoch: [44][301/817]	Loss 0.0098 (0.0137)	
training:	Epoch: [44][302/817]	Loss 0.0016 (0.0137)	
training:	Epoch: [44][303/817]	Loss 0.0012 (0.0137)	
training:	Epoch: [44][304/817]	Loss 0.0013 (0.0136)	
training:	Epoch: [44][305/817]	Loss 0.0013 (0.0136)	
training:	Epoch: [44][306/817]	Loss 0.0013 (0.0135)	
training:	Epoch: [44][307/817]	Loss 0.0019 (0.0135)	
training:	Epoch: [44][308/817]	Loss 0.0460 (0.0136)	
training:	Epoch: [44][309/817]	Loss 0.0014 (0.0136)	
training:	Epoch: [44][310/817]	Loss 0.0038 (0.0135)	
training:	Epoch: [44][311/817]	Loss 0.0018 (0.0135)	
training:	Epoch: [44][312/817]	Loss 0.0021 (0.0135)	
training:	Epoch: [44][313/817]	Loss 0.0019 (0.0134)	
training:	Epoch: [44][314/817]	Loss 0.0012 (0.0134)	
training:	Epoch: [44][315/817]	Loss 0.0464 (0.0135)	
training:	Epoch: [44][316/817]	Loss 0.0014 (0.0134)	
training:	Epoch: [44][317/817]	Loss 0.0158 (0.0135)	
training:	Epoch: [44][318/817]	Loss 0.0013 (0.0134)	
training:	Epoch: [44][319/817]	Loss 0.0015 (0.0134)	
training:	Epoch: [44][320/817]	Loss 0.0021 (0.0133)	
training:	Epoch: [44][321/817]	Loss 0.0013 (0.0133)	
training:	Epoch: [44][322/817]	Loss 0.0016 (0.0133)	
training:	Epoch: [44][323/817]	Loss 0.0022 (0.0132)	
training:	Epoch: [44][324/817]	Loss 0.0034 (0.0132)	
training:	Epoch: [44][325/817]	Loss 0.0013 (0.0132)	
training:	Epoch: [44][326/817]	Loss 0.6737 (0.0152)	
training:	Epoch: [44][327/817]	Loss 0.0024 (0.0152)	
training:	Epoch: [44][328/817]	Loss 0.0023 (0.0151)	
training:	Epoch: [44][329/817]	Loss 0.0015 (0.0151)	
training:	Epoch: [44][330/817]	Loss 0.0012 (0.0150)	
training:	Epoch: [44][331/817]	Loss 0.0055 (0.0150)	
training:	Epoch: [44][332/817]	Loss 0.0023 (0.0150)	
training:	Epoch: [44][333/817]	Loss 0.0432 (0.0151)	
training:	Epoch: [44][334/817]	Loss 0.0012 (0.0150)	
training:	Epoch: [44][335/817]	Loss 0.0043 (0.0150)	
training:	Epoch: [44][336/817]	Loss 0.0013 (0.0149)	
training:	Epoch: [44][337/817]	Loss 0.0026 (0.0149)	
training:	Epoch: [44][338/817]	Loss 0.0046 (0.0149)	
training:	Epoch: [44][339/817]	Loss 0.0019 (0.0148)	
training:	Epoch: [44][340/817]	Loss 0.0048 (0.0148)	
training:	Epoch: [44][341/817]	Loss 0.0028 (0.0148)	
training:	Epoch: [44][342/817]	Loss 0.0016 (0.0147)	
training:	Epoch: [44][343/817]	Loss 0.0019 (0.0147)	
training:	Epoch: [44][344/817]	Loss 0.0013 (0.0147)	
training:	Epoch: [44][345/817]	Loss 0.0027 (0.0146)	
training:	Epoch: [44][346/817]	Loss 0.0021 (0.0146)	
training:	Epoch: [44][347/817]	Loss 0.0017 (0.0145)	
training:	Epoch: [44][348/817]	Loss 0.0017 (0.0145)	
training:	Epoch: [44][349/817]	Loss 0.0172 (0.0145)	
training:	Epoch: [44][350/817]	Loss 0.0072 (0.0145)	
training:	Epoch: [44][351/817]	Loss 0.0021 (0.0145)	
training:	Epoch: [44][352/817]	Loss 0.0013 (0.0144)	
training:	Epoch: [44][353/817]	Loss 0.0688 (0.0146)	
training:	Epoch: [44][354/817]	Loss 0.0158 (0.0146)	
training:	Epoch: [44][355/817]	Loss 0.0014 (0.0145)	
training:	Epoch: [44][356/817]	Loss 0.0014 (0.0145)	
training:	Epoch: [44][357/817]	Loss 0.0013 (0.0145)	
training:	Epoch: [44][358/817]	Loss 0.0013 (0.0144)	
training:	Epoch: [44][359/817]	Loss 0.0022 (0.0144)	
training:	Epoch: [44][360/817]	Loss 0.0072 (0.0144)	
training:	Epoch: [44][361/817]	Loss 0.0058 (0.0144)	
training:	Epoch: [44][362/817]	Loss 0.0028 (0.0143)	
training:	Epoch: [44][363/817]	Loss 0.0017 (0.0143)	
training:	Epoch: [44][364/817]	Loss 0.0015 (0.0143)	
training:	Epoch: [44][365/817]	Loss 0.0079 (0.0142)	
training:	Epoch: [44][366/817]	Loss 0.0034 (0.0142)	
training:	Epoch: [44][367/817]	Loss 0.0019 (0.0142)	
training:	Epoch: [44][368/817]	Loss 0.0026 (0.0141)	
training:	Epoch: [44][369/817]	Loss 0.0032 (0.0141)	
training:	Epoch: [44][370/817]	Loss 0.0061 (0.0141)	
training:	Epoch: [44][371/817]	Loss 0.0012 (0.0141)	
training:	Epoch: [44][372/817]	Loss 0.0019 (0.0140)	
training:	Epoch: [44][373/817]	Loss 0.0032 (0.0140)	
training:	Epoch: [44][374/817]	Loss 0.0021 (0.0140)	
training:	Epoch: [44][375/817]	Loss 0.0460 (0.0140)	
training:	Epoch: [44][376/817]	Loss 0.0021 (0.0140)	
training:	Epoch: [44][377/817]	Loss 0.0017 (0.0140)	
training:	Epoch: [44][378/817]	Loss 0.0018 (0.0139)	
training:	Epoch: [44][379/817]	Loss 0.0018 (0.0139)	
training:	Epoch: [44][380/817]	Loss 0.0034 (0.0139)	
training:	Epoch: [44][381/817]	Loss 0.0013 (0.0139)	
training:	Epoch: [44][382/817]	Loss 0.0042 (0.0138)	
training:	Epoch: [44][383/817]	Loss 0.0015 (0.0138)	
training:	Epoch: [44][384/817]	Loss 0.0042 (0.0138)	
training:	Epoch: [44][385/817]	Loss 0.0041 (0.0137)	
training:	Epoch: [44][386/817]	Loss 0.0038 (0.0137)	
training:	Epoch: [44][387/817]	Loss 0.0013 (0.0137)	
training:	Epoch: [44][388/817]	Loss 0.0021 (0.0137)	
training:	Epoch: [44][389/817]	Loss 0.0017 (0.0136)	
training:	Epoch: [44][390/817]	Loss 0.0032 (0.0136)	
training:	Epoch: [44][391/817]	Loss 0.0014 (0.0136)	
training:	Epoch: [44][392/817]	Loss 0.0049 (0.0136)	
training:	Epoch: [44][393/817]	Loss 0.0018 (0.0135)	
training:	Epoch: [44][394/817]	Loss 0.0136 (0.0135)	
training:	Epoch: [44][395/817]	Loss 0.0023 (0.0135)	
training:	Epoch: [44][396/817]	Loss 0.0054 (0.0135)	
training:	Epoch: [44][397/817]	Loss 0.0025 (0.0134)	
training:	Epoch: [44][398/817]	Loss 0.0022 (0.0134)	
training:	Epoch: [44][399/817]	Loss 0.0037 (0.0134)	
training:	Epoch: [44][400/817]	Loss 0.0016 (0.0134)	
training:	Epoch: [44][401/817]	Loss 0.0013 (0.0133)	
training:	Epoch: [44][402/817]	Loss 0.0017 (0.0133)	
training:	Epoch: [44][403/817]	Loss 0.0013 (0.0133)	
training:	Epoch: [44][404/817]	Loss 0.0015 (0.0132)	
training:	Epoch: [44][405/817]	Loss 0.0014 (0.0132)	
training:	Epoch: [44][406/817]	Loss 0.0017 (0.0132)	
training:	Epoch: [44][407/817]	Loss 0.0022 (0.0132)	
training:	Epoch: [44][408/817]	Loss 0.0013 (0.0131)	
training:	Epoch: [44][409/817]	Loss 0.0053 (0.0131)	
training:	Epoch: [44][410/817]	Loss 0.0049 (0.0131)	
training:	Epoch: [44][411/817]	Loss 0.0012 (0.0131)	
training:	Epoch: [44][412/817]	Loss 0.0088 (0.0131)	
training:	Epoch: [44][413/817]	Loss 0.0023 (0.0130)	
training:	Epoch: [44][414/817]	Loss 0.0013 (0.0130)	
training:	Epoch: [44][415/817]	Loss 0.0024 (0.0130)	
training:	Epoch: [44][416/817]	Loss 0.0013 (0.0129)	
training:	Epoch: [44][417/817]	Loss 0.0871 (0.0131)	
training:	Epoch: [44][418/817]	Loss 0.0016 (0.0131)	
training:	Epoch: [44][419/817]	Loss 0.0016 (0.0131)	
training:	Epoch: [44][420/817]	Loss 0.0038 (0.0130)	
training:	Epoch: [44][421/817]	Loss 0.0029 (0.0130)	
training:	Epoch: [44][422/817]	Loss 0.0036 (0.0130)	
training:	Epoch: [44][423/817]	Loss 0.0019 (0.0130)	
training:	Epoch: [44][424/817]	Loss 0.0023 (0.0129)	
training:	Epoch: [44][425/817]	Loss 0.0012 (0.0129)	
training:	Epoch: [44][426/817]	Loss 0.0019 (0.0129)	
training:	Epoch: [44][427/817]	Loss 0.0015 (0.0129)	
training:	Epoch: [44][428/817]	Loss 0.0014 (0.0128)	
training:	Epoch: [44][429/817]	Loss 0.0014 (0.0128)	
training:	Epoch: [44][430/817]	Loss 0.0012 (0.0128)	
training:	Epoch: [44][431/817]	Loss 0.0029 (0.0128)	
training:	Epoch: [44][432/817]	Loss 0.0108 (0.0128)	
training:	Epoch: [44][433/817]	Loss 0.0019 (0.0127)	
training:	Epoch: [44][434/817]	Loss 0.0012 (0.0127)	
training:	Epoch: [44][435/817]	Loss 0.0017 (0.0127)	
training:	Epoch: [44][436/817]	Loss 0.0012 (0.0127)	
training:	Epoch: [44][437/817]	Loss 0.0016 (0.0126)	
training:	Epoch: [44][438/817]	Loss 0.0017 (0.0126)	
training:	Epoch: [44][439/817]	Loss 0.0014 (0.0126)	
training:	Epoch: [44][440/817]	Loss 0.0012 (0.0126)	
training:	Epoch: [44][441/817]	Loss 0.0148 (0.0126)	
training:	Epoch: [44][442/817]	Loss 0.0021 (0.0125)	
training:	Epoch: [44][443/817]	Loss 0.0011 (0.0125)	
training:	Epoch: [44][444/817]	Loss 0.0016 (0.0125)	
training:	Epoch: [44][445/817]	Loss 0.0021 (0.0125)	
training:	Epoch: [44][446/817]	Loss 0.0016 (0.0124)	
training:	Epoch: [44][447/817]	Loss 0.0123 (0.0124)	
training:	Epoch: [44][448/817]	Loss 0.0022 (0.0124)	
training:	Epoch: [44][449/817]	Loss 0.0012 (0.0124)	
training:	Epoch: [44][450/817]	Loss 0.0105 (0.0124)	
training:	Epoch: [44][451/817]	Loss 0.0147 (0.0124)	
training:	Epoch: [44][452/817]	Loss 0.0011 (0.0124)	
training:	Epoch: [44][453/817]	Loss 0.0013 (0.0123)	
training:	Epoch: [44][454/817]	Loss 0.0014 (0.0123)	
training:	Epoch: [44][455/817]	Loss 0.0042 (0.0123)	
training:	Epoch: [44][456/817]	Loss 0.0016 (0.0123)	
training:	Epoch: [44][457/817]	Loss 0.0013 (0.0123)	
training:	Epoch: [44][458/817]	Loss 0.0015 (0.0122)	
training:	Epoch: [44][459/817]	Loss 0.0019 (0.0122)	
training:	Epoch: [44][460/817]	Loss 0.0018 (0.0122)	
training:	Epoch: [44][461/817]	Loss 0.0128 (0.0122)	
training:	Epoch: [44][462/817]	Loss 0.0015 (0.0122)	
training:	Epoch: [44][463/817]	Loss 0.0018 (0.0121)	
training:	Epoch: [44][464/817]	Loss 0.0016 (0.0121)	
training:	Epoch: [44][465/817]	Loss 0.0020 (0.0121)	
training:	Epoch: [44][466/817]	Loss 0.0015 (0.0121)	
training:	Epoch: [44][467/817]	Loss 0.0016 (0.0120)	
training:	Epoch: [44][468/817]	Loss 0.0013 (0.0120)	
training:	Epoch: [44][469/817]	Loss 0.0011 (0.0120)	
training:	Epoch: [44][470/817]	Loss 0.0017 (0.0120)	
training:	Epoch: [44][471/817]	Loss 0.0013 (0.0120)	
training:	Epoch: [44][472/817]	Loss 0.0013 (0.0119)	
training:	Epoch: [44][473/817]	Loss 0.0018 (0.0119)	
training:	Epoch: [44][474/817]	Loss 0.0012 (0.0119)	
training:	Epoch: [44][475/817]	Loss 0.0016 (0.0119)	
training:	Epoch: [44][476/817]	Loss 0.0016 (0.0118)	
training:	Epoch: [44][477/817]	Loss 0.0017 (0.0118)	
training:	Epoch: [44][478/817]	Loss 0.0019 (0.0118)	
training:	Epoch: [44][479/817]	Loss 0.0012 (0.0118)	
training:	Epoch: [44][480/817]	Loss 0.0018 (0.0118)	
training:	Epoch: [44][481/817]	Loss 0.0012 (0.0117)	
training:	Epoch: [44][482/817]	Loss 0.0042 (0.0117)	
training:	Epoch: [44][483/817]	Loss 0.0397 (0.0118)	
training:	Epoch: [44][484/817]	Loss 0.0029 (0.0118)	
training:	Epoch: [44][485/817]	Loss 0.0015 (0.0117)	
training:	Epoch: [44][486/817]	Loss 0.0023 (0.0117)	
training:	Epoch: [44][487/817]	Loss 0.0028 (0.0117)	
training:	Epoch: [44][488/817]	Loss 0.0041 (0.0117)	
training:	Epoch: [44][489/817]	Loss 0.0010 (0.0117)	
training:	Epoch: [44][490/817]	Loss 0.0017 (0.0116)	
training:	Epoch: [44][491/817]	Loss 0.0016 (0.0116)	
training:	Epoch: [44][492/817]	Loss 0.0792 (0.0118)	
training:	Epoch: [44][493/817]	Loss 0.0023 (0.0117)	
training:	Epoch: [44][494/817]	Loss 0.0013 (0.0117)	
training:	Epoch: [44][495/817]	Loss 0.0018 (0.0117)	
training:	Epoch: [44][496/817]	Loss 0.0018 (0.0117)	
training:	Epoch: [44][497/817]	Loss 0.0014 (0.0117)	
training:	Epoch: [44][498/817]	Loss 0.0010 (0.0116)	
training:	Epoch: [44][499/817]	Loss 0.0021 (0.0116)	
training:	Epoch: [44][500/817]	Loss 0.0023 (0.0116)	
training:	Epoch: [44][501/817]	Loss 0.0014 (0.0116)	
training:	Epoch: [44][502/817]	Loss 0.0012 (0.0116)	
training:	Epoch: [44][503/817]	Loss 0.0019 (0.0115)	
training:	Epoch: [44][504/817]	Loss 0.0014 (0.0115)	
training:	Epoch: [44][505/817]	Loss 0.0022 (0.0115)	
training:	Epoch: [44][506/817]	Loss 0.0017 (0.0115)	
training:	Epoch: [44][507/817]	Loss 0.0017 (0.0115)	
training:	Epoch: [44][508/817]	Loss 0.0013 (0.0114)	
training:	Epoch: [44][509/817]	Loss 0.0014 (0.0114)	
training:	Epoch: [44][510/817]	Loss 0.0013 (0.0114)	
training:	Epoch: [44][511/817]	Loss 0.0017 (0.0114)	
training:	Epoch: [44][512/817]	Loss 0.2114 (0.0118)	
training:	Epoch: [44][513/817]	Loss 0.0025 (0.0118)	
training:	Epoch: [44][514/817]	Loss 0.0018 (0.0117)	
training:	Epoch: [44][515/817]	Loss 0.0044 (0.0117)	
training:	Epoch: [44][516/817]	Loss 0.6622 (0.0130)	
training:	Epoch: [44][517/817]	Loss 0.0022 (0.0130)	
training:	Epoch: [44][518/817]	Loss 0.0016 (0.0129)	
training:	Epoch: [44][519/817]	Loss 0.0024 (0.0129)	
training:	Epoch: [44][520/817]	Loss 0.0146 (0.0129)	
training:	Epoch: [44][521/817]	Loss 0.0224 (0.0129)	
training:	Epoch: [44][522/817]	Loss 0.0014 (0.0129)	
training:	Epoch: [44][523/817]	Loss 0.0681 (0.0130)	
training:	Epoch: [44][524/817]	Loss 0.0012 (0.0130)	
training:	Epoch: [44][525/817]	Loss 0.0030 (0.0130)	
training:	Epoch: [44][526/817]	Loss 0.0021 (0.0130)	
training:	Epoch: [44][527/817]	Loss 0.0022 (0.0129)	
training:	Epoch: [44][528/817]	Loss 0.0018 (0.0129)	
training:	Epoch: [44][529/817]	Loss 0.0069 (0.0129)	
training:	Epoch: [44][530/817]	Loss 0.0014 (0.0129)	
training:	Epoch: [44][531/817]	Loss 0.0016 (0.0129)	
training:	Epoch: [44][532/817]	Loss 0.0022 (0.0129)	
training:	Epoch: [44][533/817]	Loss 0.0011 (0.0128)	
training:	Epoch: [44][534/817]	Loss 0.0014 (0.0128)	
training:	Epoch: [44][535/817]	Loss 0.0014 (0.0128)	
training:	Epoch: [44][536/817]	Loss 0.0019 (0.0128)	
training:	Epoch: [44][537/817]	Loss 0.1400 (0.0130)	
training:	Epoch: [44][538/817]	Loss 0.0105 (0.0130)	
training:	Epoch: [44][539/817]	Loss 0.0023 (0.0130)	
training:	Epoch: [44][540/817]	Loss 0.0033 (0.0130)	
training:	Epoch: [44][541/817]	Loss 0.0028 (0.0129)	
training:	Epoch: [44][542/817]	Loss 0.0017 (0.0129)	
training:	Epoch: [44][543/817]	Loss 0.0046 (0.0129)	
training:	Epoch: [44][544/817]	Loss 0.0017 (0.0129)	
training:	Epoch: [44][545/817]	Loss 0.0016 (0.0129)	
training:	Epoch: [44][546/817]	Loss 0.0101 (0.0129)	
training:	Epoch: [44][547/817]	Loss 0.0016 (0.0128)	
training:	Epoch: [44][548/817]	Loss 0.0052 (0.0128)	
training:	Epoch: [44][549/817]	Loss 0.0014 (0.0128)	
training:	Epoch: [44][550/817]	Loss 0.0023 (0.0128)	
training:	Epoch: [44][551/817]	Loss 0.0181 (0.0128)	
training:	Epoch: [44][552/817]	Loss 0.2899 (0.0133)	
training:	Epoch: [44][553/817]	Loss 0.0023 (0.0133)	
training:	Epoch: [44][554/817]	Loss 0.1347 (0.0135)	
training:	Epoch: [44][555/817]	Loss 0.0022 (0.0135)	
training:	Epoch: [44][556/817]	Loss 0.0033 (0.0135)	
training:	Epoch: [44][557/817]	Loss 0.0016 (0.0134)	
training:	Epoch: [44][558/817]	Loss 0.0015 (0.0134)	
training:	Epoch: [44][559/817]	Loss 0.0011 (0.0134)	
training:	Epoch: [44][560/817]	Loss 0.0014 (0.0134)	
training:	Epoch: [44][561/817]	Loss 0.0074 (0.0134)	
training:	Epoch: [44][562/817]	Loss 0.0039 (0.0133)	
training:	Epoch: [44][563/817]	Loss 0.0014 (0.0133)	
training:	Epoch: [44][564/817]	Loss 0.0012 (0.0133)	
training:	Epoch: [44][565/817]	Loss 0.0034 (0.0133)	
training:	Epoch: [44][566/817]	Loss 0.0012 (0.0133)	
training:	Epoch: [44][567/817]	Loss 0.0012 (0.0132)	
training:	Epoch: [44][568/817]	Loss 0.0015 (0.0132)	
training:	Epoch: [44][569/817]	Loss 0.0079 (0.0132)	
training:	Epoch: [44][570/817]	Loss 0.0854 (0.0133)	
training:	Epoch: [44][571/817]	Loss 0.0023 (0.0133)	
training:	Epoch: [44][572/817]	Loss 0.0261 (0.0133)	
training:	Epoch: [44][573/817]	Loss 0.4174 (0.0140)	
training:	Epoch: [44][574/817]	Loss 0.0225 (0.0141)	
training:	Epoch: [44][575/817]	Loss 0.0040 (0.0140)	
training:	Epoch: [44][576/817]	Loss 0.0012 (0.0140)	
training:	Epoch: [44][577/817]	Loss 0.0023 (0.0140)	
training:	Epoch: [44][578/817]	Loss 0.7151 (0.0152)	
training:	Epoch: [44][579/817]	Loss 0.0015 (0.0152)	
training:	Epoch: [44][580/817]	Loss 0.0023 (0.0152)	
training:	Epoch: [44][581/817]	Loss 0.0021 (0.0151)	
training:	Epoch: [44][582/817]	Loss 0.0012 (0.0151)	
training:	Epoch: [44][583/817]	Loss 0.0020 (0.0151)	
training:	Epoch: [44][584/817]	Loss 0.0017 (0.0151)	
training:	Epoch: [44][585/817]	Loss 0.1227 (0.0153)	
training:	Epoch: [44][586/817]	Loss 0.7412 (0.0165)	
training:	Epoch: [44][587/817]	Loss 0.0013 (0.0165)	
training:	Epoch: [44][588/817]	Loss 0.0043 (0.0165)	
training:	Epoch: [44][589/817]	Loss 0.0013 (0.0164)	
training:	Epoch: [44][590/817]	Loss 0.0067 (0.0164)	
training:	Epoch: [44][591/817]	Loss 0.0016 (0.0164)	
training:	Epoch: [44][592/817]	Loss 0.0021 (0.0164)	
training:	Epoch: [44][593/817]	Loss 0.0017 (0.0163)	
training:	Epoch: [44][594/817]	Loss 0.0019 (0.0163)	
training:	Epoch: [44][595/817]	Loss 0.0252 (0.0163)	
training:	Epoch: [44][596/817]	Loss 0.0021 (0.0163)	
training:	Epoch: [44][597/817]	Loss 0.0016 (0.0163)	
training:	Epoch: [44][598/817]	Loss 0.0023 (0.0163)	
training:	Epoch: [44][599/817]	Loss 0.0019 (0.0162)	
training:	Epoch: [44][600/817]	Loss 0.0021 (0.0162)	
training:	Epoch: [44][601/817]	Loss 0.0040 (0.0162)	
training:	Epoch: [44][602/817]	Loss 0.0041 (0.0162)	
training:	Epoch: [44][603/817]	Loss 0.0021 (0.0161)	
training:	Epoch: [44][604/817]	Loss 0.0049 (0.0161)	
training:	Epoch: [44][605/817]	Loss 0.0015 (0.0161)	
training:	Epoch: [44][606/817]	Loss 0.0013 (0.0161)	
training:	Epoch: [44][607/817]	Loss 0.0194 (0.0161)	
training:	Epoch: [44][608/817]	Loss 0.0018 (0.0161)	
training:	Epoch: [44][609/817]	Loss 0.0681 (0.0161)	
training:	Epoch: [44][610/817]	Loss 0.0032 (0.0161)	
training:	Epoch: [44][611/817]	Loss 0.0212 (0.0161)	
training:	Epoch: [44][612/817]	Loss 0.0030 (0.0161)	
training:	Epoch: [44][613/817]	Loss 0.0030 (0.0161)	
training:	Epoch: [44][614/817]	Loss 0.0015 (0.0161)	
training:	Epoch: [44][615/817]	Loss 0.0017 (0.0160)	
training:	Epoch: [44][616/817]	Loss 0.0019 (0.0160)	
training:	Epoch: [44][617/817]	Loss 0.0024 (0.0160)	
training:	Epoch: [44][618/817]	Loss 0.0019 (0.0160)	
training:	Epoch: [44][619/817]	Loss 0.7041 (0.0171)	
training:	Epoch: [44][620/817]	Loss 0.0015 (0.0171)	
training:	Epoch: [44][621/817]	Loss 0.0082 (0.0170)	
training:	Epoch: [44][622/817]	Loss 0.0056 (0.0170)	
training:	Epoch: [44][623/817]	Loss 0.0021 (0.0170)	
training:	Epoch: [44][624/817]	Loss 0.0214 (0.0170)	
training:	Epoch: [44][625/817]	Loss 0.0074 (0.0170)	
training:	Epoch: [44][626/817]	Loss 0.0026 (0.0170)	
training:	Epoch: [44][627/817]	Loss 0.0252 (0.0170)	
training:	Epoch: [44][628/817]	Loss 0.0016 (0.0170)	
training:	Epoch: [44][629/817]	Loss 0.2376 (0.0173)	
training:	Epoch: [44][630/817]	Loss 0.0324 (0.0173)	
training:	Epoch: [44][631/817]	Loss 0.0015 (0.0173)	
training:	Epoch: [44][632/817]	Loss 0.0017 (0.0173)	
training:	Epoch: [44][633/817]	Loss 0.0102 (0.0173)	
training:	Epoch: [44][634/817]	Loss 0.0023 (0.0172)	
training:	Epoch: [44][635/817]	Loss 0.0026 (0.0172)	
training:	Epoch: [44][636/817]	Loss 0.0014 (0.0172)	
training:	Epoch: [44][637/817]	Loss 0.0020 (0.0172)	
training:	Epoch: [44][638/817]	Loss 0.0115 (0.0172)	
training:	Epoch: [44][639/817]	Loss 0.0055 (0.0172)	
training:	Epoch: [44][640/817]	Loss 0.0060 (0.0171)	
training:	Epoch: [44][641/817]	Loss 0.0016 (0.0171)	
training:	Epoch: [44][642/817]	Loss 0.0054 (0.0171)	
training:	Epoch: [44][643/817]	Loss 0.0013 (0.0171)	
training:	Epoch: [44][644/817]	Loss 0.0046 (0.0170)	
training:	Epoch: [44][645/817]	Loss 0.0153 (0.0170)	
training:	Epoch: [44][646/817]	Loss 0.0014 (0.0170)	
training:	Epoch: [44][647/817]	Loss 0.0620 (0.0171)	
training:	Epoch: [44][648/817]	Loss 0.0036 (0.0171)	
training:	Epoch: [44][649/817]	Loss 0.0052 (0.0171)	
training:	Epoch: [44][650/817]	Loss 0.0021 (0.0170)	
training:	Epoch: [44][651/817]	Loss 0.0016 (0.0170)	
training:	Epoch: [44][652/817]	Loss 0.0015 (0.0170)	
training:	Epoch: [44][653/817]	Loss 0.0026 (0.0170)	
training:	Epoch: [44][654/817]	Loss 0.0027 (0.0169)	
training:	Epoch: [44][655/817]	Loss 0.0013 (0.0169)	
training:	Epoch: [44][656/817]	Loss 0.0078 (0.0169)	
training:	Epoch: [44][657/817]	Loss 0.0039 (0.0169)	
training:	Epoch: [44][658/817]	Loss 0.0012 (0.0169)	
training:	Epoch: [44][659/817]	Loss 0.0017 (0.0168)	
training:	Epoch: [44][660/817]	Loss 0.0016 (0.0168)	
training:	Epoch: [44][661/817]	Loss 0.0012 (0.0168)	
training:	Epoch: [44][662/817]	Loss 0.0085 (0.0168)	
training:	Epoch: [44][663/817]	Loss 0.0100 (0.0168)	
training:	Epoch: [44][664/817]	Loss 0.0020 (0.0167)	
training:	Epoch: [44][665/817]	Loss 0.0476 (0.0168)	
training:	Epoch: [44][666/817]	Loss 0.0124 (0.0168)	
training:	Epoch: [44][667/817]	Loss 0.0022 (0.0168)	
training:	Epoch: [44][668/817]	Loss 0.0099 (0.0167)	
training:	Epoch: [44][669/817]	Loss 0.0015 (0.0167)	
training:	Epoch: [44][670/817]	Loss 0.0018 (0.0167)	
training:	Epoch: [44][671/817]	Loss 0.0017 (0.0167)	
training:	Epoch: [44][672/817]	Loss 0.0027 (0.0167)	
training:	Epoch: [44][673/817]	Loss 0.0026 (0.0166)	
training:	Epoch: [44][674/817]	Loss 0.3376 (0.0171)	
training:	Epoch: [44][675/817]	Loss 0.0021 (0.0171)	
training:	Epoch: [44][676/817]	Loss 0.0042 (0.0171)	
training:	Epoch: [44][677/817]	Loss 0.0014 (0.0171)	
training:	Epoch: [44][678/817]	Loss 0.0012 (0.0170)	
training:	Epoch: [44][679/817]	Loss 0.0023 (0.0170)	
training:	Epoch: [44][680/817]	Loss 0.0019 (0.0170)	
training:	Epoch: [44][681/817]	Loss 0.0034 (0.0170)	
training:	Epoch: [44][682/817]	Loss 0.0080 (0.0169)	
training:	Epoch: [44][683/817]	Loss 0.0053 (0.0169)	
training:	Epoch: [44][684/817]	Loss 0.0084 (0.0169)	
training:	Epoch: [44][685/817]	Loss 0.0018 (0.0169)	
training:	Epoch: [44][686/817]	Loss 0.0040 (0.0169)	
training:	Epoch: [44][687/817]	Loss 0.0026 (0.0169)	
training:	Epoch: [44][688/817]	Loss 0.0516 (0.0169)	
training:	Epoch: [44][689/817]	Loss 0.0019 (0.0169)	
training:	Epoch: [44][690/817]	Loss 0.0015 (0.0169)	
training:	Epoch: [44][691/817]	Loss 0.6161 (0.0177)	
training:	Epoch: [44][692/817]	Loss 0.0022 (0.0177)	
training:	Epoch: [44][693/817]	Loss 0.0022 (0.0177)	
training:	Epoch: [44][694/817]	Loss 0.0102 (0.0177)	
training:	Epoch: [44][695/817]	Loss 0.0050 (0.0177)	
training:	Epoch: [44][696/817]	Loss 0.0031 (0.0176)	
training:	Epoch: [44][697/817]	Loss 0.0014 (0.0176)	
training:	Epoch: [44][698/817]	Loss 0.0021 (0.0176)	
training:	Epoch: [44][699/817]	Loss 0.0016 (0.0176)	
training:	Epoch: [44][700/817]	Loss 0.0044 (0.0176)	
training:	Epoch: [44][701/817]	Loss 0.0016 (0.0175)	
training:	Epoch: [44][702/817]	Loss 0.0026 (0.0175)	
training:	Epoch: [44][703/817]	Loss 0.1642 (0.0177)	
training:	Epoch: [44][704/817]	Loss 0.0016 (0.0177)	
training:	Epoch: [44][705/817]	Loss 0.0014 (0.0177)	
training:	Epoch: [44][706/817]	Loss 0.0018 (0.0176)	
training:	Epoch: [44][707/817]	Loss 0.0026 (0.0176)	
training:	Epoch: [44][708/817]	Loss 0.0184 (0.0176)	
training:	Epoch: [44][709/817]	Loss 0.0013 (0.0176)	
training:	Epoch: [44][710/817]	Loss 0.1528 (0.0178)	
training:	Epoch: [44][711/817]	Loss 0.0012 (0.0178)	
training:	Epoch: [44][712/817]	Loss 0.0015 (0.0177)	
training:	Epoch: [44][713/817]	Loss 0.0490 (0.0178)	
training:	Epoch: [44][714/817]	Loss 0.0033 (0.0178)	
training:	Epoch: [44][715/817]	Loss 0.0017 (0.0177)	
training:	Epoch: [44][716/817]	Loss 0.0351 (0.0178)	
training:	Epoch: [44][717/817]	Loss 0.0017 (0.0177)	
training:	Epoch: [44][718/817]	Loss 0.0036 (0.0177)	
training:	Epoch: [44][719/817]	Loss 0.0036 (0.0177)	
training:	Epoch: [44][720/817]	Loss 0.2344 (0.0180)	
training:	Epoch: [44][721/817]	Loss 0.0100 (0.0180)	
training:	Epoch: [44][722/817]	Loss 0.0057 (0.0180)	
training:	Epoch: [44][723/817]	Loss 0.0011 (0.0180)	
training:	Epoch: [44][724/817]	Loss 0.0440 (0.0180)	
training:	Epoch: [44][725/817]	Loss 0.0014 (0.0180)	
training:	Epoch: [44][726/817]	Loss 0.0018 (0.0180)	
training:	Epoch: [44][727/817]	Loss 0.0197 (0.0180)	
training:	Epoch: [44][728/817]	Loss 0.0013 (0.0179)	
training:	Epoch: [44][729/817]	Loss 0.0020 (0.0179)	
training:	Epoch: [44][730/817]	Loss 0.0086 (0.0179)	
training:	Epoch: [44][731/817]	Loss 0.0102 (0.0179)	
training:	Epoch: [44][732/817]	Loss 0.0015 (0.0179)	
training:	Epoch: [44][733/817]	Loss 0.0015 (0.0178)	
training:	Epoch: [44][734/817]	Loss 0.0073 (0.0178)	
training:	Epoch: [44][735/817]	Loss 0.1023 (0.0179)	
training:	Epoch: [44][736/817]	Loss 0.3363 (0.0184)	
training:	Epoch: [44][737/817]	Loss 0.0030 (0.0184)	
training:	Epoch: [44][738/817]	Loss 0.0407 (0.0184)	
training:	Epoch: [44][739/817]	Loss 0.0042 (0.0184)	
training:	Epoch: [44][740/817]	Loss 0.0027 (0.0183)	
training:	Epoch: [44][741/817]	Loss 0.0021 (0.0183)	
training:	Epoch: [44][742/817]	Loss 0.0017 (0.0183)	
training:	Epoch: [44][743/817]	Loss 0.0302 (0.0183)	
training:	Epoch: [44][744/817]	Loss 0.0022 (0.0183)	
training:	Epoch: [44][745/817]	Loss 0.3380 (0.0187)	
training:	Epoch: [44][746/817]	Loss 0.0084 (0.0187)	
training:	Epoch: [44][747/817]	Loss 0.0015 (0.0187)	
training:	Epoch: [44][748/817]	Loss 0.0048 (0.0187)	
training:	Epoch: [44][749/817]	Loss 0.0075 (0.0187)	
training:	Epoch: [44][750/817]	Loss 0.0013 (0.0186)	
training:	Epoch: [44][751/817]	Loss 0.0026 (0.0186)	
training:	Epoch: [44][752/817]	Loss 0.0169 (0.0186)	
training:	Epoch: [44][753/817]	Loss 0.0031 (0.0186)	
training:	Epoch: [44][754/817]	Loss 0.0362 (0.0186)	
training:	Epoch: [44][755/817]	Loss 0.0015 (0.0186)	
training:	Epoch: [44][756/817]	Loss 0.0018 (0.0186)	
training:	Epoch: [44][757/817]	Loss 0.0013 (0.0185)	
training:	Epoch: [44][758/817]	Loss 0.0023 (0.0185)	
training:	Epoch: [44][759/817]	Loss 0.0013 (0.0185)	
training:	Epoch: [44][760/817]	Loss 0.0031 (0.0185)	
training:	Epoch: [44][761/817]	Loss 0.0178 (0.0185)	
training:	Epoch: [44][762/817]	Loss 0.0012 (0.0185)	
training:	Epoch: [44][763/817]	Loss 0.0016 (0.0184)	
training:	Epoch: [44][764/817]	Loss 0.0034 (0.0184)	
training:	Epoch: [44][765/817]	Loss 0.0028 (0.0184)	
training:	Epoch: [44][766/817]	Loss 0.0035 (0.0184)	
training:	Epoch: [44][767/817]	Loss 0.0416 (0.0184)	
training:	Epoch: [44][768/817]	Loss 0.0018 (0.0184)	
training:	Epoch: [44][769/817]	Loss 0.0040 (0.0184)	
training:	Epoch: [44][770/817]	Loss 0.0055 (0.0183)	
training:	Epoch: [44][771/817]	Loss 0.0089 (0.0183)	
training:	Epoch: [44][772/817]	Loss 0.0016 (0.0183)	
training:	Epoch: [44][773/817]	Loss 0.0017 (0.0183)	
training:	Epoch: [44][774/817]	Loss 0.0039 (0.0183)	
training:	Epoch: [44][775/817]	Loss 0.0057 (0.0183)	
training:	Epoch: [44][776/817]	Loss 0.0012 (0.0182)	
training:	Epoch: [44][777/817]	Loss 0.0017 (0.0182)	
training:	Epoch: [44][778/817]	Loss 0.0022 (0.0182)	
training:	Epoch: [44][779/817]	Loss 0.0017 (0.0182)	
training:	Epoch: [44][780/817]	Loss 0.0022 (0.0181)	
training:	Epoch: [44][781/817]	Loss 0.0043 (0.0181)	
training:	Epoch: [44][782/817]	Loss 0.0016 (0.0181)	
training:	Epoch: [44][783/817]	Loss 0.0014 (0.0181)	
training:	Epoch: [44][784/817]	Loss 0.0023 (0.0181)	
training:	Epoch: [44][785/817]	Loss 0.0024 (0.0180)	
training:	Epoch: [44][786/817]	Loss 0.0166 (0.0180)	
training:	Epoch: [44][787/817]	Loss 0.1332 (0.0182)	
training:	Epoch: [44][788/817]	Loss 0.0186 (0.0182)	
training:	Epoch: [44][789/817]	Loss 0.0010 (0.0182)	
training:	Epoch: [44][790/817]	Loss 0.0012 (0.0181)	
training:	Epoch: [44][791/817]	Loss 0.0034 (0.0181)	
training:	Epoch: [44][792/817]	Loss 0.0118 (0.0181)	
training:	Epoch: [44][793/817]	Loss 0.0017 (0.0181)	
training:	Epoch: [44][794/817]	Loss 0.0029 (0.0181)	
training:	Epoch: [44][795/817]	Loss 0.0495 (0.0181)	
training:	Epoch: [44][796/817]	Loss 0.0015 (0.0181)	
training:	Epoch: [44][797/817]	Loss 0.0075 (0.0181)	
training:	Epoch: [44][798/817]	Loss 0.0055 (0.0181)	
training:	Epoch: [44][799/817]	Loss 0.0012 (0.0181)	
training:	Epoch: [44][800/817]	Loss 0.0024 (0.0180)	
training:	Epoch: [44][801/817]	Loss 0.0188 (0.0180)	
training:	Epoch: [44][802/817]	Loss 0.0013 (0.0180)	
training:	Epoch: [44][803/817]	Loss 0.0013 (0.0180)	
training:	Epoch: [44][804/817]	Loss 0.0029 (0.0180)	
training:	Epoch: [44][805/817]	Loss 0.0045 (0.0180)	
training:	Epoch: [44][806/817]	Loss 0.0013 (0.0179)	
training:	Epoch: [44][807/817]	Loss 0.0027 (0.0179)	
training:	Epoch: [44][808/817]	Loss 0.0012 (0.0179)	
training:	Epoch: [44][809/817]	Loss 0.3521 (0.0183)	
training:	Epoch: [44][810/817]	Loss 0.0054 (0.0183)	
training:	Epoch: [44][811/817]	Loss 0.0035 (0.0183)	
training:	Epoch: [44][812/817]	Loss 0.0018 (0.0183)	
training:	Epoch: [44][813/817]	Loss 0.3430 (0.0187)	
training:	Epoch: [44][814/817]	Loss 0.0029 (0.0186)	
training:	Epoch: [44][815/817]	Loss 0.0011 (0.0186)	
training:	Epoch: [44][816/817]	Loss 0.0019 (0.0186)	
training:	Epoch: [44][817/817]	Loss 0.0143 (0.0186)	
Training:	 Loss: 0.0186

Training:	 ACC: 0.9984 0.9985 0.9997 0.9971
Validation:	 ACC: 0.7870 0.7892 0.8362 0.7377
Validation:	 Best_BACC: 0.8223 0.8218 0.8127 0.8318
Validation:	 Loss: 1.0831
Pretraining:	Epoch 45/200
----------
training:	Epoch: [45][1/817]	Loss 0.0025 (0.0025)	
training:	Epoch: [45][2/817]	Loss 0.0011 (0.0018)	
training:	Epoch: [45][3/817]	Loss 0.0015 (0.0017)	
training:	Epoch: [45][4/817]	Loss 0.0014 (0.0016)	
training:	Epoch: [45][5/817]	Loss 0.0014 (0.0016)	
training:	Epoch: [45][6/817]	Loss 0.0048 (0.0021)	
training:	Epoch: [45][7/817]	Loss 0.0096 (0.0032)	
training:	Epoch: [45][8/817]	Loss 0.1445 (0.0208)	
training:	Epoch: [45][9/817]	Loss 0.0017 (0.0187)	
training:	Epoch: [45][10/817]	Loss 0.0016 (0.0170)	
training:	Epoch: [45][11/817]	Loss 0.0021 (0.0156)	
training:	Epoch: [45][12/817]	Loss 0.0012 (0.0144)	
training:	Epoch: [45][13/817]	Loss 0.0016 (0.0135)	
training:	Epoch: [45][14/817]	Loss 0.0015 (0.0126)	
training:	Epoch: [45][15/817]	Loss 0.2447 (0.0281)	
training:	Epoch: [45][16/817]	Loss 0.0011 (0.0264)	
training:	Epoch: [45][17/817]	Loss 0.0018 (0.0249)	
training:	Epoch: [45][18/817]	Loss 0.0018 (0.0237)	
training:	Epoch: [45][19/817]	Loss 0.0018 (0.0225)	
training:	Epoch: [45][20/817]	Loss 0.0017 (0.0215)	
training:	Epoch: [45][21/817]	Loss 0.0067 (0.0208)	
training:	Epoch: [45][22/817]	Loss 0.0102 (0.0203)	
training:	Epoch: [45][23/817]	Loss 0.0015 (0.0195)	
training:	Epoch: [45][24/817]	Loss 0.0020 (0.0187)	
training:	Epoch: [45][25/817]	Loss 0.0018 (0.0181)	
training:	Epoch: [45][26/817]	Loss 0.0021 (0.0175)	
training:	Epoch: [45][27/817]	Loss 0.0033 (0.0169)	
training:	Epoch: [45][28/817]	Loss 0.0026 (0.0164)	
training:	Epoch: [45][29/817]	Loss 0.0014 (0.0159)	
training:	Epoch: [45][30/817]	Loss 0.3926 (0.0285)	
training:	Epoch: [45][31/817]	Loss 0.0040 (0.0277)	
training:	Epoch: [45][32/817]	Loss 0.0016 (0.0269)	
training:	Epoch: [45][33/817]	Loss 0.0014 (0.0261)	
training:	Epoch: [45][34/817]	Loss 0.0036 (0.0254)	
training:	Epoch: [45][35/817]	Loss 0.0045 (0.0248)	
training:	Epoch: [45][36/817]	Loss 0.0013 (0.0242)	
training:	Epoch: [45][37/817]	Loss 0.0015 (0.0236)	
training:	Epoch: [45][38/817]	Loss 0.0022 (0.0230)	
training:	Epoch: [45][39/817]	Loss 0.0030 (0.0225)	
training:	Epoch: [45][40/817]	Loss 0.0044 (0.0220)	
training:	Epoch: [45][41/817]	Loss 0.0015 (0.0215)	
training:	Epoch: [45][42/817]	Loss 0.0014 (0.0210)	
training:	Epoch: [45][43/817]	Loss 0.0014 (0.0206)	
training:	Epoch: [45][44/817]	Loss 0.0016 (0.0202)	
training:	Epoch: [45][45/817]	Loss 0.0016 (0.0197)	
training:	Epoch: [45][46/817]	Loss 0.0012 (0.0193)	
training:	Epoch: [45][47/817]	Loss 0.0042 (0.0190)	
training:	Epoch: [45][48/817]	Loss 0.0017 (0.0187)	
training:	Epoch: [45][49/817]	Loss 0.0016 (0.0183)	
training:	Epoch: [45][50/817]	Loss 0.0012 (0.0180)	
training:	Epoch: [45][51/817]	Loss 0.0411 (0.0184)	
training:	Epoch: [45][52/817]	Loss 0.0016 (0.0181)	
training:	Epoch: [45][53/817]	Loss 0.0046 (0.0178)	
training:	Epoch: [45][54/817]	Loss 0.0015 (0.0175)	
training:	Epoch: [45][55/817]	Loss 0.0015 (0.0173)	
training:	Epoch: [45][56/817]	Loss 0.0024 (0.0170)	
training:	Epoch: [45][57/817]	Loss 0.0019 (0.0167)	
training:	Epoch: [45][58/817]	Loss 0.0014 (0.0165)	
training:	Epoch: [45][59/817]	Loss 0.0016 (0.0162)	
training:	Epoch: [45][60/817]	Loss 0.0019 (0.0160)	
training:	Epoch: [45][61/817]	Loss 0.0023 (0.0157)	
training:	Epoch: [45][62/817]	Loss 0.0031 (0.0155)	
training:	Epoch: [45][63/817]	Loss 0.0030 (0.0153)	
training:	Epoch: [45][64/817]	Loss 0.0015 (0.0151)	
training:	Epoch: [45][65/817]	Loss 0.0221 (0.0152)	
training:	Epoch: [45][66/817]	Loss 0.0017 (0.0150)	
training:	Epoch: [45][67/817]	Loss 0.0097 (0.0149)	
training:	Epoch: [45][68/817]	Loss 0.0025 (0.0148)	
training:	Epoch: [45][69/817]	Loss 0.0020 (0.0146)	
training:	Epoch: [45][70/817]	Loss 0.0025 (0.0144)	
training:	Epoch: [45][71/817]	Loss 0.0022 (0.0142)	
training:	Epoch: [45][72/817]	Loss 0.0014 (0.0141)	
training:	Epoch: [45][73/817]	Loss 0.0020 (0.0139)	
training:	Epoch: [45][74/817]	Loss 0.0031 (0.0137)	
training:	Epoch: [45][75/817]	Loss 0.0014 (0.0136)	
training:	Epoch: [45][76/817]	Loss 0.0037 (0.0134)	
training:	Epoch: [45][77/817]	Loss 0.0024 (0.0133)	
training:	Epoch: [45][78/817]	Loss 0.0015 (0.0132)	
training:	Epoch: [45][79/817]	Loss 0.0030 (0.0130)	
training:	Epoch: [45][80/817]	Loss 0.0013 (0.0129)	
training:	Epoch: [45][81/817]	Loss 0.0013 (0.0127)	
training:	Epoch: [45][82/817]	Loss 0.0015 (0.0126)	
training:	Epoch: [45][83/817]	Loss 0.0013 (0.0125)	
training:	Epoch: [45][84/817]	Loss 0.0020 (0.0123)	
training:	Epoch: [45][85/817]	Loss 0.0014 (0.0122)	
training:	Epoch: [45][86/817]	Loss 0.0026 (0.0121)	
training:	Epoch: [45][87/817]	Loss 0.0020 (0.0120)	
training:	Epoch: [45][88/817]	Loss 0.0015 (0.0119)	
training:	Epoch: [45][89/817]	Loss 0.0017 (0.0117)	
training:	Epoch: [45][90/817]	Loss 0.0015 (0.0116)	
training:	Epoch: [45][91/817]	Loss 0.0016 (0.0115)	
training:	Epoch: [45][92/817]	Loss 0.0026 (0.0114)	
training:	Epoch: [45][93/817]	Loss 0.0021 (0.0113)	
training:	Epoch: [45][94/817]	Loss 0.0057 (0.0113)	
training:	Epoch: [45][95/817]	Loss 0.0015 (0.0112)	
training:	Epoch: [45][96/817]	Loss 0.0017 (0.0111)	
training:	Epoch: [45][97/817]	Loss 0.0119 (0.0111)	
training:	Epoch: [45][98/817]	Loss 0.0015 (0.0110)	
training:	Epoch: [45][99/817]	Loss 0.0023 (0.0109)	
training:	Epoch: [45][100/817]	Loss 0.0015 (0.0108)	
training:	Epoch: [45][101/817]	Loss 0.0045 (0.0107)	
training:	Epoch: [45][102/817]	Loss 0.0057 (0.0107)	
training:	Epoch: [45][103/817]	Loss 0.0013 (0.0106)	
training:	Epoch: [45][104/817]	Loss 0.0013 (0.0105)	
training:	Epoch: [45][105/817]	Loss 0.0016 (0.0104)	
training:	Epoch: [45][106/817]	Loss 0.1837 (0.0121)	
training:	Epoch: [45][107/817]	Loss 0.0025 (0.0120)	
training:	Epoch: [45][108/817]	Loss 0.0015 (0.0119)	
training:	Epoch: [45][109/817]	Loss 0.0017 (0.0118)	
training:	Epoch: [45][110/817]	Loss 0.0016 (0.0117)	
training:	Epoch: [45][111/817]	Loss 0.0013 (0.0116)	
training:	Epoch: [45][112/817]	Loss 0.0012 (0.0115)	
training:	Epoch: [45][113/817]	Loss 0.0026 (0.0114)	
training:	Epoch: [45][114/817]	Loss 0.0018 (0.0113)	
training:	Epoch: [45][115/817]	Loss 0.0018 (0.0112)	
training:	Epoch: [45][116/817]	Loss 0.0019 (0.0112)	
training:	Epoch: [45][117/817]	Loss 0.0024 (0.0111)	
training:	Epoch: [45][118/817]	Loss 0.0020 (0.0110)	
training:	Epoch: [45][119/817]	Loss 0.0018 (0.0109)	
training:	Epoch: [45][120/817]	Loss 0.0010 (0.0109)	
training:	Epoch: [45][121/817]	Loss 0.0017 (0.0108)	
training:	Epoch: [45][122/817]	Loss 0.0016 (0.0107)	
training:	Epoch: [45][123/817]	Loss 0.0016 (0.0106)	
training:	Epoch: [45][124/817]	Loss 0.0012 (0.0106)	
training:	Epoch: [45][125/817]	Loss 0.0033 (0.0105)	
training:	Epoch: [45][126/817]	Loss 0.0018 (0.0104)	
training:	Epoch: [45][127/817]	Loss 0.0465 (0.0107)	
training:	Epoch: [45][128/817]	Loss 0.0023 (0.0106)	
training:	Epoch: [45][129/817]	Loss 0.0013 (0.0106)	
training:	Epoch: [45][130/817]	Loss 0.0015 (0.0105)	
training:	Epoch: [45][131/817]	Loss 0.0098 (0.0105)	
training:	Epoch: [45][132/817]	Loss 0.0017 (0.0104)	
training:	Epoch: [45][133/817]	Loss 0.0014 (0.0104)	
training:	Epoch: [45][134/817]	Loss 0.0016 (0.0103)	
training:	Epoch: [45][135/817]	Loss 0.0016 (0.0102)	
training:	Epoch: [45][136/817]	Loss 0.0019 (0.0102)	
training:	Epoch: [45][137/817]	Loss 0.0024 (0.0101)	
training:	Epoch: [45][138/817]	Loss 0.0015 (0.0101)	
training:	Epoch: [45][139/817]	Loss 0.0026 (0.0100)	
training:	Epoch: [45][140/817]	Loss 0.0020 (0.0099)	
training:	Epoch: [45][141/817]	Loss 0.0947 (0.0105)	
training:	Epoch: [45][142/817]	Loss 0.1322 (0.0114)	
training:	Epoch: [45][143/817]	Loss 0.0012 (0.0113)	
training:	Epoch: [45][144/817]	Loss 0.0652 (0.0117)	
training:	Epoch: [45][145/817]	Loss 0.0052 (0.0117)	
training:	Epoch: [45][146/817]	Loss 0.0075 (0.0116)	
training:	Epoch: [45][147/817]	Loss 0.0132 (0.0116)	
training:	Epoch: [45][148/817]	Loss 0.0036 (0.0116)	
training:	Epoch: [45][149/817]	Loss 0.0017 (0.0115)	
training:	Epoch: [45][150/817]	Loss 0.0017 (0.0115)	
training:	Epoch: [45][151/817]	Loss 0.0052 (0.0114)	
training:	Epoch: [45][152/817]	Loss 0.0018 (0.0113)	
training:	Epoch: [45][153/817]	Loss 0.0229 (0.0114)	
training:	Epoch: [45][154/817]	Loss 0.0023 (0.0114)	
training:	Epoch: [45][155/817]	Loss 0.0019 (0.0113)	
training:	Epoch: [45][156/817]	Loss 0.0013 (0.0112)	
training:	Epoch: [45][157/817]	Loss 0.0014 (0.0112)	
training:	Epoch: [45][158/817]	Loss 0.0025 (0.0111)	
training:	Epoch: [45][159/817]	Loss 0.0011 (0.0111)	
training:	Epoch: [45][160/817]	Loss 0.0080 (0.0110)	
training:	Epoch: [45][161/817]	Loss 0.0252 (0.0111)	
training:	Epoch: [45][162/817]	Loss 0.0013 (0.0111)	
training:	Epoch: [45][163/817]	Loss 0.0016 (0.0110)	
training:	Epoch: [45][164/817]	Loss 0.0144 (0.0110)	
training:	Epoch: [45][165/817]	Loss 0.0014 (0.0110)	
training:	Epoch: [45][166/817]	Loss 0.0022 (0.0109)	
training:	Epoch: [45][167/817]	Loss 0.0229 (0.0110)	
training:	Epoch: [45][168/817]	Loss 0.0076 (0.0110)	
training:	Epoch: [45][169/817]	Loss 0.0102 (0.0110)	
training:	Epoch: [45][170/817]	Loss 0.0037 (0.0109)	
training:	Epoch: [45][171/817]	Loss 0.0017 (0.0109)	
training:	Epoch: [45][172/817]	Loss 0.0038 (0.0108)	
training:	Epoch: [45][173/817]	Loss 0.0019 (0.0108)	
training:	Epoch: [45][174/817]	Loss 0.0034 (0.0107)	
training:	Epoch: [45][175/817]	Loss 0.0012 (0.0107)	
training:	Epoch: [45][176/817]	Loss 0.0014 (0.0106)	
training:	Epoch: [45][177/817]	Loss 0.0143 (0.0106)	
training:	Epoch: [45][178/817]	Loss 0.0013 (0.0106)	
training:	Epoch: [45][179/817]	Loss 0.0015 (0.0105)	
training:	Epoch: [45][180/817]	Loss 0.0011 (0.0105)	
training:	Epoch: [45][181/817]	Loss 0.0047 (0.0105)	
training:	Epoch: [45][182/817]	Loss 0.0016 (0.0104)	
training:	Epoch: [45][183/817]	Loss 0.0033 (0.0104)	
training:	Epoch: [45][184/817]	Loss 0.0019 (0.0103)	
training:	Epoch: [45][185/817]	Loss 0.0012 (0.0103)	
training:	Epoch: [45][186/817]	Loss 0.0158 (0.0103)	
training:	Epoch: [45][187/817]	Loss 0.0292 (0.0104)	
training:	Epoch: [45][188/817]	Loss 0.0014 (0.0104)	
training:	Epoch: [45][189/817]	Loss 0.0024 (0.0103)	
training:	Epoch: [45][190/817]	Loss 0.0015 (0.0103)	
training:	Epoch: [45][191/817]	Loss 0.0088 (0.0103)	
training:	Epoch: [45][192/817]	Loss 0.0016 (0.0102)	
training:	Epoch: [45][193/817]	Loss 0.0048 (0.0102)	
training:	Epoch: [45][194/817]	Loss 0.0018 (0.0101)	
training:	Epoch: [45][195/817]	Loss 0.0052 (0.0101)	
training:	Epoch: [45][196/817]	Loss 0.0017 (0.0101)	
training:	Epoch: [45][197/817]	Loss 0.0014 (0.0100)	
training:	Epoch: [45][198/817]	Loss 0.0016 (0.0100)	
training:	Epoch: [45][199/817]	Loss 0.0191 (0.0100)	
training:	Epoch: [45][200/817]	Loss 0.0018 (0.0100)	
training:	Epoch: [45][201/817]	Loss 0.0015 (0.0100)	
training:	Epoch: [45][202/817]	Loss 0.0019 (0.0099)	
training:	Epoch: [45][203/817]	Loss 0.0013 (0.0099)	
training:	Epoch: [45][204/817]	Loss 0.0013 (0.0098)	
training:	Epoch: [45][205/817]	Loss 0.0116 (0.0098)	
training:	Epoch: [45][206/817]	Loss 0.0025 (0.0098)	
training:	Epoch: [45][207/817]	Loss 0.0018 (0.0098)	
training:	Epoch: [45][208/817]	Loss 0.0012 (0.0097)	
training:	Epoch: [45][209/817]	Loss 0.0013 (0.0097)	
training:	Epoch: [45][210/817]	Loss 0.0014 (0.0096)	
training:	Epoch: [45][211/817]	Loss 0.0021 (0.0096)	
training:	Epoch: [45][212/817]	Loss 0.0013 (0.0096)	
training:	Epoch: [45][213/817]	Loss 0.0014 (0.0095)	
training:	Epoch: [45][214/817]	Loss 0.0014 (0.0095)	
training:	Epoch: [45][215/817]	Loss 0.0022 (0.0095)	
training:	Epoch: [45][216/817]	Loss 0.0012 (0.0094)	
training:	Epoch: [45][217/817]	Loss 0.0012 (0.0094)	
training:	Epoch: [45][218/817]	Loss 0.0017 (0.0093)	
training:	Epoch: [45][219/817]	Loss 0.0015 (0.0093)	
training:	Epoch: [45][220/817]	Loss 0.0013 (0.0093)	
training:	Epoch: [45][221/817]	Loss 0.0027 (0.0092)	
training:	Epoch: [45][222/817]	Loss 0.0015 (0.0092)	
training:	Epoch: [45][223/817]	Loss 0.0047 (0.0092)	
training:	Epoch: [45][224/817]	Loss 0.0017 (0.0092)	
training:	Epoch: [45][225/817]	Loss 0.0129 (0.0092)	
training:	Epoch: [45][226/817]	Loss 0.0013 (0.0091)	
training:	Epoch: [45][227/817]	Loss 0.0394 (0.0093)	
training:	Epoch: [45][228/817]	Loss 0.0014 (0.0092)	
training:	Epoch: [45][229/817]	Loss 0.0012 (0.0092)	
training:	Epoch: [45][230/817]	Loss 0.0018 (0.0092)	
training:	Epoch: [45][231/817]	Loss 0.0016 (0.0091)	
training:	Epoch: [45][232/817]	Loss 0.0012 (0.0091)	
training:	Epoch: [45][233/817]	Loss 0.0217 (0.0092)	
training:	Epoch: [45][234/817]	Loss 0.0018 (0.0091)	
training:	Epoch: [45][235/817]	Loss 0.0018 (0.0091)	
training:	Epoch: [45][236/817]	Loss 0.0019 (0.0091)	
training:	Epoch: [45][237/817]	Loss 0.0018 (0.0090)	
training:	Epoch: [45][238/817]	Loss 0.0013 (0.0090)	
training:	Epoch: [45][239/817]	Loss 0.0018 (0.0090)	
training:	Epoch: [45][240/817]	Loss 0.0475 (0.0091)	
training:	Epoch: [45][241/817]	Loss 0.0012 (0.0091)	
training:	Epoch: [45][242/817]	Loss 0.0064 (0.0091)	
training:	Epoch: [45][243/817]	Loss 0.0025 (0.0091)	
training:	Epoch: [45][244/817]	Loss 0.0018 (0.0090)	
training:	Epoch: [45][245/817]	Loss 0.0014 (0.0090)	
training:	Epoch: [45][246/817]	Loss 0.0025 (0.0090)	
training:	Epoch: [45][247/817]	Loss 0.0063 (0.0090)	
training:	Epoch: [45][248/817]	Loss 0.0163 (0.0090)	
training:	Epoch: [45][249/817]	Loss 0.0020 (0.0090)	
training:	Epoch: [45][250/817]	Loss 0.0012 (0.0089)	
training:	Epoch: [45][251/817]	Loss 0.0012 (0.0089)	
training:	Epoch: [45][252/817]	Loss 0.0020 (0.0089)	
training:	Epoch: [45][253/817]	Loss 0.0018 (0.0088)	
training:	Epoch: [45][254/817]	Loss 0.0014 (0.0088)	
training:	Epoch: [45][255/817]	Loss 0.0262 (0.0089)	
training:	Epoch: [45][256/817]	Loss 0.0022 (0.0089)	
training:	Epoch: [45][257/817]	Loss 0.0015 (0.0088)	
training:	Epoch: [45][258/817]	Loss 0.0014 (0.0088)	
training:	Epoch: [45][259/817]	Loss 0.0018 (0.0088)	
training:	Epoch: [45][260/817]	Loss 0.0012 (0.0087)	
training:	Epoch: [45][261/817]	Loss 0.0019 (0.0087)	
training:	Epoch: [45][262/817]	Loss 0.0012 (0.0087)	
training:	Epoch: [45][263/817]	Loss 0.0013 (0.0087)	
training:	Epoch: [45][264/817]	Loss 0.0109 (0.0087)	
training:	Epoch: [45][265/817]	Loss 0.0012 (0.0086)	
training:	Epoch: [45][266/817]	Loss 0.0013 (0.0086)	
training:	Epoch: [45][267/817]	Loss 0.0017 (0.0086)	
training:	Epoch: [45][268/817]	Loss 0.0029 (0.0086)	
training:	Epoch: [45][269/817]	Loss 0.0017 (0.0085)	
training:	Epoch: [45][270/817]	Loss 0.0015 (0.0085)	
training:	Epoch: [45][271/817]	Loss 0.0012 (0.0085)	
training:	Epoch: [45][272/817]	Loss 0.0012 (0.0085)	
training:	Epoch: [45][273/817]	Loss 0.0082 (0.0085)	
training:	Epoch: [45][274/817]	Loss 0.0253 (0.0085)	
training:	Epoch: [45][275/817]	Loss 0.0018 (0.0085)	
training:	Epoch: [45][276/817]	Loss 0.0042 (0.0085)	
training:	Epoch: [45][277/817]	Loss 0.0015 (0.0085)	
training:	Epoch: [45][278/817]	Loss 0.0013 (0.0084)	
training:	Epoch: [45][279/817]	Loss 0.0015 (0.0084)	
training:	Epoch: [45][280/817]	Loss 0.0051 (0.0084)	
training:	Epoch: [45][281/817]	Loss 0.0016 (0.0084)	
training:	Epoch: [45][282/817]	Loss 0.0015 (0.0083)	
training:	Epoch: [45][283/817]	Loss 0.3090 (0.0094)	
training:	Epoch: [45][284/817]	Loss 0.0016 (0.0094)	
training:	Epoch: [45][285/817]	Loss 0.0013 (0.0094)	
training:	Epoch: [45][286/817]	Loss 0.0057 (0.0093)	
training:	Epoch: [45][287/817]	Loss 0.0014 (0.0093)	
training:	Epoch: [45][288/817]	Loss 0.0014 (0.0093)	
training:	Epoch: [45][289/817]	Loss 0.0443 (0.0094)	
training:	Epoch: [45][290/817]	Loss 0.0014 (0.0094)	
training:	Epoch: [45][291/817]	Loss 0.0016 (0.0094)	
training:	Epoch: [45][292/817]	Loss 0.0022 (0.0093)	
training:	Epoch: [45][293/817]	Loss 0.0011 (0.0093)	
training:	Epoch: [45][294/817]	Loss 0.0018 (0.0093)	
training:	Epoch: [45][295/817]	Loss 0.0218 (0.0093)	
training:	Epoch: [45][296/817]	Loss 0.0012 (0.0093)	
training:	Epoch: [45][297/817]	Loss 0.0014 (0.0093)	
training:	Epoch: [45][298/817]	Loss 0.0394 (0.0094)	
training:	Epoch: [45][299/817]	Loss 0.0017 (0.0093)	
training:	Epoch: [45][300/817]	Loss 0.0029 (0.0093)	
training:	Epoch: [45][301/817]	Loss 0.0019 (0.0093)	
training:	Epoch: [45][302/817]	Loss 0.0032 (0.0093)	
training:	Epoch: [45][303/817]	Loss 0.0014 (0.0092)	
training:	Epoch: [45][304/817]	Loss 0.0013 (0.0092)	
training:	Epoch: [45][305/817]	Loss 0.0019 (0.0092)	
training:	Epoch: [45][306/817]	Loss 0.0056 (0.0092)	
training:	Epoch: [45][307/817]	Loss 0.0105 (0.0092)	
training:	Epoch: [45][308/817]	Loss 0.0016 (0.0092)	
training:	Epoch: [45][309/817]	Loss 0.0025 (0.0091)	
training:	Epoch: [45][310/817]	Loss 0.0338 (0.0092)	
training:	Epoch: [45][311/817]	Loss 0.0019 (0.0092)	
training:	Epoch: [45][312/817]	Loss 0.0016 (0.0092)	
training:	Epoch: [45][313/817]	Loss 0.0016 (0.0091)	
training:	Epoch: [45][314/817]	Loss 0.0016 (0.0091)	
training:	Epoch: [45][315/817]	Loss 0.0021 (0.0091)	
training:	Epoch: [45][316/817]	Loss 0.0016 (0.0091)	
training:	Epoch: [45][317/817]	Loss 0.0016 (0.0091)	
training:	Epoch: [45][318/817]	Loss 0.0016 (0.0090)	
training:	Epoch: [45][319/817]	Loss 0.0013 (0.0090)	
training:	Epoch: [45][320/817]	Loss 0.0012 (0.0090)	
training:	Epoch: [45][321/817]	Loss 0.0046 (0.0090)	
training:	Epoch: [45][322/817]	Loss 0.0204 (0.0090)	
training:	Epoch: [45][323/817]	Loss 0.0012 (0.0090)	
training:	Epoch: [45][324/817]	Loss 0.0029 (0.0090)	
training:	Epoch: [45][325/817]	Loss 0.0017 (0.0089)	
training:	Epoch: [45][326/817]	Loss 0.0043 (0.0089)	
training:	Epoch: [45][327/817]	Loss 0.0016 (0.0089)	
training:	Epoch: [45][328/817]	Loss 0.0014 (0.0089)	
training:	Epoch: [45][329/817]	Loss 0.0015 (0.0089)	
training:	Epoch: [45][330/817]	Loss 0.0015 (0.0088)	
training:	Epoch: [45][331/817]	Loss 0.0020 (0.0088)	
training:	Epoch: [45][332/817]	Loss 0.0040 (0.0088)	
training:	Epoch: [45][333/817]	Loss 0.0034 (0.0088)	
training:	Epoch: [45][334/817]	Loss 0.3211 (0.0097)	
training:	Epoch: [45][335/817]	Loss 0.0024 (0.0097)	
training:	Epoch: [45][336/817]	Loss 0.2713 (0.0105)	
training:	Epoch: [45][337/817]	Loss 0.0015 (0.0104)	
training:	Epoch: [45][338/817]	Loss 0.0070 (0.0104)	
training:	Epoch: [45][339/817]	Loss 0.0013 (0.0104)	
training:	Epoch: [45][340/817]	Loss 0.0013 (0.0104)	
training:	Epoch: [45][341/817]	Loss 0.0032 (0.0104)	
training:	Epoch: [45][342/817]	Loss 0.0015 (0.0103)	
training:	Epoch: [45][343/817]	Loss 0.0015 (0.0103)	
training:	Epoch: [45][344/817]	Loss 0.0014 (0.0103)	
training:	Epoch: [45][345/817]	Loss 0.0120 (0.0103)	
training:	Epoch: [45][346/817]	Loss 0.0023 (0.0103)	
training:	Epoch: [45][347/817]	Loss 0.0085 (0.0103)	
training:	Epoch: [45][348/817]	Loss 0.0266 (0.0103)	
training:	Epoch: [45][349/817]	Loss 0.1236 (0.0106)	
training:	Epoch: [45][350/817]	Loss 0.1624 (0.0111)	
training:	Epoch: [45][351/817]	Loss 0.0185 (0.0111)	
training:	Epoch: [45][352/817]	Loss 0.0017 (0.0111)	
training:	Epoch: [45][353/817]	Loss 0.0013 (0.0110)	
training:	Epoch: [45][354/817]	Loss 0.0012 (0.0110)	
training:	Epoch: [45][355/817]	Loss 0.0026 (0.0110)	
training:	Epoch: [45][356/817]	Loss 0.0013 (0.0110)	
training:	Epoch: [45][357/817]	Loss 0.0013 (0.0109)	
training:	Epoch: [45][358/817]	Loss 0.0063 (0.0109)	
training:	Epoch: [45][359/817]	Loss 0.0019 (0.0109)	
training:	Epoch: [45][360/817]	Loss 0.0020 (0.0109)	
training:	Epoch: [45][361/817]	Loss 0.0029 (0.0108)	
training:	Epoch: [45][362/817]	Loss 0.0112 (0.0108)	
training:	Epoch: [45][363/817]	Loss 0.0085 (0.0108)	
training:	Epoch: [45][364/817]	Loss 0.0023 (0.0108)	
training:	Epoch: [45][365/817]	Loss 0.0033 (0.0108)	
training:	Epoch: [45][366/817]	Loss 0.0015 (0.0108)	
training:	Epoch: [45][367/817]	Loss 0.0093 (0.0108)	
training:	Epoch: [45][368/817]	Loss 0.0018 (0.0107)	
training:	Epoch: [45][369/817]	Loss 0.0022 (0.0107)	
training:	Epoch: [45][370/817]	Loss 0.0033 (0.0107)	
training:	Epoch: [45][371/817]	Loss 0.0012 (0.0107)	
training:	Epoch: [45][372/817]	Loss 0.0014 (0.0106)	
training:	Epoch: [45][373/817]	Loss 0.0196 (0.0107)	
training:	Epoch: [45][374/817]	Loss 0.0019 (0.0106)	
training:	Epoch: [45][375/817]	Loss 0.0016 (0.0106)	
training:	Epoch: [45][376/817]	Loss 0.0174 (0.0106)	
training:	Epoch: [45][377/817]	Loss 0.0070 (0.0106)	
training:	Epoch: [45][378/817]	Loss 0.0034 (0.0106)	
training:	Epoch: [45][379/817]	Loss 0.0025 (0.0106)	
training:	Epoch: [45][380/817]	Loss 0.0013 (0.0106)	
training:	Epoch: [45][381/817]	Loss 0.0024 (0.0105)	
training:	Epoch: [45][382/817]	Loss 0.0015 (0.0105)	
training:	Epoch: [45][383/817]	Loss 0.0019 (0.0105)	
training:	Epoch: [45][384/817]	Loss 0.0019 (0.0105)	
training:	Epoch: [45][385/817]	Loss 0.0015 (0.0105)	
training:	Epoch: [45][386/817]	Loss 0.0041 (0.0104)	
training:	Epoch: [45][387/817]	Loss 0.0013 (0.0104)	
training:	Epoch: [45][388/817]	Loss 0.0014 (0.0104)	
training:	Epoch: [45][389/817]	Loss 0.0022 (0.0104)	
training:	Epoch: [45][390/817]	Loss 0.0645 (0.0105)	
training:	Epoch: [45][391/817]	Loss 0.0014 (0.0105)	
training:	Epoch: [45][392/817]	Loss 0.0022 (0.0105)	
training:	Epoch: [45][393/817]	Loss 0.2515 (0.0111)	
training:	Epoch: [45][394/817]	Loss 0.0019 (0.0111)	
training:	Epoch: [45][395/817]	Loss 0.0014 (0.0110)	
training:	Epoch: [45][396/817]	Loss 0.0013 (0.0110)	
training:	Epoch: [45][397/817]	Loss 0.0068 (0.0110)	
training:	Epoch: [45][398/817]	Loss 0.0018 (0.0110)	
training:	Epoch: [45][399/817]	Loss 0.0020 (0.0109)	
training:	Epoch: [45][400/817]	Loss 0.0018 (0.0109)	
training:	Epoch: [45][401/817]	Loss 0.0014 (0.0109)	
training:	Epoch: [45][402/817]	Loss 0.0018 (0.0109)	
training:	Epoch: [45][403/817]	Loss 0.0010 (0.0109)	
training:	Epoch: [45][404/817]	Loss 0.0015 (0.0108)	
training:	Epoch: [45][405/817]	Loss 0.0022 (0.0108)	
training:	Epoch: [45][406/817]	Loss 0.0019 (0.0108)	
training:	Epoch: [45][407/817]	Loss 0.0026 (0.0108)	
training:	Epoch: [45][408/817]	Loss 0.0012 (0.0107)	
training:	Epoch: [45][409/817]	Loss 0.0058 (0.0107)	
training:	Epoch: [45][410/817]	Loss 0.0202 (0.0108)	
training:	Epoch: [45][411/817]	Loss 0.0025 (0.0107)	
training:	Epoch: [45][412/817]	Loss 0.0012 (0.0107)	
training:	Epoch: [45][413/817]	Loss 0.0201 (0.0107)	
training:	Epoch: [45][414/817]	Loss 0.0012 (0.0107)	
training:	Epoch: [45][415/817]	Loss 0.0015 (0.0107)	
training:	Epoch: [45][416/817]	Loss 0.0011 (0.0107)	
training:	Epoch: [45][417/817]	Loss 0.0027 (0.0106)	
training:	Epoch: [45][418/817]	Loss 0.0013 (0.0106)	
training:	Epoch: [45][419/817]	Loss 0.0012 (0.0106)	
training:	Epoch: [45][420/817]	Loss 0.0019 (0.0106)	
training:	Epoch: [45][421/817]	Loss 0.0012 (0.0106)	
training:	Epoch: [45][422/817]	Loss 0.0022 (0.0105)	
training:	Epoch: [45][423/817]	Loss 0.0038 (0.0105)	
training:	Epoch: [45][424/817]	Loss 0.0012 (0.0105)	
training:	Epoch: [45][425/817]	Loss 0.0027 (0.0105)	
training:	Epoch: [45][426/817]	Loss 0.0016 (0.0105)	
training:	Epoch: [45][427/817]	Loss 0.0076 (0.0105)	
training:	Epoch: [45][428/817]	Loss 0.0016 (0.0104)	
training:	Epoch: [45][429/817]	Loss 0.0827 (0.0106)	
training:	Epoch: [45][430/817]	Loss 0.0011 (0.0106)	
training:	Epoch: [45][431/817]	Loss 0.0019 (0.0106)	
training:	Epoch: [45][432/817]	Loss 0.1501 (0.0109)	
training:	Epoch: [45][433/817]	Loss 0.0041 (0.0109)	
training:	Epoch: [45][434/817]	Loss 0.0017 (0.0108)	
training:	Epoch: [45][435/817]	Loss 0.0091 (0.0108)	
training:	Epoch: [45][436/817]	Loss 0.0660 (0.0110)	
training:	Epoch: [45][437/817]	Loss 0.0070 (0.0110)	
training:	Epoch: [45][438/817]	Loss 0.0018 (0.0109)	
training:	Epoch: [45][439/817]	Loss 0.0016 (0.0109)	
training:	Epoch: [45][440/817]	Loss 0.0013 (0.0109)	
training:	Epoch: [45][441/817]	Loss 0.0172 (0.0109)	
training:	Epoch: [45][442/817]	Loss 0.0014 (0.0109)	
training:	Epoch: [45][443/817]	Loss 0.0013 (0.0109)	
training:	Epoch: [45][444/817]	Loss 0.0029 (0.0109)	
training:	Epoch: [45][445/817]	Loss 0.0014 (0.0108)	
training:	Epoch: [45][446/817]	Loss 0.0022 (0.0108)	
training:	Epoch: [45][447/817]	Loss 0.0017 (0.0108)	
training:	Epoch: [45][448/817]	Loss 0.0025 (0.0108)	
training:	Epoch: [45][449/817]	Loss 0.0024 (0.0108)	
training:	Epoch: [45][450/817]	Loss 0.0448 (0.0108)	
training:	Epoch: [45][451/817]	Loss 0.0017 (0.0108)	
training:	Epoch: [45][452/817]	Loss 0.3042 (0.0115)	
training:	Epoch: [45][453/817]	Loss 0.0020 (0.0114)	
training:	Epoch: [45][454/817]	Loss 0.6432 (0.0128)	
training:	Epoch: [45][455/817]	Loss 0.0012 (0.0128)	
training:	Epoch: [45][456/817]	Loss 0.0016 (0.0128)	
training:	Epoch: [45][457/817]	Loss 0.0070 (0.0128)	
training:	Epoch: [45][458/817]	Loss 0.0274 (0.0128)	
training:	Epoch: [45][459/817]	Loss 0.0033 (0.0128)	
training:	Epoch: [45][460/817]	Loss 0.0014 (0.0128)	
training:	Epoch: [45][461/817]	Loss 0.0018 (0.0127)	
training:	Epoch: [45][462/817]	Loss 0.0060 (0.0127)	
training:	Epoch: [45][463/817]	Loss 0.0012 (0.0127)	
training:	Epoch: [45][464/817]	Loss 0.0027 (0.0127)	
training:	Epoch: [45][465/817]	Loss 0.0153 (0.0127)	
training:	Epoch: [45][466/817]	Loss 0.0012 (0.0126)	
training:	Epoch: [45][467/817]	Loss 0.0029 (0.0126)	
training:	Epoch: [45][468/817]	Loss 0.0013 (0.0126)	
training:	Epoch: [45][469/817]	Loss 0.0026 (0.0126)	
training:	Epoch: [45][470/817]	Loss 0.0041 (0.0126)	
training:	Epoch: [45][471/817]	Loss 0.0060 (0.0126)	
training:	Epoch: [45][472/817]	Loss 0.0012 (0.0125)	
training:	Epoch: [45][473/817]	Loss 0.0020 (0.0125)	
training:	Epoch: [45][474/817]	Loss 0.0011 (0.0125)	
training:	Epoch: [45][475/817]	Loss 0.0022 (0.0125)	
training:	Epoch: [45][476/817]	Loss 0.0014 (0.0124)	
training:	Epoch: [45][477/817]	Loss 0.0034 (0.0124)	
training:	Epoch: [45][478/817]	Loss 0.0016 (0.0124)	
training:	Epoch: [45][479/817]	Loss 0.0058 (0.0124)	
training:	Epoch: [45][480/817]	Loss 0.0103 (0.0124)	
training:	Epoch: [45][481/817]	Loss 0.0021 (0.0124)	
training:	Epoch: [45][482/817]	Loss 0.0018 (0.0123)	
training:	Epoch: [45][483/817]	Loss 0.0018 (0.0123)	
training:	Epoch: [45][484/817]	Loss 0.0299 (0.0123)	
training:	Epoch: [45][485/817]	Loss 0.0011 (0.0123)	
training:	Epoch: [45][486/817]	Loss 0.0013 (0.0123)	
training:	Epoch: [45][487/817]	Loss 0.0021 (0.0123)	
training:	Epoch: [45][488/817]	Loss 0.0017 (0.0123)	
training:	Epoch: [45][489/817]	Loss 0.0086 (0.0123)	
training:	Epoch: [45][490/817]	Loss 0.0031 (0.0122)	
training:	Epoch: [45][491/817]	Loss 0.0020 (0.0122)	
training:	Epoch: [45][492/817]	Loss 0.0014 (0.0122)	
training:	Epoch: [45][493/817]	Loss 0.0027 (0.0122)	
training:	Epoch: [45][494/817]	Loss 0.0012 (0.0121)	
training:	Epoch: [45][495/817]	Loss 0.0030 (0.0121)	
training:	Epoch: [45][496/817]	Loss 0.0050 (0.0121)	
training:	Epoch: [45][497/817]	Loss 0.0025 (0.0121)	
training:	Epoch: [45][498/817]	Loss 0.0027 (0.0121)	
training:	Epoch: [45][499/817]	Loss 0.0034 (0.0121)	
training:	Epoch: [45][500/817]	Loss 0.0014 (0.0120)	
training:	Epoch: [45][501/817]	Loss 0.0029 (0.0120)	
training:	Epoch: [45][502/817]	Loss 0.0137 (0.0120)	
training:	Epoch: [45][503/817]	Loss 0.0051 (0.0120)	
training:	Epoch: [45][504/817]	Loss 0.0025 (0.0120)	
training:	Epoch: [45][505/817]	Loss 0.0070 (0.0120)	
training:	Epoch: [45][506/817]	Loss 0.0115 (0.0120)	
training:	Epoch: [45][507/817]	Loss 0.0012 (0.0120)	
training:	Epoch: [45][508/817]	Loss 0.0016 (0.0119)	
training:	Epoch: [45][509/817]	Loss 0.0024 (0.0119)	
training:	Epoch: [45][510/817]	Loss 0.0252 (0.0119)	
training:	Epoch: [45][511/817]	Loss 0.0011 (0.0119)	
training:	Epoch: [45][512/817]	Loss 0.0035 (0.0119)	
training:	Epoch: [45][513/817]	Loss 0.0024 (0.0119)	
training:	Epoch: [45][514/817]	Loss 0.0013 (0.0119)	
training:	Epoch: [45][515/817]	Loss 0.0014 (0.0118)	
training:	Epoch: [45][516/817]	Loss 0.0019 (0.0118)	
training:	Epoch: [45][517/817]	Loss 0.0092 (0.0118)	
training:	Epoch: [45][518/817]	Loss 0.0019 (0.0118)	
training:	Epoch: [45][519/817]	Loss 0.1228 (0.0120)	
training:	Epoch: [45][520/817]	Loss 0.0020 (0.0120)	
training:	Epoch: [45][521/817]	Loss 0.0016 (0.0120)	
training:	Epoch: [45][522/817]	Loss 0.0014 (0.0120)	
training:	Epoch: [45][523/817]	Loss 0.0014 (0.0119)	
training:	Epoch: [45][524/817]	Loss 0.0020 (0.0119)	
training:	Epoch: [45][525/817]	Loss 0.0015 (0.0119)	
training:	Epoch: [45][526/817]	Loss 0.0012 (0.0119)	
training:	Epoch: [45][527/817]	Loss 0.0020 (0.0119)	
training:	Epoch: [45][528/817]	Loss 0.0011 (0.0118)	
training:	Epoch: [45][529/817]	Loss 0.0031 (0.0118)	
training:	Epoch: [45][530/817]	Loss 0.0015 (0.0118)	
training:	Epoch: [45][531/817]	Loss 0.0020 (0.0118)	
training:	Epoch: [45][532/817]	Loss 0.0013 (0.0118)	
training:	Epoch: [45][533/817]	Loss 0.0014 (0.0117)	
training:	Epoch: [45][534/817]	Loss 0.0031 (0.0117)	
training:	Epoch: [45][535/817]	Loss 0.0011 (0.0117)	
training:	Epoch: [45][536/817]	Loss 0.0059 (0.0117)	
training:	Epoch: [45][537/817]	Loss 0.0019 (0.0117)	
training:	Epoch: [45][538/817]	Loss 0.0011 (0.0117)	
training:	Epoch: [45][539/817]	Loss 0.0658 (0.0118)	
training:	Epoch: [45][540/817]	Loss 0.0017 (0.0117)	
training:	Epoch: [45][541/817]	Loss 0.0020 (0.0117)	
training:	Epoch: [45][542/817]	Loss 0.0013 (0.0117)	
training:	Epoch: [45][543/817]	Loss 0.0039 (0.0117)	
training:	Epoch: [45][544/817]	Loss 0.0017 (0.0117)	
training:	Epoch: [45][545/817]	Loss 0.0012 (0.0117)	
training:	Epoch: [45][546/817]	Loss 0.0020 (0.0116)	
training:	Epoch: [45][547/817]	Loss 0.0011 (0.0116)	
training:	Epoch: [45][548/817]	Loss 0.0019 (0.0116)	
training:	Epoch: [45][549/817]	Loss 0.0254 (0.0116)	
training:	Epoch: [45][550/817]	Loss 0.0012 (0.0116)	
training:	Epoch: [45][551/817]	Loss 0.0037 (0.0116)	
training:	Epoch: [45][552/817]	Loss 0.0084 (0.0116)	
training:	Epoch: [45][553/817]	Loss 0.0020 (0.0116)	
training:	Epoch: [45][554/817]	Loss 0.0012 (0.0115)	
training:	Epoch: [45][555/817]	Loss 0.0013 (0.0115)	
training:	Epoch: [45][556/817]	Loss 0.0028 (0.0115)	
training:	Epoch: [45][557/817]	Loss 0.0272 (0.0115)	
training:	Epoch: [45][558/817]	Loss 0.0020 (0.0115)	
training:	Epoch: [45][559/817]	Loss 0.0012 (0.0115)	
training:	Epoch: [45][560/817]	Loss 0.0017 (0.0115)	
training:	Epoch: [45][561/817]	Loss 0.0051 (0.0115)	
training:	Epoch: [45][562/817]	Loss 0.0021 (0.0115)	
training:	Epoch: [45][563/817]	Loss 0.0036 (0.0114)	
training:	Epoch: [45][564/817]	Loss 0.0021 (0.0114)	
training:	Epoch: [45][565/817]	Loss 0.0012 (0.0114)	
training:	Epoch: [45][566/817]	Loss 0.0011 (0.0114)	
training:	Epoch: [45][567/817]	Loss 0.0013 (0.0114)	
training:	Epoch: [45][568/817]	Loss 0.0176 (0.0114)	
training:	Epoch: [45][569/817]	Loss 0.0065 (0.0114)	
training:	Epoch: [45][570/817]	Loss 0.0011 (0.0114)	
training:	Epoch: [45][571/817]	Loss 0.0013 (0.0113)	
training:	Epoch: [45][572/817]	Loss 0.0033 (0.0113)	
training:	Epoch: [45][573/817]	Loss 0.0012 (0.0113)	
training:	Epoch: [45][574/817]	Loss 0.0010 (0.0113)	
training:	Epoch: [45][575/817]	Loss 0.0018 (0.0113)	
training:	Epoch: [45][576/817]	Loss 0.0010 (0.0113)	
training:	Epoch: [45][577/817]	Loss 0.0013 (0.0112)	
training:	Epoch: [45][578/817]	Loss 0.0012 (0.0112)	
training:	Epoch: [45][579/817]	Loss 0.0011 (0.0112)	
training:	Epoch: [45][580/817]	Loss 0.0015 (0.0112)	
training:	Epoch: [45][581/817]	Loss 0.0013 (0.0112)	
training:	Epoch: [45][582/817]	Loss 0.0018 (0.0112)	
training:	Epoch: [45][583/817]	Loss 0.0017 (0.0111)	
training:	Epoch: [45][584/817]	Loss 0.0016 (0.0111)	
training:	Epoch: [45][585/817]	Loss 0.0012 (0.0111)	
training:	Epoch: [45][586/817]	Loss 0.0011 (0.0111)	
training:	Epoch: [45][587/817]	Loss 0.0017 (0.0111)	
training:	Epoch: [45][588/817]	Loss 0.0013 (0.0111)	
training:	Epoch: [45][589/817]	Loss 0.0099 (0.0111)	
training:	Epoch: [45][590/817]	Loss 0.0013 (0.0110)	
training:	Epoch: [45][591/817]	Loss 0.0017 (0.0110)	
training:	Epoch: [45][592/817]	Loss 0.0012 (0.0110)	
training:	Epoch: [45][593/817]	Loss 0.0015 (0.0110)	
training:	Epoch: [45][594/817]	Loss 0.0014 (0.0110)	
training:	Epoch: [45][595/817]	Loss 0.0015 (0.0110)	
training:	Epoch: [45][596/817]	Loss 0.0238 (0.0110)	
training:	Epoch: [45][597/817]	Loss 0.0014 (0.0110)	
training:	Epoch: [45][598/817]	Loss 0.0016 (0.0109)	
training:	Epoch: [45][599/817]	Loss 0.0025 (0.0109)	
training:	Epoch: [45][600/817]	Loss 0.0018 (0.0109)	
training:	Epoch: [45][601/817]	Loss 0.0066 (0.0109)	
training:	Epoch: [45][602/817]	Loss 0.0014 (0.0109)	
training:	Epoch: [45][603/817]	Loss 0.0015 (0.0109)	
training:	Epoch: [45][604/817]	Loss 0.0011 (0.0109)	
training:	Epoch: [45][605/817]	Loss 0.0018 (0.0109)	
training:	Epoch: [45][606/817]	Loss 0.0019 (0.0108)	
training:	Epoch: [45][607/817]	Loss 0.0058 (0.0108)	
training:	Epoch: [45][608/817]	Loss 0.0014 (0.0108)	
training:	Epoch: [45][609/817]	Loss 0.0022 (0.0108)	
training:	Epoch: [45][610/817]	Loss 0.0012 (0.0108)	
training:	Epoch: [45][611/817]	Loss 0.0019 (0.0108)	
training:	Epoch: [45][612/817]	Loss 0.0012 (0.0108)	
training:	Epoch: [45][613/817]	Loss 0.0034 (0.0107)	
training:	Epoch: [45][614/817]	Loss 0.0271 (0.0108)	
training:	Epoch: [45][615/817]	Loss 0.0017 (0.0108)	
training:	Epoch: [45][616/817]	Loss 0.0016 (0.0107)	
training:	Epoch: [45][617/817]	Loss 0.0014 (0.0107)	
training:	Epoch: [45][618/817]	Loss 0.0074 (0.0107)	
training:	Epoch: [45][619/817]	Loss 0.0019 (0.0107)	
training:	Epoch: [45][620/817]	Loss 0.0012 (0.0107)	
training:	Epoch: [45][621/817]	Loss 0.0018 (0.0107)	
training:	Epoch: [45][622/817]	Loss 0.0097 (0.0107)	
training:	Epoch: [45][623/817]	Loss 0.0012 (0.0107)	
training:	Epoch: [45][624/817]	Loss 0.0021 (0.0106)	
training:	Epoch: [45][625/817]	Loss 0.0125 (0.0106)	
training:	Epoch: [45][626/817]	Loss 0.0014 (0.0106)	
training:	Epoch: [45][627/817]	Loss 0.0012 (0.0106)	
training:	Epoch: [45][628/817]	Loss 0.1831 (0.0109)	
training:	Epoch: [45][629/817]	Loss 0.0051 (0.0109)	
training:	Epoch: [45][630/817]	Loss 0.0011 (0.0109)	
training:	Epoch: [45][631/817]	Loss 0.0023 (0.0109)	
training:	Epoch: [45][632/817]	Loss 0.0030 (0.0108)	
training:	Epoch: [45][633/817]	Loss 0.0020 (0.0108)	
training:	Epoch: [45][634/817]	Loss 0.0016 (0.0108)	
training:	Epoch: [45][635/817]	Loss 0.0017 (0.0108)	
training:	Epoch: [45][636/817]	Loss 0.0903 (0.0109)	
training:	Epoch: [45][637/817]	Loss 0.0025 (0.0109)	
training:	Epoch: [45][638/817]	Loss 0.1991 (0.0112)	
training:	Epoch: [45][639/817]	Loss 0.0014 (0.0112)	
training:	Epoch: [45][640/817]	Loss 0.0015 (0.0112)	
training:	Epoch: [45][641/817]	Loss 0.0022 (0.0112)	
training:	Epoch: [45][642/817]	Loss 0.0012 (0.0111)	
training:	Epoch: [45][643/817]	Loss 0.0032 (0.0111)	
training:	Epoch: [45][644/817]	Loss 0.0015 (0.0111)	
training:	Epoch: [45][645/817]	Loss 0.0037 (0.0111)	
training:	Epoch: [45][646/817]	Loss 0.0021 (0.0111)	
training:	Epoch: [45][647/817]	Loss 0.0020 (0.0111)	
training:	Epoch: [45][648/817]	Loss 0.0248 (0.0111)	
training:	Epoch: [45][649/817]	Loss 0.0021 (0.0111)	
training:	Epoch: [45][650/817]	Loss 0.0019 (0.0111)	
training:	Epoch: [45][651/817]	Loss 0.0014 (0.0111)	
training:	Epoch: [45][652/817]	Loss 0.0013 (0.0110)	
training:	Epoch: [45][653/817]	Loss 0.0034 (0.0110)	
training:	Epoch: [45][654/817]	Loss 0.0017 (0.0110)	
training:	Epoch: [45][655/817]	Loss 0.0030 (0.0110)	
training:	Epoch: [45][656/817]	Loss 0.0035 (0.0110)	
training:	Epoch: [45][657/817]	Loss 0.0013 (0.0110)	
training:	Epoch: [45][658/817]	Loss 0.0013 (0.0110)	
training:	Epoch: [45][659/817]	Loss 0.0014 (0.0109)	
training:	Epoch: [45][660/817]	Loss 0.0023 (0.0109)	
training:	Epoch: [45][661/817]	Loss 0.0032 (0.0109)	
training:	Epoch: [45][662/817]	Loss 0.0125 (0.0109)	
training:	Epoch: [45][663/817]	Loss 0.0017 (0.0109)	
training:	Epoch: [45][664/817]	Loss 0.0015 (0.0109)	
training:	Epoch: [45][665/817]	Loss 0.0056 (0.0109)	
training:	Epoch: [45][666/817]	Loss 0.0011 (0.0109)	
training:	Epoch: [45][667/817]	Loss 0.0012 (0.0109)	
training:	Epoch: [45][668/817]	Loss 0.0012 (0.0108)	
training:	Epoch: [45][669/817]	Loss 0.0011 (0.0108)	
training:	Epoch: [45][670/817]	Loss 0.0011 (0.0108)	
training:	Epoch: [45][671/817]	Loss 0.0017 (0.0108)	
training:	Epoch: [45][672/817]	Loss 0.0058 (0.0108)	
training:	Epoch: [45][673/817]	Loss 0.0012 (0.0108)	
training:	Epoch: [45][674/817]	Loss 0.0012 (0.0108)	
training:	Epoch: [45][675/817]	Loss 0.0013 (0.0108)	
training:	Epoch: [45][676/817]	Loss 0.0014 (0.0107)	
training:	Epoch: [45][677/817]	Loss 0.0017 (0.0107)	
training:	Epoch: [45][678/817]	Loss 0.0016 (0.0107)	
training:	Epoch: [45][679/817]	Loss 0.0014 (0.0107)	
training:	Epoch: [45][680/817]	Loss 0.0014 (0.0107)	
training:	Epoch: [45][681/817]	Loss 0.0011 (0.0107)	
training:	Epoch: [45][682/817]	Loss 0.0674 (0.0108)	
training:	Epoch: [45][683/817]	Loss 0.0014 (0.0107)	
training:	Epoch: [45][684/817]	Loss 0.0013 (0.0107)	
training:	Epoch: [45][685/817]	Loss 0.0011 (0.0107)	
training:	Epoch: [45][686/817]	Loss 0.0020 (0.0107)	
training:	Epoch: [45][687/817]	Loss 0.0017 (0.0107)	
training:	Epoch: [45][688/817]	Loss 0.0013 (0.0107)	
training:	Epoch: [45][689/817]	Loss 0.0013 (0.0107)	
training:	Epoch: [45][690/817]	Loss 0.0019 (0.0106)	
training:	Epoch: [45][691/817]	Loss 0.0015 (0.0106)	
training:	Epoch: [45][692/817]	Loss 0.0226 (0.0106)	
training:	Epoch: [45][693/817]	Loss 0.0014 (0.0106)	
training:	Epoch: [45][694/817]	Loss 0.0016 (0.0106)	
training:	Epoch: [45][695/817]	Loss 0.0030 (0.0106)	
training:	Epoch: [45][696/817]	Loss 0.0014 (0.0106)	
training:	Epoch: [45][697/817]	Loss 0.0014 (0.0106)	
training:	Epoch: [45][698/817]	Loss 0.0012 (0.0106)	
training:	Epoch: [45][699/817]	Loss 0.0013 (0.0106)	
training:	Epoch: [45][700/817]	Loss 0.0033 (0.0105)	
training:	Epoch: [45][701/817]	Loss 0.0024 (0.0105)	
training:	Epoch: [45][702/817]	Loss 0.0028 (0.0105)	
training:	Epoch: [45][703/817]	Loss 0.0011 (0.0105)	
training:	Epoch: [45][704/817]	Loss 0.0013 (0.0105)	
training:	Epoch: [45][705/817]	Loss 0.0015 (0.0105)	
training:	Epoch: [45][706/817]	Loss 0.0011 (0.0105)	
training:	Epoch: [45][707/817]	Loss 0.0012 (0.0105)	
training:	Epoch: [45][708/817]	Loss 0.0017 (0.0104)	
training:	Epoch: [45][709/817]	Loss 0.0017 (0.0104)	
training:	Epoch: [45][710/817]	Loss 0.0019 (0.0104)	
training:	Epoch: [45][711/817]	Loss 0.0012 (0.0104)	
training:	Epoch: [45][712/817]	Loss 0.0012 (0.0104)	
training:	Epoch: [45][713/817]	Loss 0.0010 (0.0104)	
training:	Epoch: [45][714/817]	Loss 0.0017 (0.0104)	
training:	Epoch: [45][715/817]	Loss 0.0019 (0.0104)	
training:	Epoch: [45][716/817]	Loss 0.0016 (0.0103)	
training:	Epoch: [45][717/817]	Loss 0.0014 (0.0103)	
training:	Epoch: [45][718/817]	Loss 0.0012 (0.0103)	
training:	Epoch: [45][719/817]	Loss 0.0011 (0.0103)	
training:	Epoch: [45][720/817]	Loss 0.0024 (0.0103)	
training:	Epoch: [45][721/817]	Loss 0.0014 (0.0103)	
training:	Epoch: [45][722/817]	Loss 0.0011 (0.0103)	
training:	Epoch: [45][723/817]	Loss 0.0055 (0.0103)	
training:	Epoch: [45][724/817]	Loss 0.0011 (0.0103)	
training:	Epoch: [45][725/817]	Loss 0.0015 (0.0102)	
training:	Epoch: [45][726/817]	Loss 0.0013 (0.0102)	
training:	Epoch: [45][727/817]	Loss 0.0012 (0.0102)	
training:	Epoch: [45][728/817]	Loss 0.0037 (0.0102)	
training:	Epoch: [45][729/817]	Loss 0.0017 (0.0102)	
training:	Epoch: [45][730/817]	Loss 0.0013 (0.0102)	
training:	Epoch: [45][731/817]	Loss 0.0013 (0.0102)	
training:	Epoch: [45][732/817]	Loss 0.0010 (0.0102)	
training:	Epoch: [45][733/817]	Loss 0.0030 (0.0102)	
training:	Epoch: [45][734/817]	Loss 0.0016 (0.0101)	
training:	Epoch: [45][735/817]	Loss 0.0010 (0.0101)	
training:	Epoch: [45][736/817]	Loss 0.0012 (0.0101)	
training:	Epoch: [45][737/817]	Loss 0.0013 (0.0101)	
training:	Epoch: [45][738/817]	Loss 0.0014 (0.0101)	
training:	Epoch: [45][739/817]	Loss 0.0028 (0.0101)	
training:	Epoch: [45][740/817]	Loss 0.0036 (0.0101)	
training:	Epoch: [45][741/817]	Loss 0.0028 (0.0101)	
training:	Epoch: [45][742/817]	Loss 0.0014 (0.0101)	
training:	Epoch: [45][743/817]	Loss 0.0011 (0.0100)	
training:	Epoch: [45][744/817]	Loss 0.0023 (0.0100)	
training:	Epoch: [45][745/817]	Loss 0.0013 (0.0100)	
training:	Epoch: [45][746/817]	Loss 0.0019 (0.0100)	
training:	Epoch: [45][747/817]	Loss 0.0015 (0.0100)	
training:	Epoch: [45][748/817]	Loss 0.0011 (0.0100)	
training:	Epoch: [45][749/817]	Loss 0.0012 (0.0100)	
training:	Epoch: [45][750/817]	Loss 0.0013 (0.0100)	
training:	Epoch: [45][751/817]	Loss 0.0011 (0.0099)	
training:	Epoch: [45][752/817]	Loss 0.0025 (0.0099)	
training:	Epoch: [45][753/817]	Loss 0.0014 (0.0099)	
training:	Epoch: [45][754/817]	Loss 0.0015 (0.0099)	
training:	Epoch: [45][755/817]	Loss 0.0013 (0.0099)	
training:	Epoch: [45][756/817]	Loss 0.0010 (0.0099)	
training:	Epoch: [45][757/817]	Loss 0.0014 (0.0099)	
training:	Epoch: [45][758/817]	Loss 0.0016 (0.0099)	
training:	Epoch: [45][759/817]	Loss 0.0011 (0.0099)	
training:	Epoch: [45][760/817]	Loss 0.0011 (0.0098)	
training:	Epoch: [45][761/817]	Loss 0.0019 (0.0098)	
training:	Epoch: [45][762/817]	Loss 0.0015 (0.0098)	
training:	Epoch: [45][763/817]	Loss 0.0027 (0.0098)	
training:	Epoch: [45][764/817]	Loss 0.0018 (0.0098)	
training:	Epoch: [45][765/817]	Loss 0.0014 (0.0098)	
training:	Epoch: [45][766/817]	Loss 0.0035 (0.0098)	
training:	Epoch: [45][767/817]	Loss 0.0012 (0.0098)	
training:	Epoch: [45][768/817]	Loss 0.0027 (0.0098)	
training:	Epoch: [45][769/817]	Loss 0.0013 (0.0098)	
training:	Epoch: [45][770/817]	Loss 0.0010 (0.0097)	
training:	Epoch: [45][771/817]	Loss 0.0012 (0.0097)	
training:	Epoch: [45][772/817]	Loss 0.0013 (0.0097)	
training:	Epoch: [45][773/817]	Loss 0.0022 (0.0097)	
training:	Epoch: [45][774/817]	Loss 0.0011 (0.0097)	
training:	Epoch: [45][775/817]	Loss 0.0014 (0.0097)	
training:	Epoch: [45][776/817]	Loss 0.0011 (0.0097)	
training:	Epoch: [45][777/817]	Loss 0.0012 (0.0097)	
training:	Epoch: [45][778/817]	Loss 0.0016 (0.0097)	
training:	Epoch: [45][779/817]	Loss 0.0018 (0.0096)	
training:	Epoch: [45][780/817]	Loss 0.0028 (0.0096)	
training:	Epoch: [45][781/817]	Loss 0.0224 (0.0097)	
training:	Epoch: [45][782/817]	Loss 0.0014 (0.0096)	
training:	Epoch: [45][783/817]	Loss 0.0021 (0.0096)	
training:	Epoch: [45][784/817]	Loss 0.0012 (0.0096)	
training:	Epoch: [45][785/817]	Loss 0.0011 (0.0096)	
training:	Epoch: [45][786/817]	Loss 0.0062 (0.0096)	
training:	Epoch: [45][787/817]	Loss 0.0014 (0.0096)	
training:	Epoch: [45][788/817]	Loss 0.0011 (0.0096)	
training:	Epoch: [45][789/817]	Loss 0.0021 (0.0096)	
training:	Epoch: [45][790/817]	Loss 0.3699 (0.0100)	
training:	Epoch: [45][791/817]	Loss 0.0011 (0.0100)	
training:	Epoch: [45][792/817]	Loss 0.0018 (0.0100)	
training:	Epoch: [45][793/817]	Loss 0.0013 (0.0100)	
training:	Epoch: [45][794/817]	Loss 0.0206 (0.0100)	
training:	Epoch: [45][795/817]	Loss 0.0316 (0.0100)	
training:	Epoch: [45][796/817]	Loss 0.0279 (0.0101)	
training:	Epoch: [45][797/817]	Loss 0.0013 (0.0101)	
training:	Epoch: [45][798/817]	Loss 0.0019 (0.0100)	
training:	Epoch: [45][799/817]	Loss 0.0017 (0.0100)	
training:	Epoch: [45][800/817]	Loss 0.0027 (0.0100)	
training:	Epoch: [45][801/817]	Loss 0.0015 (0.0100)	
training:	Epoch: [45][802/817]	Loss 0.0829 (0.0101)	
training:	Epoch: [45][803/817]	Loss 0.0013 (0.0101)	
training:	Epoch: [45][804/817]	Loss 0.0017 (0.0101)	
training:	Epoch: [45][805/817]	Loss 0.0009 (0.0101)	
training:	Epoch: [45][806/817]	Loss 0.0026 (0.0101)	
training:	Epoch: [45][807/817]	Loss 0.0011 (0.0101)	
training:	Epoch: [45][808/817]	Loss 0.0053 (0.0100)	
training:	Epoch: [45][809/817]	Loss 0.0018 (0.0100)	
training:	Epoch: [45][810/817]	Loss 0.0018 (0.0100)	
training:	Epoch: [45][811/817]	Loss 0.0035 (0.0100)	
training:	Epoch: [45][812/817]	Loss 0.0012 (0.0100)	
training:	Epoch: [45][813/817]	Loss 0.0018 (0.0100)	
training:	Epoch: [45][814/817]	Loss 0.0023 (0.0100)	
training:	Epoch: [45][815/817]	Loss 0.0044 (0.0100)	
training:	Epoch: [45][816/817]	Loss 0.0018 (0.0100)	
training:	Epoch: [45][817/817]	Loss 0.0012 (0.0100)	
Training:	 Loss: 0.0100

Training:	 ACC: 0.9998 0.9998 1.0000 0.9997
Validation:	 ACC: 0.7884 0.7881 0.7820 0.7948
Validation:	 Best_BACC: 0.8223 0.8218 0.8127 0.8318
Validation:	 Loss: 1.0753
Pretraining:	Epoch 46/200
----------
training:	Epoch: [46][1/817]	Loss 0.0020 (0.0020)	
training:	Epoch: [46][2/817]	Loss 0.0016 (0.0018)	
training:	Epoch: [46][3/817]	Loss 0.0011 (0.0016)	
training:	Epoch: [46][4/817]	Loss 0.0013 (0.0015)	
training:	Epoch: [46][5/817]	Loss 0.0017 (0.0016)	
training:	Epoch: [46][6/817]	Loss 0.0012 (0.0015)	
training:	Epoch: [46][7/817]	Loss 0.0015 (0.0015)	
training:	Epoch: [46][8/817]	Loss 0.0264 (0.0046)	
training:	Epoch: [46][9/817]	Loss 0.0011 (0.0042)	
training:	Epoch: [46][10/817]	Loss 0.0011 (0.0039)	
training:	Epoch: [46][11/817]	Loss 0.0013 (0.0037)	
training:	Epoch: [46][12/817]	Loss 0.0013 (0.0035)	
training:	Epoch: [46][13/817]	Loss 0.0010 (0.0033)	
training:	Epoch: [46][14/817]	Loss 0.0027 (0.0032)	
training:	Epoch: [46][15/817]	Loss 0.0015 (0.0031)	
training:	Epoch: [46][16/817]	Loss 0.0015 (0.0030)	
training:	Epoch: [46][17/817]	Loss 0.0043 (0.0031)	
training:	Epoch: [46][18/817]	Loss 0.0013 (0.0030)	
training:	Epoch: [46][19/817]	Loss 0.0047 (0.0031)	
training:	Epoch: [46][20/817]	Loss 0.0010 (0.0030)	
training:	Epoch: [46][21/817]	Loss 0.0011 (0.0029)	
training:	Epoch: [46][22/817]	Loss 0.0016 (0.0028)	
training:	Epoch: [46][23/817]	Loss 0.0111 (0.0032)	
training:	Epoch: [46][24/817]	Loss 0.0016 (0.0031)	
training:	Epoch: [46][25/817]	Loss 0.0013 (0.0031)	
training:	Epoch: [46][26/817]	Loss 0.0108 (0.0034)	
training:	Epoch: [46][27/817]	Loss 0.0033 (0.0034)	
training:	Epoch: [46][28/817]	Loss 0.0037 (0.0034)	
training:	Epoch: [46][29/817]	Loss 0.0011 (0.0033)	
training:	Epoch: [46][30/817]	Loss 0.0039 (0.0033)	
training:	Epoch: [46][31/817]	Loss 0.0021 (0.0033)	
training:	Epoch: [46][32/817]	Loss 0.0012 (0.0032)	
training:	Epoch: [46][33/817]	Loss 0.0010 (0.0031)	
training:	Epoch: [46][34/817]	Loss 0.0013 (0.0031)	
training:	Epoch: [46][35/817]	Loss 0.0014 (0.0030)	
training:	Epoch: [46][36/817]	Loss 0.0083 (0.0032)	
training:	Epoch: [46][37/817]	Loss 0.0141 (0.0035)	
training:	Epoch: [46][38/817]	Loss 0.0110 (0.0037)	
training:	Epoch: [46][39/817]	Loss 0.0053 (0.0037)	
training:	Epoch: [46][40/817]	Loss 0.0041 (0.0037)	
training:	Epoch: [46][41/817]	Loss 0.0020 (0.0037)	
training:	Epoch: [46][42/817]	Loss 0.0013 (0.0036)	
training:	Epoch: [46][43/817]	Loss 0.0015 (0.0036)	
training:	Epoch: [46][44/817]	Loss 0.0593 (0.0048)	
training:	Epoch: [46][45/817]	Loss 0.0015 (0.0048)	
training:	Epoch: [46][46/817]	Loss 0.0011 (0.0047)	
training:	Epoch: [46][47/817]	Loss 0.0021 (0.0046)	
training:	Epoch: [46][48/817]	Loss 0.0014 (0.0046)	
training:	Epoch: [46][49/817]	Loss 0.0034 (0.0045)	
training:	Epoch: [46][50/817]	Loss 0.0015 (0.0045)	
training:	Epoch: [46][51/817]	Loss 0.0010 (0.0044)	
training:	Epoch: [46][52/817]	Loss 0.0016 (0.0044)	
training:	Epoch: [46][53/817]	Loss 0.0142 (0.0045)	
training:	Epoch: [46][54/817]	Loss 0.0312 (0.0050)	
training:	Epoch: [46][55/817]	Loss 0.0026 (0.0050)	
training:	Epoch: [46][56/817]	Loss 0.0016 (0.0049)	
training:	Epoch: [46][57/817]	Loss 0.0012 (0.0049)	
training:	Epoch: [46][58/817]	Loss 0.0013 (0.0048)	
training:	Epoch: [46][59/817]	Loss 0.0015 (0.0048)	
training:	Epoch: [46][60/817]	Loss 0.0039 (0.0047)	
training:	Epoch: [46][61/817]	Loss 0.0716 (0.0058)	
training:	Epoch: [46][62/817]	Loss 0.0013 (0.0058)	
training:	Epoch: [46][63/817]	Loss 0.0010 (0.0057)	
training:	Epoch: [46][64/817]	Loss 0.0018 (0.0056)	
training:	Epoch: [46][65/817]	Loss 0.0080 (0.0057)	
training:	Epoch: [46][66/817]	Loss 0.0015 (0.0056)	
training:	Epoch: [46][67/817]	Loss 0.0015 (0.0055)	
training:	Epoch: [46][68/817]	Loss 0.0013 (0.0055)	
training:	Epoch: [46][69/817]	Loss 0.0014 (0.0054)	
training:	Epoch: [46][70/817]	Loss 0.0036 (0.0054)	
training:	Epoch: [46][71/817]	Loss 0.0015 (0.0053)	
training:	Epoch: [46][72/817]	Loss 0.0029 (0.0053)	
training:	Epoch: [46][73/817]	Loss 0.0010 (0.0052)	
training:	Epoch: [46][74/817]	Loss 0.0015 (0.0052)	
training:	Epoch: [46][75/817]	Loss 0.0010 (0.0051)	
training:	Epoch: [46][76/817]	Loss 0.0017 (0.0051)	
training:	Epoch: [46][77/817]	Loss 0.0015 (0.0050)	
training:	Epoch: [46][78/817]	Loss 0.0015 (0.0050)	
training:	Epoch: [46][79/817]	Loss 0.0013 (0.0050)	
training:	Epoch: [46][80/817]	Loss 0.0129 (0.0051)	
training:	Epoch: [46][81/817]	Loss 0.0016 (0.0050)	
training:	Epoch: [46][82/817]	Loss 0.0020 (0.0050)	
training:	Epoch: [46][83/817]	Loss 0.0013 (0.0049)	
training:	Epoch: [46][84/817]	Loss 0.0022 (0.0049)	
training:	Epoch: [46][85/817]	Loss 0.0015 (0.0049)	
training:	Epoch: [46][86/817]	Loss 0.0021 (0.0048)	
training:	Epoch: [46][87/817]	Loss 0.0012 (0.0048)	
training:	Epoch: [46][88/817]	Loss 0.0022 (0.0048)	
training:	Epoch: [46][89/817]	Loss 0.0015 (0.0047)	
training:	Epoch: [46][90/817]	Loss 0.0013 (0.0047)	
training:	Epoch: [46][91/817]	Loss 0.0014 (0.0046)	
training:	Epoch: [46][92/817]	Loss 0.0015 (0.0046)	
training:	Epoch: [46][93/817]	Loss 0.0159 (0.0047)	
training:	Epoch: [46][94/817]	Loss 0.0033 (0.0047)	
training:	Epoch: [46][95/817]	Loss 0.0033 (0.0047)	
training:	Epoch: [46][96/817]	Loss 0.0016 (0.0047)	
training:	Epoch: [46][97/817]	Loss 0.0011 (0.0046)	
training:	Epoch: [46][98/817]	Loss 0.0127 (0.0047)	
training:	Epoch: [46][99/817]	Loss 0.0013 (0.0047)	
training:	Epoch: [46][100/817]	Loss 0.0015 (0.0046)	
training:	Epoch: [46][101/817]	Loss 0.0022 (0.0046)	
training:	Epoch: [46][102/817]	Loss 0.0011 (0.0046)	
training:	Epoch: [46][103/817]	Loss 0.0041 (0.0046)	
training:	Epoch: [46][104/817]	Loss 0.0013 (0.0046)	
training:	Epoch: [46][105/817]	Loss 0.0012 (0.0045)	
training:	Epoch: [46][106/817]	Loss 0.0017 (0.0045)	
training:	Epoch: [46][107/817]	Loss 0.0012 (0.0045)	
training:	Epoch: [46][108/817]	Loss 0.0012 (0.0044)	
training:	Epoch: [46][109/817]	Loss 0.0013 (0.0044)	
training:	Epoch: [46][110/817]	Loss 0.0016 (0.0044)	
training:	Epoch: [46][111/817]	Loss 0.0016 (0.0044)	
training:	Epoch: [46][112/817]	Loss 0.0023 (0.0043)	
training:	Epoch: [46][113/817]	Loss 0.0014 (0.0043)	
training:	Epoch: [46][114/817]	Loss 0.0019 (0.0043)	
training:	Epoch: [46][115/817]	Loss 0.0013 (0.0043)	
training:	Epoch: [46][116/817]	Loss 0.0010 (0.0042)	
training:	Epoch: [46][117/817]	Loss 0.0021 (0.0042)	
training:	Epoch: [46][118/817]	Loss 0.0045 (0.0042)	
training:	Epoch: [46][119/817]	Loss 0.0013 (0.0042)	
training:	Epoch: [46][120/817]	Loss 0.0010 (0.0042)	
training:	Epoch: [46][121/817]	Loss 0.0009 (0.0041)	
training:	Epoch: [46][122/817]	Loss 0.0040 (0.0041)	
training:	Epoch: [46][123/817]	Loss 0.0012 (0.0041)	
training:	Epoch: [46][124/817]	Loss 0.0016 (0.0041)	
training:	Epoch: [46][125/817]	Loss 0.0013 (0.0041)	
training:	Epoch: [46][126/817]	Loss 0.0011 (0.0040)	
training:	Epoch: [46][127/817]	Loss 0.0017 (0.0040)	
training:	Epoch: [46][128/817]	Loss 0.0050 (0.0040)	
training:	Epoch: [46][129/817]	Loss 0.0012 (0.0040)	
training:	Epoch: [46][130/817]	Loss 0.0012 (0.0040)	
training:	Epoch: [46][131/817]	Loss 0.0018 (0.0040)	
training:	Epoch: [46][132/817]	Loss 0.0011 (0.0040)	
training:	Epoch: [46][133/817]	Loss 0.0022 (0.0039)	
training:	Epoch: [46][134/817]	Loss 0.0012 (0.0039)	
training:	Epoch: [46][135/817]	Loss 0.0012 (0.0039)	
training:	Epoch: [46][136/817]	Loss 0.0036 (0.0039)	
training:	Epoch: [46][137/817]	Loss 0.0031 (0.0039)	
training:	Epoch: [46][138/817]	Loss 0.0014 (0.0039)	
training:	Epoch: [46][139/817]	Loss 0.0023 (0.0039)	
training:	Epoch: [46][140/817]	Loss 0.0012 (0.0038)	
training:	Epoch: [46][141/817]	Loss 0.0009 (0.0038)	
training:	Epoch: [46][142/817]	Loss 0.0012 (0.0038)	
training:	Epoch: [46][143/817]	Loss 0.0021 (0.0038)	
training:	Epoch: [46][144/817]	Loss 0.0016 (0.0038)	
training:	Epoch: [46][145/817]	Loss 0.0175 (0.0039)	
training:	Epoch: [46][146/817]	Loss 0.0013 (0.0039)	
training:	Epoch: [46][147/817]	Loss 0.0013 (0.0038)	
training:	Epoch: [46][148/817]	Loss 0.0011 (0.0038)	
training:	Epoch: [46][149/817]	Loss 0.0012 (0.0038)	
training:	Epoch: [46][150/817]	Loss 0.0011 (0.0038)	
training:	Epoch: [46][151/817]	Loss 0.0015 (0.0038)	
training:	Epoch: [46][152/817]	Loss 0.0011 (0.0038)	
training:	Epoch: [46][153/817]	Loss 0.0012 (0.0037)	
training:	Epoch: [46][154/817]	Loss 0.0012 (0.0037)	
training:	Epoch: [46][155/817]	Loss 0.0022 (0.0037)	
training:	Epoch: [46][156/817]	Loss 0.0011 (0.0037)	
training:	Epoch: [46][157/817]	Loss 0.0012 (0.0037)	
training:	Epoch: [46][158/817]	Loss 0.0014 (0.0037)	
training:	Epoch: [46][159/817]	Loss 0.0012 (0.0036)	
training:	Epoch: [46][160/817]	Loss 0.1263 (0.0044)	
training:	Epoch: [46][161/817]	Loss 0.0013 (0.0044)	
training:	Epoch: [46][162/817]	Loss 0.0012 (0.0044)	
training:	Epoch: [46][163/817]	Loss 0.0011 (0.0044)	
training:	Epoch: [46][164/817]	Loss 0.0015 (0.0043)	
training:	Epoch: [46][165/817]	Loss 0.0021 (0.0043)	
training:	Epoch: [46][166/817]	Loss 0.0012 (0.0043)	
training:	Epoch: [46][167/817]	Loss 0.0022 (0.0043)	
training:	Epoch: [46][168/817]	Loss 0.0012 (0.0043)	
training:	Epoch: [46][169/817]	Loss 0.0017 (0.0043)	
training:	Epoch: [46][170/817]	Loss 0.0017 (0.0042)	
training:	Epoch: [46][171/817]	Loss 0.0588 (0.0046)	
training:	Epoch: [46][172/817]	Loss 0.0016 (0.0045)	
training:	Epoch: [46][173/817]	Loss 0.0010 (0.0045)	
training:	Epoch: [46][174/817]	Loss 0.0012 (0.0045)	
training:	Epoch: [46][175/817]	Loss 0.0023 (0.0045)	
training:	Epoch: [46][176/817]	Loss 0.0012 (0.0045)	
training:	Epoch: [46][177/817]	Loss 0.0026 (0.0045)	
training:	Epoch: [46][178/817]	Loss 0.0076 (0.0045)	
training:	Epoch: [46][179/817]	Loss 0.0020 (0.0045)	
training:	Epoch: [46][180/817]	Loss 0.0107 (0.0045)	
training:	Epoch: [46][181/817]	Loss 0.0025 (0.0045)	
training:	Epoch: [46][182/817]	Loss 0.0025 (0.0045)	
training:	Epoch: [46][183/817]	Loss 0.0014 (0.0045)	
training:	Epoch: [46][184/817]	Loss 0.0014 (0.0044)	
training:	Epoch: [46][185/817]	Loss 0.0012 (0.0044)	
training:	Epoch: [46][186/817]	Loss 0.0018 (0.0044)	
training:	Epoch: [46][187/817]	Loss 0.0017 (0.0044)	
training:	Epoch: [46][188/817]	Loss 0.0019 (0.0044)	
training:	Epoch: [46][189/817]	Loss 0.0018 (0.0044)	
training:	Epoch: [46][190/817]	Loss 0.0012 (0.0044)	
training:	Epoch: [46][191/817]	Loss 0.0009 (0.0043)	
training:	Epoch: [46][192/817]	Loss 0.0010 (0.0043)	
training:	Epoch: [46][193/817]	Loss 0.0031 (0.0043)	
training:	Epoch: [46][194/817]	Loss 0.0017 (0.0043)	
training:	Epoch: [46][195/817]	Loss 0.0014 (0.0043)	
training:	Epoch: [46][196/817]	Loss 0.0010 (0.0043)	
training:	Epoch: [46][197/817]	Loss 0.0017 (0.0043)	
training:	Epoch: [46][198/817]	Loss 0.0012 (0.0042)	
training:	Epoch: [46][199/817]	Loss 0.0019 (0.0042)	
training:	Epoch: [46][200/817]	Loss 0.0012 (0.0042)	
training:	Epoch: [46][201/817]	Loss 0.0175 (0.0043)	
training:	Epoch: [46][202/817]	Loss 0.0054 (0.0043)	
training:	Epoch: [46][203/817]	Loss 0.0451 (0.0045)	
training:	Epoch: [46][204/817]	Loss 0.0009 (0.0045)	
training:	Epoch: [46][205/817]	Loss 0.0013 (0.0045)	
training:	Epoch: [46][206/817]	Loss 0.0018 (0.0044)	
training:	Epoch: [46][207/817]	Loss 0.0014 (0.0044)	
training:	Epoch: [46][208/817]	Loss 0.0011 (0.0044)	
training:	Epoch: [46][209/817]	Loss 0.0035 (0.0044)	
training:	Epoch: [46][210/817]	Loss 0.0012 (0.0044)	
training:	Epoch: [46][211/817]	Loss 0.0013 (0.0044)	
training:	Epoch: [46][212/817]	Loss 0.0023 (0.0044)	
training:	Epoch: [46][213/817]	Loss 0.0013 (0.0044)	
training:	Epoch: [46][214/817]	Loss 0.0050 (0.0044)	
training:	Epoch: [46][215/817]	Loss 0.0012 (0.0043)	
training:	Epoch: [46][216/817]	Loss 0.0013 (0.0043)	
training:	Epoch: [46][217/817]	Loss 0.0013 (0.0043)	
training:	Epoch: [46][218/817]	Loss 0.0023 (0.0043)	
training:	Epoch: [46][219/817]	Loss 0.0015 (0.0043)	
training:	Epoch: [46][220/817]	Loss 0.0011 (0.0043)	
training:	Epoch: [46][221/817]	Loss 0.0014 (0.0043)	
training:	Epoch: [46][222/817]	Loss 0.0012 (0.0042)	
training:	Epoch: [46][223/817]	Loss 0.1615 (0.0050)	
training:	Epoch: [46][224/817]	Loss 0.0020 (0.0049)	
training:	Epoch: [46][225/817]	Loss 0.0012 (0.0049)	
training:	Epoch: [46][226/817]	Loss 0.0013 (0.0049)	
training:	Epoch: [46][227/817]	Loss 0.0015 (0.0049)	
training:	Epoch: [46][228/817]	Loss 0.0010 (0.0049)	
training:	Epoch: [46][229/817]	Loss 0.0014 (0.0049)	
training:	Epoch: [46][230/817]	Loss 0.0018 (0.0048)	
training:	Epoch: [46][231/817]	Loss 0.0049 (0.0048)	
training:	Epoch: [46][232/817]	Loss 0.0030 (0.0048)	
training:	Epoch: [46][233/817]	Loss 0.0011 (0.0048)	
training:	Epoch: [46][234/817]	Loss 0.0011 (0.0048)	
training:	Epoch: [46][235/817]	Loss 0.0027 (0.0048)	
training:	Epoch: [46][236/817]	Loss 0.0018 (0.0048)	
training:	Epoch: [46][237/817]	Loss 0.0011 (0.0048)	
training:	Epoch: [46][238/817]	Loss 0.0012 (0.0048)	
training:	Epoch: [46][239/817]	Loss 0.0018 (0.0047)	
training:	Epoch: [46][240/817]	Loss 0.0010 (0.0047)	
training:	Epoch: [46][241/817]	Loss 0.0012 (0.0047)	
training:	Epoch: [46][242/817]	Loss 0.0010 (0.0047)	
training:	Epoch: [46][243/817]	Loss 0.0012 (0.0047)	
training:	Epoch: [46][244/817]	Loss 0.0019 (0.0047)	
training:	Epoch: [46][245/817]	Loss 0.0011 (0.0047)	
training:	Epoch: [46][246/817]	Loss 0.0034 (0.0047)	
training:	Epoch: [46][247/817]	Loss 0.0036 (0.0046)	
training:	Epoch: [46][248/817]	Loss 0.0013 (0.0046)	
training:	Epoch: [46][249/817]	Loss 0.0013 (0.0046)	
training:	Epoch: [46][250/817]	Loss 0.0059 (0.0046)	
training:	Epoch: [46][251/817]	Loss 0.0013 (0.0046)	
training:	Epoch: [46][252/817]	Loss 0.0021 (0.0046)	
training:	Epoch: [46][253/817]	Loss 0.0020 (0.0046)	
training:	Epoch: [46][254/817]	Loss 0.0014 (0.0046)	
training:	Epoch: [46][255/817]	Loss 0.0011 (0.0046)	
training:	Epoch: [46][256/817]	Loss 0.0147 (0.0046)	
training:	Epoch: [46][257/817]	Loss 0.0035 (0.0046)	
training:	Epoch: [46][258/817]	Loss 0.0010 (0.0046)	
training:	Epoch: [46][259/817]	Loss 0.0011 (0.0046)	
training:	Epoch: [46][260/817]	Loss 0.0021 (0.0046)	
training:	Epoch: [46][261/817]	Loss 0.0540 (0.0048)	
training:	Epoch: [46][262/817]	Loss 0.0013 (0.0047)	
training:	Epoch: [46][263/817]	Loss 0.0020 (0.0047)	
training:	Epoch: [46][264/817]	Loss 0.0012 (0.0047)	
training:	Epoch: [46][265/817]	Loss 0.0011 (0.0047)	
training:	Epoch: [46][266/817]	Loss 0.0275 (0.0048)	
training:	Epoch: [46][267/817]	Loss 0.0012 (0.0048)	
training:	Epoch: [46][268/817]	Loss 0.0020 (0.0048)	
training:	Epoch: [46][269/817]	Loss 0.0012 (0.0048)	
training:	Epoch: [46][270/817]	Loss 0.0206 (0.0048)	
training:	Epoch: [46][271/817]	Loss 0.0011 (0.0048)	
training:	Epoch: [46][272/817]	Loss 0.0012 (0.0048)	
training:	Epoch: [46][273/817]	Loss 0.0026 (0.0048)	
training:	Epoch: [46][274/817]	Loss 0.0010 (0.0048)	
training:	Epoch: [46][275/817]	Loss 0.0017 (0.0048)	
training:	Epoch: [46][276/817]	Loss 0.0010 (0.0047)	
training:	Epoch: [46][277/817]	Loss 0.0024 (0.0047)	
training:	Epoch: [46][278/817]	Loss 0.0012 (0.0047)	
training:	Epoch: [46][279/817]	Loss 0.0018 (0.0047)	
training:	Epoch: [46][280/817]	Loss 0.0107 (0.0047)	
training:	Epoch: [46][281/817]	Loss 0.0018 (0.0047)	
training:	Epoch: [46][282/817]	Loss 0.0156 (0.0048)	
training:	Epoch: [46][283/817]	Loss 0.0014 (0.0047)	
training:	Epoch: [46][284/817]	Loss 0.0020 (0.0047)	
training:	Epoch: [46][285/817]	Loss 0.0012 (0.0047)	
training:	Epoch: [46][286/817]	Loss 0.0016 (0.0047)	
training:	Epoch: [46][287/817]	Loss 0.0020 (0.0047)	
training:	Epoch: [46][288/817]	Loss 0.0016 (0.0047)	
training:	Epoch: [46][289/817]	Loss 0.0027 (0.0047)	
training:	Epoch: [46][290/817]	Loss 0.0012 (0.0047)	
training:	Epoch: [46][291/817]	Loss 0.0011 (0.0047)	
training:	Epoch: [46][292/817]	Loss 0.0071 (0.0047)	
training:	Epoch: [46][293/817]	Loss 0.0146 (0.0047)	
training:	Epoch: [46][294/817]	Loss 0.0014 (0.0047)	
training:	Epoch: [46][295/817]	Loss 0.0012 (0.0047)	
training:	Epoch: [46][296/817]	Loss 0.0012 (0.0047)	
training:	Epoch: [46][297/817]	Loss 0.0024 (0.0047)	
training:	Epoch: [46][298/817]	Loss 0.0013 (0.0046)	
training:	Epoch: [46][299/817]	Loss 0.0033 (0.0046)	
training:	Epoch: [46][300/817]	Loss 0.0011 (0.0046)	
training:	Epoch: [46][301/817]	Loss 0.1615 (0.0052)	
training:	Epoch: [46][302/817]	Loss 0.0015 (0.0051)	
training:	Epoch: [46][303/817]	Loss 0.0011 (0.0051)	
training:	Epoch: [46][304/817]	Loss 0.0020 (0.0051)	
training:	Epoch: [46][305/817]	Loss 0.0019 (0.0051)	
training:	Epoch: [46][306/817]	Loss 0.0014 (0.0051)	
training:	Epoch: [46][307/817]	Loss 0.0016 (0.0051)	
training:	Epoch: [46][308/817]	Loss 0.0061 (0.0051)	
training:	Epoch: [46][309/817]	Loss 0.0011 (0.0051)	
training:	Epoch: [46][310/817]	Loss 0.0015 (0.0051)	
training:	Epoch: [46][311/817]	Loss 0.0158 (0.0051)	
training:	Epoch: [46][312/817]	Loss 0.0010 (0.0051)	
training:	Epoch: [46][313/817]	Loss 0.0039 (0.0051)	
training:	Epoch: [46][314/817]	Loss 0.0014 (0.0051)	
training:	Epoch: [46][315/817]	Loss 0.0014 (0.0051)	
training:	Epoch: [46][316/817]	Loss 0.0111 (0.0051)	
training:	Epoch: [46][317/817]	Loss 0.0011 (0.0051)	
training:	Epoch: [46][318/817]	Loss 0.0011 (0.0051)	
training:	Epoch: [46][319/817]	Loss 0.0019 (0.0050)	
training:	Epoch: [46][320/817]	Loss 0.0024 (0.0050)	
training:	Epoch: [46][321/817]	Loss 0.0011 (0.0050)	
training:	Epoch: [46][322/817]	Loss 0.0010 (0.0050)	
training:	Epoch: [46][323/817]	Loss 0.0013 (0.0050)	
training:	Epoch: [46][324/817]	Loss 0.0017 (0.0050)	
training:	Epoch: [46][325/817]	Loss 0.0012 (0.0050)	
training:	Epoch: [46][326/817]	Loss 0.0010 (0.0050)	
training:	Epoch: [46][327/817]	Loss 0.0015 (0.0050)	
training:	Epoch: [46][328/817]	Loss 0.0010 (0.0049)	
training:	Epoch: [46][329/817]	Loss 0.0011 (0.0049)	
training:	Epoch: [46][330/817]	Loss 0.0009 (0.0049)	
training:	Epoch: [46][331/817]	Loss 0.0015 (0.0049)	
training:	Epoch: [46][332/817]	Loss 0.0051 (0.0049)	
training:	Epoch: [46][333/817]	Loss 0.0014 (0.0049)	
training:	Epoch: [46][334/817]	Loss 0.0011 (0.0049)	
training:	Epoch: [46][335/817]	Loss 0.0011 (0.0049)	
training:	Epoch: [46][336/817]	Loss 0.0031 (0.0049)	
training:	Epoch: [46][337/817]	Loss 0.0011 (0.0049)	
training:	Epoch: [46][338/817]	Loss 0.0012 (0.0048)	
training:	Epoch: [46][339/817]	Loss 0.0013 (0.0048)	
training:	Epoch: [46][340/817]	Loss 0.0014 (0.0048)	
training:	Epoch: [46][341/817]	Loss 0.0043 (0.0048)	
training:	Epoch: [46][342/817]	Loss 0.0010 (0.0048)	
training:	Epoch: [46][343/817]	Loss 0.0009 (0.0048)	
training:	Epoch: [46][344/817]	Loss 0.0014 (0.0048)	
training:	Epoch: [46][345/817]	Loss 0.0112 (0.0048)	
training:	Epoch: [46][346/817]	Loss 0.0011 (0.0048)	
training:	Epoch: [46][347/817]	Loss 0.0010 (0.0048)	
training:	Epoch: [46][348/817]	Loss 0.0010 (0.0048)	
training:	Epoch: [46][349/817]	Loss 0.0009 (0.0048)	
training:	Epoch: [46][350/817]	Loss 0.0009 (0.0048)	
training:	Epoch: [46][351/817]	Loss 0.0014 (0.0047)	
training:	Epoch: [46][352/817]	Loss 0.0020 (0.0047)	
training:	Epoch: [46][353/817]	Loss 0.0049 (0.0047)	
training:	Epoch: [46][354/817]	Loss 0.0011 (0.0047)	
training:	Epoch: [46][355/817]	Loss 0.0012 (0.0047)	
training:	Epoch: [46][356/817]	Loss 0.0015 (0.0047)	
training:	Epoch: [46][357/817]	Loss 0.0012 (0.0047)	
training:	Epoch: [46][358/817]	Loss 0.0010 (0.0047)	
training:	Epoch: [46][359/817]	Loss 0.0012 (0.0047)	
training:	Epoch: [46][360/817]	Loss 0.0009 (0.0047)	
training:	Epoch: [46][361/817]	Loss 0.0009 (0.0047)	
training:	Epoch: [46][362/817]	Loss 0.0011 (0.0046)	
training:	Epoch: [46][363/817]	Loss 0.0009 (0.0046)	
training:	Epoch: [46][364/817]	Loss 0.0009 (0.0046)	
training:	Epoch: [46][365/817]	Loss 0.0010 (0.0046)	
training:	Epoch: [46][366/817]	Loss 0.0010 (0.0046)	
training:	Epoch: [46][367/817]	Loss 0.0011 (0.0046)	
training:	Epoch: [46][368/817]	Loss 0.0182 (0.0046)	
training:	Epoch: [46][369/817]	Loss 0.0010 (0.0046)	
training:	Epoch: [46][370/817]	Loss 0.0041 (0.0046)	
training:	Epoch: [46][371/817]	Loss 0.0020 (0.0046)	
training:	Epoch: [46][372/817]	Loss 0.0017 (0.0046)	
training:	Epoch: [46][373/817]	Loss 0.0020 (0.0046)	
training:	Epoch: [46][374/817]	Loss 0.0017 (0.0046)	
training:	Epoch: [46][375/817]	Loss 0.0010 (0.0046)	
training:	Epoch: [46][376/817]	Loss 0.0012 (0.0046)	
training:	Epoch: [46][377/817]	Loss 0.0012 (0.0046)	
training:	Epoch: [46][378/817]	Loss 0.0039 (0.0046)	
training:	Epoch: [46][379/817]	Loss 0.0012 (0.0046)	
training:	Epoch: [46][380/817]	Loss 0.0206 (0.0046)	
training:	Epoch: [46][381/817]	Loss 0.0012 (0.0046)	
training:	Epoch: [46][382/817]	Loss 0.0010 (0.0046)	
training:	Epoch: [46][383/817]	Loss 0.0010 (0.0046)	
training:	Epoch: [46][384/817]	Loss 0.0010 (0.0046)	
training:	Epoch: [46][385/817]	Loss 0.1728 (0.0050)	
training:	Epoch: [46][386/817]	Loss 0.0010 (0.0050)	
training:	Epoch: [46][387/817]	Loss 0.0010 (0.0050)	
training:	Epoch: [46][388/817]	Loss 0.0011 (0.0050)	
training:	Epoch: [46][389/817]	Loss 0.0011 (0.0050)	
training:	Epoch: [46][390/817]	Loss 0.0011 (0.0049)	
training:	Epoch: [46][391/817]	Loss 0.0011 (0.0049)	
training:	Epoch: [46][392/817]	Loss 0.0013 (0.0049)	
training:	Epoch: [46][393/817]	Loss 0.0012 (0.0049)	
training:	Epoch: [46][394/817]	Loss 0.0015 (0.0049)	
training:	Epoch: [46][395/817]	Loss 0.0012 (0.0049)	
training:	Epoch: [46][396/817]	Loss 0.0012 (0.0049)	
training:	Epoch: [46][397/817]	Loss 0.0023 (0.0049)	
training:	Epoch: [46][398/817]	Loss 0.0020 (0.0049)	
training:	Epoch: [46][399/817]	Loss 0.0011 (0.0049)	
training:	Epoch: [46][400/817]	Loss 0.0029 (0.0049)	
training:	Epoch: [46][401/817]	Loss 0.0011 (0.0049)	
training:	Epoch: [46][402/817]	Loss 0.0021 (0.0048)	
training:	Epoch: [46][403/817]	Loss 0.0014 (0.0048)	
training:	Epoch: [46][404/817]	Loss 0.0018 (0.0048)	
training:	Epoch: [46][405/817]	Loss 0.0019 (0.0048)	
training:	Epoch: [46][406/817]	Loss 0.0013 (0.0048)	
training:	Epoch: [46][407/817]	Loss 0.0010 (0.0048)	
training:	Epoch: [46][408/817]	Loss 0.0009 (0.0048)	
training:	Epoch: [46][409/817]	Loss 0.0012 (0.0048)	
training:	Epoch: [46][410/817]	Loss 0.0012 (0.0048)	
training:	Epoch: [46][411/817]	Loss 0.0025 (0.0048)	
training:	Epoch: [46][412/817]	Loss 0.0012 (0.0048)	
training:	Epoch: [46][413/817]	Loss 0.0010 (0.0048)	
training:	Epoch: [46][414/817]	Loss 0.0024 (0.0047)	
training:	Epoch: [46][415/817]	Loss 0.0010 (0.0047)	
training:	Epoch: [46][416/817]	Loss 0.0010 (0.0047)	
training:	Epoch: [46][417/817]	Loss 0.0009 (0.0047)	
training:	Epoch: [46][418/817]	Loss 0.0019 (0.0047)	
training:	Epoch: [46][419/817]	Loss 0.0021 (0.0047)	
training:	Epoch: [46][420/817]	Loss 0.0037 (0.0047)	
training:	Epoch: [46][421/817]	Loss 0.0010 (0.0047)	
training:	Epoch: [46][422/817]	Loss 0.0014 (0.0047)	
training:	Epoch: [46][423/817]	Loss 0.0009 (0.0047)	
training:	Epoch: [46][424/817]	Loss 0.0017 (0.0047)	
training:	Epoch: [46][425/817]	Loss 0.0014 (0.0047)	
training:	Epoch: [46][426/817]	Loss 0.0021 (0.0047)	
training:	Epoch: [46][427/817]	Loss 0.0019 (0.0047)	
training:	Epoch: [46][428/817]	Loss 0.0011 (0.0046)	
training:	Epoch: [46][429/817]	Loss 0.0013 (0.0046)	
training:	Epoch: [46][430/817]	Loss 0.0019 (0.0046)	
training:	Epoch: [46][431/817]	Loss 0.0085 (0.0046)	
training:	Epoch: [46][432/817]	Loss 0.0032 (0.0046)	
training:	Epoch: [46][433/817]	Loss 0.0036 (0.0046)	
training:	Epoch: [46][434/817]	Loss 0.0061 (0.0046)	
training:	Epoch: [46][435/817]	Loss 0.0018 (0.0046)	
training:	Epoch: [46][436/817]	Loss 0.0013 (0.0046)	
training:	Epoch: [46][437/817]	Loss 0.3094 (0.0053)	
training:	Epoch: [46][438/817]	Loss 0.0014 (0.0053)	
training:	Epoch: [46][439/817]	Loss 0.0012 (0.0053)	
training:	Epoch: [46][440/817]	Loss 0.0149 (0.0053)	
training:	Epoch: [46][441/817]	Loss 0.0010 (0.0053)	
training:	Epoch: [46][442/817]	Loss 0.0030 (0.0053)	
training:	Epoch: [46][443/817]	Loss 0.0011 (0.0053)	
training:	Epoch: [46][444/817]	Loss 0.0013 (0.0053)	
training:	Epoch: [46][445/817]	Loss 0.0011 (0.0053)	
training:	Epoch: [46][446/817]	Loss 0.0015 (0.0053)	
training:	Epoch: [46][447/817]	Loss 0.0012 (0.0053)	
training:	Epoch: [46][448/817]	Loss 0.0065 (0.0053)	
training:	Epoch: [46][449/817]	Loss 0.0012 (0.0053)	
training:	Epoch: [46][450/817]	Loss 0.0014 (0.0052)	
training:	Epoch: [46][451/817]	Loss 0.0021 (0.0052)	
training:	Epoch: [46][452/817]	Loss 0.0009 (0.0052)	
training:	Epoch: [46][453/817]	Loss 0.0012 (0.0052)	
training:	Epoch: [46][454/817]	Loss 0.0569 (0.0053)	
training:	Epoch: [46][455/817]	Loss 0.0908 (0.0055)	
training:	Epoch: [46][456/817]	Loss 0.0020 (0.0055)	
training:	Epoch: [46][457/817]	Loss 0.1294 (0.0058)	
training:	Epoch: [46][458/817]	Loss 0.0035 (0.0058)	
training:	Epoch: [46][459/817]	Loss 0.0106 (0.0058)	
training:	Epoch: [46][460/817]	Loss 0.0018 (0.0058)	
training:	Epoch: [46][461/817]	Loss 0.0075 (0.0058)	
training:	Epoch: [46][462/817]	Loss 0.0011 (0.0058)	
training:	Epoch: [46][463/817]	Loss 0.0011 (0.0058)	
training:	Epoch: [46][464/817]	Loss 0.0054 (0.0058)	
training:	Epoch: [46][465/817]	Loss 0.0021 (0.0058)	
training:	Epoch: [46][466/817]	Loss 0.0011 (0.0057)	
training:	Epoch: [46][467/817]	Loss 0.0022 (0.0057)	
training:	Epoch: [46][468/817]	Loss 0.0019 (0.0057)	
training:	Epoch: [46][469/817]	Loss 0.0013 (0.0057)	
training:	Epoch: [46][470/817]	Loss 0.0014 (0.0057)	
training:	Epoch: [46][471/817]	Loss 0.0017 (0.0057)	
training:	Epoch: [46][472/817]	Loss 0.0009 (0.0057)	
training:	Epoch: [46][473/817]	Loss 0.0012 (0.0057)	
training:	Epoch: [46][474/817]	Loss 0.0011 (0.0057)	
training:	Epoch: [46][475/817]	Loss 0.0010 (0.0057)	
training:	Epoch: [46][476/817]	Loss 0.0011 (0.0057)	
training:	Epoch: [46][477/817]	Loss 0.0011 (0.0056)	
training:	Epoch: [46][478/817]	Loss 0.0013 (0.0056)	
training:	Epoch: [46][479/817]	Loss 0.0013 (0.0056)	
training:	Epoch: [46][480/817]	Loss 0.0029 (0.0056)	
training:	Epoch: [46][481/817]	Loss 0.0013 (0.0056)	
training:	Epoch: [46][482/817]	Loss 0.0037 (0.0056)	
training:	Epoch: [46][483/817]	Loss 0.0011 (0.0056)	
training:	Epoch: [46][484/817]	Loss 0.0012 (0.0056)	
training:	Epoch: [46][485/817]	Loss 0.0027 (0.0056)	
training:	Epoch: [46][486/817]	Loss 0.0016 (0.0056)	
training:	Epoch: [46][487/817]	Loss 0.0016 (0.0056)	
training:	Epoch: [46][488/817]	Loss 0.0017 (0.0056)	
training:	Epoch: [46][489/817]	Loss 0.0010 (0.0056)	
training:	Epoch: [46][490/817]	Loss 0.0052 (0.0056)	
training:	Epoch: [46][491/817]	Loss 0.0014 (0.0055)	
training:	Epoch: [46][492/817]	Loss 0.0011 (0.0055)	
training:	Epoch: [46][493/817]	Loss 0.0010 (0.0055)	
training:	Epoch: [46][494/817]	Loss 0.0012 (0.0055)	
training:	Epoch: [46][495/817]	Loss 0.0031 (0.0055)	
training:	Epoch: [46][496/817]	Loss 0.0012 (0.0055)	
training:	Epoch: [46][497/817]	Loss 0.0027 (0.0055)	
training:	Epoch: [46][498/817]	Loss 0.0009 (0.0055)	
training:	Epoch: [46][499/817]	Loss 0.0013 (0.0055)	
training:	Epoch: [46][500/817]	Loss 0.0014 (0.0055)	
training:	Epoch: [46][501/817]	Loss 0.0016 (0.0055)	
training:	Epoch: [46][502/817]	Loss 0.0012 (0.0055)	
training:	Epoch: [46][503/817]	Loss 0.0015 (0.0054)	
training:	Epoch: [46][504/817]	Loss 0.0010 (0.0054)	
training:	Epoch: [46][505/817]	Loss 0.0029 (0.0054)	
training:	Epoch: [46][506/817]	Loss 0.0019 (0.0054)	
training:	Epoch: [46][507/817]	Loss 0.0035 (0.0054)	
training:	Epoch: [46][508/817]	Loss 0.0018 (0.0054)	
training:	Epoch: [46][509/817]	Loss 0.0010 (0.0054)	
training:	Epoch: [46][510/817]	Loss 0.0013 (0.0054)	
training:	Epoch: [46][511/817]	Loss 0.0010 (0.0054)	
training:	Epoch: [46][512/817]	Loss 0.0011 (0.0054)	
training:	Epoch: [46][513/817]	Loss 0.0012 (0.0054)	
training:	Epoch: [46][514/817]	Loss 0.0039 (0.0054)	
training:	Epoch: [46][515/817]	Loss 0.0012 (0.0054)	
training:	Epoch: [46][516/817]	Loss 0.0010 (0.0054)	
training:	Epoch: [46][517/817]	Loss 0.0010 (0.0053)	
training:	Epoch: [46][518/817]	Loss 0.0010 (0.0053)	
training:	Epoch: [46][519/817]	Loss 0.0203 (0.0054)	
training:	Epoch: [46][520/817]	Loss 0.0010 (0.0054)	
training:	Epoch: [46][521/817]	Loss 0.0012 (0.0054)	
training:	Epoch: [46][522/817]	Loss 0.0009 (0.0053)	
training:	Epoch: [46][523/817]	Loss 0.0010 (0.0053)	
training:	Epoch: [46][524/817]	Loss 0.0009 (0.0053)	
training:	Epoch: [46][525/817]	Loss 0.0010 (0.0053)	
training:	Epoch: [46][526/817]	Loss 0.0014 (0.0053)	
training:	Epoch: [46][527/817]	Loss 0.0209 (0.0053)	
training:	Epoch: [46][528/817]	Loss 0.0014 (0.0053)	
training:	Epoch: [46][529/817]	Loss 0.0020 (0.0053)	
training:	Epoch: [46][530/817]	Loss 0.0012 (0.0053)	
training:	Epoch: [46][531/817]	Loss 0.0013 (0.0053)	
training:	Epoch: [46][532/817]	Loss 0.0010 (0.0053)	
training:	Epoch: [46][533/817]	Loss 0.0015 (0.0053)	
training:	Epoch: [46][534/817]	Loss 0.0014 (0.0053)	
training:	Epoch: [46][535/817]	Loss 0.0010 (0.0053)	
training:	Epoch: [46][536/817]	Loss 0.0034 (0.0053)	
training:	Epoch: [46][537/817]	Loss 0.0010 (0.0053)	
training:	Epoch: [46][538/817]	Loss 0.0023 (0.0053)	
training:	Epoch: [46][539/817]	Loss 0.0112 (0.0053)	
training:	Epoch: [46][540/817]	Loss 0.0438 (0.0053)	
training:	Epoch: [46][541/817]	Loss 0.0013 (0.0053)	
training:	Epoch: [46][542/817]	Loss 0.0011 (0.0053)	
training:	Epoch: [46][543/817]	Loss 0.0011 (0.0053)	
training:	Epoch: [46][544/817]	Loss 0.0019 (0.0053)	
training:	Epoch: [46][545/817]	Loss 0.0035 (0.0053)	
training:	Epoch: [46][546/817]	Loss 0.0013 (0.0053)	
training:	Epoch: [46][547/817]	Loss 0.0024 (0.0053)	
training:	Epoch: [46][548/817]	Loss 0.0012 (0.0053)	
training:	Epoch: [46][549/817]	Loss 0.0012 (0.0053)	
training:	Epoch: [46][550/817]	Loss 0.0009 (0.0053)	
training:	Epoch: [46][551/817]	Loss 0.0014 (0.0053)	
training:	Epoch: [46][552/817]	Loss 0.0015 (0.0053)	
training:	Epoch: [46][553/817]	Loss 0.0011 (0.0053)	
training:	Epoch: [46][554/817]	Loss 0.0202 (0.0053)	
training:	Epoch: [46][555/817]	Loss 0.0010 (0.0053)	
training:	Epoch: [46][556/817]	Loss 0.0011 (0.0053)	
training:	Epoch: [46][557/817]	Loss 0.0037 (0.0053)	
training:	Epoch: [46][558/817]	Loss 0.0010 (0.0053)	
training:	Epoch: [46][559/817]	Loss 0.0015 (0.0052)	
training:	Epoch: [46][560/817]	Loss 0.0014 (0.0052)	
training:	Epoch: [46][561/817]	Loss 0.0009 (0.0052)	
training:	Epoch: [46][562/817]	Loss 0.0014 (0.0052)	
training:	Epoch: [46][563/817]	Loss 0.0332 (0.0053)	
training:	Epoch: [46][564/817]	Loss 0.0012 (0.0053)	
training:	Epoch: [46][565/817]	Loss 0.0011 (0.0053)	
training:	Epoch: [46][566/817]	Loss 0.0010 (0.0053)	
training:	Epoch: [46][567/817]	Loss 0.0021 (0.0052)	
training:	Epoch: [46][568/817]	Loss 0.0010 (0.0052)	
training:	Epoch: [46][569/817]	Loss 0.0010 (0.0052)	
training:	Epoch: [46][570/817]	Loss 0.0949 (0.0054)	
training:	Epoch: [46][571/817]	Loss 0.0010 (0.0054)	
training:	Epoch: [46][572/817]	Loss 0.0025 (0.0054)	
training:	Epoch: [46][573/817]	Loss 0.0192 (0.0054)	
training:	Epoch: [46][574/817]	Loss 0.0011 (0.0054)	
training:	Epoch: [46][575/817]	Loss 0.0164 (0.0054)	
training:	Epoch: [46][576/817]	Loss 0.0011 (0.0054)	
training:	Epoch: [46][577/817]	Loss 0.0017 (0.0054)	
training:	Epoch: [46][578/817]	Loss 0.0013 (0.0054)	
training:	Epoch: [46][579/817]	Loss 0.0010 (0.0054)	
training:	Epoch: [46][580/817]	Loss 0.0010 (0.0054)	
training:	Epoch: [46][581/817]	Loss 0.0014 (0.0054)	
training:	Epoch: [46][582/817]	Loss 0.0010 (0.0054)	
training:	Epoch: [46][583/817]	Loss 0.0015 (0.0054)	
training:	Epoch: [46][584/817]	Loss 0.0011 (0.0054)	
training:	Epoch: [46][585/817]	Loss 0.0010 (0.0053)	
training:	Epoch: [46][586/817]	Loss 0.0099 (0.0054)	
training:	Epoch: [46][587/817]	Loss 0.0011 (0.0053)	
training:	Epoch: [46][588/817]	Loss 0.0020 (0.0053)	
training:	Epoch: [46][589/817]	Loss 0.0023 (0.0053)	
training:	Epoch: [46][590/817]	Loss 0.0011 (0.0053)	
training:	Epoch: [46][591/817]	Loss 0.0020 (0.0053)	
training:	Epoch: [46][592/817]	Loss 0.0014 (0.0053)	
training:	Epoch: [46][593/817]	Loss 0.0012 (0.0053)	
training:	Epoch: [46][594/817]	Loss 0.0010 (0.0053)	
training:	Epoch: [46][595/817]	Loss 0.0009 (0.0053)	
training:	Epoch: [46][596/817]	Loss 0.0011 (0.0053)	
training:	Epoch: [46][597/817]	Loss 0.0016 (0.0053)	
training:	Epoch: [46][598/817]	Loss 0.0028 (0.0053)	
training:	Epoch: [46][599/817]	Loss 0.0017 (0.0053)	
training:	Epoch: [46][600/817]	Loss 0.0010 (0.0053)	
training:	Epoch: [46][601/817]	Loss 0.0014 (0.0053)	
training:	Epoch: [46][602/817]	Loss 0.0014 (0.0052)	
training:	Epoch: [46][603/817]	Loss 0.2592 (0.0057)	
training:	Epoch: [46][604/817]	Loss 0.0024 (0.0057)	
training:	Epoch: [46][605/817]	Loss 0.0011 (0.0057)	
training:	Epoch: [46][606/817]	Loss 0.0009 (0.0056)	
training:	Epoch: [46][607/817]	Loss 0.0009 (0.0056)	
training:	Epoch: [46][608/817]	Loss 0.0011 (0.0056)	
training:	Epoch: [46][609/817]	Loss 0.0113 (0.0056)	
training:	Epoch: [46][610/817]	Loss 0.0114 (0.0057)	
training:	Epoch: [46][611/817]	Loss 0.0051 (0.0057)	
training:	Epoch: [46][612/817]	Loss 0.0063 (0.0057)	
training:	Epoch: [46][613/817]	Loss 0.0037 (0.0056)	
training:	Epoch: [46][614/817]	Loss 0.0013 (0.0056)	
training:	Epoch: [46][615/817]	Loss 0.0009 (0.0056)	
training:	Epoch: [46][616/817]	Loss 0.0010 (0.0056)	
training:	Epoch: [46][617/817]	Loss 0.0015 (0.0056)	
training:	Epoch: [46][618/817]	Loss 0.0647 (0.0057)	
training:	Epoch: [46][619/817]	Loss 0.0013 (0.0057)	
training:	Epoch: [46][620/817]	Loss 0.0016 (0.0057)	
training:	Epoch: [46][621/817]	Loss 0.0080 (0.0057)	
training:	Epoch: [46][622/817]	Loss 0.0014 (0.0057)	
training:	Epoch: [46][623/817]	Loss 0.0030 (0.0057)	
training:	Epoch: [46][624/817]	Loss 0.0117 (0.0057)	
training:	Epoch: [46][625/817]	Loss 0.0012 (0.0057)	
training:	Epoch: [46][626/817]	Loss 0.0023 (0.0057)	
training:	Epoch: [46][627/817]	Loss 0.0170 (0.0057)	
training:	Epoch: [46][628/817]	Loss 0.0010 (0.0057)	
training:	Epoch: [46][629/817]	Loss 0.0013 (0.0057)	
training:	Epoch: [46][630/817]	Loss 0.0272 (0.0057)	
training:	Epoch: [46][631/817]	Loss 0.0010 (0.0057)	
training:	Epoch: [46][632/817]	Loss 0.0009 (0.0057)	
training:	Epoch: [46][633/817]	Loss 0.0188 (0.0057)	
training:	Epoch: [46][634/817]	Loss 0.0010 (0.0057)	
training:	Epoch: [46][635/817]	Loss 0.0338 (0.0058)	
training:	Epoch: [46][636/817]	Loss 0.2883 (0.0062)	
training:	Epoch: [46][637/817]	Loss 0.0110 (0.0062)	
training:	Epoch: [46][638/817]	Loss 0.0030 (0.0062)	
training:	Epoch: [46][639/817]	Loss 0.0013 (0.0062)	
training:	Epoch: [46][640/817]	Loss 0.0013 (0.0062)	
training:	Epoch: [46][641/817]	Loss 0.0024 (0.0062)	
training:	Epoch: [46][642/817]	Loss 0.0016 (0.0062)	
training:	Epoch: [46][643/817]	Loss 0.0098 (0.0062)	
training:	Epoch: [46][644/817]	Loss 0.0020 (0.0062)	
training:	Epoch: [46][645/817]	Loss 0.0580 (0.0063)	
training:	Epoch: [46][646/817]	Loss 0.0036 (0.0063)	
training:	Epoch: [46][647/817]	Loss 0.0011 (0.0063)	
training:	Epoch: [46][648/817]	Loss 0.0064 (0.0063)	
training:	Epoch: [46][649/817]	Loss 0.0015 (0.0063)	
training:	Epoch: [46][650/817]	Loss 0.0022 (0.0062)	
training:	Epoch: [46][651/817]	Loss 0.0011 (0.0062)	
training:	Epoch: [46][652/817]	Loss 0.0014 (0.0062)	
training:	Epoch: [46][653/817]	Loss 0.0018 (0.0062)	
training:	Epoch: [46][654/817]	Loss 0.0329 (0.0063)	
training:	Epoch: [46][655/817]	Loss 0.0012 (0.0063)	
training:	Epoch: [46][656/817]	Loss 0.0011 (0.0062)	
training:	Epoch: [46][657/817]	Loss 0.0009 (0.0062)	
training:	Epoch: [46][658/817]	Loss 0.0019 (0.0062)	
training:	Epoch: [46][659/817]	Loss 0.0011 (0.0062)	
training:	Epoch: [46][660/817]	Loss 0.0011 (0.0062)	
training:	Epoch: [46][661/817]	Loss 0.0012 (0.0062)	
training:	Epoch: [46][662/817]	Loss 0.0049 (0.0062)	
training:	Epoch: [46][663/817]	Loss 0.0058 (0.0062)	
training:	Epoch: [46][664/817]	Loss 0.0010 (0.0062)	
training:	Epoch: [46][665/817]	Loss 0.0011 (0.0062)	
training:	Epoch: [46][666/817]	Loss 0.0009 (0.0062)	
training:	Epoch: [46][667/817]	Loss 0.0205 (0.0062)	
training:	Epoch: [46][668/817]	Loss 0.0040 (0.0062)	
training:	Epoch: [46][669/817]	Loss 0.0011 (0.0062)	
training:	Epoch: [46][670/817]	Loss 0.0010 (0.0062)	
training:	Epoch: [46][671/817]	Loss 0.0020 (0.0062)	
training:	Epoch: [46][672/817]	Loss 0.0025 (0.0062)	
training:	Epoch: [46][673/817]	Loss 0.0019 (0.0062)	
training:	Epoch: [46][674/817]	Loss 0.0022 (0.0062)	
training:	Epoch: [46][675/817]	Loss 0.0045 (0.0062)	
training:	Epoch: [46][676/817]	Loss 0.0011 (0.0062)	
training:	Epoch: [46][677/817]	Loss 0.0012 (0.0061)	
training:	Epoch: [46][678/817]	Loss 0.0010 (0.0061)	
training:	Epoch: [46][679/817]	Loss 0.0018 (0.0061)	
training:	Epoch: [46][680/817]	Loss 0.0011 (0.0061)	
training:	Epoch: [46][681/817]	Loss 0.0115 (0.0061)	
training:	Epoch: [46][682/817]	Loss 0.0011 (0.0061)	
training:	Epoch: [46][683/817]	Loss 0.0100 (0.0061)	
training:	Epoch: [46][684/817]	Loss 0.0018 (0.0061)	
training:	Epoch: [46][685/817]	Loss 0.0012 (0.0061)	
training:	Epoch: [46][686/817]	Loss 0.0018 (0.0061)	
training:	Epoch: [46][687/817]	Loss 0.0020 (0.0061)	
training:	Epoch: [46][688/817]	Loss 0.0018 (0.0061)	
training:	Epoch: [46][689/817]	Loss 0.0009 (0.0061)	
training:	Epoch: [46][690/817]	Loss 0.0011 (0.0061)	
training:	Epoch: [46][691/817]	Loss 0.0009 (0.0061)	
training:	Epoch: [46][692/817]	Loss 0.0009 (0.0061)	
training:	Epoch: [46][693/817]	Loss 0.0010 (0.0061)	
training:	Epoch: [46][694/817]	Loss 0.0010 (0.0061)	
training:	Epoch: [46][695/817]	Loss 0.0013 (0.0060)	
training:	Epoch: [46][696/817]	Loss 0.0009 (0.0060)	
training:	Epoch: [46][697/817]	Loss 0.0014 (0.0060)	
training:	Epoch: [46][698/817]	Loss 0.0013 (0.0060)	
training:	Epoch: [46][699/817]	Loss 0.0010 (0.0060)	
training:	Epoch: [46][700/817]	Loss 0.0018 (0.0060)	
training:	Epoch: [46][701/817]	Loss 0.0026 (0.0060)	
training:	Epoch: [46][702/817]	Loss 0.0009 (0.0060)	
training:	Epoch: [46][703/817]	Loss 0.0018 (0.0060)	
training:	Epoch: [46][704/817]	Loss 0.0012 (0.0060)	
training:	Epoch: [46][705/817]	Loss 0.0024 (0.0060)	
training:	Epoch: [46][706/817]	Loss 0.0015 (0.0060)	
training:	Epoch: [46][707/817]	Loss 0.0012 (0.0060)	
training:	Epoch: [46][708/817]	Loss 0.0010 (0.0060)	
training:	Epoch: [46][709/817]	Loss 0.0024 (0.0060)	
training:	Epoch: [46][710/817]	Loss 0.0012 (0.0060)	
training:	Epoch: [46][711/817]	Loss 0.0017 (0.0059)	
training:	Epoch: [46][712/817]	Loss 0.0009 (0.0059)	
training:	Epoch: [46][713/817]	Loss 0.0012 (0.0059)	
training:	Epoch: [46][714/817]	Loss 0.0030 (0.0059)	
training:	Epoch: [46][715/817]	Loss 0.0023 (0.0059)	
training:	Epoch: [46][716/817]	Loss 0.0087 (0.0059)	
training:	Epoch: [46][717/817]	Loss 0.0010 (0.0059)	
training:	Epoch: [46][718/817]	Loss 0.0025 (0.0059)	
training:	Epoch: [46][719/817]	Loss 0.0014 (0.0059)	
training:	Epoch: [46][720/817]	Loss 0.2842 (0.0063)	
training:	Epoch: [46][721/817]	Loss 0.0012 (0.0063)	
training:	Epoch: [46][722/817]	Loss 0.0010 (0.0063)	
training:	Epoch: [46][723/817]	Loss 0.0010 (0.0063)	
training:	Epoch: [46][724/817]	Loss 0.0011 (0.0063)	
training:	Epoch: [46][725/817]	Loss 0.0013 (0.0063)	
training:	Epoch: [46][726/817]	Loss 0.0009 (0.0063)	
training:	Epoch: [46][727/817]	Loss 0.0012 (0.0062)	
training:	Epoch: [46][728/817]	Loss 0.0034 (0.0062)	
training:	Epoch: [46][729/817]	Loss 0.0028 (0.0062)	
training:	Epoch: [46][730/817]	Loss 0.0012 (0.0062)	
training:	Epoch: [46][731/817]	Loss 0.0123 (0.0062)	
training:	Epoch: [46][732/817]	Loss 0.0012 (0.0062)	
training:	Epoch: [46][733/817]	Loss 0.0021 (0.0062)	
training:	Epoch: [46][734/817]	Loss 0.0010 (0.0062)	
training:	Epoch: [46][735/817]	Loss 0.0010 (0.0062)	
training:	Epoch: [46][736/817]	Loss 0.0022 (0.0062)	
training:	Epoch: [46][737/817]	Loss 0.0010 (0.0062)	
training:	Epoch: [46][738/817]	Loss 0.0010 (0.0062)	
training:	Epoch: [46][739/817]	Loss 0.0015 (0.0062)	
training:	Epoch: [46][740/817]	Loss 0.0021 (0.0062)	
training:	Epoch: [46][741/817]	Loss 0.0013 (0.0062)	
training:	Epoch: [46][742/817]	Loss 0.0010 (0.0062)	
training:	Epoch: [46][743/817]	Loss 0.0019 (0.0062)	
training:	Epoch: [46][744/817]	Loss 0.0140 (0.0062)	
training:	Epoch: [46][745/817]	Loss 0.0013 (0.0062)	
training:	Epoch: [46][746/817]	Loss 0.0018 (0.0062)	
training:	Epoch: [46][747/817]	Loss 0.0012 (0.0062)	
training:	Epoch: [46][748/817]	Loss 0.0014 (0.0061)	
training:	Epoch: [46][749/817]	Loss 0.0012 (0.0061)	
training:	Epoch: [46][750/817]	Loss 0.0013 (0.0061)	
training:	Epoch: [46][751/817]	Loss 0.0014 (0.0061)	
training:	Epoch: [46][752/817]	Loss 0.0017 (0.0061)	
training:	Epoch: [46][753/817]	Loss 0.0038 (0.0061)	
training:	Epoch: [46][754/817]	Loss 0.0016 (0.0061)	
training:	Epoch: [46][755/817]	Loss 0.0031 (0.0061)	
training:	Epoch: [46][756/817]	Loss 0.0032 (0.0061)	
training:	Epoch: [46][757/817]	Loss 0.0016 (0.0061)	
training:	Epoch: [46][758/817]	Loss 0.0150 (0.0061)	
training:	Epoch: [46][759/817]	Loss 0.0022 (0.0061)	
training:	Epoch: [46][760/817]	Loss 0.0020 (0.0061)	
training:	Epoch: [46][761/817]	Loss 0.0009 (0.0061)	
training:	Epoch: [46][762/817]	Loss 0.0011 (0.0061)	
training:	Epoch: [46][763/817]	Loss 0.0010 (0.0061)	
training:	Epoch: [46][764/817]	Loss 0.0011 (0.0061)	
training:	Epoch: [46][765/817]	Loss 0.0009 (0.0061)	
training:	Epoch: [46][766/817]	Loss 0.0015 (0.0061)	
training:	Epoch: [46][767/817]	Loss 0.0009 (0.0061)	
training:	Epoch: [46][768/817]	Loss 0.0009 (0.0060)	
training:	Epoch: [46][769/817]	Loss 0.0014 (0.0060)	
training:	Epoch: [46][770/817]	Loss 0.0013 (0.0060)	
training:	Epoch: [46][771/817]	Loss 0.0011 (0.0060)	
training:	Epoch: [46][772/817]	Loss 0.0012 (0.0060)	
training:	Epoch: [46][773/817]	Loss 0.0015 (0.0060)	
training:	Epoch: [46][774/817]	Loss 0.0012 (0.0060)	
training:	Epoch: [46][775/817]	Loss 0.0010 (0.0060)	
training:	Epoch: [46][776/817]	Loss 0.0010 (0.0060)	
training:	Epoch: [46][777/817]	Loss 0.0010 (0.0060)	
training:	Epoch: [46][778/817]	Loss 0.0010 (0.0060)	
training:	Epoch: [46][779/817]	Loss 0.0011 (0.0060)	
training:	Epoch: [46][780/817]	Loss 0.0010 (0.0060)	
training:	Epoch: [46][781/817]	Loss 0.0082 (0.0060)	
training:	Epoch: [46][782/817]	Loss 0.0011 (0.0060)	
training:	Epoch: [46][783/817]	Loss 0.0011 (0.0060)	
training:	Epoch: [46][784/817]	Loss 0.0103 (0.0060)	
training:	Epoch: [46][785/817]	Loss 0.0014 (0.0060)	
training:	Epoch: [46][786/817]	Loss 0.0010 (0.0060)	
training:	Epoch: [46][787/817]	Loss 0.0010 (0.0059)	
training:	Epoch: [46][788/817]	Loss 0.0019 (0.0059)	
training:	Epoch: [46][789/817]	Loss 0.0017 (0.0059)	
training:	Epoch: [46][790/817]	Loss 0.0023 (0.0059)	
training:	Epoch: [46][791/817]	Loss 0.0009 (0.0059)	
training:	Epoch: [46][792/817]	Loss 0.0010 (0.0059)	
training:	Epoch: [46][793/817]	Loss 0.0015 (0.0059)	
training:	Epoch: [46][794/817]	Loss 0.0011 (0.0059)	
training:	Epoch: [46][795/817]	Loss 0.0017 (0.0059)	
training:	Epoch: [46][796/817]	Loss 0.0011 (0.0059)	
training:	Epoch: [46][797/817]	Loss 0.0013 (0.0059)	
training:	Epoch: [46][798/817]	Loss 0.0055 (0.0059)	
training:	Epoch: [46][799/817]	Loss 0.0009 (0.0059)	
training:	Epoch: [46][800/817]	Loss 0.0083 (0.0059)	
training:	Epoch: [46][801/817]	Loss 0.0009 (0.0059)	
training:	Epoch: [46][802/817]	Loss 0.0014 (0.0059)	
training:	Epoch: [46][803/817]	Loss 0.0013 (0.0059)	
training:	Epoch: [46][804/817]	Loss 0.0011 (0.0059)	
training:	Epoch: [46][805/817]	Loss 0.0030 (0.0059)	
training:	Epoch: [46][806/817]	Loss 0.0012 (0.0059)	
training:	Epoch: [46][807/817]	Loss 0.0010 (0.0058)	
training:	Epoch: [46][808/817]	Loss 0.0012 (0.0058)	
training:	Epoch: [46][809/817]	Loss 0.0010 (0.0058)	
training:	Epoch: [46][810/817]	Loss 0.0116 (0.0058)	
training:	Epoch: [46][811/817]	Loss 0.0079 (0.0058)	
training:	Epoch: [46][812/817]	Loss 0.1759 (0.0061)	
training:	Epoch: [46][813/817]	Loss 0.0017 (0.0061)	
training:	Epoch: [46][814/817]	Loss 0.0011 (0.0060)	
training:	Epoch: [46][815/817]	Loss 0.0019 (0.0060)	
training:	Epoch: [46][816/817]	Loss 0.0096 (0.0060)	
training:	Epoch: [46][817/817]	Loss 0.0014 (0.0060)	
Training:	 Loss: 0.0060

Training:	 ACC: 0.9991 0.9991 0.9988 0.9994
Validation:	 ACC: 0.7846 0.7822 0.7329 0.8363
Validation:	 Best_BACC: 0.8223 0.8218 0.8127 0.8318
Validation:	 Loss: 1.1899
Pretraining:	Epoch 47/200
----------
training:	Epoch: [47][1/817]	Loss 0.0011 (0.0011)	
training:	Epoch: [47][2/817]	Loss 0.0010 (0.0010)	
training:	Epoch: [47][3/817]	Loss 0.0025 (0.0015)	
training:	Epoch: [47][4/817]	Loss 0.0045 (0.0023)	
training:	Epoch: [47][5/817]	Loss 0.0023 (0.0023)	
training:	Epoch: [47][6/817]	Loss 0.0010 (0.0021)	
training:	Epoch: [47][7/817]	Loss 0.0018 (0.0020)	
training:	Epoch: [47][8/817]	Loss 0.3325 (0.0433)	
training:	Epoch: [47][9/817]	Loss 0.0017 (0.0387)	
training:	Epoch: [47][10/817]	Loss 0.0049 (0.0353)	
training:	Epoch: [47][11/817]	Loss 0.0011 (0.0322)	
training:	Epoch: [47][12/817]	Loss 0.0013 (0.0296)	
training:	Epoch: [47][13/817]	Loss 0.0010 (0.0274)	
training:	Epoch: [47][14/817]	Loss 0.0012 (0.0256)	
training:	Epoch: [47][15/817]	Loss 0.0013 (0.0239)	
training:	Epoch: [47][16/817]	Loss 0.0009 (0.0225)	
training:	Epoch: [47][17/817]	Loss 0.0013 (0.0213)	
training:	Epoch: [47][18/817]	Loss 0.0034 (0.0203)	
training:	Epoch: [47][19/817]	Loss 0.0015 (0.0193)	
training:	Epoch: [47][20/817]	Loss 0.0010 (0.0184)	
training:	Epoch: [47][21/817]	Loss 0.0011 (0.0175)	
training:	Epoch: [47][22/817]	Loss 0.0013 (0.0168)	
training:	Epoch: [47][23/817]	Loss 0.0009 (0.0161)	
training:	Epoch: [47][24/817]	Loss 0.0010 (0.0155)	
training:	Epoch: [47][25/817]	Loss 0.0018 (0.0149)	
training:	Epoch: [47][26/817]	Loss 0.0090 (0.0147)	
training:	Epoch: [47][27/817]	Loss 0.0015 (0.0142)	
training:	Epoch: [47][28/817]	Loss 0.0015 (0.0138)	
training:	Epoch: [47][29/817]	Loss 0.0021 (0.0134)	
training:	Epoch: [47][30/817]	Loss 0.0018 (0.0130)	
training:	Epoch: [47][31/817]	Loss 0.0016 (0.0126)	
training:	Epoch: [47][32/817]	Loss 0.0018 (0.0123)	
training:	Epoch: [47][33/817]	Loss 0.0035 (0.0120)	
training:	Epoch: [47][34/817]	Loss 0.0010 (0.0117)	
training:	Epoch: [47][35/817]	Loss 0.0012 (0.0114)	
training:	Epoch: [47][36/817]	Loss 0.0101 (0.0113)	
training:	Epoch: [47][37/817]	Loss 0.0016 (0.0111)	
training:	Epoch: [47][38/817]	Loss 0.0011 (0.0108)	
training:	Epoch: [47][39/817]	Loss 0.0011 (0.0106)	
training:	Epoch: [47][40/817]	Loss 0.0024 (0.0104)	
training:	Epoch: [47][41/817]	Loss 0.0010 (0.0101)	
training:	Epoch: [47][42/817]	Loss 0.0009 (0.0099)	
training:	Epoch: [47][43/817]	Loss 0.0027 (0.0098)	
training:	Epoch: [47][44/817]	Loss 0.0010 (0.0096)	
training:	Epoch: [47][45/817]	Loss 0.0014 (0.0094)	
training:	Epoch: [47][46/817]	Loss 0.0009 (0.0092)	
training:	Epoch: [47][47/817]	Loss 0.0682 (0.0104)	
training:	Epoch: [47][48/817]	Loss 0.0026 (0.0103)	
training:	Epoch: [47][49/817]	Loss 0.0011 (0.0101)	
training:	Epoch: [47][50/817]	Loss 0.0009 (0.0099)	
training:	Epoch: [47][51/817]	Loss 0.0017 (0.0097)	
training:	Epoch: [47][52/817]	Loss 0.0009 (0.0096)	
training:	Epoch: [47][53/817]	Loss 0.0012 (0.0094)	
training:	Epoch: [47][54/817]	Loss 0.0015 (0.0093)	
training:	Epoch: [47][55/817]	Loss 0.0010 (0.0091)	
training:	Epoch: [47][56/817]	Loss 0.0051 (0.0091)	
training:	Epoch: [47][57/817]	Loss 0.0059 (0.0090)	
training:	Epoch: [47][58/817]	Loss 0.0015 (0.0089)	
training:	Epoch: [47][59/817]	Loss 0.0011 (0.0087)	
training:	Epoch: [47][60/817]	Loss 0.0009 (0.0086)	
training:	Epoch: [47][61/817]	Loss 0.0011 (0.0085)	
training:	Epoch: [47][62/817]	Loss 0.0016 (0.0084)	
training:	Epoch: [47][63/817]	Loss 0.0012 (0.0083)	
training:	Epoch: [47][64/817]	Loss 0.0012 (0.0081)	
training:	Epoch: [47][65/817]	Loss 0.0011 (0.0080)	
training:	Epoch: [47][66/817]	Loss 0.0016 (0.0079)	
training:	Epoch: [47][67/817]	Loss 0.0484 (0.0085)	
training:	Epoch: [47][68/817]	Loss 0.0009 (0.0084)	
training:	Epoch: [47][69/817]	Loss 0.0127 (0.0085)	
training:	Epoch: [47][70/817]	Loss 0.0013 (0.0084)	
training:	Epoch: [47][71/817]	Loss 0.0010 (0.0083)	
training:	Epoch: [47][72/817]	Loss 0.0011 (0.0082)	
training:	Epoch: [47][73/817]	Loss 0.0119 (0.0082)	
training:	Epoch: [47][74/817]	Loss 0.0012 (0.0081)	
training:	Epoch: [47][75/817]	Loss 0.0013 (0.0081)	
training:	Epoch: [47][76/817]	Loss 0.0010 (0.0080)	
training:	Epoch: [47][77/817]	Loss 0.0012 (0.0079)	
training:	Epoch: [47][78/817]	Loss 0.0020 (0.0078)	
training:	Epoch: [47][79/817]	Loss 0.0016 (0.0077)	
training:	Epoch: [47][80/817]	Loss 0.0055 (0.0077)	
training:	Epoch: [47][81/817]	Loss 0.0012 (0.0076)	
training:	Epoch: [47][82/817]	Loss 0.0012 (0.0075)	
training:	Epoch: [47][83/817]	Loss 0.0011 (0.0075)	
training:	Epoch: [47][84/817]	Loss 0.0009 (0.0074)	
training:	Epoch: [47][85/817]	Loss 0.0550 (0.0079)	
training:	Epoch: [47][86/817]	Loss 0.0016 (0.0079)	
training:	Epoch: [47][87/817]	Loss 0.0013 (0.0078)	
training:	Epoch: [47][88/817]	Loss 0.0015 (0.0077)	
training:	Epoch: [47][89/817]	Loss 0.0012 (0.0076)	
training:	Epoch: [47][90/817]	Loss 0.0011 (0.0076)	
training:	Epoch: [47][91/817]	Loss 0.0010 (0.0075)	
training:	Epoch: [47][92/817]	Loss 0.0012 (0.0074)	
training:	Epoch: [47][93/817]	Loss 0.0008 (0.0074)	
training:	Epoch: [47][94/817]	Loss 0.0009 (0.0073)	
training:	Epoch: [47][95/817]	Loss 0.0010 (0.0072)	
training:	Epoch: [47][96/817]	Loss 0.0018 (0.0072)	
training:	Epoch: [47][97/817]	Loss 0.0010 (0.0071)	
training:	Epoch: [47][98/817]	Loss 0.0040 (0.0071)	
training:	Epoch: [47][99/817]	Loss 0.0025 (0.0070)	
training:	Epoch: [47][100/817]	Loss 0.0009 (0.0070)	
training:	Epoch: [47][101/817]	Loss 0.0018 (0.0069)	
training:	Epoch: [47][102/817]	Loss 0.0010 (0.0069)	
training:	Epoch: [47][103/817]	Loss 0.0009 (0.0068)	
training:	Epoch: [47][104/817]	Loss 0.0012 (0.0067)	
training:	Epoch: [47][105/817]	Loss 0.0024 (0.0067)	
training:	Epoch: [47][106/817]	Loss 0.0012 (0.0066)	
training:	Epoch: [47][107/817]	Loss 0.0010 (0.0066)	
training:	Epoch: [47][108/817]	Loss 0.0008 (0.0065)	
training:	Epoch: [47][109/817]	Loss 0.0012 (0.0065)	
training:	Epoch: [47][110/817]	Loss 0.0011 (0.0064)	
training:	Epoch: [47][111/817]	Loss 0.0015 (0.0064)	
training:	Epoch: [47][112/817]	Loss 0.0106 (0.0064)	
training:	Epoch: [47][113/817]	Loss 0.0016 (0.0064)	
training:	Epoch: [47][114/817]	Loss 0.0018 (0.0064)	
training:	Epoch: [47][115/817]	Loss 0.0011 (0.0063)	
training:	Epoch: [47][116/817]	Loss 0.0009 (0.0063)	
training:	Epoch: [47][117/817]	Loss 0.0009 (0.0062)	
training:	Epoch: [47][118/817]	Loss 0.0099 (0.0062)	
training:	Epoch: [47][119/817]	Loss 0.0040 (0.0062)	
training:	Epoch: [47][120/817]	Loss 0.0008 (0.0062)	
training:	Epoch: [47][121/817]	Loss 0.0012 (0.0061)	
training:	Epoch: [47][122/817]	Loss 0.0019 (0.0061)	
training:	Epoch: [47][123/817]	Loss 0.0010 (0.0061)	
training:	Epoch: [47][124/817]	Loss 0.0011 (0.0060)	
training:	Epoch: [47][125/817]	Loss 0.0009 (0.0060)	
training:	Epoch: [47][126/817]	Loss 0.0013 (0.0059)	
training:	Epoch: [47][127/817]	Loss 0.0010 (0.0059)	
training:	Epoch: [47][128/817]	Loss 0.0013 (0.0059)	
training:	Epoch: [47][129/817]	Loss 0.0014 (0.0058)	
training:	Epoch: [47][130/817]	Loss 0.0013 (0.0058)	
training:	Epoch: [47][131/817]	Loss 0.0016 (0.0058)	
training:	Epoch: [47][132/817]	Loss 0.0013 (0.0057)	
training:	Epoch: [47][133/817]	Loss 0.0010 (0.0057)	
training:	Epoch: [47][134/817]	Loss 0.0009 (0.0057)	
training:	Epoch: [47][135/817]	Loss 0.0011 (0.0056)	
training:	Epoch: [47][136/817]	Loss 0.0011 (0.0056)	
training:	Epoch: [47][137/817]	Loss 0.0011 (0.0056)	
training:	Epoch: [47][138/817]	Loss 0.0210 (0.0057)	
training:	Epoch: [47][139/817]	Loss 0.0009 (0.0056)	
training:	Epoch: [47][140/817]	Loss 0.0016 (0.0056)	
training:	Epoch: [47][141/817]	Loss 0.0009 (0.0056)	
training:	Epoch: [47][142/817]	Loss 0.0013 (0.0055)	
training:	Epoch: [47][143/817]	Loss 0.0014 (0.0055)	
training:	Epoch: [47][144/817]	Loss 0.0011 (0.0055)	
training:	Epoch: [47][145/817]	Loss 0.0009 (0.0055)	
training:	Epoch: [47][146/817]	Loss 0.0036 (0.0054)	
training:	Epoch: [47][147/817]	Loss 0.0010 (0.0054)	
training:	Epoch: [47][148/817]	Loss 0.0008 (0.0054)	
training:	Epoch: [47][149/817]	Loss 0.0017 (0.0054)	
training:	Epoch: [47][150/817]	Loss 0.0011 (0.0053)	
training:	Epoch: [47][151/817]	Loss 0.0013 (0.0053)	
training:	Epoch: [47][152/817]	Loss 0.0009 (0.0053)	
training:	Epoch: [47][153/817]	Loss 0.0010 (0.0052)	
training:	Epoch: [47][154/817]	Loss 0.0014 (0.0052)	
training:	Epoch: [47][155/817]	Loss 0.0008 (0.0052)	
training:	Epoch: [47][156/817]	Loss 0.0013 (0.0052)	
training:	Epoch: [47][157/817]	Loss 0.0011 (0.0051)	
training:	Epoch: [47][158/817]	Loss 0.0014 (0.0051)	
training:	Epoch: [47][159/817]	Loss 0.0010 (0.0051)	
training:	Epoch: [47][160/817]	Loss 0.0177 (0.0052)	
training:	Epoch: [47][161/817]	Loss 0.0012 (0.0051)	
training:	Epoch: [47][162/817]	Loss 0.0177 (0.0052)	
training:	Epoch: [47][163/817]	Loss 0.0011 (0.0052)	
training:	Epoch: [47][164/817]	Loss 0.0011 (0.0052)	
training:	Epoch: [47][165/817]	Loss 0.0009 (0.0051)	
training:	Epoch: [47][166/817]	Loss 0.0018 (0.0051)	
training:	Epoch: [47][167/817]	Loss 0.0022 (0.0051)	
training:	Epoch: [47][168/817]	Loss 0.0010 (0.0051)	
training:	Epoch: [47][169/817]	Loss 0.0062 (0.0051)	
training:	Epoch: [47][170/817]	Loss 0.0010 (0.0051)	
training:	Epoch: [47][171/817]	Loss 0.0011 (0.0050)	
training:	Epoch: [47][172/817]	Loss 0.0013 (0.0050)	
training:	Epoch: [47][173/817]	Loss 0.0197 (0.0051)	
training:	Epoch: [47][174/817]	Loss 0.0009 (0.0051)	
training:	Epoch: [47][175/817]	Loss 0.0008 (0.0051)	
training:	Epoch: [47][176/817]	Loss 0.0011 (0.0050)	
training:	Epoch: [47][177/817]	Loss 0.0011 (0.0050)	
training:	Epoch: [47][178/817]	Loss 0.0014 (0.0050)	
training:	Epoch: [47][179/817]	Loss 0.0009 (0.0050)	
training:	Epoch: [47][180/817]	Loss 0.0674 (0.0053)	
training:	Epoch: [47][181/817]	Loss 0.0009 (0.0053)	
training:	Epoch: [47][182/817]	Loss 0.0014 (0.0053)	
training:	Epoch: [47][183/817]	Loss 0.0010 (0.0052)	
training:	Epoch: [47][184/817]	Loss 0.0022 (0.0052)	
training:	Epoch: [47][185/817]	Loss 0.0013 (0.0052)	
training:	Epoch: [47][186/817]	Loss 0.0010 (0.0052)	
training:	Epoch: [47][187/817]	Loss 0.0017 (0.0052)	
training:	Epoch: [47][188/817]	Loss 0.0011 (0.0051)	
training:	Epoch: [47][189/817]	Loss 0.0015 (0.0051)	
training:	Epoch: [47][190/817]	Loss 0.0014 (0.0051)	
training:	Epoch: [47][191/817]	Loss 0.0024 (0.0051)	
training:	Epoch: [47][192/817]	Loss 0.0009 (0.0051)	
training:	Epoch: [47][193/817]	Loss 0.0018 (0.0051)	
training:	Epoch: [47][194/817]	Loss 0.0016 (0.0050)	
training:	Epoch: [47][195/817]	Loss 0.0010 (0.0050)	
training:	Epoch: [47][196/817]	Loss 0.0016 (0.0050)	
training:	Epoch: [47][197/817]	Loss 0.0075 (0.0050)	
training:	Epoch: [47][198/817]	Loss 0.0029 (0.0050)	
training:	Epoch: [47][199/817]	Loss 0.0013 (0.0050)	
training:	Epoch: [47][200/817]	Loss 0.0010 (0.0050)	
training:	Epoch: [47][201/817]	Loss 0.0011 (0.0049)	
training:	Epoch: [47][202/817]	Loss 0.0010 (0.0049)	
training:	Epoch: [47][203/817]	Loss 0.0011 (0.0049)	
training:	Epoch: [47][204/817]	Loss 0.0012 (0.0049)	
training:	Epoch: [47][205/817]	Loss 0.0015 (0.0049)	
training:	Epoch: [47][206/817]	Loss 0.0057 (0.0049)	
training:	Epoch: [47][207/817]	Loss 0.0009 (0.0049)	
training:	Epoch: [47][208/817]	Loss 0.0008 (0.0048)	
training:	Epoch: [47][209/817]	Loss 0.0011 (0.0048)	
training:	Epoch: [47][210/817]	Loss 0.0009 (0.0048)	
training:	Epoch: [47][211/817]	Loss 0.0017 (0.0048)	
training:	Epoch: [47][212/817]	Loss 0.0009 (0.0048)	
training:	Epoch: [47][213/817]	Loss 0.0011 (0.0048)	
training:	Epoch: [47][214/817]	Loss 0.0009 (0.0047)	
training:	Epoch: [47][215/817]	Loss 0.0031 (0.0047)	
training:	Epoch: [47][216/817]	Loss 0.0010 (0.0047)	
training:	Epoch: [47][217/817]	Loss 0.0009 (0.0047)	
training:	Epoch: [47][218/817]	Loss 0.0008 (0.0047)	
training:	Epoch: [47][219/817]	Loss 0.0081 (0.0047)	
training:	Epoch: [47][220/817]	Loss 0.0009 (0.0047)	
training:	Epoch: [47][221/817]	Loss 0.0358 (0.0048)	
training:	Epoch: [47][222/817]	Loss 0.0011 (0.0048)	
training:	Epoch: [47][223/817]	Loss 0.0066 (0.0048)	
training:	Epoch: [47][224/817]	Loss 0.0009 (0.0048)	
training:	Epoch: [47][225/817]	Loss 0.0085 (0.0048)	
training:	Epoch: [47][226/817]	Loss 0.0013 (0.0048)	
training:	Epoch: [47][227/817]	Loss 0.0011 (0.0048)	
training:	Epoch: [47][228/817]	Loss 0.0010 (0.0048)	
training:	Epoch: [47][229/817]	Loss 0.0012 (0.0047)	
training:	Epoch: [47][230/817]	Loss 0.0010 (0.0047)	
training:	Epoch: [47][231/817]	Loss 0.0010 (0.0047)	
training:	Epoch: [47][232/817]	Loss 0.0008 (0.0047)	
training:	Epoch: [47][233/817]	Loss 0.0010 (0.0047)	
training:	Epoch: [47][234/817]	Loss 0.0017 (0.0047)	
training:	Epoch: [47][235/817]	Loss 0.0012 (0.0046)	
training:	Epoch: [47][236/817]	Loss 0.0022 (0.0046)	
training:	Epoch: [47][237/817]	Loss 0.0063 (0.0046)	
training:	Epoch: [47][238/817]	Loss 0.0059 (0.0046)	
training:	Epoch: [47][239/817]	Loss 0.0010 (0.0046)	
training:	Epoch: [47][240/817]	Loss 0.0011 (0.0046)	
training:	Epoch: [47][241/817]	Loss 0.0016 (0.0046)	
training:	Epoch: [47][242/817]	Loss 0.0016 (0.0046)	
training:	Epoch: [47][243/817]	Loss 0.0014 (0.0046)	
training:	Epoch: [47][244/817]	Loss 0.0014 (0.0046)	
training:	Epoch: [47][245/817]	Loss 0.0053 (0.0046)	
training:	Epoch: [47][246/817]	Loss 0.0010 (0.0046)	
training:	Epoch: [47][247/817]	Loss 0.0012 (0.0045)	
training:	Epoch: [47][248/817]	Loss 0.0008 (0.0045)	
training:	Epoch: [47][249/817]	Loss 0.0016 (0.0045)	
training:	Epoch: [47][250/817]	Loss 0.0068 (0.0045)	
training:	Epoch: [47][251/817]	Loss 0.0008 (0.0045)	
training:	Epoch: [47][252/817]	Loss 0.0009 (0.0045)	
training:	Epoch: [47][253/817]	Loss 0.0039 (0.0045)	
training:	Epoch: [47][254/817]	Loss 0.0008 (0.0045)	
training:	Epoch: [47][255/817]	Loss 0.0011 (0.0045)	
training:	Epoch: [47][256/817]	Loss 0.0011 (0.0044)	
training:	Epoch: [47][257/817]	Loss 0.0018 (0.0044)	
training:	Epoch: [47][258/817]	Loss 0.0014 (0.0044)	
training:	Epoch: [47][259/817]	Loss 0.0014 (0.0044)	
training:	Epoch: [47][260/817]	Loss 0.0011 (0.0044)	
training:	Epoch: [47][261/817]	Loss 0.0015 (0.0044)	
training:	Epoch: [47][262/817]	Loss 0.0010 (0.0044)	
training:	Epoch: [47][263/817]	Loss 0.0017 (0.0044)	
training:	Epoch: [47][264/817]	Loss 0.0009 (0.0044)	
training:	Epoch: [47][265/817]	Loss 0.0011 (0.0043)	
training:	Epoch: [47][266/817]	Loss 0.0956 (0.0047)	
training:	Epoch: [47][267/817]	Loss 0.0013 (0.0047)	
training:	Epoch: [47][268/817]	Loss 0.0009 (0.0047)	
training:	Epoch: [47][269/817]	Loss 0.0012 (0.0046)	
training:	Epoch: [47][270/817]	Loss 0.0010 (0.0046)	
training:	Epoch: [47][271/817]	Loss 0.0022 (0.0046)	
training:	Epoch: [47][272/817]	Loss 0.0018 (0.0046)	
training:	Epoch: [47][273/817]	Loss 0.0010 (0.0046)	
training:	Epoch: [47][274/817]	Loss 0.0019 (0.0046)	
training:	Epoch: [47][275/817]	Loss 0.0188 (0.0046)	
training:	Epoch: [47][276/817]	Loss 0.0086 (0.0047)	
training:	Epoch: [47][277/817]	Loss 0.0009 (0.0046)	
training:	Epoch: [47][278/817]	Loss 0.0023 (0.0046)	
training:	Epoch: [47][279/817]	Loss 0.0009 (0.0046)	
training:	Epoch: [47][280/817]	Loss 0.0012 (0.0046)	
training:	Epoch: [47][281/817]	Loss 0.0374 (0.0047)	
training:	Epoch: [47][282/817]	Loss 0.0417 (0.0049)	
training:	Epoch: [47][283/817]	Loss 0.0073 (0.0049)	
training:	Epoch: [47][284/817]	Loss 0.0013 (0.0049)	
training:	Epoch: [47][285/817]	Loss 0.0011 (0.0048)	
training:	Epoch: [47][286/817]	Loss 0.0010 (0.0048)	
training:	Epoch: [47][287/817]	Loss 0.0015 (0.0048)	
training:	Epoch: [47][288/817]	Loss 0.0055 (0.0048)	
training:	Epoch: [47][289/817]	Loss 0.0033 (0.0048)	
training:	Epoch: [47][290/817]	Loss 0.0010 (0.0048)	
training:	Epoch: [47][291/817]	Loss 0.0009 (0.0048)	
training:	Epoch: [47][292/817]	Loss 0.1381 (0.0052)	
training:	Epoch: [47][293/817]	Loss 0.0011 (0.0052)	
training:	Epoch: [47][294/817]	Loss 0.0020 (0.0052)	
training:	Epoch: [47][295/817]	Loss 0.0011 (0.0052)	
training:	Epoch: [47][296/817]	Loss 0.0011 (0.0052)	
training:	Epoch: [47][297/817]	Loss 0.0035 (0.0052)	
training:	Epoch: [47][298/817]	Loss 0.0079 (0.0052)	
training:	Epoch: [47][299/817]	Loss 0.0013 (0.0052)	
training:	Epoch: [47][300/817]	Loss 0.0010 (0.0052)	
training:	Epoch: [47][301/817]	Loss 0.0075 (0.0052)	
training:	Epoch: [47][302/817]	Loss 0.0026 (0.0052)	
training:	Epoch: [47][303/817]	Loss 0.0011 (0.0052)	
training:	Epoch: [47][304/817]	Loss 0.0012 (0.0051)	
training:	Epoch: [47][305/817]	Loss 0.1375 (0.0056)	
training:	Epoch: [47][306/817]	Loss 0.0359 (0.0057)	
training:	Epoch: [47][307/817]	Loss 0.0009 (0.0057)	
training:	Epoch: [47][308/817]	Loss 0.0026 (0.0056)	
training:	Epoch: [47][309/817]	Loss 0.0220 (0.0057)	
training:	Epoch: [47][310/817]	Loss 0.0012 (0.0057)	
training:	Epoch: [47][311/817]	Loss 0.0050 (0.0057)	
training:	Epoch: [47][312/817]	Loss 0.0031 (0.0057)	
training:	Epoch: [47][313/817]	Loss 0.0012 (0.0057)	
training:	Epoch: [47][314/817]	Loss 0.0011 (0.0056)	
training:	Epoch: [47][315/817]	Loss 0.0009 (0.0056)	
training:	Epoch: [47][316/817]	Loss 0.0009 (0.0056)	
training:	Epoch: [47][317/817]	Loss 0.2542 (0.0064)	
training:	Epoch: [47][318/817]	Loss 0.0012 (0.0064)	
training:	Epoch: [47][319/817]	Loss 0.0011 (0.0064)	
training:	Epoch: [47][320/817]	Loss 0.0017 (0.0064)	
training:	Epoch: [47][321/817]	Loss 0.0009 (0.0063)	
training:	Epoch: [47][322/817]	Loss 0.0012 (0.0063)	
training:	Epoch: [47][323/817]	Loss 0.0708 (0.0065)	
training:	Epoch: [47][324/817]	Loss 0.0009 (0.0065)	
training:	Epoch: [47][325/817]	Loss 0.0013 (0.0065)	
training:	Epoch: [47][326/817]	Loss 0.0012 (0.0065)	
training:	Epoch: [47][327/817]	Loss 0.0010 (0.0065)	
training:	Epoch: [47][328/817]	Loss 0.0015 (0.0064)	
training:	Epoch: [47][329/817]	Loss 0.0008 (0.0064)	
training:	Epoch: [47][330/817]	Loss 0.0013 (0.0064)	
training:	Epoch: [47][331/817]	Loss 0.0012 (0.0064)	
training:	Epoch: [47][332/817]	Loss 0.0023 (0.0064)	
training:	Epoch: [47][333/817]	Loss 0.0017 (0.0064)	
training:	Epoch: [47][334/817]	Loss 0.0010 (0.0063)	
training:	Epoch: [47][335/817]	Loss 0.0009 (0.0063)	
training:	Epoch: [47][336/817]	Loss 0.0010 (0.0063)	
training:	Epoch: [47][337/817]	Loss 0.0013 (0.0063)	
training:	Epoch: [47][338/817]	Loss 0.0010 (0.0063)	
training:	Epoch: [47][339/817]	Loss 0.0008 (0.0063)	
training:	Epoch: [47][340/817]	Loss 0.0111 (0.0063)	
training:	Epoch: [47][341/817]	Loss 0.0020 (0.0063)	
training:	Epoch: [47][342/817]	Loss 0.0009 (0.0063)	
training:	Epoch: [47][343/817]	Loss 0.0011 (0.0062)	
training:	Epoch: [47][344/817]	Loss 0.0018 (0.0062)	
training:	Epoch: [47][345/817]	Loss 0.0013 (0.0062)	
training:	Epoch: [47][346/817]	Loss 0.0011 (0.0062)	
training:	Epoch: [47][347/817]	Loss 0.0009 (0.0062)	
training:	Epoch: [47][348/817]	Loss 0.0015 (0.0062)	
training:	Epoch: [47][349/817]	Loss 0.0058 (0.0062)	
training:	Epoch: [47][350/817]	Loss 0.0611 (0.0063)	
training:	Epoch: [47][351/817]	Loss 0.0011 (0.0063)	
training:	Epoch: [47][352/817]	Loss 0.0030 (0.0063)	
training:	Epoch: [47][353/817]	Loss 0.0015 (0.0063)	
training:	Epoch: [47][354/817]	Loss 0.0013 (0.0063)	
training:	Epoch: [47][355/817]	Loss 0.0020 (0.0063)	
training:	Epoch: [47][356/817]	Loss 0.0009 (0.0062)	
training:	Epoch: [47][357/817]	Loss 0.0070 (0.0062)	
training:	Epoch: [47][358/817]	Loss 0.0014 (0.0062)	
training:	Epoch: [47][359/817]	Loss 0.0012 (0.0062)	
training:	Epoch: [47][360/817]	Loss 0.0009 (0.0062)	
training:	Epoch: [47][361/817]	Loss 0.0009 (0.0062)	
training:	Epoch: [47][362/817]	Loss 0.0012 (0.0062)	
training:	Epoch: [47][363/817]	Loss 0.0010 (0.0062)	
training:	Epoch: [47][364/817]	Loss 0.0009 (0.0061)	
training:	Epoch: [47][365/817]	Loss 0.0009 (0.0061)	
training:	Epoch: [47][366/817]	Loss 0.0009 (0.0061)	
training:	Epoch: [47][367/817]	Loss 0.0053 (0.0061)	
training:	Epoch: [47][368/817]	Loss 0.0013 (0.0061)	
training:	Epoch: [47][369/817]	Loss 0.0023 (0.0061)	
training:	Epoch: [47][370/817]	Loss 0.0008 (0.0061)	
training:	Epoch: [47][371/817]	Loss 0.0016 (0.0061)	
training:	Epoch: [47][372/817]	Loss 0.0009 (0.0061)	
training:	Epoch: [47][373/817]	Loss 0.0012 (0.0060)	
training:	Epoch: [47][374/817]	Loss 0.0007 (0.0060)	
training:	Epoch: [47][375/817]	Loss 0.0012 (0.0060)	
training:	Epoch: [47][376/817]	Loss 0.0010 (0.0060)	
training:	Epoch: [47][377/817]	Loss 0.0009 (0.0060)	
training:	Epoch: [47][378/817]	Loss 0.0015 (0.0060)	
training:	Epoch: [47][379/817]	Loss 0.0010 (0.0060)	
training:	Epoch: [47][380/817]	Loss 0.0039 (0.0060)	
training:	Epoch: [47][381/817]	Loss 0.0010 (0.0059)	
training:	Epoch: [47][382/817]	Loss 0.0009 (0.0059)	
training:	Epoch: [47][383/817]	Loss 0.0009 (0.0059)	
training:	Epoch: [47][384/817]	Loss 0.0027 (0.0059)	
training:	Epoch: [47][385/817]	Loss 0.0011 (0.0059)	
training:	Epoch: [47][386/817]	Loss 0.0019 (0.0059)	
training:	Epoch: [47][387/817]	Loss 0.0009 (0.0059)	
training:	Epoch: [47][388/817]	Loss 0.0010 (0.0059)	
training:	Epoch: [47][389/817]	Loss 0.0009 (0.0058)	
training:	Epoch: [47][390/817]	Loss 0.0008 (0.0058)	
training:	Epoch: [47][391/817]	Loss 0.0010 (0.0058)	
training:	Epoch: [47][392/817]	Loss 0.0009 (0.0058)	
training:	Epoch: [47][393/817]	Loss 0.0026 (0.0058)	
training:	Epoch: [47][394/817]	Loss 0.0018 (0.0058)	
training:	Epoch: [47][395/817]	Loss 0.0024 (0.0058)	
training:	Epoch: [47][396/817]	Loss 0.0027 (0.0058)	
training:	Epoch: [47][397/817]	Loss 0.0010 (0.0058)	
training:	Epoch: [47][398/817]	Loss 0.0009 (0.0057)	
training:	Epoch: [47][399/817]	Loss 0.0019 (0.0057)	
training:	Epoch: [47][400/817]	Loss 0.0010 (0.0057)	
training:	Epoch: [47][401/817]	Loss 0.0016 (0.0057)	
training:	Epoch: [47][402/817]	Loss 0.0018 (0.0057)	
training:	Epoch: [47][403/817]	Loss 0.0023 (0.0057)	
training:	Epoch: [47][404/817]	Loss 0.0049 (0.0057)	
training:	Epoch: [47][405/817]	Loss 0.0014 (0.0057)	
training:	Epoch: [47][406/817]	Loss 0.0012 (0.0057)	
training:	Epoch: [47][407/817]	Loss 0.0174 (0.0057)	
training:	Epoch: [47][408/817]	Loss 0.0009 (0.0057)	
training:	Epoch: [47][409/817]	Loss 0.0013 (0.0057)	
training:	Epoch: [47][410/817]	Loss 0.0032 (0.0057)	
training:	Epoch: [47][411/817]	Loss 0.0025 (0.0057)	
training:	Epoch: [47][412/817]	Loss 0.0092 (0.0057)	
training:	Epoch: [47][413/817]	Loss 0.2485 (0.0063)	
training:	Epoch: [47][414/817]	Loss 0.0021 (0.0063)	
training:	Epoch: [47][415/817]	Loss 0.0069 (0.0063)	
training:	Epoch: [47][416/817]	Loss 0.0012 (0.0062)	
training:	Epoch: [47][417/817]	Loss 0.0011 (0.0062)	
training:	Epoch: [47][418/817]	Loss 0.0008 (0.0062)	
training:	Epoch: [47][419/817]	Loss 0.0021 (0.0062)	
training:	Epoch: [47][420/817]	Loss 0.0012 (0.0062)	
training:	Epoch: [47][421/817]	Loss 0.0009 (0.0062)	
training:	Epoch: [47][422/817]	Loss 0.0009 (0.0062)	
training:	Epoch: [47][423/817]	Loss 0.0018 (0.0062)	
training:	Epoch: [47][424/817]	Loss 0.0010 (0.0061)	
training:	Epoch: [47][425/817]	Loss 0.0009 (0.0061)	
training:	Epoch: [47][426/817]	Loss 0.2071 (0.0066)	
training:	Epoch: [47][427/817]	Loss 0.0010 (0.0066)	
training:	Epoch: [47][428/817]	Loss 0.0012 (0.0066)	
training:	Epoch: [47][429/817]	Loss 0.0010 (0.0066)	
training:	Epoch: [47][430/817]	Loss 0.0117 (0.0066)	
training:	Epoch: [47][431/817]	Loss 0.0013 (0.0066)	
training:	Epoch: [47][432/817]	Loss 0.0010 (0.0066)	
training:	Epoch: [47][433/817]	Loss 0.0015 (0.0065)	
training:	Epoch: [47][434/817]	Loss 0.0013 (0.0065)	
training:	Epoch: [47][435/817]	Loss 0.0014 (0.0065)	
training:	Epoch: [47][436/817]	Loss 0.0011 (0.0065)	
training:	Epoch: [47][437/817]	Loss 0.0049 (0.0065)	
training:	Epoch: [47][438/817]	Loss 0.0017 (0.0065)	
training:	Epoch: [47][439/817]	Loss 0.0009 (0.0065)	
training:	Epoch: [47][440/817]	Loss 0.0010 (0.0065)	
training:	Epoch: [47][441/817]	Loss 0.0014 (0.0065)	
training:	Epoch: [47][442/817]	Loss 0.0017 (0.0064)	
training:	Epoch: [47][443/817]	Loss 0.0017 (0.0064)	
training:	Epoch: [47][444/817]	Loss 0.0011 (0.0064)	
training:	Epoch: [47][445/817]	Loss 0.0026 (0.0064)	
training:	Epoch: [47][446/817]	Loss 0.0011 (0.0064)	
training:	Epoch: [47][447/817]	Loss 0.0008 (0.0064)	
training:	Epoch: [47][448/817]	Loss 0.0013 (0.0064)	
training:	Epoch: [47][449/817]	Loss 0.0009 (0.0064)	
training:	Epoch: [47][450/817]	Loss 0.0010 (0.0064)	
training:	Epoch: [47][451/817]	Loss 0.0010 (0.0063)	
training:	Epoch: [47][452/817]	Loss 0.0016 (0.0063)	
training:	Epoch: [47][453/817]	Loss 0.0026 (0.0063)	
training:	Epoch: [47][454/817]	Loss 0.0028 (0.0063)	
training:	Epoch: [47][455/817]	Loss 0.0012 (0.0063)	
training:	Epoch: [47][456/817]	Loss 0.0013 (0.0063)	
training:	Epoch: [47][457/817]	Loss 0.0015 (0.0063)	
training:	Epoch: [47][458/817]	Loss 0.1011 (0.0065)	
training:	Epoch: [47][459/817]	Loss 0.0019 (0.0065)	
training:	Epoch: [47][460/817]	Loss 0.0019 (0.0065)	
training:	Epoch: [47][461/817]	Loss 0.0011 (0.0065)	
training:	Epoch: [47][462/817]	Loss 0.0041 (0.0065)	
training:	Epoch: [47][463/817]	Loss 0.0009 (0.0064)	
training:	Epoch: [47][464/817]	Loss 0.0010 (0.0064)	
training:	Epoch: [47][465/817]	Loss 0.0013 (0.0064)	
training:	Epoch: [47][466/817]	Loss 0.0010 (0.0064)	
training:	Epoch: [47][467/817]	Loss 0.0013 (0.0064)	
training:	Epoch: [47][468/817]	Loss 0.0011 (0.0064)	
training:	Epoch: [47][469/817]	Loss 0.0013 (0.0064)	
training:	Epoch: [47][470/817]	Loss 0.0213 (0.0064)	
training:	Epoch: [47][471/817]	Loss 0.0011 (0.0064)	
training:	Epoch: [47][472/817]	Loss 0.0011 (0.0064)	
training:	Epoch: [47][473/817]	Loss 0.0014 (0.0064)	
training:	Epoch: [47][474/817]	Loss 0.0010 (0.0064)	
training:	Epoch: [47][475/817]	Loss 0.0010 (0.0064)	
training:	Epoch: [47][476/817]	Loss 0.0010 (0.0063)	
training:	Epoch: [47][477/817]	Loss 0.0009 (0.0063)	
training:	Epoch: [47][478/817]	Loss 0.0062 (0.0063)	
training:	Epoch: [47][479/817]	Loss 0.0014 (0.0063)	
training:	Epoch: [47][480/817]	Loss 0.0118 (0.0063)	
training:	Epoch: [47][481/817]	Loss 0.0009 (0.0063)	
training:	Epoch: [47][482/817]	Loss 0.0009 (0.0063)	
training:	Epoch: [47][483/817]	Loss 0.0022 (0.0063)	
training:	Epoch: [47][484/817]	Loss 0.0357 (0.0064)	
training:	Epoch: [47][485/817]	Loss 0.0032 (0.0064)	
training:	Epoch: [47][486/817]	Loss 0.0155 (0.0064)	
training:	Epoch: [47][487/817]	Loss 0.0013 (0.0064)	
training:	Epoch: [47][488/817]	Loss 0.0014 (0.0064)	
training:	Epoch: [47][489/817]	Loss 0.0012 (0.0063)	
training:	Epoch: [47][490/817]	Loss 0.0020 (0.0063)	
training:	Epoch: [47][491/817]	Loss 0.0013 (0.0063)	
training:	Epoch: [47][492/817]	Loss 0.0013 (0.0063)	
training:	Epoch: [47][493/817]	Loss 0.0013 (0.0063)	
training:	Epoch: [47][494/817]	Loss 0.0008 (0.0063)	
training:	Epoch: [47][495/817]	Loss 0.0015 (0.0063)	
training:	Epoch: [47][496/817]	Loss 0.0028 (0.0063)	
training:	Epoch: [47][497/817]	Loss 0.0009 (0.0063)	
training:	Epoch: [47][498/817]	Loss 0.0081 (0.0063)	
training:	Epoch: [47][499/817]	Loss 0.0009 (0.0063)	
training:	Epoch: [47][500/817]	Loss 0.0009 (0.0062)	
training:	Epoch: [47][501/817]	Loss 0.0009 (0.0062)	
training:	Epoch: [47][502/817]	Loss 0.0010 (0.0062)	
training:	Epoch: [47][503/817]	Loss 0.0010 (0.0062)	
training:	Epoch: [47][504/817]	Loss 0.0012 (0.0062)	
training:	Epoch: [47][505/817]	Loss 0.0008 (0.0062)	
training:	Epoch: [47][506/817]	Loss 0.0010 (0.0062)	
training:	Epoch: [47][507/817]	Loss 0.0036 (0.0062)	
training:	Epoch: [47][508/817]	Loss 0.0011 (0.0062)	
training:	Epoch: [47][509/817]	Loss 0.0017 (0.0062)	
training:	Epoch: [47][510/817]	Loss 0.0009 (0.0061)	
training:	Epoch: [47][511/817]	Loss 0.0009 (0.0061)	
training:	Epoch: [47][512/817]	Loss 0.0020 (0.0061)	
training:	Epoch: [47][513/817]	Loss 0.0008 (0.0061)	
training:	Epoch: [47][514/817]	Loss 0.0011 (0.0061)	
training:	Epoch: [47][515/817]	Loss 0.0194 (0.0061)	
training:	Epoch: [47][516/817]	Loss 0.0030 (0.0061)	
training:	Epoch: [47][517/817]	Loss 0.0052 (0.0061)	
training:	Epoch: [47][518/817]	Loss 0.0053 (0.0061)	
training:	Epoch: [47][519/817]	Loss 0.0012 (0.0061)	
training:	Epoch: [47][520/817]	Loss 0.0012 (0.0061)	
training:	Epoch: [47][521/817]	Loss 0.0010 (0.0061)	
training:	Epoch: [47][522/817]	Loss 0.0011 (0.0061)	
training:	Epoch: [47][523/817]	Loss 0.0008 (0.0061)	
training:	Epoch: [47][524/817]	Loss 0.0010 (0.0061)	
training:	Epoch: [47][525/817]	Loss 0.0500 (0.0062)	
training:	Epoch: [47][526/817]	Loss 0.0010 (0.0061)	
training:	Epoch: [47][527/817]	Loss 0.0034 (0.0061)	
training:	Epoch: [47][528/817]	Loss 0.0009 (0.0061)	
training:	Epoch: [47][529/817]	Loss 0.0010 (0.0061)	
training:	Epoch: [47][530/817]	Loss 0.0009 (0.0061)	
training:	Epoch: [47][531/817]	Loss 0.0009 (0.0061)	
training:	Epoch: [47][532/817]	Loss 0.0009 (0.0061)	
training:	Epoch: [47][533/817]	Loss 0.0010 (0.0061)	
training:	Epoch: [47][534/817]	Loss 0.0010 (0.0061)	
training:	Epoch: [47][535/817]	Loss 0.0017 (0.0061)	
training:	Epoch: [47][536/817]	Loss 0.0015 (0.0061)	
training:	Epoch: [47][537/817]	Loss 0.0010 (0.0060)	
training:	Epoch: [47][538/817]	Loss 0.0012 (0.0060)	
training:	Epoch: [47][539/817]	Loss 0.0010 (0.0060)	
training:	Epoch: [47][540/817]	Loss 0.0014 (0.0060)	
training:	Epoch: [47][541/817]	Loss 0.0009 (0.0060)	
training:	Epoch: [47][542/817]	Loss 0.0015 (0.0060)	
training:	Epoch: [47][543/817]	Loss 0.0019 (0.0060)	
training:	Epoch: [47][544/817]	Loss 0.0045 (0.0060)	
training:	Epoch: [47][545/817]	Loss 0.0015 (0.0060)	
training:	Epoch: [47][546/817]	Loss 0.0012 (0.0060)	
training:	Epoch: [47][547/817]	Loss 0.0010 (0.0060)	
training:	Epoch: [47][548/817]	Loss 0.0369 (0.0060)	
training:	Epoch: [47][549/817]	Loss 0.0092 (0.0060)	
training:	Epoch: [47][550/817]	Loss 0.0021 (0.0060)	
training:	Epoch: [47][551/817]	Loss 0.0088 (0.0060)	
training:	Epoch: [47][552/817]	Loss 0.0045 (0.0060)	
training:	Epoch: [47][553/817]	Loss 0.0016 (0.0060)	
training:	Epoch: [47][554/817]	Loss 0.0011 (0.0060)	
training:	Epoch: [47][555/817]	Loss 0.0009 (0.0060)	
training:	Epoch: [47][556/817]	Loss 0.0011 (0.0060)	
training:	Epoch: [47][557/817]	Loss 0.0012 (0.0060)	
training:	Epoch: [47][558/817]	Loss 0.0010 (0.0060)	
training:	Epoch: [47][559/817]	Loss 0.0048 (0.0060)	
training:	Epoch: [47][560/817]	Loss 0.0014 (0.0060)	
training:	Epoch: [47][561/817]	Loss 0.0010 (0.0059)	
training:	Epoch: [47][562/817]	Loss 0.0009 (0.0059)	
training:	Epoch: [47][563/817]	Loss 0.0010 (0.0059)	
training:	Epoch: [47][564/817]	Loss 0.0010 (0.0059)	
training:	Epoch: [47][565/817]	Loss 0.0019 (0.0059)	
training:	Epoch: [47][566/817]	Loss 0.0010 (0.0059)	
training:	Epoch: [47][567/817]	Loss 0.0010 (0.0059)	
training:	Epoch: [47][568/817]	Loss 0.0016 (0.0059)	
training:	Epoch: [47][569/817]	Loss 0.0024 (0.0059)	
training:	Epoch: [47][570/817]	Loss 0.0027 (0.0059)	
training:	Epoch: [47][571/817]	Loss 0.0008 (0.0059)	
training:	Epoch: [47][572/817]	Loss 0.0010 (0.0059)	
training:	Epoch: [47][573/817]	Loss 0.0018 (0.0059)	
training:	Epoch: [47][574/817]	Loss 0.0009 (0.0058)	
training:	Epoch: [47][575/817]	Loss 0.0012 (0.0058)	
training:	Epoch: [47][576/817]	Loss 0.0015 (0.0058)	
training:	Epoch: [47][577/817]	Loss 0.0068 (0.0058)	
training:	Epoch: [47][578/817]	Loss 0.0017 (0.0058)	
training:	Epoch: [47][579/817]	Loss 0.0234 (0.0059)	
training:	Epoch: [47][580/817]	Loss 0.0103 (0.0059)	
training:	Epoch: [47][581/817]	Loss 0.0010 (0.0059)	
training:	Epoch: [47][582/817]	Loss 0.0120 (0.0059)	
training:	Epoch: [47][583/817]	Loss 0.0020 (0.0059)	
training:	Epoch: [47][584/817]	Loss 0.0009 (0.0058)	
training:	Epoch: [47][585/817]	Loss 0.0009 (0.0058)	
training:	Epoch: [47][586/817]	Loss 0.0009 (0.0058)	
training:	Epoch: [47][587/817]	Loss 0.0009 (0.0058)	
training:	Epoch: [47][588/817]	Loss 0.0007 (0.0058)	
training:	Epoch: [47][589/817]	Loss 0.0010 (0.0058)	
training:	Epoch: [47][590/817]	Loss 0.0011 (0.0058)	
training:	Epoch: [47][591/817]	Loss 0.0009 (0.0058)	
training:	Epoch: [47][592/817]	Loss 0.0008 (0.0058)	
training:	Epoch: [47][593/817]	Loss 0.0009 (0.0058)	
training:	Epoch: [47][594/817]	Loss 0.0025 (0.0058)	
training:	Epoch: [47][595/817]	Loss 0.0011 (0.0058)	
training:	Epoch: [47][596/817]	Loss 0.0041 (0.0058)	
training:	Epoch: [47][597/817]	Loss 0.0142 (0.0058)	
training:	Epoch: [47][598/817]	Loss 0.1815 (0.0061)	
training:	Epoch: [47][599/817]	Loss 0.0009 (0.0061)	
training:	Epoch: [47][600/817]	Loss 0.0010 (0.0060)	
training:	Epoch: [47][601/817]	Loss 0.0012 (0.0060)	
training:	Epoch: [47][602/817]	Loss 0.0009 (0.0060)	
training:	Epoch: [47][603/817]	Loss 0.0012 (0.0060)	
training:	Epoch: [47][604/817]	Loss 0.0010 (0.0060)	
training:	Epoch: [47][605/817]	Loss 0.0007 (0.0060)	
training:	Epoch: [47][606/817]	Loss 0.0020 (0.0060)	
training:	Epoch: [47][607/817]	Loss 0.0011 (0.0060)	
training:	Epoch: [47][608/817]	Loss 0.0009 (0.0060)	
training:	Epoch: [47][609/817]	Loss 0.0011 (0.0060)	
training:	Epoch: [47][610/817]	Loss 0.2217 (0.0063)	
training:	Epoch: [47][611/817]	Loss 0.0018 (0.0063)	
training:	Epoch: [47][612/817]	Loss 0.0007 (0.0063)	
training:	Epoch: [47][613/817]	Loss 0.0025 (0.0063)	
training:	Epoch: [47][614/817]	Loss 0.0009 (0.0063)	
training:	Epoch: [47][615/817]	Loss 0.0021 (0.0063)	
training:	Epoch: [47][616/817]	Loss 0.0020 (0.0063)	
training:	Epoch: [47][617/817]	Loss 0.0013 (0.0063)	
training:	Epoch: [47][618/817]	Loss 0.0008 (0.0063)	
training:	Epoch: [47][619/817]	Loss 0.0015 (0.0063)	
training:	Epoch: [47][620/817]	Loss 0.0019 (0.0063)	
training:	Epoch: [47][621/817]	Loss 0.0150 (0.0063)	
training:	Epoch: [47][622/817]	Loss 0.0011 (0.0063)	
training:	Epoch: [47][623/817]	Loss 0.0011 (0.0062)	
training:	Epoch: [47][624/817]	Loss 0.0015 (0.0062)	
training:	Epoch: [47][625/817]	Loss 0.0009 (0.0062)	
training:	Epoch: [47][626/817]	Loss 0.0010 (0.0062)	
training:	Epoch: [47][627/817]	Loss 0.0011 (0.0062)	
training:	Epoch: [47][628/817]	Loss 0.0014 (0.0062)	
training:	Epoch: [47][629/817]	Loss 0.0008 (0.0062)	
training:	Epoch: [47][630/817]	Loss 0.0029 (0.0062)	
training:	Epoch: [47][631/817]	Loss 0.0012 (0.0062)	
training:	Epoch: [47][632/817]	Loss 0.0008 (0.0062)	
training:	Epoch: [47][633/817]	Loss 0.0151 (0.0062)	
training:	Epoch: [47][634/817]	Loss 0.0011 (0.0062)	
training:	Epoch: [47][635/817]	Loss 0.0029 (0.0062)	
training:	Epoch: [47][636/817]	Loss 0.0017 (0.0062)	
training:	Epoch: [47][637/817]	Loss 0.0011 (0.0062)	
training:	Epoch: [47][638/817]	Loss 0.0010 (0.0062)	
training:	Epoch: [47][639/817]	Loss 0.0009 (0.0061)	
training:	Epoch: [47][640/817]	Loss 0.0009 (0.0061)	
training:	Epoch: [47][641/817]	Loss 0.0010 (0.0061)	
training:	Epoch: [47][642/817]	Loss 0.0008 (0.0061)	
training:	Epoch: [47][643/817]	Loss 0.0015 (0.0061)	
training:	Epoch: [47][644/817]	Loss 0.0082 (0.0061)	
training:	Epoch: [47][645/817]	Loss 0.0012 (0.0061)	
training:	Epoch: [47][646/817]	Loss 0.0009 (0.0061)	
training:	Epoch: [47][647/817]	Loss 0.0012 (0.0061)	
training:	Epoch: [47][648/817]	Loss 0.0021 (0.0061)	
training:	Epoch: [47][649/817]	Loss 0.0015 (0.0061)	
training:	Epoch: [47][650/817]	Loss 0.0009 (0.0061)	
training:	Epoch: [47][651/817]	Loss 0.0021 (0.0061)	
training:	Epoch: [47][652/817]	Loss 0.0518 (0.0061)	
training:	Epoch: [47][653/817]	Loss 0.0009 (0.0061)	
training:	Epoch: [47][654/817]	Loss 0.0029 (0.0061)	
training:	Epoch: [47][655/817]	Loss 0.0020 (0.0061)	
training:	Epoch: [47][656/817]	Loss 0.0009 (0.0061)	
training:	Epoch: [47][657/817]	Loss 0.0287 (0.0061)	
training:	Epoch: [47][658/817]	Loss 0.0011 (0.0061)	
training:	Epoch: [47][659/817]	Loss 0.0016 (0.0061)	
training:	Epoch: [47][660/817]	Loss 0.0009 (0.0061)	
training:	Epoch: [47][661/817]	Loss 0.0026 (0.0061)	
training:	Epoch: [47][662/817]	Loss 0.0010 (0.0061)	
training:	Epoch: [47][663/817]	Loss 0.0007 (0.0061)	
training:	Epoch: [47][664/817]	Loss 0.0024 (0.0061)	
training:	Epoch: [47][665/817]	Loss 0.0018 (0.0061)	
training:	Epoch: [47][666/817]	Loss 0.0016 (0.0061)	
training:	Epoch: [47][667/817]	Loss 0.0010 (0.0061)	
training:	Epoch: [47][668/817]	Loss 0.0010 (0.0061)	
training:	Epoch: [47][669/817]	Loss 0.0009 (0.0061)	
training:	Epoch: [47][670/817]	Loss 0.0014 (0.0061)	
training:	Epoch: [47][671/817]	Loss 0.0010 (0.0060)	
training:	Epoch: [47][672/817]	Loss 0.0012 (0.0060)	
training:	Epoch: [47][673/817]	Loss 0.0010 (0.0060)	
training:	Epoch: [47][674/817]	Loss 0.0019 (0.0060)	
training:	Epoch: [47][675/817]	Loss 0.0030 (0.0060)	
training:	Epoch: [47][676/817]	Loss 0.0010 (0.0060)	
training:	Epoch: [47][677/817]	Loss 0.0015 (0.0060)	
training:	Epoch: [47][678/817]	Loss 0.0089 (0.0060)	
training:	Epoch: [47][679/817]	Loss 0.0021 (0.0060)	
training:	Epoch: [47][680/817]	Loss 0.0008 (0.0060)	
training:	Epoch: [47][681/817]	Loss 0.0110 (0.0060)	
training:	Epoch: [47][682/817]	Loss 0.0008 (0.0060)	
training:	Epoch: [47][683/817]	Loss 0.0008 (0.0060)	
training:	Epoch: [47][684/817]	Loss 0.0009 (0.0060)	
training:	Epoch: [47][685/817]	Loss 0.0012 (0.0060)	
training:	Epoch: [47][686/817]	Loss 0.0008 (0.0060)	
training:	Epoch: [47][687/817]	Loss 0.0008 (0.0060)	
training:	Epoch: [47][688/817]	Loss 0.0008 (0.0060)	
training:	Epoch: [47][689/817]	Loss 0.0010 (0.0059)	
training:	Epoch: [47][690/817]	Loss 0.0012 (0.0059)	
training:	Epoch: [47][691/817]	Loss 0.0014 (0.0059)	
training:	Epoch: [47][692/817]	Loss 0.0024 (0.0059)	
training:	Epoch: [47][693/817]	Loss 0.0121 (0.0059)	
training:	Epoch: [47][694/817]	Loss 0.0010 (0.0059)	
training:	Epoch: [47][695/817]	Loss 0.0009 (0.0059)	
training:	Epoch: [47][696/817]	Loss 0.0054 (0.0059)	
training:	Epoch: [47][697/817]	Loss 0.0009 (0.0059)	
training:	Epoch: [47][698/817]	Loss 0.0012 (0.0059)	
training:	Epoch: [47][699/817]	Loss 0.0104 (0.0059)	
training:	Epoch: [47][700/817]	Loss 0.0231 (0.0059)	
training:	Epoch: [47][701/817]	Loss 0.0010 (0.0059)	
training:	Epoch: [47][702/817]	Loss 0.0009 (0.0059)	
training:	Epoch: [47][703/817]	Loss 0.0008 (0.0059)	
training:	Epoch: [47][704/817]	Loss 0.7637 (0.0070)	
training:	Epoch: [47][705/817]	Loss 0.0025 (0.0070)	
training:	Epoch: [47][706/817]	Loss 0.0130 (0.0070)	
training:	Epoch: [47][707/817]	Loss 0.0008 (0.0070)	
training:	Epoch: [47][708/817]	Loss 0.0008 (0.0070)	
training:	Epoch: [47][709/817]	Loss 0.0029 (0.0070)	
training:	Epoch: [47][710/817]	Loss 0.0008 (0.0070)	
training:	Epoch: [47][711/817]	Loss 0.0012 (0.0070)	
training:	Epoch: [47][712/817]	Loss 0.0011 (0.0069)	
training:	Epoch: [47][713/817]	Loss 0.0011 (0.0069)	
training:	Epoch: [47][714/817]	Loss 0.0022 (0.0069)	
training:	Epoch: [47][715/817]	Loss 0.0012 (0.0069)	
training:	Epoch: [47][716/817]	Loss 0.0009 (0.0069)	
training:	Epoch: [47][717/817]	Loss 0.0009 (0.0069)	
training:	Epoch: [47][718/817]	Loss 0.0008 (0.0069)	
training:	Epoch: [47][719/817]	Loss 0.0091 (0.0069)	
training:	Epoch: [47][720/817]	Loss 0.0010 (0.0069)	
training:	Epoch: [47][721/817]	Loss 0.0009 (0.0069)	
training:	Epoch: [47][722/817]	Loss 0.0010 (0.0069)	
training:	Epoch: [47][723/817]	Loss 0.0014 (0.0069)	
training:	Epoch: [47][724/817]	Loss 0.0009 (0.0069)	
training:	Epoch: [47][725/817]	Loss 0.0010 (0.0069)	
training:	Epoch: [47][726/817]	Loss 0.0038 (0.0068)	
training:	Epoch: [47][727/817]	Loss 0.0009 (0.0068)	
training:	Epoch: [47][728/817]	Loss 0.0009 (0.0068)	
training:	Epoch: [47][729/817]	Loss 0.5064 (0.0075)	
training:	Epoch: [47][730/817]	Loss 0.0011 (0.0075)	
training:	Epoch: [47][731/817]	Loss 0.0009 (0.0075)	
training:	Epoch: [47][732/817]	Loss 0.0048 (0.0075)	
training:	Epoch: [47][733/817]	Loss 0.0027 (0.0075)	
training:	Epoch: [47][734/817]	Loss 0.0009 (0.0075)	
training:	Epoch: [47][735/817]	Loss 0.0010 (0.0075)	
training:	Epoch: [47][736/817]	Loss 0.0011 (0.0075)	
training:	Epoch: [47][737/817]	Loss 0.0010 (0.0075)	
training:	Epoch: [47][738/817]	Loss 0.0016 (0.0074)	
training:	Epoch: [47][739/817]	Loss 0.0010 (0.0074)	
training:	Epoch: [47][740/817]	Loss 0.0025 (0.0074)	
training:	Epoch: [47][741/817]	Loss 0.0013 (0.0074)	
training:	Epoch: [47][742/817]	Loss 0.0018 (0.0074)	
training:	Epoch: [47][743/817]	Loss 0.0011 (0.0074)	
training:	Epoch: [47][744/817]	Loss 0.0009 (0.0074)	
training:	Epoch: [47][745/817]	Loss 0.0011 (0.0074)	
training:	Epoch: [47][746/817]	Loss 0.0011 (0.0074)	
training:	Epoch: [47][747/817]	Loss 0.0010 (0.0074)	
training:	Epoch: [47][748/817]	Loss 0.0013 (0.0074)	
training:	Epoch: [47][749/817]	Loss 0.0014 (0.0074)	
training:	Epoch: [47][750/817]	Loss 0.0023 (0.0073)	
training:	Epoch: [47][751/817]	Loss 0.0013 (0.0073)	
training:	Epoch: [47][752/817]	Loss 0.0012 (0.0073)	
training:	Epoch: [47][753/817]	Loss 0.0011 (0.0073)	
training:	Epoch: [47][754/817]	Loss 0.0010 (0.0073)	
training:	Epoch: [47][755/817]	Loss 0.0014 (0.0073)	
training:	Epoch: [47][756/817]	Loss 0.0018 (0.0073)	
training:	Epoch: [47][757/817]	Loss 0.0010 (0.0073)	
training:	Epoch: [47][758/817]	Loss 0.0021 (0.0073)	
training:	Epoch: [47][759/817]	Loss 0.0013 (0.0073)	
training:	Epoch: [47][760/817]	Loss 0.0008 (0.0073)	
training:	Epoch: [47][761/817]	Loss 0.0128 (0.0073)	
training:	Epoch: [47][762/817]	Loss 0.0047 (0.0073)	
training:	Epoch: [47][763/817]	Loss 0.0007 (0.0073)	
training:	Epoch: [47][764/817]	Loss 0.0030 (0.0073)	
training:	Epoch: [47][765/817]	Loss 0.0159 (0.0073)	
training:	Epoch: [47][766/817]	Loss 0.0012 (0.0073)	
training:	Epoch: [47][767/817]	Loss 0.0092 (0.0073)	
training:	Epoch: [47][768/817]	Loss 0.0015 (0.0073)	
training:	Epoch: [47][769/817]	Loss 0.0007 (0.0072)	
training:	Epoch: [47][770/817]	Loss 0.0008 (0.0072)	
training:	Epoch: [47][771/817]	Loss 0.0011 (0.0072)	
training:	Epoch: [47][772/817]	Loss 0.0018 (0.0072)	
training:	Epoch: [47][773/817]	Loss 0.0021 (0.0072)	
training:	Epoch: [47][774/817]	Loss 0.0207 (0.0072)	
training:	Epoch: [47][775/817]	Loss 0.0007 (0.0072)	
training:	Epoch: [47][776/817]	Loss 0.0014 (0.0072)	
training:	Epoch: [47][777/817]	Loss 0.0009 (0.0072)	
training:	Epoch: [47][778/817]	Loss 0.0009 (0.0072)	
training:	Epoch: [47][779/817]	Loss 0.0026 (0.0072)	
training:	Epoch: [47][780/817]	Loss 0.0011 (0.0072)	
training:	Epoch: [47][781/817]	Loss 0.0010 (0.0072)	
training:	Epoch: [47][782/817]	Loss 0.0018 (0.0072)	
training:	Epoch: [47][783/817]	Loss 0.0008 (0.0072)	
training:	Epoch: [47][784/817]	Loss 0.0009 (0.0072)	
training:	Epoch: [47][785/817]	Loss 0.0009 (0.0072)	
training:	Epoch: [47][786/817]	Loss 0.0010 (0.0071)	
training:	Epoch: [47][787/817]	Loss 0.0013 (0.0071)	
training:	Epoch: [47][788/817]	Loss 0.0087 (0.0071)	
training:	Epoch: [47][789/817]	Loss 0.0018 (0.0071)	
training:	Epoch: [47][790/817]	Loss 0.0012 (0.0071)	
training:	Epoch: [47][791/817]	Loss 0.0012 (0.0071)	
training:	Epoch: [47][792/817]	Loss 0.0013 (0.0071)	
training:	Epoch: [47][793/817]	Loss 0.0008 (0.0071)	
training:	Epoch: [47][794/817]	Loss 0.0008 (0.0071)	
training:	Epoch: [47][795/817]	Loss 0.0014 (0.0071)	
training:	Epoch: [47][796/817]	Loss 0.0905 (0.0072)	
training:	Epoch: [47][797/817]	Loss 0.0008 (0.0072)	
training:	Epoch: [47][798/817]	Loss 0.0010 (0.0072)	
training:	Epoch: [47][799/817]	Loss 0.0014 (0.0072)	
training:	Epoch: [47][800/817]	Loss 0.0014 (0.0072)	
training:	Epoch: [47][801/817]	Loss 0.0020 (0.0072)	
training:	Epoch: [47][802/817]	Loss 0.0013 (0.0071)	
training:	Epoch: [47][803/817]	Loss 0.0037 (0.0071)	
training:	Epoch: [47][804/817]	Loss 0.0016 (0.0071)	
training:	Epoch: [47][805/817]	Loss 0.0010 (0.0071)	
training:	Epoch: [47][806/817]	Loss 0.0039 (0.0071)	
training:	Epoch: [47][807/817]	Loss 0.0007 (0.0071)	
training:	Epoch: [47][808/817]	Loss 0.0068 (0.0071)	
training:	Epoch: [47][809/817]	Loss 0.0012 (0.0071)	
training:	Epoch: [47][810/817]	Loss 0.0009 (0.0071)	
training:	Epoch: [47][811/817]	Loss 0.0010 (0.0071)	
training:	Epoch: [47][812/817]	Loss 0.0009 (0.0071)	
training:	Epoch: [47][813/817]	Loss 0.1192 (0.0072)	
training:	Epoch: [47][814/817]	Loss 0.0100 (0.0072)	
training:	Epoch: [47][815/817]	Loss 0.0013 (0.0072)	
training:	Epoch: [47][816/817]	Loss 0.0013 (0.0072)	
training:	Epoch: [47][817/817]	Loss 0.0009 (0.0072)	
Training:	 Loss: 0.0072

