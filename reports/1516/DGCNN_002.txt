Namespace(inputDirectory='data', outputDirectory='Nos_norm', task='sex', type='binary', feature=['Nos', 'FA1'], CUDA_id='0', data_id='0', dataset='Classification', norm=True, channels=1, epochs=200, tensorboard=True, net_architecture='DGCNN', batch_size=32, rate=1e-06, weight=0.0, sched_step=100, sched_gamma=0.1, printing_frequency=1, seed=3, alpha=0, remix_kappa=0, remix_tau=0, loss='CE', sigma=0, k=20)
Training the 'DGCNN' architecture

The following parameters are used:
Batch size:	32
Number of workers:	0
Learning rate:	1e-06
Weight decay:	0.0
Scheduler steps:	100
Scheduler gamma:	0.1
Number of epochs of training:	200
Number of input channels:	1

Data preparation
Reading data from:	./data
Training set size:	7473
Validation set size:	1869

Performing calculations on:	cuda:0

Pretraining:	Epoch 1/200
----------
training:	Epoch: [1][1/233]	Loss 0.7850 (0.7850)	
training:	Epoch: [1][2/233]	Loss 0.7252 (0.7551)	
training:	Epoch: [1][3/233]	Loss 0.7037 (0.7380)	
training:	Epoch: [1][4/233]	Loss 0.7145 (0.7321)	
training:	Epoch: [1][5/233]	Loss 0.6341 (0.7125)	
training:	Epoch: [1][6/233]	Loss 0.6844 (0.7078)	
training:	Epoch: [1][7/233]	Loss 0.6229 (0.6957)	
training:	Epoch: [1][8/233]	Loss 0.6444 (0.6893)	
training:	Epoch: [1][9/233]	Loss 0.6584 (0.6859)	
training:	Epoch: [1][10/233]	Loss 0.6219 (0.6795)	
training:	Epoch: [1][11/233]	Loss 0.6847 (0.6799)	
training:	Epoch: [1][12/233]	Loss 0.6265 (0.6755)	
training:	Epoch: [1][13/233]	Loss 0.6142 (0.6708)	
training:	Epoch: [1][14/233]	Loss 0.7419 (0.6759)	
training:	Epoch: [1][15/233]	Loss 0.6631 (0.6750)	
training:	Epoch: [1][16/233]	Loss 0.6703 (0.6747)	
training:	Epoch: [1][17/233]	Loss 0.7456 (0.6789)	
training:	Epoch: [1][18/233]	Loss 0.6107 (0.6751)	
training:	Epoch: [1][19/233]	Loss 0.7530 (0.6792)	
training:	Epoch: [1][20/233]	Loss 0.6070 (0.6756)	
training:	Epoch: [1][21/233]	Loss 0.5740 (0.6707)	
training:	Epoch: [1][22/233]	Loss 0.6439 (0.6695)	
training:	Epoch: [1][23/233]	Loss 0.6187 (0.6673)	
training:	Epoch: [1][24/233]	Loss 0.6552 (0.6668)	
training:	Epoch: [1][25/233]	Loss 0.5971 (0.6640)	
training:	Epoch: [1][26/233]	Loss 0.6004 (0.6616)	
training:	Epoch: [1][27/233]	Loss 0.6695 (0.6619)	
training:	Epoch: [1][28/233]	Loss 0.6094 (0.6600)	
training:	Epoch: [1][29/233]	Loss 0.5365 (0.6557)	
training:	Epoch: [1][30/233]	Loss 0.6155 (0.6544)	
training:	Epoch: [1][31/233]	Loss 0.6247 (0.6534)	
training:	Epoch: [1][32/233]	Loss 0.6577 (0.6536)	
training:	Epoch: [1][33/233]	Loss 0.6380 (0.6531)	
training:	Epoch: [1][34/233]	Loss 0.6243 (0.6522)	
training:	Epoch: [1][35/233]	Loss 0.6298 (0.6516)	
training:	Epoch: [1][36/233]	Loss 0.7259 (0.6537)	
training:	Epoch: [1][37/233]	Loss 0.5922 (0.6520)	
training:	Epoch: [1][38/233]	Loss 0.6906 (0.6530)	
training:	Epoch: [1][39/233]	Loss 0.7423 (0.6553)	
training:	Epoch: [1][40/233]	Loss 0.5993 (0.6539)	
training:	Epoch: [1][41/233]	Loss 0.6117 (0.6529)	
training:	Epoch: [1][42/233]	Loss 0.5771 (0.6511)	
training:	Epoch: [1][43/233]	Loss 0.5562 (0.6489)	
training:	Epoch: [1][44/233]	Loss 0.5480 (0.6466)	
training:	Epoch: [1][45/233]	Loss 0.6822 (0.6474)	
training:	Epoch: [1][46/233]	Loss 0.5715 (0.6457)	
training:	Epoch: [1][47/233]	Loss 0.5790 (0.6443)	
training:	Epoch: [1][48/233]	Loss 0.6203 (0.6438)	
training:	Epoch: [1][49/233]	Loss 0.5249 (0.6414)	
training:	Epoch: [1][50/233]	Loss 0.5404 (0.6394)	
training:	Epoch: [1][51/233]	Loss 0.5215 (0.6370)	
training:	Epoch: [1][52/233]	Loss 0.5923 (0.6362)	
training:	Epoch: [1][53/233]	Loss 0.7602 (0.6385)	
training:	Epoch: [1][54/233]	Loss 0.6397 (0.6385)	
training:	Epoch: [1][55/233]	Loss 0.5913 (0.6377)	
training:	Epoch: [1][56/233]	Loss 0.6621 (0.6381)	
training:	Epoch: [1][57/233]	Loss 0.5869 (0.6372)	
training:	Epoch: [1][58/233]	Loss 0.6740 (0.6379)	
training:	Epoch: [1][59/233]	Loss 0.5988 (0.6372)	
training:	Epoch: [1][60/233]	Loss 0.6671 (0.6377)	
training:	Epoch: [1][61/233]	Loss 0.6531 (0.6379)	
training:	Epoch: [1][62/233]	Loss 0.5306 (0.6362)	
training:	Epoch: [1][63/233]	Loss 0.7260 (0.6376)	
training:	Epoch: [1][64/233]	Loss 0.6572 (0.6379)	
training:	Epoch: [1][65/233]	Loss 0.6411 (0.6380)	
training:	Epoch: [1][66/233]	Loss 0.5771 (0.6371)	
training:	Epoch: [1][67/233]	Loss 0.5774 (0.6362)	
training:	Epoch: [1][68/233]	Loss 0.6540 (0.6364)	
training:	Epoch: [1][69/233]	Loss 0.5477 (0.6352)	
training:	Epoch: [1][70/233]	Loss 0.4743 (0.6329)	
training:	Epoch: [1][71/233]	Loss 0.5846 (0.6322)	
training:	Epoch: [1][72/233]	Loss 0.5091 (0.6305)	
training:	Epoch: [1][73/233]	Loss 0.5943 (0.6300)	
training:	Epoch: [1][74/233]	Loss 0.6750 (0.6306)	
training:	Epoch: [1][75/233]	Loss 0.5970 (0.6301)	
training:	Epoch: [1][76/233]	Loss 0.6267 (0.6301)	
training:	Epoch: [1][77/233]	Loss 0.6551 (0.6304)	
training:	Epoch: [1][78/233]	Loss 0.6286 (0.6304)	
training:	Epoch: [1][79/233]	Loss 0.5443 (0.6293)	
training:	Epoch: [1][80/233]	Loss 0.5477 (0.6283)	
training:	Epoch: [1][81/233]	Loss 0.6174 (0.6281)	
training:	Epoch: [1][82/233]	Loss 0.5000 (0.6266)	
training:	Epoch: [1][83/233]	Loss 0.5748 (0.6260)	
training:	Epoch: [1][84/233]	Loss 0.6077 (0.6257)	
training:	Epoch: [1][85/233]	Loss 0.4638 (0.6238)	
training:	Epoch: [1][86/233]	Loss 0.5837 (0.6234)	
training:	Epoch: [1][87/233]	Loss 0.5967 (0.6231)	
training:	Epoch: [1][88/233]	Loss 0.5378 (0.6221)	
training:	Epoch: [1][89/233]	Loss 0.5054 (0.6208)	
training:	Epoch: [1][90/233]	Loss 0.4678 (0.6191)	
training:	Epoch: [1][91/233]	Loss 0.5157 (0.6180)	
training:	Epoch: [1][92/233]	Loss 0.4838 (0.6165)	
training:	Epoch: [1][93/233]	Loss 0.4881 (0.6151)	
training:	Epoch: [1][94/233]	Loss 0.6103 (0.6151)	
training:	Epoch: [1][95/233]	Loss 0.5373 (0.6142)	
training:	Epoch: [1][96/233]	Loss 0.5994 (0.6141)	
training:	Epoch: [1][97/233]	Loss 0.5995 (0.6139)	
training:	Epoch: [1][98/233]	Loss 0.4907 (0.6127)	
training:	Epoch: [1][99/233]	Loss 0.5202 (0.6117)	
training:	Epoch: [1][100/233]	Loss 0.5874 (0.6115)	
training:	Epoch: [1][101/233]	Loss 0.6002 (0.6114)	
training:	Epoch: [1][102/233]	Loss 0.7539 (0.6128)	
training:	Epoch: [1][103/233]	Loss 0.5606 (0.6123)	
training:	Epoch: [1][104/233]	Loss 0.5427 (0.6116)	
training:	Epoch: [1][105/233]	Loss 0.4072 (0.6097)	
training:	Epoch: [1][106/233]	Loss 0.6005 (0.6096)	
training:	Epoch: [1][107/233]	Loss 0.5890 (0.6094)	
training:	Epoch: [1][108/233]	Loss 0.4882 (0.6083)	
training:	Epoch: [1][109/233]	Loss 0.6146 (0.6083)	
training:	Epoch: [1][110/233]	Loss 0.6262 (0.6085)	
training:	Epoch: [1][111/233]	Loss 0.5379 (0.6079)	
training:	Epoch: [1][112/233]	Loss 0.4893 (0.6068)	
training:	Epoch: [1][113/233]	Loss 0.5028 (0.6059)	
training:	Epoch: [1][114/233]	Loss 0.5958 (0.6058)	
training:	Epoch: [1][115/233]	Loss 0.6072 (0.6058)	
training:	Epoch: [1][116/233]	Loss 0.4854 (0.6048)	
training:	Epoch: [1][117/233]	Loss 0.5497 (0.6043)	
training:	Epoch: [1][118/233]	Loss 0.4776 (0.6032)	
training:	Epoch: [1][119/233]	Loss 0.6171 (0.6033)	
training:	Epoch: [1][120/233]	Loss 0.7468 (0.6045)	
training:	Epoch: [1][121/233]	Loss 0.5860 (0.6044)	
training:	Epoch: [1][122/233]	Loss 0.6037 (0.6044)	
training:	Epoch: [1][123/233]	Loss 0.5589 (0.6040)	
training:	Epoch: [1][124/233]	Loss 0.5610 (0.6037)	
training:	Epoch: [1][125/233]	Loss 0.4997 (0.6028)	
training:	Epoch: [1][126/233]	Loss 0.4898 (0.6019)	
training:	Epoch: [1][127/233]	Loss 0.4547 (0.6008)	
training:	Epoch: [1][128/233]	Loss 0.5622 (0.6005)	
training:	Epoch: [1][129/233]	Loss 0.6018 (0.6005)	
training:	Epoch: [1][130/233]	Loss 0.4454 (0.5993)	
training:	Epoch: [1][131/233]	Loss 0.5229 (0.5987)	
training:	Epoch: [1][132/233]	Loss 0.5988 (0.5987)	
training:	Epoch: [1][133/233]	Loss 0.6507 (0.5991)	
training:	Epoch: [1][134/233]	Loss 0.7074 (0.5999)	
training:	Epoch: [1][135/233]	Loss 0.5840 (0.5998)	
training:	Epoch: [1][136/233]	Loss 0.5435 (0.5994)	
training:	Epoch: [1][137/233]	Loss 0.5188 (0.5988)	
training:	Epoch: [1][138/233]	Loss 0.5721 (0.5986)	
training:	Epoch: [1][139/233]	Loss 0.4369 (0.5974)	
training:	Epoch: [1][140/233]	Loss 0.4741 (0.5965)	
training:	Epoch: [1][141/233]	Loss 0.5468 (0.5962)	
training:	Epoch: [1][142/233]	Loss 0.4563 (0.5952)	
training:	Epoch: [1][143/233]	Loss 0.4508 (0.5942)	
training:	Epoch: [1][144/233]	Loss 0.5669 (0.5940)	
training:	Epoch: [1][145/233]	Loss 0.5302 (0.5936)	
training:	Epoch: [1][146/233]	Loss 0.4620 (0.5927)	
training:	Epoch: [1][147/233]	Loss 0.4604 (0.5918)	
training:	Epoch: [1][148/233]	Loss 0.3720 (0.5903)	
training:	Epoch: [1][149/233]	Loss 0.7076 (0.5911)	
training:	Epoch: [1][150/233]	Loss 0.5445 (0.5908)	
training:	Epoch: [1][151/233]	Loss 0.7165 (0.5916)	
training:	Epoch: [1][152/233]	Loss 0.6094 (0.5917)	
training:	Epoch: [1][153/233]	Loss 0.4584 (0.5908)	
training:	Epoch: [1][154/233]	Loss 0.5618 (0.5906)	
training:	Epoch: [1][155/233]	Loss 0.6752 (0.5912)	
training:	Epoch: [1][156/233]	Loss 0.5776 (0.5911)	
training:	Epoch: [1][157/233]	Loss 0.6247 (0.5913)	
training:	Epoch: [1][158/233]	Loss 0.5451 (0.5910)	
training:	Epoch: [1][159/233]	Loss 0.5779 (0.5909)	
training:	Epoch: [1][160/233]	Loss 0.6781 (0.5915)	
training:	Epoch: [1][161/233]	Loss 0.6305 (0.5917)	
training:	Epoch: [1][162/233]	Loss 0.4211 (0.5907)	
training:	Epoch: [1][163/233]	Loss 0.4616 (0.5899)	
training:	Epoch: [1][164/233]	Loss 0.4511 (0.5890)	
training:	Epoch: [1][165/233]	Loss 0.6063 (0.5891)	
training:	Epoch: [1][166/233]	Loss 0.5510 (0.5889)	
training:	Epoch: [1][167/233]	Loss 0.7184 (0.5897)	
training:	Epoch: [1][168/233]	Loss 0.5013 (0.5892)	
training:	Epoch: [1][169/233]	Loss 0.4888 (0.5886)	
training:	Epoch: [1][170/233]	Loss 0.5793 (0.5885)	
training:	Epoch: [1][171/233]	Loss 0.4660 (0.5878)	
training:	Epoch: [1][172/233]	Loss 0.5482 (0.5876)	
training:	Epoch: [1][173/233]	Loss 0.4430 (0.5867)	
training:	Epoch: [1][174/233]	Loss 0.5519 (0.5865)	
training:	Epoch: [1][175/233]	Loss 0.5655 (0.5864)	
training:	Epoch: [1][176/233]	Loss 0.6001 (0.5865)	
training:	Epoch: [1][177/233]	Loss 0.4582 (0.5858)	
training:	Epoch: [1][178/233]	Loss 0.4608 (0.5851)	
training:	Epoch: [1][179/233]	Loss 0.5324 (0.5848)	
training:	Epoch: [1][180/233]	Loss 0.5948 (0.5848)	
training:	Epoch: [1][181/233]	Loss 0.4987 (0.5843)	
training:	Epoch: [1][182/233]	Loss 0.5731 (0.5843)	
training:	Epoch: [1][183/233]	Loss 0.5575 (0.5841)	
training:	Epoch: [1][184/233]	Loss 0.5150 (0.5838)	
training:	Epoch: [1][185/233]	Loss 0.4777 (0.5832)	
training:	Epoch: [1][186/233]	Loss 0.5155 (0.5828)	
training:	Epoch: [1][187/233]	Loss 0.5804 (0.5828)	
training:	Epoch: [1][188/233]	Loss 0.5001 (0.5824)	
training:	Epoch: [1][189/233]	Loss 0.4308 (0.5816)	
training:	Epoch: [1][190/233]	Loss 0.6002 (0.5817)	
training:	Epoch: [1][191/233]	Loss 0.4786 (0.5811)	
training:	Epoch: [1][192/233]	Loss 0.4713 (0.5806)	
training:	Epoch: [1][193/233]	Loss 0.4554 (0.5799)	
training:	Epoch: [1][194/233]	Loss 0.5352 (0.5797)	
training:	Epoch: [1][195/233]	Loss 0.5627 (0.5796)	
training:	Epoch: [1][196/233]	Loss 0.5809 (0.5796)	
training:	Epoch: [1][197/233]	Loss 0.5127 (0.5793)	
training:	Epoch: [1][198/233]	Loss 0.4895 (0.5788)	
training:	Epoch: [1][199/233]	Loss 0.6369 (0.5791)	
training:	Epoch: [1][200/233]	Loss 0.4589 (0.5785)	
training:	Epoch: [1][201/233]	Loss 0.4838 (0.5780)	
training:	Epoch: [1][202/233]	Loss 0.6354 (0.5783)	
training:	Epoch: [1][203/233]	Loss 0.4123 (0.5775)	
training:	Epoch: [1][204/233]	Loss 0.5093 (0.5772)	
training:	Epoch: [1][205/233]	Loss 0.5690 (0.5771)	
training:	Epoch: [1][206/233]	Loss 0.4567 (0.5765)	
training:	Epoch: [1][207/233]	Loss 0.4474 (0.5759)	
training:	Epoch: [1][208/233]	Loss 0.4419 (0.5753)	
training:	Epoch: [1][209/233]	Loss 0.5049 (0.5749)	
training:	Epoch: [1][210/233]	Loss 0.5501 (0.5748)	
training:	Epoch: [1][211/233]	Loss 0.4480 (0.5742)	
training:	Epoch: [1][212/233]	Loss 0.4301 (0.5735)	
training:	Epoch: [1][213/233]	Loss 0.5047 (0.5732)	
training:	Epoch: [1][214/233]	Loss 0.5024 (0.5729)	
training:	Epoch: [1][215/233]	Loss 0.4482 (0.5723)	
training:	Epoch: [1][216/233]	Loss 0.4480 (0.5717)	
training:	Epoch: [1][217/233]	Loss 0.5500 (0.5716)	
training:	Epoch: [1][218/233]	Loss 0.5293 (0.5714)	
training:	Epoch: [1][219/233]	Loss 0.6063 (0.5716)	
training:	Epoch: [1][220/233]	Loss 0.4672 (0.5711)	
training:	Epoch: [1][221/233]	Loss 0.5183 (0.5709)	
training:	Epoch: [1][222/233]	Loss 0.5246 (0.5707)	
training:	Epoch: [1][223/233]	Loss 0.4801 (0.5703)	
training:	Epoch: [1][224/233]	Loss 0.6013 (0.5704)	
training:	Epoch: [1][225/233]	Loss 0.7155 (0.5710)	
training:	Epoch: [1][226/233]	Loss 0.4440 (0.5705)	
training:	Epoch: [1][227/233]	Loss 0.5870 (0.5706)	
training:	Epoch: [1][228/233]	Loss 0.3719 (0.5697)	
training:	Epoch: [1][229/233]	Loss 0.5191 (0.5695)	
training:	Epoch: [1][230/233]	Loss 0.4200 (0.5688)	
training:	Epoch: [1][231/233]	Loss 0.4646 (0.5684)	
training:	Epoch: [1][232/233]	Loss 0.5268 (0.5682)	
training:	Epoch: [1][233/233]	Loss 0.4255 (0.5676)	
Training:	 Loss: 0.5663

Training:	 ACC: 0.8030 0.8024 0.7891 0.8169
Validation:	 ACC: 0.7635 0.7635 0.7636 0.7635
Validation:	 Best_BACC: 0.7635 0.7635 0.7636 0.7635
Validation:	 Loss: 0.4948
Pretraining:	Epoch 2/200
----------
training:	Epoch: [2][1/233]	Loss 0.5363 (0.5363)	
training:	Epoch: [2][2/233]	Loss 0.4898 (0.5130)	
training:	Epoch: [2][3/233]	Loss 0.5163 (0.5141)	
training:	Epoch: [2][4/233]	Loss 0.5580 (0.5251)	
training:	Epoch: [2][5/233]	Loss 0.4579 (0.5117)	
training:	Epoch: [2][6/233]	Loss 0.4071 (0.4942)	
training:	Epoch: [2][7/233]	Loss 0.5069 (0.4960)	
training:	Epoch: [2][8/233]	Loss 0.4347 (0.4884)	
training:	Epoch: [2][9/233]	Loss 0.3908 (0.4775)	
training:	Epoch: [2][10/233]	Loss 0.6290 (0.4927)	
training:	Epoch: [2][11/233]	Loss 0.4227 (0.4863)	
training:	Epoch: [2][12/233]	Loss 0.3722 (0.4768)	
training:	Epoch: [2][13/233]	Loss 0.5598 (0.4832)	
training:	Epoch: [2][14/233]	Loss 0.5614 (0.4888)	
training:	Epoch: [2][15/233]	Loss 0.6131 (0.4971)	
training:	Epoch: [2][16/233]	Loss 0.4462 (0.4939)	
training:	Epoch: [2][17/233]	Loss 0.4878 (0.4935)	
training:	Epoch: [2][18/233]	Loss 0.3810 (0.4873)	
training:	Epoch: [2][19/233]	Loss 0.3975 (0.4825)	
training:	Epoch: [2][20/233]	Loss 0.4327 (0.4801)	
training:	Epoch: [2][21/233]	Loss 0.5366 (0.4828)	
training:	Epoch: [2][22/233]	Loss 0.5837 (0.4873)	
training:	Epoch: [2][23/233]	Loss 0.4631 (0.4863)	
training:	Epoch: [2][24/233]	Loss 0.4032 (0.4828)	
training:	Epoch: [2][25/233]	Loss 0.5160 (0.4841)	
training:	Epoch: [2][26/233]	Loss 0.5061 (0.4850)	
training:	Epoch: [2][27/233]	Loss 0.4980 (0.4855)	
training:	Epoch: [2][28/233]	Loss 0.4576 (0.4845)	
training:	Epoch: [2][29/233]	Loss 0.4122 (0.4820)	
training:	Epoch: [2][30/233]	Loss 0.3863 (0.4788)	
training:	Epoch: [2][31/233]	Loss 0.4018 (0.4763)	
training:	Epoch: [2][32/233]	Loss 0.4540 (0.4756)	
training:	Epoch: [2][33/233]	Loss 0.4338 (0.4743)	
training:	Epoch: [2][34/233]	Loss 0.4355 (0.4732)	
training:	Epoch: [2][35/233]	Loss 0.3994 (0.4711)	
training:	Epoch: [2][36/233]	Loss 0.4299 (0.4699)	
training:	Epoch: [2][37/233]	Loss 0.4722 (0.4700)	
training:	Epoch: [2][38/233]	Loss 0.4422 (0.4693)	
training:	Epoch: [2][39/233]	Loss 0.5166 (0.4705)	
training:	Epoch: [2][40/233]	Loss 0.4446 (0.4698)	
training:	Epoch: [2][41/233]	Loss 0.4098 (0.4684)	
training:	Epoch: [2][42/233]	Loss 0.4192 (0.4672)	
training:	Epoch: [2][43/233]	Loss 0.3898 (0.4654)	
training:	Epoch: [2][44/233]	Loss 0.5637 (0.4676)	
training:	Epoch: [2][45/233]	Loss 0.3652 (0.4654)	
training:	Epoch: [2][46/233]	Loss 0.3743 (0.4634)	
training:	Epoch: [2][47/233]	Loss 0.4129 (0.4623)	
training:	Epoch: [2][48/233]	Loss 0.3791 (0.4606)	
training:	Epoch: [2][49/233]	Loss 0.4489 (0.4603)	
training:	Epoch: [2][50/233]	Loss 0.4915 (0.4610)	
training:	Epoch: [2][51/233]	Loss 0.4718 (0.4612)	
training:	Epoch: [2][52/233]	Loss 0.4032 (0.4601)	
training:	Epoch: [2][53/233]	Loss 0.4786 (0.4604)	
training:	Epoch: [2][54/233]	Loss 0.4209 (0.4597)	
training:	Epoch: [2][55/233]	Loss 0.3619 (0.4579)	
training:	Epoch: [2][56/233]	Loss 0.5514 (0.4596)	
training:	Epoch: [2][57/233]	Loss 0.3858 (0.4583)	
training:	Epoch: [2][58/233]	Loss 0.4889 (0.4588)	
training:	Epoch: [2][59/233]	Loss 0.5775 (0.4608)	
training:	Epoch: [2][60/233]	Loss 0.3934 (0.4597)	
training:	Epoch: [2][61/233]	Loss 0.4104 (0.4589)	
training:	Epoch: [2][62/233]	Loss 0.4643 (0.4590)	
training:	Epoch: [2][63/233]	Loss 0.4142 (0.4583)	
training:	Epoch: [2][64/233]	Loss 0.4994 (0.4589)	
training:	Epoch: [2][65/233]	Loss 0.3907 (0.4579)	
training:	Epoch: [2][66/233]	Loss 0.4346 (0.4575)	
training:	Epoch: [2][67/233]	Loss 0.4534 (0.4574)	
training:	Epoch: [2][68/233]	Loss 0.4294 (0.4570)	
training:	Epoch: [2][69/233]	Loss 0.3493 (0.4555)	
training:	Epoch: [2][70/233]	Loss 0.5152 (0.4563)	
training:	Epoch: [2][71/233]	Loss 0.4838 (0.4567)	
training:	Epoch: [2][72/233]	Loss 0.3684 (0.4555)	
training:	Epoch: [2][73/233]	Loss 0.4329 (0.4552)	
training:	Epoch: [2][74/233]	Loss 0.4135 (0.4546)	
training:	Epoch: [2][75/233]	Loss 0.3299 (0.4529)	
training:	Epoch: [2][76/233]	Loss 0.5308 (0.4540)	
training:	Epoch: [2][77/233]	Loss 0.5892 (0.4557)	
training:	Epoch: [2][78/233]	Loss 0.5816 (0.4573)	
training:	Epoch: [2][79/233]	Loss 0.3719 (0.4563)	
training:	Epoch: [2][80/233]	Loss 0.4516 (0.4562)	
training:	Epoch: [2][81/233]	Loss 0.3966 (0.4555)	
training:	Epoch: [2][82/233]	Loss 0.4717 (0.4557)	
training:	Epoch: [2][83/233]	Loss 0.6191 (0.4576)	
training:	Epoch: [2][84/233]	Loss 0.4048 (0.4570)	
training:	Epoch: [2][85/233]	Loss 0.4653 (0.4571)	
training:	Epoch: [2][86/233]	Loss 0.6334 (0.4592)	
training:	Epoch: [2][87/233]	Loss 0.3782 (0.4582)	
training:	Epoch: [2][88/233]	Loss 0.4397 (0.4580)	
training:	Epoch: [2][89/233]	Loss 0.4797 (0.4583)	
training:	Epoch: [2][90/233]	Loss 0.3803 (0.4574)	
training:	Epoch: [2][91/233]	Loss 0.4801 (0.4576)	
training:	Epoch: [2][92/233]	Loss 0.3941 (0.4569)	
training:	Epoch: [2][93/233]	Loss 0.3789 (0.4561)	
training:	Epoch: [2][94/233]	Loss 0.3700 (0.4552)	
training:	Epoch: [2][95/233]	Loss 0.3733 (0.4543)	
training:	Epoch: [2][96/233]	Loss 0.5922 (0.4558)	
training:	Epoch: [2][97/233]	Loss 0.3976 (0.4552)	
training:	Epoch: [2][98/233]	Loss 0.4585 (0.4552)	
training:	Epoch: [2][99/233]	Loss 0.3271 (0.4539)	
training:	Epoch: [2][100/233]	Loss 0.4660 (0.4540)	
training:	Epoch: [2][101/233]	Loss 0.4494 (0.4540)	
training:	Epoch: [2][102/233]	Loss 0.4408 (0.4539)	
training:	Epoch: [2][103/233]	Loss 0.3676 (0.4530)	
training:	Epoch: [2][104/233]	Loss 0.4597 (0.4531)	
training:	Epoch: [2][105/233]	Loss 0.4105 (0.4527)	
training:	Epoch: [2][106/233]	Loss 0.5047 (0.4532)	
training:	Epoch: [2][107/233]	Loss 0.7467 (0.4559)	
training:	Epoch: [2][108/233]	Loss 0.3944 (0.4553)	
training:	Epoch: [2][109/233]	Loss 0.3833 (0.4547)	
training:	Epoch: [2][110/233]	Loss 0.4991 (0.4551)	
training:	Epoch: [2][111/233]	Loss 0.4465 (0.4550)	
training:	Epoch: [2][112/233]	Loss 0.4102 (0.4546)	
training:	Epoch: [2][113/233]	Loss 0.4655 (0.4547)	
training:	Epoch: [2][114/233]	Loss 0.4188 (0.4544)	
training:	Epoch: [2][115/233]	Loss 0.4154 (0.4540)	
training:	Epoch: [2][116/233]	Loss 0.3398 (0.4531)	
training:	Epoch: [2][117/233]	Loss 0.6009 (0.4543)	
training:	Epoch: [2][118/233]	Loss 0.3441 (0.4534)	
training:	Epoch: [2][119/233]	Loss 0.4441 (0.4533)	
training:	Epoch: [2][120/233]	Loss 0.4051 (0.4529)	
training:	Epoch: [2][121/233]	Loss 0.3704 (0.4522)	
training:	Epoch: [2][122/233]	Loss 0.4281 (0.4520)	
training:	Epoch: [2][123/233]	Loss 0.5829 (0.4531)	
training:	Epoch: [2][124/233]	Loss 0.5008 (0.4535)	
training:	Epoch: [2][125/233]	Loss 0.3807 (0.4529)	
training:	Epoch: [2][126/233]	Loss 0.4378 (0.4528)	
training:	Epoch: [2][127/233]	Loss 0.3131 (0.4517)	
training:	Epoch: [2][128/233]	Loss 0.4747 (0.4519)	
training:	Epoch: [2][129/233]	Loss 0.4375 (0.4517)	
training:	Epoch: [2][130/233]	Loss 0.3990 (0.4513)	
training:	Epoch: [2][131/233]	Loss 0.3957 (0.4509)	
training:	Epoch: [2][132/233]	Loss 0.3418 (0.4501)	
training:	Epoch: [2][133/233]	Loss 0.5483 (0.4508)	
training:	Epoch: [2][134/233]	Loss 0.4453 (0.4508)	
training:	Epoch: [2][135/233]	Loss 0.4080 (0.4505)	
training:	Epoch: [2][136/233]	Loss 0.4245 (0.4503)	
training:	Epoch: [2][137/233]	Loss 0.5581 (0.4511)	
training:	Epoch: [2][138/233]	Loss 0.4285 (0.4509)	
training:	Epoch: [2][139/233]	Loss 0.5546 (0.4516)	
training:	Epoch: [2][140/233]	Loss 0.3569 (0.4510)	
training:	Epoch: [2][141/233]	Loss 0.4505 (0.4510)	
training:	Epoch: [2][142/233]	Loss 0.4062 (0.4507)	
training:	Epoch: [2][143/233]	Loss 0.5945 (0.4517)	
training:	Epoch: [2][144/233]	Loss 0.4079 (0.4514)	
training:	Epoch: [2][145/233]	Loss 0.4092 (0.4511)	
training:	Epoch: [2][146/233]	Loss 0.4465 (0.4510)	
training:	Epoch: [2][147/233]	Loss 0.4637 (0.4511)	
training:	Epoch: [2][148/233]	Loss 0.4150 (0.4509)	
training:	Epoch: [2][149/233]	Loss 0.3983 (0.4505)	
training:	Epoch: [2][150/233]	Loss 0.5251 (0.4510)	
training:	Epoch: [2][151/233]	Loss 0.4929 (0.4513)	
training:	Epoch: [2][152/233]	Loss 0.4600 (0.4514)	
training:	Epoch: [2][153/233]	Loss 0.3756 (0.4509)	
training:	Epoch: [2][154/233]	Loss 0.3846 (0.4504)	
training:	Epoch: [2][155/233]	Loss 0.3891 (0.4500)	
training:	Epoch: [2][156/233]	Loss 0.3317 (0.4493)	
training:	Epoch: [2][157/233]	Loss 0.4129 (0.4490)	
training:	Epoch: [2][158/233]	Loss 0.5070 (0.4494)	
training:	Epoch: [2][159/233]	Loss 0.3791 (0.4490)	
training:	Epoch: [2][160/233]	Loss 0.4783 (0.4492)	
training:	Epoch: [2][161/233]	Loss 0.3858 (0.4488)	
training:	Epoch: [2][162/233]	Loss 0.3794 (0.4483)	
training:	Epoch: [2][163/233]	Loss 0.4133 (0.4481)	
training:	Epoch: [2][164/233]	Loss 0.5274 (0.4486)	
training:	Epoch: [2][165/233]	Loss 0.4777 (0.4488)	
training:	Epoch: [2][166/233]	Loss 0.3187 (0.4480)	
training:	Epoch: [2][167/233]	Loss 0.4122 (0.4478)	
training:	Epoch: [2][168/233]	Loss 0.4311 (0.4477)	
training:	Epoch: [2][169/233]	Loss 0.4244 (0.4475)	
training:	Epoch: [2][170/233]	Loss 0.4062 (0.4473)	
training:	Epoch: [2][171/233]	Loss 0.4358 (0.4472)	
training:	Epoch: [2][172/233]	Loss 0.4798 (0.4474)	
training:	Epoch: [2][173/233]	Loss 0.5057 (0.4478)	
training:	Epoch: [2][174/233]	Loss 0.3884 (0.4474)	
training:	Epoch: [2][175/233]	Loss 0.3483 (0.4468)	
training:	Epoch: [2][176/233]	Loss 0.4928 (0.4471)	
training:	Epoch: [2][177/233]	Loss 0.5012 (0.4474)	
training:	Epoch: [2][178/233]	Loss 0.4593 (0.4475)	
training:	Epoch: [2][179/233]	Loss 0.4567 (0.4475)	
training:	Epoch: [2][180/233]	Loss 0.4777 (0.4477)	
training:	Epoch: [2][181/233]	Loss 0.3962 (0.4474)	
training:	Epoch: [2][182/233]	Loss 0.4922 (0.4477)	
training:	Epoch: [2][183/233]	Loss 0.4986 (0.4479)	
training:	Epoch: [2][184/233]	Loss 0.4502 (0.4480)	
training:	Epoch: [2][185/233]	Loss 0.5103 (0.4483)	
training:	Epoch: [2][186/233]	Loss 0.4804 (0.4485)	
training:	Epoch: [2][187/233]	Loss 0.4609 (0.4485)	
training:	Epoch: [2][188/233]	Loss 0.4637 (0.4486)	
training:	Epoch: [2][189/233]	Loss 0.4567 (0.4487)	
training:	Epoch: [2][190/233]	Loss 0.3803 (0.4483)	
training:	Epoch: [2][191/233]	Loss 0.4426 (0.4483)	
training:	Epoch: [2][192/233]	Loss 0.4271 (0.4482)	
training:	Epoch: [2][193/233]	Loss 0.3205 (0.4475)	
training:	Epoch: [2][194/233]	Loss 0.5109 (0.4478)	
training:	Epoch: [2][195/233]	Loss 0.3807 (0.4475)	
training:	Epoch: [2][196/233]	Loss 0.4651 (0.4476)	
training:	Epoch: [2][197/233]	Loss 0.4551 (0.4476)	
training:	Epoch: [2][198/233]	Loss 0.4455 (0.4476)	
training:	Epoch: [2][199/233]	Loss 0.3952 (0.4473)	
training:	Epoch: [2][200/233]	Loss 0.3103 (0.4466)	
training:	Epoch: [2][201/233]	Loss 0.4340 (0.4466)	
training:	Epoch: [2][202/233]	Loss 0.2713 (0.4457)	
training:	Epoch: [2][203/233]	Loss 0.2993 (0.4450)	
training:	Epoch: [2][204/233]	Loss 0.3953 (0.4447)	
training:	Epoch: [2][205/233]	Loss 0.4133 (0.4446)	
training:	Epoch: [2][206/233]	Loss 0.4258 (0.4445)	
training:	Epoch: [2][207/233]	Loss 0.4834 (0.4447)	
training:	Epoch: [2][208/233]	Loss 0.4447 (0.4447)	
training:	Epoch: [2][209/233]	Loss 0.3621 (0.4443)	
training:	Epoch: [2][210/233]	Loss 0.3725 (0.4440)	
training:	Epoch: [2][211/233]	Loss 0.5477 (0.4444)	
training:	Epoch: [2][212/233]	Loss 0.4090 (0.4443)	
training:	Epoch: [2][213/233]	Loss 0.4387 (0.4443)	
training:	Epoch: [2][214/233]	Loss 0.3943 (0.4440)	
training:	Epoch: [2][215/233]	Loss 0.3600 (0.4436)	
training:	Epoch: [2][216/233]	Loss 0.4577 (0.4437)	
training:	Epoch: [2][217/233]	Loss 0.3859 (0.4434)	
training:	Epoch: [2][218/233]	Loss 0.3709 (0.4431)	
training:	Epoch: [2][219/233]	Loss 0.5207 (0.4434)	
training:	Epoch: [2][220/233]	Loss 0.4334 (0.4434)	
training:	Epoch: [2][221/233]	Loss 0.3663 (0.4431)	
training:	Epoch: [2][222/233]	Loss 0.3082 (0.4424)	
training:	Epoch: [2][223/233]	Loss 0.4992 (0.4427)	
training:	Epoch: [2][224/233]	Loss 0.4165 (0.4426)	
training:	Epoch: [2][225/233]	Loss 0.5443 (0.4430)	
training:	Epoch: [2][226/233]	Loss 0.4538 (0.4431)	
training:	Epoch: [2][227/233]	Loss 0.4031 (0.4429)	
training:	Epoch: [2][228/233]	Loss 0.4053 (0.4427)	
training:	Epoch: [2][229/233]	Loss 0.4163 (0.4426)	
training:	Epoch: [2][230/233]	Loss 0.3971 (0.4424)	
training:	Epoch: [2][231/233]	Loss 0.4945 (0.4427)	
training:	Epoch: [2][232/233]	Loss 0.3697 (0.4423)	
training:	Epoch: [2][233/233]	Loss 0.4910 (0.4425)	
Training:	 Loss: 0.4415

Training:	 ACC: 0.8582 0.8553 0.7945 0.9219
Validation:	 ACC: 0.7864 0.7838 0.7308 0.8419
Validation:	 Best_BACC: 0.7864 0.7838 0.7308 0.8419
Validation:	 Loss: 0.4548
Pretraining:	Epoch 3/200
----------
training:	Epoch: [3][1/233]	Loss 0.3634 (0.3634)	
training:	Epoch: [3][2/233]	Loss 0.3945 (0.3790)	
training:	Epoch: [3][3/233]	Loss 0.4367 (0.3982)	
training:	Epoch: [3][4/233]	Loss 0.3364 (0.3827)	
training:	Epoch: [3][5/233]	Loss 0.4476 (0.3957)	
training:	Epoch: [3][6/233]	Loss 0.4956 (0.4124)	
training:	Epoch: [3][7/233]	Loss 0.3609 (0.4050)	
training:	Epoch: [3][8/233]	Loss 0.3904 (0.4032)	
training:	Epoch: [3][9/233]	Loss 0.4254 (0.4057)	
training:	Epoch: [3][10/233]	Loss 0.3873 (0.4038)	
training:	Epoch: [3][11/233]	Loss 0.3526 (0.3992)	
training:	Epoch: [3][12/233]	Loss 0.4383 (0.4024)	
training:	Epoch: [3][13/233]	Loss 0.3371 (0.3974)	
training:	Epoch: [3][14/233]	Loss 0.3571 (0.3945)	
training:	Epoch: [3][15/233]	Loss 0.4892 (0.4008)	
training:	Epoch: [3][16/233]	Loss 0.3862 (0.3999)	
training:	Epoch: [3][17/233]	Loss 0.3510 (0.3970)	
training:	Epoch: [3][18/233]	Loss 0.3859 (0.3964)	
training:	Epoch: [3][19/233]	Loss 0.3589 (0.3945)	
training:	Epoch: [3][20/233]	Loss 0.3321 (0.3913)	
training:	Epoch: [3][21/233]	Loss 0.3458 (0.3892)	
training:	Epoch: [3][22/233]	Loss 0.3064 (0.3854)	
training:	Epoch: [3][23/233]	Loss 0.3910 (0.3856)	
training:	Epoch: [3][24/233]	Loss 0.4451 (0.3881)	
training:	Epoch: [3][25/233]	Loss 0.3819 (0.3879)	
training:	Epoch: [3][26/233]	Loss 0.3758 (0.3874)	
training:	Epoch: [3][27/233]	Loss 0.3531 (0.3861)	
training:	Epoch: [3][28/233]	Loss 0.3220 (0.3839)	
training:	Epoch: [3][29/233]	Loss 0.2550 (0.3794)	
training:	Epoch: [3][30/233]	Loss 0.3487 (0.3784)	
training:	Epoch: [3][31/233]	Loss 0.4089 (0.3794)	
training:	Epoch: [3][32/233]	Loss 0.5340 (0.3842)	
training:	Epoch: [3][33/233]	Loss 0.3702 (0.3838)	
training:	Epoch: [3][34/233]	Loss 0.3525 (0.3829)	
training:	Epoch: [3][35/233]	Loss 0.2811 (0.3800)	
training:	Epoch: [3][36/233]	Loss 0.3452 (0.3790)	
training:	Epoch: [3][37/233]	Loss 0.3351 (0.3778)	
training:	Epoch: [3][38/233]	Loss 0.4205 (0.3789)	
training:	Epoch: [3][39/233]	Loss 0.3583 (0.3784)	
training:	Epoch: [3][40/233]	Loss 0.3789 (0.3784)	
training:	Epoch: [3][41/233]	Loss 0.2581 (0.3755)	
training:	Epoch: [3][42/233]	Loss 0.4375 (0.3770)	
training:	Epoch: [3][43/233]	Loss 0.3963 (0.3774)	
training:	Epoch: [3][44/233]	Loss 0.4103 (0.3781)	
training:	Epoch: [3][45/233]	Loss 0.3488 (0.3775)	
training:	Epoch: [3][46/233]	Loss 0.4864 (0.3799)	
training:	Epoch: [3][47/233]	Loss 0.3723 (0.3797)	
training:	Epoch: [3][48/233]	Loss 0.3000 (0.3780)	
training:	Epoch: [3][49/233]	Loss 0.3593 (0.3777)	
training:	Epoch: [3][50/233]	Loss 0.4860 (0.3798)	
training:	Epoch: [3][51/233]	Loss 0.3889 (0.3800)	
training:	Epoch: [3][52/233]	Loss 0.3604 (0.3796)	
training:	Epoch: [3][53/233]	Loss 0.3935 (0.3799)	
training:	Epoch: [3][54/233]	Loss 0.2936 (0.3783)	
training:	Epoch: [3][55/233]	Loss 0.3306 (0.3774)	
training:	Epoch: [3][56/233]	Loss 0.4496 (0.3787)	
training:	Epoch: [3][57/233]	Loss 0.3912 (0.3789)	
training:	Epoch: [3][58/233]	Loss 0.3228 (0.3780)	
training:	Epoch: [3][59/233]	Loss 0.4386 (0.3790)	
training:	Epoch: [3][60/233]	Loss 0.3138 (0.3779)	
training:	Epoch: [3][61/233]	Loss 0.3640 (0.3777)	
training:	Epoch: [3][62/233]	Loss 0.2979 (0.3764)	
training:	Epoch: [3][63/233]	Loss 0.4222 (0.3771)	
training:	Epoch: [3][64/233]	Loss 0.2541 (0.3752)	
training:	Epoch: [3][65/233]	Loss 0.3364 (0.3746)	
training:	Epoch: [3][66/233]	Loss 0.3760 (0.3746)	
training:	Epoch: [3][67/233]	Loss 0.2575 (0.3729)	
training:	Epoch: [3][68/233]	Loss 0.4129 (0.3735)	
training:	Epoch: [3][69/233]	Loss 0.5570 (0.3761)	
training:	Epoch: [3][70/233]	Loss 0.3393 (0.3756)	
training:	Epoch: [3][71/233]	Loss 0.3967 (0.3759)	
training:	Epoch: [3][72/233]	Loss 0.3269 (0.3752)	
training:	Epoch: [3][73/233]	Loss 0.3381 (0.3747)	
training:	Epoch: [3][74/233]	Loss 0.3302 (0.3741)	
training:	Epoch: [3][75/233]	Loss 0.4178 (0.3747)	
training:	Epoch: [3][76/233]	Loss 0.3415 (0.3742)	
training:	Epoch: [3][77/233]	Loss 0.4693 (0.3755)	
training:	Epoch: [3][78/233]	Loss 0.3326 (0.3749)	
training:	Epoch: [3][79/233]	Loss 0.3134 (0.3742)	
training:	Epoch: [3][80/233]	Loss 0.4951 (0.3757)	
training:	Epoch: [3][81/233]	Loss 0.3774 (0.3757)	
training:	Epoch: [3][82/233]	Loss 0.3803 (0.3757)	
training:	Epoch: [3][83/233]	Loss 0.3610 (0.3756)	
training:	Epoch: [3][84/233]	Loss 0.4251 (0.3762)	
training:	Epoch: [3][85/233]	Loss 0.5041 (0.3777)	
training:	Epoch: [3][86/233]	Loss 0.4337 (0.3783)	
training:	Epoch: [3][87/233]	Loss 0.2797 (0.3772)	
training:	Epoch: [3][88/233]	Loss 0.3977 (0.3774)	
training:	Epoch: [3][89/233]	Loss 0.4239 (0.3779)	
training:	Epoch: [3][90/233]	Loss 0.4699 (0.3790)	
training:	Epoch: [3][91/233]	Loss 0.4023 (0.3792)	
training:	Epoch: [3][92/233]	Loss 0.3924 (0.3794)	
training:	Epoch: [3][93/233]	Loss 0.2864 (0.3784)	
training:	Epoch: [3][94/233]	Loss 0.4801 (0.3794)	
training:	Epoch: [3][95/233]	Loss 0.2855 (0.3784)	
training:	Epoch: [3][96/233]	Loss 0.3763 (0.3784)	
training:	Epoch: [3][97/233]	Loss 0.3426 (0.3781)	
training:	Epoch: [3][98/233]	Loss 0.2801 (0.3771)	
training:	Epoch: [3][99/233]	Loss 0.3198 (0.3765)	
training:	Epoch: [3][100/233]	Loss 0.4108 (0.3768)	
training:	Epoch: [3][101/233]	Loss 0.3331 (0.3764)	
training:	Epoch: [3][102/233]	Loss 0.4438 (0.3771)	
training:	Epoch: [3][103/233]	Loss 0.2698 (0.3760)	
training:	Epoch: [3][104/233]	Loss 0.3088 (0.3754)	
training:	Epoch: [3][105/233]	Loss 0.3234 (0.3749)	
training:	Epoch: [3][106/233]	Loss 0.3702 (0.3748)	
training:	Epoch: [3][107/233]	Loss 0.4669 (0.3757)	
training:	Epoch: [3][108/233]	Loss 0.3216 (0.3752)	
training:	Epoch: [3][109/233]	Loss 0.5582 (0.3769)	
training:	Epoch: [3][110/233]	Loss 0.4903 (0.3779)	
training:	Epoch: [3][111/233]	Loss 0.3790 (0.3779)	
training:	Epoch: [3][112/233]	Loss 0.4208 (0.3783)	
training:	Epoch: [3][113/233]	Loss 0.3850 (0.3783)	
training:	Epoch: [3][114/233]	Loss 0.3010 (0.3777)	
training:	Epoch: [3][115/233]	Loss 0.4756 (0.3785)	
training:	Epoch: [3][116/233]	Loss 0.3628 (0.3784)	
training:	Epoch: [3][117/233]	Loss 0.3639 (0.3783)	
training:	Epoch: [3][118/233]	Loss 0.2845 (0.3775)	
training:	Epoch: [3][119/233]	Loss 0.4547 (0.3781)	
training:	Epoch: [3][120/233]	Loss 0.4351 (0.3786)	
training:	Epoch: [3][121/233]	Loss 0.4834 (0.3795)	
training:	Epoch: [3][122/233]	Loss 0.4515 (0.3800)	
training:	Epoch: [3][123/233]	Loss 0.3483 (0.3798)	
training:	Epoch: [3][124/233]	Loss 0.4097 (0.3800)	
training:	Epoch: [3][125/233]	Loss 0.4680 (0.3807)	
training:	Epoch: [3][126/233]	Loss 0.4116 (0.3810)	
training:	Epoch: [3][127/233]	Loss 0.3232 (0.3805)	
training:	Epoch: [3][128/233]	Loss 0.3680 (0.3804)	
training:	Epoch: [3][129/233]	Loss 0.3825 (0.3804)	
training:	Epoch: [3][130/233]	Loss 0.3842 (0.3805)	
training:	Epoch: [3][131/233]	Loss 0.3807 (0.3805)	
training:	Epoch: [3][132/233]	Loss 0.3014 (0.3799)	
training:	Epoch: [3][133/233]	Loss 0.3482 (0.3796)	
training:	Epoch: [3][134/233]	Loss 0.4233 (0.3800)	
training:	Epoch: [3][135/233]	Loss 0.3525 (0.3798)	
training:	Epoch: [3][136/233]	Loss 0.3389 (0.3795)	
training:	Epoch: [3][137/233]	Loss 0.3980 (0.3796)	
training:	Epoch: [3][138/233]	Loss 0.3292 (0.3792)	
training:	Epoch: [3][139/233]	Loss 0.2293 (0.3781)	
training:	Epoch: [3][140/233]	Loss 0.3848 (0.3782)	
training:	Epoch: [3][141/233]	Loss 0.4114 (0.3784)	
training:	Epoch: [3][142/233]	Loss 0.4045 (0.3786)	
training:	Epoch: [3][143/233]	Loss 0.4056 (0.3788)	
training:	Epoch: [3][144/233]	Loss 0.4063 (0.3790)	
training:	Epoch: [3][145/233]	Loss 0.3110 (0.3785)	
training:	Epoch: [3][146/233]	Loss 0.4361 (0.3789)	
training:	Epoch: [3][147/233]	Loss 0.5139 (0.3798)	
training:	Epoch: [3][148/233]	Loss 0.3774 (0.3798)	
training:	Epoch: [3][149/233]	Loss 0.3294 (0.3795)	
training:	Epoch: [3][150/233]	Loss 0.4131 (0.3797)	
training:	Epoch: [3][151/233]	Loss 0.3793 (0.3797)	
training:	Epoch: [3][152/233]	Loss 0.3715 (0.3796)	
training:	Epoch: [3][153/233]	Loss 0.4579 (0.3802)	
training:	Epoch: [3][154/233]	Loss 0.5236 (0.3811)	
training:	Epoch: [3][155/233]	Loss 0.3344 (0.3808)	
training:	Epoch: [3][156/233]	Loss 0.5012 (0.3816)	
training:	Epoch: [3][157/233]	Loss 0.3748 (0.3815)	
training:	Epoch: [3][158/233]	Loss 0.4963 (0.3822)	
training:	Epoch: [3][159/233]	Loss 0.3193 (0.3818)	
training:	Epoch: [3][160/233]	Loss 0.3311 (0.3815)	
training:	Epoch: [3][161/233]	Loss 0.5266 (0.3824)	
training:	Epoch: [3][162/233]	Loss 0.3104 (0.3820)	
training:	Epoch: [3][163/233]	Loss 0.3322 (0.3817)	
training:	Epoch: [3][164/233]	Loss 0.3310 (0.3814)	
training:	Epoch: [3][165/233]	Loss 0.3084 (0.3809)	
training:	Epoch: [3][166/233]	Loss 0.4188 (0.3812)	
training:	Epoch: [3][167/233]	Loss 0.3363 (0.3809)	
training:	Epoch: [3][168/233]	Loss 0.3006 (0.3804)	
training:	Epoch: [3][169/233]	Loss 0.3729 (0.3804)	
training:	Epoch: [3][170/233]	Loss 0.3754 (0.3803)	
training:	Epoch: [3][171/233]	Loss 0.4117 (0.3805)	
training:	Epoch: [3][172/233]	Loss 0.3828 (0.3805)	
training:	Epoch: [3][173/233]	Loss 0.3301 (0.3802)	
training:	Epoch: [3][174/233]	Loss 0.3677 (0.3802)	
training:	Epoch: [3][175/233]	Loss 0.3841 (0.3802)	
training:	Epoch: [3][176/233]	Loss 0.3810 (0.3802)	
training:	Epoch: [3][177/233]	Loss 0.4283 (0.3805)	
training:	Epoch: [3][178/233]	Loss 0.3254 (0.3802)	
training:	Epoch: [3][179/233]	Loss 0.4844 (0.3807)	
training:	Epoch: [3][180/233]	Loss 0.4233 (0.3810)	
training:	Epoch: [3][181/233]	Loss 0.3798 (0.3810)	
training:	Epoch: [3][182/233]	Loss 0.3561 (0.3808)	
training:	Epoch: [3][183/233]	Loss 0.3683 (0.3808)	
training:	Epoch: [3][184/233]	Loss 0.2643 (0.3801)	
training:	Epoch: [3][185/233]	Loss 0.3503 (0.3800)	
training:	Epoch: [3][186/233]	Loss 0.3994 (0.3801)	
training:	Epoch: [3][187/233]	Loss 0.4745 (0.3806)	
training:	Epoch: [3][188/233]	Loss 0.2621 (0.3800)	
training:	Epoch: [3][189/233]	Loss 0.4229 (0.3802)	
training:	Epoch: [3][190/233]	Loss 0.3870 (0.3802)	
training:	Epoch: [3][191/233]	Loss 0.2362 (0.3795)	
training:	Epoch: [3][192/233]	Loss 0.3649 (0.3794)	
training:	Epoch: [3][193/233]	Loss 0.3446 (0.3792)	
training:	Epoch: [3][194/233]	Loss 0.2948 (0.3788)	
training:	Epoch: [3][195/233]	Loss 0.3433 (0.3786)	
training:	Epoch: [3][196/233]	Loss 0.3070 (0.3782)	
training:	Epoch: [3][197/233]	Loss 0.4088 (0.3784)	
training:	Epoch: [3][198/233]	Loss 0.4244 (0.3786)	
training:	Epoch: [3][199/233]	Loss 0.3949 (0.3787)	
training:	Epoch: [3][200/233]	Loss 0.3537 (0.3786)	
training:	Epoch: [3][201/233]	Loss 0.3446 (0.3784)	
training:	Epoch: [3][202/233]	Loss 0.4850 (0.3789)	
training:	Epoch: [3][203/233]	Loss 0.4242 (0.3791)	
training:	Epoch: [3][204/233]	Loss 0.3526 (0.3790)	
training:	Epoch: [3][205/233]	Loss 0.4354 (0.3793)	
training:	Epoch: [3][206/233]	Loss 0.4003 (0.3794)	
training:	Epoch: [3][207/233]	Loss 0.3158 (0.3791)	
training:	Epoch: [3][208/233]	Loss 0.3825 (0.3791)	
training:	Epoch: [3][209/233]	Loss 0.3588 (0.3790)	
training:	Epoch: [3][210/233]	Loss 0.3857 (0.3790)	
training:	Epoch: [3][211/233]	Loss 0.4151 (0.3792)	
training:	Epoch: [3][212/233]	Loss 0.4343 (0.3795)	
training:	Epoch: [3][213/233]	Loss 0.5085 (0.3801)	
training:	Epoch: [3][214/233]	Loss 0.3275 (0.3798)	
training:	Epoch: [3][215/233]	Loss 0.4207 (0.3800)	
training:	Epoch: [3][216/233]	Loss 0.3583 (0.3799)	
training:	Epoch: [3][217/233]	Loss 0.3669 (0.3799)	
training:	Epoch: [3][218/233]	Loss 0.3254 (0.3796)	
training:	Epoch: [3][219/233]	Loss 0.4878 (0.3801)	
training:	Epoch: [3][220/233]	Loss 0.4744 (0.3805)	
training:	Epoch: [3][221/233]	Loss 0.3947 (0.3806)	
training:	Epoch: [3][222/233]	Loss 0.4255 (0.3808)	
training:	Epoch: [3][223/233]	Loss 0.4076 (0.3809)	
training:	Epoch: [3][224/233]	Loss 0.3229 (0.3807)	
training:	Epoch: [3][225/233]	Loss 0.2854 (0.3802)	
training:	Epoch: [3][226/233]	Loss 0.4354 (0.3805)	
training:	Epoch: [3][227/233]	Loss 0.3804 (0.3805)	
training:	Epoch: [3][228/233]	Loss 0.3523 (0.3804)	
training:	Epoch: [3][229/233]	Loss 0.2775 (0.3799)	
training:	Epoch: [3][230/233]	Loss 0.4615 (0.3803)	
training:	Epoch: [3][231/233]	Loss 0.3923 (0.3803)	
training:	Epoch: [3][232/233]	Loss 0.3441 (0.3802)	
training:	Epoch: [3][233/233]	Loss 0.3729 (0.3801)	
Training:	 Loss: 0.3793

Training:	 ACC: 0.8966 0.8952 0.8649 0.9283
Validation:	 ACC: 0.8065 0.8052 0.7779 0.8352
Validation:	 Best_BACC: 0.8065 0.8052 0.7779 0.8352
Validation:	 Loss: 0.4216
Pretraining:	Epoch 4/200
----------
training:	Epoch: [4][1/233]	Loss 0.3203 (0.3203)	
training:	Epoch: [4][2/233]	Loss 0.3599 (0.3401)	
training:	Epoch: [4][3/233]	Loss 0.4699 (0.3833)	
training:	Epoch: [4][4/233]	Loss 0.4372 (0.3968)	
training:	Epoch: [4][5/233]	Loss 0.3460 (0.3866)	
training:	Epoch: [4][6/233]	Loss 0.3105 (0.3740)	
training:	Epoch: [4][7/233]	Loss 0.3479 (0.3702)	
training:	Epoch: [4][8/233]	Loss 0.2867 (0.3598)	
training:	Epoch: [4][9/233]	Loss 0.2818 (0.3511)	
training:	Epoch: [4][10/233]	Loss 0.2605 (0.3421)	
training:	Epoch: [4][11/233]	Loss 0.3696 (0.3446)	
training:	Epoch: [4][12/233]	Loss 0.4072 (0.3498)	
training:	Epoch: [4][13/233]	Loss 0.3073 (0.3465)	
training:	Epoch: [4][14/233]	Loss 0.2697 (0.3410)	
training:	Epoch: [4][15/233]	Loss 0.3413 (0.3411)	
training:	Epoch: [4][16/233]	Loss 0.4511 (0.3479)	
training:	Epoch: [4][17/233]	Loss 0.3050 (0.3454)	
training:	Epoch: [4][18/233]	Loss 0.2623 (0.3408)	
training:	Epoch: [4][19/233]	Loss 0.3947 (0.3436)	
training:	Epoch: [4][20/233]	Loss 0.2741 (0.3402)	
training:	Epoch: [4][21/233]	Loss 0.5297 (0.3492)	
training:	Epoch: [4][22/233]	Loss 0.4224 (0.3525)	
training:	Epoch: [4][23/233]	Loss 0.4611 (0.3572)	
training:	Epoch: [4][24/233]	Loss 0.3396 (0.3565)	
training:	Epoch: [4][25/233]	Loss 0.2330 (0.3516)	
training:	Epoch: [4][26/233]	Loss 0.3018 (0.3496)	
training:	Epoch: [4][27/233]	Loss 0.3517 (0.3497)	
training:	Epoch: [4][28/233]	Loss 0.2994 (0.3479)	
training:	Epoch: [4][29/233]	Loss 0.3070 (0.3465)	
training:	Epoch: [4][30/233]	Loss 0.2689 (0.3439)	
training:	Epoch: [4][31/233]	Loss 0.3985 (0.3457)	
training:	Epoch: [4][32/233]	Loss 0.3403 (0.3455)	
training:	Epoch: [4][33/233]	Loss 0.4060 (0.3473)	
training:	Epoch: [4][34/233]	Loss 0.3292 (0.3468)	
training:	Epoch: [4][35/233]	Loss 0.3367 (0.3465)	
training:	Epoch: [4][36/233]	Loss 0.2759 (0.3446)	
training:	Epoch: [4][37/233]	Loss 0.2271 (0.3414)	
training:	Epoch: [4][38/233]	Loss 0.3318 (0.3411)	
training:	Epoch: [4][39/233]	Loss 0.3868 (0.3423)	
training:	Epoch: [4][40/233]	Loss 0.2814 (0.3408)	
training:	Epoch: [4][41/233]	Loss 0.3763 (0.3416)	
training:	Epoch: [4][42/233]	Loss 0.3697 (0.3423)	
training:	Epoch: [4][43/233]	Loss 0.3787 (0.3432)	
training:	Epoch: [4][44/233]	Loss 0.2890 (0.3419)	
training:	Epoch: [4][45/233]	Loss 0.2648 (0.3402)	
training:	Epoch: [4][46/233]	Loss 0.3426 (0.3403)	
training:	Epoch: [4][47/233]	Loss 0.4351 (0.3423)	
training:	Epoch: [4][48/233]	Loss 0.3129 (0.3417)	
training:	Epoch: [4][49/233]	Loss 0.3288 (0.3414)	
training:	Epoch: [4][50/233]	Loss 0.4022 (0.3426)	
training:	Epoch: [4][51/233]	Loss 0.3765 (0.3433)	
training:	Epoch: [4][52/233]	Loss 0.3919 (0.3442)	
training:	Epoch: [4][53/233]	Loss 0.2992 (0.3434)	
training:	Epoch: [4][54/233]	Loss 0.3698 (0.3439)	
training:	Epoch: [4][55/233]	Loss 0.3179 (0.3434)	
training:	Epoch: [4][56/233]	Loss 0.3719 (0.3439)	
training:	Epoch: [4][57/233]	Loss 0.4378 (0.3455)	
training:	Epoch: [4][58/233]	Loss 0.2465 (0.3438)	
training:	Epoch: [4][59/233]	Loss 0.3030 (0.3431)	
training:	Epoch: [4][60/233]	Loss 0.2922 (0.3423)	
training:	Epoch: [4][61/233]	Loss 0.2894 (0.3414)	
training:	Epoch: [4][62/233]	Loss 0.3268 (0.3412)	
training:	Epoch: [4][63/233]	Loss 0.3823 (0.3419)	
training:	Epoch: [4][64/233]	Loss 0.3265 (0.3416)	
training:	Epoch: [4][65/233]	Loss 0.3558 (0.3418)	
training:	Epoch: [4][66/233]	Loss 0.3442 (0.3419)	
training:	Epoch: [4][67/233]	Loss 0.2276 (0.3402)	
training:	Epoch: [4][68/233]	Loss 0.4883 (0.3423)	
training:	Epoch: [4][69/233]	Loss 0.3711 (0.3428)	
training:	Epoch: [4][70/233]	Loss 0.3085 (0.3423)	
training:	Epoch: [4][71/233]	Loss 0.3867 (0.3429)	
training:	Epoch: [4][72/233]	Loss 0.3281 (0.3427)	
training:	Epoch: [4][73/233]	Loss 0.3511 (0.3428)	
training:	Epoch: [4][74/233]	Loss 0.3674 (0.3431)	
training:	Epoch: [4][75/233]	Loss 0.3195 (0.3428)	
training:	Epoch: [4][76/233]	Loss 0.3326 (0.3427)	
training:	Epoch: [4][77/233]	Loss 0.3129 (0.3423)	
training:	Epoch: [4][78/233]	Loss 0.3087 (0.3419)	
training:	Epoch: [4][79/233]	Loss 0.4147 (0.3428)	
training:	Epoch: [4][80/233]	Loss 0.2750 (0.3419)	
training:	Epoch: [4][81/233]	Loss 0.3937 (0.3426)	
training:	Epoch: [4][82/233]	Loss 0.2806 (0.3418)	
training:	Epoch: [4][83/233]	Loss 0.2334 (0.3405)	
training:	Epoch: [4][84/233]	Loss 0.3340 (0.3404)	
training:	Epoch: [4][85/233]	Loss 0.2756 (0.3397)	
training:	Epoch: [4][86/233]	Loss 0.2861 (0.3391)	
training:	Epoch: [4][87/233]	Loss 0.4701 (0.3406)	
training:	Epoch: [4][88/233]	Loss 0.2609 (0.3397)	
training:	Epoch: [4][89/233]	Loss 0.3481 (0.3398)	
training:	Epoch: [4][90/233]	Loss 0.4072 (0.3405)	
training:	Epoch: [4][91/233]	Loss 0.4594 (0.3418)	
training:	Epoch: [4][92/233]	Loss 0.2941 (0.3413)	
training:	Epoch: [4][93/233]	Loss 0.3847 (0.3418)	
training:	Epoch: [4][94/233]	Loss 0.3230 (0.3416)	
training:	Epoch: [4][95/233]	Loss 0.3396 (0.3415)	
training:	Epoch: [4][96/233]	Loss 0.2980 (0.3411)	
training:	Epoch: [4][97/233]	Loss 0.2687 (0.3403)	
training:	Epoch: [4][98/233]	Loss 0.2816 (0.3397)	
training:	Epoch: [4][99/233]	Loss 0.3906 (0.3402)	
training:	Epoch: [4][100/233]	Loss 0.2870 (0.3397)	
training:	Epoch: [4][101/233]	Loss 0.3668 (0.3400)	
training:	Epoch: [4][102/233]	Loss 0.2725 (0.3393)	
training:	Epoch: [4][103/233]	Loss 0.3973 (0.3399)	
training:	Epoch: [4][104/233]	Loss 0.3415 (0.3399)	
training:	Epoch: [4][105/233]	Loss 0.3477 (0.3400)	
training:	Epoch: [4][106/233]	Loss 0.4068 (0.3406)	
training:	Epoch: [4][107/233]	Loss 0.3240 (0.3405)	
training:	Epoch: [4][108/233]	Loss 0.3839 (0.3409)	
training:	Epoch: [4][109/233]	Loss 0.2665 (0.3402)	
training:	Epoch: [4][110/233]	Loss 0.2691 (0.3395)	
training:	Epoch: [4][111/233]	Loss 0.3280 (0.3394)	
training:	Epoch: [4][112/233]	Loss 0.2511 (0.3386)	
training:	Epoch: [4][113/233]	Loss 0.4728 (0.3398)	
training:	Epoch: [4][114/233]	Loss 0.3104 (0.3396)	
training:	Epoch: [4][115/233]	Loss 0.3034 (0.3392)	
training:	Epoch: [4][116/233]	Loss 0.3336 (0.3392)	
training:	Epoch: [4][117/233]	Loss 0.2659 (0.3386)	
training:	Epoch: [4][118/233]	Loss 0.2999 (0.3382)	
training:	Epoch: [4][119/233]	Loss 0.2806 (0.3378)	
training:	Epoch: [4][120/233]	Loss 0.3063 (0.3375)	
training:	Epoch: [4][121/233]	Loss 0.3706 (0.3378)	
training:	Epoch: [4][122/233]	Loss 0.3033 (0.3375)	
training:	Epoch: [4][123/233]	Loss 0.3143 (0.3373)	
training:	Epoch: [4][124/233]	Loss 0.3142 (0.3371)	
training:	Epoch: [4][125/233]	Loss 0.3195 (0.3370)	
training:	Epoch: [4][126/233]	Loss 0.3257 (0.3369)	
training:	Epoch: [4][127/233]	Loss 0.2895 (0.3365)	
training:	Epoch: [4][128/233]	Loss 0.2870 (0.3361)	
training:	Epoch: [4][129/233]	Loss 0.4304 (0.3369)	
training:	Epoch: [4][130/233]	Loss 0.3150 (0.3367)	
training:	Epoch: [4][131/233]	Loss 0.2918 (0.3363)	
training:	Epoch: [4][132/233]	Loss 0.2956 (0.3360)	
training:	Epoch: [4][133/233]	Loss 0.2764 (0.3356)	
training:	Epoch: [4][134/233]	Loss 0.4020 (0.3361)	
training:	Epoch: [4][135/233]	Loss 0.3810 (0.3364)	
training:	Epoch: [4][136/233]	Loss 0.4141 (0.3370)	
training:	Epoch: [4][137/233]	Loss 0.2934 (0.3367)	
training:	Epoch: [4][138/233]	Loss 0.2968 (0.3364)	
training:	Epoch: [4][139/233]	Loss 0.3446 (0.3364)	
training:	Epoch: [4][140/233]	Loss 0.2729 (0.3360)	
training:	Epoch: [4][141/233]	Loss 0.4314 (0.3367)	
training:	Epoch: [4][142/233]	Loss 0.3453 (0.3367)	
training:	Epoch: [4][143/233]	Loss 0.4233 (0.3373)	
training:	Epoch: [4][144/233]	Loss 0.3218 (0.3372)	
training:	Epoch: [4][145/233]	Loss 0.3696 (0.3374)	
training:	Epoch: [4][146/233]	Loss 0.4362 (0.3381)	
training:	Epoch: [4][147/233]	Loss 0.3456 (0.3382)	
training:	Epoch: [4][148/233]	Loss 0.3630 (0.3383)	
training:	Epoch: [4][149/233]	Loss 0.3017 (0.3381)	
training:	Epoch: [4][150/233]	Loss 0.3163 (0.3379)	
training:	Epoch: [4][151/233]	Loss 0.3120 (0.3378)	
training:	Epoch: [4][152/233]	Loss 0.2879 (0.3374)	
training:	Epoch: [4][153/233]	Loss 0.2493 (0.3369)	
training:	Epoch: [4][154/233]	Loss 0.3003 (0.3366)	
training:	Epoch: [4][155/233]	Loss 0.2457 (0.3360)	
training:	Epoch: [4][156/233]	Loss 0.3292 (0.3360)	
training:	Epoch: [4][157/233]	Loss 0.5952 (0.3377)	
training:	Epoch: [4][158/233]	Loss 0.4110 (0.3381)	
training:	Epoch: [4][159/233]	Loss 0.3523 (0.3382)	
training:	Epoch: [4][160/233]	Loss 0.3119 (0.3380)	
training:	Epoch: [4][161/233]	Loss 0.2879 (0.3377)	
training:	Epoch: [4][162/233]	Loss 0.3531 (0.3378)	
training:	Epoch: [4][163/233]	Loss 0.4587 (0.3386)	
training:	Epoch: [4][164/233]	Loss 0.3453 (0.3386)	
training:	Epoch: [4][165/233]	Loss 0.3533 (0.3387)	
training:	Epoch: [4][166/233]	Loss 0.2798 (0.3383)	
training:	Epoch: [4][167/233]	Loss 0.4424 (0.3390)	
training:	Epoch: [4][168/233]	Loss 0.3231 (0.3389)	
training:	Epoch: [4][169/233]	Loss 0.3181 (0.3387)	
training:	Epoch: [4][170/233]	Loss 0.3759 (0.3390)	
training:	Epoch: [4][171/233]	Loss 0.3166 (0.3388)	
training:	Epoch: [4][172/233]	Loss 0.2674 (0.3384)	
training:	Epoch: [4][173/233]	Loss 0.3128 (0.3383)	
training:	Epoch: [4][174/233]	Loss 0.2254 (0.3376)	
training:	Epoch: [4][175/233]	Loss 0.3815 (0.3379)	
training:	Epoch: [4][176/233]	Loss 0.3140 (0.3377)	
training:	Epoch: [4][177/233]	Loss 0.3365 (0.3377)	
training:	Epoch: [4][178/233]	Loss 0.2647 (0.3373)	
training:	Epoch: [4][179/233]	Loss 0.3862 (0.3376)	
training:	Epoch: [4][180/233]	Loss 0.2947 (0.3374)	
training:	Epoch: [4][181/233]	Loss 0.2460 (0.3368)	
training:	Epoch: [4][182/233]	Loss 0.2800 (0.3365)	
training:	Epoch: [4][183/233]	Loss 0.3618 (0.3367)	
training:	Epoch: [4][184/233]	Loss 0.3421 (0.3367)	
training:	Epoch: [4][185/233]	Loss 0.3128 (0.3366)	
training:	Epoch: [4][186/233]	Loss 0.3348 (0.3366)	
training:	Epoch: [4][187/233]	Loss 0.2664 (0.3362)	
training:	Epoch: [4][188/233]	Loss 0.3349 (0.3362)	
training:	Epoch: [4][189/233]	Loss 0.3662 (0.3363)	
training:	Epoch: [4][190/233]	Loss 0.3370 (0.3363)	
training:	Epoch: [4][191/233]	Loss 0.2892 (0.3361)	
training:	Epoch: [4][192/233]	Loss 0.3815 (0.3363)	
training:	Epoch: [4][193/233]	Loss 0.3150 (0.3362)	
training:	Epoch: [4][194/233]	Loss 0.2874 (0.3360)	
training:	Epoch: [4][195/233]	Loss 0.3481 (0.3360)	
training:	Epoch: [4][196/233]	Loss 0.3080 (0.3359)	
training:	Epoch: [4][197/233]	Loss 0.2533 (0.3355)	
training:	Epoch: [4][198/233]	Loss 0.3877 (0.3357)	
training:	Epoch: [4][199/233]	Loss 0.3160 (0.3356)	
training:	Epoch: [4][200/233]	Loss 0.2960 (0.3354)	
training:	Epoch: [4][201/233]	Loss 0.3936 (0.3357)	
training:	Epoch: [4][202/233]	Loss 0.3394 (0.3357)	
training:	Epoch: [4][203/233]	Loss 0.2652 (0.3354)	
training:	Epoch: [4][204/233]	Loss 0.2605 (0.3350)	
training:	Epoch: [4][205/233]	Loss 0.2562 (0.3346)	
training:	Epoch: [4][206/233]	Loss 0.3442 (0.3347)	
training:	Epoch: [4][207/233]	Loss 0.2809 (0.3344)	
training:	Epoch: [4][208/233]	Loss 0.3017 (0.3343)	
training:	Epoch: [4][209/233]	Loss 0.3626 (0.3344)	
training:	Epoch: [4][210/233]	Loss 0.4931 (0.3352)	
training:	Epoch: [4][211/233]	Loss 0.4309 (0.3356)	
training:	Epoch: [4][212/233]	Loss 0.3373 (0.3356)	
training:	Epoch: [4][213/233]	Loss 0.3000 (0.3355)	
training:	Epoch: [4][214/233]	Loss 0.3099 (0.3353)	
training:	Epoch: [4][215/233]	Loss 0.3501 (0.3354)	
training:	Epoch: [4][216/233]	Loss 0.2849 (0.3352)	
training:	Epoch: [4][217/233]	Loss 0.2990 (0.3350)	
training:	Epoch: [4][218/233]	Loss 0.2735 (0.3347)	
training:	Epoch: [4][219/233]	Loss 0.2707 (0.3344)	
training:	Epoch: [4][220/233]	Loss 0.3049 (0.3343)	
training:	Epoch: [4][221/233]	Loss 0.4218 (0.3347)	
training:	Epoch: [4][222/233]	Loss 0.4355 (0.3352)	
training:	Epoch: [4][223/233]	Loss 0.3349 (0.3352)	
training:	Epoch: [4][224/233]	Loss 0.2806 (0.3349)	
training:	Epoch: [4][225/233]	Loss 0.2659 (0.3346)	
training:	Epoch: [4][226/233]	Loss 0.2654 (0.3343)	
training:	Epoch: [4][227/233]	Loss 0.2332 (0.3338)	
training:	Epoch: [4][228/233]	Loss 0.3076 (0.3337)	
training:	Epoch: [4][229/233]	Loss 0.3171 (0.3337)	
training:	Epoch: [4][230/233]	Loss 0.2977 (0.3335)	
training:	Epoch: [4][231/233]	Loss 0.2567 (0.3332)	
training:	Epoch: [4][232/233]	Loss 0.2583 (0.3329)	
training:	Epoch: [4][233/233]	Loss 0.3201 (0.3328)	
Training:	 Loss: 0.3320

Training:	 ACC: 0.9204 0.9193 0.8957 0.9451
Validation:	 ACC: 0.8201 0.8192 0.7984 0.8419
Validation:	 Best_BACC: 0.8201 0.8192 0.7984 0.8419
Validation:	 Loss: 0.4063
Pretraining:	Epoch 5/200
----------
training:	Epoch: [5][1/233]	Loss 0.2969 (0.2969)	
training:	Epoch: [5][2/233]	Loss 0.2886 (0.2928)	
training:	Epoch: [5][3/233]	Loss 0.2610 (0.2822)	
training:	Epoch: [5][4/233]	Loss 0.3761 (0.3057)	
training:	Epoch: [5][5/233]	Loss 0.2126 (0.2871)	
training:	Epoch: [5][6/233]	Loss 0.2640 (0.2832)	
training:	Epoch: [5][7/233]	Loss 0.2404 (0.2771)	
training:	Epoch: [5][8/233]	Loss 0.3338 (0.2842)	
training:	Epoch: [5][9/233]	Loss 0.2247 (0.2776)	
training:	Epoch: [5][10/233]	Loss 0.4008 (0.2899)	
training:	Epoch: [5][11/233]	Loss 0.3330 (0.2938)	
training:	Epoch: [5][12/233]	Loss 0.3610 (0.2994)	
training:	Epoch: [5][13/233]	Loss 0.2795 (0.2979)	
training:	Epoch: [5][14/233]	Loss 0.2651 (0.2955)	
training:	Epoch: [5][15/233]	Loss 0.3190 (0.2971)	
training:	Epoch: [5][16/233]	Loss 0.2663 (0.2952)	
training:	Epoch: [5][17/233]	Loss 0.2304 (0.2914)	
training:	Epoch: [5][18/233]	Loss 0.3342 (0.2938)	
training:	Epoch: [5][19/233]	Loss 0.3043 (0.2943)	
training:	Epoch: [5][20/233]	Loss 0.3115 (0.2952)	
training:	Epoch: [5][21/233]	Loss 0.2781 (0.2944)	
training:	Epoch: [5][22/233]	Loss 0.3470 (0.2968)	
training:	Epoch: [5][23/233]	Loss 0.3917 (0.3009)	
training:	Epoch: [5][24/233]	Loss 0.2370 (0.2982)	
training:	Epoch: [5][25/233]	Loss 0.3445 (0.3001)	
training:	Epoch: [5][26/233]	Loss 0.3740 (0.3029)	
training:	Epoch: [5][27/233]	Loss 0.2754 (0.3019)	
training:	Epoch: [5][28/233]	Loss 0.3080 (0.3021)	
training:	Epoch: [5][29/233]	Loss 0.3365 (0.3033)	
training:	Epoch: [5][30/233]	Loss 0.3764 (0.3057)	
training:	Epoch: [5][31/233]	Loss 0.2523 (0.3040)	
training:	Epoch: [5][32/233]	Loss 0.2481 (0.3023)	
training:	Epoch: [5][33/233]	Loss 0.2315 (0.3001)	
training:	Epoch: [5][34/233]	Loss 0.2525 (0.2987)	
training:	Epoch: [5][35/233]	Loss 0.2877 (0.2984)	
training:	Epoch: [5][36/233]	Loss 0.3131 (0.2988)	
training:	Epoch: [5][37/233]	Loss 0.1900 (0.2959)	
training:	Epoch: [5][38/233]	Loss 0.2255 (0.2940)	
training:	Epoch: [5][39/233]	Loss 0.2860 (0.2938)	
training:	Epoch: [5][40/233]	Loss 0.2761 (0.2934)	
training:	Epoch: [5][41/233]	Loss 0.2558 (0.2925)	
training:	Epoch: [5][42/233]	Loss 0.3467 (0.2938)	
training:	Epoch: [5][43/233]	Loss 0.2925 (0.2937)	
training:	Epoch: [5][44/233]	Loss 0.4250 (0.2967)	
training:	Epoch: [5][45/233]	Loss 0.2550 (0.2958)	
training:	Epoch: [5][46/233]	Loss 0.3355 (0.2966)	
training:	Epoch: [5][47/233]	Loss 0.2834 (0.2964)	
training:	Epoch: [5][48/233]	Loss 0.3014 (0.2965)	
training:	Epoch: [5][49/233]	Loss 0.3664 (0.2979)	
training:	Epoch: [5][50/233]	Loss 0.3046 (0.2980)	
training:	Epoch: [5][51/233]	Loss 0.3521 (0.2991)	
training:	Epoch: [5][52/233]	Loss 0.2401 (0.2980)	
training:	Epoch: [5][53/233]	Loss 0.1930 (0.2960)	
training:	Epoch: [5][54/233]	Loss 0.2985 (0.2960)	
training:	Epoch: [5][55/233]	Loss 0.2992 (0.2961)	
training:	Epoch: [5][56/233]	Loss 0.2840 (0.2959)	
training:	Epoch: [5][57/233]	Loss 0.4332 (0.2983)	
training:	Epoch: [5][58/233]	Loss 0.3334 (0.2989)	
training:	Epoch: [5][59/233]	Loss 0.3761 (0.3002)	
training:	Epoch: [5][60/233]	Loss 0.2633 (0.2996)	
training:	Epoch: [5][61/233]	Loss 0.3047 (0.2997)	
training:	Epoch: [5][62/233]	Loss 0.3239 (0.3001)	
training:	Epoch: [5][63/233]	Loss 0.2992 (0.3000)	
training:	Epoch: [5][64/233]	Loss 0.2943 (0.2999)	
training:	Epoch: [5][65/233]	Loss 0.3170 (0.3002)	
training:	Epoch: [5][66/233]	Loss 0.2058 (0.2988)	
training:	Epoch: [5][67/233]	Loss 0.3198 (0.2991)	
training:	Epoch: [5][68/233]	Loss 0.2504 (0.2984)	
training:	Epoch: [5][69/233]	Loss 0.2768 (0.2981)	
training:	Epoch: [5][70/233]	Loss 0.2701 (0.2977)	
training:	Epoch: [5][71/233]	Loss 0.3486 (0.2984)	
training:	Epoch: [5][72/233]	Loss 0.2759 (0.2981)	
training:	Epoch: [5][73/233]	Loss 0.2018 (0.2968)	
training:	Epoch: [5][74/233]	Loss 0.2648 (0.2963)	
training:	Epoch: [5][75/233]	Loss 0.3145 (0.2966)	
training:	Epoch: [5][76/233]	Loss 0.2113 (0.2954)	
training:	Epoch: [5][77/233]	Loss 0.2805 (0.2952)	
training:	Epoch: [5][78/233]	Loss 0.3709 (0.2962)	
training:	Epoch: [5][79/233]	Loss 0.2775 (0.2960)	
training:	Epoch: [5][80/233]	Loss 0.3500 (0.2967)	
training:	Epoch: [5][81/233]	Loss 0.3077 (0.2968)	
training:	Epoch: [5][82/233]	Loss 0.2552 (0.2963)	
training:	Epoch: [5][83/233]	Loss 0.2703 (0.2960)	
training:	Epoch: [5][84/233]	Loss 0.4340 (0.2976)	
training:	Epoch: [5][85/233]	Loss 0.3426 (0.2981)	
training:	Epoch: [5][86/233]	Loss 0.3161 (0.2984)	
training:	Epoch: [5][87/233]	Loss 0.2591 (0.2979)	
training:	Epoch: [5][88/233]	Loss 0.2732 (0.2976)	
training:	Epoch: [5][89/233]	Loss 0.3275 (0.2980)	
training:	Epoch: [5][90/233]	Loss 0.3713 (0.2988)	
training:	Epoch: [5][91/233]	Loss 0.4117 (0.3000)	
training:	Epoch: [5][92/233]	Loss 0.3324 (0.3004)	
training:	Epoch: [5][93/233]	Loss 0.2405 (0.2997)	
training:	Epoch: [5][94/233]	Loss 0.2526 (0.2992)	
training:	Epoch: [5][95/233]	Loss 0.2840 (0.2991)	
training:	Epoch: [5][96/233]	Loss 0.2868 (0.2989)	
training:	Epoch: [5][97/233]	Loss 0.2995 (0.2989)	
training:	Epoch: [5][98/233]	Loss 0.3077 (0.2990)	
training:	Epoch: [5][99/233]	Loss 0.3083 (0.2991)	
training:	Epoch: [5][100/233]	Loss 0.3219 (0.2993)	
training:	Epoch: [5][101/233]	Loss 0.3013 (0.2994)	
training:	Epoch: [5][102/233]	Loss 0.3380 (0.2997)	
training:	Epoch: [5][103/233]	Loss 0.2102 (0.2989)	
training:	Epoch: [5][104/233]	Loss 0.2630 (0.2985)	
training:	Epoch: [5][105/233]	Loss 0.3343 (0.2989)	
training:	Epoch: [5][106/233]	Loss 0.3177 (0.2990)	
training:	Epoch: [5][107/233]	Loss 0.2802 (0.2989)	
training:	Epoch: [5][108/233]	Loss 0.2374 (0.2983)	
training:	Epoch: [5][109/233]	Loss 0.1870 (0.2973)	
training:	Epoch: [5][110/233]	Loss 0.2100 (0.2965)	
training:	Epoch: [5][111/233]	Loss 0.3085 (0.2966)	
training:	Epoch: [5][112/233]	Loss 0.3225 (0.2968)	
training:	Epoch: [5][113/233]	Loss 0.3278 (0.2971)	
training:	Epoch: [5][114/233]	Loss 0.2991 (0.2971)	
training:	Epoch: [5][115/233]	Loss 0.3162 (0.2973)	
training:	Epoch: [5][116/233]	Loss 0.3332 (0.2976)	
training:	Epoch: [5][117/233]	Loss 0.2579 (0.2973)	
training:	Epoch: [5][118/233]	Loss 0.4120 (0.2982)	
training:	Epoch: [5][119/233]	Loss 0.3312 (0.2985)	
training:	Epoch: [5][120/233]	Loss 0.3080 (0.2986)	
training:	Epoch: [5][121/233]	Loss 0.2708 (0.2984)	
training:	Epoch: [5][122/233]	Loss 0.3479 (0.2988)	
training:	Epoch: [5][123/233]	Loss 0.2423 (0.2983)	
training:	Epoch: [5][124/233]	Loss 0.2505 (0.2979)	
training:	Epoch: [5][125/233]	Loss 0.2409 (0.2975)	
training:	Epoch: [5][126/233]	Loss 0.2879 (0.2974)	
training:	Epoch: [5][127/233]	Loss 0.2844 (0.2973)	
training:	Epoch: [5][128/233]	Loss 0.3195 (0.2975)	
training:	Epoch: [5][129/233]	Loss 0.2450 (0.2971)	
training:	Epoch: [5][130/233]	Loss 0.2610 (0.2968)	
training:	Epoch: [5][131/233]	Loss 0.3302 (0.2970)	
training:	Epoch: [5][132/233]	Loss 0.2092 (0.2964)	
training:	Epoch: [5][133/233]	Loss 0.2573 (0.2961)	
training:	Epoch: [5][134/233]	Loss 0.2698 (0.2959)	
training:	Epoch: [5][135/233]	Loss 0.2710 (0.2957)	
training:	Epoch: [5][136/233]	Loss 0.2607 (0.2954)	
training:	Epoch: [5][137/233]	Loss 0.3972 (0.2962)	
training:	Epoch: [5][138/233]	Loss 0.2316 (0.2957)	
training:	Epoch: [5][139/233]	Loss 0.3264 (0.2959)	
training:	Epoch: [5][140/233]	Loss 0.3571 (0.2964)	
training:	Epoch: [5][141/233]	Loss 0.2426 (0.2960)	
training:	Epoch: [5][142/233]	Loss 0.2835 (0.2959)	
training:	Epoch: [5][143/233]	Loss 0.3027 (0.2959)	
training:	Epoch: [5][144/233]	Loss 0.2616 (0.2957)	
training:	Epoch: [5][145/233]	Loss 0.2946 (0.2957)	
training:	Epoch: [5][146/233]	Loss 0.2050 (0.2951)	
training:	Epoch: [5][147/233]	Loss 0.2884 (0.2950)	
training:	Epoch: [5][148/233]	Loss 0.3494 (0.2954)	
training:	Epoch: [5][149/233]	Loss 0.2573 (0.2951)	
training:	Epoch: [5][150/233]	Loss 0.2833 (0.2951)	
training:	Epoch: [5][151/233]	Loss 0.3225 (0.2952)	
training:	Epoch: [5][152/233]	Loss 0.2558 (0.2950)	
training:	Epoch: [5][153/233]	Loss 0.2339 (0.2946)	
training:	Epoch: [5][154/233]	Loss 0.3854 (0.2952)	
training:	Epoch: [5][155/233]	Loss 0.2753 (0.2950)	
training:	Epoch: [5][156/233]	Loss 0.4164 (0.2958)	
training:	Epoch: [5][157/233]	Loss 0.3616 (0.2962)	
training:	Epoch: [5][158/233]	Loss 0.3058 (0.2963)	
training:	Epoch: [5][159/233]	Loss 0.2926 (0.2963)	
training:	Epoch: [5][160/233]	Loss 0.2571 (0.2960)	
training:	Epoch: [5][161/233]	Loss 0.3807 (0.2966)	
training:	Epoch: [5][162/233]	Loss 0.3638 (0.2970)	
training:	Epoch: [5][163/233]	Loss 0.2775 (0.2969)	
training:	Epoch: [5][164/233]	Loss 0.3131 (0.2970)	
training:	Epoch: [5][165/233]	Loss 0.2382 (0.2966)	
training:	Epoch: [5][166/233]	Loss 0.2818 (0.2965)	
training:	Epoch: [5][167/233]	Loss 0.3510 (0.2968)	
training:	Epoch: [5][168/233]	Loss 0.2712 (0.2967)	
training:	Epoch: [5][169/233]	Loss 0.2948 (0.2967)	
training:	Epoch: [5][170/233]	Loss 0.2516 (0.2964)	
training:	Epoch: [5][171/233]	Loss 0.3303 (0.2966)	
training:	Epoch: [5][172/233]	Loss 0.2556 (0.2964)	
training:	Epoch: [5][173/233]	Loss 0.2936 (0.2964)	
training:	Epoch: [5][174/233]	Loss 0.2757 (0.2962)	
training:	Epoch: [5][175/233]	Loss 0.2695 (0.2961)	
training:	Epoch: [5][176/233]	Loss 0.2420 (0.2958)	
training:	Epoch: [5][177/233]	Loss 0.2878 (0.2957)	
training:	Epoch: [5][178/233]	Loss 0.3196 (0.2959)	
training:	Epoch: [5][179/233]	Loss 0.3940 (0.2964)	
training:	Epoch: [5][180/233]	Loss 0.5567 (0.2979)	
training:	Epoch: [5][181/233]	Loss 0.1764 (0.2972)	
training:	Epoch: [5][182/233]	Loss 0.2989 (0.2972)	
training:	Epoch: [5][183/233]	Loss 0.3775 (0.2976)	
training:	Epoch: [5][184/233]	Loss 0.2786 (0.2975)	
training:	Epoch: [5][185/233]	Loss 0.3150 (0.2976)	
training:	Epoch: [5][186/233]	Loss 0.2854 (0.2976)	
training:	Epoch: [5][187/233]	Loss 0.2257 (0.2972)	
training:	Epoch: [5][188/233]	Loss 0.3122 (0.2973)	
training:	Epoch: [5][189/233]	Loss 0.2704 (0.2971)	
training:	Epoch: [5][190/233]	Loss 0.1929 (0.2966)	
training:	Epoch: [5][191/233]	Loss 0.3276 (0.2967)	
training:	Epoch: [5][192/233]	Loss 0.3013 (0.2968)	
training:	Epoch: [5][193/233]	Loss 0.2204 (0.2964)	
training:	Epoch: [5][194/233]	Loss 0.3299 (0.2965)	
training:	Epoch: [5][195/233]	Loss 0.3446 (0.2968)	
training:	Epoch: [5][196/233]	Loss 0.2920 (0.2967)	
training:	Epoch: [5][197/233]	Loss 0.3033 (0.2968)	
training:	Epoch: [5][198/233]	Loss 0.3350 (0.2970)	
training:	Epoch: [5][199/233]	Loss 0.2677 (0.2968)	
training:	Epoch: [5][200/233]	Loss 0.2107 (0.2964)	
training:	Epoch: [5][201/233]	Loss 0.4337 (0.2971)	
training:	Epoch: [5][202/233]	Loss 0.2982 (0.2971)	
training:	Epoch: [5][203/233]	Loss 0.3439 (0.2973)	
training:	Epoch: [5][204/233]	Loss 0.3745 (0.2977)	
training:	Epoch: [5][205/233]	Loss 0.1953 (0.2972)	
training:	Epoch: [5][206/233]	Loss 0.2240 (0.2968)	
training:	Epoch: [5][207/233]	Loss 0.3607 (0.2971)	
training:	Epoch: [5][208/233]	Loss 0.2785 (0.2971)	
training:	Epoch: [5][209/233]	Loss 0.3440 (0.2973)	
training:	Epoch: [5][210/233]	Loss 0.3306 (0.2974)	
training:	Epoch: [5][211/233]	Loss 0.2105 (0.2970)	
training:	Epoch: [5][212/233]	Loss 0.3050 (0.2971)	
training:	Epoch: [5][213/233]	Loss 0.3061 (0.2971)	
training:	Epoch: [5][214/233]	Loss 0.2468 (0.2969)	
training:	Epoch: [5][215/233]	Loss 0.2949 (0.2969)	
training:	Epoch: [5][216/233]	Loss 0.2759 (0.2968)	
training:	Epoch: [5][217/233]	Loss 0.3484 (0.2970)	
training:	Epoch: [5][218/233]	Loss 0.2878 (0.2970)	
training:	Epoch: [5][219/233]	Loss 0.2243 (0.2966)	
training:	Epoch: [5][220/233]	Loss 0.2225 (0.2963)	
training:	Epoch: [5][221/233]	Loss 0.2988 (0.2963)	
training:	Epoch: [5][222/233]	Loss 0.2718 (0.2962)	
training:	Epoch: [5][223/233]	Loss 0.2590 (0.2960)	
training:	Epoch: [5][224/233]	Loss 0.2774 (0.2959)	
training:	Epoch: [5][225/233]	Loss 0.3557 (0.2962)	
training:	Epoch: [5][226/233]	Loss 0.3421 (0.2964)	
training:	Epoch: [5][227/233]	Loss 0.3469 (0.2966)	
training:	Epoch: [5][228/233]	Loss 0.2665 (0.2965)	
training:	Epoch: [5][229/233]	Loss 0.2898 (0.2965)	
training:	Epoch: [5][230/233]	Loss 0.3615 (0.2968)	
training:	Epoch: [5][231/233]	Loss 0.1918 (0.2963)	
training:	Epoch: [5][232/233]	Loss 0.2722 (0.2962)	
training:	Epoch: [5][233/233]	Loss 0.2340 (0.2959)	
Training:	 Loss: 0.2953

Training:	 ACC: 0.9377 0.9370 0.9203 0.9552
Validation:	 ACC: 0.8309 0.8304 0.8188 0.8430
Validation:	 Best_BACC: 0.8309 0.8304 0.8188 0.8430
Validation:	 Loss: 0.3918
Pretraining:	Epoch 6/200
----------
training:	Epoch: [6][1/233]	Loss 0.3348 (0.3348)	
training:	Epoch: [6][2/233]	Loss 0.2656 (0.3002)	
training:	Epoch: [6][3/233]	Loss 0.2506 (0.2837)	
training:	Epoch: [6][4/233]	Loss 0.2610 (0.2780)	
training:	Epoch: [6][5/233]	Loss 0.2225 (0.2669)	
training:	Epoch: [6][6/233]	Loss 0.2515 (0.2643)	
training:	Epoch: [6][7/233]	Loss 0.2351 (0.2601)	
training:	Epoch: [6][8/233]	Loss 0.2329 (0.2567)	
training:	Epoch: [6][9/233]	Loss 0.3534 (0.2675)	
training:	Epoch: [6][10/233]	Loss 0.2786 (0.2686)	
training:	Epoch: [6][11/233]	Loss 0.2513 (0.2670)	
training:	Epoch: [6][12/233]	Loss 0.2953 (0.2694)	
training:	Epoch: [6][13/233]	Loss 0.2255 (0.2660)	
training:	Epoch: [6][14/233]	Loss 0.2770 (0.2668)	
training:	Epoch: [6][15/233]	Loss 0.2392 (0.2649)	
training:	Epoch: [6][16/233]	Loss 0.2269 (0.2626)	
training:	Epoch: [6][17/233]	Loss 0.2457 (0.2616)	
training:	Epoch: [6][18/233]	Loss 0.1940 (0.2578)	
training:	Epoch: [6][19/233]	Loss 0.3915 (0.2649)	
training:	Epoch: [6][20/233]	Loss 0.2722 (0.2652)	
training:	Epoch: [6][21/233]	Loss 0.3066 (0.2672)	
training:	Epoch: [6][22/233]	Loss 0.2631 (0.2670)	
training:	Epoch: [6][23/233]	Loss 0.2789 (0.2675)	
training:	Epoch: [6][24/233]	Loss 0.2740 (0.2678)	
training:	Epoch: [6][25/233]	Loss 0.2653 (0.2677)	
training:	Epoch: [6][26/233]	Loss 0.2718 (0.2679)	
training:	Epoch: [6][27/233]	Loss 0.3020 (0.2691)	
training:	Epoch: [6][28/233]	Loss 0.1754 (0.2658)	
training:	Epoch: [6][29/233]	Loss 0.2569 (0.2655)	
training:	Epoch: [6][30/233]	Loss 0.2694 (0.2656)	
training:	Epoch: [6][31/233]	Loss 0.2121 (0.2639)	
training:	Epoch: [6][32/233]	Loss 0.2540 (0.2636)	
training:	Epoch: [6][33/233]	Loss 0.2580 (0.2634)	
training:	Epoch: [6][34/233]	Loss 0.2447 (0.2628)	
training:	Epoch: [6][35/233]	Loss 0.2494 (0.2625)	
training:	Epoch: [6][36/233]	Loss 0.2664 (0.2626)	
training:	Epoch: [6][37/233]	Loss 0.2813 (0.2631)	
training:	Epoch: [6][38/233]	Loss 0.2573 (0.2629)	
training:	Epoch: [6][39/233]	Loss 0.2819 (0.2634)	
training:	Epoch: [6][40/233]	Loss 0.2691 (0.2636)	
training:	Epoch: [6][41/233]	Loss 0.2758 (0.2638)	
training:	Epoch: [6][42/233]	Loss 0.2890 (0.2644)	
training:	Epoch: [6][43/233]	Loss 0.2828 (0.2649)	
training:	Epoch: [6][44/233]	Loss 0.2453 (0.2644)	
training:	Epoch: [6][45/233]	Loss 0.2470 (0.2640)	
training:	Epoch: [6][46/233]	Loss 0.2248 (0.2632)	
training:	Epoch: [6][47/233]	Loss 0.2844 (0.2636)	
training:	Epoch: [6][48/233]	Loss 0.3150 (0.2647)	
training:	Epoch: [6][49/233]	Loss 0.2994 (0.2654)	
training:	Epoch: [6][50/233]	Loss 0.2303 (0.2647)	
training:	Epoch: [6][51/233]	Loss 0.3176 (0.2658)	
training:	Epoch: [6][52/233]	Loss 0.3136 (0.2667)	
training:	Epoch: [6][53/233]	Loss 0.3205 (0.2677)	
training:	Epoch: [6][54/233]	Loss 0.1971 (0.2664)	
training:	Epoch: [6][55/233]	Loss 0.2988 (0.2670)	
training:	Epoch: [6][56/233]	Loss 0.2481 (0.2666)	
training:	Epoch: [6][57/233]	Loss 0.2527 (0.2664)	
training:	Epoch: [6][58/233]	Loss 0.3056 (0.2671)	
training:	Epoch: [6][59/233]	Loss 0.2855 (0.2674)	
training:	Epoch: [6][60/233]	Loss 0.2547 (0.2672)	
training:	Epoch: [6][61/233]	Loss 0.1843 (0.2658)	
training:	Epoch: [6][62/233]	Loss 0.2128 (0.2650)	
training:	Epoch: [6][63/233]	Loss 0.2574 (0.2648)	
training:	Epoch: [6][64/233]	Loss 0.2470 (0.2646)	
training:	Epoch: [6][65/233]	Loss 0.3017 (0.2651)	
training:	Epoch: [6][66/233]	Loss 0.2901 (0.2655)	
training:	Epoch: [6][67/233]	Loss 0.2782 (0.2657)	
training:	Epoch: [6][68/233]	Loss 0.2434 (0.2654)	
training:	Epoch: [6][69/233]	Loss 0.1808 (0.2641)	
training:	Epoch: [6][70/233]	Loss 0.3459 (0.2653)	
training:	Epoch: [6][71/233]	Loss 0.2374 (0.2649)	
training:	Epoch: [6][72/233]	Loss 0.2700 (0.2650)	
training:	Epoch: [6][73/233]	Loss 0.3343 (0.2659)	
training:	Epoch: [6][74/233]	Loss 0.3551 (0.2671)	
training:	Epoch: [6][75/233]	Loss 0.2275 (0.2666)	
training:	Epoch: [6][76/233]	Loss 0.2714 (0.2667)	
training:	Epoch: [6][77/233]	Loss 0.2773 (0.2668)	
training:	Epoch: [6][78/233]	Loss 0.2854 (0.2671)	
training:	Epoch: [6][79/233]	Loss 0.2793 (0.2672)	
training:	Epoch: [6][80/233]	Loss 0.2585 (0.2671)	
training:	Epoch: [6][81/233]	Loss 0.2202 (0.2665)	
training:	Epoch: [6][82/233]	Loss 0.3054 (0.2670)	
training:	Epoch: [6][83/233]	Loss 0.2985 (0.2674)	
training:	Epoch: [6][84/233]	Loss 0.2212 (0.2668)	
training:	Epoch: [6][85/233]	Loss 0.3359 (0.2676)	
training:	Epoch: [6][86/233]	Loss 0.2720 (0.2677)	
training:	Epoch: [6][87/233]	Loss 0.2381 (0.2673)	
training:	Epoch: [6][88/233]	Loss 0.2817 (0.2675)	
training:	Epoch: [6][89/233]	Loss 0.2150 (0.2669)	
training:	Epoch: [6][90/233]	Loss 0.2999 (0.2673)	
training:	Epoch: [6][91/233]	Loss 0.2984 (0.2676)	
training:	Epoch: [6][92/233]	Loss 0.2472 (0.2674)	
training:	Epoch: [6][93/233]	Loss 0.3434 (0.2682)	
training:	Epoch: [6][94/233]	Loss 0.2478 (0.2680)	
training:	Epoch: [6][95/233]	Loss 0.2235 (0.2675)	
training:	Epoch: [6][96/233]	Loss 0.2659 (0.2675)	
training:	Epoch: [6][97/233]	Loss 0.3238 (0.2681)	
training:	Epoch: [6][98/233]	Loss 0.2857 (0.2683)	
training:	Epoch: [6][99/233]	Loss 0.3124 (0.2687)	
training:	Epoch: [6][100/233]	Loss 0.3328 (0.2694)	
training:	Epoch: [6][101/233]	Loss 0.2530 (0.2692)	
training:	Epoch: [6][102/233]	Loss 0.2447 (0.2690)	
training:	Epoch: [6][103/233]	Loss 0.3715 (0.2700)	
training:	Epoch: [6][104/233]	Loss 0.2706 (0.2700)	
training:	Epoch: [6][105/233]	Loss 0.2345 (0.2696)	
training:	Epoch: [6][106/233]	Loss 0.2487 (0.2694)	
training:	Epoch: [6][107/233]	Loss 0.3823 (0.2705)	
training:	Epoch: [6][108/233]	Loss 0.2226 (0.2700)	
training:	Epoch: [6][109/233]	Loss 0.2407 (0.2698)	
training:	Epoch: [6][110/233]	Loss 0.2351 (0.2695)	
training:	Epoch: [6][111/233]	Loss 0.3274 (0.2700)	
training:	Epoch: [6][112/233]	Loss 0.2994 (0.2702)	
training:	Epoch: [6][113/233]	Loss 0.3160 (0.2706)	
training:	Epoch: [6][114/233]	Loss 0.2084 (0.2701)	
training:	Epoch: [6][115/233]	Loss 0.3169 (0.2705)	
training:	Epoch: [6][116/233]	Loss 0.2444 (0.2703)	
training:	Epoch: [6][117/233]	Loss 0.2957 (0.2705)	
training:	Epoch: [6][118/233]	Loss 0.2864 (0.2706)	
training:	Epoch: [6][119/233]	Loss 0.2606 (0.2706)	
training:	Epoch: [6][120/233]	Loss 0.2741 (0.2706)	
training:	Epoch: [6][121/233]	Loss 0.2652 (0.2705)	
training:	Epoch: [6][122/233]	Loss 0.2837 (0.2706)	
training:	Epoch: [6][123/233]	Loss 0.1780 (0.2699)	
training:	Epoch: [6][124/233]	Loss 0.2528 (0.2698)	
training:	Epoch: [6][125/233]	Loss 0.3257 (0.2702)	
training:	Epoch: [6][126/233]	Loss 0.2136 (0.2698)	
training:	Epoch: [6][127/233]	Loss 0.2629 (0.2697)	
training:	Epoch: [6][128/233]	Loss 0.2038 (0.2692)	
training:	Epoch: [6][129/233]	Loss 0.2808 (0.2693)	
training:	Epoch: [6][130/233]	Loss 0.2123 (0.2688)	
training:	Epoch: [6][131/233]	Loss 0.2520 (0.2687)	
training:	Epoch: [6][132/233]	Loss 0.2953 (0.2689)	
training:	Epoch: [6][133/233]	Loss 0.2797 (0.2690)	
training:	Epoch: [6][134/233]	Loss 0.2984 (0.2692)	
training:	Epoch: [6][135/233]	Loss 0.2192 (0.2688)	
training:	Epoch: [6][136/233]	Loss 0.2576 (0.2688)	
training:	Epoch: [6][137/233]	Loss 0.1763 (0.2681)	
training:	Epoch: [6][138/233]	Loss 0.2515 (0.2680)	
training:	Epoch: [6][139/233]	Loss 0.3313 (0.2684)	
training:	Epoch: [6][140/233]	Loss 0.2351 (0.2682)	
training:	Epoch: [6][141/233]	Loss 0.2189 (0.2678)	
training:	Epoch: [6][142/233]	Loss 0.2627 (0.2678)	
training:	Epoch: [6][143/233]	Loss 0.3062 (0.2681)	
training:	Epoch: [6][144/233]	Loss 0.4177 (0.2691)	
training:	Epoch: [6][145/233]	Loss 0.2803 (0.2692)	
training:	Epoch: [6][146/233]	Loss 0.2099 (0.2688)	
training:	Epoch: [6][147/233]	Loss 0.2481 (0.2686)	
training:	Epoch: [6][148/233]	Loss 0.2534 (0.2685)	
training:	Epoch: [6][149/233]	Loss 0.2352 (0.2683)	
training:	Epoch: [6][150/233]	Loss 0.2329 (0.2681)	
training:	Epoch: [6][151/233]	Loss 0.2231 (0.2678)	
training:	Epoch: [6][152/233]	Loss 0.3681 (0.2684)	
training:	Epoch: [6][153/233]	Loss 0.2410 (0.2682)	
training:	Epoch: [6][154/233]	Loss 0.2465 (0.2681)	
training:	Epoch: [6][155/233]	Loss 0.1883 (0.2676)	
training:	Epoch: [6][156/233]	Loss 0.2529 (0.2675)	
training:	Epoch: [6][157/233]	Loss 0.2504 (0.2674)	
training:	Epoch: [6][158/233]	Loss 0.2944 (0.2676)	
training:	Epoch: [6][159/233]	Loss 0.2212 (0.2673)	
training:	Epoch: [6][160/233]	Loss 0.2397 (0.2671)	
training:	Epoch: [6][161/233]	Loss 0.4058 (0.2680)	
training:	Epoch: [6][162/233]	Loss 0.2760 (0.2680)	
training:	Epoch: [6][163/233]	Loss 0.2540 (0.2679)	
training:	Epoch: [6][164/233]	Loss 0.2972 (0.2681)	
training:	Epoch: [6][165/233]	Loss 0.2109 (0.2678)	
training:	Epoch: [6][166/233]	Loss 0.2807 (0.2678)	
training:	Epoch: [6][167/233]	Loss 0.2943 (0.2680)	
training:	Epoch: [6][168/233]	Loss 0.2840 (0.2681)	
training:	Epoch: [6][169/233]	Loss 0.2587 (0.2680)	
training:	Epoch: [6][170/233]	Loss 0.2860 (0.2681)	
training:	Epoch: [6][171/233]	Loss 0.2663 (0.2681)	
training:	Epoch: [6][172/233]	Loss 0.2481 (0.2680)	
training:	Epoch: [6][173/233]	Loss 0.2886 (0.2681)	
training:	Epoch: [6][174/233]	Loss 0.2694 (0.2681)	
training:	Epoch: [6][175/233]	Loss 0.3230 (0.2684)	
training:	Epoch: [6][176/233]	Loss 0.3196 (0.2687)	
training:	Epoch: [6][177/233]	Loss 0.2599 (0.2687)	
training:	Epoch: [6][178/233]	Loss 0.2644 (0.2687)	
training:	Epoch: [6][179/233]	Loss 0.2601 (0.2686)	
training:	Epoch: [6][180/233]	Loss 0.3493 (0.2691)	
training:	Epoch: [6][181/233]	Loss 0.2907 (0.2692)	
training:	Epoch: [6][182/233]	Loss 0.1926 (0.2688)	
training:	Epoch: [6][183/233]	Loss 0.3094 (0.2690)	
training:	Epoch: [6][184/233]	Loss 0.3142 (0.2692)	
training:	Epoch: [6][185/233]	Loss 0.3058 (0.2694)	
training:	Epoch: [6][186/233]	Loss 0.2435 (0.2693)	
training:	Epoch: [6][187/233]	Loss 0.3097 (0.2695)	
training:	Epoch: [6][188/233]	Loss 0.2885 (0.2696)	
training:	Epoch: [6][189/233]	Loss 0.2837 (0.2697)	
training:	Epoch: [6][190/233]	Loss 0.1967 (0.2693)	
training:	Epoch: [6][191/233]	Loss 0.1949 (0.2689)	
training:	Epoch: [6][192/233]	Loss 0.2791 (0.2690)	
training:	Epoch: [6][193/233]	Loss 0.2823 (0.2690)	
training:	Epoch: [6][194/233]	Loss 0.2858 (0.2691)	
training:	Epoch: [6][195/233]	Loss 0.3335 (0.2694)	
training:	Epoch: [6][196/233]	Loss 0.2462 (0.2693)	
training:	Epoch: [6][197/233]	Loss 0.3256 (0.2696)	
training:	Epoch: [6][198/233]	Loss 0.2739 (0.2696)	
training:	Epoch: [6][199/233]	Loss 0.2656 (0.2696)	
training:	Epoch: [6][200/233]	Loss 0.3266 (0.2699)	
training:	Epoch: [6][201/233]	Loss 0.2643 (0.2699)	
training:	Epoch: [6][202/233]	Loss 0.2316 (0.2697)	
training:	Epoch: [6][203/233]	Loss 0.2713 (0.2697)	
training:	Epoch: [6][204/233]	Loss 0.2585 (0.2696)	
training:	Epoch: [6][205/233]	Loss 0.2275 (0.2694)	
training:	Epoch: [6][206/233]	Loss 0.2537 (0.2694)	
training:	Epoch: [6][207/233]	Loss 0.3610 (0.2698)	
training:	Epoch: [6][208/233]	Loss 0.2855 (0.2699)	
training:	Epoch: [6][209/233]	Loss 0.3213 (0.2701)	
training:	Epoch: [6][210/233]	Loss 0.2803 (0.2702)	
training:	Epoch: [6][211/233]	Loss 0.2287 (0.2700)	
training:	Epoch: [6][212/233]	Loss 0.3354 (0.2703)	
training:	Epoch: [6][213/233]	Loss 0.2626 (0.2702)	
training:	Epoch: [6][214/233]	Loss 0.2867 (0.2703)	
training:	Epoch: [6][215/233]	Loss 0.2683 (0.2703)	
training:	Epoch: [6][216/233]	Loss 0.2511 (0.2702)	
training:	Epoch: [6][217/233]	Loss 0.2615 (0.2702)	
training:	Epoch: [6][218/233]	Loss 0.2433 (0.2701)	
training:	Epoch: [6][219/233]	Loss 0.2023 (0.2697)	
training:	Epoch: [6][220/233]	Loss 0.2523 (0.2697)	
training:	Epoch: [6][221/233]	Loss 0.2056 (0.2694)	
training:	Epoch: [6][222/233]	Loss 0.2924 (0.2695)	
training:	Epoch: [6][223/233]	Loss 0.3215 (0.2697)	
training:	Epoch: [6][224/233]	Loss 0.1791 (0.2693)	
training:	Epoch: [6][225/233]	Loss 0.2717 (0.2693)	
training:	Epoch: [6][226/233]	Loss 0.2382 (0.2692)	
training:	Epoch: [6][227/233]	Loss 0.3039 (0.2693)	
training:	Epoch: [6][228/233]	Loss 0.2372 (0.2692)	
training:	Epoch: [6][229/233]	Loss 0.2797 (0.2692)	
training:	Epoch: [6][230/233]	Loss 0.2955 (0.2694)	
training:	Epoch: [6][231/233]	Loss 0.2775 (0.2694)	
training:	Epoch: [6][232/233]	Loss 0.2464 (0.2693)	
training:	Epoch: [6][233/233]	Loss 0.2743 (0.2693)	
Training:	 Loss: 0.2687

Training:	 ACC: 0.9539 0.9533 0.9403 0.9675
Validation:	 ACC: 0.8356 0.8352 0.8260 0.8453
Validation:	 Best_BACC: 0.8356 0.8352 0.8260 0.8453
Validation:	 Loss: 0.3820
Pretraining:	Epoch 7/200
----------
training:	Epoch: [7][1/233]	Loss 0.2106 (0.2106)	
training:	Epoch: [7][2/233]	Loss 0.2190 (0.2148)	
training:	Epoch: [7][3/233]	Loss 0.2168 (0.2154)	
training:	Epoch: [7][4/233]	Loss 0.2259 (0.2181)	
training:	Epoch: [7][5/233]	Loss 0.2065 (0.2157)	
training:	Epoch: [7][6/233]	Loss 0.3183 (0.2328)	
training:	Epoch: [7][7/233]	Loss 0.2357 (0.2332)	
training:	Epoch: [7][8/233]	Loss 0.2768 (0.2387)	
training:	Epoch: [7][9/233]	Loss 0.1851 (0.2327)	
training:	Epoch: [7][10/233]	Loss 0.3637 (0.2458)	
training:	Epoch: [7][11/233]	Loss 0.2715 (0.2482)	
training:	Epoch: [7][12/233]	Loss 0.1963 (0.2438)	
training:	Epoch: [7][13/233]	Loss 0.2485 (0.2442)	
training:	Epoch: [7][14/233]	Loss 0.2368 (0.2437)	
training:	Epoch: [7][15/233]	Loss 0.2562 (0.2445)	
training:	Epoch: [7][16/233]	Loss 0.2177 (0.2428)	
training:	Epoch: [7][17/233]	Loss 0.2497 (0.2432)	
training:	Epoch: [7][18/233]	Loss 0.3231 (0.2477)	
training:	Epoch: [7][19/233]	Loss 0.2597 (0.2483)	
training:	Epoch: [7][20/233]	Loss 0.2638 (0.2491)	
training:	Epoch: [7][21/233]	Loss 0.2809 (0.2506)	
training:	Epoch: [7][22/233]	Loss 0.1911 (0.2479)	
training:	Epoch: [7][23/233]	Loss 0.2872 (0.2496)	
training:	Epoch: [7][24/233]	Loss 0.2889 (0.2512)	
training:	Epoch: [7][25/233]	Loss 0.2897 (0.2528)	
training:	Epoch: [7][26/233]	Loss 0.3185 (0.2553)	
training:	Epoch: [7][27/233]	Loss 0.3243 (0.2579)	
training:	Epoch: [7][28/233]	Loss 0.2230 (0.2566)	
training:	Epoch: [7][29/233]	Loss 0.2338 (0.2558)	
training:	Epoch: [7][30/233]	Loss 0.2068 (0.2542)	
training:	Epoch: [7][31/233]	Loss 0.2076 (0.2527)	
training:	Epoch: [7][32/233]	Loss 0.2205 (0.2517)	
training:	Epoch: [7][33/233]	Loss 0.2503 (0.2516)	
training:	Epoch: [7][34/233]	Loss 0.1779 (0.2495)	
training:	Epoch: [7][35/233]	Loss 0.3185 (0.2514)	
training:	Epoch: [7][36/233]	Loss 0.2526 (0.2515)	
training:	Epoch: [7][37/233]	Loss 0.2254 (0.2508)	
training:	Epoch: [7][38/233]	Loss 0.2687 (0.2512)	
training:	Epoch: [7][39/233]	Loss 0.2903 (0.2522)	
training:	Epoch: [7][40/233]	Loss 0.1557 (0.2498)	
training:	Epoch: [7][41/233]	Loss 0.2722 (0.2504)	
training:	Epoch: [7][42/233]	Loss 0.1912 (0.2490)	
training:	Epoch: [7][43/233]	Loss 0.2286 (0.2485)	
training:	Epoch: [7][44/233]	Loss 0.2019 (0.2474)	
training:	Epoch: [7][45/233]	Loss 0.2489 (0.2475)	
training:	Epoch: [7][46/233]	Loss 0.2460 (0.2474)	
training:	Epoch: [7][47/233]	Loss 0.3560 (0.2497)	
training:	Epoch: [7][48/233]	Loss 0.2279 (0.2493)	
training:	Epoch: [7][49/233]	Loss 0.2027 (0.2483)	
training:	Epoch: [7][50/233]	Loss 0.1820 (0.2470)	
training:	Epoch: [7][51/233]	Loss 0.2364 (0.2468)	
training:	Epoch: [7][52/233]	Loss 0.3162 (0.2481)	
training:	Epoch: [7][53/233]	Loss 0.2871 (0.2489)	
training:	Epoch: [7][54/233]	Loss 0.3029 (0.2499)	
training:	Epoch: [7][55/233]	Loss 0.2556 (0.2500)	
training:	Epoch: [7][56/233]	Loss 0.2679 (0.2503)	
training:	Epoch: [7][57/233]	Loss 0.2732 (0.2507)	
training:	Epoch: [7][58/233]	Loss 0.3128 (0.2518)	
training:	Epoch: [7][59/233]	Loss 0.1973 (0.2508)	
training:	Epoch: [7][60/233]	Loss 0.2224 (0.2504)	
training:	Epoch: [7][61/233]	Loss 0.2416 (0.2502)	
training:	Epoch: [7][62/233]	Loss 0.2391 (0.2501)	
training:	Epoch: [7][63/233]	Loss 0.2549 (0.2501)	
training:	Epoch: [7][64/233]	Loss 0.3554 (0.2518)	
training:	Epoch: [7][65/233]	Loss 0.2910 (0.2524)	
training:	Epoch: [7][66/233]	Loss 0.2430 (0.2522)	
training:	Epoch: [7][67/233]	Loss 0.1734 (0.2511)	
training:	Epoch: [7][68/233]	Loss 0.2443 (0.2510)	
training:	Epoch: [7][69/233]	Loss 0.1829 (0.2500)	
training:	Epoch: [7][70/233]	Loss 0.3031 (0.2507)	
training:	Epoch: [7][71/233]	Loss 0.2626 (0.2509)	
training:	Epoch: [7][72/233]	Loss 0.1879 (0.2500)	
training:	Epoch: [7][73/233]	Loss 0.2863 (0.2505)	
training:	Epoch: [7][74/233]	Loss 0.2063 (0.2499)	
training:	Epoch: [7][75/233]	Loss 0.3482 (0.2512)	
training:	Epoch: [7][76/233]	Loss 0.2307 (0.2510)	
training:	Epoch: [7][77/233]	Loss 0.2321 (0.2507)	
training:	Epoch: [7][78/233]	Loss 0.2084 (0.2502)	
training:	Epoch: [7][79/233]	Loss 0.2491 (0.2502)	
training:	Epoch: [7][80/233]	Loss 0.1725 (0.2492)	
training:	Epoch: [7][81/233]	Loss 0.2207 (0.2488)	
training:	Epoch: [7][82/233]	Loss 0.2140 (0.2484)	
training:	Epoch: [7][83/233]	Loss 0.2662 (0.2486)	
training:	Epoch: [7][84/233]	Loss 0.2350 (0.2485)	
training:	Epoch: [7][85/233]	Loss 0.3341 (0.2495)	
training:	Epoch: [7][86/233]	Loss 0.2811 (0.2498)	
training:	Epoch: [7][87/233]	Loss 0.2561 (0.2499)	
training:	Epoch: [7][88/233]	Loss 0.1543 (0.2488)	
training:	Epoch: [7][89/233]	Loss 0.2783 (0.2492)	
training:	Epoch: [7][90/233]	Loss 0.2130 (0.2488)	
training:	Epoch: [7][91/233]	Loss 0.1755 (0.2480)	
training:	Epoch: [7][92/233]	Loss 0.3307 (0.2489)	
training:	Epoch: [7][93/233]	Loss 0.2586 (0.2490)	
training:	Epoch: [7][94/233]	Loss 0.2135 (0.2486)	
training:	Epoch: [7][95/233]	Loss 0.2203 (0.2483)	
training:	Epoch: [7][96/233]	Loss 0.1951 (0.2477)	
training:	Epoch: [7][97/233]	Loss 0.2914 (0.2482)	
training:	Epoch: [7][98/233]	Loss 0.2830 (0.2485)	
training:	Epoch: [7][99/233]	Loss 0.2371 (0.2484)	
training:	Epoch: [7][100/233]	Loss 0.2405 (0.2483)	
training:	Epoch: [7][101/233]	Loss 0.2120 (0.2480)	
training:	Epoch: [7][102/233]	Loss 0.2378 (0.2479)	
training:	Epoch: [7][103/233]	Loss 0.2078 (0.2475)	
training:	Epoch: [7][104/233]	Loss 0.1712 (0.2468)	
training:	Epoch: [7][105/233]	Loss 0.1790 (0.2461)	
training:	Epoch: [7][106/233]	Loss 0.1930 (0.2456)	
training:	Epoch: [7][107/233]	Loss 0.2009 (0.2452)	
training:	Epoch: [7][108/233]	Loss 0.2439 (0.2452)	
training:	Epoch: [7][109/233]	Loss 0.2658 (0.2454)	
training:	Epoch: [7][110/233]	Loss 0.2514 (0.2454)	
training:	Epoch: [7][111/233]	Loss 0.2458 (0.2454)	
training:	Epoch: [7][112/233]	Loss 0.2195 (0.2452)	
training:	Epoch: [7][113/233]	Loss 0.2161 (0.2449)	
training:	Epoch: [7][114/233]	Loss 0.1954 (0.2445)	
training:	Epoch: [7][115/233]	Loss 0.1972 (0.2441)	
training:	Epoch: [7][116/233]	Loss 0.3485 (0.2450)	
training:	Epoch: [7][117/233]	Loss 0.3050 (0.2455)	
training:	Epoch: [7][118/233]	Loss 0.1876 (0.2450)	
training:	Epoch: [7][119/233]	Loss 0.2342 (0.2449)	
training:	Epoch: [7][120/233]	Loss 0.2492 (0.2450)	
training:	Epoch: [7][121/233]	Loss 0.2035 (0.2446)	
training:	Epoch: [7][122/233]	Loss 0.2886 (0.2450)	
training:	Epoch: [7][123/233]	Loss 0.2335 (0.2449)	
training:	Epoch: [7][124/233]	Loss 0.1979 (0.2445)	
training:	Epoch: [7][125/233]	Loss 0.2147 (0.2443)	
training:	Epoch: [7][126/233]	Loss 0.2150 (0.2440)	
training:	Epoch: [7][127/233]	Loss 0.2332 (0.2440)	
training:	Epoch: [7][128/233]	Loss 0.1952 (0.2436)	
training:	Epoch: [7][129/233]	Loss 0.2035 (0.2433)	
training:	Epoch: [7][130/233]	Loss 0.2807 (0.2435)	
training:	Epoch: [7][131/233]	Loss 0.2022 (0.2432)	
training:	Epoch: [7][132/233]	Loss 0.2444 (0.2432)	
training:	Epoch: [7][133/233]	Loss 0.2095 (0.2430)	
training:	Epoch: [7][134/233]	Loss 0.2629 (0.2431)	
training:	Epoch: [7][135/233]	Loss 0.2572 (0.2432)	
training:	Epoch: [7][136/233]	Loss 0.2611 (0.2434)	
training:	Epoch: [7][137/233]	Loss 0.1816 (0.2429)	
training:	Epoch: [7][138/233]	Loss 0.2550 (0.2430)	
training:	Epoch: [7][139/233]	Loss 0.2520 (0.2431)	
training:	Epoch: [7][140/233]	Loss 0.2129 (0.2429)	
training:	Epoch: [7][141/233]	Loss 0.2820 (0.2431)	
training:	Epoch: [7][142/233]	Loss 0.2908 (0.2435)	
training:	Epoch: [7][143/233]	Loss 0.2673 (0.2436)	
training:	Epoch: [7][144/233]	Loss 0.2226 (0.2435)	
training:	Epoch: [7][145/233]	Loss 0.2067 (0.2432)	
training:	Epoch: [7][146/233]	Loss 0.1902 (0.2429)	
training:	Epoch: [7][147/233]	Loss 0.2087 (0.2426)	
training:	Epoch: [7][148/233]	Loss 0.2310 (0.2426)	
training:	Epoch: [7][149/233]	Loss 0.2487 (0.2426)	
training:	Epoch: [7][150/233]	Loss 0.3267 (0.2432)	
training:	Epoch: [7][151/233]	Loss 0.3177 (0.2437)	
training:	Epoch: [7][152/233]	Loss 0.1986 (0.2434)	
training:	Epoch: [7][153/233]	Loss 0.2214 (0.2432)	
training:	Epoch: [7][154/233]	Loss 0.2638 (0.2434)	
training:	Epoch: [7][155/233]	Loss 0.2354 (0.2433)	
training:	Epoch: [7][156/233]	Loss 0.2952 (0.2436)	
training:	Epoch: [7][157/233]	Loss 0.3625 (0.2444)	
training:	Epoch: [7][158/233]	Loss 0.3081 (0.2448)	
training:	Epoch: [7][159/233]	Loss 0.2372 (0.2447)	
training:	Epoch: [7][160/233]	Loss 0.2858 (0.2450)	
training:	Epoch: [7][161/233]	Loss 0.2404 (0.2450)	
training:	Epoch: [7][162/233]	Loss 0.1976 (0.2447)	
training:	Epoch: [7][163/233]	Loss 0.2364 (0.2446)	
training:	Epoch: [7][164/233]	Loss 0.4292 (0.2458)	
training:	Epoch: [7][165/233]	Loss 0.1824 (0.2454)	
training:	Epoch: [7][166/233]	Loss 0.2412 (0.2453)	
training:	Epoch: [7][167/233]	Loss 0.2859 (0.2456)	
training:	Epoch: [7][168/233]	Loss 0.3048 (0.2459)	
training:	Epoch: [7][169/233]	Loss 0.2378 (0.2459)	
training:	Epoch: [7][170/233]	Loss 0.2540 (0.2459)	
training:	Epoch: [7][171/233]	Loss 0.1957 (0.2456)	
training:	Epoch: [7][172/233]	Loss 0.2745 (0.2458)	
training:	Epoch: [7][173/233]	Loss 0.3323 (0.2463)	
training:	Epoch: [7][174/233]	Loss 0.2320 (0.2462)	
training:	Epoch: [7][175/233]	Loss 0.2479 (0.2462)	
training:	Epoch: [7][176/233]	Loss 0.2196 (0.2461)	
training:	Epoch: [7][177/233]	Loss 0.1947 (0.2458)	
training:	Epoch: [7][178/233]	Loss 0.3277 (0.2463)	
training:	Epoch: [7][179/233]	Loss 0.2546 (0.2463)	
training:	Epoch: [7][180/233]	Loss 0.2153 (0.2461)	
training:	Epoch: [7][181/233]	Loss 0.2022 (0.2459)	
training:	Epoch: [7][182/233]	Loss 0.2038 (0.2457)	
training:	Epoch: [7][183/233]	Loss 0.2948 (0.2459)	
training:	Epoch: [7][184/233]	Loss 0.2356 (0.2459)	
training:	Epoch: [7][185/233]	Loss 0.3105 (0.2462)	
training:	Epoch: [7][186/233]	Loss 0.2463 (0.2462)	
training:	Epoch: [7][187/233]	Loss 0.2188 (0.2461)	
training:	Epoch: [7][188/233]	Loss 0.1974 (0.2458)	
training:	Epoch: [7][189/233]	Loss 0.2697 (0.2459)	
training:	Epoch: [7][190/233]	Loss 0.2443 (0.2459)	
training:	Epoch: [7][191/233]	Loss 0.1655 (0.2455)	
training:	Epoch: [7][192/233]	Loss 0.2311 (0.2454)	
training:	Epoch: [7][193/233]	Loss 0.3273 (0.2459)	
training:	Epoch: [7][194/233]	Loss 0.2636 (0.2460)	
training:	Epoch: [7][195/233]	Loss 0.2598 (0.2460)	
training:	Epoch: [7][196/233]	Loss 0.2066 (0.2458)	
training:	Epoch: [7][197/233]	Loss 0.2943 (0.2461)	
training:	Epoch: [7][198/233]	Loss 0.2382 (0.2460)	
training:	Epoch: [7][199/233]	Loss 0.2467 (0.2460)	
training:	Epoch: [7][200/233]	Loss 0.2677 (0.2461)	
training:	Epoch: [7][201/233]	Loss 0.2368 (0.2461)	
training:	Epoch: [7][202/233]	Loss 0.2673 (0.2462)	
training:	Epoch: [7][203/233]	Loss 0.2509 (0.2462)	
training:	Epoch: [7][204/233]	Loss 0.2735 (0.2464)	
training:	Epoch: [7][205/233]	Loss 0.1830 (0.2460)	
training:	Epoch: [7][206/233]	Loss 0.2838 (0.2462)	
training:	Epoch: [7][207/233]	Loss 0.2660 (0.2463)	
training:	Epoch: [7][208/233]	Loss 0.3139 (0.2467)	
training:	Epoch: [7][209/233]	Loss 0.2786 (0.2468)	
training:	Epoch: [7][210/233]	Loss 0.1990 (0.2466)	
training:	Epoch: [7][211/233]	Loss 0.1934 (0.2463)	
training:	Epoch: [7][212/233]	Loss 0.1973 (0.2461)	
training:	Epoch: [7][213/233]	Loss 0.2487 (0.2461)	
training:	Epoch: [7][214/233]	Loss 0.2650 (0.2462)	
training:	Epoch: [7][215/233]	Loss 0.2392 (0.2462)	
training:	Epoch: [7][216/233]	Loss 0.3221 (0.2465)	
training:	Epoch: [7][217/233]	Loss 0.3431 (0.2470)	
training:	Epoch: [7][218/233]	Loss 0.2143 (0.2468)	
training:	Epoch: [7][219/233]	Loss 0.2394 (0.2468)	
training:	Epoch: [7][220/233]	Loss 0.2829 (0.2469)	
training:	Epoch: [7][221/233]	Loss 0.2489 (0.2469)	
training:	Epoch: [7][222/233]	Loss 0.1692 (0.2466)	
training:	Epoch: [7][223/233]	Loss 0.2802 (0.2467)	
training:	Epoch: [7][224/233]	Loss 0.2047 (0.2466)	
training:	Epoch: [7][225/233]	Loss 0.3068 (0.2468)	
training:	Epoch: [7][226/233]	Loss 0.2283 (0.2467)	
training:	Epoch: [7][227/233]	Loss 0.2304 (0.2467)	
training:	Epoch: [7][228/233]	Loss 0.3137 (0.2470)	
training:	Epoch: [7][229/233]	Loss 0.2529 (0.2470)	
training:	Epoch: [7][230/233]	Loss 0.3070 (0.2473)	
training:	Epoch: [7][231/233]	Loss 0.2816 (0.2474)	
training:	Epoch: [7][232/233]	Loss 0.1983 (0.2472)	
training:	Epoch: [7][233/233]	Loss 0.1842 (0.2469)	
Training:	 Loss: 0.2464

Training:	 ACC: 0.9629 0.9621 0.9446 0.9812
Validation:	 ACC: 0.8426 0.8411 0.8096 0.8756
Validation:	 Best_BACC: 0.8426 0.8411 0.8096 0.8756
Validation:	 Loss: 0.3762
Pretraining:	Epoch 8/200
----------
training:	Epoch: [8][1/233]	Loss 0.1781 (0.1781)	
training:	Epoch: [8][2/233]	Loss 0.2780 (0.2281)	
training:	Epoch: [8][3/233]	Loss 0.1844 (0.2135)	
training:	Epoch: [8][4/233]	Loss 0.2255 (0.2165)	
training:	Epoch: [8][5/233]	Loss 0.2093 (0.2151)	
training:	Epoch: [8][6/233]	Loss 0.1863 (0.2103)	
training:	Epoch: [8][7/233]	Loss 0.2224 (0.2120)	
training:	Epoch: [8][8/233]	Loss 0.1966 (0.2101)	
training:	Epoch: [8][9/233]	Loss 0.1777 (0.2065)	
training:	Epoch: [8][10/233]	Loss 0.2726 (0.2131)	
training:	Epoch: [8][11/233]	Loss 0.1651 (0.2087)	
training:	Epoch: [8][12/233]	Loss 0.1967 (0.2077)	
training:	Epoch: [8][13/233]	Loss 0.1797 (0.2056)	
training:	Epoch: [8][14/233]	Loss 0.2613 (0.2096)	
training:	Epoch: [8][15/233]	Loss 0.2434 (0.2118)	
training:	Epoch: [8][16/233]	Loss 0.2484 (0.2141)	
training:	Epoch: [8][17/233]	Loss 0.1339 (0.2094)	
training:	Epoch: [8][18/233]	Loss 0.2456 (0.2114)	
training:	Epoch: [8][19/233]	Loss 0.1807 (0.2098)	
training:	Epoch: [8][20/233]	Loss 0.2305 (0.2108)	
training:	Epoch: [8][21/233]	Loss 0.1818 (0.2094)	
training:	Epoch: [8][22/233]	Loss 0.2012 (0.2091)	
training:	Epoch: [8][23/233]	Loss 0.3300 (0.2143)	
training:	Epoch: [8][24/233]	Loss 0.2742 (0.2168)	
training:	Epoch: [8][25/233]	Loss 0.1681 (0.2149)	
training:	Epoch: [8][26/233]	Loss 0.2196 (0.2150)	
training:	Epoch: [8][27/233]	Loss 0.2099 (0.2149)	
training:	Epoch: [8][28/233]	Loss 0.2508 (0.2161)	
training:	Epoch: [8][29/233]	Loss 0.2632 (0.2178)	
training:	Epoch: [8][30/233]	Loss 0.1890 (0.2168)	
training:	Epoch: [8][31/233]	Loss 0.2374 (0.2175)	
training:	Epoch: [8][32/233]	Loss 0.1972 (0.2168)	
training:	Epoch: [8][33/233]	Loss 0.1839 (0.2158)	
training:	Epoch: [8][34/233]	Loss 0.2159 (0.2158)	
training:	Epoch: [8][35/233]	Loss 0.2223 (0.2160)	
training:	Epoch: [8][36/233]	Loss 0.2279 (0.2164)	
training:	Epoch: [8][37/233]	Loss 0.2579 (0.2175)	
training:	Epoch: [8][38/233]	Loss 0.2579 (0.2185)	
training:	Epoch: [8][39/233]	Loss 0.2179 (0.2185)	
training:	Epoch: [8][40/233]	Loss 0.2367 (0.2190)	
training:	Epoch: [8][41/233]	Loss 0.1871 (0.2182)	
training:	Epoch: [8][42/233]	Loss 0.2295 (0.2185)	
training:	Epoch: [8][43/233]	Loss 0.1806 (0.2176)	
training:	Epoch: [8][44/233]	Loss 0.2107 (0.2174)	
training:	Epoch: [8][45/233]	Loss 0.2039 (0.2171)	
training:	Epoch: [8][46/233]	Loss 0.1804 (0.2163)	
training:	Epoch: [8][47/233]	Loss 0.1967 (0.2159)	
training:	Epoch: [8][48/233]	Loss 0.2380 (0.2164)	
training:	Epoch: [8][49/233]	Loss 0.1966 (0.2160)	
training:	Epoch: [8][50/233]	Loss 0.2505 (0.2167)	
training:	Epoch: [8][51/233]	Loss 0.1462 (0.2153)	
training:	Epoch: [8][52/233]	Loss 0.2328 (0.2156)	
training:	Epoch: [8][53/233]	Loss 0.2504 (0.2163)	
training:	Epoch: [8][54/233]	Loss 0.1399 (0.2149)	
training:	Epoch: [8][55/233]	Loss 0.2252 (0.2150)	
training:	Epoch: [8][56/233]	Loss 0.1873 (0.2146)	
training:	Epoch: [8][57/233]	Loss 0.2418 (0.2150)	
training:	Epoch: [8][58/233]	Loss 0.1656 (0.2142)	
training:	Epoch: [8][59/233]	Loss 0.2529 (0.2148)	
training:	Epoch: [8][60/233]	Loss 0.2192 (0.2149)	
training:	Epoch: [8][61/233]	Loss 0.3049 (0.2164)	
training:	Epoch: [8][62/233]	Loss 0.2079 (0.2162)	
training:	Epoch: [8][63/233]	Loss 0.2220 (0.2163)	
training:	Epoch: [8][64/233]	Loss 0.2031 (0.2161)	
training:	Epoch: [8][65/233]	Loss 0.3834 (0.2187)	
training:	Epoch: [8][66/233]	Loss 0.2922 (0.2198)	
training:	Epoch: [8][67/233]	Loss 0.1479 (0.2187)	
training:	Epoch: [8][68/233]	Loss 0.2780 (0.2196)	
training:	Epoch: [8][69/233]	Loss 0.1584 (0.2187)	
training:	Epoch: [8][70/233]	Loss 0.2648 (0.2194)	
training:	Epoch: [8][71/233]	Loss 0.1844 (0.2189)	
training:	Epoch: [8][72/233]	Loss 0.1811 (0.2184)	
training:	Epoch: [8][73/233]	Loss 0.3357 (0.2200)	
training:	Epoch: [8][74/233]	Loss 0.1768 (0.2194)	
training:	Epoch: [8][75/233]	Loss 0.2119 (0.2193)	
training:	Epoch: [8][76/233]	Loss 0.1953 (0.2190)	
training:	Epoch: [8][77/233]	Loss 0.2219 (0.2190)	
training:	Epoch: [8][78/233]	Loss 0.3272 (0.2204)	
training:	Epoch: [8][79/233]	Loss 0.1500 (0.2195)	
training:	Epoch: [8][80/233]	Loss 0.2231 (0.2196)	
training:	Epoch: [8][81/233]	Loss 0.2054 (0.2194)	
training:	Epoch: [8][82/233]	Loss 0.3030 (0.2204)	
training:	Epoch: [8][83/233]	Loss 0.1888 (0.2200)	
training:	Epoch: [8][84/233]	Loss 0.2263 (0.2201)	
training:	Epoch: [8][85/233]	Loss 0.3114 (0.2212)	
training:	Epoch: [8][86/233]	Loss 0.2022 (0.2209)	
training:	Epoch: [8][87/233]	Loss 0.2598 (0.2214)	
training:	Epoch: [8][88/233]	Loss 0.1895 (0.2210)	
training:	Epoch: [8][89/233]	Loss 0.1632 (0.2204)	
training:	Epoch: [8][90/233]	Loss 0.3561 (0.2219)	
training:	Epoch: [8][91/233]	Loss 0.2196 (0.2219)	
training:	Epoch: [8][92/233]	Loss 0.1843 (0.2215)	
training:	Epoch: [8][93/233]	Loss 0.2165 (0.2214)	
training:	Epoch: [8][94/233]	Loss 0.2084 (0.2213)	
training:	Epoch: [8][95/233]	Loss 0.2678 (0.2218)	
training:	Epoch: [8][96/233]	Loss 0.1834 (0.2214)	
training:	Epoch: [8][97/233]	Loss 0.2638 (0.2218)	
training:	Epoch: [8][98/233]	Loss 0.3085 (0.2227)	
training:	Epoch: [8][99/233]	Loss 0.2293 (0.2227)	
training:	Epoch: [8][100/233]	Loss 0.2165 (0.2227)	
training:	Epoch: [8][101/233]	Loss 0.1887 (0.2223)	
training:	Epoch: [8][102/233]	Loss 0.3102 (0.2232)	
training:	Epoch: [8][103/233]	Loss 0.1587 (0.2226)	
training:	Epoch: [8][104/233]	Loss 0.2934 (0.2233)	
training:	Epoch: [8][105/233]	Loss 0.2189 (0.2232)	
training:	Epoch: [8][106/233]	Loss 0.1831 (0.2228)	
training:	Epoch: [8][107/233]	Loss 0.2597 (0.2232)	
training:	Epoch: [8][108/233]	Loss 0.2402 (0.2233)	
training:	Epoch: [8][109/233]	Loss 0.2146 (0.2233)	
training:	Epoch: [8][110/233]	Loss 0.2004 (0.2231)	
training:	Epoch: [8][111/233]	Loss 0.2603 (0.2234)	
training:	Epoch: [8][112/233]	Loss 0.2282 (0.2234)	
training:	Epoch: [8][113/233]	Loss 0.2711 (0.2239)	
training:	Epoch: [8][114/233]	Loss 0.2291 (0.2239)	
training:	Epoch: [8][115/233]	Loss 0.2260 (0.2239)	
training:	Epoch: [8][116/233]	Loss 0.2669 (0.2243)	
training:	Epoch: [8][117/233]	Loss 0.2391 (0.2244)	
training:	Epoch: [8][118/233]	Loss 0.2334 (0.2245)	
training:	Epoch: [8][119/233]	Loss 0.2860 (0.2250)	
training:	Epoch: [8][120/233]	Loss 0.1630 (0.2245)	
training:	Epoch: [8][121/233]	Loss 0.2713 (0.2249)	
training:	Epoch: [8][122/233]	Loss 0.3180 (0.2256)	
training:	Epoch: [8][123/233]	Loss 0.2408 (0.2258)	
training:	Epoch: [8][124/233]	Loss 0.1759 (0.2254)	
training:	Epoch: [8][125/233]	Loss 0.2010 (0.2252)	
training:	Epoch: [8][126/233]	Loss 0.2976 (0.2257)	
training:	Epoch: [8][127/233]	Loss 0.3081 (0.2264)	
training:	Epoch: [8][128/233]	Loss 0.2578 (0.2266)	
training:	Epoch: [8][129/233]	Loss 0.2711 (0.2270)	
training:	Epoch: [8][130/233]	Loss 0.2200 (0.2269)	
training:	Epoch: [8][131/233]	Loss 0.1702 (0.2265)	
training:	Epoch: [8][132/233]	Loss 0.2000 (0.2263)	
training:	Epoch: [8][133/233]	Loss 0.2204 (0.2262)	
training:	Epoch: [8][134/233]	Loss 0.2724 (0.2266)	
training:	Epoch: [8][135/233]	Loss 0.2511 (0.2268)	
training:	Epoch: [8][136/233]	Loss 0.2558 (0.2270)	
training:	Epoch: [8][137/233]	Loss 0.1819 (0.2267)	
training:	Epoch: [8][138/233]	Loss 0.2271 (0.2267)	
training:	Epoch: [8][139/233]	Loss 0.2427 (0.2268)	
training:	Epoch: [8][140/233]	Loss 0.2042 (0.2266)	
training:	Epoch: [8][141/233]	Loss 0.2342 (0.2267)	
training:	Epoch: [8][142/233]	Loss 0.2456 (0.2268)	
training:	Epoch: [8][143/233]	Loss 0.2536 (0.2270)	
training:	Epoch: [8][144/233]	Loss 0.2377 (0.2271)	
training:	Epoch: [8][145/233]	Loss 0.2027 (0.2269)	
training:	Epoch: [8][146/233]	Loss 0.2471 (0.2270)	
training:	Epoch: [8][147/233]	Loss 0.1717 (0.2267)	
training:	Epoch: [8][148/233]	Loss 0.2476 (0.2268)	
training:	Epoch: [8][149/233]	Loss 0.2575 (0.2270)	
training:	Epoch: [8][150/233]	Loss 0.1709 (0.2266)	
training:	Epoch: [8][151/233]	Loss 0.2318 (0.2267)	
training:	Epoch: [8][152/233]	Loss 0.3267 (0.2273)	
training:	Epoch: [8][153/233]	Loss 0.1767 (0.2270)	
training:	Epoch: [8][154/233]	Loss 0.2082 (0.2269)	
training:	Epoch: [8][155/233]	Loss 0.1832 (0.2266)	
training:	Epoch: [8][156/233]	Loss 0.2469 (0.2267)	
training:	Epoch: [8][157/233]	Loss 0.1762 (0.2264)	
training:	Epoch: [8][158/233]	Loss 0.2349 (0.2265)	
training:	Epoch: [8][159/233]	Loss 0.2754 (0.2268)	
training:	Epoch: [8][160/233]	Loss 0.2154 (0.2267)	
training:	Epoch: [8][161/233]	Loss 0.2459 (0.2268)	
training:	Epoch: [8][162/233]	Loss 0.2459 (0.2269)	
training:	Epoch: [8][163/233]	Loss 0.2474 (0.2271)	
training:	Epoch: [8][164/233]	Loss 0.2171 (0.2270)	
training:	Epoch: [8][165/233]	Loss 0.4092 (0.2281)	
training:	Epoch: [8][166/233]	Loss 0.2236 (0.2281)	
training:	Epoch: [8][167/233]	Loss 0.1631 (0.2277)	
training:	Epoch: [8][168/233]	Loss 0.2059 (0.2276)	
training:	Epoch: [8][169/233]	Loss 0.2431 (0.2276)	
training:	Epoch: [8][170/233]	Loss 0.2076 (0.2275)	
training:	Epoch: [8][171/233]	Loss 0.3096 (0.2280)	
training:	Epoch: [8][172/233]	Loss 0.2954 (0.2284)	
training:	Epoch: [8][173/233]	Loss 0.1707 (0.2281)	
training:	Epoch: [8][174/233]	Loss 0.1908 (0.2278)	
training:	Epoch: [8][175/233]	Loss 0.1912 (0.2276)	
training:	Epoch: [8][176/233]	Loss 0.1520 (0.2272)	
training:	Epoch: [8][177/233]	Loss 0.1943 (0.2270)	
training:	Epoch: [8][178/233]	Loss 0.2892 (0.2274)	
training:	Epoch: [8][179/233]	Loss 0.2022 (0.2272)	
training:	Epoch: [8][180/233]	Loss 0.2470 (0.2273)	
training:	Epoch: [8][181/233]	Loss 0.2178 (0.2273)	
training:	Epoch: [8][182/233]	Loss 0.2301 (0.2273)	
training:	Epoch: [8][183/233]	Loss 0.2956 (0.2277)	
training:	Epoch: [8][184/233]	Loss 0.2475 (0.2278)	
training:	Epoch: [8][185/233]	Loss 0.1923 (0.2276)	
training:	Epoch: [8][186/233]	Loss 0.1733 (0.2273)	
training:	Epoch: [8][187/233]	Loss 0.2590 (0.2275)	
training:	Epoch: [8][188/233]	Loss 0.2281 (0.2275)	
training:	Epoch: [8][189/233]	Loss 0.2037 (0.2273)	
training:	Epoch: [8][190/233]	Loss 0.1970 (0.2272)	
training:	Epoch: [8][191/233]	Loss 0.2223 (0.2272)	
training:	Epoch: [8][192/233]	Loss 0.1467 (0.2267)	
training:	Epoch: [8][193/233]	Loss 0.2402 (0.2268)	
training:	Epoch: [8][194/233]	Loss 0.2597 (0.2270)	
training:	Epoch: [8][195/233]	Loss 0.1835 (0.2268)	
training:	Epoch: [8][196/233]	Loss 0.3132 (0.2272)	
training:	Epoch: [8][197/233]	Loss 0.1939 (0.2270)	
training:	Epoch: [8][198/233]	Loss 0.2723 (0.2273)	
training:	Epoch: [8][199/233]	Loss 0.2004 (0.2271)	
training:	Epoch: [8][200/233]	Loss 0.2240 (0.2271)	
training:	Epoch: [8][201/233]	Loss 0.2705 (0.2273)	
training:	Epoch: [8][202/233]	Loss 0.1752 (0.2271)	
training:	Epoch: [8][203/233]	Loss 0.2126 (0.2270)	
training:	Epoch: [8][204/233]	Loss 0.1903 (0.2268)	
training:	Epoch: [8][205/233]	Loss 0.2165 (0.2268)	
training:	Epoch: [8][206/233]	Loss 0.2665 (0.2270)	
training:	Epoch: [8][207/233]	Loss 0.1968 (0.2268)	
training:	Epoch: [8][208/233]	Loss 0.1940 (0.2267)	
training:	Epoch: [8][209/233]	Loss 0.2021 (0.2265)	
training:	Epoch: [8][210/233]	Loss 0.1749 (0.2263)	
training:	Epoch: [8][211/233]	Loss 0.1961 (0.2261)	
training:	Epoch: [8][212/233]	Loss 0.2197 (0.2261)	
training:	Epoch: [8][213/233]	Loss 0.1820 (0.2259)	
training:	Epoch: [8][214/233]	Loss 0.1768 (0.2257)	
training:	Epoch: [8][215/233]	Loss 0.1973 (0.2255)	
training:	Epoch: [8][216/233]	Loss 0.1821 (0.2253)	
training:	Epoch: [8][217/233]	Loss 0.2033 (0.2252)	
training:	Epoch: [8][218/233]	Loss 0.2035 (0.2251)	
training:	Epoch: [8][219/233]	Loss 0.2177 (0.2251)	
training:	Epoch: [8][220/233]	Loss 0.1966 (0.2250)	
training:	Epoch: [8][221/233]	Loss 0.2201 (0.2250)	
training:	Epoch: [8][222/233]	Loss 0.1786 (0.2248)	
training:	Epoch: [8][223/233]	Loss 0.2049 (0.2247)	
training:	Epoch: [8][224/233]	Loss 0.2677 (0.2249)	
training:	Epoch: [8][225/233]	Loss 0.2643 (0.2250)	
training:	Epoch: [8][226/233]	Loss 0.2143 (0.2250)	
training:	Epoch: [8][227/233]	Loss 0.2457 (0.2251)	
training:	Epoch: [8][228/233]	Loss 0.2306 (0.2251)	
training:	Epoch: [8][229/233]	Loss 0.2175 (0.2251)	
training:	Epoch: [8][230/233]	Loss 0.2230 (0.2251)	
training:	Epoch: [8][231/233]	Loss 0.2078 (0.2250)	
training:	Epoch: [8][232/233]	Loss 0.1503 (0.2247)	
training:	Epoch: [8][233/233]	Loss 0.2966 (0.2250)	
Training:	 Loss: 0.2245

Training:	 ACC: 0.9730 0.9726 0.9631 0.9829
Validation:	 ACC: 0.8490 0.8480 0.8291 0.8688
Validation:	 Best_BACC: 0.8490 0.8480 0.8291 0.8688
Validation:	 Loss: 0.3662
Pretraining:	Epoch 9/200
----------
training:	Epoch: [9][1/233]	Loss 0.2804 (0.2804)	
training:	Epoch: [9][2/233]	Loss 0.1809 (0.2307)	
training:	Epoch: [9][3/233]	Loss 0.2315 (0.2309)	
training:	Epoch: [9][4/233]	Loss 0.2215 (0.2286)	
training:	Epoch: [9][5/233]	Loss 0.2696 (0.2368)	
training:	Epoch: [9][6/233]	Loss 0.2405 (0.2374)	
training:	Epoch: [9][7/233]	Loss 0.1699 (0.2278)	
training:	Epoch: [9][8/233]	Loss 0.1680 (0.2203)	
training:	Epoch: [9][9/233]	Loss 0.2162 (0.2198)	
training:	Epoch: [9][10/233]	Loss 0.1983 (0.2177)	
training:	Epoch: [9][11/233]	Loss 0.2107 (0.2170)	
training:	Epoch: [9][12/233]	Loss 0.1751 (0.2136)	
training:	Epoch: [9][13/233]	Loss 0.5015 (0.2357)	
training:	Epoch: [9][14/233]	Loss 0.1985 (0.2331)	
training:	Epoch: [9][15/233]	Loss 0.2111 (0.2316)	
training:	Epoch: [9][16/233]	Loss 0.2278 (0.2313)	
training:	Epoch: [9][17/233]	Loss 0.2550 (0.2327)	
training:	Epoch: [9][18/233]	Loss 0.2196 (0.2320)	
training:	Epoch: [9][19/233]	Loss 0.3240 (0.2368)	
training:	Epoch: [9][20/233]	Loss 0.1939 (0.2347)	
training:	Epoch: [9][21/233]	Loss 0.1729 (0.2318)	
training:	Epoch: [9][22/233]	Loss 0.2487 (0.2325)	
training:	Epoch: [9][23/233]	Loss 0.1846 (0.2304)	
training:	Epoch: [9][24/233]	Loss 0.2051 (0.2294)	
training:	Epoch: [9][25/233]	Loss 0.1860 (0.2277)	
training:	Epoch: [9][26/233]	Loss 0.1879 (0.2261)	
training:	Epoch: [9][27/233]	Loss 0.1649 (0.2239)	
training:	Epoch: [9][28/233]	Loss 0.1753 (0.2221)	
training:	Epoch: [9][29/233]	Loss 0.2044 (0.2215)	
training:	Epoch: [9][30/233]	Loss 0.2542 (0.2226)	
training:	Epoch: [9][31/233]	Loss 0.1869 (0.2214)	
training:	Epoch: [9][32/233]	Loss 0.1787 (0.2201)	
training:	Epoch: [9][33/233]	Loss 0.2163 (0.2200)	
training:	Epoch: [9][34/233]	Loss 0.2011 (0.2194)	
training:	Epoch: [9][35/233]	Loss 0.1628 (0.2178)	
training:	Epoch: [9][36/233]	Loss 0.2694 (0.2193)	
training:	Epoch: [9][37/233]	Loss 0.2611 (0.2204)	
training:	Epoch: [9][38/233]	Loss 0.2119 (0.2202)	
training:	Epoch: [9][39/233]	Loss 0.1877 (0.2193)	
training:	Epoch: [9][40/233]	Loss 0.1630 (0.2179)	
training:	Epoch: [9][41/233]	Loss 0.1871 (0.2172)	
training:	Epoch: [9][42/233]	Loss 0.2631 (0.2183)	
training:	Epoch: [9][43/233]	Loss 0.2873 (0.2199)	
training:	Epoch: [9][44/233]	Loss 0.1682 (0.2187)	
training:	Epoch: [9][45/233]	Loss 0.1913 (0.2181)	
training:	Epoch: [9][46/233]	Loss 0.2476 (0.2187)	
training:	Epoch: [9][47/233]	Loss 0.2933 (0.2203)	
training:	Epoch: [9][48/233]	Loss 0.1834 (0.2195)	
training:	Epoch: [9][49/233]	Loss 0.2210 (0.2196)	
training:	Epoch: [9][50/233]	Loss 0.1438 (0.2181)	
training:	Epoch: [9][51/233]	Loss 0.1774 (0.2173)	
training:	Epoch: [9][52/233]	Loss 0.1793 (0.2165)	
training:	Epoch: [9][53/233]	Loss 0.2166 (0.2165)	
training:	Epoch: [9][54/233]	Loss 0.2189 (0.2166)	
training:	Epoch: [9][55/233]	Loss 0.2721 (0.2176)	
training:	Epoch: [9][56/233]	Loss 0.2097 (0.2174)	
training:	Epoch: [9][57/233]	Loss 0.2423 (0.2179)	
training:	Epoch: [9][58/233]	Loss 0.1998 (0.2176)	
training:	Epoch: [9][59/233]	Loss 0.1611 (0.2166)	
training:	Epoch: [9][60/233]	Loss 0.2088 (0.2165)	
training:	Epoch: [9][61/233]	Loss 0.1591 (0.2155)	
training:	Epoch: [9][62/233]	Loss 0.1920 (0.2152)	
training:	Epoch: [9][63/233]	Loss 0.1409 (0.2140)	
training:	Epoch: [9][64/233]	Loss 0.1798 (0.2134)	
training:	Epoch: [9][65/233]	Loss 0.1981 (0.2132)	
training:	Epoch: [9][66/233]	Loss 0.1655 (0.2125)	
training:	Epoch: [9][67/233]	Loss 0.2325 (0.2128)	
training:	Epoch: [9][68/233]	Loss 0.1732 (0.2122)	
training:	Epoch: [9][69/233]	Loss 0.3026 (0.2135)	
training:	Epoch: [9][70/233]	Loss 0.1787 (0.2130)	
training:	Epoch: [9][71/233]	Loss 0.1826 (0.2126)	
training:	Epoch: [9][72/233]	Loss 0.1511 (0.2117)	
training:	Epoch: [9][73/233]	Loss 0.2347 (0.2121)	
training:	Epoch: [9][74/233]	Loss 0.1831 (0.2117)	
training:	Epoch: [9][75/233]	Loss 0.1738 (0.2112)	
training:	Epoch: [9][76/233]	Loss 0.2123 (0.2112)	
training:	Epoch: [9][77/233]	Loss 0.4056 (0.2137)	
training:	Epoch: [9][78/233]	Loss 0.1658 (0.2131)	
training:	Epoch: [9][79/233]	Loss 0.1487 (0.2123)	
training:	Epoch: [9][80/233]	Loss 0.2947 (0.2133)	
training:	Epoch: [9][81/233]	Loss 0.1920 (0.2130)	
training:	Epoch: [9][82/233]	Loss 0.1764 (0.2126)	
training:	Epoch: [9][83/233]	Loss 0.2634 (0.2132)	
training:	Epoch: [9][84/233]	Loss 0.2136 (0.2132)	
training:	Epoch: [9][85/233]	Loss 0.1655 (0.2126)	
training:	Epoch: [9][86/233]	Loss 0.1930 (0.2124)	
training:	Epoch: [9][87/233]	Loss 0.1777 (0.2120)	
training:	Epoch: [9][88/233]	Loss 0.1994 (0.2119)	
training:	Epoch: [9][89/233]	Loss 0.1745 (0.2115)	
training:	Epoch: [9][90/233]	Loss 0.2251 (0.2116)	
training:	Epoch: [9][91/233]	Loss 0.2215 (0.2117)	
training:	Epoch: [9][92/233]	Loss 0.2313 (0.2119)	
training:	Epoch: [9][93/233]	Loss 0.3067 (0.2129)	
training:	Epoch: [9][94/233]	Loss 0.2271 (0.2131)	
training:	Epoch: [9][95/233]	Loss 0.1956 (0.2129)	
training:	Epoch: [9][96/233]	Loss 0.3297 (0.2141)	
training:	Epoch: [9][97/233]	Loss 0.1911 (0.2139)	
training:	Epoch: [9][98/233]	Loss 0.1639 (0.2134)	
training:	Epoch: [9][99/233]	Loss 0.2321 (0.2136)	
training:	Epoch: [9][100/233]	Loss 0.2545 (0.2140)	
training:	Epoch: [9][101/233]	Loss 0.1912 (0.2138)	
training:	Epoch: [9][102/233]	Loss 0.1938 (0.2136)	
training:	Epoch: [9][103/233]	Loss 0.1962 (0.2134)	
training:	Epoch: [9][104/233]	Loss 0.2607 (0.2138)	
training:	Epoch: [9][105/233]	Loss 0.1685 (0.2134)	
training:	Epoch: [9][106/233]	Loss 0.2093 (0.2134)	
training:	Epoch: [9][107/233]	Loss 0.1576 (0.2129)	
training:	Epoch: [9][108/233]	Loss 0.2237 (0.2130)	
training:	Epoch: [9][109/233]	Loss 0.2232 (0.2130)	
training:	Epoch: [9][110/233]	Loss 0.2573 (0.2134)	
training:	Epoch: [9][111/233]	Loss 0.2495 (0.2138)	
training:	Epoch: [9][112/233]	Loss 0.2270 (0.2139)	
training:	Epoch: [9][113/233]	Loss 0.1590 (0.2134)	
training:	Epoch: [9][114/233]	Loss 0.2641 (0.2139)	
training:	Epoch: [9][115/233]	Loss 0.2351 (0.2140)	
training:	Epoch: [9][116/233]	Loss 0.2253 (0.2141)	
training:	Epoch: [9][117/233]	Loss 0.2354 (0.2143)	
training:	Epoch: [9][118/233]	Loss 0.2597 (0.2147)	
training:	Epoch: [9][119/233]	Loss 0.3156 (0.2155)	
training:	Epoch: [9][120/233]	Loss 0.2423 (0.2158)	
training:	Epoch: [9][121/233]	Loss 0.2064 (0.2157)	
training:	Epoch: [9][122/233]	Loss 0.1961 (0.2155)	
training:	Epoch: [9][123/233]	Loss 0.2403 (0.2157)	
training:	Epoch: [9][124/233]	Loss 0.2294 (0.2158)	
training:	Epoch: [9][125/233]	Loss 0.2232 (0.2159)	
training:	Epoch: [9][126/233]	Loss 0.2173 (0.2159)	
training:	Epoch: [9][127/233]	Loss 0.1780 (0.2156)	
training:	Epoch: [9][128/233]	Loss 0.2007 (0.2155)	
training:	Epoch: [9][129/233]	Loss 0.1982 (0.2154)	
training:	Epoch: [9][130/233]	Loss 0.2051 (0.2153)	
training:	Epoch: [9][131/233]	Loss 0.2383 (0.2155)	
training:	Epoch: [9][132/233]	Loss 0.1518 (0.2150)	
training:	Epoch: [9][133/233]	Loss 0.2152 (0.2150)	
training:	Epoch: [9][134/233]	Loss 0.1859 (0.2148)	
training:	Epoch: [9][135/233]	Loss 0.2099 (0.2147)	
training:	Epoch: [9][136/233]	Loss 0.2140 (0.2147)	
training:	Epoch: [9][137/233]	Loss 0.2300 (0.2148)	
training:	Epoch: [9][138/233]	Loss 0.3122 (0.2155)	
training:	Epoch: [9][139/233]	Loss 0.1579 (0.2151)	
training:	Epoch: [9][140/233]	Loss 0.2875 (0.2156)	
training:	Epoch: [9][141/233]	Loss 0.2125 (0.2156)	
training:	Epoch: [9][142/233]	Loss 0.1941 (0.2155)	
training:	Epoch: [9][143/233]	Loss 0.1328 (0.2149)	
training:	Epoch: [9][144/233]	Loss 0.2493 (0.2151)	
training:	Epoch: [9][145/233]	Loss 0.2664 (0.2155)	
training:	Epoch: [9][146/233]	Loss 0.1626 (0.2151)	
training:	Epoch: [9][147/233]	Loss 0.1926 (0.2150)	
training:	Epoch: [9][148/233]	Loss 0.1471 (0.2145)	
training:	Epoch: [9][149/233]	Loss 0.1753 (0.2142)	
training:	Epoch: [9][150/233]	Loss 0.2806 (0.2147)	
training:	Epoch: [9][151/233]	Loss 0.2547 (0.2150)	
training:	Epoch: [9][152/233]	Loss 0.1752 (0.2147)	
training:	Epoch: [9][153/233]	Loss 0.1976 (0.2146)	
training:	Epoch: [9][154/233]	Loss 0.2057 (0.2145)	
training:	Epoch: [9][155/233]	Loss 0.2790 (0.2149)	
training:	Epoch: [9][156/233]	Loss 0.2282 (0.2150)	
training:	Epoch: [9][157/233]	Loss 0.1734 (0.2148)	
training:	Epoch: [9][158/233]	Loss 0.2104 (0.2147)	
training:	Epoch: [9][159/233]	Loss 0.2167 (0.2147)	
training:	Epoch: [9][160/233]	Loss 0.1655 (0.2144)	
training:	Epoch: [9][161/233]	Loss 0.2722 (0.2148)	
training:	Epoch: [9][162/233]	Loss 0.1923 (0.2147)	
training:	Epoch: [9][163/233]	Loss 0.1801 (0.2144)	
training:	Epoch: [9][164/233]	Loss 0.1975 (0.2143)	
training:	Epoch: [9][165/233]	Loss 0.2328 (0.2145)	
training:	Epoch: [9][166/233]	Loss 0.2076 (0.2144)	
training:	Epoch: [9][167/233]	Loss 0.2453 (0.2146)	
training:	Epoch: [9][168/233]	Loss 0.1771 (0.2144)	
training:	Epoch: [9][169/233]	Loss 0.2258 (0.2144)	
training:	Epoch: [9][170/233]	Loss 0.2246 (0.2145)	
training:	Epoch: [9][171/233]	Loss 0.2222 (0.2145)	
training:	Epoch: [9][172/233]	Loss 0.2208 (0.2146)	
training:	Epoch: [9][173/233]	Loss 0.2168 (0.2146)	
training:	Epoch: [9][174/233]	Loss 0.1244 (0.2141)	
training:	Epoch: [9][175/233]	Loss 0.1630 (0.2138)	
training:	Epoch: [9][176/233]	Loss 0.1625 (0.2135)	
training:	Epoch: [9][177/233]	Loss 0.2504 (0.2137)	
training:	Epoch: [9][178/233]	Loss 0.1613 (0.2134)	
training:	Epoch: [9][179/233]	Loss 0.1998 (0.2133)	
training:	Epoch: [9][180/233]	Loss 0.1710 (0.2131)	
training:	Epoch: [9][181/233]	Loss 0.2045 (0.2130)	
training:	Epoch: [9][182/233]	Loss 0.2601 (0.2133)	
training:	Epoch: [9][183/233]	Loss 0.1480 (0.2129)	
training:	Epoch: [9][184/233]	Loss 0.1857 (0.2128)	
training:	Epoch: [9][185/233]	Loss 0.1881 (0.2127)	
training:	Epoch: [9][186/233]	Loss 0.2594 (0.2129)	
training:	Epoch: [9][187/233]	Loss 0.1998 (0.2128)	
training:	Epoch: [9][188/233]	Loss 0.3281 (0.2135)	
training:	Epoch: [9][189/233]	Loss 0.1834 (0.2133)	
training:	Epoch: [9][190/233]	Loss 0.1696 (0.2131)	
training:	Epoch: [9][191/233]	Loss 0.2291 (0.2132)	
training:	Epoch: [9][192/233]	Loss 0.1880 (0.2130)	
training:	Epoch: [9][193/233]	Loss 0.2948 (0.2134)	
training:	Epoch: [9][194/233]	Loss 0.1381 (0.2131)	
training:	Epoch: [9][195/233]	Loss 0.1746 (0.2129)	
training:	Epoch: [9][196/233]	Loss 0.2348 (0.2130)	
training:	Epoch: [9][197/233]	Loss 0.1820 (0.2128)	
training:	Epoch: [9][198/233]	Loss 0.1455 (0.2125)	
training:	Epoch: [9][199/233]	Loss 0.2787 (0.2128)	
training:	Epoch: [9][200/233]	Loss 0.1911 (0.2127)	
training:	Epoch: [9][201/233]	Loss 0.1797 (0.2125)	
training:	Epoch: [9][202/233]	Loss 0.2219 (0.2126)	
training:	Epoch: [9][203/233]	Loss 0.1707 (0.2124)	
training:	Epoch: [9][204/233]	Loss 0.1857 (0.2122)	
training:	Epoch: [9][205/233]	Loss 0.2465 (0.2124)	
training:	Epoch: [9][206/233]	Loss 0.1529 (0.2121)	
training:	Epoch: [9][207/233]	Loss 0.2036 (0.2121)	
training:	Epoch: [9][208/233]	Loss 0.2435 (0.2122)	
training:	Epoch: [9][209/233]	Loss 0.2389 (0.2124)	
training:	Epoch: [9][210/233]	Loss 0.1666 (0.2121)	
training:	Epoch: [9][211/233]	Loss 0.1676 (0.2119)	
training:	Epoch: [9][212/233]	Loss 0.1652 (0.2117)	
training:	Epoch: [9][213/233]	Loss 0.1864 (0.2116)	
training:	Epoch: [9][214/233]	Loss 0.1799 (0.2114)	
training:	Epoch: [9][215/233]	Loss 0.2466 (0.2116)	
training:	Epoch: [9][216/233]	Loss 0.2149 (0.2116)	
training:	Epoch: [9][217/233]	Loss 0.2084 (0.2116)	
training:	Epoch: [9][218/233]	Loss 0.2364 (0.2117)	
training:	Epoch: [9][219/233]	Loss 0.1469 (0.2114)	
training:	Epoch: [9][220/233]	Loss 0.2387 (0.2116)	
training:	Epoch: [9][221/233]	Loss 0.1882 (0.2114)	
training:	Epoch: [9][222/233]	Loss 0.1914 (0.2114)	
training:	Epoch: [9][223/233]	Loss 0.2305 (0.2114)	
training:	Epoch: [9][224/233]	Loss 0.2140 (0.2115)	
training:	Epoch: [9][225/233]	Loss 0.1581 (0.2112)	
training:	Epoch: [9][226/233]	Loss 0.2022 (0.2112)	
training:	Epoch: [9][227/233]	Loss 0.2367 (0.2113)	
training:	Epoch: [9][228/233]	Loss 0.1832 (0.2112)	
training:	Epoch: [9][229/233]	Loss 0.1714 (0.2110)	
training:	Epoch: [9][230/233]	Loss 0.2283 (0.2111)	
training:	Epoch: [9][231/233]	Loss 0.2231 (0.2111)	
training:	Epoch: [9][232/233]	Loss 0.1576 (0.2109)	
training:	Epoch: [9][233/233]	Loss 0.2075 (0.2109)	
Training:	 Loss: 0.2104

Training:	 ACC: 0.9818 0.9815 0.9762 0.9874
Validation:	 ACC: 0.8523 0.8518 0.8403 0.8643
Validation:	 Best_BACC: 0.8523 0.8518 0.8403 0.8643
Validation:	 Loss: 0.3611
Pretraining:	Epoch 10/200
----------
training:	Epoch: [10][1/233]	Loss 0.1786 (0.1786)	
training:	Epoch: [10][2/233]	Loss 0.2932 (0.2359)	
training:	Epoch: [10][3/233]	Loss 0.2350 (0.2356)	
training:	Epoch: [10][4/233]	Loss 0.1896 (0.2241)	
training:	Epoch: [10][5/233]	Loss 0.1194 (0.2032)	
training:	Epoch: [10][6/233]	Loss 0.1433 (0.1932)	
training:	Epoch: [10][7/233]	Loss 0.1877 (0.1924)	
training:	Epoch: [10][8/233]	Loss 0.1856 (0.1915)	
training:	Epoch: [10][9/233]	Loss 0.1784 (0.1901)	
training:	Epoch: [10][10/233]	Loss 0.2433 (0.1954)	
training:	Epoch: [10][11/233]	Loss 0.1710 (0.1932)	
training:	Epoch: [10][12/233]	Loss 0.2849 (0.2008)	
training:	Epoch: [10][13/233]	Loss 0.1973 (0.2006)	
training:	Epoch: [10][14/233]	Loss 0.1823 (0.1993)	
training:	Epoch: [10][15/233]	Loss 0.2136 (0.2002)	
training:	Epoch: [10][16/233]	Loss 0.1520 (0.1972)	
training:	Epoch: [10][17/233]	Loss 0.1316 (0.1933)	
training:	Epoch: [10][18/233]	Loss 0.1810 (0.1927)	
training:	Epoch: [10][19/233]	Loss 0.1808 (0.1920)	
training:	Epoch: [10][20/233]	Loss 0.2083 (0.1928)	
training:	Epoch: [10][21/233]	Loss 0.1578 (0.1912)	
training:	Epoch: [10][22/233]	Loss 0.1745 (0.1904)	
training:	Epoch: [10][23/233]	Loss 0.1900 (0.1904)	
training:	Epoch: [10][24/233]	Loss 0.1921 (0.1905)	
training:	Epoch: [10][25/233]	Loss 0.1194 (0.1876)	
training:	Epoch: [10][26/233]	Loss 0.1397 (0.1858)	
training:	Epoch: [10][27/233]	Loss 0.1605 (0.1848)	
training:	Epoch: [10][28/233]	Loss 0.1383 (0.1832)	
training:	Epoch: [10][29/233]	Loss 0.1878 (0.1833)	
training:	Epoch: [10][30/233]	Loss 0.1990 (0.1839)	
training:	Epoch: [10][31/233]	Loss 0.1245 (0.1819)	
training:	Epoch: [10][32/233]	Loss 0.1520 (0.1810)	
training:	Epoch: [10][33/233]	Loss 0.2747 (0.1838)	
training:	Epoch: [10][34/233]	Loss 0.2455 (0.1857)	
training:	Epoch: [10][35/233]	Loss 0.1687 (0.1852)	
training:	Epoch: [10][36/233]	Loss 0.2247 (0.1863)	
training:	Epoch: [10][37/233]	Loss 0.2496 (0.1880)	
training:	Epoch: [10][38/233]	Loss 0.1577 (0.1872)	
training:	Epoch: [10][39/233]	Loss 0.1711 (0.1868)	
training:	Epoch: [10][40/233]	Loss 0.1601 (0.1861)	
training:	Epoch: [10][41/233]	Loss 0.1688 (0.1857)	
training:	Epoch: [10][42/233]	Loss 0.1820 (0.1856)	
training:	Epoch: [10][43/233]	Loss 0.2007 (0.1860)	
training:	Epoch: [10][44/233]	Loss 0.2336 (0.1870)	
training:	Epoch: [10][45/233]	Loss 0.1756 (0.1868)	
training:	Epoch: [10][46/233]	Loss 0.1555 (0.1861)	
training:	Epoch: [10][47/233]	Loss 0.1732 (0.1858)	
training:	Epoch: [10][48/233]	Loss 0.1743 (0.1856)	
training:	Epoch: [10][49/233]	Loss 0.1223 (0.1843)	
training:	Epoch: [10][50/233]	Loss 0.2110 (0.1848)	
training:	Epoch: [10][51/233]	Loss 0.1465 (0.1841)	
training:	Epoch: [10][52/233]	Loss 0.1685 (0.1838)	
training:	Epoch: [10][53/233]	Loss 0.1862 (0.1838)	
training:	Epoch: [10][54/233]	Loss 0.2270 (0.1846)	
training:	Epoch: [10][55/233]	Loss 0.1698 (0.1844)	
training:	Epoch: [10][56/233]	Loss 0.1935 (0.1845)	
training:	Epoch: [10][57/233]	Loss 0.1743 (0.1843)	
training:	Epoch: [10][58/233]	Loss 0.1843 (0.1843)	
training:	Epoch: [10][59/233]	Loss 0.2046 (0.1847)	
training:	Epoch: [10][60/233]	Loss 0.1806 (0.1846)	
training:	Epoch: [10][61/233]	Loss 0.1653 (0.1843)	
training:	Epoch: [10][62/233]	Loss 0.1842 (0.1843)	
training:	Epoch: [10][63/233]	Loss 0.1470 (0.1837)	
training:	Epoch: [10][64/233]	Loss 0.1573 (0.1833)	
training:	Epoch: [10][65/233]	Loss 0.1818 (0.1833)	
training:	Epoch: [10][66/233]	Loss 0.1447 (0.1827)	
training:	Epoch: [10][67/233]	Loss 0.1487 (0.1822)	
training:	Epoch: [10][68/233]	Loss 0.1380 (0.1815)	
training:	Epoch: [10][69/233]	Loss 0.2072 (0.1819)	
training:	Epoch: [10][70/233]	Loss 0.2566 (0.1830)	
training:	Epoch: [10][71/233]	Loss 0.1821 (0.1830)	
training:	Epoch: [10][72/233]	Loss 0.1984 (0.1832)	
training:	Epoch: [10][73/233]	Loss 0.1582 (0.1828)	
training:	Epoch: [10][74/233]	Loss 0.2404 (0.1836)	
training:	Epoch: [10][75/233]	Loss 0.2253 (0.1842)	
training:	Epoch: [10][76/233]	Loss 0.1455 (0.1836)	
training:	Epoch: [10][77/233]	Loss 0.1512 (0.1832)	
training:	Epoch: [10][78/233]	Loss 0.1502 (0.1828)	
training:	Epoch: [10][79/233]	Loss 0.1715 (0.1827)	
training:	Epoch: [10][80/233]	Loss 0.2097 (0.1830)	
training:	Epoch: [10][81/233]	Loss 0.1575 (0.1827)	
training:	Epoch: [10][82/233]	Loss 0.1596 (0.1824)	
training:	Epoch: [10][83/233]	Loss 0.2562 (0.1833)	
training:	Epoch: [10][84/233]	Loss 0.2033 (0.1835)	
training:	Epoch: [10][85/233]	Loss 0.2466 (0.1843)	
training:	Epoch: [10][86/233]	Loss 0.2141 (0.1846)	
training:	Epoch: [10][87/233]	Loss 0.1550 (0.1843)	
training:	Epoch: [10][88/233]	Loss 0.1703 (0.1841)	
training:	Epoch: [10][89/233]	Loss 0.1606 (0.1839)	
training:	Epoch: [10][90/233]	Loss 0.2329 (0.1844)	
training:	Epoch: [10][91/233]	Loss 0.1558 (0.1841)	
training:	Epoch: [10][92/233]	Loss 0.2828 (0.1852)	
training:	Epoch: [10][93/233]	Loss 0.1608 (0.1849)	
training:	Epoch: [10][94/233]	Loss 0.2110 (0.1852)	
training:	Epoch: [10][95/233]	Loss 0.2290 (0.1856)	
training:	Epoch: [10][96/233]	Loss 0.2132 (0.1859)	
training:	Epoch: [10][97/233]	Loss 0.1349 (0.1854)	
training:	Epoch: [10][98/233]	Loss 0.1653 (0.1852)	
training:	Epoch: [10][99/233]	Loss 0.1747 (0.1851)	
training:	Epoch: [10][100/233]	Loss 0.2075 (0.1853)	
training:	Epoch: [10][101/233]	Loss 0.1402 (0.1849)	
training:	Epoch: [10][102/233]	Loss 0.1706 (0.1847)	
training:	Epoch: [10][103/233]	Loss 0.1978 (0.1848)	
training:	Epoch: [10][104/233]	Loss 0.1646 (0.1847)	
training:	Epoch: [10][105/233]	Loss 0.1274 (0.1841)	
training:	Epoch: [10][106/233]	Loss 0.1794 (0.1841)	
training:	Epoch: [10][107/233]	Loss 0.1704 (0.1839)	
training:	Epoch: [10][108/233]	Loss 0.2332 (0.1844)	
training:	Epoch: [10][109/233]	Loss 0.1726 (0.1843)	
training:	Epoch: [10][110/233]	Loss 0.2303 (0.1847)	
training:	Epoch: [10][111/233]	Loss 0.1874 (0.1847)	
training:	Epoch: [10][112/233]	Loss 0.2384 (0.1852)	
training:	Epoch: [10][113/233]	Loss 0.2244 (0.1856)	
training:	Epoch: [10][114/233]	Loss 0.1962 (0.1856)	
training:	Epoch: [10][115/233]	Loss 0.2601 (0.1863)	
training:	Epoch: [10][116/233]	Loss 0.2157 (0.1865)	
training:	Epoch: [10][117/233]	Loss 0.1857 (0.1865)	
training:	Epoch: [10][118/233]	Loss 0.1335 (0.1861)	
training:	Epoch: [10][119/233]	Loss 0.1949 (0.1862)	
training:	Epoch: [10][120/233]	Loss 0.2334 (0.1866)	
training:	Epoch: [10][121/233]	Loss 0.2024 (0.1867)	
training:	Epoch: [10][122/233]	Loss 0.1920 (0.1867)	
training:	Epoch: [10][123/233]	Loss 0.1862 (0.1867)	
training:	Epoch: [10][124/233]	Loss 0.1517 (0.1864)	
training:	Epoch: [10][125/233]	Loss 0.1856 (0.1864)	
training:	Epoch: [10][126/233]	Loss 0.1864 (0.1864)	
training:	Epoch: [10][127/233]	Loss 0.1691 (0.1863)	
training:	Epoch: [10][128/233]	Loss 0.2366 (0.1867)	
training:	Epoch: [10][129/233]	Loss 0.2036 (0.1868)	
training:	Epoch: [10][130/233]	Loss 0.2000 (0.1869)	
training:	Epoch: [10][131/233]	Loss 0.1147 (0.1864)	
training:	Epoch: [10][132/233]	Loss 0.2193 (0.1866)	
training:	Epoch: [10][133/233]	Loss 0.1844 (0.1866)	
training:	Epoch: [10][134/233]	Loss 0.2240 (0.1869)	
training:	Epoch: [10][135/233]	Loss 0.2287 (0.1872)	
training:	Epoch: [10][136/233]	Loss 0.1911 (0.1872)	
training:	Epoch: [10][137/233]	Loss 0.1681 (0.1871)	
training:	Epoch: [10][138/233]	Loss 0.2054 (0.1872)	
training:	Epoch: [10][139/233]	Loss 0.1788 (0.1872)	
training:	Epoch: [10][140/233]	Loss 0.1649 (0.1870)	
training:	Epoch: [10][141/233]	Loss 0.2246 (0.1873)	
training:	Epoch: [10][142/233]	Loss 0.3421 (0.1884)	
training:	Epoch: [10][143/233]	Loss 0.1980 (0.1884)	
training:	Epoch: [10][144/233]	Loss 0.1738 (0.1883)	
training:	Epoch: [10][145/233]	Loss 0.2746 (0.1889)	
training:	Epoch: [10][146/233]	Loss 0.2210 (0.1891)	
training:	Epoch: [10][147/233]	Loss 0.1865 (0.1891)	
training:	Epoch: [10][148/233]	Loss 0.1716 (0.1890)	
training:	Epoch: [10][149/233]	Loss 0.3131 (0.1898)	
training:	Epoch: [10][150/233]	Loss 0.1727 (0.1897)	
training:	Epoch: [10][151/233]	Loss 0.1901 (0.1897)	
training:	Epoch: [10][152/233]	Loss 0.2632 (0.1902)	
training:	Epoch: [10][153/233]	Loss 0.2182 (0.1904)	
training:	Epoch: [10][154/233]	Loss 0.1793 (0.1903)	
training:	Epoch: [10][155/233]	Loss 0.1820 (0.1903)	
training:	Epoch: [10][156/233]	Loss 0.1782 (0.1902)	
training:	Epoch: [10][157/233]	Loss 0.1842 (0.1901)	
training:	Epoch: [10][158/233]	Loss 0.2179 (0.1903)	
training:	Epoch: [10][159/233]	Loss 0.1864 (0.1903)	
training:	Epoch: [10][160/233]	Loss 0.1797 (0.1902)	
training:	Epoch: [10][161/233]	Loss 0.1580 (0.1900)	
training:	Epoch: [10][162/233]	Loss 0.2292 (0.1903)	
training:	Epoch: [10][163/233]	Loss 0.1685 (0.1901)	
training:	Epoch: [10][164/233]	Loss 0.3634 (0.1912)	
training:	Epoch: [10][165/233]	Loss 0.1999 (0.1912)	
training:	Epoch: [10][166/233]	Loss 0.2456 (0.1916)	
training:	Epoch: [10][167/233]	Loss 0.2295 (0.1918)	
training:	Epoch: [10][168/233]	Loss 0.1570 (0.1916)	
training:	Epoch: [10][169/233]	Loss 0.1893 (0.1916)	
training:	Epoch: [10][170/233]	Loss 0.1947 (0.1916)	
training:	Epoch: [10][171/233]	Loss 0.2277 (0.1918)	
training:	Epoch: [10][172/233]	Loss 0.1835 (0.1918)	
training:	Epoch: [10][173/233]	Loss 0.1976 (0.1918)	
training:	Epoch: [10][174/233]	Loss 0.2452 (0.1921)	
training:	Epoch: [10][175/233]	Loss 0.2725 (0.1926)	
training:	Epoch: [10][176/233]	Loss 0.1713 (0.1924)	
training:	Epoch: [10][177/233]	Loss 0.2006 (0.1925)	
training:	Epoch: [10][178/233]	Loss 0.1806 (0.1924)	
training:	Epoch: [10][179/233]	Loss 0.1873 (0.1924)	
training:	Epoch: [10][180/233]	Loss 0.2138 (0.1925)	
training:	Epoch: [10][181/233]	Loss 0.2149 (0.1926)	
training:	Epoch: [10][182/233]	Loss 0.3236 (0.1934)	
training:	Epoch: [10][183/233]	Loss 0.1992 (0.1934)	
training:	Epoch: [10][184/233]	Loss 0.1727 (0.1933)	
training:	Epoch: [10][185/233]	Loss 0.1878 (0.1932)	
training:	Epoch: [10][186/233]	Loss 0.3205 (0.1939)	
training:	Epoch: [10][187/233]	Loss 0.2317 (0.1941)	
training:	Epoch: [10][188/233]	Loss 0.1808 (0.1941)	
training:	Epoch: [10][189/233]	Loss 0.1854 (0.1940)	
training:	Epoch: [10][190/233]	Loss 0.1911 (0.1940)	
training:	Epoch: [10][191/233]	Loss 0.1329 (0.1937)	
training:	Epoch: [10][192/233]	Loss 0.1875 (0.1936)	
training:	Epoch: [10][193/233]	Loss 0.2236 (0.1938)	
training:	Epoch: [10][194/233]	Loss 0.2106 (0.1939)	
training:	Epoch: [10][195/233]	Loss 0.1435 (0.1936)	
training:	Epoch: [10][196/233]	Loss 0.1786 (0.1936)	
training:	Epoch: [10][197/233]	Loss 0.2035 (0.1936)	
training:	Epoch: [10][198/233]	Loss 0.1751 (0.1935)	
training:	Epoch: [10][199/233]	Loss 0.2073 (0.1936)	
training:	Epoch: [10][200/233]	Loss 0.1999 (0.1936)	
training:	Epoch: [10][201/233]	Loss 0.2092 (0.1937)	
training:	Epoch: [10][202/233]	Loss 0.1622 (0.1935)	
training:	Epoch: [10][203/233]	Loss 0.1711 (0.1934)	
training:	Epoch: [10][204/233]	Loss 0.1773 (0.1933)	
training:	Epoch: [10][205/233]	Loss 0.1431 (0.1931)	
training:	Epoch: [10][206/233]	Loss 0.1450 (0.1929)	
training:	Epoch: [10][207/233]	Loss 0.2290 (0.1930)	
training:	Epoch: [10][208/233]	Loss 0.2330 (0.1932)	
training:	Epoch: [10][209/233]	Loss 0.1595 (0.1931)	
training:	Epoch: [10][210/233]	Loss 0.1750 (0.1930)	
training:	Epoch: [10][211/233]	Loss 0.1741 (0.1929)	
training:	Epoch: [10][212/233]	Loss 0.1480 (0.1927)	
training:	Epoch: [10][213/233]	Loss 0.1995 (0.1927)	
training:	Epoch: [10][214/233]	Loss 0.1494 (0.1925)	
training:	Epoch: [10][215/233]	Loss 0.2773 (0.1929)	
training:	Epoch: [10][216/233]	Loss 0.2178 (0.1930)	
training:	Epoch: [10][217/233]	Loss 0.2035 (0.1931)	
training:	Epoch: [10][218/233]	Loss 0.2158 (0.1932)	
training:	Epoch: [10][219/233]	Loss 0.2017 (0.1932)	
training:	Epoch: [10][220/233]	Loss 0.1893 (0.1932)	
training:	Epoch: [10][221/233]	Loss 0.1729 (0.1931)	
training:	Epoch: [10][222/233]	Loss 0.1934 (0.1931)	
training:	Epoch: [10][223/233]	Loss 0.1624 (0.1930)	
training:	Epoch: [10][224/233]	Loss 0.2584 (0.1933)	
training:	Epoch: [10][225/233]	Loss 0.1997 (0.1933)	
training:	Epoch: [10][226/233]	Loss 0.1611 (0.1931)	
training:	Epoch: [10][227/233]	Loss 0.2609 (0.1934)	
training:	Epoch: [10][228/233]	Loss 0.1800 (0.1934)	
training:	Epoch: [10][229/233]	Loss 0.2223 (0.1935)	
training:	Epoch: [10][230/233]	Loss 0.1456 (0.1933)	
training:	Epoch: [10][231/233]	Loss 0.2084 (0.1934)	
training:	Epoch: [10][232/233]	Loss 0.2005 (0.1934)	
training:	Epoch: [10][233/233]	Loss 0.1458 (0.1932)	
Training:	 Loss: 0.1928

Training:	 ACC: 0.9848 0.9843 0.9754 0.9941
Validation:	 ACC: 0.8467 0.8448 0.8066 0.8868
Validation:	 Best_BACC: 0.8523 0.8518 0.8403 0.8643
Validation:	 Loss: 0.3605
Pretraining:	Epoch 11/200
----------
training:	Epoch: [11][1/233]	Loss 0.1737 (0.1737)	
training:	Epoch: [11][2/233]	Loss 0.2043 (0.1890)	
training:	Epoch: [11][3/233]	Loss 0.1744 (0.1841)	
training:	Epoch: [11][4/233]	Loss 0.1671 (0.1799)	
training:	Epoch: [11][5/233]	Loss 0.2814 (0.2002)	
training:	Epoch: [11][6/233]	Loss 0.1538 (0.1924)	
training:	Epoch: [11][7/233]	Loss 0.1888 (0.1919)	
training:	Epoch: [11][8/233]	Loss 0.1814 (0.1906)	
training:	Epoch: [11][9/233]	Loss 0.1768 (0.1891)	
training:	Epoch: [11][10/233]	Loss 0.1490 (0.1851)	
training:	Epoch: [11][11/233]	Loss 0.1689 (0.1836)	
training:	Epoch: [11][12/233]	Loss 0.1841 (0.1836)	
training:	Epoch: [11][13/233]	Loss 0.1799 (0.1833)	
training:	Epoch: [11][14/233]	Loss 0.1526 (0.1811)	
training:	Epoch: [11][15/233]	Loss 0.1656 (0.1801)	
training:	Epoch: [11][16/233]	Loss 0.1716 (0.1796)	
training:	Epoch: [11][17/233]	Loss 0.1617 (0.1785)	
training:	Epoch: [11][18/233]	Loss 0.1550 (0.1772)	
training:	Epoch: [11][19/233]	Loss 0.1831 (0.1775)	
training:	Epoch: [11][20/233]	Loss 0.2218 (0.1797)	
training:	Epoch: [11][21/233]	Loss 0.1484 (0.1783)	
training:	Epoch: [11][22/233]	Loss 0.2196 (0.1801)	
training:	Epoch: [11][23/233]	Loss 0.1415 (0.1785)	
training:	Epoch: [11][24/233]	Loss 0.1957 (0.1792)	
training:	Epoch: [11][25/233]	Loss 0.1402 (0.1776)	
training:	Epoch: [11][26/233]	Loss 0.1515 (0.1766)	
training:	Epoch: [11][27/233]	Loss 0.1363 (0.1751)	
training:	Epoch: [11][28/233]	Loss 0.1665 (0.1748)	
training:	Epoch: [11][29/233]	Loss 0.1592 (0.1743)	
training:	Epoch: [11][30/233]	Loss 0.1605 (0.1738)	
training:	Epoch: [11][31/233]	Loss 0.1787 (0.1740)	
training:	Epoch: [11][32/233]	Loss 0.1635 (0.1736)	
training:	Epoch: [11][33/233]	Loss 0.1582 (0.1732)	
training:	Epoch: [11][34/233]	Loss 0.1577 (0.1727)	
training:	Epoch: [11][35/233]	Loss 0.1920 (0.1733)	
training:	Epoch: [11][36/233]	Loss 0.1475 (0.1726)	
training:	Epoch: [11][37/233]	Loss 0.1755 (0.1726)	
training:	Epoch: [11][38/233]	Loss 0.1913 (0.1731)	
training:	Epoch: [11][39/233]	Loss 0.2196 (0.1743)	
training:	Epoch: [11][40/233]	Loss 0.2773 (0.1769)	
training:	Epoch: [11][41/233]	Loss 0.1854 (0.1771)	
training:	Epoch: [11][42/233]	Loss 0.2069 (0.1778)	
training:	Epoch: [11][43/233]	Loss 0.1140 (0.1763)	
training:	Epoch: [11][44/233]	Loss 0.1936 (0.1767)	
training:	Epoch: [11][45/233]	Loss 0.1533 (0.1762)	
training:	Epoch: [11][46/233]	Loss 0.1935 (0.1766)	
training:	Epoch: [11][47/233]	Loss 0.1926 (0.1769)	
training:	Epoch: [11][48/233]	Loss 0.1258 (0.1758)	
training:	Epoch: [11][49/233]	Loss 0.2086 (0.1765)	
training:	Epoch: [11][50/233]	Loss 0.1516 (0.1760)	
training:	Epoch: [11][51/233]	Loss 0.1683 (0.1759)	
training:	Epoch: [11][52/233]	Loss 0.1140 (0.1747)	
training:	Epoch: [11][53/233]	Loss 0.1195 (0.1736)	
training:	Epoch: [11][54/233]	Loss 0.1207 (0.1727)	
training:	Epoch: [11][55/233]	Loss 0.1550 (0.1723)	
training:	Epoch: [11][56/233]	Loss 0.1790 (0.1725)	
training:	Epoch: [11][57/233]	Loss 0.1784 (0.1726)	
training:	Epoch: [11][58/233]	Loss 0.1799 (0.1727)	
training:	Epoch: [11][59/233]	Loss 0.1858 (0.1729)	
training:	Epoch: [11][60/233]	Loss 0.1944 (0.1733)	
training:	Epoch: [11][61/233]	Loss 0.1384 (0.1727)	
training:	Epoch: [11][62/233]	Loss 0.1600 (0.1725)	
training:	Epoch: [11][63/233]	Loss 0.1894 (0.1728)	
training:	Epoch: [11][64/233]	Loss 0.2556 (0.1741)	
training:	Epoch: [11][65/233]	Loss 0.2098 (0.1746)	
training:	Epoch: [11][66/233]	Loss 0.1662 (0.1745)	
training:	Epoch: [11][67/233]	Loss 0.1650 (0.1743)	
training:	Epoch: [11][68/233]	Loss 0.1196 (0.1735)	
training:	Epoch: [11][69/233]	Loss 0.2099 (0.1741)	
training:	Epoch: [11][70/233]	Loss 0.1771 (0.1741)	
training:	Epoch: [11][71/233]	Loss 0.1449 (0.1737)	
training:	Epoch: [11][72/233]	Loss 0.1503 (0.1734)	
training:	Epoch: [11][73/233]	Loss 0.1912 (0.1736)	
training:	Epoch: [11][74/233]	Loss 0.2160 (0.1742)	
training:	Epoch: [11][75/233]	Loss 0.1710 (0.1741)	
training:	Epoch: [11][76/233]	Loss 0.1844 (0.1743)	
training:	Epoch: [11][77/233]	Loss 0.1456 (0.1739)	
training:	Epoch: [11][78/233]	Loss 0.1929 (0.1741)	
training:	Epoch: [11][79/233]	Loss 0.1476 (0.1738)	
training:	Epoch: [11][80/233]	Loss 0.2113 (0.1743)	
training:	Epoch: [11][81/233]	Loss 0.2316 (0.1750)	
training:	Epoch: [11][82/233]	Loss 0.1746 (0.1750)	
training:	Epoch: [11][83/233]	Loss 0.1708 (0.1749)	
training:	Epoch: [11][84/233]	Loss 0.1528 (0.1747)	
training:	Epoch: [11][85/233]	Loss 0.1526 (0.1744)	
training:	Epoch: [11][86/233]	Loss 0.1678 (0.1743)	
training:	Epoch: [11][87/233]	Loss 0.1884 (0.1745)	
training:	Epoch: [11][88/233]	Loss 0.2139 (0.1749)	
training:	Epoch: [11][89/233]	Loss 0.1819 (0.1750)	
training:	Epoch: [11][90/233]	Loss 0.2030 (0.1753)	
training:	Epoch: [11][91/233]	Loss 0.2218 (0.1758)	
training:	Epoch: [11][92/233]	Loss 0.1792 (0.1759)	
training:	Epoch: [11][93/233]	Loss 0.1642 (0.1757)	
training:	Epoch: [11][94/233]	Loss 0.2268 (0.1763)	
training:	Epoch: [11][95/233]	Loss 0.2222 (0.1768)	
training:	Epoch: [11][96/233]	Loss 0.3037 (0.1781)	
training:	Epoch: [11][97/233]	Loss 0.1811 (0.1781)	
training:	Epoch: [11][98/233]	Loss 0.2101 (0.1785)	
training:	Epoch: [11][99/233]	Loss 0.2030 (0.1787)	
training:	Epoch: [11][100/233]	Loss 0.1309 (0.1782)	
training:	Epoch: [11][101/233]	Loss 0.2161 (0.1786)	
training:	Epoch: [11][102/233]	Loss 0.1766 (0.1786)	
training:	Epoch: [11][103/233]	Loss 0.2075 (0.1789)	
training:	Epoch: [11][104/233]	Loss 0.2003 (0.1791)	
training:	Epoch: [11][105/233]	Loss 0.1739 (0.1790)	
training:	Epoch: [11][106/233]	Loss 0.1592 (0.1788)	
training:	Epoch: [11][107/233]	Loss 0.1452 (0.1785)	
training:	Epoch: [11][108/233]	Loss 0.1769 (0.1785)	
training:	Epoch: [11][109/233]	Loss 0.1811 (0.1785)	
training:	Epoch: [11][110/233]	Loss 0.1795 (0.1785)	
training:	Epoch: [11][111/233]	Loss 0.1949 (0.1787)	
training:	Epoch: [11][112/233]	Loss 0.3171 (0.1799)	
training:	Epoch: [11][113/233]	Loss 0.1879 (0.1800)	
training:	Epoch: [11][114/233]	Loss 0.2045 (0.1802)	
training:	Epoch: [11][115/233]	Loss 0.2575 (0.1809)	
training:	Epoch: [11][116/233]	Loss 0.1074 (0.1802)	
training:	Epoch: [11][117/233]	Loss 0.1461 (0.1799)	
training:	Epoch: [11][118/233]	Loss 0.1157 (0.1794)	
training:	Epoch: [11][119/233]	Loss 0.2416 (0.1799)	
training:	Epoch: [11][120/233]	Loss 0.1898 (0.1800)	
training:	Epoch: [11][121/233]	Loss 0.1513 (0.1798)	
training:	Epoch: [11][122/233]	Loss 0.1579 (0.1796)	
training:	Epoch: [11][123/233]	Loss 0.1587 (0.1794)	
training:	Epoch: [11][124/233]	Loss 0.1976 (0.1796)	
training:	Epoch: [11][125/233]	Loss 0.1578 (0.1794)	
training:	Epoch: [11][126/233]	Loss 0.1849 (0.1794)	
training:	Epoch: [11][127/233]	Loss 0.1541 (0.1792)	
training:	Epoch: [11][128/233]	Loss 0.1935 (0.1794)	
training:	Epoch: [11][129/233]	Loss 0.1995 (0.1795)	
training:	Epoch: [11][130/233]	Loss 0.1636 (0.1794)	
training:	Epoch: [11][131/233]	Loss 0.1792 (0.1794)	
training:	Epoch: [11][132/233]	Loss 0.1623 (0.1793)	
training:	Epoch: [11][133/233]	Loss 0.1731 (0.1792)	
training:	Epoch: [11][134/233]	Loss 0.2655 (0.1799)	
training:	Epoch: [11][135/233]	Loss 0.2914 (0.1807)	
training:	Epoch: [11][136/233]	Loss 0.2127 (0.1809)	
training:	Epoch: [11][137/233]	Loss 0.1635 (0.1808)	
training:	Epoch: [11][138/233]	Loss 0.1956 (0.1809)	
training:	Epoch: [11][139/233]	Loss 0.1551 (0.1807)	
training:	Epoch: [11][140/233]	Loss 0.1882 (0.1808)	
training:	Epoch: [11][141/233]	Loss 0.1462 (0.1805)	
training:	Epoch: [11][142/233]	Loss 0.1654 (0.1804)	
training:	Epoch: [11][143/233]	Loss 0.1931 (0.1805)	
training:	Epoch: [11][144/233]	Loss 0.1477 (0.1803)	
training:	Epoch: [11][145/233]	Loss 0.2119 (0.1805)	
training:	Epoch: [11][146/233]	Loss 0.1625 (0.1804)	
training:	Epoch: [11][147/233]	Loss 0.1984 (0.1805)	
training:	Epoch: [11][148/233]	Loss 0.2077 (0.1807)	
training:	Epoch: [11][149/233]	Loss 0.2238 (0.1810)	
training:	Epoch: [11][150/233]	Loss 0.1265 (0.1806)	
training:	Epoch: [11][151/233]	Loss 0.1574 (0.1804)	
training:	Epoch: [11][152/233]	Loss 0.1498 (0.1802)	
training:	Epoch: [11][153/233]	Loss 0.1511 (0.1801)	
training:	Epoch: [11][154/233]	Loss 0.1576 (0.1799)	
training:	Epoch: [11][155/233]	Loss 0.1448 (0.1797)	
training:	Epoch: [11][156/233]	Loss 0.1710 (0.1796)	
training:	Epoch: [11][157/233]	Loss 0.1867 (0.1797)	
training:	Epoch: [11][158/233]	Loss 0.2540 (0.1801)	
training:	Epoch: [11][159/233]	Loss 0.1970 (0.1802)	
training:	Epoch: [11][160/233]	Loss 0.1398 (0.1800)	
training:	Epoch: [11][161/233]	Loss 0.1787 (0.1800)	
training:	Epoch: [11][162/233]	Loss 0.1402 (0.1797)	
training:	Epoch: [11][163/233]	Loss 0.1699 (0.1797)	
training:	Epoch: [11][164/233]	Loss 0.1564 (0.1795)	
training:	Epoch: [11][165/233]	Loss 0.2392 (0.1799)	
training:	Epoch: [11][166/233]	Loss 0.1653 (0.1798)	
training:	Epoch: [11][167/233]	Loss 0.1429 (0.1796)	
training:	Epoch: [11][168/233]	Loss 0.2464 (0.1800)	
training:	Epoch: [11][169/233]	Loss 0.1433 (0.1798)	
training:	Epoch: [11][170/233]	Loss 0.1653 (0.1797)	
training:	Epoch: [11][171/233]	Loss 0.1268 (0.1794)	
training:	Epoch: [11][172/233]	Loss 0.2035 (0.1795)	
training:	Epoch: [11][173/233]	Loss 0.1546 (0.1794)	
training:	Epoch: [11][174/233]	Loss 0.1257 (0.1791)	
training:	Epoch: [11][175/233]	Loss 0.1287 (0.1788)	
training:	Epoch: [11][176/233]	Loss 0.1843 (0.1788)	
training:	Epoch: [11][177/233]	Loss 0.2820 (0.1794)	
training:	Epoch: [11][178/233]	Loss 0.1833 (0.1794)	
training:	Epoch: [11][179/233]	Loss 0.1445 (0.1792)	
training:	Epoch: [11][180/233]	Loss 0.1511 (0.1791)	
training:	Epoch: [11][181/233]	Loss 0.1572 (0.1789)	
training:	Epoch: [11][182/233]	Loss 0.2913 (0.1796)	
training:	Epoch: [11][183/233]	Loss 0.1690 (0.1795)	
training:	Epoch: [11][184/233]	Loss 0.1426 (0.1793)	
training:	Epoch: [11][185/233]	Loss 0.1586 (0.1792)	
training:	Epoch: [11][186/233]	Loss 0.1404 (0.1790)	
training:	Epoch: [11][187/233]	Loss 0.1408 (0.1788)	
training:	Epoch: [11][188/233]	Loss 0.1454 (0.1786)	
training:	Epoch: [11][189/233]	Loss 0.1853 (0.1786)	
training:	Epoch: [11][190/233]	Loss 0.2155 (0.1788)	
training:	Epoch: [11][191/233]	Loss 0.1791 (0.1788)	
training:	Epoch: [11][192/233]	Loss 0.1916 (0.1789)	
training:	Epoch: [11][193/233]	Loss 0.3178 (0.1796)	
training:	Epoch: [11][194/233]	Loss 0.2504 (0.1800)	
training:	Epoch: [11][195/233]	Loss 0.1891 (0.1800)	
training:	Epoch: [11][196/233]	Loss 0.1661 (0.1800)	
training:	Epoch: [11][197/233]	Loss 0.2179 (0.1802)	
training:	Epoch: [11][198/233]	Loss 0.1301 (0.1799)	
training:	Epoch: [11][199/233]	Loss 0.2305 (0.1802)	
training:	Epoch: [11][200/233]	Loss 0.2548 (0.1805)	
training:	Epoch: [11][201/233]	Loss 0.1475 (0.1804)	
training:	Epoch: [11][202/233]	Loss 0.2138 (0.1805)	
training:	Epoch: [11][203/233]	Loss 0.2161 (0.1807)	
training:	Epoch: [11][204/233]	Loss 0.1384 (0.1805)	
training:	Epoch: [11][205/233]	Loss 0.1626 (0.1804)	
training:	Epoch: [11][206/233]	Loss 0.1927 (0.1805)	
training:	Epoch: [11][207/233]	Loss 0.1760 (0.1804)	
training:	Epoch: [11][208/233]	Loss 0.1985 (0.1805)	
training:	Epoch: [11][209/233]	Loss 0.1554 (0.1804)	
training:	Epoch: [11][210/233]	Loss 0.1326 (0.1802)	
training:	Epoch: [11][211/233]	Loss 0.2109 (0.1803)	
training:	Epoch: [11][212/233]	Loss 0.1486 (0.1802)	
training:	Epoch: [11][213/233]	Loss 0.1183 (0.1799)	
training:	Epoch: [11][214/233]	Loss 0.2479 (0.1802)	
training:	Epoch: [11][215/233]	Loss 0.1817 (0.1802)	
training:	Epoch: [11][216/233]	Loss 0.1605 (0.1801)	
training:	Epoch: [11][217/233]	Loss 0.1730 (0.1801)	
training:	Epoch: [11][218/233]	Loss 0.1585 (0.1800)	
training:	Epoch: [11][219/233]	Loss 0.2996 (0.1805)	
training:	Epoch: [11][220/233]	Loss 0.2114 (0.1807)	
training:	Epoch: [11][221/233]	Loss 0.1490 (0.1805)	
training:	Epoch: [11][222/233]	Loss 0.1507 (0.1804)	
training:	Epoch: [11][223/233]	Loss 0.2325 (0.1806)	
training:	Epoch: [11][224/233]	Loss 0.1196 (0.1804)	
training:	Epoch: [11][225/233]	Loss 0.1426 (0.1802)	
training:	Epoch: [11][226/233]	Loss 0.1845 (0.1802)	
training:	Epoch: [11][227/233]	Loss 0.2237 (0.1804)	
training:	Epoch: [11][228/233]	Loss 0.1477 (0.1803)	
training:	Epoch: [11][229/233]	Loss 0.1869 (0.1803)	
training:	Epoch: [11][230/233]	Loss 0.2703 (0.1807)	
training:	Epoch: [11][231/233]	Loss 0.1586 (0.1806)	
training:	Epoch: [11][232/233]	Loss 0.1697 (0.1805)	
training:	Epoch: [11][233/233]	Loss 0.1561 (0.1804)	
Training:	 Loss: 0.1800

Training:	 ACC: 0.9871 0.9868 0.9782 0.9961
Validation:	 ACC: 0.8493 0.8475 0.8096 0.8890
Validation:	 Best_BACC: 0.8523 0.8518 0.8403 0.8643
Validation:	 Loss: 0.3568
Pretraining:	Epoch 12/200
----------
training:	Epoch: [12][1/233]	Loss 0.2250 (0.2250)	
training:	Epoch: [12][2/233]	Loss 0.1837 (0.2043)	
training:	Epoch: [12][3/233]	Loss 0.1664 (0.1917)	
training:	Epoch: [12][4/233]	Loss 0.1498 (0.1812)	
training:	Epoch: [12][5/233]	Loss 0.1318 (0.1713)	
training:	Epoch: [12][6/233]	Loss 0.1971 (0.1756)	
training:	Epoch: [12][7/233]	Loss 0.1526 (0.1723)	
training:	Epoch: [12][8/233]	Loss 0.1611 (0.1709)	
training:	Epoch: [12][9/233]	Loss 0.1699 (0.1708)	
training:	Epoch: [12][10/233]	Loss 0.1551 (0.1692)	
training:	Epoch: [12][11/233]	Loss 0.2390 (0.1756)	
training:	Epoch: [12][12/233]	Loss 0.1478 (0.1733)	
training:	Epoch: [12][13/233]	Loss 0.2107 (0.1762)	
training:	Epoch: [12][14/233]	Loss 0.1410 (0.1736)	
training:	Epoch: [12][15/233]	Loss 0.1823 (0.1742)	
training:	Epoch: [12][16/233]	Loss 0.1639 (0.1736)	
training:	Epoch: [12][17/233]	Loss 0.1221 (0.1706)	
training:	Epoch: [12][18/233]	Loss 0.1906 (0.1717)	
training:	Epoch: [12][19/233]	Loss 0.1646 (0.1713)	
training:	Epoch: [12][20/233]	Loss 0.1599 (0.1707)	
training:	Epoch: [12][21/233]	Loss 0.1427 (0.1694)	
training:	Epoch: [12][22/233]	Loss 0.2025 (0.1709)	
training:	Epoch: [12][23/233]	Loss 0.1478 (0.1699)	
training:	Epoch: [12][24/233]	Loss 0.2154 (0.1718)	
training:	Epoch: [12][25/233]	Loss 0.1470 (0.1708)	
training:	Epoch: [12][26/233]	Loss 0.1755 (0.1710)	
training:	Epoch: [12][27/233]	Loss 0.1811 (0.1714)	
training:	Epoch: [12][28/233]	Loss 0.1107 (0.1692)	
training:	Epoch: [12][29/233]	Loss 0.1900 (0.1699)	
training:	Epoch: [12][30/233]	Loss 0.1780 (0.1702)	
training:	Epoch: [12][31/233]	Loss 0.1400 (0.1692)	
training:	Epoch: [12][32/233]	Loss 0.1641 (0.1690)	
training:	Epoch: [12][33/233]	Loss 0.1694 (0.1691)	
training:	Epoch: [12][34/233]	Loss 0.1532 (0.1686)	
training:	Epoch: [12][35/233]	Loss 0.1739 (0.1687)	
training:	Epoch: [12][36/233]	Loss 0.1581 (0.1684)	
training:	Epoch: [12][37/233]	Loss 0.2017 (0.1693)	
training:	Epoch: [12][38/233]	Loss 0.1966 (0.1701)	
training:	Epoch: [12][39/233]	Loss 0.1726 (0.1701)	
training:	Epoch: [12][40/233]	Loss 0.1672 (0.1701)	
training:	Epoch: [12][41/233]	Loss 0.1480 (0.1695)	
training:	Epoch: [12][42/233]	Loss 0.1369 (0.1687)	
training:	Epoch: [12][43/233]	Loss 0.1299 (0.1678)	
training:	Epoch: [12][44/233]	Loss 0.1643 (0.1678)	
training:	Epoch: [12][45/233]	Loss 0.1503 (0.1674)	
training:	Epoch: [12][46/233]	Loss 0.1876 (0.1678)	
training:	Epoch: [12][47/233]	Loss 0.1314 (0.1670)	
training:	Epoch: [12][48/233]	Loss 0.1494 (0.1667)	
training:	Epoch: [12][49/233]	Loss 0.1831 (0.1670)	
training:	Epoch: [12][50/233]	Loss 0.1891 (0.1674)	
training:	Epoch: [12][51/233]	Loss 0.2028 (0.1681)	
training:	Epoch: [12][52/233]	Loss 0.1579 (0.1679)	
training:	Epoch: [12][53/233]	Loss 0.1490 (0.1676)	
training:	Epoch: [12][54/233]	Loss 0.1503 (0.1673)	
training:	Epoch: [12][55/233]	Loss 0.1253 (0.1665)	
training:	Epoch: [12][56/233]	Loss 0.1476 (0.1662)	
training:	Epoch: [12][57/233]	Loss 0.1360 (0.1656)	
training:	Epoch: [12][58/233]	Loss 0.1472 (0.1653)	
training:	Epoch: [12][59/233]	Loss 0.1800 (0.1656)	
training:	Epoch: [12][60/233]	Loss 0.1399 (0.1651)	
training:	Epoch: [12][61/233]	Loss 0.1762 (0.1653)	
training:	Epoch: [12][62/233]	Loss 0.1725 (0.1654)	
training:	Epoch: [12][63/233]	Loss 0.1871 (0.1658)	
training:	Epoch: [12][64/233]	Loss 0.1509 (0.1655)	
training:	Epoch: [12][65/233]	Loss 0.2479 (0.1668)	
training:	Epoch: [12][66/233]	Loss 0.1913 (0.1672)	
training:	Epoch: [12][67/233]	Loss 0.1985 (0.1676)	
training:	Epoch: [12][68/233]	Loss 0.1556 (0.1675)	
training:	Epoch: [12][69/233]	Loss 0.1664 (0.1675)	
training:	Epoch: [12][70/233]	Loss 0.1505 (0.1672)	
training:	Epoch: [12][71/233]	Loss 0.1962 (0.1676)	
training:	Epoch: [12][72/233]	Loss 0.1886 (0.1679)	
training:	Epoch: [12][73/233]	Loss 0.1881 (0.1682)	
training:	Epoch: [12][74/233]	Loss 0.1226 (0.1676)	
training:	Epoch: [12][75/233]	Loss 0.1372 (0.1672)	
training:	Epoch: [12][76/233]	Loss 0.1595 (0.1671)	
training:	Epoch: [12][77/233]	Loss 0.1644 (0.1670)	
training:	Epoch: [12][78/233]	Loss 0.1368 (0.1666)	
training:	Epoch: [12][79/233]	Loss 0.2645 (0.1679)	
training:	Epoch: [12][80/233]	Loss 0.1266 (0.1674)	
training:	Epoch: [12][81/233]	Loss 0.1404 (0.1670)	
training:	Epoch: [12][82/233]	Loss 0.1433 (0.1667)	
training:	Epoch: [12][83/233]	Loss 0.0941 (0.1659)	
training:	Epoch: [12][84/233]	Loss 0.1900 (0.1662)	
training:	Epoch: [12][85/233]	Loss 0.1378 (0.1658)	
training:	Epoch: [12][86/233]	Loss 0.1734 (0.1659)	
training:	Epoch: [12][87/233]	Loss 0.2112 (0.1664)	
training:	Epoch: [12][88/233]	Loss 0.1397 (0.1661)	
training:	Epoch: [12][89/233]	Loss 0.2153 (0.1667)	
training:	Epoch: [12][90/233]	Loss 0.1729 (0.1667)	
training:	Epoch: [12][91/233]	Loss 0.1656 (0.1667)	
training:	Epoch: [12][92/233]	Loss 0.1713 (0.1668)	
training:	Epoch: [12][93/233]	Loss 0.1489 (0.1666)	
training:	Epoch: [12][94/233]	Loss 0.1662 (0.1666)	
training:	Epoch: [12][95/233]	Loss 0.2889 (0.1679)	
training:	Epoch: [12][96/233]	Loss 0.2013 (0.1682)	
training:	Epoch: [12][97/233]	Loss 0.1831 (0.1684)	
training:	Epoch: [12][98/233]	Loss 0.1834 (0.1685)	
training:	Epoch: [12][99/233]	Loss 0.1289 (0.1681)	
training:	Epoch: [12][100/233]	Loss 0.1937 (0.1684)	
training:	Epoch: [12][101/233]	Loss 0.2631 (0.1693)	
training:	Epoch: [12][102/233]	Loss 0.1248 (0.1689)	
training:	Epoch: [12][103/233]	Loss 0.1424 (0.1686)	
training:	Epoch: [12][104/233]	Loss 0.1938 (0.1689)	
training:	Epoch: [12][105/233]	Loss 0.1334 (0.1685)	
training:	Epoch: [12][106/233]	Loss 0.1357 (0.1682)	
training:	Epoch: [12][107/233]	Loss 0.1354 (0.1679)	
training:	Epoch: [12][108/233]	Loss 0.1678 (0.1679)	
training:	Epoch: [12][109/233]	Loss 0.2481 (0.1687)	
training:	Epoch: [12][110/233]	Loss 0.1657 (0.1686)	
training:	Epoch: [12][111/233]	Loss 0.2156 (0.1690)	
training:	Epoch: [12][112/233]	Loss 0.1585 (0.1690)	
training:	Epoch: [12][113/233]	Loss 0.1863 (0.1691)	
training:	Epoch: [12][114/233]	Loss 0.1749 (0.1692)	
training:	Epoch: [12][115/233]	Loss 0.1314 (0.1688)	
training:	Epoch: [12][116/233]	Loss 0.2144 (0.1692)	
training:	Epoch: [12][117/233]	Loss 0.1530 (0.1691)	
training:	Epoch: [12][118/233]	Loss 0.2158 (0.1695)	
training:	Epoch: [12][119/233]	Loss 0.1225 (0.1691)	
training:	Epoch: [12][120/233]	Loss 0.2391 (0.1697)	
training:	Epoch: [12][121/233]	Loss 0.1538 (0.1695)	
training:	Epoch: [12][122/233]	Loss 0.1427 (0.1693)	
training:	Epoch: [12][123/233]	Loss 0.1396 (0.1691)	
training:	Epoch: [12][124/233]	Loss 0.2645 (0.1698)	
training:	Epoch: [12][125/233]	Loss 0.1932 (0.1700)	
training:	Epoch: [12][126/233]	Loss 0.1528 (0.1699)	
training:	Epoch: [12][127/233]	Loss 0.1767 (0.1699)	
training:	Epoch: [12][128/233]	Loss 0.1651 (0.1699)	
training:	Epoch: [12][129/233]	Loss 0.1010 (0.1694)	
training:	Epoch: [12][130/233]	Loss 0.2069 (0.1697)	
training:	Epoch: [12][131/233]	Loss 0.1021 (0.1692)	
training:	Epoch: [12][132/233]	Loss 0.1673 (0.1691)	
training:	Epoch: [12][133/233]	Loss 0.1616 (0.1691)	
training:	Epoch: [12][134/233]	Loss 0.1617 (0.1690)	
training:	Epoch: [12][135/233]	Loss 0.1019 (0.1685)	
training:	Epoch: [12][136/233]	Loss 0.1916 (0.1687)	
training:	Epoch: [12][137/233]	Loss 0.2040 (0.1690)	
training:	Epoch: [12][138/233]	Loss 0.1426 (0.1688)	
training:	Epoch: [12][139/233]	Loss 0.1055 (0.1683)	
training:	Epoch: [12][140/233]	Loss 0.1429 (0.1681)	
training:	Epoch: [12][141/233]	Loss 0.1573 (0.1680)	
training:	Epoch: [12][142/233]	Loss 0.1826 (0.1682)	
training:	Epoch: [12][143/233]	Loss 0.2015 (0.1684)	
training:	Epoch: [12][144/233]	Loss 0.2496 (0.1689)	
training:	Epoch: [12][145/233]	Loss 0.2163 (0.1693)	
training:	Epoch: [12][146/233]	Loss 0.1039 (0.1688)	
training:	Epoch: [12][147/233]	Loss 0.1583 (0.1688)	
training:	Epoch: [12][148/233]	Loss 0.1629 (0.1687)	
training:	Epoch: [12][149/233]	Loss 0.1844 (0.1688)	
training:	Epoch: [12][150/233]	Loss 0.1828 (0.1689)	
training:	Epoch: [12][151/233]	Loss 0.1329 (0.1687)	
training:	Epoch: [12][152/233]	Loss 0.1385 (0.1685)	
training:	Epoch: [12][153/233]	Loss 0.1892 (0.1686)	
training:	Epoch: [12][154/233]	Loss 0.1728 (0.1686)	
training:	Epoch: [12][155/233]	Loss 0.1939 (0.1688)	
training:	Epoch: [12][156/233]	Loss 0.1440 (0.1686)	
training:	Epoch: [12][157/233]	Loss 0.1881 (0.1688)	
training:	Epoch: [12][158/233]	Loss 0.1903 (0.1689)	
training:	Epoch: [12][159/233]	Loss 0.2111 (0.1692)	
training:	Epoch: [12][160/233]	Loss 0.1805 (0.1692)	
training:	Epoch: [12][161/233]	Loss 0.1563 (0.1692)	
training:	Epoch: [12][162/233]	Loss 0.1956 (0.1693)	
training:	Epoch: [12][163/233]	Loss 0.1699 (0.1693)	
training:	Epoch: [12][164/233]	Loss 0.1869 (0.1694)	
training:	Epoch: [12][165/233]	Loss 0.1205 (0.1691)	
training:	Epoch: [12][166/233]	Loss 0.2466 (0.1696)	
training:	Epoch: [12][167/233]	Loss 0.1598 (0.1695)	
training:	Epoch: [12][168/233]	Loss 0.2204 (0.1698)	
training:	Epoch: [12][169/233]	Loss 0.1795 (0.1699)	
training:	Epoch: [12][170/233]	Loss 0.1874 (0.1700)	
training:	Epoch: [12][171/233]	Loss 0.1742 (0.1700)	
training:	Epoch: [12][172/233]	Loss 0.2509 (0.1705)	
training:	Epoch: [12][173/233]	Loss 0.1889 (0.1706)	
training:	Epoch: [12][174/233]	Loss 0.1445 (0.1705)	
training:	Epoch: [12][175/233]	Loss 0.2676 (0.1710)	
training:	Epoch: [12][176/233]	Loss 0.1365 (0.1708)	
training:	Epoch: [12][177/233]	Loss 0.2160 (0.1711)	
training:	Epoch: [12][178/233]	Loss 0.1533 (0.1710)	
training:	Epoch: [12][179/233]	Loss 0.1755 (0.1710)	
training:	Epoch: [12][180/233]	Loss 0.1356 (0.1708)	
training:	Epoch: [12][181/233]	Loss 0.1264 (0.1706)	
training:	Epoch: [12][182/233]	Loss 0.1604 (0.1705)	
training:	Epoch: [12][183/233]	Loss 0.1944 (0.1706)	
training:	Epoch: [12][184/233]	Loss 0.1791 (0.1707)	
training:	Epoch: [12][185/233]	Loss 0.1816 (0.1707)	
training:	Epoch: [12][186/233]	Loss 0.1464 (0.1706)	
training:	Epoch: [12][187/233]	Loss 0.1442 (0.1705)	
training:	Epoch: [12][188/233]	Loss 0.1596 (0.1704)	
training:	Epoch: [12][189/233]	Loss 0.1534 (0.1703)	
training:	Epoch: [12][190/233]	Loss 0.1340 (0.1701)	
training:	Epoch: [12][191/233]	Loss 0.2563 (0.1706)	
training:	Epoch: [12][192/233]	Loss 0.3042 (0.1713)	
training:	Epoch: [12][193/233]	Loss 0.2153 (0.1715)	
training:	Epoch: [12][194/233]	Loss 0.1686 (0.1715)	
training:	Epoch: [12][195/233]	Loss 0.1505 (0.1714)	
training:	Epoch: [12][196/233]	Loss 0.1623 (0.1713)	
training:	Epoch: [12][197/233]	Loss 0.1448 (0.1712)	
training:	Epoch: [12][198/233]	Loss 0.1711 (0.1712)	
training:	Epoch: [12][199/233]	Loss 0.1279 (0.1710)	
training:	Epoch: [12][200/233]	Loss 0.1179 (0.1707)	
training:	Epoch: [12][201/233]	Loss 0.1382 (0.1706)	
training:	Epoch: [12][202/233]	Loss 0.1880 (0.1706)	
training:	Epoch: [12][203/233]	Loss 0.2268 (0.1709)	
training:	Epoch: [12][204/233]	Loss 0.1562 (0.1708)	
training:	Epoch: [12][205/233]	Loss 0.1576 (0.1708)	
training:	Epoch: [12][206/233]	Loss 0.1522 (0.1707)	
training:	Epoch: [12][207/233]	Loss 0.1484 (0.1706)	
training:	Epoch: [12][208/233]	Loss 0.1552 (0.1705)	
training:	Epoch: [12][209/233]	Loss 0.1780 (0.1705)	
training:	Epoch: [12][210/233]	Loss 0.1176 (0.1703)	
training:	Epoch: [12][211/233]	Loss 0.1750 (0.1703)	
training:	Epoch: [12][212/233]	Loss 0.2778 (0.1708)	
training:	Epoch: [12][213/233]	Loss 0.2076 (0.1710)	
training:	Epoch: [12][214/233]	Loss 0.1296 (0.1708)	
training:	Epoch: [12][215/233]	Loss 0.1911 (0.1709)	
training:	Epoch: [12][216/233]	Loss 0.1538 (0.1708)	
training:	Epoch: [12][217/233]	Loss 0.1536 (0.1707)	
training:	Epoch: [12][218/233]	Loss 0.1599 (0.1707)	
training:	Epoch: [12][219/233]	Loss 0.0933 (0.1703)	
training:	Epoch: [12][220/233]	Loss 0.1329 (0.1702)	
training:	Epoch: [12][221/233]	Loss 0.2451 (0.1705)	
training:	Epoch: [12][222/233]	Loss 0.1696 (0.1705)	
training:	Epoch: [12][223/233]	Loss 0.1469 (0.1704)	
training:	Epoch: [12][224/233]	Loss 0.1740 (0.1704)	
training:	Epoch: [12][225/233]	Loss 0.2085 (0.1706)	
training:	Epoch: [12][226/233]	Loss 0.2376 (0.1709)	
training:	Epoch: [12][227/233]	Loss 0.1769 (0.1709)	
training:	Epoch: [12][228/233]	Loss 0.1403 (0.1708)	
training:	Epoch: [12][229/233]	Loss 0.2398 (0.1711)	
training:	Epoch: [12][230/233]	Loss 0.1949 (0.1712)	
training:	Epoch: [12][231/233]	Loss 0.1288 (0.1710)	
training:	Epoch: [12][232/233]	Loss 0.1847 (0.1710)	
training:	Epoch: [12][233/233]	Loss 0.1297 (0.1709)	
Training:	 Loss: 0.1705

Training:	 ACC: 0.9938 0.9937 0.9918 0.9958
Validation:	 ACC: 0.8619 0.8620 0.8628 0.8610
Validation:	 Best_BACC: 0.8619 0.8620 0.8628 0.8610
Validation:	 Loss: 0.3474
Pretraining:	Epoch 13/200
----------
training:	Epoch: [13][1/233]	Loss 0.1442 (0.1442)	
training:	Epoch: [13][2/233]	Loss 0.1391 (0.1417)	
training:	Epoch: [13][3/233]	Loss 0.1556 (0.1463)	
training:	Epoch: [13][4/233]	Loss 0.2796 (0.1796)	
training:	Epoch: [13][5/233]	Loss 0.1865 (0.1810)	
training:	Epoch: [13][6/233]	Loss 0.1424 (0.1746)	
training:	Epoch: [13][7/233]	Loss 0.1261 (0.1676)	
training:	Epoch: [13][8/233]	Loss 0.1384 (0.1640)	
training:	Epoch: [13][9/233]	Loss 0.1255 (0.1597)	
training:	Epoch: [13][10/233]	Loss 0.1137 (0.1551)	
training:	Epoch: [13][11/233]	Loss 0.1599 (0.1555)	
training:	Epoch: [13][12/233]	Loss 0.1658 (0.1564)	
training:	Epoch: [13][13/233]	Loss 0.1939 (0.1593)	
training:	Epoch: [13][14/233]	Loss 0.1337 (0.1575)	
training:	Epoch: [13][15/233]	Loss 0.1408 (0.1563)	
training:	Epoch: [13][16/233]	Loss 0.1704 (0.1572)	
training:	Epoch: [13][17/233]	Loss 0.1614 (0.1575)	
training:	Epoch: [13][18/233]	Loss 0.1586 (0.1575)	
training:	Epoch: [13][19/233]	Loss 0.1185 (0.1555)	
training:	Epoch: [13][20/233]	Loss 0.1477 (0.1551)	
training:	Epoch: [13][21/233]	Loss 0.1680 (0.1557)	
training:	Epoch: [13][22/233]	Loss 0.1235 (0.1542)	
training:	Epoch: [13][23/233]	Loss 0.1424 (0.1537)	
training:	Epoch: [13][24/233]	Loss 0.1444 (0.1533)	
training:	Epoch: [13][25/233]	Loss 0.1790 (0.1544)	
training:	Epoch: [13][26/233]	Loss 0.1327 (0.1535)	
training:	Epoch: [13][27/233]	Loss 0.1657 (0.1540)	
training:	Epoch: [13][28/233]	Loss 0.1809 (0.1549)	
training:	Epoch: [13][29/233]	Loss 0.1110 (0.1534)	
training:	Epoch: [13][30/233]	Loss 0.1424 (0.1531)	
training:	Epoch: [13][31/233]	Loss 0.1179 (0.1519)	
training:	Epoch: [13][32/233]	Loss 0.1076 (0.1505)	
training:	Epoch: [13][33/233]	Loss 0.2095 (0.1523)	
training:	Epoch: [13][34/233]	Loss 0.1405 (0.1520)	
training:	Epoch: [13][35/233]	Loss 0.3259 (0.1569)	
training:	Epoch: [13][36/233]	Loss 0.1393 (0.1565)	
training:	Epoch: [13][37/233]	Loss 0.1461 (0.1562)	
training:	Epoch: [13][38/233]	Loss 0.1573 (0.1562)	
training:	Epoch: [13][39/233]	Loss 0.1240 (0.1554)	
training:	Epoch: [13][40/233]	Loss 0.1986 (0.1565)	
training:	Epoch: [13][41/233]	Loss 0.1550 (0.1564)	
training:	Epoch: [13][42/233]	Loss 0.1257 (0.1557)	
training:	Epoch: [13][43/233]	Loss 0.1693 (0.1560)	
training:	Epoch: [13][44/233]	Loss 0.1131 (0.1550)	
training:	Epoch: [13][45/233]	Loss 0.1183 (0.1542)	
training:	Epoch: [13][46/233]	Loss 0.1581 (0.1543)	
training:	Epoch: [13][47/233]	Loss 0.1267 (0.1537)	
training:	Epoch: [13][48/233]	Loss 0.1332 (0.1533)	
training:	Epoch: [13][49/233]	Loss 0.1530 (0.1533)	
training:	Epoch: [13][50/233]	Loss 0.2033 (0.1543)	
training:	Epoch: [13][51/233]	Loss 0.1280 (0.1538)	
training:	Epoch: [13][52/233]	Loss 0.2054 (0.1548)	
training:	Epoch: [13][53/233]	Loss 0.1500 (0.1547)	
training:	Epoch: [13][54/233]	Loss 0.1986 (0.1555)	
training:	Epoch: [13][55/233]	Loss 0.1491 (0.1554)	
training:	Epoch: [13][56/233]	Loss 0.1245 (0.1548)	
training:	Epoch: [13][57/233]	Loss 0.1482 (0.1547)	
training:	Epoch: [13][58/233]	Loss 0.1125 (0.1540)	
training:	Epoch: [13][59/233]	Loss 0.1589 (0.1541)	
training:	Epoch: [13][60/233]	Loss 0.1320 (0.1537)	
training:	Epoch: [13][61/233]	Loss 0.1934 (0.1543)	
training:	Epoch: [13][62/233]	Loss 0.1333 (0.1540)	
training:	Epoch: [13][63/233]	Loss 0.1697 (0.1542)	
training:	Epoch: [13][64/233]	Loss 0.1972 (0.1549)	
training:	Epoch: [13][65/233]	Loss 0.1548 (0.1549)	
training:	Epoch: [13][66/233]	Loss 0.1489 (0.1548)	
training:	Epoch: [13][67/233]	Loss 0.1541 (0.1548)	
training:	Epoch: [13][68/233]	Loss 0.1530 (0.1548)	
training:	Epoch: [13][69/233]	Loss 0.1494 (0.1547)	
training:	Epoch: [13][70/233]	Loss 0.1305 (0.1544)	
training:	Epoch: [13][71/233]	Loss 0.1417 (0.1542)	
training:	Epoch: [13][72/233]	Loss 0.1649 (0.1543)	
training:	Epoch: [13][73/233]	Loss 0.1476 (0.1542)	
training:	Epoch: [13][74/233]	Loss 0.1754 (0.1545)	
training:	Epoch: [13][75/233]	Loss 0.1089 (0.1539)	
training:	Epoch: [13][76/233]	Loss 0.1698 (0.1541)	
training:	Epoch: [13][77/233]	Loss 0.1601 (0.1542)	
training:	Epoch: [13][78/233]	Loss 0.1526 (0.1542)	
training:	Epoch: [13][79/233]	Loss 0.1373 (0.1540)	
training:	Epoch: [13][80/233]	Loss 0.1437 (0.1538)	
training:	Epoch: [13][81/233]	Loss 0.1195 (0.1534)	
training:	Epoch: [13][82/233]	Loss 0.1480 (0.1534)	
training:	Epoch: [13][83/233]	Loss 0.1346 (0.1531)	
training:	Epoch: [13][84/233]	Loss 0.1358 (0.1529)	
training:	Epoch: [13][85/233]	Loss 0.0928 (0.1522)	
training:	Epoch: [13][86/233]	Loss 0.1164 (0.1518)	
training:	Epoch: [13][87/233]	Loss 0.1957 (0.1523)	
training:	Epoch: [13][88/233]	Loss 0.1517 (0.1523)	
training:	Epoch: [13][89/233]	Loss 0.1949 (0.1528)	
training:	Epoch: [13][90/233]	Loss 0.1300 (0.1525)	
training:	Epoch: [13][91/233]	Loss 0.1322 (0.1523)	
training:	Epoch: [13][92/233]	Loss 0.1644 (0.1524)	
training:	Epoch: [13][93/233]	Loss 0.2556 (0.1535)	
training:	Epoch: [13][94/233]	Loss 0.1525 (0.1535)	
training:	Epoch: [13][95/233]	Loss 0.1474 (0.1535)	
training:	Epoch: [13][96/233]	Loss 0.1547 (0.1535)	
training:	Epoch: [13][97/233]	Loss 0.1085 (0.1530)	
training:	Epoch: [13][98/233]	Loss 0.1543 (0.1530)	
training:	Epoch: [13][99/233]	Loss 0.1395 (0.1529)	
training:	Epoch: [13][100/233]	Loss 0.1034 (0.1524)	
training:	Epoch: [13][101/233]	Loss 0.1382 (0.1523)	
training:	Epoch: [13][102/233]	Loss 0.1621 (0.1524)	
training:	Epoch: [13][103/233]	Loss 0.1613 (0.1524)	
training:	Epoch: [13][104/233]	Loss 0.1353 (0.1523)	
training:	Epoch: [13][105/233]	Loss 0.1840 (0.1526)	
training:	Epoch: [13][106/233]	Loss 0.1348 (0.1524)	
training:	Epoch: [13][107/233]	Loss 0.1603 (0.1525)	
training:	Epoch: [13][108/233]	Loss 0.1176 (0.1522)	
training:	Epoch: [13][109/233]	Loss 0.1559 (0.1522)	
training:	Epoch: [13][110/233]	Loss 0.1304 (0.1520)	
training:	Epoch: [13][111/233]	Loss 0.1202 (0.1517)	
training:	Epoch: [13][112/233]	Loss 0.1200 (0.1514)	
training:	Epoch: [13][113/233]	Loss 0.1595 (0.1515)	
training:	Epoch: [13][114/233]	Loss 0.1212 (0.1512)	
training:	Epoch: [13][115/233]	Loss 0.1508 (0.1512)	
training:	Epoch: [13][116/233]	Loss 0.1324 (0.1511)	
training:	Epoch: [13][117/233]	Loss 0.1608 (0.1511)	
training:	Epoch: [13][118/233]	Loss 0.1253 (0.1509)	
training:	Epoch: [13][119/233]	Loss 0.1634 (0.1510)	
training:	Epoch: [13][120/233]	Loss 0.2547 (0.1519)	
training:	Epoch: [13][121/233]	Loss 0.2040 (0.1523)	
training:	Epoch: [13][122/233]	Loss 0.1671 (0.1524)	
training:	Epoch: [13][123/233]	Loss 0.1269 (0.1522)	
training:	Epoch: [13][124/233]	Loss 0.1642 (0.1523)	
training:	Epoch: [13][125/233]	Loss 0.1281 (0.1521)	
training:	Epoch: [13][126/233]	Loss 0.1545 (0.1522)	
training:	Epoch: [13][127/233]	Loss 0.1751 (0.1523)	
training:	Epoch: [13][128/233]	Loss 0.1632 (0.1524)	
training:	Epoch: [13][129/233]	Loss 0.1442 (0.1524)	
training:	Epoch: [13][130/233]	Loss 0.1758 (0.1525)	
training:	Epoch: [13][131/233]	Loss 0.1529 (0.1525)	
training:	Epoch: [13][132/233]	Loss 0.1708 (0.1527)	
training:	Epoch: [13][133/233]	Loss 0.1442 (0.1526)	
training:	Epoch: [13][134/233]	Loss 0.1591 (0.1527)	
training:	Epoch: [13][135/233]	Loss 0.1817 (0.1529)	
training:	Epoch: [13][136/233]	Loss 0.1262 (0.1527)	
training:	Epoch: [13][137/233]	Loss 0.1526 (0.1527)	
training:	Epoch: [13][138/233]	Loss 0.1430 (0.1526)	
training:	Epoch: [13][139/233]	Loss 0.1378 (0.1525)	
training:	Epoch: [13][140/233]	Loss 0.1248 (0.1523)	
training:	Epoch: [13][141/233]	Loss 0.1352 (0.1522)	
training:	Epoch: [13][142/233]	Loss 0.1753 (0.1524)	
training:	Epoch: [13][143/233]	Loss 0.1333 (0.1522)	
training:	Epoch: [13][144/233]	Loss 0.1469 (0.1522)	
training:	Epoch: [13][145/233]	Loss 0.1352 (0.1521)	
training:	Epoch: [13][146/233]	Loss 0.1732 (0.1522)	
training:	Epoch: [13][147/233]	Loss 0.1136 (0.1519)	
training:	Epoch: [13][148/233]	Loss 0.1154 (0.1517)	
training:	Epoch: [13][149/233]	Loss 0.1314 (0.1516)	
training:	Epoch: [13][150/233]	Loss 0.1912 (0.1518)	
training:	Epoch: [13][151/233]	Loss 0.1436 (0.1518)	
training:	Epoch: [13][152/233]	Loss 0.1503 (0.1518)	
training:	Epoch: [13][153/233]	Loss 0.1355 (0.1517)	
training:	Epoch: [13][154/233]	Loss 0.1998 (0.1520)	
training:	Epoch: [13][155/233]	Loss 0.1656 (0.1521)	
training:	Epoch: [13][156/233]	Loss 0.2326 (0.1526)	
training:	Epoch: [13][157/233]	Loss 0.1288 (0.1524)	
training:	Epoch: [13][158/233]	Loss 0.1726 (0.1526)	
training:	Epoch: [13][159/233]	Loss 0.1550 (0.1526)	
training:	Epoch: [13][160/233]	Loss 0.1861 (0.1528)	
training:	Epoch: [13][161/233]	Loss 0.1253 (0.1526)	
training:	Epoch: [13][162/233]	Loss 0.1310 (0.1525)	
training:	Epoch: [13][163/233]	Loss 0.1924 (0.1527)	
training:	Epoch: [13][164/233]	Loss 0.2904 (0.1536)	
training:	Epoch: [13][165/233]	Loss 0.1737 (0.1537)	
training:	Epoch: [13][166/233]	Loss 0.1781 (0.1538)	
training:	Epoch: [13][167/233]	Loss 0.1655 (0.1539)	
training:	Epoch: [13][168/233]	Loss 0.1229 (0.1537)	
training:	Epoch: [13][169/233]	Loss 0.1698 (0.1538)	
training:	Epoch: [13][170/233]	Loss 0.2191 (0.1542)	
training:	Epoch: [13][171/233]	Loss 0.1574 (0.1542)	
training:	Epoch: [13][172/233]	Loss 0.1111 (0.1540)	
training:	Epoch: [13][173/233]	Loss 0.1475 (0.1539)	
training:	Epoch: [13][174/233]	Loss 0.1561 (0.1539)	
training:	Epoch: [13][175/233]	Loss 0.1366 (0.1538)	
training:	Epoch: [13][176/233]	Loss 0.1872 (0.1540)	
training:	Epoch: [13][177/233]	Loss 0.2066 (0.1543)	
training:	Epoch: [13][178/233]	Loss 0.1391 (0.1542)	
training:	Epoch: [13][179/233]	Loss 0.1768 (0.1544)	
training:	Epoch: [13][180/233]	Loss 0.1324 (0.1542)	
training:	Epoch: [13][181/233]	Loss 0.1103 (0.1540)	
training:	Epoch: [13][182/233]	Loss 0.2042 (0.1543)	
training:	Epoch: [13][183/233]	Loss 0.1206 (0.1541)	
training:	Epoch: [13][184/233]	Loss 0.1826 (0.1542)	
training:	Epoch: [13][185/233]	Loss 0.1635 (0.1543)	
training:	Epoch: [13][186/233]	Loss 0.1317 (0.1542)	
training:	Epoch: [13][187/233]	Loss 0.1192 (0.1540)	
training:	Epoch: [13][188/233]	Loss 0.1912 (0.1542)	
training:	Epoch: [13][189/233]	Loss 0.0957 (0.1539)	
training:	Epoch: [13][190/233]	Loss 0.1929 (0.1541)	
training:	Epoch: [13][191/233]	Loss 0.2030 (0.1543)	
training:	Epoch: [13][192/233]	Loss 0.1828 (0.1545)	
training:	Epoch: [13][193/233]	Loss 0.1937 (0.1547)	
training:	Epoch: [13][194/233]	Loss 0.1907 (0.1549)	
training:	Epoch: [13][195/233]	Loss 0.2002 (0.1551)	
training:	Epoch: [13][196/233]	Loss 0.1655 (0.1552)	
training:	Epoch: [13][197/233]	Loss 0.2168 (0.1555)	
training:	Epoch: [13][198/233]	Loss 0.1262 (0.1553)	
training:	Epoch: [13][199/233]	Loss 0.1447 (0.1553)	
training:	Epoch: [13][200/233]	Loss 0.1959 (0.1555)	
training:	Epoch: [13][201/233]	Loss 0.1461 (0.1554)	
training:	Epoch: [13][202/233]	Loss 0.1213 (0.1553)	
training:	Epoch: [13][203/233]	Loss 0.1627 (0.1553)	
training:	Epoch: [13][204/233]	Loss 0.1543 (0.1553)	
training:	Epoch: [13][205/233]	Loss 0.1414 (0.1552)	
training:	Epoch: [13][206/233]	Loss 0.1659 (0.1553)	
training:	Epoch: [13][207/233]	Loss 0.1866 (0.1554)	
training:	Epoch: [13][208/233]	Loss 0.1303 (0.1553)	
training:	Epoch: [13][209/233]	Loss 0.1353 (0.1552)	
training:	Epoch: [13][210/233]	Loss 0.1304 (0.1551)	
training:	Epoch: [13][211/233]	Loss 0.1449 (0.1550)	
training:	Epoch: [13][212/233]	Loss 0.1371 (0.1550)	
training:	Epoch: [13][213/233]	Loss 0.1300 (0.1548)	
training:	Epoch: [13][214/233]	Loss 0.3520 (0.1558)	
training:	Epoch: [13][215/233]	Loss 0.1934 (0.1559)	
training:	Epoch: [13][216/233]	Loss 0.1125 (0.1557)	
training:	Epoch: [13][217/233]	Loss 0.1353 (0.1556)	
training:	Epoch: [13][218/233]	Loss 0.1208 (0.1555)	
training:	Epoch: [13][219/233]	Loss 0.1384 (0.1554)	
training:	Epoch: [13][220/233]	Loss 0.1838 (0.1555)	
training:	Epoch: [13][221/233]	Loss 0.1414 (0.1555)	
training:	Epoch: [13][222/233]	Loss 0.2925 (0.1561)	
training:	Epoch: [13][223/233]	Loss 0.1517 (0.1561)	
training:	Epoch: [13][224/233]	Loss 0.1324 (0.1560)	
training:	Epoch: [13][225/233]	Loss 0.1430 (0.1559)	
training:	Epoch: [13][226/233]	Loss 0.1722 (0.1560)	
training:	Epoch: [13][227/233]	Loss 0.1340 (0.1559)	
training:	Epoch: [13][228/233]	Loss 0.0978 (0.1556)	
training:	Epoch: [13][229/233]	Loss 0.1440 (0.1556)	
training:	Epoch: [13][230/233]	Loss 0.1522 (0.1556)	
training:	Epoch: [13][231/233]	Loss 0.1829 (0.1557)	
training:	Epoch: [13][232/233]	Loss 0.1663 (0.1557)	
training:	Epoch: [13][233/233]	Loss 0.0980 (0.1555)	
Training:	 Loss: 0.1551

Training:	 ACC: 0.9950 0.9948 0.9905 0.9994
Validation:	 ACC: 0.8507 0.8491 0.8158 0.8857
Validation:	 Best_BACC: 0.8619 0.8620 0.8628 0.8610
Validation:	 Loss: 0.3491
Pretraining:	Epoch 14/200
----------
training:	Epoch: [14][1/233]	Loss 0.1337 (0.1337)	
training:	Epoch: [14][2/233]	Loss 0.1451 (0.1394)	
training:	Epoch: [14][3/233]	Loss 0.2065 (0.1618)	
training:	Epoch: [14][4/233]	Loss 0.1363 (0.1554)	
training:	Epoch: [14][5/233]	Loss 0.1882 (0.1620)	
training:	Epoch: [14][6/233]	Loss 0.1279 (0.1563)	
training:	Epoch: [14][7/233]	Loss 0.1818 (0.1600)	
training:	Epoch: [14][8/233]	Loss 0.1301 (0.1562)	
training:	Epoch: [14][9/233]	Loss 0.0940 (0.1493)	
training:	Epoch: [14][10/233]	Loss 0.1611 (0.1505)	
training:	Epoch: [14][11/233]	Loss 0.1004 (0.1459)	
training:	Epoch: [14][12/233]	Loss 0.1361 (0.1451)	
training:	Epoch: [14][13/233]	Loss 0.1520 (0.1457)	
training:	Epoch: [14][14/233]	Loss 0.1244 (0.1441)	
training:	Epoch: [14][15/233]	Loss 0.1916 (0.1473)	
training:	Epoch: [14][16/233]	Loss 0.1767 (0.1491)	
training:	Epoch: [14][17/233]	Loss 0.1224 (0.1476)	
training:	Epoch: [14][18/233]	Loss 0.1629 (0.1484)	
training:	Epoch: [14][19/233]	Loss 0.1689 (0.1495)	
training:	Epoch: [14][20/233]	Loss 0.1114 (0.1476)	
training:	Epoch: [14][21/233]	Loss 0.1123 (0.1459)	
training:	Epoch: [14][22/233]	Loss 0.1899 (0.1479)	
training:	Epoch: [14][23/233]	Loss 0.1373 (0.1475)	
training:	Epoch: [14][24/233]	Loss 0.1518 (0.1476)	
training:	Epoch: [14][25/233]	Loss 0.1236 (0.1467)	
training:	Epoch: [14][26/233]	Loss 0.1384 (0.1464)	
training:	Epoch: [14][27/233]	Loss 0.1959 (0.1482)	
training:	Epoch: [14][28/233]	Loss 0.1205 (0.1472)	
training:	Epoch: [14][29/233]	Loss 0.2186 (0.1497)	
training:	Epoch: [14][30/233]	Loss 0.1205 (0.1487)	
training:	Epoch: [14][31/233]	Loss 0.1401 (0.1484)	
training:	Epoch: [14][32/233]	Loss 0.1559 (0.1486)	
training:	Epoch: [14][33/233]	Loss 0.1688 (0.1493)	
training:	Epoch: [14][34/233]	Loss 0.1282 (0.1486)	
training:	Epoch: [14][35/233]	Loss 0.1462 (0.1486)	
training:	Epoch: [14][36/233]	Loss 0.2007 (0.1500)	
training:	Epoch: [14][37/233]	Loss 0.1418 (0.1498)	
training:	Epoch: [14][38/233]	Loss 0.1998 (0.1511)	
training:	Epoch: [14][39/233]	Loss 0.1582 (0.1513)	
training:	Epoch: [14][40/233]	Loss 0.1433 (0.1511)	
training:	Epoch: [14][41/233]	Loss 0.1661 (0.1515)	
training:	Epoch: [14][42/233]	Loss 0.1550 (0.1515)	
training:	Epoch: [14][43/233]	Loss 0.1657 (0.1519)	
training:	Epoch: [14][44/233]	Loss 0.1168 (0.1511)	
training:	Epoch: [14][45/233]	Loss 0.1005 (0.1500)	
training:	Epoch: [14][46/233]	Loss 0.1129 (0.1491)	
training:	Epoch: [14][47/233]	Loss 0.1303 (0.1487)	
training:	Epoch: [14][48/233]	Loss 0.1260 (0.1483)	
training:	Epoch: [14][49/233]	Loss 0.1687 (0.1487)	
training:	Epoch: [14][50/233]	Loss 0.1261 (0.1482)	
training:	Epoch: [14][51/233]	Loss 0.1963 (0.1492)	
training:	Epoch: [14][52/233]	Loss 0.3257 (0.1526)	
training:	Epoch: [14][53/233]	Loss 0.1314 (0.1522)	
training:	Epoch: [14][54/233]	Loss 0.1452 (0.1520)	
training:	Epoch: [14][55/233]	Loss 0.1177 (0.1514)	
training:	Epoch: [14][56/233]	Loss 0.1916 (0.1521)	
training:	Epoch: [14][57/233]	Loss 0.1104 (0.1514)	
training:	Epoch: [14][58/233]	Loss 0.1805 (0.1519)	
training:	Epoch: [14][59/233]	Loss 0.0995 (0.1510)	
training:	Epoch: [14][60/233]	Loss 0.1507 (0.1510)	
training:	Epoch: [14][61/233]	Loss 0.1304 (0.1507)	
training:	Epoch: [14][62/233]	Loss 0.1273 (0.1503)	
training:	Epoch: [14][63/233]	Loss 0.1109 (0.1497)	
training:	Epoch: [14][64/233]	Loss 0.1449 (0.1496)	
training:	Epoch: [14][65/233]	Loss 0.1023 (0.1489)	
training:	Epoch: [14][66/233]	Loss 0.1157 (0.1484)	
training:	Epoch: [14][67/233]	Loss 0.1005 (0.1477)	
training:	Epoch: [14][68/233]	Loss 0.1553 (0.1478)	
training:	Epoch: [14][69/233]	Loss 0.1949 (0.1485)	
training:	Epoch: [14][70/233]	Loss 0.1481 (0.1484)	
training:	Epoch: [14][71/233]	Loss 0.1548 (0.1485)	
training:	Epoch: [14][72/233]	Loss 0.1363 (0.1484)	
training:	Epoch: [14][73/233]	Loss 0.1536 (0.1484)	
training:	Epoch: [14][74/233]	Loss 0.1570 (0.1486)	
training:	Epoch: [14][75/233]	Loss 0.1608 (0.1487)	
training:	Epoch: [14][76/233]	Loss 0.1322 (0.1485)	
training:	Epoch: [14][77/233]	Loss 0.1078 (0.1480)	
training:	Epoch: [14][78/233]	Loss 0.1334 (0.1478)	
training:	Epoch: [14][79/233]	Loss 0.1393 (0.1477)	
training:	Epoch: [14][80/233]	Loss 0.1302 (0.1475)	
training:	Epoch: [14][81/233]	Loss 0.1513 (0.1475)	
training:	Epoch: [14][82/233]	Loss 0.1238 (0.1472)	
training:	Epoch: [14][83/233]	Loss 0.1385 (0.1471)	
training:	Epoch: [14][84/233]	Loss 0.1804 (0.1475)	
training:	Epoch: [14][85/233]	Loss 0.2013 (0.1481)	
training:	Epoch: [14][86/233]	Loss 0.1299 (0.1479)	
training:	Epoch: [14][87/233]	Loss 0.1685 (0.1482)	
training:	Epoch: [14][88/233]	Loss 0.1586 (0.1483)	
training:	Epoch: [14][89/233]	Loss 0.1274 (0.1481)	
training:	Epoch: [14][90/233]	Loss 0.1846 (0.1485)	
training:	Epoch: [14][91/233]	Loss 0.1316 (0.1483)	
training:	Epoch: [14][92/233]	Loss 0.1456 (0.1482)	
training:	Epoch: [14][93/233]	Loss 0.1824 (0.1486)	
training:	Epoch: [14][94/233]	Loss 0.1224 (0.1483)	
training:	Epoch: [14][95/233]	Loss 0.1387 (0.1482)	
training:	Epoch: [14][96/233]	Loss 0.1420 (0.1482)	
training:	Epoch: [14][97/233]	Loss 0.1967 (0.1487)	
training:	Epoch: [14][98/233]	Loss 0.1819 (0.1490)	
training:	Epoch: [14][99/233]	Loss 0.1452 (0.1490)	
training:	Epoch: [14][100/233]	Loss 0.1348 (0.1488)	
training:	Epoch: [14][101/233]	Loss 0.1053 (0.1484)	
training:	Epoch: [14][102/233]	Loss 0.1571 (0.1485)	
training:	Epoch: [14][103/233]	Loss 0.1485 (0.1485)	
training:	Epoch: [14][104/233]	Loss 0.1526 (0.1485)	
training:	Epoch: [14][105/233]	Loss 0.1595 (0.1486)	
training:	Epoch: [14][106/233]	Loss 0.1498 (0.1486)	
training:	Epoch: [14][107/233]	Loss 0.1237 (0.1484)	
training:	Epoch: [14][108/233]	Loss 0.1315 (0.1482)	
training:	Epoch: [14][109/233]	Loss 0.1078 (0.1479)	
training:	Epoch: [14][110/233]	Loss 0.1217 (0.1476)	
training:	Epoch: [14][111/233]	Loss 0.1082 (0.1473)	
training:	Epoch: [14][112/233]	Loss 0.1477 (0.1473)	
training:	Epoch: [14][113/233]	Loss 0.1263 (0.1471)	
training:	Epoch: [14][114/233]	Loss 0.1728 (0.1473)	
training:	Epoch: [14][115/233]	Loss 0.1146 (0.1470)	
training:	Epoch: [14][116/233]	Loss 0.2064 (0.1476)	
training:	Epoch: [14][117/233]	Loss 0.1173 (0.1473)	
training:	Epoch: [14][118/233]	Loss 0.1418 (0.1472)	
training:	Epoch: [14][119/233]	Loss 0.1175 (0.1470)	
training:	Epoch: [14][120/233]	Loss 0.1850 (0.1473)	
training:	Epoch: [14][121/233]	Loss 0.1764 (0.1476)	
training:	Epoch: [14][122/233]	Loss 0.1386 (0.1475)	
training:	Epoch: [14][123/233]	Loss 0.1425 (0.1474)	
training:	Epoch: [14][124/233]	Loss 0.1451 (0.1474)	
training:	Epoch: [14][125/233]	Loss 0.1441 (0.1474)	
training:	Epoch: [14][126/233]	Loss 0.1475 (0.1474)	
training:	Epoch: [14][127/233]	Loss 0.2076 (0.1479)	
training:	Epoch: [14][128/233]	Loss 0.1417 (0.1478)	
training:	Epoch: [14][129/233]	Loss 0.1297 (0.1477)	
training:	Epoch: [14][130/233]	Loss 0.1320 (0.1476)	
training:	Epoch: [14][131/233]	Loss 0.1494 (0.1476)	
training:	Epoch: [14][132/233]	Loss 0.1247 (0.1474)	
training:	Epoch: [14][133/233]	Loss 0.1260 (0.1472)	
training:	Epoch: [14][134/233]	Loss 0.1567 (0.1473)	
training:	Epoch: [14][135/233]	Loss 0.1665 (0.1475)	
training:	Epoch: [14][136/233]	Loss 0.1179 (0.1472)	
training:	Epoch: [14][137/233]	Loss 0.1643 (0.1474)	
training:	Epoch: [14][138/233]	Loss 0.1645 (0.1475)	
training:	Epoch: [14][139/233]	Loss 0.2363 (0.1481)	
training:	Epoch: [14][140/233]	Loss 0.1510 (0.1481)	
training:	Epoch: [14][141/233]	Loss 0.1598 (0.1482)	
training:	Epoch: [14][142/233]	Loss 0.1183 (0.1480)	
training:	Epoch: [14][143/233]	Loss 0.1721 (0.1482)	
training:	Epoch: [14][144/233]	Loss 0.1456 (0.1482)	
training:	Epoch: [14][145/233]	Loss 0.1819 (0.1484)	
training:	Epoch: [14][146/233]	Loss 0.1147 (0.1482)	
training:	Epoch: [14][147/233]	Loss 0.1563 (0.1482)	
training:	Epoch: [14][148/233]	Loss 0.1243 (0.1481)	
training:	Epoch: [14][149/233]	Loss 0.1284 (0.1479)	
training:	Epoch: [14][150/233]	Loss 0.1465 (0.1479)	
training:	Epoch: [14][151/233]	Loss 0.1315 (0.1478)	
training:	Epoch: [14][152/233]	Loss 0.1330 (0.1477)	
training:	Epoch: [14][153/233]	Loss 0.1054 (0.1474)	
training:	Epoch: [14][154/233]	Loss 0.1375 (0.1474)	
training:	Epoch: [14][155/233]	Loss 0.1333 (0.1473)	
training:	Epoch: [14][156/233]	Loss 0.1649 (0.1474)	
training:	Epoch: [14][157/233]	Loss 0.1199 (0.1472)	
training:	Epoch: [14][158/233]	Loss 0.1364 (0.1471)	
training:	Epoch: [14][159/233]	Loss 0.1115 (0.1469)	
training:	Epoch: [14][160/233]	Loss 0.1224 (0.1468)	
training:	Epoch: [14][161/233]	Loss 0.1058 (0.1465)	
training:	Epoch: [14][162/233]	Loss 0.1196 (0.1464)	
training:	Epoch: [14][163/233]	Loss 0.1158 (0.1462)	
training:	Epoch: [14][164/233]	Loss 0.1306 (0.1461)	
training:	Epoch: [14][165/233]	Loss 0.1094 (0.1458)	
training:	Epoch: [14][166/233]	Loss 0.1786 (0.1460)	
training:	Epoch: [14][167/233]	Loss 0.0902 (0.1457)	
training:	Epoch: [14][168/233]	Loss 0.1233 (0.1456)	
training:	Epoch: [14][169/233]	Loss 0.1361 (0.1455)	
training:	Epoch: [14][170/233]	Loss 0.1024 (0.1453)	
training:	Epoch: [14][171/233]	Loss 0.1384 (0.1452)	
training:	Epoch: [14][172/233]	Loss 0.1399 (0.1452)	
training:	Epoch: [14][173/233]	Loss 0.1182 (0.1450)	
training:	Epoch: [14][174/233]	Loss 0.1469 (0.1450)	
training:	Epoch: [14][175/233]	Loss 0.1450 (0.1450)	
training:	Epoch: [14][176/233]	Loss 0.1670 (0.1452)	
training:	Epoch: [14][177/233]	Loss 0.1059 (0.1450)	
training:	Epoch: [14][178/233]	Loss 0.1899 (0.1452)	
training:	Epoch: [14][179/233]	Loss 0.1956 (0.1455)	
training:	Epoch: [14][180/233]	Loss 0.1027 (0.1452)	
training:	Epoch: [14][181/233]	Loss 0.1057 (0.1450)	
training:	Epoch: [14][182/233]	Loss 0.1529 (0.1451)	
training:	Epoch: [14][183/233]	Loss 0.1662 (0.1452)	
training:	Epoch: [14][184/233]	Loss 0.1349 (0.1451)	
training:	Epoch: [14][185/233]	Loss 0.1709 (0.1453)	
training:	Epoch: [14][186/233]	Loss 0.0894 (0.1450)	
training:	Epoch: [14][187/233]	Loss 0.1531 (0.1450)	
training:	Epoch: [14][188/233]	Loss 0.1214 (0.1449)	
training:	Epoch: [14][189/233]	Loss 0.3124 (0.1458)	
training:	Epoch: [14][190/233]	Loss 0.2128 (0.1461)	
training:	Epoch: [14][191/233]	Loss 0.1284 (0.1460)	
training:	Epoch: [14][192/233]	Loss 0.1371 (0.1460)	
training:	Epoch: [14][193/233]	Loss 0.1475 (0.1460)	
training:	Epoch: [14][194/233]	Loss 0.1681 (0.1461)	
training:	Epoch: [14][195/233]	Loss 0.1167 (0.1460)	
training:	Epoch: [14][196/233]	Loss 0.2042 (0.1463)	
training:	Epoch: [14][197/233]	Loss 0.1303 (0.1462)	
training:	Epoch: [14][198/233]	Loss 0.1683 (0.1463)	
training:	Epoch: [14][199/233]	Loss 0.1489 (0.1463)	
training:	Epoch: [14][200/233]	Loss 0.1336 (0.1462)	
training:	Epoch: [14][201/233]	Loss 0.1131 (0.1461)	
training:	Epoch: [14][202/233]	Loss 0.1280 (0.1460)	
training:	Epoch: [14][203/233]	Loss 0.1190 (0.1459)	
training:	Epoch: [14][204/233]	Loss 0.1312 (0.1458)	
training:	Epoch: [14][205/233]	Loss 0.1570 (0.1458)	
training:	Epoch: [14][206/233]	Loss 0.1114 (0.1457)	
training:	Epoch: [14][207/233]	Loss 0.1544 (0.1457)	
training:	Epoch: [14][208/233]	Loss 0.1546 (0.1458)	
training:	Epoch: [14][209/233]	Loss 0.1262 (0.1457)	
training:	Epoch: [14][210/233]	Loss 0.1522 (0.1457)	
training:	Epoch: [14][211/233]	Loss 0.2262 (0.1461)	
training:	Epoch: [14][212/233]	Loss 0.1429 (0.1461)	
training:	Epoch: [14][213/233]	Loss 0.1445 (0.1460)	
training:	Epoch: [14][214/233]	Loss 0.0943 (0.1458)	
training:	Epoch: [14][215/233]	Loss 0.1339 (0.1458)	
training:	Epoch: [14][216/233]	Loss 0.1486 (0.1458)	
training:	Epoch: [14][217/233]	Loss 0.1340 (0.1457)	
training:	Epoch: [14][218/233]	Loss 0.1534 (0.1457)	
training:	Epoch: [14][219/233]	Loss 0.1244 (0.1456)	
training:	Epoch: [14][220/233]	Loss 0.1810 (0.1458)	
training:	Epoch: [14][221/233]	Loss 0.1127 (0.1457)	
training:	Epoch: [14][222/233]	Loss 0.1885 (0.1459)	
training:	Epoch: [14][223/233]	Loss 0.1866 (0.1460)	
training:	Epoch: [14][224/233]	Loss 0.1347 (0.1460)	
training:	Epoch: [14][225/233]	Loss 0.1220 (0.1459)	
training:	Epoch: [14][226/233]	Loss 0.1348 (0.1458)	
training:	Epoch: [14][227/233]	Loss 0.1201 (0.1457)	
training:	Epoch: [14][228/233]	Loss 0.1278 (0.1456)	
training:	Epoch: [14][229/233]	Loss 0.1706 (0.1457)	
training:	Epoch: [14][230/233]	Loss 0.1557 (0.1458)	
training:	Epoch: [14][231/233]	Loss 0.1488 (0.1458)	
training:	Epoch: [14][232/233]	Loss 0.1632 (0.1459)	
training:	Epoch: [14][233/233]	Loss 0.1197 (0.1458)	
Training:	 Loss: 0.1454

Training:	 ACC: 0.9980 0.9980 0.9974 0.9986
Validation:	 ACC: 0.8630 0.8636 0.8762 0.8498
Validation:	 Best_BACC: 0.8630 0.8636 0.8762 0.8498
Validation:	 Loss: 0.3385
Pretraining:	Epoch 15/200
----------
training:	Epoch: [15][1/233]	Loss 0.1434 (0.1434)	
training:	Epoch: [15][2/233]	Loss 0.1129 (0.1282)	
training:	Epoch: [15][3/233]	Loss 0.1300 (0.1288)	
training:	Epoch: [15][4/233]	Loss 0.1744 (0.1402)	
training:	Epoch: [15][5/233]	Loss 0.1371 (0.1396)	
training:	Epoch: [15][6/233]	Loss 0.1398 (0.1396)	
training:	Epoch: [15][7/233]	Loss 0.1154 (0.1362)	
training:	Epoch: [15][8/233]	Loss 0.1232 (0.1345)	
training:	Epoch: [15][9/233]	Loss 0.1679 (0.1383)	
training:	Epoch: [15][10/233]	Loss 0.1212 (0.1365)	
training:	Epoch: [15][11/233]	Loss 0.1286 (0.1358)	
training:	Epoch: [15][12/233]	Loss 0.1548 (0.1374)	
training:	Epoch: [15][13/233]	Loss 0.1784 (0.1406)	
training:	Epoch: [15][14/233]	Loss 0.2099 (0.1455)	
training:	Epoch: [15][15/233]	Loss 0.1047 (0.1428)	
training:	Epoch: [15][16/233]	Loss 0.1595 (0.1438)	
training:	Epoch: [15][17/233]	Loss 0.1338 (0.1432)	
training:	Epoch: [15][18/233]	Loss 0.1350 (0.1428)	
training:	Epoch: [15][19/233]	Loss 0.1093 (0.1410)	
training:	Epoch: [15][20/233]	Loss 0.1191 (0.1399)	
training:	Epoch: [15][21/233]	Loss 0.1190 (0.1389)	
training:	Epoch: [15][22/233]	Loss 0.0937 (0.1369)	
training:	Epoch: [15][23/233]	Loss 0.1843 (0.1389)	
training:	Epoch: [15][24/233]	Loss 0.1209 (0.1382)	
training:	Epoch: [15][25/233]	Loss 0.0978 (0.1366)	
training:	Epoch: [15][26/233]	Loss 0.1311 (0.1364)	
training:	Epoch: [15][27/233]	Loss 0.1463 (0.1367)	
training:	Epoch: [15][28/233]	Loss 0.1419 (0.1369)	
training:	Epoch: [15][29/233]	Loss 0.1208 (0.1363)	
training:	Epoch: [15][30/233]	Loss 0.1305 (0.1362)	
training:	Epoch: [15][31/233]	Loss 0.1425 (0.1364)	
training:	Epoch: [15][32/233]	Loss 0.1418 (0.1365)	
training:	Epoch: [15][33/233]	Loss 0.1334 (0.1364)	
training:	Epoch: [15][34/233]	Loss 0.1082 (0.1356)	
training:	Epoch: [15][35/233]	Loss 0.1159 (0.1350)	
training:	Epoch: [15][36/233]	Loss 0.1816 (0.1363)	
training:	Epoch: [15][37/233]	Loss 0.0894 (0.1351)	
training:	Epoch: [15][38/233]	Loss 0.1414 (0.1352)	
training:	Epoch: [15][39/233]	Loss 0.1027 (0.1344)	
training:	Epoch: [15][40/233]	Loss 0.1024 (0.1336)	
training:	Epoch: [15][41/233]	Loss 0.1329 (0.1336)	
training:	Epoch: [15][42/233]	Loss 0.1263 (0.1334)	
training:	Epoch: [15][43/233]	Loss 0.1229 (0.1332)	
training:	Epoch: [15][44/233]	Loss 0.1169 (0.1328)	
training:	Epoch: [15][45/233]	Loss 0.1290 (0.1327)	
training:	Epoch: [15][46/233]	Loss 0.1055 (0.1321)	
training:	Epoch: [15][47/233]	Loss 0.1536 (0.1326)	
training:	Epoch: [15][48/233]	Loss 0.1269 (0.1325)	
training:	Epoch: [15][49/233]	Loss 0.1259 (0.1323)	
training:	Epoch: [15][50/233]	Loss 0.1146 (0.1320)	
training:	Epoch: [15][51/233]	Loss 0.1220 (0.1318)	
training:	Epoch: [15][52/233]	Loss 0.0956 (0.1311)	
training:	Epoch: [15][53/233]	Loss 0.0998 (0.1305)	
training:	Epoch: [15][54/233]	Loss 0.1061 (0.1300)	
training:	Epoch: [15][55/233]	Loss 0.1251 (0.1299)	
training:	Epoch: [15][56/233]	Loss 0.1609 (0.1305)	
training:	Epoch: [15][57/233]	Loss 0.1039 (0.1300)	
training:	Epoch: [15][58/233]	Loss 0.1146 (0.1298)	
training:	Epoch: [15][59/233]	Loss 0.1275 (0.1297)	
training:	Epoch: [15][60/233]	Loss 0.1289 (0.1297)	
training:	Epoch: [15][61/233]	Loss 0.0971 (0.1292)	
training:	Epoch: [15][62/233]	Loss 0.1508 (0.1295)	
training:	Epoch: [15][63/233]	Loss 0.1226 (0.1294)	
training:	Epoch: [15][64/233]	Loss 0.1066 (0.1291)	
training:	Epoch: [15][65/233]	Loss 0.1220 (0.1290)	
training:	Epoch: [15][66/233]	Loss 0.1356 (0.1291)	
training:	Epoch: [15][67/233]	Loss 0.1537 (0.1294)	
training:	Epoch: [15][68/233]	Loss 0.1093 (0.1291)	
training:	Epoch: [15][69/233]	Loss 0.1481 (0.1294)	
training:	Epoch: [15][70/233]	Loss 0.1230 (0.1293)	
training:	Epoch: [15][71/233]	Loss 0.1627 (0.1298)	
training:	Epoch: [15][72/233]	Loss 0.1384 (0.1299)	
training:	Epoch: [15][73/233]	Loss 0.1531 (0.1302)	
training:	Epoch: [15][74/233]	Loss 0.1363 (0.1303)	
training:	Epoch: [15][75/233]	Loss 0.1524 (0.1306)	
training:	Epoch: [15][76/233]	Loss 0.1216 (0.1305)	
training:	Epoch: [15][77/233]	Loss 0.1409 (0.1306)	
training:	Epoch: [15][78/233]	Loss 0.1340 (0.1307)	
training:	Epoch: [15][79/233]	Loss 0.1603 (0.1310)	
training:	Epoch: [15][80/233]	Loss 0.1308 (0.1310)	
training:	Epoch: [15][81/233]	Loss 0.1335 (0.1311)	
training:	Epoch: [15][82/233]	Loss 0.1149 (0.1309)	
training:	Epoch: [15][83/233]	Loss 0.1208 (0.1307)	
training:	Epoch: [15][84/233]	Loss 0.1436 (0.1309)	
training:	Epoch: [15][85/233]	Loss 0.1227 (0.1308)	
training:	Epoch: [15][86/233]	Loss 0.0917 (0.1303)	
training:	Epoch: [15][87/233]	Loss 0.1095 (0.1301)	
training:	Epoch: [15][88/233]	Loss 0.1276 (0.1301)	
training:	Epoch: [15][89/233]	Loss 0.1402 (0.1302)	
training:	Epoch: [15][90/233]	Loss 0.1238 (0.1301)	
training:	Epoch: [15][91/233]	Loss 0.1626 (0.1305)	
training:	Epoch: [15][92/233]	Loss 0.1306 (0.1305)	
training:	Epoch: [15][93/233]	Loss 0.1341 (0.1305)	
training:	Epoch: [15][94/233]	Loss 0.1838 (0.1311)	
training:	Epoch: [15][95/233]	Loss 0.1530 (0.1313)	
training:	Epoch: [15][96/233]	Loss 0.1327 (0.1313)	
training:	Epoch: [15][97/233]	Loss 0.1594 (0.1316)	
training:	Epoch: [15][98/233]	Loss 0.1440 (0.1317)	
training:	Epoch: [15][99/233]	Loss 0.1321 (0.1317)	
training:	Epoch: [15][100/233]	Loss 0.1612 (0.1320)	
training:	Epoch: [15][101/233]	Loss 0.1097 (0.1318)	
training:	Epoch: [15][102/233]	Loss 0.1448 (0.1319)	
training:	Epoch: [15][103/233]	Loss 0.1322 (0.1319)	
training:	Epoch: [15][104/233]	Loss 0.1433 (0.1321)	
training:	Epoch: [15][105/233]	Loss 0.1334 (0.1321)	
training:	Epoch: [15][106/233]	Loss 0.1197 (0.1320)	
training:	Epoch: [15][107/233]	Loss 0.1211 (0.1319)	
training:	Epoch: [15][108/233]	Loss 0.1305 (0.1318)	
training:	Epoch: [15][109/233]	Loss 0.1018 (0.1316)	
training:	Epoch: [15][110/233]	Loss 0.1386 (0.1316)	
training:	Epoch: [15][111/233]	Loss 0.1082 (0.1314)	
training:	Epoch: [15][112/233]	Loss 0.1732 (0.1318)	
training:	Epoch: [15][113/233]	Loss 0.1381 (0.1318)	
training:	Epoch: [15][114/233]	Loss 0.1351 (0.1319)	
training:	Epoch: [15][115/233]	Loss 0.1296 (0.1319)	
training:	Epoch: [15][116/233]	Loss 0.1752 (0.1322)	
training:	Epoch: [15][117/233]	Loss 0.1261 (0.1322)	
training:	Epoch: [15][118/233]	Loss 0.1349 (0.1322)	
training:	Epoch: [15][119/233]	Loss 0.1169 (0.1321)	
training:	Epoch: [15][120/233]	Loss 0.1012 (0.1318)	
training:	Epoch: [15][121/233]	Loss 0.1585 (0.1320)	
training:	Epoch: [15][122/233]	Loss 0.1052 (0.1318)	
training:	Epoch: [15][123/233]	Loss 0.1173 (0.1317)	
training:	Epoch: [15][124/233]	Loss 0.1034 (0.1315)	
training:	Epoch: [15][125/233]	Loss 0.1454 (0.1316)	
training:	Epoch: [15][126/233]	Loss 0.1172 (0.1315)	
training:	Epoch: [15][127/233]	Loss 0.1010 (0.1312)	
training:	Epoch: [15][128/233]	Loss 0.1450 (0.1313)	
training:	Epoch: [15][129/233]	Loss 0.1566 (0.1315)	
training:	Epoch: [15][130/233]	Loss 0.1428 (0.1316)	
training:	Epoch: [15][131/233]	Loss 0.2022 (0.1322)	
training:	Epoch: [15][132/233]	Loss 0.1072 (0.1320)	
training:	Epoch: [15][133/233]	Loss 0.1310 (0.1320)	
training:	Epoch: [15][134/233]	Loss 0.1296 (0.1319)	
training:	Epoch: [15][135/233]	Loss 0.1078 (0.1318)	
training:	Epoch: [15][136/233]	Loss 0.1208 (0.1317)	
training:	Epoch: [15][137/233]	Loss 0.1373 (0.1317)	
training:	Epoch: [15][138/233]	Loss 0.1134 (0.1316)	
training:	Epoch: [15][139/233]	Loss 0.1023 (0.1314)	
training:	Epoch: [15][140/233]	Loss 0.1056 (0.1312)	
training:	Epoch: [15][141/233]	Loss 0.1546 (0.1314)	
training:	Epoch: [15][142/233]	Loss 0.0953 (0.1311)	
training:	Epoch: [15][143/233]	Loss 0.1075 (0.1309)	
training:	Epoch: [15][144/233]	Loss 0.1007 (0.1307)	
training:	Epoch: [15][145/233]	Loss 0.1318 (0.1307)	
training:	Epoch: [15][146/233]	Loss 0.1152 (0.1306)	
training:	Epoch: [15][147/233]	Loss 0.1497 (0.1308)	
training:	Epoch: [15][148/233]	Loss 0.1091 (0.1306)	
training:	Epoch: [15][149/233]	Loss 0.1524 (0.1308)	
training:	Epoch: [15][150/233]	Loss 0.0894 (0.1305)	
training:	Epoch: [15][151/233]	Loss 0.1556 (0.1307)	
training:	Epoch: [15][152/233]	Loss 0.1322 (0.1307)	
training:	Epoch: [15][153/233]	Loss 0.1260 (0.1306)	
training:	Epoch: [15][154/233]	Loss 0.1155 (0.1305)	
training:	Epoch: [15][155/233]	Loss 0.1346 (0.1306)	
training:	Epoch: [15][156/233]	Loss 0.1631 (0.1308)	
training:	Epoch: [15][157/233]	Loss 0.1893 (0.1311)	
training:	Epoch: [15][158/233]	Loss 0.2181 (0.1317)	
training:	Epoch: [15][159/233]	Loss 0.1212 (0.1316)	
training:	Epoch: [15][160/233]	Loss 0.1454 (0.1317)	
training:	Epoch: [15][161/233]	Loss 0.1739 (0.1320)	
training:	Epoch: [15][162/233]	Loss 0.1445 (0.1321)	
training:	Epoch: [15][163/233]	Loss 0.1527 (0.1322)	
training:	Epoch: [15][164/233]	Loss 0.0993 (0.1320)	
training:	Epoch: [15][165/233]	Loss 0.0848 (0.1317)	
training:	Epoch: [15][166/233]	Loss 0.0953 (0.1315)	
training:	Epoch: [15][167/233]	Loss 0.1072 (0.1313)	
training:	Epoch: [15][168/233]	Loss 0.1312 (0.1313)	
training:	Epoch: [15][169/233]	Loss 0.1452 (0.1314)	
training:	Epoch: [15][170/233]	Loss 0.1701 (0.1316)	
training:	Epoch: [15][171/233]	Loss 0.1113 (0.1315)	
training:	Epoch: [15][172/233]	Loss 0.1979 (0.1319)	
training:	Epoch: [15][173/233]	Loss 0.1204 (0.1318)	
training:	Epoch: [15][174/233]	Loss 0.1170 (0.1318)	
training:	Epoch: [15][175/233]	Loss 0.1491 (0.1318)	
training:	Epoch: [15][176/233]	Loss 0.1335 (0.1319)	
training:	Epoch: [15][177/233]	Loss 0.1359 (0.1319)	
training:	Epoch: [15][178/233]	Loss 0.1214 (0.1318)	
training:	Epoch: [15][179/233]	Loss 0.1444 (0.1319)	
training:	Epoch: [15][180/233]	Loss 0.1759 (0.1321)	
training:	Epoch: [15][181/233]	Loss 0.1220 (0.1321)	
training:	Epoch: [15][182/233]	Loss 0.1973 (0.1324)	
training:	Epoch: [15][183/233]	Loss 0.1573 (0.1326)	
training:	Epoch: [15][184/233]	Loss 0.1820 (0.1328)	
training:	Epoch: [15][185/233]	Loss 0.0983 (0.1327)	
training:	Epoch: [15][186/233]	Loss 0.1373 (0.1327)	
training:	Epoch: [15][187/233]	Loss 0.1456 (0.1328)	
training:	Epoch: [15][188/233]	Loss 0.1077 (0.1326)	
training:	Epoch: [15][189/233]	Loss 0.1221 (0.1326)	
training:	Epoch: [15][190/233]	Loss 0.0976 (0.1324)	
training:	Epoch: [15][191/233]	Loss 0.1089 (0.1323)	
training:	Epoch: [15][192/233]	Loss 0.1271 (0.1322)	
training:	Epoch: [15][193/233]	Loss 0.1357 (0.1322)	
training:	Epoch: [15][194/233]	Loss 0.1173 (0.1322)	
training:	Epoch: [15][195/233]	Loss 0.1689 (0.1324)	
training:	Epoch: [15][196/233]	Loss 0.1137 (0.1323)	
training:	Epoch: [15][197/233]	Loss 0.1062 (0.1321)	
training:	Epoch: [15][198/233]	Loss 0.1920 (0.1324)	
training:	Epoch: [15][199/233]	Loss 0.1468 (0.1325)	
training:	Epoch: [15][200/233]	Loss 0.1284 (0.1325)	
training:	Epoch: [15][201/233]	Loss 0.1909 (0.1328)	
training:	Epoch: [15][202/233]	Loss 0.1578 (0.1329)	
training:	Epoch: [15][203/233]	Loss 0.1602 (0.1330)	
training:	Epoch: [15][204/233]	Loss 0.1193 (0.1330)	
training:	Epoch: [15][205/233]	Loss 0.1791 (0.1332)	
training:	Epoch: [15][206/233]	Loss 0.1474 (0.1333)	
training:	Epoch: [15][207/233]	Loss 0.1168 (0.1332)	
training:	Epoch: [15][208/233]	Loss 0.1045 (0.1330)	
training:	Epoch: [15][209/233]	Loss 0.1879 (0.1333)	
training:	Epoch: [15][210/233]	Loss 0.1130 (0.1332)	
training:	Epoch: [15][211/233]	Loss 0.1261 (0.1332)	
training:	Epoch: [15][212/233]	Loss 0.1320 (0.1332)	
training:	Epoch: [15][213/233]	Loss 0.0944 (0.1330)	
training:	Epoch: [15][214/233]	Loss 0.1601 (0.1331)	
training:	Epoch: [15][215/233]	Loss 0.1146 (0.1330)	
training:	Epoch: [15][216/233]	Loss 0.1324 (0.1330)	
training:	Epoch: [15][217/233]	Loss 0.1471 (0.1331)	
training:	Epoch: [15][218/233]	Loss 0.1186 (0.1330)	
training:	Epoch: [15][219/233]	Loss 0.1149 (0.1329)	
training:	Epoch: [15][220/233]	Loss 0.1405 (0.1330)	
training:	Epoch: [15][221/233]	Loss 0.1722 (0.1332)	
training:	Epoch: [15][222/233]	Loss 0.1148 (0.1331)	
training:	Epoch: [15][223/233]	Loss 0.1352 (0.1331)	
training:	Epoch: [15][224/233]	Loss 0.2032 (0.1334)	
training:	Epoch: [15][225/233]	Loss 0.1325 (0.1334)	
training:	Epoch: [15][226/233]	Loss 0.1387 (0.1334)	
training:	Epoch: [15][227/233]	Loss 0.1499 (0.1335)	
training:	Epoch: [15][228/233]	Loss 0.1234 (0.1334)	
training:	Epoch: [15][229/233]	Loss 0.2345 (0.1339)	
training:	Epoch: [15][230/233]	Loss 0.1684 (0.1340)	
training:	Epoch: [15][231/233]	Loss 0.1126 (0.1339)	
training:	Epoch: [15][232/233]	Loss 0.1294 (0.1339)	
training:	Epoch: [15][233/233]	Loss 0.1695 (0.1341)	
Training:	 Loss: 0.1338

Training:	 ACC: 0.9988 0.9988 0.9979 0.9997
Validation:	 ACC: 0.8596 0.8593 0.8536 0.8655
Validation:	 Best_BACC: 0.8630 0.8636 0.8762 0.8498
Validation:	 Loss: 0.3391
Pretraining:	Epoch 16/200
----------
training:	Epoch: [16][1/233]	Loss 0.1319 (0.1319)	
training:	Epoch: [16][2/233]	Loss 0.1298 (0.1308)	
training:	Epoch: [16][3/233]	Loss 0.1072 (0.1230)	
training:	Epoch: [16][4/233]	Loss 0.1176 (0.1216)	
training:	Epoch: [16][5/233]	Loss 0.1566 (0.1286)	
training:	Epoch: [16][6/233]	Loss 0.0839 (0.1212)	
training:	Epoch: [16][7/233]	Loss 0.1093 (0.1195)	
training:	Epoch: [16][8/233]	Loss 0.1377 (0.1217)	
training:	Epoch: [16][9/233]	Loss 0.1252 (0.1221)	
training:	Epoch: [16][10/233]	Loss 0.1361 (0.1235)	
training:	Epoch: [16][11/233]	Loss 0.1380 (0.1248)	
training:	Epoch: [16][12/233]	Loss 0.1288 (0.1252)	
training:	Epoch: [16][13/233]	Loss 0.1416 (0.1264)	
training:	Epoch: [16][14/233]	Loss 0.1215 (0.1261)	
training:	Epoch: [16][15/233]	Loss 0.1147 (0.1253)	
training:	Epoch: [16][16/233]	Loss 0.1551 (0.1272)	
training:	Epoch: [16][17/233]	Loss 0.2353 (0.1335)	
training:	Epoch: [16][18/233]	Loss 0.1231 (0.1330)	
training:	Epoch: [16][19/233]	Loss 0.1221 (0.1324)	
training:	Epoch: [16][20/233]	Loss 0.1471 (0.1331)	
training:	Epoch: [16][21/233]	Loss 0.1251 (0.1327)	
training:	Epoch: [16][22/233]	Loss 0.1033 (0.1314)	
training:	Epoch: [16][23/233]	Loss 0.1421 (0.1319)	
training:	Epoch: [16][24/233]	Loss 0.1611 (0.1331)	
training:	Epoch: [16][25/233]	Loss 0.1580 (0.1341)	
training:	Epoch: [16][26/233]	Loss 0.2015 (0.1367)	
training:	Epoch: [16][27/233]	Loss 0.1135 (0.1358)	
training:	Epoch: [16][28/233]	Loss 0.1184 (0.1352)	
training:	Epoch: [16][29/233]	Loss 0.0894 (0.1336)	
training:	Epoch: [16][30/233]	Loss 0.1143 (0.1330)	
training:	Epoch: [16][31/233]	Loss 0.1107 (0.1323)	
training:	Epoch: [16][32/233]	Loss 0.0888 (0.1309)	
training:	Epoch: [16][33/233]	Loss 0.1037 (0.1301)	
training:	Epoch: [16][34/233]	Loss 0.1336 (0.1302)	
training:	Epoch: [16][35/233]	Loss 0.1138 (0.1297)	
training:	Epoch: [16][36/233]	Loss 0.0997 (0.1289)	
training:	Epoch: [16][37/233]	Loss 0.1221 (0.1287)	
training:	Epoch: [16][38/233]	Loss 0.1151 (0.1283)	
training:	Epoch: [16][39/233]	Loss 0.1327 (0.1284)	
training:	Epoch: [16][40/233]	Loss 0.1331 (0.1286)	
training:	Epoch: [16][41/233]	Loss 0.1822 (0.1299)	
training:	Epoch: [16][42/233]	Loss 0.1329 (0.1299)	
training:	Epoch: [16][43/233]	Loss 0.1484 (0.1304)	
training:	Epoch: [16][44/233]	Loss 0.1078 (0.1299)	
training:	Epoch: [16][45/233]	Loss 0.1141 (0.1295)	
training:	Epoch: [16][46/233]	Loss 0.1082 (0.1290)	
training:	Epoch: [16][47/233]	Loss 0.0965 (0.1284)	
training:	Epoch: [16][48/233]	Loss 0.1218 (0.1282)	
training:	Epoch: [16][49/233]	Loss 0.1254 (0.1282)	
training:	Epoch: [16][50/233]	Loss 0.1028 (0.1277)	
training:	Epoch: [16][51/233]	Loss 0.1289 (0.1277)	
training:	Epoch: [16][52/233]	Loss 0.0991 (0.1271)	
training:	Epoch: [16][53/233]	Loss 0.1244 (0.1271)	
training:	Epoch: [16][54/233]	Loss 0.0906 (0.1264)	
training:	Epoch: [16][55/233]	Loss 0.1799 (0.1274)	
training:	Epoch: [16][56/233]	Loss 0.1340 (0.1275)	
training:	Epoch: [16][57/233]	Loss 0.1029 (0.1271)	
training:	Epoch: [16][58/233]	Loss 0.1296 (0.1271)	
training:	Epoch: [16][59/233]	Loss 0.1082 (0.1268)	
training:	Epoch: [16][60/233]	Loss 0.1069 (0.1265)	
training:	Epoch: [16][61/233]	Loss 0.1758 (0.1273)	
training:	Epoch: [16][62/233]	Loss 0.1170 (0.1271)	
training:	Epoch: [16][63/233]	Loss 0.1100 (0.1268)	
training:	Epoch: [16][64/233]	Loss 0.1363 (0.1270)	
training:	Epoch: [16][65/233]	Loss 0.1149 (0.1268)	
training:	Epoch: [16][66/233]	Loss 0.1008 (0.1264)	
training:	Epoch: [16][67/233]	Loss 0.1105 (0.1262)	
training:	Epoch: [16][68/233]	Loss 0.1127 (0.1260)	
training:	Epoch: [16][69/233]	Loss 0.1302 (0.1260)	
training:	Epoch: [16][70/233]	Loss 0.1329 (0.1261)	
training:	Epoch: [16][71/233]	Loss 0.1251 (0.1261)	
training:	Epoch: [16][72/233]	Loss 0.1078 (0.1258)	
training:	Epoch: [16][73/233]	Loss 0.1034 (0.1255)	
training:	Epoch: [16][74/233]	Loss 0.1343 (0.1257)	
training:	Epoch: [16][75/233]	Loss 0.1578 (0.1261)	
training:	Epoch: [16][76/233]	Loss 0.1413 (0.1263)	
training:	Epoch: [16][77/233]	Loss 0.1037 (0.1260)	
training:	Epoch: [16][78/233]	Loss 0.1034 (0.1257)	
training:	Epoch: [16][79/233]	Loss 0.1403 (0.1259)	
training:	Epoch: [16][80/233]	Loss 0.1135 (0.1257)	
training:	Epoch: [16][81/233]	Loss 0.1019 (0.1254)	
training:	Epoch: [16][82/233]	Loss 0.1044 (0.1252)	
training:	Epoch: [16][83/233]	Loss 0.1315 (0.1253)	
training:	Epoch: [16][84/233]	Loss 0.1036 (0.1250)	
training:	Epoch: [16][85/233]	Loss 0.1511 (0.1253)	
training:	Epoch: [16][86/233]	Loss 0.2175 (0.1264)	
training:	Epoch: [16][87/233]	Loss 0.1150 (0.1262)	
training:	Epoch: [16][88/233]	Loss 0.1909 (0.1270)	
training:	Epoch: [16][89/233]	Loss 0.1593 (0.1273)	
training:	Epoch: [16][90/233]	Loss 0.1416 (0.1275)	
training:	Epoch: [16][91/233]	Loss 0.1360 (0.1276)	
training:	Epoch: [16][92/233]	Loss 0.1529 (0.1279)	
training:	Epoch: [16][93/233]	Loss 0.1204 (0.1278)	
training:	Epoch: [16][94/233]	Loss 0.1384 (0.1279)	
training:	Epoch: [16][95/233]	Loss 0.1121 (0.1277)	
training:	Epoch: [16][96/233]	Loss 0.0946 (0.1274)	
training:	Epoch: [16][97/233]	Loss 0.0938 (0.1270)	
training:	Epoch: [16][98/233]	Loss 0.1007 (0.1268)	
training:	Epoch: [16][99/233]	Loss 0.1068 (0.1266)	
training:	Epoch: [16][100/233]	Loss 0.0992 (0.1263)	
training:	Epoch: [16][101/233]	Loss 0.1186 (0.1262)	
training:	Epoch: [16][102/233]	Loss 0.1016 (0.1260)	
training:	Epoch: [16][103/233]	Loss 0.1062 (0.1258)	
training:	Epoch: [16][104/233]	Loss 0.1312 (0.1258)	
training:	Epoch: [16][105/233]	Loss 0.1121 (0.1257)	
training:	Epoch: [16][106/233]	Loss 0.1269 (0.1257)	
training:	Epoch: [16][107/233]	Loss 0.1080 (0.1256)	
training:	Epoch: [16][108/233]	Loss 0.2643 (0.1268)	
training:	Epoch: [16][109/233]	Loss 0.0893 (0.1265)	
training:	Epoch: [16][110/233]	Loss 0.1061 (0.1263)	
training:	Epoch: [16][111/233]	Loss 0.1580 (0.1266)	
training:	Epoch: [16][112/233]	Loss 0.1206 (0.1265)	
training:	Epoch: [16][113/233]	Loss 0.1023 (0.1263)	
training:	Epoch: [16][114/233]	Loss 0.0860 (0.1260)	
training:	Epoch: [16][115/233]	Loss 0.1047 (0.1258)	
training:	Epoch: [16][116/233]	Loss 0.1534 (0.1260)	
training:	Epoch: [16][117/233]	Loss 0.1852 (0.1265)	
training:	Epoch: [16][118/233]	Loss 0.1228 (0.1265)	
training:	Epoch: [16][119/233]	Loss 0.2649 (0.1277)	
training:	Epoch: [16][120/233]	Loss 0.1034 (0.1275)	
training:	Epoch: [16][121/233]	Loss 0.1559 (0.1277)	
training:	Epoch: [16][122/233]	Loss 0.1250 (0.1277)	
training:	Epoch: [16][123/233]	Loss 0.1468 (0.1278)	
training:	Epoch: [16][124/233]	Loss 0.1907 (0.1283)	
training:	Epoch: [16][125/233]	Loss 0.1375 (0.1284)	
training:	Epoch: [16][126/233]	Loss 0.1023 (0.1282)	
training:	Epoch: [16][127/233]	Loss 0.1662 (0.1285)	
training:	Epoch: [16][128/233]	Loss 0.1126 (0.1284)	
training:	Epoch: [16][129/233]	Loss 0.1195 (0.1283)	
training:	Epoch: [16][130/233]	Loss 0.1167 (0.1282)	
training:	Epoch: [16][131/233]	Loss 0.1070 (0.1281)	
training:	Epoch: [16][132/233]	Loss 0.1229 (0.1280)	
training:	Epoch: [16][133/233]	Loss 0.1505 (0.1282)	
training:	Epoch: [16][134/233]	Loss 0.1339 (0.1282)	
training:	Epoch: [16][135/233]	Loss 0.0762 (0.1278)	
training:	Epoch: [16][136/233]	Loss 0.1313 (0.1279)	
training:	Epoch: [16][137/233]	Loss 0.1494 (0.1280)	
training:	Epoch: [16][138/233]	Loss 0.1087 (0.1279)	
training:	Epoch: [16][139/233]	Loss 0.1154 (0.1278)	
training:	Epoch: [16][140/233]	Loss 0.1246 (0.1278)	
training:	Epoch: [16][141/233]	Loss 0.0917 (0.1275)	
training:	Epoch: [16][142/233]	Loss 0.1276 (0.1275)	
training:	Epoch: [16][143/233]	Loss 0.1253 (0.1275)	
training:	Epoch: [16][144/233]	Loss 0.1124 (0.1274)	
training:	Epoch: [16][145/233]	Loss 0.1123 (0.1273)	
training:	Epoch: [16][146/233]	Loss 0.1087 (0.1272)	
training:	Epoch: [16][147/233]	Loss 0.1390 (0.1273)	
training:	Epoch: [16][148/233]	Loss 0.1104 (0.1271)	
training:	Epoch: [16][149/233]	Loss 0.1075 (0.1270)	
training:	Epoch: [16][150/233]	Loss 0.1212 (0.1270)	
training:	Epoch: [16][151/233]	Loss 0.1478 (0.1271)	
training:	Epoch: [16][152/233]	Loss 0.1131 (0.1270)	
training:	Epoch: [16][153/233]	Loss 0.1652 (0.1273)	
training:	Epoch: [16][154/233]	Loss 0.1101 (0.1272)	
training:	Epoch: [16][155/233]	Loss 0.1197 (0.1271)	
training:	Epoch: [16][156/233]	Loss 0.1131 (0.1270)	
training:	Epoch: [16][157/233]	Loss 0.1653 (0.1273)	
training:	Epoch: [16][158/233]	Loss 0.1349 (0.1273)	
training:	Epoch: [16][159/233]	Loss 0.1209 (0.1273)	
training:	Epoch: [16][160/233]	Loss 0.1025 (0.1271)	
training:	Epoch: [16][161/233]	Loss 0.1236 (0.1271)	
training:	Epoch: [16][162/233]	Loss 0.1028 (0.1269)	
training:	Epoch: [16][163/233]	Loss 0.1143 (0.1269)	
training:	Epoch: [16][164/233]	Loss 0.1130 (0.1268)	
training:	Epoch: [16][165/233]	Loss 0.0831 (0.1265)	
training:	Epoch: [16][166/233]	Loss 0.1203 (0.1265)	
training:	Epoch: [16][167/233]	Loss 0.1241 (0.1265)	
training:	Epoch: [16][168/233]	Loss 0.1132 (0.1264)	
training:	Epoch: [16][169/233]	Loss 0.1181 (0.1263)	
training:	Epoch: [16][170/233]	Loss 0.1295 (0.1264)	
training:	Epoch: [16][171/233]	Loss 0.1040 (0.1262)	
training:	Epoch: [16][172/233]	Loss 0.1201 (0.1262)	
training:	Epoch: [16][173/233]	Loss 0.1633 (0.1264)	
training:	Epoch: [16][174/233]	Loss 0.1401 (0.1265)	
training:	Epoch: [16][175/233]	Loss 0.1365 (0.1265)	
training:	Epoch: [16][176/233]	Loss 0.1217 (0.1265)	
training:	Epoch: [16][177/233]	Loss 0.1194 (0.1265)	
training:	Epoch: [16][178/233]	Loss 0.1242 (0.1265)	
training:	Epoch: [16][179/233]	Loss 0.1201 (0.1264)	
training:	Epoch: [16][180/233]	Loss 0.1311 (0.1264)	
training:	Epoch: [16][181/233]	Loss 0.1562 (0.1266)	
training:	Epoch: [16][182/233]	Loss 0.1233 (0.1266)	
training:	Epoch: [16][183/233]	Loss 0.1445 (0.1267)	
training:	Epoch: [16][184/233]	Loss 0.1243 (0.1267)	
training:	Epoch: [16][185/233]	Loss 0.1680 (0.1269)	
training:	Epoch: [16][186/233]	Loss 0.1038 (0.1268)	
training:	Epoch: [16][187/233]	Loss 0.1422 (0.1269)	
training:	Epoch: [16][188/233]	Loss 0.1286 (0.1269)	
training:	Epoch: [16][189/233]	Loss 0.0989 (0.1267)	
training:	Epoch: [16][190/233]	Loss 0.1736 (0.1270)	
training:	Epoch: [16][191/233]	Loss 0.1926 (0.1273)	
training:	Epoch: [16][192/233]	Loss 0.1150 (0.1272)	
training:	Epoch: [16][193/233]	Loss 0.1688 (0.1275)	
training:	Epoch: [16][194/233]	Loss 0.1018 (0.1273)	
training:	Epoch: [16][195/233]	Loss 0.1036 (0.1272)	
training:	Epoch: [16][196/233]	Loss 0.1126 (0.1271)	
training:	Epoch: [16][197/233]	Loss 0.1249 (0.1271)	
training:	Epoch: [16][198/233]	Loss 0.0969 (0.1270)	
training:	Epoch: [16][199/233]	Loss 0.1567 (0.1271)	
training:	Epoch: [16][200/233]	Loss 0.1468 (0.1272)	
training:	Epoch: [16][201/233]	Loss 0.1393 (0.1273)	
training:	Epoch: [16][202/233]	Loss 0.1137 (0.1272)	
training:	Epoch: [16][203/233]	Loss 0.1112 (0.1271)	
training:	Epoch: [16][204/233]	Loss 0.2063 (0.1275)	
training:	Epoch: [16][205/233]	Loss 0.1514 (0.1276)	
training:	Epoch: [16][206/233]	Loss 0.1528 (0.1278)	
training:	Epoch: [16][207/233]	Loss 0.0963 (0.1276)	
training:	Epoch: [16][208/233]	Loss 0.2002 (0.1280)	
training:	Epoch: [16][209/233]	Loss 0.1176 (0.1279)	
training:	Epoch: [16][210/233]	Loss 0.1029 (0.1278)	
training:	Epoch: [16][211/233]	Loss 0.1446 (0.1279)	
training:	Epoch: [16][212/233]	Loss 0.1175 (0.1278)	
training:	Epoch: [16][213/233]	Loss 0.1278 (0.1278)	
training:	Epoch: [16][214/233]	Loss 0.1056 (0.1277)	
training:	Epoch: [16][215/233]	Loss 0.1290 (0.1277)	
training:	Epoch: [16][216/233]	Loss 0.1089 (0.1276)	
training:	Epoch: [16][217/233]	Loss 0.1344 (0.1277)	
training:	Epoch: [16][218/233]	Loss 0.1258 (0.1277)	
training:	Epoch: [16][219/233]	Loss 0.2271 (0.1281)	
training:	Epoch: [16][220/233]	Loss 0.1402 (0.1282)	
training:	Epoch: [16][221/233]	Loss 0.1041 (0.1281)	
training:	Epoch: [16][222/233]	Loss 0.1158 (0.1280)	
training:	Epoch: [16][223/233]	Loss 0.1130 (0.1279)	
training:	Epoch: [16][224/233]	Loss 0.1325 (0.1280)	
training:	Epoch: [16][225/233]	Loss 0.1380 (0.1280)	
training:	Epoch: [16][226/233]	Loss 0.1079 (0.1279)	
training:	Epoch: [16][227/233]	Loss 0.0934 (0.1278)	
training:	Epoch: [16][228/233]	Loss 0.1485 (0.1278)	
training:	Epoch: [16][229/233]	Loss 0.0970 (0.1277)	
training:	Epoch: [16][230/233]	Loss 0.1208 (0.1277)	
training:	Epoch: [16][231/233]	Loss 0.1225 (0.1277)	
training:	Epoch: [16][232/233]	Loss 0.1434 (0.1277)	
training:	Epoch: [16][233/233]	Loss 0.1057 (0.1276)	
Training:	 Loss: 0.1273

Training:	 ACC: 0.9996 0.9996 0.9992 1.0000
Validation:	 ACC: 0.8635 0.8636 0.8649 0.8621
Validation:	 Best_BACC: 0.8635 0.8636 0.8649 0.8621
Validation:	 Loss: 0.3375
Pretraining:	Epoch 17/200
----------
training:	Epoch: [17][1/233]	Loss 0.0970 (0.0970)	
training:	Epoch: [17][2/233]	Loss 0.1041 (0.1006)	
training:	Epoch: [17][3/233]	Loss 0.1611 (0.1207)	
training:	Epoch: [17][4/233]	Loss 0.0983 (0.1151)	
training:	Epoch: [17][5/233]	Loss 0.1383 (0.1198)	
training:	Epoch: [17][6/233]	Loss 0.1075 (0.1177)	
training:	Epoch: [17][7/233]	Loss 0.1156 (0.1174)	
training:	Epoch: [17][8/233]	Loss 0.1171 (0.1174)	
training:	Epoch: [17][9/233]	Loss 0.1031 (0.1158)	
training:	Epoch: [17][10/233]	Loss 0.1995 (0.1242)	
training:	Epoch: [17][11/233]	Loss 0.1403 (0.1256)	
training:	Epoch: [17][12/233]	Loss 0.1166 (0.1249)	
training:	Epoch: [17][13/233]	Loss 0.1412 (0.1261)	
training:	Epoch: [17][14/233]	Loss 0.1148 (0.1253)	
training:	Epoch: [17][15/233]	Loss 0.1104 (0.1243)	
training:	Epoch: [17][16/233]	Loss 0.1210 (0.1241)	
training:	Epoch: [17][17/233]	Loss 0.1408 (0.1251)	
training:	Epoch: [17][18/233]	Loss 0.1224 (0.1250)	
training:	Epoch: [17][19/233]	Loss 0.0991 (0.1236)	
training:	Epoch: [17][20/233]	Loss 0.2142 (0.1281)	
training:	Epoch: [17][21/233]	Loss 0.1232 (0.1279)	
training:	Epoch: [17][22/233]	Loss 0.1038 (0.1268)	
training:	Epoch: [17][23/233]	Loss 0.0980 (0.1255)	
training:	Epoch: [17][24/233]	Loss 0.1895 (0.1282)	
training:	Epoch: [17][25/233]	Loss 0.1184 (0.1278)	
training:	Epoch: [17][26/233]	Loss 0.1070 (0.1270)	
training:	Epoch: [17][27/233]	Loss 0.1161 (0.1266)	
training:	Epoch: [17][28/233]	Loss 0.1064 (0.1259)	
training:	Epoch: [17][29/233]	Loss 0.1145 (0.1255)	
training:	Epoch: [17][30/233]	Loss 0.0875 (0.1242)	
training:	Epoch: [17][31/233]	Loss 0.0858 (0.1230)	
training:	Epoch: [17][32/233]	Loss 0.0857 (0.1218)	
training:	Epoch: [17][33/233]	Loss 0.1007 (0.1212)	
training:	Epoch: [17][34/233]	Loss 0.1357 (0.1216)	
training:	Epoch: [17][35/233]	Loss 0.1097 (0.1213)	
training:	Epoch: [17][36/233]	Loss 0.1005 (0.1207)	
training:	Epoch: [17][37/233]	Loss 0.1374 (0.1211)	
training:	Epoch: [17][38/233]	Loss 0.1344 (0.1215)	
training:	Epoch: [17][39/233]	Loss 0.1071 (0.1211)	
training:	Epoch: [17][40/233]	Loss 0.1272 (0.1213)	
training:	Epoch: [17][41/233]	Loss 0.1101 (0.1210)	
training:	Epoch: [17][42/233]	Loss 0.1066 (0.1207)	
training:	Epoch: [17][43/233]	Loss 0.1277 (0.1208)	
training:	Epoch: [17][44/233]	Loss 0.0995 (0.1203)	
training:	Epoch: [17][45/233]	Loss 0.1221 (0.1204)	
training:	Epoch: [17][46/233]	Loss 0.0927 (0.1198)	
training:	Epoch: [17][47/233]	Loss 0.0968 (0.1193)	
training:	Epoch: [17][48/233]	Loss 0.1278 (0.1195)	
training:	Epoch: [17][49/233]	Loss 0.0903 (0.1189)	
training:	Epoch: [17][50/233]	Loss 0.1131 (0.1188)	
training:	Epoch: [17][51/233]	Loss 0.1137 (0.1187)	
training:	Epoch: [17][52/233]	Loss 0.1509 (0.1193)	
training:	Epoch: [17][53/233]	Loss 0.1665 (0.1202)	
training:	Epoch: [17][54/233]	Loss 0.1179 (0.1201)	
training:	Epoch: [17][55/233]	Loss 0.1372 (0.1204)	
training:	Epoch: [17][56/233]	Loss 0.0906 (0.1199)	
training:	Epoch: [17][57/233]	Loss 0.1460 (0.1204)	
training:	Epoch: [17][58/233]	Loss 0.0913 (0.1199)	
training:	Epoch: [17][59/233]	Loss 0.1143 (0.1198)	
training:	Epoch: [17][60/233]	Loss 0.1140 (0.1197)	
training:	Epoch: [17][61/233]	Loss 0.1043 (0.1194)	
training:	Epoch: [17][62/233]	Loss 0.1002 (0.1191)	
training:	Epoch: [17][63/233]	Loss 0.0915 (0.1187)	
training:	Epoch: [17][64/233]	Loss 0.1124 (0.1186)	
training:	Epoch: [17][65/233]	Loss 0.1065 (0.1184)	
training:	Epoch: [17][66/233]	Loss 0.0920 (0.1180)	
training:	Epoch: [17][67/233]	Loss 0.1323 (0.1182)	
training:	Epoch: [17][68/233]	Loss 0.1204 (0.1182)	
training:	Epoch: [17][69/233]	Loss 0.0901 (0.1178)	
training:	Epoch: [17][70/233]	Loss 0.1115 (0.1177)	
training:	Epoch: [17][71/233]	Loss 0.1590 (0.1183)	
training:	Epoch: [17][72/233]	Loss 0.0968 (0.1180)	
training:	Epoch: [17][73/233]	Loss 0.1238 (0.1181)	
training:	Epoch: [17][74/233]	Loss 0.1490 (0.1185)	
training:	Epoch: [17][75/233]	Loss 0.1009 (0.1183)	
training:	Epoch: [17][76/233]	Loss 0.1603 (0.1188)	
training:	Epoch: [17][77/233]	Loss 0.1078 (0.1187)	
training:	Epoch: [17][78/233]	Loss 0.1023 (0.1185)	
training:	Epoch: [17][79/233]	Loss 0.1116 (0.1184)	
training:	Epoch: [17][80/233]	Loss 0.1217 (0.1184)	
training:	Epoch: [17][81/233]	Loss 0.1596 (0.1189)	
training:	Epoch: [17][82/233]	Loss 0.1545 (0.1194)	
training:	Epoch: [17][83/233]	Loss 0.1032 (0.1192)	
training:	Epoch: [17][84/233]	Loss 0.1251 (0.1192)	
training:	Epoch: [17][85/233]	Loss 0.1013 (0.1190)	
training:	Epoch: [17][86/233]	Loss 0.1582 (0.1195)	
training:	Epoch: [17][87/233]	Loss 0.0891 (0.1191)	
training:	Epoch: [17][88/233]	Loss 0.1188 (0.1191)	
training:	Epoch: [17][89/233]	Loss 0.1585 (0.1196)	
training:	Epoch: [17][90/233]	Loss 0.1017 (0.1194)	
training:	Epoch: [17][91/233]	Loss 0.2244 (0.1205)	
training:	Epoch: [17][92/233]	Loss 0.1090 (0.1204)	
training:	Epoch: [17][93/233]	Loss 0.1279 (0.1205)	
training:	Epoch: [17][94/233]	Loss 0.1158 (0.1204)	
training:	Epoch: [17][95/233]	Loss 0.1112 (0.1203)	
training:	Epoch: [17][96/233]	Loss 0.1053 (0.1202)	
training:	Epoch: [17][97/233]	Loss 0.1258 (0.1202)	
training:	Epoch: [17][98/233]	Loss 0.1257 (0.1203)	
training:	Epoch: [17][99/233]	Loss 0.1153 (0.1203)	
training:	Epoch: [17][100/233]	Loss 0.1131 (0.1202)	
training:	Epoch: [17][101/233]	Loss 0.1182 (0.1202)	
training:	Epoch: [17][102/233]	Loss 0.1042 (0.1200)	
training:	Epoch: [17][103/233]	Loss 0.0858 (0.1197)	
training:	Epoch: [17][104/233]	Loss 0.1213 (0.1197)	
training:	Epoch: [17][105/233]	Loss 0.1058 (0.1196)	
training:	Epoch: [17][106/233]	Loss 0.0823 (0.1192)	
training:	Epoch: [17][107/233]	Loss 0.1123 (0.1191)	
training:	Epoch: [17][108/233]	Loss 0.1074 (0.1190)	
training:	Epoch: [17][109/233]	Loss 0.1773 (0.1196)	
training:	Epoch: [17][110/233]	Loss 0.0943 (0.1193)	
training:	Epoch: [17][111/233]	Loss 0.0910 (0.1191)	
training:	Epoch: [17][112/233]	Loss 0.1010 (0.1189)	
training:	Epoch: [17][113/233]	Loss 0.1056 (0.1188)	
training:	Epoch: [17][114/233]	Loss 0.1261 (0.1189)	
training:	Epoch: [17][115/233]	Loss 0.1257 (0.1189)	
training:	Epoch: [17][116/233]	Loss 0.1450 (0.1191)	
training:	Epoch: [17][117/233]	Loss 0.1201 (0.1192)	
training:	Epoch: [17][118/233]	Loss 0.1275 (0.1192)	
training:	Epoch: [17][119/233]	Loss 0.1268 (0.1193)	
training:	Epoch: [17][120/233]	Loss 0.1027 (0.1192)	
training:	Epoch: [17][121/233]	Loss 0.1297 (0.1192)	
training:	Epoch: [17][122/233]	Loss 0.1471 (0.1195)	
training:	Epoch: [17][123/233]	Loss 0.1163 (0.1194)	
training:	Epoch: [17][124/233]	Loss 0.0877 (0.1192)	
training:	Epoch: [17][125/233]	Loss 0.1167 (0.1192)	
training:	Epoch: [17][126/233]	Loss 0.1026 (0.1190)	
training:	Epoch: [17][127/233]	Loss 0.1258 (0.1191)	
training:	Epoch: [17][128/233]	Loss 0.1666 (0.1195)	
training:	Epoch: [17][129/233]	Loss 0.0980 (0.1193)	
training:	Epoch: [17][130/233]	Loss 0.1206 (0.1193)	
training:	Epoch: [17][131/233]	Loss 0.1106 (0.1192)	
training:	Epoch: [17][132/233]	Loss 0.1654 (0.1196)	
training:	Epoch: [17][133/233]	Loss 0.1030 (0.1195)	
training:	Epoch: [17][134/233]	Loss 0.0951 (0.1193)	
training:	Epoch: [17][135/233]	Loss 0.1477 (0.1195)	
training:	Epoch: [17][136/233]	Loss 0.1349 (0.1196)	
training:	Epoch: [17][137/233]	Loss 0.1204 (0.1196)	
training:	Epoch: [17][138/233]	Loss 0.1242 (0.1196)	
training:	Epoch: [17][139/233]	Loss 0.1138 (0.1196)	
training:	Epoch: [17][140/233]	Loss 0.1117 (0.1195)	
training:	Epoch: [17][141/233]	Loss 0.1146 (0.1195)	
training:	Epoch: [17][142/233]	Loss 0.4435 (0.1218)	
training:	Epoch: [17][143/233]	Loss 0.1905 (0.1223)	
training:	Epoch: [17][144/233]	Loss 0.1172 (0.1222)	
training:	Epoch: [17][145/233]	Loss 0.0957 (0.1221)	
training:	Epoch: [17][146/233]	Loss 0.2489 (0.1229)	
training:	Epoch: [17][147/233]	Loss 0.0996 (0.1228)	
training:	Epoch: [17][148/233]	Loss 0.1470 (0.1229)	
training:	Epoch: [17][149/233]	Loss 0.1193 (0.1229)	
training:	Epoch: [17][150/233]	Loss 0.1163 (0.1229)	
training:	Epoch: [17][151/233]	Loss 0.1266 (0.1229)	
training:	Epoch: [17][152/233]	Loss 0.0784 (0.1226)	
training:	Epoch: [17][153/233]	Loss 0.1089 (0.1225)	
training:	Epoch: [17][154/233]	Loss 0.1215 (0.1225)	
training:	Epoch: [17][155/233]	Loss 0.1364 (0.1226)	
training:	Epoch: [17][156/233]	Loss 0.1420 (0.1227)	
training:	Epoch: [17][157/233]	Loss 0.1290 (0.1227)	
training:	Epoch: [17][158/233]	Loss 0.1896 (0.1232)	
training:	Epoch: [17][159/233]	Loss 0.1456 (0.1233)	
training:	Epoch: [17][160/233]	Loss 0.1370 (0.1234)	
training:	Epoch: [17][161/233]	Loss 0.1246 (0.1234)	
training:	Epoch: [17][162/233]	Loss 0.1170 (0.1234)	
training:	Epoch: [17][163/233]	Loss 0.1034 (0.1232)	
training:	Epoch: [17][164/233]	Loss 0.1441 (0.1234)	
training:	Epoch: [17][165/233]	Loss 0.0992 (0.1232)	
training:	Epoch: [17][166/233]	Loss 0.1003 (0.1231)	
training:	Epoch: [17][167/233]	Loss 0.0984 (0.1229)	
training:	Epoch: [17][168/233]	Loss 0.1341 (0.1230)	
training:	Epoch: [17][169/233]	Loss 0.1139 (0.1230)	
training:	Epoch: [17][170/233]	Loss 0.1549 (0.1231)	
training:	Epoch: [17][171/233]	Loss 0.1414 (0.1232)	
training:	Epoch: [17][172/233]	Loss 0.1279 (0.1233)	
training:	Epoch: [17][173/233]	Loss 0.1423 (0.1234)	
training:	Epoch: [17][174/233]	Loss 0.1083 (0.1233)	
training:	Epoch: [17][175/233]	Loss 0.1094 (0.1232)	
training:	Epoch: [17][176/233]	Loss 0.1026 (0.1231)	
training:	Epoch: [17][177/233]	Loss 0.1307 (0.1231)	
training:	Epoch: [17][178/233]	Loss 0.1012 (0.1230)	
training:	Epoch: [17][179/233]	Loss 0.0933 (0.1229)	
training:	Epoch: [17][180/233]	Loss 0.0975 (0.1227)	
training:	Epoch: [17][181/233]	Loss 0.1760 (0.1230)	
training:	Epoch: [17][182/233]	Loss 0.1259 (0.1230)	
training:	Epoch: [17][183/233]	Loss 0.1204 (0.1230)	
training:	Epoch: [17][184/233]	Loss 0.1034 (0.1229)	
training:	Epoch: [17][185/233]	Loss 0.1113 (0.1228)	
training:	Epoch: [17][186/233]	Loss 0.1257 (0.1229)	
training:	Epoch: [17][187/233]	Loss 0.1137 (0.1228)	
training:	Epoch: [17][188/233]	Loss 0.0826 (0.1226)	
training:	Epoch: [17][189/233]	Loss 0.1096 (0.1225)	
training:	Epoch: [17][190/233]	Loss 0.1043 (0.1224)	
training:	Epoch: [17][191/233]	Loss 0.1031 (0.1223)	
training:	Epoch: [17][192/233]	Loss 0.1467 (0.1225)	
training:	Epoch: [17][193/233]	Loss 0.1420 (0.1226)	
training:	Epoch: [17][194/233]	Loss 0.1929 (0.1229)	
training:	Epoch: [17][195/233]	Loss 0.1192 (0.1229)	
training:	Epoch: [17][196/233]	Loss 0.1352 (0.1230)	
training:	Epoch: [17][197/233]	Loss 0.2276 (0.1235)	
training:	Epoch: [17][198/233]	Loss 0.2340 (0.1240)	
training:	Epoch: [17][199/233]	Loss 0.1117 (0.1240)	
training:	Epoch: [17][200/233]	Loss 0.1302 (0.1240)	
training:	Epoch: [17][201/233]	Loss 0.0987 (0.1239)	
training:	Epoch: [17][202/233]	Loss 0.1171 (0.1239)	
training:	Epoch: [17][203/233]	Loss 0.0917 (0.1237)	
training:	Epoch: [17][204/233]	Loss 0.1360 (0.1238)	
training:	Epoch: [17][205/233]	Loss 0.1435 (0.1239)	
training:	Epoch: [17][206/233]	Loss 0.1266 (0.1239)	
training:	Epoch: [17][207/233]	Loss 0.0887 (0.1237)	
training:	Epoch: [17][208/233]	Loss 0.1183 (0.1237)	
training:	Epoch: [17][209/233]	Loss 0.0961 (0.1235)	
training:	Epoch: [17][210/233]	Loss 0.0964 (0.1234)	
training:	Epoch: [17][211/233]	Loss 0.0780 (0.1232)	
training:	Epoch: [17][212/233]	Loss 0.1292 (0.1232)	
training:	Epoch: [17][213/233]	Loss 0.1466 (0.1233)	
training:	Epoch: [17][214/233]	Loss 0.1140 (0.1233)	
training:	Epoch: [17][215/233]	Loss 0.1006 (0.1232)	
training:	Epoch: [17][216/233]	Loss 0.1042 (0.1231)	
training:	Epoch: [17][217/233]	Loss 0.1035 (0.1230)	
training:	Epoch: [17][218/233]	Loss 0.1234 (0.1230)	
training:	Epoch: [17][219/233]	Loss 0.0930 (0.1229)	
training:	Epoch: [17][220/233]	Loss 0.0957 (0.1227)	
training:	Epoch: [17][221/233]	Loss 0.0922 (0.1226)	
training:	Epoch: [17][222/233]	Loss 0.1292 (0.1226)	
training:	Epoch: [17][223/233]	Loss 0.1014 (0.1225)	
training:	Epoch: [17][224/233]	Loss 0.0887 (0.1224)	
training:	Epoch: [17][225/233]	Loss 0.1348 (0.1224)	
training:	Epoch: [17][226/233]	Loss 0.1928 (0.1228)	
training:	Epoch: [17][227/233]	Loss 0.1210 (0.1228)	
training:	Epoch: [17][228/233]	Loss 0.1361 (0.1228)	
training:	Epoch: [17][229/233]	Loss 0.2726 (0.1235)	
training:	Epoch: [17][230/233]	Loss 0.1220 (0.1235)	
training:	Epoch: [17][231/233]	Loss 0.1144 (0.1234)	
training:	Epoch: [17][232/233]	Loss 0.1352 (0.1235)	
training:	Epoch: [17][233/233]	Loss 0.1241 (0.1235)	
Training:	 Loss: 0.1232

Training:	 ACC: 0.9999 0.9999 0.9997 1.0000
Validation:	 ACC: 0.8595 0.8604 0.8792 0.8397
Validation:	 Best_BACC: 0.8635 0.8636 0.8649 0.8621
Validation:	 Loss: 0.3334
Pretraining:	Epoch 18/200
----------
training:	Epoch: [18][1/233]	Loss 0.1149 (0.1149)	
training:	Epoch: [18][2/233]	Loss 0.1144 (0.1146)	
training:	Epoch: [18][3/233]	Loss 0.2013 (0.1435)	
training:	Epoch: [18][4/233]	Loss 0.1186 (0.1373)	
training:	Epoch: [18][5/233]	Loss 0.1115 (0.1321)	
training:	Epoch: [18][6/233]	Loss 0.1248 (0.1309)	
training:	Epoch: [18][7/233]	Loss 0.1110 (0.1281)	
training:	Epoch: [18][8/233]	Loss 0.0976 (0.1243)	
training:	Epoch: [18][9/233]	Loss 0.1151 (0.1232)	
training:	Epoch: [18][10/233]	Loss 0.0891 (0.1198)	
training:	Epoch: [18][11/233]	Loss 0.0899 (0.1171)	
training:	Epoch: [18][12/233]	Loss 0.1030 (0.1159)	
training:	Epoch: [18][13/233]	Loss 0.0959 (0.1144)	
training:	Epoch: [18][14/233]	Loss 0.1391 (0.1162)	
training:	Epoch: [18][15/233]	Loss 0.0934 (0.1146)	
training:	Epoch: [18][16/233]	Loss 0.1214 (0.1151)	
training:	Epoch: [18][17/233]	Loss 0.1130 (0.1149)	
training:	Epoch: [18][18/233]	Loss 0.1090 (0.1146)	
training:	Epoch: [18][19/233]	Loss 0.2533 (0.1219)	
training:	Epoch: [18][20/233]	Loss 0.1610 (0.1239)	
training:	Epoch: [18][21/233]	Loss 0.1229 (0.1238)	
training:	Epoch: [18][22/233]	Loss 0.0965 (0.1226)	
training:	Epoch: [18][23/233]	Loss 0.1634 (0.1243)	
training:	Epoch: [18][24/233]	Loss 0.0907 (0.1229)	
training:	Epoch: [18][25/233]	Loss 0.0956 (0.1219)	
training:	Epoch: [18][26/233]	Loss 0.0969 (0.1209)	
training:	Epoch: [18][27/233]	Loss 0.0929 (0.1199)	
training:	Epoch: [18][28/233]	Loss 0.1010 (0.1192)	
training:	Epoch: [18][29/233]	Loss 0.1287 (0.1195)	
training:	Epoch: [18][30/233]	Loss 0.1025 (0.1189)	
training:	Epoch: [18][31/233]	Loss 0.1460 (0.1198)	
training:	Epoch: [18][32/233]	Loss 0.1057 (0.1194)	
training:	Epoch: [18][33/233]	Loss 0.1017 (0.1188)	
training:	Epoch: [18][34/233]	Loss 0.1328 (0.1193)	
training:	Epoch: [18][35/233]	Loss 0.1165 (0.1192)	
training:	Epoch: [18][36/233]	Loss 0.0872 (0.1183)	
training:	Epoch: [18][37/233]	Loss 0.0740 (0.1171)	
training:	Epoch: [18][38/233]	Loss 0.1079 (0.1168)	
training:	Epoch: [18][39/233]	Loss 0.1266 (0.1171)	
training:	Epoch: [18][40/233]	Loss 0.1160 (0.1171)	
training:	Epoch: [18][41/233]	Loss 0.1319 (0.1174)	
training:	Epoch: [18][42/233]	Loss 0.1119 (0.1173)	
training:	Epoch: [18][43/233]	Loss 0.0944 (0.1168)	
training:	Epoch: [18][44/233]	Loss 0.1011 (0.1164)	
training:	Epoch: [18][45/233]	Loss 0.0803 (0.1156)	
training:	Epoch: [18][46/233]	Loss 0.1099 (0.1155)	
training:	Epoch: [18][47/233]	Loss 0.1000 (0.1152)	
training:	Epoch: [18][48/233]	Loss 0.1369 (0.1156)	
training:	Epoch: [18][49/233]	Loss 0.1096 (0.1155)	
training:	Epoch: [18][50/233]	Loss 0.1099 (0.1154)	
training:	Epoch: [18][51/233]	Loss 0.1319 (0.1157)	
training:	Epoch: [18][52/233]	Loss 0.0822 (0.1151)	
training:	Epoch: [18][53/233]	Loss 0.1321 (0.1154)	
training:	Epoch: [18][54/233]	Loss 0.0972 (0.1150)	
training:	Epoch: [18][55/233]	Loss 0.0956 (0.1147)	
training:	Epoch: [18][56/233]	Loss 0.1068 (0.1145)	
training:	Epoch: [18][57/233]	Loss 0.1455 (0.1151)	
training:	Epoch: [18][58/233]	Loss 0.1091 (0.1150)	
training:	Epoch: [18][59/233]	Loss 0.1079 (0.1149)	
training:	Epoch: [18][60/233]	Loss 0.0806 (0.1143)	
training:	Epoch: [18][61/233]	Loss 0.0986 (0.1140)	
training:	Epoch: [18][62/233]	Loss 0.1106 (0.1140)	
training:	Epoch: [18][63/233]	Loss 0.1028 (0.1138)	
training:	Epoch: [18][64/233]	Loss 0.0757 (0.1132)	
training:	Epoch: [18][65/233]	Loss 0.0755 (0.1126)	
training:	Epoch: [18][66/233]	Loss 0.1199 (0.1127)	
training:	Epoch: [18][67/233]	Loss 0.1244 (0.1129)	
training:	Epoch: [18][68/233]	Loss 0.1140 (0.1129)	
training:	Epoch: [18][69/233]	Loss 0.1054 (0.1128)	
training:	Epoch: [18][70/233]	Loss 0.1084 (0.1128)	
training:	Epoch: [18][71/233]	Loss 0.1091 (0.1127)	
training:	Epoch: [18][72/233]	Loss 0.1243 (0.1129)	
training:	Epoch: [18][73/233]	Loss 0.1223 (0.1130)	
training:	Epoch: [18][74/233]	Loss 0.1064 (0.1129)	
training:	Epoch: [18][75/233]	Loss 0.1147 (0.1129)	
training:	Epoch: [18][76/233]	Loss 0.0884 (0.1126)	
training:	Epoch: [18][77/233]	Loss 0.0940 (0.1124)	
training:	Epoch: [18][78/233]	Loss 0.1046 (0.1123)	
training:	Epoch: [18][79/233]	Loss 0.0916 (0.1120)	
training:	Epoch: [18][80/233]	Loss 0.1034 (0.1119)	
training:	Epoch: [18][81/233]	Loss 0.0956 (0.1117)	
training:	Epoch: [18][82/233]	Loss 0.1278 (0.1119)	
training:	Epoch: [18][83/233]	Loss 0.1628 (0.1125)	
training:	Epoch: [18][84/233]	Loss 0.0993 (0.1123)	
training:	Epoch: [18][85/233]	Loss 0.0974 (0.1122)	
training:	Epoch: [18][86/233]	Loss 0.1281 (0.1124)	
training:	Epoch: [18][87/233]	Loss 0.1111 (0.1123)	
training:	Epoch: [18][88/233]	Loss 0.2022 (0.1134)	
training:	Epoch: [18][89/233]	Loss 0.1025 (0.1132)	
training:	Epoch: [18][90/233]	Loss 0.0932 (0.1130)	
training:	Epoch: [18][91/233]	Loss 0.0967 (0.1128)	
training:	Epoch: [18][92/233]	Loss 0.0945 (0.1126)	
training:	Epoch: [18][93/233]	Loss 0.1088 (0.1126)	
training:	Epoch: [18][94/233]	Loss 0.0848 (0.1123)	
training:	Epoch: [18][95/233]	Loss 0.1020 (0.1122)	
training:	Epoch: [18][96/233]	Loss 0.1094 (0.1122)	
training:	Epoch: [18][97/233]	Loss 0.1077 (0.1121)	
training:	Epoch: [18][98/233]	Loss 0.1413 (0.1124)	
training:	Epoch: [18][99/233]	Loss 0.0902 (0.1122)	
training:	Epoch: [18][100/233]	Loss 0.1692 (0.1128)	
training:	Epoch: [18][101/233]	Loss 0.1356 (0.1130)	
training:	Epoch: [18][102/233]	Loss 0.1158 (0.1130)	
training:	Epoch: [18][103/233]	Loss 0.1088 (0.1130)	
training:	Epoch: [18][104/233]	Loss 0.1156 (0.1130)	
training:	Epoch: [18][105/233]	Loss 0.1113 (0.1130)	
training:	Epoch: [18][106/233]	Loss 0.0903 (0.1128)	
training:	Epoch: [18][107/233]	Loss 0.1152 (0.1128)	
training:	Epoch: [18][108/233]	Loss 0.0984 (0.1127)	
training:	Epoch: [18][109/233]	Loss 0.1472 (0.1130)	
training:	Epoch: [18][110/233]	Loss 0.0997 (0.1129)	
training:	Epoch: [18][111/233]	Loss 0.1141 (0.1129)	
training:	Epoch: [18][112/233]	Loss 0.0953 (0.1127)	
training:	Epoch: [18][113/233]	Loss 0.0934 (0.1125)	
training:	Epoch: [18][114/233]	Loss 0.1293 (0.1127)	
training:	Epoch: [18][115/233]	Loss 0.0998 (0.1126)	
training:	Epoch: [18][116/233]	Loss 0.1401 (0.1128)	
training:	Epoch: [18][117/233]	Loss 0.0924 (0.1126)	
training:	Epoch: [18][118/233]	Loss 0.0788 (0.1124)	
training:	Epoch: [18][119/233]	Loss 0.1111 (0.1123)	
training:	Epoch: [18][120/233]	Loss 0.1350 (0.1125)	
training:	Epoch: [18][121/233]	Loss 0.1337 (0.1127)	
training:	Epoch: [18][122/233]	Loss 0.0905 (0.1125)	
training:	Epoch: [18][123/233]	Loss 0.0993 (0.1124)	
training:	Epoch: [18][124/233]	Loss 0.0834 (0.1122)	
training:	Epoch: [18][125/233]	Loss 0.1173 (0.1122)	
training:	Epoch: [18][126/233]	Loss 0.1891 (0.1128)	
training:	Epoch: [18][127/233]	Loss 0.0802 (0.1126)	
training:	Epoch: [18][128/233]	Loss 0.1370 (0.1128)	
training:	Epoch: [18][129/233]	Loss 0.1091 (0.1127)	
training:	Epoch: [18][130/233]	Loss 0.1011 (0.1126)	
training:	Epoch: [18][131/233]	Loss 0.0723 (0.1123)	
training:	Epoch: [18][132/233]	Loss 0.1236 (0.1124)	
training:	Epoch: [18][133/233]	Loss 0.1183 (0.1125)	
training:	Epoch: [18][134/233]	Loss 0.1173 (0.1125)	
training:	Epoch: [18][135/233]	Loss 0.1051 (0.1125)	
training:	Epoch: [18][136/233]	Loss 0.1146 (0.1125)	
training:	Epoch: [18][137/233]	Loss 0.1384 (0.1127)	
training:	Epoch: [18][138/233]	Loss 0.0938 (0.1125)	
training:	Epoch: [18][139/233]	Loss 0.1102 (0.1125)	
training:	Epoch: [18][140/233]	Loss 0.1032 (0.1124)	
training:	Epoch: [18][141/233]	Loss 0.0780 (0.1122)	
training:	Epoch: [18][142/233]	Loss 0.0979 (0.1121)	
training:	Epoch: [18][143/233]	Loss 0.1255 (0.1122)	
training:	Epoch: [18][144/233]	Loss 0.1012 (0.1121)	
training:	Epoch: [18][145/233]	Loss 0.1415 (0.1123)	
training:	Epoch: [18][146/233]	Loss 0.0894 (0.1122)	
training:	Epoch: [18][147/233]	Loss 0.1024 (0.1121)	
training:	Epoch: [18][148/233]	Loss 0.1139 (0.1121)	
training:	Epoch: [18][149/233]	Loss 0.0681 (0.1118)	
training:	Epoch: [18][150/233]	Loss 0.1161 (0.1118)	
training:	Epoch: [18][151/233]	Loss 0.1376 (0.1120)	
training:	Epoch: [18][152/233]	Loss 0.0734 (0.1118)	
training:	Epoch: [18][153/233]	Loss 0.1611 (0.1121)	
training:	Epoch: [18][154/233]	Loss 0.0897 (0.1119)	
training:	Epoch: [18][155/233]	Loss 0.0996 (0.1118)	
training:	Epoch: [18][156/233]	Loss 0.1125 (0.1119)	
training:	Epoch: [18][157/233]	Loss 0.0828 (0.1117)	
training:	Epoch: [18][158/233]	Loss 0.1081 (0.1116)	
training:	Epoch: [18][159/233]	Loss 0.1515 (0.1119)	
training:	Epoch: [18][160/233]	Loss 0.1143 (0.1119)	
training:	Epoch: [18][161/233]	Loss 0.1096 (0.1119)	
training:	Epoch: [18][162/233]	Loss 0.0978 (0.1118)	
training:	Epoch: [18][163/233]	Loss 0.1249 (0.1119)	
training:	Epoch: [18][164/233]	Loss 0.0980 (0.1118)	
training:	Epoch: [18][165/233]	Loss 0.0916 (0.1117)	
training:	Epoch: [18][166/233]	Loss 0.0844 (0.1115)	
training:	Epoch: [18][167/233]	Loss 0.1327 (0.1116)	
training:	Epoch: [18][168/233]	Loss 0.0976 (0.1116)	
training:	Epoch: [18][169/233]	Loss 0.1072 (0.1115)	
training:	Epoch: [18][170/233]	Loss 0.1179 (0.1116)	
training:	Epoch: [18][171/233]	Loss 0.0797 (0.1114)	
training:	Epoch: [18][172/233]	Loss 0.2128 (0.1120)	
training:	Epoch: [18][173/233]	Loss 0.0908 (0.1119)	
training:	Epoch: [18][174/233]	Loss 0.1445 (0.1120)	
training:	Epoch: [18][175/233]	Loss 0.1157 (0.1121)	
training:	Epoch: [18][176/233]	Loss 0.1115 (0.1121)	
training:	Epoch: [18][177/233]	Loss 0.0883 (0.1119)	
training:	Epoch: [18][178/233]	Loss 0.0785 (0.1117)	
training:	Epoch: [18][179/233]	Loss 0.1129 (0.1117)	
training:	Epoch: [18][180/233]	Loss 0.1000 (0.1117)	
training:	Epoch: [18][181/233]	Loss 0.1073 (0.1117)	
training:	Epoch: [18][182/233]	Loss 0.1168 (0.1117)	
training:	Epoch: [18][183/233]	Loss 0.1624 (0.1120)	
training:	Epoch: [18][184/233]	Loss 0.1041 (0.1119)	
training:	Epoch: [18][185/233]	Loss 0.0837 (0.1118)	
training:	Epoch: [18][186/233]	Loss 0.1023 (0.1117)	
training:	Epoch: [18][187/233]	Loss 0.1131 (0.1117)	
training:	Epoch: [18][188/233]	Loss 0.0869 (0.1116)	
training:	Epoch: [18][189/233]	Loss 0.0962 (0.1115)	
training:	Epoch: [18][190/233]	Loss 0.1888 (0.1119)	
training:	Epoch: [18][191/233]	Loss 0.1784 (0.1123)	
training:	Epoch: [18][192/233]	Loss 0.0954 (0.1122)	
training:	Epoch: [18][193/233]	Loss 0.1352 (0.1123)	
training:	Epoch: [18][194/233]	Loss 0.0805 (0.1121)	
training:	Epoch: [18][195/233]	Loss 0.1347 (0.1122)	
training:	Epoch: [18][196/233]	Loss 0.1089 (0.1122)	
training:	Epoch: [18][197/233]	Loss 0.1099 (0.1122)	
training:	Epoch: [18][198/233]	Loss 0.0894 (0.1121)	
training:	Epoch: [18][199/233]	Loss 0.1040 (0.1121)	
training:	Epoch: [18][200/233]	Loss 0.1174 (0.1121)	
training:	Epoch: [18][201/233]	Loss 0.1141 (0.1121)	
training:	Epoch: [18][202/233]	Loss 0.0861 (0.1120)	
training:	Epoch: [18][203/233]	Loss 0.1281 (0.1120)	
training:	Epoch: [18][204/233]	Loss 0.1255 (0.1121)	
training:	Epoch: [18][205/233]	Loss 0.1546 (0.1123)	
training:	Epoch: [18][206/233]	Loss 0.1189 (0.1124)	
training:	Epoch: [18][207/233]	Loss 0.1011 (0.1123)	
training:	Epoch: [18][208/233]	Loss 0.1078 (0.1123)	
training:	Epoch: [18][209/233]	Loss 0.1239 (0.1123)	
training:	Epoch: [18][210/233]	Loss 0.1281 (0.1124)	
training:	Epoch: [18][211/233]	Loss 0.1050 (0.1124)	
training:	Epoch: [18][212/233]	Loss 0.1132 (0.1124)	
training:	Epoch: [18][213/233]	Loss 0.1555 (0.1126)	
training:	Epoch: [18][214/233]	Loss 0.1295 (0.1127)	
training:	Epoch: [18][215/233]	Loss 0.1060 (0.1126)	
training:	Epoch: [18][216/233]	Loss 0.1333 (0.1127)	
training:	Epoch: [18][217/233]	Loss 0.1113 (0.1127)	
training:	Epoch: [18][218/233]	Loss 0.1049 (0.1127)	
training:	Epoch: [18][219/233]	Loss 0.1394 (0.1128)	
training:	Epoch: [18][220/233]	Loss 0.1114 (0.1128)	
training:	Epoch: [18][221/233]	Loss 0.1431 (0.1129)	
training:	Epoch: [18][222/233]	Loss 0.1375 (0.1130)	
training:	Epoch: [18][223/233]	Loss 0.1016 (0.1130)	
training:	Epoch: [18][224/233]	Loss 0.1621 (0.1132)	
training:	Epoch: [18][225/233]	Loss 0.0892 (0.1131)	
training:	Epoch: [18][226/233]	Loss 0.0885 (0.1130)	
training:	Epoch: [18][227/233]	Loss 0.1751 (0.1133)	
training:	Epoch: [18][228/233]	Loss 0.1020 (0.1132)	
training:	Epoch: [18][229/233]	Loss 0.1187 (0.1132)	
training:	Epoch: [18][230/233]	Loss 0.1811 (0.1135)	
training:	Epoch: [18][231/233]	Loss 0.1062 (0.1135)	
training:	Epoch: [18][232/233]	Loss 0.1171 (0.1135)	
training:	Epoch: [18][233/233]	Loss 0.1011 (0.1135)	
Training:	 Loss: 0.1132

Training:	 ACC: 1.0000 1.0000 1.0000 1.0000
Validation:	 ACC: 0.8621 0.8620 0.8588 0.8655
Validation:	 Best_BACC: 0.8635 0.8636 0.8649 0.8621
Validation:	 Loss: 0.3356
Pretraining:	Epoch 19/200
----------
training:	Epoch: [19][1/233]	Loss 0.1419 (0.1419)	
training:	Epoch: [19][2/233]	Loss 0.0931 (0.1175)	
training:	Epoch: [19][3/233]	Loss 0.2185 (0.1511)	
training:	Epoch: [19][4/233]	Loss 0.1131 (0.1416)	
training:	Epoch: [19][5/233]	Loss 0.1459 (0.1425)	
training:	Epoch: [19][6/233]	Loss 0.0785 (0.1318)	
training:	Epoch: [19][7/233]	Loss 0.0873 (0.1255)	
training:	Epoch: [19][8/233]	Loss 0.1071 (0.1232)	
training:	Epoch: [19][9/233]	Loss 0.1013 (0.1207)	
training:	Epoch: [19][10/233]	Loss 0.1015 (0.1188)	
training:	Epoch: [19][11/233]	Loss 0.0926 (0.1164)	
training:	Epoch: [19][12/233]	Loss 0.1007 (0.1151)	
training:	Epoch: [19][13/233]	Loss 0.0733 (0.1119)	
training:	Epoch: [19][14/233]	Loss 0.0982 (0.1109)	
training:	Epoch: [19][15/233]	Loss 0.0893 (0.1095)	
training:	Epoch: [19][16/233]	Loss 0.0838 (0.1079)	
training:	Epoch: [19][17/233]	Loss 0.1123 (0.1081)	
training:	Epoch: [19][18/233]	Loss 0.1152 (0.1085)	
training:	Epoch: [19][19/233]	Loss 0.1368 (0.1100)	
training:	Epoch: [19][20/233]	Loss 0.1054 (0.1098)	
training:	Epoch: [19][21/233]	Loss 0.0936 (0.1090)	
training:	Epoch: [19][22/233]	Loss 0.0862 (0.1080)	
training:	Epoch: [19][23/233]	Loss 0.0954 (0.1074)	
training:	Epoch: [19][24/233]	Loss 0.0920 (0.1068)	
training:	Epoch: [19][25/233]	Loss 0.0758 (0.1055)	
training:	Epoch: [19][26/233]	Loss 0.1309 (0.1065)	
training:	Epoch: [19][27/233]	Loss 0.1190 (0.1070)	
training:	Epoch: [19][28/233]	Loss 0.0787 (0.1060)	
training:	Epoch: [19][29/233]	Loss 0.0818 (0.1051)	
training:	Epoch: [19][30/233]	Loss 0.0864 (0.1045)	
training:	Epoch: [19][31/233]	Loss 0.0933 (0.1042)	
training:	Epoch: [19][32/233]	Loss 0.1166 (0.1045)	
training:	Epoch: [19][33/233]	Loss 0.1016 (0.1045)	
training:	Epoch: [19][34/233]	Loss 0.1236 (0.1050)	
training:	Epoch: [19][35/233]	Loss 0.1235 (0.1055)	
training:	Epoch: [19][36/233]	Loss 0.1145 (0.1058)	
training:	Epoch: [19][37/233]	Loss 0.1010 (0.1057)	
training:	Epoch: [19][38/233]	Loss 0.0911 (0.1053)	
training:	Epoch: [19][39/233]	Loss 0.1079 (0.1053)	
training:	Epoch: [19][40/233]	Loss 0.1085 (0.1054)	
training:	Epoch: [19][41/233]	Loss 0.0838 (0.1049)	
training:	Epoch: [19][42/233]	Loss 0.0866 (0.1045)	
training:	Epoch: [19][43/233]	Loss 0.0880 (0.1041)	
training:	Epoch: [19][44/233]	Loss 0.1156 (0.1043)	
training:	Epoch: [19][45/233]	Loss 0.0694 (0.1036)	
training:	Epoch: [19][46/233]	Loss 0.0888 (0.1032)	
training:	Epoch: [19][47/233]	Loss 0.1083 (0.1034)	
training:	Epoch: [19][48/233]	Loss 0.1341 (0.1040)	
training:	Epoch: [19][49/233]	Loss 0.1132 (0.1042)	
training:	Epoch: [19][50/233]	Loss 0.0917 (0.1039)	
training:	Epoch: [19][51/233]	Loss 0.0983 (0.1038)	
training:	Epoch: [19][52/233]	Loss 0.0987 (0.1037)	
training:	Epoch: [19][53/233]	Loss 0.1425 (0.1045)	
training:	Epoch: [19][54/233]	Loss 0.1175 (0.1047)	
training:	Epoch: [19][55/233]	Loss 0.1410 (0.1054)	
training:	Epoch: [19][56/233]	Loss 0.0854 (0.1050)	
training:	Epoch: [19][57/233]	Loss 0.1340 (0.1055)	
training:	Epoch: [19][58/233]	Loss 0.0952 (0.1053)	
training:	Epoch: [19][59/233]	Loss 0.0816 (0.1049)	
training:	Epoch: [19][60/233]	Loss 0.1130 (0.1051)	
training:	Epoch: [19][61/233]	Loss 0.0889 (0.1048)	
training:	Epoch: [19][62/233]	Loss 0.1147 (0.1050)	
training:	Epoch: [19][63/233]	Loss 0.1041 (0.1049)	
training:	Epoch: [19][64/233]	Loss 0.1021 (0.1049)	
training:	Epoch: [19][65/233]	Loss 0.1259 (0.1052)	
training:	Epoch: [19][66/233]	Loss 0.1301 (0.1056)	
training:	Epoch: [19][67/233]	Loss 0.0947 (0.1054)	
training:	Epoch: [19][68/233]	Loss 0.0755 (0.1050)	
training:	Epoch: [19][69/233]	Loss 0.1050 (0.1050)	
training:	Epoch: [19][70/233]	Loss 0.0766 (0.1046)	
training:	Epoch: [19][71/233]	Loss 0.1447 (0.1052)	
training:	Epoch: [19][72/233]	Loss 0.0937 (0.1050)	
training:	Epoch: [19][73/233]	Loss 0.1102 (0.1051)	
training:	Epoch: [19][74/233]	Loss 0.1071 (0.1051)	
training:	Epoch: [19][75/233]	Loss 0.1513 (0.1057)	
training:	Epoch: [19][76/233]	Loss 0.0698 (0.1052)	
training:	Epoch: [19][77/233]	Loss 0.1176 (0.1054)	
training:	Epoch: [19][78/233]	Loss 0.1739 (0.1063)	
training:	Epoch: [19][79/233]	Loss 0.1011 (0.1062)	
training:	Epoch: [19][80/233]	Loss 0.1013 (0.1062)	
training:	Epoch: [19][81/233]	Loss 0.1034 (0.1061)	
training:	Epoch: [19][82/233]	Loss 0.1175 (0.1063)	
training:	Epoch: [19][83/233]	Loss 0.1211 (0.1064)	
training:	Epoch: [19][84/233]	Loss 0.0828 (0.1062)	
training:	Epoch: [19][85/233]	Loss 0.1090 (0.1062)	
training:	Epoch: [19][86/233]	Loss 0.0971 (0.1061)	
training:	Epoch: [19][87/233]	Loss 0.1434 (0.1065)	
training:	Epoch: [19][88/233]	Loss 0.1092 (0.1065)	
training:	Epoch: [19][89/233]	Loss 0.0965 (0.1064)	
training:	Epoch: [19][90/233]	Loss 0.0944 (0.1063)	
training:	Epoch: [19][91/233]	Loss 0.1023 (0.1063)	
training:	Epoch: [19][92/233]	Loss 0.1369 (0.1066)	
training:	Epoch: [19][93/233]	Loss 0.0972 (0.1065)	
training:	Epoch: [19][94/233]	Loss 0.0956 (0.1064)	
training:	Epoch: [19][95/233]	Loss 0.0980 (0.1063)	
training:	Epoch: [19][96/233]	Loss 0.1065 (0.1063)	
training:	Epoch: [19][97/233]	Loss 0.0805 (0.1060)	
training:	Epoch: [19][98/233]	Loss 0.0973 (0.1059)	
training:	Epoch: [19][99/233]	Loss 0.1408 (0.1063)	
training:	Epoch: [19][100/233]	Loss 0.1231 (0.1064)	
training:	Epoch: [19][101/233]	Loss 0.0863 (0.1062)	
training:	Epoch: [19][102/233]	Loss 0.1194 (0.1064)	
training:	Epoch: [19][103/233]	Loss 0.1061 (0.1064)	
training:	Epoch: [19][104/233]	Loss 0.1081 (0.1064)	
training:	Epoch: [19][105/233]	Loss 0.0840 (0.1062)	
training:	Epoch: [19][106/233]	Loss 0.0925 (0.1060)	
training:	Epoch: [19][107/233]	Loss 0.1128 (0.1061)	
training:	Epoch: [19][108/233]	Loss 0.0771 (0.1058)	
training:	Epoch: [19][109/233]	Loss 0.1083 (0.1059)	
training:	Epoch: [19][110/233]	Loss 0.0929 (0.1057)	
training:	Epoch: [19][111/233]	Loss 0.1025 (0.1057)	
training:	Epoch: [19][112/233]	Loss 0.0773 (0.1055)	
training:	Epoch: [19][113/233]	Loss 0.1216 (0.1056)	
training:	Epoch: [19][114/233]	Loss 0.0906 (0.1055)	
training:	Epoch: [19][115/233]	Loss 0.1104 (0.1055)	
training:	Epoch: [19][116/233]	Loss 0.0819 (0.1053)	
training:	Epoch: [19][117/233]	Loss 0.0815 (0.1051)	
training:	Epoch: [19][118/233]	Loss 0.0987 (0.1051)	
training:	Epoch: [19][119/233]	Loss 0.1054 (0.1051)	
training:	Epoch: [19][120/233]	Loss 0.0874 (0.1049)	
training:	Epoch: [19][121/233]	Loss 0.0838 (0.1047)	
training:	Epoch: [19][122/233]	Loss 0.0882 (0.1046)	
training:	Epoch: [19][123/233]	Loss 0.1024 (0.1046)	
training:	Epoch: [19][124/233]	Loss 0.0983 (0.1045)	
training:	Epoch: [19][125/233]	Loss 0.1056 (0.1045)	
training:	Epoch: [19][126/233]	Loss 0.1076 (0.1046)	
training:	Epoch: [19][127/233]	Loss 0.0996 (0.1045)	
training:	Epoch: [19][128/233]	Loss 0.0705 (0.1043)	
training:	Epoch: [19][129/233]	Loss 0.1150 (0.1043)	
training:	Epoch: [19][130/233]	Loss 0.1385 (0.1046)	
training:	Epoch: [19][131/233]	Loss 0.1192 (0.1047)	
training:	Epoch: [19][132/233]	Loss 0.0713 (0.1045)	
training:	Epoch: [19][133/233]	Loss 0.1448 (0.1048)	
training:	Epoch: [19][134/233]	Loss 0.1082 (0.1048)	
training:	Epoch: [19][135/233]	Loss 0.0901 (0.1047)	
training:	Epoch: [19][136/233]	Loss 0.0943 (0.1046)	
training:	Epoch: [19][137/233]	Loss 0.1026 (0.1046)	
training:	Epoch: [19][138/233]	Loss 0.0955 (0.1045)	
training:	Epoch: [19][139/233]	Loss 0.1023 (0.1045)	
training:	Epoch: [19][140/233]	Loss 0.1196 (0.1046)	
training:	Epoch: [19][141/233]	Loss 0.0989 (0.1046)	
training:	Epoch: [19][142/233]	Loss 0.1441 (0.1049)	
training:	Epoch: [19][143/233]	Loss 0.1076 (0.1049)	
training:	Epoch: [19][144/233]	Loss 0.1088 (0.1049)	
training:	Epoch: [19][145/233]	Loss 0.0750 (0.1047)	
training:	Epoch: [19][146/233]	Loss 0.0879 (0.1046)	
training:	Epoch: [19][147/233]	Loss 0.1478 (0.1049)	
training:	Epoch: [19][148/233]	Loss 0.1182 (0.1050)	
training:	Epoch: [19][149/233]	Loss 0.1287 (0.1051)	
training:	Epoch: [19][150/233]	Loss 0.1230 (0.1052)	
training:	Epoch: [19][151/233]	Loss 0.0965 (0.1052)	
training:	Epoch: [19][152/233]	Loss 0.1126 (0.1052)	
training:	Epoch: [19][153/233]	Loss 0.0907 (0.1051)	
training:	Epoch: [19][154/233]	Loss 0.1167 (0.1052)	
training:	Epoch: [19][155/233]	Loss 0.1013 (0.1052)	
training:	Epoch: [19][156/233]	Loss 0.1021 (0.1052)	
training:	Epoch: [19][157/233]	Loss 0.1128 (0.1052)	
training:	Epoch: [19][158/233]	Loss 0.1298 (0.1054)	
training:	Epoch: [19][159/233]	Loss 0.0878 (0.1053)	
training:	Epoch: [19][160/233]	Loss 0.1132 (0.1053)	
training:	Epoch: [19][161/233]	Loss 0.1060 (0.1053)	
training:	Epoch: [19][162/233]	Loss 0.0867 (0.1052)	
training:	Epoch: [19][163/233]	Loss 0.1237 (0.1053)	
training:	Epoch: [19][164/233]	Loss 0.0930 (0.1052)	
training:	Epoch: [19][165/233]	Loss 0.0873 (0.1051)	
training:	Epoch: [19][166/233]	Loss 0.1127 (0.1052)	
training:	Epoch: [19][167/233]	Loss 0.0867 (0.1051)	
training:	Epoch: [19][168/233]	Loss 0.0987 (0.1050)	
training:	Epoch: [19][169/233]	Loss 0.0791 (0.1049)	
training:	Epoch: [19][170/233]	Loss 0.1081 (0.1049)	
training:	Epoch: [19][171/233]	Loss 0.1048 (0.1049)	
training:	Epoch: [19][172/233]	Loss 0.1367 (0.1051)	
training:	Epoch: [19][173/233]	Loss 0.1064 (0.1051)	
training:	Epoch: [19][174/233]	Loss 0.1329 (0.1052)	
training:	Epoch: [19][175/233]	Loss 0.1002 (0.1052)	
training:	Epoch: [19][176/233]	Loss 0.1110 (0.1053)	
training:	Epoch: [19][177/233]	Loss 0.0818 (0.1051)	
training:	Epoch: [19][178/233]	Loss 0.1102 (0.1051)	
training:	Epoch: [19][179/233]	Loss 0.1264 (0.1053)	
training:	Epoch: [19][180/233]	Loss 0.0971 (0.1052)	
training:	Epoch: [19][181/233]	Loss 0.0936 (0.1052)	
training:	Epoch: [19][182/233]	Loss 0.0822 (0.1050)	
training:	Epoch: [19][183/233]	Loss 0.0773 (0.1049)	
training:	Epoch: [19][184/233]	Loss 0.1261 (0.1050)	
training:	Epoch: [19][185/233]	Loss 0.0785 (0.1049)	
training:	Epoch: [19][186/233]	Loss 0.1092 (0.1049)	
training:	Epoch: [19][187/233]	Loss 0.0989 (0.1048)	
training:	Epoch: [19][188/233]	Loss 0.1053 (0.1048)	
training:	Epoch: [19][189/233]	Loss 0.1070 (0.1049)	
training:	Epoch: [19][190/233]	Loss 0.1303 (0.1050)	
training:	Epoch: [19][191/233]	Loss 0.1286 (0.1051)	
training:	Epoch: [19][192/233]	Loss 0.0937 (0.1051)	
training:	Epoch: [19][193/233]	Loss 0.1045 (0.1051)	
training:	Epoch: [19][194/233]	Loss 0.1439 (0.1053)	
training:	Epoch: [19][195/233]	Loss 0.1415 (0.1054)	
training:	Epoch: [19][196/233]	Loss 0.0943 (0.1054)	
training:	Epoch: [19][197/233]	Loss 0.0992 (0.1053)	
training:	Epoch: [19][198/233]	Loss 0.1099 (0.1054)	
training:	Epoch: [19][199/233]	Loss 0.1051 (0.1054)	
training:	Epoch: [19][200/233]	Loss 0.1070 (0.1054)	
training:	Epoch: [19][201/233]	Loss 0.1247 (0.1055)	
training:	Epoch: [19][202/233]	Loss 0.1014 (0.1055)	
training:	Epoch: [19][203/233]	Loss 0.1014 (0.1054)	
training:	Epoch: [19][204/233]	Loss 0.0832 (0.1053)	
training:	Epoch: [19][205/233]	Loss 0.0859 (0.1052)	
training:	Epoch: [19][206/233]	Loss 0.1068 (0.1052)	
training:	Epoch: [19][207/233]	Loss 0.0935 (0.1052)	
training:	Epoch: [19][208/233]	Loss 0.1089 (0.1052)	
training:	Epoch: [19][209/233]	Loss 0.1159 (0.1053)	
training:	Epoch: [19][210/233]	Loss 0.0868 (0.1052)	
training:	Epoch: [19][211/233]	Loss 0.1066 (0.1052)	
training:	Epoch: [19][212/233]	Loss 0.0998 (0.1051)	
training:	Epoch: [19][213/233]	Loss 0.1104 (0.1052)	
training:	Epoch: [19][214/233]	Loss 0.1249 (0.1053)	
training:	Epoch: [19][215/233]	Loss 0.0952 (0.1052)	
training:	Epoch: [19][216/233]	Loss 0.1089 (0.1052)	
training:	Epoch: [19][217/233]	Loss 0.0869 (0.1051)	
training:	Epoch: [19][218/233]	Loss 0.0922 (0.1051)	
training:	Epoch: [19][219/233]	Loss 0.1035 (0.1051)	
training:	Epoch: [19][220/233]	Loss 0.0845 (0.1050)	
training:	Epoch: [19][221/233]	Loss 0.0994 (0.1050)	
training:	Epoch: [19][222/233]	Loss 0.0839 (0.1049)	
training:	Epoch: [19][223/233]	Loss 0.0735 (0.1047)	
training:	Epoch: [19][224/233]	Loss 0.0863 (0.1046)	
training:	Epoch: [19][225/233]	Loss 0.1097 (0.1047)	
training:	Epoch: [19][226/233]	Loss 0.0909 (0.1046)	
training:	Epoch: [19][227/233]	Loss 0.1460 (0.1048)	
training:	Epoch: [19][228/233]	Loss 0.0769 (0.1047)	
training:	Epoch: [19][229/233]	Loss 0.1052 (0.1047)	
training:	Epoch: [19][230/233]	Loss 0.0769 (0.1045)	
training:	Epoch: [19][231/233]	Loss 0.1626 (0.1048)	
training:	Epoch: [19][232/233]	Loss 0.1424 (0.1050)	
training:	Epoch: [19][233/233]	Loss 0.0957 (0.1049)	
Training:	 Loss: 0.1047

Training:	 ACC: 0.9999 0.9999 0.9997 1.0000
Validation:	 ACC: 0.8647 0.8646 0.8639 0.8655
Validation:	 Best_BACC: 0.8647 0.8646 0.8639 0.8655
Validation:	 Loss: 0.3304
Pretraining:	Epoch 20/200
----------
training:	Epoch: [20][1/233]	Loss 0.1038 (0.1038)	
training:	Epoch: [20][2/233]	Loss 0.1003 (0.1021)	
training:	Epoch: [20][3/233]	Loss 0.0799 (0.0947)	
training:	Epoch: [20][4/233]	Loss 0.1025 (0.0966)	
training:	Epoch: [20][5/233]	Loss 0.0913 (0.0956)	
training:	Epoch: [20][6/233]	Loss 0.1571 (0.1058)	
training:	Epoch: [20][7/233]	Loss 0.0878 (0.1032)	
training:	Epoch: [20][8/233]	Loss 0.1074 (0.1038)	
training:	Epoch: [20][9/233]	Loss 0.0849 (0.1017)	
training:	Epoch: [20][10/233]	Loss 0.0762 (0.0991)	
training:	Epoch: [20][11/233]	Loss 0.0865 (0.0980)	
training:	Epoch: [20][12/233]	Loss 0.1154 (0.0994)	
training:	Epoch: [20][13/233]	Loss 0.0891 (0.0986)	
training:	Epoch: [20][14/233]	Loss 0.1015 (0.0988)	
training:	Epoch: [20][15/233]	Loss 0.1121 (0.0997)	
training:	Epoch: [20][16/233]	Loss 0.1167 (0.1008)	
training:	Epoch: [20][17/233]	Loss 0.0917 (0.1002)	
training:	Epoch: [20][18/233]	Loss 0.1134 (0.1010)	
training:	Epoch: [20][19/233]	Loss 0.1556 (0.1039)	
training:	Epoch: [20][20/233]	Loss 0.0973 (0.1035)	
training:	Epoch: [20][21/233]	Loss 0.0971 (0.1032)	
training:	Epoch: [20][22/233]	Loss 0.0933 (0.1028)	
training:	Epoch: [20][23/233]	Loss 0.1436 (0.1045)	
training:	Epoch: [20][24/233]	Loss 0.0776 (0.1034)	
training:	Epoch: [20][25/233]	Loss 0.0845 (0.1027)	
training:	Epoch: [20][26/233]	Loss 0.1920 (0.1061)	
training:	Epoch: [20][27/233]	Loss 0.1258 (0.1068)	
training:	Epoch: [20][28/233]	Loss 0.0890 (0.1062)	
training:	Epoch: [20][29/233]	Loss 0.0875 (0.1056)	
training:	Epoch: [20][30/233]	Loss 0.0887 (0.1050)	
training:	Epoch: [20][31/233]	Loss 0.0865 (0.1044)	
training:	Epoch: [20][32/233]	Loss 0.0737 (0.1034)	
training:	Epoch: [20][33/233]	Loss 0.0890 (0.1030)	
training:	Epoch: [20][34/233]	Loss 0.1035 (0.1030)	
training:	Epoch: [20][35/233]	Loss 0.0844 (0.1025)	
training:	Epoch: [20][36/233]	Loss 0.1166 (0.1029)	
training:	Epoch: [20][37/233]	Loss 0.1156 (0.1032)	
training:	Epoch: [20][38/233]	Loss 0.0935 (0.1030)	
training:	Epoch: [20][39/233]	Loss 0.0851 (0.1025)	
training:	Epoch: [20][40/233]	Loss 0.0870 (0.1021)	
training:	Epoch: [20][41/233]	Loss 0.0968 (0.1020)	
training:	Epoch: [20][42/233]	Loss 0.0828 (0.1015)	
training:	Epoch: [20][43/233]	Loss 0.1280 (0.1021)	
training:	Epoch: [20][44/233]	Loss 0.1362 (0.1029)	
training:	Epoch: [20][45/233]	Loss 0.1003 (0.1029)	
training:	Epoch: [20][46/233]	Loss 0.1068 (0.1029)	
training:	Epoch: [20][47/233]	Loss 0.1540 (0.1040)	
training:	Epoch: [20][48/233]	Loss 0.0890 (0.1037)	
training:	Epoch: [20][49/233]	Loss 0.0846 (0.1033)	
training:	Epoch: [20][50/233]	Loss 0.1014 (0.1033)	
training:	Epoch: [20][51/233]	Loss 0.1108 (0.1034)	
training:	Epoch: [20][52/233]	Loss 0.1005 (0.1034)	
training:	Epoch: [20][53/233]	Loss 0.1131 (0.1036)	
training:	Epoch: [20][54/233]	Loss 0.0927 (0.1034)	
training:	Epoch: [20][55/233]	Loss 0.0924 (0.1032)	
training:	Epoch: [20][56/233]	Loss 0.0895 (0.1029)	
training:	Epoch: [20][57/233]	Loss 0.0847 (0.1026)	
training:	Epoch: [20][58/233]	Loss 0.0957 (0.1025)	
training:	Epoch: [20][59/233]	Loss 0.0875 (0.1022)	
training:	Epoch: [20][60/233]	Loss 0.0836 (0.1019)	
training:	Epoch: [20][61/233]	Loss 0.1974 (0.1035)	
training:	Epoch: [20][62/233]	Loss 0.0959 (0.1034)	
training:	Epoch: [20][63/233]	Loss 0.0780 (0.1030)	
training:	Epoch: [20][64/233]	Loss 0.0961 (0.1028)	
training:	Epoch: [20][65/233]	Loss 0.0832 (0.1025)	
training:	Epoch: [20][66/233]	Loss 0.0903 (0.1024)	
training:	Epoch: [20][67/233]	Loss 0.0890 (0.1022)	
training:	Epoch: [20][68/233]	Loss 0.0872 (0.1019)	
training:	Epoch: [20][69/233]	Loss 0.0908 (0.1018)	
training:	Epoch: [20][70/233]	Loss 0.1111 (0.1019)	
training:	Epoch: [20][71/233]	Loss 0.0833 (0.1016)	
training:	Epoch: [20][72/233]	Loss 0.1227 (0.1019)	
training:	Epoch: [20][73/233]	Loss 0.1811 (0.1030)	
training:	Epoch: [20][74/233]	Loss 0.0810 (0.1027)	
training:	Epoch: [20][75/233]	Loss 0.0921 (0.1026)	
training:	Epoch: [20][76/233]	Loss 0.0798 (0.1023)	
training:	Epoch: [20][77/233]	Loss 0.0848 (0.1021)	
training:	Epoch: [20][78/233]	Loss 0.1046 (0.1021)	
training:	Epoch: [20][79/233]	Loss 0.0750 (0.1017)	
training:	Epoch: [20][80/233]	Loss 0.1399 (0.1022)	
training:	Epoch: [20][81/233]	Loss 0.0948 (0.1021)	
training:	Epoch: [20][82/233]	Loss 0.0912 (0.1020)	
training:	Epoch: [20][83/233]	Loss 0.0935 (0.1019)	
training:	Epoch: [20][84/233]	Loss 0.0874 (0.1017)	
training:	Epoch: [20][85/233]	Loss 0.1053 (0.1018)	
training:	Epoch: [20][86/233]	Loss 0.0837 (0.1016)	
training:	Epoch: [20][87/233]	Loss 0.1116 (0.1017)	
training:	Epoch: [20][88/233]	Loss 0.1052 (0.1017)	
training:	Epoch: [20][89/233]	Loss 0.0860 (0.1015)	
training:	Epoch: [20][90/233]	Loss 0.0889 (0.1014)	
training:	Epoch: [20][91/233]	Loss 0.0759 (0.1011)	
training:	Epoch: [20][92/233]	Loss 0.0892 (0.1010)	
training:	Epoch: [20][93/233]	Loss 0.0931 (0.1009)	
training:	Epoch: [20][94/233]	Loss 0.1345 (0.1013)	
training:	Epoch: [20][95/233]	Loss 0.0707 (0.1009)	
training:	Epoch: [20][96/233]	Loss 0.0992 (0.1009)	
training:	Epoch: [20][97/233]	Loss 0.0854 (0.1008)	
training:	Epoch: [20][98/233]	Loss 0.1020 (0.1008)	
training:	Epoch: [20][99/233]	Loss 0.0905 (0.1007)	
training:	Epoch: [20][100/233]	Loss 0.0785 (0.1004)	
training:	Epoch: [20][101/233]	Loss 0.0775 (0.1002)	
training:	Epoch: [20][102/233]	Loss 0.1100 (0.1003)	
training:	Epoch: [20][103/233]	Loss 0.0765 (0.1001)	
training:	Epoch: [20][104/233]	Loss 0.1075 (0.1002)	
training:	Epoch: [20][105/233]	Loss 0.0906 (0.1001)	
training:	Epoch: [20][106/233]	Loss 0.1116 (0.1002)	
training:	Epoch: [20][107/233]	Loss 0.0675 (0.0999)	
training:	Epoch: [20][108/233]	Loss 0.0838 (0.0997)	
training:	Epoch: [20][109/233]	Loss 0.0977 (0.0997)	
training:	Epoch: [20][110/233]	Loss 0.1285 (0.1000)	
training:	Epoch: [20][111/233]	Loss 0.0840 (0.0998)	
training:	Epoch: [20][112/233]	Loss 0.0623 (0.0995)	
training:	Epoch: [20][113/233]	Loss 0.0715 (0.0992)	
training:	Epoch: [20][114/233]	Loss 0.1206 (0.0994)	
training:	Epoch: [20][115/233]	Loss 0.0995 (0.0994)	
training:	Epoch: [20][116/233]	Loss 0.1323 (0.0997)	
training:	Epoch: [20][117/233]	Loss 0.0835 (0.0996)	
training:	Epoch: [20][118/233]	Loss 0.0891 (0.0995)	
training:	Epoch: [20][119/233]	Loss 0.1219 (0.0997)	
training:	Epoch: [20][120/233]	Loss 0.0912 (0.0996)	
training:	Epoch: [20][121/233]	Loss 0.0772 (0.0994)	
training:	Epoch: [20][122/233]	Loss 0.1019 (0.0994)	
training:	Epoch: [20][123/233]	Loss 0.0892 (0.0993)	
training:	Epoch: [20][124/233]	Loss 0.0904 (0.0993)	
training:	Epoch: [20][125/233]	Loss 0.1441 (0.0996)	
training:	Epoch: [20][126/233]	Loss 0.1380 (0.0999)	
training:	Epoch: [20][127/233]	Loss 0.0907 (0.0999)	
training:	Epoch: [20][128/233]	Loss 0.0912 (0.0998)	
training:	Epoch: [20][129/233]	Loss 0.0820 (0.0997)	
training:	Epoch: [20][130/233]	Loss 0.0727 (0.0995)	
training:	Epoch: [20][131/233]	Loss 0.1274 (0.0997)	
training:	Epoch: [20][132/233]	Loss 0.0822 (0.0995)	
training:	Epoch: [20][133/233]	Loss 0.0876 (0.0994)	
training:	Epoch: [20][134/233]	Loss 0.0910 (0.0994)	
training:	Epoch: [20][135/233]	Loss 0.0887 (0.0993)	
training:	Epoch: [20][136/233]	Loss 0.1126 (0.0994)	
training:	Epoch: [20][137/233]	Loss 0.0874 (0.0993)	
training:	Epoch: [20][138/233]	Loss 0.1008 (0.0993)	
training:	Epoch: [20][139/233]	Loss 0.0886 (0.0992)	
training:	Epoch: [20][140/233]	Loss 0.1058 (0.0993)	
training:	Epoch: [20][141/233]	Loss 0.0861 (0.0992)	
training:	Epoch: [20][142/233]	Loss 0.1182 (0.0993)	
training:	Epoch: [20][143/233]	Loss 0.0883 (0.0993)	
training:	Epoch: [20][144/233]	Loss 0.0901 (0.0992)	
training:	Epoch: [20][145/233]	Loss 0.1169 (0.0993)	
training:	Epoch: [20][146/233]	Loss 0.1105 (0.0994)	
training:	Epoch: [20][147/233]	Loss 0.1033 (0.0994)	
training:	Epoch: [20][148/233]	Loss 0.0984 (0.0994)	
training:	Epoch: [20][149/233]	Loss 0.0974 (0.0994)	
training:	Epoch: [20][150/233]	Loss 0.1165 (0.0995)	
training:	Epoch: [20][151/233]	Loss 0.0728 (0.0993)	
training:	Epoch: [20][152/233]	Loss 0.0829 (0.0992)	
training:	Epoch: [20][153/233]	Loss 0.0876 (0.0992)	
training:	Epoch: [20][154/233]	Loss 0.0952 (0.0991)	
training:	Epoch: [20][155/233]	Loss 0.1268 (0.0993)	
training:	Epoch: [20][156/233]	Loss 0.0914 (0.0993)	
training:	Epoch: [20][157/233]	Loss 0.1082 (0.0993)	
training:	Epoch: [20][158/233]	Loss 0.0997 (0.0993)	
training:	Epoch: [20][159/233]	Loss 0.1306 (0.0995)	
training:	Epoch: [20][160/233]	Loss 0.1227 (0.0997)	
training:	Epoch: [20][161/233]	Loss 0.0878 (0.0996)	
training:	Epoch: [20][162/233]	Loss 0.1274 (0.0998)	
training:	Epoch: [20][163/233]	Loss 0.0949 (0.0997)	
training:	Epoch: [20][164/233]	Loss 0.1060 (0.0998)	
training:	Epoch: [20][165/233]	Loss 0.0782 (0.0996)	
training:	Epoch: [20][166/233]	Loss 0.0967 (0.0996)	
training:	Epoch: [20][167/233]	Loss 0.1266 (0.0998)	
training:	Epoch: [20][168/233]	Loss 0.1022 (0.0998)	
training:	Epoch: [20][169/233]	Loss 0.0953 (0.0998)	
training:	Epoch: [20][170/233]	Loss 0.1377 (0.1000)	
training:	Epoch: [20][171/233]	Loss 0.1259 (0.1001)	
training:	Epoch: [20][172/233]	Loss 0.0857 (0.1001)	
training:	Epoch: [20][173/233]	Loss 0.1310 (0.1002)	
training:	Epoch: [20][174/233]	Loss 0.1777 (0.1007)	
training:	Epoch: [20][175/233]	Loss 0.0918 (0.1006)	
training:	Epoch: [20][176/233]	Loss 0.2527 (0.1015)	
training:	Epoch: [20][177/233]	Loss 0.1250 (0.1016)	
training:	Epoch: [20][178/233]	Loss 0.0732 (0.1015)	
training:	Epoch: [20][179/233]	Loss 0.1206 (0.1016)	
training:	Epoch: [20][180/233]	Loss 0.1621 (0.1019)	
training:	Epoch: [20][181/233]	Loss 0.0640 (0.1017)	
training:	Epoch: [20][182/233]	Loss 0.0926 (0.1016)	
training:	Epoch: [20][183/233]	Loss 0.0816 (0.1015)	
training:	Epoch: [20][184/233]	Loss 0.0947 (0.1015)	
training:	Epoch: [20][185/233]	Loss 0.0865 (0.1014)	
training:	Epoch: [20][186/233]	Loss 0.1253 (0.1015)	
training:	Epoch: [20][187/233]	Loss 0.1287 (0.1017)	
training:	Epoch: [20][188/233]	Loss 0.0918 (0.1016)	
training:	Epoch: [20][189/233]	Loss 0.0951 (0.1016)	
training:	Epoch: [20][190/233]	Loss 0.0802 (0.1015)	
training:	Epoch: [20][191/233]	Loss 0.1011 (0.1015)	
training:	Epoch: [20][192/233]	Loss 0.1201 (0.1016)	
training:	Epoch: [20][193/233]	Loss 0.1752 (0.1020)	
training:	Epoch: [20][194/233]	Loss 0.0793 (0.1019)	
training:	Epoch: [20][195/233]	Loss 0.0989 (0.1018)	
training:	Epoch: [20][196/233]	Loss 0.0899 (0.1018)	
training:	Epoch: [20][197/233]	Loss 0.1079 (0.1018)	
training:	Epoch: [20][198/233]	Loss 0.0953 (0.1018)	
training:	Epoch: [20][199/233]	Loss 0.0803 (0.1017)	
training:	Epoch: [20][200/233]	Loss 0.0926 (0.1016)	
training:	Epoch: [20][201/233]	Loss 0.2142 (0.1022)	
training:	Epoch: [20][202/233]	Loss 0.0845 (0.1021)	
training:	Epoch: [20][203/233]	Loss 0.0953 (0.1021)	
training:	Epoch: [20][204/233]	Loss 0.1153 (0.1021)	
training:	Epoch: [20][205/233]	Loss 0.1061 (0.1021)	
training:	Epoch: [20][206/233]	Loss 0.1052 (0.1022)	
training:	Epoch: [20][207/233]	Loss 0.1224 (0.1023)	
training:	Epoch: [20][208/233]	Loss 0.0849 (0.1022)	
training:	Epoch: [20][209/233]	Loss 0.0853 (0.1021)	
training:	Epoch: [20][210/233]	Loss 0.0851 (0.1020)	
training:	Epoch: [20][211/233]	Loss 0.1180 (0.1021)	
training:	Epoch: [20][212/233]	Loss 0.1388 (0.1023)	
training:	Epoch: [20][213/233]	Loss 0.1446 (0.1025)	
training:	Epoch: [20][214/233]	Loss 0.1106 (0.1025)	
training:	Epoch: [20][215/233]	Loss 0.0884 (0.1024)	
training:	Epoch: [20][216/233]	Loss 0.1366 (0.1026)	
training:	Epoch: [20][217/233]	Loss 0.0839 (0.1025)	
training:	Epoch: [20][218/233]	Loss 0.0931 (0.1025)	
training:	Epoch: [20][219/233]	Loss 0.0988 (0.1024)	
training:	Epoch: [20][220/233]	Loss 0.1385 (0.1026)	
training:	Epoch: [20][221/233]	Loss 0.1245 (0.1027)	
training:	Epoch: [20][222/233]	Loss 0.1399 (0.1029)	
training:	Epoch: [20][223/233]	Loss 0.0904 (0.1028)	
training:	Epoch: [20][224/233]	Loss 0.0979 (0.1028)	
training:	Epoch: [20][225/233]	Loss 0.0931 (0.1028)	
training:	Epoch: [20][226/233]	Loss 0.1346 (0.1029)	
training:	Epoch: [20][227/233]	Loss 0.0795 (0.1028)	
training:	Epoch: [20][228/233]	Loss 0.0854 (0.1027)	
training:	Epoch: [20][229/233]	Loss 0.0907 (0.1027)	
training:	Epoch: [20][230/233]	Loss 0.0763 (0.1025)	
training:	Epoch: [20][231/233]	Loss 0.0932 (0.1025)	
training:	Epoch: [20][232/233]	Loss 0.1110 (0.1025)	
training:	Epoch: [20][233/233]	Loss 0.1129 (0.1026)	
Training:	 Loss: 0.1024

Training:	 ACC: 0.9999 0.9999 0.9997 1.0000
Validation:	 ACC: 0.8616 0.8604 0.8342 0.8890
Validation:	 Best_BACC: 0.8647 0.8646 0.8639 0.8655
Validation:	 Loss: 0.3362
Pretraining:	Epoch 21/200
----------
training:	Epoch: [21][1/233]	Loss 0.0931 (0.0931)	
training:	Epoch: [21][2/233]	Loss 0.1151 (0.1041)	
training:	Epoch: [21][3/233]	Loss 0.1015 (0.1032)	
training:	Epoch: [21][4/233]	Loss 0.0762 (0.0965)	
training:	Epoch: [21][5/233]	Loss 0.0669 (0.0906)	
training:	Epoch: [21][6/233]	Loss 0.0957 (0.0914)	
training:	Epoch: [21][7/233]	Loss 0.1709 (0.1028)	
training:	Epoch: [21][8/233]	Loss 0.0810 (0.1001)	
training:	Epoch: [21][9/233]	Loss 0.0662 (0.0963)	
training:	Epoch: [21][10/233]	Loss 0.0813 (0.0948)	
training:	Epoch: [21][11/233]	Loss 0.1026 (0.0955)	
training:	Epoch: [21][12/233]	Loss 0.0889 (0.0949)	
training:	Epoch: [21][13/233]	Loss 0.0897 (0.0945)	
training:	Epoch: [21][14/233]	Loss 0.0738 (0.0931)	
training:	Epoch: [21][15/233]	Loss 0.1118 (0.0943)	
training:	Epoch: [21][16/233]	Loss 0.1621 (0.0985)	
training:	Epoch: [21][17/233]	Loss 0.0808 (0.0975)	
training:	Epoch: [21][18/233]	Loss 0.1077 (0.0981)	
training:	Epoch: [21][19/233]	Loss 0.0882 (0.0975)	
training:	Epoch: [21][20/233]	Loss 0.0866 (0.0970)	
training:	Epoch: [21][21/233]	Loss 0.0925 (0.0968)	
training:	Epoch: [21][22/233]	Loss 0.0919 (0.0966)	
training:	Epoch: [21][23/233]	Loss 0.0828 (0.0960)	
training:	Epoch: [21][24/233]	Loss 0.0957 (0.0959)	
training:	Epoch: [21][25/233]	Loss 0.0853 (0.0955)	
training:	Epoch: [21][26/233]	Loss 0.0912 (0.0954)	
training:	Epoch: [21][27/233]	Loss 0.0789 (0.0947)	
training:	Epoch: [21][28/233]	Loss 0.0829 (0.0943)	
training:	Epoch: [21][29/233]	Loss 0.0841 (0.0940)	
training:	Epoch: [21][30/233]	Loss 0.0928 (0.0939)	
training:	Epoch: [21][31/233]	Loss 0.1047 (0.0943)	
training:	Epoch: [21][32/233]	Loss 0.0707 (0.0935)	
training:	Epoch: [21][33/233]	Loss 0.0712 (0.0929)	
training:	Epoch: [21][34/233]	Loss 0.1118 (0.0934)	
training:	Epoch: [21][35/233]	Loss 0.0973 (0.0935)	
training:	Epoch: [21][36/233]	Loss 0.0763 (0.0931)	
training:	Epoch: [21][37/233]	Loss 0.0779 (0.0926)	
training:	Epoch: [21][38/233]	Loss 0.1007 (0.0929)	
training:	Epoch: [21][39/233]	Loss 0.0829 (0.0926)	
training:	Epoch: [21][40/233]	Loss 0.0830 (0.0924)	
training:	Epoch: [21][41/233]	Loss 0.0908 (0.0923)	
training:	Epoch: [21][42/233]	Loss 0.0824 (0.0921)	
training:	Epoch: [21][43/233]	Loss 0.1075 (0.0924)	
training:	Epoch: [21][44/233]	Loss 0.0987 (0.0926)	
training:	Epoch: [21][45/233]	Loss 0.0822 (0.0924)	
training:	Epoch: [21][46/233]	Loss 0.0900 (0.0923)	
training:	Epoch: [21][47/233]	Loss 0.1091 (0.0927)	
training:	Epoch: [21][48/233]	Loss 0.1277 (0.0934)	
training:	Epoch: [21][49/233]	Loss 0.0878 (0.0933)	
training:	Epoch: [21][50/233]	Loss 0.0873 (0.0932)	
training:	Epoch: [21][51/233]	Loss 0.1019 (0.0933)	
training:	Epoch: [21][52/233]	Loss 0.0760 (0.0930)	
training:	Epoch: [21][53/233]	Loss 0.1208 (0.0935)	
training:	Epoch: [21][54/233]	Loss 0.0760 (0.0932)	
training:	Epoch: [21][55/233]	Loss 0.0888 (0.0931)	
training:	Epoch: [21][56/233]	Loss 0.0811 (0.0929)	
training:	Epoch: [21][57/233]	Loss 0.0856 (0.0928)	
training:	Epoch: [21][58/233]	Loss 0.0930 (0.0928)	
training:	Epoch: [21][59/233]	Loss 0.0740 (0.0925)	
training:	Epoch: [21][60/233]	Loss 0.0756 (0.0922)	
training:	Epoch: [21][61/233]	Loss 0.1038 (0.0924)	
training:	Epoch: [21][62/233]	Loss 0.1467 (0.0932)	
training:	Epoch: [21][63/233]	Loss 0.1059 (0.0934)	
training:	Epoch: [21][64/233]	Loss 0.0936 (0.0934)	
training:	Epoch: [21][65/233]	Loss 0.1047 (0.0936)	
training:	Epoch: [21][66/233]	Loss 0.1498 (0.0945)	
training:	Epoch: [21][67/233]	Loss 0.1009 (0.0946)	
training:	Epoch: [21][68/233]	Loss 0.0617 (0.0941)	
training:	Epoch: [21][69/233]	Loss 0.0964 (0.0941)	
training:	Epoch: [21][70/233]	Loss 0.0708 (0.0938)	
training:	Epoch: [21][71/233]	Loss 0.1581 (0.0947)	
training:	Epoch: [21][72/233]	Loss 0.0963 (0.0947)	
training:	Epoch: [21][73/233]	Loss 0.0814 (0.0945)	
training:	Epoch: [21][74/233]	Loss 0.0770 (0.0943)	
training:	Epoch: [21][75/233]	Loss 0.0892 (0.0942)	
training:	Epoch: [21][76/233]	Loss 0.0788 (0.0940)	
training:	Epoch: [21][77/233]	Loss 0.0784 (0.0938)	
training:	Epoch: [21][78/233]	Loss 0.0967 (0.0939)	
training:	Epoch: [21][79/233]	Loss 0.0861 (0.0938)	
training:	Epoch: [21][80/233]	Loss 0.0695 (0.0935)	
training:	Epoch: [21][81/233]	Loss 0.0872 (0.0934)	
training:	Epoch: [21][82/233]	Loss 0.1016 (0.0935)	
training:	Epoch: [21][83/233]	Loss 0.1000 (0.0936)	
training:	Epoch: [21][84/233]	Loss 0.1043 (0.0937)	
training:	Epoch: [21][85/233]	Loss 0.1001 (0.0938)	
training:	Epoch: [21][86/233]	Loss 0.0825 (0.0936)	
training:	Epoch: [21][87/233]	Loss 0.0860 (0.0935)	
training:	Epoch: [21][88/233]	Loss 0.0821 (0.0934)	
training:	Epoch: [21][89/233]	Loss 0.0935 (0.0934)	
training:	Epoch: [21][90/233]	Loss 0.0999 (0.0935)	
training:	Epoch: [21][91/233]	Loss 0.1382 (0.0940)	
training:	Epoch: [21][92/233]	Loss 0.0873 (0.0939)	
training:	Epoch: [21][93/233]	Loss 0.0893 (0.0939)	
training:	Epoch: [21][94/233]	Loss 0.1446 (0.0944)	
training:	Epoch: [21][95/233]	Loss 0.0758 (0.0942)	
training:	Epoch: [21][96/233]	Loss 0.1259 (0.0945)	
training:	Epoch: [21][97/233]	Loss 0.1175 (0.0948)	
training:	Epoch: [21][98/233]	Loss 0.0834 (0.0946)	
training:	Epoch: [21][99/233]	Loss 0.1003 (0.0947)	
training:	Epoch: [21][100/233]	Loss 0.1202 (0.0950)	
training:	Epoch: [21][101/233]	Loss 0.0923 (0.0949)	
training:	Epoch: [21][102/233]	Loss 0.1047 (0.0950)	
training:	Epoch: [21][103/233]	Loss 0.0751 (0.0948)	
training:	Epoch: [21][104/233]	Loss 0.0713 (0.0946)	
training:	Epoch: [21][105/233]	Loss 0.0926 (0.0946)	
training:	Epoch: [21][106/233]	Loss 0.0974 (0.0946)	
training:	Epoch: [21][107/233]	Loss 0.0809 (0.0945)	
training:	Epoch: [21][108/233]	Loss 0.0985 (0.0945)	
training:	Epoch: [21][109/233]	Loss 0.1122 (0.0947)	
training:	Epoch: [21][110/233]	Loss 0.1081 (0.0948)	
training:	Epoch: [21][111/233]	Loss 0.0799 (0.0947)	
training:	Epoch: [21][112/233]	Loss 0.0870 (0.0946)	
training:	Epoch: [21][113/233]	Loss 0.0618 (0.0943)	
training:	Epoch: [21][114/233]	Loss 0.0793 (0.0942)	
training:	Epoch: [21][115/233]	Loss 0.0757 (0.0940)	
training:	Epoch: [21][116/233]	Loss 0.0839 (0.0939)	
training:	Epoch: [21][117/233]	Loss 0.0834 (0.0938)	
training:	Epoch: [21][118/233]	Loss 0.0693 (0.0936)	
training:	Epoch: [21][119/233]	Loss 0.0945 (0.0936)	
training:	Epoch: [21][120/233]	Loss 0.0787 (0.0935)	
training:	Epoch: [21][121/233]	Loss 0.1087 (0.0936)	
training:	Epoch: [21][122/233]	Loss 0.0673 (0.0934)	
training:	Epoch: [21][123/233]	Loss 0.0864 (0.0934)	
training:	Epoch: [21][124/233]	Loss 0.1283 (0.0937)	
training:	Epoch: [21][125/233]	Loss 0.1079 (0.0938)	
training:	Epoch: [21][126/233]	Loss 0.0860 (0.0937)	
training:	Epoch: [21][127/233]	Loss 0.0724 (0.0935)	
training:	Epoch: [21][128/233]	Loss 0.1091 (0.0937)	
training:	Epoch: [21][129/233]	Loss 0.0656 (0.0934)	
training:	Epoch: [21][130/233]	Loss 0.1137 (0.0936)	
training:	Epoch: [21][131/233]	Loss 0.0940 (0.0936)	
training:	Epoch: [21][132/233]	Loss 0.0731 (0.0934)	
training:	Epoch: [21][133/233]	Loss 0.0865 (0.0934)	
training:	Epoch: [21][134/233]	Loss 0.0875 (0.0934)	
training:	Epoch: [21][135/233]	Loss 0.0768 (0.0932)	
training:	Epoch: [21][136/233]	Loss 0.0750 (0.0931)	
training:	Epoch: [21][137/233]	Loss 0.0897 (0.0931)	
training:	Epoch: [21][138/233]	Loss 0.0850 (0.0930)	
training:	Epoch: [21][139/233]	Loss 0.0791 (0.0929)	
training:	Epoch: [21][140/233]	Loss 0.0760 (0.0928)	
training:	Epoch: [21][141/233]	Loss 0.1947 (0.0935)	
training:	Epoch: [21][142/233]	Loss 0.0845 (0.0935)	
training:	Epoch: [21][143/233]	Loss 0.0794 (0.0934)	
training:	Epoch: [21][144/233]	Loss 0.1174 (0.0935)	
training:	Epoch: [21][145/233]	Loss 0.1031 (0.0936)	
training:	Epoch: [21][146/233]	Loss 0.1244 (0.0938)	
training:	Epoch: [21][147/233]	Loss 0.0854 (0.0937)	
training:	Epoch: [21][148/233]	Loss 0.0919 (0.0937)	
training:	Epoch: [21][149/233]	Loss 0.0860 (0.0937)	
training:	Epoch: [21][150/233]	Loss 0.0918 (0.0937)	
training:	Epoch: [21][151/233]	Loss 0.0894 (0.0936)	
training:	Epoch: [21][152/233]	Loss 0.1210 (0.0938)	
training:	Epoch: [21][153/233]	Loss 0.0867 (0.0938)	
training:	Epoch: [21][154/233]	Loss 0.0745 (0.0936)	
training:	Epoch: [21][155/233]	Loss 0.0886 (0.0936)	
training:	Epoch: [21][156/233]	Loss 0.0805 (0.0935)	
training:	Epoch: [21][157/233]	Loss 0.1118 (0.0936)	
training:	Epoch: [21][158/233]	Loss 0.0759 (0.0935)	
training:	Epoch: [21][159/233]	Loss 0.0833 (0.0935)	
training:	Epoch: [21][160/233]	Loss 0.0894 (0.0934)	
training:	Epoch: [21][161/233]	Loss 0.0972 (0.0935)	
training:	Epoch: [21][162/233]	Loss 0.0846 (0.0934)	
training:	Epoch: [21][163/233]	Loss 0.0881 (0.0934)	
training:	Epoch: [21][164/233]	Loss 0.0720 (0.0932)	
training:	Epoch: [21][165/233]	Loss 0.1079 (0.0933)	
training:	Epoch: [21][166/233]	Loss 0.0939 (0.0933)	
training:	Epoch: [21][167/233]	Loss 0.0888 (0.0933)	
training:	Epoch: [21][168/233]	Loss 0.1051 (0.0934)	
training:	Epoch: [21][169/233]	Loss 0.0759 (0.0933)	
training:	Epoch: [21][170/233]	Loss 0.0857 (0.0932)	
training:	Epoch: [21][171/233]	Loss 0.0814 (0.0932)	
training:	Epoch: [21][172/233]	Loss 0.0905 (0.0931)	
training:	Epoch: [21][173/233]	Loss 0.0960 (0.0932)	
training:	Epoch: [21][174/233]	Loss 0.0883 (0.0931)	
training:	Epoch: [21][175/233]	Loss 0.0927 (0.0931)	
training:	Epoch: [21][176/233]	Loss 0.1707 (0.0936)	
training:	Epoch: [21][177/233]	Loss 0.1051 (0.0936)	
training:	Epoch: [21][178/233]	Loss 0.1082 (0.0937)	
training:	Epoch: [21][179/233]	Loss 0.1056 (0.0938)	
training:	Epoch: [21][180/233]	Loss 0.0994 (0.0938)	
training:	Epoch: [21][181/233]	Loss 0.0845 (0.0938)	
training:	Epoch: [21][182/233]	Loss 0.0968 (0.0938)	
training:	Epoch: [21][183/233]	Loss 0.0966 (0.0938)	
training:	Epoch: [21][184/233]	Loss 0.0877 (0.0938)	
training:	Epoch: [21][185/233]	Loss 0.0825 (0.0937)	
training:	Epoch: [21][186/233]	Loss 0.0827 (0.0936)	
training:	Epoch: [21][187/233]	Loss 0.0988 (0.0937)	
training:	Epoch: [21][188/233]	Loss 0.0774 (0.0936)	
training:	Epoch: [21][189/233]	Loss 0.0854 (0.0935)	
training:	Epoch: [21][190/233]	Loss 0.1303 (0.0937)	
training:	Epoch: [21][191/233]	Loss 0.0848 (0.0937)	
training:	Epoch: [21][192/233]	Loss 0.1515 (0.0940)	
training:	Epoch: [21][193/233]	Loss 0.0951 (0.0940)	
training:	Epoch: [21][194/233]	Loss 0.0783 (0.0939)	
training:	Epoch: [21][195/233]	Loss 0.0767 (0.0938)	
training:	Epoch: [21][196/233]	Loss 0.0830 (0.0938)	
training:	Epoch: [21][197/233]	Loss 0.0847 (0.0937)	
training:	Epoch: [21][198/233]	Loss 0.0803 (0.0937)	
training:	Epoch: [21][199/233]	Loss 0.1192 (0.0938)	
training:	Epoch: [21][200/233]	Loss 0.1008 (0.0938)	
training:	Epoch: [21][201/233]	Loss 0.0857 (0.0938)	
training:	Epoch: [21][202/233]	Loss 0.0828 (0.0937)	
training:	Epoch: [21][203/233]	Loss 0.0965 (0.0937)	
training:	Epoch: [21][204/233]	Loss 0.0781 (0.0937)	
training:	Epoch: [21][205/233]	Loss 0.1199 (0.0938)	
training:	Epoch: [21][206/233]	Loss 0.0793 (0.0937)	
training:	Epoch: [21][207/233]	Loss 0.0905 (0.0937)	
training:	Epoch: [21][208/233]	Loss 0.0948 (0.0937)	
training:	Epoch: [21][209/233]	Loss 0.0972 (0.0937)	
training:	Epoch: [21][210/233]	Loss 0.0813 (0.0937)	
training:	Epoch: [21][211/233]	Loss 0.1229 (0.0938)	
training:	Epoch: [21][212/233]	Loss 0.0837 (0.0938)	
training:	Epoch: [21][213/233]	Loss 0.1042 (0.0938)	
training:	Epoch: [21][214/233]	Loss 0.0647 (0.0937)	
training:	Epoch: [21][215/233]	Loss 0.1065 (0.0937)	
training:	Epoch: [21][216/233]	Loss 0.1020 (0.0938)	
training:	Epoch: [21][217/233]	Loss 0.1115 (0.0939)	
training:	Epoch: [21][218/233]	Loss 0.0862 (0.0938)	
training:	Epoch: [21][219/233]	Loss 0.0774 (0.0937)	
training:	Epoch: [21][220/233]	Loss 0.1069 (0.0938)	
training:	Epoch: [21][221/233]	Loss 0.0874 (0.0938)	
training:	Epoch: [21][222/233]	Loss 0.0917 (0.0938)	
training:	Epoch: [21][223/233]	Loss 0.0910 (0.0938)	
training:	Epoch: [21][224/233]	Loss 0.0750 (0.0937)	
training:	Epoch: [21][225/233]	Loss 0.0955 (0.0937)	
training:	Epoch: [21][226/233]	Loss 0.0835 (0.0936)	
training:	Epoch: [21][227/233]	Loss 0.0650 (0.0935)	
training:	Epoch: [21][228/233]	Loss 0.0897 (0.0935)	
training:	Epoch: [21][229/233]	Loss 0.0979 (0.0935)	
training:	Epoch: [21][230/233]	Loss 0.0865 (0.0935)	
training:	Epoch: [21][231/233]	Loss 0.0781 (0.0934)	
training:	Epoch: [21][232/233]	Loss 0.0788 (0.0933)	
training:	Epoch: [21][233/233]	Loss 0.0855 (0.0933)	
Training:	 Loss: 0.0931

Training:	 ACC: 1.0000 1.0000 1.0000 1.0000
Validation:	 ACC: 0.8623 0.8620 0.8547 0.8700
Validation:	 Best_BACC: 0.8647 0.8646 0.8639 0.8655
Validation:	 Loss: 0.3301
Pretraining:	Epoch 22/200
----------
training:	Epoch: [22][1/233]	Loss 0.0683 (0.0683)	
training:	Epoch: [22][2/233]	Loss 0.0924 (0.0804)	
training:	Epoch: [22][3/233]	Loss 0.0721 (0.0776)	
training:	Epoch: [22][4/233]	Loss 0.0716 (0.0761)	
training:	Epoch: [22][5/233]	Loss 0.0784 (0.0766)	
training:	Epoch: [22][6/233]	Loss 0.1339 (0.0861)	
training:	Epoch: [22][7/233]	Loss 0.0884 (0.0865)	
training:	Epoch: [22][8/233]	Loss 0.0769 (0.0853)	
training:	Epoch: [22][9/233]	Loss 0.0891 (0.0857)	
training:	Epoch: [22][10/233]	Loss 0.0702 (0.0841)	
training:	Epoch: [22][11/233]	Loss 0.0740 (0.0832)	
training:	Epoch: [22][12/233]	Loss 0.1310 (0.0872)	
training:	Epoch: [22][13/233]	Loss 0.0703 (0.0859)	
training:	Epoch: [22][14/233]	Loss 0.1203 (0.0884)	
training:	Epoch: [22][15/233]	Loss 0.0847 (0.0881)	
training:	Epoch: [22][16/233]	Loss 0.0816 (0.0877)	
training:	Epoch: [22][17/233]	Loss 0.0872 (0.0877)	
training:	Epoch: [22][18/233]	Loss 0.0659 (0.0865)	
training:	Epoch: [22][19/233]	Loss 0.0952 (0.0869)	
training:	Epoch: [22][20/233]	Loss 0.0771 (0.0864)	
training:	Epoch: [22][21/233]	Loss 0.1570 (0.0898)	
training:	Epoch: [22][22/233]	Loss 0.0734 (0.0890)	
training:	Epoch: [22][23/233]	Loss 0.0634 (0.0879)	
training:	Epoch: [22][24/233]	Loss 0.0707 (0.0872)	
training:	Epoch: [22][25/233]	Loss 0.0759 (0.0868)	
training:	Epoch: [22][26/233]	Loss 0.0807 (0.0865)	
training:	Epoch: [22][27/233]	Loss 0.1023 (0.0871)	
training:	Epoch: [22][28/233]	Loss 0.0903 (0.0872)	
training:	Epoch: [22][29/233]	Loss 0.1200 (0.0884)	
training:	Epoch: [22][30/233]	Loss 0.0855 (0.0883)	
training:	Epoch: [22][31/233]	Loss 0.0821 (0.0881)	
training:	Epoch: [22][32/233]	Loss 0.0801 (0.0878)	
training:	Epoch: [22][33/233]	Loss 0.0877 (0.0878)	
training:	Epoch: [22][34/233]	Loss 0.0865 (0.0878)	
training:	Epoch: [22][35/233]	Loss 0.0572 (0.0869)	
training:	Epoch: [22][36/233]	Loss 0.1232 (0.0879)	
training:	Epoch: [22][37/233]	Loss 0.1357 (0.0892)	
training:	Epoch: [22][38/233]	Loss 0.0713 (0.0887)	
training:	Epoch: [22][39/233]	Loss 0.0764 (0.0884)	
training:	Epoch: [22][40/233]	Loss 0.0720 (0.0880)	
training:	Epoch: [22][41/233]	Loss 0.0750 (0.0877)	
training:	Epoch: [22][42/233]	Loss 0.0813 (0.0875)	
training:	Epoch: [22][43/233]	Loss 0.0876 (0.0875)	
training:	Epoch: [22][44/233]	Loss 0.0754 (0.0873)	
training:	Epoch: [22][45/233]	Loss 0.0805 (0.0871)	
training:	Epoch: [22][46/233]	Loss 0.1041 (0.0875)	
training:	Epoch: [22][47/233]	Loss 0.0589 (0.0869)	
training:	Epoch: [22][48/233]	Loss 0.0828 (0.0868)	
training:	Epoch: [22][49/233]	Loss 0.1008 (0.0871)	
training:	Epoch: [22][50/233]	Loss 0.1135 (0.0876)	
training:	Epoch: [22][51/233]	Loss 0.0882 (0.0876)	
training:	Epoch: [22][52/233]	Loss 0.1087 (0.0880)	
training:	Epoch: [22][53/233]	Loss 0.0981 (0.0882)	
training:	Epoch: [22][54/233]	Loss 0.0820 (0.0881)	
training:	Epoch: [22][55/233]	Loss 0.0720 (0.0878)	
training:	Epoch: [22][56/233]	Loss 0.0829 (0.0877)	
training:	Epoch: [22][57/233]	Loss 0.0810 (0.0876)	
training:	Epoch: [22][58/233]	Loss 0.0863 (0.0876)	
training:	Epoch: [22][59/233]	Loss 0.1187 (0.0881)	
training:	Epoch: [22][60/233]	Loss 0.0764 (0.0879)	
training:	Epoch: [22][61/233]	Loss 0.1876 (0.0895)	
training:	Epoch: [22][62/233]	Loss 0.1558 (0.0906)	
training:	Epoch: [22][63/233]	Loss 0.0716 (0.0903)	
training:	Epoch: [22][64/233]	Loss 0.1339 (0.0910)	
training:	Epoch: [22][65/233]	Loss 0.0730 (0.0907)	
training:	Epoch: [22][66/233]	Loss 0.0883 (0.0907)	
training:	Epoch: [22][67/233]	Loss 0.1672 (0.0918)	
training:	Epoch: [22][68/233]	Loss 0.0716 (0.0915)	
training:	Epoch: [22][69/233]	Loss 0.1164 (0.0919)	
training:	Epoch: [22][70/233]	Loss 0.1209 (0.0923)	
training:	Epoch: [22][71/233]	Loss 0.0766 (0.0921)	
training:	Epoch: [22][72/233]	Loss 0.0843 (0.0920)	
training:	Epoch: [22][73/233]	Loss 0.1118 (0.0922)	
training:	Epoch: [22][74/233]	Loss 0.0751 (0.0920)	
training:	Epoch: [22][75/233]	Loss 0.0793 (0.0918)	
training:	Epoch: [22][76/233]	Loss 0.0797 (0.0917)	
training:	Epoch: [22][77/233]	Loss 0.0633 (0.0913)	
training:	Epoch: [22][78/233]	Loss 0.1228 (0.0917)	
training:	Epoch: [22][79/233]	Loss 0.0814 (0.0916)	
training:	Epoch: [22][80/233]	Loss 0.0958 (0.0916)	
training:	Epoch: [22][81/233]	Loss 0.0666 (0.0913)	
training:	Epoch: [22][82/233]	Loss 0.0943 (0.0914)	
training:	Epoch: [22][83/233]	Loss 0.0918 (0.0914)	
training:	Epoch: [22][84/233]	Loss 0.0730 (0.0911)	
training:	Epoch: [22][85/233]	Loss 0.0742 (0.0909)	
training:	Epoch: [22][86/233]	Loss 0.0983 (0.0910)	
training:	Epoch: [22][87/233]	Loss 0.0845 (0.0910)	
training:	Epoch: [22][88/233]	Loss 0.0630 (0.0906)	
training:	Epoch: [22][89/233]	Loss 0.0795 (0.0905)	
training:	Epoch: [22][90/233]	Loss 0.1089 (0.0907)	
training:	Epoch: [22][91/233]	Loss 0.1049 (0.0909)	
training:	Epoch: [22][92/233]	Loss 0.0881 (0.0908)	
training:	Epoch: [22][93/233]	Loss 0.0915 (0.0909)	
training:	Epoch: [22][94/233]	Loss 0.1053 (0.0910)	
training:	Epoch: [22][95/233]	Loss 0.1083 (0.0912)	
training:	Epoch: [22][96/233]	Loss 0.0698 (0.0910)	
training:	Epoch: [22][97/233]	Loss 0.0884 (0.0909)	
training:	Epoch: [22][98/233]	Loss 0.0912 (0.0909)	
training:	Epoch: [22][99/233]	Loss 0.0777 (0.0908)	
training:	Epoch: [22][100/233]	Loss 0.2158 (0.0921)	
training:	Epoch: [22][101/233]	Loss 0.0688 (0.0918)	
training:	Epoch: [22][102/233]	Loss 0.0833 (0.0917)	
training:	Epoch: [22][103/233]	Loss 0.0920 (0.0917)	
training:	Epoch: [22][104/233]	Loss 0.1406 (0.0922)	
training:	Epoch: [22][105/233]	Loss 0.1021 (0.0923)	
training:	Epoch: [22][106/233]	Loss 0.2026 (0.0934)	
training:	Epoch: [22][107/233]	Loss 0.0904 (0.0933)	
training:	Epoch: [22][108/233]	Loss 0.1649 (0.0940)	
training:	Epoch: [22][109/233]	Loss 0.0887 (0.0939)	
training:	Epoch: [22][110/233]	Loss 0.1109 (0.0941)	
training:	Epoch: [22][111/233]	Loss 0.1004 (0.0941)	
training:	Epoch: [22][112/233]	Loss 0.0742 (0.0940)	
training:	Epoch: [22][113/233]	Loss 0.1305 (0.0943)	
training:	Epoch: [22][114/233]	Loss 0.0967 (0.0943)	
training:	Epoch: [22][115/233]	Loss 0.0718 (0.0941)	
training:	Epoch: [22][116/233]	Loss 0.0700 (0.0939)	
training:	Epoch: [22][117/233]	Loss 0.0769 (0.0938)	
training:	Epoch: [22][118/233]	Loss 0.0968 (0.0938)	
training:	Epoch: [22][119/233]	Loss 0.0792 (0.0937)	
training:	Epoch: [22][120/233]	Loss 0.0687 (0.0935)	
training:	Epoch: [22][121/233]	Loss 0.1027 (0.0935)	
training:	Epoch: [22][122/233]	Loss 0.0997 (0.0936)	
training:	Epoch: [22][123/233]	Loss 0.0870 (0.0935)	
training:	Epoch: [22][124/233]	Loss 0.0774 (0.0934)	
training:	Epoch: [22][125/233]	Loss 0.0842 (0.0933)	
training:	Epoch: [22][126/233]	Loss 0.1168 (0.0935)	
training:	Epoch: [22][127/233]	Loss 0.0702 (0.0933)	
training:	Epoch: [22][128/233]	Loss 0.0990 (0.0934)	
training:	Epoch: [22][129/233]	Loss 0.1674 (0.0940)	
training:	Epoch: [22][130/233]	Loss 0.0748 (0.0938)	
training:	Epoch: [22][131/233]	Loss 0.0636 (0.0936)	
training:	Epoch: [22][132/233]	Loss 0.1184 (0.0938)	
training:	Epoch: [22][133/233]	Loss 0.0747 (0.0936)	
training:	Epoch: [22][134/233]	Loss 0.0820 (0.0935)	
training:	Epoch: [22][135/233]	Loss 0.0689 (0.0933)	
training:	Epoch: [22][136/233]	Loss 0.0759 (0.0932)	
training:	Epoch: [22][137/233]	Loss 0.0859 (0.0932)	
training:	Epoch: [22][138/233]	Loss 0.0675 (0.0930)	
training:	Epoch: [22][139/233]	Loss 0.0742 (0.0928)	
training:	Epoch: [22][140/233]	Loss 0.0734 (0.0927)	
training:	Epoch: [22][141/233]	Loss 0.0745 (0.0926)	
training:	Epoch: [22][142/233]	Loss 0.1028 (0.0927)	
training:	Epoch: [22][143/233]	Loss 0.0902 (0.0926)	
training:	Epoch: [22][144/233]	Loss 0.0675 (0.0925)	
training:	Epoch: [22][145/233]	Loss 0.1124 (0.0926)	
training:	Epoch: [22][146/233]	Loss 0.0900 (0.0926)	
training:	Epoch: [22][147/233]	Loss 0.0763 (0.0925)	
training:	Epoch: [22][148/233]	Loss 0.0988 (0.0925)	
training:	Epoch: [22][149/233]	Loss 0.0802 (0.0924)	
training:	Epoch: [22][150/233]	Loss 0.0793 (0.0923)	
training:	Epoch: [22][151/233]	Loss 0.0979 (0.0924)	
training:	Epoch: [22][152/233]	Loss 0.0952 (0.0924)	
training:	Epoch: [22][153/233]	Loss 0.0997 (0.0924)	
training:	Epoch: [22][154/233]	Loss 0.0799 (0.0924)	
training:	Epoch: [22][155/233]	Loss 0.1093 (0.0925)	
training:	Epoch: [22][156/233]	Loss 0.0776 (0.0924)	
training:	Epoch: [22][157/233]	Loss 0.0641 (0.0922)	
training:	Epoch: [22][158/233]	Loss 0.0754 (0.0921)	
training:	Epoch: [22][159/233]	Loss 0.0810 (0.0920)	
training:	Epoch: [22][160/233]	Loss 0.1298 (0.0923)	
training:	Epoch: [22][161/233]	Loss 0.1110 (0.0924)	
training:	Epoch: [22][162/233]	Loss 0.0845 (0.0923)	
training:	Epoch: [22][163/233]	Loss 0.0795 (0.0922)	
training:	Epoch: [22][164/233]	Loss 0.1013 (0.0923)	
training:	Epoch: [22][165/233]	Loss 0.1156 (0.0924)	
training:	Epoch: [22][166/233]	Loss 0.0619 (0.0923)	
training:	Epoch: [22][167/233]	Loss 0.0920 (0.0923)	
training:	Epoch: [22][168/233]	Loss 0.1082 (0.0924)	
training:	Epoch: [22][169/233]	Loss 0.1491 (0.0927)	
training:	Epoch: [22][170/233]	Loss 0.1104 (0.0928)	
training:	Epoch: [22][171/233]	Loss 0.1023 (0.0928)	
training:	Epoch: [22][172/233]	Loss 0.0959 (0.0929)	
training:	Epoch: [22][173/233]	Loss 0.1147 (0.0930)	
training:	Epoch: [22][174/233]	Loss 0.0832 (0.0929)	
training:	Epoch: [22][175/233]	Loss 0.0957 (0.0930)	
training:	Epoch: [22][176/233]	Loss 0.1198 (0.0931)	
training:	Epoch: [22][177/233]	Loss 0.0970 (0.0931)	
training:	Epoch: [22][178/233]	Loss 0.1035 (0.0932)	
training:	Epoch: [22][179/233]	Loss 0.1078 (0.0933)	
training:	Epoch: [22][180/233]	Loss 0.0903 (0.0932)	
training:	Epoch: [22][181/233]	Loss 0.0949 (0.0933)	
training:	Epoch: [22][182/233]	Loss 0.0934 (0.0933)	
training:	Epoch: [22][183/233]	Loss 0.0749 (0.0932)	
training:	Epoch: [22][184/233]	Loss 0.0687 (0.0930)	
training:	Epoch: [22][185/233]	Loss 0.1342 (0.0932)	
training:	Epoch: [22][186/233]	Loss 0.0826 (0.0932)	
training:	Epoch: [22][187/233]	Loss 0.0859 (0.0932)	
training:	Epoch: [22][188/233]	Loss 0.1024 (0.0932)	
training:	Epoch: [22][189/233]	Loss 0.0747 (0.0931)	
training:	Epoch: [22][190/233]	Loss 0.0697 (0.0930)	
training:	Epoch: [22][191/233]	Loss 0.0802 (0.0929)	
training:	Epoch: [22][192/233]	Loss 0.1035 (0.0930)	
training:	Epoch: [22][193/233]	Loss 0.0849 (0.0929)	
training:	Epoch: [22][194/233]	Loss 0.1069 (0.0930)	
training:	Epoch: [22][195/233]	Loss 0.1009 (0.0930)	
training:	Epoch: [22][196/233]	Loss 0.0866 (0.0930)	
training:	Epoch: [22][197/233]	Loss 0.0768 (0.0929)	
training:	Epoch: [22][198/233]	Loss 0.1098 (0.0930)	
training:	Epoch: [22][199/233]	Loss 0.0882 (0.0930)	
training:	Epoch: [22][200/233]	Loss 0.1097 (0.0931)	
training:	Epoch: [22][201/233]	Loss 0.0840 (0.0930)	
training:	Epoch: [22][202/233]	Loss 0.1717 (0.0934)	
training:	Epoch: [22][203/233]	Loss 0.0744 (0.0933)	
training:	Epoch: [22][204/233]	Loss 0.1145 (0.0934)	
training:	Epoch: [22][205/233]	Loss 0.0721 (0.0933)	
training:	Epoch: [22][206/233]	Loss 0.0782 (0.0932)	
training:	Epoch: [22][207/233]	Loss 0.0726 (0.0931)	
training:	Epoch: [22][208/233]	Loss 0.0790 (0.0931)	
training:	Epoch: [22][209/233]	Loss 0.1005 (0.0931)	
training:	Epoch: [22][210/233]	Loss 0.0880 (0.0931)	
training:	Epoch: [22][211/233]	Loss 0.0839 (0.0930)	
training:	Epoch: [22][212/233]	Loss 0.0836 (0.0930)	
training:	Epoch: [22][213/233]	Loss 0.1253 (0.0932)	
training:	Epoch: [22][214/233]	Loss 0.0738 (0.0931)	
training:	Epoch: [22][215/233]	Loss 0.0795 (0.0930)	
training:	Epoch: [22][216/233]	Loss 0.0735 (0.0929)	
training:	Epoch: [22][217/233]	Loss 0.0798 (0.0928)	
training:	Epoch: [22][218/233]	Loss 0.0654 (0.0927)	
training:	Epoch: [22][219/233]	Loss 0.0941 (0.0927)	
training:	Epoch: [22][220/233]	Loss 0.1016 (0.0928)	
training:	Epoch: [22][221/233]	Loss 0.0653 (0.0926)	
training:	Epoch: [22][222/233]	Loss 0.1199 (0.0928)	
training:	Epoch: [22][223/233]	Loss 0.0812 (0.0927)	
training:	Epoch: [22][224/233]	Loss 0.1322 (0.0929)	
training:	Epoch: [22][225/233]	Loss 0.0688 (0.0928)	
training:	Epoch: [22][226/233]	Loss 0.0937 (0.0928)	
training:	Epoch: [22][227/233]	Loss 0.0832 (0.0927)	
training:	Epoch: [22][228/233]	Loss 0.0956 (0.0928)	
training:	Epoch: [22][229/233]	Loss 0.0806 (0.0927)	
training:	Epoch: [22][230/233]	Loss 0.0945 (0.0927)	
training:	Epoch: [22][231/233]	Loss 0.1050 (0.0928)	
training:	Epoch: [22][232/233]	Loss 0.0632 (0.0926)	
training:	Epoch: [22][233/233]	Loss 0.2179 (0.0932)	
Training:	 Loss: 0.0930

Training:	 ACC: 1.0000 1.0000 1.0000 1.0000
Validation:	 ACC: 0.8599 0.8582 0.8219 0.8980
Validation:	 Best_BACC: 0.8647 0.8646 0.8639 0.8655
Validation:	 Loss: 0.3360
Pretraining:	Epoch 23/200
----------
training:	Epoch: [23][1/233]	Loss 0.0805 (0.0805)	
training:	Epoch: [23][2/233]	Loss 0.0803 (0.0804)	
training:	Epoch: [23][3/233]	Loss 0.0973 (0.0860)	
training:	Epoch: [23][4/233]	Loss 0.0870 (0.0863)	
training:	Epoch: [23][5/233]	Loss 0.0821 (0.0854)	
training:	Epoch: [23][6/233]	Loss 0.0891 (0.0860)	
training:	Epoch: [23][7/233]	Loss 0.0585 (0.0821)	
training:	Epoch: [23][8/233]	Loss 0.0705 (0.0807)	
training:	Epoch: [23][9/233]	Loss 0.0935 (0.0821)	
training:	Epoch: [23][10/233]	Loss 0.0814 (0.0820)	
training:	Epoch: [23][11/233]	Loss 0.0915 (0.0829)	
training:	Epoch: [23][12/233]	Loss 0.0678 (0.0816)	
training:	Epoch: [23][13/233]	Loss 0.0902 (0.0823)	
training:	Epoch: [23][14/233]	Loss 0.0605 (0.0807)	
training:	Epoch: [23][15/233]	Loss 0.0804 (0.0807)	
training:	Epoch: [23][16/233]	Loss 0.0734 (0.0802)	
training:	Epoch: [23][17/233]	Loss 0.0854 (0.0806)	
training:	Epoch: [23][18/233]	Loss 0.1119 (0.0823)	
training:	Epoch: [23][19/233]	Loss 0.0751 (0.0819)	
training:	Epoch: [23][20/233]	Loss 0.0714 (0.0814)	
training:	Epoch: [23][21/233]	Loss 0.1195 (0.0832)	
training:	Epoch: [23][22/233]	Loss 0.0617 (0.0822)	
training:	Epoch: [23][23/233]	Loss 0.0586 (0.0812)	
training:	Epoch: [23][24/233]	Loss 0.1132 (0.0825)	
training:	Epoch: [23][25/233]	Loss 0.0945 (0.0830)	
training:	Epoch: [23][26/233]	Loss 0.0670 (0.0824)	
training:	Epoch: [23][27/233]	Loss 0.0547 (0.0814)	
training:	Epoch: [23][28/233]	Loss 0.0762 (0.0812)	
training:	Epoch: [23][29/233]	Loss 0.1000 (0.0818)	
training:	Epoch: [23][30/233]	Loss 0.0759 (0.0816)	
training:	Epoch: [23][31/233]	Loss 0.0835 (0.0817)	
training:	Epoch: [23][32/233]	Loss 0.0907 (0.0820)	
training:	Epoch: [23][33/233]	Loss 0.0811 (0.0820)	
training:	Epoch: [23][34/233]	Loss 0.1063 (0.0827)	
training:	Epoch: [23][35/233]	Loss 0.0824 (0.0827)	
training:	Epoch: [23][36/233]	Loss 0.0599 (0.0820)	
training:	Epoch: [23][37/233]	Loss 0.0695 (0.0817)	
training:	Epoch: [23][38/233]	Loss 0.0710 (0.0814)	
training:	Epoch: [23][39/233]	Loss 0.1172 (0.0823)	
training:	Epoch: [23][40/233]	Loss 0.1813 (0.0848)	
training:	Epoch: [23][41/233]	Loss 0.1189 (0.0856)	
training:	Epoch: [23][42/233]	Loss 0.0851 (0.0856)	
training:	Epoch: [23][43/233]	Loss 0.0826 (0.0856)	
training:	Epoch: [23][44/233]	Loss 0.0704 (0.0852)	
training:	Epoch: [23][45/233]	Loss 0.0636 (0.0847)	
training:	Epoch: [23][46/233]	Loss 0.0705 (0.0844)	
training:	Epoch: [23][47/233]	Loss 0.1589 (0.0860)	
training:	Epoch: [23][48/233]	Loss 0.0715 (0.0857)	
training:	Epoch: [23][49/233]	Loss 0.0818 (0.0856)	
training:	Epoch: [23][50/233]	Loss 0.0657 (0.0852)	
training:	Epoch: [23][51/233]	Loss 0.0603 (0.0847)	
training:	Epoch: [23][52/233]	Loss 0.0728 (0.0845)	
training:	Epoch: [23][53/233]	Loss 0.0727 (0.0843)	
training:	Epoch: [23][54/233]	Loss 0.0576 (0.0838)	
training:	Epoch: [23][55/233]	Loss 0.0958 (0.0840)	
training:	Epoch: [23][56/233]	Loss 0.0787 (0.0839)	
training:	Epoch: [23][57/233]	Loss 0.0892 (0.0840)	
training:	Epoch: [23][58/233]	Loss 0.0924 (0.0841)	
training:	Epoch: [23][59/233]	Loss 0.1060 (0.0845)	
training:	Epoch: [23][60/233]	Loss 0.0746 (0.0844)	
training:	Epoch: [23][61/233]	Loss 0.0917 (0.0845)	
training:	Epoch: [23][62/233]	Loss 0.0676 (0.0842)	
training:	Epoch: [23][63/233]	Loss 0.0765 (0.0841)	
training:	Epoch: [23][64/233]	Loss 0.0743 (0.0839)	
training:	Epoch: [23][65/233]	Loss 0.0761 (0.0838)	
training:	Epoch: [23][66/233]	Loss 0.0774 (0.0837)	
training:	Epoch: [23][67/233]	Loss 0.0801 (0.0837)	
training:	Epoch: [23][68/233]	Loss 0.0782 (0.0836)	
training:	Epoch: [23][69/233]	Loss 0.1393 (0.0844)	
training:	Epoch: [23][70/233]	Loss 0.0633 (0.0841)	
training:	Epoch: [23][71/233]	Loss 0.0807 (0.0840)	
training:	Epoch: [23][72/233]	Loss 0.0876 (0.0841)	
training:	Epoch: [23][73/233]	Loss 0.0734 (0.0839)	
training:	Epoch: [23][74/233]	Loss 0.1121 (0.0843)	
training:	Epoch: [23][75/233]	Loss 0.0682 (0.0841)	
training:	Epoch: [23][76/233]	Loss 0.0793 (0.0840)	
training:	Epoch: [23][77/233]	Loss 0.1426 (0.0848)	
training:	Epoch: [23][78/233]	Loss 0.0590 (0.0845)	
training:	Epoch: [23][79/233]	Loss 0.0757 (0.0844)	
training:	Epoch: [23][80/233]	Loss 0.1097 (0.0847)	
training:	Epoch: [23][81/233]	Loss 0.0685 (0.0845)	
training:	Epoch: [23][82/233]	Loss 0.0940 (0.0846)	
training:	Epoch: [23][83/233]	Loss 0.0773 (0.0845)	
training:	Epoch: [23][84/233]	Loss 0.0722 (0.0844)	
training:	Epoch: [23][85/233]	Loss 0.0598 (0.0841)	
training:	Epoch: [23][86/233]	Loss 0.0848 (0.0841)	
training:	Epoch: [23][87/233]	Loss 0.1282 (0.0846)	
training:	Epoch: [23][88/233]	Loss 0.0949 (0.0847)	
training:	Epoch: [23][89/233]	Loss 0.0736 (0.0846)	
training:	Epoch: [23][90/233]	Loss 0.0614 (0.0843)	
training:	Epoch: [23][91/233]	Loss 0.0747 (0.0842)	
training:	Epoch: [23][92/233]	Loss 0.0714 (0.0841)	
training:	Epoch: [23][93/233]	Loss 0.0704 (0.0839)	
training:	Epoch: [23][94/233]	Loss 0.0791 (0.0839)	
training:	Epoch: [23][95/233]	Loss 0.0881 (0.0839)	
training:	Epoch: [23][96/233]	Loss 0.0700 (0.0838)	
training:	Epoch: [23][97/233]	Loss 0.0954 (0.0839)	
training:	Epoch: [23][98/233]	Loss 0.0931 (0.0840)	
training:	Epoch: [23][99/233]	Loss 0.0744 (0.0839)	
training:	Epoch: [23][100/233]	Loss 0.0777 (0.0838)	
training:	Epoch: [23][101/233]	Loss 0.0999 (0.0840)	
training:	Epoch: [23][102/233]	Loss 0.0918 (0.0841)	
training:	Epoch: [23][103/233]	Loss 0.0980 (0.0842)	
training:	Epoch: [23][104/233]	Loss 0.0688 (0.0841)	
training:	Epoch: [23][105/233]	Loss 0.0846 (0.0841)	
training:	Epoch: [23][106/233]	Loss 0.0869 (0.0841)	
training:	Epoch: [23][107/233]	Loss 0.0696 (0.0839)	
training:	Epoch: [23][108/233]	Loss 0.0849 (0.0840)	
training:	Epoch: [23][109/233]	Loss 0.0710 (0.0838)	
training:	Epoch: [23][110/233]	Loss 0.0719 (0.0837)	
training:	Epoch: [23][111/233]	Loss 0.0750 (0.0836)	
training:	Epoch: [23][112/233]	Loss 0.0829 (0.0836)	
training:	Epoch: [23][113/233]	Loss 0.0953 (0.0837)	
training:	Epoch: [23][114/233]	Loss 0.0884 (0.0838)	
training:	Epoch: [23][115/233]	Loss 0.0702 (0.0837)	
training:	Epoch: [23][116/233]	Loss 0.0708 (0.0836)	
training:	Epoch: [23][117/233]	Loss 0.0743 (0.0835)	
training:	Epoch: [23][118/233]	Loss 0.0954 (0.0836)	
training:	Epoch: [23][119/233]	Loss 0.0770 (0.0835)	
training:	Epoch: [23][120/233]	Loss 0.1106 (0.0837)	
training:	Epoch: [23][121/233]	Loss 0.0930 (0.0838)	
training:	Epoch: [23][122/233]	Loss 0.0658 (0.0837)	
training:	Epoch: [23][123/233]	Loss 0.0896 (0.0837)	
training:	Epoch: [23][124/233]	Loss 0.0739 (0.0836)	
training:	Epoch: [23][125/233]	Loss 0.0662 (0.0835)	
training:	Epoch: [23][126/233]	Loss 0.0701 (0.0834)	
training:	Epoch: [23][127/233]	Loss 0.0701 (0.0833)	
training:	Epoch: [23][128/233]	Loss 0.1020 (0.0834)	
training:	Epoch: [23][129/233]	Loss 0.0772 (0.0834)	
training:	Epoch: [23][130/233]	Loss 0.1015 (0.0835)	
training:	Epoch: [23][131/233]	Loss 0.0958 (0.0836)	
training:	Epoch: [23][132/233]	Loss 0.0747 (0.0836)	
training:	Epoch: [23][133/233]	Loss 0.0740 (0.0835)	
training:	Epoch: [23][134/233]	Loss 0.0791 (0.0835)	
training:	Epoch: [23][135/233]	Loss 0.0767 (0.0834)	
training:	Epoch: [23][136/233]	Loss 0.0869 (0.0834)	
training:	Epoch: [23][137/233]	Loss 0.0730 (0.0834)	
training:	Epoch: [23][138/233]	Loss 0.0648 (0.0832)	
training:	Epoch: [23][139/233]	Loss 0.0799 (0.0832)	
training:	Epoch: [23][140/233]	Loss 0.1269 (0.0835)	
training:	Epoch: [23][141/233]	Loss 0.0937 (0.0836)	
training:	Epoch: [23][142/233]	Loss 0.1360 (0.0839)	
training:	Epoch: [23][143/233]	Loss 0.0715 (0.0839)	
training:	Epoch: [23][144/233]	Loss 0.0788 (0.0838)	
training:	Epoch: [23][145/233]	Loss 0.0916 (0.0839)	
training:	Epoch: [23][146/233]	Loss 0.0767 (0.0838)	
training:	Epoch: [23][147/233]	Loss 0.0814 (0.0838)	
training:	Epoch: [23][148/233]	Loss 0.0956 (0.0839)	
training:	Epoch: [23][149/233]	Loss 0.0869 (0.0839)	
training:	Epoch: [23][150/233]	Loss 0.0630 (0.0838)	
training:	Epoch: [23][151/233]	Loss 0.0892 (0.0838)	
training:	Epoch: [23][152/233]	Loss 0.0802 (0.0838)	
training:	Epoch: [23][153/233]	Loss 0.1249 (0.0841)	
training:	Epoch: [23][154/233]	Loss 0.0745 (0.0840)	
training:	Epoch: [23][155/233]	Loss 0.0697 (0.0839)	
training:	Epoch: [23][156/233]	Loss 0.0708 (0.0838)	
training:	Epoch: [23][157/233]	Loss 0.0804 (0.0838)	
training:	Epoch: [23][158/233]	Loss 0.0846 (0.0838)	
training:	Epoch: [23][159/233]	Loss 0.0895 (0.0838)	
training:	Epoch: [23][160/233]	Loss 0.0868 (0.0839)	
training:	Epoch: [23][161/233]	Loss 0.0793 (0.0838)	
training:	Epoch: [23][162/233]	Loss 0.1013 (0.0839)	
training:	Epoch: [23][163/233]	Loss 0.0785 (0.0839)	
training:	Epoch: [23][164/233]	Loss 0.1017 (0.0840)	
training:	Epoch: [23][165/233]	Loss 0.0576 (0.0839)	
training:	Epoch: [23][166/233]	Loss 0.0768 (0.0838)	
training:	Epoch: [23][167/233]	Loss 0.1082 (0.0840)	
training:	Epoch: [23][168/233]	Loss 0.1324 (0.0842)	
training:	Epoch: [23][169/233]	Loss 0.0896 (0.0843)	
training:	Epoch: [23][170/233]	Loss 0.1201 (0.0845)	
training:	Epoch: [23][171/233]	Loss 0.1006 (0.0846)	
training:	Epoch: [23][172/233]	Loss 0.0628 (0.0845)	
training:	Epoch: [23][173/233]	Loss 0.0876 (0.0845)	
training:	Epoch: [23][174/233]	Loss 0.1233 (0.0847)	
training:	Epoch: [23][175/233]	Loss 0.0699 (0.0846)	
training:	Epoch: [23][176/233]	Loss 0.0942 (0.0847)	
training:	Epoch: [23][177/233]	Loss 0.0731 (0.0846)	
training:	Epoch: [23][178/233]	Loss 0.0974 (0.0847)	
training:	Epoch: [23][179/233]	Loss 0.0968 (0.0847)	
training:	Epoch: [23][180/233]	Loss 0.0738 (0.0847)	
training:	Epoch: [23][181/233]	Loss 0.1075 (0.0848)	
training:	Epoch: [23][182/233]	Loss 0.0686 (0.0847)	
training:	Epoch: [23][183/233]	Loss 0.0929 (0.0848)	
training:	Epoch: [23][184/233]	Loss 0.0825 (0.0847)	
training:	Epoch: [23][185/233]	Loss 0.0775 (0.0847)	
training:	Epoch: [23][186/233]	Loss 0.0958 (0.0848)	
training:	Epoch: [23][187/233]	Loss 0.1087 (0.0849)	
training:	Epoch: [23][188/233]	Loss 0.0815 (0.0849)	
training:	Epoch: [23][189/233]	Loss 0.1091 (0.0850)	
training:	Epoch: [23][190/233]	Loss 0.0746 (0.0850)	
training:	Epoch: [23][191/233]	Loss 0.0774 (0.0849)	
training:	Epoch: [23][192/233]	Loss 0.0787 (0.0849)	
training:	Epoch: [23][193/233]	Loss 0.0686 (0.0848)	
training:	Epoch: [23][194/233]	Loss 0.0734 (0.0847)	
training:	Epoch: [23][195/233]	Loss 0.0890 (0.0848)	
training:	Epoch: [23][196/233]	Loss 0.0892 (0.0848)	
training:	Epoch: [23][197/233]	Loss 0.0751 (0.0847)	
training:	Epoch: [23][198/233]	Loss 0.0680 (0.0846)	
training:	Epoch: [23][199/233]	Loss 0.1093 (0.0848)	
training:	Epoch: [23][200/233]	Loss 0.0608 (0.0847)	
training:	Epoch: [23][201/233]	Loss 0.0872 (0.0847)	
training:	Epoch: [23][202/233]	Loss 0.1076 (0.0848)	
training:	Epoch: [23][203/233]	Loss 0.0733 (0.0847)	
training:	Epoch: [23][204/233]	Loss 0.0856 (0.0847)	
training:	Epoch: [23][205/233]	Loss 0.1184 (0.0849)	
training:	Epoch: [23][206/233]	Loss 0.0997 (0.0850)	
training:	Epoch: [23][207/233]	Loss 0.0713 (0.0849)	
training:	Epoch: [23][208/233]	Loss 0.0813 (0.0849)	
training:	Epoch: [23][209/233]	Loss 0.0787 (0.0848)	
training:	Epoch: [23][210/233]	Loss 0.0727 (0.0848)	
training:	Epoch: [23][211/233]	Loss 0.0666 (0.0847)	
training:	Epoch: [23][212/233]	Loss 0.0748 (0.0847)	
training:	Epoch: [23][213/233]	Loss 0.0714 (0.0846)	
training:	Epoch: [23][214/233]	Loss 0.0648 (0.0845)	
training:	Epoch: [23][215/233]	Loss 0.0691 (0.0844)	
training:	Epoch: [23][216/233]	Loss 0.0882 (0.0844)	
training:	Epoch: [23][217/233]	Loss 0.0746 (0.0844)	
training:	Epoch: [23][218/233]	Loss 0.0754 (0.0844)	
training:	Epoch: [23][219/233]	Loss 0.1231 (0.0845)	
training:	Epoch: [23][220/233]	Loss 0.0809 (0.0845)	
training:	Epoch: [23][221/233]	Loss 0.1427 (0.0848)	
training:	Epoch: [23][222/233]	Loss 0.0878 (0.0848)	
training:	Epoch: [23][223/233]	Loss 0.0822 (0.0848)	
training:	Epoch: [23][224/233]	Loss 0.0771 (0.0848)	
training:	Epoch: [23][225/233]	Loss 0.0647 (0.0847)	
training:	Epoch: [23][226/233]	Loss 0.1396 (0.0849)	
training:	Epoch: [23][227/233]	Loss 0.0877 (0.0849)	
training:	Epoch: [23][228/233]	Loss 0.0948 (0.0850)	
training:	Epoch: [23][229/233]	Loss 0.0734 (0.0849)	
training:	Epoch: [23][230/233]	Loss 0.0859 (0.0849)	
training:	Epoch: [23][231/233]	Loss 0.0765 (0.0849)	
training:	Epoch: [23][232/233]	Loss 0.0613 (0.0848)	
training:	Epoch: [23][233/233]	Loss 0.0812 (0.0848)	
Training:	 Loss: 0.0846

Training:	 ACC: 1.0000 1.0000 1.0000 1.0000
Validation:	 ACC: 0.8633 0.8625 0.8465 0.8800
Validation:	 Best_BACC: 0.8647 0.8646 0.8639 0.8655
Validation:	 Loss: 0.3280
Pretraining:	Epoch 24/200
----------
training:	Epoch: [24][1/233]	Loss 0.1487 (0.1487)	
training:	Epoch: [24][2/233]	Loss 0.0492 (0.0990)	
training:	Epoch: [24][3/233]	Loss 0.0607 (0.0862)	
training:	Epoch: [24][4/233]	Loss 0.0808 (0.0849)	
training:	Epoch: [24][5/233]	Loss 0.0735 (0.0826)	
training:	Epoch: [24][6/233]	Loss 0.0798 (0.0821)	
training:	Epoch: [24][7/233]	Loss 0.0692 (0.0803)	
training:	Epoch: [24][8/233]	Loss 0.1073 (0.0837)	
training:	Epoch: [24][9/233]	Loss 0.0565 (0.0807)	
training:	Epoch: [24][10/233]	Loss 0.1011 (0.0827)	
training:	Epoch: [24][11/233]	Loss 0.0926 (0.0836)	
training:	Epoch: [24][12/233]	Loss 0.0667 (0.0822)	
training:	Epoch: [24][13/233]	Loss 0.0678 (0.0811)	
training:	Epoch: [24][14/233]	Loss 0.0771 (0.0808)	
training:	Epoch: [24][15/233]	Loss 0.0766 (0.0805)	
training:	Epoch: [24][16/233]	Loss 0.0559 (0.0790)	
training:	Epoch: [24][17/233]	Loss 0.1390 (0.0825)	
training:	Epoch: [24][18/233]	Loss 0.0683 (0.0817)	
training:	Epoch: [24][19/233]	Loss 0.1093 (0.0832)	
training:	Epoch: [24][20/233]	Loss 0.0581 (0.0819)	
training:	Epoch: [24][21/233]	Loss 0.0633 (0.0810)	
training:	Epoch: [24][22/233]	Loss 0.0880 (0.0813)	
training:	Epoch: [24][23/233]	Loss 0.0654 (0.0807)	
training:	Epoch: [24][24/233]	Loss 0.0653 (0.0800)	
training:	Epoch: [24][25/233]	Loss 0.0905 (0.0804)	
training:	Epoch: [24][26/233]	Loss 0.0713 (0.0801)	
training:	Epoch: [24][27/233]	Loss 0.0800 (0.0801)	
training:	Epoch: [24][28/233]	Loss 0.0579 (0.0793)	
training:	Epoch: [24][29/233]	Loss 0.0752 (0.0791)	
training:	Epoch: [24][30/233]	Loss 0.0683 (0.0788)	
training:	Epoch: [24][31/233]	Loss 0.0833 (0.0789)	
training:	Epoch: [24][32/233]	Loss 0.0632 (0.0784)	
training:	Epoch: [24][33/233]	Loss 0.0714 (0.0782)	
training:	Epoch: [24][34/233]	Loss 0.0733 (0.0781)	
training:	Epoch: [24][35/233]	Loss 0.0885 (0.0784)	
training:	Epoch: [24][36/233]	Loss 0.0806 (0.0784)	
training:	Epoch: [24][37/233]	Loss 0.0644 (0.0781)	
training:	Epoch: [24][38/233]	Loss 0.0824 (0.0782)	
training:	Epoch: [24][39/233]	Loss 0.0724 (0.0780)	
training:	Epoch: [24][40/233]	Loss 0.0600 (0.0776)	
training:	Epoch: [24][41/233]	Loss 0.0649 (0.0773)	
training:	Epoch: [24][42/233]	Loss 0.0953 (0.0777)	
training:	Epoch: [24][43/233]	Loss 0.0901 (0.0780)	
training:	Epoch: [24][44/233]	Loss 0.0703 (0.0778)	
training:	Epoch: [24][45/233]	Loss 0.0695 (0.0776)	
training:	Epoch: [24][46/233]	Loss 0.0801 (0.0777)	
training:	Epoch: [24][47/233]	Loss 0.0608 (0.0773)	
training:	Epoch: [24][48/233]	Loss 0.0669 (0.0771)	
training:	Epoch: [24][49/233]	Loss 0.1060 (0.0777)	
training:	Epoch: [24][50/233]	Loss 0.0817 (0.0778)	
training:	Epoch: [24][51/233]	Loss 0.0768 (0.0778)	
training:	Epoch: [24][52/233]	Loss 0.1017 (0.0782)	
training:	Epoch: [24][53/233]	Loss 0.0784 (0.0782)	
training:	Epoch: [24][54/233]	Loss 0.0654 (0.0780)	
training:	Epoch: [24][55/233]	Loss 0.1034 (0.0784)	
training:	Epoch: [24][56/233]	Loss 0.0745 (0.0784)	
training:	Epoch: [24][57/233]	Loss 0.0728 (0.0783)	
training:	Epoch: [24][58/233]	Loss 0.0603 (0.0780)	
training:	Epoch: [24][59/233]	Loss 0.0914 (0.0782)	
training:	Epoch: [24][60/233]	Loss 0.0815 (0.0782)	
training:	Epoch: [24][61/233]	Loss 0.1008 (0.0786)	
training:	Epoch: [24][62/233]	Loss 0.1898 (0.0804)	
training:	Epoch: [24][63/233]	Loss 0.0878 (0.0805)	
training:	Epoch: [24][64/233]	Loss 0.0641 (0.0803)	
training:	Epoch: [24][65/233]	Loss 0.0846 (0.0803)	
training:	Epoch: [24][66/233]	Loss 0.0966 (0.0806)	
training:	Epoch: [24][67/233]	Loss 0.0972 (0.0808)	
training:	Epoch: [24][68/233]	Loss 0.0704 (0.0807)	
training:	Epoch: [24][69/233]	Loss 0.0550 (0.0803)	
training:	Epoch: [24][70/233]	Loss 0.0641 (0.0801)	
training:	Epoch: [24][71/233]	Loss 0.0862 (0.0802)	
training:	Epoch: [24][72/233]	Loss 0.1049 (0.0805)	
training:	Epoch: [24][73/233]	Loss 0.0688 (0.0803)	
training:	Epoch: [24][74/233]	Loss 0.0927 (0.0805)	
training:	Epoch: [24][75/233]	Loss 0.0704 (0.0804)	
training:	Epoch: [24][76/233]	Loss 0.0635 (0.0802)	
training:	Epoch: [24][77/233]	Loss 0.0822 (0.0802)	
training:	Epoch: [24][78/233]	Loss 0.0743 (0.0801)	
training:	Epoch: [24][79/233]	Loss 0.0694 (0.0800)	
training:	Epoch: [24][80/233]	Loss 0.0733 (0.0799)	
training:	Epoch: [24][81/233]	Loss 0.0804 (0.0799)	
training:	Epoch: [24][82/233]	Loss 0.0738 (0.0798)	
training:	Epoch: [24][83/233]	Loss 0.0605 (0.0796)	
training:	Epoch: [24][84/233]	Loss 0.0783 (0.0796)	
training:	Epoch: [24][85/233]	Loss 0.0675 (0.0794)	
training:	Epoch: [24][86/233]	Loss 0.0693 (0.0793)	
training:	Epoch: [24][87/233]	Loss 0.1073 (0.0796)	
training:	Epoch: [24][88/233]	Loss 0.0659 (0.0795)	
training:	Epoch: [24][89/233]	Loss 0.0719 (0.0794)	
training:	Epoch: [24][90/233]	Loss 0.0850 (0.0795)	
training:	Epoch: [24][91/233]	Loss 0.0913 (0.0796)	
training:	Epoch: [24][92/233]	Loss 0.1000 (0.0798)	
training:	Epoch: [24][93/233]	Loss 0.0867 (0.0799)	
training:	Epoch: [24][94/233]	Loss 0.0839 (0.0799)	
training:	Epoch: [24][95/233]	Loss 0.0746 (0.0799)	
training:	Epoch: [24][96/233]	Loss 0.0683 (0.0797)	
training:	Epoch: [24][97/233]	Loss 0.0834 (0.0798)	
training:	Epoch: [24][98/233]	Loss 0.0871 (0.0799)	
training:	Epoch: [24][99/233]	Loss 0.0724 (0.0798)	
training:	Epoch: [24][100/233]	Loss 0.0659 (0.0796)	
training:	Epoch: [24][101/233]	Loss 0.0614 (0.0795)	
training:	Epoch: [24][102/233]	Loss 0.0632 (0.0793)	
training:	Epoch: [24][103/233]	Loss 0.0668 (0.0792)	
training:	Epoch: [24][104/233]	Loss 0.0623 (0.0790)	
training:	Epoch: [24][105/233]	Loss 0.1128 (0.0793)	
training:	Epoch: [24][106/233]	Loss 0.1248 (0.0798)	
training:	Epoch: [24][107/233]	Loss 0.0705 (0.0797)	
training:	Epoch: [24][108/233]	Loss 0.1030 (0.0799)	
training:	Epoch: [24][109/233]	Loss 0.0741 (0.0798)	
training:	Epoch: [24][110/233]	Loss 0.0982 (0.0800)	
training:	Epoch: [24][111/233]	Loss 0.0803 (0.0800)	
training:	Epoch: [24][112/233]	Loss 0.0632 (0.0799)	
training:	Epoch: [24][113/233]	Loss 0.0662 (0.0797)	
training:	Epoch: [24][114/233]	Loss 0.1390 (0.0803)	
training:	Epoch: [24][115/233]	Loss 0.0964 (0.0804)	
training:	Epoch: [24][116/233]	Loss 0.0689 (0.0803)	
training:	Epoch: [24][117/233]	Loss 0.1090 (0.0805)	
training:	Epoch: [24][118/233]	Loss 0.0615 (0.0804)	
training:	Epoch: [24][119/233]	Loss 0.0761 (0.0804)	
training:	Epoch: [24][120/233]	Loss 0.0695 (0.0803)	
training:	Epoch: [24][121/233]	Loss 0.0773 (0.0802)	
training:	Epoch: [24][122/233]	Loss 0.0826 (0.0803)	
training:	Epoch: [24][123/233]	Loss 0.0722 (0.0802)	
training:	Epoch: [24][124/233]	Loss 0.1053 (0.0804)	
training:	Epoch: [24][125/233]	Loss 0.1727 (0.0811)	
training:	Epoch: [24][126/233]	Loss 0.0854 (0.0812)	
training:	Epoch: [24][127/233]	Loss 0.0948 (0.0813)	
training:	Epoch: [24][128/233]	Loss 0.0658 (0.0812)	
training:	Epoch: [24][129/233]	Loss 0.0960 (0.0813)	
training:	Epoch: [24][130/233]	Loss 0.0689 (0.0812)	
training:	Epoch: [24][131/233]	Loss 0.0946 (0.0813)	
training:	Epoch: [24][132/233]	Loss 0.0722 (0.0812)	
training:	Epoch: [24][133/233]	Loss 0.0727 (0.0811)	
training:	Epoch: [24][134/233]	Loss 0.0614 (0.0810)	
training:	Epoch: [24][135/233]	Loss 0.0745 (0.0809)	
training:	Epoch: [24][136/233]	Loss 0.0724 (0.0809)	
training:	Epoch: [24][137/233]	Loss 0.0670 (0.0808)	
training:	Epoch: [24][138/233]	Loss 0.0946 (0.0809)	
training:	Epoch: [24][139/233]	Loss 0.1056 (0.0811)	
training:	Epoch: [24][140/233]	Loss 0.1227 (0.0814)	
training:	Epoch: [24][141/233]	Loss 0.0785 (0.0813)	
training:	Epoch: [24][142/233]	Loss 0.0705 (0.0813)	
training:	Epoch: [24][143/233]	Loss 0.0606 (0.0811)	
training:	Epoch: [24][144/233]	Loss 0.0890 (0.0812)	
training:	Epoch: [24][145/233]	Loss 0.0819 (0.0812)	
training:	Epoch: [24][146/233]	Loss 0.0796 (0.0812)	
training:	Epoch: [24][147/233]	Loss 0.0705 (0.0811)	
training:	Epoch: [24][148/233]	Loss 0.0735 (0.0810)	
training:	Epoch: [24][149/233]	Loss 0.0760 (0.0810)	
training:	Epoch: [24][150/233]	Loss 0.0639 (0.0809)	
training:	Epoch: [24][151/233]	Loss 0.0851 (0.0809)	
training:	Epoch: [24][152/233]	Loss 0.0737 (0.0809)	
training:	Epoch: [24][153/233]	Loss 0.0888 (0.0809)	
training:	Epoch: [24][154/233]	Loss 0.0682 (0.0808)	
training:	Epoch: [24][155/233]	Loss 0.1098 (0.0810)	
training:	Epoch: [24][156/233]	Loss 0.0763 (0.0810)	
training:	Epoch: [24][157/233]	Loss 0.0841 (0.0810)	
training:	Epoch: [24][158/233]	Loss 0.2030 (0.0818)	
training:	Epoch: [24][159/233]	Loss 0.0675 (0.0817)	
training:	Epoch: [24][160/233]	Loss 0.1069 (0.0819)	
training:	Epoch: [24][161/233]	Loss 0.0661 (0.0818)	
training:	Epoch: [24][162/233]	Loss 0.0976 (0.0819)	
training:	Epoch: [24][163/233]	Loss 0.0717 (0.0818)	
training:	Epoch: [24][164/233]	Loss 0.0720 (0.0817)	
training:	Epoch: [24][165/233]	Loss 0.0736 (0.0817)	
training:	Epoch: [24][166/233]	Loss 0.0788 (0.0817)	
training:	Epoch: [24][167/233]	Loss 0.0801 (0.0817)	
training:	Epoch: [24][168/233]	Loss 0.1467 (0.0820)	
training:	Epoch: [24][169/233]	Loss 0.0572 (0.0819)	
training:	Epoch: [24][170/233]	Loss 0.0689 (0.0818)	
training:	Epoch: [24][171/233]	Loss 0.1646 (0.0823)	
training:	Epoch: [24][172/233]	Loss 0.1116 (0.0825)	
training:	Epoch: [24][173/233]	Loss 0.0578 (0.0823)	
training:	Epoch: [24][174/233]	Loss 0.0754 (0.0823)	
training:	Epoch: [24][175/233]	Loss 0.0698 (0.0822)	
training:	Epoch: [24][176/233]	Loss 0.0962 (0.0823)	
training:	Epoch: [24][177/233]	Loss 0.0627 (0.0822)	
training:	Epoch: [24][178/233]	Loss 0.0837 (0.0822)	
training:	Epoch: [24][179/233]	Loss 0.0803 (0.0822)	
training:	Epoch: [24][180/233]	Loss 0.0758 (0.0822)	
training:	Epoch: [24][181/233]	Loss 0.0750 (0.0821)	
training:	Epoch: [24][182/233]	Loss 0.0728 (0.0821)	
training:	Epoch: [24][183/233]	Loss 0.0684 (0.0820)	
training:	Epoch: [24][184/233]	Loss 0.0593 (0.0819)	
training:	Epoch: [24][185/233]	Loss 0.0677 (0.0818)	
training:	Epoch: [24][186/233]	Loss 0.0875 (0.0818)	
training:	Epoch: [24][187/233]	Loss 0.0728 (0.0818)	
training:	Epoch: [24][188/233]	Loss 0.0697 (0.0817)	
training:	Epoch: [24][189/233]	Loss 0.1314 (0.0820)	
training:	Epoch: [24][190/233]	Loss 0.0774 (0.0819)	
training:	Epoch: [24][191/233]	Loss 0.0723 (0.0819)	
training:	Epoch: [24][192/233]	Loss 0.2816 (0.0829)	
training:	Epoch: [24][193/233]	Loss 0.0791 (0.0829)	
training:	Epoch: [24][194/233]	Loss 0.0607 (0.0828)	
training:	Epoch: [24][195/233]	Loss 0.0684 (0.0827)	
training:	Epoch: [24][196/233]	Loss 0.0973 (0.0828)	
training:	Epoch: [24][197/233]	Loss 0.0749 (0.0828)	
training:	Epoch: [24][198/233]	Loss 0.0713 (0.0827)	
training:	Epoch: [24][199/233]	Loss 0.0779 (0.0827)	
training:	Epoch: [24][200/233]	Loss 0.0733 (0.0826)	
training:	Epoch: [24][201/233]	Loss 0.0647 (0.0825)	
training:	Epoch: [24][202/233]	Loss 0.0875 (0.0826)	
training:	Epoch: [24][203/233]	Loss 0.1019 (0.0827)	
training:	Epoch: [24][204/233]	Loss 0.0818 (0.0827)	
training:	Epoch: [24][205/233]	Loss 0.0902 (0.0827)	
training:	Epoch: [24][206/233]	Loss 0.0886 (0.0827)	
training:	Epoch: [24][207/233]	Loss 0.0678 (0.0827)	
training:	Epoch: [24][208/233]	Loss 0.1094 (0.0828)	
training:	Epoch: [24][209/233]	Loss 0.0749 (0.0827)	
training:	Epoch: [24][210/233]	Loss 0.0601 (0.0826)	
training:	Epoch: [24][211/233]	Loss 0.0621 (0.0825)	
training:	Epoch: [24][212/233]	Loss 0.0643 (0.0825)	
training:	Epoch: [24][213/233]	Loss 0.0691 (0.0824)	
training:	Epoch: [24][214/233]	Loss 0.0875 (0.0824)	
training:	Epoch: [24][215/233]	Loss 0.0729 (0.0824)	
training:	Epoch: [24][216/233]	Loss 0.0816 (0.0824)	
training:	Epoch: [24][217/233]	Loss 0.1444 (0.0827)	
training:	Epoch: [24][218/233]	Loss 0.0853 (0.0827)	
training:	Epoch: [24][219/233]	Loss 0.0673 (0.0826)	
training:	Epoch: [24][220/233]	Loss 0.0724 (0.0825)	
training:	Epoch: [24][221/233]	Loss 0.0771 (0.0825)	
training:	Epoch: [24][222/233]	Loss 0.0607 (0.0824)	
training:	Epoch: [24][223/233]	Loss 0.0718 (0.0824)	
training:	Epoch: [24][224/233]	Loss 0.0680 (0.0823)	
training:	Epoch: [24][225/233]	Loss 0.0771 (0.0823)	
training:	Epoch: [24][226/233]	Loss 0.1368 (0.0825)	
training:	Epoch: [24][227/233]	Loss 0.1450 (0.0828)	
training:	Epoch: [24][228/233]	Loss 0.0662 (0.0827)	
training:	Epoch: [24][229/233]	Loss 0.0700 (0.0827)	
training:	Epoch: [24][230/233]	Loss 0.1140 (0.0828)	
training:	Epoch: [24][231/233]	Loss 0.0809 (0.0828)	
training:	Epoch: [24][232/233]	Loss 0.0911 (0.0828)	
training:	Epoch: [24][233/233]	Loss 0.0705 (0.0828)	
Training:	 Loss: 0.0826

Training:	 ACC: 1.0000 1.0000 1.0000 1.0000
Validation:	 ACC: 0.8623 0.8620 0.8547 0.8700
Validation:	 Best_BACC: 0.8647 0.8646 0.8639 0.8655
Validation:	 Loss: 0.3268
Pretraining:	Epoch 25/200
----------
training:	Epoch: [25][1/233]	Loss 0.0909 (0.0909)	
training:	Epoch: [25][2/233]	Loss 0.0910 (0.0909)	
training:	Epoch: [25][3/233]	Loss 0.0626 (0.0815)	
training:	Epoch: [25][4/233]	Loss 0.0543 (0.0747)	
training:	Epoch: [25][5/233]	Loss 0.0621 (0.0722)	
training:	Epoch: [25][6/233]	Loss 0.0698 (0.0718)	
training:	Epoch: [25][7/233]	Loss 0.0695 (0.0714)	
training:	Epoch: [25][8/233]	Loss 0.1556 (0.0820)	
training:	Epoch: [25][9/233]	Loss 0.0617 (0.0797)	
training:	Epoch: [25][10/233]	Loss 0.0695 (0.0787)	
training:	Epoch: [25][11/233]	Loss 0.0929 (0.0800)	
training:	Epoch: [25][12/233]	Loss 0.0662 (0.0788)	
training:	Epoch: [25][13/233]	Loss 0.0726 (0.0784)	
training:	Epoch: [25][14/233]	Loss 0.0763 (0.0782)	
training:	Epoch: [25][15/233]	Loss 0.0693 (0.0776)	
training:	Epoch: [25][16/233]	Loss 0.0804 (0.0778)	
training:	Epoch: [25][17/233]	Loss 0.1167 (0.0801)	
training:	Epoch: [25][18/233]	Loss 0.0617 (0.0791)	
training:	Epoch: [25][19/233]	Loss 0.0646 (0.0783)	
training:	Epoch: [25][20/233]	Loss 0.1050 (0.0796)	
training:	Epoch: [25][21/233]	Loss 0.0607 (0.0787)	
training:	Epoch: [25][22/233]	Loss 0.0696 (0.0783)	
training:	Epoch: [25][23/233]	Loss 0.0778 (0.0783)	
training:	Epoch: [25][24/233]	Loss 0.0640 (0.0777)	
training:	Epoch: [25][25/233]	Loss 0.0603 (0.0770)	
training:	Epoch: [25][26/233]	Loss 0.0766 (0.0770)	
training:	Epoch: [25][27/233]	Loss 0.0900 (0.0775)	
training:	Epoch: [25][28/233]	Loss 0.0803 (0.0776)	
training:	Epoch: [25][29/233]	Loss 0.0662 (0.0772)	
training:	Epoch: [25][30/233]	Loss 0.0834 (0.0774)	
training:	Epoch: [25][31/233]	Loss 0.0942 (0.0779)	
training:	Epoch: [25][32/233]	Loss 0.0553 (0.0772)	
training:	Epoch: [25][33/233]	Loss 0.1189 (0.0785)	
training:	Epoch: [25][34/233]	Loss 0.0876 (0.0787)	
training:	Epoch: [25][35/233]	Loss 0.0658 (0.0784)	
training:	Epoch: [25][36/233]	Loss 0.0679 (0.0781)	
training:	Epoch: [25][37/233]	Loss 0.0799 (0.0781)	
training:	Epoch: [25][38/233]	Loss 0.0640 (0.0778)	
training:	Epoch: [25][39/233]	Loss 0.0796 (0.0778)	
training:	Epoch: [25][40/233]	Loss 0.0996 (0.0784)	
training:	Epoch: [25][41/233]	Loss 0.0584 (0.0779)	
training:	Epoch: [25][42/233]	Loss 0.0571 (0.0774)	
training:	Epoch: [25][43/233]	Loss 0.0984 (0.0779)	
training:	Epoch: [25][44/233]	Loss 0.1057 (0.0785)	
training:	Epoch: [25][45/233]	Loss 0.0725 (0.0784)	
training:	Epoch: [25][46/233]	Loss 0.0716 (0.0782)	
training:	Epoch: [25][47/233]	Loss 0.0690 (0.0780)	
training:	Epoch: [25][48/233]	Loss 0.0642 (0.0777)	
training:	Epoch: [25][49/233]	Loss 0.1154 (0.0785)	
training:	Epoch: [25][50/233]	Loss 0.0742 (0.0784)	
training:	Epoch: [25][51/233]	Loss 0.0723 (0.0783)	
training:	Epoch: [25][52/233]	Loss 0.0686 (0.0781)	
training:	Epoch: [25][53/233]	Loss 0.0563 (0.0777)	
training:	Epoch: [25][54/233]	Loss 0.0598 (0.0774)	
training:	Epoch: [25][55/233]	Loss 0.1051 (0.0779)	
training:	Epoch: [25][56/233]	Loss 0.0685 (0.0777)	
training:	Epoch: [25][57/233]	Loss 0.0778 (0.0777)	
training:	Epoch: [25][58/233]	Loss 0.0565 (0.0773)	
training:	Epoch: [25][59/233]	Loss 0.0938 (0.0776)	
training:	Epoch: [25][60/233]	Loss 0.0706 (0.0775)	
training:	Epoch: [25][61/233]	Loss 0.0721 (0.0774)	
training:	Epoch: [25][62/233]	Loss 0.0704 (0.0773)	
training:	Epoch: [25][63/233]	Loss 0.0760 (0.0773)	
training:	Epoch: [25][64/233]	Loss 0.1195 (0.0779)	
training:	Epoch: [25][65/233]	Loss 0.0929 (0.0782)	
training:	Epoch: [25][66/233]	Loss 0.0639 (0.0780)	
training:	Epoch: [25][67/233]	Loss 0.0941 (0.0782)	
training:	Epoch: [25][68/233]	Loss 0.0779 (0.0782)	
training:	Epoch: [25][69/233]	Loss 0.0631 (0.0780)	
training:	Epoch: [25][70/233]	Loss 0.0707 (0.0779)	
training:	Epoch: [25][71/233]	Loss 0.1165 (0.0784)	
training:	Epoch: [25][72/233]	Loss 0.0731 (0.0783)	
training:	Epoch: [25][73/233]	Loss 0.0758 (0.0783)	
training:	Epoch: [25][74/233]	Loss 0.0802 (0.0783)	
training:	Epoch: [25][75/233]	Loss 0.0966 (0.0786)	
training:	Epoch: [25][76/233]	Loss 0.0728 (0.0785)	
training:	Epoch: [25][77/233]	Loss 0.0721 (0.0784)	
training:	Epoch: [25][78/233]	Loss 0.0650 (0.0782)	
training:	Epoch: [25][79/233]	Loss 0.0807 (0.0783)	
training:	Epoch: [25][80/233]	Loss 0.0764 (0.0782)	
training:	Epoch: [25][81/233]	Loss 0.0744 (0.0782)	
training:	Epoch: [25][82/233]	Loss 0.0906 (0.0784)	
training:	Epoch: [25][83/233]	Loss 0.0765 (0.0783)	
training:	Epoch: [25][84/233]	Loss 0.0755 (0.0783)	
training:	Epoch: [25][85/233]	Loss 0.1038 (0.0786)	
training:	Epoch: [25][86/233]	Loss 0.0625 (0.0784)	
training:	Epoch: [25][87/233]	Loss 0.0604 (0.0782)	
training:	Epoch: [25][88/233]	Loss 0.0684 (0.0781)	
training:	Epoch: [25][89/233]	Loss 0.1162 (0.0785)	
training:	Epoch: [25][90/233]	Loss 0.0634 (0.0784)	
training:	Epoch: [25][91/233]	Loss 0.0717 (0.0783)	
training:	Epoch: [25][92/233]	Loss 0.0712 (0.0782)	
training:	Epoch: [25][93/233]	Loss 0.0935 (0.0784)	
training:	Epoch: [25][94/233]	Loss 0.0984 (0.0786)	
training:	Epoch: [25][95/233]	Loss 0.0909 (0.0787)	
training:	Epoch: [25][96/233]	Loss 0.0518 (0.0784)	
training:	Epoch: [25][97/233]	Loss 0.1540 (0.0792)	
training:	Epoch: [25][98/233]	Loss 0.0830 (0.0792)	
training:	Epoch: [25][99/233]	Loss 0.1657 (0.0801)	
training:	Epoch: [25][100/233]	Loss 0.0686 (0.0800)	
training:	Epoch: [25][101/233]	Loss 0.0846 (0.0801)	
training:	Epoch: [25][102/233]	Loss 0.0570 (0.0798)	
training:	Epoch: [25][103/233]	Loss 0.0857 (0.0799)	
training:	Epoch: [25][104/233]	Loss 0.1069 (0.0801)	
training:	Epoch: [25][105/233]	Loss 0.0623 (0.0800)	
training:	Epoch: [25][106/233]	Loss 0.0708 (0.0799)	
training:	Epoch: [25][107/233]	Loss 0.0802 (0.0799)	
training:	Epoch: [25][108/233]	Loss 0.0703 (0.0798)	
training:	Epoch: [25][109/233]	Loss 0.0799 (0.0798)	
training:	Epoch: [25][110/233]	Loss 0.0743 (0.0798)	
training:	Epoch: [25][111/233]	Loss 0.0640 (0.0796)	
training:	Epoch: [25][112/233]	Loss 0.0769 (0.0796)	
training:	Epoch: [25][113/233]	Loss 0.0731 (0.0795)	
training:	Epoch: [25][114/233]	Loss 0.0799 (0.0795)	
training:	Epoch: [25][115/233]	Loss 0.0881 (0.0796)	
training:	Epoch: [25][116/233]	Loss 0.0670 (0.0795)	
training:	Epoch: [25][117/233]	Loss 0.0749 (0.0795)	
training:	Epoch: [25][118/233]	Loss 0.0720 (0.0794)	
training:	Epoch: [25][119/233]	Loss 0.0756 (0.0794)	
training:	Epoch: [25][120/233]	Loss 0.0715 (0.0793)	
training:	Epoch: [25][121/233]	Loss 0.0542 (0.0791)	
training:	Epoch: [25][122/233]	Loss 0.0677 (0.0790)	
training:	Epoch: [25][123/233]	Loss 0.0669 (0.0789)	
training:	Epoch: [25][124/233]	Loss 0.0648 (0.0788)	
training:	Epoch: [25][125/233]	Loss 0.0759 (0.0788)	
training:	Epoch: [25][126/233]	Loss 0.1589 (0.0794)	
training:	Epoch: [25][127/233]	Loss 0.0756 (0.0794)	
training:	Epoch: [25][128/233]	Loss 0.0712 (0.0793)	
training:	Epoch: [25][129/233]	Loss 0.0715 (0.0792)	
training:	Epoch: [25][130/233]	Loss 0.1127 (0.0795)	
training:	Epoch: [25][131/233]	Loss 0.0681 (0.0794)	
training:	Epoch: [25][132/233]	Loss 0.0711 (0.0794)	
training:	Epoch: [25][133/233]	Loss 0.0568 (0.0792)	
training:	Epoch: [25][134/233]	Loss 0.0669 (0.0791)	
training:	Epoch: [25][135/233]	Loss 0.0588 (0.0789)	
training:	Epoch: [25][136/233]	Loss 0.0722 (0.0789)	
training:	Epoch: [25][137/233]	Loss 0.1602 (0.0795)	
training:	Epoch: [25][138/233]	Loss 0.0621 (0.0794)	
training:	Epoch: [25][139/233]	Loss 0.0703 (0.0793)	
training:	Epoch: [25][140/233]	Loss 0.0664 (0.0792)	
training:	Epoch: [25][141/233]	Loss 0.0699 (0.0791)	
training:	Epoch: [25][142/233]	Loss 0.0896 (0.0792)	
training:	Epoch: [25][143/233]	Loss 0.1100 (0.0794)	
training:	Epoch: [25][144/233]	Loss 0.0762 (0.0794)	
training:	Epoch: [25][145/233]	Loss 0.0541 (0.0792)	
training:	Epoch: [25][146/233]	Loss 0.0683 (0.0792)	
training:	Epoch: [25][147/233]	Loss 0.1312 (0.0795)	
training:	Epoch: [25][148/233]	Loss 0.0720 (0.0795)	
training:	Epoch: [25][149/233]	Loss 0.0902 (0.0795)	
training:	Epoch: [25][150/233]	Loss 0.0691 (0.0795)	
training:	Epoch: [25][151/233]	Loss 0.0683 (0.0794)	
training:	Epoch: [25][152/233]	Loss 0.0627 (0.0793)	
training:	Epoch: [25][153/233]	Loss 0.0868 (0.0793)	
training:	Epoch: [25][154/233]	Loss 0.0767 (0.0793)	
training:	Epoch: [25][155/233]	Loss 0.0739 (0.0793)	
training:	Epoch: [25][156/233]	Loss 0.0707 (0.0792)	
training:	Epoch: [25][157/233]	Loss 0.0681 (0.0791)	
training:	Epoch: [25][158/233]	Loss 0.0659 (0.0791)	
training:	Epoch: [25][159/233]	Loss 0.0652 (0.0790)	
training:	Epoch: [25][160/233]	Loss 0.0588 (0.0788)	
training:	Epoch: [25][161/233]	Loss 0.0762 (0.0788)	
training:	Epoch: [25][162/233]	Loss 0.0691 (0.0788)	
training:	Epoch: [25][163/233]	Loss 0.0684 (0.0787)	
training:	Epoch: [25][164/233]	Loss 0.0601 (0.0786)	
training:	Epoch: [25][165/233]	Loss 0.0635 (0.0785)	
training:	Epoch: [25][166/233]	Loss 0.0869 (0.0786)	
training:	Epoch: [25][167/233]	Loss 0.0673 (0.0785)	
training:	Epoch: [25][168/233]	Loss 0.0837 (0.0785)	
training:	Epoch: [25][169/233]	Loss 0.0457 (0.0783)	
training:	Epoch: [25][170/233]	Loss 0.0706 (0.0783)	
training:	Epoch: [25][171/233]	Loss 0.0652 (0.0782)	
training:	Epoch: [25][172/233]	Loss 0.0819 (0.0782)	
training:	Epoch: [25][173/233]	Loss 0.0803 (0.0782)	
training:	Epoch: [25][174/233]	Loss 0.0696 (0.0782)	
training:	Epoch: [25][175/233]	Loss 0.0649 (0.0781)	
training:	Epoch: [25][176/233]	Loss 0.0624 (0.0780)	
training:	Epoch: [25][177/233]	Loss 0.0667 (0.0780)	
training:	Epoch: [25][178/233]	Loss 0.0823 (0.0780)	
training:	Epoch: [25][179/233]	Loss 0.0606 (0.0779)	
training:	Epoch: [25][180/233]	Loss 0.0632 (0.0778)	
training:	Epoch: [25][181/233]	Loss 0.0723 (0.0778)	
training:	Epoch: [25][182/233]	Loss 0.0698 (0.0777)	
training:	Epoch: [25][183/233]	Loss 0.0821 (0.0778)	
training:	Epoch: [25][184/233]	Loss 0.0655 (0.0777)	
training:	Epoch: [25][185/233]	Loss 0.0837 (0.0777)	
training:	Epoch: [25][186/233]	Loss 0.0790 (0.0777)	
training:	Epoch: [25][187/233]	Loss 0.0518 (0.0776)	
training:	Epoch: [25][188/233]	Loss 0.0778 (0.0776)	
training:	Epoch: [25][189/233]	Loss 0.0760 (0.0776)	
training:	Epoch: [25][190/233]	Loss 0.0624 (0.0775)	
training:	Epoch: [25][191/233]	Loss 0.0857 (0.0775)	
training:	Epoch: [25][192/233]	Loss 0.0708 (0.0775)	
training:	Epoch: [25][193/233]	Loss 0.0647 (0.0774)	
training:	Epoch: [25][194/233]	Loss 0.0711 (0.0774)	
training:	Epoch: [25][195/233]	Loss 0.1120 (0.0776)	
training:	Epoch: [25][196/233]	Loss 0.0712 (0.0776)	
training:	Epoch: [25][197/233]	Loss 0.0699 (0.0775)	
training:	Epoch: [25][198/233]	Loss 0.0667 (0.0775)	
training:	Epoch: [25][199/233]	Loss 0.0726 (0.0774)	
training:	Epoch: [25][200/233]	Loss 0.0696 (0.0774)	
training:	Epoch: [25][201/233]	Loss 0.1127 (0.0776)	
training:	Epoch: [25][202/233]	Loss 0.0629 (0.0775)	
training:	Epoch: [25][203/233]	Loss 0.1146 (0.0777)	
training:	Epoch: [25][204/233]	Loss 0.0610 (0.0776)	
training:	Epoch: [25][205/233]	Loss 0.0679 (0.0776)	
training:	Epoch: [25][206/233]	Loss 0.0601 (0.0775)	
training:	Epoch: [25][207/233]	Loss 0.0740 (0.0775)	
training:	Epoch: [25][208/233]	Loss 0.0812 (0.0775)	
training:	Epoch: [25][209/233]	Loss 0.0711 (0.0774)	
training:	Epoch: [25][210/233]	Loss 0.0712 (0.0774)	
training:	Epoch: [25][211/233]	Loss 0.0682 (0.0774)	
training:	Epoch: [25][212/233]	Loss 0.0933 (0.0774)	
training:	Epoch: [25][213/233]	Loss 0.0971 (0.0775)	
training:	Epoch: [25][214/233]	Loss 0.0531 (0.0774)	
training:	Epoch: [25][215/233]	Loss 0.0665 (0.0774)	
training:	Epoch: [25][216/233]	Loss 0.0739 (0.0774)	
training:	Epoch: [25][217/233]	Loss 0.0968 (0.0774)	
training:	Epoch: [25][218/233]	Loss 0.0750 (0.0774)	
training:	Epoch: [25][219/233]	Loss 0.0707 (0.0774)	
training:	Epoch: [25][220/233]	Loss 0.0571 (0.0773)	
training:	Epoch: [25][221/233]	Loss 0.1096 (0.0775)	
training:	Epoch: [25][222/233]	Loss 0.0640 (0.0774)	
training:	Epoch: [25][223/233]	Loss 0.0728 (0.0774)	
training:	Epoch: [25][224/233]	Loss 0.0625 (0.0773)	
training:	Epoch: [25][225/233]	Loss 0.0704 (0.0773)	
training:	Epoch: [25][226/233]	Loss 0.0737 (0.0773)	
training:	Epoch: [25][227/233]	Loss 0.0686 (0.0772)	
training:	Epoch: [25][228/233]	Loss 0.0626 (0.0772)	
training:	Epoch: [25][229/233]	Loss 0.0763 (0.0772)	
training:	Epoch: [25][230/233]	Loss 0.0970 (0.0772)	
training:	Epoch: [25][231/233]	Loss 0.0869 (0.0773)	
training:	Epoch: [25][232/233]	Loss 0.0625 (0.0772)	
training:	Epoch: [25][233/233]	Loss 0.0615 (0.0771)	
Training:	 Loss: 0.0770

Training:	 ACC: 1.0000 1.0000 1.0000 1.0000
Validation:	 ACC: 0.8600 0.8604 0.8669 0.8531
Validation:	 Best_BACC: 0.8647 0.8646 0.8639 0.8655
Validation:	 Loss: 0.3214
Pretraining:	Epoch 26/200
----------
training:	Epoch: [26][1/233]	Loss 0.0940 (0.0940)	
training:	Epoch: [26][2/233]	Loss 0.0617 (0.0779)	
training:	Epoch: [26][3/233]	Loss 0.0692 (0.0750)	
training:	Epoch: [26][4/233]	Loss 0.1089 (0.0835)	
training:	Epoch: [26][5/233]	Loss 0.0619 (0.0792)	
training:	Epoch: [26][6/233]	Loss 0.0657 (0.0769)	
training:	Epoch: [26][7/233]	Loss 0.0814 (0.0775)	
training:	Epoch: [26][8/233]	Loss 0.0528 (0.0745)	
training:	Epoch: [26][9/233]	Loss 0.0647 (0.0734)	
training:	Epoch: [26][10/233]	Loss 0.0548 (0.0715)	
training:	Epoch: [26][11/233]	Loss 0.0695 (0.0713)	
training:	Epoch: [26][12/233]	Loss 0.0547 (0.0699)	
training:	Epoch: [26][13/233]	Loss 0.0618 (0.0693)	
training:	Epoch: [26][14/233]	Loss 0.0486 (0.0678)	
training:	Epoch: [26][15/233]	Loss 0.0673 (0.0678)	
training:	Epoch: [26][16/233]	Loss 0.0737 (0.0682)	
training:	Epoch: [26][17/233]	Loss 0.0981 (0.0699)	
training:	Epoch: [26][18/233]	Loss 0.0593 (0.0693)	
training:	Epoch: [26][19/233]	Loss 0.0638 (0.0690)	
training:	Epoch: [26][20/233]	Loss 0.0729 (0.0692)	
training:	Epoch: [26][21/233]	Loss 0.0894 (0.0702)	
training:	Epoch: [26][22/233]	Loss 0.0612 (0.0698)	
training:	Epoch: [26][23/233]	Loss 0.0647 (0.0696)	
training:	Epoch: [26][24/233]	Loss 0.0565 (0.0690)	
training:	Epoch: [26][25/233]	Loss 0.1665 (0.0729)	
training:	Epoch: [26][26/233]	Loss 0.0697 (0.0728)	
training:	Epoch: [26][27/233]	Loss 0.0662 (0.0726)	
training:	Epoch: [26][28/233]	Loss 0.0844 (0.0730)	
training:	Epoch: [26][29/233]	Loss 0.0875 (0.0735)	
training:	Epoch: [26][30/233]	Loss 0.0611 (0.0731)	
training:	Epoch: [26][31/233]	Loss 0.0681 (0.0729)	
training:	Epoch: [26][32/233]	Loss 0.0686 (0.0728)	
training:	Epoch: [26][33/233]	Loss 0.0633 (0.0725)	
training:	Epoch: [26][34/233]	Loss 0.0714 (0.0725)	
training:	Epoch: [26][35/233]	Loss 0.1049 (0.0734)	
training:	Epoch: [26][36/233]	Loss 0.0678 (0.0732)	
training:	Epoch: [26][37/233]	Loss 0.0697 (0.0731)	
training:	Epoch: [26][38/233]	Loss 0.0751 (0.0732)	
training:	Epoch: [26][39/233]	Loss 0.0921 (0.0737)	
training:	Epoch: [26][40/233]	Loss 0.0730 (0.0737)	
training:	Epoch: [26][41/233]	Loss 0.0662 (0.0735)	
training:	Epoch: [26][42/233]	Loss 0.0532 (0.0730)	
training:	Epoch: [26][43/233]	Loss 0.0684 (0.0729)	
training:	Epoch: [26][44/233]	Loss 0.0639 (0.0727)	
training:	Epoch: [26][45/233]	Loss 0.0822 (0.0729)	
training:	Epoch: [26][46/233]	Loss 0.0765 (0.0730)	
training:	Epoch: [26][47/233]	Loss 0.0597 (0.0727)	
training:	Epoch: [26][48/233]	Loss 0.0978 (0.0732)	
training:	Epoch: [26][49/233]	Loss 0.0582 (0.0729)	
training:	Epoch: [26][50/233]	Loss 0.0635 (0.0727)	
training:	Epoch: [26][51/233]	Loss 0.0819 (0.0729)	
training:	Epoch: [26][52/233]	Loss 0.0694 (0.0728)	
training:	Epoch: [26][53/233]	Loss 0.1031 (0.0734)	
training:	Epoch: [26][54/233]	Loss 0.0681 (0.0733)	
training:	Epoch: [26][55/233]	Loss 0.0728 (0.0733)	
training:	Epoch: [26][56/233]	Loss 0.0508 (0.0729)	
training:	Epoch: [26][57/233]	Loss 0.0587 (0.0726)	
training:	Epoch: [26][58/233]	Loss 0.0632 (0.0725)	
training:	Epoch: [26][59/233]	Loss 0.0641 (0.0723)	
training:	Epoch: [26][60/233]	Loss 0.1093 (0.0730)	
training:	Epoch: [26][61/233]	Loss 0.0823 (0.0731)	
training:	Epoch: [26][62/233]	Loss 0.0554 (0.0728)	
training:	Epoch: [26][63/233]	Loss 0.0619 (0.0726)	
training:	Epoch: [26][64/233]	Loss 0.0663 (0.0725)	
training:	Epoch: [26][65/233]	Loss 0.0787 (0.0726)	
training:	Epoch: [26][66/233]	Loss 0.0638 (0.0725)	
training:	Epoch: [26][67/233]	Loss 0.0703 (0.0725)	
training:	Epoch: [26][68/233]	Loss 0.0844 (0.0727)	
training:	Epoch: [26][69/233]	Loss 0.1052 (0.0731)	
training:	Epoch: [26][70/233]	Loss 0.0591 (0.0729)	
training:	Epoch: [26][71/233]	Loss 0.0646 (0.0728)	
training:	Epoch: [26][72/233]	Loss 0.0511 (0.0725)	
training:	Epoch: [26][73/233]	Loss 0.0855 (0.0727)	
training:	Epoch: [26][74/233]	Loss 0.0599 (0.0725)	
training:	Epoch: [26][75/233]	Loss 0.0679 (0.0724)	
training:	Epoch: [26][76/233]	Loss 0.0563 (0.0722)	
training:	Epoch: [26][77/233]	Loss 0.0729 (0.0722)	
training:	Epoch: [26][78/233]	Loss 0.0556 (0.0720)	
training:	Epoch: [26][79/233]	Loss 0.0593 (0.0719)	
training:	Epoch: [26][80/233]	Loss 0.0725 (0.0719)	
training:	Epoch: [26][81/233]	Loss 0.0712 (0.0719)	
training:	Epoch: [26][82/233]	Loss 0.0629 (0.0718)	
training:	Epoch: [26][83/233]	Loss 0.0707 (0.0717)	
training:	Epoch: [26][84/233]	Loss 0.0608 (0.0716)	
training:	Epoch: [26][85/233]	Loss 0.0560 (0.0714)	
training:	Epoch: [26][86/233]	Loss 0.1542 (0.0724)	
training:	Epoch: [26][87/233]	Loss 0.1049 (0.0728)	
training:	Epoch: [26][88/233]	Loss 0.0426 (0.0724)	
training:	Epoch: [26][89/233]	Loss 0.0587 (0.0723)	
training:	Epoch: [26][90/233]	Loss 0.0735 (0.0723)	
training:	Epoch: [26][91/233]	Loss 0.0608 (0.0722)	
training:	Epoch: [26][92/233]	Loss 0.0619 (0.0720)	
training:	Epoch: [26][93/233]	Loss 0.0508 (0.0718)	
training:	Epoch: [26][94/233]	Loss 0.0808 (0.0719)	
training:	Epoch: [26][95/233]	Loss 0.0554 (0.0717)	
training:	Epoch: [26][96/233]	Loss 0.0755 (0.0718)	
training:	Epoch: [26][97/233]	Loss 0.0586 (0.0716)	
training:	Epoch: [26][98/233]	Loss 0.0593 (0.0715)	
training:	Epoch: [26][99/233]	Loss 0.0728 (0.0715)	
training:	Epoch: [26][100/233]	Loss 0.0631 (0.0714)	
training:	Epoch: [26][101/233]	Loss 0.0599 (0.0713)	
training:	Epoch: [26][102/233]	Loss 0.0619 (0.0712)	
training:	Epoch: [26][103/233]	Loss 0.0846 (0.0714)	
training:	Epoch: [26][104/233]	Loss 0.0690 (0.0713)	
training:	Epoch: [26][105/233]	Loss 0.0632 (0.0713)	
training:	Epoch: [26][106/233]	Loss 0.0640 (0.0712)	
training:	Epoch: [26][107/233]	Loss 0.0885 (0.0714)	
training:	Epoch: [26][108/233]	Loss 0.0600 (0.0713)	
training:	Epoch: [26][109/233]	Loss 0.0697 (0.0712)	
training:	Epoch: [26][110/233]	Loss 0.0691 (0.0712)	
training:	Epoch: [26][111/233]	Loss 0.0613 (0.0711)	
training:	Epoch: [26][112/233]	Loss 0.0561 (0.0710)	
training:	Epoch: [26][113/233]	Loss 0.0515 (0.0708)	
training:	Epoch: [26][114/233]	Loss 0.0695 (0.0708)	
training:	Epoch: [26][115/233]	Loss 0.0628 (0.0707)	
training:	Epoch: [26][116/233]	Loss 0.0752 (0.0708)	
training:	Epoch: [26][117/233]	Loss 0.1213 (0.0712)	
training:	Epoch: [26][118/233]	Loss 0.0643 (0.0712)	
training:	Epoch: [26][119/233]	Loss 0.0688 (0.0711)	
training:	Epoch: [26][120/233]	Loss 0.0781 (0.0712)	
training:	Epoch: [26][121/233]	Loss 0.0703 (0.0712)	
training:	Epoch: [26][122/233]	Loss 0.0683 (0.0712)	
training:	Epoch: [26][123/233]	Loss 0.0908 (0.0713)	
training:	Epoch: [26][124/233]	Loss 0.0534 (0.0712)	
training:	Epoch: [26][125/233]	Loss 0.0709 (0.0712)	
training:	Epoch: [26][126/233]	Loss 0.0575 (0.0711)	
training:	Epoch: [26][127/233]	Loss 0.0620 (0.0710)	
training:	Epoch: [26][128/233]	Loss 0.0663 (0.0710)	
training:	Epoch: [26][129/233]	Loss 0.0539 (0.0708)	
training:	Epoch: [26][130/233]	Loss 0.0645 (0.0708)	
training:	Epoch: [26][131/233]	Loss 0.0701 (0.0708)	
training:	Epoch: [26][132/233]	Loss 0.0661 (0.0707)	
training:	Epoch: [26][133/233]	Loss 0.0503 (0.0706)	
training:	Epoch: [26][134/233]	Loss 0.0646 (0.0705)	
training:	Epoch: [26][135/233]	Loss 0.0896 (0.0707)	
training:	Epoch: [26][136/233]	Loss 0.0695 (0.0707)	
training:	Epoch: [26][137/233]	Loss 0.0539 (0.0705)	
training:	Epoch: [26][138/233]	Loss 0.0741 (0.0706)	
training:	Epoch: [26][139/233]	Loss 0.0738 (0.0706)	
training:	Epoch: [26][140/233]	Loss 0.0672 (0.0706)	
training:	Epoch: [26][141/233]	Loss 0.0605 (0.0705)	
training:	Epoch: [26][142/233]	Loss 0.0975 (0.0707)	
training:	Epoch: [26][143/233]	Loss 0.0770 (0.0707)	
training:	Epoch: [26][144/233]	Loss 0.1147 (0.0710)	
training:	Epoch: [26][145/233]	Loss 0.0542 (0.0709)	
training:	Epoch: [26][146/233]	Loss 0.0601 (0.0709)	
training:	Epoch: [26][147/233]	Loss 0.0793 (0.0709)	
training:	Epoch: [26][148/233]	Loss 0.0567 (0.0708)	
training:	Epoch: [26][149/233]	Loss 0.0757 (0.0708)	
training:	Epoch: [26][150/233]	Loss 0.0535 (0.0707)	
training:	Epoch: [26][151/233]	Loss 0.0663 (0.0707)	
training:	Epoch: [26][152/233]	Loss 0.0757 (0.0707)	
training:	Epoch: [26][153/233]	Loss 0.0601 (0.0707)	
training:	Epoch: [26][154/233]	Loss 0.0850 (0.0708)	
training:	Epoch: [26][155/233]	Loss 0.0932 (0.0709)	
training:	Epoch: [26][156/233]	Loss 0.0764 (0.0709)	
training:	Epoch: [26][157/233]	Loss 0.0683 (0.0709)	
training:	Epoch: [26][158/233]	Loss 0.0945 (0.0711)	
training:	Epoch: [26][159/233]	Loss 0.0973 (0.0712)	
training:	Epoch: [26][160/233]	Loss 0.0507 (0.0711)	
training:	Epoch: [26][161/233]	Loss 0.0694 (0.0711)	
training:	Epoch: [26][162/233]	Loss 0.0507 (0.0710)	
training:	Epoch: [26][163/233]	Loss 0.0678 (0.0710)	
training:	Epoch: [26][164/233]	Loss 0.0712 (0.0710)	
training:	Epoch: [26][165/233]	Loss 0.0848 (0.0710)	
training:	Epoch: [26][166/233]	Loss 0.0679 (0.0710)	
training:	Epoch: [26][167/233]	Loss 0.0608 (0.0710)	
training:	Epoch: [26][168/233]	Loss 0.0828 (0.0710)	
training:	Epoch: [26][169/233]	Loss 0.0579 (0.0709)	
training:	Epoch: [26][170/233]	Loss 0.0696 (0.0709)	
training:	Epoch: [26][171/233]	Loss 0.1205 (0.0712)	
training:	Epoch: [26][172/233]	Loss 0.1027 (0.0714)	
training:	Epoch: [26][173/233]	Loss 0.0678 (0.0714)	
training:	Epoch: [26][174/233]	Loss 0.0698 (0.0714)	
training:	Epoch: [26][175/233]	Loss 0.1095 (0.0716)	
training:	Epoch: [26][176/233]	Loss 0.1707 (0.0722)	
training:	Epoch: [26][177/233]	Loss 0.0521 (0.0721)	
training:	Epoch: [26][178/233]	Loss 0.0777 (0.0721)	
training:	Epoch: [26][179/233]	Loss 0.0600 (0.0720)	
training:	Epoch: [26][180/233]	Loss 0.0688 (0.0720)	
training:	Epoch: [26][181/233]	Loss 0.0590 (0.0719)	
training:	Epoch: [26][182/233]	Loss 0.0685 (0.0719)	
training:	Epoch: [26][183/233]	Loss 0.0789 (0.0719)	
training:	Epoch: [26][184/233]	Loss 0.0581 (0.0719)	
training:	Epoch: [26][185/233]	Loss 0.0791 (0.0719)	
training:	Epoch: [26][186/233]	Loss 0.0799 (0.0720)	
training:	Epoch: [26][187/233]	Loss 0.0554 (0.0719)	
training:	Epoch: [26][188/233]	Loss 0.0780 (0.0719)	
training:	Epoch: [26][189/233]	Loss 0.0683 (0.0719)	
training:	Epoch: [26][190/233]	Loss 0.0553 (0.0718)	
training:	Epoch: [26][191/233]	Loss 0.0533 (0.0717)	
training:	Epoch: [26][192/233]	Loss 0.0542 (0.0716)	
training:	Epoch: [26][193/233]	Loss 0.0732 (0.0716)	
training:	Epoch: [26][194/233]	Loss 0.1783 (0.0722)	
training:	Epoch: [26][195/233]	Loss 0.0673 (0.0721)	
training:	Epoch: [26][196/233]	Loss 0.0614 (0.0721)	
training:	Epoch: [26][197/233]	Loss 0.0556 (0.0720)	
training:	Epoch: [26][198/233]	Loss 0.0607 (0.0719)	
training:	Epoch: [26][199/233]	Loss 0.0868 (0.0720)	
training:	Epoch: [26][200/233]	Loss 0.0472 (0.0719)	
training:	Epoch: [26][201/233]	Loss 0.0869 (0.0720)	
training:	Epoch: [26][202/233]	Loss 0.0733 (0.0720)	
training:	Epoch: [26][203/233]	Loss 0.0718 (0.0720)	
training:	Epoch: [26][204/233]	Loss 0.0719 (0.0720)	
training:	Epoch: [26][205/233]	Loss 0.0913 (0.0721)	
training:	Epoch: [26][206/233]	Loss 0.0563 (0.0720)	
training:	Epoch: [26][207/233]	Loss 0.1059 (0.0722)	
training:	Epoch: [26][208/233]	Loss 0.0774 (0.0722)	
training:	Epoch: [26][209/233]	Loss 0.0773 (0.0722)	
training:	Epoch: [26][210/233]	Loss 0.0687 (0.0722)	
training:	Epoch: [26][211/233]	Loss 0.0604 (0.0721)	
training:	Epoch: [26][212/233]	Loss 0.0699 (0.0721)	
training:	Epoch: [26][213/233]	Loss 0.0779 (0.0721)	
training:	Epoch: [26][214/233]	Loss 0.0841 (0.0722)	
training:	Epoch: [26][215/233]	Loss 0.0652 (0.0722)	
training:	Epoch: [26][216/233]	Loss 0.0932 (0.0723)	
training:	Epoch: [26][217/233]	Loss 0.0621 (0.0722)	
training:	Epoch: [26][218/233]	Loss 0.0661 (0.0722)	
training:	Epoch: [26][219/233]	Loss 0.1072 (0.0724)	
training:	Epoch: [26][220/233]	Loss 0.0657 (0.0723)	
training:	Epoch: [26][221/233]	Loss 0.0679 (0.0723)	
training:	Epoch: [26][222/233]	Loss 0.0946 (0.0724)	
training:	Epoch: [26][223/233]	Loss 0.0887 (0.0725)	
training:	Epoch: [26][224/233]	Loss 0.0891 (0.0725)	
training:	Epoch: [26][225/233]	Loss 0.0665 (0.0725)	
training:	Epoch: [26][226/233]	Loss 0.0608 (0.0725)	
training:	Epoch: [26][227/233]	Loss 0.0596 (0.0724)	
training:	Epoch: [26][228/233]	Loss 0.0558 (0.0723)	
training:	Epoch: [26][229/233]	Loss 0.0881 (0.0724)	
training:	Epoch: [26][230/233]	Loss 0.0996 (0.0725)	
training:	Epoch: [26][231/233]	Loss 0.0634 (0.0725)	
training:	Epoch: [26][232/233]	Loss 0.0774 (0.0725)	
training:	Epoch: [26][233/233]	Loss 0.0695 (0.0725)	
Training:	 Loss: 0.0723

Training:	 ACC: 1.0000 1.0000 1.0000 1.0000
Validation:	 ACC: 0.8627 0.8614 0.8352 0.8901
Validation:	 Best_BACC: 0.8647 0.8646 0.8639 0.8655
Validation:	 Loss: 0.3302
Pretraining:	Epoch 27/200
----------
training:	Epoch: [27][1/233]	Loss 0.0587 (0.0587)	
training:	Epoch: [27][2/233]	Loss 0.0708 (0.0648)	
training:	Epoch: [27][3/233]	Loss 0.0605 (0.0633)	
training:	Epoch: [27][4/233]	Loss 0.0522 (0.0606)	
training:	Epoch: [27][5/233]	Loss 0.0582 (0.0601)	
training:	Epoch: [27][6/233]	Loss 0.0771 (0.0629)	
training:	Epoch: [27][7/233]	Loss 0.0638 (0.0630)	
training:	Epoch: [27][8/233]	Loss 0.1104 (0.0690)	
training:	Epoch: [27][9/233]	Loss 0.0553 (0.0674)	
training:	Epoch: [27][10/233]	Loss 0.0966 (0.0704)	
training:	Epoch: [27][11/233]	Loss 0.0640 (0.0698)	
training:	Epoch: [27][12/233]	Loss 0.0693 (0.0697)	
training:	Epoch: [27][13/233]	Loss 0.0968 (0.0718)	
training:	Epoch: [27][14/233]	Loss 0.1284 (0.0759)	
training:	Epoch: [27][15/233]	Loss 0.0683 (0.0754)	
training:	Epoch: [27][16/233]	Loss 0.0739 (0.0753)	
training:	Epoch: [27][17/233]	Loss 0.0629 (0.0745)	
training:	Epoch: [27][18/233]	Loss 0.0636 (0.0739)	
training:	Epoch: [27][19/233]	Loss 0.0537 (0.0729)	
training:	Epoch: [27][20/233]	Loss 0.0462 (0.0715)	
training:	Epoch: [27][21/233]	Loss 0.0691 (0.0714)	
training:	Epoch: [27][22/233]	Loss 0.0571 (0.0708)	
training:	Epoch: [27][23/233]	Loss 0.0621 (0.0704)	
training:	Epoch: [27][24/233]	Loss 0.0515 (0.0696)	
training:	Epoch: [27][25/233]	Loss 0.0599 (0.0692)	
training:	Epoch: [27][26/233]	Loss 0.0746 (0.0694)	
training:	Epoch: [27][27/233]	Loss 0.0930 (0.0703)	
training:	Epoch: [27][28/233]	Loss 0.0917 (0.0711)	
training:	Epoch: [27][29/233]	Loss 0.0617 (0.0707)	
training:	Epoch: [27][30/233]	Loss 0.0654 (0.0706)	
training:	Epoch: [27][31/233]	Loss 0.0552 (0.0701)	
training:	Epoch: [27][32/233]	Loss 0.0545 (0.0696)	
training:	Epoch: [27][33/233]	Loss 0.0647 (0.0694)	
training:	Epoch: [27][34/233]	Loss 0.0606 (0.0692)	
training:	Epoch: [27][35/233]	Loss 0.0753 (0.0693)	
training:	Epoch: [27][36/233]	Loss 0.0831 (0.0697)	
training:	Epoch: [27][37/233]	Loss 0.0644 (0.0696)	
training:	Epoch: [27][38/233]	Loss 0.0619 (0.0694)	
training:	Epoch: [27][39/233]	Loss 0.0651 (0.0693)	
training:	Epoch: [27][40/233]	Loss 0.0570 (0.0690)	
training:	Epoch: [27][41/233]	Loss 0.0552 (0.0686)	
training:	Epoch: [27][42/233]	Loss 0.0584 (0.0684)	
training:	Epoch: [27][43/233]	Loss 0.0747 (0.0685)	
training:	Epoch: [27][44/233]	Loss 0.1110 (0.0695)	
training:	Epoch: [27][45/233]	Loss 0.0566 (0.0692)	
training:	Epoch: [27][46/233]	Loss 0.0653 (0.0691)	
training:	Epoch: [27][47/233]	Loss 0.0630 (0.0690)	
training:	Epoch: [27][48/233]	Loss 0.0658 (0.0689)	
training:	Epoch: [27][49/233]	Loss 0.1275 (0.0701)	
training:	Epoch: [27][50/233]	Loss 0.0746 (0.0702)	
training:	Epoch: [27][51/233]	Loss 0.0598 (0.0700)	
training:	Epoch: [27][52/233]	Loss 0.0549 (0.0697)	
training:	Epoch: [27][53/233]	Loss 0.0845 (0.0700)	
training:	Epoch: [27][54/233]	Loss 0.0520 (0.0697)	
training:	Epoch: [27][55/233]	Loss 0.0627 (0.0695)	
training:	Epoch: [27][56/233]	Loss 0.0604 (0.0694)	
training:	Epoch: [27][57/233]	Loss 0.1022 (0.0700)	
training:	Epoch: [27][58/233]	Loss 0.0698 (0.0699)	
training:	Epoch: [27][59/233]	Loss 0.0504 (0.0696)	
training:	Epoch: [27][60/233]	Loss 0.0696 (0.0696)	
training:	Epoch: [27][61/233]	Loss 0.0592 (0.0694)	
training:	Epoch: [27][62/233]	Loss 0.0726 (0.0695)	
training:	Epoch: [27][63/233]	Loss 0.0581 (0.0693)	
training:	Epoch: [27][64/233]	Loss 0.0542 (0.0691)	
training:	Epoch: [27][65/233]	Loss 0.0965 (0.0695)	
training:	Epoch: [27][66/233]	Loss 0.0781 (0.0696)	
training:	Epoch: [27][67/233]	Loss 0.0591 (0.0695)	
training:	Epoch: [27][68/233]	Loss 0.0576 (0.0693)	
training:	Epoch: [27][69/233]	Loss 0.0615 (0.0692)	
training:	Epoch: [27][70/233]	Loss 0.0600 (0.0691)	
training:	Epoch: [27][71/233]	Loss 0.0603 (0.0689)	
training:	Epoch: [27][72/233]	Loss 0.0756 (0.0690)	
training:	Epoch: [27][73/233]	Loss 0.0762 (0.0691)	
training:	Epoch: [27][74/233]	Loss 0.0633 (0.0690)	
training:	Epoch: [27][75/233]	Loss 0.0654 (0.0690)	
training:	Epoch: [27][76/233]	Loss 0.0934 (0.0693)	
training:	Epoch: [27][77/233]	Loss 0.0705 (0.0693)	
training:	Epoch: [27][78/233]	Loss 0.0567 (0.0692)	
training:	Epoch: [27][79/233]	Loss 0.0969 (0.0695)	
training:	Epoch: [27][80/233]	Loss 0.0479 (0.0692)	
training:	Epoch: [27][81/233]	Loss 0.0543 (0.0691)	
training:	Epoch: [27][82/233]	Loss 0.0680 (0.0690)	
training:	Epoch: [27][83/233]	Loss 0.0685 (0.0690)	
training:	Epoch: [27][84/233]	Loss 0.0635 (0.0690)	
training:	Epoch: [27][85/233]	Loss 0.0864 (0.0692)	
training:	Epoch: [27][86/233]	Loss 0.0979 (0.0695)	
training:	Epoch: [27][87/233]	Loss 0.0555 (0.0694)	
training:	Epoch: [27][88/233]	Loss 0.0834 (0.0695)	
training:	Epoch: [27][89/233]	Loss 0.0770 (0.0696)	
training:	Epoch: [27][90/233]	Loss 0.0767 (0.0697)	
training:	Epoch: [27][91/233]	Loss 0.1216 (0.0702)	
training:	Epoch: [27][92/233]	Loss 0.0714 (0.0703)	
training:	Epoch: [27][93/233]	Loss 0.0705 (0.0703)	
training:	Epoch: [27][94/233]	Loss 0.0685 (0.0702)	
training:	Epoch: [27][95/233]	Loss 0.0878 (0.0704)	
training:	Epoch: [27][96/233]	Loss 0.0643 (0.0704)	
training:	Epoch: [27][97/233]	Loss 0.0644 (0.0703)	
training:	Epoch: [27][98/233]	Loss 0.0643 (0.0702)	
training:	Epoch: [27][99/233]	Loss 0.0662 (0.0702)	
training:	Epoch: [27][100/233]	Loss 0.0531 (0.0700)	
training:	Epoch: [27][101/233]	Loss 0.0689 (0.0700)	
training:	Epoch: [27][102/233]	Loss 0.0835 (0.0702)	
training:	Epoch: [27][103/233]	Loss 0.0534 (0.0700)	
training:	Epoch: [27][104/233]	Loss 0.0728 (0.0700)	
training:	Epoch: [27][105/233]	Loss 0.0548 (0.0699)	
training:	Epoch: [27][106/233]	Loss 0.0535 (0.0697)	
training:	Epoch: [27][107/233]	Loss 0.1637 (0.0706)	
training:	Epoch: [27][108/233]	Loss 0.0514 (0.0704)	
training:	Epoch: [27][109/233]	Loss 0.0577 (0.0703)	
training:	Epoch: [27][110/233]	Loss 0.0685 (0.0703)	
training:	Epoch: [27][111/233]	Loss 0.0449 (0.0701)	
training:	Epoch: [27][112/233]	Loss 0.0563 (0.0699)	
training:	Epoch: [27][113/233]	Loss 0.0517 (0.0698)	
training:	Epoch: [27][114/233]	Loss 0.0793 (0.0699)	
training:	Epoch: [27][115/233]	Loss 0.0565 (0.0697)	
training:	Epoch: [27][116/233]	Loss 0.0655 (0.0697)	
training:	Epoch: [27][117/233]	Loss 0.0833 (0.0698)	
training:	Epoch: [27][118/233]	Loss 0.0607 (0.0697)	
training:	Epoch: [27][119/233]	Loss 0.0751 (0.0698)	
training:	Epoch: [27][120/233]	Loss 0.0567 (0.0697)	
training:	Epoch: [27][121/233]	Loss 0.0545 (0.0696)	
training:	Epoch: [27][122/233]	Loss 0.0705 (0.0696)	
training:	Epoch: [27][123/233]	Loss 0.0495 (0.0694)	
training:	Epoch: [27][124/233]	Loss 0.0636 (0.0693)	
training:	Epoch: [27][125/233]	Loss 0.0772 (0.0694)	
training:	Epoch: [27][126/233]	Loss 0.0606 (0.0693)	
training:	Epoch: [27][127/233]	Loss 0.0721 (0.0694)	
training:	Epoch: [27][128/233]	Loss 0.0639 (0.0693)	
training:	Epoch: [27][129/233]	Loss 0.0668 (0.0693)	
training:	Epoch: [27][130/233]	Loss 0.0651 (0.0693)	
training:	Epoch: [27][131/233]	Loss 0.1544 (0.0699)	
training:	Epoch: [27][132/233]	Loss 0.0673 (0.0699)	
training:	Epoch: [27][133/233]	Loss 0.0740 (0.0699)	
training:	Epoch: [27][134/233]	Loss 0.0521 (0.0698)	
training:	Epoch: [27][135/233]	Loss 0.0594 (0.0697)	
training:	Epoch: [27][136/233]	Loss 0.0514 (0.0696)	
training:	Epoch: [27][137/233]	Loss 0.0747 (0.0696)	
training:	Epoch: [27][138/233]	Loss 0.0629 (0.0696)	
training:	Epoch: [27][139/233]	Loss 0.0931 (0.0697)	
training:	Epoch: [27][140/233]	Loss 0.0768 (0.0698)	
training:	Epoch: [27][141/233]	Loss 0.0803 (0.0699)	
training:	Epoch: [27][142/233]	Loss 0.0636 (0.0698)	
training:	Epoch: [27][143/233]	Loss 0.0607 (0.0698)	
training:	Epoch: [27][144/233]	Loss 0.0848 (0.0699)	
training:	Epoch: [27][145/233]	Loss 0.0538 (0.0698)	
training:	Epoch: [27][146/233]	Loss 0.0707 (0.0698)	
training:	Epoch: [27][147/233]	Loss 0.0495 (0.0696)	
training:	Epoch: [27][148/233]	Loss 0.0660 (0.0696)	
training:	Epoch: [27][149/233]	Loss 0.0708 (0.0696)	
training:	Epoch: [27][150/233]	Loss 0.0671 (0.0696)	
training:	Epoch: [27][151/233]	Loss 0.0629 (0.0695)	
training:	Epoch: [27][152/233]	Loss 0.1194 (0.0699)	
training:	Epoch: [27][153/233]	Loss 0.0590 (0.0698)	
training:	Epoch: [27][154/233]	Loss 0.1233 (0.0701)	
training:	Epoch: [27][155/233]	Loss 0.0526 (0.0700)	
training:	Epoch: [27][156/233]	Loss 0.0639 (0.0700)	
training:	Epoch: [27][157/233]	Loss 0.0578 (0.0699)	
training:	Epoch: [27][158/233]	Loss 0.0568 (0.0698)	
training:	Epoch: [27][159/233]	Loss 0.0714 (0.0698)	
training:	Epoch: [27][160/233]	Loss 0.0580 (0.0698)	
training:	Epoch: [27][161/233]	Loss 0.1004 (0.0700)	
training:	Epoch: [27][162/233]	Loss 0.0555 (0.0699)	
training:	Epoch: [27][163/233]	Loss 0.0541 (0.0698)	
training:	Epoch: [27][164/233]	Loss 0.0564 (0.0697)	
training:	Epoch: [27][165/233]	Loss 0.0572 (0.0696)	
training:	Epoch: [27][166/233]	Loss 0.0714 (0.0696)	
training:	Epoch: [27][167/233]	Loss 0.0605 (0.0696)	
training:	Epoch: [27][168/233]	Loss 0.0698 (0.0696)	
training:	Epoch: [27][169/233]	Loss 0.0666 (0.0696)	
training:	Epoch: [27][170/233]	Loss 0.0545 (0.0695)	
training:	Epoch: [27][171/233]	Loss 0.0624 (0.0694)	
training:	Epoch: [27][172/233]	Loss 0.0581 (0.0694)	
training:	Epoch: [27][173/233]	Loss 0.0536 (0.0693)	
training:	Epoch: [27][174/233]	Loss 0.0650 (0.0692)	
training:	Epoch: [27][175/233]	Loss 0.0551 (0.0692)	
training:	Epoch: [27][176/233]	Loss 0.0550 (0.0691)	
training:	Epoch: [27][177/233]	Loss 0.0586 (0.0690)	
training:	Epoch: [27][178/233]	Loss 0.0827 (0.0691)	
training:	Epoch: [27][179/233]	Loss 0.0575 (0.0690)	
training:	Epoch: [27][180/233]	Loss 0.0790 (0.0691)	
training:	Epoch: [27][181/233]	Loss 0.0609 (0.0690)	
training:	Epoch: [27][182/233]	Loss 0.0584 (0.0690)	
training:	Epoch: [27][183/233]	Loss 0.0683 (0.0690)	
training:	Epoch: [27][184/233]	Loss 0.0583 (0.0689)	
training:	Epoch: [27][185/233]	Loss 0.0613 (0.0689)	
training:	Epoch: [27][186/233]	Loss 0.0681 (0.0689)	
training:	Epoch: [27][187/233]	Loss 0.0956 (0.0690)	
training:	Epoch: [27][188/233]	Loss 0.0650 (0.0690)	
training:	Epoch: [27][189/233]	Loss 0.0536 (0.0689)	
training:	Epoch: [27][190/233]	Loss 0.1143 (0.0692)	
training:	Epoch: [27][191/233]	Loss 0.0539 (0.0691)	
training:	Epoch: [27][192/233]	Loss 0.0522 (0.0690)	
training:	Epoch: [27][193/233]	Loss 0.0570 (0.0689)	
training:	Epoch: [27][194/233]	Loss 0.0653 (0.0689)	
training:	Epoch: [27][195/233]	Loss 0.0690 (0.0689)	
training:	Epoch: [27][196/233]	Loss 0.0758 (0.0689)	
training:	Epoch: [27][197/233]	Loss 0.0564 (0.0689)	
training:	Epoch: [27][198/233]	Loss 0.0526 (0.0688)	
training:	Epoch: [27][199/233]	Loss 0.0619 (0.0688)	
training:	Epoch: [27][200/233]	Loss 0.0533 (0.0687)	
training:	Epoch: [27][201/233]	Loss 0.0611 (0.0687)	
training:	Epoch: [27][202/233]	Loss 0.0639 (0.0686)	
training:	Epoch: [27][203/233]	Loss 0.0608 (0.0686)	
training:	Epoch: [27][204/233]	Loss 0.0530 (0.0685)	
training:	Epoch: [27][205/233]	Loss 0.0735 (0.0685)	
training:	Epoch: [27][206/233]	Loss 0.0658 (0.0685)	
training:	Epoch: [27][207/233]	Loss 0.0473 (0.0684)	
training:	Epoch: [27][208/233]	Loss 0.0476 (0.0683)	
training:	Epoch: [27][209/233]	Loss 0.1006 (0.0685)	
training:	Epoch: [27][210/233]	Loss 0.0741 (0.0685)	
training:	Epoch: [27][211/233]	Loss 0.1162 (0.0687)	
training:	Epoch: [27][212/233]	Loss 0.0552 (0.0687)	
training:	Epoch: [27][213/233]	Loss 0.0585 (0.0686)	
training:	Epoch: [27][214/233]	Loss 0.0621 (0.0686)	
training:	Epoch: [27][215/233]	Loss 0.0748 (0.0686)	
training:	Epoch: [27][216/233]	Loss 0.0721 (0.0686)	
training:	Epoch: [27][217/233]	Loss 0.0563 (0.0686)	
training:	Epoch: [27][218/233]	Loss 0.0795 (0.0686)	
training:	Epoch: [27][219/233]	Loss 0.0553 (0.0686)	
training:	Epoch: [27][220/233]	Loss 0.0490 (0.0685)	
training:	Epoch: [27][221/233]	Loss 0.1404 (0.0688)	
training:	Epoch: [27][222/233]	Loss 0.0663 (0.0688)	
training:	Epoch: [27][223/233]	Loss 0.0784 (0.0688)	
training:	Epoch: [27][224/233]	Loss 0.0541 (0.0688)	
training:	Epoch: [27][225/233]	Loss 0.0605 (0.0687)	
training:	Epoch: [27][226/233]	Loss 0.0604 (0.0687)	
training:	Epoch: [27][227/233]	Loss 0.0649 (0.0687)	
training:	Epoch: [27][228/233]	Loss 0.0721 (0.0687)	
training:	Epoch: [27][229/233]	Loss 0.0865 (0.0688)	
training:	Epoch: [27][230/233]	Loss 0.0638 (0.0687)	
training:	Epoch: [27][231/233]	Loss 0.0453 (0.0686)	
training:	Epoch: [27][232/233]	Loss 0.0558 (0.0686)	
training:	Epoch: [27][233/233]	Loss 0.0723 (0.0686)	
Training:	 Loss: 0.0685

Training:	 ACC: 1.0000 1.0000 1.0000 1.0000
Validation:	 ACC: 0.8631 0.8625 0.8506 0.8756
Validation:	 Best_BACC: 0.8647 0.8646 0.8639 0.8655
Validation:	 Loss: 0.3231
Pretraining:	Epoch 28/200
----------
training:	Epoch: [28][1/233]	Loss 0.0618 (0.0618)	
training:	Epoch: [28][2/233]	Loss 0.0591 (0.0604)	
training:	Epoch: [28][3/233]	Loss 0.0566 (0.0592)	
training:	Epoch: [28][4/233]	Loss 0.0954 (0.0682)	
training:	Epoch: [28][5/233]	Loss 0.0564 (0.0658)	
training:	Epoch: [28][6/233]	Loss 0.0572 (0.0644)	
training:	Epoch: [28][7/233]	Loss 0.0708 (0.0653)	
training:	Epoch: [28][8/233]	Loss 0.0575 (0.0643)	
training:	Epoch: [28][9/233]	Loss 0.0533 (0.0631)	
training:	Epoch: [28][10/233]	Loss 0.0792 (0.0647)	
training:	Epoch: [28][11/233]	Loss 0.0705 (0.0652)	
training:	Epoch: [28][12/233]	Loss 0.0521 (0.0641)	
training:	Epoch: [28][13/233]	Loss 0.0585 (0.0637)	
training:	Epoch: [28][14/233]	Loss 0.0594 (0.0634)	
training:	Epoch: [28][15/233]	Loss 0.0635 (0.0634)	
training:	Epoch: [28][16/233]	Loss 0.0517 (0.0627)	
training:	Epoch: [28][17/233]	Loss 0.0632 (0.0627)	
training:	Epoch: [28][18/233]	Loss 0.0826 (0.0638)	
training:	Epoch: [28][19/233]	Loss 0.0557 (0.0634)	
training:	Epoch: [28][20/233]	Loss 0.0693 (0.0637)	
training:	Epoch: [28][21/233]	Loss 0.0467 (0.0629)	
training:	Epoch: [28][22/233]	Loss 0.1192 (0.0654)	
training:	Epoch: [28][23/233]	Loss 0.0488 (0.0647)	
training:	Epoch: [28][24/233]	Loss 0.0659 (0.0648)	
training:	Epoch: [28][25/233]	Loss 0.0931 (0.0659)	
training:	Epoch: [28][26/233]	Loss 0.0705 (0.0661)	
training:	Epoch: [28][27/233]	Loss 0.0910 (0.0670)	
training:	Epoch: [28][28/233]	Loss 0.0766 (0.0673)	
training:	Epoch: [28][29/233]	Loss 0.0630 (0.0672)	
training:	Epoch: [28][30/233]	Loss 0.0562 (0.0668)	
training:	Epoch: [28][31/233]	Loss 0.0637 (0.0667)	
training:	Epoch: [28][32/233]	Loss 0.0501 (0.0662)	
training:	Epoch: [28][33/233]	Loss 0.0513 (0.0657)	
training:	Epoch: [28][34/233]	Loss 0.0566 (0.0655)	
training:	Epoch: [28][35/233]	Loss 0.0648 (0.0655)	
training:	Epoch: [28][36/233]	Loss 0.0552 (0.0652)	
training:	Epoch: [28][37/233]	Loss 0.0710 (0.0653)	
training:	Epoch: [28][38/233]	Loss 0.1266 (0.0669)	
training:	Epoch: [28][39/233]	Loss 0.0666 (0.0669)	
training:	Epoch: [28][40/233]	Loss 0.0611 (0.0668)	
training:	Epoch: [28][41/233]	Loss 0.0538 (0.0665)	
training:	Epoch: [28][42/233]	Loss 0.0544 (0.0662)	
training:	Epoch: [28][43/233]	Loss 0.0793 (0.0665)	
training:	Epoch: [28][44/233]	Loss 0.0497 (0.0661)	
training:	Epoch: [28][45/233]	Loss 0.0842 (0.0665)	
training:	Epoch: [28][46/233]	Loss 0.0556 (0.0663)	
training:	Epoch: [28][47/233]	Loss 0.0556 (0.0661)	
training:	Epoch: [28][48/233]	Loss 0.0635 (0.0660)	
training:	Epoch: [28][49/233]	Loss 0.0744 (0.0662)	
training:	Epoch: [28][50/233]	Loss 0.0458 (0.0658)	
training:	Epoch: [28][51/233]	Loss 0.0562 (0.0656)	
training:	Epoch: [28][52/233]	Loss 0.0581 (0.0654)	
training:	Epoch: [28][53/233]	Loss 0.0554 (0.0652)	
training:	Epoch: [28][54/233]	Loss 0.0671 (0.0653)	
training:	Epoch: [28][55/233]	Loss 0.0674 (0.0653)	
training:	Epoch: [28][56/233]	Loss 0.0789 (0.0656)	
training:	Epoch: [28][57/233]	Loss 0.0581 (0.0654)	
training:	Epoch: [28][58/233]	Loss 0.0819 (0.0657)	
training:	Epoch: [28][59/233]	Loss 0.0573 (0.0656)	
training:	Epoch: [28][60/233]	Loss 0.1187 (0.0665)	
training:	Epoch: [28][61/233]	Loss 0.0571 (0.0663)	
training:	Epoch: [28][62/233]	Loss 0.0572 (0.0662)	
training:	Epoch: [28][63/233]	Loss 0.0604 (0.0661)	
training:	Epoch: [28][64/233]	Loss 0.0907 (0.0664)	
training:	Epoch: [28][65/233]	Loss 0.0776 (0.0666)	
training:	Epoch: [28][66/233]	Loss 0.0894 (0.0670)	
training:	Epoch: [28][67/233]	Loss 0.0668 (0.0670)	
training:	Epoch: [28][68/233]	Loss 0.0526 (0.0667)	
training:	Epoch: [28][69/233]	Loss 0.0753 (0.0669)	
training:	Epoch: [28][70/233]	Loss 0.0525 (0.0667)	
training:	Epoch: [28][71/233]	Loss 0.0650 (0.0666)	
training:	Epoch: [28][72/233]	Loss 0.0851 (0.0669)	
training:	Epoch: [28][73/233]	Loss 0.0531 (0.0667)	
training:	Epoch: [28][74/233]	Loss 0.0598 (0.0666)	
training:	Epoch: [28][75/233]	Loss 0.0687 (0.0666)	
training:	Epoch: [28][76/233]	Loss 0.0721 (0.0667)	
training:	Epoch: [28][77/233]	Loss 0.0979 (0.0671)	
training:	Epoch: [28][78/233]	Loss 0.0496 (0.0669)	
training:	Epoch: [28][79/233]	Loss 0.0570 (0.0668)	
training:	Epoch: [28][80/233]	Loss 0.0709 (0.0668)	
training:	Epoch: [28][81/233]	Loss 0.0776 (0.0670)	
training:	Epoch: [28][82/233]	Loss 0.0619 (0.0669)	
training:	Epoch: [28][83/233]	Loss 0.0603 (0.0668)	
training:	Epoch: [28][84/233]	Loss 0.0589 (0.0667)	
training:	Epoch: [28][85/233]	Loss 0.0546 (0.0666)	
training:	Epoch: [28][86/233]	Loss 0.0530 (0.0664)	
training:	Epoch: [28][87/233]	Loss 0.0592 (0.0663)	
training:	Epoch: [28][88/233]	Loss 0.0703 (0.0664)	
training:	Epoch: [28][89/233]	Loss 0.0589 (0.0663)	
training:	Epoch: [28][90/233]	Loss 0.0702 (0.0663)	
training:	Epoch: [28][91/233]	Loss 0.0562 (0.0662)	
training:	Epoch: [28][92/233]	Loss 0.0624 (0.0662)	
training:	Epoch: [28][93/233]	Loss 0.0564 (0.0661)	
training:	Epoch: [28][94/233]	Loss 0.0648 (0.0661)	
training:	Epoch: [28][95/233]	Loss 0.0561 (0.0660)	
training:	Epoch: [28][96/233]	Loss 0.0575 (0.0659)	
training:	Epoch: [28][97/233]	Loss 0.0532 (0.0657)	
training:	Epoch: [28][98/233]	Loss 0.0673 (0.0658)	
training:	Epoch: [28][99/233]	Loss 0.0839 (0.0659)	
training:	Epoch: [28][100/233]	Loss 0.0537 (0.0658)	
training:	Epoch: [28][101/233]	Loss 0.0946 (0.0661)	
training:	Epoch: [28][102/233]	Loss 0.0699 (0.0661)	
training:	Epoch: [28][103/233]	Loss 0.0562 (0.0660)	
training:	Epoch: [28][104/233]	Loss 0.0676 (0.0661)	
training:	Epoch: [28][105/233]	Loss 0.0500 (0.0659)	
training:	Epoch: [28][106/233]	Loss 0.0557 (0.0658)	
training:	Epoch: [28][107/233]	Loss 0.0517 (0.0657)	
training:	Epoch: [28][108/233]	Loss 0.1291 (0.0663)	
training:	Epoch: [28][109/233]	Loss 0.0628 (0.0662)	
training:	Epoch: [28][110/233]	Loss 0.0620 (0.0662)	
training:	Epoch: [28][111/233]	Loss 0.0777 (0.0663)	
training:	Epoch: [28][112/233]	Loss 0.0603 (0.0662)	
training:	Epoch: [28][113/233]	Loss 0.0608 (0.0662)	
training:	Epoch: [28][114/233]	Loss 0.0480 (0.0660)	
training:	Epoch: [28][115/233]	Loss 0.0626 (0.0660)	
training:	Epoch: [28][116/233]	Loss 0.0497 (0.0659)	
training:	Epoch: [28][117/233]	Loss 0.0604 (0.0658)	
training:	Epoch: [28][118/233]	Loss 0.0680 (0.0658)	
training:	Epoch: [28][119/233]	Loss 0.0434 (0.0657)	
training:	Epoch: [28][120/233]	Loss 0.0612 (0.0656)	
training:	Epoch: [28][121/233]	Loss 0.0489 (0.0655)	
training:	Epoch: [28][122/233]	Loss 0.0511 (0.0654)	
training:	Epoch: [28][123/233]	Loss 0.0539 (0.0653)	
training:	Epoch: [28][124/233]	Loss 0.0721 (0.0653)	
training:	Epoch: [28][125/233]	Loss 0.0523 (0.0652)	
training:	Epoch: [28][126/233]	Loss 0.0562 (0.0651)	
training:	Epoch: [28][127/233]	Loss 0.0640 (0.0651)	
training:	Epoch: [28][128/233]	Loss 0.0627 (0.0651)	
training:	Epoch: [28][129/233]	Loss 0.0555 (0.0650)	
training:	Epoch: [28][130/233]	Loss 0.0723 (0.0651)	
training:	Epoch: [28][131/233]	Loss 0.0689 (0.0651)	
training:	Epoch: [28][132/233]	Loss 0.0582 (0.0651)	
training:	Epoch: [28][133/233]	Loss 0.0534 (0.0650)	
training:	Epoch: [28][134/233]	Loss 0.0947 (0.0652)	
training:	Epoch: [28][135/233]	Loss 0.0584 (0.0652)	
training:	Epoch: [28][136/233]	Loss 0.0514 (0.0651)	
training:	Epoch: [28][137/233]	Loss 0.0646 (0.0651)	
training:	Epoch: [28][138/233]	Loss 0.0624 (0.0650)	
training:	Epoch: [28][139/233]	Loss 0.0705 (0.0651)	
training:	Epoch: [28][140/233]	Loss 0.1178 (0.0655)	
training:	Epoch: [28][141/233]	Loss 0.0777 (0.0655)	
training:	Epoch: [28][142/233]	Loss 0.0740 (0.0656)	
training:	Epoch: [28][143/233]	Loss 0.0641 (0.0656)	
training:	Epoch: [28][144/233]	Loss 0.0855 (0.0657)	
training:	Epoch: [28][145/233]	Loss 0.0883 (0.0659)	
training:	Epoch: [28][146/233]	Loss 0.0590 (0.0658)	
training:	Epoch: [28][147/233]	Loss 0.0512 (0.0657)	
training:	Epoch: [28][148/233]	Loss 0.0759 (0.0658)	
training:	Epoch: [28][149/233]	Loss 0.0707 (0.0658)	
training:	Epoch: [28][150/233]	Loss 0.0626 (0.0658)	
training:	Epoch: [28][151/233]	Loss 0.0891 (0.0660)	
training:	Epoch: [28][152/233]	Loss 0.0567 (0.0659)	
training:	Epoch: [28][153/233]	Loss 0.0477 (0.0658)	
training:	Epoch: [28][154/233]	Loss 0.0604 (0.0658)	
training:	Epoch: [28][155/233]	Loss 0.0889 (0.0659)	
training:	Epoch: [28][156/233]	Loss 0.0742 (0.0660)	
training:	Epoch: [28][157/233]	Loss 0.0729 (0.0660)	
training:	Epoch: [28][158/233]	Loss 0.0794 (0.0661)	
training:	Epoch: [28][159/233]	Loss 0.0827 (0.0662)	
training:	Epoch: [28][160/233]	Loss 0.0500 (0.0661)	
training:	Epoch: [28][161/233]	Loss 0.0432 (0.0659)	
training:	Epoch: [28][162/233]	Loss 0.0510 (0.0659)	
training:	Epoch: [28][163/233]	Loss 0.0678 (0.0659)	
training:	Epoch: [28][164/233]	Loss 0.0808 (0.0660)	
training:	Epoch: [28][165/233]	Loss 0.0693 (0.0660)	
training:	Epoch: [28][166/233]	Loss 0.0555 (0.0659)	
training:	Epoch: [28][167/233]	Loss 0.0540 (0.0658)	
training:	Epoch: [28][168/233]	Loss 0.0545 (0.0658)	
training:	Epoch: [28][169/233]	Loss 0.0682 (0.0658)	
training:	Epoch: [28][170/233]	Loss 0.0748 (0.0658)	
training:	Epoch: [28][171/233]	Loss 0.0565 (0.0658)	
training:	Epoch: [28][172/233]	Loss 0.0551 (0.0657)	
training:	Epoch: [28][173/233]	Loss 0.0653 (0.0657)	
training:	Epoch: [28][174/233]	Loss 0.0566 (0.0657)	
training:	Epoch: [28][175/233]	Loss 0.0841 (0.0658)	
training:	Epoch: [28][176/233]	Loss 0.0651 (0.0658)	
training:	Epoch: [28][177/233]	Loss 0.0743 (0.0658)	
training:	Epoch: [28][178/233]	Loss 0.0687 (0.0658)	
training:	Epoch: [28][179/233]	Loss 0.0602 (0.0658)	
training:	Epoch: [28][180/233]	Loss 0.0628 (0.0658)	
training:	Epoch: [28][181/233]	Loss 0.0895 (0.0659)	
training:	Epoch: [28][182/233]	Loss 0.0602 (0.0659)	
training:	Epoch: [28][183/233]	Loss 0.0547 (0.0658)	
training:	Epoch: [28][184/233]	Loss 0.0577 (0.0658)	
training:	Epoch: [28][185/233]	Loss 0.0669 (0.0658)	
training:	Epoch: [28][186/233]	Loss 0.0727 (0.0658)	
training:	Epoch: [28][187/233]	Loss 0.1075 (0.0660)	
training:	Epoch: [28][188/233]	Loss 0.0567 (0.0660)	
training:	Epoch: [28][189/233]	Loss 0.1268 (0.0663)	
training:	Epoch: [28][190/233]	Loss 0.0692 (0.0663)	
training:	Epoch: [28][191/233]	Loss 0.0588 (0.0663)	
training:	Epoch: [28][192/233]	Loss 0.0723 (0.0663)	
training:	Epoch: [28][193/233]	Loss 0.0567 (0.0663)	
training:	Epoch: [28][194/233]	Loss 0.0802 (0.0664)	
training:	Epoch: [28][195/233]	Loss 0.0529 (0.0663)	
training:	Epoch: [28][196/233]	Loss 0.0522 (0.0662)	
training:	Epoch: [28][197/233]	Loss 0.0629 (0.0662)	
training:	Epoch: [28][198/233]	Loss 0.0873 (0.0663)	
training:	Epoch: [28][199/233]	Loss 0.0459 (0.0662)	
training:	Epoch: [28][200/233]	Loss 0.1176 (0.0665)	
training:	Epoch: [28][201/233]	Loss 0.0580 (0.0664)	
training:	Epoch: [28][202/233]	Loss 0.0625 (0.0664)	
training:	Epoch: [28][203/233]	Loss 0.0613 (0.0664)	
training:	Epoch: [28][204/233]	Loss 0.0646 (0.0664)	
training:	Epoch: [28][205/233]	Loss 0.0469 (0.0663)	
training:	Epoch: [28][206/233]	Loss 0.0571 (0.0662)	
training:	Epoch: [28][207/233]	Loss 0.0768 (0.0663)	
training:	Epoch: [28][208/233]	Loss 0.0549 (0.0662)	
training:	Epoch: [28][209/233]	Loss 0.0718 (0.0662)	
training:	Epoch: [28][210/233]	Loss 0.0645 (0.0662)	
training:	Epoch: [28][211/233]	Loss 0.0562 (0.0662)	
training:	Epoch: [28][212/233]	Loss 0.0605 (0.0662)	
training:	Epoch: [28][213/233]	Loss 0.0534 (0.0661)	
training:	Epoch: [28][214/233]	Loss 0.0596 (0.0661)	
training:	Epoch: [28][215/233]	Loss 0.0662 (0.0661)	
training:	Epoch: [28][216/233]	Loss 0.0554 (0.0660)	
training:	Epoch: [28][217/233]	Loss 0.0721 (0.0661)	
training:	Epoch: [28][218/233]	Loss 0.0473 (0.0660)	
training:	Epoch: [28][219/233]	Loss 0.0503 (0.0659)	
training:	Epoch: [28][220/233]	Loss 0.0608 (0.0659)	
training:	Epoch: [28][221/233]	Loss 0.0526 (0.0658)	
training:	Epoch: [28][222/233]	Loss 0.0895 (0.0659)	
training:	Epoch: [28][223/233]	Loss 0.0655 (0.0659)	
training:	Epoch: [28][224/233]	Loss 0.0666 (0.0659)	
training:	Epoch: [28][225/233]	Loss 0.0668 (0.0659)	
training:	Epoch: [28][226/233]	Loss 0.0503 (0.0659)	
training:	Epoch: [28][227/233]	Loss 0.0557 (0.0658)	
training:	Epoch: [28][228/233]	Loss 0.0670 (0.0658)	
training:	Epoch: [28][229/233]	Loss 0.0626 (0.0658)	
training:	Epoch: [28][230/233]	Loss 0.0995 (0.0659)	
training:	Epoch: [28][231/233]	Loss 0.0623 (0.0659)	
training:	Epoch: [28][232/233]	Loss 0.0581 (0.0659)	
training:	Epoch: [28][233/233]	Loss 0.0601 (0.0659)	
Training:	 Loss: 0.0657

Training:	 ACC: 1.0000 1.0000 1.0000 1.0000
Validation:	 ACC: 0.8627 0.8625 0.8577 0.8677
Validation:	 Best_BACC: 0.8647 0.8646 0.8639 0.8655
Validation:	 Loss: 0.3213
Pretraining:	Epoch 29/200
----------
training:	Epoch: [29][1/233]	Loss 0.0454 (0.0454)	
training:	Epoch: [29][2/233]	Loss 0.0548 (0.0501)	
training:	Epoch: [29][3/233]	Loss 0.0650 (0.0551)	
training:	Epoch: [29][4/233]	Loss 0.0540 (0.0548)	
training:	Epoch: [29][5/233]	Loss 0.0634 (0.0565)	
training:	Epoch: [29][6/233]	Loss 0.0435 (0.0544)	
training:	Epoch: [29][7/233]	Loss 0.0536 (0.0542)	
training:	Epoch: [29][8/233]	Loss 0.0612 (0.0551)	
training:	Epoch: [29][9/233]	Loss 0.0582 (0.0555)	
training:	Epoch: [29][10/233]	Loss 0.0556 (0.0555)	
training:	Epoch: [29][11/233]	Loss 0.0592 (0.0558)	
training:	Epoch: [29][12/233]	Loss 0.0435 (0.0548)	
training:	Epoch: [29][13/233]	Loss 0.0575 (0.0550)	
training:	Epoch: [29][14/233]	Loss 0.0547 (0.0550)	
training:	Epoch: [29][15/233]	Loss 0.0747 (0.0563)	
training:	Epoch: [29][16/233]	Loss 0.0445 (0.0555)	
training:	Epoch: [29][17/233]	Loss 0.0535 (0.0554)	
training:	Epoch: [29][18/233]	Loss 0.0529 (0.0553)	
training:	Epoch: [29][19/233]	Loss 0.0632 (0.0557)	
training:	Epoch: [29][20/233]	Loss 0.0484 (0.0553)	
training:	Epoch: [29][21/233]	Loss 0.0555 (0.0553)	
training:	Epoch: [29][22/233]	Loss 0.0582 (0.0555)	
training:	Epoch: [29][23/233]	Loss 0.0840 (0.0567)	
training:	Epoch: [29][24/233]	Loss 0.0878 (0.0580)	
training:	Epoch: [29][25/233]	Loss 0.0612 (0.0581)	
training:	Epoch: [29][26/233]	Loss 0.0598 (0.0582)	
training:	Epoch: [29][27/233]	Loss 0.0503 (0.0579)	
training:	Epoch: [29][28/233]	Loss 0.0660 (0.0582)	
training:	Epoch: [29][29/233]	Loss 0.0450 (0.0577)	
training:	Epoch: [29][30/233]	Loss 0.0463 (0.0574)	
training:	Epoch: [29][31/233]	Loss 0.1127 (0.0591)	
training:	Epoch: [29][32/233]	Loss 0.0497 (0.0589)	
training:	Epoch: [29][33/233]	Loss 0.0447 (0.0584)	
training:	Epoch: [29][34/233]	Loss 0.0578 (0.0584)	
training:	Epoch: [29][35/233]	Loss 0.0553 (0.0583)	
training:	Epoch: [29][36/233]	Loss 0.0709 (0.0587)	
training:	Epoch: [29][37/233]	Loss 0.0516 (0.0585)	
training:	Epoch: [29][38/233]	Loss 0.0658 (0.0587)	
training:	Epoch: [29][39/233]	Loss 0.0595 (0.0587)	
training:	Epoch: [29][40/233]	Loss 0.0676 (0.0589)	
training:	Epoch: [29][41/233]	Loss 0.0823 (0.0595)	
training:	Epoch: [29][42/233]	Loss 0.0589 (0.0595)	
training:	Epoch: [29][43/233]	Loss 0.0553 (0.0594)	
training:	Epoch: [29][44/233]	Loss 0.0655 (0.0595)	
training:	Epoch: [29][45/233]	Loss 0.0676 (0.0597)	
training:	Epoch: [29][46/233]	Loss 0.0816 (0.0602)	
training:	Epoch: [29][47/233]	Loss 0.0778 (0.0605)	
training:	Epoch: [29][48/233]	Loss 0.0566 (0.0605)	
training:	Epoch: [29][49/233]	Loss 0.0536 (0.0603)	
training:	Epoch: [29][50/233]	Loss 0.0631 (0.0604)	
training:	Epoch: [29][51/233]	Loss 0.0519 (0.0602)	
training:	Epoch: [29][52/233]	Loss 0.0526 (0.0601)	
training:	Epoch: [29][53/233]	Loss 0.0568 (0.0600)	
training:	Epoch: [29][54/233]	Loss 0.0534 (0.0599)	
training:	Epoch: [29][55/233]	Loss 0.0764 (0.0602)	
training:	Epoch: [29][56/233]	Loss 0.0530 (0.0601)	
training:	Epoch: [29][57/233]	Loss 0.0591 (0.0600)	
training:	Epoch: [29][58/233]	Loss 0.0557 (0.0600)	
training:	Epoch: [29][59/233]	Loss 0.0478 (0.0598)	
training:	Epoch: [29][60/233]	Loss 0.0584 (0.0597)	
training:	Epoch: [29][61/233]	Loss 0.0810 (0.0601)	
training:	Epoch: [29][62/233]	Loss 0.0529 (0.0600)	
training:	Epoch: [29][63/233]	Loss 0.0578 (0.0599)	
training:	Epoch: [29][64/233]	Loss 0.0517 (0.0598)	
training:	Epoch: [29][65/233]	Loss 0.0738 (0.0600)	
training:	Epoch: [29][66/233]	Loss 0.0617 (0.0600)	
training:	Epoch: [29][67/233]	Loss 0.0575 (0.0600)	
training:	Epoch: [29][68/233]	Loss 0.0604 (0.0600)	
training:	Epoch: [29][69/233]	Loss 0.0590 (0.0600)	
training:	Epoch: [29][70/233]	Loss 0.1124 (0.0607)	
training:	Epoch: [29][71/233]	Loss 0.0490 (0.0606)	
training:	Epoch: [29][72/233]	Loss 0.1374 (0.0616)	
training:	Epoch: [29][73/233]	Loss 0.0578 (0.0616)	
training:	Epoch: [29][74/233]	Loss 0.0619 (0.0616)	
training:	Epoch: [29][75/233]	Loss 0.0479 (0.0614)	
training:	Epoch: [29][76/233]	Loss 0.0790 (0.0616)	
training:	Epoch: [29][77/233]	Loss 0.0589 (0.0616)	
training:	Epoch: [29][78/233]	Loss 0.0547 (0.0615)	
training:	Epoch: [29][79/233]	Loss 0.0562 (0.0615)	
training:	Epoch: [29][80/233]	Loss 0.0525 (0.0613)	
training:	Epoch: [29][81/233]	Loss 0.0603 (0.0613)	
training:	Epoch: [29][82/233]	Loss 0.0474 (0.0612)	
training:	Epoch: [29][83/233]	Loss 0.0481 (0.0610)	
training:	Epoch: [29][84/233]	Loss 0.0528 (0.0609)	
training:	Epoch: [29][85/233]	Loss 0.2189 (0.0628)	
training:	Epoch: [29][86/233]	Loss 0.0498 (0.0626)	
training:	Epoch: [29][87/233]	Loss 0.0445 (0.0624)	
training:	Epoch: [29][88/233]	Loss 0.0857 (0.0627)	
training:	Epoch: [29][89/233]	Loss 0.0522 (0.0626)	
training:	Epoch: [29][90/233]	Loss 0.0895 (0.0629)	
training:	Epoch: [29][91/233]	Loss 0.0521 (0.0627)	
training:	Epoch: [29][92/233]	Loss 0.0606 (0.0627)	
training:	Epoch: [29][93/233]	Loss 0.1775 (0.0639)	
training:	Epoch: [29][94/233]	Loss 0.0537 (0.0638)	
training:	Epoch: [29][95/233]	Loss 0.0644 (0.0638)	
training:	Epoch: [29][96/233]	Loss 0.0572 (0.0638)	
training:	Epoch: [29][97/233]	Loss 0.0579 (0.0637)	
training:	Epoch: [29][98/233]	Loss 0.0669 (0.0637)	
training:	Epoch: [29][99/233]	Loss 0.0430 (0.0635)	
training:	Epoch: [29][100/233]	Loss 0.0466 (0.0634)	
training:	Epoch: [29][101/233]	Loss 0.0489 (0.0632)	
training:	Epoch: [29][102/233]	Loss 0.0987 (0.0636)	
training:	Epoch: [29][103/233]	Loss 0.1110 (0.0640)	
training:	Epoch: [29][104/233]	Loss 0.0823 (0.0642)	
training:	Epoch: [29][105/233]	Loss 0.0678 (0.0642)	
training:	Epoch: [29][106/233]	Loss 0.0590 (0.0642)	
training:	Epoch: [29][107/233]	Loss 0.0540 (0.0641)	
training:	Epoch: [29][108/233]	Loss 0.0526 (0.0640)	
training:	Epoch: [29][109/233]	Loss 0.0617 (0.0640)	
training:	Epoch: [29][110/233]	Loss 0.0658 (0.0640)	
training:	Epoch: [29][111/233]	Loss 0.0830 (0.0642)	
training:	Epoch: [29][112/233]	Loss 0.0461 (0.0640)	
training:	Epoch: [29][113/233]	Loss 0.1919 (0.0651)	
training:	Epoch: [29][114/233]	Loss 0.0532 (0.0650)	
training:	Epoch: [29][115/233]	Loss 0.0636 (0.0650)	
training:	Epoch: [29][116/233]	Loss 0.0519 (0.0649)	
training:	Epoch: [29][117/233]	Loss 0.0607 (0.0649)	
training:	Epoch: [29][118/233]	Loss 0.0624 (0.0648)	
training:	Epoch: [29][119/233]	Loss 0.0482 (0.0647)	
training:	Epoch: [29][120/233]	Loss 0.0507 (0.0646)	
training:	Epoch: [29][121/233]	Loss 0.0523 (0.0645)	
training:	Epoch: [29][122/233]	Loss 0.0575 (0.0644)	
training:	Epoch: [29][123/233]	Loss 0.0576 (0.0644)	
training:	Epoch: [29][124/233]	Loss 0.0664 (0.0644)	
training:	Epoch: [29][125/233]	Loss 0.0531 (0.0643)	
training:	Epoch: [29][126/233]	Loss 0.0541 (0.0642)	
training:	Epoch: [29][127/233]	Loss 0.0561 (0.0642)	
training:	Epoch: [29][128/233]	Loss 0.0509 (0.0640)	
training:	Epoch: [29][129/233]	Loss 0.0786 (0.0642)	
training:	Epoch: [29][130/233]	Loss 0.0550 (0.0641)	
training:	Epoch: [29][131/233]	Loss 0.0464 (0.0640)	
training:	Epoch: [29][132/233]	Loss 0.0550 (0.0639)	
training:	Epoch: [29][133/233]	Loss 0.0547 (0.0638)	
training:	Epoch: [29][134/233]	Loss 0.1061 (0.0641)	
training:	Epoch: [29][135/233]	Loss 0.1082 (0.0645)	
training:	Epoch: [29][136/233]	Loss 0.0559 (0.0644)	
training:	Epoch: [29][137/233]	Loss 0.0497 (0.0643)	
training:	Epoch: [29][138/233]	Loss 0.0483 (0.0642)	
training:	Epoch: [29][139/233]	Loss 0.0501 (0.0641)	
training:	Epoch: [29][140/233]	Loss 0.0579 (0.0640)	
training:	Epoch: [29][141/233]	Loss 0.0648 (0.0640)	
training:	Epoch: [29][142/233]	Loss 0.1227 (0.0644)	
training:	Epoch: [29][143/233]	Loss 0.0657 (0.0645)	
training:	Epoch: [29][144/233]	Loss 0.0637 (0.0645)	
training:	Epoch: [29][145/233]	Loss 0.0787 (0.0645)	
training:	Epoch: [29][146/233]	Loss 0.0898 (0.0647)	
training:	Epoch: [29][147/233]	Loss 0.0448 (0.0646)	
training:	Epoch: [29][148/233]	Loss 0.1038 (0.0649)	
training:	Epoch: [29][149/233]	Loss 0.0668 (0.0649)	
training:	Epoch: [29][150/233]	Loss 0.0471 (0.0647)	
training:	Epoch: [29][151/233]	Loss 0.1170 (0.0651)	
training:	Epoch: [29][152/233]	Loss 0.0535 (0.0650)	
training:	Epoch: [29][153/233]	Loss 0.0749 (0.0651)	
training:	Epoch: [29][154/233]	Loss 0.1210 (0.0654)	
training:	Epoch: [29][155/233]	Loss 0.0527 (0.0654)	
training:	Epoch: [29][156/233]	Loss 0.0491 (0.0653)	
training:	Epoch: [29][157/233]	Loss 0.0932 (0.0654)	
training:	Epoch: [29][158/233]	Loss 0.0514 (0.0653)	
training:	Epoch: [29][159/233]	Loss 0.0539 (0.0653)	
training:	Epoch: [29][160/233]	Loss 0.0497 (0.0652)	
training:	Epoch: [29][161/233]	Loss 0.0493 (0.0651)	
training:	Epoch: [29][162/233]	Loss 0.0863 (0.0652)	
training:	Epoch: [29][163/233]	Loss 0.0625 (0.0652)	
training:	Epoch: [29][164/233]	Loss 0.0562 (0.0651)	
training:	Epoch: [29][165/233]	Loss 0.0478 (0.0650)	
training:	Epoch: [29][166/233]	Loss 0.0539 (0.0650)	
training:	Epoch: [29][167/233]	Loss 0.0621 (0.0649)	
training:	Epoch: [29][168/233]	Loss 0.0590 (0.0649)	
training:	Epoch: [29][169/233]	Loss 0.0738 (0.0650)	
training:	Epoch: [29][170/233]	Loss 0.0564 (0.0649)	
training:	Epoch: [29][171/233]	Loss 0.0490 (0.0648)	
training:	Epoch: [29][172/233]	Loss 0.0524 (0.0647)	
training:	Epoch: [29][173/233]	Loss 0.0612 (0.0647)	
training:	Epoch: [29][174/233]	Loss 0.0615 (0.0647)	
training:	Epoch: [29][175/233]	Loss 0.1282 (0.0651)	
training:	Epoch: [29][176/233]	Loss 0.1016 (0.0653)	
training:	Epoch: [29][177/233]	Loss 0.0635 (0.0653)	
training:	Epoch: [29][178/233]	Loss 0.0532 (0.0652)	
training:	Epoch: [29][179/233]	Loss 0.0638 (0.0652)	
training:	Epoch: [29][180/233]	Loss 0.0687 (0.0652)	
training:	Epoch: [29][181/233]	Loss 0.0478 (0.0651)	
training:	Epoch: [29][182/233]	Loss 0.0567 (0.0651)	
training:	Epoch: [29][183/233]	Loss 0.0722 (0.0651)	
training:	Epoch: [29][184/233]	Loss 0.0500 (0.0650)	
training:	Epoch: [29][185/233]	Loss 0.0641 (0.0650)	
training:	Epoch: [29][186/233]	Loss 0.1615 (0.0655)	
training:	Epoch: [29][187/233]	Loss 0.0474 (0.0654)	
training:	Epoch: [29][188/233]	Loss 0.0534 (0.0654)	
training:	Epoch: [29][189/233]	Loss 0.0507 (0.0653)	
training:	Epoch: [29][190/233]	Loss 0.0630 (0.0653)	
training:	Epoch: [29][191/233]	Loss 0.1186 (0.0656)	
training:	Epoch: [29][192/233]	Loss 0.0526 (0.0655)	
training:	Epoch: [29][193/233]	Loss 0.0749 (0.0656)	
training:	Epoch: [29][194/233]	Loss 0.0579 (0.0655)	
training:	Epoch: [29][195/233]	Loss 0.1081 (0.0657)	
training:	Epoch: [29][196/233]	Loss 0.0550 (0.0657)	
training:	Epoch: [29][197/233]	Loss 0.0742 (0.0657)	
training:	Epoch: [29][198/233]	Loss 0.0630 (0.0657)	
training:	Epoch: [29][199/233]	Loss 0.0722 (0.0657)	
training:	Epoch: [29][200/233]	Loss 0.0586 (0.0657)	
training:	Epoch: [29][201/233]	Loss 0.0632 (0.0657)	
training:	Epoch: [29][202/233]	Loss 0.0561 (0.0656)	
training:	Epoch: [29][203/233]	Loss 0.0502 (0.0656)	
training:	Epoch: [29][204/233]	Loss 0.0650 (0.0656)	
training:	Epoch: [29][205/233]	Loss 0.0521 (0.0655)	
training:	Epoch: [29][206/233]	Loss 0.0544 (0.0654)	
training:	Epoch: [29][207/233]	Loss 0.0601 (0.0654)	
training:	Epoch: [29][208/233]	Loss 0.0772 (0.0655)	
training:	Epoch: [29][209/233]	Loss 0.0860 (0.0656)	
training:	Epoch: [29][210/233]	Loss 0.0515 (0.0655)	
training:	Epoch: [29][211/233]	Loss 0.0504 (0.0654)	
training:	Epoch: [29][212/233]	Loss 0.0452 (0.0653)	
training:	Epoch: [29][213/233]	Loss 0.0523 (0.0653)	
training:	Epoch: [29][214/233]	Loss 0.0649 (0.0653)	
training:	Epoch: [29][215/233]	Loss 0.0459 (0.0652)	
training:	Epoch: [29][216/233]	Loss 0.0538 (0.0651)	
training:	Epoch: [29][217/233]	Loss 0.0720 (0.0652)	
training:	Epoch: [29][218/233]	Loss 0.0899 (0.0653)	
training:	Epoch: [29][219/233]	Loss 0.0507 (0.0652)	
training:	Epoch: [29][220/233]	Loss 0.0556 (0.0652)	
training:	Epoch: [29][221/233]	Loss 0.0574 (0.0651)	
training:	Epoch: [29][222/233]	Loss 0.0602 (0.0651)	
training:	Epoch: [29][223/233]	Loss 0.0639 (0.0651)	
training:	Epoch: [29][224/233]	Loss 0.0780 (0.0652)	
training:	Epoch: [29][225/233]	Loss 0.0686 (0.0652)	
training:	Epoch: [29][226/233]	Loss 0.1039 (0.0653)	
training:	Epoch: [29][227/233]	Loss 0.0787 (0.0654)	
training:	Epoch: [29][228/233]	Loss 0.0587 (0.0654)	
training:	Epoch: [29][229/233]	Loss 0.0512 (0.0653)	
training:	Epoch: [29][230/233]	Loss 0.0454 (0.0652)	
training:	Epoch: [29][231/233]	Loss 0.0682 (0.0652)	
training:	Epoch: [29][232/233]	Loss 0.0768 (0.0653)	
training:	Epoch: [29][233/233]	Loss 0.0768 (0.0653)	
Training:	 Loss: 0.0652

Training:	 ACC: 1.0000 1.0000 1.0000 1.0000
Validation:	 ACC: 0.8615 0.8609 0.8485 0.8744
Validation:	 Best_BACC: 0.8647 0.8646 0.8639 0.8655
Validation:	 Loss: 0.3238
Pretraining:	Epoch 30/200
----------
training:	Epoch: [30][1/233]	Loss 0.0497 (0.0497)	
training:	Epoch: [30][2/233]	Loss 0.0482 (0.0490)	
training:	Epoch: [30][3/233]	Loss 0.0446 (0.0475)	
training:	Epoch: [30][4/233]	Loss 0.0472 (0.0474)	
training:	Epoch: [30][5/233]	Loss 0.0753 (0.0530)	
training:	Epoch: [30][6/233]	Loss 0.0783 (0.0572)	
training:	Epoch: [30][7/233]	Loss 0.0557 (0.0570)	
training:	Epoch: [30][8/233]	Loss 0.0530 (0.0565)	
training:	Epoch: [30][9/233]	Loss 0.0521 (0.0560)	
training:	Epoch: [30][10/233]	Loss 0.0535 (0.0558)	
training:	Epoch: [30][11/233]	Loss 0.0747 (0.0575)	
training:	Epoch: [30][12/233]	Loss 0.0470 (0.0566)	
training:	Epoch: [30][13/233]	Loss 0.0735 (0.0579)	
training:	Epoch: [30][14/233]	Loss 0.0516 (0.0575)	
training:	Epoch: [30][15/233]	Loss 0.0605 (0.0577)	
training:	Epoch: [30][16/233]	Loss 0.0719 (0.0585)	
training:	Epoch: [30][17/233]	Loss 0.0521 (0.0582)	
training:	Epoch: [30][18/233]	Loss 0.0983 (0.0604)	
training:	Epoch: [30][19/233]	Loss 0.0568 (0.0602)	
training:	Epoch: [30][20/233]	Loss 0.0522 (0.0598)	
training:	Epoch: [30][21/233]	Loss 0.0448 (0.0591)	
training:	Epoch: [30][22/233]	Loss 0.0557 (0.0589)	
training:	Epoch: [30][23/233]	Loss 0.0889 (0.0602)	
training:	Epoch: [30][24/233]	Loss 0.0569 (0.0601)	
training:	Epoch: [30][25/233]	Loss 0.0600 (0.0601)	
training:	Epoch: [30][26/233]	Loss 0.0858 (0.0611)	
training:	Epoch: [30][27/233]	Loss 0.0546 (0.0608)	
training:	Epoch: [30][28/233]	Loss 0.0493 (0.0604)	
training:	Epoch: [30][29/233]	Loss 0.0462 (0.0599)	
training:	Epoch: [30][30/233]	Loss 0.0538 (0.0597)	
training:	Epoch: [30][31/233]	Loss 0.0565 (0.0596)	
training:	Epoch: [30][32/233]	Loss 0.0538 (0.0594)	
training:	Epoch: [30][33/233]	Loss 0.0567 (0.0594)	
training:	Epoch: [30][34/233]	Loss 0.0474 (0.0590)	
training:	Epoch: [30][35/233]	Loss 0.0632 (0.0591)	
training:	Epoch: [30][36/233]	Loss 0.0717 (0.0595)	
training:	Epoch: [30][37/233]	Loss 0.0603 (0.0595)	
training:	Epoch: [30][38/233]	Loss 0.0650 (0.0596)	
training:	Epoch: [30][39/233]	Loss 0.0525 (0.0595)	
training:	Epoch: [30][40/233]	Loss 0.0481 (0.0592)	
training:	Epoch: [30][41/233]	Loss 0.0466 (0.0589)	
training:	Epoch: [30][42/233]	Loss 0.0641 (0.0590)	
training:	Epoch: [30][43/233]	Loss 0.0739 (0.0593)	
training:	Epoch: [30][44/233]	Loss 0.0661 (0.0595)	
training:	Epoch: [30][45/233]	Loss 0.0523 (0.0593)	
training:	Epoch: [30][46/233]	Loss 0.0556 (0.0593)	
training:	Epoch: [30][47/233]	Loss 0.0482 (0.0590)	
training:	Epoch: [30][48/233]	Loss 0.0454 (0.0587)	
training:	Epoch: [30][49/233]	Loss 0.0559 (0.0587)	
training:	Epoch: [30][50/233]	Loss 0.0537 (0.0586)	
training:	Epoch: [30][51/233]	Loss 0.0606 (0.0586)	
training:	Epoch: [30][52/233]	Loss 0.0482 (0.0584)	
training:	Epoch: [30][53/233]	Loss 0.0438 (0.0581)	
training:	Epoch: [30][54/233]	Loss 0.0593 (0.0582)	
training:	Epoch: [30][55/233]	Loss 0.0558 (0.0581)	
training:	Epoch: [30][56/233]	Loss 0.0554 (0.0581)	
training:	Epoch: [30][57/233]	Loss 0.0477 (0.0579)	
training:	Epoch: [30][58/233]	Loss 0.0665 (0.0580)	
training:	Epoch: [30][59/233]	Loss 0.0460 (0.0578)	
training:	Epoch: [30][60/233]	Loss 0.0504 (0.0577)	
training:	Epoch: [30][61/233]	Loss 0.0550 (0.0577)	
training:	Epoch: [30][62/233]	Loss 0.0582 (0.0577)	
training:	Epoch: [30][63/233]	Loss 0.0569 (0.0577)	
training:	Epoch: [30][64/233]	Loss 0.0468 (0.0575)	
training:	Epoch: [30][65/233]	Loss 0.0474 (0.0573)	
training:	Epoch: [30][66/233]	Loss 0.0535 (0.0573)	
training:	Epoch: [30][67/233]	Loss 0.0607 (0.0573)	
training:	Epoch: [30][68/233]	Loss 0.0597 (0.0574)	
training:	Epoch: [30][69/233]	Loss 0.0515 (0.0573)	
training:	Epoch: [30][70/233]	Loss 0.0595 (0.0573)	
training:	Epoch: [30][71/233]	Loss 0.0476 (0.0572)	
training:	Epoch: [30][72/233]	Loss 0.0809 (0.0575)	
training:	Epoch: [30][73/233]	Loss 0.0612 (0.0576)	
training:	Epoch: [30][74/233]	Loss 0.0568 (0.0575)	
training:	Epoch: [30][75/233]	Loss 0.0621 (0.0576)	
training:	Epoch: [30][76/233]	Loss 0.0549 (0.0576)	
training:	Epoch: [30][77/233]	Loss 0.0494 (0.0575)	
training:	Epoch: [30][78/233]	Loss 0.0518 (0.0574)	
training:	Epoch: [30][79/233]	Loss 0.0538 (0.0574)	
training:	Epoch: [30][80/233]	Loss 0.0520 (0.0573)	
training:	Epoch: [30][81/233]	Loss 0.0688 (0.0574)	
training:	Epoch: [30][82/233]	Loss 0.0544 (0.0574)	
training:	Epoch: [30][83/233]	Loss 0.0713 (0.0576)	
training:	Epoch: [30][84/233]	Loss 0.0629 (0.0576)	
training:	Epoch: [30][85/233]	Loss 0.0485 (0.0575)	
training:	Epoch: [30][86/233]	Loss 0.0811 (0.0578)	
training:	Epoch: [30][87/233]	Loss 0.0590 (0.0578)	
training:	Epoch: [30][88/233]	Loss 0.0502 (0.0577)	
training:	Epoch: [30][89/233]	Loss 0.0441 (0.0576)	
training:	Epoch: [30][90/233]	Loss 0.0459 (0.0574)	
training:	Epoch: [30][91/233]	Loss 0.0410 (0.0573)	
training:	Epoch: [30][92/233]	Loss 0.0410 (0.0571)	
training:	Epoch: [30][93/233]	Loss 0.0591 (0.0571)	
training:	Epoch: [30][94/233]	Loss 0.1097 (0.0577)	
training:	Epoch: [30][95/233]	Loss 0.0460 (0.0575)	
training:	Epoch: [30][96/233]	Loss 0.0941 (0.0579)	
training:	Epoch: [30][97/233]	Loss 0.0522 (0.0579)	
training:	Epoch: [30][98/233]	Loss 0.0532 (0.0578)	
training:	Epoch: [30][99/233]	Loss 0.0464 (0.0577)	
training:	Epoch: [30][100/233]	Loss 0.0930 (0.0580)	
training:	Epoch: [30][101/233]	Loss 0.0464 (0.0579)	
training:	Epoch: [30][102/233]	Loss 0.0669 (0.0580)	
training:	Epoch: [30][103/233]	Loss 0.0846 (0.0583)	
training:	Epoch: [30][104/233]	Loss 0.1549 (0.0592)	
training:	Epoch: [30][105/233]	Loss 0.0687 (0.0593)	
training:	Epoch: [30][106/233]	Loss 0.0517 (0.0592)	
training:	Epoch: [30][107/233]	Loss 0.0477 (0.0591)	
training:	Epoch: [30][108/233]	Loss 0.0842 (0.0593)	
training:	Epoch: [30][109/233]	Loss 0.0442 (0.0592)	
training:	Epoch: [30][110/233]	Loss 0.0497 (0.0591)	
training:	Epoch: [30][111/233]	Loss 0.0439 (0.0590)	
training:	Epoch: [30][112/233]	Loss 0.0487 (0.0589)	
training:	Epoch: [30][113/233]	Loss 0.1598 (0.0598)	
training:	Epoch: [30][114/233]	Loss 0.0735 (0.0599)	
training:	Epoch: [30][115/233]	Loss 0.0584 (0.0599)	
training:	Epoch: [30][116/233]	Loss 0.0578 (0.0599)	
training:	Epoch: [30][117/233]	Loss 0.0541 (0.0598)	
training:	Epoch: [30][118/233]	Loss 0.0560 (0.0598)	
training:	Epoch: [30][119/233]	Loss 0.0763 (0.0599)	
training:	Epoch: [30][120/233]	Loss 0.0715 (0.0600)	
training:	Epoch: [30][121/233]	Loss 0.0570 (0.0600)	
training:	Epoch: [30][122/233]	Loss 0.0453 (0.0599)	
training:	Epoch: [30][123/233]	Loss 0.0695 (0.0600)	
training:	Epoch: [30][124/233]	Loss 0.0543 (0.0599)	
training:	Epoch: [30][125/233]	Loss 0.0534 (0.0599)	
training:	Epoch: [30][126/233]	Loss 0.0519 (0.0598)	
training:	Epoch: [30][127/233]	Loss 0.0778 (0.0599)	
training:	Epoch: [30][128/233]	Loss 0.0455 (0.0598)	
training:	Epoch: [30][129/233]	Loss 0.0527 (0.0598)	
training:	Epoch: [30][130/233]	Loss 0.0605 (0.0598)	
training:	Epoch: [30][131/233]	Loss 0.0489 (0.0597)	
training:	Epoch: [30][132/233]	Loss 0.0743 (0.0598)	
training:	Epoch: [30][133/233]	Loss 0.0493 (0.0597)	
training:	Epoch: [30][134/233]	Loss 0.0532 (0.0597)	
training:	Epoch: [30][135/233]	Loss 0.0576 (0.0597)	
training:	Epoch: [30][136/233]	Loss 0.0475 (0.0596)	
training:	Epoch: [30][137/233]	Loss 0.3117 (0.0614)	
training:	Epoch: [30][138/233]	Loss 0.0603 (0.0614)	
training:	Epoch: [30][139/233]	Loss 0.0569 (0.0614)	
training:	Epoch: [30][140/233]	Loss 0.0591 (0.0614)	
training:	Epoch: [30][141/233]	Loss 0.0492 (0.0613)	
training:	Epoch: [30][142/233]	Loss 0.0496 (0.0612)	
training:	Epoch: [30][143/233]	Loss 0.0824 (0.0613)	
training:	Epoch: [30][144/233]	Loss 0.0834 (0.0615)	
training:	Epoch: [30][145/233]	Loss 0.0756 (0.0616)	
training:	Epoch: [30][146/233]	Loss 0.0560 (0.0616)	
training:	Epoch: [30][147/233]	Loss 0.0502 (0.0615)	
training:	Epoch: [30][148/233]	Loss 0.0467 (0.0614)	
training:	Epoch: [30][149/233]	Loss 0.0479 (0.0613)	
training:	Epoch: [30][150/233]	Loss 0.0442 (0.0612)	
training:	Epoch: [30][151/233]	Loss 0.0800 (0.0613)	
training:	Epoch: [30][152/233]	Loss 0.0538 (0.0612)	
training:	Epoch: [30][153/233]	Loss 0.0693 (0.0613)	
training:	Epoch: [30][154/233]	Loss 0.0656 (0.0613)	
training:	Epoch: [30][155/233]	Loss 0.0745 (0.0614)	
training:	Epoch: [30][156/233]	Loss 0.0521 (0.0613)	
training:	Epoch: [30][157/233]	Loss 0.0525 (0.0613)	
training:	Epoch: [30][158/233]	Loss 0.0527 (0.0612)	
training:	Epoch: [30][159/233]	Loss 0.0606 (0.0612)	
training:	Epoch: [30][160/233]	Loss 0.0604 (0.0612)	
training:	Epoch: [30][161/233]	Loss 0.0875 (0.0614)	
training:	Epoch: [30][162/233]	Loss 0.0637 (0.0614)	
training:	Epoch: [30][163/233]	Loss 0.0474 (0.0613)	
training:	Epoch: [30][164/233]	Loss 0.0785 (0.0614)	
training:	Epoch: [30][165/233]	Loss 0.0556 (0.0614)	
training:	Epoch: [30][166/233]	Loss 0.0643 (0.0614)	
training:	Epoch: [30][167/233]	Loss 0.0536 (0.0614)	
training:	Epoch: [30][168/233]	Loss 0.0486 (0.0613)	
training:	Epoch: [30][169/233]	Loss 0.0520 (0.0612)	
training:	Epoch: [30][170/233]	Loss 0.0491 (0.0612)	
training:	Epoch: [30][171/233]	Loss 0.0843 (0.0613)	
training:	Epoch: [30][172/233]	Loss 0.0452 (0.0612)	
training:	Epoch: [30][173/233]	Loss 0.0730 (0.0613)	
training:	Epoch: [30][174/233]	Loss 0.0508 (0.0612)	
training:	Epoch: [30][175/233]	Loss 0.0551 (0.0612)	
training:	Epoch: [30][176/233]	Loss 0.0557 (0.0611)	
training:	Epoch: [30][177/233]	Loss 0.0621 (0.0611)	
training:	Epoch: [30][178/233]	Loss 0.0585 (0.0611)	
training:	Epoch: [30][179/233]	Loss 0.0827 (0.0613)	
training:	Epoch: [30][180/233]	Loss 0.0552 (0.0612)	
training:	Epoch: [30][181/233]	Loss 0.0394 (0.0611)	
training:	Epoch: [30][182/233]	Loss 0.0478 (0.0610)	
training:	Epoch: [30][183/233]	Loss 0.0544 (0.0610)	
training:	Epoch: [30][184/233]	Loss 0.0692 (0.0610)	
training:	Epoch: [30][185/233]	Loss 0.0445 (0.0609)	
training:	Epoch: [30][186/233]	Loss 0.0561 (0.0609)	
training:	Epoch: [30][187/233]	Loss 0.0802 (0.0610)	
training:	Epoch: [30][188/233]	Loss 0.0554 (0.0610)	
training:	Epoch: [30][189/233]	Loss 0.0484 (0.0609)	
training:	Epoch: [30][190/233]	Loss 0.0534 (0.0609)	
training:	Epoch: [30][191/233]	Loss 0.0421 (0.0608)	
training:	Epoch: [30][192/233]	Loss 0.0502 (0.0607)	
training:	Epoch: [30][193/233]	Loss 0.0653 (0.0608)	
training:	Epoch: [30][194/233]	Loss 0.0883 (0.0609)	
training:	Epoch: [30][195/233]	Loss 0.0506 (0.0608)	
training:	Epoch: [30][196/233]	Loss 0.0500 (0.0608)	
training:	Epoch: [30][197/233]	Loss 0.0632 (0.0608)	
training:	Epoch: [30][198/233]	Loss 0.0563 (0.0608)	
training:	Epoch: [30][199/233]	Loss 0.0468 (0.0607)	
training:	Epoch: [30][200/233]	Loss 0.1025 (0.0609)	
training:	Epoch: [30][201/233]	Loss 0.0556 (0.0609)	
training:	Epoch: [30][202/233]	Loss 0.0611 (0.0609)	
training:	Epoch: [30][203/233]	Loss 0.0522 (0.0609)	
training:	Epoch: [30][204/233]	Loss 0.0462 (0.0608)	
training:	Epoch: [30][205/233]	Loss 0.0577 (0.0608)	
training:	Epoch: [30][206/233]	Loss 0.0568 (0.0607)	
training:	Epoch: [30][207/233]	Loss 0.0749 (0.0608)	
training:	Epoch: [30][208/233]	Loss 0.1046 (0.0610)	
training:	Epoch: [30][209/233]	Loss 0.0673 (0.0611)	
training:	Epoch: [30][210/233]	Loss 0.0455 (0.0610)	
training:	Epoch: [30][211/233]	Loss 0.0483 (0.0609)	
training:	Epoch: [30][212/233]	Loss 0.0641 (0.0609)	
training:	Epoch: [30][213/233]	Loss 0.0596 (0.0609)	
training:	Epoch: [30][214/233]	Loss 0.0677 (0.0610)	
training:	Epoch: [30][215/233]	Loss 0.0484 (0.0609)	
training:	Epoch: [30][216/233]	Loss 0.0426 (0.0608)	
training:	Epoch: [30][217/233]	Loss 0.0820 (0.0609)	
training:	Epoch: [30][218/233]	Loss 0.0562 (0.0609)	
training:	Epoch: [30][219/233]	Loss 0.0664 (0.0609)	
training:	Epoch: [30][220/233]	Loss 0.0564 (0.0609)	
training:	Epoch: [30][221/233]	Loss 0.0679 (0.0609)	
training:	Epoch: [30][222/233]	Loss 0.0594 (0.0609)	
training:	Epoch: [30][223/233]	Loss 0.0915 (0.0611)	
training:	Epoch: [30][224/233]	Loss 0.0472 (0.0610)	
training:	Epoch: [30][225/233]	Loss 0.0429 (0.0609)	
training:	Epoch: [30][226/233]	Loss 0.0544 (0.0609)	
training:	Epoch: [30][227/233]	Loss 0.0545 (0.0609)	
training:	Epoch: [30][228/233]	Loss 0.0900 (0.0610)	
training:	Epoch: [30][229/233]	Loss 0.0491 (0.0609)	
training:	Epoch: [30][230/233]	Loss 0.0586 (0.0609)	
training:	Epoch: [30][231/233]	Loss 0.0534 (0.0609)	
training:	Epoch: [30][232/233]	Loss 0.0547 (0.0609)	
training:	Epoch: [30][233/233]	Loss 0.0545 (0.0608)	
Training:	 Loss: 0.0607

Training:	 ACC: 1.0000 1.0000 1.0000 1.0000
Validation:	 ACC: 0.8666 0.8662 0.8588 0.8744
Validation:	 Best_BACC: 0.8666 0.8662 0.8588 0.8744
Validation:	 Loss: 0.3200
Pretraining:	Epoch 31/200
----------
training:	Epoch: [31][1/233]	Loss 0.0541 (0.0541)	
training:	Epoch: [31][2/233]	Loss 0.0942 (0.0741)	
training:	Epoch: [31][3/233]	Loss 0.0444 (0.0642)	
training:	Epoch: [31][4/233]	Loss 0.0498 (0.0606)	
training:	Epoch: [31][5/233]	Loss 0.0435 (0.0572)	
training:	Epoch: [31][6/233]	Loss 0.0610 (0.0578)	
training:	Epoch: [31][7/233]	Loss 0.0483 (0.0564)	
training:	Epoch: [31][8/233]	Loss 0.0399 (0.0544)	
training:	Epoch: [31][9/233]	Loss 0.0615 (0.0552)	
training:	Epoch: [31][10/233]	Loss 0.0565 (0.0553)	
training:	Epoch: [31][11/233]	Loss 0.0454 (0.0544)	
training:	Epoch: [31][12/233]	Loss 0.0494 (0.0540)	
training:	Epoch: [31][13/233]	Loss 0.0634 (0.0547)	
training:	Epoch: [31][14/233]	Loss 0.0509 (0.0544)	
training:	Epoch: [31][15/233]	Loss 0.0686 (0.0554)	
training:	Epoch: [31][16/233]	Loss 0.0530 (0.0552)	
training:	Epoch: [31][17/233]	Loss 0.0575 (0.0554)	
training:	Epoch: [31][18/233]	Loss 0.0561 (0.0554)	
training:	Epoch: [31][19/233]	Loss 0.0678 (0.0561)	
training:	Epoch: [31][20/233]	Loss 0.0477 (0.0557)	
training:	Epoch: [31][21/233]	Loss 0.0460 (0.0552)	
training:	Epoch: [31][22/233]	Loss 0.0497 (0.0549)	
training:	Epoch: [31][23/233]	Loss 0.0495 (0.0547)	
training:	Epoch: [31][24/233]	Loss 0.0634 (0.0551)	
training:	Epoch: [31][25/233]	Loss 0.0488 (0.0548)	
training:	Epoch: [31][26/233]	Loss 0.1234 (0.0575)	
training:	Epoch: [31][27/233]	Loss 0.0439 (0.0570)	
training:	Epoch: [31][28/233]	Loss 0.0443 (0.0565)	
training:	Epoch: [31][29/233]	Loss 0.1199 (0.0587)	
training:	Epoch: [31][30/233]	Loss 0.0425 (0.0582)	
training:	Epoch: [31][31/233]	Loss 0.0504 (0.0579)	
training:	Epoch: [31][32/233]	Loss 0.0486 (0.0576)	
training:	Epoch: [31][33/233]	Loss 0.0433 (0.0572)	
training:	Epoch: [31][34/233]	Loss 0.0670 (0.0575)	
training:	Epoch: [31][35/233]	Loss 0.0497 (0.0572)	
training:	Epoch: [31][36/233]	Loss 0.0664 (0.0575)	
training:	Epoch: [31][37/233]	Loss 0.0473 (0.0572)	
training:	Epoch: [31][38/233]	Loss 0.0471 (0.0570)	
training:	Epoch: [31][39/233]	Loss 0.0413 (0.0566)	
training:	Epoch: [31][40/233]	Loss 0.0568 (0.0566)	
training:	Epoch: [31][41/233]	Loss 0.0457 (0.0563)	
training:	Epoch: [31][42/233]	Loss 0.0549 (0.0563)	
training:	Epoch: [31][43/233]	Loss 0.0497 (0.0561)	
training:	Epoch: [31][44/233]	Loss 0.0447 (0.0559)	
training:	Epoch: [31][45/233]	Loss 0.0657 (0.0561)	
training:	Epoch: [31][46/233]	Loss 0.1878 (0.0589)	
training:	Epoch: [31][47/233]	Loss 0.0448 (0.0586)	
training:	Epoch: [31][48/233]	Loss 0.0457 (0.0584)	
training:	Epoch: [31][49/233]	Loss 0.0543 (0.0583)	
training:	Epoch: [31][50/233]	Loss 0.0899 (0.0589)	
training:	Epoch: [31][51/233]	Loss 0.0563 (0.0589)	
training:	Epoch: [31][52/233]	Loss 0.0534 (0.0588)	
training:	Epoch: [31][53/233]	Loss 0.0489 (0.0586)	
training:	Epoch: [31][54/233]	Loss 0.1242 (0.0598)	
training:	Epoch: [31][55/233]	Loss 0.0488 (0.0596)	
training:	Epoch: [31][56/233]	Loss 0.0471 (0.0594)	
training:	Epoch: [31][57/233]	Loss 0.0449 (0.0591)	
training:	Epoch: [31][58/233]	Loss 0.0582 (0.0591)	
training:	Epoch: [31][59/233]	Loss 0.0517 (0.0590)	
training:	Epoch: [31][60/233]	Loss 0.0523 (0.0589)	
training:	Epoch: [31][61/233]	Loss 0.2108 (0.0613)	
training:	Epoch: [31][62/233]	Loss 0.0444 (0.0611)	
training:	Epoch: [31][63/233]	Loss 0.0507 (0.0609)	
training:	Epoch: [31][64/233]	Loss 0.0536 (0.0608)	
training:	Epoch: [31][65/233]	Loss 0.0502 (0.0606)	
training:	Epoch: [31][66/233]	Loss 0.0378 (0.0603)	
training:	Epoch: [31][67/233]	Loss 0.0438 (0.0600)	
training:	Epoch: [31][68/233]	Loss 0.0495 (0.0599)	
training:	Epoch: [31][69/233]	Loss 0.0510 (0.0598)	
training:	Epoch: [31][70/233]	Loss 0.1639 (0.0612)	
training:	Epoch: [31][71/233]	Loss 0.0455 (0.0610)	
training:	Epoch: [31][72/233]	Loss 0.0482 (0.0608)	
training:	Epoch: [31][73/233]	Loss 0.0536 (0.0607)	
training:	Epoch: [31][74/233]	Loss 0.0668 (0.0608)	
training:	Epoch: [31][75/233]	Loss 0.0517 (0.0607)	
training:	Epoch: [31][76/233]	Loss 0.0577 (0.0607)	
training:	Epoch: [31][77/233]	Loss 0.0502 (0.0605)	
training:	Epoch: [31][78/233]	Loss 0.0458 (0.0603)	
training:	Epoch: [31][79/233]	Loss 0.0474 (0.0602)	
training:	Epoch: [31][80/233]	Loss 0.0546 (0.0601)	
training:	Epoch: [31][81/233]	Loss 0.0463 (0.0599)	
training:	Epoch: [31][82/233]	Loss 0.0505 (0.0598)	
training:	Epoch: [31][83/233]	Loss 0.0644 (0.0599)	
training:	Epoch: [31][84/233]	Loss 0.0621 (0.0599)	
training:	Epoch: [31][85/233]	Loss 0.0538 (0.0598)	
training:	Epoch: [31][86/233]	Loss 0.0484 (0.0597)	
training:	Epoch: [31][87/233]	Loss 0.0745 (0.0599)	
training:	Epoch: [31][88/233]	Loss 0.0787 (0.0601)	
training:	Epoch: [31][89/233]	Loss 0.0448 (0.0599)	
training:	Epoch: [31][90/233]	Loss 0.0667 (0.0600)	
training:	Epoch: [31][91/233]	Loss 0.0846 (0.0603)	
training:	Epoch: [31][92/233]	Loss 0.0487 (0.0601)	
training:	Epoch: [31][93/233]	Loss 0.0453 (0.0600)	
training:	Epoch: [31][94/233]	Loss 0.0522 (0.0599)	
training:	Epoch: [31][95/233]	Loss 0.0500 (0.0598)	
training:	Epoch: [31][96/233]	Loss 0.0428 (0.0596)	
training:	Epoch: [31][97/233]	Loss 0.0512 (0.0595)	
training:	Epoch: [31][98/233]	Loss 0.0613 (0.0595)	
training:	Epoch: [31][99/233]	Loss 0.0478 (0.0594)	
training:	Epoch: [31][100/233]	Loss 0.0555 (0.0594)	
training:	Epoch: [31][101/233]	Loss 0.0843 (0.0596)	
training:	Epoch: [31][102/233]	Loss 0.0438 (0.0595)	
training:	Epoch: [31][103/233]	Loss 0.0605 (0.0595)	
training:	Epoch: [31][104/233]	Loss 0.0722 (0.0596)	
training:	Epoch: [31][105/233]	Loss 0.0714 (0.0597)	
training:	Epoch: [31][106/233]	Loss 0.0489 (0.0596)	
training:	Epoch: [31][107/233]	Loss 0.0767 (0.0598)	
training:	Epoch: [31][108/233]	Loss 0.0759 (0.0599)	
training:	Epoch: [31][109/233]	Loss 0.1057 (0.0603)	
training:	Epoch: [31][110/233]	Loss 0.0540 (0.0603)	
training:	Epoch: [31][111/233]	Loss 0.0599 (0.0603)	
training:	Epoch: [31][112/233]	Loss 0.0575 (0.0603)	
training:	Epoch: [31][113/233]	Loss 0.0621 (0.0603)	
training:	Epoch: [31][114/233]	Loss 0.0664 (0.0603)	
training:	Epoch: [31][115/233]	Loss 0.0524 (0.0603)	
training:	Epoch: [31][116/233]	Loss 0.0484 (0.0602)	
training:	Epoch: [31][117/233]	Loss 0.0531 (0.0601)	
training:	Epoch: [31][118/233]	Loss 0.0542 (0.0600)	
training:	Epoch: [31][119/233]	Loss 0.0480 (0.0599)	
training:	Epoch: [31][120/233]	Loss 0.0534 (0.0599)	
training:	Epoch: [31][121/233]	Loss 0.0570 (0.0599)	
training:	Epoch: [31][122/233]	Loss 0.0500 (0.0598)	
training:	Epoch: [31][123/233]	Loss 0.0432 (0.0597)	
training:	Epoch: [31][124/233]	Loss 0.0518 (0.0596)	
training:	Epoch: [31][125/233]	Loss 0.0555 (0.0596)	
training:	Epoch: [31][126/233]	Loss 0.0680 (0.0596)	
training:	Epoch: [31][127/233]	Loss 0.0470 (0.0595)	
training:	Epoch: [31][128/233]	Loss 0.0493 (0.0594)	
training:	Epoch: [31][129/233]	Loss 0.0596 (0.0594)	
training:	Epoch: [31][130/233]	Loss 0.0602 (0.0594)	
training:	Epoch: [31][131/233]	Loss 0.0439 (0.0593)	
training:	Epoch: [31][132/233]	Loss 0.0985 (0.0596)	
training:	Epoch: [31][133/233]	Loss 0.0818 (0.0598)	
training:	Epoch: [31][134/233]	Loss 0.0491 (0.0597)	
training:	Epoch: [31][135/233]	Loss 0.0490 (0.0596)	
training:	Epoch: [31][136/233]	Loss 0.0488 (0.0596)	
training:	Epoch: [31][137/233]	Loss 0.0582 (0.0595)	
training:	Epoch: [31][138/233]	Loss 0.0541 (0.0595)	
training:	Epoch: [31][139/233]	Loss 0.0743 (0.0596)	
training:	Epoch: [31][140/233]	Loss 0.0438 (0.0595)	
training:	Epoch: [31][141/233]	Loss 0.0441 (0.0594)	
training:	Epoch: [31][142/233]	Loss 0.0532 (0.0593)	
training:	Epoch: [31][143/233]	Loss 0.0474 (0.0593)	
training:	Epoch: [31][144/233]	Loss 0.0550 (0.0592)	
training:	Epoch: [31][145/233]	Loss 0.0431 (0.0591)	
training:	Epoch: [31][146/233]	Loss 0.0523 (0.0591)	
training:	Epoch: [31][147/233]	Loss 0.0559 (0.0591)	
training:	Epoch: [31][148/233]	Loss 0.0670 (0.0591)	
training:	Epoch: [31][149/233]	Loss 0.0462 (0.0590)	
training:	Epoch: [31][150/233]	Loss 0.0685 (0.0591)	
training:	Epoch: [31][151/233]	Loss 0.0427 (0.0590)	
training:	Epoch: [31][152/233]	Loss 0.0557 (0.0590)	
training:	Epoch: [31][153/233]	Loss 0.0941 (0.0592)	
training:	Epoch: [31][154/233]	Loss 0.0499 (0.0591)	
training:	Epoch: [31][155/233]	Loss 0.0459 (0.0590)	
training:	Epoch: [31][156/233]	Loss 0.0452 (0.0589)	
training:	Epoch: [31][157/233]	Loss 0.0448 (0.0589)	
training:	Epoch: [31][158/233]	Loss 0.0663 (0.0589)	
training:	Epoch: [31][159/233]	Loss 0.0486 (0.0588)	
training:	Epoch: [31][160/233]	Loss 0.0487 (0.0588)	
training:	Epoch: [31][161/233]	Loss 0.0613 (0.0588)	
training:	Epoch: [31][162/233]	Loss 0.0553 (0.0588)	
training:	Epoch: [31][163/233]	Loss 0.0552 (0.0588)	
training:	Epoch: [31][164/233]	Loss 0.0760 (0.0589)	
training:	Epoch: [31][165/233]	Loss 0.0936 (0.0591)	
training:	Epoch: [31][166/233]	Loss 0.0491 (0.0590)	
training:	Epoch: [31][167/233]	Loss 0.0407 (0.0589)	
training:	Epoch: [31][168/233]	Loss 0.0494 (0.0588)	
training:	Epoch: [31][169/233]	Loss 0.0554 (0.0588)	
training:	Epoch: [31][170/233]	Loss 0.0458 (0.0587)	
training:	Epoch: [31][171/233]	Loss 0.0543 (0.0587)	
training:	Epoch: [31][172/233]	Loss 0.0529 (0.0587)	
training:	Epoch: [31][173/233]	Loss 0.0476 (0.0586)	
training:	Epoch: [31][174/233]	Loss 0.0526 (0.0586)	
training:	Epoch: [31][175/233]	Loss 0.0833 (0.0587)	
training:	Epoch: [31][176/233]	Loss 0.0469 (0.0587)	
training:	Epoch: [31][177/233]	Loss 0.0470 (0.0586)	
training:	Epoch: [31][178/233]	Loss 0.0760 (0.0587)	
training:	Epoch: [31][179/233]	Loss 0.0617 (0.0587)	
training:	Epoch: [31][180/233]	Loss 0.0497 (0.0587)	
training:	Epoch: [31][181/233]	Loss 0.0582 (0.0587)	
training:	Epoch: [31][182/233]	Loss 0.0489 (0.0586)	
training:	Epoch: [31][183/233]	Loss 0.0550 (0.0586)	
training:	Epoch: [31][184/233]	Loss 0.0675 (0.0586)	
training:	Epoch: [31][185/233]	Loss 0.0537 (0.0586)	
training:	Epoch: [31][186/233]	Loss 0.0856 (0.0587)	
training:	Epoch: [31][187/233]	Loss 0.0398 (0.0586)	
training:	Epoch: [31][188/233]	Loss 0.0485 (0.0586)	
training:	Epoch: [31][189/233]	Loss 0.0643 (0.0586)	
training:	Epoch: [31][190/233]	Loss 0.0940 (0.0588)	
training:	Epoch: [31][191/233]	Loss 0.0846 (0.0589)	
training:	Epoch: [31][192/233]	Loss 0.0487 (0.0589)	
training:	Epoch: [31][193/233]	Loss 0.0420 (0.0588)	
training:	Epoch: [31][194/233]	Loss 0.0568 (0.0588)	
training:	Epoch: [31][195/233]	Loss 0.0584 (0.0588)	
training:	Epoch: [31][196/233]	Loss 0.0584 (0.0588)	
training:	Epoch: [31][197/233]	Loss 0.0396 (0.0587)	
training:	Epoch: [31][198/233]	Loss 0.0637 (0.0587)	
training:	Epoch: [31][199/233]	Loss 0.0452 (0.0586)	
training:	Epoch: [31][200/233]	Loss 0.0727 (0.0587)	
training:	Epoch: [31][201/233]	Loss 0.0448 (0.0587)	
training:	Epoch: [31][202/233]	Loss 0.0642 (0.0587)	
training:	Epoch: [31][203/233]	Loss 0.0651 (0.0587)	
training:	Epoch: [31][204/233]	Loss 0.0428 (0.0586)	
training:	Epoch: [31][205/233]	Loss 0.0499 (0.0586)	
training:	Epoch: [31][206/233]	Loss 0.0648 (0.0586)	
training:	Epoch: [31][207/233]	Loss 0.0786 (0.0587)	
training:	Epoch: [31][208/233]	Loss 0.0471 (0.0587)	
training:	Epoch: [31][209/233]	Loss 0.0531 (0.0586)	
training:	Epoch: [31][210/233]	Loss 0.0483 (0.0586)	
training:	Epoch: [31][211/233]	Loss 0.0725 (0.0586)	
training:	Epoch: [31][212/233]	Loss 0.0513 (0.0586)	
training:	Epoch: [31][213/233]	Loss 0.0457 (0.0586)	
training:	Epoch: [31][214/233]	Loss 0.0512 (0.0585)	
training:	Epoch: [31][215/233]	Loss 0.0531 (0.0585)	
training:	Epoch: [31][216/233]	Loss 0.0488 (0.0584)	
training:	Epoch: [31][217/233]	Loss 0.0464 (0.0584)	
training:	Epoch: [31][218/233]	Loss 0.0598 (0.0584)	
training:	Epoch: [31][219/233]	Loss 0.0482 (0.0584)	
training:	Epoch: [31][220/233]	Loss 0.0479 (0.0583)	
training:	Epoch: [31][221/233]	Loss 0.0777 (0.0584)	
training:	Epoch: [31][222/233]	Loss 0.0502 (0.0584)	
training:	Epoch: [31][223/233]	Loss 0.0534 (0.0583)	
training:	Epoch: [31][224/233]	Loss 0.0555 (0.0583)	
training:	Epoch: [31][225/233]	Loss 0.0533 (0.0583)	
training:	Epoch: [31][226/233]	Loss 0.0536 (0.0583)	
training:	Epoch: [31][227/233]	Loss 0.0567 (0.0583)	
training:	Epoch: [31][228/233]	Loss 0.0395 (0.0582)	
training:	Epoch: [31][229/233]	Loss 0.0510 (0.0582)	
training:	Epoch: [31][230/233]	Loss 0.0494 (0.0581)	
training:	Epoch: [31][231/233]	Loss 0.0552 (0.0581)	
training:	Epoch: [31][232/233]	Loss 0.0549 (0.0581)	
training:	Epoch: [31][233/233]	Loss 0.0491 (0.0581)	
Training:	 Loss: 0.0579

Training:	 ACC: 1.0000 1.0000 1.0000 1.0000
Validation:	 ACC: 0.8660 0.8646 0.8352 0.8969
Validation:	 Best_BACC: 0.8666 0.8662 0.8588 0.8744
Validation:	 Loss: 0.3251
Pretraining:	Epoch 32/200
----------
training:	Epoch: [32][1/233]	Loss 0.0416 (0.0416)	
training:	Epoch: [32][2/233]	Loss 0.0458 (0.0437)	
training:	Epoch: [32][3/233]	Loss 0.0435 (0.0436)	
training:	Epoch: [32][4/233]	Loss 0.0908 (0.0554)	
training:	Epoch: [32][5/233]	Loss 0.0506 (0.0545)	
training:	Epoch: [32][6/233]	Loss 0.0668 (0.0565)	
training:	Epoch: [32][7/233]	Loss 0.0566 (0.0565)	
training:	Epoch: [32][8/233]	Loss 0.0402 (0.0545)	
training:	Epoch: [32][9/233]	Loss 0.0495 (0.0539)	
training:	Epoch: [32][10/233]	Loss 0.0479 (0.0533)	
training:	Epoch: [32][11/233]	Loss 0.0546 (0.0534)	
training:	Epoch: [32][12/233]	Loss 0.0559 (0.0536)	
training:	Epoch: [32][13/233]	Loss 0.0551 (0.0538)	
training:	Epoch: [32][14/233]	Loss 0.0497 (0.0535)	
training:	Epoch: [32][15/233]	Loss 0.0535 (0.0535)	
training:	Epoch: [32][16/233]	Loss 0.0473 (0.0531)	
training:	Epoch: [32][17/233]	Loss 0.0527 (0.0531)	
training:	Epoch: [32][18/233]	Loss 0.0398 (0.0523)	
training:	Epoch: [32][19/233]	Loss 0.0472 (0.0521)	
training:	Epoch: [32][20/233]	Loss 0.0422 (0.0516)	
training:	Epoch: [32][21/233]	Loss 0.0549 (0.0517)	
training:	Epoch: [32][22/233]	Loss 0.0481 (0.0516)	
training:	Epoch: [32][23/233]	Loss 0.0597 (0.0519)	
training:	Epoch: [32][24/233]	Loss 0.0433 (0.0516)	
training:	Epoch: [32][25/233]	Loss 0.0693 (0.0523)	
training:	Epoch: [32][26/233]	Loss 0.0558 (0.0524)	
training:	Epoch: [32][27/233]	Loss 0.0464 (0.0522)	
training:	Epoch: [32][28/233]	Loss 0.0456 (0.0519)	
training:	Epoch: [32][29/233]	Loss 0.0683 (0.0525)	
training:	Epoch: [32][30/233]	Loss 0.0486 (0.0524)	
training:	Epoch: [32][31/233]	Loss 0.0512 (0.0523)	
training:	Epoch: [32][32/233]	Loss 0.0490 (0.0522)	
training:	Epoch: [32][33/233]	Loss 0.0705 (0.0528)	
training:	Epoch: [32][34/233]	Loss 0.0499 (0.0527)	
training:	Epoch: [32][35/233]	Loss 0.0498 (0.0526)	
training:	Epoch: [32][36/233]	Loss 0.0704 (0.0531)	
training:	Epoch: [32][37/233]	Loss 0.0470 (0.0530)	
training:	Epoch: [32][38/233]	Loss 0.0426 (0.0527)	
training:	Epoch: [32][39/233]	Loss 0.0688 (0.0531)	
training:	Epoch: [32][40/233]	Loss 0.0517 (0.0531)	
training:	Epoch: [32][41/233]	Loss 0.0556 (0.0531)	
training:	Epoch: [32][42/233]	Loss 0.0454 (0.0529)	
training:	Epoch: [32][43/233]	Loss 0.0521 (0.0529)	
training:	Epoch: [32][44/233]	Loss 0.0503 (0.0529)	
training:	Epoch: [32][45/233]	Loss 0.0615 (0.0531)	
training:	Epoch: [32][46/233]	Loss 0.0466 (0.0529)	
training:	Epoch: [32][47/233]	Loss 0.0446 (0.0527)	
training:	Epoch: [32][48/233]	Loss 0.0582 (0.0528)	
training:	Epoch: [32][49/233]	Loss 0.0434 (0.0527)	
training:	Epoch: [32][50/233]	Loss 0.0542 (0.0527)	
training:	Epoch: [32][51/233]	Loss 0.0586 (0.0528)	
training:	Epoch: [32][52/233]	Loss 0.0526 (0.0528)	
training:	Epoch: [32][53/233]	Loss 0.0450 (0.0527)	
training:	Epoch: [32][54/233]	Loss 0.0438 (0.0525)	
training:	Epoch: [32][55/233]	Loss 0.0614 (0.0527)	
training:	Epoch: [32][56/233]	Loss 0.1203 (0.0539)	
training:	Epoch: [32][57/233]	Loss 0.0497 (0.0538)	
training:	Epoch: [32][58/233]	Loss 0.0525 (0.0538)	
training:	Epoch: [32][59/233]	Loss 0.0503 (0.0537)	
training:	Epoch: [32][60/233]	Loss 0.0425 (0.0535)	
training:	Epoch: [32][61/233]	Loss 0.0435 (0.0534)	
training:	Epoch: [32][62/233]	Loss 0.0397 (0.0531)	
training:	Epoch: [32][63/233]	Loss 0.0458 (0.0530)	
training:	Epoch: [32][64/233]	Loss 0.0487 (0.0529)	
training:	Epoch: [32][65/233]	Loss 0.0477 (0.0529)	
training:	Epoch: [32][66/233]	Loss 0.1311 (0.0541)	
training:	Epoch: [32][67/233]	Loss 0.0495 (0.0540)	
training:	Epoch: [32][68/233]	Loss 0.0536 (0.0540)	
training:	Epoch: [32][69/233]	Loss 0.0459 (0.0539)	
training:	Epoch: [32][70/233]	Loss 0.0407 (0.0537)	
training:	Epoch: [32][71/233]	Loss 0.0538 (0.0537)	
training:	Epoch: [32][72/233]	Loss 0.0439 (0.0535)	
training:	Epoch: [32][73/233]	Loss 0.0674 (0.0537)	
training:	Epoch: [32][74/233]	Loss 0.0443 (0.0536)	
training:	Epoch: [32][75/233]	Loss 0.0527 (0.0536)	
training:	Epoch: [32][76/233]	Loss 0.0489 (0.0535)	
training:	Epoch: [32][77/233]	Loss 0.0427 (0.0534)	
training:	Epoch: [32][78/233]	Loss 0.0460 (0.0533)	
training:	Epoch: [32][79/233]	Loss 0.0565 (0.0533)	
training:	Epoch: [32][80/233]	Loss 0.0534 (0.0533)	
training:	Epoch: [32][81/233]	Loss 0.0456 (0.0532)	
training:	Epoch: [32][82/233]	Loss 0.0624 (0.0534)	
training:	Epoch: [32][83/233]	Loss 0.0528 (0.0533)	
training:	Epoch: [32][84/233]	Loss 0.0476 (0.0533)	
training:	Epoch: [32][85/233]	Loss 0.0762 (0.0535)	
training:	Epoch: [32][86/233]	Loss 0.0445 (0.0534)	
training:	Epoch: [32][87/233]	Loss 0.0525 (0.0534)	
training:	Epoch: [32][88/233]	Loss 0.0659 (0.0536)	
training:	Epoch: [32][89/233]	Loss 0.0411 (0.0534)	
training:	Epoch: [32][90/233]	Loss 0.0470 (0.0534)	
training:	Epoch: [32][91/233]	Loss 0.0756 (0.0536)	
training:	Epoch: [32][92/233]	Loss 0.0507 (0.0536)	
training:	Epoch: [32][93/233]	Loss 0.0741 (0.0538)	
training:	Epoch: [32][94/233]	Loss 0.0423 (0.0537)	
training:	Epoch: [32][95/233]	Loss 0.0482 (0.0536)	
training:	Epoch: [32][96/233]	Loss 0.0520 (0.0536)	
training:	Epoch: [32][97/233]	Loss 0.0408 (0.0535)	
training:	Epoch: [32][98/233]	Loss 0.0545 (0.0535)	
training:	Epoch: [32][99/233]	Loss 0.0456 (0.0534)	
training:	Epoch: [32][100/233]	Loss 0.0757 (0.0536)	
training:	Epoch: [32][101/233]	Loss 0.0376 (0.0535)	
training:	Epoch: [32][102/233]	Loss 0.0617 (0.0535)	
training:	Epoch: [32][103/233]	Loss 0.0463 (0.0535)	
training:	Epoch: [32][104/233]	Loss 0.0465 (0.0534)	
training:	Epoch: [32][105/233]	Loss 0.0485 (0.0534)	
training:	Epoch: [32][106/233]	Loss 0.0538 (0.0534)	
training:	Epoch: [32][107/233]	Loss 0.0690 (0.0535)	
training:	Epoch: [32][108/233]	Loss 0.0572 (0.0535)	
training:	Epoch: [32][109/233]	Loss 0.0663 (0.0537)	
training:	Epoch: [32][110/233]	Loss 0.0660 (0.0538)	
training:	Epoch: [32][111/233]	Loss 0.0563 (0.0538)	
training:	Epoch: [32][112/233]	Loss 0.0495 (0.0538)	
training:	Epoch: [32][113/233]	Loss 0.0584 (0.0538)	
training:	Epoch: [32][114/233]	Loss 0.0480 (0.0537)	
training:	Epoch: [32][115/233]	Loss 0.0564 (0.0538)	
training:	Epoch: [32][116/233]	Loss 0.0641 (0.0539)	
training:	Epoch: [32][117/233]	Loss 0.0527 (0.0538)	
training:	Epoch: [32][118/233]	Loss 0.0516 (0.0538)	
training:	Epoch: [32][119/233]	Loss 0.0443 (0.0537)	
training:	Epoch: [32][120/233]	Loss 0.0548 (0.0538)	
training:	Epoch: [32][121/233]	Loss 0.0444 (0.0537)	
training:	Epoch: [32][122/233]	Loss 0.0511 (0.0537)	
training:	Epoch: [32][123/233]	Loss 0.0432 (0.0536)	
training:	Epoch: [32][124/233]	Loss 0.0562 (0.0536)	
training:	Epoch: [32][125/233]	Loss 0.0461 (0.0535)	
training:	Epoch: [32][126/233]	Loss 0.0499 (0.0535)	
training:	Epoch: [32][127/233]	Loss 0.0503 (0.0535)	
training:	Epoch: [32][128/233]	Loss 0.0652 (0.0536)	
training:	Epoch: [32][129/233]	Loss 0.0947 (0.0539)	
training:	Epoch: [32][130/233]	Loss 0.0594 (0.0539)	
training:	Epoch: [32][131/233]	Loss 0.0407 (0.0538)	
training:	Epoch: [32][132/233]	Loss 0.0738 (0.0540)	
training:	Epoch: [32][133/233]	Loss 0.0526 (0.0540)	
training:	Epoch: [32][134/233]	Loss 0.0655 (0.0541)	
training:	Epoch: [32][135/233]	Loss 0.0475 (0.0540)	
training:	Epoch: [32][136/233]	Loss 0.1046 (0.0544)	
training:	Epoch: [32][137/233]	Loss 0.0435 (0.0543)	
training:	Epoch: [32][138/233]	Loss 0.0393 (0.0542)	
training:	Epoch: [32][139/233]	Loss 0.0469 (0.0541)	
training:	Epoch: [32][140/233]	Loss 0.0581 (0.0542)	
training:	Epoch: [32][141/233]	Loss 0.0477 (0.0541)	
training:	Epoch: [32][142/233]	Loss 0.0424 (0.0540)	
training:	Epoch: [32][143/233]	Loss 0.0511 (0.0540)	
training:	Epoch: [32][144/233]	Loss 0.0431 (0.0539)	
training:	Epoch: [32][145/233]	Loss 0.0424 (0.0539)	
training:	Epoch: [32][146/233]	Loss 0.0520 (0.0539)	
training:	Epoch: [32][147/233]	Loss 0.0419 (0.0538)	
training:	Epoch: [32][148/233]	Loss 0.0453 (0.0537)	
training:	Epoch: [32][149/233]	Loss 0.0369 (0.0536)	
training:	Epoch: [32][150/233]	Loss 0.0715 (0.0537)	
training:	Epoch: [32][151/233]	Loss 0.0453 (0.0537)	
training:	Epoch: [32][152/233]	Loss 0.0504 (0.0536)	
training:	Epoch: [32][153/233]	Loss 0.0533 (0.0536)	
training:	Epoch: [32][154/233]	Loss 0.0487 (0.0536)	
training:	Epoch: [32][155/233]	Loss 0.0475 (0.0536)	
training:	Epoch: [32][156/233]	Loss 0.0444 (0.0535)	
training:	Epoch: [32][157/233]	Loss 0.0443 (0.0535)	
training:	Epoch: [32][158/233]	Loss 0.0482 (0.0534)	
training:	Epoch: [32][159/233]	Loss 0.0414 (0.0533)	
training:	Epoch: [32][160/233]	Loss 0.0574 (0.0534)	
training:	Epoch: [32][161/233]	Loss 0.0477 (0.0533)	
training:	Epoch: [32][162/233]	Loss 0.0428 (0.0533)	
training:	Epoch: [32][163/233]	Loss 0.0607 (0.0533)	
training:	Epoch: [32][164/233]	Loss 0.0486 (0.0533)	
training:	Epoch: [32][165/233]	Loss 0.0548 (0.0533)	
training:	Epoch: [32][166/233]	Loss 0.0563 (0.0533)	
training:	Epoch: [32][167/233]	Loss 0.0519 (0.0533)	
training:	Epoch: [32][168/233]	Loss 0.0565 (0.0533)	
training:	Epoch: [32][169/233]	Loss 0.0513 (0.0533)	
training:	Epoch: [32][170/233]	Loss 0.0477 (0.0533)	
training:	Epoch: [32][171/233]	Loss 0.0428 (0.0532)	
training:	Epoch: [32][172/233]	Loss 0.0518 (0.0532)	
training:	Epoch: [32][173/233]	Loss 0.0517 (0.0532)	
training:	Epoch: [32][174/233]	Loss 0.0557 (0.0532)	
training:	Epoch: [32][175/233]	Loss 0.0593 (0.0532)	
training:	Epoch: [32][176/233]	Loss 0.0458 (0.0532)	
training:	Epoch: [32][177/233]	Loss 0.0626 (0.0533)	
training:	Epoch: [32][178/233]	Loss 0.0724 (0.0534)	
training:	Epoch: [32][179/233]	Loss 0.0534 (0.0534)	
training:	Epoch: [32][180/233]	Loss 0.1145 (0.0537)	
training:	Epoch: [32][181/233]	Loss 0.0815 (0.0539)	
training:	Epoch: [32][182/233]	Loss 0.0535 (0.0539)	
training:	Epoch: [32][183/233]	Loss 0.0517 (0.0538)	
training:	Epoch: [32][184/233]	Loss 0.0556 (0.0539)	
training:	Epoch: [32][185/233]	Loss 0.0520 (0.0538)	
training:	Epoch: [32][186/233]	Loss 0.0393 (0.0538)	
training:	Epoch: [32][187/233]	Loss 0.0561 (0.0538)	
training:	Epoch: [32][188/233]	Loss 0.0522 (0.0538)	
training:	Epoch: [32][189/233]	Loss 0.0505 (0.0538)	
training:	Epoch: [32][190/233]	Loss 0.1056 (0.0540)	
training:	Epoch: [32][191/233]	Loss 0.0480 (0.0540)	
training:	Epoch: [32][192/233]	Loss 0.0586 (0.0540)	
training:	Epoch: [32][193/233]	Loss 0.0410 (0.0540)	
training:	Epoch: [32][194/233]	Loss 0.0394 (0.0539)	
training:	Epoch: [32][195/233]	Loss 0.0534 (0.0539)	
training:	Epoch: [32][196/233]	Loss 0.0518 (0.0539)	
training:	Epoch: [32][197/233]	Loss 0.0454 (0.0538)	
training:	Epoch: [32][198/233]	Loss 0.0529 (0.0538)	
training:	Epoch: [32][199/233]	Loss 0.0851 (0.0540)	
training:	Epoch: [32][200/233]	Loss 0.0500 (0.0540)	
training:	Epoch: [32][201/233]	Loss 0.0544 (0.0540)	
training:	Epoch: [32][202/233]	Loss 0.0457 (0.0539)	
training:	Epoch: [32][203/233]	Loss 0.0583 (0.0539)	
training:	Epoch: [32][204/233]	Loss 0.0597 (0.0540)	
training:	Epoch: [32][205/233]	Loss 0.0460 (0.0539)	
training:	Epoch: [32][206/233]	Loss 0.0583 (0.0539)	
training:	Epoch: [32][207/233]	Loss 0.0813 (0.0541)	
training:	Epoch: [32][208/233]	Loss 0.0590 (0.0541)	
training:	Epoch: [32][209/233]	Loss 0.0422 (0.0540)	
training:	Epoch: [32][210/233]	Loss 0.0491 (0.0540)	
training:	Epoch: [32][211/233]	Loss 0.0450 (0.0540)	
training:	Epoch: [32][212/233]	Loss 0.0436 (0.0539)	
training:	Epoch: [32][213/233]	Loss 0.0791 (0.0540)	
training:	Epoch: [32][214/233]	Loss 0.0454 (0.0540)	
training:	Epoch: [32][215/233]	Loss 0.0490 (0.0540)	
training:	Epoch: [32][216/233]	Loss 0.0464 (0.0539)	
training:	Epoch: [32][217/233]	Loss 0.0459 (0.0539)	
training:	Epoch: [32][218/233]	Loss 0.0474 (0.0539)	
training:	Epoch: [32][219/233]	Loss 0.0479 (0.0539)	
training:	Epoch: [32][220/233]	Loss 0.0535 (0.0539)	
training:	Epoch: [32][221/233]	Loss 0.0531 (0.0539)	
training:	Epoch: [32][222/233]	Loss 0.0527 (0.0538)	
training:	Epoch: [32][223/233]	Loss 0.0457 (0.0538)	
training:	Epoch: [32][224/233]	Loss 0.0445 (0.0538)	
training:	Epoch: [32][225/233]	Loss 0.0764 (0.0539)	
training:	Epoch: [32][226/233]	Loss 0.0479 (0.0538)	
training:	Epoch: [32][227/233]	Loss 0.0584 (0.0539)	
training:	Epoch: [32][228/233]	Loss 0.0456 (0.0538)	
training:	Epoch: [32][229/233]	Loss 0.0431 (0.0538)	
training:	Epoch: [32][230/233]	Loss 0.0452 (0.0537)	
training:	Epoch: [32][231/233]	Loss 0.0503 (0.0537)	
training:	Epoch: [32][232/233]	Loss 0.0520 (0.0537)	
training:	Epoch: [32][233/233]	Loss 0.0470 (0.0537)	
Training:	 Loss: 0.0536

Training:	 ACC: 1.0000 1.0000 1.0000 1.0000
Validation:	 ACC: 0.8621 0.8614 0.8475 0.8767
Validation:	 Best_BACC: 0.8666 0.8662 0.8588 0.8744
Validation:	 Loss: 0.3188
Pretraining:	Epoch 33/200
----------
training:	Epoch: [33][1/233]	Loss 0.0530 (0.0530)	
training:	Epoch: [33][2/233]	Loss 0.0895 (0.0713)	
training:	Epoch: [33][3/233]	Loss 0.0505 (0.0643)	
training:	Epoch: [33][4/233]	Loss 0.0404 (0.0583)	
training:	Epoch: [33][5/233]	Loss 0.0433 (0.0553)	
training:	Epoch: [33][6/233]	Loss 0.0563 (0.0555)	
training:	Epoch: [33][7/233]	Loss 0.0544 (0.0553)	
training:	Epoch: [33][8/233]	Loss 0.0380 (0.0532)	
training:	Epoch: [33][9/233]	Loss 0.0658 (0.0546)	
training:	Epoch: [33][10/233]	Loss 0.0494 (0.0541)	
training:	Epoch: [33][11/233]	Loss 0.0399 (0.0528)	
training:	Epoch: [33][12/233]	Loss 0.0495 (0.0525)	
training:	Epoch: [33][13/233]	Loss 0.0510 (0.0524)	
training:	Epoch: [33][14/233]	Loss 0.0432 (0.0517)	
training:	Epoch: [33][15/233]	Loss 0.0477 (0.0515)	
training:	Epoch: [33][16/233]	Loss 0.0481 (0.0513)	
training:	Epoch: [33][17/233]	Loss 0.0416 (0.0507)	
training:	Epoch: [33][18/233]	Loss 0.0660 (0.0515)	
training:	Epoch: [33][19/233]	Loss 0.0391 (0.0509)	
training:	Epoch: [33][20/233]	Loss 0.0430 (0.0505)	
training:	Epoch: [33][21/233]	Loss 0.0413 (0.0500)	
training:	Epoch: [33][22/233]	Loss 0.0434 (0.0497)	
training:	Epoch: [33][23/233]	Loss 0.0749 (0.0508)	
training:	Epoch: [33][24/233]	Loss 0.0433 (0.0505)	
training:	Epoch: [33][25/233]	Loss 0.0413 (0.0501)	
training:	Epoch: [33][26/233]	Loss 0.0444 (0.0499)	
training:	Epoch: [33][27/233]	Loss 0.0466 (0.0498)	
training:	Epoch: [33][28/233]	Loss 0.0345 (0.0493)	
training:	Epoch: [33][29/233]	Loss 0.0685 (0.0499)	
training:	Epoch: [33][30/233]	Loss 0.0572 (0.0502)	
training:	Epoch: [33][31/233]	Loss 0.0376 (0.0498)	
training:	Epoch: [33][32/233]	Loss 0.0412 (0.0495)	
training:	Epoch: [33][33/233]	Loss 0.0549 (0.0497)	
training:	Epoch: [33][34/233]	Loss 0.0515 (0.0497)	
training:	Epoch: [33][35/233]	Loss 0.0517 (0.0498)	
training:	Epoch: [33][36/233]	Loss 0.0463 (0.0497)	
training:	Epoch: [33][37/233]	Loss 0.0720 (0.0503)	
training:	Epoch: [33][38/233]	Loss 0.0455 (0.0501)	
training:	Epoch: [33][39/233]	Loss 0.0450 (0.0500)	
training:	Epoch: [33][40/233]	Loss 0.0432 (0.0498)	
training:	Epoch: [33][41/233]	Loss 0.0462 (0.0498)	
training:	Epoch: [33][42/233]	Loss 0.0639 (0.0501)	
training:	Epoch: [33][43/233]	Loss 0.0498 (0.0501)	
training:	Epoch: [33][44/233]	Loss 0.0453 (0.0500)	
training:	Epoch: [33][45/233]	Loss 0.0441 (0.0498)	
training:	Epoch: [33][46/233]	Loss 0.0399 (0.0496)	
training:	Epoch: [33][47/233]	Loss 0.0435 (0.0495)	
training:	Epoch: [33][48/233]	Loss 0.0421 (0.0493)	
training:	Epoch: [33][49/233]	Loss 0.0498 (0.0494)	
training:	Epoch: [33][50/233]	Loss 0.0470 (0.0493)	
training:	Epoch: [33][51/233]	Loss 0.0432 (0.0492)	
training:	Epoch: [33][52/233]	Loss 0.0377 (0.0490)	
training:	Epoch: [33][53/233]	Loss 0.0888 (0.0497)	
training:	Epoch: [33][54/233]	Loss 0.0486 (0.0497)	
training:	Epoch: [33][55/233]	Loss 0.0492 (0.0497)	
training:	Epoch: [33][56/233]	Loss 0.0461 (0.0496)	
training:	Epoch: [33][57/233]	Loss 0.0421 (0.0495)	
training:	Epoch: [33][58/233]	Loss 0.0411 (0.0493)	
training:	Epoch: [33][59/233]	Loss 0.0541 (0.0494)	
training:	Epoch: [33][60/233]	Loss 0.0769 (0.0499)	
training:	Epoch: [33][61/233]	Loss 0.0412 (0.0497)	
training:	Epoch: [33][62/233]	Loss 0.0414 (0.0496)	
training:	Epoch: [33][63/233]	Loss 0.0475 (0.0496)	
training:	Epoch: [33][64/233]	Loss 0.0478 (0.0496)	
training:	Epoch: [33][65/233]	Loss 0.0639 (0.0498)	
training:	Epoch: [33][66/233]	Loss 0.0675 (0.0500)	
training:	Epoch: [33][67/233]	Loss 0.0461 (0.0500)	
training:	Epoch: [33][68/233]	Loss 0.0786 (0.0504)	
training:	Epoch: [33][69/233]	Loss 0.0649 (0.0506)	
training:	Epoch: [33][70/233]	Loss 0.0490 (0.0506)	
training:	Epoch: [33][71/233]	Loss 0.0452 (0.0505)	
training:	Epoch: [33][72/233]	Loss 0.0724 (0.0508)	
training:	Epoch: [33][73/233]	Loss 0.0433 (0.0507)	
training:	Epoch: [33][74/233]	Loss 0.0412 (0.0506)	
training:	Epoch: [33][75/233]	Loss 0.0475 (0.0505)	
training:	Epoch: [33][76/233]	Loss 0.0624 (0.0507)	
training:	Epoch: [33][77/233]	Loss 0.0441 (0.0506)	
training:	Epoch: [33][78/233]	Loss 0.0803 (0.0510)	
training:	Epoch: [33][79/233]	Loss 0.0538 (0.0510)	
training:	Epoch: [33][80/233]	Loss 0.0503 (0.0510)	
training:	Epoch: [33][81/233]	Loss 0.0525 (0.0510)	
training:	Epoch: [33][82/233]	Loss 0.0633 (0.0512)	
training:	Epoch: [33][83/233]	Loss 0.1290 (0.0521)	
training:	Epoch: [33][84/233]	Loss 0.0437 (0.0520)	
training:	Epoch: [33][85/233]	Loss 0.0455 (0.0520)	
training:	Epoch: [33][86/233]	Loss 0.0478 (0.0519)	
training:	Epoch: [33][87/233]	Loss 0.0417 (0.0518)	
training:	Epoch: [33][88/233]	Loss 0.1707 (0.0531)	
training:	Epoch: [33][89/233]	Loss 0.0422 (0.0530)	
training:	Epoch: [33][90/233]	Loss 0.0663 (0.0532)	
training:	Epoch: [33][91/233]	Loss 0.0703 (0.0534)	
training:	Epoch: [33][92/233]	Loss 0.0526 (0.0533)	
training:	Epoch: [33][93/233]	Loss 0.0446 (0.0532)	
training:	Epoch: [33][94/233]	Loss 0.0436 (0.0531)	
training:	Epoch: [33][95/233]	Loss 0.0858 (0.0535)	
training:	Epoch: [33][96/233]	Loss 0.0901 (0.0539)	
training:	Epoch: [33][97/233]	Loss 0.0447 (0.0538)	
training:	Epoch: [33][98/233]	Loss 0.0512 (0.0538)	
training:	Epoch: [33][99/233]	Loss 0.0547 (0.0538)	
training:	Epoch: [33][100/233]	Loss 0.0423 (0.0536)	
training:	Epoch: [33][101/233]	Loss 0.0533 (0.0536)	
training:	Epoch: [33][102/233]	Loss 0.0655 (0.0538)	
training:	Epoch: [33][103/233]	Loss 0.0417 (0.0536)	
training:	Epoch: [33][104/233]	Loss 0.0686 (0.0538)	
training:	Epoch: [33][105/233]	Loss 0.0394 (0.0536)	
training:	Epoch: [33][106/233]	Loss 0.0385 (0.0535)	
training:	Epoch: [33][107/233]	Loss 0.0445 (0.0534)	
training:	Epoch: [33][108/233]	Loss 0.0909 (0.0538)	
training:	Epoch: [33][109/233]	Loss 0.0738 (0.0540)	
training:	Epoch: [33][110/233]	Loss 0.0407 (0.0538)	
training:	Epoch: [33][111/233]	Loss 0.1104 (0.0543)	
training:	Epoch: [33][112/233]	Loss 0.0565 (0.0544)	
training:	Epoch: [33][113/233]	Loss 0.0770 (0.0546)	
training:	Epoch: [33][114/233]	Loss 0.0397 (0.0544)	
training:	Epoch: [33][115/233]	Loss 0.0467 (0.0544)	
training:	Epoch: [33][116/233]	Loss 0.0458 (0.0543)	
training:	Epoch: [33][117/233]	Loss 0.0402 (0.0542)	
training:	Epoch: [33][118/233]	Loss 0.0408 (0.0541)	
training:	Epoch: [33][119/233]	Loss 0.0542 (0.0541)	
training:	Epoch: [33][120/233]	Loss 0.0539 (0.0541)	
training:	Epoch: [33][121/233]	Loss 0.0427 (0.0540)	
training:	Epoch: [33][122/233]	Loss 0.0600 (0.0540)	
training:	Epoch: [33][123/233]	Loss 0.0459 (0.0539)	
training:	Epoch: [33][124/233]	Loss 0.0521 (0.0539)	
training:	Epoch: [33][125/233]	Loss 0.0575 (0.0540)	
training:	Epoch: [33][126/233]	Loss 0.0406 (0.0539)	
training:	Epoch: [33][127/233]	Loss 0.0582 (0.0539)	
training:	Epoch: [33][128/233]	Loss 0.0502 (0.0539)	
training:	Epoch: [33][129/233]	Loss 0.0396 (0.0538)	
training:	Epoch: [33][130/233]	Loss 0.0450 (0.0537)	
training:	Epoch: [33][131/233]	Loss 0.0570 (0.0537)	
training:	Epoch: [33][132/233]	Loss 0.0651 (0.0538)	
training:	Epoch: [33][133/233]	Loss 0.0633 (0.0539)	
training:	Epoch: [33][134/233]	Loss 0.0506 (0.0538)	
training:	Epoch: [33][135/233]	Loss 0.0651 (0.0539)	
training:	Epoch: [33][136/233]	Loss 0.0469 (0.0539)	
training:	Epoch: [33][137/233]	Loss 0.0484 (0.0538)	
training:	Epoch: [33][138/233]	Loss 0.0466 (0.0538)	
training:	Epoch: [33][139/233]	Loss 0.0452 (0.0537)	
training:	Epoch: [33][140/233]	Loss 0.0436 (0.0536)	
training:	Epoch: [33][141/233]	Loss 0.0510 (0.0536)	
training:	Epoch: [33][142/233]	Loss 0.0481 (0.0536)	
training:	Epoch: [33][143/233]	Loss 0.0705 (0.0537)	
training:	Epoch: [33][144/233]	Loss 0.0740 (0.0538)	
training:	Epoch: [33][145/233]	Loss 0.0485 (0.0538)	
training:	Epoch: [33][146/233]	Loss 0.0628 (0.0539)	
training:	Epoch: [33][147/233]	Loss 0.0490 (0.0538)	
training:	Epoch: [33][148/233]	Loss 0.0434 (0.0538)	
training:	Epoch: [33][149/233]	Loss 0.0778 (0.0539)	
training:	Epoch: [33][150/233]	Loss 0.0462 (0.0539)	
training:	Epoch: [33][151/233]	Loss 0.0522 (0.0539)	
training:	Epoch: [33][152/233]	Loss 0.0481 (0.0538)	
training:	Epoch: [33][153/233]	Loss 0.0454 (0.0538)	
training:	Epoch: [33][154/233]	Loss 0.0417 (0.0537)	
training:	Epoch: [33][155/233]	Loss 0.0876 (0.0539)	
training:	Epoch: [33][156/233]	Loss 0.0455 (0.0539)	
training:	Epoch: [33][157/233]	Loss 0.0492 (0.0538)	
training:	Epoch: [33][158/233]	Loss 0.0433 (0.0538)	
training:	Epoch: [33][159/233]	Loss 0.0494 (0.0537)	
training:	Epoch: [33][160/233]	Loss 0.0438 (0.0537)	
training:	Epoch: [33][161/233]	Loss 0.0501 (0.0537)	
training:	Epoch: [33][162/233]	Loss 0.0468 (0.0536)	
training:	Epoch: [33][163/233]	Loss 0.0593 (0.0536)	
training:	Epoch: [33][164/233]	Loss 0.0488 (0.0536)	
training:	Epoch: [33][165/233]	Loss 0.0992 (0.0539)	
training:	Epoch: [33][166/233]	Loss 0.0584 (0.0539)	
training:	Epoch: [33][167/233]	Loss 0.0460 (0.0539)	
training:	Epoch: [33][168/233]	Loss 0.0478 (0.0538)	
training:	Epoch: [33][169/233]	Loss 0.0427 (0.0538)	
training:	Epoch: [33][170/233]	Loss 0.0580 (0.0538)	
training:	Epoch: [33][171/233]	Loss 0.0439 (0.0537)	
training:	Epoch: [33][172/233]	Loss 0.0384 (0.0536)	
training:	Epoch: [33][173/233]	Loss 0.0436 (0.0536)	
training:	Epoch: [33][174/233]	Loss 0.0670 (0.0537)	
training:	Epoch: [33][175/233]	Loss 0.0571 (0.0537)	
training:	Epoch: [33][176/233]	Loss 0.0469 (0.0536)	
training:	Epoch: [33][177/233]	Loss 0.0472 (0.0536)	
training:	Epoch: [33][178/233]	Loss 0.0706 (0.0537)	
training:	Epoch: [33][179/233]	Loss 0.0514 (0.0537)	
training:	Epoch: [33][180/233]	Loss 0.0436 (0.0536)	
training:	Epoch: [33][181/233]	Loss 0.0583 (0.0537)	
training:	Epoch: [33][182/233]	Loss 0.0765 (0.0538)	
training:	Epoch: [33][183/233]	Loss 0.0455 (0.0537)	
training:	Epoch: [33][184/233]	Loss 0.0395 (0.0537)	
training:	Epoch: [33][185/233]	Loss 0.0435 (0.0536)	
training:	Epoch: [33][186/233]	Loss 0.0502 (0.0536)	
training:	Epoch: [33][187/233]	Loss 0.0490 (0.0536)	
training:	Epoch: [33][188/233]	Loss 0.0446 (0.0535)	
training:	Epoch: [33][189/233]	Loss 0.0361 (0.0534)	
training:	Epoch: [33][190/233]	Loss 0.0688 (0.0535)	
training:	Epoch: [33][191/233]	Loss 0.0427 (0.0535)	
training:	Epoch: [33][192/233]	Loss 0.0370 (0.0534)	
training:	Epoch: [33][193/233]	Loss 0.0612 (0.0534)	
training:	Epoch: [33][194/233]	Loss 0.0475 (0.0534)	
training:	Epoch: [33][195/233]	Loss 0.0489 (0.0534)	
training:	Epoch: [33][196/233]	Loss 0.0614 (0.0534)	
training:	Epoch: [33][197/233]	Loss 0.0560 (0.0534)	
training:	Epoch: [33][198/233]	Loss 0.0472 (0.0534)	
training:	Epoch: [33][199/233]	Loss 0.0434 (0.0533)	
training:	Epoch: [33][200/233]	Loss 0.0485 (0.0533)	
training:	Epoch: [33][201/233]	Loss 0.0467 (0.0533)	
training:	Epoch: [33][202/233]	Loss 0.0500 (0.0533)	
training:	Epoch: [33][203/233]	Loss 0.0428 (0.0532)	
training:	Epoch: [33][204/233]	Loss 0.0595 (0.0532)	
training:	Epoch: [33][205/233]	Loss 0.0389 (0.0532)	
training:	Epoch: [33][206/233]	Loss 0.0620 (0.0532)	
training:	Epoch: [33][207/233]	Loss 0.0463 (0.0532)	
training:	Epoch: [33][208/233]	Loss 0.0500 (0.0532)	
training:	Epoch: [33][209/233]	Loss 0.0572 (0.0532)	
training:	Epoch: [33][210/233]	Loss 0.0500 (0.0532)	
training:	Epoch: [33][211/233]	Loss 0.0428 (0.0531)	
training:	Epoch: [33][212/233]	Loss 0.0467 (0.0531)	
training:	Epoch: [33][213/233]	Loss 0.0511 (0.0531)	
training:	Epoch: [33][214/233]	Loss 0.0441 (0.0530)	
training:	Epoch: [33][215/233]	Loss 0.0415 (0.0530)	
training:	Epoch: [33][216/233]	Loss 0.0379 (0.0529)	
training:	Epoch: [33][217/233]	Loss 0.0411 (0.0529)	
training:	Epoch: [33][218/233]	Loss 0.0432 (0.0528)	
training:	Epoch: [33][219/233]	Loss 0.0516 (0.0528)	
training:	Epoch: [33][220/233]	Loss 0.0595 (0.0528)	
training:	Epoch: [33][221/233]	Loss 0.0400 (0.0528)	
training:	Epoch: [33][222/233]	Loss 0.0541 (0.0528)	
training:	Epoch: [33][223/233]	Loss 0.0451 (0.0527)	
training:	Epoch: [33][224/233]	Loss 0.0681 (0.0528)	
training:	Epoch: [33][225/233]	Loss 0.0458 (0.0528)	
training:	Epoch: [33][226/233]	Loss 0.0529 (0.0528)	
training:	Epoch: [33][227/233]	Loss 0.0399 (0.0527)	
training:	Epoch: [33][228/233]	Loss 0.0390 (0.0527)	
training:	Epoch: [33][229/233]	Loss 0.0387 (0.0526)	
training:	Epoch: [33][230/233]	Loss 0.0387 (0.0525)	
training:	Epoch: [33][231/233]	Loss 0.1490 (0.0530)	
training:	Epoch: [33][232/233]	Loss 0.0466 (0.0529)	
training:	Epoch: [33][233/233]	Loss 0.0519 (0.0529)	
Training:	 Loss: 0.0528

Training:	 ACC: 1.0000 1.0000 1.0000 1.0000
Validation:	 ACC: 0.8623 0.8625 0.8669 0.8576
Validation:	 Best_BACC: 0.8666 0.8662 0.8588 0.8744
Validation:	 Loss: 0.3177
Pretraining:	Epoch 34/200
----------
training:	Epoch: [34][1/233]	Loss 0.0423 (0.0423)	
training:	Epoch: [34][2/233]	Loss 0.0643 (0.0533)	
training:	Epoch: [34][3/233]	Loss 0.0410 (0.0492)	
training:	Epoch: [34][4/233]	Loss 0.0808 (0.0571)	
training:	Epoch: [34][5/233]	Loss 0.0591 (0.0575)	
training:	Epoch: [34][6/233]	Loss 0.0386 (0.0544)	
training:	Epoch: [34][7/233]	Loss 0.0360 (0.0517)	
training:	Epoch: [34][8/233]	Loss 0.0445 (0.0508)	
training:	Epoch: [34][9/233]	Loss 0.0431 (0.0500)	
training:	Epoch: [34][10/233]	Loss 0.0444 (0.0494)	
training:	Epoch: [34][11/233]	Loss 0.0848 (0.0526)	
training:	Epoch: [34][12/233]	Loss 0.1085 (0.0573)	
training:	Epoch: [34][13/233]	Loss 0.0708 (0.0583)	
training:	Epoch: [34][14/233]	Loss 0.0498 (0.0577)	
training:	Epoch: [34][15/233]	Loss 0.0498 (0.0572)	
training:	Epoch: [34][16/233]	Loss 0.0395 (0.0561)	
training:	Epoch: [34][17/233]	Loss 0.0460 (0.0555)	
training:	Epoch: [34][18/233]	Loss 0.0454 (0.0549)	
training:	Epoch: [34][19/233]	Loss 0.0405 (0.0542)	
training:	Epoch: [34][20/233]	Loss 0.0446 (0.0537)	
training:	Epoch: [34][21/233]	Loss 0.0419 (0.0531)	
training:	Epoch: [34][22/233]	Loss 0.0503 (0.0530)	
training:	Epoch: [34][23/233]	Loss 0.0364 (0.0523)	
training:	Epoch: [34][24/233]	Loss 0.0359 (0.0516)	
training:	Epoch: [34][25/233]	Loss 0.0411 (0.0512)	
training:	Epoch: [34][26/233]	Loss 0.0623 (0.0516)	
training:	Epoch: [34][27/233]	Loss 0.0663 (0.0521)	
training:	Epoch: [34][28/233]	Loss 0.0429 (0.0518)	
training:	Epoch: [34][29/233]	Loss 0.0422 (0.0515)	
training:	Epoch: [34][30/233]	Loss 0.0407 (0.0511)	
training:	Epoch: [34][31/233]	Loss 0.0482 (0.0510)	
training:	Epoch: [34][32/233]	Loss 0.0786 (0.0519)	
training:	Epoch: [34][33/233]	Loss 0.0421 (0.0516)	
training:	Epoch: [34][34/233]	Loss 0.0434 (0.0514)	
training:	Epoch: [34][35/233]	Loss 0.0571 (0.0515)	
training:	Epoch: [34][36/233]	Loss 0.0437 (0.0513)	
training:	Epoch: [34][37/233]	Loss 0.0614 (0.0516)	
training:	Epoch: [34][38/233]	Loss 0.0431 (0.0513)	
training:	Epoch: [34][39/233]	Loss 0.0393 (0.0510)	
training:	Epoch: [34][40/233]	Loss 0.0462 (0.0509)	
training:	Epoch: [34][41/233]	Loss 0.0418 (0.0507)	
training:	Epoch: [34][42/233]	Loss 0.0478 (0.0506)	
training:	Epoch: [34][43/233]	Loss 0.0452 (0.0505)	
training:	Epoch: [34][44/233]	Loss 0.0420 (0.0503)	
training:	Epoch: [34][45/233]	Loss 0.0442 (0.0502)	
training:	Epoch: [34][46/233]	Loss 0.0371 (0.0499)	
training:	Epoch: [34][47/233]	Loss 0.0411 (0.0497)	
training:	Epoch: [34][48/233]	Loss 0.0493 (0.0497)	
training:	Epoch: [34][49/233]	Loss 0.0380 (0.0495)	
training:	Epoch: [34][50/233]	Loss 0.0596 (0.0497)	
training:	Epoch: [34][51/233]	Loss 0.0393 (0.0495)	
training:	Epoch: [34][52/233]	Loss 0.0484 (0.0494)	
training:	Epoch: [34][53/233]	Loss 0.0402 (0.0493)	
training:	Epoch: [34][54/233]	Loss 0.0497 (0.0493)	
training:	Epoch: [34][55/233]	Loss 0.0405 (0.0491)	
training:	Epoch: [34][56/233]	Loss 0.0485 (0.0491)	
training:	Epoch: [34][57/233]	Loss 0.1443 (0.0508)	
training:	Epoch: [34][58/233]	Loss 0.0434 (0.0506)	
training:	Epoch: [34][59/233]	Loss 0.0623 (0.0508)	
training:	Epoch: [34][60/233]	Loss 0.0365 (0.0506)	
training:	Epoch: [34][61/233]	Loss 0.0792 (0.0511)	
training:	Epoch: [34][62/233]	Loss 0.0470 (0.0510)	
training:	Epoch: [34][63/233]	Loss 0.0415 (0.0509)	
training:	Epoch: [34][64/233]	Loss 0.0892 (0.0514)	
training:	Epoch: [34][65/233]	Loss 0.0523 (0.0515)	
training:	Epoch: [34][66/233]	Loss 0.0393 (0.0513)	
training:	Epoch: [34][67/233]	Loss 0.0468 (0.0512)	
training:	Epoch: [34][68/233]	Loss 0.0533 (0.0512)	
training:	Epoch: [34][69/233]	Loss 0.0625 (0.0514)	
training:	Epoch: [34][70/233]	Loss 0.0384 (0.0512)	
training:	Epoch: [34][71/233]	Loss 0.0440 (0.0511)	
training:	Epoch: [34][72/233]	Loss 0.0387 (0.0509)	
training:	Epoch: [34][73/233]	Loss 0.0777 (0.0513)	
training:	Epoch: [34][74/233]	Loss 0.0520 (0.0513)	
training:	Epoch: [34][75/233]	Loss 0.0404 (0.0512)	
training:	Epoch: [34][76/233]	Loss 0.0373 (0.0510)	
training:	Epoch: [34][77/233]	Loss 0.0570 (0.0511)	
training:	Epoch: [34][78/233]	Loss 0.0469 (0.0510)	
training:	Epoch: [34][79/233]	Loss 0.0428 (0.0509)	
training:	Epoch: [34][80/233]	Loss 0.0516 (0.0509)	
training:	Epoch: [34][81/233]	Loss 0.0536 (0.0510)	
training:	Epoch: [34][82/233]	Loss 0.0588 (0.0511)	
training:	Epoch: [34][83/233]	Loss 0.0455 (0.0510)	
training:	Epoch: [34][84/233]	Loss 0.0533 (0.0510)	
training:	Epoch: [34][85/233]	Loss 0.0787 (0.0513)	
training:	Epoch: [34][86/233]	Loss 0.0502 (0.0513)	
training:	Epoch: [34][87/233]	Loss 0.0639 (0.0515)	
training:	Epoch: [34][88/233]	Loss 0.0422 (0.0514)	
training:	Epoch: [34][89/233]	Loss 0.0477 (0.0513)	
training:	Epoch: [34][90/233]	Loss 0.0462 (0.0513)	
training:	Epoch: [34][91/233]	Loss 0.0384 (0.0511)	
training:	Epoch: [34][92/233]	Loss 0.0393 (0.0510)	
training:	Epoch: [34][93/233]	Loss 0.0571 (0.0511)	
training:	Epoch: [34][94/233]	Loss 0.0422 (0.0510)	
training:	Epoch: [34][95/233]	Loss 0.0437 (0.0509)	
training:	Epoch: [34][96/233]	Loss 0.0478 (0.0509)	
training:	Epoch: [34][97/233]	Loss 0.0393 (0.0507)	
training:	Epoch: [34][98/233]	Loss 0.0391 (0.0506)	
training:	Epoch: [34][99/233]	Loss 0.0406 (0.0505)	
training:	Epoch: [34][100/233]	Loss 0.0422 (0.0504)	
training:	Epoch: [34][101/233]	Loss 0.0434 (0.0504)	
training:	Epoch: [34][102/233]	Loss 0.0510 (0.0504)	
training:	Epoch: [34][103/233]	Loss 0.0456 (0.0503)	
training:	Epoch: [34][104/233]	Loss 0.0446 (0.0503)	
training:	Epoch: [34][105/233]	Loss 0.0385 (0.0502)	
training:	Epoch: [34][106/233]	Loss 0.0530 (0.0502)	
training:	Epoch: [34][107/233]	Loss 0.0422 (0.0501)	
training:	Epoch: [34][108/233]	Loss 0.0418 (0.0500)	
training:	Epoch: [34][109/233]	Loss 0.0455 (0.0500)	
training:	Epoch: [34][110/233]	Loss 0.0561 (0.0501)	
training:	Epoch: [34][111/233]	Loss 0.0500 (0.0501)	
training:	Epoch: [34][112/233]	Loss 0.0385 (0.0499)	
training:	Epoch: [34][113/233]	Loss 0.0418 (0.0499)	
training:	Epoch: [34][114/233]	Loss 0.0651 (0.0500)	
training:	Epoch: [34][115/233]	Loss 0.0391 (0.0499)	
training:	Epoch: [34][116/233]	Loss 0.0608 (0.0500)	
training:	Epoch: [34][117/233]	Loss 0.0432 (0.0500)	
training:	Epoch: [34][118/233]	Loss 0.0485 (0.0499)	
training:	Epoch: [34][119/233]	Loss 0.0895 (0.0503)	
training:	Epoch: [34][120/233]	Loss 0.0416 (0.0502)	
training:	Epoch: [34][121/233]	Loss 0.0416 (0.0501)	
training:	Epoch: [34][122/233]	Loss 0.0480 (0.0501)	
training:	Epoch: [34][123/233]	Loss 0.0437 (0.0501)	
training:	Epoch: [34][124/233]	Loss 0.0437 (0.0500)	
training:	Epoch: [34][125/233]	Loss 0.0455 (0.0500)	
training:	Epoch: [34][126/233]	Loss 0.0747 (0.0502)	
training:	Epoch: [34][127/233]	Loss 0.0400 (0.0501)	
training:	Epoch: [34][128/233]	Loss 0.0529 (0.0501)	
training:	Epoch: [34][129/233]	Loss 0.0418 (0.0500)	
training:	Epoch: [34][130/233]	Loss 0.0439 (0.0500)	
training:	Epoch: [34][131/233]	Loss 0.0592 (0.0501)	
training:	Epoch: [34][132/233]	Loss 0.0403 (0.0500)	
training:	Epoch: [34][133/233]	Loss 0.0485 (0.0500)	
training:	Epoch: [34][134/233]	Loss 0.0387 (0.0499)	
training:	Epoch: [34][135/233]	Loss 0.0440 (0.0499)	
training:	Epoch: [34][136/233]	Loss 0.0367 (0.0498)	
training:	Epoch: [34][137/233]	Loss 0.0432 (0.0497)	
training:	Epoch: [34][138/233]	Loss 0.0472 (0.0497)	
training:	Epoch: [34][139/233]	Loss 0.0483 (0.0497)	
training:	Epoch: [34][140/233]	Loss 0.0509 (0.0497)	
training:	Epoch: [34][141/233]	Loss 0.0531 (0.0497)	
training:	Epoch: [34][142/233]	Loss 0.0397 (0.0496)	
training:	Epoch: [34][143/233]	Loss 0.0512 (0.0497)	
training:	Epoch: [34][144/233]	Loss 0.0414 (0.0496)	
training:	Epoch: [34][145/233]	Loss 0.0420 (0.0495)	
training:	Epoch: [34][146/233]	Loss 0.0561 (0.0496)	
training:	Epoch: [34][147/233]	Loss 0.0491 (0.0496)	
training:	Epoch: [34][148/233]	Loss 0.0428 (0.0495)	
training:	Epoch: [34][149/233]	Loss 0.0411 (0.0495)	
training:	Epoch: [34][150/233]	Loss 0.0507 (0.0495)	
training:	Epoch: [34][151/233]	Loss 0.0424 (0.0494)	
training:	Epoch: [34][152/233]	Loss 0.0402 (0.0494)	
training:	Epoch: [34][153/233]	Loss 0.0440 (0.0493)	
training:	Epoch: [34][154/233]	Loss 0.0385 (0.0493)	
training:	Epoch: [34][155/233]	Loss 0.0415 (0.0492)	
training:	Epoch: [34][156/233]	Loss 0.0443 (0.0492)	
training:	Epoch: [34][157/233]	Loss 0.0424 (0.0492)	
training:	Epoch: [34][158/233]	Loss 0.0513 (0.0492)	
training:	Epoch: [34][159/233]	Loss 0.0414 (0.0491)	
training:	Epoch: [34][160/233]	Loss 0.0480 (0.0491)	
training:	Epoch: [34][161/233]	Loss 0.0687 (0.0492)	
training:	Epoch: [34][162/233]	Loss 0.0355 (0.0491)	
training:	Epoch: [34][163/233]	Loss 0.0474 (0.0491)	
training:	Epoch: [34][164/233]	Loss 0.0731 (0.0493)	
training:	Epoch: [34][165/233]	Loss 0.0715 (0.0494)	
training:	Epoch: [34][166/233]	Loss 0.0527 (0.0494)	
training:	Epoch: [34][167/233]	Loss 0.0434 (0.0494)	
training:	Epoch: [34][168/233]	Loss 0.0440 (0.0494)	
training:	Epoch: [34][169/233]	Loss 0.0466 (0.0494)	
training:	Epoch: [34][170/233]	Loss 0.0854 (0.0496)	
training:	Epoch: [34][171/233]	Loss 0.0369 (0.0495)	
training:	Epoch: [34][172/233]	Loss 0.0377 (0.0494)	
training:	Epoch: [34][173/233]	Loss 0.0489 (0.0494)	
training:	Epoch: [34][174/233]	Loss 0.0456 (0.0494)	
training:	Epoch: [34][175/233]	Loss 0.0467 (0.0494)	
training:	Epoch: [34][176/233]	Loss 0.0652 (0.0495)	
training:	Epoch: [34][177/233]	Loss 0.0446 (0.0494)	
training:	Epoch: [34][178/233]	Loss 0.0486 (0.0494)	
training:	Epoch: [34][179/233]	Loss 0.0897 (0.0497)	
training:	Epoch: [34][180/233]	Loss 0.0416 (0.0496)	
training:	Epoch: [34][181/233]	Loss 0.0634 (0.0497)	
training:	Epoch: [34][182/233]	Loss 0.0701 (0.0498)	
training:	Epoch: [34][183/233]	Loss 0.0613 (0.0499)	
training:	Epoch: [34][184/233]	Loss 0.0514 (0.0499)	
training:	Epoch: [34][185/233]	Loss 0.0474 (0.0499)	
training:	Epoch: [34][186/233]	Loss 0.0368 (0.0498)	
training:	Epoch: [34][187/233]	Loss 0.0477 (0.0498)	
training:	Epoch: [34][188/233]	Loss 0.0481 (0.0498)	
training:	Epoch: [34][189/233]	Loss 0.1166 (0.0501)	
training:	Epoch: [34][190/233]	Loss 0.0729 (0.0502)	
training:	Epoch: [34][191/233]	Loss 0.0425 (0.0502)	
training:	Epoch: [34][192/233]	Loss 0.0483 (0.0502)	
training:	Epoch: [34][193/233]	Loss 0.0466 (0.0502)	
training:	Epoch: [34][194/233]	Loss 0.0453 (0.0502)	
training:	Epoch: [34][195/233]	Loss 0.0383 (0.0501)	
training:	Epoch: [34][196/233]	Loss 0.0595 (0.0501)	
training:	Epoch: [34][197/233]	Loss 0.0343 (0.0501)	
training:	Epoch: [34][198/233]	Loss 0.0390 (0.0500)	
training:	Epoch: [34][199/233]	Loss 0.0426 (0.0500)	
training:	Epoch: [34][200/233]	Loss 0.0562 (0.0500)	
training:	Epoch: [34][201/233]	Loss 0.0509 (0.0500)	
training:	Epoch: [34][202/233]	Loss 0.0582 (0.0500)	
training:	Epoch: [34][203/233]	Loss 0.0534 (0.0501)	
training:	Epoch: [34][204/233]	Loss 0.0666 (0.0501)	
training:	Epoch: [34][205/233]	Loss 0.0487 (0.0501)	
training:	Epoch: [34][206/233]	Loss 0.0450 (0.0501)	
training:	Epoch: [34][207/233]	Loss 0.0503 (0.0501)	
training:	Epoch: [34][208/233]	Loss 0.0409 (0.0501)	
training:	Epoch: [34][209/233]	Loss 0.0429 (0.0500)	
training:	Epoch: [34][210/233]	Loss 0.0426 (0.0500)	
training:	Epoch: [34][211/233]	Loss 0.0409 (0.0500)	
training:	Epoch: [34][212/233]	Loss 0.0625 (0.0500)	
training:	Epoch: [34][213/233]	Loss 0.0471 (0.0500)	
training:	Epoch: [34][214/233]	Loss 0.0441 (0.0500)	
training:	Epoch: [34][215/233]	Loss 0.1433 (0.0504)	
training:	Epoch: [34][216/233]	Loss 0.0474 (0.0504)	
training:	Epoch: [34][217/233]	Loss 0.0377 (0.0503)	
training:	Epoch: [34][218/233]	Loss 0.0516 (0.0503)	
training:	Epoch: [34][219/233]	Loss 0.0739 (0.0504)	
training:	Epoch: [34][220/233]	Loss 0.0866 (0.0506)	
training:	Epoch: [34][221/233]	Loss 0.0434 (0.0506)	
training:	Epoch: [34][222/233]	Loss 0.0977 (0.0508)	
training:	Epoch: [34][223/233]	Loss 0.0412 (0.0507)	
training:	Epoch: [34][224/233]	Loss 0.0422 (0.0507)	
training:	Epoch: [34][225/233]	Loss 0.0571 (0.0507)	
training:	Epoch: [34][226/233]	Loss 0.0491 (0.0507)	
training:	Epoch: [34][227/233]	Loss 0.0429 (0.0507)	
training:	Epoch: [34][228/233]	Loss 0.0898 (0.0509)	
training:	Epoch: [34][229/233]	Loss 0.0409 (0.0508)	
training:	Epoch: [34][230/233]	Loss 0.0630 (0.0509)	
training:	Epoch: [34][231/233]	Loss 0.0445 (0.0508)	
training:	Epoch: [34][232/233]	Loss 0.0590 (0.0509)	
training:	Epoch: [34][233/233]	Loss 0.0451 (0.0509)	
Training:	 Loss: 0.0507

Training:	 ACC: 1.0000 1.0000 1.0000 1.0000
Validation:	 ACC: 0.8671 0.8662 0.8485 0.8857
Validation:	 Best_BACC: 0.8671 0.8662 0.8485 0.8857
Validation:	 Loss: 0.3205
Pretraining:	Epoch 35/200
----------
training:	Epoch: [35][1/233]	Loss 0.0384 (0.0384)	
training:	Epoch: [35][2/233]	Loss 0.0438 (0.0411)	
training:	Epoch: [35][3/233]	Loss 0.0421 (0.0414)	
training:	Epoch: [35][4/233]	Loss 0.0407 (0.0413)	
training:	Epoch: [35][5/233]	Loss 0.0809 (0.0492)	
training:	Epoch: [35][6/233]	Loss 0.0401 (0.0477)	
training:	Epoch: [35][7/233]	Loss 0.0380 (0.0463)	
training:	Epoch: [35][8/233]	Loss 0.0392 (0.0454)	
training:	Epoch: [35][9/233]	Loss 0.0600 (0.0470)	
training:	Epoch: [35][10/233]	Loss 0.0616 (0.0485)	
training:	Epoch: [35][11/233]	Loss 0.0398 (0.0477)	
training:	Epoch: [35][12/233]	Loss 0.0400 (0.0471)	
training:	Epoch: [35][13/233]	Loss 0.0447 (0.0469)	
training:	Epoch: [35][14/233]	Loss 0.0406 (0.0464)	
training:	Epoch: [35][15/233]	Loss 0.0390 (0.0459)	
training:	Epoch: [35][16/233]	Loss 0.0455 (0.0459)	
training:	Epoch: [35][17/233]	Loss 0.0500 (0.0461)	
training:	Epoch: [35][18/233]	Loss 0.0503 (0.0464)	
training:	Epoch: [35][19/233]	Loss 0.0443 (0.0463)	
training:	Epoch: [35][20/233]	Loss 0.0479 (0.0463)	
training:	Epoch: [35][21/233]	Loss 0.0432 (0.0462)	
training:	Epoch: [35][22/233]	Loss 0.0400 (0.0459)	
training:	Epoch: [35][23/233]	Loss 0.0756 (0.0472)	
training:	Epoch: [35][24/233]	Loss 0.0478 (0.0472)	
training:	Epoch: [35][25/233]	Loss 0.0540 (0.0475)	
training:	Epoch: [35][26/233]	Loss 0.0353 (0.0470)	
training:	Epoch: [35][27/233]	Loss 0.0442 (0.0469)	
training:	Epoch: [35][28/233]	Loss 0.0397 (0.0467)	
training:	Epoch: [35][29/233]	Loss 0.0391 (0.0464)	
training:	Epoch: [35][30/233]	Loss 0.0460 (0.0464)	
training:	Epoch: [35][31/233]	Loss 0.0524 (0.0466)	
training:	Epoch: [35][32/233]	Loss 0.0431 (0.0465)	
training:	Epoch: [35][33/233]	Loss 0.0403 (0.0463)	
training:	Epoch: [35][34/233]	Loss 0.0465 (0.0463)	
training:	Epoch: [35][35/233]	Loss 0.0724 (0.0470)	
training:	Epoch: [35][36/233]	Loss 0.0523 (0.0472)	
training:	Epoch: [35][37/233]	Loss 0.0401 (0.0470)	
training:	Epoch: [35][38/233]	Loss 0.0482 (0.0470)	
training:	Epoch: [35][39/233]	Loss 0.0417 (0.0469)	
training:	Epoch: [35][40/233]	Loss 0.0452 (0.0469)	
training:	Epoch: [35][41/233]	Loss 0.0404 (0.0467)	
training:	Epoch: [35][42/233]	Loss 0.0456 (0.0467)	
training:	Epoch: [35][43/233]	Loss 0.0383 (0.0465)	
training:	Epoch: [35][44/233]	Loss 0.0341 (0.0462)	
training:	Epoch: [35][45/233]	Loss 0.0495 (0.0463)	
training:	Epoch: [35][46/233]	Loss 0.0353 (0.0460)	
training:	Epoch: [35][47/233]	Loss 0.0617 (0.0464)	
training:	Epoch: [35][48/233]	Loss 0.0402 (0.0462)	
training:	Epoch: [35][49/233]	Loss 0.0482 (0.0463)	
training:	Epoch: [35][50/233]	Loss 0.0440 (0.0462)	
training:	Epoch: [35][51/233]	Loss 0.0430 (0.0462)	
training:	Epoch: [35][52/233]	Loss 0.0522 (0.0463)	
training:	Epoch: [35][53/233]	Loss 0.0428 (0.0462)	
training:	Epoch: [35][54/233]	Loss 0.0784 (0.0468)	
training:	Epoch: [35][55/233]	Loss 0.0403 (0.0467)	
training:	Epoch: [35][56/233]	Loss 0.0393 (0.0466)	
training:	Epoch: [35][57/233]	Loss 0.0474 (0.0466)	
training:	Epoch: [35][58/233]	Loss 0.0527 (0.0467)	
training:	Epoch: [35][59/233]	Loss 0.0683 (0.0471)	
training:	Epoch: [35][60/233]	Loss 0.0529 (0.0471)	
training:	Epoch: [35][61/233]	Loss 0.0385 (0.0470)	
training:	Epoch: [35][62/233]	Loss 0.0367 (0.0468)	
training:	Epoch: [35][63/233]	Loss 0.0398 (0.0467)	
training:	Epoch: [35][64/233]	Loss 0.0365 (0.0466)	
training:	Epoch: [35][65/233]	Loss 0.0488 (0.0466)	
training:	Epoch: [35][66/233]	Loss 0.0381 (0.0465)	
training:	Epoch: [35][67/233]	Loss 0.0355 (0.0463)	
training:	Epoch: [35][68/233]	Loss 0.0873 (0.0469)	
training:	Epoch: [35][69/233]	Loss 0.0350 (0.0467)	
training:	Epoch: [35][70/233]	Loss 0.0451 (0.0467)	
training:	Epoch: [35][71/233]	Loss 0.0345 (0.0465)	
training:	Epoch: [35][72/233]	Loss 0.0667 (0.0468)	
training:	Epoch: [35][73/233]	Loss 0.0429 (0.0468)	
training:	Epoch: [35][74/233]	Loss 0.0348 (0.0466)	
training:	Epoch: [35][75/233]	Loss 0.0388 (0.0465)	
training:	Epoch: [35][76/233]	Loss 0.0346 (0.0463)	
training:	Epoch: [35][77/233]	Loss 0.0371 (0.0462)	
training:	Epoch: [35][78/233]	Loss 0.0376 (0.0461)	
training:	Epoch: [35][79/233]	Loss 0.0442 (0.0461)	
training:	Epoch: [35][80/233]	Loss 0.0336 (0.0459)	
training:	Epoch: [35][81/233]	Loss 0.0568 (0.0461)	
training:	Epoch: [35][82/233]	Loss 0.0478 (0.0461)	
training:	Epoch: [35][83/233]	Loss 0.0491 (0.0461)	
training:	Epoch: [35][84/233]	Loss 0.0532 (0.0462)	
training:	Epoch: [35][85/233]	Loss 0.0392 (0.0461)	
training:	Epoch: [35][86/233]	Loss 0.0387 (0.0460)	
training:	Epoch: [35][87/233]	Loss 0.0438 (0.0460)	
training:	Epoch: [35][88/233]	Loss 0.0727 (0.0463)	
training:	Epoch: [35][89/233]	Loss 0.0432 (0.0463)	
training:	Epoch: [35][90/233]	Loss 0.0385 (0.0462)	
training:	Epoch: [35][91/233]	Loss 0.0419 (0.0462)	
training:	Epoch: [35][92/233]	Loss 0.0394 (0.0461)	
training:	Epoch: [35][93/233]	Loss 0.0321 (0.0459)	
training:	Epoch: [35][94/233]	Loss 0.0404 (0.0459)	
training:	Epoch: [35][95/233]	Loss 0.0492 (0.0459)	
training:	Epoch: [35][96/233]	Loss 0.0376 (0.0458)	
training:	Epoch: [35][97/233]	Loss 0.0386 (0.0457)	
training:	Epoch: [35][98/233]	Loss 0.0506 (0.0458)	
training:	Epoch: [35][99/233]	Loss 0.0416 (0.0458)	
training:	Epoch: [35][100/233]	Loss 0.0381 (0.0457)	
training:	Epoch: [35][101/233]	Loss 0.0665 (0.0459)	
training:	Epoch: [35][102/233]	Loss 0.0393 (0.0458)	
training:	Epoch: [35][103/233]	Loss 0.0491 (0.0458)	
training:	Epoch: [35][104/233]	Loss 0.0435 (0.0458)	
training:	Epoch: [35][105/233]	Loss 0.0406 (0.0458)	
training:	Epoch: [35][106/233]	Loss 0.0394 (0.0457)	
training:	Epoch: [35][107/233]	Loss 0.0406 (0.0457)	
training:	Epoch: [35][108/233]	Loss 0.0341 (0.0456)	
training:	Epoch: [35][109/233]	Loss 0.0356 (0.0455)	
training:	Epoch: [35][110/233]	Loss 0.0577 (0.0456)	
training:	Epoch: [35][111/233]	Loss 0.1217 (0.0463)	
training:	Epoch: [35][112/233]	Loss 0.0343 (0.0462)	
training:	Epoch: [35][113/233]	Loss 0.0675 (0.0463)	
training:	Epoch: [35][114/233]	Loss 0.0409 (0.0463)	
training:	Epoch: [35][115/233]	Loss 0.0482 (0.0463)	
training:	Epoch: [35][116/233]	Loss 0.0595 (0.0464)	
training:	Epoch: [35][117/233]	Loss 0.0363 (0.0463)	
training:	Epoch: [35][118/233]	Loss 0.0605 (0.0465)	
training:	Epoch: [35][119/233]	Loss 0.0385 (0.0464)	
training:	Epoch: [35][120/233]	Loss 0.0424 (0.0464)	
training:	Epoch: [35][121/233]	Loss 0.0413 (0.0463)	
training:	Epoch: [35][122/233]	Loss 0.0389 (0.0463)	
training:	Epoch: [35][123/233]	Loss 0.0807 (0.0465)	
training:	Epoch: [35][124/233]	Loss 0.0676 (0.0467)	
training:	Epoch: [35][125/233]	Loss 0.0637 (0.0468)	
training:	Epoch: [35][126/233]	Loss 0.0473 (0.0468)	
training:	Epoch: [35][127/233]	Loss 0.0502 (0.0469)	
training:	Epoch: [35][128/233]	Loss 0.0344 (0.0468)	
training:	Epoch: [35][129/233]	Loss 0.0851 (0.0471)	
training:	Epoch: [35][130/233]	Loss 0.0387 (0.0470)	
training:	Epoch: [35][131/233]	Loss 0.1132 (0.0475)	
training:	Epoch: [35][132/233]	Loss 0.0597 (0.0476)	
training:	Epoch: [35][133/233]	Loss 0.0524 (0.0476)	
training:	Epoch: [35][134/233]	Loss 0.0451 (0.0476)	
training:	Epoch: [35][135/233]	Loss 0.0638 (0.0477)	
training:	Epoch: [35][136/233]	Loss 0.0415 (0.0477)	
training:	Epoch: [35][137/233]	Loss 0.0479 (0.0477)	
training:	Epoch: [35][138/233]	Loss 0.0406 (0.0476)	
training:	Epoch: [35][139/233]	Loss 0.0445 (0.0476)	
training:	Epoch: [35][140/233]	Loss 0.0418 (0.0476)	
training:	Epoch: [35][141/233]	Loss 0.0483 (0.0476)	
training:	Epoch: [35][142/233]	Loss 0.0466 (0.0476)	
training:	Epoch: [35][143/233]	Loss 0.0367 (0.0475)	
training:	Epoch: [35][144/233]	Loss 0.0434 (0.0475)	
training:	Epoch: [35][145/233]	Loss 0.0461 (0.0475)	
training:	Epoch: [35][146/233]	Loss 0.0394 (0.0474)	
training:	Epoch: [35][147/233]	Loss 0.0406 (0.0474)	
training:	Epoch: [35][148/233]	Loss 0.0406 (0.0473)	
training:	Epoch: [35][149/233]	Loss 0.0465 (0.0473)	
training:	Epoch: [35][150/233]	Loss 0.0393 (0.0473)	
training:	Epoch: [35][151/233]	Loss 0.0417 (0.0472)	
training:	Epoch: [35][152/233]	Loss 0.0378 (0.0472)	
training:	Epoch: [35][153/233]	Loss 0.0550 (0.0472)	
training:	Epoch: [35][154/233]	Loss 0.0374 (0.0471)	
training:	Epoch: [35][155/233]	Loss 0.0586 (0.0472)	
training:	Epoch: [35][156/233]	Loss 0.0479 (0.0472)	
training:	Epoch: [35][157/233]	Loss 0.0384 (0.0472)	
training:	Epoch: [35][158/233]	Loss 0.0416 (0.0471)	
training:	Epoch: [35][159/233]	Loss 0.0428 (0.0471)	
training:	Epoch: [35][160/233]	Loss 0.0501 (0.0471)	
training:	Epoch: [35][161/233]	Loss 0.0425 (0.0471)	
training:	Epoch: [35][162/233]	Loss 0.0496 (0.0471)	
training:	Epoch: [35][163/233]	Loss 0.0424 (0.0471)	
training:	Epoch: [35][164/233]	Loss 0.0453 (0.0471)	
training:	Epoch: [35][165/233]	Loss 0.0421 (0.0470)	
training:	Epoch: [35][166/233]	Loss 0.0455 (0.0470)	
training:	Epoch: [35][167/233]	Loss 0.0410 (0.0470)	
training:	Epoch: [35][168/233]	Loss 0.0457 (0.0470)	
training:	Epoch: [35][169/233]	Loss 0.0358 (0.0469)	
training:	Epoch: [35][170/233]	Loss 0.0382 (0.0469)	
training:	Epoch: [35][171/233]	Loss 0.0456 (0.0469)	
training:	Epoch: [35][172/233]	Loss 0.0574 (0.0469)	
training:	Epoch: [35][173/233]	Loss 0.0483 (0.0469)	
training:	Epoch: [35][174/233]	Loss 0.0412 (0.0469)	
training:	Epoch: [35][175/233]	Loss 0.0485 (0.0469)	
training:	Epoch: [35][176/233]	Loss 0.0404 (0.0469)	
training:	Epoch: [35][177/233]	Loss 0.0394 (0.0468)	
training:	Epoch: [35][178/233]	Loss 0.0382 (0.0468)	
training:	Epoch: [35][179/233]	Loss 0.0408 (0.0468)	
training:	Epoch: [35][180/233]	Loss 0.0426 (0.0467)	
training:	Epoch: [35][181/233]	Loss 0.0427 (0.0467)	
training:	Epoch: [35][182/233]	Loss 0.0397 (0.0467)	
training:	Epoch: [35][183/233]	Loss 0.0400 (0.0466)	
training:	Epoch: [35][184/233]	Loss 0.0450 (0.0466)	
training:	Epoch: [35][185/233]	Loss 0.1252 (0.0470)	
training:	Epoch: [35][186/233]	Loss 0.0432 (0.0470)	
training:	Epoch: [35][187/233]	Loss 0.0436 (0.0470)	
training:	Epoch: [35][188/233]	Loss 0.0737 (0.0472)	
training:	Epoch: [35][189/233]	Loss 0.0433 (0.0471)	
training:	Epoch: [35][190/233]	Loss 0.0476 (0.0471)	
training:	Epoch: [35][191/233]	Loss 0.0429 (0.0471)	
training:	Epoch: [35][192/233]	Loss 0.0399 (0.0471)	
training:	Epoch: [35][193/233]	Loss 0.0506 (0.0471)	
training:	Epoch: [35][194/233]	Loss 0.0470 (0.0471)	
training:	Epoch: [35][195/233]	Loss 0.0511 (0.0471)	
training:	Epoch: [35][196/233]	Loss 0.0372 (0.0471)	
training:	Epoch: [35][197/233]	Loss 0.0374 (0.0470)	
training:	Epoch: [35][198/233]	Loss 0.0418 (0.0470)	
training:	Epoch: [35][199/233]	Loss 0.0400 (0.0469)	
training:	Epoch: [35][200/233]	Loss 0.0378 (0.0469)	
training:	Epoch: [35][201/233]	Loss 0.0441 (0.0469)	
training:	Epoch: [35][202/233]	Loss 0.0732 (0.0470)	
training:	Epoch: [35][203/233]	Loss 0.0366 (0.0470)	
training:	Epoch: [35][204/233]	Loss 0.0396 (0.0469)	
training:	Epoch: [35][205/233]	Loss 0.0490 (0.0469)	
training:	Epoch: [35][206/233]	Loss 0.0429 (0.0469)	
training:	Epoch: [35][207/233]	Loss 0.0362 (0.0469)	
training:	Epoch: [35][208/233]	Loss 0.0503 (0.0469)	
training:	Epoch: [35][209/233]	Loss 0.0549 (0.0469)	
training:	Epoch: [35][210/233]	Loss 0.1131 (0.0472)	
training:	Epoch: [35][211/233]	Loss 0.0860 (0.0474)	
training:	Epoch: [35][212/233]	Loss 0.0504 (0.0474)	
training:	Epoch: [35][213/233]	Loss 0.0395 (0.0474)	
training:	Epoch: [35][214/233]	Loss 0.0443 (0.0474)	
training:	Epoch: [35][215/233]	Loss 0.0393 (0.0473)	
training:	Epoch: [35][216/233]	Loss 0.0478 (0.0474)	
training:	Epoch: [35][217/233]	Loss 0.0337 (0.0473)	
training:	Epoch: [35][218/233]	Loss 0.0446 (0.0473)	
training:	Epoch: [35][219/233]	Loss 0.0402 (0.0472)	
training:	Epoch: [35][220/233]	Loss 0.0579 (0.0473)	
training:	Epoch: [35][221/233]	Loss 0.0353 (0.0472)	
training:	Epoch: [35][222/233]	Loss 0.0418 (0.0472)	
training:	Epoch: [35][223/233]	Loss 0.0397 (0.0472)	
training:	Epoch: [35][224/233]	Loss 0.0439 (0.0472)	
training:	Epoch: [35][225/233]	Loss 0.0451 (0.0472)	
training:	Epoch: [35][226/233]	Loss 0.0357 (0.0471)	
training:	Epoch: [35][227/233]	Loss 0.0693 (0.0472)	
training:	Epoch: [35][228/233]	Loss 0.0411 (0.0472)	
training:	Epoch: [35][229/233]	Loss 0.0386 (0.0471)	
training:	Epoch: [35][230/233]	Loss 0.0834 (0.0473)	
training:	Epoch: [35][231/233]	Loss 0.0427 (0.0473)	
training:	Epoch: [35][232/233]	Loss 0.0386 (0.0472)	
training:	Epoch: [35][233/233]	Loss 0.0669 (0.0473)	
Training:	 Loss: 0.0472

Training:	 ACC: 1.0000 1.0000 1.0000 1.0000
Validation:	 ACC: 0.8596 0.8604 0.8772 0.8419
Validation:	 Best_BACC: 0.8671 0.8662 0.8485 0.8857
Validation:	 Loss: 0.3162
Pretraining:	Epoch 36/200
----------
training:	Epoch: [36][1/233]	Loss 0.0365 (0.0365)	
training:	Epoch: [36][2/233]	Loss 0.0457 (0.0411)	
training:	Epoch: [36][3/233]	Loss 0.0468 (0.0430)	
training:	Epoch: [36][4/233]	Loss 0.0395 (0.0421)	
training:	Epoch: [36][5/233]	Loss 0.0388 (0.0415)	
training:	Epoch: [36][6/233]	Loss 0.0468 (0.0424)	
training:	Epoch: [36][7/233]	Loss 0.0373 (0.0416)	
training:	Epoch: [36][8/233]	Loss 0.0401 (0.0414)	
training:	Epoch: [36][9/233]	Loss 0.0589 (0.0434)	
training:	Epoch: [36][10/233]	Loss 0.0485 (0.0439)	
training:	Epoch: [36][11/233]	Loss 0.0413 (0.0437)	
training:	Epoch: [36][12/233]	Loss 0.0385 (0.0432)	
training:	Epoch: [36][13/233]	Loss 0.0349 (0.0426)	
training:	Epoch: [36][14/233]	Loss 0.0420 (0.0425)	
training:	Epoch: [36][15/233]	Loss 0.0484 (0.0429)	
training:	Epoch: [36][16/233]	Loss 0.0382 (0.0426)	
training:	Epoch: [36][17/233]	Loss 0.0357 (0.0422)	
training:	Epoch: [36][18/233]	Loss 0.0684 (0.0437)	
training:	Epoch: [36][19/233]	Loss 0.0416 (0.0436)	
training:	Epoch: [36][20/233]	Loss 0.1173 (0.0473)	
training:	Epoch: [36][21/233]	Loss 0.0366 (0.0468)	
training:	Epoch: [36][22/233]	Loss 0.0377 (0.0464)	
training:	Epoch: [36][23/233]	Loss 0.0424 (0.0462)	
training:	Epoch: [36][24/233]	Loss 0.0353 (0.0457)	
training:	Epoch: [36][25/233]	Loss 0.0433 (0.0456)	
training:	Epoch: [36][26/233]	Loss 0.0473 (0.0457)	
training:	Epoch: [36][27/233]	Loss 0.0411 (0.0455)	
training:	Epoch: [36][28/233]	Loss 0.0451 (0.0455)	
training:	Epoch: [36][29/233]	Loss 0.0369 (0.0452)	
training:	Epoch: [36][30/233]	Loss 0.0358 (0.0449)	
training:	Epoch: [36][31/233]	Loss 0.0426 (0.0448)	
training:	Epoch: [36][32/233]	Loss 0.0405 (0.0447)	
training:	Epoch: [36][33/233]	Loss 0.0348 (0.0444)	
training:	Epoch: [36][34/233]	Loss 0.0360 (0.0441)	
training:	Epoch: [36][35/233]	Loss 0.0364 (0.0439)	
training:	Epoch: [36][36/233]	Loss 0.0390 (0.0438)	
training:	Epoch: [36][37/233]	Loss 0.0413 (0.0437)	
training:	Epoch: [36][38/233]	Loss 0.0416 (0.0437)	
training:	Epoch: [36][39/233]	Loss 0.0366 (0.0435)	
training:	Epoch: [36][40/233]	Loss 0.0539 (0.0437)	
training:	Epoch: [36][41/233]	Loss 0.0384 (0.0436)	
training:	Epoch: [36][42/233]	Loss 0.0384 (0.0435)	
training:	Epoch: [36][43/233]	Loss 0.0428 (0.0435)	
training:	Epoch: [36][44/233]	Loss 0.0305 (0.0432)	
training:	Epoch: [36][45/233]	Loss 0.0317 (0.0429)	
training:	Epoch: [36][46/233]	Loss 0.0339 (0.0427)	
training:	Epoch: [36][47/233]	Loss 0.0462 (0.0428)	
training:	Epoch: [36][48/233]	Loss 0.0425 (0.0428)	
training:	Epoch: [36][49/233]	Loss 0.0384 (0.0427)	
training:	Epoch: [36][50/233]	Loss 0.0319 (0.0425)	
training:	Epoch: [36][51/233]	Loss 0.0352 (0.0423)	
training:	Epoch: [36][52/233]	Loss 0.0459 (0.0424)	
training:	Epoch: [36][53/233]	Loss 0.0423 (0.0424)	
training:	Epoch: [36][54/233]	Loss 0.0399 (0.0424)	
training:	Epoch: [36][55/233]	Loss 0.0436 (0.0424)	
training:	Epoch: [36][56/233]	Loss 0.0373 (0.0423)	
training:	Epoch: [36][57/233]	Loss 0.0368 (0.0422)	
training:	Epoch: [36][58/233]	Loss 0.0420 (0.0422)	
training:	Epoch: [36][59/233]	Loss 0.0925 (0.0430)	
training:	Epoch: [36][60/233]	Loss 0.0363 (0.0429)	
training:	Epoch: [36][61/233]	Loss 0.0398 (0.0429)	
training:	Epoch: [36][62/233]	Loss 0.0364 (0.0428)	
training:	Epoch: [36][63/233]	Loss 0.0386 (0.0427)	
training:	Epoch: [36][64/233]	Loss 0.0432 (0.0427)	
training:	Epoch: [36][65/233]	Loss 0.0345 (0.0426)	
training:	Epoch: [36][66/233]	Loss 0.0410 (0.0426)	
training:	Epoch: [36][67/233]	Loss 0.0409 (0.0425)	
training:	Epoch: [36][68/233]	Loss 0.0448 (0.0426)	
training:	Epoch: [36][69/233]	Loss 0.0360 (0.0425)	
training:	Epoch: [36][70/233]	Loss 0.0398 (0.0424)	
training:	Epoch: [36][71/233]	Loss 0.0362 (0.0424)	
training:	Epoch: [36][72/233]	Loss 0.0448 (0.0424)	
training:	Epoch: [36][73/233]	Loss 0.0311 (0.0422)	
training:	Epoch: [36][74/233]	Loss 0.0384 (0.0422)	
training:	Epoch: [36][75/233]	Loss 0.0437 (0.0422)	
training:	Epoch: [36][76/233]	Loss 0.0382 (0.0422)	
training:	Epoch: [36][77/233]	Loss 0.0935 (0.0428)	
training:	Epoch: [36][78/233]	Loss 0.0345 (0.0427)	
training:	Epoch: [36][79/233]	Loss 0.0325 (0.0426)	
training:	Epoch: [36][80/233]	Loss 0.0460 (0.0426)	
training:	Epoch: [36][81/233]	Loss 0.0333 (0.0425)	
training:	Epoch: [36][82/233]	Loss 0.0459 (0.0426)	
training:	Epoch: [36][83/233]	Loss 0.0463 (0.0426)	
training:	Epoch: [36][84/233]	Loss 0.0457 (0.0426)	
training:	Epoch: [36][85/233]	Loss 0.0487 (0.0427)	
training:	Epoch: [36][86/233]	Loss 0.0395 (0.0427)	
training:	Epoch: [36][87/233]	Loss 0.0365 (0.0426)	
training:	Epoch: [36][88/233]	Loss 0.0460 (0.0426)	
training:	Epoch: [36][89/233]	Loss 0.0430 (0.0426)	
training:	Epoch: [36][90/233]	Loss 0.0399 (0.0426)	
training:	Epoch: [36][91/233]	Loss 0.0513 (0.0427)	
training:	Epoch: [36][92/233]	Loss 0.0433 (0.0427)	
training:	Epoch: [36][93/233]	Loss 0.0470 (0.0428)	
training:	Epoch: [36][94/233]	Loss 0.0470 (0.0428)	
training:	Epoch: [36][95/233]	Loss 0.0491 (0.0429)	
training:	Epoch: [36][96/233]	Loss 0.0416 (0.0429)	
training:	Epoch: [36][97/233]	Loss 0.0375 (0.0428)	
training:	Epoch: [36][98/233]	Loss 0.0769 (0.0432)	
training:	Epoch: [36][99/233]	Loss 0.0393 (0.0431)	
training:	Epoch: [36][100/233]	Loss 0.0405 (0.0431)	
training:	Epoch: [36][101/233]	Loss 0.0423 (0.0431)	
training:	Epoch: [36][102/233]	Loss 0.0374 (0.0430)	
training:	Epoch: [36][103/233]	Loss 0.0410 (0.0430)	
training:	Epoch: [36][104/233]	Loss 0.0450 (0.0430)	
training:	Epoch: [36][105/233]	Loss 0.0466 (0.0431)	
training:	Epoch: [36][106/233]	Loss 0.0576 (0.0432)	
training:	Epoch: [36][107/233]	Loss 0.0434 (0.0432)	
training:	Epoch: [36][108/233]	Loss 0.0405 (0.0432)	
training:	Epoch: [36][109/233]	Loss 0.0364 (0.0431)	
training:	Epoch: [36][110/233]	Loss 0.0351 (0.0430)	
training:	Epoch: [36][111/233]	Loss 0.0369 (0.0430)	
training:	Epoch: [36][112/233]	Loss 0.0424 (0.0430)	
training:	Epoch: [36][113/233]	Loss 0.0445 (0.0430)	
training:	Epoch: [36][114/233]	Loss 0.0410 (0.0430)	
training:	Epoch: [36][115/233]	Loss 0.0383 (0.0429)	
training:	Epoch: [36][116/233]	Loss 0.0441 (0.0429)	
training:	Epoch: [36][117/233]	Loss 0.0412 (0.0429)	
training:	Epoch: [36][118/233]	Loss 0.0414 (0.0429)	
training:	Epoch: [36][119/233]	Loss 0.0434 (0.0429)	
training:	Epoch: [36][120/233]	Loss 0.0360 (0.0429)	
training:	Epoch: [36][121/233]	Loss 0.0492 (0.0429)	
training:	Epoch: [36][122/233]	Loss 0.0515 (0.0430)	
training:	Epoch: [36][123/233]	Loss 0.0371 (0.0429)	
training:	Epoch: [36][124/233]	Loss 0.0412 (0.0429)	
training:	Epoch: [36][125/233]	Loss 0.0345 (0.0429)	
training:	Epoch: [36][126/233]	Loss 0.0468 (0.0429)	
training:	Epoch: [36][127/233]	Loss 0.0660 (0.0431)	
training:	Epoch: [36][128/233]	Loss 0.0537 (0.0431)	
training:	Epoch: [36][129/233]	Loss 0.0451 (0.0432)	
training:	Epoch: [36][130/233]	Loss 0.0413 (0.0432)	
training:	Epoch: [36][131/233]	Loss 0.0539 (0.0432)	
training:	Epoch: [36][132/233]	Loss 0.0384 (0.0432)	
training:	Epoch: [36][133/233]	Loss 0.0462 (0.0432)	
training:	Epoch: [36][134/233]	Loss 0.0443 (0.0432)	
training:	Epoch: [36][135/233]	Loss 0.0835 (0.0435)	
training:	Epoch: [36][136/233]	Loss 0.0407 (0.0435)	
training:	Epoch: [36][137/233]	Loss 0.0474 (0.0435)	
training:	Epoch: [36][138/233]	Loss 0.0510 (0.0436)	
training:	Epoch: [36][139/233]	Loss 0.0408 (0.0436)	
training:	Epoch: [36][140/233]	Loss 0.0355 (0.0435)	
training:	Epoch: [36][141/233]	Loss 0.0388 (0.0435)	
training:	Epoch: [36][142/233]	Loss 0.0347 (0.0434)	
training:	Epoch: [36][143/233]	Loss 0.0387 (0.0434)	
training:	Epoch: [36][144/233]	Loss 0.0429 (0.0434)	
training:	Epoch: [36][145/233]	Loss 0.0612 (0.0435)	
training:	Epoch: [36][146/233]	Loss 0.0745 (0.0437)	
training:	Epoch: [36][147/233]	Loss 0.0378 (0.0437)	
training:	Epoch: [36][148/233]	Loss 0.0664 (0.0438)	
training:	Epoch: [36][149/233]	Loss 0.0349 (0.0438)	
training:	Epoch: [36][150/233]	Loss 0.0503 (0.0438)	
training:	Epoch: [36][151/233]	Loss 0.0380 (0.0438)	
training:	Epoch: [36][152/233]	Loss 0.0499 (0.0438)	
training:	Epoch: [36][153/233]	Loss 0.0390 (0.0438)	
training:	Epoch: [36][154/233]	Loss 0.0408 (0.0438)	
training:	Epoch: [36][155/233]	Loss 0.0353 (0.0437)	
training:	Epoch: [36][156/233]	Loss 0.0507 (0.0438)	
training:	Epoch: [36][157/233]	Loss 0.0413 (0.0437)	
training:	Epoch: [36][158/233]	Loss 0.0378 (0.0437)	
training:	Epoch: [36][159/233]	Loss 0.0470 (0.0437)	
training:	Epoch: [36][160/233]	Loss 0.0422 (0.0437)	
training:	Epoch: [36][161/233]	Loss 0.0355 (0.0437)	
training:	Epoch: [36][162/233]	Loss 0.0410 (0.0436)	
training:	Epoch: [36][163/233]	Loss 0.0545 (0.0437)	
training:	Epoch: [36][164/233]	Loss 0.0538 (0.0438)	
training:	Epoch: [36][165/233]	Loss 0.0428 (0.0438)	
training:	Epoch: [36][166/233]	Loss 0.0375 (0.0437)	
training:	Epoch: [36][167/233]	Loss 0.0352 (0.0437)	
training:	Epoch: [36][168/233]	Loss 0.0470 (0.0437)	
training:	Epoch: [36][169/233]	Loss 0.0648 (0.0438)	
training:	Epoch: [36][170/233]	Loss 0.0346 (0.0438)	
training:	Epoch: [36][171/233]	Loss 0.0622 (0.0439)	
training:	Epoch: [36][172/233]	Loss 0.0377 (0.0438)	
training:	Epoch: [36][173/233]	Loss 0.0899 (0.0441)	
training:	Epoch: [36][174/233]	Loss 0.0353 (0.0441)	
training:	Epoch: [36][175/233]	Loss 0.0432 (0.0440)	
training:	Epoch: [36][176/233]	Loss 0.0473 (0.0441)	
training:	Epoch: [36][177/233]	Loss 0.0365 (0.0440)	
training:	Epoch: [36][178/233]	Loss 0.0508 (0.0441)	
training:	Epoch: [36][179/233]	Loss 0.0322 (0.0440)	
training:	Epoch: [36][180/233]	Loss 0.0363 (0.0440)	
training:	Epoch: [36][181/233]	Loss 0.0462 (0.0440)	
training:	Epoch: [36][182/233]	Loss 0.0496 (0.0440)	
training:	Epoch: [36][183/233]	Loss 0.0379 (0.0440)	
training:	Epoch: [36][184/233]	Loss 0.0483 (0.0440)	
training:	Epoch: [36][185/233]	Loss 0.0343 (0.0439)	
training:	Epoch: [36][186/233]	Loss 0.0390 (0.0439)	
training:	Epoch: [36][187/233]	Loss 0.0405 (0.0439)	
training:	Epoch: [36][188/233]	Loss 0.0516 (0.0439)	
training:	Epoch: [36][189/233]	Loss 0.0559 (0.0440)	
training:	Epoch: [36][190/233]	Loss 0.0395 (0.0440)	
training:	Epoch: [36][191/233]	Loss 0.0307 (0.0439)	
training:	Epoch: [36][192/233]	Loss 0.0389 (0.0439)	
training:	Epoch: [36][193/233]	Loss 0.0389 (0.0438)	
training:	Epoch: [36][194/233]	Loss 0.0329 (0.0438)	
training:	Epoch: [36][195/233]	Loss 0.0419 (0.0438)	
training:	Epoch: [36][196/233]	Loss 0.0435 (0.0438)	
training:	Epoch: [36][197/233]	Loss 0.0353 (0.0437)	
training:	Epoch: [36][198/233]	Loss 0.0450 (0.0437)	
training:	Epoch: [36][199/233]	Loss 0.0711 (0.0439)	
training:	Epoch: [36][200/233]	Loss 0.0755 (0.0440)	
training:	Epoch: [36][201/233]	Loss 0.0469 (0.0441)	
training:	Epoch: [36][202/233]	Loss 0.0511 (0.0441)	
training:	Epoch: [36][203/233]	Loss 0.0476 (0.0441)	
training:	Epoch: [36][204/233]	Loss 0.0449 (0.0441)	
training:	Epoch: [36][205/233]	Loss 0.0672 (0.0442)	
training:	Epoch: [36][206/233]	Loss 0.0721 (0.0444)	
training:	Epoch: [36][207/233]	Loss 0.0488 (0.0444)	
training:	Epoch: [36][208/233]	Loss 0.0391 (0.0444)	
training:	Epoch: [36][209/233]	Loss 0.0687 (0.0445)	
training:	Epoch: [36][210/233]	Loss 0.1022 (0.0447)	
training:	Epoch: [36][211/233]	Loss 0.0400 (0.0447)	
training:	Epoch: [36][212/233]	Loss 0.0543 (0.0448)	
training:	Epoch: [36][213/233]	Loss 0.0358 (0.0447)	
training:	Epoch: [36][214/233]	Loss 0.0379 (0.0447)	
training:	Epoch: [36][215/233]	Loss 0.0357 (0.0447)	
training:	Epoch: [36][216/233]	Loss 0.0362 (0.0446)	
training:	Epoch: [36][217/233]	Loss 0.0366 (0.0446)	
training:	Epoch: [36][218/233]	Loss 0.0371 (0.0445)	
training:	Epoch: [36][219/233]	Loss 0.0688 (0.0447)	
training:	Epoch: [36][220/233]	Loss 0.0389 (0.0446)	
training:	Epoch: [36][221/233]	Loss 0.0411 (0.0446)	
training:	Epoch: [36][222/233]	Loss 0.0391 (0.0446)	
training:	Epoch: [36][223/233]	Loss 0.0339 (0.0445)	
training:	Epoch: [36][224/233]	Loss 0.0439 (0.0445)	
training:	Epoch: [36][225/233]	Loss 0.0335 (0.0445)	
training:	Epoch: [36][226/233]	Loss 0.0586 (0.0445)	
training:	Epoch: [36][227/233]	Loss 0.0478 (0.0446)	
training:	Epoch: [36][228/233]	Loss 0.0439 (0.0446)	
training:	Epoch: [36][229/233]	Loss 0.0412 (0.0445)	
training:	Epoch: [36][230/233]	Loss 0.0392 (0.0445)	
training:	Epoch: [36][231/233]	Loss 0.0590 (0.0446)	
training:	Epoch: [36][232/233]	Loss 0.0370 (0.0446)	
training:	Epoch: [36][233/233]	Loss 0.0733 (0.0447)	
Training:	 Loss: 0.0446

Training:	 ACC: 1.0000 1.0000 1.0000 1.0000
Validation:	 ACC: 0.8642 0.8636 0.8495 0.8789
Validation:	 Best_BACC: 0.8671 0.8662 0.8485 0.8857
Validation:	 Loss: 0.3188
Pretraining:	Epoch 37/200
----------
training:	Epoch: [37][1/233]	Loss 0.0625 (0.0625)	
training:	Epoch: [37][2/233]	Loss 0.0328 (0.0477)	
training:	Epoch: [37][3/233]	Loss 0.0327 (0.0427)	
training:	Epoch: [37][4/233]	Loss 0.0399 (0.0420)	
training:	Epoch: [37][5/233]	Loss 0.0375 (0.0411)	
training:	Epoch: [37][6/233]	Loss 0.0350 (0.0401)	
training:	Epoch: [37][7/233]	Loss 0.0354 (0.0394)	
training:	Epoch: [37][8/233]	Loss 0.0364 (0.0390)	
training:	Epoch: [37][9/233]	Loss 0.0370 (0.0388)	
training:	Epoch: [37][10/233]	Loss 0.0429 (0.0392)	
training:	Epoch: [37][11/233]	Loss 0.0336 (0.0387)	
training:	Epoch: [37][12/233]	Loss 0.0364 (0.0385)	
training:	Epoch: [37][13/233]	Loss 0.0529 (0.0396)	
training:	Epoch: [37][14/233]	Loss 0.0348 (0.0393)	
training:	Epoch: [37][15/233]	Loss 0.0310 (0.0387)	
training:	Epoch: [37][16/233]	Loss 0.0688 (0.0406)	
training:	Epoch: [37][17/233]	Loss 0.0346 (0.0402)	
training:	Epoch: [37][18/233]	Loss 0.0420 (0.0403)	
training:	Epoch: [37][19/233]	Loss 0.0500 (0.0409)	
training:	Epoch: [37][20/233]	Loss 0.0607 (0.0418)	
training:	Epoch: [37][21/233]	Loss 0.0352 (0.0415)	
training:	Epoch: [37][22/233]	Loss 0.0333 (0.0412)	
training:	Epoch: [37][23/233]	Loss 0.0455 (0.0413)	
training:	Epoch: [37][24/233]	Loss 0.0329 (0.0410)	
training:	Epoch: [37][25/233]	Loss 0.0326 (0.0407)	
training:	Epoch: [37][26/233]	Loss 0.0436 (0.0408)	
training:	Epoch: [37][27/233]	Loss 0.0415 (0.0408)	
training:	Epoch: [37][28/233]	Loss 0.0348 (0.0406)	
training:	Epoch: [37][29/233]	Loss 0.0314 (0.0403)	
training:	Epoch: [37][30/233]	Loss 0.0377 (0.0402)	
training:	Epoch: [37][31/233]	Loss 0.0357 (0.0400)	
training:	Epoch: [37][32/233]	Loss 0.0394 (0.0400)	
training:	Epoch: [37][33/233]	Loss 0.0343 (0.0398)	
training:	Epoch: [37][34/233]	Loss 0.0392 (0.0398)	
training:	Epoch: [37][35/233]	Loss 0.0724 (0.0408)	
training:	Epoch: [37][36/233]	Loss 0.0378 (0.0407)	
training:	Epoch: [37][37/233]	Loss 0.0420 (0.0407)	
training:	Epoch: [37][38/233]	Loss 0.0513 (0.0410)	
training:	Epoch: [37][39/233]	Loss 0.0336 (0.0408)	
training:	Epoch: [37][40/233]	Loss 0.0437 (0.0409)	
training:	Epoch: [37][41/233]	Loss 0.0349 (0.0407)	
training:	Epoch: [37][42/233]	Loss 0.0367 (0.0406)	
training:	Epoch: [37][43/233]	Loss 0.0466 (0.0408)	
training:	Epoch: [37][44/233]	Loss 0.0346 (0.0406)	
training:	Epoch: [37][45/233]	Loss 0.0364 (0.0405)	
training:	Epoch: [37][46/233]	Loss 0.0342 (0.0404)	
training:	Epoch: [37][47/233]	Loss 0.0364 (0.0403)	
training:	Epoch: [37][48/233]	Loss 0.0446 (0.0404)	
training:	Epoch: [37][49/233]	Loss 0.0488 (0.0406)	
training:	Epoch: [37][50/233]	Loss 0.0371 (0.0405)	
training:	Epoch: [37][51/233]	Loss 0.0369 (0.0404)	
training:	Epoch: [37][52/233]	Loss 0.0376 (0.0404)	
training:	Epoch: [37][53/233]	Loss 0.0412 (0.0404)	
training:	Epoch: [37][54/233]	Loss 0.0378 (0.0403)	
training:	Epoch: [37][55/233]	Loss 0.0968 (0.0414)	
training:	Epoch: [37][56/233]	Loss 0.0450 (0.0414)	
training:	Epoch: [37][57/233]	Loss 0.0338 (0.0413)	
training:	Epoch: [37][58/233]	Loss 0.0330 (0.0412)	
training:	Epoch: [37][59/233]	Loss 0.0315 (0.0410)	
training:	Epoch: [37][60/233]	Loss 0.0397 (0.0410)	
training:	Epoch: [37][61/233]	Loss 0.0383 (0.0409)	
training:	Epoch: [37][62/233]	Loss 0.0357 (0.0408)	
training:	Epoch: [37][63/233]	Loss 0.0334 (0.0407)	
training:	Epoch: [37][64/233]	Loss 0.0476 (0.0408)	
training:	Epoch: [37][65/233]	Loss 0.0495 (0.0410)	
training:	Epoch: [37][66/233]	Loss 0.0411 (0.0410)	
training:	Epoch: [37][67/233]	Loss 0.0330 (0.0408)	
training:	Epoch: [37][68/233]	Loss 0.0488 (0.0410)	
training:	Epoch: [37][69/233]	Loss 0.0384 (0.0409)	
training:	Epoch: [37][70/233]	Loss 0.0410 (0.0409)	
training:	Epoch: [37][71/233]	Loss 0.0393 (0.0409)	
training:	Epoch: [37][72/233]	Loss 0.0313 (0.0408)	
training:	Epoch: [37][73/233]	Loss 0.0347 (0.0407)	
training:	Epoch: [37][74/233]	Loss 0.0385 (0.0407)	
training:	Epoch: [37][75/233]	Loss 0.0337 (0.0406)	
training:	Epoch: [37][76/233]	Loss 0.0387 (0.0405)	
training:	Epoch: [37][77/233]	Loss 0.0387 (0.0405)	
training:	Epoch: [37][78/233]	Loss 0.0429 (0.0405)	
training:	Epoch: [37][79/233]	Loss 0.0457 (0.0406)	
training:	Epoch: [37][80/233]	Loss 0.0428 (0.0406)	
training:	Epoch: [37][81/233]	Loss 0.0335 (0.0406)	
training:	Epoch: [37][82/233]	Loss 0.0301 (0.0404)	
training:	Epoch: [37][83/233]	Loss 0.0481 (0.0405)	
training:	Epoch: [37][84/233]	Loss 0.0602 (0.0408)	
training:	Epoch: [37][85/233]	Loss 0.0367 (0.0407)	
training:	Epoch: [37][86/233]	Loss 0.0725 (0.0411)	
training:	Epoch: [37][87/233]	Loss 0.0386 (0.0410)	
training:	Epoch: [37][88/233]	Loss 0.0325 (0.0409)	
training:	Epoch: [37][89/233]	Loss 0.0342 (0.0409)	
training:	Epoch: [37][90/233]	Loss 0.0360 (0.0408)	
training:	Epoch: [37][91/233]	Loss 0.0505 (0.0409)	
training:	Epoch: [37][92/233]	Loss 0.0362 (0.0409)	
training:	Epoch: [37][93/233]	Loss 0.0329 (0.0408)	
training:	Epoch: [37][94/233]	Loss 0.0446 (0.0408)	
training:	Epoch: [37][95/233]	Loss 0.0380 (0.0408)	
training:	Epoch: [37][96/233]	Loss 0.0451 (0.0408)	
training:	Epoch: [37][97/233]	Loss 0.0838 (0.0413)	
training:	Epoch: [37][98/233]	Loss 0.0370 (0.0412)	
training:	Epoch: [37][99/233]	Loss 0.0501 (0.0413)	
training:	Epoch: [37][100/233]	Loss 0.0381 (0.0413)	
training:	Epoch: [37][101/233]	Loss 0.0359 (0.0412)	
training:	Epoch: [37][102/233]	Loss 0.0462 (0.0413)	
training:	Epoch: [37][103/233]	Loss 0.0678 (0.0416)	
training:	Epoch: [37][104/233]	Loss 0.0362 (0.0415)	
training:	Epoch: [37][105/233]	Loss 0.0358 (0.0414)	
training:	Epoch: [37][106/233]	Loss 0.0368 (0.0414)	
training:	Epoch: [37][107/233]	Loss 0.0442 (0.0414)	
training:	Epoch: [37][108/233]	Loss 0.1006 (0.0420)	
training:	Epoch: [37][109/233]	Loss 0.0489 (0.0420)	
training:	Epoch: [37][110/233]	Loss 0.0729 (0.0423)	
training:	Epoch: [37][111/233]	Loss 0.0304 (0.0422)	
training:	Epoch: [37][112/233]	Loss 0.0425 (0.0422)	
training:	Epoch: [37][113/233]	Loss 0.0420 (0.0422)	
training:	Epoch: [37][114/233]	Loss 0.0497 (0.0423)	
training:	Epoch: [37][115/233]	Loss 0.0304 (0.0422)	
training:	Epoch: [37][116/233]	Loss 0.0389 (0.0421)	
training:	Epoch: [37][117/233]	Loss 0.0318 (0.0421)	
training:	Epoch: [37][118/233]	Loss 0.0336 (0.0420)	
training:	Epoch: [37][119/233]	Loss 0.0319 (0.0419)	
training:	Epoch: [37][120/233]	Loss 0.0556 (0.0420)	
training:	Epoch: [37][121/233]	Loss 0.0444 (0.0420)	
training:	Epoch: [37][122/233]	Loss 0.0349 (0.0420)	
training:	Epoch: [37][123/233]	Loss 0.0438 (0.0420)	
training:	Epoch: [37][124/233]	Loss 0.0402 (0.0420)	
training:	Epoch: [37][125/233]	Loss 0.1303 (0.0427)	
training:	Epoch: [37][126/233]	Loss 0.0319 (0.0426)	
training:	Epoch: [37][127/233]	Loss 0.0377 (0.0426)	
training:	Epoch: [37][128/233]	Loss 0.0426 (0.0426)	
training:	Epoch: [37][129/233]	Loss 0.0369 (0.0425)	
training:	Epoch: [37][130/233]	Loss 0.0384 (0.0425)	
training:	Epoch: [37][131/233]	Loss 0.0403 (0.0425)	
training:	Epoch: [37][132/233]	Loss 0.0357 (0.0424)	
training:	Epoch: [37][133/233]	Loss 0.0493 (0.0425)	
training:	Epoch: [37][134/233]	Loss 0.0380 (0.0424)	
training:	Epoch: [37][135/233]	Loss 0.0671 (0.0426)	
training:	Epoch: [37][136/233]	Loss 0.0683 (0.0428)	
training:	Epoch: [37][137/233]	Loss 0.0390 (0.0428)	
training:	Epoch: [37][138/233]	Loss 0.0370 (0.0427)	
training:	Epoch: [37][139/233]	Loss 0.0432 (0.0427)	
training:	Epoch: [37][140/233]	Loss 0.0342 (0.0427)	
training:	Epoch: [37][141/233]	Loss 0.0360 (0.0426)	
training:	Epoch: [37][142/233]	Loss 0.0537 (0.0427)	
training:	Epoch: [37][143/233]	Loss 0.0599 (0.0428)	
training:	Epoch: [37][144/233]	Loss 0.0355 (0.0428)	
training:	Epoch: [37][145/233]	Loss 0.0556 (0.0429)	
training:	Epoch: [37][146/233]	Loss 0.0414 (0.0429)	
training:	Epoch: [37][147/233]	Loss 0.0721 (0.0431)	
training:	Epoch: [37][148/233]	Loss 0.0412 (0.0430)	
training:	Epoch: [37][149/233]	Loss 0.0348 (0.0430)	
training:	Epoch: [37][150/233]	Loss 0.0618 (0.0431)	
training:	Epoch: [37][151/233]	Loss 0.0313 (0.0430)	
training:	Epoch: [37][152/233]	Loss 0.0415 (0.0430)	
training:	Epoch: [37][153/233]	Loss 0.0667 (0.0432)	
training:	Epoch: [37][154/233]	Loss 0.0418 (0.0432)	
training:	Epoch: [37][155/233]	Loss 0.0333 (0.0431)	
training:	Epoch: [37][156/233]	Loss 0.0676 (0.0433)	
training:	Epoch: [37][157/233]	Loss 0.0392 (0.0432)	
training:	Epoch: [37][158/233]	Loss 0.0401 (0.0432)	
training:	Epoch: [37][159/233]	Loss 0.0361 (0.0432)	
training:	Epoch: [37][160/233]	Loss 0.0358 (0.0431)	
training:	Epoch: [37][161/233]	Loss 0.0337 (0.0431)	
training:	Epoch: [37][162/233]	Loss 0.0354 (0.0430)	
training:	Epoch: [37][163/233]	Loss 0.0408 (0.0430)	
training:	Epoch: [37][164/233]	Loss 0.0396 (0.0430)	
training:	Epoch: [37][165/233]	Loss 0.0316 (0.0429)	
training:	Epoch: [37][166/233]	Loss 0.0367 (0.0429)	
training:	Epoch: [37][167/233]	Loss 0.0345 (0.0428)	
training:	Epoch: [37][168/233]	Loss 0.0399 (0.0428)	
training:	Epoch: [37][169/233]	Loss 0.0343 (0.0428)	
training:	Epoch: [37][170/233]	Loss 0.1152 (0.0432)	
training:	Epoch: [37][171/233]	Loss 0.0419 (0.0432)	
training:	Epoch: [37][172/233]	Loss 0.0285 (0.0431)	
training:	Epoch: [37][173/233]	Loss 0.0383 (0.0431)	
training:	Epoch: [37][174/233]	Loss 0.0446 (0.0431)	
training:	Epoch: [37][175/233]	Loss 0.0476 (0.0431)	
training:	Epoch: [37][176/233]	Loss 0.0465 (0.0431)	
training:	Epoch: [37][177/233]	Loss 0.0789 (0.0433)	
training:	Epoch: [37][178/233]	Loss 0.0346 (0.0433)	
training:	Epoch: [37][179/233]	Loss 0.0427 (0.0433)	
training:	Epoch: [37][180/233]	Loss 0.0488 (0.0433)	
training:	Epoch: [37][181/233]	Loss 0.0389 (0.0433)	
training:	Epoch: [37][182/233]	Loss 0.0334 (0.0432)	
training:	Epoch: [37][183/233]	Loss 0.0369 (0.0432)	
training:	Epoch: [37][184/233]	Loss 0.0506 (0.0432)	
training:	Epoch: [37][185/233]	Loss 0.1257 (0.0437)	
training:	Epoch: [37][186/233]	Loss 0.0409 (0.0437)	
training:	Epoch: [37][187/233]	Loss 0.0339 (0.0436)	
training:	Epoch: [37][188/233]	Loss 0.0745 (0.0438)	
training:	Epoch: [37][189/233]	Loss 0.0482 (0.0438)	
training:	Epoch: [37][190/233]	Loss 0.0464 (0.0438)	
training:	Epoch: [37][191/233]	Loss 0.0525 (0.0439)	
training:	Epoch: [37][192/233]	Loss 0.0394 (0.0438)	
training:	Epoch: [37][193/233]	Loss 0.0591 (0.0439)	
training:	Epoch: [37][194/233]	Loss 0.0427 (0.0439)	
training:	Epoch: [37][195/233]	Loss 0.0435 (0.0439)	
training:	Epoch: [37][196/233]	Loss 0.0375 (0.0439)	
training:	Epoch: [37][197/233]	Loss 0.0924 (0.0441)	
training:	Epoch: [37][198/233]	Loss 0.0411 (0.0441)	
training:	Epoch: [37][199/233]	Loss 0.0413 (0.0441)	
training:	Epoch: [37][200/233]	Loss 0.0466 (0.0441)	
training:	Epoch: [37][201/233]	Loss 0.0428 (0.0441)	
training:	Epoch: [37][202/233]	Loss 0.0316 (0.0440)	
training:	Epoch: [37][203/233]	Loss 0.0404 (0.0440)	
training:	Epoch: [37][204/233]	Loss 0.0537 (0.0441)	
training:	Epoch: [37][205/233]	Loss 0.0379 (0.0440)	
training:	Epoch: [37][206/233]	Loss 0.0349 (0.0440)	
training:	Epoch: [37][207/233]	Loss 0.0386 (0.0440)	
training:	Epoch: [37][208/233]	Loss 0.0419 (0.0440)	
training:	Epoch: [37][209/233]	Loss 0.0374 (0.0439)	
training:	Epoch: [37][210/233]	Loss 0.0514 (0.0440)	
training:	Epoch: [37][211/233]	Loss 0.0390 (0.0439)	
training:	Epoch: [37][212/233]	Loss 0.0466 (0.0439)	
training:	Epoch: [37][213/233]	Loss 0.0334 (0.0439)	
training:	Epoch: [37][214/233]	Loss 0.0365 (0.0439)	
training:	Epoch: [37][215/233]	Loss 0.0493 (0.0439)	
training:	Epoch: [37][216/233]	Loss 0.0587 (0.0440)	
training:	Epoch: [37][217/233]	Loss 0.0382 (0.0439)	
training:	Epoch: [37][218/233]	Loss 0.0629 (0.0440)	
training:	Epoch: [37][219/233]	Loss 0.0402 (0.0440)	
training:	Epoch: [37][220/233]	Loss 0.0530 (0.0440)	
training:	Epoch: [37][221/233]	Loss 0.0407 (0.0440)	
training:	Epoch: [37][222/233]	Loss 0.0411 (0.0440)	
training:	Epoch: [37][223/233]	Loss 0.0395 (0.0440)	
training:	Epoch: [37][224/233]	Loss 0.0506 (0.0440)	
training:	Epoch: [37][225/233]	Loss 0.0453 (0.0440)	
training:	Epoch: [37][226/233]	Loss 0.0360 (0.0440)	
training:	Epoch: [37][227/233]	Loss 0.0410 (0.0440)	
training:	Epoch: [37][228/233]	Loss 0.0362 (0.0439)	
training:	Epoch: [37][229/233]	Loss 0.0372 (0.0439)	
training:	Epoch: [37][230/233]	Loss 0.0370 (0.0439)	
training:	Epoch: [37][231/233]	Loss 0.0408 (0.0439)	
training:	Epoch: [37][232/233]	Loss 0.0355 (0.0438)	
training:	Epoch: [37][233/233]	Loss 0.0390 (0.0438)	
Training:	 Loss: 0.0437

Training:	 ACC: 1.0000 1.0000 1.0000 1.0000
Validation:	 ACC: 0.8636 0.8636 0.8618 0.8655
Validation:	 Best_BACC: 0.8671 0.8662 0.8485 0.8857
Validation:	 Loss: 0.3168
Pretraining:	Epoch 38/200
----------
training:	Epoch: [38][1/233]	Loss 0.0607 (0.0607)	
training:	Epoch: [38][2/233]	Loss 0.0429 (0.0518)	
training:	Epoch: [38][3/233]	Loss 0.0343 (0.0460)	
training:	Epoch: [38][4/233]	Loss 0.0431 (0.0453)	
training:	Epoch: [38][5/233]	Loss 0.0401 (0.0442)	
training:	Epoch: [38][6/233]	Loss 0.0509 (0.0453)	
training:	Epoch: [38][7/233]	Loss 0.0349 (0.0438)	
training:	Epoch: [38][8/233]	Loss 0.0397 (0.0433)	
training:	Epoch: [38][9/233]	Loss 0.0374 (0.0427)	
training:	Epoch: [38][10/233]	Loss 0.0342 (0.0418)	
training:	Epoch: [38][11/233]	Loss 0.0294 (0.0407)	
training:	Epoch: [38][12/233]	Loss 0.0416 (0.0408)	
training:	Epoch: [38][13/233]	Loss 0.0399 (0.0407)	
training:	Epoch: [38][14/233]	Loss 0.0437 (0.0409)	
training:	Epoch: [38][15/233]	Loss 0.0567 (0.0420)	
training:	Epoch: [38][16/233]	Loss 0.0332 (0.0414)	
training:	Epoch: [38][17/233]	Loss 0.0355 (0.0411)	
training:	Epoch: [38][18/233]	Loss 0.0349 (0.0407)	
training:	Epoch: [38][19/233]	Loss 0.0757 (0.0426)	
training:	Epoch: [38][20/233]	Loss 0.0473 (0.0428)	
training:	Epoch: [38][21/233]	Loss 0.0401 (0.0427)	
training:	Epoch: [38][22/233]	Loss 0.0435 (0.0427)	
training:	Epoch: [38][23/233]	Loss 0.0372 (0.0425)	
training:	Epoch: [38][24/233]	Loss 0.0433 (0.0425)	
training:	Epoch: [38][25/233]	Loss 0.0389 (0.0424)	
training:	Epoch: [38][26/233]	Loss 0.0471 (0.0426)	
training:	Epoch: [38][27/233]	Loss 0.0404 (0.0425)	
training:	Epoch: [38][28/233]	Loss 0.0348 (0.0422)	
training:	Epoch: [38][29/233]	Loss 0.0354 (0.0420)	
training:	Epoch: [38][30/233]	Loss 0.0310 (0.0416)	
training:	Epoch: [38][31/233]	Loss 0.0337 (0.0413)	
training:	Epoch: [38][32/233]	Loss 0.0607 (0.0419)	
training:	Epoch: [38][33/233]	Loss 0.0304 (0.0416)	
training:	Epoch: [38][34/233]	Loss 0.0435 (0.0417)	
training:	Epoch: [38][35/233]	Loss 0.0345 (0.0414)	
training:	Epoch: [38][36/233]	Loss 0.0473 (0.0416)	
training:	Epoch: [38][37/233]	Loss 0.0505 (0.0418)	
training:	Epoch: [38][38/233]	Loss 0.0410 (0.0418)	
training:	Epoch: [38][39/233]	Loss 0.0378 (0.0417)	
training:	Epoch: [38][40/233]	Loss 0.0303 (0.0414)	
training:	Epoch: [38][41/233]	Loss 0.0371 (0.0413)	
training:	Epoch: [38][42/233]	Loss 0.0322 (0.0411)	
training:	Epoch: [38][43/233]	Loss 0.0358 (0.0410)	
training:	Epoch: [38][44/233]	Loss 0.0375 (0.0409)	
training:	Epoch: [38][45/233]	Loss 0.0422 (0.0409)	
training:	Epoch: [38][46/233]	Loss 0.0349 (0.0408)	
training:	Epoch: [38][47/233]	Loss 0.0355 (0.0407)	
training:	Epoch: [38][48/233]	Loss 0.0298 (0.0405)	
training:	Epoch: [38][49/233]	Loss 0.0349 (0.0404)	
training:	Epoch: [38][50/233]	Loss 0.0383 (0.0403)	
training:	Epoch: [38][51/233]	Loss 0.0400 (0.0403)	
training:	Epoch: [38][52/233]	Loss 0.0370 (0.0402)	
training:	Epoch: [38][53/233]	Loss 0.0390 (0.0402)	
training:	Epoch: [38][54/233]	Loss 0.0337 (0.0401)	
training:	Epoch: [38][55/233]	Loss 0.0887 (0.0410)	
training:	Epoch: [38][56/233]	Loss 0.0372 (0.0409)	
training:	Epoch: [38][57/233]	Loss 0.0345 (0.0408)	
training:	Epoch: [38][58/233]	Loss 0.0344 (0.0407)	
training:	Epoch: [38][59/233]	Loss 0.0352 (0.0406)	
training:	Epoch: [38][60/233]	Loss 0.0535 (0.0408)	
training:	Epoch: [38][61/233]	Loss 0.0318 (0.0407)	
training:	Epoch: [38][62/233]	Loss 0.0368 (0.0406)	
training:	Epoch: [38][63/233]	Loss 0.0326 (0.0405)	
training:	Epoch: [38][64/233]	Loss 0.0336 (0.0404)	
training:	Epoch: [38][65/233]	Loss 0.0390 (0.0404)	
training:	Epoch: [38][66/233]	Loss 0.0781 (0.0409)	
training:	Epoch: [38][67/233]	Loss 0.0339 (0.0408)	
training:	Epoch: [38][68/233]	Loss 0.0407 (0.0408)	
training:	Epoch: [38][69/233]	Loss 0.0479 (0.0409)	
training:	Epoch: [38][70/233]	Loss 0.0376 (0.0409)	
training:	Epoch: [38][71/233]	Loss 0.0372 (0.0408)	
training:	Epoch: [38][72/233]	Loss 0.0331 (0.0407)	
training:	Epoch: [38][73/233]	Loss 0.0445 (0.0408)	
training:	Epoch: [38][74/233]	Loss 0.0459 (0.0408)	
training:	Epoch: [38][75/233]	Loss 0.0462 (0.0409)	
training:	Epoch: [38][76/233]	Loss 0.0395 (0.0409)	
training:	Epoch: [38][77/233]	Loss 0.0337 (0.0408)	
training:	Epoch: [38][78/233]	Loss 0.0343 (0.0407)	
training:	Epoch: [38][79/233]	Loss 0.0431 (0.0407)	
training:	Epoch: [38][80/233]	Loss 0.0336 (0.0407)	
training:	Epoch: [38][81/233]	Loss 0.0339 (0.0406)	
training:	Epoch: [38][82/233]	Loss 0.0339 (0.0405)	
training:	Epoch: [38][83/233]	Loss 0.0348 (0.0404)	
training:	Epoch: [38][84/233]	Loss 0.0366 (0.0404)	
training:	Epoch: [38][85/233]	Loss 0.0663 (0.0407)	
training:	Epoch: [38][86/233]	Loss 0.0350 (0.0406)	
training:	Epoch: [38][87/233]	Loss 0.0342 (0.0405)	
training:	Epoch: [38][88/233]	Loss 0.0469 (0.0406)	
training:	Epoch: [38][89/233]	Loss 0.0484 (0.0407)	
training:	Epoch: [38][90/233]	Loss 0.0310 (0.0406)	
training:	Epoch: [38][91/233]	Loss 0.0336 (0.0405)	
training:	Epoch: [38][92/233]	Loss 0.0898 (0.0411)	
training:	Epoch: [38][93/233]	Loss 0.0393 (0.0410)	
training:	Epoch: [38][94/233]	Loss 0.0411 (0.0410)	
training:	Epoch: [38][95/233]	Loss 0.0321 (0.0409)	
training:	Epoch: [38][96/233]	Loss 0.0357 (0.0409)	
training:	Epoch: [38][97/233]	Loss 0.0311 (0.0408)	
training:	Epoch: [38][98/233]	Loss 0.0329 (0.0407)	
training:	Epoch: [38][99/233]	Loss 0.0322 (0.0406)	
training:	Epoch: [38][100/233]	Loss 0.0443 (0.0407)	
training:	Epoch: [38][101/233]	Loss 0.0413 (0.0407)	
training:	Epoch: [38][102/233]	Loss 0.0435 (0.0407)	
training:	Epoch: [38][103/233]	Loss 0.0355 (0.0406)	
training:	Epoch: [38][104/233]	Loss 0.0322 (0.0406)	
training:	Epoch: [38][105/233]	Loss 0.0397 (0.0405)	
training:	Epoch: [38][106/233]	Loss 0.0421 (0.0406)	
training:	Epoch: [38][107/233]	Loss 0.0344 (0.0405)	
training:	Epoch: [38][108/233]	Loss 0.0357 (0.0405)	
training:	Epoch: [38][109/233]	Loss 0.0331 (0.0404)	
training:	Epoch: [38][110/233]	Loss 0.0580 (0.0406)	
training:	Epoch: [38][111/233]	Loss 0.0344 (0.0405)	
training:	Epoch: [38][112/233]	Loss 0.0525 (0.0406)	
training:	Epoch: [38][113/233]	Loss 0.0339 (0.0405)	
training:	Epoch: [38][114/233]	Loss 0.0400 (0.0405)	
training:	Epoch: [38][115/233]	Loss 0.0548 (0.0407)	
training:	Epoch: [38][116/233]	Loss 0.0621 (0.0409)	
training:	Epoch: [38][117/233]	Loss 0.0360 (0.0408)	
training:	Epoch: [38][118/233]	Loss 0.0335 (0.0407)	
training:	Epoch: [38][119/233]	Loss 0.0518 (0.0408)	
training:	Epoch: [38][120/233]	Loss 0.0364 (0.0408)	
training:	Epoch: [38][121/233]	Loss 0.0398 (0.0408)	
training:	Epoch: [38][122/233]	Loss 0.0323 (0.0407)	
training:	Epoch: [38][123/233]	Loss 0.0320 (0.0407)	
training:	Epoch: [38][124/233]	Loss 0.0380 (0.0406)	
training:	Epoch: [38][125/233]	Loss 0.0414 (0.0406)	
training:	Epoch: [38][126/233]	Loss 0.0353 (0.0406)	
training:	Epoch: [38][127/233]	Loss 0.0336 (0.0405)	
training:	Epoch: [38][128/233]	Loss 0.0356 (0.0405)	
training:	Epoch: [38][129/233]	Loss 0.0323 (0.0404)	
training:	Epoch: [38][130/233]	Loss 0.0365 (0.0404)	
training:	Epoch: [38][131/233]	Loss 0.0363 (0.0404)	
training:	Epoch: [38][132/233]	Loss 0.0382 (0.0404)	
training:	Epoch: [38][133/233]	Loss 0.0424 (0.0404)	
training:	Epoch: [38][134/233]	Loss 0.0411 (0.0404)	
training:	Epoch: [38][135/233]	Loss 0.0628 (0.0406)	
training:	Epoch: [38][136/233]	Loss 0.0302 (0.0405)	
training:	Epoch: [38][137/233]	Loss 0.0457 (0.0405)	
training:	Epoch: [38][138/233]	Loss 0.0669 (0.0407)	
training:	Epoch: [38][139/233]	Loss 0.0305 (0.0406)	
training:	Epoch: [38][140/233]	Loss 0.0294 (0.0405)	
training:	Epoch: [38][141/233]	Loss 0.0488 (0.0406)	
training:	Epoch: [38][142/233]	Loss 0.0313 (0.0405)	
training:	Epoch: [38][143/233]	Loss 0.0376 (0.0405)	
training:	Epoch: [38][144/233]	Loss 0.0372 (0.0405)	
training:	Epoch: [38][145/233]	Loss 0.0409 (0.0405)	
training:	Epoch: [38][146/233]	Loss 0.0383 (0.0405)	
training:	Epoch: [38][147/233]	Loss 0.0450 (0.0405)	
training:	Epoch: [38][148/233]	Loss 0.0408 (0.0405)	
training:	Epoch: [38][149/233]	Loss 0.0363 (0.0405)	
training:	Epoch: [38][150/233]	Loss 0.0436 (0.0405)	
training:	Epoch: [38][151/233]	Loss 0.0410 (0.0405)	
training:	Epoch: [38][152/233]	Loss 0.0360 (0.0405)	
training:	Epoch: [38][153/233]	Loss 0.0379 (0.0405)	
training:	Epoch: [38][154/233]	Loss 0.0388 (0.0405)	
training:	Epoch: [38][155/233]	Loss 0.0389 (0.0404)	
training:	Epoch: [38][156/233]	Loss 0.0358 (0.0404)	
training:	Epoch: [38][157/233]	Loss 0.0496 (0.0405)	
training:	Epoch: [38][158/233]	Loss 0.0319 (0.0404)	
training:	Epoch: [38][159/233]	Loss 0.0334 (0.0404)	
training:	Epoch: [38][160/233]	Loss 0.0558 (0.0405)	
training:	Epoch: [38][161/233]	Loss 0.0643 (0.0406)	
training:	Epoch: [38][162/233]	Loss 0.0394 (0.0406)	
training:	Epoch: [38][163/233]	Loss 0.0550 (0.0407)	
training:	Epoch: [38][164/233]	Loss 0.0418 (0.0407)	
training:	Epoch: [38][165/233]	Loss 0.0450 (0.0407)	
training:	Epoch: [38][166/233]	Loss 0.0520 (0.0408)	
training:	Epoch: [38][167/233]	Loss 0.0467 (0.0408)	
training:	Epoch: [38][168/233]	Loss 0.0315 (0.0408)	
training:	Epoch: [38][169/233]	Loss 0.0698 (0.0410)	
training:	Epoch: [38][170/233]	Loss 0.0386 (0.0409)	
training:	Epoch: [38][171/233]	Loss 0.0385 (0.0409)	
training:	Epoch: [38][172/233]	Loss 0.0350 (0.0409)	
training:	Epoch: [38][173/233]	Loss 0.0370 (0.0409)	
training:	Epoch: [38][174/233]	Loss 0.0316 (0.0408)	
training:	Epoch: [38][175/233]	Loss 0.0391 (0.0408)	
training:	Epoch: [38][176/233]	Loss 0.0336 (0.0408)	
training:	Epoch: [38][177/233]	Loss 0.0602 (0.0409)	
training:	Epoch: [38][178/233]	Loss 0.0363 (0.0408)	
training:	Epoch: [38][179/233]	Loss 0.0582 (0.0409)	
training:	Epoch: [38][180/233]	Loss 0.0394 (0.0409)	
training:	Epoch: [38][181/233]	Loss 0.0347 (0.0409)	
training:	Epoch: [38][182/233]	Loss 0.0370 (0.0409)	
training:	Epoch: [38][183/233]	Loss 0.0597 (0.0410)	
training:	Epoch: [38][184/233]	Loss 0.0413 (0.0410)	
training:	Epoch: [38][185/233]	Loss 0.0462 (0.0410)	
training:	Epoch: [38][186/233]	Loss 0.0355 (0.0410)	
training:	Epoch: [38][187/233]	Loss 0.0566 (0.0411)	
training:	Epoch: [38][188/233]	Loss 0.0380 (0.0411)	
training:	Epoch: [38][189/233]	Loss 0.0328 (0.0410)	
training:	Epoch: [38][190/233]	Loss 0.0330 (0.0410)	
training:	Epoch: [38][191/233]	Loss 0.0489 (0.0410)	
training:	Epoch: [38][192/233]	Loss 0.0332 (0.0410)	
training:	Epoch: [38][193/233]	Loss 0.0656 (0.0411)	
training:	Epoch: [38][194/233]	Loss 0.0791 (0.0413)	
training:	Epoch: [38][195/233]	Loss 0.0358 (0.0413)	
training:	Epoch: [38][196/233]	Loss 0.0342 (0.0412)	
training:	Epoch: [38][197/233]	Loss 0.0390 (0.0412)	
training:	Epoch: [38][198/233]	Loss 0.0375 (0.0412)	
training:	Epoch: [38][199/233]	Loss 0.0702 (0.0413)	
training:	Epoch: [38][200/233]	Loss 0.0432 (0.0414)	
training:	Epoch: [38][201/233]	Loss 0.0296 (0.0413)	
training:	Epoch: [38][202/233]	Loss 0.0550 (0.0414)	
training:	Epoch: [38][203/233]	Loss 0.0517 (0.0414)	
training:	Epoch: [38][204/233]	Loss 0.0378 (0.0414)	
training:	Epoch: [38][205/233]	Loss 0.0664 (0.0415)	
training:	Epoch: [38][206/233]	Loss 0.0483 (0.0415)	
training:	Epoch: [38][207/233]	Loss 0.0290 (0.0415)	
training:	Epoch: [38][208/233]	Loss 0.0407 (0.0415)	
training:	Epoch: [38][209/233]	Loss 0.0377 (0.0415)	
training:	Epoch: [38][210/233]	Loss 0.0466 (0.0415)	
training:	Epoch: [38][211/233]	Loss 0.0425 (0.0415)	
training:	Epoch: [38][212/233]	Loss 0.1249 (0.0419)	
training:	Epoch: [38][213/233]	Loss 0.0417 (0.0419)	
training:	Epoch: [38][214/233]	Loss 0.0308 (0.0418)	
training:	Epoch: [38][215/233]	Loss 0.0409 (0.0418)	
training:	Epoch: [38][216/233]	Loss 0.0360 (0.0418)	
training:	Epoch: [38][217/233]	Loss 0.0351 (0.0418)	
training:	Epoch: [38][218/233]	Loss 0.0329 (0.0417)	
training:	Epoch: [38][219/233]	Loss 0.0341 (0.0417)	
training:	Epoch: [38][220/233]	Loss 0.0337 (0.0417)	
training:	Epoch: [38][221/233]	Loss 0.0349 (0.0416)	
training:	Epoch: [38][222/233]	Loss 0.0951 (0.0419)	
training:	Epoch: [38][223/233]	Loss 0.0367 (0.0418)	
training:	Epoch: [38][224/233]	Loss 0.0347 (0.0418)	
training:	Epoch: [38][225/233]	Loss 0.0367 (0.0418)	
training:	Epoch: [38][226/233]	Loss 0.0334 (0.0418)	
training:	Epoch: [38][227/233]	Loss 0.0541 (0.0418)	
training:	Epoch: [38][228/233]	Loss 0.0365 (0.0418)	
training:	Epoch: [38][229/233]	Loss 0.0489 (0.0418)	
training:	Epoch: [38][230/233]	Loss 0.0328 (0.0418)	
training:	Epoch: [38][231/233]	Loss 0.0371 (0.0418)	
training:	Epoch: [38][232/233]	Loss 0.0478 (0.0418)	
training:	Epoch: [38][233/233]	Loss 0.0506 (0.0418)	
Training:	 Loss: 0.0417

Training:	 ACC: 1.0000 1.0000 1.0000 1.0000
Validation:	 ACC: 0.8661 0.8657 0.8577 0.8744
Validation:	 Best_BACC: 0.8671 0.8662 0.8485 0.8857
Validation:	 Loss: 0.3165
Pretraining:	Epoch 39/200
----------
training:	Epoch: [39][1/233]	Loss 0.0312 (0.0312)	
training:	Epoch: [39][2/233]	Loss 0.0354 (0.0333)	
training:	Epoch: [39][3/233]	Loss 0.0527 (0.0398)	
training:	Epoch: [39][4/233]	Loss 0.0336 (0.0382)	
training:	Epoch: [39][5/233]	Loss 0.0387 (0.0383)	
training:	Epoch: [39][6/233]	Loss 0.0306 (0.0370)	
training:	Epoch: [39][7/233]	Loss 0.0357 (0.0368)	
training:	Epoch: [39][8/233]	Loss 0.0352 (0.0366)	
training:	Epoch: [39][9/233]	Loss 0.0362 (0.0366)	
training:	Epoch: [39][10/233]	Loss 0.0346 (0.0364)	
training:	Epoch: [39][11/233]	Loss 0.0330 (0.0361)	
training:	Epoch: [39][12/233]	Loss 0.0314 (0.0357)	
training:	Epoch: [39][13/233]	Loss 0.0324 (0.0354)	
training:	Epoch: [39][14/233]	Loss 0.0287 (0.0350)	
training:	Epoch: [39][15/233]	Loss 0.0381 (0.0352)	
training:	Epoch: [39][16/233]	Loss 0.0529 (0.0363)	
training:	Epoch: [39][17/233]	Loss 0.0671 (0.0381)	
training:	Epoch: [39][18/233]	Loss 0.0300 (0.0376)	
training:	Epoch: [39][19/233]	Loss 0.0592 (0.0388)	
training:	Epoch: [39][20/233]	Loss 0.0662 (0.0402)	
training:	Epoch: [39][21/233]	Loss 0.0318 (0.0398)	
training:	Epoch: [39][22/233]	Loss 0.0327 (0.0394)	
training:	Epoch: [39][23/233]	Loss 0.0361 (0.0393)	
training:	Epoch: [39][24/233]	Loss 0.0353 (0.0391)	
training:	Epoch: [39][25/233]	Loss 0.0349 (0.0390)	
training:	Epoch: [39][26/233]	Loss 0.0539 (0.0395)	
training:	Epoch: [39][27/233]	Loss 0.0354 (0.0394)	
training:	Epoch: [39][28/233]	Loss 0.0364 (0.0393)	
training:	Epoch: [39][29/233]	Loss 0.0467 (0.0395)	
training:	Epoch: [39][30/233]	Loss 0.0437 (0.0397)	
training:	Epoch: [39][31/233]	Loss 0.0300 (0.0394)	
training:	Epoch: [39][32/233]	Loss 0.0306 (0.0391)	
training:	Epoch: [39][33/233]	Loss 0.0301 (0.0388)	
training:	Epoch: [39][34/233]	Loss 0.0351 (0.0387)	
training:	Epoch: [39][35/233]	Loss 0.0322 (0.0385)	
training:	Epoch: [39][36/233]	Loss 0.0344 (0.0384)	
training:	Epoch: [39][37/233]	Loss 0.0311 (0.0382)	
training:	Epoch: [39][38/233]	Loss 0.0444 (0.0384)	
training:	Epoch: [39][39/233]	Loss 0.0386 (0.0384)	
training:	Epoch: [39][40/233]	Loss 0.0370 (0.0383)	
training:	Epoch: [39][41/233]	Loss 0.0274 (0.0381)	
training:	Epoch: [39][42/233]	Loss 0.0311 (0.0379)	
training:	Epoch: [39][43/233]	Loss 0.0408 (0.0380)	
training:	Epoch: [39][44/233]	Loss 0.0416 (0.0381)	
training:	Epoch: [39][45/233]	Loss 0.0424 (0.0382)	
training:	Epoch: [39][46/233]	Loss 0.0322 (0.0380)	
training:	Epoch: [39][47/233]	Loss 0.0347 (0.0380)	
training:	Epoch: [39][48/233]	Loss 0.0333 (0.0379)	
training:	Epoch: [39][49/233]	Loss 0.0335 (0.0378)	
training:	Epoch: [39][50/233]	Loss 0.0493 (0.0380)	
training:	Epoch: [39][51/233]	Loss 0.0279 (0.0378)	
training:	Epoch: [39][52/233]	Loss 0.0325 (0.0377)	
training:	Epoch: [39][53/233]	Loss 0.0333 (0.0376)	
training:	Epoch: [39][54/233]	Loss 0.0332 (0.0375)	
training:	Epoch: [39][55/233]	Loss 0.0434 (0.0376)	
training:	Epoch: [39][56/233]	Loss 0.0359 (0.0376)	
training:	Epoch: [39][57/233]	Loss 0.0480 (0.0378)	
training:	Epoch: [39][58/233]	Loss 0.0364 (0.0378)	
training:	Epoch: [39][59/233]	Loss 0.0497 (0.0380)	
training:	Epoch: [39][60/233]	Loss 0.0359 (0.0379)	
training:	Epoch: [39][61/233]	Loss 0.0434 (0.0380)	
training:	Epoch: [39][62/233]	Loss 0.0464 (0.0382)	
training:	Epoch: [39][63/233]	Loss 0.0360 (0.0381)	
training:	Epoch: [39][64/233]	Loss 0.0334 (0.0380)	
training:	Epoch: [39][65/233]	Loss 0.0321 (0.0380)	
training:	Epoch: [39][66/233]	Loss 0.0298 (0.0378)	
training:	Epoch: [39][67/233]	Loss 0.0623 (0.0382)	
training:	Epoch: [39][68/233]	Loss 0.0507 (0.0384)	
training:	Epoch: [39][69/233]	Loss 0.0330 (0.0383)	
training:	Epoch: [39][70/233]	Loss 0.0424 (0.0384)	
training:	Epoch: [39][71/233]	Loss 0.0405 (0.0384)	
training:	Epoch: [39][72/233]	Loss 0.0918 (0.0391)	
training:	Epoch: [39][73/233]	Loss 0.0464 (0.0392)	
training:	Epoch: [39][74/233]	Loss 0.0317 (0.0391)	
training:	Epoch: [39][75/233]	Loss 0.0307 (0.0390)	
training:	Epoch: [39][76/233]	Loss 0.0396 (0.0390)	
training:	Epoch: [39][77/233]	Loss 0.0355 (0.0390)	
training:	Epoch: [39][78/233]	Loss 0.0336 (0.0389)	
training:	Epoch: [39][79/233]	Loss 0.0335 (0.0388)	
training:	Epoch: [39][80/233]	Loss 0.0298 (0.0387)	
training:	Epoch: [39][81/233]	Loss 0.0427 (0.0388)	
training:	Epoch: [39][82/233]	Loss 0.0362 (0.0388)	
training:	Epoch: [39][83/233]	Loss 0.0305 (0.0387)	
training:	Epoch: [39][84/233]	Loss 0.0409 (0.0387)	
training:	Epoch: [39][85/233]	Loss 0.0407 (0.0387)	
training:	Epoch: [39][86/233]	Loss 0.0331 (0.0386)	
training:	Epoch: [39][87/233]	Loss 0.0383 (0.0386)	
training:	Epoch: [39][88/233]	Loss 0.0399 (0.0386)	
training:	Epoch: [39][89/233]	Loss 0.0328 (0.0386)	
training:	Epoch: [39][90/233]	Loss 0.0496 (0.0387)	
training:	Epoch: [39][91/233]	Loss 0.0354 (0.0387)	
training:	Epoch: [39][92/233]	Loss 0.0537 (0.0388)	
training:	Epoch: [39][93/233]	Loss 0.0326 (0.0388)	
training:	Epoch: [39][94/233]	Loss 0.0346 (0.0387)	
training:	Epoch: [39][95/233]	Loss 0.0329 (0.0387)	
training:	Epoch: [39][96/233]	Loss 0.0874 (0.0392)	
training:	Epoch: [39][97/233]	Loss 0.0344 (0.0391)	
training:	Epoch: [39][98/233]	Loss 0.0453 (0.0392)	
training:	Epoch: [39][99/233]	Loss 0.0352 (0.0391)	
training:	Epoch: [39][100/233]	Loss 0.0334 (0.0391)	
training:	Epoch: [39][101/233]	Loss 0.0385 (0.0391)	
training:	Epoch: [39][102/233]	Loss 0.0455 (0.0391)	
training:	Epoch: [39][103/233]	Loss 0.0327 (0.0391)	
training:	Epoch: [39][104/233]	Loss 0.0379 (0.0391)	
training:	Epoch: [39][105/233]	Loss 0.0377 (0.0391)	
training:	Epoch: [39][106/233]	Loss 0.0350 (0.0390)	
training:	Epoch: [39][107/233]	Loss 0.0502 (0.0391)	
training:	Epoch: [39][108/233]	Loss 0.0298 (0.0390)	
training:	Epoch: [39][109/233]	Loss 0.0329 (0.0390)	
training:	Epoch: [39][110/233]	Loss 0.0346 (0.0389)	
training:	Epoch: [39][111/233]	Loss 0.0319 (0.0389)	
training:	Epoch: [39][112/233]	Loss 0.0369 (0.0389)	
training:	Epoch: [39][113/233]	Loss 0.0427 (0.0389)	
training:	Epoch: [39][114/233]	Loss 0.0625 (0.0391)	
training:	Epoch: [39][115/233]	Loss 0.0322 (0.0390)	
training:	Epoch: [39][116/233]	Loss 0.0415 (0.0391)	
training:	Epoch: [39][117/233]	Loss 0.0277 (0.0390)	
training:	Epoch: [39][118/233]	Loss 0.0523 (0.0391)	
training:	Epoch: [39][119/233]	Loss 0.0442 (0.0391)	
training:	Epoch: [39][120/233]	Loss 0.0464 (0.0392)	
training:	Epoch: [39][121/233]	Loss 0.0328 (0.0391)	
training:	Epoch: [39][122/233]	Loss 0.0364 (0.0391)	
training:	Epoch: [39][123/233]	Loss 0.0440 (0.0391)	
training:	Epoch: [39][124/233]	Loss 0.0420 (0.0392)	
training:	Epoch: [39][125/233]	Loss 0.0650 (0.0394)	
training:	Epoch: [39][126/233]	Loss 0.0310 (0.0393)	
training:	Epoch: [39][127/233]	Loss 0.0363 (0.0393)	
training:	Epoch: [39][128/233]	Loss 0.0342 (0.0392)	
training:	Epoch: [39][129/233]	Loss 0.0311 (0.0392)	
training:	Epoch: [39][130/233]	Loss 0.0355 (0.0392)	
training:	Epoch: [39][131/233]	Loss 0.0373 (0.0391)	
training:	Epoch: [39][132/233]	Loss 0.0283 (0.0391)	
training:	Epoch: [39][133/233]	Loss 0.0337 (0.0390)	
training:	Epoch: [39][134/233]	Loss 0.0301 (0.0389)	
training:	Epoch: [39][135/233]	Loss 0.0275 (0.0389)	
training:	Epoch: [39][136/233]	Loss 0.0300 (0.0388)	
training:	Epoch: [39][137/233]	Loss 0.0375 (0.0388)	
training:	Epoch: [39][138/233]	Loss 0.0428 (0.0388)	
training:	Epoch: [39][139/233]	Loss 0.0425 (0.0388)	
training:	Epoch: [39][140/233]	Loss 0.0338 (0.0388)	
training:	Epoch: [39][141/233]	Loss 0.0413 (0.0388)	
training:	Epoch: [39][142/233]	Loss 0.0390 (0.0388)	
training:	Epoch: [39][143/233]	Loss 0.0611 (0.0390)	
training:	Epoch: [39][144/233]	Loss 0.0341 (0.0390)	
training:	Epoch: [39][145/233]	Loss 0.0318 (0.0389)	
training:	Epoch: [39][146/233]	Loss 0.0332 (0.0389)	
training:	Epoch: [39][147/233]	Loss 0.0543 (0.0390)	
training:	Epoch: [39][148/233]	Loss 0.0343 (0.0389)	
training:	Epoch: [39][149/233]	Loss 0.0327 (0.0389)	
training:	Epoch: [39][150/233]	Loss 0.0379 (0.0389)	
training:	Epoch: [39][151/233]	Loss 0.0372 (0.0389)	
training:	Epoch: [39][152/233]	Loss 0.0314 (0.0388)	
training:	Epoch: [39][153/233]	Loss 0.0336 (0.0388)	
training:	Epoch: [39][154/233]	Loss 0.0417 (0.0388)	
training:	Epoch: [39][155/233]	Loss 0.0426 (0.0388)	
training:	Epoch: [39][156/233]	Loss 0.0393 (0.0388)	
training:	Epoch: [39][157/233]	Loss 0.0315 (0.0388)	
training:	Epoch: [39][158/233]	Loss 0.0331 (0.0388)	
training:	Epoch: [39][159/233]	Loss 0.0317 (0.0387)	
training:	Epoch: [39][160/233]	Loss 0.0317 (0.0387)	
training:	Epoch: [39][161/233]	Loss 0.0435 (0.0387)	
training:	Epoch: [39][162/233]	Loss 0.0463 (0.0387)	
training:	Epoch: [39][163/233]	Loss 0.0322 (0.0387)	
training:	Epoch: [39][164/233]	Loss 0.0350 (0.0387)	
training:	Epoch: [39][165/233]	Loss 0.0341 (0.0387)	
training:	Epoch: [39][166/233]	Loss 0.0337 (0.0386)	
training:	Epoch: [39][167/233]	Loss 0.0512 (0.0387)	
training:	Epoch: [39][168/233]	Loss 0.0393 (0.0387)	
training:	Epoch: [39][169/233]	Loss 0.0523 (0.0388)	
training:	Epoch: [39][170/233]	Loss 0.0323 (0.0387)	
training:	Epoch: [39][171/233]	Loss 0.0397 (0.0388)	
training:	Epoch: [39][172/233]	Loss 0.0296 (0.0387)	
training:	Epoch: [39][173/233]	Loss 0.0354 (0.0387)	
training:	Epoch: [39][174/233]	Loss 0.0399 (0.0387)	
training:	Epoch: [39][175/233]	Loss 0.0378 (0.0387)	
training:	Epoch: [39][176/233]	Loss 0.0443 (0.0387)	
training:	Epoch: [39][177/233]	Loss 0.0317 (0.0387)	
training:	Epoch: [39][178/233]	Loss 0.0474 (0.0387)	
training:	Epoch: [39][179/233]	Loss 0.0602 (0.0388)	
training:	Epoch: [39][180/233]	Loss 0.0355 (0.0388)	
training:	Epoch: [39][181/233]	Loss 0.0305 (0.0388)	
training:	Epoch: [39][182/233]	Loss 0.0413 (0.0388)	
training:	Epoch: [39][183/233]	Loss 0.0388 (0.0388)	
training:	Epoch: [39][184/233]	Loss 0.0324 (0.0388)	
training:	Epoch: [39][185/233]	Loss 0.0306 (0.0387)	
training:	Epoch: [39][186/233]	Loss 0.0293 (0.0387)	
training:	Epoch: [39][187/233]	Loss 0.0517 (0.0387)	
training:	Epoch: [39][188/233]	Loss 0.0407 (0.0387)	
training:	Epoch: [39][189/233]	Loss 0.0332 (0.0387)	
training:	Epoch: [39][190/233]	Loss 0.0394 (0.0387)	
training:	Epoch: [39][191/233]	Loss 0.0346 (0.0387)	
training:	Epoch: [39][192/233]	Loss 0.0531 (0.0388)	
training:	Epoch: [39][193/233]	Loss 0.0372 (0.0388)	
training:	Epoch: [39][194/233]	Loss 0.0478 (0.0388)	
training:	Epoch: [39][195/233]	Loss 0.0365 (0.0388)	
training:	Epoch: [39][196/233]	Loss 0.0328 (0.0388)	
training:	Epoch: [39][197/233]	Loss 0.0385 (0.0388)	
training:	Epoch: [39][198/233]	Loss 0.0344 (0.0387)	
training:	Epoch: [39][199/233]	Loss 0.0332 (0.0387)	
training:	Epoch: [39][200/233]	Loss 0.0389 (0.0387)	
training:	Epoch: [39][201/233]	Loss 0.0406 (0.0387)	
training:	Epoch: [39][202/233]	Loss 0.0403 (0.0387)	
training:	Epoch: [39][203/233]	Loss 0.0370 (0.0387)	
training:	Epoch: [39][204/233]	Loss 0.0409 (0.0387)	
training:	Epoch: [39][205/233]	Loss 0.0411 (0.0387)	
training:	Epoch: [39][206/233]	Loss 0.0453 (0.0388)	
training:	Epoch: [39][207/233]	Loss 0.0732 (0.0389)	
training:	Epoch: [39][208/233]	Loss 0.0308 (0.0389)	
training:	Epoch: [39][209/233]	Loss 0.0307 (0.0389)	
training:	Epoch: [39][210/233]	Loss 0.0339 (0.0388)	
training:	Epoch: [39][211/233]	Loss 0.0357 (0.0388)	
training:	Epoch: [39][212/233]	Loss 0.0352 (0.0388)	
training:	Epoch: [39][213/233]	Loss 0.0363 (0.0388)	
training:	Epoch: [39][214/233]	Loss 0.0597 (0.0389)	
training:	Epoch: [39][215/233]	Loss 0.0323 (0.0389)	
training:	Epoch: [39][216/233]	Loss 0.0629 (0.0390)	
training:	Epoch: [39][217/233]	Loss 0.0351 (0.0390)	
training:	Epoch: [39][218/233]	Loss 0.0927 (0.0392)	
training:	Epoch: [39][219/233]	Loss 0.0401 (0.0392)	
training:	Epoch: [39][220/233]	Loss 0.0424 (0.0392)	
training:	Epoch: [39][221/233]	Loss 0.0358 (0.0392)	
training:	Epoch: [39][222/233]	Loss 0.0413 (0.0392)	
training:	Epoch: [39][223/233]	Loss 0.0299 (0.0392)	
training:	Epoch: [39][224/233]	Loss 0.0305 (0.0391)	
training:	Epoch: [39][225/233]	Loss 0.0325 (0.0391)	
training:	Epoch: [39][226/233]	Loss 0.0414 (0.0391)	
training:	Epoch: [39][227/233]	Loss 0.0317 (0.0391)	
training:	Epoch: [39][228/233]	Loss 0.0352 (0.0391)	
training:	Epoch: [39][229/233]	Loss 0.0349 (0.0391)	
training:	Epoch: [39][230/233]	Loss 0.0305 (0.0390)	
training:	Epoch: [39][231/233]	Loss 0.0349 (0.0390)	
training:	Epoch: [39][232/233]	Loss 0.0423 (0.0390)	
training:	Epoch: [39][233/233]	Loss 0.0287 (0.0390)	
Training:	 Loss: 0.0389

Training:	 ACC: 1.0000 1.0000 1.0000 1.0000
Validation:	 ACC: 0.8656 0.8652 0.8557 0.8756
Validation:	 Best_BACC: 0.8671 0.8662 0.8485 0.8857
Validation:	 Loss: 0.3159
Pretraining:	Epoch 40/200
----------
training:	Epoch: [40][1/233]	Loss 0.0298 (0.0298)	
training:	Epoch: [40][2/233]	Loss 0.0338 (0.0318)	
training:	Epoch: [40][3/233]	Loss 0.0323 (0.0320)	
training:	Epoch: [40][4/233]	Loss 0.0309 (0.0317)	
training:	Epoch: [40][5/233]	Loss 0.0444 (0.0342)	
training:	Epoch: [40][6/233]	Loss 0.0362 (0.0346)	
training:	Epoch: [40][7/233]	Loss 0.0270 (0.0335)	
training:	Epoch: [40][8/233]	Loss 0.0347 (0.0336)	
training:	Epoch: [40][9/233]	Loss 0.0386 (0.0342)	
training:	Epoch: [40][10/233]	Loss 0.0274 (0.0335)	
training:	Epoch: [40][11/233]	Loss 0.0350 (0.0336)	
training:	Epoch: [40][12/233]	Loss 0.0314 (0.0334)	
training:	Epoch: [40][13/233]	Loss 0.0515 (0.0348)	
training:	Epoch: [40][14/233]	Loss 0.0288 (0.0344)	
training:	Epoch: [40][15/233]	Loss 0.0291 (0.0340)	
training:	Epoch: [40][16/233]	Loss 0.0318 (0.0339)	
training:	Epoch: [40][17/233]	Loss 0.0332 (0.0339)	
training:	Epoch: [40][18/233]	Loss 0.0317 (0.0337)	
training:	Epoch: [40][19/233]	Loss 0.0333 (0.0337)	
training:	Epoch: [40][20/233]	Loss 0.0340 (0.0337)	
training:	Epoch: [40][21/233]	Loss 0.0396 (0.0340)	
training:	Epoch: [40][22/233]	Loss 0.0387 (0.0342)	
training:	Epoch: [40][23/233]	Loss 0.0308 (0.0341)	
training:	Epoch: [40][24/233]	Loss 0.0431 (0.0344)	
training:	Epoch: [40][25/233]	Loss 0.0297 (0.0343)	
training:	Epoch: [40][26/233]	Loss 0.0299 (0.0341)	
training:	Epoch: [40][27/233]	Loss 0.0299 (0.0339)	
training:	Epoch: [40][28/233]	Loss 0.0331 (0.0339)	
training:	Epoch: [40][29/233]	Loss 0.0356 (0.0340)	
training:	Epoch: [40][30/233]	Loss 0.0554 (0.0347)	
training:	Epoch: [40][31/233]	Loss 0.0262 (0.0344)	
training:	Epoch: [40][32/233]	Loss 0.0352 (0.0344)	
training:	Epoch: [40][33/233]	Loss 0.0341 (0.0344)	
training:	Epoch: [40][34/233]	Loss 0.0329 (0.0344)	
training:	Epoch: [40][35/233]	Loss 0.0317 (0.0343)	
training:	Epoch: [40][36/233]	Loss 0.0321 (0.0342)	
training:	Epoch: [40][37/233]	Loss 0.0347 (0.0343)	
training:	Epoch: [40][38/233]	Loss 0.0258 (0.0340)	
training:	Epoch: [40][39/233]	Loss 0.0553 (0.0346)	
training:	Epoch: [40][40/233]	Loss 0.0267 (0.0344)	
training:	Epoch: [40][41/233]	Loss 0.0300 (0.0343)	
training:	Epoch: [40][42/233]	Loss 0.0330 (0.0342)	
training:	Epoch: [40][43/233]	Loss 0.0511 (0.0346)	
training:	Epoch: [40][44/233]	Loss 0.0315 (0.0346)	
training:	Epoch: [40][45/233]	Loss 0.0290 (0.0344)	
training:	Epoch: [40][46/233]	Loss 0.0326 (0.0344)	
training:	Epoch: [40][47/233]	Loss 0.0291 (0.0343)	
training:	Epoch: [40][48/233]	Loss 0.0378 (0.0344)	
training:	Epoch: [40][49/233]	Loss 0.0334 (0.0343)	
training:	Epoch: [40][50/233]	Loss 0.0375 (0.0344)	
training:	Epoch: [40][51/233]	Loss 0.0402 (0.0345)	
training:	Epoch: [40][52/233]	Loss 0.0308 (0.0344)	
training:	Epoch: [40][53/233]	Loss 0.0316 (0.0344)	
training:	Epoch: [40][54/233]	Loss 0.0380 (0.0345)	
training:	Epoch: [40][55/233]	Loss 0.0328 (0.0344)	
training:	Epoch: [40][56/233]	Loss 0.0315 (0.0344)	
training:	Epoch: [40][57/233]	Loss 0.0319 (0.0343)	
training:	Epoch: [40][58/233]	Loss 0.0280 (0.0342)	
training:	Epoch: [40][59/233]	Loss 0.0381 (0.0343)	
training:	Epoch: [40][60/233]	Loss 0.0368 (0.0343)	
training:	Epoch: [40][61/233]	Loss 0.0303 (0.0343)	
training:	Epoch: [40][62/233]	Loss 0.0295 (0.0342)	
training:	Epoch: [40][63/233]	Loss 0.0302 (0.0341)	
training:	Epoch: [40][64/233]	Loss 0.0351 (0.0341)	
training:	Epoch: [40][65/233]	Loss 0.0334 (0.0341)	
training:	Epoch: [40][66/233]	Loss 0.0407 (0.0342)	
training:	Epoch: [40][67/233]	Loss 0.0282 (0.0341)	
training:	Epoch: [40][68/233]	Loss 0.0677 (0.0346)	
training:	Epoch: [40][69/233]	Loss 0.0297 (0.0346)	
training:	Epoch: [40][70/233]	Loss 0.0370 (0.0346)	
training:	Epoch: [40][71/233]	Loss 0.0381 (0.0346)	
training:	Epoch: [40][72/233]	Loss 0.0500 (0.0349)	
training:	Epoch: [40][73/233]	Loss 0.0310 (0.0348)	
training:	Epoch: [40][74/233]	Loss 0.0286 (0.0347)	
training:	Epoch: [40][75/233]	Loss 0.0647 (0.0351)	
training:	Epoch: [40][76/233]	Loss 0.0298 (0.0351)	
training:	Epoch: [40][77/233]	Loss 0.0304 (0.0350)	
training:	Epoch: [40][78/233]	Loss 0.0278 (0.0349)	
training:	Epoch: [40][79/233]	Loss 0.0325 (0.0349)	
training:	Epoch: [40][80/233]	Loss 0.0374 (0.0349)	
training:	Epoch: [40][81/233]	Loss 0.0449 (0.0350)	
training:	Epoch: [40][82/233]	Loss 0.0544 (0.0353)	
training:	Epoch: [40][83/233]	Loss 0.0405 (0.0353)	
training:	Epoch: [40][84/233]	Loss 0.0320 (0.0353)	
training:	Epoch: [40][85/233]	Loss 0.0445 (0.0354)	
training:	Epoch: [40][86/233]	Loss 0.0289 (0.0353)	
training:	Epoch: [40][87/233]	Loss 0.0295 (0.0352)	
training:	Epoch: [40][88/233]	Loss 0.0513 (0.0354)	
training:	Epoch: [40][89/233]	Loss 0.0306 (0.0354)	
training:	Epoch: [40][90/233]	Loss 0.0331 (0.0354)	
training:	Epoch: [40][91/233]	Loss 0.0279 (0.0353)	
training:	Epoch: [40][92/233]	Loss 0.0390 (0.0353)	
training:	Epoch: [40][93/233]	Loss 0.0262 (0.0352)	
training:	Epoch: [40][94/233]	Loss 0.0325 (0.0352)	
training:	Epoch: [40][95/233]	Loss 0.0298 (0.0351)	
training:	Epoch: [40][96/233]	Loss 0.0292 (0.0351)	
training:	Epoch: [40][97/233]	Loss 0.0347 (0.0351)	
training:	Epoch: [40][98/233]	Loss 0.0302 (0.0350)	
training:	Epoch: [40][99/233]	Loss 0.0396 (0.0351)	
training:	Epoch: [40][100/233]	Loss 0.0315 (0.0350)	
training:	Epoch: [40][101/233]	Loss 0.0450 (0.0351)	
training:	Epoch: [40][102/233]	Loss 0.0340 (0.0351)	
training:	Epoch: [40][103/233]	Loss 0.0295 (0.0351)	
training:	Epoch: [40][104/233]	Loss 0.0334 (0.0350)	
training:	Epoch: [40][105/233]	Loss 0.0326 (0.0350)	
training:	Epoch: [40][106/233]	Loss 0.0336 (0.0350)	
training:	Epoch: [40][107/233]	Loss 0.0402 (0.0351)	
training:	Epoch: [40][108/233]	Loss 0.0424 (0.0351)	
training:	Epoch: [40][109/233]	Loss 0.0287 (0.0351)	
training:	Epoch: [40][110/233]	Loss 0.0405 (0.0351)	
training:	Epoch: [40][111/233]	Loss 0.0404 (0.0352)	
training:	Epoch: [40][112/233]	Loss 0.0438 (0.0352)	
training:	Epoch: [40][113/233]	Loss 0.0334 (0.0352)	
training:	Epoch: [40][114/233]	Loss 0.0323 (0.0352)	
training:	Epoch: [40][115/233]	Loss 0.0280 (0.0351)	
training:	Epoch: [40][116/233]	Loss 0.0396 (0.0352)	
training:	Epoch: [40][117/233]	Loss 0.0357 (0.0352)	
training:	Epoch: [40][118/233]	Loss 0.0402 (0.0352)	
training:	Epoch: [40][119/233]	Loss 0.0282 (0.0352)	
training:	Epoch: [40][120/233]	Loss 0.0466 (0.0353)	
training:	Epoch: [40][121/233]	Loss 0.0417 (0.0353)	
training:	Epoch: [40][122/233]	Loss 0.0349 (0.0353)	
training:	Epoch: [40][123/233]	Loss 0.0319 (0.0353)	
training:	Epoch: [40][124/233]	Loss 0.0433 (0.0353)	
training:	Epoch: [40][125/233]	Loss 0.0305 (0.0353)	
training:	Epoch: [40][126/233]	Loss 0.0375 (0.0353)	
training:	Epoch: [40][127/233]	Loss 0.0452 (0.0354)	
training:	Epoch: [40][128/233]	Loss 0.0328 (0.0354)	
training:	Epoch: [40][129/233]	Loss 0.0305 (0.0353)	
training:	Epoch: [40][130/233]	Loss 0.0580 (0.0355)	
training:	Epoch: [40][131/233]	Loss 0.0407 (0.0356)	
training:	Epoch: [40][132/233]	Loss 0.0309 (0.0355)	
training:	Epoch: [40][133/233]	Loss 0.0383 (0.0355)	
training:	Epoch: [40][134/233]	Loss 0.0431 (0.0356)	
training:	Epoch: [40][135/233]	Loss 0.0324 (0.0356)	
training:	Epoch: [40][136/233]	Loss 0.0332 (0.0356)	
training:	Epoch: [40][137/233]	Loss 0.0335 (0.0355)	
training:	Epoch: [40][138/233]	Loss 0.0387 (0.0356)	
training:	Epoch: [40][139/233]	Loss 0.0437 (0.0356)	
training:	Epoch: [40][140/233]	Loss 0.0471 (0.0357)	
training:	Epoch: [40][141/233]	Loss 0.0351 (0.0357)	
training:	Epoch: [40][142/233]	Loss 0.0298 (0.0357)	
training:	Epoch: [40][143/233]	Loss 0.0358 (0.0357)	
training:	Epoch: [40][144/233]	Loss 0.0328 (0.0356)	
training:	Epoch: [40][145/233]	Loss 0.0361 (0.0356)	
training:	Epoch: [40][146/233]	Loss 0.0325 (0.0356)	
training:	Epoch: [40][147/233]	Loss 0.0342 (0.0356)	
training:	Epoch: [40][148/233]	Loss 0.0355 (0.0356)	
training:	Epoch: [40][149/233]	Loss 0.0411 (0.0356)	
training:	Epoch: [40][150/233]	Loss 0.0302 (0.0356)	
training:	Epoch: [40][151/233]	Loss 0.0318 (0.0356)	
training:	Epoch: [40][152/233]	Loss 0.0364 (0.0356)	
training:	Epoch: [40][153/233]	Loss 0.0364 (0.0356)	
training:	Epoch: [40][154/233]	Loss 0.0279 (0.0355)	
training:	Epoch: [40][155/233]	Loss 0.0397 (0.0356)	
training:	Epoch: [40][156/233]	Loss 0.0307 (0.0355)	
training:	Epoch: [40][157/233]	Loss 0.0301 (0.0355)	
training:	Epoch: [40][158/233]	Loss 0.0344 (0.0355)	
training:	Epoch: [40][159/233]	Loss 0.0346 (0.0355)	
training:	Epoch: [40][160/233]	Loss 0.0338 (0.0355)	
training:	Epoch: [40][161/233]	Loss 0.0289 (0.0354)	
training:	Epoch: [40][162/233]	Loss 0.0308 (0.0354)	
training:	Epoch: [40][163/233]	Loss 0.0327 (0.0354)	
training:	Epoch: [40][164/233]	Loss 0.0313 (0.0354)	
training:	Epoch: [40][165/233]	Loss 0.0642 (0.0355)	
training:	Epoch: [40][166/233]	Loss 0.0348 (0.0355)	
training:	Epoch: [40][167/233]	Loss 0.0304 (0.0355)	
training:	Epoch: [40][168/233]	Loss 0.0430 (0.0356)	
training:	Epoch: [40][169/233]	Loss 0.0338 (0.0355)	
training:	Epoch: [40][170/233]	Loss 0.0320 (0.0355)	
training:	Epoch: [40][171/233]	Loss 0.0383 (0.0355)	
training:	Epoch: [40][172/233]	Loss 0.0374 (0.0356)	
training:	Epoch: [40][173/233]	Loss 0.0270 (0.0355)	
training:	Epoch: [40][174/233]	Loss 0.0461 (0.0356)	
training:	Epoch: [40][175/233]	Loss 0.0277 (0.0355)	
training:	Epoch: [40][176/233]	Loss 0.0346 (0.0355)	
training:	Epoch: [40][177/233]	Loss 0.0294 (0.0355)	
training:	Epoch: [40][178/233]	Loss 0.0307 (0.0355)	
training:	Epoch: [40][179/233]	Loss 0.0299 (0.0354)	
training:	Epoch: [40][180/233]	Loss 0.0389 (0.0354)	
training:	Epoch: [40][181/233]	Loss 0.0329 (0.0354)	
training:	Epoch: [40][182/233]	Loss 0.0292 (0.0354)	
training:	Epoch: [40][183/233]	Loss 0.0359 (0.0354)	
training:	Epoch: [40][184/233]	Loss 0.0316 (0.0354)	
training:	Epoch: [40][185/233]	Loss 0.0607 (0.0355)	
training:	Epoch: [40][186/233]	Loss 0.0330 (0.0355)	
training:	Epoch: [40][187/233]	Loss 0.0405 (0.0355)	
training:	Epoch: [40][188/233]	Loss 0.0375 (0.0355)	
training:	Epoch: [40][189/233]	Loss 0.0371 (0.0355)	
training:	Epoch: [40][190/233]	Loss 0.0329 (0.0355)	
training:	Epoch: [40][191/233]	Loss 0.0398 (0.0356)	
training:	Epoch: [40][192/233]	Loss 0.0319 (0.0355)	
training:	Epoch: [40][193/233]	Loss 0.0312 (0.0355)	
training:	Epoch: [40][194/233]	Loss 0.0257 (0.0355)	
training:	Epoch: [40][195/233]	Loss 0.0333 (0.0354)	
training:	Epoch: [40][196/233]	Loss 0.0338 (0.0354)	
training:	Epoch: [40][197/233]	Loss 0.0378 (0.0355)	
training:	Epoch: [40][198/233]	Loss 0.0296 (0.0354)	
training:	Epoch: [40][199/233]	Loss 0.0343 (0.0354)	
training:	Epoch: [40][200/233]	Loss 0.0443 (0.0355)	
training:	Epoch: [40][201/233]	Loss 0.0332 (0.0355)	
training:	Epoch: [40][202/233]	Loss 0.0413 (0.0355)	
training:	Epoch: [40][203/233]	Loss 0.0428 (0.0355)	
training:	Epoch: [40][204/233]	Loss 0.0334 (0.0355)	
training:	Epoch: [40][205/233]	Loss 0.0323 (0.0355)	
training:	Epoch: [40][206/233]	Loss 0.0391 (0.0355)	
training:	Epoch: [40][207/233]	Loss 0.0366 (0.0355)	
training:	Epoch: [40][208/233]	Loss 0.0314 (0.0355)	
training:	Epoch: [40][209/233]	Loss 0.0260 (0.0354)	
training:	Epoch: [40][210/233]	Loss 0.0295 (0.0354)	
training:	Epoch: [40][211/233]	Loss 0.0280 (0.0354)	
training:	Epoch: [40][212/233]	Loss 0.0367 (0.0354)	
training:	Epoch: [40][213/233]	Loss 0.0707 (0.0356)	
training:	Epoch: [40][214/233]	Loss 0.0344 (0.0355)	
training:	Epoch: [40][215/233]	Loss 0.0406 (0.0356)	
training:	Epoch: [40][216/233]	Loss 0.0452 (0.0356)	
training:	Epoch: [40][217/233]	Loss 0.0299 (0.0356)	
training:	Epoch: [40][218/233]	Loss 0.0337 (0.0356)	
training:	Epoch: [40][219/233]	Loss 0.0348 (0.0356)	
training:	Epoch: [40][220/233]	Loss 0.0446 (0.0356)	
training:	Epoch: [40][221/233]	Loss 0.0549 (0.0357)	
training:	Epoch: [40][222/233]	Loss 0.0349 (0.0357)	
training:	Epoch: [40][223/233]	Loss 0.0347 (0.0357)	
training:	Epoch: [40][224/233]	Loss 0.0499 (0.0358)	
training:	Epoch: [40][225/233]	Loss 0.0497 (0.0358)	
training:	Epoch: [40][226/233]	Loss 0.0339 (0.0358)	
training:	Epoch: [40][227/233]	Loss 0.0293 (0.0358)	
training:	Epoch: [40][228/233]	Loss 0.0389 (0.0358)	
training:	Epoch: [40][229/233]	Loss 0.0301 (0.0358)	
training:	Epoch: [40][230/233]	Loss 0.0305 (0.0358)	
training:	Epoch: [40][231/233]	Loss 0.0369 (0.0358)	
training:	Epoch: [40][232/233]	Loss 0.0303 (0.0357)	
training:	Epoch: [40][233/233]	Loss 0.0401 (0.0358)	
Training:	 Loss: 0.0357

Training:	 ACC: 1.0000 1.0000 1.0000 1.0000
Validation:	 ACC: 0.8652 0.8652 0.8639 0.8666
Validation:	 Best_BACC: 0.8671 0.8662 0.8485 0.8857
Validation:	 Loss: 0.3155
Pretraining:	Epoch 41/200
----------
training:	Epoch: [41][1/233]	Loss 0.0314 (0.0314)	
training:	Epoch: [41][2/233]	Loss 0.0322 (0.0318)	
training:	Epoch: [41][3/233]	Loss 0.0305 (0.0314)	
training:	Epoch: [41][4/233]	Loss 0.0607 (0.0387)	
training:	Epoch: [41][5/233]	Loss 0.0376 (0.0385)	
training:	Epoch: [41][6/233]	Loss 0.0414 (0.0390)	
training:	Epoch: [41][7/233]	Loss 0.0269 (0.0372)	
training:	Epoch: [41][8/233]	Loss 0.0268 (0.0359)	
training:	Epoch: [41][9/233]	Loss 0.0358 (0.0359)	
training:	Epoch: [41][10/233]	Loss 0.0317 (0.0355)	
training:	Epoch: [41][11/233]	Loss 0.0351 (0.0355)	
training:	Epoch: [41][12/233]	Loss 0.0330 (0.0353)	
training:	Epoch: [41][13/233]	Loss 0.0277 (0.0347)	
training:	Epoch: [41][14/233]	Loss 0.0370 (0.0349)	
training:	Epoch: [41][15/233]	Loss 0.0330 (0.0347)	
training:	Epoch: [41][16/233]	Loss 0.0601 (0.0363)	
training:	Epoch: [41][17/233]	Loss 0.0298 (0.0359)	
training:	Epoch: [41][18/233]	Loss 0.0363 (0.0360)	
training:	Epoch: [41][19/233]	Loss 0.0459 (0.0365)	
training:	Epoch: [41][20/233]	Loss 0.0289 (0.0361)	
training:	Epoch: [41][21/233]	Loss 0.0324 (0.0359)	
training:	Epoch: [41][22/233]	Loss 0.0299 (0.0356)	
training:	Epoch: [41][23/233]	Loss 0.0273 (0.0353)	
training:	Epoch: [41][24/233]	Loss 0.0373 (0.0354)	
training:	Epoch: [41][25/233]	Loss 0.0409 (0.0356)	
training:	Epoch: [41][26/233]	Loss 0.0326 (0.0355)	
training:	Epoch: [41][27/233]	Loss 0.0490 (0.0360)	
training:	Epoch: [41][28/233]	Loss 0.0349 (0.0359)	
training:	Epoch: [41][29/233]	Loss 0.0328 (0.0358)	
training:	Epoch: [41][30/233]	Loss 0.0452 (0.0361)	
training:	Epoch: [41][31/233]	Loss 0.0292 (0.0359)	
training:	Epoch: [41][32/233]	Loss 0.0537 (0.0365)	
training:	Epoch: [41][33/233]	Loss 0.0300 (0.0363)	
training:	Epoch: [41][34/233]	Loss 0.0353 (0.0362)	
training:	Epoch: [41][35/233]	Loss 0.0372 (0.0363)	
training:	Epoch: [41][36/233]	Loss 0.0364 (0.0363)	
training:	Epoch: [41][37/233]	Loss 0.0323 (0.0362)	
training:	Epoch: [41][38/233]	Loss 0.0437 (0.0364)	
training:	Epoch: [41][39/233]	Loss 0.0306 (0.0362)	
training:	Epoch: [41][40/233]	Loss 0.0320 (0.0361)	
training:	Epoch: [41][41/233]	Loss 0.0234 (0.0358)	
training:	Epoch: [41][42/233]	Loss 0.0295 (0.0357)	
training:	Epoch: [41][43/233]	Loss 0.0272 (0.0355)	
training:	Epoch: [41][44/233]	Loss 0.0435 (0.0356)	
training:	Epoch: [41][45/233]	Loss 0.0361 (0.0357)	
training:	Epoch: [41][46/233]	Loss 0.0319 (0.0356)	
training:	Epoch: [41][47/233]	Loss 0.0314 (0.0355)	
training:	Epoch: [41][48/233]	Loss 0.0413 (0.0356)	
training:	Epoch: [41][49/233]	Loss 0.0488 (0.0359)	
training:	Epoch: [41][50/233]	Loss 0.0261 (0.0357)	
training:	Epoch: [41][51/233]	Loss 0.0414 (0.0358)	
training:	Epoch: [41][52/233]	Loss 0.0335 (0.0357)	
training:	Epoch: [41][53/233]	Loss 0.0360 (0.0358)	
training:	Epoch: [41][54/233]	Loss 0.0277 (0.0356)	
training:	Epoch: [41][55/233]	Loss 0.0316 (0.0355)	
training:	Epoch: [41][56/233]	Loss 0.0399 (0.0356)	
training:	Epoch: [41][57/233]	Loss 0.0402 (0.0357)	
training:	Epoch: [41][58/233]	Loss 0.0276 (0.0356)	
training:	Epoch: [41][59/233]	Loss 0.0319 (0.0355)	
training:	Epoch: [41][60/233]	Loss 0.0318 (0.0354)	
training:	Epoch: [41][61/233]	Loss 0.0404 (0.0355)	
training:	Epoch: [41][62/233]	Loss 0.0332 (0.0355)	
training:	Epoch: [41][63/233]	Loss 0.0293 (0.0354)	
training:	Epoch: [41][64/233]	Loss 0.0339 (0.0354)	
training:	Epoch: [41][65/233]	Loss 0.0276 (0.0352)	
training:	Epoch: [41][66/233]	Loss 0.0316 (0.0352)	
training:	Epoch: [41][67/233]	Loss 0.0346 (0.0352)	
training:	Epoch: [41][68/233]	Loss 0.0298 (0.0351)	
training:	Epoch: [41][69/233]	Loss 0.0284 (0.0350)	
training:	Epoch: [41][70/233]	Loss 0.0268 (0.0349)	
training:	Epoch: [41][71/233]	Loss 0.0306 (0.0348)	
training:	Epoch: [41][72/233]	Loss 0.0270 (0.0347)	
training:	Epoch: [41][73/233]	Loss 0.0378 (0.0347)	
training:	Epoch: [41][74/233]	Loss 0.0309 (0.0347)	
training:	Epoch: [41][75/233]	Loss 0.0391 (0.0348)	
training:	Epoch: [41][76/233]	Loss 0.0608 (0.0351)	
training:	Epoch: [41][77/233]	Loss 0.0312 (0.0350)	
training:	Epoch: [41][78/233]	Loss 0.0360 (0.0351)	
training:	Epoch: [41][79/233]	Loss 0.0272 (0.0350)	
training:	Epoch: [41][80/233]	Loss 0.0331 (0.0349)	
training:	Epoch: [41][81/233]	Loss 0.0283 (0.0349)	
training:	Epoch: [41][82/233]	Loss 0.0296 (0.0348)	
training:	Epoch: [41][83/233]	Loss 0.0273 (0.0347)	
training:	Epoch: [41][84/233]	Loss 0.0316 (0.0347)	
training:	Epoch: [41][85/233]	Loss 0.0298 (0.0346)	
training:	Epoch: [41][86/233]	Loss 0.0316 (0.0346)	
training:	Epoch: [41][87/233]	Loss 0.0297 (0.0345)	
training:	Epoch: [41][88/233]	Loss 0.0352 (0.0345)	
training:	Epoch: [41][89/233]	Loss 0.0289 (0.0345)	
training:	Epoch: [41][90/233]	Loss 0.0285 (0.0344)	
training:	Epoch: [41][91/233]	Loss 0.0369 (0.0344)	
training:	Epoch: [41][92/233]	Loss 0.0362 (0.0344)	
training:	Epoch: [41][93/233]	Loss 0.0291 (0.0344)	
training:	Epoch: [41][94/233]	Loss 0.0296 (0.0343)	
training:	Epoch: [41][95/233]	Loss 0.0376 (0.0344)	
training:	Epoch: [41][96/233]	Loss 0.0402 (0.0344)	
training:	Epoch: [41][97/233]	Loss 0.0329 (0.0344)	
training:	Epoch: [41][98/233]	Loss 0.0353 (0.0344)	
training:	Epoch: [41][99/233]	Loss 0.0384 (0.0345)	
training:	Epoch: [41][100/233]	Loss 0.0286 (0.0344)	
training:	Epoch: [41][101/233]	Loss 0.0298 (0.0344)	
training:	Epoch: [41][102/233]	Loss 0.0400 (0.0344)	
training:	Epoch: [41][103/233]	Loss 0.0432 (0.0345)	
training:	Epoch: [41][104/233]	Loss 0.0286 (0.0344)	
training:	Epoch: [41][105/233]	Loss 0.0306 (0.0344)	
training:	Epoch: [41][106/233]	Loss 0.0578 (0.0346)	
training:	Epoch: [41][107/233]	Loss 0.0301 (0.0346)	
training:	Epoch: [41][108/233]	Loss 0.0474 (0.0347)	
training:	Epoch: [41][109/233]	Loss 0.0275 (0.0346)	
training:	Epoch: [41][110/233]	Loss 0.0392 (0.0347)	
training:	Epoch: [41][111/233]	Loss 0.0372 (0.0347)	
training:	Epoch: [41][112/233]	Loss 0.0439 (0.0348)	
training:	Epoch: [41][113/233]	Loss 0.0384 (0.0348)	
training:	Epoch: [41][114/233]	Loss 0.0344 (0.0348)	
training:	Epoch: [41][115/233]	Loss 0.0374 (0.0348)	
training:	Epoch: [41][116/233]	Loss 0.0578 (0.0350)	
training:	Epoch: [41][117/233]	Loss 0.0312 (0.0350)	
training:	Epoch: [41][118/233]	Loss 0.0338 (0.0350)	
training:	Epoch: [41][119/233]	Loss 0.0307 (0.0349)	
training:	Epoch: [41][120/233]	Loss 0.0361 (0.0350)	
training:	Epoch: [41][121/233]	Loss 0.0610 (0.0352)	
training:	Epoch: [41][122/233]	Loss 0.0305 (0.0351)	
training:	Epoch: [41][123/233]	Loss 0.0289 (0.0351)	
training:	Epoch: [41][124/233]	Loss 0.0303 (0.0350)	
training:	Epoch: [41][125/233]	Loss 0.0311 (0.0350)	
training:	Epoch: [41][126/233]	Loss 0.0315 (0.0350)	
training:	Epoch: [41][127/233]	Loss 0.0275 (0.0349)	
training:	Epoch: [41][128/233]	Loss 0.0392 (0.0350)	
training:	Epoch: [41][129/233]	Loss 0.0780 (0.0353)	
training:	Epoch: [41][130/233]	Loss 0.0414 (0.0353)	
training:	Epoch: [41][131/233]	Loss 0.0305 (0.0353)	
training:	Epoch: [41][132/233]	Loss 0.0409 (0.0353)	
training:	Epoch: [41][133/233]	Loss 0.0477 (0.0354)	
training:	Epoch: [41][134/233]	Loss 0.0329 (0.0354)	
training:	Epoch: [41][135/233]	Loss 0.0519 (0.0355)	
training:	Epoch: [41][136/233]	Loss 0.0273 (0.0355)	
training:	Epoch: [41][137/233]	Loss 0.0294 (0.0354)	
training:	Epoch: [41][138/233]	Loss 0.0289 (0.0354)	
training:	Epoch: [41][139/233]	Loss 0.0397 (0.0354)	
training:	Epoch: [41][140/233]	Loss 0.0413 (0.0355)	
training:	Epoch: [41][141/233]	Loss 0.0354 (0.0355)	
training:	Epoch: [41][142/233]	Loss 0.0301 (0.0354)	
training:	Epoch: [41][143/233]	Loss 0.0416 (0.0355)	
training:	Epoch: [41][144/233]	Loss 0.0409 (0.0355)	
training:	Epoch: [41][145/233]	Loss 0.0309 (0.0355)	
training:	Epoch: [41][146/233]	Loss 0.1234 (0.0361)	
training:	Epoch: [41][147/233]	Loss 0.0286 (0.0360)	
training:	Epoch: [41][148/233]	Loss 0.0406 (0.0361)	
training:	Epoch: [41][149/233]	Loss 0.0317 (0.0360)	
training:	Epoch: [41][150/233]	Loss 0.0417 (0.0361)	
training:	Epoch: [41][151/233]	Loss 0.0520 (0.0362)	
training:	Epoch: [41][152/233]	Loss 0.0291 (0.0361)	
training:	Epoch: [41][153/233]	Loss 0.0364 (0.0361)	
training:	Epoch: [41][154/233]	Loss 0.0327 (0.0361)	
training:	Epoch: [41][155/233]	Loss 0.0363 (0.0361)	
training:	Epoch: [41][156/233]	Loss 0.0280 (0.0361)	
training:	Epoch: [41][157/233]	Loss 0.0426 (0.0361)	
training:	Epoch: [41][158/233]	Loss 0.0317 (0.0361)	
training:	Epoch: [41][159/233]	Loss 0.0279 (0.0360)	
training:	Epoch: [41][160/233]	Loss 0.0283 (0.0360)	
training:	Epoch: [41][161/233]	Loss 0.0435 (0.0360)	
training:	Epoch: [41][162/233]	Loss 0.0545 (0.0361)	
training:	Epoch: [41][163/233]	Loss 0.0448 (0.0362)	
training:	Epoch: [41][164/233]	Loss 0.0304 (0.0361)	
training:	Epoch: [41][165/233]	Loss 0.0342 (0.0361)	
training:	Epoch: [41][166/233]	Loss 0.0352 (0.0361)	
training:	Epoch: [41][167/233]	Loss 0.0300 (0.0361)	
training:	Epoch: [41][168/233]	Loss 0.0309 (0.0361)	
training:	Epoch: [41][169/233]	Loss 0.0307 (0.0360)	
training:	Epoch: [41][170/233]	Loss 0.0486 (0.0361)	
training:	Epoch: [41][171/233]	Loss 0.0364 (0.0361)	
training:	Epoch: [41][172/233]	Loss 0.0334 (0.0361)	
training:	Epoch: [41][173/233]	Loss 0.1228 (0.0366)	
training:	Epoch: [41][174/233]	Loss 0.0303 (0.0366)	
training:	Epoch: [41][175/233]	Loss 0.0285 (0.0365)	
training:	Epoch: [41][176/233]	Loss 0.0325 (0.0365)	
training:	Epoch: [41][177/233]	Loss 0.0384 (0.0365)	
training:	Epoch: [41][178/233]	Loss 0.0298 (0.0365)	
training:	Epoch: [41][179/233]	Loss 0.0316 (0.0364)	
training:	Epoch: [41][180/233]	Loss 0.0325 (0.0364)	
training:	Epoch: [41][181/233]	Loss 0.0331 (0.0364)	
training:	Epoch: [41][182/233]	Loss 0.0299 (0.0364)	
training:	Epoch: [41][183/233]	Loss 0.0344 (0.0363)	
training:	Epoch: [41][184/233]	Loss 0.0493 (0.0364)	
training:	Epoch: [41][185/233]	Loss 0.0668 (0.0366)	
training:	Epoch: [41][186/233]	Loss 0.0350 (0.0366)	
training:	Epoch: [41][187/233]	Loss 0.0294 (0.0365)	
training:	Epoch: [41][188/233]	Loss 0.0424 (0.0366)	
training:	Epoch: [41][189/233]	Loss 0.0311 (0.0365)	
training:	Epoch: [41][190/233]	Loss 0.0288 (0.0365)	
training:	Epoch: [41][191/233]	Loss 0.0319 (0.0365)	
training:	Epoch: [41][192/233]	Loss 0.0267 (0.0364)	
training:	Epoch: [41][193/233]	Loss 0.0325 (0.0364)	
training:	Epoch: [41][194/233]	Loss 0.0318 (0.0364)	
training:	Epoch: [41][195/233]	Loss 0.0271 (0.0363)	
training:	Epoch: [41][196/233]	Loss 0.0571 (0.0364)	
training:	Epoch: [41][197/233]	Loss 0.0477 (0.0365)	
training:	Epoch: [41][198/233]	Loss 0.0381 (0.0365)	
training:	Epoch: [41][199/233]	Loss 0.0312 (0.0365)	
training:	Epoch: [41][200/233]	Loss 0.0293 (0.0364)	
training:	Epoch: [41][201/233]	Loss 0.0369 (0.0364)	
training:	Epoch: [41][202/233]	Loss 0.0292 (0.0364)	
training:	Epoch: [41][203/233]	Loss 0.0273 (0.0364)	
training:	Epoch: [41][204/233]	Loss 0.0282 (0.0363)	
training:	Epoch: [41][205/233]	Loss 0.0498 (0.0364)	
training:	Epoch: [41][206/233]	Loss 0.0314 (0.0364)	
training:	Epoch: [41][207/233]	Loss 0.0348 (0.0364)	
training:	Epoch: [41][208/233]	Loss 0.0334 (0.0363)	
training:	Epoch: [41][209/233]	Loss 0.0319 (0.0363)	
training:	Epoch: [41][210/233]	Loss 0.0356 (0.0363)	
training:	Epoch: [41][211/233]	Loss 0.0351 (0.0363)	
training:	Epoch: [41][212/233]	Loss 0.0341 (0.0363)	
training:	Epoch: [41][213/233]	Loss 0.0565 (0.0364)	
training:	Epoch: [41][214/233]	Loss 0.0521 (0.0365)	
training:	Epoch: [41][215/233]	Loss 0.0309 (0.0364)	
training:	Epoch: [41][216/233]	Loss 0.0351 (0.0364)	
training:	Epoch: [41][217/233]	Loss 0.0309 (0.0364)	
training:	Epoch: [41][218/233]	Loss 0.0415 (0.0364)	
training:	Epoch: [41][219/233]	Loss 0.0303 (0.0364)	
training:	Epoch: [41][220/233]	Loss 0.0663 (0.0365)	
training:	Epoch: [41][221/233]	Loss 0.0437 (0.0366)	
training:	Epoch: [41][222/233]	Loss 0.0295 (0.0365)	
training:	Epoch: [41][223/233]	Loss 0.0287 (0.0365)	
training:	Epoch: [41][224/233]	Loss 0.0455 (0.0365)	
training:	Epoch: [41][225/233]	Loss 0.0512 (0.0366)	
training:	Epoch: [41][226/233]	Loss 0.0287 (0.0366)	
training:	Epoch: [41][227/233]	Loss 0.0486 (0.0366)	
training:	Epoch: [41][228/233]	Loss 0.0377 (0.0366)	
training:	Epoch: [41][229/233]	Loss 0.0291 (0.0366)	
training:	Epoch: [41][230/233]	Loss 0.0324 (0.0366)	
training:	Epoch: [41][231/233]	Loss 0.0305 (0.0366)	
training:	Epoch: [41][232/233]	Loss 0.0564 (0.0366)	
training:	Epoch: [41][233/233]	Loss 0.0335 (0.0366)	
Training:	 Loss: 0.0365

Training:	 ACC: 1.0000 1.0000 1.0000 1.0000
Validation:	 ACC: 0.8673 0.8657 0.8332 0.9013
Validation:	 Best_BACC: 0.8673 0.8657 0.8332 0.9013
Validation:	 Loss: 0.3257
Pretraining:	Epoch 42/200
----------
training:	Epoch: [42][1/233]	Loss 0.0309 (0.0309)	
training:	Epoch: [42][2/233]	Loss 0.0384 (0.0346)	
training:	Epoch: [42][3/233]	Loss 0.0298 (0.0330)	
training:	Epoch: [42][4/233]	Loss 0.0421 (0.0353)	
training:	Epoch: [42][5/233]	Loss 0.0526 (0.0388)	
training:	Epoch: [42][6/233]	Loss 0.0293 (0.0372)	
training:	Epoch: [42][7/233]	Loss 0.0292 (0.0361)	
training:	Epoch: [42][8/233]	Loss 0.0327 (0.0356)	
training:	Epoch: [42][9/233]	Loss 0.0287 (0.0349)	
training:	Epoch: [42][10/233]	Loss 0.0313 (0.0345)	
training:	Epoch: [42][11/233]	Loss 0.0305 (0.0341)	
training:	Epoch: [42][12/233]	Loss 0.0273 (0.0336)	
training:	Epoch: [42][13/233]	Loss 0.0330 (0.0335)	
training:	Epoch: [42][14/233]	Loss 0.0331 (0.0335)	
training:	Epoch: [42][15/233]	Loss 0.0395 (0.0339)	
training:	Epoch: [42][16/233]	Loss 0.0303 (0.0337)	
training:	Epoch: [42][17/233]	Loss 0.0325 (0.0336)	
training:	Epoch: [42][18/233]	Loss 0.0291 (0.0334)	
training:	Epoch: [42][19/233]	Loss 0.0488 (0.0342)	
training:	Epoch: [42][20/233]	Loss 0.0304 (0.0340)	
training:	Epoch: [42][21/233]	Loss 0.0356 (0.0341)	
training:	Epoch: [42][22/233]	Loss 0.0373 (0.0342)	
training:	Epoch: [42][23/233]	Loss 0.0256 (0.0338)	
training:	Epoch: [42][24/233]	Loss 0.0333 (0.0338)	
training:	Epoch: [42][25/233]	Loss 0.0403 (0.0341)	
training:	Epoch: [42][26/233]	Loss 0.0366 (0.0342)	
training:	Epoch: [42][27/233]	Loss 0.0287 (0.0340)	
training:	Epoch: [42][28/233]	Loss 0.0340 (0.0340)	
training:	Epoch: [42][29/233]	Loss 0.0302 (0.0338)	
training:	Epoch: [42][30/233]	Loss 0.0306 (0.0337)	
training:	Epoch: [42][31/233]	Loss 0.0379 (0.0339)	
training:	Epoch: [42][32/233]	Loss 0.0288 (0.0337)	
training:	Epoch: [42][33/233]	Loss 0.0379 (0.0338)	
training:	Epoch: [42][34/233]	Loss 0.0383 (0.0340)	
training:	Epoch: [42][35/233]	Loss 0.0326 (0.0339)	
training:	Epoch: [42][36/233]	Loss 0.0388 (0.0341)	
training:	Epoch: [42][37/233]	Loss 0.0462 (0.0344)	
training:	Epoch: [42][38/233]	Loss 0.0409 (0.0346)	
training:	Epoch: [42][39/233]	Loss 0.0250 (0.0343)	
training:	Epoch: [42][40/233]	Loss 0.0251 (0.0341)	
training:	Epoch: [42][41/233]	Loss 0.0444 (0.0343)	
training:	Epoch: [42][42/233]	Loss 0.0289 (0.0342)	
training:	Epoch: [42][43/233]	Loss 0.0654 (0.0349)	
training:	Epoch: [42][44/233]	Loss 0.0698 (0.0357)	
training:	Epoch: [42][45/233]	Loss 0.0312 (0.0356)	
training:	Epoch: [42][46/233]	Loss 0.0273 (0.0354)	
training:	Epoch: [42][47/233]	Loss 0.0285 (0.0353)	
training:	Epoch: [42][48/233]	Loss 0.0337 (0.0353)	
training:	Epoch: [42][49/233]	Loss 0.0298 (0.0351)	
training:	Epoch: [42][50/233]	Loss 0.0245 (0.0349)	
training:	Epoch: [42][51/233]	Loss 0.0282 (0.0348)	
training:	Epoch: [42][52/233]	Loss 0.0464 (0.0350)	
training:	Epoch: [42][53/233]	Loss 0.0264 (0.0349)	
training:	Epoch: [42][54/233]	Loss 0.0265 (0.0347)	
training:	Epoch: [42][55/233]	Loss 0.0316 (0.0347)	
training:	Epoch: [42][56/233]	Loss 0.0271 (0.0345)	
training:	Epoch: [42][57/233]	Loss 0.0344 (0.0345)	
training:	Epoch: [42][58/233]	Loss 0.0288 (0.0344)	
training:	Epoch: [42][59/233]	Loss 0.0630 (0.0349)	
training:	Epoch: [42][60/233]	Loss 0.0319 (0.0349)	
training:	Epoch: [42][61/233]	Loss 0.0299 (0.0348)	
training:	Epoch: [42][62/233]	Loss 0.0334 (0.0348)	
training:	Epoch: [42][63/233]	Loss 0.0285 (0.0347)	
training:	Epoch: [42][64/233]	Loss 0.0546 (0.0350)	
training:	Epoch: [42][65/233]	Loss 0.0292 (0.0349)	
training:	Epoch: [42][66/233]	Loss 0.0358 (0.0349)	
training:	Epoch: [42][67/233]	Loss 0.0274 (0.0348)	
training:	Epoch: [42][68/233]	Loss 0.0297 (0.0347)	
training:	Epoch: [42][69/233]	Loss 0.0268 (0.0346)	
training:	Epoch: [42][70/233]	Loss 0.0574 (0.0349)	
training:	Epoch: [42][71/233]	Loss 0.0333 (0.0349)	
training:	Epoch: [42][72/233]	Loss 0.0286 (0.0348)	
training:	Epoch: [42][73/233]	Loss 0.0271 (0.0347)	
training:	Epoch: [42][74/233]	Loss 0.0757 (0.0353)	
training:	Epoch: [42][75/233]	Loss 0.0428 (0.0354)	
training:	Epoch: [42][76/233]	Loss 0.0256 (0.0352)	
training:	Epoch: [42][77/233]	Loss 0.0317 (0.0352)	
training:	Epoch: [42][78/233]	Loss 0.0297 (0.0351)	
training:	Epoch: [42][79/233]	Loss 0.0314 (0.0351)	
training:	Epoch: [42][80/233]	Loss 0.0473 (0.0352)	
training:	Epoch: [42][81/233]	Loss 0.0287 (0.0351)	
training:	Epoch: [42][82/233]	Loss 0.0249 (0.0350)	
training:	Epoch: [42][83/233]	Loss 0.0757 (0.0355)	
training:	Epoch: [42][84/233]	Loss 0.0442 (0.0356)	
training:	Epoch: [42][85/233]	Loss 0.0309 (0.0355)	
training:	Epoch: [42][86/233]	Loss 0.0277 (0.0355)	
training:	Epoch: [42][87/233]	Loss 0.0436 (0.0356)	
training:	Epoch: [42][88/233]	Loss 0.0352 (0.0355)	
training:	Epoch: [42][89/233]	Loss 0.0273 (0.0355)	
training:	Epoch: [42][90/233]	Loss 0.0285 (0.0354)	
training:	Epoch: [42][91/233]	Loss 0.0290 (0.0353)	
training:	Epoch: [42][92/233]	Loss 0.0313 (0.0353)	
training:	Epoch: [42][93/233]	Loss 0.0279 (0.0352)	
training:	Epoch: [42][94/233]	Loss 0.0323 (0.0352)	
training:	Epoch: [42][95/233]	Loss 0.0345 (0.0351)	
training:	Epoch: [42][96/233]	Loss 0.0320 (0.0351)	
training:	Epoch: [42][97/233]	Loss 0.0525 (0.0353)	
training:	Epoch: [42][98/233]	Loss 0.0415 (0.0354)	
training:	Epoch: [42][99/233]	Loss 0.0334 (0.0353)	
training:	Epoch: [42][100/233]	Loss 0.0359 (0.0353)	
training:	Epoch: [42][101/233]	Loss 0.0408 (0.0354)	
training:	Epoch: [42][102/233]	Loss 0.0432 (0.0355)	
training:	Epoch: [42][103/233]	Loss 0.0254 (0.0354)	
training:	Epoch: [42][104/233]	Loss 0.0396 (0.0354)	
training:	Epoch: [42][105/233]	Loss 0.0347 (0.0354)	
training:	Epoch: [42][106/233]	Loss 0.0293 (0.0353)	
training:	Epoch: [42][107/233]	Loss 0.0314 (0.0353)	
training:	Epoch: [42][108/233]	Loss 0.0340 (0.0353)	
training:	Epoch: [42][109/233]	Loss 0.0536 (0.0355)	
training:	Epoch: [42][110/233]	Loss 0.0304 (0.0354)	
training:	Epoch: [42][111/233]	Loss 0.0322 (0.0354)	
training:	Epoch: [42][112/233]	Loss 0.0520 (0.0355)	
training:	Epoch: [42][113/233]	Loss 0.0340 (0.0355)	
training:	Epoch: [42][114/233]	Loss 0.0259 (0.0354)	
training:	Epoch: [42][115/233]	Loss 0.0304 (0.0354)	
training:	Epoch: [42][116/233]	Loss 0.0304 (0.0354)	
training:	Epoch: [42][117/233]	Loss 0.0284 (0.0353)	
training:	Epoch: [42][118/233]	Loss 0.0355 (0.0353)	
training:	Epoch: [42][119/233]	Loss 0.0291 (0.0352)	
training:	Epoch: [42][120/233]	Loss 0.0387 (0.0353)	
training:	Epoch: [42][121/233]	Loss 0.0293 (0.0352)	
training:	Epoch: [42][122/233]	Loss 0.0344 (0.0352)	
training:	Epoch: [42][123/233]	Loss 0.0307 (0.0352)	
training:	Epoch: [42][124/233]	Loss 0.0407 (0.0352)	
training:	Epoch: [42][125/233]	Loss 0.0353 (0.0352)	
training:	Epoch: [42][126/233]	Loss 0.0267 (0.0352)	
training:	Epoch: [42][127/233]	Loss 0.0383 (0.0352)	
training:	Epoch: [42][128/233]	Loss 0.0442 (0.0353)	
training:	Epoch: [42][129/233]	Loss 0.1092 (0.0358)	
training:	Epoch: [42][130/233]	Loss 0.0298 (0.0358)	
training:	Epoch: [42][131/233]	Loss 0.0412 (0.0358)	
training:	Epoch: [42][132/233]	Loss 0.0326 (0.0358)	
training:	Epoch: [42][133/233]	Loss 0.0295 (0.0357)	
training:	Epoch: [42][134/233]	Loss 0.0366 (0.0358)	
training:	Epoch: [42][135/233]	Loss 0.0368 (0.0358)	
training:	Epoch: [42][136/233]	Loss 0.0371 (0.0358)	
training:	Epoch: [42][137/233]	Loss 0.0272 (0.0357)	
training:	Epoch: [42][138/233]	Loss 0.0356 (0.0357)	
training:	Epoch: [42][139/233]	Loss 0.0580 (0.0359)	
training:	Epoch: [42][140/233]	Loss 0.0373 (0.0359)	
training:	Epoch: [42][141/233]	Loss 0.0298 (0.0358)	
training:	Epoch: [42][142/233]	Loss 0.0476 (0.0359)	
training:	Epoch: [42][143/233]	Loss 0.0278 (0.0359)	
training:	Epoch: [42][144/233]	Loss 0.0287 (0.0358)	
training:	Epoch: [42][145/233]	Loss 0.0545 (0.0359)	
training:	Epoch: [42][146/233]	Loss 0.0409 (0.0360)	
training:	Epoch: [42][147/233]	Loss 0.0310 (0.0359)	
training:	Epoch: [42][148/233]	Loss 0.0355 (0.0359)	
training:	Epoch: [42][149/233]	Loss 0.1136 (0.0365)	
training:	Epoch: [42][150/233]	Loss 0.0287 (0.0364)	
training:	Epoch: [42][151/233]	Loss 0.0251 (0.0363)	
training:	Epoch: [42][152/233]	Loss 0.0284 (0.0363)	
training:	Epoch: [42][153/233]	Loss 0.0301 (0.0362)	
training:	Epoch: [42][154/233]	Loss 0.0429 (0.0363)	
training:	Epoch: [42][155/233]	Loss 0.0306 (0.0362)	
training:	Epoch: [42][156/233]	Loss 0.0273 (0.0362)	
training:	Epoch: [42][157/233]	Loss 0.0278 (0.0361)	
training:	Epoch: [42][158/233]	Loss 0.0322 (0.0361)	
training:	Epoch: [42][159/233]	Loss 0.0273 (0.0361)	
training:	Epoch: [42][160/233]	Loss 0.0417 (0.0361)	
training:	Epoch: [42][161/233]	Loss 0.0271 (0.0360)	
training:	Epoch: [42][162/233]	Loss 0.0573 (0.0362)	
training:	Epoch: [42][163/233]	Loss 0.0288 (0.0361)	
training:	Epoch: [42][164/233]	Loss 0.0281 (0.0361)	
training:	Epoch: [42][165/233]	Loss 0.0306 (0.0360)	
training:	Epoch: [42][166/233]	Loss 0.0313 (0.0360)	
training:	Epoch: [42][167/233]	Loss 0.0287 (0.0360)	
training:	Epoch: [42][168/233]	Loss 0.0274 (0.0359)	
training:	Epoch: [42][169/233]	Loss 0.0315 (0.0359)	
training:	Epoch: [42][170/233]	Loss 0.0292 (0.0359)	
training:	Epoch: [42][171/233]	Loss 0.0290 (0.0358)	
training:	Epoch: [42][172/233]	Loss 0.0362 (0.0358)	
training:	Epoch: [42][173/233]	Loss 0.0266 (0.0358)	
training:	Epoch: [42][174/233]	Loss 0.0381 (0.0358)	
training:	Epoch: [42][175/233]	Loss 0.0301 (0.0357)	
training:	Epoch: [42][176/233]	Loss 0.0665 (0.0359)	
training:	Epoch: [42][177/233]	Loss 0.0256 (0.0359)	
training:	Epoch: [42][178/233]	Loss 0.0705 (0.0361)	
training:	Epoch: [42][179/233]	Loss 0.0269 (0.0360)	
training:	Epoch: [42][180/233]	Loss 0.0321 (0.0360)	
training:	Epoch: [42][181/233]	Loss 0.0367 (0.0360)	
training:	Epoch: [42][182/233]	Loss 0.0311 (0.0360)	
training:	Epoch: [42][183/233]	Loss 0.0375 (0.0360)	
training:	Epoch: [42][184/233]	Loss 0.0335 (0.0360)	
training:	Epoch: [42][185/233]	Loss 0.0290 (0.0359)	
training:	Epoch: [42][186/233]	Loss 0.0735 (0.0361)	
training:	Epoch: [42][187/233]	Loss 0.0351 (0.0361)	
training:	Epoch: [42][188/233]	Loss 0.0263 (0.0361)	
training:	Epoch: [42][189/233]	Loss 0.0249 (0.0360)	
training:	Epoch: [42][190/233]	Loss 0.0388 (0.0360)	
training:	Epoch: [42][191/233]	Loss 0.0357 (0.0360)	
training:	Epoch: [42][192/233]	Loss 0.0407 (0.0360)	
training:	Epoch: [42][193/233]	Loss 0.0330 (0.0360)	
training:	Epoch: [42][194/233]	Loss 0.0388 (0.0360)	
training:	Epoch: [42][195/233]	Loss 0.2424 (0.0371)	
training:	Epoch: [42][196/233]	Loss 0.0387 (0.0371)	
training:	Epoch: [42][197/233]	Loss 0.0550 (0.0372)	
training:	Epoch: [42][198/233]	Loss 0.0301 (0.0372)	
training:	Epoch: [42][199/233]	Loss 0.0282 (0.0371)	
training:	Epoch: [42][200/233]	Loss 0.0460 (0.0372)	
training:	Epoch: [42][201/233]	Loss 0.0263 (0.0371)	
training:	Epoch: [42][202/233]	Loss 0.0279 (0.0371)	
training:	Epoch: [42][203/233]	Loss 0.0659 (0.0372)	
training:	Epoch: [42][204/233]	Loss 0.0518 (0.0373)	
training:	Epoch: [42][205/233]	Loss 0.0300 (0.0372)	
training:	Epoch: [42][206/233]	Loss 0.0416 (0.0373)	
training:	Epoch: [42][207/233]	Loss 0.0371 (0.0373)	
training:	Epoch: [42][208/233]	Loss 0.0432 (0.0373)	
training:	Epoch: [42][209/233]	Loss 0.0285 (0.0372)	
training:	Epoch: [42][210/233]	Loss 0.0462 (0.0373)	
training:	Epoch: [42][211/233]	Loss 0.0364 (0.0373)	
training:	Epoch: [42][212/233]	Loss 0.0325 (0.0373)	
training:	Epoch: [42][213/233]	Loss 0.0306 (0.0372)	
training:	Epoch: [42][214/233]	Loss 0.0310 (0.0372)	
training:	Epoch: [42][215/233]	Loss 0.0334 (0.0372)	
training:	Epoch: [42][216/233]	Loss 0.0304 (0.0371)	
training:	Epoch: [42][217/233]	Loss 0.0289 (0.0371)	
training:	Epoch: [42][218/233]	Loss 0.0304 (0.0371)	
training:	Epoch: [42][219/233]	Loss 0.0271 (0.0370)	
training:	Epoch: [42][220/233]	Loss 0.0300 (0.0370)	
training:	Epoch: [42][221/233]	Loss 0.0321 (0.0370)	
training:	Epoch: [42][222/233]	Loss 0.0311 (0.0370)	
training:	Epoch: [42][223/233]	Loss 0.0880 (0.0372)	
training:	Epoch: [42][224/233]	Loss 0.0382 (0.0372)	
training:	Epoch: [42][225/233]	Loss 0.0571 (0.0373)	
training:	Epoch: [42][226/233]	Loss 0.0292 (0.0372)	
training:	Epoch: [42][227/233]	Loss 0.0295 (0.0372)	
training:	Epoch: [42][228/233]	Loss 0.0295 (0.0372)	
training:	Epoch: [42][229/233]	Loss 0.0629 (0.0373)	
training:	Epoch: [42][230/233]	Loss 0.0288 (0.0372)	
training:	Epoch: [42][231/233]	Loss 0.0304 (0.0372)	
training:	Epoch: [42][232/233]	Loss 0.0301 (0.0372)	
training:	Epoch: [42][233/233]	Loss 0.0283 (0.0371)	
Training:	 Loss: 0.0371

Training:	 ACC: 1.0000 1.0000 1.0000 1.0000
Validation:	 ACC: 0.8635 0.8625 0.8403 0.8868
Validation:	 Best_BACC: 0.8673 0.8657 0.8332 0.9013
Validation:	 Loss: 0.3206
Pretraining:	Epoch 43/200
----------
training:	Epoch: [43][1/233]	Loss 0.0290 (0.0290)	
training:	Epoch: [43][2/233]	Loss 0.0266 (0.0278)	
training:	Epoch: [43][3/233]	Loss 0.0339 (0.0298)	
training:	Epoch: [43][4/233]	Loss 0.0337 (0.0308)	
training:	Epoch: [43][5/233]	Loss 0.0253 (0.0297)	
training:	Epoch: [43][6/233]	Loss 0.0374 (0.0310)	
training:	Epoch: [43][7/233]	Loss 0.1326 (0.0455)	
training:	Epoch: [43][8/233]	Loss 0.0296 (0.0435)	
training:	Epoch: [43][9/233]	Loss 0.0336 (0.0424)	
training:	Epoch: [43][10/233]	Loss 0.0232 (0.0405)	
training:	Epoch: [43][11/233]	Loss 0.0326 (0.0398)	
training:	Epoch: [43][12/233]	Loss 0.0301 (0.0390)	
training:	Epoch: [43][13/233]	Loss 0.0299 (0.0383)	
training:	Epoch: [43][14/233]	Loss 0.0277 (0.0375)	
training:	Epoch: [43][15/233]	Loss 0.0308 (0.0371)	
training:	Epoch: [43][16/233]	Loss 0.0324 (0.0368)	
training:	Epoch: [43][17/233]	Loss 0.0241 (0.0360)	
training:	Epoch: [43][18/233]	Loss 0.0309 (0.0357)	
training:	Epoch: [43][19/233]	Loss 0.0301 (0.0354)	
training:	Epoch: [43][20/233]	Loss 0.0258 (0.0350)	
training:	Epoch: [43][21/233]	Loss 0.0267 (0.0346)	
training:	Epoch: [43][22/233]	Loss 0.0270 (0.0342)	
training:	Epoch: [43][23/233]	Loss 0.0273 (0.0339)	
training:	Epoch: [43][24/233]	Loss 0.0488 (0.0345)	
training:	Epoch: [43][25/233]	Loss 0.0273 (0.0342)	
training:	Epoch: [43][26/233]	Loss 0.0360 (0.0343)	
training:	Epoch: [43][27/233]	Loss 0.0271 (0.0341)	
training:	Epoch: [43][28/233]	Loss 0.0562 (0.0348)	
training:	Epoch: [43][29/233]	Loss 0.0429 (0.0351)	
training:	Epoch: [43][30/233]	Loss 0.0354 (0.0351)	
training:	Epoch: [43][31/233]	Loss 0.0298 (0.0350)	
training:	Epoch: [43][32/233]	Loss 0.1097 (0.0373)	
training:	Epoch: [43][33/233]	Loss 0.0321 (0.0371)	
training:	Epoch: [43][34/233]	Loss 0.0359 (0.0371)	
training:	Epoch: [43][35/233]	Loss 0.0502 (0.0375)	
training:	Epoch: [43][36/233]	Loss 0.0287 (0.0372)	
training:	Epoch: [43][37/233]	Loss 0.0280 (0.0370)	
training:	Epoch: [43][38/233]	Loss 0.0254 (0.0367)	
training:	Epoch: [43][39/233]	Loss 0.0392 (0.0367)	
training:	Epoch: [43][40/233]	Loss 0.0282 (0.0365)	
training:	Epoch: [43][41/233]	Loss 0.0257 (0.0363)	
training:	Epoch: [43][42/233]	Loss 0.0326 (0.0362)	
training:	Epoch: [43][43/233]	Loss 0.0485 (0.0365)	
training:	Epoch: [43][44/233]	Loss 0.0299 (0.0363)	
training:	Epoch: [43][45/233]	Loss 0.0244 (0.0360)	
training:	Epoch: [43][46/233]	Loss 0.0313 (0.0359)	
training:	Epoch: [43][47/233]	Loss 0.0247 (0.0357)	
training:	Epoch: [43][48/233]	Loss 0.0296 (0.0356)	
training:	Epoch: [43][49/233]	Loss 0.0258 (0.0354)	
training:	Epoch: [43][50/233]	Loss 0.0337 (0.0353)	
training:	Epoch: [43][51/233]	Loss 0.0381 (0.0354)	
training:	Epoch: [43][52/233]	Loss 0.0301 (0.0353)	
training:	Epoch: [43][53/233]	Loss 0.0286 (0.0352)	
training:	Epoch: [43][54/233]	Loss 0.0249 (0.0350)	
training:	Epoch: [43][55/233]	Loss 0.0538 (0.0353)	
training:	Epoch: [43][56/233]	Loss 0.0513 (0.0356)	
training:	Epoch: [43][57/233]	Loss 0.0315 (0.0355)	
training:	Epoch: [43][58/233]	Loss 0.0471 (0.0357)	
training:	Epoch: [43][59/233]	Loss 0.0244 (0.0355)	
training:	Epoch: [43][60/233]	Loss 0.0266 (0.0354)	
training:	Epoch: [43][61/233]	Loss 0.0271 (0.0353)	
training:	Epoch: [43][62/233]	Loss 0.0273 (0.0351)	
training:	Epoch: [43][63/233]	Loss 0.0352 (0.0351)	
training:	Epoch: [43][64/233]	Loss 0.0402 (0.0352)	
training:	Epoch: [43][65/233]	Loss 0.0291 (0.0351)	
training:	Epoch: [43][66/233]	Loss 0.0293 (0.0350)	
training:	Epoch: [43][67/233]	Loss 0.0375 (0.0351)	
training:	Epoch: [43][68/233]	Loss 0.0292 (0.0350)	
training:	Epoch: [43][69/233]	Loss 0.0460 (0.0351)	
training:	Epoch: [43][70/233]	Loss 0.0287 (0.0350)	
training:	Epoch: [43][71/233]	Loss 0.0238 (0.0349)	
training:	Epoch: [43][72/233]	Loss 0.0296 (0.0348)	
training:	Epoch: [43][73/233]	Loss 0.0454 (0.0350)	
training:	Epoch: [43][74/233]	Loss 0.0250 (0.0348)	
training:	Epoch: [43][75/233]	Loss 0.0343 (0.0348)	
training:	Epoch: [43][76/233]	Loss 0.0302 (0.0348)	
training:	Epoch: [43][77/233]	Loss 0.0734 (0.0353)	
training:	Epoch: [43][78/233]	Loss 0.0294 (0.0352)	
training:	Epoch: [43][79/233]	Loss 0.0362 (0.0352)	
training:	Epoch: [43][80/233]	Loss 0.0319 (0.0352)	
training:	Epoch: [43][81/233]	Loss 0.0338 (0.0351)	
training:	Epoch: [43][82/233]	Loss 0.0341 (0.0351)	
training:	Epoch: [43][83/233]	Loss 0.0320 (0.0351)	
training:	Epoch: [43][84/233]	Loss 0.0249 (0.0350)	
training:	Epoch: [43][85/233]	Loss 0.0350 (0.0350)	
training:	Epoch: [43][86/233]	Loss 0.0266 (0.0349)	
training:	Epoch: [43][87/233]	Loss 0.0312 (0.0348)	
training:	Epoch: [43][88/233]	Loss 0.0267 (0.0347)	
training:	Epoch: [43][89/233]	Loss 0.0262 (0.0346)	
training:	Epoch: [43][90/233]	Loss 0.0327 (0.0346)	
training:	Epoch: [43][91/233]	Loss 0.0477 (0.0348)	
training:	Epoch: [43][92/233]	Loss 0.0290 (0.0347)	
training:	Epoch: [43][93/233]	Loss 0.0306 (0.0347)	
training:	Epoch: [43][94/233]	Loss 0.0281 (0.0346)	
training:	Epoch: [43][95/233]	Loss 0.0282 (0.0345)	
training:	Epoch: [43][96/233]	Loss 0.0264 (0.0344)	
training:	Epoch: [43][97/233]	Loss 0.0535 (0.0346)	
training:	Epoch: [43][98/233]	Loss 0.0276 (0.0346)	
training:	Epoch: [43][99/233]	Loss 0.0417 (0.0346)	
training:	Epoch: [43][100/233]	Loss 0.0347 (0.0346)	
training:	Epoch: [43][101/233]	Loss 0.0394 (0.0347)	
training:	Epoch: [43][102/233]	Loss 0.0281 (0.0346)	
training:	Epoch: [43][103/233]	Loss 0.0342 (0.0346)	
training:	Epoch: [43][104/233]	Loss 0.0310 (0.0346)	
training:	Epoch: [43][105/233]	Loss 0.0271 (0.0345)	
training:	Epoch: [43][106/233]	Loss 0.0260 (0.0344)	
training:	Epoch: [43][107/233]	Loss 0.0282 (0.0344)	
training:	Epoch: [43][108/233]	Loss 0.0258 (0.0343)	
training:	Epoch: [43][109/233]	Loss 0.0335 (0.0343)	
training:	Epoch: [43][110/233]	Loss 0.0382 (0.0343)	
training:	Epoch: [43][111/233]	Loss 0.0267 (0.0342)	
training:	Epoch: [43][112/233]	Loss 0.0283 (0.0342)	
training:	Epoch: [43][113/233]	Loss 0.0283 (0.0341)	
training:	Epoch: [43][114/233]	Loss 0.0288 (0.0341)	
training:	Epoch: [43][115/233]	Loss 0.0394 (0.0341)	
training:	Epoch: [43][116/233]	Loss 0.0398 (0.0342)	
training:	Epoch: [43][117/233]	Loss 0.0316 (0.0342)	
training:	Epoch: [43][118/233]	Loss 0.0265 (0.0341)	
training:	Epoch: [43][119/233]	Loss 0.0317 (0.0341)	
training:	Epoch: [43][120/233]	Loss 0.0462 (0.0342)	
training:	Epoch: [43][121/233]	Loss 0.0459 (0.0343)	
training:	Epoch: [43][122/233]	Loss 0.0284 (0.0342)	
training:	Epoch: [43][123/233]	Loss 0.0311 (0.0342)	
training:	Epoch: [43][124/233]	Loss 0.0328 (0.0342)	
training:	Epoch: [43][125/233]	Loss 0.0350 (0.0342)	
training:	Epoch: [43][126/233]	Loss 0.0276 (0.0341)	
training:	Epoch: [43][127/233]	Loss 0.0291 (0.0341)	
training:	Epoch: [43][128/233]	Loss 0.0265 (0.0340)	
training:	Epoch: [43][129/233]	Loss 0.0339 (0.0340)	
training:	Epoch: [43][130/233]	Loss 0.0261 (0.0340)	
training:	Epoch: [43][131/233]	Loss 0.0316 (0.0340)	
training:	Epoch: [43][132/233]	Loss 0.0303 (0.0339)	
training:	Epoch: [43][133/233]	Loss 0.0292 (0.0339)	
training:	Epoch: [43][134/233]	Loss 0.0320 (0.0339)	
training:	Epoch: [43][135/233]	Loss 0.0368 (0.0339)	
training:	Epoch: [43][136/233]	Loss 0.0375 (0.0339)	
training:	Epoch: [43][137/233]	Loss 0.0287 (0.0339)	
training:	Epoch: [43][138/233]	Loss 0.0269 (0.0338)	
training:	Epoch: [43][139/233]	Loss 0.0267 (0.0338)	
training:	Epoch: [43][140/233]	Loss 0.0256 (0.0337)	
training:	Epoch: [43][141/233]	Loss 0.0269 (0.0337)	
training:	Epoch: [43][142/233]	Loss 0.0262 (0.0336)	
training:	Epoch: [43][143/233]	Loss 0.0781 (0.0339)	
training:	Epoch: [43][144/233]	Loss 0.0570 (0.0341)	
training:	Epoch: [43][145/233]	Loss 0.0280 (0.0341)	
training:	Epoch: [43][146/233]	Loss 0.0295 (0.0340)	
training:	Epoch: [43][147/233]	Loss 0.0291 (0.0340)	
training:	Epoch: [43][148/233]	Loss 0.0323 (0.0340)	
training:	Epoch: [43][149/233]	Loss 0.0388 (0.0340)	
training:	Epoch: [43][150/233]	Loss 0.0336 (0.0340)	
training:	Epoch: [43][151/233]	Loss 0.0304 (0.0340)	
training:	Epoch: [43][152/233]	Loss 0.0278 (0.0340)	
training:	Epoch: [43][153/233]	Loss 0.0276 (0.0339)	
training:	Epoch: [43][154/233]	Loss 0.0349 (0.0339)	
training:	Epoch: [43][155/233]	Loss 0.0246 (0.0339)	
training:	Epoch: [43][156/233]	Loss 0.0262 (0.0338)	
training:	Epoch: [43][157/233]	Loss 0.0304 (0.0338)	
training:	Epoch: [43][158/233]	Loss 0.0369 (0.0338)	
training:	Epoch: [43][159/233]	Loss 0.0287 (0.0338)	
training:	Epoch: [43][160/233]	Loss 0.0381 (0.0338)	
training:	Epoch: [43][161/233]	Loss 0.0272 (0.0338)	
training:	Epoch: [43][162/233]	Loss 0.0285 (0.0337)	
training:	Epoch: [43][163/233]	Loss 0.0301 (0.0337)	
training:	Epoch: [43][164/233]	Loss 0.0278 (0.0337)	
training:	Epoch: [43][165/233]	Loss 0.0300 (0.0336)	
training:	Epoch: [43][166/233]	Loss 0.0348 (0.0337)	
training:	Epoch: [43][167/233]	Loss 0.0250 (0.0336)	
training:	Epoch: [43][168/233]	Loss 0.0364 (0.0336)	
training:	Epoch: [43][169/233]	Loss 0.0413 (0.0337)	
training:	Epoch: [43][170/233]	Loss 0.0267 (0.0336)	
training:	Epoch: [43][171/233]	Loss 0.0370 (0.0336)	
training:	Epoch: [43][172/233]	Loss 0.0329 (0.0336)	
training:	Epoch: [43][173/233]	Loss 0.0323 (0.0336)	
training:	Epoch: [43][174/233]	Loss 0.0423 (0.0337)	
training:	Epoch: [43][175/233]	Loss 0.0265 (0.0336)	
training:	Epoch: [43][176/233]	Loss 0.0293 (0.0336)	
training:	Epoch: [43][177/233]	Loss 0.0383 (0.0336)	
training:	Epoch: [43][178/233]	Loss 0.0294 (0.0336)	
training:	Epoch: [43][179/233]	Loss 0.0322 (0.0336)	
training:	Epoch: [43][180/233]	Loss 0.0287 (0.0336)	
training:	Epoch: [43][181/233]	Loss 0.0595 (0.0337)	
training:	Epoch: [43][182/233]	Loss 0.0374 (0.0337)	
training:	Epoch: [43][183/233]	Loss 0.0288 (0.0337)	
training:	Epoch: [43][184/233]	Loss 0.0344 (0.0337)	
training:	Epoch: [43][185/233]	Loss 0.0327 (0.0337)	
training:	Epoch: [43][186/233]	Loss 0.0294 (0.0337)	
training:	Epoch: [43][187/233]	Loss 0.0421 (0.0337)	
training:	Epoch: [43][188/233]	Loss 0.0497 (0.0338)	
training:	Epoch: [43][189/233]	Loss 0.0236 (0.0338)	
training:	Epoch: [43][190/233]	Loss 0.0257 (0.0337)	
training:	Epoch: [43][191/233]	Loss 0.0269 (0.0337)	
training:	Epoch: [43][192/233]	Loss 0.0321 (0.0337)	
training:	Epoch: [43][193/233]	Loss 0.0292 (0.0337)	
training:	Epoch: [43][194/233]	Loss 0.0281 (0.0336)	
training:	Epoch: [43][195/233]	Loss 0.0260 (0.0336)	
training:	Epoch: [43][196/233]	Loss 0.0300 (0.0336)	
training:	Epoch: [43][197/233]	Loss 0.0302 (0.0336)	
training:	Epoch: [43][198/233]	Loss 0.0259 (0.0335)	
training:	Epoch: [43][199/233]	Loss 0.0279 (0.0335)	
training:	Epoch: [43][200/233]	Loss 0.0594 (0.0336)	
training:	Epoch: [43][201/233]	Loss 0.0296 (0.0336)	
training:	Epoch: [43][202/233]	Loss 0.0251 (0.0336)	
training:	Epoch: [43][203/233]	Loss 0.0374 (0.0336)	
training:	Epoch: [43][204/233]	Loss 0.0293 (0.0336)	
training:	Epoch: [43][205/233]	Loss 0.0261 (0.0335)	
training:	Epoch: [43][206/233]	Loss 0.1025 (0.0339)	
training:	Epoch: [43][207/233]	Loss 0.0301 (0.0338)	
training:	Epoch: [43][208/233]	Loss 0.0264 (0.0338)	
training:	Epoch: [43][209/233]	Loss 0.0361 (0.0338)	
training:	Epoch: [43][210/233]	Loss 0.0261 (0.0338)	
training:	Epoch: [43][211/233]	Loss 0.0271 (0.0337)	
training:	Epoch: [43][212/233]	Loss 0.0291 (0.0337)	
training:	Epoch: [43][213/233]	Loss 0.0400 (0.0338)	
training:	Epoch: [43][214/233]	Loss 0.0292 (0.0337)	
training:	Epoch: [43][215/233]	Loss 0.0543 (0.0338)	
training:	Epoch: [43][216/233]	Loss 0.0502 (0.0339)	
training:	Epoch: [43][217/233]	Loss 0.0233 (0.0339)	
training:	Epoch: [43][218/233]	Loss 0.0402 (0.0339)	
training:	Epoch: [43][219/233]	Loss 0.0357 (0.0339)	
training:	Epoch: [43][220/233]	Loss 0.0488 (0.0340)	
training:	Epoch: [43][221/233]	Loss 0.0292 (0.0339)	
training:	Epoch: [43][222/233]	Loss 0.0377 (0.0340)	
training:	Epoch: [43][223/233]	Loss 0.0294 (0.0339)	
training:	Epoch: [43][224/233]	Loss 0.0316 (0.0339)	
training:	Epoch: [43][225/233]	Loss 0.0578 (0.0340)	
training:	Epoch: [43][226/233]	Loss 0.0267 (0.0340)	
training:	Epoch: [43][227/233]	Loss 0.0351 (0.0340)	
training:	Epoch: [43][228/233]	Loss 0.0326 (0.0340)	
training:	Epoch: [43][229/233]	Loss 0.0269 (0.0340)	
training:	Epoch: [43][230/233]	Loss 0.0452 (0.0340)	
training:	Epoch: [43][231/233]	Loss 0.0346 (0.0340)	
training:	Epoch: [43][232/233]	Loss 0.0386 (0.0340)	
training:	Epoch: [43][233/233]	Loss 0.0276 (0.0340)	
Training:	 Loss: 0.0339

Training:	 ACC: 1.0000 1.0000 1.0000 1.0000
Validation:	 ACC: 0.8590 0.8566 0.8055 0.9126
Validation:	 Best_BACC: 0.8673 0.8657 0.8332 0.9013
Validation:	 Loss: 0.3362
Pretraining:	Epoch 44/200
----------
training:	Epoch: [44][1/233]	Loss 0.0305 (0.0305)	
training:	Epoch: [44][2/233]	Loss 0.0287 (0.0296)	
training:	Epoch: [44][3/233]	Loss 0.0246 (0.0279)	
training:	Epoch: [44][4/233]	Loss 0.0318 (0.0289)	
training:	Epoch: [44][5/233]	Loss 0.0244 (0.0280)	
training:	Epoch: [44][6/233]	Loss 0.0332 (0.0289)	
training:	Epoch: [44][7/233]	Loss 0.0244 (0.0282)	
training:	Epoch: [44][8/233]	Loss 0.0290 (0.0283)	
training:	Epoch: [44][9/233]	Loss 0.0282 (0.0283)	
training:	Epoch: [44][10/233]	Loss 0.0337 (0.0288)	
training:	Epoch: [44][11/233]	Loss 0.0235 (0.0284)	
training:	Epoch: [44][12/233]	Loss 0.0241 (0.0280)	
training:	Epoch: [44][13/233]	Loss 0.0411 (0.0290)	
training:	Epoch: [44][14/233]	Loss 0.0238 (0.0286)	
training:	Epoch: [44][15/233]	Loss 0.0246 (0.0284)	
training:	Epoch: [44][16/233]	Loss 0.0374 (0.0289)	
training:	Epoch: [44][17/233]	Loss 0.0355 (0.0293)	
training:	Epoch: [44][18/233]	Loss 0.0298 (0.0294)	
training:	Epoch: [44][19/233]	Loss 0.0381 (0.0298)	
training:	Epoch: [44][20/233]	Loss 0.0266 (0.0297)	
training:	Epoch: [44][21/233]	Loss 0.0371 (0.0300)	
training:	Epoch: [44][22/233]	Loss 0.0317 (0.0301)	
training:	Epoch: [44][23/233]	Loss 0.0256 (0.0299)	
training:	Epoch: [44][24/233]	Loss 0.0293 (0.0299)	
training:	Epoch: [44][25/233]	Loss 0.0298 (0.0299)	
training:	Epoch: [44][26/233]	Loss 0.0272 (0.0298)	
training:	Epoch: [44][27/233]	Loss 0.0250 (0.0296)	
training:	Epoch: [44][28/233]	Loss 0.0341 (0.0297)	
training:	Epoch: [44][29/233]	Loss 0.0265 (0.0296)	
training:	Epoch: [44][30/233]	Loss 0.0266 (0.0295)	
training:	Epoch: [44][31/233]	Loss 0.0402 (0.0299)	
training:	Epoch: [44][32/233]	Loss 0.0400 (0.0302)	
training:	Epoch: [44][33/233]	Loss 0.0247 (0.0300)	
training:	Epoch: [44][34/233]	Loss 0.0253 (0.0299)	
training:	Epoch: [44][35/233]	Loss 0.0255 (0.0298)	
training:	Epoch: [44][36/233]	Loss 0.0355 (0.0299)	
training:	Epoch: [44][37/233]	Loss 0.0463 (0.0304)	
training:	Epoch: [44][38/233]	Loss 0.0467 (0.0308)	
training:	Epoch: [44][39/233]	Loss 0.0288 (0.0307)	
training:	Epoch: [44][40/233]	Loss 0.0358 (0.0309)	
training:	Epoch: [44][41/233]	Loss 0.0278 (0.0308)	
training:	Epoch: [44][42/233]	Loss 0.0269 (0.0307)	
training:	Epoch: [44][43/233]	Loss 0.0270 (0.0306)	
training:	Epoch: [44][44/233]	Loss 0.0360 (0.0307)	
training:	Epoch: [44][45/233]	Loss 0.0282 (0.0307)	
training:	Epoch: [44][46/233]	Loss 0.0258 (0.0306)	
training:	Epoch: [44][47/233]	Loss 0.0277 (0.0305)	
training:	Epoch: [44][48/233]	Loss 0.0267 (0.0304)	
training:	Epoch: [44][49/233]	Loss 0.0242 (0.0303)	
training:	Epoch: [44][50/233]	Loss 0.0317 (0.0303)	
training:	Epoch: [44][51/233]	Loss 0.0259 (0.0302)	
training:	Epoch: [44][52/233]	Loss 0.0284 (0.0302)	
training:	Epoch: [44][53/233]	Loss 0.0290 (0.0302)	
training:	Epoch: [44][54/233]	Loss 0.0266 (0.0301)	
training:	Epoch: [44][55/233]	Loss 0.0265 (0.0301)	
training:	Epoch: [44][56/233]	Loss 0.0312 (0.0301)	
training:	Epoch: [44][57/233]	Loss 0.0270 (0.0300)	
training:	Epoch: [44][58/233]	Loss 0.0277 (0.0300)	
training:	Epoch: [44][59/233]	Loss 0.0285 (0.0300)	
training:	Epoch: [44][60/233]	Loss 0.0300 (0.0300)	
training:	Epoch: [44][61/233]	Loss 0.0283 (0.0299)	
training:	Epoch: [44][62/233]	Loss 0.0505 (0.0303)	
training:	Epoch: [44][63/233]	Loss 0.0250 (0.0302)	
training:	Epoch: [44][64/233]	Loss 0.0271 (0.0301)	
training:	Epoch: [44][65/233]	Loss 0.0253 (0.0301)	
training:	Epoch: [44][66/233]	Loss 0.0398 (0.0302)	
training:	Epoch: [44][67/233]	Loss 0.0304 (0.0302)	
training:	Epoch: [44][68/233]	Loss 0.0280 (0.0302)	
training:	Epoch: [44][69/233]	Loss 0.0312 (0.0302)	
training:	Epoch: [44][70/233]	Loss 0.0316 (0.0302)	
training:	Epoch: [44][71/233]	Loss 0.0580 (0.0306)	
training:	Epoch: [44][72/233]	Loss 0.0754 (0.0312)	
training:	Epoch: [44][73/233]	Loss 0.0244 (0.0311)	
training:	Epoch: [44][74/233]	Loss 0.0263 (0.0311)	
training:	Epoch: [44][75/233]	Loss 0.0260 (0.0310)	
training:	Epoch: [44][76/233]	Loss 0.0259 (0.0309)	
training:	Epoch: [44][77/233]	Loss 0.0297 (0.0309)	
training:	Epoch: [44][78/233]	Loss 0.0254 (0.0308)	
training:	Epoch: [44][79/233]	Loss 0.0439 (0.0310)	
training:	Epoch: [44][80/233]	Loss 0.0276 (0.0310)	
training:	Epoch: [44][81/233]	Loss 0.0268 (0.0309)	
training:	Epoch: [44][82/233]	Loss 0.0331 (0.0309)	
training:	Epoch: [44][83/233]	Loss 0.0314 (0.0309)	
training:	Epoch: [44][84/233]	Loss 0.0266 (0.0309)	
training:	Epoch: [44][85/233]	Loss 0.0240 (0.0308)	
training:	Epoch: [44][86/233]	Loss 0.0368 (0.0309)	
training:	Epoch: [44][87/233]	Loss 0.0283 (0.0309)	
training:	Epoch: [44][88/233]	Loss 0.0282 (0.0308)	
training:	Epoch: [44][89/233]	Loss 0.0259 (0.0308)	
training:	Epoch: [44][90/233]	Loss 0.0239 (0.0307)	
training:	Epoch: [44][91/233]	Loss 0.0283 (0.0307)	
training:	Epoch: [44][92/233]	Loss 0.0328 (0.0307)	
training:	Epoch: [44][93/233]	Loss 0.0514 (0.0309)	
training:	Epoch: [44][94/233]	Loss 0.0376 (0.0310)	
training:	Epoch: [44][95/233]	Loss 0.0626 (0.0313)	
training:	Epoch: [44][96/233]	Loss 0.0274 (0.0313)	
training:	Epoch: [44][97/233]	Loss 0.0290 (0.0313)	
training:	Epoch: [44][98/233]	Loss 0.0225 (0.0312)	
training:	Epoch: [44][99/233]	Loss 0.0239 (0.0311)	
training:	Epoch: [44][100/233]	Loss 0.0617 (0.0314)	
training:	Epoch: [44][101/233]	Loss 0.0230 (0.0313)	
training:	Epoch: [44][102/233]	Loss 0.0243 (0.0312)	
training:	Epoch: [44][103/233]	Loss 0.0352 (0.0313)	
training:	Epoch: [44][104/233]	Loss 0.0284 (0.0313)	
training:	Epoch: [44][105/233]	Loss 0.0276 (0.0312)	
training:	Epoch: [44][106/233]	Loss 0.0260 (0.0312)	
training:	Epoch: [44][107/233]	Loss 0.0272 (0.0311)	
training:	Epoch: [44][108/233]	Loss 0.0951 (0.0317)	
training:	Epoch: [44][109/233]	Loss 0.0313 (0.0317)	
training:	Epoch: [44][110/233]	Loss 0.0287 (0.0317)	
training:	Epoch: [44][111/233]	Loss 0.0328 (0.0317)	
training:	Epoch: [44][112/233]	Loss 0.0267 (0.0317)	
training:	Epoch: [44][113/233]	Loss 0.0326 (0.0317)	
training:	Epoch: [44][114/233]	Loss 0.0627 (0.0319)	
training:	Epoch: [44][115/233]	Loss 0.0247 (0.0319)	
training:	Epoch: [44][116/233]	Loss 0.0262 (0.0318)	
training:	Epoch: [44][117/233]	Loss 0.0286 (0.0318)	
training:	Epoch: [44][118/233]	Loss 0.0555 (0.0320)	
training:	Epoch: [44][119/233]	Loss 0.0355 (0.0320)	
training:	Epoch: [44][120/233]	Loss 0.0569 (0.0322)	
training:	Epoch: [44][121/233]	Loss 0.0245 (0.0322)	
training:	Epoch: [44][122/233]	Loss 0.0539 (0.0324)	
training:	Epoch: [44][123/233]	Loss 0.0298 (0.0323)	
training:	Epoch: [44][124/233]	Loss 0.0258 (0.0323)	
training:	Epoch: [44][125/233]	Loss 0.0396 (0.0323)	
training:	Epoch: [44][126/233]	Loss 0.0259 (0.0323)	
training:	Epoch: [44][127/233]	Loss 0.0346 (0.0323)	
training:	Epoch: [44][128/233]	Loss 0.0268 (0.0323)	
training:	Epoch: [44][129/233]	Loss 0.0425 (0.0323)	
training:	Epoch: [44][130/233]	Loss 0.0285 (0.0323)	
training:	Epoch: [44][131/233]	Loss 0.0444 (0.0324)	
training:	Epoch: [44][132/233]	Loss 0.0481 (0.0325)	
training:	Epoch: [44][133/233]	Loss 0.0324 (0.0325)	
training:	Epoch: [44][134/233]	Loss 0.0506 (0.0327)	
training:	Epoch: [44][135/233]	Loss 0.0292 (0.0326)	
training:	Epoch: [44][136/233]	Loss 0.0266 (0.0326)	
training:	Epoch: [44][137/233]	Loss 0.0317 (0.0326)	
training:	Epoch: [44][138/233]	Loss 0.0402 (0.0326)	
training:	Epoch: [44][139/233]	Loss 0.0273 (0.0326)	
training:	Epoch: [44][140/233]	Loss 0.0297 (0.0326)	
training:	Epoch: [44][141/233]	Loss 0.0475 (0.0327)	
training:	Epoch: [44][142/233]	Loss 0.0254 (0.0326)	
training:	Epoch: [44][143/233]	Loss 0.0347 (0.0326)	
training:	Epoch: [44][144/233]	Loss 0.0312 (0.0326)	
training:	Epoch: [44][145/233]	Loss 0.0322 (0.0326)	
training:	Epoch: [44][146/233]	Loss 0.0241 (0.0326)	
training:	Epoch: [44][147/233]	Loss 0.0578 (0.0327)	
training:	Epoch: [44][148/233]	Loss 0.0316 (0.0327)	
training:	Epoch: [44][149/233]	Loss 0.0262 (0.0327)	
training:	Epoch: [44][150/233]	Loss 0.0291 (0.0327)	
training:	Epoch: [44][151/233]	Loss 0.0327 (0.0327)	
training:	Epoch: [44][152/233]	Loss 0.0309 (0.0327)	
training:	Epoch: [44][153/233]	Loss 0.0334 (0.0327)	
training:	Epoch: [44][154/233]	Loss 0.0256 (0.0326)	
training:	Epoch: [44][155/233]	Loss 0.0261 (0.0326)	
training:	Epoch: [44][156/233]	Loss 0.0253 (0.0325)	
training:	Epoch: [44][157/233]	Loss 0.0392 (0.0326)	
training:	Epoch: [44][158/233]	Loss 0.0276 (0.0325)	
training:	Epoch: [44][159/233]	Loss 0.0264 (0.0325)	
training:	Epoch: [44][160/233]	Loss 0.0278 (0.0325)	
training:	Epoch: [44][161/233]	Loss 0.0410 (0.0325)	
training:	Epoch: [44][162/233]	Loss 0.0581 (0.0327)	
training:	Epoch: [44][163/233]	Loss 0.0351 (0.0327)	
training:	Epoch: [44][164/233]	Loss 0.0303 (0.0327)	
training:	Epoch: [44][165/233]	Loss 0.0266 (0.0326)	
training:	Epoch: [44][166/233]	Loss 0.0316 (0.0326)	
training:	Epoch: [44][167/233]	Loss 0.0269 (0.0326)	
training:	Epoch: [44][168/233]	Loss 0.0657 (0.0328)	
training:	Epoch: [44][169/233]	Loss 0.0253 (0.0328)	
training:	Epoch: [44][170/233]	Loss 0.0555 (0.0329)	
training:	Epoch: [44][171/233]	Loss 0.0267 (0.0329)	
training:	Epoch: [44][172/233]	Loss 0.0436 (0.0329)	
training:	Epoch: [44][173/233]	Loss 0.0315 (0.0329)	
training:	Epoch: [44][174/233]	Loss 0.0442 (0.0330)	
training:	Epoch: [44][175/233]	Loss 0.0286 (0.0330)	
training:	Epoch: [44][176/233]	Loss 0.0325 (0.0329)	
training:	Epoch: [44][177/233]	Loss 0.0441 (0.0330)	
training:	Epoch: [44][178/233]	Loss 0.0552 (0.0331)	
training:	Epoch: [44][179/233]	Loss 0.0259 (0.0331)	
training:	Epoch: [44][180/233]	Loss 0.0280 (0.0331)	
training:	Epoch: [44][181/233]	Loss 0.0243 (0.0330)	
training:	Epoch: [44][182/233]	Loss 0.0267 (0.0330)	
training:	Epoch: [44][183/233]	Loss 0.0308 (0.0330)	
training:	Epoch: [44][184/233]	Loss 0.0358 (0.0330)	
training:	Epoch: [44][185/233]	Loss 0.0278 (0.0330)	
training:	Epoch: [44][186/233]	Loss 0.0278 (0.0329)	
training:	Epoch: [44][187/233]	Loss 0.0285 (0.0329)	
training:	Epoch: [44][188/233]	Loss 0.0289 (0.0329)	
training:	Epoch: [44][189/233]	Loss 0.0297 (0.0329)	
training:	Epoch: [44][190/233]	Loss 0.0247 (0.0328)	
training:	Epoch: [44][191/233]	Loss 0.0270 (0.0328)	
training:	Epoch: [44][192/233]	Loss 0.0298 (0.0328)	
training:	Epoch: [44][193/233]	Loss 0.0296 (0.0328)	
training:	Epoch: [44][194/233]	Loss 0.0470 (0.0328)	
training:	Epoch: [44][195/233]	Loss 0.0327 (0.0328)	
training:	Epoch: [44][196/233]	Loss 0.0265 (0.0328)	
training:	Epoch: [44][197/233]	Loss 0.0361 (0.0328)	
training:	Epoch: [44][198/233]	Loss 0.0325 (0.0328)	
training:	Epoch: [44][199/233]	Loss 0.0253 (0.0328)	
training:	Epoch: [44][200/233]	Loss 0.0409 (0.0328)	
training:	Epoch: [44][201/233]	Loss 0.0340 (0.0328)	
training:	Epoch: [44][202/233]	Loss 0.0226 (0.0328)	
training:	Epoch: [44][203/233]	Loss 0.0276 (0.0328)	
training:	Epoch: [44][204/233]	Loss 0.0261 (0.0327)	
training:	Epoch: [44][205/233]	Loss 0.0340 (0.0327)	
training:	Epoch: [44][206/233]	Loss 0.0251 (0.0327)	
training:	Epoch: [44][207/233]	Loss 0.0247 (0.0327)	
training:	Epoch: [44][208/233]	Loss 0.0297 (0.0326)	
training:	Epoch: [44][209/233]	Loss 0.0245 (0.0326)	
training:	Epoch: [44][210/233]	Loss 0.0239 (0.0326)	
training:	Epoch: [44][211/233]	Loss 0.0245 (0.0325)	
training:	Epoch: [44][212/233]	Loss 0.0336 (0.0325)	
training:	Epoch: [44][213/233]	Loss 0.0299 (0.0325)	
training:	Epoch: [44][214/233]	Loss 0.0241 (0.0325)	
training:	Epoch: [44][215/233]	Loss 0.0283 (0.0325)	
training:	Epoch: [44][216/233]	Loss 0.0258 (0.0324)	
training:	Epoch: [44][217/233]	Loss 0.0376 (0.0324)	
training:	Epoch: [44][218/233]	Loss 0.0311 (0.0324)	
training:	Epoch: [44][219/233]	Loss 0.0328 (0.0324)	
training:	Epoch: [44][220/233]	Loss 0.0439 (0.0325)	
training:	Epoch: [44][221/233]	Loss 0.0257 (0.0325)	
training:	Epoch: [44][222/233]	Loss 0.0666 (0.0326)	
training:	Epoch: [44][223/233]	Loss 0.0247 (0.0326)	
training:	Epoch: [44][224/233]	Loss 0.0302 (0.0326)	
training:	Epoch: [44][225/233]	Loss 0.0342 (0.0326)	
training:	Epoch: [44][226/233]	Loss 0.0259 (0.0325)	
training:	Epoch: [44][227/233]	Loss 0.0266 (0.0325)	
training:	Epoch: [44][228/233]	Loss 0.0307 (0.0325)	
training:	Epoch: [44][229/233]	Loss 0.0291 (0.0325)	
training:	Epoch: [44][230/233]	Loss 0.0262 (0.0325)	
training:	Epoch: [44][231/233]	Loss 0.0253 (0.0324)	
training:	Epoch: [44][232/233]	Loss 0.0259 (0.0324)	
training:	Epoch: [44][233/233]	Loss 0.0271 (0.0324)	
Training:	 Loss: 0.0323

Training:	 ACC: 1.0000 1.0000 1.0000 1.0000
Validation:	 ACC: 0.8649 0.8646 0.8598 0.8700
Validation:	 Best_BACC: 0.8673 0.8657 0.8332 0.9013
Validation:	 Loss: 0.3158
Pretraining:	Epoch 45/200
----------
training:	Epoch: [45][1/233]	Loss 0.0271 (0.0271)	
training:	Epoch: [45][2/233]	Loss 0.0250 (0.0260)	
training:	Epoch: [45][3/233]	Loss 0.0309 (0.0276)	
training:	Epoch: [45][4/233]	Loss 0.0310 (0.0285)	
training:	Epoch: [45][5/233]	Loss 0.0250 (0.0278)	
training:	Epoch: [45][6/233]	Loss 0.0248 (0.0273)	
training:	Epoch: [45][7/233]	Loss 0.0208 (0.0264)	
training:	Epoch: [45][8/233]	Loss 0.0455 (0.0287)	
training:	Epoch: [45][9/233]	Loss 0.0242 (0.0282)	
training:	Epoch: [45][10/233]	Loss 0.0316 (0.0286)	
training:	Epoch: [45][11/233]	Loss 0.0379 (0.0294)	
training:	Epoch: [45][12/233]	Loss 0.0252 (0.0291)	
training:	Epoch: [45][13/233]	Loss 0.0252 (0.0288)	
training:	Epoch: [45][14/233]	Loss 0.0256 (0.0285)	
training:	Epoch: [45][15/233]	Loss 0.0276 (0.0285)	
training:	Epoch: [45][16/233]	Loss 0.0271 (0.0284)	
training:	Epoch: [45][17/233]	Loss 0.0250 (0.0282)	
training:	Epoch: [45][18/233]	Loss 0.0286 (0.0282)	
training:	Epoch: [45][19/233]	Loss 0.0293 (0.0283)	
training:	Epoch: [45][20/233]	Loss 0.0299 (0.0284)	
training:	Epoch: [45][21/233]	Loss 0.0261 (0.0282)	
training:	Epoch: [45][22/233]	Loss 0.0303 (0.0283)	
training:	Epoch: [45][23/233]	Loss 0.0283 (0.0283)	
training:	Epoch: [45][24/233]	Loss 0.0291 (0.0284)	
training:	Epoch: [45][25/233]	Loss 0.0284 (0.0284)	
training:	Epoch: [45][26/233]	Loss 0.0647 (0.0298)	
training:	Epoch: [45][27/233]	Loss 0.0284 (0.0297)	
training:	Epoch: [45][28/233]	Loss 0.0284 (0.0297)	
training:	Epoch: [45][29/233]	Loss 0.0358 (0.0299)	
training:	Epoch: [45][30/233]	Loss 0.0320 (0.0300)	
training:	Epoch: [45][31/233]	Loss 0.0248 (0.0298)	
training:	Epoch: [45][32/233]	Loss 0.0280 (0.0297)	
training:	Epoch: [45][33/233]	Loss 0.0290 (0.0297)	
training:	Epoch: [45][34/233]	Loss 0.0292 (0.0297)	
training:	Epoch: [45][35/233]	Loss 0.0278 (0.0296)	
training:	Epoch: [45][36/233]	Loss 0.0249 (0.0295)	
training:	Epoch: [45][37/233]	Loss 0.0338 (0.0296)	
training:	Epoch: [45][38/233]	Loss 0.0256 (0.0295)	
training:	Epoch: [45][39/233]	Loss 0.0276 (0.0295)	
training:	Epoch: [45][40/233]	Loss 0.0396 (0.0297)	
training:	Epoch: [45][41/233]	Loss 0.0305 (0.0297)	
training:	Epoch: [45][42/233]	Loss 0.0244 (0.0296)	
training:	Epoch: [45][43/233]	Loss 0.0366 (0.0298)	
training:	Epoch: [45][44/233]	Loss 0.0254 (0.0297)	
training:	Epoch: [45][45/233]	Loss 0.0220 (0.0295)	
training:	Epoch: [45][46/233]	Loss 0.0522 (0.0300)	
training:	Epoch: [45][47/233]	Loss 0.0255 (0.0299)	
training:	Epoch: [45][48/233]	Loss 0.0283 (0.0299)	
training:	Epoch: [45][49/233]	Loss 0.0289 (0.0298)	
training:	Epoch: [45][50/233]	Loss 0.0278 (0.0298)	
training:	Epoch: [45][51/233]	Loss 0.0319 (0.0298)	
training:	Epoch: [45][52/233]	Loss 0.0267 (0.0298)	
training:	Epoch: [45][53/233]	Loss 0.0260 (0.0297)	
training:	Epoch: [45][54/233]	Loss 0.0272 (0.0297)	
training:	Epoch: [45][55/233]	Loss 0.0484 (0.0300)	
training:	Epoch: [45][56/233]	Loss 0.0247 (0.0299)	
training:	Epoch: [45][57/233]	Loss 0.0242 (0.0298)	
training:	Epoch: [45][58/233]	Loss 0.0316 (0.0298)	
training:	Epoch: [45][59/233]	Loss 0.0562 (0.0303)	
training:	Epoch: [45][60/233]	Loss 0.0455 (0.0305)	
training:	Epoch: [45][61/233]	Loss 0.0258 (0.0305)	
training:	Epoch: [45][62/233]	Loss 0.0292 (0.0304)	
training:	Epoch: [45][63/233]	Loss 0.0274 (0.0304)	
training:	Epoch: [45][64/233]	Loss 0.0270 (0.0303)	
training:	Epoch: [45][65/233]	Loss 0.0968 (0.0314)	
training:	Epoch: [45][66/233]	Loss 0.0263 (0.0313)	
training:	Epoch: [45][67/233]	Loss 0.0239 (0.0312)	
training:	Epoch: [45][68/233]	Loss 0.0423 (0.0313)	
training:	Epoch: [45][69/233]	Loss 0.0294 (0.0313)	
training:	Epoch: [45][70/233]	Loss 0.0221 (0.0312)	
training:	Epoch: [45][71/233]	Loss 0.0229 (0.0311)	
training:	Epoch: [45][72/233]	Loss 0.0276 (0.0310)	
training:	Epoch: [45][73/233]	Loss 0.0291 (0.0310)	
training:	Epoch: [45][74/233]	Loss 0.0290 (0.0310)	
training:	Epoch: [45][75/233]	Loss 0.0275 (0.0309)	
training:	Epoch: [45][76/233]	Loss 0.0292 (0.0309)	
training:	Epoch: [45][77/233]	Loss 0.0350 (0.0310)	
training:	Epoch: [45][78/233]	Loss 0.0299 (0.0309)	
training:	Epoch: [45][79/233]	Loss 0.0308 (0.0309)	
training:	Epoch: [45][80/233]	Loss 0.0287 (0.0309)	
training:	Epoch: [45][81/233]	Loss 0.0335 (0.0309)	
training:	Epoch: [45][82/233]	Loss 0.0420 (0.0311)	
training:	Epoch: [45][83/233]	Loss 0.0259 (0.0310)	
training:	Epoch: [45][84/233]	Loss 0.0260 (0.0310)	
training:	Epoch: [45][85/233]	Loss 0.0253 (0.0309)	
training:	Epoch: [45][86/233]	Loss 0.0388 (0.0310)	
training:	Epoch: [45][87/233]	Loss 0.0267 (0.0309)	
training:	Epoch: [45][88/233]	Loss 0.0272 (0.0309)	
training:	Epoch: [45][89/233]	Loss 0.0301 (0.0309)	
training:	Epoch: [45][90/233]	Loss 0.0277 (0.0308)	
training:	Epoch: [45][91/233]	Loss 0.0280 (0.0308)	
training:	Epoch: [45][92/233]	Loss 0.0424 (0.0309)	
training:	Epoch: [45][93/233]	Loss 0.0263 (0.0309)	
training:	Epoch: [45][94/233]	Loss 0.0337 (0.0309)	
training:	Epoch: [45][95/233]	Loss 0.0291 (0.0309)	
training:	Epoch: [45][96/233]	Loss 0.0694 (0.0313)	
training:	Epoch: [45][97/233]	Loss 0.0251 (0.0312)	
training:	Epoch: [45][98/233]	Loss 0.0286 (0.0312)	
training:	Epoch: [45][99/233]	Loss 0.0312 (0.0312)	
training:	Epoch: [45][100/233]	Loss 0.0251 (0.0311)	
training:	Epoch: [45][101/233]	Loss 0.0249 (0.0311)	
training:	Epoch: [45][102/233]	Loss 0.0481 (0.0313)	
training:	Epoch: [45][103/233]	Loss 0.0530 (0.0315)	
training:	Epoch: [45][104/233]	Loss 0.0525 (0.0317)	
training:	Epoch: [45][105/233]	Loss 0.0486 (0.0318)	
training:	Epoch: [45][106/233]	Loss 0.0309 (0.0318)	
training:	Epoch: [45][107/233]	Loss 0.0342 (0.0318)	
training:	Epoch: [45][108/233]	Loss 0.0263 (0.0318)	
training:	Epoch: [45][109/233]	Loss 0.0264 (0.0317)	
training:	Epoch: [45][110/233]	Loss 0.0230 (0.0317)	
training:	Epoch: [45][111/233]	Loss 0.0248 (0.0316)	
training:	Epoch: [45][112/233]	Loss 0.0230 (0.0315)	
training:	Epoch: [45][113/233]	Loss 0.0359 (0.0316)	
training:	Epoch: [45][114/233]	Loss 0.0256 (0.0315)	
training:	Epoch: [45][115/233]	Loss 0.0278 (0.0315)	
training:	Epoch: [45][116/233]	Loss 0.0256 (0.0314)	
training:	Epoch: [45][117/233]	Loss 0.0530 (0.0316)	
training:	Epoch: [45][118/233]	Loss 0.0256 (0.0316)	
training:	Epoch: [45][119/233]	Loss 0.0339 (0.0316)	
training:	Epoch: [45][120/233]	Loss 0.0283 (0.0315)	
training:	Epoch: [45][121/233]	Loss 0.0243 (0.0315)	
training:	Epoch: [45][122/233]	Loss 0.0257 (0.0314)	
training:	Epoch: [45][123/233]	Loss 0.0233 (0.0314)	
training:	Epoch: [45][124/233]	Loss 0.0263 (0.0313)	
training:	Epoch: [45][125/233]	Loss 0.0240 (0.0313)	
training:	Epoch: [45][126/233]	Loss 0.0259 (0.0312)	
training:	Epoch: [45][127/233]	Loss 0.0287 (0.0312)	
training:	Epoch: [45][128/233]	Loss 0.0268 (0.0312)	
training:	Epoch: [45][129/233]	Loss 0.0277 (0.0312)	
training:	Epoch: [45][130/233]	Loss 0.0310 (0.0312)	
training:	Epoch: [45][131/233]	Loss 0.0259 (0.0311)	
training:	Epoch: [45][132/233]	Loss 0.0301 (0.0311)	
training:	Epoch: [45][133/233]	Loss 0.0245 (0.0311)	
training:	Epoch: [45][134/233]	Loss 0.0278 (0.0310)	
training:	Epoch: [45][135/233]	Loss 0.0278 (0.0310)	
training:	Epoch: [45][136/233]	Loss 0.0270 (0.0310)	
training:	Epoch: [45][137/233]	Loss 0.0672 (0.0312)	
training:	Epoch: [45][138/233]	Loss 0.0316 (0.0312)	
training:	Epoch: [45][139/233]	Loss 0.0267 (0.0312)	
training:	Epoch: [45][140/233]	Loss 0.0261 (0.0312)	
training:	Epoch: [45][141/233]	Loss 0.0299 (0.0312)	
training:	Epoch: [45][142/233]	Loss 0.0239 (0.0311)	
training:	Epoch: [45][143/233]	Loss 0.0358 (0.0311)	
training:	Epoch: [45][144/233]	Loss 0.0277 (0.0311)	
training:	Epoch: [45][145/233]	Loss 0.0350 (0.0311)	
training:	Epoch: [45][146/233]	Loss 0.0306 (0.0311)	
training:	Epoch: [45][147/233]	Loss 0.0386 (0.0312)	
training:	Epoch: [45][148/233]	Loss 0.0340 (0.0312)	
training:	Epoch: [45][149/233]	Loss 0.0510 (0.0313)	
training:	Epoch: [45][150/233]	Loss 0.0326 (0.0314)	
training:	Epoch: [45][151/233]	Loss 0.0321 (0.0314)	
training:	Epoch: [45][152/233]	Loss 0.0405 (0.0314)	
training:	Epoch: [45][153/233]	Loss 0.0251 (0.0314)	
training:	Epoch: [45][154/233]	Loss 0.0290 (0.0314)	
training:	Epoch: [45][155/233]	Loss 0.0323 (0.0314)	
training:	Epoch: [45][156/233]	Loss 0.0220 (0.0313)	
training:	Epoch: [45][157/233]	Loss 0.0389 (0.0314)	
training:	Epoch: [45][158/233]	Loss 0.0317 (0.0314)	
training:	Epoch: [45][159/233]	Loss 0.0280 (0.0313)	
training:	Epoch: [45][160/233]	Loss 0.0505 (0.0315)	
training:	Epoch: [45][161/233]	Loss 0.0238 (0.0314)	
training:	Epoch: [45][162/233]	Loss 0.0238 (0.0314)	
training:	Epoch: [45][163/233]	Loss 0.0356 (0.0314)	
training:	Epoch: [45][164/233]	Loss 0.0432 (0.0315)	
training:	Epoch: [45][165/233]	Loss 0.0263 (0.0314)	
training:	Epoch: [45][166/233]	Loss 0.0234 (0.0314)	
training:	Epoch: [45][167/233]	Loss 0.0267 (0.0314)	
training:	Epoch: [45][168/233]	Loss 0.0389 (0.0314)	
training:	Epoch: [45][169/233]	Loss 0.0382 (0.0314)	
training:	Epoch: [45][170/233]	Loss 0.0370 (0.0315)	
training:	Epoch: [45][171/233]	Loss 0.0262 (0.0314)	
training:	Epoch: [45][172/233]	Loss 0.0370 (0.0315)	
training:	Epoch: [45][173/233]	Loss 0.0262 (0.0314)	
training:	Epoch: [45][174/233]	Loss 0.0254 (0.0314)	
training:	Epoch: [45][175/233]	Loss 0.0750 (0.0317)	
training:	Epoch: [45][176/233]	Loss 0.0241 (0.0316)	
training:	Epoch: [45][177/233]	Loss 0.0226 (0.0316)	
training:	Epoch: [45][178/233]	Loss 0.0278 (0.0315)	
training:	Epoch: [45][179/233]	Loss 0.0253 (0.0315)	
training:	Epoch: [45][180/233]	Loss 0.0259 (0.0315)	
training:	Epoch: [45][181/233]	Loss 0.0313 (0.0315)	
training:	Epoch: [45][182/233]	Loss 0.0496 (0.0316)	
training:	Epoch: [45][183/233]	Loss 0.0239 (0.0315)	
training:	Epoch: [45][184/233]	Loss 0.0285 (0.0315)	
training:	Epoch: [45][185/233]	Loss 0.0249 (0.0315)	
training:	Epoch: [45][186/233]	Loss 0.0326 (0.0315)	
training:	Epoch: [45][187/233]	Loss 0.0293 (0.0315)	
training:	Epoch: [45][188/233]	Loss 0.0250 (0.0314)	
training:	Epoch: [45][189/233]	Loss 0.0267 (0.0314)	
training:	Epoch: [45][190/233]	Loss 0.0293 (0.0314)	
training:	Epoch: [45][191/233]	Loss 0.0287 (0.0314)	
training:	Epoch: [45][192/233]	Loss 0.0253 (0.0314)	
training:	Epoch: [45][193/233]	Loss 0.0240 (0.0313)	
training:	Epoch: [45][194/233]	Loss 0.0272 (0.0313)	
training:	Epoch: [45][195/233]	Loss 0.0245 (0.0313)	
training:	Epoch: [45][196/233]	Loss 0.0248 (0.0312)	
training:	Epoch: [45][197/233]	Loss 0.0295 (0.0312)	
training:	Epoch: [45][198/233]	Loss 0.0481 (0.0313)	
training:	Epoch: [45][199/233]	Loss 0.0427 (0.0314)	
training:	Epoch: [45][200/233]	Loss 0.0744 (0.0316)	
training:	Epoch: [45][201/233]	Loss 0.0300 (0.0316)	
training:	Epoch: [45][202/233]	Loss 0.0283 (0.0316)	
training:	Epoch: [45][203/233]	Loss 0.0422 (0.0316)	
training:	Epoch: [45][204/233]	Loss 0.0229 (0.0316)	
training:	Epoch: [45][205/233]	Loss 0.0574 (0.0317)	
training:	Epoch: [45][206/233]	Loss 0.0284 (0.0317)	
training:	Epoch: [45][207/233]	Loss 0.0255 (0.0316)	
training:	Epoch: [45][208/233]	Loss 0.0258 (0.0316)	
training:	Epoch: [45][209/233]	Loss 0.0284 (0.0316)	
training:	Epoch: [45][210/233]	Loss 0.0406 (0.0316)	
training:	Epoch: [45][211/233]	Loss 0.0269 (0.0316)	
training:	Epoch: [45][212/233]	Loss 0.0257 (0.0316)	
training:	Epoch: [45][213/233]	Loss 0.0342 (0.0316)	
training:	Epoch: [45][214/233]	Loss 0.0311 (0.0316)	
training:	Epoch: [45][215/233]	Loss 0.0242 (0.0316)	
training:	Epoch: [45][216/233]	Loss 0.0422 (0.0316)	
training:	Epoch: [45][217/233]	Loss 0.0280 (0.0316)	
training:	Epoch: [45][218/233]	Loss 0.0452 (0.0317)	
training:	Epoch: [45][219/233]	Loss 0.0243 (0.0316)	
training:	Epoch: [45][220/233]	Loss 0.0221 (0.0316)	
training:	Epoch: [45][221/233]	Loss 0.0275 (0.0316)	
training:	Epoch: [45][222/233]	Loss 0.0243 (0.0315)	
training:	Epoch: [45][223/233]	Loss 0.0538 (0.0316)	
training:	Epoch: [45][224/233]	Loss 0.0265 (0.0316)	
training:	Epoch: [45][225/233]	Loss 0.0281 (0.0316)	
training:	Epoch: [45][226/233]	Loss 0.0256 (0.0316)	
training:	Epoch: [45][227/233]	Loss 0.0278 (0.0316)	
training:	Epoch: [45][228/233]	Loss 0.0234 (0.0315)	
training:	Epoch: [45][229/233]	Loss 0.0233 (0.0315)	
training:	Epoch: [45][230/233]	Loss 0.0368 (0.0315)	
training:	Epoch: [45][231/233]	Loss 0.0279 (0.0315)	
training:	Epoch: [45][232/233]	Loss 0.0282 (0.0315)	
training:	Epoch: [45][233/233]	Loss 0.0262 (0.0315)	
Training:	 Loss: 0.0314

Training:	 ACC: 1.0000 1.0000 1.0000 1.0000
Validation:	 ACC: 0.8648 0.8641 0.8485 0.8812
Validation:	 Best_BACC: 0.8673 0.8657 0.8332 0.9013
Validation:	 Loss: 0.3183
Pretraining:	Epoch 46/200
----------
training:	Epoch: [46][1/233]	Loss 0.0271 (0.0271)	
training:	Epoch: [46][2/233]	Loss 0.0378 (0.0325)	
training:	Epoch: [46][3/233]	Loss 0.0252 (0.0300)	
training:	Epoch: [46][4/233]	Loss 0.0264 (0.0291)	
training:	Epoch: [46][5/233]	Loss 0.0288 (0.0291)	
training:	Epoch: [46][6/233]	Loss 0.0475 (0.0321)	
training:	Epoch: [46][7/233]	Loss 0.0314 (0.0320)	
training:	Epoch: [46][8/233]	Loss 0.0229 (0.0309)	
training:	Epoch: [46][9/233]	Loss 0.0250 (0.0302)	
training:	Epoch: [46][10/233]	Loss 0.0269 (0.0299)	
training:	Epoch: [46][11/233]	Loss 0.0580 (0.0325)	
training:	Epoch: [46][12/233]	Loss 0.0304 (0.0323)	
training:	Epoch: [46][13/233]	Loss 0.0241 (0.0316)	
training:	Epoch: [46][14/233]	Loss 0.0219 (0.0310)	
training:	Epoch: [46][15/233]	Loss 0.0289 (0.0308)	
training:	Epoch: [46][16/233]	Loss 0.0229 (0.0303)	
training:	Epoch: [46][17/233]	Loss 0.0244 (0.0300)	
training:	Epoch: [46][18/233]	Loss 0.0487 (0.0310)	
training:	Epoch: [46][19/233]	Loss 0.0274 (0.0308)	
training:	Epoch: [46][20/233]	Loss 0.0303 (0.0308)	
training:	Epoch: [46][21/233]	Loss 0.0240 (0.0305)	
training:	Epoch: [46][22/233]	Loss 0.0307 (0.0305)	
training:	Epoch: [46][23/233]	Loss 0.0448 (0.0311)	
training:	Epoch: [46][24/233]	Loss 0.0280 (0.0310)	
training:	Epoch: [46][25/233]	Loss 0.0214 (0.0306)	
training:	Epoch: [46][26/233]	Loss 0.0266 (0.0304)	
training:	Epoch: [46][27/233]	Loss 0.0265 (0.0303)	
training:	Epoch: [46][28/233]	Loss 0.0232 (0.0300)	
training:	Epoch: [46][29/233]	Loss 0.0253 (0.0299)	
training:	Epoch: [46][30/233]	Loss 0.0264 (0.0298)	
training:	Epoch: [46][31/233]	Loss 0.0299 (0.0298)	
training:	Epoch: [46][32/233]	Loss 0.0225 (0.0295)	
training:	Epoch: [46][33/233]	Loss 0.0215 (0.0293)	
training:	Epoch: [46][34/233]	Loss 0.0254 (0.0292)	
training:	Epoch: [46][35/233]	Loss 0.0248 (0.0291)	
training:	Epoch: [46][36/233]	Loss 0.0234 (0.0289)	
training:	Epoch: [46][37/233]	Loss 0.0232 (0.0287)	
training:	Epoch: [46][38/233]	Loss 0.0238 (0.0286)	
training:	Epoch: [46][39/233]	Loss 0.0249 (0.0285)	
training:	Epoch: [46][40/233]	Loss 0.0277 (0.0285)	
training:	Epoch: [46][41/233]	Loss 0.0263 (0.0284)	
training:	Epoch: [46][42/233]	Loss 0.0302 (0.0285)	
training:	Epoch: [46][43/233]	Loss 0.0248 (0.0284)	
training:	Epoch: [46][44/233]	Loss 0.0236 (0.0283)	
training:	Epoch: [46][45/233]	Loss 0.0235 (0.0282)	
training:	Epoch: [46][46/233]	Loss 0.0240 (0.0281)	
training:	Epoch: [46][47/233]	Loss 0.0310 (0.0282)	
training:	Epoch: [46][48/233]	Loss 0.0404 (0.0284)	
training:	Epoch: [46][49/233]	Loss 0.0408 (0.0287)	
training:	Epoch: [46][50/233]	Loss 0.0217 (0.0285)	
training:	Epoch: [46][51/233]	Loss 0.0269 (0.0285)	
training:	Epoch: [46][52/233]	Loss 0.0372 (0.0287)	
training:	Epoch: [46][53/233]	Loss 0.0231 (0.0286)	
training:	Epoch: [46][54/233]	Loss 0.0370 (0.0287)	
training:	Epoch: [46][55/233]	Loss 0.0271 (0.0287)	
training:	Epoch: [46][56/233]	Loss 0.0321 (0.0287)	
training:	Epoch: [46][57/233]	Loss 0.0214 (0.0286)	
training:	Epoch: [46][58/233]	Loss 0.0299 (0.0286)	
training:	Epoch: [46][59/233]	Loss 0.0330 (0.0287)	
training:	Epoch: [46][60/233]	Loss 0.0262 (0.0287)	
training:	Epoch: [46][61/233]	Loss 0.0224 (0.0286)	
training:	Epoch: [46][62/233]	Loss 0.0258 (0.0285)	
training:	Epoch: [46][63/233]	Loss 0.0233 (0.0284)	
training:	Epoch: [46][64/233]	Loss 0.0345 (0.0285)	
training:	Epoch: [46][65/233]	Loss 0.0227 (0.0284)	
training:	Epoch: [46][66/233]	Loss 0.0261 (0.0284)	
training:	Epoch: [46][67/233]	Loss 0.0229 (0.0283)	
training:	Epoch: [46][68/233]	Loss 0.0215 (0.0282)	
training:	Epoch: [46][69/233]	Loss 0.0232 (0.0282)	
training:	Epoch: [46][70/233]	Loss 0.0265 (0.0281)	
training:	Epoch: [46][71/233]	Loss 0.0237 (0.0281)	
training:	Epoch: [46][72/233]	Loss 0.0425 (0.0283)	
training:	Epoch: [46][73/233]	Loss 0.0326 (0.0283)	
training:	Epoch: [46][74/233]	Loss 0.0256 (0.0283)	
training:	Epoch: [46][75/233]	Loss 0.0326 (0.0284)	
training:	Epoch: [46][76/233]	Loss 0.0340 (0.0284)	
training:	Epoch: [46][77/233]	Loss 0.0246 (0.0284)	
training:	Epoch: [46][78/233]	Loss 0.0359 (0.0285)	
training:	Epoch: [46][79/233]	Loss 0.0312 (0.0285)	
training:	Epoch: [46][80/233]	Loss 0.0245 (0.0285)	
training:	Epoch: [46][81/233]	Loss 0.0249 (0.0284)	
training:	Epoch: [46][82/233]	Loss 0.0226 (0.0283)	
training:	Epoch: [46][83/233]	Loss 0.0230 (0.0283)	
training:	Epoch: [46][84/233]	Loss 0.0243 (0.0282)	
training:	Epoch: [46][85/233]	Loss 0.0355 (0.0283)	
training:	Epoch: [46][86/233]	Loss 0.0255 (0.0283)	
training:	Epoch: [46][87/233]	Loss 0.0345 (0.0284)	
training:	Epoch: [46][88/233]	Loss 0.0257 (0.0283)	
training:	Epoch: [46][89/233]	Loss 0.0266 (0.0283)	
training:	Epoch: [46][90/233]	Loss 0.0240 (0.0283)	
training:	Epoch: [46][91/233]	Loss 0.0258 (0.0282)	
training:	Epoch: [46][92/233]	Loss 0.0517 (0.0285)	
training:	Epoch: [46][93/233]	Loss 0.0293 (0.0285)	
training:	Epoch: [46][94/233]	Loss 0.0332 (0.0285)	
training:	Epoch: [46][95/233]	Loss 0.0224 (0.0285)	
training:	Epoch: [46][96/233]	Loss 0.0257 (0.0285)	
training:	Epoch: [46][97/233]	Loss 0.0348 (0.0285)	
training:	Epoch: [46][98/233]	Loss 0.0258 (0.0285)	
training:	Epoch: [46][99/233]	Loss 0.0398 (0.0286)	
training:	Epoch: [46][100/233]	Loss 0.0299 (0.0286)	
training:	Epoch: [46][101/233]	Loss 0.0333 (0.0287)	
training:	Epoch: [46][102/233]	Loss 0.0259 (0.0286)	
training:	Epoch: [46][103/233]	Loss 0.0227 (0.0286)	
training:	Epoch: [46][104/233]	Loss 0.0241 (0.0285)	
training:	Epoch: [46][105/233]	Loss 0.0275 (0.0285)	
training:	Epoch: [46][106/233]	Loss 0.0230 (0.0285)	
training:	Epoch: [46][107/233]	Loss 0.0437 (0.0286)	
training:	Epoch: [46][108/233]	Loss 0.0291 (0.0286)	
training:	Epoch: [46][109/233]	Loss 0.0232 (0.0286)	
training:	Epoch: [46][110/233]	Loss 0.0278 (0.0286)	
training:	Epoch: [46][111/233]	Loss 0.0270 (0.0286)	
training:	Epoch: [46][112/233]	Loss 0.0271 (0.0285)	
training:	Epoch: [46][113/233]	Loss 0.0247 (0.0285)	
training:	Epoch: [46][114/233]	Loss 0.0216 (0.0284)	
training:	Epoch: [46][115/233]	Loss 0.0239 (0.0284)	
training:	Epoch: [46][116/233]	Loss 0.0239 (0.0284)	
training:	Epoch: [46][117/233]	Loss 0.0305 (0.0284)	
training:	Epoch: [46][118/233]	Loss 0.0501 (0.0286)	
training:	Epoch: [46][119/233]	Loss 0.0280 (0.0286)	
training:	Epoch: [46][120/233]	Loss 0.0243 (0.0285)	
training:	Epoch: [46][121/233]	Loss 0.0261 (0.0285)	
training:	Epoch: [46][122/233]	Loss 0.0262 (0.0285)	
training:	Epoch: [46][123/233]	Loss 0.0227 (0.0284)	
training:	Epoch: [46][124/233]	Loss 0.0211 (0.0284)	
training:	Epoch: [46][125/233]	Loss 0.0227 (0.0283)	
training:	Epoch: [46][126/233]	Loss 0.0351 (0.0284)	
training:	Epoch: [46][127/233]	Loss 0.0254 (0.0284)	
training:	Epoch: [46][128/233]	Loss 0.0361 (0.0284)	
training:	Epoch: [46][129/233]	Loss 0.0358 (0.0285)	
training:	Epoch: [46][130/233]	Loss 0.0275 (0.0285)	
training:	Epoch: [46][131/233]	Loss 0.0273 (0.0285)	
training:	Epoch: [46][132/233]	Loss 0.0239 (0.0284)	
training:	Epoch: [46][133/233]	Loss 0.0228 (0.0284)	
training:	Epoch: [46][134/233]	Loss 0.0268 (0.0284)	
training:	Epoch: [46][135/233]	Loss 0.0212 (0.0283)	
training:	Epoch: [46][136/233]	Loss 0.0264 (0.0283)	
training:	Epoch: [46][137/233]	Loss 0.0476 (0.0285)	
training:	Epoch: [46][138/233]	Loss 0.0252 (0.0284)	
training:	Epoch: [46][139/233]	Loss 0.0217 (0.0284)	
training:	Epoch: [46][140/233]	Loss 0.0238 (0.0283)	
training:	Epoch: [46][141/233]	Loss 0.0267 (0.0283)	
training:	Epoch: [46][142/233]	Loss 0.0306 (0.0283)	
training:	Epoch: [46][143/233]	Loss 0.0228 (0.0283)	
training:	Epoch: [46][144/233]	Loss 0.0251 (0.0283)	
training:	Epoch: [46][145/233]	Loss 0.0249 (0.0283)	
training:	Epoch: [46][146/233]	Loss 0.0257 (0.0282)	
training:	Epoch: [46][147/233]	Loss 0.0422 (0.0283)	
training:	Epoch: [46][148/233]	Loss 0.0254 (0.0283)	
training:	Epoch: [46][149/233]	Loss 0.0259 (0.0283)	
training:	Epoch: [46][150/233]	Loss 0.0264 (0.0283)	
training:	Epoch: [46][151/233]	Loss 0.0248 (0.0283)	
training:	Epoch: [46][152/233]	Loss 0.0253 (0.0283)	
training:	Epoch: [46][153/233]	Loss 0.0324 (0.0283)	
training:	Epoch: [46][154/233]	Loss 0.0216 (0.0282)	
training:	Epoch: [46][155/233]	Loss 0.0205 (0.0282)	
training:	Epoch: [46][156/233]	Loss 0.0278 (0.0282)	
training:	Epoch: [46][157/233]	Loss 0.0252 (0.0282)	
training:	Epoch: [46][158/233]	Loss 0.0304 (0.0282)	
training:	Epoch: [46][159/233]	Loss 0.0269 (0.0282)	
training:	Epoch: [46][160/233]	Loss 0.0238 (0.0281)	
training:	Epoch: [46][161/233]	Loss 0.0418 (0.0282)	
training:	Epoch: [46][162/233]	Loss 0.0314 (0.0282)	
training:	Epoch: [46][163/233]	Loss 0.0324 (0.0283)	
training:	Epoch: [46][164/233]	Loss 0.0255 (0.0283)	
training:	Epoch: [46][165/233]	Loss 0.0557 (0.0284)	
training:	Epoch: [46][166/233]	Loss 0.0323 (0.0284)	
training:	Epoch: [46][167/233]	Loss 0.0282 (0.0284)	
training:	Epoch: [46][168/233]	Loss 0.0305 (0.0285)	
training:	Epoch: [46][169/233]	Loss 0.0223 (0.0284)	
training:	Epoch: [46][170/233]	Loss 0.0356 (0.0285)	
training:	Epoch: [46][171/233]	Loss 0.0283 (0.0285)	
training:	Epoch: [46][172/233]	Loss 0.0253 (0.0284)	
training:	Epoch: [46][173/233]	Loss 0.0252 (0.0284)	
training:	Epoch: [46][174/233]	Loss 0.0233 (0.0284)	
training:	Epoch: [46][175/233]	Loss 0.0357 (0.0284)	
training:	Epoch: [46][176/233]	Loss 0.0249 (0.0284)	
training:	Epoch: [46][177/233]	Loss 0.0448 (0.0285)	
training:	Epoch: [46][178/233]	Loss 0.0234 (0.0285)	
training:	Epoch: [46][179/233]	Loss 0.0339 (0.0285)	
training:	Epoch: [46][180/233]	Loss 0.0428 (0.0286)	
training:	Epoch: [46][181/233]	Loss 0.0303 (0.0286)	
training:	Epoch: [46][182/233]	Loss 0.0230 (0.0286)	
training:	Epoch: [46][183/233]	Loss 0.0846 (0.0289)	
training:	Epoch: [46][184/233]	Loss 0.0315 (0.0289)	
training:	Epoch: [46][185/233]	Loss 0.0233 (0.0289)	
training:	Epoch: [46][186/233]	Loss 0.0309 (0.0289)	
training:	Epoch: [46][187/233]	Loss 0.0316 (0.0289)	
training:	Epoch: [46][188/233]	Loss 0.0299 (0.0289)	
training:	Epoch: [46][189/233]	Loss 0.0264 (0.0289)	
training:	Epoch: [46][190/233]	Loss 0.0256 (0.0289)	
training:	Epoch: [46][191/233]	Loss 0.0256 (0.0288)	
training:	Epoch: [46][192/233]	Loss 0.0281 (0.0288)	
training:	Epoch: [46][193/233]	Loss 0.0251 (0.0288)	
training:	Epoch: [46][194/233]	Loss 0.0248 (0.0288)	
training:	Epoch: [46][195/233]	Loss 0.0270 (0.0288)	
training:	Epoch: [46][196/233]	Loss 0.0237 (0.0288)	
training:	Epoch: [46][197/233]	Loss 0.0335 (0.0288)	
training:	Epoch: [46][198/233]	Loss 0.0321 (0.0288)	
training:	Epoch: [46][199/233]	Loss 0.0246 (0.0288)	
training:	Epoch: [46][200/233]	Loss 0.0276 (0.0288)	
training:	Epoch: [46][201/233]	Loss 0.0559 (0.0289)	
training:	Epoch: [46][202/233]	Loss 0.0258 (0.0289)	
training:	Epoch: [46][203/233]	Loss 0.0287 (0.0289)	
training:	Epoch: [46][204/233]	Loss 0.0245 (0.0289)	
training:	Epoch: [46][205/233]	Loss 0.0244 (0.0289)	
training:	Epoch: [46][206/233]	Loss 0.0335 (0.0289)	
training:	Epoch: [46][207/233]	Loss 0.0259 (0.0289)	
training:	Epoch: [46][208/233]	Loss 0.0244 (0.0288)	
training:	Epoch: [46][209/233]	Loss 0.0229 (0.0288)	
training:	Epoch: [46][210/233]	Loss 0.0325 (0.0288)	
training:	Epoch: [46][211/233]	Loss 0.0342 (0.0289)	
training:	Epoch: [46][212/233]	Loss 0.0495 (0.0290)	
training:	Epoch: [46][213/233]	Loss 0.0221 (0.0289)	
training:	Epoch: [46][214/233]	Loss 0.0273 (0.0289)	
training:	Epoch: [46][215/233]	Loss 0.0282 (0.0289)	
training:	Epoch: [46][216/233]	Loss 0.0532 (0.0290)	
training:	Epoch: [46][217/233]	Loss 0.0248 (0.0290)	
training:	Epoch: [46][218/233]	Loss 0.0385 (0.0290)	
training:	Epoch: [46][219/233]	Loss 0.0571 (0.0292)	
training:	Epoch: [46][220/233]	Loss 0.0263 (0.0292)	
training:	Epoch: [46][221/233]	Loss 0.0228 (0.0291)	
training:	Epoch: [46][222/233]	Loss 0.0348 (0.0292)	
training:	Epoch: [46][223/233]	Loss 0.0229 (0.0291)	
training:	Epoch: [46][224/233]	Loss 0.0284 (0.0291)	
training:	Epoch: [46][225/233]	Loss 0.0275 (0.0291)	
training:	Epoch: [46][226/233]	Loss 0.0595 (0.0293)	
training:	Epoch: [46][227/233]	Loss 0.0325 (0.0293)	
training:	Epoch: [46][228/233]	Loss 0.0233 (0.0292)	
training:	Epoch: [46][229/233]	Loss 0.0309 (0.0292)	
training:	Epoch: [46][230/233]	Loss 0.0261 (0.0292)	
training:	Epoch: [46][231/233]	Loss 0.0255 (0.0292)	
training:	Epoch: [46][232/233]	Loss 0.0232 (0.0292)	
training:	Epoch: [46][233/233]	Loss 0.0276 (0.0292)	
Training:	 Loss: 0.0291

Training:	 ACC: 1.0000 1.0000 1.0000 1.0000
Validation:	 ACC: 0.8651 0.8646 0.8557 0.8744
Validation:	 Best_BACC: 0.8673 0.8657 0.8332 0.9013
Validation:	 Loss: 0.3182
Pretraining:	Epoch 47/200
----------
training:	Epoch: [47][1/233]	Loss 0.0238 (0.0238)	
training:	Epoch: [47][2/233]	Loss 0.0325 (0.0281)	
training:	Epoch: [47][3/233]	Loss 0.0226 (0.0263)	
training:	Epoch: [47][4/233]	Loss 0.0213 (0.0250)	
training:	Epoch: [47][5/233]	Loss 0.0237 (0.0248)	
training:	Epoch: [47][6/233]	Loss 0.0232 (0.0245)	
training:	Epoch: [47][7/233]	Loss 0.0244 (0.0245)	
training:	Epoch: [47][8/233]	Loss 0.0264 (0.0247)	
training:	Epoch: [47][9/233]	Loss 0.0294 (0.0252)	
training:	Epoch: [47][10/233]	Loss 0.0253 (0.0253)	
training:	Epoch: [47][11/233]	Loss 0.0561 (0.0281)	
training:	Epoch: [47][12/233]	Loss 0.0240 (0.0277)	
training:	Epoch: [47][13/233]	Loss 0.0216 (0.0273)	
training:	Epoch: [47][14/233]	Loss 0.0223 (0.0269)	
training:	Epoch: [47][15/233]	Loss 0.0278 (0.0270)	
training:	Epoch: [47][16/233]	Loss 0.0569 (0.0288)	
training:	Epoch: [47][17/233]	Loss 0.0243 (0.0286)	
training:	Epoch: [47][18/233]	Loss 0.0311 (0.0287)	
training:	Epoch: [47][19/233]	Loss 0.0252 (0.0285)	
training:	Epoch: [47][20/233]	Loss 0.0345 (0.0288)	
training:	Epoch: [47][21/233]	Loss 0.0315 (0.0290)	
training:	Epoch: [47][22/233]	Loss 0.0324 (0.0291)	
training:	Epoch: [47][23/233]	Loss 0.0239 (0.0289)	
training:	Epoch: [47][24/233]	Loss 0.0394 (0.0293)	
training:	Epoch: [47][25/233]	Loss 0.0274 (0.0292)	
training:	Epoch: [47][26/233]	Loss 0.0215 (0.0289)	
training:	Epoch: [47][27/233]	Loss 0.0302 (0.0290)	
training:	Epoch: [47][28/233]	Loss 0.0202 (0.0287)	
training:	Epoch: [47][29/233]	Loss 0.0312 (0.0288)	
training:	Epoch: [47][30/233]	Loss 0.0773 (0.0304)	
training:	Epoch: [47][31/233]	Loss 0.0224 (0.0301)	
training:	Epoch: [47][32/233]	Loss 0.0233 (0.0299)	
training:	Epoch: [47][33/233]	Loss 0.0346 (0.0300)	
training:	Epoch: [47][34/233]	Loss 0.0231 (0.0298)	
training:	Epoch: [47][35/233]	Loss 0.0240 (0.0297)	
training:	Epoch: [47][36/233]	Loss 0.0254 (0.0296)	
training:	Epoch: [47][37/233]	Loss 0.0263 (0.0295)	
training:	Epoch: [47][38/233]	Loss 0.0259 (0.0294)	
training:	Epoch: [47][39/233]	Loss 0.0500 (0.0299)	
training:	Epoch: [47][40/233]	Loss 0.0250 (0.0298)	
training:	Epoch: [47][41/233]	Loss 0.0226 (0.0296)	
training:	Epoch: [47][42/233]	Loss 0.0237 (0.0295)	
training:	Epoch: [47][43/233]	Loss 0.0291 (0.0295)	
training:	Epoch: [47][44/233]	Loss 0.0237 (0.0293)	
training:	Epoch: [47][45/233]	Loss 0.0417 (0.0296)	
training:	Epoch: [47][46/233]	Loss 0.0272 (0.0296)	
training:	Epoch: [47][47/233]	Loss 0.0250 (0.0295)	
training:	Epoch: [47][48/233]	Loss 0.0727 (0.0304)	
training:	Epoch: [47][49/233]	Loss 0.0224 (0.0302)	
training:	Epoch: [47][50/233]	Loss 0.0246 (0.0301)	
training:	Epoch: [47][51/233]	Loss 0.0311 (0.0301)	
training:	Epoch: [47][52/233]	Loss 0.0298 (0.0301)	
training:	Epoch: [47][53/233]	Loss 0.0255 (0.0300)	
training:	Epoch: [47][54/233]	Loss 0.0269 (0.0300)	
training:	Epoch: [47][55/233]	Loss 0.0263 (0.0299)	
training:	Epoch: [47][56/233]	Loss 0.0260 (0.0298)	
training:	Epoch: [47][57/233]	Loss 0.0237 (0.0297)	
training:	Epoch: [47][58/233]	Loss 0.0244 (0.0296)	
training:	Epoch: [47][59/233]	Loss 0.0236 (0.0295)	
training:	Epoch: [47][60/233]	Loss 0.0266 (0.0295)	
training:	Epoch: [47][61/233]	Loss 0.0388 (0.0296)	
training:	Epoch: [47][62/233]	Loss 0.0252 (0.0295)	
training:	Epoch: [47][63/233]	Loss 0.0218 (0.0294)	
training:	Epoch: [47][64/233]	Loss 0.0255 (0.0294)	
training:	Epoch: [47][65/233]	Loss 0.0233 (0.0293)	
training:	Epoch: [47][66/233]	Loss 0.0233 (0.0292)	
training:	Epoch: [47][67/233]	Loss 0.0226 (0.0291)	
training:	Epoch: [47][68/233]	Loss 0.0201 (0.0289)	
training:	Epoch: [47][69/233]	Loss 0.0251 (0.0289)	
training:	Epoch: [47][70/233]	Loss 0.0926 (0.0298)	
training:	Epoch: [47][71/233]	Loss 0.0222 (0.0297)	
training:	Epoch: [47][72/233]	Loss 0.0225 (0.0296)	
training:	Epoch: [47][73/233]	Loss 0.0340 (0.0297)	
training:	Epoch: [47][74/233]	Loss 0.0228 (0.0296)	
training:	Epoch: [47][75/233]	Loss 0.0242 (0.0295)	
training:	Epoch: [47][76/233]	Loss 0.0242 (0.0294)	
training:	Epoch: [47][77/233]	Loss 0.0325 (0.0295)	
training:	Epoch: [47][78/233]	Loss 0.0237 (0.0294)	
training:	Epoch: [47][79/233]	Loss 0.0267 (0.0294)	
training:	Epoch: [47][80/233]	Loss 0.0215 (0.0293)	
training:	Epoch: [47][81/233]	Loss 0.0229 (0.0292)	
training:	Epoch: [47][82/233]	Loss 0.0229 (0.0291)	
training:	Epoch: [47][83/233]	Loss 0.0244 (0.0290)	
training:	Epoch: [47][84/233]	Loss 0.0243 (0.0290)	
training:	Epoch: [47][85/233]	Loss 0.0240 (0.0289)	
training:	Epoch: [47][86/233]	Loss 0.0316 (0.0290)	
training:	Epoch: [47][87/233]	Loss 0.0243 (0.0289)	
training:	Epoch: [47][88/233]	Loss 0.0232 (0.0288)	
training:	Epoch: [47][89/233]	Loss 0.0285 (0.0288)	
training:	Epoch: [47][90/233]	Loss 0.0238 (0.0288)	
training:	Epoch: [47][91/233]	Loss 0.0282 (0.0288)	
training:	Epoch: [47][92/233]	Loss 0.0239 (0.0287)	
training:	Epoch: [47][93/233]	Loss 0.0261 (0.0287)	
training:	Epoch: [47][94/233]	Loss 0.0254 (0.0287)	
training:	Epoch: [47][95/233]	Loss 0.0233 (0.0286)	
training:	Epoch: [47][96/233]	Loss 0.0263 (0.0286)	
training:	Epoch: [47][97/233]	Loss 0.0276 (0.0286)	
training:	Epoch: [47][98/233]	Loss 0.0223 (0.0285)	
training:	Epoch: [47][99/233]	Loss 0.0239 (0.0285)	
training:	Epoch: [47][100/233]	Loss 0.0242 (0.0284)	
training:	Epoch: [47][101/233]	Loss 0.0226 (0.0284)	
training:	Epoch: [47][102/233]	Loss 0.0217 (0.0283)	
training:	Epoch: [47][103/233]	Loss 0.0325 (0.0283)	
training:	Epoch: [47][104/233]	Loss 0.0409 (0.0285)	
training:	Epoch: [47][105/233]	Loss 0.0261 (0.0284)	
training:	Epoch: [47][106/233]	Loss 0.0352 (0.0285)	
training:	Epoch: [47][107/233]	Loss 0.0432 (0.0286)	
training:	Epoch: [47][108/233]	Loss 0.0309 (0.0287)	
training:	Epoch: [47][109/233]	Loss 0.0244 (0.0286)	
training:	Epoch: [47][110/233]	Loss 0.0273 (0.0286)	
training:	Epoch: [47][111/233]	Loss 0.0245 (0.0286)	
training:	Epoch: [47][112/233]	Loss 0.0250 (0.0285)	
training:	Epoch: [47][113/233]	Loss 0.0222 (0.0285)	
training:	Epoch: [47][114/233]	Loss 0.0400 (0.0286)	
training:	Epoch: [47][115/233]	Loss 0.0534 (0.0288)	
training:	Epoch: [47][116/233]	Loss 0.0227 (0.0287)	
training:	Epoch: [47][117/233]	Loss 0.0498 (0.0289)	
training:	Epoch: [47][118/233]	Loss 0.0303 (0.0289)	
training:	Epoch: [47][119/233]	Loss 0.0246 (0.0289)	
training:	Epoch: [47][120/233]	Loss 0.0251 (0.0289)	
training:	Epoch: [47][121/233]	Loss 0.0210 (0.0288)	
training:	Epoch: [47][122/233]	Loss 0.0299 (0.0288)	
training:	Epoch: [47][123/233]	Loss 0.0316 (0.0288)	
training:	Epoch: [47][124/233]	Loss 0.0270 (0.0288)	
training:	Epoch: [47][125/233]	Loss 0.0253 (0.0288)	
training:	Epoch: [47][126/233]	Loss 0.0268 (0.0288)	
training:	Epoch: [47][127/233]	Loss 0.0377 (0.0288)	
training:	Epoch: [47][128/233]	Loss 0.0402 (0.0289)	
training:	Epoch: [47][129/233]	Loss 0.0264 (0.0289)	
training:	Epoch: [47][130/233]	Loss 0.0361 (0.0290)	
training:	Epoch: [47][131/233]	Loss 0.0312 (0.0290)	
training:	Epoch: [47][132/233]	Loss 0.0293 (0.0290)	
training:	Epoch: [47][133/233]	Loss 0.0432 (0.0291)	
training:	Epoch: [47][134/233]	Loss 0.0223 (0.0290)	
training:	Epoch: [47][135/233]	Loss 0.0213 (0.0290)	
training:	Epoch: [47][136/233]	Loss 0.0318 (0.0290)	
training:	Epoch: [47][137/233]	Loss 0.0521 (0.0292)	
training:	Epoch: [47][138/233]	Loss 0.0311 (0.0292)	
training:	Epoch: [47][139/233]	Loss 0.0217 (0.0291)	
training:	Epoch: [47][140/233]	Loss 0.0271 (0.0291)	
training:	Epoch: [47][141/233]	Loss 0.0232 (0.0291)	
training:	Epoch: [47][142/233]	Loss 0.0285 (0.0291)	
training:	Epoch: [47][143/233]	Loss 0.0241 (0.0290)	
training:	Epoch: [47][144/233]	Loss 0.0245 (0.0290)	
training:	Epoch: [47][145/233]	Loss 0.0242 (0.0290)	
training:	Epoch: [47][146/233]	Loss 0.0230 (0.0289)	
training:	Epoch: [47][147/233]	Loss 0.0345 (0.0290)	
training:	Epoch: [47][148/233]	Loss 0.0225 (0.0289)	
training:	Epoch: [47][149/233]	Loss 0.0541 (0.0291)	
training:	Epoch: [47][150/233]	Loss 0.0605 (0.0293)	
training:	Epoch: [47][151/233]	Loss 0.0392 (0.0294)	
training:	Epoch: [47][152/233]	Loss 0.0235 (0.0293)	
training:	Epoch: [47][153/233]	Loss 0.0257 (0.0293)	
training:	Epoch: [47][154/233]	Loss 0.0400 (0.0294)	
training:	Epoch: [47][155/233]	Loss 0.0247 (0.0294)	
training:	Epoch: [47][156/233]	Loss 0.0247 (0.0293)	
training:	Epoch: [47][157/233]	Loss 0.0257 (0.0293)	
training:	Epoch: [47][158/233]	Loss 0.0281 (0.0293)	
training:	Epoch: [47][159/233]	Loss 0.0214 (0.0292)	
training:	Epoch: [47][160/233]	Loss 0.0708 (0.0295)	
training:	Epoch: [47][161/233]	Loss 0.0337 (0.0295)	
training:	Epoch: [47][162/233]	Loss 0.0207 (0.0295)	
training:	Epoch: [47][163/233]	Loss 0.0244 (0.0294)	
training:	Epoch: [47][164/233]	Loss 0.0252 (0.0294)	
training:	Epoch: [47][165/233]	Loss 0.0207 (0.0294)	
training:	Epoch: [47][166/233]	Loss 0.0251 (0.0293)	
training:	Epoch: [47][167/233]	Loss 0.0244 (0.0293)	
training:	Epoch: [47][168/233]	Loss 0.0248 (0.0293)	
training:	Epoch: [47][169/233]	Loss 0.0252 (0.0293)	
training:	Epoch: [47][170/233]	Loss 0.0234 (0.0292)	
training:	Epoch: [47][171/233]	Loss 0.0243 (0.0292)	
training:	Epoch: [47][172/233]	Loss 0.0303 (0.0292)	
training:	Epoch: [47][173/233]	Loss 0.0202 (0.0291)	
training:	Epoch: [47][174/233]	Loss 0.0269 (0.0291)	
training:	Epoch: [47][175/233]	Loss 0.0582 (0.0293)	
training:	Epoch: [47][176/233]	Loss 0.0265 (0.0293)	
training:	Epoch: [47][177/233]	Loss 0.0220 (0.0292)	
training:	Epoch: [47][178/233]	Loss 0.0237 (0.0292)	
training:	Epoch: [47][179/233]	Loss 0.0285 (0.0292)	
training:	Epoch: [47][180/233]	Loss 0.0230 (0.0292)	
training:	Epoch: [47][181/233]	Loss 0.0224 (0.0291)	
training:	Epoch: [47][182/233]	Loss 0.0196 (0.0291)	
training:	Epoch: [47][183/233]	Loss 0.0372 (0.0291)	
training:	Epoch: [47][184/233]	Loss 0.0232 (0.0291)	
training:	Epoch: [47][185/233]	Loss 0.0358 (0.0291)	
training:	Epoch: [47][186/233]	Loss 0.0214 (0.0291)	
training:	Epoch: [47][187/233]	Loss 0.0276 (0.0291)	
training:	Epoch: [47][188/233]	Loss 0.0376 (0.0291)	
training:	Epoch: [47][189/233]	Loss 0.0518 (0.0292)	
training:	Epoch: [47][190/233]	Loss 0.0252 (0.0292)	
training:	Epoch: [47][191/233]	Loss 0.0440 (0.0293)	
training:	Epoch: [47][192/233]	Loss 0.0284 (0.0293)	
training:	Epoch: [47][193/233]	Loss 0.0209 (0.0293)	
training:	Epoch: [47][194/233]	Loss 0.0275 (0.0292)	
training:	Epoch: [47][195/233]	Loss 0.0227 (0.0292)	
training:	Epoch: [47][196/233]	Loss 0.0411 (0.0293)	
training:	Epoch: [47][197/233]	Loss 0.0390 (0.0293)	
training:	Epoch: [47][198/233]	Loss 0.0273 (0.0293)	
training:	Epoch: [47][199/233]	Loss 0.0393 (0.0294)	
training:	Epoch: [47][200/233]	Loss 0.0234 (0.0293)	
training:	Epoch: [47][201/233]	Loss 0.0225 (0.0293)	
training:	Epoch: [47][202/233]	Loss 0.0262 (0.0293)	
training:	Epoch: [47][203/233]	Loss 0.0295 (0.0293)	
training:	Epoch: [47][204/233]	Loss 0.0357 (0.0293)	
training:	Epoch: [47][205/233]	Loss 0.0251 (0.0293)	
training:	Epoch: [47][206/233]	Loss 0.0217 (0.0293)	
training:	Epoch: [47][207/233]	Loss 0.0422 (0.0293)	
training:	Epoch: [47][208/233]	Loss 0.0289 (0.0293)	
training:	Epoch: [47][209/233]	Loss 0.0263 (0.0293)	
training:	Epoch: [47][210/233]	Loss 0.0344 (0.0293)	
training:	Epoch: [47][211/233]	Loss 0.0222 (0.0293)	
training:	Epoch: [47][212/233]	Loss 0.0285 (0.0293)	
training:	Epoch: [47][213/233]	Loss 0.0215 (0.0293)	
training:	Epoch: [47][214/233]	Loss 0.0230 (0.0292)	
training:	Epoch: [47][215/233]	Loss 0.0380 (0.0293)	
training:	Epoch: [47][216/233]	Loss 0.0236 (0.0292)	
training:	Epoch: [47][217/233]	Loss 0.0373 (0.0293)	
training:	Epoch: [47][218/233]	Loss 0.0275 (0.0293)	
training:	Epoch: [47][219/233]	Loss 0.0496 (0.0294)	
training:	Epoch: [47][220/233]	Loss 0.0364 (0.0294)	
training:	Epoch: [47][221/233]	Loss 0.0483 (0.0295)	
training:	Epoch: [47][222/233]	Loss 0.0226 (0.0294)	
training:	Epoch: [47][223/233]	Loss 0.0258 (0.0294)	
training:	Epoch: [47][224/233]	Loss 0.0227 (0.0294)	
training:	Epoch: [47][225/233]	Loss 0.0374 (0.0294)	
training:	Epoch: [47][226/233]	Loss 0.0257 (0.0294)	
training:	Epoch: [47][227/233]	Loss 0.0247 (0.0294)	
training:	Epoch: [47][228/233]	Loss 0.0212 (0.0294)	
training:	Epoch: [47][229/233]	Loss 0.0237 (0.0293)	
training:	Epoch: [47][230/233]	Loss 0.0223 (0.0293)	
training:	Epoch: [47][231/233]	Loss 0.0259 (0.0293)	
training:	Epoch: [47][232/233]	Loss 0.0250 (0.0293)	
training:	Epoch: [47][233/233]	Loss 0.0286 (0.0293)	
Training:	 Loss: 0.0292

Training:	 ACC: 1.0000 1.0000 1.0000 1.0000
Validation:	 ACC: 0.8656 0.8646 0.8444 0.8868
Validation:	 Best_BACC: 0.8673 0.8657 0.8332 0.9013
Validation:	 Loss: 0.3208
Pretraining:	Epoch 48/200
----------
training:	Epoch: [48][1/233]	Loss 0.0220 (0.0220)	
training:	Epoch: [48][2/233]	Loss 0.0291 (0.0256)	
training:	Epoch: [48][3/233]	Loss 0.0255 (0.0255)	
training:	Epoch: [48][4/233]	Loss 0.0192 (0.0240)	
training:	Epoch: [48][5/233]	Loss 0.0223 (0.0236)	
training:	Epoch: [48][6/233]	Loss 0.0205 (0.0231)	
training:	Epoch: [48][7/233]	Loss 0.0215 (0.0229)	
training:	Epoch: [48][8/233]	Loss 0.0373 (0.0247)	
training:	Epoch: [48][9/233]	Loss 0.0230 (0.0245)	
training:	Epoch: [48][10/233]	Loss 0.0300 (0.0250)	
training:	Epoch: [48][11/233]	Loss 0.0340 (0.0258)	
training:	Epoch: [48][12/233]	Loss 0.0460 (0.0275)	
training:	Epoch: [48][13/233]	Loss 0.0341 (0.0280)	
training:	Epoch: [48][14/233]	Loss 0.0211 (0.0275)	
training:	Epoch: [48][15/233]	Loss 0.0320 (0.0278)	
training:	Epoch: [48][16/233]	Loss 0.0281 (0.0278)	
training:	Epoch: [48][17/233]	Loss 0.0262 (0.0278)	
training:	Epoch: [48][18/233]	Loss 0.0234 (0.0275)	
training:	Epoch: [48][19/233]	Loss 0.0344 (0.0279)	
training:	Epoch: [48][20/233]	Loss 0.0294 (0.0279)	
training:	Epoch: [48][21/233]	Loss 0.0243 (0.0278)	
training:	Epoch: [48][22/233]	Loss 0.0226 (0.0275)	
training:	Epoch: [48][23/233]	Loss 0.0258 (0.0275)	
training:	Epoch: [48][24/233]	Loss 0.0336 (0.0277)	
training:	Epoch: [48][25/233]	Loss 0.0279 (0.0277)	
training:	Epoch: [48][26/233]	Loss 0.0201 (0.0274)	
training:	Epoch: [48][27/233]	Loss 0.0203 (0.0272)	
training:	Epoch: [48][28/233]	Loss 0.0403 (0.0276)	
training:	Epoch: [48][29/233]	Loss 0.0406 (0.0281)	
training:	Epoch: [48][30/233]	Loss 0.0284 (0.0281)	
training:	Epoch: [48][31/233]	Loss 0.0343 (0.0283)	
training:	Epoch: [48][32/233]	Loss 0.0233 (0.0281)	
training:	Epoch: [48][33/233]	Loss 0.0308 (0.0282)	
training:	Epoch: [48][34/233]	Loss 0.0233 (0.0281)	
training:	Epoch: [48][35/233]	Loss 0.0227 (0.0279)	
training:	Epoch: [48][36/233]	Loss 0.0309 (0.0280)	
training:	Epoch: [48][37/233]	Loss 0.0244 (0.0279)	
training:	Epoch: [48][38/233]	Loss 0.0538 (0.0286)	
training:	Epoch: [48][39/233]	Loss 0.0215 (0.0284)	
training:	Epoch: [48][40/233]	Loss 0.0214 (0.0282)	
training:	Epoch: [48][41/233]	Loss 0.0273 (0.0282)	
training:	Epoch: [48][42/233]	Loss 0.0259 (0.0282)	
training:	Epoch: [48][43/233]	Loss 0.0295 (0.0282)	
training:	Epoch: [48][44/233]	Loss 0.0287 (0.0282)	
training:	Epoch: [48][45/233]	Loss 0.0891 (0.0295)	
training:	Epoch: [48][46/233]	Loss 0.0211 (0.0294)	
training:	Epoch: [48][47/233]	Loss 0.0235 (0.0292)	
training:	Epoch: [48][48/233]	Loss 0.0339 (0.0293)	
training:	Epoch: [48][49/233]	Loss 0.0219 (0.0292)	
training:	Epoch: [48][50/233]	Loss 0.0480 (0.0296)	
training:	Epoch: [48][51/233]	Loss 0.0221 (0.0294)	
training:	Epoch: [48][52/233]	Loss 0.0487 (0.0298)	
training:	Epoch: [48][53/233]	Loss 0.0224 (0.0296)	
training:	Epoch: [48][54/233]	Loss 0.0244 (0.0296)	
training:	Epoch: [48][55/233]	Loss 0.0243 (0.0295)	
training:	Epoch: [48][56/233]	Loss 0.0229 (0.0293)	
training:	Epoch: [48][57/233]	Loss 0.0202 (0.0292)	
training:	Epoch: [48][58/233]	Loss 0.0230 (0.0291)	
training:	Epoch: [48][59/233]	Loss 0.0219 (0.0289)	
training:	Epoch: [48][60/233]	Loss 0.0192 (0.0288)	
training:	Epoch: [48][61/233]	Loss 0.0457 (0.0291)	
training:	Epoch: [48][62/233]	Loss 0.0663 (0.0297)	
training:	Epoch: [48][63/233]	Loss 0.0258 (0.0296)	
training:	Epoch: [48][64/233]	Loss 0.0209 (0.0295)	
training:	Epoch: [48][65/233]	Loss 0.0235 (0.0294)	
training:	Epoch: [48][66/233]	Loss 0.0469 (0.0296)	
training:	Epoch: [48][67/233]	Loss 0.0194 (0.0295)	
training:	Epoch: [48][68/233]	Loss 0.0351 (0.0296)	
training:	Epoch: [48][69/233]	Loss 0.0232 (0.0295)	
training:	Epoch: [48][70/233]	Loss 0.0204 (0.0293)	
training:	Epoch: [48][71/233]	Loss 0.0227 (0.0293)	
training:	Epoch: [48][72/233]	Loss 0.0261 (0.0292)	
training:	Epoch: [48][73/233]	Loss 0.0209 (0.0291)	
training:	Epoch: [48][74/233]	Loss 0.0229 (0.0290)	
training:	Epoch: [48][75/233]	Loss 0.0239 (0.0289)	
training:	Epoch: [48][76/233]	Loss 0.0217 (0.0288)	
training:	Epoch: [48][77/233]	Loss 0.0235 (0.0288)	
training:	Epoch: [48][78/233]	Loss 0.0244 (0.0287)	
training:	Epoch: [48][79/233]	Loss 0.0372 (0.0288)	
training:	Epoch: [48][80/233]	Loss 0.0244 (0.0288)	
training:	Epoch: [48][81/233]	Loss 0.0319 (0.0288)	
training:	Epoch: [48][82/233]	Loss 0.0233 (0.0287)	
training:	Epoch: [48][83/233]	Loss 0.0232 (0.0287)	
training:	Epoch: [48][84/233]	Loss 0.0335 (0.0287)	
training:	Epoch: [48][85/233]	Loss 0.0236 (0.0287)	
training:	Epoch: [48][86/233]	Loss 0.0227 (0.0286)	
training:	Epoch: [48][87/233]	Loss 0.0223 (0.0285)	
training:	Epoch: [48][88/233]	Loss 0.0225 (0.0285)	
training:	Epoch: [48][89/233]	Loss 0.0250 (0.0284)	
training:	Epoch: [48][90/233]	Loss 0.0305 (0.0284)	
training:	Epoch: [48][91/233]	Loss 0.0227 (0.0284)	
training:	Epoch: [48][92/233]	Loss 0.0255 (0.0284)	
training:	Epoch: [48][93/233]	Loss 0.0282 (0.0284)	
training:	Epoch: [48][94/233]	Loss 0.0224 (0.0283)	
training:	Epoch: [48][95/233]	Loss 0.0357 (0.0284)	
training:	Epoch: [48][96/233]	Loss 0.0223 (0.0283)	
training:	Epoch: [48][97/233]	Loss 0.0237 (0.0283)	
training:	Epoch: [48][98/233]	Loss 0.0206 (0.0282)	
training:	Epoch: [48][99/233]	Loss 0.0237 (0.0281)	
training:	Epoch: [48][100/233]	Loss 0.0248 (0.0281)	
training:	Epoch: [48][101/233]	Loss 0.0196 (0.0280)	
training:	Epoch: [48][102/233]	Loss 0.0274 (0.0280)	
training:	Epoch: [48][103/233]	Loss 0.0257 (0.0280)	
training:	Epoch: [48][104/233]	Loss 0.0335 (0.0280)	
training:	Epoch: [48][105/233]	Loss 0.0231 (0.0280)	
training:	Epoch: [48][106/233]	Loss 0.0243 (0.0280)	
training:	Epoch: [48][107/233]	Loss 0.0232 (0.0279)	
training:	Epoch: [48][108/233]	Loss 0.0248 (0.0279)	
training:	Epoch: [48][109/233]	Loss 0.0210 (0.0278)	
training:	Epoch: [48][110/233]	Loss 0.0227 (0.0278)	
training:	Epoch: [48][111/233]	Loss 0.0196 (0.0277)	
training:	Epoch: [48][112/233]	Loss 0.0240 (0.0277)	
training:	Epoch: [48][113/233]	Loss 0.0300 (0.0277)	
training:	Epoch: [48][114/233]	Loss 0.0243 (0.0277)	
training:	Epoch: [48][115/233]	Loss 0.0253 (0.0276)	
training:	Epoch: [48][116/233]	Loss 0.0224 (0.0276)	
training:	Epoch: [48][117/233]	Loss 0.0229 (0.0276)	
training:	Epoch: [48][118/233]	Loss 0.0280 (0.0276)	
training:	Epoch: [48][119/233]	Loss 0.0315 (0.0276)	
training:	Epoch: [48][120/233]	Loss 0.0259 (0.0276)	
training:	Epoch: [48][121/233]	Loss 0.0274 (0.0276)	
training:	Epoch: [48][122/233]	Loss 0.0231 (0.0275)	
training:	Epoch: [48][123/233]	Loss 0.0559 (0.0278)	
training:	Epoch: [48][124/233]	Loss 0.0334 (0.0278)	
training:	Epoch: [48][125/233]	Loss 0.0301 (0.0278)	
training:	Epoch: [48][126/233]	Loss 0.0289 (0.0278)	
training:	Epoch: [48][127/233]	Loss 0.0229 (0.0278)	
training:	Epoch: [48][128/233]	Loss 0.0223 (0.0278)	
training:	Epoch: [48][129/233]	Loss 0.0256 (0.0277)	
training:	Epoch: [48][130/233]	Loss 0.0226 (0.0277)	
training:	Epoch: [48][131/233]	Loss 0.0201 (0.0276)	
training:	Epoch: [48][132/233]	Loss 0.0230 (0.0276)	
training:	Epoch: [48][133/233]	Loss 0.0292 (0.0276)	
training:	Epoch: [48][134/233]	Loss 0.0277 (0.0276)	
training:	Epoch: [48][135/233]	Loss 0.0268 (0.0276)	
training:	Epoch: [48][136/233]	Loss 0.0243 (0.0276)	
training:	Epoch: [48][137/233]	Loss 0.0201 (0.0275)	
training:	Epoch: [48][138/233]	Loss 0.0255 (0.0275)	
training:	Epoch: [48][139/233]	Loss 0.0424 (0.0276)	
training:	Epoch: [48][140/233]	Loss 0.0393 (0.0277)	
training:	Epoch: [48][141/233]	Loss 0.0446 (0.0278)	
training:	Epoch: [48][142/233]	Loss 0.0264 (0.0278)	
training:	Epoch: [48][143/233]	Loss 0.0311 (0.0278)	
training:	Epoch: [48][144/233]	Loss 0.0239 (0.0278)	
training:	Epoch: [48][145/233]	Loss 0.0208 (0.0278)	
training:	Epoch: [48][146/233]	Loss 0.0232 (0.0277)	
training:	Epoch: [48][147/233]	Loss 0.0219 (0.0277)	
training:	Epoch: [48][148/233]	Loss 0.0216 (0.0277)	
training:	Epoch: [48][149/233]	Loss 0.0246 (0.0276)	
training:	Epoch: [48][150/233]	Loss 0.0323 (0.0277)	
training:	Epoch: [48][151/233]	Loss 0.0244 (0.0276)	
training:	Epoch: [48][152/233]	Loss 0.0268 (0.0276)	
training:	Epoch: [48][153/233]	Loss 0.0322 (0.0277)	
training:	Epoch: [48][154/233]	Loss 0.0226 (0.0276)	
training:	Epoch: [48][155/233]	Loss 0.0238 (0.0276)	
training:	Epoch: [48][156/233]	Loss 0.0215 (0.0276)	
training:	Epoch: [48][157/233]	Loss 0.0487 (0.0277)	
training:	Epoch: [48][158/233]	Loss 0.0232 (0.0277)	
training:	Epoch: [48][159/233]	Loss 0.0251 (0.0277)	
training:	Epoch: [48][160/233]	Loss 0.0239 (0.0276)	
training:	Epoch: [48][161/233]	Loss 0.0236 (0.0276)	
training:	Epoch: [48][162/233]	Loss 0.0210 (0.0276)	
training:	Epoch: [48][163/233]	Loss 0.0294 (0.0276)	
training:	Epoch: [48][164/233]	Loss 0.0236 (0.0276)	
training:	Epoch: [48][165/233]	Loss 0.0213 (0.0275)	
training:	Epoch: [48][166/233]	Loss 0.0282 (0.0275)	
training:	Epoch: [48][167/233]	Loss 0.0247 (0.0275)	
training:	Epoch: [48][168/233]	Loss 0.0275 (0.0275)	
training:	Epoch: [48][169/233]	Loss 0.0284 (0.0275)	
training:	Epoch: [48][170/233]	Loss 0.0332 (0.0275)	
training:	Epoch: [48][171/233]	Loss 0.0202 (0.0275)	
training:	Epoch: [48][172/233]	Loss 0.0211 (0.0275)	
training:	Epoch: [48][173/233]	Loss 0.0255 (0.0275)	
training:	Epoch: [48][174/233]	Loss 0.0225 (0.0274)	
training:	Epoch: [48][175/233]	Loss 0.0244 (0.0274)	
training:	Epoch: [48][176/233]	Loss 0.0202 (0.0274)	
training:	Epoch: [48][177/233]	Loss 0.0208 (0.0273)	
training:	Epoch: [48][178/233]	Loss 0.0291 (0.0273)	
training:	Epoch: [48][179/233]	Loss 0.0581 (0.0275)	
training:	Epoch: [48][180/233]	Loss 0.0236 (0.0275)	
training:	Epoch: [48][181/233]	Loss 0.0237 (0.0275)	
training:	Epoch: [48][182/233]	Loss 0.0285 (0.0275)	
training:	Epoch: [48][183/233]	Loss 0.0213 (0.0274)	
training:	Epoch: [48][184/233]	Loss 0.0224 (0.0274)	
training:	Epoch: [48][185/233]	Loss 0.0251 (0.0274)	
training:	Epoch: [48][186/233]	Loss 0.0210 (0.0274)	
training:	Epoch: [48][187/233]	Loss 0.0222 (0.0273)	
training:	Epoch: [48][188/233]	Loss 0.0440 (0.0274)	
training:	Epoch: [48][189/233]	Loss 0.0225 (0.0274)	
training:	Epoch: [48][190/233]	Loss 0.0278 (0.0274)	
training:	Epoch: [48][191/233]	Loss 0.0266 (0.0274)	
training:	Epoch: [48][192/233]	Loss 0.0269 (0.0274)	
training:	Epoch: [48][193/233]	Loss 0.0352 (0.0274)	
training:	Epoch: [48][194/233]	Loss 0.0250 (0.0274)	
training:	Epoch: [48][195/233]	Loss 0.0205 (0.0274)	
training:	Epoch: [48][196/233]	Loss 0.0224 (0.0274)	
training:	Epoch: [48][197/233]	Loss 0.0273 (0.0274)	
training:	Epoch: [48][198/233]	Loss 0.0241 (0.0274)	
training:	Epoch: [48][199/233]	Loss 0.0262 (0.0273)	
training:	Epoch: [48][200/233]	Loss 0.0214 (0.0273)	
training:	Epoch: [48][201/233]	Loss 0.0300 (0.0273)	
training:	Epoch: [48][202/233]	Loss 0.0226 (0.0273)	
training:	Epoch: [48][203/233]	Loss 0.0238 (0.0273)	
training:	Epoch: [48][204/233]	Loss 0.0233 (0.0273)	
training:	Epoch: [48][205/233]	Loss 0.0225 (0.0272)	
training:	Epoch: [48][206/233]	Loss 0.0251 (0.0272)	
training:	Epoch: [48][207/233]	Loss 0.0225 (0.0272)	
training:	Epoch: [48][208/233]	Loss 0.0262 (0.0272)	
training:	Epoch: [48][209/233]	Loss 0.0259 (0.0272)	
training:	Epoch: [48][210/233]	Loss 0.0267 (0.0272)	
training:	Epoch: [48][211/233]	Loss 0.0219 (0.0272)	
training:	Epoch: [48][212/233]	Loss 0.0209 (0.0271)	
training:	Epoch: [48][213/233]	Loss 0.0511 (0.0273)	
training:	Epoch: [48][214/233]	Loss 0.0390 (0.0273)	
training:	Epoch: [48][215/233]	Loss 0.0244 (0.0273)	
training:	Epoch: [48][216/233]	Loss 0.0227 (0.0273)	
training:	Epoch: [48][217/233]	Loss 0.0235 (0.0273)	
training:	Epoch: [48][218/233]	Loss 0.0227 (0.0272)	
training:	Epoch: [48][219/233]	Loss 0.0244 (0.0272)	
training:	Epoch: [48][220/233]	Loss 0.0224 (0.0272)	
training:	Epoch: [48][221/233]	Loss 0.0243 (0.0272)	
training:	Epoch: [48][222/233]	Loss 0.0259 (0.0272)	
training:	Epoch: [48][223/233]	Loss 0.0246 (0.0272)	
training:	Epoch: [48][224/233]	Loss 0.0220 (0.0271)	
training:	Epoch: [48][225/233]	Loss 0.0248 (0.0271)	
training:	Epoch: [48][226/233]	Loss 0.0235 (0.0271)	
training:	Epoch: [48][227/233]	Loss 0.0259 (0.0271)	
training:	Epoch: [48][228/233]	Loss 0.0217 (0.0271)	
training:	Epoch: [48][229/233]	Loss 0.0200 (0.0271)	
training:	Epoch: [48][230/233]	Loss 0.0227 (0.0270)	
training:	Epoch: [48][231/233]	Loss 0.0216 (0.0270)	
training:	Epoch: [48][232/233]	Loss 0.0285 (0.0270)	
training:	Epoch: [48][233/233]	Loss 0.0300 (0.0270)	
Training:	 Loss: 0.0270

Training:	 ACC: 1.0000 1.0000 1.0000 1.0000
Validation:	 ACC: 0.8666 0.8668 0.8710 0.8621
Validation:	 Best_BACC: 0.8673 0.8657 0.8332 0.9013
Validation:	 Loss: 0.3166
Pretraining:	Epoch 49/200
----------
training:	Epoch: [49][1/233]	Loss 0.0221 (0.0221)	
training:	Epoch: [49][2/233]	Loss 0.0210 (0.0215)	
training:	Epoch: [49][3/233]	Loss 0.0243 (0.0225)	
training:	Epoch: [49][4/233]	Loss 0.0253 (0.0232)	
training:	Epoch: [49][5/233]	Loss 0.0229 (0.0231)	
training:	Epoch: [49][6/233]	Loss 0.0237 (0.0232)	
training:	Epoch: [49][7/233]	Loss 0.0261 (0.0236)	
training:	Epoch: [49][8/233]	Loss 0.0331 (0.0248)	
training:	Epoch: [49][9/233]	Loss 0.0267 (0.0250)	
training:	Epoch: [49][10/233]	Loss 0.0203 (0.0245)	
training:	Epoch: [49][11/233]	Loss 0.0231 (0.0244)	
training:	Epoch: [49][12/233]	Loss 0.0366 (0.0254)	
training:	Epoch: [49][13/233]	Loss 0.0221 (0.0252)	
training:	Epoch: [49][14/233]	Loss 0.0210 (0.0249)	
training:	Epoch: [49][15/233]	Loss 0.0188 (0.0245)	
training:	Epoch: [49][16/233]	Loss 0.0219 (0.0243)	
training:	Epoch: [49][17/233]	Loss 0.0375 (0.0251)	
training:	Epoch: [49][18/233]	Loss 0.0207 (0.0248)	
training:	Epoch: [49][19/233]	Loss 0.0284 (0.0250)	
training:	Epoch: [49][20/233]	Loss 0.0217 (0.0249)	
training:	Epoch: [49][21/233]	Loss 0.0235 (0.0248)	
training:	Epoch: [49][22/233]	Loss 0.0285 (0.0250)	
training:	Epoch: [49][23/233]	Loss 0.0210 (0.0248)	
training:	Epoch: [49][24/233]	Loss 0.0294 (0.0250)	
training:	Epoch: [49][25/233]	Loss 0.0212 (0.0248)	
training:	Epoch: [49][26/233]	Loss 0.0243 (0.0248)	
training:	Epoch: [49][27/233]	Loss 0.0241 (0.0248)	
training:	Epoch: [49][28/233]	Loss 0.0449 (0.0255)	
training:	Epoch: [49][29/233]	Loss 0.0270 (0.0256)	
training:	Epoch: [49][30/233]	Loss 0.0241 (0.0255)	
training:	Epoch: [49][31/233]	Loss 0.0180 (0.0253)	
training:	Epoch: [49][32/233]	Loss 0.0291 (0.0254)	
training:	Epoch: [49][33/233]	Loss 0.0210 (0.0253)	
training:	Epoch: [49][34/233]	Loss 0.0821 (0.0269)	
training:	Epoch: [49][35/233]	Loss 0.0223 (0.0268)	
training:	Epoch: [49][36/233]	Loss 0.0227 (0.0267)	
training:	Epoch: [49][37/233]	Loss 0.0236 (0.0266)	
training:	Epoch: [49][38/233]	Loss 0.0222 (0.0265)	
training:	Epoch: [49][39/233]	Loss 0.0246 (0.0264)	
training:	Epoch: [49][40/233]	Loss 0.0262 (0.0264)	
training:	Epoch: [49][41/233]	Loss 0.0214 (0.0263)	
training:	Epoch: [49][42/233]	Loss 0.0203 (0.0262)	
training:	Epoch: [49][43/233]	Loss 0.0219 (0.0261)	
training:	Epoch: [49][44/233]	Loss 0.0274 (0.0261)	
training:	Epoch: [49][45/233]	Loss 0.0212 (0.0260)	
training:	Epoch: [49][46/233]	Loss 0.0265 (0.0260)	
training:	Epoch: [49][47/233]	Loss 0.0203 (0.0259)	
training:	Epoch: [49][48/233]	Loss 0.0199 (0.0257)	
training:	Epoch: [49][49/233]	Loss 0.0238 (0.0257)	
training:	Epoch: [49][50/233]	Loss 0.0229 (0.0257)	
training:	Epoch: [49][51/233]	Loss 0.0193 (0.0255)	
training:	Epoch: [49][52/233]	Loss 0.0297 (0.0256)	
training:	Epoch: [49][53/233]	Loss 0.0281 (0.0257)	
training:	Epoch: [49][54/233]	Loss 0.0260 (0.0257)	
training:	Epoch: [49][55/233]	Loss 0.0211 (0.0256)	
training:	Epoch: [49][56/233]	Loss 0.0193 (0.0255)	
training:	Epoch: [49][57/233]	Loss 0.0265 (0.0255)	
training:	Epoch: [49][58/233]	Loss 0.0217 (0.0254)	
training:	Epoch: [49][59/233]	Loss 0.0280 (0.0255)	
training:	Epoch: [49][60/233]	Loss 0.0265 (0.0255)	
training:	Epoch: [49][61/233]	Loss 0.0193 (0.0254)	
training:	Epoch: [49][62/233]	Loss 0.0239 (0.0254)	
training:	Epoch: [49][63/233]	Loss 0.0765 (0.0262)	
training:	Epoch: [49][64/233]	Loss 0.0193 (0.0261)	
training:	Epoch: [49][65/233]	Loss 0.0198 (0.0260)	
training:	Epoch: [49][66/233]	Loss 0.0357 (0.0261)	
training:	Epoch: [49][67/233]	Loss 0.0207 (0.0260)	
training:	Epoch: [49][68/233]	Loss 0.0254 (0.0260)	
training:	Epoch: [49][69/233]	Loss 0.0241 (0.0260)	
training:	Epoch: [49][70/233]	Loss 0.0479 (0.0263)	
training:	Epoch: [49][71/233]	Loss 0.0231 (0.0263)	
training:	Epoch: [49][72/233]	Loss 0.0279 (0.0263)	
training:	Epoch: [49][73/233]	Loss 0.0217 (0.0262)	
training:	Epoch: [49][74/233]	Loss 0.0194 (0.0261)	
training:	Epoch: [49][75/233]	Loss 0.0243 (0.0261)	
training:	Epoch: [49][76/233]	Loss 0.0233 (0.0261)	
training:	Epoch: [49][77/233]	Loss 0.0216 (0.0260)	
training:	Epoch: [49][78/233]	Loss 0.0228 (0.0260)	
training:	Epoch: [49][79/233]	Loss 0.0291 (0.0260)	
training:	Epoch: [49][80/233]	Loss 0.0303 (0.0261)	
training:	Epoch: [49][81/233]	Loss 0.0219 (0.0260)	
training:	Epoch: [49][82/233]	Loss 0.0210 (0.0259)	
training:	Epoch: [49][83/233]	Loss 0.0249 (0.0259)	
training:	Epoch: [49][84/233]	Loss 0.0209 (0.0259)	
training:	Epoch: [49][85/233]	Loss 0.0267 (0.0259)	
training:	Epoch: [49][86/233]	Loss 0.0276 (0.0259)	
training:	Epoch: [49][87/233]	Loss 0.0332 (0.0260)	
training:	Epoch: [49][88/233]	Loss 0.0288 (0.0260)	
training:	Epoch: [49][89/233]	Loss 0.0228 (0.0260)	
training:	Epoch: [49][90/233]	Loss 0.0193 (0.0259)	
training:	Epoch: [49][91/233]	Loss 0.0258 (0.0259)	
training:	Epoch: [49][92/233]	Loss 0.0253 (0.0259)	
training:	Epoch: [49][93/233]	Loss 0.0548 (0.0262)	
training:	Epoch: [49][94/233]	Loss 0.0228 (0.0262)	
training:	Epoch: [49][95/233]	Loss 0.0267 (0.0262)	
training:	Epoch: [49][96/233]	Loss 0.0244 (0.0262)	
training:	Epoch: [49][97/233]	Loss 0.0219 (0.0261)	
training:	Epoch: [49][98/233]	Loss 0.0236 (0.0261)	
training:	Epoch: [49][99/233]	Loss 0.0212 (0.0260)	
training:	Epoch: [49][100/233]	Loss 0.0264 (0.0260)	
training:	Epoch: [49][101/233]	Loss 0.0316 (0.0261)	
training:	Epoch: [49][102/233]	Loss 0.0207 (0.0260)	
training:	Epoch: [49][103/233]	Loss 0.0272 (0.0261)	
training:	Epoch: [49][104/233]	Loss 0.0291 (0.0261)	
training:	Epoch: [49][105/233]	Loss 0.0300 (0.0261)	
training:	Epoch: [49][106/233]	Loss 0.0226 (0.0261)	
training:	Epoch: [49][107/233]	Loss 0.0226 (0.0261)	
training:	Epoch: [49][108/233]	Loss 0.0221 (0.0260)	
training:	Epoch: [49][109/233]	Loss 0.0248 (0.0260)	
training:	Epoch: [49][110/233]	Loss 0.0197 (0.0260)	
training:	Epoch: [49][111/233]	Loss 0.0227 (0.0259)	
training:	Epoch: [49][112/233]	Loss 0.0312 (0.0260)	
training:	Epoch: [49][113/233]	Loss 0.0264 (0.0260)	
training:	Epoch: [49][114/233]	Loss 0.0230 (0.0259)	
training:	Epoch: [49][115/233]	Loss 0.0224 (0.0259)	
training:	Epoch: [49][116/233]	Loss 0.0257 (0.0259)	
training:	Epoch: [49][117/233]	Loss 0.0315 (0.0260)	
training:	Epoch: [49][118/233]	Loss 0.0256 (0.0260)	
training:	Epoch: [49][119/233]	Loss 0.0207 (0.0259)	
training:	Epoch: [49][120/233]	Loss 0.0342 (0.0260)	
training:	Epoch: [49][121/233]	Loss 0.0384 (0.0261)	
training:	Epoch: [49][122/233]	Loss 0.0601 (0.0264)	
training:	Epoch: [49][123/233]	Loss 0.0251 (0.0264)	
training:	Epoch: [49][124/233]	Loss 0.0268 (0.0264)	
training:	Epoch: [49][125/233]	Loss 0.0435 (0.0265)	
training:	Epoch: [49][126/233]	Loss 0.0208 (0.0265)	
training:	Epoch: [49][127/233]	Loss 0.0272 (0.0265)	
training:	Epoch: [49][128/233]	Loss 0.0293 (0.0265)	
training:	Epoch: [49][129/233]	Loss 0.0221 (0.0264)	
training:	Epoch: [49][130/233]	Loss 0.0317 (0.0265)	
training:	Epoch: [49][131/233]	Loss 0.0307 (0.0265)	
training:	Epoch: [49][132/233]	Loss 0.0218 (0.0265)	
training:	Epoch: [49][133/233]	Loss 0.0226 (0.0265)	
training:	Epoch: [49][134/233]	Loss 0.0191 (0.0264)	
training:	Epoch: [49][135/233]	Loss 0.0212 (0.0264)	
training:	Epoch: [49][136/233]	Loss 0.0224 (0.0263)	
training:	Epoch: [49][137/233]	Loss 0.0189 (0.0263)	
training:	Epoch: [49][138/233]	Loss 0.0269 (0.0263)	
training:	Epoch: [49][139/233]	Loss 0.0222 (0.0263)	
training:	Epoch: [49][140/233]	Loss 0.0236 (0.0262)	
training:	Epoch: [49][141/233]	Loss 0.0536 (0.0264)	
training:	Epoch: [49][142/233]	Loss 0.0217 (0.0264)	
training:	Epoch: [49][143/233]	Loss 0.0233 (0.0264)	
training:	Epoch: [49][144/233]	Loss 0.0201 (0.0263)	
training:	Epoch: [49][145/233]	Loss 0.0384 (0.0264)	
training:	Epoch: [49][146/233]	Loss 0.0278 (0.0264)	
training:	Epoch: [49][147/233]	Loss 0.0223 (0.0264)	
training:	Epoch: [49][148/233]	Loss 0.0211 (0.0264)	
training:	Epoch: [49][149/233]	Loss 0.0298 (0.0264)	
training:	Epoch: [49][150/233]	Loss 0.0214 (0.0263)	
training:	Epoch: [49][151/233]	Loss 0.0287 (0.0264)	
training:	Epoch: [49][152/233]	Loss 0.0226 (0.0263)	
training:	Epoch: [49][153/233]	Loss 0.0241 (0.0263)	
training:	Epoch: [49][154/233]	Loss 0.0188 (0.0263)	
training:	Epoch: [49][155/233]	Loss 0.0324 (0.0263)	
training:	Epoch: [49][156/233]	Loss 0.0290 (0.0263)	
training:	Epoch: [49][157/233]	Loss 0.0199 (0.0263)	
training:	Epoch: [49][158/233]	Loss 0.0246 (0.0263)	
training:	Epoch: [49][159/233]	Loss 0.0209 (0.0262)	
training:	Epoch: [49][160/233]	Loss 0.0240 (0.0262)	
training:	Epoch: [49][161/233]	Loss 0.0226 (0.0262)	
training:	Epoch: [49][162/233]	Loss 0.0442 (0.0263)	
training:	Epoch: [49][163/233]	Loss 0.0208 (0.0263)	
training:	Epoch: [49][164/233]	Loss 0.0234 (0.0263)	
training:	Epoch: [49][165/233]	Loss 0.0251 (0.0263)	
training:	Epoch: [49][166/233]	Loss 0.0243 (0.0263)	
training:	Epoch: [49][167/233]	Loss 0.0248 (0.0262)	
training:	Epoch: [49][168/233]	Loss 0.0248 (0.0262)	
training:	Epoch: [49][169/233]	Loss 0.0210 (0.0262)	
training:	Epoch: [49][170/233]	Loss 0.0316 (0.0262)	
training:	Epoch: [49][171/233]	Loss 0.0224 (0.0262)	
training:	Epoch: [49][172/233]	Loss 0.0223 (0.0262)	
training:	Epoch: [49][173/233]	Loss 0.0208 (0.0262)	
training:	Epoch: [49][174/233]	Loss 0.0216 (0.0261)	
training:	Epoch: [49][175/233]	Loss 0.0271 (0.0261)	
training:	Epoch: [49][176/233]	Loss 0.0252 (0.0261)	
training:	Epoch: [49][177/233]	Loss 0.0265 (0.0261)	
training:	Epoch: [49][178/233]	Loss 0.0216 (0.0261)	
training:	Epoch: [49][179/233]	Loss 0.0229 (0.0261)	
training:	Epoch: [49][180/233]	Loss 0.0260 (0.0261)	
training:	Epoch: [49][181/233]	Loss 0.0244 (0.0261)	
training:	Epoch: [49][182/233]	Loss 0.0217 (0.0261)	
training:	Epoch: [49][183/233]	Loss 0.0230 (0.0260)	
training:	Epoch: [49][184/233]	Loss 0.0464 (0.0262)	
training:	Epoch: [49][185/233]	Loss 0.0286 (0.0262)	
training:	Epoch: [49][186/233]	Loss 0.0207 (0.0261)	
training:	Epoch: [49][187/233]	Loss 0.0230 (0.0261)	
training:	Epoch: [49][188/233]	Loss 0.0214 (0.0261)	
training:	Epoch: [49][189/233]	Loss 0.0255 (0.0261)	
training:	Epoch: [49][190/233]	Loss 0.0246 (0.0261)	
training:	Epoch: [49][191/233]	Loss 0.0222 (0.0261)	
training:	Epoch: [49][192/233]	Loss 0.0233 (0.0260)	
training:	Epoch: [49][193/233]	Loss 0.0244 (0.0260)	
training:	Epoch: [49][194/233]	Loss 0.0215 (0.0260)	
training:	Epoch: [49][195/233]	Loss 0.0244 (0.0260)	
training:	Epoch: [49][196/233]	Loss 0.0249 (0.0260)	
training:	Epoch: [49][197/233]	Loss 0.0215 (0.0260)	
training:	Epoch: [49][198/233]	Loss 0.0334 (0.0260)	
training:	Epoch: [49][199/233]	Loss 0.0244 (0.0260)	
training:	Epoch: [49][200/233]	Loss 0.0192 (0.0260)	
training:	Epoch: [49][201/233]	Loss 0.0223 (0.0260)	
training:	Epoch: [49][202/233]	Loss 0.0235 (0.0259)	
training:	Epoch: [49][203/233]	Loss 0.0287 (0.0260)	
training:	Epoch: [49][204/233]	Loss 0.0190 (0.0259)	
training:	Epoch: [49][205/233]	Loss 0.0217 (0.0259)	
training:	Epoch: [49][206/233]	Loss 0.0194 (0.0259)	
training:	Epoch: [49][207/233]	Loss 0.0255 (0.0259)	
training:	Epoch: [49][208/233]	Loss 0.0230 (0.0259)	
training:	Epoch: [49][209/233]	Loss 0.0298 (0.0259)	
training:	Epoch: [49][210/233]	Loss 0.0217 (0.0259)	
training:	Epoch: [49][211/233]	Loss 0.0269 (0.0259)	
training:	Epoch: [49][212/233]	Loss 0.0225 (0.0258)	
training:	Epoch: [49][213/233]	Loss 0.0385 (0.0259)	
training:	Epoch: [49][214/233]	Loss 0.0264 (0.0259)	
training:	Epoch: [49][215/233]	Loss 0.0232 (0.0259)	
training:	Epoch: [49][216/233]	Loss 0.0228 (0.0259)	
training:	Epoch: [49][217/233]	Loss 0.0211 (0.0259)	
training:	Epoch: [49][218/233]	Loss 0.0226 (0.0258)	
training:	Epoch: [49][219/233]	Loss 0.0231 (0.0258)	
training:	Epoch: [49][220/233]	Loss 0.0213 (0.0258)	
training:	Epoch: [49][221/233]	Loss 0.0230 (0.0258)	
training:	Epoch: [49][222/233]	Loss 0.0278 (0.0258)	
training:	Epoch: [49][223/233]	Loss 0.0265 (0.0258)	
training:	Epoch: [49][224/233]	Loss 0.0215 (0.0258)	
training:	Epoch: [49][225/233]	Loss 0.0273 (0.0258)	
training:	Epoch: [49][226/233]	Loss 0.0222 (0.0258)	
training:	Epoch: [49][227/233]	Loss 0.0207 (0.0258)	
training:	Epoch: [49][228/233]	Loss 0.0343 (0.0258)	
training:	Epoch: [49][229/233]	Loss 0.0233 (0.0258)	
training:	Epoch: [49][230/233]	Loss 0.0222 (0.0258)	
training:	Epoch: [49][231/233]	Loss 0.0305 (0.0258)	
training:	Epoch: [49][232/233]	Loss 0.0258 (0.0258)	
training:	Epoch: [49][233/233]	Loss 0.0249 (0.0258)	
Training:	 Loss: 0.0257

Training:	 ACC: 1.0000 1.0000 1.0000 1.0000
Validation:	 ACC: 0.8665 0.8652 0.8373 0.8957
Validation:	 Best_BACC: 0.8673 0.8657 0.8332 0.9013
Validation:	 Loss: 0.3238
Pretraining:	Epoch 50/200
----------
training:	Epoch: [50][1/233]	Loss 0.0218 (0.0218)	
training:	Epoch: [50][2/233]	Loss 0.0193 (0.0206)	
training:	Epoch: [50][3/233]	Loss 0.0189 (0.0200)	
training:	Epoch: [50][4/233]	Loss 0.0257 (0.0214)	
training:	Epoch: [50][5/233]	Loss 0.0292 (0.0230)	
training:	Epoch: [50][6/233]	Loss 0.0227 (0.0229)	
training:	Epoch: [50][7/233]	Loss 0.0228 (0.0229)	
training:	Epoch: [50][8/233]	Loss 0.0193 (0.0225)	
training:	Epoch: [50][9/233]	Loss 0.0210 (0.0223)	
training:	Epoch: [50][10/233]	Loss 0.0838 (0.0285)	
training:	Epoch: [50][11/233]	Loss 0.0198 (0.0277)	
training:	Epoch: [50][12/233]	Loss 0.0332 (0.0281)	
training:	Epoch: [50][13/233]	Loss 0.0231 (0.0277)	
training:	Epoch: [50][14/233]	Loss 0.0230 (0.0274)	
training:	Epoch: [50][15/233]	Loss 0.0245 (0.0272)	
training:	Epoch: [50][16/233]	Loss 0.0182 (0.0266)	
training:	Epoch: [50][17/233]	Loss 0.0229 (0.0264)	
training:	Epoch: [50][18/233]	Loss 0.0243 (0.0263)	
training:	Epoch: [50][19/233]	Loss 0.0362 (0.0268)	
training:	Epoch: [50][20/233]	Loss 0.0289 (0.0269)	
training:	Epoch: [50][21/233]	Loss 0.0211 (0.0267)	
training:	Epoch: [50][22/233]	Loss 0.0239 (0.0265)	
training:	Epoch: [50][23/233]	Loss 0.0209 (0.0263)	
training:	Epoch: [50][24/233]	Loss 0.0218 (0.0261)	
training:	Epoch: [50][25/233]	Loss 0.0241 (0.0260)	
training:	Epoch: [50][26/233]	Loss 0.0207 (0.0258)	
training:	Epoch: [50][27/233]	Loss 0.0189 (0.0256)	
training:	Epoch: [50][28/233]	Loss 0.0199 (0.0254)	
training:	Epoch: [50][29/233]	Loss 0.0306 (0.0255)	
training:	Epoch: [50][30/233]	Loss 0.0208 (0.0254)	
training:	Epoch: [50][31/233]	Loss 0.0244 (0.0253)	
training:	Epoch: [50][32/233]	Loss 0.0275 (0.0254)	
training:	Epoch: [50][33/233]	Loss 0.0268 (0.0255)	
training:	Epoch: [50][34/233]	Loss 0.0207 (0.0253)	
training:	Epoch: [50][35/233]	Loss 0.0380 (0.0257)	
training:	Epoch: [50][36/233]	Loss 0.0223 (0.0256)	
training:	Epoch: [50][37/233]	Loss 0.0192 (0.0254)	
training:	Epoch: [50][38/233]	Loss 0.0205 (0.0253)	
training:	Epoch: [50][39/233]	Loss 0.0570 (0.0261)	
training:	Epoch: [50][40/233]	Loss 0.0264 (0.0261)	
training:	Epoch: [50][41/233]	Loss 0.0212 (0.0260)	
training:	Epoch: [50][42/233]	Loss 0.0231 (0.0259)	
training:	Epoch: [50][43/233]	Loss 0.0201 (0.0258)	
training:	Epoch: [50][44/233]	Loss 0.0225 (0.0257)	
training:	Epoch: [50][45/233]	Loss 0.0348 (0.0259)	
training:	Epoch: [50][46/233]	Loss 0.0280 (0.0260)	
training:	Epoch: [50][47/233]	Loss 0.0224 (0.0259)	
training:	Epoch: [50][48/233]	Loss 0.0349 (0.0261)	
training:	Epoch: [50][49/233]	Loss 0.0265 (0.0261)	
training:	Epoch: [50][50/233]	Loss 0.0212 (0.0260)	
training:	Epoch: [50][51/233]	Loss 0.0196 (0.0259)	
training:	Epoch: [50][52/233]	Loss 0.0240 (0.0258)	
training:	Epoch: [50][53/233]	Loss 0.0302 (0.0259)	
training:	Epoch: [50][54/233]	Loss 0.0367 (0.0261)	
training:	Epoch: [50][55/233]	Loss 0.0245 (0.0261)	
training:	Epoch: [50][56/233]	Loss 0.0234 (0.0260)	
training:	Epoch: [50][57/233]	Loss 0.0199 (0.0259)	
training:	Epoch: [50][58/233]	Loss 0.0296 (0.0260)	
training:	Epoch: [50][59/233]	Loss 0.0321 (0.0261)	
training:	Epoch: [50][60/233]	Loss 0.0279 (0.0261)	
training:	Epoch: [50][61/233]	Loss 0.0282 (0.0261)	
training:	Epoch: [50][62/233]	Loss 0.0281 (0.0262)	
training:	Epoch: [50][63/233]	Loss 0.0203 (0.0261)	
training:	Epoch: [50][64/233]	Loss 0.0273 (0.0261)	
training:	Epoch: [50][65/233]	Loss 0.0230 (0.0261)	
training:	Epoch: [50][66/233]	Loss 0.0392 (0.0263)	
training:	Epoch: [50][67/233]	Loss 0.0214 (0.0262)	
training:	Epoch: [50][68/233]	Loss 0.0440 (0.0264)	
training:	Epoch: [50][69/233]	Loss 0.0188 (0.0263)	
training:	Epoch: [50][70/233]	Loss 0.0207 (0.0263)	
training:	Epoch: [50][71/233]	Loss 0.0298 (0.0263)	
training:	Epoch: [50][72/233]	Loss 0.0237 (0.0263)	
training:	Epoch: [50][73/233]	Loss 0.0211 (0.0262)	
training:	Epoch: [50][74/233]	Loss 0.0388 (0.0264)	
training:	Epoch: [50][75/233]	Loss 0.0226 (0.0263)	
training:	Epoch: [50][76/233]	Loss 0.0357 (0.0264)	
training:	Epoch: [50][77/233]	Loss 0.0210 (0.0264)	
training:	Epoch: [50][78/233]	Loss 0.0234 (0.0263)	
training:	Epoch: [50][79/233]	Loss 0.0197 (0.0262)	
training:	Epoch: [50][80/233]	Loss 0.0212 (0.0262)	
training:	Epoch: [50][81/233]	Loss 0.0192 (0.0261)	
training:	Epoch: [50][82/233]	Loss 0.0226 (0.0261)	
training:	Epoch: [50][83/233]	Loss 0.0487 (0.0263)	
training:	Epoch: [50][84/233]	Loss 0.0286 (0.0264)	
training:	Epoch: [50][85/233]	Loss 0.0346 (0.0265)	
training:	Epoch: [50][86/233]	Loss 0.0204 (0.0264)	
training:	Epoch: [50][87/233]	Loss 0.0232 (0.0263)	
training:	Epoch: [50][88/233]	Loss 0.0246 (0.0263)	
training:	Epoch: [50][89/233]	Loss 0.0192 (0.0262)	
training:	Epoch: [50][90/233]	Loss 0.0250 (0.0262)	
training:	Epoch: [50][91/233]	Loss 0.0185 (0.0261)	
training:	Epoch: [50][92/233]	Loss 0.0228 (0.0261)	
training:	Epoch: [50][93/233]	Loss 0.0453 (0.0263)	
training:	Epoch: [50][94/233]	Loss 0.0204 (0.0263)	
training:	Epoch: [50][95/233]	Loss 0.0200 (0.0262)	
training:	Epoch: [50][96/233]	Loss 0.0245 (0.0262)	
training:	Epoch: [50][97/233]	Loss 0.0205 (0.0261)	
training:	Epoch: [50][98/233]	Loss 0.0233 (0.0261)	
training:	Epoch: [50][99/233]	Loss 0.0196 (0.0260)	
training:	Epoch: [50][100/233]	Loss 0.0251 (0.0260)	
training:	Epoch: [50][101/233]	Loss 0.0191 (0.0259)	
training:	Epoch: [50][102/233]	Loss 0.0217 (0.0259)	
training:	Epoch: [50][103/233]	Loss 0.0214 (0.0259)	
training:	Epoch: [50][104/233]	Loss 0.0213 (0.0258)	
training:	Epoch: [50][105/233]	Loss 0.0191 (0.0257)	
training:	Epoch: [50][106/233]	Loss 0.0189 (0.0257)	
training:	Epoch: [50][107/233]	Loss 0.0237 (0.0257)	
training:	Epoch: [50][108/233]	Loss 0.0249 (0.0257)	
training:	Epoch: [50][109/233]	Loss 0.0189 (0.0256)	
training:	Epoch: [50][110/233]	Loss 0.0315 (0.0256)	
training:	Epoch: [50][111/233]	Loss 0.0240 (0.0256)	
training:	Epoch: [50][112/233]	Loss 0.0203 (0.0256)	
training:	Epoch: [50][113/233]	Loss 0.0207 (0.0255)	
training:	Epoch: [50][114/233]	Loss 0.0250 (0.0255)	
training:	Epoch: [50][115/233]	Loss 0.0250 (0.0255)	
training:	Epoch: [50][116/233]	Loss 0.0255 (0.0255)	
training:	Epoch: [50][117/233]	Loss 0.0196 (0.0255)	
training:	Epoch: [50][118/233]	Loss 0.0239 (0.0255)	
training:	Epoch: [50][119/233]	Loss 0.0410 (0.0256)	
training:	Epoch: [50][120/233]	Loss 0.0199 (0.0256)	
training:	Epoch: [50][121/233]	Loss 0.0209 (0.0255)	
training:	Epoch: [50][122/233]	Loss 0.0454 (0.0257)	
training:	Epoch: [50][123/233]	Loss 0.0197 (0.0256)	
training:	Epoch: [50][124/233]	Loss 0.0211 (0.0256)	
training:	Epoch: [50][125/233]	Loss 0.0232 (0.0256)	
training:	Epoch: [50][126/233]	Loss 0.0216 (0.0255)	
training:	Epoch: [50][127/233]	Loss 0.0337 (0.0256)	
training:	Epoch: [50][128/233]	Loss 0.0220 (0.0256)	
training:	Epoch: [50][129/233]	Loss 0.0433 (0.0257)	
training:	Epoch: [50][130/233]	Loss 0.0232 (0.0257)	
training:	Epoch: [50][131/233]	Loss 0.0212 (0.0257)	
training:	Epoch: [50][132/233]	Loss 0.0256 (0.0257)	
training:	Epoch: [50][133/233]	Loss 0.0180 (0.0256)	
training:	Epoch: [50][134/233]	Loss 0.0322 (0.0257)	
training:	Epoch: [50][135/233]	Loss 0.0349 (0.0257)	
training:	Epoch: [50][136/233]	Loss 0.0189 (0.0257)	
training:	Epoch: [50][137/233]	Loss 0.0180 (0.0256)	
training:	Epoch: [50][138/233]	Loss 0.0228 (0.0256)	
training:	Epoch: [50][139/233]	Loss 0.0271 (0.0256)	
training:	Epoch: [50][140/233]	Loss 0.0315 (0.0256)	
training:	Epoch: [50][141/233]	Loss 0.0334 (0.0257)	
training:	Epoch: [50][142/233]	Loss 0.0265 (0.0257)	
training:	Epoch: [50][143/233]	Loss 0.0306 (0.0257)	
training:	Epoch: [50][144/233]	Loss 0.0231 (0.0257)	
training:	Epoch: [50][145/233]	Loss 0.0270 (0.0257)	
training:	Epoch: [50][146/233]	Loss 0.0260 (0.0257)	
training:	Epoch: [50][147/233]	Loss 0.0218 (0.0257)	
training:	Epoch: [50][148/233]	Loss 0.0227 (0.0257)	
training:	Epoch: [50][149/233]	Loss 0.0241 (0.0257)	
training:	Epoch: [50][150/233]	Loss 0.0219 (0.0257)	
training:	Epoch: [50][151/233]	Loss 0.0225 (0.0256)	
training:	Epoch: [50][152/233]	Loss 0.0214 (0.0256)	
training:	Epoch: [50][153/233]	Loss 0.0232 (0.0256)	
training:	Epoch: [50][154/233]	Loss 0.0302 (0.0256)	
training:	Epoch: [50][155/233]	Loss 0.0262 (0.0256)	
training:	Epoch: [50][156/233]	Loss 0.0251 (0.0256)	
training:	Epoch: [50][157/233]	Loss 0.0271 (0.0256)	
training:	Epoch: [50][158/233]	Loss 0.0271 (0.0256)	
training:	Epoch: [50][159/233]	Loss 0.0192 (0.0256)	
training:	Epoch: [50][160/233]	Loss 0.0202 (0.0256)	
training:	Epoch: [50][161/233]	Loss 0.0265 (0.0256)	
training:	Epoch: [50][162/233]	Loss 0.0261 (0.0256)	
training:	Epoch: [50][163/233]	Loss 0.0204 (0.0255)	
training:	Epoch: [50][164/233]	Loss 0.0317 (0.0256)	
training:	Epoch: [50][165/233]	Loss 0.0259 (0.0256)	
training:	Epoch: [50][166/233]	Loss 0.0242 (0.0256)	
training:	Epoch: [50][167/233]	Loss 0.0204 (0.0255)	
training:	Epoch: [50][168/233]	Loss 0.0232 (0.0255)	
training:	Epoch: [50][169/233]	Loss 0.0213 (0.0255)	
training:	Epoch: [50][170/233]	Loss 0.0247 (0.0255)	
training:	Epoch: [50][171/233]	Loss 0.0194 (0.0255)	
training:	Epoch: [50][172/233]	Loss 0.0199 (0.0254)	
training:	Epoch: [50][173/233]	Loss 0.0181 (0.0254)	
training:	Epoch: [50][174/233]	Loss 0.0219 (0.0254)	
training:	Epoch: [50][175/233]	Loss 0.0277 (0.0254)	
training:	Epoch: [50][176/233]	Loss 0.0252 (0.0254)	
training:	Epoch: [50][177/233]	Loss 0.0191 (0.0253)	
training:	Epoch: [50][178/233]	Loss 0.0201 (0.0253)	
training:	Epoch: [50][179/233]	Loss 0.0197 (0.0253)	
training:	Epoch: [50][180/233]	Loss 0.0194 (0.0253)	
training:	Epoch: [50][181/233]	Loss 0.0206 (0.0252)	
training:	Epoch: [50][182/233]	Loss 0.0224 (0.0252)	
training:	Epoch: [50][183/233]	Loss 0.0305 (0.0252)	
training:	Epoch: [50][184/233]	Loss 0.0205 (0.0252)	
training:	Epoch: [50][185/233]	Loss 0.0290 (0.0252)	
training:	Epoch: [50][186/233]	Loss 0.0260 (0.0252)	
training:	Epoch: [50][187/233]	Loss 0.0299 (0.0253)	
training:	Epoch: [50][188/233]	Loss 0.0274 (0.0253)	
training:	Epoch: [50][189/233]	Loss 0.0217 (0.0253)	
training:	Epoch: [50][190/233]	Loss 0.0172 (0.0252)	
training:	Epoch: [50][191/233]	Loss 0.0219 (0.0252)	
training:	Epoch: [50][192/233]	Loss 0.0206 (0.0252)	
training:	Epoch: [50][193/233]	Loss 0.0217 (0.0252)	
training:	Epoch: [50][194/233]	Loss 0.0277 (0.0252)	
training:	Epoch: [50][195/233]	Loss 0.0289 (0.0252)	
training:	Epoch: [50][196/233]	Loss 0.0224 (0.0252)	
training:	Epoch: [50][197/233]	Loss 0.0208 (0.0251)	
training:	Epoch: [50][198/233]	Loss 0.0352 (0.0252)	
training:	Epoch: [50][199/233]	Loss 0.0212 (0.0252)	
training:	Epoch: [50][200/233]	Loss 0.0264 (0.0252)	
training:	Epoch: [50][201/233]	Loss 0.0199 (0.0252)	
training:	Epoch: [50][202/233]	Loss 0.0466 (0.0253)	
training:	Epoch: [50][203/233]	Loss 0.0214 (0.0252)	
training:	Epoch: [50][204/233]	Loss 0.0213 (0.0252)	
training:	Epoch: [50][205/233]	Loss 0.0291 (0.0252)	
training:	Epoch: [50][206/233]	Loss 0.0188 (0.0252)	
training:	Epoch: [50][207/233]	Loss 0.0208 (0.0252)	
training:	Epoch: [50][208/233]	Loss 0.0232 (0.0252)	
training:	Epoch: [50][209/233]	Loss 0.0235 (0.0252)	
training:	Epoch: [50][210/233]	Loss 0.0193 (0.0251)	
training:	Epoch: [50][211/233]	Loss 0.0219 (0.0251)	
training:	Epoch: [50][212/233]	Loss 0.0252 (0.0251)	
training:	Epoch: [50][213/233]	Loss 0.0239 (0.0251)	
training:	Epoch: [50][214/233]	Loss 0.0179 (0.0251)	
training:	Epoch: [50][215/233]	Loss 0.0264 (0.0251)	
training:	Epoch: [50][216/233]	Loss 0.0189 (0.0251)	
training:	Epoch: [50][217/233]	Loss 0.0345 (0.0251)	
training:	Epoch: [50][218/233]	Loss 0.0214 (0.0251)	
training:	Epoch: [50][219/233]	Loss 0.0378 (0.0252)	
training:	Epoch: [50][220/233]	Loss 0.0225 (0.0251)	
training:	Epoch: [50][221/233]	Loss 0.0202 (0.0251)	
training:	Epoch: [50][222/233]	Loss 0.0279 (0.0251)	
training:	Epoch: [50][223/233]	Loss 0.0288 (0.0251)	
training:	Epoch: [50][224/233]	Loss 0.0285 (0.0252)	
training:	Epoch: [50][225/233]	Loss 0.0215 (0.0251)	
training:	Epoch: [50][226/233]	Loss 0.0407 (0.0252)	
training:	Epoch: [50][227/233]	Loss 0.0188 (0.0252)	
training:	Epoch: [50][228/233]	Loss 0.0561 (0.0253)	
training:	Epoch: [50][229/233]	Loss 0.0229 (0.0253)	
training:	Epoch: [50][230/233]	Loss 0.0202 (0.0253)	
training:	Epoch: [50][231/233]	Loss 0.0235 (0.0253)	
training:	Epoch: [50][232/233]	Loss 0.0205 (0.0253)	
training:	Epoch: [50][233/233]	Loss 0.0296 (0.0253)	
Training:	 Loss: 0.0252

Training:	 ACC: 1.0000 1.0000 1.0000 1.0000
Validation:	 ACC: 0.8650 0.8641 0.8444 0.8857
Validation:	 Best_BACC: 0.8673 0.8657 0.8332 0.9013
Validation:	 Loss: 0.3201
Pretraining:	Epoch 51/200
----------
training:	Epoch: [51][1/233]	Loss 0.0320 (0.0320)	
training:	Epoch: [51][2/233]	Loss 0.0543 (0.0432)	
training:	Epoch: [51][3/233]	Loss 0.0221 (0.0361)	
training:	Epoch: [51][4/233]	Loss 0.0203 (0.0322)	
training:	Epoch: [51][5/233]	Loss 0.0226 (0.0302)	
training:	Epoch: [51][6/233]	Loss 0.0487 (0.0333)	
training:	Epoch: [51][7/233]	Loss 0.0177 (0.0311)	
training:	Epoch: [51][8/233]	Loss 0.0234 (0.0301)	
training:	Epoch: [51][9/233]	Loss 0.0213 (0.0292)	
training:	Epoch: [51][10/233]	Loss 0.0207 (0.0283)	
training:	Epoch: [51][11/233]	Loss 0.0190 (0.0275)	
training:	Epoch: [51][12/233]	Loss 0.0413 (0.0286)	
training:	Epoch: [51][13/233]	Loss 0.0214 (0.0281)	
training:	Epoch: [51][14/233]	Loss 0.0205 (0.0275)	
training:	Epoch: [51][15/233]	Loss 0.0262 (0.0274)	
training:	Epoch: [51][16/233]	Loss 0.0439 (0.0285)	
training:	Epoch: [51][17/233]	Loss 0.0192 (0.0279)	
training:	Epoch: [51][18/233]	Loss 0.0307 (0.0281)	
training:	Epoch: [51][19/233]	Loss 0.0240 (0.0279)	
training:	Epoch: [51][20/233]	Loss 0.0281 (0.0279)	
training:	Epoch: [51][21/233]	Loss 0.0319 (0.0281)	
training:	Epoch: [51][22/233]	Loss 0.0190 (0.0276)	
training:	Epoch: [51][23/233]	Loss 0.0176 (0.0272)	
training:	Epoch: [51][24/233]	Loss 0.0393 (0.0277)	
training:	Epoch: [51][25/233]	Loss 0.0228 (0.0275)	
training:	Epoch: [51][26/233]	Loss 0.0188 (0.0272)	
training:	Epoch: [51][27/233]	Loss 0.0197 (0.0269)	
training:	Epoch: [51][28/233]	Loss 0.0185 (0.0266)	
training:	Epoch: [51][29/233]	Loss 0.0230 (0.0265)	
training:	Epoch: [51][30/233]	Loss 0.0173 (0.0262)	
training:	Epoch: [51][31/233]	Loss 0.0171 (0.0259)	
training:	Epoch: [51][32/233]	Loss 0.0218 (0.0258)	
training:	Epoch: [51][33/233]	Loss 0.0224 (0.0257)	
training:	Epoch: [51][34/233]	Loss 0.0243 (0.0256)	
training:	Epoch: [51][35/233]	Loss 0.0204 (0.0255)	
training:	Epoch: [51][36/233]	Loss 0.0190 (0.0253)	
training:	Epoch: [51][37/233]	Loss 0.0308 (0.0254)	
training:	Epoch: [51][38/233]	Loss 0.0204 (0.0253)	
training:	Epoch: [51][39/233]	Loss 0.0221 (0.0252)	
training:	Epoch: [51][40/233]	Loss 0.0243 (0.0252)	
training:	Epoch: [51][41/233]	Loss 0.0220 (0.0251)	
training:	Epoch: [51][42/233]	Loss 0.0199 (0.0250)	
training:	Epoch: [51][43/233]	Loss 0.0391 (0.0253)	
training:	Epoch: [51][44/233]	Loss 0.0245 (0.0253)	
training:	Epoch: [51][45/233]	Loss 0.0324 (0.0255)	
training:	Epoch: [51][46/233]	Loss 0.0276 (0.0255)	
training:	Epoch: [51][47/233]	Loss 0.0247 (0.0255)	
training:	Epoch: [51][48/233]	Loss 0.0297 (0.0256)	
training:	Epoch: [51][49/233]	Loss 0.0332 (0.0257)	
training:	Epoch: [51][50/233]	Loss 0.0189 (0.0256)	
training:	Epoch: [51][51/233]	Loss 0.0201 (0.0255)	
training:	Epoch: [51][52/233]	Loss 0.0192 (0.0254)	
training:	Epoch: [51][53/233]	Loss 0.0206 (0.0253)	
training:	Epoch: [51][54/233]	Loss 0.0217 (0.0252)	
training:	Epoch: [51][55/233]	Loss 0.0413 (0.0255)	
training:	Epoch: [51][56/233]	Loss 0.0424 (0.0258)	
training:	Epoch: [51][57/233]	Loss 0.0305 (0.0259)	
training:	Epoch: [51][58/233]	Loss 0.0298 (0.0260)	
training:	Epoch: [51][59/233]	Loss 0.0370 (0.0261)	
training:	Epoch: [51][60/233]	Loss 0.0221 (0.0261)	
training:	Epoch: [51][61/233]	Loss 0.0177 (0.0259)	
training:	Epoch: [51][62/233]	Loss 0.0246 (0.0259)	
training:	Epoch: [51][63/233]	Loss 0.0366 (0.0261)	
training:	Epoch: [51][64/233]	Loss 0.0193 (0.0260)	
training:	Epoch: [51][65/233]	Loss 0.0189 (0.0259)	
training:	Epoch: [51][66/233]	Loss 0.0274 (0.0259)	
training:	Epoch: [51][67/233]	Loss 0.0195 (0.0258)	
training:	Epoch: [51][68/233]	Loss 0.0293 (0.0259)	
training:	Epoch: [51][69/233]	Loss 0.0227 (0.0258)	
training:	Epoch: [51][70/233]	Loss 0.0178 (0.0257)	
training:	Epoch: [51][71/233]	Loss 0.0230 (0.0257)	
training:	Epoch: [51][72/233]	Loss 0.0216 (0.0256)	
training:	Epoch: [51][73/233]	Loss 0.0208 (0.0255)	
training:	Epoch: [51][74/233]	Loss 0.0299 (0.0256)	
training:	Epoch: [51][75/233]	Loss 0.0213 (0.0255)	
training:	Epoch: [51][76/233]	Loss 0.0217 (0.0255)	
training:	Epoch: [51][77/233]	Loss 0.0297 (0.0255)	
training:	Epoch: [51][78/233]	Loss 0.0350 (0.0257)	
training:	Epoch: [51][79/233]	Loss 0.0187 (0.0256)	
training:	Epoch: [51][80/233]	Loss 0.0364 (0.0257)	
training:	Epoch: [51][81/233]	Loss 0.0349 (0.0258)	
training:	Epoch: [51][82/233]	Loss 0.0316 (0.0259)	
training:	Epoch: [51][83/233]	Loss 0.0187 (0.0258)	
training:	Epoch: [51][84/233]	Loss 0.0196 (0.0257)	
training:	Epoch: [51][85/233]	Loss 0.0198 (0.0257)	
training:	Epoch: [51][86/233]	Loss 0.0220 (0.0256)	
training:	Epoch: [51][87/233]	Loss 0.0283 (0.0256)	
training:	Epoch: [51][88/233]	Loss 0.0178 (0.0256)	
training:	Epoch: [51][89/233]	Loss 0.0248 (0.0255)	
training:	Epoch: [51][90/233]	Loss 0.0387 (0.0257)	
training:	Epoch: [51][91/233]	Loss 0.0172 (0.0256)	
training:	Epoch: [51][92/233]	Loss 0.0181 (0.0255)	
training:	Epoch: [51][93/233]	Loss 0.0197 (0.0255)	
training:	Epoch: [51][94/233]	Loss 0.0210 (0.0254)	
training:	Epoch: [51][95/233]	Loss 0.0449 (0.0256)	
training:	Epoch: [51][96/233]	Loss 0.0404 (0.0258)	
training:	Epoch: [51][97/233]	Loss 0.0215 (0.0257)	
training:	Epoch: [51][98/233]	Loss 0.0235 (0.0257)	
training:	Epoch: [51][99/233]	Loss 0.0223 (0.0257)	
training:	Epoch: [51][100/233]	Loss 0.0221 (0.0256)	
training:	Epoch: [51][101/233]	Loss 0.0204 (0.0256)	
training:	Epoch: [51][102/233]	Loss 0.0209 (0.0255)	
training:	Epoch: [51][103/233]	Loss 0.0210 (0.0255)	
training:	Epoch: [51][104/233]	Loss 0.0199 (0.0254)	
training:	Epoch: [51][105/233]	Loss 0.0223 (0.0254)	
training:	Epoch: [51][106/233]	Loss 0.0224 (0.0254)	
training:	Epoch: [51][107/233]	Loss 0.0177 (0.0253)	
training:	Epoch: [51][108/233]	Loss 0.0213 (0.0253)	
training:	Epoch: [51][109/233]	Loss 0.0220 (0.0252)	
training:	Epoch: [51][110/233]	Loss 0.0203 (0.0252)	
training:	Epoch: [51][111/233]	Loss 0.0572 (0.0255)	
training:	Epoch: [51][112/233]	Loss 0.0200 (0.0254)	
training:	Epoch: [51][113/233]	Loss 0.0605 (0.0257)	
training:	Epoch: [51][114/233]	Loss 0.0202 (0.0257)	
training:	Epoch: [51][115/233]	Loss 0.0563 (0.0260)	
training:	Epoch: [51][116/233]	Loss 0.0214 (0.0259)	
training:	Epoch: [51][117/233]	Loss 0.0238 (0.0259)	
training:	Epoch: [51][118/233]	Loss 0.0338 (0.0260)	
training:	Epoch: [51][119/233]	Loss 0.0270 (0.0260)	
training:	Epoch: [51][120/233]	Loss 0.0238 (0.0260)	
training:	Epoch: [51][121/233]	Loss 0.0377 (0.0261)	
training:	Epoch: [51][122/233]	Loss 0.0200 (0.0260)	
training:	Epoch: [51][123/233]	Loss 0.0332 (0.0261)	
training:	Epoch: [51][124/233]	Loss 0.0332 (0.0261)	
training:	Epoch: [51][125/233]	Loss 0.0519 (0.0263)	
training:	Epoch: [51][126/233]	Loss 0.0498 (0.0265)	
training:	Epoch: [51][127/233]	Loss 0.0193 (0.0265)	
training:	Epoch: [51][128/233]	Loss 0.0178 (0.0264)	
training:	Epoch: [51][129/233]	Loss 0.0212 (0.0264)	
training:	Epoch: [51][130/233]	Loss 0.0178 (0.0263)	
training:	Epoch: [51][131/233]	Loss 0.0198 (0.0262)	
training:	Epoch: [51][132/233]	Loss 0.0175 (0.0262)	
training:	Epoch: [51][133/233]	Loss 0.0288 (0.0262)	
training:	Epoch: [51][134/233]	Loss 0.0199 (0.0261)	
training:	Epoch: [51][135/233]	Loss 0.0188 (0.0261)	
training:	Epoch: [51][136/233]	Loss 0.0221 (0.0261)	
training:	Epoch: [51][137/233]	Loss 0.0330 (0.0261)	
training:	Epoch: [51][138/233]	Loss 0.0258 (0.0261)	
training:	Epoch: [51][139/233]	Loss 0.0178 (0.0261)	
training:	Epoch: [51][140/233]	Loss 0.0193 (0.0260)	
training:	Epoch: [51][141/233]	Loss 0.0195 (0.0260)	
training:	Epoch: [51][142/233]	Loss 0.0215 (0.0259)	
training:	Epoch: [51][143/233]	Loss 0.0201 (0.0259)	
training:	Epoch: [51][144/233]	Loss 0.0239 (0.0259)	
training:	Epoch: [51][145/233]	Loss 0.0250 (0.0259)	
training:	Epoch: [51][146/233]	Loss 0.0196 (0.0258)	
training:	Epoch: [51][147/233]	Loss 0.0234 (0.0258)	
training:	Epoch: [51][148/233]	Loss 0.0341 (0.0259)	
training:	Epoch: [51][149/233]	Loss 0.0230 (0.0258)	
training:	Epoch: [51][150/233]	Loss 0.0220 (0.0258)	
training:	Epoch: [51][151/233]	Loss 0.0281 (0.0258)	
training:	Epoch: [51][152/233]	Loss 0.0211 (0.0258)	
training:	Epoch: [51][153/233]	Loss 0.0195 (0.0258)	
training:	Epoch: [51][154/233]	Loss 0.0219 (0.0257)	
training:	Epoch: [51][155/233]	Loss 0.0263 (0.0257)	
training:	Epoch: [51][156/233]	Loss 0.0211 (0.0257)	
training:	Epoch: [51][157/233]	Loss 0.0230 (0.0257)	
training:	Epoch: [51][158/233]	Loss 0.0190 (0.0256)	
training:	Epoch: [51][159/233]	Loss 0.0238 (0.0256)	
training:	Epoch: [51][160/233]	Loss 0.0425 (0.0257)	
training:	Epoch: [51][161/233]	Loss 0.0367 (0.0258)	
training:	Epoch: [51][162/233]	Loss 0.0821 (0.0262)	
training:	Epoch: [51][163/233]	Loss 0.0182 (0.0261)	
training:	Epoch: [51][164/233]	Loss 0.0453 (0.0262)	
training:	Epoch: [51][165/233]	Loss 0.0515 (0.0264)	
training:	Epoch: [51][166/233]	Loss 0.0292 (0.0264)	
training:	Epoch: [51][167/233]	Loss 0.0268 (0.0264)	
training:	Epoch: [51][168/233]	Loss 0.0224 (0.0264)	
training:	Epoch: [51][169/233]	Loss 0.0251 (0.0264)	
training:	Epoch: [51][170/233]	Loss 0.0174 (0.0263)	
training:	Epoch: [51][171/233]	Loss 0.0176 (0.0263)	
training:	Epoch: [51][172/233]	Loss 0.0222 (0.0262)	
training:	Epoch: [51][173/233]	Loss 0.0509 (0.0264)	
training:	Epoch: [51][174/233]	Loss 0.0207 (0.0263)	
training:	Epoch: [51][175/233]	Loss 0.0216 (0.0263)	
training:	Epoch: [51][176/233]	Loss 0.0204 (0.0263)	
training:	Epoch: [51][177/233]	Loss 0.0223 (0.0263)	
training:	Epoch: [51][178/233]	Loss 0.0221 (0.0262)	
training:	Epoch: [51][179/233]	Loss 0.0201 (0.0262)	
training:	Epoch: [51][180/233]	Loss 0.0228 (0.0262)	
training:	Epoch: [51][181/233]	Loss 0.0195 (0.0262)	
training:	Epoch: [51][182/233]	Loss 0.0181 (0.0261)	
training:	Epoch: [51][183/233]	Loss 0.0244 (0.0261)	
training:	Epoch: [51][184/233]	Loss 0.0550 (0.0263)	
training:	Epoch: [51][185/233]	Loss 0.0246 (0.0262)	
training:	Epoch: [51][186/233]	Loss 0.0209 (0.0262)	
training:	Epoch: [51][187/233]	Loss 0.0205 (0.0262)	
training:	Epoch: [51][188/233]	Loss 0.0192 (0.0261)	
training:	Epoch: [51][189/233]	Loss 0.0254 (0.0261)	
training:	Epoch: [51][190/233]	Loss 0.0229 (0.0261)	
training:	Epoch: [51][191/233]	Loss 0.0288 (0.0261)	
training:	Epoch: [51][192/233]	Loss 0.0217 (0.0261)	
training:	Epoch: [51][193/233]	Loss 0.0304 (0.0261)	
training:	Epoch: [51][194/233]	Loss 0.0333 (0.0262)	
training:	Epoch: [51][195/233]	Loss 0.0192 (0.0261)	
training:	Epoch: [51][196/233]	Loss 0.0213 (0.0261)	
training:	Epoch: [51][197/233]	Loss 0.0230 (0.0261)	
training:	Epoch: [51][198/233]	Loss 0.0199 (0.0261)	
training:	Epoch: [51][199/233]	Loss 0.0275 (0.0261)	
training:	Epoch: [51][200/233]	Loss 0.0233 (0.0261)	
training:	Epoch: [51][201/233]	Loss 0.0208 (0.0260)	
training:	Epoch: [51][202/233]	Loss 0.0241 (0.0260)	
training:	Epoch: [51][203/233]	Loss 0.0233 (0.0260)	
training:	Epoch: [51][204/233]	Loss 0.0194 (0.0260)	
training:	Epoch: [51][205/233]	Loss 0.0384 (0.0260)	
training:	Epoch: [51][206/233]	Loss 0.0190 (0.0260)	
training:	Epoch: [51][207/233]	Loss 0.0277 (0.0260)	
training:	Epoch: [51][208/233]	Loss 0.0242 (0.0260)	
training:	Epoch: [51][209/233]	Loss 0.0211 (0.0260)	
training:	Epoch: [51][210/233]	Loss 0.0185 (0.0259)	
training:	Epoch: [51][211/233]	Loss 0.0225 (0.0259)	
training:	Epoch: [51][212/233]	Loss 0.0180 (0.0259)	
training:	Epoch: [51][213/233]	Loss 0.0218 (0.0259)	
training:	Epoch: [51][214/233]	Loss 0.0211 (0.0259)	
training:	Epoch: [51][215/233]	Loss 0.0266 (0.0259)	
training:	Epoch: [51][216/233]	Loss 0.0253 (0.0259)	
training:	Epoch: [51][217/233]	Loss 0.0209 (0.0258)	
training:	Epoch: [51][218/233]	Loss 0.0265 (0.0258)	
training:	Epoch: [51][219/233]	Loss 0.0231 (0.0258)	
training:	Epoch: [51][220/233]	Loss 0.0331 (0.0259)	
training:	Epoch: [51][221/233]	Loss 0.0280 (0.0259)	
training:	Epoch: [51][222/233]	Loss 0.0189 (0.0258)	
training:	Epoch: [51][223/233]	Loss 0.0270 (0.0258)	
training:	Epoch: [51][224/233]	Loss 0.0187 (0.0258)	
training:	Epoch: [51][225/233]	Loss 0.0202 (0.0258)	
training:	Epoch: [51][226/233]	Loss 0.0247 (0.0258)	
training:	Epoch: [51][227/233]	Loss 0.0198 (0.0258)	
training:	Epoch: [51][228/233]	Loss 0.0673 (0.0259)	
training:	Epoch: [51][229/233]	Loss 0.0199 (0.0259)	
training:	Epoch: [51][230/233]	Loss 0.0221 (0.0259)	
training:	Epoch: [51][231/233]	Loss 0.0239 (0.0259)	
training:	Epoch: [51][232/233]	Loss 0.0208 (0.0259)	
training:	Epoch: [51][233/233]	Loss 0.0306 (0.0259)	
Training:	 Loss: 0.0258

Training:	 ACC: 1.0000 1.0000 1.0000 1.0000
Validation:	 ACC: 0.8625 0.8636 0.8864 0.8386
Validation:	 Best_BACC: 0.8673 0.8657 0.8332 0.9013
Validation:	 Loss: 0.3175
Pretraining:	Epoch 52/200
----------
training:	Epoch: [52][1/233]	Loss 0.0185 (0.0185)	
training:	Epoch: [52][2/233]	Loss 0.0193 (0.0189)	
training:	Epoch: [52][3/233]	Loss 0.0204 (0.0194)	
training:	Epoch: [52][4/233]	Loss 0.0262 (0.0211)	
training:	Epoch: [52][5/233]	Loss 0.0285 (0.0226)	
training:	Epoch: [52][6/233]	Loss 0.0195 (0.0221)	
training:	Epoch: [52][7/233]	Loss 0.0188 (0.0216)	
training:	Epoch: [52][8/233]	Loss 0.0262 (0.0222)	
training:	Epoch: [52][9/233]	Loss 0.0203 (0.0220)	
training:	Epoch: [52][10/233]	Loss 0.0325 (0.0230)	
training:	Epoch: [52][11/233]	Loss 0.0416 (0.0247)	
training:	Epoch: [52][12/233]	Loss 0.0229 (0.0246)	
training:	Epoch: [52][13/233]	Loss 0.0334 (0.0252)	
training:	Epoch: [52][14/233]	Loss 0.0213 (0.0250)	
training:	Epoch: [52][15/233]	Loss 0.0221 (0.0248)	
training:	Epoch: [52][16/233]	Loss 0.0309 (0.0251)	
training:	Epoch: [52][17/233]	Loss 0.0279 (0.0253)	
training:	Epoch: [52][18/233]	Loss 0.0211 (0.0251)	
training:	Epoch: [52][19/233]	Loss 0.0208 (0.0248)	
training:	Epoch: [52][20/233]	Loss 0.0387 (0.0255)	
training:	Epoch: [52][21/233]	Loss 0.0195 (0.0253)	
training:	Epoch: [52][22/233]	Loss 0.0254 (0.0253)	
training:	Epoch: [52][23/233]	Loss 0.0262 (0.0253)	
training:	Epoch: [52][24/233]	Loss 0.0188 (0.0250)	
training:	Epoch: [52][25/233]	Loss 0.0857 (0.0275)	
training:	Epoch: [52][26/233]	Loss 0.0172 (0.0271)	
training:	Epoch: [52][27/233]	Loss 0.0174 (0.0267)	
training:	Epoch: [52][28/233]	Loss 0.0734 (0.0284)	
training:	Epoch: [52][29/233]	Loss 0.0242 (0.0282)	
training:	Epoch: [52][30/233]	Loss 0.0182 (0.0279)	
training:	Epoch: [52][31/233]	Loss 0.0617 (0.0290)	
training:	Epoch: [52][32/233]	Loss 0.0304 (0.0290)	
training:	Epoch: [52][33/233]	Loss 0.0189 (0.0287)	
training:	Epoch: [52][34/233]	Loss 0.0198 (0.0285)	
training:	Epoch: [52][35/233]	Loss 0.0205 (0.0282)	
training:	Epoch: [52][36/233]	Loss 0.0193 (0.0280)	
training:	Epoch: [52][37/233]	Loss 0.0187 (0.0277)	
training:	Epoch: [52][38/233]	Loss 0.0217 (0.0276)	
training:	Epoch: [52][39/233]	Loss 0.0210 (0.0274)	
training:	Epoch: [52][40/233]	Loss 0.0269 (0.0274)	
training:	Epoch: [52][41/233]	Loss 0.0247 (0.0273)	
training:	Epoch: [52][42/233]	Loss 0.0265 (0.0273)	
training:	Epoch: [52][43/233]	Loss 0.0170 (0.0271)	
training:	Epoch: [52][44/233]	Loss 0.0241 (0.0270)	
training:	Epoch: [52][45/233]	Loss 0.0238 (0.0269)	
training:	Epoch: [52][46/233]	Loss 0.0246 (0.0269)	
training:	Epoch: [52][47/233]	Loss 0.0212 (0.0268)	
training:	Epoch: [52][48/233]	Loss 0.0330 (0.0269)	
training:	Epoch: [52][49/233]	Loss 0.0194 (0.0267)	
training:	Epoch: [52][50/233]	Loss 0.0196 (0.0266)	
training:	Epoch: [52][51/233]	Loss 0.0407 (0.0269)	
training:	Epoch: [52][52/233]	Loss 0.0174 (0.0267)	
training:	Epoch: [52][53/233]	Loss 0.0183 (0.0265)	
training:	Epoch: [52][54/233]	Loss 0.0221 (0.0264)	
training:	Epoch: [52][55/233]	Loss 0.0301 (0.0265)	
training:	Epoch: [52][56/233]	Loss 0.0438 (0.0268)	
training:	Epoch: [52][57/233]	Loss 0.0224 (0.0267)	
training:	Epoch: [52][58/233]	Loss 0.0247 (0.0267)	
training:	Epoch: [52][59/233]	Loss 0.0401 (0.0269)	
training:	Epoch: [52][60/233]	Loss 0.0209 (0.0268)	
training:	Epoch: [52][61/233]	Loss 0.0181 (0.0267)	
training:	Epoch: [52][62/233]	Loss 0.0269 (0.0267)	
training:	Epoch: [52][63/233]	Loss 0.0245 (0.0267)	
training:	Epoch: [52][64/233]	Loss 0.0243 (0.0266)	
training:	Epoch: [52][65/233]	Loss 0.0187 (0.0265)	
training:	Epoch: [52][66/233]	Loss 0.0260 (0.0265)	
training:	Epoch: [52][67/233]	Loss 0.0200 (0.0264)	
training:	Epoch: [52][68/233]	Loss 0.0195 (0.0263)	
training:	Epoch: [52][69/233]	Loss 0.0381 (0.0265)	
training:	Epoch: [52][70/233]	Loss 0.0188 (0.0264)	
training:	Epoch: [52][71/233]	Loss 0.0183 (0.0262)	
training:	Epoch: [52][72/233]	Loss 0.0179 (0.0261)	
training:	Epoch: [52][73/233]	Loss 0.0242 (0.0261)	
training:	Epoch: [52][74/233]	Loss 0.0189 (0.0260)	
training:	Epoch: [52][75/233]	Loss 0.0317 (0.0261)	
training:	Epoch: [52][76/233]	Loss 0.0196 (0.0260)	
training:	Epoch: [52][77/233]	Loss 0.0467 (0.0263)	
training:	Epoch: [52][78/233]	Loss 0.0229 (0.0262)	
training:	Epoch: [52][79/233]	Loss 0.0342 (0.0263)	
training:	Epoch: [52][80/233]	Loss 0.0183 (0.0262)	
training:	Epoch: [52][81/233]	Loss 0.0400 (0.0264)	
training:	Epoch: [52][82/233]	Loss 0.0218 (0.0263)	
training:	Epoch: [52][83/233]	Loss 0.0194 (0.0262)	
training:	Epoch: [52][84/233]	Loss 0.0223 (0.0262)	
training:	Epoch: [52][85/233]	Loss 0.0364 (0.0263)	
training:	Epoch: [52][86/233]	Loss 0.0215 (0.0263)	
training:	Epoch: [52][87/233]	Loss 0.0184 (0.0262)	
training:	Epoch: [52][88/233]	Loss 0.0173 (0.0261)	
training:	Epoch: [52][89/233]	Loss 0.0169 (0.0260)	
training:	Epoch: [52][90/233]	Loss 0.0220 (0.0259)	
training:	Epoch: [52][91/233]	Loss 0.0313 (0.0260)	
training:	Epoch: [52][92/233]	Loss 0.0287 (0.0260)	
training:	Epoch: [52][93/233]	Loss 0.0214 (0.0260)	
training:	Epoch: [52][94/233]	Loss 0.0335 (0.0260)	
training:	Epoch: [52][95/233]	Loss 0.0179 (0.0260)	
training:	Epoch: [52][96/233]	Loss 0.0218 (0.0259)	
training:	Epoch: [52][97/233]	Loss 0.0173 (0.0258)	
training:	Epoch: [52][98/233]	Loss 0.0250 (0.0258)	
training:	Epoch: [52][99/233]	Loss 0.0368 (0.0259)	
training:	Epoch: [52][100/233]	Loss 0.0228 (0.0259)	
training:	Epoch: [52][101/233]	Loss 0.0210 (0.0259)	
training:	Epoch: [52][102/233]	Loss 0.0377 (0.0260)	
training:	Epoch: [52][103/233]	Loss 0.0210 (0.0259)	
training:	Epoch: [52][104/233]	Loss 0.0210 (0.0259)	
training:	Epoch: [52][105/233]	Loss 0.0228 (0.0258)	
training:	Epoch: [52][106/233]	Loss 0.0204 (0.0258)	
training:	Epoch: [52][107/233]	Loss 0.0175 (0.0257)	
training:	Epoch: [52][108/233]	Loss 0.0431 (0.0259)	
training:	Epoch: [52][109/233]	Loss 0.0181 (0.0258)	
training:	Epoch: [52][110/233]	Loss 0.0169 (0.0257)	
training:	Epoch: [52][111/233]	Loss 0.0235 (0.0257)	
training:	Epoch: [52][112/233]	Loss 0.0285 (0.0257)	
training:	Epoch: [52][113/233]	Loss 0.0242 (0.0257)	
training:	Epoch: [52][114/233]	Loss 0.0217 (0.0257)	
training:	Epoch: [52][115/233]	Loss 0.0188 (0.0256)	
training:	Epoch: [52][116/233]	Loss 0.0232 (0.0256)	
training:	Epoch: [52][117/233]	Loss 0.0185 (0.0255)	
training:	Epoch: [52][118/233]	Loss 0.0191 (0.0255)	
training:	Epoch: [52][119/233]	Loss 0.0257 (0.0255)	
training:	Epoch: [52][120/233]	Loss 0.0218 (0.0255)	
training:	Epoch: [52][121/233]	Loss 0.0228 (0.0254)	
training:	Epoch: [52][122/233]	Loss 0.0190 (0.0254)	
training:	Epoch: [52][123/233]	Loss 0.0201 (0.0253)	
training:	Epoch: [52][124/233]	Loss 0.0283 (0.0254)	
training:	Epoch: [52][125/233]	Loss 0.0196 (0.0253)	
training:	Epoch: [52][126/233]	Loss 0.0193 (0.0253)	
training:	Epoch: [52][127/233]	Loss 0.0188 (0.0252)	
training:	Epoch: [52][128/233]	Loss 0.0217 (0.0252)	
training:	Epoch: [52][129/233]	Loss 0.0193 (0.0251)	
training:	Epoch: [52][130/233]	Loss 0.0262 (0.0252)	
training:	Epoch: [52][131/233]	Loss 0.0208 (0.0251)	
training:	Epoch: [52][132/233]	Loss 0.0230 (0.0251)	
training:	Epoch: [52][133/233]	Loss 0.0216 (0.0251)	
training:	Epoch: [52][134/233]	Loss 0.0204 (0.0250)	
training:	Epoch: [52][135/233]	Loss 0.0184 (0.0250)	
training:	Epoch: [52][136/233]	Loss 0.0288 (0.0250)	
training:	Epoch: [52][137/233]	Loss 0.0188 (0.0250)	
training:	Epoch: [52][138/233]	Loss 0.0210 (0.0249)	
training:	Epoch: [52][139/233]	Loss 0.0202 (0.0249)	
training:	Epoch: [52][140/233]	Loss 0.0265 (0.0249)	
training:	Epoch: [52][141/233]	Loss 0.0248 (0.0249)	
training:	Epoch: [52][142/233]	Loss 0.0172 (0.0249)	
training:	Epoch: [52][143/233]	Loss 0.0256 (0.0249)	
training:	Epoch: [52][144/233]	Loss 0.0262 (0.0249)	
training:	Epoch: [52][145/233]	Loss 0.0236 (0.0249)	
training:	Epoch: [52][146/233]	Loss 0.0191 (0.0248)	
training:	Epoch: [52][147/233]	Loss 0.0211 (0.0248)	
training:	Epoch: [52][148/233]	Loss 0.0176 (0.0248)	
training:	Epoch: [52][149/233]	Loss 0.0225 (0.0247)	
training:	Epoch: [52][150/233]	Loss 0.0252 (0.0247)	
training:	Epoch: [52][151/233]	Loss 0.0268 (0.0248)	
training:	Epoch: [52][152/233]	Loss 0.0178 (0.0247)	
training:	Epoch: [52][153/233]	Loss 0.0223 (0.0247)	
training:	Epoch: [52][154/233]	Loss 0.0182 (0.0247)	
training:	Epoch: [52][155/233]	Loss 0.0240 (0.0247)	
training:	Epoch: [52][156/233]	Loss 0.0195 (0.0246)	
training:	Epoch: [52][157/233]	Loss 0.0245 (0.0246)	
training:	Epoch: [52][158/233]	Loss 0.0193 (0.0246)	
training:	Epoch: [52][159/233]	Loss 0.0194 (0.0246)	
training:	Epoch: [52][160/233]	Loss 0.0561 (0.0247)	
training:	Epoch: [52][161/233]	Loss 0.0251 (0.0248)	
training:	Epoch: [52][162/233]	Loss 0.0216 (0.0247)	
training:	Epoch: [52][163/233]	Loss 0.0234 (0.0247)	
training:	Epoch: [52][164/233]	Loss 0.0240 (0.0247)	
training:	Epoch: [52][165/233]	Loss 0.0224 (0.0247)	
training:	Epoch: [52][166/233]	Loss 0.0178 (0.0247)	
training:	Epoch: [52][167/233]	Loss 0.0242 (0.0247)	
training:	Epoch: [52][168/233]	Loss 0.0186 (0.0246)	
training:	Epoch: [52][169/233]	Loss 0.0182 (0.0246)	
training:	Epoch: [52][170/233]	Loss 0.0179 (0.0245)	
training:	Epoch: [52][171/233]	Loss 0.0206 (0.0245)	
training:	Epoch: [52][172/233]	Loss 0.0234 (0.0245)	
training:	Epoch: [52][173/233]	Loss 0.0208 (0.0245)	
training:	Epoch: [52][174/233]	Loss 0.0241 (0.0245)	
training:	Epoch: [52][175/233]	Loss 0.0212 (0.0245)	
training:	Epoch: [52][176/233]	Loss 0.0181 (0.0244)	
training:	Epoch: [52][177/233]	Loss 0.0196 (0.0244)	
training:	Epoch: [52][178/233]	Loss 0.0251 (0.0244)	
training:	Epoch: [52][179/233]	Loss 0.0200 (0.0244)	
training:	Epoch: [52][180/233]	Loss 0.0477 (0.0245)	
training:	Epoch: [52][181/233]	Loss 0.0585 (0.0247)	
training:	Epoch: [52][182/233]	Loss 0.0189 (0.0247)	
training:	Epoch: [52][183/233]	Loss 0.0299 (0.0247)	
training:	Epoch: [52][184/233]	Loss 0.0192 (0.0247)	
training:	Epoch: [52][185/233]	Loss 0.0193 (0.0246)	
training:	Epoch: [52][186/233]	Loss 0.0249 (0.0246)	
training:	Epoch: [52][187/233]	Loss 0.0212 (0.0246)	
training:	Epoch: [52][188/233]	Loss 0.0201 (0.0246)	
training:	Epoch: [52][189/233]	Loss 0.0203 (0.0246)	
training:	Epoch: [52][190/233]	Loss 0.0208 (0.0246)	
training:	Epoch: [52][191/233]	Loss 0.0182 (0.0245)	
training:	Epoch: [52][192/233]	Loss 0.0193 (0.0245)	
training:	Epoch: [52][193/233]	Loss 0.0200 (0.0245)	
training:	Epoch: [52][194/233]	Loss 0.0247 (0.0245)	
training:	Epoch: [52][195/233]	Loss 0.0202 (0.0245)	
training:	Epoch: [52][196/233]	Loss 0.0266 (0.0245)	
training:	Epoch: [52][197/233]	Loss 0.0400 (0.0245)	
training:	Epoch: [52][198/233]	Loss 0.0195 (0.0245)	
training:	Epoch: [52][199/233]	Loss 0.0517 (0.0247)	
training:	Epoch: [52][200/233]	Loss 0.0187 (0.0246)	
training:	Epoch: [52][201/233]	Loss 0.0189 (0.0246)	
training:	Epoch: [52][202/233]	Loss 0.0216 (0.0246)	
training:	Epoch: [52][203/233]	Loss 0.0250 (0.0246)	
training:	Epoch: [52][204/233]	Loss 0.0287 (0.0246)	
training:	Epoch: [52][205/233]	Loss 0.0195 (0.0246)	
training:	Epoch: [52][206/233]	Loss 0.0218 (0.0246)	
training:	Epoch: [52][207/233]	Loss 0.0256 (0.0246)	
training:	Epoch: [52][208/233]	Loss 0.0194 (0.0245)	
training:	Epoch: [52][209/233]	Loss 0.0243 (0.0245)	
training:	Epoch: [52][210/233]	Loss 0.0190 (0.0245)	
training:	Epoch: [52][211/233]	Loss 0.0173 (0.0245)	
training:	Epoch: [52][212/233]	Loss 0.0229 (0.0245)	
training:	Epoch: [52][213/233]	Loss 0.0217 (0.0245)	
training:	Epoch: [52][214/233]	Loss 0.0228 (0.0245)	
training:	Epoch: [52][215/233]	Loss 0.0185 (0.0244)	
training:	Epoch: [52][216/233]	Loss 0.0239 (0.0244)	
training:	Epoch: [52][217/233]	Loss 0.0212 (0.0244)	
training:	Epoch: [52][218/233]	Loss 0.0200 (0.0244)	
training:	Epoch: [52][219/233]	Loss 0.0226 (0.0244)	
training:	Epoch: [52][220/233]	Loss 0.0272 (0.0244)	
training:	Epoch: [52][221/233]	Loss 0.0177 (0.0244)	
training:	Epoch: [52][222/233]	Loss 0.0264 (0.0244)	
training:	Epoch: [52][223/233]	Loss 0.0376 (0.0244)	
training:	Epoch: [52][224/233]	Loss 0.0219 (0.0244)	
training:	Epoch: [52][225/233]	Loss 0.0220 (0.0244)	
training:	Epoch: [52][226/233]	Loss 0.0241 (0.0244)	
training:	Epoch: [52][227/233]	Loss 0.0194 (0.0244)	
training:	Epoch: [52][228/233]	Loss 0.0221 (0.0244)	
training:	Epoch: [52][229/233]	Loss 0.0206 (0.0244)	
training:	Epoch: [52][230/233]	Loss 0.0175 (0.0243)	
training:	Epoch: [52][231/233]	Loss 0.0208 (0.0243)	
training:	Epoch: [52][232/233]	Loss 0.0203 (0.0243)	
training:	Epoch: [52][233/233]	Loss 0.0249 (0.0243)	
Training:	 Loss: 0.0242

Training:	 ACC: 1.0000 1.0000 1.0000 1.0000
Validation:	 ACC: 0.8649 0.8652 0.8710 0.8587
Validation:	 Best_BACC: 0.8673 0.8657 0.8332 0.9013
Validation:	 Loss: 0.3167
Pretraining:	Epoch 53/200
----------
training:	Epoch: [53][1/233]	Loss 0.0176 (0.0176)	
training:	Epoch: [53][2/233]	Loss 0.0185 (0.0181)	
training:	Epoch: [53][3/233]	Loss 0.0182 (0.0181)	
training:	Epoch: [53][4/233]	Loss 0.0198 (0.0185)	
training:	Epoch: [53][5/233]	Loss 0.0169 (0.0182)	
training:	Epoch: [53][6/233]	Loss 0.0378 (0.0215)	
training:	Epoch: [53][7/233]	Loss 0.0213 (0.0214)	
training:	Epoch: [53][8/233]	Loss 0.0229 (0.0216)	
training:	Epoch: [53][9/233]	Loss 0.0196 (0.0214)	
training:	Epoch: [53][10/233]	Loss 0.0244 (0.0217)	
training:	Epoch: [53][11/233]	Loss 0.0184 (0.0214)	
training:	Epoch: [53][12/233]	Loss 0.0176 (0.0211)	
training:	Epoch: [53][13/233]	Loss 0.0202 (0.0210)	
training:	Epoch: [53][14/233]	Loss 0.0154 (0.0206)	
training:	Epoch: [53][15/233]	Loss 0.0232 (0.0208)	
training:	Epoch: [53][16/233]	Loss 0.0157 (0.0205)	
training:	Epoch: [53][17/233]	Loss 0.0419 (0.0217)	
training:	Epoch: [53][18/233]	Loss 0.0250 (0.0219)	
training:	Epoch: [53][19/233]	Loss 0.0179 (0.0217)	
training:	Epoch: [53][20/233]	Loss 0.0249 (0.0219)	
training:	Epoch: [53][21/233]	Loss 0.0223 (0.0219)	
training:	Epoch: [53][22/233]	Loss 0.0178 (0.0217)	
training:	Epoch: [53][23/233]	Loss 0.0206 (0.0216)	
training:	Epoch: [53][24/233]	Loss 0.0201 (0.0216)	
training:	Epoch: [53][25/233]	Loss 0.0271 (0.0218)	
training:	Epoch: [53][26/233]	Loss 0.0174 (0.0216)	
training:	Epoch: [53][27/233]	Loss 0.0200 (0.0216)	
training:	Epoch: [53][28/233]	Loss 0.0182 (0.0214)	
training:	Epoch: [53][29/233]	Loss 0.0175 (0.0213)	
training:	Epoch: [53][30/233]	Loss 0.0309 (0.0216)	
training:	Epoch: [53][31/233]	Loss 0.0503 (0.0226)	
training:	Epoch: [53][32/233]	Loss 0.0201 (0.0225)	
training:	Epoch: [53][33/233]	Loss 0.0190 (0.0224)	
training:	Epoch: [53][34/233]	Loss 0.0197 (0.0223)	
training:	Epoch: [53][35/233]	Loss 0.0184 (0.0222)	
training:	Epoch: [53][36/233]	Loss 0.0250 (0.0223)	
training:	Epoch: [53][37/233]	Loss 0.0216 (0.0222)	
training:	Epoch: [53][38/233]	Loss 0.0229 (0.0223)	
training:	Epoch: [53][39/233]	Loss 0.0214 (0.0222)	
training:	Epoch: [53][40/233]	Loss 0.0175 (0.0221)	
training:	Epoch: [53][41/233]	Loss 0.0188 (0.0220)	
training:	Epoch: [53][42/233]	Loss 0.0234 (0.0221)	
training:	Epoch: [53][43/233]	Loss 0.0190 (0.0220)	
training:	Epoch: [53][44/233]	Loss 0.0204 (0.0220)	
training:	Epoch: [53][45/233]	Loss 0.0183 (0.0219)	
training:	Epoch: [53][46/233]	Loss 0.0186 (0.0218)	
training:	Epoch: [53][47/233]	Loss 0.0227 (0.0218)	
training:	Epoch: [53][48/233]	Loss 0.0228 (0.0219)	
training:	Epoch: [53][49/233]	Loss 0.0183 (0.0218)	
training:	Epoch: [53][50/233]	Loss 0.0258 (0.0219)	
training:	Epoch: [53][51/233]	Loss 0.0201 (0.0218)	
training:	Epoch: [53][52/233]	Loss 0.0211 (0.0218)	
training:	Epoch: [53][53/233]	Loss 0.0205 (0.0218)	
training:	Epoch: [53][54/233]	Loss 0.0236 (0.0218)	
training:	Epoch: [53][55/233]	Loss 0.0191 (0.0218)	
training:	Epoch: [53][56/233]	Loss 0.0348 (0.0220)	
training:	Epoch: [53][57/233]	Loss 0.0203 (0.0220)	
training:	Epoch: [53][58/233]	Loss 0.0203 (0.0219)	
training:	Epoch: [53][59/233]	Loss 0.0241 (0.0220)	
training:	Epoch: [53][60/233]	Loss 0.0254 (0.0220)	
training:	Epoch: [53][61/233]	Loss 0.0201 (0.0220)	
training:	Epoch: [53][62/233]	Loss 0.0173 (0.0219)	
training:	Epoch: [53][63/233]	Loss 0.0184 (0.0219)	
training:	Epoch: [53][64/233]	Loss 0.0181 (0.0218)	
training:	Epoch: [53][65/233]	Loss 0.0281 (0.0219)	
training:	Epoch: [53][66/233]	Loss 0.0234 (0.0219)	
training:	Epoch: [53][67/233]	Loss 0.0188 (0.0219)	
training:	Epoch: [53][68/233]	Loss 0.0275 (0.0220)	
training:	Epoch: [53][69/233]	Loss 0.0161 (0.0219)	
training:	Epoch: [53][70/233]	Loss 0.0271 (0.0220)	
training:	Epoch: [53][71/233]	Loss 0.0183 (0.0219)	
training:	Epoch: [53][72/233]	Loss 0.0283 (0.0220)	
training:	Epoch: [53][73/233]	Loss 0.0187 (0.0220)	
training:	Epoch: [53][74/233]	Loss 0.0207 (0.0219)	
training:	Epoch: [53][75/233]	Loss 0.0229 (0.0219)	
training:	Epoch: [53][76/233]	Loss 0.0177 (0.0219)	
training:	Epoch: [53][77/233]	Loss 0.0168 (0.0218)	
training:	Epoch: [53][78/233]	Loss 0.0318 (0.0220)	
training:	Epoch: [53][79/233]	Loss 0.0321 (0.0221)	
training:	Epoch: [53][80/233]	Loss 0.0214 (0.0221)	
training:	Epoch: [53][81/233]	Loss 0.0172 (0.0220)	
training:	Epoch: [53][82/233]	Loss 0.0188 (0.0220)	
training:	Epoch: [53][83/233]	Loss 0.0187 (0.0219)	
training:	Epoch: [53][84/233]	Loss 0.0183 (0.0219)	
training:	Epoch: [53][85/233]	Loss 0.0200 (0.0219)	
training:	Epoch: [53][86/233]	Loss 0.0240 (0.0219)	
training:	Epoch: [53][87/233]	Loss 0.0224 (0.0219)	
training:	Epoch: [53][88/233]	Loss 0.0174 (0.0218)	
training:	Epoch: [53][89/233]	Loss 0.0286 (0.0219)	
training:	Epoch: [53][90/233]	Loss 0.0178 (0.0219)	
training:	Epoch: [53][91/233]	Loss 0.0198 (0.0219)	
training:	Epoch: [53][92/233]	Loss 0.0259 (0.0219)	
training:	Epoch: [53][93/233]	Loss 0.0448 (0.0221)	
training:	Epoch: [53][94/233]	Loss 0.0201 (0.0221)	
training:	Epoch: [53][95/233]	Loss 0.0248 (0.0222)	
training:	Epoch: [53][96/233]	Loss 0.0267 (0.0222)	
training:	Epoch: [53][97/233]	Loss 0.0181 (0.0222)	
training:	Epoch: [53][98/233]	Loss 0.0195 (0.0221)	
training:	Epoch: [53][99/233]	Loss 0.0174 (0.0221)	
training:	Epoch: [53][100/233]	Loss 0.0227 (0.0221)	
training:	Epoch: [53][101/233]	Loss 0.0173 (0.0220)	
training:	Epoch: [53][102/233]	Loss 0.0179 (0.0220)	
training:	Epoch: [53][103/233]	Loss 0.0264 (0.0220)	
training:	Epoch: [53][104/233]	Loss 0.0314 (0.0221)	
training:	Epoch: [53][105/233]	Loss 0.0200 (0.0221)	
training:	Epoch: [53][106/233]	Loss 0.0274 (0.0222)	
training:	Epoch: [53][107/233]	Loss 0.0181 (0.0221)	
training:	Epoch: [53][108/233]	Loss 0.0227 (0.0221)	
training:	Epoch: [53][109/233]	Loss 0.0192 (0.0221)	
training:	Epoch: [53][110/233]	Loss 0.0176 (0.0221)	
training:	Epoch: [53][111/233]	Loss 0.0211 (0.0221)	
training:	Epoch: [53][112/233]	Loss 0.0173 (0.0220)	
training:	Epoch: [53][113/233]	Loss 0.0286 (0.0221)	
training:	Epoch: [53][114/233]	Loss 0.1798 (0.0235)	
training:	Epoch: [53][115/233]	Loss 0.0194 (0.0234)	
training:	Epoch: [53][116/233]	Loss 0.0203 (0.0234)	
training:	Epoch: [53][117/233]	Loss 0.0181 (0.0233)	
training:	Epoch: [53][118/233]	Loss 0.0209 (0.0233)	
training:	Epoch: [53][119/233]	Loss 0.0177 (0.0233)	
training:	Epoch: [53][120/233]	Loss 0.0181 (0.0232)	
training:	Epoch: [53][121/233]	Loss 0.0238 (0.0232)	
training:	Epoch: [53][122/233]	Loss 0.0205 (0.0232)	
training:	Epoch: [53][123/233]	Loss 0.0263 (0.0232)	
training:	Epoch: [53][124/233]	Loss 0.0198 (0.0232)	
training:	Epoch: [53][125/233]	Loss 0.0175 (0.0232)	
training:	Epoch: [53][126/233]	Loss 0.0403 (0.0233)	
training:	Epoch: [53][127/233]	Loss 0.0235 (0.0233)	
training:	Epoch: [53][128/233]	Loss 0.0276 (0.0233)	
training:	Epoch: [53][129/233]	Loss 0.0199 (0.0233)	
training:	Epoch: [53][130/233]	Loss 0.0298 (0.0234)	
training:	Epoch: [53][131/233]	Loss 0.0221 (0.0234)	
training:	Epoch: [53][132/233]	Loss 0.0203 (0.0233)	
training:	Epoch: [53][133/233]	Loss 0.0179 (0.0233)	
training:	Epoch: [53][134/233]	Loss 0.0248 (0.0233)	
training:	Epoch: [53][135/233]	Loss 0.0194 (0.0233)	
training:	Epoch: [53][136/233]	Loss 0.0369 (0.0234)	
training:	Epoch: [53][137/233]	Loss 0.1342 (0.0242)	
training:	Epoch: [53][138/233]	Loss 0.0207 (0.0242)	
training:	Epoch: [53][139/233]	Loss 0.0222 (0.0241)	
training:	Epoch: [53][140/233]	Loss 0.0191 (0.0241)	
training:	Epoch: [53][141/233]	Loss 0.0200 (0.0241)	
training:	Epoch: [53][142/233]	Loss 0.0191 (0.0240)	
training:	Epoch: [53][143/233]	Loss 0.0174 (0.0240)	
training:	Epoch: [53][144/233]	Loss 0.0218 (0.0240)	
training:	Epoch: [53][145/233]	Loss 0.0168 (0.0239)	
training:	Epoch: [53][146/233]	Loss 0.0258 (0.0239)	
training:	Epoch: [53][147/233]	Loss 0.0193 (0.0239)	
training:	Epoch: [53][148/233]	Loss 0.0172 (0.0239)	
training:	Epoch: [53][149/233]	Loss 0.0302 (0.0239)	
training:	Epoch: [53][150/233]	Loss 0.0166 (0.0239)	
training:	Epoch: [53][151/233]	Loss 0.0211 (0.0238)	
training:	Epoch: [53][152/233]	Loss 0.0262 (0.0239)	
training:	Epoch: [53][153/233]	Loss 0.0310 (0.0239)	
training:	Epoch: [53][154/233]	Loss 0.0597 (0.0241)	
training:	Epoch: [53][155/233]	Loss 0.0176 (0.0241)	
training:	Epoch: [53][156/233]	Loss 0.0180 (0.0241)	
training:	Epoch: [53][157/233]	Loss 0.0239 (0.0241)	
training:	Epoch: [53][158/233]	Loss 0.0257 (0.0241)	
training:	Epoch: [53][159/233]	Loss 0.0183 (0.0240)	
training:	Epoch: [53][160/233]	Loss 0.0203 (0.0240)	
training:	Epoch: [53][161/233]	Loss 0.0232 (0.0240)	
training:	Epoch: [53][162/233]	Loss 0.0473 (0.0241)	
training:	Epoch: [53][163/233]	Loss 0.0247 (0.0241)	
training:	Epoch: [53][164/233]	Loss 0.0208 (0.0241)	
training:	Epoch: [53][165/233]	Loss 0.0205 (0.0241)	
training:	Epoch: [53][166/233]	Loss 0.0178 (0.0241)	
training:	Epoch: [53][167/233]	Loss 0.0186 (0.0240)	
training:	Epoch: [53][168/233]	Loss 0.0186 (0.0240)	
training:	Epoch: [53][169/233]	Loss 0.0382 (0.0241)	
training:	Epoch: [53][170/233]	Loss 0.0316 (0.0241)	
training:	Epoch: [53][171/233]	Loss 0.0183 (0.0241)	
training:	Epoch: [53][172/233]	Loss 0.0345 (0.0242)	
training:	Epoch: [53][173/233]	Loss 0.0181 (0.0241)	
training:	Epoch: [53][174/233]	Loss 0.0217 (0.0241)	
training:	Epoch: [53][175/233]	Loss 0.0385 (0.0242)	
training:	Epoch: [53][176/233]	Loss 0.0208 (0.0242)	
training:	Epoch: [53][177/233]	Loss 0.0234 (0.0242)	
training:	Epoch: [53][178/233]	Loss 0.0207 (0.0241)	
training:	Epoch: [53][179/233]	Loss 0.0237 (0.0241)	
training:	Epoch: [53][180/233]	Loss 0.0249 (0.0241)	
training:	Epoch: [53][181/233]	Loss 0.0208 (0.0241)	
training:	Epoch: [53][182/233]	Loss 0.0194 (0.0241)	
training:	Epoch: [53][183/233]	Loss 0.0211 (0.0241)	
training:	Epoch: [53][184/233]	Loss 0.0216 (0.0241)	
training:	Epoch: [53][185/233]	Loss 0.0191 (0.0240)	
training:	Epoch: [53][186/233]	Loss 0.0191 (0.0240)	
training:	Epoch: [53][187/233]	Loss 0.0175 (0.0240)	
training:	Epoch: [53][188/233]	Loss 0.0199 (0.0240)	
training:	Epoch: [53][189/233]	Loss 0.0243 (0.0240)	
training:	Epoch: [53][190/233]	Loss 0.0318 (0.0240)	
training:	Epoch: [53][191/233]	Loss 0.0246 (0.0240)	
training:	Epoch: [53][192/233]	Loss 0.0229 (0.0240)	
training:	Epoch: [53][193/233]	Loss 0.0173 (0.0240)	
training:	Epoch: [53][194/233]	Loss 0.0194 (0.0239)	
training:	Epoch: [53][195/233]	Loss 0.0236 (0.0239)	
training:	Epoch: [53][196/233]	Loss 0.0185 (0.0239)	
training:	Epoch: [53][197/233]	Loss 0.0182 (0.0239)	
training:	Epoch: [53][198/233]	Loss 0.0337 (0.0239)	
training:	Epoch: [53][199/233]	Loss 0.0220 (0.0239)	
training:	Epoch: [53][200/233]	Loss 0.0198 (0.0239)	
training:	Epoch: [53][201/233]	Loss 0.0262 (0.0239)	
training:	Epoch: [53][202/233]	Loss 0.0203 (0.0239)	
training:	Epoch: [53][203/233]	Loss 0.0310 (0.0239)	
training:	Epoch: [53][204/233]	Loss 0.0173 (0.0239)	
training:	Epoch: [53][205/233]	Loss 0.0223 (0.0239)	
training:	Epoch: [53][206/233]	Loss 0.0277 (0.0239)	
training:	Epoch: [53][207/233]	Loss 0.0206 (0.0239)	
training:	Epoch: [53][208/233]	Loss 0.0336 (0.0239)	
training:	Epoch: [53][209/233]	Loss 0.0283 (0.0240)	
training:	Epoch: [53][210/233]	Loss 0.0191 (0.0239)	
training:	Epoch: [53][211/233]	Loss 0.0187 (0.0239)	
training:	Epoch: [53][212/233]	Loss 0.0169 (0.0239)	
training:	Epoch: [53][213/233]	Loss 0.0189 (0.0239)	
training:	Epoch: [53][214/233]	Loss 0.0216 (0.0238)	
training:	Epoch: [53][215/233]	Loss 0.0191 (0.0238)	
training:	Epoch: [53][216/233]	Loss 0.0192 (0.0238)	
training:	Epoch: [53][217/233]	Loss 0.0194 (0.0238)	
training:	Epoch: [53][218/233]	Loss 0.0243 (0.0238)	
training:	Epoch: [53][219/233]	Loss 0.0358 (0.0238)	
training:	Epoch: [53][220/233]	Loss 0.0203 (0.0238)	
training:	Epoch: [53][221/233]	Loss 0.0195 (0.0238)	
training:	Epoch: [53][222/233]	Loss 0.0323 (0.0238)	
training:	Epoch: [53][223/233]	Loss 0.0186 (0.0238)	
training:	Epoch: [53][224/233]	Loss 0.0199 (0.0238)	
training:	Epoch: [53][225/233]	Loss 0.0180 (0.0238)	
training:	Epoch: [53][226/233]	Loss 0.0184 (0.0238)	
training:	Epoch: [53][227/233]	Loss 0.0182 (0.0237)	
training:	Epoch: [53][228/233]	Loss 0.0200 (0.0237)	
training:	Epoch: [53][229/233]	Loss 0.0189 (0.0237)	
training:	Epoch: [53][230/233]	Loss 0.0279 (0.0237)	
training:	Epoch: [53][231/233]	Loss 0.0266 (0.0237)	
training:	Epoch: [53][232/233]	Loss 0.0251 (0.0237)	
training:	Epoch: [53][233/233]	Loss 0.0270 (0.0237)	
Training:	 Loss: 0.0237

Training:	 ACC: 1.0000 1.0000 1.0000 1.0000
Validation:	 ACC: 0.8672 0.8673 0.8690 0.8655
Validation:	 Best_BACC: 0.8673 0.8657 0.8332 0.9013
Validation:	 Loss: 0.3148
Pretraining:	Epoch 54/200
----------
training:	Epoch: [54][1/233]	Loss 0.0214 (0.0214)	
training:	Epoch: [54][2/233]	Loss 0.0181 (0.0197)	
training:	Epoch: [54][3/233]	Loss 0.0172 (0.0189)	
training:	Epoch: [54][4/233]	Loss 0.0172 (0.0185)	
training:	Epoch: [54][5/233]	Loss 0.0165 (0.0181)	
training:	Epoch: [54][6/233]	Loss 0.0198 (0.0184)	
training:	Epoch: [54][7/233]	Loss 0.0178 (0.0183)	
training:	Epoch: [54][8/233]	Loss 0.0170 (0.0181)	
training:	Epoch: [54][9/233]	Loss 0.0213 (0.0185)	
training:	Epoch: [54][10/233]	Loss 0.0242 (0.0191)	
training:	Epoch: [54][11/233]	Loss 0.0174 (0.0189)	
training:	Epoch: [54][12/233]	Loss 0.0282 (0.0197)	
training:	Epoch: [54][13/233]	Loss 0.0175 (0.0195)	
training:	Epoch: [54][14/233]	Loss 0.0215 (0.0197)	
training:	Epoch: [54][15/233]	Loss 0.0201 (0.0197)	
training:	Epoch: [54][16/233]	Loss 0.0237 (0.0199)	
training:	Epoch: [54][17/233]	Loss 0.0444 (0.0214)	
training:	Epoch: [54][18/233]	Loss 0.0180 (0.0212)	
training:	Epoch: [54][19/233]	Loss 0.0191 (0.0211)	
training:	Epoch: [54][20/233]	Loss 0.0201 (0.0210)	
training:	Epoch: [54][21/233]	Loss 0.0219 (0.0211)	
training:	Epoch: [54][22/233]	Loss 0.0187 (0.0210)	
training:	Epoch: [54][23/233]	Loss 0.0275 (0.0212)	
training:	Epoch: [54][24/233]	Loss 0.0223 (0.0213)	
training:	Epoch: [54][25/233]	Loss 0.0162 (0.0211)	
training:	Epoch: [54][26/233]	Loss 0.0235 (0.0212)	
training:	Epoch: [54][27/233]	Loss 0.0196 (0.0211)	
training:	Epoch: [54][28/233]	Loss 0.0166 (0.0210)	
training:	Epoch: [54][29/233]	Loss 0.0275 (0.0212)	
training:	Epoch: [54][30/233]	Loss 0.0239 (0.0213)	
training:	Epoch: [54][31/233]	Loss 0.0245 (0.0214)	
training:	Epoch: [54][32/233]	Loss 0.0197 (0.0213)	
training:	Epoch: [54][33/233]	Loss 0.0182 (0.0212)	
training:	Epoch: [54][34/233]	Loss 0.0177 (0.0211)	
training:	Epoch: [54][35/233]	Loss 0.0251 (0.0212)	
training:	Epoch: [54][36/233]	Loss 0.0200 (0.0212)	
training:	Epoch: [54][37/233]	Loss 0.0172 (0.0211)	
training:	Epoch: [54][38/233]	Loss 0.0258 (0.0212)	
training:	Epoch: [54][39/233]	Loss 0.0190 (0.0212)	
training:	Epoch: [54][40/233]	Loss 0.0184 (0.0211)	
training:	Epoch: [54][41/233]	Loss 0.0160 (0.0210)	
training:	Epoch: [54][42/233]	Loss 0.0184 (0.0209)	
training:	Epoch: [54][43/233]	Loss 0.0181 (0.0208)	
training:	Epoch: [54][44/233]	Loss 0.0197 (0.0208)	
training:	Epoch: [54][45/233]	Loss 0.0383 (0.0212)	
training:	Epoch: [54][46/233]	Loss 0.0182 (0.0211)	
training:	Epoch: [54][47/233]	Loss 0.0174 (0.0211)	
training:	Epoch: [54][48/233]	Loss 0.0224 (0.0211)	
training:	Epoch: [54][49/233]	Loss 0.0195 (0.0211)	
training:	Epoch: [54][50/233]	Loss 0.0171 (0.0210)	
training:	Epoch: [54][51/233]	Loss 0.0572 (0.0217)	
training:	Epoch: [54][52/233]	Loss 0.0298 (0.0218)	
training:	Epoch: [54][53/233]	Loss 0.0240 (0.0219)	
training:	Epoch: [54][54/233]	Loss 0.0348 (0.0221)	
training:	Epoch: [54][55/233]	Loss 0.0284 (0.0222)	
training:	Epoch: [54][56/233]	Loss 0.0170 (0.0221)	
training:	Epoch: [54][57/233]	Loss 0.0267 (0.0222)	
training:	Epoch: [54][58/233]	Loss 0.0170 (0.0221)	
training:	Epoch: [54][59/233]	Loss 0.0175 (0.0221)	
training:	Epoch: [54][60/233]	Loss 0.0203 (0.0220)	
training:	Epoch: [54][61/233]	Loss 0.0196 (0.0220)	
training:	Epoch: [54][62/233]	Loss 0.0183 (0.0219)	
training:	Epoch: [54][63/233]	Loss 0.0180 (0.0219)	
training:	Epoch: [54][64/233]	Loss 0.0192 (0.0218)	
training:	Epoch: [54][65/233]	Loss 0.0207 (0.0218)	
training:	Epoch: [54][66/233]	Loss 0.0606 (0.0224)	
training:	Epoch: [54][67/233]	Loss 0.0165 (0.0223)	
training:	Epoch: [54][68/233]	Loss 0.0274 (0.0224)	
training:	Epoch: [54][69/233]	Loss 0.0289 (0.0225)	
training:	Epoch: [54][70/233]	Loss 0.0165 (0.0224)	
training:	Epoch: [54][71/233]	Loss 0.0210 (0.0224)	
training:	Epoch: [54][72/233]	Loss 0.0178 (0.0223)	
training:	Epoch: [54][73/233]	Loss 0.0222 (0.0223)	
training:	Epoch: [54][74/233]	Loss 0.0158 (0.0222)	
training:	Epoch: [54][75/233]	Loss 0.0201 (0.0222)	
training:	Epoch: [54][76/233]	Loss 0.0169 (0.0221)	
training:	Epoch: [54][77/233]	Loss 0.0180 (0.0221)	
training:	Epoch: [54][78/233]	Loss 0.0231 (0.0221)	
training:	Epoch: [54][79/233]	Loss 0.0229 (0.0221)	
training:	Epoch: [54][80/233]	Loss 0.0190 (0.0221)	
training:	Epoch: [54][81/233]	Loss 0.0296 (0.0221)	
training:	Epoch: [54][82/233]	Loss 0.0173 (0.0221)	
training:	Epoch: [54][83/233]	Loss 0.0198 (0.0221)	
training:	Epoch: [54][84/233]	Loss 0.0184 (0.0220)	
training:	Epoch: [54][85/233]	Loss 0.0235 (0.0220)	
training:	Epoch: [54][86/233]	Loss 0.0215 (0.0220)	
training:	Epoch: [54][87/233]	Loss 0.0208 (0.0220)	
training:	Epoch: [54][88/233]	Loss 0.0183 (0.0220)	
training:	Epoch: [54][89/233]	Loss 0.0178 (0.0219)	
training:	Epoch: [54][90/233]	Loss 0.0187 (0.0219)	
training:	Epoch: [54][91/233]	Loss 0.0175 (0.0218)	
training:	Epoch: [54][92/233]	Loss 0.0262 (0.0219)	
training:	Epoch: [54][93/233]	Loss 0.0345 (0.0220)	
training:	Epoch: [54][94/233]	Loss 0.0216 (0.0220)	
training:	Epoch: [54][95/233]	Loss 0.0170 (0.0220)	
training:	Epoch: [54][96/233]	Loss 0.0169 (0.0219)	
training:	Epoch: [54][97/233]	Loss 0.0200 (0.0219)	
training:	Epoch: [54][98/233]	Loss 0.0208 (0.0219)	
training:	Epoch: [54][99/233]	Loss 0.0210 (0.0219)	
training:	Epoch: [54][100/233]	Loss 0.0295 (0.0220)	
training:	Epoch: [54][101/233]	Loss 0.0163 (0.0219)	
training:	Epoch: [54][102/233]	Loss 0.0161 (0.0218)	
training:	Epoch: [54][103/233]	Loss 0.0175 (0.0218)	
training:	Epoch: [54][104/233]	Loss 0.0182 (0.0218)	
training:	Epoch: [54][105/233]	Loss 0.0203 (0.0217)	
training:	Epoch: [54][106/233]	Loss 0.0258 (0.0218)	
training:	Epoch: [54][107/233]	Loss 0.0175 (0.0217)	
training:	Epoch: [54][108/233]	Loss 0.0302 (0.0218)	
training:	Epoch: [54][109/233]	Loss 0.0243 (0.0218)	
training:	Epoch: [54][110/233]	Loss 0.0200 (0.0218)	
training:	Epoch: [54][111/233]	Loss 0.0279 (0.0219)	
training:	Epoch: [54][112/233]	Loss 0.0165 (0.0218)	
training:	Epoch: [54][113/233]	Loss 0.0174 (0.0218)	
training:	Epoch: [54][114/233]	Loss 0.0170 (0.0218)	
training:	Epoch: [54][115/233]	Loss 0.0204 (0.0217)	
training:	Epoch: [54][116/233]	Loss 0.0212 (0.0217)	
training:	Epoch: [54][117/233]	Loss 0.0310 (0.0218)	
training:	Epoch: [54][118/233]	Loss 0.0214 (0.0218)	
training:	Epoch: [54][119/233]	Loss 0.0166 (0.0218)	
training:	Epoch: [54][120/233]	Loss 0.0176 (0.0217)	
training:	Epoch: [54][121/233]	Loss 0.0205 (0.0217)	
training:	Epoch: [54][122/233]	Loss 0.0200 (0.0217)	
training:	Epoch: [54][123/233]	Loss 0.0241 (0.0217)	
training:	Epoch: [54][124/233]	Loss 0.0270 (0.0218)	
training:	Epoch: [54][125/233]	Loss 0.0191 (0.0218)	
training:	Epoch: [54][126/233]	Loss 0.0243 (0.0218)	
training:	Epoch: [54][127/233]	Loss 0.0168 (0.0217)	
training:	Epoch: [54][128/233]	Loss 0.0216 (0.0217)	
training:	Epoch: [54][129/233]	Loss 0.0239 (0.0217)	
training:	Epoch: [54][130/233]	Loss 0.0217 (0.0217)	
training:	Epoch: [54][131/233]	Loss 0.0188 (0.0217)	
training:	Epoch: [54][132/233]	Loss 0.0249 (0.0217)	
training:	Epoch: [54][133/233]	Loss 0.0177 (0.0217)	
training:	Epoch: [54][134/233]	Loss 0.0166 (0.0217)	
training:	Epoch: [54][135/233]	Loss 0.0175 (0.0216)	
training:	Epoch: [54][136/233]	Loss 0.0241 (0.0217)	
training:	Epoch: [54][137/233]	Loss 0.0171 (0.0216)	
training:	Epoch: [54][138/233]	Loss 0.0224 (0.0216)	
training:	Epoch: [54][139/233]	Loss 0.0230 (0.0216)	
training:	Epoch: [54][140/233]	Loss 0.0194 (0.0216)	
training:	Epoch: [54][141/233]	Loss 0.0198 (0.0216)	
training:	Epoch: [54][142/233]	Loss 0.0491 (0.0218)	
training:	Epoch: [54][143/233]	Loss 0.0178 (0.0218)	
training:	Epoch: [54][144/233]	Loss 0.0338 (0.0219)	
training:	Epoch: [54][145/233]	Loss 0.0278 (0.0219)	
training:	Epoch: [54][146/233]	Loss 0.0205 (0.0219)	
training:	Epoch: [54][147/233]	Loss 0.0305 (0.0220)	
training:	Epoch: [54][148/233]	Loss 0.0383 (0.0221)	
training:	Epoch: [54][149/233]	Loss 0.0155 (0.0220)	
training:	Epoch: [54][150/233]	Loss 0.0166 (0.0220)	
training:	Epoch: [54][151/233]	Loss 0.0187 (0.0220)	
training:	Epoch: [54][152/233]	Loss 0.0176 (0.0219)	
training:	Epoch: [54][153/233]	Loss 0.0194 (0.0219)	
training:	Epoch: [54][154/233]	Loss 0.0201 (0.0219)	
training:	Epoch: [54][155/233]	Loss 0.0193 (0.0219)	
training:	Epoch: [54][156/233]	Loss 0.0238 (0.0219)	
training:	Epoch: [54][157/233]	Loss 0.0177 (0.0219)	
training:	Epoch: [54][158/233]	Loss 0.0398 (0.0220)	
training:	Epoch: [54][159/233]	Loss 0.0255 (0.0220)	
training:	Epoch: [54][160/233]	Loss 0.0179 (0.0220)	
training:	Epoch: [54][161/233]	Loss 0.0193 (0.0220)	
training:	Epoch: [54][162/233]	Loss 0.0357 (0.0221)	
training:	Epoch: [54][163/233]	Loss 0.0203 (0.0220)	
training:	Epoch: [54][164/233]	Loss 0.0265 (0.0221)	
training:	Epoch: [54][165/233]	Loss 0.0189 (0.0221)	
training:	Epoch: [54][166/233]	Loss 0.0209 (0.0220)	
training:	Epoch: [54][167/233]	Loss 0.0161 (0.0220)	
training:	Epoch: [54][168/233]	Loss 0.0171 (0.0220)	
training:	Epoch: [54][169/233]	Loss 0.0189 (0.0220)	
training:	Epoch: [54][170/233]	Loss 0.0190 (0.0219)	
training:	Epoch: [54][171/233]	Loss 0.0368 (0.0220)	
training:	Epoch: [54][172/233]	Loss 0.0270 (0.0221)	
training:	Epoch: [54][173/233]	Loss 0.0203 (0.0221)	
training:	Epoch: [54][174/233]	Loss 0.0202 (0.0220)	
training:	Epoch: [54][175/233]	Loss 0.0212 (0.0220)	
training:	Epoch: [54][176/233]	Loss 0.0158 (0.0220)	
training:	Epoch: [54][177/233]	Loss 0.0187 (0.0220)	
training:	Epoch: [54][178/233]	Loss 0.0176 (0.0220)	
training:	Epoch: [54][179/233]	Loss 0.0656 (0.0222)	
training:	Epoch: [54][180/233]	Loss 0.0182 (0.0222)	
training:	Epoch: [54][181/233]	Loss 0.0383 (0.0223)	
training:	Epoch: [54][182/233]	Loss 0.0200 (0.0223)	
training:	Epoch: [54][183/233]	Loss 0.0340 (0.0223)	
training:	Epoch: [54][184/233]	Loss 0.0208 (0.0223)	
training:	Epoch: [54][185/233]	Loss 0.0291 (0.0223)	
training:	Epoch: [54][186/233]	Loss 0.0180 (0.0223)	
training:	Epoch: [54][187/233]	Loss 0.0279 (0.0224)	
training:	Epoch: [54][188/233]	Loss 0.0167 (0.0223)	
training:	Epoch: [54][189/233]	Loss 0.0176 (0.0223)	
training:	Epoch: [54][190/233]	Loss 0.0276 (0.0223)	
training:	Epoch: [54][191/233]	Loss 0.0194 (0.0223)	
training:	Epoch: [54][192/233]	Loss 0.0173 (0.0223)	
training:	Epoch: [54][193/233]	Loss 0.0180 (0.0223)	
training:	Epoch: [54][194/233]	Loss 0.0170 (0.0222)	
training:	Epoch: [54][195/233]	Loss 0.0183 (0.0222)	
training:	Epoch: [54][196/233]	Loss 0.0222 (0.0222)	
training:	Epoch: [54][197/233]	Loss 0.0202 (0.0222)	
training:	Epoch: [54][198/233]	Loss 0.0205 (0.0222)	
training:	Epoch: [54][199/233]	Loss 0.0175 (0.0222)	
training:	Epoch: [54][200/233]	Loss 0.0193 (0.0222)	
training:	Epoch: [54][201/233]	Loss 0.0166 (0.0221)	
training:	Epoch: [54][202/233]	Loss 0.0187 (0.0221)	
training:	Epoch: [54][203/233]	Loss 0.0265 (0.0221)	
training:	Epoch: [54][204/233]	Loss 0.0178 (0.0221)	
training:	Epoch: [54][205/233]	Loss 0.0206 (0.0221)	
training:	Epoch: [54][206/233]	Loss 0.0184 (0.0221)	
training:	Epoch: [54][207/233]	Loss 0.0169 (0.0221)	
training:	Epoch: [54][208/233]	Loss 0.0204 (0.0221)	
training:	Epoch: [54][209/233]	Loss 0.0161 (0.0220)	
training:	Epoch: [54][210/233]	Loss 0.0232 (0.0220)	
training:	Epoch: [54][211/233]	Loss 0.0252 (0.0220)	
training:	Epoch: [54][212/233]	Loss 0.0196 (0.0220)	
training:	Epoch: [54][213/233]	Loss 0.0203 (0.0220)	
training:	Epoch: [54][214/233]	Loss 0.0161 (0.0220)	
training:	Epoch: [54][215/233]	Loss 0.0163 (0.0220)	
training:	Epoch: [54][216/233]	Loss 0.0204 (0.0220)	
training:	Epoch: [54][217/233]	Loss 0.0241 (0.0220)	
training:	Epoch: [54][218/233]	Loss 0.0266 (0.0220)	
training:	Epoch: [54][219/233]	Loss 0.0192 (0.0220)	
training:	Epoch: [54][220/233]	Loss 0.0201 (0.0220)	
training:	Epoch: [54][221/233]	Loss 0.0404 (0.0221)	
training:	Epoch: [54][222/233]	Loss 0.0366 (0.0221)	
training:	Epoch: [54][223/233]	Loss 0.0191 (0.0221)	
training:	Epoch: [54][224/233]	Loss 0.0173 (0.0221)	
training:	Epoch: [54][225/233]	Loss 0.0187 (0.0221)	
training:	Epoch: [54][226/233]	Loss 0.0202 (0.0221)	
training:	Epoch: [54][227/233]	Loss 0.0172 (0.0220)	
training:	Epoch: [54][228/233]	Loss 0.0176 (0.0220)	
training:	Epoch: [54][229/233]	Loss 0.0219 (0.0220)	
training:	Epoch: [54][230/233]	Loss 0.0284 (0.0221)	
training:	Epoch: [54][231/233]	Loss 0.0165 (0.0220)	
training:	Epoch: [54][232/233]	Loss 0.0189 (0.0220)	
training:	Epoch: [54][233/233]	Loss 0.0204 (0.0220)	
Training:	 Loss: 0.0220

Training:	 ACC: 1.0000 1.0000 1.0000 1.0000
Validation:	 ACC: 0.8670 0.8668 0.8618 0.8722
Validation:	 Best_BACC: 0.8673 0.8657 0.8332 0.9013
Validation:	 Loss: 0.3178
Pretraining:	Epoch 55/200
----------
training:	Epoch: [55][1/233]	Loss 0.0240 (0.0240)	
training:	Epoch: [55][2/233]	Loss 0.0171 (0.0206)	
training:	Epoch: [55][3/233]	Loss 0.0172 (0.0194)	
training:	Epoch: [55][4/233]	Loss 0.0193 (0.0194)	
training:	Epoch: [55][5/233]	Loss 0.0207 (0.0197)	
training:	Epoch: [55][6/233]	Loss 0.0335 (0.0220)	
training:	Epoch: [55][7/233]	Loss 0.0197 (0.0216)	
training:	Epoch: [55][8/233]	Loss 0.0189 (0.0213)	
training:	Epoch: [55][9/233]	Loss 0.0190 (0.0210)	
training:	Epoch: [55][10/233]	Loss 0.0223 (0.0212)	
training:	Epoch: [55][11/233]	Loss 0.0194 (0.0210)	
training:	Epoch: [55][12/233]	Loss 0.0178 (0.0207)	
training:	Epoch: [55][13/233]	Loss 0.0353 (0.0219)	
training:	Epoch: [55][14/233]	Loss 0.0185 (0.0216)	
training:	Epoch: [55][15/233]	Loss 0.0328 (0.0224)	
training:	Epoch: [55][16/233]	Loss 0.0205 (0.0222)	
training:	Epoch: [55][17/233]	Loss 0.0170 (0.0219)	
training:	Epoch: [55][18/233]	Loss 0.0172 (0.0217)	
training:	Epoch: [55][19/233]	Loss 0.0234 (0.0218)	
training:	Epoch: [55][20/233]	Loss 0.0293 (0.0221)	
training:	Epoch: [55][21/233]	Loss 0.0172 (0.0219)	
training:	Epoch: [55][22/233]	Loss 0.0250 (0.0220)	
training:	Epoch: [55][23/233]	Loss 0.0181 (0.0219)	
training:	Epoch: [55][24/233]	Loss 0.0236 (0.0219)	
training:	Epoch: [55][25/233]	Loss 0.0189 (0.0218)	
training:	Epoch: [55][26/233]	Loss 0.0174 (0.0217)	
training:	Epoch: [55][27/233]	Loss 0.0152 (0.0214)	
training:	Epoch: [55][28/233]	Loss 0.0160 (0.0212)	
training:	Epoch: [55][29/233]	Loss 0.0162 (0.0211)	
training:	Epoch: [55][30/233]	Loss 0.0173 (0.0209)	
training:	Epoch: [55][31/233]	Loss 0.0181 (0.0208)	
training:	Epoch: [55][32/233]	Loss 0.0166 (0.0207)	
training:	Epoch: [55][33/233]	Loss 0.0162 (0.0206)	
training:	Epoch: [55][34/233]	Loss 0.0161 (0.0204)	
training:	Epoch: [55][35/233]	Loss 0.0182 (0.0204)	
training:	Epoch: [55][36/233]	Loss 0.0173 (0.0203)	
training:	Epoch: [55][37/233]	Loss 0.0172 (0.0202)	
training:	Epoch: [55][38/233]	Loss 0.0217 (0.0202)	
training:	Epoch: [55][39/233]	Loss 0.0195 (0.0202)	
training:	Epoch: [55][40/233]	Loss 0.0235 (0.0203)	
training:	Epoch: [55][41/233]	Loss 0.0163 (0.0202)	
training:	Epoch: [55][42/233]	Loss 0.0152 (0.0201)	
training:	Epoch: [55][43/233]	Loss 0.0181 (0.0200)	
training:	Epoch: [55][44/233]	Loss 0.0254 (0.0202)	
training:	Epoch: [55][45/233]	Loss 0.0222 (0.0202)	
training:	Epoch: [55][46/233]	Loss 0.0217 (0.0202)	
training:	Epoch: [55][47/233]	Loss 0.0165 (0.0202)	
training:	Epoch: [55][48/233]	Loss 0.0180 (0.0201)	
training:	Epoch: [55][49/233]	Loss 0.0163 (0.0200)	
training:	Epoch: [55][50/233]	Loss 0.0286 (0.0202)	
training:	Epoch: [55][51/233]	Loss 0.0227 (0.0203)	
training:	Epoch: [55][52/233]	Loss 0.0190 (0.0202)	
training:	Epoch: [55][53/233]	Loss 0.0226 (0.0203)	
training:	Epoch: [55][54/233]	Loss 0.0220 (0.0203)	
training:	Epoch: [55][55/233]	Loss 0.0226 (0.0203)	
training:	Epoch: [55][56/233]	Loss 0.0169 (0.0203)	
training:	Epoch: [55][57/233]	Loss 0.0208 (0.0203)	
training:	Epoch: [55][58/233]	Loss 0.0217 (0.0203)	
training:	Epoch: [55][59/233]	Loss 0.0178 (0.0203)	
training:	Epoch: [55][60/233]	Loss 0.0161 (0.0202)	
training:	Epoch: [55][61/233]	Loss 0.0241 (0.0203)	
training:	Epoch: [55][62/233]	Loss 0.0174 (0.0202)	
training:	Epoch: [55][63/233]	Loss 0.0290 (0.0204)	
training:	Epoch: [55][64/233]	Loss 0.0282 (0.0205)	
training:	Epoch: [55][65/233]	Loss 0.0165 (0.0204)	
training:	Epoch: [55][66/233]	Loss 0.0171 (0.0204)	
training:	Epoch: [55][67/233]	Loss 0.0182 (0.0203)	
training:	Epoch: [55][68/233]	Loss 0.0160 (0.0203)	
training:	Epoch: [55][69/233]	Loss 0.0236 (0.0203)	
training:	Epoch: [55][70/233]	Loss 0.0174 (0.0203)	
training:	Epoch: [55][71/233]	Loss 0.0184 (0.0203)	
training:	Epoch: [55][72/233]	Loss 0.0169 (0.0202)	
training:	Epoch: [55][73/233]	Loss 0.0182 (0.0202)	
training:	Epoch: [55][74/233]	Loss 0.0189 (0.0202)	
training:	Epoch: [55][75/233]	Loss 0.0193 (0.0202)	
training:	Epoch: [55][76/233]	Loss 0.0273 (0.0203)	
training:	Epoch: [55][77/233]	Loss 0.0530 (0.0207)	
training:	Epoch: [55][78/233]	Loss 0.0396 (0.0209)	
training:	Epoch: [55][79/233]	Loss 0.0367 (0.0211)	
training:	Epoch: [55][80/233]	Loss 0.0238 (0.0212)	
training:	Epoch: [55][81/233]	Loss 0.0176 (0.0211)	
training:	Epoch: [55][82/233]	Loss 0.0158 (0.0210)	
training:	Epoch: [55][83/233]	Loss 0.0159 (0.0210)	
training:	Epoch: [55][84/233]	Loss 0.0213 (0.0210)	
training:	Epoch: [55][85/233]	Loss 0.0218 (0.0210)	
training:	Epoch: [55][86/233]	Loss 0.0157 (0.0209)	
training:	Epoch: [55][87/233]	Loss 0.0148 (0.0209)	
training:	Epoch: [55][88/233]	Loss 0.0186 (0.0208)	
training:	Epoch: [55][89/233]	Loss 0.0205 (0.0208)	
training:	Epoch: [55][90/233]	Loss 0.0175 (0.0208)	
training:	Epoch: [55][91/233]	Loss 0.0177 (0.0208)	
training:	Epoch: [55][92/233]	Loss 0.0158 (0.0207)	
training:	Epoch: [55][93/233]	Loss 0.0176 (0.0207)	
training:	Epoch: [55][94/233]	Loss 0.0157 (0.0206)	
training:	Epoch: [55][95/233]	Loss 0.0167 (0.0206)	
training:	Epoch: [55][96/233]	Loss 0.0200 (0.0206)	
training:	Epoch: [55][97/233]	Loss 0.0183 (0.0205)	
training:	Epoch: [55][98/233]	Loss 0.0356 (0.0207)	
training:	Epoch: [55][99/233]	Loss 0.0165 (0.0207)	
training:	Epoch: [55][100/233]	Loss 0.0164 (0.0206)	
training:	Epoch: [55][101/233]	Loss 0.0225 (0.0206)	
training:	Epoch: [55][102/233]	Loss 0.0243 (0.0207)	
training:	Epoch: [55][103/233]	Loss 0.0779 (0.0212)	
training:	Epoch: [55][104/233]	Loss 0.0183 (0.0212)	
training:	Epoch: [55][105/233]	Loss 0.0274 (0.0213)	
training:	Epoch: [55][106/233]	Loss 0.0517 (0.0215)	
training:	Epoch: [55][107/233]	Loss 0.0219 (0.0216)	
training:	Epoch: [55][108/233]	Loss 0.0174 (0.0215)	
training:	Epoch: [55][109/233]	Loss 0.0189 (0.0215)	
training:	Epoch: [55][110/233]	Loss 0.0160 (0.0214)	
training:	Epoch: [55][111/233]	Loss 0.0182 (0.0214)	
training:	Epoch: [55][112/233]	Loss 0.0269 (0.0215)	
training:	Epoch: [55][113/233]	Loss 0.0170 (0.0214)	
training:	Epoch: [55][114/233]	Loss 0.0284 (0.0215)	
training:	Epoch: [55][115/233]	Loss 0.0261 (0.0215)	
training:	Epoch: [55][116/233]	Loss 0.0161 (0.0215)	
training:	Epoch: [55][117/233]	Loss 0.0201 (0.0215)	
training:	Epoch: [55][118/233]	Loss 0.0160 (0.0214)	
training:	Epoch: [55][119/233]	Loss 0.0163 (0.0214)	
training:	Epoch: [55][120/233]	Loss 0.0150 (0.0213)	
training:	Epoch: [55][121/233]	Loss 0.0283 (0.0214)	
training:	Epoch: [55][122/233]	Loss 0.0222 (0.0214)	
training:	Epoch: [55][123/233]	Loss 0.0201 (0.0214)	
training:	Epoch: [55][124/233]	Loss 0.0190 (0.0214)	
training:	Epoch: [55][125/233]	Loss 0.0197 (0.0213)	
training:	Epoch: [55][126/233]	Loss 0.0177 (0.0213)	
training:	Epoch: [55][127/233]	Loss 0.0183 (0.0213)	
training:	Epoch: [55][128/233]	Loss 0.0185 (0.0213)	
training:	Epoch: [55][129/233]	Loss 0.0213 (0.0213)	
training:	Epoch: [55][130/233]	Loss 0.0487 (0.0215)	
training:	Epoch: [55][131/233]	Loss 0.0371 (0.0216)	
training:	Epoch: [55][132/233]	Loss 0.0161 (0.0216)	
training:	Epoch: [55][133/233]	Loss 0.0173 (0.0215)	
training:	Epoch: [55][134/233]	Loss 0.0157 (0.0215)	
training:	Epoch: [55][135/233]	Loss 0.0273 (0.0215)	
training:	Epoch: [55][136/233]	Loss 0.0322 (0.0216)	
training:	Epoch: [55][137/233]	Loss 0.0345 (0.0217)	
training:	Epoch: [55][138/233]	Loss 0.0178 (0.0217)	
training:	Epoch: [55][139/233]	Loss 0.0171 (0.0216)	
training:	Epoch: [55][140/233]	Loss 0.0157 (0.0216)	
training:	Epoch: [55][141/233]	Loss 0.0174 (0.0216)	
training:	Epoch: [55][142/233]	Loss 0.0264 (0.0216)	
training:	Epoch: [55][143/233]	Loss 0.0191 (0.0216)	
training:	Epoch: [55][144/233]	Loss 0.0170 (0.0215)	
training:	Epoch: [55][145/233]	Loss 0.0205 (0.0215)	
training:	Epoch: [55][146/233]	Loss 0.0180 (0.0215)	
training:	Epoch: [55][147/233]	Loss 0.0186 (0.0215)	
training:	Epoch: [55][148/233]	Loss 0.0217 (0.0215)	
training:	Epoch: [55][149/233]	Loss 0.0158 (0.0215)	
training:	Epoch: [55][150/233]	Loss 0.0168 (0.0214)	
training:	Epoch: [55][151/233]	Loss 0.0172 (0.0214)	
training:	Epoch: [55][152/233]	Loss 0.0204 (0.0214)	
training:	Epoch: [55][153/233]	Loss 0.0186 (0.0214)	
training:	Epoch: [55][154/233]	Loss 0.0313 (0.0214)	
training:	Epoch: [55][155/233]	Loss 0.0181 (0.0214)	
training:	Epoch: [55][156/233]	Loss 0.0163 (0.0214)	
training:	Epoch: [55][157/233]	Loss 0.0315 (0.0214)	
training:	Epoch: [55][158/233]	Loss 0.0159 (0.0214)	
training:	Epoch: [55][159/233]	Loss 0.0174 (0.0214)	
training:	Epoch: [55][160/233]	Loss 0.0194 (0.0214)	
training:	Epoch: [55][161/233]	Loss 0.0243 (0.0214)	
training:	Epoch: [55][162/233]	Loss 0.0223 (0.0214)	
training:	Epoch: [55][163/233]	Loss 0.0188 (0.0214)	
training:	Epoch: [55][164/233]	Loss 0.0203 (0.0214)	
training:	Epoch: [55][165/233]	Loss 0.0215 (0.0214)	
training:	Epoch: [55][166/233]	Loss 0.0169 (0.0214)	
training:	Epoch: [55][167/233]	Loss 0.0200 (0.0213)	
training:	Epoch: [55][168/233]	Loss 0.0184 (0.0213)	
training:	Epoch: [55][169/233]	Loss 0.0232 (0.0213)	
training:	Epoch: [55][170/233]	Loss 0.0199 (0.0213)	
training:	Epoch: [55][171/233]	Loss 0.0309 (0.0214)	
training:	Epoch: [55][172/233]	Loss 0.0172 (0.0214)	
training:	Epoch: [55][173/233]	Loss 0.0186 (0.0213)	
training:	Epoch: [55][174/233]	Loss 0.0224 (0.0213)	
training:	Epoch: [55][175/233]	Loss 0.0157 (0.0213)	
training:	Epoch: [55][176/233]	Loss 0.0556 (0.0215)	
training:	Epoch: [55][177/233]	Loss 0.0197 (0.0215)	
training:	Epoch: [55][178/233]	Loss 0.0188 (0.0215)	
training:	Epoch: [55][179/233]	Loss 0.0368 (0.0216)	
training:	Epoch: [55][180/233]	Loss 0.0189 (0.0216)	
training:	Epoch: [55][181/233]	Loss 0.0180 (0.0215)	
training:	Epoch: [55][182/233]	Loss 0.0346 (0.0216)	
training:	Epoch: [55][183/233]	Loss 0.0171 (0.0216)	
training:	Epoch: [55][184/233]	Loss 0.0198 (0.0216)	
training:	Epoch: [55][185/233]	Loss 0.0226 (0.0216)	
training:	Epoch: [55][186/233]	Loss 0.0187 (0.0216)	
training:	Epoch: [55][187/233]	Loss 0.0192 (0.0216)	
training:	Epoch: [55][188/233]	Loss 0.0207 (0.0215)	
training:	Epoch: [55][189/233]	Loss 0.0283 (0.0216)	
training:	Epoch: [55][190/233]	Loss 0.0261 (0.0216)	
training:	Epoch: [55][191/233]	Loss 0.0189 (0.0216)	
training:	Epoch: [55][192/233]	Loss 0.0187 (0.0216)	
training:	Epoch: [55][193/233]	Loss 0.0179 (0.0216)	
training:	Epoch: [55][194/233]	Loss 0.0189 (0.0215)	
training:	Epoch: [55][195/233]	Loss 0.0172 (0.0215)	
training:	Epoch: [55][196/233]	Loss 0.0196 (0.0215)	
training:	Epoch: [55][197/233]	Loss 0.0175 (0.0215)	
training:	Epoch: [55][198/233]	Loss 0.0184 (0.0215)	
training:	Epoch: [55][199/233]	Loss 0.0169 (0.0215)	
training:	Epoch: [55][200/233]	Loss 0.0170 (0.0214)	
training:	Epoch: [55][201/233]	Loss 0.0171 (0.0214)	
training:	Epoch: [55][202/233]	Loss 0.0169 (0.0214)	
training:	Epoch: [55][203/233]	Loss 0.0177 (0.0214)	
training:	Epoch: [55][204/233]	Loss 0.0227 (0.0214)	
training:	Epoch: [55][205/233]	Loss 0.0165 (0.0214)	
training:	Epoch: [55][206/233]	Loss 0.0159 (0.0213)	
training:	Epoch: [55][207/233]	Loss 0.0200 (0.0213)	
training:	Epoch: [55][208/233]	Loss 0.0204 (0.0213)	
training:	Epoch: [55][209/233]	Loss 0.0199 (0.0213)	
training:	Epoch: [55][210/233]	Loss 0.0158 (0.0213)	
training:	Epoch: [55][211/233]	Loss 0.0159 (0.0213)	
training:	Epoch: [55][212/233]	Loss 0.0169 (0.0212)	
training:	Epoch: [55][213/233]	Loss 0.0161 (0.0212)	
training:	Epoch: [55][214/233]	Loss 0.0221 (0.0212)	
training:	Epoch: [55][215/233]	Loss 0.0156 (0.0212)	
training:	Epoch: [55][216/233]	Loss 0.0164 (0.0212)	
training:	Epoch: [55][217/233]	Loss 0.0301 (0.0212)	
training:	Epoch: [55][218/233]	Loss 0.0191 (0.0212)	
training:	Epoch: [55][219/233]	Loss 0.0181 (0.0212)	
training:	Epoch: [55][220/233]	Loss 0.0472 (0.0213)	
training:	Epoch: [55][221/233]	Loss 0.0153 (0.0213)	
training:	Epoch: [55][222/233]	Loss 0.0225 (0.0213)	
training:	Epoch: [55][223/233]	Loss 0.0268 (0.0213)	
training:	Epoch: [55][224/233]	Loss 0.0212 (0.0213)	
training:	Epoch: [55][225/233]	Loss 0.0322 (0.0214)	
training:	Epoch: [55][226/233]	Loss 0.0166 (0.0213)	
training:	Epoch: [55][227/233]	Loss 0.0171 (0.0213)	
training:	Epoch: [55][228/233]	Loss 0.0243 (0.0213)	
training:	Epoch: [55][229/233]	Loss 0.0252 (0.0213)	
training:	Epoch: [55][230/233]	Loss 0.0275 (0.0214)	
training:	Epoch: [55][231/233]	Loss 0.0416 (0.0215)	
training:	Epoch: [55][232/233]	Loss 0.0229 (0.0215)	
training:	Epoch: [55][233/233]	Loss 0.0175 (0.0214)	
Training:	 Loss: 0.0214

Training:	 ACC: 1.0000 1.0000 1.0000 1.0000
Validation:	 ACC: 0.8638 0.8625 0.8352 0.8924
Validation:	 Best_BACC: 0.8673 0.8657 0.8332 0.9013
Validation:	 Loss: 0.3263
Pretraining:	Epoch 56/200
----------
training:	Epoch: [56][1/233]	Loss 0.0158 (0.0158)	
training:	Epoch: [56][2/233]	Loss 0.0197 (0.0178)	
training:	Epoch: [56][3/233]	Loss 0.0167 (0.0174)	
training:	Epoch: [56][4/233]	Loss 0.0184 (0.0177)	
training:	Epoch: [56][5/233]	Loss 0.0215 (0.0184)	
training:	Epoch: [56][6/233]	Loss 0.0238 (0.0193)	
training:	Epoch: [56][7/233]	Loss 0.0159 (0.0188)	
training:	Epoch: [56][8/233]	Loss 0.0169 (0.0186)	
training:	Epoch: [56][9/233]	Loss 0.0334 (0.0202)	
training:	Epoch: [56][10/233]	Loss 0.0157 (0.0198)	
training:	Epoch: [56][11/233]	Loss 0.0319 (0.0209)	
training:	Epoch: [56][12/233]	Loss 0.0158 (0.0205)	
training:	Epoch: [56][13/233]	Loss 0.0242 (0.0208)	
training:	Epoch: [56][14/233]	Loss 0.0387 (0.0220)	
training:	Epoch: [56][15/233]	Loss 0.0148 (0.0216)	
training:	Epoch: [56][16/233]	Loss 0.0160 (0.0212)	
training:	Epoch: [56][17/233]	Loss 0.0192 (0.0211)	
training:	Epoch: [56][18/233]	Loss 0.0292 (0.0215)	
training:	Epoch: [56][19/233]	Loss 0.0164 (0.0213)	
training:	Epoch: [56][20/233]	Loss 0.0164 (0.0210)	
training:	Epoch: [56][21/233]	Loss 0.0203 (0.0210)	
training:	Epoch: [56][22/233]	Loss 0.0157 (0.0208)	
training:	Epoch: [56][23/233]	Loss 0.0192 (0.0207)	
training:	Epoch: [56][24/233]	Loss 0.0166 (0.0205)	
training:	Epoch: [56][25/233]	Loss 0.0179 (0.0204)	
training:	Epoch: [56][26/233]	Loss 0.0148 (0.0202)	
training:	Epoch: [56][27/233]	Loss 0.0163 (0.0200)	
training:	Epoch: [56][28/233]	Loss 0.0200 (0.0200)	
training:	Epoch: [56][29/233]	Loss 0.0173 (0.0200)	
training:	Epoch: [56][30/233]	Loss 0.0261 (0.0202)	
training:	Epoch: [56][31/233]	Loss 0.0259 (0.0203)	
training:	Epoch: [56][32/233]	Loss 0.0189 (0.0203)	
training:	Epoch: [56][33/233]	Loss 0.0153 (0.0201)	
training:	Epoch: [56][34/233]	Loss 0.0259 (0.0203)	
training:	Epoch: [56][35/233]	Loss 0.0231 (0.0204)	
training:	Epoch: [56][36/233]	Loss 0.0158 (0.0203)	
training:	Epoch: [56][37/233]	Loss 0.0165 (0.0202)	
training:	Epoch: [56][38/233]	Loss 0.0178 (0.0201)	
training:	Epoch: [56][39/233]	Loss 0.0188 (0.0201)	
training:	Epoch: [56][40/233]	Loss 0.0160 (0.0200)	
training:	Epoch: [56][41/233]	Loss 0.0152 (0.0199)	
training:	Epoch: [56][42/233]	Loss 0.0258 (0.0200)	
training:	Epoch: [56][43/233]	Loss 0.0249 (0.0201)	
training:	Epoch: [56][44/233]	Loss 0.0254 (0.0202)	
training:	Epoch: [56][45/233]	Loss 0.0172 (0.0202)	
training:	Epoch: [56][46/233]	Loss 0.0199 (0.0202)	
training:	Epoch: [56][47/233]	Loss 0.0159 (0.0201)	
training:	Epoch: [56][48/233]	Loss 0.0231 (0.0201)	
training:	Epoch: [56][49/233]	Loss 0.0256 (0.0202)	
training:	Epoch: [56][50/233]	Loss 0.0161 (0.0202)	
training:	Epoch: [56][51/233]	Loss 0.0214 (0.0202)	
training:	Epoch: [56][52/233]	Loss 0.0302 (0.0204)	
training:	Epoch: [56][53/233]	Loss 0.0170 (0.0203)	
training:	Epoch: [56][54/233]	Loss 0.0344 (0.0206)	
training:	Epoch: [56][55/233]	Loss 0.0216 (0.0206)	
training:	Epoch: [56][56/233]	Loss 0.0259 (0.0207)	
training:	Epoch: [56][57/233]	Loss 0.0205 (0.0207)	
training:	Epoch: [56][58/233]	Loss 0.0193 (0.0207)	
training:	Epoch: [56][59/233]	Loss 0.0153 (0.0206)	
training:	Epoch: [56][60/233]	Loss 0.0235 (0.0206)	
training:	Epoch: [56][61/233]	Loss 0.0219 (0.0206)	
training:	Epoch: [56][62/233]	Loss 0.0162 (0.0206)	
training:	Epoch: [56][63/233]	Loss 0.0164 (0.0205)	
training:	Epoch: [56][64/233]	Loss 0.0178 (0.0205)	
training:	Epoch: [56][65/233]	Loss 0.0276 (0.0206)	
training:	Epoch: [56][66/233]	Loss 0.0176 (0.0205)	
training:	Epoch: [56][67/233]	Loss 0.0192 (0.0205)	
training:	Epoch: [56][68/233]	Loss 0.0181 (0.0205)	
training:	Epoch: [56][69/233]	Loss 0.0151 (0.0204)	
training:	Epoch: [56][70/233]	Loss 0.0163 (0.0203)	
training:	Epoch: [56][71/233]	Loss 0.0450 (0.0207)	
training:	Epoch: [56][72/233]	Loss 0.0258 (0.0208)	
training:	Epoch: [56][73/233]	Loss 0.0181 (0.0207)	
training:	Epoch: [56][74/233]	Loss 0.0154 (0.0206)	
training:	Epoch: [56][75/233]	Loss 0.0196 (0.0206)	
training:	Epoch: [56][76/233]	Loss 0.0169 (0.0206)	
training:	Epoch: [56][77/233]	Loss 0.0189 (0.0206)	
training:	Epoch: [56][78/233]	Loss 0.0153 (0.0205)	
training:	Epoch: [56][79/233]	Loss 0.0180 (0.0205)	
training:	Epoch: [56][80/233]	Loss 0.0183 (0.0204)	
training:	Epoch: [56][81/233]	Loss 0.0173 (0.0204)	
training:	Epoch: [56][82/233]	Loss 0.0288 (0.0205)	
training:	Epoch: [56][83/233]	Loss 0.0156 (0.0204)	
training:	Epoch: [56][84/233]	Loss 0.0168 (0.0204)	
training:	Epoch: [56][85/233]	Loss 0.0181 (0.0204)	
training:	Epoch: [56][86/233]	Loss 0.0164 (0.0203)	
training:	Epoch: [56][87/233]	Loss 0.0269 (0.0204)	
training:	Epoch: [56][88/233]	Loss 0.0161 (0.0203)	
training:	Epoch: [56][89/233]	Loss 0.0156 (0.0203)	
training:	Epoch: [56][90/233]	Loss 0.0233 (0.0203)	
training:	Epoch: [56][91/233]	Loss 0.0493 (0.0206)	
training:	Epoch: [56][92/233]	Loss 0.0178 (0.0206)	
training:	Epoch: [56][93/233]	Loss 0.0171 (0.0206)	
training:	Epoch: [56][94/233]	Loss 0.0225 (0.0206)	
training:	Epoch: [56][95/233]	Loss 0.0195 (0.0206)	
training:	Epoch: [56][96/233]	Loss 0.0479 (0.0209)	
training:	Epoch: [56][97/233]	Loss 0.0208 (0.0209)	
training:	Epoch: [56][98/233]	Loss 0.0196 (0.0209)	
training:	Epoch: [56][99/233]	Loss 0.0170 (0.0208)	
training:	Epoch: [56][100/233]	Loss 0.0192 (0.0208)	
training:	Epoch: [56][101/233]	Loss 0.0255 (0.0208)	
training:	Epoch: [56][102/233]	Loss 0.0227 (0.0209)	
training:	Epoch: [56][103/233]	Loss 0.0167 (0.0208)	
training:	Epoch: [56][104/233]	Loss 0.0180 (0.0208)	
training:	Epoch: [56][105/233]	Loss 0.0203 (0.0208)	
training:	Epoch: [56][106/233]	Loss 0.0208 (0.0208)	
training:	Epoch: [56][107/233]	Loss 0.0169 (0.0208)	
training:	Epoch: [56][108/233]	Loss 0.0205 (0.0208)	
training:	Epoch: [56][109/233]	Loss 0.0177 (0.0207)	
training:	Epoch: [56][110/233]	Loss 0.0365 (0.0209)	
training:	Epoch: [56][111/233]	Loss 0.0196 (0.0209)	
training:	Epoch: [56][112/233]	Loss 0.0210 (0.0209)	
training:	Epoch: [56][113/233]	Loss 0.0176 (0.0208)	
training:	Epoch: [56][114/233]	Loss 0.0226 (0.0208)	
training:	Epoch: [56][115/233]	Loss 0.0154 (0.0208)	
training:	Epoch: [56][116/233]	Loss 0.0158 (0.0208)	
training:	Epoch: [56][117/233]	Loss 0.0354 (0.0209)	
training:	Epoch: [56][118/233]	Loss 0.0306 (0.0210)	
training:	Epoch: [56][119/233]	Loss 0.0231 (0.0210)	
training:	Epoch: [56][120/233]	Loss 0.0177 (0.0210)	
training:	Epoch: [56][121/233]	Loss 0.0168 (0.0209)	
training:	Epoch: [56][122/233]	Loss 0.0168 (0.0209)	
training:	Epoch: [56][123/233]	Loss 0.0176 (0.0209)	
training:	Epoch: [56][124/233]	Loss 0.0196 (0.0208)	
training:	Epoch: [56][125/233]	Loss 0.0165 (0.0208)	
training:	Epoch: [56][126/233]	Loss 0.0323 (0.0209)	
training:	Epoch: [56][127/233]	Loss 0.0166 (0.0209)	
training:	Epoch: [56][128/233]	Loss 0.0178 (0.0208)	
training:	Epoch: [56][129/233]	Loss 0.0266 (0.0209)	
training:	Epoch: [56][130/233]	Loss 0.0178 (0.0209)	
training:	Epoch: [56][131/233]	Loss 0.0145 (0.0208)	
training:	Epoch: [56][132/233]	Loss 0.0185 (0.0208)	
training:	Epoch: [56][133/233]	Loss 0.0231 (0.0208)	
training:	Epoch: [56][134/233]	Loss 0.0220 (0.0208)	
training:	Epoch: [56][135/233]	Loss 0.0214 (0.0208)	
training:	Epoch: [56][136/233]	Loss 0.0179 (0.0208)	
training:	Epoch: [56][137/233]	Loss 0.0203 (0.0208)	
training:	Epoch: [56][138/233]	Loss 0.0451 (0.0210)	
training:	Epoch: [56][139/233]	Loss 0.0181 (0.0210)	
training:	Epoch: [56][140/233]	Loss 0.0229 (0.0210)	
training:	Epoch: [56][141/233]	Loss 0.0213 (0.0210)	
training:	Epoch: [56][142/233]	Loss 0.0162 (0.0209)	
training:	Epoch: [56][143/233]	Loss 0.0155 (0.0209)	
training:	Epoch: [56][144/233]	Loss 0.0170 (0.0209)	
training:	Epoch: [56][145/233]	Loss 0.0353 (0.0210)	
training:	Epoch: [56][146/233]	Loss 0.0185 (0.0210)	
training:	Epoch: [56][147/233]	Loss 0.0182 (0.0209)	
training:	Epoch: [56][148/233]	Loss 0.0170 (0.0209)	
training:	Epoch: [56][149/233]	Loss 0.0196 (0.0209)	
training:	Epoch: [56][150/233]	Loss 0.0211 (0.0209)	
training:	Epoch: [56][151/233]	Loss 0.0263 (0.0209)	
training:	Epoch: [56][152/233]	Loss 0.0208 (0.0209)	
training:	Epoch: [56][153/233]	Loss 0.0181 (0.0209)	
training:	Epoch: [56][154/233]	Loss 0.0183 (0.0209)	
training:	Epoch: [56][155/233]	Loss 0.0189 (0.0209)	
training:	Epoch: [56][156/233]	Loss 0.0172 (0.0209)	
training:	Epoch: [56][157/233]	Loss 0.0165 (0.0208)	
training:	Epoch: [56][158/233]	Loss 0.0186 (0.0208)	
training:	Epoch: [56][159/233]	Loss 0.0199 (0.0208)	
training:	Epoch: [56][160/233]	Loss 0.0141 (0.0208)	
training:	Epoch: [56][161/233]	Loss 0.0366 (0.0209)	
training:	Epoch: [56][162/233]	Loss 0.0224 (0.0209)	
training:	Epoch: [56][163/233]	Loss 0.0163 (0.0209)	
training:	Epoch: [56][164/233]	Loss 0.0181 (0.0208)	
training:	Epoch: [56][165/233]	Loss 0.0156 (0.0208)	
training:	Epoch: [56][166/233]	Loss 0.0182 (0.0208)	
training:	Epoch: [56][167/233]	Loss 0.0205 (0.0208)	
training:	Epoch: [56][168/233]	Loss 0.0162 (0.0208)	
training:	Epoch: [56][169/233]	Loss 0.0162 (0.0207)	
training:	Epoch: [56][170/233]	Loss 0.0348 (0.0208)	
training:	Epoch: [56][171/233]	Loss 0.0169 (0.0208)	
training:	Epoch: [56][172/233]	Loss 0.0220 (0.0208)	
training:	Epoch: [56][173/233]	Loss 0.0181 (0.0208)	
training:	Epoch: [56][174/233]	Loss 0.0280 (0.0208)	
training:	Epoch: [56][175/233]	Loss 0.0166 (0.0208)	
training:	Epoch: [56][176/233]	Loss 0.0149 (0.0208)	
training:	Epoch: [56][177/233]	Loss 0.0428 (0.0209)	
training:	Epoch: [56][178/233]	Loss 0.0423 (0.0210)	
training:	Epoch: [56][179/233]	Loss 0.0213 (0.0210)	
training:	Epoch: [56][180/233]	Loss 0.0159 (0.0210)	
training:	Epoch: [56][181/233]	Loss 0.0173 (0.0210)	
training:	Epoch: [56][182/233]	Loss 0.0184 (0.0210)	
training:	Epoch: [56][183/233]	Loss 0.0247 (0.0210)	
training:	Epoch: [56][184/233]	Loss 0.0169 (0.0210)	
training:	Epoch: [56][185/233]	Loss 0.0162 (0.0209)	
training:	Epoch: [56][186/233]	Loss 0.0202 (0.0209)	
training:	Epoch: [56][187/233]	Loss 0.0164 (0.0209)	
training:	Epoch: [56][188/233]	Loss 0.0208 (0.0209)	
training:	Epoch: [56][189/233]	Loss 0.0269 (0.0209)	
training:	Epoch: [56][190/233]	Loss 0.0378 (0.0210)	
training:	Epoch: [56][191/233]	Loss 0.0173 (0.0210)	
training:	Epoch: [56][192/233]	Loss 0.0223 (0.0210)	
training:	Epoch: [56][193/233]	Loss 0.0252 (0.0210)	
training:	Epoch: [56][194/233]	Loss 0.0200 (0.0210)	
training:	Epoch: [56][195/233]	Loss 0.0169 (0.0210)	
training:	Epoch: [56][196/233]	Loss 0.0165 (0.0210)	
training:	Epoch: [56][197/233]	Loss 0.0161 (0.0210)	
training:	Epoch: [56][198/233]	Loss 0.0191 (0.0209)	
training:	Epoch: [56][199/233]	Loss 0.0218 (0.0210)	
training:	Epoch: [56][200/233]	Loss 0.0223 (0.0210)	
training:	Epoch: [56][201/233]	Loss 0.0302 (0.0210)	
training:	Epoch: [56][202/233]	Loss 0.0163 (0.0210)	
training:	Epoch: [56][203/233]	Loss 0.0154 (0.0210)	
training:	Epoch: [56][204/233]	Loss 0.0158 (0.0209)	
training:	Epoch: [56][205/233]	Loss 0.0158 (0.0209)	
training:	Epoch: [56][206/233]	Loss 0.0161 (0.0209)	
training:	Epoch: [56][207/233]	Loss 0.0191 (0.0209)	
training:	Epoch: [56][208/233]	Loss 0.0155 (0.0208)	
training:	Epoch: [56][209/233]	Loss 0.0193 (0.0208)	
training:	Epoch: [56][210/233]	Loss 0.0324 (0.0209)	
training:	Epoch: [56][211/233]	Loss 0.0241 (0.0209)	
training:	Epoch: [56][212/233]	Loss 0.0220 (0.0209)	
training:	Epoch: [56][213/233]	Loss 0.0253 (0.0209)	
training:	Epoch: [56][214/233]	Loss 0.0151 (0.0209)	
training:	Epoch: [56][215/233]	Loss 0.0219 (0.0209)	
training:	Epoch: [56][216/233]	Loss 0.0259 (0.0209)	
training:	Epoch: [56][217/233]	Loss 0.0176 (0.0209)	
training:	Epoch: [56][218/233]	Loss 0.0221 (0.0209)	
training:	Epoch: [56][219/233]	Loss 0.0157 (0.0209)	
training:	Epoch: [56][220/233]	Loss 0.0262 (0.0209)	
training:	Epoch: [56][221/233]	Loss 0.0156 (0.0209)	
training:	Epoch: [56][222/233]	Loss 0.0178 (0.0209)	
training:	Epoch: [56][223/233]	Loss 0.0203 (0.0209)	
training:	Epoch: [56][224/233]	Loss 0.0171 (0.0209)	
training:	Epoch: [56][225/233]	Loss 0.0187 (0.0209)	
training:	Epoch: [56][226/233]	Loss 0.0168 (0.0208)	
training:	Epoch: [56][227/233]	Loss 0.0372 (0.0209)	
training:	Epoch: [56][228/233]	Loss 0.0346 (0.0210)	
training:	Epoch: [56][229/233]	Loss 0.0321 (0.0210)	
training:	Epoch: [56][230/233]	Loss 0.0189 (0.0210)	
training:	Epoch: [56][231/233]	Loss 0.0271 (0.0210)	
training:	Epoch: [56][232/233]	Loss 0.0280 (0.0211)	
training:	Epoch: [56][233/233]	Loss 0.0185 (0.0211)	
Training:	 Loss: 0.0210

Training:	 ACC: 1.0000 1.0000 1.0000 1.0000
Validation:	 ACC: 0.8622 0.8604 0.8219 0.9025
Validation:	 Best_BACC: 0.8673 0.8657 0.8332 0.9013
Validation:	 Loss: 0.3303
Pretraining:	Epoch 57/200
----------
training:	Epoch: [57][1/233]	Loss 0.0177 (0.0177)	
training:	Epoch: [57][2/233]	Loss 0.0203 (0.0190)	
training:	Epoch: [57][3/233]	Loss 0.0192 (0.0191)	
training:	Epoch: [57][4/233]	Loss 0.0157 (0.0182)	
training:	Epoch: [57][5/233]	Loss 0.0201 (0.0186)	
training:	Epoch: [57][6/233]	Loss 0.0286 (0.0203)	
training:	Epoch: [57][7/233]	Loss 0.0163 (0.0197)	
training:	Epoch: [57][8/233]	Loss 0.0218 (0.0200)	
training:	Epoch: [57][9/233]	Loss 0.0179 (0.0197)	
training:	Epoch: [57][10/233]	Loss 0.0144 (0.0192)	
training:	Epoch: [57][11/233]	Loss 0.0155 (0.0189)	
training:	Epoch: [57][12/233]	Loss 0.0164 (0.0187)	
training:	Epoch: [57][13/233]	Loss 0.0200 (0.0188)	
training:	Epoch: [57][14/233]	Loss 0.0175 (0.0187)	
training:	Epoch: [57][15/233]	Loss 0.0206 (0.0188)	
training:	Epoch: [57][16/233]	Loss 0.0253 (0.0192)	
training:	Epoch: [57][17/233]	Loss 0.0174 (0.0191)	
training:	Epoch: [57][18/233]	Loss 0.0184 (0.0191)	
training:	Epoch: [57][19/233]	Loss 0.0202 (0.0191)	
training:	Epoch: [57][20/233]	Loss 0.0179 (0.0191)	
training:	Epoch: [57][21/233]	Loss 0.0205 (0.0191)	
training:	Epoch: [57][22/233]	Loss 0.0151 (0.0189)	
training:	Epoch: [57][23/233]	Loss 0.0185 (0.0189)	
training:	Epoch: [57][24/233]	Loss 0.0165 (0.0188)	
training:	Epoch: [57][25/233]	Loss 0.0241 (0.0190)	
training:	Epoch: [57][26/233]	Loss 0.0224 (0.0192)	
training:	Epoch: [57][27/233]	Loss 0.0166 (0.0191)	
training:	Epoch: [57][28/233]	Loss 0.0151 (0.0189)	
training:	Epoch: [57][29/233]	Loss 0.0150 (0.0188)	
training:	Epoch: [57][30/233]	Loss 0.0171 (0.0187)	
training:	Epoch: [57][31/233]	Loss 0.0173 (0.0187)	
training:	Epoch: [57][32/233]	Loss 0.0181 (0.0187)	
training:	Epoch: [57][33/233]	Loss 0.0176 (0.0186)	
training:	Epoch: [57][34/233]	Loss 0.0189 (0.0186)	
training:	Epoch: [57][35/233]	Loss 0.0176 (0.0186)	
training:	Epoch: [57][36/233]	Loss 0.0164 (0.0186)	
training:	Epoch: [57][37/233]	Loss 0.0186 (0.0186)	
training:	Epoch: [57][38/233]	Loss 0.0309 (0.0189)	
training:	Epoch: [57][39/233]	Loss 0.0164 (0.0188)	
training:	Epoch: [57][40/233]	Loss 0.0175 (0.0188)	
training:	Epoch: [57][41/233]	Loss 0.0164 (0.0187)	
training:	Epoch: [57][42/233]	Loss 0.0157 (0.0187)	
training:	Epoch: [57][43/233]	Loss 0.0166 (0.0186)	
training:	Epoch: [57][44/233]	Loss 0.0203 (0.0186)	
training:	Epoch: [57][45/233]	Loss 0.0180 (0.0186)	
training:	Epoch: [57][46/233]	Loss 0.0171 (0.0186)	
training:	Epoch: [57][47/233]	Loss 0.0547 (0.0194)	
training:	Epoch: [57][48/233]	Loss 0.0175 (0.0193)	
training:	Epoch: [57][49/233]	Loss 0.0164 (0.0193)	
training:	Epoch: [57][50/233]	Loss 0.0233 (0.0193)	
training:	Epoch: [57][51/233]	Loss 0.0315 (0.0196)	
training:	Epoch: [57][52/233]	Loss 0.0247 (0.0197)	
training:	Epoch: [57][53/233]	Loss 0.0204 (0.0197)	
training:	Epoch: [57][54/233]	Loss 0.0157 (0.0196)	
training:	Epoch: [57][55/233]	Loss 0.0243 (0.0197)	
training:	Epoch: [57][56/233]	Loss 0.0138 (0.0196)	
training:	Epoch: [57][57/233]	Loss 0.0159 (0.0195)	
training:	Epoch: [57][58/233]	Loss 0.0175 (0.0195)	
training:	Epoch: [57][59/233]	Loss 0.0145 (0.0194)	
training:	Epoch: [57][60/233]	Loss 0.0156 (0.0194)	
training:	Epoch: [57][61/233]	Loss 0.0210 (0.0194)	
training:	Epoch: [57][62/233]	Loss 0.0199 (0.0194)	
training:	Epoch: [57][63/233]	Loss 0.0205 (0.0194)	
training:	Epoch: [57][64/233]	Loss 0.0175 (0.0194)	
training:	Epoch: [57][65/233]	Loss 0.0179 (0.0193)	
training:	Epoch: [57][66/233]	Loss 0.0156 (0.0193)	
training:	Epoch: [57][67/233]	Loss 0.0204 (0.0193)	
training:	Epoch: [57][68/233]	Loss 0.0184 (0.0193)	
training:	Epoch: [57][69/233]	Loss 0.0165 (0.0193)	
training:	Epoch: [57][70/233]	Loss 0.0178 (0.0192)	
training:	Epoch: [57][71/233]	Loss 0.0171 (0.0192)	
training:	Epoch: [57][72/233]	Loss 0.0153 (0.0192)	
training:	Epoch: [57][73/233]	Loss 0.0143 (0.0191)	
training:	Epoch: [57][74/233]	Loss 0.0182 (0.0191)	
training:	Epoch: [57][75/233]	Loss 0.0173 (0.0191)	
training:	Epoch: [57][76/233]	Loss 0.0150 (0.0190)	
training:	Epoch: [57][77/233]	Loss 0.0215 (0.0190)	
training:	Epoch: [57][78/233]	Loss 0.0165 (0.0190)	
training:	Epoch: [57][79/233]	Loss 0.0157 (0.0190)	
training:	Epoch: [57][80/233]	Loss 0.0171 (0.0189)	
training:	Epoch: [57][81/233]	Loss 0.0233 (0.0190)	
training:	Epoch: [57][82/233]	Loss 0.0163 (0.0190)	
training:	Epoch: [57][83/233]	Loss 0.0203 (0.0190)	
training:	Epoch: [57][84/233]	Loss 0.0177 (0.0190)	
training:	Epoch: [57][85/233]	Loss 0.0173 (0.0189)	
training:	Epoch: [57][86/233]	Loss 0.0155 (0.0189)	
training:	Epoch: [57][87/233]	Loss 0.0463 (0.0192)	
training:	Epoch: [57][88/233]	Loss 0.0193 (0.0192)	
training:	Epoch: [57][89/233]	Loss 0.0156 (0.0192)	
training:	Epoch: [57][90/233]	Loss 0.0183 (0.0192)	
training:	Epoch: [57][91/233]	Loss 0.0264 (0.0192)	
training:	Epoch: [57][92/233]	Loss 0.0148 (0.0192)	
training:	Epoch: [57][93/233]	Loss 0.0194 (0.0192)	
training:	Epoch: [57][94/233]	Loss 0.0261 (0.0193)	
training:	Epoch: [57][95/233]	Loss 0.0160 (0.0192)	
training:	Epoch: [57][96/233]	Loss 0.0184 (0.0192)	
training:	Epoch: [57][97/233]	Loss 0.0257 (0.0193)	
training:	Epoch: [57][98/233]	Loss 0.0191 (0.0193)	
training:	Epoch: [57][99/233]	Loss 0.0156 (0.0193)	
training:	Epoch: [57][100/233]	Loss 0.0238 (0.0193)	
training:	Epoch: [57][101/233]	Loss 0.0215 (0.0193)	
training:	Epoch: [57][102/233]	Loss 0.0157 (0.0193)	
training:	Epoch: [57][103/233]	Loss 0.0242 (0.0193)	
training:	Epoch: [57][104/233]	Loss 0.0190 (0.0193)	
training:	Epoch: [57][105/233]	Loss 0.0361 (0.0195)	
training:	Epoch: [57][106/233]	Loss 0.0154 (0.0195)	
training:	Epoch: [57][107/233]	Loss 0.0190 (0.0194)	
training:	Epoch: [57][108/233]	Loss 0.0178 (0.0194)	
training:	Epoch: [57][109/233]	Loss 0.0165 (0.0194)	
training:	Epoch: [57][110/233]	Loss 0.0158 (0.0194)	
training:	Epoch: [57][111/233]	Loss 0.0181 (0.0194)	
training:	Epoch: [57][112/233]	Loss 0.0180 (0.0193)	
training:	Epoch: [57][113/233]	Loss 0.0150 (0.0193)	
training:	Epoch: [57][114/233]	Loss 0.0208 (0.0193)	
training:	Epoch: [57][115/233]	Loss 0.0164 (0.0193)	
training:	Epoch: [57][116/233]	Loss 0.0157 (0.0193)	
training:	Epoch: [57][117/233]	Loss 0.0177 (0.0193)	
training:	Epoch: [57][118/233]	Loss 0.0162 (0.0192)	
training:	Epoch: [57][119/233]	Loss 0.0177 (0.0192)	
training:	Epoch: [57][120/233]	Loss 0.0202 (0.0192)	
training:	Epoch: [57][121/233]	Loss 0.0288 (0.0193)	
training:	Epoch: [57][122/233]	Loss 0.0277 (0.0194)	
training:	Epoch: [57][123/233]	Loss 0.0194 (0.0194)	
training:	Epoch: [57][124/233]	Loss 0.0152 (0.0193)	
training:	Epoch: [57][125/233]	Loss 0.0176 (0.0193)	
training:	Epoch: [57][126/233]	Loss 0.0258 (0.0194)	
training:	Epoch: [57][127/233]	Loss 0.0417 (0.0196)	
training:	Epoch: [57][128/233]	Loss 0.0179 (0.0195)	
training:	Epoch: [57][129/233]	Loss 0.0304 (0.0196)	
training:	Epoch: [57][130/233]	Loss 0.0360 (0.0197)	
training:	Epoch: [57][131/233]	Loss 0.0152 (0.0197)	
training:	Epoch: [57][132/233]	Loss 0.0154 (0.0197)	
training:	Epoch: [57][133/233]	Loss 0.0409 (0.0198)	
training:	Epoch: [57][134/233]	Loss 0.0235 (0.0199)	
training:	Epoch: [57][135/233]	Loss 0.0141 (0.0198)	
training:	Epoch: [57][136/233]	Loss 0.0157 (0.0198)	
training:	Epoch: [57][137/233]	Loss 0.0346 (0.0199)	
training:	Epoch: [57][138/233]	Loss 0.0166 (0.0199)	
training:	Epoch: [57][139/233]	Loss 0.0177 (0.0199)	
training:	Epoch: [57][140/233]	Loss 0.0259 (0.0199)	
training:	Epoch: [57][141/233]	Loss 0.0163 (0.0199)	
training:	Epoch: [57][142/233]	Loss 0.0217 (0.0199)	
training:	Epoch: [57][143/233]	Loss 0.0177 (0.0199)	
training:	Epoch: [57][144/233]	Loss 0.0166 (0.0199)	
training:	Epoch: [57][145/233]	Loss 0.0319 (0.0199)	
training:	Epoch: [57][146/233]	Loss 0.0162 (0.0199)	
training:	Epoch: [57][147/233]	Loss 0.0158 (0.0199)	
training:	Epoch: [57][148/233]	Loss 0.0155 (0.0199)	
training:	Epoch: [57][149/233]	Loss 0.0245 (0.0199)	
training:	Epoch: [57][150/233]	Loss 0.0303 (0.0200)	
training:	Epoch: [57][151/233]	Loss 0.0140 (0.0199)	
training:	Epoch: [57][152/233]	Loss 0.0194 (0.0199)	
training:	Epoch: [57][153/233]	Loss 0.0220 (0.0199)	
training:	Epoch: [57][154/233]	Loss 0.0276 (0.0200)	
training:	Epoch: [57][155/233]	Loss 0.0176 (0.0200)	
training:	Epoch: [57][156/233]	Loss 0.0223 (0.0200)	
training:	Epoch: [57][157/233]	Loss 0.0182 (0.0200)	
training:	Epoch: [57][158/233]	Loss 0.0191 (0.0200)	
training:	Epoch: [57][159/233]	Loss 0.0308 (0.0200)	
training:	Epoch: [57][160/233]	Loss 0.0164 (0.0200)	
training:	Epoch: [57][161/233]	Loss 0.0186 (0.0200)	
training:	Epoch: [57][162/233]	Loss 0.0242 (0.0200)	
training:	Epoch: [57][163/233]	Loss 0.0165 (0.0200)	
training:	Epoch: [57][164/233]	Loss 0.0208 (0.0200)	
training:	Epoch: [57][165/233]	Loss 0.0216 (0.0200)	
training:	Epoch: [57][166/233]	Loss 0.0152 (0.0200)	
training:	Epoch: [57][167/233]	Loss 0.0154 (0.0200)	
training:	Epoch: [57][168/233]	Loss 0.0190 (0.0200)	
training:	Epoch: [57][169/233]	Loss 0.0278 (0.0200)	
training:	Epoch: [57][170/233]	Loss 0.0162 (0.0200)	
training:	Epoch: [57][171/233]	Loss 0.0181 (0.0200)	
training:	Epoch: [57][172/233]	Loss 0.0157 (0.0199)	
training:	Epoch: [57][173/233]	Loss 0.0156 (0.0199)	
training:	Epoch: [57][174/233]	Loss 0.0160 (0.0199)	
training:	Epoch: [57][175/233]	Loss 0.0138 (0.0199)	
training:	Epoch: [57][176/233]	Loss 0.0238 (0.0199)	
training:	Epoch: [57][177/233]	Loss 0.0158 (0.0199)	
training:	Epoch: [57][178/233]	Loss 0.0162 (0.0198)	
training:	Epoch: [57][179/233]	Loss 0.0415 (0.0200)	
training:	Epoch: [57][180/233]	Loss 0.0433 (0.0201)	
training:	Epoch: [57][181/233]	Loss 0.0197 (0.0201)	
training:	Epoch: [57][182/233]	Loss 0.0205 (0.0201)	
training:	Epoch: [57][183/233]	Loss 0.0178 (0.0201)	
training:	Epoch: [57][184/233]	Loss 0.0170 (0.0201)	
training:	Epoch: [57][185/233]	Loss 0.0143 (0.0200)	
training:	Epoch: [57][186/233]	Loss 0.0185 (0.0200)	
training:	Epoch: [57][187/233]	Loss 0.0248 (0.0200)	
training:	Epoch: [57][188/233]	Loss 0.0181 (0.0200)	
training:	Epoch: [57][189/233]	Loss 0.0185 (0.0200)	
training:	Epoch: [57][190/233]	Loss 0.0159 (0.0200)	
training:	Epoch: [57][191/233]	Loss 0.0218 (0.0200)	
training:	Epoch: [57][192/233]	Loss 0.0207 (0.0200)	
training:	Epoch: [57][193/233]	Loss 0.0195 (0.0200)	
training:	Epoch: [57][194/233]	Loss 0.0173 (0.0200)	
training:	Epoch: [57][195/233]	Loss 0.0160 (0.0200)	
training:	Epoch: [57][196/233]	Loss 0.0177 (0.0200)	
training:	Epoch: [57][197/233]	Loss 0.0192 (0.0200)	
training:	Epoch: [57][198/233]	Loss 0.0163 (0.0199)	
training:	Epoch: [57][199/233]	Loss 0.0163 (0.0199)	
training:	Epoch: [57][200/233]	Loss 0.0152 (0.0199)	
training:	Epoch: [57][201/233]	Loss 0.0159 (0.0199)	
training:	Epoch: [57][202/233]	Loss 0.0141 (0.0199)	
training:	Epoch: [57][203/233]	Loss 0.0217 (0.0199)	
training:	Epoch: [57][204/233]	Loss 0.0255 (0.0199)	
training:	Epoch: [57][205/233]	Loss 0.0554 (0.0201)	
training:	Epoch: [57][206/233]	Loss 0.0148 (0.0200)	
training:	Epoch: [57][207/233]	Loss 0.0150 (0.0200)	
training:	Epoch: [57][208/233]	Loss 0.0161 (0.0200)	
training:	Epoch: [57][209/233]	Loss 0.0151 (0.0200)	
training:	Epoch: [57][210/233]	Loss 0.0258 (0.0200)	
training:	Epoch: [57][211/233]	Loss 0.0163 (0.0200)	
training:	Epoch: [57][212/233]	Loss 0.0201 (0.0200)	
training:	Epoch: [57][213/233]	Loss 0.0166 (0.0200)	
training:	Epoch: [57][214/233]	Loss 0.0180 (0.0200)	
training:	Epoch: [57][215/233]	Loss 0.0170 (0.0199)	
training:	Epoch: [57][216/233]	Loss 0.0679 (0.0202)	
training:	Epoch: [57][217/233]	Loss 0.0233 (0.0202)	
training:	Epoch: [57][218/233]	Loss 0.0143 (0.0202)	
training:	Epoch: [57][219/233]	Loss 0.0176 (0.0201)	
training:	Epoch: [57][220/233]	Loss 0.0157 (0.0201)	
training:	Epoch: [57][221/233]	Loss 0.0155 (0.0201)	
training:	Epoch: [57][222/233]	Loss 0.0155 (0.0201)	
training:	Epoch: [57][223/233]	Loss 0.0419 (0.0202)	
training:	Epoch: [57][224/233]	Loss 0.0247 (0.0202)	
training:	Epoch: [57][225/233]	Loss 0.0172 (0.0202)	
training:	Epoch: [57][226/233]	Loss 0.0238 (0.0202)	
training:	Epoch: [57][227/233]	Loss 0.0267 (0.0202)	
training:	Epoch: [57][228/233]	Loss 0.0179 (0.0202)	
training:	Epoch: [57][229/233]	Loss 0.0172 (0.0202)	
training:	Epoch: [57][230/233]	Loss 0.0212 (0.0202)	
training:	Epoch: [57][231/233]	Loss 0.0159 (0.0202)	
training:	Epoch: [57][232/233]	Loss 0.0180 (0.0202)	
training:	Epoch: [57][233/233]	Loss 0.0197 (0.0202)	
Training:	 Loss: 0.0201

Training:	 ACC: 1.0000 1.0000 1.0000 1.0000
Validation:	 ACC: 0.8647 0.8636 0.8403 0.8890
Validation:	 Best_BACC: 0.8673 0.8657 0.8332 0.9013
Validation:	 Loss: 0.3246
Pretraining:	Epoch 58/200
----------
training:	Epoch: [58][1/233]	Loss 0.0144 (0.0144)	
training:	Epoch: [58][2/233]	Loss 0.0157 (0.0151)	
training:	Epoch: [58][3/233]	Loss 0.0171 (0.0158)	
training:	Epoch: [58][4/233]	Loss 0.0146 (0.0155)	
training:	Epoch: [58][5/233]	Loss 0.0219 (0.0167)	
training:	Epoch: [58][6/233]	Loss 0.0159 (0.0166)	
training:	Epoch: [58][7/233]	Loss 0.0290 (0.0184)	
training:	Epoch: [58][8/233]	Loss 0.0149 (0.0179)	
training:	Epoch: [58][9/233]	Loss 0.0162 (0.0177)	
training:	Epoch: [58][10/233]	Loss 0.0189 (0.0179)	
training:	Epoch: [58][11/233]	Loss 0.0161 (0.0177)	
training:	Epoch: [58][12/233]	Loss 0.0211 (0.0180)	
training:	Epoch: [58][13/233]	Loss 0.0181 (0.0180)	
training:	Epoch: [58][14/233]	Loss 0.0144 (0.0177)	
training:	Epoch: [58][15/233]	Loss 0.0249 (0.0182)	
training:	Epoch: [58][16/233]	Loss 0.0157 (0.0181)	
training:	Epoch: [58][17/233]	Loss 0.0139 (0.0178)	
training:	Epoch: [58][18/233]	Loss 0.0157 (0.0177)	
training:	Epoch: [58][19/233]	Loss 0.0143 (0.0175)	
training:	Epoch: [58][20/233]	Loss 0.0257 (0.0179)	
training:	Epoch: [58][21/233]	Loss 0.0181 (0.0179)	
training:	Epoch: [58][22/233]	Loss 0.0142 (0.0178)	
training:	Epoch: [58][23/233]	Loss 0.0156 (0.0177)	
training:	Epoch: [58][24/233]	Loss 0.0220 (0.0178)	
training:	Epoch: [58][25/233]	Loss 0.0187 (0.0179)	
training:	Epoch: [58][26/233]	Loss 0.0326 (0.0184)	
training:	Epoch: [58][27/233]	Loss 0.0210 (0.0185)	
training:	Epoch: [58][28/233]	Loss 0.0185 (0.0185)	
training:	Epoch: [58][29/233]	Loss 0.0240 (0.0187)	
training:	Epoch: [58][30/233]	Loss 0.0140 (0.0186)	
training:	Epoch: [58][31/233]	Loss 0.0153 (0.0185)	
training:	Epoch: [58][32/233]	Loss 0.0348 (0.0190)	
training:	Epoch: [58][33/233]	Loss 0.0176 (0.0189)	
training:	Epoch: [58][34/233]	Loss 0.0174 (0.0189)	
training:	Epoch: [58][35/233]	Loss 0.0166 (0.0188)	
training:	Epoch: [58][36/233]	Loss 0.0314 (0.0192)	
training:	Epoch: [58][37/233]	Loss 0.0142 (0.0190)	
training:	Epoch: [58][38/233]	Loss 0.0162 (0.0190)	
training:	Epoch: [58][39/233]	Loss 0.0303 (0.0193)	
training:	Epoch: [58][40/233]	Loss 0.0176 (0.0192)	
training:	Epoch: [58][41/233]	Loss 0.0158 (0.0191)	
training:	Epoch: [58][42/233]	Loss 0.0162 (0.0191)	
training:	Epoch: [58][43/233]	Loss 0.0139 (0.0189)	
training:	Epoch: [58][44/233]	Loss 0.0181 (0.0189)	
training:	Epoch: [58][45/233]	Loss 0.0196 (0.0189)	
training:	Epoch: [58][46/233]	Loss 0.0152 (0.0189)	
training:	Epoch: [58][47/233]	Loss 0.0272 (0.0190)	
training:	Epoch: [58][48/233]	Loss 0.0400 (0.0195)	
training:	Epoch: [58][49/233]	Loss 0.0195 (0.0195)	
training:	Epoch: [58][50/233]	Loss 0.0170 (0.0194)	
training:	Epoch: [58][51/233]	Loss 0.0315 (0.0197)	
training:	Epoch: [58][52/233]	Loss 0.0154 (0.0196)	
training:	Epoch: [58][53/233]	Loss 0.0201 (0.0196)	
training:	Epoch: [58][54/233]	Loss 0.0174 (0.0195)	
training:	Epoch: [58][55/233]	Loss 0.0144 (0.0194)	
training:	Epoch: [58][56/233]	Loss 0.0167 (0.0194)	
training:	Epoch: [58][57/233]	Loss 0.0167 (0.0194)	
training:	Epoch: [58][58/233]	Loss 0.0193 (0.0194)	
training:	Epoch: [58][59/233]	Loss 0.0145 (0.0193)	
training:	Epoch: [58][60/233]	Loss 0.0155 (0.0192)	
training:	Epoch: [58][61/233]	Loss 0.0161 (0.0192)	
training:	Epoch: [58][62/233]	Loss 0.0140 (0.0191)	
training:	Epoch: [58][63/233]	Loss 0.0157 (0.0190)	
training:	Epoch: [58][64/233]	Loss 0.0284 (0.0192)	
training:	Epoch: [58][65/233]	Loss 0.0152 (0.0191)	
training:	Epoch: [58][66/233]	Loss 0.0233 (0.0192)	
training:	Epoch: [58][67/233]	Loss 0.0163 (0.0191)	
training:	Epoch: [58][68/233]	Loss 0.0200 (0.0191)	
training:	Epoch: [58][69/233]	Loss 0.0282 (0.0193)	
training:	Epoch: [58][70/233]	Loss 0.0426 (0.0196)	
training:	Epoch: [58][71/233]	Loss 0.0259 (0.0197)	
training:	Epoch: [58][72/233]	Loss 0.0194 (0.0197)	
training:	Epoch: [58][73/233]	Loss 0.0185 (0.0197)	
training:	Epoch: [58][74/233]	Loss 0.0169 (0.0196)	
training:	Epoch: [58][75/233]	Loss 0.0196 (0.0196)	
training:	Epoch: [58][76/233]	Loss 0.0297 (0.0198)	
training:	Epoch: [58][77/233]	Loss 0.0150 (0.0197)	
training:	Epoch: [58][78/233]	Loss 0.0327 (0.0199)	
training:	Epoch: [58][79/233]	Loss 0.0203 (0.0199)	
training:	Epoch: [58][80/233]	Loss 0.0183 (0.0199)	
training:	Epoch: [58][81/233]	Loss 0.0163 (0.0198)	
training:	Epoch: [58][82/233]	Loss 0.0148 (0.0198)	
training:	Epoch: [58][83/233]	Loss 0.0191 (0.0197)	
training:	Epoch: [58][84/233]	Loss 0.0180 (0.0197)	
training:	Epoch: [58][85/233]	Loss 0.0196 (0.0197)	
training:	Epoch: [58][86/233]	Loss 0.0150 (0.0197)	
training:	Epoch: [58][87/233]	Loss 0.0157 (0.0196)	
training:	Epoch: [58][88/233]	Loss 0.0201 (0.0196)	
training:	Epoch: [58][89/233]	Loss 0.0143 (0.0196)	
training:	Epoch: [58][90/233]	Loss 0.0158 (0.0195)	
training:	Epoch: [58][91/233]	Loss 0.0151 (0.0195)	
training:	Epoch: [58][92/233]	Loss 0.0194 (0.0195)	
training:	Epoch: [58][93/233]	Loss 0.0161 (0.0194)	
training:	Epoch: [58][94/233]	Loss 0.0552 (0.0198)	
training:	Epoch: [58][95/233]	Loss 0.0569 (0.0202)	
training:	Epoch: [58][96/233]	Loss 0.0188 (0.0202)	
training:	Epoch: [58][97/233]	Loss 0.0243 (0.0202)	
training:	Epoch: [58][98/233]	Loss 0.0192 (0.0202)	
training:	Epoch: [58][99/233]	Loss 0.0207 (0.0202)	
training:	Epoch: [58][100/233]	Loss 0.0148 (0.0202)	
training:	Epoch: [58][101/233]	Loss 0.0173 (0.0202)	
training:	Epoch: [58][102/233]	Loss 0.0146 (0.0201)	
training:	Epoch: [58][103/233]	Loss 0.0196 (0.0201)	
training:	Epoch: [58][104/233]	Loss 0.0200 (0.0201)	
training:	Epoch: [58][105/233]	Loss 0.0178 (0.0201)	
training:	Epoch: [58][106/233]	Loss 0.0215 (0.0201)	
training:	Epoch: [58][107/233]	Loss 0.0146 (0.0200)	
training:	Epoch: [58][108/233]	Loss 0.0155 (0.0200)	
training:	Epoch: [58][109/233]	Loss 0.0138 (0.0199)	
training:	Epoch: [58][110/233]	Loss 0.0146 (0.0199)	
training:	Epoch: [58][111/233]	Loss 0.0200 (0.0199)	
training:	Epoch: [58][112/233]	Loss 0.0255 (0.0199)	
training:	Epoch: [58][113/233]	Loss 0.0276 (0.0200)	
training:	Epoch: [58][114/233]	Loss 0.0158 (0.0200)	
training:	Epoch: [58][115/233]	Loss 0.0167 (0.0199)	
training:	Epoch: [58][116/233]	Loss 0.0161 (0.0199)	
training:	Epoch: [58][117/233]	Loss 0.0159 (0.0199)	
training:	Epoch: [58][118/233]	Loss 0.0186 (0.0199)	
training:	Epoch: [58][119/233]	Loss 0.0199 (0.0199)	
training:	Epoch: [58][120/233]	Loss 0.0232 (0.0199)	
training:	Epoch: [58][121/233]	Loss 0.0207 (0.0199)	
training:	Epoch: [58][122/233]	Loss 0.0151 (0.0199)	
training:	Epoch: [58][123/233]	Loss 0.0149 (0.0198)	
training:	Epoch: [58][124/233]	Loss 0.0146 (0.0198)	
training:	Epoch: [58][125/233]	Loss 0.0195 (0.0198)	
training:	Epoch: [58][126/233]	Loss 0.0163 (0.0197)	
training:	Epoch: [58][127/233]	Loss 0.0154 (0.0197)	
training:	Epoch: [58][128/233]	Loss 0.0162 (0.0197)	
training:	Epoch: [58][129/233]	Loss 0.0249 (0.0197)	
training:	Epoch: [58][130/233]	Loss 0.0140 (0.0197)	
training:	Epoch: [58][131/233]	Loss 0.0160 (0.0197)	
training:	Epoch: [58][132/233]	Loss 0.0212 (0.0197)	
training:	Epoch: [58][133/233]	Loss 0.0157 (0.0196)	
training:	Epoch: [58][134/233]	Loss 0.0197 (0.0196)	
training:	Epoch: [58][135/233]	Loss 0.0153 (0.0196)	
training:	Epoch: [58][136/233]	Loss 0.0159 (0.0196)	
training:	Epoch: [58][137/233]	Loss 0.0171 (0.0196)	
training:	Epoch: [58][138/233]	Loss 0.0194 (0.0196)	
training:	Epoch: [58][139/233]	Loss 0.0142 (0.0195)	
training:	Epoch: [58][140/233]	Loss 0.0156 (0.0195)	
training:	Epoch: [58][141/233]	Loss 0.0170 (0.0195)	
training:	Epoch: [58][142/233]	Loss 0.0146 (0.0194)	
training:	Epoch: [58][143/233]	Loss 0.0197 (0.0194)	
training:	Epoch: [58][144/233]	Loss 0.0168 (0.0194)	
training:	Epoch: [58][145/233]	Loss 0.0174 (0.0194)	
training:	Epoch: [58][146/233]	Loss 0.0218 (0.0194)	
training:	Epoch: [58][147/233]	Loss 0.0158 (0.0194)	
training:	Epoch: [58][148/233]	Loss 0.0169 (0.0194)	
training:	Epoch: [58][149/233]	Loss 0.0174 (0.0194)	
training:	Epoch: [58][150/233]	Loss 0.0175 (0.0194)	
training:	Epoch: [58][151/233]	Loss 0.0148 (0.0193)	
training:	Epoch: [58][152/233]	Loss 0.0151 (0.0193)	
training:	Epoch: [58][153/233]	Loss 0.0169 (0.0193)	
training:	Epoch: [58][154/233]	Loss 0.0145 (0.0193)	
training:	Epoch: [58][155/233]	Loss 0.0250 (0.0193)	
training:	Epoch: [58][156/233]	Loss 0.0172 (0.0193)	
training:	Epoch: [58][157/233]	Loss 0.0148 (0.0192)	
training:	Epoch: [58][158/233]	Loss 0.0147 (0.0192)	
training:	Epoch: [58][159/233]	Loss 0.0142 (0.0192)	
training:	Epoch: [58][160/233]	Loss 0.0149 (0.0192)	
training:	Epoch: [58][161/233]	Loss 0.0150 (0.0191)	
training:	Epoch: [58][162/233]	Loss 0.0180 (0.0191)	
training:	Epoch: [58][163/233]	Loss 0.0252 (0.0192)	
training:	Epoch: [58][164/233]	Loss 0.0160 (0.0191)	
training:	Epoch: [58][165/233]	Loss 0.0138 (0.0191)	
training:	Epoch: [58][166/233]	Loss 0.0167 (0.0191)	
training:	Epoch: [58][167/233]	Loss 0.0148 (0.0191)	
training:	Epoch: [58][168/233]	Loss 0.0174 (0.0191)	
training:	Epoch: [58][169/233]	Loss 0.0148 (0.0190)	
training:	Epoch: [58][170/233]	Loss 0.0175 (0.0190)	
training:	Epoch: [58][171/233]	Loss 0.0148 (0.0190)	
training:	Epoch: [58][172/233]	Loss 0.0316 (0.0191)	
training:	Epoch: [58][173/233]	Loss 0.0151 (0.0191)	
training:	Epoch: [58][174/233]	Loss 0.0160 (0.0190)	
training:	Epoch: [58][175/233]	Loss 0.0163 (0.0190)	
training:	Epoch: [58][176/233]	Loss 0.0196 (0.0190)	
training:	Epoch: [58][177/233]	Loss 0.0135 (0.0190)	
training:	Epoch: [58][178/233]	Loss 0.0212 (0.0190)	
training:	Epoch: [58][179/233]	Loss 0.0153 (0.0190)	
training:	Epoch: [58][180/233]	Loss 0.0147 (0.0190)	
training:	Epoch: [58][181/233]	Loss 0.0162 (0.0189)	
training:	Epoch: [58][182/233]	Loss 0.0215 (0.0190)	
training:	Epoch: [58][183/233]	Loss 0.0165 (0.0189)	
training:	Epoch: [58][184/233]	Loss 0.0178 (0.0189)	
training:	Epoch: [58][185/233]	Loss 0.0139 (0.0189)	
training:	Epoch: [58][186/233]	Loss 0.0263 (0.0190)	
training:	Epoch: [58][187/233]	Loss 0.0149 (0.0189)	
training:	Epoch: [58][188/233]	Loss 0.0223 (0.0189)	
training:	Epoch: [58][189/233]	Loss 0.0146 (0.0189)	
training:	Epoch: [58][190/233]	Loss 0.0144 (0.0189)	
training:	Epoch: [58][191/233]	Loss 0.0170 (0.0189)	
training:	Epoch: [58][192/233]	Loss 0.0167 (0.0189)	
training:	Epoch: [58][193/233]	Loss 0.0164 (0.0189)	
training:	Epoch: [58][194/233]	Loss 0.0139 (0.0188)	
training:	Epoch: [58][195/233]	Loss 0.0186 (0.0188)	
training:	Epoch: [58][196/233]	Loss 0.0153 (0.0188)	
training:	Epoch: [58][197/233]	Loss 0.0161 (0.0188)	
training:	Epoch: [58][198/233]	Loss 0.0490 (0.0190)	
training:	Epoch: [58][199/233]	Loss 0.0211 (0.0190)	
training:	Epoch: [58][200/233]	Loss 0.0229 (0.0190)	
training:	Epoch: [58][201/233]	Loss 0.0293 (0.0190)	
training:	Epoch: [58][202/233]	Loss 0.0138 (0.0190)	
training:	Epoch: [58][203/233]	Loss 0.0177 (0.0190)	
training:	Epoch: [58][204/233]	Loss 0.0165 (0.0190)	
training:	Epoch: [58][205/233]	Loss 0.0155 (0.0190)	
training:	Epoch: [58][206/233]	Loss 0.0175 (0.0190)	
training:	Epoch: [58][207/233]	Loss 0.0146 (0.0190)	
training:	Epoch: [58][208/233]	Loss 0.0183 (0.0189)	
training:	Epoch: [58][209/233]	Loss 0.0152 (0.0189)	
training:	Epoch: [58][210/233]	Loss 0.0182 (0.0189)	
training:	Epoch: [58][211/233]	Loss 0.0248 (0.0190)	
training:	Epoch: [58][212/233]	Loss 0.0194 (0.0190)	
training:	Epoch: [58][213/233]	Loss 0.0170 (0.0189)	
training:	Epoch: [58][214/233]	Loss 0.0255 (0.0190)	
training:	Epoch: [58][215/233]	Loss 0.0154 (0.0190)	
training:	Epoch: [58][216/233]	Loss 0.0390 (0.0191)	
training:	Epoch: [58][217/233]	Loss 0.0236 (0.0191)	
training:	Epoch: [58][218/233]	Loss 0.0171 (0.0191)	
training:	Epoch: [58][219/233]	Loss 0.0323 (0.0191)	
training:	Epoch: [58][220/233]	Loss 0.0178 (0.0191)	
training:	Epoch: [58][221/233]	Loss 0.0154 (0.0191)	
training:	Epoch: [58][222/233]	Loss 0.0164 (0.0191)	
training:	Epoch: [58][223/233]	Loss 0.0214 (0.0191)	
training:	Epoch: [58][224/233]	Loss 0.0154 (0.0191)	
training:	Epoch: [58][225/233]	Loss 0.0146 (0.0191)	
training:	Epoch: [58][226/233]	Loss 0.0151 (0.0190)	
training:	Epoch: [58][227/233]	Loss 0.0304 (0.0191)	
training:	Epoch: [58][228/233]	Loss 0.0178 (0.0191)	
training:	Epoch: [58][229/233]	Loss 0.0166 (0.0191)	
training:	Epoch: [58][230/233]	Loss 0.0166 (0.0191)	
training:	Epoch: [58][231/233]	Loss 0.0148 (0.0191)	
training:	Epoch: [58][232/233]	Loss 0.0149 (0.0190)	
training:	Epoch: [58][233/233]	Loss 0.0226 (0.0191)	
Training:	 Loss: 0.0190

Training:	 ACC: 1.0000 1.0000 1.0000 1.0000
Validation:	 ACC: 0.8640 0.8646 0.8772 0.8509
Validation:	 Best_BACC: 0.8673 0.8657 0.8332 0.9013
Validation:	 Loss: 0.3179
Pretraining:	Epoch 59/200
----------
training:	Epoch: [59][1/233]	Loss 0.0299 (0.0299)	
training:	Epoch: [59][2/233]	Loss 0.0160 (0.0230)	
training:	Epoch: [59][3/233]	Loss 0.0157 (0.0205)	
training:	Epoch: [59][4/233]	Loss 0.0129 (0.0186)	
training:	Epoch: [59][5/233]	Loss 0.0157 (0.0180)	
training:	Epoch: [59][6/233]	Loss 0.0533 (0.0239)	
training:	Epoch: [59][7/233]	Loss 0.0196 (0.0233)	
training:	Epoch: [59][8/233]	Loss 0.0139 (0.0221)	
training:	Epoch: [59][9/233]	Loss 0.0148 (0.0213)	
training:	Epoch: [59][10/233]	Loss 0.0137 (0.0205)	
training:	Epoch: [59][11/233]	Loss 0.0146 (0.0200)	
training:	Epoch: [59][12/233]	Loss 0.0143 (0.0195)	
training:	Epoch: [59][13/233]	Loss 0.0155 (0.0192)	
training:	Epoch: [59][14/233]	Loss 0.0662 (0.0226)	
training:	Epoch: [59][15/233]	Loss 0.0156 (0.0221)	
training:	Epoch: [59][16/233]	Loss 0.0293 (0.0226)	
training:	Epoch: [59][17/233]	Loss 0.0175 (0.0223)	
training:	Epoch: [59][18/233]	Loss 0.0169 (0.0220)	
training:	Epoch: [59][19/233]	Loss 0.0292 (0.0223)	
training:	Epoch: [59][20/233]	Loss 0.0161 (0.0220)	
training:	Epoch: [59][21/233]	Loss 0.0285 (0.0223)	
training:	Epoch: [59][22/233]	Loss 0.0132 (0.0219)	
training:	Epoch: [59][23/233]	Loss 0.0156 (0.0217)	
training:	Epoch: [59][24/233]	Loss 0.0140 (0.0213)	
training:	Epoch: [59][25/233]	Loss 0.0169 (0.0212)	
training:	Epoch: [59][26/233]	Loss 0.0135 (0.0209)	
training:	Epoch: [59][27/233]	Loss 0.0141 (0.0206)	
training:	Epoch: [59][28/233]	Loss 0.0160 (0.0204)	
training:	Epoch: [59][29/233]	Loss 0.0137 (0.0202)	
training:	Epoch: [59][30/233]	Loss 0.0310 (0.0206)	
training:	Epoch: [59][31/233]	Loss 0.0178 (0.0205)	
training:	Epoch: [59][32/233]	Loss 0.0257 (0.0206)	
training:	Epoch: [59][33/233]	Loss 0.0176 (0.0206)	
training:	Epoch: [59][34/233]	Loss 0.0321 (0.0209)	
training:	Epoch: [59][35/233]	Loss 0.0147 (0.0207)	
training:	Epoch: [59][36/233]	Loss 0.0167 (0.0206)	
training:	Epoch: [59][37/233]	Loss 0.0205 (0.0206)	
training:	Epoch: [59][38/233]	Loss 0.0157 (0.0205)	
training:	Epoch: [59][39/233]	Loss 0.0195 (0.0204)	
training:	Epoch: [59][40/233]	Loss 0.0169 (0.0204)	
training:	Epoch: [59][41/233]	Loss 0.0234 (0.0204)	
training:	Epoch: [59][42/233]	Loss 0.0180 (0.0204)	
training:	Epoch: [59][43/233]	Loss 0.0176 (0.0203)	
training:	Epoch: [59][44/233]	Loss 0.0134 (0.0202)	
training:	Epoch: [59][45/233]	Loss 0.0303 (0.0204)	
training:	Epoch: [59][46/233]	Loss 0.0142 (0.0202)	
training:	Epoch: [59][47/233]	Loss 0.0205 (0.0202)	
training:	Epoch: [59][48/233]	Loss 0.0175 (0.0202)	
training:	Epoch: [59][49/233]	Loss 0.0195 (0.0202)	
training:	Epoch: [59][50/233]	Loss 0.0163 (0.0201)	
training:	Epoch: [59][51/233]	Loss 0.0141 (0.0200)	
training:	Epoch: [59][52/233]	Loss 0.0221 (0.0200)	
training:	Epoch: [59][53/233]	Loss 0.0158 (0.0199)	
training:	Epoch: [59][54/233]	Loss 0.0151 (0.0199)	
training:	Epoch: [59][55/233]	Loss 0.0144 (0.0198)	
training:	Epoch: [59][56/233]	Loss 0.0185 (0.0197)	
training:	Epoch: [59][57/233]	Loss 0.0146 (0.0196)	
training:	Epoch: [59][58/233]	Loss 0.0136 (0.0195)	
training:	Epoch: [59][59/233]	Loss 0.0141 (0.0194)	
training:	Epoch: [59][60/233]	Loss 0.0157 (0.0194)	
training:	Epoch: [59][61/233]	Loss 0.0211 (0.0194)	
training:	Epoch: [59][62/233]	Loss 0.0145 (0.0193)	
training:	Epoch: [59][63/233]	Loss 0.0169 (0.0193)	
training:	Epoch: [59][64/233]	Loss 0.0174 (0.0193)	
training:	Epoch: [59][65/233]	Loss 0.0144 (0.0192)	
training:	Epoch: [59][66/233]	Loss 0.0205 (0.0192)	
training:	Epoch: [59][67/233]	Loss 0.0135 (0.0191)	
training:	Epoch: [59][68/233]	Loss 0.0267 (0.0192)	
training:	Epoch: [59][69/233]	Loss 0.0281 (0.0194)	
training:	Epoch: [59][70/233]	Loss 0.0155 (0.0193)	
training:	Epoch: [59][71/233]	Loss 0.0195 (0.0193)	
training:	Epoch: [59][72/233]	Loss 0.0150 (0.0192)	
training:	Epoch: [59][73/233]	Loss 0.0139 (0.0192)	
training:	Epoch: [59][74/233]	Loss 0.0157 (0.0191)	
training:	Epoch: [59][75/233]	Loss 0.0192 (0.0191)	
training:	Epoch: [59][76/233]	Loss 0.0147 (0.0191)	
training:	Epoch: [59][77/233]	Loss 0.0152 (0.0190)	
training:	Epoch: [59][78/233]	Loss 0.0197 (0.0190)	
training:	Epoch: [59][79/233]	Loss 0.0356 (0.0192)	
training:	Epoch: [59][80/233]	Loss 0.0148 (0.0192)	
training:	Epoch: [59][81/233]	Loss 0.0251 (0.0193)	
training:	Epoch: [59][82/233]	Loss 0.0169 (0.0192)	
training:	Epoch: [59][83/233]	Loss 0.0151 (0.0192)	
training:	Epoch: [59][84/233]	Loss 0.0163 (0.0191)	
training:	Epoch: [59][85/233]	Loss 0.0279 (0.0192)	
training:	Epoch: [59][86/233]	Loss 0.0168 (0.0192)	
training:	Epoch: [59][87/233]	Loss 0.0138 (0.0192)	
training:	Epoch: [59][88/233]	Loss 0.0223 (0.0192)	
training:	Epoch: [59][89/233]	Loss 0.0209 (0.0192)	
training:	Epoch: [59][90/233]	Loss 0.0169 (0.0192)	
training:	Epoch: [59][91/233]	Loss 0.0251 (0.0193)	
training:	Epoch: [59][92/233]	Loss 0.0135 (0.0192)	
training:	Epoch: [59][93/233]	Loss 0.0522 (0.0195)	
training:	Epoch: [59][94/233]	Loss 0.0159 (0.0195)	
training:	Epoch: [59][95/233]	Loss 0.0165 (0.0195)	
training:	Epoch: [59][96/233]	Loss 0.0150 (0.0194)	
training:	Epoch: [59][97/233]	Loss 0.0197 (0.0194)	
training:	Epoch: [59][98/233]	Loss 0.0325 (0.0196)	
training:	Epoch: [59][99/233]	Loss 0.0160 (0.0195)	
training:	Epoch: [59][100/233]	Loss 0.0177 (0.0195)	
training:	Epoch: [59][101/233]	Loss 0.0143 (0.0195)	
training:	Epoch: [59][102/233]	Loss 0.0351 (0.0196)	
training:	Epoch: [59][103/233]	Loss 0.0178 (0.0196)	
training:	Epoch: [59][104/233]	Loss 0.0148 (0.0195)	
training:	Epoch: [59][105/233]	Loss 0.0218 (0.0196)	
training:	Epoch: [59][106/233]	Loss 0.0144 (0.0195)	
training:	Epoch: [59][107/233]	Loss 0.0152 (0.0195)	
training:	Epoch: [59][108/233]	Loss 0.0142 (0.0194)	
training:	Epoch: [59][109/233]	Loss 0.0144 (0.0194)	
training:	Epoch: [59][110/233]	Loss 0.0141 (0.0193)	
training:	Epoch: [59][111/233]	Loss 0.0251 (0.0194)	
training:	Epoch: [59][112/233]	Loss 0.0152 (0.0194)	
training:	Epoch: [59][113/233]	Loss 0.0143 (0.0193)	
training:	Epoch: [59][114/233]	Loss 0.0218 (0.0193)	
training:	Epoch: [59][115/233]	Loss 0.0172 (0.0193)	
training:	Epoch: [59][116/233]	Loss 0.0242 (0.0194)	
training:	Epoch: [59][117/233]	Loss 0.0264 (0.0194)	
training:	Epoch: [59][118/233]	Loss 0.0175 (0.0194)	
training:	Epoch: [59][119/233]	Loss 0.0154 (0.0194)	
training:	Epoch: [59][120/233]	Loss 0.0195 (0.0194)	
training:	Epoch: [59][121/233]	Loss 0.0139 (0.0193)	
training:	Epoch: [59][122/233]	Loss 0.0187 (0.0193)	
training:	Epoch: [59][123/233]	Loss 0.0189 (0.0193)	
training:	Epoch: [59][124/233]	Loss 0.0199 (0.0193)	
training:	Epoch: [59][125/233]	Loss 0.0156 (0.0193)	
training:	Epoch: [59][126/233]	Loss 0.0221 (0.0193)	
training:	Epoch: [59][127/233]	Loss 0.0136 (0.0193)	
training:	Epoch: [59][128/233]	Loss 0.0159 (0.0192)	
training:	Epoch: [59][129/233]	Loss 0.0163 (0.0192)	
training:	Epoch: [59][130/233]	Loss 0.0174 (0.0192)	
training:	Epoch: [59][131/233]	Loss 0.0175 (0.0192)	
training:	Epoch: [59][132/233]	Loss 0.0141 (0.0191)	
training:	Epoch: [59][133/233]	Loss 0.0255 (0.0192)	
training:	Epoch: [59][134/233]	Loss 0.0145 (0.0192)	
training:	Epoch: [59][135/233]	Loss 0.0159 (0.0191)	
training:	Epoch: [59][136/233]	Loss 0.0158 (0.0191)	
training:	Epoch: [59][137/233]	Loss 0.0177 (0.0191)	
training:	Epoch: [59][138/233]	Loss 0.0167 (0.0191)	
training:	Epoch: [59][139/233]	Loss 0.0176 (0.0191)	
training:	Epoch: [59][140/233]	Loss 0.0162 (0.0191)	
training:	Epoch: [59][141/233]	Loss 0.0224 (0.0191)	
training:	Epoch: [59][142/233]	Loss 0.0218 (0.0191)	
training:	Epoch: [59][143/233]	Loss 0.0171 (0.0191)	
training:	Epoch: [59][144/233]	Loss 0.0246 (0.0191)	
training:	Epoch: [59][145/233]	Loss 0.0133 (0.0191)	
training:	Epoch: [59][146/233]	Loss 0.0134 (0.0190)	
training:	Epoch: [59][147/233]	Loss 0.0147 (0.0190)	
training:	Epoch: [59][148/233]	Loss 0.0164 (0.0190)	
training:	Epoch: [59][149/233]	Loss 0.0155 (0.0190)	
training:	Epoch: [59][150/233]	Loss 0.0747 (0.0193)	
training:	Epoch: [59][151/233]	Loss 0.0178 (0.0193)	
training:	Epoch: [59][152/233]	Loss 0.0180 (0.0193)	
training:	Epoch: [59][153/233]	Loss 0.0139 (0.0193)	
training:	Epoch: [59][154/233]	Loss 0.0187 (0.0193)	
training:	Epoch: [59][155/233]	Loss 0.0218 (0.0193)	
training:	Epoch: [59][156/233]	Loss 0.0153 (0.0193)	
training:	Epoch: [59][157/233]	Loss 0.0173 (0.0193)	
training:	Epoch: [59][158/233]	Loss 0.0162 (0.0192)	
training:	Epoch: [59][159/233]	Loss 0.0322 (0.0193)	
training:	Epoch: [59][160/233]	Loss 0.0148 (0.0193)	
training:	Epoch: [59][161/233]	Loss 0.0152 (0.0193)	
training:	Epoch: [59][162/233]	Loss 0.0172 (0.0193)	
training:	Epoch: [59][163/233]	Loss 0.0155 (0.0192)	
training:	Epoch: [59][164/233]	Loss 0.0160 (0.0192)	
training:	Epoch: [59][165/233]	Loss 0.0140 (0.0192)	
training:	Epoch: [59][166/233]	Loss 0.0149 (0.0192)	
training:	Epoch: [59][167/233]	Loss 0.0143 (0.0191)	
training:	Epoch: [59][168/233]	Loss 0.0159 (0.0191)	
training:	Epoch: [59][169/233]	Loss 0.0234 (0.0191)	
training:	Epoch: [59][170/233]	Loss 0.0169 (0.0191)	
training:	Epoch: [59][171/233]	Loss 0.0142 (0.0191)	
training:	Epoch: [59][172/233]	Loss 0.0500 (0.0193)	
training:	Epoch: [59][173/233]	Loss 0.0146 (0.0192)	
training:	Epoch: [59][174/233]	Loss 0.0161 (0.0192)	
training:	Epoch: [59][175/233]	Loss 0.0130 (0.0192)	
training:	Epoch: [59][176/233]	Loss 0.0140 (0.0192)	
training:	Epoch: [59][177/233]	Loss 0.0232 (0.0192)	
training:	Epoch: [59][178/233]	Loss 0.0168 (0.0192)	
training:	Epoch: [59][179/233]	Loss 0.0132 (0.0191)	
training:	Epoch: [59][180/233]	Loss 0.0148 (0.0191)	
training:	Epoch: [59][181/233]	Loss 0.0145 (0.0191)	
training:	Epoch: [59][182/233]	Loss 0.0146 (0.0191)	
training:	Epoch: [59][183/233]	Loss 0.0166 (0.0191)	
training:	Epoch: [59][184/233]	Loss 0.0170 (0.0190)	
training:	Epoch: [59][185/233]	Loss 0.0148 (0.0190)	
training:	Epoch: [59][186/233]	Loss 0.0161 (0.0190)	
training:	Epoch: [59][187/233]	Loss 0.0171 (0.0190)	
training:	Epoch: [59][188/233]	Loss 0.0259 (0.0190)	
training:	Epoch: [59][189/233]	Loss 0.0171 (0.0190)	
training:	Epoch: [59][190/233]	Loss 0.0137 (0.0190)	
training:	Epoch: [59][191/233]	Loss 0.0364 (0.0191)	
training:	Epoch: [59][192/233]	Loss 0.0177 (0.0191)	
training:	Epoch: [59][193/233]	Loss 0.0234 (0.0191)	
training:	Epoch: [59][194/233]	Loss 0.0154 (0.0191)	
training:	Epoch: [59][195/233]	Loss 0.0193 (0.0191)	
training:	Epoch: [59][196/233]	Loss 0.0142 (0.0191)	
training:	Epoch: [59][197/233]	Loss 0.0138 (0.0190)	
training:	Epoch: [59][198/233]	Loss 0.0364 (0.0191)	
training:	Epoch: [59][199/233]	Loss 0.0342 (0.0192)	
training:	Epoch: [59][200/233]	Loss 0.0177 (0.0192)	
training:	Epoch: [59][201/233]	Loss 0.0138 (0.0192)	
training:	Epoch: [59][202/233]	Loss 0.0167 (0.0191)	
training:	Epoch: [59][203/233]	Loss 0.0191 (0.0191)	
training:	Epoch: [59][204/233]	Loss 0.0131 (0.0191)	
training:	Epoch: [59][205/233]	Loss 0.0143 (0.0191)	
training:	Epoch: [59][206/233]	Loss 0.0122 (0.0191)	
training:	Epoch: [59][207/233]	Loss 0.0187 (0.0191)	
training:	Epoch: [59][208/233]	Loss 0.0240 (0.0191)	
training:	Epoch: [59][209/233]	Loss 0.0167 (0.0191)	
training:	Epoch: [59][210/233]	Loss 0.0130 (0.0190)	
training:	Epoch: [59][211/233]	Loss 0.0132 (0.0190)	
training:	Epoch: [59][212/233]	Loss 0.0171 (0.0190)	
training:	Epoch: [59][213/233]	Loss 0.0178 (0.0190)	
training:	Epoch: [59][214/233]	Loss 0.0223 (0.0190)	
training:	Epoch: [59][215/233]	Loss 0.0147 (0.0190)	
training:	Epoch: [59][216/233]	Loss 0.0167 (0.0190)	
training:	Epoch: [59][217/233]	Loss 0.0283 (0.0190)	
training:	Epoch: [59][218/233]	Loss 0.0164 (0.0190)	
training:	Epoch: [59][219/233]	Loss 0.0144 (0.0190)	
training:	Epoch: [59][220/233]	Loss 0.0151 (0.0190)	
training:	Epoch: [59][221/233]	Loss 0.0397 (0.0191)	
training:	Epoch: [59][222/233]	Loss 0.0157 (0.0191)	
training:	Epoch: [59][223/233]	Loss 0.0136 (0.0190)	
training:	Epoch: [59][224/233]	Loss 0.0171 (0.0190)	
training:	Epoch: [59][225/233]	Loss 0.0132 (0.0190)	
training:	Epoch: [59][226/233]	Loss 0.0140 (0.0190)	
training:	Epoch: [59][227/233]	Loss 0.0154 (0.0190)	
training:	Epoch: [59][228/233]	Loss 0.0152 (0.0189)	
training:	Epoch: [59][229/233]	Loss 0.0171 (0.0189)	
training:	Epoch: [59][230/233]	Loss 0.0218 (0.0189)	
training:	Epoch: [59][231/233]	Loss 0.0163 (0.0189)	
training:	Epoch: [59][232/233]	Loss 0.0170 (0.0189)	
training:	Epoch: [59][233/233]	Loss 0.0148 (0.0189)	
Training:	 Loss: 0.0189

Training:	 ACC: 1.0000 1.0000 1.0000 1.0000
Validation:	 ACC: 0.8682 0.8678 0.8598 0.8767
Validation:	 Best_BACC: 0.8682 0.8678 0.8598 0.8767
Validation:	 Loss: 0.3203
Pretraining:	Epoch 60/200
----------
training:	Epoch: [60][1/233]	Loss 0.0144 (0.0144)	
training:	Epoch: [60][2/233]	Loss 0.0243 (0.0193)	
training:	Epoch: [60][3/233]	Loss 0.0193 (0.0193)	
training:	Epoch: [60][4/233]	Loss 0.0137 (0.0179)	
training:	Epoch: [60][5/233]	Loss 0.0138 (0.0171)	
training:	Epoch: [60][6/233]	Loss 0.0141 (0.0166)	
training:	Epoch: [60][7/233]	Loss 0.0207 (0.0172)	
training:	Epoch: [60][8/233]	Loss 0.0140 (0.0168)	
training:	Epoch: [60][9/233]	Loss 0.0158 (0.0167)	
training:	Epoch: [60][10/233]	Loss 0.0289 (0.0179)	
training:	Epoch: [60][11/233]	Loss 0.0174 (0.0178)	
training:	Epoch: [60][12/233]	Loss 0.0141 (0.0175)	
training:	Epoch: [60][13/233]	Loss 0.0524 (0.0202)	
training:	Epoch: [60][14/233]	Loss 0.0181 (0.0201)	
training:	Epoch: [60][15/233]	Loss 0.0141 (0.0197)	
training:	Epoch: [60][16/233]	Loss 0.0190 (0.0196)	
training:	Epoch: [60][17/233]	Loss 0.0163 (0.0194)	
training:	Epoch: [60][18/233]	Loss 0.0138 (0.0191)	
training:	Epoch: [60][19/233]	Loss 0.0145 (0.0189)	
training:	Epoch: [60][20/233]	Loss 0.0137 (0.0186)	
training:	Epoch: [60][21/233]	Loss 0.0144 (0.0184)	
training:	Epoch: [60][22/233]	Loss 0.0150 (0.0183)	
training:	Epoch: [60][23/233]	Loss 0.0156 (0.0181)	
training:	Epoch: [60][24/233]	Loss 0.0196 (0.0182)	
training:	Epoch: [60][25/233]	Loss 0.0142 (0.0180)	
training:	Epoch: [60][26/233]	Loss 0.0156 (0.0179)	
training:	Epoch: [60][27/233]	Loss 0.0175 (0.0179)	
training:	Epoch: [60][28/233]	Loss 0.0163 (0.0179)	
training:	Epoch: [60][29/233]	Loss 0.0125 (0.0177)	
training:	Epoch: [60][30/233]	Loss 0.0170 (0.0177)	
training:	Epoch: [60][31/233]	Loss 0.0189 (0.0177)	
training:	Epoch: [60][32/233]	Loss 0.0175 (0.0177)	
training:	Epoch: [60][33/233]	Loss 0.0187 (0.0177)	
training:	Epoch: [60][34/233]	Loss 0.0185 (0.0178)	
training:	Epoch: [60][35/233]	Loss 0.0177 (0.0178)	
training:	Epoch: [60][36/233]	Loss 0.0381 (0.0183)	
training:	Epoch: [60][37/233]	Loss 0.0135 (0.0182)	
training:	Epoch: [60][38/233]	Loss 0.0156 (0.0181)	
training:	Epoch: [60][39/233]	Loss 0.0138 (0.0180)	
training:	Epoch: [60][40/233]	Loss 0.0137 (0.0179)	
training:	Epoch: [60][41/233]	Loss 0.0136 (0.0178)	
training:	Epoch: [60][42/233]	Loss 0.0148 (0.0177)	
training:	Epoch: [60][43/233]	Loss 0.0227 (0.0178)	
training:	Epoch: [60][44/233]	Loss 0.0184 (0.0179)	
training:	Epoch: [60][45/233]	Loss 0.0142 (0.0178)	
training:	Epoch: [60][46/233]	Loss 0.0140 (0.0177)	
training:	Epoch: [60][47/233]	Loss 0.0166 (0.0177)	
training:	Epoch: [60][48/233]	Loss 0.0139 (0.0176)	
training:	Epoch: [60][49/233]	Loss 0.0131 (0.0175)	
training:	Epoch: [60][50/233]	Loss 0.0208 (0.0176)	
training:	Epoch: [60][51/233]	Loss 0.0152 (0.0175)	
training:	Epoch: [60][52/233]	Loss 0.0160 (0.0175)	
training:	Epoch: [60][53/233]	Loss 0.0169 (0.0175)	
training:	Epoch: [60][54/233]	Loss 0.0210 (0.0175)	
training:	Epoch: [60][55/233]	Loss 0.0173 (0.0175)	
training:	Epoch: [60][56/233]	Loss 0.0147 (0.0175)	
training:	Epoch: [60][57/233]	Loss 0.0135 (0.0174)	
training:	Epoch: [60][58/233]	Loss 0.0149 (0.0174)	
training:	Epoch: [60][59/233]	Loss 0.0144 (0.0173)	
training:	Epoch: [60][60/233]	Loss 0.0170 (0.0173)	
training:	Epoch: [60][61/233]	Loss 0.0310 (0.0175)	
training:	Epoch: [60][62/233]	Loss 0.0129 (0.0175)	
training:	Epoch: [60][63/233]	Loss 0.0157 (0.0174)	
training:	Epoch: [60][64/233]	Loss 0.0165 (0.0174)	
training:	Epoch: [60][65/233]	Loss 0.0203 (0.0175)	
training:	Epoch: [60][66/233]	Loss 0.0129 (0.0174)	
training:	Epoch: [60][67/233]	Loss 0.0200 (0.0174)	
training:	Epoch: [60][68/233]	Loss 0.0154 (0.0174)	
training:	Epoch: [60][69/233]	Loss 0.0189 (0.0174)	
training:	Epoch: [60][70/233]	Loss 0.0192 (0.0175)	
training:	Epoch: [60][71/233]	Loss 0.0188 (0.0175)	
training:	Epoch: [60][72/233]	Loss 0.0163 (0.0175)	
training:	Epoch: [60][73/233]	Loss 0.0126 (0.0174)	
training:	Epoch: [60][74/233]	Loss 0.0131 (0.0173)	
training:	Epoch: [60][75/233]	Loss 0.0143 (0.0173)	
training:	Epoch: [60][76/233]	Loss 0.0292 (0.0174)	
training:	Epoch: [60][77/233]	Loss 0.0139 (0.0174)	
training:	Epoch: [60][78/233]	Loss 0.0173 (0.0174)	
training:	Epoch: [60][79/233]	Loss 0.0140 (0.0174)	
training:	Epoch: [60][80/233]	Loss 0.0153 (0.0173)	
training:	Epoch: [60][81/233]	Loss 0.0184 (0.0173)	
training:	Epoch: [60][82/233]	Loss 0.0166 (0.0173)	
training:	Epoch: [60][83/233]	Loss 0.0433 (0.0176)	
training:	Epoch: [60][84/233]	Loss 0.0137 (0.0176)	
training:	Epoch: [60][85/233]	Loss 0.0161 (0.0176)	
training:	Epoch: [60][86/233]	Loss 0.0137 (0.0175)	
training:	Epoch: [60][87/233]	Loss 0.0180 (0.0175)	
training:	Epoch: [60][88/233]	Loss 0.0202 (0.0176)	
training:	Epoch: [60][89/233]	Loss 0.0131 (0.0175)	
training:	Epoch: [60][90/233]	Loss 0.0139 (0.0175)	
training:	Epoch: [60][91/233]	Loss 0.0214 (0.0175)	
training:	Epoch: [60][92/233]	Loss 0.0143 (0.0175)	
training:	Epoch: [60][93/233]	Loss 0.0203 (0.0175)	
training:	Epoch: [60][94/233]	Loss 0.0163 (0.0175)	
training:	Epoch: [60][95/233]	Loss 0.0193 (0.0175)	
training:	Epoch: [60][96/233]	Loss 0.0251 (0.0176)	
training:	Epoch: [60][97/233]	Loss 0.0226 (0.0177)	
training:	Epoch: [60][98/233]	Loss 0.0358 (0.0178)	
training:	Epoch: [60][99/233]	Loss 0.0144 (0.0178)	
training:	Epoch: [60][100/233]	Loss 0.0300 (0.0179)	
training:	Epoch: [60][101/233]	Loss 0.0182 (0.0179)	
training:	Epoch: [60][102/233]	Loss 0.0124 (0.0179)	
training:	Epoch: [60][103/233]	Loss 0.0166 (0.0179)	
training:	Epoch: [60][104/233]	Loss 0.0142 (0.0178)	
training:	Epoch: [60][105/233]	Loss 0.0135 (0.0178)	
training:	Epoch: [60][106/233]	Loss 0.0130 (0.0177)	
training:	Epoch: [60][107/233]	Loss 0.0324 (0.0179)	
training:	Epoch: [60][108/233]	Loss 0.0133 (0.0178)	
training:	Epoch: [60][109/233]	Loss 0.0137 (0.0178)	
training:	Epoch: [60][110/233]	Loss 0.0264 (0.0179)	
training:	Epoch: [60][111/233]	Loss 0.0177 (0.0179)	
training:	Epoch: [60][112/233]	Loss 0.0148 (0.0179)	
training:	Epoch: [60][113/233]	Loss 0.0147 (0.0178)	
training:	Epoch: [60][114/233]	Loss 0.0148 (0.0178)	
training:	Epoch: [60][115/233]	Loss 0.0135 (0.0178)	
training:	Epoch: [60][116/233]	Loss 0.0191 (0.0178)	
training:	Epoch: [60][117/233]	Loss 0.0132 (0.0177)	
training:	Epoch: [60][118/233]	Loss 0.0129 (0.0177)	
training:	Epoch: [60][119/233]	Loss 0.0158 (0.0177)	
training:	Epoch: [60][120/233]	Loss 0.0170 (0.0177)	
training:	Epoch: [60][121/233]	Loss 0.0140 (0.0176)	
training:	Epoch: [60][122/233]	Loss 0.0165 (0.0176)	
training:	Epoch: [60][123/233]	Loss 0.0171 (0.0176)	
training:	Epoch: [60][124/233]	Loss 0.0202 (0.0176)	
training:	Epoch: [60][125/233]	Loss 0.0185 (0.0177)	
training:	Epoch: [60][126/233]	Loss 0.0187 (0.0177)	
training:	Epoch: [60][127/233]	Loss 0.0129 (0.0176)	
training:	Epoch: [60][128/233]	Loss 0.0148 (0.0176)	
training:	Epoch: [60][129/233]	Loss 0.0221 (0.0176)	
training:	Epoch: [60][130/233]	Loss 0.0156 (0.0176)	
training:	Epoch: [60][131/233]	Loss 0.0152 (0.0176)	
training:	Epoch: [60][132/233]	Loss 0.0155 (0.0176)	
training:	Epoch: [60][133/233]	Loss 0.0141 (0.0176)	
training:	Epoch: [60][134/233]	Loss 0.0165 (0.0176)	
training:	Epoch: [60][135/233]	Loss 0.0199 (0.0176)	
training:	Epoch: [60][136/233]	Loss 0.0143 (0.0175)	
training:	Epoch: [60][137/233]	Loss 0.0173 (0.0175)	
training:	Epoch: [60][138/233]	Loss 0.0174 (0.0175)	
training:	Epoch: [60][139/233]	Loss 0.0153 (0.0175)	
training:	Epoch: [60][140/233]	Loss 0.0137 (0.0175)	
training:	Epoch: [60][141/233]	Loss 0.0151 (0.0175)	
training:	Epoch: [60][142/233]	Loss 0.0146 (0.0175)	
training:	Epoch: [60][143/233]	Loss 0.0284 (0.0175)	
training:	Epoch: [60][144/233]	Loss 0.0203 (0.0176)	
training:	Epoch: [60][145/233]	Loss 0.0198 (0.0176)	
training:	Epoch: [60][146/233]	Loss 0.0152 (0.0176)	
training:	Epoch: [60][147/233]	Loss 0.0212 (0.0176)	
training:	Epoch: [60][148/233]	Loss 0.0154 (0.0176)	
training:	Epoch: [60][149/233]	Loss 0.0144 (0.0175)	
training:	Epoch: [60][150/233]	Loss 0.0184 (0.0176)	
training:	Epoch: [60][151/233]	Loss 0.0123 (0.0175)	
training:	Epoch: [60][152/233]	Loss 0.0220 (0.0175)	
training:	Epoch: [60][153/233]	Loss 0.0222 (0.0176)	
training:	Epoch: [60][154/233]	Loss 0.0136 (0.0176)	
training:	Epoch: [60][155/233]	Loss 0.0162 (0.0175)	
training:	Epoch: [60][156/233]	Loss 0.0156 (0.0175)	
training:	Epoch: [60][157/233]	Loss 0.0237 (0.0176)	
training:	Epoch: [60][158/233]	Loss 0.0142 (0.0175)	
training:	Epoch: [60][159/233]	Loss 0.0129 (0.0175)	
training:	Epoch: [60][160/233]	Loss 0.0165 (0.0175)	
training:	Epoch: [60][161/233]	Loss 0.0276 (0.0176)	
training:	Epoch: [60][162/233]	Loss 0.0158 (0.0176)	
training:	Epoch: [60][163/233]	Loss 0.0169 (0.0176)	
training:	Epoch: [60][164/233]	Loss 0.0194 (0.0176)	
training:	Epoch: [60][165/233]	Loss 0.0142 (0.0176)	
training:	Epoch: [60][166/233]	Loss 0.0153 (0.0175)	
training:	Epoch: [60][167/233]	Loss 0.0233 (0.0176)	
training:	Epoch: [60][168/233]	Loss 0.0141 (0.0176)	
training:	Epoch: [60][169/233]	Loss 0.0162 (0.0175)	
training:	Epoch: [60][170/233]	Loss 0.0141 (0.0175)	
training:	Epoch: [60][171/233]	Loss 0.0364 (0.0176)	
training:	Epoch: [60][172/233]	Loss 0.0157 (0.0176)	
training:	Epoch: [60][173/233]	Loss 0.0415 (0.0178)	
training:	Epoch: [60][174/233]	Loss 0.0150 (0.0177)	
training:	Epoch: [60][175/233]	Loss 0.0161 (0.0177)	
training:	Epoch: [60][176/233]	Loss 0.0142 (0.0177)	
training:	Epoch: [60][177/233]	Loss 0.0238 (0.0177)	
training:	Epoch: [60][178/233]	Loss 0.0133 (0.0177)	
training:	Epoch: [60][179/233]	Loss 0.0160 (0.0177)	
training:	Epoch: [60][180/233]	Loss 0.0284 (0.0178)	
training:	Epoch: [60][181/233]	Loss 0.0160 (0.0178)	
training:	Epoch: [60][182/233]	Loss 0.0181 (0.0178)	
training:	Epoch: [60][183/233]	Loss 0.0151 (0.0178)	
training:	Epoch: [60][184/233]	Loss 0.0250 (0.0178)	
training:	Epoch: [60][185/233]	Loss 0.0131 (0.0178)	
training:	Epoch: [60][186/233]	Loss 0.0218 (0.0178)	
training:	Epoch: [60][187/233]	Loss 0.0137 (0.0178)	
training:	Epoch: [60][188/233]	Loss 0.0243 (0.0178)	
training:	Epoch: [60][189/233]	Loss 0.0209 (0.0178)	
training:	Epoch: [60][190/233]	Loss 0.0149 (0.0178)	
training:	Epoch: [60][191/233]	Loss 0.0150 (0.0178)	
training:	Epoch: [60][192/233]	Loss 0.0320 (0.0179)	
training:	Epoch: [60][193/233]	Loss 0.0139 (0.0178)	
training:	Epoch: [60][194/233]	Loss 0.0194 (0.0178)	
training:	Epoch: [60][195/233]	Loss 0.0138 (0.0178)	
training:	Epoch: [60][196/233]	Loss 0.0152 (0.0178)	
training:	Epoch: [60][197/233]	Loss 0.0131 (0.0178)	
training:	Epoch: [60][198/233]	Loss 0.0256 (0.0178)	
training:	Epoch: [60][199/233]	Loss 0.0142 (0.0178)	
training:	Epoch: [60][200/233]	Loss 0.0143 (0.0178)	
training:	Epoch: [60][201/233]	Loss 0.0157 (0.0178)	
training:	Epoch: [60][202/233]	Loss 0.0144 (0.0178)	
training:	Epoch: [60][203/233]	Loss 0.0161 (0.0178)	
training:	Epoch: [60][204/233]	Loss 0.0162 (0.0178)	
training:	Epoch: [60][205/233]	Loss 0.0136 (0.0177)	
training:	Epoch: [60][206/233]	Loss 0.0148 (0.0177)	
training:	Epoch: [60][207/233]	Loss 0.0202 (0.0177)	
training:	Epoch: [60][208/233]	Loss 0.0134 (0.0177)	
training:	Epoch: [60][209/233]	Loss 0.0167 (0.0177)	
training:	Epoch: [60][210/233]	Loss 0.0142 (0.0177)	
training:	Epoch: [60][211/233]	Loss 0.0137 (0.0177)	
training:	Epoch: [60][212/233]	Loss 0.0166 (0.0177)	
training:	Epoch: [60][213/233]	Loss 0.0180 (0.0177)	
training:	Epoch: [60][214/233]	Loss 0.0187 (0.0177)	
training:	Epoch: [60][215/233]	Loss 0.0324 (0.0177)	
training:	Epoch: [60][216/233]	Loss 0.0283 (0.0178)	
training:	Epoch: [60][217/233]	Loss 0.0360 (0.0179)	
training:	Epoch: [60][218/233]	Loss 0.0141 (0.0179)	
training:	Epoch: [60][219/233]	Loss 0.0245 (0.0179)	
training:	Epoch: [60][220/233]	Loss 0.0158 (0.0179)	
training:	Epoch: [60][221/233]	Loss 0.0137 (0.0179)	
training:	Epoch: [60][222/233]	Loss 0.0142 (0.0178)	
training:	Epoch: [60][223/233]	Loss 0.0151 (0.0178)	
training:	Epoch: [60][224/233]	Loss 0.0143 (0.0178)	
training:	Epoch: [60][225/233]	Loss 0.0135 (0.0178)	
training:	Epoch: [60][226/233]	Loss 0.0166 (0.0178)	
training:	Epoch: [60][227/233]	Loss 0.0229 (0.0178)	
training:	Epoch: [60][228/233]	Loss 0.0178 (0.0178)	
training:	Epoch: [60][229/233]	Loss 0.0342 (0.0179)	
training:	Epoch: [60][230/233]	Loss 0.0151 (0.0179)	
training:	Epoch: [60][231/233]	Loss 0.0149 (0.0179)	
training:	Epoch: [60][232/233]	Loss 0.0260 (0.0179)	
training:	Epoch: [60][233/233]	Loss 0.0139 (0.0179)	
Training:	 Loss: 0.0178

Training:	 ACC: 1.0000 1.0000 1.0000 1.0000
Validation:	 ACC: 0.8671 0.8673 0.8721 0.8621
Validation:	 Best_BACC: 0.8682 0.8678 0.8598 0.8767
Validation:	 Loss: 0.3211
Pretraining:	Epoch 61/200
----------
training:	Epoch: [61][1/233]	Loss 0.0128 (0.0128)	
training:	Epoch: [61][2/233]	Loss 0.0144 (0.0136)	
training:	Epoch: [61][3/233]	Loss 0.0132 (0.0135)	
training:	Epoch: [61][4/233]	Loss 0.0164 (0.0142)	
training:	Epoch: [61][5/233]	Loss 0.0167 (0.0147)	
training:	Epoch: [61][6/233]	Loss 0.0143 (0.0146)	
training:	Epoch: [61][7/233]	Loss 0.0147 (0.0146)	
training:	Epoch: [61][8/233]	Loss 0.0395 (0.0178)	
training:	Epoch: [61][9/233]	Loss 0.0167 (0.0176)	
training:	Epoch: [61][10/233]	Loss 0.0263 (0.0185)	
training:	Epoch: [61][11/233]	Loss 0.0125 (0.0180)	
training:	Epoch: [61][12/233]	Loss 0.0279 (0.0188)	
training:	Epoch: [61][13/233]	Loss 0.0127 (0.0183)	
training:	Epoch: [61][14/233]	Loss 0.0174 (0.0183)	
training:	Epoch: [61][15/233]	Loss 0.0255 (0.0187)	
training:	Epoch: [61][16/233]	Loss 0.0137 (0.0184)	
training:	Epoch: [61][17/233]	Loss 0.0160 (0.0183)	
training:	Epoch: [61][18/233]	Loss 0.0142 (0.0181)	
training:	Epoch: [61][19/233]	Loss 0.0189 (0.0181)	
training:	Epoch: [61][20/233]	Loss 0.0238 (0.0184)	
training:	Epoch: [61][21/233]	Loss 0.0171 (0.0183)	
training:	Epoch: [61][22/233]	Loss 0.0171 (0.0183)	
training:	Epoch: [61][23/233]	Loss 0.0156 (0.0182)	
training:	Epoch: [61][24/233]	Loss 0.0135 (0.0180)	
training:	Epoch: [61][25/233]	Loss 0.0156 (0.0179)	
training:	Epoch: [61][26/233]	Loss 0.0142 (0.0177)	
training:	Epoch: [61][27/233]	Loss 0.0132 (0.0176)	
training:	Epoch: [61][28/233]	Loss 0.0129 (0.0174)	
training:	Epoch: [61][29/233]	Loss 0.0157 (0.0173)	
training:	Epoch: [61][30/233]	Loss 0.0129 (0.0172)	
training:	Epoch: [61][31/233]	Loss 0.0139 (0.0171)	
training:	Epoch: [61][32/233]	Loss 0.0199 (0.0172)	
training:	Epoch: [61][33/233]	Loss 0.0129 (0.0170)	
training:	Epoch: [61][34/233]	Loss 0.0181 (0.0171)	
training:	Epoch: [61][35/233]	Loss 0.0145 (0.0170)	
training:	Epoch: [61][36/233]	Loss 0.0132 (0.0169)	
training:	Epoch: [61][37/233]	Loss 0.0229 (0.0171)	
training:	Epoch: [61][38/233]	Loss 0.0195 (0.0171)	
training:	Epoch: [61][39/233]	Loss 0.0149 (0.0171)	
training:	Epoch: [61][40/233]	Loss 0.0168 (0.0171)	
training:	Epoch: [61][41/233]	Loss 0.0128 (0.0170)	
training:	Epoch: [61][42/233]	Loss 0.0153 (0.0169)	
training:	Epoch: [61][43/233]	Loss 0.0135 (0.0168)	
training:	Epoch: [61][44/233]	Loss 0.0197 (0.0169)	
training:	Epoch: [61][45/233]	Loss 0.0138 (0.0168)	
training:	Epoch: [61][46/233]	Loss 0.0128 (0.0167)	
training:	Epoch: [61][47/233]	Loss 0.0172 (0.0168)	
training:	Epoch: [61][48/233]	Loss 0.0132 (0.0167)	
training:	Epoch: [61][49/233]	Loss 0.0150 (0.0166)	
training:	Epoch: [61][50/233]	Loss 0.0160 (0.0166)	
training:	Epoch: [61][51/233]	Loss 0.0227 (0.0167)	
training:	Epoch: [61][52/233]	Loss 0.0153 (0.0167)	
training:	Epoch: [61][53/233]	Loss 0.0125 (0.0166)	
training:	Epoch: [61][54/233]	Loss 0.0130 (0.0166)	
training:	Epoch: [61][55/233]	Loss 0.0134 (0.0165)	
training:	Epoch: [61][56/233]	Loss 0.0130 (0.0165)	
training:	Epoch: [61][57/233]	Loss 0.0136 (0.0164)	
training:	Epoch: [61][58/233]	Loss 0.0129 (0.0163)	
training:	Epoch: [61][59/233]	Loss 0.0149 (0.0163)	
training:	Epoch: [61][60/233]	Loss 0.0319 (0.0166)	
training:	Epoch: [61][61/233]	Loss 0.0144 (0.0165)	
training:	Epoch: [61][62/233]	Loss 0.0168 (0.0165)	
training:	Epoch: [61][63/233]	Loss 0.0208 (0.0166)	
training:	Epoch: [61][64/233]	Loss 0.0218 (0.0167)	
training:	Epoch: [61][65/233]	Loss 0.0132 (0.0166)	
training:	Epoch: [61][66/233]	Loss 0.0153 (0.0166)	
training:	Epoch: [61][67/233]	Loss 0.0127 (0.0166)	
training:	Epoch: [61][68/233]	Loss 0.0154 (0.0165)	
training:	Epoch: [61][69/233]	Loss 0.0176 (0.0166)	
training:	Epoch: [61][70/233]	Loss 0.0147 (0.0165)	
training:	Epoch: [61][71/233]	Loss 0.0197 (0.0166)	
training:	Epoch: [61][72/233]	Loss 0.0212 (0.0166)	
training:	Epoch: [61][73/233]	Loss 0.0152 (0.0166)	
training:	Epoch: [61][74/233]	Loss 0.0125 (0.0166)	
training:	Epoch: [61][75/233]	Loss 0.0245 (0.0167)	
training:	Epoch: [61][76/233]	Loss 0.0165 (0.0167)	
training:	Epoch: [61][77/233]	Loss 0.0177 (0.0167)	
training:	Epoch: [61][78/233]	Loss 0.0147 (0.0167)	
training:	Epoch: [61][79/233]	Loss 0.0160 (0.0167)	
training:	Epoch: [61][80/233]	Loss 0.0147 (0.0166)	
training:	Epoch: [61][81/233]	Loss 0.0197 (0.0167)	
training:	Epoch: [61][82/233]	Loss 0.0145 (0.0166)	
training:	Epoch: [61][83/233]	Loss 0.0155 (0.0166)	
training:	Epoch: [61][84/233]	Loss 0.0160 (0.0166)	
training:	Epoch: [61][85/233]	Loss 0.0150 (0.0166)	
training:	Epoch: [61][86/233]	Loss 0.0129 (0.0166)	
training:	Epoch: [61][87/233]	Loss 0.0134 (0.0165)	
training:	Epoch: [61][88/233]	Loss 0.0137 (0.0165)	
training:	Epoch: [61][89/233]	Loss 0.0200 (0.0165)	
training:	Epoch: [61][90/233]	Loss 0.0185 (0.0165)	
training:	Epoch: [61][91/233]	Loss 0.0153 (0.0165)	
training:	Epoch: [61][92/233]	Loss 0.0138 (0.0165)	
training:	Epoch: [61][93/233]	Loss 0.0316 (0.0167)	
training:	Epoch: [61][94/233]	Loss 0.0298 (0.0168)	
training:	Epoch: [61][95/233]	Loss 0.0158 (0.0168)	
training:	Epoch: [61][96/233]	Loss 0.0135 (0.0168)	
training:	Epoch: [61][97/233]	Loss 0.0201 (0.0168)	
training:	Epoch: [61][98/233]	Loss 0.0160 (0.0168)	
training:	Epoch: [61][99/233]	Loss 0.0180 (0.0168)	
training:	Epoch: [61][100/233]	Loss 0.0134 (0.0168)	
training:	Epoch: [61][101/233]	Loss 0.0182 (0.0168)	
training:	Epoch: [61][102/233]	Loss 0.0299 (0.0169)	
training:	Epoch: [61][103/233]	Loss 0.0174 (0.0169)	
training:	Epoch: [61][104/233]	Loss 0.0167 (0.0169)	
training:	Epoch: [61][105/233]	Loss 0.0157 (0.0169)	
training:	Epoch: [61][106/233]	Loss 0.0142 (0.0169)	
training:	Epoch: [61][107/233]	Loss 0.0143 (0.0169)	
training:	Epoch: [61][108/233]	Loss 0.0128 (0.0168)	
training:	Epoch: [61][109/233]	Loss 0.0135 (0.0168)	
training:	Epoch: [61][110/233]	Loss 0.0137 (0.0168)	
training:	Epoch: [61][111/233]	Loss 0.0397 (0.0170)	
training:	Epoch: [61][112/233]	Loss 0.0153 (0.0169)	
training:	Epoch: [61][113/233]	Loss 0.0161 (0.0169)	
training:	Epoch: [61][114/233]	Loss 0.0162 (0.0169)	
training:	Epoch: [61][115/233]	Loss 0.0210 (0.0170)	
training:	Epoch: [61][116/233]	Loss 0.0252 (0.0170)	
training:	Epoch: [61][117/233]	Loss 0.0223 (0.0171)	
training:	Epoch: [61][118/233]	Loss 0.0214 (0.0171)	
training:	Epoch: [61][119/233]	Loss 0.0145 (0.0171)	
training:	Epoch: [61][120/233]	Loss 0.0365 (0.0173)	
training:	Epoch: [61][121/233]	Loss 0.0136 (0.0172)	
training:	Epoch: [61][122/233]	Loss 0.0239 (0.0173)	
training:	Epoch: [61][123/233]	Loss 0.0147 (0.0173)	
training:	Epoch: [61][124/233]	Loss 0.0160 (0.0173)	
training:	Epoch: [61][125/233]	Loss 0.0137 (0.0172)	
training:	Epoch: [61][126/233]	Loss 0.0218 (0.0173)	
training:	Epoch: [61][127/233]	Loss 0.0160 (0.0173)	
training:	Epoch: [61][128/233]	Loss 0.0148 (0.0172)	
training:	Epoch: [61][129/233]	Loss 0.0157 (0.0172)	
training:	Epoch: [61][130/233]	Loss 0.0181 (0.0172)	
training:	Epoch: [61][131/233]	Loss 0.0173 (0.0172)	
training:	Epoch: [61][132/233]	Loss 0.0154 (0.0172)	
training:	Epoch: [61][133/233]	Loss 0.0144 (0.0172)	
training:	Epoch: [61][134/233]	Loss 0.0167 (0.0172)	
training:	Epoch: [61][135/233]	Loss 0.0130 (0.0172)	
training:	Epoch: [61][136/233]	Loss 0.0285 (0.0172)	
training:	Epoch: [61][137/233]	Loss 0.0182 (0.0173)	
training:	Epoch: [61][138/233]	Loss 0.0143 (0.0172)	
training:	Epoch: [61][139/233]	Loss 0.0187 (0.0172)	
training:	Epoch: [61][140/233]	Loss 0.0175 (0.0172)	
training:	Epoch: [61][141/233]	Loss 0.0252 (0.0173)	
training:	Epoch: [61][142/233]	Loss 0.0135 (0.0173)	
training:	Epoch: [61][143/233]	Loss 0.0223 (0.0173)	
training:	Epoch: [61][144/233]	Loss 0.0139 (0.0173)	
training:	Epoch: [61][145/233]	Loss 0.0181 (0.0173)	
training:	Epoch: [61][146/233]	Loss 0.0209 (0.0173)	
training:	Epoch: [61][147/233]	Loss 0.0149 (0.0173)	
training:	Epoch: [61][148/233]	Loss 0.0129 (0.0173)	
training:	Epoch: [61][149/233]	Loss 0.0152 (0.0173)	
training:	Epoch: [61][150/233]	Loss 0.0163 (0.0172)	
training:	Epoch: [61][151/233]	Loss 0.0138 (0.0172)	
training:	Epoch: [61][152/233]	Loss 0.0213 (0.0173)	
training:	Epoch: [61][153/233]	Loss 0.0134 (0.0172)	
training:	Epoch: [61][154/233]	Loss 0.0180 (0.0172)	
training:	Epoch: [61][155/233]	Loss 0.0141 (0.0172)	
training:	Epoch: [61][156/233]	Loss 0.0140 (0.0172)	
training:	Epoch: [61][157/233]	Loss 0.0130 (0.0172)	
training:	Epoch: [61][158/233]	Loss 0.0181 (0.0172)	
training:	Epoch: [61][159/233]	Loss 0.0141 (0.0171)	
training:	Epoch: [61][160/233]	Loss 0.0139 (0.0171)	
training:	Epoch: [61][161/233]	Loss 0.0146 (0.0171)	
training:	Epoch: [61][162/233]	Loss 0.0197 (0.0171)	
training:	Epoch: [61][163/233]	Loss 0.0147 (0.0171)	
training:	Epoch: [61][164/233]	Loss 0.0171 (0.0171)	
training:	Epoch: [61][165/233]	Loss 0.0183 (0.0171)	
training:	Epoch: [61][166/233]	Loss 0.0172 (0.0171)	
training:	Epoch: [61][167/233]	Loss 0.0142 (0.0171)	
training:	Epoch: [61][168/233]	Loss 0.0137 (0.0171)	
training:	Epoch: [61][169/233]	Loss 0.0147 (0.0171)	
training:	Epoch: [61][170/233]	Loss 0.0146 (0.0171)	
training:	Epoch: [61][171/233]	Loss 0.1097 (0.0176)	
training:	Epoch: [61][172/233]	Loss 0.0149 (0.0176)	
training:	Epoch: [61][173/233]	Loss 0.0142 (0.0176)	
training:	Epoch: [61][174/233]	Loss 0.0139 (0.0175)	
training:	Epoch: [61][175/233]	Loss 0.0293 (0.0176)	
training:	Epoch: [61][176/233]	Loss 0.0189 (0.0176)	
training:	Epoch: [61][177/233]	Loss 0.0133 (0.0176)	
training:	Epoch: [61][178/233]	Loss 0.0282 (0.0177)	
training:	Epoch: [61][179/233]	Loss 0.0149 (0.0176)	
training:	Epoch: [61][180/233]	Loss 0.0146 (0.0176)	
training:	Epoch: [61][181/233]	Loss 0.0196 (0.0176)	
training:	Epoch: [61][182/233]	Loss 0.0187 (0.0176)	
training:	Epoch: [61][183/233]	Loss 0.0144 (0.0176)	
training:	Epoch: [61][184/233]	Loss 0.0152 (0.0176)	
training:	Epoch: [61][185/233]	Loss 0.0172 (0.0176)	
training:	Epoch: [61][186/233]	Loss 0.0157 (0.0176)	
training:	Epoch: [61][187/233]	Loss 0.0138 (0.0176)	
training:	Epoch: [61][188/233]	Loss 0.0130 (0.0175)	
training:	Epoch: [61][189/233]	Loss 0.0142 (0.0175)	
training:	Epoch: [61][190/233]	Loss 0.0248 (0.0176)	
training:	Epoch: [61][191/233]	Loss 0.0161 (0.0176)	
training:	Epoch: [61][192/233]	Loss 0.0131 (0.0175)	
training:	Epoch: [61][193/233]	Loss 0.0169 (0.0175)	
training:	Epoch: [61][194/233]	Loss 0.0164 (0.0175)	
training:	Epoch: [61][195/233]	Loss 0.0132 (0.0175)	
training:	Epoch: [61][196/233]	Loss 0.0215 (0.0175)	
training:	Epoch: [61][197/233]	Loss 0.0122 (0.0175)	
training:	Epoch: [61][198/233]	Loss 0.0149 (0.0175)	
training:	Epoch: [61][199/233]	Loss 0.0278 (0.0175)	
training:	Epoch: [61][200/233]	Loss 0.0153 (0.0175)	
training:	Epoch: [61][201/233]	Loss 0.0152 (0.0175)	
training:	Epoch: [61][202/233]	Loss 0.0131 (0.0175)	
training:	Epoch: [61][203/233]	Loss 0.0153 (0.0175)	
training:	Epoch: [61][204/233]	Loss 0.0127 (0.0175)	
training:	Epoch: [61][205/233]	Loss 0.0223 (0.0175)	
training:	Epoch: [61][206/233]	Loss 0.0146 (0.0175)	
training:	Epoch: [61][207/233]	Loss 0.0137 (0.0175)	
training:	Epoch: [61][208/233]	Loss 0.0161 (0.0174)	
training:	Epoch: [61][209/233]	Loss 0.0344 (0.0175)	
training:	Epoch: [61][210/233]	Loss 0.0140 (0.0175)	
training:	Epoch: [61][211/233]	Loss 0.0125 (0.0175)	
training:	Epoch: [61][212/233]	Loss 0.0140 (0.0175)	
training:	Epoch: [61][213/233]	Loss 0.0162 (0.0175)	
training:	Epoch: [61][214/233]	Loss 0.0394 (0.0176)	
training:	Epoch: [61][215/233]	Loss 0.0135 (0.0175)	
training:	Epoch: [61][216/233]	Loss 0.0141 (0.0175)	
training:	Epoch: [61][217/233]	Loss 0.0137 (0.0175)	
training:	Epoch: [61][218/233]	Loss 0.0143 (0.0175)	
training:	Epoch: [61][219/233]	Loss 0.0176 (0.0175)	
training:	Epoch: [61][220/233]	Loss 0.0593 (0.0177)	
training:	Epoch: [61][221/233]	Loss 0.0528 (0.0178)	
training:	Epoch: [61][222/233]	Loss 0.0220 (0.0179)	
training:	Epoch: [61][223/233]	Loss 0.0132 (0.0178)	
training:	Epoch: [61][224/233]	Loss 0.0159 (0.0178)	
training:	Epoch: [61][225/233]	Loss 0.0147 (0.0178)	
training:	Epoch: [61][226/233]	Loss 0.0144 (0.0178)	
training:	Epoch: [61][227/233]	Loss 0.0149 (0.0178)	
training:	Epoch: [61][228/233]	Loss 0.0295 (0.0178)	
training:	Epoch: [61][229/233]	Loss 0.0323 (0.0179)	
training:	Epoch: [61][230/233]	Loss 0.0155 (0.0179)	
training:	Epoch: [61][231/233]	Loss 0.0186 (0.0179)	
training:	Epoch: [61][232/233]	Loss 0.0138 (0.0179)	
training:	Epoch: [61][233/233]	Loss 0.0141 (0.0179)	
Training:	 Loss: 0.0178

Training:	 ACC: 1.0000 1.0000 1.0000 1.0000
Validation:	 ACC: 0.8621 0.8614 0.8465 0.8778
Validation:	 Best_BACC: 0.8682 0.8678 0.8598 0.8767
Validation:	 Loss: 0.3246
Pretraining:	Epoch 62/200
----------
training:	Epoch: [62][1/233]	Loss 0.0352 (0.0352)	
training:	Epoch: [62][2/233]	Loss 0.0124 (0.0238)	
training:	Epoch: [62][3/233]	Loss 0.0186 (0.0221)	
training:	Epoch: [62][4/233]	Loss 0.0128 (0.0197)	
training:	Epoch: [62][5/233]	Loss 0.0136 (0.0185)	
training:	Epoch: [62][6/233]	Loss 0.0203 (0.0188)	
training:	Epoch: [62][7/233]	Loss 0.0131 (0.0180)	
training:	Epoch: [62][8/233]	Loss 0.0159 (0.0177)	
training:	Epoch: [62][9/233]	Loss 0.0123 (0.0171)	
training:	Epoch: [62][10/233]	Loss 0.0184 (0.0173)	
training:	Epoch: [62][11/233]	Loss 0.0151 (0.0171)	
training:	Epoch: [62][12/233]	Loss 0.0158 (0.0170)	
training:	Epoch: [62][13/233]	Loss 0.0151 (0.0168)	
training:	Epoch: [62][14/233]	Loss 0.0129 (0.0165)	
training:	Epoch: [62][15/233]	Loss 0.0137 (0.0163)	
training:	Epoch: [62][16/233]	Loss 0.0156 (0.0163)	
training:	Epoch: [62][17/233]	Loss 0.0136 (0.0161)	
training:	Epoch: [62][18/233]	Loss 0.0160 (0.0161)	
training:	Epoch: [62][19/233]	Loss 0.0137 (0.0160)	
training:	Epoch: [62][20/233]	Loss 0.0138 (0.0159)	
training:	Epoch: [62][21/233]	Loss 0.0137 (0.0158)	
training:	Epoch: [62][22/233]	Loss 0.0170 (0.0158)	
training:	Epoch: [62][23/233]	Loss 0.0280 (0.0164)	
training:	Epoch: [62][24/233]	Loss 0.0139 (0.0163)	
training:	Epoch: [62][25/233]	Loss 0.0144 (0.0162)	
training:	Epoch: [62][26/233]	Loss 0.0139 (0.0161)	
training:	Epoch: [62][27/233]	Loss 0.0123 (0.0160)	
training:	Epoch: [62][28/233]	Loss 0.0129 (0.0159)	
training:	Epoch: [62][29/233]	Loss 0.0153 (0.0158)	
training:	Epoch: [62][30/233]	Loss 0.0145 (0.0158)	
training:	Epoch: [62][31/233]	Loss 0.0208 (0.0159)	
training:	Epoch: [62][32/233]	Loss 0.0129 (0.0159)	
training:	Epoch: [62][33/233]	Loss 0.0190 (0.0159)	
training:	Epoch: [62][34/233]	Loss 0.0128 (0.0159)	
training:	Epoch: [62][35/233]	Loss 0.0177 (0.0159)	
training:	Epoch: [62][36/233]	Loss 0.0127 (0.0158)	
training:	Epoch: [62][37/233]	Loss 0.0133 (0.0158)	
training:	Epoch: [62][38/233]	Loss 0.0132 (0.0157)	
training:	Epoch: [62][39/233]	Loss 0.0147 (0.0157)	
training:	Epoch: [62][40/233]	Loss 0.0241 (0.0159)	
training:	Epoch: [62][41/233]	Loss 0.0141 (0.0158)	
training:	Epoch: [62][42/233]	Loss 0.0132 (0.0158)	
training:	Epoch: [62][43/233]	Loss 0.0173 (0.0158)	
training:	Epoch: [62][44/233]	Loss 0.0150 (0.0158)	
training:	Epoch: [62][45/233]	Loss 0.0135 (0.0157)	
training:	Epoch: [62][46/233]	Loss 0.0162 (0.0157)	
training:	Epoch: [62][47/233]	Loss 0.0150 (0.0157)	
training:	Epoch: [62][48/233]	Loss 0.0136 (0.0157)	
training:	Epoch: [62][49/233]	Loss 0.0137 (0.0156)	
training:	Epoch: [62][50/233]	Loss 0.0133 (0.0156)	
training:	Epoch: [62][51/233]	Loss 0.0133 (0.0155)	
training:	Epoch: [62][52/233]	Loss 0.0271 (0.0158)	
training:	Epoch: [62][53/233]	Loss 0.0156 (0.0158)	
training:	Epoch: [62][54/233]	Loss 0.0140 (0.0157)	
training:	Epoch: [62][55/233]	Loss 0.0125 (0.0157)	
training:	Epoch: [62][56/233]	Loss 0.0256 (0.0159)	
training:	Epoch: [62][57/233]	Loss 0.0222 (0.0160)	
training:	Epoch: [62][58/233]	Loss 0.0163 (0.0160)	
training:	Epoch: [62][59/233]	Loss 0.0133 (0.0159)	
training:	Epoch: [62][60/233]	Loss 0.0137 (0.0159)	
training:	Epoch: [62][61/233]	Loss 0.0188 (0.0159)	
training:	Epoch: [62][62/233]	Loss 0.0254 (0.0161)	
training:	Epoch: [62][63/233]	Loss 0.0156 (0.0161)	
training:	Epoch: [62][64/233]	Loss 0.0293 (0.0163)	
training:	Epoch: [62][65/233]	Loss 0.0324 (0.0165)	
training:	Epoch: [62][66/233]	Loss 0.0146 (0.0165)	
training:	Epoch: [62][67/233]	Loss 0.0194 (0.0165)	
training:	Epoch: [62][68/233]	Loss 0.0130 (0.0165)	
training:	Epoch: [62][69/233]	Loss 0.0139 (0.0165)	
training:	Epoch: [62][70/233]	Loss 0.0189 (0.0165)	
training:	Epoch: [62][71/233]	Loss 0.0133 (0.0164)	
training:	Epoch: [62][72/233]	Loss 0.0143 (0.0164)	
training:	Epoch: [62][73/233]	Loss 0.0161 (0.0164)	
training:	Epoch: [62][74/233]	Loss 0.0158 (0.0164)	
training:	Epoch: [62][75/233]	Loss 0.0138 (0.0164)	
training:	Epoch: [62][76/233]	Loss 0.0130 (0.0163)	
training:	Epoch: [62][77/233]	Loss 0.0173 (0.0163)	
training:	Epoch: [62][78/233]	Loss 0.0134 (0.0163)	
training:	Epoch: [62][79/233]	Loss 0.0138 (0.0163)	
training:	Epoch: [62][80/233]	Loss 0.0224 (0.0163)	
training:	Epoch: [62][81/233]	Loss 0.0148 (0.0163)	
training:	Epoch: [62][82/233]	Loss 0.0165 (0.0163)	
training:	Epoch: [62][83/233]	Loss 0.0289 (0.0165)	
training:	Epoch: [62][84/233]	Loss 0.0217 (0.0165)	
training:	Epoch: [62][85/233]	Loss 0.0128 (0.0165)	
training:	Epoch: [62][86/233]	Loss 0.0133 (0.0165)	
training:	Epoch: [62][87/233]	Loss 0.0215 (0.0165)	
training:	Epoch: [62][88/233]	Loss 0.0155 (0.0165)	
training:	Epoch: [62][89/233]	Loss 0.0168 (0.0165)	
training:	Epoch: [62][90/233]	Loss 0.0139 (0.0165)	
training:	Epoch: [62][91/233]	Loss 0.0136 (0.0164)	
training:	Epoch: [62][92/233]	Loss 0.0119 (0.0164)	
training:	Epoch: [62][93/233]	Loss 0.0238 (0.0165)	
training:	Epoch: [62][94/233]	Loss 0.0123 (0.0164)	
training:	Epoch: [62][95/233]	Loss 0.0212 (0.0165)	
training:	Epoch: [62][96/233]	Loss 0.0125 (0.0164)	
training:	Epoch: [62][97/233]	Loss 0.0214 (0.0165)	
training:	Epoch: [62][98/233]	Loss 0.0175 (0.0165)	
training:	Epoch: [62][99/233]	Loss 0.0148 (0.0165)	
training:	Epoch: [62][100/233]	Loss 0.0146 (0.0165)	
training:	Epoch: [62][101/233]	Loss 0.0135 (0.0164)	
training:	Epoch: [62][102/233]	Loss 0.0198 (0.0165)	
training:	Epoch: [62][103/233]	Loss 0.0136 (0.0164)	
training:	Epoch: [62][104/233]	Loss 0.0117 (0.0164)	
training:	Epoch: [62][105/233]	Loss 0.0130 (0.0164)	
training:	Epoch: [62][106/233]	Loss 0.0183 (0.0164)	
training:	Epoch: [62][107/233]	Loss 0.0139 (0.0164)	
training:	Epoch: [62][108/233]	Loss 0.0129 (0.0163)	
training:	Epoch: [62][109/233]	Loss 0.0133 (0.0163)	
training:	Epoch: [62][110/233]	Loss 0.0280 (0.0164)	
training:	Epoch: [62][111/233]	Loss 0.0154 (0.0164)	
training:	Epoch: [62][112/233]	Loss 0.0214 (0.0164)	
training:	Epoch: [62][113/233]	Loss 0.0299 (0.0166)	
training:	Epoch: [62][114/233]	Loss 0.0165 (0.0166)	
training:	Epoch: [62][115/233]	Loss 0.0203 (0.0166)	
training:	Epoch: [62][116/233]	Loss 0.0152 (0.0166)	
training:	Epoch: [62][117/233]	Loss 0.0164 (0.0166)	
training:	Epoch: [62][118/233]	Loss 0.0205 (0.0166)	
training:	Epoch: [62][119/233]	Loss 0.0130 (0.0166)	
training:	Epoch: [62][120/233]	Loss 0.0319 (0.0167)	
training:	Epoch: [62][121/233]	Loss 0.0130 (0.0167)	
training:	Epoch: [62][122/233]	Loss 0.0147 (0.0167)	
training:	Epoch: [62][123/233]	Loss 0.0164 (0.0167)	
training:	Epoch: [62][124/233]	Loss 0.0157 (0.0167)	
training:	Epoch: [62][125/233]	Loss 0.0124 (0.0166)	
training:	Epoch: [62][126/233]	Loss 0.0139 (0.0166)	
training:	Epoch: [62][127/233]	Loss 0.0136 (0.0166)	
training:	Epoch: [62][128/233]	Loss 0.0138 (0.0166)	
training:	Epoch: [62][129/233]	Loss 0.0171 (0.0166)	
training:	Epoch: [62][130/233]	Loss 0.0209 (0.0166)	
training:	Epoch: [62][131/233]	Loss 0.0661 (0.0170)	
training:	Epoch: [62][132/233]	Loss 0.0184 (0.0170)	
training:	Epoch: [62][133/233]	Loss 0.0128 (0.0169)	
training:	Epoch: [62][134/233]	Loss 0.0144 (0.0169)	
training:	Epoch: [62][135/233]	Loss 0.0126 (0.0169)	
training:	Epoch: [62][136/233]	Loss 0.0165 (0.0169)	
training:	Epoch: [62][137/233]	Loss 0.0123 (0.0169)	
training:	Epoch: [62][138/233]	Loss 0.0139 (0.0168)	
training:	Epoch: [62][139/233]	Loss 0.0561 (0.0171)	
training:	Epoch: [62][140/233]	Loss 0.0154 (0.0171)	
training:	Epoch: [62][141/233]	Loss 0.0119 (0.0171)	
training:	Epoch: [62][142/233]	Loss 0.0282 (0.0171)	
training:	Epoch: [62][143/233]	Loss 0.0139 (0.0171)	
training:	Epoch: [62][144/233]	Loss 0.0132 (0.0171)	
training:	Epoch: [62][145/233]	Loss 0.0133 (0.0171)	
training:	Epoch: [62][146/233]	Loss 0.0186 (0.0171)	
training:	Epoch: [62][147/233]	Loss 0.0147 (0.0171)	
training:	Epoch: [62][148/233]	Loss 0.0134 (0.0170)	
training:	Epoch: [62][149/233]	Loss 0.0156 (0.0170)	
training:	Epoch: [62][150/233]	Loss 0.0129 (0.0170)	
training:	Epoch: [62][151/233]	Loss 0.0202 (0.0170)	
training:	Epoch: [62][152/233]	Loss 0.0148 (0.0170)	
training:	Epoch: [62][153/233]	Loss 0.0152 (0.0170)	
training:	Epoch: [62][154/233]	Loss 0.0119 (0.0170)	
training:	Epoch: [62][155/233]	Loss 0.0179 (0.0170)	
training:	Epoch: [62][156/233]	Loss 0.0118 (0.0169)	
training:	Epoch: [62][157/233]	Loss 0.0135 (0.0169)	
training:	Epoch: [62][158/233]	Loss 0.0172 (0.0169)	
training:	Epoch: [62][159/233]	Loss 0.0264 (0.0170)	
training:	Epoch: [62][160/233]	Loss 0.0202 (0.0170)	
training:	Epoch: [62][161/233]	Loss 0.0149 (0.0170)	
training:	Epoch: [62][162/233]	Loss 0.0136 (0.0170)	
training:	Epoch: [62][163/233]	Loss 0.0123 (0.0169)	
training:	Epoch: [62][164/233]	Loss 0.0152 (0.0169)	
training:	Epoch: [62][165/233]	Loss 0.0141 (0.0169)	
training:	Epoch: [62][166/233]	Loss 0.0164 (0.0169)	
training:	Epoch: [62][167/233]	Loss 0.0136 (0.0169)	
training:	Epoch: [62][168/233]	Loss 0.0239 (0.0169)	
training:	Epoch: [62][169/233]	Loss 0.0131 (0.0169)	
training:	Epoch: [62][170/233]	Loss 0.0238 (0.0169)	
training:	Epoch: [62][171/233]	Loss 0.0230 (0.0170)	
training:	Epoch: [62][172/233]	Loss 0.0131 (0.0170)	
training:	Epoch: [62][173/233]	Loss 0.0220 (0.0170)	
training:	Epoch: [62][174/233]	Loss 0.0144 (0.0170)	
training:	Epoch: [62][175/233]	Loss 0.0147 (0.0170)	
training:	Epoch: [62][176/233]	Loss 0.0145 (0.0169)	
training:	Epoch: [62][177/233]	Loss 0.0162 (0.0169)	
training:	Epoch: [62][178/233]	Loss 0.0177 (0.0169)	
training:	Epoch: [62][179/233]	Loss 0.0254 (0.0170)	
training:	Epoch: [62][180/233]	Loss 0.0154 (0.0170)	
training:	Epoch: [62][181/233]	Loss 0.0119 (0.0170)	
training:	Epoch: [62][182/233]	Loss 0.0127 (0.0169)	
training:	Epoch: [62][183/233]	Loss 0.0144 (0.0169)	
training:	Epoch: [62][184/233]	Loss 0.0120 (0.0169)	
training:	Epoch: [62][185/233]	Loss 0.0262 (0.0169)	
training:	Epoch: [62][186/233]	Loss 0.0170 (0.0169)	
training:	Epoch: [62][187/233]	Loss 0.0159 (0.0169)	
training:	Epoch: [62][188/233]	Loss 0.0243 (0.0170)	
training:	Epoch: [62][189/233]	Loss 0.0147 (0.0170)	
training:	Epoch: [62][190/233]	Loss 0.0135 (0.0169)	
training:	Epoch: [62][191/233]	Loss 0.0137 (0.0169)	
training:	Epoch: [62][192/233]	Loss 0.0243 (0.0170)	
training:	Epoch: [62][193/233]	Loss 0.0130 (0.0169)	
training:	Epoch: [62][194/233]	Loss 0.0236 (0.0170)	
training:	Epoch: [62][195/233]	Loss 0.0227 (0.0170)	
training:	Epoch: [62][196/233]	Loss 0.0124 (0.0170)	
training:	Epoch: [62][197/233]	Loss 0.0153 (0.0170)	
training:	Epoch: [62][198/233]	Loss 0.0196 (0.0170)	
training:	Epoch: [62][199/233]	Loss 0.0183 (0.0170)	
training:	Epoch: [62][200/233]	Loss 0.0280 (0.0171)	
training:	Epoch: [62][201/233]	Loss 0.0253 (0.0171)	
training:	Epoch: [62][202/233]	Loss 0.0139 (0.0171)	
training:	Epoch: [62][203/233]	Loss 0.0144 (0.0171)	
training:	Epoch: [62][204/233]	Loss 0.0179 (0.0171)	
training:	Epoch: [62][205/233]	Loss 0.0160 (0.0171)	
training:	Epoch: [62][206/233]	Loss 0.0216 (0.0171)	
training:	Epoch: [62][207/233]	Loss 0.0132 (0.0171)	
training:	Epoch: [62][208/233]	Loss 0.0127 (0.0170)	
training:	Epoch: [62][209/233]	Loss 0.0124 (0.0170)	
training:	Epoch: [62][210/233]	Loss 0.0132 (0.0170)	
training:	Epoch: [62][211/233]	Loss 0.0164 (0.0170)	
training:	Epoch: [62][212/233]	Loss 0.0133 (0.0170)	
training:	Epoch: [62][213/233]	Loss 0.0133 (0.0170)	
training:	Epoch: [62][214/233]	Loss 0.0120 (0.0169)	
training:	Epoch: [62][215/233]	Loss 0.0158 (0.0169)	
training:	Epoch: [62][216/233]	Loss 0.0132 (0.0169)	
training:	Epoch: [62][217/233]	Loss 0.0155 (0.0169)	
training:	Epoch: [62][218/233]	Loss 0.0165 (0.0169)	
training:	Epoch: [62][219/233]	Loss 0.0115 (0.0169)	
training:	Epoch: [62][220/233]	Loss 0.0149 (0.0169)	
training:	Epoch: [62][221/233]	Loss 0.0169 (0.0169)	
training:	Epoch: [62][222/233]	Loss 0.0134 (0.0169)	
training:	Epoch: [62][223/233]	Loss 0.0155 (0.0169)	
training:	Epoch: [62][224/233]	Loss 0.0110 (0.0168)	
training:	Epoch: [62][225/233]	Loss 0.0135 (0.0168)	
training:	Epoch: [62][226/233]	Loss 0.0131 (0.0168)	
training:	Epoch: [62][227/233]	Loss 0.0223 (0.0168)	
training:	Epoch: [62][228/233]	Loss 0.0137 (0.0168)	
training:	Epoch: [62][229/233]	Loss 0.0173 (0.0168)	
training:	Epoch: [62][230/233]	Loss 0.0141 (0.0168)	
training:	Epoch: [62][231/233]	Loss 0.0169 (0.0168)	
training:	Epoch: [62][232/233]	Loss 0.0159 (0.0168)	
training:	Epoch: [62][233/233]	Loss 0.0230 (0.0168)	
Training:	 Loss: 0.0168

Training:	 ACC: 1.0000 1.0000 1.0000 1.0000
Validation:	 ACC: 0.8636 0.8636 0.8628 0.8643
Validation:	 Best_BACC: 0.8682 0.8678 0.8598 0.8767
Validation:	 Loss: 0.3207
Pretraining:	Epoch 63/200
----------
training:	Epoch: [63][1/233]	Loss 0.0165 (0.0165)	
training:	Epoch: [63][2/233]	Loss 0.0125 (0.0145)	
training:	Epoch: [63][3/233]	Loss 0.0137 (0.0143)	
training:	Epoch: [63][4/233]	Loss 0.0122 (0.0137)	
training:	Epoch: [63][5/233]	Loss 0.0132 (0.0136)	
training:	Epoch: [63][6/233]	Loss 0.0124 (0.0134)	
training:	Epoch: [63][7/233]	Loss 0.0135 (0.0134)	
training:	Epoch: [63][8/233]	Loss 0.0190 (0.0141)	
training:	Epoch: [63][9/233]	Loss 0.0125 (0.0140)	
training:	Epoch: [63][10/233]	Loss 0.0119 (0.0137)	
training:	Epoch: [63][11/233]	Loss 0.0182 (0.0141)	
training:	Epoch: [63][12/233]	Loss 0.0206 (0.0147)	
training:	Epoch: [63][13/233]	Loss 0.0133 (0.0146)	
training:	Epoch: [63][14/233]	Loss 0.0163 (0.0147)	
training:	Epoch: [63][15/233]	Loss 0.0132 (0.0146)	
training:	Epoch: [63][16/233]	Loss 0.0148 (0.0146)	
training:	Epoch: [63][17/233]	Loss 0.0162 (0.0147)	
training:	Epoch: [63][18/233]	Loss 0.0111 (0.0145)	
training:	Epoch: [63][19/233]	Loss 0.0136 (0.0145)	
training:	Epoch: [63][20/233]	Loss 0.0157 (0.0145)	
training:	Epoch: [63][21/233]	Loss 0.0177 (0.0147)	
training:	Epoch: [63][22/233]	Loss 0.0177 (0.0148)	
training:	Epoch: [63][23/233]	Loss 0.0250 (0.0152)	
training:	Epoch: [63][24/233]	Loss 0.0138 (0.0152)	
training:	Epoch: [63][25/233]	Loss 0.0222 (0.0155)	
training:	Epoch: [63][26/233]	Loss 0.0125 (0.0154)	
training:	Epoch: [63][27/233]	Loss 0.0125 (0.0153)	
training:	Epoch: [63][28/233]	Loss 0.0192 (0.0154)	
training:	Epoch: [63][29/233]	Loss 0.0154 (0.0154)	
training:	Epoch: [63][30/233]	Loss 0.0125 (0.0153)	
training:	Epoch: [63][31/233]	Loss 0.0124 (0.0152)	
training:	Epoch: [63][32/233]	Loss 0.0291 (0.0156)	
training:	Epoch: [63][33/233]	Loss 0.0137 (0.0156)	
training:	Epoch: [63][34/233]	Loss 0.0147 (0.0156)	
training:	Epoch: [63][35/233]	Loss 0.0249 (0.0158)	
training:	Epoch: [63][36/233]	Loss 0.0335 (0.0163)	
training:	Epoch: [63][37/233]	Loss 0.0122 (0.0162)	
training:	Epoch: [63][38/233]	Loss 0.0159 (0.0162)	
training:	Epoch: [63][39/233]	Loss 0.0122 (0.0161)	
training:	Epoch: [63][40/233]	Loss 0.0148 (0.0161)	
training:	Epoch: [63][41/233]	Loss 0.0143 (0.0160)	
training:	Epoch: [63][42/233]	Loss 0.0132 (0.0159)	
training:	Epoch: [63][43/233]	Loss 0.0126 (0.0159)	
training:	Epoch: [63][44/233]	Loss 0.0122 (0.0158)	
training:	Epoch: [63][45/233]	Loss 0.0145 (0.0158)	
training:	Epoch: [63][46/233]	Loss 0.0120 (0.0157)	
training:	Epoch: [63][47/233]	Loss 0.0139 (0.0156)	
training:	Epoch: [63][48/233]	Loss 0.0120 (0.0156)	
training:	Epoch: [63][49/233]	Loss 0.0158 (0.0156)	
training:	Epoch: [63][50/233]	Loss 0.0131 (0.0155)	
training:	Epoch: [63][51/233]	Loss 0.0127 (0.0155)	
training:	Epoch: [63][52/233]	Loss 0.0296 (0.0157)	
training:	Epoch: [63][53/233]	Loss 0.0138 (0.0157)	
training:	Epoch: [63][54/233]	Loss 0.0185 (0.0158)	
training:	Epoch: [63][55/233]	Loss 0.0185 (0.0158)	
training:	Epoch: [63][56/233]	Loss 0.0132 (0.0158)	
training:	Epoch: [63][57/233]	Loss 0.0161 (0.0158)	
training:	Epoch: [63][58/233]	Loss 0.0387 (0.0162)	
training:	Epoch: [63][59/233]	Loss 0.0133 (0.0161)	
training:	Epoch: [63][60/233]	Loss 0.0144 (0.0161)	
training:	Epoch: [63][61/233]	Loss 0.0112 (0.0160)	
training:	Epoch: [63][62/233]	Loss 0.0202 (0.0161)	
training:	Epoch: [63][63/233]	Loss 0.0208 (0.0161)	
training:	Epoch: [63][64/233]	Loss 0.0173 (0.0162)	
training:	Epoch: [63][65/233]	Loss 0.0205 (0.0162)	
training:	Epoch: [63][66/233]	Loss 0.0125 (0.0162)	
training:	Epoch: [63][67/233]	Loss 0.0225 (0.0163)	
training:	Epoch: [63][68/233]	Loss 0.0121 (0.0162)	
training:	Epoch: [63][69/233]	Loss 0.0121 (0.0161)	
training:	Epoch: [63][70/233]	Loss 0.0134 (0.0161)	
training:	Epoch: [63][71/233]	Loss 0.0323 (0.0163)	
training:	Epoch: [63][72/233]	Loss 0.0124 (0.0163)	
training:	Epoch: [63][73/233]	Loss 0.0125 (0.0162)	
training:	Epoch: [63][74/233]	Loss 0.0120 (0.0162)	
training:	Epoch: [63][75/233]	Loss 0.0128 (0.0161)	
training:	Epoch: [63][76/233]	Loss 0.0137 (0.0161)	
training:	Epoch: [63][77/233]	Loss 0.0248 (0.0162)	
training:	Epoch: [63][78/233]	Loss 0.0169 (0.0162)	
training:	Epoch: [63][79/233]	Loss 0.0134 (0.0162)	
training:	Epoch: [63][80/233]	Loss 0.0142 (0.0162)	
training:	Epoch: [63][81/233]	Loss 0.0193 (0.0162)	
training:	Epoch: [63][82/233]	Loss 0.0168 (0.0162)	
training:	Epoch: [63][83/233]	Loss 0.0179 (0.0162)	
training:	Epoch: [63][84/233]	Loss 0.0216 (0.0163)	
training:	Epoch: [63][85/233]	Loss 0.0235 (0.0164)	
training:	Epoch: [63][86/233]	Loss 0.0175 (0.0164)	
training:	Epoch: [63][87/233]	Loss 0.0126 (0.0163)	
training:	Epoch: [63][88/233]	Loss 0.0134 (0.0163)	
training:	Epoch: [63][89/233]	Loss 0.0130 (0.0163)	
training:	Epoch: [63][90/233]	Loss 0.0222 (0.0163)	
training:	Epoch: [63][91/233]	Loss 0.0242 (0.0164)	
training:	Epoch: [63][92/233]	Loss 0.0114 (0.0164)	
training:	Epoch: [63][93/233]	Loss 0.0240 (0.0165)	
training:	Epoch: [63][94/233]	Loss 0.0128 (0.0164)	
training:	Epoch: [63][95/233]	Loss 0.0184 (0.0164)	
training:	Epoch: [63][96/233]	Loss 0.0138 (0.0164)	
training:	Epoch: [63][97/233]	Loss 0.0268 (0.0165)	
training:	Epoch: [63][98/233]	Loss 0.0149 (0.0165)	
training:	Epoch: [63][99/233]	Loss 0.0137 (0.0165)	
training:	Epoch: [63][100/233]	Loss 0.0172 (0.0165)	
training:	Epoch: [63][101/233]	Loss 0.0120 (0.0164)	
training:	Epoch: [63][102/233]	Loss 0.0117 (0.0164)	
training:	Epoch: [63][103/233]	Loss 0.0129 (0.0164)	
training:	Epoch: [63][104/233]	Loss 0.0164 (0.0164)	
training:	Epoch: [63][105/233]	Loss 0.0151 (0.0163)	
training:	Epoch: [63][106/233]	Loss 0.0132 (0.0163)	
training:	Epoch: [63][107/233]	Loss 0.0190 (0.0163)	
training:	Epoch: [63][108/233]	Loss 0.0471 (0.0166)	
training:	Epoch: [63][109/233]	Loss 0.0139 (0.0166)	
training:	Epoch: [63][110/233]	Loss 0.0135 (0.0166)	
training:	Epoch: [63][111/233]	Loss 0.0130 (0.0165)	
training:	Epoch: [63][112/233]	Loss 0.0118 (0.0165)	
training:	Epoch: [63][113/233]	Loss 0.0182 (0.0165)	
training:	Epoch: [63][114/233]	Loss 0.0354 (0.0167)	
training:	Epoch: [63][115/233]	Loss 0.0125 (0.0166)	
training:	Epoch: [63][116/233]	Loss 0.0299 (0.0168)	
training:	Epoch: [63][117/233]	Loss 0.0128 (0.0167)	
training:	Epoch: [63][118/233]	Loss 0.0148 (0.0167)	
training:	Epoch: [63][119/233]	Loss 0.0151 (0.0167)	
training:	Epoch: [63][120/233]	Loss 0.0124 (0.0167)	
training:	Epoch: [63][121/233]	Loss 0.0139 (0.0166)	
training:	Epoch: [63][122/233]	Loss 0.0139 (0.0166)	
training:	Epoch: [63][123/233]	Loss 0.0134 (0.0166)	
training:	Epoch: [63][124/233]	Loss 0.0173 (0.0166)	
training:	Epoch: [63][125/233]	Loss 0.0126 (0.0166)	
training:	Epoch: [63][126/233]	Loss 0.0180 (0.0166)	
training:	Epoch: [63][127/233]	Loss 0.0188 (0.0166)	
training:	Epoch: [63][128/233]	Loss 0.0127 (0.0166)	
training:	Epoch: [63][129/233]	Loss 0.0204 (0.0166)	
training:	Epoch: [63][130/233]	Loss 0.0121 (0.0166)	
training:	Epoch: [63][131/233]	Loss 0.0122 (0.0165)	
training:	Epoch: [63][132/233]	Loss 0.0215 (0.0166)	
training:	Epoch: [63][133/233]	Loss 0.0214 (0.0166)	
training:	Epoch: [63][134/233]	Loss 0.0156 (0.0166)	
training:	Epoch: [63][135/233]	Loss 0.0129 (0.0166)	
training:	Epoch: [63][136/233]	Loss 0.0125 (0.0165)	
training:	Epoch: [63][137/233]	Loss 0.0141 (0.0165)	
training:	Epoch: [63][138/233]	Loss 0.0124 (0.0165)	
training:	Epoch: [63][139/233]	Loss 0.0127 (0.0165)	
training:	Epoch: [63][140/233]	Loss 0.0126 (0.0164)	
training:	Epoch: [63][141/233]	Loss 0.0143 (0.0164)	
training:	Epoch: [63][142/233]	Loss 0.0135 (0.0164)	
training:	Epoch: [63][143/233]	Loss 0.0176 (0.0164)	
training:	Epoch: [63][144/233]	Loss 0.0134 (0.0164)	
training:	Epoch: [63][145/233]	Loss 0.0144 (0.0164)	
training:	Epoch: [63][146/233]	Loss 0.0122 (0.0163)	
training:	Epoch: [63][147/233]	Loss 0.0128 (0.0163)	
training:	Epoch: [63][148/233]	Loss 0.0153 (0.0163)	
training:	Epoch: [63][149/233]	Loss 0.0136 (0.0163)	
training:	Epoch: [63][150/233]	Loss 0.0187 (0.0163)	
training:	Epoch: [63][151/233]	Loss 0.0128 (0.0163)	
training:	Epoch: [63][152/233]	Loss 0.0132 (0.0163)	
training:	Epoch: [63][153/233]	Loss 0.0184 (0.0163)	
training:	Epoch: [63][154/233]	Loss 0.0130 (0.0163)	
training:	Epoch: [63][155/233]	Loss 0.0144 (0.0162)	
training:	Epoch: [63][156/233]	Loss 0.0124 (0.0162)	
training:	Epoch: [63][157/233]	Loss 0.0129 (0.0162)	
training:	Epoch: [63][158/233]	Loss 0.0119 (0.0162)	
training:	Epoch: [63][159/233]	Loss 0.0161 (0.0162)	
training:	Epoch: [63][160/233]	Loss 0.0118 (0.0161)	
training:	Epoch: [63][161/233]	Loss 0.0153 (0.0161)	
training:	Epoch: [63][162/233]	Loss 0.0180 (0.0161)	
training:	Epoch: [63][163/233]	Loss 0.0205 (0.0162)	
training:	Epoch: [63][164/233]	Loss 0.0116 (0.0161)	
training:	Epoch: [63][165/233]	Loss 0.0130 (0.0161)	
training:	Epoch: [63][166/233]	Loss 0.0167 (0.0161)	
training:	Epoch: [63][167/233]	Loss 0.0156 (0.0161)	
training:	Epoch: [63][168/233]	Loss 0.0135 (0.0161)	
training:	Epoch: [63][169/233]	Loss 0.0143 (0.0161)	
training:	Epoch: [63][170/233]	Loss 0.0155 (0.0161)	
training:	Epoch: [63][171/233]	Loss 0.0165 (0.0161)	
training:	Epoch: [63][172/233]	Loss 0.0166 (0.0161)	
training:	Epoch: [63][173/233]	Loss 0.0133 (0.0161)	
training:	Epoch: [63][174/233]	Loss 0.0133 (0.0161)	
training:	Epoch: [63][175/233]	Loss 0.0989 (0.0165)	
training:	Epoch: [63][176/233]	Loss 0.0122 (0.0165)	
training:	Epoch: [63][177/233]	Loss 0.0465 (0.0167)	
training:	Epoch: [63][178/233]	Loss 0.0131 (0.0167)	
training:	Epoch: [63][179/233]	Loss 0.0117 (0.0166)	
training:	Epoch: [63][180/233]	Loss 0.0135 (0.0166)	
training:	Epoch: [63][181/233]	Loss 0.0149 (0.0166)	
training:	Epoch: [63][182/233]	Loss 0.0119 (0.0166)	
training:	Epoch: [63][183/233]	Loss 0.0141 (0.0166)	
training:	Epoch: [63][184/233]	Loss 0.0139 (0.0166)	
training:	Epoch: [63][185/233]	Loss 0.0379 (0.0167)	
training:	Epoch: [63][186/233]	Loss 0.0198 (0.0167)	
training:	Epoch: [63][187/233]	Loss 0.0220 (0.0167)	
training:	Epoch: [63][188/233]	Loss 0.0125 (0.0167)	
training:	Epoch: [63][189/233]	Loss 0.0124 (0.0167)	
training:	Epoch: [63][190/233]	Loss 0.0203 (0.0167)	
training:	Epoch: [63][191/233]	Loss 0.0143 (0.0167)	
training:	Epoch: [63][192/233]	Loss 0.0119 (0.0167)	
training:	Epoch: [63][193/233]	Loss 0.0226 (0.0167)	
training:	Epoch: [63][194/233]	Loss 0.0138 (0.0167)	
training:	Epoch: [63][195/233]	Loss 0.0178 (0.0167)	
training:	Epoch: [63][196/233]	Loss 0.0119 (0.0167)	
training:	Epoch: [63][197/233]	Loss 0.0145 (0.0166)	
training:	Epoch: [63][198/233]	Loss 0.0122 (0.0166)	
training:	Epoch: [63][199/233]	Loss 0.0127 (0.0166)	
training:	Epoch: [63][200/233]	Loss 0.0134 (0.0166)	
training:	Epoch: [63][201/233]	Loss 0.0182 (0.0166)	
training:	Epoch: [63][202/233]	Loss 0.0133 (0.0166)	
training:	Epoch: [63][203/233]	Loss 0.0146 (0.0166)	
training:	Epoch: [63][204/233]	Loss 0.0161 (0.0166)	
training:	Epoch: [63][205/233]	Loss 0.0131 (0.0165)	
training:	Epoch: [63][206/233]	Loss 0.0219 (0.0166)	
training:	Epoch: [63][207/233]	Loss 0.0138 (0.0166)	
training:	Epoch: [63][208/233]	Loss 0.0160 (0.0166)	
training:	Epoch: [63][209/233]	Loss 0.0134 (0.0165)	
training:	Epoch: [63][210/233]	Loss 0.0113 (0.0165)	
training:	Epoch: [63][211/233]	Loss 0.0165 (0.0165)	
training:	Epoch: [63][212/233]	Loss 0.0136 (0.0165)	
training:	Epoch: [63][213/233]	Loss 0.0185 (0.0165)	
training:	Epoch: [63][214/233]	Loss 0.0141 (0.0165)	
training:	Epoch: [63][215/233]	Loss 0.0124 (0.0165)	
training:	Epoch: [63][216/233]	Loss 0.0214 (0.0165)	
training:	Epoch: [63][217/233]	Loss 0.0160 (0.0165)	
training:	Epoch: [63][218/233]	Loss 0.0162 (0.0165)	
training:	Epoch: [63][219/233]	Loss 0.0120 (0.0165)	
training:	Epoch: [63][220/233]	Loss 0.0153 (0.0165)	
training:	Epoch: [63][221/233]	Loss 0.0139 (0.0165)	
training:	Epoch: [63][222/233]	Loss 0.0137 (0.0164)	
training:	Epoch: [63][223/233]	Loss 0.0157 (0.0164)	
training:	Epoch: [63][224/233]	Loss 0.0169 (0.0164)	
training:	Epoch: [63][225/233]	Loss 0.0501 (0.0166)	
training:	Epoch: [63][226/233]	Loss 0.0123 (0.0166)	
training:	Epoch: [63][227/233]	Loss 0.0254 (0.0166)	
training:	Epoch: [63][228/233]	Loss 0.0131 (0.0166)	
training:	Epoch: [63][229/233]	Loss 0.0124 (0.0166)	
training:	Epoch: [63][230/233]	Loss 0.0125 (0.0166)	
training:	Epoch: [63][231/233]	Loss 0.0177 (0.0166)	
training:	Epoch: [63][232/233]	Loss 0.0111 (0.0165)	
training:	Epoch: [63][233/233]	Loss 0.0132 (0.0165)	
Training:	 Loss: 0.0165

Training:	 ACC: 1.0000 1.0000 1.0000 1.0000
Validation:	 ACC: 0.8645 0.8636 0.8434 0.8857
Validation:	 Best_BACC: 0.8682 0.8678 0.8598 0.8767
Validation:	 Loss: 0.3270
Pretraining:	Epoch 64/200
----------
training:	Epoch: [64][1/233]	Loss 0.0135 (0.0135)	
training:	Epoch: [64][2/233]	Loss 0.0122 (0.0129)	
training:	Epoch: [64][3/233]	Loss 0.0255 (0.0171)	
training:	Epoch: [64][4/233]	Loss 0.0437 (0.0237)	
training:	Epoch: [64][5/233]	Loss 0.0121 (0.0214)	
training:	Epoch: [64][6/233]	Loss 0.0166 (0.0206)	
training:	Epoch: [64][7/233]	Loss 0.0249 (0.0212)	
training:	Epoch: [64][8/233]	Loss 0.0124 (0.0201)	
training:	Epoch: [64][9/233]	Loss 0.0129 (0.0193)	
training:	Epoch: [64][10/233]	Loss 0.0126 (0.0186)	
training:	Epoch: [64][11/233]	Loss 0.0141 (0.0182)	
training:	Epoch: [64][12/233]	Loss 0.0377 (0.0198)	
training:	Epoch: [64][13/233]	Loss 0.0139 (0.0194)	
training:	Epoch: [64][14/233]	Loss 0.0153 (0.0191)	
training:	Epoch: [64][15/233]	Loss 0.0121 (0.0186)	
training:	Epoch: [64][16/233]	Loss 0.0177 (0.0186)	
training:	Epoch: [64][17/233]	Loss 0.0145 (0.0183)	
training:	Epoch: [64][18/233]	Loss 0.0137 (0.0181)	
training:	Epoch: [64][19/233]	Loss 0.0233 (0.0184)	
training:	Epoch: [64][20/233]	Loss 0.0178 (0.0183)	
training:	Epoch: [64][21/233]	Loss 0.0116 (0.0180)	
training:	Epoch: [64][22/233]	Loss 0.0133 (0.0178)	
training:	Epoch: [64][23/233]	Loss 0.0143 (0.0176)	
training:	Epoch: [64][24/233]	Loss 0.0157 (0.0176)	
training:	Epoch: [64][25/233]	Loss 0.0124 (0.0174)	
training:	Epoch: [64][26/233]	Loss 0.0167 (0.0173)	
training:	Epoch: [64][27/233]	Loss 0.0112 (0.0171)	
training:	Epoch: [64][28/233]	Loss 0.0124 (0.0169)	
training:	Epoch: [64][29/233]	Loss 0.0132 (0.0168)	
training:	Epoch: [64][30/233]	Loss 0.0183 (0.0169)	
training:	Epoch: [64][31/233]	Loss 0.0341 (0.0174)	
training:	Epoch: [64][32/233]	Loss 0.0162 (0.0174)	
training:	Epoch: [64][33/233]	Loss 0.0168 (0.0174)	
training:	Epoch: [64][34/233]	Loss 0.0149 (0.0173)	
training:	Epoch: [64][35/233]	Loss 0.0119 (0.0171)	
training:	Epoch: [64][36/233]	Loss 0.0119 (0.0170)	
training:	Epoch: [64][37/233]	Loss 0.0211 (0.0171)	
training:	Epoch: [64][38/233]	Loss 0.0126 (0.0170)	
training:	Epoch: [64][39/233]	Loss 0.0256 (0.0172)	
training:	Epoch: [64][40/233]	Loss 0.0200 (0.0173)	
training:	Epoch: [64][41/233]	Loss 0.0367 (0.0177)	
training:	Epoch: [64][42/233]	Loss 0.0160 (0.0177)	
training:	Epoch: [64][43/233]	Loss 0.0208 (0.0178)	
training:	Epoch: [64][44/233]	Loss 0.0155 (0.0177)	
training:	Epoch: [64][45/233]	Loss 0.0129 (0.0176)	
training:	Epoch: [64][46/233]	Loss 0.0176 (0.0176)	
training:	Epoch: [64][47/233]	Loss 0.0176 (0.0176)	
training:	Epoch: [64][48/233]	Loss 0.0131 (0.0175)	
training:	Epoch: [64][49/233]	Loss 0.0121 (0.0174)	
training:	Epoch: [64][50/233]	Loss 0.0149 (0.0174)	
training:	Epoch: [64][51/233]	Loss 0.0255 (0.0175)	
training:	Epoch: [64][52/233]	Loss 0.0131 (0.0174)	
training:	Epoch: [64][53/233]	Loss 0.0117 (0.0173)	
training:	Epoch: [64][54/233]	Loss 0.0120 (0.0172)	
training:	Epoch: [64][55/233]	Loss 0.0153 (0.0172)	
training:	Epoch: [64][56/233]	Loss 0.0132 (0.0171)	
training:	Epoch: [64][57/233]	Loss 0.0249 (0.0173)	
training:	Epoch: [64][58/233]	Loss 0.0117 (0.0172)	
training:	Epoch: [64][59/233]	Loss 0.0131 (0.0171)	
training:	Epoch: [64][60/233]	Loss 0.0178 (0.0171)	
training:	Epoch: [64][61/233]	Loss 0.0199 (0.0172)	
training:	Epoch: [64][62/233]	Loss 0.0107 (0.0170)	
training:	Epoch: [64][63/233]	Loss 0.0178 (0.0171)	
training:	Epoch: [64][64/233]	Loss 0.0134 (0.0170)	
training:	Epoch: [64][65/233]	Loss 0.0115 (0.0169)	
training:	Epoch: [64][66/233]	Loss 0.0144 (0.0169)	
training:	Epoch: [64][67/233]	Loss 0.0180 (0.0169)	
training:	Epoch: [64][68/233]	Loss 0.0141 (0.0169)	
training:	Epoch: [64][69/233]	Loss 0.0177 (0.0169)	
training:	Epoch: [64][70/233]	Loss 0.0147 (0.0168)	
training:	Epoch: [64][71/233]	Loss 0.0217 (0.0169)	
training:	Epoch: [64][72/233]	Loss 0.0162 (0.0169)	
training:	Epoch: [64][73/233]	Loss 0.0125 (0.0168)	
training:	Epoch: [64][74/233]	Loss 0.0154 (0.0168)	
training:	Epoch: [64][75/233]	Loss 0.0124 (0.0168)	
training:	Epoch: [64][76/233]	Loss 0.0150 (0.0167)	
training:	Epoch: [64][77/233]	Loss 0.0123 (0.0167)	
training:	Epoch: [64][78/233]	Loss 0.0132 (0.0166)	
training:	Epoch: [64][79/233]	Loss 0.0235 (0.0167)	
training:	Epoch: [64][80/233]	Loss 0.0151 (0.0167)	
training:	Epoch: [64][81/233]	Loss 0.0116 (0.0166)	
training:	Epoch: [64][82/233]	Loss 0.0176 (0.0166)	
training:	Epoch: [64][83/233]	Loss 0.0295 (0.0168)	
training:	Epoch: [64][84/233]	Loss 0.0131 (0.0168)	
training:	Epoch: [64][85/233]	Loss 0.0129 (0.0167)	
training:	Epoch: [64][86/233]	Loss 0.0141 (0.0167)	
training:	Epoch: [64][87/233]	Loss 0.0216 (0.0167)	
training:	Epoch: [64][88/233]	Loss 0.0205 (0.0168)	
training:	Epoch: [64][89/233]	Loss 0.0328 (0.0170)	
training:	Epoch: [64][90/233]	Loss 0.0125 (0.0169)	
training:	Epoch: [64][91/233]	Loss 0.0121 (0.0169)	
training:	Epoch: [64][92/233]	Loss 0.0126 (0.0168)	
training:	Epoch: [64][93/233]	Loss 0.0114 (0.0168)	
training:	Epoch: [64][94/233]	Loss 0.0183 (0.0168)	
training:	Epoch: [64][95/233]	Loss 0.0133 (0.0167)	
training:	Epoch: [64][96/233]	Loss 0.0125 (0.0167)	
training:	Epoch: [64][97/233]	Loss 0.0108 (0.0166)	
training:	Epoch: [64][98/233]	Loss 0.0171 (0.0166)	
training:	Epoch: [64][99/233]	Loss 0.0156 (0.0166)	
training:	Epoch: [64][100/233]	Loss 0.0108 (0.0166)	
training:	Epoch: [64][101/233]	Loss 0.0129 (0.0165)	
training:	Epoch: [64][102/233]	Loss 0.0163 (0.0165)	
training:	Epoch: [64][103/233]	Loss 0.0121 (0.0165)	
training:	Epoch: [64][104/233]	Loss 0.0182 (0.0165)	
training:	Epoch: [64][105/233]	Loss 0.0126 (0.0165)	
training:	Epoch: [64][106/233]	Loss 0.0148 (0.0164)	
training:	Epoch: [64][107/233]	Loss 0.0176 (0.0165)	
training:	Epoch: [64][108/233]	Loss 0.0195 (0.0165)	
training:	Epoch: [64][109/233]	Loss 0.0124 (0.0164)	
training:	Epoch: [64][110/233]	Loss 0.0115 (0.0164)	
training:	Epoch: [64][111/233]	Loss 0.0110 (0.0164)	
training:	Epoch: [64][112/233]	Loss 0.0160 (0.0163)	
training:	Epoch: [64][113/233]	Loss 0.0123 (0.0163)	
training:	Epoch: [64][114/233]	Loss 0.0154 (0.0163)	
training:	Epoch: [64][115/233]	Loss 0.0131 (0.0163)	
training:	Epoch: [64][116/233]	Loss 0.0126 (0.0162)	
training:	Epoch: [64][117/233]	Loss 0.0214 (0.0163)	
training:	Epoch: [64][118/233]	Loss 0.0114 (0.0162)	
training:	Epoch: [64][119/233]	Loss 0.0157 (0.0162)	
training:	Epoch: [64][120/233]	Loss 0.0138 (0.0162)	
training:	Epoch: [64][121/233]	Loss 0.0185 (0.0162)	
training:	Epoch: [64][122/233]	Loss 0.0120 (0.0162)	
training:	Epoch: [64][123/233]	Loss 0.0144 (0.0162)	
training:	Epoch: [64][124/233]	Loss 0.0127 (0.0162)	
training:	Epoch: [64][125/233]	Loss 0.0115 (0.0161)	
training:	Epoch: [64][126/233]	Loss 0.0255 (0.0162)	
training:	Epoch: [64][127/233]	Loss 0.0149 (0.0162)	
training:	Epoch: [64][128/233]	Loss 0.0178 (0.0162)	
training:	Epoch: [64][129/233]	Loss 0.0113 (0.0162)	
training:	Epoch: [64][130/233]	Loss 0.0126 (0.0161)	
training:	Epoch: [64][131/233]	Loss 0.0120 (0.0161)	
training:	Epoch: [64][132/233]	Loss 0.0126 (0.0161)	
training:	Epoch: [64][133/233]	Loss 0.0192 (0.0161)	
training:	Epoch: [64][134/233]	Loss 0.0154 (0.0161)	
training:	Epoch: [64][135/233]	Loss 0.0160 (0.0161)	
training:	Epoch: [64][136/233]	Loss 0.0134 (0.0161)	
training:	Epoch: [64][137/233]	Loss 0.0127 (0.0161)	
training:	Epoch: [64][138/233]	Loss 0.0145 (0.0160)	
training:	Epoch: [64][139/233]	Loss 0.0125 (0.0160)	
training:	Epoch: [64][140/233]	Loss 0.0145 (0.0160)	
training:	Epoch: [64][141/233]	Loss 0.0134 (0.0160)	
training:	Epoch: [64][142/233]	Loss 0.0132 (0.0160)	
training:	Epoch: [64][143/233]	Loss 0.0205 (0.0160)	
training:	Epoch: [64][144/233]	Loss 0.0300 (0.0161)	
training:	Epoch: [64][145/233]	Loss 0.0115 (0.0161)	
training:	Epoch: [64][146/233]	Loss 0.0131 (0.0160)	
training:	Epoch: [64][147/233]	Loss 0.0138 (0.0160)	
training:	Epoch: [64][148/233]	Loss 0.0149 (0.0160)	
training:	Epoch: [64][149/233]	Loss 0.0121 (0.0160)	
training:	Epoch: [64][150/233]	Loss 0.0129 (0.0160)	
training:	Epoch: [64][151/233]	Loss 0.0182 (0.0160)	
training:	Epoch: [64][152/233]	Loss 0.0128 (0.0160)	
training:	Epoch: [64][153/233]	Loss 0.0144 (0.0160)	
training:	Epoch: [64][154/233]	Loss 0.0174 (0.0160)	
training:	Epoch: [64][155/233]	Loss 0.0104 (0.0159)	
training:	Epoch: [64][156/233]	Loss 0.0300 (0.0160)	
training:	Epoch: [64][157/233]	Loss 0.0161 (0.0160)	
training:	Epoch: [64][158/233]	Loss 0.0133 (0.0160)	
training:	Epoch: [64][159/233]	Loss 0.0134 (0.0160)	
training:	Epoch: [64][160/233]	Loss 0.0131 (0.0160)	
training:	Epoch: [64][161/233]	Loss 0.0124 (0.0159)	
training:	Epoch: [64][162/233]	Loss 0.0111 (0.0159)	
training:	Epoch: [64][163/233]	Loss 0.0175 (0.0159)	
training:	Epoch: [64][164/233]	Loss 0.0340 (0.0160)	
training:	Epoch: [64][165/233]	Loss 0.0128 (0.0160)	
training:	Epoch: [64][166/233]	Loss 0.0125 (0.0160)	
training:	Epoch: [64][167/233]	Loss 0.0130 (0.0160)	
training:	Epoch: [64][168/233]	Loss 0.0139 (0.0160)	
training:	Epoch: [64][169/233]	Loss 0.0127 (0.0159)	
training:	Epoch: [64][170/233]	Loss 0.0128 (0.0159)	
training:	Epoch: [64][171/233]	Loss 0.0130 (0.0159)	
training:	Epoch: [64][172/233]	Loss 0.0117 (0.0159)	
training:	Epoch: [64][173/233]	Loss 0.0122 (0.0159)	
training:	Epoch: [64][174/233]	Loss 0.0174 (0.0159)	
training:	Epoch: [64][175/233]	Loss 0.0196 (0.0159)	
training:	Epoch: [64][176/233]	Loss 0.0156 (0.0159)	
training:	Epoch: [64][177/233]	Loss 0.0117 (0.0159)	
training:	Epoch: [64][178/233]	Loss 0.0125 (0.0159)	
training:	Epoch: [64][179/233]	Loss 0.0173 (0.0159)	
training:	Epoch: [64][180/233]	Loss 0.0113 (0.0158)	
training:	Epoch: [64][181/233]	Loss 0.0135 (0.0158)	
training:	Epoch: [64][182/233]	Loss 0.0117 (0.0158)	
training:	Epoch: [64][183/233]	Loss 0.0119 (0.0158)	
training:	Epoch: [64][184/233]	Loss 0.0128 (0.0158)	
training:	Epoch: [64][185/233]	Loss 0.0187 (0.0158)	
training:	Epoch: [64][186/233]	Loss 0.0134 (0.0158)	
training:	Epoch: [64][187/233]	Loss 0.0120 (0.0157)	
training:	Epoch: [64][188/233]	Loss 0.0129 (0.0157)	
training:	Epoch: [64][189/233]	Loss 0.0170 (0.0157)	
training:	Epoch: [64][190/233]	Loss 0.0143 (0.0157)	
training:	Epoch: [64][191/233]	Loss 0.0146 (0.0157)	
training:	Epoch: [64][192/233]	Loss 0.0152 (0.0157)	
training:	Epoch: [64][193/233]	Loss 0.0141 (0.0157)	
training:	Epoch: [64][194/233]	Loss 0.0212 (0.0157)	
training:	Epoch: [64][195/233]	Loss 0.0117 (0.0157)	
training:	Epoch: [64][196/233]	Loss 0.0174 (0.0157)	
training:	Epoch: [64][197/233]	Loss 0.0131 (0.0157)	
training:	Epoch: [64][198/233]	Loss 0.0115 (0.0157)	
training:	Epoch: [64][199/233]	Loss 0.0172 (0.0157)	
training:	Epoch: [64][200/233]	Loss 0.0141 (0.0157)	
training:	Epoch: [64][201/233]	Loss 0.0292 (0.0158)	
training:	Epoch: [64][202/233]	Loss 0.0119 (0.0157)	
training:	Epoch: [64][203/233]	Loss 0.0134 (0.0157)	
training:	Epoch: [64][204/233]	Loss 0.0146 (0.0157)	
training:	Epoch: [64][205/233]	Loss 0.0122 (0.0157)	
training:	Epoch: [64][206/233]	Loss 0.0198 (0.0157)	
training:	Epoch: [64][207/233]	Loss 0.0135 (0.0157)	
training:	Epoch: [64][208/233]	Loss 0.0125 (0.0157)	
training:	Epoch: [64][209/233]	Loss 0.0116 (0.0157)	
training:	Epoch: [64][210/233]	Loss 0.0145 (0.0157)	
training:	Epoch: [64][211/233]	Loss 0.0145 (0.0157)	
training:	Epoch: [64][212/233]	Loss 0.0229 (0.0157)	
training:	Epoch: [64][213/233]	Loss 0.0129 (0.0157)	
training:	Epoch: [64][214/233]	Loss 0.0267 (0.0157)	
training:	Epoch: [64][215/233]	Loss 0.0127 (0.0157)	
training:	Epoch: [64][216/233]	Loss 0.0118 (0.0157)	
training:	Epoch: [64][217/233]	Loss 0.0177 (0.0157)	
training:	Epoch: [64][218/233]	Loss 0.0143 (0.0157)	
training:	Epoch: [64][219/233]	Loss 0.0139 (0.0157)	
training:	Epoch: [64][220/233]	Loss 0.0149 (0.0157)	
training:	Epoch: [64][221/233]	Loss 0.0117 (0.0157)	
training:	Epoch: [64][222/233]	Loss 0.0108 (0.0157)	
training:	Epoch: [64][223/233]	Loss 0.0152 (0.0157)	
training:	Epoch: [64][224/233]	Loss 0.0128 (0.0156)	
training:	Epoch: [64][225/233]	Loss 0.0119 (0.0156)	
training:	Epoch: [64][226/233]	Loss 0.0149 (0.0156)	
training:	Epoch: [64][227/233]	Loss 0.0200 (0.0156)	
training:	Epoch: [64][228/233]	Loss 0.0245 (0.0157)	
training:	Epoch: [64][229/233]	Loss 0.0137 (0.0157)	
training:	Epoch: [64][230/233]	Loss 0.0148 (0.0157)	
training:	Epoch: [64][231/233]	Loss 0.0168 (0.0157)	
training:	Epoch: [64][232/233]	Loss 0.0125 (0.0157)	
training:	Epoch: [64][233/233]	Loss 0.0146 (0.0157)	
Training:	 Loss: 0.0156

Training:	 ACC: 1.0000 1.0000 1.0000 1.0000
Validation:	 ACC: 0.8612 0.8604 0.8424 0.8800
Validation:	 Best_BACC: 0.8682 0.8678 0.8598 0.8767
Validation:	 Loss: 0.3258
Pretraining:	Epoch 65/200
----------
training:	Epoch: [65][1/233]	Loss 0.0140 (0.0140)	
training:	Epoch: [65][2/233]	Loss 0.0121 (0.0130)	
training:	Epoch: [65][3/233]	Loss 0.0175 (0.0145)	
training:	Epoch: [65][4/233]	Loss 0.0122 (0.0140)	
training:	Epoch: [65][5/233]	Loss 0.0119 (0.0135)	
training:	Epoch: [65][6/233]	Loss 0.0129 (0.0134)	
training:	Epoch: [65][7/233]	Loss 0.0120 (0.0132)	
training:	Epoch: [65][8/233]	Loss 0.0107 (0.0129)	
training:	Epoch: [65][9/233]	Loss 0.0120 (0.0128)	
training:	Epoch: [65][10/233]	Loss 0.0164 (0.0132)	
training:	Epoch: [65][11/233]	Loss 0.0255 (0.0143)	
training:	Epoch: [65][12/233]	Loss 0.0216 (0.0149)	
training:	Epoch: [65][13/233]	Loss 0.0195 (0.0153)	
training:	Epoch: [65][14/233]	Loss 0.0105 (0.0149)	
training:	Epoch: [65][15/233]	Loss 0.0160 (0.0150)	
training:	Epoch: [65][16/233]	Loss 0.0110 (0.0147)	
training:	Epoch: [65][17/233]	Loss 0.0136 (0.0147)	
training:	Epoch: [65][18/233]	Loss 0.0120 (0.0145)	
training:	Epoch: [65][19/233]	Loss 0.0136 (0.0145)	
training:	Epoch: [65][20/233]	Loss 0.0136 (0.0144)	
training:	Epoch: [65][21/233]	Loss 0.0215 (0.0148)	
training:	Epoch: [65][22/233]	Loss 0.0131 (0.0147)	
training:	Epoch: [65][23/233]	Loss 0.0223 (0.0150)	
training:	Epoch: [65][24/233]	Loss 0.0149 (0.0150)	
training:	Epoch: [65][25/233]	Loss 0.0120 (0.0149)	
training:	Epoch: [65][26/233]	Loss 0.0127 (0.0148)	
training:	Epoch: [65][27/233]	Loss 0.0134 (0.0148)	
training:	Epoch: [65][28/233]	Loss 0.0156 (0.0148)	
training:	Epoch: [65][29/233]	Loss 0.0113 (0.0147)	
training:	Epoch: [65][30/233]	Loss 0.0203 (0.0149)	
training:	Epoch: [65][31/233]	Loss 0.0144 (0.0148)	
training:	Epoch: [65][32/233]	Loss 0.0155 (0.0149)	
training:	Epoch: [65][33/233]	Loss 0.0130 (0.0148)	
training:	Epoch: [65][34/233]	Loss 0.0343 (0.0154)	
training:	Epoch: [65][35/233]	Loss 0.0125 (0.0153)	
training:	Epoch: [65][36/233]	Loss 0.0194 (0.0154)	
training:	Epoch: [65][37/233]	Loss 0.0174 (0.0155)	
training:	Epoch: [65][38/233]	Loss 0.0236 (0.0157)	
training:	Epoch: [65][39/233]	Loss 0.0161 (0.0157)	
training:	Epoch: [65][40/233]	Loss 0.0217 (0.0158)	
training:	Epoch: [65][41/233]	Loss 0.0252 (0.0161)	
training:	Epoch: [65][42/233]	Loss 0.0183 (0.0161)	
training:	Epoch: [65][43/233]	Loss 0.0138 (0.0161)	
training:	Epoch: [65][44/233]	Loss 0.0195 (0.0161)	
training:	Epoch: [65][45/233]	Loss 0.0126 (0.0161)	
training:	Epoch: [65][46/233]	Loss 0.0283 (0.0163)	
training:	Epoch: [65][47/233]	Loss 0.0118 (0.0162)	
training:	Epoch: [65][48/233]	Loss 0.0315 (0.0166)	
training:	Epoch: [65][49/233]	Loss 0.0162 (0.0165)	
training:	Epoch: [65][50/233]	Loss 0.0115 (0.0164)	
training:	Epoch: [65][51/233]	Loss 0.0127 (0.0164)	
training:	Epoch: [65][52/233]	Loss 0.0163 (0.0164)	
training:	Epoch: [65][53/233]	Loss 0.0146 (0.0163)	
training:	Epoch: [65][54/233]	Loss 0.0130 (0.0163)	
training:	Epoch: [65][55/233]	Loss 0.0116 (0.0162)	
training:	Epoch: [65][56/233]	Loss 0.0144 (0.0162)	
training:	Epoch: [65][57/233]	Loss 0.0156 (0.0161)	
training:	Epoch: [65][58/233]	Loss 0.0386 (0.0165)	
training:	Epoch: [65][59/233]	Loss 0.0111 (0.0164)	
training:	Epoch: [65][60/233]	Loss 0.0134 (0.0164)	
training:	Epoch: [65][61/233]	Loss 0.0157 (0.0164)	
training:	Epoch: [65][62/233]	Loss 0.0110 (0.0163)	
training:	Epoch: [65][63/233]	Loss 0.0126 (0.0162)	
training:	Epoch: [65][64/233]	Loss 0.0117 (0.0162)	
training:	Epoch: [65][65/233]	Loss 0.0181 (0.0162)	
training:	Epoch: [65][66/233]	Loss 0.0262 (0.0163)	
training:	Epoch: [65][67/233]	Loss 0.0128 (0.0163)	
training:	Epoch: [65][68/233]	Loss 0.0120 (0.0162)	
training:	Epoch: [65][69/233]	Loss 0.0122 (0.0162)	
training:	Epoch: [65][70/233]	Loss 0.0143 (0.0161)	
training:	Epoch: [65][71/233]	Loss 0.0210 (0.0162)	
training:	Epoch: [65][72/233]	Loss 0.0146 (0.0162)	
training:	Epoch: [65][73/233]	Loss 0.0125 (0.0161)	
training:	Epoch: [65][74/233]	Loss 0.0173 (0.0162)	
training:	Epoch: [65][75/233]	Loss 0.0444 (0.0165)	
training:	Epoch: [65][76/233]	Loss 0.0174 (0.0165)	
training:	Epoch: [65][77/233]	Loss 0.0123 (0.0165)	
training:	Epoch: [65][78/233]	Loss 0.0126 (0.0164)	
training:	Epoch: [65][79/233]	Loss 0.0111 (0.0164)	
training:	Epoch: [65][80/233]	Loss 0.0137 (0.0163)	
training:	Epoch: [65][81/233]	Loss 0.0147 (0.0163)	
training:	Epoch: [65][82/233]	Loss 0.0139 (0.0163)	
training:	Epoch: [65][83/233]	Loss 0.0141 (0.0163)	
training:	Epoch: [65][84/233]	Loss 0.0117 (0.0162)	
training:	Epoch: [65][85/233]	Loss 0.0131 (0.0162)	
training:	Epoch: [65][86/233]	Loss 0.0119 (0.0161)	
training:	Epoch: [65][87/233]	Loss 0.0164 (0.0161)	
training:	Epoch: [65][88/233]	Loss 0.0137 (0.0161)	
training:	Epoch: [65][89/233]	Loss 0.0116 (0.0160)	
training:	Epoch: [65][90/233]	Loss 0.0175 (0.0161)	
training:	Epoch: [65][91/233]	Loss 0.0228 (0.0161)	
training:	Epoch: [65][92/233]	Loss 0.0143 (0.0161)	
training:	Epoch: [65][93/233]	Loss 0.0116 (0.0161)	
training:	Epoch: [65][94/233]	Loss 0.0122 (0.0160)	
training:	Epoch: [65][95/233]	Loss 0.0118 (0.0160)	
training:	Epoch: [65][96/233]	Loss 0.0128 (0.0159)	
training:	Epoch: [65][97/233]	Loss 0.0121 (0.0159)	
training:	Epoch: [65][98/233]	Loss 0.0203 (0.0160)	
training:	Epoch: [65][99/233]	Loss 0.0109 (0.0159)	
training:	Epoch: [65][100/233]	Loss 0.0199 (0.0159)	
training:	Epoch: [65][101/233]	Loss 0.0159 (0.0159)	
training:	Epoch: [65][102/233]	Loss 0.0123 (0.0159)	
training:	Epoch: [65][103/233]	Loss 0.0126 (0.0159)	
training:	Epoch: [65][104/233]	Loss 0.0119 (0.0158)	
training:	Epoch: [65][105/233]	Loss 0.0126 (0.0158)	
training:	Epoch: [65][106/233]	Loss 0.0222 (0.0159)	
training:	Epoch: [65][107/233]	Loss 0.0172 (0.0159)	
training:	Epoch: [65][108/233]	Loss 0.0133 (0.0159)	
training:	Epoch: [65][109/233]	Loss 0.0206 (0.0159)	
training:	Epoch: [65][110/233]	Loss 0.0124 (0.0159)	
training:	Epoch: [65][111/233]	Loss 0.0548 (0.0162)	
training:	Epoch: [65][112/233]	Loss 0.0132 (0.0162)	
training:	Epoch: [65][113/233]	Loss 0.0151 (0.0162)	
training:	Epoch: [65][114/233]	Loss 0.0129 (0.0162)	
training:	Epoch: [65][115/233]	Loss 0.0144 (0.0161)	
training:	Epoch: [65][116/233]	Loss 0.0286 (0.0162)	
training:	Epoch: [65][117/233]	Loss 0.0111 (0.0162)	
training:	Epoch: [65][118/233]	Loss 0.0128 (0.0162)	
training:	Epoch: [65][119/233]	Loss 0.0115 (0.0161)	
training:	Epoch: [65][120/233]	Loss 0.0131 (0.0161)	
training:	Epoch: [65][121/233]	Loss 0.0118 (0.0161)	
training:	Epoch: [65][122/233]	Loss 0.0129 (0.0160)	
training:	Epoch: [65][123/233]	Loss 0.0135 (0.0160)	
training:	Epoch: [65][124/233]	Loss 0.0181 (0.0160)	
training:	Epoch: [65][125/233]	Loss 0.0113 (0.0160)	
training:	Epoch: [65][126/233]	Loss 0.0128 (0.0160)	
training:	Epoch: [65][127/233]	Loss 0.0196 (0.0160)	
training:	Epoch: [65][128/233]	Loss 0.0123 (0.0160)	
training:	Epoch: [65][129/233]	Loss 0.0857 (0.0165)	
training:	Epoch: [65][130/233]	Loss 0.0244 (0.0166)	
training:	Epoch: [65][131/233]	Loss 0.0126 (0.0165)	
training:	Epoch: [65][132/233]	Loss 0.0125 (0.0165)	
training:	Epoch: [65][133/233]	Loss 0.0121 (0.0165)	
training:	Epoch: [65][134/233]	Loss 0.0128 (0.0165)	
training:	Epoch: [65][135/233]	Loss 0.0129 (0.0164)	
training:	Epoch: [65][136/233]	Loss 0.0117 (0.0164)	
training:	Epoch: [65][137/233]	Loss 0.0112 (0.0164)	
training:	Epoch: [65][138/233]	Loss 0.0116 (0.0163)	
training:	Epoch: [65][139/233]	Loss 0.0117 (0.0163)	
training:	Epoch: [65][140/233]	Loss 0.0190 (0.0163)	
training:	Epoch: [65][141/233]	Loss 0.0214 (0.0163)	
training:	Epoch: [65][142/233]	Loss 0.0146 (0.0163)	
training:	Epoch: [65][143/233]	Loss 0.0400 (0.0165)	
training:	Epoch: [65][144/233]	Loss 0.0135 (0.0165)	
training:	Epoch: [65][145/233]	Loss 0.0153 (0.0165)	
training:	Epoch: [65][146/233]	Loss 0.0112 (0.0164)	
training:	Epoch: [65][147/233]	Loss 0.0121 (0.0164)	
training:	Epoch: [65][148/233]	Loss 0.0180 (0.0164)	
training:	Epoch: [65][149/233]	Loss 0.0155 (0.0164)	
training:	Epoch: [65][150/233]	Loss 0.0146 (0.0164)	
training:	Epoch: [65][151/233]	Loss 0.0129 (0.0164)	
training:	Epoch: [65][152/233]	Loss 0.0152 (0.0164)	
training:	Epoch: [65][153/233]	Loss 0.0118 (0.0163)	
training:	Epoch: [65][154/233]	Loss 0.0122 (0.0163)	
training:	Epoch: [65][155/233]	Loss 0.0279 (0.0164)	
training:	Epoch: [65][156/233]	Loss 0.0178 (0.0164)	
training:	Epoch: [65][157/233]	Loss 0.0123 (0.0164)	
training:	Epoch: [65][158/233]	Loss 0.0115 (0.0163)	
training:	Epoch: [65][159/233]	Loss 0.0126 (0.0163)	
training:	Epoch: [65][160/233]	Loss 0.0219 (0.0163)	
training:	Epoch: [65][161/233]	Loss 0.0135 (0.0163)	
training:	Epoch: [65][162/233]	Loss 0.0133 (0.0163)	
training:	Epoch: [65][163/233]	Loss 0.0113 (0.0163)	
training:	Epoch: [65][164/233]	Loss 0.0139 (0.0163)	
training:	Epoch: [65][165/233]	Loss 0.0153 (0.0163)	
training:	Epoch: [65][166/233]	Loss 0.0198 (0.0163)	
training:	Epoch: [65][167/233]	Loss 0.0311 (0.0164)	
training:	Epoch: [65][168/233]	Loss 0.0123 (0.0163)	
training:	Epoch: [65][169/233]	Loss 0.0113 (0.0163)	
training:	Epoch: [65][170/233]	Loss 0.0130 (0.0163)	
training:	Epoch: [65][171/233]	Loss 0.0177 (0.0163)	
training:	Epoch: [65][172/233]	Loss 0.0127 (0.0163)	
training:	Epoch: [65][173/233]	Loss 0.0118 (0.0163)	
training:	Epoch: [65][174/233]	Loss 0.0165 (0.0163)	
training:	Epoch: [65][175/233]	Loss 0.0121 (0.0162)	
training:	Epoch: [65][176/233]	Loss 0.0138 (0.0162)	
training:	Epoch: [65][177/233]	Loss 0.0180 (0.0162)	
training:	Epoch: [65][178/233]	Loss 0.0119 (0.0162)	
training:	Epoch: [65][179/233]	Loss 0.0109 (0.0162)	
training:	Epoch: [65][180/233]	Loss 0.0236 (0.0162)	
training:	Epoch: [65][181/233]	Loss 0.0127 (0.0162)	
training:	Epoch: [65][182/233]	Loss 0.0138 (0.0162)	
training:	Epoch: [65][183/233]	Loss 0.0123 (0.0162)	
training:	Epoch: [65][184/233]	Loss 0.0193 (0.0162)	
training:	Epoch: [65][185/233]	Loss 0.0126 (0.0162)	
training:	Epoch: [65][186/233]	Loss 0.0119 (0.0161)	
training:	Epoch: [65][187/233]	Loss 0.0128 (0.0161)	
training:	Epoch: [65][188/233]	Loss 0.0138 (0.0161)	
training:	Epoch: [65][189/233]	Loss 0.0119 (0.0161)	
training:	Epoch: [65][190/233]	Loss 0.0119 (0.0161)	
training:	Epoch: [65][191/233]	Loss 0.0111 (0.0160)	
training:	Epoch: [65][192/233]	Loss 0.0261 (0.0161)	
training:	Epoch: [65][193/233]	Loss 0.0200 (0.0161)	
training:	Epoch: [65][194/233]	Loss 0.0122 (0.0161)	
training:	Epoch: [65][195/233]	Loss 0.0118 (0.0161)	
training:	Epoch: [65][196/233]	Loss 0.0128 (0.0161)	
training:	Epoch: [65][197/233]	Loss 0.0225 (0.0161)	
training:	Epoch: [65][198/233]	Loss 0.0116 (0.0161)	
training:	Epoch: [65][199/233]	Loss 0.0187 (0.0161)	
training:	Epoch: [65][200/233]	Loss 0.0113 (0.0161)	
training:	Epoch: [65][201/233]	Loss 0.0217 (0.0161)	
training:	Epoch: [65][202/233]	Loss 0.0129 (0.0161)	
training:	Epoch: [65][203/233]	Loss 0.0193 (0.0161)	
training:	Epoch: [65][204/233]	Loss 0.0123 (0.0161)	
training:	Epoch: [65][205/233]	Loss 0.0132 (0.0160)	
training:	Epoch: [65][206/233]	Loss 0.0130 (0.0160)	
training:	Epoch: [65][207/233]	Loss 0.0170 (0.0160)	
training:	Epoch: [65][208/233]	Loss 0.0106 (0.0160)	
training:	Epoch: [65][209/233]	Loss 0.0111 (0.0160)	
training:	Epoch: [65][210/233]	Loss 0.0152 (0.0160)	
training:	Epoch: [65][211/233]	Loss 0.0224 (0.0160)	
training:	Epoch: [65][212/233]	Loss 0.0263 (0.0161)	
training:	Epoch: [65][213/233]	Loss 0.0460 (0.0162)	
training:	Epoch: [65][214/233]	Loss 0.0109 (0.0162)	
training:	Epoch: [65][215/233]	Loss 0.0127 (0.0162)	
training:	Epoch: [65][216/233]	Loss 0.0111 (0.0161)	
training:	Epoch: [65][217/233]	Loss 0.0142 (0.0161)	
training:	Epoch: [65][218/233]	Loss 0.0122 (0.0161)	
training:	Epoch: [65][219/233]	Loss 0.0132 (0.0161)	
training:	Epoch: [65][220/233]	Loss 0.0139 (0.0161)	
training:	Epoch: [65][221/233]	Loss 0.0150 (0.0161)	
training:	Epoch: [65][222/233]	Loss 0.0120 (0.0161)	
training:	Epoch: [65][223/233]	Loss 0.0121 (0.0160)	
training:	Epoch: [65][224/233]	Loss 0.0127 (0.0160)	
training:	Epoch: [65][225/233]	Loss 0.0130 (0.0160)	
training:	Epoch: [65][226/233]	Loss 0.0113 (0.0160)	
training:	Epoch: [65][227/233]	Loss 0.0123 (0.0160)	
training:	Epoch: [65][228/233]	Loss 0.0137 (0.0160)	
training:	Epoch: [65][229/233]	Loss 0.0134 (0.0160)	
training:	Epoch: [65][230/233]	Loss 0.0116 (0.0159)	
training:	Epoch: [65][231/233]	Loss 0.0165 (0.0159)	
training:	Epoch: [65][232/233]	Loss 0.0114 (0.0159)	
training:	Epoch: [65][233/233]	Loss 0.0578 (0.0161)	
Training:	 Loss: 0.0161

Training:	 ACC: 1.0000 1.0000 1.0000 1.0000
Validation:	 ACC: 0.8633 0.8620 0.8332 0.8935
Validation:	 Best_BACC: 0.8682 0.8678 0.8598 0.8767
Validation:	 Loss: 0.3304
Pretraining:	Epoch 66/200
----------
training:	Epoch: [66][1/233]	Loss 0.0116 (0.0116)	
training:	Epoch: [66][2/233]	Loss 0.0109 (0.0112)	
training:	Epoch: [66][3/233]	Loss 0.0121 (0.0115)	
training:	Epoch: [66][4/233]	Loss 0.0134 (0.0120)	
training:	Epoch: [66][5/233]	Loss 0.0150 (0.0126)	
training:	Epoch: [66][6/233]	Loss 0.0203 (0.0139)	
training:	Epoch: [66][7/233]	Loss 0.0123 (0.0137)	
training:	Epoch: [66][8/233]	Loss 0.0140 (0.0137)	
training:	Epoch: [66][9/233]	Loss 0.0123 (0.0135)	
training:	Epoch: [66][10/233]	Loss 0.0122 (0.0134)	
training:	Epoch: [66][11/233]	Loss 0.0259 (0.0145)	
training:	Epoch: [66][12/233]	Loss 0.0134 (0.0145)	
training:	Epoch: [66][13/233]	Loss 0.0122 (0.0143)	
training:	Epoch: [66][14/233]	Loss 0.0173 (0.0145)	
training:	Epoch: [66][15/233]	Loss 0.0187 (0.0148)	
training:	Epoch: [66][16/233]	Loss 0.0204 (0.0151)	
training:	Epoch: [66][17/233]	Loss 0.0151 (0.0151)	
training:	Epoch: [66][18/233]	Loss 0.0192 (0.0154)	
training:	Epoch: [66][19/233]	Loss 0.0143 (0.0153)	
training:	Epoch: [66][20/233]	Loss 0.0106 (0.0151)	
training:	Epoch: [66][21/233]	Loss 0.0113 (0.0149)	
training:	Epoch: [66][22/233]	Loss 0.0125 (0.0148)	
training:	Epoch: [66][23/233]	Loss 0.0162 (0.0148)	
training:	Epoch: [66][24/233]	Loss 0.0241 (0.0152)	
training:	Epoch: [66][25/233]	Loss 0.0117 (0.0151)	
training:	Epoch: [66][26/233]	Loss 0.0118 (0.0150)	
training:	Epoch: [66][27/233]	Loss 0.0135 (0.0149)	
training:	Epoch: [66][28/233]	Loss 0.0135 (0.0149)	
training:	Epoch: [66][29/233]	Loss 0.0160 (0.0149)	
training:	Epoch: [66][30/233]	Loss 0.0136 (0.0149)	
training:	Epoch: [66][31/233]	Loss 0.0110 (0.0147)	
training:	Epoch: [66][32/233]	Loss 0.0115 (0.0146)	
training:	Epoch: [66][33/233]	Loss 0.0123 (0.0146)	
training:	Epoch: [66][34/233]	Loss 0.0145 (0.0146)	
training:	Epoch: [66][35/233]	Loss 0.0155 (0.0146)	
training:	Epoch: [66][36/233]	Loss 0.0109 (0.0145)	
training:	Epoch: [66][37/233]	Loss 0.0128 (0.0144)	
training:	Epoch: [66][38/233]	Loss 0.0113 (0.0144)	
training:	Epoch: [66][39/233]	Loss 0.0145 (0.0144)	
training:	Epoch: [66][40/233]	Loss 0.0118 (0.0143)	
training:	Epoch: [66][41/233]	Loss 0.0106 (0.0142)	
training:	Epoch: [66][42/233]	Loss 0.0111 (0.0141)	
training:	Epoch: [66][43/233]	Loss 0.0265 (0.0144)	
training:	Epoch: [66][44/233]	Loss 0.0154 (0.0144)	
training:	Epoch: [66][45/233]	Loss 0.0145 (0.0144)	
training:	Epoch: [66][46/233]	Loss 0.0201 (0.0146)	
training:	Epoch: [66][47/233]	Loss 0.0184 (0.0146)	
training:	Epoch: [66][48/233]	Loss 0.0120 (0.0146)	
training:	Epoch: [66][49/233]	Loss 0.0170 (0.0146)	
training:	Epoch: [66][50/233]	Loss 0.0105 (0.0146)	
training:	Epoch: [66][51/233]	Loss 0.0117 (0.0145)	
training:	Epoch: [66][52/233]	Loss 0.0116 (0.0144)	
training:	Epoch: [66][53/233]	Loss 0.0131 (0.0144)	
training:	Epoch: [66][54/233]	Loss 0.0108 (0.0144)	
training:	Epoch: [66][55/233]	Loss 0.0107 (0.0143)	
training:	Epoch: [66][56/233]	Loss 0.0125 (0.0143)	
training:	Epoch: [66][57/233]	Loss 0.0260 (0.0145)	
training:	Epoch: [66][58/233]	Loss 0.0123 (0.0144)	
training:	Epoch: [66][59/233]	Loss 0.0165 (0.0145)	
training:	Epoch: [66][60/233]	Loss 0.0129 (0.0144)	
training:	Epoch: [66][61/233]	Loss 0.0110 (0.0144)	
training:	Epoch: [66][62/233]	Loss 0.0121 (0.0143)	
training:	Epoch: [66][63/233]	Loss 0.0122 (0.0143)	
training:	Epoch: [66][64/233]	Loss 0.0142 (0.0143)	
training:	Epoch: [66][65/233]	Loss 0.0111 (0.0143)	
training:	Epoch: [66][66/233]	Loss 0.0138 (0.0143)	
training:	Epoch: [66][67/233]	Loss 0.0128 (0.0142)	
training:	Epoch: [66][68/233]	Loss 0.0163 (0.0143)	
training:	Epoch: [66][69/233]	Loss 0.0155 (0.0143)	
training:	Epoch: [66][70/233]	Loss 0.0128 (0.0143)	
training:	Epoch: [66][71/233]	Loss 0.0136 (0.0142)	
training:	Epoch: [66][72/233]	Loss 0.0144 (0.0142)	
training:	Epoch: [66][73/233]	Loss 0.0112 (0.0142)	
training:	Epoch: [66][74/233]	Loss 0.0176 (0.0143)	
training:	Epoch: [66][75/233]	Loss 0.0115 (0.0142)	
training:	Epoch: [66][76/233]	Loss 0.0127 (0.0142)	
training:	Epoch: [66][77/233]	Loss 0.0194 (0.0143)	
training:	Epoch: [66][78/233]	Loss 0.0120 (0.0142)	
training:	Epoch: [66][79/233]	Loss 0.0121 (0.0142)	
training:	Epoch: [66][80/233]	Loss 0.0122 (0.0142)	
training:	Epoch: [66][81/233]	Loss 0.0125 (0.0142)	
training:	Epoch: [66][82/233]	Loss 0.0139 (0.0142)	
training:	Epoch: [66][83/233]	Loss 0.0249 (0.0143)	
training:	Epoch: [66][84/233]	Loss 0.0120 (0.0143)	
training:	Epoch: [66][85/233]	Loss 0.0200 (0.0143)	
training:	Epoch: [66][86/233]	Loss 0.0122 (0.0143)	
training:	Epoch: [66][87/233]	Loss 0.0124 (0.0143)	
training:	Epoch: [66][88/233]	Loss 0.0118 (0.0143)	
training:	Epoch: [66][89/233]	Loss 0.0295 (0.0144)	
training:	Epoch: [66][90/233]	Loss 0.0220 (0.0145)	
training:	Epoch: [66][91/233]	Loss 0.0120 (0.0145)	
training:	Epoch: [66][92/233]	Loss 0.0119 (0.0145)	
training:	Epoch: [66][93/233]	Loss 0.0114 (0.0144)	
training:	Epoch: [66][94/233]	Loss 0.0223 (0.0145)	
training:	Epoch: [66][95/233]	Loss 0.0132 (0.0145)	
training:	Epoch: [66][96/233]	Loss 0.0107 (0.0145)	
training:	Epoch: [66][97/233]	Loss 0.0276 (0.0146)	
training:	Epoch: [66][98/233]	Loss 0.0140 (0.0146)	
training:	Epoch: [66][99/233]	Loss 0.0115 (0.0145)	
training:	Epoch: [66][100/233]	Loss 0.0190 (0.0146)	
training:	Epoch: [66][101/233]	Loss 0.0106 (0.0146)	
training:	Epoch: [66][102/233]	Loss 0.0141 (0.0145)	
training:	Epoch: [66][103/233]	Loss 0.0111 (0.0145)	
training:	Epoch: [66][104/233]	Loss 0.0153 (0.0145)	
training:	Epoch: [66][105/233]	Loss 0.0114 (0.0145)	
training:	Epoch: [66][106/233]	Loss 0.0139 (0.0145)	
training:	Epoch: [66][107/233]	Loss 0.0138 (0.0145)	
training:	Epoch: [66][108/233]	Loss 0.0127 (0.0145)	
training:	Epoch: [66][109/233]	Loss 0.0155 (0.0145)	
training:	Epoch: [66][110/233]	Loss 0.0110 (0.0144)	
training:	Epoch: [66][111/233]	Loss 0.0140 (0.0144)	
training:	Epoch: [66][112/233]	Loss 0.0119 (0.0144)	
training:	Epoch: [66][113/233]	Loss 0.0207 (0.0145)	
training:	Epoch: [66][114/233]	Loss 0.0103 (0.0144)	
training:	Epoch: [66][115/233]	Loss 0.0149 (0.0144)	
training:	Epoch: [66][116/233]	Loss 0.0117 (0.0144)	
training:	Epoch: [66][117/233]	Loss 0.0195 (0.0145)	
training:	Epoch: [66][118/233]	Loss 0.0121 (0.0144)	
training:	Epoch: [66][119/233]	Loss 0.0128 (0.0144)	
training:	Epoch: [66][120/233]	Loss 0.0142 (0.0144)	
training:	Epoch: [66][121/233]	Loss 0.0371 (0.0146)	
training:	Epoch: [66][122/233]	Loss 0.0117 (0.0146)	
training:	Epoch: [66][123/233]	Loss 0.0153 (0.0146)	
training:	Epoch: [66][124/233]	Loss 0.0109 (0.0146)	
training:	Epoch: [66][125/233]	Loss 0.0146 (0.0146)	
training:	Epoch: [66][126/233]	Loss 0.0123 (0.0145)	
training:	Epoch: [66][127/233]	Loss 0.0100 (0.0145)	
training:	Epoch: [66][128/233]	Loss 0.0324 (0.0146)	
training:	Epoch: [66][129/233]	Loss 0.0146 (0.0146)	
training:	Epoch: [66][130/233]	Loss 0.0262 (0.0147)	
training:	Epoch: [66][131/233]	Loss 0.0135 (0.0147)	
training:	Epoch: [66][132/233]	Loss 0.0131 (0.0147)	
training:	Epoch: [66][133/233]	Loss 0.0108 (0.0147)	
training:	Epoch: [66][134/233]	Loss 0.0118 (0.0147)	
training:	Epoch: [66][135/233]	Loss 0.0103 (0.0146)	
training:	Epoch: [66][136/233]	Loss 0.0113 (0.0146)	
training:	Epoch: [66][137/233]	Loss 0.0146 (0.0146)	
training:	Epoch: [66][138/233]	Loss 0.0133 (0.0146)	
training:	Epoch: [66][139/233]	Loss 0.0195 (0.0146)	
training:	Epoch: [66][140/233]	Loss 0.0189 (0.0147)	
training:	Epoch: [66][141/233]	Loss 0.0125 (0.0146)	
training:	Epoch: [66][142/233]	Loss 0.0117 (0.0146)	
training:	Epoch: [66][143/233]	Loss 0.0128 (0.0146)	
training:	Epoch: [66][144/233]	Loss 0.0140 (0.0146)	
training:	Epoch: [66][145/233]	Loss 0.0111 (0.0146)	
training:	Epoch: [66][146/233]	Loss 0.0117 (0.0146)	
training:	Epoch: [66][147/233]	Loss 0.0111 (0.0145)	
training:	Epoch: [66][148/233]	Loss 0.0136 (0.0145)	
training:	Epoch: [66][149/233]	Loss 0.0148 (0.0145)	
training:	Epoch: [66][150/233]	Loss 0.0125 (0.0145)	
training:	Epoch: [66][151/233]	Loss 0.0109 (0.0145)	
training:	Epoch: [66][152/233]	Loss 0.0123 (0.0145)	
training:	Epoch: [66][153/233]	Loss 0.0119 (0.0145)	
training:	Epoch: [66][154/233]	Loss 0.0219 (0.0145)	
training:	Epoch: [66][155/233]	Loss 0.0134 (0.0145)	
training:	Epoch: [66][156/233]	Loss 0.0129 (0.0145)	
training:	Epoch: [66][157/233]	Loss 0.0127 (0.0145)	
training:	Epoch: [66][158/233]	Loss 0.0117 (0.0145)	
training:	Epoch: [66][159/233]	Loss 0.0115 (0.0145)	
training:	Epoch: [66][160/233]	Loss 0.0165 (0.0145)	
training:	Epoch: [66][161/233]	Loss 0.0132 (0.0145)	
training:	Epoch: [66][162/233]	Loss 0.0256 (0.0145)	
training:	Epoch: [66][163/233]	Loss 0.0224 (0.0146)	
training:	Epoch: [66][164/233]	Loss 0.0130 (0.0146)	
training:	Epoch: [66][165/233]	Loss 0.0212 (0.0146)	
training:	Epoch: [66][166/233]	Loss 0.0151 (0.0146)	
training:	Epoch: [66][167/233]	Loss 0.0115 (0.0146)	
training:	Epoch: [66][168/233]	Loss 0.0155 (0.0146)	
training:	Epoch: [66][169/233]	Loss 0.0205 (0.0146)	
training:	Epoch: [66][170/233]	Loss 0.0137 (0.0146)	
training:	Epoch: [66][171/233]	Loss 0.0159 (0.0146)	
training:	Epoch: [66][172/233]	Loss 0.0120 (0.0146)	
training:	Epoch: [66][173/233]	Loss 0.0109 (0.0146)	
training:	Epoch: [66][174/233]	Loss 0.0129 (0.0146)	
training:	Epoch: [66][175/233]	Loss 0.0188 (0.0146)	
training:	Epoch: [66][176/233]	Loss 0.0141 (0.0146)	
training:	Epoch: [66][177/233]	Loss 0.0118 (0.0146)	
training:	Epoch: [66][178/233]	Loss 0.0209 (0.0146)	
training:	Epoch: [66][179/233]	Loss 0.0120 (0.0146)	
training:	Epoch: [66][180/233]	Loss 0.0106 (0.0146)	
training:	Epoch: [66][181/233]	Loss 0.0125 (0.0146)	
training:	Epoch: [66][182/233]	Loss 0.0229 (0.0146)	
training:	Epoch: [66][183/233]	Loss 0.0135 (0.0146)	
training:	Epoch: [66][184/233]	Loss 0.0122 (0.0146)	
training:	Epoch: [66][185/233]	Loss 0.0115 (0.0146)	
training:	Epoch: [66][186/233]	Loss 0.0121 (0.0146)	
training:	Epoch: [66][187/233]	Loss 0.0102 (0.0146)	
training:	Epoch: [66][188/233]	Loss 0.0111 (0.0145)	
training:	Epoch: [66][189/233]	Loss 0.0107 (0.0145)	
training:	Epoch: [66][190/233]	Loss 0.0111 (0.0145)	
training:	Epoch: [66][191/233]	Loss 0.0110 (0.0145)	
training:	Epoch: [66][192/233]	Loss 0.0123 (0.0145)	
training:	Epoch: [66][193/233]	Loss 0.0113 (0.0144)	
training:	Epoch: [66][194/233]	Loss 0.0109 (0.0144)	
training:	Epoch: [66][195/233]	Loss 0.0130 (0.0144)	
training:	Epoch: [66][196/233]	Loss 0.0117 (0.0144)	
training:	Epoch: [66][197/233]	Loss 0.0123 (0.0144)	
training:	Epoch: [66][198/233]	Loss 0.0197 (0.0144)	
training:	Epoch: [66][199/233]	Loss 0.0134 (0.0144)	
training:	Epoch: [66][200/233]	Loss 0.0117 (0.0144)	
training:	Epoch: [66][201/233]	Loss 0.0139 (0.0144)	
training:	Epoch: [66][202/233]	Loss 0.0140 (0.0144)	
training:	Epoch: [66][203/233]	Loss 0.0131 (0.0144)	
training:	Epoch: [66][204/233]	Loss 0.0114 (0.0144)	
training:	Epoch: [66][205/233]	Loss 0.0129 (0.0144)	
training:	Epoch: [66][206/233]	Loss 0.0140 (0.0144)	
training:	Epoch: [66][207/233]	Loss 0.0132 (0.0144)	
training:	Epoch: [66][208/233]	Loss 0.0124 (0.0144)	
training:	Epoch: [66][209/233]	Loss 0.0264 (0.0144)	
training:	Epoch: [66][210/233]	Loss 0.0136 (0.0144)	
training:	Epoch: [66][211/233]	Loss 0.0194 (0.0144)	
training:	Epoch: [66][212/233]	Loss 0.0124 (0.0144)	
training:	Epoch: [66][213/233]	Loss 0.0218 (0.0145)	
training:	Epoch: [66][214/233]	Loss 0.0143 (0.0145)	
training:	Epoch: [66][215/233]	Loss 0.0186 (0.0145)	
training:	Epoch: [66][216/233]	Loss 0.0119 (0.0145)	
training:	Epoch: [66][217/233]	Loss 0.0243 (0.0145)	
training:	Epoch: [66][218/233]	Loss 0.0122 (0.0145)	
training:	Epoch: [66][219/233]	Loss 0.0110 (0.0145)	
training:	Epoch: [66][220/233]	Loss 0.0124 (0.0145)	
training:	Epoch: [66][221/233]	Loss 0.0111 (0.0145)	
training:	Epoch: [66][222/233]	Loss 0.0114 (0.0144)	
training:	Epoch: [66][223/233]	Loss 0.0128 (0.0144)	
training:	Epoch: [66][224/233]	Loss 0.0120 (0.0144)	
training:	Epoch: [66][225/233]	Loss 0.0119 (0.0144)	
training:	Epoch: [66][226/233]	Loss 0.0129 (0.0144)	
training:	Epoch: [66][227/233]	Loss 0.0112 (0.0144)	
training:	Epoch: [66][228/233]	Loss 0.0261 (0.0144)	
training:	Epoch: [66][229/233]	Loss 0.0141 (0.0144)	
training:	Epoch: [66][230/233]	Loss 0.0128 (0.0144)	
training:	Epoch: [66][231/233]	Loss 0.0112 (0.0144)	
training:	Epoch: [66][232/233]	Loss 0.0110 (0.0144)	
training:	Epoch: [66][233/233]	Loss 0.0114 (0.0144)	
Training:	 Loss: 0.0144

Training:	 ACC: 1.0000 1.0000 1.0000 1.0000
Validation:	 ACC: 0.8644 0.8636 0.8454 0.8834
Validation:	 Best_BACC: 0.8682 0.8678 0.8598 0.8767
Validation:	 Loss: 0.3278
Pretraining:	Epoch 67/200
----------
training:	Epoch: [67][1/233]	Loss 0.0126 (0.0126)	
training:	Epoch: [67][2/233]	Loss 0.0133 (0.0130)	
training:	Epoch: [67][3/233]	Loss 0.0125 (0.0128)	
training:	Epoch: [67][4/233]	Loss 0.0430 (0.0204)	
training:	Epoch: [67][5/233]	Loss 0.0192 (0.0201)	
training:	Epoch: [67][6/233]	Loss 0.0104 (0.0185)	
training:	Epoch: [67][7/233]	Loss 0.0141 (0.0179)	
training:	Epoch: [67][8/233]	Loss 0.0176 (0.0178)	
training:	Epoch: [67][9/233]	Loss 0.0098 (0.0169)	
training:	Epoch: [67][10/233]	Loss 0.0105 (0.0163)	
training:	Epoch: [67][11/233]	Loss 0.0182 (0.0165)	
training:	Epoch: [67][12/233]	Loss 0.0112 (0.0160)	
training:	Epoch: [67][13/233]	Loss 0.0131 (0.0158)	
training:	Epoch: [67][14/233]	Loss 0.0121 (0.0155)	
training:	Epoch: [67][15/233]	Loss 0.0119 (0.0153)	
training:	Epoch: [67][16/233]	Loss 0.0117 (0.0151)	
training:	Epoch: [67][17/233]	Loss 0.0120 (0.0149)	
training:	Epoch: [67][18/233]	Loss 0.0149 (0.0149)	
training:	Epoch: [67][19/233]	Loss 0.0110 (0.0147)	
training:	Epoch: [67][20/233]	Loss 0.0243 (0.0152)	
training:	Epoch: [67][21/233]	Loss 0.0112 (0.0150)	
training:	Epoch: [67][22/233]	Loss 0.0232 (0.0154)	
training:	Epoch: [67][23/233]	Loss 0.0118 (0.0152)	
training:	Epoch: [67][24/233]	Loss 0.0163 (0.0152)	
training:	Epoch: [67][25/233]	Loss 0.0132 (0.0152)	
training:	Epoch: [67][26/233]	Loss 0.0116 (0.0150)	
training:	Epoch: [67][27/233]	Loss 0.0114 (0.0149)	
training:	Epoch: [67][28/233]	Loss 0.0187 (0.0150)	
training:	Epoch: [67][29/233]	Loss 0.0130 (0.0150)	
training:	Epoch: [67][30/233]	Loss 0.0128 (0.0149)	
training:	Epoch: [67][31/233]	Loss 0.0105 (0.0147)	
training:	Epoch: [67][32/233]	Loss 0.0134 (0.0147)	
training:	Epoch: [67][33/233]	Loss 0.0098 (0.0146)	
training:	Epoch: [67][34/233]	Loss 0.0209 (0.0147)	
training:	Epoch: [67][35/233]	Loss 0.0193 (0.0149)	
training:	Epoch: [67][36/233]	Loss 0.0130 (0.0148)	
training:	Epoch: [67][37/233]	Loss 0.0155 (0.0148)	
training:	Epoch: [67][38/233]	Loss 0.0200 (0.0150)	
training:	Epoch: [67][39/233]	Loss 0.0121 (0.0149)	
training:	Epoch: [67][40/233]	Loss 0.0123 (0.0148)	
training:	Epoch: [67][41/233]	Loss 0.0147 (0.0148)	
training:	Epoch: [67][42/233]	Loss 0.0126 (0.0148)	
training:	Epoch: [67][43/233]	Loss 0.0174 (0.0148)	
training:	Epoch: [67][44/233]	Loss 0.0100 (0.0147)	
training:	Epoch: [67][45/233]	Loss 0.0103 (0.0146)	
training:	Epoch: [67][46/233]	Loss 0.0123 (0.0146)	
training:	Epoch: [67][47/233]	Loss 0.0115 (0.0145)	
training:	Epoch: [67][48/233]	Loss 0.0120 (0.0145)	
training:	Epoch: [67][49/233]	Loss 0.0159 (0.0145)	
training:	Epoch: [67][50/233]	Loss 0.0106 (0.0144)	
training:	Epoch: [67][51/233]	Loss 0.0119 (0.0144)	
training:	Epoch: [67][52/233]	Loss 0.0106 (0.0143)	
training:	Epoch: [67][53/233]	Loss 0.0144 (0.0143)	
training:	Epoch: [67][54/233]	Loss 0.0126 (0.0143)	
training:	Epoch: [67][55/233]	Loss 0.0127 (0.0142)	
training:	Epoch: [67][56/233]	Loss 0.0128 (0.0142)	
training:	Epoch: [67][57/233]	Loss 0.0116 (0.0142)	
training:	Epoch: [67][58/233]	Loss 0.0106 (0.0141)	
training:	Epoch: [67][59/233]	Loss 0.0109 (0.0141)	
training:	Epoch: [67][60/233]	Loss 0.0104 (0.0140)	
training:	Epoch: [67][61/233]	Loss 0.0141 (0.0140)	
training:	Epoch: [67][62/233]	Loss 0.0136 (0.0140)	
training:	Epoch: [67][63/233]	Loss 0.0104 (0.0139)	
training:	Epoch: [67][64/233]	Loss 0.0125 (0.0139)	
training:	Epoch: [67][65/233]	Loss 0.0135 (0.0139)	
training:	Epoch: [67][66/233]	Loss 0.0107 (0.0138)	
training:	Epoch: [67][67/233]	Loss 0.0118 (0.0138)	
training:	Epoch: [67][68/233]	Loss 0.0125 (0.0138)	
training:	Epoch: [67][69/233]	Loss 0.0108 (0.0138)	
training:	Epoch: [67][70/233]	Loss 0.0163 (0.0138)	
training:	Epoch: [67][71/233]	Loss 0.0116 (0.0138)	
training:	Epoch: [67][72/233]	Loss 0.0125 (0.0137)	
training:	Epoch: [67][73/233]	Loss 0.0146 (0.0138)	
training:	Epoch: [67][74/233]	Loss 0.0143 (0.0138)	
training:	Epoch: [67][75/233]	Loss 0.0118 (0.0137)	
training:	Epoch: [67][76/233]	Loss 0.0294 (0.0139)	
training:	Epoch: [67][77/233]	Loss 0.0135 (0.0139)	
training:	Epoch: [67][78/233]	Loss 0.0118 (0.0139)	
training:	Epoch: [67][79/233]	Loss 0.0242 (0.0140)	
training:	Epoch: [67][80/233]	Loss 0.0123 (0.0140)	
training:	Epoch: [67][81/233]	Loss 0.0250 (0.0142)	
training:	Epoch: [67][82/233]	Loss 0.0388 (0.0145)	
training:	Epoch: [67][83/233]	Loss 0.0139 (0.0144)	
training:	Epoch: [67][84/233]	Loss 0.0110 (0.0144)	
training:	Epoch: [67][85/233]	Loss 0.0118 (0.0144)	
training:	Epoch: [67][86/233]	Loss 0.0202 (0.0144)	
training:	Epoch: [67][87/233]	Loss 0.0134 (0.0144)	
training:	Epoch: [67][88/233]	Loss 0.0127 (0.0144)	
training:	Epoch: [67][89/233]	Loss 0.0134 (0.0144)	
training:	Epoch: [67][90/233]	Loss 0.0109 (0.0144)	
training:	Epoch: [67][91/233]	Loss 0.0127 (0.0143)	
training:	Epoch: [67][92/233]	Loss 0.0244 (0.0144)	
training:	Epoch: [67][93/233]	Loss 0.0163 (0.0145)	
training:	Epoch: [67][94/233]	Loss 0.0192 (0.0145)	
training:	Epoch: [67][95/233]	Loss 0.0121 (0.0145)	
training:	Epoch: [67][96/233]	Loss 0.0106 (0.0145)	
training:	Epoch: [67][97/233]	Loss 0.0126 (0.0144)	
training:	Epoch: [67][98/233]	Loss 0.0109 (0.0144)	
training:	Epoch: [67][99/233]	Loss 0.0153 (0.0144)	
training:	Epoch: [67][100/233]	Loss 0.0121 (0.0144)	
training:	Epoch: [67][101/233]	Loss 0.0108 (0.0144)	
training:	Epoch: [67][102/233]	Loss 0.0211 (0.0144)	
training:	Epoch: [67][103/233]	Loss 0.0148 (0.0144)	
training:	Epoch: [67][104/233]	Loss 0.0117 (0.0144)	
training:	Epoch: [67][105/233]	Loss 0.0105 (0.0144)	
training:	Epoch: [67][106/233]	Loss 0.0095 (0.0143)	
training:	Epoch: [67][107/233]	Loss 0.0147 (0.0143)	
training:	Epoch: [67][108/233]	Loss 0.0102 (0.0143)	
training:	Epoch: [67][109/233]	Loss 0.0111 (0.0142)	
training:	Epoch: [67][110/233]	Loss 0.0133 (0.0142)	
training:	Epoch: [67][111/233]	Loss 0.0108 (0.0142)	
training:	Epoch: [67][112/233]	Loss 0.0306 (0.0144)	
training:	Epoch: [67][113/233]	Loss 0.0241 (0.0144)	
training:	Epoch: [67][114/233]	Loss 0.0120 (0.0144)	
training:	Epoch: [67][115/233]	Loss 0.0185 (0.0145)	
training:	Epoch: [67][116/233]	Loss 0.0233 (0.0145)	
training:	Epoch: [67][117/233]	Loss 0.0129 (0.0145)	
training:	Epoch: [67][118/233]	Loss 0.0106 (0.0145)	
training:	Epoch: [67][119/233]	Loss 0.0169 (0.0145)	
training:	Epoch: [67][120/233]	Loss 0.0144 (0.0145)	
training:	Epoch: [67][121/233]	Loss 0.0237 (0.0146)	
training:	Epoch: [67][122/233]	Loss 0.0108 (0.0145)	
training:	Epoch: [67][123/233]	Loss 0.0274 (0.0147)	
training:	Epoch: [67][124/233]	Loss 0.0190 (0.0147)	
training:	Epoch: [67][125/233]	Loss 0.0101 (0.0147)	
training:	Epoch: [67][126/233]	Loss 0.0114 (0.0146)	
training:	Epoch: [67][127/233]	Loss 0.0121 (0.0146)	
training:	Epoch: [67][128/233]	Loss 0.0143 (0.0146)	
training:	Epoch: [67][129/233]	Loss 0.0132 (0.0146)	
training:	Epoch: [67][130/233]	Loss 0.0115 (0.0146)	
training:	Epoch: [67][131/233]	Loss 0.0161 (0.0146)	
training:	Epoch: [67][132/233]	Loss 0.0140 (0.0146)	
training:	Epoch: [67][133/233]	Loss 0.0132 (0.0146)	
training:	Epoch: [67][134/233]	Loss 0.0128 (0.0146)	
training:	Epoch: [67][135/233]	Loss 0.0111 (0.0145)	
training:	Epoch: [67][136/233]	Loss 0.0161 (0.0145)	
training:	Epoch: [67][137/233]	Loss 0.0107 (0.0145)	
training:	Epoch: [67][138/233]	Loss 0.0130 (0.0145)	
training:	Epoch: [67][139/233]	Loss 0.0147 (0.0145)	
training:	Epoch: [67][140/233]	Loss 0.0104 (0.0145)	
training:	Epoch: [67][141/233]	Loss 0.0186 (0.0145)	
training:	Epoch: [67][142/233]	Loss 0.0191 (0.0145)	
training:	Epoch: [67][143/233]	Loss 0.0112 (0.0145)	
training:	Epoch: [67][144/233]	Loss 0.0121 (0.0145)	
training:	Epoch: [67][145/233]	Loss 0.0118 (0.0145)	
training:	Epoch: [67][146/233]	Loss 0.0154 (0.0145)	
training:	Epoch: [67][147/233]	Loss 0.0099 (0.0144)	
training:	Epoch: [67][148/233]	Loss 0.0115 (0.0144)	
training:	Epoch: [67][149/233]	Loss 0.0150 (0.0144)	
training:	Epoch: [67][150/233]	Loss 0.0134 (0.0144)	
training:	Epoch: [67][151/233]	Loss 0.0114 (0.0144)	
training:	Epoch: [67][152/233]	Loss 0.0145 (0.0144)	
training:	Epoch: [67][153/233]	Loss 0.0114 (0.0144)	
training:	Epoch: [67][154/233]	Loss 0.0157 (0.0144)	
training:	Epoch: [67][155/233]	Loss 0.0115 (0.0144)	
training:	Epoch: [67][156/233]	Loss 0.0144 (0.0144)	
training:	Epoch: [67][157/233]	Loss 0.0131 (0.0144)	
training:	Epoch: [67][158/233]	Loss 0.0116 (0.0144)	
training:	Epoch: [67][159/233]	Loss 0.0106 (0.0143)	
training:	Epoch: [67][160/233]	Loss 0.0124 (0.0143)	
training:	Epoch: [67][161/233]	Loss 0.0108 (0.0143)	
training:	Epoch: [67][162/233]	Loss 0.0116 (0.0143)	
training:	Epoch: [67][163/233]	Loss 0.0166 (0.0143)	
training:	Epoch: [67][164/233]	Loss 0.0202 (0.0143)	
training:	Epoch: [67][165/233]	Loss 0.0109 (0.0143)	
training:	Epoch: [67][166/233]	Loss 0.0281 (0.0144)	
training:	Epoch: [67][167/233]	Loss 0.0156 (0.0144)	
training:	Epoch: [67][168/233]	Loss 0.0108 (0.0144)	
training:	Epoch: [67][169/233]	Loss 0.0110 (0.0144)	
training:	Epoch: [67][170/233]	Loss 0.0121 (0.0143)	
training:	Epoch: [67][171/233]	Loss 0.0114 (0.0143)	
training:	Epoch: [67][172/233]	Loss 0.0134 (0.0143)	
training:	Epoch: [67][173/233]	Loss 0.0287 (0.0144)	
training:	Epoch: [67][174/233]	Loss 0.0112 (0.0144)	
training:	Epoch: [67][175/233]	Loss 0.0114 (0.0144)	
training:	Epoch: [67][176/233]	Loss 0.0114 (0.0144)	
training:	Epoch: [67][177/233]	Loss 0.0107 (0.0143)	
training:	Epoch: [67][178/233]	Loss 0.0189 (0.0144)	
training:	Epoch: [67][179/233]	Loss 0.0103 (0.0143)	
training:	Epoch: [67][180/233]	Loss 0.0130 (0.0143)	
training:	Epoch: [67][181/233]	Loss 0.0105 (0.0143)	
training:	Epoch: [67][182/233]	Loss 0.0112 (0.0143)	
training:	Epoch: [67][183/233]	Loss 0.0141 (0.0143)	
training:	Epoch: [67][184/233]	Loss 0.0108 (0.0143)	
training:	Epoch: [67][185/233]	Loss 0.0117 (0.0143)	
training:	Epoch: [67][186/233]	Loss 0.0119 (0.0142)	
training:	Epoch: [67][187/233]	Loss 0.0167 (0.0143)	
training:	Epoch: [67][188/233]	Loss 0.0136 (0.0143)	
training:	Epoch: [67][189/233]	Loss 0.0109 (0.0142)	
training:	Epoch: [67][190/233]	Loss 0.0110 (0.0142)	
training:	Epoch: [67][191/233]	Loss 0.0107 (0.0142)	
training:	Epoch: [67][192/233]	Loss 0.0118 (0.0142)	
training:	Epoch: [67][193/233]	Loss 0.0110 (0.0142)	
training:	Epoch: [67][194/233]	Loss 0.0161 (0.0142)	
training:	Epoch: [67][195/233]	Loss 0.0142 (0.0142)	
training:	Epoch: [67][196/233]	Loss 0.0137 (0.0142)	
training:	Epoch: [67][197/233]	Loss 0.0155 (0.0142)	
training:	Epoch: [67][198/233]	Loss 0.0106 (0.0142)	
training:	Epoch: [67][199/233]	Loss 0.0106 (0.0141)	
training:	Epoch: [67][200/233]	Loss 0.0113 (0.0141)	
training:	Epoch: [67][201/233]	Loss 0.0137 (0.0141)	
training:	Epoch: [67][202/233]	Loss 0.0132 (0.0141)	
training:	Epoch: [67][203/233]	Loss 0.0124 (0.0141)	
training:	Epoch: [67][204/233]	Loss 0.0158 (0.0141)	
training:	Epoch: [67][205/233]	Loss 0.0104 (0.0141)	
training:	Epoch: [67][206/233]	Loss 0.0120 (0.0141)	
training:	Epoch: [67][207/233]	Loss 0.0158 (0.0141)	
training:	Epoch: [67][208/233]	Loss 0.0120 (0.0141)	
training:	Epoch: [67][209/233]	Loss 0.0121 (0.0141)	
training:	Epoch: [67][210/233]	Loss 0.0134 (0.0141)	
training:	Epoch: [67][211/233]	Loss 0.0132 (0.0141)	
training:	Epoch: [67][212/233]	Loss 0.0111 (0.0141)	
training:	Epoch: [67][213/233]	Loss 0.0162 (0.0141)	
training:	Epoch: [67][214/233]	Loss 0.0122 (0.0141)	
training:	Epoch: [67][215/233]	Loss 0.0120 (0.0141)	
training:	Epoch: [67][216/233]	Loss 0.0117 (0.0140)	
training:	Epoch: [67][217/233]	Loss 0.0113 (0.0140)	
training:	Epoch: [67][218/233]	Loss 0.0111 (0.0140)	
training:	Epoch: [67][219/233]	Loss 0.0121 (0.0140)	
training:	Epoch: [67][220/233]	Loss 0.0115 (0.0140)	
training:	Epoch: [67][221/233]	Loss 0.0148 (0.0140)	
training:	Epoch: [67][222/233]	Loss 0.0107 (0.0140)	
training:	Epoch: [67][223/233]	Loss 0.0102 (0.0140)	
training:	Epoch: [67][224/233]	Loss 0.0186 (0.0140)	
training:	Epoch: [67][225/233]	Loss 0.0180 (0.0140)	
training:	Epoch: [67][226/233]	Loss 0.0129 (0.0140)	
training:	Epoch: [67][227/233]	Loss 0.0198 (0.0140)	
training:	Epoch: [67][228/233]	Loss 0.0172 (0.0140)	
training:	Epoch: [67][229/233]	Loss 0.0115 (0.0140)	
training:	Epoch: [67][230/233]	Loss 0.0135 (0.0140)	
training:	Epoch: [67][231/233]	Loss 0.0125 (0.0140)	
training:	Epoch: [67][232/233]	Loss 0.0126 (0.0140)	
training:	Epoch: [67][233/233]	Loss 0.0111 (0.0140)	
Training:	 Loss: 0.0140

Training:	 ACC: 1.0000 1.0000 1.0000 1.0000
Validation:	 ACC: 0.8635 0.8630 0.8536 0.8733
Validation:	 Best_BACC: 0.8682 0.8678 0.8598 0.8767
Validation:	 Loss: 0.3247
Pretraining:	Epoch 68/200
----------
training:	Epoch: [68][1/233]	Loss 0.0191 (0.0191)	
training:	Epoch: [68][2/233]	Loss 0.0111 (0.0151)	
training:	Epoch: [68][3/233]	Loss 0.0107 (0.0136)	
training:	Epoch: [68][4/233]	Loss 0.0111 (0.0130)	
training:	Epoch: [68][5/233]	Loss 0.0125 (0.0129)	
training:	Epoch: [68][6/233]	Loss 0.0101 (0.0124)	
training:	Epoch: [68][7/233]	Loss 0.0112 (0.0123)	
training:	Epoch: [68][8/233]	Loss 0.0126 (0.0123)	
training:	Epoch: [68][9/233]	Loss 0.0114 (0.0122)	
training:	Epoch: [68][10/233]	Loss 0.0256 (0.0136)	
training:	Epoch: [68][11/233]	Loss 0.0119 (0.0134)	
training:	Epoch: [68][12/233]	Loss 0.0104 (0.0132)	
training:	Epoch: [68][13/233]	Loss 0.0150 (0.0133)	
training:	Epoch: [68][14/233]	Loss 0.0123 (0.0132)	
training:	Epoch: [68][15/233]	Loss 0.0107 (0.0131)	
training:	Epoch: [68][16/233]	Loss 0.0246 (0.0138)	
training:	Epoch: [68][17/233]	Loss 0.0223 (0.0143)	
training:	Epoch: [68][18/233]	Loss 0.0113 (0.0141)	
training:	Epoch: [68][19/233]	Loss 0.0420 (0.0156)	
training:	Epoch: [68][20/233]	Loss 0.0131 (0.0155)	
training:	Epoch: [68][21/233]	Loss 0.0107 (0.0152)	
training:	Epoch: [68][22/233]	Loss 0.0124 (0.0151)	
training:	Epoch: [68][23/233]	Loss 0.0155 (0.0151)	
training:	Epoch: [68][24/233]	Loss 0.0133 (0.0150)	
training:	Epoch: [68][25/233]	Loss 0.0228 (0.0154)	
training:	Epoch: [68][26/233]	Loss 0.0121 (0.0152)	
training:	Epoch: [68][27/233]	Loss 0.0168 (0.0153)	
training:	Epoch: [68][28/233]	Loss 0.0106 (0.0151)	
training:	Epoch: [68][29/233]	Loss 0.0116 (0.0150)	
training:	Epoch: [68][30/233]	Loss 0.0119 (0.0149)	
training:	Epoch: [68][31/233]	Loss 0.0138 (0.0149)	
training:	Epoch: [68][32/233]	Loss 0.0121 (0.0148)	
training:	Epoch: [68][33/233]	Loss 0.0099 (0.0146)	
training:	Epoch: [68][34/233]	Loss 0.0249 (0.0149)	
training:	Epoch: [68][35/233]	Loss 0.0155 (0.0149)	
training:	Epoch: [68][36/233]	Loss 0.0106 (0.0148)	
training:	Epoch: [68][37/233]	Loss 0.0193 (0.0149)	
training:	Epoch: [68][38/233]	Loss 0.0119 (0.0149)	
training:	Epoch: [68][39/233]	Loss 0.0107 (0.0148)	
training:	Epoch: [68][40/233]	Loss 0.0174 (0.0148)	
training:	Epoch: [68][41/233]	Loss 0.0125 (0.0148)	
training:	Epoch: [68][42/233]	Loss 0.0170 (0.0148)	
training:	Epoch: [68][43/233]	Loss 0.0124 (0.0148)	
training:	Epoch: [68][44/233]	Loss 0.0118 (0.0147)	
training:	Epoch: [68][45/233]	Loss 0.0228 (0.0149)	
training:	Epoch: [68][46/233]	Loss 0.0104 (0.0148)	
training:	Epoch: [68][47/233]	Loss 0.0122 (0.0147)	
training:	Epoch: [68][48/233]	Loss 0.0126 (0.0147)	
training:	Epoch: [68][49/233]	Loss 0.0103 (0.0146)	
training:	Epoch: [68][50/233]	Loss 0.0114 (0.0145)	
training:	Epoch: [68][51/233]	Loss 0.0143 (0.0145)	
training:	Epoch: [68][52/233]	Loss 0.0102 (0.0144)	
training:	Epoch: [68][53/233]	Loss 0.0112 (0.0144)	
training:	Epoch: [68][54/233]	Loss 0.0103 (0.0143)	
training:	Epoch: [68][55/233]	Loss 0.0123 (0.0143)	
training:	Epoch: [68][56/233]	Loss 0.0101 (0.0142)	
training:	Epoch: [68][57/233]	Loss 0.0113 (0.0141)	
training:	Epoch: [68][58/233]	Loss 0.0122 (0.0141)	
training:	Epoch: [68][59/233]	Loss 0.0109 (0.0141)	
training:	Epoch: [68][60/233]	Loss 0.0120 (0.0140)	
training:	Epoch: [68][61/233]	Loss 0.0102 (0.0140)	
training:	Epoch: [68][62/233]	Loss 0.0115 (0.0139)	
training:	Epoch: [68][63/233]	Loss 0.0114 (0.0139)	
training:	Epoch: [68][64/233]	Loss 0.0148 (0.0139)	
training:	Epoch: [68][65/233]	Loss 0.0137 (0.0139)	
training:	Epoch: [68][66/233]	Loss 0.0118 (0.0139)	
training:	Epoch: [68][67/233]	Loss 0.0109 (0.0138)	
training:	Epoch: [68][68/233]	Loss 0.0127 (0.0138)	
training:	Epoch: [68][69/233]	Loss 0.0213 (0.0139)	
training:	Epoch: [68][70/233]	Loss 0.0120 (0.0139)	
training:	Epoch: [68][71/233]	Loss 0.0120 (0.0139)	
training:	Epoch: [68][72/233]	Loss 0.0114 (0.0138)	
training:	Epoch: [68][73/233]	Loss 0.0103 (0.0138)	
training:	Epoch: [68][74/233]	Loss 0.0166 (0.0138)	
training:	Epoch: [68][75/233]	Loss 0.0120 (0.0138)	
training:	Epoch: [68][76/233]	Loss 0.0178 (0.0138)	
training:	Epoch: [68][77/233]	Loss 0.0137 (0.0138)	
training:	Epoch: [68][78/233]	Loss 0.0173 (0.0139)	
training:	Epoch: [68][79/233]	Loss 0.0185 (0.0139)	
training:	Epoch: [68][80/233]	Loss 0.0129 (0.0139)	
training:	Epoch: [68][81/233]	Loss 0.0303 (0.0141)	
training:	Epoch: [68][82/233]	Loss 0.0111 (0.0141)	
training:	Epoch: [68][83/233]	Loss 0.0151 (0.0141)	
training:	Epoch: [68][84/233]	Loss 0.0130 (0.0141)	
training:	Epoch: [68][85/233]	Loss 0.0111 (0.0141)	
training:	Epoch: [68][86/233]	Loss 0.0119 (0.0140)	
training:	Epoch: [68][87/233]	Loss 0.0190 (0.0141)	
training:	Epoch: [68][88/233]	Loss 0.0104 (0.0140)	
training:	Epoch: [68][89/233]	Loss 0.0158 (0.0141)	
training:	Epoch: [68][90/233]	Loss 0.0102 (0.0140)	
training:	Epoch: [68][91/233]	Loss 0.0124 (0.0140)	
training:	Epoch: [68][92/233]	Loss 0.0150 (0.0140)	
training:	Epoch: [68][93/233]	Loss 0.0102 (0.0140)	
training:	Epoch: [68][94/233]	Loss 0.0115 (0.0139)	
training:	Epoch: [68][95/233]	Loss 0.0116 (0.0139)	
training:	Epoch: [68][96/233]	Loss 0.0103 (0.0139)	
training:	Epoch: [68][97/233]	Loss 0.0112 (0.0139)	
training:	Epoch: [68][98/233]	Loss 0.0095 (0.0138)	
training:	Epoch: [68][99/233]	Loss 0.0099 (0.0138)	
training:	Epoch: [68][100/233]	Loss 0.0115 (0.0137)	
training:	Epoch: [68][101/233]	Loss 0.0103 (0.0137)	
training:	Epoch: [68][102/233]	Loss 0.0176 (0.0138)	
training:	Epoch: [68][103/233]	Loss 0.0106 (0.0137)	
training:	Epoch: [68][104/233]	Loss 0.0146 (0.0137)	
training:	Epoch: [68][105/233]	Loss 0.0109 (0.0137)	
training:	Epoch: [68][106/233]	Loss 0.0105 (0.0137)	
training:	Epoch: [68][107/233]	Loss 0.0112 (0.0136)	
training:	Epoch: [68][108/233]	Loss 0.0180 (0.0137)	
training:	Epoch: [68][109/233]	Loss 0.0206 (0.0138)	
training:	Epoch: [68][110/233]	Loss 0.0126 (0.0137)	
training:	Epoch: [68][111/233]	Loss 0.0169 (0.0138)	
training:	Epoch: [68][112/233]	Loss 0.0172 (0.0138)	
training:	Epoch: [68][113/233]	Loss 0.0108 (0.0138)	
training:	Epoch: [68][114/233]	Loss 0.0128 (0.0138)	
training:	Epoch: [68][115/233]	Loss 0.0125 (0.0138)	
training:	Epoch: [68][116/233]	Loss 0.0128 (0.0137)	
training:	Epoch: [68][117/233]	Loss 0.0097 (0.0137)	
training:	Epoch: [68][118/233]	Loss 0.0227 (0.0138)	
training:	Epoch: [68][119/233]	Loss 0.0113 (0.0138)	
training:	Epoch: [68][120/233]	Loss 0.0106 (0.0137)	
training:	Epoch: [68][121/233]	Loss 0.0137 (0.0137)	
training:	Epoch: [68][122/233]	Loss 0.0106 (0.0137)	
training:	Epoch: [68][123/233]	Loss 0.0103 (0.0137)	
training:	Epoch: [68][124/233]	Loss 0.0104 (0.0137)	
training:	Epoch: [68][125/233]	Loss 0.0216 (0.0137)	
training:	Epoch: [68][126/233]	Loss 0.0129 (0.0137)	
training:	Epoch: [68][127/233]	Loss 0.0131 (0.0137)	
training:	Epoch: [68][128/233]	Loss 0.0176 (0.0137)	
training:	Epoch: [68][129/233]	Loss 0.0110 (0.0137)	
training:	Epoch: [68][130/233]	Loss 0.0122 (0.0137)	
training:	Epoch: [68][131/233]	Loss 0.0127 (0.0137)	
training:	Epoch: [68][132/233]	Loss 0.0152 (0.0137)	
training:	Epoch: [68][133/233]	Loss 0.0215 (0.0138)	
training:	Epoch: [68][134/233]	Loss 0.0128 (0.0138)	
training:	Epoch: [68][135/233]	Loss 0.0210 (0.0138)	
training:	Epoch: [68][136/233]	Loss 0.0146 (0.0138)	
training:	Epoch: [68][137/233]	Loss 0.0109 (0.0138)	
training:	Epoch: [68][138/233]	Loss 0.0146 (0.0138)	
training:	Epoch: [68][139/233]	Loss 0.0115 (0.0138)	
training:	Epoch: [68][140/233]	Loss 0.0121 (0.0138)	
training:	Epoch: [68][141/233]	Loss 0.0109 (0.0138)	
training:	Epoch: [68][142/233]	Loss 0.0116 (0.0137)	
training:	Epoch: [68][143/233]	Loss 0.0105 (0.0137)	
training:	Epoch: [68][144/233]	Loss 0.0113 (0.0137)	
training:	Epoch: [68][145/233]	Loss 0.0197 (0.0137)	
training:	Epoch: [68][146/233]	Loss 0.0140 (0.0138)	
training:	Epoch: [68][147/233]	Loss 0.0117 (0.0137)	
training:	Epoch: [68][148/233]	Loss 0.0101 (0.0137)	
training:	Epoch: [68][149/233]	Loss 0.0112 (0.0137)	
training:	Epoch: [68][150/233]	Loss 0.0171 (0.0137)	
training:	Epoch: [68][151/233]	Loss 0.0189 (0.0138)	
training:	Epoch: [68][152/233]	Loss 0.0109 (0.0137)	
training:	Epoch: [68][153/233]	Loss 0.0194 (0.0138)	
training:	Epoch: [68][154/233]	Loss 0.0156 (0.0138)	
training:	Epoch: [68][155/233]	Loss 0.0121 (0.0138)	
training:	Epoch: [68][156/233]	Loss 0.0114 (0.0138)	
training:	Epoch: [68][157/233]	Loss 0.0104 (0.0137)	
training:	Epoch: [68][158/233]	Loss 0.0103 (0.0137)	
training:	Epoch: [68][159/233]	Loss 0.0140 (0.0137)	
training:	Epoch: [68][160/233]	Loss 0.0166 (0.0137)	
training:	Epoch: [68][161/233]	Loss 0.0112 (0.0137)	
training:	Epoch: [68][162/233]	Loss 0.0101 (0.0137)	
training:	Epoch: [68][163/233]	Loss 0.0112 (0.0137)	
training:	Epoch: [68][164/233]	Loss 0.0107 (0.0137)	
training:	Epoch: [68][165/233]	Loss 0.0113 (0.0136)	
training:	Epoch: [68][166/233]	Loss 0.0131 (0.0136)	
training:	Epoch: [68][167/233]	Loss 0.0097 (0.0136)	
training:	Epoch: [68][168/233]	Loss 0.0101 (0.0136)	
training:	Epoch: [68][169/233]	Loss 0.0123 (0.0136)	
training:	Epoch: [68][170/233]	Loss 0.0115 (0.0136)	
training:	Epoch: [68][171/233]	Loss 0.0153 (0.0136)	
training:	Epoch: [68][172/233]	Loss 0.0127 (0.0136)	
training:	Epoch: [68][173/233]	Loss 0.0121 (0.0136)	
training:	Epoch: [68][174/233]	Loss 0.0123 (0.0136)	
training:	Epoch: [68][175/233]	Loss 0.0239 (0.0136)	
training:	Epoch: [68][176/233]	Loss 0.0156 (0.0136)	
training:	Epoch: [68][177/233]	Loss 0.0297 (0.0137)	
training:	Epoch: [68][178/233]	Loss 0.0307 (0.0138)	
training:	Epoch: [68][179/233]	Loss 0.0185 (0.0139)	
training:	Epoch: [68][180/233]	Loss 0.0327 (0.0140)	
training:	Epoch: [68][181/233]	Loss 0.0141 (0.0140)	
training:	Epoch: [68][182/233]	Loss 0.0132 (0.0140)	
training:	Epoch: [68][183/233]	Loss 0.0111 (0.0139)	
training:	Epoch: [68][184/233]	Loss 0.0126 (0.0139)	
training:	Epoch: [68][185/233]	Loss 0.0113 (0.0139)	
training:	Epoch: [68][186/233]	Loss 0.0093 (0.0139)	
training:	Epoch: [68][187/233]	Loss 0.0105 (0.0139)	
training:	Epoch: [68][188/233]	Loss 0.0133 (0.0139)	
training:	Epoch: [68][189/233]	Loss 0.0127 (0.0139)	
training:	Epoch: [68][190/233]	Loss 0.0112 (0.0138)	
training:	Epoch: [68][191/233]	Loss 0.0113 (0.0138)	
training:	Epoch: [68][192/233]	Loss 0.0109 (0.0138)	
training:	Epoch: [68][193/233]	Loss 0.0118 (0.0138)	
training:	Epoch: [68][194/233]	Loss 0.0102 (0.0138)	
training:	Epoch: [68][195/233]	Loss 0.0102 (0.0138)	
training:	Epoch: [68][196/233]	Loss 0.0155 (0.0138)	
training:	Epoch: [68][197/233]	Loss 0.0108 (0.0138)	
training:	Epoch: [68][198/233]	Loss 0.0130 (0.0138)	
training:	Epoch: [68][199/233]	Loss 0.0165 (0.0138)	
training:	Epoch: [68][200/233]	Loss 0.0120 (0.0138)	
training:	Epoch: [68][201/233]	Loss 0.0098 (0.0137)	
training:	Epoch: [68][202/233]	Loss 0.0112 (0.0137)	
training:	Epoch: [68][203/233]	Loss 0.0123 (0.0137)	
training:	Epoch: [68][204/233]	Loss 0.0109 (0.0137)	
training:	Epoch: [68][205/233]	Loss 0.0098 (0.0137)	
training:	Epoch: [68][206/233]	Loss 0.0101 (0.0137)	
training:	Epoch: [68][207/233]	Loss 0.0248 (0.0137)	
training:	Epoch: [68][208/233]	Loss 0.0121 (0.0137)	
training:	Epoch: [68][209/233]	Loss 0.0158 (0.0137)	
training:	Epoch: [68][210/233]	Loss 0.0147 (0.0137)	
training:	Epoch: [68][211/233]	Loss 0.0244 (0.0138)	
training:	Epoch: [68][212/233]	Loss 0.0112 (0.0138)	
training:	Epoch: [68][213/233]	Loss 0.0113 (0.0138)	
training:	Epoch: [68][214/233]	Loss 0.0123 (0.0138)	
training:	Epoch: [68][215/233]	Loss 0.0136 (0.0138)	
training:	Epoch: [68][216/233]	Loss 0.0129 (0.0138)	
training:	Epoch: [68][217/233]	Loss 0.0180 (0.0138)	
training:	Epoch: [68][218/233]	Loss 0.0103 (0.0138)	
training:	Epoch: [68][219/233]	Loss 0.0109 (0.0137)	
training:	Epoch: [68][220/233]	Loss 0.0101 (0.0137)	
training:	Epoch: [68][221/233]	Loss 0.0106 (0.0137)	
training:	Epoch: [68][222/233]	Loss 0.0091 (0.0137)	
training:	Epoch: [68][223/233]	Loss 0.0102 (0.0137)	
training:	Epoch: [68][224/233]	Loss 0.0176 (0.0137)	
training:	Epoch: [68][225/233]	Loss 0.0116 (0.0137)	
training:	Epoch: [68][226/233]	Loss 0.0119 (0.0137)	
training:	Epoch: [68][227/233]	Loss 0.0348 (0.0138)	
training:	Epoch: [68][228/233]	Loss 0.0111 (0.0138)	
training:	Epoch: [68][229/233]	Loss 0.0115 (0.0137)	
training:	Epoch: [68][230/233]	Loss 0.0140 (0.0137)	
training:	Epoch: [68][231/233]	Loss 0.0107 (0.0137)	
training:	Epoch: [68][232/233]	Loss 0.0104 (0.0137)	
training:	Epoch: [68][233/233]	Loss 0.0107 (0.0137)	
Training:	 Loss: 0.0137

Training:	 ACC: 1.0000 1.0000 1.0000 1.0000
Validation:	 ACC: 0.8630 0.8641 0.8874 0.8386
Validation:	 Best_BACC: 0.8682 0.8678 0.8598 0.8767
Validation:	 Loss: 0.3258
Pretraining:	Epoch 69/200
----------
training:	Epoch: [69][1/233]	Loss 0.0112 (0.0112)	
training:	Epoch: [69][2/233]	Loss 0.0127 (0.0119)	
training:	Epoch: [69][3/233]	Loss 0.0103 (0.0114)	
training:	Epoch: [69][4/233]	Loss 0.0165 (0.0127)	
training:	Epoch: [69][5/233]	Loss 0.0134 (0.0128)	
training:	Epoch: [69][6/233]	Loss 0.0105 (0.0124)	
training:	Epoch: [69][7/233]	Loss 0.0109 (0.0122)	
training:	Epoch: [69][8/233]	Loss 0.0154 (0.0126)	
training:	Epoch: [69][9/233]	Loss 0.0109 (0.0124)	
training:	Epoch: [69][10/233]	Loss 0.0107 (0.0123)	
training:	Epoch: [69][11/233]	Loss 0.0225 (0.0132)	
training:	Epoch: [69][12/233]	Loss 0.0228 (0.0140)	
training:	Epoch: [69][13/233]	Loss 0.0106 (0.0137)	
training:	Epoch: [69][14/233]	Loss 0.0102 (0.0135)	
training:	Epoch: [69][15/233]	Loss 0.0156 (0.0136)	
training:	Epoch: [69][16/233]	Loss 0.0113 (0.0135)	
training:	Epoch: [69][17/233]	Loss 0.0121 (0.0134)	
training:	Epoch: [69][18/233]	Loss 0.0200 (0.0138)	
training:	Epoch: [69][19/233]	Loss 0.0311 (0.0147)	
training:	Epoch: [69][20/233]	Loss 0.0096 (0.0144)	
training:	Epoch: [69][21/233]	Loss 0.0103 (0.0142)	
training:	Epoch: [69][22/233]	Loss 0.0127 (0.0142)	
training:	Epoch: [69][23/233]	Loss 0.0108 (0.0140)	
training:	Epoch: [69][24/233]	Loss 0.0135 (0.0140)	
training:	Epoch: [69][25/233]	Loss 0.0176 (0.0141)	
training:	Epoch: [69][26/233]	Loss 0.0112 (0.0140)	
training:	Epoch: [69][27/233]	Loss 0.0091 (0.0138)	
training:	Epoch: [69][28/233]	Loss 0.0115 (0.0138)	
training:	Epoch: [69][29/233]	Loss 0.0138 (0.0138)	
training:	Epoch: [69][30/233]	Loss 0.0115 (0.0137)	
training:	Epoch: [69][31/233]	Loss 0.0181 (0.0138)	
training:	Epoch: [69][32/233]	Loss 0.0131 (0.0138)	
training:	Epoch: [69][33/233]	Loss 0.0097 (0.0137)	
training:	Epoch: [69][34/233]	Loss 0.0111 (0.0136)	
training:	Epoch: [69][35/233]	Loss 0.0115 (0.0135)	
training:	Epoch: [69][36/233]	Loss 0.0100 (0.0134)	
training:	Epoch: [69][37/233]	Loss 0.0124 (0.0134)	
training:	Epoch: [69][38/233]	Loss 0.0114 (0.0134)	
training:	Epoch: [69][39/233]	Loss 0.0194 (0.0135)	
training:	Epoch: [69][40/233]	Loss 0.0116 (0.0135)	
training:	Epoch: [69][41/233]	Loss 0.0117 (0.0134)	
training:	Epoch: [69][42/233]	Loss 0.0137 (0.0134)	
training:	Epoch: [69][43/233]	Loss 0.0118 (0.0134)	
training:	Epoch: [69][44/233]	Loss 0.0120 (0.0134)	
training:	Epoch: [69][45/233]	Loss 0.0111 (0.0133)	
training:	Epoch: [69][46/233]	Loss 0.0109 (0.0133)	
training:	Epoch: [69][47/233]	Loss 0.0114 (0.0132)	
training:	Epoch: [69][48/233]	Loss 0.0157 (0.0133)	
training:	Epoch: [69][49/233]	Loss 0.0100 (0.0132)	
training:	Epoch: [69][50/233]	Loss 0.0195 (0.0133)	
training:	Epoch: [69][51/233]	Loss 0.0116 (0.0133)	
training:	Epoch: [69][52/233]	Loss 0.0159 (0.0133)	
training:	Epoch: [69][53/233]	Loss 0.0132 (0.0133)	
training:	Epoch: [69][54/233]	Loss 0.0301 (0.0137)	
training:	Epoch: [69][55/233]	Loss 0.0121 (0.0136)	
training:	Epoch: [69][56/233]	Loss 0.0124 (0.0136)	
training:	Epoch: [69][57/233]	Loss 0.0142 (0.0136)	
training:	Epoch: [69][58/233]	Loss 0.0145 (0.0136)	
training:	Epoch: [69][59/233]	Loss 0.0105 (0.0136)	
training:	Epoch: [69][60/233]	Loss 0.0108 (0.0135)	
training:	Epoch: [69][61/233]	Loss 0.0127 (0.0135)	
training:	Epoch: [69][62/233]	Loss 0.0099 (0.0135)	
training:	Epoch: [69][63/233]	Loss 0.0189 (0.0135)	
training:	Epoch: [69][64/233]	Loss 0.0112 (0.0135)	
training:	Epoch: [69][65/233]	Loss 0.0107 (0.0135)	
training:	Epoch: [69][66/233]	Loss 0.0098 (0.0134)	
training:	Epoch: [69][67/233]	Loss 0.0104 (0.0134)	
training:	Epoch: [69][68/233]	Loss 0.0164 (0.0134)	
training:	Epoch: [69][69/233]	Loss 0.0180 (0.0135)	
training:	Epoch: [69][70/233]	Loss 0.0110 (0.0134)	
training:	Epoch: [69][71/233]	Loss 0.0100 (0.0134)	
training:	Epoch: [69][72/233]	Loss 0.0104 (0.0133)	
training:	Epoch: [69][73/233]	Loss 0.0113 (0.0133)	
training:	Epoch: [69][74/233]	Loss 0.0100 (0.0133)	
training:	Epoch: [69][75/233]	Loss 0.0168 (0.0133)	
training:	Epoch: [69][76/233]	Loss 0.0116 (0.0133)	
training:	Epoch: [69][77/233]	Loss 0.0132 (0.0133)	
training:	Epoch: [69][78/233]	Loss 0.0103 (0.0133)	
training:	Epoch: [69][79/233]	Loss 0.0157 (0.0133)	
training:	Epoch: [69][80/233]	Loss 0.0122 (0.0133)	
training:	Epoch: [69][81/233]	Loss 0.0119 (0.0133)	
training:	Epoch: [69][82/233]	Loss 0.0103 (0.0132)	
training:	Epoch: [69][83/233]	Loss 0.0149 (0.0132)	
training:	Epoch: [69][84/233]	Loss 0.0101 (0.0132)	
training:	Epoch: [69][85/233]	Loss 0.0217 (0.0133)	
training:	Epoch: [69][86/233]	Loss 0.0141 (0.0133)	
training:	Epoch: [69][87/233]	Loss 0.0102 (0.0133)	
training:	Epoch: [69][88/233]	Loss 0.0168 (0.0133)	
training:	Epoch: [69][89/233]	Loss 0.0129 (0.0133)	
training:	Epoch: [69][90/233]	Loss 0.0107 (0.0133)	
training:	Epoch: [69][91/233]	Loss 0.0102 (0.0132)	
training:	Epoch: [69][92/233]	Loss 0.0108 (0.0132)	
training:	Epoch: [69][93/233]	Loss 0.0107 (0.0132)	
training:	Epoch: [69][94/233]	Loss 0.0228 (0.0133)	
training:	Epoch: [69][95/233]	Loss 0.0128 (0.0133)	
training:	Epoch: [69][96/233]	Loss 0.0206 (0.0134)	
training:	Epoch: [69][97/233]	Loss 0.0105 (0.0133)	
training:	Epoch: [69][98/233]	Loss 0.0122 (0.0133)	
training:	Epoch: [69][99/233]	Loss 0.0110 (0.0133)	
training:	Epoch: [69][100/233]	Loss 0.0106 (0.0133)	
training:	Epoch: [69][101/233]	Loss 0.0108 (0.0133)	
training:	Epoch: [69][102/233]	Loss 0.0394 (0.0135)	
training:	Epoch: [69][103/233]	Loss 0.0141 (0.0135)	
training:	Epoch: [69][104/233]	Loss 0.0109 (0.0135)	
training:	Epoch: [69][105/233]	Loss 0.0113 (0.0135)	
training:	Epoch: [69][106/233]	Loss 0.0110 (0.0134)	
training:	Epoch: [69][107/233]	Loss 0.0119 (0.0134)	
training:	Epoch: [69][108/233]	Loss 0.0100 (0.0134)	
training:	Epoch: [69][109/233]	Loss 0.0133 (0.0134)	
training:	Epoch: [69][110/233]	Loss 0.0113 (0.0134)	
training:	Epoch: [69][111/233]	Loss 0.0222 (0.0135)	
training:	Epoch: [69][112/233]	Loss 0.0144 (0.0135)	
training:	Epoch: [69][113/233]	Loss 0.0121 (0.0135)	
training:	Epoch: [69][114/233]	Loss 0.0114 (0.0134)	
training:	Epoch: [69][115/233]	Loss 0.0103 (0.0134)	
training:	Epoch: [69][116/233]	Loss 0.0153 (0.0134)	
training:	Epoch: [69][117/233]	Loss 0.0146 (0.0134)	
training:	Epoch: [69][118/233]	Loss 0.0102 (0.0134)	
training:	Epoch: [69][119/233]	Loss 0.0095 (0.0134)	
training:	Epoch: [69][120/233]	Loss 0.0112 (0.0134)	
training:	Epoch: [69][121/233]	Loss 0.0108 (0.0133)	
training:	Epoch: [69][122/233]	Loss 0.0093 (0.0133)	
training:	Epoch: [69][123/233]	Loss 0.0574 (0.0137)	
training:	Epoch: [69][124/233]	Loss 0.0143 (0.0137)	
training:	Epoch: [69][125/233]	Loss 0.0112 (0.0136)	
training:	Epoch: [69][126/233]	Loss 0.0104 (0.0136)	
training:	Epoch: [69][127/233]	Loss 0.0123 (0.0136)	
training:	Epoch: [69][128/233]	Loss 0.0110 (0.0136)	
training:	Epoch: [69][129/233]	Loss 0.0104 (0.0136)	
training:	Epoch: [69][130/233]	Loss 0.0141 (0.0136)	
training:	Epoch: [69][131/233]	Loss 0.0143 (0.0136)	
training:	Epoch: [69][132/233]	Loss 0.0096 (0.0135)	
training:	Epoch: [69][133/233]	Loss 0.0109 (0.0135)	
training:	Epoch: [69][134/233]	Loss 0.0108 (0.0135)	
training:	Epoch: [69][135/233]	Loss 0.0117 (0.0135)	
training:	Epoch: [69][136/233]	Loss 0.0105 (0.0135)	
training:	Epoch: [69][137/233]	Loss 0.0139 (0.0135)	
training:	Epoch: [69][138/233]	Loss 0.0102 (0.0134)	
training:	Epoch: [69][139/233]	Loss 0.0116 (0.0134)	
training:	Epoch: [69][140/233]	Loss 0.0165 (0.0135)	
training:	Epoch: [69][141/233]	Loss 0.0133 (0.0135)	
training:	Epoch: [69][142/233]	Loss 0.0219 (0.0135)	
training:	Epoch: [69][143/233]	Loss 0.0126 (0.0135)	
training:	Epoch: [69][144/233]	Loss 0.0162 (0.0135)	
training:	Epoch: [69][145/233]	Loss 0.0102 (0.0135)	
training:	Epoch: [69][146/233]	Loss 0.0151 (0.0135)	
training:	Epoch: [69][147/233]	Loss 0.0095 (0.0135)	
training:	Epoch: [69][148/233]	Loss 0.0108 (0.0135)	
training:	Epoch: [69][149/233]	Loss 0.0184 (0.0135)	
training:	Epoch: [69][150/233]	Loss 0.0118 (0.0135)	
training:	Epoch: [69][151/233]	Loss 0.0101 (0.0135)	
training:	Epoch: [69][152/233]	Loss 0.0120 (0.0135)	
training:	Epoch: [69][153/233]	Loss 0.0123 (0.0135)	
training:	Epoch: [69][154/233]	Loss 0.0121 (0.0134)	
training:	Epoch: [69][155/233]	Loss 0.0109 (0.0134)	
training:	Epoch: [69][156/233]	Loss 0.0160 (0.0134)	
training:	Epoch: [69][157/233]	Loss 0.0097 (0.0134)	
training:	Epoch: [69][158/233]	Loss 0.0182 (0.0135)	
training:	Epoch: [69][159/233]	Loss 0.0166 (0.0135)	
training:	Epoch: [69][160/233]	Loss 0.0106 (0.0135)	
training:	Epoch: [69][161/233]	Loss 0.0113 (0.0134)	
training:	Epoch: [69][162/233]	Loss 0.0111 (0.0134)	
training:	Epoch: [69][163/233]	Loss 0.0104 (0.0134)	
training:	Epoch: [69][164/233]	Loss 0.0112 (0.0134)	
training:	Epoch: [69][165/233]	Loss 0.0159 (0.0134)	
training:	Epoch: [69][166/233]	Loss 0.0127 (0.0134)	
training:	Epoch: [69][167/233]	Loss 0.0183 (0.0134)	
training:	Epoch: [69][168/233]	Loss 0.0188 (0.0135)	
training:	Epoch: [69][169/233]	Loss 0.0117 (0.0135)	
training:	Epoch: [69][170/233]	Loss 0.0118 (0.0134)	
training:	Epoch: [69][171/233]	Loss 0.0118 (0.0134)	
training:	Epoch: [69][172/233]	Loss 0.0124 (0.0134)	
training:	Epoch: [69][173/233]	Loss 0.0120 (0.0134)	
training:	Epoch: [69][174/233]	Loss 0.0195 (0.0135)	
training:	Epoch: [69][175/233]	Loss 0.0212 (0.0135)	
training:	Epoch: [69][176/233]	Loss 0.0113 (0.0135)	
training:	Epoch: [69][177/233]	Loss 0.0116 (0.0135)	
training:	Epoch: [69][178/233]	Loss 0.0232 (0.0135)	
training:	Epoch: [69][179/233]	Loss 0.0202 (0.0136)	
training:	Epoch: [69][180/233]	Loss 0.0146 (0.0136)	
training:	Epoch: [69][181/233]	Loss 0.0173 (0.0136)	
training:	Epoch: [69][182/233]	Loss 0.0128 (0.0136)	
training:	Epoch: [69][183/233]	Loss 0.0177 (0.0136)	
training:	Epoch: [69][184/233]	Loss 0.0102 (0.0136)	
training:	Epoch: [69][185/233]	Loss 0.0100 (0.0136)	
training:	Epoch: [69][186/233]	Loss 0.0096 (0.0136)	
training:	Epoch: [69][187/233]	Loss 0.0105 (0.0135)	
training:	Epoch: [69][188/233]	Loss 0.0134 (0.0135)	
training:	Epoch: [69][189/233]	Loss 0.0108 (0.0135)	
training:	Epoch: [69][190/233]	Loss 0.0130 (0.0135)	
training:	Epoch: [69][191/233]	Loss 0.0147 (0.0135)	
training:	Epoch: [69][192/233]	Loss 0.0213 (0.0136)	
training:	Epoch: [69][193/233]	Loss 0.0104 (0.0135)	
training:	Epoch: [69][194/233]	Loss 0.0105 (0.0135)	
training:	Epoch: [69][195/233]	Loss 0.0117 (0.0135)	
training:	Epoch: [69][196/233]	Loss 0.0127 (0.0135)	
training:	Epoch: [69][197/233]	Loss 0.0111 (0.0135)	
training:	Epoch: [69][198/233]	Loss 0.0108 (0.0135)	
training:	Epoch: [69][199/233]	Loss 0.0103 (0.0135)	
training:	Epoch: [69][200/233]	Loss 0.0109 (0.0135)	
training:	Epoch: [69][201/233]	Loss 0.0133 (0.0135)	
training:	Epoch: [69][202/233]	Loss 0.0147 (0.0135)	
training:	Epoch: [69][203/233]	Loss 0.0102 (0.0135)	
training:	Epoch: [69][204/233]	Loss 0.0103 (0.0134)	
training:	Epoch: [69][205/233]	Loss 0.0105 (0.0134)	
training:	Epoch: [69][206/233]	Loss 0.0102 (0.0134)	
training:	Epoch: [69][207/233]	Loss 0.0101 (0.0134)	
training:	Epoch: [69][208/233]	Loss 0.0102 (0.0134)	
training:	Epoch: [69][209/233]	Loss 0.0120 (0.0134)	
training:	Epoch: [69][210/233]	Loss 0.0101 (0.0134)	
training:	Epoch: [69][211/233]	Loss 0.0097 (0.0133)	
training:	Epoch: [69][212/233]	Loss 0.0107 (0.0133)	
training:	Epoch: [69][213/233]	Loss 0.0105 (0.0133)	
training:	Epoch: [69][214/233]	Loss 0.0109 (0.0133)	
training:	Epoch: [69][215/233]	Loss 0.0117 (0.0133)	
training:	Epoch: [69][216/233]	Loss 0.0100 (0.0133)	
training:	Epoch: [69][217/233]	Loss 0.0108 (0.0133)	
training:	Epoch: [69][218/233]	Loss 0.0104 (0.0133)	
training:	Epoch: [69][219/233]	Loss 0.0100 (0.0132)	
training:	Epoch: [69][220/233]	Loss 0.0209 (0.0133)	
training:	Epoch: [69][221/233]	Loss 0.0313 (0.0134)	
training:	Epoch: [69][222/233]	Loss 0.0114 (0.0133)	
training:	Epoch: [69][223/233]	Loss 0.0101 (0.0133)	
training:	Epoch: [69][224/233]	Loss 0.0119 (0.0133)	
training:	Epoch: [69][225/233]	Loss 0.0100 (0.0133)	
training:	Epoch: [69][226/233]	Loss 0.0109 (0.0133)	
training:	Epoch: [69][227/233]	Loss 0.0121 (0.0133)	
training:	Epoch: [69][228/233]	Loss 0.0097 (0.0133)	
training:	Epoch: [69][229/233]	Loss 0.0123 (0.0133)	
training:	Epoch: [69][230/233]	Loss 0.0112 (0.0133)	
training:	Epoch: [69][231/233]	Loss 0.0184 (0.0133)	
training:	Epoch: [69][232/233]	Loss 0.0096 (0.0133)	
training:	Epoch: [69][233/233]	Loss 0.0094 (0.0133)	
Training:	 Loss: 0.0132

Training:	 ACC: 1.0000 1.0000 1.0000 1.0000
Validation:	 ACC: 0.8593 0.8582 0.8362 0.8823
Validation:	 Best_BACC: 0.8682 0.8678 0.8598 0.8767
Validation:	 Loss: 0.3274
Pretraining:	Epoch 70/200
----------
training:	Epoch: [70][1/233]	Loss 0.0174 (0.0174)	
training:	Epoch: [70][2/233]	Loss 0.0136 (0.0155)	
training:	Epoch: [70][3/233]	Loss 0.0098 (0.0136)	
training:	Epoch: [70][4/233]	Loss 0.0103 (0.0128)	
training:	Epoch: [70][5/233]	Loss 0.0095 (0.0121)	
training:	Epoch: [70][6/233]	Loss 0.0111 (0.0120)	
training:	Epoch: [70][7/233]	Loss 0.0107 (0.0118)	
training:	Epoch: [70][8/233]	Loss 0.0105 (0.0116)	
training:	Epoch: [70][9/233]	Loss 0.0133 (0.0118)	
training:	Epoch: [70][10/233]	Loss 0.0108 (0.0117)	
training:	Epoch: [70][11/233]	Loss 0.0124 (0.0118)	
training:	Epoch: [70][12/233]	Loss 0.0094 (0.0116)	
training:	Epoch: [70][13/233]	Loss 0.0100 (0.0114)	
training:	Epoch: [70][14/233]	Loss 0.0096 (0.0113)	
training:	Epoch: [70][15/233]	Loss 0.0093 (0.0112)	
training:	Epoch: [70][16/233]	Loss 0.0116 (0.0112)	
training:	Epoch: [70][17/233]	Loss 0.0110 (0.0112)	
training:	Epoch: [70][18/233]	Loss 0.0131 (0.0113)	
training:	Epoch: [70][19/233]	Loss 0.0120 (0.0113)	
training:	Epoch: [70][20/233]	Loss 0.0125 (0.0114)	
training:	Epoch: [70][21/233]	Loss 0.0116 (0.0114)	
training:	Epoch: [70][22/233]	Loss 0.0190 (0.0118)	
training:	Epoch: [70][23/233]	Loss 0.0119 (0.0118)	
training:	Epoch: [70][24/233]	Loss 0.0167 (0.0120)	
training:	Epoch: [70][25/233]	Loss 0.0109 (0.0119)	
training:	Epoch: [70][26/233]	Loss 0.0095 (0.0118)	
training:	Epoch: [70][27/233]	Loss 0.0107 (0.0118)	
training:	Epoch: [70][28/233]	Loss 0.0092 (0.0117)	
training:	Epoch: [70][29/233]	Loss 0.0101 (0.0116)	
training:	Epoch: [70][30/233]	Loss 0.0107 (0.0116)	
training:	Epoch: [70][31/233]	Loss 0.0103 (0.0116)	
training:	Epoch: [70][32/233]	Loss 0.0093 (0.0115)	
training:	Epoch: [70][33/233]	Loss 0.0094 (0.0114)	
training:	Epoch: [70][34/233]	Loss 0.0094 (0.0114)	
training:	Epoch: [70][35/233]	Loss 0.0097 (0.0113)	
training:	Epoch: [70][36/233]	Loss 0.0139 (0.0114)	
training:	Epoch: [70][37/233]	Loss 0.0106 (0.0114)	
training:	Epoch: [70][38/233]	Loss 0.0102 (0.0113)	
training:	Epoch: [70][39/233]	Loss 0.0115 (0.0113)	
training:	Epoch: [70][40/233]	Loss 0.0097 (0.0113)	
training:	Epoch: [70][41/233]	Loss 0.0095 (0.0113)	
training:	Epoch: [70][42/233]	Loss 0.0099 (0.0112)	
training:	Epoch: [70][43/233]	Loss 0.0100 (0.0112)	
training:	Epoch: [70][44/233]	Loss 0.0275 (0.0116)	
training:	Epoch: [70][45/233]	Loss 0.0147 (0.0116)	
training:	Epoch: [70][46/233]	Loss 0.0122 (0.0117)	
training:	Epoch: [70][47/233]	Loss 0.0097 (0.0116)	
training:	Epoch: [70][48/233]	Loss 0.0105 (0.0116)	
training:	Epoch: [70][49/233]	Loss 0.0095 (0.0115)	
training:	Epoch: [70][50/233]	Loss 0.0127 (0.0116)	
training:	Epoch: [70][51/233]	Loss 0.0180 (0.0117)	
training:	Epoch: [70][52/233]	Loss 0.0114 (0.0117)	
training:	Epoch: [70][53/233]	Loss 0.0104 (0.0117)	
training:	Epoch: [70][54/233]	Loss 0.0109 (0.0117)	
training:	Epoch: [70][55/233]	Loss 0.0215 (0.0118)	
training:	Epoch: [70][56/233]	Loss 0.0109 (0.0118)	
training:	Epoch: [70][57/233]	Loss 0.0162 (0.0119)	
training:	Epoch: [70][58/233]	Loss 0.0201 (0.0120)	
training:	Epoch: [70][59/233]	Loss 0.0119 (0.0120)	
training:	Epoch: [70][60/233]	Loss 0.0099 (0.0120)	
training:	Epoch: [70][61/233]	Loss 0.0098 (0.0120)	
training:	Epoch: [70][62/233]	Loss 0.0095 (0.0119)	
training:	Epoch: [70][63/233]	Loss 0.0105 (0.0119)	
training:	Epoch: [70][64/233]	Loss 0.0089 (0.0119)	
training:	Epoch: [70][65/233]	Loss 0.0097 (0.0118)	
training:	Epoch: [70][66/233]	Loss 0.0108 (0.0118)	
training:	Epoch: [70][67/233]	Loss 0.0218 (0.0120)	
training:	Epoch: [70][68/233]	Loss 0.0105 (0.0119)	
training:	Epoch: [70][69/233]	Loss 0.0133 (0.0120)	
training:	Epoch: [70][70/233]	Loss 0.0154 (0.0120)	
training:	Epoch: [70][71/233]	Loss 0.0104 (0.0120)	
training:	Epoch: [70][72/233]	Loss 0.0104 (0.0120)	
training:	Epoch: [70][73/233]	Loss 0.0099 (0.0119)	
training:	Epoch: [70][74/233]	Loss 0.0111 (0.0119)	
training:	Epoch: [70][75/233]	Loss 0.0093 (0.0119)	
training:	Epoch: [70][76/233]	Loss 0.0172 (0.0120)	
training:	Epoch: [70][77/233]	Loss 0.0096 (0.0119)	
training:	Epoch: [70][78/233]	Loss 0.0143 (0.0120)	
training:	Epoch: [70][79/233]	Loss 0.0092 (0.0119)	
training:	Epoch: [70][80/233]	Loss 0.0158 (0.0120)	
training:	Epoch: [70][81/233]	Loss 0.0104 (0.0119)	
training:	Epoch: [70][82/233]	Loss 0.0122 (0.0119)	
training:	Epoch: [70][83/233]	Loss 0.0131 (0.0120)	
training:	Epoch: [70][84/233]	Loss 0.0136 (0.0120)	
training:	Epoch: [70][85/233]	Loss 0.0099 (0.0120)	
training:	Epoch: [70][86/233]	Loss 0.0120 (0.0120)	
training:	Epoch: [70][87/233]	Loss 0.0099 (0.0119)	
training:	Epoch: [70][88/233]	Loss 0.0139 (0.0120)	
training:	Epoch: [70][89/233]	Loss 0.0205 (0.0121)	
training:	Epoch: [70][90/233]	Loss 0.0099 (0.0120)	
training:	Epoch: [70][91/233]	Loss 0.0115 (0.0120)	
training:	Epoch: [70][92/233]	Loss 0.0171 (0.0121)	
training:	Epoch: [70][93/233]	Loss 0.0113 (0.0121)	
training:	Epoch: [70][94/233]	Loss 0.0116 (0.0121)	
training:	Epoch: [70][95/233]	Loss 0.0115 (0.0121)	
training:	Epoch: [70][96/233]	Loss 0.0319 (0.0123)	
training:	Epoch: [70][97/233]	Loss 0.0100 (0.0122)	
training:	Epoch: [70][98/233]	Loss 0.0109 (0.0122)	
training:	Epoch: [70][99/233]	Loss 0.0156 (0.0123)	
training:	Epoch: [70][100/233]	Loss 0.0142 (0.0123)	
training:	Epoch: [70][101/233]	Loss 0.0108 (0.0123)	
training:	Epoch: [70][102/233]	Loss 0.0120 (0.0123)	
training:	Epoch: [70][103/233]	Loss 0.0268 (0.0124)	
training:	Epoch: [70][104/233]	Loss 0.0139 (0.0124)	
training:	Epoch: [70][105/233]	Loss 0.0142 (0.0124)	
training:	Epoch: [70][106/233]	Loss 0.0172 (0.0125)	
training:	Epoch: [70][107/233]	Loss 0.0176 (0.0125)	
training:	Epoch: [70][108/233]	Loss 0.0121 (0.0125)	
training:	Epoch: [70][109/233]	Loss 0.0121 (0.0125)	
training:	Epoch: [70][110/233]	Loss 0.0130 (0.0125)	
training:	Epoch: [70][111/233]	Loss 0.0131 (0.0125)	
training:	Epoch: [70][112/233]	Loss 0.0109 (0.0125)	
training:	Epoch: [70][113/233]	Loss 0.0120 (0.0125)	
training:	Epoch: [70][114/233]	Loss 0.0096 (0.0125)	
training:	Epoch: [70][115/233]	Loss 0.0115 (0.0125)	
training:	Epoch: [70][116/233]	Loss 0.0117 (0.0125)	
training:	Epoch: [70][117/233]	Loss 0.0403 (0.0127)	
training:	Epoch: [70][118/233]	Loss 0.0104 (0.0127)	
training:	Epoch: [70][119/233]	Loss 0.0100 (0.0127)	
training:	Epoch: [70][120/233]	Loss 0.0102 (0.0126)	
training:	Epoch: [70][121/233]	Loss 0.0093 (0.0126)	
training:	Epoch: [70][122/233]	Loss 0.0104 (0.0126)	
training:	Epoch: [70][123/233]	Loss 0.0110 (0.0126)	
training:	Epoch: [70][124/233]	Loss 0.0119 (0.0126)	
training:	Epoch: [70][125/233]	Loss 0.0107 (0.0126)	
training:	Epoch: [70][126/233]	Loss 0.0098 (0.0125)	
training:	Epoch: [70][127/233]	Loss 0.0139 (0.0126)	
training:	Epoch: [70][128/233]	Loss 0.0103 (0.0125)	
training:	Epoch: [70][129/233]	Loss 0.0096 (0.0125)	
training:	Epoch: [70][130/233]	Loss 0.0126 (0.0125)	
training:	Epoch: [70][131/233]	Loss 0.0107 (0.0125)	
training:	Epoch: [70][132/233]	Loss 0.0142 (0.0125)	
training:	Epoch: [70][133/233]	Loss 0.0102 (0.0125)	
training:	Epoch: [70][134/233]	Loss 0.0106 (0.0125)	
training:	Epoch: [70][135/233]	Loss 0.0106 (0.0125)	
training:	Epoch: [70][136/233]	Loss 0.0117 (0.0125)	
training:	Epoch: [70][137/233]	Loss 0.0126 (0.0125)	
training:	Epoch: [70][138/233]	Loss 0.0119 (0.0125)	
training:	Epoch: [70][139/233]	Loss 0.0114 (0.0125)	
training:	Epoch: [70][140/233]	Loss 0.0111 (0.0124)	
training:	Epoch: [70][141/233]	Loss 0.0115 (0.0124)	
training:	Epoch: [70][142/233]	Loss 0.0117 (0.0124)	
training:	Epoch: [70][143/233]	Loss 0.0119 (0.0124)	
training:	Epoch: [70][144/233]	Loss 0.0217 (0.0125)	
training:	Epoch: [70][145/233]	Loss 0.0312 (0.0126)	
training:	Epoch: [70][146/233]	Loss 0.0151 (0.0126)	
training:	Epoch: [70][147/233]	Loss 0.0102 (0.0126)	
training:	Epoch: [70][148/233]	Loss 0.0100 (0.0126)	
training:	Epoch: [70][149/233]	Loss 0.0094 (0.0126)	
training:	Epoch: [70][150/233]	Loss 0.0098 (0.0126)	
training:	Epoch: [70][151/233]	Loss 0.0093 (0.0125)	
training:	Epoch: [70][152/233]	Loss 0.0099 (0.0125)	
training:	Epoch: [70][153/233]	Loss 0.0095 (0.0125)	
training:	Epoch: [70][154/233]	Loss 0.0225 (0.0126)	
training:	Epoch: [70][155/233]	Loss 0.0135 (0.0126)	
training:	Epoch: [70][156/233]	Loss 0.0264 (0.0127)	
training:	Epoch: [70][157/233]	Loss 0.0112 (0.0127)	
training:	Epoch: [70][158/233]	Loss 0.0100 (0.0126)	
training:	Epoch: [70][159/233]	Loss 0.0094 (0.0126)	
training:	Epoch: [70][160/233]	Loss 0.0099 (0.0126)	
training:	Epoch: [70][161/233]	Loss 0.0104 (0.0126)	
training:	Epoch: [70][162/233]	Loss 0.0111 (0.0126)	
training:	Epoch: [70][163/233]	Loss 0.0126 (0.0126)	
training:	Epoch: [70][164/233]	Loss 0.0121 (0.0126)	
training:	Epoch: [70][165/233]	Loss 0.0108 (0.0126)	
training:	Epoch: [70][166/233]	Loss 0.0104 (0.0126)	
training:	Epoch: [70][167/233]	Loss 0.0105 (0.0125)	
training:	Epoch: [70][168/233]	Loss 0.0122 (0.0125)	
training:	Epoch: [70][169/233]	Loss 0.0400 (0.0127)	
training:	Epoch: [70][170/233]	Loss 0.0099 (0.0127)	
training:	Epoch: [70][171/233]	Loss 0.0118 (0.0127)	
training:	Epoch: [70][172/233]	Loss 0.0097 (0.0127)	
training:	Epoch: [70][173/233]	Loss 0.0123 (0.0127)	
training:	Epoch: [70][174/233]	Loss 0.0102 (0.0126)	
training:	Epoch: [70][175/233]	Loss 0.0152 (0.0127)	
training:	Epoch: [70][176/233]	Loss 0.0098 (0.0126)	
training:	Epoch: [70][177/233]	Loss 0.0107 (0.0126)	
training:	Epoch: [70][178/233]	Loss 0.0113 (0.0126)	
training:	Epoch: [70][179/233]	Loss 0.0100 (0.0126)	
training:	Epoch: [70][180/233]	Loss 0.0194 (0.0126)	
training:	Epoch: [70][181/233]	Loss 0.0121 (0.0126)	
training:	Epoch: [70][182/233]	Loss 0.0152 (0.0127)	
training:	Epoch: [70][183/233]	Loss 0.0116 (0.0127)	
training:	Epoch: [70][184/233]	Loss 0.0114 (0.0126)	
training:	Epoch: [70][185/233]	Loss 0.0151 (0.0127)	
training:	Epoch: [70][186/233]	Loss 0.0146 (0.0127)	
training:	Epoch: [70][187/233]	Loss 0.0185 (0.0127)	
training:	Epoch: [70][188/233]	Loss 0.0099 (0.0127)	
training:	Epoch: [70][189/233]	Loss 0.0094 (0.0127)	
training:	Epoch: [70][190/233]	Loss 0.0251 (0.0127)	
training:	Epoch: [70][191/233]	Loss 0.0111 (0.0127)	
training:	Epoch: [70][192/233]	Loss 0.0114 (0.0127)	
training:	Epoch: [70][193/233]	Loss 0.0116 (0.0127)	
training:	Epoch: [70][194/233]	Loss 0.0100 (0.0127)	
training:	Epoch: [70][195/233]	Loss 0.0138 (0.0127)	
training:	Epoch: [70][196/233]	Loss 0.0104 (0.0127)	
training:	Epoch: [70][197/233]	Loss 0.0129 (0.0127)	
training:	Epoch: [70][198/233]	Loss 0.0140 (0.0127)	
training:	Epoch: [70][199/233]	Loss 0.0109 (0.0127)	
training:	Epoch: [70][200/233]	Loss 0.0126 (0.0127)	
training:	Epoch: [70][201/233]	Loss 0.0107 (0.0127)	
training:	Epoch: [70][202/233]	Loss 0.0122 (0.0127)	
training:	Epoch: [70][203/233]	Loss 0.0135 (0.0127)	
training:	Epoch: [70][204/233]	Loss 0.0105 (0.0127)	
training:	Epoch: [70][205/233]	Loss 0.0356 (0.0128)	
training:	Epoch: [70][206/233]	Loss 0.0128 (0.0128)	
training:	Epoch: [70][207/233]	Loss 0.0116 (0.0128)	
training:	Epoch: [70][208/233]	Loss 0.0193 (0.0128)	
training:	Epoch: [70][209/233]	Loss 0.0094 (0.0128)	
training:	Epoch: [70][210/233]	Loss 0.0122 (0.0128)	
training:	Epoch: [70][211/233]	Loss 0.0115 (0.0128)	
training:	Epoch: [70][212/233]	Loss 0.0097 (0.0128)	
training:	Epoch: [70][213/233]	Loss 0.0152 (0.0128)	
training:	Epoch: [70][214/233]	Loss 0.0142 (0.0128)	
training:	Epoch: [70][215/233]	Loss 0.0146 (0.0128)	
training:	Epoch: [70][216/233]	Loss 0.0134 (0.0128)	
training:	Epoch: [70][217/233]	Loss 0.0100 (0.0128)	
training:	Epoch: [70][218/233]	Loss 0.0096 (0.0128)	
training:	Epoch: [70][219/233]	Loss 0.0108 (0.0128)	
training:	Epoch: [70][220/233]	Loss 0.0196 (0.0128)	
training:	Epoch: [70][221/233]	Loss 0.0113 (0.0128)	
training:	Epoch: [70][222/233]	Loss 0.0145 (0.0128)	
training:	Epoch: [70][223/233]	Loss 0.0112 (0.0128)	
training:	Epoch: [70][224/233]	Loss 0.0147 (0.0128)	
training:	Epoch: [70][225/233]	Loss 0.0233 (0.0128)	
training:	Epoch: [70][226/233]	Loss 0.0151 (0.0129)	
training:	Epoch: [70][227/233]	Loss 0.0140 (0.0129)	
training:	Epoch: [70][228/233]	Loss 0.0114 (0.0128)	
training:	Epoch: [70][229/233]	Loss 0.0098 (0.0128)	
training:	Epoch: [70][230/233]	Loss 0.0113 (0.0128)	
training:	Epoch: [70][231/233]	Loss 0.0118 (0.0128)	
training:	Epoch: [70][232/233]	Loss 0.0111 (0.0128)	
training:	Epoch: [70][233/233]	Loss 0.0095 (0.0128)	
Training:	 Loss: 0.0128

Training:	 ACC: 1.0000 1.0000 1.0000 1.0000
Validation:	 ACC: 0.8604 0.8598 0.8485 0.8722
Validation:	 Best_BACC: 0.8682 0.8678 0.8598 0.8767
Validation:	 Loss: 0.3264
Pretraining:	Epoch 71/200
----------
training:	Epoch: [71][1/233]	Loss 0.0137 (0.0137)	
training:	Epoch: [71][2/233]	Loss 0.0095 (0.0116)	
training:	Epoch: [71][3/233]	Loss 0.0167 (0.0133)	
training:	Epoch: [71][4/233]	Loss 0.0099 (0.0124)	
training:	Epoch: [71][5/233]	Loss 0.0098 (0.0119)	
training:	Epoch: [71][6/233]	Loss 0.0094 (0.0115)	
training:	Epoch: [71][7/233]	Loss 0.0107 (0.0114)	
training:	Epoch: [71][8/233]	Loss 0.0090 (0.0111)	
training:	Epoch: [71][9/233]	Loss 0.0098 (0.0109)	
training:	Epoch: [71][10/233]	Loss 0.0126 (0.0111)	
training:	Epoch: [71][11/233]	Loss 0.0098 (0.0110)	
training:	Epoch: [71][12/233]	Loss 0.0125 (0.0111)	
training:	Epoch: [71][13/233]	Loss 0.0147 (0.0114)	
training:	Epoch: [71][14/233]	Loss 0.0089 (0.0112)	
training:	Epoch: [71][15/233]	Loss 0.0096 (0.0111)	
training:	Epoch: [71][16/233]	Loss 0.0141 (0.0113)	
training:	Epoch: [71][17/233]	Loss 0.0109 (0.0113)	
training:	Epoch: [71][18/233]	Loss 0.0213 (0.0118)	
training:	Epoch: [71][19/233]	Loss 0.0107 (0.0118)	
training:	Epoch: [71][20/233]	Loss 0.0157 (0.0120)	
training:	Epoch: [71][21/233]	Loss 0.0196 (0.0123)	
training:	Epoch: [71][22/233]	Loss 0.0099 (0.0122)	
training:	Epoch: [71][23/233]	Loss 0.0128 (0.0122)	
training:	Epoch: [71][24/233]	Loss 0.0093 (0.0121)	
training:	Epoch: [71][25/233]	Loss 0.0098 (0.0120)	
training:	Epoch: [71][26/233]	Loss 0.0105 (0.0120)	
training:	Epoch: [71][27/233]	Loss 0.0137 (0.0120)	
training:	Epoch: [71][28/233]	Loss 0.0110 (0.0120)	
training:	Epoch: [71][29/233]	Loss 0.0092 (0.0119)	
training:	Epoch: [71][30/233]	Loss 0.0101 (0.0118)	
training:	Epoch: [71][31/233]	Loss 0.0140 (0.0119)	
training:	Epoch: [71][32/233]	Loss 0.0099 (0.0118)	
training:	Epoch: [71][33/233]	Loss 0.0138 (0.0119)	
training:	Epoch: [71][34/233]	Loss 0.0106 (0.0119)	
training:	Epoch: [71][35/233]	Loss 0.0096 (0.0118)	
training:	Epoch: [71][36/233]	Loss 0.0118 (0.0118)	
training:	Epoch: [71][37/233]	Loss 0.0104 (0.0118)	
training:	Epoch: [71][38/233]	Loss 0.0100 (0.0117)	
training:	Epoch: [71][39/233]	Loss 0.0098 (0.0117)	
training:	Epoch: [71][40/233]	Loss 0.0090 (0.0116)	
training:	Epoch: [71][41/233]	Loss 0.0172 (0.0117)	
training:	Epoch: [71][42/233]	Loss 0.0141 (0.0118)	
training:	Epoch: [71][43/233]	Loss 0.0134 (0.0118)	
training:	Epoch: [71][44/233]	Loss 0.0100 (0.0118)	
training:	Epoch: [71][45/233]	Loss 0.0141 (0.0118)	
training:	Epoch: [71][46/233]	Loss 0.0113 (0.0118)	
training:	Epoch: [71][47/233]	Loss 0.0129 (0.0119)	
training:	Epoch: [71][48/233]	Loss 0.0119 (0.0119)	
training:	Epoch: [71][49/233]	Loss 0.0199 (0.0120)	
training:	Epoch: [71][50/233]	Loss 0.0213 (0.0122)	
training:	Epoch: [71][51/233]	Loss 0.0122 (0.0122)	
training:	Epoch: [71][52/233]	Loss 0.0116 (0.0122)	
training:	Epoch: [71][53/233]	Loss 0.0091 (0.0121)	
training:	Epoch: [71][54/233]	Loss 0.0194 (0.0123)	
training:	Epoch: [71][55/233]	Loss 0.0092 (0.0122)	
training:	Epoch: [71][56/233]	Loss 0.0096 (0.0122)	
training:	Epoch: [71][57/233]	Loss 0.0102 (0.0121)	
training:	Epoch: [71][58/233]	Loss 0.0098 (0.0121)	
training:	Epoch: [71][59/233]	Loss 0.0201 (0.0122)	
training:	Epoch: [71][60/233]	Loss 0.0105 (0.0122)	
training:	Epoch: [71][61/233]	Loss 0.0179 (0.0123)	
training:	Epoch: [71][62/233]	Loss 0.0133 (0.0123)	
training:	Epoch: [71][63/233]	Loss 0.0131 (0.0123)	
training:	Epoch: [71][64/233]	Loss 0.0114 (0.0123)	
training:	Epoch: [71][65/233]	Loss 0.0095 (0.0123)	
training:	Epoch: [71][66/233]	Loss 0.0106 (0.0122)	
training:	Epoch: [71][67/233]	Loss 0.0109 (0.0122)	
training:	Epoch: [71][68/233]	Loss 0.0100 (0.0122)	
training:	Epoch: [71][69/233]	Loss 0.0095 (0.0121)	
training:	Epoch: [71][70/233]	Loss 0.0109 (0.0121)	
training:	Epoch: [71][71/233]	Loss 0.0090 (0.0121)	
training:	Epoch: [71][72/233]	Loss 0.0105 (0.0121)	
training:	Epoch: [71][73/233]	Loss 0.0208 (0.0122)	
training:	Epoch: [71][74/233]	Loss 0.0097 (0.0121)	
training:	Epoch: [71][75/233]	Loss 0.0106 (0.0121)	
training:	Epoch: [71][76/233]	Loss 0.0120 (0.0121)	
training:	Epoch: [71][77/233]	Loss 0.0107 (0.0121)	
training:	Epoch: [71][78/233]	Loss 0.0100 (0.0121)	
training:	Epoch: [71][79/233]	Loss 0.0143 (0.0121)	
training:	Epoch: [71][80/233]	Loss 0.0101 (0.0121)	
training:	Epoch: [71][81/233]	Loss 0.0180 (0.0122)	
training:	Epoch: [71][82/233]	Loss 0.0094 (0.0121)	
training:	Epoch: [71][83/233]	Loss 0.0144 (0.0122)	
training:	Epoch: [71][84/233]	Loss 0.0101 (0.0121)	
training:	Epoch: [71][85/233]	Loss 0.0153 (0.0122)	
training:	Epoch: [71][86/233]	Loss 0.0102 (0.0121)	
training:	Epoch: [71][87/233]	Loss 0.0135 (0.0122)	
training:	Epoch: [71][88/233]	Loss 0.0182 (0.0122)	
training:	Epoch: [71][89/233]	Loss 0.0085 (0.0122)	
training:	Epoch: [71][90/233]	Loss 0.0096 (0.0122)	
training:	Epoch: [71][91/233]	Loss 0.0100 (0.0121)	
training:	Epoch: [71][92/233]	Loss 0.0102 (0.0121)	
training:	Epoch: [71][93/233]	Loss 0.0140 (0.0121)	
training:	Epoch: [71][94/233]	Loss 0.0139 (0.0121)	
training:	Epoch: [71][95/233]	Loss 0.0101 (0.0121)	
training:	Epoch: [71][96/233]	Loss 0.0133 (0.0121)	
training:	Epoch: [71][97/233]	Loss 0.0120 (0.0121)	
training:	Epoch: [71][98/233]	Loss 0.0303 (0.0123)	
training:	Epoch: [71][99/233]	Loss 0.0118 (0.0123)	
training:	Epoch: [71][100/233]	Loss 0.0133 (0.0123)	
training:	Epoch: [71][101/233]	Loss 0.0099 (0.0123)	
training:	Epoch: [71][102/233]	Loss 0.0097 (0.0123)	
training:	Epoch: [71][103/233]	Loss 0.0093 (0.0122)	
training:	Epoch: [71][104/233]	Loss 0.0096 (0.0122)	
training:	Epoch: [71][105/233]	Loss 0.0099 (0.0122)	
training:	Epoch: [71][106/233]	Loss 0.0102 (0.0122)	
training:	Epoch: [71][107/233]	Loss 0.0103 (0.0122)	
training:	Epoch: [71][108/233]	Loss 0.0123 (0.0122)	
training:	Epoch: [71][109/233]	Loss 0.0097 (0.0121)	
training:	Epoch: [71][110/233]	Loss 0.0096 (0.0121)	
training:	Epoch: [71][111/233]	Loss 0.0106 (0.0121)	
training:	Epoch: [71][112/233]	Loss 0.0094 (0.0121)	
training:	Epoch: [71][113/233]	Loss 0.0093 (0.0121)	
training:	Epoch: [71][114/233]	Loss 0.0147 (0.0121)	
training:	Epoch: [71][115/233]	Loss 0.0109 (0.0121)	
training:	Epoch: [71][116/233]	Loss 0.0108 (0.0121)	
training:	Epoch: [71][117/233]	Loss 0.0152 (0.0121)	
training:	Epoch: [71][118/233]	Loss 0.0197 (0.0121)	
training:	Epoch: [71][119/233]	Loss 0.0113 (0.0121)	
training:	Epoch: [71][120/233]	Loss 0.0114 (0.0121)	
training:	Epoch: [71][121/233]	Loss 0.0099 (0.0121)	
training:	Epoch: [71][122/233]	Loss 0.0095 (0.0121)	
training:	Epoch: [71][123/233]	Loss 0.0123 (0.0121)	
training:	Epoch: [71][124/233]	Loss 0.0117 (0.0121)	
training:	Epoch: [71][125/233]	Loss 0.0098 (0.0121)	
training:	Epoch: [71][126/233]	Loss 0.0112 (0.0121)	
training:	Epoch: [71][127/233]	Loss 0.0307 (0.0122)	
training:	Epoch: [71][128/233]	Loss 0.0165 (0.0122)	
training:	Epoch: [71][129/233]	Loss 0.0096 (0.0122)	
training:	Epoch: [71][130/233]	Loss 0.0134 (0.0122)	
training:	Epoch: [71][131/233]	Loss 0.0100 (0.0122)	
training:	Epoch: [71][132/233]	Loss 0.0134 (0.0122)	
training:	Epoch: [71][133/233]	Loss 0.0109 (0.0122)	
training:	Epoch: [71][134/233]	Loss 0.0101 (0.0122)	
training:	Epoch: [71][135/233]	Loss 0.0101 (0.0122)	
training:	Epoch: [71][136/233]	Loss 0.0100 (0.0122)	
training:	Epoch: [71][137/233]	Loss 0.0176 (0.0122)	
training:	Epoch: [71][138/233]	Loss 0.0150 (0.0122)	
training:	Epoch: [71][139/233]	Loss 0.0086 (0.0122)	
training:	Epoch: [71][140/233]	Loss 0.0136 (0.0122)	
training:	Epoch: [71][141/233]	Loss 0.0114 (0.0122)	
training:	Epoch: [71][142/233]	Loss 0.0172 (0.0122)	
training:	Epoch: [71][143/233]	Loss 0.0141 (0.0123)	
training:	Epoch: [71][144/233]	Loss 0.0092 (0.0122)	
training:	Epoch: [71][145/233]	Loss 0.0147 (0.0123)	
training:	Epoch: [71][146/233]	Loss 0.0203 (0.0123)	
training:	Epoch: [71][147/233]	Loss 0.0110 (0.0123)	
training:	Epoch: [71][148/233]	Loss 0.0097 (0.0123)	
training:	Epoch: [71][149/233]	Loss 0.0116 (0.0123)	
training:	Epoch: [71][150/233]	Loss 0.0118 (0.0123)	
training:	Epoch: [71][151/233]	Loss 0.0109 (0.0123)	
training:	Epoch: [71][152/233]	Loss 0.0235 (0.0123)	
training:	Epoch: [71][153/233]	Loss 0.0129 (0.0123)	
training:	Epoch: [71][154/233]	Loss 0.0100 (0.0123)	
training:	Epoch: [71][155/233]	Loss 0.0151 (0.0123)	
training:	Epoch: [71][156/233]	Loss 0.0105 (0.0123)	
training:	Epoch: [71][157/233]	Loss 0.0093 (0.0123)	
training:	Epoch: [71][158/233]	Loss 0.0181 (0.0124)	
training:	Epoch: [71][159/233]	Loss 0.0094 (0.0123)	
training:	Epoch: [71][160/233]	Loss 0.0107 (0.0123)	
training:	Epoch: [71][161/233]	Loss 0.0093 (0.0123)	
training:	Epoch: [71][162/233]	Loss 0.0102 (0.0123)	
training:	Epoch: [71][163/233]	Loss 0.0105 (0.0123)	
training:	Epoch: [71][164/233]	Loss 0.0161 (0.0123)	
training:	Epoch: [71][165/233]	Loss 0.0099 (0.0123)	
training:	Epoch: [71][166/233]	Loss 0.0174 (0.0123)	
training:	Epoch: [71][167/233]	Loss 0.0105 (0.0123)	
training:	Epoch: [71][168/233]	Loss 0.0233 (0.0124)	
training:	Epoch: [71][169/233]	Loss 0.0104 (0.0124)	
training:	Epoch: [71][170/233]	Loss 0.0121 (0.0124)	
training:	Epoch: [71][171/233]	Loss 0.0101 (0.0123)	
training:	Epoch: [71][172/233]	Loss 0.0098 (0.0123)	
training:	Epoch: [71][173/233]	Loss 0.0091 (0.0123)	
training:	Epoch: [71][174/233]	Loss 0.0133 (0.0123)	
training:	Epoch: [71][175/233]	Loss 0.0099 (0.0123)	
training:	Epoch: [71][176/233]	Loss 0.0103 (0.0123)	
training:	Epoch: [71][177/233]	Loss 0.0176 (0.0123)	
training:	Epoch: [71][178/233]	Loss 0.0098 (0.0123)	
training:	Epoch: [71][179/233]	Loss 0.0096 (0.0123)	
training:	Epoch: [71][180/233]	Loss 0.0097 (0.0123)	
training:	Epoch: [71][181/233]	Loss 0.0142 (0.0123)	
training:	Epoch: [71][182/233]	Loss 0.0141 (0.0123)	
training:	Epoch: [71][183/233]	Loss 0.0116 (0.0123)	
training:	Epoch: [71][184/233]	Loss 0.0126 (0.0123)	
training:	Epoch: [71][185/233]	Loss 0.0120 (0.0123)	
training:	Epoch: [71][186/233]	Loss 0.0136 (0.0123)	
training:	Epoch: [71][187/233]	Loss 0.0108 (0.0123)	
training:	Epoch: [71][188/233]	Loss 0.0092 (0.0123)	
training:	Epoch: [71][189/233]	Loss 0.0142 (0.0123)	
training:	Epoch: [71][190/233]	Loss 0.0146 (0.0123)	
training:	Epoch: [71][191/233]	Loss 0.0113 (0.0123)	
training:	Epoch: [71][192/233]	Loss 0.0123 (0.0123)	
training:	Epoch: [71][193/233]	Loss 0.0105 (0.0123)	
training:	Epoch: [71][194/233]	Loss 0.0107 (0.0123)	
training:	Epoch: [71][195/233]	Loss 0.0095 (0.0123)	
training:	Epoch: [71][196/233]	Loss 0.0099 (0.0123)	
training:	Epoch: [71][197/233]	Loss 0.0095 (0.0122)	
training:	Epoch: [71][198/233]	Loss 0.0148 (0.0123)	
training:	Epoch: [71][199/233]	Loss 0.0124 (0.0123)	
training:	Epoch: [71][200/233]	Loss 0.0124 (0.0123)	
training:	Epoch: [71][201/233]	Loss 0.0097 (0.0122)	
training:	Epoch: [71][202/233]	Loss 0.0120 (0.0122)	
training:	Epoch: [71][203/233]	Loss 0.0109 (0.0122)	
training:	Epoch: [71][204/233]	Loss 0.0150 (0.0122)	
training:	Epoch: [71][205/233]	Loss 0.0094 (0.0122)	
training:	Epoch: [71][206/233]	Loss 0.0121 (0.0122)	
training:	Epoch: [71][207/233]	Loss 0.0087 (0.0122)	
training:	Epoch: [71][208/233]	Loss 0.0095 (0.0122)	
training:	Epoch: [71][209/233]	Loss 0.0103 (0.0122)	
training:	Epoch: [71][210/233]	Loss 0.0159 (0.0122)	
training:	Epoch: [71][211/233]	Loss 0.0282 (0.0123)	
training:	Epoch: [71][212/233]	Loss 0.0097 (0.0123)	
training:	Epoch: [71][213/233]	Loss 0.0208 (0.0123)	
training:	Epoch: [71][214/233]	Loss 0.0114 (0.0123)	
training:	Epoch: [71][215/233]	Loss 0.0102 (0.0123)	
training:	Epoch: [71][216/233]	Loss 0.0108 (0.0123)	
training:	Epoch: [71][217/233]	Loss 0.0360 (0.0124)	
training:	Epoch: [71][218/233]	Loss 0.0123 (0.0124)	
training:	Epoch: [71][219/233]	Loss 0.0104 (0.0124)	
training:	Epoch: [71][220/233]	Loss 0.0137 (0.0124)	
training:	Epoch: [71][221/233]	Loss 0.0120 (0.0124)	
training:	Epoch: [71][222/233]	Loss 0.0123 (0.0124)	
training:	Epoch: [71][223/233]	Loss 0.0090 (0.0124)	
training:	Epoch: [71][224/233]	Loss 0.0182 (0.0124)	
training:	Epoch: [71][225/233]	Loss 0.0121 (0.0124)	
training:	Epoch: [71][226/233]	Loss 0.0098 (0.0124)	
training:	Epoch: [71][227/233]	Loss 0.0097 (0.0124)	
training:	Epoch: [71][228/233]	Loss 0.0118 (0.0124)	
training:	Epoch: [71][229/233]	Loss 0.0099 (0.0124)	
training:	Epoch: [71][230/233]	Loss 0.0100 (0.0124)	
training:	Epoch: [71][231/233]	Loss 0.0095 (0.0123)	
training:	Epoch: [71][232/233]	Loss 0.0094 (0.0123)	
training:	Epoch: [71][233/233]	Loss 0.0123 (0.0123)	
Training:	 Loss: 0.0123

Training:	 ACC: 1.0000 1.0000 1.0000 1.0000
Validation:	 ACC: 0.8602 0.8604 0.8639 0.8565
Validation:	 Best_BACC: 0.8682 0.8678 0.8598 0.8767
Validation:	 Loss: 0.3241
Pretraining:	Epoch 72/200
----------
training:	Epoch: [72][1/233]	Loss 0.0158 (0.0158)	
training:	Epoch: [72][2/233]	Loss 0.0099 (0.0128)	
training:	Epoch: [72][3/233]	Loss 0.0089 (0.0115)	
training:	Epoch: [72][4/233]	Loss 0.0126 (0.0118)	
training:	Epoch: [72][5/233]	Loss 0.0139 (0.0122)	
training:	Epoch: [72][6/233]	Loss 0.0099 (0.0118)	
training:	Epoch: [72][7/233]	Loss 0.0088 (0.0114)	
training:	Epoch: [72][8/233]	Loss 0.0090 (0.0111)	
training:	Epoch: [72][9/233]	Loss 0.0100 (0.0110)	
training:	Epoch: [72][10/233]	Loss 0.0142 (0.0113)	
training:	Epoch: [72][11/233]	Loss 0.0111 (0.0113)	
training:	Epoch: [72][12/233]	Loss 0.0108 (0.0112)	
training:	Epoch: [72][13/233]	Loss 0.0113 (0.0112)	
training:	Epoch: [72][14/233]	Loss 0.0104 (0.0112)	
training:	Epoch: [72][15/233]	Loss 0.0089 (0.0110)	
training:	Epoch: [72][16/233]	Loss 0.0100 (0.0110)	
training:	Epoch: [72][17/233]	Loss 0.0092 (0.0109)	
training:	Epoch: [72][18/233]	Loss 0.0127 (0.0110)	
training:	Epoch: [72][19/233]	Loss 0.0118 (0.0110)	
training:	Epoch: [72][20/233]	Loss 0.0166 (0.0113)	
training:	Epoch: [72][21/233]	Loss 0.0087 (0.0112)	
training:	Epoch: [72][22/233]	Loss 0.0097 (0.0111)	
training:	Epoch: [72][23/233]	Loss 0.0102 (0.0111)	
training:	Epoch: [72][24/233]	Loss 0.0089 (0.0110)	
training:	Epoch: [72][25/233]	Loss 0.0088 (0.0109)	
training:	Epoch: [72][26/233]	Loss 0.0093 (0.0108)	
training:	Epoch: [72][27/233]	Loss 0.0086 (0.0107)	
training:	Epoch: [72][28/233]	Loss 0.0110 (0.0107)	
training:	Epoch: [72][29/233]	Loss 0.0121 (0.0108)	
training:	Epoch: [72][30/233]	Loss 0.0110 (0.0108)	
training:	Epoch: [72][31/233]	Loss 0.0143 (0.0109)	
training:	Epoch: [72][32/233]	Loss 0.0130 (0.0110)	
training:	Epoch: [72][33/233]	Loss 0.0132 (0.0110)	
training:	Epoch: [72][34/233]	Loss 0.0138 (0.0111)	
training:	Epoch: [72][35/233]	Loss 0.0154 (0.0112)	
training:	Epoch: [72][36/233]	Loss 0.0090 (0.0112)	
training:	Epoch: [72][37/233]	Loss 0.0103 (0.0112)	
training:	Epoch: [72][38/233]	Loss 0.0088 (0.0111)	
training:	Epoch: [72][39/233]	Loss 0.0129 (0.0111)	
training:	Epoch: [72][40/233]	Loss 0.0152 (0.0112)	
training:	Epoch: [72][41/233]	Loss 0.0096 (0.0112)	
training:	Epoch: [72][42/233]	Loss 0.0136 (0.0113)	
training:	Epoch: [72][43/233]	Loss 0.0201 (0.0115)	
training:	Epoch: [72][44/233]	Loss 0.0105 (0.0114)	
training:	Epoch: [72][45/233]	Loss 0.0144 (0.0115)	
training:	Epoch: [72][46/233]	Loss 0.0094 (0.0115)	
training:	Epoch: [72][47/233]	Loss 0.0096 (0.0114)	
training:	Epoch: [72][48/233]	Loss 0.0098 (0.0114)	
training:	Epoch: [72][49/233]	Loss 0.0098 (0.0114)	
training:	Epoch: [72][50/233]	Loss 0.0128 (0.0114)	
training:	Epoch: [72][51/233]	Loss 0.0092 (0.0113)	
training:	Epoch: [72][52/233]	Loss 0.0109 (0.0113)	
training:	Epoch: [72][53/233]	Loss 0.0122 (0.0114)	
training:	Epoch: [72][54/233]	Loss 0.0146 (0.0114)	
training:	Epoch: [72][55/233]	Loss 0.0099 (0.0114)	
training:	Epoch: [72][56/233]	Loss 0.0100 (0.0114)	
training:	Epoch: [72][57/233]	Loss 0.0287 (0.0117)	
training:	Epoch: [72][58/233]	Loss 0.0139 (0.0117)	
training:	Epoch: [72][59/233]	Loss 0.0094 (0.0117)	
training:	Epoch: [72][60/233]	Loss 0.0098 (0.0116)	
training:	Epoch: [72][61/233]	Loss 0.0100 (0.0116)	
training:	Epoch: [72][62/233]	Loss 0.0092 (0.0116)	
training:	Epoch: [72][63/233]	Loss 0.0101 (0.0115)	
training:	Epoch: [72][64/233]	Loss 0.0163 (0.0116)	
training:	Epoch: [72][65/233]	Loss 0.0099 (0.0116)	
training:	Epoch: [72][66/233]	Loss 0.0096 (0.0116)	
training:	Epoch: [72][67/233]	Loss 0.0117 (0.0116)	
training:	Epoch: [72][68/233]	Loss 0.0112 (0.0116)	
training:	Epoch: [72][69/233]	Loss 0.0114 (0.0116)	
training:	Epoch: [72][70/233]	Loss 0.0101 (0.0115)	
training:	Epoch: [72][71/233]	Loss 0.0159 (0.0116)	
training:	Epoch: [72][72/233]	Loss 0.0102 (0.0116)	
training:	Epoch: [72][73/233]	Loss 0.0090 (0.0115)	
training:	Epoch: [72][74/233]	Loss 0.0134 (0.0116)	
training:	Epoch: [72][75/233]	Loss 0.0105 (0.0116)	
training:	Epoch: [72][76/233]	Loss 0.0095 (0.0115)	
training:	Epoch: [72][77/233]	Loss 0.0119 (0.0115)	
training:	Epoch: [72][78/233]	Loss 0.0146 (0.0116)	
training:	Epoch: [72][79/233]	Loss 0.0110 (0.0116)	
training:	Epoch: [72][80/233]	Loss 0.0140 (0.0116)	
training:	Epoch: [72][81/233]	Loss 0.0109 (0.0116)	
training:	Epoch: [72][82/233]	Loss 0.0097 (0.0116)	
training:	Epoch: [72][83/233]	Loss 0.0114 (0.0116)	
training:	Epoch: [72][84/233]	Loss 0.0259 (0.0117)	
training:	Epoch: [72][85/233]	Loss 0.0192 (0.0118)	
training:	Epoch: [72][86/233]	Loss 0.0094 (0.0118)	
training:	Epoch: [72][87/233]	Loss 0.0095 (0.0118)	
training:	Epoch: [72][88/233]	Loss 0.0151 (0.0118)	
training:	Epoch: [72][89/233]	Loss 0.0096 (0.0118)	
training:	Epoch: [72][90/233]	Loss 0.0105 (0.0118)	
training:	Epoch: [72][91/233]	Loss 0.0100 (0.0117)	
training:	Epoch: [72][92/233]	Loss 0.0092 (0.0117)	
training:	Epoch: [72][93/233]	Loss 0.0100 (0.0117)	
training:	Epoch: [72][94/233]	Loss 0.0148 (0.0117)	
training:	Epoch: [72][95/233]	Loss 0.0152 (0.0118)	
training:	Epoch: [72][96/233]	Loss 0.0090 (0.0117)	
training:	Epoch: [72][97/233]	Loss 0.0100 (0.0117)	
training:	Epoch: [72][98/233]	Loss 0.0093 (0.0117)	
training:	Epoch: [72][99/233]	Loss 0.0159 (0.0117)	
training:	Epoch: [72][100/233]	Loss 0.0121 (0.0117)	
training:	Epoch: [72][101/233]	Loss 0.0093 (0.0117)	
training:	Epoch: [72][102/233]	Loss 0.0098 (0.0117)	
training:	Epoch: [72][103/233]	Loss 0.0111 (0.0117)	
training:	Epoch: [72][104/233]	Loss 0.0212 (0.0118)	
training:	Epoch: [72][105/233]	Loss 0.0116 (0.0118)	
training:	Epoch: [72][106/233]	Loss 0.0099 (0.0118)	
training:	Epoch: [72][107/233]	Loss 0.0121 (0.0118)	
training:	Epoch: [72][108/233]	Loss 0.0120 (0.0118)	
training:	Epoch: [72][109/233]	Loss 0.0101 (0.0118)	
training:	Epoch: [72][110/233]	Loss 0.0088 (0.0117)	
training:	Epoch: [72][111/233]	Loss 0.0148 (0.0118)	
training:	Epoch: [72][112/233]	Loss 0.0137 (0.0118)	
training:	Epoch: [72][113/233]	Loss 0.0151 (0.0118)	
training:	Epoch: [72][114/233]	Loss 0.0168 (0.0118)	
training:	Epoch: [72][115/233]	Loss 0.0097 (0.0118)	
training:	Epoch: [72][116/233]	Loss 0.0097 (0.0118)	
training:	Epoch: [72][117/233]	Loss 0.0095 (0.0118)	
training:	Epoch: [72][118/233]	Loss 0.0093 (0.0118)	
training:	Epoch: [72][119/233]	Loss 0.0097 (0.0118)	
training:	Epoch: [72][120/233]	Loss 0.0090 (0.0117)	
training:	Epoch: [72][121/233]	Loss 0.0099 (0.0117)	
training:	Epoch: [72][122/233]	Loss 0.0201 (0.0118)	
training:	Epoch: [72][123/233]	Loss 0.0107 (0.0118)	
training:	Epoch: [72][124/233]	Loss 0.0102 (0.0118)	
training:	Epoch: [72][125/233]	Loss 0.0107 (0.0118)	
training:	Epoch: [72][126/233]	Loss 0.0098 (0.0117)	
training:	Epoch: [72][127/233]	Loss 0.0132 (0.0117)	
training:	Epoch: [72][128/233]	Loss 0.0093 (0.0117)	
training:	Epoch: [72][129/233]	Loss 0.0159 (0.0118)	
training:	Epoch: [72][130/233]	Loss 0.0100 (0.0117)	
training:	Epoch: [72][131/233]	Loss 0.0108 (0.0117)	
training:	Epoch: [72][132/233]	Loss 0.0114 (0.0117)	
training:	Epoch: [72][133/233]	Loss 0.0085 (0.0117)	
training:	Epoch: [72][134/233]	Loss 0.0097 (0.0117)	
training:	Epoch: [72][135/233]	Loss 0.0094 (0.0117)	
training:	Epoch: [72][136/233]	Loss 0.0141 (0.0117)	
training:	Epoch: [72][137/233]	Loss 0.0137 (0.0117)	
training:	Epoch: [72][138/233]	Loss 0.0110 (0.0117)	
training:	Epoch: [72][139/233]	Loss 0.0100 (0.0117)	
training:	Epoch: [72][140/233]	Loss 0.0140 (0.0117)	
training:	Epoch: [72][141/233]	Loss 0.0171 (0.0118)	
training:	Epoch: [72][142/233]	Loss 0.0095 (0.0117)	
training:	Epoch: [72][143/233]	Loss 0.0103 (0.0117)	
training:	Epoch: [72][144/233]	Loss 0.0095 (0.0117)	
training:	Epoch: [72][145/233]	Loss 0.0096 (0.0117)	
training:	Epoch: [72][146/233]	Loss 0.0106 (0.0117)	
training:	Epoch: [72][147/233]	Loss 0.0114 (0.0117)	
training:	Epoch: [72][148/233]	Loss 0.0175 (0.0117)	
training:	Epoch: [72][149/233]	Loss 0.0095 (0.0117)	
training:	Epoch: [72][150/233]	Loss 0.0095 (0.0117)	
training:	Epoch: [72][151/233]	Loss 0.0115 (0.0117)	
training:	Epoch: [72][152/233]	Loss 0.0090 (0.0117)	
training:	Epoch: [72][153/233]	Loss 0.0136 (0.0117)	
training:	Epoch: [72][154/233]	Loss 0.0096 (0.0117)	
training:	Epoch: [72][155/233]	Loss 0.0126 (0.0117)	
training:	Epoch: [72][156/233]	Loss 0.0111 (0.0117)	
training:	Epoch: [72][157/233]	Loss 0.0115 (0.0117)	
training:	Epoch: [72][158/233]	Loss 0.0090 (0.0117)	
training:	Epoch: [72][159/233]	Loss 0.0167 (0.0117)	
training:	Epoch: [72][160/233]	Loss 0.0089 (0.0117)	
training:	Epoch: [72][161/233]	Loss 0.0134 (0.0117)	
training:	Epoch: [72][162/233]	Loss 0.0181 (0.0117)	
training:	Epoch: [72][163/233]	Loss 0.0155 (0.0117)	
training:	Epoch: [72][164/233]	Loss 0.0109 (0.0117)	
training:	Epoch: [72][165/233]	Loss 0.0116 (0.0117)	
training:	Epoch: [72][166/233]	Loss 0.0142 (0.0118)	
training:	Epoch: [72][167/233]	Loss 0.0113 (0.0118)	
training:	Epoch: [72][168/233]	Loss 0.0162 (0.0118)	
training:	Epoch: [72][169/233]	Loss 0.0094 (0.0118)	
training:	Epoch: [72][170/233]	Loss 0.0114 (0.0118)	
training:	Epoch: [72][171/233]	Loss 0.0260 (0.0118)	
training:	Epoch: [72][172/233]	Loss 0.0104 (0.0118)	
training:	Epoch: [72][173/233]	Loss 0.0126 (0.0118)	
training:	Epoch: [72][174/233]	Loss 0.0099 (0.0118)	
training:	Epoch: [72][175/233]	Loss 0.0137 (0.0118)	
training:	Epoch: [72][176/233]	Loss 0.0103 (0.0118)	
training:	Epoch: [72][177/233]	Loss 0.0130 (0.0118)	
training:	Epoch: [72][178/233]	Loss 0.0135 (0.0119)	
training:	Epoch: [72][179/233]	Loss 0.0132 (0.0119)	
training:	Epoch: [72][180/233]	Loss 0.0208 (0.0119)	
training:	Epoch: [72][181/233]	Loss 0.0165 (0.0119)	
training:	Epoch: [72][182/233]	Loss 0.0089 (0.0119)	
training:	Epoch: [72][183/233]	Loss 0.0088 (0.0119)	
training:	Epoch: [72][184/233]	Loss 0.0130 (0.0119)	
training:	Epoch: [72][185/233]	Loss 0.0108 (0.0119)	
training:	Epoch: [72][186/233]	Loss 0.0094 (0.0119)	
training:	Epoch: [72][187/233]	Loss 0.0098 (0.0119)	
training:	Epoch: [72][188/233]	Loss 0.0100 (0.0119)	
training:	Epoch: [72][189/233]	Loss 0.0163 (0.0119)	
training:	Epoch: [72][190/233]	Loss 0.0300 (0.0120)	
training:	Epoch: [72][191/233]	Loss 0.0097 (0.0120)	
training:	Epoch: [72][192/233]	Loss 0.0131 (0.0120)	
training:	Epoch: [72][193/233]	Loss 0.0099 (0.0120)	
training:	Epoch: [72][194/233]	Loss 0.0094 (0.0120)	
training:	Epoch: [72][195/233]	Loss 0.0116 (0.0120)	
training:	Epoch: [72][196/233]	Loss 0.0274 (0.0120)	
training:	Epoch: [72][197/233]	Loss 0.0126 (0.0120)	
training:	Epoch: [72][198/233]	Loss 0.0105 (0.0120)	
training:	Epoch: [72][199/233]	Loss 0.0177 (0.0121)	
training:	Epoch: [72][200/233]	Loss 0.0100 (0.0120)	
training:	Epoch: [72][201/233]	Loss 0.0136 (0.0121)	
training:	Epoch: [72][202/233]	Loss 0.0107 (0.0120)	
training:	Epoch: [72][203/233]	Loss 0.0091 (0.0120)	
training:	Epoch: [72][204/233]	Loss 0.0086 (0.0120)	
training:	Epoch: [72][205/233]	Loss 0.0089 (0.0120)	
training:	Epoch: [72][206/233]	Loss 0.0093 (0.0120)	
training:	Epoch: [72][207/233]	Loss 0.0222 (0.0120)	
training:	Epoch: [72][208/233]	Loss 0.0100 (0.0120)	
training:	Epoch: [72][209/233]	Loss 0.0110 (0.0120)	
training:	Epoch: [72][210/233]	Loss 0.0203 (0.0121)	
training:	Epoch: [72][211/233]	Loss 0.0159 (0.0121)	
training:	Epoch: [72][212/233]	Loss 0.0093 (0.0121)	
training:	Epoch: [72][213/233]	Loss 0.0099 (0.0121)	
training:	Epoch: [72][214/233]	Loss 0.0099 (0.0120)	
training:	Epoch: [72][215/233]	Loss 0.0100 (0.0120)	
training:	Epoch: [72][216/233]	Loss 0.0107 (0.0120)	
training:	Epoch: [72][217/233]	Loss 0.0132 (0.0120)	
training:	Epoch: [72][218/233]	Loss 0.0095 (0.0120)	
training:	Epoch: [72][219/233]	Loss 0.0109 (0.0120)	
training:	Epoch: [72][220/233]	Loss 0.0114 (0.0120)	
training:	Epoch: [72][221/233]	Loss 0.0130 (0.0120)	
training:	Epoch: [72][222/233]	Loss 0.0106 (0.0120)	
training:	Epoch: [72][223/233]	Loss 0.0119 (0.0120)	
training:	Epoch: [72][224/233]	Loss 0.0100 (0.0120)	
training:	Epoch: [72][225/233]	Loss 0.0119 (0.0120)	
training:	Epoch: [72][226/233]	Loss 0.0172 (0.0120)	
training:	Epoch: [72][227/233]	Loss 0.0158 (0.0120)	
training:	Epoch: [72][228/233]	Loss 0.0105 (0.0120)	
training:	Epoch: [72][229/233]	Loss 0.0121 (0.0120)	
training:	Epoch: [72][230/233]	Loss 0.0182 (0.0121)	
training:	Epoch: [72][231/233]	Loss 0.0122 (0.0121)	
training:	Epoch: [72][232/233]	Loss 0.0299 (0.0121)	
training:	Epoch: [72][233/233]	Loss 0.0106 (0.0121)	
Training:	 Loss: 0.0121

Training:	 ACC: 1.0000 1.0000 1.0000 1.0000
Validation:	 ACC: 0.8548 0.8534 0.8250 0.8845
Validation:	 Best_BACC: 0.8682 0.8678 0.8598 0.8767
Validation:	 Loss: 0.3364
Pretraining:	Epoch 73/200
----------
training:	Epoch: [73][1/233]	Loss 0.0099 (0.0099)	
training:	Epoch: [73][2/233]	Loss 0.0100 (0.0099)	
training:	Epoch: [73][3/233]	Loss 0.0103 (0.0100)	
training:	Epoch: [73][4/233]	Loss 0.0108 (0.0102)	
training:	Epoch: [73][5/233]	Loss 0.0103 (0.0102)	
training:	Epoch: [73][6/233]	Loss 0.0109 (0.0103)	
training:	Epoch: [73][7/233]	Loss 0.0093 (0.0102)	
training:	Epoch: [73][8/233]	Loss 0.0096 (0.0101)	
training:	Epoch: [73][9/233]	Loss 0.0110 (0.0102)	
training:	Epoch: [73][10/233]	Loss 0.0168 (0.0109)	
training:	Epoch: [73][11/233]	Loss 0.0111 (0.0109)	
training:	Epoch: [73][12/233]	Loss 0.0088 (0.0107)	
training:	Epoch: [73][13/233]	Loss 0.0162 (0.0112)	
training:	Epoch: [73][14/233]	Loss 0.0091 (0.0110)	
training:	Epoch: [73][15/233]	Loss 0.0136 (0.0112)	
training:	Epoch: [73][16/233]	Loss 0.0097 (0.0111)	
training:	Epoch: [73][17/233]	Loss 0.0095 (0.0110)	
training:	Epoch: [73][18/233]	Loss 0.0085 (0.0109)	
training:	Epoch: [73][19/233]	Loss 0.0107 (0.0109)	
training:	Epoch: [73][20/233]	Loss 0.0114 (0.0109)	
training:	Epoch: [73][21/233]	Loss 0.0097 (0.0108)	
training:	Epoch: [73][22/233]	Loss 0.0142 (0.0110)	
training:	Epoch: [73][23/233]	Loss 0.0097 (0.0109)	
training:	Epoch: [73][24/233]	Loss 0.0088 (0.0108)	
training:	Epoch: [73][25/233]	Loss 0.0101 (0.0108)	
training:	Epoch: [73][26/233]	Loss 0.0083 (0.0107)	
training:	Epoch: [73][27/233]	Loss 0.0101 (0.0107)	
training:	Epoch: [73][28/233]	Loss 0.0098 (0.0107)	
training:	Epoch: [73][29/233]	Loss 0.0088 (0.0106)	
training:	Epoch: [73][30/233]	Loss 0.0107 (0.0106)	
training:	Epoch: [73][31/233]	Loss 0.0095 (0.0106)	
training:	Epoch: [73][32/233]	Loss 0.0129 (0.0106)	
training:	Epoch: [73][33/233]	Loss 0.0119 (0.0107)	
training:	Epoch: [73][34/233]	Loss 0.0159 (0.0108)	
training:	Epoch: [73][35/233]	Loss 0.0303 (0.0114)	
training:	Epoch: [73][36/233]	Loss 0.0098 (0.0113)	
training:	Epoch: [73][37/233]	Loss 0.0094 (0.0113)	
training:	Epoch: [73][38/233]	Loss 0.0097 (0.0112)	
training:	Epoch: [73][39/233]	Loss 0.0101 (0.0112)	
training:	Epoch: [73][40/233]	Loss 0.0093 (0.0112)	
training:	Epoch: [73][41/233]	Loss 0.0090 (0.0111)	
training:	Epoch: [73][42/233]	Loss 0.0089 (0.0111)	
training:	Epoch: [73][43/233]	Loss 0.0155 (0.0112)	
training:	Epoch: [73][44/233]	Loss 0.0093 (0.0111)	
training:	Epoch: [73][45/233]	Loss 0.0095 (0.0111)	
training:	Epoch: [73][46/233]	Loss 0.0085 (0.0110)	
training:	Epoch: [73][47/233]	Loss 0.0092 (0.0110)	
training:	Epoch: [73][48/233]	Loss 0.0090 (0.0109)	
training:	Epoch: [73][49/233]	Loss 0.0093 (0.0109)	
training:	Epoch: [73][50/233]	Loss 0.0087 (0.0109)	
training:	Epoch: [73][51/233]	Loss 0.0096 (0.0108)	
training:	Epoch: [73][52/233]	Loss 0.0107 (0.0108)	
training:	Epoch: [73][53/233]	Loss 0.0142 (0.0109)	
training:	Epoch: [73][54/233]	Loss 0.0113 (0.0109)	
training:	Epoch: [73][55/233]	Loss 0.0085 (0.0109)	
training:	Epoch: [73][56/233]	Loss 0.0196 (0.0110)	
training:	Epoch: [73][57/233]	Loss 0.0090 (0.0110)	
training:	Epoch: [73][58/233]	Loss 0.0103 (0.0110)	
training:	Epoch: [73][59/233]	Loss 0.0085 (0.0109)	
training:	Epoch: [73][60/233]	Loss 0.0088 (0.0109)	
training:	Epoch: [73][61/233]	Loss 0.0129 (0.0109)	
training:	Epoch: [73][62/233]	Loss 0.0107 (0.0109)	
training:	Epoch: [73][63/233]	Loss 0.0094 (0.0109)	
training:	Epoch: [73][64/233]	Loss 0.0121 (0.0109)	
training:	Epoch: [73][65/233]	Loss 0.0092 (0.0109)	
training:	Epoch: [73][66/233]	Loss 0.0108 (0.0109)	
training:	Epoch: [73][67/233]	Loss 0.0089 (0.0109)	
training:	Epoch: [73][68/233]	Loss 0.0093 (0.0108)	
training:	Epoch: [73][69/233]	Loss 0.0125 (0.0109)	
training:	Epoch: [73][70/233]	Loss 0.0129 (0.0109)	
training:	Epoch: [73][71/233]	Loss 0.0112 (0.0109)	
training:	Epoch: [73][72/233]	Loss 0.0202 (0.0110)	
training:	Epoch: [73][73/233]	Loss 0.0102 (0.0110)	
training:	Epoch: [73][74/233]	Loss 0.0097 (0.0110)	
training:	Epoch: [73][75/233]	Loss 0.0098 (0.0110)	
training:	Epoch: [73][76/233]	Loss 0.0097 (0.0110)	
training:	Epoch: [73][77/233]	Loss 0.0112 (0.0110)	
training:	Epoch: [73][78/233]	Loss 0.0096 (0.0110)	
training:	Epoch: [73][79/233]	Loss 0.0128 (0.0110)	
training:	Epoch: [73][80/233]	Loss 0.0119 (0.0110)	
training:	Epoch: [73][81/233]	Loss 0.0101 (0.0110)	
training:	Epoch: [73][82/233]	Loss 0.0091 (0.0110)	
training:	Epoch: [73][83/233]	Loss 0.0096 (0.0109)	
training:	Epoch: [73][84/233]	Loss 0.0098 (0.0109)	
training:	Epoch: [73][85/233]	Loss 0.0089 (0.0109)	
training:	Epoch: [73][86/233]	Loss 0.0091 (0.0109)	
training:	Epoch: [73][87/233]	Loss 0.0105 (0.0109)	
training:	Epoch: [73][88/233]	Loss 0.0080 (0.0108)	
training:	Epoch: [73][89/233]	Loss 0.0094 (0.0108)	
training:	Epoch: [73][90/233]	Loss 0.0106 (0.0108)	
training:	Epoch: [73][91/233]	Loss 0.0106 (0.0108)	
training:	Epoch: [73][92/233]	Loss 0.0092 (0.0108)	
training:	Epoch: [73][93/233]	Loss 0.0092 (0.0108)	
training:	Epoch: [73][94/233]	Loss 0.0098 (0.0108)	
training:	Epoch: [73][95/233]	Loss 0.0143 (0.0108)	
training:	Epoch: [73][96/233]	Loss 0.0105 (0.0108)	
training:	Epoch: [73][97/233]	Loss 0.0128 (0.0108)	
training:	Epoch: [73][98/233]	Loss 0.0104 (0.0108)	
training:	Epoch: [73][99/233]	Loss 0.0088 (0.0108)	
training:	Epoch: [73][100/233]	Loss 0.0086 (0.0108)	
training:	Epoch: [73][101/233]	Loss 0.0093 (0.0108)	
training:	Epoch: [73][102/233]	Loss 0.0125 (0.0108)	
training:	Epoch: [73][103/233]	Loss 0.0108 (0.0108)	
training:	Epoch: [73][104/233]	Loss 0.0091 (0.0108)	
training:	Epoch: [73][105/233]	Loss 0.0101 (0.0108)	
training:	Epoch: [73][106/233]	Loss 0.0113 (0.0108)	
training:	Epoch: [73][107/233]	Loss 0.0169 (0.0108)	
training:	Epoch: [73][108/233]	Loss 0.0177 (0.0109)	
training:	Epoch: [73][109/233]	Loss 0.0143 (0.0109)	
training:	Epoch: [73][110/233]	Loss 0.0099 (0.0109)	
training:	Epoch: [73][111/233]	Loss 0.0133 (0.0109)	
training:	Epoch: [73][112/233]	Loss 0.0123 (0.0109)	
training:	Epoch: [73][113/233]	Loss 0.0195 (0.0110)	
training:	Epoch: [73][114/233]	Loss 0.0201 (0.0111)	
training:	Epoch: [73][115/233]	Loss 0.0091 (0.0111)	
training:	Epoch: [73][116/233]	Loss 0.0108 (0.0111)	
training:	Epoch: [73][117/233]	Loss 0.0217 (0.0112)	
training:	Epoch: [73][118/233]	Loss 0.0155 (0.0112)	
training:	Epoch: [73][119/233]	Loss 0.0086 (0.0112)	
training:	Epoch: [73][120/233]	Loss 0.0095 (0.0112)	
training:	Epoch: [73][121/233]	Loss 0.0104 (0.0112)	
training:	Epoch: [73][122/233]	Loss 0.0120 (0.0112)	
training:	Epoch: [73][123/233]	Loss 0.0107 (0.0112)	
training:	Epoch: [73][124/233]	Loss 0.0096 (0.0112)	
training:	Epoch: [73][125/233]	Loss 0.0162 (0.0112)	
training:	Epoch: [73][126/233]	Loss 0.0084 (0.0112)	
training:	Epoch: [73][127/233]	Loss 0.0117 (0.0112)	
training:	Epoch: [73][128/233]	Loss 0.0100 (0.0112)	
training:	Epoch: [73][129/233]	Loss 0.0152 (0.0112)	
training:	Epoch: [73][130/233]	Loss 0.0143 (0.0112)	
training:	Epoch: [73][131/233]	Loss 0.0295 (0.0114)	
training:	Epoch: [73][132/233]	Loss 0.0106 (0.0114)	
training:	Epoch: [73][133/233]	Loss 0.0116 (0.0114)	
training:	Epoch: [73][134/233]	Loss 0.0149 (0.0114)	
training:	Epoch: [73][135/233]	Loss 0.0220 (0.0115)	
training:	Epoch: [73][136/233]	Loss 0.0172 (0.0115)	
training:	Epoch: [73][137/233]	Loss 0.0084 (0.0115)	
training:	Epoch: [73][138/233]	Loss 0.0131 (0.0115)	
training:	Epoch: [73][139/233]	Loss 0.0088 (0.0115)	
training:	Epoch: [73][140/233]	Loss 0.0087 (0.0115)	
training:	Epoch: [73][141/233]	Loss 0.0113 (0.0115)	
training:	Epoch: [73][142/233]	Loss 0.0085 (0.0114)	
training:	Epoch: [73][143/233]	Loss 0.0120 (0.0114)	
training:	Epoch: [73][144/233]	Loss 0.0114 (0.0114)	
training:	Epoch: [73][145/233]	Loss 0.0111 (0.0114)	
training:	Epoch: [73][146/233]	Loss 0.0092 (0.0114)	
training:	Epoch: [73][147/233]	Loss 0.0092 (0.0114)	
training:	Epoch: [73][148/233]	Loss 0.0104 (0.0114)	
training:	Epoch: [73][149/233]	Loss 0.0090 (0.0114)	
training:	Epoch: [73][150/233]	Loss 0.0107 (0.0114)	
training:	Epoch: [73][151/233]	Loss 0.0083 (0.0114)	
training:	Epoch: [73][152/233]	Loss 0.0126 (0.0114)	
training:	Epoch: [73][153/233]	Loss 0.0098 (0.0114)	
training:	Epoch: [73][154/233]	Loss 0.0116 (0.0114)	
training:	Epoch: [73][155/233]	Loss 0.0147 (0.0114)	
training:	Epoch: [73][156/233]	Loss 0.0136 (0.0114)	
training:	Epoch: [73][157/233]	Loss 0.0115 (0.0114)	
training:	Epoch: [73][158/233]	Loss 0.0286 (0.0115)	
training:	Epoch: [73][159/233]	Loss 0.0085 (0.0115)	
training:	Epoch: [73][160/233]	Loss 0.0112 (0.0115)	
training:	Epoch: [73][161/233]	Loss 0.0099 (0.0115)	
training:	Epoch: [73][162/233]	Loss 0.0094 (0.0115)	
training:	Epoch: [73][163/233]	Loss 0.0095 (0.0114)	
training:	Epoch: [73][164/233]	Loss 0.0129 (0.0115)	
training:	Epoch: [73][165/233]	Loss 0.0122 (0.0115)	
training:	Epoch: [73][166/233]	Loss 0.0093 (0.0114)	
training:	Epoch: [73][167/233]	Loss 0.0108 (0.0114)	
training:	Epoch: [73][168/233]	Loss 0.0200 (0.0115)	
training:	Epoch: [73][169/233]	Loss 0.0087 (0.0115)	
training:	Epoch: [73][170/233]	Loss 0.0095 (0.0115)	
training:	Epoch: [73][171/233]	Loss 0.0130 (0.0115)	
training:	Epoch: [73][172/233]	Loss 0.0094 (0.0115)	
training:	Epoch: [73][173/233]	Loss 0.0079 (0.0114)	
training:	Epoch: [73][174/233]	Loss 0.0099 (0.0114)	
training:	Epoch: [73][175/233]	Loss 0.0104 (0.0114)	
training:	Epoch: [73][176/233]	Loss 0.0209 (0.0115)	
training:	Epoch: [73][177/233]	Loss 0.0090 (0.0115)	
training:	Epoch: [73][178/233]	Loss 0.0107 (0.0115)	
training:	Epoch: [73][179/233]	Loss 0.0125 (0.0115)	
training:	Epoch: [73][180/233]	Loss 0.0132 (0.0115)	
training:	Epoch: [73][181/233]	Loss 0.0084 (0.0115)	
training:	Epoch: [73][182/233]	Loss 0.0099 (0.0115)	
training:	Epoch: [73][183/233]	Loss 0.0116 (0.0115)	
training:	Epoch: [73][184/233]	Loss 0.0096 (0.0114)	
training:	Epoch: [73][185/233]	Loss 0.0161 (0.0115)	
training:	Epoch: [73][186/233]	Loss 0.0184 (0.0115)	
training:	Epoch: [73][187/233]	Loss 0.0091 (0.0115)	
training:	Epoch: [73][188/233]	Loss 0.0104 (0.0115)	
training:	Epoch: [73][189/233]	Loss 0.0131 (0.0115)	
training:	Epoch: [73][190/233]	Loss 0.0100 (0.0115)	
training:	Epoch: [73][191/233]	Loss 0.0110 (0.0115)	
training:	Epoch: [73][192/233]	Loss 0.0085 (0.0115)	
training:	Epoch: [73][193/233]	Loss 0.0084 (0.0115)	
training:	Epoch: [73][194/233]	Loss 0.0094 (0.0114)	
training:	Epoch: [73][195/233]	Loss 0.0089 (0.0114)	
training:	Epoch: [73][196/233]	Loss 0.0091 (0.0114)	
training:	Epoch: [73][197/233]	Loss 0.0095 (0.0114)	
training:	Epoch: [73][198/233]	Loss 0.0121 (0.0114)	
training:	Epoch: [73][199/233]	Loss 0.0090 (0.0114)	
training:	Epoch: [73][200/233]	Loss 0.0095 (0.0114)	
training:	Epoch: [73][201/233]	Loss 0.0086 (0.0114)	
training:	Epoch: [73][202/233]	Loss 0.0127 (0.0114)	
training:	Epoch: [73][203/233]	Loss 0.0098 (0.0114)	
training:	Epoch: [73][204/233]	Loss 0.0091 (0.0114)	
training:	Epoch: [73][205/233]	Loss 0.0093 (0.0114)	
training:	Epoch: [73][206/233]	Loss 0.0132 (0.0114)	
training:	Epoch: [73][207/233]	Loss 0.0099 (0.0114)	
training:	Epoch: [73][208/233]	Loss 0.0092 (0.0113)	
training:	Epoch: [73][209/233]	Loss 0.0102 (0.0113)	
training:	Epoch: [73][210/233]	Loss 0.0111 (0.0113)	
training:	Epoch: [73][211/233]	Loss 0.0088 (0.0113)	
training:	Epoch: [73][212/233]	Loss 0.0090 (0.0113)	
training:	Epoch: [73][213/233]	Loss 0.0095 (0.0113)	
training:	Epoch: [73][214/233]	Loss 0.0167 (0.0113)	
training:	Epoch: [73][215/233]	Loss 0.0130 (0.0113)	
training:	Epoch: [73][216/233]	Loss 0.0112 (0.0113)	
training:	Epoch: [73][217/233]	Loss 0.0100 (0.0113)	
training:	Epoch: [73][218/233]	Loss 0.0140 (0.0113)	
training:	Epoch: [73][219/233]	Loss 0.0100 (0.0113)	
training:	Epoch: [73][220/233]	Loss 0.0116 (0.0113)	
training:	Epoch: [73][221/233]	Loss 0.0082 (0.0113)	
training:	Epoch: [73][222/233]	Loss 0.0436 (0.0115)	
training:	Epoch: [73][223/233]	Loss 0.0111 (0.0115)	
training:	Epoch: [73][224/233]	Loss 0.0092 (0.0115)	
training:	Epoch: [73][225/233]	Loss 0.0105 (0.0115)	
training:	Epoch: [73][226/233]	Loss 0.0128 (0.0115)	
training:	Epoch: [73][227/233]	Loss 0.0101 (0.0115)	
training:	Epoch: [73][228/233]	Loss 0.0120 (0.0115)	
training:	Epoch: [73][229/233]	Loss 0.0087 (0.0114)	
training:	Epoch: [73][230/233]	Loss 0.0130 (0.0115)	
training:	Epoch: [73][231/233]	Loss 0.0105 (0.0115)	
training:	Epoch: [73][232/233]	Loss 0.0141 (0.0115)	
training:	Epoch: [73][233/233]	Loss 0.0105 (0.0115)	
Training:	 Loss: 0.0114

Training:	 ACC: 1.0000 1.0000 1.0000 1.0000
Validation:	 ACC: 0.8623 0.8630 0.8792 0.8453
Validation:	 Best_BACC: 0.8682 0.8678 0.8598 0.8767
Validation:	 Loss: 0.3248
Pretraining:	Epoch 74/200
----------
training:	Epoch: [74][1/233]	Loss 0.0081 (0.0081)	
training:	Epoch: [74][2/233]	Loss 0.0101 (0.0091)	
training:	Epoch: [74][3/233]	Loss 0.0095 (0.0092)	
training:	Epoch: [74][4/233]	Loss 0.0185 (0.0115)	
training:	Epoch: [74][5/233]	Loss 0.0131 (0.0119)	
training:	Epoch: [74][6/233]	Loss 0.0084 (0.0113)	
training:	Epoch: [74][7/233]	Loss 0.0101 (0.0111)	
training:	Epoch: [74][8/233]	Loss 0.0239 (0.0127)	
training:	Epoch: [74][9/233]	Loss 0.0122 (0.0127)	
training:	Epoch: [74][10/233]	Loss 0.0194 (0.0133)	
training:	Epoch: [74][11/233]	Loss 0.0105 (0.0131)	
training:	Epoch: [74][12/233]	Loss 0.0083 (0.0127)	
training:	Epoch: [74][13/233]	Loss 0.0086 (0.0124)	
training:	Epoch: [74][14/233]	Loss 0.0093 (0.0121)	
training:	Epoch: [74][15/233]	Loss 0.0114 (0.0121)	
training:	Epoch: [74][16/233]	Loss 0.0107 (0.0120)	
training:	Epoch: [74][17/233]	Loss 0.0102 (0.0119)	
training:	Epoch: [74][18/233]	Loss 0.0086 (0.0117)	
training:	Epoch: [74][19/233]	Loss 0.0100 (0.0116)	
training:	Epoch: [74][20/233]	Loss 0.0175 (0.0119)	
training:	Epoch: [74][21/233]	Loss 0.0311 (0.0128)	
training:	Epoch: [74][22/233]	Loss 0.0086 (0.0126)	
training:	Epoch: [74][23/233]	Loss 0.0107 (0.0126)	
training:	Epoch: [74][24/233]	Loss 0.0093 (0.0124)	
training:	Epoch: [74][25/233]	Loss 0.0087 (0.0123)	
training:	Epoch: [74][26/233]	Loss 0.0081 (0.0121)	
training:	Epoch: [74][27/233]	Loss 0.0191 (0.0124)	
training:	Epoch: [74][28/233]	Loss 0.0203 (0.0127)	
training:	Epoch: [74][29/233]	Loss 0.0084 (0.0125)	
training:	Epoch: [74][30/233]	Loss 0.0104 (0.0124)	
training:	Epoch: [74][31/233]	Loss 0.0092 (0.0123)	
training:	Epoch: [74][32/233]	Loss 0.0100 (0.0123)	
training:	Epoch: [74][33/233]	Loss 0.0122 (0.0123)	
training:	Epoch: [74][34/233]	Loss 0.0113 (0.0122)	
training:	Epoch: [74][35/233]	Loss 0.0091 (0.0121)	
training:	Epoch: [74][36/233]	Loss 0.0136 (0.0122)	
training:	Epoch: [74][37/233]	Loss 0.0320 (0.0127)	
training:	Epoch: [74][38/233]	Loss 0.0083 (0.0126)	
training:	Epoch: [74][39/233]	Loss 0.0086 (0.0125)	
training:	Epoch: [74][40/233]	Loss 0.0098 (0.0124)	
training:	Epoch: [74][41/233]	Loss 0.0160 (0.0125)	
training:	Epoch: [74][42/233]	Loss 0.0106 (0.0125)	
training:	Epoch: [74][43/233]	Loss 0.0127 (0.0125)	
training:	Epoch: [74][44/233]	Loss 0.0104 (0.0124)	
training:	Epoch: [74][45/233]	Loss 0.0141 (0.0125)	
training:	Epoch: [74][46/233]	Loss 0.0085 (0.0124)	
training:	Epoch: [74][47/233]	Loss 0.0095 (0.0123)	
training:	Epoch: [74][48/233]	Loss 0.0088 (0.0122)	
training:	Epoch: [74][49/233]	Loss 0.0117 (0.0122)	
training:	Epoch: [74][50/233]	Loss 0.0149 (0.0123)	
training:	Epoch: [74][51/233]	Loss 0.0122 (0.0123)	
training:	Epoch: [74][52/233]	Loss 0.0104 (0.0123)	
training:	Epoch: [74][53/233]	Loss 0.0086 (0.0122)	
training:	Epoch: [74][54/233]	Loss 0.0088 (0.0121)	
training:	Epoch: [74][55/233]	Loss 0.0106 (0.0121)	
training:	Epoch: [74][56/233]	Loss 0.0089 (0.0120)	
training:	Epoch: [74][57/233]	Loss 0.0116 (0.0120)	
training:	Epoch: [74][58/233]	Loss 0.0095 (0.0120)	
training:	Epoch: [74][59/233]	Loss 0.0118 (0.0120)	
training:	Epoch: [74][60/233]	Loss 0.0087 (0.0119)	
training:	Epoch: [74][61/233]	Loss 0.0145 (0.0120)	
training:	Epoch: [74][62/233]	Loss 0.0090 (0.0119)	
training:	Epoch: [74][63/233]	Loss 0.0131 (0.0119)	
training:	Epoch: [74][64/233]	Loss 0.0091 (0.0119)	
training:	Epoch: [74][65/233]	Loss 0.0092 (0.0119)	
training:	Epoch: [74][66/233]	Loss 0.0109 (0.0118)	
training:	Epoch: [74][67/233]	Loss 0.0086 (0.0118)	
training:	Epoch: [74][68/233]	Loss 0.0123 (0.0118)	
training:	Epoch: [74][69/233]	Loss 0.0097 (0.0118)	
training:	Epoch: [74][70/233]	Loss 0.0151 (0.0118)	
training:	Epoch: [74][71/233]	Loss 0.0086 (0.0118)	
training:	Epoch: [74][72/233]	Loss 0.0241 (0.0119)	
training:	Epoch: [74][73/233]	Loss 0.0084 (0.0119)	
training:	Epoch: [74][74/233]	Loss 0.0127 (0.0119)	
training:	Epoch: [74][75/233]	Loss 0.0159 (0.0120)	
training:	Epoch: [74][76/233]	Loss 0.0141 (0.0120)	
training:	Epoch: [74][77/233]	Loss 0.0150 (0.0120)	
training:	Epoch: [74][78/233]	Loss 0.0081 (0.0120)	
training:	Epoch: [74][79/233]	Loss 0.0086 (0.0119)	
training:	Epoch: [74][80/233]	Loss 0.0231 (0.0121)	
training:	Epoch: [74][81/233]	Loss 0.0096 (0.0120)	
training:	Epoch: [74][82/233]	Loss 0.0117 (0.0120)	
training:	Epoch: [74][83/233]	Loss 0.0096 (0.0120)	
training:	Epoch: [74][84/233]	Loss 0.0092 (0.0120)	
training:	Epoch: [74][85/233]	Loss 0.0089 (0.0119)	
training:	Epoch: [74][86/233]	Loss 0.0088 (0.0119)	
training:	Epoch: [74][87/233]	Loss 0.0108 (0.0119)	
training:	Epoch: [74][88/233]	Loss 0.0148 (0.0119)	
training:	Epoch: [74][89/233]	Loss 0.0089 (0.0119)	
training:	Epoch: [74][90/233]	Loss 0.0224 (0.0120)	
training:	Epoch: [74][91/233]	Loss 0.0121 (0.0120)	
training:	Epoch: [74][92/233]	Loss 0.0088 (0.0120)	
training:	Epoch: [74][93/233]	Loss 0.0086 (0.0119)	
training:	Epoch: [74][94/233]	Loss 0.0095 (0.0119)	
training:	Epoch: [74][95/233]	Loss 0.0095 (0.0119)	
training:	Epoch: [74][96/233]	Loss 0.0108 (0.0119)	
training:	Epoch: [74][97/233]	Loss 0.0093 (0.0118)	
training:	Epoch: [74][98/233]	Loss 0.0086 (0.0118)	
training:	Epoch: [74][99/233]	Loss 0.0089 (0.0118)	
training:	Epoch: [74][100/233]	Loss 0.0083 (0.0117)	
training:	Epoch: [74][101/233]	Loss 0.0084 (0.0117)	
training:	Epoch: [74][102/233]	Loss 0.0318 (0.0119)	
training:	Epoch: [74][103/233]	Loss 0.0091 (0.0119)	
training:	Epoch: [74][104/233]	Loss 0.0134 (0.0119)	
training:	Epoch: [74][105/233]	Loss 0.0199 (0.0120)	
training:	Epoch: [74][106/233]	Loss 0.0084 (0.0119)	
training:	Epoch: [74][107/233]	Loss 0.0082 (0.0119)	
training:	Epoch: [74][108/233]	Loss 0.0093 (0.0119)	
training:	Epoch: [74][109/233]	Loss 0.0212 (0.0120)	
training:	Epoch: [74][110/233]	Loss 0.0095 (0.0119)	
training:	Epoch: [74][111/233]	Loss 0.0191 (0.0120)	
training:	Epoch: [74][112/233]	Loss 0.0120 (0.0120)	
training:	Epoch: [74][113/233]	Loss 0.0110 (0.0120)	
training:	Epoch: [74][114/233]	Loss 0.0088 (0.0120)	
training:	Epoch: [74][115/233]	Loss 0.0100 (0.0120)	
training:	Epoch: [74][116/233]	Loss 0.0102 (0.0119)	
training:	Epoch: [74][117/233]	Loss 0.0084 (0.0119)	
training:	Epoch: [74][118/233]	Loss 0.0133 (0.0119)	
training:	Epoch: [74][119/233]	Loss 0.0090 (0.0119)	
training:	Epoch: [74][120/233]	Loss 0.0444 (0.0122)	
training:	Epoch: [74][121/233]	Loss 0.0217 (0.0122)	
training:	Epoch: [74][122/233]	Loss 0.0102 (0.0122)	
training:	Epoch: [74][123/233]	Loss 0.0093 (0.0122)	
training:	Epoch: [74][124/233]	Loss 0.0105 (0.0122)	
training:	Epoch: [74][125/233]	Loss 0.0089 (0.0122)	
training:	Epoch: [74][126/233]	Loss 0.0080 (0.0121)	
training:	Epoch: [74][127/233]	Loss 0.0105 (0.0121)	
training:	Epoch: [74][128/233]	Loss 0.0104 (0.0121)	
training:	Epoch: [74][129/233]	Loss 0.0105 (0.0121)	
training:	Epoch: [74][130/233]	Loss 0.0199 (0.0122)	
training:	Epoch: [74][131/233]	Loss 0.0096 (0.0121)	
training:	Epoch: [74][132/233]	Loss 0.0132 (0.0121)	
training:	Epoch: [74][133/233]	Loss 0.0122 (0.0121)	
training:	Epoch: [74][134/233]	Loss 0.0094 (0.0121)	
training:	Epoch: [74][135/233]	Loss 0.0087 (0.0121)	
training:	Epoch: [74][136/233]	Loss 0.0086 (0.0121)	
training:	Epoch: [74][137/233]	Loss 0.0088 (0.0120)	
training:	Epoch: [74][138/233]	Loss 0.0111 (0.0120)	
training:	Epoch: [74][139/233]	Loss 0.0117 (0.0120)	
training:	Epoch: [74][140/233]	Loss 0.0103 (0.0120)	
training:	Epoch: [74][141/233]	Loss 0.0088 (0.0120)	
training:	Epoch: [74][142/233]	Loss 0.0089 (0.0120)	
training:	Epoch: [74][143/233]	Loss 0.0085 (0.0120)	
training:	Epoch: [74][144/233]	Loss 0.0093 (0.0119)	
training:	Epoch: [74][145/233]	Loss 0.0112 (0.0119)	
training:	Epoch: [74][146/233]	Loss 0.0107 (0.0119)	
training:	Epoch: [74][147/233]	Loss 0.0198 (0.0120)	
training:	Epoch: [74][148/233]	Loss 0.0260 (0.0121)	
training:	Epoch: [74][149/233]	Loss 0.0109 (0.0121)	
training:	Epoch: [74][150/233]	Loss 0.0120 (0.0121)	
training:	Epoch: [74][151/233]	Loss 0.0136 (0.0121)	
training:	Epoch: [74][152/233]	Loss 0.0092 (0.0121)	
training:	Epoch: [74][153/233]	Loss 0.0134 (0.0121)	
training:	Epoch: [74][154/233]	Loss 0.0104 (0.0121)	
training:	Epoch: [74][155/233]	Loss 0.0110 (0.0120)	
training:	Epoch: [74][156/233]	Loss 0.0078 (0.0120)	
training:	Epoch: [74][157/233]	Loss 0.0119 (0.0120)	
training:	Epoch: [74][158/233]	Loss 0.0121 (0.0120)	
training:	Epoch: [74][159/233]	Loss 0.0084 (0.0120)	
training:	Epoch: [74][160/233]	Loss 0.0097 (0.0120)	
training:	Epoch: [74][161/233]	Loss 0.0101 (0.0120)	
training:	Epoch: [74][162/233]	Loss 0.0232 (0.0120)	
training:	Epoch: [74][163/233]	Loss 0.0138 (0.0120)	
training:	Epoch: [74][164/233]	Loss 0.0094 (0.0120)	
training:	Epoch: [74][165/233]	Loss 0.0112 (0.0120)	
training:	Epoch: [74][166/233]	Loss 0.0128 (0.0120)	
training:	Epoch: [74][167/233]	Loss 0.0112 (0.0120)	
training:	Epoch: [74][168/233]	Loss 0.0085 (0.0120)	
training:	Epoch: [74][169/233]	Loss 0.0084 (0.0120)	
training:	Epoch: [74][170/233]	Loss 0.0085 (0.0120)	
training:	Epoch: [74][171/233]	Loss 0.0096 (0.0120)	
training:	Epoch: [74][172/233]	Loss 0.0081 (0.0119)	
training:	Epoch: [74][173/233]	Loss 0.0088 (0.0119)	
training:	Epoch: [74][174/233]	Loss 0.0105 (0.0119)	
training:	Epoch: [74][175/233]	Loss 0.0098 (0.0119)	
training:	Epoch: [74][176/233]	Loss 0.0092 (0.0119)	
training:	Epoch: [74][177/233]	Loss 0.0099 (0.0119)	
training:	Epoch: [74][178/233]	Loss 0.0122 (0.0119)	
training:	Epoch: [74][179/233]	Loss 0.0089 (0.0119)	
training:	Epoch: [74][180/233]	Loss 0.0186 (0.0119)	
training:	Epoch: [74][181/233]	Loss 0.0106 (0.0119)	
training:	Epoch: [74][182/233]	Loss 0.0110 (0.0119)	
training:	Epoch: [74][183/233]	Loss 0.0082 (0.0119)	
training:	Epoch: [74][184/233]	Loss 0.0095 (0.0118)	
training:	Epoch: [74][185/233]	Loss 0.0145 (0.0119)	
training:	Epoch: [74][186/233]	Loss 0.0102 (0.0118)	
training:	Epoch: [74][187/233]	Loss 0.0097 (0.0118)	
training:	Epoch: [74][188/233]	Loss 0.0154 (0.0119)	
training:	Epoch: [74][189/233]	Loss 0.0109 (0.0119)	
training:	Epoch: [74][190/233]	Loss 0.0094 (0.0118)	
training:	Epoch: [74][191/233]	Loss 0.0095 (0.0118)	
training:	Epoch: [74][192/233]	Loss 0.0176 (0.0119)	
training:	Epoch: [74][193/233]	Loss 0.0106 (0.0118)	
training:	Epoch: [74][194/233]	Loss 0.0195 (0.0119)	
training:	Epoch: [74][195/233]	Loss 0.0107 (0.0119)	
training:	Epoch: [74][196/233]	Loss 0.0120 (0.0119)	
training:	Epoch: [74][197/233]	Loss 0.0095 (0.0119)	
training:	Epoch: [74][198/233]	Loss 0.0129 (0.0119)	
training:	Epoch: [74][199/233]	Loss 0.0105 (0.0119)	
training:	Epoch: [74][200/233]	Loss 0.0085 (0.0119)	
training:	Epoch: [74][201/233]	Loss 0.0102 (0.0118)	
training:	Epoch: [74][202/233]	Loss 0.0092 (0.0118)	
training:	Epoch: [74][203/233]	Loss 0.0097 (0.0118)	
training:	Epoch: [74][204/233]	Loss 0.0105 (0.0118)	
training:	Epoch: [74][205/233]	Loss 0.0108 (0.0118)	
training:	Epoch: [74][206/233]	Loss 0.0090 (0.0118)	
training:	Epoch: [74][207/233]	Loss 0.0215 (0.0118)	
training:	Epoch: [74][208/233]	Loss 0.0092 (0.0118)	
training:	Epoch: [74][209/233]	Loss 0.0099 (0.0118)	
training:	Epoch: [74][210/233]	Loss 0.0140 (0.0118)	
training:	Epoch: [74][211/233]	Loss 0.0103 (0.0118)	
training:	Epoch: [74][212/233]	Loss 0.0086 (0.0118)	
training:	Epoch: [74][213/233]	Loss 0.0093 (0.0118)	
training:	Epoch: [74][214/233]	Loss 0.0084 (0.0118)	
training:	Epoch: [74][215/233]	Loss 0.0106 (0.0118)	
training:	Epoch: [74][216/233]	Loss 0.0103 (0.0118)	
training:	Epoch: [74][217/233]	Loss 0.0108 (0.0118)	
training:	Epoch: [74][218/233]	Loss 0.0088 (0.0117)	
training:	Epoch: [74][219/233]	Loss 0.0093 (0.0117)	
training:	Epoch: [74][220/233]	Loss 0.0086 (0.0117)	
training:	Epoch: [74][221/233]	Loss 0.0105 (0.0117)	
training:	Epoch: [74][222/233]	Loss 0.0120 (0.0117)	
training:	Epoch: [74][223/233]	Loss 0.0087 (0.0117)	
training:	Epoch: [74][224/233]	Loss 0.0092 (0.0117)	
training:	Epoch: [74][225/233]	Loss 0.0124 (0.0117)	
training:	Epoch: [74][226/233]	Loss 0.0253 (0.0118)	
training:	Epoch: [74][227/233]	Loss 0.0093 (0.0117)	
training:	Epoch: [74][228/233]	Loss 0.0095 (0.0117)	
training:	Epoch: [74][229/233]	Loss 0.0124 (0.0117)	
training:	Epoch: [74][230/233]	Loss 0.0090 (0.0117)	
training:	Epoch: [74][231/233]	Loss 0.0129 (0.0117)	
training:	Epoch: [74][232/233]	Loss 0.0087 (0.0117)	
training:	Epoch: [74][233/233]	Loss 0.0107 (0.0117)	
Training:	 Loss: 0.0117

Training:	 ACC: 1.0000 1.0000 1.0000 1.0000
Validation:	 ACC: 0.8592 0.8582 0.8373 0.8812
Validation:	 Best_BACC: 0.8682 0.8678 0.8598 0.8767
Validation:	 Loss: 0.3314
Pretraining:	Epoch 75/200
----------
training:	Epoch: [75][1/233]	Loss 0.0097 (0.0097)	
training:	Epoch: [75][2/233]	Loss 0.0089 (0.0093)	
training:	Epoch: [75][3/233]	Loss 0.0089 (0.0092)	
training:	Epoch: [75][4/233]	Loss 0.0107 (0.0095)	
training:	Epoch: [75][5/233]	Loss 0.0084 (0.0093)	
training:	Epoch: [75][6/233]	Loss 0.0098 (0.0094)	
training:	Epoch: [75][7/233]	Loss 0.0139 (0.0100)	
training:	Epoch: [75][8/233]	Loss 0.0087 (0.0099)	
training:	Epoch: [75][9/233]	Loss 0.0240 (0.0114)	
training:	Epoch: [75][10/233]	Loss 0.0114 (0.0114)	
training:	Epoch: [75][11/233]	Loss 0.0145 (0.0117)	
training:	Epoch: [75][12/233]	Loss 0.0124 (0.0118)	
training:	Epoch: [75][13/233]	Loss 0.0163 (0.0121)	
training:	Epoch: [75][14/233]	Loss 0.0162 (0.0124)	
training:	Epoch: [75][15/233]	Loss 0.0092 (0.0122)	
training:	Epoch: [75][16/233]	Loss 0.0084 (0.0120)	
training:	Epoch: [75][17/233]	Loss 0.0104 (0.0119)	
training:	Epoch: [75][18/233]	Loss 0.0138 (0.0120)	
training:	Epoch: [75][19/233]	Loss 0.0113 (0.0119)	
training:	Epoch: [75][20/233]	Loss 0.0094 (0.0118)	
training:	Epoch: [75][21/233]	Loss 0.0240 (0.0124)	
training:	Epoch: [75][22/233]	Loss 0.0087 (0.0122)	
training:	Epoch: [75][23/233]	Loss 0.0085 (0.0121)	
training:	Epoch: [75][24/233]	Loss 0.0089 (0.0119)	
training:	Epoch: [75][25/233]	Loss 0.0079 (0.0118)	
training:	Epoch: [75][26/233]	Loss 0.0088 (0.0117)	
training:	Epoch: [75][27/233]	Loss 0.0118 (0.0117)	
training:	Epoch: [75][28/233]	Loss 0.0150 (0.0118)	
training:	Epoch: [75][29/233]	Loss 0.0089 (0.0117)	
training:	Epoch: [75][30/233]	Loss 0.0089 (0.0116)	
training:	Epoch: [75][31/233]	Loss 0.0135 (0.0117)	
training:	Epoch: [75][32/233]	Loss 0.0109 (0.0116)	
training:	Epoch: [75][33/233]	Loss 0.0086 (0.0115)	
training:	Epoch: [75][34/233]	Loss 0.0089 (0.0115)	
training:	Epoch: [75][35/233]	Loss 0.0088 (0.0114)	
training:	Epoch: [75][36/233]	Loss 0.0097 (0.0113)	
training:	Epoch: [75][37/233]	Loss 0.0139 (0.0114)	
training:	Epoch: [75][38/233]	Loss 0.0142 (0.0115)	
training:	Epoch: [75][39/233]	Loss 0.0159 (0.0116)	
training:	Epoch: [75][40/233]	Loss 0.0101 (0.0116)	
training:	Epoch: [75][41/233]	Loss 0.0100 (0.0115)	
training:	Epoch: [75][42/233]	Loss 0.0121 (0.0115)	
training:	Epoch: [75][43/233]	Loss 0.0142 (0.0116)	
training:	Epoch: [75][44/233]	Loss 0.0095 (0.0115)	
training:	Epoch: [75][45/233]	Loss 0.0157 (0.0116)	
training:	Epoch: [75][46/233]	Loss 0.0092 (0.0116)	
training:	Epoch: [75][47/233]	Loss 0.0181 (0.0117)	
training:	Epoch: [75][48/233]	Loss 0.0116 (0.0117)	
training:	Epoch: [75][49/233]	Loss 0.0090 (0.0117)	
training:	Epoch: [75][50/233]	Loss 0.0102 (0.0116)	
training:	Epoch: [75][51/233]	Loss 0.0091 (0.0116)	
training:	Epoch: [75][52/233]	Loss 0.0086 (0.0115)	
training:	Epoch: [75][53/233]	Loss 0.0087 (0.0115)	
training:	Epoch: [75][54/233]	Loss 0.0089 (0.0114)	
training:	Epoch: [75][55/233]	Loss 0.0116 (0.0114)	
training:	Epoch: [75][56/233]	Loss 0.0122 (0.0114)	
training:	Epoch: [75][57/233]	Loss 0.0129 (0.0115)	
training:	Epoch: [75][58/233]	Loss 0.0089 (0.0114)	
training:	Epoch: [75][59/233]	Loss 0.0101 (0.0114)	
training:	Epoch: [75][60/233]	Loss 0.0080 (0.0113)	
training:	Epoch: [75][61/233]	Loss 0.0089 (0.0113)	
training:	Epoch: [75][62/233]	Loss 0.0122 (0.0113)	
training:	Epoch: [75][63/233]	Loss 0.0095 (0.0113)	
training:	Epoch: [75][64/233]	Loss 0.0106 (0.0113)	
training:	Epoch: [75][65/233]	Loss 0.0100 (0.0113)	
training:	Epoch: [75][66/233]	Loss 0.0083 (0.0112)	
training:	Epoch: [75][67/233]	Loss 0.0087 (0.0112)	
training:	Epoch: [75][68/233]	Loss 0.0219 (0.0113)	
training:	Epoch: [75][69/233]	Loss 0.0086 (0.0113)	
training:	Epoch: [75][70/233]	Loss 0.0207 (0.0114)	
training:	Epoch: [75][71/233]	Loss 0.0088 (0.0114)	
training:	Epoch: [75][72/233]	Loss 0.0104 (0.0114)	
training:	Epoch: [75][73/233]	Loss 0.0106 (0.0114)	
training:	Epoch: [75][74/233]	Loss 0.0085 (0.0113)	
training:	Epoch: [75][75/233]	Loss 0.0094 (0.0113)	
training:	Epoch: [75][76/233]	Loss 0.0144 (0.0113)	
training:	Epoch: [75][77/233]	Loss 0.0099 (0.0113)	
training:	Epoch: [75][78/233]	Loss 0.0103 (0.0113)	
training:	Epoch: [75][79/233]	Loss 0.0084 (0.0113)	
training:	Epoch: [75][80/233]	Loss 0.0092 (0.0113)	
training:	Epoch: [75][81/233]	Loss 0.0089 (0.0112)	
training:	Epoch: [75][82/233]	Loss 0.0087 (0.0112)	
training:	Epoch: [75][83/233]	Loss 0.0101 (0.0112)	
training:	Epoch: [75][84/233]	Loss 0.0153 (0.0112)	
training:	Epoch: [75][85/233]	Loss 0.0091 (0.0112)	
training:	Epoch: [75][86/233]	Loss 0.0117 (0.0112)	
training:	Epoch: [75][87/233]	Loss 0.0080 (0.0112)	
training:	Epoch: [75][88/233]	Loss 0.0098 (0.0112)	
training:	Epoch: [75][89/233]	Loss 0.0108 (0.0112)	
training:	Epoch: [75][90/233]	Loss 0.0089 (0.0111)	
training:	Epoch: [75][91/233]	Loss 0.0113 (0.0111)	
training:	Epoch: [75][92/233]	Loss 0.0089 (0.0111)	
training:	Epoch: [75][93/233]	Loss 0.0171 (0.0112)	
training:	Epoch: [75][94/233]	Loss 0.0089 (0.0111)	
training:	Epoch: [75][95/233]	Loss 0.0085 (0.0111)	
training:	Epoch: [75][96/233]	Loss 0.0096 (0.0111)	
training:	Epoch: [75][97/233]	Loss 0.0108 (0.0111)	
training:	Epoch: [75][98/233]	Loss 0.0082 (0.0111)	
training:	Epoch: [75][99/233]	Loss 0.0103 (0.0111)	
training:	Epoch: [75][100/233]	Loss 0.0134 (0.0111)	
training:	Epoch: [75][101/233]	Loss 0.0087 (0.0111)	
training:	Epoch: [75][102/233]	Loss 0.0084 (0.0110)	
training:	Epoch: [75][103/233]	Loss 0.0161 (0.0111)	
training:	Epoch: [75][104/233]	Loss 0.0091 (0.0111)	
training:	Epoch: [75][105/233]	Loss 0.0093 (0.0111)	
training:	Epoch: [75][106/233]	Loss 0.0085 (0.0110)	
training:	Epoch: [75][107/233]	Loss 0.0087 (0.0110)	
training:	Epoch: [75][108/233]	Loss 0.0103 (0.0110)	
training:	Epoch: [75][109/233]	Loss 0.0081 (0.0110)	
training:	Epoch: [75][110/233]	Loss 0.0116 (0.0110)	
training:	Epoch: [75][111/233]	Loss 0.0098 (0.0110)	
training:	Epoch: [75][112/233]	Loss 0.0288 (0.0111)	
training:	Epoch: [75][113/233]	Loss 0.0100 (0.0111)	
training:	Epoch: [75][114/233]	Loss 0.0083 (0.0111)	
training:	Epoch: [75][115/233]	Loss 0.0082 (0.0111)	
training:	Epoch: [75][116/233]	Loss 0.0151 (0.0111)	
training:	Epoch: [75][117/233]	Loss 0.0095 (0.0111)	
training:	Epoch: [75][118/233]	Loss 0.0092 (0.0111)	
training:	Epoch: [75][119/233]	Loss 0.0193 (0.0111)	
training:	Epoch: [75][120/233]	Loss 0.0176 (0.0112)	
training:	Epoch: [75][121/233]	Loss 0.0094 (0.0112)	
training:	Epoch: [75][122/233]	Loss 0.0087 (0.0112)	
training:	Epoch: [75][123/233]	Loss 0.0086 (0.0111)	
training:	Epoch: [75][124/233]	Loss 0.0087 (0.0111)	
training:	Epoch: [75][125/233]	Loss 0.0099 (0.0111)	
training:	Epoch: [75][126/233]	Loss 0.0122 (0.0111)	
training:	Epoch: [75][127/233]	Loss 0.0082 (0.0111)	
training:	Epoch: [75][128/233]	Loss 0.0093 (0.0111)	
training:	Epoch: [75][129/233]	Loss 0.0100 (0.0111)	
training:	Epoch: [75][130/233]	Loss 0.0120 (0.0111)	
training:	Epoch: [75][131/233]	Loss 0.0165 (0.0111)	
training:	Epoch: [75][132/233]	Loss 0.0091 (0.0111)	
training:	Epoch: [75][133/233]	Loss 0.0157 (0.0111)	
training:	Epoch: [75][134/233]	Loss 0.0117 (0.0111)	
training:	Epoch: [75][135/233]	Loss 0.0466 (0.0114)	
training:	Epoch: [75][136/233]	Loss 0.0095 (0.0114)	
training:	Epoch: [75][137/233]	Loss 0.0093 (0.0114)	
training:	Epoch: [75][138/233]	Loss 0.0093 (0.0114)	
training:	Epoch: [75][139/233]	Loss 0.0097 (0.0114)	
training:	Epoch: [75][140/233]	Loss 0.0114 (0.0114)	
training:	Epoch: [75][141/233]	Loss 0.0087 (0.0113)	
training:	Epoch: [75][142/233]	Loss 0.0096 (0.0113)	
training:	Epoch: [75][143/233]	Loss 0.0082 (0.0113)	
training:	Epoch: [75][144/233]	Loss 0.0102 (0.0113)	
training:	Epoch: [75][145/233]	Loss 0.0088 (0.0113)	
training:	Epoch: [75][146/233]	Loss 0.0075 (0.0112)	
training:	Epoch: [75][147/233]	Loss 0.0089 (0.0112)	
training:	Epoch: [75][148/233]	Loss 0.0138 (0.0112)	
training:	Epoch: [75][149/233]	Loss 0.0104 (0.0112)	
training:	Epoch: [75][150/233]	Loss 0.0117 (0.0112)	
training:	Epoch: [75][151/233]	Loss 0.0094 (0.0112)	
training:	Epoch: [75][152/233]	Loss 0.0114 (0.0112)	
training:	Epoch: [75][153/233]	Loss 0.0153 (0.0113)	
training:	Epoch: [75][154/233]	Loss 0.0119 (0.0113)	
training:	Epoch: [75][155/233]	Loss 0.0105 (0.0113)	
training:	Epoch: [75][156/233]	Loss 0.0119 (0.0113)	
training:	Epoch: [75][157/233]	Loss 0.0105 (0.0113)	
training:	Epoch: [75][158/233]	Loss 0.0090 (0.0112)	
training:	Epoch: [75][159/233]	Loss 0.0093 (0.0112)	
training:	Epoch: [75][160/233]	Loss 0.0080 (0.0112)	
training:	Epoch: [75][161/233]	Loss 0.0089 (0.0112)	
training:	Epoch: [75][162/233]	Loss 0.0101 (0.0112)	
training:	Epoch: [75][163/233]	Loss 0.0087 (0.0112)	
training:	Epoch: [75][164/233]	Loss 0.0105 (0.0112)	
training:	Epoch: [75][165/233]	Loss 0.0092 (0.0112)	
training:	Epoch: [75][166/233]	Loss 0.0088 (0.0111)	
training:	Epoch: [75][167/233]	Loss 0.0079 (0.0111)	
training:	Epoch: [75][168/233]	Loss 0.0208 (0.0112)	
training:	Epoch: [75][169/233]	Loss 0.0074 (0.0112)	
training:	Epoch: [75][170/233]	Loss 0.0120 (0.0112)	
training:	Epoch: [75][171/233]	Loss 0.0083 (0.0111)	
training:	Epoch: [75][172/233]	Loss 0.0085 (0.0111)	
training:	Epoch: [75][173/233]	Loss 0.0090 (0.0111)	
training:	Epoch: [75][174/233]	Loss 0.0087 (0.0111)	
training:	Epoch: [75][175/233]	Loss 0.0113 (0.0111)	
training:	Epoch: [75][176/233]	Loss 0.0091 (0.0111)	
training:	Epoch: [75][177/233]	Loss 0.0203 (0.0111)	
training:	Epoch: [75][178/233]	Loss 0.0091 (0.0111)	
training:	Epoch: [75][179/233]	Loss 0.0093 (0.0111)	
training:	Epoch: [75][180/233]	Loss 0.0103 (0.0111)	
training:	Epoch: [75][181/233]	Loss 0.0105 (0.0111)	
training:	Epoch: [75][182/233]	Loss 0.0091 (0.0111)	
training:	Epoch: [75][183/233]	Loss 0.0088 (0.0111)	
training:	Epoch: [75][184/233]	Loss 0.0115 (0.0111)	
training:	Epoch: [75][185/233]	Loss 0.0081 (0.0111)	
training:	Epoch: [75][186/233]	Loss 0.0092 (0.0111)	
training:	Epoch: [75][187/233]	Loss 0.0093 (0.0111)	
training:	Epoch: [75][188/233]	Loss 0.0134 (0.0111)	
training:	Epoch: [75][189/233]	Loss 0.0099 (0.0111)	
training:	Epoch: [75][190/233]	Loss 0.0087 (0.0111)	
training:	Epoch: [75][191/233]	Loss 0.0144 (0.0111)	
training:	Epoch: [75][192/233]	Loss 0.0131 (0.0111)	
training:	Epoch: [75][193/233]	Loss 0.0120 (0.0111)	
training:	Epoch: [75][194/233]	Loss 0.0094 (0.0111)	
training:	Epoch: [75][195/233]	Loss 0.0086 (0.0111)	
training:	Epoch: [75][196/233]	Loss 0.0085 (0.0111)	
training:	Epoch: [75][197/233]	Loss 0.0093 (0.0110)	
training:	Epoch: [75][198/233]	Loss 0.0091 (0.0110)	
training:	Epoch: [75][199/233]	Loss 0.0113 (0.0110)	
training:	Epoch: [75][200/233]	Loss 0.0088 (0.0110)	
training:	Epoch: [75][201/233]	Loss 0.0082 (0.0110)	
training:	Epoch: [75][202/233]	Loss 0.0129 (0.0110)	
training:	Epoch: [75][203/233]	Loss 0.0180 (0.0111)	
training:	Epoch: [75][204/233]	Loss 0.0083 (0.0110)	
training:	Epoch: [75][205/233]	Loss 0.0095 (0.0110)	
training:	Epoch: [75][206/233]	Loss 0.0079 (0.0110)	
training:	Epoch: [75][207/233]	Loss 0.0116 (0.0110)	
training:	Epoch: [75][208/233]	Loss 0.0080 (0.0110)	
training:	Epoch: [75][209/233]	Loss 0.0098 (0.0110)	
training:	Epoch: [75][210/233]	Loss 0.0077 (0.0110)	
training:	Epoch: [75][211/233]	Loss 0.0094 (0.0110)	
training:	Epoch: [75][212/233]	Loss 0.0083 (0.0110)	
training:	Epoch: [75][213/233]	Loss 0.0122 (0.0110)	
training:	Epoch: [75][214/233]	Loss 0.0094 (0.0110)	
training:	Epoch: [75][215/233]	Loss 0.0120 (0.0110)	
training:	Epoch: [75][216/233]	Loss 0.0235 (0.0110)	
training:	Epoch: [75][217/233]	Loss 0.0089 (0.0110)	
training:	Epoch: [75][218/233]	Loss 0.0104 (0.0110)	
training:	Epoch: [75][219/233]	Loss 0.0091 (0.0110)	
training:	Epoch: [75][220/233]	Loss 0.0092 (0.0110)	
training:	Epoch: [75][221/233]	Loss 0.0118 (0.0110)	
training:	Epoch: [75][222/233]	Loss 0.0089 (0.0110)	
training:	Epoch: [75][223/233]	Loss 0.0132 (0.0110)	
training:	Epoch: [75][224/233]	Loss 0.0085 (0.0110)	
training:	Epoch: [75][225/233]	Loss 0.0127 (0.0110)	
training:	Epoch: [75][226/233]	Loss 0.0106 (0.0110)	
training:	Epoch: [75][227/233]	Loss 0.0115 (0.0110)	
training:	Epoch: [75][228/233]	Loss 0.0088 (0.0110)	
training:	Epoch: [75][229/233]	Loss 0.0082 (0.0110)	
training:	Epoch: [75][230/233]	Loss 0.0143 (0.0110)	
training:	Epoch: [75][231/233]	Loss 0.0108 (0.0110)	
training:	Epoch: [75][232/233]	Loss 0.0088 (0.0110)	
training:	Epoch: [75][233/233]	Loss 0.0091 (0.0110)	
Training:	 Loss: 0.0109

Training:	 ACC: 1.0000 1.0000 1.0000 1.0000
