Namespace(inputDirectory='data', outputDirectory='nval', DisFile='data/dis_sort_roi2dis2000.npy', task='sex', type='binary', feature=['Nos', 'FA1'], CUDA_id='0', data_id='0', dataset='Classification', norm=True, channels=1, epochs=200, tensorboard=True, net_architecture='TractGraphormer', batch_size=32, rate=3e-05, weight=0.0, sched_step=100, sched_gamma=0.1, printing_frequency=1, seed=0, alpha=0, remix_kappa=0, remix_tau=0, loss='CE', sigma=0, k=20, fl=64, nh=1)
Training the 'TractGraphormer' architecture

The following parameters are used:
Batch size:	32
Number of workers:	0
Learning rate:	3e-05
Weight decay:	0.0
Scheduler steps:	100
Scheduler gamma:	0.1
Number of epochs of training:	200
Number of input channels:	1

Data preparation
Reading data from:	./data
Training set size:	7473
Validation set size:	1869

Performing calculations on:	cuda:0

Pretraining:	Epoch 1/200
----------
training:	Epoch: [1][1/233]	Loss 0.8569 (0.8569)	
training:	Epoch: [1][2/233]	Loss 0.8822 (0.8696)	
training:	Epoch: [1][3/233]	Loss 0.6519 (0.7970)	
training:	Epoch: [1][4/233]	Loss 0.7291 (0.7800)	
training:	Epoch: [1][5/233]	Loss 0.6902 (0.7621)	
training:	Epoch: [1][6/233]	Loss 0.7402 (0.7584)	
training:	Epoch: [1][7/233]	Loss 0.6764 (0.7467)	
training:	Epoch: [1][8/233]	Loss 0.6705 (0.7372)	
training:	Epoch: [1][9/233]	Loss 0.6818 (0.7310)	
training:	Epoch: [1][10/233]	Loss 0.6828 (0.7262)	
training:	Epoch: [1][11/233]	Loss 0.6841 (0.7224)	
training:	Epoch: [1][12/233]	Loss 0.6299 (0.7147)	
training:	Epoch: [1][13/233]	Loss 0.6839 (0.7123)	
training:	Epoch: [1][14/233]	Loss 0.7161 (0.7126)	
training:	Epoch: [1][15/233]	Loss 0.7004 (0.7118)	
training:	Epoch: [1][16/233]	Loss 0.6998 (0.7110)	
training:	Epoch: [1][17/233]	Loss 0.6902 (0.7098)	
training:	Epoch: [1][18/233]	Loss 0.7084 (0.7097)	
training:	Epoch: [1][19/233]	Loss 0.6656 (0.7074)	
training:	Epoch: [1][20/233]	Loss 0.6664 (0.7053)	
training:	Epoch: [1][21/233]	Loss 0.6419 (0.7023)	
training:	Epoch: [1][22/233]	Loss 0.6828 (0.7014)	
training:	Epoch: [1][23/233]	Loss 0.6933 (0.7011)	
training:	Epoch: [1][24/233]	Loss 0.6868 (0.7005)	
training:	Epoch: [1][25/233]	Loss 0.6576 (0.6988)	
training:	Epoch: [1][26/233]	Loss 0.6319 (0.6962)	
training:	Epoch: [1][27/233]	Loss 0.6941 (0.6961)	
training:	Epoch: [1][28/233]	Loss 0.6690 (0.6952)	
training:	Epoch: [1][29/233]	Loss 0.6830 (0.6947)	
training:	Epoch: [1][30/233]	Loss 0.6624 (0.6937)	
training:	Epoch: [1][31/233]	Loss 0.6876 (0.6935)	
training:	Epoch: [1][32/233]	Loss 0.6983 (0.6936)	
training:	Epoch: [1][33/233]	Loss 0.6581 (0.6925)	
training:	Epoch: [1][34/233]	Loss 0.6644 (0.6917)	
training:	Epoch: [1][35/233]	Loss 0.6398 (0.6902)	
training:	Epoch: [1][36/233]	Loss 0.6520 (0.6892)	
training:	Epoch: [1][37/233]	Loss 0.6825 (0.6890)	
training:	Epoch: [1][38/233]	Loss 0.6588 (0.6882)	
training:	Epoch: [1][39/233]	Loss 0.6273 (0.6866)	
training:	Epoch: [1][40/233]	Loss 0.6271 (0.6851)	
training:	Epoch: [1][41/233]	Loss 0.6682 (0.6847)	
training:	Epoch: [1][42/233]	Loss 0.6663 (0.6843)	
training:	Epoch: [1][43/233]	Loss 0.6478 (0.6834)	
training:	Epoch: [1][44/233]	Loss 0.6541 (0.6828)	
training:	Epoch: [1][45/233]	Loss 0.7414 (0.6841)	
training:	Epoch: [1][46/233]	Loss 0.7067 (0.6846)	
training:	Epoch: [1][47/233]	Loss 0.6945 (0.6848)	
training:	Epoch: [1][48/233]	Loss 0.6618 (0.6843)	
training:	Epoch: [1][49/233]	Loss 0.6750 (0.6841)	
training:	Epoch: [1][50/233]	Loss 0.7268 (0.6850)	
training:	Epoch: [1][51/233]	Loss 0.6629 (0.6845)	
training:	Epoch: [1][52/233]	Loss 0.6895 (0.6846)	
training:	Epoch: [1][53/233]	Loss 0.7065 (0.6850)	
training:	Epoch: [1][54/233]	Loss 0.7392 (0.6860)	
training:	Epoch: [1][55/233]	Loss 0.6477 (0.6853)	
training:	Epoch: [1][56/233]	Loss 0.6049 (0.6839)	
training:	Epoch: [1][57/233]	Loss 0.6272 (0.6829)	
training:	Epoch: [1][58/233]	Loss 0.6869 (0.6830)	
training:	Epoch: [1][59/233]	Loss 0.6621 (0.6826)	
training:	Epoch: [1][60/233]	Loss 0.6772 (0.6825)	
training:	Epoch: [1][61/233]	Loss 0.6638 (0.6822)	
training:	Epoch: [1][62/233]	Loss 0.6367 (0.6815)	
training:	Epoch: [1][63/233]	Loss 0.6070 (0.6803)	
training:	Epoch: [1][64/233]	Loss 0.6703 (0.6802)	
training:	Epoch: [1][65/233]	Loss 0.6389 (0.6795)	
training:	Epoch: [1][66/233]	Loss 0.6479 (0.6790)	
training:	Epoch: [1][67/233]	Loss 0.6464 (0.6786)	
training:	Epoch: [1][68/233]	Loss 0.6182 (0.6777)	
training:	Epoch: [1][69/233]	Loss 0.6373 (0.6771)	
training:	Epoch: [1][70/233]	Loss 0.6373 (0.6765)	
training:	Epoch: [1][71/233]	Loss 0.6182 (0.6757)	
training:	Epoch: [1][72/233]	Loss 0.6385 (0.6752)	
training:	Epoch: [1][73/233]	Loss 0.6093 (0.6743)	
training:	Epoch: [1][74/233]	Loss 0.5892 (0.6731)	
training:	Epoch: [1][75/233]	Loss 0.6395 (0.6727)	
training:	Epoch: [1][76/233]	Loss 0.7317 (0.6735)	
training:	Epoch: [1][77/233]	Loss 0.6705 (0.6734)	
training:	Epoch: [1][78/233]	Loss 0.6725 (0.6734)	
training:	Epoch: [1][79/233]	Loss 0.6491 (0.6731)	
training:	Epoch: [1][80/233]	Loss 0.6738 (0.6731)	
training:	Epoch: [1][81/233]	Loss 0.6315 (0.6726)	
training:	Epoch: [1][82/233]	Loss 0.6183 (0.6719)	
training:	Epoch: [1][83/233]	Loss 0.5826 (0.6708)	
training:	Epoch: [1][84/233]	Loss 0.6157 (0.6702)	
training:	Epoch: [1][85/233]	Loss 0.6628 (0.6701)	
training:	Epoch: [1][86/233]	Loss 0.6507 (0.6699)	
training:	Epoch: [1][87/233]	Loss 0.6562 (0.6697)	
training:	Epoch: [1][88/233]	Loss 0.6047 (0.6690)	
training:	Epoch: [1][89/233]	Loss 0.6872 (0.6692)	
training:	Epoch: [1][90/233]	Loss 0.6770 (0.6693)	
training:	Epoch: [1][91/233]	Loss 0.6774 (0.6694)	
training:	Epoch: [1][92/233]	Loss 0.6479 (0.6691)	
training:	Epoch: [1][93/233]	Loss 0.6810 (0.6693)	
training:	Epoch: [1][94/233]	Loss 0.6837 (0.6694)	
training:	Epoch: [1][95/233]	Loss 0.6676 (0.6694)	
training:	Epoch: [1][96/233]	Loss 0.6989 (0.6697)	
training:	Epoch: [1][97/233]	Loss 0.5469 (0.6684)	
training:	Epoch: [1][98/233]	Loss 0.5749 (0.6675)	
training:	Epoch: [1][99/233]	Loss 0.6994 (0.6678)	
training:	Epoch: [1][100/233]	Loss 0.6401 (0.6675)	
training:	Epoch: [1][101/233]	Loss 0.6526 (0.6674)	
training:	Epoch: [1][102/233]	Loss 0.6951 (0.6677)	
training:	Epoch: [1][103/233]	Loss 0.6716 (0.6677)	
training:	Epoch: [1][104/233]	Loss 0.6503 (0.6675)	
training:	Epoch: [1][105/233]	Loss 0.6312 (0.6672)	
training:	Epoch: [1][106/233]	Loss 0.5777 (0.6663)	
training:	Epoch: [1][107/233]	Loss 0.6364 (0.6661)	
training:	Epoch: [1][108/233]	Loss 0.6764 (0.6661)	
training:	Epoch: [1][109/233]	Loss 0.6407 (0.6659)	
training:	Epoch: [1][110/233]	Loss 0.6315 (0.6656)	
training:	Epoch: [1][111/233]	Loss 0.6593 (0.6655)	
training:	Epoch: [1][112/233]	Loss 0.6358 (0.6653)	
training:	Epoch: [1][113/233]	Loss 0.7298 (0.6659)	
training:	Epoch: [1][114/233]	Loss 0.6563 (0.6658)	
training:	Epoch: [1][115/233]	Loss 0.7032 (0.6661)	
training:	Epoch: [1][116/233]	Loss 0.6897 (0.6663)	
training:	Epoch: [1][117/233]	Loss 0.6255 (0.6659)	
training:	Epoch: [1][118/233]	Loss 0.6652 (0.6659)	
training:	Epoch: [1][119/233]	Loss 0.6318 (0.6657)	
training:	Epoch: [1][120/233]	Loss 0.6391 (0.6654)	
training:	Epoch: [1][121/233]	Loss 0.5724 (0.6647)	
training:	Epoch: [1][122/233]	Loss 0.6991 (0.6649)	
training:	Epoch: [1][123/233]	Loss 0.5449 (0.6640)	
training:	Epoch: [1][124/233]	Loss 0.6423 (0.6638)	
training:	Epoch: [1][125/233]	Loss 0.6574 (0.6637)	
training:	Epoch: [1][126/233]	Loss 0.6566 (0.6637)	
training:	Epoch: [1][127/233]	Loss 0.5577 (0.6629)	
training:	Epoch: [1][128/233]	Loss 0.7123 (0.6632)	
training:	Epoch: [1][129/233]	Loss 0.6272 (0.6630)	
training:	Epoch: [1][130/233]	Loss 0.6632 (0.6630)	
training:	Epoch: [1][131/233]	Loss 0.5886 (0.6624)	
training:	Epoch: [1][132/233]	Loss 0.6656 (0.6624)	
training:	Epoch: [1][133/233]	Loss 0.6378 (0.6622)	
training:	Epoch: [1][134/233]	Loss 0.6433 (0.6621)	
training:	Epoch: [1][135/233]	Loss 0.6657 (0.6621)	
training:	Epoch: [1][136/233]	Loss 0.6574 (0.6621)	
training:	Epoch: [1][137/233]	Loss 0.5923 (0.6616)	
training:	Epoch: [1][138/233]	Loss 0.6976 (0.6618)	
training:	Epoch: [1][139/233]	Loss 0.6944 (0.6621)	
training:	Epoch: [1][140/233]	Loss 0.6404 (0.6619)	
training:	Epoch: [1][141/233]	Loss 0.6138 (0.6616)	
training:	Epoch: [1][142/233]	Loss 0.5677 (0.6609)	
training:	Epoch: [1][143/233]	Loss 0.6888 (0.6611)	
training:	Epoch: [1][144/233]	Loss 0.5518 (0.6603)	
training:	Epoch: [1][145/233]	Loss 0.5978 (0.6599)	
training:	Epoch: [1][146/233]	Loss 0.6660 (0.6600)	
training:	Epoch: [1][147/233]	Loss 0.5252 (0.6590)	
training:	Epoch: [1][148/233]	Loss 0.6477 (0.6590)	
training:	Epoch: [1][149/233]	Loss 0.5738 (0.6584)	
training:	Epoch: [1][150/233]	Loss 0.7407 (0.6589)	
training:	Epoch: [1][151/233]	Loss 0.6775 (0.6591)	
training:	Epoch: [1][152/233]	Loss 0.6364 (0.6589)	
training:	Epoch: [1][153/233]	Loss 0.6008 (0.6585)	
training:	Epoch: [1][154/233]	Loss 0.5594 (0.6579)	
training:	Epoch: [1][155/233]	Loss 0.5946 (0.6575)	
training:	Epoch: [1][156/233]	Loss 0.6343 (0.6573)	
training:	Epoch: [1][157/233]	Loss 0.6696 (0.6574)	
training:	Epoch: [1][158/233]	Loss 0.6355 (0.6573)	
training:	Epoch: [1][159/233]	Loss 0.5711 (0.6567)	
training:	Epoch: [1][160/233]	Loss 0.5997 (0.6564)	
training:	Epoch: [1][161/233]	Loss 0.6485 (0.6563)	
training:	Epoch: [1][162/233]	Loss 0.5947 (0.6559)	
training:	Epoch: [1][163/233]	Loss 0.6850 (0.6561)	
training:	Epoch: [1][164/233]	Loss 0.6619 (0.6562)	
training:	Epoch: [1][165/233]	Loss 0.6304 (0.6560)	
training:	Epoch: [1][166/233]	Loss 0.5527 (0.6554)	
training:	Epoch: [1][167/233]	Loss 0.6171 (0.6552)	
training:	Epoch: [1][168/233]	Loss 0.6690 (0.6552)	
training:	Epoch: [1][169/233]	Loss 0.5598 (0.6547)	
training:	Epoch: [1][170/233]	Loss 0.5466 (0.6540)	
training:	Epoch: [1][171/233]	Loss 0.6544 (0.6540)	
training:	Epoch: [1][172/233]	Loss 0.6459 (0.6540)	
training:	Epoch: [1][173/233]	Loss 0.5520 (0.6534)	
training:	Epoch: [1][174/233]	Loss 0.6610 (0.6534)	
training:	Epoch: [1][175/233]	Loss 0.6623 (0.6535)	
training:	Epoch: [1][176/233]	Loss 0.7264 (0.6539)	
training:	Epoch: [1][177/233]	Loss 0.5309 (0.6532)	
training:	Epoch: [1][178/233]	Loss 0.5520 (0.6526)	
training:	Epoch: [1][179/233]	Loss 0.5890 (0.6523)	
training:	Epoch: [1][180/233]	Loss 0.6955 (0.6525)	
training:	Epoch: [1][181/233]	Loss 0.6979 (0.6528)	
training:	Epoch: [1][182/233]	Loss 0.7648 (0.6534)	
training:	Epoch: [1][183/233]	Loss 0.6303 (0.6533)	
training:	Epoch: [1][184/233]	Loss 0.5892 (0.6529)	
training:	Epoch: [1][185/233]	Loss 0.5930 (0.6526)	
training:	Epoch: [1][186/233]	Loss 0.6949 (0.6528)	
training:	Epoch: [1][187/233]	Loss 0.6107 (0.6526)	
training:	Epoch: [1][188/233]	Loss 0.6877 (0.6528)	
training:	Epoch: [1][189/233]	Loss 0.5993 (0.6525)	
training:	Epoch: [1][190/233]	Loss 0.6452 (0.6525)	
training:	Epoch: [1][191/233]	Loss 0.6463 (0.6524)	
training:	Epoch: [1][192/233]	Loss 0.6380 (0.6524)	
training:	Epoch: [1][193/233]	Loss 0.6220 (0.6522)	
training:	Epoch: [1][194/233]	Loss 0.6728 (0.6523)	
training:	Epoch: [1][195/233]	Loss 0.5795 (0.6519)	
training:	Epoch: [1][196/233]	Loss 0.6075 (0.6517)	
training:	Epoch: [1][197/233]	Loss 0.6541 (0.6517)	
training:	Epoch: [1][198/233]	Loss 0.6032 (0.6515)	
training:	Epoch: [1][199/233]	Loss 0.6850 (0.6516)	
training:	Epoch: [1][200/233]	Loss 0.6137 (0.6515)	
training:	Epoch: [1][201/233]	Loss 0.6535 (0.6515)	
training:	Epoch: [1][202/233]	Loss 0.5756 (0.6511)	
training:	Epoch: [1][203/233]	Loss 0.5797 (0.6507)	
training:	Epoch: [1][204/233]	Loss 0.6600 (0.6508)	
training:	Epoch: [1][205/233]	Loss 0.6633 (0.6508)	
training:	Epoch: [1][206/233]	Loss 0.5919 (0.6506)	
training:	Epoch: [1][207/233]	Loss 0.5644 (0.6501)	
training:	Epoch: [1][208/233]	Loss 0.6245 (0.6500)	
training:	Epoch: [1][209/233]	Loss 0.6661 (0.6501)	
training:	Epoch: [1][210/233]	Loss 0.6543 (0.6501)	
training:	Epoch: [1][211/233]	Loss 0.6333 (0.6500)	
training:	Epoch: [1][212/233]	Loss 0.5806 (0.6497)	
training:	Epoch: [1][213/233]	Loss 0.5903 (0.6494)	
training:	Epoch: [1][214/233]	Loss 0.6030 (0.6492)	
training:	Epoch: [1][215/233]	Loss 0.7436 (0.6497)	
training:	Epoch: [1][216/233]	Loss 0.5853 (0.6494)	
training:	Epoch: [1][217/233]	Loss 0.6300 (0.6493)	
training:	Epoch: [1][218/233]	Loss 0.7230 (0.6496)	
training:	Epoch: [1][219/233]	Loss 0.5630 (0.6492)	
training:	Epoch: [1][220/233]	Loss 0.6205 (0.6491)	
training:	Epoch: [1][221/233]	Loss 0.6188 (0.6489)	
training:	Epoch: [1][222/233]	Loss 0.6521 (0.6490)	
training:	Epoch: [1][223/233]	Loss 0.5469 (0.6485)	
training:	Epoch: [1][224/233]	Loss 0.5501 (0.6481)	
training:	Epoch: [1][225/233]	Loss 0.6150 (0.6479)	
training:	Epoch: [1][226/233]	Loss 0.6462 (0.6479)	
training:	Epoch: [1][227/233]	Loss 0.5606 (0.6475)	
training:	Epoch: [1][228/233]	Loss 0.6494 (0.6475)	
training:	Epoch: [1][229/233]	Loss 0.5396 (0.6471)	
training:	Epoch: [1][230/233]	Loss 0.5767 (0.6467)	
training:	Epoch: [1][231/233]	Loss 0.5994 (0.6465)	
training:	Epoch: [1][232/233]	Loss 0.6659 (0.6466)	
training:	Epoch: [1][233/233]	Loss 0.5943 (0.6464)	
Training:	 Loss: 0.6449

Training:	 ACC: 0.6725 0.6776 0.7888 0.5561
Validation:	 ACC: 0.6684 0.6747 0.8076 0.5291
Validation:	 Best_BACC: 0.6684 0.6747 0.8076 0.5291
Validation:	 Loss: 0.6005
Pretraining:	Epoch 2/200
----------
training:	Epoch: [2][1/233]	Loss 0.5395 (0.5395)	
training:	Epoch: [2][2/233]	Loss 0.6970 (0.6182)	
training:	Epoch: [2][3/233]	Loss 0.5741 (0.6035)	
training:	Epoch: [2][4/233]	Loss 0.5582 (0.5922)	
training:	Epoch: [2][5/233]	Loss 0.5641 (0.5866)	
training:	Epoch: [2][6/233]	Loss 0.5810 (0.5857)	
training:	Epoch: [2][7/233]	Loss 0.5667 (0.5830)	
training:	Epoch: [2][8/233]	Loss 0.6404 (0.5901)	
training:	Epoch: [2][9/233]	Loss 0.5239 (0.5828)	
training:	Epoch: [2][10/233]	Loss 0.7059 (0.5951)	
training:	Epoch: [2][11/233]	Loss 0.6928 (0.6040)	
training:	Epoch: [2][12/233]	Loss 0.5547 (0.5999)	
training:	Epoch: [2][13/233]	Loss 0.5470 (0.5958)	
training:	Epoch: [2][14/233]	Loss 0.7327 (0.6056)	
training:	Epoch: [2][15/233]	Loss 0.5644 (0.6028)	
training:	Epoch: [2][16/233]	Loss 0.5490 (0.5995)	
training:	Epoch: [2][17/233]	Loss 0.6004 (0.5995)	
training:	Epoch: [2][18/233]	Loss 0.5573 (0.5972)	
training:	Epoch: [2][19/233]	Loss 0.5394 (0.5941)	
training:	Epoch: [2][20/233]	Loss 0.6020 (0.5945)	
training:	Epoch: [2][21/233]	Loss 0.6785 (0.5985)	
training:	Epoch: [2][22/233]	Loss 0.7387 (0.6049)	
training:	Epoch: [2][23/233]	Loss 0.5332 (0.6018)	
training:	Epoch: [2][24/233]	Loss 0.6360 (0.6032)	
training:	Epoch: [2][25/233]	Loss 0.6735 (0.6060)	
training:	Epoch: [2][26/233]	Loss 0.6676 (0.6084)	
training:	Epoch: [2][27/233]	Loss 0.6093 (0.6084)	
training:	Epoch: [2][28/233]	Loss 0.5715 (0.6071)	
training:	Epoch: [2][29/233]	Loss 0.5776 (0.6061)	
training:	Epoch: [2][30/233]	Loss 0.5377 (0.6038)	
training:	Epoch: [2][31/233]	Loss 0.5510 (0.6021)	
training:	Epoch: [2][32/233]	Loss 0.6149 (0.6025)	
training:	Epoch: [2][33/233]	Loss 0.5503 (0.6009)	
training:	Epoch: [2][34/233]	Loss 0.6781 (0.6032)	
training:	Epoch: [2][35/233]	Loss 0.4417 (0.5986)	
training:	Epoch: [2][36/233]	Loss 0.5504 (0.5972)	
training:	Epoch: [2][37/233]	Loss 0.7494 (0.6014)	
training:	Epoch: [2][38/233]	Loss 0.5813 (0.6008)	
training:	Epoch: [2][39/233]	Loss 0.5589 (0.5997)	
training:	Epoch: [2][40/233]	Loss 0.5157 (0.5976)	
training:	Epoch: [2][41/233]	Loss 0.7839 (0.6022)	
training:	Epoch: [2][42/233]	Loss 0.5735 (0.6015)	
training:	Epoch: [2][43/233]	Loss 0.5600 (0.6005)	
training:	Epoch: [2][44/233]	Loss 0.6395 (0.6014)	
training:	Epoch: [2][45/233]	Loss 0.4563 (0.5982)	
training:	Epoch: [2][46/233]	Loss 0.5262 (0.5966)	
training:	Epoch: [2][47/233]	Loss 0.5510 (0.5957)	
training:	Epoch: [2][48/233]	Loss 0.5861 (0.5955)	
training:	Epoch: [2][49/233]	Loss 0.5831 (0.5952)	
training:	Epoch: [2][50/233]	Loss 0.5397 (0.5941)	
training:	Epoch: [2][51/233]	Loss 0.5967 (0.5942)	
training:	Epoch: [2][52/233]	Loss 0.4970 (0.5923)	
training:	Epoch: [2][53/233]	Loss 0.5044 (0.5906)	
training:	Epoch: [2][54/233]	Loss 0.6027 (0.5909)	
training:	Epoch: [2][55/233]	Loss 0.6939 (0.5927)	
training:	Epoch: [2][56/233]	Loss 0.5863 (0.5926)	
training:	Epoch: [2][57/233]	Loss 0.6346 (0.5933)	
training:	Epoch: [2][58/233]	Loss 0.5365 (0.5924)	
training:	Epoch: [2][59/233]	Loss 0.5717 (0.5920)	
training:	Epoch: [2][60/233]	Loss 0.5513 (0.5913)	
training:	Epoch: [2][61/233]	Loss 0.6187 (0.5918)	
training:	Epoch: [2][62/233]	Loss 0.5820 (0.5916)	
training:	Epoch: [2][63/233]	Loss 0.6178 (0.5920)	
training:	Epoch: [2][64/233]	Loss 0.5506 (0.5914)	
training:	Epoch: [2][65/233]	Loss 0.6716 (0.5926)	
training:	Epoch: [2][66/233]	Loss 0.5189 (0.5915)	
training:	Epoch: [2][67/233]	Loss 0.5979 (0.5916)	
training:	Epoch: [2][68/233]	Loss 0.4956 (0.5902)	
training:	Epoch: [2][69/233]	Loss 0.4957 (0.5888)	
training:	Epoch: [2][70/233]	Loss 0.5195 (0.5878)	
training:	Epoch: [2][71/233]	Loss 0.5070 (0.5867)	
training:	Epoch: [2][72/233]	Loss 0.6339 (0.5874)	
training:	Epoch: [2][73/233]	Loss 0.6324 (0.5880)	
training:	Epoch: [2][74/233]	Loss 0.6072 (0.5882)	
training:	Epoch: [2][75/233]	Loss 0.5731 (0.5880)	
training:	Epoch: [2][76/233]	Loss 0.5668 (0.5877)	
training:	Epoch: [2][77/233]	Loss 0.6312 (0.5883)	
training:	Epoch: [2][78/233]	Loss 0.6447 (0.5890)	
training:	Epoch: [2][79/233]	Loss 0.4751 (0.5876)	
training:	Epoch: [2][80/233]	Loss 0.6522 (0.5884)	
training:	Epoch: [2][81/233]	Loss 0.5796 (0.5883)	
training:	Epoch: [2][82/233]	Loss 0.5412 (0.5877)	
training:	Epoch: [2][83/233]	Loss 0.5766 (0.5876)	
training:	Epoch: [2][84/233]	Loss 0.4697 (0.5862)	
training:	Epoch: [2][85/233]	Loss 0.5191 (0.5854)	
training:	Epoch: [2][86/233]	Loss 0.6178 (0.5858)	
training:	Epoch: [2][87/233]	Loss 0.4999 (0.5848)	
training:	Epoch: [2][88/233]	Loss 0.6117 (0.5851)	
training:	Epoch: [2][89/233]	Loss 0.5923 (0.5852)	
training:	Epoch: [2][90/233]	Loss 0.4720 (0.5839)	
training:	Epoch: [2][91/233]	Loss 0.5678 (0.5837)	
training:	Epoch: [2][92/233]	Loss 0.6187 (0.5841)	
training:	Epoch: [2][93/233]	Loss 0.5249 (0.5835)	
training:	Epoch: [2][94/233]	Loss 0.6219 (0.5839)	
training:	Epoch: [2][95/233]	Loss 0.6324 (0.5844)	
training:	Epoch: [2][96/233]	Loss 0.4833 (0.5833)	
training:	Epoch: [2][97/233]	Loss 0.5664 (0.5832)	
training:	Epoch: [2][98/233]	Loss 0.6343 (0.5837)	
training:	Epoch: [2][99/233]	Loss 0.5985 (0.5838)	
training:	Epoch: [2][100/233]	Loss 0.4542 (0.5825)	
training:	Epoch: [2][101/233]	Loss 0.4593 (0.5813)	
training:	Epoch: [2][102/233]	Loss 0.5866 (0.5814)	
training:	Epoch: [2][103/233]	Loss 0.5301 (0.5809)	
training:	Epoch: [2][104/233]	Loss 0.5147 (0.5802)	
training:	Epoch: [2][105/233]	Loss 0.5558 (0.5800)	
training:	Epoch: [2][106/233]	Loss 0.5062 (0.5793)	
training:	Epoch: [2][107/233]	Loss 0.5404 (0.5790)	
training:	Epoch: [2][108/233]	Loss 0.5380 (0.5786)	
training:	Epoch: [2][109/233]	Loss 0.5767 (0.5786)	
training:	Epoch: [2][110/233]	Loss 0.5556 (0.5783)	
training:	Epoch: [2][111/233]	Loss 0.5908 (0.5785)	
training:	Epoch: [2][112/233]	Loss 0.4933 (0.5777)	
training:	Epoch: [2][113/233]	Loss 0.5473 (0.5774)	
training:	Epoch: [2][114/233]	Loss 0.5635 (0.5773)	
training:	Epoch: [2][115/233]	Loss 0.6212 (0.5777)	
training:	Epoch: [2][116/233]	Loss 0.5361 (0.5773)	
training:	Epoch: [2][117/233]	Loss 0.5318 (0.5769)	
training:	Epoch: [2][118/233]	Loss 0.5833 (0.5770)	
training:	Epoch: [2][119/233]	Loss 0.5576 (0.5768)	
training:	Epoch: [2][120/233]	Loss 0.6292 (0.5773)	
training:	Epoch: [2][121/233]	Loss 0.4598 (0.5763)	
training:	Epoch: [2][122/233]	Loss 0.5299 (0.5759)	
training:	Epoch: [2][123/233]	Loss 0.5610 (0.5758)	
training:	Epoch: [2][124/233]	Loss 0.5831 (0.5759)	
training:	Epoch: [2][125/233]	Loss 0.6331 (0.5763)	
training:	Epoch: [2][126/233]	Loss 0.5430 (0.5760)	
training:	Epoch: [2][127/233]	Loss 0.6006 (0.5762)	
training:	Epoch: [2][128/233]	Loss 0.5562 (0.5761)	
training:	Epoch: [2][129/233]	Loss 0.4907 (0.5754)	
training:	Epoch: [2][130/233]	Loss 0.5934 (0.5756)	
training:	Epoch: [2][131/233]	Loss 0.4785 (0.5748)	
training:	Epoch: [2][132/233]	Loss 0.5024 (0.5743)	
training:	Epoch: [2][133/233]	Loss 0.4886 (0.5736)	
training:	Epoch: [2][134/233]	Loss 0.4428 (0.5727)	
training:	Epoch: [2][135/233]	Loss 0.5510 (0.5725)	
training:	Epoch: [2][136/233]	Loss 0.5551 (0.5724)	
training:	Epoch: [2][137/233]	Loss 0.5391 (0.5721)	
training:	Epoch: [2][138/233]	Loss 0.4800 (0.5715)	
training:	Epoch: [2][139/233]	Loss 0.5559 (0.5713)	
training:	Epoch: [2][140/233]	Loss 0.5466 (0.5712)	
training:	Epoch: [2][141/233]	Loss 0.5356 (0.5709)	
training:	Epoch: [2][142/233]	Loss 0.4858 (0.5703)	
training:	Epoch: [2][143/233]	Loss 0.5295 (0.5700)	
training:	Epoch: [2][144/233]	Loss 0.4828 (0.5694)	
training:	Epoch: [2][145/233]	Loss 0.3964 (0.5682)	
training:	Epoch: [2][146/233]	Loss 0.6379 (0.5687)	
training:	Epoch: [2][147/233]	Loss 0.5144 (0.5683)	
training:	Epoch: [2][148/233]	Loss 0.4897 (0.5678)	
training:	Epoch: [2][149/233]	Loss 0.4945 (0.5673)	
training:	Epoch: [2][150/233]	Loss 0.4814 (0.5667)	
training:	Epoch: [2][151/233]	Loss 0.5201 (0.5664)	
training:	Epoch: [2][152/233]	Loss 0.4702 (0.5658)	
training:	Epoch: [2][153/233]	Loss 0.5676 (0.5658)	
training:	Epoch: [2][154/233]	Loss 0.5903 (0.5660)	
training:	Epoch: [2][155/233]	Loss 0.5339 (0.5658)	
training:	Epoch: [2][156/233]	Loss 0.6390 (0.5662)	
training:	Epoch: [2][157/233]	Loss 0.7129 (0.5672)	
training:	Epoch: [2][158/233]	Loss 0.5544 (0.5671)	
training:	Epoch: [2][159/233]	Loss 0.4513 (0.5664)	
training:	Epoch: [2][160/233]	Loss 0.6531 (0.5669)	
training:	Epoch: [2][161/233]	Loss 0.5152 (0.5666)	
training:	Epoch: [2][162/233]	Loss 0.5652 (0.5666)	
training:	Epoch: [2][163/233]	Loss 0.4777 (0.5660)	
training:	Epoch: [2][164/233]	Loss 0.6143 (0.5663)	
training:	Epoch: [2][165/233]	Loss 0.6620 (0.5669)	
training:	Epoch: [2][166/233]	Loss 0.4730 (0.5663)	
training:	Epoch: [2][167/233]	Loss 0.5775 (0.5664)	
training:	Epoch: [2][168/233]	Loss 0.5028 (0.5660)	
training:	Epoch: [2][169/233]	Loss 0.5662 (0.5660)	
training:	Epoch: [2][170/233]	Loss 0.6908 (0.5668)	
training:	Epoch: [2][171/233]	Loss 0.6447 (0.5672)	
training:	Epoch: [2][172/233]	Loss 0.4726 (0.5667)	
training:	Epoch: [2][173/233]	Loss 0.4078 (0.5657)	
training:	Epoch: [2][174/233]	Loss 0.6028 (0.5660)	
training:	Epoch: [2][175/233]	Loss 0.5580 (0.5659)	
training:	Epoch: [2][176/233]	Loss 0.4897 (0.5655)	
training:	Epoch: [2][177/233]	Loss 0.6000 (0.5657)	
training:	Epoch: [2][178/233]	Loss 0.5747 (0.5657)	
training:	Epoch: [2][179/233]	Loss 0.4162 (0.5649)	
training:	Epoch: [2][180/233]	Loss 0.4018 (0.5640)	
training:	Epoch: [2][181/233]	Loss 0.5589 (0.5640)	
training:	Epoch: [2][182/233]	Loss 0.4365 (0.5633)	
training:	Epoch: [2][183/233]	Loss 0.4005 (0.5624)	
training:	Epoch: [2][184/233]	Loss 0.4551 (0.5618)	
training:	Epoch: [2][185/233]	Loss 0.5318 (0.5616)	
training:	Epoch: [2][186/233]	Loss 0.4960 (0.5613)	
training:	Epoch: [2][187/233]	Loss 0.4132 (0.5605)	
training:	Epoch: [2][188/233]	Loss 0.5007 (0.5602)	
training:	Epoch: [2][189/233]	Loss 0.5520 (0.5601)	
training:	Epoch: [2][190/233]	Loss 0.4576 (0.5596)	
training:	Epoch: [2][191/233]	Loss 0.5468 (0.5595)	
training:	Epoch: [2][192/233]	Loss 0.6093 (0.5598)	
training:	Epoch: [2][193/233]	Loss 0.4234 (0.5591)	
training:	Epoch: [2][194/233]	Loss 0.5817 (0.5592)	
training:	Epoch: [2][195/233]	Loss 0.4786 (0.5588)	
training:	Epoch: [2][196/233]	Loss 0.4190 (0.5581)	
training:	Epoch: [2][197/233]	Loss 0.6022 (0.5583)	
training:	Epoch: [2][198/233]	Loss 0.3702 (0.5573)	
training:	Epoch: [2][199/233]	Loss 0.4134 (0.5566)	
training:	Epoch: [2][200/233]	Loss 0.5128 (0.5564)	
training:	Epoch: [2][201/233]	Loss 0.4290 (0.5557)	
training:	Epoch: [2][202/233]	Loss 0.4920 (0.5554)	
training:	Epoch: [2][203/233]	Loss 0.5458 (0.5554)	
training:	Epoch: [2][204/233]	Loss 0.5839 (0.5555)	
training:	Epoch: [2][205/233]	Loss 0.4637 (0.5551)	
training:	Epoch: [2][206/233]	Loss 0.5233 (0.5549)	
training:	Epoch: [2][207/233]	Loss 0.4150 (0.5542)	
training:	Epoch: [2][208/233]	Loss 0.4705 (0.5538)	
training:	Epoch: [2][209/233]	Loss 0.4159 (0.5532)	
training:	Epoch: [2][210/233]	Loss 0.4958 (0.5529)	
training:	Epoch: [2][211/233]	Loss 0.4051 (0.5522)	
training:	Epoch: [2][212/233]	Loss 0.6336 (0.5526)	
training:	Epoch: [2][213/233]	Loss 0.4619 (0.5522)	
training:	Epoch: [2][214/233]	Loss 0.4211 (0.5516)	
training:	Epoch: [2][215/233]	Loss 0.5233 (0.5514)	
training:	Epoch: [2][216/233]	Loss 0.4933 (0.5512)	
training:	Epoch: [2][217/233]	Loss 0.4069 (0.5505)	
training:	Epoch: [2][218/233]	Loss 0.4753 (0.5501)	
training:	Epoch: [2][219/233]	Loss 0.6475 (0.5506)	
training:	Epoch: [2][220/233]	Loss 0.7017 (0.5513)	
training:	Epoch: [2][221/233]	Loss 0.4607 (0.5509)	
training:	Epoch: [2][222/233]	Loss 0.4683 (0.5505)	
training:	Epoch: [2][223/233]	Loss 0.4963 (0.5503)	
training:	Epoch: [2][224/233]	Loss 0.5667 (0.5503)	
training:	Epoch: [2][225/233]	Loss 0.4662 (0.5500)	
training:	Epoch: [2][226/233]	Loss 0.4573 (0.5495)	
training:	Epoch: [2][227/233]	Loss 0.5552 (0.5496)	
training:	Epoch: [2][228/233]	Loss 0.4995 (0.5493)	
training:	Epoch: [2][229/233]	Loss 0.4567 (0.5489)	
training:	Epoch: [2][230/233]	Loss 0.4057 (0.5483)	
training:	Epoch: [2][231/233]	Loss 0.5457 (0.5483)	
training:	Epoch: [2][232/233]	Loss 0.4818 (0.5480)	
training:	Epoch: [2][233/233]	Loss 0.4647 (0.5477)	
Training:	 Loss: 0.5464

Training:	 ACC: 0.7831 0.7803 0.7191 0.8471
Validation:	 ACC: 0.7774 0.7747 0.7185 0.8363
Validation:	 Best_BACC: 0.7774 0.7747 0.7185 0.8363
Validation:	 Loss: 0.4811
Pretraining:	Epoch 3/200
----------
training:	Epoch: [3][1/233]	Loss 0.3852 (0.3852)	
training:	Epoch: [3][2/233]	Loss 0.4002 (0.3927)	
training:	Epoch: [3][3/233]	Loss 0.4459 (0.4104)	
training:	Epoch: [3][4/233]	Loss 0.4671 (0.4246)	
training:	Epoch: [3][5/233]	Loss 0.4747 (0.4346)	
training:	Epoch: [3][6/233]	Loss 0.5030 (0.4460)	
training:	Epoch: [3][7/233]	Loss 0.5760 (0.4646)	
training:	Epoch: [3][8/233]	Loss 0.4783 (0.4663)	
training:	Epoch: [3][9/233]	Loss 0.4108 (0.4601)	
training:	Epoch: [3][10/233]	Loss 0.4612 (0.4602)	
training:	Epoch: [3][11/233]	Loss 0.5629 (0.4696)	
training:	Epoch: [3][12/233]	Loss 0.5675 (0.4777)	
training:	Epoch: [3][13/233]	Loss 0.4550 (0.4760)	
training:	Epoch: [3][14/233]	Loss 0.4750 (0.4759)	
training:	Epoch: [3][15/233]	Loss 0.4540 (0.4745)	
training:	Epoch: [3][16/233]	Loss 0.4245 (0.4713)	
training:	Epoch: [3][17/233]	Loss 0.4959 (0.4728)	
training:	Epoch: [3][18/233]	Loss 0.4775 (0.4730)	
training:	Epoch: [3][19/233]	Loss 0.5146 (0.4752)	
training:	Epoch: [3][20/233]	Loss 0.5641 (0.4797)	
training:	Epoch: [3][21/233]	Loss 0.5612 (0.4836)	
training:	Epoch: [3][22/233]	Loss 0.4202 (0.4807)	
training:	Epoch: [3][23/233]	Loss 0.3410 (0.4746)	
training:	Epoch: [3][24/233]	Loss 0.3666 (0.4701)	
training:	Epoch: [3][25/233]	Loss 0.5119 (0.4718)	
training:	Epoch: [3][26/233]	Loss 0.5169 (0.4735)	
training:	Epoch: [3][27/233]	Loss 0.3207 (0.4679)	
training:	Epoch: [3][28/233]	Loss 0.3419 (0.4634)	
training:	Epoch: [3][29/233]	Loss 0.4367 (0.4624)	
training:	Epoch: [3][30/233]	Loss 0.5247 (0.4645)	
training:	Epoch: [3][31/233]	Loss 0.3390 (0.4605)	
training:	Epoch: [3][32/233]	Loss 0.4662 (0.4606)	
training:	Epoch: [3][33/233]	Loss 0.6427 (0.4662)	
training:	Epoch: [3][34/233]	Loss 0.4703 (0.4663)	
training:	Epoch: [3][35/233]	Loss 0.4914 (0.4670)	
training:	Epoch: [3][36/233]	Loss 0.5482 (0.4693)	
training:	Epoch: [3][37/233]	Loss 0.4526 (0.4688)	
training:	Epoch: [3][38/233]	Loss 0.5058 (0.4698)	
training:	Epoch: [3][39/233]	Loss 0.6188 (0.4736)	
training:	Epoch: [3][40/233]	Loss 0.5505 (0.4755)	
training:	Epoch: [3][41/233]	Loss 0.5009 (0.4761)	
training:	Epoch: [3][42/233]	Loss 0.4718 (0.4760)	
training:	Epoch: [3][43/233]	Loss 0.4053 (0.4744)	
training:	Epoch: [3][44/233]	Loss 0.4053 (0.4728)	
training:	Epoch: [3][45/233]	Loss 0.4789 (0.4730)	
training:	Epoch: [3][46/233]	Loss 0.4586 (0.4726)	
training:	Epoch: [3][47/233]	Loss 0.5825 (0.4750)	
training:	Epoch: [3][48/233]	Loss 0.6953 (0.4796)	
training:	Epoch: [3][49/233]	Loss 0.4936 (0.4799)	
training:	Epoch: [3][50/233]	Loss 0.3945 (0.4781)	
training:	Epoch: [3][51/233]	Loss 0.4614 (0.4778)	
training:	Epoch: [3][52/233]	Loss 0.6725 (0.4816)	
training:	Epoch: [3][53/233]	Loss 0.3680 (0.4794)	
training:	Epoch: [3][54/233]	Loss 0.4627 (0.4791)	
training:	Epoch: [3][55/233]	Loss 0.4829 (0.4792)	
training:	Epoch: [3][56/233]	Loss 0.4584 (0.4788)	
training:	Epoch: [3][57/233]	Loss 0.5021 (0.4792)	
training:	Epoch: [3][58/233]	Loss 0.5561 (0.4805)	
training:	Epoch: [3][59/233]	Loss 0.4040 (0.4792)	
training:	Epoch: [3][60/233]	Loss 0.5991 (0.4812)	
training:	Epoch: [3][61/233]	Loss 0.5042 (0.4816)	
training:	Epoch: [3][62/233]	Loss 0.4190 (0.4806)	
training:	Epoch: [3][63/233]	Loss 0.4767 (0.4805)	
training:	Epoch: [3][64/233]	Loss 0.3850 (0.4791)	
training:	Epoch: [3][65/233]	Loss 0.4161 (0.4781)	
training:	Epoch: [3][66/233]	Loss 0.5883 (0.4798)	
training:	Epoch: [3][67/233]	Loss 0.4141 (0.4788)	
training:	Epoch: [3][68/233]	Loss 0.4408 (0.4782)	
training:	Epoch: [3][69/233]	Loss 0.4448 (0.4777)	
training:	Epoch: [3][70/233]	Loss 0.3382 (0.4757)	
training:	Epoch: [3][71/233]	Loss 0.6111 (0.4776)	
training:	Epoch: [3][72/233]	Loss 0.5299 (0.4784)	
training:	Epoch: [3][73/233]	Loss 0.4907 (0.4785)	
training:	Epoch: [3][74/233]	Loss 0.5023 (0.4789)	
training:	Epoch: [3][75/233]	Loss 0.3163 (0.4767)	
training:	Epoch: [3][76/233]	Loss 0.4082 (0.4758)	
training:	Epoch: [3][77/233]	Loss 0.4147 (0.4750)	
training:	Epoch: [3][78/233]	Loss 0.4644 (0.4749)	
training:	Epoch: [3][79/233]	Loss 0.3144 (0.4728)	
training:	Epoch: [3][80/233]	Loss 0.4118 (0.4721)	
training:	Epoch: [3][81/233]	Loss 0.5220 (0.4727)	
training:	Epoch: [3][82/233]	Loss 0.4366 (0.4722)	
training:	Epoch: [3][83/233]	Loss 0.5860 (0.4736)	
training:	Epoch: [3][84/233]	Loss 0.4595 (0.4735)	
training:	Epoch: [3][85/233]	Loss 0.5292 (0.4741)	
training:	Epoch: [3][86/233]	Loss 0.3732 (0.4729)	
training:	Epoch: [3][87/233]	Loss 0.5934 (0.4743)	
training:	Epoch: [3][88/233]	Loss 0.4280 (0.4738)	
training:	Epoch: [3][89/233]	Loss 0.4265 (0.4733)	
training:	Epoch: [3][90/233]	Loss 0.4808 (0.4733)	
training:	Epoch: [3][91/233]	Loss 0.5826 (0.4745)	
training:	Epoch: [3][92/233]	Loss 0.3631 (0.4733)	
training:	Epoch: [3][93/233]	Loss 0.7736 (0.4766)	
training:	Epoch: [3][94/233]	Loss 0.5072 (0.4769)	
training:	Epoch: [3][95/233]	Loss 0.5002 (0.4771)	
training:	Epoch: [3][96/233]	Loss 0.3573 (0.4759)	
training:	Epoch: [3][97/233]	Loss 0.4718 (0.4758)	
training:	Epoch: [3][98/233]	Loss 0.3040 (0.4741)	
training:	Epoch: [3][99/233]	Loss 0.3533 (0.4729)	
training:	Epoch: [3][100/233]	Loss 0.3802 (0.4719)	
training:	Epoch: [3][101/233]	Loss 0.3834 (0.4711)	
training:	Epoch: [3][102/233]	Loss 0.4990 (0.4713)	
training:	Epoch: [3][103/233]	Loss 0.3958 (0.4706)	
training:	Epoch: [3][104/233]	Loss 0.3451 (0.4694)	
training:	Epoch: [3][105/233]	Loss 0.4202 (0.4689)	
training:	Epoch: [3][106/233]	Loss 0.3816 (0.4681)	
training:	Epoch: [3][107/233]	Loss 0.4766 (0.4682)	
training:	Epoch: [3][108/233]	Loss 0.3778 (0.4673)	
training:	Epoch: [3][109/233]	Loss 0.4453 (0.4671)	
training:	Epoch: [3][110/233]	Loss 0.5954 (0.4683)	
training:	Epoch: [3][111/233]	Loss 0.4964 (0.4686)	
training:	Epoch: [3][112/233]	Loss 0.4500 (0.4684)	
training:	Epoch: [3][113/233]	Loss 0.6166 (0.4697)	
training:	Epoch: [3][114/233]	Loss 0.4930 (0.4699)	
training:	Epoch: [3][115/233]	Loss 0.3549 (0.4689)	
training:	Epoch: [3][116/233]	Loss 0.4780 (0.4690)	
training:	Epoch: [3][117/233]	Loss 0.5600 (0.4698)	
training:	Epoch: [3][118/233]	Loss 0.3353 (0.4686)	
training:	Epoch: [3][119/233]	Loss 0.4212 (0.4682)	
training:	Epoch: [3][120/233]	Loss 0.3553 (0.4673)	
training:	Epoch: [3][121/233]	Loss 0.3811 (0.4666)	
training:	Epoch: [3][122/233]	Loss 0.5133 (0.4670)	
training:	Epoch: [3][123/233]	Loss 0.4816 (0.4671)	
training:	Epoch: [3][124/233]	Loss 0.4336 (0.4668)	
training:	Epoch: [3][125/233]	Loss 0.3794 (0.4661)	
training:	Epoch: [3][126/233]	Loss 0.4396 (0.4659)	
training:	Epoch: [3][127/233]	Loss 0.3506 (0.4650)	
training:	Epoch: [3][128/233]	Loss 0.4245 (0.4647)	
training:	Epoch: [3][129/233]	Loss 0.5332 (0.4652)	
training:	Epoch: [3][130/233]	Loss 0.3944 (0.4647)	
training:	Epoch: [3][131/233]	Loss 0.4029 (0.4642)	
training:	Epoch: [3][132/233]	Loss 0.3887 (0.4636)	
training:	Epoch: [3][133/233]	Loss 0.5306 (0.4641)	
training:	Epoch: [3][134/233]	Loss 0.5491 (0.4648)	
training:	Epoch: [3][135/233]	Loss 0.5044 (0.4651)	
training:	Epoch: [3][136/233]	Loss 0.4137 (0.4647)	
training:	Epoch: [3][137/233]	Loss 0.3803 (0.4641)	
training:	Epoch: [3][138/233]	Loss 0.4991 (0.4643)	
training:	Epoch: [3][139/233]	Loss 0.3779 (0.4637)	
training:	Epoch: [3][140/233]	Loss 0.3405 (0.4628)	
training:	Epoch: [3][141/233]	Loss 0.4140 (0.4625)	
training:	Epoch: [3][142/233]	Loss 0.3490 (0.4617)	
training:	Epoch: [3][143/233]	Loss 0.4946 (0.4619)	
training:	Epoch: [3][144/233]	Loss 0.3241 (0.4609)	
training:	Epoch: [3][145/233]	Loss 0.3349 (0.4601)	
training:	Epoch: [3][146/233]	Loss 0.4452 (0.4600)	
training:	Epoch: [3][147/233]	Loss 0.4246 (0.4597)	
training:	Epoch: [3][148/233]	Loss 0.4453 (0.4596)	
training:	Epoch: [3][149/233]	Loss 0.5588 (0.4603)	
training:	Epoch: [3][150/233]	Loss 0.4537 (0.4603)	
training:	Epoch: [3][151/233]	Loss 0.5012 (0.4605)	
training:	Epoch: [3][152/233]	Loss 0.3974 (0.4601)	
training:	Epoch: [3][153/233]	Loss 0.4388 (0.4600)	
training:	Epoch: [3][154/233]	Loss 0.4986 (0.4602)	
training:	Epoch: [3][155/233]	Loss 0.4538 (0.4602)	
training:	Epoch: [3][156/233]	Loss 0.4784 (0.4603)	
training:	Epoch: [3][157/233]	Loss 0.3953 (0.4599)	
training:	Epoch: [3][158/233]	Loss 0.3737 (0.4593)	
training:	Epoch: [3][159/233]	Loss 0.3577 (0.4587)	
training:	Epoch: [3][160/233]	Loss 0.5262 (0.4591)	
training:	Epoch: [3][161/233]	Loss 0.4793 (0.4592)	
training:	Epoch: [3][162/233]	Loss 0.5002 (0.4595)	
training:	Epoch: [3][163/233]	Loss 0.5968 (0.4603)	
training:	Epoch: [3][164/233]	Loss 0.3804 (0.4599)	
training:	Epoch: [3][165/233]	Loss 0.3147 (0.4590)	
training:	Epoch: [3][166/233]	Loss 0.3765 (0.4585)	
training:	Epoch: [3][167/233]	Loss 0.3813 (0.4580)	
training:	Epoch: [3][168/233]	Loss 0.4207 (0.4578)	
training:	Epoch: [3][169/233]	Loss 0.4364 (0.4577)	
training:	Epoch: [3][170/233]	Loss 0.4623 (0.4577)	
training:	Epoch: [3][171/233]	Loss 0.4386 (0.4576)	
training:	Epoch: [3][172/233]	Loss 0.3743 (0.4571)	
training:	Epoch: [3][173/233]	Loss 0.2529 (0.4559)	
training:	Epoch: [3][174/233]	Loss 0.4120 (0.4557)	
training:	Epoch: [3][175/233]	Loss 0.5267 (0.4561)	
training:	Epoch: [3][176/233]	Loss 0.4869 (0.4562)	
training:	Epoch: [3][177/233]	Loss 0.3843 (0.4558)	
training:	Epoch: [3][178/233]	Loss 0.3777 (0.4554)	
training:	Epoch: [3][179/233]	Loss 0.3981 (0.4551)	
training:	Epoch: [3][180/233]	Loss 0.5043 (0.4554)	
training:	Epoch: [3][181/233]	Loss 0.3363 (0.4547)	
training:	Epoch: [3][182/233]	Loss 0.4567 (0.4547)	
training:	Epoch: [3][183/233]	Loss 0.3639 (0.4542)	
training:	Epoch: [3][184/233]	Loss 0.4286 (0.4541)	
training:	Epoch: [3][185/233]	Loss 0.4988 (0.4543)	
training:	Epoch: [3][186/233]	Loss 0.3879 (0.4540)	
training:	Epoch: [3][187/233]	Loss 0.4328 (0.4538)	
training:	Epoch: [3][188/233]	Loss 0.4291 (0.4537)	
training:	Epoch: [3][189/233]	Loss 0.4565 (0.4537)	
training:	Epoch: [3][190/233]	Loss 0.5321 (0.4541)	
training:	Epoch: [3][191/233]	Loss 0.3692 (0.4537)	
training:	Epoch: [3][192/233]	Loss 0.2961 (0.4529)	
training:	Epoch: [3][193/233]	Loss 0.4591 (0.4529)	
training:	Epoch: [3][194/233]	Loss 0.5879 (0.4536)	
training:	Epoch: [3][195/233]	Loss 0.5624 (0.4542)	
training:	Epoch: [3][196/233]	Loss 0.2778 (0.4533)	
training:	Epoch: [3][197/233]	Loss 0.3392 (0.4527)	
training:	Epoch: [3][198/233]	Loss 0.5115 (0.4530)	
training:	Epoch: [3][199/233]	Loss 0.4603 (0.4530)	
training:	Epoch: [3][200/233]	Loss 0.4981 (0.4532)	
training:	Epoch: [3][201/233]	Loss 0.4755 (0.4533)	
training:	Epoch: [3][202/233]	Loss 0.5423 (0.4538)	
training:	Epoch: [3][203/233]	Loss 0.5098 (0.4541)	
training:	Epoch: [3][204/233]	Loss 0.4545 (0.4541)	
training:	Epoch: [3][205/233]	Loss 0.3284 (0.4535)	
training:	Epoch: [3][206/233]	Loss 0.4124 (0.4533)	
training:	Epoch: [3][207/233]	Loss 0.4494 (0.4532)	
training:	Epoch: [3][208/233]	Loss 0.5453 (0.4537)	
training:	Epoch: [3][209/233]	Loss 0.4593 (0.4537)	
training:	Epoch: [3][210/233]	Loss 0.5775 (0.4543)	
training:	Epoch: [3][211/233]	Loss 0.4161 (0.4541)	
training:	Epoch: [3][212/233]	Loss 0.3656 (0.4537)	
training:	Epoch: [3][213/233]	Loss 0.3214 (0.4531)	
training:	Epoch: [3][214/233]	Loss 0.3790 (0.4527)	
training:	Epoch: [3][215/233]	Loss 0.5781 (0.4533)	
training:	Epoch: [3][216/233]	Loss 0.3981 (0.4531)	
training:	Epoch: [3][217/233]	Loss 0.4262 (0.4529)	
training:	Epoch: [3][218/233]	Loss 0.3813 (0.4526)	
training:	Epoch: [3][219/233]	Loss 0.5104 (0.4529)	
training:	Epoch: [3][220/233]	Loss 0.4709 (0.4530)	
training:	Epoch: [3][221/233]	Loss 0.2945 (0.4522)	
training:	Epoch: [3][222/233]	Loss 0.3024 (0.4516)	
training:	Epoch: [3][223/233]	Loss 0.6037 (0.4522)	
training:	Epoch: [3][224/233]	Loss 0.4445 (0.4522)	
training:	Epoch: [3][225/233]	Loss 0.4257 (0.4521)	
training:	Epoch: [3][226/233]	Loss 0.3044 (0.4514)	
training:	Epoch: [3][227/233]	Loss 0.4578 (0.4515)	
training:	Epoch: [3][228/233]	Loss 0.3734 (0.4511)	
training:	Epoch: [3][229/233]	Loss 0.3659 (0.4507)	
training:	Epoch: [3][230/233]	Loss 0.4535 (0.4508)	
training:	Epoch: [3][231/233]	Loss 0.4193 (0.4506)	
training:	Epoch: [3][232/233]	Loss 0.4051 (0.4504)	
training:	Epoch: [3][233/233]	Loss 0.5144 (0.4507)	
Training:	 Loss: 0.4497

