Namespace(inputDirectory='data', outputDirectory='nval', DisFile='data/dis_sort_roi2dis2000.npy', task='sex', type='binary', feature=['Nos', 'FA1'], CUDA_id='0', data_id='0', dataset='Classification', norm=True, channels=1, epochs=200, tensorboard=True, net_architecture='TractGraphormer', batch_size=32, rate=3e-05, weight=0.0, sched_step=300, sched_gamma=0.1, printing_frequency=1, seed=0, alpha=0, remix_kappa=0, remix_tau=0, loss='CE', sigma=0, k=20, fl=64, nh=1)
Training the 'TractGraphormer' architecture

The following parameters are used:
Batch size:	32
Number of workers:	0
Learning rate:	3e-05
Weight decay:	0.0
Scheduler steps:	300
Scheduler gamma:	0.1
Number of epochs of training:	200
Number of input channels:	1

Data preparation
Reading data from:	./data
Training set size:	7473
Validation set size:	1869

Performing calculations on:	cuda:0

Pretraining:	Epoch 1/200
----------
training:	Epoch: [1][1/233]	Loss 0.8553 (0.8553)	
training:	Epoch: [1][2/233]	Loss 0.8353 (0.8453)	
training:	Epoch: [1][3/233]	Loss 0.8390 (0.8432)	
training:	Epoch: [1][4/233]	Loss 0.6602 (0.7975)	
training:	Epoch: [1][5/233]	Loss 0.7344 (0.7848)	
training:	Epoch: [1][6/233]	Loss 0.8368 (0.7935)	
training:	Epoch: [1][7/233]	Loss 0.7312 (0.7846)	
training:	Epoch: [1][8/233]	Loss 0.7148 (0.7759)	
training:	Epoch: [1][9/233]	Loss 0.6308 (0.7597)	
training:	Epoch: [1][10/233]	Loss 0.7307 (0.7568)	
training:	Epoch: [1][11/233]	Loss 0.7102 (0.7526)	
training:	Epoch: [1][12/233]	Loss 0.6959 (0.7479)	
training:	Epoch: [1][13/233]	Loss 0.7438 (0.7476)	
training:	Epoch: [1][14/233]	Loss 0.7228 (0.7458)	
training:	Epoch: [1][15/233]	Loss 0.6971 (0.7425)	
training:	Epoch: [1][16/233]	Loss 0.7116 (0.7406)	
training:	Epoch: [1][17/233]	Loss 0.6949 (0.7379)	
training:	Epoch: [1][18/233]	Loss 0.7139 (0.7366)	
training:	Epoch: [1][19/233]	Loss 0.6806 (0.7336)	
training:	Epoch: [1][20/233]	Loss 0.6905 (0.7315)	
training:	Epoch: [1][21/233]	Loss 0.7064 (0.7303)	
training:	Epoch: [1][22/233]	Loss 0.7307 (0.7303)	
training:	Epoch: [1][23/233]	Loss 0.6880 (0.7285)	
training:	Epoch: [1][24/233]	Loss 0.6871 (0.7267)	
training:	Epoch: [1][25/233]	Loss 0.6917 (0.7253)	
training:	Epoch: [1][26/233]	Loss 0.6613 (0.7229)	
training:	Epoch: [1][27/233]	Loss 0.6778 (0.7212)	
training:	Epoch: [1][28/233]	Loss 0.6909 (0.7201)	
training:	Epoch: [1][29/233]	Loss 0.7007 (0.7195)	
training:	Epoch: [1][30/233]	Loss 0.6697 (0.7178)	
training:	Epoch: [1][31/233]	Loss 0.6750 (0.7164)	
training:	Epoch: [1][32/233]	Loss 0.6915 (0.7156)	
training:	Epoch: [1][33/233]	Loss 0.7101 (0.7155)	
training:	Epoch: [1][34/233]	Loss 0.6841 (0.7146)	
training:	Epoch: [1][35/233]	Loss 0.7013 (0.7142)	
training:	Epoch: [1][36/233]	Loss 0.6808 (0.7133)	
training:	Epoch: [1][37/233]	Loss 0.6813 (0.7124)	
training:	Epoch: [1][38/233]	Loss 0.6707 (0.7113)	
training:	Epoch: [1][39/233]	Loss 0.6744 (0.7103)	
training:	Epoch: [1][40/233]	Loss 0.6660 (0.7092)	
training:	Epoch: [1][41/233]	Loss 0.6967 (0.7089)	
training:	Epoch: [1][42/233]	Loss 0.6720 (0.7081)	
training:	Epoch: [1][43/233]	Loss 0.6870 (0.7076)	
training:	Epoch: [1][44/233]	Loss 0.6652 (0.7066)	
training:	Epoch: [1][45/233]	Loss 0.6646 (0.7057)	
training:	Epoch: [1][46/233]	Loss 0.6783 (0.7051)	
training:	Epoch: [1][47/233]	Loss 0.6535 (0.7040)	
training:	Epoch: [1][48/233]	Loss 0.6921 (0.7037)	
training:	Epoch: [1][49/233]	Loss 0.6853 (0.7034)	
training:	Epoch: [1][50/233]	Loss 0.6749 (0.7028)	
training:	Epoch: [1][51/233]	Loss 0.6618 (0.7020)	
training:	Epoch: [1][52/233]	Loss 0.6578 (0.7011)	
training:	Epoch: [1][53/233]	Loss 0.6278 (0.6997)	
training:	Epoch: [1][54/233]	Loss 0.6366 (0.6986)	
training:	Epoch: [1][55/233]	Loss 0.6471 (0.6976)	
training:	Epoch: [1][56/233]	Loss 0.6769 (0.6973)	
training:	Epoch: [1][57/233]	Loss 0.6833 (0.6970)	
training:	Epoch: [1][58/233]	Loss 0.6554 (0.6963)	
training:	Epoch: [1][59/233]	Loss 0.6751 (0.6959)	
training:	Epoch: [1][60/233]	Loss 0.6311 (0.6949)	
training:	Epoch: [1][61/233]	Loss 0.6379 (0.6939)	
training:	Epoch: [1][62/233]	Loss 0.6750 (0.6936)	
training:	Epoch: [1][63/233]	Loss 0.7183 (0.6940)	
training:	Epoch: [1][64/233]	Loss 0.6400 (0.6932)	
training:	Epoch: [1][65/233]	Loss 0.6056 (0.6918)	
training:	Epoch: [1][66/233]	Loss 0.6816 (0.6917)	
training:	Epoch: [1][67/233]	Loss 0.6623 (0.6912)	
training:	Epoch: [1][68/233]	Loss 0.6590 (0.6908)	
training:	Epoch: [1][69/233]	Loss 0.6710 (0.6905)	
training:	Epoch: [1][70/233]	Loss 0.7364 (0.6911)	
training:	Epoch: [1][71/233]	Loss 0.6569 (0.6907)	
training:	Epoch: [1][72/233]	Loss 0.6298 (0.6898)	
training:	Epoch: [1][73/233]	Loss 0.6554 (0.6893)	
training:	Epoch: [1][74/233]	Loss 0.6941 (0.6894)	
training:	Epoch: [1][75/233]	Loss 0.6625 (0.6890)	
training:	Epoch: [1][76/233]	Loss 0.6642 (0.6887)	
training:	Epoch: [1][77/233]	Loss 0.6249 (0.6879)	
training:	Epoch: [1][78/233]	Loss 0.6549 (0.6875)	
training:	Epoch: [1][79/233]	Loss 0.5930 (0.6863)	
training:	Epoch: [1][80/233]	Loss 0.6254 (0.6855)	
training:	Epoch: [1][81/233]	Loss 0.6256 (0.6848)	
training:	Epoch: [1][82/233]	Loss 0.7134 (0.6851)	
training:	Epoch: [1][83/233]	Loss 0.6608 (0.6848)	
training:	Epoch: [1][84/233]	Loss 0.6234 (0.6841)	
training:	Epoch: [1][85/233]	Loss 0.6364 (0.6835)	
training:	Epoch: [1][86/233]	Loss 0.6944 (0.6837)	
training:	Epoch: [1][87/233]	Loss 0.6056 (0.6828)	
training:	Epoch: [1][88/233]	Loss 0.7067 (0.6830)	
training:	Epoch: [1][89/233]	Loss 0.7140 (0.6834)	
training:	Epoch: [1][90/233]	Loss 0.6416 (0.6829)	
training:	Epoch: [1][91/233]	Loss 0.6783 (0.6829)	
training:	Epoch: [1][92/233]	Loss 0.7360 (0.6834)	
training:	Epoch: [1][93/233]	Loss 0.6924 (0.6835)	
training:	Epoch: [1][94/233]	Loss 0.6796 (0.6835)	
training:	Epoch: [1][95/233]	Loss 0.6809 (0.6835)	
training:	Epoch: [1][96/233]	Loss 0.6208 (0.6828)	
training:	Epoch: [1][97/233]	Loss 0.6082 (0.6820)	
training:	Epoch: [1][98/233]	Loss 0.5974 (0.6812)	
training:	Epoch: [1][99/233]	Loss 0.6596 (0.6810)	
training:	Epoch: [1][100/233]	Loss 0.6493 (0.6806)	
training:	Epoch: [1][101/233]	Loss 0.7089 (0.6809)	
training:	Epoch: [1][102/233]	Loss 0.6852 (0.6810)	
training:	Epoch: [1][103/233]	Loss 0.6427 (0.6806)	
training:	Epoch: [1][104/233]	Loss 0.6700 (0.6805)	
training:	Epoch: [1][105/233]	Loss 0.6141 (0.6799)	
training:	Epoch: [1][106/233]	Loss 0.6462 (0.6795)	
training:	Epoch: [1][107/233]	Loss 0.6484 (0.6793)	
training:	Epoch: [1][108/233]	Loss 0.6453 (0.6789)	
training:	Epoch: [1][109/233]	Loss 0.6344 (0.6785)	
training:	Epoch: [1][110/233]	Loss 0.7102 (0.6788)	
training:	Epoch: [1][111/233]	Loss 0.5309 (0.6775)	
training:	Epoch: [1][112/233]	Loss 0.6680 (0.6774)	
training:	Epoch: [1][113/233]	Loss 0.6343 (0.6770)	
training:	Epoch: [1][114/233]	Loss 0.6511 (0.6768)	
training:	Epoch: [1][115/233]	Loss 0.6538 (0.6766)	
training:	Epoch: [1][116/233]	Loss 0.6259 (0.6762)	
training:	Epoch: [1][117/233]	Loss 0.6650 (0.6761)	
training:	Epoch: [1][118/233]	Loss 0.6158 (0.6756)	
training:	Epoch: [1][119/233]	Loss 0.6936 (0.6757)	
training:	Epoch: [1][120/233]	Loss 0.5547 (0.6747)	
training:	Epoch: [1][121/233]	Loss 0.6303 (0.6743)	
training:	Epoch: [1][122/233]	Loss 0.6748 (0.6743)	
training:	Epoch: [1][123/233]	Loss 0.5678 (0.6735)	
training:	Epoch: [1][124/233]	Loss 0.6278 (0.6731)	
training:	Epoch: [1][125/233]	Loss 0.5511 (0.6721)	
training:	Epoch: [1][126/233]	Loss 0.6614 (0.6720)	
training:	Epoch: [1][127/233]	Loss 0.5687 (0.6712)	
training:	Epoch: [1][128/233]	Loss 0.5993 (0.6707)	
training:	Epoch: [1][129/233]	Loss 0.7064 (0.6709)	
training:	Epoch: [1][130/233]	Loss 0.5830 (0.6703)	
training:	Epoch: [1][131/233]	Loss 0.6026 (0.6697)	
training:	Epoch: [1][132/233]	Loss 0.7532 (0.6704)	
training:	Epoch: [1][133/233]	Loss 0.5874 (0.6698)	
training:	Epoch: [1][134/233]	Loss 0.6802 (0.6698)	
training:	Epoch: [1][135/233]	Loss 0.6508 (0.6697)	
training:	Epoch: [1][136/233]	Loss 0.6662 (0.6697)	
training:	Epoch: [1][137/233]	Loss 0.6778 (0.6697)	
training:	Epoch: [1][138/233]	Loss 0.6200 (0.6694)	
training:	Epoch: [1][139/233]	Loss 0.6121 (0.6690)	
training:	Epoch: [1][140/233]	Loss 0.6159 (0.6686)	
training:	Epoch: [1][141/233]	Loss 0.6539 (0.6685)	
training:	Epoch: [1][142/233]	Loss 0.6147 (0.6681)	
training:	Epoch: [1][143/233]	Loss 0.6786 (0.6682)	
training:	Epoch: [1][144/233]	Loss 0.5744 (0.6675)	
training:	Epoch: [1][145/233]	Loss 0.5865 (0.6670)	
training:	Epoch: [1][146/233]	Loss 0.6484 (0.6668)	
training:	Epoch: [1][147/233]	Loss 0.6195 (0.6665)	
training:	Epoch: [1][148/233]	Loss 0.5315 (0.6656)	
training:	Epoch: [1][149/233]	Loss 0.6273 (0.6653)	
training:	Epoch: [1][150/233]	Loss 0.5956 (0.6649)	
training:	Epoch: [1][151/233]	Loss 0.5779 (0.6643)	
training:	Epoch: [1][152/233]	Loss 0.5475 (0.6635)	
training:	Epoch: [1][153/233]	Loss 0.6059 (0.6632)	
training:	Epoch: [1][154/233]	Loss 0.6291 (0.6629)	
training:	Epoch: [1][155/233]	Loss 0.5683 (0.6623)	
training:	Epoch: [1][156/233]	Loss 0.6756 (0.6624)	
training:	Epoch: [1][157/233]	Loss 0.6342 (0.6622)	
training:	Epoch: [1][158/233]	Loss 0.8118 (0.6632)	
training:	Epoch: [1][159/233]	Loss 0.5478 (0.6624)	
training:	Epoch: [1][160/233]	Loss 0.6383 (0.6623)	
training:	Epoch: [1][161/233]	Loss 0.5590 (0.6617)	
training:	Epoch: [1][162/233]	Loss 0.5990 (0.6613)	
training:	Epoch: [1][163/233]	Loss 0.6935 (0.6615)	
training:	Epoch: [1][164/233]	Loss 0.5942 (0.6611)	
training:	Epoch: [1][165/233]	Loss 0.6383 (0.6609)	
training:	Epoch: [1][166/233]	Loss 0.6429 (0.6608)	
training:	Epoch: [1][167/233]	Loss 0.6386 (0.6607)	
training:	Epoch: [1][168/233]	Loss 0.6231 (0.6605)	
training:	Epoch: [1][169/233]	Loss 0.6546 (0.6604)	
training:	Epoch: [1][170/233]	Loss 0.6177 (0.6602)	
training:	Epoch: [1][171/233]	Loss 0.6621 (0.6602)	
training:	Epoch: [1][172/233]	Loss 0.5754 (0.6597)	
training:	Epoch: [1][173/233]	Loss 0.6346 (0.6595)	
training:	Epoch: [1][174/233]	Loss 0.5327 (0.6588)	
training:	Epoch: [1][175/233]	Loss 0.5841 (0.6584)	
training:	Epoch: [1][176/233]	Loss 0.6362 (0.6583)	
training:	Epoch: [1][177/233]	Loss 0.5206 (0.6575)	
training:	Epoch: [1][178/233]	Loss 0.6675 (0.6575)	
training:	Epoch: [1][179/233]	Loss 0.7371 (0.6580)	
training:	Epoch: [1][180/233]	Loss 0.5690 (0.6575)	
training:	Epoch: [1][181/233]	Loss 0.6626 (0.6575)	
training:	Epoch: [1][182/233]	Loss 0.5424 (0.6569)	
training:	Epoch: [1][183/233]	Loss 0.6195 (0.6567)	
training:	Epoch: [1][184/233]	Loss 0.5863 (0.6563)	
training:	Epoch: [1][185/233]	Loss 0.5971 (0.6560)	
training:	Epoch: [1][186/233]	Loss 0.7662 (0.6566)	
training:	Epoch: [1][187/233]	Loss 0.6442 (0.6565)	
training:	Epoch: [1][188/233]	Loss 0.5811 (0.6561)	
training:	Epoch: [1][189/233]	Loss 0.5370 (0.6555)	
training:	Epoch: [1][190/233]	Loss 0.5621 (0.6550)	
training:	Epoch: [1][191/233]	Loss 0.6114 (0.6548)	
training:	Epoch: [1][192/233]	Loss 0.6544 (0.6547)	
training:	Epoch: [1][193/233]	Loss 0.6665 (0.6548)	
training:	Epoch: [1][194/233]	Loss 0.6593 (0.6548)	
training:	Epoch: [1][195/233]	Loss 0.5766 (0.6544)	
training:	Epoch: [1][196/233]	Loss 0.7323 (0.6548)	
training:	Epoch: [1][197/233]	Loss 0.6599 (0.6549)	
training:	Epoch: [1][198/233]	Loss 0.5355 (0.6543)	
training:	Epoch: [1][199/233]	Loss 0.5881 (0.6539)	
training:	Epoch: [1][200/233]	Loss 0.5602 (0.6535)	
training:	Epoch: [1][201/233]	Loss 0.5147 (0.6528)	
training:	Epoch: [1][202/233]	Loss 0.6741 (0.6529)	
training:	Epoch: [1][203/233]	Loss 0.6469 (0.6528)	
training:	Epoch: [1][204/233]	Loss 0.6169 (0.6527)	
training:	Epoch: [1][205/233]	Loss 0.6049 (0.6524)	
training:	Epoch: [1][206/233]	Loss 0.6850 (0.6526)	
training:	Epoch: [1][207/233]	Loss 0.5851 (0.6523)	
training:	Epoch: [1][208/233]	Loss 0.5405 (0.6517)	
training:	Epoch: [1][209/233]	Loss 0.5433 (0.6512)	
training:	Epoch: [1][210/233]	Loss 0.6537 (0.6512)	
training:	Epoch: [1][211/233]	Loss 0.7196 (0.6515)	
training:	Epoch: [1][212/233]	Loss 0.5661 (0.6511)	
training:	Epoch: [1][213/233]	Loss 0.6004 (0.6509)	
training:	Epoch: [1][214/233]	Loss 0.5856 (0.6506)	
training:	Epoch: [1][215/233]	Loss 0.6009 (0.6504)	
training:	Epoch: [1][216/233]	Loss 0.6848 (0.6505)	
training:	Epoch: [1][217/233]	Loss 0.6719 (0.6506)	
training:	Epoch: [1][218/233]	Loss 0.6578 (0.6507)	
training:	Epoch: [1][219/233]	Loss 0.6318 (0.6506)	
training:	Epoch: [1][220/233]	Loss 0.6897 (0.6507)	
training:	Epoch: [1][221/233]	Loss 0.6778 (0.6509)	
training:	Epoch: [1][222/233]	Loss 0.6022 (0.6506)	
training:	Epoch: [1][223/233]	Loss 0.6117 (0.6505)	
training:	Epoch: [1][224/233]	Loss 0.5338 (0.6500)	
training:	Epoch: [1][225/233]	Loss 0.5398 (0.6495)	
training:	Epoch: [1][226/233]	Loss 0.6252 (0.6494)	
training:	Epoch: [1][227/233]	Loss 0.6630 (0.6494)	
training:	Epoch: [1][228/233]	Loss 0.5875 (0.6491)	
training:	Epoch: [1][229/233]	Loss 0.6350 (0.6491)	
training:	Epoch: [1][230/233]	Loss 0.6278 (0.6490)	
training:	Epoch: [1][231/233]	Loss 0.6267 (0.6489)	
training:	Epoch: [1][232/233]	Loss 0.7195 (0.6492)	
training:	Epoch: [1][233/233]	Loss 0.6645 (0.6493)	
Training:	 Loss: 0.6478

Training:	 ACC: 0.6629 0.6695 0.8138 0.5119
Validation:	 ACC: 0.6494 0.6565 0.7988 0.5000
Validation:	 Best_BACC: 0.6494 0.6565 0.7988 0.5000
Validation:	 Loss: 0.6213
Pretraining:	Epoch 2/200
----------
training:	Epoch: [2][1/233]	Loss 0.5422 (0.5422)	
training:	Epoch: [2][2/233]	Loss 0.6298 (0.5860)	
training:	Epoch: [2][3/233]	Loss 0.6199 (0.5973)	
training:	Epoch: [2][4/233]	Loss 0.5375 (0.5824)	
training:	Epoch: [2][5/233]	Loss 0.5872 (0.5833)	
training:	Epoch: [2][6/233]	Loss 0.5311 (0.5746)	
training:	Epoch: [2][7/233]	Loss 0.6322 (0.5828)	
training:	Epoch: [2][8/233]	Loss 0.4827 (0.5703)	
training:	Epoch: [2][9/233]	Loss 0.6028 (0.5739)	
training:	Epoch: [2][10/233]	Loss 0.6594 (0.5825)	
training:	Epoch: [2][11/233]	Loss 0.6808 (0.5914)	
training:	Epoch: [2][12/233]	Loss 0.5829 (0.5907)	
training:	Epoch: [2][13/233]	Loss 0.6665 (0.5965)	
training:	Epoch: [2][14/233]	Loss 0.5484 (0.5931)	
training:	Epoch: [2][15/233]	Loss 0.5510 (0.5903)	
training:	Epoch: [2][16/233]	Loss 0.5421 (0.5873)	
training:	Epoch: [2][17/233]	Loss 0.6103 (0.5886)	
training:	Epoch: [2][18/233]	Loss 0.6054 (0.5896)	
training:	Epoch: [2][19/233]	Loss 0.6560 (0.5931)	
training:	Epoch: [2][20/233]	Loss 0.6098 (0.5939)	
training:	Epoch: [2][21/233]	Loss 0.4965 (0.5893)	
training:	Epoch: [2][22/233]	Loss 0.6088 (0.5901)	
training:	Epoch: [2][23/233]	Loss 0.5724 (0.5894)	
training:	Epoch: [2][24/233]	Loss 0.6065 (0.5901)	
training:	Epoch: [2][25/233]	Loss 0.6584 (0.5928)	
training:	Epoch: [2][26/233]	Loss 0.4734 (0.5882)	
training:	Epoch: [2][27/233]	Loss 0.5386 (0.5864)	
training:	Epoch: [2][28/233]	Loss 0.6390 (0.5883)	
training:	Epoch: [2][29/233]	Loss 0.5493 (0.5869)	
training:	Epoch: [2][30/233]	Loss 0.5896 (0.5870)	
training:	Epoch: [2][31/233]	Loss 0.5104 (0.5845)	
training:	Epoch: [2][32/233]	Loss 0.6667 (0.5871)	
training:	Epoch: [2][33/233]	Loss 0.5377 (0.5856)	
training:	Epoch: [2][34/233]	Loss 0.5287 (0.5839)	
training:	Epoch: [2][35/233]	Loss 0.6676 (0.5863)	
training:	Epoch: [2][36/233]	Loss 0.6405 (0.5878)	
training:	Epoch: [2][37/233]	Loss 0.6854 (0.5905)	
training:	Epoch: [2][38/233]	Loss 0.4985 (0.5881)	
training:	Epoch: [2][39/233]	Loss 0.6063 (0.5885)	
training:	Epoch: [2][40/233]	Loss 0.4846 (0.5859)	
training:	Epoch: [2][41/233]	Loss 0.5542 (0.5851)	
training:	Epoch: [2][42/233]	Loss 0.7081 (0.5881)	
training:	Epoch: [2][43/233]	Loss 0.5041 (0.5861)	
training:	Epoch: [2][44/233]	Loss 0.5328 (0.5849)	
training:	Epoch: [2][45/233]	Loss 0.6503 (0.5864)	
training:	Epoch: [2][46/233]	Loss 0.5434 (0.5854)	
training:	Epoch: [2][47/233]	Loss 0.5619 (0.5849)	
training:	Epoch: [2][48/233]	Loss 0.6543 (0.5864)	
training:	Epoch: [2][49/233]	Loss 0.6539 (0.5877)	
training:	Epoch: [2][50/233]	Loss 0.4792 (0.5856)	
training:	Epoch: [2][51/233]	Loss 0.6062 (0.5860)	
training:	Epoch: [2][52/233]	Loss 0.6477 (0.5872)	
training:	Epoch: [2][53/233]	Loss 0.6519 (0.5884)	
training:	Epoch: [2][54/233]	Loss 0.5792 (0.5882)	
training:	Epoch: [2][55/233]	Loss 0.5273 (0.5871)	
training:	Epoch: [2][56/233]	Loss 0.6441 (0.5881)	
training:	Epoch: [2][57/233]	Loss 0.5333 (0.5872)	
training:	Epoch: [2][58/233]	Loss 0.5820 (0.5871)	
training:	Epoch: [2][59/233]	Loss 0.6184 (0.5876)	
training:	Epoch: [2][60/233]	Loss 0.6798 (0.5891)	
training:	Epoch: [2][61/233]	Loss 0.6518 (0.5902)	
training:	Epoch: [2][62/233]	Loss 0.4789 (0.5884)	
training:	Epoch: [2][63/233]	Loss 0.6139 (0.5888)	
training:	Epoch: [2][64/233]	Loss 0.5757 (0.5886)	
training:	Epoch: [2][65/233]	Loss 0.6706 (0.5898)	
training:	Epoch: [2][66/233]	Loss 0.6735 (0.5911)	
training:	Epoch: [2][67/233]	Loss 0.5779 (0.5909)	
training:	Epoch: [2][68/233]	Loss 0.6060 (0.5911)	
training:	Epoch: [2][69/233]	Loss 0.5934 (0.5912)	
training:	Epoch: [2][70/233]	Loss 0.3704 (0.5880)	
training:	Epoch: [2][71/233]	Loss 0.6020 (0.5882)	
training:	Epoch: [2][72/233]	Loss 0.6600 (0.5892)	
training:	Epoch: [2][73/233]	Loss 0.4720 (0.5876)	
training:	Epoch: [2][74/233]	Loss 0.5459 (0.5870)	
training:	Epoch: [2][75/233]	Loss 0.5815 (0.5870)	
training:	Epoch: [2][76/233]	Loss 0.5967 (0.5871)	
training:	Epoch: [2][77/233]	Loss 0.6446 (0.5878)	
training:	Epoch: [2][78/233]	Loss 0.5934 (0.5879)	
training:	Epoch: [2][79/233]	Loss 0.6672 (0.5889)	
training:	Epoch: [2][80/233]	Loss 0.5476 (0.5884)	
training:	Epoch: [2][81/233]	Loss 0.6532 (0.5892)	
training:	Epoch: [2][82/233]	Loss 0.5330 (0.5885)	
training:	Epoch: [2][83/233]	Loss 0.4836 (0.5872)	
training:	Epoch: [2][84/233]	Loss 0.4656 (0.5858)	
training:	Epoch: [2][85/233]	Loss 0.5778 (0.5857)	
training:	Epoch: [2][86/233]	Loss 0.5746 (0.5856)	
training:	Epoch: [2][87/233]	Loss 0.6489 (0.5863)	
training:	Epoch: [2][88/233]	Loss 0.5740 (0.5862)	
training:	Epoch: [2][89/233]	Loss 0.5868 (0.5862)	
training:	Epoch: [2][90/233]	Loss 0.7469 (0.5880)	
training:	Epoch: [2][91/233]	Loss 0.5776 (0.5878)	
training:	Epoch: [2][92/233]	Loss 0.4879 (0.5868)	
training:	Epoch: [2][93/233]	Loss 0.7048 (0.5880)	
training:	Epoch: [2][94/233]	Loss 0.5167 (0.5873)	
training:	Epoch: [2][95/233]	Loss 0.6053 (0.5875)	
training:	Epoch: [2][96/233]	Loss 0.6898 (0.5885)	
training:	Epoch: [2][97/233]	Loss 0.4753 (0.5874)	
training:	Epoch: [2][98/233]	Loss 0.6510 (0.5880)	
training:	Epoch: [2][99/233]	Loss 0.6158 (0.5883)	
training:	Epoch: [2][100/233]	Loss 0.4704 (0.5871)	
training:	Epoch: [2][101/233]	Loss 0.5272 (0.5865)	
training:	Epoch: [2][102/233]	Loss 0.4681 (0.5854)	
training:	Epoch: [2][103/233]	Loss 0.5803 (0.5853)	
training:	Epoch: [2][104/233]	Loss 0.6224 (0.5857)	
training:	Epoch: [2][105/233]	Loss 0.5369 (0.5852)	
training:	Epoch: [2][106/233]	Loss 0.5258 (0.5846)	
training:	Epoch: [2][107/233]	Loss 0.5611 (0.5844)	
training:	Epoch: [2][108/233]	Loss 0.6175 (0.5847)	
training:	Epoch: [2][109/233]	Loss 0.5640 (0.5845)	
training:	Epoch: [2][110/233]	Loss 0.4936 (0.5837)	
training:	Epoch: [2][111/233]	Loss 0.5321 (0.5832)	
training:	Epoch: [2][112/233]	Loss 0.6166 (0.5835)	
training:	Epoch: [2][113/233]	Loss 0.5129 (0.5829)	
training:	Epoch: [2][114/233]	Loss 0.4848 (0.5821)	
training:	Epoch: [2][115/233]	Loss 0.6795 (0.5829)	
training:	Epoch: [2][116/233]	Loss 0.5019 (0.5822)	
training:	Epoch: [2][117/233]	Loss 0.5708 (0.5821)	
training:	Epoch: [2][118/233]	Loss 0.5566 (0.5819)	
training:	Epoch: [2][119/233]	Loss 0.5584 (0.5817)	
training:	Epoch: [2][120/233]	Loss 0.6275 (0.5821)	
training:	Epoch: [2][121/233]	Loss 0.5538 (0.5818)	
training:	Epoch: [2][122/233]	Loss 0.5057 (0.5812)	
training:	Epoch: [2][123/233]	Loss 0.6263 (0.5816)	
training:	Epoch: [2][124/233]	Loss 0.7326 (0.5828)	
training:	Epoch: [2][125/233]	Loss 0.5203 (0.5823)	
training:	Epoch: [2][126/233]	Loss 0.5720 (0.5822)	
training:	Epoch: [2][127/233]	Loss 0.4846 (0.5814)	
training:	Epoch: [2][128/233]	Loss 0.6529 (0.5820)	
training:	Epoch: [2][129/233]	Loss 0.6653 (0.5827)	
training:	Epoch: [2][130/233]	Loss 0.6510 (0.5832)	
training:	Epoch: [2][131/233]	Loss 0.5478 (0.5829)	
training:	Epoch: [2][132/233]	Loss 0.4406 (0.5818)	
training:	Epoch: [2][133/233]	Loss 0.5220 (0.5814)	
training:	Epoch: [2][134/233]	Loss 0.4632 (0.5805)	
training:	Epoch: [2][135/233]	Loss 0.5389 (0.5802)	
training:	Epoch: [2][136/233]	Loss 0.5367 (0.5799)	
training:	Epoch: [2][137/233]	Loss 0.5672 (0.5798)	
training:	Epoch: [2][138/233]	Loss 0.6059 (0.5800)	
training:	Epoch: [2][139/233]	Loss 0.5547 (0.5798)	
training:	Epoch: [2][140/233]	Loss 0.6137 (0.5800)	
training:	Epoch: [2][141/233]	Loss 0.4963 (0.5794)	
training:	Epoch: [2][142/233]	Loss 0.4948 (0.5788)	
training:	Epoch: [2][143/233]	Loss 0.6050 (0.5790)	
training:	Epoch: [2][144/233]	Loss 0.5835 (0.5791)	
training:	Epoch: [2][145/233]	Loss 0.6246 (0.5794)	
training:	Epoch: [2][146/233]	Loss 0.6065 (0.5796)	
training:	Epoch: [2][147/233]	Loss 0.5500 (0.5794)	
training:	Epoch: [2][148/233]	Loss 0.5138 (0.5789)	
training:	Epoch: [2][149/233]	Loss 0.6369 (0.5793)	
training:	Epoch: [2][150/233]	Loss 0.6508 (0.5798)	
training:	Epoch: [2][151/233]	Loss 0.4753 (0.5791)	
training:	Epoch: [2][152/233]	Loss 0.4835 (0.5785)	
training:	Epoch: [2][153/233]	Loss 0.6261 (0.5788)	
training:	Epoch: [2][154/233]	Loss 0.4018 (0.5776)	
training:	Epoch: [2][155/233]	Loss 0.5508 (0.5774)	
training:	Epoch: [2][156/233]	Loss 0.5233 (0.5771)	
training:	Epoch: [2][157/233]	Loss 0.4658 (0.5764)	
training:	Epoch: [2][158/233]	Loss 0.5921 (0.5765)	
training:	Epoch: [2][159/233]	Loss 0.5669 (0.5764)	
training:	Epoch: [2][160/233]	Loss 0.5475 (0.5762)	
training:	Epoch: [2][161/233]	Loss 0.4771 (0.5756)	
training:	Epoch: [2][162/233]	Loss 0.4802 (0.5750)	
training:	Epoch: [2][163/233]	Loss 0.6614 (0.5756)	
training:	Epoch: [2][164/233]	Loss 0.6078 (0.5758)	
training:	Epoch: [2][165/233]	Loss 0.5250 (0.5755)	
training:	Epoch: [2][166/233]	Loss 0.4760 (0.5749)	
training:	Epoch: [2][167/233]	Loss 0.5151 (0.5745)	
training:	Epoch: [2][168/233]	Loss 0.4601 (0.5738)	
training:	Epoch: [2][169/233]	Loss 0.4701 (0.5732)	
training:	Epoch: [2][170/233]	Loss 0.6176 (0.5735)	
training:	Epoch: [2][171/233]	Loss 0.5078 (0.5731)	
training:	Epoch: [2][172/233]	Loss 0.5074 (0.5727)	
training:	Epoch: [2][173/233]	Loss 0.4913 (0.5722)	
training:	Epoch: [2][174/233]	Loss 0.5091 (0.5719)	
training:	Epoch: [2][175/233]	Loss 0.5049 (0.5715)	
training:	Epoch: [2][176/233]	Loss 0.4526 (0.5708)	
training:	Epoch: [2][177/233]	Loss 0.4836 (0.5703)	
training:	Epoch: [2][178/233]	Loss 0.7277 (0.5712)	
training:	Epoch: [2][179/233]	Loss 0.5128 (0.5709)	
training:	Epoch: [2][180/233]	Loss 0.4498 (0.5702)	
training:	Epoch: [2][181/233]	Loss 0.5809 (0.5703)	
training:	Epoch: [2][182/233]	Loss 0.5619 (0.5702)	
training:	Epoch: [2][183/233]	Loss 0.5102 (0.5699)	
training:	Epoch: [2][184/233]	Loss 0.4354 (0.5692)	
training:	Epoch: [2][185/233]	Loss 0.5509 (0.5691)	
training:	Epoch: [2][186/233]	Loss 0.4654 (0.5685)	
training:	Epoch: [2][187/233]	Loss 0.5507 (0.5684)	
training:	Epoch: [2][188/233]	Loss 0.4896 (0.5680)	
training:	Epoch: [2][189/233]	Loss 0.5537 (0.5679)	
training:	Epoch: [2][190/233]	Loss 0.5096 (0.5676)	
training:	Epoch: [2][191/233]	Loss 0.6711 (0.5681)	
training:	Epoch: [2][192/233]	Loss 0.5620 (0.5681)	
training:	Epoch: [2][193/233]	Loss 0.5004 (0.5678)	
training:	Epoch: [2][194/233]	Loss 0.4437 (0.5671)	
training:	Epoch: [2][195/233]	Loss 0.6677 (0.5676)	
training:	Epoch: [2][196/233]	Loss 0.5636 (0.5676)	
training:	Epoch: [2][197/233]	Loss 0.6077 (0.5678)	
training:	Epoch: [2][198/233]	Loss 0.5772 (0.5679)	
training:	Epoch: [2][199/233]	Loss 0.4972 (0.5675)	
training:	Epoch: [2][200/233]	Loss 0.4351 (0.5669)	
training:	Epoch: [2][201/233]	Loss 0.6074 (0.5671)	
training:	Epoch: [2][202/233]	Loss 0.6415 (0.5674)	
training:	Epoch: [2][203/233]	Loss 0.5070 (0.5671)	
training:	Epoch: [2][204/233]	Loss 0.5945 (0.5673)	
training:	Epoch: [2][205/233]	Loss 0.5132 (0.5670)	
training:	Epoch: [2][206/233]	Loss 0.4802 (0.5666)	
training:	Epoch: [2][207/233]	Loss 0.4987 (0.5662)	
training:	Epoch: [2][208/233]	Loss 0.4386 (0.5656)	
training:	Epoch: [2][209/233]	Loss 0.5931 (0.5658)	
training:	Epoch: [2][210/233]	Loss 0.5519 (0.5657)	
training:	Epoch: [2][211/233]	Loss 0.4797 (0.5653)	
training:	Epoch: [2][212/233]	Loss 0.5237 (0.5651)	
training:	Epoch: [2][213/233]	Loss 0.4853 (0.5647)	
training:	Epoch: [2][214/233]	Loss 0.4310 (0.5641)	
training:	Epoch: [2][215/233]	Loss 0.4580 (0.5636)	
training:	Epoch: [2][216/233]	Loss 0.5850 (0.5637)	
training:	Epoch: [2][217/233]	Loss 0.4635 (0.5632)	
training:	Epoch: [2][218/233]	Loss 0.3837 (0.5624)	
training:	Epoch: [2][219/233]	Loss 0.4002 (0.5617)	
training:	Epoch: [2][220/233]	Loss 0.3780 (0.5608)	
training:	Epoch: [2][221/233]	Loss 0.7008 (0.5615)	
training:	Epoch: [2][222/233]	Loss 0.5127 (0.5613)	
training:	Epoch: [2][223/233]	Loss 0.4438 (0.5607)	
training:	Epoch: [2][224/233]	Loss 0.5644 (0.5607)	
training:	Epoch: [2][225/233]	Loss 0.4800 (0.5604)	
training:	Epoch: [2][226/233]	Loss 0.5111 (0.5602)	
training:	Epoch: [2][227/233]	Loss 0.4546 (0.5597)	
training:	Epoch: [2][228/233]	Loss 0.4688 (0.5593)	
training:	Epoch: [2][229/233]	Loss 0.3603 (0.5584)	
training:	Epoch: [2][230/233]	Loss 0.4170 (0.5578)	
training:	Epoch: [2][231/233]	Loss 0.4706 (0.5574)	
training:	Epoch: [2][232/233]	Loss 0.5416 (0.5574)	
training:	Epoch: [2][233/233]	Loss 0.4564 (0.5569)	
Training:	 Loss: 0.5557

Training:	 ACC: 0.7680 0.7644 0.6846 0.8514
Validation:	 ACC: 0.7334 0.7293 0.6466 0.8202
Validation:	 Best_BACC: 0.7334 0.7293 0.6466 0.8202
Validation:	 Loss: 0.5369
Pretraining:	Epoch 3/200
----------
training:	Epoch: [3][1/233]	Loss 0.4197 (0.4197)	
training:	Epoch: [3][2/233]	Loss 0.4216 (0.4206)	
training:	Epoch: [3][3/233]	Loss 0.5710 (0.4708)	
training:	Epoch: [3][4/233]	Loss 0.5984 (0.5027)	
training:	Epoch: [3][5/233]	Loss 0.4149 (0.4851)	
training:	Epoch: [3][6/233]	Loss 0.5266 (0.4920)	
training:	Epoch: [3][7/233]	Loss 0.5346 (0.4981)	
training:	Epoch: [3][8/233]	Loss 0.6074 (0.5118)	
training:	Epoch: [3][9/233]	Loss 0.6239 (0.5242)	
training:	Epoch: [3][10/233]	Loss 0.6405 (0.5359)	
training:	Epoch: [3][11/233]	Loss 0.4824 (0.5310)	
training:	Epoch: [3][12/233]	Loss 0.4148 (0.5213)	
training:	Epoch: [3][13/233]	Loss 0.5256 (0.5217)	
training:	Epoch: [3][14/233]	Loss 0.4269 (0.5149)	
training:	Epoch: [3][15/233]	Loss 0.3587 (0.5045)	
training:	Epoch: [3][16/233]	Loss 0.5050 (0.5045)	
training:	Epoch: [3][17/233]	Loss 0.5634 (0.5080)	
training:	Epoch: [3][18/233]	Loss 0.5237 (0.5088)	
training:	Epoch: [3][19/233]	Loss 0.4809 (0.5074)	
training:	Epoch: [3][20/233]	Loss 0.6062 (0.5123)	
training:	Epoch: [3][21/233]	Loss 0.4439 (0.5091)	
training:	Epoch: [3][22/233]	Loss 0.6107 (0.5137)	
training:	Epoch: [3][23/233]	Loss 0.4800 (0.5122)	
training:	Epoch: [3][24/233]	Loss 0.3905 (0.5071)	
training:	Epoch: [3][25/233]	Loss 0.4530 (0.5050)	
training:	Epoch: [3][26/233]	Loss 0.4536 (0.5030)	
training:	Epoch: [3][27/233]	Loss 0.4019 (0.4993)	
training:	Epoch: [3][28/233]	Loss 0.5689 (0.5017)	
training:	Epoch: [3][29/233]	Loss 0.5618 (0.5038)	
training:	Epoch: [3][30/233]	Loss 0.4397 (0.5017)	
training:	Epoch: [3][31/233]	Loss 0.4290 (0.4993)	
training:	Epoch: [3][32/233]	Loss 0.4322 (0.4972)	
training:	Epoch: [3][33/233]	Loss 0.4100 (0.4946)	
training:	Epoch: [3][34/233]	Loss 0.5849 (0.4972)	
training:	Epoch: [3][35/233]	Loss 0.5272 (0.4981)	
training:	Epoch: [3][36/233]	Loss 0.4969 (0.4981)	
training:	Epoch: [3][37/233]	Loss 0.5218 (0.4987)	
training:	Epoch: [3][38/233]	Loss 0.5192 (0.4992)	
training:	Epoch: [3][39/233]	Loss 0.4534 (0.4981)	
training:	Epoch: [3][40/233]	Loss 0.4681 (0.4973)	
training:	Epoch: [3][41/233]	Loss 0.5119 (0.4977)	
training:	Epoch: [3][42/233]	Loss 0.4603 (0.4968)	
training:	Epoch: [3][43/233]	Loss 0.3337 (0.4930)	
training:	Epoch: [3][44/233]	Loss 0.4646 (0.4924)	
training:	Epoch: [3][45/233]	Loss 0.5961 (0.4947)	
training:	Epoch: [3][46/233]	Loss 0.3611 (0.4918)	
training:	Epoch: [3][47/233]	Loss 0.5249 (0.4925)	
training:	Epoch: [3][48/233]	Loss 0.4325 (0.4912)	
training:	Epoch: [3][49/233]	Loss 0.3564 (0.4885)	
training:	Epoch: [3][50/233]	Loss 0.5655 (0.4900)	
training:	Epoch: [3][51/233]	Loss 0.4733 (0.4897)	
training:	Epoch: [3][52/233]	Loss 0.4405 (0.4887)	
training:	Epoch: [3][53/233]	Loss 0.5374 (0.4896)	
training:	Epoch: [3][54/233]	Loss 0.4681 (0.4892)	
training:	Epoch: [3][55/233]	Loss 0.3888 (0.4874)	
training:	Epoch: [3][56/233]	Loss 0.3878 (0.4856)	
training:	Epoch: [3][57/233]	Loss 0.3894 (0.4840)	
training:	Epoch: [3][58/233]	Loss 0.4601 (0.4835)	
training:	Epoch: [3][59/233]	Loss 0.4680 (0.4833)	
training:	Epoch: [3][60/233]	Loss 0.5186 (0.4839)	
training:	Epoch: [3][61/233]	Loss 0.5177 (0.4844)	
training:	Epoch: [3][62/233]	Loss 0.4287 (0.4835)	
training:	Epoch: [3][63/233]	Loss 0.6311 (0.4859)	
training:	Epoch: [3][64/233]	Loss 0.5370 (0.4867)	
training:	Epoch: [3][65/233]	Loss 0.5253 (0.4873)	
training:	Epoch: [3][66/233]	Loss 0.6273 (0.4894)	
training:	Epoch: [3][67/233]	Loss 0.5322 (0.4900)	
training:	Epoch: [3][68/233]	Loss 0.4082 (0.4888)	
training:	Epoch: [3][69/233]	Loss 0.5711 (0.4900)	
training:	Epoch: [3][70/233]	Loss 0.3876 (0.4885)	
training:	Epoch: [3][71/233]	Loss 0.5151 (0.4889)	
training:	Epoch: [3][72/233]	Loss 0.3589 (0.4871)	
training:	Epoch: [3][73/233]	Loss 0.4015 (0.4859)	
training:	Epoch: [3][74/233]	Loss 0.5446 (0.4867)	
training:	Epoch: [3][75/233]	Loss 0.4021 (0.4856)	
training:	Epoch: [3][76/233]	Loss 0.4124 (0.4846)	
training:	Epoch: [3][77/233]	Loss 0.4849 (0.4846)	
training:	Epoch: [3][78/233]	Loss 0.3673 (0.4831)	
training:	Epoch: [3][79/233]	Loss 0.3665 (0.4817)	
training:	Epoch: [3][80/233]	Loss 0.5808 (0.4829)	
training:	Epoch: [3][81/233]	Loss 0.4448 (0.4824)	
training:	Epoch: [3][82/233]	Loss 0.3932 (0.4813)	
training:	Epoch: [3][83/233]	Loss 0.4449 (0.4809)	
training:	Epoch: [3][84/233]	Loss 0.7017 (0.4835)	
training:	Epoch: [3][85/233]	Loss 0.4566 (0.4832)	
training:	Epoch: [3][86/233]	Loss 0.5427 (0.4839)	
training:	Epoch: [3][87/233]	Loss 0.4833 (0.4839)	
training:	Epoch: [3][88/233]	Loss 0.6677 (0.4860)	
training:	Epoch: [3][89/233]	Loss 0.5042 (0.4862)	
training:	Epoch: [3][90/233]	Loss 0.4629 (0.4859)	
training:	Epoch: [3][91/233]	Loss 0.5458 (0.4866)	
training:	Epoch: [3][92/233]	Loss 0.4473 (0.4862)	
training:	Epoch: [3][93/233]	Loss 0.4313 (0.4856)	
training:	Epoch: [3][94/233]	Loss 0.4611 (0.4853)	
training:	Epoch: [3][95/233]	Loss 0.6435 (0.4870)	
training:	Epoch: [3][96/233]	Loss 0.4119 (0.4862)	
training:	Epoch: [3][97/233]	Loss 0.4333 (0.4857)	
training:	Epoch: [3][98/233]	Loss 0.2929 (0.4837)	
training:	Epoch: [3][99/233]	Loss 0.5971 (0.4848)	
training:	Epoch: [3][100/233]	Loss 0.3379 (0.4834)	
training:	Epoch: [3][101/233]	Loss 0.3765 (0.4823)	
training:	Epoch: [3][102/233]	Loss 0.6104 (0.4836)	
training:	Epoch: [3][103/233]	Loss 0.4453 (0.4832)	
training:	Epoch: [3][104/233]	Loss 0.4248 (0.4826)	
training:	Epoch: [3][105/233]	Loss 0.4103 (0.4819)	
training:	Epoch: [3][106/233]	Loss 0.3036 (0.4803)	
training:	Epoch: [3][107/233]	Loss 0.3370 (0.4789)	
training:	Epoch: [3][108/233]	Loss 0.3444 (0.4777)	
training:	Epoch: [3][109/233]	Loss 0.6150 (0.4789)	
training:	Epoch: [3][110/233]	Loss 0.3888 (0.4781)	
training:	Epoch: [3][111/233]	Loss 0.4330 (0.4777)	
training:	Epoch: [3][112/233]	Loss 0.4124 (0.4771)	
training:	Epoch: [3][113/233]	Loss 0.4986 (0.4773)	
training:	Epoch: [3][114/233]	Loss 0.4741 (0.4773)	
training:	Epoch: [3][115/233]	Loss 0.4144 (0.4767)	
training:	Epoch: [3][116/233]	Loss 0.5638 (0.4775)	
training:	Epoch: [3][117/233]	Loss 0.5583 (0.4782)	
training:	Epoch: [3][118/233]	Loss 0.5188 (0.4785)	
training:	Epoch: [3][119/233]	Loss 0.5326 (0.4790)	
training:	Epoch: [3][120/233]	Loss 0.4964 (0.4791)	
training:	Epoch: [3][121/233]	Loss 0.4072 (0.4785)	
training:	Epoch: [3][122/233]	Loss 0.3267 (0.4773)	
training:	Epoch: [3][123/233]	Loss 0.4819 (0.4773)	
training:	Epoch: [3][124/233]	Loss 0.4510 (0.4771)	
training:	Epoch: [3][125/233]	Loss 0.5487 (0.4777)	
training:	Epoch: [3][126/233]	Loss 0.4732 (0.4776)	
training:	Epoch: [3][127/233]	Loss 0.4309 (0.4773)	
training:	Epoch: [3][128/233]	Loss 0.4204 (0.4768)	
training:	Epoch: [3][129/233]	Loss 0.4627 (0.4767)	
training:	Epoch: [3][130/233]	Loss 0.3671 (0.4759)	
training:	Epoch: [3][131/233]	Loss 0.5366 (0.4763)	
training:	Epoch: [3][132/233]	Loss 0.4705 (0.4763)	
training:	Epoch: [3][133/233]	Loss 0.4272 (0.4759)	
training:	Epoch: [3][134/233]	Loss 0.3778 (0.4752)	
training:	Epoch: [3][135/233]	Loss 0.3982 (0.4746)	
training:	Epoch: [3][136/233]	Loss 0.5013 (0.4748)	
training:	Epoch: [3][137/233]	Loss 0.5264 (0.4752)	
training:	Epoch: [3][138/233]	Loss 0.4061 (0.4747)	
training:	Epoch: [3][139/233]	Loss 0.4341 (0.4744)	
training:	Epoch: [3][140/233]	Loss 0.4912 (0.4745)	
training:	Epoch: [3][141/233]	Loss 0.4168 (0.4741)	
training:	Epoch: [3][142/233]	Loss 0.4568 (0.4740)	
training:	Epoch: [3][143/233]	Loss 0.5761 (0.4747)	
training:	Epoch: [3][144/233]	Loss 0.3632 (0.4739)	
training:	Epoch: [3][145/233]	Loss 0.5102 (0.4742)	
training:	Epoch: [3][146/233]	Loss 0.4819 (0.4742)	
training:	Epoch: [3][147/233]	Loss 0.5258 (0.4746)	
training:	Epoch: [3][148/233]	Loss 0.5389 (0.4750)	
training:	Epoch: [3][149/233]	Loss 0.4233 (0.4747)	
training:	Epoch: [3][150/233]	Loss 0.5234 (0.4750)	
training:	Epoch: [3][151/233]	Loss 0.5587 (0.4756)	
training:	Epoch: [3][152/233]	Loss 0.3080 (0.4745)	
training:	Epoch: [3][153/233]	Loss 0.4717 (0.4744)	
training:	Epoch: [3][154/233]	Loss 0.5289 (0.4748)	
training:	Epoch: [3][155/233]	Loss 0.3918 (0.4743)	
training:	Epoch: [3][156/233]	Loss 0.4900 (0.4744)	
training:	Epoch: [3][157/233]	Loss 0.4317 (0.4741)	
training:	Epoch: [3][158/233]	Loss 0.4731 (0.4741)	
training:	Epoch: [3][159/233]	Loss 0.5068 (0.4743)	
training:	Epoch: [3][160/233]	Loss 0.3339 (0.4734)	
training:	Epoch: [3][161/233]	Loss 0.3399 (0.4726)	
training:	Epoch: [3][162/233]	Loss 0.4402 (0.4724)	
training:	Epoch: [3][163/233]	Loss 0.4599 (0.4723)	
training:	Epoch: [3][164/233]	Loss 0.3760 (0.4717)	
training:	Epoch: [3][165/233]	Loss 0.5876 (0.4724)	
training:	Epoch: [3][166/233]	Loss 0.4503 (0.4723)	
training:	Epoch: [3][167/233]	Loss 0.6414 (0.4733)	
training:	Epoch: [3][168/233]	Loss 0.5824 (0.4739)	
training:	Epoch: [3][169/233]	Loss 0.3421 (0.4732)	
training:	Epoch: [3][170/233]	Loss 0.4321 (0.4729)	
training:	Epoch: [3][171/233]	Loss 0.4477 (0.4728)	
training:	Epoch: [3][172/233]	Loss 0.5311 (0.4731)	
training:	Epoch: [3][173/233]	Loss 0.6505 (0.4741)	
training:	Epoch: [3][174/233]	Loss 0.6218 (0.4750)	
training:	Epoch: [3][175/233]	Loss 0.3757 (0.4744)	
training:	Epoch: [3][176/233]	Loss 0.3254 (0.4736)	
training:	Epoch: [3][177/233]	Loss 0.3440 (0.4728)	
training:	Epoch: [3][178/233]	Loss 0.4511 (0.4727)	
training:	Epoch: [3][179/233]	Loss 0.4846 (0.4728)	
training:	Epoch: [3][180/233]	Loss 0.4751 (0.4728)	
training:	Epoch: [3][181/233]	Loss 0.3977 (0.4724)	
training:	Epoch: [3][182/233]	Loss 0.4723 (0.4724)	
training:	Epoch: [3][183/233]	Loss 0.5040 (0.4726)	
training:	Epoch: [3][184/233]	Loss 0.4701 (0.4725)	
training:	Epoch: [3][185/233]	Loss 0.3565 (0.4719)	
training:	Epoch: [3][186/233]	Loss 0.4990 (0.4721)	
training:	Epoch: [3][187/233]	Loss 0.5709 (0.4726)	
training:	Epoch: [3][188/233]	Loss 0.4539 (0.4725)	
training:	Epoch: [3][189/233]	Loss 0.4932 (0.4726)	
training:	Epoch: [3][190/233]	Loss 0.4231 (0.4723)	
training:	Epoch: [3][191/233]	Loss 0.5617 (0.4728)	
training:	Epoch: [3][192/233]	Loss 0.4044 (0.4725)	
training:	Epoch: [3][193/233]	Loss 0.4630 (0.4724)	
training:	Epoch: [3][194/233]	Loss 0.3983 (0.4720)	
training:	Epoch: [3][195/233]	Loss 0.4780 (0.4721)	
training:	Epoch: [3][196/233]	Loss 0.3621 (0.4715)	
training:	Epoch: [3][197/233]	Loss 0.5778 (0.4720)	
training:	Epoch: [3][198/233]	Loss 0.7140 (0.4733)	
training:	Epoch: [3][199/233]	Loss 0.4386 (0.4731)	
training:	Epoch: [3][200/233]	Loss 0.5006 (0.4732)	
training:	Epoch: [3][201/233]	Loss 0.5039 (0.4734)	
training:	Epoch: [3][202/233]	Loss 0.4325 (0.4732)	
training:	Epoch: [3][203/233]	Loss 0.5349 (0.4735)	
training:	Epoch: [3][204/233]	Loss 0.4037 (0.4731)	
training:	Epoch: [3][205/233]	Loss 0.3436 (0.4725)	
training:	Epoch: [3][206/233]	Loss 0.3633 (0.4720)	
training:	Epoch: [3][207/233]	Loss 0.3949 (0.4716)	
training:	Epoch: [3][208/233]	Loss 0.4652 (0.4716)	
training:	Epoch: [3][209/233]	Loss 0.4076 (0.4713)	
training:	Epoch: [3][210/233]	Loss 0.4235 (0.4710)	
training:	Epoch: [3][211/233]	Loss 0.5538 (0.4714)	
training:	Epoch: [3][212/233]	Loss 0.4219 (0.4712)	
training:	Epoch: [3][213/233]	Loss 0.4382 (0.4710)	
training:	Epoch: [3][214/233]	Loss 0.5413 (0.4714)	
training:	Epoch: [3][215/233]	Loss 0.4258 (0.4712)	
training:	Epoch: [3][216/233]	Loss 0.5374 (0.4715)	
training:	Epoch: [3][217/233]	Loss 0.3635 (0.4710)	
training:	Epoch: [3][218/233]	Loss 0.3352 (0.4703)	
training:	Epoch: [3][219/233]	Loss 0.5608 (0.4708)	
training:	Epoch: [3][220/233]	Loss 0.3646 (0.4703)	
training:	Epoch: [3][221/233]	Loss 0.3762 (0.4698)	
training:	Epoch: [3][222/233]	Loss 0.4016 (0.4695)	
training:	Epoch: [3][223/233]	Loss 0.4837 (0.4696)	
training:	Epoch: [3][224/233]	Loss 0.4353 (0.4694)	
training:	Epoch: [3][225/233]	Loss 0.4419 (0.4693)	
training:	Epoch: [3][226/233]	Loss 0.4395 (0.4692)	
training:	Epoch: [3][227/233]	Loss 0.3578 (0.4687)	
training:	Epoch: [3][228/233]	Loss 0.3952 (0.4684)	
training:	Epoch: [3][229/233]	Loss 0.3198 (0.4677)	
training:	Epoch: [3][230/233]	Loss 0.3999 (0.4674)	
training:	Epoch: [3][231/233]	Loss 0.3522 (0.4669)	
training:	Epoch: [3][232/233]	Loss 0.3927 (0.4666)	
training:	Epoch: [3][233/233]	Loss 0.3774 (0.4662)	
Training:	 Loss: 0.4652

Training:	 ACC: 0.8206 0.8200 0.8077 0.8335
Validation:	 ACC: 0.7870 0.7865 0.7763 0.7978
Validation:	 Best_BACC: 0.7870 0.7865 0.7763 0.7978
Validation:	 Loss: 0.4669
Pretraining:	Epoch 4/200
----------
training:	Epoch: [4][1/233]	Loss 0.3081 (0.3081)	
training:	Epoch: [4][2/233]	Loss 0.4514 (0.3797)	
training:	Epoch: [4][3/233]	Loss 0.2302 (0.3299)	
training:	Epoch: [4][4/233]	Loss 0.4688 (0.3646)	
training:	Epoch: [4][5/233]	Loss 0.3868 (0.3691)	
training:	Epoch: [4][6/233]	Loss 0.3261 (0.3619)	
training:	Epoch: [4][7/233]	Loss 0.5217 (0.3847)	
training:	Epoch: [4][8/233]	Loss 0.2568 (0.3687)	
training:	Epoch: [4][9/233]	Loss 0.4261 (0.3751)	
training:	Epoch: [4][10/233]	Loss 0.3318 (0.3708)	
training:	Epoch: [4][11/233]	Loss 0.5108 (0.3835)	
training:	Epoch: [4][12/233]	Loss 0.3269 (0.3788)	
training:	Epoch: [4][13/233]	Loss 0.5092 (0.3888)	
training:	Epoch: [4][14/233]	Loss 0.4636 (0.3942)	
training:	Epoch: [4][15/233]	Loss 0.3905 (0.3939)	
training:	Epoch: [4][16/233]	Loss 0.4060 (0.3947)	
training:	Epoch: [4][17/233]	Loss 0.3354 (0.3912)	
training:	Epoch: [4][18/233]	Loss 0.3367 (0.3882)	
training:	Epoch: [4][19/233]	Loss 0.4739 (0.3927)	
training:	Epoch: [4][20/233]	Loss 0.3445 (0.3903)	
training:	Epoch: [4][21/233]	Loss 0.3383 (0.3878)	
training:	Epoch: [4][22/233]	Loss 0.3255 (0.3850)	
training:	Epoch: [4][23/233]	Loss 0.5892 (0.3938)	
training:	Epoch: [4][24/233]	Loss 0.5620 (0.4008)	
training:	Epoch: [4][25/233]	Loss 0.4247 (0.4018)	
training:	Epoch: [4][26/233]	Loss 0.2929 (0.3976)	
training:	Epoch: [4][27/233]	Loss 0.3834 (0.3971)	
training:	Epoch: [4][28/233]	Loss 0.3511 (0.3954)	
training:	Epoch: [4][29/233]	Loss 0.3668 (0.3945)	
training:	Epoch: [4][30/233]	Loss 0.4901 (0.3976)	
training:	Epoch: [4][31/233]	Loss 0.4239 (0.3985)	
training:	Epoch: [4][32/233]	Loss 0.3746 (0.3977)	
training:	Epoch: [4][33/233]	Loss 0.3370 (0.3959)	
training:	Epoch: [4][34/233]	Loss 0.3976 (0.3960)	
training:	Epoch: [4][35/233]	Loss 0.3732 (0.3953)	
training:	Epoch: [4][36/233]	Loss 0.3452 (0.3939)	
training:	Epoch: [4][37/233]	Loss 0.4961 (0.3967)	
training:	Epoch: [4][38/233]	Loss 0.4365 (0.3977)	
training:	Epoch: [4][39/233]	Loss 0.2902 (0.3950)	
training:	Epoch: [4][40/233]	Loss 0.3968 (0.3950)	
training:	Epoch: [4][41/233]	Loss 0.6262 (0.4006)	
training:	Epoch: [4][42/233]	Loss 0.5306 (0.4037)	
training:	Epoch: [4][43/233]	Loss 0.4204 (0.4041)	
training:	Epoch: [4][44/233]	Loss 0.6453 (0.4096)	
training:	Epoch: [4][45/233]	Loss 0.3178 (0.4076)	
training:	Epoch: [4][46/233]	Loss 0.4903 (0.4094)	
training:	Epoch: [4][47/233]	Loss 0.2931 (0.4069)	
training:	Epoch: [4][48/233]	Loss 0.5310 (0.4095)	
training:	Epoch: [4][49/233]	Loss 0.5050 (0.4114)	
training:	Epoch: [4][50/233]	Loss 0.4689 (0.4126)	
training:	Epoch: [4][51/233]	Loss 0.5377 (0.4150)	
training:	Epoch: [4][52/233]	Loss 0.4045 (0.4148)	
training:	Epoch: [4][53/233]	Loss 0.2774 (0.4122)	
training:	Epoch: [4][54/233]	Loss 0.4407 (0.4128)	
training:	Epoch: [4][55/233]	Loss 0.4465 (0.4134)	
training:	Epoch: [4][56/233]	Loss 0.4214 (0.4135)	
training:	Epoch: [4][57/233]	Loss 0.4036 (0.4133)	
training:	Epoch: [4][58/233]	Loss 0.4892 (0.4147)	
training:	Epoch: [4][59/233]	Loss 0.3168 (0.4130)	
training:	Epoch: [4][60/233]	Loss 0.4390 (0.4134)	
training:	Epoch: [4][61/233]	Loss 0.4389 (0.4138)	
training:	Epoch: [4][62/233]	Loss 0.5139 (0.4155)	
training:	Epoch: [4][63/233]	Loss 0.5803 (0.4181)	
training:	Epoch: [4][64/233]	Loss 0.4338 (0.4183)	
training:	Epoch: [4][65/233]	Loss 0.4133 (0.4182)	
training:	Epoch: [4][66/233]	Loss 0.5331 (0.4200)	
training:	Epoch: [4][67/233]	Loss 0.4972 (0.4211)	
training:	Epoch: [4][68/233]	Loss 0.5359 (0.4228)	
training:	Epoch: [4][69/233]	Loss 0.4017 (0.4225)	
training:	Epoch: [4][70/233]	Loss 0.6780 (0.4262)	
training:	Epoch: [4][71/233]	Loss 0.4367 (0.4263)	
training:	Epoch: [4][72/233]	Loss 0.4561 (0.4267)	
training:	Epoch: [4][73/233]	Loss 0.4503 (0.4271)	
training:	Epoch: [4][74/233]	Loss 0.3998 (0.4267)	
training:	Epoch: [4][75/233]	Loss 0.3592 (0.4258)	
training:	Epoch: [4][76/233]	Loss 0.3930 (0.4254)	
training:	Epoch: [4][77/233]	Loss 0.2801 (0.4235)	
training:	Epoch: [4][78/233]	Loss 0.3039 (0.4219)	
training:	Epoch: [4][79/233]	Loss 0.3570 (0.4211)	
training:	Epoch: [4][80/233]	Loss 0.3663 (0.4204)	
training:	Epoch: [4][81/233]	Loss 0.5043 (0.4215)	
training:	Epoch: [4][82/233]	Loss 0.5787 (0.4234)	
training:	Epoch: [4][83/233]	Loss 0.4918 (0.4242)	
training:	Epoch: [4][84/233]	Loss 0.3495 (0.4233)	
training:	Epoch: [4][85/233]	Loss 0.3349 (0.4223)	
training:	Epoch: [4][86/233]	Loss 0.4242 (0.4223)	
training:	Epoch: [4][87/233]	Loss 0.3766 (0.4218)	
training:	Epoch: [4][88/233]	Loss 0.3441 (0.4209)	
training:	Epoch: [4][89/233]	Loss 0.4742 (0.4215)	
training:	Epoch: [4][90/233]	Loss 0.3552 (0.4208)	
training:	Epoch: [4][91/233]	Loss 0.5428 (0.4221)	
training:	Epoch: [4][92/233]	Loss 0.5246 (0.4232)	
training:	Epoch: [4][93/233]	Loss 0.4385 (0.4234)	
training:	Epoch: [4][94/233]	Loss 0.4825 (0.4240)	
training:	Epoch: [4][95/233]	Loss 0.4171 (0.4239)	
training:	Epoch: [4][96/233]	Loss 0.3335 (0.4230)	
training:	Epoch: [4][97/233]	Loss 0.5422 (0.4242)	
training:	Epoch: [4][98/233]	Loss 0.5202 (0.4252)	
training:	Epoch: [4][99/233]	Loss 0.4074 (0.4250)	
training:	Epoch: [4][100/233]	Loss 0.4004 (0.4248)	
training:	Epoch: [4][101/233]	Loss 0.4394 (0.4249)	
training:	Epoch: [4][102/233]	Loss 0.4133 (0.4248)	
training:	Epoch: [4][103/233]	Loss 0.4249 (0.4248)	
training:	Epoch: [4][104/233]	Loss 0.3081 (0.4237)	
training:	Epoch: [4][105/233]	Loss 0.3153 (0.4226)	
training:	Epoch: [4][106/233]	Loss 0.4749 (0.4231)	
training:	Epoch: [4][107/233]	Loss 0.4440 (0.4233)	
training:	Epoch: [4][108/233]	Loss 0.3952 (0.4231)	
training:	Epoch: [4][109/233]	Loss 0.4077 (0.4229)	
training:	Epoch: [4][110/233]	Loss 0.4237 (0.4229)	
training:	Epoch: [4][111/233]	Loss 0.4606 (0.4233)	
training:	Epoch: [4][112/233]	Loss 0.2225 (0.4215)	
training:	Epoch: [4][113/233]	Loss 0.3446 (0.4208)	
training:	Epoch: [4][114/233]	Loss 0.2969 (0.4197)	
training:	Epoch: [4][115/233]	Loss 0.4699 (0.4202)	
training:	Epoch: [4][116/233]	Loss 0.3280 (0.4194)	
training:	Epoch: [4][117/233]	Loss 0.4216 (0.4194)	
training:	Epoch: [4][118/233]	Loss 0.4846 (0.4199)	
training:	Epoch: [4][119/233]	Loss 0.5619 (0.4211)	
training:	Epoch: [4][120/233]	Loss 0.3733 (0.4207)	
training:	Epoch: [4][121/233]	Loss 0.3173 (0.4199)	
training:	Epoch: [4][122/233]	Loss 0.5429 (0.4209)	
training:	Epoch: [4][123/233]	Loss 0.3442 (0.4203)	
training:	Epoch: [4][124/233]	Loss 0.3931 (0.4200)	
training:	Epoch: [4][125/233]	Loss 0.5130 (0.4208)	
training:	Epoch: [4][126/233]	Loss 0.4048 (0.4207)	
training:	Epoch: [4][127/233]	Loss 0.4430 (0.4208)	
training:	Epoch: [4][128/233]	Loss 0.3032 (0.4199)	
training:	Epoch: [4][129/233]	Loss 0.5073 (0.4206)	
training:	Epoch: [4][130/233]	Loss 0.3846 (0.4203)	
training:	Epoch: [4][131/233]	Loss 0.4075 (0.4202)	
training:	Epoch: [4][132/233]	Loss 0.3405 (0.4196)	
training:	Epoch: [4][133/233]	Loss 0.5861 (0.4209)	
training:	Epoch: [4][134/233]	Loss 0.5128 (0.4215)	
training:	Epoch: [4][135/233]	Loss 0.3326 (0.4209)	
training:	Epoch: [4][136/233]	Loss 0.3182 (0.4201)	
training:	Epoch: [4][137/233]	Loss 0.3564 (0.4197)	
training:	Epoch: [4][138/233]	Loss 0.3496 (0.4192)	
training:	Epoch: [4][139/233]	Loss 0.3212 (0.4185)	
training:	Epoch: [4][140/233]	Loss 0.3855 (0.4182)	
training:	Epoch: [4][141/233]	Loss 0.3279 (0.4176)	
training:	Epoch: [4][142/233]	Loss 0.2725 (0.4166)	
training:	Epoch: [4][143/233]	Loss 0.5933 (0.4178)	
training:	Epoch: [4][144/233]	Loss 0.3541 (0.4174)	
training:	Epoch: [4][145/233]	Loss 0.3373 (0.4168)	
training:	Epoch: [4][146/233]	Loss 0.3827 (0.4166)	
training:	Epoch: [4][147/233]	Loss 0.3202 (0.4159)	
training:	Epoch: [4][148/233]	Loss 0.4330 (0.4160)	
training:	Epoch: [4][149/233]	Loss 0.4493 (0.4163)	
training:	Epoch: [4][150/233]	Loss 0.6175 (0.4176)	
training:	Epoch: [4][151/233]	Loss 0.4110 (0.4175)	
training:	Epoch: [4][152/233]	Loss 0.5191 (0.4182)	
training:	Epoch: [4][153/233]	Loss 0.3353 (0.4177)	
training:	Epoch: [4][154/233]	Loss 0.4847 (0.4181)	
training:	Epoch: [4][155/233]	Loss 0.4113 (0.4181)	
training:	Epoch: [4][156/233]	Loss 0.3174 (0.4174)	
training:	Epoch: [4][157/233]	Loss 0.3247 (0.4168)	
training:	Epoch: [4][158/233]	Loss 0.2689 (0.4159)	
training:	Epoch: [4][159/233]	Loss 0.5281 (0.4166)	
training:	Epoch: [4][160/233]	Loss 0.3390 (0.4161)	
training:	Epoch: [4][161/233]	Loss 0.4472 (0.4163)	
training:	Epoch: [4][162/233]	Loss 0.4686 (0.4166)	
training:	Epoch: [4][163/233]	Loss 0.4450 (0.4168)	
training:	Epoch: [4][164/233]	Loss 0.5022 (0.4173)	
training:	Epoch: [4][165/233]	Loss 0.4054 (0.4173)	
training:	Epoch: [4][166/233]	Loss 0.5252 (0.4179)	
training:	Epoch: [4][167/233]	Loss 0.4133 (0.4179)	
training:	Epoch: [4][168/233]	Loss 0.2807 (0.4171)	
training:	Epoch: [4][169/233]	Loss 0.4557 (0.4173)	
training:	Epoch: [4][170/233]	Loss 0.4098 (0.4172)	
training:	Epoch: [4][171/233]	Loss 0.3981 (0.4171)	
training:	Epoch: [4][172/233]	Loss 0.4315 (0.4172)	
training:	Epoch: [4][173/233]	Loss 0.4241 (0.4173)	
training:	Epoch: [4][174/233]	Loss 0.3318 (0.4168)	
training:	Epoch: [4][175/233]	Loss 0.4687 (0.4171)	
training:	Epoch: [4][176/233]	Loss 0.4586 (0.4173)	
training:	Epoch: [4][177/233]	Loss 0.2613 (0.4164)	
training:	Epoch: [4][178/233]	Loss 0.4268 (0.4165)	
training:	Epoch: [4][179/233]	Loss 0.4797 (0.4168)	
training:	Epoch: [4][180/233]	Loss 0.5647 (0.4176)	
training:	Epoch: [4][181/233]	Loss 0.4347 (0.4177)	
training:	Epoch: [4][182/233]	Loss 0.2351 (0.4167)	
training:	Epoch: [4][183/233]	Loss 0.3475 (0.4164)	
training:	Epoch: [4][184/233]	Loss 0.3468 (0.4160)	
training:	Epoch: [4][185/233]	Loss 0.5285 (0.4166)	
training:	Epoch: [4][186/233]	Loss 0.3011 (0.4160)	
training:	Epoch: [4][187/233]	Loss 0.5138 (0.4165)	
training:	Epoch: [4][188/233]	Loss 0.3594 (0.4162)	
training:	Epoch: [4][189/233]	Loss 0.5003 (0.4166)	
training:	Epoch: [4][190/233]	Loss 0.3313 (0.4162)	
training:	Epoch: [4][191/233]	Loss 0.3208 (0.4157)	
training:	Epoch: [4][192/233]	Loss 0.2542 (0.4148)	
training:	Epoch: [4][193/233]	Loss 0.4219 (0.4149)	
training:	Epoch: [4][194/233]	Loss 0.5000 (0.4153)	
training:	Epoch: [4][195/233]	Loss 0.4733 (0.4156)	
training:	Epoch: [4][196/233]	Loss 0.2961 (0.4150)	
training:	Epoch: [4][197/233]	Loss 0.4581 (0.4152)	
training:	Epoch: [4][198/233]	Loss 0.4695 (0.4155)	
training:	Epoch: [4][199/233]	Loss 0.4351 (0.4156)	
training:	Epoch: [4][200/233]	Loss 0.4967 (0.4160)	
training:	Epoch: [4][201/233]	Loss 0.4152 (0.4160)	
training:	Epoch: [4][202/233]	Loss 0.3576 (0.4157)	
training:	Epoch: [4][203/233]	Loss 0.2534 (0.4149)	
training:	Epoch: [4][204/233]	Loss 0.6001 (0.4158)	
training:	Epoch: [4][205/233]	Loss 0.4819 (0.4161)	
training:	Epoch: [4][206/233]	Loss 0.3712 (0.4159)	
training:	Epoch: [4][207/233]	Loss 0.2539 (0.4151)	
training:	Epoch: [4][208/233]	Loss 0.3945 (0.4150)	
training:	Epoch: [4][209/233]	Loss 0.3693 (0.4148)	
training:	Epoch: [4][210/233]	Loss 0.4885 (0.4152)	
training:	Epoch: [4][211/233]	Loss 0.2254 (0.4143)	
training:	Epoch: [4][212/233]	Loss 0.3622 (0.4140)	
training:	Epoch: [4][213/233]	Loss 0.3832 (0.4139)	
training:	Epoch: [4][214/233]	Loss 0.4659 (0.4141)	
training:	Epoch: [4][215/233]	Loss 0.3572 (0.4139)	
training:	Epoch: [4][216/233]	Loss 0.2862 (0.4133)	
training:	Epoch: [4][217/233]	Loss 0.4588 (0.4135)	
training:	Epoch: [4][218/233]	Loss 0.4847 (0.4138)	
training:	Epoch: [4][219/233]	Loss 0.2108 (0.4129)	
training:	Epoch: [4][220/233]	Loss 0.3105 (0.4124)	
training:	Epoch: [4][221/233]	Loss 0.4819 (0.4127)	
training:	Epoch: [4][222/233]	Loss 0.4964 (0.4131)	
training:	Epoch: [4][223/233]	Loss 0.4578 (0.4133)	
training:	Epoch: [4][224/233]	Loss 0.3195 (0.4129)	
training:	Epoch: [4][225/233]	Loss 0.4129 (0.4129)	
training:	Epoch: [4][226/233]	Loss 0.4525 (0.4131)	
training:	Epoch: [4][227/233]	Loss 0.3679 (0.4129)	
training:	Epoch: [4][228/233]	Loss 0.4984 (0.4132)	
training:	Epoch: [4][229/233]	Loss 0.3498 (0.4130)	
training:	Epoch: [4][230/233]	Loss 0.3981 (0.4129)	
training:	Epoch: [4][231/233]	Loss 0.3872 (0.4128)	
training:	Epoch: [4][232/233]	Loss 0.3440 (0.4125)	
training:	Epoch: [4][233/233]	Loss 0.4904 (0.4128)	
Training:	 Loss: 0.4119

Training:	 ACC: 0.8367 0.8381 0.8692 0.8041
Validation:	 ACC: 0.7895 0.7919 0.8396 0.7393
Validation:	 Best_BACC: 0.7895 0.7919 0.8396 0.7393
Validation:	 Loss: 0.4510
Pretraining:	Epoch 5/200
----------
training:	Epoch: [5][1/233]	Loss 0.4943 (0.4943)	
training:	Epoch: [5][2/233]	Loss 0.2209 (0.3576)	
training:	Epoch: [5][3/233]	Loss 0.4036 (0.3729)	
training:	Epoch: [5][4/233]	Loss 0.2693 (0.3470)	
training:	Epoch: [5][5/233]	Loss 0.2554 (0.3287)	
training:	Epoch: [5][6/233]	Loss 0.3015 (0.3242)	
training:	Epoch: [5][7/233]	Loss 0.5883 (0.3619)	
training:	Epoch: [5][8/233]	Loss 0.3646 (0.3622)	
training:	Epoch: [5][9/233]	Loss 0.4853 (0.3759)	
training:	Epoch: [5][10/233]	Loss 0.5389 (0.3922)	
training:	Epoch: [5][11/233]	Loss 0.2649 (0.3806)	
training:	Epoch: [5][12/233]	Loss 0.5992 (0.3988)	
training:	Epoch: [5][13/233]	Loss 0.3725 (0.3968)	
training:	Epoch: [5][14/233]	Loss 0.4538 (0.4009)	
training:	Epoch: [5][15/233]	Loss 0.4235 (0.4024)	
training:	Epoch: [5][16/233]	Loss 0.4741 (0.4069)	
training:	Epoch: [5][17/233]	Loss 0.5421 (0.4148)	
training:	Epoch: [5][18/233]	Loss 0.4038 (0.4142)	
training:	Epoch: [5][19/233]	Loss 0.5565 (0.4217)	
training:	Epoch: [5][20/233]	Loss 0.2257 (0.4119)	
training:	Epoch: [5][21/233]	Loss 0.3146 (0.4073)	
training:	Epoch: [5][22/233]	Loss 0.3438 (0.4044)	
training:	Epoch: [5][23/233]	Loss 0.3932 (0.4039)	
training:	Epoch: [5][24/233]	Loss 0.3564 (0.4019)	
training:	Epoch: [5][25/233]	Loss 0.3985 (0.4018)	
training:	Epoch: [5][26/233]	Loss 0.4300 (0.4029)	
training:	Epoch: [5][27/233]	Loss 0.3250 (0.4000)	
training:	Epoch: [5][28/233]	Loss 0.2490 (0.3946)	
training:	Epoch: [5][29/233]	Loss 0.2140 (0.3884)	
training:	Epoch: [5][30/233]	Loss 0.3222 (0.3862)	
training:	Epoch: [5][31/233]	Loss 0.5183 (0.3904)	
training:	Epoch: [5][32/233]	Loss 0.2289 (0.3854)	
training:	Epoch: [5][33/233]	Loss 0.3126 (0.3832)	
training:	Epoch: [5][34/233]	Loss 0.5218 (0.3872)	
training:	Epoch: [5][35/233]	Loss 0.4244 (0.3883)	
training:	Epoch: [5][36/233]	Loss 0.2308 (0.3839)	
training:	Epoch: [5][37/233]	Loss 0.4792 (0.3865)	
training:	Epoch: [5][38/233]	Loss 0.1707 (0.3808)	
training:	Epoch: [5][39/233]	Loss 0.3260 (0.3794)	
training:	Epoch: [5][40/233]	Loss 0.4443 (0.3810)	
training:	Epoch: [5][41/233]	Loss 0.3673 (0.3807)	
training:	Epoch: [5][42/233]	Loss 0.3925 (0.3810)	
training:	Epoch: [5][43/233]	Loss 0.5056 (0.3839)	
training:	Epoch: [5][44/233]	Loss 0.2824 (0.3816)	
training:	Epoch: [5][45/233]	Loss 0.4196 (0.3824)	
training:	Epoch: [5][46/233]	Loss 0.3102 (0.3809)	
training:	Epoch: [5][47/233]	Loss 0.4280 (0.3819)	
training:	Epoch: [5][48/233]	Loss 0.4656 (0.3836)	
training:	Epoch: [5][49/233]	Loss 0.4170 (0.3843)	
training:	Epoch: [5][50/233]	Loss 0.5546 (0.3877)	
training:	Epoch: [5][51/233]	Loss 0.3450 (0.3869)	
training:	Epoch: [5][52/233]	Loss 0.5564 (0.3901)	
training:	Epoch: [5][53/233]	Loss 0.2983 (0.3884)	
training:	Epoch: [5][54/233]	Loss 0.5134 (0.3907)	
training:	Epoch: [5][55/233]	Loss 0.4552 (0.3919)	
training:	Epoch: [5][56/233]	Loss 0.3290 (0.3907)	
training:	Epoch: [5][57/233]	Loss 0.4969 (0.3926)	
training:	Epoch: [5][58/233]	Loss 0.2910 (0.3909)	
training:	Epoch: [5][59/233]	Loss 0.5266 (0.3932)	
training:	Epoch: [5][60/233]	Loss 0.3604 (0.3926)	
training:	Epoch: [5][61/233]	Loss 0.3868 (0.3925)	
training:	Epoch: [5][62/233]	Loss 0.3709 (0.3922)	
training:	Epoch: [5][63/233]	Loss 0.3506 (0.3915)	
training:	Epoch: [5][64/233]	Loss 0.3407 (0.3907)	
training:	Epoch: [5][65/233]	Loss 0.4043 (0.3909)	
training:	Epoch: [5][66/233]	Loss 0.3133 (0.3897)	
training:	Epoch: [5][67/233]	Loss 0.2478 (0.3876)	
training:	Epoch: [5][68/233]	Loss 0.3764 (0.3875)	
training:	Epoch: [5][69/233]	Loss 0.3432 (0.3868)	
training:	Epoch: [5][70/233]	Loss 0.2545 (0.3849)	
training:	Epoch: [5][71/233]	Loss 0.3441 (0.3844)	
training:	Epoch: [5][72/233]	Loss 0.5396 (0.3865)	
training:	Epoch: [5][73/233]	Loss 0.3585 (0.3861)	
training:	Epoch: [5][74/233]	Loss 0.4542 (0.3871)	
training:	Epoch: [5][75/233]	Loss 0.2377 (0.3851)	
training:	Epoch: [5][76/233]	Loss 0.3250 (0.3843)	
training:	Epoch: [5][77/233]	Loss 0.4839 (0.3856)	
training:	Epoch: [5][78/233]	Loss 0.3240 (0.3848)	
training:	Epoch: [5][79/233]	Loss 0.3601 (0.3845)	
training:	Epoch: [5][80/233]	Loss 0.1900 (0.3820)	
training:	Epoch: [5][81/233]	Loss 0.2957 (0.3810)	
training:	Epoch: [5][82/233]	Loss 0.3572 (0.3807)	
training:	Epoch: [5][83/233]	Loss 0.3896 (0.3808)	
training:	Epoch: [5][84/233]	Loss 0.3737 (0.3807)	
training:	Epoch: [5][85/233]	Loss 0.2164 (0.3788)	
training:	Epoch: [5][86/233]	Loss 0.3508 (0.3784)	
training:	Epoch: [5][87/233]	Loss 0.3565 (0.3782)	
training:	Epoch: [5][88/233]	Loss 0.3082 (0.3774)	
training:	Epoch: [5][89/233]	Loss 0.5019 (0.3788)	
training:	Epoch: [5][90/233]	Loss 0.6129 (0.3814)	
training:	Epoch: [5][91/233]	Loss 0.2702 (0.3802)	
training:	Epoch: [5][92/233]	Loss 0.3859 (0.3802)	
training:	Epoch: [5][93/233]	Loss 0.4995 (0.3815)	
training:	Epoch: [5][94/233]	Loss 0.4928 (0.3827)	
training:	Epoch: [5][95/233]	Loss 0.3694 (0.3826)	
training:	Epoch: [5][96/233]	Loss 0.5002 (0.3838)	
training:	Epoch: [5][97/233]	Loss 0.4273 (0.3842)	
training:	Epoch: [5][98/233]	Loss 0.3041 (0.3834)	
training:	Epoch: [5][99/233]	Loss 0.3099 (0.3827)	
training:	Epoch: [5][100/233]	Loss 0.4523 (0.3834)	
training:	Epoch: [5][101/233]	Loss 0.2367 (0.3819)	
training:	Epoch: [5][102/233]	Loss 0.4654 (0.3827)	
training:	Epoch: [5][103/233]	Loss 0.2775 (0.3817)	
training:	Epoch: [5][104/233]	Loss 0.3819 (0.3817)	
training:	Epoch: [5][105/233]	Loss 0.4106 (0.3820)	
training:	Epoch: [5][106/233]	Loss 0.3479 (0.3817)	
training:	Epoch: [5][107/233]	Loss 0.3797 (0.3816)	
training:	Epoch: [5][108/233]	Loss 0.4334 (0.3821)	
training:	Epoch: [5][109/233]	Loss 0.3769 (0.3821)	
training:	Epoch: [5][110/233]	Loss 0.4148 (0.3824)	
training:	Epoch: [5][111/233]	Loss 0.3549 (0.3821)	
training:	Epoch: [5][112/233]	Loss 0.3896 (0.3822)	
training:	Epoch: [5][113/233]	Loss 0.3279 (0.3817)	
training:	Epoch: [5][114/233]	Loss 0.3710 (0.3816)	
training:	Epoch: [5][115/233]	Loss 0.4449 (0.3822)	
training:	Epoch: [5][116/233]	Loss 0.3664 (0.3820)	
training:	Epoch: [5][117/233]	Loss 0.3373 (0.3817)	
training:	Epoch: [5][118/233]	Loss 0.3272 (0.3812)	
training:	Epoch: [5][119/233]	Loss 0.5450 (0.3826)	
training:	Epoch: [5][120/233]	Loss 0.3420 (0.3822)	
training:	Epoch: [5][121/233]	Loss 0.4671 (0.3829)	
training:	Epoch: [5][122/233]	Loss 0.5174 (0.3840)	
training:	Epoch: [5][123/233]	Loss 0.4003 (0.3842)	
training:	Epoch: [5][124/233]	Loss 0.3969 (0.3843)	
training:	Epoch: [5][125/233]	Loss 0.3649 (0.3841)	
training:	Epoch: [5][126/233]	Loss 0.3676 (0.3840)	
training:	Epoch: [5][127/233]	Loss 0.4703 (0.3847)	
training:	Epoch: [5][128/233]	Loss 0.3767 (0.3846)	
training:	Epoch: [5][129/233]	Loss 0.3829 (0.3846)	
training:	Epoch: [5][130/233]	Loss 0.3518 (0.3843)	
training:	Epoch: [5][131/233]	Loss 0.4139 (0.3846)	
training:	Epoch: [5][132/233]	Loss 0.5596 (0.3859)	
training:	Epoch: [5][133/233]	Loss 0.2824 (0.3851)	
training:	Epoch: [5][134/233]	Loss 0.4546 (0.3856)	
training:	Epoch: [5][135/233]	Loss 0.3883 (0.3856)	
training:	Epoch: [5][136/233]	Loss 0.2432 (0.3846)	
training:	Epoch: [5][137/233]	Loss 0.3419 (0.3843)	
training:	Epoch: [5][138/233]	Loss 0.4869 (0.3850)	
training:	Epoch: [5][139/233]	Loss 0.4187 (0.3853)	
training:	Epoch: [5][140/233]	Loss 0.3672 (0.3851)	
training:	Epoch: [5][141/233]	Loss 0.4915 (0.3859)	
training:	Epoch: [5][142/233]	Loss 0.6510 (0.3878)	
training:	Epoch: [5][143/233]	Loss 0.5098 (0.3886)	
training:	Epoch: [5][144/233]	Loss 0.4158 (0.3888)	
training:	Epoch: [5][145/233]	Loss 0.4476 (0.3892)	
training:	Epoch: [5][146/233]	Loss 0.4445 (0.3896)	
training:	Epoch: [5][147/233]	Loss 0.3645 (0.3894)	
training:	Epoch: [5][148/233]	Loss 0.4301 (0.3897)	
training:	Epoch: [5][149/233]	Loss 0.1957 (0.3884)	
training:	Epoch: [5][150/233]	Loss 0.3600 (0.3882)	
training:	Epoch: [5][151/233]	Loss 0.3788 (0.3881)	
training:	Epoch: [5][152/233]	Loss 0.3169 (0.3877)	
training:	Epoch: [5][153/233]	Loss 0.4755 (0.3882)	
training:	Epoch: [5][154/233]	Loss 0.2894 (0.3876)	
training:	Epoch: [5][155/233]	Loss 0.3821 (0.3876)	
training:	Epoch: [5][156/233]	Loss 0.3608 (0.3874)	
training:	Epoch: [5][157/233]	Loss 0.4252 (0.3876)	
training:	Epoch: [5][158/233]	Loss 0.3665 (0.3875)	
training:	Epoch: [5][159/233]	Loss 0.4032 (0.3876)	
training:	Epoch: [5][160/233]	Loss 0.3666 (0.3875)	
training:	Epoch: [5][161/233]	Loss 0.3902 (0.3875)	
training:	Epoch: [5][162/233]	Loss 0.3418 (0.3872)	
training:	Epoch: [5][163/233]	Loss 0.5856 (0.3884)	
training:	Epoch: [5][164/233]	Loss 0.2354 (0.3875)	
training:	Epoch: [5][165/233]	Loss 0.5082 (0.3882)	
training:	Epoch: [5][166/233]	Loss 0.3261 (0.3878)	
training:	Epoch: [5][167/233]	Loss 0.3703 (0.3877)	
training:	Epoch: [5][168/233]	Loss 0.2733 (0.3871)	
training:	Epoch: [5][169/233]	Loss 0.3974 (0.3871)	
training:	Epoch: [5][170/233]	Loss 0.3168 (0.3867)	
training:	Epoch: [5][171/233]	Loss 0.2260 (0.3858)	
training:	Epoch: [5][172/233]	Loss 0.2726 (0.3851)	
training:	Epoch: [5][173/233]	Loss 0.3270 (0.3848)	
training:	Epoch: [5][174/233]	Loss 0.3464 (0.3846)	
training:	Epoch: [5][175/233]	Loss 0.3319 (0.3843)	
training:	Epoch: [5][176/233]	Loss 0.3413 (0.3840)	
training:	Epoch: [5][177/233]	Loss 0.2616 (0.3833)	
training:	Epoch: [5][178/233]	Loss 0.2279 (0.3824)	
training:	Epoch: [5][179/233]	Loss 0.2579 (0.3818)	
training:	Epoch: [5][180/233]	Loss 0.3719 (0.3817)	
training:	Epoch: [5][181/233]	Loss 0.2633 (0.3810)	
training:	Epoch: [5][182/233]	Loss 0.3387 (0.3808)	
training:	Epoch: [5][183/233]	Loss 0.4317 (0.3811)	
training:	Epoch: [5][184/233]	Loss 0.4759 (0.3816)	
training:	Epoch: [5][185/233]	Loss 0.4582 (0.3820)	
training:	Epoch: [5][186/233]	Loss 0.4417 (0.3823)	
training:	Epoch: [5][187/233]	Loss 0.2835 (0.3818)	
training:	Epoch: [5][188/233]	Loss 0.2688 (0.3812)	
training:	Epoch: [5][189/233]	Loss 0.2766 (0.3807)	
training:	Epoch: [5][190/233]	Loss 0.3001 (0.3802)	
training:	Epoch: [5][191/233]	Loss 0.5331 (0.3810)	
training:	Epoch: [5][192/233]	Loss 0.5088 (0.3817)	
training:	Epoch: [5][193/233]	Loss 0.4237 (0.3819)	
training:	Epoch: [5][194/233]	Loss 0.2089 (0.3810)	
training:	Epoch: [5][195/233]	Loss 0.3592 (0.3809)	
training:	Epoch: [5][196/233]	Loss 0.3338 (0.3807)	
training:	Epoch: [5][197/233]	Loss 0.4416 (0.3810)	
training:	Epoch: [5][198/233]	Loss 0.4249 (0.3812)	
training:	Epoch: [5][199/233]	Loss 0.4177 (0.3814)	
training:	Epoch: [5][200/233]	Loss 0.2898 (0.3809)	
training:	Epoch: [5][201/233]	Loss 0.4356 (0.3812)	
training:	Epoch: [5][202/233]	Loss 0.3778 (0.3812)	
training:	Epoch: [5][203/233]	Loss 0.3798 (0.3812)	
training:	Epoch: [5][204/233]	Loss 0.2673 (0.3806)	
training:	Epoch: [5][205/233]	Loss 0.3310 (0.3804)	
training:	Epoch: [5][206/233]	Loss 0.2737 (0.3799)	
training:	Epoch: [5][207/233]	Loss 0.4507 (0.3802)	
training:	Epoch: [5][208/233]	Loss 0.2690 (0.3797)	
training:	Epoch: [5][209/233]	Loss 0.4099 (0.3798)	
training:	Epoch: [5][210/233]	Loss 0.3474 (0.3797)	
training:	Epoch: [5][211/233]	Loss 0.3402 (0.3795)	
training:	Epoch: [5][212/233]	Loss 0.5569 (0.3803)	
training:	Epoch: [5][213/233]	Loss 0.4938 (0.3808)	
training:	Epoch: [5][214/233]	Loss 0.3079 (0.3805)	
training:	Epoch: [5][215/233]	Loss 0.2731 (0.3800)	
training:	Epoch: [5][216/233]	Loss 0.3122 (0.3797)	
training:	Epoch: [5][217/233]	Loss 0.4270 (0.3799)	
training:	Epoch: [5][218/233]	Loss 0.4913 (0.3804)	
training:	Epoch: [5][219/233]	Loss 0.3040 (0.3801)	
training:	Epoch: [5][220/233]	Loss 0.4683 (0.3805)	
training:	Epoch: [5][221/233]	Loss 0.4397 (0.3807)	
training:	Epoch: [5][222/233]	Loss 0.2878 (0.3803)	
training:	Epoch: [5][223/233]	Loss 0.4635 (0.3807)	
training:	Epoch: [5][224/233]	Loss 0.3797 (0.3807)	
training:	Epoch: [5][225/233]	Loss 0.3329 (0.3805)	
training:	Epoch: [5][226/233]	Loss 0.4415 (0.3807)	
training:	Epoch: [5][227/233]	Loss 0.5683 (0.3816)	
training:	Epoch: [5][228/233]	Loss 0.5622 (0.3824)	
training:	Epoch: [5][229/233]	Loss 0.2838 (0.3819)	
training:	Epoch: [5][230/233]	Loss 0.3683 (0.3819)	
training:	Epoch: [5][231/233]	Loss 0.4351 (0.3821)	
training:	Epoch: [5][232/233]	Loss 0.3992 (0.3822)	
training:	Epoch: [5][233/233]	Loss 0.4680 (0.3825)	
Training:	 Loss: 0.3817

Training:	 ACC: 0.8537 0.8516 0.8056 0.9018
Validation:	 ACC: 0.8027 0.8004 0.7559 0.8494
Validation:	 Best_BACC: 0.8027 0.8004 0.7559 0.8494
Validation:	 Loss: 0.4442
Pretraining:	Epoch 6/200
----------
training:	Epoch: [6][1/233]	Loss 0.3535 (0.3535)	
training:	Epoch: [6][2/233]	Loss 0.4914 (0.4225)	
training:	Epoch: [6][3/233]	Loss 0.3947 (0.4132)	
training:	Epoch: [6][4/233]	Loss 0.3783 (0.4045)	
training:	Epoch: [6][5/233]	Loss 0.2985 (0.3833)	
training:	Epoch: [6][6/233]	Loss 0.2334 (0.3583)	
training:	Epoch: [6][7/233]	Loss 0.4298 (0.3685)	
training:	Epoch: [6][8/233]	Loss 0.2314 (0.3514)	
training:	Epoch: [6][9/233]	Loss 0.1781 (0.3321)	
training:	Epoch: [6][10/233]	Loss 0.3805 (0.3370)	
training:	Epoch: [6][11/233]	Loss 0.4485 (0.3471)	
training:	Epoch: [6][12/233]	Loss 0.3053 (0.3436)	
training:	Epoch: [6][13/233]	Loss 0.2917 (0.3396)	
training:	Epoch: [6][14/233]	Loss 0.4249 (0.3457)	
training:	Epoch: [6][15/233]	Loss 0.4288 (0.3513)	
training:	Epoch: [6][16/233]	Loss 0.2851 (0.3471)	
training:	Epoch: [6][17/233]	Loss 0.2530 (0.3416)	
training:	Epoch: [6][18/233]	Loss 0.3339 (0.3412)	
training:	Epoch: [6][19/233]	Loss 0.3396 (0.3411)	
training:	Epoch: [6][20/233]	Loss 0.4077 (0.3444)	
training:	Epoch: [6][21/233]	Loss 0.3113 (0.3428)	
training:	Epoch: [6][22/233]	Loss 0.3109 (0.3414)	
training:	Epoch: [6][23/233]	Loss 0.2585 (0.3378)	
training:	Epoch: [6][24/233]	Loss 0.3615 (0.3388)	
training:	Epoch: [6][25/233]	Loss 0.2829 (0.3365)	
training:	Epoch: [6][26/233]	Loss 0.2777 (0.3343)	
training:	Epoch: [6][27/233]	Loss 0.4220 (0.3375)	
training:	Epoch: [6][28/233]	Loss 0.5382 (0.3447)	
training:	Epoch: [6][29/233]	Loss 0.3167 (0.3437)	
training:	Epoch: [6][30/233]	Loss 0.3936 (0.3454)	
training:	Epoch: [6][31/233]	Loss 0.4615 (0.3491)	
training:	Epoch: [6][32/233]	Loss 0.3966 (0.3506)	
training:	Epoch: [6][33/233]	Loss 0.4857 (0.3547)	
training:	Epoch: [6][34/233]	Loss 0.4268 (0.3568)	
training:	Epoch: [6][35/233]	Loss 0.5218 (0.3615)	
training:	Epoch: [6][36/233]	Loss 0.5397 (0.3665)	
training:	Epoch: [6][37/233]	Loss 0.2848 (0.3643)	
training:	Epoch: [6][38/233]	Loss 0.4164 (0.3657)	
training:	Epoch: [6][39/233]	Loss 0.4284 (0.3673)	
training:	Epoch: [6][40/233]	Loss 0.2695 (0.3648)	
training:	Epoch: [6][41/233]	Loss 0.4018 (0.3657)	
training:	Epoch: [6][42/233]	Loss 0.4469 (0.3677)	
training:	Epoch: [6][43/233]	Loss 0.2400 (0.3647)	
training:	Epoch: [6][44/233]	Loss 0.3544 (0.3645)	
training:	Epoch: [6][45/233]	Loss 0.5123 (0.3677)	
training:	Epoch: [6][46/233]	Loss 0.4946 (0.3705)	
training:	Epoch: [6][47/233]	Loss 0.2969 (0.3689)	
training:	Epoch: [6][48/233]	Loss 0.3208 (0.3679)	
training:	Epoch: [6][49/233]	Loss 0.4857 (0.3703)	
training:	Epoch: [6][50/233]	Loss 0.4674 (0.3723)	
training:	Epoch: [6][51/233]	Loss 0.4573 (0.3739)	
training:	Epoch: [6][52/233]	Loss 0.5098 (0.3766)	
training:	Epoch: [6][53/233]	Loss 0.3457 (0.3760)	
training:	Epoch: [6][54/233]	Loss 0.3165 (0.3749)	
training:	Epoch: [6][55/233]	Loss 0.2650 (0.3729)	
training:	Epoch: [6][56/233]	Loss 0.2983 (0.3715)	
training:	Epoch: [6][57/233]	Loss 0.2331 (0.3691)	
training:	Epoch: [6][58/233]	Loss 0.3028 (0.3680)	
training:	Epoch: [6][59/233]	Loss 0.5874 (0.3717)	
training:	Epoch: [6][60/233]	Loss 0.2693 (0.3700)	
training:	Epoch: [6][61/233]	Loss 0.4102 (0.3706)	
training:	Epoch: [6][62/233]	Loss 0.2907 (0.3694)	
training:	Epoch: [6][63/233]	Loss 0.3646 (0.3693)	
training:	Epoch: [6][64/233]	Loss 0.4083 (0.3699)	
training:	Epoch: [6][65/233]	Loss 0.4234 (0.3707)	
training:	Epoch: [6][66/233]	Loss 0.3439 (0.3703)	
training:	Epoch: [6][67/233]	Loss 0.3679 (0.3703)	
training:	Epoch: [6][68/233]	Loss 0.4777 (0.3718)	
training:	Epoch: [6][69/233]	Loss 0.3786 (0.3719)	
training:	Epoch: [6][70/233]	Loss 0.3898 (0.3722)	
training:	Epoch: [6][71/233]	Loss 0.3107 (0.3713)	
training:	Epoch: [6][72/233]	Loss 0.3196 (0.3706)	
training:	Epoch: [6][73/233]	Loss 0.5850 (0.3736)	
training:	Epoch: [6][74/233]	Loss 0.4040 (0.3740)	
training:	Epoch: [6][75/233]	Loss 0.3470 (0.3736)	
training:	Epoch: [6][76/233]	Loss 0.3103 (0.3728)	
training:	Epoch: [6][77/233]	Loss 0.2676 (0.3714)	
training:	Epoch: [6][78/233]	Loss 0.4022 (0.3718)	
training:	Epoch: [6][79/233]	Loss 0.4164 (0.3724)	
training:	Epoch: [6][80/233]	Loss 0.2393 (0.3707)	
training:	Epoch: [6][81/233]	Loss 0.3292 (0.3702)	
training:	Epoch: [6][82/233]	Loss 0.4052 (0.3706)	
training:	Epoch: [6][83/233]	Loss 0.2588 (0.3693)	
training:	Epoch: [6][84/233]	Loss 0.2803 (0.3682)	
training:	Epoch: [6][85/233]	Loss 0.2428 (0.3667)	
training:	Epoch: [6][86/233]	Loss 0.3897 (0.3670)	
training:	Epoch: [6][87/233]	Loss 0.2451 (0.3656)	
training:	Epoch: [6][88/233]	Loss 0.3781 (0.3657)	
training:	Epoch: [6][89/233]	Loss 0.5373 (0.3677)	
training:	Epoch: [6][90/233]	Loss 0.2377 (0.3662)	
training:	Epoch: [6][91/233]	Loss 0.3037 (0.3655)	
training:	Epoch: [6][92/233]	Loss 0.5641 (0.3677)	
training:	Epoch: [6][93/233]	Loss 0.5102 (0.3692)	
training:	Epoch: [6][94/233]	Loss 0.4404 (0.3700)	
training:	Epoch: [6][95/233]	Loss 0.3963 (0.3703)	
training:	Epoch: [6][96/233]	Loss 0.2936 (0.3695)	
training:	Epoch: [6][97/233]	Loss 0.1614 (0.3673)	
training:	Epoch: [6][98/233]	Loss 0.3032 (0.3667)	
training:	Epoch: [6][99/233]	Loss 0.3676 (0.3667)	
training:	Epoch: [6][100/233]	Loss 0.3286 (0.3663)	
training:	Epoch: [6][101/233]	Loss 0.2590 (0.3652)	
training:	Epoch: [6][102/233]	Loss 0.4680 (0.3662)	
training:	Epoch: [6][103/233]	Loss 0.3214 (0.3658)	
training:	Epoch: [6][104/233]	Loss 0.2025 (0.3642)	
training:	Epoch: [6][105/233]	Loss 0.5523 (0.3660)	
training:	Epoch: [6][106/233]	Loss 0.2663 (0.3651)	
training:	Epoch: [6][107/233]	Loss 0.3334 (0.3648)	
training:	Epoch: [6][108/233]	Loss 0.3131 (0.3643)	
training:	Epoch: [6][109/233]	Loss 0.3924 (0.3646)	
training:	Epoch: [6][110/233]	Loss 0.3261 (0.3642)	
training:	Epoch: [6][111/233]	Loss 0.5210 (0.3656)	
training:	Epoch: [6][112/233]	Loss 0.3020 (0.3651)	
training:	Epoch: [6][113/233]	Loss 0.3631 (0.3650)	
training:	Epoch: [6][114/233]	Loss 0.3443 (0.3649)	
training:	Epoch: [6][115/233]	Loss 0.2844 (0.3642)	
training:	Epoch: [6][116/233]	Loss 0.2005 (0.3628)	
training:	Epoch: [6][117/233]	Loss 0.4200 (0.3632)	
training:	Epoch: [6][118/233]	Loss 0.2480 (0.3623)	
training:	Epoch: [6][119/233]	Loss 0.3142 (0.3619)	
training:	Epoch: [6][120/233]	Loss 0.3592 (0.3618)	
training:	Epoch: [6][121/233]	Loss 0.3991 (0.3621)	
training:	Epoch: [6][122/233]	Loss 0.4232 (0.3626)	
training:	Epoch: [6][123/233]	Loss 0.2656 (0.3619)	
training:	Epoch: [6][124/233]	Loss 0.2624 (0.3611)	
training:	Epoch: [6][125/233]	Loss 0.3738 (0.3612)	
training:	Epoch: [6][126/233]	Loss 0.2300 (0.3601)	
training:	Epoch: [6][127/233]	Loss 0.1894 (0.3588)	
training:	Epoch: [6][128/233]	Loss 0.3818 (0.3590)	
training:	Epoch: [6][129/233]	Loss 0.3694 (0.3590)	
training:	Epoch: [6][130/233]	Loss 0.2136 (0.3579)	
training:	Epoch: [6][131/233]	Loss 0.1486 (0.3563)	
training:	Epoch: [6][132/233]	Loss 0.3931 (0.3566)	
training:	Epoch: [6][133/233]	Loss 0.4951 (0.3576)	
training:	Epoch: [6][134/233]	Loss 0.4012 (0.3580)	
training:	Epoch: [6][135/233]	Loss 0.2988 (0.3575)	
training:	Epoch: [6][136/233]	Loss 0.5141 (0.3587)	
training:	Epoch: [6][137/233]	Loss 0.2890 (0.3582)	
training:	Epoch: [6][138/233]	Loss 0.3317 (0.3580)	
training:	Epoch: [6][139/233]	Loss 0.3702 (0.3581)	
training:	Epoch: [6][140/233]	Loss 0.3339 (0.3579)	
training:	Epoch: [6][141/233]	Loss 0.4299 (0.3584)	
training:	Epoch: [6][142/233]	Loss 0.4646 (0.3591)	
training:	Epoch: [6][143/233]	Loss 0.4247 (0.3596)	
training:	Epoch: [6][144/233]	Loss 0.4880 (0.3605)	
training:	Epoch: [6][145/233]	Loss 0.4993 (0.3615)	
training:	Epoch: [6][146/233]	Loss 0.3182 (0.3612)	
training:	Epoch: [6][147/233]	Loss 0.3921 (0.3614)	
training:	Epoch: [6][148/233]	Loss 0.4013 (0.3616)	
training:	Epoch: [6][149/233]	Loss 0.2321 (0.3608)	
training:	Epoch: [6][150/233]	Loss 0.2745 (0.3602)	
training:	Epoch: [6][151/233]	Loss 0.2045 (0.3592)	
training:	Epoch: [6][152/233]	Loss 0.3863 (0.3593)	
training:	Epoch: [6][153/233]	Loss 0.3909 (0.3595)	
training:	Epoch: [6][154/233]	Loss 0.6063 (0.3612)	
training:	Epoch: [6][155/233]	Loss 0.2356 (0.3603)	
training:	Epoch: [6][156/233]	Loss 0.3015 (0.3600)	
training:	Epoch: [6][157/233]	Loss 0.4295 (0.3604)	
training:	Epoch: [6][158/233]	Loss 0.2208 (0.3595)	
training:	Epoch: [6][159/233]	Loss 0.2915 (0.3591)	
training:	Epoch: [6][160/233]	Loss 0.4159 (0.3595)	
training:	Epoch: [6][161/233]	Loss 0.4564 (0.3601)	
training:	Epoch: [6][162/233]	Loss 0.3558 (0.3600)	
training:	Epoch: [6][163/233]	Loss 0.4507 (0.3606)	
training:	Epoch: [6][164/233]	Loss 0.2458 (0.3599)	
training:	Epoch: [6][165/233]	Loss 0.1952 (0.3589)	
training:	Epoch: [6][166/233]	Loss 0.4219 (0.3593)	
training:	Epoch: [6][167/233]	Loss 0.3911 (0.3595)	
training:	Epoch: [6][168/233]	Loss 0.2667 (0.3589)	
training:	Epoch: [6][169/233]	Loss 0.3475 (0.3588)	
training:	Epoch: [6][170/233]	Loss 0.2955 (0.3585)	
training:	Epoch: [6][171/233]	Loss 0.2847 (0.3580)	
training:	Epoch: [6][172/233]	Loss 0.4217 (0.3584)	
training:	Epoch: [6][173/233]	Loss 0.2587 (0.3578)	
training:	Epoch: [6][174/233]	Loss 0.5242 (0.3588)	
training:	Epoch: [6][175/233]	Loss 0.3923 (0.3590)	
training:	Epoch: [6][176/233]	Loss 0.3606 (0.3590)	
training:	Epoch: [6][177/233]	Loss 0.3793 (0.3591)	
training:	Epoch: [6][178/233]	Loss 0.3846 (0.3592)	
training:	Epoch: [6][179/233]	Loss 0.3959 (0.3594)	
training:	Epoch: [6][180/233]	Loss 0.4010 (0.3597)	
training:	Epoch: [6][181/233]	Loss 0.2728 (0.3592)	
training:	Epoch: [6][182/233]	Loss 0.2739 (0.3587)	
training:	Epoch: [6][183/233]	Loss 0.4658 (0.3593)	
training:	Epoch: [6][184/233]	Loss 0.4320 (0.3597)	
training:	Epoch: [6][185/233]	Loss 0.3421 (0.3596)	
training:	Epoch: [6][186/233]	Loss 0.5022 (0.3604)	
training:	Epoch: [6][187/233]	Loss 0.5001 (0.3611)	
training:	Epoch: [6][188/233]	Loss 0.2037 (0.3603)	
training:	Epoch: [6][189/233]	Loss 0.2933 (0.3599)	
training:	Epoch: [6][190/233]	Loss 0.6225 (0.3613)	
training:	Epoch: [6][191/233]	Loss 0.2990 (0.3610)	
training:	Epoch: [6][192/233]	Loss 0.3559 (0.3610)	
training:	Epoch: [6][193/233]	Loss 0.4692 (0.3615)	
training:	Epoch: [6][194/233]	Loss 0.2842 (0.3611)	
training:	Epoch: [6][195/233]	Loss 0.4104 (0.3614)	
training:	Epoch: [6][196/233]	Loss 0.3080 (0.3611)	
training:	Epoch: [6][197/233]	Loss 0.3287 (0.3609)	
training:	Epoch: [6][198/233]	Loss 0.3361 (0.3608)	
training:	Epoch: [6][199/233]	Loss 0.3895 (0.3610)	
training:	Epoch: [6][200/233]	Loss 0.3150 (0.3607)	
training:	Epoch: [6][201/233]	Loss 0.2171 (0.3600)	
training:	Epoch: [6][202/233]	Loss 0.3756 (0.3601)	
training:	Epoch: [6][203/233]	Loss 0.6266 (0.3614)	
training:	Epoch: [6][204/233]	Loss 0.5130 (0.3622)	
training:	Epoch: [6][205/233]	Loss 0.3650 (0.3622)	
training:	Epoch: [6][206/233]	Loss 0.5721 (0.3632)	
training:	Epoch: [6][207/233]	Loss 0.3650 (0.3632)	
training:	Epoch: [6][208/233]	Loss 0.4252 (0.3635)	
training:	Epoch: [6][209/233]	Loss 0.3319 (0.3633)	
training:	Epoch: [6][210/233]	Loss 0.3296 (0.3632)	
training:	Epoch: [6][211/233]	Loss 0.3511 (0.3631)	
training:	Epoch: [6][212/233]	Loss 0.2921 (0.3628)	
training:	Epoch: [6][213/233]	Loss 0.4745 (0.3633)	
training:	Epoch: [6][214/233]	Loss 0.3505 (0.3633)	
training:	Epoch: [6][215/233]	Loss 0.3539 (0.3632)	
training:	Epoch: [6][216/233]	Loss 0.4920 (0.3638)	
training:	Epoch: [6][217/233]	Loss 0.3871 (0.3639)	
training:	Epoch: [6][218/233]	Loss 0.5303 (0.3647)	
training:	Epoch: [6][219/233]	Loss 0.2734 (0.3643)	
training:	Epoch: [6][220/233]	Loss 0.3541 (0.3642)	
training:	Epoch: [6][221/233]	Loss 0.4175 (0.3645)	
training:	Epoch: [6][222/233]	Loss 0.4190 (0.3647)	
training:	Epoch: [6][223/233]	Loss 0.2138 (0.3640)	
training:	Epoch: [6][224/233]	Loss 0.3048 (0.3638)	
training:	Epoch: [6][225/233]	Loss 0.4843 (0.3643)	
training:	Epoch: [6][226/233]	Loss 0.4265 (0.3646)	
training:	Epoch: [6][227/233]	Loss 0.5349 (0.3653)	
training:	Epoch: [6][228/233]	Loss 0.4231 (0.3656)	
training:	Epoch: [6][229/233]	Loss 0.3247 (0.3654)	
training:	Epoch: [6][230/233]	Loss 0.3677 (0.3654)	
training:	Epoch: [6][231/233]	Loss 0.3733 (0.3654)	
training:	Epoch: [6][232/233]	Loss 0.2936 (0.3651)	
training:	Epoch: [6][233/233]	Loss 0.2812 (0.3648)	
Training:	 Loss: 0.3639

Training:	 ACC: 0.8617 0.8628 0.8869 0.8366
Validation:	 ACC: 0.8043 0.8058 0.8345 0.7742
Validation:	 Best_BACC: 0.8043 0.8058 0.8345 0.7742
Validation:	 Loss: 0.4285
Pretraining:	Epoch 7/200
----------
training:	Epoch: [7][1/233]	Loss 0.4826 (0.4826)	
training:	Epoch: [7][2/233]	Loss 0.2476 (0.3651)	
training:	Epoch: [7][3/233]	Loss 0.2496 (0.3266)	
training:	Epoch: [7][4/233]	Loss 0.3215 (0.3253)	
training:	Epoch: [7][5/233]	Loss 0.2725 (0.3148)	
training:	Epoch: [7][6/233]	Loss 0.3061 (0.3133)	
training:	Epoch: [7][7/233]	Loss 0.2947 (0.3107)	
training:	Epoch: [7][8/233]	Loss 0.5069 (0.3352)	
training:	Epoch: [7][9/233]	Loss 0.3571 (0.3376)	
training:	Epoch: [7][10/233]	Loss 0.3147 (0.3353)	
training:	Epoch: [7][11/233]	Loss 0.2550 (0.3280)	
training:	Epoch: [7][12/233]	Loss 0.2025 (0.3176)	
training:	Epoch: [7][13/233]	Loss 0.2960 (0.3159)	
training:	Epoch: [7][14/233]	Loss 0.3600 (0.3191)	
training:	Epoch: [7][15/233]	Loss 0.5381 (0.3337)	
training:	Epoch: [7][16/233]	Loss 0.3813 (0.3366)	
training:	Epoch: [7][17/233]	Loss 0.2225 (0.3299)	
training:	Epoch: [7][18/233]	Loss 0.1651 (0.3208)	
training:	Epoch: [7][19/233]	Loss 0.4279 (0.3264)	
training:	Epoch: [7][20/233]	Loss 0.4490 (0.3325)	
training:	Epoch: [7][21/233]	Loss 0.3820 (0.3349)	
training:	Epoch: [7][22/233]	Loss 0.2238 (0.3299)	
training:	Epoch: [7][23/233]	Loss 0.1949 (0.3240)	
training:	Epoch: [7][24/233]	Loss 0.5033 (0.3315)	
training:	Epoch: [7][25/233]	Loss 0.5015 (0.3383)	
training:	Epoch: [7][26/233]	Loss 0.3770 (0.3398)	
training:	Epoch: [7][27/233]	Loss 0.3095 (0.3386)	
training:	Epoch: [7][28/233]	Loss 0.4579 (0.3429)	
training:	Epoch: [7][29/233]	Loss 0.3763 (0.3440)	
training:	Epoch: [7][30/233]	Loss 0.3180 (0.3432)	
training:	Epoch: [7][31/233]	Loss 0.2769 (0.3410)	
training:	Epoch: [7][32/233]	Loss 0.1830 (0.3361)	
training:	Epoch: [7][33/233]	Loss 0.3586 (0.3368)	
training:	Epoch: [7][34/233]	Loss 0.3345 (0.3367)	
training:	Epoch: [7][35/233]	Loss 0.3534 (0.3372)	
training:	Epoch: [7][36/233]	Loss 0.3555 (0.3377)	
training:	Epoch: [7][37/233]	Loss 0.3591 (0.3383)	
training:	Epoch: [7][38/233]	Loss 0.3573 (0.3388)	
training:	Epoch: [7][39/233]	Loss 0.3349 (0.3387)	
training:	Epoch: [7][40/233]	Loss 0.4150 (0.3406)	
training:	Epoch: [7][41/233]	Loss 0.4386 (0.3430)	
training:	Epoch: [7][42/233]	Loss 0.3023 (0.3420)	
training:	Epoch: [7][43/233]	Loss 0.2746 (0.3404)	
training:	Epoch: [7][44/233]	Loss 0.5520 (0.3452)	
training:	Epoch: [7][45/233]	Loss 0.3528 (0.3454)	
training:	Epoch: [7][46/233]	Loss 0.5937 (0.3508)	
training:	Epoch: [7][47/233]	Loss 0.2940 (0.3496)	
training:	Epoch: [7][48/233]	Loss 0.3117 (0.3488)	
training:	Epoch: [7][49/233]	Loss 0.3699 (0.3492)	
training:	Epoch: [7][50/233]	Loss 0.3252 (0.3488)	
training:	Epoch: [7][51/233]	Loss 0.2477 (0.3468)	
training:	Epoch: [7][52/233]	Loss 0.5055 (0.3498)	
training:	Epoch: [7][53/233]	Loss 0.3669 (0.3502)	
training:	Epoch: [7][54/233]	Loss 0.2549 (0.3484)	
training:	Epoch: [7][55/233]	Loss 0.2878 (0.3473)	
training:	Epoch: [7][56/233]	Loss 0.2337 (0.3453)	
training:	Epoch: [7][57/233]	Loss 0.3749 (0.3458)	
training:	Epoch: [7][58/233]	Loss 0.3054 (0.3451)	
training:	Epoch: [7][59/233]	Loss 0.2829 (0.3440)	
training:	Epoch: [7][60/233]	Loss 0.4068 (0.3451)	
training:	Epoch: [7][61/233]	Loss 0.3174 (0.3446)	
training:	Epoch: [7][62/233]	Loss 0.3657 (0.3450)	
training:	Epoch: [7][63/233]	Loss 0.4546 (0.3467)	
training:	Epoch: [7][64/233]	Loss 0.2756 (0.3456)	
training:	Epoch: [7][65/233]	Loss 0.1811 (0.3431)	
training:	Epoch: [7][66/233]	Loss 0.2340 (0.3414)	
training:	Epoch: [7][67/233]	Loss 0.3144 (0.3410)	
training:	Epoch: [7][68/233]	Loss 0.3513 (0.3412)	
training:	Epoch: [7][69/233]	Loss 0.3027 (0.3406)	
training:	Epoch: [7][70/233]	Loss 0.4374 (0.3420)	
training:	Epoch: [7][71/233]	Loss 0.2805 (0.3411)	
training:	Epoch: [7][72/233]	Loss 0.3259 (0.3409)	
training:	Epoch: [7][73/233]	Loss 0.2984 (0.3403)	
training:	Epoch: [7][74/233]	Loss 0.5667 (0.3434)	
training:	Epoch: [7][75/233]	Loss 0.4374 (0.3446)	
training:	Epoch: [7][76/233]	Loss 0.3418 (0.3446)	
training:	Epoch: [7][77/233]	Loss 0.2780 (0.3437)	
training:	Epoch: [7][78/233]	Loss 0.2311 (0.3423)	
training:	Epoch: [7][79/233]	Loss 0.3901 (0.3429)	
training:	Epoch: [7][80/233]	Loss 0.3830 (0.3434)	
training:	Epoch: [7][81/233]	Loss 0.2413 (0.3421)	
training:	Epoch: [7][82/233]	Loss 0.4468 (0.3434)	
training:	Epoch: [7][83/233]	Loss 0.3071 (0.3430)	
training:	Epoch: [7][84/233]	Loss 0.2068 (0.3414)	
training:	Epoch: [7][85/233]	Loss 0.4676 (0.3428)	
training:	Epoch: [7][86/233]	Loss 0.4941 (0.3446)	
training:	Epoch: [7][87/233]	Loss 0.5782 (0.3473)	
training:	Epoch: [7][88/233]	Loss 0.4261 (0.3482)	
training:	Epoch: [7][89/233]	Loss 0.3681 (0.3484)	
training:	Epoch: [7][90/233]	Loss 0.4302 (0.3493)	
training:	Epoch: [7][91/233]	Loss 0.3265 (0.3491)	
training:	Epoch: [7][92/233]	Loss 0.2162 (0.3476)	
training:	Epoch: [7][93/233]	Loss 0.4352 (0.3486)	
training:	Epoch: [7][94/233]	Loss 0.2405 (0.3474)	
training:	Epoch: [7][95/233]	Loss 0.4458 (0.3484)	
training:	Epoch: [7][96/233]	Loss 0.4313 (0.3493)	
training:	Epoch: [7][97/233]	Loss 0.3795 (0.3496)	
training:	Epoch: [7][98/233]	Loss 0.1671 (0.3478)	
training:	Epoch: [7][99/233]	Loss 0.3952 (0.3482)	
training:	Epoch: [7][100/233]	Loss 0.4700 (0.3495)	
training:	Epoch: [7][101/233]	Loss 0.3927 (0.3499)	
training:	Epoch: [7][102/233]	Loss 0.4749 (0.3511)	
training:	Epoch: [7][103/233]	Loss 0.4255 (0.3518)	
training:	Epoch: [7][104/233]	Loss 0.2630 (0.3510)	
training:	Epoch: [7][105/233]	Loss 0.3110 (0.3506)	
training:	Epoch: [7][106/233]	Loss 0.3943 (0.3510)	
training:	Epoch: [7][107/233]	Loss 0.3314 (0.3508)	
training:	Epoch: [7][108/233]	Loss 0.3420 (0.3507)	
training:	Epoch: [7][109/233]	Loss 0.2852 (0.3501)	
training:	Epoch: [7][110/233]	Loss 0.4165 (0.3507)	
training:	Epoch: [7][111/233]	Loss 0.3876 (0.3511)	
training:	Epoch: [7][112/233]	Loss 0.4268 (0.3518)	
training:	Epoch: [7][113/233]	Loss 0.3571 (0.3518)	
training:	Epoch: [7][114/233]	Loss 0.3482 (0.3518)	
training:	Epoch: [7][115/233]	Loss 0.4188 (0.3524)	
training:	Epoch: [7][116/233]	Loss 0.4423 (0.3531)	
training:	Epoch: [7][117/233]	Loss 0.4299 (0.3538)	
training:	Epoch: [7][118/233]	Loss 0.2849 (0.3532)	
training:	Epoch: [7][119/233]	Loss 0.4203 (0.3538)	
training:	Epoch: [7][120/233]	Loss 0.2965 (0.3533)	
training:	Epoch: [7][121/233]	Loss 0.2738 (0.3526)	
training:	Epoch: [7][122/233]	Loss 0.2349 (0.3517)	
training:	Epoch: [7][123/233]	Loss 0.3410 (0.3516)	
training:	Epoch: [7][124/233]	Loss 0.3168 (0.3513)	
training:	Epoch: [7][125/233]	Loss 0.3562 (0.3513)	
training:	Epoch: [7][126/233]	Loss 0.3521 (0.3513)	
training:	Epoch: [7][127/233]	Loss 0.3429 (0.3513)	
training:	Epoch: [7][128/233]	Loss 0.2797 (0.3507)	
training:	Epoch: [7][129/233]	Loss 0.4681 (0.3516)	
training:	Epoch: [7][130/233]	Loss 0.4725 (0.3526)	
training:	Epoch: [7][131/233]	Loss 0.2976 (0.3521)	
training:	Epoch: [7][132/233]	Loss 0.4131 (0.3526)	
training:	Epoch: [7][133/233]	Loss 0.4573 (0.3534)	
training:	Epoch: [7][134/233]	Loss 0.3292 (0.3532)	
training:	Epoch: [7][135/233]	Loss 0.3663 (0.3533)	
training:	Epoch: [7][136/233]	Loss 0.2329 (0.3524)	
training:	Epoch: [7][137/233]	Loss 0.2844 (0.3519)	
training:	Epoch: [7][138/233]	Loss 0.4991 (0.3530)	
training:	Epoch: [7][139/233]	Loss 0.3143 (0.3527)	
training:	Epoch: [7][140/233]	Loss 0.3770 (0.3529)	
training:	Epoch: [7][141/233]	Loss 0.2115 (0.3519)	
training:	Epoch: [7][142/233]	Loss 0.3015 (0.3515)	
training:	Epoch: [7][143/233]	Loss 0.4709 (0.3524)	
training:	Epoch: [7][144/233]	Loss 0.2482 (0.3516)	
training:	Epoch: [7][145/233]	Loss 0.2701 (0.3511)	
training:	Epoch: [7][146/233]	Loss 0.4858 (0.3520)	
training:	Epoch: [7][147/233]	Loss 0.3452 (0.3520)	
training:	Epoch: [7][148/233]	Loss 0.2729 (0.3514)	
training:	Epoch: [7][149/233]	Loss 0.6565 (0.3535)	
training:	Epoch: [7][150/233]	Loss 0.4578 (0.3542)	
training:	Epoch: [7][151/233]	Loss 0.2690 (0.3536)	
training:	Epoch: [7][152/233]	Loss 0.2227 (0.3527)	
training:	Epoch: [7][153/233]	Loss 0.7213 (0.3551)	
training:	Epoch: [7][154/233]	Loss 0.3401 (0.3550)	
training:	Epoch: [7][155/233]	Loss 0.4741 (0.3558)	
training:	Epoch: [7][156/233]	Loss 0.4246 (0.3563)	
training:	Epoch: [7][157/233]	Loss 0.2945 (0.3559)	
training:	Epoch: [7][158/233]	Loss 0.3007 (0.3555)	
training:	Epoch: [7][159/233]	Loss 0.2534 (0.3549)	
training:	Epoch: [7][160/233]	Loss 0.3658 (0.3549)	
training:	Epoch: [7][161/233]	Loss 0.3758 (0.3551)	
training:	Epoch: [7][162/233]	Loss 0.3767 (0.3552)	
training:	Epoch: [7][163/233]	Loss 0.3577 (0.3552)	
training:	Epoch: [7][164/233]	Loss 0.3498 (0.3552)	
training:	Epoch: [7][165/233]	Loss 0.2543 (0.3546)	
training:	Epoch: [7][166/233]	Loss 0.1906 (0.3536)	
training:	Epoch: [7][167/233]	Loss 0.4155 (0.3540)	
training:	Epoch: [7][168/233]	Loss 0.3884 (0.3542)	
training:	Epoch: [7][169/233]	Loss 0.3176 (0.3539)	
training:	Epoch: [7][170/233]	Loss 0.2388 (0.3533)	
training:	Epoch: [7][171/233]	Loss 0.3869 (0.3535)	
training:	Epoch: [7][172/233]	Loss 0.1986 (0.3526)	
training:	Epoch: [7][173/233]	Loss 0.3238 (0.3524)	
training:	Epoch: [7][174/233]	Loss 0.3816 (0.3526)	
training:	Epoch: [7][175/233]	Loss 0.2395 (0.3519)	
training:	Epoch: [7][176/233]	Loss 0.3329 (0.3518)	
training:	Epoch: [7][177/233]	Loss 0.4944 (0.3526)	
training:	Epoch: [7][178/233]	Loss 0.3005 (0.3523)	
training:	Epoch: [7][179/233]	Loss 0.2896 (0.3520)	
training:	Epoch: [7][180/233]	Loss 0.3219 (0.3518)	
training:	Epoch: [7][181/233]	Loss 0.4332 (0.3523)	
training:	Epoch: [7][182/233]	Loss 0.3556 (0.3523)	
training:	Epoch: [7][183/233]	Loss 0.3228 (0.3521)	
training:	Epoch: [7][184/233]	Loss 0.3997 (0.3524)	
training:	Epoch: [7][185/233]	Loss 0.2975 (0.3521)	
training:	Epoch: [7][186/233]	Loss 0.1811 (0.3512)	
training:	Epoch: [7][187/233]	Loss 0.3385 (0.3511)	
training:	Epoch: [7][188/233]	Loss 0.5421 (0.3521)	
training:	Epoch: [7][189/233]	Loss 0.6835 (0.3539)	
training:	Epoch: [7][190/233]	Loss 0.2573 (0.3533)	
training:	Epoch: [7][191/233]	Loss 0.2940 (0.3530)	
training:	Epoch: [7][192/233]	Loss 0.5332 (0.3540)	
training:	Epoch: [7][193/233]	Loss 0.2966 (0.3537)	
training:	Epoch: [7][194/233]	Loss 0.4354 (0.3541)	
training:	Epoch: [7][195/233]	Loss 0.2470 (0.3536)	
training:	Epoch: [7][196/233]	Loss 0.2343 (0.3529)	
training:	Epoch: [7][197/233]	Loss 0.3773 (0.3531)	
training:	Epoch: [7][198/233]	Loss 0.3275 (0.3529)	
training:	Epoch: [7][199/233]	Loss 0.3748 (0.3530)	
training:	Epoch: [7][200/233]	Loss 0.3664 (0.3531)	
training:	Epoch: [7][201/233]	Loss 0.2863 (0.3528)	
training:	Epoch: [7][202/233]	Loss 0.1965 (0.3520)	
training:	Epoch: [7][203/233]	Loss 0.2174 (0.3513)	
training:	Epoch: [7][204/233]	Loss 0.3189 (0.3512)	
training:	Epoch: [7][205/233]	Loss 0.5533 (0.3522)	
training:	Epoch: [7][206/233]	Loss 0.3394 (0.3521)	
training:	Epoch: [7][207/233]	Loss 0.1680 (0.3512)	
training:	Epoch: [7][208/233]	Loss 0.3221 (0.3511)	
training:	Epoch: [7][209/233]	Loss 0.2055 (0.3504)	
training:	Epoch: [7][210/233]	Loss 0.2570 (0.3499)	
training:	Epoch: [7][211/233]	Loss 0.4319 (0.3503)	
training:	Epoch: [7][212/233]	Loss 0.3322 (0.3502)	
training:	Epoch: [7][213/233]	Loss 0.4041 (0.3505)	
training:	Epoch: [7][214/233]	Loss 0.2575 (0.3501)	
training:	Epoch: [7][215/233]	Loss 0.4585 (0.3506)	
training:	Epoch: [7][216/233]	Loss 0.3454 (0.3505)	
training:	Epoch: [7][217/233]	Loss 0.4485 (0.3510)	
training:	Epoch: [7][218/233]	Loss 0.3288 (0.3509)	
training:	Epoch: [7][219/233]	Loss 0.2982 (0.3507)	
training:	Epoch: [7][220/233]	Loss 0.5139 (0.3514)	
training:	Epoch: [7][221/233]	Loss 0.2785 (0.3511)	
training:	Epoch: [7][222/233]	Loss 0.3066 (0.3509)	
training:	Epoch: [7][223/233]	Loss 0.1774 (0.3501)	
training:	Epoch: [7][224/233]	Loss 0.4067 (0.3503)	
training:	Epoch: [7][225/233]	Loss 0.3894 (0.3505)	
training:	Epoch: [7][226/233]	Loss 0.2523 (0.3501)	
training:	Epoch: [7][227/233]	Loss 0.4034 (0.3503)	
training:	Epoch: [7][228/233]	Loss 0.4400 (0.3507)	
training:	Epoch: [7][229/233]	Loss 0.1744 (0.3499)	
training:	Epoch: [7][230/233]	Loss 0.1935 (0.3493)	
training:	Epoch: [7][231/233]	Loss 0.3907 (0.3494)	
training:	Epoch: [7][232/233]	Loss 0.2951 (0.3492)	
training:	Epoch: [7][233/233]	Loss 0.1716 (0.3484)	
Training:	 Loss: 0.3476

Training:	 ACC: 0.8750 0.8743 0.8597 0.8903
Validation:	 ACC: 0.8162 0.8154 0.7998 0.8326
Validation:	 Best_BACC: 0.8162 0.8154 0.7998 0.8326
Validation:	 Loss: 0.4257
Pretraining:	Epoch 8/200
----------
training:	Epoch: [8][1/233]	Loss 0.2281 (0.2281)	
training:	Epoch: [8][2/233]	Loss 0.3224 (0.2753)	
training:	Epoch: [8][3/233]	Loss 0.1520 (0.2342)	
training:	Epoch: [8][4/233]	Loss 0.2433 (0.2364)	
training:	Epoch: [8][5/233]	Loss 0.2680 (0.2428)	
training:	Epoch: [8][6/233]	Loss 0.4556 (0.2782)	
training:	Epoch: [8][7/233]	Loss 0.2532 (0.2746)	
training:	Epoch: [8][8/233]	Loss 0.3302 (0.2816)	
training:	Epoch: [8][9/233]	Loss 0.3675 (0.2911)	
training:	Epoch: [8][10/233]	Loss 0.4068 (0.3027)	
training:	Epoch: [8][11/233]	Loss 0.4532 (0.3164)	
training:	Epoch: [8][12/233]	Loss 0.4296 (0.3258)	
training:	Epoch: [8][13/233]	Loss 0.2498 (0.3200)	
training:	Epoch: [8][14/233]	Loss 0.3376 (0.3212)	
training:	Epoch: [8][15/233]	Loss 0.4986 (0.3331)	
training:	Epoch: [8][16/233]	Loss 0.2785 (0.3296)	
training:	Epoch: [8][17/233]	Loss 0.5266 (0.3412)	
training:	Epoch: [8][18/233]	Loss 0.2928 (0.3385)	
training:	Epoch: [8][19/233]	Loss 0.2041 (0.3315)	
training:	Epoch: [8][20/233]	Loss 0.3626 (0.3330)	
training:	Epoch: [8][21/233]	Loss 0.2692 (0.3300)	
training:	Epoch: [8][22/233]	Loss 0.4082 (0.3335)	
training:	Epoch: [8][23/233]	Loss 0.3395 (0.3338)	
training:	Epoch: [8][24/233]	Loss 0.3692 (0.3353)	
training:	Epoch: [8][25/233]	Loss 0.3274 (0.3350)	
training:	Epoch: [8][26/233]	Loss 0.2960 (0.3335)	
training:	Epoch: [8][27/233]	Loss 0.4686 (0.3385)	
training:	Epoch: [8][28/233]	Loss 0.3465 (0.3387)	
training:	Epoch: [8][29/233]	Loss 0.4880 (0.3439)	
training:	Epoch: [8][30/233]	Loss 0.2857 (0.3420)	
training:	Epoch: [8][31/233]	Loss 0.1903 (0.3371)	
training:	Epoch: [8][32/233]	Loss 0.2888 (0.3356)	
training:	Epoch: [8][33/233]	Loss 0.3672 (0.3365)	
training:	Epoch: [8][34/233]	Loss 0.2276 (0.3333)	
training:	Epoch: [8][35/233]	Loss 0.3260 (0.3331)	
training:	Epoch: [8][36/233]	Loss 0.2044 (0.3295)	
training:	Epoch: [8][37/233]	Loss 0.4278 (0.3322)	
training:	Epoch: [8][38/233]	Loss 0.2402 (0.3298)	
training:	Epoch: [8][39/233]	Loss 0.3637 (0.3306)	
training:	Epoch: [8][40/233]	Loss 0.4398 (0.3334)	
training:	Epoch: [8][41/233]	Loss 0.3896 (0.3347)	
training:	Epoch: [8][42/233]	Loss 0.1405 (0.3301)	
training:	Epoch: [8][43/233]	Loss 0.2074 (0.3273)	
training:	Epoch: [8][44/233]	Loss 0.3531 (0.3278)	
training:	Epoch: [8][45/233]	Loss 0.2395 (0.3259)	
training:	Epoch: [8][46/233]	Loss 0.5555 (0.3309)	
training:	Epoch: [8][47/233]	Loss 0.3924 (0.3322)	
training:	Epoch: [8][48/233]	Loss 0.4374 (0.3344)	
training:	Epoch: [8][49/233]	Loss 0.1972 (0.3316)	
training:	Epoch: [8][50/233]	Loss 0.3424 (0.3318)	
training:	Epoch: [8][51/233]	Loss 0.4053 (0.3332)	
training:	Epoch: [8][52/233]	Loss 0.4365 (0.3352)	
training:	Epoch: [8][53/233]	Loss 0.1416 (0.3316)	
training:	Epoch: [8][54/233]	Loss 0.5351 (0.3353)	
training:	Epoch: [8][55/233]	Loss 0.3767 (0.3361)	
training:	Epoch: [8][56/233]	Loss 0.4559 (0.3382)	
training:	Epoch: [8][57/233]	Loss 0.3290 (0.3381)	
training:	Epoch: [8][58/233]	Loss 0.3036 (0.3375)	
training:	Epoch: [8][59/233]	Loss 0.3139 (0.3371)	
training:	Epoch: [8][60/233]	Loss 0.3950 (0.3380)	
training:	Epoch: [8][61/233]	Loss 0.4621 (0.3401)	
training:	Epoch: [8][62/233]	Loss 0.2967 (0.3394)	
training:	Epoch: [8][63/233]	Loss 0.4355 (0.3409)	
training:	Epoch: [8][64/233]	Loss 0.5216 (0.3437)	
training:	Epoch: [8][65/233]	Loss 0.2985 (0.3430)	
training:	Epoch: [8][66/233]	Loss 0.3446 (0.3430)	
training:	Epoch: [8][67/233]	Loss 0.3730 (0.3435)	
training:	Epoch: [8][68/233]	Loss 0.2914 (0.3427)	
training:	Epoch: [8][69/233]	Loss 0.2594 (0.3415)	
training:	Epoch: [8][70/233]	Loss 0.3808 (0.3421)	
training:	Epoch: [8][71/233]	Loss 0.2288 (0.3405)	
training:	Epoch: [8][72/233]	Loss 0.4014 (0.3413)	
training:	Epoch: [8][73/233]	Loss 0.2676 (0.3403)	
training:	Epoch: [8][74/233]	Loss 0.5481 (0.3431)	
training:	Epoch: [8][75/233]	Loss 0.4233 (0.3442)	
training:	Epoch: [8][76/233]	Loss 0.3537 (0.3443)	
training:	Epoch: [8][77/233]	Loss 0.3115 (0.3439)	
training:	Epoch: [8][78/233]	Loss 0.3232 (0.3436)	
training:	Epoch: [8][79/233]	Loss 0.5200 (0.3459)	
training:	Epoch: [8][80/233]	Loss 0.2697 (0.3449)	
training:	Epoch: [8][81/233]	Loss 0.4288 (0.3459)	
training:	Epoch: [8][82/233]	Loss 0.1859 (0.3440)	
training:	Epoch: [8][83/233]	Loss 0.3526 (0.3441)	
training:	Epoch: [8][84/233]	Loss 0.1927 (0.3423)	
training:	Epoch: [8][85/233]	Loss 0.3844 (0.3428)	
training:	Epoch: [8][86/233]	Loss 0.2565 (0.3418)	
training:	Epoch: [8][87/233]	Loss 0.6249 (0.3450)	
training:	Epoch: [8][88/233]	Loss 0.5039 (0.3468)	
training:	Epoch: [8][89/233]	Loss 0.3119 (0.3465)	
training:	Epoch: [8][90/233]	Loss 0.3937 (0.3470)	
training:	Epoch: [8][91/233]	Loss 0.3365 (0.3469)	
training:	Epoch: [8][92/233]	Loss 0.2992 (0.3463)	
training:	Epoch: [8][93/233]	Loss 0.4222 (0.3472)	
training:	Epoch: [8][94/233]	Loss 0.3345 (0.3470)	
training:	Epoch: [8][95/233]	Loss 0.2981 (0.3465)	
training:	Epoch: [8][96/233]	Loss 0.2326 (0.3453)	
training:	Epoch: [8][97/233]	Loss 0.2772 (0.3446)	
training:	Epoch: [8][98/233]	Loss 0.3302 (0.3445)	
training:	Epoch: [8][99/233]	Loss 0.3233 (0.3443)	
training:	Epoch: [8][100/233]	Loss 0.2595 (0.3434)	
training:	Epoch: [8][101/233]	Loss 0.2244 (0.3422)	
training:	Epoch: [8][102/233]	Loss 0.2347 (0.3412)	
training:	Epoch: [8][103/233]	Loss 0.5547 (0.3433)	
training:	Epoch: [8][104/233]	Loss 0.2316 (0.3422)	
training:	Epoch: [8][105/233]	Loss 0.4644 (0.3433)	
training:	Epoch: [8][106/233]	Loss 0.3562 (0.3435)	
training:	Epoch: [8][107/233]	Loss 0.2912 (0.3430)	
training:	Epoch: [8][108/233]	Loss 0.2275 (0.3419)	
training:	Epoch: [8][109/233]	Loss 0.2884 (0.3414)	
training:	Epoch: [8][110/233]	Loss 0.3026 (0.3411)	
training:	Epoch: [8][111/233]	Loss 0.5395 (0.3429)	
training:	Epoch: [8][112/233]	Loss 0.5147 (0.3444)	
training:	Epoch: [8][113/233]	Loss 0.2878 (0.3439)	
training:	Epoch: [8][114/233]	Loss 0.3120 (0.3436)	
training:	Epoch: [8][115/233]	Loss 0.3368 (0.3435)	
training:	Epoch: [8][116/233]	Loss 0.4795 (0.3447)	
training:	Epoch: [8][117/233]	Loss 0.5461 (0.3464)	
training:	Epoch: [8][118/233]	Loss 0.2246 (0.3454)	
training:	Epoch: [8][119/233]	Loss 0.3750 (0.3457)	
training:	Epoch: [8][120/233]	Loss 0.2096 (0.3445)	
training:	Epoch: [8][121/233]	Loss 0.3958 (0.3449)	
training:	Epoch: [8][122/233]	Loss 0.2924 (0.3445)	
training:	Epoch: [8][123/233]	Loss 0.3911 (0.3449)	
training:	Epoch: [8][124/233]	Loss 0.3705 (0.3451)	
training:	Epoch: [8][125/233]	Loss 0.5916 (0.3471)	
training:	Epoch: [8][126/233]	Loss 0.3990 (0.3475)	
training:	Epoch: [8][127/233]	Loss 0.3230 (0.3473)	
training:	Epoch: [8][128/233]	Loss 0.1951 (0.3461)	
training:	Epoch: [8][129/233]	Loss 0.5927 (0.3480)	
training:	Epoch: [8][130/233]	Loss 0.6157 (0.3501)	
training:	Epoch: [8][131/233]	Loss 0.2507 (0.3493)	
training:	Epoch: [8][132/233]	Loss 0.2109 (0.3483)	
training:	Epoch: [8][133/233]	Loss 0.3573 (0.3483)	
training:	Epoch: [8][134/233]	Loss 0.3538 (0.3484)	
training:	Epoch: [8][135/233]	Loss 0.3228 (0.3482)	
training:	Epoch: [8][136/233]	Loss 0.2822 (0.3477)	
training:	Epoch: [8][137/233]	Loss 0.3185 (0.3475)	
training:	Epoch: [8][138/233]	Loss 0.2777 (0.3470)	
training:	Epoch: [8][139/233]	Loss 0.2802 (0.3465)	
training:	Epoch: [8][140/233]	Loss 0.3285 (0.3464)	
training:	Epoch: [8][141/233]	Loss 0.3182 (0.3462)	
training:	Epoch: [8][142/233]	Loss 0.1338 (0.3447)	
training:	Epoch: [8][143/233]	Loss 0.4159 (0.3452)	
training:	Epoch: [8][144/233]	Loss 0.3252 (0.3450)	
training:	Epoch: [8][145/233]	Loss 0.4148 (0.3455)	
training:	Epoch: [8][146/233]	Loss 0.3983 (0.3459)	
training:	Epoch: [8][147/233]	Loss 0.3286 (0.3458)	
training:	Epoch: [8][148/233]	Loss 0.2427 (0.3451)	
training:	Epoch: [8][149/233]	Loss 0.2722 (0.3446)	
training:	Epoch: [8][150/233]	Loss 0.2224 (0.3438)	
training:	Epoch: [8][151/233]	Loss 0.3482 (0.3438)	
training:	Epoch: [8][152/233]	Loss 0.1949 (0.3428)	
training:	Epoch: [8][153/233]	Loss 0.2197 (0.3420)	
training:	Epoch: [8][154/233]	Loss 0.1855 (0.3410)	
training:	Epoch: [8][155/233]	Loss 0.3198 (0.3409)	
training:	Epoch: [8][156/233]	Loss 0.2584 (0.3403)	
training:	Epoch: [8][157/233]	Loss 0.3066 (0.3401)	
training:	Epoch: [8][158/233]	Loss 0.3754 (0.3403)	
training:	Epoch: [8][159/233]	Loss 0.3070 (0.3401)	
training:	Epoch: [8][160/233]	Loss 0.2870 (0.3398)	
training:	Epoch: [8][161/233]	Loss 0.3868 (0.3401)	
training:	Epoch: [8][162/233]	Loss 0.3327 (0.3400)	
training:	Epoch: [8][163/233]	Loss 0.3094 (0.3399)	
training:	Epoch: [8][164/233]	Loss 0.3100 (0.3397)	
training:	Epoch: [8][165/233]	Loss 0.3212 (0.3396)	
training:	Epoch: [8][166/233]	Loss 0.2042 (0.3387)	
training:	Epoch: [8][167/233]	Loss 0.4329 (0.3393)	
training:	Epoch: [8][168/233]	Loss 0.3001 (0.3391)	
training:	Epoch: [8][169/233]	Loss 0.4582 (0.3398)	
training:	Epoch: [8][170/233]	Loss 0.2730 (0.3394)	
training:	Epoch: [8][171/233]	Loss 0.1258 (0.3381)	
training:	Epoch: [8][172/233]	Loss 0.4101 (0.3386)	
training:	Epoch: [8][173/233]	Loss 0.4724 (0.3393)	
training:	Epoch: [8][174/233]	Loss 0.2459 (0.3388)	
training:	Epoch: [8][175/233]	Loss 0.3680 (0.3390)	
training:	Epoch: [8][176/233]	Loss 0.2681 (0.3386)	
training:	Epoch: [8][177/233]	Loss 0.3086 (0.3384)	
training:	Epoch: [8][178/233]	Loss 0.3233 (0.3383)	
training:	Epoch: [8][179/233]	Loss 0.3180 (0.3382)	
training:	Epoch: [8][180/233]	Loss 0.2792 (0.3379)	
training:	Epoch: [8][181/233]	Loss 0.2387 (0.3373)	
training:	Epoch: [8][182/233]	Loss 0.4191 (0.3378)	
training:	Epoch: [8][183/233]	Loss 0.2334 (0.3372)	
training:	Epoch: [8][184/233]	Loss 0.4295 (0.3377)	
training:	Epoch: [8][185/233]	Loss 0.4326 (0.3382)	
training:	Epoch: [8][186/233]	Loss 0.4074 (0.3386)	
training:	Epoch: [8][187/233]	Loss 0.1921 (0.3378)	
training:	Epoch: [8][188/233]	Loss 0.3538 (0.3379)	
training:	Epoch: [8][189/233]	Loss 0.2688 (0.3375)	
training:	Epoch: [8][190/233]	Loss 0.3876 (0.3378)	
training:	Epoch: [8][191/233]	Loss 0.4112 (0.3382)	
training:	Epoch: [8][192/233]	Loss 0.2672 (0.3378)	
training:	Epoch: [8][193/233]	Loss 0.2310 (0.3372)	
training:	Epoch: [8][194/233]	Loss 0.1979 (0.3365)	
training:	Epoch: [8][195/233]	Loss 0.2899 (0.3363)	
training:	Epoch: [8][196/233]	Loss 0.5053 (0.3371)	
training:	Epoch: [8][197/233]	Loss 0.3004 (0.3370)	
training:	Epoch: [8][198/233]	Loss 0.2435 (0.3365)	
training:	Epoch: [8][199/233]	Loss 0.2791 (0.3362)	
training:	Epoch: [8][200/233]	Loss 0.2817 (0.3359)	
training:	Epoch: [8][201/233]	Loss 0.2772 (0.3356)	
training:	Epoch: [8][202/233]	Loss 0.4081 (0.3360)	
training:	Epoch: [8][203/233]	Loss 0.3155 (0.3359)	
training:	Epoch: [8][204/233]	Loss 0.2944 (0.3357)	
training:	Epoch: [8][205/233]	Loss 0.3915 (0.3360)	
training:	Epoch: [8][206/233]	Loss 0.3441 (0.3360)	
training:	Epoch: [8][207/233]	Loss 0.1409 (0.3351)	
training:	Epoch: [8][208/233]	Loss 0.3335 (0.3350)	
training:	Epoch: [8][209/233]	Loss 0.2075 (0.3344)	
training:	Epoch: [8][210/233]	Loss 0.4697 (0.3351)	
training:	Epoch: [8][211/233]	Loss 0.2454 (0.3347)	
training:	Epoch: [8][212/233]	Loss 0.3487 (0.3347)	
training:	Epoch: [8][213/233]	Loss 0.2983 (0.3346)	
training:	Epoch: [8][214/233]	Loss 0.3479 (0.3346)	
training:	Epoch: [8][215/233]	Loss 0.2575 (0.3343)	
training:	Epoch: [8][216/233]	Loss 0.2057 (0.3337)	
training:	Epoch: [8][217/233]	Loss 0.4382 (0.3341)	
training:	Epoch: [8][218/233]	Loss 0.3057 (0.3340)	
training:	Epoch: [8][219/233]	Loss 0.2245 (0.3335)	
training:	Epoch: [8][220/233]	Loss 0.4953 (0.3342)	
training:	Epoch: [8][221/233]	Loss 0.1465 (0.3334)	
training:	Epoch: [8][222/233]	Loss 0.4496 (0.3339)	
training:	Epoch: [8][223/233]	Loss 0.4067 (0.3342)	
training:	Epoch: [8][224/233]	Loss 0.2744 (0.3340)	
training:	Epoch: [8][225/233]	Loss 0.2906 (0.3338)	
training:	Epoch: [8][226/233]	Loss 0.3421 (0.3338)	
training:	Epoch: [8][227/233]	Loss 0.2325 (0.3334)	
training:	Epoch: [8][228/233]	Loss 0.1535 (0.3326)	
training:	Epoch: [8][229/233]	Loss 0.6137 (0.3338)	
training:	Epoch: [8][230/233]	Loss 0.6411 (0.3352)	
training:	Epoch: [8][231/233]	Loss 0.4680 (0.3357)	
training:	Epoch: [8][232/233]	Loss 0.3821 (0.3359)	
training:	Epoch: [8][233/233]	Loss 0.3233 (0.3359)	
Training:	 Loss: 0.3351

Training:	 ACC: 0.8817 0.8816 0.8787 0.8847
Validation:	 ACC: 0.8149 0.8149 0.8141 0.8157
Validation:	 Best_BACC: 0.8162 0.8154 0.7998 0.8326
Validation:	 Loss: 0.4279
Pretraining:	Epoch 9/200
----------
training:	Epoch: [9][1/233]	Loss 0.3817 (0.3817)	
training:	Epoch: [9][2/233]	Loss 0.2736 (0.3277)	
training:	Epoch: [9][3/233]	Loss 0.3128 (0.3227)	
training:	Epoch: [9][4/233]	Loss 0.3651 (0.3333)	
training:	Epoch: [9][5/233]	Loss 0.3769 (0.3420)	
training:	Epoch: [9][6/233]	Loss 0.2750 (0.3309)	
training:	Epoch: [9][7/233]	Loss 0.3510 (0.3337)	
training:	Epoch: [9][8/233]	Loss 0.2950 (0.3289)	
training:	Epoch: [9][9/233]	Loss 0.3539 (0.3317)	
training:	Epoch: [9][10/233]	Loss 0.3395 (0.3325)	
training:	Epoch: [9][11/233]	Loss 0.2405 (0.3241)	
training:	Epoch: [9][12/233]	Loss 0.3874 (0.3294)	
training:	Epoch: [9][13/233]	Loss 0.1370 (0.3146)	
training:	Epoch: [9][14/233]	Loss 0.3510 (0.3172)	
training:	Epoch: [9][15/233]	Loss 0.2621 (0.3135)	
training:	Epoch: [9][16/233]	Loss 0.3398 (0.3152)	
training:	Epoch: [9][17/233]	Loss 0.3154 (0.3152)	
training:	Epoch: [9][18/233]	Loss 0.3392 (0.3165)	
training:	Epoch: [9][19/233]	Loss 0.3288 (0.3171)	
training:	Epoch: [9][20/233]	Loss 0.3164 (0.3171)	
training:	Epoch: [9][21/233]	Loss 0.3182 (0.3172)	
training:	Epoch: [9][22/233]	Loss 0.2763 (0.3153)	
training:	Epoch: [9][23/233]	Loss 0.3653 (0.3175)	
training:	Epoch: [9][24/233]	Loss 0.2684 (0.3154)	
training:	Epoch: [9][25/233]	Loss 0.1846 (0.3102)	
training:	Epoch: [9][26/233]	Loss 0.2248 (0.3069)	
training:	Epoch: [9][27/233]	Loss 0.3428 (0.3082)	
training:	Epoch: [9][28/233]	Loss 0.2028 (0.3045)	
training:	Epoch: [9][29/233]	Loss 0.4346 (0.3090)	
training:	Epoch: [9][30/233]	Loss 0.2950 (0.3085)	
training:	Epoch: [9][31/233]	Loss 0.3435 (0.3096)	
training:	Epoch: [9][32/233]	Loss 0.3522 (0.3110)	
training:	Epoch: [9][33/233]	Loss 0.3594 (0.3124)	
training:	Epoch: [9][34/233]	Loss 0.3503 (0.3135)	
training:	Epoch: [9][35/233]	Loss 0.3806 (0.3155)	
training:	Epoch: [9][36/233]	Loss 0.2568 (0.3138)	
training:	Epoch: [9][37/233]	Loss 0.1685 (0.3099)	
training:	Epoch: [9][38/233]	Loss 0.3247 (0.3103)	
training:	Epoch: [9][39/233]	Loss 0.3330 (0.3109)	
training:	Epoch: [9][40/233]	Loss 0.4068 (0.3133)	
training:	Epoch: [9][41/233]	Loss 0.3159 (0.3133)	
training:	Epoch: [9][42/233]	Loss 0.2411 (0.3116)	
training:	Epoch: [9][43/233]	Loss 0.2344 (0.3098)	
training:	Epoch: [9][44/233]	Loss 0.4717 (0.3135)	
training:	Epoch: [9][45/233]	Loss 0.2522 (0.3121)	
training:	Epoch: [9][46/233]	Loss 0.3139 (0.3122)	
training:	Epoch: [9][47/233]	Loss 0.2443 (0.3107)	
training:	Epoch: [9][48/233]	Loss 0.4047 (0.3127)	
training:	Epoch: [9][49/233]	Loss 0.1913 (0.3102)	
training:	Epoch: [9][50/233]	Loss 0.2027 (0.3081)	
training:	Epoch: [9][51/233]	Loss 0.2547 (0.3070)	
training:	Epoch: [9][52/233]	Loss 0.3999 (0.3088)	
training:	Epoch: [9][53/233]	Loss 0.2566 (0.3078)	
training:	Epoch: [9][54/233]	Loss 0.2731 (0.3072)	
training:	Epoch: [9][55/233]	Loss 0.4637 (0.3100)	
training:	Epoch: [9][56/233]	Loss 0.4852 (0.3132)	
training:	Epoch: [9][57/233]	Loss 0.3188 (0.3132)	
training:	Epoch: [9][58/233]	Loss 0.2342 (0.3119)	
training:	Epoch: [9][59/233]	Loss 0.2969 (0.3116)	
training:	Epoch: [9][60/233]	Loss 0.2893 (0.3113)	
training:	Epoch: [9][61/233]	Loss 0.4114 (0.3129)	
training:	Epoch: [9][62/233]	Loss 0.3812 (0.3140)	
training:	Epoch: [9][63/233]	Loss 0.2918 (0.3137)	
training:	Epoch: [9][64/233]	Loss 0.4362 (0.3156)	
training:	Epoch: [9][65/233]	Loss 0.4879 (0.3182)	
training:	Epoch: [9][66/233]	Loss 0.2414 (0.3171)	
training:	Epoch: [9][67/233]	Loss 0.3980 (0.3183)	
training:	Epoch: [9][68/233]	Loss 0.5668 (0.3219)	
training:	Epoch: [9][69/233]	Loss 0.4429 (0.3237)	
training:	Epoch: [9][70/233]	Loss 0.2374 (0.3224)	
training:	Epoch: [9][71/233]	Loss 0.3477 (0.3228)	
training:	Epoch: [9][72/233]	Loss 0.1819 (0.3208)	
training:	Epoch: [9][73/233]	Loss 0.3505 (0.3212)	
training:	Epoch: [9][74/233]	Loss 0.3739 (0.3220)	
training:	Epoch: [9][75/233]	Loss 0.4236 (0.3233)	
training:	Epoch: [9][76/233]	Loss 0.4112 (0.3245)	
training:	Epoch: [9][77/233]	Loss 0.5416 (0.3273)	
training:	Epoch: [9][78/233]	Loss 0.2649 (0.3265)	
training:	Epoch: [9][79/233]	Loss 0.4685 (0.3283)	
training:	Epoch: [9][80/233]	Loss 0.4085 (0.3293)	
training:	Epoch: [9][81/233]	Loss 0.2872 (0.3288)	
training:	Epoch: [9][82/233]	Loss 0.4055 (0.3297)	
training:	Epoch: [9][83/233]	Loss 0.1852 (0.3280)	
training:	Epoch: [9][84/233]	Loss 0.3507 (0.3282)	
training:	Epoch: [9][85/233]	Loss 0.2393 (0.3272)	
training:	Epoch: [9][86/233]	Loss 0.3167 (0.3271)	
training:	Epoch: [9][87/233]	Loss 0.3918 (0.3278)	
training:	Epoch: [9][88/233]	Loss 0.2667 (0.3271)	
training:	Epoch: [9][89/233]	Loss 0.2900 (0.3267)	
training:	Epoch: [9][90/233]	Loss 0.1284 (0.3245)	
training:	Epoch: [9][91/233]	Loss 0.3907 (0.3252)	
training:	Epoch: [9][92/233]	Loss 0.2551 (0.3245)	
training:	Epoch: [9][93/233]	Loss 0.4710 (0.3260)	
training:	Epoch: [9][94/233]	Loss 0.2459 (0.3252)	
training:	Epoch: [9][95/233]	Loss 0.2481 (0.3244)	
training:	Epoch: [9][96/233]	Loss 0.2696 (0.3238)	
training:	Epoch: [9][97/233]	Loss 0.2795 (0.3233)	
training:	Epoch: [9][98/233]	Loss 0.4634 (0.3248)	
training:	Epoch: [9][99/233]	Loss 0.4753 (0.3263)	
training:	Epoch: [9][100/233]	Loss 0.1973 (0.3250)	
training:	Epoch: [9][101/233]	Loss 0.3553 (0.3253)	
training:	Epoch: [9][102/233]	Loss 0.2568 (0.3246)	
training:	Epoch: [9][103/233]	Loss 0.2266 (0.3237)	
training:	Epoch: [9][104/233]	Loss 0.2284 (0.3228)	
training:	Epoch: [9][105/233]	Loss 0.1851 (0.3215)	
training:	Epoch: [9][106/233]	Loss 0.2400 (0.3207)	
training:	Epoch: [9][107/233]	Loss 0.5960 (0.3233)	
training:	Epoch: [9][108/233]	Loss 0.2904 (0.3230)	
training:	Epoch: [9][109/233]	Loss 0.3357 (0.3231)	
training:	Epoch: [9][110/233]	Loss 0.4050 (0.3238)	
training:	Epoch: [9][111/233]	Loss 0.2778 (0.3234)	
training:	Epoch: [9][112/233]	Loss 0.2287 (0.3226)	
training:	Epoch: [9][113/233]	Loss 0.4794 (0.3239)	
training:	Epoch: [9][114/233]	Loss 0.2612 (0.3234)	
training:	Epoch: [9][115/233]	Loss 0.2964 (0.3232)	
training:	Epoch: [9][116/233]	Loss 0.2565 (0.3226)	
training:	Epoch: [9][117/233]	Loss 0.4330 (0.3235)	
training:	Epoch: [9][118/233]	Loss 0.1614 (0.3222)	
training:	Epoch: [9][119/233]	Loss 0.2379 (0.3214)	
training:	Epoch: [9][120/233]	Loss 0.2300 (0.3207)	
training:	Epoch: [9][121/233]	Loss 0.3914 (0.3213)	
training:	Epoch: [9][122/233]	Loss 0.3649 (0.3216)	
training:	Epoch: [9][123/233]	Loss 0.1386 (0.3201)	
training:	Epoch: [9][124/233]	Loss 0.3378 (0.3203)	
training:	Epoch: [9][125/233]	Loss 0.3006 (0.3201)	
training:	Epoch: [9][126/233]	Loss 0.4221 (0.3209)	
training:	Epoch: [9][127/233]	Loss 0.1103 (0.3193)	
training:	Epoch: [9][128/233]	Loss 0.4550 (0.3203)	
training:	Epoch: [9][129/233]	Loss 0.2009 (0.3194)	
training:	Epoch: [9][130/233]	Loss 0.5139 (0.3209)	
training:	Epoch: [9][131/233]	Loss 0.4316 (0.3218)	
training:	Epoch: [9][132/233]	Loss 0.2584 (0.3213)	
training:	Epoch: [9][133/233]	Loss 0.4382 (0.3221)	
training:	Epoch: [9][134/233]	Loss 0.1917 (0.3212)	
training:	Epoch: [9][135/233]	Loss 0.4092 (0.3218)	
training:	Epoch: [9][136/233]	Loss 0.1898 (0.3209)	
training:	Epoch: [9][137/233]	Loss 0.2972 (0.3207)	
training:	Epoch: [9][138/233]	Loss 0.2859 (0.3204)	
training:	Epoch: [9][139/233]	Loss 0.2930 (0.3202)	
training:	Epoch: [9][140/233]	Loss 0.4938 (0.3215)	
training:	Epoch: [9][141/233]	Loss 0.4024 (0.3220)	
training:	Epoch: [9][142/233]	Loss 0.4309 (0.3228)	
training:	Epoch: [9][143/233]	Loss 0.4366 (0.3236)	
training:	Epoch: [9][144/233]	Loss 0.2244 (0.3229)	
training:	Epoch: [9][145/233]	Loss 0.5316 (0.3244)	
training:	Epoch: [9][146/233]	Loss 0.2429 (0.3238)	
training:	Epoch: [9][147/233]	Loss 0.3000 (0.3236)	
training:	Epoch: [9][148/233]	Loss 0.4413 (0.3244)	
training:	Epoch: [9][149/233]	Loss 0.2082 (0.3237)	
training:	Epoch: [9][150/233]	Loss 0.4633 (0.3246)	
training:	Epoch: [9][151/233]	Loss 0.3849 (0.3250)	
training:	Epoch: [9][152/233]	Loss 0.2385 (0.3244)	
training:	Epoch: [9][153/233]	Loss 0.1893 (0.3235)	
training:	Epoch: [9][154/233]	Loss 0.3288 (0.3236)	
training:	Epoch: [9][155/233]	Loss 0.4585 (0.3244)	
training:	Epoch: [9][156/233]	Loss 0.3430 (0.3246)	
training:	Epoch: [9][157/233]	Loss 0.3823 (0.3249)	
training:	Epoch: [9][158/233]	Loss 0.2703 (0.3246)	
training:	Epoch: [9][159/233]	Loss 0.1977 (0.3238)	
training:	Epoch: [9][160/233]	Loss 0.3084 (0.3237)	
training:	Epoch: [9][161/233]	Loss 0.2585 (0.3233)	
training:	Epoch: [9][162/233]	Loss 0.2331 (0.3227)	
training:	Epoch: [9][163/233]	Loss 0.2106 (0.3220)	
training:	Epoch: [9][164/233]	Loss 0.2163 (0.3214)	
training:	Epoch: [9][165/233]	Loss 0.3423 (0.3215)	
training:	Epoch: [9][166/233]	Loss 0.3031 (0.3214)	
training:	Epoch: [9][167/233]	Loss 0.2374 (0.3209)	
training:	Epoch: [9][168/233]	Loss 0.2467 (0.3205)	
training:	Epoch: [9][169/233]	Loss 0.2792 (0.3202)	
training:	Epoch: [9][170/233]	Loss 0.2707 (0.3199)	
training:	Epoch: [9][171/233]	Loss 0.2395 (0.3195)	
training:	Epoch: [9][172/233]	Loss 0.3201 (0.3195)	
training:	Epoch: [9][173/233]	Loss 0.3161 (0.3194)	
training:	Epoch: [9][174/233]	Loss 0.2881 (0.3193)	
training:	Epoch: [9][175/233]	Loss 0.2513 (0.3189)	
training:	Epoch: [9][176/233]	Loss 0.1357 (0.3178)	
training:	Epoch: [9][177/233]	Loss 0.2141 (0.3172)	
training:	Epoch: [9][178/233]	Loss 0.2408 (0.3168)	
training:	Epoch: [9][179/233]	Loss 0.3181 (0.3168)	
training:	Epoch: [9][180/233]	Loss 0.4634 (0.3176)	
training:	Epoch: [9][181/233]	Loss 0.2488 (0.3173)	
training:	Epoch: [9][182/233]	Loss 0.2369 (0.3168)	
training:	Epoch: [9][183/233]	Loss 0.2665 (0.3165)	
training:	Epoch: [9][184/233]	Loss 0.5358 (0.3177)	
training:	Epoch: [9][185/233]	Loss 0.4065 (0.3182)	
training:	Epoch: [9][186/233]	Loss 0.1424 (0.3173)	
training:	Epoch: [9][187/233]	Loss 0.3924 (0.3177)	
training:	Epoch: [9][188/233]	Loss 0.3769 (0.3180)	
training:	Epoch: [9][189/233]	Loss 0.3808 (0.3183)	
training:	Epoch: [9][190/233]	Loss 0.2628 (0.3180)	
training:	Epoch: [9][191/233]	Loss 0.3711 (0.3183)	
training:	Epoch: [9][192/233]	Loss 0.3691 (0.3186)	
training:	Epoch: [9][193/233]	Loss 0.4147 (0.3191)	
training:	Epoch: [9][194/233]	Loss 0.2231 (0.3186)	
training:	Epoch: [9][195/233]	Loss 0.4061 (0.3190)	
training:	Epoch: [9][196/233]	Loss 0.3312 (0.3191)	
training:	Epoch: [9][197/233]	Loss 0.3070 (0.3190)	
training:	Epoch: [9][198/233]	Loss 0.2770 (0.3188)	
training:	Epoch: [9][199/233]	Loss 0.4226 (0.3193)	
training:	Epoch: [9][200/233]	Loss 0.3867 (0.3197)	
training:	Epoch: [9][201/233]	Loss 0.3788 (0.3200)	
training:	Epoch: [9][202/233]	Loss 0.6030 (0.3214)	
training:	Epoch: [9][203/233]	Loss 0.4429 (0.3220)	
training:	Epoch: [9][204/233]	Loss 0.4374 (0.3225)	
training:	Epoch: [9][205/233]	Loss 0.5719 (0.3237)	
training:	Epoch: [9][206/233]	Loss 0.3304 (0.3238)	
training:	Epoch: [9][207/233]	Loss 0.4438 (0.3244)	
training:	Epoch: [9][208/233]	Loss 0.3773 (0.3246)	
training:	Epoch: [9][209/233]	Loss 0.2762 (0.3244)	
training:	Epoch: [9][210/233]	Loss 0.4380 (0.3249)	
training:	Epoch: [9][211/233]	Loss 0.4326 (0.3254)	
training:	Epoch: [9][212/233]	Loss 0.1899 (0.3248)	
training:	Epoch: [9][213/233]	Loss 0.4304 (0.3253)	
training:	Epoch: [9][214/233]	Loss 0.1705 (0.3246)	
training:	Epoch: [9][215/233]	Loss 0.2618 (0.3243)	
training:	Epoch: [9][216/233]	Loss 0.3393 (0.3243)	
training:	Epoch: [9][217/233]	Loss 0.2125 (0.3238)	
training:	Epoch: [9][218/233]	Loss 0.4564 (0.3244)	
training:	Epoch: [9][219/233]	Loss 0.4019 (0.3248)	
training:	Epoch: [9][220/233]	Loss 0.2965 (0.3247)	
training:	Epoch: [9][221/233]	Loss 0.3172 (0.3246)	
training:	Epoch: [9][222/233]	Loss 0.2832 (0.3244)	
training:	Epoch: [9][223/233]	Loss 0.3163 (0.3244)	
training:	Epoch: [9][224/233]	Loss 0.4101 (0.3248)	
training:	Epoch: [9][225/233]	Loss 0.3657 (0.3250)	
training:	Epoch: [9][226/233]	Loss 0.2135 (0.3245)	
training:	Epoch: [9][227/233]	Loss 0.2786 (0.3243)	
training:	Epoch: [9][228/233]	Loss 0.2109 (0.3238)	
training:	Epoch: [9][229/233]	Loss 0.2368 (0.3234)	
training:	Epoch: [9][230/233]	Loss 0.2524 (0.3231)	
training:	Epoch: [9][231/233]	Loss 0.4235 (0.3235)	
training:	Epoch: [9][232/233]	Loss 0.5535 (0.3245)	
training:	Epoch: [9][233/233]	Loss 0.2525 (0.3242)	
Training:	 Loss: 0.3235

Training:	 ACC: 0.8777 0.8790 0.9082 0.8472
Validation:	 ACC: 0.8099 0.8117 0.8468 0.7730
Validation:	 Best_BACC: 0.8162 0.8154 0.7998 0.8326
Validation:	 Loss: 0.4283
Pretraining:	Epoch 10/200
----------
training:	Epoch: [10][1/233]	Loss 0.2644 (0.2644)	
training:	Epoch: [10][2/233]	Loss 0.5870 (0.4257)	
training:	Epoch: [10][3/233]	Loss 0.3103 (0.3873)	
training:	Epoch: [10][4/233]	Loss 0.2878 (0.3624)	
training:	Epoch: [10][5/233]	Loss 0.1544 (0.3208)	
training:	Epoch: [10][6/233]	Loss 0.1618 (0.2943)	
training:	Epoch: [10][7/233]	Loss 0.3660 (0.3046)	
training:	Epoch: [10][8/233]	Loss 0.3733 (0.3131)	
training:	Epoch: [10][9/233]	Loss 0.3448 (0.3167)	
training:	Epoch: [10][10/233]	Loss 0.3101 (0.3160)	
training:	Epoch: [10][11/233]	Loss 0.4716 (0.3302)	
training:	Epoch: [10][12/233]	Loss 0.2697 (0.3251)	
training:	Epoch: [10][13/233]	Loss 0.3646 (0.3282)	
training:	Epoch: [10][14/233]	Loss 0.1970 (0.3188)	
training:	Epoch: [10][15/233]	Loss 0.3332 (0.3197)	
training:	Epoch: [10][16/233]	Loss 0.4438 (0.3275)	
training:	Epoch: [10][17/233]	Loss 0.3165 (0.3269)	
training:	Epoch: [10][18/233]	Loss 0.2989 (0.3253)	
training:	Epoch: [10][19/233]	Loss 0.2147 (0.3195)	
training:	Epoch: [10][20/233]	Loss 0.2649 (0.3168)	
training:	Epoch: [10][21/233]	Loss 0.5705 (0.3288)	
training:	Epoch: [10][22/233]	Loss 0.2066 (0.3233)	
training:	Epoch: [10][23/233]	Loss 0.4279 (0.3278)	
training:	Epoch: [10][24/233]	Loss 0.4738 (0.3339)	
training:	Epoch: [10][25/233]	Loss 0.2991 (0.3325)	
training:	Epoch: [10][26/233]	Loss 0.3242 (0.3322)	
training:	Epoch: [10][27/233]	Loss 0.2763 (0.3301)	
training:	Epoch: [10][28/233]	Loss 0.2459 (0.3271)	
training:	Epoch: [10][29/233]	Loss 0.3253 (0.3271)	
training:	Epoch: [10][30/233]	Loss 0.2436 (0.3243)	
training:	Epoch: [10][31/233]	Loss 0.1488 (0.3186)	
training:	Epoch: [10][32/233]	Loss 0.2784 (0.3174)	
training:	Epoch: [10][33/233]	Loss 0.3438 (0.3182)	
training:	Epoch: [10][34/233]	Loss 0.3713 (0.3197)	
training:	Epoch: [10][35/233]	Loss 0.2048 (0.3164)	
training:	Epoch: [10][36/233]	Loss 0.3693 (0.3179)	
training:	Epoch: [10][37/233]	Loss 0.5403 (0.3239)	
training:	Epoch: [10][38/233]	Loss 0.2029 (0.3207)	
training:	Epoch: [10][39/233]	Loss 0.3098 (0.3205)	
training:	Epoch: [10][40/233]	Loss 0.4001 (0.3224)	
training:	Epoch: [10][41/233]	Loss 0.2265 (0.3201)	
training:	Epoch: [10][42/233]	Loss 0.5660 (0.3260)	
training:	Epoch: [10][43/233]	Loss 0.3942 (0.3275)	
training:	Epoch: [10][44/233]	Loss 0.3155 (0.3273)	
training:	Epoch: [10][45/233]	Loss 0.4034 (0.3290)	
training:	Epoch: [10][46/233]	Loss 0.2024 (0.3262)	
training:	Epoch: [10][47/233]	Loss 0.1329 (0.3221)	
training:	Epoch: [10][48/233]	Loss 0.3563 (0.3228)	
training:	Epoch: [10][49/233]	Loss 0.2685 (0.3217)	
training:	Epoch: [10][50/233]	Loss 0.3676 (0.3226)	
training:	Epoch: [10][51/233]	Loss 0.2343 (0.3209)	
training:	Epoch: [10][52/233]	Loss 0.3165 (0.3208)	
training:	Epoch: [10][53/233]	Loss 0.2764 (0.3200)	
training:	Epoch: [10][54/233]	Loss 0.2216 (0.3181)	
training:	Epoch: [10][55/233]	Loss 0.4427 (0.3204)	
training:	Epoch: [10][56/233]	Loss 0.4676 (0.3230)	
training:	Epoch: [10][57/233]	Loss 0.2402 (0.3216)	
training:	Epoch: [10][58/233]	Loss 0.2005 (0.3195)	
training:	Epoch: [10][59/233]	Loss 0.2827 (0.3189)	
training:	Epoch: [10][60/233]	Loss 0.2092 (0.3170)	
training:	Epoch: [10][61/233]	Loss 0.1643 (0.3145)	
training:	Epoch: [10][62/233]	Loss 0.2388 (0.3133)	
training:	Epoch: [10][63/233]	Loss 0.4090 (0.3148)	
training:	Epoch: [10][64/233]	Loss 0.3477 (0.3154)	
training:	Epoch: [10][65/233]	Loss 0.3040 (0.3152)	
training:	Epoch: [10][66/233]	Loss 0.2412 (0.3141)	
training:	Epoch: [10][67/233]	Loss 0.1589 (0.3117)	
training:	Epoch: [10][68/233]	Loss 0.2950 (0.3115)	
training:	Epoch: [10][69/233]	Loss 0.2779 (0.3110)	
training:	Epoch: [10][70/233]	Loss 0.1616 (0.3089)	
training:	Epoch: [10][71/233]	Loss 0.3010 (0.3088)	
training:	Epoch: [10][72/233]	Loss 0.4168 (0.3103)	
training:	Epoch: [10][73/233]	Loss 0.4547 (0.3122)	
training:	Epoch: [10][74/233]	Loss 0.2812 (0.3118)	
training:	Epoch: [10][75/233]	Loss 0.2284 (0.3107)	
training:	Epoch: [10][76/233]	Loss 0.2767 (0.3103)	
training:	Epoch: [10][77/233]	Loss 0.2882 (0.3100)	
training:	Epoch: [10][78/233]	Loss 0.4652 (0.3120)	
training:	Epoch: [10][79/233]	Loss 0.2887 (0.3117)	
training:	Epoch: [10][80/233]	Loss 0.2594 (0.3110)	
training:	Epoch: [10][81/233]	Loss 0.3536 (0.3115)	
training:	Epoch: [10][82/233]	Loss 0.2648 (0.3110)	
training:	Epoch: [10][83/233]	Loss 0.2968 (0.3108)	
training:	Epoch: [10][84/233]	Loss 0.3441 (0.3112)	
training:	Epoch: [10][85/233]	Loss 0.3491 (0.3116)	
training:	Epoch: [10][86/233]	Loss 0.2716 (0.3112)	
training:	Epoch: [10][87/233]	Loss 0.1552 (0.3094)	
training:	Epoch: [10][88/233]	Loss 0.2622 (0.3088)	
training:	Epoch: [10][89/233]	Loss 0.1549 (0.3071)	
training:	Epoch: [10][90/233]	Loss 0.4022 (0.3082)	
training:	Epoch: [10][91/233]	Loss 0.3329 (0.3084)	
training:	Epoch: [10][92/233]	Loss 0.3974 (0.3094)	
training:	Epoch: [10][93/233]	Loss 0.4207 (0.3106)	
training:	Epoch: [10][94/233]	Loss 0.2879 (0.3104)	
training:	Epoch: [10][95/233]	Loss 0.2751 (0.3100)	
training:	Epoch: [10][96/233]	Loss 0.1073 (0.3079)	
training:	Epoch: [10][97/233]	Loss 0.2641 (0.3074)	
training:	Epoch: [10][98/233]	Loss 0.3507 (0.3079)	
training:	Epoch: [10][99/233]	Loss 0.2331 (0.3071)	
training:	Epoch: [10][100/233]	Loss 0.3308 (0.3074)	
training:	Epoch: [10][101/233]	Loss 0.1855 (0.3062)	
training:	Epoch: [10][102/233]	Loss 0.2453 (0.3056)	
training:	Epoch: [10][103/233]	Loss 0.2515 (0.3050)	
training:	Epoch: [10][104/233]	Loss 0.1599 (0.3036)	
training:	Epoch: [10][105/233]	Loss 0.2395 (0.3030)	
training:	Epoch: [10][106/233]	Loss 0.1938 (0.3020)	
training:	Epoch: [10][107/233]	Loss 0.2692 (0.3017)	
training:	Epoch: [10][108/233]	Loss 0.2826 (0.3015)	
training:	Epoch: [10][109/233]	Loss 0.3634 (0.3021)	
training:	Epoch: [10][110/233]	Loss 0.2124 (0.3013)	
training:	Epoch: [10][111/233]	Loss 0.2472 (0.3008)	
training:	Epoch: [10][112/233]	Loss 0.2474 (0.3003)	
training:	Epoch: [10][113/233]	Loss 0.3900 (0.3011)	
training:	Epoch: [10][114/233]	Loss 0.3455 (0.3015)	
training:	Epoch: [10][115/233]	Loss 0.2690 (0.3012)	
training:	Epoch: [10][116/233]	Loss 0.1830 (0.3002)	
training:	Epoch: [10][117/233]	Loss 0.3123 (0.3003)	
training:	Epoch: [10][118/233]	Loss 0.2221 (0.2996)	
training:	Epoch: [10][119/233]	Loss 0.4165 (0.3006)	
training:	Epoch: [10][120/233]	Loss 0.4404 (0.3018)	
training:	Epoch: [10][121/233]	Loss 0.3152 (0.3019)	
training:	Epoch: [10][122/233]	Loss 0.3454 (0.3022)	
training:	Epoch: [10][123/233]	Loss 0.2251 (0.3016)	
training:	Epoch: [10][124/233]	Loss 0.2264 (0.3010)	
training:	Epoch: [10][125/233]	Loss 0.3629 (0.3015)	
training:	Epoch: [10][126/233]	Loss 0.3289 (0.3017)	
training:	Epoch: [10][127/233]	Loss 0.3031 (0.3017)	
training:	Epoch: [10][128/233]	Loss 0.3779 (0.3023)	
training:	Epoch: [10][129/233]	Loss 0.2652 (0.3020)	
training:	Epoch: [10][130/233]	Loss 0.3121 (0.3021)	
training:	Epoch: [10][131/233]	Loss 0.1852 (0.3012)	
training:	Epoch: [10][132/233]	Loss 0.3303 (0.3014)	
training:	Epoch: [10][133/233]	Loss 0.2935 (0.3014)	
training:	Epoch: [10][134/233]	Loss 0.2905 (0.3013)	
training:	Epoch: [10][135/233]	Loss 0.4415 (0.3023)	
training:	Epoch: [10][136/233]	Loss 0.3075 (0.3024)	
training:	Epoch: [10][137/233]	Loss 0.4536 (0.3035)	
training:	Epoch: [10][138/233]	Loss 0.2581 (0.3032)	
training:	Epoch: [10][139/233]	Loss 0.2334 (0.3026)	
training:	Epoch: [10][140/233]	Loss 0.3632 (0.3031)	
training:	Epoch: [10][141/233]	Loss 0.5187 (0.3046)	
training:	Epoch: [10][142/233]	Loss 0.3686 (0.3051)	
training:	Epoch: [10][143/233]	Loss 0.2307 (0.3045)	
training:	Epoch: [10][144/233]	Loss 0.2382 (0.3041)	
training:	Epoch: [10][145/233]	Loss 0.4523 (0.3051)	
training:	Epoch: [10][146/233]	Loss 0.1665 (0.3042)	
training:	Epoch: [10][147/233]	Loss 0.4606 (0.3052)	
training:	Epoch: [10][148/233]	Loss 0.3443 (0.3055)	
training:	Epoch: [10][149/233]	Loss 0.2826 (0.3053)	
training:	Epoch: [10][150/233]	Loss 0.3504 (0.3056)	
training:	Epoch: [10][151/233]	Loss 0.3063 (0.3056)	
training:	Epoch: [10][152/233]	Loss 0.4078 (0.3063)	
training:	Epoch: [10][153/233]	Loss 0.3859 (0.3068)	
training:	Epoch: [10][154/233]	Loss 0.3088 (0.3068)	
training:	Epoch: [10][155/233]	Loss 0.3480 (0.3071)	
training:	Epoch: [10][156/233]	Loss 0.1851 (0.3063)	
training:	Epoch: [10][157/233]	Loss 0.2424 (0.3059)	
training:	Epoch: [10][158/233]	Loss 0.6141 (0.3079)	
training:	Epoch: [10][159/233]	Loss 0.4669 (0.3089)	
training:	Epoch: [10][160/233]	Loss 0.3529 (0.3091)	
training:	Epoch: [10][161/233]	Loss 0.2839 (0.3090)	
training:	Epoch: [10][162/233]	Loss 0.1455 (0.3080)	
training:	Epoch: [10][163/233]	Loss 0.3376 (0.3082)	
training:	Epoch: [10][164/233]	Loss 0.3250 (0.3083)	
training:	Epoch: [10][165/233]	Loss 0.3023 (0.3082)	
training:	Epoch: [10][166/233]	Loss 0.2060 (0.3076)	
training:	Epoch: [10][167/233]	Loss 0.1735 (0.3068)	
training:	Epoch: [10][168/233]	Loss 0.3786 (0.3072)	
training:	Epoch: [10][169/233]	Loss 0.3545 (0.3075)	
training:	Epoch: [10][170/233]	Loss 0.2901 (0.3074)	
training:	Epoch: [10][171/233]	Loss 0.2654 (0.3072)	
training:	Epoch: [10][172/233]	Loss 0.2371 (0.3068)	
training:	Epoch: [10][173/233]	Loss 0.4893 (0.3078)	
training:	Epoch: [10][174/233]	Loss 0.2480 (0.3075)	
training:	Epoch: [10][175/233]	Loss 0.5152 (0.3087)	
training:	Epoch: [10][176/233]	Loss 0.5162 (0.3098)	
training:	Epoch: [10][177/233]	Loss 0.2278 (0.3094)	
training:	Epoch: [10][178/233]	Loss 0.5323 (0.3106)	
training:	Epoch: [10][179/233]	Loss 0.1758 (0.3099)	
training:	Epoch: [10][180/233]	Loss 0.3079 (0.3099)	
training:	Epoch: [10][181/233]	Loss 0.1496 (0.3090)	
training:	Epoch: [10][182/233]	Loss 0.4305 (0.3096)	
training:	Epoch: [10][183/233]	Loss 0.3497 (0.3099)	
training:	Epoch: [10][184/233]	Loss 0.2887 (0.3097)	
training:	Epoch: [10][185/233]	Loss 0.2591 (0.3095)	
training:	Epoch: [10][186/233]	Loss 0.2593 (0.3092)	
training:	Epoch: [10][187/233]	Loss 0.2652 (0.3090)	
training:	Epoch: [10][188/233]	Loss 0.3949 (0.3094)	
training:	Epoch: [10][189/233]	Loss 0.5245 (0.3106)	
training:	Epoch: [10][190/233]	Loss 0.2392 (0.3102)	
training:	Epoch: [10][191/233]	Loss 0.3452 (0.3104)	
training:	Epoch: [10][192/233]	Loss 0.3722 (0.3107)	
training:	Epoch: [10][193/233]	Loss 0.4473 (0.3114)	
training:	Epoch: [10][194/233]	Loss 0.2527 (0.3111)	
training:	Epoch: [10][195/233]	Loss 0.3978 (0.3115)	
training:	Epoch: [10][196/233]	Loss 0.2450 (0.3112)	
training:	Epoch: [10][197/233]	Loss 0.2698 (0.3110)	
training:	Epoch: [10][198/233]	Loss 0.3310 (0.3111)	
training:	Epoch: [10][199/233]	Loss 0.2020 (0.3105)	
training:	Epoch: [10][200/233]	Loss 0.1973 (0.3100)	
training:	Epoch: [10][201/233]	Loss 0.4592 (0.3107)	
training:	Epoch: [10][202/233]	Loss 0.4257 (0.3113)	
training:	Epoch: [10][203/233]	Loss 0.2877 (0.3112)	
training:	Epoch: [10][204/233]	Loss 0.3381 (0.3113)	
training:	Epoch: [10][205/233]	Loss 0.3524 (0.3115)	
training:	Epoch: [10][206/233]	Loss 0.4263 (0.3121)	
training:	Epoch: [10][207/233]	Loss 0.4029 (0.3125)	
training:	Epoch: [10][208/233]	Loss 0.2677 (0.3123)	
training:	Epoch: [10][209/233]	Loss 0.5571 (0.3135)	
training:	Epoch: [10][210/233]	Loss 0.4349 (0.3140)	
training:	Epoch: [10][211/233]	Loss 0.2388 (0.3137)	
training:	Epoch: [10][212/233]	Loss 0.3719 (0.3140)	
training:	Epoch: [10][213/233]	Loss 0.2749 (0.3138)	
training:	Epoch: [10][214/233]	Loss 0.3147 (0.3138)	
training:	Epoch: [10][215/233]	Loss 0.2971 (0.3137)	
training:	Epoch: [10][216/233]	Loss 0.3019 (0.3136)	
training:	Epoch: [10][217/233]	Loss 0.2451 (0.3133)	
training:	Epoch: [10][218/233]	Loss 0.2706 (0.3131)	
training:	Epoch: [10][219/233]	Loss 0.3518 (0.3133)	
training:	Epoch: [10][220/233]	Loss 0.3778 (0.3136)	
training:	Epoch: [10][221/233]	Loss 0.3971 (0.3140)	
training:	Epoch: [10][222/233]	Loss 0.3682 (0.3142)	
training:	Epoch: [10][223/233]	Loss 0.2322 (0.3139)	
training:	Epoch: [10][224/233]	Loss 0.2031 (0.3134)	
training:	Epoch: [10][225/233]	Loss 0.3423 (0.3135)	
training:	Epoch: [10][226/233]	Loss 0.3755 (0.3138)	
training:	Epoch: [10][227/233]	Loss 0.3147 (0.3138)	
training:	Epoch: [10][228/233]	Loss 0.3115 (0.3138)	
training:	Epoch: [10][229/233]	Loss 0.2870 (0.3136)	
training:	Epoch: [10][230/233]	Loss 0.1889 (0.3131)	
training:	Epoch: [10][231/233]	Loss 0.1577 (0.3124)	
training:	Epoch: [10][232/233]	Loss 0.4103 (0.3128)	
training:	Epoch: [10][233/233]	Loss 0.3633 (0.3131)	
Training:	 Loss: 0.3124

Training:	 ACC: 0.8911 0.8896 0.8562 0.9261
Validation:	 ACC: 0.8103 0.8085 0.7722 0.8483
Validation:	 Best_BACC: 0.8162 0.8154 0.7998 0.8326
Validation:	 Loss: 0.4277
Pretraining:	Epoch 11/200
----------
training:	Epoch: [11][1/233]	Loss 0.4580 (0.4580)	
training:	Epoch: [11][2/233]	Loss 0.1591 (0.3085)	
training:	Epoch: [11][3/233]	Loss 0.3021 (0.3064)	
training:	Epoch: [11][4/233]	Loss 0.2632 (0.2956)	
training:	Epoch: [11][5/233]	Loss 0.1980 (0.2761)	
training:	Epoch: [11][6/233]	Loss 0.4635 (0.3073)	
training:	Epoch: [11][7/233]	Loss 0.2925 (0.3052)	
training:	Epoch: [11][8/233]	Loss 0.2978 (0.3043)	
training:	Epoch: [11][9/233]	Loss 0.2168 (0.2946)	
training:	Epoch: [11][10/233]	Loss 0.1881 (0.2839)	
training:	Epoch: [11][11/233]	Loss 0.2822 (0.2838)	
training:	Epoch: [11][12/233]	Loss 0.3612 (0.2902)	
training:	Epoch: [11][13/233]	Loss 0.3978 (0.2985)	
training:	Epoch: [11][14/233]	Loss 0.2338 (0.2939)	
training:	Epoch: [11][15/233]	Loss 0.6490 (0.3175)	
training:	Epoch: [11][16/233]	Loss 0.3504 (0.3196)	
training:	Epoch: [11][17/233]	Loss 0.2055 (0.3129)	
training:	Epoch: [11][18/233]	Loss 0.2259 (0.3081)	
training:	Epoch: [11][19/233]	Loss 0.5149 (0.3189)	
training:	Epoch: [11][20/233]	Loss 0.2347 (0.3147)	
training:	Epoch: [11][21/233]	Loss 0.2093 (0.3097)	
training:	Epoch: [11][22/233]	Loss 0.4525 (0.3162)	
training:	Epoch: [11][23/233]	Loss 0.2460 (0.3131)	
training:	Epoch: [11][24/233]	Loss 0.2481 (0.3104)	
training:	Epoch: [11][25/233]	Loss 0.3514 (0.3121)	
training:	Epoch: [11][26/233]	Loss 0.3202 (0.3124)	
training:	Epoch: [11][27/233]	Loss 0.4145 (0.3162)	
training:	Epoch: [11][28/233]	Loss 0.3211 (0.3163)	
training:	Epoch: [11][29/233]	Loss 0.1491 (0.3106)	
training:	Epoch: [11][30/233]	Loss 0.2994 (0.3102)	
training:	Epoch: [11][31/233]	Loss 0.3421 (0.3112)	
training:	Epoch: [11][32/233]	Loss 0.3402 (0.3121)	
training:	Epoch: [11][33/233]	Loss 0.4821 (0.3173)	
training:	Epoch: [11][34/233]	Loss 0.2815 (0.3162)	
training:	Epoch: [11][35/233]	Loss 0.2042 (0.3130)	
training:	Epoch: [11][36/233]	Loss 0.3427 (0.3139)	
training:	Epoch: [11][37/233]	Loss 0.2903 (0.3132)	
training:	Epoch: [11][38/233]	Loss 0.1867 (0.3099)	
training:	Epoch: [11][39/233]	Loss 0.2348 (0.3080)	
training:	Epoch: [11][40/233]	Loss 0.2287 (0.3060)	
training:	Epoch: [11][41/233]	Loss 0.1372 (0.3019)	
training:	Epoch: [11][42/233]	Loss 0.2103 (0.2997)	
training:	Epoch: [11][43/233]	Loss 0.3117 (0.3000)	
training:	Epoch: [11][44/233]	Loss 0.2153 (0.2980)	
training:	Epoch: [11][45/233]	Loss 0.2916 (0.2979)	
training:	Epoch: [11][46/233]	Loss 0.2595 (0.2971)	
training:	Epoch: [11][47/233]	Loss 0.4226 (0.2997)	
training:	Epoch: [11][48/233]	Loss 0.3149 (0.3001)	
training:	Epoch: [11][49/233]	Loss 0.3467 (0.3010)	
training:	Epoch: [11][50/233]	Loss 0.4211 (0.3034)	
training:	Epoch: [11][51/233]	Loss 0.4262 (0.3058)	
training:	Epoch: [11][52/233]	Loss 0.2895 (0.3055)	
training:	Epoch: [11][53/233]	Loss 0.2032 (0.3036)	
training:	Epoch: [11][54/233]	Loss 0.3363 (0.3042)	
training:	Epoch: [11][55/233]	Loss 0.2672 (0.3035)	
training:	Epoch: [11][56/233]	Loss 0.4829 (0.3067)	
training:	Epoch: [11][57/233]	Loss 0.5552 (0.3111)	
training:	Epoch: [11][58/233]	Loss 0.2246 (0.3096)	
training:	Epoch: [11][59/233]	Loss 0.2796 (0.3091)	
training:	Epoch: [11][60/233]	Loss 0.3226 (0.3093)	
training:	Epoch: [11][61/233]	Loss 0.2092 (0.3077)	
training:	Epoch: [11][62/233]	Loss 0.2207 (0.3063)	
training:	Epoch: [11][63/233]	Loss 0.1941 (0.3045)	
training:	Epoch: [11][64/233]	Loss 0.3510 (0.3052)	
training:	Epoch: [11][65/233]	Loss 0.5239 (0.3086)	
training:	Epoch: [11][66/233]	Loss 0.3937 (0.3099)	
training:	Epoch: [11][67/233]	Loss 0.2138 (0.3084)	
training:	Epoch: [11][68/233]	Loss 0.1691 (0.3064)	
training:	Epoch: [11][69/233]	Loss 0.3506 (0.3070)	
training:	Epoch: [11][70/233]	Loss 0.3296 (0.3073)	
training:	Epoch: [11][71/233]	Loss 0.2952 (0.3072)	
training:	Epoch: [11][72/233]	Loss 0.3436 (0.3077)	
training:	Epoch: [11][73/233]	Loss 0.4694 (0.3099)	
training:	Epoch: [11][74/233]	Loss 0.3251 (0.3101)	
training:	Epoch: [11][75/233]	Loss 0.3340 (0.3104)	
training:	Epoch: [11][76/233]	Loss 0.4684 (0.3125)	
training:	Epoch: [11][77/233]	Loss 0.2745 (0.3120)	
training:	Epoch: [11][78/233]	Loss 0.3805 (0.3129)	
training:	Epoch: [11][79/233]	Loss 0.2057 (0.3115)	
training:	Epoch: [11][80/233]	Loss 0.3405 (0.3119)	
training:	Epoch: [11][81/233]	Loss 0.3770 (0.3127)	
training:	Epoch: [11][82/233]	Loss 0.2898 (0.3124)	
training:	Epoch: [11][83/233]	Loss 0.3202 (0.3125)	
training:	Epoch: [11][84/233]	Loss 0.2436 (0.3117)	
training:	Epoch: [11][85/233]	Loss 0.2636 (0.3111)	
training:	Epoch: [11][86/233]	Loss 0.3461 (0.3115)	
training:	Epoch: [11][87/233]	Loss 0.2277 (0.3106)	
training:	Epoch: [11][88/233]	Loss 0.3686 (0.3112)	
training:	Epoch: [11][89/233]	Loss 0.3937 (0.3121)	
training:	Epoch: [11][90/233]	Loss 0.1607 (0.3105)	
training:	Epoch: [11][91/233]	Loss 0.3389 (0.3108)	
training:	Epoch: [11][92/233]	Loss 0.2552 (0.3102)	
training:	Epoch: [11][93/233]	Loss 0.2792 (0.3098)	
training:	Epoch: [11][94/233]	Loss 0.1842 (0.3085)	
training:	Epoch: [11][95/233]	Loss 0.3035 (0.3084)	
training:	Epoch: [11][96/233]	Loss 0.2840 (0.3082)	
training:	Epoch: [11][97/233]	Loss 0.2300 (0.3074)	
training:	Epoch: [11][98/233]	Loss 0.2656 (0.3070)	
training:	Epoch: [11][99/233]	Loss 0.2919 (0.3068)	
training:	Epoch: [11][100/233]	Loss 0.1904 (0.3056)	
training:	Epoch: [11][101/233]	Loss 0.3694 (0.3063)	
training:	Epoch: [11][102/233]	Loss 0.2879 (0.3061)	
training:	Epoch: [11][103/233]	Loss 0.4811 (0.3078)	
training:	Epoch: [11][104/233]	Loss 0.5711 (0.3103)	
training:	Epoch: [11][105/233]	Loss 0.1840 (0.3091)	
training:	Epoch: [11][106/233]	Loss 0.3211 (0.3092)	
training:	Epoch: [11][107/233]	Loss 0.3000 (0.3091)	
training:	Epoch: [11][108/233]	Loss 0.3069 (0.3091)	
training:	Epoch: [11][109/233]	Loss 0.3201 (0.3092)	
training:	Epoch: [11][110/233]	Loss 0.2785 (0.3089)	
training:	Epoch: [11][111/233]	Loss 0.3167 (0.3090)	
training:	Epoch: [11][112/233]	Loss 0.2211 (0.3082)	
training:	Epoch: [11][113/233]	Loss 0.4077 (0.3091)	
training:	Epoch: [11][114/233]	Loss 0.2730 (0.3088)	
training:	Epoch: [11][115/233]	Loss 0.2487 (0.3083)	
training:	Epoch: [11][116/233]	Loss 0.4270 (0.3093)	
training:	Epoch: [11][117/233]	Loss 0.3452 (0.3096)	
training:	Epoch: [11][118/233]	Loss 0.3031 (0.3095)	
training:	Epoch: [11][119/233]	Loss 0.3392 (0.3098)	
training:	Epoch: [11][120/233]	Loss 0.3486 (0.3101)	
training:	Epoch: [11][121/233]	Loss 0.1893 (0.3091)	
training:	Epoch: [11][122/233]	Loss 0.4556 (0.3103)	
training:	Epoch: [11][123/233]	Loss 0.5818 (0.3125)	
training:	Epoch: [11][124/233]	Loss 0.3513 (0.3128)	
training:	Epoch: [11][125/233]	Loss 0.2775 (0.3126)	
training:	Epoch: [11][126/233]	Loss 0.1489 (0.3113)	
training:	Epoch: [11][127/233]	Loss 0.3665 (0.3117)	
training:	Epoch: [11][128/233]	Loss 0.2119 (0.3109)	
training:	Epoch: [11][129/233]	Loss 0.5111 (0.3125)	
training:	Epoch: [11][130/233]	Loss 0.2072 (0.3117)	
training:	Epoch: [11][131/233]	Loss 0.2558 (0.3112)	
training:	Epoch: [11][132/233]	Loss 0.3180 (0.3113)	
training:	Epoch: [11][133/233]	Loss 0.4747 (0.3125)	
training:	Epoch: [11][134/233]	Loss 0.2093 (0.3117)	
training:	Epoch: [11][135/233]	Loss 0.3438 (0.3120)	
training:	Epoch: [11][136/233]	Loss 0.3052 (0.3119)	
training:	Epoch: [11][137/233]	Loss 0.1742 (0.3109)	
training:	Epoch: [11][138/233]	Loss 0.4361 (0.3118)	
training:	Epoch: [11][139/233]	Loss 0.2392 (0.3113)	
training:	Epoch: [11][140/233]	Loss 0.2046 (0.3105)	
training:	Epoch: [11][141/233]	Loss 0.3641 (0.3109)	
training:	Epoch: [11][142/233]	Loss 0.3413 (0.3111)	
training:	Epoch: [11][143/233]	Loss 0.3420 (0.3114)	
training:	Epoch: [11][144/233]	Loss 0.4043 (0.3120)	
training:	Epoch: [11][145/233]	Loss 0.1590 (0.3109)	
training:	Epoch: [11][146/233]	Loss 0.3874 (0.3115)	
training:	Epoch: [11][147/233]	Loss 0.2125 (0.3108)	
training:	Epoch: [11][148/233]	Loss 0.2801 (0.3106)	
training:	Epoch: [11][149/233]	Loss 0.1899 (0.3098)	
training:	Epoch: [11][150/233]	Loss 0.3079 (0.3098)	
training:	Epoch: [11][151/233]	Loss 0.2466 (0.3093)	
training:	Epoch: [11][152/233]	Loss 0.3504 (0.3096)	
training:	Epoch: [11][153/233]	Loss 0.3522 (0.3099)	
training:	Epoch: [11][154/233]	Loss 0.3309 (0.3100)	
training:	Epoch: [11][155/233]	Loss 0.2310 (0.3095)	
training:	Epoch: [11][156/233]	Loss 0.2431 (0.3091)	
training:	Epoch: [11][157/233]	Loss 0.1995 (0.3084)	
training:	Epoch: [11][158/233]	Loss 0.4488 (0.3093)	
training:	Epoch: [11][159/233]	Loss 0.3687 (0.3097)	
training:	Epoch: [11][160/233]	Loss 0.2490 (0.3093)	
training:	Epoch: [11][161/233]	Loss 0.3188 (0.3093)	
training:	Epoch: [11][162/233]	Loss 0.2006 (0.3087)	
training:	Epoch: [11][163/233]	Loss 0.2997 (0.3086)	
training:	Epoch: [11][164/233]	Loss 0.1282 (0.3075)	
training:	Epoch: [11][165/233]	Loss 0.3593 (0.3078)	
training:	Epoch: [11][166/233]	Loss 0.0921 (0.3065)	
training:	Epoch: [11][167/233]	Loss 0.3419 (0.3067)	
training:	Epoch: [11][168/233]	Loss 0.5615 (0.3083)	
training:	Epoch: [11][169/233]	Loss 0.3863 (0.3087)	
training:	Epoch: [11][170/233]	Loss 0.3089 (0.3087)	
training:	Epoch: [11][171/233]	Loss 0.2923 (0.3086)	
training:	Epoch: [11][172/233]	Loss 0.3262 (0.3087)	
training:	Epoch: [11][173/233]	Loss 0.3084 (0.3087)	
training:	Epoch: [11][174/233]	Loss 0.2021 (0.3081)	
training:	Epoch: [11][175/233]	Loss 0.1255 (0.3071)	
training:	Epoch: [11][176/233]	Loss 0.2479 (0.3067)	
training:	Epoch: [11][177/233]	Loss 0.3207 (0.3068)	
training:	Epoch: [11][178/233]	Loss 0.3363 (0.3070)	
training:	Epoch: [11][179/233]	Loss 0.2423 (0.3066)	
training:	Epoch: [11][180/233]	Loss 0.2308 (0.3062)	
training:	Epoch: [11][181/233]	Loss 0.2689 (0.3060)	
training:	Epoch: [11][182/233]	Loss 0.2268 (0.3056)	
training:	Epoch: [11][183/233]	Loss 0.4032 (0.3061)	
training:	Epoch: [11][184/233]	Loss 0.4288 (0.3068)	
training:	Epoch: [11][185/233]	Loss 0.1236 (0.3058)	
training:	Epoch: [11][186/233]	Loss 0.1951 (0.3052)	
training:	Epoch: [11][187/233]	Loss 0.4167 (0.3058)	
training:	Epoch: [11][188/233]	Loss 0.1972 (0.3052)	
training:	Epoch: [11][189/233]	Loss 0.3555 (0.3055)	
training:	Epoch: [11][190/233]	Loss 0.5338 (0.3067)	
training:	Epoch: [11][191/233]	Loss 0.3718 (0.3070)	
training:	Epoch: [11][192/233]	Loss 0.4627 (0.3078)	
training:	Epoch: [11][193/233]	Loss 0.3498 (0.3080)	
training:	Epoch: [11][194/233]	Loss 0.3883 (0.3084)	
training:	Epoch: [11][195/233]	Loss 0.3967 (0.3089)	
training:	Epoch: [11][196/233]	Loss 0.4749 (0.3097)	
training:	Epoch: [11][197/233]	Loss 0.2291 (0.3093)	
training:	Epoch: [11][198/233]	Loss 0.4460 (0.3100)	
training:	Epoch: [11][199/233]	Loss 0.4235 (0.3106)	
training:	Epoch: [11][200/233]	Loss 0.2930 (0.3105)	
training:	Epoch: [11][201/233]	Loss 0.2789 (0.3103)	
training:	Epoch: [11][202/233]	Loss 0.2790 (0.3102)	
training:	Epoch: [11][203/233]	Loss 0.3145 (0.3102)	
training:	Epoch: [11][204/233]	Loss 0.3746 (0.3105)	
training:	Epoch: [11][205/233]	Loss 0.3758 (0.3108)	
training:	Epoch: [11][206/233]	Loss 0.2588 (0.3106)	
training:	Epoch: [11][207/233]	Loss 0.3407 (0.3107)	
training:	Epoch: [11][208/233]	Loss 0.2024 (0.3102)	
training:	Epoch: [11][209/233]	Loss 0.2498 (0.3099)	
training:	Epoch: [11][210/233]	Loss 0.2444 (0.3096)	
training:	Epoch: [11][211/233]	Loss 0.5246 (0.3106)	
training:	Epoch: [11][212/233]	Loss 0.4048 (0.3111)	
training:	Epoch: [11][213/233]	Loss 0.5031 (0.3120)	
training:	Epoch: [11][214/233]	Loss 0.2741 (0.3118)	
training:	Epoch: [11][215/233]	Loss 0.1946 (0.3113)	
training:	Epoch: [11][216/233]	Loss 0.1902 (0.3107)	
training:	Epoch: [11][217/233]	Loss 0.2251 (0.3103)	
training:	Epoch: [11][218/233]	Loss 0.3456 (0.3105)	
training:	Epoch: [11][219/233]	Loss 0.3969 (0.3109)	
training:	Epoch: [11][220/233]	Loss 0.1969 (0.3103)	
training:	Epoch: [11][221/233]	Loss 0.1551 (0.3096)	
training:	Epoch: [11][222/233]	Loss 0.5160 (0.3106)	
training:	Epoch: [11][223/233]	Loss 0.2886 (0.3105)	
training:	Epoch: [11][224/233]	Loss 0.2580 (0.3102)	
training:	Epoch: [11][225/233]	Loss 0.3602 (0.3105)	
training:	Epoch: [11][226/233]	Loss 0.2409 (0.3102)	
training:	Epoch: [11][227/233]	Loss 0.3735 (0.3104)	
training:	Epoch: [11][228/233]	Loss 0.2696 (0.3103)	
training:	Epoch: [11][229/233]	Loss 0.3175 (0.3103)	
training:	Epoch: [11][230/233]	Loss 0.2645 (0.3101)	
training:	Epoch: [11][231/233]	Loss 0.4110 (0.3105)	
training:	Epoch: [11][232/233]	Loss 0.2365 (0.3102)	
training:	Epoch: [11][233/233]	Loss 0.2480 (0.3099)	
Training:	 Loss: 0.3092

Training:	 ACC: 0.8835 0.8804 0.8128 0.9541
Validation:	 ACC: 0.8043 0.8004 0.7232 0.8854
Validation:	 Best_BACC: 0.8162 0.8154 0.7998 0.8326
Validation:	 Loss: 0.4493
Pretraining:	Epoch 12/200
----------
training:	Epoch: [12][1/233]	Loss 0.2498 (0.2498)	
training:	Epoch: [12][2/233]	Loss 0.2775 (0.2637)	
training:	Epoch: [12][3/233]	Loss 0.2986 (0.2753)	
training:	Epoch: [12][4/233]	Loss 0.3765 (0.3006)	
training:	Epoch: [12][5/233]	Loss 0.2889 (0.2983)	
training:	Epoch: [12][6/233]	Loss 0.1524 (0.2740)	
training:	Epoch: [12][7/233]	Loss 0.1567 (0.2572)	
training:	Epoch: [12][8/233]	Loss 0.2592 (0.2574)	
training:	Epoch: [12][9/233]	Loss 0.1964 (0.2507)	
training:	Epoch: [12][10/233]	Loss 0.4270 (0.2683)	
training:	Epoch: [12][11/233]	Loss 0.2055 (0.2626)	
training:	Epoch: [12][12/233]	Loss 0.2728 (0.2634)	
training:	Epoch: [12][13/233]	Loss 0.1396 (0.2539)	
training:	Epoch: [12][14/233]	Loss 0.1928 (0.2495)	
training:	Epoch: [12][15/233]	Loss 0.1836 (0.2452)	
training:	Epoch: [12][16/233]	Loss 0.1863 (0.2415)	
training:	Epoch: [12][17/233]	Loss 0.4537 (0.2540)	
training:	Epoch: [12][18/233]	Loss 0.2849 (0.2557)	
training:	Epoch: [12][19/233]	Loss 0.2367 (0.2547)	
training:	Epoch: [12][20/233]	Loss 0.1376 (0.2488)	
training:	Epoch: [12][21/233]	Loss 0.2100 (0.2470)	
training:	Epoch: [12][22/233]	Loss 0.2145 (0.2455)	
training:	Epoch: [12][23/233]	Loss 0.3196 (0.2487)	
training:	Epoch: [12][24/233]	Loss 0.1834 (0.2460)	
training:	Epoch: [12][25/233]	Loss 0.2261 (0.2452)	
training:	Epoch: [12][26/233]	Loss 0.2715 (0.2462)	
training:	Epoch: [12][27/233]	Loss 0.4669 (0.2544)	
training:	Epoch: [12][28/233]	Loss 0.2187 (0.2531)	
training:	Epoch: [12][29/233]	Loss 0.3102 (0.2551)	
training:	Epoch: [12][30/233]	Loss 0.2640 (0.2554)	
training:	Epoch: [12][31/233]	Loss 0.1778 (0.2529)	
training:	Epoch: [12][32/233]	Loss 0.4717 (0.2597)	
training:	Epoch: [12][33/233]	Loss 0.3064 (0.2611)	
training:	Epoch: [12][34/233]	Loss 0.2477 (0.2607)	
training:	Epoch: [12][35/233]	Loss 0.2287 (0.2598)	
training:	Epoch: [12][36/233]	Loss 0.3244 (0.2616)	
training:	Epoch: [12][37/233]	Loss 0.2290 (0.2607)	
training:	Epoch: [12][38/233]	Loss 0.1933 (0.2590)	
training:	Epoch: [12][39/233]	Loss 0.2787 (0.2595)	
training:	Epoch: [12][40/233]	Loss 0.3498 (0.2617)	
training:	Epoch: [12][41/233]	Loss 0.2763 (0.2621)	
training:	Epoch: [12][42/233]	Loss 0.2355 (0.2614)	
training:	Epoch: [12][43/233]	Loss 0.2510 (0.2612)	
training:	Epoch: [12][44/233]	Loss 0.3053 (0.2622)	
training:	Epoch: [12][45/233]	Loss 0.1801 (0.2604)	
training:	Epoch: [12][46/233]	Loss 0.3545 (0.2624)	
training:	Epoch: [12][47/233]	Loss 0.3450 (0.2642)	
training:	Epoch: [12][48/233]	Loss 0.2565 (0.2640)	
training:	Epoch: [12][49/233]	Loss 0.4712 (0.2682)	
training:	Epoch: [12][50/233]	Loss 0.4023 (0.2709)	
training:	Epoch: [12][51/233]	Loss 0.3340 (0.2722)	
training:	Epoch: [12][52/233]	Loss 0.2466 (0.2717)	
training:	Epoch: [12][53/233]	Loss 0.3718 (0.2736)	
training:	Epoch: [12][54/233]	Loss 0.3472 (0.2749)	
training:	Epoch: [12][55/233]	Loss 0.4893 (0.2788)	
training:	Epoch: [12][56/233]	Loss 0.3180 (0.2795)	
training:	Epoch: [12][57/233]	Loss 0.3703 (0.2811)	
training:	Epoch: [12][58/233]	Loss 0.2635 (0.2808)	
training:	Epoch: [12][59/233]	Loss 0.5369 (0.2852)	
training:	Epoch: [12][60/233]	Loss 0.3148 (0.2856)	
training:	Epoch: [12][61/233]	Loss 0.2012 (0.2843)	
training:	Epoch: [12][62/233]	Loss 0.3907 (0.2860)	
training:	Epoch: [12][63/233]	Loss 0.3039 (0.2863)	
training:	Epoch: [12][64/233]	Loss 0.3719 (0.2876)	
training:	Epoch: [12][65/233]	Loss 0.5155 (0.2911)	
training:	Epoch: [12][66/233]	Loss 0.4221 (0.2931)	
training:	Epoch: [12][67/233]	Loss 0.3541 (0.2940)	
training:	Epoch: [12][68/233]	Loss 0.2396 (0.2932)	
training:	Epoch: [12][69/233]	Loss 0.1652 (0.2913)	
training:	Epoch: [12][70/233]	Loss 0.2551 (0.2908)	
training:	Epoch: [12][71/233]	Loss 0.4290 (0.2928)	
training:	Epoch: [12][72/233]	Loss 0.1775 (0.2912)	
training:	Epoch: [12][73/233]	Loss 0.2767 (0.2910)	
training:	Epoch: [12][74/233]	Loss 0.3282 (0.2915)	
training:	Epoch: [12][75/233]	Loss 0.2741 (0.2912)	
training:	Epoch: [12][76/233]	Loss 0.3036 (0.2914)	
training:	Epoch: [12][77/233]	Loss 0.1702 (0.2898)	
training:	Epoch: [12][78/233]	Loss 0.3364 (0.2904)	
training:	Epoch: [12][79/233]	Loss 0.3622 (0.2913)	
training:	Epoch: [12][80/233]	Loss 0.2862 (0.2913)	
training:	Epoch: [12][81/233]	Loss 0.1663 (0.2897)	
training:	Epoch: [12][82/233]	Loss 0.2429 (0.2892)	
training:	Epoch: [12][83/233]	Loss 0.3612 (0.2900)	
training:	Epoch: [12][84/233]	Loss 0.2474 (0.2895)	
training:	Epoch: [12][85/233]	Loss 0.3343 (0.2901)	
training:	Epoch: [12][86/233]	Loss 0.2509 (0.2896)	
training:	Epoch: [12][87/233]	Loss 0.1270 (0.2877)	
training:	Epoch: [12][88/233]	Loss 0.2848 (0.2877)	
training:	Epoch: [12][89/233]	Loss 0.4795 (0.2898)	
training:	Epoch: [12][90/233]	Loss 0.2323 (0.2892)	
training:	Epoch: [12][91/233]	Loss 0.3162 (0.2895)	
training:	Epoch: [12][92/233]	Loss 0.2728 (0.2893)	
training:	Epoch: [12][93/233]	Loss 0.3968 (0.2905)	
training:	Epoch: [12][94/233]	Loss 0.3146 (0.2907)	
training:	Epoch: [12][95/233]	Loss 0.2707 (0.2905)	
training:	Epoch: [12][96/233]	Loss 0.4882 (0.2926)	
training:	Epoch: [12][97/233]	Loss 0.1944 (0.2916)	
training:	Epoch: [12][98/233]	Loss 0.2574 (0.2912)	
training:	Epoch: [12][99/233]	Loss 0.4435 (0.2928)	
training:	Epoch: [12][100/233]	Loss 0.2754 (0.2926)	
training:	Epoch: [12][101/233]	Loss 0.2197 (0.2919)	
training:	Epoch: [12][102/233]	Loss 0.2890 (0.2918)	
training:	Epoch: [12][103/233]	Loss 0.2973 (0.2919)	
training:	Epoch: [12][104/233]	Loss 0.3285 (0.2922)	
training:	Epoch: [12][105/233]	Loss 0.3338 (0.2926)	
training:	Epoch: [12][106/233]	Loss 0.3247 (0.2929)	
training:	Epoch: [12][107/233]	Loss 0.5777 (0.2956)	
training:	Epoch: [12][108/233]	Loss 0.4443 (0.2970)	
training:	Epoch: [12][109/233]	Loss 0.2278 (0.2963)	
training:	Epoch: [12][110/233]	Loss 0.2702 (0.2961)	
training:	Epoch: [12][111/233]	Loss 0.2169 (0.2954)	
training:	Epoch: [12][112/233]	Loss 0.4014 (0.2963)	
training:	Epoch: [12][113/233]	Loss 0.2667 (0.2961)	
training:	Epoch: [12][114/233]	Loss 0.2210 (0.2954)	
training:	Epoch: [12][115/233]	Loss 0.1669 (0.2943)	
training:	Epoch: [12][116/233]	Loss 0.3497 (0.2948)	
training:	Epoch: [12][117/233]	Loss 0.3614 (0.2953)	
training:	Epoch: [12][118/233]	Loss 0.1863 (0.2944)	
training:	Epoch: [12][119/233]	Loss 0.3559 (0.2949)	
training:	Epoch: [12][120/233]	Loss 0.4916 (0.2966)	
training:	Epoch: [12][121/233]	Loss 0.2799 (0.2964)	
training:	Epoch: [12][122/233]	Loss 0.3373 (0.2968)	
training:	Epoch: [12][123/233]	Loss 0.2609 (0.2965)	
training:	Epoch: [12][124/233]	Loss 0.2407 (0.2960)	
training:	Epoch: [12][125/233]	Loss 0.2904 (0.2960)	
training:	Epoch: [12][126/233]	Loss 0.2631 (0.2957)	
training:	Epoch: [12][127/233]	Loss 0.2063 (0.2950)	
training:	Epoch: [12][128/233]	Loss 0.2879 (0.2950)	
training:	Epoch: [12][129/233]	Loss 0.2287 (0.2945)	
training:	Epoch: [12][130/233]	Loss 0.2371 (0.2940)	
training:	Epoch: [12][131/233]	Loss 0.1977 (0.2933)	
training:	Epoch: [12][132/233]	Loss 0.2883 (0.2932)	
training:	Epoch: [12][133/233]	Loss 0.1783 (0.2924)	
training:	Epoch: [12][134/233]	Loss 0.3977 (0.2932)	
training:	Epoch: [12][135/233]	Loss 0.1797 (0.2923)	
training:	Epoch: [12][136/233]	Loss 0.2262 (0.2918)	
training:	Epoch: [12][137/233]	Loss 0.2282 (0.2914)	
training:	Epoch: [12][138/233]	Loss 0.2654 (0.2912)	
training:	Epoch: [12][139/233]	Loss 0.1073 (0.2899)	
training:	Epoch: [12][140/233]	Loss 0.4115 (0.2907)	
training:	Epoch: [12][141/233]	Loss 0.3150 (0.2909)	
training:	Epoch: [12][142/233]	Loss 0.3444 (0.2913)	
training:	Epoch: [12][143/233]	Loss 0.2143 (0.2907)	
training:	Epoch: [12][144/233]	Loss 0.2167 (0.2902)	
training:	Epoch: [12][145/233]	Loss 0.3356 (0.2905)	
training:	Epoch: [12][146/233]	Loss 0.2665 (0.2904)	
training:	Epoch: [12][147/233]	Loss 0.2930 (0.2904)	
training:	Epoch: [12][148/233]	Loss 0.3136 (0.2905)	
training:	Epoch: [12][149/233]	Loss 0.2518 (0.2903)	
training:	Epoch: [12][150/233]	Loss 0.5730 (0.2922)	
training:	Epoch: [12][151/233]	Loss 0.1595 (0.2913)	
training:	Epoch: [12][152/233]	Loss 0.2946 (0.2913)	
training:	Epoch: [12][153/233]	Loss 0.1726 (0.2905)	
training:	Epoch: [12][154/233]	Loss 0.4590 (0.2916)	
training:	Epoch: [12][155/233]	Loss 0.2566 (0.2914)	
training:	Epoch: [12][156/233]	Loss 0.3059 (0.2915)	
training:	Epoch: [12][157/233]	Loss 0.2735 (0.2914)	
training:	Epoch: [12][158/233]	Loss 0.1929 (0.2908)	
training:	Epoch: [12][159/233]	Loss 0.2169 (0.2903)	
training:	Epoch: [12][160/233]	Loss 0.2411 (0.2900)	
training:	Epoch: [12][161/233]	Loss 0.2282 (0.2896)	
training:	Epoch: [12][162/233]	Loss 0.3080 (0.2897)	
training:	Epoch: [12][163/233]	Loss 0.2748 (0.2896)	
training:	Epoch: [12][164/233]	Loss 0.4638 (0.2907)	
training:	Epoch: [12][165/233]	Loss 0.3994 (0.2913)	
training:	Epoch: [12][166/233]	Loss 0.3568 (0.2917)	
training:	Epoch: [12][167/233]	Loss 0.2999 (0.2918)	
training:	Epoch: [12][168/233]	Loss 0.2913 (0.2918)	
training:	Epoch: [12][169/233]	Loss 0.1647 (0.2910)	
training:	Epoch: [12][170/233]	Loss 0.4022 (0.2917)	
training:	Epoch: [12][171/233]	Loss 0.2380 (0.2914)	
training:	Epoch: [12][172/233]	Loss 0.1760 (0.2907)	
training:	Epoch: [12][173/233]	Loss 0.3370 (0.2910)	
training:	Epoch: [12][174/233]	Loss 0.3302 (0.2912)	
training:	Epoch: [12][175/233]	Loss 0.3288 (0.2914)	
training:	Epoch: [12][176/233]	Loss 0.5708 (0.2930)	
training:	Epoch: [12][177/233]	Loss 0.2746 (0.2929)	
training:	Epoch: [12][178/233]	Loss 0.2317 (0.2926)	
training:	Epoch: [12][179/233]	Loss 0.2125 (0.2921)	
training:	Epoch: [12][180/233]	Loss 0.2523 (0.2919)	
training:	Epoch: [12][181/233]	Loss 0.2563 (0.2917)	
training:	Epoch: [12][182/233]	Loss 0.3951 (0.2923)	
training:	Epoch: [12][183/233]	Loss 0.1719 (0.2916)	
training:	Epoch: [12][184/233]	Loss 0.2406 (0.2913)	
training:	Epoch: [12][185/233]	Loss 0.3179 (0.2915)	
training:	Epoch: [12][186/233]	Loss 0.2287 (0.2911)	
training:	Epoch: [12][187/233]	Loss 0.1429 (0.2903)	
training:	Epoch: [12][188/233]	Loss 0.2068 (0.2899)	
training:	Epoch: [12][189/233]	Loss 0.1216 (0.2890)	
training:	Epoch: [12][190/233]	Loss 0.2601 (0.2888)	
training:	Epoch: [12][191/233]	Loss 0.2562 (0.2887)	
training:	Epoch: [12][192/233]	Loss 0.3348 (0.2889)	
training:	Epoch: [12][193/233]	Loss 0.1640 (0.2883)	
training:	Epoch: [12][194/233]	Loss 0.3616 (0.2886)	
training:	Epoch: [12][195/233]	Loss 0.4941 (0.2897)	
training:	Epoch: [12][196/233]	Loss 0.1494 (0.2890)	
training:	Epoch: [12][197/233]	Loss 0.1448 (0.2883)	
training:	Epoch: [12][198/233]	Loss 0.4961 (0.2893)	
training:	Epoch: [12][199/233]	Loss 0.1817 (0.2888)	
training:	Epoch: [12][200/233]	Loss 0.1612 (0.2881)	
training:	Epoch: [12][201/233]	Loss 0.3043 (0.2882)	
training:	Epoch: [12][202/233]	Loss 0.2166 (0.2879)	
training:	Epoch: [12][203/233]	Loss 0.4626 (0.2887)	
training:	Epoch: [12][204/233]	Loss 0.3231 (0.2889)	
training:	Epoch: [12][205/233]	Loss 0.3815 (0.2893)	
training:	Epoch: [12][206/233]	Loss 0.4345 (0.2900)	
training:	Epoch: [12][207/233]	Loss 0.3305 (0.2902)	
training:	Epoch: [12][208/233]	Loss 0.3501 (0.2905)	
training:	Epoch: [12][209/233]	Loss 0.2752 (0.2904)	
training:	Epoch: [12][210/233]	Loss 0.3816 (0.2909)	
training:	Epoch: [12][211/233]	Loss 0.2877 (0.2909)	
training:	Epoch: [12][212/233]	Loss 0.1221 (0.2901)	
training:	Epoch: [12][213/233]	Loss 0.2746 (0.2900)	
training:	Epoch: [12][214/233]	Loss 0.3323 (0.2902)	
training:	Epoch: [12][215/233]	Loss 0.1677 (0.2896)	
training:	Epoch: [12][216/233]	Loss 0.5936 (0.2910)	
training:	Epoch: [12][217/233]	Loss 0.3911 (0.2915)	
training:	Epoch: [12][218/233]	Loss 0.2757 (0.2914)	
training:	Epoch: [12][219/233]	Loss 0.3110 (0.2915)	
training:	Epoch: [12][220/233]	Loss 0.4985 (0.2925)	
training:	Epoch: [12][221/233]	Loss 0.1960 (0.2920)	
training:	Epoch: [12][222/233]	Loss 0.3008 (0.2921)	
training:	Epoch: [12][223/233]	Loss 0.1483 (0.2914)	
training:	Epoch: [12][224/233]	Loss 0.4082 (0.2919)	
training:	Epoch: [12][225/233]	Loss 0.3752 (0.2923)	
training:	Epoch: [12][226/233]	Loss 0.3770 (0.2927)	
training:	Epoch: [12][227/233]	Loss 0.5812 (0.2939)	
training:	Epoch: [12][228/233]	Loss 0.2978 (0.2940)	
training:	Epoch: [12][229/233]	Loss 0.2939 (0.2940)	
training:	Epoch: [12][230/233]	Loss 0.2410 (0.2937)	
training:	Epoch: [12][231/233]	Loss 0.1787 (0.2932)	
training:	Epoch: [12][232/233]	Loss 0.4831 (0.2941)	
training:	Epoch: [12][233/233]	Loss 0.2849 (0.2940)	
Training:	 Loss: 0.2933

Training:	 ACC: 0.9002 0.9011 0.9210 0.8794
Validation:	 ACC: 0.8098 0.8111 0.8386 0.7809
Validation:	 Best_BACC: 0.8162 0.8154 0.7998 0.8326
Validation:	 Loss: 0.4331
Pretraining:	Epoch 13/200
----------
training:	Epoch: [13][1/233]	Loss 0.2110 (0.2110)	
training:	Epoch: [13][2/233]	Loss 0.4118 (0.3114)	
training:	Epoch: [13][3/233]	Loss 0.4592 (0.3606)	
training:	Epoch: [13][4/233]	Loss 0.2777 (0.3399)	
training:	Epoch: [13][5/233]	Loss 0.3059 (0.3331)	
training:	Epoch: [13][6/233]	Loss 0.2884 (0.3257)	
training:	Epoch: [13][7/233]	Loss 0.3634 (0.3311)	
training:	Epoch: [13][8/233]	Loss 0.2185 (0.3170)	
training:	Epoch: [13][9/233]	Loss 0.1720 (0.3009)	
training:	Epoch: [13][10/233]	Loss 0.4151 (0.3123)	
training:	Epoch: [13][11/233]	Loss 0.2173 (0.3037)	
training:	Epoch: [13][12/233]	Loss 0.2708 (0.3009)	
training:	Epoch: [13][13/233]	Loss 0.1982 (0.2930)	
training:	Epoch: [13][14/233]	Loss 0.3303 (0.2957)	
training:	Epoch: [13][15/233]	Loss 0.2380 (0.2918)	
training:	Epoch: [13][16/233]	Loss 0.4212 (0.2999)	
training:	Epoch: [13][17/233]	Loss 0.2162 (0.2950)	
training:	Epoch: [13][18/233]	Loss 0.3158 (0.2962)	
training:	Epoch: [13][19/233]	Loss 0.1496 (0.2884)	
training:	Epoch: [13][20/233]	Loss 0.4577 (0.2969)	
training:	Epoch: [13][21/233]	Loss 0.1556 (0.2902)	
training:	Epoch: [13][22/233]	Loss 0.3714 (0.2939)	
training:	Epoch: [13][23/233]	Loss 0.1123 (0.2860)	
training:	Epoch: [13][24/233]	Loss 0.2790 (0.2857)	
training:	Epoch: [13][25/233]	Loss 0.2145 (0.2828)	
training:	Epoch: [13][26/233]	Loss 0.2117 (0.2801)	
training:	Epoch: [13][27/233]	Loss 0.1900 (0.2768)	
training:	Epoch: [13][28/233]	Loss 0.4281 (0.2822)	
training:	Epoch: [13][29/233]	Loss 0.3062 (0.2830)	
training:	Epoch: [13][30/233]	Loss 0.1634 (0.2790)	
training:	Epoch: [13][31/233]	Loss 0.3041 (0.2798)	
training:	Epoch: [13][32/233]	Loss 0.3471 (0.2819)	
training:	Epoch: [13][33/233]	Loss 0.1818 (0.2789)	
training:	Epoch: [13][34/233]	Loss 0.3557 (0.2811)	
training:	Epoch: [13][35/233]	Loss 0.1551 (0.2775)	
training:	Epoch: [13][36/233]	Loss 0.1610 (0.2743)	
training:	Epoch: [13][37/233]	Loss 0.3797 (0.2772)	
training:	Epoch: [13][38/233]	Loss 0.2010 (0.2752)	
training:	Epoch: [13][39/233]	Loss 0.2999 (0.2758)	
training:	Epoch: [13][40/233]	Loss 0.2816 (0.2759)	
training:	Epoch: [13][41/233]	Loss 0.4178 (0.2794)	
training:	Epoch: [13][42/233]	Loss 0.2166 (0.2779)	
training:	Epoch: [13][43/233]	Loss 0.1487 (0.2749)	
training:	Epoch: [13][44/233]	Loss 0.1902 (0.2730)	
training:	Epoch: [13][45/233]	Loss 0.3074 (0.2737)	
training:	Epoch: [13][46/233]	Loss 0.5231 (0.2792)	
training:	Epoch: [13][47/233]	Loss 0.4021 (0.2818)	
training:	Epoch: [13][48/233]	Loss 0.3682 (0.2836)	
training:	Epoch: [13][49/233]	Loss 0.1014 (0.2799)	
training:	Epoch: [13][50/233]	Loss 0.3745 (0.2817)	
training:	Epoch: [13][51/233]	Loss 0.3158 (0.2824)	
training:	Epoch: [13][52/233]	Loss 0.1855 (0.2805)	
training:	Epoch: [13][53/233]	Loss 0.1043 (0.2772)	
training:	Epoch: [13][54/233]	Loss 0.3783 (0.2791)	
training:	Epoch: [13][55/233]	Loss 0.4491 (0.2822)	
training:	Epoch: [13][56/233]	Loss 0.5061 (0.2862)	
training:	Epoch: [13][57/233]	Loss 0.2055 (0.2848)	
training:	Epoch: [13][58/233]	Loss 0.1118 (0.2818)	
training:	Epoch: [13][59/233]	Loss 0.2743 (0.2817)	
training:	Epoch: [13][60/233]	Loss 0.2329 (0.2808)	
training:	Epoch: [13][61/233]	Loss 0.3614 (0.2822)	
training:	Epoch: [13][62/233]	Loss 0.3598 (0.2834)	
training:	Epoch: [13][63/233]	Loss 0.3558 (0.2846)	
training:	Epoch: [13][64/233]	Loss 0.2191 (0.2835)	
training:	Epoch: [13][65/233]	Loss 0.5007 (0.2869)	
training:	Epoch: [13][66/233]	Loss 0.1994 (0.2856)	
training:	Epoch: [13][67/233]	Loss 0.1984 (0.2843)	
training:	Epoch: [13][68/233]	Loss 0.1883 (0.2829)	
training:	Epoch: [13][69/233]	Loss 0.3329 (0.2836)	
training:	Epoch: [13][70/233]	Loss 0.2210 (0.2827)	
training:	Epoch: [13][71/233]	Loss 0.2268 (0.2819)	
training:	Epoch: [13][72/233]	Loss 0.1674 (0.2803)	
training:	Epoch: [13][73/233]	Loss 0.1606 (0.2787)	
training:	Epoch: [13][74/233]	Loss 0.2235 (0.2779)	
training:	Epoch: [13][75/233]	Loss 0.2103 (0.2770)	
training:	Epoch: [13][76/233]	Loss 0.1854 (0.2758)	
training:	Epoch: [13][77/233]	Loss 0.1978 (0.2748)	
training:	Epoch: [13][78/233]	Loss 0.3948 (0.2763)	
training:	Epoch: [13][79/233]	Loss 0.2246 (0.2757)	
training:	Epoch: [13][80/233]	Loss 0.3142 (0.2762)	
training:	Epoch: [13][81/233]	Loss 0.2041 (0.2753)	
training:	Epoch: [13][82/233]	Loss 0.4848 (0.2778)	
training:	Epoch: [13][83/233]	Loss 0.1768 (0.2766)	
training:	Epoch: [13][84/233]	Loss 0.3894 (0.2780)	
training:	Epoch: [13][85/233]	Loss 0.3034 (0.2783)	
training:	Epoch: [13][86/233]	Loss 0.4283 (0.2800)	
training:	Epoch: [13][87/233]	Loss 0.4027 (0.2814)	
training:	Epoch: [13][88/233]	Loss 0.0896 (0.2792)	
training:	Epoch: [13][89/233]	Loss 0.2525 (0.2789)	
training:	Epoch: [13][90/233]	Loss 0.1362 (0.2773)	
training:	Epoch: [13][91/233]	Loss 0.3410 (0.2780)	
training:	Epoch: [13][92/233]	Loss 0.2384 (0.2776)	
training:	Epoch: [13][93/233]	Loss 0.3632 (0.2785)	
training:	Epoch: [13][94/233]	Loss 0.2787 (0.2785)	
training:	Epoch: [13][95/233]	Loss 0.2608 (0.2784)	
training:	Epoch: [13][96/233]	Loss 0.2341 (0.2779)	
training:	Epoch: [13][97/233]	Loss 0.3015 (0.2781)	
training:	Epoch: [13][98/233]	Loss 0.3988 (0.2794)	
training:	Epoch: [13][99/233]	Loss 0.2021 (0.2786)	
training:	Epoch: [13][100/233]	Loss 0.2392 (0.2782)	
training:	Epoch: [13][101/233]	Loss 0.2451 (0.2779)	
training:	Epoch: [13][102/233]	Loss 0.6538 (0.2815)	
training:	Epoch: [13][103/233]	Loss 0.3382 (0.2821)	
training:	Epoch: [13][104/233]	Loss 0.3787 (0.2830)	
training:	Epoch: [13][105/233]	Loss 0.4150 (0.2843)	
training:	Epoch: [13][106/233]	Loss 0.2641 (0.2841)	
training:	Epoch: [13][107/233]	Loss 0.4546 (0.2857)	
training:	Epoch: [13][108/233]	Loss 0.3602 (0.2864)	
training:	Epoch: [13][109/233]	Loss 0.2590 (0.2861)	
training:	Epoch: [13][110/233]	Loss 0.2677 (0.2860)	
training:	Epoch: [13][111/233]	Loss 0.3064 (0.2861)	
training:	Epoch: [13][112/233]	Loss 0.3416 (0.2866)	
training:	Epoch: [13][113/233]	Loss 0.2151 (0.2860)	
training:	Epoch: [13][114/233]	Loss 0.3546 (0.2866)	
training:	Epoch: [13][115/233]	Loss 0.1648 (0.2855)	
training:	Epoch: [13][116/233]	Loss 0.4681 (0.2871)	
training:	Epoch: [13][117/233]	Loss 0.2841 (0.2871)	
training:	Epoch: [13][118/233]	Loss 0.1277 (0.2857)	
training:	Epoch: [13][119/233]	Loss 0.4813 (0.2874)	
training:	Epoch: [13][120/233]	Loss 0.2651 (0.2872)	
training:	Epoch: [13][121/233]	Loss 0.3946 (0.2881)	
training:	Epoch: [13][122/233]	Loss 0.2012 (0.2874)	
training:	Epoch: [13][123/233]	Loss 0.4971 (0.2891)	
training:	Epoch: [13][124/233]	Loss 0.4486 (0.2904)	
training:	Epoch: [13][125/233]	Loss 0.2094 (0.2897)	
training:	Epoch: [13][126/233]	Loss 0.2150 (0.2891)	
training:	Epoch: [13][127/233]	Loss 0.1670 (0.2882)	
training:	Epoch: [13][128/233]	Loss 0.5438 (0.2902)	
training:	Epoch: [13][129/233]	Loss 0.1898 (0.2894)	
training:	Epoch: [13][130/233]	Loss 0.4327 (0.2905)	
training:	Epoch: [13][131/233]	Loss 0.3959 (0.2913)	
training:	Epoch: [13][132/233]	Loss 0.1938 (0.2906)	
training:	Epoch: [13][133/233]	Loss 0.1959 (0.2898)	
training:	Epoch: [13][134/233]	Loss 0.3125 (0.2900)	
training:	Epoch: [13][135/233]	Loss 0.2907 (0.2900)	
training:	Epoch: [13][136/233]	Loss 0.2126 (0.2894)	
training:	Epoch: [13][137/233]	Loss 0.3043 (0.2896)	
training:	Epoch: [13][138/233]	Loss 0.1682 (0.2887)	
training:	Epoch: [13][139/233]	Loss 0.2752 (0.2886)	
training:	Epoch: [13][140/233]	Loss 0.2231 (0.2881)	
training:	Epoch: [13][141/233]	Loss 0.3070 (0.2882)	
training:	Epoch: [13][142/233]	Loss 0.1791 (0.2875)	
training:	Epoch: [13][143/233]	Loss 0.3201 (0.2877)	
training:	Epoch: [13][144/233]	Loss 0.2702 (0.2876)	
training:	Epoch: [13][145/233]	Loss 0.2210 (0.2871)	
training:	Epoch: [13][146/233]	Loss 0.1792 (0.2864)	
training:	Epoch: [13][147/233]	Loss 0.2829 (0.2864)	
training:	Epoch: [13][148/233]	Loss 0.3298 (0.2867)	
training:	Epoch: [13][149/233]	Loss 0.2140 (0.2862)	
training:	Epoch: [13][150/233]	Loss 0.2543 (0.2860)	
training:	Epoch: [13][151/233]	Loss 0.3730 (0.2865)	
training:	Epoch: [13][152/233]	Loss 0.4585 (0.2877)	
training:	Epoch: [13][153/233]	Loss 0.2715 (0.2876)	
training:	Epoch: [13][154/233]	Loss 0.3190 (0.2878)	
training:	Epoch: [13][155/233]	Loss 0.2835 (0.2877)	
training:	Epoch: [13][156/233]	Loss 0.4017 (0.2885)	
training:	Epoch: [13][157/233]	Loss 0.1164 (0.2874)	
training:	Epoch: [13][158/233]	Loss 0.2404 (0.2871)	
training:	Epoch: [13][159/233]	Loss 0.2345 (0.2867)	
training:	Epoch: [13][160/233]	Loss 0.2733 (0.2867)	
training:	Epoch: [13][161/233]	Loss 0.3088 (0.2868)	
training:	Epoch: [13][162/233]	Loss 0.3949 (0.2875)	
training:	Epoch: [13][163/233]	Loss 0.1690 (0.2867)	
training:	Epoch: [13][164/233]	Loss 0.2429 (0.2865)	
training:	Epoch: [13][165/233]	Loss 0.3361 (0.2868)	
training:	Epoch: [13][166/233]	Loss 0.3097 (0.2869)	
training:	Epoch: [13][167/233]	Loss 0.2187 (0.2865)	
training:	Epoch: [13][168/233]	Loss 0.3455 (0.2868)	
training:	Epoch: [13][169/233]	Loss 0.2252 (0.2865)	
training:	Epoch: [13][170/233]	Loss 0.3583 (0.2869)	
training:	Epoch: [13][171/233]	Loss 0.3228 (0.2871)	
training:	Epoch: [13][172/233]	Loss 0.1467 (0.2863)	
training:	Epoch: [13][173/233]	Loss 0.1553 (0.2855)	
training:	Epoch: [13][174/233]	Loss 0.3911 (0.2861)	
training:	Epoch: [13][175/233]	Loss 0.1997 (0.2857)	
training:	Epoch: [13][176/233]	Loss 0.1618 (0.2850)	
training:	Epoch: [13][177/233]	Loss 0.2938 (0.2850)	
training:	Epoch: [13][178/233]	Loss 0.1945 (0.2845)	
training:	Epoch: [13][179/233]	Loss 0.3505 (0.2849)	
training:	Epoch: [13][180/233]	Loss 0.2160 (0.2845)	
training:	Epoch: [13][181/233]	Loss 0.2518 (0.2843)	
training:	Epoch: [13][182/233]	Loss 0.3243 (0.2845)	
training:	Epoch: [13][183/233]	Loss 0.4368 (0.2854)	
training:	Epoch: [13][184/233]	Loss 0.4951 (0.2865)	
training:	Epoch: [13][185/233]	Loss 0.2348 (0.2862)	
training:	Epoch: [13][186/233]	Loss 0.5739 (0.2878)	
training:	Epoch: [13][187/233]	Loss 0.2430 (0.2875)	
training:	Epoch: [13][188/233]	Loss 0.1809 (0.2870)	
training:	Epoch: [13][189/233]	Loss 0.1566 (0.2863)	
training:	Epoch: [13][190/233]	Loss 0.3603 (0.2867)	
training:	Epoch: [13][191/233]	Loss 0.2611 (0.2865)	
training:	Epoch: [13][192/233]	Loss 0.5025 (0.2876)	
training:	Epoch: [13][193/233]	Loss 0.2151 (0.2873)	
training:	Epoch: [13][194/233]	Loss 0.2993 (0.2873)	
training:	Epoch: [13][195/233]	Loss 0.2223 (0.2870)	
training:	Epoch: [13][196/233]	Loss 0.2848 (0.2870)	
training:	Epoch: [13][197/233]	Loss 0.2823 (0.2870)	
training:	Epoch: [13][198/233]	Loss 0.2250 (0.2866)	
training:	Epoch: [13][199/233]	Loss 0.2451 (0.2864)	
training:	Epoch: [13][200/233]	Loss 0.3635 (0.2868)	
training:	Epoch: [13][201/233]	Loss 0.2991 (0.2869)	
training:	Epoch: [13][202/233]	Loss 0.2793 (0.2868)	
training:	Epoch: [13][203/233]	Loss 0.3428 (0.2871)	
training:	Epoch: [13][204/233]	Loss 0.3095 (0.2872)	
training:	Epoch: [13][205/233]	Loss 0.2853 (0.2872)	
training:	Epoch: [13][206/233]	Loss 0.3275 (0.2874)	
training:	Epoch: [13][207/233]	Loss 0.2805 (0.2874)	
training:	Epoch: [13][208/233]	Loss 0.2664 (0.2873)	
training:	Epoch: [13][209/233]	Loss 0.3019 (0.2874)	
training:	Epoch: [13][210/233]	Loss 0.4007 (0.2879)	
training:	Epoch: [13][211/233]	Loss 0.2715 (0.2878)	
training:	Epoch: [13][212/233]	Loss 0.3262 (0.2880)	
training:	Epoch: [13][213/233]	Loss 0.5148 (0.2891)	
training:	Epoch: [13][214/233]	Loss 0.3657 (0.2894)	
training:	Epoch: [13][215/233]	Loss 0.3502 (0.2897)	
training:	Epoch: [13][216/233]	Loss 0.2009 (0.2893)	
training:	Epoch: [13][217/233]	Loss 0.2758 (0.2892)	
training:	Epoch: [13][218/233]	Loss 0.3102 (0.2893)	
training:	Epoch: [13][219/233]	Loss 0.2362 (0.2891)	
training:	Epoch: [13][220/233]	Loss 0.2375 (0.2888)	
training:	Epoch: [13][221/233]	Loss 0.1241 (0.2881)	
training:	Epoch: [13][222/233]	Loss 0.3220 (0.2883)	
training:	Epoch: [13][223/233]	Loss 0.3547 (0.2886)	
training:	Epoch: [13][224/233]	Loss 0.2792 (0.2885)	
training:	Epoch: [13][225/233]	Loss 0.1716 (0.2880)	
training:	Epoch: [13][226/233]	Loss 0.2545 (0.2878)	
training:	Epoch: [13][227/233]	Loss 0.3180 (0.2880)	
training:	Epoch: [13][228/233]	Loss 0.2542 (0.2878)	
training:	Epoch: [13][229/233]	Loss 0.6312 (0.2893)	
training:	Epoch: [13][230/233]	Loss 0.1770 (0.2888)	
training:	Epoch: [13][231/233]	Loss 0.1956 (0.2884)	
training:	Epoch: [13][232/233]	Loss 0.3893 (0.2889)	
training:	Epoch: [13][233/233]	Loss 0.2529 (0.2887)	
Training:	 Loss: 0.2881

Training:	 ACC: 0.9071 0.9061 0.8826 0.9317
Validation:	 ACC: 0.8085 0.8074 0.7845 0.8326
Validation:	 Best_BACC: 0.8162 0.8154 0.7998 0.8326
Validation:	 Loss: 0.4363
Pretraining:	Epoch 14/200
----------
training:	Epoch: [14][1/233]	Loss 0.1509 (0.1509)	
training:	Epoch: [14][2/233]	Loss 0.3319 (0.2414)	
training:	Epoch: [14][3/233]	Loss 0.3122 (0.2650)	
training:	Epoch: [14][4/233]	Loss 0.1763 (0.2428)	
training:	Epoch: [14][5/233]	Loss 0.1939 (0.2330)	
training:	Epoch: [14][6/233]	Loss 0.2372 (0.2337)	
training:	Epoch: [14][7/233]	Loss 0.2010 (0.2291)	
training:	Epoch: [14][8/233]	Loss 0.2188 (0.2278)	
training:	Epoch: [14][9/233]	Loss 0.2023 (0.2249)	
training:	Epoch: [14][10/233]	Loss 0.3202 (0.2345)	
training:	Epoch: [14][11/233]	Loss 0.2719 (0.2379)	
training:	Epoch: [14][12/233]	Loss 0.1863 (0.2336)	
training:	Epoch: [14][13/233]	Loss 0.3537 (0.2428)	
training:	Epoch: [14][14/233]	Loss 0.2470 (0.2431)	
training:	Epoch: [14][15/233]	Loss 0.3031 (0.2471)	
training:	Epoch: [14][16/233]	Loss 0.2768 (0.2490)	
training:	Epoch: [14][17/233]	Loss 0.3105 (0.2526)	
training:	Epoch: [14][18/233]	Loss 0.2640 (0.2532)	
training:	Epoch: [14][19/233]	Loss 0.3110 (0.2563)	
training:	Epoch: [14][20/233]	Loss 0.2962 (0.2583)	
training:	Epoch: [14][21/233]	Loss 0.3641 (0.2633)	
training:	Epoch: [14][22/233]	Loss 0.3250 (0.2661)	
training:	Epoch: [14][23/233]	Loss 0.2248 (0.2643)	
training:	Epoch: [14][24/233]	Loss 0.1747 (0.2606)	
training:	Epoch: [14][25/233]	Loss 0.2921 (0.2618)	
training:	Epoch: [14][26/233]	Loss 0.3475 (0.2651)	
training:	Epoch: [14][27/233]	Loss 0.2448 (0.2644)	
training:	Epoch: [14][28/233]	Loss 0.1648 (0.2608)	
training:	Epoch: [14][29/233]	Loss 0.1919 (0.2584)	
training:	Epoch: [14][30/233]	Loss 0.3741 (0.2623)	
training:	Epoch: [14][31/233]	Loss 0.3851 (0.2663)	
training:	Epoch: [14][32/233]	Loss 0.4995 (0.2735)	
training:	Epoch: [14][33/233]	Loss 0.2810 (0.2738)	
training:	Epoch: [14][34/233]	Loss 0.3248 (0.2753)	
training:	Epoch: [14][35/233]	Loss 0.3937 (0.2787)	
training:	Epoch: [14][36/233]	Loss 0.2958 (0.2791)	
training:	Epoch: [14][37/233]	Loss 0.2548 (0.2785)	
training:	Epoch: [14][38/233]	Loss 0.3317 (0.2799)	
training:	Epoch: [14][39/233]	Loss 0.3066 (0.2806)	
training:	Epoch: [14][40/233]	Loss 0.1116 (0.2763)	
training:	Epoch: [14][41/233]	Loss 0.2827 (0.2765)	
training:	Epoch: [14][42/233]	Loss 0.2807 (0.2766)	
training:	Epoch: [14][43/233]	Loss 0.2432 (0.2758)	
training:	Epoch: [14][44/233]	Loss 0.0975 (0.2718)	
training:	Epoch: [14][45/233]	Loss 0.2681 (0.2717)	
training:	Epoch: [14][46/233]	Loss 0.2720 (0.2717)	
training:	Epoch: [14][47/233]	Loss 0.1793 (0.2697)	
training:	Epoch: [14][48/233]	Loss 0.1987 (0.2682)	
training:	Epoch: [14][49/233]	Loss 0.3339 (0.2696)	
training:	Epoch: [14][50/233]	Loss 0.2051 (0.2683)	
training:	Epoch: [14][51/233]	Loss 0.1520 (0.2660)	
training:	Epoch: [14][52/233]	Loss 0.4427 (0.2694)	
training:	Epoch: [14][53/233]	Loss 0.3244 (0.2705)	
training:	Epoch: [14][54/233]	Loss 0.3549 (0.2720)	
training:	Epoch: [14][55/233]	Loss 0.2632 (0.2719)	
training:	Epoch: [14][56/233]	Loss 0.3478 (0.2732)	
training:	Epoch: [14][57/233]	Loss 0.3286 (0.2742)	
training:	Epoch: [14][58/233]	Loss 0.2380 (0.2736)	
training:	Epoch: [14][59/233]	Loss 0.5798 (0.2787)	
training:	Epoch: [14][60/233]	Loss 0.1774 (0.2771)	
training:	Epoch: [14][61/233]	Loss 0.2844 (0.2772)	
training:	Epoch: [14][62/233]	Loss 0.4541 (0.2800)	
training:	Epoch: [14][63/233]	Loss 0.2486 (0.2795)	
training:	Epoch: [14][64/233]	Loss 0.2930 (0.2797)	
training:	Epoch: [14][65/233]	Loss 0.4514 (0.2824)	
training:	Epoch: [14][66/233]	Loss 0.3210 (0.2830)	
training:	Epoch: [14][67/233]	Loss 0.2668 (0.2827)	
training:	Epoch: [14][68/233]	Loss 0.2143 (0.2817)	
training:	Epoch: [14][69/233]	Loss 0.2480 (0.2812)	
training:	Epoch: [14][70/233]	Loss 0.2589 (0.2809)	
training:	Epoch: [14][71/233]	Loss 0.3320 (0.2816)	
training:	Epoch: [14][72/233]	Loss 0.2409 (0.2811)	
training:	Epoch: [14][73/233]	Loss 0.2138 (0.2801)	
training:	Epoch: [14][74/233]	Loss 0.3800 (0.2815)	
training:	Epoch: [14][75/233]	Loss 0.2941 (0.2817)	
training:	Epoch: [14][76/233]	Loss 0.1982 (0.2806)	
training:	Epoch: [14][77/233]	Loss 0.1670 (0.2791)	
training:	Epoch: [14][78/233]	Loss 0.3997 (0.2806)	
training:	Epoch: [14][79/233]	Loss 0.2838 (0.2807)	
training:	Epoch: [14][80/233]	Loss 0.5153 (0.2836)	
training:	Epoch: [14][81/233]	Loss 0.2444 (0.2831)	
training:	Epoch: [14][82/233]	Loss 0.2312 (0.2825)	
training:	Epoch: [14][83/233]	Loss 0.2596 (0.2822)	
training:	Epoch: [14][84/233]	Loss 0.2404 (0.2817)	
training:	Epoch: [14][85/233]	Loss 0.3640 (0.2827)	
training:	Epoch: [14][86/233]	Loss 0.1474 (0.2811)	
training:	Epoch: [14][87/233]	Loss 0.3563 (0.2820)	
training:	Epoch: [14][88/233]	Loss 0.4579 (0.2840)	
training:	Epoch: [14][89/233]	Loss 0.3585 (0.2848)	
training:	Epoch: [14][90/233]	Loss 0.2267 (0.2842)	
training:	Epoch: [14][91/233]	Loss 0.2506 (0.2838)	
training:	Epoch: [14][92/233]	Loss 0.3372 (0.2844)	
training:	Epoch: [14][93/233]	Loss 0.4368 (0.2860)	
training:	Epoch: [14][94/233]	Loss 0.4573 (0.2878)	
training:	Epoch: [14][95/233]	Loss 0.1447 (0.2863)	
training:	Epoch: [14][96/233]	Loss 0.3130 (0.2866)	
training:	Epoch: [14][97/233]	Loss 0.2393 (0.2861)	
training:	Epoch: [14][98/233]	Loss 0.4514 (0.2878)	
training:	Epoch: [14][99/233]	Loss 0.3116 (0.2881)	
training:	Epoch: [14][100/233]	Loss 0.4075 (0.2892)	
training:	Epoch: [14][101/233]	Loss 0.2327 (0.2887)	
training:	Epoch: [14][102/233]	Loss 0.3228 (0.2890)	
training:	Epoch: [14][103/233]	Loss 0.1901 (0.2881)	
training:	Epoch: [14][104/233]	Loss 0.1671 (0.2869)	
training:	Epoch: [14][105/233]	Loss 0.1665 (0.2858)	
training:	Epoch: [14][106/233]	Loss 0.0889 (0.2839)	
training:	Epoch: [14][107/233]	Loss 0.1453 (0.2826)	
training:	Epoch: [14][108/233]	Loss 0.1753 (0.2816)	
training:	Epoch: [14][109/233]	Loss 0.2774 (0.2816)	
training:	Epoch: [14][110/233]	Loss 0.3626 (0.2823)	
training:	Epoch: [14][111/233]	Loss 0.2570 (0.2821)	
training:	Epoch: [14][112/233]	Loss 0.3340 (0.2825)	
training:	Epoch: [14][113/233]	Loss 0.1843 (0.2817)	
training:	Epoch: [14][114/233]	Loss 0.1894 (0.2809)	
training:	Epoch: [14][115/233]	Loss 0.1933 (0.2801)	
training:	Epoch: [14][116/233]	Loss 0.3007 (0.2803)	
training:	Epoch: [14][117/233]	Loss 0.3278 (0.2807)	
training:	Epoch: [14][118/233]	Loss 0.4840 (0.2824)	
training:	Epoch: [14][119/233]	Loss 0.1219 (0.2811)	
training:	Epoch: [14][120/233]	Loss 0.1193 (0.2797)	
training:	Epoch: [14][121/233]	Loss 0.2679 (0.2796)	
training:	Epoch: [14][122/233]	Loss 0.1538 (0.2786)	
training:	Epoch: [14][123/233]	Loss 0.1526 (0.2776)	
training:	Epoch: [14][124/233]	Loss 0.2425 (0.2773)	
training:	Epoch: [14][125/233]	Loss 0.2216 (0.2768)	
training:	Epoch: [14][126/233]	Loss 0.2249 (0.2764)	
training:	Epoch: [14][127/233]	Loss 0.2570 (0.2763)	
training:	Epoch: [14][128/233]	Loss 0.3294 (0.2767)	
training:	Epoch: [14][129/233]	Loss 0.3291 (0.2771)	
training:	Epoch: [14][130/233]	Loss 0.1806 (0.2763)	
training:	Epoch: [14][131/233]	Loss 0.1503 (0.2754)	
training:	Epoch: [14][132/233]	Loss 0.3341 (0.2758)	
training:	Epoch: [14][133/233]	Loss 0.2180 (0.2754)	
training:	Epoch: [14][134/233]	Loss 0.2517 (0.2752)	
training:	Epoch: [14][135/233]	Loss 0.2553 (0.2751)	
training:	Epoch: [14][136/233]	Loss 0.1880 (0.2744)	
training:	Epoch: [14][137/233]	Loss 0.2785 (0.2745)	
training:	Epoch: [14][138/233]	Loss 0.1239 (0.2734)	
training:	Epoch: [14][139/233]	Loss 0.2546 (0.2732)	
training:	Epoch: [14][140/233]	Loss 0.3845 (0.2740)	
training:	Epoch: [14][141/233]	Loss 0.2443 (0.2738)	
training:	Epoch: [14][142/233]	Loss 0.2213 (0.2734)	
training:	Epoch: [14][143/233]	Loss 0.1536 (0.2726)	
training:	Epoch: [14][144/233]	Loss 0.2682 (0.2726)	
training:	Epoch: [14][145/233]	Loss 0.4279 (0.2736)	
training:	Epoch: [14][146/233]	Loss 0.1597 (0.2729)	
training:	Epoch: [14][147/233]	Loss 0.3867 (0.2736)	
training:	Epoch: [14][148/233]	Loss 0.4950 (0.2751)	
training:	Epoch: [14][149/233]	Loss 0.3775 (0.2758)	
training:	Epoch: [14][150/233]	Loss 0.1842 (0.2752)	
training:	Epoch: [14][151/233]	Loss 0.1879 (0.2746)	
training:	Epoch: [14][152/233]	Loss 0.4684 (0.2759)	
training:	Epoch: [14][153/233]	Loss 0.2284 (0.2756)	
training:	Epoch: [14][154/233]	Loss 0.4022 (0.2764)	
training:	Epoch: [14][155/233]	Loss 0.1481 (0.2756)	
training:	Epoch: [14][156/233]	Loss 0.2819 (0.2756)	
training:	Epoch: [14][157/233]	Loss 0.1618 (0.2749)	
training:	Epoch: [14][158/233]	Loss 0.1250 (0.2740)	
training:	Epoch: [14][159/233]	Loss 0.4817 (0.2753)	
training:	Epoch: [14][160/233]	Loss 0.2858 (0.2753)	
training:	Epoch: [14][161/233]	Loss 0.3663 (0.2759)	
training:	Epoch: [14][162/233]	Loss 0.1276 (0.2750)	
training:	Epoch: [14][163/233]	Loss 0.4735 (0.2762)	
training:	Epoch: [14][164/233]	Loss 0.2891 (0.2763)	
training:	Epoch: [14][165/233]	Loss 0.4596 (0.2774)	
training:	Epoch: [14][166/233]	Loss 0.1862 (0.2768)	
training:	Epoch: [14][167/233]	Loss 0.3092 (0.2770)	
training:	Epoch: [14][168/233]	Loss 0.3716 (0.2776)	
training:	Epoch: [14][169/233]	Loss 0.1940 (0.2771)	
training:	Epoch: [14][170/233]	Loss 0.2095 (0.2767)	
training:	Epoch: [14][171/233]	Loss 0.1243 (0.2758)	
training:	Epoch: [14][172/233]	Loss 0.2832 (0.2759)	
training:	Epoch: [14][173/233]	Loss 0.2621 (0.2758)	
training:	Epoch: [14][174/233]	Loss 0.1888 (0.2753)	
training:	Epoch: [14][175/233]	Loss 0.3498 (0.2757)	
training:	Epoch: [14][176/233]	Loss 0.2571 (0.2756)	
training:	Epoch: [14][177/233]	Loss 0.1602 (0.2749)	
training:	Epoch: [14][178/233]	Loss 0.4138 (0.2757)	
training:	Epoch: [14][179/233]	Loss 0.3504 (0.2761)	
training:	Epoch: [14][180/233]	Loss 0.3047 (0.2763)	
training:	Epoch: [14][181/233]	Loss 0.3321 (0.2766)	
training:	Epoch: [14][182/233]	Loss 0.2268 (0.2763)	
training:	Epoch: [14][183/233]	Loss 0.2161 (0.2760)	
training:	Epoch: [14][184/233]	Loss 0.2665 (0.2760)	
training:	Epoch: [14][185/233]	Loss 0.3125 (0.2762)	
training:	Epoch: [14][186/233]	Loss 0.1860 (0.2757)	
training:	Epoch: [14][187/233]	Loss 0.2072 (0.2753)	
training:	Epoch: [14][188/233]	Loss 0.3689 (0.2758)	
training:	Epoch: [14][189/233]	Loss 0.2552 (0.2757)	
training:	Epoch: [14][190/233]	Loss 0.2073 (0.2753)	
training:	Epoch: [14][191/233]	Loss 0.2177 (0.2750)	
training:	Epoch: [14][192/233]	Loss 0.1718 (0.2745)	
training:	Epoch: [14][193/233]	Loss 0.4385 (0.2753)	
training:	Epoch: [14][194/233]	Loss 0.2932 (0.2754)	
training:	Epoch: [14][195/233]	Loss 0.5224 (0.2767)	
training:	Epoch: [14][196/233]	Loss 0.1846 (0.2762)	
training:	Epoch: [14][197/233]	Loss 0.2289 (0.2760)	
training:	Epoch: [14][198/233]	Loss 0.3780 (0.2765)	
training:	Epoch: [14][199/233]	Loss 0.5039 (0.2776)	
training:	Epoch: [14][200/233]	Loss 0.3856 (0.2782)	
training:	Epoch: [14][201/233]	Loss 0.2699 (0.2781)	
training:	Epoch: [14][202/233]	Loss 0.3370 (0.2784)	
training:	Epoch: [14][203/233]	Loss 0.4576 (0.2793)	
training:	Epoch: [14][204/233]	Loss 0.3359 (0.2796)	
training:	Epoch: [14][205/233]	Loss 0.1690 (0.2791)	
training:	Epoch: [14][206/233]	Loss 0.4068 (0.2797)	
training:	Epoch: [14][207/233]	Loss 0.2921 (0.2797)	
training:	Epoch: [14][208/233]	Loss 0.1376 (0.2791)	
training:	Epoch: [14][209/233]	Loss 0.3594 (0.2794)	
training:	Epoch: [14][210/233]	Loss 0.1115 (0.2786)	
training:	Epoch: [14][211/233]	Loss 0.2390 (0.2785)	
training:	Epoch: [14][212/233]	Loss 0.2304 (0.2782)	
training:	Epoch: [14][213/233]	Loss 0.1363 (0.2776)	
training:	Epoch: [14][214/233]	Loss 0.1279 (0.2769)	
training:	Epoch: [14][215/233]	Loss 0.2180 (0.2766)	
training:	Epoch: [14][216/233]	Loss 0.1668 (0.2761)	
training:	Epoch: [14][217/233]	Loss 0.2691 (0.2760)	
training:	Epoch: [14][218/233]	Loss 0.3620 (0.2764)	
training:	Epoch: [14][219/233]	Loss 0.1870 (0.2760)	
training:	Epoch: [14][220/233]	Loss 0.2271 (0.2758)	
training:	Epoch: [14][221/233]	Loss 0.4131 (0.2764)	
training:	Epoch: [14][222/233]	Loss 0.3430 (0.2767)	
training:	Epoch: [14][223/233]	Loss 0.2449 (0.2766)	
training:	Epoch: [14][224/233]	Loss 0.1648 (0.2761)	
training:	Epoch: [14][225/233]	Loss 0.3154 (0.2763)	
training:	Epoch: [14][226/233]	Loss 0.3373 (0.2765)	
training:	Epoch: [14][227/233]	Loss 0.3754 (0.2770)	
training:	Epoch: [14][228/233]	Loss 0.3893 (0.2775)	
training:	Epoch: [14][229/233]	Loss 0.1137 (0.2767)	
training:	Epoch: [14][230/233]	Loss 0.3185 (0.2769)	
training:	Epoch: [14][231/233]	Loss 0.2926 (0.2770)	
training:	Epoch: [14][232/233]	Loss 0.3027 (0.2771)	
training:	Epoch: [14][233/233]	Loss 0.2409 (0.2770)	
Training:	 Loss: 0.2763

Training:	 ACC: 0.9151 0.9150 0.9128 0.9174
Validation:	 ACC: 0.8146 0.8143 0.8100 0.8191
Validation:	 Best_BACC: 0.8162 0.8154 0.7998 0.8326
Validation:	 Loss: 0.4418
Pretraining:	Epoch 15/200
----------
training:	Epoch: [15][1/233]	Loss 0.3656 (0.3656)	
training:	Epoch: [15][2/233]	Loss 0.3633 (0.3645)	
training:	Epoch: [15][3/233]	Loss 0.2169 (0.3153)	
training:	Epoch: [15][4/233]	Loss 0.3517 (0.3244)	
training:	Epoch: [15][5/233]	Loss 0.3191 (0.3233)	
training:	Epoch: [15][6/233]	Loss 0.3957 (0.3354)	
training:	Epoch: [15][7/233]	Loss 0.2411 (0.3219)	
training:	Epoch: [15][8/233]	Loss 0.1915 (0.3056)	
training:	Epoch: [15][9/233]	Loss 0.2408 (0.2984)	
training:	Epoch: [15][10/233]	Loss 0.2163 (0.2902)	
training:	Epoch: [15][11/233]	Loss 0.2755 (0.2889)	
training:	Epoch: [15][12/233]	Loss 0.1218 (0.2750)	
training:	Epoch: [15][13/233]	Loss 0.2311 (0.2716)	
training:	Epoch: [15][14/233]	Loss 0.1696 (0.2643)	
training:	Epoch: [15][15/233]	Loss 0.3782 (0.2719)	
training:	Epoch: [15][16/233]	Loss 0.2714 (0.2719)	
training:	Epoch: [15][17/233]	Loss 0.1351 (0.2638)	
training:	Epoch: [15][18/233]	Loss 0.2064 (0.2606)	
training:	Epoch: [15][19/233]	Loss 0.3036 (0.2629)	
training:	Epoch: [15][20/233]	Loss 0.3998 (0.2697)	
training:	Epoch: [15][21/233]	Loss 0.1974 (0.2663)	
training:	Epoch: [15][22/233]	Loss 0.2149 (0.2640)	
training:	Epoch: [15][23/233]	Loss 0.1862 (0.2606)	
training:	Epoch: [15][24/233]	Loss 0.2099 (0.2585)	
training:	Epoch: [15][25/233]	Loss 0.1119 (0.2526)	
training:	Epoch: [15][26/233]	Loss 0.1966 (0.2504)	
training:	Epoch: [15][27/233]	Loss 0.2824 (0.2516)	
training:	Epoch: [15][28/233]	Loss 0.3232 (0.2542)	
training:	Epoch: [15][29/233]	Loss 0.3789 (0.2585)	
training:	Epoch: [15][30/233]	Loss 0.3143 (0.2603)	
training:	Epoch: [15][31/233]	Loss 0.2456 (0.2599)	
training:	Epoch: [15][32/233]	Loss 0.3073 (0.2614)	
training:	Epoch: [15][33/233]	Loss 0.3051 (0.2627)	
training:	Epoch: [15][34/233]	Loss 0.2338 (0.2618)	
training:	Epoch: [15][35/233]	Loss 0.3355 (0.2639)	
training:	Epoch: [15][36/233]	Loss 0.3990 (0.2677)	
training:	Epoch: [15][37/233]	Loss 0.2792 (0.2680)	
training:	Epoch: [15][38/233]	Loss 0.1966 (0.2661)	
training:	Epoch: [15][39/233]	Loss 0.1715 (0.2637)	
training:	Epoch: [15][40/233]	Loss 0.2917 (0.2644)	
training:	Epoch: [15][41/233]	Loss 0.2923 (0.2651)	
training:	Epoch: [15][42/233]	Loss 0.2617 (0.2650)	
training:	Epoch: [15][43/233]	Loss 0.2150 (0.2638)	
training:	Epoch: [15][44/233]	Loss 0.3385 (0.2655)	
training:	Epoch: [15][45/233]	Loss 0.3600 (0.2676)	
training:	Epoch: [15][46/233]	Loss 0.1218 (0.2645)	
training:	Epoch: [15][47/233]	Loss 0.2157 (0.2634)	
training:	Epoch: [15][48/233]	Loss 0.4556 (0.2674)	
training:	Epoch: [15][49/233]	Loss 0.2936 (0.2680)	
training:	Epoch: [15][50/233]	Loss 0.1997 (0.2666)	
training:	Epoch: [15][51/233]	Loss 0.2842 (0.2669)	
training:	Epoch: [15][52/233]	Loss 0.2537 (0.2667)	
training:	Epoch: [15][53/233]	Loss 0.1692 (0.2648)	
training:	Epoch: [15][54/233]	Loss 0.1786 (0.2632)	
training:	Epoch: [15][55/233]	Loss 0.2813 (0.2636)	
training:	Epoch: [15][56/233]	Loss 0.2292 (0.2630)	
training:	Epoch: [15][57/233]	Loss 0.2014 (0.2619)	
training:	Epoch: [15][58/233]	Loss 0.2347 (0.2614)	
training:	Epoch: [15][59/233]	Loss 0.3676 (0.2632)	
training:	Epoch: [15][60/233]	Loss 0.2678 (0.2633)	
training:	Epoch: [15][61/233]	Loss 0.2842 (0.2636)	
training:	Epoch: [15][62/233]	Loss 0.2286 (0.2631)	
training:	Epoch: [15][63/233]	Loss 0.3038 (0.2637)	
training:	Epoch: [15][64/233]	Loss 0.2837 (0.2640)	
training:	Epoch: [15][65/233]	Loss 0.3429 (0.2652)	
training:	Epoch: [15][66/233]	Loss 0.1870 (0.2641)	
training:	Epoch: [15][67/233]	Loss 0.2727 (0.2642)	
training:	Epoch: [15][68/233]	Loss 0.2500 (0.2640)	
training:	Epoch: [15][69/233]	Loss 0.2014 (0.2631)	
training:	Epoch: [15][70/233]	Loss 0.1495 (0.2614)	
training:	Epoch: [15][71/233]	Loss 0.1777 (0.2603)	
training:	Epoch: [15][72/233]	Loss 0.2447 (0.2601)	
training:	Epoch: [15][73/233]	Loss 0.2659 (0.2601)	
training:	Epoch: [15][74/233]	Loss 0.2438 (0.2599)	
training:	Epoch: [15][75/233]	Loss 0.1717 (0.2587)	
training:	Epoch: [15][76/233]	Loss 0.3156 (0.2595)	
training:	Epoch: [15][77/233]	Loss 0.2953 (0.2599)	
training:	Epoch: [15][78/233]	Loss 0.3094 (0.2606)	
training:	Epoch: [15][79/233]	Loss 0.2408 (0.2603)	
training:	Epoch: [15][80/233]	Loss 0.2261 (0.2599)	
training:	Epoch: [15][81/233]	Loss 0.2633 (0.2599)	
training:	Epoch: [15][82/233]	Loss 0.1572 (0.2587)	
training:	Epoch: [15][83/233]	Loss 0.1230 (0.2571)	
training:	Epoch: [15][84/233]	Loss 0.2905 (0.2575)	
training:	Epoch: [15][85/233]	Loss 0.4627 (0.2599)	
training:	Epoch: [15][86/233]	Loss 0.3291 (0.2607)	
training:	Epoch: [15][87/233]	Loss 0.4483 (0.2628)	
training:	Epoch: [15][88/233]	Loss 0.1863 (0.2620)	
training:	Epoch: [15][89/233]	Loss 0.1540 (0.2607)	
training:	Epoch: [15][90/233]	Loss 0.2428 (0.2606)	
training:	Epoch: [15][91/233]	Loss 0.1615 (0.2595)	
training:	Epoch: [15][92/233]	Loss 0.3482 (0.2604)	
training:	Epoch: [15][93/233]	Loss 0.4833 (0.2628)	
training:	Epoch: [15][94/233]	Loss 0.3570 (0.2638)	
training:	Epoch: [15][95/233]	Loss 0.1849 (0.2630)	
training:	Epoch: [15][96/233]	Loss 0.2163 (0.2625)	
training:	Epoch: [15][97/233]	Loss 0.4072 (0.2640)	
training:	Epoch: [15][98/233]	Loss 0.4487 (0.2659)	
training:	Epoch: [15][99/233]	Loss 0.5032 (0.2683)	
training:	Epoch: [15][100/233]	Loss 0.2503 (0.2681)	
training:	Epoch: [15][101/233]	Loss 0.1704 (0.2671)	
training:	Epoch: [15][102/233]	Loss 0.1146 (0.2656)	
training:	Epoch: [15][103/233]	Loss 0.2754 (0.2657)	
training:	Epoch: [15][104/233]	Loss 0.1609 (0.2647)	
training:	Epoch: [15][105/233]	Loss 0.4552 (0.2665)	
training:	Epoch: [15][106/233]	Loss 0.2959 (0.2668)	
training:	Epoch: [15][107/233]	Loss 0.2149 (0.2663)	
training:	Epoch: [15][108/233]	Loss 0.2320 (0.2660)	
training:	Epoch: [15][109/233]	Loss 0.2743 (0.2661)	
training:	Epoch: [15][110/233]	Loss 0.2637 (0.2661)	
training:	Epoch: [15][111/233]	Loss 0.2510 (0.2659)	
training:	Epoch: [15][112/233]	Loss 0.4641 (0.2677)	
training:	Epoch: [15][113/233]	Loss 0.2043 (0.2671)	
training:	Epoch: [15][114/233]	Loss 0.1628 (0.2662)	
training:	Epoch: [15][115/233]	Loss 0.2073 (0.2657)	
training:	Epoch: [15][116/233]	Loss 0.3178 (0.2662)	
training:	Epoch: [15][117/233]	Loss 0.2526 (0.2660)	
training:	Epoch: [15][118/233]	Loss 0.3547 (0.2668)	
training:	Epoch: [15][119/233]	Loss 0.2048 (0.2663)	
training:	Epoch: [15][120/233]	Loss 0.1406 (0.2652)	
training:	Epoch: [15][121/233]	Loss 0.1761 (0.2645)	
training:	Epoch: [15][122/233]	Loss 0.2545 (0.2644)	
training:	Epoch: [15][123/233]	Loss 0.2095 (0.2640)	
training:	Epoch: [15][124/233]	Loss 0.2898 (0.2642)	
training:	Epoch: [15][125/233]	Loss 0.1809 (0.2635)	
training:	Epoch: [15][126/233]	Loss 0.2853 (0.2637)	
training:	Epoch: [15][127/233]	Loss 0.1258 (0.2626)	
training:	Epoch: [15][128/233]	Loss 0.5015 (0.2645)	
training:	Epoch: [15][129/233]	Loss 0.1387 (0.2635)	
training:	Epoch: [15][130/233]	Loss 0.1867 (0.2629)	
training:	Epoch: [15][131/233]	Loss 0.4992 (0.2647)	
training:	Epoch: [15][132/233]	Loss 0.2036 (0.2642)	
training:	Epoch: [15][133/233]	Loss 0.2557 (0.2642)	
training:	Epoch: [15][134/233]	Loss 0.2522 (0.2641)	
training:	Epoch: [15][135/233]	Loss 0.2399 (0.2639)	
training:	Epoch: [15][136/233]	Loss 0.2844 (0.2641)	
training:	Epoch: [15][137/233]	Loss 0.1057 (0.2629)	
training:	Epoch: [15][138/233]	Loss 0.3819 (0.2638)	
training:	Epoch: [15][139/233]	Loss 0.3606 (0.2645)	
training:	Epoch: [15][140/233]	Loss 0.4142 (0.2655)	
training:	Epoch: [15][141/233]	Loss 0.2961 (0.2657)	
training:	Epoch: [15][142/233]	Loss 0.3207 (0.2661)	
training:	Epoch: [15][143/233]	Loss 0.1211 (0.2651)	
training:	Epoch: [15][144/233]	Loss 0.2546 (0.2650)	
training:	Epoch: [15][145/233]	Loss 0.1933 (0.2645)	
training:	Epoch: [15][146/233]	Loss 0.3799 (0.2653)	
training:	Epoch: [15][147/233]	Loss 0.3924 (0.2662)	
training:	Epoch: [15][148/233]	Loss 0.2687 (0.2662)	
training:	Epoch: [15][149/233]	Loss 0.2659 (0.2662)	
training:	Epoch: [15][150/233]	Loss 0.3112 (0.2665)	
training:	Epoch: [15][151/233]	Loss 0.3925 (0.2674)	
training:	Epoch: [15][152/233]	Loss 0.2944 (0.2675)	
training:	Epoch: [15][153/233]	Loss 0.2378 (0.2673)	
training:	Epoch: [15][154/233]	Loss 0.2672 (0.2673)	
training:	Epoch: [15][155/233]	Loss 0.1056 (0.2663)	
training:	Epoch: [15][156/233]	Loss 0.1331 (0.2654)	
training:	Epoch: [15][157/233]	Loss 0.3599 (0.2660)	
training:	Epoch: [15][158/233]	Loss 0.1997 (0.2656)	
training:	Epoch: [15][159/233]	Loss 0.3467 (0.2661)	
training:	Epoch: [15][160/233]	Loss 0.2571 (0.2661)	
training:	Epoch: [15][161/233]	Loss 0.3048 (0.2663)	
training:	Epoch: [15][162/233]	Loss 0.1506 (0.2656)	
training:	Epoch: [15][163/233]	Loss 0.2502 (0.2655)	
training:	Epoch: [15][164/233]	Loss 0.1444 (0.2648)	
training:	Epoch: [15][165/233]	Loss 0.2071 (0.2644)	
training:	Epoch: [15][166/233]	Loss 0.1352 (0.2636)	
training:	Epoch: [15][167/233]	Loss 0.1441 (0.2629)	
training:	Epoch: [15][168/233]	Loss 0.1009 (0.2620)	
training:	Epoch: [15][169/233]	Loss 0.3339 (0.2624)	
training:	Epoch: [15][170/233]	Loss 0.1639 (0.2618)	
training:	Epoch: [15][171/233]	Loss 0.1723 (0.2613)	
training:	Epoch: [15][172/233]	Loss 0.2625 (0.2613)	
training:	Epoch: [15][173/233]	Loss 0.1125 (0.2604)	
training:	Epoch: [15][174/233]	Loss 0.2776 (0.2605)	
training:	Epoch: [15][175/233]	Loss 0.1795 (0.2601)	
training:	Epoch: [15][176/233]	Loss 0.6096 (0.2620)	
training:	Epoch: [15][177/233]	Loss 0.1481 (0.2614)	
training:	Epoch: [15][178/233]	Loss 0.1593 (0.2608)	
training:	Epoch: [15][179/233]	Loss 0.2982 (0.2610)	
training:	Epoch: [15][180/233]	Loss 0.1573 (0.2605)	
training:	Epoch: [15][181/233]	Loss 0.1531 (0.2599)	
training:	Epoch: [15][182/233]	Loss 0.4784 (0.2611)	
training:	Epoch: [15][183/233]	Loss 0.2433 (0.2610)	
training:	Epoch: [15][184/233]	Loss 0.2083 (0.2607)	
training:	Epoch: [15][185/233]	Loss 0.5477 (0.2622)	
training:	Epoch: [15][186/233]	Loss 0.3173 (0.2625)	
training:	Epoch: [15][187/233]	Loss 0.2339 (0.2624)	
training:	Epoch: [15][188/233]	Loss 0.2946 (0.2626)	
training:	Epoch: [15][189/233]	Loss 0.2861 (0.2627)	
training:	Epoch: [15][190/233]	Loss 0.5907 (0.2644)	
training:	Epoch: [15][191/233]	Loss 0.3180 (0.2647)	
training:	Epoch: [15][192/233]	Loss 0.3238 (0.2650)	
training:	Epoch: [15][193/233]	Loss 0.2315 (0.2648)	
training:	Epoch: [15][194/233]	Loss 0.2402 (0.2647)	
training:	Epoch: [15][195/233]	Loss 0.2653 (0.2647)	
training:	Epoch: [15][196/233]	Loss 0.2071 (0.2644)	
training:	Epoch: [15][197/233]	Loss 0.3063 (0.2646)	
training:	Epoch: [15][198/233]	Loss 0.3326 (0.2650)	
training:	Epoch: [15][199/233]	Loss 0.2048 (0.2647)	
training:	Epoch: [15][200/233]	Loss 0.2435 (0.2646)	
training:	Epoch: [15][201/233]	Loss 0.4135 (0.2653)	
training:	Epoch: [15][202/233]	Loss 0.4990 (0.2664)	
training:	Epoch: [15][203/233]	Loss 0.1984 (0.2661)	
training:	Epoch: [15][204/233]	Loss 0.4301 (0.2669)	
training:	Epoch: [15][205/233]	Loss 0.2587 (0.2669)	
training:	Epoch: [15][206/233]	Loss 0.1668 (0.2664)	
training:	Epoch: [15][207/233]	Loss 0.1763 (0.2660)	
training:	Epoch: [15][208/233]	Loss 0.2930 (0.2661)	
training:	Epoch: [15][209/233]	Loss 0.3616 (0.2665)	
training:	Epoch: [15][210/233]	Loss 0.3436 (0.2669)	
training:	Epoch: [15][211/233]	Loss 0.2454 (0.2668)	
training:	Epoch: [15][212/233]	Loss 0.2554 (0.2668)	
training:	Epoch: [15][213/233]	Loss 0.1953 (0.2664)	
training:	Epoch: [15][214/233]	Loss 0.1162 (0.2657)	
training:	Epoch: [15][215/233]	Loss 0.2400 (0.2656)	
training:	Epoch: [15][216/233]	Loss 0.3301 (0.2659)	
training:	Epoch: [15][217/233]	Loss 0.2136 (0.2657)	
training:	Epoch: [15][218/233]	Loss 0.2440 (0.2656)	
training:	Epoch: [15][219/233]	Loss 0.3614 (0.2660)	
training:	Epoch: [15][220/233]	Loss 0.2263 (0.2658)	
training:	Epoch: [15][221/233]	Loss 0.3062 (0.2660)	
training:	Epoch: [15][222/233]	Loss 0.5355 (0.2672)	
training:	Epoch: [15][223/233]	Loss 0.1457 (0.2667)	
training:	Epoch: [15][224/233]	Loss 0.2378 (0.2665)	
training:	Epoch: [15][225/233]	Loss 0.3270 (0.2668)	
training:	Epoch: [15][226/233]	Loss 0.2635 (0.2668)	
training:	Epoch: [15][227/233]	Loss 0.2843 (0.2669)	
training:	Epoch: [15][228/233]	Loss 0.5183 (0.2680)	
training:	Epoch: [15][229/233]	Loss 0.2429 (0.2679)	
training:	Epoch: [15][230/233]	Loss 0.0582 (0.2669)	
training:	Epoch: [15][231/233]	Loss 0.2798 (0.2670)	
training:	Epoch: [15][232/233]	Loss 0.1396 (0.2665)	
training:	Epoch: [15][233/233]	Loss 0.1985 (0.2662)	
Training:	 Loss: 0.2656

Training:	 ACC: 0.9205 0.9197 0.9026 0.9384
Validation:	 ACC: 0.8128 0.8122 0.7998 0.8258
Validation:	 Best_BACC: 0.8162 0.8154 0.7998 0.8326
Validation:	 Loss: 0.4506
Pretraining:	Epoch 16/200
----------
training:	Epoch: [16][1/233]	Loss 0.0800 (0.0800)	
training:	Epoch: [16][2/233]	Loss 0.2897 (0.1849)	
training:	Epoch: [16][3/233]	Loss 0.1401 (0.1699)	
training:	Epoch: [16][4/233]	Loss 0.2302 (0.1850)	
training:	Epoch: [16][5/233]	Loss 0.2686 (0.2017)	
training:	Epoch: [16][6/233]	Loss 0.2539 (0.2104)	
training:	Epoch: [16][7/233]	Loss 0.1957 (0.2083)	
training:	Epoch: [16][8/233]	Loss 0.3133 (0.2214)	
training:	Epoch: [16][9/233]	Loss 0.1713 (0.2159)	
training:	Epoch: [16][10/233]	Loss 0.1835 (0.2126)	
training:	Epoch: [16][11/233]	Loss 0.3635 (0.2263)	
training:	Epoch: [16][12/233]	Loss 0.2289 (0.2266)	
training:	Epoch: [16][13/233]	Loss 0.2505 (0.2284)	
training:	Epoch: [16][14/233]	Loss 0.2347 (0.2288)	
training:	Epoch: [16][15/233]	Loss 0.1234 (0.2218)	
training:	Epoch: [16][16/233]	Loss 0.3213 (0.2280)	
training:	Epoch: [16][17/233]	Loss 0.2934 (0.2319)	
training:	Epoch: [16][18/233]	Loss 0.2424 (0.2325)	
training:	Epoch: [16][19/233]	Loss 0.2699 (0.2344)	
training:	Epoch: [16][20/233]	Loss 0.2244 (0.2339)	
training:	Epoch: [16][21/233]	Loss 0.2519 (0.2348)	
training:	Epoch: [16][22/233]	Loss 0.3201 (0.2387)	
training:	Epoch: [16][23/233]	Loss 0.1136 (0.2332)	
training:	Epoch: [16][24/233]	Loss 0.2346 (0.2333)	
training:	Epoch: [16][25/233]	Loss 0.2765 (0.2350)	
training:	Epoch: [16][26/233]	Loss 0.1500 (0.2317)	
training:	Epoch: [16][27/233]	Loss 0.4153 (0.2385)	
training:	Epoch: [16][28/233]	Loss 0.1476 (0.2353)	
training:	Epoch: [16][29/233]	Loss 0.3744 (0.2401)	
training:	Epoch: [16][30/233]	Loss 0.2743 (0.2412)	
training:	Epoch: [16][31/233]	Loss 0.1682 (0.2389)	
training:	Epoch: [16][32/233]	Loss 0.3733 (0.2431)	
training:	Epoch: [16][33/233]	Loss 0.2328 (0.2428)	
training:	Epoch: [16][34/233]	Loss 0.1919 (0.2413)	
training:	Epoch: [16][35/233]	Loss 0.3788 (0.2452)	
training:	Epoch: [16][36/233]	Loss 0.2817 (0.2462)	
training:	Epoch: [16][37/233]	Loss 0.3266 (0.2484)	
training:	Epoch: [16][38/233]	Loss 0.1912 (0.2469)	
training:	Epoch: [16][39/233]	Loss 0.2657 (0.2474)	
training:	Epoch: [16][40/233]	Loss 0.2057 (0.2463)	
training:	Epoch: [16][41/233]	Loss 0.2924 (0.2474)	
training:	Epoch: [16][42/233]	Loss 0.3417 (0.2497)	
training:	Epoch: [16][43/233]	Loss 0.2191 (0.2490)	
training:	Epoch: [16][44/233]	Loss 0.2616 (0.2493)	
training:	Epoch: [16][45/233]	Loss 0.1289 (0.2466)	
training:	Epoch: [16][46/233]	Loss 0.2282 (0.2462)	
training:	Epoch: [16][47/233]	Loss 0.2630 (0.2465)	
training:	Epoch: [16][48/233]	Loss 0.6889 (0.2558)	
training:	Epoch: [16][49/233]	Loss 0.1748 (0.2541)	
training:	Epoch: [16][50/233]	Loss 0.2010 (0.2530)	
training:	Epoch: [16][51/233]	Loss 0.1599 (0.2512)	
training:	Epoch: [16][52/233]	Loss 0.1650 (0.2496)	
training:	Epoch: [16][53/233]	Loss 0.1713 (0.2481)	
training:	Epoch: [16][54/233]	Loss 0.2265 (0.2477)	
training:	Epoch: [16][55/233]	Loss 0.3715 (0.2499)	
training:	Epoch: [16][56/233]	Loss 0.2463 (0.2499)	
training:	Epoch: [16][57/233]	Loss 0.1999 (0.2490)	
training:	Epoch: [16][58/233]	Loss 0.0860 (0.2462)	
training:	Epoch: [16][59/233]	Loss 0.3803 (0.2485)	
training:	Epoch: [16][60/233]	Loss 0.3263 (0.2498)	
training:	Epoch: [16][61/233]	Loss 0.3825 (0.2519)	
training:	Epoch: [16][62/233]	Loss 0.1829 (0.2508)	
training:	Epoch: [16][63/233]	Loss 0.2228 (0.2504)	
training:	Epoch: [16][64/233]	Loss 0.3554 (0.2520)	
training:	Epoch: [16][65/233]	Loss 0.2825 (0.2525)	
training:	Epoch: [16][66/233]	Loss 0.1463 (0.2509)	
training:	Epoch: [16][67/233]	Loss 0.2907 (0.2515)	
training:	Epoch: [16][68/233]	Loss 0.3103 (0.2523)	
training:	Epoch: [16][69/233]	Loss 0.1245 (0.2505)	
training:	Epoch: [16][70/233]	Loss 0.1985 (0.2497)	
training:	Epoch: [16][71/233]	Loss 0.3190 (0.2507)	
training:	Epoch: [16][72/233]	Loss 0.1616 (0.2495)	
training:	Epoch: [16][73/233]	Loss 0.2410 (0.2494)	
training:	Epoch: [16][74/233]	Loss 0.2056 (0.2488)	
training:	Epoch: [16][75/233]	Loss 0.3044 (0.2495)	
training:	Epoch: [16][76/233]	Loss 0.2246 (0.2492)	
training:	Epoch: [16][77/233]	Loss 0.2975 (0.2498)	
training:	Epoch: [16][78/233]	Loss 0.2715 (0.2501)	
training:	Epoch: [16][79/233]	Loss 0.0685 (0.2478)	
training:	Epoch: [16][80/233]	Loss 0.2656 (0.2480)	
training:	Epoch: [16][81/233]	Loss 0.2546 (0.2481)	
training:	Epoch: [16][82/233]	Loss 0.2042 (0.2476)	
training:	Epoch: [16][83/233]	Loss 0.1935 (0.2469)	
training:	Epoch: [16][84/233]	Loss 0.2147 (0.2465)	
training:	Epoch: [16][85/233]	Loss 0.2433 (0.2465)	
training:	Epoch: [16][86/233]	Loss 0.2783 (0.2469)	
training:	Epoch: [16][87/233]	Loss 0.1617 (0.2459)	
training:	Epoch: [16][88/233]	Loss 0.1852 (0.2452)	
training:	Epoch: [16][89/233]	Loss 0.1975 (0.2447)	
training:	Epoch: [16][90/233]	Loss 0.1399 (0.2435)	
training:	Epoch: [16][91/233]	Loss 0.2563 (0.2436)	
training:	Epoch: [16][92/233]	Loss 0.3081 (0.2443)	
training:	Epoch: [16][93/233]	Loss 0.2643 (0.2445)	
training:	Epoch: [16][94/233]	Loss 0.1102 (0.2431)	
training:	Epoch: [16][95/233]	Loss 0.4549 (0.2453)	
training:	Epoch: [16][96/233]	Loss 0.2161 (0.2450)	
training:	Epoch: [16][97/233]	Loss 0.4243 (0.2469)	
training:	Epoch: [16][98/233]	Loss 0.3522 (0.2480)	
training:	Epoch: [16][99/233]	Loss 0.3515 (0.2490)	
training:	Epoch: [16][100/233]	Loss 0.2814 (0.2493)	
training:	Epoch: [16][101/233]	Loss 0.2724 (0.2496)	
training:	Epoch: [16][102/233]	Loss 0.3158 (0.2502)	
training:	Epoch: [16][103/233]	Loss 0.2714 (0.2504)	
training:	Epoch: [16][104/233]	Loss 0.2157 (0.2501)	
training:	Epoch: [16][105/233]	Loss 0.2454 (0.2500)	
training:	Epoch: [16][106/233]	Loss 0.1255 (0.2489)	
training:	Epoch: [16][107/233]	Loss 0.2870 (0.2492)	
training:	Epoch: [16][108/233]	Loss 0.1785 (0.2486)	
training:	Epoch: [16][109/233]	Loss 0.1957 (0.2481)	
training:	Epoch: [16][110/233]	Loss 0.1862 (0.2475)	
training:	Epoch: [16][111/233]	Loss 0.2170 (0.2472)	
training:	Epoch: [16][112/233]	Loss 0.2462 (0.2472)	
training:	Epoch: [16][113/233]	Loss 0.4869 (0.2494)	
training:	Epoch: [16][114/233]	Loss 0.2992 (0.2498)	
training:	Epoch: [16][115/233]	Loss 0.1886 (0.2493)	
training:	Epoch: [16][116/233]	Loss 0.3653 (0.2503)	
training:	Epoch: [16][117/233]	Loss 0.2170 (0.2500)	
training:	Epoch: [16][118/233]	Loss 0.2558 (0.2500)	
training:	Epoch: [16][119/233]	Loss 0.2215 (0.2498)	
training:	Epoch: [16][120/233]	Loss 0.2004 (0.2494)	
training:	Epoch: [16][121/233]	Loss 0.3125 (0.2499)	
training:	Epoch: [16][122/233]	Loss 0.2592 (0.2500)	
training:	Epoch: [16][123/233]	Loss 0.2546 (0.2500)	
training:	Epoch: [16][124/233]	Loss 0.1108 (0.2489)	
training:	Epoch: [16][125/233]	Loss 0.2541 (0.2489)	
training:	Epoch: [16][126/233]	Loss 0.1300 (0.2480)	
training:	Epoch: [16][127/233]	Loss 0.2000 (0.2476)	
training:	Epoch: [16][128/233]	Loss 0.2842 (0.2479)	
training:	Epoch: [16][129/233]	Loss 0.3651 (0.2488)	
training:	Epoch: [16][130/233]	Loss 0.2100 (0.2485)	
training:	Epoch: [16][131/233]	Loss 0.1475 (0.2477)	
training:	Epoch: [16][132/233]	Loss 0.2072 (0.2474)	
training:	Epoch: [16][133/233]	Loss 0.2402 (0.2474)	
training:	Epoch: [16][134/233]	Loss 0.1413 (0.2466)	
training:	Epoch: [16][135/233]	Loss 0.3087 (0.2470)	
training:	Epoch: [16][136/233]	Loss 0.2878 (0.2473)	
training:	Epoch: [16][137/233]	Loss 0.3957 (0.2484)	
training:	Epoch: [16][138/233]	Loss 0.4347 (0.2498)	
training:	Epoch: [16][139/233]	Loss 0.3345 (0.2504)	
training:	Epoch: [16][140/233]	Loss 0.2554 (0.2504)	
training:	Epoch: [16][141/233]	Loss 0.1907 (0.2500)	
training:	Epoch: [16][142/233]	Loss 0.2187 (0.2498)	
training:	Epoch: [16][143/233]	Loss 0.2462 (0.2497)	
training:	Epoch: [16][144/233]	Loss 0.2861 (0.2500)	
training:	Epoch: [16][145/233]	Loss 0.1025 (0.2490)	
training:	Epoch: [16][146/233]	Loss 0.0978 (0.2479)	
training:	Epoch: [16][147/233]	Loss 0.2602 (0.2480)	
training:	Epoch: [16][148/233]	Loss 0.2911 (0.2483)	
training:	Epoch: [16][149/233]	Loss 0.2095 (0.2481)	
training:	Epoch: [16][150/233]	Loss 0.2056 (0.2478)	
training:	Epoch: [16][151/233]	Loss 0.2040 (0.2475)	
training:	Epoch: [16][152/233]	Loss 0.2867 (0.2477)	
training:	Epoch: [16][153/233]	Loss 0.1161 (0.2469)	
training:	Epoch: [16][154/233]	Loss 0.2831 (0.2471)	
training:	Epoch: [16][155/233]	Loss 0.2080 (0.2469)	
training:	Epoch: [16][156/233]	Loss 0.2452 (0.2469)	
training:	Epoch: [16][157/233]	Loss 0.2989 (0.2472)	
training:	Epoch: [16][158/233]	Loss 0.1316 (0.2465)	
training:	Epoch: [16][159/233]	Loss 0.1941 (0.2461)	
training:	Epoch: [16][160/233]	Loss 0.3121 (0.2465)	
training:	Epoch: [16][161/233]	Loss 0.2000 (0.2463)	
training:	Epoch: [16][162/233]	Loss 0.3696 (0.2470)	
training:	Epoch: [16][163/233]	Loss 0.3007 (0.2473)	
training:	Epoch: [16][164/233]	Loss 0.4392 (0.2485)	
training:	Epoch: [16][165/233]	Loss 0.2146 (0.2483)	
training:	Epoch: [16][166/233]	Loss 0.1457 (0.2477)	
training:	Epoch: [16][167/233]	Loss 0.1772 (0.2473)	
training:	Epoch: [16][168/233]	Loss 0.1417 (0.2466)	
training:	Epoch: [16][169/233]	Loss 0.4015 (0.2476)	
training:	Epoch: [16][170/233]	Loss 0.2277 (0.2474)	
training:	Epoch: [16][171/233]	Loss 0.3941 (0.2483)	
training:	Epoch: [16][172/233]	Loss 0.2885 (0.2485)	
training:	Epoch: [16][173/233]	Loss 0.2886 (0.2488)	
training:	Epoch: [16][174/233]	Loss 0.2964 (0.2490)	
training:	Epoch: [16][175/233]	Loss 0.3513 (0.2496)	
training:	Epoch: [16][176/233]	Loss 0.1680 (0.2492)	
training:	Epoch: [16][177/233]	Loss 0.3733 (0.2499)	
training:	Epoch: [16][178/233]	Loss 0.3017 (0.2501)	
training:	Epoch: [16][179/233]	Loss 0.4609 (0.2513)	
training:	Epoch: [16][180/233]	Loss 0.1846 (0.2510)	
training:	Epoch: [16][181/233]	Loss 0.3607 (0.2516)	
training:	Epoch: [16][182/233]	Loss 0.2686 (0.2517)	
training:	Epoch: [16][183/233]	Loss 0.1613 (0.2512)	
training:	Epoch: [16][184/233]	Loss 0.1876 (0.2508)	
training:	Epoch: [16][185/233]	Loss 0.2263 (0.2507)	
training:	Epoch: [16][186/233]	Loss 0.1707 (0.2503)	
training:	Epoch: [16][187/233]	Loss 0.2951 (0.2505)	
training:	Epoch: [16][188/233]	Loss 0.1522 (0.2500)	
training:	Epoch: [16][189/233]	Loss 0.1963 (0.2497)	
training:	Epoch: [16][190/233]	Loss 0.3457 (0.2502)	
training:	Epoch: [16][191/233]	Loss 0.2673 (0.2503)	
training:	Epoch: [16][192/233]	Loss 0.4213 (0.2512)	
training:	Epoch: [16][193/233]	Loss 0.1908 (0.2509)	
training:	Epoch: [16][194/233]	Loss 0.2186 (0.2507)	
training:	Epoch: [16][195/233]	Loss 0.2355 (0.2506)	
training:	Epoch: [16][196/233]	Loss 0.1882 (0.2503)	
training:	Epoch: [16][197/233]	Loss 0.0828 (0.2494)	
training:	Epoch: [16][198/233]	Loss 0.4898 (0.2507)	
training:	Epoch: [16][199/233]	Loss 0.2428 (0.2506)	
training:	Epoch: [16][200/233]	Loss 0.1596 (0.2502)	
training:	Epoch: [16][201/233]	Loss 0.1627 (0.2497)	
training:	Epoch: [16][202/233]	Loss 0.2821 (0.2499)	
training:	Epoch: [16][203/233]	Loss 0.4072 (0.2507)	
training:	Epoch: [16][204/233]	Loss 0.3541 (0.2512)	
training:	Epoch: [16][205/233]	Loss 0.2326 (0.2511)	
training:	Epoch: [16][206/233]	Loss 0.2218 (0.2509)	
training:	Epoch: [16][207/233]	Loss 0.3797 (0.2516)	
training:	Epoch: [16][208/233]	Loss 0.1193 (0.2509)	
training:	Epoch: [16][209/233]	Loss 0.2938 (0.2511)	
training:	Epoch: [16][210/233]	Loss 0.1740 (0.2508)	
training:	Epoch: [16][211/233]	Loss 0.2232 (0.2506)	
training:	Epoch: [16][212/233]	Loss 0.3927 (0.2513)	
training:	Epoch: [16][213/233]	Loss 0.2382 (0.2512)	
training:	Epoch: [16][214/233]	Loss 0.2300 (0.2511)	
training:	Epoch: [16][215/233]	Loss 0.4257 (0.2520)	
training:	Epoch: [16][216/233]	Loss 0.4569 (0.2529)	
training:	Epoch: [16][217/233]	Loss 0.2475 (0.2529)	
training:	Epoch: [16][218/233]	Loss 0.3535 (0.2533)	
training:	Epoch: [16][219/233]	Loss 0.2430 (0.2533)	
training:	Epoch: [16][220/233]	Loss 0.1843 (0.2530)	
training:	Epoch: [16][221/233]	Loss 0.3213 (0.2533)	
training:	Epoch: [16][222/233]	Loss 0.3139 (0.2536)	
training:	Epoch: [16][223/233]	Loss 0.2583 (0.2536)	
training:	Epoch: [16][224/233]	Loss 0.3404 (0.2540)	
training:	Epoch: [16][225/233]	Loss 0.1628 (0.2536)	
training:	Epoch: [16][226/233]	Loss 0.1913 (0.2533)	
training:	Epoch: [16][227/233]	Loss 0.3675 (0.2538)	
training:	Epoch: [16][228/233]	Loss 0.1863 (0.2535)	
training:	Epoch: [16][229/233]	Loss 0.1816 (0.2532)	
training:	Epoch: [16][230/233]	Loss 0.1735 (0.2528)	
training:	Epoch: [16][231/233]	Loss 0.2986 (0.2530)	
training:	Epoch: [16][232/233]	Loss 0.3297 (0.2534)	
training:	Epoch: [16][233/233]	Loss 0.2253 (0.2532)	
Training:	 Loss: 0.2527

Training:	 ACC: 0.9307 0.9305 0.9279 0.9334
Validation:	 ACC: 0.8101 0.8101 0.8100 0.8101
Validation:	 Best_BACC: 0.8162 0.8154 0.7998 0.8326
Validation:	 Loss: 0.4603
Pretraining:	Epoch 17/200
----------
training:	Epoch: [17][1/233]	Loss 0.2834 (0.2834)	
training:	Epoch: [17][2/233]	Loss 0.3127 (0.2981)	
training:	Epoch: [17][3/233]	Loss 0.3875 (0.3279)	
training:	Epoch: [17][4/233]	Loss 0.2273 (0.3027)	
training:	Epoch: [17][5/233]	Loss 0.1480 (0.2718)	
training:	Epoch: [17][6/233]	Loss 0.1749 (0.2556)	
training:	Epoch: [17][7/233]	Loss 0.1217 (0.2365)	
training:	Epoch: [17][8/233]	Loss 0.3671 (0.2528)	
training:	Epoch: [17][9/233]	Loss 0.0976 (0.2356)	
training:	Epoch: [17][10/233]	Loss 0.2669 (0.2387)	
training:	Epoch: [17][11/233]	Loss 0.3933 (0.2528)	
training:	Epoch: [17][12/233]	Loss 0.1313 (0.2426)	
training:	Epoch: [17][13/233]	Loss 0.3150 (0.2482)	
training:	Epoch: [17][14/233]	Loss 0.1667 (0.2424)	
training:	Epoch: [17][15/233]	Loss 0.4252 (0.2546)	
training:	Epoch: [17][16/233]	Loss 0.2439 (0.2539)	
training:	Epoch: [17][17/233]	Loss 0.3022 (0.2567)	
training:	Epoch: [17][18/233]	Loss 0.2001 (0.2536)	
training:	Epoch: [17][19/233]	Loss 0.3620 (0.2593)	
training:	Epoch: [17][20/233]	Loss 0.2664 (0.2597)	
training:	Epoch: [17][21/233]	Loss 0.0572 (0.2500)	
training:	Epoch: [17][22/233]	Loss 0.2380 (0.2495)	
training:	Epoch: [17][23/233]	Loss 0.1274 (0.2442)	
training:	Epoch: [17][24/233]	Loss 0.2456 (0.2442)	
training:	Epoch: [17][25/233]	Loss 0.2952 (0.2463)	
training:	Epoch: [17][26/233]	Loss 0.2853 (0.2478)	
training:	Epoch: [17][27/233]	Loss 0.1355 (0.2436)	
training:	Epoch: [17][28/233]	Loss 0.4070 (0.2494)	
training:	Epoch: [17][29/233]	Loss 0.1622 (0.2464)	
training:	Epoch: [17][30/233]	Loss 0.1310 (0.2426)	
training:	Epoch: [17][31/233]	Loss 0.2666 (0.2434)	
training:	Epoch: [17][32/233]	Loss 0.1966 (0.2419)	
training:	Epoch: [17][33/233]	Loss 0.1454 (0.2390)	
training:	Epoch: [17][34/233]	Loss 0.2018 (0.2379)	
training:	Epoch: [17][35/233]	Loss 0.3194 (0.2402)	
training:	Epoch: [17][36/233]	Loss 0.1416 (0.2375)	
training:	Epoch: [17][37/233]	Loss 0.2949 (0.2390)	
training:	Epoch: [17][38/233]	Loss 0.2768 (0.2400)	
training:	Epoch: [17][39/233]	Loss 0.3064 (0.2417)	
training:	Epoch: [17][40/233]	Loss 0.2895 (0.2429)	
training:	Epoch: [17][41/233]	Loss 0.1613 (0.2409)	
training:	Epoch: [17][42/233]	Loss 0.2647 (0.2415)	
training:	Epoch: [17][43/233]	Loss 0.4317 (0.2459)	
training:	Epoch: [17][44/233]	Loss 0.1607 (0.2440)	
training:	Epoch: [17][45/233]	Loss 0.2187 (0.2434)	
training:	Epoch: [17][46/233]	Loss 0.1685 (0.2418)	
training:	Epoch: [17][47/233]	Loss 0.1894 (0.2407)	
training:	Epoch: [17][48/233]	Loss 0.1301 (0.2384)	
training:	Epoch: [17][49/233]	Loss 0.2916 (0.2395)	
training:	Epoch: [17][50/233]	Loss 0.1125 (0.2369)	
training:	Epoch: [17][51/233]	Loss 0.2861 (0.2379)	
training:	Epoch: [17][52/233]	Loss 0.3491 (0.2400)	
training:	Epoch: [17][53/233]	Loss 0.2656 (0.2405)	
training:	Epoch: [17][54/233]	Loss 0.1779 (0.2393)	
training:	Epoch: [17][55/233]	Loss 0.5296 (0.2446)	
training:	Epoch: [17][56/233]	Loss 0.1161 (0.2423)	
training:	Epoch: [17][57/233]	Loss 0.3413 (0.2441)	
training:	Epoch: [17][58/233]	Loss 0.2626 (0.2444)	
training:	Epoch: [17][59/233]	Loss 0.2176 (0.2439)	
training:	Epoch: [17][60/233]	Loss 0.3546 (0.2458)	
training:	Epoch: [17][61/233]	Loss 0.1093 (0.2435)	
training:	Epoch: [17][62/233]	Loss 0.3078 (0.2446)	
training:	Epoch: [17][63/233]	Loss 0.2622 (0.2449)	
training:	Epoch: [17][64/233]	Loss 0.2442 (0.2448)	
training:	Epoch: [17][65/233]	Loss 0.2429 (0.2448)	
training:	Epoch: [17][66/233]	Loss 0.3246 (0.2460)	
training:	Epoch: [17][67/233]	Loss 0.3204 (0.2471)	
training:	Epoch: [17][68/233]	Loss 0.2258 (0.2468)	
training:	Epoch: [17][69/233]	Loss 0.4093 (0.2492)	
training:	Epoch: [17][70/233]	Loss 0.1242 (0.2474)	
training:	Epoch: [17][71/233]	Loss 0.4759 (0.2506)	
training:	Epoch: [17][72/233]	Loss 0.1643 (0.2494)	
training:	Epoch: [17][73/233]	Loss 0.2142 (0.2489)	
training:	Epoch: [17][74/233]	Loss 0.4233 (0.2513)	
training:	Epoch: [17][75/233]	Loss 0.1042 (0.2493)	
training:	Epoch: [17][76/233]	Loss 0.2274 (0.2490)	
training:	Epoch: [17][77/233]	Loss 0.1550 (0.2478)	
training:	Epoch: [17][78/233]	Loss 0.1762 (0.2469)	
training:	Epoch: [17][79/233]	Loss 0.2558 (0.2470)	
training:	Epoch: [17][80/233]	Loss 0.2001 (0.2464)	
training:	Epoch: [17][81/233]	Loss 0.4315 (0.2487)	
training:	Epoch: [17][82/233]	Loss 0.3219 (0.2496)	
training:	Epoch: [17][83/233]	Loss 0.2668 (0.2498)	
training:	Epoch: [17][84/233]	Loss 0.0738 (0.2477)	
training:	Epoch: [17][85/233]	Loss 0.2368 (0.2476)	
training:	Epoch: [17][86/233]	Loss 0.1688 (0.2467)	
training:	Epoch: [17][87/233]	Loss 0.2746 (0.2470)	
training:	Epoch: [17][88/233]	Loss 0.2197 (0.2467)	
training:	Epoch: [17][89/233]	Loss 0.2437 (0.2466)	
training:	Epoch: [17][90/233]	Loss 0.2943 (0.2472)	
training:	Epoch: [17][91/233]	Loss 0.1659 (0.2463)	
training:	Epoch: [17][92/233]	Loss 0.0959 (0.2446)	
training:	Epoch: [17][93/233]	Loss 0.3330 (0.2456)	
training:	Epoch: [17][94/233]	Loss 0.1139 (0.2442)	
training:	Epoch: [17][95/233]	Loss 0.1441 (0.2431)	
training:	Epoch: [17][96/233]	Loss 0.1951 (0.2426)	
training:	Epoch: [17][97/233]	Loss 0.1505 (0.2417)	
training:	Epoch: [17][98/233]	Loss 0.1419 (0.2407)	
training:	Epoch: [17][99/233]	Loss 0.1433 (0.2397)	
training:	Epoch: [17][100/233]	Loss 0.2075 (0.2394)	
training:	Epoch: [17][101/233]	Loss 0.2121 (0.2391)	
training:	Epoch: [17][102/233]	Loss 0.4844 (0.2415)	
training:	Epoch: [17][103/233]	Loss 0.2728 (0.2418)	
training:	Epoch: [17][104/233]	Loss 0.2665 (0.2420)	
training:	Epoch: [17][105/233]	Loss 0.3040 (0.2426)	
training:	Epoch: [17][106/233]	Loss 0.1548 (0.2418)	
training:	Epoch: [17][107/233]	Loss 0.3938 (0.2432)	
training:	Epoch: [17][108/233]	Loss 0.2619 (0.2434)	
training:	Epoch: [17][109/233]	Loss 0.2475 (0.2434)	
training:	Epoch: [17][110/233]	Loss 0.2993 (0.2439)	
training:	Epoch: [17][111/233]	Loss 0.1649 (0.2432)	
training:	Epoch: [17][112/233]	Loss 0.2073 (0.2429)	
training:	Epoch: [17][113/233]	Loss 0.2623 (0.2431)	
training:	Epoch: [17][114/233]	Loss 0.2377 (0.2430)	
training:	Epoch: [17][115/233]	Loss 0.0903 (0.2417)	
training:	Epoch: [17][116/233]	Loss 0.3525 (0.2427)	
training:	Epoch: [17][117/233]	Loss 0.2418 (0.2427)	
training:	Epoch: [17][118/233]	Loss 0.2687 (0.2429)	
training:	Epoch: [17][119/233]	Loss 0.2887 (0.2433)	
training:	Epoch: [17][120/233]	Loss 0.2531 (0.2433)	
training:	Epoch: [17][121/233]	Loss 0.3439 (0.2442)	
training:	Epoch: [17][122/233]	Loss 0.2204 (0.2440)	
training:	Epoch: [17][123/233]	Loss 0.1305 (0.2431)	
training:	Epoch: [17][124/233]	Loss 0.3507 (0.2439)	
training:	Epoch: [17][125/233]	Loss 0.1032 (0.2428)	
training:	Epoch: [17][126/233]	Loss 0.1524 (0.2421)	
training:	Epoch: [17][127/233]	Loss 0.3338 (0.2428)	
training:	Epoch: [17][128/233]	Loss 0.2513 (0.2429)	
training:	Epoch: [17][129/233]	Loss 0.1937 (0.2425)	
training:	Epoch: [17][130/233]	Loss 0.3493 (0.2433)	
training:	Epoch: [17][131/233]	Loss 0.3867 (0.2444)	
training:	Epoch: [17][132/233]	Loss 0.1903 (0.2440)	
training:	Epoch: [17][133/233]	Loss 0.3014 (0.2444)	
training:	Epoch: [17][134/233]	Loss 0.1298 (0.2436)	
training:	Epoch: [17][135/233]	Loss 0.1486 (0.2429)	
training:	Epoch: [17][136/233]	Loss 0.3465 (0.2436)	
training:	Epoch: [17][137/233]	Loss 0.2297 (0.2435)	
training:	Epoch: [17][138/233]	Loss 0.1377 (0.2428)	
training:	Epoch: [17][139/233]	Loss 0.2675 (0.2429)	
training:	Epoch: [17][140/233]	Loss 0.1965 (0.2426)	
training:	Epoch: [17][141/233]	Loss 0.1346 (0.2418)	
training:	Epoch: [17][142/233]	Loss 0.5293 (0.2439)	
training:	Epoch: [17][143/233]	Loss 0.4849 (0.2456)	
training:	Epoch: [17][144/233]	Loss 0.1202 (0.2447)	
training:	Epoch: [17][145/233]	Loss 0.2892 (0.2450)	
training:	Epoch: [17][146/233]	Loss 0.1056 (0.2440)	
training:	Epoch: [17][147/233]	Loss 0.1318 (0.2433)	
training:	Epoch: [17][148/233]	Loss 0.3155 (0.2438)	
training:	Epoch: [17][149/233]	Loss 0.2574 (0.2439)	
training:	Epoch: [17][150/233]	Loss 0.4589 (0.2453)	
training:	Epoch: [17][151/233]	Loss 0.1782 (0.2448)	
training:	Epoch: [17][152/233]	Loss 0.1672 (0.2443)	
training:	Epoch: [17][153/233]	Loss 0.1965 (0.2440)	
training:	Epoch: [17][154/233]	Loss 0.3935 (0.2450)	
training:	Epoch: [17][155/233]	Loss 0.1675 (0.2445)	
training:	Epoch: [17][156/233]	Loss 0.2974 (0.2448)	
training:	Epoch: [17][157/233]	Loss 0.2824 (0.2451)	
training:	Epoch: [17][158/233]	Loss 0.2878 (0.2453)	
training:	Epoch: [17][159/233]	Loss 0.5168 (0.2470)	
training:	Epoch: [17][160/233]	Loss 0.2646 (0.2472)	
training:	Epoch: [17][161/233]	Loss 0.1566 (0.2466)	
training:	Epoch: [17][162/233]	Loss 0.1992 (0.2463)	
training:	Epoch: [17][163/233]	Loss 0.1126 (0.2455)	
training:	Epoch: [17][164/233]	Loss 0.1838 (0.2451)	
training:	Epoch: [17][165/233]	Loss 0.2022 (0.2448)	
training:	Epoch: [17][166/233]	Loss 0.2577 (0.2449)	
training:	Epoch: [17][167/233]	Loss 0.3464 (0.2455)	
training:	Epoch: [17][168/233]	Loss 0.1324 (0.2449)	
training:	Epoch: [17][169/233]	Loss 0.2858 (0.2451)	
training:	Epoch: [17][170/233]	Loss 0.1760 (0.2447)	
training:	Epoch: [17][171/233]	Loss 0.1894 (0.2444)	
training:	Epoch: [17][172/233]	Loss 0.2236 (0.2442)	
training:	Epoch: [17][173/233]	Loss 0.2064 (0.2440)	
training:	Epoch: [17][174/233]	Loss 0.2194 (0.2439)	
training:	Epoch: [17][175/233]	Loss 0.1782 (0.2435)	
training:	Epoch: [17][176/233]	Loss 0.1957 (0.2432)	
training:	Epoch: [17][177/233]	Loss 0.5024 (0.2447)	
training:	Epoch: [17][178/233]	Loss 0.3685 (0.2454)	
training:	Epoch: [17][179/233]	Loss 0.2585 (0.2455)	
training:	Epoch: [17][180/233]	Loss 0.2882 (0.2457)	
training:	Epoch: [17][181/233]	Loss 0.3520 (0.2463)	
training:	Epoch: [17][182/233]	Loss 0.1330 (0.2457)	
training:	Epoch: [17][183/233]	Loss 0.2079 (0.2455)	
training:	Epoch: [17][184/233]	Loss 0.1980 (0.2452)	
training:	Epoch: [17][185/233]	Loss 0.2622 (0.2453)	
training:	Epoch: [17][186/233]	Loss 0.4232 (0.2463)	
training:	Epoch: [17][187/233]	Loss 0.1334 (0.2457)	
training:	Epoch: [17][188/233]	Loss 0.2084 (0.2455)	
training:	Epoch: [17][189/233]	Loss 0.2202 (0.2453)	
training:	Epoch: [17][190/233]	Loss 0.1985 (0.2451)	
training:	Epoch: [17][191/233]	Loss 0.1395 (0.2445)	
training:	Epoch: [17][192/233]	Loss 0.4455 (0.2456)	
training:	Epoch: [17][193/233]	Loss 0.1313 (0.2450)	
training:	Epoch: [17][194/233]	Loss 0.2913 (0.2452)	
training:	Epoch: [17][195/233]	Loss 0.2411 (0.2452)	
training:	Epoch: [17][196/233]	Loss 0.4288 (0.2461)	
training:	Epoch: [17][197/233]	Loss 0.4296 (0.2471)	
training:	Epoch: [17][198/233]	Loss 0.2479 (0.2471)	
training:	Epoch: [17][199/233]	Loss 0.4363 (0.2480)	
training:	Epoch: [17][200/233]	Loss 0.3470 (0.2485)	
training:	Epoch: [17][201/233]	Loss 0.2711 (0.2486)	
training:	Epoch: [17][202/233]	Loss 0.3019 (0.2489)	
training:	Epoch: [17][203/233]	Loss 0.3150 (0.2492)	
training:	Epoch: [17][204/233]	Loss 0.1650 (0.2488)	
training:	Epoch: [17][205/233]	Loss 0.1669 (0.2484)	
training:	Epoch: [17][206/233]	Loss 0.1255 (0.2478)	
training:	Epoch: [17][207/233]	Loss 0.3519 (0.2483)	
training:	Epoch: [17][208/233]	Loss 0.1958 (0.2481)	
training:	Epoch: [17][209/233]	Loss 0.1467 (0.2476)	
training:	Epoch: [17][210/233]	Loss 0.1131 (0.2469)	
training:	Epoch: [17][211/233]	Loss 0.1702 (0.2466)	
training:	Epoch: [17][212/233]	Loss 0.3240 (0.2469)	
training:	Epoch: [17][213/233]	Loss 0.2634 (0.2470)	
training:	Epoch: [17][214/233]	Loss 0.1367 (0.2465)	
training:	Epoch: [17][215/233]	Loss 0.2622 (0.2466)	
training:	Epoch: [17][216/233]	Loss 0.3089 (0.2469)	
training:	Epoch: [17][217/233]	Loss 0.1419 (0.2464)	
training:	Epoch: [17][218/233]	Loss 0.1389 (0.2459)	
training:	Epoch: [17][219/233]	Loss 0.1441 (0.2454)	
training:	Epoch: [17][220/233]	Loss 0.2315 (0.2454)	
training:	Epoch: [17][221/233]	Loss 0.2961 (0.2456)	
training:	Epoch: [17][222/233]	Loss 0.3542 (0.2461)	
training:	Epoch: [17][223/233]	Loss 0.1918 (0.2458)	
training:	Epoch: [17][224/233]	Loss 0.3054 (0.2461)	
training:	Epoch: [17][225/233]	Loss 0.2175 (0.2460)	
training:	Epoch: [17][226/233]	Loss 0.1522 (0.2456)	
training:	Epoch: [17][227/233]	Loss 0.1748 (0.2452)	
training:	Epoch: [17][228/233]	Loss 0.1366 (0.2448)	
training:	Epoch: [17][229/233]	Loss 0.2834 (0.2449)	
training:	Epoch: [17][230/233]	Loss 0.1727 (0.2446)	
training:	Epoch: [17][231/233]	Loss 0.1920 (0.2444)	
training:	Epoch: [17][232/233]	Loss 0.2588 (0.2445)	
training:	Epoch: [17][233/233]	Loss 0.2813 (0.2446)	
Training:	 Loss: 0.2441

Training:	 ACC: 0.9359 0.9351 0.9172 0.9547
Validation:	 ACC: 0.8107 0.8095 0.7865 0.8348
Validation:	 Best_BACC: 0.8162 0.8154 0.7998 0.8326
Validation:	 Loss: 0.4739
Pretraining:	Epoch 18/200
----------
training:	Epoch: [18][1/233]	Loss 0.2074 (0.2074)	
training:	Epoch: [18][2/233]	Loss 0.3351 (0.2712)	
training:	Epoch: [18][3/233]	Loss 0.3285 (0.2903)	
training:	Epoch: [18][4/233]	Loss 0.2955 (0.2916)	
training:	Epoch: [18][5/233]	Loss 0.2337 (0.2800)	
training:	Epoch: [18][6/233]	Loss 0.2208 (0.2702)	
training:	Epoch: [18][7/233]	Loss 0.2110 (0.2617)	
training:	Epoch: [18][8/233]	Loss 0.2054 (0.2547)	
training:	Epoch: [18][9/233]	Loss 0.1423 (0.2422)	
training:	Epoch: [18][10/233]	Loss 0.2015 (0.2381)	
training:	Epoch: [18][11/233]	Loss 0.2851 (0.2424)	
training:	Epoch: [18][12/233]	Loss 0.1313 (0.2331)	
training:	Epoch: [18][13/233]	Loss 0.1368 (0.2257)	
training:	Epoch: [18][14/233]	Loss 0.1893 (0.2231)	
training:	Epoch: [18][15/233]	Loss 0.1992 (0.2215)	
training:	Epoch: [18][16/233]	Loss 0.2413 (0.2228)	
training:	Epoch: [18][17/233]	Loss 0.2362 (0.2235)	
training:	Epoch: [18][18/233]	Loss 0.3680 (0.2316)	
training:	Epoch: [18][19/233]	Loss 0.0922 (0.2242)	
training:	Epoch: [18][20/233]	Loss 0.1603 (0.2210)	
training:	Epoch: [18][21/233]	Loss 0.2758 (0.2236)	
training:	Epoch: [18][22/233]	Loss 0.3080 (0.2275)	
training:	Epoch: [18][23/233]	Loss 0.1274 (0.2231)	
training:	Epoch: [18][24/233]	Loss 0.2872 (0.2258)	
training:	Epoch: [18][25/233]	Loss 0.1756 (0.2238)	
training:	Epoch: [18][26/233]	Loss 0.2498 (0.2248)	
training:	Epoch: [18][27/233]	Loss 0.4338 (0.2325)	
training:	Epoch: [18][28/233]	Loss 0.0883 (0.2274)	
training:	Epoch: [18][29/233]	Loss 0.1662 (0.2253)	
training:	Epoch: [18][30/233]	Loss 0.1040 (0.2212)	
training:	Epoch: [18][31/233]	Loss 0.1366 (0.2185)	
training:	Epoch: [18][32/233]	Loss 0.0589 (0.2135)	
training:	Epoch: [18][33/233]	Loss 0.2772 (0.2154)	
training:	Epoch: [18][34/233]	Loss 0.1418 (0.2133)	
training:	Epoch: [18][35/233]	Loss 0.2886 (0.2154)	
training:	Epoch: [18][36/233]	Loss 0.0945 (0.2121)	
training:	Epoch: [18][37/233]	Loss 0.0856 (0.2087)	
training:	Epoch: [18][38/233]	Loss 0.1079 (0.2060)	
training:	Epoch: [18][39/233]	Loss 0.2453 (0.2070)	
training:	Epoch: [18][40/233]	Loss 0.3193 (0.2098)	
training:	Epoch: [18][41/233]	Loss 0.2817 (0.2116)	
training:	Epoch: [18][42/233]	Loss 0.1837 (0.2109)	
training:	Epoch: [18][43/233]	Loss 0.3011 (0.2130)	
training:	Epoch: [18][44/233]	Loss 0.1432 (0.2114)	
training:	Epoch: [18][45/233]	Loss 0.1565 (0.2102)	
training:	Epoch: [18][46/233]	Loss 0.1316 (0.2085)	
training:	Epoch: [18][47/233]	Loss 0.2232 (0.2088)	
training:	Epoch: [18][48/233]	Loss 0.1216 (0.2070)	
training:	Epoch: [18][49/233]	Loss 0.2014 (0.2069)	
training:	Epoch: [18][50/233]	Loss 0.2515 (0.2078)	
training:	Epoch: [18][51/233]	Loss 0.1372 (0.2064)	
training:	Epoch: [18][52/233]	Loss 0.1131 (0.2046)	
training:	Epoch: [18][53/233]	Loss 0.0802 (0.2022)	
training:	Epoch: [18][54/233]	Loss 0.2046 (0.2023)	
training:	Epoch: [18][55/233]	Loss 0.2752 (0.2036)	
training:	Epoch: [18][56/233]	Loss 0.2396 (0.2043)	
training:	Epoch: [18][57/233]	Loss 0.1220 (0.2028)	
training:	Epoch: [18][58/233]	Loss 0.3697 (0.2057)	
training:	Epoch: [18][59/233]	Loss 0.0773 (0.2035)	
training:	Epoch: [18][60/233]	Loss 0.0495 (0.2009)	
training:	Epoch: [18][61/233]	Loss 0.1376 (0.1999)	
training:	Epoch: [18][62/233]	Loss 0.2240 (0.2003)	
training:	Epoch: [18][63/233]	Loss 0.2134 (0.2005)	
training:	Epoch: [18][64/233]	Loss 0.2281 (0.2009)	
training:	Epoch: [18][65/233]	Loss 0.2207 (0.2012)	
training:	Epoch: [18][66/233]	Loss 0.1266 (0.2001)	
training:	Epoch: [18][67/233]	Loss 0.1544 (0.1994)	
training:	Epoch: [18][68/233]	Loss 0.1291 (0.1984)	
training:	Epoch: [18][69/233]	Loss 0.1587 (0.1978)	
training:	Epoch: [18][70/233]	Loss 0.2198 (0.1981)	
training:	Epoch: [18][71/233]	Loss 0.1401 (0.1973)	
training:	Epoch: [18][72/233]	Loss 0.1458 (0.1966)	
training:	Epoch: [18][73/233]	Loss 0.3903 (0.1993)	
training:	Epoch: [18][74/233]	Loss 0.2752 (0.2003)	
training:	Epoch: [18][75/233]	Loss 0.1088 (0.1991)	
training:	Epoch: [18][76/233]	Loss 0.2171 (0.1993)	
training:	Epoch: [18][77/233]	Loss 0.3121 (0.2008)	
training:	Epoch: [18][78/233]	Loss 0.3613 (0.2028)	
training:	Epoch: [18][79/233]	Loss 0.1423 (0.2021)	
training:	Epoch: [18][80/233]	Loss 0.2209 (0.2023)	
training:	Epoch: [18][81/233]	Loss 0.1109 (0.2012)	
training:	Epoch: [18][82/233]	Loss 0.1569 (0.2006)	
training:	Epoch: [18][83/233]	Loss 0.2624 (0.2014)	
training:	Epoch: [18][84/233]	Loss 0.1491 (0.2007)	
training:	Epoch: [18][85/233]	Loss 0.2215 (0.2010)	
training:	Epoch: [18][86/233]	Loss 0.3007 (0.2021)	
training:	Epoch: [18][87/233]	Loss 0.2832 (0.2031)	
training:	Epoch: [18][88/233]	Loss 0.3876 (0.2052)	
training:	Epoch: [18][89/233]	Loss 0.2791 (0.2060)	
training:	Epoch: [18][90/233]	Loss 0.1319 (0.2052)	
training:	Epoch: [18][91/233]	Loss 0.1912 (0.2050)	
training:	Epoch: [18][92/233]	Loss 0.2329 (0.2053)	
training:	Epoch: [18][93/233]	Loss 0.3456 (0.2068)	
training:	Epoch: [18][94/233]	Loss 0.2120 (0.2069)	
training:	Epoch: [18][95/233]	Loss 0.3476 (0.2084)	
training:	Epoch: [18][96/233]	Loss 0.2321 (0.2086)	
training:	Epoch: [18][97/233]	Loss 0.2825 (0.2094)	
training:	Epoch: [18][98/233]	Loss 0.3224 (0.2105)	
training:	Epoch: [18][99/233]	Loss 0.2321 (0.2108)	
training:	Epoch: [18][100/233]	Loss 0.1427 (0.2101)	
training:	Epoch: [18][101/233]	Loss 0.1903 (0.2099)	
training:	Epoch: [18][102/233]	Loss 0.2555 (0.2103)	
training:	Epoch: [18][103/233]	Loss 0.2081 (0.2103)	
training:	Epoch: [18][104/233]	Loss 0.2794 (0.2110)	
training:	Epoch: [18][105/233]	Loss 0.3683 (0.2125)	
training:	Epoch: [18][106/233]	Loss 0.0823 (0.2112)	
training:	Epoch: [18][107/233]	Loss 0.1350 (0.2105)	
training:	Epoch: [18][108/233]	Loss 0.2137 (0.2106)	
training:	Epoch: [18][109/233]	Loss 0.1537 (0.2100)	
training:	Epoch: [18][110/233]	Loss 0.1820 (0.2098)	
training:	Epoch: [18][111/233]	Loss 0.3734 (0.2113)	
training:	Epoch: [18][112/233]	Loss 0.2824 (0.2119)	
training:	Epoch: [18][113/233]	Loss 0.1892 (0.2117)	
training:	Epoch: [18][114/233]	Loss 0.1927 (0.2115)	
training:	Epoch: [18][115/233]	Loss 0.4676 (0.2138)	
training:	Epoch: [18][116/233]	Loss 0.2421 (0.2140)	
training:	Epoch: [18][117/233]	Loss 0.1850 (0.2137)	
training:	Epoch: [18][118/233]	Loss 0.2991 (0.2145)	
training:	Epoch: [18][119/233]	Loss 0.2511 (0.2148)	
training:	Epoch: [18][120/233]	Loss 0.1074 (0.2139)	
training:	Epoch: [18][121/233]	Loss 0.3244 (0.2148)	
training:	Epoch: [18][122/233]	Loss 0.1332 (0.2141)	
training:	Epoch: [18][123/233]	Loss 0.1409 (0.2135)	
training:	Epoch: [18][124/233]	Loss 0.3947 (0.2150)	
training:	Epoch: [18][125/233]	Loss 0.3866 (0.2164)	
training:	Epoch: [18][126/233]	Loss 0.2952 (0.2170)	
training:	Epoch: [18][127/233]	Loss 0.1260 (0.2163)	
training:	Epoch: [18][128/233]	Loss 0.1073 (0.2154)	
training:	Epoch: [18][129/233]	Loss 0.3631 (0.2166)	
training:	Epoch: [18][130/233]	Loss 0.3664 (0.2177)	
training:	Epoch: [18][131/233]	Loss 0.1460 (0.2172)	
training:	Epoch: [18][132/233]	Loss 0.1955 (0.2170)	
training:	Epoch: [18][133/233]	Loss 0.2522 (0.2173)	
training:	Epoch: [18][134/233]	Loss 0.1735 (0.2169)	
training:	Epoch: [18][135/233]	Loss 0.2323 (0.2171)	
training:	Epoch: [18][136/233]	Loss 0.1745 (0.2167)	
training:	Epoch: [18][137/233]	Loss 0.2102 (0.2167)	
training:	Epoch: [18][138/233]	Loss 0.2882 (0.2172)	
training:	Epoch: [18][139/233]	Loss 0.2284 (0.2173)	
training:	Epoch: [18][140/233]	Loss 0.1498 (0.2168)	
training:	Epoch: [18][141/233]	Loss 0.1267 (0.2162)	
training:	Epoch: [18][142/233]	Loss 0.1234 (0.2155)	
training:	Epoch: [18][143/233]	Loss 0.3179 (0.2162)	
training:	Epoch: [18][144/233]	Loss 0.2189 (0.2163)	
training:	Epoch: [18][145/233]	Loss 0.1117 (0.2155)	
training:	Epoch: [18][146/233]	Loss 0.3375 (0.2164)	
training:	Epoch: [18][147/233]	Loss 0.3583 (0.2173)	
training:	Epoch: [18][148/233]	Loss 0.2826 (0.2178)	
training:	Epoch: [18][149/233]	Loss 0.1875 (0.2176)	
training:	Epoch: [18][150/233]	Loss 0.1258 (0.2170)	
training:	Epoch: [18][151/233]	Loss 0.2011 (0.2169)	
training:	Epoch: [18][152/233]	Loss 0.3372 (0.2177)	
training:	Epoch: [18][153/233]	Loss 0.1156 (0.2170)	
training:	Epoch: [18][154/233]	Loss 0.0946 (0.2162)	
training:	Epoch: [18][155/233]	Loss 0.1367 (0.2157)	
training:	Epoch: [18][156/233]	Loss 0.3188 (0.2163)	
training:	Epoch: [18][157/233]	Loss 0.2008 (0.2162)	
training:	Epoch: [18][158/233]	Loss 0.1617 (0.2159)	
training:	Epoch: [18][159/233]	Loss 0.2990 (0.2164)	
training:	Epoch: [18][160/233]	Loss 0.1634 (0.2161)	
training:	Epoch: [18][161/233]	Loss 0.2712 (0.2164)	
training:	Epoch: [18][162/233]	Loss 0.1941 (0.2163)	
training:	Epoch: [18][163/233]	Loss 0.2108 (0.2163)	
training:	Epoch: [18][164/233]	Loss 0.0828 (0.2154)	
training:	Epoch: [18][165/233]	Loss 0.2270 (0.2155)	
training:	Epoch: [18][166/233]	Loss 0.2473 (0.2157)	
training:	Epoch: [18][167/233]	Loss 0.1019 (0.2150)	
training:	Epoch: [18][168/233]	Loss 0.1828 (0.2148)	
training:	Epoch: [18][169/233]	Loss 0.4775 (0.2164)	
training:	Epoch: [18][170/233]	Loss 0.2004 (0.2163)	
training:	Epoch: [18][171/233]	Loss 0.2097 (0.2163)	
training:	Epoch: [18][172/233]	Loss 0.4416 (0.2176)	
training:	Epoch: [18][173/233]	Loss 0.1082 (0.2169)	
training:	Epoch: [18][174/233]	Loss 0.4225 (0.2181)	
training:	Epoch: [18][175/233]	Loss 0.3290 (0.2187)	
training:	Epoch: [18][176/233]	Loss 0.2797 (0.2191)	
training:	Epoch: [18][177/233]	Loss 0.3754 (0.2200)	
training:	Epoch: [18][178/233]	Loss 0.1778 (0.2197)	
training:	Epoch: [18][179/233]	Loss 0.2801 (0.2201)	
training:	Epoch: [18][180/233]	Loss 0.1932 (0.2199)	
training:	Epoch: [18][181/233]	Loss 0.2611 (0.2202)	
training:	Epoch: [18][182/233]	Loss 0.2361 (0.2202)	
training:	Epoch: [18][183/233]	Loss 0.2676 (0.2205)	
training:	Epoch: [18][184/233]	Loss 0.2277 (0.2205)	
training:	Epoch: [18][185/233]	Loss 0.4101 (0.2216)	
training:	Epoch: [18][186/233]	Loss 0.1339 (0.2211)	
training:	Epoch: [18][187/233]	Loss 0.1983 (0.2210)	
training:	Epoch: [18][188/233]	Loss 0.0900 (0.2203)	
training:	Epoch: [18][189/233]	Loss 0.2122 (0.2202)	
training:	Epoch: [18][190/233]	Loss 0.1753 (0.2200)	
training:	Epoch: [18][191/233]	Loss 0.2480 (0.2201)	
training:	Epoch: [18][192/233]	Loss 0.0845 (0.2194)	
training:	Epoch: [18][193/233]	Loss 0.2794 (0.2197)	
training:	Epoch: [18][194/233]	Loss 0.4644 (0.2210)	
training:	Epoch: [18][195/233]	Loss 0.1176 (0.2205)	
training:	Epoch: [18][196/233]	Loss 0.3151 (0.2210)	
training:	Epoch: [18][197/233]	Loss 0.2320 (0.2210)	
training:	Epoch: [18][198/233]	Loss 0.4344 (0.2221)	
training:	Epoch: [18][199/233]	Loss 0.1457 (0.2217)	
training:	Epoch: [18][200/233]	Loss 0.3968 (0.2226)	
training:	Epoch: [18][201/233]	Loss 0.1499 (0.2222)	
training:	Epoch: [18][202/233]	Loss 0.2735 (0.2225)	
training:	Epoch: [18][203/233]	Loss 0.1035 (0.2219)	
training:	Epoch: [18][204/233]	Loss 0.0621 (0.2211)	
training:	Epoch: [18][205/233]	Loss 0.1742 (0.2209)	
training:	Epoch: [18][206/233]	Loss 0.4210 (0.2218)	
training:	Epoch: [18][207/233]	Loss 0.2167 (0.2218)	
training:	Epoch: [18][208/233]	Loss 0.2480 (0.2220)	
training:	Epoch: [18][209/233]	Loss 0.2178 (0.2219)	
training:	Epoch: [18][210/233]	Loss 0.3071 (0.2223)	
training:	Epoch: [18][211/233]	Loss 0.3324 (0.2229)	
training:	Epoch: [18][212/233]	Loss 0.1976 (0.2227)	
training:	Epoch: [18][213/233]	Loss 0.1770 (0.2225)	
training:	Epoch: [18][214/233]	Loss 0.1750 (0.2223)	
training:	Epoch: [18][215/233]	Loss 0.1742 (0.2221)	
training:	Epoch: [18][216/233]	Loss 0.2543 (0.2222)	
training:	Epoch: [18][217/233]	Loss 0.1704 (0.2220)	
training:	Epoch: [18][218/233]	Loss 0.1005 (0.2214)	
training:	Epoch: [18][219/233]	Loss 0.2201 (0.2214)	
training:	Epoch: [18][220/233]	Loss 0.2592 (0.2216)	
training:	Epoch: [18][221/233]	Loss 0.3038 (0.2220)	
training:	Epoch: [18][222/233]	Loss 0.1364 (0.2216)	
training:	Epoch: [18][223/233]	Loss 0.0872 (0.2210)	
training:	Epoch: [18][224/233]	Loss 0.2894 (0.2213)	
training:	Epoch: [18][225/233]	Loss 0.3143 (0.2217)	
training:	Epoch: [18][226/233]	Loss 0.2152 (0.2217)	
training:	Epoch: [18][227/233]	Loss 0.2107 (0.2216)	
training:	Epoch: [18][228/233]	Loss 0.3070 (0.2220)	
training:	Epoch: [18][229/233]	Loss 0.2186 (0.2220)	
training:	Epoch: [18][230/233]	Loss 0.2235 (0.2220)	
training:	Epoch: [18][231/233]	Loss 0.5845 (0.2236)	
training:	Epoch: [18][232/233]	Loss 0.1743 (0.2233)	
training:	Epoch: [18][233/233]	Loss 0.1649 (0.2231)	
Training:	 Loss: 0.2226

Training:	 ACC: 0.9405 0.9406 0.9428 0.9381
Validation:	 ACC: 0.8007 0.8010 0.8069 0.7944
Validation:	 Best_BACC: 0.8162 0.8154 0.7998 0.8326
Validation:	 Loss: 0.4978
Pretraining:	Epoch 19/200
----------
training:	Epoch: [19][1/233]	Loss 0.2693 (0.2693)	
training:	Epoch: [19][2/233]	Loss 0.1319 (0.2006)	
training:	Epoch: [19][3/233]	Loss 0.2036 (0.2016)	
training:	Epoch: [19][4/233]	Loss 0.1929 (0.1994)	
training:	Epoch: [19][5/233]	Loss 0.3319 (0.2259)	
training:	Epoch: [19][6/233]	Loss 0.1106 (0.2067)	
training:	Epoch: [19][7/233]	Loss 0.1253 (0.1951)	
training:	Epoch: [19][8/233]	Loss 0.2745 (0.2050)	
training:	Epoch: [19][9/233]	Loss 0.2302 (0.2078)	
training:	Epoch: [19][10/233]	Loss 0.3030 (0.2173)	
training:	Epoch: [19][11/233]	Loss 0.1213 (0.2086)	
training:	Epoch: [19][12/233]	Loss 0.1969 (0.2076)	
training:	Epoch: [19][13/233]	Loss 0.1755 (0.2051)	
training:	Epoch: [19][14/233]	Loss 0.3901 (0.2183)	
training:	Epoch: [19][15/233]	Loss 0.1740 (0.2154)	
training:	Epoch: [19][16/233]	Loss 0.2629 (0.2184)	
training:	Epoch: [19][17/233]	Loss 0.1717 (0.2156)	
training:	Epoch: [19][18/233]	Loss 0.1429 (0.2116)	
training:	Epoch: [19][19/233]	Loss 0.1360 (0.2076)	
training:	Epoch: [19][20/233]	Loss 0.1705 (0.2057)	
training:	Epoch: [19][21/233]	Loss 0.1261 (0.2020)	
training:	Epoch: [19][22/233]	Loss 0.1612 (0.2001)	
training:	Epoch: [19][23/233]	Loss 0.0791 (0.1948)	
training:	Epoch: [19][24/233]	Loss 0.2633 (0.1977)	
training:	Epoch: [19][25/233]	Loss 0.2232 (0.1987)	
training:	Epoch: [19][26/233]	Loss 0.2789 (0.2018)	
training:	Epoch: [19][27/233]	Loss 0.3858 (0.2086)	
training:	Epoch: [19][28/233]	Loss 0.1516 (0.2066)	
training:	Epoch: [19][29/233]	Loss 0.1862 (0.2059)	
training:	Epoch: [19][30/233]	Loss 0.1665 (0.2046)	
training:	Epoch: [19][31/233]	Loss 0.3057 (0.2078)	
training:	Epoch: [19][32/233]	Loss 0.0551 (0.2030)	
training:	Epoch: [19][33/233]	Loss 0.1781 (0.2023)	
training:	Epoch: [19][34/233]	Loss 0.3436 (0.2064)	
training:	Epoch: [19][35/233]	Loss 0.1736 (0.2055)	
training:	Epoch: [19][36/233]	Loss 0.3927 (0.2107)	
training:	Epoch: [19][37/233]	Loss 0.3462 (0.2144)	
training:	Epoch: [19][38/233]	Loss 0.1176 (0.2118)	
training:	Epoch: [19][39/233]	Loss 0.1178 (0.2094)	
training:	Epoch: [19][40/233]	Loss 0.3370 (0.2126)	
training:	Epoch: [19][41/233]	Loss 0.1127 (0.2102)	
training:	Epoch: [19][42/233]	Loss 0.1845 (0.2096)	
training:	Epoch: [19][43/233]	Loss 0.3242 (0.2122)	
training:	Epoch: [19][44/233]	Loss 0.1297 (0.2103)	
training:	Epoch: [19][45/233]	Loss 0.3112 (0.2126)	
training:	Epoch: [19][46/233]	Loss 0.2291 (0.2129)	
training:	Epoch: [19][47/233]	Loss 0.2377 (0.2135)	
training:	Epoch: [19][48/233]	Loss 0.3290 (0.2159)	
training:	Epoch: [19][49/233]	Loss 0.2632 (0.2168)	
training:	Epoch: [19][50/233]	Loss 0.1704 (0.2159)	
training:	Epoch: [19][51/233]	Loss 0.1268 (0.2142)	
training:	Epoch: [19][52/233]	Loss 0.2011 (0.2139)	
training:	Epoch: [19][53/233]	Loss 0.3901 (0.2172)	
training:	Epoch: [19][54/233]	Loss 0.0774 (0.2147)	
training:	Epoch: [19][55/233]	Loss 0.2773 (0.2158)	
training:	Epoch: [19][56/233]	Loss 0.1954 (0.2154)	
training:	Epoch: [19][57/233]	Loss 0.1003 (0.2134)	
training:	Epoch: [19][58/233]	Loss 0.1980 (0.2131)	
training:	Epoch: [19][59/233]	Loss 0.1359 (0.2118)	
training:	Epoch: [19][60/233]	Loss 0.4572 (0.2159)	
training:	Epoch: [19][61/233]	Loss 0.2646 (0.2167)	
training:	Epoch: [19][62/233]	Loss 0.0931 (0.2147)	
training:	Epoch: [19][63/233]	Loss 0.3263 (0.2165)	
training:	Epoch: [19][64/233]	Loss 0.2148 (0.2165)	
training:	Epoch: [19][65/233]	Loss 0.1843 (0.2160)	
training:	Epoch: [19][66/233]	Loss 0.2221 (0.2161)	
training:	Epoch: [19][67/233]	Loss 0.2155 (0.2161)	
training:	Epoch: [19][68/233]	Loss 0.2293 (0.2163)	
training:	Epoch: [19][69/233]	Loss 0.1383 (0.2151)	
training:	Epoch: [19][70/233]	Loss 0.1994 (0.2149)	
training:	Epoch: [19][71/233]	Loss 0.2925 (0.2160)	
training:	Epoch: [19][72/233]	Loss 0.2519 (0.2165)	
training:	Epoch: [19][73/233]	Loss 0.1441 (0.2155)	
training:	Epoch: [19][74/233]	Loss 0.2018 (0.2153)	
training:	Epoch: [19][75/233]	Loss 0.0925 (0.2137)	
training:	Epoch: [19][76/233]	Loss 0.2163 (0.2137)	
training:	Epoch: [19][77/233]	Loss 0.3631 (0.2157)	
training:	Epoch: [19][78/233]	Loss 0.3118 (0.2169)	
training:	Epoch: [19][79/233]	Loss 0.1297 (0.2158)	
training:	Epoch: [19][80/233]	Loss 0.4474 (0.2187)	
training:	Epoch: [19][81/233]	Loss 0.2520 (0.2191)	
training:	Epoch: [19][82/233]	Loss 0.1089 (0.2177)	
training:	Epoch: [19][83/233]	Loss 0.1489 (0.2169)	
training:	Epoch: [19][84/233]	Loss 0.1736 (0.2164)	
training:	Epoch: [19][85/233]	Loss 0.1065 (0.2151)	
training:	Epoch: [19][86/233]	Loss 0.0738 (0.2135)	
training:	Epoch: [19][87/233]	Loss 0.3371 (0.2149)	
training:	Epoch: [19][88/233]	Loss 0.2407 (0.2152)	
training:	Epoch: [19][89/233]	Loss 0.1121 (0.2140)	
training:	Epoch: [19][90/233]	Loss 0.2786 (0.2147)	
training:	Epoch: [19][91/233]	Loss 0.3646 (0.2164)	
training:	Epoch: [19][92/233]	Loss 0.2124 (0.2163)	
training:	Epoch: [19][93/233]	Loss 0.1967 (0.2161)	
training:	Epoch: [19][94/233]	Loss 0.2475 (0.2165)	
training:	Epoch: [19][95/233]	Loss 0.1240 (0.2155)	
training:	Epoch: [19][96/233]	Loss 0.2850 (0.2162)	
training:	Epoch: [19][97/233]	Loss 0.2501 (0.2166)	
training:	Epoch: [19][98/233]	Loss 0.3332 (0.2178)	
training:	Epoch: [19][99/233]	Loss 0.1448 (0.2170)	
training:	Epoch: [19][100/233]	Loss 0.2318 (0.2172)	
training:	Epoch: [19][101/233]	Loss 0.1905 (0.2169)	
training:	Epoch: [19][102/233]	Loss 0.0847 (0.2156)	
training:	Epoch: [19][103/233]	Loss 0.2102 (0.2156)	
training:	Epoch: [19][104/233]	Loss 0.2310 (0.2157)	
training:	Epoch: [19][105/233]	Loss 0.0947 (0.2146)	
training:	Epoch: [19][106/233]	Loss 0.3288 (0.2156)	
training:	Epoch: [19][107/233]	Loss 0.3753 (0.2171)	
training:	Epoch: [19][108/233]	Loss 0.1950 (0.2169)	
training:	Epoch: [19][109/233]	Loss 0.0926 (0.2158)	
training:	Epoch: [19][110/233]	Loss 0.2003 (0.2156)	
training:	Epoch: [19][111/233]	Loss 0.1988 (0.2155)	
training:	Epoch: [19][112/233]	Loss 0.0639 (0.2141)	
training:	Epoch: [19][113/233]	Loss 0.1090 (0.2132)	
training:	Epoch: [19][114/233]	Loss 0.1190 (0.2124)	
training:	Epoch: [19][115/233]	Loss 0.1987 (0.2123)	
training:	Epoch: [19][116/233]	Loss 0.1749 (0.2119)	
training:	Epoch: [19][117/233]	Loss 0.2211 (0.2120)	
training:	Epoch: [19][118/233]	Loss 0.1666 (0.2116)	
training:	Epoch: [19][119/233]	Loss 0.2626 (0.2121)	
training:	Epoch: [19][120/233]	Loss 0.2743 (0.2126)	
training:	Epoch: [19][121/233]	Loss 0.2830 (0.2132)	
training:	Epoch: [19][122/233]	Loss 0.2446 (0.2134)	
training:	Epoch: [19][123/233]	Loss 0.1068 (0.2125)	
training:	Epoch: [19][124/233]	Loss 0.2113 (0.2125)	
training:	Epoch: [19][125/233]	Loss 0.0914 (0.2116)	
training:	Epoch: [19][126/233]	Loss 0.3305 (0.2125)	
training:	Epoch: [19][127/233]	Loss 0.3461 (0.2136)	
training:	Epoch: [19][128/233]	Loss 0.2448 (0.2138)	
training:	Epoch: [19][129/233]	Loss 0.2806 (0.2143)	
training:	Epoch: [19][130/233]	Loss 0.1649 (0.2139)	
training:	Epoch: [19][131/233]	Loss 0.1276 (0.2133)	
training:	Epoch: [19][132/233]	Loss 0.2267 (0.2134)	
training:	Epoch: [19][133/233]	Loss 0.1494 (0.2129)	
training:	Epoch: [19][134/233]	Loss 0.1356 (0.2123)	
training:	Epoch: [19][135/233]	Loss 0.2903 (0.2129)	
training:	Epoch: [19][136/233]	Loss 0.1716 (0.2126)	
training:	Epoch: [19][137/233]	Loss 0.1709 (0.2123)	
training:	Epoch: [19][138/233]	Loss 0.1975 (0.2122)	
training:	Epoch: [19][139/233]	Loss 0.3756 (0.2134)	
training:	Epoch: [19][140/233]	Loss 0.0702 (0.2123)	
training:	Epoch: [19][141/233]	Loss 0.3540 (0.2133)	
training:	Epoch: [19][142/233]	Loss 0.2423 (0.2136)	
training:	Epoch: [19][143/233]	Loss 0.2339 (0.2137)	
training:	Epoch: [19][144/233]	Loss 0.2784 (0.2141)	
training:	Epoch: [19][145/233]	Loss 0.2270 (0.2142)	
training:	Epoch: [19][146/233]	Loss 0.0750 (0.2133)	
training:	Epoch: [19][147/233]	Loss 0.2025 (0.2132)	
training:	Epoch: [19][148/233]	Loss 0.1549 (0.2128)	
training:	Epoch: [19][149/233]	Loss 0.2704 (0.2132)	
training:	Epoch: [19][150/233]	Loss 0.2805 (0.2136)	
training:	Epoch: [19][151/233]	Loss 0.4222 (0.2150)	
training:	Epoch: [19][152/233]	Loss 0.4638 (0.2167)	
training:	Epoch: [19][153/233]	Loss 0.2912 (0.2172)	
training:	Epoch: [19][154/233]	Loss 0.3376 (0.2179)	
training:	Epoch: [19][155/233]	Loss 0.0839 (0.2171)	
training:	Epoch: [19][156/233]	Loss 0.2323 (0.2172)	
training:	Epoch: [19][157/233]	Loss 0.1617 (0.2168)	
training:	Epoch: [19][158/233]	Loss 0.1598 (0.2165)	
training:	Epoch: [19][159/233]	Loss 0.1828 (0.2162)	
training:	Epoch: [19][160/233]	Loss 0.3544 (0.2171)	
training:	Epoch: [19][161/233]	Loss 0.0807 (0.2163)	
training:	Epoch: [19][162/233]	Loss 0.1438 (0.2158)	
training:	Epoch: [19][163/233]	Loss 0.1064 (0.2151)	
training:	Epoch: [19][164/233]	Loss 0.1824 (0.2149)	
training:	Epoch: [19][165/233]	Loss 0.1771 (0.2147)	
training:	Epoch: [19][166/233]	Loss 0.1485 (0.2143)	
training:	Epoch: [19][167/233]	Loss 0.1103 (0.2137)	
training:	Epoch: [19][168/233]	Loss 0.1355 (0.2132)	
training:	Epoch: [19][169/233]	Loss 0.3707 (0.2142)	
training:	Epoch: [19][170/233]	Loss 0.2269 (0.2142)	
training:	Epoch: [19][171/233]	Loss 0.1098 (0.2136)	
training:	Epoch: [19][172/233]	Loss 0.1389 (0.2132)	
training:	Epoch: [19][173/233]	Loss 0.1446 (0.2128)	
training:	Epoch: [19][174/233]	Loss 0.1508 (0.2124)	
training:	Epoch: [19][175/233]	Loss 0.0771 (0.2117)	
training:	Epoch: [19][176/233]	Loss 0.1744 (0.2114)	
training:	Epoch: [19][177/233]	Loss 0.2156 (0.2115)	
training:	Epoch: [19][178/233]	Loss 0.2630 (0.2118)	
training:	Epoch: [19][179/233]	Loss 0.3197 (0.2124)	
training:	Epoch: [19][180/233]	Loss 0.2352 (0.2125)	
training:	Epoch: [19][181/233]	Loss 0.2993 (0.2130)	
training:	Epoch: [19][182/233]	Loss 0.1942 (0.2129)	
training:	Epoch: [19][183/233]	Loss 0.1028 (0.2123)	
training:	Epoch: [19][184/233]	Loss 0.0621 (0.2114)	
training:	Epoch: [19][185/233]	Loss 0.2923 (0.2119)	
training:	Epoch: [19][186/233]	Loss 0.2999 (0.2124)	
training:	Epoch: [19][187/233]	Loss 0.0959 (0.2117)	
training:	Epoch: [19][188/233]	Loss 0.1331 (0.2113)	
training:	Epoch: [19][189/233]	Loss 0.2581 (0.2116)	
training:	Epoch: [19][190/233]	Loss 0.0815 (0.2109)	
training:	Epoch: [19][191/233]	Loss 0.1470 (0.2105)	
training:	Epoch: [19][192/233]	Loss 0.1999 (0.2105)	
training:	Epoch: [19][193/233]	Loss 0.2837 (0.2109)	
training:	Epoch: [19][194/233]	Loss 0.1569 (0.2106)	
training:	Epoch: [19][195/233]	Loss 0.1917 (0.2105)	
training:	Epoch: [19][196/233]	Loss 0.2217 (0.2106)	
training:	Epoch: [19][197/233]	Loss 0.2516 (0.2108)	
training:	Epoch: [19][198/233]	Loss 0.3701 (0.2116)	
training:	Epoch: [19][199/233]	Loss 0.2878 (0.2119)	
training:	Epoch: [19][200/233]	Loss 0.1471 (0.2116)	
training:	Epoch: [19][201/233]	Loss 0.2484 (0.2118)	
training:	Epoch: [19][202/233]	Loss 0.1092 (0.2113)	
training:	Epoch: [19][203/233]	Loss 0.1790 (0.2111)	
training:	Epoch: [19][204/233]	Loss 0.0873 (0.2105)	
training:	Epoch: [19][205/233]	Loss 0.2484 (0.2107)	
training:	Epoch: [19][206/233]	Loss 0.1991 (0.2107)	
training:	Epoch: [19][207/233]	Loss 0.2370 (0.2108)	
training:	Epoch: [19][208/233]	Loss 0.1783 (0.2106)	
training:	Epoch: [19][209/233]	Loss 0.2555 (0.2108)	
training:	Epoch: [19][210/233]	Loss 0.2990 (0.2113)	
training:	Epoch: [19][211/233]	Loss 0.2086 (0.2113)	
training:	Epoch: [19][212/233]	Loss 0.1100 (0.2108)	
training:	Epoch: [19][213/233]	Loss 0.2264 (0.2108)	
training:	Epoch: [19][214/233]	Loss 0.3739 (0.2116)	
training:	Epoch: [19][215/233]	Loss 0.2674 (0.2119)	
training:	Epoch: [19][216/233]	Loss 0.1627 (0.2116)	
training:	Epoch: [19][217/233]	Loss 0.1273 (0.2113)	
training:	Epoch: [19][218/233]	Loss 0.2964 (0.2116)	
training:	Epoch: [19][219/233]	Loss 0.1661 (0.2114)	
training:	Epoch: [19][220/233]	Loss 0.3915 (0.2123)	
training:	Epoch: [19][221/233]	Loss 0.1280 (0.2119)	
training:	Epoch: [19][222/233]	Loss 0.1463 (0.2116)	
training:	Epoch: [19][223/233]	Loss 0.2039 (0.2115)	
training:	Epoch: [19][224/233]	Loss 0.1476 (0.2113)	
training:	Epoch: [19][225/233]	Loss 0.6220 (0.2131)	
training:	Epoch: [19][226/233]	Loss 0.1948 (0.2130)	
training:	Epoch: [19][227/233]	Loss 0.1989 (0.2129)	
training:	Epoch: [19][228/233]	Loss 0.1416 (0.2126)	
training:	Epoch: [19][229/233]	Loss 0.1037 (0.2122)	
training:	Epoch: [19][230/233]	Loss 0.1949 (0.2121)	
training:	Epoch: [19][231/233]	Loss 0.2473 (0.2122)	
training:	Epoch: [19][232/233]	Loss 0.1420 (0.2119)	
training:	Epoch: [19][233/233]	Loss 0.4535 (0.2130)	
Training:	 Loss: 0.2125

Training:	 ACC: 0.9491 0.9481 0.9264 0.9717
Validation:	 ACC: 0.8040 0.8026 0.7732 0.8348
Validation:	 Best_BACC: 0.8162 0.8154 0.7998 0.8326
Validation:	 Loss: 0.5088
Pretraining:	Epoch 20/200
----------
training:	Epoch: [20][1/233]	Loss 0.4094 (0.4094)	
training:	Epoch: [20][2/233]	Loss 0.0961 (0.2527)	
training:	Epoch: [20][3/233]	Loss 0.2496 (0.2517)	
training:	Epoch: [20][4/233]	Loss 0.0897 (0.2112)	
training:	Epoch: [20][5/233]	Loss 0.2532 (0.2196)	
training:	Epoch: [20][6/233]	Loss 0.1084 (0.2011)	
training:	Epoch: [20][7/233]	Loss 0.1043 (0.1872)	
training:	Epoch: [20][8/233]	Loss 0.1546 (0.1832)	
training:	Epoch: [20][9/233]	Loss 0.1698 (0.1817)	
training:	Epoch: [20][10/233]	Loss 0.2237 (0.1859)	
training:	Epoch: [20][11/233]	Loss 0.2419 (0.1910)	
training:	Epoch: [20][12/233]	Loss 0.4155 (0.2097)	
training:	Epoch: [20][13/233]	Loss 0.2179 (0.2103)	
training:	Epoch: [20][14/233]	Loss 0.1144 (0.2035)	
training:	Epoch: [20][15/233]	Loss 0.1591 (0.2005)	
training:	Epoch: [20][16/233]	Loss 0.1296 (0.1961)	
training:	Epoch: [20][17/233]	Loss 0.1151 (0.1913)	
training:	Epoch: [20][18/233]	Loss 0.2680 (0.1956)	
training:	Epoch: [20][19/233]	Loss 0.2413 (0.1980)	
training:	Epoch: [20][20/233]	Loss 0.2275 (0.1994)	
training:	Epoch: [20][21/233]	Loss 0.3002 (0.2042)	
training:	Epoch: [20][22/233]	Loss 0.1110 (0.2000)	
training:	Epoch: [20][23/233]	Loss 0.0735 (0.1945)	
training:	Epoch: [20][24/233]	Loss 0.2510 (0.1969)	
training:	Epoch: [20][25/233]	Loss 0.0860 (0.1924)	
training:	Epoch: [20][26/233]	Loss 0.2352 (0.1941)	
training:	Epoch: [20][27/233]	Loss 0.1899 (0.1939)	
training:	Epoch: [20][28/233]	Loss 0.1434 (0.1921)	
training:	Epoch: [20][29/233]	Loss 0.2284 (0.1934)	
training:	Epoch: [20][30/233]	Loss 0.1052 (0.1904)	
training:	Epoch: [20][31/233]	Loss 0.1600 (0.1894)	
training:	Epoch: [20][32/233]	Loss 0.1399 (0.1879)	
training:	Epoch: [20][33/233]	Loss 0.2342 (0.1893)	
training:	Epoch: [20][34/233]	Loss 0.0733 (0.1859)	
training:	Epoch: [20][35/233]	Loss 0.2650 (0.1881)	
training:	Epoch: [20][36/233]	Loss 0.2167 (0.1889)	
training:	Epoch: [20][37/233]	Loss 0.1241 (0.1872)	
training:	Epoch: [20][38/233]	Loss 0.0995 (0.1849)	
training:	Epoch: [20][39/233]	Loss 0.1579 (0.1842)	
training:	Epoch: [20][40/233]	Loss 0.1721 (0.1839)	
training:	Epoch: [20][41/233]	Loss 0.1253 (0.1825)	
training:	Epoch: [20][42/233]	Loss 0.1668 (0.1821)	
training:	Epoch: [20][43/233]	Loss 0.1474 (0.1813)	
training:	Epoch: [20][44/233]	Loss 0.3196 (0.1844)	
training:	Epoch: [20][45/233]	Loss 0.1269 (0.1831)	
training:	Epoch: [20][46/233]	Loss 0.1973 (0.1834)	
training:	Epoch: [20][47/233]	Loss 0.3419 (0.1868)	
training:	Epoch: [20][48/233]	Loss 0.1512 (0.1861)	
training:	Epoch: [20][49/233]	Loss 0.0751 (0.1838)	
training:	Epoch: [20][50/233]	Loss 0.0901 (0.1819)	
training:	Epoch: [20][51/233]	Loss 0.1034 (0.1804)	
training:	Epoch: [20][52/233]	Loss 0.2676 (0.1821)	
training:	Epoch: [20][53/233]	Loss 0.2520 (0.1834)	
training:	Epoch: [20][54/233]	Loss 0.2996 (0.1855)	
training:	Epoch: [20][55/233]	Loss 0.1875 (0.1856)	
training:	Epoch: [20][56/233]	Loss 0.1806 (0.1855)	
training:	Epoch: [20][57/233]	Loss 0.1753 (0.1853)	
training:	Epoch: [20][58/233]	Loss 0.1170 (0.1841)	
training:	Epoch: [20][59/233]	Loss 0.1967 (0.1844)	
training:	Epoch: [20][60/233]	Loss 0.2694 (0.1858)	
training:	Epoch: [20][61/233]	Loss 0.3690 (0.1888)	
training:	Epoch: [20][62/233]	Loss 0.2108 (0.1891)	
training:	Epoch: [20][63/233]	Loss 0.1405 (0.1884)	
training:	Epoch: [20][64/233]	Loss 0.1528 (0.1878)	
training:	Epoch: [20][65/233]	Loss 0.1214 (0.1868)	
training:	Epoch: [20][66/233]	Loss 0.1299 (0.1859)	
training:	Epoch: [20][67/233]	Loss 0.1605 (0.1855)	
training:	Epoch: [20][68/233]	Loss 0.1879 (0.1856)	
training:	Epoch: [20][69/233]	Loss 0.3544 (0.1880)	
training:	Epoch: [20][70/233]	Loss 0.2567 (0.1890)	
training:	Epoch: [20][71/233]	Loss 0.1867 (0.1890)	
training:	Epoch: [20][72/233]	Loss 0.3805 (0.1916)	
training:	Epoch: [20][73/233]	Loss 0.1240 (0.1907)	
training:	Epoch: [20][74/233]	Loss 0.2494 (0.1915)	
training:	Epoch: [20][75/233]	Loss 0.2044 (0.1917)	
training:	Epoch: [20][76/233]	Loss 0.2902 (0.1930)	
training:	Epoch: [20][77/233]	Loss 0.0856 (0.1916)	
training:	Epoch: [20][78/233]	Loss 0.1871 (0.1915)	
training:	Epoch: [20][79/233]	Loss 0.1352 (0.1908)	
training:	Epoch: [20][80/233]	Loss 0.2052 (0.1910)	
training:	Epoch: [20][81/233]	Loss 0.1374 (0.1903)	
training:	Epoch: [20][82/233]	Loss 0.2310 (0.1908)	
training:	Epoch: [20][83/233]	Loss 0.2740 (0.1918)	
training:	Epoch: [20][84/233]	Loss 0.2667 (0.1927)	
training:	Epoch: [20][85/233]	Loss 0.1164 (0.1918)	
training:	Epoch: [20][86/233]	Loss 0.2443 (0.1924)	
training:	Epoch: [20][87/233]	Loss 0.1997 (0.1925)	
training:	Epoch: [20][88/233]	Loss 0.1160 (0.1916)	
training:	Epoch: [20][89/233]	Loss 0.2811 (0.1926)	
training:	Epoch: [20][90/233]	Loss 0.1150 (0.1918)	
training:	Epoch: [20][91/233]	Loss 0.2046 (0.1919)	
training:	Epoch: [20][92/233]	Loss 0.1748 (0.1917)	
training:	Epoch: [20][93/233]	Loss 0.0595 (0.1903)	
training:	Epoch: [20][94/233]	Loss 0.1833 (0.1902)	
training:	Epoch: [20][95/233]	Loss 0.2958 (0.1913)	
training:	Epoch: [20][96/233]	Loss 0.2158 (0.1916)	
training:	Epoch: [20][97/233]	Loss 0.4402 (0.1942)	
training:	Epoch: [20][98/233]	Loss 0.1761 (0.1940)	
training:	Epoch: [20][99/233]	Loss 0.1537 (0.1936)	
training:	Epoch: [20][100/233]	Loss 0.1695 (0.1933)	
training:	Epoch: [20][101/233]	Loss 0.2856 (0.1942)	
training:	Epoch: [20][102/233]	Loss 0.3203 (0.1955)	
training:	Epoch: [20][103/233]	Loss 0.0673 (0.1942)	
training:	Epoch: [20][104/233]	Loss 0.2594 (0.1949)	
training:	Epoch: [20][105/233]	Loss 0.1183 (0.1941)	
training:	Epoch: [20][106/233]	Loss 0.0887 (0.1931)	
training:	Epoch: [20][107/233]	Loss 0.2965 (0.1941)	
training:	Epoch: [20][108/233]	Loss 0.2572 (0.1947)	
training:	Epoch: [20][109/233]	Loss 0.2214 (0.1949)	
training:	Epoch: [20][110/233]	Loss 0.0830 (0.1939)	
training:	Epoch: [20][111/233]	Loss 0.1411 (0.1934)	
training:	Epoch: [20][112/233]	Loss 0.2619 (0.1941)	
training:	Epoch: [20][113/233]	Loss 0.1056 (0.1933)	
training:	Epoch: [20][114/233]	Loss 0.1773 (0.1931)	
training:	Epoch: [20][115/233]	Loss 0.0948 (0.1923)	
training:	Epoch: [20][116/233]	Loss 0.1526 (0.1919)	
training:	Epoch: [20][117/233]	Loss 0.1442 (0.1915)	
training:	Epoch: [20][118/233]	Loss 0.0695 (0.1905)	
training:	Epoch: [20][119/233]	Loss 0.1190 (0.1899)	
training:	Epoch: [20][120/233]	Loss 0.1840 (0.1898)	
training:	Epoch: [20][121/233]	Loss 0.0952 (0.1891)	
training:	Epoch: [20][122/233]	Loss 0.1508 (0.1887)	
training:	Epoch: [20][123/233]	Loss 0.0818 (0.1879)	
training:	Epoch: [20][124/233]	Loss 0.1695 (0.1877)	
training:	Epoch: [20][125/233]	Loss 0.0437 (0.1866)	
training:	Epoch: [20][126/233]	Loss 0.2899 (0.1874)	
training:	Epoch: [20][127/233]	Loss 0.2202 (0.1877)	
training:	Epoch: [20][128/233]	Loss 0.2196 (0.1879)	
training:	Epoch: [20][129/233]	Loss 0.2857 (0.1887)	
training:	Epoch: [20][130/233]	Loss 0.3547 (0.1899)	
training:	Epoch: [20][131/233]	Loss 0.1627 (0.1897)	
training:	Epoch: [20][132/233]	Loss 0.0601 (0.1887)	
training:	Epoch: [20][133/233]	Loss 0.0981 (0.1881)	
training:	Epoch: [20][134/233]	Loss 0.1984 (0.1881)	
training:	Epoch: [20][135/233]	Loss 0.2154 (0.1883)	
training:	Epoch: [20][136/233]	Loss 0.2871 (0.1891)	
training:	Epoch: [20][137/233]	Loss 0.1026 (0.1884)	
training:	Epoch: [20][138/233]	Loss 0.2909 (0.1892)	
training:	Epoch: [20][139/233]	Loss 0.2657 (0.1897)	
training:	Epoch: [20][140/233]	Loss 0.1223 (0.1892)	
training:	Epoch: [20][141/233]	Loss 0.3270 (0.1902)	
training:	Epoch: [20][142/233]	Loss 0.0780 (0.1894)	
training:	Epoch: [20][143/233]	Loss 0.3892 (0.1908)	
training:	Epoch: [20][144/233]	Loss 0.1489 (0.1905)	
training:	Epoch: [20][145/233]	Loss 0.0726 (0.1897)	
training:	Epoch: [20][146/233]	Loss 0.1193 (0.1892)	
training:	Epoch: [20][147/233]	Loss 0.1155 (0.1887)	
training:	Epoch: [20][148/233]	Loss 0.0561 (0.1878)	
training:	Epoch: [20][149/233]	Loss 0.2150 (0.1880)	
training:	Epoch: [20][150/233]	Loss 0.3426 (0.1891)	
training:	Epoch: [20][151/233]	Loss 0.1202 (0.1886)	
training:	Epoch: [20][152/233]	Loss 0.1146 (0.1881)	
training:	Epoch: [20][153/233]	Loss 0.2166 (0.1883)	
training:	Epoch: [20][154/233]	Loss 0.2434 (0.1887)	
training:	Epoch: [20][155/233]	Loss 0.1273 (0.1883)	
training:	Epoch: [20][156/233]	Loss 0.0863 (0.1876)	
training:	Epoch: [20][157/233]	Loss 0.3077 (0.1884)	
training:	Epoch: [20][158/233]	Loss 0.4158 (0.1898)	
training:	Epoch: [20][159/233]	Loss 0.2218 (0.1900)	
training:	Epoch: [20][160/233]	Loss 0.2949 (0.1907)	
training:	Epoch: [20][161/233]	Loss 0.1498 (0.1904)	
training:	Epoch: [20][162/233]	Loss 0.3608 (0.1915)	
training:	Epoch: [20][163/233]	Loss 0.1719 (0.1914)	
training:	Epoch: [20][164/233]	Loss 0.0952 (0.1908)	
training:	Epoch: [20][165/233]	Loss 0.1197 (0.1903)	
training:	Epoch: [20][166/233]	Loss 0.0799 (0.1897)	
training:	Epoch: [20][167/233]	Loss 0.3063 (0.1904)	
training:	Epoch: [20][168/233]	Loss 0.2066 (0.1905)	
training:	Epoch: [20][169/233]	Loss 0.0989 (0.1899)	
training:	Epoch: [20][170/233]	Loss 0.1883 (0.1899)	
training:	Epoch: [20][171/233]	Loss 0.1121 (0.1895)	
training:	Epoch: [20][172/233]	Loss 0.0814 (0.1888)	
training:	Epoch: [20][173/233]	Loss 0.1680 (0.1887)	
training:	Epoch: [20][174/233]	Loss 0.2982 (0.1893)	
training:	Epoch: [20][175/233]	Loss 0.1914 (0.1894)	
training:	Epoch: [20][176/233]	Loss 0.2258 (0.1896)	
training:	Epoch: [20][177/233]	Loss 0.1692 (0.1894)	
training:	Epoch: [20][178/233]	Loss 0.1628 (0.1893)	
training:	Epoch: [20][179/233]	Loss 0.1218 (0.1889)	
training:	Epoch: [20][180/233]	Loss 0.3519 (0.1898)	
training:	Epoch: [20][181/233]	Loss 0.0942 (0.1893)	
training:	Epoch: [20][182/233]	Loss 0.3065 (0.1899)	
training:	Epoch: [20][183/233]	Loss 0.2116 (0.1901)	
training:	Epoch: [20][184/233]	Loss 0.3076 (0.1907)	
training:	Epoch: [20][185/233]	Loss 0.1259 (0.1903)	
training:	Epoch: [20][186/233]	Loss 0.4223 (0.1916)	
training:	Epoch: [20][187/233]	Loss 0.2088 (0.1917)	
training:	Epoch: [20][188/233]	Loss 0.2291 (0.1919)	
training:	Epoch: [20][189/233]	Loss 0.4234 (0.1931)	
training:	Epoch: [20][190/233]	Loss 0.1612 (0.1929)	
training:	Epoch: [20][191/233]	Loss 0.2213 (0.1931)	
training:	Epoch: [20][192/233]	Loss 0.2676 (0.1935)	
training:	Epoch: [20][193/233]	Loss 0.2347 (0.1937)	
training:	Epoch: [20][194/233]	Loss 0.0901 (0.1932)	
training:	Epoch: [20][195/233]	Loss 0.4744 (0.1946)	
training:	Epoch: [20][196/233]	Loss 0.3980 (0.1956)	
training:	Epoch: [20][197/233]	Loss 0.2977 (0.1962)	
training:	Epoch: [20][198/233]	Loss 0.2084 (0.1962)	
training:	Epoch: [20][199/233]	Loss 0.0937 (0.1957)	
training:	Epoch: [20][200/233]	Loss 0.1001 (0.1952)	
training:	Epoch: [20][201/233]	Loss 0.3123 (0.1958)	
training:	Epoch: [20][202/233]	Loss 0.2511 (0.1961)	
training:	Epoch: [20][203/233]	Loss 0.1824 (0.1960)	
training:	Epoch: [20][204/233]	Loss 0.2202 (0.1961)	
training:	Epoch: [20][205/233]	Loss 0.2655 (0.1965)	
training:	Epoch: [20][206/233]	Loss 0.1278 (0.1961)	
training:	Epoch: [20][207/233]	Loss 0.1070 (0.1957)	
training:	Epoch: [20][208/233]	Loss 0.2040 (0.1957)	
training:	Epoch: [20][209/233]	Loss 0.0600 (0.1951)	
training:	Epoch: [20][210/233]	Loss 0.1024 (0.1947)	
training:	Epoch: [20][211/233]	Loss 0.2582 (0.1950)	
training:	Epoch: [20][212/233]	Loss 0.1368 (0.1947)	
training:	Epoch: [20][213/233]	Loss 0.3949 (0.1956)	
training:	Epoch: [20][214/233]	Loss 0.1526 (0.1954)	
training:	Epoch: [20][215/233]	Loss 0.2147 (0.1955)	
training:	Epoch: [20][216/233]	Loss 0.2542 (0.1958)	
training:	Epoch: [20][217/233]	Loss 0.3076 (0.1963)	
training:	Epoch: [20][218/233]	Loss 0.4526 (0.1975)	
training:	Epoch: [20][219/233]	Loss 0.4009 (0.1984)	
training:	Epoch: [20][220/233]	Loss 0.0586 (0.1978)	
training:	Epoch: [20][221/233]	Loss 0.2146 (0.1978)	
training:	Epoch: [20][222/233]	Loss 0.3780 (0.1987)	
training:	Epoch: [20][223/233]	Loss 0.2251 (0.1988)	
training:	Epoch: [20][224/233]	Loss 0.1933 (0.1987)	
training:	Epoch: [20][225/233]	Loss 0.2223 (0.1989)	
training:	Epoch: [20][226/233]	Loss 0.1869 (0.1988)	
training:	Epoch: [20][227/233]	Loss 0.1548 (0.1986)	
training:	Epoch: [20][228/233]	Loss 0.1872 (0.1986)	
training:	Epoch: [20][229/233]	Loss 0.1276 (0.1982)	
training:	Epoch: [20][230/233]	Loss 0.3453 (0.1989)	
training:	Epoch: [20][231/233]	Loss 0.3032 (0.1993)	
training:	Epoch: [20][232/233]	Loss 0.0587 (0.1987)	
training:	Epoch: [20][233/233]	Loss 0.2576 (0.1990)	
Training:	 Loss: 0.1985

Training:	 ACC: 0.9586 0.9585 0.9574 0.9597
Validation:	 ACC: 0.7988 0.7994 0.8100 0.7876
Validation:	 Best_BACC: 0.8162 0.8154 0.7998 0.8326
Validation:	 Loss: 0.5142
Pretraining:	Epoch 21/200
----------
training:	Epoch: [21][1/233]	Loss 0.1946 (0.1946)	
training:	Epoch: [21][2/233]	Loss 0.2523 (0.2235)	
training:	Epoch: [21][3/233]	Loss 0.1073 (0.1848)	
training:	Epoch: [21][4/233]	Loss 0.0701 (0.1561)	
training:	Epoch: [21][5/233]	Loss 0.0956 (0.1440)	
training:	Epoch: [21][6/233]	Loss 0.1003 (0.1367)	
training:	Epoch: [21][7/233]	Loss 0.0829 (0.1290)	
training:	Epoch: [21][8/233]	Loss 0.0845 (0.1235)	
training:	Epoch: [21][9/233]	Loss 0.2253 (0.1348)	
training:	Epoch: [21][10/233]	Loss 0.0918 (0.1305)	
training:	Epoch: [21][11/233]	Loss 0.0652 (0.1246)	
training:	Epoch: [21][12/233]	Loss 0.1860 (0.1297)	
training:	Epoch: [21][13/233]	Loss 0.1244 (0.1293)	
training:	Epoch: [21][14/233]	Loss 0.1532 (0.1310)	
training:	Epoch: [21][15/233]	Loss 0.2954 (0.1419)	
training:	Epoch: [21][16/233]	Loss 0.3170 (0.1529)	
training:	Epoch: [21][17/233]	Loss 0.2624 (0.1593)	
training:	Epoch: [21][18/233]	Loss 0.2224 (0.1628)	
training:	Epoch: [21][19/233]	Loss 0.3758 (0.1740)	
training:	Epoch: [21][20/233]	Loss 0.0580 (0.1682)	
training:	Epoch: [21][21/233]	Loss 0.2512 (0.1722)	
training:	Epoch: [21][22/233]	Loss 0.2173 (0.1742)	
training:	Epoch: [21][23/233]	Loss 0.1234 (0.1720)	
training:	Epoch: [21][24/233]	Loss 0.0981 (0.1690)	
training:	Epoch: [21][25/233]	Loss 0.1834 (0.1695)	
training:	Epoch: [21][26/233]	Loss 0.2053 (0.1709)	
training:	Epoch: [21][27/233]	Loss 0.0758 (0.1674)	
training:	Epoch: [21][28/233]	Loss 0.1559 (0.1670)	
training:	Epoch: [21][29/233]	Loss 0.0956 (0.1645)	
training:	Epoch: [21][30/233]	Loss 0.4021 (0.1724)	
training:	Epoch: [21][31/233]	Loss 0.1436 (0.1715)	
training:	Epoch: [21][32/233]	Loss 0.1301 (0.1702)	
training:	Epoch: [21][33/233]	Loss 0.2199 (0.1717)	
training:	Epoch: [21][34/233]	Loss 0.3375 (0.1766)	
training:	Epoch: [21][35/233]	Loss 0.1104 (0.1747)	
training:	Epoch: [21][36/233]	Loss 0.1219 (0.1732)	
training:	Epoch: [21][37/233]	Loss 0.1903 (0.1737)	
training:	Epoch: [21][38/233]	Loss 0.0644 (0.1708)	
training:	Epoch: [21][39/233]	Loss 0.1703 (0.1708)	
training:	Epoch: [21][40/233]	Loss 0.1671 (0.1707)	
training:	Epoch: [21][41/233]	Loss 0.1190 (0.1695)	
training:	Epoch: [21][42/233]	Loss 0.2429 (0.1712)	
training:	Epoch: [21][43/233]	Loss 0.2411 (0.1728)	
training:	Epoch: [21][44/233]	Loss 0.1319 (0.1719)	
training:	Epoch: [21][45/233]	Loss 0.1563 (0.1715)	
training:	Epoch: [21][46/233]	Loss 0.1289 (0.1706)	
training:	Epoch: [21][47/233]	Loss 0.2238 (0.1718)	
training:	Epoch: [21][48/233]	Loss 0.1253 (0.1708)	
training:	Epoch: [21][49/233]	Loss 0.1800 (0.1710)	
training:	Epoch: [21][50/233]	Loss 0.0638 (0.1688)	
training:	Epoch: [21][51/233]	Loss 0.1173 (0.1678)	
training:	Epoch: [21][52/233]	Loss 0.0778 (0.1661)	
training:	Epoch: [21][53/233]	Loss 0.1253 (0.1653)	
training:	Epoch: [21][54/233]	Loss 0.1502 (0.1650)	
training:	Epoch: [21][55/233]	Loss 0.0715 (0.1633)	
training:	Epoch: [21][56/233]	Loss 0.4061 (0.1677)	
training:	Epoch: [21][57/233]	Loss 0.1179 (0.1668)	
training:	Epoch: [21][58/233]	Loss 0.0494 (0.1648)	
training:	Epoch: [21][59/233]	Loss 0.1690 (0.1648)	
training:	Epoch: [21][60/233]	Loss 0.0976 (0.1637)	
training:	Epoch: [21][61/233]	Loss 0.1182 (0.1630)	
training:	Epoch: [21][62/233]	Loss 0.1018 (0.1620)	
training:	Epoch: [21][63/233]	Loss 0.0736 (0.1606)	
training:	Epoch: [21][64/233]	Loss 0.2676 (0.1623)	
training:	Epoch: [21][65/233]	Loss 0.0836 (0.1610)	
training:	Epoch: [21][66/233]	Loss 0.1855 (0.1614)	
training:	Epoch: [21][67/233]	Loss 0.2330 (0.1625)	
training:	Epoch: [21][68/233]	Loss 0.1121 (0.1617)	
training:	Epoch: [21][69/233]	Loss 0.2429 (0.1629)	
training:	Epoch: [21][70/233]	Loss 0.0656 (0.1615)	
training:	Epoch: [21][71/233]	Loss 0.1990 (0.1621)	
training:	Epoch: [21][72/233]	Loss 0.1734 (0.1622)	
training:	Epoch: [21][73/233]	Loss 0.2849 (0.1639)	
training:	Epoch: [21][74/233]	Loss 0.2039 (0.1644)	
training:	Epoch: [21][75/233]	Loss 0.1928 (0.1648)	
training:	Epoch: [21][76/233]	Loss 0.0527 (0.1633)	
training:	Epoch: [21][77/233]	Loss 0.1646 (0.1634)	
training:	Epoch: [21][78/233]	Loss 0.3232 (0.1654)	
training:	Epoch: [21][79/233]	Loss 0.1273 (0.1649)	
training:	Epoch: [21][80/233]	Loss 0.0687 (0.1637)	
training:	Epoch: [21][81/233]	Loss 0.2677 (0.1650)	
training:	Epoch: [21][82/233]	Loss 0.2030 (0.1655)	
training:	Epoch: [21][83/233]	Loss 0.2801 (0.1668)	
training:	Epoch: [21][84/233]	Loss 0.1264 (0.1664)	
training:	Epoch: [21][85/233]	Loss 0.0843 (0.1654)	
training:	Epoch: [21][86/233]	Loss 0.0626 (0.1642)	
training:	Epoch: [21][87/233]	Loss 0.1857 (0.1645)	
training:	Epoch: [21][88/233]	Loss 0.1629 (0.1644)	
training:	Epoch: [21][89/233]	Loss 0.2293 (0.1652)	
training:	Epoch: [21][90/233]	Loss 0.2220 (0.1658)	
training:	Epoch: [21][91/233]	Loss 0.2245 (0.1664)	
training:	Epoch: [21][92/233]	Loss 0.1429 (0.1662)	
training:	Epoch: [21][93/233]	Loss 0.2131 (0.1667)	
training:	Epoch: [21][94/233]	Loss 0.1100 (0.1661)	
training:	Epoch: [21][95/233]	Loss 0.1577 (0.1660)	
training:	Epoch: [21][96/233]	Loss 0.1700 (0.1660)	
training:	Epoch: [21][97/233]	Loss 0.1491 (0.1659)	
training:	Epoch: [21][98/233]	Loss 0.2540 (0.1668)	
training:	Epoch: [21][99/233]	Loss 0.1091 (0.1662)	
training:	Epoch: [21][100/233]	Loss 0.1609 (0.1661)	
training:	Epoch: [21][101/233]	Loss 0.1328 (0.1658)	
training:	Epoch: [21][102/233]	Loss 0.1692 (0.1658)	
training:	Epoch: [21][103/233]	Loss 0.0656 (0.1649)	
training:	Epoch: [21][104/233]	Loss 0.1263 (0.1645)	
training:	Epoch: [21][105/233]	Loss 0.2311 (0.1651)	
training:	Epoch: [21][106/233]	Loss 0.0619 (0.1641)	
training:	Epoch: [21][107/233]	Loss 0.0958 (0.1635)	
training:	Epoch: [21][108/233]	Loss 0.1254 (0.1632)	
training:	Epoch: [21][109/233]	Loss 0.1038 (0.1626)	
training:	Epoch: [21][110/233]	Loss 0.1751 (0.1627)	
training:	Epoch: [21][111/233]	Loss 0.1530 (0.1626)	
training:	Epoch: [21][112/233]	Loss 0.1647 (0.1627)	
training:	Epoch: [21][113/233]	Loss 0.1107 (0.1622)	
training:	Epoch: [21][114/233]	Loss 0.2567 (0.1630)	
training:	Epoch: [21][115/233]	Loss 0.0874 (0.1624)	
training:	Epoch: [21][116/233]	Loss 0.2450 (0.1631)	
training:	Epoch: [21][117/233]	Loss 0.4099 (0.1652)	
training:	Epoch: [21][118/233]	Loss 0.2395 (0.1658)	
training:	Epoch: [21][119/233]	Loss 0.2691 (0.1667)	
training:	Epoch: [21][120/233]	Loss 0.1579 (0.1666)	
training:	Epoch: [21][121/233]	Loss 0.2338 (0.1672)	
training:	Epoch: [21][122/233]	Loss 0.1668 (0.1672)	
training:	Epoch: [21][123/233]	Loss 0.2725 (0.1680)	
training:	Epoch: [21][124/233]	Loss 0.0673 (0.1672)	
training:	Epoch: [21][125/233]	Loss 0.1028 (0.1667)	
training:	Epoch: [21][126/233]	Loss 0.0920 (0.1661)	
training:	Epoch: [21][127/233]	Loss 0.0477 (0.1652)	
training:	Epoch: [21][128/233]	Loss 0.4373 (0.1673)	
training:	Epoch: [21][129/233]	Loss 0.0568 (0.1664)	
training:	Epoch: [21][130/233]	Loss 0.1637 (0.1664)	
training:	Epoch: [21][131/233]	Loss 0.0550 (0.1656)	
training:	Epoch: [21][132/233]	Loss 0.2421 (0.1661)	
training:	Epoch: [21][133/233]	Loss 0.2765 (0.1670)	
training:	Epoch: [21][134/233]	Loss 0.1024 (0.1665)	
training:	Epoch: [21][135/233]	Loss 0.0659 (0.1658)	
training:	Epoch: [21][136/233]	Loss 0.2471 (0.1663)	
training:	Epoch: [21][137/233]	Loss 0.1714 (0.1664)	
training:	Epoch: [21][138/233]	Loss 0.3049 (0.1674)	
training:	Epoch: [21][139/233]	Loss 0.5036 (0.1698)	
training:	Epoch: [21][140/233]	Loss 0.2723 (0.1705)	
training:	Epoch: [21][141/233]	Loss 0.1313 (0.1703)	
training:	Epoch: [21][142/233]	Loss 0.0946 (0.1697)	
training:	Epoch: [21][143/233]	Loss 0.1840 (0.1698)	
training:	Epoch: [21][144/233]	Loss 0.2827 (0.1706)	
training:	Epoch: [21][145/233]	Loss 0.2599 (0.1712)	
training:	Epoch: [21][146/233]	Loss 0.2197 (0.1716)	
training:	Epoch: [21][147/233]	Loss 0.0898 (0.1710)	
training:	Epoch: [21][148/233]	Loss 0.0825 (0.1704)	
training:	Epoch: [21][149/233]	Loss 0.3488 (0.1716)	
training:	Epoch: [21][150/233]	Loss 0.2051 (0.1718)	
training:	Epoch: [21][151/233]	Loss 0.3323 (0.1729)	
training:	Epoch: [21][152/233]	Loss 0.0883 (0.1723)	
training:	Epoch: [21][153/233]	Loss 0.1361 (0.1721)	
training:	Epoch: [21][154/233]	Loss 0.0923 (0.1716)	
training:	Epoch: [21][155/233]	Loss 0.0795 (0.1710)	
training:	Epoch: [21][156/233]	Loss 0.0411 (0.1701)	
training:	Epoch: [21][157/233]	Loss 0.1491 (0.1700)	
training:	Epoch: [21][158/233]	Loss 0.0973 (0.1696)	
training:	Epoch: [21][159/233]	Loss 0.2552 (0.1701)	
training:	Epoch: [21][160/233]	Loss 0.0786 (0.1695)	
training:	Epoch: [21][161/233]	Loss 0.1259 (0.1693)	
training:	Epoch: [21][162/233]	Loss 0.0846 (0.1687)	
training:	Epoch: [21][163/233]	Loss 0.1077 (0.1684)	
training:	Epoch: [21][164/233]	Loss 0.0613 (0.1677)	
training:	Epoch: [21][165/233]	Loss 0.2810 (0.1684)	
training:	Epoch: [21][166/233]	Loss 0.3141 (0.1693)	
training:	Epoch: [21][167/233]	Loss 0.1859 (0.1694)	
training:	Epoch: [21][168/233]	Loss 0.0727 (0.1688)	
training:	Epoch: [21][169/233]	Loss 0.0808 (0.1683)	
training:	Epoch: [21][170/233]	Loss 0.3686 (0.1694)	
training:	Epoch: [21][171/233]	Loss 0.0547 (0.1688)	
training:	Epoch: [21][172/233]	Loss 0.2339 (0.1692)	
training:	Epoch: [21][173/233]	Loss 0.1865 (0.1693)	
training:	Epoch: [21][174/233]	Loss 0.0982 (0.1688)	
training:	Epoch: [21][175/233]	Loss 0.2128 (0.1691)	
training:	Epoch: [21][176/233]	Loss 0.2138 (0.1694)	
training:	Epoch: [21][177/233]	Loss 0.0877 (0.1689)	
training:	Epoch: [21][178/233]	Loss 0.2040 (0.1691)	
training:	Epoch: [21][179/233]	Loss 0.2171 (0.1694)	
training:	Epoch: [21][180/233]	Loss 0.0355 (0.1686)	
training:	Epoch: [21][181/233]	Loss 0.0973 (0.1682)	
training:	Epoch: [21][182/233]	Loss 0.2310 (0.1686)	
training:	Epoch: [21][183/233]	Loss 0.4381 (0.1700)	
training:	Epoch: [21][184/233]	Loss 0.2543 (0.1705)	
training:	Epoch: [21][185/233]	Loss 0.1825 (0.1706)	
training:	Epoch: [21][186/233]	Loss 0.1491 (0.1704)	
training:	Epoch: [21][187/233]	Loss 0.0559 (0.1698)	
training:	Epoch: [21][188/233]	Loss 0.0877 (0.1694)	
training:	Epoch: [21][189/233]	Loss 0.2271 (0.1697)	
training:	Epoch: [21][190/233]	Loss 0.0435 (0.1690)	
training:	Epoch: [21][191/233]	Loss 0.0847 (0.1686)	
training:	Epoch: [21][192/233]	Loss 0.2721 (0.1691)	
training:	Epoch: [21][193/233]	Loss 0.2394 (0.1695)	
training:	Epoch: [21][194/233]	Loss 0.1118 (0.1692)	
training:	Epoch: [21][195/233]	Loss 0.0699 (0.1687)	
training:	Epoch: [21][196/233]	Loss 0.3229 (0.1695)	
training:	Epoch: [21][197/233]	Loss 0.1255 (0.1693)	
training:	Epoch: [21][198/233]	Loss 0.0747 (0.1688)	
training:	Epoch: [21][199/233]	Loss 0.0378 (0.1681)	
training:	Epoch: [21][200/233]	Loss 0.0778 (0.1677)	
training:	Epoch: [21][201/233]	Loss 0.0591 (0.1671)	
training:	Epoch: [21][202/233]	Loss 0.2593 (0.1676)	
training:	Epoch: [21][203/233]	Loss 0.2594 (0.1680)	
training:	Epoch: [21][204/233]	Loss 0.2757 (0.1686)	
training:	Epoch: [21][205/233]	Loss 0.1859 (0.1686)	
training:	Epoch: [21][206/233]	Loss 0.3104 (0.1693)	
training:	Epoch: [21][207/233]	Loss 0.3008 (0.1700)	
training:	Epoch: [21][208/233]	Loss 0.2477 (0.1703)	
training:	Epoch: [21][209/233]	Loss 0.0488 (0.1698)	
training:	Epoch: [21][210/233]	Loss 0.3060 (0.1704)	
training:	Epoch: [21][211/233]	Loss 0.2141 (0.1706)	
training:	Epoch: [21][212/233]	Loss 0.2111 (0.1708)	
training:	Epoch: [21][213/233]	Loss 0.2528 (0.1712)	
training:	Epoch: [21][214/233]	Loss 0.2265 (0.1715)	
training:	Epoch: [21][215/233]	Loss 0.1417 (0.1713)	
training:	Epoch: [21][216/233]	Loss 0.3409 (0.1721)	
training:	Epoch: [21][217/233]	Loss 0.1123 (0.1718)	
training:	Epoch: [21][218/233]	Loss 0.1776 (0.1719)	
training:	Epoch: [21][219/233]	Loss 0.1094 (0.1716)	
training:	Epoch: [21][220/233]	Loss 0.0919 (0.1712)	
training:	Epoch: [21][221/233]	Loss 0.2540 (0.1716)	
training:	Epoch: [21][222/233]	Loss 0.0867 (0.1712)	
training:	Epoch: [21][223/233]	Loss 0.1018 (0.1709)	
training:	Epoch: [21][224/233]	Loss 0.1946 (0.1710)	
training:	Epoch: [21][225/233]	Loss 0.1016 (0.1707)	
training:	Epoch: [21][226/233]	Loss 0.1888 (0.1708)	
training:	Epoch: [21][227/233]	Loss 0.2985 (0.1713)	
training:	Epoch: [21][228/233]	Loss 0.0922 (0.1710)	
training:	Epoch: [21][229/233]	Loss 0.1925 (0.1711)	
training:	Epoch: [21][230/233]	Loss 0.1595 (0.1710)	
training:	Epoch: [21][231/233]	Loss 0.1577 (0.1710)	
training:	Epoch: [21][232/233]	Loss 0.1452 (0.1709)	
training:	Epoch: [21][233/233]	Loss 0.0558 (0.1704)	
Training:	 Loss: 0.1700

Training:	 ACC: 0.9631 0.9633 0.9685 0.9577
Validation:	 ACC: 0.8019 0.8031 0.8274 0.7764
Validation:	 Best_BACC: 0.8162 0.8154 0.7998 0.8326
Validation:	 Loss: 0.5432
Pretraining:	Epoch 22/200
----------
training:	Epoch: [22][1/233]	Loss 0.1472 (0.1472)	
training:	Epoch: [22][2/233]	Loss 0.1315 (0.1393)	
training:	Epoch: [22][3/233]	Loss 0.1240 (0.1342)	
training:	Epoch: [22][4/233]	Loss 0.1541 (0.1392)	
training:	Epoch: [22][5/233]	Loss 0.1104 (0.1334)	
training:	Epoch: [22][6/233]	Loss 0.2603 (0.1546)	
training:	Epoch: [22][7/233]	Loss 0.0491 (0.1395)	
training:	Epoch: [22][8/233]	Loss 0.1533 (0.1412)	
training:	Epoch: [22][9/233]	Loss 0.1673 (0.1441)	
training:	Epoch: [22][10/233]	Loss 0.0671 (0.1364)	
training:	Epoch: [22][11/233]	Loss 0.1988 (0.1421)	
training:	Epoch: [22][12/233]	Loss 0.1376 (0.1417)	
training:	Epoch: [22][13/233]	Loss 0.0516 (0.1348)	
training:	Epoch: [22][14/233]	Loss 0.0735 (0.1304)	
training:	Epoch: [22][15/233]	Loss 0.1918 (0.1345)	
training:	Epoch: [22][16/233]	Loss 0.2679 (0.1428)	
training:	Epoch: [22][17/233]	Loss 0.3302 (0.1539)	
training:	Epoch: [22][18/233]	Loss 0.1163 (0.1518)	
training:	Epoch: [22][19/233]	Loss 0.1318 (0.1507)	
training:	Epoch: [22][20/233]	Loss 0.0877 (0.1476)	
training:	Epoch: [22][21/233]	Loss 0.1384 (0.1471)	
training:	Epoch: [22][22/233]	Loss 0.2154 (0.1502)	
training:	Epoch: [22][23/233]	Loss 0.1713 (0.1512)	
training:	Epoch: [22][24/233]	Loss 0.1565 (0.1514)	
training:	Epoch: [22][25/233]	Loss 0.1456 (0.1512)	
training:	Epoch: [22][26/233]	Loss 0.0394 (0.1469)	
training:	Epoch: [22][27/233]	Loss 0.2153 (0.1494)	
training:	Epoch: [22][28/233]	Loss 0.1898 (0.1508)	
training:	Epoch: [22][29/233]	Loss 0.2407 (0.1539)	
training:	Epoch: [22][30/233]	Loss 0.1812 (0.1548)	
training:	Epoch: [22][31/233]	Loss 0.0753 (0.1523)	
training:	Epoch: [22][32/233]	Loss 0.1428 (0.1520)	
training:	Epoch: [22][33/233]	Loss 0.0695 (0.1495)	
training:	Epoch: [22][34/233]	Loss 0.0826 (0.1475)	
training:	Epoch: [22][35/233]	Loss 0.0677 (0.1452)	
training:	Epoch: [22][36/233]	Loss 0.1616 (0.1457)	
training:	Epoch: [22][37/233]	Loss 0.1922 (0.1469)	
training:	Epoch: [22][38/233]	Loss 0.2642 (0.1500)	
training:	Epoch: [22][39/233]	Loss 0.1100 (0.1490)	
training:	Epoch: [22][40/233]	Loss 0.3903 (0.1550)	
training:	Epoch: [22][41/233]	Loss 0.0899 (0.1534)	
training:	Epoch: [22][42/233]	Loss 0.1582 (0.1536)	
training:	Epoch: [22][43/233]	Loss 0.3165 (0.1574)	
training:	Epoch: [22][44/233]	Loss 0.0550 (0.1550)	
training:	Epoch: [22][45/233]	Loss 0.0868 (0.1535)	
training:	Epoch: [22][46/233]	Loss 0.0728 (0.1518)	
training:	Epoch: [22][47/233]	Loss 0.1205 (0.1511)	
training:	Epoch: [22][48/233]	Loss 0.1797 (0.1517)	
training:	Epoch: [22][49/233]	Loss 0.2427 (0.1535)	
training:	Epoch: [22][50/233]	Loss 0.2257 (0.1550)	
training:	Epoch: [22][51/233]	Loss 0.0649 (0.1532)	
training:	Epoch: [22][52/233]	Loss 0.0979 (0.1522)	
training:	Epoch: [22][53/233]	Loss 0.3186 (0.1553)	
training:	Epoch: [22][54/233]	Loss 0.1333 (0.1549)	
training:	Epoch: [22][55/233]	Loss 0.2281 (0.1562)	
training:	Epoch: [22][56/233]	Loss 0.4650 (0.1617)	
training:	Epoch: [22][57/233]	Loss 0.0979 (0.1606)	
training:	Epoch: [22][58/233]	Loss 0.2356 (0.1619)	
training:	Epoch: [22][59/233]	Loss 0.1334 (0.1614)	
training:	Epoch: [22][60/233]	Loss 0.1467 (0.1612)	
training:	Epoch: [22][61/233]	Loss 0.0413 (0.1592)	
training:	Epoch: [22][62/233]	Loss 0.2076 (0.1600)	
training:	Epoch: [22][63/233]	Loss 0.2232 (0.1610)	
training:	Epoch: [22][64/233]	Loss 0.0945 (0.1600)	
training:	Epoch: [22][65/233]	Loss 0.3238 (0.1625)	
training:	Epoch: [22][66/233]	Loss 0.1244 (0.1619)	
training:	Epoch: [22][67/233]	Loss 0.0892 (0.1608)	
training:	Epoch: [22][68/233]	Loss 0.2068 (0.1615)	
training:	Epoch: [22][69/233]	Loss 0.0859 (0.1604)	
training:	Epoch: [22][70/233]	Loss 0.0577 (0.1589)	
training:	Epoch: [22][71/233]	Loss 0.2127 (0.1597)	
training:	Epoch: [22][72/233]	Loss 0.1179 (0.1591)	
training:	Epoch: [22][73/233]	Loss 0.2839 (0.1608)	
training:	Epoch: [22][74/233]	Loss 0.0988 (0.1600)	
training:	Epoch: [22][75/233]	Loss 0.1490 (0.1598)	
training:	Epoch: [22][76/233]	Loss 0.3654 (0.1625)	
training:	Epoch: [22][77/233]	Loss 0.0829 (0.1615)	
training:	Epoch: [22][78/233]	Loss 0.1617 (0.1615)	
training:	Epoch: [22][79/233]	Loss 0.1100 (0.1609)	
training:	Epoch: [22][80/233]	Loss 0.0713 (0.1597)	
training:	Epoch: [22][81/233]	Loss 0.1380 (0.1595)	
training:	Epoch: [22][82/233]	Loss 0.2083 (0.1601)	
training:	Epoch: [22][83/233]	Loss 0.1812 (0.1603)	
training:	Epoch: [22][84/233]	Loss 0.1578 (0.1603)	
training:	Epoch: [22][85/233]	Loss 0.2996 (0.1619)	
training:	Epoch: [22][86/233]	Loss 0.1159 (0.1614)	
training:	Epoch: [22][87/233]	Loss 0.1489 (0.1612)	
training:	Epoch: [22][88/233]	Loss 0.0846 (0.1604)	
training:	Epoch: [22][89/233]	Loss 0.3443 (0.1624)	
training:	Epoch: [22][90/233]	Loss 0.1383 (0.1622)	
training:	Epoch: [22][91/233]	Loss 0.1508 (0.1621)	
training:	Epoch: [22][92/233]	Loss 0.1950 (0.1624)	
training:	Epoch: [22][93/233]	Loss 0.0688 (0.1614)	
training:	Epoch: [22][94/233]	Loss 0.1289 (0.1611)	
training:	Epoch: [22][95/233]	Loss 0.1614 (0.1611)	
training:	Epoch: [22][96/233]	Loss 0.0776 (0.1602)	
training:	Epoch: [22][97/233]	Loss 0.0803 (0.1594)	
training:	Epoch: [22][98/233]	Loss 0.1491 (0.1593)	
training:	Epoch: [22][99/233]	Loss 0.1112 (0.1588)	
training:	Epoch: [22][100/233]	Loss 0.2543 (0.1597)	
training:	Epoch: [22][101/233]	Loss 0.2202 (0.1603)	
training:	Epoch: [22][102/233]	Loss 0.1209 (0.1599)	
training:	Epoch: [22][103/233]	Loss 0.3629 (0.1619)	
training:	Epoch: [22][104/233]	Loss 0.2321 (0.1626)	
training:	Epoch: [22][105/233]	Loss 0.1535 (0.1625)	
training:	Epoch: [22][106/233]	Loss 0.2124 (0.1630)	
training:	Epoch: [22][107/233]	Loss 0.1956 (0.1633)	
training:	Epoch: [22][108/233]	Loss 0.1644 (0.1633)	
training:	Epoch: [22][109/233]	Loss 0.2231 (0.1638)	
training:	Epoch: [22][110/233]	Loss 0.1951 (0.1641)	
training:	Epoch: [22][111/233]	Loss 0.2418 (0.1648)	
training:	Epoch: [22][112/233]	Loss 0.1774 (0.1649)	
training:	Epoch: [22][113/233]	Loss 0.1473 (0.1648)	
training:	Epoch: [22][114/233]	Loss 0.2461 (0.1655)	
training:	Epoch: [22][115/233]	Loss 0.1242 (0.1651)	
training:	Epoch: [22][116/233]	Loss 0.0544 (0.1642)	
training:	Epoch: [22][117/233]	Loss 0.0916 (0.1636)	
training:	Epoch: [22][118/233]	Loss 0.1014 (0.1630)	
training:	Epoch: [22][119/233]	Loss 0.0395 (0.1620)	
training:	Epoch: [22][120/233]	Loss 0.2261 (0.1625)	
training:	Epoch: [22][121/233]	Loss 0.0902 (0.1619)	
training:	Epoch: [22][122/233]	Loss 0.2005 (0.1622)	
training:	Epoch: [22][123/233]	Loss 0.1063 (0.1618)	
training:	Epoch: [22][124/233]	Loss 0.0540 (0.1609)	
training:	Epoch: [22][125/233]	Loss 0.0667 (0.1602)	
training:	Epoch: [22][126/233]	Loss 0.0405 (0.1592)	
training:	Epoch: [22][127/233]	Loss 0.1046 (0.1588)	
training:	Epoch: [22][128/233]	Loss 0.0540 (0.1580)	
training:	Epoch: [22][129/233]	Loss 0.1587 (0.1580)	
training:	Epoch: [22][130/233]	Loss 0.1374 (0.1578)	
training:	Epoch: [22][131/233]	Loss 0.0965 (0.1573)	
training:	Epoch: [22][132/233]	Loss 0.1256 (0.1571)	
training:	Epoch: [22][133/233]	Loss 0.1329 (0.1569)	
training:	Epoch: [22][134/233]	Loss 0.0531 (0.1562)	
training:	Epoch: [22][135/233]	Loss 0.0719 (0.1555)	
training:	Epoch: [22][136/233]	Loss 0.0934 (0.1551)	
training:	Epoch: [22][137/233]	Loss 0.2716 (0.1559)	
training:	Epoch: [22][138/233]	Loss 0.0963 (0.1555)	
training:	Epoch: [22][139/233]	Loss 0.1730 (0.1556)	
training:	Epoch: [22][140/233]	Loss 0.0803 (0.1551)	
training:	Epoch: [22][141/233]	Loss 0.0578 (0.1544)	
training:	Epoch: [22][142/233]	Loss 0.1369 (0.1543)	
training:	Epoch: [22][143/233]	Loss 0.0645 (0.1536)	
training:	Epoch: [22][144/233]	Loss 0.0692 (0.1530)	
training:	Epoch: [22][145/233]	Loss 0.1412 (0.1530)	
training:	Epoch: [22][146/233]	Loss 0.0572 (0.1523)	
training:	Epoch: [22][147/233]	Loss 0.2330 (0.1529)	
training:	Epoch: [22][148/233]	Loss 0.0896 (0.1524)	
training:	Epoch: [22][149/233]	Loss 0.2162 (0.1529)	
training:	Epoch: [22][150/233]	Loss 0.1105 (0.1526)	
training:	Epoch: [22][151/233]	Loss 0.2007 (0.1529)	
training:	Epoch: [22][152/233]	Loss 0.1605 (0.1529)	
training:	Epoch: [22][153/233]	Loss 0.0577 (0.1523)	
training:	Epoch: [22][154/233]	Loss 0.0321 (0.1515)	
training:	Epoch: [22][155/233]	Loss 0.1496 (0.1515)	
training:	Epoch: [22][156/233]	Loss 0.1957 (0.1518)	
training:	Epoch: [22][157/233]	Loss 0.2240 (0.1523)	
training:	Epoch: [22][158/233]	Loss 0.0470 (0.1516)	
training:	Epoch: [22][159/233]	Loss 0.1347 (0.1515)	
training:	Epoch: [22][160/233]	Loss 0.0408 (0.1508)	
training:	Epoch: [22][161/233]	Loss 0.0999 (0.1505)	
training:	Epoch: [22][162/233]	Loss 0.1137 (0.1503)	
training:	Epoch: [22][163/233]	Loss 0.1597 (0.1503)	
training:	Epoch: [22][164/233]	Loss 0.0625 (0.1498)	
training:	Epoch: [22][165/233]	Loss 0.0580 (0.1492)	
training:	Epoch: [22][166/233]	Loss 0.2951 (0.1501)	
training:	Epoch: [22][167/233]	Loss 0.3247 (0.1512)	
training:	Epoch: [22][168/233]	Loss 0.3586 (0.1524)	
training:	Epoch: [22][169/233]	Loss 0.0620 (0.1519)	
training:	Epoch: [22][170/233]	Loss 0.3473 (0.1530)	
training:	Epoch: [22][171/233]	Loss 0.1442 (0.1530)	
training:	Epoch: [22][172/233]	Loss 0.0936 (0.1526)	
training:	Epoch: [22][173/233]	Loss 0.1589 (0.1526)	
training:	Epoch: [22][174/233]	Loss 0.1104 (0.1524)	
training:	Epoch: [22][175/233]	Loss 0.1915 (0.1526)	
training:	Epoch: [22][176/233]	Loss 0.1086 (0.1524)	
training:	Epoch: [22][177/233]	Loss 0.0944 (0.1521)	
training:	Epoch: [22][178/233]	Loss 0.1248 (0.1519)	
training:	Epoch: [22][179/233]	Loss 0.0882 (0.1515)	
training:	Epoch: [22][180/233]	Loss 0.2937 (0.1523)	
training:	Epoch: [22][181/233]	Loss 0.2251 (0.1527)	
training:	Epoch: [22][182/233]	Loss 0.4052 (0.1541)	
training:	Epoch: [22][183/233]	Loss 0.2623 (0.1547)	
training:	Epoch: [22][184/233]	Loss 0.2913 (0.1555)	
training:	Epoch: [22][185/233]	Loss 0.2376 (0.1559)	
training:	Epoch: [22][186/233]	Loss 0.2645 (0.1565)	
training:	Epoch: [22][187/233]	Loss 0.1584 (0.1565)	
training:	Epoch: [22][188/233]	Loss 0.2978 (0.1572)	
training:	Epoch: [22][189/233]	Loss 0.2230 (0.1576)	
training:	Epoch: [22][190/233]	Loss 0.1648 (0.1576)	
training:	Epoch: [22][191/233]	Loss 0.1658 (0.1577)	
training:	Epoch: [22][192/233]	Loss 0.1982 (0.1579)	
training:	Epoch: [22][193/233]	Loss 0.2682 (0.1585)	
training:	Epoch: [22][194/233]	Loss 0.1649 (0.1585)	
training:	Epoch: [22][195/233]	Loss 0.0527 (0.1579)	
training:	Epoch: [22][196/233]	Loss 0.0545 (0.1574)	
training:	Epoch: [22][197/233]	Loss 0.4356 (0.1588)	
training:	Epoch: [22][198/233]	Loss 0.0858 (0.1585)	
training:	Epoch: [22][199/233]	Loss 0.0461 (0.1579)	
training:	Epoch: [22][200/233]	Loss 0.1806 (0.1580)	
training:	Epoch: [22][201/233]	Loss 0.1303 (0.1579)	
training:	Epoch: [22][202/233]	Loss 0.0722 (0.1574)	
training:	Epoch: [22][203/233]	Loss 0.1714 (0.1575)	
training:	Epoch: [22][204/233]	Loss 0.1519 (0.1575)	
training:	Epoch: [22][205/233]	Loss 0.1667 (0.1575)	
training:	Epoch: [22][206/233]	Loss 0.2798 (0.1581)	
training:	Epoch: [22][207/233]	Loss 0.1885 (0.1583)	
training:	Epoch: [22][208/233]	Loss 0.1647 (0.1583)	
training:	Epoch: [22][209/233]	Loss 0.0609 (0.1578)	
training:	Epoch: [22][210/233]	Loss 0.2083 (0.1581)	
training:	Epoch: [22][211/233]	Loss 0.2911 (0.1587)	
training:	Epoch: [22][212/233]	Loss 0.1031 (0.1584)	
training:	Epoch: [22][213/233]	Loss 0.0812 (0.1581)	
training:	Epoch: [22][214/233]	Loss 0.1899 (0.1582)	
training:	Epoch: [22][215/233]	Loss 0.0807 (0.1579)	
training:	Epoch: [22][216/233]	Loss 0.0609 (0.1574)	
training:	Epoch: [22][217/233]	Loss 0.2626 (0.1579)	
training:	Epoch: [22][218/233]	Loss 0.1873 (0.1580)	
training:	Epoch: [22][219/233]	Loss 0.0867 (0.1577)	
training:	Epoch: [22][220/233]	Loss 0.2369 (0.1581)	
training:	Epoch: [22][221/233]	Loss 0.3337 (0.1589)	
training:	Epoch: [22][222/233]	Loss 0.2434 (0.1593)	
training:	Epoch: [22][223/233]	Loss 0.2689 (0.1597)	
training:	Epoch: [22][224/233]	Loss 0.2821 (0.1603)	
training:	Epoch: [22][225/233]	Loss 0.1057 (0.1600)	
training:	Epoch: [22][226/233]	Loss 0.2966 (0.1607)	
training:	Epoch: [22][227/233]	Loss 0.0356 (0.1601)	
training:	Epoch: [22][228/233]	Loss 0.0761 (0.1597)	
training:	Epoch: [22][229/233]	Loss 0.0463 (0.1592)	
training:	Epoch: [22][230/233]	Loss 0.0875 (0.1589)	
training:	Epoch: [22][231/233]	Loss 0.1535 (0.1589)	
training:	Epoch: [22][232/233]	Loss 0.1052 (0.1587)	
training:	Epoch: [22][233/233]	Loss 0.1589 (0.1587)	
Training:	 Loss: 0.1583

Training:	 ACC: 0.9714 0.9708 0.9587 0.9840
Validation:	 ACC: 0.8024 0.8015 0.7835 0.8213
Validation:	 Best_BACC: 0.8162 0.8154 0.7998 0.8326
Validation:	 Loss: 0.5664
Pretraining:	Epoch 23/200
----------
training:	Epoch: [23][1/233]	Loss 0.1145 (0.1145)	
training:	Epoch: [23][2/233]	Loss 0.1441 (0.1293)	
training:	Epoch: [23][3/233]	Loss 0.1387 (0.1324)	
training:	Epoch: [23][4/233]	Loss 0.0725 (0.1174)	
training:	Epoch: [23][5/233]	Loss 0.1562 (0.1252)	
training:	Epoch: [23][6/233]	Loss 0.2759 (0.1503)	
training:	Epoch: [23][7/233]	Loss 0.2296 (0.1616)	
training:	Epoch: [23][8/233]	Loss 0.1119 (0.1554)	
training:	Epoch: [23][9/233]	Loss 0.0487 (0.1436)	
training:	Epoch: [23][10/233]	Loss 0.2240 (0.1516)	
training:	Epoch: [23][11/233]	Loss 0.1715 (0.1534)	
training:	Epoch: [23][12/233]	Loss 0.1936 (0.1568)	
training:	Epoch: [23][13/233]	Loss 0.0308 (0.1471)	
training:	Epoch: [23][14/233]	Loss 0.1297 (0.1458)	
training:	Epoch: [23][15/233]	Loss 0.1091 (0.1434)	
training:	Epoch: [23][16/233]	Loss 0.1639 (0.1447)	
training:	Epoch: [23][17/233]	Loss 0.0634 (0.1399)	
training:	Epoch: [23][18/233]	Loss 0.0372 (0.1342)	
training:	Epoch: [23][19/233]	Loss 0.0385 (0.1291)	
training:	Epoch: [23][20/233]	Loss 0.1098 (0.1282)	
training:	Epoch: [23][21/233]	Loss 0.2723 (0.1350)	
training:	Epoch: [23][22/233]	Loss 0.0982 (0.1334)	
training:	Epoch: [23][23/233]	Loss 0.1368 (0.1335)	
training:	Epoch: [23][24/233]	Loss 0.0470 (0.1299)	
training:	Epoch: [23][25/233]	Loss 0.1442 (0.1305)	
training:	Epoch: [23][26/233]	Loss 0.1260 (0.1303)	
training:	Epoch: [23][27/233]	Loss 0.1563 (0.1313)	
training:	Epoch: [23][28/233]	Loss 0.2522 (0.1356)	
training:	Epoch: [23][29/233]	Loss 0.0577 (0.1329)	
training:	Epoch: [23][30/233]	Loss 0.0748 (0.1310)	
training:	Epoch: [23][31/233]	Loss 0.2134 (0.1336)	
training:	Epoch: [23][32/233]	Loss 0.1295 (0.1335)	
training:	Epoch: [23][33/233]	Loss 0.1301 (0.1334)	
training:	Epoch: [23][34/233]	Loss 0.0754 (0.1317)	
training:	Epoch: [23][35/233]	Loss 0.0535 (0.1295)	
training:	Epoch: [23][36/233]	Loss 0.1628 (0.1304)	
training:	Epoch: [23][37/233]	Loss 0.1083 (0.1298)	
training:	Epoch: [23][38/233]	Loss 0.1820 (0.1312)	
training:	Epoch: [23][39/233]	Loss 0.0624 (0.1294)	
training:	Epoch: [23][40/233]	Loss 0.2157 (0.1315)	
training:	Epoch: [23][41/233]	Loss 0.3399 (0.1366)	
training:	Epoch: [23][42/233]	Loss 0.1038 (0.1358)	
training:	Epoch: [23][43/233]	Loss 0.1296 (0.1357)	
training:	Epoch: [23][44/233]	Loss 0.0692 (0.1342)	
training:	Epoch: [23][45/233]	Loss 0.2094 (0.1359)	
training:	Epoch: [23][46/233]	Loss 0.0494 (0.1340)	
training:	Epoch: [23][47/233]	Loss 0.3702 (0.1390)	
training:	Epoch: [23][48/233]	Loss 0.2528 (0.1414)	
training:	Epoch: [23][49/233]	Loss 0.0611 (0.1397)	
training:	Epoch: [23][50/233]	Loss 0.0888 (0.1387)	
training:	Epoch: [23][51/233]	Loss 0.1353 (0.1387)	
training:	Epoch: [23][52/233]	Loss 0.1264 (0.1384)	
training:	Epoch: [23][53/233]	Loss 0.1490 (0.1386)	
training:	Epoch: [23][54/233]	Loss 0.2071 (0.1399)	
training:	Epoch: [23][55/233]	Loss 0.0690 (0.1386)	
training:	Epoch: [23][56/233]	Loss 0.3555 (0.1425)	
training:	Epoch: [23][57/233]	Loss 0.0751 (0.1413)	
training:	Epoch: [23][58/233]	Loss 0.2002 (0.1423)	
training:	Epoch: [23][59/233]	Loss 0.1801 (0.1430)	
training:	Epoch: [23][60/233]	Loss 0.2597 (0.1449)	
training:	Epoch: [23][61/233]	Loss 0.0584 (0.1435)	
training:	Epoch: [23][62/233]	Loss 0.1170 (0.1431)	
training:	Epoch: [23][63/233]	Loss 0.3043 (0.1456)	
training:	Epoch: [23][64/233]	Loss 0.2916 (0.1479)	
training:	Epoch: [23][65/233]	Loss 0.0356 (0.1462)	
training:	Epoch: [23][66/233]	Loss 0.1647 (0.1464)	
training:	Epoch: [23][67/233]	Loss 0.0920 (0.1456)	
training:	Epoch: [23][68/233]	Loss 0.0879 (0.1448)	
training:	Epoch: [23][69/233]	Loss 0.0513 (0.1434)	
training:	Epoch: [23][70/233]	Loss 0.1618 (0.1437)	
training:	Epoch: [23][71/233]	Loss 0.3142 (0.1461)	
training:	Epoch: [23][72/233]	Loss 0.0980 (0.1454)	
training:	Epoch: [23][73/233]	Loss 0.1765 (0.1459)	
training:	Epoch: [23][74/233]	Loss 0.1921 (0.1465)	
training:	Epoch: [23][75/233]	Loss 0.0568 (0.1453)	
training:	Epoch: [23][76/233]	Loss 0.0414 (0.1439)	
training:	Epoch: [23][77/233]	Loss 0.1211 (0.1436)	
training:	Epoch: [23][78/233]	Loss 0.2712 (0.1453)	
training:	Epoch: [23][79/233]	Loss 0.1077 (0.1448)	
training:	Epoch: [23][80/233]	Loss 0.0554 (0.1437)	
training:	Epoch: [23][81/233]	Loss 0.0670 (0.1427)	
training:	Epoch: [23][82/233]	Loss 0.1096 (0.1423)	
training:	Epoch: [23][83/233]	Loss 0.1027 (0.1418)	
training:	Epoch: [23][84/233]	Loss 0.0431 (0.1407)	
training:	Epoch: [23][85/233]	Loss 0.1146 (0.1403)	
training:	Epoch: [23][86/233]	Loss 0.1660 (0.1406)	
training:	Epoch: [23][87/233]	Loss 0.1553 (0.1408)	
training:	Epoch: [23][88/233]	Loss 0.1278 (0.1407)	
training:	Epoch: [23][89/233]	Loss 0.1452 (0.1407)	
training:	Epoch: [23][90/233]	Loss 0.1078 (0.1404)	
training:	Epoch: [23][91/233]	Loss 0.1632 (0.1406)	
training:	Epoch: [23][92/233]	Loss 0.2442 (0.1417)	
training:	Epoch: [23][93/233]	Loss 0.0389 (0.1406)	
training:	Epoch: [23][94/233]	Loss 0.2993 (0.1423)	
training:	Epoch: [23][95/233]	Loss 0.0495 (0.1413)	
training:	Epoch: [23][96/233]	Loss 0.1822 (0.1418)	
training:	Epoch: [23][97/233]	Loss 0.1555 (0.1419)	
training:	Epoch: [23][98/233]	Loss 0.0614 (0.1411)	
training:	Epoch: [23][99/233]	Loss 0.0821 (0.1405)	
training:	Epoch: [23][100/233]	Loss 0.1452 (0.1405)	
training:	Epoch: [23][101/233]	Loss 0.1713 (0.1408)	
training:	Epoch: [23][102/233]	Loss 0.2309 (0.1417)	
training:	Epoch: [23][103/233]	Loss 0.2404 (0.1427)	
training:	Epoch: [23][104/233]	Loss 0.3622 (0.1448)	
training:	Epoch: [23][105/233]	Loss 0.3774 (0.1470)	
training:	Epoch: [23][106/233]	Loss 0.1256 (0.1468)	
training:	Epoch: [23][107/233]	Loss 0.2596 (0.1479)	
training:	Epoch: [23][108/233]	Loss 0.0594 (0.1470)	
training:	Epoch: [23][109/233]	Loss 0.1695 (0.1472)	
training:	Epoch: [23][110/233]	Loss 0.1751 (0.1475)	
training:	Epoch: [23][111/233]	Loss 0.1583 (0.1476)	
training:	Epoch: [23][112/233]	Loss 0.2030 (0.1481)	
training:	Epoch: [23][113/233]	Loss 0.0603 (0.1473)	
training:	Epoch: [23][114/233]	Loss 0.0612 (0.1466)	
training:	Epoch: [23][115/233]	Loss 0.0696 (0.1459)	
training:	Epoch: [23][116/233]	Loss 0.1950 (0.1463)	
training:	Epoch: [23][117/233]	Loss 0.0951 (0.1459)	
training:	Epoch: [23][118/233]	Loss 0.1525 (0.1459)	
training:	Epoch: [23][119/233]	Loss 0.0751 (0.1453)	
training:	Epoch: [23][120/233]	Loss 0.0873 (0.1449)	
training:	Epoch: [23][121/233]	Loss 0.0438 (0.1440)	
training:	Epoch: [23][122/233]	Loss 0.1924 (0.1444)	
training:	Epoch: [23][123/233]	Loss 0.1844 (0.1447)	
training:	Epoch: [23][124/233]	Loss 0.1460 (0.1447)	
training:	Epoch: [23][125/233]	Loss 0.0345 (0.1439)	
training:	Epoch: [23][126/233]	Loss 0.1901 (0.1442)	
training:	Epoch: [23][127/233]	Loss 0.0635 (0.1436)	
training:	Epoch: [23][128/233]	Loss 0.0876 (0.1432)	
training:	Epoch: [23][129/233]	Loss 0.0556 (0.1425)	
training:	Epoch: [23][130/233]	Loss 0.0621 (0.1419)	
training:	Epoch: [23][131/233]	Loss 0.2488 (0.1427)	
training:	Epoch: [23][132/233]	Loss 0.1183 (0.1425)	
training:	Epoch: [23][133/233]	Loss 0.0778 (0.1420)	
training:	Epoch: [23][134/233]	Loss 0.0450 (0.1413)	
training:	Epoch: [23][135/233]	Loss 0.0688 (0.1407)	
training:	Epoch: [23][136/233]	Loss 0.3555 (0.1423)	
training:	Epoch: [23][137/233]	Loss 0.1525 (0.1424)	
training:	Epoch: [23][138/233]	Loss 0.0869 (0.1420)	
training:	Epoch: [23][139/233]	Loss 0.2123 (0.1425)	
training:	Epoch: [23][140/233]	Loss 0.2244 (0.1431)	
training:	Epoch: [23][141/233]	Loss 0.2482 (0.1438)	
training:	Epoch: [23][142/233]	Loss 0.1596 (0.1439)	
training:	Epoch: [23][143/233]	Loss 0.0987 (0.1436)	
training:	Epoch: [23][144/233]	Loss 0.1989 (0.1440)	
training:	Epoch: [23][145/233]	Loss 0.1476 (0.1440)	
training:	Epoch: [23][146/233]	Loss 0.0566 (0.1434)	
training:	Epoch: [23][147/233]	Loss 0.1679 (0.1436)	
training:	Epoch: [23][148/233]	Loss 0.0229 (0.1428)	
training:	Epoch: [23][149/233]	Loss 0.1198 (0.1426)	
training:	Epoch: [23][150/233]	Loss 0.1089 (0.1424)	
training:	Epoch: [23][151/233]	Loss 0.1160 (0.1422)	
training:	Epoch: [23][152/233]	Loss 0.2545 (0.1430)	
training:	Epoch: [23][153/233]	Loss 0.1393 (0.1430)	
training:	Epoch: [23][154/233]	Loss 0.0498 (0.1423)	
training:	Epoch: [23][155/233]	Loss 0.1656 (0.1425)	
training:	Epoch: [23][156/233]	Loss 0.0727 (0.1420)	
training:	Epoch: [23][157/233]	Loss 0.2385 (0.1427)	
training:	Epoch: [23][158/233]	Loss 0.0818 (0.1423)	
training:	Epoch: [23][159/233]	Loss 0.1189 (0.1421)	
training:	Epoch: [23][160/233]	Loss 0.1121 (0.1419)	
training:	Epoch: [23][161/233]	Loss 0.2460 (0.1426)	
training:	Epoch: [23][162/233]	Loss 0.0850 (0.1422)	
training:	Epoch: [23][163/233]	Loss 0.1021 (0.1420)	
training:	Epoch: [23][164/233]	Loss 0.1373 (0.1420)	
training:	Epoch: [23][165/233]	Loss 0.0952 (0.1417)	
training:	Epoch: [23][166/233]	Loss 0.3143 (0.1427)	
training:	Epoch: [23][167/233]	Loss 0.0899 (0.1424)	
training:	Epoch: [23][168/233]	Loss 0.0868 (0.1421)	
training:	Epoch: [23][169/233]	Loss 0.0669 (0.1416)	
training:	Epoch: [23][170/233]	Loss 0.2127 (0.1420)	
training:	Epoch: [23][171/233]	Loss 0.0534 (0.1415)	
training:	Epoch: [23][172/233]	Loss 0.2072 (0.1419)	
training:	Epoch: [23][173/233]	Loss 0.2537 (0.1426)	
training:	Epoch: [23][174/233]	Loss 0.2085 (0.1429)	
training:	Epoch: [23][175/233]	Loss 0.1720 (0.1431)	
training:	Epoch: [23][176/233]	Loss 0.1067 (0.1429)	
training:	Epoch: [23][177/233]	Loss 0.1989 (0.1432)	
training:	Epoch: [23][178/233]	Loss 0.2169 (0.1436)	
training:	Epoch: [23][179/233]	Loss 0.2165 (0.1440)	
training:	Epoch: [23][180/233]	Loss 0.0863 (0.1437)	
training:	Epoch: [23][181/233]	Loss 0.0445 (0.1432)	
training:	Epoch: [23][182/233]	Loss 0.2406 (0.1437)	
training:	Epoch: [23][183/233]	Loss 0.1991 (0.1440)	
training:	Epoch: [23][184/233]	Loss 0.2057 (0.1443)	
training:	Epoch: [23][185/233]	Loss 0.0690 (0.1439)	
training:	Epoch: [23][186/233]	Loss 0.1942 (0.1442)	
training:	Epoch: [23][187/233]	Loss 0.0294 (0.1436)	
training:	Epoch: [23][188/233]	Loss 0.1292 (0.1435)	
training:	Epoch: [23][189/233]	Loss 0.0413 (0.1430)	
training:	Epoch: [23][190/233]	Loss 0.1419 (0.1430)	
training:	Epoch: [23][191/233]	Loss 0.0380 (0.1424)	
training:	Epoch: [23][192/233]	Loss 0.2277 (0.1429)	
training:	Epoch: [23][193/233]	Loss 0.2448 (0.1434)	
training:	Epoch: [23][194/233]	Loss 0.1910 (0.1436)	
training:	Epoch: [23][195/233]	Loss 0.1813 (0.1438)	
training:	Epoch: [23][196/233]	Loss 0.2437 (0.1443)	
training:	Epoch: [23][197/233]	Loss 0.0639 (0.1439)	
training:	Epoch: [23][198/233]	Loss 0.0754 (0.1436)	
training:	Epoch: [23][199/233]	Loss 0.0906 (0.1433)	
training:	Epoch: [23][200/233]	Loss 0.1034 (0.1431)	
training:	Epoch: [23][201/233]	Loss 0.0989 (0.1429)	
training:	Epoch: [23][202/233]	Loss 0.3669 (0.1440)	
training:	Epoch: [23][203/233]	Loss 0.1298 (0.1439)	
training:	Epoch: [23][204/233]	Loss 0.1110 (0.1438)	
training:	Epoch: [23][205/233]	Loss 0.1310 (0.1437)	
training:	Epoch: [23][206/233]	Loss 0.1830 (0.1439)	
training:	Epoch: [23][207/233]	Loss 0.1006 (0.1437)	
training:	Epoch: [23][208/233]	Loss 0.1271 (0.1436)	
training:	Epoch: [23][209/233]	Loss 0.1150 (0.1435)	
training:	Epoch: [23][210/233]	Loss 0.0946 (0.1432)	
training:	Epoch: [23][211/233]	Loss 0.1150 (0.1431)	
training:	Epoch: [23][212/233]	Loss 0.0427 (0.1426)	
training:	Epoch: [23][213/233]	Loss 0.1205 (0.1425)	
training:	Epoch: [23][214/233]	Loss 0.0634 (0.1422)	
training:	Epoch: [23][215/233]	Loss 0.0412 (0.1417)	
training:	Epoch: [23][216/233]	Loss 0.0642 (0.1413)	
training:	Epoch: [23][217/233]	Loss 0.0778 (0.1410)	
training:	Epoch: [23][218/233]	Loss 0.0695 (0.1407)	
training:	Epoch: [23][219/233]	Loss 0.4885 (0.1423)	
training:	Epoch: [23][220/233]	Loss 0.1718 (0.1424)	
training:	Epoch: [23][221/233]	Loss 0.1489 (0.1425)	
training:	Epoch: [23][222/233]	Loss 0.0701 (0.1421)	
training:	Epoch: [23][223/233]	Loss 0.0309 (0.1416)	
training:	Epoch: [23][224/233]	Loss 0.2150 (0.1420)	
training:	Epoch: [23][225/233]	Loss 0.1587 (0.1420)	
training:	Epoch: [23][226/233]	Loss 0.1403 (0.1420)	
training:	Epoch: [23][227/233]	Loss 0.0704 (0.1417)	
training:	Epoch: [23][228/233]	Loss 0.3209 (0.1425)	
training:	Epoch: [23][229/233]	Loss 0.2361 (0.1429)	
training:	Epoch: [23][230/233]	Loss 0.2243 (0.1433)	
training:	Epoch: [23][231/233]	Loss 0.1601 (0.1433)	
training:	Epoch: [23][232/233]	Loss 0.3093 (0.1440)	
training:	Epoch: [23][233/233]	Loss 0.1338 (0.1440)	
Training:	 Loss: 0.1437

Training:	 ACC: 0.9763 0.9758 0.9633 0.9894
Validation:	 ACC: 0.7946 0.7929 0.7600 0.8292
Validation:	 Best_BACC: 0.8162 0.8154 0.7998 0.8326
Validation:	 Loss: 0.5899
Pretraining:	Epoch 24/200
----------
training:	Epoch: [24][1/233]	Loss 0.0582 (0.0582)	
training:	Epoch: [24][2/233]	Loss 0.1598 (0.1090)	
training:	Epoch: [24][3/233]	Loss 0.1556 (0.1245)	
training:	Epoch: [24][4/233]	Loss 0.1453 (0.1297)	
training:	Epoch: [24][5/233]	Loss 0.1207 (0.1279)	
training:	Epoch: [24][6/233]	Loss 0.1162 (0.1260)	
training:	Epoch: [24][7/233]	Loss 0.1032 (0.1227)	
training:	Epoch: [24][8/233]	Loss 0.0430 (0.1127)	
training:	Epoch: [24][9/233]	Loss 0.1498 (0.1169)	
training:	Epoch: [24][10/233]	Loss 0.0544 (0.1106)	
training:	Epoch: [24][11/233]	Loss 0.1475 (0.1140)	
training:	Epoch: [24][12/233]	Loss 0.0461 (0.1083)	
training:	Epoch: [24][13/233]	Loss 0.0574 (0.1044)	
training:	Epoch: [24][14/233]	Loss 0.0753 (0.1023)	
training:	Epoch: [24][15/233]	Loss 0.1087 (0.1027)	
training:	Epoch: [24][16/233]	Loss 0.1847 (0.1079)	
training:	Epoch: [24][17/233]	Loss 0.0413 (0.1040)	
training:	Epoch: [24][18/233]	Loss 0.1440 (0.1062)	
training:	Epoch: [24][19/233]	Loss 0.0685 (0.1042)	
training:	Epoch: [24][20/233]	Loss 0.2089 (0.1094)	
training:	Epoch: [24][21/233]	Loss 0.0405 (0.1061)	
training:	Epoch: [24][22/233]	Loss 0.3033 (0.1151)	
training:	Epoch: [24][23/233]	Loss 0.0736 (0.1133)	
training:	Epoch: [24][24/233]	Loss 0.0483 (0.1106)	
training:	Epoch: [24][25/233]	Loss 0.0268 (0.1072)	
training:	Epoch: [24][26/233]	Loss 0.0891 (0.1065)	
training:	Epoch: [24][27/233]	Loss 0.0567 (0.1047)	
training:	Epoch: [24][28/233]	Loss 0.1061 (0.1047)	
training:	Epoch: [24][29/233]	Loss 0.1170 (0.1052)	
training:	Epoch: [24][30/233]	Loss 0.0476 (0.1033)	
training:	Epoch: [24][31/233]	Loss 0.0606 (0.1019)	
training:	Epoch: [24][32/233]	Loss 0.2233 (0.1057)	
training:	Epoch: [24][33/233]	Loss 0.0847 (0.1050)	
training:	Epoch: [24][34/233]	Loss 0.0748 (0.1041)	
training:	Epoch: [24][35/233]	Loss 0.0748 (0.1033)	
training:	Epoch: [24][36/233]	Loss 0.0569 (0.1020)	
training:	Epoch: [24][37/233]	Loss 0.0997 (0.1020)	
training:	Epoch: [24][38/233]	Loss 0.0669 (0.1010)	
training:	Epoch: [24][39/233]	Loss 0.0591 (0.1000)	
training:	Epoch: [24][40/233]	Loss 0.1645 (0.1016)	
training:	Epoch: [24][41/233]	Loss 0.0729 (0.1009)	
training:	Epoch: [24][42/233]	Loss 0.0955 (0.1007)	
training:	Epoch: [24][43/233]	Loss 0.0680 (0.1000)	
training:	Epoch: [24][44/233]	Loss 0.1393 (0.1009)	
training:	Epoch: [24][45/233]	Loss 0.0546 (0.0998)	
training:	Epoch: [24][46/233]	Loss 0.0506 (0.0988)	
training:	Epoch: [24][47/233]	Loss 0.2539 (0.1021)	
training:	Epoch: [24][48/233]	Loss 0.0443 (0.1009)	
training:	Epoch: [24][49/233]	Loss 0.0370 (0.0996)	
training:	Epoch: [24][50/233]	Loss 0.0349 (0.0983)	
training:	Epoch: [24][51/233]	Loss 0.1414 (0.0991)	
training:	Epoch: [24][52/233]	Loss 0.1866 (0.1008)	
training:	Epoch: [24][53/233]	Loss 0.1455 (0.1016)	
training:	Epoch: [24][54/233]	Loss 0.1099 (0.1018)	
training:	Epoch: [24][55/233]	Loss 0.0500 (0.1009)	
training:	Epoch: [24][56/233]	Loss 0.2091 (0.1028)	
training:	Epoch: [24][57/233]	Loss 0.0521 (0.1019)	
training:	Epoch: [24][58/233]	Loss 0.2294 (0.1041)	
training:	Epoch: [24][59/233]	Loss 0.0253 (0.1028)	
training:	Epoch: [24][60/233]	Loss 0.2279 (0.1049)	
training:	Epoch: [24][61/233]	Loss 0.0370 (0.1037)	
training:	Epoch: [24][62/233]	Loss 0.3442 (0.1076)	
training:	Epoch: [24][63/233]	Loss 0.0691 (0.1070)	
training:	Epoch: [24][64/233]	Loss 0.1284 (0.1073)	
training:	Epoch: [24][65/233]	Loss 0.0676 (0.1067)	
training:	Epoch: [24][66/233]	Loss 0.0889 (0.1065)	
training:	Epoch: [24][67/233]	Loss 0.1591 (0.1072)	
training:	Epoch: [24][68/233]	Loss 0.0514 (0.1064)	
training:	Epoch: [24][69/233]	Loss 0.1540 (0.1071)	
training:	Epoch: [24][70/233]	Loss 0.0753 (0.1067)	
training:	Epoch: [24][71/233]	Loss 0.4027 (0.1108)	
training:	Epoch: [24][72/233]	Loss 0.0843 (0.1105)	
training:	Epoch: [24][73/233]	Loss 0.3242 (0.1134)	
training:	Epoch: [24][74/233]	Loss 0.0596 (0.1127)	
training:	Epoch: [24][75/233]	Loss 0.0313 (0.1116)	
training:	Epoch: [24][76/233]	Loss 0.3368 (0.1145)	
training:	Epoch: [24][77/233]	Loss 0.1609 (0.1151)	
training:	Epoch: [24][78/233]	Loss 0.2134 (0.1164)	
training:	Epoch: [24][79/233]	Loss 0.0313 (0.1153)	
training:	Epoch: [24][80/233]	Loss 0.0516 (0.1145)	
training:	Epoch: [24][81/233]	Loss 0.2043 (0.1156)	
training:	Epoch: [24][82/233]	Loss 0.2371 (0.1171)	
training:	Epoch: [24][83/233]	Loss 0.0298 (0.1161)	
training:	Epoch: [24][84/233]	Loss 0.0601 (0.1154)	
training:	Epoch: [24][85/233]	Loss 0.0774 (0.1150)	
training:	Epoch: [24][86/233]	Loss 0.0660 (0.1144)	
training:	Epoch: [24][87/233]	Loss 0.1686 (0.1150)	
training:	Epoch: [24][88/233]	Loss 0.0740 (0.1145)	
training:	Epoch: [24][89/233]	Loss 0.2196 (0.1157)	
training:	Epoch: [24][90/233]	Loss 0.0487 (0.1150)	
training:	Epoch: [24][91/233]	Loss 0.1713 (0.1156)	
training:	Epoch: [24][92/233]	Loss 0.2156 (0.1167)	
training:	Epoch: [24][93/233]	Loss 0.2578 (0.1182)	
training:	Epoch: [24][94/233]	Loss 0.1962 (0.1190)	
training:	Epoch: [24][95/233]	Loss 0.2001 (0.1199)	
training:	Epoch: [24][96/233]	Loss 0.1009 (0.1197)	
training:	Epoch: [24][97/233]	Loss 0.1360 (0.1199)	
training:	Epoch: [24][98/233]	Loss 0.1114 (0.1198)	
training:	Epoch: [24][99/233]	Loss 0.2835 (0.1214)	
training:	Epoch: [24][100/233]	Loss 0.0993 (0.1212)	
training:	Epoch: [24][101/233]	Loss 0.0460 (0.1205)	
training:	Epoch: [24][102/233]	Loss 0.0230 (0.1195)	
training:	Epoch: [24][103/233]	Loss 0.0648 (0.1190)	
training:	Epoch: [24][104/233]	Loss 0.1647 (0.1194)	
training:	Epoch: [24][105/233]	Loss 0.2102 (0.1203)	
training:	Epoch: [24][106/233]	Loss 0.2871 (0.1218)	
training:	Epoch: [24][107/233]	Loss 0.0928 (0.1216)	
training:	Epoch: [24][108/233]	Loss 0.0449 (0.1209)	
training:	Epoch: [24][109/233]	Loss 0.1913 (0.1215)	
training:	Epoch: [24][110/233]	Loss 0.0802 (0.1211)	
training:	Epoch: [24][111/233]	Loss 0.0676 (0.1207)	
training:	Epoch: [24][112/233]	Loss 0.1749 (0.1211)	
training:	Epoch: [24][113/233]	Loss 0.0626 (0.1206)	
training:	Epoch: [24][114/233]	Loss 0.1412 (0.1208)	
training:	Epoch: [24][115/233]	Loss 0.0768 (0.1204)	
training:	Epoch: [24][116/233]	Loss 0.1075 (0.1203)	
training:	Epoch: [24][117/233]	Loss 0.0888 (0.1200)	
training:	Epoch: [24][118/233]	Loss 0.0955 (0.1198)	
training:	Epoch: [24][119/233]	Loss 0.0929 (0.1196)	
training:	Epoch: [24][120/233]	Loss 0.0678 (0.1192)	
training:	Epoch: [24][121/233]	Loss 0.0823 (0.1189)	
training:	Epoch: [24][122/233]	Loss 0.1653 (0.1192)	
training:	Epoch: [24][123/233]	Loss 0.0650 (0.1188)	
training:	Epoch: [24][124/233]	Loss 0.0739 (0.1184)	
training:	Epoch: [24][125/233]	Loss 0.0337 (0.1178)	
training:	Epoch: [24][126/233]	Loss 0.0253 (0.1170)	
training:	Epoch: [24][127/233]	Loss 0.4285 (0.1195)	
training:	Epoch: [24][128/233]	Loss 0.0305 (0.1188)	
training:	Epoch: [24][129/233]	Loss 0.1367 (0.1189)	
training:	Epoch: [24][130/233]	Loss 0.1041 (0.1188)	
training:	Epoch: [24][131/233]	Loss 0.0486 (0.1183)	
training:	Epoch: [24][132/233]	Loss 0.2403 (0.1192)	
training:	Epoch: [24][133/233]	Loss 0.1459 (0.1194)	
training:	Epoch: [24][134/233]	Loss 0.2541 (0.1204)	
training:	Epoch: [24][135/233]	Loss 0.0824 (0.1201)	
training:	Epoch: [24][136/233]	Loss 0.0690 (0.1198)	
training:	Epoch: [24][137/233]	Loss 0.0497 (0.1192)	
training:	Epoch: [24][138/233]	Loss 0.0704 (0.1189)	
training:	Epoch: [24][139/233]	Loss 0.1279 (0.1190)	
training:	Epoch: [24][140/233]	Loss 0.1382 (0.1191)	
training:	Epoch: [24][141/233]	Loss 0.1183 (0.1191)	
training:	Epoch: [24][142/233]	Loss 0.0524 (0.1186)	
training:	Epoch: [24][143/233]	Loss 0.0579 (0.1182)	
training:	Epoch: [24][144/233]	Loss 0.0868 (0.1180)	
training:	Epoch: [24][145/233]	Loss 0.0354 (0.1174)	
training:	Epoch: [24][146/233]	Loss 0.0376 (0.1169)	
training:	Epoch: [24][147/233]	Loss 0.2569 (0.1178)	
training:	Epoch: [24][148/233]	Loss 0.0375 (0.1173)	
training:	Epoch: [24][149/233]	Loss 0.0540 (0.1168)	
training:	Epoch: [24][150/233]	Loss 0.1459 (0.1170)	
training:	Epoch: [24][151/233]	Loss 0.0439 (0.1165)	
training:	Epoch: [24][152/233]	Loss 0.2452 (0.1174)	
training:	Epoch: [24][153/233]	Loss 0.0563 (0.1170)	
training:	Epoch: [24][154/233]	Loss 0.0489 (0.1166)	
training:	Epoch: [24][155/233]	Loss 0.0632 (0.1162)	
training:	Epoch: [24][156/233]	Loss 0.0722 (0.1159)	
training:	Epoch: [24][157/233]	Loss 0.0564 (0.1155)	
training:	Epoch: [24][158/233]	Loss 0.0978 (0.1154)	
training:	Epoch: [24][159/233]	Loss 0.1261 (0.1155)	
training:	Epoch: [24][160/233]	Loss 0.1963 (0.1160)	
training:	Epoch: [24][161/233]	Loss 0.0620 (0.1157)	
training:	Epoch: [24][162/233]	Loss 0.2287 (0.1164)	
training:	Epoch: [24][163/233]	Loss 0.2723 (0.1173)	
training:	Epoch: [24][164/233]	Loss 0.1117 (0.1173)	
training:	Epoch: [24][165/233]	Loss 0.1925 (0.1178)	
training:	Epoch: [24][166/233]	Loss 0.1948 (0.1182)	
training:	Epoch: [24][167/233]	Loss 0.1508 (0.1184)	
training:	Epoch: [24][168/233]	Loss 0.0325 (0.1179)	
training:	Epoch: [24][169/233]	Loss 0.0645 (0.1176)	
training:	Epoch: [24][170/233]	Loss 0.5043 (0.1199)	
training:	Epoch: [24][171/233]	Loss 0.1890 (0.1203)	
training:	Epoch: [24][172/233]	Loss 0.1919 (0.1207)	
training:	Epoch: [24][173/233]	Loss 0.2545 (0.1215)	
training:	Epoch: [24][174/233]	Loss 0.0395 (0.1210)	
training:	Epoch: [24][175/233]	Loss 0.1665 (0.1212)	
training:	Epoch: [24][176/233]	Loss 0.1489 (0.1214)	
training:	Epoch: [24][177/233]	Loss 0.0443 (0.1210)	
training:	Epoch: [24][178/233]	Loss 0.2047 (0.1214)	
training:	Epoch: [24][179/233]	Loss 0.0435 (0.1210)	
training:	Epoch: [24][180/233]	Loss 0.0996 (0.1209)	
training:	Epoch: [24][181/233]	Loss 0.1903 (0.1213)	
training:	Epoch: [24][182/233]	Loss 0.1790 (0.1216)	
training:	Epoch: [24][183/233]	Loss 0.0857 (0.1214)	
training:	Epoch: [24][184/233]	Loss 0.0874 (0.1212)	
training:	Epoch: [24][185/233]	Loss 0.0308 (0.1207)	
training:	Epoch: [24][186/233]	Loss 0.2629 (0.1215)	
training:	Epoch: [24][187/233]	Loss 0.0582 (0.1211)	
training:	Epoch: [24][188/233]	Loss 0.0756 (0.1209)	
training:	Epoch: [24][189/233]	Loss 0.1191 (0.1209)	
training:	Epoch: [24][190/233]	Loss 0.0745 (0.1206)	
training:	Epoch: [24][191/233]	Loss 0.1597 (0.1208)	
training:	Epoch: [24][192/233]	Loss 0.1212 (0.1208)	
training:	Epoch: [24][193/233]	Loss 0.1293 (0.1209)	
training:	Epoch: [24][194/233]	Loss 0.1443 (0.1210)	
training:	Epoch: [24][195/233]	Loss 0.0754 (0.1208)	
training:	Epoch: [24][196/233]	Loss 0.0501 (0.1204)	
training:	Epoch: [24][197/233]	Loss 0.1504 (0.1206)	
training:	Epoch: [24][198/233]	Loss 0.0917 (0.1204)	
training:	Epoch: [24][199/233]	Loss 0.2031 (0.1208)	
training:	Epoch: [24][200/233]	Loss 0.1760 (0.1211)	
training:	Epoch: [24][201/233]	Loss 0.0835 (0.1209)	
training:	Epoch: [24][202/233]	Loss 0.1939 (0.1213)	
training:	Epoch: [24][203/233]	Loss 0.2958 (0.1221)	
training:	Epoch: [24][204/233]	Loss 0.0825 (0.1220)	
training:	Epoch: [24][205/233]	Loss 0.0523 (0.1216)	
training:	Epoch: [24][206/233]	Loss 0.0330 (0.1212)	
training:	Epoch: [24][207/233]	Loss 0.2703 (0.1219)	
training:	Epoch: [24][208/233]	Loss 0.0442 (0.1215)	
training:	Epoch: [24][209/233]	Loss 0.1909 (0.1219)	
training:	Epoch: [24][210/233]	Loss 0.1990 (0.1222)	
training:	Epoch: [24][211/233]	Loss 0.0365 (0.1218)	
training:	Epoch: [24][212/233]	Loss 0.1362 (0.1219)	
training:	Epoch: [24][213/233]	Loss 0.0687 (0.1216)	
training:	Epoch: [24][214/233]	Loss 0.1630 (0.1218)	
training:	Epoch: [24][215/233]	Loss 0.0958 (0.1217)	
training:	Epoch: [24][216/233]	Loss 0.1646 (0.1219)	
training:	Epoch: [24][217/233]	Loss 0.3187 (0.1228)	
training:	Epoch: [24][218/233]	Loss 0.1965 (0.1232)	
training:	Epoch: [24][219/233]	Loss 0.0558 (0.1228)	
training:	Epoch: [24][220/233]	Loss 0.0314 (0.1224)	
training:	Epoch: [24][221/233]	Loss 0.0647 (0.1222)	
training:	Epoch: [24][222/233]	Loss 0.2050 (0.1225)	
training:	Epoch: [24][223/233]	Loss 0.1642 (0.1227)	
training:	Epoch: [24][224/233]	Loss 0.1167 (0.1227)	
training:	Epoch: [24][225/233]	Loss 0.0354 (0.1223)	
training:	Epoch: [24][226/233]	Loss 0.1584 (0.1225)	
training:	Epoch: [24][227/233]	Loss 0.1367 (0.1225)	
training:	Epoch: [24][228/233]	Loss 0.2388 (0.1230)	
training:	Epoch: [24][229/233]	Loss 0.1126 (0.1230)	
training:	Epoch: [24][230/233]	Loss 0.1486 (0.1231)	
training:	Epoch: [24][231/233]	Loss 0.0464 (0.1228)	
training:	Epoch: [24][232/233]	Loss 0.0353 (0.1224)	
training:	Epoch: [24][233/233]	Loss 0.0683 (0.1222)	
Training:	 Loss: 0.1219

Training:	 ACC: 0.9799 0.9799 0.9808 0.9790
Validation:	 ACC: 0.7960 0.7972 0.8223 0.7697
Validation:	 Best_BACC: 0.8162 0.8154 0.7998 0.8326
Validation:	 Loss: 0.6216
Pretraining:	Epoch 25/200
----------
training:	Epoch: [25][1/233]	Loss 0.1647 (0.1647)	
training:	Epoch: [25][2/233]	Loss 0.1584 (0.1616)	
training:	Epoch: [25][3/233]	Loss 0.0706 (0.1312)	
training:	Epoch: [25][4/233]	Loss 0.1556 (0.1373)	
training:	Epoch: [25][5/233]	Loss 0.0610 (0.1221)	
training:	Epoch: [25][6/233]	Loss 0.2476 (0.1430)	
training:	Epoch: [25][7/233]	Loss 0.1759 (0.1477)	
training:	Epoch: [25][8/233]	Loss 0.0206 (0.1318)	
training:	Epoch: [25][9/233]	Loss 0.0581 (0.1236)	
training:	Epoch: [25][10/233]	Loss 0.1011 (0.1214)	
training:	Epoch: [25][11/233]	Loss 0.1106 (0.1204)	
training:	Epoch: [25][12/233]	Loss 0.1401 (0.1220)	
training:	Epoch: [25][13/233]	Loss 0.1245 (0.1222)	
training:	Epoch: [25][14/233]	Loss 0.1631 (0.1251)	
training:	Epoch: [25][15/233]	Loss 0.0759 (0.1218)	
training:	Epoch: [25][16/233]	Loss 0.1888 (0.1260)	
training:	Epoch: [25][17/233]	Loss 0.1560 (0.1278)	
training:	Epoch: [25][18/233]	Loss 0.1046 (0.1265)	
training:	Epoch: [25][19/233]	Loss 0.1550 (0.1280)	
training:	Epoch: [25][20/233]	Loss 0.2180 (0.1325)	
training:	Epoch: [25][21/233]	Loss 0.0594 (0.1290)	
training:	Epoch: [25][22/233]	Loss 0.1790 (0.1313)	
training:	Epoch: [25][23/233]	Loss 0.1694 (0.1330)	
training:	Epoch: [25][24/233]	Loss 0.0438 (0.1292)	
training:	Epoch: [25][25/233]	Loss 0.0385 (0.1256)	
training:	Epoch: [25][26/233]	Loss 0.1095 (0.1250)	
training:	Epoch: [25][27/233]	Loss 0.0350 (0.1217)	
training:	Epoch: [25][28/233]	Loss 0.2768 (0.1272)	
training:	Epoch: [25][29/233]	Loss 0.2235 (0.1305)	
training:	Epoch: [25][30/233]	Loss 0.1265 (0.1304)	
training:	Epoch: [25][31/233]	Loss 0.0615 (0.1282)	
training:	Epoch: [25][32/233]	Loss 0.1036 (0.1274)	
training:	Epoch: [25][33/233]	Loss 0.1492 (0.1281)	
training:	Epoch: [25][34/233]	Loss 0.0535 (0.1259)	
training:	Epoch: [25][35/233]	Loss 0.2801 (0.1303)	
training:	Epoch: [25][36/233]	Loss 0.2810 (0.1345)	
training:	Epoch: [25][37/233]	Loss 0.2194 (0.1368)	
training:	Epoch: [25][38/233]	Loss 0.0856 (0.1354)	
training:	Epoch: [25][39/233]	Loss 0.1456 (0.1357)	
training:	Epoch: [25][40/233]	Loss 0.0477 (0.1335)	
training:	Epoch: [25][41/233]	Loss 0.0359 (0.1311)	
training:	Epoch: [25][42/233]	Loss 0.0567 (0.1293)	
training:	Epoch: [25][43/233]	Loss 0.0724 (0.1280)	
training:	Epoch: [25][44/233]	Loss 0.0492 (0.1262)	
training:	Epoch: [25][45/233]	Loss 0.1556 (0.1269)	
training:	Epoch: [25][46/233]	Loss 0.0313 (0.1248)	
training:	Epoch: [25][47/233]	Loss 0.2475 (0.1274)	
training:	Epoch: [25][48/233]	Loss 0.0303 (0.1254)	
training:	Epoch: [25][49/233]	Loss 0.0712 (0.1243)	
training:	Epoch: [25][50/233]	Loss 0.2227 (0.1262)	
training:	Epoch: [25][51/233]	Loss 0.1318 (0.1263)	
training:	Epoch: [25][52/233]	Loss 0.1015 (0.1259)	
training:	Epoch: [25][53/233]	Loss 0.2070 (0.1274)	
training:	Epoch: [25][54/233]	Loss 0.0494 (0.1259)	
training:	Epoch: [25][55/233]	Loss 0.1082 (0.1256)	
training:	Epoch: [25][56/233]	Loss 0.0353 (0.1240)	
training:	Epoch: [25][57/233]	Loss 0.0801 (0.1232)	
training:	Epoch: [25][58/233]	Loss 0.0579 (0.1221)	
training:	Epoch: [25][59/233]	Loss 0.0347 (0.1206)	
training:	Epoch: [25][60/233]	Loss 0.0333 (0.1192)	
training:	Epoch: [25][61/233]	Loss 0.2906 (0.1220)	
training:	Epoch: [25][62/233]	Loss 0.2190 (0.1236)	
training:	Epoch: [25][63/233]	Loss 0.2460 (0.1255)	
training:	Epoch: [25][64/233]	Loss 0.3590 (0.1291)	
training:	Epoch: [25][65/233]	Loss 0.0595 (0.1281)	
training:	Epoch: [25][66/233]	Loss 0.0255 (0.1265)	
training:	Epoch: [25][67/233]	Loss 0.1832 (0.1274)	
training:	Epoch: [25][68/233]	Loss 0.0358 (0.1260)	
training:	Epoch: [25][69/233]	Loss 0.0585 (0.1250)	
training:	Epoch: [25][70/233]	Loss 0.1749 (0.1258)	
training:	Epoch: [25][71/233]	Loss 0.1567 (0.1262)	
training:	Epoch: [25][72/233]	Loss 0.3886 (0.1298)	
training:	Epoch: [25][73/233]	Loss 0.1012 (0.1294)	
training:	Epoch: [25][74/233]	Loss 0.1025 (0.1291)	
training:	Epoch: [25][75/233]	Loss 0.0339 (0.1278)	
training:	Epoch: [25][76/233]	Loss 0.0326 (0.1266)	
training:	Epoch: [25][77/233]	Loss 0.0703 (0.1258)	
training:	Epoch: [25][78/233]	Loss 0.1666 (0.1263)	
training:	Epoch: [25][79/233]	Loss 0.0482 (0.1254)	
training:	Epoch: [25][80/233]	Loss 0.0242 (0.1241)	
training:	Epoch: [25][81/233]	Loss 0.0554 (0.1232)	
training:	Epoch: [25][82/233]	Loss 0.0333 (0.1222)	
training:	Epoch: [25][83/233]	Loss 0.1812 (0.1229)	
training:	Epoch: [25][84/233]	Loss 0.1910 (0.1237)	
training:	Epoch: [25][85/233]	Loss 0.0236 (0.1225)	
training:	Epoch: [25][86/233]	Loss 0.1889 (0.1233)	
training:	Epoch: [25][87/233]	Loss 0.0667 (0.1226)	
training:	Epoch: [25][88/233]	Loss 0.1252 (0.1226)	
training:	Epoch: [25][89/233]	Loss 0.0217 (0.1215)	
training:	Epoch: [25][90/233]	Loss 0.0247 (0.1204)	
training:	Epoch: [25][91/233]	Loss 0.0313 (0.1195)	
training:	Epoch: [25][92/233]	Loss 0.0424 (0.1186)	
training:	Epoch: [25][93/233]	Loss 0.0576 (0.1180)	
training:	Epoch: [25][94/233]	Loss 0.1800 (0.1186)	
training:	Epoch: [25][95/233]	Loss 0.1196 (0.1186)	
training:	Epoch: [25][96/233]	Loss 0.0347 (0.1178)	
training:	Epoch: [25][97/233]	Loss 0.0308 (0.1169)	
training:	Epoch: [25][98/233]	Loss 0.0888 (0.1166)	
training:	Epoch: [25][99/233]	Loss 0.0779 (0.1162)	
training:	Epoch: [25][100/233]	Loss 0.0462 (0.1155)	
training:	Epoch: [25][101/233]	Loss 0.0324 (0.1147)	
training:	Epoch: [25][102/233]	Loss 0.0857 (0.1144)	
training:	Epoch: [25][103/233]	Loss 0.0299 (0.1136)	
training:	Epoch: [25][104/233]	Loss 0.0882 (0.1133)	
training:	Epoch: [25][105/233]	Loss 0.0430 (0.1126)	
training:	Epoch: [25][106/233]	Loss 0.0397 (0.1120)	
training:	Epoch: [25][107/233]	Loss 0.1637 (0.1124)	
training:	Epoch: [25][108/233]	Loss 0.0328 (0.1117)	
training:	Epoch: [25][109/233]	Loss 0.1201 (0.1118)	
training:	Epoch: [25][110/233]	Loss 0.0409 (0.1111)	
training:	Epoch: [25][111/233]	Loss 0.1364 (0.1114)	
training:	Epoch: [25][112/233]	Loss 0.0307 (0.1106)	
training:	Epoch: [25][113/233]	Loss 0.0389 (0.1100)	
training:	Epoch: [25][114/233]	Loss 0.0462 (0.1095)	
training:	Epoch: [25][115/233]	Loss 0.0650 (0.1091)	
training:	Epoch: [25][116/233]	Loss 0.0629 (0.1087)	
training:	Epoch: [25][117/233]	Loss 0.0816 (0.1084)	
training:	Epoch: [25][118/233]	Loss 0.0390 (0.1078)	
training:	Epoch: [25][119/233]	Loss 0.1689 (0.1084)	
training:	Epoch: [25][120/233]	Loss 0.2252 (0.1093)	
training:	Epoch: [25][121/233]	Loss 0.1808 (0.1099)	
training:	Epoch: [25][122/233]	Loss 0.1550 (0.1103)	
training:	Epoch: [25][123/233]	Loss 0.1081 (0.1103)	
training:	Epoch: [25][124/233]	Loss 0.0826 (0.1101)	
training:	Epoch: [25][125/233]	Loss 0.1864 (0.1107)	
training:	Epoch: [25][126/233]	Loss 0.0447 (0.1101)	
training:	Epoch: [25][127/233]	Loss 0.1836 (0.1107)	
training:	Epoch: [25][128/233]	Loss 0.0455 (0.1102)	
training:	Epoch: [25][129/233]	Loss 0.0579 (0.1098)	
training:	Epoch: [25][130/233]	Loss 0.1846 (0.1104)	
training:	Epoch: [25][131/233]	Loss 0.0326 (0.1098)	
training:	Epoch: [25][132/233]	Loss 0.0569 (0.1094)	
training:	Epoch: [25][133/233]	Loss 0.0383 (0.1089)	
training:	Epoch: [25][134/233]	Loss 0.1577 (0.1092)	
training:	Epoch: [25][135/233]	Loss 0.0502 (0.1088)	
training:	Epoch: [25][136/233]	Loss 0.1495 (0.1091)	
training:	Epoch: [25][137/233]	Loss 0.1940 (0.1097)	
training:	Epoch: [25][138/233]	Loss 0.0886 (0.1095)	
training:	Epoch: [25][139/233]	Loss 0.1259 (0.1097)	
training:	Epoch: [25][140/233]	Loss 0.1452 (0.1099)	
training:	Epoch: [25][141/233]	Loss 0.0498 (0.1095)	
training:	Epoch: [25][142/233]	Loss 0.0928 (0.1094)	
training:	Epoch: [25][143/233]	Loss 0.2517 (0.1104)	
training:	Epoch: [25][144/233]	Loss 0.1177 (0.1104)	
training:	Epoch: [25][145/233]	Loss 0.1344 (0.1106)	
training:	Epoch: [25][146/233]	Loss 0.1661 (0.1110)	
training:	Epoch: [25][147/233]	Loss 0.2849 (0.1121)	
training:	Epoch: [25][148/233]	Loss 0.1935 (0.1127)	
training:	Epoch: [25][149/233]	Loss 0.0606 (0.1123)	
training:	Epoch: [25][150/233]	Loss 0.0516 (0.1119)	
training:	Epoch: [25][151/233]	Loss 0.1812 (0.1124)	
training:	Epoch: [25][152/233]	Loss 0.0802 (0.1122)	
training:	Epoch: [25][153/233]	Loss 0.0567 (0.1118)	
training:	Epoch: [25][154/233]	Loss 0.0377 (0.1113)	
training:	Epoch: [25][155/233]	Loss 0.1056 (0.1113)	
training:	Epoch: [25][156/233]	Loss 0.3092 (0.1126)	
training:	Epoch: [25][157/233]	Loss 0.0408 (0.1121)	
training:	Epoch: [25][158/233]	Loss 0.2074 (0.1127)	
training:	Epoch: [25][159/233]	Loss 0.0420 (0.1123)	
training:	Epoch: [25][160/233]	Loss 0.0965 (0.1122)	
training:	Epoch: [25][161/233]	Loss 0.0918 (0.1121)	
training:	Epoch: [25][162/233]	Loss 0.0290 (0.1115)	
training:	Epoch: [25][163/233]	Loss 0.2087 (0.1121)	
training:	Epoch: [25][164/233]	Loss 0.0668 (0.1119)	
training:	Epoch: [25][165/233]	Loss 0.3666 (0.1134)	
training:	Epoch: [25][166/233]	Loss 0.0893 (0.1133)	
training:	Epoch: [25][167/233]	Loss 0.1095 (0.1132)	
training:	Epoch: [25][168/233]	Loss 0.0513 (0.1129)	
training:	Epoch: [25][169/233]	Loss 0.2437 (0.1136)	
training:	Epoch: [25][170/233]	Loss 0.2216 (0.1143)	
training:	Epoch: [25][171/233]	Loss 0.0316 (0.1138)	
training:	Epoch: [25][172/233]	Loss 0.2444 (0.1146)	
training:	Epoch: [25][173/233]	Loss 0.2694 (0.1154)	
training:	Epoch: [25][174/233]	Loss 0.0313 (0.1150)	
training:	Epoch: [25][175/233]	Loss 0.1351 (0.1151)	
training:	Epoch: [25][176/233]	Loss 0.1068 (0.1150)	
training:	Epoch: [25][177/233]	Loss 0.0503 (0.1147)	
training:	Epoch: [25][178/233]	Loss 0.0704 (0.1144)	
training:	Epoch: [25][179/233]	Loss 0.1629 (0.1147)	
training:	Epoch: [25][180/233]	Loss 0.0485 (0.1143)	
training:	Epoch: [25][181/233]	Loss 0.0997 (0.1142)	
training:	Epoch: [25][182/233]	Loss 0.1317 (0.1143)	
training:	Epoch: [25][183/233]	Loss 0.0232 (0.1138)	
training:	Epoch: [25][184/233]	Loss 0.0549 (0.1135)	
training:	Epoch: [25][185/233]	Loss 0.0465 (0.1132)	
training:	Epoch: [25][186/233]	Loss 0.0932 (0.1130)	
training:	Epoch: [25][187/233]	Loss 0.0857 (0.1129)	
training:	Epoch: [25][188/233]	Loss 0.0975 (0.1128)	
training:	Epoch: [25][189/233]	Loss 0.1684 (0.1131)	
training:	Epoch: [25][190/233]	Loss 0.0433 (0.1127)	
training:	Epoch: [25][191/233]	Loss 0.2598 (0.1135)	
training:	Epoch: [25][192/233]	Loss 0.0737 (0.1133)	
training:	Epoch: [25][193/233]	Loss 0.2136 (0.1138)	
training:	Epoch: [25][194/233]	Loss 0.1901 (0.1142)	
training:	Epoch: [25][195/233]	Loss 0.0474 (0.1139)	
training:	Epoch: [25][196/233]	Loss 0.1029 (0.1138)	
training:	Epoch: [25][197/233]	Loss 0.1563 (0.1140)	
training:	Epoch: [25][198/233]	Loss 0.0548 (0.1137)	
training:	Epoch: [25][199/233]	Loss 0.1324 (0.1138)	
training:	Epoch: [25][200/233]	Loss 0.0837 (0.1137)	
training:	Epoch: [25][201/233]	Loss 0.0566 (0.1134)	
training:	Epoch: [25][202/233]	Loss 0.0860 (0.1133)	
training:	Epoch: [25][203/233]	Loss 0.0312 (0.1129)	
training:	Epoch: [25][204/233]	Loss 0.1125 (0.1129)	
training:	Epoch: [25][205/233]	Loss 0.2939 (0.1137)	
training:	Epoch: [25][206/233]	Loss 0.1920 (0.1141)	
training:	Epoch: [25][207/233]	Loss 0.1184 (0.1141)	
training:	Epoch: [25][208/233]	Loss 0.2919 (0.1150)	
training:	Epoch: [25][209/233]	Loss 0.1881 (0.1153)	
training:	Epoch: [25][210/233]	Loss 0.1669 (0.1156)	
training:	Epoch: [25][211/233]	Loss 0.0485 (0.1153)	
training:	Epoch: [25][212/233]	Loss 0.0495 (0.1150)	
training:	Epoch: [25][213/233]	Loss 0.1785 (0.1153)	
training:	Epoch: [25][214/233]	Loss 0.0697 (0.1150)	
training:	Epoch: [25][215/233]	Loss 0.3447 (0.1161)	
training:	Epoch: [25][216/233]	Loss 0.0486 (0.1158)	
training:	Epoch: [25][217/233]	Loss 0.2889 (0.1166)	
training:	Epoch: [25][218/233]	Loss 0.0365 (0.1162)	
training:	Epoch: [25][219/233]	Loss 0.0246 (0.1158)	
training:	Epoch: [25][220/233]	Loss 0.0778 (0.1156)	
training:	Epoch: [25][221/233]	Loss 0.2939 (0.1165)	
training:	Epoch: [25][222/233]	Loss 0.0293 (0.1161)	
training:	Epoch: [25][223/233]	Loss 0.3315 (0.1170)	
training:	Epoch: [25][224/233]	Loss 0.0833 (0.1169)	
training:	Epoch: [25][225/233]	Loss 0.0466 (0.1166)	
training:	Epoch: [25][226/233]	Loss 0.1212 (0.1166)	
training:	Epoch: [25][227/233]	Loss 0.0319 (0.1162)	
training:	Epoch: [25][228/233]	Loss 0.1051 (0.1162)	
training:	Epoch: [25][229/233]	Loss 0.0487 (0.1159)	
training:	Epoch: [25][230/233]	Loss 0.0746 (0.1157)	
training:	Epoch: [25][231/233]	Loss 0.0492 (0.1154)	
training:	Epoch: [25][232/233]	Loss 0.2242 (0.1159)	
training:	Epoch: [25][233/233]	Loss 0.1402 (0.1160)	
Training:	 Loss: 0.1157

Training:	 ACC: 0.9831 0.9827 0.9746 0.9916
Validation:	 ACC: 0.7993 0.7983 0.7783 0.8202
Validation:	 Best_BACC: 0.8162 0.8154 0.7998 0.8326
Validation:	 Loss: 0.6289
Pretraining:	Epoch 26/200
----------
training:	Epoch: [26][1/233]	Loss 0.1994 (0.1994)	
training:	Epoch: [26][2/233]	Loss 0.0647 (0.1320)	
training:	Epoch: [26][3/233]	Loss 0.0932 (0.1191)	
training:	Epoch: [26][4/233]	Loss 0.0501 (0.1019)	
training:	Epoch: [26][5/233]	Loss 0.0514 (0.0918)	
training:	Epoch: [26][6/233]	Loss 0.2619 (0.1201)	
training:	Epoch: [26][7/233]	Loss 0.0245 (0.1065)	
training:	Epoch: [26][8/233]	Loss 0.2811 (0.1283)	
training:	Epoch: [26][9/233]	Loss 0.1285 (0.1283)	
training:	Epoch: [26][10/233]	Loss 0.1297 (0.1285)	
training:	Epoch: [26][11/233]	Loss 0.0349 (0.1199)	
training:	Epoch: [26][12/233]	Loss 0.0233 (0.1119)	
training:	Epoch: [26][13/233]	Loss 0.3364 (0.1292)	
training:	Epoch: [26][14/233]	Loss 0.0355 (0.1225)	
training:	Epoch: [26][15/233]	Loss 0.0207 (0.1157)	
training:	Epoch: [26][16/233]	Loss 0.0295 (0.1103)	
training:	Epoch: [26][17/233]	Loss 0.0208 (0.1050)	
training:	Epoch: [26][18/233]	Loss 0.0280 (0.1008)	
training:	Epoch: [26][19/233]	Loss 0.0819 (0.0998)	
training:	Epoch: [26][20/233]	Loss 0.2369 (0.1066)	
training:	Epoch: [26][21/233]	Loss 0.1236 (0.1074)	
training:	Epoch: [26][22/233]	Loss 0.1857 (0.1110)	
training:	Epoch: [26][23/233]	Loss 0.1302 (0.1118)	
training:	Epoch: [26][24/233]	Loss 0.1377 (0.1129)	
training:	Epoch: [26][25/233]	Loss 0.0367 (0.1099)	
training:	Epoch: [26][26/233]	Loss 0.2128 (0.1138)	
training:	Epoch: [26][27/233]	Loss 0.0901 (0.1129)	
training:	Epoch: [26][28/233]	Loss 0.0208 (0.1096)	
training:	Epoch: [26][29/233]	Loss 0.0361 (0.1071)	
training:	Epoch: [26][30/233]	Loss 0.0532 (0.1053)	
training:	Epoch: [26][31/233]	Loss 0.0411 (0.1032)	
training:	Epoch: [26][32/233]	Loss 0.0262 (0.1008)	
training:	Epoch: [26][33/233]	Loss 0.0372 (0.0989)	
training:	Epoch: [26][34/233]	Loss 0.0284 (0.0968)	
training:	Epoch: [26][35/233]	Loss 0.0458 (0.0954)	
training:	Epoch: [26][36/233]	Loss 0.0864 (0.0951)	
training:	Epoch: [26][37/233]	Loss 0.0238 (0.0932)	
training:	Epoch: [26][38/233]	Loss 0.1620 (0.0950)	
training:	Epoch: [26][39/233]	Loss 0.1103 (0.0954)	
training:	Epoch: [26][40/233]	Loss 0.1206 (0.0960)	
training:	Epoch: [26][41/233]	Loss 0.0261 (0.0943)	
training:	Epoch: [26][42/233]	Loss 0.0416 (0.0931)	
training:	Epoch: [26][43/233]	Loss 0.1922 (0.0954)	
training:	Epoch: [26][44/233]	Loss 0.0396 (0.0941)	
training:	Epoch: [26][45/233]	Loss 0.0376 (0.0928)	
training:	Epoch: [26][46/233]	Loss 0.0279 (0.0914)	
training:	Epoch: [26][47/233]	Loss 0.0299 (0.0901)	
training:	Epoch: [26][48/233]	Loss 0.1771 (0.0919)	
training:	Epoch: [26][49/233]	Loss 0.0315 (0.0907)	
training:	Epoch: [26][50/233]	Loss 0.0371 (0.0896)	
training:	Epoch: [26][51/233]	Loss 0.3041 (0.0938)	
training:	Epoch: [26][52/233]	Loss 0.0228 (0.0925)	
training:	Epoch: [26][53/233]	Loss 0.0257 (0.0912)	
training:	Epoch: [26][54/233]	Loss 0.0433 (0.0903)	
training:	Epoch: [26][55/233]	Loss 0.2029 (0.0924)	
training:	Epoch: [26][56/233]	Loss 0.1634 (0.0936)	
training:	Epoch: [26][57/233]	Loss 0.0561 (0.0930)	
training:	Epoch: [26][58/233]	Loss 0.1793 (0.0945)	
training:	Epoch: [26][59/233]	Loss 0.0949 (0.0945)	
training:	Epoch: [26][60/233]	Loss 0.1463 (0.0953)	
training:	Epoch: [26][61/233]	Loss 0.0372 (0.0944)	
training:	Epoch: [26][62/233]	Loss 0.2515 (0.0969)	
training:	Epoch: [26][63/233]	Loss 0.0244 (0.0958)	
training:	Epoch: [26][64/233]	Loss 0.1555 (0.0967)	
training:	Epoch: [26][65/233]	Loss 0.2033 (0.0983)	
training:	Epoch: [26][66/233]	Loss 0.1130 (0.0986)	
training:	Epoch: [26][67/233]	Loss 0.1740 (0.0997)	
training:	Epoch: [26][68/233]	Loss 0.2784 (0.1023)	
training:	Epoch: [26][69/233]	Loss 0.0232 (0.1012)	
training:	Epoch: [26][70/233]	Loss 0.0386 (0.1003)	
training:	Epoch: [26][71/233]	Loss 0.1801 (0.1014)	
training:	Epoch: [26][72/233]	Loss 0.0459 (0.1006)	
training:	Epoch: [26][73/233]	Loss 0.0560 (0.1000)	
training:	Epoch: [26][74/233]	Loss 0.0630 (0.0995)	
training:	Epoch: [26][75/233]	Loss 0.1163 (0.0997)	
training:	Epoch: [26][76/233]	Loss 0.0388 (0.0989)	
training:	Epoch: [26][77/233]	Loss 0.0653 (0.0985)	
training:	Epoch: [26][78/233]	Loss 0.0287 (0.0976)	
training:	Epoch: [26][79/233]	Loss 0.2508 (0.0996)	
training:	Epoch: [26][80/233]	Loss 0.0593 (0.0990)	
training:	Epoch: [26][81/233]	Loss 0.0328 (0.0982)	
training:	Epoch: [26][82/233]	Loss 0.1583 (0.0990)	
training:	Epoch: [26][83/233]	Loss 0.1648 (0.0998)	
training:	Epoch: [26][84/233]	Loss 0.1074 (0.0998)	
training:	Epoch: [26][85/233]	Loss 0.1052 (0.0999)	
training:	Epoch: [26][86/233]	Loss 0.3263 (0.1025)	
training:	Epoch: [26][87/233]	Loss 0.0310 (0.1017)	
training:	Epoch: [26][88/233]	Loss 0.0254 (0.1009)	
training:	Epoch: [26][89/233]	Loss 0.0210 (0.1000)	
training:	Epoch: [26][90/233]	Loss 0.3138 (0.1023)	
training:	Epoch: [26][91/233]	Loss 0.0570 (0.1018)	
training:	Epoch: [26][92/233]	Loss 0.0233 (0.1010)	
training:	Epoch: [26][93/233]	Loss 0.0600 (0.1005)	
training:	Epoch: [26][94/233]	Loss 0.1179 (0.1007)	
training:	Epoch: [26][95/233]	Loss 0.0212 (0.0999)	
training:	Epoch: [26][96/233]	Loss 0.1065 (0.1000)	
training:	Epoch: [26][97/233]	Loss 0.0192 (0.0991)	
training:	Epoch: [26][98/233]	Loss 0.0433 (0.0986)	
training:	Epoch: [26][99/233]	Loss 0.1728 (0.0993)	
training:	Epoch: [26][100/233]	Loss 0.0277 (0.0986)	
training:	Epoch: [26][101/233]	Loss 0.1879 (0.0995)	
training:	Epoch: [26][102/233]	Loss 0.0269 (0.0988)	
training:	Epoch: [26][103/233]	Loss 0.1571 (0.0993)	
training:	Epoch: [26][104/233]	Loss 0.0409 (0.0988)	
training:	Epoch: [26][105/233]	Loss 0.2454 (0.1002)	
training:	Epoch: [26][106/233]	Loss 0.0339 (0.0995)	
training:	Epoch: [26][107/233]	Loss 0.0282 (0.0989)	
training:	Epoch: [26][108/233]	Loss 0.0250 (0.0982)	
training:	Epoch: [26][109/233]	Loss 0.0390 (0.0976)	
training:	Epoch: [26][110/233]	Loss 0.0241 (0.0970)	
training:	Epoch: [26][111/233]	Loss 0.3153 (0.0989)	
training:	Epoch: [26][112/233]	Loss 0.0394 (0.0984)	
training:	Epoch: [26][113/233]	Loss 0.0175 (0.0977)	
training:	Epoch: [26][114/233]	Loss 0.0371 (0.0972)	
training:	Epoch: [26][115/233]	Loss 0.0296 (0.0966)	
training:	Epoch: [26][116/233]	Loss 0.0609 (0.0963)	
training:	Epoch: [26][117/233]	Loss 0.0824 (0.0961)	
training:	Epoch: [26][118/233]	Loss 0.1360 (0.0965)	
training:	Epoch: [26][119/233]	Loss 0.1597 (0.0970)	
training:	Epoch: [26][120/233]	Loss 0.4034 (0.0996)	
training:	Epoch: [26][121/233]	Loss 0.0801 (0.0994)	
training:	Epoch: [26][122/233]	Loss 0.0282 (0.0988)	
training:	Epoch: [26][123/233]	Loss 0.0694 (0.0986)	
training:	Epoch: [26][124/233]	Loss 0.0885 (0.0985)	
training:	Epoch: [26][125/233]	Loss 0.0476 (0.0981)	
training:	Epoch: [26][126/233]	Loss 0.0810 (0.0980)	
training:	Epoch: [26][127/233]	Loss 0.1308 (0.0982)	
training:	Epoch: [26][128/233]	Loss 0.0495 (0.0978)	
training:	Epoch: [26][129/233]	Loss 0.1518 (0.0983)	
training:	Epoch: [26][130/233]	Loss 0.1207 (0.0984)	
training:	Epoch: [26][131/233]	Loss 0.0264 (0.0979)	
training:	Epoch: [26][132/233]	Loss 0.0525 (0.0975)	
training:	Epoch: [26][133/233]	Loss 0.1730 (0.0981)	
training:	Epoch: [26][134/233]	Loss 0.2549 (0.0993)	
training:	Epoch: [26][135/233]	Loss 0.0611 (0.0990)	
training:	Epoch: [26][136/233]	Loss 0.1798 (0.0996)	
training:	Epoch: [26][137/233]	Loss 0.0346 (0.0991)	
training:	Epoch: [26][138/233]	Loss 0.0370 (0.0987)	
training:	Epoch: [26][139/233]	Loss 0.1344 (0.0989)	
training:	Epoch: [26][140/233]	Loss 0.2204 (0.0998)	
training:	Epoch: [26][141/233]	Loss 0.1935 (0.1005)	
training:	Epoch: [26][142/233]	Loss 0.1458 (0.1008)	
training:	Epoch: [26][143/233]	Loss 0.0439 (0.1004)	
training:	Epoch: [26][144/233]	Loss 0.0791 (0.1002)	
training:	Epoch: [26][145/233]	Loss 0.0248 (0.0997)	
training:	Epoch: [26][146/233]	Loss 0.0871 (0.0996)	
training:	Epoch: [26][147/233]	Loss 0.2484 (0.1006)	
training:	Epoch: [26][148/233]	Loss 0.1271 (0.1008)	
training:	Epoch: [26][149/233]	Loss 0.6205 (0.1043)	
training:	Epoch: [26][150/233]	Loss 0.1844 (0.1048)	
training:	Epoch: [26][151/233]	Loss 0.0497 (0.1045)	
training:	Epoch: [26][152/233]	Loss 0.0317 (0.1040)	
training:	Epoch: [26][153/233]	Loss 0.0604 (0.1037)	
training:	Epoch: [26][154/233]	Loss 0.0261 (0.1032)	
training:	Epoch: [26][155/233]	Loss 0.0416 (0.1028)	
training:	Epoch: [26][156/233]	Loss 0.0440 (0.1024)	
training:	Epoch: [26][157/233]	Loss 0.0746 (0.1022)	
training:	Epoch: [26][158/233]	Loss 0.0964 (0.1022)	
training:	Epoch: [26][159/233]	Loss 0.0817 (0.1021)	
training:	Epoch: [26][160/233]	Loss 0.0937 (0.1020)	
training:	Epoch: [26][161/233]	Loss 0.0810 (0.1019)	
training:	Epoch: [26][162/233]	Loss 0.1287 (0.1021)	
training:	Epoch: [26][163/233]	Loss 0.0770 (0.1019)	
training:	Epoch: [26][164/233]	Loss 0.1117 (0.1020)	
training:	Epoch: [26][165/233]	Loss 0.1238 (0.1021)	
training:	Epoch: [26][166/233]	Loss 0.0430 (0.1017)	
training:	Epoch: [26][167/233]	Loss 0.0417 (0.1014)	
training:	Epoch: [26][168/233]	Loss 0.0410 (0.1010)	
training:	Epoch: [26][169/233]	Loss 0.1615 (0.1014)	
training:	Epoch: [26][170/233]	Loss 0.1574 (0.1017)	
training:	Epoch: [26][171/233]	Loss 0.0346 (0.1013)	
training:	Epoch: [26][172/233]	Loss 0.1174 (0.1014)	
training:	Epoch: [26][173/233]	Loss 0.0252 (0.1010)	
training:	Epoch: [26][174/233]	Loss 0.1230 (0.1011)	
training:	Epoch: [26][175/233]	Loss 0.2812 (0.1021)	
training:	Epoch: [26][176/233]	Loss 0.1014 (0.1021)	
training:	Epoch: [26][177/233]	Loss 0.0369 (0.1018)	
training:	Epoch: [26][178/233]	Loss 0.0285 (0.1013)	
training:	Epoch: [26][179/233]	Loss 0.0889 (0.1013)	
training:	Epoch: [26][180/233]	Loss 0.0676 (0.1011)	
training:	Epoch: [26][181/233]	Loss 0.0340 (0.1007)	
training:	Epoch: [26][182/233]	Loss 0.0368 (0.1004)	
training:	Epoch: [26][183/233]	Loss 0.0444 (0.1001)	
training:	Epoch: [26][184/233]	Loss 0.3876 (0.1016)	
training:	Epoch: [26][185/233]	Loss 0.1496 (0.1019)	
training:	Epoch: [26][186/233]	Loss 0.3927 (0.1034)	
training:	Epoch: [26][187/233]	Loss 0.0648 (0.1032)	
training:	Epoch: [26][188/233]	Loss 0.0598 (0.1030)	
training:	Epoch: [26][189/233]	Loss 0.0279 (0.1026)	
training:	Epoch: [26][190/233]	Loss 0.1994 (0.1031)	
training:	Epoch: [26][191/233]	Loss 0.0362 (0.1028)	
training:	Epoch: [26][192/233]	Loss 0.1136 (0.1028)	
training:	Epoch: [26][193/233]	Loss 0.0954 (0.1028)	
training:	Epoch: [26][194/233]	Loss 0.2777 (0.1037)	
training:	Epoch: [26][195/233]	Loss 0.2802 (0.1046)	
training:	Epoch: [26][196/233]	Loss 0.1458 (0.1048)	
training:	Epoch: [26][197/233]	Loss 0.0331 (0.1044)	
training:	Epoch: [26][198/233]	Loss 0.0333 (0.1041)	
training:	Epoch: [26][199/233]	Loss 0.0817 (0.1040)	
training:	Epoch: [26][200/233]	Loss 0.1589 (0.1042)	
training:	Epoch: [26][201/233]	Loss 0.1797 (0.1046)	
training:	Epoch: [26][202/233]	Loss 0.1376 (0.1048)	
training:	Epoch: [26][203/233]	Loss 0.0493 (0.1045)	
training:	Epoch: [26][204/233]	Loss 0.0384 (0.1042)	
training:	Epoch: [26][205/233]	Loss 0.0555 (0.1039)	
training:	Epoch: [26][206/233]	Loss 0.1328 (0.1041)	
training:	Epoch: [26][207/233]	Loss 0.1251 (0.1042)	
training:	Epoch: [26][208/233]	Loss 0.0521 (0.1039)	
training:	Epoch: [26][209/233]	Loss 0.0356 (0.1036)	
training:	Epoch: [26][210/233]	Loss 0.1147 (0.1037)	
training:	Epoch: [26][211/233]	Loss 0.0323 (0.1033)	
training:	Epoch: [26][212/233]	Loss 0.1049 (0.1033)	
training:	Epoch: [26][213/233]	Loss 0.0709 (0.1032)	
training:	Epoch: [26][214/233]	Loss 0.2936 (0.1041)	
training:	Epoch: [26][215/233]	Loss 0.1571 (0.1043)	
training:	Epoch: [26][216/233]	Loss 0.0310 (0.1040)	
training:	Epoch: [26][217/233]	Loss 0.2492 (0.1046)	
training:	Epoch: [26][218/233]	Loss 0.0653 (0.1045)	
training:	Epoch: [26][219/233]	Loss 0.0845 (0.1044)	
training:	Epoch: [26][220/233]	Loss 0.0671 (0.1042)	
training:	Epoch: [26][221/233]	Loss 0.2512 (0.1049)	
training:	Epoch: [26][222/233]	Loss 0.1370 (0.1050)	
training:	Epoch: [26][223/233]	Loss 0.1046 (0.1050)	
training:	Epoch: [26][224/233]	Loss 0.0375 (0.1047)	
training:	Epoch: [26][225/233]	Loss 0.0765 (0.1046)	
training:	Epoch: [26][226/233]	Loss 0.0654 (0.1044)	
training:	Epoch: [26][227/233]	Loss 0.1178 (0.1045)	
training:	Epoch: [26][228/233]	Loss 0.1019 (0.1045)	
training:	Epoch: [26][229/233]	Loss 0.0268 (0.1041)	
training:	Epoch: [26][230/233]	Loss 0.0407 (0.1038)	
training:	Epoch: [26][231/233]	Loss 0.0262 (0.1035)	
training:	Epoch: [26][232/233]	Loss 0.0500 (0.1033)	
training:	Epoch: [26][233/233]	Loss 0.1142 (0.1033)	
Training:	 Loss: 0.1031

Training:	 ACC: 0.9860 0.9857 0.9790 0.9930
Validation:	 ACC: 0.7944 0.7929 0.7630 0.8258
Validation:	 Best_BACC: 0.8162 0.8154 0.7998 0.8326
Validation:	 Loss: 0.6371
Pretraining:	Epoch 27/200
----------
training:	Epoch: [27][1/233]	Loss 0.0277 (0.0277)	
training:	Epoch: [27][2/233]	Loss 0.0184 (0.0231)	
training:	Epoch: [27][3/233]	Loss 0.0241 (0.0234)	
training:	Epoch: [27][4/233]	Loss 0.0411 (0.0278)	
training:	Epoch: [27][5/233]	Loss 0.2136 (0.0650)	
training:	Epoch: [27][6/233]	Loss 0.0520 (0.0628)	
training:	Epoch: [27][7/233]	Loss 0.0776 (0.0649)	
training:	Epoch: [27][8/233]	Loss 0.1840 (0.0798)	
training:	Epoch: [27][9/233]	Loss 0.0433 (0.0758)	
training:	Epoch: [27][10/233]	Loss 0.1416 (0.0824)	
training:	Epoch: [27][11/233]	Loss 0.1505 (0.0885)	
training:	Epoch: [27][12/233]	Loss 0.0427 (0.0847)	
training:	Epoch: [27][13/233]	Loss 0.1629 (0.0907)	
training:	Epoch: [27][14/233]	Loss 0.0332 (0.0866)	
training:	Epoch: [27][15/233]	Loss 0.1005 (0.0876)	
training:	Epoch: [27][16/233]	Loss 0.1461 (0.0912)	
training:	Epoch: [27][17/233]	Loss 0.1029 (0.0919)	
training:	Epoch: [27][18/233]	Loss 0.1111 (0.0930)	
training:	Epoch: [27][19/233]	Loss 0.0983 (0.0932)	
training:	Epoch: [27][20/233]	Loss 0.0266 (0.0899)	
training:	Epoch: [27][21/233]	Loss 0.0546 (0.0882)	
training:	Epoch: [27][22/233]	Loss 0.0963 (0.0886)	
training:	Epoch: [27][23/233]	Loss 0.1229 (0.0901)	
training:	Epoch: [27][24/233]	Loss 0.0789 (0.0896)	
training:	Epoch: [27][25/233]	Loss 0.2436 (0.0958)	
training:	Epoch: [27][26/233]	Loss 0.1451 (0.0977)	
training:	Epoch: [27][27/233]	Loss 0.0270 (0.0951)	
training:	Epoch: [27][28/233]	Loss 0.1005 (0.0953)	
training:	Epoch: [27][29/233]	Loss 0.0250 (0.0928)	
training:	Epoch: [27][30/233]	Loss 0.0405 (0.0911)	
training:	Epoch: [27][31/233]	Loss 0.1666 (0.0935)	
training:	Epoch: [27][32/233]	Loss 0.0335 (0.0917)	
training:	Epoch: [27][33/233]	Loss 0.0576 (0.0906)	
training:	Epoch: [27][34/233]	Loss 0.0204 (0.0886)	
training:	Epoch: [27][35/233]	Loss 0.0249 (0.0867)	
training:	Epoch: [27][36/233]	Loss 0.0473 (0.0856)	
training:	Epoch: [27][37/233]	Loss 0.0250 (0.0840)	
training:	Epoch: [27][38/233]	Loss 0.0348 (0.0827)	
training:	Epoch: [27][39/233]	Loss 0.0240 (0.0812)	
training:	Epoch: [27][40/233]	Loss 0.0357 (0.0801)	
training:	Epoch: [27][41/233]	Loss 0.0258 (0.0787)	
training:	Epoch: [27][42/233]	Loss 0.0271 (0.0775)	
training:	Epoch: [27][43/233]	Loss 0.0236 (0.0763)	
training:	Epoch: [27][44/233]	Loss 0.0368 (0.0754)	
training:	Epoch: [27][45/233]	Loss 0.0240 (0.0742)	
training:	Epoch: [27][46/233]	Loss 0.0238 (0.0731)	
training:	Epoch: [27][47/233]	Loss 0.0900 (0.0735)	
training:	Epoch: [27][48/233]	Loss 0.1335 (0.0747)	
training:	Epoch: [27][49/233]	Loss 0.1307 (0.0759)	
training:	Epoch: [27][50/233]	Loss 0.2081 (0.0785)	
training:	Epoch: [27][51/233]	Loss 0.0551 (0.0781)	
training:	Epoch: [27][52/233]	Loss 0.1723 (0.0799)	
training:	Epoch: [27][53/233]	Loss 0.0287 (0.0789)	
training:	Epoch: [27][54/233]	Loss 0.1596 (0.0804)	
training:	Epoch: [27][55/233]	Loss 0.2168 (0.0829)	
training:	Epoch: [27][56/233]	Loss 0.1892 (0.0848)	
training:	Epoch: [27][57/233]	Loss 0.0929 (0.0849)	
training:	Epoch: [27][58/233]	Loss 0.3310 (0.0892)	
training:	Epoch: [27][59/233]	Loss 0.0207 (0.0880)	
training:	Epoch: [27][60/233]	Loss 0.2424 (0.0906)	
training:	Epoch: [27][61/233]	Loss 0.0580 (0.0900)	
training:	Epoch: [27][62/233]	Loss 0.0941 (0.0901)	
training:	Epoch: [27][63/233]	Loss 0.1083 (0.0904)	
training:	Epoch: [27][64/233]	Loss 0.0270 (0.0894)	
training:	Epoch: [27][65/233]	Loss 0.0983 (0.0895)	
training:	Epoch: [27][66/233]	Loss 0.0575 (0.0891)	
training:	Epoch: [27][67/233]	Loss 0.0830 (0.0890)	
training:	Epoch: [27][68/233]	Loss 0.0879 (0.0890)	
training:	Epoch: [27][69/233]	Loss 0.0506 (0.0884)	
training:	Epoch: [27][70/233]	Loss 0.0429 (0.0877)	
training:	Epoch: [27][71/233]	Loss 0.0240 (0.0868)	
training:	Epoch: [27][72/233]	Loss 0.0988 (0.0870)	
training:	Epoch: [27][73/233]	Loss 0.0442 (0.0864)	
training:	Epoch: [27][74/233]	Loss 0.0376 (0.0858)	
training:	Epoch: [27][75/233]	Loss 0.2281 (0.0877)	
training:	Epoch: [27][76/233]	Loss 0.1620 (0.0886)	
training:	Epoch: [27][77/233]	Loss 0.0637 (0.0883)	
training:	Epoch: [27][78/233]	Loss 0.1581 (0.0892)	
training:	Epoch: [27][79/233]	Loss 0.1962 (0.0906)	
training:	Epoch: [27][80/233]	Loss 0.1482 (0.0913)	
training:	Epoch: [27][81/233]	Loss 0.1583 (0.0921)	
training:	Epoch: [27][82/233]	Loss 0.0428 (0.0915)	
training:	Epoch: [27][83/233]	Loss 0.0875 (0.0915)	
training:	Epoch: [27][84/233]	Loss 0.0374 (0.0908)	
training:	Epoch: [27][85/233]	Loss 0.0657 (0.0905)	
training:	Epoch: [27][86/233]	Loss 0.0716 (0.0903)	
training:	Epoch: [27][87/233]	Loss 0.1441 (0.0909)	
training:	Epoch: [27][88/233]	Loss 0.0250 (0.0902)	
training:	Epoch: [27][89/233]	Loss 0.0959 (0.0902)	
training:	Epoch: [27][90/233]	Loss 0.1047 (0.0904)	
training:	Epoch: [27][91/233]	Loss 0.1300 (0.0908)	
training:	Epoch: [27][92/233]	Loss 0.3592 (0.0938)	
training:	Epoch: [27][93/233]	Loss 0.2730 (0.0957)	
training:	Epoch: [27][94/233]	Loss 0.0546 (0.0952)	
training:	Epoch: [27][95/233]	Loss 0.1622 (0.0960)	
training:	Epoch: [27][96/233]	Loss 0.0204 (0.0952)	
training:	Epoch: [27][97/233]	Loss 0.0374 (0.0946)	
training:	Epoch: [27][98/233]	Loss 0.0596 (0.0942)	
training:	Epoch: [27][99/233]	Loss 0.0853 (0.0941)	
training:	Epoch: [27][100/233]	Loss 0.0216 (0.0934)	
training:	Epoch: [27][101/233]	Loss 0.0502 (0.0930)	
training:	Epoch: [27][102/233]	Loss 0.1747 (0.0938)	
training:	Epoch: [27][103/233]	Loss 0.0756 (0.0936)	
training:	Epoch: [27][104/233]	Loss 0.1993 (0.0946)	
training:	Epoch: [27][105/233]	Loss 0.0697 (0.0944)	
training:	Epoch: [27][106/233]	Loss 0.1795 (0.0952)	
training:	Epoch: [27][107/233]	Loss 0.1215 (0.0954)	
training:	Epoch: [27][108/233]	Loss 0.0387 (0.0949)	
training:	Epoch: [27][109/233]	Loss 0.0936 (0.0949)	
training:	Epoch: [27][110/233]	Loss 0.0256 (0.0943)	
training:	Epoch: [27][111/233]	Loss 0.1240 (0.0945)	
training:	Epoch: [27][112/233]	Loss 0.0489 (0.0941)	
training:	Epoch: [27][113/233]	Loss 0.0358 (0.0936)	
training:	Epoch: [27][114/233]	Loss 0.1859 (0.0944)	
training:	Epoch: [27][115/233]	Loss 0.0562 (0.0941)	
training:	Epoch: [27][116/233]	Loss 0.0940 (0.0941)	
training:	Epoch: [27][117/233]	Loss 0.0234 (0.0935)	
training:	Epoch: [27][118/233]	Loss 0.2457 (0.0948)	
training:	Epoch: [27][119/233]	Loss 0.0633 (0.0945)	
training:	Epoch: [27][120/233]	Loss 0.3776 (0.0969)	
training:	Epoch: [27][121/233]	Loss 0.0941 (0.0968)	
training:	Epoch: [27][122/233]	Loss 0.0293 (0.0963)	
training:	Epoch: [27][123/233]	Loss 0.1112 (0.0964)	
training:	Epoch: [27][124/233]	Loss 0.2638 (0.0978)	
training:	Epoch: [27][125/233]	Loss 0.0350 (0.0972)	
training:	Epoch: [27][126/233]	Loss 0.0261 (0.0967)	
training:	Epoch: [27][127/233]	Loss 0.2337 (0.0978)	
training:	Epoch: [27][128/233]	Loss 0.0906 (0.0977)	
training:	Epoch: [27][129/233]	Loss 0.0549 (0.0974)	
training:	Epoch: [27][130/233]	Loss 0.1319 (0.0976)	
training:	Epoch: [27][131/233]	Loss 0.1029 (0.0977)	
training:	Epoch: [27][132/233]	Loss 0.1685 (0.0982)	
training:	Epoch: [27][133/233]	Loss 0.0680 (0.0980)	
training:	Epoch: [27][134/233]	Loss 0.0559 (0.0977)	
training:	Epoch: [27][135/233]	Loss 0.1398 (0.0980)	
training:	Epoch: [27][136/233]	Loss 0.0569 (0.0977)	
training:	Epoch: [27][137/233]	Loss 0.0252 (0.0972)	
training:	Epoch: [27][138/233]	Loss 0.0590 (0.0969)	
training:	Epoch: [27][139/233]	Loss 0.0311 (0.0964)	
training:	Epoch: [27][140/233]	Loss 0.1735 (0.0970)	
training:	Epoch: [27][141/233]	Loss 0.0302 (0.0965)	
training:	Epoch: [27][142/233]	Loss 0.0276 (0.0960)	
training:	Epoch: [27][143/233]	Loss 0.0964 (0.0960)	
training:	Epoch: [27][144/233]	Loss 0.0366 (0.0956)	
training:	Epoch: [27][145/233]	Loss 0.1570 (0.0960)	
training:	Epoch: [27][146/233]	Loss 0.0461 (0.0957)	
training:	Epoch: [27][147/233]	Loss 0.0296 (0.0952)	
training:	Epoch: [27][148/233]	Loss 0.0164 (0.0947)	
training:	Epoch: [27][149/233]	Loss 0.0728 (0.0945)	
training:	Epoch: [27][150/233]	Loss 0.0204 (0.0940)	
training:	Epoch: [27][151/233]	Loss 0.1406 (0.0944)	
training:	Epoch: [27][152/233]	Loss 0.0322 (0.0939)	
training:	Epoch: [27][153/233]	Loss 0.0173 (0.0934)	
training:	Epoch: [27][154/233]	Loss 0.1209 (0.0936)	
training:	Epoch: [27][155/233]	Loss 0.0320 (0.0932)	
training:	Epoch: [27][156/233]	Loss 0.0225 (0.0928)	
training:	Epoch: [27][157/233]	Loss 0.0440 (0.0925)	
training:	Epoch: [27][158/233]	Loss 0.2379 (0.0934)	
training:	Epoch: [27][159/233]	Loss 0.2385 (0.0943)	
training:	Epoch: [27][160/233]	Loss 0.0390 (0.0939)	
training:	Epoch: [27][161/233]	Loss 0.0300 (0.0936)	
training:	Epoch: [27][162/233]	Loss 0.1538 (0.0939)	
training:	Epoch: [27][163/233]	Loss 0.0265 (0.0935)	
training:	Epoch: [27][164/233]	Loss 0.0433 (0.0932)	
training:	Epoch: [27][165/233]	Loss 0.0324 (0.0928)	
training:	Epoch: [27][166/233]	Loss 0.1070 (0.0929)	
training:	Epoch: [27][167/233]	Loss 0.0384 (0.0926)	
training:	Epoch: [27][168/233]	Loss 0.0387 (0.0923)	
training:	Epoch: [27][169/233]	Loss 0.0570 (0.0921)	
training:	Epoch: [27][170/233]	Loss 0.1545 (0.0924)	
training:	Epoch: [27][171/233]	Loss 0.0677 (0.0923)	
training:	Epoch: [27][172/233]	Loss 0.1200 (0.0924)	
training:	Epoch: [27][173/233]	Loss 0.0206 (0.0920)	
training:	Epoch: [27][174/233]	Loss 0.0727 (0.0919)	
training:	Epoch: [27][175/233]	Loss 0.0615 (0.0917)	
training:	Epoch: [27][176/233]	Loss 0.1001 (0.0918)	
training:	Epoch: [27][177/233]	Loss 0.0210 (0.0914)	
training:	Epoch: [27][178/233]	Loss 0.1344 (0.0916)	
training:	Epoch: [27][179/233]	Loss 0.1774 (0.0921)	
training:	Epoch: [27][180/233]	Loss 0.0297 (0.0918)	
training:	Epoch: [27][181/233]	Loss 0.2131 (0.0924)	
training:	Epoch: [27][182/233]	Loss 0.0675 (0.0923)	
training:	Epoch: [27][183/233]	Loss 0.1203 (0.0925)	
training:	Epoch: [27][184/233]	Loss 0.0300 (0.0921)	
training:	Epoch: [27][185/233]	Loss 0.0154 (0.0917)	
training:	Epoch: [27][186/233]	Loss 0.0227 (0.0913)	
training:	Epoch: [27][187/233]	Loss 0.1727 (0.0918)	
training:	Epoch: [27][188/233]	Loss 0.0933 (0.0918)	
training:	Epoch: [27][189/233]	Loss 0.2213 (0.0925)	
training:	Epoch: [27][190/233]	Loss 0.3984 (0.0941)	
training:	Epoch: [27][191/233]	Loss 0.0652 (0.0939)	
training:	Epoch: [27][192/233]	Loss 0.1778 (0.0944)	
training:	Epoch: [27][193/233]	Loss 0.0281 (0.0940)	
training:	Epoch: [27][194/233]	Loss 0.2428 (0.0948)	
training:	Epoch: [27][195/233]	Loss 0.0495 (0.0945)	
training:	Epoch: [27][196/233]	Loss 0.1550 (0.0949)	
training:	Epoch: [27][197/233]	Loss 0.0595 (0.0947)	
training:	Epoch: [27][198/233]	Loss 0.1812 (0.0951)	
training:	Epoch: [27][199/233]	Loss 0.2055 (0.0957)	
training:	Epoch: [27][200/233]	Loss 0.4325 (0.0973)	
training:	Epoch: [27][201/233]	Loss 0.2621 (0.0982)	
training:	Epoch: [27][202/233]	Loss 0.0404 (0.0979)	
training:	Epoch: [27][203/233]	Loss 0.0640 (0.0977)	
training:	Epoch: [27][204/233]	Loss 0.0215 (0.0973)	
training:	Epoch: [27][205/233]	Loss 0.0475 (0.0971)	
training:	Epoch: [27][206/233]	Loss 0.2457 (0.0978)	
training:	Epoch: [27][207/233]	Loss 0.0220 (0.0975)	
training:	Epoch: [27][208/233]	Loss 0.0272 (0.0971)	
training:	Epoch: [27][209/233]	Loss 0.2317 (0.0978)	
training:	Epoch: [27][210/233]	Loss 0.0226 (0.0974)	
training:	Epoch: [27][211/233]	Loss 0.0634 (0.0972)	
training:	Epoch: [27][212/233]	Loss 0.0306 (0.0969)	
training:	Epoch: [27][213/233]	Loss 0.0992 (0.0969)	
training:	Epoch: [27][214/233]	Loss 0.0734 (0.0968)	
training:	Epoch: [27][215/233]	Loss 0.0375 (0.0966)	
training:	Epoch: [27][216/233]	Loss 0.1849 (0.0970)	
training:	Epoch: [27][217/233]	Loss 0.0265 (0.0966)	
training:	Epoch: [27][218/233]	Loss 0.1709 (0.0970)	
training:	Epoch: [27][219/233]	Loss 0.0938 (0.0970)	
training:	Epoch: [27][220/233]	Loss 0.0476 (0.0967)	
training:	Epoch: [27][221/233]	Loss 0.0370 (0.0965)	
training:	Epoch: [27][222/233]	Loss 0.1224 (0.0966)	
training:	Epoch: [27][223/233]	Loss 0.0230 (0.0963)	
training:	Epoch: [27][224/233]	Loss 0.0547 (0.0961)	
training:	Epoch: [27][225/233]	Loss 0.0561 (0.0959)	
training:	Epoch: [27][226/233]	Loss 0.0259 (0.0956)	
training:	Epoch: [27][227/233]	Loss 0.1995 (0.0960)	
training:	Epoch: [27][228/233]	Loss 0.1642 (0.0963)	
training:	Epoch: [27][229/233]	Loss 0.1643 (0.0966)	
training:	Epoch: [27][230/233]	Loss 0.0164 (0.0963)	
training:	Epoch: [27][231/233]	Loss 0.1779 (0.0966)	
training:	Epoch: [27][232/233]	Loss 0.1929 (0.0971)	
training:	Epoch: [27][233/233]	Loss 0.1142 (0.0971)	
Training:	 Loss: 0.0969

Training:	 ACC: 0.9888 0.9886 0.9856 0.9919
Validation:	 ACC: 0.7962 0.7961 0.7957 0.7966
Validation:	 Best_BACC: 0.8162 0.8154 0.7998 0.8326
Validation:	 Loss: 0.6616
Pretraining:	Epoch 28/200
----------
training:	Epoch: [28][1/233]	Loss 0.0468 (0.0468)	
training:	Epoch: [28][2/233]	Loss 0.0408 (0.0438)	
training:	Epoch: [28][3/233]	Loss 0.1446 (0.0774)	
training:	Epoch: [28][4/233]	Loss 0.1415 (0.0934)	
training:	Epoch: [28][5/233]	Loss 0.1686 (0.1085)	
training:	Epoch: [28][6/233]	Loss 0.2171 (0.1266)	
training:	Epoch: [28][7/233]	Loss 0.0250 (0.1121)	
training:	Epoch: [28][8/233]	Loss 0.0385 (0.1029)	
training:	Epoch: [28][9/233]	Loss 0.0682 (0.0990)	
training:	Epoch: [28][10/233]	Loss 0.1414 (0.1033)	
training:	Epoch: [28][11/233]	Loss 0.0256 (0.0962)	
training:	Epoch: [28][12/233]	Loss 0.0684 (0.0939)	
training:	Epoch: [28][13/233]	Loss 0.0287 (0.0889)	
training:	Epoch: [28][14/233]	Loss 0.2389 (0.0996)	
training:	Epoch: [28][15/233]	Loss 0.0854 (0.0986)	
training:	Epoch: [28][16/233]	Loss 0.0171 (0.0935)	
training:	Epoch: [28][17/233]	Loss 0.1533 (0.0971)	
training:	Epoch: [28][18/233]	Loss 0.0567 (0.0948)	
training:	Epoch: [28][19/233]	Loss 0.0962 (0.0949)	
training:	Epoch: [28][20/233]	Loss 0.0206 (0.0912)	
training:	Epoch: [28][21/233]	Loss 0.0339 (0.0885)	
training:	Epoch: [28][22/233]	Loss 0.2575 (0.0961)	
training:	Epoch: [28][23/233]	Loss 0.1864 (0.1001)	
training:	Epoch: [28][24/233]	Loss 0.0251 (0.0969)	
training:	Epoch: [28][25/233]	Loss 0.0915 (0.0967)	
training:	Epoch: [28][26/233]	Loss 0.0400 (0.0945)	
training:	Epoch: [28][27/233]	Loss 0.0229 (0.0919)	
training:	Epoch: [28][28/233]	Loss 0.0282 (0.0896)	
training:	Epoch: [28][29/233]	Loss 0.1902 (0.0931)	
training:	Epoch: [28][30/233]	Loss 0.1839 (0.0961)	
training:	Epoch: [28][31/233]	Loss 0.0228 (0.0937)	
training:	Epoch: [28][32/233]	Loss 0.0257 (0.0916)	
training:	Epoch: [28][33/233]	Loss 0.1366 (0.0930)	
training:	Epoch: [28][34/233]	Loss 0.1743 (0.0954)	
training:	Epoch: [28][35/233]	Loss 0.0894 (0.0952)	
training:	Epoch: [28][36/233]	Loss 0.0498 (0.0939)	
training:	Epoch: [28][37/233]	Loss 0.0990 (0.0941)	
training:	Epoch: [28][38/233]	Loss 0.0171 (0.0920)	
training:	Epoch: [28][39/233]	Loss 0.1789 (0.0943)	
training:	Epoch: [28][40/233]	Loss 0.0222 (0.0925)	
training:	Epoch: [28][41/233]	Loss 0.1489 (0.0938)	
training:	Epoch: [28][42/233]	Loss 0.0255 (0.0922)	
training:	Epoch: [28][43/233]	Loss 0.0259 (0.0907)	
training:	Epoch: [28][44/233]	Loss 0.1901 (0.0929)	
training:	Epoch: [28][45/233]	Loss 0.1166 (0.0935)	
training:	Epoch: [28][46/233]	Loss 0.0278 (0.0920)	
training:	Epoch: [28][47/233]	Loss 0.0170 (0.0904)	
training:	Epoch: [28][48/233]	Loss 0.0405 (0.0894)	
training:	Epoch: [28][49/233]	Loss 0.0415 (0.0884)	
training:	Epoch: [28][50/233]	Loss 0.0317 (0.0873)	
training:	Epoch: [28][51/233]	Loss 0.2086 (0.0897)	
training:	Epoch: [28][52/233]	Loss 0.0250 (0.0884)	
training:	Epoch: [28][53/233]	Loss 0.1504 (0.0896)	
training:	Epoch: [28][54/233]	Loss 0.0262 (0.0884)	
training:	Epoch: [28][55/233]	Loss 0.0238 (0.0872)	
training:	Epoch: [28][56/233]	Loss 0.0672 (0.0869)	
training:	Epoch: [28][57/233]	Loss 0.0215 (0.0857)	
training:	Epoch: [28][58/233]	Loss 0.0233 (0.0847)	
training:	Epoch: [28][59/233]	Loss 0.0369 (0.0839)	
training:	Epoch: [28][60/233]	Loss 0.0240 (0.0829)	
training:	Epoch: [28][61/233]	Loss 0.0152 (0.0817)	
training:	Epoch: [28][62/233]	Loss 0.0258 (0.0808)	
training:	Epoch: [28][63/233]	Loss 0.0357 (0.0801)	
training:	Epoch: [28][64/233]	Loss 0.0496 (0.0796)	
training:	Epoch: [28][65/233]	Loss 0.0840 (0.0797)	
training:	Epoch: [28][66/233]	Loss 0.0391 (0.0791)	
training:	Epoch: [28][67/233]	Loss 0.0227 (0.0783)	
training:	Epoch: [28][68/233]	Loss 0.0344 (0.0776)	
training:	Epoch: [28][69/233]	Loss 0.1775 (0.0791)	
training:	Epoch: [28][70/233]	Loss 0.2866 (0.0820)	
training:	Epoch: [28][71/233]	Loss 0.0867 (0.0821)	
training:	Epoch: [28][72/233]	Loss 0.1552 (0.0831)	
training:	Epoch: [28][73/233]	Loss 0.0287 (0.0824)	
training:	Epoch: [28][74/233]	Loss 0.0258 (0.0816)	
training:	Epoch: [28][75/233]	Loss 0.0235 (0.0808)	
training:	Epoch: [28][76/233]	Loss 0.0195 (0.0800)	
training:	Epoch: [28][77/233]	Loss 0.1581 (0.0810)	
training:	Epoch: [28][78/233]	Loss 0.0314 (0.0804)	
training:	Epoch: [28][79/233]	Loss 0.1636 (0.0814)	
training:	Epoch: [28][80/233]	Loss 0.2173 (0.0831)	
training:	Epoch: [28][81/233]	Loss 0.1175 (0.0836)	
training:	Epoch: [28][82/233]	Loss 0.1225 (0.0840)	
training:	Epoch: [28][83/233]	Loss 0.0317 (0.0834)	
training:	Epoch: [28][84/233]	Loss 0.0278 (0.0828)	
training:	Epoch: [28][85/233]	Loss 0.0326 (0.0822)	
training:	Epoch: [28][86/233]	Loss 0.1339 (0.0828)	
training:	Epoch: [28][87/233]	Loss 0.1606 (0.0837)	
training:	Epoch: [28][88/233]	Loss 0.0868 (0.0837)	
training:	Epoch: [28][89/233]	Loss 0.0353 (0.0832)	
training:	Epoch: [28][90/233]	Loss 0.0257 (0.0825)	
training:	Epoch: [28][91/233]	Loss 0.2173 (0.0840)	
training:	Epoch: [28][92/233]	Loss 0.0216 (0.0833)	
training:	Epoch: [28][93/233]	Loss 0.1243 (0.0838)	
training:	Epoch: [28][94/233]	Loss 0.0965 (0.0839)	
training:	Epoch: [28][95/233]	Loss 0.0437 (0.0835)	
training:	Epoch: [28][96/233]	Loss 0.0676 (0.0833)	
training:	Epoch: [28][97/233]	Loss 0.1505 (0.0840)	
training:	Epoch: [28][98/233]	Loss 0.1733 (0.0849)	
training:	Epoch: [28][99/233]	Loss 0.0320 (0.0844)	
training:	Epoch: [28][100/233]	Loss 0.0393 (0.0839)	
training:	Epoch: [28][101/233]	Loss 0.0317 (0.0834)	
training:	Epoch: [28][102/233]	Loss 0.0172 (0.0828)	
training:	Epoch: [28][103/233]	Loss 0.0136 (0.0821)	
training:	Epoch: [28][104/233]	Loss 0.0285 (0.0816)	
training:	Epoch: [28][105/233]	Loss 0.0814 (0.0816)	
training:	Epoch: [28][106/233]	Loss 0.0201 (0.0810)	
training:	Epoch: [28][107/233]	Loss 0.1745 (0.0819)	
training:	Epoch: [28][108/233]	Loss 0.0201 (0.0813)	
training:	Epoch: [28][109/233]	Loss 0.0186 (0.0807)	
training:	Epoch: [28][110/233]	Loss 0.0352 (0.0803)	
training:	Epoch: [28][111/233]	Loss 0.0157 (0.0797)	
training:	Epoch: [28][112/233]	Loss 0.1781 (0.0806)	
training:	Epoch: [28][113/233]	Loss 0.0527 (0.0804)	
training:	Epoch: [28][114/233]	Loss 0.1832 (0.0813)	
training:	Epoch: [28][115/233]	Loss 0.0230 (0.0807)	
training:	Epoch: [28][116/233]	Loss 0.0616 (0.0806)	
training:	Epoch: [28][117/233]	Loss 0.2166 (0.0817)	
training:	Epoch: [28][118/233]	Loss 0.0217 (0.0812)	
training:	Epoch: [28][119/233]	Loss 0.0214 (0.0807)	
training:	Epoch: [28][120/233]	Loss 0.0421 (0.0804)	
training:	Epoch: [28][121/233]	Loss 0.0337 (0.0800)	
training:	Epoch: [28][122/233]	Loss 0.0437 (0.0797)	
training:	Epoch: [28][123/233]	Loss 0.3094 (0.0816)	
training:	Epoch: [28][124/233]	Loss 0.0572 (0.0814)	
training:	Epoch: [28][125/233]	Loss 0.4010 (0.0840)	
training:	Epoch: [28][126/233]	Loss 0.0832 (0.0839)	
training:	Epoch: [28][127/233]	Loss 0.0396 (0.0836)	
training:	Epoch: [28][128/233]	Loss 0.1323 (0.0840)	
training:	Epoch: [28][129/233]	Loss 0.3024 (0.0857)	
training:	Epoch: [28][130/233]	Loss 0.1568 (0.0862)	
training:	Epoch: [28][131/233]	Loss 0.0764 (0.0861)	
training:	Epoch: [28][132/233]	Loss 0.1784 (0.0868)	
training:	Epoch: [28][133/233]	Loss 0.0218 (0.0864)	
training:	Epoch: [28][134/233]	Loss 0.0453 (0.0861)	
training:	Epoch: [28][135/233]	Loss 0.1936 (0.0868)	
training:	Epoch: [28][136/233]	Loss 0.0140 (0.0863)	
training:	Epoch: [28][137/233]	Loss 0.0725 (0.0862)	
training:	Epoch: [28][138/233]	Loss 0.2909 (0.0877)	
training:	Epoch: [28][139/233]	Loss 0.3190 (0.0894)	
training:	Epoch: [28][140/233]	Loss 0.0195 (0.0889)	
training:	Epoch: [28][141/233]	Loss 0.0222 (0.0884)	
training:	Epoch: [28][142/233]	Loss 0.1770 (0.0890)	
training:	Epoch: [28][143/233]	Loss 0.0324 (0.0886)	
training:	Epoch: [28][144/233]	Loss 0.1488 (0.0890)	
training:	Epoch: [28][145/233]	Loss 0.0262 (0.0886)	
training:	Epoch: [28][146/233]	Loss 0.0803 (0.0885)	
training:	Epoch: [28][147/233]	Loss 0.0356 (0.0882)	
training:	Epoch: [28][148/233]	Loss 0.0183 (0.0877)	
training:	Epoch: [28][149/233]	Loss 0.0214 (0.0873)	
training:	Epoch: [28][150/233]	Loss 0.1294 (0.0875)	
training:	Epoch: [28][151/233]	Loss 0.0212 (0.0871)	
training:	Epoch: [28][152/233]	Loss 0.0517 (0.0869)	
training:	Epoch: [28][153/233]	Loss 0.3692 (0.0887)	
training:	Epoch: [28][154/233]	Loss 0.0246 (0.0883)	
training:	Epoch: [28][155/233]	Loss 0.2005 (0.0890)	
training:	Epoch: [28][156/233]	Loss 0.2551 (0.0901)	
training:	Epoch: [28][157/233]	Loss 0.0227 (0.0897)	
training:	Epoch: [28][158/233]	Loss 0.0432 (0.0894)	
training:	Epoch: [28][159/233]	Loss 0.1769 (0.0899)	
training:	Epoch: [28][160/233]	Loss 0.0205 (0.0895)	
training:	Epoch: [28][161/233]	Loss 0.0254 (0.0891)	
training:	Epoch: [28][162/233]	Loss 0.0365 (0.0888)	
training:	Epoch: [28][163/233]	Loss 0.1554 (0.0892)	
training:	Epoch: [28][164/233]	Loss 0.1699 (0.0897)	
training:	Epoch: [28][165/233]	Loss 0.0244 (0.0893)	
training:	Epoch: [28][166/233]	Loss 0.0260 (0.0889)	
training:	Epoch: [28][167/233]	Loss 0.1441 (0.0892)	
training:	Epoch: [28][168/233]	Loss 0.1055 (0.0893)	
training:	Epoch: [28][169/233]	Loss 0.1083 (0.0894)	
training:	Epoch: [28][170/233]	Loss 0.2330 (0.0903)	
training:	Epoch: [28][171/233]	Loss 0.0272 (0.0899)	
training:	Epoch: [28][172/233]	Loss 0.1707 (0.0904)	
training:	Epoch: [28][173/233]	Loss 0.0280 (0.0900)	
training:	Epoch: [28][174/233]	Loss 0.0394 (0.0897)	
training:	Epoch: [28][175/233]	Loss 0.0202 (0.0893)	
training:	Epoch: [28][176/233]	Loss 0.1558 (0.0897)	
training:	Epoch: [28][177/233]	Loss 0.0471 (0.0895)	
training:	Epoch: [28][178/233]	Loss 0.0290 (0.0891)	
training:	Epoch: [28][179/233]	Loss 0.0195 (0.0887)	
training:	Epoch: [28][180/233]	Loss 0.0526 (0.0885)	
training:	Epoch: [28][181/233]	Loss 0.0222 (0.0882)	
training:	Epoch: [28][182/233]	Loss 0.0330 (0.0879)	
training:	Epoch: [28][183/233]	Loss 0.1641 (0.0883)	
training:	Epoch: [28][184/233]	Loss 0.0276 (0.0880)	
training:	Epoch: [28][185/233]	Loss 0.0686 (0.0878)	
training:	Epoch: [28][186/233]	Loss 0.0594 (0.0877)	
training:	Epoch: [28][187/233]	Loss 0.1346 (0.0879)	
training:	Epoch: [28][188/233]	Loss 0.1006 (0.0880)	
training:	Epoch: [28][189/233]	Loss 0.0226 (0.0877)	
training:	Epoch: [28][190/233]	Loss 0.1766 (0.0881)	
training:	Epoch: [28][191/233]	Loss 0.1726 (0.0886)	
training:	Epoch: [28][192/233]	Loss 0.1155 (0.0887)	
training:	Epoch: [28][193/233]	Loss 0.0312 (0.0884)	
training:	Epoch: [28][194/233]	Loss 0.0378 (0.0882)	
training:	Epoch: [28][195/233]	Loss 0.0211 (0.0878)	
training:	Epoch: [28][196/233]	Loss 0.0335 (0.0875)	
training:	Epoch: [28][197/233]	Loss 0.0792 (0.0875)	
training:	Epoch: [28][198/233]	Loss 0.0145 (0.0871)	
training:	Epoch: [28][199/233]	Loss 0.0320 (0.0868)	
training:	Epoch: [28][200/233]	Loss 0.0843 (0.0868)	
training:	Epoch: [28][201/233]	Loss 0.0169 (0.0865)	
training:	Epoch: [28][202/233]	Loss 0.0221 (0.0862)	
training:	Epoch: [28][203/233]	Loss 0.0415 (0.0859)	
training:	Epoch: [28][204/233]	Loss 0.0565 (0.0858)	
training:	Epoch: [28][205/233]	Loss 0.0258 (0.0855)	
training:	Epoch: [28][206/233]	Loss 0.0280 (0.0852)	
training:	Epoch: [28][207/233]	Loss 0.0228 (0.0849)	
training:	Epoch: [28][208/233]	Loss 0.0330 (0.0847)	
training:	Epoch: [28][209/233]	Loss 0.0478 (0.0845)	
training:	Epoch: [28][210/233]	Loss 0.0585 (0.0844)	
training:	Epoch: [28][211/233]	Loss 0.0208 (0.0841)	
training:	Epoch: [28][212/233]	Loss 0.2349 (0.0848)	
training:	Epoch: [28][213/233]	Loss 0.0260 (0.0845)	
training:	Epoch: [28][214/233]	Loss 0.0191 (0.0842)	
training:	Epoch: [28][215/233]	Loss 0.1543 (0.0845)	
training:	Epoch: [28][216/233]	Loss 0.0321 (0.0843)	
training:	Epoch: [28][217/233]	Loss 0.0364 (0.0841)	
training:	Epoch: [28][218/233]	Loss 0.0665 (0.0840)	
training:	Epoch: [28][219/233]	Loss 0.0452 (0.0838)	
training:	Epoch: [28][220/233]	Loss 0.0212 (0.0835)	
training:	Epoch: [28][221/233]	Loss 0.0598 (0.0834)	
training:	Epoch: [28][222/233]	Loss 0.1706 (0.0838)	
training:	Epoch: [28][223/233]	Loss 0.0186 (0.0835)	
training:	Epoch: [28][224/233]	Loss 0.0450 (0.0834)	
training:	Epoch: [28][225/233]	Loss 0.0266 (0.0831)	
training:	Epoch: [28][226/233]	Loss 0.0150 (0.0828)	
training:	Epoch: [28][227/233]	Loss 0.0463 (0.0826)	
training:	Epoch: [28][228/233]	Loss 0.1665 (0.0830)	
training:	Epoch: [28][229/233]	Loss 0.0414 (0.0828)	
training:	Epoch: [28][230/233]	Loss 0.0230 (0.0826)	
training:	Epoch: [28][231/233]	Loss 0.0252 (0.0823)	
training:	Epoch: [28][232/233]	Loss 0.1560 (0.0826)	
training:	Epoch: [28][233/233]	Loss 0.0282 (0.0824)	
Training:	 Loss: 0.0822

Training:	 ACC: 0.9898 0.9897 0.9882 0.9913
Validation:	 ACC: 0.7919 0.7924 0.8029 0.7809
Validation:	 Best_BACC: 0.8162 0.8154 0.7998 0.8326
Validation:	 Loss: 0.6876
Pretraining:	Epoch 29/200
----------
training:	Epoch: [29][1/233]	Loss 0.0831 (0.0831)	
training:	Epoch: [29][2/233]	Loss 0.0185 (0.0508)	
training:	Epoch: [29][3/233]	Loss 0.0226 (0.0414)	
training:	Epoch: [29][4/233]	Loss 0.0552 (0.0449)	
training:	Epoch: [29][5/233]	Loss 0.0572 (0.0473)	
training:	Epoch: [29][6/233]	Loss 0.0177 (0.0424)	
training:	Epoch: [29][7/233]	Loss 0.1149 (0.0527)	
training:	Epoch: [29][8/233]	Loss 0.0200 (0.0486)	
training:	Epoch: [29][9/233]	Loss 0.1523 (0.0602)	
training:	Epoch: [29][10/233]	Loss 0.0427 (0.0584)	
training:	Epoch: [29][11/233]	Loss 0.0370 (0.0565)	
training:	Epoch: [29][12/233]	Loss 0.0326 (0.0545)	
training:	Epoch: [29][13/233]	Loss 0.0194 (0.0518)	
training:	Epoch: [29][14/233]	Loss 0.0183 (0.0494)	
training:	Epoch: [29][15/233]	Loss 0.0308 (0.0481)	
training:	Epoch: [29][16/233]	Loss 0.0587 (0.0488)	
training:	Epoch: [29][17/233]	Loss 0.1932 (0.0573)	
training:	Epoch: [29][18/233]	Loss 0.0226 (0.0554)	
training:	Epoch: [29][19/233]	Loss 0.0445 (0.0548)	
training:	Epoch: [29][20/233]	Loss 0.0353 (0.0538)	
training:	Epoch: [29][21/233]	Loss 0.0132 (0.0519)	
training:	Epoch: [29][22/233]	Loss 0.0204 (0.0505)	
training:	Epoch: [29][23/233]	Loss 0.0240 (0.0493)	
training:	Epoch: [29][24/233]	Loss 0.0401 (0.0489)	
training:	Epoch: [29][25/233]	Loss 0.2085 (0.0553)	
training:	Epoch: [29][26/233]	Loss 0.0204 (0.0540)	
training:	Epoch: [29][27/233]	Loss 0.0134 (0.0525)	
training:	Epoch: [29][28/233]	Loss 0.0188 (0.0513)	
training:	Epoch: [29][29/233]	Loss 0.4569 (0.0652)	
training:	Epoch: [29][30/233]	Loss 0.2287 (0.0707)	
training:	Epoch: [29][31/233]	Loss 0.0381 (0.0696)	
training:	Epoch: [29][32/233]	Loss 0.1627 (0.0726)	
training:	Epoch: [29][33/233]	Loss 0.0236 (0.0711)	
training:	Epoch: [29][34/233]	Loss 0.0263 (0.0698)	
training:	Epoch: [29][35/233]	Loss 0.0162 (0.0682)	
training:	Epoch: [29][36/233]	Loss 0.1993 (0.0719)	
training:	Epoch: [29][37/233]	Loss 0.0367 (0.0709)	
training:	Epoch: [29][38/233]	Loss 0.1643 (0.0734)	
training:	Epoch: [29][39/233]	Loss 0.0250 (0.0721)	
training:	Epoch: [29][40/233]	Loss 0.0176 (0.0708)	
training:	Epoch: [29][41/233]	Loss 0.0689 (0.0707)	
training:	Epoch: [29][42/233]	Loss 0.1459 (0.0725)	
training:	Epoch: [29][43/233]	Loss 0.0246 (0.0714)	
training:	Epoch: [29][44/233]	Loss 0.0698 (0.0714)	
training:	Epoch: [29][45/233]	Loss 0.1245 (0.0725)	
training:	Epoch: [29][46/233]	Loss 0.1752 (0.0748)	
training:	Epoch: [29][47/233]	Loss 0.0426 (0.0741)	
training:	Epoch: [29][48/233]	Loss 0.0344 (0.0733)	
training:	Epoch: [29][49/233]	Loss 0.0992 (0.0738)	
training:	Epoch: [29][50/233]	Loss 0.1864 (0.0760)	
training:	Epoch: [29][51/233]	Loss 0.0258 (0.0751)	
training:	Epoch: [29][52/233]	Loss 0.0226 (0.0740)	
training:	Epoch: [29][53/233]	Loss 0.0184 (0.0730)	
training:	Epoch: [29][54/233]	Loss 0.0996 (0.0735)	
training:	Epoch: [29][55/233]	Loss 0.1765 (0.0754)	
training:	Epoch: [29][56/233]	Loss 0.0158 (0.0743)	
training:	Epoch: [29][57/233]	Loss 0.2540 (0.0774)	
training:	Epoch: [29][58/233]	Loss 0.1847 (0.0793)	
training:	Epoch: [29][59/233]	Loss 0.1841 (0.0811)	
training:	Epoch: [29][60/233]	Loss 0.0248 (0.0801)	
training:	Epoch: [29][61/233]	Loss 0.1830 (0.0818)	
training:	Epoch: [29][62/233]	Loss 0.0199 (0.0808)	
training:	Epoch: [29][63/233]	Loss 0.0241 (0.0799)	
training:	Epoch: [29][64/233]	Loss 0.3005 (0.0834)	
training:	Epoch: [29][65/233]	Loss 0.0734 (0.0832)	
training:	Epoch: [29][66/233]	Loss 0.2317 (0.0855)	
training:	Epoch: [29][67/233]	Loss 0.0286 (0.0846)	
training:	Epoch: [29][68/233]	Loss 0.0977 (0.0848)	
training:	Epoch: [29][69/233]	Loss 0.0384 (0.0841)	
training:	Epoch: [29][70/233]	Loss 0.0675 (0.0839)	
training:	Epoch: [29][71/233]	Loss 0.0187 (0.0830)	
training:	Epoch: [29][72/233]	Loss 0.0425 (0.0824)	
training:	Epoch: [29][73/233]	Loss 0.2858 (0.0852)	
training:	Epoch: [29][74/233]	Loss 0.1881 (0.0866)	
training:	Epoch: [29][75/233]	Loss 0.0331 (0.0859)	
training:	Epoch: [29][76/233]	Loss 0.0992 (0.0861)	
training:	Epoch: [29][77/233]	Loss 0.0880 (0.0861)	
training:	Epoch: [29][78/233]	Loss 0.1959 (0.0875)	
training:	Epoch: [29][79/233]	Loss 0.0139 (0.0866)	
training:	Epoch: [29][80/233]	Loss 0.0246 (0.0858)	
training:	Epoch: [29][81/233]	Loss 0.0287 (0.0851)	
training:	Epoch: [29][82/233]	Loss 0.1222 (0.0855)	
training:	Epoch: [29][83/233]	Loss 0.0185 (0.0847)	
training:	Epoch: [29][84/233]	Loss 0.0208 (0.0840)	
training:	Epoch: [29][85/233]	Loss 0.0197 (0.0832)	
training:	Epoch: [29][86/233]	Loss 0.1417 (0.0839)	
training:	Epoch: [29][87/233]	Loss 0.1503 (0.0847)	
training:	Epoch: [29][88/233]	Loss 0.1438 (0.0853)	
training:	Epoch: [29][89/233]	Loss 0.0295 (0.0847)	
training:	Epoch: [29][90/233]	Loss 0.0441 (0.0842)	
training:	Epoch: [29][91/233]	Loss 0.0816 (0.0842)	
training:	Epoch: [29][92/233]	Loss 0.0260 (0.0836)	
training:	Epoch: [29][93/233]	Loss 0.1358 (0.0841)	
training:	Epoch: [29][94/233]	Loss 0.0158 (0.0834)	
training:	Epoch: [29][95/233]	Loss 0.0216 (0.0828)	
training:	Epoch: [29][96/233]	Loss 0.1023 (0.0830)	
training:	Epoch: [29][97/233]	Loss 0.1489 (0.0837)	
training:	Epoch: [29][98/233]	Loss 0.1899 (0.0847)	
training:	Epoch: [29][99/233]	Loss 0.0199 (0.0841)	
training:	Epoch: [29][100/233]	Loss 0.1822 (0.0851)	
training:	Epoch: [29][101/233]	Loss 0.0237 (0.0845)	
training:	Epoch: [29][102/233]	Loss 0.0264 (0.0839)	
training:	Epoch: [29][103/233]	Loss 0.0138 (0.0832)	
training:	Epoch: [29][104/233]	Loss 0.1761 (0.0841)	
training:	Epoch: [29][105/233]	Loss 0.0150 (0.0834)	
training:	Epoch: [29][106/233]	Loss 0.0320 (0.0830)	
training:	Epoch: [29][107/233]	Loss 0.1229 (0.0833)	
training:	Epoch: [29][108/233]	Loss 0.0248 (0.0828)	
training:	Epoch: [29][109/233]	Loss 0.0150 (0.0822)	
training:	Epoch: [29][110/233]	Loss 0.1967 (0.0832)	
training:	Epoch: [29][111/233]	Loss 0.0194 (0.0826)	
training:	Epoch: [29][112/233]	Loss 0.0303 (0.0822)	
training:	Epoch: [29][113/233]	Loss 0.0408 (0.0818)	
training:	Epoch: [29][114/233]	Loss 0.0672 (0.0817)	
training:	Epoch: [29][115/233]	Loss 0.0690 (0.0816)	
training:	Epoch: [29][116/233]	Loss 0.1190 (0.0819)	
training:	Epoch: [29][117/233]	Loss 0.0182 (0.0813)	
training:	Epoch: [29][118/233]	Loss 0.0199 (0.0808)	
training:	Epoch: [29][119/233]	Loss 0.0695 (0.0807)	
training:	Epoch: [29][120/233]	Loss 0.1493 (0.0813)	
training:	Epoch: [29][121/233]	Loss 0.0188 (0.0808)	
training:	Epoch: [29][122/233]	Loss 0.0159 (0.0802)	
training:	Epoch: [29][123/233]	Loss 0.1119 (0.0805)	
training:	Epoch: [29][124/233]	Loss 0.0682 (0.0804)	
training:	Epoch: [29][125/233]	Loss 0.0260 (0.0800)	
training:	Epoch: [29][126/233]	Loss 0.0293 (0.0796)	
training:	Epoch: [29][127/233]	Loss 0.2229 (0.0807)	
training:	Epoch: [29][128/233]	Loss 0.1344 (0.0811)	
training:	Epoch: [29][129/233]	Loss 0.0165 (0.0806)	
training:	Epoch: [29][130/233]	Loss 0.1408 (0.0811)	
training:	Epoch: [29][131/233]	Loss 0.1056 (0.0813)	
training:	Epoch: [29][132/233]	Loss 0.0398 (0.0809)	
training:	Epoch: [29][133/233]	Loss 0.1473 (0.0814)	
training:	Epoch: [29][134/233]	Loss 0.0350 (0.0811)	
training:	Epoch: [29][135/233]	Loss 0.0267 (0.0807)	
training:	Epoch: [29][136/233]	Loss 0.0328 (0.0803)	
training:	Epoch: [29][137/233]	Loss 0.0418 (0.0801)	
training:	Epoch: [29][138/233]	Loss 0.0577 (0.0799)	
training:	Epoch: [29][139/233]	Loss 0.0180 (0.0795)	
training:	Epoch: [29][140/233]	Loss 0.0342 (0.0791)	
training:	Epoch: [29][141/233]	Loss 0.0153 (0.0787)	
training:	Epoch: [29][142/233]	Loss 0.0691 (0.0786)	
training:	Epoch: [29][143/233]	Loss 0.0674 (0.0785)	
training:	Epoch: [29][144/233]	Loss 0.1012 (0.0787)	
training:	Epoch: [29][145/233]	Loss 0.0388 (0.0784)	
training:	Epoch: [29][146/233]	Loss 0.0125 (0.0780)	
training:	Epoch: [29][147/233]	Loss 0.0151 (0.0775)	
training:	Epoch: [29][148/233]	Loss 0.1188 (0.0778)	
training:	Epoch: [29][149/233]	Loss 0.0290 (0.0775)	
training:	Epoch: [29][150/233]	Loss 0.0431 (0.0773)	
training:	Epoch: [29][151/233]	Loss 0.0206 (0.0769)	
training:	Epoch: [29][152/233]	Loss 0.1531 (0.0774)	
training:	Epoch: [29][153/233]	Loss 0.0654 (0.0773)	
training:	Epoch: [29][154/233]	Loss 0.1531 (0.0778)	
training:	Epoch: [29][155/233]	Loss 0.1671 (0.0784)	
training:	Epoch: [29][156/233]	Loss 0.2082 (0.0792)	
training:	Epoch: [29][157/233]	Loss 0.0269 (0.0789)	
training:	Epoch: [29][158/233]	Loss 0.0178 (0.0785)	
training:	Epoch: [29][159/233]	Loss 0.1298 (0.0788)	
training:	Epoch: [29][160/233]	Loss 0.0212 (0.0785)	
training:	Epoch: [29][161/233]	Loss 0.0204 (0.0781)	
training:	Epoch: [29][162/233]	Loss 0.0186 (0.0777)	
training:	Epoch: [29][163/233]	Loss 0.0141 (0.0773)	
training:	Epoch: [29][164/233]	Loss 0.0189 (0.0770)	
training:	Epoch: [29][165/233]	Loss 0.0443 (0.0768)	
training:	Epoch: [29][166/233]	Loss 0.1371 (0.0771)	
training:	Epoch: [29][167/233]	Loss 0.0529 (0.0770)	
training:	Epoch: [29][168/233]	Loss 0.0185 (0.0766)	
training:	Epoch: [29][169/233]	Loss 0.1450 (0.0771)	
training:	Epoch: [29][170/233]	Loss 0.0356 (0.0768)	
training:	Epoch: [29][171/233]	Loss 0.0302 (0.0765)	
training:	Epoch: [29][172/233]	Loss 0.3734 (0.0783)	
training:	Epoch: [29][173/233]	Loss 0.1563 (0.0787)	
training:	Epoch: [29][174/233]	Loss 0.0187 (0.0784)	
training:	Epoch: [29][175/233]	Loss 0.0207 (0.0780)	
training:	Epoch: [29][176/233]	Loss 0.0190 (0.0777)	
training:	Epoch: [29][177/233]	Loss 0.0654 (0.0776)	
training:	Epoch: [29][178/233]	Loss 0.0505 (0.0775)	
training:	Epoch: [29][179/233]	Loss 0.0681 (0.0774)	
training:	Epoch: [29][180/233]	Loss 0.0304 (0.0772)	
training:	Epoch: [29][181/233]	Loss 0.1682 (0.0777)	
training:	Epoch: [29][182/233]	Loss 0.0854 (0.0777)	
training:	Epoch: [29][183/233]	Loss 0.0545 (0.0776)	
training:	Epoch: [29][184/233]	Loss 0.0597 (0.0775)	
training:	Epoch: [29][185/233]	Loss 0.1218 (0.0777)	
training:	Epoch: [29][186/233]	Loss 0.0247 (0.0774)	
training:	Epoch: [29][187/233]	Loss 0.0662 (0.0774)	
training:	Epoch: [29][188/233]	Loss 0.0988 (0.0775)	
training:	Epoch: [29][189/233]	Loss 0.1462 (0.0779)	
training:	Epoch: [29][190/233]	Loss 0.0158 (0.0775)	
training:	Epoch: [29][191/233]	Loss 0.0680 (0.0775)	
training:	Epoch: [29][192/233]	Loss 0.0145 (0.0772)	
training:	Epoch: [29][193/233]	Loss 0.0314 (0.0769)	
training:	Epoch: [29][194/233]	Loss 0.0258 (0.0767)	
training:	Epoch: [29][195/233]	Loss 0.0389 (0.0765)	
training:	Epoch: [29][196/233]	Loss 0.0206 (0.0762)	
training:	Epoch: [29][197/233]	Loss 0.0246 (0.0759)	
training:	Epoch: [29][198/233]	Loss 0.0360 (0.0757)	
training:	Epoch: [29][199/233]	Loss 0.0241 (0.0755)	
training:	Epoch: [29][200/233]	Loss 0.3592 (0.0769)	
training:	Epoch: [29][201/233]	Loss 0.1504 (0.0772)	
training:	Epoch: [29][202/233]	Loss 0.0151 (0.0769)	
training:	Epoch: [29][203/233]	Loss 0.1282 (0.0772)	
training:	Epoch: [29][204/233]	Loss 0.0283 (0.0769)	
training:	Epoch: [29][205/233]	Loss 0.0231 (0.0767)	
training:	Epoch: [29][206/233]	Loss 0.1777 (0.0772)	
training:	Epoch: [29][207/233]	Loss 0.0726 (0.0771)	
training:	Epoch: [29][208/233]	Loss 0.1009 (0.0773)	
training:	Epoch: [29][209/233]	Loss 0.0928 (0.0773)	
training:	Epoch: [29][210/233]	Loss 0.0158 (0.0770)	
training:	Epoch: [29][211/233]	Loss 0.0598 (0.0770)	
training:	Epoch: [29][212/233]	Loss 0.3608 (0.0783)	
training:	Epoch: [29][213/233]	Loss 0.0228 (0.0780)	
training:	Epoch: [29][214/233]	Loss 0.0225 (0.0778)	
training:	Epoch: [29][215/233]	Loss 0.0220 (0.0775)	
training:	Epoch: [29][216/233]	Loss 0.0146 (0.0772)	
training:	Epoch: [29][217/233]	Loss 0.0227 (0.0770)	
training:	Epoch: [29][218/233]	Loss 0.0301 (0.0768)	
training:	Epoch: [29][219/233]	Loss 0.0126 (0.0765)	
training:	Epoch: [29][220/233]	Loss 0.1655 (0.0769)	
training:	Epoch: [29][221/233]	Loss 0.0154 (0.0766)	
training:	Epoch: [29][222/233]	Loss 0.2769 (0.0775)	
training:	Epoch: [29][223/233]	Loss 0.0177 (0.0772)	
training:	Epoch: [29][224/233]	Loss 0.1174 (0.0774)	
training:	Epoch: [29][225/233]	Loss 0.0514 (0.0773)	
training:	Epoch: [29][226/233]	Loss 0.0225 (0.0771)	
training:	Epoch: [29][227/233]	Loss 0.1359 (0.0773)	
training:	Epoch: [29][228/233]	Loss 0.1030 (0.0774)	
training:	Epoch: [29][229/233]	Loss 0.2518 (0.0782)	
training:	Epoch: [29][230/233]	Loss 0.0255 (0.0780)	
training:	Epoch: [29][231/233]	Loss 0.0565 (0.0779)	
training:	Epoch: [29][232/233]	Loss 0.0260 (0.0776)	
training:	Epoch: [29][233/233]	Loss 0.0525 (0.0775)	
Training:	 Loss: 0.0774

Training:	 ACC: 0.9919 0.9918 0.9895 0.9944
Validation:	 ACC: 0.7926 0.7929 0.7998 0.7854
Validation:	 Best_BACC: 0.8162 0.8154 0.7998 0.8326
Validation:	 Loss: 0.7076
Pretraining:	Epoch 30/200
----------
training:	Epoch: [30][1/233]	Loss 0.0190 (0.0190)	
training:	Epoch: [30][2/233]	Loss 0.2419 (0.1305)	
training:	Epoch: [30][3/233]	Loss 0.0359 (0.0989)	
training:	Epoch: [30][4/233]	Loss 0.0443 (0.0853)	
training:	Epoch: [30][5/233]	Loss 0.0212 (0.0725)	
training:	Epoch: [30][6/233]	Loss 0.1030 (0.0775)	
training:	Epoch: [30][7/233]	Loss 0.2720 (0.1053)	
training:	Epoch: [30][8/233]	Loss 0.1682 (0.1132)	
training:	Epoch: [30][9/233]	Loss 0.0346 (0.1045)	
training:	Epoch: [30][10/233]	Loss 0.0161 (0.0956)	
training:	Epoch: [30][11/233]	Loss 0.0569 (0.0921)	
training:	Epoch: [30][12/233]	Loss 0.1854 (0.0999)	
training:	Epoch: [30][13/233]	Loss 0.0137 (0.0933)	
training:	Epoch: [30][14/233]	Loss 0.0320 (0.0889)	
training:	Epoch: [30][15/233]	Loss 0.0149 (0.0839)	
training:	Epoch: [30][16/233]	Loss 0.0684 (0.0830)	
training:	Epoch: [30][17/233]	Loss 0.1431 (0.0865)	
training:	Epoch: [30][18/233]	Loss 0.1433 (0.0897)	
training:	Epoch: [30][19/233]	Loss 0.2147 (0.0962)	
training:	Epoch: [30][20/233]	Loss 0.0315 (0.0930)	
training:	Epoch: [30][21/233]	Loss 0.1026 (0.0935)	
training:	Epoch: [30][22/233]	Loss 0.0141 (0.0899)	
training:	Epoch: [30][23/233]	Loss 0.0752 (0.0892)	
training:	Epoch: [30][24/233]	Loss 0.0147 (0.0861)	
training:	Epoch: [30][25/233]	Loss 0.1620 (0.0892)	
training:	Epoch: [30][26/233]	Loss 0.0151 (0.0863)	
training:	Epoch: [30][27/233]	Loss 0.0221 (0.0839)	
training:	Epoch: [30][28/233]	Loss 0.1449 (0.0861)	
training:	Epoch: [30][29/233]	Loss 0.0598 (0.0852)	
training:	Epoch: [30][30/233]	Loss 0.1371 (0.0869)	
training:	Epoch: [30][31/233]	Loss 0.0256 (0.0849)	
training:	Epoch: [30][32/233]	Loss 0.0128 (0.0827)	
training:	Epoch: [30][33/233]	Loss 0.0335 (0.0812)	
training:	Epoch: [30][34/233]	Loss 0.1017 (0.0818)	
training:	Epoch: [30][35/233]	Loss 0.0286 (0.0803)	
training:	Epoch: [30][36/233]	Loss 0.0180 (0.0786)	
training:	Epoch: [30][37/233]	Loss 0.0319 (0.0773)	
training:	Epoch: [30][38/233]	Loss 0.3217 (0.0837)	
training:	Epoch: [30][39/233]	Loss 0.0348 (0.0825)	
training:	Epoch: [30][40/233]	Loss 0.0701 (0.0822)	
training:	Epoch: [30][41/233]	Loss 0.0246 (0.0808)	
training:	Epoch: [30][42/233]	Loss 0.0552 (0.0801)	
training:	Epoch: [30][43/233]	Loss 0.0143 (0.0786)	
training:	Epoch: [30][44/233]	Loss 0.1126 (0.0794)	
training:	Epoch: [30][45/233]	Loss 0.1002 (0.0799)	
training:	Epoch: [30][46/233]	Loss 0.0235 (0.0786)	
training:	Epoch: [30][47/233]	Loss 0.0174 (0.0773)	
training:	Epoch: [30][48/233]	Loss 0.0322 (0.0764)	
training:	Epoch: [30][49/233]	Loss 0.3364 (0.0817)	
training:	Epoch: [30][50/233]	Loss 0.0558 (0.0812)	
training:	Epoch: [30][51/233]	Loss 0.1176 (0.0819)	
training:	Epoch: [30][52/233]	Loss 0.0416 (0.0811)	
training:	Epoch: [30][53/233]	Loss 0.0768 (0.0810)	
training:	Epoch: [30][54/233]	Loss 0.1626 (0.0825)	
training:	Epoch: [30][55/233]	Loss 0.0451 (0.0819)	
training:	Epoch: [30][56/233]	Loss 0.0133 (0.0806)	
training:	Epoch: [30][57/233]	Loss 0.0261 (0.0797)	
training:	Epoch: [30][58/233]	Loss 0.0203 (0.0787)	
training:	Epoch: [30][59/233]	Loss 0.1792 (0.0804)	
training:	Epoch: [30][60/233]	Loss 0.0620 (0.0801)	
training:	Epoch: [30][61/233]	Loss 0.1491 (0.0812)	
training:	Epoch: [30][62/233]	Loss 0.0248 (0.0803)	
training:	Epoch: [30][63/233]	Loss 0.0386 (0.0796)	
training:	Epoch: [30][64/233]	Loss 0.2884 (0.0829)	
training:	Epoch: [30][65/233]	Loss 0.0146 (0.0818)	
training:	Epoch: [30][66/233]	Loss 0.0201 (0.0809)	
training:	Epoch: [30][67/233]	Loss 0.0201 (0.0800)	
training:	Epoch: [30][68/233]	Loss 0.0469 (0.0795)	
training:	Epoch: [30][69/233]	Loss 0.2815 (0.0824)	
training:	Epoch: [30][70/233]	Loss 0.0332 (0.0817)	
training:	Epoch: [30][71/233]	Loss 0.0884 (0.0818)	
training:	Epoch: [30][72/233]	Loss 0.0197 (0.0810)	
training:	Epoch: [30][73/233]	Loss 0.0191 (0.0801)	
training:	Epoch: [30][74/233]	Loss 0.0266 (0.0794)	
training:	Epoch: [30][75/233]	Loss 0.0208 (0.0786)	
training:	Epoch: [30][76/233]	Loss 0.0159 (0.0778)	
training:	Epoch: [30][77/233]	Loss 0.1021 (0.0781)	
training:	Epoch: [30][78/233]	Loss 0.0398 (0.0776)	
training:	Epoch: [30][79/233]	Loss 0.0205 (0.0769)	
training:	Epoch: [30][80/233]	Loss 0.0153 (0.0761)	
training:	Epoch: [30][81/233]	Loss 0.0226 (0.0754)	
training:	Epoch: [30][82/233]	Loss 0.1398 (0.0762)	
training:	Epoch: [30][83/233]	Loss 0.1077 (0.0766)	
training:	Epoch: [30][84/233]	Loss 0.0224 (0.0760)	
training:	Epoch: [30][85/233]	Loss 0.0512 (0.0757)	
training:	Epoch: [30][86/233]	Loss 0.1534 (0.0766)	
training:	Epoch: [30][87/233]	Loss 0.0104 (0.0758)	
training:	Epoch: [30][88/233]	Loss 0.0206 (0.0752)	
training:	Epoch: [30][89/233]	Loss 0.0789 (0.0752)	
training:	Epoch: [30][90/233]	Loss 0.0352 (0.0748)	
training:	Epoch: [30][91/233]	Loss 0.1421 (0.0755)	
training:	Epoch: [30][92/233]	Loss 0.0358 (0.0751)	
training:	Epoch: [30][93/233]	Loss 0.0314 (0.0746)	
training:	Epoch: [30][94/233]	Loss 0.1534 (0.0755)	
training:	Epoch: [30][95/233]	Loss 0.0147 (0.0748)	
training:	Epoch: [30][96/233]	Loss 0.0130 (0.0742)	
training:	Epoch: [30][97/233]	Loss 0.0192 (0.0736)	
training:	Epoch: [30][98/233]	Loss 0.1364 (0.0743)	
training:	Epoch: [30][99/233]	Loss 0.0356 (0.0739)	
training:	Epoch: [30][100/233]	Loss 0.0424 (0.0735)	
training:	Epoch: [30][101/233]	Loss 0.1243 (0.0741)	
training:	Epoch: [30][102/233]	Loss 0.0138 (0.0735)	
training:	Epoch: [30][103/233]	Loss 0.0123 (0.0729)	
training:	Epoch: [30][104/233]	Loss 0.1561 (0.0737)	
training:	Epoch: [30][105/233]	Loss 0.1654 (0.0745)	
training:	Epoch: [30][106/233]	Loss 0.0319 (0.0741)	
training:	Epoch: [30][107/233]	Loss 0.1008 (0.0744)	
training:	Epoch: [30][108/233]	Loss 0.0921 (0.0746)	
training:	Epoch: [30][109/233]	Loss 0.0678 (0.0745)	
training:	Epoch: [30][110/233]	Loss 0.1699 (0.0754)	
training:	Epoch: [30][111/233]	Loss 0.0162 (0.0748)	
training:	Epoch: [30][112/233]	Loss 0.1784 (0.0757)	
training:	Epoch: [30][113/233]	Loss 0.0714 (0.0757)	
training:	Epoch: [30][114/233]	Loss 0.1219 (0.0761)	
training:	Epoch: [30][115/233]	Loss 0.0248 (0.0757)	
training:	Epoch: [30][116/233]	Loss 0.0198 (0.0752)	
training:	Epoch: [30][117/233]	Loss 0.0247 (0.0748)	
training:	Epoch: [30][118/233]	Loss 0.0236 (0.0743)	
training:	Epoch: [30][119/233]	Loss 0.0173 (0.0738)	
training:	Epoch: [30][120/233]	Loss 0.0780 (0.0739)	
training:	Epoch: [30][121/233]	Loss 0.0160 (0.0734)	
training:	Epoch: [30][122/233]	Loss 0.0329 (0.0731)	
training:	Epoch: [30][123/233]	Loss 0.0353 (0.0728)	
training:	Epoch: [30][124/233]	Loss 0.0919 (0.0729)	
training:	Epoch: [30][125/233]	Loss 0.1590 (0.0736)	
training:	Epoch: [30][126/233]	Loss 0.0149 (0.0731)	
training:	Epoch: [30][127/233]	Loss 0.2186 (0.0743)	
training:	Epoch: [30][128/233]	Loss 0.0176 (0.0738)	
training:	Epoch: [30][129/233]	Loss 0.0232 (0.0734)	
training:	Epoch: [30][130/233]	Loss 0.0184 (0.0730)	
training:	Epoch: [30][131/233]	Loss 0.0142 (0.0726)	
training:	Epoch: [30][132/233]	Loss 0.0150 (0.0721)	
training:	Epoch: [30][133/233]	Loss 0.0307 (0.0718)	
training:	Epoch: [30][134/233]	Loss 0.0866 (0.0719)	
training:	Epoch: [30][135/233]	Loss 0.0320 (0.0716)	
training:	Epoch: [30][136/233]	Loss 0.2129 (0.0727)	
training:	Epoch: [30][137/233]	Loss 0.0402 (0.0724)	
training:	Epoch: [30][138/233]	Loss 0.0120 (0.0720)	
training:	Epoch: [30][139/233]	Loss 0.0240 (0.0717)	
training:	Epoch: [30][140/233]	Loss 0.0185 (0.0713)	
training:	Epoch: [30][141/233]	Loss 0.0268 (0.0710)	
training:	Epoch: [30][142/233]	Loss 0.2207 (0.0720)	
training:	Epoch: [30][143/233]	Loss 0.0157 (0.0716)	
training:	Epoch: [30][144/233]	Loss 0.0173 (0.0712)	
training:	Epoch: [30][145/233]	Loss 0.0167 (0.0709)	
training:	Epoch: [30][146/233]	Loss 0.0288 (0.0706)	
training:	Epoch: [30][147/233]	Loss 0.0226 (0.0703)	
training:	Epoch: [30][148/233]	Loss 0.0219 (0.0699)	
training:	Epoch: [30][149/233]	Loss 0.0163 (0.0696)	
training:	Epoch: [30][150/233]	Loss 0.0523 (0.0695)	
training:	Epoch: [30][151/233]	Loss 0.0369 (0.0692)	
training:	Epoch: [30][152/233]	Loss 0.1979 (0.0701)	
training:	Epoch: [30][153/233]	Loss 0.0208 (0.0698)	
training:	Epoch: [30][154/233]	Loss 0.0205 (0.0694)	
training:	Epoch: [30][155/233]	Loss 0.0795 (0.0695)	
training:	Epoch: [30][156/233]	Loss 0.0248 (0.0692)	
training:	Epoch: [30][157/233]	Loss 0.0305 (0.0690)	
training:	Epoch: [30][158/233]	Loss 0.0156 (0.0686)	
training:	Epoch: [30][159/233]	Loss 0.0375 (0.0684)	
training:	Epoch: [30][160/233]	Loss 0.0251 (0.0682)	
training:	Epoch: [30][161/233]	Loss 0.1418 (0.0686)	
training:	Epoch: [30][162/233]	Loss 0.1272 (0.0690)	
training:	Epoch: [30][163/233]	Loss 0.0224 (0.0687)	
training:	Epoch: [30][164/233]	Loss 0.1807 (0.0694)	
training:	Epoch: [30][165/233]	Loss 0.0155 (0.0691)	
training:	Epoch: [30][166/233]	Loss 0.0153 (0.0687)	
training:	Epoch: [30][167/233]	Loss 0.1592 (0.0693)	
training:	Epoch: [30][168/233]	Loss 0.1175 (0.0696)	
training:	Epoch: [30][169/233]	Loss 0.1607 (0.0701)	
training:	Epoch: [30][170/233]	Loss 0.0578 (0.0700)	
training:	Epoch: [30][171/233]	Loss 0.0526 (0.0699)	
training:	Epoch: [30][172/233]	Loss 0.0660 (0.0699)	
training:	Epoch: [30][173/233]	Loss 0.0579 (0.0698)	
training:	Epoch: [30][174/233]	Loss 0.1280 (0.0702)	
training:	Epoch: [30][175/233]	Loss 0.0315 (0.0700)	
training:	Epoch: [30][176/233]	Loss 0.0547 (0.0699)	
training:	Epoch: [30][177/233]	Loss 0.0512 (0.0698)	
training:	Epoch: [30][178/233]	Loss 0.0150 (0.0695)	
training:	Epoch: [30][179/233]	Loss 0.0417 (0.0693)	
training:	Epoch: [30][180/233]	Loss 0.0608 (0.0692)	
training:	Epoch: [30][181/233]	Loss 0.0293 (0.0690)	
training:	Epoch: [30][182/233]	Loss 0.0178 (0.0687)	
training:	Epoch: [30][183/233]	Loss 0.2512 (0.0697)	
training:	Epoch: [30][184/233]	Loss 0.0453 (0.0696)	
training:	Epoch: [30][185/233]	Loss 0.0313 (0.0694)	
training:	Epoch: [30][186/233]	Loss 0.1443 (0.0698)	
training:	Epoch: [30][187/233]	Loss 0.0247 (0.0696)	
training:	Epoch: [30][188/233]	Loss 0.0110 (0.0693)	
training:	Epoch: [30][189/233]	Loss 0.0950 (0.0694)	
training:	Epoch: [30][190/233]	Loss 0.0265 (0.0692)	
training:	Epoch: [30][191/233]	Loss 0.0117 (0.0689)	
training:	Epoch: [30][192/233]	Loss 0.2757 (0.0699)	
training:	Epoch: [30][193/233]	Loss 0.0248 (0.0697)	
training:	Epoch: [30][194/233]	Loss 0.0203 (0.0695)	
training:	Epoch: [30][195/233]	Loss 0.0371 (0.0693)	
training:	Epoch: [30][196/233]	Loss 0.0220 (0.0690)	
training:	Epoch: [30][197/233]	Loss 0.0357 (0.0689)	
training:	Epoch: [30][198/233]	Loss 0.0463 (0.0688)	
training:	Epoch: [30][199/233]	Loss 0.0900 (0.0689)	
training:	Epoch: [30][200/233]	Loss 0.0923 (0.0690)	
training:	Epoch: [30][201/233]	Loss 0.1346 (0.0693)	
training:	Epoch: [30][202/233]	Loss 0.0139 (0.0690)	
training:	Epoch: [30][203/233]	Loss 0.1562 (0.0695)	
training:	Epoch: [30][204/233]	Loss 0.0445 (0.0693)	
training:	Epoch: [30][205/233]	Loss 0.0725 (0.0694)	
training:	Epoch: [30][206/233]	Loss 0.0278 (0.0692)	
training:	Epoch: [30][207/233]	Loss 0.0213 (0.0689)	
training:	Epoch: [30][208/233]	Loss 0.0200 (0.0687)	
training:	Epoch: [30][209/233]	Loss 0.1659 (0.0692)	
training:	Epoch: [30][210/233]	Loss 0.1630 (0.0696)	
training:	Epoch: [30][211/233]	Loss 0.0192 (0.0694)	
training:	Epoch: [30][212/233]	Loss 0.0251 (0.0692)	
training:	Epoch: [30][213/233]	Loss 0.0140 (0.0689)	
training:	Epoch: [30][214/233]	Loss 0.0495 (0.0688)	
training:	Epoch: [30][215/233]	Loss 0.1165 (0.0690)	
training:	Epoch: [30][216/233]	Loss 0.0170 (0.0688)	
training:	Epoch: [30][217/233]	Loss 0.0773 (0.0688)	
training:	Epoch: [30][218/233]	Loss 0.1171 (0.0690)	
training:	Epoch: [30][219/233]	Loss 0.1995 (0.0696)	
training:	Epoch: [30][220/233]	Loss 0.0972 (0.0698)	
training:	Epoch: [30][221/233]	Loss 0.1423 (0.0701)	
training:	Epoch: [30][222/233]	Loss 0.0983 (0.0702)	
training:	Epoch: [30][223/233]	Loss 0.0306 (0.0700)	
training:	Epoch: [30][224/233]	Loss 0.2130 (0.0707)	
training:	Epoch: [30][225/233]	Loss 0.0907 (0.0708)	
training:	Epoch: [30][226/233]	Loss 0.1511 (0.0711)	
training:	Epoch: [30][227/233]	Loss 0.0192 (0.0709)	
training:	Epoch: [30][228/233]	Loss 0.0222 (0.0707)	
training:	Epoch: [30][229/233]	Loss 0.0283 (0.0705)	
training:	Epoch: [30][230/233]	Loss 0.0282 (0.0703)	
training:	Epoch: [30][231/233]	Loss 0.0154 (0.0701)	
training:	Epoch: [30][232/233]	Loss 0.0280 (0.0699)	
training:	Epoch: [30][233/233]	Loss 0.0522 (0.0698)	
Training:	 Loss: 0.0697

Training:	 ACC: 0.9921 0.9920 0.9887 0.9955
Validation:	 ACC: 0.7917 0.7903 0.7610 0.8225
Validation:	 Best_BACC: 0.8162 0.8154 0.7998 0.8326
Validation:	 Loss: 0.7340
Pretraining:	Epoch 31/200
----------
training:	Epoch: [31][1/233]	Loss 0.0155 (0.0155)	
training:	Epoch: [31][2/233]	Loss 0.1505 (0.0830)	
training:	Epoch: [31][3/233]	Loss 0.0214 (0.0625)	
training:	Epoch: [31][4/233]	Loss 0.0955 (0.0707)	
training:	Epoch: [31][5/233]	Loss 0.0859 (0.0738)	
training:	Epoch: [31][6/233]	Loss 0.1362 (0.0842)	
training:	Epoch: [31][7/233]	Loss 0.0185 (0.0748)	
training:	Epoch: [31][8/233]	Loss 0.1770 (0.0876)	
training:	Epoch: [31][9/233]	Loss 0.1138 (0.0905)	
training:	Epoch: [31][10/233]	Loss 0.0130 (0.0827)	
training:	Epoch: [31][11/233]	Loss 0.1484 (0.0887)	
training:	Epoch: [31][12/233]	Loss 0.0615 (0.0864)	
training:	Epoch: [31][13/233]	Loss 0.0215 (0.0815)	
training:	Epoch: [31][14/233]	Loss 0.2642 (0.0945)	
training:	Epoch: [31][15/233]	Loss 0.1528 (0.0984)	
training:	Epoch: [31][16/233]	Loss 0.1438 (0.1012)	
training:	Epoch: [31][17/233]	Loss 0.1644 (0.1049)	
training:	Epoch: [31][18/233]	Loss 0.0179 (0.1001)	
training:	Epoch: [31][19/233]	Loss 0.0197 (0.0959)	
training:	Epoch: [31][20/233]	Loss 0.1781 (0.1000)	
training:	Epoch: [31][21/233]	Loss 0.0264 (0.0965)	
training:	Epoch: [31][22/233]	Loss 0.0212 (0.0931)	
training:	Epoch: [31][23/233]	Loss 0.0512 (0.0912)	
training:	Epoch: [31][24/233]	Loss 0.0118 (0.0879)	
training:	Epoch: [31][25/233]	Loss 0.0173 (0.0851)	
training:	Epoch: [31][26/233]	Loss 0.0286 (0.0829)	
training:	Epoch: [31][27/233]	Loss 0.0205 (0.0806)	
training:	Epoch: [31][28/233]	Loss 0.0133 (0.0782)	
training:	Epoch: [31][29/233]	Loss 0.0522 (0.0773)	
training:	Epoch: [31][30/233]	Loss 0.0211 (0.0754)	
training:	Epoch: [31][31/233]	Loss 0.1516 (0.0779)	
training:	Epoch: [31][32/233]	Loss 0.0159 (0.0760)	
training:	Epoch: [31][33/233]	Loss 0.0198 (0.0743)	
training:	Epoch: [31][34/233]	Loss 0.0211 (0.0727)	
training:	Epoch: [31][35/233]	Loss 0.0287 (0.0714)	
training:	Epoch: [31][36/233]	Loss 0.0408 (0.0706)	
training:	Epoch: [31][37/233]	Loss 0.1500 (0.0727)	
training:	Epoch: [31][38/233]	Loss 0.1288 (0.0742)	
training:	Epoch: [31][39/233]	Loss 0.0159 (0.0727)	
training:	Epoch: [31][40/233]	Loss 0.0146 (0.0713)	
training:	Epoch: [31][41/233]	Loss 0.0156 (0.0699)	
training:	Epoch: [31][42/233]	Loss 0.1094 (0.0708)	
training:	Epoch: [31][43/233]	Loss 0.1972 (0.0738)	
training:	Epoch: [31][44/233]	Loss 0.0936 (0.0742)	
training:	Epoch: [31][45/233]	Loss 0.0122 (0.0729)	
training:	Epoch: [31][46/233]	Loss 0.1696 (0.0750)	
training:	Epoch: [31][47/233]	Loss 0.0129 (0.0736)	
training:	Epoch: [31][48/233]	Loss 0.0119 (0.0724)	
training:	Epoch: [31][49/233]	Loss 0.0528 (0.0720)	
training:	Epoch: [31][50/233]	Loss 0.0220 (0.0710)	
training:	Epoch: [31][51/233]	Loss 0.0161 (0.0699)	
training:	Epoch: [31][52/233]	Loss 0.1721 (0.0718)	
training:	Epoch: [31][53/233]	Loss 0.0138 (0.0708)	
training:	Epoch: [31][54/233]	Loss 0.0822 (0.0710)	
training:	Epoch: [31][55/233]	Loss 0.0802 (0.0711)	
training:	Epoch: [31][56/233]	Loss 0.1364 (0.0723)	
training:	Epoch: [31][57/233]	Loss 0.0159 (0.0713)	
training:	Epoch: [31][58/233]	Loss 0.0176 (0.0704)	
training:	Epoch: [31][59/233]	Loss 0.0238 (0.0696)	
training:	Epoch: [31][60/233]	Loss 0.0220 (0.0688)	
training:	Epoch: [31][61/233]	Loss 0.0125 (0.0679)	
training:	Epoch: [31][62/233]	Loss 0.1437 (0.0691)	
training:	Epoch: [31][63/233]	Loss 0.1136 (0.0698)	
training:	Epoch: [31][64/233]	Loss 0.0126 (0.0689)	
training:	Epoch: [31][65/233]	Loss 0.1649 (0.0704)	
training:	Epoch: [31][66/233]	Loss 0.1269 (0.0712)	
training:	Epoch: [31][67/233]	Loss 0.0576 (0.0710)	
training:	Epoch: [31][68/233]	Loss 0.0108 (0.0702)	
training:	Epoch: [31][69/233]	Loss 0.0679 (0.0701)	
training:	Epoch: [31][70/233]	Loss 0.2492 (0.0727)	
training:	Epoch: [31][71/233]	Loss 0.1744 (0.0741)	
training:	Epoch: [31][72/233]	Loss 0.0130 (0.0733)	
training:	Epoch: [31][73/233]	Loss 0.1736 (0.0746)	
training:	Epoch: [31][74/233]	Loss 0.1271 (0.0753)	
training:	Epoch: [31][75/233]	Loss 0.0530 (0.0751)	
training:	Epoch: [31][76/233]	Loss 0.0111 (0.0742)	
training:	Epoch: [31][77/233]	Loss 0.0159 (0.0735)	
training:	Epoch: [31][78/233]	Loss 0.1515 (0.0745)	
training:	Epoch: [31][79/233]	Loss 0.0371 (0.0740)	
training:	Epoch: [31][80/233]	Loss 0.0151 (0.0732)	
training:	Epoch: [31][81/233]	Loss 0.1830 (0.0746)	
training:	Epoch: [31][82/233]	Loss 0.0649 (0.0745)	
training:	Epoch: [31][83/233]	Loss 0.1599 (0.0755)	
training:	Epoch: [31][84/233]	Loss 0.0128 (0.0748)	
training:	Epoch: [31][85/233]	Loss 0.1706 (0.0759)	
training:	Epoch: [31][86/233]	Loss 0.0239 (0.0753)	
training:	Epoch: [31][87/233]	Loss 0.0204 (0.0747)	
training:	Epoch: [31][88/233]	Loss 0.1442 (0.0754)	
training:	Epoch: [31][89/233]	Loss 0.0611 (0.0753)	
training:	Epoch: [31][90/233]	Loss 0.1404 (0.0760)	
training:	Epoch: [31][91/233]	Loss 0.0158 (0.0753)	
training:	Epoch: [31][92/233]	Loss 0.0375 (0.0749)	
training:	Epoch: [31][93/233]	Loss 0.0197 (0.0743)	
training:	Epoch: [31][94/233]	Loss 0.1554 (0.0752)	
training:	Epoch: [31][95/233]	Loss 0.1557 (0.0761)	
training:	Epoch: [31][96/233]	Loss 0.0160 (0.0754)	
training:	Epoch: [31][97/233]	Loss 0.0165 (0.0748)	
training:	Epoch: [31][98/233]	Loss 0.0254 (0.0743)	
training:	Epoch: [31][99/233]	Loss 0.0309 (0.0739)	
training:	Epoch: [31][100/233]	Loss 0.0633 (0.0738)	
training:	Epoch: [31][101/233]	Loss 0.2221 (0.0752)	
training:	Epoch: [31][102/233]	Loss 0.0573 (0.0751)	
training:	Epoch: [31][103/233]	Loss 0.0931 (0.0752)	
training:	Epoch: [31][104/233]	Loss 0.0191 (0.0747)	
training:	Epoch: [31][105/233]	Loss 0.0122 (0.0741)	
training:	Epoch: [31][106/233]	Loss 0.1504 (0.0748)	
training:	Epoch: [31][107/233]	Loss 0.0189 (0.0743)	
training:	Epoch: [31][108/233]	Loss 0.0672 (0.0742)	
training:	Epoch: [31][109/233]	Loss 0.1751 (0.0752)	
training:	Epoch: [31][110/233]	Loss 0.0205 (0.0747)	
training:	Epoch: [31][111/233]	Loss 0.0760 (0.0747)	
training:	Epoch: [31][112/233]	Loss 0.0511 (0.0745)	
training:	Epoch: [31][113/233]	Loss 0.1562 (0.0752)	
training:	Epoch: [31][114/233]	Loss 0.0387 (0.0749)	
training:	Epoch: [31][115/233]	Loss 0.0125 (0.0743)	
training:	Epoch: [31][116/233]	Loss 0.0124 (0.0738)	
training:	Epoch: [31][117/233]	Loss 0.0729 (0.0738)	
training:	Epoch: [31][118/233]	Loss 0.1002 (0.0740)	
training:	Epoch: [31][119/233]	Loss 0.0323 (0.0737)	
training:	Epoch: [31][120/233]	Loss 0.0124 (0.0732)	
training:	Epoch: [31][121/233]	Loss 0.0195 (0.0727)	
training:	Epoch: [31][122/233]	Loss 0.0210 (0.0723)	
training:	Epoch: [31][123/233]	Loss 0.0148 (0.0718)	
training:	Epoch: [31][124/233]	Loss 0.1425 (0.0724)	
training:	Epoch: [31][125/233]	Loss 0.0547 (0.0722)	
training:	Epoch: [31][126/233]	Loss 0.0126 (0.0718)	
training:	Epoch: [31][127/233]	Loss 0.0157 (0.0713)	
training:	Epoch: [31][128/233]	Loss 0.0164 (0.0709)	
training:	Epoch: [31][129/233]	Loss 0.0301 (0.0706)	
training:	Epoch: [31][130/233]	Loss 0.0977 (0.0708)	
training:	Epoch: [31][131/233]	Loss 0.1652 (0.0715)	
training:	Epoch: [31][132/233]	Loss 0.0349 (0.0712)	
training:	Epoch: [31][133/233]	Loss 0.0122 (0.0708)	
training:	Epoch: [31][134/233]	Loss 0.0914 (0.0709)	
training:	Epoch: [31][135/233]	Loss 0.0881 (0.0711)	
training:	Epoch: [31][136/233]	Loss 0.1489 (0.0716)	
training:	Epoch: [31][137/233]	Loss 0.1165 (0.0720)	
training:	Epoch: [31][138/233]	Loss 0.0185 (0.0716)	
training:	Epoch: [31][139/233]	Loss 0.0301 (0.0713)	
training:	Epoch: [31][140/233]	Loss 0.0341 (0.0710)	
training:	Epoch: [31][141/233]	Loss 0.0158 (0.0706)	
training:	Epoch: [31][142/233]	Loss 0.0288 (0.0703)	
training:	Epoch: [31][143/233]	Loss 0.1283 (0.0707)	
training:	Epoch: [31][144/233]	Loss 0.0494 (0.0706)	
training:	Epoch: [31][145/233]	Loss 0.1001 (0.0708)	
training:	Epoch: [31][146/233]	Loss 0.0151 (0.0704)	
training:	Epoch: [31][147/233]	Loss 0.0304 (0.0701)	
training:	Epoch: [31][148/233]	Loss 0.1626 (0.0708)	
training:	Epoch: [31][149/233]	Loss 0.0338 (0.0705)	
training:	Epoch: [31][150/233]	Loss 0.0308 (0.0703)	
training:	Epoch: [31][151/233]	Loss 0.0340 (0.0700)	
training:	Epoch: [31][152/233]	Loss 0.0350 (0.0698)	
training:	Epoch: [31][153/233]	Loss 0.0183 (0.0694)	
training:	Epoch: [31][154/233]	Loss 0.2218 (0.0704)	
training:	Epoch: [31][155/233]	Loss 0.1956 (0.0712)	
training:	Epoch: [31][156/233]	Loss 0.0751 (0.0713)	
training:	Epoch: [31][157/233]	Loss 0.0584 (0.0712)	
training:	Epoch: [31][158/233]	Loss 0.0508 (0.0711)	
training:	Epoch: [31][159/233]	Loss 0.0325 (0.0708)	
training:	Epoch: [31][160/233]	Loss 0.0157 (0.0705)	
training:	Epoch: [31][161/233]	Loss 0.1732 (0.0711)	
training:	Epoch: [31][162/233]	Loss 0.1550 (0.0716)	
training:	Epoch: [31][163/233]	Loss 0.0100 (0.0713)	
training:	Epoch: [31][164/233]	Loss 0.1718 (0.0719)	
training:	Epoch: [31][165/233]	Loss 0.3031 (0.0733)	
training:	Epoch: [31][166/233]	Loss 0.0143 (0.0729)	
training:	Epoch: [31][167/233]	Loss 0.0253 (0.0726)	
training:	Epoch: [31][168/233]	Loss 0.0267 (0.0724)	
training:	Epoch: [31][169/233]	Loss 0.0188 (0.0720)	
training:	Epoch: [31][170/233]	Loss 0.0460 (0.0719)	
training:	Epoch: [31][171/233]	Loss 0.0238 (0.0716)	
training:	Epoch: [31][172/233]	Loss 0.0419 (0.0714)	
training:	Epoch: [31][173/233]	Loss 0.0119 (0.0711)	
training:	Epoch: [31][174/233]	Loss 0.0217 (0.0708)	
training:	Epoch: [31][175/233]	Loss 0.0206 (0.0705)	
training:	Epoch: [31][176/233]	Loss 0.0241 (0.0702)	
training:	Epoch: [31][177/233]	Loss 0.0279 (0.0700)	
training:	Epoch: [31][178/233]	Loss 0.0230 (0.0697)	
training:	Epoch: [31][179/233]	Loss 0.0153 (0.0694)	
training:	Epoch: [31][180/233]	Loss 0.0192 (0.0692)	
training:	Epoch: [31][181/233]	Loss 0.3651 (0.0708)	
training:	Epoch: [31][182/233]	Loss 0.0199 (0.0705)	
training:	Epoch: [31][183/233]	Loss 0.0168 (0.0702)	
training:	Epoch: [31][184/233]	Loss 0.1809 (0.0708)	
training:	Epoch: [31][185/233]	Loss 0.0306 (0.0706)	
training:	Epoch: [31][186/233]	Loss 0.0314 (0.0704)	
training:	Epoch: [31][187/233]	Loss 0.1322 (0.0707)	
training:	Epoch: [31][188/233]	Loss 0.0180 (0.0704)	
training:	Epoch: [31][189/233]	Loss 0.0145 (0.0702)	
training:	Epoch: [31][190/233]	Loss 0.0902 (0.0703)	
training:	Epoch: [31][191/233]	Loss 0.2063 (0.0710)	
training:	Epoch: [31][192/233]	Loss 0.1702 (0.0715)	
training:	Epoch: [31][193/233]	Loss 0.0165 (0.0712)	
training:	Epoch: [31][194/233]	Loss 0.0806 (0.0712)	
training:	Epoch: [31][195/233]	Loss 0.0345 (0.0711)	
training:	Epoch: [31][196/233]	Loss 0.0317 (0.0709)	
training:	Epoch: [31][197/233]	Loss 0.0157 (0.0706)	
training:	Epoch: [31][198/233]	Loss 0.0148 (0.0703)	
training:	Epoch: [31][199/233]	Loss 0.0159 (0.0700)	
training:	Epoch: [31][200/233]	Loss 0.1380 (0.0704)	
training:	Epoch: [31][201/233]	Loss 0.1524 (0.0708)	
training:	Epoch: [31][202/233]	Loss 0.0487 (0.0707)	
training:	Epoch: [31][203/233]	Loss 0.0323 (0.0705)	
training:	Epoch: [31][204/233]	Loss 0.1191 (0.0707)	
training:	Epoch: [31][205/233]	Loss 0.0330 (0.0705)	
training:	Epoch: [31][206/233]	Loss 0.0599 (0.0705)	
training:	Epoch: [31][207/233]	Loss 0.0338 (0.0703)	
training:	Epoch: [31][208/233]	Loss 0.0176 (0.0700)	
training:	Epoch: [31][209/233]	Loss 0.0799 (0.0701)	
training:	Epoch: [31][210/233]	Loss 0.0374 (0.0699)	
training:	Epoch: [31][211/233]	Loss 0.0156 (0.0697)	
training:	Epoch: [31][212/233]	Loss 0.0334 (0.0695)	
training:	Epoch: [31][213/233]	Loss 0.1290 (0.0698)	
training:	Epoch: [31][214/233]	Loss 0.0137 (0.0695)	
training:	Epoch: [31][215/233]	Loss 0.0207 (0.0693)	
training:	Epoch: [31][216/233]	Loss 0.0140 (0.0690)	
training:	Epoch: [31][217/233]	Loss 0.0145 (0.0688)	
training:	Epoch: [31][218/233]	Loss 0.0153 (0.0685)	
training:	Epoch: [31][219/233]	Loss 0.0352 (0.0684)	
training:	Epoch: [31][220/233]	Loss 0.0168 (0.0682)	
training:	Epoch: [31][221/233]	Loss 0.0123 (0.0679)	
training:	Epoch: [31][222/233]	Loss 0.0195 (0.0677)	
training:	Epoch: [31][223/233]	Loss 0.0442 (0.0676)	
training:	Epoch: [31][224/233]	Loss 0.0105 (0.0673)	
training:	Epoch: [31][225/233]	Loss 0.0345 (0.0672)	
training:	Epoch: [31][226/233]	Loss 0.0291 (0.0670)	
training:	Epoch: [31][227/233]	Loss 0.0169 (0.0668)	
training:	Epoch: [31][228/233]	Loss 0.0606 (0.0668)	
training:	Epoch: [31][229/233]	Loss 0.0161 (0.0665)	
training:	Epoch: [31][230/233]	Loss 0.1548 (0.0669)	
training:	Epoch: [31][231/233]	Loss 0.0242 (0.0667)	
training:	Epoch: [31][232/233]	Loss 0.0624 (0.0667)	
training:	Epoch: [31][233/233]	Loss 0.0297 (0.0666)	
Training:	 Loss: 0.0664

Training:	 ACC: 0.9925 0.9924 0.9895 0.9955
Validation:	 ACC: 0.7884 0.7871 0.7600 0.8169
Validation:	 Best_BACC: 0.8162 0.8154 0.7998 0.8326
Validation:	 Loss: 0.7516
Pretraining:	Epoch 32/200
----------
training:	Epoch: [32][1/233]	Loss 0.0581 (0.0581)	
training:	Epoch: [32][2/233]	Loss 0.0196 (0.0389)	
training:	Epoch: [32][3/233]	Loss 0.0189 (0.0322)	
training:	Epoch: [32][4/233]	Loss 0.1500 (0.0617)	
training:	Epoch: [32][5/233]	Loss 0.0334 (0.0560)	
training:	Epoch: [32][6/233]	Loss 0.1856 (0.0776)	
training:	Epoch: [32][7/233]	Loss 0.0953 (0.0801)	
training:	Epoch: [32][8/233]	Loss 0.1288 (0.0862)	
training:	Epoch: [32][9/233]	Loss 0.3242 (0.1126)	
training:	Epoch: [32][10/233]	Loss 0.0247 (0.1039)	
training:	Epoch: [32][11/233]	Loss 0.0248 (0.0967)	
training:	Epoch: [32][12/233]	Loss 0.0330 (0.0914)	
training:	Epoch: [32][13/233]	Loss 0.0137 (0.0854)	
training:	Epoch: [32][14/233]	Loss 0.0124 (0.0802)	
training:	Epoch: [32][15/233]	Loss 0.0152 (0.0758)	
training:	Epoch: [32][16/233]	Loss 0.0172 (0.0722)	
training:	Epoch: [32][17/233]	Loss 0.0874 (0.0731)	
training:	Epoch: [32][18/233]	Loss 0.0182 (0.0700)	
training:	Epoch: [32][19/233]	Loss 0.0198 (0.0674)	
training:	Epoch: [32][20/233]	Loss 0.0162 (0.0648)	
training:	Epoch: [32][21/233]	Loss 0.1315 (0.0680)	
training:	Epoch: [32][22/233]	Loss 0.0203 (0.0658)	
training:	Epoch: [32][23/233]	Loss 0.0275 (0.0642)	
training:	Epoch: [32][24/233]	Loss 0.0175 (0.0622)	
training:	Epoch: [32][25/233]	Loss 0.2231 (0.0687)	
training:	Epoch: [32][26/233]	Loss 0.0297 (0.0672)	
training:	Epoch: [32][27/233]	Loss 0.0112 (0.0651)	
training:	Epoch: [32][28/233]	Loss 0.0318 (0.0639)	
training:	Epoch: [32][29/233]	Loss 0.1651 (0.0674)	
training:	Epoch: [32][30/233]	Loss 0.0288 (0.0661)	
training:	Epoch: [32][31/233]	Loss 0.0151 (0.0645)	
training:	Epoch: [32][32/233]	Loss 0.0200 (0.0631)	
training:	Epoch: [32][33/233]	Loss 0.0279 (0.0620)	
training:	Epoch: [32][34/233]	Loss 0.1715 (0.0652)	
training:	Epoch: [32][35/233]	Loss 0.1272 (0.0670)	
training:	Epoch: [32][36/233]	Loss 0.0125 (0.0655)	
training:	Epoch: [32][37/233]	Loss 0.0338 (0.0646)	
training:	Epoch: [32][38/233]	Loss 0.1390 (0.0666)	
training:	Epoch: [32][39/233]	Loss 0.0212 (0.0654)	
training:	Epoch: [32][40/233]	Loss 0.1165 (0.0667)	
training:	Epoch: [32][41/233]	Loss 0.1521 (0.0688)	
training:	Epoch: [32][42/233]	Loss 0.0178 (0.0676)	
training:	Epoch: [32][43/233]	Loss 0.0247 (0.0666)	
training:	Epoch: [32][44/233]	Loss 0.0870 (0.0670)	
training:	Epoch: [32][45/233]	Loss 0.0173 (0.0659)	
training:	Epoch: [32][46/233]	Loss 0.1148 (0.0670)	
training:	Epoch: [32][47/233]	Loss 0.0147 (0.0659)	
training:	Epoch: [32][48/233]	Loss 0.0306 (0.0651)	
training:	Epoch: [32][49/233]	Loss 0.0133 (0.0641)	
training:	Epoch: [32][50/233]	Loss 0.0547 (0.0639)	
training:	Epoch: [32][51/233]	Loss 0.2292 (0.0671)	
training:	Epoch: [32][52/233]	Loss 0.1380 (0.0685)	
training:	Epoch: [32][53/233]	Loss 0.0171 (0.0675)	
training:	Epoch: [32][54/233]	Loss 0.2004 (0.0700)	
training:	Epoch: [32][55/233]	Loss 0.1732 (0.0719)	
training:	Epoch: [32][56/233]	Loss 0.0119 (0.0708)	
training:	Epoch: [32][57/233]	Loss 0.1148 (0.0716)	
training:	Epoch: [32][58/233]	Loss 0.1324 (0.0726)	
training:	Epoch: [32][59/233]	Loss 0.2712 (0.0760)	
training:	Epoch: [32][60/233]	Loss 0.0330 (0.0753)	
training:	Epoch: [32][61/233]	Loss 0.0121 (0.0742)	
training:	Epoch: [32][62/233]	Loss 0.0218 (0.0734)	
training:	Epoch: [32][63/233]	Loss 0.0727 (0.0734)	
training:	Epoch: [32][64/233]	Loss 0.0157 (0.0725)	
training:	Epoch: [32][65/233]	Loss 0.3005 (0.0760)	
training:	Epoch: [32][66/233]	Loss 0.3987 (0.0809)	
training:	Epoch: [32][67/233]	Loss 0.0836 (0.0809)	
training:	Epoch: [32][68/233]	Loss 0.0638 (0.0807)	
training:	Epoch: [32][69/233]	Loss 0.0517 (0.0802)	
training:	Epoch: [32][70/233]	Loss 0.0471 (0.0798)	
training:	Epoch: [32][71/233]	Loss 0.0197 (0.0789)	
training:	Epoch: [32][72/233]	Loss 0.0409 (0.0784)	
training:	Epoch: [32][73/233]	Loss 0.1105 (0.0788)	
training:	Epoch: [32][74/233]	Loss 0.1618 (0.0800)	
training:	Epoch: [32][75/233]	Loss 0.0338 (0.0793)	
training:	Epoch: [32][76/233]	Loss 0.1557 (0.0803)	
training:	Epoch: [32][77/233]	Loss 0.0558 (0.0800)	
training:	Epoch: [32][78/233]	Loss 0.0144 (0.0792)	
training:	Epoch: [32][79/233]	Loss 0.1793 (0.0804)	
training:	Epoch: [32][80/233]	Loss 0.1472 (0.0813)	
training:	Epoch: [32][81/233]	Loss 0.0118 (0.0804)	
training:	Epoch: [32][82/233]	Loss 0.0158 (0.0796)	
training:	Epoch: [32][83/233]	Loss 0.0205 (0.0789)	
training:	Epoch: [32][84/233]	Loss 0.0166 (0.0782)	
training:	Epoch: [32][85/233]	Loss 0.0190 (0.0775)	
training:	Epoch: [32][86/233]	Loss 0.0190 (0.0768)	
training:	Epoch: [32][87/233]	Loss 0.0162 (0.0761)	
training:	Epoch: [32][88/233]	Loss 0.0169 (0.0754)	
training:	Epoch: [32][89/233]	Loss 0.0233 (0.0749)	
training:	Epoch: [32][90/233]	Loss 0.0344 (0.0744)	
training:	Epoch: [32][91/233]	Loss 0.1439 (0.0752)	
training:	Epoch: [32][92/233]	Loss 0.0200 (0.0746)	
training:	Epoch: [32][93/233]	Loss 0.0107 (0.0739)	
training:	Epoch: [32][94/233]	Loss 0.1078 (0.0742)	
training:	Epoch: [32][95/233]	Loss 0.0869 (0.0744)	
training:	Epoch: [32][96/233]	Loss 0.0161 (0.0738)	
training:	Epoch: [32][97/233]	Loss 0.1953 (0.0750)	
training:	Epoch: [32][98/233]	Loss 0.0154 (0.0744)	
training:	Epoch: [32][99/233]	Loss 0.0295 (0.0740)	
training:	Epoch: [32][100/233]	Loss 0.0136 (0.0734)	
training:	Epoch: [32][101/233]	Loss 0.0127 (0.0728)	
training:	Epoch: [32][102/233]	Loss 0.2839 (0.0748)	
training:	Epoch: [32][103/233]	Loss 0.0144 (0.0742)	
training:	Epoch: [32][104/233]	Loss 0.0354 (0.0739)	
training:	Epoch: [32][105/233]	Loss 0.0198 (0.0733)	
training:	Epoch: [32][106/233]	Loss 0.0158 (0.0728)	
training:	Epoch: [32][107/233]	Loss 0.0462 (0.0726)	
training:	Epoch: [32][108/233]	Loss 0.0223 (0.0721)	
training:	Epoch: [32][109/233]	Loss 0.0346 (0.0717)	
training:	Epoch: [32][110/233]	Loss 0.0262 (0.0713)	
training:	Epoch: [32][111/233]	Loss 0.0172 (0.0708)	
training:	Epoch: [32][112/233]	Loss 0.0351 (0.0705)	
training:	Epoch: [32][113/233]	Loss 0.0178 (0.0701)	
training:	Epoch: [32][114/233]	Loss 0.0180 (0.0696)	
training:	Epoch: [32][115/233]	Loss 0.0435 (0.0694)	
training:	Epoch: [32][116/233]	Loss 0.0152 (0.0689)	
training:	Epoch: [32][117/233]	Loss 0.0151 (0.0684)	
training:	Epoch: [32][118/233]	Loss 0.0397 (0.0682)	
training:	Epoch: [32][119/233]	Loss 0.0102 (0.0677)	
training:	Epoch: [32][120/233]	Loss 0.3428 (0.0700)	
training:	Epoch: [32][121/233]	Loss 0.1543 (0.0707)	
training:	Epoch: [32][122/233]	Loss 0.0211 (0.0703)	
training:	Epoch: [32][123/233]	Loss 0.0778 (0.0704)	
training:	Epoch: [32][124/233]	Loss 0.1005 (0.0706)	
training:	Epoch: [32][125/233]	Loss 0.0135 (0.0701)	
training:	Epoch: [32][126/233]	Loss 0.0211 (0.0698)	
training:	Epoch: [32][127/233]	Loss 0.0874 (0.0699)	
training:	Epoch: [32][128/233]	Loss 0.1968 (0.0709)	
training:	Epoch: [32][129/233]	Loss 0.0194 (0.0705)	
training:	Epoch: [32][130/233]	Loss 0.0182 (0.0701)	
training:	Epoch: [32][131/233]	Loss 0.1691 (0.0708)	
training:	Epoch: [32][132/233]	Loss 0.0995 (0.0711)	
training:	Epoch: [32][133/233]	Loss 0.0622 (0.0710)	
training:	Epoch: [32][134/233]	Loss 0.1878 (0.0719)	
training:	Epoch: [32][135/233]	Loss 0.0220 (0.0715)	
training:	Epoch: [32][136/233]	Loss 0.0132 (0.0711)	
training:	Epoch: [32][137/233]	Loss 0.0801 (0.0711)	
training:	Epoch: [32][138/233]	Loss 0.0293 (0.0708)	
training:	Epoch: [32][139/233]	Loss 0.0181 (0.0705)	
training:	Epoch: [32][140/233]	Loss 0.0228 (0.0701)	
training:	Epoch: [32][141/233]	Loss 0.0146 (0.0697)	
training:	Epoch: [32][142/233]	Loss 0.0113 (0.0693)	
training:	Epoch: [32][143/233]	Loss 0.0251 (0.0690)	
training:	Epoch: [32][144/233]	Loss 0.1308 (0.0694)	
training:	Epoch: [32][145/233]	Loss 0.1307 (0.0698)	
training:	Epoch: [32][146/233]	Loss 0.1498 (0.0704)	
training:	Epoch: [32][147/233]	Loss 0.0142 (0.0700)	
training:	Epoch: [32][148/233]	Loss 0.0222 (0.0697)	
training:	Epoch: [32][149/233]	Loss 0.0414 (0.0695)	
training:	Epoch: [32][150/233]	Loss 0.0308 (0.0692)	
training:	Epoch: [32][151/233]	Loss 0.0379 (0.0690)	
training:	Epoch: [32][152/233]	Loss 0.0188 (0.0687)	
training:	Epoch: [32][153/233]	Loss 0.0186 (0.0684)	
training:	Epoch: [32][154/233]	Loss 0.1530 (0.0689)	
training:	Epoch: [32][155/233]	Loss 0.0123 (0.0686)	
training:	Epoch: [32][156/233]	Loss 0.0154 (0.0682)	
training:	Epoch: [32][157/233]	Loss 0.0410 (0.0680)	
training:	Epoch: [32][158/233]	Loss 0.0279 (0.0678)	
training:	Epoch: [32][159/233]	Loss 0.0161 (0.0675)	
training:	Epoch: [32][160/233]	Loss 0.0231 (0.0672)	
training:	Epoch: [32][161/233]	Loss 0.0308 (0.0670)	
training:	Epoch: [32][162/233]	Loss 0.1406 (0.0674)	
training:	Epoch: [32][163/233]	Loss 0.0195 (0.0671)	
training:	Epoch: [32][164/233]	Loss 0.0513 (0.0670)	
training:	Epoch: [32][165/233]	Loss 0.0605 (0.0670)	
training:	Epoch: [32][166/233]	Loss 0.0137 (0.0667)	
training:	Epoch: [32][167/233]	Loss 0.0121 (0.0663)	
training:	Epoch: [32][168/233]	Loss 0.1132 (0.0666)	
training:	Epoch: [32][169/233]	Loss 0.0159 (0.0663)	
training:	Epoch: [32][170/233]	Loss 0.0109 (0.0660)	
training:	Epoch: [32][171/233]	Loss 0.0355 (0.0658)	
training:	Epoch: [32][172/233]	Loss 0.1579 (0.0664)	
training:	Epoch: [32][173/233]	Loss 0.0465 (0.0662)	
training:	Epoch: [32][174/233]	Loss 0.0125 (0.0659)	
training:	Epoch: [32][175/233]	Loss 0.0210 (0.0657)	
training:	Epoch: [32][176/233]	Loss 0.0122 (0.0654)	
training:	Epoch: [32][177/233]	Loss 0.0619 (0.0653)	
training:	Epoch: [32][178/233]	Loss 0.0907 (0.0655)	
training:	Epoch: [32][179/233]	Loss 0.0097 (0.0652)	
training:	Epoch: [32][180/233]	Loss 0.0749 (0.0652)	
training:	Epoch: [32][181/233]	Loss 0.0109 (0.0649)	
training:	Epoch: [32][182/233]	Loss 0.0135 (0.0646)	
training:	Epoch: [32][183/233]	Loss 0.1445 (0.0651)	
training:	Epoch: [32][184/233]	Loss 0.0161 (0.0648)	
training:	Epoch: [32][185/233]	Loss 0.0427 (0.0647)	
training:	Epoch: [32][186/233]	Loss 0.0126 (0.0644)	
training:	Epoch: [32][187/233]	Loss 0.2896 (0.0656)	
training:	Epoch: [32][188/233]	Loss 0.1732 (0.0662)	
training:	Epoch: [32][189/233]	Loss 0.0150 (0.0659)	
training:	Epoch: [32][190/233]	Loss 0.0131 (0.0656)	
training:	Epoch: [32][191/233]	Loss 0.0299 (0.0655)	
training:	Epoch: [32][192/233]	Loss 0.0150 (0.0652)	
training:	Epoch: [32][193/233]	Loss 0.0158 (0.0649)	
training:	Epoch: [32][194/233]	Loss 0.0139 (0.0647)	
training:	Epoch: [32][195/233]	Loss 0.0115 (0.0644)	
training:	Epoch: [32][196/233]	Loss 0.0145 (0.0641)	
training:	Epoch: [32][197/233]	Loss 0.0313 (0.0640)	
training:	Epoch: [32][198/233]	Loss 0.0120 (0.0637)	
training:	Epoch: [32][199/233]	Loss 0.0316 (0.0636)	
training:	Epoch: [32][200/233]	Loss 0.0113 (0.0633)	
training:	Epoch: [32][201/233]	Loss 0.0220 (0.0631)	
training:	Epoch: [32][202/233]	Loss 0.0142 (0.0628)	
training:	Epoch: [32][203/233]	Loss 0.1334 (0.0632)	
training:	Epoch: [32][204/233]	Loss 0.1328 (0.0635)	
training:	Epoch: [32][205/233]	Loss 0.1250 (0.0638)	
training:	Epoch: [32][206/233]	Loss 0.0111 (0.0636)	
training:	Epoch: [32][207/233]	Loss 0.1066 (0.0638)	
training:	Epoch: [32][208/233]	Loss 0.1424 (0.0642)	
training:	Epoch: [32][209/233]	Loss 0.2167 (0.0649)	
training:	Epoch: [32][210/233]	Loss 0.0141 (0.0647)	
training:	Epoch: [32][211/233]	Loss 0.0623 (0.0646)	
training:	Epoch: [32][212/233]	Loss 0.0293 (0.0645)	
training:	Epoch: [32][213/233]	Loss 0.0947 (0.0646)	
training:	Epoch: [32][214/233]	Loss 0.0253 (0.0644)	
training:	Epoch: [32][215/233]	Loss 0.2692 (0.0654)	
training:	Epoch: [32][216/233]	Loss 0.0285 (0.0652)	
training:	Epoch: [32][217/233]	Loss 0.1578 (0.0656)	
training:	Epoch: [32][218/233]	Loss 0.0120 (0.0654)	
training:	Epoch: [32][219/233]	Loss 0.1287 (0.0657)	
training:	Epoch: [32][220/233]	Loss 0.0168 (0.0655)	
training:	Epoch: [32][221/233]	Loss 0.0264 (0.0653)	
training:	Epoch: [32][222/233]	Loss 0.1460 (0.0657)	
training:	Epoch: [32][223/233]	Loss 0.0230 (0.0655)	
training:	Epoch: [32][224/233]	Loss 0.1502 (0.0658)	
training:	Epoch: [32][225/233]	Loss 0.0134 (0.0656)	
training:	Epoch: [32][226/233]	Loss 0.0368 (0.0655)	
training:	Epoch: [32][227/233]	Loss 0.0135 (0.0653)	
training:	Epoch: [32][228/233]	Loss 0.0119 (0.0650)	
training:	Epoch: [32][229/233]	Loss 0.0353 (0.0649)	
training:	Epoch: [32][230/233]	Loss 0.0819 (0.0650)	
training:	Epoch: [32][231/233]	Loss 0.0154 (0.0647)	
training:	Epoch: [32][232/233]	Loss 0.0144 (0.0645)	
training:	Epoch: [32][233/233]	Loss 0.0533 (0.0645)	
Training:	 Loss: 0.0643

Training:	 ACC: 0.9931 0.9930 0.9910 0.9952
Validation:	 ACC: 0.7931 0.7929 0.7896 0.7966
Validation:	 Best_BACC: 0.8162 0.8154 0.7998 0.8326
Validation:	 Loss: 0.7506
Pretraining:	Epoch 33/200
----------
training:	Epoch: [33][1/233]	Loss 0.0157 (0.0157)	
training:	Epoch: [33][2/233]	Loss 0.0142 (0.0149)	
training:	Epoch: [33][3/233]	Loss 0.0302 (0.0200)	
training:	Epoch: [33][4/233]	Loss 0.1417 (0.0505)	
training:	Epoch: [33][5/233]	Loss 0.0360 (0.0476)	
training:	Epoch: [33][6/233]	Loss 0.0209 (0.0431)	
training:	Epoch: [33][7/233]	Loss 0.0335 (0.0417)	
training:	Epoch: [33][8/233]	Loss 0.0156 (0.0385)	
training:	Epoch: [33][9/233]	Loss 0.0134 (0.0357)	
training:	Epoch: [33][10/233]	Loss 0.0209 (0.0342)	
training:	Epoch: [33][11/233]	Loss 0.0147 (0.0324)	
training:	Epoch: [33][12/233]	Loss 0.0225 (0.0316)	
training:	Epoch: [33][13/233]	Loss 0.0309 (0.0316)	
training:	Epoch: [33][14/233]	Loss 0.0145 (0.0303)	
training:	Epoch: [33][15/233]	Loss 0.0262 (0.0301)	
training:	Epoch: [33][16/233]	Loss 0.0472 (0.0311)	
training:	Epoch: [33][17/233]	Loss 0.0528 (0.0324)	
training:	Epoch: [33][18/233]	Loss 0.0412 (0.0329)	
training:	Epoch: [33][19/233]	Loss 0.0367 (0.0331)	
training:	Epoch: [33][20/233]	Loss 0.0110 (0.0320)	
training:	Epoch: [33][21/233]	Loss 0.0137 (0.0311)	
training:	Epoch: [33][22/233]	Loss 0.0120 (0.0303)	
training:	Epoch: [33][23/233]	Loss 0.0583 (0.0315)	
training:	Epoch: [33][24/233]	Loss 0.1507 (0.0364)	
training:	Epoch: [33][25/233]	Loss 0.0212 (0.0358)	
training:	Epoch: [33][26/233]	Loss 0.0163 (0.0351)	
training:	Epoch: [33][27/233]	Loss 0.0110 (0.0342)	
training:	Epoch: [33][28/233]	Loss 0.0293 (0.0340)	
training:	Epoch: [33][29/233]	Loss 0.0131 (0.0333)	
training:	Epoch: [33][30/233]	Loss 0.0585 (0.0341)	
training:	Epoch: [33][31/233]	Loss 0.1373 (0.0375)	
training:	Epoch: [33][32/233]	Loss 0.1734 (0.0417)	
training:	Epoch: [33][33/233]	Loss 0.0655 (0.0424)	
training:	Epoch: [33][34/233]	Loss 0.1008 (0.0442)	
training:	Epoch: [33][35/233]	Loss 0.0315 (0.0438)	
training:	Epoch: [33][36/233]	Loss 0.0103 (0.0429)	
training:	Epoch: [33][37/233]	Loss 0.0122 (0.0420)	
training:	Epoch: [33][38/233]	Loss 0.0543 (0.0424)	
training:	Epoch: [33][39/233]	Loss 0.0097 (0.0415)	
training:	Epoch: [33][40/233]	Loss 0.0119 (0.0408)	
training:	Epoch: [33][41/233]	Loss 0.1276 (0.0429)	
training:	Epoch: [33][42/233]	Loss 0.0103 (0.0421)	
training:	Epoch: [33][43/233]	Loss 0.1470 (0.0446)	
training:	Epoch: [33][44/233]	Loss 0.0112 (0.0438)	
training:	Epoch: [33][45/233]	Loss 0.0146 (0.0432)	
training:	Epoch: [33][46/233]	Loss 0.0695 (0.0437)	
training:	Epoch: [33][47/233]	Loss 0.0099 (0.0430)	
training:	Epoch: [33][48/233]	Loss 0.0240 (0.0426)	
training:	Epoch: [33][49/233]	Loss 0.1654 (0.0451)	
training:	Epoch: [33][50/233]	Loss 0.0387 (0.0450)	
training:	Epoch: [33][51/233]	Loss 0.1805 (0.0476)	
training:	Epoch: [33][52/233]	Loss 0.0114 (0.0469)	
training:	Epoch: [33][53/233]	Loss 0.0120 (0.0463)	
training:	Epoch: [33][54/233]	Loss 0.0174 (0.0458)	
training:	Epoch: [33][55/233]	Loss 0.0169 (0.0452)	
training:	Epoch: [33][56/233]	Loss 0.0722 (0.0457)	
training:	Epoch: [33][57/233]	Loss 0.0705 (0.0461)	
training:	Epoch: [33][58/233]	Loss 0.0099 (0.0455)	
training:	Epoch: [33][59/233]	Loss 0.2878 (0.0496)	
training:	Epoch: [33][60/233]	Loss 0.1330 (0.0510)	
training:	Epoch: [33][61/233]	Loss 0.1863 (0.0532)	
training:	Epoch: [33][62/233]	Loss 0.0113 (0.0526)	
training:	Epoch: [33][63/233]	Loss 0.0101 (0.0519)	
training:	Epoch: [33][64/233]	Loss 0.0968 (0.0526)	
training:	Epoch: [33][65/233]	Loss 0.0133 (0.0520)	
training:	Epoch: [33][66/233]	Loss 0.1815 (0.0539)	
training:	Epoch: [33][67/233]	Loss 0.3500 (0.0584)	
training:	Epoch: [33][68/233]	Loss 0.0212 (0.0578)	
training:	Epoch: [33][69/233]	Loss 0.0586 (0.0578)	
training:	Epoch: [33][70/233]	Loss 0.0333 (0.0575)	
training:	Epoch: [33][71/233]	Loss 0.1523 (0.0588)	
training:	Epoch: [33][72/233]	Loss 0.0644 (0.0589)	
training:	Epoch: [33][73/233]	Loss 0.1250 (0.0598)	
training:	Epoch: [33][74/233]	Loss 0.1354 (0.0608)	
training:	Epoch: [33][75/233]	Loss 0.2878 (0.0638)	
training:	Epoch: [33][76/233]	Loss 0.0695 (0.0639)	
training:	Epoch: [33][77/233]	Loss 0.0118 (0.0632)	
training:	Epoch: [33][78/233]	Loss 0.1931 (0.0649)	
training:	Epoch: [33][79/233]	Loss 0.0212 (0.0644)	
training:	Epoch: [33][80/233]	Loss 0.0120 (0.0637)	
training:	Epoch: [33][81/233]	Loss 0.0943 (0.0641)	
training:	Epoch: [33][82/233]	Loss 0.0089 (0.0634)	
training:	Epoch: [33][83/233]	Loss 0.0349 (0.0631)	
training:	Epoch: [33][84/233]	Loss 0.0168 (0.0625)	
training:	Epoch: [33][85/233]	Loss 0.0118 (0.0619)	
training:	Epoch: [33][86/233]	Loss 0.0142 (0.0614)	
training:	Epoch: [33][87/233]	Loss 0.0435 (0.0612)	
training:	Epoch: [33][88/233]	Loss 0.0217 (0.0607)	
training:	Epoch: [33][89/233]	Loss 0.1247 (0.0614)	
training:	Epoch: [33][90/233]	Loss 0.0236 (0.0610)	
training:	Epoch: [33][91/233]	Loss 0.2319 (0.0629)	
training:	Epoch: [33][92/233]	Loss 0.0208 (0.0624)	
training:	Epoch: [33][93/233]	Loss 0.0095 (0.0619)	
training:	Epoch: [33][94/233]	Loss 0.0986 (0.0622)	
training:	Epoch: [33][95/233]	Loss 0.0168 (0.0618)	
training:	Epoch: [33][96/233]	Loss 0.1166 (0.0623)	
training:	Epoch: [33][97/233]	Loss 0.0177 (0.0619)	
training:	Epoch: [33][98/233]	Loss 0.0470 (0.0617)	
training:	Epoch: [33][99/233]	Loss 0.0170 (0.0613)	
training:	Epoch: [33][100/233]	Loss 0.1708 (0.0624)	
training:	Epoch: [33][101/233]	Loss 0.0187 (0.0619)	
training:	Epoch: [33][102/233]	Loss 0.0520 (0.0618)	
training:	Epoch: [33][103/233]	Loss 0.0277 (0.0615)	
training:	Epoch: [33][104/233]	Loss 0.0206 (0.0611)	
training:	Epoch: [33][105/233]	Loss 0.0236 (0.0608)	
training:	Epoch: [33][106/233]	Loss 0.0111 (0.0603)	
training:	Epoch: [33][107/233]	Loss 0.0141 (0.0599)	
training:	Epoch: [33][108/233]	Loss 0.0103 (0.0594)	
training:	Epoch: [33][109/233]	Loss 0.1481 (0.0602)	
training:	Epoch: [33][110/233]	Loss 0.0115 (0.0598)	
training:	Epoch: [33][111/233]	Loss 0.0162 (0.0594)	
training:	Epoch: [33][112/233]	Loss 0.0113 (0.0590)	
training:	Epoch: [33][113/233]	Loss 0.1442 (0.0597)	
training:	Epoch: [33][114/233]	Loss 0.0161 (0.0593)	
training:	Epoch: [33][115/233]	Loss 0.1394 (0.0600)	
training:	Epoch: [33][116/233]	Loss 0.0137 (0.0596)	
training:	Epoch: [33][117/233]	Loss 0.1151 (0.0601)	
training:	Epoch: [33][118/233]	Loss 0.0179 (0.0597)	
training:	Epoch: [33][119/233]	Loss 0.1637 (0.0606)	
training:	Epoch: [33][120/233]	Loss 0.0127 (0.0602)	
training:	Epoch: [33][121/233]	Loss 0.1612 (0.0610)	
training:	Epoch: [33][122/233]	Loss 0.0880 (0.0613)	
training:	Epoch: [33][123/233]	Loss 0.0747 (0.0614)	
training:	Epoch: [33][124/233]	Loss 0.0139 (0.0610)	
training:	Epoch: [33][125/233]	Loss 0.0133 (0.0606)	
training:	Epoch: [33][126/233]	Loss 0.0192 (0.0603)	
training:	Epoch: [33][127/233]	Loss 0.0151 (0.0599)	
training:	Epoch: [33][128/233]	Loss 0.1213 (0.0604)	
training:	Epoch: [33][129/233]	Loss 0.0139 (0.0600)	
training:	Epoch: [33][130/233]	Loss 0.0174 (0.0597)	
training:	Epoch: [33][131/233]	Loss 0.0266 (0.0595)	
training:	Epoch: [33][132/233]	Loss 0.2656 (0.0610)	
training:	Epoch: [33][133/233]	Loss 0.0300 (0.0608)	
training:	Epoch: [33][134/233]	Loss 0.0492 (0.0607)	
training:	Epoch: [33][135/233]	Loss 0.1523 (0.0614)	
training:	Epoch: [33][136/233]	Loss 0.0244 (0.0611)	
training:	Epoch: [33][137/233]	Loss 0.0113 (0.0608)	
training:	Epoch: [33][138/233]	Loss 0.0268 (0.0605)	
training:	Epoch: [33][139/233]	Loss 0.1925 (0.0615)	
training:	Epoch: [33][140/233]	Loss 0.0104 (0.0611)	
training:	Epoch: [33][141/233]	Loss 0.0518 (0.0610)	
training:	Epoch: [33][142/233]	Loss 0.0108 (0.0607)	
training:	Epoch: [33][143/233]	Loss 0.1833 (0.0615)	
training:	Epoch: [33][144/233]	Loss 0.1574 (0.0622)	
training:	Epoch: [33][145/233]	Loss 0.0216 (0.0619)	
training:	Epoch: [33][146/233]	Loss 0.0111 (0.0616)	
training:	Epoch: [33][147/233]	Loss 0.1885 (0.0624)	
training:	Epoch: [33][148/233]	Loss 0.0112 (0.0621)	
training:	Epoch: [33][149/233]	Loss 0.2175 (0.0631)	
training:	Epoch: [33][150/233]	Loss 0.1061 (0.0634)	
training:	Epoch: [33][151/233]	Loss 0.1822 (0.0642)	
training:	Epoch: [33][152/233]	Loss 0.0127 (0.0639)	
training:	Epoch: [33][153/233]	Loss 0.0269 (0.0636)	
training:	Epoch: [33][154/233]	Loss 0.1791 (0.0644)	
training:	Epoch: [33][155/233]	Loss 0.1454 (0.0649)	
training:	Epoch: [33][156/233]	Loss 0.0131 (0.0646)	
training:	Epoch: [33][157/233]	Loss 0.0226 (0.0643)	
training:	Epoch: [33][158/233]	Loss 0.0631 (0.0643)	
training:	Epoch: [33][159/233]	Loss 0.0179 (0.0640)	
training:	Epoch: [33][160/233]	Loss 0.0118 (0.0637)	
training:	Epoch: [33][161/233]	Loss 0.0121 (0.0633)	
training:	Epoch: [33][162/233]	Loss 0.0719 (0.0634)	
training:	Epoch: [33][163/233]	Loss 0.0208 (0.0631)	
training:	Epoch: [33][164/233]	Loss 0.0115 (0.0628)	
training:	Epoch: [33][165/233]	Loss 0.1525 (0.0634)	
training:	Epoch: [33][166/233]	Loss 0.0144 (0.0631)	
training:	Epoch: [33][167/233]	Loss 0.0125 (0.0628)	
training:	Epoch: [33][168/233]	Loss 0.0141 (0.0625)	
training:	Epoch: [33][169/233]	Loss 0.1380 (0.0629)	
training:	Epoch: [33][170/233]	Loss 0.0684 (0.0630)	
training:	Epoch: [33][171/233]	Loss 0.0180 (0.0627)	
training:	Epoch: [33][172/233]	Loss 0.0742 (0.0628)	
training:	Epoch: [33][173/233]	Loss 0.0154 (0.0625)	
training:	Epoch: [33][174/233]	Loss 0.0204 (0.0622)	
training:	Epoch: [33][175/233]	Loss 0.0199 (0.0620)	
training:	Epoch: [33][176/233]	Loss 0.0827 (0.0621)	
training:	Epoch: [33][177/233]	Loss 0.0196 (0.0619)	
training:	Epoch: [33][178/233]	Loss 0.1246 (0.0622)	
training:	Epoch: [33][179/233]	Loss 0.0269 (0.0620)	
training:	Epoch: [33][180/233]	Loss 0.3184 (0.0635)	
training:	Epoch: [33][181/233]	Loss 0.0563 (0.0634)	
training:	Epoch: [33][182/233]	Loss 0.0522 (0.0634)	
training:	Epoch: [33][183/233]	Loss 0.0658 (0.0634)	
training:	Epoch: [33][184/233]	Loss 0.1023 (0.0636)	
training:	Epoch: [33][185/233]	Loss 0.0285 (0.0634)	
training:	Epoch: [33][186/233]	Loss 0.0170 (0.0631)	
training:	Epoch: [33][187/233]	Loss 0.0462 (0.0631)	
training:	Epoch: [33][188/233]	Loss 0.0136 (0.0628)	
training:	Epoch: [33][189/233]	Loss 0.0240 (0.0626)	
training:	Epoch: [33][190/233]	Loss 0.0730 (0.0626)	
training:	Epoch: [33][191/233]	Loss 0.0155 (0.0624)	
training:	Epoch: [33][192/233]	Loss 0.0145 (0.0621)	
training:	Epoch: [33][193/233]	Loss 0.0140 (0.0619)	
training:	Epoch: [33][194/233]	Loss 0.0307 (0.0617)	
training:	Epoch: [33][195/233]	Loss 0.0119 (0.0615)	
training:	Epoch: [33][196/233]	Loss 0.0211 (0.0613)	
training:	Epoch: [33][197/233]	Loss 0.0301 (0.0611)	
training:	Epoch: [33][198/233]	Loss 0.0250 (0.0609)	
training:	Epoch: [33][199/233]	Loss 0.0542 (0.0609)	
training:	Epoch: [33][200/233]	Loss 0.0106 (0.0606)	
training:	Epoch: [33][201/233]	Loss 0.1646 (0.0612)	
training:	Epoch: [33][202/233]	Loss 0.1222 (0.0615)	
training:	Epoch: [33][203/233]	Loss 0.0204 (0.0613)	
training:	Epoch: [33][204/233]	Loss 0.0831 (0.0614)	
training:	Epoch: [33][205/233]	Loss 0.1644 (0.0619)	
training:	Epoch: [33][206/233]	Loss 0.0322 (0.0617)	
training:	Epoch: [33][207/233]	Loss 0.0088 (0.0615)	
training:	Epoch: [33][208/233]	Loss 0.0138 (0.0612)	
training:	Epoch: [33][209/233]	Loss 0.0237 (0.0611)	
training:	Epoch: [33][210/233]	Loss 0.0102 (0.0608)	
training:	Epoch: [33][211/233]	Loss 0.0121 (0.0606)	
training:	Epoch: [33][212/233]	Loss 0.0374 (0.0605)	
training:	Epoch: [33][213/233]	Loss 0.1463 (0.0609)	
training:	Epoch: [33][214/233]	Loss 0.0305 (0.0607)	
training:	Epoch: [33][215/233]	Loss 0.0474 (0.0607)	
training:	Epoch: [33][216/233]	Loss 0.0161 (0.0605)	
training:	Epoch: [33][217/233]	Loss 0.0683 (0.0605)	
training:	Epoch: [33][218/233]	Loss 0.0129 (0.0603)	
training:	Epoch: [33][219/233]	Loss 0.0103 (0.0601)	
training:	Epoch: [33][220/233]	Loss 0.0193 (0.0599)	
training:	Epoch: [33][221/233]	Loss 0.0139 (0.0597)	
training:	Epoch: [33][222/233]	Loss 0.0686 (0.0597)	
training:	Epoch: [33][223/233]	Loss 0.0395 (0.0596)	
training:	Epoch: [33][224/233]	Loss 0.0184 (0.0594)	
training:	Epoch: [33][225/233]	Loss 0.0224 (0.0593)	
training:	Epoch: [33][226/233]	Loss 0.1637 (0.0597)	
training:	Epoch: [33][227/233]	Loss 0.0217 (0.0596)	
training:	Epoch: [33][228/233]	Loss 0.0163 (0.0594)	
training:	Epoch: [33][229/233]	Loss 0.0180 (0.0592)	
training:	Epoch: [33][230/233]	Loss 0.0521 (0.0592)	
training:	Epoch: [33][231/233]	Loss 0.3132 (0.0603)	
training:	Epoch: [33][232/233]	Loss 0.0214 (0.0601)	
training:	Epoch: [33][233/233]	Loss 0.0121 (0.0599)	
Training:	 Loss: 0.0598

Training:	 ACC: 0.9928 0.9926 0.9892 0.9964
Validation:	 ACC: 0.7927 0.7908 0.7518 0.8337
Validation:	 Best_BACC: 0.8162 0.8154 0.7998 0.8326
Validation:	 Loss: 0.7687
Pretraining:	Epoch 34/200
----------
training:	Epoch: [34][1/233]	Loss 0.3031 (0.3031)	
training:	Epoch: [34][2/233]	Loss 0.0132 (0.1582)	
training:	Epoch: [34][3/233]	Loss 0.0102 (0.1088)	
training:	Epoch: [34][4/233]	Loss 0.0708 (0.0993)	
training:	Epoch: [34][5/233]	Loss 0.0181 (0.0831)	
training:	Epoch: [34][6/233]	Loss 0.0109 (0.0710)	
training:	Epoch: [34][7/233]	Loss 0.0481 (0.0678)	
training:	Epoch: [34][8/233]	Loss 0.0268 (0.0626)	
training:	Epoch: [34][9/233]	Loss 0.1298 (0.0701)	
training:	Epoch: [34][10/233]	Loss 0.0214 (0.0652)	
training:	Epoch: [34][11/233]	Loss 0.0184 (0.0610)	
training:	Epoch: [34][12/233]	Loss 0.0131 (0.0570)	
training:	Epoch: [34][13/233]	Loss 0.0692 (0.0579)	
training:	Epoch: [34][14/233]	Loss 0.0125 (0.0547)	
training:	Epoch: [34][15/233]	Loss 0.0134 (0.0519)	
training:	Epoch: [34][16/233]	Loss 0.0159 (0.0497)	
training:	Epoch: [34][17/233]	Loss 0.0187 (0.0479)	
training:	Epoch: [34][18/233]	Loss 0.0088 (0.0457)	
training:	Epoch: [34][19/233]	Loss 0.0113 (0.0439)	
training:	Epoch: [34][20/233]	Loss 0.0252 (0.0429)	
training:	Epoch: [34][21/233]	Loss 0.0168 (0.0417)	
training:	Epoch: [34][22/233]	Loss 0.0094 (0.0402)	
training:	Epoch: [34][23/233]	Loss 0.2570 (0.0497)	
training:	Epoch: [34][24/233]	Loss 0.0178 (0.0483)	
training:	Epoch: [34][25/233]	Loss 0.0226 (0.0473)	
training:	Epoch: [34][26/233]	Loss 0.0153 (0.0461)	
training:	Epoch: [34][27/233]	Loss 0.0107 (0.0448)	
training:	Epoch: [34][28/233]	Loss 0.0111 (0.0436)	
training:	Epoch: [34][29/233]	Loss 0.0910 (0.0452)	
training:	Epoch: [34][30/233]	Loss 0.1188 (0.0476)	
training:	Epoch: [34][31/233]	Loss 0.0133 (0.0465)	
training:	Epoch: [34][32/233]	Loss 0.0764 (0.0475)	
training:	Epoch: [34][33/233]	Loss 0.1120 (0.0494)	
training:	Epoch: [34][34/233]	Loss 0.1099 (0.0512)	
training:	Epoch: [34][35/233]	Loss 0.0558 (0.0513)	
training:	Epoch: [34][36/233]	Loss 0.1823 (0.0550)	
training:	Epoch: [34][37/233]	Loss 0.0199 (0.0540)	
training:	Epoch: [34][38/233]	Loss 0.1263 (0.0559)	
training:	Epoch: [34][39/233]	Loss 0.0342 (0.0554)	
training:	Epoch: [34][40/233]	Loss 0.0213 (0.0545)	
training:	Epoch: [34][41/233]	Loss 0.0135 (0.0535)	
training:	Epoch: [34][42/233]	Loss 0.0118 (0.0525)	
training:	Epoch: [34][43/233]	Loss 0.1734 (0.0553)	
training:	Epoch: [34][44/233]	Loss 0.0298 (0.0548)	
training:	Epoch: [34][45/233]	Loss 0.0334 (0.0543)	
training:	Epoch: [34][46/233]	Loss 0.0302 (0.0538)	
training:	Epoch: [34][47/233]	Loss 0.0100 (0.0528)	
training:	Epoch: [34][48/233]	Loss 0.0118 (0.0520)	
training:	Epoch: [34][49/233]	Loss 0.0099 (0.0511)	
training:	Epoch: [34][50/233]	Loss 0.0275 (0.0506)	
training:	Epoch: [34][51/233]	Loss 0.0153 (0.0499)	
training:	Epoch: [34][52/233]	Loss 0.0113 (0.0492)	
training:	Epoch: [34][53/233]	Loss 0.0896 (0.0500)	
training:	Epoch: [34][54/233]	Loss 0.0136 (0.0493)	
training:	Epoch: [34][55/233]	Loss 0.1001 (0.0502)	
training:	Epoch: [34][56/233]	Loss 0.0192 (0.0497)	
training:	Epoch: [34][57/233]	Loss 0.0158 (0.0491)	
training:	Epoch: [34][58/233]	Loss 0.0131 (0.0484)	
training:	Epoch: [34][59/233]	Loss 0.1337 (0.0499)	
training:	Epoch: [34][60/233]	Loss 0.0181 (0.0494)	
training:	Epoch: [34][61/233]	Loss 0.0308 (0.0491)	
training:	Epoch: [34][62/233]	Loss 0.0241 (0.0487)	
training:	Epoch: [34][63/233]	Loss 0.0104 (0.0480)	
training:	Epoch: [34][64/233]	Loss 0.0086 (0.0474)	
training:	Epoch: [34][65/233]	Loss 0.1486 (0.0490)	
training:	Epoch: [34][66/233]	Loss 0.0096 (0.0484)	
training:	Epoch: [34][67/233]	Loss 0.0157 (0.0479)	
training:	Epoch: [34][68/233]	Loss 0.2821 (0.0513)	
training:	Epoch: [34][69/233]	Loss 0.3079 (0.0551)	
training:	Epoch: [34][70/233]	Loss 0.0163 (0.0545)	
training:	Epoch: [34][71/233]	Loss 0.2142 (0.0568)	
training:	Epoch: [34][72/233]	Loss 0.0148 (0.0562)	
training:	Epoch: [34][73/233]	Loss 0.0099 (0.0555)	
training:	Epoch: [34][74/233]	Loss 0.0141 (0.0550)	
training:	Epoch: [34][75/233]	Loss 0.0117 (0.0544)	
training:	Epoch: [34][76/233]	Loss 0.0403 (0.0542)	
training:	Epoch: [34][77/233]	Loss 0.0117 (0.0537)	
training:	Epoch: [34][78/233]	Loss 0.0610 (0.0538)	
training:	Epoch: [34][79/233]	Loss 0.0358 (0.0535)	
training:	Epoch: [34][80/233]	Loss 0.0265 (0.0532)	
training:	Epoch: [34][81/233]	Loss 0.1735 (0.0547)	
training:	Epoch: [34][82/233]	Loss 0.0640 (0.0548)	
training:	Epoch: [34][83/233]	Loss 0.0119 (0.0543)	
training:	Epoch: [34][84/233]	Loss 0.0232 (0.0539)	
training:	Epoch: [34][85/233]	Loss 0.0123 (0.0534)	
training:	Epoch: [34][86/233]	Loss 0.0178 (0.0530)	
training:	Epoch: [34][87/233]	Loss 0.1263 (0.0538)	
training:	Epoch: [34][88/233]	Loss 0.0139 (0.0534)	
training:	Epoch: [34][89/233]	Loss 0.0175 (0.0530)	
training:	Epoch: [34][90/233]	Loss 0.0126 (0.0525)	
training:	Epoch: [34][91/233]	Loss 0.0141 (0.0521)	
training:	Epoch: [34][92/233]	Loss 0.0617 (0.0522)	
training:	Epoch: [34][93/233]	Loss 0.0147 (0.0518)	
training:	Epoch: [34][94/233]	Loss 0.0616 (0.0519)	
training:	Epoch: [34][95/233]	Loss 0.0173 (0.0516)	
training:	Epoch: [34][96/233]	Loss 0.0432 (0.0515)	
training:	Epoch: [34][97/233]	Loss 0.0458 (0.0514)	
training:	Epoch: [34][98/233]	Loss 0.0802 (0.0517)	
training:	Epoch: [34][99/233]	Loss 0.0414 (0.0516)	
training:	Epoch: [34][100/233]	Loss 0.1269 (0.0524)	
training:	Epoch: [34][101/233]	Loss 0.0503 (0.0523)	
training:	Epoch: [34][102/233]	Loss 0.0098 (0.0519)	
training:	Epoch: [34][103/233]	Loss 0.0158 (0.0516)	
training:	Epoch: [34][104/233]	Loss 0.0325 (0.0514)	
training:	Epoch: [34][105/233]	Loss 0.0161 (0.0510)	
training:	Epoch: [34][106/233]	Loss 0.0104 (0.0507)	
training:	Epoch: [34][107/233]	Loss 0.0134 (0.0503)	
training:	Epoch: [34][108/233]	Loss 0.0096 (0.0499)	
training:	Epoch: [34][109/233]	Loss 0.0131 (0.0496)	
training:	Epoch: [34][110/233]	Loss 0.1366 (0.0504)	
training:	Epoch: [34][111/233]	Loss 0.0100 (0.0500)	
training:	Epoch: [34][112/233]	Loss 0.1581 (0.0510)	
training:	Epoch: [34][113/233]	Loss 0.2525 (0.0528)	
training:	Epoch: [34][114/233]	Loss 0.0463 (0.0527)	
training:	Epoch: [34][115/233]	Loss 0.1546 (0.0536)	
training:	Epoch: [34][116/233]	Loss 0.0378 (0.0535)	
training:	Epoch: [34][117/233]	Loss 0.0151 (0.0531)	
training:	Epoch: [34][118/233]	Loss 0.0107 (0.0528)	
training:	Epoch: [34][119/233]	Loss 0.0090 (0.0524)	
training:	Epoch: [34][120/233]	Loss 0.0175 (0.0521)	
training:	Epoch: [34][121/233]	Loss 0.1883 (0.0533)	
training:	Epoch: [34][122/233]	Loss 0.0191 (0.0530)	
training:	Epoch: [34][123/233]	Loss 0.0106 (0.0526)	
training:	Epoch: [34][124/233]	Loss 0.0169 (0.0523)	
training:	Epoch: [34][125/233]	Loss 0.0161 (0.0520)	
training:	Epoch: [34][126/233]	Loss 0.1590 (0.0529)	
training:	Epoch: [34][127/233]	Loss 0.1097 (0.0533)	
training:	Epoch: [34][128/233]	Loss 0.0146 (0.0530)	
training:	Epoch: [34][129/233]	Loss 0.0692 (0.0532)	
training:	Epoch: [34][130/233]	Loss 0.1305 (0.0538)	
training:	Epoch: [34][131/233]	Loss 0.0358 (0.0536)	
training:	Epoch: [34][132/233]	Loss 0.0229 (0.0534)	
training:	Epoch: [34][133/233]	Loss 0.0265 (0.0532)	
training:	Epoch: [34][134/233]	Loss 0.0180 (0.0529)	
training:	Epoch: [34][135/233]	Loss 0.0448 (0.0529)	
training:	Epoch: [34][136/233]	Loss 0.0091 (0.0525)	
training:	Epoch: [34][137/233]	Loss 0.0107 (0.0522)	
training:	Epoch: [34][138/233]	Loss 0.0107 (0.0519)	
training:	Epoch: [34][139/233]	Loss 0.1099 (0.0524)	
training:	Epoch: [34][140/233]	Loss 0.1799 (0.0533)	
training:	Epoch: [34][141/233]	Loss 0.0092 (0.0530)	
training:	Epoch: [34][142/233]	Loss 0.0284 (0.0528)	
training:	Epoch: [34][143/233]	Loss 0.0092 (0.0525)	
training:	Epoch: [34][144/233]	Loss 0.0882 (0.0527)	
training:	Epoch: [34][145/233]	Loss 0.0114 (0.0524)	
training:	Epoch: [34][146/233]	Loss 0.1236 (0.0529)	
training:	Epoch: [34][147/233]	Loss 0.1294 (0.0534)	
training:	Epoch: [34][148/233]	Loss 0.1771 (0.0543)	
training:	Epoch: [34][149/233]	Loss 0.2900 (0.0559)	
training:	Epoch: [34][150/233]	Loss 0.1073 (0.0562)	
training:	Epoch: [34][151/233]	Loss 0.0150 (0.0559)	
training:	Epoch: [34][152/233]	Loss 0.0082 (0.0556)	
training:	Epoch: [34][153/233]	Loss 0.1771 (0.0564)	
training:	Epoch: [34][154/233]	Loss 0.1746 (0.0572)	
training:	Epoch: [34][155/233]	Loss 0.0075 (0.0569)	
training:	Epoch: [34][156/233]	Loss 0.1480 (0.0574)	
training:	Epoch: [34][157/233]	Loss 0.0110 (0.0571)	
training:	Epoch: [34][158/233]	Loss 0.0150 (0.0569)	
training:	Epoch: [34][159/233]	Loss 0.1152 (0.0572)	
training:	Epoch: [34][160/233]	Loss 0.1638 (0.0579)	
training:	Epoch: [34][161/233]	Loss 0.0308 (0.0577)	
training:	Epoch: [34][162/233]	Loss 0.1587 (0.0584)	
training:	Epoch: [34][163/233]	Loss 0.0264 (0.0582)	
training:	Epoch: [34][164/233]	Loss 0.0161 (0.0579)	
training:	Epoch: [34][165/233]	Loss 0.0125 (0.0576)	
training:	Epoch: [34][166/233]	Loss 0.0104 (0.0574)	
training:	Epoch: [34][167/233]	Loss 0.0107 (0.0571)	
training:	Epoch: [34][168/233]	Loss 0.0234 (0.0569)	
training:	Epoch: [34][169/233]	Loss 0.0614 (0.0569)	
training:	Epoch: [34][170/233]	Loss 0.0171 (0.0567)	
training:	Epoch: [34][171/233]	Loss 0.1872 (0.0574)	
training:	Epoch: [34][172/233]	Loss 0.0302 (0.0573)	
training:	Epoch: [34][173/233]	Loss 0.0119 (0.0570)	
training:	Epoch: [34][174/233]	Loss 0.1568 (0.0576)	
training:	Epoch: [34][175/233]	Loss 0.1436 (0.0581)	
training:	Epoch: [34][176/233]	Loss 0.0132 (0.0578)	
training:	Epoch: [34][177/233]	Loss 0.0383 (0.0577)	
training:	Epoch: [34][178/233]	Loss 0.0100 (0.0574)	
training:	Epoch: [34][179/233]	Loss 0.0149 (0.0572)	
training:	Epoch: [34][180/233]	Loss 0.1653 (0.0578)	
training:	Epoch: [34][181/233]	Loss 0.0141 (0.0576)	
training:	Epoch: [34][182/233]	Loss 0.0243 (0.0574)	
training:	Epoch: [34][183/233]	Loss 0.0125 (0.0571)	
training:	Epoch: [34][184/233]	Loss 0.0083 (0.0569)	
training:	Epoch: [34][185/233]	Loss 0.0884 (0.0570)	
training:	Epoch: [34][186/233]	Loss 0.0180 (0.0568)	
training:	Epoch: [34][187/233]	Loss 0.0134 (0.0566)	
training:	Epoch: [34][188/233]	Loss 0.0107 (0.0564)	
training:	Epoch: [34][189/233]	Loss 0.0264 (0.0562)	
training:	Epoch: [34][190/233]	Loss 0.0641 (0.0562)	
training:	Epoch: [34][191/233]	Loss 0.0206 (0.0561)	
training:	Epoch: [34][192/233]	Loss 0.0885 (0.0562)	
training:	Epoch: [34][193/233]	Loss 0.0169 (0.0560)	
training:	Epoch: [34][194/233]	Loss 0.0113 (0.0558)	
training:	Epoch: [34][195/233]	Loss 0.0458 (0.0557)	
training:	Epoch: [34][196/233]	Loss 0.1543 (0.0562)	
training:	Epoch: [34][197/233]	Loss 0.0503 (0.0562)	
training:	Epoch: [34][198/233]	Loss 0.0337 (0.0561)	
training:	Epoch: [34][199/233]	Loss 0.0104 (0.0559)	
training:	Epoch: [34][200/233]	Loss 0.1542 (0.0564)	
training:	Epoch: [34][201/233]	Loss 0.0099 (0.0561)	
training:	Epoch: [34][202/233]	Loss 0.0135 (0.0559)	
training:	Epoch: [34][203/233]	Loss 0.0280 (0.0558)	
training:	Epoch: [34][204/233]	Loss 0.0133 (0.0556)	
training:	Epoch: [34][205/233]	Loss 0.0117 (0.0554)	
training:	Epoch: [34][206/233]	Loss 0.1340 (0.0557)	
training:	Epoch: [34][207/233]	Loss 0.0330 (0.0556)	
training:	Epoch: [34][208/233]	Loss 0.0233 (0.0555)	
training:	Epoch: [34][209/233]	Loss 0.2153 (0.0562)	
training:	Epoch: [34][210/233]	Loss 0.0165 (0.0560)	
training:	Epoch: [34][211/233]	Loss 0.1441 (0.0565)	
training:	Epoch: [34][212/233]	Loss 0.1810 (0.0571)	
training:	Epoch: [34][213/233]	Loss 0.0211 (0.0569)	
training:	Epoch: [34][214/233]	Loss 0.0127 (0.0567)	
training:	Epoch: [34][215/233]	Loss 0.0153 (0.0565)	
training:	Epoch: [34][216/233]	Loss 0.2052 (0.0572)	
training:	Epoch: [34][217/233]	Loss 0.0103 (0.0570)	
training:	Epoch: [34][218/233]	Loss 0.0195 (0.0568)	
training:	Epoch: [34][219/233]	Loss 0.0120 (0.0566)	
training:	Epoch: [34][220/233]	Loss 0.0110 (0.0564)	
training:	Epoch: [34][221/233]	Loss 0.2389 (0.0572)	
training:	Epoch: [34][222/233]	Loss 0.0209 (0.0570)	
training:	Epoch: [34][223/233]	Loss 0.0601 (0.0571)	
training:	Epoch: [34][224/233]	Loss 0.0133 (0.0569)	
training:	Epoch: [34][225/233]	Loss 0.0136 (0.0567)	
training:	Epoch: [34][226/233]	Loss 0.0379 (0.0566)	
training:	Epoch: [34][227/233]	Loss 0.0094 (0.0564)	
training:	Epoch: [34][228/233]	Loss 0.0154 (0.0562)	
training:	Epoch: [34][229/233]	Loss 0.0125 (0.0560)	
training:	Epoch: [34][230/233]	Loss 0.2870 (0.0570)	
training:	Epoch: [34][231/233]	Loss 0.1076 (0.0572)	
training:	Epoch: [34][232/233]	Loss 0.0259 (0.0571)	
training:	Epoch: [34][233/233]	Loss 0.0249 (0.0570)	
Training:	 Loss: 0.0568

Training:	 ACC: 0.9938 0.9937 0.9918 0.9958
Validation:	 ACC: 0.7958 0.7951 0.7814 0.8101
Validation:	 Best_BACC: 0.8162 0.8154 0.7998 0.8326
Validation:	 Loss: 0.7495
Pretraining:	Epoch 35/200
----------
training:	Epoch: [35][1/233]	Loss 0.1416 (0.1416)	
training:	Epoch: [35][2/233]	Loss 0.1331 (0.1374)	
training:	Epoch: [35][3/233]	Loss 0.2042 (0.1596)	
training:	Epoch: [35][4/233]	Loss 0.0706 (0.1374)	
training:	Epoch: [35][5/233]	Loss 0.0141 (0.1127)	
training:	Epoch: [35][6/233]	Loss 0.0138 (0.0962)	
training:	Epoch: [35][7/233]	Loss 0.0156 (0.0847)	
training:	Epoch: [35][8/233]	Loss 0.1770 (0.0963)	
training:	Epoch: [35][9/233]	Loss 0.1748 (0.1050)	
training:	Epoch: [35][10/233]	Loss 0.0095 (0.0954)	
training:	Epoch: [35][11/233]	Loss 0.0166 (0.0883)	
training:	Epoch: [35][12/233]	Loss 0.0126 (0.0820)	
training:	Epoch: [35][13/233]	Loss 0.1379 (0.0863)	
training:	Epoch: [35][14/233]	Loss 0.0829 (0.0860)	
training:	Epoch: [35][15/233]	Loss 0.0092 (0.0809)	
training:	Epoch: [35][16/233]	Loss 0.0282 (0.0776)	
training:	Epoch: [35][17/233]	Loss 0.0155 (0.0740)	
training:	Epoch: [35][18/233]	Loss 0.0145 (0.0707)	
training:	Epoch: [35][19/233]	Loss 0.0092 (0.0674)	
training:	Epoch: [35][20/233]	Loss 0.0188 (0.0650)	
training:	Epoch: [35][21/233]	Loss 0.0143 (0.0626)	
training:	Epoch: [35][22/233]	Loss 0.0122 (0.0603)	
training:	Epoch: [35][23/233]	Loss 0.0138 (0.0583)	
training:	Epoch: [35][24/233]	Loss 0.0119 (0.0563)	
training:	Epoch: [35][25/233]	Loss 0.0852 (0.0575)	
training:	Epoch: [35][26/233]	Loss 0.0187 (0.0560)	
training:	Epoch: [35][27/233]	Loss 0.0108 (0.0543)	
training:	Epoch: [35][28/233]	Loss 0.0718 (0.0549)	
training:	Epoch: [35][29/233]	Loss 0.0647 (0.0553)	
training:	Epoch: [35][30/233]	Loss 0.0499 (0.0551)	
training:	Epoch: [35][31/233]	Loss 0.0122 (0.0537)	
training:	Epoch: [35][32/233]	Loss 0.1120 (0.0555)	
training:	Epoch: [35][33/233]	Loss 0.0172 (0.0544)	
training:	Epoch: [35][34/233]	Loss 0.0188 (0.0533)	
training:	Epoch: [35][35/233]	Loss 0.0156 (0.0523)	
training:	Epoch: [35][36/233]	Loss 0.1468 (0.0549)	
training:	Epoch: [35][37/233]	Loss 0.0535 (0.0548)	
training:	Epoch: [35][38/233]	Loss 0.0129 (0.0537)	
training:	Epoch: [35][39/233]	Loss 0.0118 (0.0527)	
training:	Epoch: [35][40/233]	Loss 0.0100 (0.0516)	
training:	Epoch: [35][41/233]	Loss 0.1182 (0.0532)	
training:	Epoch: [35][42/233]	Loss 0.1550 (0.0556)	
training:	Epoch: [35][43/233]	Loss 0.0190 (0.0548)	
training:	Epoch: [35][44/233]	Loss 0.0179 (0.0540)	
training:	Epoch: [35][45/233]	Loss 0.0125 (0.0530)	
training:	Epoch: [35][46/233]	Loss 0.2911 (0.0582)	
training:	Epoch: [35][47/233]	Loss 0.0252 (0.0575)	
training:	Epoch: [35][48/233]	Loss 0.0327 (0.0570)	
training:	Epoch: [35][49/233]	Loss 0.0516 (0.0569)	
training:	Epoch: [35][50/233]	Loss 0.0278 (0.0563)	
training:	Epoch: [35][51/233]	Loss 0.0229 (0.0556)	
training:	Epoch: [35][52/233]	Loss 0.0419 (0.0554)	
training:	Epoch: [35][53/233]	Loss 0.0815 (0.0559)	
training:	Epoch: [35][54/233]	Loss 0.1112 (0.0569)	
training:	Epoch: [35][55/233]	Loss 0.0131 (0.0561)	
training:	Epoch: [35][56/233]	Loss 0.1033 (0.0569)	
training:	Epoch: [35][57/233]	Loss 0.0172 (0.0562)	
training:	Epoch: [35][58/233]	Loss 0.0288 (0.0558)	
training:	Epoch: [35][59/233]	Loss 0.0299 (0.0553)	
training:	Epoch: [35][60/233]	Loss 0.0312 (0.0549)	
training:	Epoch: [35][61/233]	Loss 0.0174 (0.0543)	
training:	Epoch: [35][62/233]	Loss 0.0146 (0.0537)	
training:	Epoch: [35][63/233]	Loss 0.1678 (0.0555)	
training:	Epoch: [35][64/233]	Loss 0.0150 (0.0549)	
training:	Epoch: [35][65/233]	Loss 0.0137 (0.0542)	
training:	Epoch: [35][66/233]	Loss 0.0206 (0.0537)	
training:	Epoch: [35][67/233]	Loss 0.0739 (0.0540)	
training:	Epoch: [35][68/233]	Loss 0.0224 (0.0536)	
training:	Epoch: [35][69/233]	Loss 0.0142 (0.0530)	
training:	Epoch: [35][70/233]	Loss 0.1969 (0.0550)	
training:	Epoch: [35][71/233]	Loss 0.0767 (0.0553)	
training:	Epoch: [35][72/233]	Loss 0.0088 (0.0547)	
training:	Epoch: [35][73/233]	Loss 0.0474 (0.0546)	
training:	Epoch: [35][74/233]	Loss 0.0195 (0.0541)	
training:	Epoch: [35][75/233]	Loss 0.0203 (0.0537)	
training:	Epoch: [35][76/233]	Loss 0.0127 (0.0531)	
training:	Epoch: [35][77/233]	Loss 0.0118 (0.0526)	
training:	Epoch: [35][78/233]	Loss 0.1796 (0.0542)	
training:	Epoch: [35][79/233]	Loss 0.1734 (0.0557)	
training:	Epoch: [35][80/233]	Loss 0.0111 (0.0552)	
training:	Epoch: [35][81/233]	Loss 0.0114 (0.0546)	
training:	Epoch: [35][82/233]	Loss 0.0266 (0.0543)	
training:	Epoch: [35][83/233]	Loss 0.0087 (0.0537)	
training:	Epoch: [35][84/233]	Loss 0.0087 (0.0532)	
training:	Epoch: [35][85/233]	Loss 0.0153 (0.0528)	
training:	Epoch: [35][86/233]	Loss 0.0109 (0.0523)	
training:	Epoch: [35][87/233]	Loss 0.0333 (0.0521)	
training:	Epoch: [35][88/233]	Loss 0.0528 (0.0521)	
training:	Epoch: [35][89/233]	Loss 0.0667 (0.0522)	
training:	Epoch: [35][90/233]	Loss 0.0093 (0.0517)	
training:	Epoch: [35][91/233]	Loss 0.0164 (0.0514)	
training:	Epoch: [35][92/233]	Loss 0.0147 (0.0510)	
training:	Epoch: [35][93/233]	Loss 0.0121 (0.0505)	
training:	Epoch: [35][94/233]	Loss 0.1252 (0.0513)	
training:	Epoch: [35][95/233]	Loss 0.0188 (0.0510)	
training:	Epoch: [35][96/233]	Loss 0.1317 (0.0518)	
training:	Epoch: [35][97/233]	Loss 0.0114 (0.0514)	
training:	Epoch: [35][98/233]	Loss 0.0094 (0.0510)	
training:	Epoch: [35][99/233]	Loss 0.0144 (0.0506)	
training:	Epoch: [35][100/233]	Loss 0.2444 (0.0526)	
training:	Epoch: [35][101/233]	Loss 0.0302 (0.0523)	
training:	Epoch: [35][102/233]	Loss 0.1992 (0.0538)	
training:	Epoch: [35][103/233]	Loss 0.0101 (0.0534)	
training:	Epoch: [35][104/233]	Loss 0.3783 (0.0565)	
training:	Epoch: [35][105/233]	Loss 0.0174 (0.0561)	
training:	Epoch: [35][106/233]	Loss 0.0392 (0.0559)	
training:	Epoch: [35][107/233]	Loss 0.0115 (0.0555)	
training:	Epoch: [35][108/233]	Loss 0.1267 (0.0562)	
training:	Epoch: [35][109/233]	Loss 0.0994 (0.0566)	
training:	Epoch: [35][110/233]	Loss 0.0090 (0.0562)	
training:	Epoch: [35][111/233]	Loss 0.1299 (0.0568)	
training:	Epoch: [35][112/233]	Loss 0.0513 (0.0568)	
training:	Epoch: [35][113/233]	Loss 0.0097 (0.0564)	
training:	Epoch: [35][114/233]	Loss 0.0990 (0.0567)	
training:	Epoch: [35][115/233]	Loss 0.1578 (0.0576)	
training:	Epoch: [35][116/233]	Loss 0.0754 (0.0578)	
training:	Epoch: [35][117/233]	Loss 0.0413 (0.0576)	
training:	Epoch: [35][118/233]	Loss 0.2230 (0.0590)	
training:	Epoch: [35][119/233]	Loss 0.0098 (0.0586)	
training:	Epoch: [35][120/233]	Loss 0.0085 (0.0582)	
training:	Epoch: [35][121/233]	Loss 0.0271 (0.0579)	
training:	Epoch: [35][122/233]	Loss 0.1420 (0.0586)	
training:	Epoch: [35][123/233]	Loss 0.0108 (0.0582)	
training:	Epoch: [35][124/233]	Loss 0.0136 (0.0579)	
training:	Epoch: [35][125/233]	Loss 0.0188 (0.0576)	
training:	Epoch: [35][126/233]	Loss 0.0535 (0.0575)	
training:	Epoch: [35][127/233]	Loss 0.0317 (0.0573)	
training:	Epoch: [35][128/233]	Loss 0.0180 (0.0570)	
training:	Epoch: [35][129/233]	Loss 0.0097 (0.0567)	
training:	Epoch: [35][130/233]	Loss 0.0102 (0.0563)	
training:	Epoch: [35][131/233]	Loss 0.0097 (0.0559)	
training:	Epoch: [35][132/233]	Loss 0.1606 (0.0567)	
training:	Epoch: [35][133/233]	Loss 0.2327 (0.0581)	
training:	Epoch: [35][134/233]	Loss 0.0097 (0.0577)	
training:	Epoch: [35][135/233]	Loss 0.0510 (0.0576)	
training:	Epoch: [35][136/233]	Loss 0.0132 (0.0573)	
training:	Epoch: [35][137/233]	Loss 0.0145 (0.0570)	
training:	Epoch: [35][138/233]	Loss 0.0136 (0.0567)	
training:	Epoch: [35][139/233]	Loss 0.0771 (0.0568)	
training:	Epoch: [35][140/233]	Loss 0.0093 (0.0565)	
training:	Epoch: [35][141/233]	Loss 0.1724 (0.0573)	
training:	Epoch: [35][142/233]	Loss 0.0368 (0.0572)	
training:	Epoch: [35][143/233]	Loss 0.0220 (0.0569)	
training:	Epoch: [35][144/233]	Loss 0.0299 (0.0567)	
training:	Epoch: [35][145/233]	Loss 0.0130 (0.0564)	
training:	Epoch: [35][146/233]	Loss 0.0463 (0.0564)	
training:	Epoch: [35][147/233]	Loss 0.0265 (0.0562)	
training:	Epoch: [35][148/233]	Loss 0.0140 (0.0559)	
training:	Epoch: [35][149/233]	Loss 0.0212 (0.0556)	
training:	Epoch: [35][150/233]	Loss 0.0109 (0.0553)	
training:	Epoch: [35][151/233]	Loss 0.1754 (0.0561)	
training:	Epoch: [35][152/233]	Loss 0.0376 (0.0560)	
training:	Epoch: [35][153/233]	Loss 0.0574 (0.0560)	
training:	Epoch: [35][154/233]	Loss 0.1721 (0.0568)	
training:	Epoch: [35][155/233]	Loss 0.0162 (0.0565)	
training:	Epoch: [35][156/233]	Loss 0.0130 (0.0562)	
training:	Epoch: [35][157/233]	Loss 0.2907 (0.0577)	
training:	Epoch: [35][158/233]	Loss 0.0146 (0.0575)	
training:	Epoch: [35][159/233]	Loss 0.0610 (0.0575)	
training:	Epoch: [35][160/233]	Loss 0.0815 (0.0576)	
training:	Epoch: [35][161/233]	Loss 0.0141 (0.0574)	
training:	Epoch: [35][162/233]	Loss 0.0201 (0.0571)	
training:	Epoch: [35][163/233]	Loss 0.0101 (0.0568)	
training:	Epoch: [35][164/233]	Loss 0.0528 (0.0568)	
training:	Epoch: [35][165/233]	Loss 0.0479 (0.0568)	
training:	Epoch: [35][166/233]	Loss 0.1769 (0.0575)	
training:	Epoch: [35][167/233]	Loss 0.0187 (0.0573)	
training:	Epoch: [35][168/233]	Loss 0.1588 (0.0579)	
training:	Epoch: [35][169/233]	Loss 0.0132 (0.0576)	
training:	Epoch: [35][170/233]	Loss 0.0110 (0.0573)	
training:	Epoch: [35][171/233]	Loss 0.0158 (0.0571)	
training:	Epoch: [35][172/233]	Loss 0.0185 (0.0569)	
training:	Epoch: [35][173/233]	Loss 0.0085 (0.0566)	
training:	Epoch: [35][174/233]	Loss 0.0144 (0.0563)	
training:	Epoch: [35][175/233]	Loss 0.0100 (0.0561)	
training:	Epoch: [35][176/233]	Loss 0.0179 (0.0559)	
training:	Epoch: [35][177/233]	Loss 0.0156 (0.0556)	
training:	Epoch: [35][178/233]	Loss 0.0180 (0.0554)	
training:	Epoch: [35][179/233]	Loss 0.0441 (0.0554)	
training:	Epoch: [35][180/233]	Loss 0.0111 (0.0551)	
training:	Epoch: [35][181/233]	Loss 0.0434 (0.0550)	
training:	Epoch: [35][182/233]	Loss 0.0193 (0.0548)	
training:	Epoch: [35][183/233]	Loss 0.1521 (0.0554)	
training:	Epoch: [35][184/233]	Loss 0.0126 (0.0551)	
training:	Epoch: [35][185/233]	Loss 0.1007 (0.0554)	
training:	Epoch: [35][186/233]	Loss 0.0412 (0.0553)	
training:	Epoch: [35][187/233]	Loss 0.0389 (0.0552)	
training:	Epoch: [35][188/233]	Loss 0.0180 (0.0550)	
training:	Epoch: [35][189/233]	Loss 0.0114 (0.0548)	
training:	Epoch: [35][190/233]	Loss 0.1545 (0.0553)	
training:	Epoch: [35][191/233]	Loss 0.0664 (0.0554)	
training:	Epoch: [35][192/233]	Loss 0.0230 (0.0552)	
training:	Epoch: [35][193/233]	Loss 0.0103 (0.0550)	
training:	Epoch: [35][194/233]	Loss 0.1100 (0.0553)	
training:	Epoch: [35][195/233]	Loss 0.0088 (0.0550)	
training:	Epoch: [35][196/233]	Loss 0.0771 (0.0551)	
training:	Epoch: [35][197/233]	Loss 0.0825 (0.0553)	
training:	Epoch: [35][198/233]	Loss 0.1530 (0.0558)	
training:	Epoch: [35][199/233]	Loss 0.0172 (0.0556)	
training:	Epoch: [35][200/233]	Loss 0.0114 (0.0554)	
training:	Epoch: [35][201/233]	Loss 0.0162 (0.0552)	
training:	Epoch: [35][202/233]	Loss 0.1259 (0.0555)	
training:	Epoch: [35][203/233]	Loss 0.0095 (0.0553)	
training:	Epoch: [35][204/233]	Loss 0.0149 (0.0551)	
training:	Epoch: [35][205/233]	Loss 0.0207 (0.0549)	
training:	Epoch: [35][206/233]	Loss 0.1051 (0.0552)	
training:	Epoch: [35][207/233]	Loss 0.0271 (0.0550)	
training:	Epoch: [35][208/233]	Loss 0.0154 (0.0548)	
training:	Epoch: [35][209/233]	Loss 0.0114 (0.0546)	
training:	Epoch: [35][210/233]	Loss 0.0413 (0.0546)	
training:	Epoch: [35][211/233]	Loss 0.1966 (0.0552)	
training:	Epoch: [35][212/233]	Loss 0.0136 (0.0550)	
training:	Epoch: [35][213/233]	Loss 0.0124 (0.0548)	
training:	Epoch: [35][214/233]	Loss 0.0084 (0.0546)	
training:	Epoch: [35][215/233]	Loss 0.0791 (0.0547)	
training:	Epoch: [35][216/233]	Loss 0.0110 (0.0545)	
training:	Epoch: [35][217/233]	Loss 0.0379 (0.0545)	
training:	Epoch: [35][218/233]	Loss 0.0105 (0.0543)	
training:	Epoch: [35][219/233]	Loss 0.0190 (0.0541)	
training:	Epoch: [35][220/233]	Loss 0.0202 (0.0539)	
training:	Epoch: [35][221/233]	Loss 0.0304 (0.0538)	
training:	Epoch: [35][222/233]	Loss 0.0477 (0.0538)	
training:	Epoch: [35][223/233]	Loss 0.1376 (0.0542)	
training:	Epoch: [35][224/233]	Loss 0.0211 (0.0540)	
training:	Epoch: [35][225/233]	Loss 0.0131 (0.0539)	
training:	Epoch: [35][226/233]	Loss 0.0106 (0.0537)	
training:	Epoch: [35][227/233]	Loss 0.0389 (0.0536)	
training:	Epoch: [35][228/233]	Loss 0.0217 (0.0535)	
training:	Epoch: [35][229/233]	Loss 0.2972 (0.0545)	
training:	Epoch: [35][230/233]	Loss 0.0126 (0.0543)	
training:	Epoch: [35][231/233]	Loss 0.0118 (0.0542)	
training:	Epoch: [35][232/233]	Loss 0.0246 (0.0540)	
training:	Epoch: [35][233/233]	Loss 0.0119 (0.0538)	
Training:	 Loss: 0.0537

Training:	 ACC: 0.9945 0.9944 0.9923 0.9966
Validation:	 ACC: 0.7931 0.7919 0.7681 0.8180
Validation:	 Best_BACC: 0.8162 0.8154 0.7998 0.8326
Validation:	 Loss: 0.7741
Pretraining:	Epoch 36/200
----------
training:	Epoch: [36][1/233]	Loss 0.0202 (0.0202)	
training:	Epoch: [36][2/233]	Loss 0.0225 (0.0214)	
training:	Epoch: [36][3/233]	Loss 0.0124 (0.0184)	
training:	Epoch: [36][4/233]	Loss 0.1628 (0.0545)	
training:	Epoch: [36][5/233]	Loss 0.0215 (0.0479)	
training:	Epoch: [36][6/233]	Loss 0.0099 (0.0416)	
training:	Epoch: [36][7/233]	Loss 0.0154 (0.0378)	
training:	Epoch: [36][8/233]	Loss 0.0608 (0.0407)	
training:	Epoch: [36][9/233]	Loss 0.0130 (0.0376)	
training:	Epoch: [36][10/233]	Loss 0.1241 (0.0463)	
training:	Epoch: [36][11/233]	Loss 0.0108 (0.0430)	
training:	Epoch: [36][12/233]	Loss 0.0186 (0.0410)	
training:	Epoch: [36][13/233]	Loss 0.0198 (0.0394)	
training:	Epoch: [36][14/233]	Loss 0.1572 (0.0478)	
training:	Epoch: [36][15/233]	Loss 0.0104 (0.0453)	
training:	Epoch: [36][16/233]	Loss 0.0109 (0.0431)	
training:	Epoch: [36][17/233]	Loss 0.0295 (0.0423)	
training:	Epoch: [36][18/233]	Loss 0.1750 (0.0497)	
training:	Epoch: [36][19/233]	Loss 0.1501 (0.0550)	
training:	Epoch: [36][20/233]	Loss 0.0128 (0.0529)	
training:	Epoch: [36][21/233]	Loss 0.1512 (0.0576)	
training:	Epoch: [36][22/233]	Loss 0.0135 (0.0556)	
training:	Epoch: [36][23/233]	Loss 0.0150 (0.0538)	
training:	Epoch: [36][24/233]	Loss 0.2605 (0.0624)	
training:	Epoch: [36][25/233]	Loss 0.0107 (0.0603)	
training:	Epoch: [36][26/233]	Loss 0.0091 (0.0584)	
training:	Epoch: [36][27/233]	Loss 0.0606 (0.0585)	
training:	Epoch: [36][28/233]	Loss 0.0227 (0.0572)	
training:	Epoch: [36][29/233]	Loss 0.2626 (0.0643)	
training:	Epoch: [36][30/233]	Loss 0.0093 (0.0624)	
training:	Epoch: [36][31/233]	Loss 0.0394 (0.0617)	
training:	Epoch: [36][32/233]	Loss 0.0700 (0.0619)	
training:	Epoch: [36][33/233]	Loss 0.0136 (0.0605)	
training:	Epoch: [36][34/233]	Loss 0.0786 (0.0610)	
training:	Epoch: [36][35/233]	Loss 0.0094 (0.0595)	
training:	Epoch: [36][36/233]	Loss 0.0107 (0.0582)	
training:	Epoch: [36][37/233]	Loss 0.0119 (0.0569)	
training:	Epoch: [36][38/233]	Loss 0.1049 (0.0582)	
training:	Epoch: [36][39/233]	Loss 0.0435 (0.0578)	
training:	Epoch: [36][40/233]	Loss 0.1275 (0.0596)	
training:	Epoch: [36][41/233]	Loss 0.0092 (0.0583)	
training:	Epoch: [36][42/233]	Loss 0.0139 (0.0573)	
training:	Epoch: [36][43/233]	Loss 0.0121 (0.0562)	
training:	Epoch: [36][44/233]	Loss 0.0124 (0.0552)	
training:	Epoch: [36][45/233]	Loss 0.0155 (0.0543)	
training:	Epoch: [36][46/233]	Loss 0.0109 (0.0534)	
training:	Epoch: [36][47/233]	Loss 0.0110 (0.0525)	
training:	Epoch: [36][48/233]	Loss 0.0298 (0.0520)	
training:	Epoch: [36][49/233]	Loss 0.1067 (0.0531)	
training:	Epoch: [36][50/233]	Loss 0.1579 (0.0552)	
training:	Epoch: [36][51/233]	Loss 0.0159 (0.0545)	
training:	Epoch: [36][52/233]	Loss 0.0095 (0.0536)	
training:	Epoch: [36][53/233]	Loss 0.0171 (0.0529)	
training:	Epoch: [36][54/233]	Loss 0.0212 (0.0523)	
training:	Epoch: [36][55/233]	Loss 0.0103 (0.0516)	
training:	Epoch: [36][56/233]	Loss 0.1754 (0.0538)	
training:	Epoch: [36][57/233]	Loss 0.0155 (0.0531)	
training:	Epoch: [36][58/233]	Loss 0.0108 (0.0524)	
training:	Epoch: [36][59/233]	Loss 0.0121 (0.0517)	
training:	Epoch: [36][60/233]	Loss 0.0129 (0.0510)	
training:	Epoch: [36][61/233]	Loss 0.0152 (0.0504)	
training:	Epoch: [36][62/233]	Loss 0.0089 (0.0498)	
training:	Epoch: [36][63/233]	Loss 0.0110 (0.0492)	
training:	Epoch: [36][64/233]	Loss 0.0640 (0.0494)	
training:	Epoch: [36][65/233]	Loss 0.0161 (0.0489)	
training:	Epoch: [36][66/233]	Loss 0.1591 (0.0506)	
training:	Epoch: [36][67/233]	Loss 0.1064 (0.0514)	
training:	Epoch: [36][68/233]	Loss 0.0087 (0.0508)	
training:	Epoch: [36][69/233]	Loss 0.2027 (0.0530)	
training:	Epoch: [36][70/233]	Loss 0.0548 (0.0530)	
training:	Epoch: [36][71/233]	Loss 0.2144 (0.0553)	
training:	Epoch: [36][72/233]	Loss 0.0130 (0.0547)	
training:	Epoch: [36][73/233]	Loss 0.0800 (0.0550)	
training:	Epoch: [36][74/233]	Loss 0.1025 (0.0557)	
training:	Epoch: [36][75/233]	Loss 0.0152 (0.0551)	
training:	Epoch: [36][76/233]	Loss 0.1366 (0.0562)	
training:	Epoch: [36][77/233]	Loss 0.2128 (0.0582)	
training:	Epoch: [36][78/233]	Loss 0.0086 (0.0576)	
training:	Epoch: [36][79/233]	Loss 0.0266 (0.0572)	
training:	Epoch: [36][80/233]	Loss 0.0269 (0.0568)	
training:	Epoch: [36][81/233]	Loss 0.0106 (0.0563)	
training:	Epoch: [36][82/233]	Loss 0.0120 (0.0557)	
training:	Epoch: [36][83/233]	Loss 0.0728 (0.0559)	
training:	Epoch: [36][84/233]	Loss 0.1497 (0.0570)	
training:	Epoch: [36][85/233]	Loss 0.0097 (0.0565)	
training:	Epoch: [36][86/233]	Loss 0.0148 (0.0560)	
training:	Epoch: [36][87/233]	Loss 0.0122 (0.0555)	
training:	Epoch: [36][88/233]	Loss 0.1163 (0.0562)	
training:	Epoch: [36][89/233]	Loss 0.1327 (0.0570)	
training:	Epoch: [36][90/233]	Loss 0.1602 (0.0582)	
training:	Epoch: [36][91/233]	Loss 0.1820 (0.0596)	
training:	Epoch: [36][92/233]	Loss 0.0194 (0.0591)	
training:	Epoch: [36][93/233]	Loss 0.0963 (0.0595)	
training:	Epoch: [36][94/233]	Loss 0.0186 (0.0591)	
training:	Epoch: [36][95/233]	Loss 0.0173 (0.0586)	
training:	Epoch: [36][96/233]	Loss 0.1760 (0.0599)	
training:	Epoch: [36][97/233]	Loss 0.0333 (0.0596)	
training:	Epoch: [36][98/233]	Loss 0.0920 (0.0599)	
training:	Epoch: [36][99/233]	Loss 0.0114 (0.0594)	
training:	Epoch: [36][100/233]	Loss 0.1829 (0.0607)	
training:	Epoch: [36][101/233]	Loss 0.1646 (0.0617)	
training:	Epoch: [36][102/233]	Loss 0.0098 (0.0612)	
training:	Epoch: [36][103/233]	Loss 0.1718 (0.0623)	
training:	Epoch: [36][104/233]	Loss 0.1532 (0.0631)	
training:	Epoch: [36][105/233]	Loss 0.0238 (0.0628)	
training:	Epoch: [36][106/233]	Loss 0.0160 (0.0623)	
training:	Epoch: [36][107/233]	Loss 0.0298 (0.0620)	
training:	Epoch: [36][108/233]	Loss 0.0127 (0.0616)	
training:	Epoch: [36][109/233]	Loss 0.0117 (0.0611)	
training:	Epoch: [36][110/233]	Loss 0.0182 (0.0607)	
training:	Epoch: [36][111/233]	Loss 0.0215 (0.0604)	
training:	Epoch: [36][112/233]	Loss 0.0157 (0.0600)	
training:	Epoch: [36][113/233]	Loss 0.0358 (0.0597)	
training:	Epoch: [36][114/233]	Loss 0.0081 (0.0593)	
training:	Epoch: [36][115/233]	Loss 0.0102 (0.0589)	
training:	Epoch: [36][116/233]	Loss 0.1383 (0.0595)	
training:	Epoch: [36][117/233]	Loss 0.0114 (0.0591)	
training:	Epoch: [36][118/233]	Loss 0.0477 (0.0590)	
training:	Epoch: [36][119/233]	Loss 0.0081 (0.0586)	
training:	Epoch: [36][120/233]	Loss 0.0098 (0.0582)	
training:	Epoch: [36][121/233]	Loss 0.0094 (0.0578)	
training:	Epoch: [36][122/233]	Loss 0.0173 (0.0575)	
training:	Epoch: [36][123/233]	Loss 0.0487 (0.0574)	
training:	Epoch: [36][124/233]	Loss 0.0320 (0.0572)	
training:	Epoch: [36][125/233]	Loss 0.0111 (0.0568)	
training:	Epoch: [36][126/233]	Loss 0.1357 (0.0574)	
training:	Epoch: [36][127/233]	Loss 0.0131 (0.0571)	
training:	Epoch: [36][128/233]	Loss 0.0870 (0.0573)	
training:	Epoch: [36][129/233]	Loss 0.0088 (0.0570)	
training:	Epoch: [36][130/233]	Loss 0.0278 (0.0567)	
training:	Epoch: [36][131/233]	Loss 0.0093 (0.0564)	
training:	Epoch: [36][132/233]	Loss 0.0090 (0.0560)	
training:	Epoch: [36][133/233]	Loss 0.0121 (0.0557)	
training:	Epoch: [36][134/233]	Loss 0.0135 (0.0554)	
training:	Epoch: [36][135/233]	Loss 0.0879 (0.0556)	
training:	Epoch: [36][136/233]	Loss 0.0099 (0.0553)	
training:	Epoch: [36][137/233]	Loss 0.0132 (0.0550)	
training:	Epoch: [36][138/233]	Loss 0.1045 (0.0553)	
training:	Epoch: [36][139/233]	Loss 0.0110 (0.0550)	
training:	Epoch: [36][140/233]	Loss 0.1195 (0.0555)	
training:	Epoch: [36][141/233]	Loss 0.0092 (0.0551)	
training:	Epoch: [36][142/233]	Loss 0.0362 (0.0550)	
training:	Epoch: [36][143/233]	Loss 0.0323 (0.0548)	
training:	Epoch: [36][144/233]	Loss 0.0390 (0.0547)	
training:	Epoch: [36][145/233]	Loss 0.0134 (0.0545)	
training:	Epoch: [36][146/233]	Loss 0.0124 (0.0542)	
training:	Epoch: [36][147/233]	Loss 0.0161 (0.0539)	
training:	Epoch: [36][148/233]	Loss 0.0096 (0.0536)	
training:	Epoch: [36][149/233]	Loss 0.1652 (0.0544)	
training:	Epoch: [36][150/233]	Loss 0.0851 (0.0546)	
training:	Epoch: [36][151/233]	Loss 0.0492 (0.0545)	
training:	Epoch: [36][152/233]	Loss 0.0088 (0.0542)	
training:	Epoch: [36][153/233]	Loss 0.0154 (0.0540)	
training:	Epoch: [36][154/233]	Loss 0.0458 (0.0539)	
training:	Epoch: [36][155/233]	Loss 0.0198 (0.0537)	
training:	Epoch: [36][156/233]	Loss 0.0611 (0.0537)	
training:	Epoch: [36][157/233]	Loss 0.0449 (0.0537)	
training:	Epoch: [36][158/233]	Loss 0.0146 (0.0534)	
training:	Epoch: [36][159/233]	Loss 0.0221 (0.0532)	
training:	Epoch: [36][160/233]	Loss 0.0796 (0.0534)	
training:	Epoch: [36][161/233]	Loss 0.0304 (0.0533)	
training:	Epoch: [36][162/233]	Loss 0.0110 (0.0530)	
training:	Epoch: [36][163/233]	Loss 0.0096 (0.0527)	
training:	Epoch: [36][164/233]	Loss 0.0699 (0.0528)	
training:	Epoch: [36][165/233]	Loss 0.0097 (0.0526)	
training:	Epoch: [36][166/233]	Loss 0.3328 (0.0543)	
training:	Epoch: [36][167/233]	Loss 0.0954 (0.0545)	
training:	Epoch: [36][168/233]	Loss 0.1552 (0.0551)	
training:	Epoch: [36][169/233]	Loss 0.0342 (0.0550)	
training:	Epoch: [36][170/233]	Loss 0.0271 (0.0548)	
training:	Epoch: [36][171/233]	Loss 0.0135 (0.0546)	
training:	Epoch: [36][172/233]	Loss 0.0133 (0.0543)	
training:	Epoch: [36][173/233]	Loss 0.0108 (0.0541)	
training:	Epoch: [36][174/233]	Loss 0.0091 (0.0538)	
training:	Epoch: [36][175/233]	Loss 0.1739 (0.0545)	
training:	Epoch: [36][176/233]	Loss 0.0138 (0.0543)	
training:	Epoch: [36][177/233]	Loss 0.0092 (0.0540)	
training:	Epoch: [36][178/233]	Loss 0.0422 (0.0540)	
training:	Epoch: [36][179/233]	Loss 0.0601 (0.0540)	
training:	Epoch: [36][180/233]	Loss 0.0097 (0.0538)	
training:	Epoch: [36][181/233]	Loss 0.0147 (0.0535)	
training:	Epoch: [36][182/233]	Loss 0.1345 (0.0540)	
training:	Epoch: [36][183/233]	Loss 0.0165 (0.0538)	
training:	Epoch: [36][184/233]	Loss 0.0078 (0.0535)	
training:	Epoch: [36][185/233]	Loss 0.1514 (0.0541)	
training:	Epoch: [36][186/233]	Loss 0.0080 (0.0538)	
training:	Epoch: [36][187/233]	Loss 0.0350 (0.0537)	
training:	Epoch: [36][188/233]	Loss 0.0355 (0.0536)	
training:	Epoch: [36][189/233]	Loss 0.0963 (0.0538)	
training:	Epoch: [36][190/233]	Loss 0.0370 (0.0538)	
training:	Epoch: [36][191/233]	Loss 0.0185 (0.0536)	
training:	Epoch: [36][192/233]	Loss 0.0163 (0.0534)	
training:	Epoch: [36][193/233]	Loss 0.0207 (0.0532)	
training:	Epoch: [36][194/233]	Loss 0.1262 (0.0536)	
training:	Epoch: [36][195/233]	Loss 0.0105 (0.0534)	
training:	Epoch: [36][196/233]	Loss 0.0173 (0.0532)	
training:	Epoch: [36][197/233]	Loss 0.0117 (0.0530)	
training:	Epoch: [36][198/233]	Loss 0.0094 (0.0527)	
training:	Epoch: [36][199/233]	Loss 0.0133 (0.0525)	
training:	Epoch: [36][200/233]	Loss 0.0103 (0.0523)	
training:	Epoch: [36][201/233]	Loss 0.0133 (0.0521)	
training:	Epoch: [36][202/233]	Loss 0.0482 (0.0521)	
training:	Epoch: [36][203/233]	Loss 0.0108 (0.0519)	
training:	Epoch: [36][204/233]	Loss 0.0300 (0.0518)	
training:	Epoch: [36][205/233]	Loss 0.1769 (0.0524)	
training:	Epoch: [36][206/233]	Loss 0.0125 (0.0522)	
training:	Epoch: [36][207/233]	Loss 0.0649 (0.0523)	
training:	Epoch: [36][208/233]	Loss 0.0109 (0.0521)	
training:	Epoch: [36][209/233]	Loss 0.0332 (0.0520)	
training:	Epoch: [36][210/233]	Loss 0.0087 (0.0518)	
training:	Epoch: [36][211/233]	Loss 0.0106 (0.0516)	
training:	Epoch: [36][212/233]	Loss 0.0334 (0.0515)	
training:	Epoch: [36][213/233]	Loss 0.0139 (0.0513)	
training:	Epoch: [36][214/233]	Loss 0.1212 (0.0517)	
training:	Epoch: [36][215/233]	Loss 0.0181 (0.0515)	
training:	Epoch: [36][216/233]	Loss 0.0180 (0.0513)	
training:	Epoch: [36][217/233]	Loss 0.0116 (0.0512)	
training:	Epoch: [36][218/233]	Loss 0.1541 (0.0516)	
training:	Epoch: [36][219/233]	Loss 0.0123 (0.0515)	
training:	Epoch: [36][220/233]	Loss 0.0210 (0.0513)	
training:	Epoch: [36][221/233]	Loss 0.0106 (0.0511)	
training:	Epoch: [36][222/233]	Loss 0.0125 (0.0510)	
training:	Epoch: [36][223/233]	Loss 0.0130 (0.0508)	
training:	Epoch: [36][224/233]	Loss 0.0571 (0.0508)	
training:	Epoch: [36][225/233]	Loss 0.0155 (0.0507)	
training:	Epoch: [36][226/233]	Loss 0.0235 (0.0505)	
training:	Epoch: [36][227/233]	Loss 0.0088 (0.0504)	
training:	Epoch: [36][228/233]	Loss 0.0137 (0.0502)	
training:	Epoch: [36][229/233]	Loss 0.0185 (0.0501)	
training:	Epoch: [36][230/233]	Loss 0.1368 (0.0504)	
training:	Epoch: [36][231/233]	Loss 0.0082 (0.0503)	
training:	Epoch: [36][232/233]	Loss 0.0101 (0.0501)	
training:	Epoch: [36][233/233]	Loss 0.0089 (0.0499)	
Training:	 Loss: 0.0498

Training:	 ACC: 0.9950 0.9949 0.9933 0.9966
Validation:	 ACC: 0.7907 0.7903 0.7814 0.8000
Validation:	 Best_BACC: 0.8162 0.8154 0.7998 0.8326
Validation:	 Loss: 0.8000
Pretraining:	Epoch 37/200
----------
training:	Epoch: [37][1/233]	Loss 0.0452 (0.0452)	
training:	Epoch: [37][2/233]	Loss 0.0133 (0.0292)	
training:	Epoch: [37][3/233]	Loss 0.1808 (0.0798)	
training:	Epoch: [37][4/233]	Loss 0.0102 (0.0624)	
training:	Epoch: [37][5/233]	Loss 0.0129 (0.0525)	
training:	Epoch: [37][6/233]	Loss 0.0528 (0.0525)	
training:	Epoch: [37][7/233]	Loss 0.1918 (0.0724)	
training:	Epoch: [37][8/233]	Loss 0.0117 (0.0648)	
training:	Epoch: [37][9/233]	Loss 0.0221 (0.0601)	
training:	Epoch: [37][10/233]	Loss 0.3102 (0.0851)	
training:	Epoch: [37][11/233]	Loss 0.0086 (0.0781)	
training:	Epoch: [37][12/233]	Loss 0.0645 (0.0770)	
training:	Epoch: [37][13/233]	Loss 0.0448 (0.0745)	
training:	Epoch: [37][14/233]	Loss 0.0162 (0.0704)	
training:	Epoch: [37][15/233]	Loss 0.1425 (0.0752)	
training:	Epoch: [37][16/233]	Loss 0.0150 (0.0714)	
training:	Epoch: [37][17/233]	Loss 0.0141 (0.0680)	
training:	Epoch: [37][18/233]	Loss 0.0730 (0.0683)	
training:	Epoch: [37][19/233]	Loss 0.0090 (0.0652)	
training:	Epoch: [37][20/233]	Loss 0.0113 (0.0625)	
training:	Epoch: [37][21/233]	Loss 0.0092 (0.0600)	
training:	Epoch: [37][22/233]	Loss 0.2176 (0.0671)	
training:	Epoch: [37][23/233]	Loss 0.1202 (0.0694)	
training:	Epoch: [37][24/233]	Loss 0.0767 (0.0697)	
training:	Epoch: [37][25/233]	Loss 0.0163 (0.0676)	
training:	Epoch: [37][26/233]	Loss 0.0094 (0.0654)	
training:	Epoch: [37][27/233]	Loss 0.0909 (0.0663)	
training:	Epoch: [37][28/233]	Loss 0.0131 (0.0644)	
training:	Epoch: [37][29/233]	Loss 0.0150 (0.0627)	
training:	Epoch: [37][30/233]	Loss 0.0095 (0.0609)	
training:	Epoch: [37][31/233]	Loss 0.0873 (0.0618)	
training:	Epoch: [37][32/233]	Loss 0.0115 (0.0602)	
training:	Epoch: [37][33/233]	Loss 0.1241 (0.0621)	
training:	Epoch: [37][34/233]	Loss 0.1681 (0.0653)	
training:	Epoch: [37][35/233]	Loss 0.0170 (0.0639)	
training:	Epoch: [37][36/233]	Loss 0.0133 (0.0625)	
training:	Epoch: [37][37/233]	Loss 0.0636 (0.0625)	
training:	Epoch: [37][38/233]	Loss 0.0117 (0.0612)	
training:	Epoch: [37][39/233]	Loss 0.0091 (0.0598)	
training:	Epoch: [37][40/233]	Loss 0.0185 (0.0588)	
training:	Epoch: [37][41/233]	Loss 0.0126 (0.0577)	
training:	Epoch: [37][42/233]	Loss 0.0087 (0.0565)	
training:	Epoch: [37][43/233]	Loss 0.0144 (0.0555)	
training:	Epoch: [37][44/233]	Loss 0.0151 (0.0546)	
training:	Epoch: [37][45/233]	Loss 0.0167 (0.0538)	
training:	Epoch: [37][46/233]	Loss 0.0323 (0.0533)	
training:	Epoch: [37][47/233]	Loss 0.0100 (0.0524)	
training:	Epoch: [37][48/233]	Loss 0.1887 (0.0552)	
training:	Epoch: [37][49/233]	Loss 0.0102 (0.0543)	
training:	Epoch: [37][50/233]	Loss 0.0109 (0.0534)	
training:	Epoch: [37][51/233]	Loss 0.1794 (0.0559)	
training:	Epoch: [37][52/233]	Loss 0.0208 (0.0552)	
training:	Epoch: [37][53/233]	Loss 0.0106 (0.0544)	
training:	Epoch: [37][54/233]	Loss 0.0077 (0.0535)	
training:	Epoch: [37][55/233]	Loss 0.0085 (0.0527)	
training:	Epoch: [37][56/233]	Loss 0.0091 (0.0519)	
training:	Epoch: [37][57/233]	Loss 0.0230 (0.0514)	
training:	Epoch: [37][58/233]	Loss 0.0902 (0.0521)	
training:	Epoch: [37][59/233]	Loss 0.1136 (0.0531)	
training:	Epoch: [37][60/233]	Loss 0.0100 (0.0524)	
training:	Epoch: [37][61/233]	Loss 0.0122 (0.0517)	
training:	Epoch: [37][62/233]	Loss 0.0973 (0.0525)	
training:	Epoch: [37][63/233]	Loss 0.0095 (0.0518)	
training:	Epoch: [37][64/233]	Loss 0.0321 (0.0515)	
training:	Epoch: [37][65/233]	Loss 0.1716 (0.0533)	
training:	Epoch: [37][66/233]	Loss 0.0144 (0.0528)	
training:	Epoch: [37][67/233]	Loss 0.1432 (0.0541)	
training:	Epoch: [37][68/233]	Loss 0.0107 (0.0535)	
training:	Epoch: [37][69/233]	Loss 0.1623 (0.0550)	
training:	Epoch: [37][70/233]	Loss 0.0113 (0.0544)	
training:	Epoch: [37][71/233]	Loss 0.0087 (0.0538)	
training:	Epoch: [37][72/233]	Loss 0.0078 (0.0531)	
training:	Epoch: [37][73/233]	Loss 0.0083 (0.0525)	
training:	Epoch: [37][74/233]	Loss 0.0393 (0.0523)	
training:	Epoch: [37][75/233]	Loss 0.0102 (0.0518)	
training:	Epoch: [37][76/233]	Loss 0.0099 (0.0512)	
training:	Epoch: [37][77/233]	Loss 0.0105 (0.0507)	
training:	Epoch: [37][78/233]	Loss 0.0270 (0.0504)	
training:	Epoch: [37][79/233]	Loss 0.0328 (0.0502)	
training:	Epoch: [37][80/233]	Loss 0.1002 (0.0508)	
training:	Epoch: [37][81/233]	Loss 0.1683 (0.0522)	
training:	Epoch: [37][82/233]	Loss 0.0128 (0.0518)	
training:	Epoch: [37][83/233]	Loss 0.0184 (0.0514)	
training:	Epoch: [37][84/233]	Loss 0.1304 (0.0523)	
training:	Epoch: [37][85/233]	Loss 0.0104 (0.0518)	
training:	Epoch: [37][86/233]	Loss 0.0077 (0.0513)	
training:	Epoch: [37][87/233]	Loss 0.1547 (0.0525)	
training:	Epoch: [37][88/233]	Loss 0.1446 (0.0535)	
training:	Epoch: [37][89/233]	Loss 0.0217 (0.0532)	
training:	Epoch: [37][90/233]	Loss 0.0755 (0.0534)	
training:	Epoch: [37][91/233]	Loss 0.0221 (0.0531)	
training:	Epoch: [37][92/233]	Loss 0.0072 (0.0526)	
training:	Epoch: [37][93/233]	Loss 0.0119 (0.0521)	
training:	Epoch: [37][94/233]	Loss 0.1584 (0.0533)	
training:	Epoch: [37][95/233]	Loss 0.0259 (0.0530)	
training:	Epoch: [37][96/233]	Loss 0.0126 (0.0526)	
training:	Epoch: [37][97/233]	Loss 0.0089 (0.0521)	
training:	Epoch: [37][98/233]	Loss 0.0101 (0.0517)	
training:	Epoch: [37][99/233]	Loss 0.0076 (0.0512)	
training:	Epoch: [37][100/233]	Loss 0.0131 (0.0509)	
training:	Epoch: [37][101/233]	Loss 0.0758 (0.0511)	
training:	Epoch: [37][102/233]	Loss 0.0087 (0.0507)	
training:	Epoch: [37][103/233]	Loss 0.0084 (0.0503)	
training:	Epoch: [37][104/233]	Loss 0.1283 (0.0510)	
training:	Epoch: [37][105/233]	Loss 0.0164 (0.0507)	
training:	Epoch: [37][106/233]	Loss 0.0122 (0.0503)	
training:	Epoch: [37][107/233]	Loss 0.0088 (0.0499)	
training:	Epoch: [37][108/233]	Loss 0.0785 (0.0502)	
training:	Epoch: [37][109/233]	Loss 0.0143 (0.0499)	
training:	Epoch: [37][110/233]	Loss 0.1793 (0.0511)	
training:	Epoch: [37][111/233]	Loss 0.0093 (0.0507)	
training:	Epoch: [37][112/233]	Loss 0.0126 (0.0503)	
training:	Epoch: [37][113/233]	Loss 0.0171 (0.0501)	
training:	Epoch: [37][114/233]	Loss 0.0567 (0.0501)	
training:	Epoch: [37][115/233]	Loss 0.0127 (0.0498)	
training:	Epoch: [37][116/233]	Loss 0.0090 (0.0494)	
training:	Epoch: [37][117/233]	Loss 0.0192 (0.0492)	
training:	Epoch: [37][118/233]	Loss 0.1660 (0.0502)	
training:	Epoch: [37][119/233]	Loss 0.0823 (0.0504)	
training:	Epoch: [37][120/233]	Loss 0.0089 (0.0501)	
training:	Epoch: [37][121/233]	Loss 0.1396 (0.0508)	
training:	Epoch: [37][122/233]	Loss 0.0109 (0.0505)	
training:	Epoch: [37][123/233]	Loss 0.1495 (0.0513)	
training:	Epoch: [37][124/233]	Loss 0.0088 (0.0510)	
training:	Epoch: [37][125/233]	Loss 0.0174 (0.0507)	
training:	Epoch: [37][126/233]	Loss 0.0289 (0.0505)	
training:	Epoch: [37][127/233]	Loss 0.0331 (0.0504)	
training:	Epoch: [37][128/233]	Loss 0.0233 (0.0502)	
training:	Epoch: [37][129/233]	Loss 0.0125 (0.0499)	
training:	Epoch: [37][130/233]	Loss 0.3177 (0.0519)	
training:	Epoch: [37][131/233]	Loss 0.0091 (0.0516)	
training:	Epoch: [37][132/233]	Loss 0.0259 (0.0514)	
training:	Epoch: [37][133/233]	Loss 0.1912 (0.0525)	
training:	Epoch: [37][134/233]	Loss 0.0092 (0.0521)	
training:	Epoch: [37][135/233]	Loss 0.0119 (0.0518)	
training:	Epoch: [37][136/233]	Loss 0.0205 (0.0516)	
training:	Epoch: [37][137/233]	Loss 0.1171 (0.0521)	
training:	Epoch: [37][138/233]	Loss 0.0146 (0.0518)	
training:	Epoch: [37][139/233]	Loss 0.1422 (0.0525)	
training:	Epoch: [37][140/233]	Loss 0.1301 (0.0530)	
training:	Epoch: [37][141/233]	Loss 0.0213 (0.0528)	
training:	Epoch: [37][142/233]	Loss 0.0499 (0.0528)	
training:	Epoch: [37][143/233]	Loss 0.1070 (0.0532)	
training:	Epoch: [37][144/233]	Loss 0.0095 (0.0529)	
training:	Epoch: [37][145/233]	Loss 0.0153 (0.0526)	
training:	Epoch: [37][146/233]	Loss 0.0602 (0.0527)	
training:	Epoch: [37][147/233]	Loss 0.1071 (0.0530)	
training:	Epoch: [37][148/233]	Loss 0.1001 (0.0533)	
training:	Epoch: [37][149/233]	Loss 0.0125 (0.0531)	
training:	Epoch: [37][150/233]	Loss 0.0115 (0.0528)	
training:	Epoch: [37][151/233]	Loss 0.0185 (0.0526)	
training:	Epoch: [37][152/233]	Loss 0.0093 (0.0523)	
training:	Epoch: [37][153/233]	Loss 0.0074 (0.0520)	
training:	Epoch: [37][154/233]	Loss 0.0237 (0.0518)	
training:	Epoch: [37][155/233]	Loss 0.0110 (0.0515)	
training:	Epoch: [37][156/233]	Loss 0.1534 (0.0522)	
training:	Epoch: [37][157/233]	Loss 0.0094 (0.0519)	
training:	Epoch: [37][158/233]	Loss 0.0102 (0.0517)	
training:	Epoch: [37][159/233]	Loss 0.1065 (0.0520)	
training:	Epoch: [37][160/233]	Loss 0.0160 (0.0518)	
training:	Epoch: [37][161/233]	Loss 0.1689 (0.0525)	
training:	Epoch: [37][162/233]	Loss 0.0076 (0.0522)	
training:	Epoch: [37][163/233]	Loss 0.0104 (0.0520)	
training:	Epoch: [37][164/233]	Loss 0.1778 (0.0527)	
training:	Epoch: [37][165/233]	Loss 0.0095 (0.0525)	
training:	Epoch: [37][166/233]	Loss 0.0145 (0.0522)	
training:	Epoch: [37][167/233]	Loss 0.1554 (0.0529)	
training:	Epoch: [37][168/233]	Loss 0.0195 (0.0527)	
training:	Epoch: [37][169/233]	Loss 0.0091 (0.0524)	
training:	Epoch: [37][170/233]	Loss 0.1064 (0.0527)	
training:	Epoch: [37][171/233]	Loss 0.0110 (0.0525)	
training:	Epoch: [37][172/233]	Loss 0.0208 (0.0523)	
training:	Epoch: [37][173/233]	Loss 0.0079 (0.0520)	
training:	Epoch: [37][174/233]	Loss 0.0144 (0.0518)	
training:	Epoch: [37][175/233]	Loss 0.0167 (0.0516)	
training:	Epoch: [37][176/233]	Loss 0.0106 (0.0514)	
training:	Epoch: [37][177/233]	Loss 0.0125 (0.0512)	
training:	Epoch: [37][178/233]	Loss 0.0211 (0.0510)	
training:	Epoch: [37][179/233]	Loss 0.0078 (0.0508)	
training:	Epoch: [37][180/233]	Loss 0.0124 (0.0505)	
training:	Epoch: [37][181/233]	Loss 0.0117 (0.0503)	
training:	Epoch: [37][182/233]	Loss 0.0110 (0.0501)	
training:	Epoch: [37][183/233]	Loss 0.1095 (0.0504)	
training:	Epoch: [37][184/233]	Loss 0.0201 (0.0503)	
training:	Epoch: [37][185/233]	Loss 0.0463 (0.0503)	
training:	Epoch: [37][186/233]	Loss 0.0083 (0.0500)	
training:	Epoch: [37][187/233]	Loss 0.0531 (0.0500)	
training:	Epoch: [37][188/233]	Loss 0.0213 (0.0499)	
training:	Epoch: [37][189/233]	Loss 0.0088 (0.0497)	
training:	Epoch: [37][190/233]	Loss 0.0085 (0.0495)	
training:	Epoch: [37][191/233]	Loss 0.0077 (0.0492)	
training:	Epoch: [37][192/233]	Loss 0.0229 (0.0491)	
training:	Epoch: [37][193/233]	Loss 0.0448 (0.0491)	
training:	Epoch: [37][194/233]	Loss 0.0121 (0.0489)	
training:	Epoch: [37][195/233]	Loss 0.0746 (0.0490)	
training:	Epoch: [37][196/233]	Loss 0.0124 (0.0488)	
training:	Epoch: [37][197/233]	Loss 0.0166 (0.0487)	
training:	Epoch: [37][198/233]	Loss 0.0124 (0.0485)	
training:	Epoch: [37][199/233]	Loss 0.1414 (0.0490)	
training:	Epoch: [37][200/233]	Loss 0.0574 (0.0490)	
training:	Epoch: [37][201/233]	Loss 0.0137 (0.0488)	
training:	Epoch: [37][202/233]	Loss 0.0083 (0.0486)	
training:	Epoch: [37][203/233]	Loss 0.0219 (0.0485)	
training:	Epoch: [37][204/233]	Loss 0.1246 (0.0489)	
training:	Epoch: [37][205/233]	Loss 0.0175 (0.0487)	
training:	Epoch: [37][206/233]	Loss 0.0083 (0.0485)	
training:	Epoch: [37][207/233]	Loss 0.0192 (0.0484)	
training:	Epoch: [37][208/233]	Loss 0.0510 (0.0484)	
training:	Epoch: [37][209/233]	Loss 0.0700 (0.0485)	
training:	Epoch: [37][210/233]	Loss 0.0170 (0.0483)	
training:	Epoch: [37][211/233]	Loss 0.1519 (0.0488)	
training:	Epoch: [37][212/233]	Loss 0.0088 (0.0486)	
training:	Epoch: [37][213/233]	Loss 0.0160 (0.0485)	
training:	Epoch: [37][214/233]	Loss 0.0260 (0.0484)	
training:	Epoch: [37][215/233]	Loss 0.0098 (0.0482)	
training:	Epoch: [37][216/233]	Loss 0.0341 (0.0481)	
training:	Epoch: [37][217/233]	Loss 0.1751 (0.0487)	
training:	Epoch: [37][218/233]	Loss 0.0087 (0.0485)	
training:	Epoch: [37][219/233]	Loss 0.0083 (0.0484)	
training:	Epoch: [37][220/233]	Loss 0.0076 (0.0482)	
training:	Epoch: [37][221/233]	Loss 0.0208 (0.0480)	
training:	Epoch: [37][222/233]	Loss 0.1757 (0.0486)	
training:	Epoch: [37][223/233]	Loss 0.0221 (0.0485)	
training:	Epoch: [37][224/233]	Loss 0.0183 (0.0484)	
training:	Epoch: [37][225/233]	Loss 0.0080 (0.0482)	
training:	Epoch: [37][226/233]	Loss 0.0112 (0.0480)	
training:	Epoch: [37][227/233]	Loss 0.0084 (0.0478)	
training:	Epoch: [37][228/233]	Loss 0.0117 (0.0477)	
training:	Epoch: [37][229/233]	Loss 0.1138 (0.0480)	
training:	Epoch: [37][230/233]	Loss 0.0173 (0.0478)	
training:	Epoch: [37][231/233]	Loss 0.0071 (0.0477)	
training:	Epoch: [37][232/233]	Loss 0.0100 (0.0475)	
training:	Epoch: [37][233/233]	Loss 0.0096 (0.0473)	
Training:	 Loss: 0.0472

Training:	 ACC: 0.9950 0.9949 0.9931 0.9969
Validation:	 ACC: 0.7919 0.7913 0.7794 0.8045
Validation:	 Best_BACC: 0.8162 0.8154 0.7998 0.8326
Validation:	 Loss: 0.7974
Pretraining:	Epoch 38/200
----------
training:	Epoch: [38][1/233]	Loss 0.0170 (0.0170)	
training:	Epoch: [38][2/233]	Loss 0.0207 (0.0189)	
training:	Epoch: [38][3/233]	Loss 0.0096 (0.0158)	
training:	Epoch: [38][4/233]	Loss 0.0068 (0.0136)	
training:	Epoch: [38][5/233]	Loss 0.0357 (0.0180)	
training:	Epoch: [38][6/233]	Loss 0.0093 (0.0165)	
training:	Epoch: [38][7/233]	Loss 0.0108 (0.0157)	
training:	Epoch: [38][8/233]	Loss 0.0323 (0.0178)	
training:	Epoch: [38][9/233]	Loss 0.0240 (0.0185)	
training:	Epoch: [38][10/233]	Loss 0.0580 (0.0224)	
training:	Epoch: [38][11/233]	Loss 0.0086 (0.0212)	
training:	Epoch: [38][12/233]	Loss 0.0149 (0.0207)	
training:	Epoch: [38][13/233]	Loss 0.0074 (0.0196)	
training:	Epoch: [38][14/233]	Loss 0.1391 (0.0282)	
training:	Epoch: [38][15/233]	Loss 0.0069 (0.0268)	
training:	Epoch: [38][16/233]	Loss 0.0142 (0.0260)	
training:	Epoch: [38][17/233]	Loss 0.0506 (0.0274)	
training:	Epoch: [38][18/233]	Loss 0.0486 (0.0286)	
training:	Epoch: [38][19/233]	Loss 0.0061 (0.0274)	
training:	Epoch: [38][20/233]	Loss 0.0134 (0.0267)	
training:	Epoch: [38][21/233]	Loss 0.0101 (0.0259)	
training:	Epoch: [38][22/233]	Loss 0.0066 (0.0251)	
training:	Epoch: [38][23/233]	Loss 0.0066 (0.0242)	
training:	Epoch: [38][24/233]	Loss 0.1636 (0.0301)	
training:	Epoch: [38][25/233]	Loss 0.0084 (0.0292)	
training:	Epoch: [38][26/233]	Loss 0.1935 (0.0355)	
training:	Epoch: [38][27/233]	Loss 0.0074 (0.0345)	
training:	Epoch: [38][28/233]	Loss 0.1399 (0.0382)	
training:	Epoch: [38][29/233]	Loss 0.0102 (0.0373)	
training:	Epoch: [38][30/233]	Loss 0.0178 (0.0366)	
training:	Epoch: [38][31/233]	Loss 0.0158 (0.0359)	
training:	Epoch: [38][32/233]	Loss 0.1768 (0.0403)	
training:	Epoch: [38][33/233]	Loss 0.0082 (0.0394)	
training:	Epoch: [38][34/233]	Loss 0.0598 (0.0400)	
training:	Epoch: [38][35/233]	Loss 0.0087 (0.0391)	
training:	Epoch: [38][36/233]	Loss 0.0769 (0.0401)	
training:	Epoch: [38][37/233]	Loss 0.0081 (0.0393)	
training:	Epoch: [38][38/233]	Loss 0.0132 (0.0386)	
training:	Epoch: [38][39/233]	Loss 0.1722 (0.0420)	
training:	Epoch: [38][40/233]	Loss 0.0119 (0.0412)	
training:	Epoch: [38][41/233]	Loss 0.0095 (0.0405)	
training:	Epoch: [38][42/233]	Loss 0.0101 (0.0397)	
training:	Epoch: [38][43/233]	Loss 0.0083 (0.0390)	
training:	Epoch: [38][44/233]	Loss 0.1839 (0.0423)	
training:	Epoch: [38][45/233]	Loss 0.0660 (0.0428)	
training:	Epoch: [38][46/233]	Loss 0.1574 (0.0453)	
training:	Epoch: [38][47/233]	Loss 0.1328 (0.0472)	
training:	Epoch: [38][48/233]	Loss 0.0083 (0.0464)	
training:	Epoch: [38][49/233]	Loss 0.0098 (0.0456)	
training:	Epoch: [38][50/233]	Loss 0.0093 (0.0449)	
training:	Epoch: [38][51/233]	Loss 0.0091 (0.0442)	
training:	Epoch: [38][52/233]	Loss 0.0096 (0.0435)	
training:	Epoch: [38][53/233]	Loss 0.1180 (0.0449)	
training:	Epoch: [38][54/233]	Loss 0.0104 (0.0443)	
training:	Epoch: [38][55/233]	Loss 0.0084 (0.0437)	
training:	Epoch: [38][56/233]	Loss 0.0749 (0.0442)	
training:	Epoch: [38][57/233]	Loss 0.1359 (0.0458)	
training:	Epoch: [38][58/233]	Loss 0.1751 (0.0480)	
training:	Epoch: [38][59/233]	Loss 0.0073 (0.0474)	
training:	Epoch: [38][60/233]	Loss 0.0085 (0.0467)	
training:	Epoch: [38][61/233]	Loss 0.0280 (0.0464)	
training:	Epoch: [38][62/233]	Loss 0.1826 (0.0486)	
training:	Epoch: [38][63/233]	Loss 0.0112 (0.0480)	
training:	Epoch: [38][64/233]	Loss 0.0141 (0.0475)	
training:	Epoch: [38][65/233]	Loss 0.0305 (0.0472)	
training:	Epoch: [38][66/233]	Loss 0.1269 (0.0484)	
training:	Epoch: [38][67/233]	Loss 0.1461 (0.0499)	
training:	Epoch: [38][68/233]	Loss 0.0092 (0.0493)	
training:	Epoch: [38][69/233]	Loss 0.0205 (0.0489)	
training:	Epoch: [38][70/233]	Loss 0.0120 (0.0483)	
training:	Epoch: [38][71/233]	Loss 0.0159 (0.0479)	
training:	Epoch: [38][72/233]	Loss 0.2599 (0.0508)	
training:	Epoch: [38][73/233]	Loss 0.1656 (0.0524)	
training:	Epoch: [38][74/233]	Loss 0.0077 (0.0518)	
training:	Epoch: [38][75/233]	Loss 0.0147 (0.0513)	
training:	Epoch: [38][76/233]	Loss 0.1098 (0.0521)	
training:	Epoch: [38][77/233]	Loss 0.0201 (0.0517)	
training:	Epoch: [38][78/233]	Loss 0.1558 (0.0530)	
training:	Epoch: [38][79/233]	Loss 0.0084 (0.0524)	
training:	Epoch: [38][80/233]	Loss 0.1285 (0.0534)	
training:	Epoch: [38][81/233]	Loss 0.1605 (0.0547)	
training:	Epoch: [38][82/233]	Loss 0.0139 (0.0542)	
training:	Epoch: [38][83/233]	Loss 0.1177 (0.0550)	
training:	Epoch: [38][84/233]	Loss 0.0104 (0.0544)	
training:	Epoch: [38][85/233]	Loss 0.0094 (0.0539)	
training:	Epoch: [38][86/233]	Loss 0.0397 (0.0537)	
training:	Epoch: [38][87/233]	Loss 0.0675 (0.0539)	
training:	Epoch: [38][88/233]	Loss 0.0110 (0.0534)	
training:	Epoch: [38][89/233]	Loss 0.0130 (0.0530)	
training:	Epoch: [38][90/233]	Loss 0.0094 (0.0525)	
training:	Epoch: [38][91/233]	Loss 0.0115 (0.0520)	
training:	Epoch: [38][92/233]	Loss 0.1361 (0.0529)	
training:	Epoch: [38][93/233]	Loss 0.0155 (0.0525)	
training:	Epoch: [38][94/233]	Loss 0.0138 (0.0521)	
training:	Epoch: [38][95/233]	Loss 0.1805 (0.0535)	
training:	Epoch: [38][96/233]	Loss 0.1016 (0.0540)	
training:	Epoch: [38][97/233]	Loss 0.0093 (0.0535)	
training:	Epoch: [38][98/233]	Loss 0.0096 (0.0531)	
training:	Epoch: [38][99/233]	Loss 0.0517 (0.0530)	
training:	Epoch: [38][100/233]	Loss 0.0085 (0.0526)	
training:	Epoch: [38][101/233]	Loss 0.1658 (0.0537)	
training:	Epoch: [38][102/233]	Loss 0.0152 (0.0533)	
training:	Epoch: [38][103/233]	Loss 0.0092 (0.0529)	
training:	Epoch: [38][104/233]	Loss 0.0111 (0.0525)	
training:	Epoch: [38][105/233]	Loss 0.0113 (0.0521)	
training:	Epoch: [38][106/233]	Loss 0.0312 (0.0519)	
training:	Epoch: [38][107/233]	Loss 0.0336 (0.0518)	
training:	Epoch: [38][108/233]	Loss 0.0096 (0.0514)	
training:	Epoch: [38][109/233]	Loss 0.0106 (0.0510)	
training:	Epoch: [38][110/233]	Loss 0.0092 (0.0506)	
training:	Epoch: [38][111/233]	Loss 0.1639 (0.0516)	
training:	Epoch: [38][112/233]	Loss 0.0725 (0.0518)	
training:	Epoch: [38][113/233]	Loss 0.0301 (0.0516)	
training:	Epoch: [38][114/233]	Loss 0.0770 (0.0518)	
training:	Epoch: [38][115/233]	Loss 0.0400 (0.0517)	
training:	Epoch: [38][116/233]	Loss 0.0288 (0.0515)	
training:	Epoch: [38][117/233]	Loss 0.0724 (0.0517)	
training:	Epoch: [38][118/233]	Loss 0.0086 (0.0514)	
training:	Epoch: [38][119/233]	Loss 0.0279 (0.0512)	
training:	Epoch: [38][120/233]	Loss 0.0977 (0.0516)	
training:	Epoch: [38][121/233]	Loss 0.0120 (0.0512)	
training:	Epoch: [38][122/233]	Loss 0.0130 (0.0509)	
training:	Epoch: [38][123/233]	Loss 0.0175 (0.0506)	
training:	Epoch: [38][124/233]	Loss 0.0283 (0.0505)	
training:	Epoch: [38][125/233]	Loss 0.0157 (0.0502)	
training:	Epoch: [38][126/233]	Loss 0.0184 (0.0499)	
training:	Epoch: [38][127/233]	Loss 0.1628 (0.0508)	
training:	Epoch: [38][128/233]	Loss 0.0099 (0.0505)	
training:	Epoch: [38][129/233]	Loss 0.0329 (0.0504)	
training:	Epoch: [38][130/233]	Loss 0.0076 (0.0500)	
training:	Epoch: [38][131/233]	Loss 0.0981 (0.0504)	
training:	Epoch: [38][132/233]	Loss 0.0514 (0.0504)	
training:	Epoch: [38][133/233]	Loss 0.0103 (0.0501)	
training:	Epoch: [38][134/233]	Loss 0.0093 (0.0498)	
training:	Epoch: [38][135/233]	Loss 0.0434 (0.0498)	
training:	Epoch: [38][136/233]	Loss 0.0131 (0.0495)	
training:	Epoch: [38][137/233]	Loss 0.0123 (0.0492)	
training:	Epoch: [38][138/233]	Loss 0.0355 (0.0491)	
training:	Epoch: [38][139/233]	Loss 0.0229 (0.0489)	
training:	Epoch: [38][140/233]	Loss 0.0291 (0.0488)	
training:	Epoch: [38][141/233]	Loss 0.0169 (0.0486)	
training:	Epoch: [38][142/233]	Loss 0.1299 (0.0491)	
training:	Epoch: [38][143/233]	Loss 0.0102 (0.0489)	
training:	Epoch: [38][144/233]	Loss 0.0095 (0.0486)	
training:	Epoch: [38][145/233]	Loss 0.0117 (0.0483)	
training:	Epoch: [38][146/233]	Loss 0.0090 (0.0481)	
training:	Epoch: [38][147/233]	Loss 0.4545 (0.0508)	
training:	Epoch: [38][148/233]	Loss 0.0137 (0.0506)	
training:	Epoch: [38][149/233]	Loss 0.0116 (0.0503)	
training:	Epoch: [38][150/233]	Loss 0.1812 (0.0512)	
training:	Epoch: [38][151/233]	Loss 0.0159 (0.0510)	
training:	Epoch: [38][152/233]	Loss 0.1401 (0.0515)	
training:	Epoch: [38][153/233]	Loss 0.1680 (0.0523)	
training:	Epoch: [38][154/233]	Loss 0.0081 (0.0520)	
training:	Epoch: [38][155/233]	Loss 0.0132 (0.0518)	
training:	Epoch: [38][156/233]	Loss 0.0094 (0.0515)	
training:	Epoch: [38][157/233]	Loss 0.0109 (0.0512)	
training:	Epoch: [38][158/233]	Loss 0.0077 (0.0510)	
training:	Epoch: [38][159/233]	Loss 0.0087 (0.0507)	
training:	Epoch: [38][160/233]	Loss 0.0095 (0.0504)	
training:	Epoch: [38][161/233]	Loss 0.0152 (0.0502)	
training:	Epoch: [38][162/233]	Loss 0.1550 (0.0509)	
training:	Epoch: [38][163/233]	Loss 0.0074 (0.0506)	
training:	Epoch: [38][164/233]	Loss 0.0078 (0.0503)	
training:	Epoch: [38][165/233]	Loss 0.0088 (0.0501)	
training:	Epoch: [38][166/233]	Loss 0.0276 (0.0499)	
training:	Epoch: [38][167/233]	Loss 0.1737 (0.0507)	
training:	Epoch: [38][168/233]	Loss 0.0125 (0.0505)	
training:	Epoch: [38][169/233]	Loss 0.0165 (0.0503)	
training:	Epoch: [38][170/233]	Loss 0.1564 (0.0509)	
training:	Epoch: [38][171/233]	Loss 0.0156 (0.0507)	
training:	Epoch: [38][172/233]	Loss 0.1789 (0.0514)	
training:	Epoch: [38][173/233]	Loss 0.0175 (0.0512)	
training:	Epoch: [38][174/233]	Loss 0.0567 (0.0513)	
training:	Epoch: [38][175/233]	Loss 0.0107 (0.0510)	
training:	Epoch: [38][176/233]	Loss 0.1861 (0.0518)	
training:	Epoch: [38][177/233]	Loss 0.0073 (0.0515)	
training:	Epoch: [38][178/233]	Loss 0.0077 (0.0513)	
training:	Epoch: [38][179/233]	Loss 0.0120 (0.0511)	
training:	Epoch: [38][180/233]	Loss 0.0096 (0.0508)	
training:	Epoch: [38][181/233]	Loss 0.0446 (0.0508)	
training:	Epoch: [38][182/233]	Loss 0.0109 (0.0506)	
training:	Epoch: [38][183/233]	Loss 0.1113 (0.0509)	
training:	Epoch: [38][184/233]	Loss 0.0080 (0.0507)	
training:	Epoch: [38][185/233]	Loss 0.0088 (0.0505)	
training:	Epoch: [38][186/233]	Loss 0.0240 (0.0503)	
training:	Epoch: [38][187/233]	Loss 0.1421 (0.0508)	
training:	Epoch: [38][188/233]	Loss 0.0101 (0.0506)	
training:	Epoch: [38][189/233]	Loss 0.0444 (0.0506)	
training:	Epoch: [38][190/233]	Loss 0.0096 (0.0503)	
training:	Epoch: [38][191/233]	Loss 0.0078 (0.0501)	
training:	Epoch: [38][192/233]	Loss 0.2033 (0.0509)	
training:	Epoch: [38][193/233]	Loss 0.0275 (0.0508)	
training:	Epoch: [38][194/233]	Loss 0.0070 (0.0506)	
training:	Epoch: [38][195/233]	Loss 0.1648 (0.0512)	
training:	Epoch: [38][196/233]	Loss 0.1522 (0.0517)	
training:	Epoch: [38][197/233]	Loss 0.0531 (0.0517)	
training:	Epoch: [38][198/233]	Loss 0.0748 (0.0518)	
training:	Epoch: [38][199/233]	Loss 0.0080 (0.0516)	
training:	Epoch: [38][200/233]	Loss 0.0150 (0.0514)	
training:	Epoch: [38][201/233]	Loss 0.0121 (0.0512)	
training:	Epoch: [38][202/233]	Loss 0.0070 (0.0510)	
training:	Epoch: [38][203/233]	Loss 0.0069 (0.0508)	
training:	Epoch: [38][204/233]	Loss 0.0328 (0.0507)	
training:	Epoch: [38][205/233]	Loss 0.0560 (0.0507)	
training:	Epoch: [38][206/233]	Loss 0.0084 (0.0505)	
training:	Epoch: [38][207/233]	Loss 0.0155 (0.0503)	
training:	Epoch: [38][208/233]	Loss 0.0218 (0.0502)	
training:	Epoch: [38][209/233]	Loss 0.0143 (0.0500)	
training:	Epoch: [38][210/233]	Loss 0.0088 (0.0498)	
training:	Epoch: [38][211/233]	Loss 0.0090 (0.0496)	
training:	Epoch: [38][212/233]	Loss 0.0126 (0.0495)	
training:	Epoch: [38][213/233]	Loss 0.0342 (0.0494)	
training:	Epoch: [38][214/233]	Loss 0.0242 (0.0493)	
training:	Epoch: [38][215/233]	Loss 0.1608 (0.0498)	
training:	Epoch: [38][216/233]	Loss 0.0198 (0.0496)	
training:	Epoch: [38][217/233]	Loss 0.0272 (0.0495)	
training:	Epoch: [38][218/233]	Loss 0.0087 (0.0494)	
training:	Epoch: [38][219/233]	Loss 0.0073 (0.0492)	
training:	Epoch: [38][220/233]	Loss 0.0147 (0.0490)	
training:	Epoch: [38][221/233]	Loss 0.0091 (0.0488)	
training:	Epoch: [38][222/233]	Loss 0.0100 (0.0487)	
training:	Epoch: [38][223/233]	Loss 0.0158 (0.0485)	
training:	Epoch: [38][224/233]	Loss 0.0137 (0.0483)	
training:	Epoch: [38][225/233]	Loss 0.0487 (0.0484)	
training:	Epoch: [38][226/233]	Loss 0.0078 (0.0482)	
training:	Epoch: [38][227/233]	Loss 0.0137 (0.0480)	
training:	Epoch: [38][228/233]	Loss 0.0114 (0.0479)	
training:	Epoch: [38][229/233]	Loss 0.0076 (0.0477)	
training:	Epoch: [38][230/233]	Loss 0.0084 (0.0475)	
training:	Epoch: [38][231/233]	Loss 0.0642 (0.0476)	
training:	Epoch: [38][232/233]	Loss 0.1256 (0.0479)	
training:	Epoch: [38][233/233]	Loss 0.0825 (0.0481)	
Training:	 Loss: 0.0480

Training:	 ACC: 0.9953 0.9953 0.9949 0.9958
Validation:	 ACC: 0.7966 0.7978 0.8202 0.7730
Validation:	 Best_BACC: 0.8162 0.8154 0.7998 0.8326
Validation:	 Loss: 0.7851
Pretraining:	Epoch 39/200
----------
training:	Epoch: [39][1/233]	Loss 0.0770 (0.0770)	
training:	Epoch: [39][2/233]	Loss 0.0316 (0.0543)	
training:	Epoch: [39][3/233]	Loss 0.0058 (0.0381)	
training:	Epoch: [39][4/233]	Loss 0.0226 (0.0343)	
training:	Epoch: [39][5/233]	Loss 0.0205 (0.0315)	
training:	Epoch: [39][6/233]	Loss 0.0081 (0.0276)	
training:	Epoch: [39][7/233]	Loss 0.0725 (0.0340)	
training:	Epoch: [39][8/233]	Loss 0.1865 (0.0531)	
training:	Epoch: [39][9/233]	Loss 0.0058 (0.0478)	
training:	Epoch: [39][10/233]	Loss 0.0333 (0.0464)	
training:	Epoch: [39][11/233]	Loss 0.0286 (0.0448)	
training:	Epoch: [39][12/233]	Loss 0.0085 (0.0417)	
training:	Epoch: [39][13/233]	Loss 0.0096 (0.0393)	
training:	Epoch: [39][14/233]	Loss 0.0090 (0.0371)	
training:	Epoch: [39][15/233]	Loss 0.0145 (0.0356)	
training:	Epoch: [39][16/233]	Loss 0.1114 (0.0403)	
training:	Epoch: [39][17/233]	Loss 0.0491 (0.0408)	
training:	Epoch: [39][18/233]	Loss 0.0217 (0.0398)	
training:	Epoch: [39][19/233]	Loss 0.0666 (0.0412)	
training:	Epoch: [39][20/233]	Loss 0.1755 (0.0479)	
training:	Epoch: [39][21/233]	Loss 0.0290 (0.0470)	
training:	Epoch: [39][22/233]	Loss 0.0128 (0.0455)	
training:	Epoch: [39][23/233]	Loss 0.0146 (0.0441)	
training:	Epoch: [39][24/233]	Loss 0.0139 (0.0429)	
training:	Epoch: [39][25/233]	Loss 0.0111 (0.0416)	
training:	Epoch: [39][26/233]	Loss 0.0084 (0.0403)	
training:	Epoch: [39][27/233]	Loss 0.0080 (0.0391)	
training:	Epoch: [39][28/233]	Loss 0.0108 (0.0381)	
training:	Epoch: [39][29/233]	Loss 0.1661 (0.0425)	
training:	Epoch: [39][30/233]	Loss 0.0077 (0.0413)	
training:	Epoch: [39][31/233]	Loss 0.0087 (0.0403)	
training:	Epoch: [39][32/233]	Loss 0.0192 (0.0396)	
training:	Epoch: [39][33/233]	Loss 0.0065 (0.0386)	
training:	Epoch: [39][34/233]	Loss 0.0107 (0.0378)	
training:	Epoch: [39][35/233]	Loss 0.0114 (0.0371)	
training:	Epoch: [39][36/233]	Loss 0.0119 (0.0364)	
training:	Epoch: [39][37/233]	Loss 0.0101 (0.0356)	
training:	Epoch: [39][38/233]	Loss 0.0232 (0.0353)	
training:	Epoch: [39][39/233]	Loss 0.0064 (0.0346)	
training:	Epoch: [39][40/233]	Loss 0.1135 (0.0366)	
training:	Epoch: [39][41/233]	Loss 0.0064 (0.0358)	
training:	Epoch: [39][42/233]	Loss 0.0112 (0.0352)	
training:	Epoch: [39][43/233]	Loss 0.0072 (0.0346)	
training:	Epoch: [39][44/233]	Loss 0.1789 (0.0379)	
training:	Epoch: [39][45/233]	Loss 0.0091 (0.0372)	
training:	Epoch: [39][46/233]	Loss 0.0088 (0.0366)	
training:	Epoch: [39][47/233]	Loss 0.0332 (0.0365)	
training:	Epoch: [39][48/233]	Loss 0.3130 (0.0423)	
training:	Epoch: [39][49/233]	Loss 0.0283 (0.0420)	
training:	Epoch: [39][50/233]	Loss 0.0077 (0.0413)	
training:	Epoch: [39][51/233]	Loss 0.0115 (0.0407)	
training:	Epoch: [39][52/233]	Loss 0.0109 (0.0402)	
training:	Epoch: [39][53/233]	Loss 0.0106 (0.0396)	
training:	Epoch: [39][54/233]	Loss 0.0416 (0.0396)	
training:	Epoch: [39][55/233]	Loss 0.1739 (0.0421)	
training:	Epoch: [39][56/233]	Loss 0.2496 (0.0458)	
training:	Epoch: [39][57/233]	Loss 0.0263 (0.0454)	
training:	Epoch: [39][58/233]	Loss 0.0397 (0.0453)	
training:	Epoch: [39][59/233]	Loss 0.0176 (0.0449)	
training:	Epoch: [39][60/233]	Loss 0.0086 (0.0443)	
training:	Epoch: [39][61/233]	Loss 0.0226 (0.0439)	
training:	Epoch: [39][62/233]	Loss 0.0194 (0.0435)	
training:	Epoch: [39][63/233]	Loss 0.0195 (0.0431)	
training:	Epoch: [39][64/233]	Loss 0.0131 (0.0427)	
training:	Epoch: [39][65/233]	Loss 0.0081 (0.0421)	
training:	Epoch: [39][66/233]	Loss 0.0156 (0.0417)	
training:	Epoch: [39][67/233]	Loss 0.0080 (0.0412)	
training:	Epoch: [39][68/233]	Loss 0.0077 (0.0407)	
training:	Epoch: [39][69/233]	Loss 0.0118 (0.0403)	
training:	Epoch: [39][70/233]	Loss 0.1036 (0.0412)	
training:	Epoch: [39][71/233]	Loss 0.0101 (0.0408)	
training:	Epoch: [39][72/233]	Loss 0.0164 (0.0404)	
training:	Epoch: [39][73/233]	Loss 0.0210 (0.0402)	
training:	Epoch: [39][74/233]	Loss 0.0445 (0.0402)	
training:	Epoch: [39][75/233]	Loss 0.0080 (0.0398)	
training:	Epoch: [39][76/233]	Loss 0.0084 (0.0394)	
training:	Epoch: [39][77/233]	Loss 0.0084 (0.0390)	
training:	Epoch: [39][78/233]	Loss 0.0083 (0.0386)	
training:	Epoch: [39][79/233]	Loss 0.0132 (0.0383)	
training:	Epoch: [39][80/233]	Loss 0.0073 (0.0379)	
training:	Epoch: [39][81/233]	Loss 0.0111 (0.0376)	
training:	Epoch: [39][82/233]	Loss 0.0521 (0.0377)	
training:	Epoch: [39][83/233]	Loss 0.0768 (0.0382)	
training:	Epoch: [39][84/233]	Loss 0.0094 (0.0379)	
training:	Epoch: [39][85/233]	Loss 0.0064 (0.0375)	
training:	Epoch: [39][86/233]	Loss 0.0106 (0.0372)	
training:	Epoch: [39][87/233]	Loss 0.0080 (0.0368)	
training:	Epoch: [39][88/233]	Loss 0.0174 (0.0366)	
training:	Epoch: [39][89/233]	Loss 0.2741 (0.0393)	
training:	Epoch: [39][90/233]	Loss 0.0098 (0.0390)	
training:	Epoch: [39][91/233]	Loss 0.1467 (0.0402)	
training:	Epoch: [39][92/233]	Loss 0.0108 (0.0398)	
training:	Epoch: [39][93/233]	Loss 0.0151 (0.0396)	
training:	Epoch: [39][94/233]	Loss 0.0076 (0.0392)	
training:	Epoch: [39][95/233]	Loss 0.0202 (0.0390)	
training:	Epoch: [39][96/233]	Loss 0.0149 (0.0388)	
training:	Epoch: [39][97/233]	Loss 0.0199 (0.0386)	
training:	Epoch: [39][98/233]	Loss 0.0118 (0.0383)	
training:	Epoch: [39][99/233]	Loss 0.1700 (0.0396)	
training:	Epoch: [39][100/233]	Loss 0.1297 (0.0405)	
training:	Epoch: [39][101/233]	Loss 0.0103 (0.0402)	
training:	Epoch: [39][102/233]	Loss 0.0097 (0.0399)	
training:	Epoch: [39][103/233]	Loss 0.0156 (0.0397)	
training:	Epoch: [39][104/233]	Loss 0.0230 (0.0395)	
training:	Epoch: [39][105/233]	Loss 0.0087 (0.0392)	
training:	Epoch: [39][106/233]	Loss 0.0097 (0.0390)	
training:	Epoch: [39][107/233]	Loss 0.0083 (0.0387)	
training:	Epoch: [39][108/233]	Loss 0.0119 (0.0384)	
training:	Epoch: [39][109/233]	Loss 0.0106 (0.0382)	
training:	Epoch: [39][110/233]	Loss 0.0081 (0.0379)	
training:	Epoch: [39][111/233]	Loss 0.0060 (0.0376)	
training:	Epoch: [39][112/233]	Loss 0.0080 (0.0374)	
training:	Epoch: [39][113/233]	Loss 0.1026 (0.0379)	
training:	Epoch: [39][114/233]	Loss 0.3429 (0.0406)	
training:	Epoch: [39][115/233]	Loss 0.0084 (0.0403)	
training:	Epoch: [39][116/233]	Loss 0.0079 (0.0400)	
training:	Epoch: [39][117/233]	Loss 0.0200 (0.0399)	
training:	Epoch: [39][118/233]	Loss 0.0152 (0.0397)	
training:	Epoch: [39][119/233]	Loss 0.0081 (0.0394)	
training:	Epoch: [39][120/233]	Loss 0.0087 (0.0391)	
training:	Epoch: [39][121/233]	Loss 0.0171 (0.0390)	
training:	Epoch: [39][122/233]	Loss 0.0073 (0.0387)	
training:	Epoch: [39][123/233]	Loss 0.0259 (0.0386)	
training:	Epoch: [39][124/233]	Loss 0.0700 (0.0389)	
training:	Epoch: [39][125/233]	Loss 0.0112 (0.0386)	
training:	Epoch: [39][126/233]	Loss 0.0119 (0.0384)	
training:	Epoch: [39][127/233]	Loss 0.0061 (0.0382)	
training:	Epoch: [39][128/233]	Loss 0.1426 (0.0390)	
training:	Epoch: [39][129/233]	Loss 0.0183 (0.0388)	
training:	Epoch: [39][130/233]	Loss 0.0281 (0.0387)	
training:	Epoch: [39][131/233]	Loss 0.0172 (0.0386)	
training:	Epoch: [39][132/233]	Loss 0.0079 (0.0383)	
training:	Epoch: [39][133/233]	Loss 0.0109 (0.0381)	
training:	Epoch: [39][134/233]	Loss 0.0168 (0.0380)	
training:	Epoch: [39][135/233]	Loss 0.0073 (0.0378)	
training:	Epoch: [39][136/233]	Loss 0.0072 (0.0375)	
training:	Epoch: [39][137/233]	Loss 0.0079 (0.0373)	
training:	Epoch: [39][138/233]	Loss 0.0085 (0.0371)	
training:	Epoch: [39][139/233]	Loss 0.0447 (0.0372)	
training:	Epoch: [39][140/233]	Loss 0.0228 (0.0371)	
training:	Epoch: [39][141/233]	Loss 0.0086 (0.0369)	
training:	Epoch: [39][142/233]	Loss 0.0079 (0.0366)	
training:	Epoch: [39][143/233]	Loss 0.0319 (0.0366)	
training:	Epoch: [39][144/233]	Loss 0.1687 (0.0375)	
training:	Epoch: [39][145/233]	Loss 0.0150 (0.0374)	
training:	Epoch: [39][146/233]	Loss 0.0082 (0.0372)	
training:	Epoch: [39][147/233]	Loss 0.0200 (0.0371)	
training:	Epoch: [39][148/233]	Loss 0.0505 (0.0371)	
training:	Epoch: [39][149/233]	Loss 0.0084 (0.0370)	
training:	Epoch: [39][150/233]	Loss 0.0078 (0.0368)	
training:	Epoch: [39][151/233]	Loss 0.0275 (0.0367)	
training:	Epoch: [39][152/233]	Loss 0.0082 (0.0365)	
training:	Epoch: [39][153/233]	Loss 0.3578 (0.0386)	
training:	Epoch: [39][154/233]	Loss 0.0097 (0.0384)	
training:	Epoch: [39][155/233]	Loss 0.0061 (0.0382)	
training:	Epoch: [39][156/233]	Loss 0.1674 (0.0390)	
training:	Epoch: [39][157/233]	Loss 0.1684 (0.0399)	
training:	Epoch: [39][158/233]	Loss 0.0103 (0.0397)	
training:	Epoch: [39][159/233]	Loss 0.0080 (0.0395)	
training:	Epoch: [39][160/233]	Loss 0.0067 (0.0393)	
training:	Epoch: [39][161/233]	Loss 0.0166 (0.0391)	
training:	Epoch: [39][162/233]	Loss 0.0112 (0.0390)	
training:	Epoch: [39][163/233]	Loss 0.0085 (0.0388)	
training:	Epoch: [39][164/233]	Loss 0.0070 (0.0386)	
training:	Epoch: [39][165/233]	Loss 0.0088 (0.0384)	
training:	Epoch: [39][166/233]	Loss 0.0101 (0.0382)	
training:	Epoch: [39][167/233]	Loss 0.0082 (0.0381)	
training:	Epoch: [39][168/233]	Loss 0.0081 (0.0379)	
training:	Epoch: [39][169/233]	Loss 0.0133 (0.0377)	
training:	Epoch: [39][170/233]	Loss 0.0062 (0.0375)	
training:	Epoch: [39][171/233]	Loss 0.0163 (0.0374)	
training:	Epoch: [39][172/233]	Loss 0.1055 (0.0378)	
training:	Epoch: [39][173/233]	Loss 0.0080 (0.0376)	
training:	Epoch: [39][174/233]	Loss 0.0089 (0.0375)	
training:	Epoch: [39][175/233]	Loss 0.0204 (0.0374)	
training:	Epoch: [39][176/233]	Loss 0.0267 (0.0373)	
training:	Epoch: [39][177/233]	Loss 0.0069 (0.0371)	
training:	Epoch: [39][178/233]	Loss 0.0127 (0.0370)	
training:	Epoch: [39][179/233]	Loss 0.0065 (0.0368)	
training:	Epoch: [39][180/233]	Loss 0.0099 (0.0367)	
training:	Epoch: [39][181/233]	Loss 0.0367 (0.0367)	
training:	Epoch: [39][182/233]	Loss 0.0160 (0.0366)	
training:	Epoch: [39][183/233]	Loss 0.0690 (0.0368)	
training:	Epoch: [39][184/233]	Loss 0.0105 (0.0366)	
training:	Epoch: [39][185/233]	Loss 0.0378 (0.0366)	
training:	Epoch: [39][186/233]	Loss 0.1766 (0.0374)	
training:	Epoch: [39][187/233]	Loss 0.1549 (0.0380)	
training:	Epoch: [39][188/233]	Loss 0.0105 (0.0379)	
training:	Epoch: [39][189/233]	Loss 0.0101 (0.0377)	
training:	Epoch: [39][190/233]	Loss 0.0263 (0.0376)	
training:	Epoch: [39][191/233]	Loss 0.0119 (0.0375)	
training:	Epoch: [39][192/233]	Loss 0.0113 (0.0374)	
training:	Epoch: [39][193/233]	Loss 0.0416 (0.0374)	
training:	Epoch: [39][194/233]	Loss 0.0063 (0.0372)	
training:	Epoch: [39][195/233]	Loss 0.0083 (0.0371)	
training:	Epoch: [39][196/233]	Loss 0.0101 (0.0369)	
training:	Epoch: [39][197/233]	Loss 0.0431 (0.0370)	
training:	Epoch: [39][198/233]	Loss 0.1096 (0.0373)	
training:	Epoch: [39][199/233]	Loss 0.0148 (0.0372)	
training:	Epoch: [39][200/233]	Loss 0.1663 (0.0379)	
training:	Epoch: [39][201/233]	Loss 0.0088 (0.0377)	
training:	Epoch: [39][202/233]	Loss 0.0079 (0.0376)	
training:	Epoch: [39][203/233]	Loss 0.0068 (0.0374)	
training:	Epoch: [39][204/233]	Loss 0.1733 (0.0381)	
training:	Epoch: [39][205/233]	Loss 0.0109 (0.0380)	
training:	Epoch: [39][206/233]	Loss 0.0106 (0.0378)	
training:	Epoch: [39][207/233]	Loss 0.0124 (0.0377)	
training:	Epoch: [39][208/233]	Loss 0.0106 (0.0376)	
training:	Epoch: [39][209/233]	Loss 0.0142 (0.0375)	
training:	Epoch: [39][210/233]	Loss 0.0100 (0.0373)	
training:	Epoch: [39][211/233]	Loss 0.1680 (0.0380)	
training:	Epoch: [39][212/233]	Loss 0.1012 (0.0383)	
training:	Epoch: [39][213/233]	Loss 0.0066 (0.0381)	
training:	Epoch: [39][214/233]	Loss 0.0100 (0.0380)	
training:	Epoch: [39][215/233]	Loss 0.0470 (0.0380)	
training:	Epoch: [39][216/233]	Loss 0.2223 (0.0389)	
training:	Epoch: [39][217/233]	Loss 0.0097 (0.0387)	
training:	Epoch: [39][218/233]	Loss 0.0141 (0.0386)	
training:	Epoch: [39][219/233]	Loss 0.1614 (0.0392)	
training:	Epoch: [39][220/233]	Loss 0.0133 (0.0391)	
training:	Epoch: [39][221/233]	Loss 0.0154 (0.0390)	
training:	Epoch: [39][222/233]	Loss 0.1837 (0.0396)	
training:	Epoch: [39][223/233]	Loss 0.0201 (0.0395)	
training:	Epoch: [39][224/233]	Loss 0.0333 (0.0395)	
training:	Epoch: [39][225/233]	Loss 0.0251 (0.0394)	
training:	Epoch: [39][226/233]	Loss 0.0078 (0.0393)	
training:	Epoch: [39][227/233]	Loss 0.0076 (0.0392)	
training:	Epoch: [39][228/233]	Loss 0.3091 (0.0403)	
training:	Epoch: [39][229/233]	Loss 0.0075 (0.0402)	
training:	Epoch: [39][230/233]	Loss 0.0077 (0.0401)	
training:	Epoch: [39][231/233]	Loss 0.1522 (0.0405)	
training:	Epoch: [39][232/233]	Loss 0.0138 (0.0404)	
training:	Epoch: [39][233/233]	Loss 0.0152 (0.0403)	
Training:	 Loss: 0.0402

Training:	 ACC: 0.9958 0.9957 0.9946 0.9969
Validation:	 ACC: 0.7975 0.7961 0.7692 0.8258
Validation:	 Best_BACC: 0.8162 0.8154 0.7998 0.8326
Validation:	 Loss: 0.8047
Pretraining:	Epoch 40/200
----------
training:	Epoch: [40][1/233]	Loss 0.0092 (0.0092)	
training:	Epoch: [40][2/233]	Loss 0.1664 (0.0878)	
training:	Epoch: [40][3/233]	Loss 0.0094 (0.0617)	
training:	Epoch: [40][4/233]	Loss 0.0129 (0.0495)	
training:	Epoch: [40][5/233]	Loss 0.3333 (0.1063)	
training:	Epoch: [40][6/233]	Loss 0.0133 (0.0908)	
training:	Epoch: [40][7/233]	Loss 0.0141 (0.0798)	
training:	Epoch: [40][8/233]	Loss 0.0070 (0.0707)	
training:	Epoch: [40][9/233]	Loss 0.0069 (0.0636)	
training:	Epoch: [40][10/233]	Loss 0.0176 (0.0590)	
training:	Epoch: [40][11/233]	Loss 0.0247 (0.0559)	
training:	Epoch: [40][12/233]	Loss 0.0264 (0.0535)	
training:	Epoch: [40][13/233]	Loss 0.0094 (0.0501)	
training:	Epoch: [40][14/233]	Loss 0.0097 (0.0472)	
training:	Epoch: [40][15/233]	Loss 0.0199 (0.0454)	
training:	Epoch: [40][16/233]	Loss 0.0280 (0.0443)	
training:	Epoch: [40][17/233]	Loss 0.0200 (0.0429)	
training:	Epoch: [40][18/233]	Loss 0.0510 (0.0433)	
training:	Epoch: [40][19/233]	Loss 0.0069 (0.0414)	
training:	Epoch: [40][20/233]	Loss 0.0105 (0.0398)	
training:	Epoch: [40][21/233]	Loss 0.0121 (0.0385)	
training:	Epoch: [40][22/233]	Loss 0.0287 (0.0381)	
training:	Epoch: [40][23/233]	Loss 0.3323 (0.0509)	
training:	Epoch: [40][24/233]	Loss 0.0098 (0.0492)	
training:	Epoch: [40][25/233]	Loss 0.0104 (0.0476)	
training:	Epoch: [40][26/233]	Loss 0.0106 (0.0462)	
training:	Epoch: [40][27/233]	Loss 0.0922 (0.0479)	
training:	Epoch: [40][28/233]	Loss 0.0284 (0.0472)	
training:	Epoch: [40][29/233]	Loss 0.0081 (0.0458)	
training:	Epoch: [40][30/233]	Loss 0.0060 (0.0445)	
training:	Epoch: [40][31/233]	Loss 0.0141 (0.0435)	
training:	Epoch: [40][32/233]	Loss 0.1753 (0.0476)	
training:	Epoch: [40][33/233]	Loss 0.0075 (0.0464)	
training:	Epoch: [40][34/233]	Loss 0.0136 (0.0455)	
training:	Epoch: [40][35/233]	Loss 0.0082 (0.0444)	
training:	Epoch: [40][36/233]	Loss 0.0116 (0.0435)	
training:	Epoch: [40][37/233]	Loss 0.2779 (0.0498)	
training:	Epoch: [40][38/233]	Loss 0.0072 (0.0487)	
training:	Epoch: [40][39/233]	Loss 0.0127 (0.0478)	
training:	Epoch: [40][40/233]	Loss 0.0239 (0.0472)	
training:	Epoch: [40][41/233]	Loss 0.0544 (0.0474)	
training:	Epoch: [40][42/233]	Loss 0.0080 (0.0464)	
training:	Epoch: [40][43/233]	Loss 0.0281 (0.0460)	
training:	Epoch: [40][44/233]	Loss 0.0140 (0.0453)	
training:	Epoch: [40][45/233]	Loss 0.0365 (0.0451)	
training:	Epoch: [40][46/233]	Loss 0.0080 (0.0443)	
training:	Epoch: [40][47/233]	Loss 0.0086 (0.0435)	
training:	Epoch: [40][48/233]	Loss 0.0535 (0.0437)	
training:	Epoch: [40][49/233]	Loss 0.1270 (0.0454)	
training:	Epoch: [40][50/233]	Loss 0.0228 (0.0450)	
training:	Epoch: [40][51/233]	Loss 0.0070 (0.0442)	
training:	Epoch: [40][52/233]	Loss 0.0072 (0.0435)	
training:	Epoch: [40][53/233]	Loss 0.1559 (0.0456)	
training:	Epoch: [40][54/233]	Loss 0.0077 (0.0449)	
training:	Epoch: [40][55/233]	Loss 0.0184 (0.0444)	
training:	Epoch: [40][56/233]	Loss 0.0340 (0.0443)	
training:	Epoch: [40][57/233]	Loss 0.0074 (0.0436)	
training:	Epoch: [40][58/233]	Loss 0.0104 (0.0430)	
training:	Epoch: [40][59/233]	Loss 0.1467 (0.0448)	
training:	Epoch: [40][60/233]	Loss 0.0092 (0.0442)	
training:	Epoch: [40][61/233]	Loss 0.0085 (0.0436)	
training:	Epoch: [40][62/233]	Loss 0.0076 (0.0430)	
training:	Epoch: [40][63/233]	Loss 0.0070 (0.0425)	
training:	Epoch: [40][64/233]	Loss 0.0225 (0.0422)	
training:	Epoch: [40][65/233]	Loss 0.0074 (0.0416)	
training:	Epoch: [40][66/233]	Loss 0.1189 (0.0428)	
training:	Epoch: [40][67/233]	Loss 0.0164 (0.0424)	
training:	Epoch: [40][68/233]	Loss 0.0101 (0.0419)	
training:	Epoch: [40][69/233]	Loss 0.0181 (0.0416)	
training:	Epoch: [40][70/233]	Loss 0.0072 (0.0411)	
training:	Epoch: [40][71/233]	Loss 0.0113 (0.0407)	
training:	Epoch: [40][72/233]	Loss 0.0104 (0.0402)	
training:	Epoch: [40][73/233]	Loss 0.0078 (0.0398)	
training:	Epoch: [40][74/233]	Loss 0.0384 (0.0398)	
training:	Epoch: [40][75/233]	Loss 0.0069 (0.0393)	
training:	Epoch: [40][76/233]	Loss 0.0075 (0.0389)	
training:	Epoch: [40][77/233]	Loss 0.0066 (0.0385)	
training:	Epoch: [40][78/233]	Loss 0.0129 (0.0382)	
training:	Epoch: [40][79/233]	Loss 0.0069 (0.0378)	
training:	Epoch: [40][80/233]	Loss 0.0109 (0.0374)	
training:	Epoch: [40][81/233]	Loss 0.0101 (0.0371)	
training:	Epoch: [40][82/233]	Loss 0.0083 (0.0368)	
training:	Epoch: [40][83/233]	Loss 0.0109 (0.0364)	
training:	Epoch: [40][84/233]	Loss 0.0346 (0.0364)	
training:	Epoch: [40][85/233]	Loss 0.0309 (0.0364)	
training:	Epoch: [40][86/233]	Loss 0.0195 (0.0362)	
training:	Epoch: [40][87/233]	Loss 0.0095 (0.0359)	
training:	Epoch: [40][88/233]	Loss 0.0075 (0.0355)	
training:	Epoch: [40][89/233]	Loss 0.0088 (0.0352)	
training:	Epoch: [40][90/233]	Loss 0.0136 (0.0350)	
training:	Epoch: [40][91/233]	Loss 0.1345 (0.0361)	
training:	Epoch: [40][92/233]	Loss 0.0084 (0.0358)	
training:	Epoch: [40][93/233]	Loss 0.0224 (0.0356)	
training:	Epoch: [40][94/233]	Loss 0.0106 (0.0354)	
training:	Epoch: [40][95/233]	Loss 0.1385 (0.0365)	
training:	Epoch: [40][96/233]	Loss 0.0078 (0.0362)	
training:	Epoch: [40][97/233]	Loss 0.0145 (0.0359)	
training:	Epoch: [40][98/233]	Loss 0.0088 (0.0357)	
training:	Epoch: [40][99/233]	Loss 0.0092 (0.0354)	
training:	Epoch: [40][100/233]	Loss 0.0875 (0.0359)	
training:	Epoch: [40][101/233]	Loss 0.1202 (0.0367)	
training:	Epoch: [40][102/233]	Loss 0.0956 (0.0373)	
training:	Epoch: [40][103/233]	Loss 0.0073 (0.0370)	
training:	Epoch: [40][104/233]	Loss 0.0087 (0.0368)	
training:	Epoch: [40][105/233]	Loss 0.0125 (0.0365)	
training:	Epoch: [40][106/233]	Loss 0.0158 (0.0363)	
training:	Epoch: [40][107/233]	Loss 0.0070 (0.0361)	
training:	Epoch: [40][108/233]	Loss 0.0118 (0.0358)	
training:	Epoch: [40][109/233]	Loss 0.0141 (0.0356)	
training:	Epoch: [40][110/233]	Loss 0.0142 (0.0354)	
training:	Epoch: [40][111/233]	Loss 0.0074 (0.0352)	
training:	Epoch: [40][112/233]	Loss 0.0061 (0.0349)	
training:	Epoch: [40][113/233]	Loss 0.0090 (0.0347)	
training:	Epoch: [40][114/233]	Loss 0.0075 (0.0345)	
training:	Epoch: [40][115/233]	Loss 0.0060 (0.0342)	
training:	Epoch: [40][116/233]	Loss 0.1577 (0.0353)	
training:	Epoch: [40][117/233]	Loss 0.0064 (0.0350)	
training:	Epoch: [40][118/233]	Loss 0.0074 (0.0348)	
training:	Epoch: [40][119/233]	Loss 0.0179 (0.0347)	
training:	Epoch: [40][120/233]	Loss 0.0076 (0.0344)	
training:	Epoch: [40][121/233]	Loss 0.3103 (0.0367)	
training:	Epoch: [40][122/233]	Loss 0.0167 (0.0365)	
training:	Epoch: [40][123/233]	Loss 0.1939 (0.0378)	
training:	Epoch: [40][124/233]	Loss 0.0075 (0.0376)	
training:	Epoch: [40][125/233]	Loss 0.0148 (0.0374)	
training:	Epoch: [40][126/233]	Loss 0.0104 (0.0372)	
training:	Epoch: [40][127/233]	Loss 0.0602 (0.0374)	
training:	Epoch: [40][128/233]	Loss 0.0068 (0.0371)	
training:	Epoch: [40][129/233]	Loss 0.0265 (0.0370)	
training:	Epoch: [40][130/233]	Loss 0.0095 (0.0368)	
training:	Epoch: [40][131/233]	Loss 0.1762 (0.0379)	
training:	Epoch: [40][132/233]	Loss 0.0079 (0.0377)	
training:	Epoch: [40][133/233]	Loss 0.0081 (0.0374)	
training:	Epoch: [40][134/233]	Loss 0.1169 (0.0380)	
training:	Epoch: [40][135/233]	Loss 0.0098 (0.0378)	
training:	Epoch: [40][136/233]	Loss 0.1567 (0.0387)	
training:	Epoch: [40][137/233]	Loss 0.0069 (0.0385)	
training:	Epoch: [40][138/233]	Loss 0.0510 (0.0386)	
training:	Epoch: [40][139/233]	Loss 0.0082 (0.0383)	
training:	Epoch: [40][140/233]	Loss 0.1144 (0.0389)	
training:	Epoch: [40][141/233]	Loss 0.0078 (0.0387)	
training:	Epoch: [40][142/233]	Loss 0.0226 (0.0386)	
training:	Epoch: [40][143/233]	Loss 0.0081 (0.0383)	
training:	Epoch: [40][144/233]	Loss 0.0061 (0.0381)	
training:	Epoch: [40][145/233]	Loss 0.0077 (0.0379)	
training:	Epoch: [40][146/233]	Loss 0.0635 (0.0381)	
training:	Epoch: [40][147/233]	Loss 0.0374 (0.0381)	
training:	Epoch: [40][148/233]	Loss 0.0080 (0.0379)	
training:	Epoch: [40][149/233]	Loss 0.0956 (0.0383)	
training:	Epoch: [40][150/233]	Loss 0.0128 (0.0381)	
training:	Epoch: [40][151/233]	Loss 0.0145 (0.0379)	
training:	Epoch: [40][152/233]	Loss 0.0844 (0.0382)	
training:	Epoch: [40][153/233]	Loss 0.1839 (0.0392)	
training:	Epoch: [40][154/233]	Loss 0.0834 (0.0395)	
training:	Epoch: [40][155/233]	Loss 0.0062 (0.0393)	
training:	Epoch: [40][156/233]	Loss 0.0629 (0.0394)	
training:	Epoch: [40][157/233]	Loss 0.0064 (0.0392)	
training:	Epoch: [40][158/233]	Loss 0.0069 (0.0390)	
training:	Epoch: [40][159/233]	Loss 0.0114 (0.0388)	
training:	Epoch: [40][160/233]	Loss 0.0071 (0.0386)	
training:	Epoch: [40][161/233]	Loss 0.0080 (0.0384)	
training:	Epoch: [40][162/233]	Loss 0.0157 (0.0383)	
training:	Epoch: [40][163/233]	Loss 0.0089 (0.0381)	
training:	Epoch: [40][164/233]	Loss 0.0199 (0.0380)	
training:	Epoch: [40][165/233]	Loss 0.1126 (0.0385)	
training:	Epoch: [40][166/233]	Loss 0.2820 (0.0399)	
training:	Epoch: [40][167/233]	Loss 0.0140 (0.0398)	
training:	Epoch: [40][168/233]	Loss 0.0199 (0.0397)	
training:	Epoch: [40][169/233]	Loss 0.0215 (0.0395)	
training:	Epoch: [40][170/233]	Loss 0.0282 (0.0395)	
training:	Epoch: [40][171/233]	Loss 0.0074 (0.0393)	
training:	Epoch: [40][172/233]	Loss 0.0353 (0.0393)	
training:	Epoch: [40][173/233]	Loss 0.0162 (0.0391)	
training:	Epoch: [40][174/233]	Loss 0.0113 (0.0390)	
training:	Epoch: [40][175/233]	Loss 0.1658 (0.0397)	
training:	Epoch: [40][176/233]	Loss 0.1132 (0.0401)	
training:	Epoch: [40][177/233]	Loss 0.1499 (0.0407)	
training:	Epoch: [40][178/233]	Loss 0.0221 (0.0406)	
training:	Epoch: [40][179/233]	Loss 0.0119 (0.0405)	
training:	Epoch: [40][180/233]	Loss 0.0068 (0.0403)	
training:	Epoch: [40][181/233]	Loss 0.0385 (0.0403)	
training:	Epoch: [40][182/233]	Loss 0.0129 (0.0401)	
training:	Epoch: [40][183/233]	Loss 0.0151 (0.0400)	
training:	Epoch: [40][184/233]	Loss 0.0178 (0.0399)	
training:	Epoch: [40][185/233]	Loss 0.0077 (0.0397)	
training:	Epoch: [40][186/233]	Loss 0.0425 (0.0397)	
training:	Epoch: [40][187/233]	Loss 0.0416 (0.0397)	
training:	Epoch: [40][188/233]	Loss 0.2027 (0.0406)	
training:	Epoch: [40][189/233]	Loss 0.0110 (0.0404)	
training:	Epoch: [40][190/233]	Loss 0.0332 (0.0404)	
training:	Epoch: [40][191/233]	Loss 0.0076 (0.0402)	
training:	Epoch: [40][192/233]	Loss 0.0065 (0.0400)	
training:	Epoch: [40][193/233]	Loss 0.0111 (0.0399)	
training:	Epoch: [40][194/233]	Loss 0.1424 (0.0404)	
training:	Epoch: [40][195/233]	Loss 0.0170 (0.0403)	
training:	Epoch: [40][196/233]	Loss 0.0225 (0.0402)	
training:	Epoch: [40][197/233]	Loss 0.0103 (0.0401)	
training:	Epoch: [40][198/233]	Loss 0.0742 (0.0402)	
training:	Epoch: [40][199/233]	Loss 0.0142 (0.0401)	
training:	Epoch: [40][200/233]	Loss 0.0647 (0.0402)	
training:	Epoch: [40][201/233]	Loss 0.1547 (0.0408)	
training:	Epoch: [40][202/233]	Loss 0.0910 (0.0410)	
training:	Epoch: [40][203/233]	Loss 0.0082 (0.0409)	
training:	Epoch: [40][204/233]	Loss 0.0144 (0.0408)	
training:	Epoch: [40][205/233]	Loss 0.0077 (0.0406)	
training:	Epoch: [40][206/233]	Loss 0.0074 (0.0404)	
training:	Epoch: [40][207/233]	Loss 0.0634 (0.0405)	
training:	Epoch: [40][208/233]	Loss 0.0068 (0.0404)	
training:	Epoch: [40][209/233]	Loss 0.0239 (0.0403)	
training:	Epoch: [40][210/233]	Loss 0.0103 (0.0402)	
training:	Epoch: [40][211/233]	Loss 0.0062 (0.0400)	
training:	Epoch: [40][212/233]	Loss 0.0276 (0.0399)	
training:	Epoch: [40][213/233]	Loss 0.3253 (0.0413)	
training:	Epoch: [40][214/233]	Loss 0.0078 (0.0411)	
training:	Epoch: [40][215/233]	Loss 0.0081 (0.0410)	
training:	Epoch: [40][216/233]	Loss 0.0176 (0.0409)	
training:	Epoch: [40][217/233]	Loss 0.0079 (0.0407)	
training:	Epoch: [40][218/233]	Loss 0.0080 (0.0406)	
training:	Epoch: [40][219/233]	Loss 0.0200 (0.0405)	
training:	Epoch: [40][220/233]	Loss 0.0093 (0.0403)	
training:	Epoch: [40][221/233]	Loss 0.0081 (0.0402)	
training:	Epoch: [40][222/233]	Loss 0.0473 (0.0402)	
training:	Epoch: [40][223/233]	Loss 0.0072 (0.0401)	
training:	Epoch: [40][224/233]	Loss 0.0067 (0.0399)	
training:	Epoch: [40][225/233]	Loss 0.0238 (0.0398)	
training:	Epoch: [40][226/233]	Loss 0.0126 (0.0397)	
training:	Epoch: [40][227/233]	Loss 0.1215 (0.0401)	
training:	Epoch: [40][228/233]	Loss 0.0080 (0.0399)	
training:	Epoch: [40][229/233]	Loss 0.1834 (0.0406)	
training:	Epoch: [40][230/233]	Loss 0.0116 (0.0404)	
training:	Epoch: [40][231/233]	Loss 0.0114 (0.0403)	
training:	Epoch: [40][232/233]	Loss 0.0056 (0.0402)	
training:	Epoch: [40][233/233]	Loss 0.0164 (0.0401)	
Training:	 Loss: 0.0400

Training:	 ACC: 0.9958 0.9957 0.9949 0.9966
Validation:	 ACC: 0.8080 0.8079 0.8069 0.8090
Validation:	 Best_BACC: 0.8162 0.8154 0.7998 0.8326
Validation:	 Loss: 0.8016
Pretraining:	Epoch 41/200
----------
training:	Epoch: [41][1/233]	Loss 0.0121 (0.0121)	
training:	Epoch: [41][2/233]	Loss 0.0116 (0.0119)	
training:	Epoch: [41][3/233]	Loss 0.1752 (0.0663)	
training:	Epoch: [41][4/233]	Loss 0.0284 (0.0568)	
training:	Epoch: [41][5/233]	Loss 0.0094 (0.0473)	
training:	Epoch: [41][6/233]	Loss 0.0173 (0.0423)	
training:	Epoch: [41][7/233]	Loss 0.0074 (0.0373)	
training:	Epoch: [41][8/233]	Loss 0.0109 (0.0340)	
training:	Epoch: [41][9/233]	Loss 0.0090 (0.0312)	
training:	Epoch: [41][10/233]	Loss 0.0057 (0.0287)	
training:	Epoch: [41][11/233]	Loss 0.0990 (0.0351)	
training:	Epoch: [41][12/233]	Loss 0.0095 (0.0330)	
training:	Epoch: [41][13/233]	Loss 0.1908 (0.0451)	
training:	Epoch: [41][14/233]	Loss 0.0144 (0.0429)	
training:	Epoch: [41][15/233]	Loss 0.0060 (0.0404)	
training:	Epoch: [41][16/233]	Loss 0.0122 (0.0387)	
training:	Epoch: [41][17/233]	Loss 0.0167 (0.0374)	
training:	Epoch: [41][18/233]	Loss 0.0141 (0.0361)	
training:	Epoch: [41][19/233]	Loss 0.0739 (0.0381)	
training:	Epoch: [41][20/233]	Loss 0.0441 (0.0384)	
training:	Epoch: [41][21/233]	Loss 0.0106 (0.0371)	
training:	Epoch: [41][22/233]	Loss 0.0060 (0.0356)	
training:	Epoch: [41][23/233]	Loss 0.1625 (0.0412)	
training:	Epoch: [41][24/233]	Loss 0.0071 (0.0397)	
training:	Epoch: [41][25/233]	Loss 0.0076 (0.0385)	
training:	Epoch: [41][26/233]	Loss 0.0082 (0.0373)	
training:	Epoch: [41][27/233]	Loss 0.0054 (0.0361)	
training:	Epoch: [41][28/233]	Loss 0.0063 (0.0351)	
training:	Epoch: [41][29/233]	Loss 0.0065 (0.0341)	
training:	Epoch: [41][30/233]	Loss 0.0069 (0.0332)	
training:	Epoch: [41][31/233]	Loss 0.0107 (0.0324)	
training:	Epoch: [41][32/233]	Loss 0.0114 (0.0318)	
training:	Epoch: [41][33/233]	Loss 0.0060 (0.0310)	
training:	Epoch: [41][34/233]	Loss 0.0068 (0.0303)	
training:	Epoch: [41][35/233]	Loss 0.0099 (0.0297)	
training:	Epoch: [41][36/233]	Loss 0.0059 (0.0290)	
training:	Epoch: [41][37/233]	Loss 0.0094 (0.0285)	
training:	Epoch: [41][38/233]	Loss 0.0918 (0.0302)	
training:	Epoch: [41][39/233]	Loss 0.0067 (0.0296)	
training:	Epoch: [41][40/233]	Loss 0.0091 (0.0291)	
training:	Epoch: [41][41/233]	Loss 0.0065 (0.0285)	
training:	Epoch: [41][42/233]	Loss 0.0062 (0.0280)	
training:	Epoch: [41][43/233]	Loss 0.0210 (0.0278)	
training:	Epoch: [41][44/233]	Loss 0.0074 (0.0274)	
training:	Epoch: [41][45/233]	Loss 0.0155 (0.0271)	
training:	Epoch: [41][46/233]	Loss 0.1844 (0.0305)	
training:	Epoch: [41][47/233]	Loss 0.0084 (0.0300)	
training:	Epoch: [41][48/233]	Loss 0.1552 (0.0326)	
training:	Epoch: [41][49/233]	Loss 0.0065 (0.0321)	
training:	Epoch: [41][50/233]	Loss 0.0138 (0.0317)	
training:	Epoch: [41][51/233]	Loss 0.0836 (0.0328)	
training:	Epoch: [41][52/233]	Loss 0.0067 (0.0323)	
training:	Epoch: [41][53/233]	Loss 0.0081 (0.0318)	
training:	Epoch: [41][54/233]	Loss 0.0061 (0.0313)	
training:	Epoch: [41][55/233]	Loss 0.0495 (0.0317)	
training:	Epoch: [41][56/233]	Loss 0.0071 (0.0312)	
training:	Epoch: [41][57/233]	Loss 0.0071 (0.0308)	
training:	Epoch: [41][58/233]	Loss 0.0131 (0.0305)	
training:	Epoch: [41][59/233]	Loss 0.0072 (0.0301)	
training:	Epoch: [41][60/233]	Loss 0.1497 (0.0321)	
training:	Epoch: [41][61/233]	Loss 0.0073 (0.0317)	
training:	Epoch: [41][62/233]	Loss 0.1598 (0.0338)	
training:	Epoch: [41][63/233]	Loss 0.0081 (0.0333)	
training:	Epoch: [41][64/233]	Loss 0.0126 (0.0330)	
training:	Epoch: [41][65/233]	Loss 0.0232 (0.0329)	
training:	Epoch: [41][66/233]	Loss 0.0081 (0.0325)	
training:	Epoch: [41][67/233]	Loss 0.0664 (0.0330)	
training:	Epoch: [41][68/233]	Loss 0.0066 (0.0326)	
training:	Epoch: [41][69/233]	Loss 0.0064 (0.0322)	
training:	Epoch: [41][70/233]	Loss 0.2034 (0.0347)	
training:	Epoch: [41][71/233]	Loss 0.0223 (0.0345)	
training:	Epoch: [41][72/233]	Loss 0.0068 (0.0341)	
training:	Epoch: [41][73/233]	Loss 0.0120 (0.0338)	
training:	Epoch: [41][74/233]	Loss 0.1601 (0.0355)	
training:	Epoch: [41][75/233]	Loss 0.0086 (0.0352)	
training:	Epoch: [41][76/233]	Loss 0.0081 (0.0348)	
training:	Epoch: [41][77/233]	Loss 0.0087 (0.0345)	
training:	Epoch: [41][78/233]	Loss 0.0059 (0.0341)	
training:	Epoch: [41][79/233]	Loss 0.0060 (0.0337)	
training:	Epoch: [41][80/233]	Loss 0.0087 (0.0334)	
training:	Epoch: [41][81/233]	Loss 0.0071 (0.0331)	
training:	Epoch: [41][82/233]	Loss 0.0081 (0.0328)	
training:	Epoch: [41][83/233]	Loss 0.0073 (0.0325)	
training:	Epoch: [41][84/233]	Loss 0.0222 (0.0324)	
training:	Epoch: [41][85/233]	Loss 0.0138 (0.0322)	
training:	Epoch: [41][86/233]	Loss 0.0069 (0.0319)	
training:	Epoch: [41][87/233]	Loss 0.3440 (0.0354)	
training:	Epoch: [41][88/233]	Loss 0.0682 (0.0358)	
training:	Epoch: [41][89/233]	Loss 0.1116 (0.0367)	
training:	Epoch: [41][90/233]	Loss 0.0060 (0.0363)	
training:	Epoch: [41][91/233]	Loss 0.0286 (0.0362)	
training:	Epoch: [41][92/233]	Loss 0.0095 (0.0360)	
training:	Epoch: [41][93/233]	Loss 0.0156 (0.0357)	
training:	Epoch: [41][94/233]	Loss 0.1230 (0.0367)	
training:	Epoch: [41][95/233]	Loss 0.1459 (0.0378)	
training:	Epoch: [41][96/233]	Loss 0.0543 (0.0380)	
training:	Epoch: [41][97/233]	Loss 0.0116 (0.0377)	
training:	Epoch: [41][98/233]	Loss 0.0089 (0.0374)	
training:	Epoch: [41][99/233]	Loss 0.0709 (0.0378)	
training:	Epoch: [41][100/233]	Loss 0.1733 (0.0391)	
training:	Epoch: [41][101/233]	Loss 0.1836 (0.0405)	
training:	Epoch: [41][102/233]	Loss 0.0096 (0.0402)	
training:	Epoch: [41][103/233]	Loss 0.0262 (0.0401)	
training:	Epoch: [41][104/233]	Loss 0.0060 (0.0398)	
training:	Epoch: [41][105/233]	Loss 0.0748 (0.0401)	
training:	Epoch: [41][106/233]	Loss 0.0096 (0.0398)	
training:	Epoch: [41][107/233]	Loss 0.0059 (0.0395)	
training:	Epoch: [41][108/233]	Loss 0.1347 (0.0404)	
training:	Epoch: [41][109/233]	Loss 0.0058 (0.0401)	
training:	Epoch: [41][110/233]	Loss 0.0079 (0.0398)	
training:	Epoch: [41][111/233]	Loss 0.3036 (0.0422)	
training:	Epoch: [41][112/233]	Loss 0.1828 (0.0434)	
training:	Epoch: [41][113/233]	Loss 0.0058 (0.0431)	
training:	Epoch: [41][114/233]	Loss 0.0297 (0.0430)	
training:	Epoch: [41][115/233]	Loss 0.0100 (0.0427)	
training:	Epoch: [41][116/233]	Loss 0.0061 (0.0424)	
training:	Epoch: [41][117/233]	Loss 0.0103 (0.0421)	
training:	Epoch: [41][118/233]	Loss 0.0263 (0.0420)	
training:	Epoch: [41][119/233]	Loss 0.0074 (0.0417)	
training:	Epoch: [41][120/233]	Loss 0.0057 (0.0414)	
training:	Epoch: [41][121/233]	Loss 0.1192 (0.0420)	
training:	Epoch: [41][122/233]	Loss 0.0102 (0.0417)	
training:	Epoch: [41][123/233]	Loss 0.0109 (0.0415)	
training:	Epoch: [41][124/233]	Loss 0.0151 (0.0413)	
training:	Epoch: [41][125/233]	Loss 0.0070 (0.0410)	
training:	Epoch: [41][126/233]	Loss 0.0151 (0.0408)	
training:	Epoch: [41][127/233]	Loss 0.0212 (0.0406)	
training:	Epoch: [41][128/233]	Loss 0.0819 (0.0410)	
training:	Epoch: [41][129/233]	Loss 0.0076 (0.0407)	
training:	Epoch: [41][130/233]	Loss 0.0087 (0.0405)	
training:	Epoch: [41][131/233]	Loss 0.0129 (0.0403)	
training:	Epoch: [41][132/233]	Loss 0.0077 (0.0400)	
training:	Epoch: [41][133/233]	Loss 0.0065 (0.0398)	
training:	Epoch: [41][134/233]	Loss 0.0293 (0.0397)	
training:	Epoch: [41][135/233]	Loss 0.0097 (0.0395)	
training:	Epoch: [41][136/233]	Loss 0.0109 (0.0392)	
training:	Epoch: [41][137/233]	Loss 0.0080 (0.0390)	
training:	Epoch: [41][138/233]	Loss 0.0071 (0.0388)	
training:	Epoch: [41][139/233]	Loss 0.0320 (0.0387)	
training:	Epoch: [41][140/233]	Loss 0.0087 (0.0385)	
training:	Epoch: [41][141/233]	Loss 0.0059 (0.0383)	
training:	Epoch: [41][142/233]	Loss 0.0065 (0.0381)	
training:	Epoch: [41][143/233]	Loss 0.0082 (0.0379)	
training:	Epoch: [41][144/233]	Loss 0.0276 (0.0378)	
training:	Epoch: [41][145/233]	Loss 0.0106 (0.0376)	
training:	Epoch: [41][146/233]	Loss 0.2160 (0.0388)	
training:	Epoch: [41][147/233]	Loss 0.1579 (0.0396)	
training:	Epoch: [41][148/233]	Loss 0.0098 (0.0394)	
training:	Epoch: [41][149/233]	Loss 0.0077 (0.0392)	
training:	Epoch: [41][150/233]	Loss 0.1370 (0.0399)	
training:	Epoch: [41][151/233]	Loss 0.0060 (0.0396)	
training:	Epoch: [41][152/233]	Loss 0.0082 (0.0394)	
training:	Epoch: [41][153/233]	Loss 0.0062 (0.0392)	
training:	Epoch: [41][154/233]	Loss 0.0073 (0.0390)	
training:	Epoch: [41][155/233]	Loss 0.1044 (0.0394)	
training:	Epoch: [41][156/233]	Loss 0.0094 (0.0392)	
training:	Epoch: [41][157/233]	Loss 0.0065 (0.0390)	
training:	Epoch: [41][158/233]	Loss 0.0064 (0.0388)	
training:	Epoch: [41][159/233]	Loss 0.0086 (0.0386)	
training:	Epoch: [41][160/233]	Loss 0.0061 (0.0384)	
training:	Epoch: [41][161/233]	Loss 0.0202 (0.0383)	
training:	Epoch: [41][162/233]	Loss 0.0075 (0.0381)	
training:	Epoch: [41][163/233]	Loss 0.0071 (0.0379)	
training:	Epoch: [41][164/233]	Loss 0.0106 (0.0378)	
training:	Epoch: [41][165/233]	Loss 0.0995 (0.0381)	
training:	Epoch: [41][166/233]	Loss 0.1522 (0.0388)	
training:	Epoch: [41][167/233]	Loss 0.0080 (0.0387)	
training:	Epoch: [41][168/233]	Loss 0.0243 (0.0386)	
training:	Epoch: [41][169/233]	Loss 0.0076 (0.0384)	
training:	Epoch: [41][170/233]	Loss 0.1676 (0.0391)	
training:	Epoch: [41][171/233]	Loss 0.0593 (0.0393)	
training:	Epoch: [41][172/233]	Loss 0.0239 (0.0392)	
training:	Epoch: [41][173/233]	Loss 0.0059 (0.0390)	
training:	Epoch: [41][174/233]	Loss 0.0181 (0.0389)	
training:	Epoch: [41][175/233]	Loss 0.0672 (0.0390)	
training:	Epoch: [41][176/233]	Loss 0.0073 (0.0388)	
training:	Epoch: [41][177/233]	Loss 0.0086 (0.0387)	
training:	Epoch: [41][178/233]	Loss 0.0942 (0.0390)	
training:	Epoch: [41][179/233]	Loss 0.0628 (0.0391)	
training:	Epoch: [41][180/233]	Loss 0.0082 (0.0389)	
training:	Epoch: [41][181/233]	Loss 0.0136 (0.0388)	
training:	Epoch: [41][182/233]	Loss 0.0959 (0.0391)	
training:	Epoch: [41][183/233]	Loss 0.0109 (0.0390)	
training:	Epoch: [41][184/233]	Loss 0.0122 (0.0388)	
training:	Epoch: [41][185/233]	Loss 0.0115 (0.0387)	
training:	Epoch: [41][186/233]	Loss 0.1324 (0.0392)	
training:	Epoch: [41][187/233]	Loss 0.0874 (0.0394)	
training:	Epoch: [41][188/233]	Loss 0.0097 (0.0393)	
training:	Epoch: [41][189/233]	Loss 0.0077 (0.0391)	
training:	Epoch: [41][190/233]	Loss 0.1334 (0.0396)	
training:	Epoch: [41][191/233]	Loss 0.0166 (0.0395)	
training:	Epoch: [41][192/233]	Loss 0.0115 (0.0393)	
training:	Epoch: [41][193/233]	Loss 0.1816 (0.0401)	
training:	Epoch: [41][194/233]	Loss 0.0103 (0.0399)	
training:	Epoch: [41][195/233]	Loss 0.0070 (0.0398)	
training:	Epoch: [41][196/233]	Loss 0.0125 (0.0396)	
training:	Epoch: [41][197/233]	Loss 0.0071 (0.0394)	
training:	Epoch: [41][198/233]	Loss 0.0115 (0.0393)	
training:	Epoch: [41][199/233]	Loss 0.0238 (0.0392)	
training:	Epoch: [41][200/233]	Loss 0.0138 (0.0391)	
training:	Epoch: [41][201/233]	Loss 0.0140 (0.0390)	
training:	Epoch: [41][202/233]	Loss 0.1831 (0.0397)	
training:	Epoch: [41][203/233]	Loss 0.1026 (0.0400)	
training:	Epoch: [41][204/233]	Loss 0.0090 (0.0398)	
training:	Epoch: [41][205/233]	Loss 0.0092 (0.0397)	
training:	Epoch: [41][206/233]	Loss 0.0079 (0.0395)	
training:	Epoch: [41][207/233]	Loss 0.0123 (0.0394)	
training:	Epoch: [41][208/233]	Loss 0.0774 (0.0396)	
training:	Epoch: [41][209/233]	Loss 0.0924 (0.0398)	
training:	Epoch: [41][210/233]	Loss 0.1644 (0.0404)	
training:	Epoch: [41][211/233]	Loss 0.0145 (0.0403)	
training:	Epoch: [41][212/233]	Loss 0.0342 (0.0403)	
training:	Epoch: [41][213/233]	Loss 0.1000 (0.0406)	
training:	Epoch: [41][214/233]	Loss 0.0072 (0.0404)	
training:	Epoch: [41][215/233]	Loss 0.0751 (0.0406)	
training:	Epoch: [41][216/233]	Loss 0.0113 (0.0404)	
training:	Epoch: [41][217/233]	Loss 0.0095 (0.0403)	
training:	Epoch: [41][218/233]	Loss 0.0064 (0.0401)	
training:	Epoch: [41][219/233]	Loss 0.0132 (0.0400)	
training:	Epoch: [41][220/233]	Loss 0.0058 (0.0399)	
training:	Epoch: [41][221/233]	Loss 0.1493 (0.0404)	
training:	Epoch: [41][222/233]	Loss 0.0067 (0.0402)	
training:	Epoch: [41][223/233]	Loss 0.0068 (0.0401)	
training:	Epoch: [41][224/233]	Loss 0.0146 (0.0399)	
training:	Epoch: [41][225/233]	Loss 0.1544 (0.0405)	
training:	Epoch: [41][226/233]	Loss 0.0081 (0.0403)	
training:	Epoch: [41][227/233]	Loss 0.0070 (0.0402)	
training:	Epoch: [41][228/233]	Loss 0.0093 (0.0400)	
training:	Epoch: [41][229/233]	Loss 0.0251 (0.0400)	
training:	Epoch: [41][230/233]	Loss 0.0061 (0.0398)	
training:	Epoch: [41][231/233]	Loss 0.1224 (0.0402)	
training:	Epoch: [41][232/233]	Loss 0.0148 (0.0401)	
training:	Epoch: [41][233/233]	Loss 0.0071 (0.0399)	
Training:	 Loss: 0.0398

Training:	 ACC: 0.9965 0.9965 0.9959 0.9972
Validation:	 ACC: 0.8005 0.8010 0.8110 0.7899
Validation:	 Best_BACC: 0.8162 0.8154 0.7998 0.8326
Validation:	 Loss: 0.8119
Pretraining:	Epoch 42/200
----------
training:	Epoch: [42][1/233]	Loss 0.0779 (0.0779)	
training:	Epoch: [42][2/233]	Loss 0.0056 (0.0418)	
training:	Epoch: [42][3/233]	Loss 0.0065 (0.0300)	
training:	Epoch: [42][4/233]	Loss 0.0081 (0.0245)	
training:	Epoch: [42][5/233]	Loss 0.0167 (0.0230)	
training:	Epoch: [42][6/233]	Loss 0.0456 (0.0267)	
training:	Epoch: [42][7/233]	Loss 0.0072 (0.0240)	
training:	Epoch: [42][8/233]	Loss 0.0122 (0.0225)	
training:	Epoch: [42][9/233]	Loss 0.1637 (0.0382)	
training:	Epoch: [42][10/233]	Loss 0.0912 (0.0435)	
training:	Epoch: [42][11/233]	Loss 0.1769 (0.0556)	
training:	Epoch: [42][12/233]	Loss 0.0092 (0.0517)	
training:	Epoch: [42][13/233]	Loss 0.0094 (0.0485)	
training:	Epoch: [42][14/233]	Loss 0.0086 (0.0456)	
training:	Epoch: [42][15/233]	Loss 0.0064 (0.0430)	
training:	Epoch: [42][16/233]	Loss 0.0583 (0.0440)	
training:	Epoch: [42][17/233]	Loss 0.0101 (0.0420)	
training:	Epoch: [42][18/233]	Loss 0.0086 (0.0401)	
training:	Epoch: [42][19/233]	Loss 0.0057 (0.0383)	
training:	Epoch: [42][20/233]	Loss 0.0108 (0.0369)	
training:	Epoch: [42][21/233]	Loss 0.0071 (0.0355)	
training:	Epoch: [42][22/233]	Loss 0.1070 (0.0388)	
training:	Epoch: [42][23/233]	Loss 0.0128 (0.0376)	
training:	Epoch: [42][24/233]	Loss 0.0120 (0.0366)	
training:	Epoch: [42][25/233]	Loss 0.0064 (0.0354)	
training:	Epoch: [42][26/233]	Loss 0.1742 (0.0407)	
training:	Epoch: [42][27/233]	Loss 0.0089 (0.0395)	
training:	Epoch: [42][28/233]	Loss 0.0083 (0.0384)	
training:	Epoch: [42][29/233]	Loss 0.0074 (0.0373)	
training:	Epoch: [42][30/233]	Loss 0.0057 (0.0363)	
training:	Epoch: [42][31/233]	Loss 0.0062 (0.0353)	
training:	Epoch: [42][32/233]	Loss 0.0059 (0.0344)	
training:	Epoch: [42][33/233]	Loss 0.0082 (0.0336)	
training:	Epoch: [42][34/233]	Loss 0.0180 (0.0331)	
training:	Epoch: [42][35/233]	Loss 0.0053 (0.0324)	
training:	Epoch: [42][36/233]	Loss 0.0061 (0.0316)	
training:	Epoch: [42][37/233]	Loss 0.0071 (0.0310)	
training:	Epoch: [42][38/233]	Loss 0.0072 (0.0303)	
training:	Epoch: [42][39/233]	Loss 0.1116 (0.0324)	
training:	Epoch: [42][40/233]	Loss 0.0172 (0.0320)	
training:	Epoch: [42][41/233]	Loss 0.0484 (0.0324)	
training:	Epoch: [42][42/233]	Loss 0.0078 (0.0318)	
training:	Epoch: [42][43/233]	Loss 0.0061 (0.0313)	
training:	Epoch: [42][44/233]	Loss 0.1692 (0.0344)	
training:	Epoch: [42][45/233]	Loss 0.0053 (0.0337)	
training:	Epoch: [42][46/233]	Loss 0.0073 (0.0332)	
training:	Epoch: [42][47/233]	Loss 0.0166 (0.0328)	
training:	Epoch: [42][48/233]	Loss 0.0072 (0.0323)	
training:	Epoch: [42][49/233]	Loss 0.0106 (0.0318)	
training:	Epoch: [42][50/233]	Loss 0.0355 (0.0319)	
training:	Epoch: [42][51/233]	Loss 0.0062 (0.0314)	
training:	Epoch: [42][52/233]	Loss 0.0058 (0.0309)	
training:	Epoch: [42][53/233]	Loss 0.0065 (0.0305)	
training:	Epoch: [42][54/233]	Loss 0.0254 (0.0304)	
training:	Epoch: [42][55/233]	Loss 0.0246 (0.0303)	
training:	Epoch: [42][56/233]	Loss 0.0167 (0.0300)	
training:	Epoch: [42][57/233]	Loss 0.0155 (0.0298)	
training:	Epoch: [42][58/233]	Loss 0.0115 (0.0294)	
training:	Epoch: [42][59/233]	Loss 0.0057 (0.0290)	
training:	Epoch: [42][60/233]	Loss 0.0060 (0.0287)	
training:	Epoch: [42][61/233]	Loss 0.0076 (0.0283)	
training:	Epoch: [42][62/233]	Loss 0.0068 (0.0280)	
training:	Epoch: [42][63/233]	Loss 0.0118 (0.0277)	
training:	Epoch: [42][64/233]	Loss 0.0073 (0.0274)	
training:	Epoch: [42][65/233]	Loss 0.0108 (0.0271)	
training:	Epoch: [42][66/233]	Loss 0.1690 (0.0293)	
training:	Epoch: [42][67/233]	Loss 0.0057 (0.0289)	
training:	Epoch: [42][68/233]	Loss 0.0063 (0.0286)	
training:	Epoch: [42][69/233]	Loss 0.0235 (0.0285)	
training:	Epoch: [42][70/233]	Loss 0.0060 (0.0282)	
training:	Epoch: [42][71/233]	Loss 0.1442 (0.0298)	
training:	Epoch: [42][72/233]	Loss 0.0089 (0.0295)	
training:	Epoch: [42][73/233]	Loss 0.0164 (0.0294)	
training:	Epoch: [42][74/233]	Loss 0.0598 (0.0298)	
training:	Epoch: [42][75/233]	Loss 0.1776 (0.0317)	
training:	Epoch: [42][76/233]	Loss 0.0192 (0.0316)	
training:	Epoch: [42][77/233]	Loss 0.0053 (0.0312)	
training:	Epoch: [42][78/233]	Loss 0.0059 (0.0309)	
training:	Epoch: [42][79/233]	Loss 0.0113 (0.0307)	
training:	Epoch: [42][80/233]	Loss 0.0071 (0.0304)	
training:	Epoch: [42][81/233]	Loss 0.0062 (0.0301)	
training:	Epoch: [42][82/233]	Loss 0.0210 (0.0300)	
training:	Epoch: [42][83/233]	Loss 0.0097 (0.0297)	
training:	Epoch: [42][84/233]	Loss 0.0105 (0.0295)	
training:	Epoch: [42][85/233]	Loss 0.0153 (0.0293)	
training:	Epoch: [42][86/233]	Loss 0.0087 (0.0291)	
training:	Epoch: [42][87/233]	Loss 0.0213 (0.0290)	
training:	Epoch: [42][88/233]	Loss 0.1605 (0.0305)	
training:	Epoch: [42][89/233]	Loss 0.1847 (0.0322)	
training:	Epoch: [42][90/233]	Loss 0.0086 (0.0320)	
training:	Epoch: [42][91/233]	Loss 0.1690 (0.0335)	
training:	Epoch: [42][92/233]	Loss 0.0134 (0.0332)	
training:	Epoch: [42][93/233]	Loss 0.0077 (0.0330)	
training:	Epoch: [42][94/233]	Loss 0.0749 (0.0334)	
training:	Epoch: [42][95/233]	Loss 0.0503 (0.0336)	
training:	Epoch: [42][96/233]	Loss 0.0772 (0.0341)	
training:	Epoch: [42][97/233]	Loss 0.1487 (0.0352)	
training:	Epoch: [42][98/233]	Loss 0.0072 (0.0349)	
training:	Epoch: [42][99/233]	Loss 0.0071 (0.0347)	
training:	Epoch: [42][100/233]	Loss 0.1324 (0.0356)	
training:	Epoch: [42][101/233]	Loss 0.0162 (0.0355)	
training:	Epoch: [42][102/233]	Loss 0.0064 (0.0352)	
training:	Epoch: [42][103/233]	Loss 0.0157 (0.0350)	
training:	Epoch: [42][104/233]	Loss 0.0268 (0.0349)	
training:	Epoch: [42][105/233]	Loss 0.0083 (0.0346)	
training:	Epoch: [42][106/233]	Loss 0.0077 (0.0344)	
training:	Epoch: [42][107/233]	Loss 0.0415 (0.0345)	
training:	Epoch: [42][108/233]	Loss 0.0056 (0.0342)	
training:	Epoch: [42][109/233]	Loss 0.3468 (0.0371)	
training:	Epoch: [42][110/233]	Loss 0.0056 (0.0368)	
training:	Epoch: [42][111/233]	Loss 0.0977 (0.0373)	
training:	Epoch: [42][112/233]	Loss 0.0090 (0.0371)	
training:	Epoch: [42][113/233]	Loss 0.0075 (0.0368)	
training:	Epoch: [42][114/233]	Loss 0.1564 (0.0379)	
training:	Epoch: [42][115/233]	Loss 0.0087 (0.0376)	
training:	Epoch: [42][116/233]	Loss 0.0654 (0.0378)	
training:	Epoch: [42][117/233]	Loss 0.0070 (0.0376)	
training:	Epoch: [42][118/233]	Loss 0.1179 (0.0383)	
training:	Epoch: [42][119/233]	Loss 0.0063 (0.0380)	
training:	Epoch: [42][120/233]	Loss 0.0066 (0.0377)	
training:	Epoch: [42][121/233]	Loss 0.1606 (0.0387)	
training:	Epoch: [42][122/233]	Loss 0.0068 (0.0385)	
training:	Epoch: [42][123/233]	Loss 0.0754 (0.0388)	
training:	Epoch: [42][124/233]	Loss 0.0058 (0.0385)	
training:	Epoch: [42][125/233]	Loss 0.0054 (0.0383)	
training:	Epoch: [42][126/233]	Loss 0.0067 (0.0380)	
training:	Epoch: [42][127/233]	Loss 0.1817 (0.0391)	
training:	Epoch: [42][128/233]	Loss 0.0543 (0.0392)	
training:	Epoch: [42][129/233]	Loss 0.1581 (0.0402)	
training:	Epoch: [42][130/233]	Loss 0.0091 (0.0399)	
training:	Epoch: [42][131/233]	Loss 0.0092 (0.0397)	
training:	Epoch: [42][132/233]	Loss 0.0139 (0.0395)	
training:	Epoch: [42][133/233]	Loss 0.0206 (0.0394)	
training:	Epoch: [42][134/233]	Loss 0.0099 (0.0391)	
training:	Epoch: [42][135/233]	Loss 0.0075 (0.0389)	
training:	Epoch: [42][136/233]	Loss 0.0152 (0.0387)	
training:	Epoch: [42][137/233]	Loss 0.0083 (0.0385)	
training:	Epoch: [42][138/233]	Loss 0.0132 (0.0383)	
training:	Epoch: [42][139/233]	Loss 0.0068 (0.0381)	
training:	Epoch: [42][140/233]	Loss 0.1496 (0.0389)	
training:	Epoch: [42][141/233]	Loss 0.0117 (0.0387)	
training:	Epoch: [42][142/233]	Loss 0.0098 (0.0385)	
training:	Epoch: [42][143/233]	Loss 0.0288 (0.0384)	
training:	Epoch: [42][144/233]	Loss 0.0117 (0.0382)	
training:	Epoch: [42][145/233]	Loss 0.0072 (0.0380)	
training:	Epoch: [42][146/233]	Loss 0.0095 (0.0378)	
training:	Epoch: [42][147/233]	Loss 0.0094 (0.0376)	
training:	Epoch: [42][148/233]	Loss 0.0158 (0.0375)	
training:	Epoch: [42][149/233]	Loss 0.0070 (0.0373)	
training:	Epoch: [42][150/233]	Loss 0.1269 (0.0379)	
training:	Epoch: [42][151/233]	Loss 0.0096 (0.0377)	
training:	Epoch: [42][152/233]	Loss 0.0152 (0.0376)	
training:	Epoch: [42][153/233]	Loss 0.0086 (0.0374)	
training:	Epoch: [42][154/233]	Loss 0.0093 (0.0372)	
training:	Epoch: [42][155/233]	Loss 0.0072 (0.0370)	
training:	Epoch: [42][156/233]	Loss 0.0072 (0.0368)	
training:	Epoch: [42][157/233]	Loss 0.0068 (0.0366)	
training:	Epoch: [42][158/233]	Loss 0.0092 (0.0364)	
training:	Epoch: [42][159/233]	Loss 0.0076 (0.0363)	
training:	Epoch: [42][160/233]	Loss 0.0078 (0.0361)	
training:	Epoch: [42][161/233]	Loss 0.0071 (0.0359)	
training:	Epoch: [42][162/233]	Loss 0.0706 (0.0361)	
training:	Epoch: [42][163/233]	Loss 0.0088 (0.0359)	
training:	Epoch: [42][164/233]	Loss 0.0110 (0.0358)	
training:	Epoch: [42][165/233]	Loss 0.0501 (0.0359)	
training:	Epoch: [42][166/233]	Loss 0.0059 (0.0357)	
training:	Epoch: [42][167/233]	Loss 0.0271 (0.0356)	
training:	Epoch: [42][168/233]	Loss 0.0084 (0.0355)	
training:	Epoch: [42][169/233]	Loss 0.0066 (0.0353)	
training:	Epoch: [42][170/233]	Loss 0.0073 (0.0351)	
training:	Epoch: [42][171/233]	Loss 0.2167 (0.0362)	
training:	Epoch: [42][172/233]	Loss 0.0058 (0.0360)	
training:	Epoch: [42][173/233]	Loss 0.0077 (0.0359)	
training:	Epoch: [42][174/233]	Loss 0.0180 (0.0358)	
training:	Epoch: [42][175/233]	Loss 0.0118 (0.0356)	
training:	Epoch: [42][176/233]	Loss 0.0255 (0.0356)	
training:	Epoch: [42][177/233]	Loss 0.0147 (0.0355)	
training:	Epoch: [42][178/233]	Loss 0.0061 (0.0353)	
training:	Epoch: [42][179/233]	Loss 0.0092 (0.0351)	
training:	Epoch: [42][180/233]	Loss 0.0082 (0.0350)	
training:	Epoch: [42][181/233]	Loss 0.0085 (0.0348)	
training:	Epoch: [42][182/233]	Loss 0.0090 (0.0347)	
training:	Epoch: [42][183/233]	Loss 0.0328 (0.0347)	
training:	Epoch: [42][184/233]	Loss 0.0166 (0.0346)	
training:	Epoch: [42][185/233]	Loss 0.0431 (0.0346)	
training:	Epoch: [42][186/233]	Loss 0.0066 (0.0345)	
training:	Epoch: [42][187/233]	Loss 0.0132 (0.0344)	
training:	Epoch: [42][188/233]	Loss 0.0083 (0.0342)	
training:	Epoch: [42][189/233]	Loss 0.0250 (0.0342)	
training:	Epoch: [42][190/233]	Loss 0.1785 (0.0349)	
training:	Epoch: [42][191/233]	Loss 0.0065 (0.0348)	
training:	Epoch: [42][192/233]	Loss 0.0240 (0.0347)	
training:	Epoch: [42][193/233]	Loss 0.0084 (0.0346)	
training:	Epoch: [42][194/233]	Loss 0.0067 (0.0345)	
training:	Epoch: [42][195/233]	Loss 0.0061 (0.0343)	
training:	Epoch: [42][196/233]	Loss 0.0063 (0.0342)	
training:	Epoch: [42][197/233]	Loss 0.1227 (0.0346)	
training:	Epoch: [42][198/233]	Loss 0.0066 (0.0345)	
training:	Epoch: [42][199/233]	Loss 0.0175 (0.0344)	
training:	Epoch: [42][200/233]	Loss 0.0219 (0.0343)	
training:	Epoch: [42][201/233]	Loss 0.0128 (0.0342)	
training:	Epoch: [42][202/233]	Loss 0.0062 (0.0341)	
training:	Epoch: [42][203/233]	Loss 0.0708 (0.0343)	
training:	Epoch: [42][204/233]	Loss 0.0078 (0.0341)	
training:	Epoch: [42][205/233]	Loss 0.0076 (0.0340)	
training:	Epoch: [42][206/233]	Loss 0.1705 (0.0347)	
training:	Epoch: [42][207/233]	Loss 0.0825 (0.0349)	
training:	Epoch: [42][208/233]	Loss 0.0111 (0.0348)	
training:	Epoch: [42][209/233]	Loss 0.0101 (0.0347)	
training:	Epoch: [42][210/233]	Loss 0.0067 (0.0345)	
training:	Epoch: [42][211/233]	Loss 0.0098 (0.0344)	
training:	Epoch: [42][212/233]	Loss 0.0069 (0.0343)	
training:	Epoch: [42][213/233]	Loss 0.0061 (0.0342)	
training:	Epoch: [42][214/233]	Loss 0.0063 (0.0340)	
training:	Epoch: [42][215/233]	Loss 0.0124 (0.0339)	
training:	Epoch: [42][216/233]	Loss 0.0086 (0.0338)	
training:	Epoch: [42][217/233]	Loss 0.0105 (0.0337)	
training:	Epoch: [42][218/233]	Loss 0.0089 (0.0336)	
training:	Epoch: [42][219/233]	Loss 0.0401 (0.0336)	
training:	Epoch: [42][220/233]	Loss 0.0049 (0.0335)	
training:	Epoch: [42][221/233]	Loss 0.0099 (0.0334)	
training:	Epoch: [42][222/233]	Loss 0.0157 (0.0333)	
training:	Epoch: [42][223/233]	Loss 0.0128 (0.0332)	
training:	Epoch: [42][224/233]	Loss 0.0221 (0.0332)	
training:	Epoch: [42][225/233]	Loss 0.1354 (0.0336)	
training:	Epoch: [42][226/233]	Loss 0.0182 (0.0335)	
training:	Epoch: [42][227/233]	Loss 0.0055 (0.0334)	
training:	Epoch: [42][228/233]	Loss 0.0288 (0.0334)	
training:	Epoch: [42][229/233]	Loss 0.0070 (0.0333)	
training:	Epoch: [42][230/233]	Loss 0.2466 (0.0342)	
training:	Epoch: [42][231/233]	Loss 0.0056 (0.0341)	
training:	Epoch: [42][232/233]	Loss 0.0084 (0.0340)	
training:	Epoch: [42][233/233]	Loss 0.0231 (0.0339)	
Training:	 Loss: 0.0339

Training:	 ACC: 0.9967 0.9967 0.9962 0.9972
Validation:	 ACC: 0.7975 0.7967 0.7804 0.8146
Validation:	 Best_BACC: 0.8162 0.8154 0.7998 0.8326
Validation:	 Loss: 0.8519
Pretraining:	Epoch 43/200
----------
training:	Epoch: [43][1/233]	Loss 0.0064 (0.0064)	
training:	Epoch: [43][2/233]	Loss 0.0066 (0.0065)	
training:	Epoch: [43][3/233]	Loss 0.0065 (0.0065)	
training:	Epoch: [43][4/233]	Loss 0.0542 (0.0184)	
training:	Epoch: [43][5/233]	Loss 0.0128 (0.0173)	
training:	Epoch: [43][6/233]	Loss 0.0148 (0.0169)	
training:	Epoch: [43][7/233]	Loss 0.0083 (0.0156)	
training:	Epoch: [43][8/233]	Loss 0.1229 (0.0291)	
training:	Epoch: [43][9/233]	Loss 0.0105 (0.0270)	
training:	Epoch: [43][10/233]	Loss 0.0131 (0.0256)	
training:	Epoch: [43][11/233]	Loss 0.0066 (0.0239)	
training:	Epoch: [43][12/233]	Loss 0.0071 (0.0225)	
training:	Epoch: [43][13/233]	Loss 0.2107 (0.0370)	
training:	Epoch: [43][14/233]	Loss 0.0100 (0.0350)	
training:	Epoch: [43][15/233]	Loss 0.0110 (0.0334)	
training:	Epoch: [43][16/233]	Loss 0.0103 (0.0320)	
training:	Epoch: [43][17/233]	Loss 0.0234 (0.0315)	
training:	Epoch: [43][18/233]	Loss 0.0549 (0.0328)	
training:	Epoch: [43][19/233]	Loss 0.0318 (0.0327)	
training:	Epoch: [43][20/233]	Loss 0.0093 (0.0316)	
training:	Epoch: [43][21/233]	Loss 0.0058 (0.0303)	
training:	Epoch: [43][22/233]	Loss 0.0066 (0.0293)	
training:	Epoch: [43][23/233]	Loss 0.0065 (0.0283)	
training:	Epoch: [43][24/233]	Loss 0.0174 (0.0278)	
training:	Epoch: [43][25/233]	Loss 0.0078 (0.0270)	
training:	Epoch: [43][26/233]	Loss 0.0171 (0.0266)	
training:	Epoch: [43][27/233]	Loss 0.0054 (0.0259)	
training:	Epoch: [43][28/233]	Loss 0.0073 (0.0252)	
training:	Epoch: [43][29/233]	Loss 0.0137 (0.0248)	
training:	Epoch: [43][30/233]	Loss 0.0091 (0.0243)	
training:	Epoch: [43][31/233]	Loss 0.0065 (0.0237)	
training:	Epoch: [43][32/233]	Loss 0.0072 (0.0232)	
training:	Epoch: [43][33/233]	Loss 0.0150 (0.0229)	
training:	Epoch: [43][34/233]	Loss 0.0056 (0.0224)	
training:	Epoch: [43][35/233]	Loss 0.0228 (0.0224)	
training:	Epoch: [43][36/233]	Loss 0.0057 (0.0220)	
training:	Epoch: [43][37/233]	Loss 0.0109 (0.0217)	
training:	Epoch: [43][38/233]	Loss 0.0418 (0.0222)	
training:	Epoch: [43][39/233]	Loss 0.0086 (0.0219)	
training:	Epoch: [43][40/233]	Loss 0.0592 (0.0228)	
training:	Epoch: [43][41/233]	Loss 0.0066 (0.0224)	
training:	Epoch: [43][42/233]	Loss 0.0135 (0.0222)	
training:	Epoch: [43][43/233]	Loss 0.0209 (0.0221)	
training:	Epoch: [43][44/233]	Loss 0.0098 (0.0219)	
training:	Epoch: [43][45/233]	Loss 0.0090 (0.0216)	
training:	Epoch: [43][46/233]	Loss 0.0089 (0.0213)	
training:	Epoch: [43][47/233]	Loss 0.0577 (0.0221)	
training:	Epoch: [43][48/233]	Loss 0.1525 (0.0248)	
training:	Epoch: [43][49/233]	Loss 0.0752 (0.0258)	
training:	Epoch: [43][50/233]	Loss 0.0056 (0.0254)	
training:	Epoch: [43][51/233]	Loss 0.0768 (0.0264)	
training:	Epoch: [43][52/233]	Loss 0.0065 (0.0260)	
training:	Epoch: [43][53/233]	Loss 0.0098 (0.0257)	
training:	Epoch: [43][54/233]	Loss 0.0051 (0.0254)	
training:	Epoch: [43][55/233]	Loss 0.0237 (0.0253)	
training:	Epoch: [43][56/233]	Loss 0.0062 (0.0250)	
training:	Epoch: [43][57/233]	Loss 0.0087 (0.0247)	
training:	Epoch: [43][58/233]	Loss 0.0064 (0.0244)	
training:	Epoch: [43][59/233]	Loss 0.0053 (0.0241)	
training:	Epoch: [43][60/233]	Loss 0.0056 (0.0237)	
training:	Epoch: [43][61/233]	Loss 0.2399 (0.0273)	
training:	Epoch: [43][62/233]	Loss 0.0079 (0.0270)	
training:	Epoch: [43][63/233]	Loss 0.0053 (0.0266)	
training:	Epoch: [43][64/233]	Loss 0.1685 (0.0289)	
training:	Epoch: [43][65/233]	Loss 0.0062 (0.0285)	
training:	Epoch: [43][66/233]	Loss 0.0135 (0.0283)	
training:	Epoch: [43][67/233]	Loss 0.0048 (0.0279)	
training:	Epoch: [43][68/233]	Loss 0.0055 (0.0276)	
training:	Epoch: [43][69/233]	Loss 0.1707 (0.0297)	
training:	Epoch: [43][70/233]	Loss 0.0083 (0.0294)	
training:	Epoch: [43][71/233]	Loss 0.1920 (0.0317)	
training:	Epoch: [43][72/233]	Loss 0.2375 (0.0345)	
training:	Epoch: [43][73/233]	Loss 0.1499 (0.0361)	
training:	Epoch: [43][74/233]	Loss 0.0062 (0.0357)	
training:	Epoch: [43][75/233]	Loss 0.0055 (0.0353)	
training:	Epoch: [43][76/233]	Loss 0.1363 (0.0366)	
training:	Epoch: [43][77/233]	Loss 0.0060 (0.0362)	
training:	Epoch: [43][78/233]	Loss 0.0083 (0.0359)	
training:	Epoch: [43][79/233]	Loss 0.0287 (0.0358)	
training:	Epoch: [43][80/233]	Loss 0.0150 (0.0355)	
training:	Epoch: [43][81/233]	Loss 0.0095 (0.0352)	
training:	Epoch: [43][82/233]	Loss 0.0053 (0.0348)	
training:	Epoch: [43][83/233]	Loss 0.0174 (0.0346)	
training:	Epoch: [43][84/233]	Loss 0.0060 (0.0343)	
training:	Epoch: [43][85/233]	Loss 0.1068 (0.0351)	
training:	Epoch: [43][86/233]	Loss 0.0090 (0.0348)	
training:	Epoch: [43][87/233]	Loss 0.0061 (0.0345)	
training:	Epoch: [43][88/233]	Loss 0.0090 (0.0342)	
training:	Epoch: [43][89/233]	Loss 0.0069 (0.0339)	
training:	Epoch: [43][90/233]	Loss 0.0074 (0.0336)	
training:	Epoch: [43][91/233]	Loss 0.0089 (0.0333)	
training:	Epoch: [43][92/233]	Loss 0.0074 (0.0330)	
training:	Epoch: [43][93/233]	Loss 0.0061 (0.0328)	
training:	Epoch: [43][94/233]	Loss 0.0099 (0.0325)	
training:	Epoch: [43][95/233]	Loss 0.0091 (0.0323)	
training:	Epoch: [43][96/233]	Loss 0.0060 (0.0320)	
training:	Epoch: [43][97/233]	Loss 0.0066 (0.0317)	
training:	Epoch: [43][98/233]	Loss 0.0058 (0.0315)	
training:	Epoch: [43][99/233]	Loss 0.0450 (0.0316)	
training:	Epoch: [43][100/233]	Loss 0.0061 (0.0314)	
training:	Epoch: [43][101/233]	Loss 0.1156 (0.0322)	
training:	Epoch: [43][102/233]	Loss 0.0066 (0.0319)	
training:	Epoch: [43][103/233]	Loss 0.1760 (0.0333)	
training:	Epoch: [43][104/233]	Loss 0.0078 (0.0331)	
training:	Epoch: [43][105/233]	Loss 0.0080 (0.0328)	
training:	Epoch: [43][106/233]	Loss 0.0393 (0.0329)	
training:	Epoch: [43][107/233]	Loss 0.0139 (0.0327)	
training:	Epoch: [43][108/233]	Loss 0.0495 (0.0329)	
training:	Epoch: [43][109/233]	Loss 0.0275 (0.0328)	
training:	Epoch: [43][110/233]	Loss 0.0090 (0.0326)	
training:	Epoch: [43][111/233]	Loss 0.0695 (0.0330)	
training:	Epoch: [43][112/233]	Loss 0.0113 (0.0328)	
training:	Epoch: [43][113/233]	Loss 0.0092 (0.0326)	
training:	Epoch: [43][114/233]	Loss 0.0058 (0.0323)	
training:	Epoch: [43][115/233]	Loss 0.0851 (0.0328)	
training:	Epoch: [43][116/233]	Loss 0.0085 (0.0326)	
training:	Epoch: [43][117/233]	Loss 0.1025 (0.0332)	
training:	Epoch: [43][118/233]	Loss 0.0935 (0.0337)	
training:	Epoch: [43][119/233]	Loss 0.1777 (0.0349)	
training:	Epoch: [43][120/233]	Loss 0.0205 (0.0348)	
training:	Epoch: [43][121/233]	Loss 0.0111 (0.0346)	
training:	Epoch: [43][122/233]	Loss 0.0375 (0.0346)	
training:	Epoch: [43][123/233]	Loss 0.1846 (0.0358)	
training:	Epoch: [43][124/233]	Loss 0.0067 (0.0356)	
training:	Epoch: [43][125/233]	Loss 0.0070 (0.0354)	
training:	Epoch: [43][126/233]	Loss 0.1790 (0.0365)	
training:	Epoch: [43][127/233]	Loss 0.0067 (0.0363)	
training:	Epoch: [43][128/233]	Loss 0.0057 (0.0360)	
training:	Epoch: [43][129/233]	Loss 0.1709 (0.0371)	
training:	Epoch: [43][130/233]	Loss 0.1066 (0.0376)	
training:	Epoch: [43][131/233]	Loss 0.0062 (0.0374)	
training:	Epoch: [43][132/233]	Loss 0.0079 (0.0371)	
training:	Epoch: [43][133/233]	Loss 0.0078 (0.0369)	
training:	Epoch: [43][134/233]	Loss 0.0343 (0.0369)	
training:	Epoch: [43][135/233]	Loss 0.0061 (0.0367)	
training:	Epoch: [43][136/233]	Loss 0.0069 (0.0364)	
training:	Epoch: [43][137/233]	Loss 0.0053 (0.0362)	
training:	Epoch: [43][138/233]	Loss 0.1693 (0.0372)	
training:	Epoch: [43][139/233]	Loss 0.0054 (0.0370)	
training:	Epoch: [43][140/233]	Loss 0.2702 (0.0386)	
training:	Epoch: [43][141/233]	Loss 0.1631 (0.0395)	
training:	Epoch: [43][142/233]	Loss 0.0278 (0.0394)	
training:	Epoch: [43][143/233]	Loss 0.0061 (0.0392)	
training:	Epoch: [43][144/233]	Loss 0.0059 (0.0390)	
training:	Epoch: [43][145/233]	Loss 0.0059 (0.0387)	
training:	Epoch: [43][146/233]	Loss 0.0050 (0.0385)	
training:	Epoch: [43][147/233]	Loss 0.0062 (0.0383)	
training:	Epoch: [43][148/233]	Loss 0.0107 (0.0381)	
training:	Epoch: [43][149/233]	Loss 0.1467 (0.0388)	
training:	Epoch: [43][150/233]	Loss 0.0326 (0.0388)	
training:	Epoch: [43][151/233]	Loss 0.0273 (0.0387)	
training:	Epoch: [43][152/233]	Loss 0.1835 (0.0397)	
training:	Epoch: [43][153/233]	Loss 0.0094 (0.0395)	
training:	Epoch: [43][154/233]	Loss 0.0112 (0.0393)	
training:	Epoch: [43][155/233]	Loss 0.0070 (0.0391)	
training:	Epoch: [43][156/233]	Loss 0.0053 (0.0389)	
training:	Epoch: [43][157/233]	Loss 0.0890 (0.0392)	
training:	Epoch: [43][158/233]	Loss 0.0072 (0.0390)	
training:	Epoch: [43][159/233]	Loss 0.1153 (0.0394)	
training:	Epoch: [43][160/233]	Loss 0.1271 (0.0400)	
training:	Epoch: [43][161/233]	Loss 0.1211 (0.0405)	
training:	Epoch: [43][162/233]	Loss 0.1323 (0.0411)	
training:	Epoch: [43][163/233]	Loss 0.0059 (0.0409)	
training:	Epoch: [43][164/233]	Loss 0.0078 (0.0406)	
training:	Epoch: [43][165/233]	Loss 0.0072 (0.0404)	
training:	Epoch: [43][166/233]	Loss 0.0066 (0.0402)	
training:	Epoch: [43][167/233]	Loss 0.0076 (0.0400)	
training:	Epoch: [43][168/233]	Loss 0.0080 (0.0399)	
training:	Epoch: [43][169/233]	Loss 0.0086 (0.0397)	
training:	Epoch: [43][170/233]	Loss 0.0581 (0.0398)	
training:	Epoch: [43][171/233]	Loss 0.0106 (0.0396)	
training:	Epoch: [43][172/233]	Loss 0.0067 (0.0394)	
training:	Epoch: [43][173/233]	Loss 0.0075 (0.0392)	
training:	Epoch: [43][174/233]	Loss 0.0177 (0.0391)	
training:	Epoch: [43][175/233]	Loss 0.0060 (0.0389)	
training:	Epoch: [43][176/233]	Loss 0.0059 (0.0387)	
training:	Epoch: [43][177/233]	Loss 0.0050 (0.0385)	
training:	Epoch: [43][178/233]	Loss 0.0076 (0.0384)	
training:	Epoch: [43][179/233]	Loss 0.0088 (0.0382)	
training:	Epoch: [43][180/233]	Loss 0.0052 (0.0380)	
training:	Epoch: [43][181/233]	Loss 0.0135 (0.0379)	
training:	Epoch: [43][182/233]	Loss 0.0144 (0.0378)	
training:	Epoch: [43][183/233]	Loss 0.0077 (0.0376)	
training:	Epoch: [43][184/233]	Loss 0.0071 (0.0374)	
training:	Epoch: [43][185/233]	Loss 0.0096 (0.0373)	
training:	Epoch: [43][186/233]	Loss 0.0080 (0.0371)	
training:	Epoch: [43][187/233]	Loss 0.0088 (0.0370)	
training:	Epoch: [43][188/233]	Loss 0.0071 (0.0368)	
training:	Epoch: [43][189/233]	Loss 0.0070 (0.0367)	
training:	Epoch: [43][190/233]	Loss 0.0140 (0.0365)	
training:	Epoch: [43][191/233]	Loss 0.0087 (0.0364)	
training:	Epoch: [43][192/233]	Loss 0.0078 (0.0362)	
training:	Epoch: [43][193/233]	Loss 0.1385 (0.0368)	
training:	Epoch: [43][194/233]	Loss 0.0291 (0.0367)	
training:	Epoch: [43][195/233]	Loss 0.1096 (0.0371)	
training:	Epoch: [43][196/233]	Loss 0.0313 (0.0371)	
training:	Epoch: [43][197/233]	Loss 0.1768 (0.0378)	
training:	Epoch: [43][198/233]	Loss 0.0634 (0.0379)	
training:	Epoch: [43][199/233]	Loss 0.0069 (0.0378)	
training:	Epoch: [43][200/233]	Loss 0.0101 (0.0376)	
training:	Epoch: [43][201/233]	Loss 0.0075 (0.0375)	
training:	Epoch: [43][202/233]	Loss 0.0087 (0.0373)	
training:	Epoch: [43][203/233]	Loss 0.0085 (0.0372)	
training:	Epoch: [43][204/233]	Loss 0.1674 (0.0378)	
training:	Epoch: [43][205/233]	Loss 0.0083 (0.0377)	
training:	Epoch: [43][206/233]	Loss 0.1853 (0.0384)	
training:	Epoch: [43][207/233]	Loss 0.0059 (0.0382)	
training:	Epoch: [43][208/233]	Loss 0.0937 (0.0385)	
training:	Epoch: [43][209/233]	Loss 0.0192 (0.0384)	
training:	Epoch: [43][210/233]	Loss 0.0064 (0.0383)	
training:	Epoch: [43][211/233]	Loss 0.0063 (0.0381)	
training:	Epoch: [43][212/233]	Loss 0.0392 (0.0381)	
training:	Epoch: [43][213/233]	Loss 0.0073 (0.0380)	
training:	Epoch: [43][214/233]	Loss 0.0242 (0.0379)	
training:	Epoch: [43][215/233]	Loss 0.0071 (0.0378)	
training:	Epoch: [43][216/233]	Loss 0.2182 (0.0386)	
training:	Epoch: [43][217/233]	Loss 0.1614 (0.0392)	
training:	Epoch: [43][218/233]	Loss 0.0196 (0.0391)	
training:	Epoch: [43][219/233]	Loss 0.0698 (0.0392)	
training:	Epoch: [43][220/233]	Loss 0.0698 (0.0394)	
training:	Epoch: [43][221/233]	Loss 0.0240 (0.0393)	
training:	Epoch: [43][222/233]	Loss 0.0137 (0.0392)	
training:	Epoch: [43][223/233]	Loss 0.0092 (0.0390)	
training:	Epoch: [43][224/233]	Loss 0.0415 (0.0390)	
training:	Epoch: [43][225/233]	Loss 0.0209 (0.0390)	
training:	Epoch: [43][226/233]	Loss 0.1701 (0.0395)	
training:	Epoch: [43][227/233]	Loss 0.0955 (0.0398)	
training:	Epoch: [43][228/233]	Loss 0.0095 (0.0397)	
training:	Epoch: [43][229/233]	Loss 0.0717 (0.0398)	
training:	Epoch: [43][230/233]	Loss 0.0126 (0.0397)	
training:	Epoch: [43][231/233]	Loss 0.0098 (0.0395)	
training:	Epoch: [43][232/233]	Loss 0.1294 (0.0399)	
training:	Epoch: [43][233/233]	Loss 0.0103 (0.0398)	
Training:	 Loss: 0.0397

Training:	 ACC: 0.9968 0.9968 0.9962 0.9975
Validation:	 ACC: 0.7888 0.7876 0.7630 0.8146
Validation:	 Best_BACC: 0.8162 0.8154 0.7998 0.8326
Validation:	 Loss: 0.8388
Pretraining:	Epoch 44/200
----------
training:	Epoch: [44][1/233]	Loss 0.0091 (0.0091)	
training:	Epoch: [44][2/233]	Loss 0.0090 (0.0091)	
training:	Epoch: [44][3/233]	Loss 0.0079 (0.0087)	
training:	Epoch: [44][4/233]	Loss 0.0259 (0.0130)	
training:	Epoch: [44][5/233]	Loss 0.0066 (0.0117)	
training:	Epoch: [44][6/233]	Loss 0.0557 (0.0190)	
training:	Epoch: [44][7/233]	Loss 0.1933 (0.0439)	
training:	Epoch: [44][8/233]	Loss 0.0086 (0.0395)	
training:	Epoch: [44][9/233]	Loss 0.1935 (0.0566)	
training:	Epoch: [44][10/233]	Loss 0.0090 (0.0519)	
training:	Epoch: [44][11/233]	Loss 0.0089 (0.0480)	
training:	Epoch: [44][12/233]	Loss 0.1139 (0.0534)	
training:	Epoch: [44][13/233]	Loss 0.0120 (0.0503)	
training:	Epoch: [44][14/233]	Loss 0.0106 (0.0474)	
training:	Epoch: [44][15/233]	Loss 0.0116 (0.0450)	
training:	Epoch: [44][16/233]	Loss 0.0075 (0.0427)	
training:	Epoch: [44][17/233]	Loss 0.1105 (0.0467)	
training:	Epoch: [44][18/233]	Loss 0.1873 (0.0545)	
training:	Epoch: [44][19/233]	Loss 0.0070 (0.0520)	
training:	Epoch: [44][20/233]	Loss 0.0068 (0.0497)	
training:	Epoch: [44][21/233]	Loss 0.0073 (0.0477)	
training:	Epoch: [44][22/233]	Loss 0.0227 (0.0466)	
training:	Epoch: [44][23/233]	Loss 0.1755 (0.0522)	
training:	Epoch: [44][24/233]	Loss 0.0112 (0.0505)	
training:	Epoch: [44][25/233]	Loss 0.0085 (0.0488)	
training:	Epoch: [44][26/233]	Loss 0.0057 (0.0471)	
training:	Epoch: [44][27/233]	Loss 0.0148 (0.0459)	
training:	Epoch: [44][28/233]	Loss 0.0073 (0.0446)	
training:	Epoch: [44][29/233]	Loss 0.0208 (0.0437)	
training:	Epoch: [44][30/233]	Loss 0.0198 (0.0429)	
training:	Epoch: [44][31/233]	Loss 0.0112 (0.0419)	
training:	Epoch: [44][32/233]	Loss 0.0421 (0.0419)	
training:	Epoch: [44][33/233]	Loss 0.1024 (0.0438)	
training:	Epoch: [44][34/233]	Loss 0.0163 (0.0429)	
training:	Epoch: [44][35/233]	Loss 0.0074 (0.0419)	
training:	Epoch: [44][36/233]	Loss 0.1573 (0.0451)	
training:	Epoch: [44][37/233]	Loss 0.0114 (0.0442)	
training:	Epoch: [44][38/233]	Loss 0.0061 (0.0432)	
training:	Epoch: [44][39/233]	Loss 0.0087 (0.0423)	
training:	Epoch: [44][40/233]	Loss 0.0276 (0.0420)	
training:	Epoch: [44][41/233]	Loss 0.0565 (0.0423)	
training:	Epoch: [44][42/233]	Loss 0.0056 (0.0414)	
training:	Epoch: [44][43/233]	Loss 0.0121 (0.0408)	
training:	Epoch: [44][44/233]	Loss 0.0066 (0.0400)	
training:	Epoch: [44][45/233]	Loss 0.0147 (0.0394)	
training:	Epoch: [44][46/233]	Loss 0.0257 (0.0391)	
training:	Epoch: [44][47/233]	Loss 0.0062 (0.0384)	
training:	Epoch: [44][48/233]	Loss 0.0204 (0.0381)	
training:	Epoch: [44][49/233]	Loss 0.0056 (0.0374)	
training:	Epoch: [44][50/233]	Loss 0.0076 (0.0368)	
training:	Epoch: [44][51/233]	Loss 0.0073 (0.0362)	
training:	Epoch: [44][52/233]	Loss 0.0166 (0.0358)	
training:	Epoch: [44][53/233]	Loss 0.0102 (0.0354)	
training:	Epoch: [44][54/233]	Loss 0.0115 (0.0349)	
training:	Epoch: [44][55/233]	Loss 0.0084 (0.0344)	
training:	Epoch: [44][56/233]	Loss 0.0059 (0.0339)	
training:	Epoch: [44][57/233]	Loss 0.0066 (0.0334)	
training:	Epoch: [44][58/233]	Loss 0.0070 (0.0330)	
training:	Epoch: [44][59/233]	Loss 0.0189 (0.0327)	
training:	Epoch: [44][60/233]	Loss 0.0090 (0.0323)	
training:	Epoch: [44][61/233]	Loss 0.0094 (0.0320)	
training:	Epoch: [44][62/233]	Loss 0.0080 (0.0316)	
training:	Epoch: [44][63/233]	Loss 0.0290 (0.0315)	
training:	Epoch: [44][64/233]	Loss 0.0133 (0.0313)	
training:	Epoch: [44][65/233]	Loss 0.0119 (0.0310)	
training:	Epoch: [44][66/233]	Loss 0.0114 (0.0307)	
training:	Epoch: [44][67/233]	Loss 0.0476 (0.0309)	
training:	Epoch: [44][68/233]	Loss 0.0208 (0.0308)	
training:	Epoch: [44][69/233]	Loss 0.0088 (0.0305)	
training:	Epoch: [44][70/233]	Loss 0.0050 (0.0301)	
training:	Epoch: [44][71/233]	Loss 0.0086 (0.0298)	
training:	Epoch: [44][72/233]	Loss 0.0051 (0.0294)	
training:	Epoch: [44][73/233]	Loss 0.0062 (0.0291)	
training:	Epoch: [44][74/233]	Loss 0.0214 (0.0290)	
training:	Epoch: [44][75/233]	Loss 0.0054 (0.0287)	
training:	Epoch: [44][76/233]	Loss 0.0247 (0.0287)	
training:	Epoch: [44][77/233]	Loss 0.0851 (0.0294)	
training:	Epoch: [44][78/233]	Loss 0.0125 (0.0292)	
training:	Epoch: [44][79/233]	Loss 0.1772 (0.0310)	
training:	Epoch: [44][80/233]	Loss 0.0229 (0.0309)	
training:	Epoch: [44][81/233]	Loss 0.0063 (0.0306)	
training:	Epoch: [44][82/233]	Loss 0.0293 (0.0306)	
training:	Epoch: [44][83/233]	Loss 0.0049 (0.0303)	
training:	Epoch: [44][84/233]	Loss 0.0124 (0.0301)	
training:	Epoch: [44][85/233]	Loss 0.1402 (0.0314)	
training:	Epoch: [44][86/233]	Loss 0.0122 (0.0312)	
training:	Epoch: [44][87/233]	Loss 0.0130 (0.0310)	
training:	Epoch: [44][88/233]	Loss 0.0145 (0.0308)	
training:	Epoch: [44][89/233]	Loss 0.0092 (0.0305)	
training:	Epoch: [44][90/233]	Loss 0.0117 (0.0303)	
training:	Epoch: [44][91/233]	Loss 0.0075 (0.0301)	
training:	Epoch: [44][92/233]	Loss 0.0067 (0.0298)	
training:	Epoch: [44][93/233]	Loss 0.0444 (0.0300)	
training:	Epoch: [44][94/233]	Loss 0.0060 (0.0297)	
training:	Epoch: [44][95/233]	Loss 0.1786 (0.0313)	
training:	Epoch: [44][96/233]	Loss 0.0127 (0.0311)	
training:	Epoch: [44][97/233]	Loss 0.0057 (0.0308)	
training:	Epoch: [44][98/233]	Loss 0.0056 (0.0306)	
training:	Epoch: [44][99/233]	Loss 0.0067 (0.0303)	
training:	Epoch: [44][100/233]	Loss 0.0056 (0.0301)	
training:	Epoch: [44][101/233]	Loss 0.0057 (0.0298)	
training:	Epoch: [44][102/233]	Loss 0.1509 (0.0310)	
training:	Epoch: [44][103/233]	Loss 0.0117 (0.0308)	
training:	Epoch: [44][104/233]	Loss 0.0086 (0.0306)	
training:	Epoch: [44][105/233]	Loss 0.0061 (0.0304)	
training:	Epoch: [44][106/233]	Loss 0.0060 (0.0302)	
training:	Epoch: [44][107/233]	Loss 0.0117 (0.0300)	
training:	Epoch: [44][108/233]	Loss 0.0109 (0.0298)	
training:	Epoch: [44][109/233]	Loss 0.0055 (0.0296)	
training:	Epoch: [44][110/233]	Loss 0.0123 (0.0294)	
training:	Epoch: [44][111/233]	Loss 0.0057 (0.0292)	
training:	Epoch: [44][112/233]	Loss 0.0050 (0.0290)	
training:	Epoch: [44][113/233]	Loss 0.0072 (0.0288)	
training:	Epoch: [44][114/233]	Loss 0.0070 (0.0286)	
training:	Epoch: [44][115/233]	Loss 0.1725 (0.0299)	
training:	Epoch: [44][116/233]	Loss 0.0059 (0.0297)	
training:	Epoch: [44][117/233]	Loss 0.0077 (0.0295)	
training:	Epoch: [44][118/233]	Loss 0.0168 (0.0294)	
training:	Epoch: [44][119/233]	Loss 0.0080 (0.0292)	
training:	Epoch: [44][120/233]	Loss 0.0145 (0.0291)	
training:	Epoch: [44][121/233]	Loss 0.0121 (0.0289)	
training:	Epoch: [44][122/233]	Loss 0.1637 (0.0300)	
training:	Epoch: [44][123/233]	Loss 0.0055 (0.0298)	
training:	Epoch: [44][124/233]	Loss 0.0083 (0.0297)	
training:	Epoch: [44][125/233]	Loss 0.0106 (0.0295)	
training:	Epoch: [44][126/233]	Loss 0.0063 (0.0293)	
training:	Epoch: [44][127/233]	Loss 0.0088 (0.0292)	
training:	Epoch: [44][128/233]	Loss 0.0086 (0.0290)	
training:	Epoch: [44][129/233]	Loss 0.0115 (0.0289)	
training:	Epoch: [44][130/233]	Loss 0.0079 (0.0287)	
training:	Epoch: [44][131/233]	Loss 0.0070 (0.0285)	
training:	Epoch: [44][132/233]	Loss 0.0055 (0.0284)	
training:	Epoch: [44][133/233]	Loss 0.0056 (0.0282)	
training:	Epoch: [44][134/233]	Loss 0.0105 (0.0281)	
training:	Epoch: [44][135/233]	Loss 0.0049 (0.0279)	
training:	Epoch: [44][136/233]	Loss 0.0087 (0.0277)	
training:	Epoch: [44][137/233]	Loss 0.0172 (0.0277)	
training:	Epoch: [44][138/233]	Loss 0.0053 (0.0275)	
training:	Epoch: [44][139/233]	Loss 0.0166 (0.0274)	
training:	Epoch: [44][140/233]	Loss 0.0069 (0.0273)	
training:	Epoch: [44][141/233]	Loss 0.0069 (0.0271)	
training:	Epoch: [44][142/233]	Loss 0.0078 (0.0270)	
training:	Epoch: [44][143/233]	Loss 0.0045 (0.0268)	
training:	Epoch: [44][144/233]	Loss 0.0056 (0.0267)	
training:	Epoch: [44][145/233]	Loss 0.0062 (0.0266)	
training:	Epoch: [44][146/233]	Loss 0.0297 (0.0266)	
training:	Epoch: [44][147/233]	Loss 0.0074 (0.0264)	
training:	Epoch: [44][148/233]	Loss 0.0082 (0.0263)	
training:	Epoch: [44][149/233]	Loss 0.0050 (0.0262)	
training:	Epoch: [44][150/233]	Loss 0.0076 (0.0261)	
training:	Epoch: [44][151/233]	Loss 0.0055 (0.0259)	
training:	Epoch: [44][152/233]	Loss 0.0119 (0.0258)	
training:	Epoch: [44][153/233]	Loss 0.0229 (0.0258)	
training:	Epoch: [44][154/233]	Loss 0.0050 (0.0257)	
training:	Epoch: [44][155/233]	Loss 0.0196 (0.0256)	
training:	Epoch: [44][156/233]	Loss 0.0065 (0.0255)	
training:	Epoch: [44][157/233]	Loss 0.0061 (0.0254)	
training:	Epoch: [44][158/233]	Loss 0.0048 (0.0253)	
training:	Epoch: [44][159/233]	Loss 0.0090 (0.0252)	
training:	Epoch: [44][160/233]	Loss 0.0076 (0.0250)	
training:	Epoch: [44][161/233]	Loss 0.0056 (0.0249)	
training:	Epoch: [44][162/233]	Loss 0.0058 (0.0248)	
training:	Epoch: [44][163/233]	Loss 0.0993 (0.0253)	
training:	Epoch: [44][164/233]	Loss 0.0214 (0.0252)	
training:	Epoch: [44][165/233]	Loss 0.0100 (0.0251)	
training:	Epoch: [44][166/233]	Loss 0.0147 (0.0251)	
training:	Epoch: [44][167/233]	Loss 0.0136 (0.0250)	
training:	Epoch: [44][168/233]	Loss 0.0048 (0.0249)	
training:	Epoch: [44][169/233]	Loss 0.1519 (0.0256)	
training:	Epoch: [44][170/233]	Loss 0.0088 (0.0255)	
training:	Epoch: [44][171/233]	Loss 0.0058 (0.0254)	
training:	Epoch: [44][172/233]	Loss 0.0053 (0.0253)	
training:	Epoch: [44][173/233]	Loss 0.0095 (0.0252)	
training:	Epoch: [44][174/233]	Loss 0.0835 (0.0256)	
training:	Epoch: [44][175/233]	Loss 0.0059 (0.0254)	
training:	Epoch: [44][176/233]	Loss 0.0084 (0.0254)	
training:	Epoch: [44][177/233]	Loss 0.0070 (0.0252)	
training:	Epoch: [44][178/233]	Loss 0.0055 (0.0251)	
training:	Epoch: [44][179/233]	Loss 0.0145 (0.0251)	
training:	Epoch: [44][180/233]	Loss 0.1381 (0.0257)	
training:	Epoch: [44][181/233]	Loss 0.0061 (0.0256)	
training:	Epoch: [44][182/233]	Loss 0.0054 (0.0255)	
training:	Epoch: [44][183/233]	Loss 0.0069 (0.0254)	
training:	Epoch: [44][184/233]	Loss 0.0190 (0.0253)	
training:	Epoch: [44][185/233]	Loss 0.0061 (0.0252)	
training:	Epoch: [44][186/233]	Loss 0.0151 (0.0252)	
training:	Epoch: [44][187/233]	Loss 0.0068 (0.0251)	
training:	Epoch: [44][188/233]	Loss 0.0056 (0.0250)	
training:	Epoch: [44][189/233]	Loss 0.0050 (0.0249)	
training:	Epoch: [44][190/233]	Loss 0.0049 (0.0248)	
training:	Epoch: [44][191/233]	Loss 0.1127 (0.0252)	
training:	Epoch: [44][192/233]	Loss 0.1999 (0.0261)	
training:	Epoch: [44][193/233]	Loss 0.1785 (0.0269)	
training:	Epoch: [44][194/233]	Loss 0.0056 (0.0268)	
training:	Epoch: [44][195/233]	Loss 0.0084 (0.0267)	
training:	Epoch: [44][196/233]	Loss 0.0057 (0.0266)	
training:	Epoch: [44][197/233]	Loss 0.0064 (0.0265)	
training:	Epoch: [44][198/233]	Loss 0.0063 (0.0264)	
training:	Epoch: [44][199/233]	Loss 0.0058 (0.0263)	
training:	Epoch: [44][200/233]	Loss 0.0094 (0.0262)	
training:	Epoch: [44][201/233]	Loss 0.0970 (0.0266)	
training:	Epoch: [44][202/233]	Loss 0.0067 (0.0265)	
training:	Epoch: [44][203/233]	Loss 0.0068 (0.0264)	
training:	Epoch: [44][204/233]	Loss 0.0056 (0.0263)	
training:	Epoch: [44][205/233]	Loss 0.0900 (0.0266)	
training:	Epoch: [44][206/233]	Loss 0.0076 (0.0265)	
training:	Epoch: [44][207/233]	Loss 0.1229 (0.0270)	
training:	Epoch: [44][208/233]	Loss 0.0082 (0.0269)	
training:	Epoch: [44][209/233]	Loss 0.0060 (0.0268)	
training:	Epoch: [44][210/233]	Loss 0.0545 (0.0269)	
training:	Epoch: [44][211/233]	Loss 0.0093 (0.0268)	
training:	Epoch: [44][212/233]	Loss 0.1697 (0.0275)	
training:	Epoch: [44][213/233]	Loss 0.0118 (0.0274)	
training:	Epoch: [44][214/233]	Loss 0.0051 (0.0273)	
training:	Epoch: [44][215/233]	Loss 0.0055 (0.0272)	
training:	Epoch: [44][216/233]	Loss 0.0052 (0.0271)	
training:	Epoch: [44][217/233]	Loss 0.0123 (0.0271)	
training:	Epoch: [44][218/233]	Loss 0.0260 (0.0270)	
training:	Epoch: [44][219/233]	Loss 0.0647 (0.0272)	
training:	Epoch: [44][220/233]	Loss 0.0052 (0.0271)	
training:	Epoch: [44][221/233]	Loss 0.1908 (0.0279)	
training:	Epoch: [44][222/233]	Loss 0.1301 (0.0283)	
training:	Epoch: [44][223/233]	Loss 0.0047 (0.0282)	
training:	Epoch: [44][224/233]	Loss 0.0328 (0.0282)	
training:	Epoch: [44][225/233]	Loss 0.0156 (0.0282)	
training:	Epoch: [44][226/233]	Loss 0.0086 (0.0281)	
training:	Epoch: [44][227/233]	Loss 0.0058 (0.0280)	
training:	Epoch: [44][228/233]	Loss 0.0057 (0.0279)	
training:	Epoch: [44][229/233]	Loss 0.0168 (0.0278)	
training:	Epoch: [44][230/233]	Loss 0.0196 (0.0278)	
training:	Epoch: [44][231/233]	Loss 0.1740 (0.0284)	
training:	Epoch: [44][232/233]	Loss 0.0094 (0.0284)	
training:	Epoch: [44][233/233]	Loss 0.0091 (0.0283)	
Training:	 Loss: 0.0282

Training:	 ACC: 0.9969 0.9969 0.9967 0.9972
Validation:	 ACC: 0.7943 0.7945 0.7998 0.7888
Validation:	 Best_BACC: 0.8162 0.8154 0.7998 0.8326
Validation:	 Loss: 0.8467
Pretraining:	Epoch 45/200
----------
training:	Epoch: [45][1/233]	Loss 0.0206 (0.0206)	
training:	Epoch: [45][2/233]	Loss 0.0647 (0.0426)	
training:	Epoch: [45][3/233]	Loss 0.1750 (0.0868)	
training:	Epoch: [45][4/233]	Loss 0.1435 (0.1009)	
training:	Epoch: [45][5/233]	Loss 0.0081 (0.0824)	
training:	Epoch: [45][6/233]	Loss 0.1796 (0.0986)	
training:	Epoch: [45][7/233]	Loss 0.0062 (0.0854)	
training:	Epoch: [45][8/233]	Loss 0.0122 (0.0762)	
training:	Epoch: [45][9/233]	Loss 0.1615 (0.0857)	
training:	Epoch: [45][10/233]	Loss 0.0080 (0.0779)	
training:	Epoch: [45][11/233]	Loss 0.0073 (0.0715)	
training:	Epoch: [45][12/233]	Loss 0.0093 (0.0663)	
training:	Epoch: [45][13/233]	Loss 0.0128 (0.0622)	
training:	Epoch: [45][14/233]	Loss 0.1488 (0.0684)	
training:	Epoch: [45][15/233]	Loss 0.0051 (0.0642)	
training:	Epoch: [45][16/233]	Loss 0.0053 (0.0605)	
training:	Epoch: [45][17/233]	Loss 0.0065 (0.0573)	
training:	Epoch: [45][18/233]	Loss 0.0134 (0.0549)	
training:	Epoch: [45][19/233]	Loss 0.1467 (0.0597)	
training:	Epoch: [45][20/233]	Loss 0.0226 (0.0579)	
training:	Epoch: [45][21/233]	Loss 0.0245 (0.0563)	
training:	Epoch: [45][22/233]	Loss 0.0045 (0.0539)	
training:	Epoch: [45][23/233]	Loss 0.0061 (0.0518)	
training:	Epoch: [45][24/233]	Loss 0.0054 (0.0499)	
training:	Epoch: [45][25/233]	Loss 0.0055 (0.0481)	
training:	Epoch: [45][26/233]	Loss 0.0048 (0.0465)	
training:	Epoch: [45][27/233]	Loss 0.0084 (0.0450)	
training:	Epoch: [45][28/233]	Loss 0.0077 (0.0437)	
training:	Epoch: [45][29/233]	Loss 0.0093 (0.0425)	
training:	Epoch: [45][30/233]	Loss 0.0061 (0.0413)	
training:	Epoch: [45][31/233]	Loss 0.1227 (0.0439)	
training:	Epoch: [45][32/233]	Loss 0.0145 (0.0430)	
training:	Epoch: [45][33/233]	Loss 0.0447 (0.0431)	
training:	Epoch: [45][34/233]	Loss 0.0057 (0.0420)	
training:	Epoch: [45][35/233]	Loss 0.0052 (0.0409)	
training:	Epoch: [45][36/233]	Loss 0.0091 (0.0400)	
training:	Epoch: [45][37/233]	Loss 0.0106 (0.0392)	
training:	Epoch: [45][38/233]	Loss 0.0076 (0.0384)	
training:	Epoch: [45][39/233]	Loss 0.0117 (0.0377)	
training:	Epoch: [45][40/233]	Loss 0.0470 (0.0380)	
training:	Epoch: [45][41/233]	Loss 0.0052 (0.0372)	
training:	Epoch: [45][42/233]	Loss 0.0056 (0.0364)	
training:	Epoch: [45][43/233]	Loss 0.0100 (0.0358)	
training:	Epoch: [45][44/233]	Loss 0.0125 (0.0353)	
training:	Epoch: [45][45/233]	Loss 0.0965 (0.0366)	
training:	Epoch: [45][46/233]	Loss 0.0057 (0.0360)	
training:	Epoch: [45][47/233]	Loss 0.0049 (0.0353)	
training:	Epoch: [45][48/233]	Loss 0.0102 (0.0348)	
training:	Epoch: [45][49/233]	Loss 0.0814 (0.0357)	
training:	Epoch: [45][50/233]	Loss 0.0053 (0.0351)	
training:	Epoch: [45][51/233]	Loss 0.0061 (0.0345)	
training:	Epoch: [45][52/233]	Loss 0.0405 (0.0347)	
training:	Epoch: [45][53/233]	Loss 0.0189 (0.0344)	
training:	Epoch: [45][54/233]	Loss 0.0116 (0.0339)	
training:	Epoch: [45][55/233]	Loss 0.0050 (0.0334)	
training:	Epoch: [45][56/233]	Loss 0.0057 (0.0329)	
training:	Epoch: [45][57/233]	Loss 0.0198 (0.0327)	
training:	Epoch: [45][58/233]	Loss 0.0131 (0.0323)	
training:	Epoch: [45][59/233]	Loss 0.1499 (0.0343)	
training:	Epoch: [45][60/233]	Loss 0.0514 (0.0346)	
training:	Epoch: [45][61/233]	Loss 0.0230 (0.0344)	
training:	Epoch: [45][62/233]	Loss 0.0070 (0.0340)	
training:	Epoch: [45][63/233]	Loss 0.0059 (0.0335)	
training:	Epoch: [45][64/233]	Loss 0.0061 (0.0331)	
training:	Epoch: [45][65/233]	Loss 0.0058 (0.0327)	
training:	Epoch: [45][66/233]	Loss 0.0080 (0.0323)	
training:	Epoch: [45][67/233]	Loss 0.0097 (0.0320)	
training:	Epoch: [45][68/233]	Loss 0.1238 (0.0333)	
training:	Epoch: [45][69/233]	Loss 0.0148 (0.0331)	
training:	Epoch: [45][70/233]	Loss 0.0051 (0.0327)	
training:	Epoch: [45][71/233]	Loss 0.0075 (0.0323)	
training:	Epoch: [45][72/233]	Loss 0.0067 (0.0320)	
training:	Epoch: [45][73/233]	Loss 0.0055 (0.0316)	
training:	Epoch: [45][74/233]	Loss 0.0058 (0.0312)	
training:	Epoch: [45][75/233]	Loss 0.0068 (0.0309)	
training:	Epoch: [45][76/233]	Loss 0.1410 (0.0324)	
training:	Epoch: [45][77/233]	Loss 0.2593 (0.0353)	
training:	Epoch: [45][78/233]	Loss 0.0107 (0.0350)	
training:	Epoch: [45][79/233]	Loss 0.0142 (0.0347)	
training:	Epoch: [45][80/233]	Loss 0.0963 (0.0355)	
training:	Epoch: [45][81/233]	Loss 0.0111 (0.0352)	
training:	Epoch: [45][82/233]	Loss 0.0076 (0.0349)	
training:	Epoch: [45][83/233]	Loss 0.0070 (0.0345)	
training:	Epoch: [45][84/233]	Loss 0.0057 (0.0342)	
training:	Epoch: [45][85/233]	Loss 0.0082 (0.0339)	
training:	Epoch: [45][86/233]	Loss 0.0107 (0.0336)	
training:	Epoch: [45][87/233]	Loss 0.1778 (0.0353)	
training:	Epoch: [45][88/233]	Loss 0.0067 (0.0349)	
training:	Epoch: [45][89/233]	Loss 0.0071 (0.0346)	
training:	Epoch: [45][90/233]	Loss 0.0076 (0.0343)	
training:	Epoch: [45][91/233]	Loss 0.0058 (0.0340)	
training:	Epoch: [45][92/233]	Loss 0.0067 (0.0337)	
training:	Epoch: [45][93/233]	Loss 0.0072 (0.0334)	
training:	Epoch: [45][94/233]	Loss 0.0569 (0.0337)	
training:	Epoch: [45][95/233]	Loss 0.0065 (0.0334)	
training:	Epoch: [45][96/233]	Loss 0.0071 (0.0331)	
training:	Epoch: [45][97/233]	Loss 0.0075 (0.0329)	
training:	Epoch: [45][98/233]	Loss 0.1407 (0.0340)	
training:	Epoch: [45][99/233]	Loss 0.0624 (0.0342)	
training:	Epoch: [45][100/233]	Loss 0.0102 (0.0340)	
training:	Epoch: [45][101/233]	Loss 0.0056 (0.0337)	
training:	Epoch: [45][102/233]	Loss 0.0071 (0.0335)	
training:	Epoch: [45][103/233]	Loss 0.0051 (0.0332)	
training:	Epoch: [45][104/233]	Loss 0.0070 (0.0329)	
training:	Epoch: [45][105/233]	Loss 0.0112 (0.0327)	
training:	Epoch: [45][106/233]	Loss 0.0065 (0.0325)	
training:	Epoch: [45][107/233]	Loss 0.0082 (0.0323)	
training:	Epoch: [45][108/233]	Loss 0.0060 (0.0320)	
training:	Epoch: [45][109/233]	Loss 0.0068 (0.0318)	
training:	Epoch: [45][110/233]	Loss 0.0046 (0.0315)	
training:	Epoch: [45][111/233]	Loss 0.1877 (0.0329)	
training:	Epoch: [45][112/233]	Loss 0.0054 (0.0327)	
training:	Epoch: [45][113/233]	Loss 0.0099 (0.0325)	
training:	Epoch: [45][114/233]	Loss 0.0857 (0.0330)	
training:	Epoch: [45][115/233]	Loss 0.0091 (0.0328)	
training:	Epoch: [45][116/233]	Loss 0.0051 (0.0325)	
training:	Epoch: [45][117/233]	Loss 0.0100 (0.0323)	
training:	Epoch: [45][118/233]	Loss 0.1672 (0.0335)	
training:	Epoch: [45][119/233]	Loss 0.0044 (0.0332)	
training:	Epoch: [45][120/233]	Loss 0.1634 (0.0343)	
training:	Epoch: [45][121/233]	Loss 0.0060 (0.0341)	
training:	Epoch: [45][122/233]	Loss 0.0404 (0.0341)	
training:	Epoch: [45][123/233]	Loss 0.1680 (0.0352)	
training:	Epoch: [45][124/233]	Loss 0.2467 (0.0369)	
training:	Epoch: [45][125/233]	Loss 0.0061 (0.0367)	
training:	Epoch: [45][126/233]	Loss 0.1782 (0.0378)	
training:	Epoch: [45][127/233]	Loss 0.0166 (0.0376)	
training:	Epoch: [45][128/233]	Loss 0.0095 (0.0374)	
training:	Epoch: [45][129/233]	Loss 0.0060 (0.0372)	
training:	Epoch: [45][130/233]	Loss 0.0048 (0.0369)	
training:	Epoch: [45][131/233]	Loss 0.0056 (0.0367)	
training:	Epoch: [45][132/233]	Loss 0.1598 (0.0376)	
training:	Epoch: [45][133/233]	Loss 0.0193 (0.0375)	
training:	Epoch: [45][134/233]	Loss 0.0105 (0.0373)	
training:	Epoch: [45][135/233]	Loss 0.0064 (0.0370)	
training:	Epoch: [45][136/233]	Loss 0.0082 (0.0368)	
training:	Epoch: [45][137/233]	Loss 0.0209 (0.0367)	
training:	Epoch: [45][138/233]	Loss 0.0295 (0.0367)	
training:	Epoch: [45][139/233]	Loss 0.0083 (0.0365)	
training:	Epoch: [45][140/233]	Loss 0.0101 (0.0363)	
training:	Epoch: [45][141/233]	Loss 0.0200 (0.0362)	
training:	Epoch: [45][142/233]	Loss 0.0073 (0.0360)	
training:	Epoch: [45][143/233]	Loss 0.0070 (0.0357)	
training:	Epoch: [45][144/233]	Loss 0.0050 (0.0355)	
training:	Epoch: [45][145/233]	Loss 0.0176 (0.0354)	
training:	Epoch: [45][146/233]	Loss 0.0123 (0.0353)	
training:	Epoch: [45][147/233]	Loss 0.0057 (0.0351)	
training:	Epoch: [45][148/233]	Loss 0.0053 (0.0349)	
training:	Epoch: [45][149/233]	Loss 0.0053 (0.0347)	
training:	Epoch: [45][150/233]	Loss 0.0054 (0.0345)	
training:	Epoch: [45][151/233]	Loss 0.0048 (0.0343)	
training:	Epoch: [45][152/233]	Loss 0.0844 (0.0346)	
training:	Epoch: [45][153/233]	Loss 0.0096 (0.0344)	
training:	Epoch: [45][154/233]	Loss 0.0111 (0.0343)	
training:	Epoch: [45][155/233]	Loss 0.0095 (0.0341)	
training:	Epoch: [45][156/233]	Loss 0.0265 (0.0341)	
training:	Epoch: [45][157/233]	Loss 0.0294 (0.0340)	
training:	Epoch: [45][158/233]	Loss 0.0049 (0.0339)	
training:	Epoch: [45][159/233]	Loss 0.0047 (0.0337)	
training:	Epoch: [45][160/233]	Loss 0.1673 (0.0345)	
training:	Epoch: [45][161/233]	Loss 0.0405 (0.0345)	
training:	Epoch: [45][162/233]	Loss 0.1010 (0.0350)	
training:	Epoch: [45][163/233]	Loss 0.0059 (0.0348)	
training:	Epoch: [45][164/233]	Loss 0.0076 (0.0346)	
training:	Epoch: [45][165/233]	Loss 0.0053 (0.0344)	
training:	Epoch: [45][166/233]	Loss 0.0065 (0.0343)	
training:	Epoch: [45][167/233]	Loss 0.0291 (0.0342)	
training:	Epoch: [45][168/233]	Loss 0.0067 (0.0341)	
training:	Epoch: [45][169/233]	Loss 0.0069 (0.0339)	
training:	Epoch: [45][170/233]	Loss 0.0886 (0.0342)	
training:	Epoch: [45][171/233]	Loss 0.0045 (0.0341)	
training:	Epoch: [45][172/233]	Loss 0.0078 (0.0339)	
training:	Epoch: [45][173/233]	Loss 0.0094 (0.0338)	
training:	Epoch: [45][174/233]	Loss 0.0063 (0.0336)	
training:	Epoch: [45][175/233]	Loss 0.0141 (0.0335)	
training:	Epoch: [45][176/233]	Loss 0.0051 (0.0333)	
training:	Epoch: [45][177/233]	Loss 0.0060 (0.0332)	
training:	Epoch: [45][178/233]	Loss 0.0076 (0.0330)	
training:	Epoch: [45][179/233]	Loss 0.0049 (0.0329)	
training:	Epoch: [45][180/233]	Loss 0.0055 (0.0327)	
training:	Epoch: [45][181/233]	Loss 0.0046 (0.0326)	
training:	Epoch: [45][182/233]	Loss 0.0210 (0.0325)	
training:	Epoch: [45][183/233]	Loss 0.0132 (0.0324)	
training:	Epoch: [45][184/233]	Loss 0.0061 (0.0323)	
training:	Epoch: [45][185/233]	Loss 0.0116 (0.0321)	
training:	Epoch: [45][186/233]	Loss 0.0620 (0.0323)	
training:	Epoch: [45][187/233]	Loss 0.0512 (0.0324)	
training:	Epoch: [45][188/233]	Loss 0.0086 (0.0323)	
training:	Epoch: [45][189/233]	Loss 0.0186 (0.0322)	
training:	Epoch: [45][190/233]	Loss 0.0083 (0.0321)	
training:	Epoch: [45][191/233]	Loss 0.0062 (0.0319)	
training:	Epoch: [45][192/233]	Loss 0.2771 (0.0332)	
training:	Epoch: [45][193/233]	Loss 0.0059 (0.0331)	
training:	Epoch: [45][194/233]	Loss 0.0570 (0.0332)	
training:	Epoch: [45][195/233]	Loss 0.0089 (0.0331)	
training:	Epoch: [45][196/233]	Loss 0.1293 (0.0336)	
training:	Epoch: [45][197/233]	Loss 0.0067 (0.0334)	
training:	Epoch: [45][198/233]	Loss 0.0111 (0.0333)	
training:	Epoch: [45][199/233]	Loss 0.0076 (0.0332)	
training:	Epoch: [45][200/233]	Loss 0.0100 (0.0331)	
training:	Epoch: [45][201/233]	Loss 0.0050 (0.0329)	
training:	Epoch: [45][202/233]	Loss 0.0053 (0.0328)	
training:	Epoch: [45][203/233]	Loss 0.0094 (0.0327)	
training:	Epoch: [45][204/233]	Loss 0.0060 (0.0326)	
training:	Epoch: [45][205/233]	Loss 0.0093 (0.0324)	
training:	Epoch: [45][206/233]	Loss 0.0054 (0.0323)	
training:	Epoch: [45][207/233]	Loss 0.0078 (0.0322)	
training:	Epoch: [45][208/233]	Loss 0.0051 (0.0321)	
training:	Epoch: [45][209/233]	Loss 0.0063 (0.0319)	
training:	Epoch: [45][210/233]	Loss 0.0065 (0.0318)	
training:	Epoch: [45][211/233]	Loss 0.0153 (0.0317)	
training:	Epoch: [45][212/233]	Loss 0.1788 (0.0324)	
training:	Epoch: [45][213/233]	Loss 0.1911 (0.0332)	
training:	Epoch: [45][214/233]	Loss 0.0064 (0.0331)	
training:	Epoch: [45][215/233]	Loss 0.1927 (0.0338)	
training:	Epoch: [45][216/233]	Loss 0.0178 (0.0337)	
training:	Epoch: [45][217/233]	Loss 0.0059 (0.0336)	
training:	Epoch: [45][218/233]	Loss 0.0055 (0.0335)	
training:	Epoch: [45][219/233]	Loss 0.0075 (0.0333)	
training:	Epoch: [45][220/233]	Loss 0.0218 (0.0333)	
training:	Epoch: [45][221/233]	Loss 0.0057 (0.0332)	
training:	Epoch: [45][222/233]	Loss 0.0070 (0.0330)	
training:	Epoch: [45][223/233]	Loss 0.0066 (0.0329)	
training:	Epoch: [45][224/233]	Loss 0.0065 (0.0328)	
training:	Epoch: [45][225/233]	Loss 0.0133 (0.0327)	
training:	Epoch: [45][226/233]	Loss 0.0057 (0.0326)	
training:	Epoch: [45][227/233]	Loss 0.1425 (0.0331)	
training:	Epoch: [45][228/233]	Loss 0.1001 (0.0334)	
training:	Epoch: [45][229/233]	Loss 0.0510 (0.0335)	
training:	Epoch: [45][230/233]	Loss 0.0184 (0.0334)	
training:	Epoch: [45][231/233]	Loss 0.0119 (0.0333)	
training:	Epoch: [45][232/233]	Loss 0.0055 (0.0332)	
training:	Epoch: [45][233/233]	Loss 0.1732 (0.0338)	
Training:	 Loss: 0.0337

Training:	 ACC: 0.9972 0.9972 0.9974 0.9969
Validation:	 ACC: 0.7846 0.7860 0.8131 0.7562
Validation:	 Best_BACC: 0.8162 0.8154 0.7998 0.8326
Validation:	 Loss: 0.8762
Pretraining:	Epoch 46/200
----------
training:	Epoch: [46][1/233]	Loss 0.0087 (0.0087)	
training:	Epoch: [46][2/233]	Loss 0.0176 (0.0131)	
training:	Epoch: [46][3/233]	Loss 0.0365 (0.0209)	
training:	Epoch: [46][4/233]	Loss 0.1116 (0.0436)	
training:	Epoch: [46][5/233]	Loss 0.0055 (0.0360)	
training:	Epoch: [46][6/233]	Loss 0.0286 (0.0347)	
training:	Epoch: [46][7/233]	Loss 0.0048 (0.0305)	
training:	Epoch: [46][8/233]	Loss 0.0108 (0.0280)	
training:	Epoch: [46][9/233]	Loss 0.0077 (0.0257)	
training:	Epoch: [46][10/233]	Loss 0.0086 (0.0240)	
training:	Epoch: [46][11/233]	Loss 0.0054 (0.0223)	
training:	Epoch: [46][12/233]	Loss 0.0085 (0.0212)	
training:	Epoch: [46][13/233]	Loss 0.0091 (0.0203)	
training:	Epoch: [46][14/233]	Loss 0.0056 (0.0192)	
training:	Epoch: [46][15/233]	Loss 0.0063 (0.0184)	
training:	Epoch: [46][16/233]	Loss 0.0721 (0.0217)	
training:	Epoch: [46][17/233]	Loss 0.0066 (0.0208)	
training:	Epoch: [46][18/233]	Loss 0.0046 (0.0199)	
training:	Epoch: [46][19/233]	Loss 0.0056 (0.0192)	
training:	Epoch: [46][20/233]	Loss 0.0125 (0.0188)	
training:	Epoch: [46][21/233]	Loss 0.0052 (0.0182)	
training:	Epoch: [46][22/233]	Loss 0.1395 (0.0237)	
training:	Epoch: [46][23/233]	Loss 0.0064 (0.0229)	
training:	Epoch: [46][24/233]	Loss 0.0128 (0.0225)	
training:	Epoch: [46][25/233]	Loss 0.0047 (0.0218)	
training:	Epoch: [46][26/233]	Loss 0.0067 (0.0212)	
training:	Epoch: [46][27/233]	Loss 0.0066 (0.0207)	
training:	Epoch: [46][28/233]	Loss 0.0057 (0.0202)	
training:	Epoch: [46][29/233]	Loss 0.0053 (0.0196)	
training:	Epoch: [46][30/233]	Loss 0.0051 (0.0192)	
training:	Epoch: [46][31/233]	Loss 0.0161 (0.0191)	
training:	Epoch: [46][32/233]	Loss 0.0054 (0.0186)	
training:	Epoch: [46][33/233]	Loss 0.0056 (0.0182)	
training:	Epoch: [46][34/233]	Loss 0.0065 (0.0179)	
training:	Epoch: [46][35/233]	Loss 0.0116 (0.0177)	
training:	Epoch: [46][36/233]	Loss 0.0287 (0.0180)	
training:	Epoch: [46][37/233]	Loss 0.0053 (0.0177)	
training:	Epoch: [46][38/233]	Loss 0.1886 (0.0222)	
training:	Epoch: [46][39/233]	Loss 0.0126 (0.0219)	
training:	Epoch: [46][40/233]	Loss 0.0115 (0.0217)	
training:	Epoch: [46][41/233]	Loss 0.0071 (0.0213)	
training:	Epoch: [46][42/233]	Loss 0.0048 (0.0209)	
training:	Epoch: [46][43/233]	Loss 0.0671 (0.0220)	
training:	Epoch: [46][44/233]	Loss 0.0087 (0.0217)	
training:	Epoch: [46][45/233]	Loss 0.0058 (0.0213)	
training:	Epoch: [46][46/233]	Loss 0.0068 (0.0210)	
training:	Epoch: [46][47/233]	Loss 0.0072 (0.0207)	
training:	Epoch: [46][48/233]	Loss 0.0052 (0.0204)	
training:	Epoch: [46][49/233]	Loss 0.0780 (0.0216)	
training:	Epoch: [46][50/233]	Loss 0.0069 (0.0213)	
training:	Epoch: [46][51/233]	Loss 0.0070 (0.0210)	
training:	Epoch: [46][52/233]	Loss 0.0061 (0.0207)	
training:	Epoch: [46][53/233]	Loss 0.0208 (0.0207)	
training:	Epoch: [46][54/233]	Loss 0.0071 (0.0205)	
training:	Epoch: [46][55/233]	Loss 0.0064 (0.0202)	
training:	Epoch: [46][56/233]	Loss 0.0084 (0.0200)	
training:	Epoch: [46][57/233]	Loss 0.1706 (0.0226)	
training:	Epoch: [46][58/233]	Loss 0.0066 (0.0224)	
training:	Epoch: [46][59/233]	Loss 0.0129 (0.0222)	
training:	Epoch: [46][60/233]	Loss 0.0284 (0.0223)	
training:	Epoch: [46][61/233]	Loss 0.0243 (0.0223)	
training:	Epoch: [46][62/233]	Loss 0.1819 (0.0249)	
training:	Epoch: [46][63/233]	Loss 0.0053 (0.0246)	
training:	Epoch: [46][64/233]	Loss 0.0127 (0.0244)	
training:	Epoch: [46][65/233]	Loss 0.0112 (0.0242)	
training:	Epoch: [46][66/233]	Loss 0.0122 (0.0240)	
training:	Epoch: [46][67/233]	Loss 0.0080 (0.0238)	
training:	Epoch: [46][68/233]	Loss 0.0205 (0.0237)	
training:	Epoch: [46][69/233]	Loss 0.0059 (0.0235)	
training:	Epoch: [46][70/233]	Loss 0.0048 (0.0232)	
training:	Epoch: [46][71/233]	Loss 0.0159 (0.0231)	
training:	Epoch: [46][72/233]	Loss 0.0058 (0.0229)	
training:	Epoch: [46][73/233]	Loss 0.0091 (0.0227)	
training:	Epoch: [46][74/233]	Loss 0.0148 (0.0226)	
training:	Epoch: [46][75/233]	Loss 0.0050 (0.0223)	
training:	Epoch: [46][76/233]	Loss 0.0063 (0.0221)	
training:	Epoch: [46][77/233]	Loss 0.0065 (0.0219)	
training:	Epoch: [46][78/233]	Loss 0.0404 (0.0222)	
training:	Epoch: [46][79/233]	Loss 0.0063 (0.0220)	
training:	Epoch: [46][80/233]	Loss 0.0057 (0.0218)	
training:	Epoch: [46][81/233]	Loss 0.5057 (0.0277)	
training:	Epoch: [46][82/233]	Loss 0.0474 (0.0280)	
training:	Epoch: [46][83/233]	Loss 0.0224 (0.0279)	
training:	Epoch: [46][84/233]	Loss 0.0055 (0.0276)	
training:	Epoch: [46][85/233]	Loss 0.0069 (0.0274)	
training:	Epoch: [46][86/233]	Loss 0.0072 (0.0272)	
training:	Epoch: [46][87/233]	Loss 0.0042 (0.0269)	
training:	Epoch: [46][88/233]	Loss 0.0043 (0.0266)	
training:	Epoch: [46][89/233]	Loss 0.1592 (0.0281)	
training:	Epoch: [46][90/233]	Loss 0.0054 (0.0279)	
training:	Epoch: [46][91/233]	Loss 0.0042 (0.0276)	
training:	Epoch: [46][92/233]	Loss 0.0072 (0.0274)	
training:	Epoch: [46][93/233]	Loss 0.0046 (0.0271)	
training:	Epoch: [46][94/233]	Loss 0.0052 (0.0269)	
training:	Epoch: [46][95/233]	Loss 0.0049 (0.0267)	
training:	Epoch: [46][96/233]	Loss 0.0300 (0.0267)	
training:	Epoch: [46][97/233]	Loss 0.0066 (0.0265)	
training:	Epoch: [46][98/233]	Loss 0.0056 (0.0263)	
training:	Epoch: [46][99/233]	Loss 0.0067 (0.0261)	
training:	Epoch: [46][100/233]	Loss 0.0410 (0.0262)	
training:	Epoch: [46][101/233]	Loss 0.0077 (0.0261)	
training:	Epoch: [46][102/233]	Loss 0.0055 (0.0259)	
training:	Epoch: [46][103/233]	Loss 0.0184 (0.0258)	
training:	Epoch: [46][104/233]	Loss 0.0076 (0.0256)	
training:	Epoch: [46][105/233]	Loss 0.0116 (0.0255)	
training:	Epoch: [46][106/233]	Loss 0.0055 (0.0253)	
training:	Epoch: [46][107/233]	Loss 0.0057 (0.0251)	
training:	Epoch: [46][108/233]	Loss 0.0103 (0.0250)	
training:	Epoch: [46][109/233]	Loss 0.0050 (0.0248)	
training:	Epoch: [46][110/233]	Loss 0.2013 (0.0264)	
training:	Epoch: [46][111/233]	Loss 0.0056 (0.0262)	
training:	Epoch: [46][112/233]	Loss 0.0094 (0.0261)	
training:	Epoch: [46][113/233]	Loss 0.0452 (0.0262)	
training:	Epoch: [46][114/233]	Loss 0.0070 (0.0261)	
training:	Epoch: [46][115/233]	Loss 0.0100 (0.0259)	
training:	Epoch: [46][116/233]	Loss 0.0108 (0.0258)	
training:	Epoch: [46][117/233]	Loss 0.0174 (0.0257)	
training:	Epoch: [46][118/233]	Loss 0.0051 (0.0255)	
training:	Epoch: [46][119/233]	Loss 0.0099 (0.0254)	
training:	Epoch: [46][120/233]	Loss 0.0652 (0.0257)	
training:	Epoch: [46][121/233]	Loss 0.0083 (0.0256)	
training:	Epoch: [46][122/233]	Loss 0.0048 (0.0254)	
training:	Epoch: [46][123/233]	Loss 0.0303 (0.0255)	
training:	Epoch: [46][124/233]	Loss 0.0054 (0.0253)	
training:	Epoch: [46][125/233]	Loss 0.0082 (0.0252)	
training:	Epoch: [46][126/233]	Loss 0.0053 (0.0250)	
training:	Epoch: [46][127/233]	Loss 0.2173 (0.0265)	
training:	Epoch: [46][128/233]	Loss 0.0060 (0.0264)	
training:	Epoch: [46][129/233]	Loss 0.0064 (0.0262)	
training:	Epoch: [46][130/233]	Loss 0.0062 (0.0261)	
training:	Epoch: [46][131/233]	Loss 0.0051 (0.0259)	
training:	Epoch: [46][132/233]	Loss 0.0086 (0.0258)	
training:	Epoch: [46][133/233]	Loss 0.0102 (0.0256)	
training:	Epoch: [46][134/233]	Loss 0.0530 (0.0259)	
training:	Epoch: [46][135/233]	Loss 0.0754 (0.0262)	
training:	Epoch: [46][136/233]	Loss 0.0051 (0.0261)	
training:	Epoch: [46][137/233]	Loss 0.0058 (0.0259)	
training:	Epoch: [46][138/233]	Loss 0.0070 (0.0258)	
training:	Epoch: [46][139/233]	Loss 0.0045 (0.0256)	
training:	Epoch: [46][140/233]	Loss 0.0063 (0.0255)	
training:	Epoch: [46][141/233]	Loss 0.0085 (0.0254)	
training:	Epoch: [46][142/233]	Loss 0.0111 (0.0253)	
training:	Epoch: [46][143/233]	Loss 0.0046 (0.0251)	
training:	Epoch: [46][144/233]	Loss 0.0093 (0.0250)	
training:	Epoch: [46][145/233]	Loss 0.0068 (0.0249)	
training:	Epoch: [46][146/233]	Loss 0.1793 (0.0259)	
training:	Epoch: [46][147/233]	Loss 0.0145 (0.0259)	
training:	Epoch: [46][148/233]	Loss 0.1697 (0.0268)	
training:	Epoch: [46][149/233]	Loss 0.0055 (0.0267)	
training:	Epoch: [46][150/233]	Loss 0.0048 (0.0265)	
training:	Epoch: [46][151/233]	Loss 0.0059 (0.0264)	
training:	Epoch: [46][152/233]	Loss 0.0048 (0.0263)	
training:	Epoch: [46][153/233]	Loss 0.0133 (0.0262)	
training:	Epoch: [46][154/233]	Loss 0.0058 (0.0261)	
training:	Epoch: [46][155/233]	Loss 0.0120 (0.0260)	
training:	Epoch: [46][156/233]	Loss 0.0051 (0.0258)	
training:	Epoch: [46][157/233]	Loss 0.0075 (0.0257)	
training:	Epoch: [46][158/233]	Loss 0.0064 (0.0256)	
training:	Epoch: [46][159/233]	Loss 0.0048 (0.0255)	
training:	Epoch: [46][160/233]	Loss 0.0060 (0.0253)	
training:	Epoch: [46][161/233]	Loss 0.0077 (0.0252)	
training:	Epoch: [46][162/233]	Loss 0.0045 (0.0251)	
training:	Epoch: [46][163/233]	Loss 0.0052 (0.0250)	
training:	Epoch: [46][164/233]	Loss 0.0068 (0.0249)	
training:	Epoch: [46][165/233]	Loss 0.1793 (0.0258)	
training:	Epoch: [46][166/233]	Loss 0.0054 (0.0257)	
training:	Epoch: [46][167/233]	Loss 0.0545 (0.0259)	
training:	Epoch: [46][168/233]	Loss 0.0112 (0.0258)	
training:	Epoch: [46][169/233]	Loss 0.0082 (0.0257)	
training:	Epoch: [46][170/233]	Loss 0.0053 (0.0255)	
training:	Epoch: [46][171/233]	Loss 0.1710 (0.0264)	
training:	Epoch: [46][172/233]	Loss 0.0525 (0.0265)	
training:	Epoch: [46][173/233]	Loss 0.0051 (0.0264)	
training:	Epoch: [46][174/233]	Loss 0.0065 (0.0263)	
training:	Epoch: [46][175/233]	Loss 0.0198 (0.0263)	
training:	Epoch: [46][176/233]	Loss 0.0072 (0.0262)	
training:	Epoch: [46][177/233]	Loss 0.0055 (0.0260)	
training:	Epoch: [46][178/233]	Loss 0.0045 (0.0259)	
training:	Epoch: [46][179/233]	Loss 0.0118 (0.0258)	
training:	Epoch: [46][180/233]	Loss 0.1517 (0.0265)	
training:	Epoch: [46][181/233]	Loss 0.0137 (0.0265)	
training:	Epoch: [46][182/233]	Loss 0.0131 (0.0264)	
training:	Epoch: [46][183/233]	Loss 0.1761 (0.0272)	
training:	Epoch: [46][184/233]	Loss 0.0072 (0.0271)	
training:	Epoch: [46][185/233]	Loss 0.0080 (0.0270)	
training:	Epoch: [46][186/233]	Loss 0.1882 (0.0279)	
training:	Epoch: [46][187/233]	Loss 0.0045 (0.0277)	
training:	Epoch: [46][188/233]	Loss 0.0051 (0.0276)	
training:	Epoch: [46][189/233]	Loss 0.0186 (0.0276)	
training:	Epoch: [46][190/233]	Loss 0.0070 (0.0275)	
training:	Epoch: [46][191/233]	Loss 0.0090 (0.0274)	
training:	Epoch: [46][192/233]	Loss 0.0073 (0.0273)	
training:	Epoch: [46][193/233]	Loss 0.0111 (0.0272)	
training:	Epoch: [46][194/233]	Loss 0.0065 (0.0271)	
training:	Epoch: [46][195/233]	Loss 0.0100 (0.0270)	
training:	Epoch: [46][196/233]	Loss 0.0117 (0.0269)	
training:	Epoch: [46][197/233]	Loss 0.0050 (0.0268)	
training:	Epoch: [46][198/233]	Loss 0.0053 (0.0267)	
training:	Epoch: [46][199/233]	Loss 0.0050 (0.0266)	
training:	Epoch: [46][200/233]	Loss 0.0402 (0.0266)	
training:	Epoch: [46][201/233]	Loss 0.1319 (0.0272)	
training:	Epoch: [46][202/233]	Loss 0.0071 (0.0271)	
training:	Epoch: [46][203/233]	Loss 0.0047 (0.0270)	
training:	Epoch: [46][204/233]	Loss 0.0045 (0.0269)	
training:	Epoch: [46][205/233]	Loss 0.0060 (0.0268)	
training:	Epoch: [46][206/233]	Loss 0.0155 (0.0267)	
training:	Epoch: [46][207/233]	Loss 0.0049 (0.0266)	
training:	Epoch: [46][208/233]	Loss 0.0052 (0.0265)	
training:	Epoch: [46][209/233]	Loss 0.0064 (0.0264)	
training:	Epoch: [46][210/233]	Loss 0.0053 (0.0263)	
training:	Epoch: [46][211/233]	Loss 0.0113 (0.0262)	
training:	Epoch: [46][212/233]	Loss 0.0062 (0.0261)	
training:	Epoch: [46][213/233]	Loss 0.1176 (0.0266)	
training:	Epoch: [46][214/233]	Loss 0.0059 (0.0265)	
training:	Epoch: [46][215/233]	Loss 0.1792 (0.0272)	
training:	Epoch: [46][216/233]	Loss 0.0044 (0.0271)	
training:	Epoch: [46][217/233]	Loss 0.0081 (0.0270)	
training:	Epoch: [46][218/233]	Loss 0.0098 (0.0269)	
training:	Epoch: [46][219/233]	Loss 0.0057 (0.0268)	
training:	Epoch: [46][220/233]	Loss 0.1725 (0.0275)	
training:	Epoch: [46][221/233]	Loss 0.1427 (0.0280)	
training:	Epoch: [46][222/233]	Loss 0.0881 (0.0283)	
training:	Epoch: [46][223/233]	Loss 0.0692 (0.0284)	
training:	Epoch: [46][224/233]	Loss 0.0052 (0.0283)	
training:	Epoch: [46][225/233]	Loss 0.0048 (0.0282)	
training:	Epoch: [46][226/233]	Loss 0.0067 (0.0281)	
training:	Epoch: [46][227/233]	Loss 0.0047 (0.0280)	
training:	Epoch: [46][228/233]	Loss 0.0060 (0.0279)	
training:	Epoch: [46][229/233]	Loss 0.0068 (0.0278)	
training:	Epoch: [46][230/233]	Loss 0.3248 (0.0291)	
training:	Epoch: [46][231/233]	Loss 0.0105 (0.0291)	
training:	Epoch: [46][232/233]	Loss 0.0052 (0.0289)	
training:	Epoch: [46][233/233]	Loss 0.0044 (0.0288)	
Training:	 Loss: 0.0288

Training:	 ACC: 0.9979 0.9979 0.9977 0.9980
Validation:	 ACC: 0.7870 0.7871 0.7886 0.7854
Validation:	 Best_BACC: 0.8162 0.8154 0.7998 0.8326
Validation:	 Loss: 0.8768
Pretraining:	Epoch 47/200
----------
training:	Epoch: [47][1/233]	Loss 0.0048 (0.0048)	
training:	Epoch: [47][2/233]	Loss 0.0047 (0.0048)	
training:	Epoch: [47][3/233]	Loss 0.0214 (0.0103)	
training:	Epoch: [47][4/233]	Loss 0.0506 (0.0204)	
training:	Epoch: [47][5/233]	Loss 0.0041 (0.0171)	
training:	Epoch: [47][6/233]	Loss 0.0054 (0.0152)	
training:	Epoch: [47][7/233]	Loss 0.0052 (0.0137)	
training:	Epoch: [47][8/233]	Loss 0.0077 (0.0130)	
training:	Epoch: [47][9/233]	Loss 0.0853 (0.0210)	
training:	Epoch: [47][10/233]	Loss 0.0600 (0.0249)	
training:	Epoch: [47][11/233]	Loss 0.0068 (0.0233)	
training:	Epoch: [47][12/233]	Loss 0.0068 (0.0219)	
training:	Epoch: [47][13/233]	Loss 0.1760 (0.0338)	
training:	Epoch: [47][14/233]	Loss 0.0041 (0.0316)	
training:	Epoch: [47][15/233]	Loss 0.0387 (0.0321)	
training:	Epoch: [47][16/233]	Loss 0.0055 (0.0304)	
training:	Epoch: [47][17/233]	Loss 0.0091 (0.0292)	
training:	Epoch: [47][18/233]	Loss 0.0061 (0.0279)	
training:	Epoch: [47][19/233]	Loss 0.0045 (0.0267)	
training:	Epoch: [47][20/233]	Loss 0.0056 (0.0256)	
training:	Epoch: [47][21/233]	Loss 0.0094 (0.0248)	
training:	Epoch: [47][22/233]	Loss 0.0058 (0.0240)	
training:	Epoch: [47][23/233]	Loss 0.0059 (0.0232)	
training:	Epoch: [47][24/233]	Loss 0.0083 (0.0226)	
training:	Epoch: [47][25/233]	Loss 0.0170 (0.0224)	
training:	Epoch: [47][26/233]	Loss 0.0042 (0.0217)	
training:	Epoch: [47][27/233]	Loss 0.0048 (0.0210)	
training:	Epoch: [47][28/233]	Loss 0.0064 (0.0205)	
training:	Epoch: [47][29/233]	Loss 0.0045 (0.0200)	
training:	Epoch: [47][30/233]	Loss 0.0064 (0.0195)	
training:	Epoch: [47][31/233]	Loss 0.0081 (0.0191)	
training:	Epoch: [47][32/233]	Loss 0.0050 (0.0187)	
training:	Epoch: [47][33/233]	Loss 0.0058 (0.0183)	
training:	Epoch: [47][34/233]	Loss 0.0217 (0.0184)	
training:	Epoch: [47][35/233]	Loss 0.0137 (0.0183)	
training:	Epoch: [47][36/233]	Loss 0.0040 (0.0179)	
training:	Epoch: [47][37/233]	Loss 0.0389 (0.0184)	
training:	Epoch: [47][38/233]	Loss 0.0060 (0.0181)	
training:	Epoch: [47][39/233]	Loss 0.0048 (0.0178)	
training:	Epoch: [47][40/233]	Loss 0.0043 (0.0174)	
training:	Epoch: [47][41/233]	Loss 0.0168 (0.0174)	
training:	Epoch: [47][42/233]	Loss 0.0059 (0.0171)	
training:	Epoch: [47][43/233]	Loss 0.0140 (0.0171)	
training:	Epoch: [47][44/233]	Loss 0.0178 (0.0171)	
training:	Epoch: [47][45/233]	Loss 0.0060 (0.0168)	
training:	Epoch: [47][46/233]	Loss 0.0075 (0.0166)	
training:	Epoch: [47][47/233]	Loss 0.0090 (0.0165)	
training:	Epoch: [47][48/233]	Loss 0.0066 (0.0163)	
training:	Epoch: [47][49/233]	Loss 0.0484 (0.0169)	
training:	Epoch: [47][50/233]	Loss 0.0173 (0.0169)	
training:	Epoch: [47][51/233]	Loss 0.0048 (0.0167)	
training:	Epoch: [47][52/233]	Loss 0.0180 (0.0167)	
training:	Epoch: [47][53/233]	Loss 0.0075 (0.0165)	
training:	Epoch: [47][54/233]	Loss 0.0114 (0.0165)	
training:	Epoch: [47][55/233]	Loss 0.0068 (0.0163)	
training:	Epoch: [47][56/233]	Loss 0.0160 (0.0163)	
training:	Epoch: [47][57/233]	Loss 0.0641 (0.0171)	
training:	Epoch: [47][58/233]	Loss 0.0847 (0.0183)	
training:	Epoch: [47][59/233]	Loss 0.0061 (0.0181)	
training:	Epoch: [47][60/233]	Loss 0.0554 (0.0187)	
training:	Epoch: [47][61/233]	Loss 0.0094 (0.0185)	
training:	Epoch: [47][62/233]	Loss 0.0152 (0.0185)	
training:	Epoch: [47][63/233]	Loss 0.0068 (0.0183)	
training:	Epoch: [47][64/233]	Loss 0.0057 (0.0181)	
training:	Epoch: [47][65/233]	Loss 0.1254 (0.0198)	
training:	Epoch: [47][66/233]	Loss 0.0065 (0.0196)	
training:	Epoch: [47][67/233]	Loss 0.1927 (0.0221)	
training:	Epoch: [47][68/233]	Loss 0.0057 (0.0219)	
training:	Epoch: [47][69/233]	Loss 0.0117 (0.0217)	
training:	Epoch: [47][70/233]	Loss 0.0039 (0.0215)	
training:	Epoch: [47][71/233]	Loss 0.0052 (0.0213)	
training:	Epoch: [47][72/233]	Loss 0.1004 (0.0224)	
training:	Epoch: [47][73/233]	Loss 0.0256 (0.0224)	
training:	Epoch: [47][74/233]	Loss 0.0078 (0.0222)	
training:	Epoch: [47][75/233]	Loss 0.1822 (0.0243)	
training:	Epoch: [47][76/233]	Loss 0.0041 (0.0241)	
training:	Epoch: [47][77/233]	Loss 0.0357 (0.0242)	
training:	Epoch: [47][78/233]	Loss 0.0057 (0.0240)	
training:	Epoch: [47][79/233]	Loss 0.0173 (0.0239)	
training:	Epoch: [47][80/233]	Loss 0.0081 (0.0237)	
training:	Epoch: [47][81/233]	Loss 0.0101 (0.0235)	
training:	Epoch: [47][82/233]	Loss 0.0050 (0.0233)	
training:	Epoch: [47][83/233]	Loss 0.0063 (0.0231)	
training:	Epoch: [47][84/233]	Loss 0.0224 (0.0231)	
training:	Epoch: [47][85/233]	Loss 0.0049 (0.0229)	
training:	Epoch: [47][86/233]	Loss 0.0045 (0.0227)	
training:	Epoch: [47][87/233]	Loss 0.0050 (0.0225)	
training:	Epoch: [47][88/233]	Loss 0.0067 (0.0223)	
training:	Epoch: [47][89/233]	Loss 0.0053 (0.0221)	
training:	Epoch: [47][90/233]	Loss 0.0059 (0.0219)	
training:	Epoch: [47][91/233]	Loss 0.2038 (0.0239)	
training:	Epoch: [47][92/233]	Loss 0.0043 (0.0237)	
training:	Epoch: [47][93/233]	Loss 0.0243 (0.0237)	
training:	Epoch: [47][94/233]	Loss 0.0172 (0.0236)	
training:	Epoch: [47][95/233]	Loss 0.0059 (0.0235)	
training:	Epoch: [47][96/233]	Loss 0.0068 (0.0233)	
training:	Epoch: [47][97/233]	Loss 0.0052 (0.0231)	
training:	Epoch: [47][98/233]	Loss 0.0704 (0.0236)	
training:	Epoch: [47][99/233]	Loss 0.0103 (0.0234)	
training:	Epoch: [47][100/233]	Loss 0.0116 (0.0233)	
training:	Epoch: [47][101/233]	Loss 0.1688 (0.0248)	
training:	Epoch: [47][102/233]	Loss 0.1256 (0.0258)	
training:	Epoch: [47][103/233]	Loss 0.0055 (0.0256)	
training:	Epoch: [47][104/233]	Loss 0.0049 (0.0254)	
training:	Epoch: [47][105/233]	Loss 0.0038 (0.0252)	
training:	Epoch: [47][106/233]	Loss 0.0057 (0.0250)	
training:	Epoch: [47][107/233]	Loss 0.0258 (0.0250)	
training:	Epoch: [47][108/233]	Loss 0.0043 (0.0248)	
training:	Epoch: [47][109/233]	Loss 0.1892 (0.0263)	
training:	Epoch: [47][110/233]	Loss 0.0129 (0.0262)	
training:	Epoch: [47][111/233]	Loss 0.0092 (0.0260)	
training:	Epoch: [47][112/233]	Loss 0.0058 (0.0258)	
training:	Epoch: [47][113/233]	Loss 0.0655 (0.0262)	
training:	Epoch: [47][114/233]	Loss 0.0073 (0.0260)	
training:	Epoch: [47][115/233]	Loss 0.1154 (0.0268)	
training:	Epoch: [47][116/233]	Loss 0.0042 (0.0266)	
training:	Epoch: [47][117/233]	Loss 0.0089 (0.0265)	
training:	Epoch: [47][118/233]	Loss 0.0050 (0.0263)	
training:	Epoch: [47][119/233]	Loss 0.1081 (0.0270)	
training:	Epoch: [47][120/233]	Loss 0.0097 (0.0268)	
training:	Epoch: [47][121/233]	Loss 0.0079 (0.0267)	
training:	Epoch: [47][122/233]	Loss 0.0066 (0.0265)	
training:	Epoch: [47][123/233]	Loss 0.0050 (0.0263)	
training:	Epoch: [47][124/233]	Loss 0.0054 (0.0262)	
training:	Epoch: [47][125/233]	Loss 0.0085 (0.0260)	
training:	Epoch: [47][126/233]	Loss 0.0921 (0.0265)	
training:	Epoch: [47][127/233]	Loss 0.0116 (0.0264)	
training:	Epoch: [47][128/233]	Loss 0.0448 (0.0266)	
training:	Epoch: [47][129/233]	Loss 0.0064 (0.0264)	
training:	Epoch: [47][130/233]	Loss 0.0054 (0.0262)	
training:	Epoch: [47][131/233]	Loss 0.0720 (0.0266)	
training:	Epoch: [47][132/233]	Loss 0.0059 (0.0264)	
training:	Epoch: [47][133/233]	Loss 0.0079 (0.0263)	
training:	Epoch: [47][134/233]	Loss 0.0083 (0.0262)	
training:	Epoch: [47][135/233]	Loss 0.0525 (0.0264)	
training:	Epoch: [47][136/233]	Loss 0.0058 (0.0262)	
training:	Epoch: [47][137/233]	Loss 0.0095 (0.0261)	
training:	Epoch: [47][138/233]	Loss 0.0089 (0.0260)	
training:	Epoch: [47][139/233]	Loss 0.0056 (0.0258)	
training:	Epoch: [47][140/233]	Loss 0.0272 (0.0258)	
training:	Epoch: [47][141/233]	Loss 0.0063 (0.0257)	
training:	Epoch: [47][142/233]	Loss 0.0137 (0.0256)	
training:	Epoch: [47][143/233]	Loss 0.0041 (0.0254)	
training:	Epoch: [47][144/233]	Loss 0.0044 (0.0253)	
training:	Epoch: [47][145/233]	Loss 0.1776 (0.0264)	
training:	Epoch: [47][146/233]	Loss 0.0101 (0.0262)	
training:	Epoch: [47][147/233]	Loss 0.0060 (0.0261)	
training:	Epoch: [47][148/233]	Loss 0.0056 (0.0260)	
training:	Epoch: [47][149/233]	Loss 0.0045 (0.0258)	
training:	Epoch: [47][150/233]	Loss 0.0090 (0.0257)	
training:	Epoch: [47][151/233]	Loss 0.0041 (0.0256)	
training:	Epoch: [47][152/233]	Loss 0.0127 (0.0255)	
training:	Epoch: [47][153/233]	Loss 0.0059 (0.0254)	
training:	Epoch: [47][154/233]	Loss 0.0066 (0.0252)	
training:	Epoch: [47][155/233]	Loss 0.0071 (0.0251)	
training:	Epoch: [47][156/233]	Loss 0.0548 (0.0253)	
training:	Epoch: [47][157/233]	Loss 0.0049 (0.0252)	
training:	Epoch: [47][158/233]	Loss 0.0238 (0.0252)	
training:	Epoch: [47][159/233]	Loss 0.0039 (0.0250)	
training:	Epoch: [47][160/233]	Loss 0.0060 (0.0249)	
training:	Epoch: [47][161/233]	Loss 0.0077 (0.0248)	
training:	Epoch: [47][162/233]	Loss 0.0774 (0.0251)	
training:	Epoch: [47][163/233]	Loss 0.0070 (0.0250)	
training:	Epoch: [47][164/233]	Loss 0.0046 (0.0249)	
training:	Epoch: [47][165/233]	Loss 0.0290 (0.0249)	
training:	Epoch: [47][166/233]	Loss 0.0051 (0.0248)	
training:	Epoch: [47][167/233]	Loss 0.0047 (0.0247)	
training:	Epoch: [47][168/233]	Loss 0.1869 (0.0256)	
training:	Epoch: [47][169/233]	Loss 0.0487 (0.0258)	
training:	Epoch: [47][170/233]	Loss 0.0042 (0.0257)	
training:	Epoch: [47][171/233]	Loss 0.0734 (0.0259)	
training:	Epoch: [47][172/233]	Loss 0.0108 (0.0258)	
training:	Epoch: [47][173/233]	Loss 0.0124 (0.0258)	
training:	Epoch: [47][174/233]	Loss 0.0970 (0.0262)	
training:	Epoch: [47][175/233]	Loss 0.0061 (0.0261)	
training:	Epoch: [47][176/233]	Loss 0.0043 (0.0259)	
training:	Epoch: [47][177/233]	Loss 0.0905 (0.0263)	
training:	Epoch: [47][178/233]	Loss 0.0203 (0.0263)	
training:	Epoch: [47][179/233]	Loss 0.0042 (0.0261)	
training:	Epoch: [47][180/233]	Loss 0.0082 (0.0260)	
training:	Epoch: [47][181/233]	Loss 0.0042 (0.0259)	
training:	Epoch: [47][182/233]	Loss 0.0052 (0.0258)	
training:	Epoch: [47][183/233]	Loss 0.1127 (0.0263)	
training:	Epoch: [47][184/233]	Loss 0.0136 (0.0262)	
training:	Epoch: [47][185/233]	Loss 0.1871 (0.0271)	
training:	Epoch: [47][186/233]	Loss 0.0080 (0.0270)	
training:	Epoch: [47][187/233]	Loss 0.0267 (0.0270)	
training:	Epoch: [47][188/233]	Loss 0.0542 (0.0271)	
training:	Epoch: [47][189/233]	Loss 0.0043 (0.0270)	
training:	Epoch: [47][190/233]	Loss 0.0041 (0.0269)	
training:	Epoch: [47][191/233]	Loss 0.1810 (0.0277)	
training:	Epoch: [47][192/233]	Loss 0.1695 (0.0284)	
training:	Epoch: [47][193/233]	Loss 0.0280 (0.0284)	
training:	Epoch: [47][194/233]	Loss 0.0059 (0.0283)	
training:	Epoch: [47][195/233]	Loss 0.0139 (0.0282)	
training:	Epoch: [47][196/233]	Loss 0.0051 (0.0281)	
training:	Epoch: [47][197/233]	Loss 0.0850 (0.0284)	
training:	Epoch: [47][198/233]	Loss 0.0648 (0.0286)	
training:	Epoch: [47][199/233]	Loss 0.0044 (0.0285)	
training:	Epoch: [47][200/233]	Loss 0.0042 (0.0284)	
training:	Epoch: [47][201/233]	Loss 0.1821 (0.0291)	
training:	Epoch: [47][202/233]	Loss 0.0055 (0.0290)	
training:	Epoch: [47][203/233]	Loss 0.0198 (0.0290)	
training:	Epoch: [47][204/233]	Loss 0.1643 (0.0296)	
training:	Epoch: [47][205/233]	Loss 0.0041 (0.0295)	
training:	Epoch: [47][206/233]	Loss 0.0393 (0.0295)	
training:	Epoch: [47][207/233]	Loss 0.0999 (0.0299)	
training:	Epoch: [47][208/233]	Loss 0.0072 (0.0298)	
training:	Epoch: [47][209/233]	Loss 0.0145 (0.0297)	
training:	Epoch: [47][210/233]	Loss 0.0042 (0.0296)	
training:	Epoch: [47][211/233]	Loss 0.0064 (0.0295)	
training:	Epoch: [47][212/233]	Loss 0.0038 (0.0293)	
training:	Epoch: [47][213/233]	Loss 0.0298 (0.0294)	
training:	Epoch: [47][214/233]	Loss 0.0049 (0.0292)	
training:	Epoch: [47][215/233]	Loss 0.0054 (0.0291)	
training:	Epoch: [47][216/233]	Loss 0.0043 (0.0290)	
training:	Epoch: [47][217/233]	Loss 0.0083 (0.0289)	
training:	Epoch: [47][218/233]	Loss 0.0148 (0.0289)	
training:	Epoch: [47][219/233]	Loss 0.0058 (0.0287)	
training:	Epoch: [47][220/233]	Loss 0.0055 (0.0286)	
training:	Epoch: [47][221/233]	Loss 0.0056 (0.0285)	
training:	Epoch: [47][222/233]	Loss 0.0380 (0.0286)	
training:	Epoch: [47][223/233]	Loss 0.0049 (0.0285)	
training:	Epoch: [47][224/233]	Loss 0.0048 (0.0284)	
training:	Epoch: [47][225/233]	Loss 0.1946 (0.0291)	
training:	Epoch: [47][226/233]	Loss 0.0415 (0.0292)	
training:	Epoch: [47][227/233]	Loss 0.0070 (0.0291)	
training:	Epoch: [47][228/233]	Loss 0.0058 (0.0290)	
training:	Epoch: [47][229/233]	Loss 0.0704 (0.0291)	
training:	Epoch: [47][230/233]	Loss 0.1707 (0.0298)	
training:	Epoch: [47][231/233]	Loss 0.0380 (0.0298)	
training:	Epoch: [47][232/233]	Loss 0.0047 (0.0297)	
training:	Epoch: [47][233/233]	Loss 0.0054 (0.0296)	
Training:	 Loss: 0.0295

Training:	 ACC: 0.9979 0.9979 0.9979 0.9978
Validation:	 ACC: 0.7864 0.7876 0.8121 0.7607
Validation:	 Best_BACC: 0.8162 0.8154 0.7998 0.8326
Validation:	 Loss: 0.8669
Pretraining:	Epoch 48/200
----------
training:	Epoch: [48][1/233]	Loss 0.0043 (0.0043)	
training:	Epoch: [48][2/233]	Loss 0.0056 (0.0050)	
training:	Epoch: [48][3/233]	Loss 0.0222 (0.0107)	
training:	Epoch: [48][4/233]	Loss 0.0189 (0.0128)	
training:	Epoch: [48][5/233]	Loss 0.0108 (0.0124)	
training:	Epoch: [48][6/233]	Loss 0.0053 (0.0112)	
training:	Epoch: [48][7/233]	Loss 0.0052 (0.0104)	
training:	Epoch: [48][8/233]	Loss 0.0063 (0.0098)	
training:	Epoch: [48][9/233]	Loss 0.0049 (0.0093)	
training:	Epoch: [48][10/233]	Loss 0.0063 (0.0090)	
training:	Epoch: [48][11/233]	Loss 0.0211 (0.0101)	
training:	Epoch: [48][12/233]	Loss 0.0044 (0.0096)	
training:	Epoch: [48][13/233]	Loss 0.0466 (0.0125)	
training:	Epoch: [48][14/233]	Loss 0.0044 (0.0119)	
training:	Epoch: [48][15/233]	Loss 0.0047 (0.0114)	
training:	Epoch: [48][16/233]	Loss 0.0052 (0.0110)	
training:	Epoch: [48][17/233]	Loss 0.0038 (0.0106)	
training:	Epoch: [48][18/233]	Loss 0.0899 (0.0150)	
training:	Epoch: [48][19/233]	Loss 0.0039 (0.0144)	
training:	Epoch: [48][20/233]	Loss 0.0106 (0.0142)	
training:	Epoch: [48][21/233]	Loss 0.0061 (0.0138)	
training:	Epoch: [48][22/233]	Loss 0.0041 (0.0134)	
training:	Epoch: [48][23/233]	Loss 0.0052 (0.0130)	
training:	Epoch: [48][24/233]	Loss 0.0058 (0.0127)	
training:	Epoch: [48][25/233]	Loss 0.0111 (0.0127)	
training:	Epoch: [48][26/233]	Loss 0.1603 (0.0184)	
training:	Epoch: [48][27/233]	Loss 0.0050 (0.0179)	
training:	Epoch: [48][28/233]	Loss 0.1576 (0.0228)	
training:	Epoch: [48][29/233]	Loss 0.0043 (0.0222)	
training:	Epoch: [48][30/233]	Loss 0.0119 (0.0219)	
training:	Epoch: [48][31/233]	Loss 0.0051 (0.0213)	
training:	Epoch: [48][32/233]	Loss 0.0950 (0.0236)	
training:	Epoch: [48][33/233]	Loss 0.0070 (0.0231)	
training:	Epoch: [48][34/233]	Loss 0.0110 (0.0228)	
training:	Epoch: [48][35/233]	Loss 0.0073 (0.0223)	
training:	Epoch: [48][36/233]	Loss 0.0110 (0.0220)	
training:	Epoch: [48][37/233]	Loss 0.0044 (0.0215)	
training:	Epoch: [48][38/233]	Loss 0.0327 (0.0218)	
training:	Epoch: [48][39/233]	Loss 0.0648 (0.0229)	
training:	Epoch: [48][40/233]	Loss 0.0081 (0.0226)	
training:	Epoch: [48][41/233]	Loss 0.0073 (0.0222)	
training:	Epoch: [48][42/233]	Loss 0.0068 (0.0218)	
training:	Epoch: [48][43/233]	Loss 0.0041 (0.0214)	
training:	Epoch: [48][44/233]	Loss 0.2581 (0.0268)	
training:	Epoch: [48][45/233]	Loss 0.0061 (0.0263)	
training:	Epoch: [48][46/233]	Loss 0.0045 (0.0259)	
training:	Epoch: [48][47/233]	Loss 0.0056 (0.0254)	
training:	Epoch: [48][48/233]	Loss 0.0061 (0.0250)	
training:	Epoch: [48][49/233]	Loss 0.0735 (0.0260)	
training:	Epoch: [48][50/233]	Loss 0.0667 (0.0268)	
training:	Epoch: [48][51/233]	Loss 0.0045 (0.0264)	
training:	Epoch: [48][52/233]	Loss 0.0118 (0.0261)	
training:	Epoch: [48][53/233]	Loss 0.0039 (0.0257)	
training:	Epoch: [48][54/233]	Loss 0.0046 (0.0253)	
training:	Epoch: [48][55/233]	Loss 0.0081 (0.0250)	
training:	Epoch: [48][56/233]	Loss 0.0042 (0.0246)	
training:	Epoch: [48][57/233]	Loss 0.0055 (0.0243)	
training:	Epoch: [48][58/233]	Loss 0.0138 (0.0241)	
training:	Epoch: [48][59/233]	Loss 0.0666 (0.0248)	
training:	Epoch: [48][60/233]	Loss 0.0046 (0.0245)	
training:	Epoch: [48][61/233]	Loss 0.0079 (0.0242)	
training:	Epoch: [48][62/233]	Loss 0.0043 (0.0239)	
training:	Epoch: [48][63/233]	Loss 0.0333 (0.0240)	
training:	Epoch: [48][64/233]	Loss 0.0064 (0.0238)	
training:	Epoch: [48][65/233]	Loss 0.0041 (0.0235)	
training:	Epoch: [48][66/233]	Loss 0.0118 (0.0233)	
training:	Epoch: [48][67/233]	Loss 0.0053 (0.0230)	
training:	Epoch: [48][68/233]	Loss 0.0061 (0.0228)	
training:	Epoch: [48][69/233]	Loss 0.0057 (0.0225)	
training:	Epoch: [48][70/233]	Loss 0.0216 (0.0225)	
training:	Epoch: [48][71/233]	Loss 0.0412 (0.0228)	
training:	Epoch: [48][72/233]	Loss 0.0306 (0.0229)	
training:	Epoch: [48][73/233]	Loss 0.0048 (0.0226)	
training:	Epoch: [48][74/233]	Loss 0.0061 (0.0224)	
training:	Epoch: [48][75/233]	Loss 0.0159 (0.0223)	
training:	Epoch: [48][76/233]	Loss 0.0058 (0.0221)	
training:	Epoch: [48][77/233]	Loss 0.0441 (0.0224)	
training:	Epoch: [48][78/233]	Loss 0.0347 (0.0225)	
training:	Epoch: [48][79/233]	Loss 0.0062 (0.0223)	
training:	Epoch: [48][80/233]	Loss 0.0040 (0.0221)	
training:	Epoch: [48][81/233]	Loss 0.0056 (0.0219)	
training:	Epoch: [48][82/233]	Loss 0.0082 (0.0217)	
training:	Epoch: [48][83/233]	Loss 0.0044 (0.0215)	
training:	Epoch: [48][84/233]	Loss 0.0066 (0.0214)	
training:	Epoch: [48][85/233]	Loss 0.0047 (0.0212)	
training:	Epoch: [48][86/233]	Loss 0.0377 (0.0213)	
training:	Epoch: [48][87/233]	Loss 0.0047 (0.0212)	
training:	Epoch: [48][88/233]	Loss 0.0085 (0.0210)	
training:	Epoch: [48][89/233]	Loss 0.0074 (0.0209)	
training:	Epoch: [48][90/233]	Loss 0.0038 (0.0207)	
training:	Epoch: [48][91/233]	Loss 0.0041 (0.0205)	
training:	Epoch: [48][92/233]	Loss 0.0044 (0.0203)	
training:	Epoch: [48][93/233]	Loss 0.0045 (0.0201)	
training:	Epoch: [48][94/233]	Loss 0.0041 (0.0200)	
training:	Epoch: [48][95/233]	Loss 0.1887 (0.0217)	
training:	Epoch: [48][96/233]	Loss 0.0113 (0.0216)	
training:	Epoch: [48][97/233]	Loss 0.0050 (0.0215)	
training:	Epoch: [48][98/233]	Loss 0.0047 (0.0213)	
training:	Epoch: [48][99/233]	Loss 0.0084 (0.0212)	
training:	Epoch: [48][100/233]	Loss 0.0049 (0.0210)	
training:	Epoch: [48][101/233]	Loss 0.0034 (0.0208)	
training:	Epoch: [48][102/233]	Loss 0.1552 (0.0221)	
training:	Epoch: [48][103/233]	Loss 0.0762 (0.0227)	
training:	Epoch: [48][104/233]	Loss 0.2819 (0.0252)	
training:	Epoch: [48][105/233]	Loss 0.0100 (0.0250)	
training:	Epoch: [48][106/233]	Loss 0.0078 (0.0249)	
training:	Epoch: [48][107/233]	Loss 0.0067 (0.0247)	
training:	Epoch: [48][108/233]	Loss 0.0043 (0.0245)	
training:	Epoch: [48][109/233]	Loss 0.0074 (0.0243)	
training:	Epoch: [48][110/233]	Loss 0.0045 (0.0242)	
training:	Epoch: [48][111/233]	Loss 0.1052 (0.0249)	
training:	Epoch: [48][112/233]	Loss 0.0106 (0.0248)	
training:	Epoch: [48][113/233]	Loss 0.0057 (0.0246)	
training:	Epoch: [48][114/233]	Loss 0.0061 (0.0244)	
training:	Epoch: [48][115/233]	Loss 0.1854 (0.0258)	
training:	Epoch: [48][116/233]	Loss 0.0072 (0.0257)	
training:	Epoch: [48][117/233]	Loss 0.0071 (0.0255)	
training:	Epoch: [48][118/233]	Loss 0.0080 (0.0254)	
training:	Epoch: [48][119/233]	Loss 0.1657 (0.0265)	
training:	Epoch: [48][120/233]	Loss 0.0961 (0.0271)	
training:	Epoch: [48][121/233]	Loss 0.1330 (0.0280)	
training:	Epoch: [48][122/233]	Loss 0.0146 (0.0279)	
training:	Epoch: [48][123/233]	Loss 0.1681 (0.0290)	
training:	Epoch: [48][124/233]	Loss 0.0080 (0.0289)	
training:	Epoch: [48][125/233]	Loss 0.0056 (0.0287)	
training:	Epoch: [48][126/233]	Loss 0.0043 (0.0285)	
training:	Epoch: [48][127/233]	Loss 0.0058 (0.0283)	
training:	Epoch: [48][128/233]	Loss 0.1806 (0.0295)	
training:	Epoch: [48][129/233]	Loss 0.0056 (0.0293)	
training:	Epoch: [48][130/233]	Loss 0.1033 (0.0299)	
training:	Epoch: [48][131/233]	Loss 0.0065 (0.0297)	
training:	Epoch: [48][132/233]	Loss 0.0042 (0.0295)	
training:	Epoch: [48][133/233]	Loss 0.1306 (0.0303)	
training:	Epoch: [48][134/233]	Loss 0.0069 (0.0301)	
training:	Epoch: [48][135/233]	Loss 0.1777 (0.0312)	
training:	Epoch: [48][136/233]	Loss 0.0050 (0.0310)	
training:	Epoch: [48][137/233]	Loss 0.0048 (0.0308)	
training:	Epoch: [48][138/233]	Loss 0.0041 (0.0306)	
training:	Epoch: [48][139/233]	Loss 0.0309 (0.0306)	
training:	Epoch: [48][140/233]	Loss 0.0065 (0.0304)	
training:	Epoch: [48][141/233]	Loss 0.0630 (0.0307)	
training:	Epoch: [48][142/233]	Loss 0.1825 (0.0317)	
training:	Epoch: [48][143/233]	Loss 0.0058 (0.0316)	
training:	Epoch: [48][144/233]	Loss 0.0046 (0.0314)	
training:	Epoch: [48][145/233]	Loss 0.0078 (0.0312)	
training:	Epoch: [48][146/233]	Loss 0.0075 (0.0310)	
training:	Epoch: [48][147/233]	Loss 0.0662 (0.0313)	
training:	Epoch: [48][148/233]	Loss 0.0892 (0.0317)	
training:	Epoch: [48][149/233]	Loss 0.0048 (0.0315)	
training:	Epoch: [48][150/233]	Loss 0.0159 (0.0314)	
training:	Epoch: [48][151/233]	Loss 0.0069 (0.0312)	
training:	Epoch: [48][152/233]	Loss 0.0041 (0.0310)	
training:	Epoch: [48][153/233]	Loss 0.0121 (0.0309)	
training:	Epoch: [48][154/233]	Loss 0.0154 (0.0308)	
training:	Epoch: [48][155/233]	Loss 0.0068 (0.0307)	
training:	Epoch: [48][156/233]	Loss 0.0264 (0.0306)	
training:	Epoch: [48][157/233]	Loss 0.0061 (0.0305)	
training:	Epoch: [48][158/233]	Loss 0.0107 (0.0304)	
training:	Epoch: [48][159/233]	Loss 0.1674 (0.0312)	
training:	Epoch: [48][160/233]	Loss 0.0045 (0.0311)	
training:	Epoch: [48][161/233]	Loss 0.0069 (0.0309)	
training:	Epoch: [48][162/233]	Loss 0.0045 (0.0307)	
training:	Epoch: [48][163/233]	Loss 0.0043 (0.0306)	
training:	Epoch: [48][164/233]	Loss 0.0195 (0.0305)	
training:	Epoch: [48][165/233]	Loss 0.0351 (0.0305)	
training:	Epoch: [48][166/233]	Loss 0.0034 (0.0304)	
training:	Epoch: [48][167/233]	Loss 0.0059 (0.0302)	
training:	Epoch: [48][168/233]	Loss 0.0175 (0.0302)	
training:	Epoch: [48][169/233]	Loss 0.0056 (0.0300)	
training:	Epoch: [48][170/233]	Loss 0.0068 (0.0299)	
training:	Epoch: [48][171/233]	Loss 0.0052 (0.0297)	
training:	Epoch: [48][172/233]	Loss 0.0081 (0.0296)	
training:	Epoch: [48][173/233]	Loss 0.0067 (0.0295)	
training:	Epoch: [48][174/233]	Loss 0.0101 (0.0294)	
training:	Epoch: [48][175/233]	Loss 0.0043 (0.0292)	
training:	Epoch: [48][176/233]	Loss 0.0112 (0.0291)	
training:	Epoch: [48][177/233]	Loss 0.0041 (0.0290)	
training:	Epoch: [48][178/233]	Loss 0.0050 (0.0288)	
training:	Epoch: [48][179/233]	Loss 0.0102 (0.0287)	
training:	Epoch: [48][180/233]	Loss 0.0478 (0.0288)	
training:	Epoch: [48][181/233]	Loss 0.1466 (0.0295)	
training:	Epoch: [48][182/233]	Loss 0.0141 (0.0294)	
training:	Epoch: [48][183/233]	Loss 0.0045 (0.0293)	
training:	Epoch: [48][184/233]	Loss 0.0038 (0.0291)	
training:	Epoch: [48][185/233]	Loss 0.0227 (0.0291)	
training:	Epoch: [48][186/233]	Loss 0.0652 (0.0293)	
training:	Epoch: [48][187/233]	Loss 0.0056 (0.0292)	
training:	Epoch: [48][188/233]	Loss 0.0046 (0.0290)	
training:	Epoch: [48][189/233]	Loss 0.0093 (0.0289)	
training:	Epoch: [48][190/233]	Loss 0.0041 (0.0288)	
training:	Epoch: [48][191/233]	Loss 0.0065 (0.0287)	
training:	Epoch: [48][192/233]	Loss 0.0076 (0.0286)	
training:	Epoch: [48][193/233]	Loss 0.1837 (0.0294)	
training:	Epoch: [48][194/233]	Loss 0.0143 (0.0293)	
training:	Epoch: [48][195/233]	Loss 0.2050 (0.0302)	
training:	Epoch: [48][196/233]	Loss 0.0057 (0.0301)	
training:	Epoch: [48][197/233]	Loss 0.0059 (0.0299)	
training:	Epoch: [48][198/233]	Loss 0.0151 (0.0299)	
training:	Epoch: [48][199/233]	Loss 0.0080 (0.0298)	
training:	Epoch: [48][200/233]	Loss 0.0146 (0.0297)	
training:	Epoch: [48][201/233]	Loss 0.1848 (0.0305)	
training:	Epoch: [48][202/233]	Loss 0.0042 (0.0303)	
training:	Epoch: [48][203/233]	Loss 0.0088 (0.0302)	
training:	Epoch: [48][204/233]	Loss 0.0058 (0.0301)	
training:	Epoch: [48][205/233]	Loss 0.0122 (0.0300)	
training:	Epoch: [48][206/233]	Loss 0.0145 (0.0299)	
training:	Epoch: [48][207/233]	Loss 0.0109 (0.0298)	
training:	Epoch: [48][208/233]	Loss 0.1741 (0.0305)	
training:	Epoch: [48][209/233]	Loss 0.0416 (0.0306)	
training:	Epoch: [48][210/233]	Loss 0.0069 (0.0305)	
training:	Epoch: [48][211/233]	Loss 0.0080 (0.0304)	
training:	Epoch: [48][212/233]	Loss 0.0050 (0.0303)	
training:	Epoch: [48][213/233]	Loss 0.0061 (0.0301)	
training:	Epoch: [48][214/233]	Loss 0.0226 (0.0301)	
training:	Epoch: [48][215/233]	Loss 0.0056 (0.0300)	
training:	Epoch: [48][216/233]	Loss 0.1188 (0.0304)	
training:	Epoch: [48][217/233]	Loss 0.0171 (0.0303)	
training:	Epoch: [48][218/233]	Loss 0.0049 (0.0302)	
training:	Epoch: [48][219/233]	Loss 0.0045 (0.0301)	
training:	Epoch: [48][220/233]	Loss 0.0050 (0.0300)	
training:	Epoch: [48][221/233]	Loss 0.0048 (0.0299)	
training:	Epoch: [48][222/233]	Loss 0.0048 (0.0298)	
training:	Epoch: [48][223/233]	Loss 0.0261 (0.0298)	
training:	Epoch: [48][224/233]	Loss 0.1984 (0.0305)	
training:	Epoch: [48][225/233]	Loss 0.0045 (0.0304)	
training:	Epoch: [48][226/233]	Loss 0.0056 (0.0303)	
training:	Epoch: [48][227/233]	Loss 0.0048 (0.0302)	
training:	Epoch: [48][228/233]	Loss 0.0051 (0.0301)	
training:	Epoch: [48][229/233]	Loss 0.0251 (0.0300)	
training:	Epoch: [48][230/233]	Loss 0.0063 (0.0299)	
training:	Epoch: [48][231/233]	Loss 0.0033 (0.0298)	
training:	Epoch: [48][232/233]	Loss 0.1851 (0.0305)	
training:	Epoch: [48][233/233]	Loss 0.0040 (0.0304)	
Training:	 Loss: 0.0303

Training:	 ACC: 0.9981 0.9981 0.9979 0.9983
Validation:	 ACC: 0.7853 0.7849 0.7763 0.7944
Validation:	 Best_BACC: 0.8162 0.8154 0.7998 0.8326
Validation:	 Loss: 0.8897
Pretraining:	Epoch 49/200
----------
training:	Epoch: [49][1/233]	Loss 0.0062 (0.0062)	
training:	Epoch: [49][2/233]	Loss 0.0043 (0.0052)	
training:	Epoch: [49][3/233]	Loss 0.0049 (0.0051)	
training:	Epoch: [49][4/233]	Loss 0.0069 (0.0056)	
training:	Epoch: [49][5/233]	Loss 0.0054 (0.0055)	
training:	Epoch: [49][6/233]	Loss 0.0051 (0.0055)	
training:	Epoch: [49][7/233]	Loss 0.0044 (0.0053)	
training:	Epoch: [49][8/233]	Loss 0.0050 (0.0053)	
training:	Epoch: [49][9/233]	Loss 0.0054 (0.0053)	
training:	Epoch: [49][10/233]	Loss 0.1368 (0.0184)	
training:	Epoch: [49][11/233]	Loss 0.0489 (0.0212)	
training:	Epoch: [49][12/233]	Loss 0.0685 (0.0251)	
training:	Epoch: [49][13/233]	Loss 0.0186 (0.0246)	
training:	Epoch: [49][14/233]	Loss 0.0194 (0.0243)	
training:	Epoch: [49][15/233]	Loss 0.0037 (0.0229)	
training:	Epoch: [49][16/233]	Loss 0.0103 (0.0221)	
training:	Epoch: [49][17/233]	Loss 0.0046 (0.0211)	
training:	Epoch: [49][18/233]	Loss 0.0039 (0.0201)	
training:	Epoch: [49][19/233]	Loss 0.0212 (0.0202)	
training:	Epoch: [49][20/233]	Loss 0.0056 (0.0194)	
training:	Epoch: [49][21/233]	Loss 0.0689 (0.0218)	
training:	Epoch: [49][22/233]	Loss 0.0201 (0.0217)	
training:	Epoch: [49][23/233]	Loss 0.0116 (0.0213)	
training:	Epoch: [49][24/233]	Loss 0.0454 (0.0223)	
training:	Epoch: [49][25/233]	Loss 0.0040 (0.0216)	
training:	Epoch: [49][26/233]	Loss 0.0135 (0.0213)	
training:	Epoch: [49][27/233]	Loss 0.0071 (0.0207)	
training:	Epoch: [49][28/233]	Loss 0.0085 (0.0203)	
training:	Epoch: [49][29/233]	Loss 0.0337 (0.0208)	
training:	Epoch: [49][30/233]	Loss 0.0058 (0.0203)	
training:	Epoch: [49][31/233]	Loss 0.0045 (0.0198)	
training:	Epoch: [49][32/233]	Loss 0.0099 (0.0194)	
training:	Epoch: [49][33/233]	Loss 0.0061 (0.0190)	
training:	Epoch: [49][34/233]	Loss 0.0048 (0.0186)	
training:	Epoch: [49][35/233]	Loss 0.0041 (0.0182)	
training:	Epoch: [49][36/233]	Loss 0.1785 (0.0227)	
training:	Epoch: [49][37/233]	Loss 0.0068 (0.0222)	
training:	Epoch: [49][38/233]	Loss 0.0039 (0.0217)	
training:	Epoch: [49][39/233]	Loss 0.0134 (0.0215)	
training:	Epoch: [49][40/233]	Loss 0.0064 (0.0212)	
training:	Epoch: [49][41/233]	Loss 0.0055 (0.0208)	
training:	Epoch: [49][42/233]	Loss 0.0111 (0.0205)	
training:	Epoch: [49][43/233]	Loss 0.0095 (0.0203)	
training:	Epoch: [49][44/233]	Loss 0.0075 (0.0200)	
training:	Epoch: [49][45/233]	Loss 0.0067 (0.0197)	
training:	Epoch: [49][46/233]	Loss 0.0096 (0.0195)	
training:	Epoch: [49][47/233]	Loss 0.0045 (0.0192)	
training:	Epoch: [49][48/233]	Loss 0.0040 (0.0188)	
training:	Epoch: [49][49/233]	Loss 0.0050 (0.0186)	
training:	Epoch: [49][50/233]	Loss 0.0193 (0.0186)	
training:	Epoch: [49][51/233]	Loss 0.0041 (0.0183)	
training:	Epoch: [49][52/233]	Loss 0.0861 (0.0196)	
training:	Epoch: [49][53/233]	Loss 0.0067 (0.0194)	
training:	Epoch: [49][54/233]	Loss 0.0045 (0.0191)	
training:	Epoch: [49][55/233]	Loss 0.0054 (0.0188)	
training:	Epoch: [49][56/233]	Loss 0.0044 (0.0186)	
training:	Epoch: [49][57/233]	Loss 0.0053 (0.0183)	
training:	Epoch: [49][58/233]	Loss 0.0097 (0.0182)	
training:	Epoch: [49][59/233]	Loss 0.0391 (0.0185)	
training:	Epoch: [49][60/233]	Loss 0.0056 (0.0183)	
training:	Epoch: [49][61/233]	Loss 0.0049 (0.0181)	
training:	Epoch: [49][62/233]	Loss 0.0897 (0.0193)	
training:	Epoch: [49][63/233]	Loss 0.0470 (0.0197)	
training:	Epoch: [49][64/233]	Loss 0.0092 (0.0195)	
training:	Epoch: [49][65/233]	Loss 0.0137 (0.0195)	
training:	Epoch: [49][66/233]	Loss 0.1905 (0.0220)	
training:	Epoch: [49][67/233]	Loss 0.0763 (0.0229)	
training:	Epoch: [49][68/233]	Loss 0.0109 (0.0227)	
training:	Epoch: [49][69/233]	Loss 0.0068 (0.0224)	
training:	Epoch: [49][70/233]	Loss 0.0061 (0.0222)	
training:	Epoch: [49][71/233]	Loss 0.0038 (0.0220)	
training:	Epoch: [49][72/233]	Loss 0.0070 (0.0217)	
training:	Epoch: [49][73/233]	Loss 0.0081 (0.0216)	
training:	Epoch: [49][74/233]	Loss 0.0042 (0.0213)	
training:	Epoch: [49][75/233]	Loss 0.0044 (0.0211)	
training:	Epoch: [49][76/233]	Loss 0.0040 (0.0209)	
training:	Epoch: [49][77/233]	Loss 0.0057 (0.0207)	
training:	Epoch: [49][78/233]	Loss 0.0046 (0.0205)	
training:	Epoch: [49][79/233]	Loss 0.0036 (0.0203)	
training:	Epoch: [49][80/233]	Loss 0.0261 (0.0203)	
training:	Epoch: [49][81/233]	Loss 0.0129 (0.0202)	
training:	Epoch: [49][82/233]	Loss 0.1527 (0.0219)	
training:	Epoch: [49][83/233]	Loss 0.0042 (0.0216)	
training:	Epoch: [49][84/233]	Loss 0.0060 (0.0215)	
training:	Epoch: [49][85/233]	Loss 0.0102 (0.0213)	
training:	Epoch: [49][86/233]	Loss 0.0057 (0.0211)	
training:	Epoch: [49][87/233]	Loss 0.0048 (0.0210)	
training:	Epoch: [49][88/233]	Loss 0.0145 (0.0209)	
training:	Epoch: [49][89/233]	Loss 0.0032 (0.0207)	
training:	Epoch: [49][90/233]	Loss 0.0042 (0.0205)	
training:	Epoch: [49][91/233]	Loss 0.0034 (0.0203)	
training:	Epoch: [49][92/233]	Loss 0.0065 (0.0202)	
training:	Epoch: [49][93/233]	Loss 0.0042 (0.0200)	
training:	Epoch: [49][94/233]	Loss 0.0157 (0.0199)	
training:	Epoch: [49][95/233]	Loss 0.0060 (0.0198)	
training:	Epoch: [49][96/233]	Loss 0.0052 (0.0196)	
training:	Epoch: [49][97/233]	Loss 0.0037 (0.0195)	
training:	Epoch: [49][98/233]	Loss 0.0671 (0.0200)	
training:	Epoch: [49][99/233]	Loss 0.2394 (0.0222)	
training:	Epoch: [49][100/233]	Loss 0.0066 (0.0220)	
training:	Epoch: [49][101/233]	Loss 0.0054 (0.0219)	
training:	Epoch: [49][102/233]	Loss 0.0065 (0.0217)	
training:	Epoch: [49][103/233]	Loss 0.0430 (0.0219)	
training:	Epoch: [49][104/233]	Loss 0.0079 (0.0218)	
training:	Epoch: [49][105/233]	Loss 0.0058 (0.0216)	
training:	Epoch: [49][106/233]	Loss 0.2553 (0.0238)	
training:	Epoch: [49][107/233]	Loss 0.0141 (0.0237)	
training:	Epoch: [49][108/233]	Loss 0.0065 (0.0236)	
training:	Epoch: [49][109/233]	Loss 0.1999 (0.0252)	
training:	Epoch: [49][110/233]	Loss 0.0047 (0.0250)	
training:	Epoch: [49][111/233]	Loss 0.0093 (0.0249)	
training:	Epoch: [49][112/233]	Loss 0.0038 (0.0247)	
training:	Epoch: [49][113/233]	Loss 0.0284 (0.0247)	
training:	Epoch: [49][114/233]	Loss 0.1462 (0.0258)	
training:	Epoch: [49][115/233]	Loss 0.0038 (0.0256)	
training:	Epoch: [49][116/233]	Loss 0.0070 (0.0254)	
training:	Epoch: [49][117/233]	Loss 0.0046 (0.0253)	
training:	Epoch: [49][118/233]	Loss 0.0049 (0.0251)	
training:	Epoch: [49][119/233]	Loss 0.0036 (0.0249)	
training:	Epoch: [49][120/233]	Loss 0.0087 (0.0248)	
training:	Epoch: [49][121/233]	Loss 0.0079 (0.0246)	
training:	Epoch: [49][122/233]	Loss 0.0742 (0.0250)	
training:	Epoch: [49][123/233]	Loss 0.0128 (0.0249)	
training:	Epoch: [49][124/233]	Loss 0.0052 (0.0248)	
training:	Epoch: [49][125/233]	Loss 0.1877 (0.0261)	
training:	Epoch: [49][126/233]	Loss 0.0068 (0.0259)	
training:	Epoch: [49][127/233]	Loss 0.1106 (0.0266)	
training:	Epoch: [49][128/233]	Loss 0.2726 (0.0285)	
training:	Epoch: [49][129/233]	Loss 0.0206 (0.0285)	
training:	Epoch: [49][130/233]	Loss 0.0046 (0.0283)	
training:	Epoch: [49][131/233]	Loss 0.0659 (0.0286)	
training:	Epoch: [49][132/233]	Loss 0.0075 (0.0284)	
training:	Epoch: [49][133/233]	Loss 0.0132 (0.0283)	
training:	Epoch: [49][134/233]	Loss 0.0121 (0.0282)	
training:	Epoch: [49][135/233]	Loss 0.0074 (0.0280)	
training:	Epoch: [49][136/233]	Loss 0.0160 (0.0279)	
training:	Epoch: [49][137/233]	Loss 0.0047 (0.0278)	
training:	Epoch: [49][138/233]	Loss 0.0196 (0.0277)	
training:	Epoch: [49][139/233]	Loss 0.0100 (0.0276)	
training:	Epoch: [49][140/233]	Loss 0.0048 (0.0274)	
training:	Epoch: [49][141/233]	Loss 0.0145 (0.0273)	
training:	Epoch: [49][142/233]	Loss 0.0139 (0.0272)	
training:	Epoch: [49][143/233]	Loss 0.0090 (0.0271)	
training:	Epoch: [49][144/233]	Loss 0.1223 (0.0277)	
training:	Epoch: [49][145/233]	Loss 0.0398 (0.0278)	
training:	Epoch: [49][146/233]	Loss 0.0040 (0.0277)	
training:	Epoch: [49][147/233]	Loss 0.0040 (0.0275)	
training:	Epoch: [49][148/233]	Loss 0.0211 (0.0275)	
training:	Epoch: [49][149/233]	Loss 0.1248 (0.0281)	
training:	Epoch: [49][150/233]	Loss 0.0042 (0.0280)	
training:	Epoch: [49][151/233]	Loss 0.0063 (0.0278)	
training:	Epoch: [49][152/233]	Loss 0.0034 (0.0277)	
training:	Epoch: [49][153/233]	Loss 0.0265 (0.0276)	
training:	Epoch: [49][154/233]	Loss 0.0062 (0.0275)	
training:	Epoch: [49][155/233]	Loss 0.1335 (0.0282)	
training:	Epoch: [49][156/233]	Loss 0.1577 (0.0290)	
training:	Epoch: [49][157/233]	Loss 0.0070 (0.0289)	
training:	Epoch: [49][158/233]	Loss 0.0072 (0.0287)	
training:	Epoch: [49][159/233]	Loss 0.0039 (0.0286)	
training:	Epoch: [49][160/233]	Loss 0.0046 (0.0284)	
training:	Epoch: [49][161/233]	Loss 0.1443 (0.0292)	
training:	Epoch: [49][162/233]	Loss 0.0044 (0.0290)	
training:	Epoch: [49][163/233]	Loss 0.0454 (0.0291)	
training:	Epoch: [49][164/233]	Loss 0.0780 (0.0294)	
training:	Epoch: [49][165/233]	Loss 0.0049 (0.0293)	
training:	Epoch: [49][166/233]	Loss 0.0378 (0.0293)	
training:	Epoch: [49][167/233]	Loss 0.0042 (0.0292)	
training:	Epoch: [49][168/233]	Loss 0.0411 (0.0292)	
training:	Epoch: [49][169/233]	Loss 0.0048 (0.0291)	
training:	Epoch: [49][170/233]	Loss 0.0447 (0.0292)	
training:	Epoch: [49][171/233]	Loss 0.0087 (0.0291)	
training:	Epoch: [49][172/233]	Loss 0.0095 (0.0289)	
training:	Epoch: [49][173/233]	Loss 0.1816 (0.0298)	
training:	Epoch: [49][174/233]	Loss 0.1378 (0.0304)	
training:	Epoch: [49][175/233]	Loss 0.0043 (0.0303)	
training:	Epoch: [49][176/233]	Loss 0.1826 (0.0312)	
training:	Epoch: [49][177/233]	Loss 0.1738 (0.0320)	
training:	Epoch: [49][178/233]	Loss 0.0050 (0.0318)	
training:	Epoch: [49][179/233]	Loss 0.0048 (0.0317)	
training:	Epoch: [49][180/233]	Loss 0.0046 (0.0315)	
training:	Epoch: [49][181/233]	Loss 0.0069 (0.0314)	
training:	Epoch: [49][182/233]	Loss 0.1096 (0.0318)	
training:	Epoch: [49][183/233]	Loss 0.0059 (0.0317)	
training:	Epoch: [49][184/233]	Loss 0.0045 (0.0315)	
training:	Epoch: [49][185/233]	Loss 0.0123 (0.0314)	
training:	Epoch: [49][186/233]	Loss 0.0094 (0.0313)	
training:	Epoch: [49][187/233]	Loss 0.0051 (0.0312)	
training:	Epoch: [49][188/233]	Loss 0.0072 (0.0310)	
training:	Epoch: [49][189/233]	Loss 0.1558 (0.0317)	
training:	Epoch: [49][190/233]	Loss 0.0096 (0.0316)	
training:	Epoch: [49][191/233]	Loss 0.0047 (0.0314)	
training:	Epoch: [49][192/233]	Loss 0.0053 (0.0313)	
training:	Epoch: [49][193/233]	Loss 0.0064 (0.0312)	
training:	Epoch: [49][194/233]	Loss 0.0042 (0.0310)	
training:	Epoch: [49][195/233]	Loss 0.0046 (0.0309)	
training:	Epoch: [49][196/233]	Loss 0.0042 (0.0308)	
training:	Epoch: [49][197/233]	Loss 0.0065 (0.0306)	
training:	Epoch: [49][198/233]	Loss 0.1863 (0.0314)	
training:	Epoch: [49][199/233]	Loss 0.0165 (0.0313)	
training:	Epoch: [49][200/233]	Loss 0.0097 (0.0312)	
training:	Epoch: [49][201/233]	Loss 0.0399 (0.0313)	
training:	Epoch: [49][202/233]	Loss 0.0502 (0.0314)	
training:	Epoch: [49][203/233]	Loss 0.1402 (0.0319)	
training:	Epoch: [49][204/233]	Loss 0.0047 (0.0318)	
training:	Epoch: [49][205/233]	Loss 0.0044 (0.0316)	
training:	Epoch: [49][206/233]	Loss 0.0045 (0.0315)	
training:	Epoch: [49][207/233]	Loss 0.0167 (0.0314)	
training:	Epoch: [49][208/233]	Loss 0.1315 (0.0319)	
training:	Epoch: [49][209/233]	Loss 0.0106 (0.0318)	
training:	Epoch: [49][210/233]	Loss 0.0089 (0.0317)	
training:	Epoch: [49][211/233]	Loss 0.0047 (0.0316)	
training:	Epoch: [49][212/233]	Loss 0.0069 (0.0315)	
training:	Epoch: [49][213/233]	Loss 0.0071 (0.0313)	
training:	Epoch: [49][214/233]	Loss 0.0038 (0.0312)	
training:	Epoch: [49][215/233]	Loss 0.0053 (0.0311)	
training:	Epoch: [49][216/233]	Loss 0.0045 (0.0310)	
training:	Epoch: [49][217/233]	Loss 0.0049 (0.0309)	
training:	Epoch: [49][218/233]	Loss 0.0120 (0.0308)	
training:	Epoch: [49][219/233]	Loss 0.0048 (0.0307)	
training:	Epoch: [49][220/233]	Loss 0.0048 (0.0305)	
training:	Epoch: [49][221/233]	Loss 0.0427 (0.0306)	
training:	Epoch: [49][222/233]	Loss 0.0064 (0.0305)	
training:	Epoch: [49][223/233]	Loss 0.0127 (0.0304)	
training:	Epoch: [49][224/233]	Loss 0.0077 (0.0303)	
training:	Epoch: [49][225/233]	Loss 0.0779 (0.0305)	
training:	Epoch: [49][226/233]	Loss 0.0057 (0.0304)	
training:	Epoch: [49][227/233]	Loss 0.0039 (0.0303)	
training:	Epoch: [49][228/233]	Loss 0.0337 (0.0303)	
training:	Epoch: [49][229/233]	Loss 0.0036 (0.0302)	
training:	Epoch: [49][230/233]	Loss 0.0046 (0.0301)	
training:	Epoch: [49][231/233]	Loss 0.0039 (0.0300)	
training:	Epoch: [49][232/233]	Loss 0.0857 (0.0302)	
training:	Epoch: [49][233/233]	Loss 0.0042 (0.0301)	
Training:	 Loss: 0.0300

Training:	 ACC: 0.9980 0.9980 0.9977 0.9983
Validation:	 ACC: 0.7855 0.7838 0.7508 0.8202
Validation:	 Best_BACC: 0.8162 0.8154 0.7998 0.8326
Validation:	 Loss: 0.9229
Pretraining:	Epoch 50/200
----------
training:	Epoch: [50][1/233]	Loss 0.0069 (0.0069)	
training:	Epoch: [50][2/233]	Loss 0.0050 (0.0059)	
training:	Epoch: [50][3/233]	Loss 0.0064 (0.0061)	
training:	Epoch: [50][4/233]	Loss 0.2024 (0.0551)	
training:	Epoch: [50][5/233]	Loss 0.0091 (0.0459)	
training:	Epoch: [50][6/233]	Loss 0.1256 (0.0592)	
training:	Epoch: [50][7/233]	Loss 0.1010 (0.0652)	
training:	Epoch: [50][8/233]	Loss 0.0050 (0.0577)	
training:	Epoch: [50][9/233]	Loss 0.0040 (0.0517)	
training:	Epoch: [50][10/233]	Loss 0.0134 (0.0479)	
training:	Epoch: [50][11/233]	Loss 0.0042 (0.0439)	
training:	Epoch: [50][12/233]	Loss 0.0098 (0.0411)	
training:	Epoch: [50][13/233]	Loss 0.0042 (0.0382)	
training:	Epoch: [50][14/233]	Loss 0.0069 (0.0360)	
training:	Epoch: [50][15/233]	Loss 0.0044 (0.0339)	
training:	Epoch: [50][16/233]	Loss 0.0044 (0.0320)	
training:	Epoch: [50][17/233]	Loss 0.0131 (0.0309)	
training:	Epoch: [50][18/233]	Loss 0.0662 (0.0329)	
training:	Epoch: [50][19/233]	Loss 0.0049 (0.0314)	
training:	Epoch: [50][20/233]	Loss 0.0054 (0.0301)	
training:	Epoch: [50][21/233]	Loss 0.0112 (0.0292)	
training:	Epoch: [50][22/233]	Loss 0.0948 (0.0322)	
training:	Epoch: [50][23/233]	Loss 0.0033 (0.0309)	
training:	Epoch: [50][24/233]	Loss 0.0045 (0.0298)	
training:	Epoch: [50][25/233]	Loss 0.0041 (0.0288)	
training:	Epoch: [50][26/233]	Loss 0.0169 (0.0283)	
training:	Epoch: [50][27/233]	Loss 0.2290 (0.0358)	
training:	Epoch: [50][28/233]	Loss 0.0044 (0.0347)	
training:	Epoch: [50][29/233]	Loss 0.0344 (0.0347)	
training:	Epoch: [50][30/233]	Loss 0.0044 (0.0336)	
training:	Epoch: [50][31/233]	Loss 0.0539 (0.0343)	
training:	Epoch: [50][32/233]	Loss 0.0051 (0.0334)	
training:	Epoch: [50][33/233]	Loss 0.0083 (0.0326)	
training:	Epoch: [50][34/233]	Loss 0.0040 (0.0318)	
training:	Epoch: [50][35/233]	Loss 0.0040 (0.0310)	
training:	Epoch: [50][36/233]	Loss 0.0060 (0.0303)	
training:	Epoch: [50][37/233]	Loss 0.0053 (0.0296)	
training:	Epoch: [50][38/233]	Loss 0.0042 (0.0290)	
training:	Epoch: [50][39/233]	Loss 0.0078 (0.0284)	
training:	Epoch: [50][40/233]	Loss 0.0050 (0.0278)	
training:	Epoch: [50][41/233]	Loss 0.0353 (0.0280)	
training:	Epoch: [50][42/233]	Loss 0.0043 (0.0274)	
training:	Epoch: [50][43/233]	Loss 0.0126 (0.0271)	
training:	Epoch: [50][44/233]	Loss 0.0053 (0.0266)	
training:	Epoch: [50][45/233]	Loss 0.0044 (0.0261)	
training:	Epoch: [50][46/233]	Loss 0.0071 (0.0257)	
training:	Epoch: [50][47/233]	Loss 0.0054 (0.0253)	
training:	Epoch: [50][48/233]	Loss 0.0041 (0.0248)	
training:	Epoch: [50][49/233]	Loss 0.0043 (0.0244)	
training:	Epoch: [50][50/233]	Loss 0.0055 (0.0240)	
training:	Epoch: [50][51/233]	Loss 0.0134 (0.0238)	
training:	Epoch: [50][52/233]	Loss 0.0230 (0.0238)	
training:	Epoch: [50][53/233]	Loss 0.0192 (0.0237)	
training:	Epoch: [50][54/233]	Loss 0.1842 (0.0267)	
training:	Epoch: [50][55/233]	Loss 0.0036 (0.0263)	
training:	Epoch: [50][56/233]	Loss 0.0042 (0.0259)	
training:	Epoch: [50][57/233]	Loss 0.1872 (0.0287)	
training:	Epoch: [50][58/233]	Loss 0.1879 (0.0314)	
training:	Epoch: [50][59/233]	Loss 0.0065 (0.0310)	
training:	Epoch: [50][60/233]	Loss 0.0085 (0.0306)	
training:	Epoch: [50][61/233]	Loss 0.0048 (0.0302)	
training:	Epoch: [50][62/233]	Loss 0.0105 (0.0299)	
training:	Epoch: [50][63/233]	Loss 0.0044 (0.0295)	
training:	Epoch: [50][64/233]	Loss 0.0077 (0.0292)	
training:	Epoch: [50][65/233]	Loss 0.0051 (0.0288)	
training:	Epoch: [50][66/233]	Loss 0.0042 (0.0284)	
training:	Epoch: [50][67/233]	Loss 0.1615 (0.0304)	
training:	Epoch: [50][68/233]	Loss 0.0435 (0.0306)	
training:	Epoch: [50][69/233]	Loss 0.0113 (0.0303)	
training:	Epoch: [50][70/233]	Loss 0.0225 (0.0302)	
training:	Epoch: [50][71/233]	Loss 0.0070 (0.0299)	
training:	Epoch: [50][72/233]	Loss 0.0042 (0.0295)	
training:	Epoch: [50][73/233]	Loss 0.0042 (0.0292)	
training:	Epoch: [50][74/233]	Loss 0.0038 (0.0288)	
training:	Epoch: [50][75/233]	Loss 0.0140 (0.0286)	
training:	Epoch: [50][76/233]	Loss 0.0034 (0.0283)	
training:	Epoch: [50][77/233]	Loss 0.0034 (0.0280)	
training:	Epoch: [50][78/233]	Loss 0.0909 (0.0288)	
training:	Epoch: [50][79/233]	Loss 0.0056 (0.0285)	
training:	Epoch: [50][80/233]	Loss 0.0040 (0.0282)	
training:	Epoch: [50][81/233]	Loss 0.0038 (0.0279)	
training:	Epoch: [50][82/233]	Loss 0.0048 (0.0276)	
training:	Epoch: [50][83/233]	Loss 0.0039 (0.0273)	
training:	Epoch: [50][84/233]	Loss 0.0262 (0.0273)	
training:	Epoch: [50][85/233]	Loss 0.0815 (0.0279)	
training:	Epoch: [50][86/233]	Loss 0.0121 (0.0278)	
training:	Epoch: [50][87/233]	Loss 0.0038 (0.0275)	
training:	Epoch: [50][88/233]	Loss 0.2045 (0.0295)	
training:	Epoch: [50][89/233]	Loss 0.0150 (0.0293)	
training:	Epoch: [50][90/233]	Loss 0.0040 (0.0291)	
training:	Epoch: [50][91/233]	Loss 0.0062 (0.0288)	
training:	Epoch: [50][92/233]	Loss 0.0139 (0.0286)	
training:	Epoch: [50][93/233]	Loss 0.0046 (0.0284)	
training:	Epoch: [50][94/233]	Loss 0.0041 (0.0281)	
training:	Epoch: [50][95/233]	Loss 0.0037 (0.0279)	
training:	Epoch: [50][96/233]	Loss 0.0059 (0.0276)	
training:	Epoch: [50][97/233]	Loss 0.0043 (0.0274)	
training:	Epoch: [50][98/233]	Loss 0.0060 (0.0272)	
training:	Epoch: [50][99/233]	Loss 0.0043 (0.0269)	
training:	Epoch: [50][100/233]	Loss 0.0037 (0.0267)	
training:	Epoch: [50][101/233]	Loss 0.0048 (0.0265)	
training:	Epoch: [50][102/233]	Loss 0.0417 (0.0266)	
training:	Epoch: [50][103/233]	Loss 0.0184 (0.0266)	
training:	Epoch: [50][104/233]	Loss 0.0041 (0.0264)	
training:	Epoch: [50][105/233]	Loss 0.0047 (0.0261)	
training:	Epoch: [50][106/233]	Loss 0.0064 (0.0260)	
training:	Epoch: [50][107/233]	Loss 0.0045 (0.0258)	
training:	Epoch: [50][108/233]	Loss 0.0102 (0.0256)	
training:	Epoch: [50][109/233]	Loss 0.0052 (0.0254)	
training:	Epoch: [50][110/233]	Loss 0.0038 (0.0252)	
training:	Epoch: [50][111/233]	Loss 0.0044 (0.0250)	
training:	Epoch: [50][112/233]	Loss 0.0056 (0.0249)	
training:	Epoch: [50][113/233]	Loss 0.0163 (0.0248)	
training:	Epoch: [50][114/233]	Loss 0.0042 (0.0246)	
training:	Epoch: [50][115/233]	Loss 0.0100 (0.0245)	
training:	Epoch: [50][116/233]	Loss 0.1823 (0.0258)	
training:	Epoch: [50][117/233]	Loss 0.0049 (0.0257)	
training:	Epoch: [50][118/233]	Loss 0.0042 (0.0255)	
training:	Epoch: [50][119/233]	Loss 0.0043 (0.0253)	
training:	Epoch: [50][120/233]	Loss 0.0312 (0.0254)	
training:	Epoch: [50][121/233]	Loss 0.0037 (0.0252)	
training:	Epoch: [50][122/233]	Loss 0.3020 (0.0274)	
training:	Epoch: [50][123/233]	Loss 0.0072 (0.0273)	
training:	Epoch: [50][124/233]	Loss 0.0064 (0.0271)	
training:	Epoch: [50][125/233]	Loss 0.0042 (0.0269)	
training:	Epoch: [50][126/233]	Loss 0.0035 (0.0267)	
training:	Epoch: [50][127/233]	Loss 0.0119 (0.0266)	
training:	Epoch: [50][128/233]	Loss 0.0036 (0.0264)	
training:	Epoch: [50][129/233]	Loss 0.1859 (0.0277)	
training:	Epoch: [50][130/233]	Loss 0.0041 (0.0275)	
training:	Epoch: [50][131/233]	Loss 0.0042 (0.0273)	
training:	Epoch: [50][132/233]	Loss 0.0064 (0.0272)	
training:	Epoch: [50][133/233]	Loss 0.0051 (0.0270)	
training:	Epoch: [50][134/233]	Loss 0.0047 (0.0268)	
training:	Epoch: [50][135/233]	Loss 0.0041 (0.0267)	
training:	Epoch: [50][136/233]	Loss 0.0075 (0.0265)	
training:	Epoch: [50][137/233]	Loss 0.0066 (0.0264)	
training:	Epoch: [50][138/233]	Loss 0.0041 (0.0262)	
training:	Epoch: [50][139/233]	Loss 0.0057 (0.0261)	
training:	Epoch: [50][140/233]	Loss 0.0062 (0.0259)	
training:	Epoch: [50][141/233]	Loss 0.0051 (0.0258)	
training:	Epoch: [50][142/233]	Loss 0.0044 (0.0256)	
training:	Epoch: [50][143/233]	Loss 0.0146 (0.0256)	
training:	Epoch: [50][144/233]	Loss 0.0039 (0.0254)	
training:	Epoch: [50][145/233]	Loss 0.0036 (0.0253)	
training:	Epoch: [50][146/233]	Loss 0.0732 (0.0256)	
training:	Epoch: [50][147/233]	Loss 0.0146 (0.0255)	
training:	Epoch: [50][148/233]	Loss 0.0166 (0.0254)	
training:	Epoch: [50][149/233]	Loss 0.0041 (0.0253)	
training:	Epoch: [50][150/233]	Loss 0.0062 (0.0252)	
training:	Epoch: [50][151/233]	Loss 0.0042 (0.0250)	
training:	Epoch: [50][152/233]	Loss 0.0046 (0.0249)	
training:	Epoch: [50][153/233]	Loss 0.0097 (0.0248)	
training:	Epoch: [50][154/233]	Loss 0.0045 (0.0247)	
training:	Epoch: [50][155/233]	Loss 0.0041 (0.0245)	
training:	Epoch: [50][156/233]	Loss 0.0052 (0.0244)	
training:	Epoch: [50][157/233]	Loss 0.1179 (0.0250)	
training:	Epoch: [50][158/233]	Loss 0.0041 (0.0249)	
training:	Epoch: [50][159/233]	Loss 0.0364 (0.0249)	
training:	Epoch: [50][160/233]	Loss 0.0039 (0.0248)	
training:	Epoch: [50][161/233]	Loss 0.0207 (0.0248)	
training:	Epoch: [50][162/233]	Loss 0.0043 (0.0247)	
training:	Epoch: [50][163/233]	Loss 0.0035 (0.0245)	
training:	Epoch: [50][164/233]	Loss 0.0057 (0.0244)	
training:	Epoch: [50][165/233]	Loss 0.0212 (0.0244)	
training:	Epoch: [50][166/233]	Loss 0.0049 (0.0243)	
training:	Epoch: [50][167/233]	Loss 0.0041 (0.0242)	
training:	Epoch: [50][168/233]	Loss 0.0560 (0.0244)	
training:	Epoch: [50][169/233]	Loss 0.0124 (0.0243)	
training:	Epoch: [50][170/233]	Loss 0.0364 (0.0244)	
training:	Epoch: [50][171/233]	Loss 0.0042 (0.0242)	
training:	Epoch: [50][172/233]	Loss 0.0047 (0.0241)	
training:	Epoch: [50][173/233]	Loss 0.0042 (0.0240)	
training:	Epoch: [50][174/233]	Loss 0.0036 (0.0239)	
training:	Epoch: [50][175/233]	Loss 0.1881 (0.0248)	
training:	Epoch: [50][176/233]	Loss 0.0042 (0.0247)	
training:	Epoch: [50][177/233]	Loss 0.0053 (0.0246)	
training:	Epoch: [50][178/233]	Loss 0.0046 (0.0245)	
training:	Epoch: [50][179/233]	Loss 0.1829 (0.0254)	
training:	Epoch: [50][180/233]	Loss 0.0086 (0.0253)	
training:	Epoch: [50][181/233]	Loss 0.0070 (0.0252)	
training:	Epoch: [50][182/233]	Loss 0.0089 (0.0251)	
training:	Epoch: [50][183/233]	Loss 0.0133 (0.0250)	
training:	Epoch: [50][184/233]	Loss 0.0057 (0.0249)	
training:	Epoch: [50][185/233]	Loss 0.0038 (0.0248)	
training:	Epoch: [50][186/233]	Loss 0.0435 (0.0249)	
training:	Epoch: [50][187/233]	Loss 0.0036 (0.0248)	
training:	Epoch: [50][188/233]	Loss 0.0066 (0.0247)	
training:	Epoch: [50][189/233]	Loss 0.0083 (0.0246)	
training:	Epoch: [50][190/233]	Loss 0.0100 (0.0245)	
training:	Epoch: [50][191/233]	Loss 0.0054 (0.0244)	
training:	Epoch: [50][192/233]	Loss 0.0148 (0.0244)	
training:	Epoch: [50][193/233]	Loss 0.0069 (0.0243)	
training:	Epoch: [50][194/233]	Loss 0.1784 (0.0251)	
training:	Epoch: [50][195/233]	Loss 0.0037 (0.0250)	
training:	Epoch: [50][196/233]	Loss 0.0406 (0.0251)	
training:	Epoch: [50][197/233]	Loss 0.0048 (0.0250)	
training:	Epoch: [50][198/233]	Loss 0.0039 (0.0248)	
training:	Epoch: [50][199/233]	Loss 0.3114 (0.0263)	
training:	Epoch: [50][200/233]	Loss 0.0052 (0.0262)	
training:	Epoch: [50][201/233]	Loss 0.0041 (0.0261)	
training:	Epoch: [50][202/233]	Loss 0.0055 (0.0260)	
training:	Epoch: [50][203/233]	Loss 0.0797 (0.0262)	
training:	Epoch: [50][204/233]	Loss 0.0204 (0.0262)	
training:	Epoch: [50][205/233]	Loss 0.0044 (0.0261)	
training:	Epoch: [50][206/233]	Loss 0.0056 (0.0260)	
training:	Epoch: [50][207/233]	Loss 0.1543 (0.0266)	
training:	Epoch: [50][208/233]	Loss 0.0181 (0.0266)	
training:	Epoch: [50][209/233]	Loss 0.0574 (0.0267)	
training:	Epoch: [50][210/233]	Loss 0.0053 (0.0266)	
training:	Epoch: [50][211/233]	Loss 0.0081 (0.0265)	
training:	Epoch: [50][212/233]	Loss 0.0079 (0.0264)	
training:	Epoch: [50][213/233]	Loss 0.0100 (0.0264)	
training:	Epoch: [50][214/233]	Loss 0.0038 (0.0263)	
training:	Epoch: [50][215/233]	Loss 0.1092 (0.0267)	
training:	Epoch: [50][216/233]	Loss 0.0241 (0.0266)	
training:	Epoch: [50][217/233]	Loss 0.0079 (0.0266)	
training:	Epoch: [50][218/233]	Loss 0.0038 (0.0264)	
training:	Epoch: [50][219/233]	Loss 0.0053 (0.0264)	
training:	Epoch: [50][220/233]	Loss 0.0044 (0.0263)	
training:	Epoch: [50][221/233]	Loss 0.1295 (0.0267)	
training:	Epoch: [50][222/233]	Loss 0.0255 (0.0267)	
training:	Epoch: [50][223/233]	Loss 0.0081 (0.0266)	
training:	Epoch: [50][224/233]	Loss 0.1022 (0.0270)	
training:	Epoch: [50][225/233]	Loss 0.0041 (0.0269)	
training:	Epoch: [50][226/233]	Loss 0.0051 (0.0268)	
training:	Epoch: [50][227/233]	Loss 0.0053 (0.0267)	
training:	Epoch: [50][228/233]	Loss 0.0053 (0.0266)	
training:	Epoch: [50][229/233]	Loss 0.0475 (0.0267)	
training:	Epoch: [50][230/233]	Loss 0.0444 (0.0267)	
training:	Epoch: [50][231/233]	Loss 0.0070 (0.0267)	
training:	Epoch: [50][232/233]	Loss 0.0779 (0.0269)	
training:	Epoch: [50][233/233]	Loss 0.0038 (0.0268)	
Training:	 Loss: 0.0267

Training:	 ACC: 0.9980 0.9980 0.9985 0.9975
Validation:	 ACC: 0.7885 0.7903 0.8253 0.7517
Validation:	 Best_BACC: 0.8162 0.8154 0.7998 0.8326
Validation:	 Loss: 0.8870
Pretraining:	Epoch 51/200
----------
training:	Epoch: [51][1/233]	Loss 0.0045 (0.0045)	
training:	Epoch: [51][2/233]	Loss 0.0705 (0.0375)	
training:	Epoch: [51][3/233]	Loss 0.0186 (0.0312)	
training:	Epoch: [51][4/233]	Loss 0.0073 (0.0252)	
training:	Epoch: [51][5/233]	Loss 0.0037 (0.0209)	
training:	Epoch: [51][6/233]	Loss 0.0039 (0.0181)	
training:	Epoch: [51][7/233]	Loss 0.0123 (0.0173)	
training:	Epoch: [51][8/233]	Loss 0.1816 (0.0378)	
training:	Epoch: [51][9/233]	Loss 0.1771 (0.0533)	
training:	Epoch: [51][10/233]	Loss 0.0035 (0.0483)	
training:	Epoch: [51][11/233]	Loss 0.0063 (0.0445)	
training:	Epoch: [51][12/233]	Loss 0.0070 (0.0413)	
training:	Epoch: [51][13/233]	Loss 0.0189 (0.0396)	
training:	Epoch: [51][14/233]	Loss 0.1463 (0.0472)	
training:	Epoch: [51][15/233]	Loss 0.0068 (0.0445)	
training:	Epoch: [51][16/233]	Loss 0.0094 (0.0423)	
training:	Epoch: [51][17/233]	Loss 0.0036 (0.0401)	
training:	Epoch: [51][18/233]	Loss 0.0044 (0.0381)	
training:	Epoch: [51][19/233]	Loss 0.0050 (0.0363)	
training:	Epoch: [51][20/233]	Loss 0.0038 (0.0347)	
training:	Epoch: [51][21/233]	Loss 0.0042 (0.0333)	
training:	Epoch: [51][22/233]	Loss 0.0087 (0.0321)	
training:	Epoch: [51][23/233]	Loss 0.0054 (0.0310)	
training:	Epoch: [51][24/233]	Loss 0.0041 (0.0299)	
training:	Epoch: [51][25/233]	Loss 0.0038 (0.0288)	
training:	Epoch: [51][26/233]	Loss 0.0612 (0.0301)	
training:	Epoch: [51][27/233]	Loss 0.0291 (0.0300)	
training:	Epoch: [51][28/233]	Loss 0.0103 (0.0293)	
training:	Epoch: [51][29/233]	Loss 0.0047 (0.0285)	
training:	Epoch: [51][30/233]	Loss 0.0058 (0.0277)	
training:	Epoch: [51][31/233]	Loss 0.0035 (0.0269)	
training:	Epoch: [51][32/233]	Loss 0.0228 (0.0268)	
training:	Epoch: [51][33/233]	Loss 0.0051 (0.0262)	
training:	Epoch: [51][34/233]	Loss 0.0575 (0.0271)	
training:	Epoch: [51][35/233]	Loss 0.0399 (0.0274)	
training:	Epoch: [51][36/233]	Loss 0.0039 (0.0268)	
training:	Epoch: [51][37/233]	Loss 0.0053 (0.0262)	
training:	Epoch: [51][38/233]	Loss 0.0134 (0.0259)	
training:	Epoch: [51][39/233]	Loss 0.0043 (0.0253)	
training:	Epoch: [51][40/233]	Loss 0.0049 (0.0248)	
training:	Epoch: [51][41/233]	Loss 0.0049 (0.0243)	
training:	Epoch: [51][42/233]	Loss 0.0082 (0.0239)	
training:	Epoch: [51][43/233]	Loss 0.0047 (0.0235)	
training:	Epoch: [51][44/233]	Loss 0.0040 (0.0231)	
training:	Epoch: [51][45/233]	Loss 0.0106 (0.0228)	
training:	Epoch: [51][46/233]	Loss 0.0039 (0.0224)	
training:	Epoch: [51][47/233]	Loss 0.0043 (0.0220)	
training:	Epoch: [51][48/233]	Loss 0.0040 (0.0216)	
training:	Epoch: [51][49/233]	Loss 0.1540 (0.0243)	
training:	Epoch: [51][50/233]	Loss 0.0098 (0.0240)	
training:	Epoch: [51][51/233]	Loss 0.0034 (0.0236)	
training:	Epoch: [51][52/233]	Loss 0.0056 (0.0233)	
training:	Epoch: [51][53/233]	Loss 0.0032 (0.0229)	
training:	Epoch: [51][54/233]	Loss 0.0096 (0.0226)	
training:	Epoch: [51][55/233]	Loss 0.0047 (0.0223)	
training:	Epoch: [51][56/233]	Loss 0.0051 (0.0220)	
training:	Epoch: [51][57/233]	Loss 0.0645 (0.0228)	
training:	Epoch: [51][58/233]	Loss 0.0035 (0.0224)	
training:	Epoch: [51][59/233]	Loss 0.0047 (0.0221)	
training:	Epoch: [51][60/233]	Loss 0.0046 (0.0218)	
training:	Epoch: [51][61/233]	Loss 0.0356 (0.0221)	
training:	Epoch: [51][62/233]	Loss 0.1834 (0.0247)	
training:	Epoch: [51][63/233]	Loss 0.0116 (0.0244)	
training:	Epoch: [51][64/233]	Loss 0.0038 (0.0241)	
training:	Epoch: [51][65/233]	Loss 0.0044 (0.0238)	
training:	Epoch: [51][66/233]	Loss 0.0242 (0.0238)	
training:	Epoch: [51][67/233]	Loss 0.0213 (0.0238)	
training:	Epoch: [51][68/233]	Loss 0.0096 (0.0236)	
training:	Epoch: [51][69/233]	Loss 0.0311 (0.0237)	
training:	Epoch: [51][70/233]	Loss 0.0049 (0.0234)	
training:	Epoch: [51][71/233]	Loss 0.0060 (0.0232)	
training:	Epoch: [51][72/233]	Loss 0.0113 (0.0230)	
training:	Epoch: [51][73/233]	Loss 0.0073 (0.0228)	
training:	Epoch: [51][74/233]	Loss 0.0036 (0.0225)	
training:	Epoch: [51][75/233]	Loss 0.2397 (0.0254)	
training:	Epoch: [51][76/233]	Loss 0.1856 (0.0275)	
training:	Epoch: [51][77/233]	Loss 0.0043 (0.0272)	
training:	Epoch: [51][78/233]	Loss 0.0034 (0.0269)	
training:	Epoch: [51][79/233]	Loss 0.0039 (0.0266)	
training:	Epoch: [51][80/233]	Loss 0.0040 (0.0264)	
training:	Epoch: [51][81/233]	Loss 0.0051 (0.0261)	
training:	Epoch: [51][82/233]	Loss 0.0126 (0.0259)	
training:	Epoch: [51][83/233]	Loss 0.1788 (0.0278)	
training:	Epoch: [51][84/233]	Loss 0.0215 (0.0277)	
training:	Epoch: [51][85/233]	Loss 0.0031 (0.0274)	
training:	Epoch: [51][86/233]	Loss 0.0064 (0.0272)	
training:	Epoch: [51][87/233]	Loss 0.0096 (0.0270)	
training:	Epoch: [51][88/233]	Loss 0.0060 (0.0267)	
training:	Epoch: [51][89/233]	Loss 0.0038 (0.0265)	
training:	Epoch: [51][90/233]	Loss 0.0065 (0.0262)	
training:	Epoch: [51][91/233]	Loss 0.0058 (0.0260)	
training:	Epoch: [51][92/233]	Loss 0.0043 (0.0258)	
training:	Epoch: [51][93/233]	Loss 0.0041 (0.0256)	
training:	Epoch: [51][94/233]	Loss 0.0070 (0.0254)	
training:	Epoch: [51][95/233]	Loss 0.0041 (0.0251)	
training:	Epoch: [51][96/233]	Loss 0.0036 (0.0249)	
training:	Epoch: [51][97/233]	Loss 0.0045 (0.0247)	
training:	Epoch: [51][98/233]	Loss 0.0035 (0.0245)	
training:	Epoch: [51][99/233]	Loss 0.0037 (0.0243)	
training:	Epoch: [51][100/233]	Loss 0.0090 (0.0241)	
training:	Epoch: [51][101/233]	Loss 0.0041 (0.0239)	
training:	Epoch: [51][102/233]	Loss 0.0035 (0.0237)	
training:	Epoch: [51][103/233]	Loss 0.0057 (0.0235)	
training:	Epoch: [51][104/233]	Loss 0.0053 (0.0234)	
training:	Epoch: [51][105/233]	Loss 0.0036 (0.0232)	
training:	Epoch: [51][106/233]	Loss 0.0055 (0.0230)	
training:	Epoch: [51][107/233]	Loss 0.0034 (0.0228)	
training:	Epoch: [51][108/233]	Loss 0.0146 (0.0228)	
training:	Epoch: [51][109/233]	Loss 0.0780 (0.0233)	
training:	Epoch: [51][110/233]	Loss 0.0041 (0.0231)	
training:	Epoch: [51][111/233]	Loss 0.0039 (0.0229)	
training:	Epoch: [51][112/233]	Loss 0.0503 (0.0232)	
training:	Epoch: [51][113/233]	Loss 0.0109 (0.0231)	
training:	Epoch: [51][114/233]	Loss 0.0037 (0.0229)	
training:	Epoch: [51][115/233]	Loss 0.0056 (0.0227)	
training:	Epoch: [51][116/233]	Loss 0.0045 (0.0226)	
training:	Epoch: [51][117/233]	Loss 0.0041 (0.0224)	
training:	Epoch: [51][118/233]	Loss 0.0035 (0.0223)	
training:	Epoch: [51][119/233]	Loss 0.1308 (0.0232)	
training:	Epoch: [51][120/233]	Loss 0.0034 (0.0230)	
training:	Epoch: [51][121/233]	Loss 0.0079 (0.0229)	
training:	Epoch: [51][122/233]	Loss 0.0042 (0.0227)	
training:	Epoch: [51][123/233]	Loss 0.1013 (0.0234)	
training:	Epoch: [51][124/233]	Loss 0.0063 (0.0232)	
training:	Epoch: [51][125/233]	Loss 0.0068 (0.0231)	
training:	Epoch: [51][126/233]	Loss 0.0063 (0.0230)	
training:	Epoch: [51][127/233]	Loss 0.0049 (0.0228)	
training:	Epoch: [51][128/233]	Loss 0.0209 (0.0228)	
training:	Epoch: [51][129/233]	Loss 0.0062 (0.0227)	
training:	Epoch: [51][130/233]	Loss 0.0199 (0.0227)	
training:	Epoch: [51][131/233]	Loss 0.0306 (0.0227)	
training:	Epoch: [51][132/233]	Loss 0.0059 (0.0226)	
training:	Epoch: [51][133/233]	Loss 0.0031 (0.0224)	
training:	Epoch: [51][134/233]	Loss 0.1966 (0.0237)	
training:	Epoch: [51][135/233]	Loss 0.1969 (0.0250)	
training:	Epoch: [51][136/233]	Loss 0.0053 (0.0249)	
training:	Epoch: [51][137/233]	Loss 0.0035 (0.0247)	
training:	Epoch: [51][138/233]	Loss 0.0123 (0.0246)	
training:	Epoch: [51][139/233]	Loss 0.0123 (0.0245)	
training:	Epoch: [51][140/233]	Loss 0.0053 (0.0244)	
training:	Epoch: [51][141/233]	Loss 0.0042 (0.0243)	
training:	Epoch: [51][142/233]	Loss 0.1852 (0.0254)	
training:	Epoch: [51][143/233]	Loss 0.0039 (0.0252)	
training:	Epoch: [51][144/233]	Loss 0.0057 (0.0251)	
training:	Epoch: [51][145/233]	Loss 0.0040 (0.0250)	
training:	Epoch: [51][146/233]	Loss 0.0087 (0.0249)	
training:	Epoch: [51][147/233]	Loss 0.0115 (0.0248)	
training:	Epoch: [51][148/233]	Loss 0.0223 (0.0247)	
training:	Epoch: [51][149/233]	Loss 0.0090 (0.0246)	
training:	Epoch: [51][150/233]	Loss 0.0052 (0.0245)	
training:	Epoch: [51][151/233]	Loss 0.0039 (0.0244)	
training:	Epoch: [51][152/233]	Loss 0.0038 (0.0242)	
training:	Epoch: [51][153/233]	Loss 0.0041 (0.0241)	
training:	Epoch: [51][154/233]	Loss 0.1824 (0.0251)	
training:	Epoch: [51][155/233]	Loss 0.1852 (0.0262)	
training:	Epoch: [51][156/233]	Loss 0.1336 (0.0269)	
training:	Epoch: [51][157/233]	Loss 0.0040 (0.0267)	
training:	Epoch: [51][158/233]	Loss 0.0070 (0.0266)	
training:	Epoch: [51][159/233]	Loss 0.0035 (0.0264)	
training:	Epoch: [51][160/233]	Loss 0.0048 (0.0263)	
training:	Epoch: [51][161/233]	Loss 0.0069 (0.0262)	
training:	Epoch: [51][162/233]	Loss 0.0045 (0.0261)	
training:	Epoch: [51][163/233]	Loss 0.0059 (0.0259)	
training:	Epoch: [51][164/233]	Loss 0.0563 (0.0261)	
training:	Epoch: [51][165/233]	Loss 0.0037 (0.0260)	
training:	Epoch: [51][166/233]	Loss 0.0042 (0.0258)	
training:	Epoch: [51][167/233]	Loss 0.0034 (0.0257)	
training:	Epoch: [51][168/233]	Loss 0.0040 (0.0256)	
training:	Epoch: [51][169/233]	Loss 0.0040 (0.0255)	
training:	Epoch: [51][170/233]	Loss 0.0039 (0.0253)	
training:	Epoch: [51][171/233]	Loss 0.0049 (0.0252)	
training:	Epoch: [51][172/233]	Loss 0.1480 (0.0259)	
training:	Epoch: [51][173/233]	Loss 0.0073 (0.0258)	
training:	Epoch: [51][174/233]	Loss 0.0047 (0.0257)	
training:	Epoch: [51][175/233]	Loss 0.1845 (0.0266)	
training:	Epoch: [51][176/233]	Loss 0.0044 (0.0265)	
training:	Epoch: [51][177/233]	Loss 0.0054 (0.0264)	
training:	Epoch: [51][178/233]	Loss 0.0035 (0.0262)	
training:	Epoch: [51][179/233]	Loss 0.0041 (0.0261)	
training:	Epoch: [51][180/233]	Loss 0.0077 (0.0260)	
training:	Epoch: [51][181/233]	Loss 0.0035 (0.0259)	
training:	Epoch: [51][182/233]	Loss 0.0057 (0.0258)	
training:	Epoch: [51][183/233]	Loss 0.0513 (0.0259)	
training:	Epoch: [51][184/233]	Loss 0.0035 (0.0258)	
training:	Epoch: [51][185/233]	Loss 0.0050 (0.0257)	
training:	Epoch: [51][186/233]	Loss 0.0083 (0.0256)	
training:	Epoch: [51][187/233]	Loss 0.0043 (0.0255)	
training:	Epoch: [51][188/233]	Loss 0.1896 (0.0263)	
training:	Epoch: [51][189/233]	Loss 0.0035 (0.0262)	
training:	Epoch: [51][190/233]	Loss 0.0033 (0.0261)	
training:	Epoch: [51][191/233]	Loss 0.0058 (0.0260)	
training:	Epoch: [51][192/233]	Loss 0.0047 (0.0259)	
training:	Epoch: [51][193/233]	Loss 0.0041 (0.0258)	
training:	Epoch: [51][194/233]	Loss 0.0055 (0.0257)	
training:	Epoch: [51][195/233]	Loss 0.0063 (0.0256)	
training:	Epoch: [51][196/233]	Loss 0.0096 (0.0255)	
training:	Epoch: [51][197/233]	Loss 0.0046 (0.0254)	
training:	Epoch: [51][198/233]	Loss 0.0043 (0.0253)	
training:	Epoch: [51][199/233]	Loss 0.0151 (0.0252)	
training:	Epoch: [51][200/233]	Loss 0.0043 (0.0251)	
training:	Epoch: [51][201/233]	Loss 0.0052 (0.0250)	
training:	Epoch: [51][202/233]	Loss 0.0046 (0.0249)	
training:	Epoch: [51][203/233]	Loss 0.0349 (0.0250)	
training:	Epoch: [51][204/233]	Loss 0.0034 (0.0249)	
training:	Epoch: [51][205/233]	Loss 0.0594 (0.0250)	
training:	Epoch: [51][206/233]	Loss 0.0038 (0.0249)	
training:	Epoch: [51][207/233]	Loss 0.0060 (0.0248)	
training:	Epoch: [51][208/233]	Loss 0.0041 (0.0247)	
training:	Epoch: [51][209/233]	Loss 0.1217 (0.0252)	
training:	Epoch: [51][210/233]	Loss 0.0043 (0.0251)	
training:	Epoch: [51][211/233]	Loss 0.0042 (0.0250)	
training:	Epoch: [51][212/233]	Loss 0.0036 (0.0249)	
training:	Epoch: [51][213/233]	Loss 0.0041 (0.0248)	
training:	Epoch: [51][214/233]	Loss 0.0046 (0.0247)	
training:	Epoch: [51][215/233]	Loss 0.0120 (0.0246)	
training:	Epoch: [51][216/233]	Loss 0.0220 (0.0246)	
training:	Epoch: [51][217/233]	Loss 0.0047 (0.0245)	
training:	Epoch: [51][218/233]	Loss 0.0337 (0.0246)	
training:	Epoch: [51][219/233]	Loss 0.0043 (0.0245)	
training:	Epoch: [51][220/233]	Loss 0.0046 (0.0244)	
training:	Epoch: [51][221/233]	Loss 0.0035 (0.0243)	
training:	Epoch: [51][222/233]	Loss 0.0047 (0.0242)	
training:	Epoch: [51][223/233]	Loss 0.1342 (0.0247)	
training:	Epoch: [51][224/233]	Loss 0.0047 (0.0246)	
training:	Epoch: [51][225/233]	Loss 0.0040 (0.0245)	
training:	Epoch: [51][226/233]	Loss 0.0079 (0.0245)	
training:	Epoch: [51][227/233]	Loss 0.0079 (0.0244)	
training:	Epoch: [51][228/233]	Loss 0.0484 (0.0245)	
training:	Epoch: [51][229/233]	Loss 0.0183 (0.0245)	
training:	Epoch: [51][230/233]	Loss 0.0037 (0.0244)	
training:	Epoch: [51][231/233]	Loss 0.1782 (0.0250)	
training:	Epoch: [51][232/233]	Loss 0.0038 (0.0249)	
training:	Epoch: [51][233/233]	Loss 0.0038 (0.0249)	
Training:	 Loss: 0.0248

Training:	 ACC: 0.9984 0.9984 0.9985 0.9983
Validation:	 ACC: 0.7897 0.7887 0.7681 0.8112
Validation:	 Best_BACC: 0.8162 0.8154 0.7998 0.8326
Validation:	 Loss: 0.9214
Pretraining:	Epoch 52/200
----------
training:	Epoch: [52][1/233]	Loss 0.0075 (0.0075)	
training:	Epoch: [52][2/233]	Loss 0.0046 (0.0061)	
training:	Epoch: [52][3/233]	Loss 0.0074 (0.0065)	
training:	Epoch: [52][4/233]	Loss 0.1683 (0.0469)	
training:	Epoch: [52][5/233]	Loss 0.0227 (0.0421)	
training:	Epoch: [52][6/233]	Loss 0.1684 (0.0631)	
training:	Epoch: [52][7/233]	Loss 0.0035 (0.0546)	
training:	Epoch: [52][8/233]	Loss 0.0454 (0.0535)	
training:	Epoch: [52][9/233]	Loss 0.0056 (0.0482)	
training:	Epoch: [52][10/233]	Loss 0.0038 (0.0437)	
training:	Epoch: [52][11/233]	Loss 0.1934 (0.0573)	
training:	Epoch: [52][12/233]	Loss 0.0186 (0.0541)	
training:	Epoch: [52][13/233]	Loss 0.0275 (0.0521)	
training:	Epoch: [52][14/233]	Loss 0.0038 (0.0486)	
training:	Epoch: [52][15/233]	Loss 0.0042 (0.0456)	
training:	Epoch: [52][16/233]	Loss 0.0062 (0.0432)	
training:	Epoch: [52][17/233]	Loss 0.0047 (0.0409)	
training:	Epoch: [52][18/233]	Loss 0.0035 (0.0388)	
training:	Epoch: [52][19/233]	Loss 0.0054 (0.0371)	
training:	Epoch: [52][20/233]	Loss 0.0063 (0.0355)	
training:	Epoch: [52][21/233]	Loss 0.0049 (0.0341)	
training:	Epoch: [52][22/233]	Loss 0.0829 (0.0363)	
training:	Epoch: [52][23/233]	Loss 0.0043 (0.0349)	
training:	Epoch: [52][24/233]	Loss 0.0108 (0.0339)	
training:	Epoch: [52][25/233]	Loss 0.0037 (0.0327)	
training:	Epoch: [52][26/233]	Loss 0.0034 (0.0316)	
training:	Epoch: [52][27/233]	Loss 0.0512 (0.0323)	
training:	Epoch: [52][28/233]	Loss 0.0039 (0.0313)	
training:	Epoch: [52][29/233]	Loss 0.0041 (0.0303)	
training:	Epoch: [52][30/233]	Loss 0.0049 (0.0295)	
training:	Epoch: [52][31/233]	Loss 0.0037 (0.0287)	
training:	Epoch: [52][32/233]	Loss 0.0126 (0.0282)	
training:	Epoch: [52][33/233]	Loss 0.0464 (0.0287)	
training:	Epoch: [52][34/233]	Loss 0.0070 (0.0281)	
training:	Epoch: [52][35/233]	Loss 0.0054 (0.0274)	
training:	Epoch: [52][36/233]	Loss 0.0089 (0.0269)	
training:	Epoch: [52][37/233]	Loss 0.0053 (0.0263)	
training:	Epoch: [52][38/233]	Loss 0.0113 (0.0259)	
training:	Epoch: [52][39/233]	Loss 0.0038 (0.0254)	
training:	Epoch: [52][40/233]	Loss 0.0044 (0.0248)	
training:	Epoch: [52][41/233]	Loss 0.0068 (0.0244)	
training:	Epoch: [52][42/233]	Loss 0.0041 (0.0239)	
training:	Epoch: [52][43/233]	Loss 0.0046 (0.0235)	
training:	Epoch: [52][44/233]	Loss 0.0119 (0.0232)	
training:	Epoch: [52][45/233]	Loss 0.0073 (0.0229)	
training:	Epoch: [52][46/233]	Loss 0.0106 (0.0226)	
training:	Epoch: [52][47/233]	Loss 0.0040 (0.0222)	
training:	Epoch: [52][48/233]	Loss 0.0039 (0.0218)	
training:	Epoch: [52][49/233]	Loss 0.0929 (0.0233)	
training:	Epoch: [52][50/233]	Loss 0.0048 (0.0229)	
training:	Epoch: [52][51/233]	Loss 0.0051 (0.0225)	
training:	Epoch: [52][52/233]	Loss 0.1792 (0.0256)	
training:	Epoch: [52][53/233]	Loss 0.0040 (0.0252)	
training:	Epoch: [52][54/233]	Loss 0.0053 (0.0248)	
training:	Epoch: [52][55/233]	Loss 0.0127 (0.0246)	
training:	Epoch: [52][56/233]	Loss 0.0042 (0.0242)	
training:	Epoch: [52][57/233]	Loss 0.0034 (0.0238)	
training:	Epoch: [52][58/233]	Loss 0.0038 (0.0235)	
training:	Epoch: [52][59/233]	Loss 0.0039 (0.0232)	
training:	Epoch: [52][60/233]	Loss 0.0031 (0.0228)	
training:	Epoch: [52][61/233]	Loss 0.0039 (0.0225)	
training:	Epoch: [52][62/233]	Loss 0.0077 (0.0223)	
training:	Epoch: [52][63/233]	Loss 0.1976 (0.0251)	
training:	Epoch: [52][64/233]	Loss 0.0427 (0.0253)	
training:	Epoch: [52][65/233]	Loss 0.0040 (0.0250)	
training:	Epoch: [52][66/233]	Loss 0.0040 (0.0247)	
training:	Epoch: [52][67/233]	Loss 0.0037 (0.0244)	
training:	Epoch: [52][68/233]	Loss 0.0058 (0.0241)	
training:	Epoch: [52][69/233]	Loss 0.0103 (0.0239)	
training:	Epoch: [52][70/233]	Loss 0.0046 (0.0236)	
training:	Epoch: [52][71/233]	Loss 0.0037 (0.0233)	
training:	Epoch: [52][72/233]	Loss 0.0054 (0.0231)	
training:	Epoch: [52][73/233]	Loss 0.0038 (0.0228)	
training:	Epoch: [52][74/233]	Loss 0.0080 (0.0226)	
training:	Epoch: [52][75/233]	Loss 0.0057 (0.0224)	
training:	Epoch: [52][76/233]	Loss 0.0095 (0.0222)	
training:	Epoch: [52][77/233]	Loss 0.0034 (0.0220)	
training:	Epoch: [52][78/233]	Loss 0.0039 (0.0218)	
training:	Epoch: [52][79/233]	Loss 0.0105 (0.0216)	
training:	Epoch: [52][80/233]	Loss 0.0052 (0.0214)	
training:	Epoch: [52][81/233]	Loss 0.0037 (0.0212)	
training:	Epoch: [52][82/233]	Loss 0.0035 (0.0210)	
training:	Epoch: [52][83/233]	Loss 0.0061 (0.0208)	
training:	Epoch: [52][84/233]	Loss 0.3037 (0.0242)	
training:	Epoch: [52][85/233]	Loss 0.0051 (0.0239)	
training:	Epoch: [52][86/233]	Loss 0.0042 (0.0237)	
training:	Epoch: [52][87/233]	Loss 0.0033 (0.0235)	
training:	Epoch: [52][88/233]	Loss 0.0098 (0.0233)	
training:	Epoch: [52][89/233]	Loss 0.0156 (0.0232)	
training:	Epoch: [52][90/233]	Loss 0.0036 (0.0230)	
training:	Epoch: [52][91/233]	Loss 0.0938 (0.0238)	
training:	Epoch: [52][92/233]	Loss 0.0039 (0.0236)	
training:	Epoch: [52][93/233]	Loss 0.0047 (0.0234)	
training:	Epoch: [52][94/233]	Loss 0.0066 (0.0232)	
training:	Epoch: [52][95/233]	Loss 0.0075 (0.0230)	
training:	Epoch: [52][96/233]	Loss 0.0092 (0.0229)	
training:	Epoch: [52][97/233]	Loss 0.0073 (0.0227)	
training:	Epoch: [52][98/233]	Loss 0.1452 (0.0240)	
training:	Epoch: [52][99/233]	Loss 0.0039 (0.0238)	
training:	Epoch: [52][100/233]	Loss 0.0035 (0.0236)	
training:	Epoch: [52][101/233]	Loss 0.0046 (0.0234)	
training:	Epoch: [52][102/233]	Loss 0.0052 (0.0232)	
training:	Epoch: [52][103/233]	Loss 0.0226 (0.0232)	
training:	Epoch: [52][104/233]	Loss 0.0039 (0.0230)	
training:	Epoch: [52][105/233]	Loss 0.0034 (0.0228)	
training:	Epoch: [52][106/233]	Loss 0.0127 (0.0227)	
training:	Epoch: [52][107/233]	Loss 0.0266 (0.0228)	
training:	Epoch: [52][108/233]	Loss 0.0045 (0.0226)	
training:	Epoch: [52][109/233]	Loss 0.0037 (0.0224)	
training:	Epoch: [52][110/233]	Loss 0.1214 (0.0233)	
training:	Epoch: [52][111/233]	Loss 0.0048 (0.0232)	
training:	Epoch: [52][112/233]	Loss 0.0033 (0.0230)	
training:	Epoch: [52][113/233]	Loss 0.0048 (0.0228)	
training:	Epoch: [52][114/233]	Loss 0.0041 (0.0227)	
training:	Epoch: [52][115/233]	Loss 0.0041 (0.0225)	
training:	Epoch: [52][116/233]	Loss 0.0113 (0.0224)	
training:	Epoch: [52][117/233]	Loss 0.0040 (0.0222)	
training:	Epoch: [52][118/233]	Loss 0.0071 (0.0221)	
training:	Epoch: [52][119/233]	Loss 0.0042 (0.0220)	
training:	Epoch: [52][120/233]	Loss 0.0038 (0.0218)	
training:	Epoch: [52][121/233]	Loss 0.0043 (0.0217)	
training:	Epoch: [52][122/233]	Loss 0.0118 (0.0216)	
training:	Epoch: [52][123/233]	Loss 0.0065 (0.0215)	
training:	Epoch: [52][124/233]	Loss 0.0060 (0.0213)	
training:	Epoch: [52][125/233]	Loss 0.0110 (0.0213)	
training:	Epoch: [52][126/233]	Loss 0.0113 (0.0212)	
training:	Epoch: [52][127/233]	Loss 0.0068 (0.0211)	
training:	Epoch: [52][128/233]	Loss 0.0370 (0.0212)	
training:	Epoch: [52][129/233]	Loss 0.0043 (0.0211)	
training:	Epoch: [52][130/233]	Loss 0.0032 (0.0209)	
training:	Epoch: [52][131/233]	Loss 0.0040 (0.0208)	
training:	Epoch: [52][132/233]	Loss 0.0034 (0.0207)	
training:	Epoch: [52][133/233]	Loss 0.0037 (0.0205)	
training:	Epoch: [52][134/233]	Loss 0.0042 (0.0204)	
training:	Epoch: [52][135/233]	Loss 0.0040 (0.0203)	
training:	Epoch: [52][136/233]	Loss 0.0194 (0.0203)	
training:	Epoch: [52][137/233]	Loss 0.0042 (0.0202)	
training:	Epoch: [52][138/233]	Loss 0.0035 (0.0200)	
training:	Epoch: [52][139/233]	Loss 0.0042 (0.0199)	
training:	Epoch: [52][140/233]	Loss 0.0037 (0.0198)	
training:	Epoch: [52][141/233]	Loss 0.0182 (0.0198)	
training:	Epoch: [52][142/233]	Loss 0.0054 (0.0197)	
training:	Epoch: [52][143/233]	Loss 0.0035 (0.0196)	
training:	Epoch: [52][144/233]	Loss 0.0081 (0.0195)	
training:	Epoch: [52][145/233]	Loss 0.0035 (0.0194)	
training:	Epoch: [52][146/233]	Loss 0.0037 (0.0193)	
training:	Epoch: [52][147/233]	Loss 0.0061 (0.0192)	
training:	Epoch: [52][148/233]	Loss 0.0224 (0.0192)	
training:	Epoch: [52][149/233]	Loss 0.0035 (0.0191)	
training:	Epoch: [52][150/233]	Loss 0.0036 (0.0190)	
training:	Epoch: [52][151/233]	Loss 0.0046 (0.0189)	
training:	Epoch: [52][152/233]	Loss 0.0038 (0.0188)	
training:	Epoch: [52][153/233]	Loss 0.0077 (0.0187)	
training:	Epoch: [52][154/233]	Loss 0.0045 (0.0187)	
training:	Epoch: [52][155/233]	Loss 0.0050 (0.0186)	
training:	Epoch: [52][156/233]	Loss 0.0101 (0.0185)	
training:	Epoch: [52][157/233]	Loss 0.0040 (0.0184)	
training:	Epoch: [52][158/233]	Loss 0.0064 (0.0183)	
training:	Epoch: [52][159/233]	Loss 0.1351 (0.0191)	
training:	Epoch: [52][160/233]	Loss 0.0096 (0.0190)	
training:	Epoch: [52][161/233]	Loss 0.0031 (0.0189)	
training:	Epoch: [52][162/233]	Loss 0.0043 (0.0188)	
training:	Epoch: [52][163/233]	Loss 0.0290 (0.0189)	
training:	Epoch: [52][164/233]	Loss 0.0050 (0.0188)	
training:	Epoch: [52][165/233]	Loss 0.0092 (0.0187)	
training:	Epoch: [52][166/233]	Loss 0.0037 (0.0187)	
training:	Epoch: [52][167/233]	Loss 0.0041 (0.0186)	
training:	Epoch: [52][168/233]	Loss 0.0036 (0.0185)	
training:	Epoch: [52][169/233]	Loss 0.0055 (0.0184)	
training:	Epoch: [52][170/233]	Loss 0.0044 (0.0183)	
training:	Epoch: [52][171/233]	Loss 0.0039 (0.0182)	
training:	Epoch: [52][172/233]	Loss 0.1882 (0.0192)	
training:	Epoch: [52][173/233]	Loss 0.0153 (0.0192)	
training:	Epoch: [52][174/233]	Loss 0.0038 (0.0191)	
training:	Epoch: [52][175/233]	Loss 0.0072 (0.0190)	
training:	Epoch: [52][176/233]	Loss 0.0053 (0.0190)	
training:	Epoch: [52][177/233]	Loss 0.0138 (0.0189)	
training:	Epoch: [52][178/233]	Loss 0.1870 (0.0199)	
training:	Epoch: [52][179/233]	Loss 0.0041 (0.0198)	
training:	Epoch: [52][180/233]	Loss 0.0042 (0.0197)	
training:	Epoch: [52][181/233]	Loss 0.1864 (0.0206)	
training:	Epoch: [52][182/233]	Loss 0.0038 (0.0205)	
training:	Epoch: [52][183/233]	Loss 0.0069 (0.0205)	
training:	Epoch: [52][184/233]	Loss 0.0062 (0.0204)	
training:	Epoch: [52][185/233]	Loss 0.0076 (0.0203)	
training:	Epoch: [52][186/233]	Loss 0.0686 (0.0206)	
training:	Epoch: [52][187/233]	Loss 0.0032 (0.0205)	
training:	Epoch: [52][188/233]	Loss 0.0054 (0.0204)	
training:	Epoch: [52][189/233]	Loss 0.0042 (0.0203)	
training:	Epoch: [52][190/233]	Loss 0.0049 (0.0202)	
training:	Epoch: [52][191/233]	Loss 0.0084 (0.0202)	
training:	Epoch: [52][192/233]	Loss 0.0040 (0.0201)	
training:	Epoch: [52][193/233]	Loss 0.0138 (0.0201)	
training:	Epoch: [52][194/233]	Loss 0.0033 (0.0200)	
training:	Epoch: [52][195/233]	Loss 0.0046 (0.0199)	
training:	Epoch: [52][196/233]	Loss 0.0039 (0.0198)	
training:	Epoch: [52][197/233]	Loss 0.0037 (0.0197)	
training:	Epoch: [52][198/233]	Loss 0.0048 (0.0196)	
training:	Epoch: [52][199/233]	Loss 0.0188 (0.0196)	
training:	Epoch: [52][200/233]	Loss 0.0037 (0.0196)	
training:	Epoch: [52][201/233]	Loss 0.0046 (0.0195)	
training:	Epoch: [52][202/233]	Loss 0.0032 (0.0194)	
training:	Epoch: [52][203/233]	Loss 0.0038 (0.0193)	
training:	Epoch: [52][204/233]	Loss 0.0045 (0.0193)	
training:	Epoch: [52][205/233]	Loss 0.0184 (0.0193)	
training:	Epoch: [52][206/233]	Loss 0.0047 (0.0192)	
training:	Epoch: [52][207/233]	Loss 0.0043 (0.0191)	
training:	Epoch: [52][208/233]	Loss 0.0037 (0.0190)	
training:	Epoch: [52][209/233]	Loss 0.0056 (0.0190)	
training:	Epoch: [52][210/233]	Loss 0.0044 (0.0189)	
training:	Epoch: [52][211/233]	Loss 0.0059 (0.0188)	
training:	Epoch: [52][212/233]	Loss 0.0038 (0.0188)	
training:	Epoch: [52][213/233]	Loss 0.0030 (0.0187)	
training:	Epoch: [52][214/233]	Loss 0.0683 (0.0189)	
training:	Epoch: [52][215/233]	Loss 0.0042 (0.0189)	
training:	Epoch: [52][216/233]	Loss 0.0056 (0.0188)	
training:	Epoch: [52][217/233]	Loss 0.0033 (0.0187)	
training:	Epoch: [52][218/233]	Loss 0.1865 (0.0195)	
training:	Epoch: [52][219/233]	Loss 0.0433 (0.0196)	
training:	Epoch: [52][220/233]	Loss 0.0032 (0.0195)	
training:	Epoch: [52][221/233]	Loss 0.0037 (0.0195)	
training:	Epoch: [52][222/233]	Loss 0.0086 (0.0194)	
training:	Epoch: [52][223/233]	Loss 0.1879 (0.0202)	
training:	Epoch: [52][224/233]	Loss 0.1487 (0.0207)	
training:	Epoch: [52][225/233]	Loss 0.0057 (0.0207)	
training:	Epoch: [52][226/233]	Loss 0.0028 (0.0206)	
training:	Epoch: [52][227/233]	Loss 0.0145 (0.0206)	
training:	Epoch: [52][228/233]	Loss 0.1863 (0.0213)	
training:	Epoch: [52][229/233]	Loss 0.0085 (0.0212)	
training:	Epoch: [52][230/233]	Loss 0.0041 (0.0212)	
training:	Epoch: [52][231/233]	Loss 0.0747 (0.0214)	
training:	Epoch: [52][232/233]	Loss 0.0046 (0.0213)	
training:	Epoch: [52][233/233]	Loss 0.0520 (0.0215)	
Training:	 Loss: 0.0214

Training:	 ACC: 0.9982 0.9983 0.9987 0.9978
Validation:	 ACC: 0.7841 0.7860 0.8233 0.7449
Validation:	 Best_BACC: 0.8162 0.8154 0.7998 0.8326
Validation:	 Loss: 0.9483
Pretraining:	Epoch 53/200
----------
training:	Epoch: [53][1/233]	Loss 0.0988 (0.0988)	
training:	Epoch: [53][2/233]	Loss 0.0031 (0.0510)	
training:	Epoch: [53][3/233]	Loss 0.0051 (0.0357)	
training:	Epoch: [53][4/233]	Loss 0.0338 (0.0352)	
training:	Epoch: [53][5/233]	Loss 0.0118 (0.0305)	
training:	Epoch: [53][6/233]	Loss 0.0066 (0.0265)	
training:	Epoch: [53][7/233]	Loss 0.0048 (0.0234)	
training:	Epoch: [53][8/233]	Loss 0.0034 (0.0209)	
training:	Epoch: [53][9/233]	Loss 0.0036 (0.0190)	
training:	Epoch: [53][10/233]	Loss 0.0047 (0.0176)	
training:	Epoch: [53][11/233]	Loss 0.0866 (0.0238)	
training:	Epoch: [53][12/233]	Loss 0.0085 (0.0226)	
training:	Epoch: [53][13/233]	Loss 0.0031 (0.0211)	
training:	Epoch: [53][14/233]	Loss 0.0032 (0.0198)	
training:	Epoch: [53][15/233]	Loss 0.1476 (0.0283)	
training:	Epoch: [53][16/233]	Loss 0.0076 (0.0270)	
training:	Epoch: [53][17/233]	Loss 0.0054 (0.0257)	
training:	Epoch: [53][18/233]	Loss 0.0039 (0.0245)	
training:	Epoch: [53][19/233]	Loss 0.0400 (0.0253)	
training:	Epoch: [53][20/233]	Loss 0.0037 (0.0243)	
training:	Epoch: [53][21/233]	Loss 0.0047 (0.0233)	
training:	Epoch: [53][22/233]	Loss 0.0045 (0.0225)	
training:	Epoch: [53][23/233]	Loss 0.0053 (0.0217)	
training:	Epoch: [53][24/233]	Loss 0.0038 (0.0210)	
training:	Epoch: [53][25/233]	Loss 0.0032 (0.0203)	
training:	Epoch: [53][26/233]	Loss 0.0036 (0.0196)	
training:	Epoch: [53][27/233]	Loss 0.0040 (0.0191)	
training:	Epoch: [53][28/233]	Loss 0.0031 (0.0185)	
training:	Epoch: [53][29/233]	Loss 0.0043 (0.0180)	
training:	Epoch: [53][30/233]	Loss 0.0447 (0.0189)	
training:	Epoch: [53][31/233]	Loss 0.0032 (0.0184)	
training:	Epoch: [53][32/233]	Loss 0.0032 (0.0179)	
training:	Epoch: [53][33/233]	Loss 0.3091 (0.0267)	
training:	Epoch: [53][34/233]	Loss 0.0076 (0.0262)	
training:	Epoch: [53][35/233]	Loss 0.0087 (0.0257)	
training:	Epoch: [53][36/233]	Loss 0.0045 (0.0251)	
training:	Epoch: [53][37/233]	Loss 0.0066 (0.0246)	
training:	Epoch: [53][38/233]	Loss 0.0613 (0.0255)	
training:	Epoch: [53][39/233]	Loss 0.0069 (0.0251)	
training:	Epoch: [53][40/233]	Loss 0.0084 (0.0247)	
training:	Epoch: [53][41/233]	Loss 0.0042 (0.0242)	
training:	Epoch: [53][42/233]	Loss 0.0036 (0.0237)	
training:	Epoch: [53][43/233]	Loss 0.0028 (0.0232)	
training:	Epoch: [53][44/233]	Loss 0.1124 (0.0252)	
training:	Epoch: [53][45/233]	Loss 0.0032 (0.0247)	
training:	Epoch: [53][46/233]	Loss 0.0038 (0.0243)	
training:	Epoch: [53][47/233]	Loss 0.0143 (0.0240)	
training:	Epoch: [53][48/233]	Loss 0.0213 (0.0240)	
training:	Epoch: [53][49/233]	Loss 0.0086 (0.0237)	
training:	Epoch: [53][50/233]	Loss 0.0036 (0.0233)	
training:	Epoch: [53][51/233]	Loss 0.0787 (0.0244)	
training:	Epoch: [53][52/233]	Loss 0.0047 (0.0240)	
training:	Epoch: [53][53/233]	Loss 0.0049 (0.0236)	
training:	Epoch: [53][54/233]	Loss 0.0043 (0.0233)	
training:	Epoch: [53][55/233]	Loss 0.0041 (0.0229)	
training:	Epoch: [53][56/233]	Loss 0.1890 (0.0259)	
training:	Epoch: [53][57/233]	Loss 0.0031 (0.0255)	
training:	Epoch: [53][58/233]	Loss 0.0036 (0.0251)	
training:	Epoch: [53][59/233]	Loss 0.0082 (0.0248)	
training:	Epoch: [53][60/233]	Loss 0.0071 (0.0245)	
training:	Epoch: [53][61/233]	Loss 0.0442 (0.0248)	
training:	Epoch: [53][62/233]	Loss 0.1495 (0.0269)	
training:	Epoch: [53][63/233]	Loss 0.0031 (0.0265)	
training:	Epoch: [53][64/233]	Loss 0.0083 (0.0262)	
training:	Epoch: [53][65/233]	Loss 0.0035 (0.0258)	
training:	Epoch: [53][66/233]	Loss 0.0043 (0.0255)	
training:	Epoch: [53][67/233]	Loss 0.0044 (0.0252)	
training:	Epoch: [53][68/233]	Loss 0.0050 (0.0249)	
training:	Epoch: [53][69/233]	Loss 0.0036 (0.0246)	
training:	Epoch: [53][70/233]	Loss 0.0043 (0.0243)	
training:	Epoch: [53][71/233]	Loss 0.0032 (0.0240)	
training:	Epoch: [53][72/233]	Loss 0.0031 (0.0237)	
training:	Epoch: [53][73/233]	Loss 0.0036 (0.0234)	
training:	Epoch: [53][74/233]	Loss 0.0051 (0.0232)	
training:	Epoch: [53][75/233]	Loss 0.0034 (0.0229)	
training:	Epoch: [53][76/233]	Loss 0.0046 (0.0227)	
training:	Epoch: [53][77/233]	Loss 0.0050 (0.0225)	
training:	Epoch: [53][78/233]	Loss 0.2164 (0.0249)	
training:	Epoch: [53][79/233]	Loss 0.0054 (0.0247)	
training:	Epoch: [53][80/233]	Loss 0.0046 (0.0245)	
training:	Epoch: [53][81/233]	Loss 0.0038 (0.0242)	
training:	Epoch: [53][82/233]	Loss 0.0168 (0.0241)	
training:	Epoch: [53][83/233]	Loss 0.0034 (0.0239)	
training:	Epoch: [53][84/233]	Loss 0.0043 (0.0236)	
training:	Epoch: [53][85/233]	Loss 0.1874 (0.0256)	
training:	Epoch: [53][86/233]	Loss 0.1009 (0.0264)	
training:	Epoch: [53][87/233]	Loss 0.0031 (0.0262)	
training:	Epoch: [53][88/233]	Loss 0.0034 (0.0259)	
training:	Epoch: [53][89/233]	Loss 0.0053 (0.0257)	
training:	Epoch: [53][90/233]	Loss 0.0063 (0.0255)	
training:	Epoch: [53][91/233]	Loss 0.0053 (0.0252)	
training:	Epoch: [53][92/233]	Loss 0.0060 (0.0250)	
training:	Epoch: [53][93/233]	Loss 0.0032 (0.0248)	
training:	Epoch: [53][94/233]	Loss 0.0157 (0.0247)	
training:	Epoch: [53][95/233]	Loss 0.0804 (0.0253)	
training:	Epoch: [53][96/233]	Loss 0.0048 (0.0251)	
training:	Epoch: [53][97/233]	Loss 0.0175 (0.0250)	
training:	Epoch: [53][98/233]	Loss 0.0176 (0.0249)	
training:	Epoch: [53][99/233]	Loss 0.0345 (0.0250)	
training:	Epoch: [53][100/233]	Loss 0.0046 (0.0248)	
training:	Epoch: [53][101/233]	Loss 0.0040 (0.0246)	
training:	Epoch: [53][102/233]	Loss 0.0090 (0.0244)	
training:	Epoch: [53][103/233]	Loss 0.0068 (0.0243)	
training:	Epoch: [53][104/233]	Loss 0.0940 (0.0249)	
training:	Epoch: [53][105/233]	Loss 0.0034 (0.0247)	
training:	Epoch: [53][106/233]	Loss 0.0032 (0.0245)	
training:	Epoch: [53][107/233]	Loss 0.1948 (0.0261)	
training:	Epoch: [53][108/233]	Loss 0.1882 (0.0276)	
training:	Epoch: [53][109/233]	Loss 0.0765 (0.0281)	
training:	Epoch: [53][110/233]	Loss 0.0057 (0.0279)	
training:	Epoch: [53][111/233]	Loss 0.0042 (0.0277)	
training:	Epoch: [53][112/233]	Loss 0.0034 (0.0274)	
training:	Epoch: [53][113/233]	Loss 0.0057 (0.0273)	
training:	Epoch: [53][114/233]	Loss 0.0051 (0.0271)	
training:	Epoch: [53][115/233]	Loss 0.0046 (0.0269)	
training:	Epoch: [53][116/233]	Loss 0.0033 (0.0267)	
training:	Epoch: [53][117/233]	Loss 0.0032 (0.0265)	
training:	Epoch: [53][118/233]	Loss 0.0031 (0.0263)	
training:	Epoch: [53][119/233]	Loss 0.0124 (0.0261)	
training:	Epoch: [53][120/233]	Loss 0.1267 (0.0270)	
training:	Epoch: [53][121/233]	Loss 0.0035 (0.0268)	
training:	Epoch: [53][122/233]	Loss 0.0142 (0.0267)	
training:	Epoch: [53][123/233]	Loss 0.0115 (0.0266)	
training:	Epoch: [53][124/233]	Loss 0.0037 (0.0264)	
training:	Epoch: [53][125/233]	Loss 0.0085 (0.0262)	
training:	Epoch: [53][126/233]	Loss 0.0037 (0.0261)	
training:	Epoch: [53][127/233]	Loss 0.0049 (0.0259)	
training:	Epoch: [53][128/233]	Loss 0.1290 (0.0267)	
training:	Epoch: [53][129/233]	Loss 0.0031 (0.0265)	
training:	Epoch: [53][130/233]	Loss 0.0035 (0.0263)	
training:	Epoch: [53][131/233]	Loss 0.0029 (0.0262)	
training:	Epoch: [53][132/233]	Loss 0.0049 (0.0260)	
training:	Epoch: [53][133/233]	Loss 0.0030 (0.0258)	
training:	Epoch: [53][134/233]	Loss 0.0117 (0.0257)	
training:	Epoch: [53][135/233]	Loss 0.0046 (0.0256)	
training:	Epoch: [53][136/233]	Loss 0.0064 (0.0254)	
training:	Epoch: [53][137/233]	Loss 0.0040 (0.0253)	
training:	Epoch: [53][138/233]	Loss 0.0039 (0.0251)	
training:	Epoch: [53][139/233]	Loss 0.0030 (0.0249)	
training:	Epoch: [53][140/233]	Loss 0.0041 (0.0248)	
training:	Epoch: [53][141/233]	Loss 0.0075 (0.0247)	
training:	Epoch: [53][142/233]	Loss 0.0033 (0.0245)	
training:	Epoch: [53][143/233]	Loss 0.0057 (0.0244)	
training:	Epoch: [53][144/233]	Loss 0.0042 (0.0243)	
training:	Epoch: [53][145/233]	Loss 0.0039 (0.0241)	
training:	Epoch: [53][146/233]	Loss 0.0033 (0.0240)	
training:	Epoch: [53][147/233]	Loss 0.0067 (0.0239)	
training:	Epoch: [53][148/233]	Loss 0.0034 (0.0237)	
training:	Epoch: [53][149/233]	Loss 0.0030 (0.0236)	
training:	Epoch: [53][150/233]	Loss 0.0051 (0.0235)	
training:	Epoch: [53][151/233]	Loss 0.0031 (0.0233)	
training:	Epoch: [53][152/233]	Loss 0.0805 (0.0237)	
training:	Epoch: [53][153/233]	Loss 0.0050 (0.0236)	
training:	Epoch: [53][154/233]	Loss 0.0038 (0.0234)	
training:	Epoch: [53][155/233]	Loss 0.0046 (0.0233)	
training:	Epoch: [53][156/233]	Loss 0.0029 (0.0232)	
training:	Epoch: [53][157/233]	Loss 0.0038 (0.0231)	
training:	Epoch: [53][158/233]	Loss 0.0047 (0.0230)	
training:	Epoch: [53][159/233]	Loss 0.0157 (0.0229)	
training:	Epoch: [53][160/233]	Loss 0.0028 (0.0228)	
training:	Epoch: [53][161/233]	Loss 0.0035 (0.0227)	
training:	Epoch: [53][162/233]	Loss 0.0057 (0.0226)	
training:	Epoch: [53][163/233]	Loss 0.0083 (0.0225)	
training:	Epoch: [53][164/233]	Loss 0.0031 (0.0224)	
training:	Epoch: [53][165/233]	Loss 0.0053 (0.0222)	
training:	Epoch: [53][166/233]	Loss 0.0037 (0.0221)	
training:	Epoch: [53][167/233]	Loss 0.0034 (0.0220)	
training:	Epoch: [53][168/233]	Loss 0.0035 (0.0219)	
training:	Epoch: [53][169/233]	Loss 0.0048 (0.0218)	
training:	Epoch: [53][170/233]	Loss 0.0117 (0.0218)	
training:	Epoch: [53][171/233]	Loss 0.0032 (0.0216)	
training:	Epoch: [53][172/233]	Loss 0.0035 (0.0215)	
training:	Epoch: [53][173/233]	Loss 0.0045 (0.0214)	
training:	Epoch: [53][174/233]	Loss 0.0028 (0.0213)	
training:	Epoch: [53][175/233]	Loss 0.1824 (0.0223)	
training:	Epoch: [53][176/233]	Loss 0.0039 (0.0222)	
training:	Epoch: [53][177/233]	Loss 0.0033 (0.0220)	
training:	Epoch: [53][178/233]	Loss 0.0226 (0.0220)	
training:	Epoch: [53][179/233]	Loss 0.0042 (0.0219)	
training:	Epoch: [53][180/233]	Loss 0.0037 (0.0218)	
training:	Epoch: [53][181/233]	Loss 0.0042 (0.0217)	
training:	Epoch: [53][182/233]	Loss 0.0028 (0.0216)	
training:	Epoch: [53][183/233]	Loss 0.1070 (0.0221)	
training:	Epoch: [53][184/233]	Loss 0.0053 (0.0220)	
training:	Epoch: [53][185/233]	Loss 0.0064 (0.0219)	
training:	Epoch: [53][186/233]	Loss 0.0220 (0.0219)	
training:	Epoch: [53][187/233]	Loss 0.0826 (0.0223)	
training:	Epoch: [53][188/233]	Loss 0.0039 (0.0222)	
training:	Epoch: [53][189/233]	Loss 0.0075 (0.0221)	
training:	Epoch: [53][190/233]	Loss 0.0088 (0.0220)	
training:	Epoch: [53][191/233]	Loss 0.0043 (0.0219)	
training:	Epoch: [53][192/233]	Loss 0.0083 (0.0219)	
training:	Epoch: [53][193/233]	Loss 0.0106 (0.0218)	
training:	Epoch: [53][194/233]	Loss 0.0038 (0.0217)	
training:	Epoch: [53][195/233]	Loss 0.1475 (0.0223)	
training:	Epoch: [53][196/233]	Loss 0.0035 (0.0222)	
training:	Epoch: [53][197/233]	Loss 0.0083 (0.0222)	
training:	Epoch: [53][198/233]	Loss 0.0722 (0.0224)	
training:	Epoch: [53][199/233]	Loss 0.0043 (0.0223)	
training:	Epoch: [53][200/233]	Loss 0.0031 (0.0222)	
training:	Epoch: [53][201/233]	Loss 0.0106 (0.0222)	
training:	Epoch: [53][202/233]	Loss 0.1946 (0.0230)	
training:	Epoch: [53][203/233]	Loss 0.0042 (0.0229)	
training:	Epoch: [53][204/233]	Loss 0.0036 (0.0229)	
training:	Epoch: [53][205/233]	Loss 0.0166 (0.0228)	
training:	Epoch: [53][206/233]	Loss 0.0577 (0.0230)	
training:	Epoch: [53][207/233]	Loss 0.0038 (0.0229)	
training:	Epoch: [53][208/233]	Loss 0.0041 (0.0228)	
training:	Epoch: [53][209/233]	Loss 0.0067 (0.0227)	
training:	Epoch: [53][210/233]	Loss 0.0032 (0.0226)	
training:	Epoch: [53][211/233]	Loss 0.0069 (0.0226)	
training:	Epoch: [53][212/233]	Loss 0.0038 (0.0225)	
training:	Epoch: [53][213/233]	Loss 0.0034 (0.0224)	
training:	Epoch: [53][214/233]	Loss 0.0041 (0.0223)	
training:	Epoch: [53][215/233]	Loss 0.0073 (0.0222)	
training:	Epoch: [53][216/233]	Loss 0.0321 (0.0223)	
training:	Epoch: [53][217/233]	Loss 0.0105 (0.0222)	
training:	Epoch: [53][218/233]	Loss 0.0500 (0.0223)	
training:	Epoch: [53][219/233]	Loss 0.1495 (0.0229)	
training:	Epoch: [53][220/233]	Loss 0.0042 (0.0228)	
training:	Epoch: [53][221/233]	Loss 0.2848 (0.0240)	
training:	Epoch: [53][222/233]	Loss 0.0028 (0.0239)	
training:	Epoch: [53][223/233]	Loss 0.0079 (0.0239)	
training:	Epoch: [53][224/233]	Loss 0.0032 (0.0238)	
training:	Epoch: [53][225/233]	Loss 0.0035 (0.0237)	
training:	Epoch: [53][226/233]	Loss 0.0320 (0.0237)	
training:	Epoch: [53][227/233]	Loss 0.0379 (0.0238)	
training:	Epoch: [53][228/233]	Loss 0.0274 (0.0238)	
training:	Epoch: [53][229/233]	Loss 0.0465 (0.0239)	
training:	Epoch: [53][230/233]	Loss 0.0042 (0.0238)	
training:	Epoch: [53][231/233]	Loss 0.0086 (0.0237)	
training:	Epoch: [53][232/233]	Loss 0.0049 (0.0237)	
training:	Epoch: [53][233/233]	Loss 0.0052 (0.0236)	
Training:	 Loss: 0.0235

Training:	 ACC: 0.9980 0.9980 0.9974 0.9986
Validation:	 ACC: 0.7875 0.7849 0.7334 0.8416
Validation:	 Best_BACC: 0.8162 0.8154 0.7998 0.8326
Validation:	 Loss: 0.9603
Pretraining:	Epoch 54/200
----------
training:	Epoch: [54][1/233]	Loss 0.0036 (0.0036)	
training:	Epoch: [54][2/233]	Loss 0.0048 (0.0042)	
training:	Epoch: [54][3/233]	Loss 0.0218 (0.0100)	
training:	Epoch: [54][4/233]	Loss 0.0115 (0.0104)	
training:	Epoch: [54][5/233]	Loss 0.1694 (0.0422)	
training:	Epoch: [54][6/233]	Loss 0.0140 (0.0375)	
training:	Epoch: [54][7/233]	Loss 0.1660 (0.0559)	
training:	Epoch: [54][8/233]	Loss 0.0070 (0.0498)	
training:	Epoch: [54][9/233]	Loss 0.0032 (0.0446)	
training:	Epoch: [54][10/233]	Loss 0.0183 (0.0420)	
training:	Epoch: [54][11/233]	Loss 0.0051 (0.0386)	
training:	Epoch: [54][12/233]	Loss 0.0060 (0.0359)	
training:	Epoch: [54][13/233]	Loss 0.0043 (0.0335)	
training:	Epoch: [54][14/233]	Loss 0.0112 (0.0319)	
training:	Epoch: [54][15/233]	Loss 0.0034 (0.0300)	
training:	Epoch: [54][16/233]	Loss 0.0030 (0.0283)	
training:	Epoch: [54][17/233]	Loss 0.0074 (0.0271)	
training:	Epoch: [54][18/233]	Loss 0.0046 (0.0258)	
training:	Epoch: [54][19/233]	Loss 0.0037 (0.0246)	
training:	Epoch: [54][20/233]	Loss 0.0041 (0.0236)	
training:	Epoch: [54][21/233]	Loss 0.0040 (0.0227)	
training:	Epoch: [54][22/233]	Loss 0.0032 (0.0218)	
training:	Epoch: [54][23/233]	Loss 0.0038 (0.0210)	
training:	Epoch: [54][24/233]	Loss 0.0054 (0.0204)	
training:	Epoch: [54][25/233]	Loss 0.0040 (0.0197)	
training:	Epoch: [54][26/233]	Loss 0.0031 (0.0191)	
training:	Epoch: [54][27/233]	Loss 0.0035 (0.0185)	
training:	Epoch: [54][28/233]	Loss 0.0037 (0.0180)	
training:	Epoch: [54][29/233]	Loss 0.0040 (0.0175)	
training:	Epoch: [54][30/233]	Loss 0.0043 (0.0170)	
training:	Epoch: [54][31/233]	Loss 0.0028 (0.0166)	
training:	Epoch: [54][32/233]	Loss 0.0141 (0.0165)	
training:	Epoch: [54][33/233]	Loss 0.0068 (0.0162)	
training:	Epoch: [54][34/233]	Loss 0.0054 (0.0159)	
training:	Epoch: [54][35/233]	Loss 0.1442 (0.0196)	
training:	Epoch: [54][36/233]	Loss 0.0653 (0.0208)	
training:	Epoch: [54][37/233]	Loss 0.0051 (0.0204)	
training:	Epoch: [54][38/233]	Loss 0.0029 (0.0199)	
training:	Epoch: [54][39/233]	Loss 0.0820 (0.0215)	
training:	Epoch: [54][40/233]	Loss 0.1170 (0.0239)	
training:	Epoch: [54][41/233]	Loss 0.0076 (0.0235)	
training:	Epoch: [54][42/233]	Loss 0.0046 (0.0231)	
training:	Epoch: [54][43/233]	Loss 0.0190 (0.0230)	
training:	Epoch: [54][44/233]	Loss 0.0036 (0.0225)	
training:	Epoch: [54][45/233]	Loss 0.0045 (0.0221)	
training:	Epoch: [54][46/233]	Loss 0.0050 (0.0218)	
training:	Epoch: [54][47/233]	Loss 0.0030 (0.0214)	
training:	Epoch: [54][48/233]	Loss 0.0040 (0.0210)	
training:	Epoch: [54][49/233]	Loss 0.0055 (0.0207)	
training:	Epoch: [54][50/233]	Loss 0.0047 (0.0204)	
training:	Epoch: [54][51/233]	Loss 0.0804 (0.0215)	
training:	Epoch: [54][52/233]	Loss 0.0039 (0.0212)	
training:	Epoch: [54][53/233]	Loss 0.0033 (0.0209)	
training:	Epoch: [54][54/233]	Loss 0.0031 (0.0205)	
training:	Epoch: [54][55/233]	Loss 0.0035 (0.0202)	
training:	Epoch: [54][56/233]	Loss 0.0090 (0.0200)	
training:	Epoch: [54][57/233]	Loss 0.0221 (0.0201)	
training:	Epoch: [54][58/233]	Loss 0.0077 (0.0198)	
training:	Epoch: [54][59/233]	Loss 0.0235 (0.0199)	
training:	Epoch: [54][60/233]	Loss 0.0033 (0.0196)	
training:	Epoch: [54][61/233]	Loss 0.0033 (0.0194)	
training:	Epoch: [54][62/233]	Loss 0.0043 (0.0191)	
training:	Epoch: [54][63/233]	Loss 0.0035 (0.0189)	
training:	Epoch: [54][64/233]	Loss 0.0038 (0.0186)	
training:	Epoch: [54][65/233]	Loss 0.0173 (0.0186)	
training:	Epoch: [54][66/233]	Loss 0.1560 (0.0207)	
training:	Epoch: [54][67/233]	Loss 0.0055 (0.0205)	
training:	Epoch: [54][68/233]	Loss 0.0034 (0.0202)	
training:	Epoch: [54][69/233]	Loss 0.0077 (0.0200)	
training:	Epoch: [54][70/233]	Loss 0.0031 (0.0198)	
training:	Epoch: [54][71/233]	Loss 0.1720 (0.0219)	
training:	Epoch: [54][72/233]	Loss 0.0190 (0.0219)	
training:	Epoch: [54][73/233]	Loss 0.0036 (0.0217)	
training:	Epoch: [54][74/233]	Loss 0.0054 (0.0214)	
training:	Epoch: [54][75/233]	Loss 0.0032 (0.0212)	
training:	Epoch: [54][76/233]	Loss 0.0046 (0.0210)	
training:	Epoch: [54][77/233]	Loss 0.0030 (0.0207)	
training:	Epoch: [54][78/233]	Loss 0.0030 (0.0205)	
training:	Epoch: [54][79/233]	Loss 0.0087 (0.0204)	
training:	Epoch: [54][80/233]	Loss 0.0047 (0.0202)	
training:	Epoch: [54][81/233]	Loss 0.0046 (0.0200)	
training:	Epoch: [54][82/233]	Loss 0.0072 (0.0198)	
training:	Epoch: [54][83/233]	Loss 0.0056 (0.0196)	
training:	Epoch: [54][84/233]	Loss 0.3761 (0.0239)	
training:	Epoch: [54][85/233]	Loss 0.0035 (0.0236)	
training:	Epoch: [54][86/233]	Loss 0.0756 (0.0243)	
training:	Epoch: [54][87/233]	Loss 0.0035 (0.0240)	
training:	Epoch: [54][88/233]	Loss 0.0048 (0.0238)	
training:	Epoch: [54][89/233]	Loss 0.0112 (0.0237)	
training:	Epoch: [54][90/233]	Loss 0.1094 (0.0246)	
training:	Epoch: [54][91/233]	Loss 0.0036 (0.0244)	
training:	Epoch: [54][92/233]	Loss 0.0053 (0.0242)	
training:	Epoch: [54][93/233]	Loss 0.0034 (0.0239)	
training:	Epoch: [54][94/233]	Loss 0.0031 (0.0237)	
training:	Epoch: [54][95/233]	Loss 0.0082 (0.0236)	
training:	Epoch: [54][96/233]	Loss 0.0321 (0.0236)	
training:	Epoch: [54][97/233]	Loss 0.0236 (0.0236)	
training:	Epoch: [54][98/233]	Loss 0.0043 (0.0235)	
training:	Epoch: [54][99/233]	Loss 0.0109 (0.0233)	
training:	Epoch: [54][100/233]	Loss 0.1874 (0.0250)	
training:	Epoch: [54][101/233]	Loss 0.0029 (0.0247)	
training:	Epoch: [54][102/233]	Loss 0.0309 (0.0248)	
training:	Epoch: [54][103/233]	Loss 0.0318 (0.0249)	
training:	Epoch: [54][104/233]	Loss 0.0082 (0.0247)	
training:	Epoch: [54][105/233]	Loss 0.0081 (0.0246)	
training:	Epoch: [54][106/233]	Loss 0.0034 (0.0244)	
training:	Epoch: [54][107/233]	Loss 0.0614 (0.0247)	
training:	Epoch: [54][108/233]	Loss 0.0249 (0.0247)	
training:	Epoch: [54][109/233]	Loss 0.0141 (0.0246)	
training:	Epoch: [54][110/233]	Loss 0.0035 (0.0244)	
training:	Epoch: [54][111/233]	Loss 0.0033 (0.0242)	
training:	Epoch: [54][112/233]	Loss 0.0076 (0.0241)	
training:	Epoch: [54][113/233]	Loss 0.0031 (0.0239)	
training:	Epoch: [54][114/233]	Loss 0.1490 (0.0250)	
training:	Epoch: [54][115/233]	Loss 0.0033 (0.0248)	
training:	Epoch: [54][116/233]	Loss 0.0035 (0.0246)	
training:	Epoch: [54][117/233]	Loss 0.0028 (0.0244)	
training:	Epoch: [54][118/233]	Loss 0.0038 (0.0243)	
training:	Epoch: [54][119/233]	Loss 0.0034 (0.0241)	
training:	Epoch: [54][120/233]	Loss 0.0042 (0.0239)	
training:	Epoch: [54][121/233]	Loss 0.0036 (0.0237)	
training:	Epoch: [54][122/233]	Loss 0.0041 (0.0236)	
training:	Epoch: [54][123/233]	Loss 0.0047 (0.0234)	
training:	Epoch: [54][124/233]	Loss 0.0039 (0.0233)	
training:	Epoch: [54][125/233]	Loss 0.0030 (0.0231)	
training:	Epoch: [54][126/233]	Loss 0.0698 (0.0235)	
training:	Epoch: [54][127/233]	Loss 0.0229 (0.0235)	
training:	Epoch: [54][128/233]	Loss 0.0030 (0.0233)	
training:	Epoch: [54][129/233]	Loss 0.0067 (0.0232)	
training:	Epoch: [54][130/233]	Loss 0.1824 (0.0244)	
training:	Epoch: [54][131/233]	Loss 0.0046 (0.0243)	
training:	Epoch: [54][132/233]	Loss 0.1854 (0.0255)	
training:	Epoch: [54][133/233]	Loss 0.0036 (0.0253)	
training:	Epoch: [54][134/233]	Loss 0.0157 (0.0252)	
training:	Epoch: [54][135/233]	Loss 0.0068 (0.0251)	
training:	Epoch: [54][136/233]	Loss 0.0028 (0.0249)	
training:	Epoch: [54][137/233]	Loss 0.0055 (0.0248)	
training:	Epoch: [54][138/233]	Loss 0.0213 (0.0248)	
training:	Epoch: [54][139/233]	Loss 0.0030 (0.0246)	
training:	Epoch: [54][140/233]	Loss 0.0035 (0.0245)	
training:	Epoch: [54][141/233]	Loss 0.1153 (0.0251)	
training:	Epoch: [54][142/233]	Loss 0.0104 (0.0250)	
training:	Epoch: [54][143/233]	Loss 0.0046 (0.0249)	
training:	Epoch: [54][144/233]	Loss 0.0168 (0.0248)	
training:	Epoch: [54][145/233]	Loss 0.0036 (0.0247)	
training:	Epoch: [54][146/233]	Loss 0.0033 (0.0245)	
training:	Epoch: [54][147/233]	Loss 0.0150 (0.0245)	
training:	Epoch: [54][148/233]	Loss 0.0038 (0.0243)	
training:	Epoch: [54][149/233]	Loss 0.0047 (0.0242)	
training:	Epoch: [54][150/233]	Loss 0.0030 (0.0240)	
training:	Epoch: [54][151/233]	Loss 0.0055 (0.0239)	
training:	Epoch: [54][152/233]	Loss 0.1917 (0.0250)	
training:	Epoch: [54][153/233]	Loss 0.0053 (0.0249)	
training:	Epoch: [54][154/233]	Loss 0.0236 (0.0249)	
training:	Epoch: [54][155/233]	Loss 0.0067 (0.0248)	
training:	Epoch: [54][156/233]	Loss 0.0031 (0.0246)	
training:	Epoch: [54][157/233]	Loss 0.2014 (0.0258)	
training:	Epoch: [54][158/233]	Loss 0.0211 (0.0257)	
training:	Epoch: [54][159/233]	Loss 0.0316 (0.0258)	
training:	Epoch: [54][160/233]	Loss 0.0398 (0.0259)	
training:	Epoch: [54][161/233]	Loss 0.0029 (0.0257)	
training:	Epoch: [54][162/233]	Loss 0.0067 (0.0256)	
training:	Epoch: [54][163/233]	Loss 0.0047 (0.0255)	
training:	Epoch: [54][164/233]	Loss 0.0163 (0.0254)	
training:	Epoch: [54][165/233]	Loss 0.0064 (0.0253)	
training:	Epoch: [54][166/233]	Loss 0.0030 (0.0252)	
training:	Epoch: [54][167/233]	Loss 0.0055 (0.0250)	
training:	Epoch: [54][168/233]	Loss 0.1323 (0.0257)	
training:	Epoch: [54][169/233]	Loss 0.0052 (0.0256)	
training:	Epoch: [54][170/233]	Loss 0.0044 (0.0254)	
training:	Epoch: [54][171/233]	Loss 0.0039 (0.0253)	
training:	Epoch: [54][172/233]	Loss 0.0042 (0.0252)	
training:	Epoch: [54][173/233]	Loss 0.0032 (0.0251)	
training:	Epoch: [54][174/233]	Loss 0.0034 (0.0249)	
training:	Epoch: [54][175/233]	Loss 0.0048 (0.0248)	
training:	Epoch: [54][176/233]	Loss 0.0037 (0.0247)	
training:	Epoch: [54][177/233]	Loss 0.0042 (0.0246)	
training:	Epoch: [54][178/233]	Loss 0.0031 (0.0245)	
training:	Epoch: [54][179/233]	Loss 0.0037 (0.0243)	
training:	Epoch: [54][180/233]	Loss 0.0036 (0.0242)	
training:	Epoch: [54][181/233]	Loss 0.0056 (0.0241)	
training:	Epoch: [54][182/233]	Loss 0.0033 (0.0240)	
training:	Epoch: [54][183/233]	Loss 0.0034 (0.0239)	
training:	Epoch: [54][184/233]	Loss 0.0033 (0.0238)	
training:	Epoch: [54][185/233]	Loss 0.0518 (0.0239)	
training:	Epoch: [54][186/233]	Loss 0.0220 (0.0239)	
training:	Epoch: [54][187/233]	Loss 0.0204 (0.0239)	
training:	Epoch: [54][188/233]	Loss 0.0198 (0.0239)	
training:	Epoch: [54][189/233]	Loss 0.0036 (0.0238)	
training:	Epoch: [54][190/233]	Loss 0.0046 (0.0237)	
training:	Epoch: [54][191/233]	Loss 0.0037 (0.0236)	
training:	Epoch: [54][192/233]	Loss 0.0041 (0.0235)	
training:	Epoch: [54][193/233]	Loss 0.0057 (0.0234)	
training:	Epoch: [54][194/233]	Loss 0.0033 (0.0233)	
training:	Epoch: [54][195/233]	Loss 0.1473 (0.0239)	
training:	Epoch: [54][196/233]	Loss 0.0032 (0.0238)	
training:	Epoch: [54][197/233]	Loss 0.0060 (0.0237)	
training:	Epoch: [54][198/233]	Loss 0.1883 (0.0245)	
training:	Epoch: [54][199/233]	Loss 0.0039 (0.0244)	
training:	Epoch: [54][200/233]	Loss 0.0030 (0.0243)	
training:	Epoch: [54][201/233]	Loss 0.0031 (0.0242)	
training:	Epoch: [54][202/233]	Loss 0.0045 (0.0241)	
training:	Epoch: [54][203/233]	Loss 0.0083 (0.0241)	
training:	Epoch: [54][204/233]	Loss 0.0089 (0.0240)	
training:	Epoch: [54][205/233]	Loss 0.0136 (0.0239)	
training:	Epoch: [54][206/233]	Loss 0.0058 (0.0238)	
training:	Epoch: [54][207/233]	Loss 0.0072 (0.0238)	
training:	Epoch: [54][208/233]	Loss 0.0207 (0.0237)	
training:	Epoch: [54][209/233]	Loss 0.0047 (0.0237)	
training:	Epoch: [54][210/233]	Loss 0.0090 (0.0236)	
training:	Epoch: [54][211/233]	Loss 0.0188 (0.0236)	
training:	Epoch: [54][212/233]	Loss 0.0041 (0.0235)	
training:	Epoch: [54][213/233]	Loss 0.0034 (0.0234)	
training:	Epoch: [54][214/233]	Loss 0.0061 (0.0233)	
training:	Epoch: [54][215/233]	Loss 0.0048 (0.0232)	
training:	Epoch: [54][216/233]	Loss 0.0029 (0.0231)	
training:	Epoch: [54][217/233]	Loss 0.0085 (0.0231)	
training:	Epoch: [54][218/233]	Loss 0.0036 (0.0230)	
training:	Epoch: [54][219/233]	Loss 0.0076 (0.0229)	
training:	Epoch: [54][220/233]	Loss 0.0036 (0.0228)	
training:	Epoch: [54][221/233]	Loss 0.0530 (0.0229)	
training:	Epoch: [54][222/233]	Loss 0.0145 (0.0229)	
training:	Epoch: [54][223/233]	Loss 0.0180 (0.0229)	
training:	Epoch: [54][224/233]	Loss 0.0055 (0.0228)	
training:	Epoch: [54][225/233]	Loss 0.0034 (0.0227)	
training:	Epoch: [54][226/233]	Loss 0.0028 (0.0226)	
training:	Epoch: [54][227/233]	Loss 0.0073 (0.0226)	
training:	Epoch: [54][228/233]	Loss 0.0039 (0.0225)	
training:	Epoch: [54][229/233]	Loss 0.0035 (0.0224)	
training:	Epoch: [54][230/233]	Loss 0.0202 (0.0224)	
training:	Epoch: [54][231/233]	Loss 0.0034 (0.0223)	
training:	Epoch: [54][232/233]	Loss 0.0031 (0.0222)	
training:	Epoch: [54][233/233]	Loss 0.0031 (0.0221)	
Training:	 Loss: 0.0221

Training:	 ACC: 0.9988 0.9988 0.9990 0.9986
Validation:	 ACC: 0.7942 0.7951 0.8121 0.7764
Validation:	 Best_BACC: 0.8162 0.8154 0.7998 0.8326
Validation:	 Loss: 0.9126
Pretraining:	Epoch 55/200
----------
training:	Epoch: [55][1/233]	Loss 0.0027 (0.0027)	
training:	Epoch: [55][2/233]	Loss 0.0063 (0.0045)	
training:	Epoch: [55][3/233]	Loss 0.0314 (0.0135)	
training:	Epoch: [55][4/233]	Loss 0.0377 (0.0195)	
training:	Epoch: [55][5/233]	Loss 0.0060 (0.0168)	
training:	Epoch: [55][6/233]	Loss 0.0030 (0.0145)	
training:	Epoch: [55][7/233]	Loss 0.0039 (0.0130)	
training:	Epoch: [55][8/233]	Loss 0.0059 (0.0121)	
training:	Epoch: [55][9/233]	Loss 0.0672 (0.0182)	
training:	Epoch: [55][10/233]	Loss 0.0037 (0.0168)	
training:	Epoch: [55][11/233]	Loss 0.0068 (0.0159)	
training:	Epoch: [55][12/233]	Loss 0.0036 (0.0148)	
training:	Epoch: [55][13/233]	Loss 0.0031 (0.0139)	
training:	Epoch: [55][14/233]	Loss 0.0040 (0.0132)	
training:	Epoch: [55][15/233]	Loss 0.0028 (0.0125)	
training:	Epoch: [55][16/233]	Loss 0.0035 (0.0120)	
training:	Epoch: [55][17/233]	Loss 0.0135 (0.0121)	
training:	Epoch: [55][18/233]	Loss 0.0065 (0.0117)	
training:	Epoch: [55][19/233]	Loss 0.0040 (0.0113)	
training:	Epoch: [55][20/233]	Loss 0.0029 (0.0109)	
training:	Epoch: [55][21/233]	Loss 0.0043 (0.0106)	
training:	Epoch: [55][22/233]	Loss 0.0113 (0.0106)	
training:	Epoch: [55][23/233]	Loss 0.0030 (0.0103)	
training:	Epoch: [55][24/233]	Loss 0.0039 (0.0100)	
training:	Epoch: [55][25/233]	Loss 0.0030 (0.0098)	
training:	Epoch: [55][26/233]	Loss 0.0036 (0.0095)	
training:	Epoch: [55][27/233]	Loss 0.0032 (0.0093)	
training:	Epoch: [55][28/233]	Loss 0.0051 (0.0091)	
training:	Epoch: [55][29/233]	Loss 0.0036 (0.0089)	
training:	Epoch: [55][30/233]	Loss 0.0036 (0.0088)	
training:	Epoch: [55][31/233]	Loss 0.0041 (0.0086)	
training:	Epoch: [55][32/233]	Loss 0.0025 (0.0084)	
training:	Epoch: [55][33/233]	Loss 0.0032 (0.0083)	
training:	Epoch: [55][34/233]	Loss 0.0031 (0.0081)	
training:	Epoch: [55][35/233]	Loss 0.0045 (0.0080)	
training:	Epoch: [55][36/233]	Loss 0.0039 (0.0079)	
training:	Epoch: [55][37/233]	Loss 0.0034 (0.0078)	
training:	Epoch: [55][38/233]	Loss 0.0029 (0.0076)	
training:	Epoch: [55][39/233]	Loss 0.1832 (0.0122)	
training:	Epoch: [55][40/233]	Loss 0.0039 (0.0119)	
training:	Epoch: [55][41/233]	Loss 0.0044 (0.0118)	
training:	Epoch: [55][42/233]	Loss 0.0076 (0.0117)	
training:	Epoch: [55][43/233]	Loss 0.0028 (0.0115)	
training:	Epoch: [55][44/233]	Loss 0.0041 (0.0113)	
training:	Epoch: [55][45/233]	Loss 0.0035 (0.0111)	
training:	Epoch: [55][46/233]	Loss 0.0027 (0.0109)	
training:	Epoch: [55][47/233]	Loss 0.0115 (0.0109)	
training:	Epoch: [55][48/233]	Loss 0.0699 (0.0122)	
training:	Epoch: [55][49/233]	Loss 0.0203 (0.0123)	
training:	Epoch: [55][50/233]	Loss 0.0320 (0.0127)	
training:	Epoch: [55][51/233]	Loss 0.0040 (0.0126)	
training:	Epoch: [55][52/233]	Loss 0.0043 (0.0124)	
training:	Epoch: [55][53/233]	Loss 0.0468 (0.0131)	
training:	Epoch: [55][54/233]	Loss 0.0036 (0.0129)	
training:	Epoch: [55][55/233]	Loss 0.0032 (0.0127)	
training:	Epoch: [55][56/233]	Loss 0.0286 (0.0130)	
training:	Epoch: [55][57/233]	Loss 0.0051 (0.0128)	
training:	Epoch: [55][58/233]	Loss 0.0038 (0.0127)	
training:	Epoch: [55][59/233]	Loss 0.0036 (0.0125)	
training:	Epoch: [55][60/233]	Loss 0.0028 (0.0124)	
training:	Epoch: [55][61/233]	Loss 0.0027 (0.0122)	
training:	Epoch: [55][62/233]	Loss 0.0751 (0.0132)	
training:	Epoch: [55][63/233]	Loss 0.0052 (0.0131)	
training:	Epoch: [55][64/233]	Loss 0.0028 (0.0129)	
training:	Epoch: [55][65/233]	Loss 0.0043 (0.0128)	
training:	Epoch: [55][66/233]	Loss 0.0144 (0.0128)	
training:	Epoch: [55][67/233]	Loss 0.0036 (0.0127)	
training:	Epoch: [55][68/233]	Loss 0.0034 (0.0126)	
training:	Epoch: [55][69/233]	Loss 0.1858 (0.0151)	
training:	Epoch: [55][70/233]	Loss 0.0038 (0.0149)	
training:	Epoch: [55][71/233]	Loss 0.0211 (0.0150)	
training:	Epoch: [55][72/233]	Loss 0.0087 (0.0149)	
training:	Epoch: [55][73/233]	Loss 0.0230 (0.0150)	
training:	Epoch: [55][74/233]	Loss 0.0028 (0.0149)	
training:	Epoch: [55][75/233]	Loss 0.0039 (0.0147)	
training:	Epoch: [55][76/233]	Loss 0.0054 (0.0146)	
training:	Epoch: [55][77/233]	Loss 0.0036 (0.0144)	
training:	Epoch: [55][78/233]	Loss 0.0954 (0.0155)	
training:	Epoch: [55][79/233]	Loss 0.0055 (0.0154)	
training:	Epoch: [55][80/233]	Loss 0.0586 (0.0159)	
training:	Epoch: [55][81/233]	Loss 0.0036 (0.0157)	
training:	Epoch: [55][82/233]	Loss 0.0035 (0.0156)	
training:	Epoch: [55][83/233]	Loss 0.0034 (0.0154)	
training:	Epoch: [55][84/233]	Loss 0.0077 (0.0154)	
training:	Epoch: [55][85/233]	Loss 0.0028 (0.0152)	
training:	Epoch: [55][86/233]	Loss 0.1845 (0.0172)	
training:	Epoch: [55][87/233]	Loss 0.0034 (0.0170)	
training:	Epoch: [55][88/233]	Loss 0.0029 (0.0169)	
training:	Epoch: [55][89/233]	Loss 0.0039 (0.0167)	
training:	Epoch: [55][90/233]	Loss 0.0176 (0.0167)	
training:	Epoch: [55][91/233]	Loss 0.0887 (0.0175)	
training:	Epoch: [55][92/233]	Loss 0.0181 (0.0175)	
training:	Epoch: [55][93/233]	Loss 0.0098 (0.0174)	
training:	Epoch: [55][94/233]	Loss 0.0037 (0.0173)	
training:	Epoch: [55][95/233]	Loss 0.0031 (0.0171)	
training:	Epoch: [55][96/233]	Loss 0.0053 (0.0170)	
training:	Epoch: [55][97/233]	Loss 0.1876 (0.0188)	
training:	Epoch: [55][98/233]	Loss 0.0107 (0.0187)	
training:	Epoch: [55][99/233]	Loss 0.0031 (0.0185)	
training:	Epoch: [55][100/233]	Loss 0.0037 (0.0184)	
training:	Epoch: [55][101/233]	Loss 0.0041 (0.0182)	
training:	Epoch: [55][102/233]	Loss 0.0046 (0.0181)	
training:	Epoch: [55][103/233]	Loss 0.0028 (0.0180)	
training:	Epoch: [55][104/233]	Loss 0.0063 (0.0179)	
training:	Epoch: [55][105/233]	Loss 0.0038 (0.0177)	
training:	Epoch: [55][106/233]	Loss 0.0061 (0.0176)	
training:	Epoch: [55][107/233]	Loss 0.0043 (0.0175)	
training:	Epoch: [55][108/233]	Loss 0.0033 (0.0174)	
training:	Epoch: [55][109/233]	Loss 0.0029 (0.0172)	
training:	Epoch: [55][110/233]	Loss 0.0030 (0.0171)	
training:	Epoch: [55][111/233]	Loss 0.0027 (0.0170)	
training:	Epoch: [55][112/233]	Loss 0.0028 (0.0168)	
training:	Epoch: [55][113/233]	Loss 0.0056 (0.0167)	
training:	Epoch: [55][114/233]	Loss 0.1884 (0.0182)	
training:	Epoch: [55][115/233]	Loss 0.0032 (0.0181)	
training:	Epoch: [55][116/233]	Loss 0.0039 (0.0180)	
training:	Epoch: [55][117/233]	Loss 0.0030 (0.0179)	
training:	Epoch: [55][118/233]	Loss 0.0161 (0.0178)	
training:	Epoch: [55][119/233]	Loss 0.0032 (0.0177)	
training:	Epoch: [55][120/233]	Loss 0.0139 (0.0177)	
training:	Epoch: [55][121/233]	Loss 0.0028 (0.0176)	
training:	Epoch: [55][122/233]	Loss 0.1968 (0.0190)	
training:	Epoch: [55][123/233]	Loss 0.0029 (0.0189)	
training:	Epoch: [55][124/233]	Loss 0.1447 (0.0199)	
training:	Epoch: [55][125/233]	Loss 0.0107 (0.0198)	
training:	Epoch: [55][126/233]	Loss 0.0304 (0.0199)	
training:	Epoch: [55][127/233]	Loss 0.0249 (0.0200)	
training:	Epoch: [55][128/233]	Loss 0.0088 (0.0199)	
training:	Epoch: [55][129/233]	Loss 0.0059 (0.0198)	
training:	Epoch: [55][130/233]	Loss 0.0030 (0.0196)	
training:	Epoch: [55][131/233]	Loss 0.0095 (0.0196)	
training:	Epoch: [55][132/233]	Loss 0.0033 (0.0194)	
training:	Epoch: [55][133/233]	Loss 0.0040 (0.0193)	
training:	Epoch: [55][134/233]	Loss 0.1869 (0.0206)	
training:	Epoch: [55][135/233]	Loss 0.0037 (0.0205)	
training:	Epoch: [55][136/233]	Loss 0.0029 (0.0203)	
training:	Epoch: [55][137/233]	Loss 0.0132 (0.0203)	
training:	Epoch: [55][138/233]	Loss 0.0025 (0.0201)	
training:	Epoch: [55][139/233]	Loss 0.0034 (0.0200)	
training:	Epoch: [55][140/233]	Loss 0.0034 (0.0199)	
training:	Epoch: [55][141/233]	Loss 0.0030 (0.0198)	
training:	Epoch: [55][142/233]	Loss 0.0033 (0.0197)	
training:	Epoch: [55][143/233]	Loss 0.0037 (0.0196)	
training:	Epoch: [55][144/233]	Loss 0.0032 (0.0194)	
training:	Epoch: [55][145/233]	Loss 0.0035 (0.0193)	
training:	Epoch: [55][146/233]	Loss 0.0046 (0.0192)	
training:	Epoch: [55][147/233]	Loss 0.0029 (0.0191)	
training:	Epoch: [55][148/233]	Loss 0.0031 (0.0190)	
training:	Epoch: [55][149/233]	Loss 0.0042 (0.0189)	
training:	Epoch: [55][150/233]	Loss 0.0031 (0.0188)	
training:	Epoch: [55][151/233]	Loss 0.0044 (0.0187)	
training:	Epoch: [55][152/233]	Loss 0.0034 (0.0186)	
training:	Epoch: [55][153/233]	Loss 0.0032 (0.0185)	
training:	Epoch: [55][154/233]	Loss 0.1985 (0.0197)	
training:	Epoch: [55][155/233]	Loss 0.0032 (0.0196)	
training:	Epoch: [55][156/233]	Loss 0.0086 (0.0195)	
training:	Epoch: [55][157/233]	Loss 0.0039 (0.0194)	
training:	Epoch: [55][158/233]	Loss 0.0037 (0.0193)	
training:	Epoch: [55][159/233]	Loss 0.0191 (0.0193)	
training:	Epoch: [55][160/233]	Loss 0.0057 (0.0192)	
training:	Epoch: [55][161/233]	Loss 0.0144 (0.0192)	
training:	Epoch: [55][162/233]	Loss 0.0047 (0.0191)	
training:	Epoch: [55][163/233]	Loss 0.0039 (0.0190)	
training:	Epoch: [55][164/233]	Loss 0.0033 (0.0189)	
training:	Epoch: [55][165/233]	Loss 0.0051 (0.0188)	
training:	Epoch: [55][166/233]	Loss 0.0031 (0.0187)	
training:	Epoch: [55][167/233]	Loss 0.0029 (0.0186)	
training:	Epoch: [55][168/233]	Loss 0.0030 (0.0185)	
training:	Epoch: [55][169/233]	Loss 0.0027 (0.0184)	
training:	Epoch: [55][170/233]	Loss 0.0034 (0.0184)	
training:	Epoch: [55][171/233]	Loss 0.0035 (0.0183)	
training:	Epoch: [55][172/233]	Loss 0.0029 (0.0182)	
training:	Epoch: [55][173/233]	Loss 0.0037 (0.0181)	
training:	Epoch: [55][174/233]	Loss 0.0068 (0.0180)	
training:	Epoch: [55][175/233]	Loss 0.0028 (0.0179)	
training:	Epoch: [55][176/233]	Loss 0.0040 (0.0179)	
training:	Epoch: [55][177/233]	Loss 0.0062 (0.0178)	
training:	Epoch: [55][178/233]	Loss 0.0031 (0.0177)	
training:	Epoch: [55][179/233]	Loss 0.0031 (0.0176)	
training:	Epoch: [55][180/233]	Loss 0.0034 (0.0176)	
training:	Epoch: [55][181/233]	Loss 0.0069 (0.0175)	
training:	Epoch: [55][182/233]	Loss 0.0033 (0.0174)	
training:	Epoch: [55][183/233]	Loss 0.0028 (0.0173)	
training:	Epoch: [55][184/233]	Loss 0.0348 (0.0174)	
training:	Epoch: [55][185/233]	Loss 0.0045 (0.0174)	
training:	Epoch: [55][186/233]	Loss 0.0026 (0.0173)	
training:	Epoch: [55][187/233]	Loss 0.1432 (0.0180)	
training:	Epoch: [55][188/233]	Loss 0.0061 (0.0179)	
training:	Epoch: [55][189/233]	Loss 0.0029 (0.0178)	
training:	Epoch: [55][190/233]	Loss 0.0031 (0.0177)	
training:	Epoch: [55][191/233]	Loss 0.0028 (0.0177)	
training:	Epoch: [55][192/233]	Loss 0.0033 (0.0176)	
training:	Epoch: [55][193/233]	Loss 0.0105 (0.0176)	
training:	Epoch: [55][194/233]	Loss 0.0063 (0.0175)	
training:	Epoch: [55][195/233]	Loss 0.0032 (0.0174)	
training:	Epoch: [55][196/233]	Loss 0.0033 (0.0173)	
training:	Epoch: [55][197/233]	Loss 0.0033 (0.0173)	
training:	Epoch: [55][198/233]	Loss 0.0031 (0.0172)	
training:	Epoch: [55][199/233]	Loss 0.0034 (0.0171)	
training:	Epoch: [55][200/233]	Loss 0.0037 (0.0171)	
training:	Epoch: [55][201/233]	Loss 0.0036 (0.0170)	
training:	Epoch: [55][202/233]	Loss 0.0029 (0.0169)	
training:	Epoch: [55][203/233]	Loss 0.0030 (0.0169)	
training:	Epoch: [55][204/233]	Loss 0.0050 (0.0168)	
training:	Epoch: [55][205/233]	Loss 0.0046 (0.0167)	
training:	Epoch: [55][206/233]	Loss 0.0320 (0.0168)	
training:	Epoch: [55][207/233]	Loss 0.0036 (0.0168)	
training:	Epoch: [55][208/233]	Loss 0.0096 (0.0167)	
training:	Epoch: [55][209/233]	Loss 0.0077 (0.0167)	
training:	Epoch: [55][210/233]	Loss 0.0239 (0.0167)	
training:	Epoch: [55][211/233]	Loss 0.0041 (0.0167)	
training:	Epoch: [55][212/233]	Loss 0.0031 (0.0166)	
training:	Epoch: [55][213/233]	Loss 0.0038 (0.0165)	
training:	Epoch: [55][214/233]	Loss 0.0048 (0.0165)	
training:	Epoch: [55][215/233]	Loss 0.0029 (0.0164)	
training:	Epoch: [55][216/233]	Loss 0.0030 (0.0163)	
training:	Epoch: [55][217/233]	Loss 0.0028 (0.0163)	
training:	Epoch: [55][218/233]	Loss 0.0028 (0.0162)	
training:	Epoch: [55][219/233]	Loss 0.1373 (0.0168)	
training:	Epoch: [55][220/233]	Loss 0.0033 (0.0167)	
training:	Epoch: [55][221/233]	Loss 0.0030 (0.0167)	
training:	Epoch: [55][222/233]	Loss 0.0033 (0.0166)	
training:	Epoch: [55][223/233]	Loss 0.0028 (0.0165)	
training:	Epoch: [55][224/233]	Loss 0.0389 (0.0166)	
training:	Epoch: [55][225/233]	Loss 0.0055 (0.0166)	
training:	Epoch: [55][226/233]	Loss 0.0030 (0.0165)	
training:	Epoch: [55][227/233]	Loss 0.0053 (0.0165)	
training:	Epoch: [55][228/233]	Loss 0.0029 (0.0164)	
training:	Epoch: [55][229/233]	Loss 0.0045 (0.0164)	
training:	Epoch: [55][230/233]	Loss 0.0029 (0.0163)	
training:	Epoch: [55][231/233]	Loss 0.0039 (0.0162)	
training:	Epoch: [55][232/233]	Loss 0.0027 (0.0162)	
training:	Epoch: [55][233/233]	Loss 0.0657 (0.0164)	
Training:	 Loss: 0.0164

Training:	 ACC: 0.9989 0.9989 0.9990 0.9989
Validation:	 ACC: 0.7949 0.7935 0.7640 0.8258
Validation:	 Best_BACC: 0.8162 0.8154 0.7998 0.8326
Validation:	 Loss: 0.9584
Pretraining:	Epoch 56/200
----------
training:	Epoch: [56][1/233]	Loss 0.0032 (0.0032)	
training:	Epoch: [56][2/233]	Loss 0.0037 (0.0035)	
training:	Epoch: [56][3/233]	Loss 0.0054 (0.0041)	
training:	Epoch: [56][4/233]	Loss 0.0109 (0.0058)	
training:	Epoch: [56][5/233]	Loss 0.0037 (0.0054)	
training:	Epoch: [56][6/233]	Loss 0.0033 (0.0050)	
training:	Epoch: [56][7/233]	Loss 0.0026 (0.0047)	
training:	Epoch: [56][8/233]	Loss 0.0041 (0.0046)	
training:	Epoch: [56][9/233]	Loss 0.1919 (0.0254)	
training:	Epoch: [56][10/233]	Loss 0.0047 (0.0233)	
training:	Epoch: [56][11/233]	Loss 0.0030 (0.0215)	
training:	Epoch: [56][12/233]	Loss 0.1574 (0.0328)	
training:	Epoch: [56][13/233]	Loss 0.0068 (0.0308)	
training:	Epoch: [56][14/233]	Loss 0.1983 (0.0428)	
training:	Epoch: [56][15/233]	Loss 0.0032 (0.0401)	
training:	Epoch: [56][16/233]	Loss 0.0035 (0.0378)	
training:	Epoch: [56][17/233]	Loss 0.0067 (0.0360)	
training:	Epoch: [56][18/233]	Loss 0.2012 (0.0452)	
training:	Epoch: [56][19/233]	Loss 0.0030 (0.0430)	
training:	Epoch: [56][20/233]	Loss 0.0034 (0.0410)	
training:	Epoch: [56][21/233]	Loss 0.0032 (0.0392)	
training:	Epoch: [56][22/233]	Loss 0.0044 (0.0376)	
training:	Epoch: [56][23/233]	Loss 0.0047 (0.0362)	
training:	Epoch: [56][24/233]	Loss 0.0177 (0.0354)	
training:	Epoch: [56][25/233]	Loss 0.0099 (0.0344)	
training:	Epoch: [56][26/233]	Loss 0.0045 (0.0332)	
training:	Epoch: [56][27/233]	Loss 0.0132 (0.0325)	
training:	Epoch: [56][28/233]	Loss 0.0080 (0.0316)	
training:	Epoch: [56][29/233]	Loss 0.0102 (0.0309)	
training:	Epoch: [56][30/233]	Loss 0.0442 (0.0313)	
training:	Epoch: [56][31/233]	Loss 0.0032 (0.0304)	
training:	Epoch: [56][32/233]	Loss 0.1275 (0.0335)	
training:	Epoch: [56][33/233]	Loss 0.0030 (0.0325)	
training:	Epoch: [56][34/233]	Loss 0.0028 (0.0317)	
training:	Epoch: [56][35/233]	Loss 0.0049 (0.0309)	
training:	Epoch: [56][36/233]	Loss 0.0040 (0.0301)	
training:	Epoch: [56][37/233]	Loss 0.0645 (0.0311)	
training:	Epoch: [56][38/233]	Loss 0.0028 (0.0303)	
training:	Epoch: [56][39/233]	Loss 0.0054 (0.0297)	
training:	Epoch: [56][40/233]	Loss 0.0028 (0.0290)	
training:	Epoch: [56][41/233]	Loss 0.0032 (0.0284)	
training:	Epoch: [56][42/233]	Loss 0.0028 (0.0278)	
training:	Epoch: [56][43/233]	Loss 0.0028 (0.0272)	
training:	Epoch: [56][44/233]	Loss 0.0039 (0.0267)	
training:	Epoch: [56][45/233]	Loss 0.0027 (0.0261)	
training:	Epoch: [56][46/233]	Loss 0.0036 (0.0256)	
training:	Epoch: [56][47/233]	Loss 0.0040 (0.0252)	
training:	Epoch: [56][48/233]	Loss 0.0757 (0.0262)	
training:	Epoch: [56][49/233]	Loss 0.0128 (0.0260)	
training:	Epoch: [56][50/233]	Loss 0.0037 (0.0255)	
training:	Epoch: [56][51/233]	Loss 0.0430 (0.0259)	
training:	Epoch: [56][52/233]	Loss 0.0034 (0.0254)	
training:	Epoch: [56][53/233]	Loss 0.0062 (0.0251)	
training:	Epoch: [56][54/233]	Loss 0.0031 (0.0247)	
training:	Epoch: [56][55/233]	Loss 0.0429 (0.0250)	
training:	Epoch: [56][56/233]	Loss 0.0054 (0.0246)	
training:	Epoch: [56][57/233]	Loss 0.0193 (0.0245)	
training:	Epoch: [56][58/233]	Loss 0.0029 (0.0242)	
training:	Epoch: [56][59/233]	Loss 0.0030 (0.0238)	
training:	Epoch: [56][60/233]	Loss 0.0047 (0.0235)	
training:	Epoch: [56][61/233]	Loss 0.0038 (0.0232)	
training:	Epoch: [56][62/233]	Loss 0.0034 (0.0229)	
training:	Epoch: [56][63/233]	Loss 0.0027 (0.0225)	
training:	Epoch: [56][64/233]	Loss 0.0038 (0.0222)	
training:	Epoch: [56][65/233]	Loss 0.0066 (0.0220)	
training:	Epoch: [56][66/233]	Loss 0.0034 (0.0217)	
training:	Epoch: [56][67/233]	Loss 0.0039 (0.0215)	
training:	Epoch: [56][68/233]	Loss 0.0039 (0.0212)	
training:	Epoch: [56][69/233]	Loss 0.0118 (0.0211)	
training:	Epoch: [56][70/233]	Loss 0.0031 (0.0208)	
training:	Epoch: [56][71/233]	Loss 0.0086 (0.0206)	
training:	Epoch: [56][72/233]	Loss 0.0047 (0.0204)	
training:	Epoch: [56][73/233]	Loss 0.0029 (0.0202)	
training:	Epoch: [56][74/233]	Loss 0.0036 (0.0199)	
training:	Epoch: [56][75/233]	Loss 0.0041 (0.0197)	
training:	Epoch: [56][76/233]	Loss 0.0073 (0.0196)	
training:	Epoch: [56][77/233]	Loss 0.0039 (0.0194)	
training:	Epoch: [56][78/233]	Loss 0.0043 (0.0192)	
training:	Epoch: [56][79/233]	Loss 0.0038 (0.0190)	
training:	Epoch: [56][80/233]	Loss 0.1544 (0.0207)	
training:	Epoch: [56][81/233]	Loss 0.0082 (0.0205)	
training:	Epoch: [56][82/233]	Loss 0.0044 (0.0203)	
training:	Epoch: [56][83/233]	Loss 0.0050 (0.0201)	
training:	Epoch: [56][84/233]	Loss 0.0031 (0.0199)	
training:	Epoch: [56][85/233]	Loss 0.0028 (0.0197)	
training:	Epoch: [56][86/233]	Loss 0.0055 (0.0196)	
training:	Epoch: [56][87/233]	Loss 0.0041 (0.0194)	
training:	Epoch: [56][88/233]	Loss 0.0233 (0.0194)	
training:	Epoch: [56][89/233]	Loss 0.0378 (0.0196)	
training:	Epoch: [56][90/233]	Loss 0.0069 (0.0195)	
training:	Epoch: [56][91/233]	Loss 0.0078 (0.0194)	
training:	Epoch: [56][92/233]	Loss 0.0033 (0.0192)	
training:	Epoch: [56][93/233]	Loss 0.0035 (0.0190)	
training:	Epoch: [56][94/233]	Loss 0.0049 (0.0189)	
training:	Epoch: [56][95/233]	Loss 0.0069 (0.0188)	
training:	Epoch: [56][96/233]	Loss 0.0267 (0.0188)	
training:	Epoch: [56][97/233]	Loss 0.0027 (0.0187)	
training:	Epoch: [56][98/233]	Loss 0.0046 (0.0185)	
training:	Epoch: [56][99/233]	Loss 0.0044 (0.0184)	
training:	Epoch: [56][100/233]	Loss 0.0043 (0.0182)	
training:	Epoch: [56][101/233]	Loss 0.0131 (0.0182)	
training:	Epoch: [56][102/233]	Loss 0.0028 (0.0180)	
training:	Epoch: [56][103/233]	Loss 0.0041 (0.0179)	
training:	Epoch: [56][104/233]	Loss 0.0267 (0.0180)	
training:	Epoch: [56][105/233]	Loss 0.1849 (0.0196)	
training:	Epoch: [56][106/233]	Loss 0.0033 (0.0194)	
training:	Epoch: [56][107/233]	Loss 0.0068 (0.0193)	
training:	Epoch: [56][108/233]	Loss 0.0063 (0.0192)	
training:	Epoch: [56][109/233]	Loss 0.0053 (0.0191)	
training:	Epoch: [56][110/233]	Loss 0.0026 (0.0189)	
training:	Epoch: [56][111/233]	Loss 0.0034 (0.0188)	
training:	Epoch: [56][112/233]	Loss 0.0103 (0.0187)	
training:	Epoch: [56][113/233]	Loss 0.2799 (0.0210)	
training:	Epoch: [56][114/233]	Loss 0.0028 (0.0208)	
training:	Epoch: [56][115/233]	Loss 0.0847 (0.0214)	
training:	Epoch: [56][116/233]	Loss 0.0026 (0.0212)	
training:	Epoch: [56][117/233]	Loss 0.0034 (0.0211)	
training:	Epoch: [56][118/233]	Loss 0.0061 (0.0210)	
training:	Epoch: [56][119/233]	Loss 0.0033 (0.0208)	
training:	Epoch: [56][120/233]	Loss 0.0033 (0.0207)	
training:	Epoch: [56][121/233]	Loss 0.0043 (0.0205)	
training:	Epoch: [56][122/233]	Loss 0.0030 (0.0204)	
training:	Epoch: [56][123/233]	Loss 0.0379 (0.0205)	
training:	Epoch: [56][124/233]	Loss 0.0032 (0.0204)	
training:	Epoch: [56][125/233]	Loss 0.1910 (0.0218)	
training:	Epoch: [56][126/233]	Loss 0.0036 (0.0216)	
training:	Epoch: [56][127/233]	Loss 0.0028 (0.0215)	
training:	Epoch: [56][128/233]	Loss 0.0193 (0.0214)	
training:	Epoch: [56][129/233]	Loss 0.0025 (0.0213)	
training:	Epoch: [56][130/233]	Loss 0.0041 (0.0212)	
training:	Epoch: [56][131/233]	Loss 0.0026 (0.0210)	
training:	Epoch: [56][132/233]	Loss 0.0168 (0.0210)	
training:	Epoch: [56][133/233]	Loss 0.0036 (0.0209)	
training:	Epoch: [56][134/233]	Loss 0.0040 (0.0207)	
training:	Epoch: [56][135/233]	Loss 0.0043 (0.0206)	
training:	Epoch: [56][136/233]	Loss 0.0094 (0.0205)	
training:	Epoch: [56][137/233]	Loss 0.0030 (0.0204)	
training:	Epoch: [56][138/233]	Loss 0.0045 (0.0203)	
training:	Epoch: [56][139/233]	Loss 0.0049 (0.0202)	
training:	Epoch: [56][140/233]	Loss 0.0028 (0.0201)	
training:	Epoch: [56][141/233]	Loss 0.0041 (0.0199)	
training:	Epoch: [56][142/233]	Loss 0.0031 (0.0198)	
training:	Epoch: [56][143/233]	Loss 0.0076 (0.0197)	
training:	Epoch: [56][144/233]	Loss 0.0096 (0.0197)	
training:	Epoch: [56][145/233]	Loss 0.0046 (0.0196)	
training:	Epoch: [56][146/233]	Loss 0.0035 (0.0195)	
training:	Epoch: [56][147/233]	Loss 0.0067 (0.0194)	
training:	Epoch: [56][148/233]	Loss 0.0375 (0.0195)	
training:	Epoch: [56][149/233]	Loss 0.0165 (0.0195)	
training:	Epoch: [56][150/233]	Loss 0.0351 (0.0196)	
training:	Epoch: [56][151/233]	Loss 0.0026 (0.0195)	
training:	Epoch: [56][152/233]	Loss 0.0160 (0.0194)	
training:	Epoch: [56][153/233]	Loss 0.0025 (0.0193)	
training:	Epoch: [56][154/233]	Loss 0.0079 (0.0193)	
training:	Epoch: [56][155/233]	Loss 0.0029 (0.0191)	
training:	Epoch: [56][156/233]	Loss 0.0040 (0.0190)	
training:	Epoch: [56][157/233]	Loss 0.0029 (0.0189)	
training:	Epoch: [56][158/233]	Loss 0.0406 (0.0191)	
training:	Epoch: [56][159/233]	Loss 0.0031 (0.0190)	
training:	Epoch: [56][160/233]	Loss 0.0051 (0.0189)	
training:	Epoch: [56][161/233]	Loss 0.0323 (0.0190)	
training:	Epoch: [56][162/233]	Loss 0.0052 (0.0189)	
training:	Epoch: [56][163/233]	Loss 0.0094 (0.0188)	
training:	Epoch: [56][164/233]	Loss 0.0038 (0.0187)	
training:	Epoch: [56][165/233]	Loss 0.0063 (0.0187)	
training:	Epoch: [56][166/233]	Loss 0.0028 (0.0186)	
training:	Epoch: [56][167/233]	Loss 0.0120 (0.0185)	
training:	Epoch: [56][168/233]	Loss 0.0702 (0.0188)	
training:	Epoch: [56][169/233]	Loss 0.0052 (0.0188)	
training:	Epoch: [56][170/233]	Loss 0.0030 (0.0187)	
training:	Epoch: [56][171/233]	Loss 0.0029 (0.0186)	
training:	Epoch: [56][172/233]	Loss 0.0034 (0.0185)	
training:	Epoch: [56][173/233]	Loss 0.0032 (0.0184)	
training:	Epoch: [56][174/233]	Loss 0.0045 (0.0183)	
training:	Epoch: [56][175/233]	Loss 0.0036 (0.0182)	
training:	Epoch: [56][176/233]	Loss 0.0079 (0.0182)	
training:	Epoch: [56][177/233]	Loss 0.0403 (0.0183)	
training:	Epoch: [56][178/233]	Loss 0.0501 (0.0185)	
training:	Epoch: [56][179/233]	Loss 0.0045 (0.0184)	
training:	Epoch: [56][180/233]	Loss 0.0150 (0.0184)	
training:	Epoch: [56][181/233]	Loss 0.0028 (0.0183)	
training:	Epoch: [56][182/233]	Loss 0.0035 (0.0182)	
training:	Epoch: [56][183/233]	Loss 0.0159 (0.0182)	
training:	Epoch: [56][184/233]	Loss 0.0028 (0.0181)	
training:	Epoch: [56][185/233]	Loss 0.0025 (0.0180)	
training:	Epoch: [56][186/233]	Loss 0.0026 (0.0180)	
training:	Epoch: [56][187/233]	Loss 0.0041 (0.0179)	
training:	Epoch: [56][188/233]	Loss 0.0025 (0.0178)	
training:	Epoch: [56][189/233]	Loss 0.0150 (0.0178)	
training:	Epoch: [56][190/233]	Loss 0.0026 (0.0177)	
training:	Epoch: [56][191/233]	Loss 0.0026 (0.0176)	
training:	Epoch: [56][192/233]	Loss 0.0056 (0.0176)	
training:	Epoch: [56][193/233]	Loss 0.0046 (0.0175)	
training:	Epoch: [56][194/233]	Loss 0.0059 (0.0174)	
training:	Epoch: [56][195/233]	Loss 0.0040 (0.0174)	
training:	Epoch: [56][196/233]	Loss 0.0046 (0.0173)	
training:	Epoch: [56][197/233]	Loss 0.0038 (0.0172)	
training:	Epoch: [56][198/233]	Loss 0.0035 (0.0172)	
training:	Epoch: [56][199/233]	Loss 0.1855 (0.0180)	
training:	Epoch: [56][200/233]	Loss 0.0032 (0.0179)	
training:	Epoch: [56][201/233]	Loss 0.0026 (0.0179)	
training:	Epoch: [56][202/233]	Loss 0.0073 (0.0178)	
training:	Epoch: [56][203/233]	Loss 0.0038 (0.0177)	
training:	Epoch: [56][204/233]	Loss 0.0031 (0.0177)	
training:	Epoch: [56][205/233]	Loss 0.1710 (0.0184)	
training:	Epoch: [56][206/233]	Loss 0.0029 (0.0183)	
training:	Epoch: [56][207/233]	Loss 0.0024 (0.0183)	
training:	Epoch: [56][208/233]	Loss 0.0029 (0.0182)	
training:	Epoch: [56][209/233]	Loss 0.1919 (0.0190)	
training:	Epoch: [56][210/233]	Loss 0.0035 (0.0189)	
training:	Epoch: [56][211/233]	Loss 0.0046 (0.0189)	
training:	Epoch: [56][212/233]	Loss 0.0050 (0.0188)	
training:	Epoch: [56][213/233]	Loss 0.0030 (0.0187)	
training:	Epoch: [56][214/233]	Loss 0.0028 (0.0187)	
training:	Epoch: [56][215/233]	Loss 0.1951 (0.0195)	
training:	Epoch: [56][216/233]	Loss 0.0029 (0.0194)	
training:	Epoch: [56][217/233]	Loss 0.0029 (0.0193)	
training:	Epoch: [56][218/233]	Loss 0.0030 (0.0193)	
training:	Epoch: [56][219/233]	Loss 0.0032 (0.0192)	
training:	Epoch: [56][220/233]	Loss 0.0028 (0.0191)	
training:	Epoch: [56][221/233]	Loss 0.0030 (0.0190)	
training:	Epoch: [56][222/233]	Loss 0.0064 (0.0190)	
training:	Epoch: [56][223/233]	Loss 0.0048 (0.0189)	
training:	Epoch: [56][224/233]	Loss 0.0033 (0.0188)	
training:	Epoch: [56][225/233]	Loss 0.0024 (0.0188)	
training:	Epoch: [56][226/233]	Loss 0.0823 (0.0191)	
training:	Epoch: [56][227/233]	Loss 0.0053 (0.0190)	
training:	Epoch: [56][228/233]	Loss 0.0494 (0.0191)	
training:	Epoch: [56][229/233]	Loss 0.0028 (0.0191)	
training:	Epoch: [56][230/233]	Loss 0.0028 (0.0190)	
training:	Epoch: [56][231/233]	Loss 0.0067 (0.0189)	
training:	Epoch: [56][232/233]	Loss 0.0025 (0.0189)	
training:	Epoch: [56][233/233]	Loss 0.0087 (0.0188)	
Training:	 Loss: 0.0188

Training:	 ACC: 0.9989 0.9989 0.9990 0.9989
Validation:	 ACC: 0.7931 0.7924 0.7794 0.8067
Validation:	 Best_BACC: 0.8162 0.8154 0.7998 0.8326
Validation:	 Loss: 0.9643
Pretraining:	Epoch 57/200
----------
training:	Epoch: [57][1/233]	Loss 0.0029 (0.0029)	
training:	Epoch: [57][2/233]	Loss 0.0031 (0.0030)	
training:	Epoch: [57][3/233]	Loss 0.0034 (0.0031)	
training:	Epoch: [57][4/233]	Loss 0.0027 (0.0030)	
training:	Epoch: [57][5/233]	Loss 0.0030 (0.0030)	
training:	Epoch: [57][6/233]	Loss 0.0028 (0.0030)	
training:	Epoch: [57][7/233]	Loss 0.0034 (0.0030)	
training:	Epoch: [57][8/233]	Loss 0.0057 (0.0034)	
training:	Epoch: [57][9/233]	Loss 0.0030 (0.0033)	
training:	Epoch: [57][10/233]	Loss 0.0028 (0.0033)	
training:	Epoch: [57][11/233]	Loss 0.0049 (0.0034)	
training:	Epoch: [57][12/233]	Loss 0.0046 (0.0035)	
training:	Epoch: [57][13/233]	Loss 0.0035 (0.0035)	
training:	Epoch: [57][14/233]	Loss 0.0055 (0.0037)	
training:	Epoch: [57][15/233]	Loss 0.0036 (0.0037)	
training:	Epoch: [57][16/233]	Loss 0.0084 (0.0040)	
training:	Epoch: [57][17/233]	Loss 0.0031 (0.0039)	
training:	Epoch: [57][18/233]	Loss 0.1873 (0.0141)	
training:	Epoch: [57][19/233]	Loss 0.0044 (0.0136)	
training:	Epoch: [57][20/233]	Loss 0.0079 (0.0133)	
training:	Epoch: [57][21/233]	Loss 0.0632 (0.0157)	
training:	Epoch: [57][22/233]	Loss 0.0037 (0.0151)	
training:	Epoch: [57][23/233]	Loss 0.0033 (0.0146)	
training:	Epoch: [57][24/233]	Loss 0.0069 (0.0143)	
training:	Epoch: [57][25/233]	Loss 0.0039 (0.0139)	
training:	Epoch: [57][26/233]	Loss 0.0030 (0.0135)	
training:	Epoch: [57][27/233]	Loss 0.0032 (0.0131)	
training:	Epoch: [57][28/233]	Loss 0.0058 (0.0128)	
training:	Epoch: [57][29/233]	Loss 0.0037 (0.0125)	
training:	Epoch: [57][30/233]	Loss 0.0512 (0.0138)	
training:	Epoch: [57][31/233]	Loss 0.0475 (0.0149)	
training:	Epoch: [57][32/233]	Loss 0.0091 (0.0147)	
training:	Epoch: [57][33/233]	Loss 0.0025 (0.0143)	
training:	Epoch: [57][34/233]	Loss 0.0046 (0.0140)	
training:	Epoch: [57][35/233]	Loss 0.0044 (0.0138)	
training:	Epoch: [57][36/233]	Loss 0.0031 (0.0135)	
training:	Epoch: [57][37/233]	Loss 0.0029 (0.0132)	
training:	Epoch: [57][38/233]	Loss 0.0025 (0.0129)	
training:	Epoch: [57][39/233]	Loss 0.0076 (0.0128)	
training:	Epoch: [57][40/233]	Loss 0.0046 (0.0126)	
training:	Epoch: [57][41/233]	Loss 0.0226 (0.0128)	
training:	Epoch: [57][42/233]	Loss 0.0061 (0.0126)	
training:	Epoch: [57][43/233]	Loss 0.0072 (0.0125)	
training:	Epoch: [57][44/233]	Loss 0.0220 (0.0127)	
training:	Epoch: [57][45/233]	Loss 0.0044 (0.0125)	
training:	Epoch: [57][46/233]	Loss 0.0062 (0.0124)	
training:	Epoch: [57][47/233]	Loss 0.0035 (0.0122)	
training:	Epoch: [57][48/233]	Loss 0.0028 (0.0120)	
training:	Epoch: [57][49/233]	Loss 0.0030 (0.0118)	
training:	Epoch: [57][50/233]	Loss 0.0032 (0.0117)	
training:	Epoch: [57][51/233]	Loss 0.0032 (0.0115)	
training:	Epoch: [57][52/233]	Loss 0.0042 (0.0114)	
training:	Epoch: [57][53/233]	Loss 0.0092 (0.0113)	
training:	Epoch: [57][54/233]	Loss 0.0031 (0.0112)	
training:	Epoch: [57][55/233]	Loss 0.0026 (0.0110)	
training:	Epoch: [57][56/233]	Loss 0.0026 (0.0109)	
training:	Epoch: [57][57/233]	Loss 0.0064 (0.0108)	
training:	Epoch: [57][58/233]	Loss 0.0034 (0.0107)	
training:	Epoch: [57][59/233]	Loss 0.0057 (0.0106)	
training:	Epoch: [57][60/233]	Loss 0.1611 (0.0131)	
training:	Epoch: [57][61/233]	Loss 0.0033 (0.0129)	
training:	Epoch: [57][62/233]	Loss 0.1135 (0.0145)	
training:	Epoch: [57][63/233]	Loss 0.0055 (0.0144)	
training:	Epoch: [57][64/233]	Loss 0.0029 (0.0142)	
training:	Epoch: [57][65/233]	Loss 0.0306 (0.0145)	
training:	Epoch: [57][66/233]	Loss 0.0139 (0.0145)	
training:	Epoch: [57][67/233]	Loss 0.0283 (0.0147)	
training:	Epoch: [57][68/233]	Loss 0.0036 (0.0145)	
training:	Epoch: [57][69/233]	Loss 0.0062 (0.0144)	
training:	Epoch: [57][70/233]	Loss 0.0030 (0.0142)	
training:	Epoch: [57][71/233]	Loss 0.0259 (0.0144)	
training:	Epoch: [57][72/233]	Loss 0.0198 (0.0145)	
training:	Epoch: [57][73/233]	Loss 0.0032 (0.0143)	
training:	Epoch: [57][74/233]	Loss 0.0032 (0.0142)	
training:	Epoch: [57][75/233]	Loss 0.0032 (0.0140)	
training:	Epoch: [57][76/233]	Loss 0.0030 (0.0139)	
training:	Epoch: [57][77/233]	Loss 0.0875 (0.0148)	
training:	Epoch: [57][78/233]	Loss 0.0027 (0.0147)	
training:	Epoch: [57][79/233]	Loss 0.0027 (0.0145)	
training:	Epoch: [57][80/233]	Loss 0.0038 (0.0144)	
training:	Epoch: [57][81/233]	Loss 0.0099 (0.0143)	
training:	Epoch: [57][82/233]	Loss 0.0031 (0.0142)	
training:	Epoch: [57][83/233]	Loss 0.0025 (0.0140)	
training:	Epoch: [57][84/233]	Loss 0.0029 (0.0139)	
training:	Epoch: [57][85/233]	Loss 0.0039 (0.0138)	
training:	Epoch: [57][86/233]	Loss 0.0050 (0.0137)	
training:	Epoch: [57][87/233]	Loss 0.0136 (0.0137)	
training:	Epoch: [57][88/233]	Loss 0.0093 (0.0136)	
training:	Epoch: [57][89/233]	Loss 0.0030 (0.0135)	
training:	Epoch: [57][90/233]	Loss 0.1879 (0.0155)	
training:	Epoch: [57][91/233]	Loss 0.0076 (0.0154)	
training:	Epoch: [57][92/233]	Loss 0.0035 (0.0152)	
training:	Epoch: [57][93/233]	Loss 0.0025 (0.0151)	
training:	Epoch: [57][94/233]	Loss 0.0049 (0.0150)	
training:	Epoch: [57][95/233]	Loss 0.0025 (0.0149)	
training:	Epoch: [57][96/233]	Loss 0.0055 (0.0148)	
training:	Epoch: [57][97/233]	Loss 0.0164 (0.0148)	
training:	Epoch: [57][98/233]	Loss 0.0040 (0.0147)	
training:	Epoch: [57][99/233]	Loss 0.0086 (0.0146)	
training:	Epoch: [57][100/233]	Loss 0.0034 (0.0145)	
training:	Epoch: [57][101/233]	Loss 0.2008 (0.0164)	
training:	Epoch: [57][102/233]	Loss 0.0838 (0.0170)	
training:	Epoch: [57][103/233]	Loss 0.0024 (0.0169)	
training:	Epoch: [57][104/233]	Loss 0.0052 (0.0168)	
training:	Epoch: [57][105/233]	Loss 0.0068 (0.0167)	
training:	Epoch: [57][106/233]	Loss 0.0058 (0.0166)	
training:	Epoch: [57][107/233]	Loss 0.1480 (0.0178)	
training:	Epoch: [57][108/233]	Loss 0.0922 (0.0185)	
training:	Epoch: [57][109/233]	Loss 0.0024 (0.0183)	
training:	Epoch: [57][110/233]	Loss 0.0029 (0.0182)	
training:	Epoch: [57][111/233]	Loss 0.0025 (0.0180)	
training:	Epoch: [57][112/233]	Loss 0.0026 (0.0179)	
training:	Epoch: [57][113/233]	Loss 0.0210 (0.0179)	
training:	Epoch: [57][114/233]	Loss 0.0031 (0.0178)	
training:	Epoch: [57][115/233]	Loss 0.0633 (0.0182)	
training:	Epoch: [57][116/233]	Loss 0.0026 (0.0181)	
training:	Epoch: [57][117/233]	Loss 0.0027 (0.0179)	
training:	Epoch: [57][118/233]	Loss 0.0027 (0.0178)	
training:	Epoch: [57][119/233]	Loss 0.0077 (0.0177)	
training:	Epoch: [57][120/233]	Loss 0.0083 (0.0176)	
training:	Epoch: [57][121/233]	Loss 0.0070 (0.0176)	
training:	Epoch: [57][122/233]	Loss 0.0025 (0.0174)	
training:	Epoch: [57][123/233]	Loss 0.0069 (0.0173)	
training:	Epoch: [57][124/233]	Loss 0.0057 (0.0173)	
training:	Epoch: [57][125/233]	Loss 0.0026 (0.0171)	
training:	Epoch: [57][126/233]	Loss 0.0431 (0.0173)	
training:	Epoch: [57][127/233]	Loss 0.0028 (0.0172)	
training:	Epoch: [57][128/233]	Loss 0.0117 (0.0172)	
training:	Epoch: [57][129/233]	Loss 0.0046 (0.0171)	
training:	Epoch: [57][130/233]	Loss 0.0219 (0.0171)	
training:	Epoch: [57][131/233]	Loss 0.0565 (0.0174)	
training:	Epoch: [57][132/233]	Loss 0.0273 (0.0175)	
training:	Epoch: [57][133/233]	Loss 0.0029 (0.0174)	
training:	Epoch: [57][134/233]	Loss 0.0067 (0.0173)	
training:	Epoch: [57][135/233]	Loss 0.0033 (0.0172)	
training:	Epoch: [57][136/233]	Loss 0.0028 (0.0171)	
training:	Epoch: [57][137/233]	Loss 0.0131 (0.0171)	
training:	Epoch: [57][138/233]	Loss 0.0034 (0.0170)	
training:	Epoch: [57][139/233]	Loss 0.0192 (0.0170)	
training:	Epoch: [57][140/233]	Loss 0.0040 (0.0169)	
training:	Epoch: [57][141/233]	Loss 0.0028 (0.0168)	
training:	Epoch: [57][142/233]	Loss 0.0025 (0.0167)	
training:	Epoch: [57][143/233]	Loss 0.0038 (0.0166)	
training:	Epoch: [57][144/233]	Loss 0.0030 (0.0165)	
training:	Epoch: [57][145/233]	Loss 0.0377 (0.0167)	
training:	Epoch: [57][146/233]	Loss 0.0053 (0.0166)	
training:	Epoch: [57][147/233]	Loss 0.0026 (0.0165)	
training:	Epoch: [57][148/233]	Loss 0.0031 (0.0164)	
training:	Epoch: [57][149/233]	Loss 0.0097 (0.0163)	
training:	Epoch: [57][150/233]	Loss 0.1824 (0.0175)	
training:	Epoch: [57][151/233]	Loss 0.0032 (0.0174)	
training:	Epoch: [57][152/233]	Loss 0.0023 (0.0173)	
training:	Epoch: [57][153/233]	Loss 0.0039 (0.0172)	
training:	Epoch: [57][154/233]	Loss 0.0056 (0.0171)	
training:	Epoch: [57][155/233]	Loss 0.2014 (0.0183)	
training:	Epoch: [57][156/233]	Loss 0.0062 (0.0182)	
training:	Epoch: [57][157/233]	Loss 0.0041 (0.0181)	
training:	Epoch: [57][158/233]	Loss 0.0030 (0.0180)	
training:	Epoch: [57][159/233]	Loss 0.0028 (0.0179)	
training:	Epoch: [57][160/233]	Loss 0.0033 (0.0178)	
training:	Epoch: [57][161/233]	Loss 0.0029 (0.0177)	
training:	Epoch: [57][162/233]	Loss 0.0025 (0.0177)	
training:	Epoch: [57][163/233]	Loss 0.0026 (0.0176)	
training:	Epoch: [57][164/233]	Loss 0.1111 (0.0181)	
training:	Epoch: [57][165/233]	Loss 0.0221 (0.0182)	
training:	Epoch: [57][166/233]	Loss 0.1953 (0.0192)	
training:	Epoch: [57][167/233]	Loss 0.0027 (0.0191)	
training:	Epoch: [57][168/233]	Loss 0.0069 (0.0190)	
training:	Epoch: [57][169/233]	Loss 0.0033 (0.0190)	
training:	Epoch: [57][170/233]	Loss 0.0032 (0.0189)	
training:	Epoch: [57][171/233]	Loss 0.0031 (0.0188)	
training:	Epoch: [57][172/233]	Loss 0.0043 (0.0187)	
training:	Epoch: [57][173/233]	Loss 0.0047 (0.0186)	
training:	Epoch: [57][174/233]	Loss 0.0030 (0.0185)	
training:	Epoch: [57][175/233]	Loss 0.0027 (0.0184)	
training:	Epoch: [57][176/233]	Loss 0.0048 (0.0184)	
training:	Epoch: [57][177/233]	Loss 0.0032 (0.0183)	
training:	Epoch: [57][178/233]	Loss 0.0025 (0.0182)	
training:	Epoch: [57][179/233]	Loss 0.0047 (0.0181)	
training:	Epoch: [57][180/233]	Loss 0.0027 (0.0180)	
training:	Epoch: [57][181/233]	Loss 0.0269 (0.0181)	
training:	Epoch: [57][182/233]	Loss 0.0032 (0.0180)	
training:	Epoch: [57][183/233]	Loss 0.0030 (0.0179)	
training:	Epoch: [57][184/233]	Loss 0.0055 (0.0178)	
training:	Epoch: [57][185/233]	Loss 0.0153 (0.0178)	
training:	Epoch: [57][186/233]	Loss 0.0036 (0.0177)	
training:	Epoch: [57][187/233]	Loss 0.0045 (0.0177)	
training:	Epoch: [57][188/233]	Loss 0.0032 (0.0176)	
training:	Epoch: [57][189/233]	Loss 0.1434 (0.0183)	
training:	Epoch: [57][190/233]	Loss 0.0059 (0.0182)	
training:	Epoch: [57][191/233]	Loss 0.0027 (0.0181)	
training:	Epoch: [57][192/233]	Loss 0.0303 (0.0182)	
training:	Epoch: [57][193/233]	Loss 0.0046 (0.0181)	
training:	Epoch: [57][194/233]	Loss 0.0028 (0.0180)	
training:	Epoch: [57][195/233]	Loss 0.0039 (0.0180)	
training:	Epoch: [57][196/233]	Loss 0.0053 (0.0179)	
training:	Epoch: [57][197/233]	Loss 0.0031 (0.0178)	
training:	Epoch: [57][198/233]	Loss 0.0036 (0.0177)	
training:	Epoch: [57][199/233]	Loss 0.0069 (0.0177)	
training:	Epoch: [57][200/233]	Loss 0.0025 (0.0176)	
training:	Epoch: [57][201/233]	Loss 0.0029 (0.0175)	
training:	Epoch: [57][202/233]	Loss 0.0039 (0.0175)	
training:	Epoch: [57][203/233]	Loss 0.0034 (0.0174)	
training:	Epoch: [57][204/233]	Loss 0.0071 (0.0174)	
training:	Epoch: [57][205/233]	Loss 0.0031 (0.0173)	
training:	Epoch: [57][206/233]	Loss 0.0048 (0.0172)	
training:	Epoch: [57][207/233]	Loss 0.0032 (0.0172)	
training:	Epoch: [57][208/233]	Loss 0.0050 (0.0171)	
training:	Epoch: [57][209/233]	Loss 0.0045 (0.0170)	
training:	Epoch: [57][210/233]	Loss 0.0028 (0.0170)	
training:	Epoch: [57][211/233]	Loss 0.0028 (0.0169)	
training:	Epoch: [57][212/233]	Loss 0.0030 (0.0168)	
training:	Epoch: [57][213/233]	Loss 0.0047 (0.0168)	
training:	Epoch: [57][214/233]	Loss 0.0033 (0.0167)	
training:	Epoch: [57][215/233]	Loss 0.0041 (0.0167)	
training:	Epoch: [57][216/233]	Loss 0.0029 (0.0166)	
training:	Epoch: [57][217/233]	Loss 0.0037 (0.0165)	
training:	Epoch: [57][218/233]	Loss 0.0024 (0.0165)	
training:	Epoch: [57][219/233]	Loss 0.0027 (0.0164)	
training:	Epoch: [57][220/233]	Loss 0.0034 (0.0163)	
training:	Epoch: [57][221/233]	Loss 0.0027 (0.0163)	
training:	Epoch: [57][222/233]	Loss 0.0025 (0.0162)	
training:	Epoch: [57][223/233]	Loss 0.0050 (0.0162)	
training:	Epoch: [57][224/233]	Loss 0.2074 (0.0170)	
training:	Epoch: [57][225/233]	Loss 0.1796 (0.0178)	
training:	Epoch: [57][226/233]	Loss 0.0024 (0.0177)	
training:	Epoch: [57][227/233]	Loss 0.0027 (0.0176)	
training:	Epoch: [57][228/233]	Loss 0.0109 (0.0176)	
training:	Epoch: [57][229/233]	Loss 0.0028 (0.0175)	
training:	Epoch: [57][230/233]	Loss 0.0027 (0.0175)	
training:	Epoch: [57][231/233]	Loss 0.0129 (0.0174)	
training:	Epoch: [57][232/233]	Loss 0.0029 (0.0174)	
training:	Epoch: [57][233/233]	Loss 0.0535 (0.0175)	
Training:	 Loss: 0.0175

Training:	 ACC: 0.9989 0.9989 0.9990 0.9989
Validation:	 ACC: 0.7912 0.7908 0.7824 0.8000
Validation:	 Best_BACC: 0.8162 0.8154 0.7998 0.8326
Validation:	 Loss: 0.9633
Pretraining:	Epoch 58/200
----------
training:	Epoch: [58][1/233]	Loss 0.0080 (0.0080)	
training:	Epoch: [58][2/233]	Loss 0.0026 (0.0053)	
training:	Epoch: [58][3/233]	Loss 0.0026 (0.0044)	
training:	Epoch: [58][4/233]	Loss 0.0214 (0.0087)	
training:	Epoch: [58][5/233]	Loss 0.0028 (0.0075)	
training:	Epoch: [58][6/233]	Loss 0.0119 (0.0082)	
training:	Epoch: [58][7/233]	Loss 0.0097 (0.0084)	
training:	Epoch: [58][8/233]	Loss 0.0033 (0.0078)	
training:	Epoch: [58][9/233]	Loss 0.0082 (0.0078)	
training:	Epoch: [58][10/233]	Loss 0.0043 (0.0075)	
training:	Epoch: [58][11/233]	Loss 0.0080 (0.0075)	
training:	Epoch: [58][12/233]	Loss 0.0024 (0.0071)	
training:	Epoch: [58][13/233]	Loss 0.0387 (0.0095)	
training:	Epoch: [58][14/233]	Loss 0.0100 (0.0096)	
training:	Epoch: [58][15/233]	Loss 0.0026 (0.0091)	
training:	Epoch: [58][16/233]	Loss 0.0036 (0.0088)	
training:	Epoch: [58][17/233]	Loss 0.0041 (0.0085)	
training:	Epoch: [58][18/233]	Loss 0.0032 (0.0082)	
training:	Epoch: [58][19/233]	Loss 0.0027 (0.0079)	
training:	Epoch: [58][20/233]	Loss 0.0053 (0.0078)	
training:	Epoch: [58][21/233]	Loss 0.0025 (0.0075)	
training:	Epoch: [58][22/233]	Loss 0.0028 (0.0073)	
training:	Epoch: [58][23/233]	Loss 0.0031 (0.0071)	
training:	Epoch: [58][24/233]	Loss 0.0026 (0.0069)	
training:	Epoch: [58][25/233]	Loss 0.0037 (0.0068)	
training:	Epoch: [58][26/233]	Loss 0.0657 (0.0091)	
training:	Epoch: [58][27/233]	Loss 0.0031 (0.0088)	
training:	Epoch: [58][28/233]	Loss 0.0035 (0.0087)	
training:	Epoch: [58][29/233]	Loss 0.0027 (0.0085)	
training:	Epoch: [58][30/233]	Loss 0.0029 (0.0083)	
training:	Epoch: [58][31/233]	Loss 0.0034 (0.0081)	
training:	Epoch: [58][32/233]	Loss 0.0029 (0.0079)	
training:	Epoch: [58][33/233]	Loss 0.0061 (0.0079)	
training:	Epoch: [58][34/233]	Loss 0.1352 (0.0116)	
training:	Epoch: [58][35/233]	Loss 0.0026 (0.0114)	
training:	Epoch: [58][36/233]	Loss 0.1887 (0.0163)	
training:	Epoch: [58][37/233]	Loss 0.1152 (0.0190)	
training:	Epoch: [58][38/233]	Loss 0.0039 (0.0186)	
training:	Epoch: [58][39/233]	Loss 0.0029 (0.0182)	
training:	Epoch: [58][40/233]	Loss 0.0025 (0.0178)	
training:	Epoch: [58][41/233]	Loss 0.0026 (0.0174)	
training:	Epoch: [58][42/233]	Loss 0.0465 (0.0181)	
training:	Epoch: [58][43/233]	Loss 0.0025 (0.0177)	
training:	Epoch: [58][44/233]	Loss 0.0032 (0.0174)	
training:	Epoch: [58][45/233]	Loss 0.0029 (0.0171)	
training:	Epoch: [58][46/233]	Loss 0.0073 (0.0169)	
training:	Epoch: [58][47/233]	Loss 0.0063 (0.0167)	
training:	Epoch: [58][48/233]	Loss 0.0024 (0.0164)	
training:	Epoch: [58][49/233]	Loss 0.0032 (0.0161)	
training:	Epoch: [58][50/233]	Loss 0.0608 (0.0170)	
training:	Epoch: [58][51/233]	Loss 0.1851 (0.0203)	
training:	Epoch: [58][52/233]	Loss 0.0029 (0.0199)	
training:	Epoch: [58][53/233]	Loss 0.0026 (0.0196)	
training:	Epoch: [58][54/233]	Loss 0.0029 (0.0193)	
training:	Epoch: [58][55/233]	Loss 0.0046 (0.0190)	
training:	Epoch: [58][56/233]	Loss 0.1418 (0.0212)	
training:	Epoch: [58][57/233]	Loss 0.0503 (0.0217)	
training:	Epoch: [58][58/233]	Loss 0.0076 (0.0215)	
training:	Epoch: [58][59/233]	Loss 0.0030 (0.0212)	
training:	Epoch: [58][60/233]	Loss 0.0029 (0.0209)	
training:	Epoch: [58][61/233]	Loss 0.0058 (0.0206)	
training:	Epoch: [58][62/233]	Loss 0.0027 (0.0203)	
training:	Epoch: [58][63/233]	Loss 0.0072 (0.0201)	
training:	Epoch: [58][64/233]	Loss 0.0233 (0.0202)	
training:	Epoch: [58][65/233]	Loss 0.0142 (0.0201)	
training:	Epoch: [58][66/233]	Loss 0.0030 (0.0198)	
training:	Epoch: [58][67/233]	Loss 0.0055 (0.0196)	
training:	Epoch: [58][68/233]	Loss 0.0027 (0.0194)	
training:	Epoch: [58][69/233]	Loss 0.0062 (0.0192)	
training:	Epoch: [58][70/233]	Loss 0.1834 (0.0215)	
training:	Epoch: [58][71/233]	Loss 0.0029 (0.0213)	
training:	Epoch: [58][72/233]	Loss 0.0027 (0.0210)	
training:	Epoch: [58][73/233]	Loss 0.0865 (0.0219)	
training:	Epoch: [58][74/233]	Loss 0.0024 (0.0216)	
training:	Epoch: [58][75/233]	Loss 0.1913 (0.0239)	
training:	Epoch: [58][76/233]	Loss 0.0135 (0.0238)	
training:	Epoch: [58][77/233]	Loss 0.0066 (0.0235)	
training:	Epoch: [58][78/233]	Loss 0.0037 (0.0233)	
training:	Epoch: [58][79/233]	Loss 0.0038 (0.0230)	
training:	Epoch: [58][80/233]	Loss 0.0029 (0.0228)	
training:	Epoch: [58][81/233]	Loss 0.0044 (0.0226)	
training:	Epoch: [58][82/233]	Loss 0.0031 (0.0223)	
training:	Epoch: [58][83/233]	Loss 0.0319 (0.0224)	
training:	Epoch: [58][84/233]	Loss 0.0468 (0.0227)	
training:	Epoch: [58][85/233]	Loss 0.0056 (0.0225)	
training:	Epoch: [58][86/233]	Loss 0.0029 (0.0223)	
training:	Epoch: [58][87/233]	Loss 0.0041 (0.0221)	
training:	Epoch: [58][88/233]	Loss 0.0730 (0.0227)	
training:	Epoch: [58][89/233]	Loss 0.0039 (0.0225)	
training:	Epoch: [58][90/233]	Loss 0.0029 (0.0222)	
training:	Epoch: [58][91/233]	Loss 0.0029 (0.0220)	
training:	Epoch: [58][92/233]	Loss 0.0029 (0.0218)	
training:	Epoch: [58][93/233]	Loss 0.0029 (0.0216)	
training:	Epoch: [58][94/233]	Loss 0.0026 (0.0214)	
training:	Epoch: [58][95/233]	Loss 0.0031 (0.0212)	
training:	Epoch: [58][96/233]	Loss 0.0029 (0.0210)	
training:	Epoch: [58][97/233]	Loss 0.0028 (0.0208)	
training:	Epoch: [58][98/233]	Loss 0.0059 (0.0207)	
training:	Epoch: [58][99/233]	Loss 0.0119 (0.0206)	
training:	Epoch: [58][100/233]	Loss 0.0027 (0.0204)	
training:	Epoch: [58][101/233]	Loss 0.0029 (0.0203)	
training:	Epoch: [58][102/233]	Loss 0.0027 (0.0201)	
training:	Epoch: [58][103/233]	Loss 0.0164 (0.0200)	
training:	Epoch: [58][104/233]	Loss 0.0070 (0.0199)	
training:	Epoch: [58][105/233]	Loss 0.0024 (0.0198)	
training:	Epoch: [58][106/233]	Loss 0.0040 (0.0196)	
training:	Epoch: [58][107/233]	Loss 0.0027 (0.0194)	
training:	Epoch: [58][108/233]	Loss 0.0024 (0.0193)	
training:	Epoch: [58][109/233]	Loss 0.0074 (0.0192)	
training:	Epoch: [58][110/233]	Loss 0.0035 (0.0190)	
training:	Epoch: [58][111/233]	Loss 0.0034 (0.0189)	
training:	Epoch: [58][112/233]	Loss 0.0025 (0.0187)	
training:	Epoch: [58][113/233]	Loss 0.0031 (0.0186)	
training:	Epoch: [58][114/233]	Loss 0.1918 (0.0201)	
training:	Epoch: [58][115/233]	Loss 0.0032 (0.0200)	
training:	Epoch: [58][116/233]	Loss 0.0026 (0.0198)	
training:	Epoch: [58][117/233]	Loss 0.0028 (0.0197)	
training:	Epoch: [58][118/233]	Loss 0.0024 (0.0195)	
training:	Epoch: [58][119/233]	Loss 0.0032 (0.0194)	
training:	Epoch: [58][120/233]	Loss 0.0052 (0.0193)	
training:	Epoch: [58][121/233]	Loss 0.1997 (0.0208)	
training:	Epoch: [58][122/233]	Loss 0.0029 (0.0206)	
training:	Epoch: [58][123/233]	Loss 0.0234 (0.0207)	
training:	Epoch: [58][124/233]	Loss 0.0515 (0.0209)	
training:	Epoch: [58][125/233]	Loss 0.0051 (0.0208)	
training:	Epoch: [58][126/233]	Loss 0.0036 (0.0206)	
training:	Epoch: [58][127/233]	Loss 0.0027 (0.0205)	
training:	Epoch: [58][128/233]	Loss 0.0023 (0.0204)	
training:	Epoch: [58][129/233]	Loss 0.0087 (0.0203)	
training:	Epoch: [58][130/233]	Loss 0.0042 (0.0201)	
training:	Epoch: [58][131/233]	Loss 0.0051 (0.0200)	
training:	Epoch: [58][132/233]	Loss 0.0034 (0.0199)	
training:	Epoch: [58][133/233]	Loss 0.0125 (0.0198)	
training:	Epoch: [58][134/233]	Loss 0.0036 (0.0197)	
training:	Epoch: [58][135/233]	Loss 0.0026 (0.0196)	
training:	Epoch: [58][136/233]	Loss 0.0026 (0.0195)	
training:	Epoch: [58][137/233]	Loss 0.0023 (0.0193)	
training:	Epoch: [58][138/233]	Loss 0.0027 (0.0192)	
training:	Epoch: [58][139/233]	Loss 0.0041 (0.0191)	
training:	Epoch: [58][140/233]	Loss 0.0064 (0.0190)	
training:	Epoch: [58][141/233]	Loss 0.0051 (0.0189)	
training:	Epoch: [58][142/233]	Loss 0.0041 (0.0188)	
training:	Epoch: [58][143/233]	Loss 0.0038 (0.0187)	
training:	Epoch: [58][144/233]	Loss 0.0062 (0.0186)	
training:	Epoch: [58][145/233]	Loss 0.0033 (0.0185)	
training:	Epoch: [58][146/233]	Loss 0.0654 (0.0188)	
training:	Epoch: [58][147/233]	Loss 0.0705 (0.0192)	
training:	Epoch: [58][148/233]	Loss 0.0036 (0.0191)	
training:	Epoch: [58][149/233]	Loss 0.0309 (0.0192)	
training:	Epoch: [58][150/233]	Loss 0.0027 (0.0191)	
training:	Epoch: [58][151/233]	Loss 0.0029 (0.0190)	
training:	Epoch: [58][152/233]	Loss 0.0036 (0.0189)	
training:	Epoch: [58][153/233]	Loss 0.0060 (0.0188)	
training:	Epoch: [58][154/233]	Loss 0.0033 (0.0187)	
training:	Epoch: [58][155/233]	Loss 0.0024 (0.0186)	
training:	Epoch: [58][156/233]	Loss 0.0026 (0.0185)	
training:	Epoch: [58][157/233]	Loss 0.0481 (0.0186)	
training:	Epoch: [58][158/233]	Loss 0.0045 (0.0186)	
training:	Epoch: [58][159/233]	Loss 0.0046 (0.0185)	
training:	Epoch: [58][160/233]	Loss 0.0028 (0.0184)	
training:	Epoch: [58][161/233]	Loss 0.0023 (0.0183)	
training:	Epoch: [58][162/233]	Loss 0.0057 (0.0182)	
training:	Epoch: [58][163/233]	Loss 0.0338 (0.0183)	
training:	Epoch: [58][164/233]	Loss 0.0036 (0.0182)	
training:	Epoch: [58][165/233]	Loss 0.0025 (0.0181)	
training:	Epoch: [58][166/233]	Loss 0.0026 (0.0180)	
training:	Epoch: [58][167/233]	Loss 0.0054 (0.0179)	
training:	Epoch: [58][168/233]	Loss 0.0026 (0.0178)	
training:	Epoch: [58][169/233]	Loss 0.0026 (0.0178)	
training:	Epoch: [58][170/233]	Loss 0.0027 (0.0177)	
training:	Epoch: [58][171/233]	Loss 0.0026 (0.0176)	
training:	Epoch: [58][172/233]	Loss 0.0036 (0.0175)	
training:	Epoch: [58][173/233]	Loss 0.0036 (0.0174)	
training:	Epoch: [58][174/233]	Loss 0.0255 (0.0175)	
training:	Epoch: [58][175/233]	Loss 0.0028 (0.0174)	
training:	Epoch: [58][176/233]	Loss 0.0050 (0.0173)	
training:	Epoch: [58][177/233]	Loss 0.0041 (0.0172)	
training:	Epoch: [58][178/233]	Loss 0.0075 (0.0172)	
training:	Epoch: [58][179/233]	Loss 0.0032 (0.0171)	
training:	Epoch: [58][180/233]	Loss 0.0058 (0.0170)	
training:	Epoch: [58][181/233]	Loss 0.0028 (0.0170)	
training:	Epoch: [58][182/233]	Loss 0.0327 (0.0170)	
training:	Epoch: [58][183/233]	Loss 0.0024 (0.0170)	
training:	Epoch: [58][184/233]	Loss 0.0026 (0.0169)	
training:	Epoch: [58][185/233]	Loss 0.0026 (0.0168)	
training:	Epoch: [58][186/233]	Loss 0.0071 (0.0168)	
training:	Epoch: [58][187/233]	Loss 0.1439 (0.0174)	
training:	Epoch: [58][188/233]	Loss 0.0050 (0.0174)	
training:	Epoch: [58][189/233]	Loss 0.0088 (0.0173)	
training:	Epoch: [58][190/233]	Loss 0.0032 (0.0173)	
training:	Epoch: [58][191/233]	Loss 0.0092 (0.0172)	
training:	Epoch: [58][192/233]	Loss 0.0035 (0.0171)	
training:	Epoch: [58][193/233]	Loss 0.0034 (0.0171)	
training:	Epoch: [58][194/233]	Loss 0.0141 (0.0171)	
training:	Epoch: [58][195/233]	Loss 0.0028 (0.0170)	
training:	Epoch: [58][196/233]	Loss 0.0036 (0.0169)	
training:	Epoch: [58][197/233]	Loss 0.0031 (0.0168)	
training:	Epoch: [58][198/233]	Loss 0.0083 (0.0168)	
training:	Epoch: [58][199/233]	Loss 0.0024 (0.0167)	
training:	Epoch: [58][200/233]	Loss 0.0073 (0.0167)	
training:	Epoch: [58][201/233]	Loss 0.0037 (0.0166)	
training:	Epoch: [58][202/233]	Loss 0.0024 (0.0165)	
training:	Epoch: [58][203/233]	Loss 0.0023 (0.0165)	
training:	Epoch: [58][204/233]	Loss 0.0250 (0.0165)	
training:	Epoch: [58][205/233]	Loss 0.0028 (0.0165)	
training:	Epoch: [58][206/233]	Loss 0.0029 (0.0164)	
training:	Epoch: [58][207/233]	Loss 0.0044 (0.0163)	
training:	Epoch: [58][208/233]	Loss 0.0043 (0.0163)	
training:	Epoch: [58][209/233]	Loss 0.0034 (0.0162)	
training:	Epoch: [58][210/233]	Loss 0.0028 (0.0161)	
training:	Epoch: [58][211/233]	Loss 0.0029 (0.0161)	
training:	Epoch: [58][212/233]	Loss 0.0029 (0.0160)	
training:	Epoch: [58][213/233]	Loss 0.0026 (0.0160)	
training:	Epoch: [58][214/233]	Loss 0.0025 (0.0159)	
training:	Epoch: [58][215/233]	Loss 0.0025 (0.0158)	
training:	Epoch: [58][216/233]	Loss 0.2130 (0.0167)	
training:	Epoch: [58][217/233]	Loss 0.0027 (0.0167)	
training:	Epoch: [58][218/233]	Loss 0.0025 (0.0166)	
training:	Epoch: [58][219/233]	Loss 0.0596 (0.0168)	
training:	Epoch: [58][220/233]	Loss 0.0023 (0.0167)	
training:	Epoch: [58][221/233]	Loss 0.0026 (0.0167)	
training:	Epoch: [58][222/233]	Loss 0.0026 (0.0166)	
training:	Epoch: [58][223/233]	Loss 0.0076 (0.0166)	
training:	Epoch: [58][224/233]	Loss 0.0101 (0.0165)	
training:	Epoch: [58][225/233]	Loss 0.0028 (0.0165)	
training:	Epoch: [58][226/233]	Loss 0.0091 (0.0165)	
training:	Epoch: [58][227/233]	Loss 0.0226 (0.0165)	
training:	Epoch: [58][228/233]	Loss 0.0027 (0.0164)	
training:	Epoch: [58][229/233]	Loss 0.0028 (0.0164)	
training:	Epoch: [58][230/233]	Loss 0.0023 (0.0163)	
training:	Epoch: [58][231/233]	Loss 0.0026 (0.0162)	
training:	Epoch: [58][232/233]	Loss 0.1481 (0.0168)	
training:	Epoch: [58][233/233]	Loss 0.0042 (0.0168)	
Training:	 Loss: 0.0167

Training:	 ACC: 0.9988 0.9988 0.9987 0.9989
Validation:	 ACC: 0.7895 0.7881 0.7600 0.8191
Validation:	 Best_BACC: 0.8162 0.8154 0.7998 0.8326
Validation:	 Loss: 0.9916
Pretraining:	Epoch 59/200
----------
training:	Epoch: [59][1/233]	Loss 0.0027 (0.0027)	
training:	Epoch: [59][2/233]	Loss 0.0033 (0.0030)	
training:	Epoch: [59][3/233]	Loss 0.0089 (0.0050)	
training:	Epoch: [59][4/233]	Loss 0.0048 (0.0049)	
training:	Epoch: [59][5/233]	Loss 0.0116 (0.0063)	
training:	Epoch: [59][6/233]	Loss 0.0055 (0.0061)	
training:	Epoch: [59][7/233]	Loss 0.0331 (0.0100)	
training:	Epoch: [59][8/233]	Loss 0.0024 (0.0090)	
training:	Epoch: [59][9/233]	Loss 0.0024 (0.0083)	
training:	Epoch: [59][10/233]	Loss 0.0031 (0.0078)	
training:	Epoch: [59][11/233]	Loss 0.0036 (0.0074)	
training:	Epoch: [59][12/233]	Loss 0.0032 (0.0070)	
training:	Epoch: [59][13/233]	Loss 0.0024 (0.0067)	
training:	Epoch: [59][14/233]	Loss 0.0112 (0.0070)	
training:	Epoch: [59][15/233]	Loss 0.0037 (0.0068)	
training:	Epoch: [59][16/233]	Loss 0.0033 (0.0066)	
training:	Epoch: [59][17/233]	Loss 0.0058 (0.0065)	
training:	Epoch: [59][18/233]	Loss 0.1332 (0.0136)	
training:	Epoch: [59][19/233]	Loss 0.2002 (0.0234)	
training:	Epoch: [59][20/233]	Loss 0.0608 (0.0253)	
training:	Epoch: [59][21/233]	Loss 0.0037 (0.0242)	
training:	Epoch: [59][22/233]	Loss 0.0041 (0.0233)	
training:	Epoch: [59][23/233]	Loss 0.0026 (0.0224)	
training:	Epoch: [59][24/233]	Loss 0.0031 (0.0216)	
training:	Epoch: [59][25/233]	Loss 0.0189 (0.0215)	
training:	Epoch: [59][26/233]	Loss 0.0023 (0.0208)	
training:	Epoch: [59][27/233]	Loss 0.0028 (0.0201)	
training:	Epoch: [59][28/233]	Loss 0.0029 (0.0195)	
training:	Epoch: [59][29/233]	Loss 0.0026 (0.0189)	
training:	Epoch: [59][30/233]	Loss 0.0023 (0.0184)	
training:	Epoch: [59][31/233]	Loss 0.0028 (0.0178)	
training:	Epoch: [59][32/233]	Loss 0.0023 (0.0174)	
training:	Epoch: [59][33/233]	Loss 0.0040 (0.0170)	
training:	Epoch: [59][34/233]	Loss 0.0030 (0.0165)	
training:	Epoch: [59][35/233]	Loss 0.0214 (0.0167)	
training:	Epoch: [59][36/233]	Loss 0.0029 (0.0163)	
training:	Epoch: [59][37/233]	Loss 0.0029 (0.0159)	
training:	Epoch: [59][38/233]	Loss 0.0028 (0.0156)	
training:	Epoch: [59][39/233]	Loss 0.0575 (0.0167)	
training:	Epoch: [59][40/233]	Loss 0.0303 (0.0170)	
training:	Epoch: [59][41/233]	Loss 0.0033 (0.0167)	
training:	Epoch: [59][42/233]	Loss 0.0287 (0.0170)	
training:	Epoch: [59][43/233]	Loss 0.0025 (0.0166)	
training:	Epoch: [59][44/233]	Loss 0.1102 (0.0188)	
training:	Epoch: [59][45/233]	Loss 0.0146 (0.0187)	
training:	Epoch: [59][46/233]	Loss 0.0030 (0.0183)	
training:	Epoch: [59][47/233]	Loss 0.0027 (0.0180)	
training:	Epoch: [59][48/233]	Loss 0.0025 (0.0177)	
training:	Epoch: [59][49/233]	Loss 0.0033 (0.0174)	
training:	Epoch: [59][50/233]	Loss 0.0045 (0.0171)	
training:	Epoch: [59][51/233]	Loss 0.1196 (0.0191)	
training:	Epoch: [59][52/233]	Loss 0.0042 (0.0188)	
training:	Epoch: [59][53/233]	Loss 0.0025 (0.0185)	
training:	Epoch: [59][54/233]	Loss 0.0980 (0.0200)	
training:	Epoch: [59][55/233]	Loss 0.0170 (0.0199)	
training:	Epoch: [59][56/233]	Loss 0.0044 (0.0197)	
training:	Epoch: [59][57/233]	Loss 0.1575 (0.0221)	
training:	Epoch: [59][58/233]	Loss 0.0034 (0.0218)	
training:	Epoch: [59][59/233]	Loss 0.0056 (0.0215)	
training:	Epoch: [59][60/233]	Loss 0.0480 (0.0219)	
training:	Epoch: [59][61/233]	Loss 0.0030 (0.0216)	
training:	Epoch: [59][62/233]	Loss 0.0029 (0.0213)	
training:	Epoch: [59][63/233]	Loss 0.0044 (0.0210)	
training:	Epoch: [59][64/233]	Loss 0.0245 (0.0211)	
training:	Epoch: [59][65/233]	Loss 0.0030 (0.0208)	
training:	Epoch: [59][66/233]	Loss 0.0057 (0.0206)	
training:	Epoch: [59][67/233]	Loss 0.1959 (0.0232)	
training:	Epoch: [59][68/233]	Loss 0.1824 (0.0256)	
training:	Epoch: [59][69/233]	Loss 0.0350 (0.0257)	
training:	Epoch: [59][70/233]	Loss 0.0113 (0.0255)	
training:	Epoch: [59][71/233]	Loss 0.0039 (0.0252)	
training:	Epoch: [59][72/233]	Loss 0.0037 (0.0249)	
training:	Epoch: [59][73/233]	Loss 0.0029 (0.0246)	
training:	Epoch: [59][74/233]	Loss 0.0024 (0.0243)	
training:	Epoch: [59][75/233]	Loss 0.0026 (0.0240)	
training:	Epoch: [59][76/233]	Loss 0.0098 (0.0238)	
training:	Epoch: [59][77/233]	Loss 0.0035 (0.0235)	
training:	Epoch: [59][78/233]	Loss 0.0029 (0.0233)	
training:	Epoch: [59][79/233]	Loss 0.0173 (0.0232)	
training:	Epoch: [59][80/233]	Loss 0.0026 (0.0229)	
training:	Epoch: [59][81/233]	Loss 0.0129 (0.0228)	
training:	Epoch: [59][82/233]	Loss 0.0029 (0.0226)	
training:	Epoch: [59][83/233]	Loss 0.0050 (0.0224)	
training:	Epoch: [59][84/233]	Loss 0.0034 (0.0221)	
training:	Epoch: [59][85/233]	Loss 0.0021 (0.0219)	
training:	Epoch: [59][86/233]	Loss 0.0063 (0.0217)	
training:	Epoch: [59][87/233]	Loss 0.0032 (0.0215)	
training:	Epoch: [59][88/233]	Loss 0.0075 (0.0213)	
training:	Epoch: [59][89/233]	Loss 0.0026 (0.0211)	
training:	Epoch: [59][90/233]	Loss 0.0027 (0.0209)	
training:	Epoch: [59][91/233]	Loss 0.0032 (0.0207)	
training:	Epoch: [59][92/233]	Loss 0.0073 (0.0206)	
training:	Epoch: [59][93/233]	Loss 0.0034 (0.0204)	
training:	Epoch: [59][94/233]	Loss 0.0023 (0.0202)	
training:	Epoch: [59][95/233]	Loss 0.0495 (0.0205)	
training:	Epoch: [59][96/233]	Loss 0.0030 (0.0203)	
training:	Epoch: [59][97/233]	Loss 0.0025 (0.0202)	
training:	Epoch: [59][98/233]	Loss 0.0031 (0.0200)	
training:	Epoch: [59][99/233]	Loss 0.0023 (0.0198)	
training:	Epoch: [59][100/233]	Loss 0.0023 (0.0196)	
training:	Epoch: [59][101/233]	Loss 0.0036 (0.0195)	
training:	Epoch: [59][102/233]	Loss 0.0027 (0.0193)	
training:	Epoch: [59][103/233]	Loss 0.0158 (0.0193)	
training:	Epoch: [59][104/233]	Loss 0.0028 (0.0191)	
training:	Epoch: [59][105/233]	Loss 0.0023 (0.0190)	
training:	Epoch: [59][106/233]	Loss 0.0028 (0.0188)	
training:	Epoch: [59][107/233]	Loss 0.0042 (0.0187)	
training:	Epoch: [59][108/233]	Loss 0.0726 (0.0192)	
training:	Epoch: [59][109/233]	Loss 0.0786 (0.0197)	
training:	Epoch: [59][110/233]	Loss 0.0795 (0.0203)	
training:	Epoch: [59][111/233]	Loss 0.0047 (0.0201)	
training:	Epoch: [59][112/233]	Loss 0.0659 (0.0205)	
training:	Epoch: [59][113/233]	Loss 0.0028 (0.0204)	
training:	Epoch: [59][114/233]	Loss 0.0025 (0.0202)	
training:	Epoch: [59][115/233]	Loss 0.0027 (0.0201)	
training:	Epoch: [59][116/233]	Loss 0.0033 (0.0199)	
training:	Epoch: [59][117/233]	Loss 0.0038 (0.0198)	
training:	Epoch: [59][118/233]	Loss 0.0081 (0.0197)	
training:	Epoch: [59][119/233]	Loss 0.0035 (0.0195)	
training:	Epoch: [59][120/233]	Loss 0.0139 (0.0195)	
training:	Epoch: [59][121/233]	Loss 0.0055 (0.0194)	
training:	Epoch: [59][122/233]	Loss 0.2055 (0.0209)	
training:	Epoch: [59][123/233]	Loss 0.0032 (0.0208)	
training:	Epoch: [59][124/233]	Loss 0.0995 (0.0214)	
training:	Epoch: [59][125/233]	Loss 0.0037 (0.0213)	
training:	Epoch: [59][126/233]	Loss 0.0329 (0.0213)	
training:	Epoch: [59][127/233]	Loss 0.1037 (0.0220)	
training:	Epoch: [59][128/233]	Loss 0.0211 (0.0220)	
training:	Epoch: [59][129/233]	Loss 0.2724 (0.0239)	
training:	Epoch: [59][130/233]	Loss 0.0030 (0.0238)	
training:	Epoch: [59][131/233]	Loss 0.0028 (0.0236)	
training:	Epoch: [59][132/233]	Loss 0.0404 (0.0237)	
training:	Epoch: [59][133/233]	Loss 0.0093 (0.0236)	
training:	Epoch: [59][134/233]	Loss 0.0039 (0.0235)	
training:	Epoch: [59][135/233]	Loss 0.1908 (0.0247)	
training:	Epoch: [59][136/233]	Loss 0.0042 (0.0246)	
training:	Epoch: [59][137/233]	Loss 0.0680 (0.0249)	
training:	Epoch: [59][138/233]	Loss 0.0041 (0.0247)	
training:	Epoch: [59][139/233]	Loss 0.0609 (0.0250)	
training:	Epoch: [59][140/233]	Loss 0.1302 (0.0257)	
training:	Epoch: [59][141/233]	Loss 0.0034 (0.0256)	
training:	Epoch: [59][142/233]	Loss 0.0025 (0.0254)	
training:	Epoch: [59][143/233]	Loss 0.0225 (0.0254)	
training:	Epoch: [59][144/233]	Loss 0.0029 (0.0252)	
training:	Epoch: [59][145/233]	Loss 0.0031 (0.0251)	
training:	Epoch: [59][146/233]	Loss 0.0024 (0.0249)	
training:	Epoch: [59][147/233]	Loss 0.0079 (0.0248)	
training:	Epoch: [59][148/233]	Loss 0.0062 (0.0247)	
training:	Epoch: [59][149/233]	Loss 0.0117 (0.0246)	
training:	Epoch: [59][150/233]	Loss 0.0029 (0.0245)	
training:	Epoch: [59][151/233]	Loss 0.0453 (0.0246)	
training:	Epoch: [59][152/233]	Loss 0.0140 (0.0245)	
training:	Epoch: [59][153/233]	Loss 0.1005 (0.0250)	
training:	Epoch: [59][154/233]	Loss 0.0055 (0.0249)	
training:	Epoch: [59][155/233]	Loss 0.0058 (0.0248)	
training:	Epoch: [59][156/233]	Loss 0.0025 (0.0246)	
training:	Epoch: [59][157/233]	Loss 0.0530 (0.0248)	
training:	Epoch: [59][158/233]	Loss 0.0061 (0.0247)	
training:	Epoch: [59][159/233]	Loss 0.0028 (0.0246)	
training:	Epoch: [59][160/233]	Loss 0.0328 (0.0246)	
training:	Epoch: [59][161/233]	Loss 0.0132 (0.0245)	
training:	Epoch: [59][162/233]	Loss 0.0096 (0.0244)	
training:	Epoch: [59][163/233]	Loss 0.0024 (0.0243)	
training:	Epoch: [59][164/233]	Loss 0.0132 (0.0242)	
training:	Epoch: [59][165/233]	Loss 0.0025 (0.0241)	
training:	Epoch: [59][166/233]	Loss 0.0090 (0.0240)	
training:	Epoch: [59][167/233]	Loss 0.0047 (0.0239)	
training:	Epoch: [59][168/233]	Loss 0.0031 (0.0238)	
training:	Epoch: [59][169/233]	Loss 0.0171 (0.0237)	
training:	Epoch: [59][170/233]	Loss 0.0037 (0.0236)	
training:	Epoch: [59][171/233]	Loss 0.0168 (0.0236)	
training:	Epoch: [59][172/233]	Loss 0.0146 (0.0235)	
training:	Epoch: [59][173/233]	Loss 0.0030 (0.0234)	
training:	Epoch: [59][174/233]	Loss 0.0042 (0.0233)	
training:	Epoch: [59][175/233]	Loss 0.1260 (0.0239)	
training:	Epoch: [59][176/233]	Loss 0.0034 (0.0238)	
training:	Epoch: [59][177/233]	Loss 0.0185 (0.0237)	
training:	Epoch: [59][178/233]	Loss 0.0117 (0.0237)	
training:	Epoch: [59][179/233]	Loss 0.0030 (0.0236)	
training:	Epoch: [59][180/233]	Loss 0.0186 (0.0235)	
training:	Epoch: [59][181/233]	Loss 0.0065 (0.0234)	
training:	Epoch: [59][182/233]	Loss 0.0150 (0.0234)	
training:	Epoch: [59][183/233]	Loss 0.0297 (0.0234)	
training:	Epoch: [59][184/233]	Loss 0.0859 (0.0238)	
training:	Epoch: [59][185/233]	Loss 0.0270 (0.0238)	
training:	Epoch: [59][186/233]	Loss 0.0027 (0.0237)	
training:	Epoch: [59][187/233]	Loss 0.0077 (0.0236)	
training:	Epoch: [59][188/233]	Loss 0.0024 (0.0235)	
training:	Epoch: [59][189/233]	Loss 0.0564 (0.0236)	
training:	Epoch: [59][190/233]	Loss 0.0043 (0.0235)	
training:	Epoch: [59][191/233]	Loss 0.0569 (0.0237)	
training:	Epoch: [59][192/233]	Loss 0.0029 (0.0236)	
training:	Epoch: [59][193/233]	Loss 0.0033 (0.0235)	
training:	Epoch: [59][194/233]	Loss 0.0026 (0.0234)	
training:	Epoch: [59][195/233]	Loss 0.0710 (0.0236)	
training:	Epoch: [59][196/233]	Loss 0.0039 (0.0235)	
training:	Epoch: [59][197/233]	Loss 0.0063 (0.0235)	
training:	Epoch: [59][198/233]	Loss 0.0297 (0.0235)	
training:	Epoch: [59][199/233]	Loss 0.0160 (0.0234)	
training:	Epoch: [59][200/233]	Loss 0.0047 (0.0234)	
training:	Epoch: [59][201/233]	Loss 0.0023 (0.0233)	
training:	Epoch: [59][202/233]	Loss 0.0031 (0.0232)	
training:	Epoch: [59][203/233]	Loss 0.0844 (0.0235)	
training:	Epoch: [59][204/233]	Loss 0.0163 (0.0234)	
training:	Epoch: [59][205/233]	Loss 0.0133 (0.0234)	
training:	Epoch: [59][206/233]	Loss 0.0075 (0.0233)	
training:	Epoch: [59][207/233]	Loss 0.0024 (0.0232)	
training:	Epoch: [59][208/233]	Loss 0.0038 (0.0231)	
training:	Epoch: [59][209/233]	Loss 0.0028 (0.0230)	
training:	Epoch: [59][210/233]	Loss 0.0029 (0.0229)	
training:	Epoch: [59][211/233]	Loss 0.0030 (0.0228)	
training:	Epoch: [59][212/233]	Loss 0.1690 (0.0235)	
training:	Epoch: [59][213/233]	Loss 0.0123 (0.0234)	
training:	Epoch: [59][214/233]	Loss 0.0846 (0.0237)	
training:	Epoch: [59][215/233]	Loss 0.0037 (0.0236)	
training:	Epoch: [59][216/233]	Loss 0.0052 (0.0236)	
training:	Epoch: [59][217/233]	Loss 0.0048 (0.0235)	
training:	Epoch: [59][218/233]	Loss 0.0033 (0.0234)	
training:	Epoch: [59][219/233]	Loss 0.0279 (0.0234)	
training:	Epoch: [59][220/233]	Loss 0.0024 (0.0233)	
training:	Epoch: [59][221/233]	Loss 0.0033 (0.0232)	
training:	Epoch: [59][222/233]	Loss 0.0023 (0.0231)	
training:	Epoch: [59][223/233]	Loss 0.0040 (0.0230)	
training:	Epoch: [59][224/233]	Loss 0.0070 (0.0230)	
training:	Epoch: [59][225/233]	Loss 0.0039 (0.0229)	
training:	Epoch: [59][226/233]	Loss 0.2785 (0.0240)	
training:	Epoch: [59][227/233]	Loss 0.0078 (0.0239)	
training:	Epoch: [59][228/233]	Loss 0.0061 (0.0239)	
training:	Epoch: [59][229/233]	Loss 0.0060 (0.0238)	
training:	Epoch: [59][230/233]	Loss 0.0025 (0.0237)	
training:	Epoch: [59][231/233]	Loss 0.0029 (0.0236)	
training:	Epoch: [59][232/233]	Loss 0.0034 (0.0235)	
training:	Epoch: [59][233/233]	Loss 0.0030 (0.0234)	
Training:	 Loss: 0.0234

Training:	 ACC: 0.9989 0.9989 0.9987 0.9992
Validation:	 ACC: 0.7876 0.7860 0.7528 0.8225
Validation:	 Best_BACC: 0.8162 0.8154 0.7998 0.8326
Validation:	 Loss: 0.9981
Pretraining:	Epoch 60/200
----------
training:	Epoch: [60][1/233]	Loss 0.0174 (0.0174)	
training:	Epoch: [60][2/233]	Loss 0.0033 (0.0104)	
training:	Epoch: [60][3/233]	Loss 0.0027 (0.0078)	
training:	Epoch: [60][4/233]	Loss 0.0025 (0.0065)	
training:	Epoch: [60][5/233]	Loss 0.0024 (0.0056)	
training:	Epoch: [60][6/233]	Loss 0.0086 (0.0061)	
training:	Epoch: [60][7/233]	Loss 0.0022 (0.0056)	
training:	Epoch: [60][8/233]	Loss 0.0068 (0.0057)	
training:	Epoch: [60][9/233]	Loss 0.0023 (0.0054)	
training:	Epoch: [60][10/233]	Loss 0.0141 (0.0062)	
training:	Epoch: [60][11/233]	Loss 0.0066 (0.0063)	
training:	Epoch: [60][12/233]	Loss 0.0047 (0.0061)	
training:	Epoch: [60][13/233]	Loss 0.0031 (0.0059)	
training:	Epoch: [60][14/233]	Loss 0.0027 (0.0057)	
training:	Epoch: [60][15/233]	Loss 0.0032 (0.0055)	
training:	Epoch: [60][16/233]	Loss 0.0028 (0.0053)	
training:	Epoch: [60][17/233]	Loss 0.0033 (0.0052)	
training:	Epoch: [60][18/233]	Loss 0.0035 (0.0051)	
training:	Epoch: [60][19/233]	Loss 0.0026 (0.0050)	
training:	Epoch: [60][20/233]	Loss 0.0023 (0.0049)	
training:	Epoch: [60][21/233]	Loss 0.0029 (0.0048)	
training:	Epoch: [60][22/233]	Loss 0.0022 (0.0046)	
training:	Epoch: [60][23/233]	Loss 0.0031 (0.0046)	
training:	Epoch: [60][24/233]	Loss 0.0028 (0.0045)	
training:	Epoch: [60][25/233]	Loss 0.0134 (0.0049)	
training:	Epoch: [60][26/233]	Loss 0.0057 (0.0049)	
training:	Epoch: [60][27/233]	Loss 0.0023 (0.0048)	
training:	Epoch: [60][28/233]	Loss 0.0032 (0.0047)	
training:	Epoch: [60][29/233]	Loss 0.0103 (0.0049)	
training:	Epoch: [60][30/233]	Loss 0.0047 (0.0049)	
training:	Epoch: [60][31/233]	Loss 0.0035 (0.0049)	
training:	Epoch: [60][32/233]	Loss 0.0547 (0.0064)	
training:	Epoch: [60][33/233]	Loss 0.0048 (0.0064)	
training:	Epoch: [60][34/233]	Loss 0.0642 (0.0081)	
training:	Epoch: [60][35/233]	Loss 0.0034 (0.0079)	
training:	Epoch: [60][36/233]	Loss 0.0041 (0.0078)	
training:	Epoch: [60][37/233]	Loss 0.0024 (0.0077)	
training:	Epoch: [60][38/233]	Loss 0.0452 (0.0087)	
training:	Epoch: [60][39/233]	Loss 0.0038 (0.0086)	
training:	Epoch: [60][40/233]	Loss 0.0039 (0.0084)	
training:	Epoch: [60][41/233]	Loss 0.0030 (0.0083)	
training:	Epoch: [60][42/233]	Loss 0.0027 (0.0082)	
training:	Epoch: [60][43/233]	Loss 0.1939 (0.0125)	
training:	Epoch: [60][44/233]	Loss 0.0067 (0.0124)	
training:	Epoch: [60][45/233]	Loss 0.0123 (0.0124)	
training:	Epoch: [60][46/233]	Loss 0.0079 (0.0123)	
training:	Epoch: [60][47/233]	Loss 0.1462 (0.0151)	
training:	Epoch: [60][48/233]	Loss 0.0036 (0.0149)	
training:	Epoch: [60][49/233]	Loss 0.2043 (0.0187)	
training:	Epoch: [60][50/233]	Loss 0.0083 (0.0185)	
training:	Epoch: [60][51/233]	Loss 0.0024 (0.0182)	
training:	Epoch: [60][52/233]	Loss 0.0053 (0.0180)	
training:	Epoch: [60][53/233]	Loss 0.0247 (0.0181)	
training:	Epoch: [60][54/233]	Loss 0.0028 (0.0178)	
training:	Epoch: [60][55/233]	Loss 0.0042 (0.0176)	
training:	Epoch: [60][56/233]	Loss 0.0041 (0.0173)	
training:	Epoch: [60][57/233]	Loss 0.0032 (0.0171)	
training:	Epoch: [60][58/233]	Loss 0.0160 (0.0171)	
training:	Epoch: [60][59/233]	Loss 0.0085 (0.0169)	
training:	Epoch: [60][60/233]	Loss 0.0032 (0.0167)	
training:	Epoch: [60][61/233]	Loss 0.0871 (0.0178)	
training:	Epoch: [60][62/233]	Loss 0.0023 (0.0176)	
training:	Epoch: [60][63/233]	Loss 0.0051 (0.0174)	
training:	Epoch: [60][64/233]	Loss 0.0026 (0.0172)	
training:	Epoch: [60][65/233]	Loss 0.0152 (0.0171)	
training:	Epoch: [60][66/233]	Loss 0.0032 (0.0169)	
training:	Epoch: [60][67/233]	Loss 0.0035 (0.0167)	
training:	Epoch: [60][68/233]	Loss 0.0246 (0.0168)	
training:	Epoch: [60][69/233]	Loss 0.0036 (0.0166)	
training:	Epoch: [60][70/233]	Loss 0.0031 (0.0164)	
training:	Epoch: [60][71/233]	Loss 0.0024 (0.0162)	
training:	Epoch: [60][72/233]	Loss 0.0023 (0.0161)	
training:	Epoch: [60][73/233]	Loss 0.0033 (0.0159)	
training:	Epoch: [60][74/233]	Loss 0.0072 (0.0158)	
training:	Epoch: [60][75/233]	Loss 0.0021 (0.0156)	
training:	Epoch: [60][76/233]	Loss 0.0027 (0.0154)	
training:	Epoch: [60][77/233]	Loss 0.0131 (0.0154)	
training:	Epoch: [60][78/233]	Loss 0.0053 (0.0153)	
training:	Epoch: [60][79/233]	Loss 0.0077 (0.0152)	
training:	Epoch: [60][80/233]	Loss 0.0033 (0.0150)	
training:	Epoch: [60][81/233]	Loss 0.0039 (0.0149)	
training:	Epoch: [60][82/233]	Loss 0.0026 (0.0147)	
training:	Epoch: [60][83/233]	Loss 0.0060 (0.0146)	
training:	Epoch: [60][84/233]	Loss 0.0024 (0.0145)	
training:	Epoch: [60][85/233]	Loss 0.0053 (0.0144)	
training:	Epoch: [60][86/233]	Loss 0.0029 (0.0142)	
training:	Epoch: [60][87/233]	Loss 0.0027 (0.0141)	
training:	Epoch: [60][88/233]	Loss 0.0021 (0.0140)	
training:	Epoch: [60][89/233]	Loss 0.0042 (0.0139)	
training:	Epoch: [60][90/233]	Loss 0.0049 (0.0138)	
training:	Epoch: [60][91/233]	Loss 0.0025 (0.0136)	
training:	Epoch: [60][92/233]	Loss 0.0038 (0.0135)	
training:	Epoch: [60][93/233]	Loss 0.0024 (0.0134)	
training:	Epoch: [60][94/233]	Loss 0.0022 (0.0133)	
training:	Epoch: [60][95/233]	Loss 0.0417 (0.0136)	
training:	Epoch: [60][96/233]	Loss 0.0027 (0.0135)	
training:	Epoch: [60][97/233]	Loss 0.0100 (0.0134)	
training:	Epoch: [60][98/233]	Loss 0.0026 (0.0133)	
training:	Epoch: [60][99/233]	Loss 0.0065 (0.0133)	
training:	Epoch: [60][100/233]	Loss 0.0288 (0.0134)	
training:	Epoch: [60][101/233]	Loss 0.0027 (0.0133)	
training:	Epoch: [60][102/233]	Loss 0.0027 (0.0132)	
training:	Epoch: [60][103/233]	Loss 0.0120 (0.0132)	
training:	Epoch: [60][104/233]	Loss 0.0054 (0.0131)	
training:	Epoch: [60][105/233]	Loss 0.0022 (0.0130)	
training:	Epoch: [60][106/233]	Loss 0.0024 (0.0129)	
training:	Epoch: [60][107/233]	Loss 0.0030 (0.0128)	
training:	Epoch: [60][108/233]	Loss 0.0039 (0.0127)	
training:	Epoch: [60][109/233]	Loss 0.0032 (0.0126)	
training:	Epoch: [60][110/233]	Loss 0.0036 (0.0126)	
training:	Epoch: [60][111/233]	Loss 0.0038 (0.0125)	
training:	Epoch: [60][112/233]	Loss 0.0061 (0.0124)	
training:	Epoch: [60][113/233]	Loss 0.0062 (0.0124)	
training:	Epoch: [60][114/233]	Loss 0.0053 (0.0123)	
training:	Epoch: [60][115/233]	Loss 0.0030 (0.0122)	
training:	Epoch: [60][116/233]	Loss 0.0030 (0.0122)	
training:	Epoch: [60][117/233]	Loss 0.0036 (0.0121)	
training:	Epoch: [60][118/233]	Loss 0.0024 (0.0120)	
training:	Epoch: [60][119/233]	Loss 0.0028 (0.0119)	
training:	Epoch: [60][120/233]	Loss 0.0024 (0.0118)	
training:	Epoch: [60][121/233]	Loss 0.0022 (0.0118)	
training:	Epoch: [60][122/233]	Loss 0.0029 (0.0117)	
training:	Epoch: [60][123/233]	Loss 0.0026 (0.0116)	
training:	Epoch: [60][124/233]	Loss 0.0026 (0.0115)	
training:	Epoch: [60][125/233]	Loss 0.0130 (0.0116)	
training:	Epoch: [60][126/233]	Loss 0.0029 (0.0115)	
training:	Epoch: [60][127/233]	Loss 0.0321 (0.0116)	
training:	Epoch: [60][128/233]	Loss 0.0024 (0.0116)	
training:	Epoch: [60][129/233]	Loss 0.0025 (0.0115)	
training:	Epoch: [60][130/233]	Loss 0.0033 (0.0114)	
training:	Epoch: [60][131/233]	Loss 0.0030 (0.0114)	
training:	Epoch: [60][132/233]	Loss 0.0030 (0.0113)	
training:	Epoch: [60][133/233]	Loss 0.0023 (0.0112)	
training:	Epoch: [60][134/233]	Loss 0.0023 (0.0112)	
training:	Epoch: [60][135/233]	Loss 0.1545 (0.0122)	
training:	Epoch: [60][136/233]	Loss 0.0045 (0.0122)	
training:	Epoch: [60][137/233]	Loss 0.1838 (0.0134)	
training:	Epoch: [60][138/233]	Loss 0.0066 (0.0134)	
training:	Epoch: [60][139/233]	Loss 0.0032 (0.0133)	
training:	Epoch: [60][140/233]	Loss 0.0046 (0.0133)	
training:	Epoch: [60][141/233]	Loss 0.0048 (0.0132)	
training:	Epoch: [60][142/233]	Loss 0.0030 (0.0131)	
training:	Epoch: [60][143/233]	Loss 0.0129 (0.0131)	
training:	Epoch: [60][144/233]	Loss 0.0311 (0.0132)	
training:	Epoch: [60][145/233]	Loss 0.0022 (0.0132)	
training:	Epoch: [60][146/233]	Loss 0.0021 (0.0131)	
training:	Epoch: [60][147/233]	Loss 0.0035 (0.0130)	
training:	Epoch: [60][148/233]	Loss 0.1080 (0.0137)	
training:	Epoch: [60][149/233]	Loss 0.0049 (0.0136)	
training:	Epoch: [60][150/233]	Loss 0.0046 (0.0135)	
training:	Epoch: [60][151/233]	Loss 0.0023 (0.0135)	
training:	Epoch: [60][152/233]	Loss 0.0027 (0.0134)	
training:	Epoch: [60][153/233]	Loss 0.0046 (0.0133)	
training:	Epoch: [60][154/233]	Loss 0.0060 (0.0133)	
training:	Epoch: [60][155/233]	Loss 0.0023 (0.0132)	
training:	Epoch: [60][156/233]	Loss 0.0020 (0.0132)	
training:	Epoch: [60][157/233]	Loss 0.0034 (0.0131)	
training:	Epoch: [60][158/233]	Loss 0.0021 (0.0130)	
training:	Epoch: [60][159/233]	Loss 0.0032 (0.0130)	
training:	Epoch: [60][160/233]	Loss 0.0048 (0.0129)	
training:	Epoch: [60][161/233]	Loss 0.0174 (0.0129)	
training:	Epoch: [60][162/233]	Loss 0.0028 (0.0129)	
training:	Epoch: [60][163/233]	Loss 0.0030 (0.0128)	
training:	Epoch: [60][164/233]	Loss 0.0023 (0.0128)	
training:	Epoch: [60][165/233]	Loss 0.0147 (0.0128)	
training:	Epoch: [60][166/233]	Loss 0.0039 (0.0127)	
training:	Epoch: [60][167/233]	Loss 0.0045 (0.0127)	
training:	Epoch: [60][168/233]	Loss 0.0903 (0.0131)	
training:	Epoch: [60][169/233]	Loss 0.0031 (0.0131)	
training:	Epoch: [60][170/233]	Loss 0.0028 (0.0130)	
training:	Epoch: [60][171/233]	Loss 0.0030 (0.0129)	
training:	Epoch: [60][172/233]	Loss 0.0049 (0.0129)	
training:	Epoch: [60][173/233]	Loss 0.0038 (0.0128)	
training:	Epoch: [60][174/233]	Loss 0.1942 (0.0139)	
training:	Epoch: [60][175/233]	Loss 0.0031 (0.0138)	
training:	Epoch: [60][176/233]	Loss 0.0339 (0.0139)	
training:	Epoch: [60][177/233]	Loss 0.0033 (0.0139)	
training:	Epoch: [60][178/233]	Loss 0.0041 (0.0138)	
training:	Epoch: [60][179/233]	Loss 0.0620 (0.0141)	
training:	Epoch: [60][180/233]	Loss 0.0021 (0.0140)	
training:	Epoch: [60][181/233]	Loss 0.0032 (0.0140)	
training:	Epoch: [60][182/233]	Loss 0.0053 (0.0139)	
training:	Epoch: [60][183/233]	Loss 0.0025 (0.0139)	
training:	Epoch: [60][184/233]	Loss 0.0023 (0.0138)	
training:	Epoch: [60][185/233]	Loss 0.0057 (0.0138)	
training:	Epoch: [60][186/233]	Loss 0.0033 (0.0137)	
training:	Epoch: [60][187/233]	Loss 0.0029 (0.0136)	
training:	Epoch: [60][188/233]	Loss 0.0034 (0.0136)	
training:	Epoch: [60][189/233]	Loss 0.0031 (0.0135)	
training:	Epoch: [60][190/233]	Loss 0.0035 (0.0135)	
training:	Epoch: [60][191/233]	Loss 0.0093 (0.0135)	
training:	Epoch: [60][192/233]	Loss 0.0031 (0.0134)	
training:	Epoch: [60][193/233]	Loss 0.0058 (0.0134)	
training:	Epoch: [60][194/233]	Loss 0.0036 (0.0133)	
training:	Epoch: [60][195/233]	Loss 0.0035 (0.0133)	
training:	Epoch: [60][196/233]	Loss 0.0846 (0.0136)	
training:	Epoch: [60][197/233]	Loss 0.0065 (0.0136)	
training:	Epoch: [60][198/233]	Loss 0.0072 (0.0136)	
training:	Epoch: [60][199/233]	Loss 0.0026 (0.0135)	
training:	Epoch: [60][200/233]	Loss 0.0037 (0.0135)	
training:	Epoch: [60][201/233]	Loss 0.0081 (0.0134)	
training:	Epoch: [60][202/233]	Loss 0.0038 (0.0134)	
training:	Epoch: [60][203/233]	Loss 0.0021 (0.0133)	
training:	Epoch: [60][204/233]	Loss 0.0036 (0.0133)	
training:	Epoch: [60][205/233]	Loss 0.0024 (0.0132)	
training:	Epoch: [60][206/233]	Loss 0.0023 (0.0132)	
training:	Epoch: [60][207/233]	Loss 0.0024 (0.0131)	
training:	Epoch: [60][208/233]	Loss 0.0073 (0.0131)	
training:	Epoch: [60][209/233]	Loss 0.0038 (0.0130)	
training:	Epoch: [60][210/233]	Loss 0.0028 (0.0130)	
training:	Epoch: [60][211/233]	Loss 0.0077 (0.0130)	
training:	Epoch: [60][212/233]	Loss 0.0021 (0.0129)	
training:	Epoch: [60][213/233]	Loss 0.2082 (0.0138)	
training:	Epoch: [60][214/233]	Loss 0.0021 (0.0138)	
training:	Epoch: [60][215/233]	Loss 0.0025 (0.0137)	
training:	Epoch: [60][216/233]	Loss 0.0320 (0.0138)	
training:	Epoch: [60][217/233]	Loss 0.0025 (0.0138)	
training:	Epoch: [60][218/233]	Loss 0.0105 (0.0137)	
training:	Epoch: [60][219/233]	Loss 0.0021 (0.0137)	
training:	Epoch: [60][220/233]	Loss 0.0032 (0.0136)	
training:	Epoch: [60][221/233]	Loss 0.1914 (0.0144)	
training:	Epoch: [60][222/233]	Loss 0.0090 (0.0144)	
training:	Epoch: [60][223/233]	Loss 0.0389 (0.0145)	
training:	Epoch: [60][224/233]	Loss 0.0035 (0.0145)	
training:	Epoch: [60][225/233]	Loss 0.0025 (0.0144)	
training:	Epoch: [60][226/233]	Loss 0.0261 (0.0145)	
training:	Epoch: [60][227/233]	Loss 0.0024 (0.0144)	
training:	Epoch: [60][228/233]	Loss 0.0027 (0.0144)	
training:	Epoch: [60][229/233]	Loss 0.0029 (0.0143)	
training:	Epoch: [60][230/233]	Loss 0.0032 (0.0143)	
training:	Epoch: [60][231/233]	Loss 0.0031 (0.0142)	
training:	Epoch: [60][232/233]	Loss 0.0026 (0.0142)	
training:	Epoch: [60][233/233]	Loss 0.0025 (0.0141)	
Training:	 Loss: 0.0141

Training:	 ACC: 0.9991 0.9991 0.9990 0.9992
Validation:	 ACC: 0.7913 0.7919 0.8029 0.7798
Validation:	 Best_BACC: 0.8162 0.8154 0.7998 0.8326
Validation:	 Loss: 0.9898
Pretraining:	Epoch 61/200
----------
training:	Epoch: [61][1/233]	Loss 0.0026 (0.0026)	
training:	Epoch: [61][2/233]	Loss 0.0361 (0.0193)	
training:	Epoch: [61][3/233]	Loss 0.0028 (0.0138)	
training:	Epoch: [61][4/233]	Loss 0.0019 (0.0108)	
training:	Epoch: [61][5/233]	Loss 0.0045 (0.0096)	
training:	Epoch: [61][6/233]	Loss 0.0024 (0.0084)	
training:	Epoch: [61][7/233]	Loss 0.0034 (0.0077)	
training:	Epoch: [61][8/233]	Loss 0.0034 (0.0071)	
training:	Epoch: [61][9/233]	Loss 0.0022 (0.0066)	
training:	Epoch: [61][10/233]	Loss 0.0022 (0.0061)	
training:	Epoch: [61][11/233]	Loss 0.0052 (0.0061)	
training:	Epoch: [61][12/233]	Loss 0.0023 (0.0057)	
training:	Epoch: [61][13/233]	Loss 0.0034 (0.0056)	
training:	Epoch: [61][14/233]	Loss 0.0070 (0.0057)	
training:	Epoch: [61][15/233]	Loss 0.1233 (0.0135)	
training:	Epoch: [61][16/233]	Loss 0.1943 (0.0248)	
training:	Epoch: [61][17/233]	Loss 0.0028 (0.0235)	
training:	Epoch: [61][18/233]	Loss 0.0065 (0.0226)	
training:	Epoch: [61][19/233]	Loss 0.1957 (0.0317)	
training:	Epoch: [61][20/233]	Loss 0.0080 (0.0305)	
training:	Epoch: [61][21/233]	Loss 0.0022 (0.0292)	
training:	Epoch: [61][22/233]	Loss 0.0035 (0.0280)	
training:	Epoch: [61][23/233]	Loss 0.0022 (0.0269)	
training:	Epoch: [61][24/233]	Loss 0.0025 (0.0259)	
training:	Epoch: [61][25/233]	Loss 0.0021 (0.0249)	
training:	Epoch: [61][26/233]	Loss 0.0080 (0.0243)	
training:	Epoch: [61][27/233]	Loss 0.0027 (0.0235)	
training:	Epoch: [61][28/233]	Loss 0.0024 (0.0227)	
training:	Epoch: [61][29/233]	Loss 0.0024 (0.0220)	
training:	Epoch: [61][30/233]	Loss 0.0021 (0.0213)	
training:	Epoch: [61][31/233]	Loss 0.0027 (0.0207)	
training:	Epoch: [61][32/233]	Loss 0.0131 (0.0205)	
training:	Epoch: [61][33/233]	Loss 0.0046 (0.0200)	
training:	Epoch: [61][34/233]	Loss 0.0029 (0.0195)	
training:	Epoch: [61][35/233]	Loss 0.0025 (0.0190)	
training:	Epoch: [61][36/233]	Loss 0.0028 (0.0186)	
training:	Epoch: [61][37/233]	Loss 0.0032 (0.0182)	
training:	Epoch: [61][38/233]	Loss 0.0024 (0.0177)	
training:	Epoch: [61][39/233]	Loss 0.0055 (0.0174)	
training:	Epoch: [61][40/233]	Loss 0.0026 (0.0171)	
training:	Epoch: [61][41/233]	Loss 0.0026 (0.0167)	
training:	Epoch: [61][42/233]	Loss 0.0026 (0.0164)	
training:	Epoch: [61][43/233]	Loss 0.0020 (0.0160)	
training:	Epoch: [61][44/233]	Loss 0.0043 (0.0158)	
training:	Epoch: [61][45/233]	Loss 0.0041 (0.0155)	
training:	Epoch: [61][46/233]	Loss 0.0036 (0.0153)	
training:	Epoch: [61][47/233]	Loss 0.0034 (0.0150)	
training:	Epoch: [61][48/233]	Loss 0.2046 (0.0190)	
training:	Epoch: [61][49/233]	Loss 0.0022 (0.0186)	
training:	Epoch: [61][50/233]	Loss 0.0447 (0.0191)	
training:	Epoch: [61][51/233]	Loss 0.0037 (0.0188)	
training:	Epoch: [61][52/233]	Loss 0.0025 (0.0185)	
training:	Epoch: [61][53/233]	Loss 0.0023 (0.0182)	
training:	Epoch: [61][54/233]	Loss 0.0023 (0.0179)	
training:	Epoch: [61][55/233]	Loss 0.0026 (0.0176)	
training:	Epoch: [61][56/233]	Loss 0.0050 (0.0174)	
training:	Epoch: [61][57/233]	Loss 0.0024 (0.0172)	
training:	Epoch: [61][58/233]	Loss 0.0116 (0.0171)	
training:	Epoch: [61][59/233]	Loss 0.0023 (0.0168)	
training:	Epoch: [61][60/233]	Loss 0.0055 (0.0166)	
training:	Epoch: [61][61/233]	Loss 0.0023 (0.0164)	
training:	Epoch: [61][62/233]	Loss 0.0041 (0.0162)	
training:	Epoch: [61][63/233]	Loss 0.0022 (0.0160)	
training:	Epoch: [61][64/233]	Loss 0.0022 (0.0157)	
training:	Epoch: [61][65/233]	Loss 0.1906 (0.0184)	
training:	Epoch: [61][66/233]	Loss 0.0024 (0.0182)	
training:	Epoch: [61][67/233]	Loss 0.0123 (0.0181)	
training:	Epoch: [61][68/233]	Loss 0.0024 (0.0179)	
training:	Epoch: [61][69/233]	Loss 0.0025 (0.0176)	
training:	Epoch: [61][70/233]	Loss 0.0245 (0.0177)	
training:	Epoch: [61][71/233]	Loss 0.0055 (0.0176)	
training:	Epoch: [61][72/233]	Loss 0.0026 (0.0174)	
training:	Epoch: [61][73/233]	Loss 0.0027 (0.0172)	
training:	Epoch: [61][74/233]	Loss 0.0025 (0.0170)	
training:	Epoch: [61][75/233]	Loss 0.0028 (0.0168)	
training:	Epoch: [61][76/233]	Loss 0.0029 (0.0166)	
training:	Epoch: [61][77/233]	Loss 0.0044 (0.0164)	
training:	Epoch: [61][78/233]	Loss 0.0170 (0.0164)	
training:	Epoch: [61][79/233]	Loss 0.0027 (0.0163)	
training:	Epoch: [61][80/233]	Loss 0.0025 (0.0161)	
training:	Epoch: [61][81/233]	Loss 0.1901 (0.0182)	
training:	Epoch: [61][82/233]	Loss 0.0024 (0.0181)	
training:	Epoch: [61][83/233]	Loss 0.0021 (0.0179)	
training:	Epoch: [61][84/233]	Loss 0.0021 (0.0177)	
training:	Epoch: [61][85/233]	Loss 0.0074 (0.0176)	
training:	Epoch: [61][86/233]	Loss 0.0021 (0.0174)	
training:	Epoch: [61][87/233]	Loss 0.0033 (0.0172)	
training:	Epoch: [61][88/233]	Loss 0.0024 (0.0170)	
training:	Epoch: [61][89/233]	Loss 0.0031 (0.0169)	
training:	Epoch: [61][90/233]	Loss 0.0020 (0.0167)	
training:	Epoch: [61][91/233]	Loss 0.0025 (0.0166)	
training:	Epoch: [61][92/233]	Loss 0.0027 (0.0164)	
training:	Epoch: [61][93/233]	Loss 0.0166 (0.0164)	
training:	Epoch: [61][94/233]	Loss 0.0028 (0.0163)	
training:	Epoch: [61][95/233]	Loss 0.0023 (0.0161)	
training:	Epoch: [61][96/233]	Loss 0.0023 (0.0160)	
training:	Epoch: [61][97/233]	Loss 0.0045 (0.0159)	
training:	Epoch: [61][98/233]	Loss 0.0020 (0.0157)	
training:	Epoch: [61][99/233]	Loss 0.0022 (0.0156)	
training:	Epoch: [61][100/233]	Loss 0.0023 (0.0154)	
training:	Epoch: [61][101/233]	Loss 0.0022 (0.0153)	
training:	Epoch: [61][102/233]	Loss 0.0048 (0.0152)	
training:	Epoch: [61][103/233]	Loss 0.0025 (0.0151)	
training:	Epoch: [61][104/233]	Loss 0.0046 (0.0150)	
training:	Epoch: [61][105/233]	Loss 0.0030 (0.0149)	
training:	Epoch: [61][106/233]	Loss 0.0173 (0.0149)	
training:	Epoch: [61][107/233]	Loss 0.0026 (0.0148)	
training:	Epoch: [61][108/233]	Loss 0.0027 (0.0147)	
training:	Epoch: [61][109/233]	Loss 0.0124 (0.0147)	
training:	Epoch: [61][110/233]	Loss 0.0026 (0.0145)	
training:	Epoch: [61][111/233]	Loss 0.0062 (0.0145)	
training:	Epoch: [61][112/233]	Loss 0.0668 (0.0149)	
training:	Epoch: [61][113/233]	Loss 0.0028 (0.0148)	
training:	Epoch: [61][114/233]	Loss 0.0097 (0.0148)	
training:	Epoch: [61][115/233]	Loss 0.0022 (0.0147)	
training:	Epoch: [61][116/233]	Loss 0.0096 (0.0146)	
training:	Epoch: [61][117/233]	Loss 0.0026 (0.0145)	
training:	Epoch: [61][118/233]	Loss 0.0020 (0.0144)	
training:	Epoch: [61][119/233]	Loss 0.0026 (0.0143)	
training:	Epoch: [61][120/233]	Loss 0.0036 (0.0142)	
training:	Epoch: [61][121/233]	Loss 0.0020 (0.0141)	
training:	Epoch: [61][122/233]	Loss 0.0026 (0.0140)	
training:	Epoch: [61][123/233]	Loss 0.0028 (0.0139)	
training:	Epoch: [61][124/233]	Loss 0.0022 (0.0138)	
training:	Epoch: [61][125/233]	Loss 0.0045 (0.0138)	
training:	Epoch: [61][126/233]	Loss 0.0027 (0.0137)	
training:	Epoch: [61][127/233]	Loss 0.0038 (0.0136)	
training:	Epoch: [61][128/233]	Loss 0.0040 (0.0135)	
training:	Epoch: [61][129/233]	Loss 0.0098 (0.0135)	
training:	Epoch: [61][130/233]	Loss 0.0028 (0.0134)	
training:	Epoch: [61][131/233]	Loss 0.0029 (0.0133)	
training:	Epoch: [61][132/233]	Loss 0.0411 (0.0136)	
training:	Epoch: [61][133/233]	Loss 0.0030 (0.0135)	
training:	Epoch: [61][134/233]	Loss 0.0732 (0.0139)	
training:	Epoch: [61][135/233]	Loss 0.0020 (0.0138)	
training:	Epoch: [61][136/233]	Loss 0.0022 (0.0137)	
training:	Epoch: [61][137/233]	Loss 0.0038 (0.0137)	
training:	Epoch: [61][138/233]	Loss 0.0031 (0.0136)	
training:	Epoch: [61][139/233]	Loss 0.2014 (0.0149)	
training:	Epoch: [61][140/233]	Loss 0.0049 (0.0149)	
training:	Epoch: [61][141/233]	Loss 0.0545 (0.0152)	
training:	Epoch: [61][142/233]	Loss 0.0064 (0.0151)	
training:	Epoch: [61][143/233]	Loss 0.0070 (0.0150)	
training:	Epoch: [61][144/233]	Loss 0.0032 (0.0150)	
training:	Epoch: [61][145/233]	Loss 0.0034 (0.0149)	
training:	Epoch: [61][146/233]	Loss 0.0326 (0.0150)	
training:	Epoch: [61][147/233]	Loss 0.0083 (0.0150)	
training:	Epoch: [61][148/233]	Loss 0.0024 (0.0149)	
training:	Epoch: [61][149/233]	Loss 0.0079 (0.0148)	
training:	Epoch: [61][150/233]	Loss 0.0027 (0.0147)	
training:	Epoch: [61][151/233]	Loss 0.0028 (0.0147)	
training:	Epoch: [61][152/233]	Loss 0.0024 (0.0146)	
training:	Epoch: [61][153/233]	Loss 0.0027 (0.0145)	
training:	Epoch: [61][154/233]	Loss 0.0030 (0.0144)	
training:	Epoch: [61][155/233]	Loss 0.0055 (0.0144)	
training:	Epoch: [61][156/233]	Loss 0.0031 (0.0143)	
training:	Epoch: [61][157/233]	Loss 0.0056 (0.0142)	
training:	Epoch: [61][158/233]	Loss 0.0021 (0.0142)	
training:	Epoch: [61][159/233]	Loss 0.0021 (0.0141)	
training:	Epoch: [61][160/233]	Loss 0.0290 (0.0142)	
training:	Epoch: [61][161/233]	Loss 0.0021 (0.0141)	
training:	Epoch: [61][162/233]	Loss 0.1018 (0.0146)	
training:	Epoch: [61][163/233]	Loss 0.0027 (0.0146)	
training:	Epoch: [61][164/233]	Loss 0.0031 (0.0145)	
training:	Epoch: [61][165/233]	Loss 0.0025 (0.0144)	
training:	Epoch: [61][166/233]	Loss 0.0026 (0.0144)	
training:	Epoch: [61][167/233]	Loss 0.0072 (0.0143)	
training:	Epoch: [61][168/233]	Loss 0.0020 (0.0142)	
training:	Epoch: [61][169/233]	Loss 0.0020 (0.0142)	
training:	Epoch: [61][170/233]	Loss 0.0025 (0.0141)	
training:	Epoch: [61][171/233]	Loss 0.0020 (0.0140)	
training:	Epoch: [61][172/233]	Loss 0.0106 (0.0140)	
training:	Epoch: [61][173/233]	Loss 0.0022 (0.0139)	
training:	Epoch: [61][174/233]	Loss 0.0037 (0.0139)	
training:	Epoch: [61][175/233]	Loss 0.0024 (0.0138)	
training:	Epoch: [61][176/233]	Loss 0.0028 (0.0138)	
training:	Epoch: [61][177/233]	Loss 0.0416 (0.0139)	
training:	Epoch: [61][178/233]	Loss 0.0301 (0.0140)	
training:	Epoch: [61][179/233]	Loss 0.0270 (0.0141)	
training:	Epoch: [61][180/233]	Loss 0.0023 (0.0140)	
training:	Epoch: [61][181/233]	Loss 0.1012 (0.0145)	
training:	Epoch: [61][182/233]	Loss 0.0021 (0.0144)	
training:	Epoch: [61][183/233]	Loss 0.0293 (0.0145)	
training:	Epoch: [61][184/233]	Loss 0.0032 (0.0144)	
training:	Epoch: [61][185/233]	Loss 0.0020 (0.0144)	
training:	Epoch: [61][186/233]	Loss 0.0020 (0.0143)	
training:	Epoch: [61][187/233]	Loss 0.0030 (0.0143)	
training:	Epoch: [61][188/233]	Loss 0.0021 (0.0142)	
training:	Epoch: [61][189/233]	Loss 0.0034 (0.0141)	
training:	Epoch: [61][190/233]	Loss 0.0022 (0.0141)	
training:	Epoch: [61][191/233]	Loss 0.0035 (0.0140)	
training:	Epoch: [61][192/233]	Loss 0.0026 (0.0140)	
training:	Epoch: [61][193/233]	Loss 0.0080 (0.0139)	
training:	Epoch: [61][194/233]	Loss 0.0031 (0.0139)	
training:	Epoch: [61][195/233]	Loss 0.1966 (0.0148)	
training:	Epoch: [61][196/233]	Loss 0.0024 (0.0147)	
training:	Epoch: [61][197/233]	Loss 0.0022 (0.0147)	
training:	Epoch: [61][198/233]	Loss 0.0023 (0.0146)	
training:	Epoch: [61][199/233]	Loss 0.0031 (0.0146)	
training:	Epoch: [61][200/233]	Loss 0.0086 (0.0145)	
training:	Epoch: [61][201/233]	Loss 0.0021 (0.0145)	
training:	Epoch: [61][202/233]	Loss 0.0029 (0.0144)	
training:	Epoch: [61][203/233]	Loss 0.0041 (0.0144)	
training:	Epoch: [61][204/233]	Loss 0.0046 (0.0143)	
training:	Epoch: [61][205/233]	Loss 0.0020 (0.0142)	
training:	Epoch: [61][206/233]	Loss 0.0022 (0.0142)	
training:	Epoch: [61][207/233]	Loss 0.0328 (0.0143)	
training:	Epoch: [61][208/233]	Loss 0.0032 (0.0142)	
training:	Epoch: [61][209/233]	Loss 0.0161 (0.0142)	
training:	Epoch: [61][210/233]	Loss 0.0020 (0.0142)	
training:	Epoch: [61][211/233]	Loss 0.0049 (0.0141)	
training:	Epoch: [61][212/233]	Loss 0.0028 (0.0141)	
training:	Epoch: [61][213/233]	Loss 0.0021 (0.0140)	
training:	Epoch: [61][214/233]	Loss 0.0113 (0.0140)	
training:	Epoch: [61][215/233]	Loss 0.0022 (0.0140)	
training:	Epoch: [61][216/233]	Loss 0.0418 (0.0141)	
training:	Epoch: [61][217/233]	Loss 0.0028 (0.0140)	
training:	Epoch: [61][218/233]	Loss 0.0045 (0.0140)	
training:	Epoch: [61][219/233]	Loss 0.0394 (0.0141)	
training:	Epoch: [61][220/233]	Loss 0.0036 (0.0141)	
training:	Epoch: [61][221/233]	Loss 0.0027 (0.0140)	
training:	Epoch: [61][222/233]	Loss 0.0029 (0.0140)	
training:	Epoch: [61][223/233]	Loss 0.0054 (0.0139)	
training:	Epoch: [61][224/233]	Loss 0.0056 (0.0139)	
training:	Epoch: [61][225/233]	Loss 0.0024 (0.0138)	
training:	Epoch: [61][226/233]	Loss 0.0025 (0.0138)	
training:	Epoch: [61][227/233]	Loss 0.0170 (0.0138)	
training:	Epoch: [61][228/233]	Loss 0.0867 (0.0141)	
training:	Epoch: [61][229/233]	Loss 0.0069 (0.0141)	
training:	Epoch: [61][230/233]	Loss 0.0019 (0.0140)	
training:	Epoch: [61][231/233]	Loss 0.0047 (0.0140)	
training:	Epoch: [61][232/233]	Loss 0.0031 (0.0139)	
training:	Epoch: [61][233/233]	Loss 0.0024 (0.0139)	
Training:	 Loss: 0.0139

Training:	 ACC: 0.9991 0.9991 0.9990 0.9992
Validation:	 ACC: 0.7866 0.7860 0.7743 0.7989
Validation:	 Best_BACC: 0.8162 0.8154 0.7998 0.8326
Validation:	 Loss: 1.0254
Pretraining:	Epoch 62/200
----------
training:	Epoch: [62][1/233]	Loss 0.0025 (0.0025)	
training:	Epoch: [62][2/233]	Loss 0.0019 (0.0022)	
training:	Epoch: [62][3/233]	Loss 0.0021 (0.0022)	
training:	Epoch: [62][4/233]	Loss 0.0273 (0.0085)	
training:	Epoch: [62][5/233]	Loss 0.0559 (0.0179)	
training:	Epoch: [62][6/233]	Loss 0.0022 (0.0153)	
training:	Epoch: [62][7/233]	Loss 0.0043 (0.0138)	
training:	Epoch: [62][8/233]	Loss 0.0032 (0.0124)	
training:	Epoch: [62][9/233]	Loss 0.0023 (0.0113)	
training:	Epoch: [62][10/233]	Loss 0.0090 (0.0111)	
training:	Epoch: [62][11/233]	Loss 0.0357 (0.0133)	
training:	Epoch: [62][12/233]	Loss 0.0034 (0.0125)	
training:	Epoch: [62][13/233]	Loss 0.0021 (0.0117)	
training:	Epoch: [62][14/233]	Loss 0.0021 (0.0110)	
training:	Epoch: [62][15/233]	Loss 0.0028 (0.0105)	
training:	Epoch: [62][16/233]	Loss 0.0156 (0.0108)	
training:	Epoch: [62][17/233]	Loss 0.0020 (0.0103)	
training:	Epoch: [62][18/233]	Loss 0.0028 (0.0098)	
training:	Epoch: [62][19/233]	Loss 0.0020 (0.0094)	
training:	Epoch: [62][20/233]	Loss 0.0019 (0.0091)	
training:	Epoch: [62][21/233]	Loss 0.0025 (0.0087)	
training:	Epoch: [62][22/233]	Loss 0.0100 (0.0088)	
training:	Epoch: [62][23/233]	Loss 0.0028 (0.0085)	
training:	Epoch: [62][24/233]	Loss 0.0022 (0.0083)	
training:	Epoch: [62][25/233]	Loss 0.0021 (0.0080)	
training:	Epoch: [62][26/233]	Loss 0.0028 (0.0078)	
training:	Epoch: [62][27/233]	Loss 0.0035 (0.0077)	
training:	Epoch: [62][28/233]	Loss 0.0020 (0.0075)	
training:	Epoch: [62][29/233]	Loss 0.0051 (0.0074)	
training:	Epoch: [62][30/233]	Loss 0.0021 (0.0072)	
training:	Epoch: [62][31/233]	Loss 0.0048 (0.0071)	
training:	Epoch: [62][32/233]	Loss 0.0022 (0.0070)	
training:	Epoch: [62][33/233]	Loss 0.0022 (0.0068)	
training:	Epoch: [62][34/233]	Loss 0.0046 (0.0068)	
training:	Epoch: [62][35/233]	Loss 0.0034 (0.0067)	
training:	Epoch: [62][36/233]	Loss 0.0023 (0.0065)	
training:	Epoch: [62][37/233]	Loss 0.0021 (0.0064)	
training:	Epoch: [62][38/233]	Loss 0.0022 (0.0063)	
training:	Epoch: [62][39/233]	Loss 0.0024 (0.0062)	
training:	Epoch: [62][40/233]	Loss 0.0056 (0.0062)	
training:	Epoch: [62][41/233]	Loss 0.0022 (0.0061)	
training:	Epoch: [62][42/233]	Loss 0.0175 (0.0064)	
training:	Epoch: [62][43/233]	Loss 0.0026 (0.0063)	
training:	Epoch: [62][44/233]	Loss 0.0023 (0.0062)	
training:	Epoch: [62][45/233]	Loss 0.1991 (0.0105)	
training:	Epoch: [62][46/233]	Loss 0.0021 (0.0103)	
training:	Epoch: [62][47/233]	Loss 0.0021 (0.0101)	
training:	Epoch: [62][48/233]	Loss 0.0021 (0.0100)	
training:	Epoch: [62][49/233]	Loss 0.0022 (0.0098)	
training:	Epoch: [62][50/233]	Loss 0.0063 (0.0097)	
training:	Epoch: [62][51/233]	Loss 0.1162 (0.0118)	
training:	Epoch: [62][52/233]	Loss 0.0074 (0.0117)	
training:	Epoch: [62][53/233]	Loss 0.0028 (0.0116)	
training:	Epoch: [62][54/233]	Loss 0.0053 (0.0114)	
training:	Epoch: [62][55/233]	Loss 0.0094 (0.0114)	
training:	Epoch: [62][56/233]	Loss 0.0022 (0.0112)	
training:	Epoch: [62][57/233]	Loss 0.0025 (0.0111)	
training:	Epoch: [62][58/233]	Loss 0.0026 (0.0109)	
training:	Epoch: [62][59/233]	Loss 0.0024 (0.0108)	
training:	Epoch: [62][60/233]	Loss 0.0109 (0.0108)	
training:	Epoch: [62][61/233]	Loss 0.0067 (0.0107)	
training:	Epoch: [62][62/233]	Loss 0.0026 (0.0106)	
training:	Epoch: [62][63/233]	Loss 0.0023 (0.0105)	
training:	Epoch: [62][64/233]	Loss 0.0348 (0.0109)	
training:	Epoch: [62][65/233]	Loss 0.0023 (0.0107)	
training:	Epoch: [62][66/233]	Loss 0.0080 (0.0107)	
training:	Epoch: [62][67/233]	Loss 0.0022 (0.0106)	
training:	Epoch: [62][68/233]	Loss 0.0020 (0.0104)	
training:	Epoch: [62][69/233]	Loss 0.1145 (0.0119)	
training:	Epoch: [62][70/233]	Loss 0.0023 (0.0118)	
training:	Epoch: [62][71/233]	Loss 0.0021 (0.0117)	
training:	Epoch: [62][72/233]	Loss 0.0762 (0.0126)	
training:	Epoch: [62][73/233]	Loss 0.0040 (0.0124)	
training:	Epoch: [62][74/233]	Loss 0.0351 (0.0127)	
training:	Epoch: [62][75/233]	Loss 0.0030 (0.0126)	
training:	Epoch: [62][76/233]	Loss 0.0028 (0.0125)	
training:	Epoch: [62][77/233]	Loss 0.0026 (0.0124)	
training:	Epoch: [62][78/233]	Loss 0.0036 (0.0122)	
training:	Epoch: [62][79/233]	Loss 0.0082 (0.0122)	
training:	Epoch: [62][80/233]	Loss 0.0830 (0.0131)	
training:	Epoch: [62][81/233]	Loss 0.0024 (0.0129)	
training:	Epoch: [62][82/233]	Loss 0.0028 (0.0128)	
training:	Epoch: [62][83/233]	Loss 0.0036 (0.0127)	
training:	Epoch: [62][84/233]	Loss 0.0032 (0.0126)	
training:	Epoch: [62][85/233]	Loss 0.1025 (0.0137)	
training:	Epoch: [62][86/233]	Loss 0.0121 (0.0136)	
training:	Epoch: [62][87/233]	Loss 0.0026 (0.0135)	
training:	Epoch: [62][88/233]	Loss 0.0032 (0.0134)	
training:	Epoch: [62][89/233]	Loss 0.0020 (0.0133)	
training:	Epoch: [62][90/233]	Loss 0.0028 (0.0132)	
training:	Epoch: [62][91/233]	Loss 0.0024 (0.0130)	
training:	Epoch: [62][92/233]	Loss 0.0033 (0.0129)	
training:	Epoch: [62][93/233]	Loss 0.0119 (0.0129)	
training:	Epoch: [62][94/233]	Loss 0.0038 (0.0128)	
training:	Epoch: [62][95/233]	Loss 0.0020 (0.0127)	
training:	Epoch: [62][96/233]	Loss 0.0021 (0.0126)	
training:	Epoch: [62][97/233]	Loss 0.0025 (0.0125)	
training:	Epoch: [62][98/233]	Loss 0.0374 (0.0127)	
training:	Epoch: [62][99/233]	Loss 0.0025 (0.0126)	
training:	Epoch: [62][100/233]	Loss 0.0021 (0.0125)	
training:	Epoch: [62][101/233]	Loss 0.0021 (0.0124)	
training:	Epoch: [62][102/233]	Loss 0.0059 (0.0124)	
training:	Epoch: [62][103/233]	Loss 0.0024 (0.0123)	
training:	Epoch: [62][104/233]	Loss 0.0021 (0.0122)	
training:	Epoch: [62][105/233]	Loss 0.0030 (0.0121)	
training:	Epoch: [62][106/233]	Loss 0.0021 (0.0120)	
training:	Epoch: [62][107/233]	Loss 0.1273 (0.0131)	
training:	Epoch: [62][108/233]	Loss 0.0023 (0.0130)	
training:	Epoch: [62][109/233]	Loss 0.0026 (0.0129)	
training:	Epoch: [62][110/233]	Loss 0.0019 (0.0128)	
training:	Epoch: [62][111/233]	Loss 0.0031 (0.0127)	
training:	Epoch: [62][112/233]	Loss 0.1637 (0.0140)	
training:	Epoch: [62][113/233]	Loss 0.0022 (0.0139)	
training:	Epoch: [62][114/233]	Loss 0.0055 (0.0139)	
training:	Epoch: [62][115/233]	Loss 0.0023 (0.0138)	
training:	Epoch: [62][116/233]	Loss 0.0032 (0.0137)	
training:	Epoch: [62][117/233]	Loss 0.0038 (0.0136)	
training:	Epoch: [62][118/233]	Loss 0.0029 (0.0135)	
training:	Epoch: [62][119/233]	Loss 0.0039 (0.0134)	
training:	Epoch: [62][120/233]	Loss 0.0044 (0.0133)	
training:	Epoch: [62][121/233]	Loss 0.0019 (0.0132)	
training:	Epoch: [62][122/233]	Loss 0.0023 (0.0132)	
training:	Epoch: [62][123/233]	Loss 0.0027 (0.0131)	
training:	Epoch: [62][124/233]	Loss 0.0027 (0.0130)	
training:	Epoch: [62][125/233]	Loss 0.0341 (0.0132)	
training:	Epoch: [62][126/233]	Loss 0.0037 (0.0131)	
training:	Epoch: [62][127/233]	Loss 0.0023 (0.0130)	
training:	Epoch: [62][128/233]	Loss 0.0030 (0.0129)	
training:	Epoch: [62][129/233]	Loss 0.0021 (0.0128)	
training:	Epoch: [62][130/233]	Loss 0.0023 (0.0127)	
training:	Epoch: [62][131/233]	Loss 0.0020 (0.0127)	
training:	Epoch: [62][132/233]	Loss 0.0039 (0.0126)	
training:	Epoch: [62][133/233]	Loss 0.0036 (0.0125)	
training:	Epoch: [62][134/233]	Loss 0.0074 (0.0125)	
training:	Epoch: [62][135/233]	Loss 0.0027 (0.0124)	
training:	Epoch: [62][136/233]	Loss 0.0022 (0.0123)	
training:	Epoch: [62][137/233]	Loss 0.0020 (0.0123)	
training:	Epoch: [62][138/233]	Loss 0.0053 (0.0122)	
training:	Epoch: [62][139/233]	Loss 0.0090 (0.0122)	
training:	Epoch: [62][140/233]	Loss 0.2071 (0.0136)	
training:	Epoch: [62][141/233]	Loss 0.0032 (0.0135)	
training:	Epoch: [62][142/233]	Loss 0.0028 (0.0134)	
training:	Epoch: [62][143/233]	Loss 0.1647 (0.0145)	
training:	Epoch: [62][144/233]	Loss 0.0104 (0.0145)	
training:	Epoch: [62][145/233]	Loss 0.0020 (0.0144)	
training:	Epoch: [62][146/233]	Loss 0.0023 (0.0143)	
training:	Epoch: [62][147/233]	Loss 0.0314 (0.0144)	
training:	Epoch: [62][148/233]	Loss 0.0020 (0.0143)	
training:	Epoch: [62][149/233]	Loss 0.1845 (0.0155)	
training:	Epoch: [62][150/233]	Loss 0.1982 (0.0167)	
training:	Epoch: [62][151/233]	Loss 0.0063 (0.0166)	
training:	Epoch: [62][152/233]	Loss 0.0020 (0.0165)	
training:	Epoch: [62][153/233]	Loss 0.0023 (0.0164)	
training:	Epoch: [62][154/233]	Loss 0.0095 (0.0164)	
training:	Epoch: [62][155/233]	Loss 0.0025 (0.0163)	
training:	Epoch: [62][156/233]	Loss 0.0098 (0.0163)	
training:	Epoch: [62][157/233]	Loss 0.0220 (0.0163)	
training:	Epoch: [62][158/233]	Loss 0.0038 (0.0162)	
training:	Epoch: [62][159/233]	Loss 0.0022 (0.0161)	
training:	Epoch: [62][160/233]	Loss 0.0467 (0.0163)	
training:	Epoch: [62][161/233]	Loss 0.0022 (0.0162)	
training:	Epoch: [62][162/233]	Loss 0.0431 (0.0164)	
training:	Epoch: [62][163/233]	Loss 0.0019 (0.0163)	
training:	Epoch: [62][164/233]	Loss 0.0031 (0.0162)	
training:	Epoch: [62][165/233]	Loss 0.0020 (0.0161)	
training:	Epoch: [62][166/233]	Loss 0.0026 (0.0161)	
training:	Epoch: [62][167/233]	Loss 0.0022 (0.0160)	
training:	Epoch: [62][168/233]	Loss 0.0042 (0.0159)	
training:	Epoch: [62][169/233]	Loss 0.1043 (0.0164)	
training:	Epoch: [62][170/233]	Loss 0.0024 (0.0163)	
training:	Epoch: [62][171/233]	Loss 0.0024 (0.0163)	
training:	Epoch: [62][172/233]	Loss 0.0037 (0.0162)	
training:	Epoch: [62][173/233]	Loss 0.0022 (0.0161)	
training:	Epoch: [62][174/233]	Loss 0.0019 (0.0160)	
training:	Epoch: [62][175/233]	Loss 0.0020 (0.0160)	
training:	Epoch: [62][176/233]	Loss 0.1971 (0.0170)	
training:	Epoch: [62][177/233]	Loss 0.0026 (0.0169)	
training:	Epoch: [62][178/233]	Loss 0.0027 (0.0168)	
training:	Epoch: [62][179/233]	Loss 0.0019 (0.0167)	
training:	Epoch: [62][180/233]	Loss 0.0038 (0.0167)	
training:	Epoch: [62][181/233]	Loss 0.0034 (0.0166)	
training:	Epoch: [62][182/233]	Loss 0.0047 (0.0165)	
training:	Epoch: [62][183/233]	Loss 0.0047 (0.0165)	
training:	Epoch: [62][184/233]	Loss 0.1442 (0.0172)	
training:	Epoch: [62][185/233]	Loss 0.0070 (0.0171)	
training:	Epoch: [62][186/233]	Loss 0.0029 (0.0170)	
training:	Epoch: [62][187/233]	Loss 0.1953 (0.0180)	
training:	Epoch: [62][188/233]	Loss 0.0020 (0.0179)	
training:	Epoch: [62][189/233]	Loss 0.0024 (0.0178)	
training:	Epoch: [62][190/233]	Loss 0.0040 (0.0177)	
training:	Epoch: [62][191/233]	Loss 0.0038 (0.0177)	
training:	Epoch: [62][192/233]	Loss 0.1395 (0.0183)	
training:	Epoch: [62][193/233]	Loss 0.0019 (0.0182)	
training:	Epoch: [62][194/233]	Loss 0.0024 (0.0181)	
training:	Epoch: [62][195/233]	Loss 0.0020 (0.0181)	
training:	Epoch: [62][196/233]	Loss 0.0160 (0.0180)	
training:	Epoch: [62][197/233]	Loss 0.0025 (0.0180)	
training:	Epoch: [62][198/233]	Loss 0.0027 (0.0179)	
training:	Epoch: [62][199/233]	Loss 0.0036 (0.0178)	
training:	Epoch: [62][200/233]	Loss 0.0029 (0.0177)	
training:	Epoch: [62][201/233]	Loss 0.0023 (0.0177)	
training:	Epoch: [62][202/233]	Loss 0.0021 (0.0176)	
training:	Epoch: [62][203/233]	Loss 0.0036 (0.0175)	
training:	Epoch: [62][204/233]	Loss 0.0184 (0.0175)	
training:	Epoch: [62][205/233]	Loss 0.0025 (0.0174)	
training:	Epoch: [62][206/233]	Loss 0.0024 (0.0174)	
training:	Epoch: [62][207/233]	Loss 0.0155 (0.0174)	
training:	Epoch: [62][208/233]	Loss 0.0036 (0.0173)	
training:	Epoch: [62][209/233]	Loss 0.0021 (0.0172)	
training:	Epoch: [62][210/233]	Loss 0.0084 (0.0172)	
training:	Epoch: [62][211/233]	Loss 0.0126 (0.0172)	
training:	Epoch: [62][212/233]	Loss 0.0046 (0.0171)	
training:	Epoch: [62][213/233]	Loss 0.0023 (0.0170)	
training:	Epoch: [62][214/233]	Loss 0.0048 (0.0170)	
training:	Epoch: [62][215/233]	Loss 0.0022 (0.0169)	
training:	Epoch: [62][216/233]	Loss 0.0024 (0.0168)	
training:	Epoch: [62][217/233]	Loss 0.0020 (0.0168)	
training:	Epoch: [62][218/233]	Loss 0.1451 (0.0174)	
training:	Epoch: [62][219/233]	Loss 0.0027 (0.0173)	
training:	Epoch: [62][220/233]	Loss 0.0022 (0.0172)	
training:	Epoch: [62][221/233]	Loss 0.0049 (0.0172)	
training:	Epoch: [62][222/233]	Loss 0.0027 (0.0171)	
training:	Epoch: [62][223/233]	Loss 0.0484 (0.0172)	
training:	Epoch: [62][224/233]	Loss 0.0024 (0.0172)	
training:	Epoch: [62][225/233]	Loss 0.0022 (0.0171)	
training:	Epoch: [62][226/233]	Loss 0.0022 (0.0170)	
training:	Epoch: [62][227/233]	Loss 0.0022 (0.0170)	
training:	Epoch: [62][228/233]	Loss 0.0021 (0.0169)	
training:	Epoch: [62][229/233]	Loss 0.0298 (0.0170)	
training:	Epoch: [62][230/233]	Loss 0.0019 (0.0169)	
training:	Epoch: [62][231/233]	Loss 0.0053 (0.0169)	
training:	Epoch: [62][232/233]	Loss 0.0033 (0.0168)	
training:	Epoch: [62][233/233]	Loss 0.0020 (0.0167)	
Training:	 Loss: 0.0167

Training:	 ACC: 0.9991 0.9991 0.9990 0.9992
Validation:	 ACC: 0.7936 0.7935 0.7916 0.7955
Validation:	 Best_BACC: 0.8162 0.8154 0.7998 0.8326
Validation:	 Loss: 1.0192
Pretraining:	Epoch 63/200
----------
training:	Epoch: [63][1/233]	Loss 0.0021 (0.0021)	
training:	Epoch: [63][2/233]	Loss 0.0019 (0.0020)	
training:	Epoch: [63][3/233]	Loss 0.0031 (0.0024)	
training:	Epoch: [63][4/233]	Loss 0.0022 (0.0023)	
training:	Epoch: [63][5/233]	Loss 0.1977 (0.0414)	
training:	Epoch: [63][6/233]	Loss 0.0024 (0.0349)	
training:	Epoch: [63][7/233]	Loss 0.0018 (0.0302)	
training:	Epoch: [63][8/233]	Loss 0.0023 (0.0267)	
training:	Epoch: [63][9/233]	Loss 0.0025 (0.0240)	
training:	Epoch: [63][10/233]	Loss 0.0042 (0.0220)	
training:	Epoch: [63][11/233]	Loss 0.0022 (0.0202)	
training:	Epoch: [63][12/233]	Loss 0.0023 (0.0187)	
training:	Epoch: [63][13/233]	Loss 0.0023 (0.0175)	
training:	Epoch: [63][14/233]	Loss 0.0786 (0.0218)	
training:	Epoch: [63][15/233]	Loss 0.0029 (0.0206)	
training:	Epoch: [63][16/233]	Loss 0.0631 (0.0232)	
training:	Epoch: [63][17/233]	Loss 0.0021 (0.0220)	
training:	Epoch: [63][18/233]	Loss 0.0143 (0.0216)	
training:	Epoch: [63][19/233]	Loss 0.0025 (0.0206)	
training:	Epoch: [63][20/233]	Loss 0.0030 (0.0197)	
training:	Epoch: [63][21/233]	Loss 0.0026 (0.0189)	
training:	Epoch: [63][22/233]	Loss 0.0051 (0.0182)	
training:	Epoch: [63][23/233]	Loss 0.0024 (0.0176)	
training:	Epoch: [63][24/233]	Loss 0.0048 (0.0170)	
training:	Epoch: [63][25/233]	Loss 0.0030 (0.0165)	
training:	Epoch: [63][26/233]	Loss 0.0050 (0.0160)	
training:	Epoch: [63][27/233]	Loss 0.0020 (0.0155)	
training:	Epoch: [63][28/233]	Loss 0.0021 (0.0150)	
training:	Epoch: [63][29/233]	Loss 0.0020 (0.0146)	
training:	Epoch: [63][30/233]	Loss 0.0021 (0.0142)	
training:	Epoch: [63][31/233]	Loss 0.0028 (0.0138)	
training:	Epoch: [63][32/233]	Loss 0.0062 (0.0136)	
training:	Epoch: [63][33/233]	Loss 0.0024 (0.0132)	
training:	Epoch: [63][34/233]	Loss 0.0437 (0.0141)	
training:	Epoch: [63][35/233]	Loss 0.0054 (0.0139)	
training:	Epoch: [63][36/233]	Loss 0.0029 (0.0136)	
training:	Epoch: [63][37/233]	Loss 0.0028 (0.0133)	
training:	Epoch: [63][38/233]	Loss 0.0120 (0.0132)	
training:	Epoch: [63][39/233]	Loss 0.0021 (0.0130)	
training:	Epoch: [63][40/233]	Loss 0.0023 (0.0127)	
training:	Epoch: [63][41/233]	Loss 0.0048 (0.0125)	
training:	Epoch: [63][42/233]	Loss 0.0156 (0.0126)	
training:	Epoch: [63][43/233]	Loss 0.0567 (0.0136)	
training:	Epoch: [63][44/233]	Loss 0.0066 (0.0134)	
training:	Epoch: [63][45/233]	Loss 0.0022 (0.0132)	
training:	Epoch: [63][46/233]	Loss 0.1419 (0.0160)	
training:	Epoch: [63][47/233]	Loss 0.0030 (0.0157)	
training:	Epoch: [63][48/233]	Loss 0.1480 (0.0185)	
training:	Epoch: [63][49/233]	Loss 0.0027 (0.0181)	
training:	Epoch: [63][50/233]	Loss 0.0037 (0.0178)	
training:	Epoch: [63][51/233]	Loss 0.1064 (0.0196)	
training:	Epoch: [63][52/233]	Loss 0.0081 (0.0194)	
training:	Epoch: [63][53/233]	Loss 0.0028 (0.0191)	
training:	Epoch: [63][54/233]	Loss 0.0021 (0.0187)	
training:	Epoch: [63][55/233]	Loss 0.0045 (0.0185)	
training:	Epoch: [63][56/233]	Loss 0.0028 (0.0182)	
training:	Epoch: [63][57/233]	Loss 0.0026 (0.0179)	
training:	Epoch: [63][58/233]	Loss 0.0021 (0.0177)	
training:	Epoch: [63][59/233]	Loss 0.0631 (0.0184)	
training:	Epoch: [63][60/233]	Loss 0.0172 (0.0184)	
training:	Epoch: [63][61/233]	Loss 0.0023 (0.0181)	
training:	Epoch: [63][62/233]	Loss 0.0030 (0.0179)	
training:	Epoch: [63][63/233]	Loss 0.0026 (0.0177)	
training:	Epoch: [63][64/233]	Loss 0.0019 (0.0174)	
training:	Epoch: [63][65/233]	Loss 0.0023 (0.0172)	
training:	Epoch: [63][66/233]	Loss 0.0084 (0.0170)	
training:	Epoch: [63][67/233]	Loss 0.0032 (0.0168)	
training:	Epoch: [63][68/233]	Loss 0.0019 (0.0166)	
training:	Epoch: [63][69/233]	Loss 0.0022 (0.0164)	
training:	Epoch: [63][70/233]	Loss 0.0022 (0.0162)	
training:	Epoch: [63][71/233]	Loss 0.0035 (0.0160)	
training:	Epoch: [63][72/233]	Loss 0.0021 (0.0158)	
training:	Epoch: [63][73/233]	Loss 0.0073 (0.0157)	
training:	Epoch: [63][74/233]	Loss 0.0038 (0.0156)	
training:	Epoch: [63][75/233]	Loss 0.0057 (0.0154)	
training:	Epoch: [63][76/233]	Loss 0.0031 (0.0153)	
training:	Epoch: [63][77/233]	Loss 0.0062 (0.0151)	
training:	Epoch: [63][78/233]	Loss 0.0036 (0.0150)	
training:	Epoch: [63][79/233]	Loss 0.0182 (0.0150)	
training:	Epoch: [63][80/233]	Loss 0.0021 (0.0149)	
training:	Epoch: [63][81/233]	Loss 0.0023 (0.0147)	
training:	Epoch: [63][82/233]	Loss 0.0323 (0.0149)	
training:	Epoch: [63][83/233]	Loss 0.0026 (0.0148)	
training:	Epoch: [63][84/233]	Loss 0.0033 (0.0146)	
training:	Epoch: [63][85/233]	Loss 0.0035 (0.0145)	
training:	Epoch: [63][86/233]	Loss 0.0022 (0.0144)	
training:	Epoch: [63][87/233]	Loss 0.0026 (0.0142)	
training:	Epoch: [63][88/233]	Loss 0.0021 (0.0141)	
training:	Epoch: [63][89/233]	Loss 0.0971 (0.0150)	
training:	Epoch: [63][90/233]	Loss 0.0123 (0.0150)	
training:	Epoch: [63][91/233]	Loss 0.0072 (0.0149)	
training:	Epoch: [63][92/233]	Loss 0.0021 (0.0148)	
training:	Epoch: [63][93/233]	Loss 0.0024 (0.0146)	
training:	Epoch: [63][94/233]	Loss 0.0019 (0.0145)	
training:	Epoch: [63][95/233]	Loss 0.0035 (0.0144)	
training:	Epoch: [63][96/233]	Loss 0.0033 (0.0143)	
training:	Epoch: [63][97/233]	Loss 0.1888 (0.0161)	
training:	Epoch: [63][98/233]	Loss 0.0031 (0.0159)	
training:	Epoch: [63][99/233]	Loss 0.0023 (0.0158)	
training:	Epoch: [63][100/233]	Loss 0.0031 (0.0157)	
training:	Epoch: [63][101/233]	Loss 0.0107 (0.0156)	
training:	Epoch: [63][102/233]	Loss 0.0080 (0.0156)	
training:	Epoch: [63][103/233]	Loss 0.0023 (0.0154)	
training:	Epoch: [63][104/233]	Loss 0.0046 (0.0153)	
training:	Epoch: [63][105/233]	Loss 0.0023 (0.0152)	
training:	Epoch: [63][106/233]	Loss 0.0021 (0.0151)	
training:	Epoch: [63][107/233]	Loss 0.0145 (0.0151)	
training:	Epoch: [63][108/233]	Loss 0.0030 (0.0150)	
training:	Epoch: [63][109/233]	Loss 0.0026 (0.0148)	
training:	Epoch: [63][110/233]	Loss 0.0079 (0.0148)	
training:	Epoch: [63][111/233]	Loss 0.0025 (0.0147)	
training:	Epoch: [63][112/233]	Loss 0.2106 (0.0164)	
training:	Epoch: [63][113/233]	Loss 0.0022 (0.0163)	
training:	Epoch: [63][114/233]	Loss 0.0020 (0.0162)	
training:	Epoch: [63][115/233]	Loss 0.0027 (0.0160)	
training:	Epoch: [63][116/233]	Loss 0.0026 (0.0159)	
training:	Epoch: [63][117/233]	Loss 0.0025 (0.0158)	
training:	Epoch: [63][118/233]	Loss 0.0020 (0.0157)	
training:	Epoch: [63][119/233]	Loss 0.0020 (0.0156)	
training:	Epoch: [63][120/233]	Loss 0.0027 (0.0155)	
training:	Epoch: [63][121/233]	Loss 0.0021 (0.0154)	
training:	Epoch: [63][122/233]	Loss 0.0320 (0.0155)	
training:	Epoch: [63][123/233]	Loss 0.0022 (0.0154)	
training:	Epoch: [63][124/233]	Loss 0.0022 (0.0153)	
training:	Epoch: [63][125/233]	Loss 0.0036 (0.0152)	
training:	Epoch: [63][126/233]	Loss 0.0018 (0.0151)	
training:	Epoch: [63][127/233]	Loss 0.0024 (0.0150)	
training:	Epoch: [63][128/233]	Loss 0.0045 (0.0149)	
training:	Epoch: [63][129/233]	Loss 0.0075 (0.0149)	
training:	Epoch: [63][130/233]	Loss 0.0048 (0.0148)	
training:	Epoch: [63][131/233]	Loss 0.0039 (0.0147)	
training:	Epoch: [63][132/233]	Loss 0.0021 (0.0146)	
training:	Epoch: [63][133/233]	Loss 0.0021 (0.0145)	
training:	Epoch: [63][134/233]	Loss 0.0036 (0.0144)	
training:	Epoch: [63][135/233]	Loss 0.0492 (0.0147)	
training:	Epoch: [63][136/233]	Loss 0.0060 (0.0146)	
training:	Epoch: [63][137/233]	Loss 0.0022 (0.0145)	
training:	Epoch: [63][138/233]	Loss 0.0019 (0.0144)	
training:	Epoch: [63][139/233]	Loss 0.0023 (0.0143)	
training:	Epoch: [63][140/233]	Loss 0.0024 (0.0143)	
training:	Epoch: [63][141/233]	Loss 0.1877 (0.0155)	
training:	Epoch: [63][142/233]	Loss 0.0019 (0.0154)	
training:	Epoch: [63][143/233]	Loss 0.2099 (0.0168)	
training:	Epoch: [63][144/233]	Loss 0.0022 (0.0167)	
training:	Epoch: [63][145/233]	Loss 0.0023 (0.0166)	
training:	Epoch: [63][146/233]	Loss 0.1717 (0.0176)	
training:	Epoch: [63][147/233]	Loss 0.0022 (0.0175)	
training:	Epoch: [63][148/233]	Loss 0.0019 (0.0174)	
training:	Epoch: [63][149/233]	Loss 0.0036 (0.0173)	
training:	Epoch: [63][150/233]	Loss 0.0888 (0.0178)	
training:	Epoch: [63][151/233]	Loss 0.0024 (0.0177)	
training:	Epoch: [63][152/233]	Loss 0.0023 (0.0176)	
training:	Epoch: [63][153/233]	Loss 0.0030 (0.0175)	
training:	Epoch: [63][154/233]	Loss 0.0032 (0.0174)	
training:	Epoch: [63][155/233]	Loss 0.0021 (0.0173)	
training:	Epoch: [63][156/233]	Loss 0.0367 (0.0174)	
training:	Epoch: [63][157/233]	Loss 0.0153 (0.0174)	
training:	Epoch: [63][158/233]	Loss 0.0923 (0.0179)	
training:	Epoch: [63][159/233]	Loss 0.0025 (0.0178)	
training:	Epoch: [63][160/233]	Loss 0.0020 (0.0177)	
training:	Epoch: [63][161/233]	Loss 0.0075 (0.0176)	
training:	Epoch: [63][162/233]	Loss 0.0335 (0.0177)	
training:	Epoch: [63][163/233]	Loss 0.0044 (0.0176)	
training:	Epoch: [63][164/233]	Loss 0.1965 (0.0187)	
training:	Epoch: [63][165/233]	Loss 0.0032 (0.0186)	
training:	Epoch: [63][166/233]	Loss 0.0111 (0.0186)	
training:	Epoch: [63][167/233]	Loss 0.0026 (0.0185)	
training:	Epoch: [63][168/233]	Loss 0.0026 (0.0184)	
training:	Epoch: [63][169/233]	Loss 0.0021 (0.0183)	
training:	Epoch: [63][170/233]	Loss 0.0051 (0.0182)	
training:	Epoch: [63][171/233]	Loss 0.0028 (0.0181)	
training:	Epoch: [63][172/233]	Loss 0.0019 (0.0180)	
training:	Epoch: [63][173/233]	Loss 0.0183 (0.0180)	
training:	Epoch: [63][174/233]	Loss 0.0032 (0.0180)	
training:	Epoch: [63][175/233]	Loss 0.0035 (0.0179)	
training:	Epoch: [63][176/233]	Loss 0.0021 (0.0178)	
training:	Epoch: [63][177/233]	Loss 0.0020 (0.0177)	
training:	Epoch: [63][178/233]	Loss 0.0023 (0.0176)	
training:	Epoch: [63][179/233]	Loss 0.0040 (0.0175)	
training:	Epoch: [63][180/233]	Loss 0.0021 (0.0174)	
training:	Epoch: [63][181/233]	Loss 0.0028 (0.0174)	
training:	Epoch: [63][182/233]	Loss 0.0020 (0.0173)	
training:	Epoch: [63][183/233]	Loss 0.0028 (0.0172)	
training:	Epoch: [63][184/233]	Loss 0.0283 (0.0173)	
training:	Epoch: [63][185/233]	Loss 0.0040 (0.0172)	
training:	Epoch: [63][186/233]	Loss 0.0090 (0.0171)	
training:	Epoch: [63][187/233]	Loss 0.0022 (0.0171)	
training:	Epoch: [63][188/233]	Loss 0.2015 (0.0181)	
training:	Epoch: [63][189/233]	Loss 0.0018 (0.0180)	
training:	Epoch: [63][190/233]	Loss 0.0087 (0.0179)	
training:	Epoch: [63][191/233]	Loss 0.0020 (0.0178)	
training:	Epoch: [63][192/233]	Loss 0.0027 (0.0178)	
training:	Epoch: [63][193/233]	Loss 0.0022 (0.0177)	
training:	Epoch: [63][194/233]	Loss 0.0019 (0.0176)	
training:	Epoch: [63][195/233]	Loss 0.0027 (0.0175)	
training:	Epoch: [63][196/233]	Loss 0.0023 (0.0174)	
training:	Epoch: [63][197/233]	Loss 0.0136 (0.0174)	
training:	Epoch: [63][198/233]	Loss 0.0029 (0.0173)	
training:	Epoch: [63][199/233]	Loss 0.1444 (0.0180)	
training:	Epoch: [63][200/233]	Loss 0.0021 (0.0179)	
training:	Epoch: [63][201/233]	Loss 0.0026 (0.0178)	
training:	Epoch: [63][202/233]	Loss 0.0052 (0.0178)	
training:	Epoch: [63][203/233]	Loss 0.0127 (0.0177)	
training:	Epoch: [63][204/233]	Loss 0.0029 (0.0177)	
training:	Epoch: [63][205/233]	Loss 0.0023 (0.0176)	
training:	Epoch: [63][206/233]	Loss 0.0028 (0.0175)	
training:	Epoch: [63][207/233]	Loss 0.0049 (0.0175)	
training:	Epoch: [63][208/233]	Loss 0.0021 (0.0174)	
training:	Epoch: [63][209/233]	Loss 0.0025 (0.0173)	
training:	Epoch: [63][210/233]	Loss 0.0038 (0.0173)	
training:	Epoch: [63][211/233]	Loss 0.0624 (0.0175)	
training:	Epoch: [63][212/233]	Loss 0.0051 (0.0174)	
training:	Epoch: [63][213/233]	Loss 0.0020 (0.0173)	
training:	Epoch: [63][214/233]	Loss 0.0077 (0.0173)	
training:	Epoch: [63][215/233]	Loss 0.0035 (0.0172)	
training:	Epoch: [63][216/233]	Loss 0.0208 (0.0172)	
training:	Epoch: [63][217/233]	Loss 0.0075 (0.0172)	
training:	Epoch: [63][218/233]	Loss 0.0023 (0.0171)	
training:	Epoch: [63][219/233]	Loss 0.0096 (0.0171)	
training:	Epoch: [63][220/233]	Loss 0.0021 (0.0170)	
training:	Epoch: [63][221/233]	Loss 0.0045 (0.0170)	
training:	Epoch: [63][222/233]	Loss 0.0027 (0.0169)	
training:	Epoch: [63][223/233]	Loss 0.0021 (0.0168)	
training:	Epoch: [63][224/233]	Loss 0.0021 (0.0168)	
training:	Epoch: [63][225/233]	Loss 0.0868 (0.0171)	
training:	Epoch: [63][226/233]	Loss 0.0042 (0.0170)	
training:	Epoch: [63][227/233]	Loss 0.0024 (0.0170)	
training:	Epoch: [63][228/233]	Loss 0.0022 (0.0169)	
training:	Epoch: [63][229/233]	Loss 0.1922 (0.0177)	
training:	Epoch: [63][230/233]	Loss 0.0057 (0.0176)	
training:	Epoch: [63][231/233]	Loss 0.0023 (0.0175)	
training:	Epoch: [63][232/233]	Loss 0.0020 (0.0175)	
training:	Epoch: [63][233/233]	Loss 0.1241 (0.0179)	
Training:	 Loss: 0.0179

Training:	 ACC: 0.9992 0.9992 0.9992 0.9992
Validation:	 ACC: 0.7931 0.7935 0.8008 0.7854
Validation:	 Best_BACC: 0.8162 0.8154 0.7998 0.8326
Validation:	 Loss: 0.9898
Pretraining:	Epoch 64/200
----------
training:	Epoch: [64][1/233]	Loss 0.0033 (0.0033)	
training:	Epoch: [64][2/233]	Loss 0.0032 (0.0032)	
training:	Epoch: [64][3/233]	Loss 0.0019 (0.0028)	
training:	Epoch: [64][4/233]	Loss 0.0027 (0.0028)	
training:	Epoch: [64][5/233]	Loss 0.0023 (0.0027)	
training:	Epoch: [64][6/233]	Loss 0.0049 (0.0030)	
training:	Epoch: [64][7/233]	Loss 0.0020 (0.0029)	
training:	Epoch: [64][8/233]	Loss 0.0024 (0.0028)	
training:	Epoch: [64][9/233]	Loss 0.0076 (0.0034)	
training:	Epoch: [64][10/233]	Loss 0.0022 (0.0032)	
training:	Epoch: [64][11/233]	Loss 0.2018 (0.0213)	
training:	Epoch: [64][12/233]	Loss 0.2076 (0.0368)	
training:	Epoch: [64][13/233]	Loss 0.0034 (0.0343)	
training:	Epoch: [64][14/233]	Loss 0.0021 (0.0320)	
training:	Epoch: [64][15/233]	Loss 0.0048 (0.0301)	
training:	Epoch: [64][16/233]	Loss 0.0099 (0.0289)	
training:	Epoch: [64][17/233]	Loss 0.0033 (0.0274)	
training:	Epoch: [64][18/233]	Loss 0.0077 (0.0263)	
training:	Epoch: [64][19/233]	Loss 0.0028 (0.0250)	
training:	Epoch: [64][20/233]	Loss 0.0019 (0.0239)	
training:	Epoch: [64][21/233]	Loss 0.0038 (0.0229)	
training:	Epoch: [64][22/233]	Loss 0.0022 (0.0220)	
training:	Epoch: [64][23/233]	Loss 0.0059 (0.0213)	
training:	Epoch: [64][24/233]	Loss 0.0688 (0.0233)	
training:	Epoch: [64][25/233]	Loss 0.0026 (0.0225)	
training:	Epoch: [64][26/233]	Loss 0.0028 (0.0217)	
training:	Epoch: [64][27/233]	Loss 0.0051 (0.0211)	
training:	Epoch: [64][28/233]	Loss 0.0030 (0.0204)	
training:	Epoch: [64][29/233]	Loss 0.0024 (0.0198)	
training:	Epoch: [64][30/233]	Loss 0.0036 (0.0193)	
training:	Epoch: [64][31/233]	Loss 0.0021 (0.0187)	
training:	Epoch: [64][32/233]	Loss 0.0027 (0.0182)	
training:	Epoch: [64][33/233]	Loss 0.0137 (0.0181)	
training:	Epoch: [64][34/233]	Loss 0.0091 (0.0178)	
training:	Epoch: [64][35/233]	Loss 0.0023 (0.0174)	
training:	Epoch: [64][36/233]	Loss 0.0530 (0.0184)	
training:	Epoch: [64][37/233]	Loss 0.0022 (0.0179)	
training:	Epoch: [64][38/233]	Loss 0.0022 (0.0175)	
training:	Epoch: [64][39/233]	Loss 0.0023 (0.0171)	
training:	Epoch: [64][40/233]	Loss 0.0025 (0.0168)	
training:	Epoch: [64][41/233]	Loss 0.0034 (0.0164)	
training:	Epoch: [64][42/233]	Loss 0.0025 (0.0161)	
training:	Epoch: [64][43/233]	Loss 0.0058 (0.0159)	
training:	Epoch: [64][44/233]	Loss 0.0020 (0.0155)	
training:	Epoch: [64][45/233]	Loss 0.0020 (0.0152)	
training:	Epoch: [64][46/233]	Loss 0.2248 (0.0198)	
training:	Epoch: [64][47/233]	Loss 0.0028 (0.0194)	
training:	Epoch: [64][48/233]	Loss 0.0189 (0.0194)	
training:	Epoch: [64][49/233]	Loss 0.0020 (0.0191)	
training:	Epoch: [64][50/233]	Loss 0.0025 (0.0187)	
training:	Epoch: [64][51/233]	Loss 0.0033 (0.0184)	
training:	Epoch: [64][52/233]	Loss 0.0031 (0.0181)	
training:	Epoch: [64][53/233]	Loss 0.0036 (0.0179)	
training:	Epoch: [64][54/233]	Loss 0.0036 (0.0176)	
training:	Epoch: [64][55/233]	Loss 0.0024 (0.0173)	
training:	Epoch: [64][56/233]	Loss 0.0021 (0.0171)	
training:	Epoch: [64][57/233]	Loss 0.0066 (0.0169)	
training:	Epoch: [64][58/233]	Loss 0.0027 (0.0166)	
training:	Epoch: [64][59/233]	Loss 0.0176 (0.0166)	
training:	Epoch: [64][60/233]	Loss 0.0023 (0.0164)	
training:	Epoch: [64][61/233]	Loss 0.0021 (0.0162)	
training:	Epoch: [64][62/233]	Loss 0.0025 (0.0160)	
training:	Epoch: [64][63/233]	Loss 0.0075 (0.0158)	
training:	Epoch: [64][64/233]	Loss 0.0344 (0.0161)	
training:	Epoch: [64][65/233]	Loss 0.0242 (0.0162)	
training:	Epoch: [64][66/233]	Loss 0.0019 (0.0160)	
training:	Epoch: [64][67/233]	Loss 0.0022 (0.0158)	
training:	Epoch: [64][68/233]	Loss 0.0020 (0.0156)	
training:	Epoch: [64][69/233]	Loss 0.0024 (0.0154)	
training:	Epoch: [64][70/233]	Loss 0.0022 (0.0152)	
training:	Epoch: [64][71/233]	Loss 0.0025 (0.0150)	
training:	Epoch: [64][72/233]	Loss 0.0033 (0.0149)	
training:	Epoch: [64][73/233]	Loss 0.0020 (0.0147)	
training:	Epoch: [64][74/233]	Loss 0.0029 (0.0145)	
training:	Epoch: [64][75/233]	Loss 0.0049 (0.0144)	
training:	Epoch: [64][76/233]	Loss 0.0024 (0.0143)	
training:	Epoch: [64][77/233]	Loss 0.0020 (0.0141)	
training:	Epoch: [64][78/233]	Loss 0.0096 (0.0140)	
training:	Epoch: [64][79/233]	Loss 0.0028 (0.0139)	
training:	Epoch: [64][80/233]	Loss 0.0249 (0.0140)	
training:	Epoch: [64][81/233]	Loss 0.0050 (0.0139)	
training:	Epoch: [64][82/233]	Loss 0.0486 (0.0143)	
training:	Epoch: [64][83/233]	Loss 0.0031 (0.0142)	
training:	Epoch: [64][84/233]	Loss 0.0028 (0.0141)	
training:	Epoch: [64][85/233]	Loss 0.0030 (0.0139)	
training:	Epoch: [64][86/233]	Loss 0.0019 (0.0138)	
training:	Epoch: [64][87/233]	Loss 0.0020 (0.0137)	
training:	Epoch: [64][88/233]	Loss 0.0020 (0.0135)	
training:	Epoch: [64][89/233]	Loss 0.0021 (0.0134)	
training:	Epoch: [64][90/233]	Loss 0.0020 (0.0133)	
training:	Epoch: [64][91/233]	Loss 0.0020 (0.0132)	
training:	Epoch: [64][92/233]	Loss 0.0046 (0.0131)	
training:	Epoch: [64][93/233]	Loss 0.0018 (0.0129)	
training:	Epoch: [64][94/233]	Loss 0.0026 (0.0128)	
training:	Epoch: [64][95/233]	Loss 0.0031 (0.0127)	
training:	Epoch: [64][96/233]	Loss 0.0428 (0.0130)	
training:	Epoch: [64][97/233]	Loss 0.0020 (0.0129)	
training:	Epoch: [64][98/233]	Loss 0.0026 (0.0128)	
training:	Epoch: [64][99/233]	Loss 0.0021 (0.0127)	
training:	Epoch: [64][100/233]	Loss 0.0022 (0.0126)	
training:	Epoch: [64][101/233]	Loss 0.0075 (0.0126)	
training:	Epoch: [64][102/233]	Loss 0.0021 (0.0125)	
training:	Epoch: [64][103/233]	Loss 0.0221 (0.0126)	
training:	Epoch: [64][104/233]	Loss 0.0029 (0.0125)	
training:	Epoch: [64][105/233]	Loss 0.0032 (0.0124)	
training:	Epoch: [64][106/233]	Loss 0.0025 (0.0123)	
training:	Epoch: [64][107/233]	Loss 0.0021 (0.0122)	
training:	Epoch: [64][108/233]	Loss 0.0024 (0.0121)	
training:	Epoch: [64][109/233]	Loss 0.0400 (0.0123)	
training:	Epoch: [64][110/233]	Loss 0.0021 (0.0123)	
training:	Epoch: [64][111/233]	Loss 0.0517 (0.0126)	
training:	Epoch: [64][112/233]	Loss 0.0021 (0.0125)	
training:	Epoch: [64][113/233]	Loss 0.1989 (0.0142)	
training:	Epoch: [64][114/233]	Loss 0.0018 (0.0141)	
training:	Epoch: [64][115/233]	Loss 0.0020 (0.0140)	
training:	Epoch: [64][116/233]	Loss 0.0023 (0.0139)	
training:	Epoch: [64][117/233]	Loss 0.0019 (0.0137)	
training:	Epoch: [64][118/233]	Loss 0.0080 (0.0137)	
training:	Epoch: [64][119/233]	Loss 0.0085 (0.0137)	
training:	Epoch: [64][120/233]	Loss 0.0027 (0.0136)	
training:	Epoch: [64][121/233]	Loss 0.0083 (0.0135)	
training:	Epoch: [64][122/233]	Loss 0.0069 (0.0135)	
training:	Epoch: [64][123/233]	Loss 0.0263 (0.0136)	
training:	Epoch: [64][124/233]	Loss 0.2014 (0.0151)	
training:	Epoch: [64][125/233]	Loss 0.0019 (0.0150)	
training:	Epoch: [64][126/233]	Loss 0.0022 (0.0149)	
training:	Epoch: [64][127/233]	Loss 0.0048 (0.0148)	
training:	Epoch: [64][128/233]	Loss 0.0020 (0.0147)	
training:	Epoch: [64][129/233]	Loss 0.0020 (0.0146)	
training:	Epoch: [64][130/233]	Loss 0.0019 (0.0145)	
training:	Epoch: [64][131/233]	Loss 0.0061 (0.0144)	
training:	Epoch: [64][132/233]	Loss 0.0018 (0.0143)	
training:	Epoch: [64][133/233]	Loss 0.0019 (0.0143)	
training:	Epoch: [64][134/233]	Loss 0.0074 (0.0142)	
training:	Epoch: [64][135/233]	Loss 0.1945 (0.0155)	
training:	Epoch: [64][136/233]	Loss 0.0018 (0.0154)	
training:	Epoch: [64][137/233]	Loss 0.0029 (0.0153)	
training:	Epoch: [64][138/233]	Loss 0.0902 (0.0159)	
training:	Epoch: [64][139/233]	Loss 0.0026 (0.0158)	
training:	Epoch: [64][140/233]	Loss 0.0436 (0.0160)	
training:	Epoch: [64][141/233]	Loss 0.1895 (0.0172)	
training:	Epoch: [64][142/233]	Loss 0.0021 (0.0171)	
training:	Epoch: [64][143/233]	Loss 0.0048 (0.0170)	
training:	Epoch: [64][144/233]	Loss 0.0024 (0.0169)	
training:	Epoch: [64][145/233]	Loss 0.0028 (0.0168)	
training:	Epoch: [64][146/233]	Loss 0.0294 (0.0169)	
training:	Epoch: [64][147/233]	Loss 0.0467 (0.0171)	
training:	Epoch: [64][148/233]	Loss 0.0019 (0.0170)	
training:	Epoch: [64][149/233]	Loss 0.0069 (0.0169)	
training:	Epoch: [64][150/233]	Loss 0.0036 (0.0169)	
training:	Epoch: [64][151/233]	Loss 0.0030 (0.0168)	
training:	Epoch: [64][152/233]	Loss 0.0102 (0.0167)	
training:	Epoch: [64][153/233]	Loss 0.1428 (0.0175)	
training:	Epoch: [64][154/233]	Loss 0.0019 (0.0174)	
training:	Epoch: [64][155/233]	Loss 0.0020 (0.0173)	
training:	Epoch: [64][156/233]	Loss 0.0692 (0.0177)	
training:	Epoch: [64][157/233]	Loss 0.0038 (0.0176)	
training:	Epoch: [64][158/233]	Loss 0.0026 (0.0175)	
training:	Epoch: [64][159/233]	Loss 0.0020 (0.0174)	
training:	Epoch: [64][160/233]	Loss 0.0025 (0.0173)	
training:	Epoch: [64][161/233]	Loss 0.0019 (0.0172)	
training:	Epoch: [64][162/233]	Loss 0.0022 (0.0171)	
training:	Epoch: [64][163/233]	Loss 0.0025 (0.0170)	
training:	Epoch: [64][164/233]	Loss 0.0102 (0.0170)	
training:	Epoch: [64][165/233]	Loss 0.0025 (0.0169)	
training:	Epoch: [64][166/233]	Loss 0.0115 (0.0169)	
training:	Epoch: [64][167/233]	Loss 0.0033 (0.0168)	
training:	Epoch: [64][168/233]	Loss 0.0020 (0.0167)	
training:	Epoch: [64][169/233]	Loss 0.0132 (0.0167)	
training:	Epoch: [64][170/233]	Loss 0.0023 (0.0166)	
training:	Epoch: [64][171/233]	Loss 0.0029 (0.0165)	
training:	Epoch: [64][172/233]	Loss 0.0183 (0.0165)	
training:	Epoch: [64][173/233]	Loss 0.0028 (0.0164)	
training:	Epoch: [64][174/233]	Loss 0.0034 (0.0164)	
training:	Epoch: [64][175/233]	Loss 0.0210 (0.0164)	
training:	Epoch: [64][176/233]	Loss 0.0023 (0.0163)	
training:	Epoch: [64][177/233]	Loss 0.0025 (0.0162)	
training:	Epoch: [64][178/233]	Loss 0.0019 (0.0162)	
training:	Epoch: [64][179/233]	Loss 0.0059 (0.0161)	
training:	Epoch: [64][180/233]	Loss 0.0022 (0.0160)	
training:	Epoch: [64][181/233]	Loss 0.0177 (0.0160)	
training:	Epoch: [64][182/233]	Loss 0.0034 (0.0160)	
training:	Epoch: [64][183/233]	Loss 0.0045 (0.0159)	
training:	Epoch: [64][184/233]	Loss 0.0042 (0.0158)	
training:	Epoch: [64][185/233]	Loss 0.0018 (0.0158)	
training:	Epoch: [64][186/233]	Loss 0.0058 (0.0157)	
training:	Epoch: [64][187/233]	Loss 0.0028 (0.0156)	
training:	Epoch: [64][188/233]	Loss 0.0018 (0.0156)	
training:	Epoch: [64][189/233]	Loss 0.0101 (0.0155)	
training:	Epoch: [64][190/233]	Loss 0.1408 (0.0162)	
training:	Epoch: [64][191/233]	Loss 0.0038 (0.0161)	
training:	Epoch: [64][192/233]	Loss 0.0043 (0.0161)	
training:	Epoch: [64][193/233]	Loss 0.0019 (0.0160)	
training:	Epoch: [64][194/233]	Loss 0.0023 (0.0159)	
training:	Epoch: [64][195/233]	Loss 0.0019 (0.0158)	
training:	Epoch: [64][196/233]	Loss 0.0023 (0.0158)	
training:	Epoch: [64][197/233]	Loss 0.0643 (0.0160)	
training:	Epoch: [64][198/233]	Loss 0.0020 (0.0160)	
training:	Epoch: [64][199/233]	Loss 0.0027 (0.0159)	
training:	Epoch: [64][200/233]	Loss 0.0027 (0.0158)	
training:	Epoch: [64][201/233]	Loss 0.0021 (0.0158)	
training:	Epoch: [64][202/233]	Loss 0.0026 (0.0157)	
training:	Epoch: [64][203/233]	Loss 0.0060 (0.0156)	
training:	Epoch: [64][204/233]	Loss 0.0023 (0.0156)	
training:	Epoch: [64][205/233]	Loss 0.0025 (0.0155)	
training:	Epoch: [64][206/233]	Loss 0.0027 (0.0154)	
training:	Epoch: [64][207/233]	Loss 0.0093 (0.0154)	
training:	Epoch: [64][208/233]	Loss 0.0028 (0.0154)	
training:	Epoch: [64][209/233]	Loss 0.0021 (0.0153)	
training:	Epoch: [64][210/233]	Loss 0.0048 (0.0152)	
training:	Epoch: [64][211/233]	Loss 0.0070 (0.0152)	
training:	Epoch: [64][212/233]	Loss 0.0023 (0.0151)	
training:	Epoch: [64][213/233]	Loss 0.0046 (0.0151)	
training:	Epoch: [64][214/233]	Loss 0.0019 (0.0150)	
training:	Epoch: [64][215/233]	Loss 0.0029 (0.0150)	
training:	Epoch: [64][216/233]	Loss 0.0084 (0.0149)	
training:	Epoch: [64][217/233]	Loss 0.0065 (0.0149)	
training:	Epoch: [64][218/233]	Loss 0.0027 (0.0149)	
training:	Epoch: [64][219/233]	Loss 0.0054 (0.0148)	
training:	Epoch: [64][220/233]	Loss 0.0020 (0.0148)	
training:	Epoch: [64][221/233]	Loss 0.0036 (0.0147)	
training:	Epoch: [64][222/233]	Loss 0.0197 (0.0147)	
training:	Epoch: [64][223/233]	Loss 0.0025 (0.0147)	
training:	Epoch: [64][224/233]	Loss 0.0530 (0.0148)	
training:	Epoch: [64][225/233]	Loss 0.0032 (0.0148)	
training:	Epoch: [64][226/233]	Loss 0.0033 (0.0147)	
training:	Epoch: [64][227/233]	Loss 0.0021 (0.0147)	
training:	Epoch: [64][228/233]	Loss 0.0019 (0.0146)	
training:	Epoch: [64][229/233]	Loss 0.0068 (0.0146)	
training:	Epoch: [64][230/233]	Loss 0.0026 (0.0145)	
training:	Epoch: [64][231/233]	Loss 0.0055 (0.0145)	
training:	Epoch: [64][232/233]	Loss 0.0027 (0.0144)	
training:	Epoch: [64][233/233]	Loss 0.0024 (0.0144)	
Training:	 Loss: 0.0144

Training:	 ACC: 0.9992 0.9992 0.9992 0.9992
Validation:	 ACC: 0.7900 0.7908 0.8069 0.7730
Validation:	 Best_BACC: 0.8162 0.8154 0.7998 0.8326
Validation:	 Loss: 1.0083
Pretraining:	Epoch 65/200
----------
training:	Epoch: [65][1/233]	Loss 0.0062 (0.0062)	
training:	Epoch: [65][2/233]	Loss 0.0021 (0.0041)	
training:	Epoch: [65][3/233]	Loss 0.0026 (0.0036)	
training:	Epoch: [65][4/233]	Loss 0.0028 (0.0034)	
training:	Epoch: [65][5/233]	Loss 0.0075 (0.0042)	
training:	Epoch: [65][6/233]	Loss 0.0018 (0.0038)	
training:	Epoch: [65][7/233]	Loss 0.0047 (0.0039)	
training:	Epoch: [65][8/233]	Loss 0.0030 (0.0038)	
training:	Epoch: [65][9/233]	Loss 0.0048 (0.0039)	
training:	Epoch: [65][10/233]	Loss 0.0026 (0.0038)	
training:	Epoch: [65][11/233]	Loss 0.0908 (0.0117)	
training:	Epoch: [65][12/233]	Loss 0.0025 (0.0109)	
training:	Epoch: [65][13/233]	Loss 0.0018 (0.0102)	
training:	Epoch: [65][14/233]	Loss 0.0066 (0.0100)	
training:	Epoch: [65][15/233]	Loss 0.0021 (0.0095)	
training:	Epoch: [65][16/233]	Loss 0.0021 (0.0090)	
training:	Epoch: [65][17/233]	Loss 0.0021 (0.0086)	
training:	Epoch: [65][18/233]	Loss 0.0022 (0.0082)	
training:	Epoch: [65][19/233]	Loss 0.1822 (0.0174)	
training:	Epoch: [65][20/233]	Loss 0.0021 (0.0166)	
training:	Epoch: [65][21/233]	Loss 0.0032 (0.0160)	
training:	Epoch: [65][22/233]	Loss 0.0038 (0.0154)	
training:	Epoch: [65][23/233]	Loss 0.0024 (0.0149)	
training:	Epoch: [65][24/233]	Loss 0.0023 (0.0143)	
training:	Epoch: [65][25/233]	Loss 0.0021 (0.0139)	
training:	Epoch: [65][26/233]	Loss 0.0031 (0.0134)	
training:	Epoch: [65][27/233]	Loss 0.0021 (0.0130)	
training:	Epoch: [65][28/233]	Loss 0.0128 (0.0130)	
training:	Epoch: [65][29/233]	Loss 0.0028 (0.0127)	
training:	Epoch: [65][30/233]	Loss 0.0559 (0.0141)	
training:	Epoch: [65][31/233]	Loss 0.0022 (0.0137)	
training:	Epoch: [65][32/233]	Loss 0.0583 (0.0151)	
training:	Epoch: [65][33/233]	Loss 0.0049 (0.0148)	
training:	Epoch: [65][34/233]	Loss 0.0026 (0.0144)	
training:	Epoch: [65][35/233]	Loss 0.0024 (0.0141)	
training:	Epoch: [65][36/233]	Loss 0.0023 (0.0138)	
training:	Epoch: [65][37/233]	Loss 0.0301 (0.0142)	
training:	Epoch: [65][38/233]	Loss 0.0020 (0.0139)	
training:	Epoch: [65][39/233]	Loss 0.1841 (0.0183)	
training:	Epoch: [65][40/233]	Loss 0.0023 (0.0179)	
training:	Epoch: [65][41/233]	Loss 0.0026 (0.0175)	
training:	Epoch: [65][42/233]	Loss 0.0135 (0.0174)	
training:	Epoch: [65][43/233]	Loss 0.0031 (0.0171)	
training:	Epoch: [65][44/233]	Loss 0.0088 (0.0169)	
training:	Epoch: [65][45/233]	Loss 0.0074 (0.0167)	
training:	Epoch: [65][46/233]	Loss 0.0196 (0.0167)	
training:	Epoch: [65][47/233]	Loss 0.0021 (0.0164)	
training:	Epoch: [65][48/233]	Loss 0.0022 (0.0161)	
training:	Epoch: [65][49/233]	Loss 0.0028 (0.0158)	
training:	Epoch: [65][50/233]	Loss 0.0177 (0.0159)	
training:	Epoch: [65][51/233]	Loss 0.0029 (0.0156)	
training:	Epoch: [65][52/233]	Loss 0.0027 (0.0154)	
training:	Epoch: [65][53/233]	Loss 0.0019 (0.0151)	
training:	Epoch: [65][54/233]	Loss 0.0028 (0.0149)	
training:	Epoch: [65][55/233]	Loss 0.0021 (0.0147)	
training:	Epoch: [65][56/233]	Loss 0.0019 (0.0144)	
training:	Epoch: [65][57/233]	Loss 0.0019 (0.0142)	
training:	Epoch: [65][58/233]	Loss 0.0052 (0.0141)	
training:	Epoch: [65][59/233]	Loss 0.0029 (0.0139)	
training:	Epoch: [65][60/233]	Loss 0.0021 (0.0137)	
training:	Epoch: [65][61/233]	Loss 0.0044 (0.0135)	
training:	Epoch: [65][62/233]	Loss 0.0051 (0.0134)	
training:	Epoch: [65][63/233]	Loss 0.0021 (0.0132)	
training:	Epoch: [65][64/233]	Loss 0.0176 (0.0133)	
training:	Epoch: [65][65/233]	Loss 0.0018 (0.0131)	
training:	Epoch: [65][66/233]	Loss 0.0023 (0.0129)	
training:	Epoch: [65][67/233]	Loss 0.0023 (0.0128)	
training:	Epoch: [65][68/233]	Loss 0.0027 (0.0126)	
training:	Epoch: [65][69/233]	Loss 0.0022 (0.0125)	
training:	Epoch: [65][70/233]	Loss 0.0033 (0.0123)	
training:	Epoch: [65][71/233]	Loss 0.0025 (0.0122)	
training:	Epoch: [65][72/233]	Loss 0.0079 (0.0121)	
training:	Epoch: [65][73/233]	Loss 0.0047 (0.0120)	
training:	Epoch: [65][74/233]	Loss 0.0025 (0.0119)	
training:	Epoch: [65][75/233]	Loss 0.0035 (0.0118)	
training:	Epoch: [65][76/233]	Loss 0.0021 (0.0117)	
training:	Epoch: [65][77/233]	Loss 0.0033 (0.0116)	
training:	Epoch: [65][78/233]	Loss 0.0020 (0.0114)	
training:	Epoch: [65][79/233]	Loss 0.0024 (0.0113)	
training:	Epoch: [65][80/233]	Loss 0.0039 (0.0112)	
training:	Epoch: [65][81/233]	Loss 0.0034 (0.0111)	
training:	Epoch: [65][82/233]	Loss 0.0036 (0.0110)	
training:	Epoch: [65][83/233]	Loss 0.0053 (0.0110)	
training:	Epoch: [65][84/233]	Loss 0.0019 (0.0109)	
training:	Epoch: [65][85/233]	Loss 0.0025 (0.0108)	
training:	Epoch: [65][86/233]	Loss 0.0026 (0.0107)	
training:	Epoch: [65][87/233]	Loss 0.0037 (0.0106)	
training:	Epoch: [65][88/233]	Loss 0.0021 (0.0105)	
training:	Epoch: [65][89/233]	Loss 0.0018 (0.0104)	
training:	Epoch: [65][90/233]	Loss 0.0030 (0.0103)	
training:	Epoch: [65][91/233]	Loss 0.0024 (0.0102)	
training:	Epoch: [65][92/233]	Loss 0.0021 (0.0101)	
training:	Epoch: [65][93/233]	Loss 0.0018 (0.0101)	
training:	Epoch: [65][94/233]	Loss 0.0021 (0.0100)	
training:	Epoch: [65][95/233]	Loss 0.0030 (0.0099)	
training:	Epoch: [65][96/233]	Loss 0.0028 (0.0098)	
training:	Epoch: [65][97/233]	Loss 0.0025 (0.0097)	
training:	Epoch: [65][98/233]	Loss 0.0040 (0.0097)	
training:	Epoch: [65][99/233]	Loss 0.0116 (0.0097)	
training:	Epoch: [65][100/233]	Loss 0.0020 (0.0096)	
training:	Epoch: [65][101/233]	Loss 0.0066 (0.0096)	
training:	Epoch: [65][102/233]	Loss 0.0030 (0.0095)	
training:	Epoch: [65][103/233]	Loss 0.0018 (0.0095)	
training:	Epoch: [65][104/233]	Loss 0.2016 (0.0113)	
training:	Epoch: [65][105/233]	Loss 0.0030 (0.0112)	
training:	Epoch: [65][106/233]	Loss 0.0051 (0.0112)	
training:	Epoch: [65][107/233]	Loss 0.0047 (0.0111)	
training:	Epoch: [65][108/233]	Loss 0.0037 (0.0110)	
training:	Epoch: [65][109/233]	Loss 0.0020 (0.0110)	
training:	Epoch: [65][110/233]	Loss 0.0017 (0.0109)	
training:	Epoch: [65][111/233]	Loss 0.0018 (0.0108)	
training:	Epoch: [65][112/233]	Loss 0.0026 (0.0107)	
training:	Epoch: [65][113/233]	Loss 0.0019 (0.0106)	
training:	Epoch: [65][114/233]	Loss 0.0029 (0.0106)	
training:	Epoch: [65][115/233]	Loss 0.0020 (0.0105)	
training:	Epoch: [65][116/233]	Loss 0.0021 (0.0104)	
training:	Epoch: [65][117/233]	Loss 0.0019 (0.0104)	
training:	Epoch: [65][118/233]	Loss 0.0020 (0.0103)	
training:	Epoch: [65][119/233]	Loss 0.0028 (0.0102)	
training:	Epoch: [65][120/233]	Loss 0.0029 (0.0102)	
training:	Epoch: [65][121/233]	Loss 0.0018 (0.0101)	
training:	Epoch: [65][122/233]	Loss 0.0023 (0.0100)	
training:	Epoch: [65][123/233]	Loss 0.0055 (0.0100)	
training:	Epoch: [65][124/233]	Loss 0.0029 (0.0099)	
training:	Epoch: [65][125/233]	Loss 0.0025 (0.0099)	
training:	Epoch: [65][126/233]	Loss 0.0023 (0.0098)	
training:	Epoch: [65][127/233]	Loss 0.0027 (0.0098)	
training:	Epoch: [65][128/233]	Loss 0.0021 (0.0097)	
training:	Epoch: [65][129/233]	Loss 0.0020 (0.0096)	
training:	Epoch: [65][130/233]	Loss 0.0026 (0.0096)	
training:	Epoch: [65][131/233]	Loss 0.2094 (0.0111)	
training:	Epoch: [65][132/233]	Loss 0.0017 (0.0110)	
training:	Epoch: [65][133/233]	Loss 0.0022 (0.0110)	
training:	Epoch: [65][134/233]	Loss 0.0018 (0.0109)	
training:	Epoch: [65][135/233]	Loss 0.0063 (0.0109)	
training:	Epoch: [65][136/233]	Loss 0.0029 (0.0108)	
training:	Epoch: [65][137/233]	Loss 0.0020 (0.0107)	
training:	Epoch: [65][138/233]	Loss 0.0017 (0.0107)	
training:	Epoch: [65][139/233]	Loss 0.0017 (0.0106)	
training:	Epoch: [65][140/233]	Loss 0.0018 (0.0106)	
training:	Epoch: [65][141/233]	Loss 0.0018 (0.0105)	
training:	Epoch: [65][142/233]	Loss 0.0019 (0.0104)	
training:	Epoch: [65][143/233]	Loss 0.0023 (0.0104)	
training:	Epoch: [65][144/233]	Loss 0.0017 (0.0103)	
training:	Epoch: [65][145/233]	Loss 0.0022 (0.0103)	
training:	Epoch: [65][146/233]	Loss 0.0048 (0.0102)	
training:	Epoch: [65][147/233]	Loss 0.0037 (0.0102)	
training:	Epoch: [65][148/233]	Loss 0.0019 (0.0101)	
training:	Epoch: [65][149/233]	Loss 0.0059 (0.0101)	
training:	Epoch: [65][150/233]	Loss 0.0266 (0.0102)	
training:	Epoch: [65][151/233]	Loss 0.0019 (0.0101)	
training:	Epoch: [65][152/233]	Loss 0.0021 (0.0101)	
training:	Epoch: [65][153/233]	Loss 0.0045 (0.0101)	
training:	Epoch: [65][154/233]	Loss 0.0052 (0.0100)	
training:	Epoch: [65][155/233]	Loss 0.0119 (0.0100)	
training:	Epoch: [65][156/233]	Loss 0.0043 (0.0100)	
training:	Epoch: [65][157/233]	Loss 0.0019 (0.0100)	
training:	Epoch: [65][158/233]	Loss 0.0022 (0.0099)	
training:	Epoch: [65][159/233]	Loss 0.0017 (0.0098)	
training:	Epoch: [65][160/233]	Loss 0.0022 (0.0098)	
training:	Epoch: [65][161/233]	Loss 0.0501 (0.0101)	
training:	Epoch: [65][162/233]	Loss 0.0035 (0.0100)	
training:	Epoch: [65][163/233]	Loss 0.0020 (0.0100)	
training:	Epoch: [65][164/233]	Loss 0.0020 (0.0099)	
training:	Epoch: [65][165/233]	Loss 0.0023 (0.0099)	
training:	Epoch: [65][166/233]	Loss 0.0022 (0.0098)	
training:	Epoch: [65][167/233]	Loss 0.0064 (0.0098)	
training:	Epoch: [65][168/233]	Loss 0.0026 (0.0098)	
training:	Epoch: [65][169/233]	Loss 0.0667 (0.0101)	
training:	Epoch: [65][170/233]	Loss 0.0022 (0.0100)	
training:	Epoch: [65][171/233]	Loss 0.0022 (0.0100)	
training:	Epoch: [65][172/233]	Loss 0.0019 (0.0100)	
training:	Epoch: [65][173/233]	Loss 0.0027 (0.0099)	
training:	Epoch: [65][174/233]	Loss 0.0018 (0.0099)	
training:	Epoch: [65][175/233]	Loss 0.1957 (0.0109)	
training:	Epoch: [65][176/233]	Loss 0.0022 (0.0109)	
training:	Epoch: [65][177/233]	Loss 0.0026 (0.0108)	
training:	Epoch: [65][178/233]	Loss 0.0026 (0.0108)	
training:	Epoch: [65][179/233]	Loss 0.0025 (0.0107)	
training:	Epoch: [65][180/233]	Loss 0.0025 (0.0107)	
training:	Epoch: [65][181/233]	Loss 0.0025 (0.0106)	
training:	Epoch: [65][182/233]	Loss 0.0021 (0.0106)	
training:	Epoch: [65][183/233]	Loss 0.0019 (0.0106)	
training:	Epoch: [65][184/233]	Loss 0.0040 (0.0105)	
training:	Epoch: [65][185/233]	Loss 0.0018 (0.0105)	
training:	Epoch: [65][186/233]	Loss 0.2007 (0.0115)	
training:	Epoch: [65][187/233]	Loss 0.0193 (0.0115)	
training:	Epoch: [65][188/233]	Loss 0.0032 (0.0115)	
training:	Epoch: [65][189/233]	Loss 0.0032 (0.0114)	
training:	Epoch: [65][190/233]	Loss 0.0026 (0.0114)	
training:	Epoch: [65][191/233]	Loss 0.0018 (0.0114)	
training:	Epoch: [65][192/233]	Loss 0.0016 (0.0113)	
training:	Epoch: [65][193/233]	Loss 0.0024 (0.0113)	
training:	Epoch: [65][194/233]	Loss 0.0020 (0.0112)	
training:	Epoch: [65][195/233]	Loss 0.0022 (0.0112)	
training:	Epoch: [65][196/233]	Loss 0.0019 (0.0111)	
training:	Epoch: [65][197/233]	Loss 0.1858 (0.0120)	
training:	Epoch: [65][198/233]	Loss 0.0107 (0.0120)	
training:	Epoch: [65][199/233]	Loss 0.1128 (0.0125)	
training:	Epoch: [65][200/233]	Loss 0.0085 (0.0125)	
training:	Epoch: [65][201/233]	Loss 0.0018 (0.0124)	
training:	Epoch: [65][202/233]	Loss 0.0033 (0.0124)	
training:	Epoch: [65][203/233]	Loss 0.0021 (0.0123)	
training:	Epoch: [65][204/233]	Loss 0.0024 (0.0123)	
training:	Epoch: [65][205/233]	Loss 0.0022 (0.0122)	
training:	Epoch: [65][206/233]	Loss 0.0102 (0.0122)	
training:	Epoch: [65][207/233]	Loss 0.1483 (0.0129)	
training:	Epoch: [65][208/233]	Loss 0.0022 (0.0128)	
training:	Epoch: [65][209/233]	Loss 0.1351 (0.0134)	
training:	Epoch: [65][210/233]	Loss 0.0021 (0.0134)	
training:	Epoch: [65][211/233]	Loss 0.0025 (0.0133)	
training:	Epoch: [65][212/233]	Loss 0.0020 (0.0133)	
training:	Epoch: [65][213/233]	Loss 0.0019 (0.0132)	
training:	Epoch: [65][214/233]	Loss 0.0022 (0.0131)	
training:	Epoch: [65][215/233]	Loss 0.0019 (0.0131)	
training:	Epoch: [65][216/233]	Loss 0.0029 (0.0131)	
training:	Epoch: [65][217/233]	Loss 0.0030 (0.0130)	
training:	Epoch: [65][218/233]	Loss 0.0163 (0.0130)	
training:	Epoch: [65][219/233]	Loss 0.0043 (0.0130)	
training:	Epoch: [65][220/233]	Loss 0.0024 (0.0129)	
training:	Epoch: [65][221/233]	Loss 0.0025 (0.0129)	
training:	Epoch: [65][222/233]	Loss 0.0033 (0.0128)	
training:	Epoch: [65][223/233]	Loss 0.0028 (0.0128)	
training:	Epoch: [65][224/233]	Loss 0.0021 (0.0127)	
training:	Epoch: [65][225/233]	Loss 0.0021 (0.0127)	
training:	Epoch: [65][226/233]	Loss 0.0076 (0.0127)	
training:	Epoch: [65][227/233]	Loss 0.0034 (0.0126)	
training:	Epoch: [65][228/233]	Loss 0.0187 (0.0127)	
training:	Epoch: [65][229/233]	Loss 0.0558 (0.0129)	
training:	Epoch: [65][230/233]	Loss 0.0026 (0.0128)	
training:	Epoch: [65][231/233]	Loss 0.0031 (0.0128)	
training:	Epoch: [65][232/233]	Loss 0.0032 (0.0127)	
training:	Epoch: [65][233/233]	Loss 0.0087 (0.0127)	
Training:	 Loss: 0.0127

Training:	 ACC: 0.9991 0.9991 0.9992 0.9989
Validation:	 ACC: 0.7860 0.7865 0.7967 0.7753
Validation:	 Best_BACC: 0.8162 0.8154 0.7998 0.8326
Validation:	 Loss: 1.0356
Pretraining:	Epoch 66/200
----------
training:	Epoch: [66][1/233]	Loss 0.0018 (0.0018)	
training:	Epoch: [66][2/233]	Loss 0.0242 (0.0130)	
training:	Epoch: [66][3/233]	Loss 0.0071 (0.0110)	
training:	Epoch: [66][4/233]	Loss 0.2505 (0.0709)	
training:	Epoch: [66][5/233]	Loss 0.0023 (0.0572)	
training:	Epoch: [66][6/233]	Loss 0.0073 (0.0489)	
training:	Epoch: [66][7/233]	Loss 0.0020 (0.0422)	
training:	Epoch: [66][8/233]	Loss 0.0019 (0.0371)	
training:	Epoch: [66][9/233]	Loss 0.0028 (0.0333)	
training:	Epoch: [66][10/233]	Loss 0.0019 (0.0302)	
training:	Epoch: [66][11/233]	Loss 0.0027 (0.0277)	
training:	Epoch: [66][12/233]	Loss 0.0028 (0.0256)	
training:	Epoch: [66][13/233]	Loss 0.0022 (0.0238)	
training:	Epoch: [66][14/233]	Loss 0.0021 (0.0223)	
training:	Epoch: [66][15/233]	Loss 0.0073 (0.0213)	
training:	Epoch: [66][16/233]	Loss 0.0023 (0.0201)	
training:	Epoch: [66][17/233]	Loss 0.0025 (0.0190)	
training:	Epoch: [66][18/233]	Loss 0.0020 (0.0181)	
training:	Epoch: [66][19/233]	Loss 0.0076 (0.0175)	
training:	Epoch: [66][20/233]	Loss 0.0043 (0.0169)	
training:	Epoch: [66][21/233]	Loss 0.0018 (0.0162)	
training:	Epoch: [66][22/233]	Loss 0.0017 (0.0155)	
training:	Epoch: [66][23/233]	Loss 0.0022 (0.0149)	
training:	Epoch: [66][24/233]	Loss 0.0023 (0.0144)	
training:	Epoch: [66][25/233]	Loss 0.0018 (0.0139)	
training:	Epoch: [66][26/233]	Loss 0.0030 (0.0135)	
training:	Epoch: [66][27/233]	Loss 0.0017 (0.0130)	
training:	Epoch: [66][28/233]	Loss 0.0021 (0.0127)	
training:	Epoch: [66][29/233]	Loss 0.0019 (0.0123)	
training:	Epoch: [66][30/233]	Loss 0.0036 (0.0120)	
training:	Epoch: [66][31/233]	Loss 0.0143 (0.0121)	
training:	Epoch: [66][32/233]	Loss 0.0019 (0.0118)	
training:	Epoch: [66][33/233]	Loss 0.0025 (0.0115)	
training:	Epoch: [66][34/233]	Loss 0.0022 (0.0112)	
training:	Epoch: [66][35/233]	Loss 0.0020 (0.0109)	
training:	Epoch: [66][36/233]	Loss 0.0022 (0.0107)	
training:	Epoch: [66][37/233]	Loss 0.0022 (0.0105)	
training:	Epoch: [66][38/233]	Loss 0.0065 (0.0104)	
training:	Epoch: [66][39/233]	Loss 0.0021 (0.0101)	
training:	Epoch: [66][40/233]	Loss 0.0041 (0.0100)	
training:	Epoch: [66][41/233]	Loss 0.0020 (0.0098)	
training:	Epoch: [66][42/233]	Loss 0.0020 (0.0096)	
training:	Epoch: [66][43/233]	Loss 0.0067 (0.0095)	
training:	Epoch: [66][44/233]	Loss 0.0020 (0.0094)	
training:	Epoch: [66][45/233]	Loss 0.0021 (0.0092)	
training:	Epoch: [66][46/233]	Loss 0.0093 (0.0092)	
training:	Epoch: [66][47/233]	Loss 0.0020 (0.0091)	
training:	Epoch: [66][48/233]	Loss 0.0017 (0.0089)	
training:	Epoch: [66][49/233]	Loss 0.0051 (0.0088)	
training:	Epoch: [66][50/233]	Loss 0.0019 (0.0087)	
training:	Epoch: [66][51/233]	Loss 0.0020 (0.0086)	
training:	Epoch: [66][52/233]	Loss 0.0021 (0.0084)	
training:	Epoch: [66][53/233]	Loss 0.0018 (0.0083)	
training:	Epoch: [66][54/233]	Loss 0.0351 (0.0088)	
training:	Epoch: [66][55/233]	Loss 0.0021 (0.0087)	
training:	Epoch: [66][56/233]	Loss 0.0021 (0.0086)	
training:	Epoch: [66][57/233]	Loss 0.0028 (0.0085)	
training:	Epoch: [66][58/233]	Loss 0.0018 (0.0084)	
training:	Epoch: [66][59/233]	Loss 0.1052 (0.0100)	
training:	Epoch: [66][60/233]	Loss 0.0183 (0.0101)	
training:	Epoch: [66][61/233]	Loss 0.0038 (0.0100)	
training:	Epoch: [66][62/233]	Loss 0.0058 (0.0100)	
training:	Epoch: [66][63/233]	Loss 0.0019 (0.0098)	
training:	Epoch: [66][64/233]	Loss 0.0083 (0.0098)	
training:	Epoch: [66][65/233]	Loss 0.0022 (0.0097)	
training:	Epoch: [66][66/233]	Loss 0.0163 (0.0098)	
training:	Epoch: [66][67/233]	Loss 0.0028 (0.0097)	
training:	Epoch: [66][68/233]	Loss 0.0312 (0.0100)	
training:	Epoch: [66][69/233]	Loss 0.0019 (0.0099)	
training:	Epoch: [66][70/233]	Loss 0.1844 (0.0124)	
training:	Epoch: [66][71/233]	Loss 0.2002 (0.0150)	
training:	Epoch: [66][72/233]	Loss 0.0026 (0.0149)	
training:	Epoch: [66][73/233]	Loss 0.0035 (0.0147)	
training:	Epoch: [66][74/233]	Loss 0.0020 (0.0145)	
training:	Epoch: [66][75/233]	Loss 0.0118 (0.0145)	
training:	Epoch: [66][76/233]	Loss 0.1352 (0.0161)	
training:	Epoch: [66][77/233]	Loss 0.0024 (0.0159)	
training:	Epoch: [66][78/233]	Loss 0.0025 (0.0157)	
training:	Epoch: [66][79/233]	Loss 0.0042 (0.0156)	
training:	Epoch: [66][80/233]	Loss 0.0020 (0.0154)	
training:	Epoch: [66][81/233]	Loss 0.0724 (0.0161)	
training:	Epoch: [66][82/233]	Loss 0.2039 (0.0184)	
training:	Epoch: [66][83/233]	Loss 0.0023 (0.0182)	
training:	Epoch: [66][84/233]	Loss 0.0022 (0.0180)	
training:	Epoch: [66][85/233]	Loss 0.0043 (0.0179)	
training:	Epoch: [66][86/233]	Loss 0.0044 (0.0177)	
training:	Epoch: [66][87/233]	Loss 0.0095 (0.0176)	
training:	Epoch: [66][88/233]	Loss 0.0370 (0.0178)	
training:	Epoch: [66][89/233]	Loss 0.0022 (0.0177)	
training:	Epoch: [66][90/233]	Loss 0.0110 (0.0176)	
training:	Epoch: [66][91/233]	Loss 0.0033 (0.0174)	
training:	Epoch: [66][92/233]	Loss 0.0103 (0.0173)	
training:	Epoch: [66][93/233]	Loss 0.0027 (0.0172)	
training:	Epoch: [66][94/233]	Loss 0.0486 (0.0175)	
training:	Epoch: [66][95/233]	Loss 0.0019 (0.0174)	
training:	Epoch: [66][96/233]	Loss 0.0018 (0.0172)	
training:	Epoch: [66][97/233]	Loss 0.0042 (0.0171)	
training:	Epoch: [66][98/233]	Loss 0.2004 (0.0189)	
training:	Epoch: [66][99/233]	Loss 0.0091 (0.0188)	
training:	Epoch: [66][100/233]	Loss 0.0036 (0.0187)	
training:	Epoch: [66][101/233]	Loss 0.0093 (0.0186)	
training:	Epoch: [66][102/233]	Loss 0.0024 (0.0184)	
training:	Epoch: [66][103/233]	Loss 0.0021 (0.0183)	
training:	Epoch: [66][104/233]	Loss 0.0045 (0.0181)	
training:	Epoch: [66][105/233]	Loss 0.0022 (0.0180)	
training:	Epoch: [66][106/233]	Loss 0.0022 (0.0178)	
training:	Epoch: [66][107/233]	Loss 0.0022 (0.0177)	
training:	Epoch: [66][108/233]	Loss 0.0310 (0.0178)	
training:	Epoch: [66][109/233]	Loss 0.0028 (0.0177)	
training:	Epoch: [66][110/233]	Loss 0.0023 (0.0175)	
training:	Epoch: [66][111/233]	Loss 0.0020 (0.0174)	
training:	Epoch: [66][112/233]	Loss 0.0041 (0.0173)	
training:	Epoch: [66][113/233]	Loss 0.0360 (0.0174)	
training:	Epoch: [66][114/233]	Loss 0.0017 (0.0173)	
training:	Epoch: [66][115/233]	Loss 0.0218 (0.0173)	
training:	Epoch: [66][116/233]	Loss 0.0018 (0.0172)	
training:	Epoch: [66][117/233]	Loss 0.0025 (0.0171)	
training:	Epoch: [66][118/233]	Loss 0.0020 (0.0170)	
training:	Epoch: [66][119/233]	Loss 0.0017 (0.0168)	
training:	Epoch: [66][120/233]	Loss 0.0040 (0.0167)	
training:	Epoch: [66][121/233]	Loss 0.1754 (0.0180)	
training:	Epoch: [66][122/233]	Loss 0.0123 (0.0180)	
training:	Epoch: [66][123/233]	Loss 0.0067 (0.0179)	
training:	Epoch: [66][124/233]	Loss 0.0022 (0.0178)	
training:	Epoch: [66][125/233]	Loss 0.0019 (0.0176)	
training:	Epoch: [66][126/233]	Loss 0.0020 (0.0175)	
training:	Epoch: [66][127/233]	Loss 0.0027 (0.0174)	
training:	Epoch: [66][128/233]	Loss 0.1264 (0.0183)	
training:	Epoch: [66][129/233]	Loss 0.0021 (0.0181)	
training:	Epoch: [66][130/233]	Loss 0.0046 (0.0180)	
training:	Epoch: [66][131/233]	Loss 0.0182 (0.0180)	
training:	Epoch: [66][132/233]	Loss 0.0020 (0.0179)	
training:	Epoch: [66][133/233]	Loss 0.0141 (0.0179)	
training:	Epoch: [66][134/233]	Loss 0.0031 (0.0178)	
training:	Epoch: [66][135/233]	Loss 0.0022 (0.0176)	
training:	Epoch: [66][136/233]	Loss 0.0287 (0.0177)	
training:	Epoch: [66][137/233]	Loss 0.0161 (0.0177)	
training:	Epoch: [66][138/233]	Loss 0.0026 (0.0176)	
training:	Epoch: [66][139/233]	Loss 0.0017 (0.0175)	
training:	Epoch: [66][140/233]	Loss 0.1147 (0.0182)	
training:	Epoch: [66][141/233]	Loss 0.0019 (0.0181)	
training:	Epoch: [66][142/233]	Loss 0.0044 (0.0180)	
training:	Epoch: [66][143/233]	Loss 0.0026 (0.0179)	
training:	Epoch: [66][144/233]	Loss 0.1276 (0.0186)	
training:	Epoch: [66][145/233]	Loss 0.0024 (0.0185)	
training:	Epoch: [66][146/233]	Loss 0.0017 (0.0184)	
training:	Epoch: [66][147/233]	Loss 0.0020 (0.0183)	
training:	Epoch: [66][148/233]	Loss 0.0068 (0.0182)	
training:	Epoch: [66][149/233]	Loss 0.0027 (0.0181)	
training:	Epoch: [66][150/233]	Loss 0.0945 (0.0186)	
training:	Epoch: [66][151/233]	Loss 0.0021 (0.0185)	
training:	Epoch: [66][152/233]	Loss 0.0057 (0.0184)	
training:	Epoch: [66][153/233]	Loss 0.0329 (0.0185)	
training:	Epoch: [66][154/233]	Loss 0.0020 (0.0184)	
training:	Epoch: [66][155/233]	Loss 0.0019 (0.0183)	
training:	Epoch: [66][156/233]	Loss 0.0019 (0.0182)	
training:	Epoch: [66][157/233]	Loss 0.0018 (0.0181)	
training:	Epoch: [66][158/233]	Loss 0.0037 (0.0180)	
training:	Epoch: [66][159/233]	Loss 0.0061 (0.0179)	
training:	Epoch: [66][160/233]	Loss 0.0053 (0.0179)	
training:	Epoch: [66][161/233]	Loss 0.0041 (0.0178)	
training:	Epoch: [66][162/233]	Loss 0.0023 (0.0177)	
training:	Epoch: [66][163/233]	Loss 0.0023 (0.0176)	
training:	Epoch: [66][164/233]	Loss 0.0043 (0.0175)	
training:	Epoch: [66][165/233]	Loss 0.0017 (0.0174)	
training:	Epoch: [66][166/233]	Loss 0.0039 (0.0173)	
training:	Epoch: [66][167/233]	Loss 0.0019 (0.0172)	
training:	Epoch: [66][168/233]	Loss 0.0028 (0.0171)	
training:	Epoch: [66][169/233]	Loss 0.0018 (0.0171)	
training:	Epoch: [66][170/233]	Loss 0.0033 (0.0170)	
training:	Epoch: [66][171/233]	Loss 0.0024 (0.0169)	
training:	Epoch: [66][172/233]	Loss 0.0028 (0.0168)	
training:	Epoch: [66][173/233]	Loss 0.0018 (0.0167)	
training:	Epoch: [66][174/233]	Loss 0.0020 (0.0166)	
training:	Epoch: [66][175/233]	Loss 0.0049 (0.0166)	
training:	Epoch: [66][176/233]	Loss 0.0020 (0.0165)	
training:	Epoch: [66][177/233]	Loss 0.0018 (0.0164)	
training:	Epoch: [66][178/233]	Loss 0.0153 (0.0164)	
training:	Epoch: [66][179/233]	Loss 0.0290 (0.0165)	
training:	Epoch: [66][180/233]	Loss 0.0079 (0.0164)	
training:	Epoch: [66][181/233]	Loss 0.0057 (0.0164)	
training:	Epoch: [66][182/233]	Loss 0.0030 (0.0163)	
training:	Epoch: [66][183/233]	Loss 0.0193 (0.0163)	
training:	Epoch: [66][184/233]	Loss 0.0117 (0.0163)	
training:	Epoch: [66][185/233]	Loss 0.0021 (0.0162)	
training:	Epoch: [66][186/233]	Loss 0.0026 (0.0161)	
training:	Epoch: [66][187/233]	Loss 0.0050 (0.0161)	
training:	Epoch: [66][188/233]	Loss 0.0022 (0.0160)	
training:	Epoch: [66][189/233]	Loss 0.0021 (0.0159)	
training:	Epoch: [66][190/233]	Loss 0.0031 (0.0159)	
training:	Epoch: [66][191/233]	Loss 0.0016 (0.0158)	
training:	Epoch: [66][192/233]	Loss 0.0026 (0.0157)	
training:	Epoch: [66][193/233]	Loss 0.0020 (0.0156)	
training:	Epoch: [66][194/233]	Loss 0.2426 (0.0168)	
training:	Epoch: [66][195/233]	Loss 0.1995 (0.0177)	
training:	Epoch: [66][196/233]	Loss 0.0383 (0.0178)	
training:	Epoch: [66][197/233]	Loss 0.0022 (0.0178)	
training:	Epoch: [66][198/233]	Loss 0.0029 (0.0177)	
training:	Epoch: [66][199/233]	Loss 0.0019 (0.0176)	
training:	Epoch: [66][200/233]	Loss 0.0025 (0.0175)	
training:	Epoch: [66][201/233]	Loss 0.0040 (0.0175)	
training:	Epoch: [66][202/233]	Loss 0.0127 (0.0174)	
training:	Epoch: [66][203/233]	Loss 0.0057 (0.0174)	
training:	Epoch: [66][204/233]	Loss 0.0022 (0.0173)	
training:	Epoch: [66][205/233]	Loss 0.0039 (0.0173)	
training:	Epoch: [66][206/233]	Loss 0.0031 (0.0172)	
training:	Epoch: [66][207/233]	Loss 0.1226 (0.0177)	
training:	Epoch: [66][208/233]	Loss 0.0017 (0.0176)	
training:	Epoch: [66][209/233]	Loss 0.0023 (0.0175)	
training:	Epoch: [66][210/233]	Loss 0.0021 (0.0175)	
training:	Epoch: [66][211/233]	Loss 0.0020 (0.0174)	
training:	Epoch: [66][212/233]	Loss 0.0037 (0.0173)	
training:	Epoch: [66][213/233]	Loss 0.0024 (0.0173)	
training:	Epoch: [66][214/233]	Loss 0.0045 (0.0172)	
training:	Epoch: [66][215/233]	Loss 0.0038 (0.0171)	
training:	Epoch: [66][216/233]	Loss 0.0018 (0.0171)	
training:	Epoch: [66][217/233]	Loss 0.0173 (0.0171)	
training:	Epoch: [66][218/233]	Loss 0.0023 (0.0170)	
training:	Epoch: [66][219/233]	Loss 0.0019 (0.0169)	
training:	Epoch: [66][220/233]	Loss 0.0041 (0.0169)	
training:	Epoch: [66][221/233]	Loss 0.0022 (0.0168)	
training:	Epoch: [66][222/233]	Loss 0.0037 (0.0167)	
training:	Epoch: [66][223/233]	Loss 0.0032 (0.0167)	
training:	Epoch: [66][224/233]	Loss 0.0026 (0.0166)	
training:	Epoch: [66][225/233]	Loss 0.0027 (0.0166)	
training:	Epoch: [66][226/233]	Loss 0.0038 (0.0165)	
training:	Epoch: [66][227/233]	Loss 0.0045 (0.0165)	
training:	Epoch: [66][228/233]	Loss 0.2104 (0.0173)	
training:	Epoch: [66][229/233]	Loss 0.1982 (0.0181)	
training:	Epoch: [66][230/233]	Loss 0.0029 (0.0180)	
training:	Epoch: [66][231/233]	Loss 0.0020 (0.0180)	
training:	Epoch: [66][232/233]	Loss 0.0018 (0.0179)	
training:	Epoch: [66][233/233]	Loss 0.0032 (0.0178)	
Training:	 Loss: 0.0178

Training:	 ACC: 0.9992 0.9992 0.9992 0.9992
Validation:	 ACC: 0.7845 0.7854 0.8049 0.7640
Validation:	 Best_BACC: 0.8162 0.8154 0.7998 0.8326
Validation:	 Loss: 1.0371
Pretraining:	Epoch 67/200
----------
training:	Epoch: [67][1/233]	Loss 0.0045 (0.0045)	
training:	Epoch: [67][2/233]	Loss 0.0038 (0.0041)	
training:	Epoch: [67][3/233]	Loss 0.0037 (0.0040)	
training:	Epoch: [67][4/233]	Loss 0.0037 (0.0039)	
training:	Epoch: [67][5/233]	Loss 0.0023 (0.0036)	
training:	Epoch: [67][6/233]	Loss 0.0054 (0.0039)	
training:	Epoch: [67][7/233]	Loss 0.0021 (0.0036)	
training:	Epoch: [67][8/233]	Loss 0.0022 (0.0035)	
training:	Epoch: [67][9/233]	Loss 0.0019 (0.0033)	
training:	Epoch: [67][10/233]	Loss 0.0031 (0.0033)	
training:	Epoch: [67][11/233]	Loss 0.0304 (0.0057)	
training:	Epoch: [67][12/233]	Loss 0.0039 (0.0056)	
training:	Epoch: [67][13/233]	Loss 0.0022 (0.0053)	
training:	Epoch: [67][14/233]	Loss 0.0025 (0.0051)	
training:	Epoch: [67][15/233]	Loss 0.0021 (0.0049)	
training:	Epoch: [67][16/233]	Loss 0.0022 (0.0047)	
training:	Epoch: [67][17/233]	Loss 0.0037 (0.0047)	
training:	Epoch: [67][18/233]	Loss 0.0026 (0.0046)	
training:	Epoch: [67][19/233]	Loss 0.0074 (0.0047)	
training:	Epoch: [67][20/233]	Loss 0.0028 (0.0046)	
training:	Epoch: [67][21/233]	Loss 0.0019 (0.0045)	
training:	Epoch: [67][22/233]	Loss 0.0024 (0.0044)	
training:	Epoch: [67][23/233]	Loss 0.0037 (0.0044)	
training:	Epoch: [67][24/233]	Loss 0.0024 (0.0043)	
training:	Epoch: [67][25/233]	Loss 0.0319 (0.0054)	
training:	Epoch: [67][26/233]	Loss 0.0086 (0.0055)	
training:	Epoch: [67][27/233]	Loss 0.0027 (0.0054)	
training:	Epoch: [67][28/233]	Loss 0.0029 (0.0053)	
training:	Epoch: [67][29/233]	Loss 0.0070 (0.0054)	
training:	Epoch: [67][30/233]	Loss 0.0293 (0.0062)	
training:	Epoch: [67][31/233]	Loss 0.0022 (0.0060)	
training:	Epoch: [67][32/233]	Loss 0.0023 (0.0059)	
training:	Epoch: [67][33/233]	Loss 0.0032 (0.0058)	
training:	Epoch: [67][34/233]	Loss 0.0100 (0.0060)	
training:	Epoch: [67][35/233]	Loss 0.0026 (0.0059)	
training:	Epoch: [67][36/233]	Loss 0.0077 (0.0059)	
training:	Epoch: [67][37/233]	Loss 0.0017 (0.0058)	
training:	Epoch: [67][38/233]	Loss 0.0019 (0.0057)	
training:	Epoch: [67][39/233]	Loss 0.0023 (0.0056)	
training:	Epoch: [67][40/233]	Loss 0.0035 (0.0056)	
training:	Epoch: [67][41/233]	Loss 0.0026 (0.0055)	
training:	Epoch: [67][42/233]	Loss 0.0017 (0.0054)	
training:	Epoch: [67][43/233]	Loss 0.0019 (0.0053)	
training:	Epoch: [67][44/233]	Loss 0.0057 (0.0053)	
training:	Epoch: [67][45/233]	Loss 0.0019 (0.0053)	
training:	Epoch: [67][46/233]	Loss 0.0053 (0.0053)	
training:	Epoch: [67][47/233]	Loss 0.0065 (0.0053)	
training:	Epoch: [67][48/233]	Loss 0.2029 (0.0094)	
training:	Epoch: [67][49/233]	Loss 0.0018 (0.0092)	
training:	Epoch: [67][50/233]	Loss 0.0128 (0.0093)	
training:	Epoch: [67][51/233]	Loss 0.0046 (0.0092)	
training:	Epoch: [67][52/233]	Loss 0.0021 (0.0091)	
training:	Epoch: [67][53/233]	Loss 0.0021 (0.0090)	
training:	Epoch: [67][54/233]	Loss 0.0043 (0.0089)	
training:	Epoch: [67][55/233]	Loss 0.0019 (0.0087)	
training:	Epoch: [67][56/233]	Loss 0.0103 (0.0088)	
training:	Epoch: [67][57/233]	Loss 0.0099 (0.0088)	
training:	Epoch: [67][58/233]	Loss 0.0021 (0.0087)	
training:	Epoch: [67][59/233]	Loss 0.0057 (0.0086)	
training:	Epoch: [67][60/233]	Loss 0.0019 (0.0085)	
training:	Epoch: [67][61/233]	Loss 0.0045 (0.0084)	
training:	Epoch: [67][62/233]	Loss 0.0085 (0.0084)	
training:	Epoch: [67][63/233]	Loss 0.0023 (0.0083)	
training:	Epoch: [67][64/233]	Loss 0.0022 (0.0083)	
training:	Epoch: [67][65/233]	Loss 0.0024 (0.0082)	
training:	Epoch: [67][66/233]	Loss 0.0228 (0.0084)	
training:	Epoch: [67][67/233]	Loss 0.0018 (0.0083)	
training:	Epoch: [67][68/233]	Loss 0.0019 (0.0082)	
training:	Epoch: [67][69/233]	Loss 0.0023 (0.0081)	
training:	Epoch: [67][70/233]	Loss 0.0020 (0.0080)	
training:	Epoch: [67][71/233]	Loss 0.0023 (0.0079)	
training:	Epoch: [67][72/233]	Loss 0.0018 (0.0079)	
training:	Epoch: [67][73/233]	Loss 0.2292 (0.0109)	
training:	Epoch: [67][74/233]	Loss 0.0043 (0.0108)	
training:	Epoch: [67][75/233]	Loss 0.0035 (0.0107)	
training:	Epoch: [67][76/233]	Loss 0.0087 (0.0107)	
training:	Epoch: [67][77/233]	Loss 0.2009 (0.0131)	
training:	Epoch: [67][78/233]	Loss 0.0218 (0.0133)	
training:	Epoch: [67][79/233]	Loss 0.0037 (0.0131)	
training:	Epoch: [67][80/233]	Loss 0.0022 (0.0130)	
training:	Epoch: [67][81/233]	Loss 0.0029 (0.0129)	
training:	Epoch: [67][82/233]	Loss 0.0044 (0.0128)	
training:	Epoch: [67][83/233]	Loss 0.0021 (0.0126)	
training:	Epoch: [67][84/233]	Loss 0.0060 (0.0126)	
training:	Epoch: [67][85/233]	Loss 0.0028 (0.0124)	
training:	Epoch: [67][86/233]	Loss 0.0021 (0.0123)	
training:	Epoch: [67][87/233]	Loss 0.0249 (0.0125)	
training:	Epoch: [67][88/233]	Loss 0.0025 (0.0124)	
training:	Epoch: [67][89/233]	Loss 0.0019 (0.0122)	
training:	Epoch: [67][90/233]	Loss 0.0250 (0.0124)	
training:	Epoch: [67][91/233]	Loss 0.0036 (0.0123)	
training:	Epoch: [67][92/233]	Loss 0.0037 (0.0122)	
training:	Epoch: [67][93/233]	Loss 0.0020 (0.0121)	
training:	Epoch: [67][94/233]	Loss 0.0031 (0.0120)	
training:	Epoch: [67][95/233]	Loss 0.1910 (0.0139)	
training:	Epoch: [67][96/233]	Loss 0.0052 (0.0138)	
training:	Epoch: [67][97/233]	Loss 0.0035 (0.0137)	
training:	Epoch: [67][98/233]	Loss 0.1785 (0.0154)	
training:	Epoch: [67][99/233]	Loss 0.0020 (0.0152)	
training:	Epoch: [67][100/233]	Loss 0.0018 (0.0151)	
training:	Epoch: [67][101/233]	Loss 0.0022 (0.0150)	
training:	Epoch: [67][102/233]	Loss 0.0036 (0.0148)	
training:	Epoch: [67][103/233]	Loss 0.0020 (0.0147)	
training:	Epoch: [67][104/233]	Loss 0.0022 (0.0146)	
training:	Epoch: [67][105/233]	Loss 0.0027 (0.0145)	
training:	Epoch: [67][106/233]	Loss 0.0019 (0.0144)	
training:	Epoch: [67][107/233]	Loss 0.0018 (0.0143)	
training:	Epoch: [67][108/233]	Loss 0.0023 (0.0141)	
training:	Epoch: [67][109/233]	Loss 0.0025 (0.0140)	
training:	Epoch: [67][110/233]	Loss 0.0035 (0.0139)	
training:	Epoch: [67][111/233]	Loss 0.0019 (0.0138)	
training:	Epoch: [67][112/233]	Loss 0.0025 (0.0137)	
training:	Epoch: [67][113/233]	Loss 0.0025 (0.0136)	
training:	Epoch: [67][114/233]	Loss 0.0030 (0.0135)	
training:	Epoch: [67][115/233]	Loss 0.0042 (0.0135)	
training:	Epoch: [67][116/233]	Loss 0.0028 (0.0134)	
training:	Epoch: [67][117/233]	Loss 0.0017 (0.0133)	
training:	Epoch: [67][118/233]	Loss 0.0018 (0.0132)	
training:	Epoch: [67][119/233]	Loss 0.0074 (0.0131)	
training:	Epoch: [67][120/233]	Loss 0.0240 (0.0132)	
training:	Epoch: [67][121/233]	Loss 0.0026 (0.0131)	
training:	Epoch: [67][122/233]	Loss 0.0020 (0.0130)	
training:	Epoch: [67][123/233]	Loss 0.0018 (0.0129)	
training:	Epoch: [67][124/233]	Loss 0.0019 (0.0128)	
training:	Epoch: [67][125/233]	Loss 0.0017 (0.0128)	
training:	Epoch: [67][126/233]	Loss 0.0026 (0.0127)	
training:	Epoch: [67][127/233]	Loss 0.0023 (0.0126)	
training:	Epoch: [67][128/233]	Loss 0.0040 (0.0125)	
training:	Epoch: [67][129/233]	Loss 0.0027 (0.0125)	
training:	Epoch: [67][130/233]	Loss 0.0021 (0.0124)	
training:	Epoch: [67][131/233]	Loss 0.0354 (0.0125)	
training:	Epoch: [67][132/233]	Loss 0.0021 (0.0125)	
training:	Epoch: [67][133/233]	Loss 0.0020 (0.0124)	
training:	Epoch: [67][134/233]	Loss 0.0437 (0.0126)	
training:	Epoch: [67][135/233]	Loss 0.0018 (0.0125)	
training:	Epoch: [67][136/233]	Loss 0.0649 (0.0129)	
training:	Epoch: [67][137/233]	Loss 0.0018 (0.0128)	
training:	Epoch: [67][138/233]	Loss 0.0029 (0.0128)	
training:	Epoch: [67][139/233]	Loss 0.0017 (0.0127)	
training:	Epoch: [67][140/233]	Loss 0.0037 (0.0126)	
training:	Epoch: [67][141/233]	Loss 0.0020 (0.0126)	
training:	Epoch: [67][142/233]	Loss 0.0017 (0.0125)	
training:	Epoch: [67][143/233]	Loss 0.0025 (0.0124)	
training:	Epoch: [67][144/233]	Loss 0.0033 (0.0123)	
training:	Epoch: [67][145/233]	Loss 0.0215 (0.0124)	
training:	Epoch: [67][146/233]	Loss 0.0026 (0.0123)	
training:	Epoch: [67][147/233]	Loss 0.0020 (0.0123)	
training:	Epoch: [67][148/233]	Loss 0.0755 (0.0127)	
training:	Epoch: [67][149/233]	Loss 0.0131 (0.0127)	
training:	Epoch: [67][150/233]	Loss 0.0028 (0.0126)	
training:	Epoch: [67][151/233]	Loss 0.1887 (0.0138)	
training:	Epoch: [67][152/233]	Loss 0.0028 (0.0137)	
training:	Epoch: [67][153/233]	Loss 0.0087 (0.0137)	
training:	Epoch: [67][154/233]	Loss 0.0016 (0.0136)	
training:	Epoch: [67][155/233]	Loss 0.0698 (0.0140)	
training:	Epoch: [67][156/233]	Loss 0.0024 (0.0139)	
training:	Epoch: [67][157/233]	Loss 0.0032 (0.0138)	
training:	Epoch: [67][158/233]	Loss 0.0018 (0.0138)	
training:	Epoch: [67][159/233]	Loss 0.0017 (0.0137)	
training:	Epoch: [67][160/233]	Loss 0.0052 (0.0136)	
training:	Epoch: [67][161/233]	Loss 0.0035 (0.0136)	
training:	Epoch: [67][162/233]	Loss 0.0069 (0.0135)	
training:	Epoch: [67][163/233]	Loss 0.0035 (0.0135)	
training:	Epoch: [67][164/233]	Loss 0.0017 (0.0134)	
training:	Epoch: [67][165/233]	Loss 0.0148 (0.0134)	
training:	Epoch: [67][166/233]	Loss 0.1545 (0.0143)	
training:	Epoch: [67][167/233]	Loss 0.0038 (0.0142)	
training:	Epoch: [67][168/233]	Loss 0.0046 (0.0141)	
training:	Epoch: [67][169/233]	Loss 0.0018 (0.0141)	
training:	Epoch: [67][170/233]	Loss 0.0019 (0.0140)	
training:	Epoch: [67][171/233]	Loss 0.0019 (0.0139)	
training:	Epoch: [67][172/233]	Loss 0.0022 (0.0139)	
training:	Epoch: [67][173/233]	Loss 0.0026 (0.0138)	
training:	Epoch: [67][174/233]	Loss 0.0021 (0.0137)	
training:	Epoch: [67][175/233]	Loss 0.0022 (0.0137)	
training:	Epoch: [67][176/233]	Loss 0.2027 (0.0147)	
training:	Epoch: [67][177/233]	Loss 0.0022 (0.0147)	
training:	Epoch: [67][178/233]	Loss 0.0022 (0.0146)	
training:	Epoch: [67][179/233]	Loss 0.0029 (0.0145)	
training:	Epoch: [67][180/233]	Loss 0.0018 (0.0145)	
training:	Epoch: [67][181/233]	Loss 0.0204 (0.0145)	
training:	Epoch: [67][182/233]	Loss 0.0018 (0.0144)	
training:	Epoch: [67][183/233]	Loss 0.0048 (0.0144)	
training:	Epoch: [67][184/233]	Loss 0.0025 (0.0143)	
training:	Epoch: [67][185/233]	Loss 0.0047 (0.0142)	
training:	Epoch: [67][186/233]	Loss 0.0661 (0.0145)	
training:	Epoch: [67][187/233]	Loss 0.0029 (0.0145)	
training:	Epoch: [67][188/233]	Loss 0.0185 (0.0145)	
training:	Epoch: [67][189/233]	Loss 0.1643 (0.0153)	
training:	Epoch: [67][190/233]	Loss 0.0025 (0.0152)	
training:	Epoch: [67][191/233]	Loss 0.0022 (0.0151)	
training:	Epoch: [67][192/233]	Loss 0.0027 (0.0151)	
training:	Epoch: [67][193/233]	Loss 0.0032 (0.0150)	
training:	Epoch: [67][194/233]	Loss 0.0228 (0.0151)	
training:	Epoch: [67][195/233]	Loss 0.0043 (0.0150)	
training:	Epoch: [67][196/233]	Loss 0.0021 (0.0149)	
training:	Epoch: [67][197/233]	Loss 0.0016 (0.0149)	
training:	Epoch: [67][198/233]	Loss 0.0069 (0.0148)	
training:	Epoch: [67][199/233]	Loss 0.0019 (0.0148)	
training:	Epoch: [67][200/233]	Loss 0.0046 (0.0147)	
training:	Epoch: [67][201/233]	Loss 0.0022 (0.0146)	
training:	Epoch: [67][202/233]	Loss 0.0021 (0.0146)	
training:	Epoch: [67][203/233]	Loss 0.0027 (0.0145)	
training:	Epoch: [67][204/233]	Loss 0.0041 (0.0145)	
training:	Epoch: [67][205/233]	Loss 0.0180 (0.0145)	
training:	Epoch: [67][206/233]	Loss 0.0019 (0.0144)	
training:	Epoch: [67][207/233]	Loss 0.0019 (0.0144)	
training:	Epoch: [67][208/233]	Loss 0.0017 (0.0143)	
training:	Epoch: [67][209/233]	Loss 0.0037 (0.0143)	
training:	Epoch: [67][210/233]	Loss 0.0028 (0.0142)	
training:	Epoch: [67][211/233]	Loss 0.0022 (0.0141)	
training:	Epoch: [67][212/233]	Loss 0.0019 (0.0141)	
training:	Epoch: [67][213/233]	Loss 0.0017 (0.0140)	
training:	Epoch: [67][214/233]	Loss 0.0019 (0.0140)	
training:	Epoch: [67][215/233]	Loss 0.0093 (0.0140)	
training:	Epoch: [67][216/233]	Loss 0.0018 (0.0139)	
training:	Epoch: [67][217/233]	Loss 0.0026 (0.0138)	
training:	Epoch: [67][218/233]	Loss 0.0021 (0.0138)	
training:	Epoch: [67][219/233]	Loss 0.0026 (0.0137)	
training:	Epoch: [67][220/233]	Loss 0.0087 (0.0137)	
training:	Epoch: [67][221/233]	Loss 0.0017 (0.0137)	
training:	Epoch: [67][222/233]	Loss 0.0020 (0.0136)	
training:	Epoch: [67][223/233]	Loss 0.0022 (0.0136)	
training:	Epoch: [67][224/233]	Loss 0.0029 (0.0135)	
training:	Epoch: [67][225/233]	Loss 0.0020 (0.0135)	
training:	Epoch: [67][226/233]	Loss 0.0027 (0.0134)	
training:	Epoch: [67][227/233]	Loss 0.0029 (0.0134)	
training:	Epoch: [67][228/233]	Loss 0.0022 (0.0133)	
training:	Epoch: [67][229/233]	Loss 0.1857 (0.0141)	
training:	Epoch: [67][230/233]	Loss 0.0020 (0.0140)	
training:	Epoch: [67][231/233]	Loss 0.0020 (0.0140)	
training:	Epoch: [67][232/233]	Loss 0.0028 (0.0139)	
training:	Epoch: [67][233/233]	Loss 0.0019 (0.0139)	
Training:	 Loss: 0.0138

Training:	 ACC: 0.9992 0.9992 0.9992 0.9992
Validation:	 ACC: 0.7910 0.7913 0.7978 0.7843
Validation:	 Best_BACC: 0.8162 0.8154 0.7998 0.8326
Validation:	 Loss: 1.0232
Pretraining:	Epoch 68/200
----------
training:	Epoch: [68][1/233]	Loss 0.0019 (0.0019)	
training:	Epoch: [68][2/233]	Loss 0.0019 (0.0019)	
training:	Epoch: [68][3/233]	Loss 0.0022 (0.0020)	
training:	Epoch: [68][4/233]	Loss 0.0020 (0.0020)	
training:	Epoch: [68][5/233]	Loss 0.0049 (0.0026)	
training:	Epoch: [68][6/233]	Loss 0.0080 (0.0035)	
training:	Epoch: [68][7/233]	Loss 0.1179 (0.0198)	
training:	Epoch: [68][8/233]	Loss 0.0167 (0.0194)	
training:	Epoch: [68][9/233]	Loss 0.0016 (0.0174)	
training:	Epoch: [68][10/233]	Loss 0.0035 (0.0160)	
training:	Epoch: [68][11/233]	Loss 0.0018 (0.0148)	
training:	Epoch: [68][12/233]	Loss 0.0018 (0.0137)	
training:	Epoch: [68][13/233]	Loss 0.0125 (0.0136)	
training:	Epoch: [68][14/233]	Loss 0.0764 (0.0181)	
training:	Epoch: [68][15/233]	Loss 0.0031 (0.0171)	
training:	Epoch: [68][16/233]	Loss 0.0020 (0.0161)	
training:	Epoch: [68][17/233]	Loss 0.0031 (0.0154)	
training:	Epoch: [68][18/233]	Loss 0.0143 (0.0153)	
training:	Epoch: [68][19/233]	Loss 0.0021 (0.0146)	
training:	Epoch: [68][20/233]	Loss 0.0019 (0.0140)	
training:	Epoch: [68][21/233]	Loss 0.0018 (0.0134)	
training:	Epoch: [68][22/233]	Loss 0.0021 (0.0129)	
training:	Epoch: [68][23/233]	Loss 0.0027 (0.0124)	
training:	Epoch: [68][24/233]	Loss 0.0030 (0.0120)	
training:	Epoch: [68][25/233]	Loss 0.0037 (0.0117)	
training:	Epoch: [68][26/233]	Loss 0.0016 (0.0113)	
training:	Epoch: [68][27/233]	Loss 0.0021 (0.0110)	
training:	Epoch: [68][28/233]	Loss 0.0020 (0.0107)	
training:	Epoch: [68][29/233]	Loss 0.0019 (0.0104)	
training:	Epoch: [68][30/233]	Loss 0.0033 (0.0101)	
training:	Epoch: [68][31/233]	Loss 0.0071 (0.0100)	
training:	Epoch: [68][32/233]	Loss 0.0020 (0.0098)	
training:	Epoch: [68][33/233]	Loss 0.0221 (0.0101)	
training:	Epoch: [68][34/233]	Loss 0.0025 (0.0099)	
training:	Epoch: [68][35/233]	Loss 0.0022 (0.0097)	
training:	Epoch: [68][36/233]	Loss 0.0019 (0.0095)	
training:	Epoch: [68][37/233]	Loss 0.0067 (0.0094)	
training:	Epoch: [68][38/233]	Loss 0.2244 (0.0151)	
training:	Epoch: [68][39/233]	Loss 0.0017 (0.0147)	
training:	Epoch: [68][40/233]	Loss 0.0022 (0.0144)	
training:	Epoch: [68][41/233]	Loss 0.0024 (0.0141)	
training:	Epoch: [68][42/233]	Loss 0.0028 (0.0138)	
training:	Epoch: [68][43/233]	Loss 0.0018 (0.0136)	
training:	Epoch: [68][44/233]	Loss 0.0835 (0.0152)	
training:	Epoch: [68][45/233]	Loss 0.0019 (0.0149)	
training:	Epoch: [68][46/233]	Loss 0.0024 (0.0146)	
training:	Epoch: [68][47/233]	Loss 0.0102 (0.0145)	
training:	Epoch: [68][48/233]	Loss 0.0030 (0.0143)	
training:	Epoch: [68][49/233]	Loss 0.0018 (0.0140)	
training:	Epoch: [68][50/233]	Loss 0.0018 (0.0138)	
training:	Epoch: [68][51/233]	Loss 0.2055 (0.0175)	
training:	Epoch: [68][52/233]	Loss 0.0024 (0.0172)	
training:	Epoch: [68][53/233]	Loss 0.0020 (0.0169)	
training:	Epoch: [68][54/233]	Loss 0.0019 (0.0167)	
training:	Epoch: [68][55/233]	Loss 0.1035 (0.0182)	
training:	Epoch: [68][56/233]	Loss 0.0037 (0.0180)	
training:	Epoch: [68][57/233]	Loss 0.0017 (0.0177)	
training:	Epoch: [68][58/233]	Loss 0.0020 (0.0174)	
training:	Epoch: [68][59/233]	Loss 0.0267 (0.0176)	
training:	Epoch: [68][60/233]	Loss 0.0028 (0.0173)	
training:	Epoch: [68][61/233]	Loss 0.0041 (0.0171)	
training:	Epoch: [68][62/233]	Loss 0.0018 (0.0169)	
training:	Epoch: [68][63/233]	Loss 0.0020 (0.0166)	
training:	Epoch: [68][64/233]	Loss 0.0018 (0.0164)	
training:	Epoch: [68][65/233]	Loss 0.0030 (0.0162)	
training:	Epoch: [68][66/233]	Loss 0.0061 (0.0160)	
training:	Epoch: [68][67/233]	Loss 0.0017 (0.0158)	
training:	Epoch: [68][68/233]	Loss 0.0020 (0.0156)	
training:	Epoch: [68][69/233]	Loss 0.0027 (0.0154)	
training:	Epoch: [68][70/233]	Loss 0.2022 (0.0181)	
training:	Epoch: [68][71/233]	Loss 0.0151 (0.0181)	
training:	Epoch: [68][72/233]	Loss 0.0017 (0.0178)	
training:	Epoch: [68][73/233]	Loss 0.0035 (0.0176)	
training:	Epoch: [68][74/233]	Loss 0.0033 (0.0174)	
training:	Epoch: [68][75/233]	Loss 0.0020 (0.0172)	
training:	Epoch: [68][76/233]	Loss 0.0132 (0.0172)	
training:	Epoch: [68][77/233]	Loss 0.0020 (0.0170)	
training:	Epoch: [68][78/233]	Loss 0.0263 (0.0171)	
training:	Epoch: [68][79/233]	Loss 0.0036 (0.0169)	
training:	Epoch: [68][80/233]	Loss 0.0020 (0.0168)	
training:	Epoch: [68][81/233]	Loss 0.0022 (0.0166)	
training:	Epoch: [68][82/233]	Loss 0.0019 (0.0164)	
training:	Epoch: [68][83/233]	Loss 0.0055 (0.0163)	
training:	Epoch: [68][84/233]	Loss 0.0019 (0.0161)	
training:	Epoch: [68][85/233]	Loss 0.0022 (0.0159)	
training:	Epoch: [68][86/233]	Loss 0.0022 (0.0158)	
training:	Epoch: [68][87/233]	Loss 0.0041 (0.0156)	
training:	Epoch: [68][88/233]	Loss 0.0333 (0.0158)	
training:	Epoch: [68][89/233]	Loss 0.0042 (0.0157)	
training:	Epoch: [68][90/233]	Loss 0.0028 (0.0156)	
training:	Epoch: [68][91/233]	Loss 0.0019 (0.0154)	
training:	Epoch: [68][92/233]	Loss 0.0039 (0.0153)	
training:	Epoch: [68][93/233]	Loss 0.0032 (0.0152)	
training:	Epoch: [68][94/233]	Loss 0.0018 (0.0150)	
training:	Epoch: [68][95/233]	Loss 0.0022 (0.0149)	
training:	Epoch: [68][96/233]	Loss 0.0017 (0.0147)	
training:	Epoch: [68][97/233]	Loss 0.0017 (0.0146)	
training:	Epoch: [68][98/233]	Loss 0.0337 (0.0148)	
training:	Epoch: [68][99/233]	Loss 0.0031 (0.0147)	
training:	Epoch: [68][100/233]	Loss 0.0159 (0.0147)	
training:	Epoch: [68][101/233]	Loss 0.0043 (0.0146)	
training:	Epoch: [68][102/233]	Loss 0.0023 (0.0145)	
training:	Epoch: [68][103/233]	Loss 0.0024 (0.0144)	
training:	Epoch: [68][104/233]	Loss 0.0024 (0.0142)	
training:	Epoch: [68][105/233]	Loss 0.0020 (0.0141)	
training:	Epoch: [68][106/233]	Loss 0.0018 (0.0140)	
training:	Epoch: [68][107/233]	Loss 0.0021 (0.0139)	
training:	Epoch: [68][108/233]	Loss 0.0018 (0.0138)	
training:	Epoch: [68][109/233]	Loss 0.0093 (0.0137)	
training:	Epoch: [68][110/233]	Loss 0.0017 (0.0136)	
training:	Epoch: [68][111/233]	Loss 0.0017 (0.0135)	
training:	Epoch: [68][112/233]	Loss 0.0018 (0.0134)	
training:	Epoch: [68][113/233]	Loss 0.0524 (0.0138)	
training:	Epoch: [68][114/233]	Loss 0.0021 (0.0137)	
training:	Epoch: [68][115/233]	Loss 0.0022 (0.0136)	
training:	Epoch: [68][116/233]	Loss 0.0034 (0.0135)	
training:	Epoch: [68][117/233]	Loss 0.0096 (0.0134)	
training:	Epoch: [68][118/233]	Loss 0.0039 (0.0134)	
training:	Epoch: [68][119/233]	Loss 0.0021 (0.0133)	
training:	Epoch: [68][120/233]	Loss 0.0026 (0.0132)	
training:	Epoch: [68][121/233]	Loss 0.0017 (0.0131)	
training:	Epoch: [68][122/233]	Loss 0.0019 (0.0130)	
training:	Epoch: [68][123/233]	Loss 0.0023 (0.0129)	
training:	Epoch: [68][124/233]	Loss 0.0027 (0.0128)	
training:	Epoch: [68][125/233]	Loss 0.0020 (0.0127)	
training:	Epoch: [68][126/233]	Loss 0.0110 (0.0127)	
training:	Epoch: [68][127/233]	Loss 0.0023 (0.0126)	
training:	Epoch: [68][128/233]	Loss 0.0022 (0.0126)	
training:	Epoch: [68][129/233]	Loss 0.0029 (0.0125)	
training:	Epoch: [68][130/233]	Loss 0.0265 (0.0126)	
training:	Epoch: [68][131/233]	Loss 0.0112 (0.0126)	
training:	Epoch: [68][132/233]	Loss 0.0018 (0.0125)	
training:	Epoch: [68][133/233]	Loss 0.0018 (0.0124)	
training:	Epoch: [68][134/233]	Loss 0.0029 (0.0124)	
training:	Epoch: [68][135/233]	Loss 0.0025 (0.0123)	
training:	Epoch: [68][136/233]	Loss 0.0572 (0.0126)	
training:	Epoch: [68][137/233]	Loss 0.0046 (0.0125)	
training:	Epoch: [68][138/233]	Loss 0.0020 (0.0125)	
training:	Epoch: [68][139/233]	Loss 0.0017 (0.0124)	
training:	Epoch: [68][140/233]	Loss 0.0020 (0.0123)	
training:	Epoch: [68][141/233]	Loss 0.0015 (0.0122)	
training:	Epoch: [68][142/233]	Loss 0.0093 (0.0122)	
training:	Epoch: [68][143/233]	Loss 0.1627 (0.0133)	
training:	Epoch: [68][144/233]	Loss 0.0017 (0.0132)	
training:	Epoch: [68][145/233]	Loss 0.0040 (0.0131)	
training:	Epoch: [68][146/233]	Loss 0.0033 (0.0131)	
training:	Epoch: [68][147/233]	Loss 0.0022 (0.0130)	
training:	Epoch: [68][148/233]	Loss 0.0024 (0.0129)	
training:	Epoch: [68][149/233]	Loss 0.0213 (0.0130)	
training:	Epoch: [68][150/233]	Loss 0.0211 (0.0130)	
training:	Epoch: [68][151/233]	Loss 0.0032 (0.0130)	
training:	Epoch: [68][152/233]	Loss 0.0017 (0.0129)	
training:	Epoch: [68][153/233]	Loss 0.0019 (0.0128)	
training:	Epoch: [68][154/233]	Loss 0.2200 (0.0142)	
training:	Epoch: [68][155/233]	Loss 0.0022 (0.0141)	
training:	Epoch: [68][156/233]	Loss 0.0022 (0.0140)	
training:	Epoch: [68][157/233]	Loss 0.0212 (0.0141)	
training:	Epoch: [68][158/233]	Loss 0.0017 (0.0140)	
training:	Epoch: [68][159/233]	Loss 0.0019 (0.0139)	
training:	Epoch: [68][160/233]	Loss 0.0019 (0.0138)	
training:	Epoch: [68][161/233]	Loss 0.0027 (0.0138)	
training:	Epoch: [68][162/233]	Loss 0.0020 (0.0137)	
training:	Epoch: [68][163/233]	Loss 0.0018 (0.0136)	
training:	Epoch: [68][164/233]	Loss 0.0020 (0.0135)	
training:	Epoch: [68][165/233]	Loss 0.0018 (0.0135)	
training:	Epoch: [68][166/233]	Loss 0.0017 (0.0134)	
training:	Epoch: [68][167/233]	Loss 0.0025 (0.0133)	
training:	Epoch: [68][168/233]	Loss 0.0016 (0.0133)	
training:	Epoch: [68][169/233]	Loss 0.0034 (0.0132)	
training:	Epoch: [68][170/233]	Loss 0.0020 (0.0131)	
training:	Epoch: [68][171/233]	Loss 0.0018 (0.0131)	
training:	Epoch: [68][172/233]	Loss 0.0034 (0.0130)	
training:	Epoch: [68][173/233]	Loss 0.0017 (0.0130)	
training:	Epoch: [68][174/233]	Loss 0.0062 (0.0129)	
training:	Epoch: [68][175/233]	Loss 0.0016 (0.0128)	
training:	Epoch: [68][176/233]	Loss 0.0018 (0.0128)	
training:	Epoch: [68][177/233]	Loss 0.0016 (0.0127)	
training:	Epoch: [68][178/233]	Loss 0.0019 (0.0127)	
training:	Epoch: [68][179/233]	Loss 0.0039 (0.0126)	
training:	Epoch: [68][180/233]	Loss 0.0021 (0.0126)	
training:	Epoch: [68][181/233]	Loss 0.0016 (0.0125)	
training:	Epoch: [68][182/233]	Loss 0.0023 (0.0124)	
training:	Epoch: [68][183/233]	Loss 0.0028 (0.0124)	
training:	Epoch: [68][184/233]	Loss 0.0475 (0.0126)	
training:	Epoch: [68][185/233]	Loss 0.0021 (0.0125)	
training:	Epoch: [68][186/233]	Loss 0.0018 (0.0125)	
training:	Epoch: [68][187/233]	Loss 0.0378 (0.0126)	
training:	Epoch: [68][188/233]	Loss 0.0019 (0.0125)	
training:	Epoch: [68][189/233]	Loss 0.0018 (0.0125)	
training:	Epoch: [68][190/233]	Loss 0.0021 (0.0124)	
training:	Epoch: [68][191/233]	Loss 0.0022 (0.0124)	
training:	Epoch: [68][192/233]	Loss 0.0017 (0.0123)	
training:	Epoch: [68][193/233]	Loss 0.0015 (0.0123)	
training:	Epoch: [68][194/233]	Loss 0.0018 (0.0122)	
training:	Epoch: [68][195/233]	Loss 0.0026 (0.0122)	
training:	Epoch: [68][196/233]	Loss 0.0067 (0.0121)	
training:	Epoch: [68][197/233]	Loss 0.0025 (0.0121)	
training:	Epoch: [68][198/233]	Loss 0.0017 (0.0120)	
training:	Epoch: [68][199/233]	Loss 0.1944 (0.0129)	
training:	Epoch: [68][200/233]	Loss 0.0022 (0.0129)	
training:	Epoch: [68][201/233]	Loss 0.0018 (0.0128)	
training:	Epoch: [68][202/233]	Loss 0.0020 (0.0128)	
training:	Epoch: [68][203/233]	Loss 0.0023 (0.0127)	
training:	Epoch: [68][204/233]	Loss 0.2138 (0.0137)	
training:	Epoch: [68][205/233]	Loss 0.0021 (0.0137)	
training:	Epoch: [68][206/233]	Loss 0.0018 (0.0136)	
training:	Epoch: [68][207/233]	Loss 0.0099 (0.0136)	
training:	Epoch: [68][208/233]	Loss 0.0022 (0.0135)	
training:	Epoch: [68][209/233]	Loss 0.0020 (0.0135)	
training:	Epoch: [68][210/233]	Loss 0.0024 (0.0134)	
training:	Epoch: [68][211/233]	Loss 0.0150 (0.0134)	
training:	Epoch: [68][212/233]	Loss 0.0020 (0.0134)	
training:	Epoch: [68][213/233]	Loss 0.0028 (0.0133)	
training:	Epoch: [68][214/233]	Loss 0.0020 (0.0133)	
training:	Epoch: [68][215/233]	Loss 0.0023 (0.0132)	
training:	Epoch: [68][216/233]	Loss 0.0025 (0.0132)	
training:	Epoch: [68][217/233]	Loss 0.0024 (0.0131)	
training:	Epoch: [68][218/233]	Loss 0.0017 (0.0131)	
training:	Epoch: [68][219/233]	Loss 0.0026 (0.0130)	
training:	Epoch: [68][220/233]	Loss 0.0020 (0.0130)	
training:	Epoch: [68][221/233]	Loss 0.0020 (0.0129)	
training:	Epoch: [68][222/233]	Loss 0.0017 (0.0129)	
training:	Epoch: [68][223/233]	Loss 0.0023 (0.0128)	
training:	Epoch: [68][224/233]	Loss 0.0018 (0.0128)	
training:	Epoch: [68][225/233]	Loss 0.0142 (0.0128)	
training:	Epoch: [68][226/233]	Loss 0.0018 (0.0127)	
training:	Epoch: [68][227/233]	Loss 0.0029 (0.0127)	
training:	Epoch: [68][228/233]	Loss 0.0029 (0.0126)	
training:	Epoch: [68][229/233]	Loss 0.0043 (0.0126)	
training:	Epoch: [68][230/233]	Loss 0.0018 (0.0126)	
training:	Epoch: [68][231/233]	Loss 0.1888 (0.0133)	
training:	Epoch: [68][232/233]	Loss 0.0080 (0.0133)	
training:	Epoch: [68][233/233]	Loss 0.0021 (0.0133)	
Training:	 Loss: 0.0132

Training:	 ACC: 0.9992 0.9992 0.9992 0.9992
Validation:	 ACC: 0.7880 0.7876 0.7794 0.7966
Validation:	 Best_BACC: 0.8162 0.8154 0.7998 0.8326
Validation:	 Loss: 1.0446
Pretraining:	Epoch 69/200
----------
training:	Epoch: [69][1/233]	Loss 0.0478 (0.0478)	
training:	Epoch: [69][2/233]	Loss 0.0019 (0.0249)	
training:	Epoch: [69][3/233]	Loss 0.0020 (0.0172)	
training:	Epoch: [69][4/233]	Loss 0.0016 (0.0133)	
training:	Epoch: [69][5/233]	Loss 0.0083 (0.0123)	
training:	Epoch: [69][6/233]	Loss 0.0021 (0.0106)	
training:	Epoch: [69][7/233]	Loss 0.0021 (0.0094)	
training:	Epoch: [69][8/233]	Loss 0.0024 (0.0085)	
training:	Epoch: [69][9/233]	Loss 0.0033 (0.0079)	
training:	Epoch: [69][10/233]	Loss 0.0025 (0.0074)	
training:	Epoch: [69][11/233]	Loss 0.0053 (0.0072)	
training:	Epoch: [69][12/233]	Loss 0.0060 (0.0071)	
training:	Epoch: [69][13/233]	Loss 0.0050 (0.0069)	
training:	Epoch: [69][14/233]	Loss 0.0018 (0.0066)	
training:	Epoch: [69][15/233]	Loss 0.0016 (0.0062)	
training:	Epoch: [69][16/233]	Loss 0.0023 (0.0060)	
training:	Epoch: [69][17/233]	Loss 0.1981 (0.0173)	
training:	Epoch: [69][18/233]	Loss 0.0017 (0.0164)	
training:	Epoch: [69][19/233]	Loss 0.0464 (0.0180)	
training:	Epoch: [69][20/233]	Loss 0.0027 (0.0172)	
training:	Epoch: [69][21/233]	Loss 0.0222 (0.0175)	
training:	Epoch: [69][22/233]	Loss 0.0018 (0.0168)	
training:	Epoch: [69][23/233]	Loss 0.0015 (0.0161)	
training:	Epoch: [69][24/233]	Loss 0.0017 (0.0155)	
training:	Epoch: [69][25/233]	Loss 0.0024 (0.0150)	
training:	Epoch: [69][26/233]	Loss 0.0018 (0.0145)	
training:	Epoch: [69][27/233]	Loss 0.0062 (0.0142)	
training:	Epoch: [69][28/233]	Loss 0.0020 (0.0137)	
training:	Epoch: [69][29/233]	Loss 0.0021 (0.0133)	
training:	Epoch: [69][30/233]	Loss 0.0017 (0.0129)	
training:	Epoch: [69][31/233]	Loss 0.0025 (0.0126)	
training:	Epoch: [69][32/233]	Loss 0.0017 (0.0123)	
training:	Epoch: [69][33/233]	Loss 0.0018 (0.0120)	
training:	Epoch: [69][34/233]	Loss 0.0542 (0.0132)	
training:	Epoch: [69][35/233]	Loss 0.0017 (0.0129)	
training:	Epoch: [69][36/233]	Loss 0.0024 (0.0126)	
training:	Epoch: [69][37/233]	Loss 0.0019 (0.0123)	
training:	Epoch: [69][38/233]	Loss 0.1707 (0.0165)	
training:	Epoch: [69][39/233]	Loss 0.0019 (0.0161)	
training:	Epoch: [69][40/233]	Loss 0.0017 (0.0157)	
training:	Epoch: [69][41/233]	Loss 0.0033 (0.0154)	
training:	Epoch: [69][42/233]	Loss 0.0035 (0.0151)	
training:	Epoch: [69][43/233]	Loss 0.0017 (0.0148)	
training:	Epoch: [69][44/233]	Loss 0.0026 (0.0145)	
training:	Epoch: [69][45/233]	Loss 0.2102 (0.0189)	
training:	Epoch: [69][46/233]	Loss 0.0033 (0.0186)	
training:	Epoch: [69][47/233]	Loss 0.0019 (0.0182)	
training:	Epoch: [69][48/233]	Loss 0.0025 (0.0179)	
training:	Epoch: [69][49/233]	Loss 0.0020 (0.0175)	
training:	Epoch: [69][50/233]	Loss 0.0018 (0.0172)	
training:	Epoch: [69][51/233]	Loss 0.0029 (0.0170)	
training:	Epoch: [69][52/233]	Loss 0.0028 (0.0167)	
training:	Epoch: [69][53/233]	Loss 0.0027 (0.0164)	
training:	Epoch: [69][54/233]	Loss 0.0021 (0.0162)	
training:	Epoch: [69][55/233]	Loss 0.0024 (0.0159)	
training:	Epoch: [69][56/233]	Loss 0.0022 (0.0157)	
training:	Epoch: [69][57/233]	Loss 0.1963 (0.0188)	
training:	Epoch: [69][58/233]	Loss 0.0023 (0.0185)	
training:	Epoch: [69][59/233]	Loss 0.0017 (0.0183)	
training:	Epoch: [69][60/233]	Loss 0.0017 (0.0180)	
training:	Epoch: [69][61/233]	Loss 0.0018 (0.0177)	
training:	Epoch: [69][62/233]	Loss 0.0018 (0.0175)	
training:	Epoch: [69][63/233]	Loss 0.0030 (0.0172)	
training:	Epoch: [69][64/233]	Loss 0.0019 (0.0170)	
training:	Epoch: [69][65/233]	Loss 0.0205 (0.0170)	
training:	Epoch: [69][66/233]	Loss 0.0068 (0.0169)	
training:	Epoch: [69][67/233]	Loss 0.0444 (0.0173)	
training:	Epoch: [69][68/233]	Loss 0.0019 (0.0171)	
training:	Epoch: [69][69/233]	Loss 0.1695 (0.0193)	
training:	Epoch: [69][70/233]	Loss 0.0045 (0.0191)	
training:	Epoch: [69][71/233]	Loss 0.0023 (0.0188)	
training:	Epoch: [69][72/233]	Loss 0.0019 (0.0186)	
training:	Epoch: [69][73/233]	Loss 0.3989 (0.0238)	
training:	Epoch: [69][74/233]	Loss 0.0049 (0.0236)	
training:	Epoch: [69][75/233]	Loss 0.0026 (0.0233)	
training:	Epoch: [69][76/233]	Loss 0.0027 (0.0230)	
training:	Epoch: [69][77/233]	Loss 0.0238 (0.0230)	
training:	Epoch: [69][78/233]	Loss 0.0018 (0.0227)	
training:	Epoch: [69][79/233]	Loss 0.0016 (0.0225)	
training:	Epoch: [69][80/233]	Loss 0.0020 (0.0222)	
training:	Epoch: [69][81/233]	Loss 0.0020 (0.0220)	
training:	Epoch: [69][82/233]	Loss 0.0050 (0.0218)	
training:	Epoch: [69][83/233]	Loss 0.0129 (0.0217)	
training:	Epoch: [69][84/233]	Loss 0.0028 (0.0214)	
training:	Epoch: [69][85/233]	Loss 0.0022 (0.0212)	
training:	Epoch: [69][86/233]	Loss 0.0019 (0.0210)	
training:	Epoch: [69][87/233]	Loss 0.0041 (0.0208)	
training:	Epoch: [69][88/233]	Loss 0.0018 (0.0206)	
training:	Epoch: [69][89/233]	Loss 0.1403 (0.0219)	
training:	Epoch: [69][90/233]	Loss 0.0019 (0.0217)	
training:	Epoch: [69][91/233]	Loss 0.0037 (0.0215)	
training:	Epoch: [69][92/233]	Loss 0.0031 (0.0213)	
training:	Epoch: [69][93/233]	Loss 0.0204 (0.0213)	
training:	Epoch: [69][94/233]	Loss 0.0021 (0.0211)	
training:	Epoch: [69][95/233]	Loss 0.1379 (0.0223)	
training:	Epoch: [69][96/233]	Loss 0.0030 (0.0221)	
training:	Epoch: [69][97/233]	Loss 0.0114 (0.0220)	
training:	Epoch: [69][98/233]	Loss 0.0286 (0.0221)	
training:	Epoch: [69][99/233]	Loss 0.0052 (0.0219)	
training:	Epoch: [69][100/233]	Loss 0.0025 (0.0217)	
training:	Epoch: [69][101/233]	Loss 0.0034 (0.0215)	
training:	Epoch: [69][102/233]	Loss 0.0019 (0.0213)	
training:	Epoch: [69][103/233]	Loss 0.0544 (0.0216)	
training:	Epoch: [69][104/233]	Loss 0.0042 (0.0215)	
training:	Epoch: [69][105/233]	Loss 0.0018 (0.0213)	
training:	Epoch: [69][106/233]	Loss 0.0022 (0.0211)	
training:	Epoch: [69][107/233]	Loss 0.0020 (0.0209)	
training:	Epoch: [69][108/233]	Loss 0.0020 (0.0208)	
training:	Epoch: [69][109/233]	Loss 0.0272 (0.0208)	
training:	Epoch: [69][110/233]	Loss 0.0018 (0.0206)	
training:	Epoch: [69][111/233]	Loss 0.0019 (0.0205)	
training:	Epoch: [69][112/233]	Loss 0.0020 (0.0203)	
training:	Epoch: [69][113/233]	Loss 0.0021 (0.0202)	
training:	Epoch: [69][114/233]	Loss 0.0026 (0.0200)	
training:	Epoch: [69][115/233]	Loss 0.0601 (0.0203)	
training:	Epoch: [69][116/233]	Loss 0.0019 (0.0202)	
training:	Epoch: [69][117/233]	Loss 0.0038 (0.0200)	
training:	Epoch: [69][118/233]	Loss 0.1707 (0.0213)	
training:	Epoch: [69][119/233]	Loss 0.0018 (0.0212)	
training:	Epoch: [69][120/233]	Loss 0.0023 (0.0210)	
training:	Epoch: [69][121/233]	Loss 0.0023 (0.0208)	
training:	Epoch: [69][122/233]	Loss 0.0020 (0.0207)	
training:	Epoch: [69][123/233]	Loss 0.0018 (0.0205)	
training:	Epoch: [69][124/233]	Loss 0.0020 (0.0204)	
training:	Epoch: [69][125/233]	Loss 0.0017 (0.0202)	
training:	Epoch: [69][126/233]	Loss 0.0029 (0.0201)	
training:	Epoch: [69][127/233]	Loss 0.0080 (0.0200)	
training:	Epoch: [69][128/233]	Loss 0.0023 (0.0199)	
training:	Epoch: [69][129/233]	Loss 0.0015 (0.0197)	
training:	Epoch: [69][130/233]	Loss 0.0051 (0.0196)	
training:	Epoch: [69][131/233]	Loss 0.0022 (0.0195)	
training:	Epoch: [69][132/233]	Loss 0.0018 (0.0193)	
training:	Epoch: [69][133/233]	Loss 0.0017 (0.0192)	
training:	Epoch: [69][134/233]	Loss 0.0058 (0.0191)	
training:	Epoch: [69][135/233]	Loss 0.0016 (0.0190)	
training:	Epoch: [69][136/233]	Loss 0.0097 (0.0189)	
training:	Epoch: [69][137/233]	Loss 0.0239 (0.0190)	
training:	Epoch: [69][138/233]	Loss 0.0017 (0.0188)	
training:	Epoch: [69][139/233]	Loss 0.0022 (0.0187)	
training:	Epoch: [69][140/233]	Loss 0.0035 (0.0186)	
training:	Epoch: [69][141/233]	Loss 0.0184 (0.0186)	
training:	Epoch: [69][142/233]	Loss 0.0019 (0.0185)	
training:	Epoch: [69][143/233]	Loss 0.0018 (0.0184)	
training:	Epoch: [69][144/233]	Loss 0.0019 (0.0182)	
training:	Epoch: [69][145/233]	Loss 0.0316 (0.0183)	
training:	Epoch: [69][146/233]	Loss 0.0028 (0.0182)	
training:	Epoch: [69][147/233]	Loss 0.0031 (0.0181)	
training:	Epoch: [69][148/233]	Loss 0.0757 (0.0185)	
training:	Epoch: [69][149/233]	Loss 0.0025 (0.0184)	
training:	Epoch: [69][150/233]	Loss 0.0019 (0.0183)	
training:	Epoch: [69][151/233]	Loss 0.0018 (0.0182)	
training:	Epoch: [69][152/233]	Loss 0.0070 (0.0181)	
training:	Epoch: [69][153/233]	Loss 0.0026 (0.0180)	
training:	Epoch: [69][154/233]	Loss 0.1397 (0.0188)	
training:	Epoch: [69][155/233]	Loss 0.0018 (0.0187)	
training:	Epoch: [69][156/233]	Loss 0.0036 (0.0186)	
training:	Epoch: [69][157/233]	Loss 0.0052 (0.0185)	
training:	Epoch: [69][158/233]	Loss 0.0026 (0.0184)	
training:	Epoch: [69][159/233]	Loss 0.0022 (0.0183)	
training:	Epoch: [69][160/233]	Loss 0.0026 (0.0182)	
training:	Epoch: [69][161/233]	Loss 0.0023 (0.0181)	
training:	Epoch: [69][162/233]	Loss 0.0160 (0.0181)	
training:	Epoch: [69][163/233]	Loss 0.0040 (0.0180)	
training:	Epoch: [69][164/233]	Loss 0.0023 (0.0179)	
training:	Epoch: [69][165/233]	Loss 0.1213 (0.0185)	
training:	Epoch: [69][166/233]	Loss 0.0035 (0.0185)	
training:	Epoch: [69][167/233]	Loss 0.0020 (0.0184)	
training:	Epoch: [69][168/233]	Loss 0.0022 (0.0183)	
training:	Epoch: [69][169/233]	Loss 0.0020 (0.0182)	
training:	Epoch: [69][170/233]	Loss 0.0070 (0.0181)	
training:	Epoch: [69][171/233]	Loss 0.0200 (0.0181)	
training:	Epoch: [69][172/233]	Loss 0.0030 (0.0180)	
training:	Epoch: [69][173/233]	Loss 0.0026 (0.0179)	
training:	Epoch: [69][174/233]	Loss 0.0022 (0.0178)	
training:	Epoch: [69][175/233]	Loss 0.0018 (0.0178)	
training:	Epoch: [69][176/233]	Loss 0.0019 (0.0177)	
training:	Epoch: [69][177/233]	Loss 0.0029 (0.0176)	
training:	Epoch: [69][178/233]	Loss 0.0026 (0.0175)	
training:	Epoch: [69][179/233]	Loss 0.0030 (0.0174)	
training:	Epoch: [69][180/233]	Loss 0.0093 (0.0174)	
training:	Epoch: [69][181/233]	Loss 0.0021 (0.0173)	
training:	Epoch: [69][182/233]	Loss 0.0023 (0.0172)	
training:	Epoch: [69][183/233]	Loss 0.0017 (0.0171)	
training:	Epoch: [69][184/233]	Loss 0.0016 (0.0170)	
training:	Epoch: [69][185/233]	Loss 0.0025 (0.0170)	
training:	Epoch: [69][186/233]	Loss 0.0023 (0.0169)	
training:	Epoch: [69][187/233]	Loss 0.0019 (0.0168)	
training:	Epoch: [69][188/233]	Loss 0.0030 (0.0167)	
training:	Epoch: [69][189/233]	Loss 0.0045 (0.0167)	
training:	Epoch: [69][190/233]	Loss 0.0032 (0.0166)	
training:	Epoch: [69][191/233]	Loss 0.0016 (0.0165)	
training:	Epoch: [69][192/233]	Loss 0.0066 (0.0165)	
training:	Epoch: [69][193/233]	Loss 0.0029 (0.0164)	
training:	Epoch: [69][194/233]	Loss 0.1192 (0.0169)	
training:	Epoch: [69][195/233]	Loss 0.0123 (0.0169)	
training:	Epoch: [69][196/233]	Loss 0.0111 (0.0169)	
training:	Epoch: [69][197/233]	Loss 0.0017 (0.0168)	
training:	Epoch: [69][198/233]	Loss 0.0018 (0.0167)	
training:	Epoch: [69][199/233]	Loss 0.0018 (0.0166)	
training:	Epoch: [69][200/233]	Loss 0.0024 (0.0166)	
training:	Epoch: [69][201/233]	Loss 0.0017 (0.0165)	
training:	Epoch: [69][202/233]	Loss 0.1859 (0.0173)	
training:	Epoch: [69][203/233]	Loss 0.0024 (0.0173)	
training:	Epoch: [69][204/233]	Loss 0.0066 (0.0172)	
training:	Epoch: [69][205/233]	Loss 0.0017 (0.0171)	
training:	Epoch: [69][206/233]	Loss 0.0029 (0.0171)	
training:	Epoch: [69][207/233]	Loss 0.0019 (0.0170)	
training:	Epoch: [69][208/233]	Loss 0.0019 (0.0169)	
training:	Epoch: [69][209/233]	Loss 0.0037 (0.0168)	
training:	Epoch: [69][210/233]	Loss 0.0019 (0.0168)	
training:	Epoch: [69][211/233]	Loss 0.0033 (0.0167)	
training:	Epoch: [69][212/233]	Loss 0.0066 (0.0167)	
training:	Epoch: [69][213/233]	Loss 0.0103 (0.0166)	
training:	Epoch: [69][214/233]	Loss 0.0020 (0.0166)	
training:	Epoch: [69][215/233]	Loss 0.0021 (0.0165)	
training:	Epoch: [69][216/233]	Loss 0.0021 (0.0164)	
training:	Epoch: [69][217/233]	Loss 0.0019 (0.0164)	
training:	Epoch: [69][218/233]	Loss 0.0018 (0.0163)	
training:	Epoch: [69][219/233]	Loss 0.0020 (0.0162)	
training:	Epoch: [69][220/233]	Loss 0.0017 (0.0162)	
training:	Epoch: [69][221/233]	Loss 0.0038 (0.0161)	
training:	Epoch: [69][222/233]	Loss 0.0021 (0.0161)	
training:	Epoch: [69][223/233]	Loss 0.0021 (0.0160)	
training:	Epoch: [69][224/233]	Loss 0.0024 (0.0159)	
training:	Epoch: [69][225/233]	Loss 0.0018 (0.0159)	
training:	Epoch: [69][226/233]	Loss 0.0019 (0.0158)	
training:	Epoch: [69][227/233]	Loss 0.0031 (0.0157)	
training:	Epoch: [69][228/233]	Loss 0.0047 (0.0157)	
training:	Epoch: [69][229/233]	Loss 0.0041 (0.0156)	
training:	Epoch: [69][230/233]	Loss 0.0018 (0.0156)	
training:	Epoch: [69][231/233]	Loss 0.0019 (0.0155)	
training:	Epoch: [69][232/233]	Loss 0.0020 (0.0155)	
training:	Epoch: [69][233/233]	Loss 0.0071 (0.0154)	
Training:	 Loss: 0.0154

Training:	 ACC: 0.9992 0.9992 0.9992 0.9992
Validation:	 ACC: 0.7902 0.7892 0.7681 0.8124
Validation:	 Best_BACC: 0.8162 0.8154 0.7998 0.8326
Validation:	 Loss: 1.0428
Pretraining:	Epoch 70/200
----------
training:	Epoch: [70][1/233]	Loss 0.0020 (0.0020)	
training:	Epoch: [70][2/233]	Loss 0.0019 (0.0020)	
training:	Epoch: [70][3/233]	Loss 0.0022 (0.0020)	
training:	Epoch: [70][4/233]	Loss 0.0018 (0.0020)	
training:	Epoch: [70][5/233]	Loss 0.0040 (0.0024)	
training:	Epoch: [70][6/233]	Loss 0.0018 (0.0023)	
training:	Epoch: [70][7/233]	Loss 0.0026 (0.0023)	
training:	Epoch: [70][8/233]	Loss 0.0021 (0.0023)	
training:	Epoch: [70][9/233]	Loss 0.0016 (0.0022)	
training:	Epoch: [70][10/233]	Loss 0.0017 (0.0022)	
training:	Epoch: [70][11/233]	Loss 0.0019 (0.0021)	
training:	Epoch: [70][12/233]	Loss 0.0017 (0.0021)	
training:	Epoch: [70][13/233]	Loss 0.0029 (0.0022)	
training:	Epoch: [70][14/233]	Loss 0.0097 (0.0027)	
training:	Epoch: [70][15/233]	Loss 0.0017 (0.0026)	
training:	Epoch: [70][16/233]	Loss 0.0019 (0.0026)	
training:	Epoch: [70][17/233]	Loss 0.0068 (0.0028)	
training:	Epoch: [70][18/233]	Loss 0.0063 (0.0030)	
training:	Epoch: [70][19/233]	Loss 0.0016 (0.0029)	
training:	Epoch: [70][20/233]	Loss 0.0178 (0.0037)	
training:	Epoch: [70][21/233]	Loss 0.0016 (0.0036)	
training:	Epoch: [70][22/233]	Loss 0.0018 (0.0035)	
training:	Epoch: [70][23/233]	Loss 0.2025 (0.0122)	
training:	Epoch: [70][24/233]	Loss 0.0030 (0.0118)	
training:	Epoch: [70][25/233]	Loss 0.0023 (0.0114)	
training:	Epoch: [70][26/233]	Loss 0.0099 (0.0113)	
training:	Epoch: [70][27/233]	Loss 0.0091 (0.0113)	
training:	Epoch: [70][28/233]	Loss 0.0018 (0.0109)	
training:	Epoch: [70][29/233]	Loss 0.0015 (0.0106)	
training:	Epoch: [70][30/233]	Loss 0.0025 (0.0103)	
training:	Epoch: [70][31/233]	Loss 0.0017 (0.0100)	
training:	Epoch: [70][32/233]	Loss 0.0019 (0.0098)	
training:	Epoch: [70][33/233]	Loss 0.0042 (0.0096)	
training:	Epoch: [70][34/233]	Loss 0.0340 (0.0103)	
training:	Epoch: [70][35/233]	Loss 0.0015 (0.0101)	
training:	Epoch: [70][36/233]	Loss 0.0028 (0.0099)	
training:	Epoch: [70][37/233]	Loss 0.0023 (0.0097)	
training:	Epoch: [70][38/233]	Loss 0.0023 (0.0095)	
training:	Epoch: [70][39/233]	Loss 0.0021 (0.0093)	
training:	Epoch: [70][40/233]	Loss 0.0018 (0.0091)	
training:	Epoch: [70][41/233]	Loss 0.0019 (0.0089)	
training:	Epoch: [70][42/233]	Loss 0.0020 (0.0088)	
training:	Epoch: [70][43/233]	Loss 0.0192 (0.0090)	
training:	Epoch: [70][44/233]	Loss 0.0017 (0.0088)	
training:	Epoch: [70][45/233]	Loss 0.0018 (0.0087)	
training:	Epoch: [70][46/233]	Loss 0.0641 (0.0099)	
training:	Epoch: [70][47/233]	Loss 0.0016 (0.0097)	
training:	Epoch: [70][48/233]	Loss 0.0018 (0.0096)	
training:	Epoch: [70][49/233]	Loss 0.0015 (0.0094)	
training:	Epoch: [70][50/233]	Loss 0.0019 (0.0092)	
training:	Epoch: [70][51/233]	Loss 0.0023 (0.0091)	
training:	Epoch: [70][52/233]	Loss 0.0016 (0.0090)	
training:	Epoch: [70][53/233]	Loss 0.0017 (0.0088)	
training:	Epoch: [70][54/233]	Loss 0.0023 (0.0087)	
training:	Epoch: [70][55/233]	Loss 0.0018 (0.0086)	
training:	Epoch: [70][56/233]	Loss 0.0022 (0.0085)	
training:	Epoch: [70][57/233]	Loss 0.0017 (0.0083)	
training:	Epoch: [70][58/233]	Loss 0.0016 (0.0082)	
training:	Epoch: [70][59/233]	Loss 0.0642 (0.0092)	
training:	Epoch: [70][60/233]	Loss 0.0027 (0.0091)	
training:	Epoch: [70][61/233]	Loss 0.0020 (0.0090)	
training:	Epoch: [70][62/233]	Loss 0.0019 (0.0088)	
training:	Epoch: [70][63/233]	Loss 0.0017 (0.0087)	
training:	Epoch: [70][64/233]	Loss 0.1896 (0.0116)	
training:	Epoch: [70][65/233]	Loss 0.0026 (0.0114)	
training:	Epoch: [70][66/233]	Loss 0.0018 (0.0113)	
training:	Epoch: [70][67/233]	Loss 0.2112 (0.0143)	
training:	Epoch: [70][68/233]	Loss 0.0017 (0.0141)	
training:	Epoch: [70][69/233]	Loss 0.0018 (0.0139)	
training:	Epoch: [70][70/233]	Loss 0.0020 (0.0137)	
training:	Epoch: [70][71/233]	Loss 0.0019 (0.0136)	
training:	Epoch: [70][72/233]	Loss 0.0036 (0.0134)	
training:	Epoch: [70][73/233]	Loss 0.0019 (0.0133)	
training:	Epoch: [70][74/233]	Loss 0.0018 (0.0131)	
training:	Epoch: [70][75/233]	Loss 0.0037 (0.0130)	
training:	Epoch: [70][76/233]	Loss 0.0016 (0.0128)	
training:	Epoch: [70][77/233]	Loss 0.0018 (0.0127)	
training:	Epoch: [70][78/233]	Loss 0.0023 (0.0126)	
training:	Epoch: [70][79/233]	Loss 0.0030 (0.0124)	
training:	Epoch: [70][80/233]	Loss 0.0025 (0.0123)	
training:	Epoch: [70][81/233]	Loss 0.0017 (0.0122)	
training:	Epoch: [70][82/233]	Loss 0.0391 (0.0125)	
training:	Epoch: [70][83/233]	Loss 0.0018 (0.0124)	
training:	Epoch: [70][84/233]	Loss 0.0018 (0.0123)	
training:	Epoch: [70][85/233]	Loss 0.0153 (0.0123)	
training:	Epoch: [70][86/233]	Loss 0.0031 (0.0122)	
training:	Epoch: [70][87/233]	Loss 0.0017 (0.0121)	
training:	Epoch: [70][88/233]	Loss 0.0022 (0.0119)	
training:	Epoch: [70][89/233]	Loss 0.0034 (0.0119)	
training:	Epoch: [70][90/233]	Loss 0.0048 (0.0118)	
training:	Epoch: [70][91/233]	Loss 0.0021 (0.0117)	
training:	Epoch: [70][92/233]	Loss 0.0022 (0.0116)	
training:	Epoch: [70][93/233]	Loss 0.0657 (0.0121)	
training:	Epoch: [70][94/233]	Loss 0.0029 (0.0120)	
training:	Epoch: [70][95/233]	Loss 0.0033 (0.0120)	
training:	Epoch: [70][96/233]	Loss 0.0017 (0.0118)	
training:	Epoch: [70][97/233]	Loss 0.0018 (0.0117)	
training:	Epoch: [70][98/233]	Loss 0.0457 (0.0121)	
training:	Epoch: [70][99/233]	Loss 0.0023 (0.0120)	
training:	Epoch: [70][100/233]	Loss 0.0017 (0.0119)	
training:	Epoch: [70][101/233]	Loss 0.0134 (0.0119)	
training:	Epoch: [70][102/233]	Loss 0.0018 (0.0118)	
training:	Epoch: [70][103/233]	Loss 0.0017 (0.0117)	
training:	Epoch: [70][104/233]	Loss 0.0016 (0.0116)	
training:	Epoch: [70][105/233]	Loss 0.0052 (0.0115)	
training:	Epoch: [70][106/233]	Loss 0.0143 (0.0116)	
training:	Epoch: [70][107/233]	Loss 0.0050 (0.0115)	
training:	Epoch: [70][108/233]	Loss 0.0178 (0.0116)	
training:	Epoch: [70][109/233]	Loss 0.0018 (0.0115)	
training:	Epoch: [70][110/233]	Loss 0.0016 (0.0114)	
training:	Epoch: [70][111/233]	Loss 0.0018 (0.0113)	
training:	Epoch: [70][112/233]	Loss 0.0017 (0.0112)	
training:	Epoch: [70][113/233]	Loss 0.0200 (0.0113)	
training:	Epoch: [70][114/233]	Loss 0.0035 (0.0112)	
training:	Epoch: [70][115/233]	Loss 0.0020 (0.0112)	
training:	Epoch: [70][116/233]	Loss 0.0017 (0.0111)	
training:	Epoch: [70][117/233]	Loss 0.0017 (0.0110)	
training:	Epoch: [70][118/233]	Loss 0.0020 (0.0109)	
training:	Epoch: [70][119/233]	Loss 0.0018 (0.0108)	
training:	Epoch: [70][120/233]	Loss 0.0023 (0.0108)	
training:	Epoch: [70][121/233]	Loss 0.0903 (0.0114)	
training:	Epoch: [70][122/233]	Loss 0.0049 (0.0114)	
training:	Epoch: [70][123/233]	Loss 0.0019 (0.0113)	
training:	Epoch: [70][124/233]	Loss 0.0390 (0.0115)	
training:	Epoch: [70][125/233]	Loss 0.1757 (0.0128)	
training:	Epoch: [70][126/233]	Loss 0.1544 (0.0139)	
training:	Epoch: [70][127/233]	Loss 0.0017 (0.0139)	
training:	Epoch: [70][128/233]	Loss 0.0015 (0.0138)	
training:	Epoch: [70][129/233]	Loss 0.0019 (0.0137)	
training:	Epoch: [70][130/233]	Loss 0.0018 (0.0136)	
training:	Epoch: [70][131/233]	Loss 0.0048 (0.0135)	
training:	Epoch: [70][132/233]	Loss 0.0036 (0.0134)	
training:	Epoch: [70][133/233]	Loss 0.0016 (0.0133)	
training:	Epoch: [70][134/233]	Loss 0.0029 (0.0133)	
training:	Epoch: [70][135/233]	Loss 0.0056 (0.0132)	
training:	Epoch: [70][136/233]	Loss 0.0019 (0.0131)	
training:	Epoch: [70][137/233]	Loss 0.0022 (0.0130)	
training:	Epoch: [70][138/233]	Loss 0.0055 (0.0130)	
training:	Epoch: [70][139/233]	Loss 0.0022 (0.0129)	
training:	Epoch: [70][140/233]	Loss 0.0019 (0.0128)	
training:	Epoch: [70][141/233]	Loss 0.0035 (0.0128)	
training:	Epoch: [70][142/233]	Loss 0.0090 (0.0127)	
training:	Epoch: [70][143/233]	Loss 0.0032 (0.0127)	
training:	Epoch: [70][144/233]	Loss 0.0041 (0.0126)	
training:	Epoch: [70][145/233]	Loss 0.0073 (0.0126)	
training:	Epoch: [70][146/233]	Loss 0.0410 (0.0128)	
training:	Epoch: [70][147/233]	Loss 0.0017 (0.0127)	
training:	Epoch: [70][148/233]	Loss 0.0024 (0.0126)	
training:	Epoch: [70][149/233]	Loss 0.0016 (0.0126)	
training:	Epoch: [70][150/233]	Loss 0.0025 (0.0125)	
training:	Epoch: [70][151/233]	Loss 0.0021 (0.0124)	
training:	Epoch: [70][152/233]	Loss 0.0021 (0.0124)	
training:	Epoch: [70][153/233]	Loss 0.0015 (0.0123)	
training:	Epoch: [70][154/233]	Loss 0.0050 (0.0122)	
training:	Epoch: [70][155/233]	Loss 0.0019 (0.0122)	
training:	Epoch: [70][156/233]	Loss 0.0024 (0.0121)	
training:	Epoch: [70][157/233]	Loss 0.0603 (0.0124)	
training:	Epoch: [70][158/233]	Loss 0.0031 (0.0124)	
training:	Epoch: [70][159/233]	Loss 0.0019 (0.0123)	
training:	Epoch: [70][160/233]	Loss 0.0021 (0.0122)	
training:	Epoch: [70][161/233]	Loss 0.0017 (0.0122)	
training:	Epoch: [70][162/233]	Loss 0.1947 (0.0133)	
training:	Epoch: [70][163/233]	Loss 0.0015 (0.0132)	
training:	Epoch: [70][164/233]	Loss 0.0027 (0.0131)	
training:	Epoch: [70][165/233]	Loss 0.0023 (0.0131)	
training:	Epoch: [70][166/233]	Loss 0.0017 (0.0130)	
training:	Epoch: [70][167/233]	Loss 0.0019 (0.0129)	
training:	Epoch: [70][168/233]	Loss 0.0025 (0.0129)	
training:	Epoch: [70][169/233]	Loss 0.0029 (0.0128)	
training:	Epoch: [70][170/233]	Loss 0.0064 (0.0128)	
training:	Epoch: [70][171/233]	Loss 0.2018 (0.0139)	
training:	Epoch: [70][172/233]	Loss 0.0017 (0.0138)	
training:	Epoch: [70][173/233]	Loss 0.0024 (0.0138)	
training:	Epoch: [70][174/233]	Loss 0.1042 (0.0143)	
training:	Epoch: [70][175/233]	Loss 0.0134 (0.0143)	
training:	Epoch: [70][176/233]	Loss 0.0016 (0.0142)	
training:	Epoch: [70][177/233]	Loss 0.0040 (0.0141)	
training:	Epoch: [70][178/233]	Loss 0.0026 (0.0141)	
training:	Epoch: [70][179/233]	Loss 0.0116 (0.0141)	
training:	Epoch: [70][180/233]	Loss 0.0023 (0.0140)	
training:	Epoch: [70][181/233]	Loss 0.0031 (0.0139)	
training:	Epoch: [70][182/233]	Loss 0.0045 (0.0139)	
training:	Epoch: [70][183/233]	Loss 0.0019 (0.0138)	
training:	Epoch: [70][184/233]	Loss 0.0032 (0.0138)	
training:	Epoch: [70][185/233]	Loss 0.0015 (0.0137)	
training:	Epoch: [70][186/233]	Loss 0.0310 (0.0138)	
training:	Epoch: [70][187/233]	Loss 0.0017 (0.0137)	
training:	Epoch: [70][188/233]	Loss 0.0021 (0.0137)	
training:	Epoch: [70][189/233]	Loss 0.2965 (0.0152)	
training:	Epoch: [70][190/233]	Loss 0.0027 (0.0151)	
training:	Epoch: [70][191/233]	Loss 0.0014 (0.0150)	
training:	Epoch: [70][192/233]	Loss 0.0016 (0.0149)	
training:	Epoch: [70][193/233]	Loss 0.0017 (0.0149)	
training:	Epoch: [70][194/233]	Loss 0.0025 (0.0148)	
training:	Epoch: [70][195/233]	Loss 0.0022 (0.0148)	
training:	Epoch: [70][196/233]	Loss 0.0017 (0.0147)	
training:	Epoch: [70][197/233]	Loss 0.0017 (0.0146)	
training:	Epoch: [70][198/233]	Loss 0.0016 (0.0146)	
training:	Epoch: [70][199/233]	Loss 0.0019 (0.0145)	
training:	Epoch: [70][200/233]	Loss 0.0020 (0.0144)	
training:	Epoch: [70][201/233]	Loss 0.0014 (0.0144)	
training:	Epoch: [70][202/233]	Loss 0.0020 (0.0143)	
training:	Epoch: [70][203/233]	Loss 0.0017 (0.0142)	
training:	Epoch: [70][204/233]	Loss 0.0016 (0.0142)	
training:	Epoch: [70][205/233]	Loss 0.0022 (0.0141)	
training:	Epoch: [70][206/233]	Loss 0.0020 (0.0141)	
training:	Epoch: [70][207/233]	Loss 0.0129 (0.0141)	
training:	Epoch: [70][208/233]	Loss 0.0243 (0.0141)	
training:	Epoch: [70][209/233]	Loss 0.0052 (0.0141)	
training:	Epoch: [70][210/233]	Loss 0.0022 (0.0140)	
training:	Epoch: [70][211/233]	Loss 0.0018 (0.0139)	
training:	Epoch: [70][212/233]	Loss 0.0036 (0.0139)	
training:	Epoch: [70][213/233]	Loss 0.0022 (0.0138)	
training:	Epoch: [70][214/233]	Loss 0.0018 (0.0138)	
training:	Epoch: [70][215/233]	Loss 0.0017 (0.0137)	
training:	Epoch: [70][216/233]	Loss 0.0016 (0.0137)	
training:	Epoch: [70][217/233]	Loss 0.0022 (0.0136)	
training:	Epoch: [70][218/233]	Loss 0.0084 (0.0136)	
training:	Epoch: [70][219/233]	Loss 0.1747 (0.0143)	
training:	Epoch: [70][220/233]	Loss 0.0041 (0.0143)	
training:	Epoch: [70][221/233]	Loss 0.0021 (0.0142)	
training:	Epoch: [70][222/233]	Loss 0.0019 (0.0142)	
training:	Epoch: [70][223/233]	Loss 0.0024 (0.0141)	
training:	Epoch: [70][224/233]	Loss 0.0285 (0.0142)	
training:	Epoch: [70][225/233]	Loss 0.0020 (0.0141)	
training:	Epoch: [70][226/233]	Loss 0.0018 (0.0141)	
training:	Epoch: [70][227/233]	Loss 0.0349 (0.0142)	
training:	Epoch: [70][228/233]	Loss 0.0025 (0.0141)	
training:	Epoch: [70][229/233]	Loss 0.0018 (0.0141)	
training:	Epoch: [70][230/233]	Loss 0.0025 (0.0140)	
training:	Epoch: [70][231/233]	Loss 0.0239 (0.0141)	
training:	Epoch: [70][232/233]	Loss 0.0016 (0.0140)	
training:	Epoch: [70][233/233]	Loss 0.0066 (0.0140)	
Training:	 Loss: 0.0139

Training:	 ACC: 0.9992 0.9992 0.9992 0.9992
Validation:	 ACC: 0.7936 0.7945 0.8141 0.7730
Validation:	 Best_BACC: 0.8162 0.8154 0.7998 0.8326
Validation:	 Loss: 1.0330
Pretraining:	Epoch 71/200
----------
training:	Epoch: [71][1/233]	Loss 0.0018 (0.0018)	
training:	Epoch: [71][2/233]	Loss 0.0018 (0.0018)	
training:	Epoch: [71][3/233]	Loss 0.1943 (0.0660)	
training:	Epoch: [71][4/233]	Loss 0.0041 (0.0505)	
training:	Epoch: [71][5/233]	Loss 0.0565 (0.0517)	
training:	Epoch: [71][6/233]	Loss 0.0405 (0.0498)	
training:	Epoch: [71][7/233]	Loss 0.0018 (0.0430)	
training:	Epoch: [71][8/233]	Loss 0.0020 (0.0379)	
training:	Epoch: [71][9/233]	Loss 0.0014 (0.0338)	
training:	Epoch: [71][10/233]	Loss 0.0359 (0.0340)	
training:	Epoch: [71][11/233]	Loss 0.0047 (0.0313)	
training:	Epoch: [71][12/233]	Loss 0.0018 (0.0289)	
training:	Epoch: [71][13/233]	Loss 0.0022 (0.0268)	
training:	Epoch: [71][14/233]	Loss 0.0030 (0.0251)	
training:	Epoch: [71][15/233]	Loss 0.0701 (0.0281)	
training:	Epoch: [71][16/233]	Loss 0.0016 (0.0265)	
training:	Epoch: [71][17/233]	Loss 0.0017 (0.0250)	
training:	Epoch: [71][18/233]	Loss 0.0018 (0.0237)	
training:	Epoch: [71][19/233]	Loss 0.0024 (0.0226)	
training:	Epoch: [71][20/233]	Loss 0.1920 (0.0311)	
training:	Epoch: [71][21/233]	Loss 0.0017 (0.0297)	
training:	Epoch: [71][22/233]	Loss 0.0021 (0.0284)	
training:	Epoch: [71][23/233]	Loss 0.0019 (0.0273)	
training:	Epoch: [71][24/233]	Loss 0.0128 (0.0267)	
training:	Epoch: [71][25/233]	Loss 0.1806 (0.0328)	
training:	Epoch: [71][26/233]	Loss 0.1724 (0.0382)	
training:	Epoch: [71][27/233]	Loss 0.0052 (0.0370)	
training:	Epoch: [71][28/233]	Loss 0.0019 (0.0357)	
training:	Epoch: [71][29/233]	Loss 0.0017 (0.0345)	
training:	Epoch: [71][30/233]	Loss 0.0017 (0.0334)	
training:	Epoch: [71][31/233]	Loss 0.0026 (0.0324)	
training:	Epoch: [71][32/233]	Loss 0.1607 (0.0365)	
training:	Epoch: [71][33/233]	Loss 0.0029 (0.0354)	
training:	Epoch: [71][34/233]	Loss 0.0016 (0.0344)	
training:	Epoch: [71][35/233]	Loss 0.0022 (0.0335)	
training:	Epoch: [71][36/233]	Loss 0.0022 (0.0326)	
training:	Epoch: [71][37/233]	Loss 0.0079 (0.0320)	
training:	Epoch: [71][38/233]	Loss 0.0022 (0.0312)	
training:	Epoch: [71][39/233]	Loss 0.0019 (0.0304)	
training:	Epoch: [71][40/233]	Loss 0.0019 (0.0297)	
training:	Epoch: [71][41/233]	Loss 0.0021 (0.0291)	
training:	Epoch: [71][42/233]	Loss 0.0031 (0.0284)	
training:	Epoch: [71][43/233]	Loss 0.0427 (0.0288)	
training:	Epoch: [71][44/233]	Loss 0.0016 (0.0282)	
training:	Epoch: [71][45/233]	Loss 0.0019 (0.0276)	
training:	Epoch: [71][46/233]	Loss 0.0070 (0.0271)	
training:	Epoch: [71][47/233]	Loss 0.0018 (0.0266)	
training:	Epoch: [71][48/233]	Loss 0.0016 (0.0261)	
training:	Epoch: [71][49/233]	Loss 0.0023 (0.0256)	
training:	Epoch: [71][50/233]	Loss 0.0020 (0.0251)	
training:	Epoch: [71][51/233]	Loss 0.0021 (0.0247)	
training:	Epoch: [71][52/233]	Loss 0.0042 (0.0243)	
training:	Epoch: [71][53/233]	Loss 0.1094 (0.0259)	
training:	Epoch: [71][54/233]	Loss 0.0016 (0.0254)	
training:	Epoch: [71][55/233]	Loss 0.0029 (0.0250)	
training:	Epoch: [71][56/233]	Loss 0.0018 (0.0246)	
training:	Epoch: [71][57/233]	Loss 0.0044 (0.0242)	
training:	Epoch: [71][58/233]	Loss 0.0019 (0.0239)	
training:	Epoch: [71][59/233]	Loss 0.0025 (0.0235)	
training:	Epoch: [71][60/233]	Loss 0.0032 (0.0232)	
training:	Epoch: [71][61/233]	Loss 0.0018 (0.0228)	
training:	Epoch: [71][62/233]	Loss 0.0032 (0.0225)	
training:	Epoch: [71][63/233]	Loss 0.0017 (0.0222)	
training:	Epoch: [71][64/233]	Loss 0.0019 (0.0218)	
training:	Epoch: [71][65/233]	Loss 0.0023 (0.0215)	
training:	Epoch: [71][66/233]	Loss 0.0016 (0.0212)	
training:	Epoch: [71][67/233]	Loss 0.0046 (0.0210)	
training:	Epoch: [71][68/233]	Loss 0.0039 (0.0207)	
training:	Epoch: [71][69/233]	Loss 0.0017 (0.0205)	
training:	Epoch: [71][70/233]	Loss 0.0019 (0.0202)	
training:	Epoch: [71][71/233]	Loss 0.0018 (0.0199)	
training:	Epoch: [71][72/233]	Loss 0.0056 (0.0197)	
training:	Epoch: [71][73/233]	Loss 0.0023 (0.0195)	
training:	Epoch: [71][74/233]	Loss 0.0022 (0.0193)	
training:	Epoch: [71][75/233]	Loss 0.0021 (0.0190)	
training:	Epoch: [71][76/233]	Loss 0.0026 (0.0188)	
training:	Epoch: [71][77/233]	Loss 0.0042 (0.0186)	
training:	Epoch: [71][78/233]	Loss 0.0017 (0.0184)	
training:	Epoch: [71][79/233]	Loss 0.0029 (0.0182)	
training:	Epoch: [71][80/233]	Loss 0.0017 (0.0180)	
training:	Epoch: [71][81/233]	Loss 0.0023 (0.0178)	
training:	Epoch: [71][82/233]	Loss 0.0020 (0.0176)	
training:	Epoch: [71][83/233]	Loss 0.0158 (0.0176)	
training:	Epoch: [71][84/233]	Loss 0.0015 (0.0174)	
training:	Epoch: [71][85/233]	Loss 0.0017 (0.0172)	
training:	Epoch: [71][86/233]	Loss 0.0025 (0.0171)	
training:	Epoch: [71][87/233]	Loss 0.0029 (0.0169)	
training:	Epoch: [71][88/233]	Loss 0.0023 (0.0167)	
training:	Epoch: [71][89/233]	Loss 0.0018 (0.0166)	
training:	Epoch: [71][90/233]	Loss 0.0061 (0.0164)	
training:	Epoch: [71][91/233]	Loss 0.0233 (0.0165)	
training:	Epoch: [71][92/233]	Loss 0.0065 (0.0164)	
training:	Epoch: [71][93/233]	Loss 0.0031 (0.0163)	
training:	Epoch: [71][94/233]	Loss 0.1383 (0.0176)	
training:	Epoch: [71][95/233]	Loss 0.0279 (0.0177)	
training:	Epoch: [71][96/233]	Loss 0.0223 (0.0177)	
training:	Epoch: [71][97/233]	Loss 0.0017 (0.0176)	
training:	Epoch: [71][98/233]	Loss 0.0019 (0.0174)	
training:	Epoch: [71][99/233]	Loss 0.0016 (0.0172)	
training:	Epoch: [71][100/233]	Loss 0.1146 (0.0182)	
training:	Epoch: [71][101/233]	Loss 0.0047 (0.0181)	
training:	Epoch: [71][102/233]	Loss 0.0017 (0.0179)	
training:	Epoch: [71][103/233]	Loss 0.0028 (0.0178)	
training:	Epoch: [71][104/233]	Loss 0.0058 (0.0177)	
training:	Epoch: [71][105/233]	Loss 0.0023 (0.0175)	
training:	Epoch: [71][106/233]	Loss 0.0563 (0.0179)	
training:	Epoch: [71][107/233]	Loss 0.0020 (0.0177)	
training:	Epoch: [71][108/233]	Loss 0.0017 (0.0176)	
training:	Epoch: [71][109/233]	Loss 0.1859 (0.0191)	
training:	Epoch: [71][110/233]	Loss 0.0029 (0.0190)	
training:	Epoch: [71][111/233]	Loss 0.0101 (0.0189)	
training:	Epoch: [71][112/233]	Loss 0.0021 (0.0187)	
training:	Epoch: [71][113/233]	Loss 0.0016 (0.0186)	
training:	Epoch: [71][114/233]	Loss 0.0020 (0.0184)	
training:	Epoch: [71][115/233]	Loss 0.0018 (0.0183)	
training:	Epoch: [71][116/233]	Loss 0.0020 (0.0182)	
training:	Epoch: [71][117/233]	Loss 0.0024 (0.0180)	
training:	Epoch: [71][118/233]	Loss 0.0023 (0.0179)	
training:	Epoch: [71][119/233]	Loss 0.1676 (0.0192)	
training:	Epoch: [71][120/233]	Loss 0.0017 (0.0190)	
training:	Epoch: [71][121/233]	Loss 0.0016 (0.0189)	
training:	Epoch: [71][122/233]	Loss 0.1034 (0.0196)	
training:	Epoch: [71][123/233]	Loss 0.0202 (0.0196)	
training:	Epoch: [71][124/233]	Loss 0.0038 (0.0194)	
training:	Epoch: [71][125/233]	Loss 0.0021 (0.0193)	
training:	Epoch: [71][126/233]	Loss 0.0127 (0.0192)	
training:	Epoch: [71][127/233]	Loss 0.0019 (0.0191)	
training:	Epoch: [71][128/233]	Loss 0.0032 (0.0190)	
training:	Epoch: [71][129/233]	Loss 0.0018 (0.0188)	
training:	Epoch: [71][130/233]	Loss 0.0022 (0.0187)	
training:	Epoch: [71][131/233]	Loss 0.0019 (0.0186)	
training:	Epoch: [71][132/233]	Loss 0.0021 (0.0185)	
training:	Epoch: [71][133/233]	Loss 0.0020 (0.0183)	
training:	Epoch: [71][134/233]	Loss 0.0023 (0.0182)	
training:	Epoch: [71][135/233]	Loss 0.0029 (0.0181)	
training:	Epoch: [71][136/233]	Loss 0.0026 (0.0180)	
training:	Epoch: [71][137/233]	Loss 0.2040 (0.0194)	
training:	Epoch: [71][138/233]	Loss 0.0029 (0.0192)	
training:	Epoch: [71][139/233]	Loss 0.0015 (0.0191)	
training:	Epoch: [71][140/233]	Loss 0.0045 (0.0190)	
training:	Epoch: [71][141/233]	Loss 0.0035 (0.0189)	
training:	Epoch: [71][142/233]	Loss 0.0020 (0.0188)	
training:	Epoch: [71][143/233]	Loss 0.0019 (0.0187)	
training:	Epoch: [71][144/233]	Loss 0.0017 (0.0185)	
training:	Epoch: [71][145/233]	Loss 0.0033 (0.0184)	
training:	Epoch: [71][146/233]	Loss 0.0861 (0.0189)	
training:	Epoch: [71][147/233]	Loss 0.0018 (0.0188)	
training:	Epoch: [71][148/233]	Loss 0.0026 (0.0187)	
training:	Epoch: [71][149/233]	Loss 0.1589 (0.0196)	
training:	Epoch: [71][150/233]	Loss 0.0988 (0.0201)	
training:	Epoch: [71][151/233]	Loss 0.0025 (0.0200)	
training:	Epoch: [71][152/233]	Loss 0.0030 (0.0199)	
training:	Epoch: [71][153/233]	Loss 0.0023 (0.0198)	
training:	Epoch: [71][154/233]	Loss 0.0021 (0.0197)	
training:	Epoch: [71][155/233]	Loss 0.0094 (0.0196)	
training:	Epoch: [71][156/233]	Loss 0.0991 (0.0201)	
training:	Epoch: [71][157/233]	Loss 0.0063 (0.0200)	
training:	Epoch: [71][158/233]	Loss 0.0222 (0.0200)	
training:	Epoch: [71][159/233]	Loss 0.0466 (0.0202)	
training:	Epoch: [71][160/233]	Loss 0.0030 (0.0201)	
training:	Epoch: [71][161/233]	Loss 0.0020 (0.0200)	
training:	Epoch: [71][162/233]	Loss 0.0020 (0.0199)	
training:	Epoch: [71][163/233]	Loss 0.0410 (0.0200)	
training:	Epoch: [71][164/233]	Loss 0.0020 (0.0199)	
training:	Epoch: [71][165/233]	Loss 0.0027 (0.0198)	
training:	Epoch: [71][166/233]	Loss 0.0045 (0.0197)	
training:	Epoch: [71][167/233]	Loss 0.1148 (0.0203)	
training:	Epoch: [71][168/233]	Loss 0.0017 (0.0202)	
training:	Epoch: [71][169/233]	Loss 0.0021 (0.0201)	
training:	Epoch: [71][170/233]	Loss 0.0222 (0.0201)	
training:	Epoch: [71][171/233]	Loss 0.0022 (0.0200)	
training:	Epoch: [71][172/233]	Loss 0.0034 (0.0199)	
training:	Epoch: [71][173/233]	Loss 0.2281 (0.0211)	
training:	Epoch: [71][174/233]	Loss 0.0025 (0.0210)	
training:	Epoch: [71][175/233]	Loss 0.0020 (0.0209)	
training:	Epoch: [71][176/233]	Loss 0.0036 (0.0208)	
training:	Epoch: [71][177/233]	Loss 0.0018 (0.0207)	
training:	Epoch: [71][178/233]	Loss 0.0028 (0.0206)	
training:	Epoch: [71][179/233]	Loss 0.0018 (0.0204)	
training:	Epoch: [71][180/233]	Loss 0.0053 (0.0204)	
training:	Epoch: [71][181/233]	Loss 0.0256 (0.0204)	
training:	Epoch: [71][182/233]	Loss 0.0023 (0.0203)	
training:	Epoch: [71][183/233]	Loss 0.0044 (0.0202)	
training:	Epoch: [71][184/233]	Loss 0.0034 (0.0201)	
training:	Epoch: [71][185/233]	Loss 0.0032 (0.0200)	
training:	Epoch: [71][186/233]	Loss 0.0020 (0.0199)	
training:	Epoch: [71][187/233]	Loss 0.0020 (0.0198)	
training:	Epoch: [71][188/233]	Loss 0.0020 (0.0197)	
training:	Epoch: [71][189/233]	Loss 0.2001 (0.0207)	
training:	Epoch: [71][190/233]	Loss 0.0779 (0.0210)	
training:	Epoch: [71][191/233]	Loss 0.0019 (0.0209)	
training:	Epoch: [71][192/233]	Loss 0.0026 (0.0208)	
training:	Epoch: [71][193/233]	Loss 0.0020 (0.0207)	
training:	Epoch: [71][194/233]	Loss 0.0021 (0.0206)	
training:	Epoch: [71][195/233]	Loss 0.1206 (0.0211)	
training:	Epoch: [71][196/233]	Loss 0.0030 (0.0210)	
training:	Epoch: [71][197/233]	Loss 0.0028 (0.0209)	
training:	Epoch: [71][198/233]	Loss 0.0020 (0.0208)	
training:	Epoch: [71][199/233]	Loss 0.0038 (0.0208)	
training:	Epoch: [71][200/233]	Loss 0.0023 (0.0207)	
training:	Epoch: [71][201/233]	Loss 0.0439 (0.0208)	
training:	Epoch: [71][202/233]	Loss 0.0022 (0.0207)	
training:	Epoch: [71][203/233]	Loss 0.0018 (0.0206)	
training:	Epoch: [71][204/233]	Loss 0.0018 (0.0205)	
training:	Epoch: [71][205/233]	Loss 0.0031 (0.0204)	
training:	Epoch: [71][206/233]	Loss 0.0135 (0.0204)	
training:	Epoch: [71][207/233]	Loss 0.0028 (0.0203)	
training:	Epoch: [71][208/233]	Loss 0.0144 (0.0203)	
training:	Epoch: [71][209/233]	Loss 0.0054 (0.0202)	
training:	Epoch: [71][210/233]	Loss 0.0034 (0.0201)	
training:	Epoch: [71][211/233]	Loss 0.1414 (0.0207)	
training:	Epoch: [71][212/233]	Loss 0.0030 (0.0206)	
training:	Epoch: [71][213/233]	Loss 0.0029 (0.0205)	
training:	Epoch: [71][214/233]	Loss 0.0808 (0.0208)	
training:	Epoch: [71][215/233]	Loss 0.0018 (0.0207)	
training:	Epoch: [71][216/233]	Loss 0.0022 (0.0206)	
training:	Epoch: [71][217/233]	Loss 0.0030 (0.0205)	
training:	Epoch: [71][218/233]	Loss 0.0042 (0.0205)	
training:	Epoch: [71][219/233]	Loss 0.0019 (0.0204)	
training:	Epoch: [71][220/233]	Loss 0.0052 (0.0203)	
training:	Epoch: [71][221/233]	Loss 0.0023 (0.0202)	
training:	Epoch: [71][222/233]	Loss 0.0019 (0.0202)	
training:	Epoch: [71][223/233]	Loss 0.0070 (0.0201)	
training:	Epoch: [71][224/233]	Loss 0.0027 (0.0200)	
training:	Epoch: [71][225/233]	Loss 0.0048 (0.0200)	
training:	Epoch: [71][226/233]	Loss 0.0027 (0.0199)	
training:	Epoch: [71][227/233]	Loss 0.0051 (0.0198)	
training:	Epoch: [71][228/233]	Loss 0.0031 (0.0197)	
training:	Epoch: [71][229/233]	Loss 0.0036 (0.0197)	
training:	Epoch: [71][230/233]	Loss 0.0054 (0.0196)	
training:	Epoch: [71][231/233]	Loss 0.0041 (0.0195)	
training:	Epoch: [71][232/233]	Loss 0.0042 (0.0195)	
training:	Epoch: [71][233/233]	Loss 0.0019 (0.0194)	
Training:	 Loss: 0.0194

Training:	 ACC: 0.9992 0.9992 0.9992 0.9992
Validation:	 ACC: 0.7852 0.7838 0.7559 0.8146
Validation:	 Best_BACC: 0.8162 0.8154 0.7998 0.8326
Validation:	 Loss: 1.0492
Pretraining:	Epoch 72/200
----------
training:	Epoch: [72][1/233]	Loss 0.0019 (0.0019)	
training:	Epoch: [72][2/233]	Loss 0.0020 (0.0020)	
training:	Epoch: [72][3/233]	Loss 0.0064 (0.0034)	
training:	Epoch: [72][4/233]	Loss 0.0022 (0.0031)	
training:	Epoch: [72][5/233]	Loss 0.0021 (0.0029)	
training:	Epoch: [72][6/233]	Loss 0.0017 (0.0027)	
training:	Epoch: [72][7/233]	Loss 0.0027 (0.0027)	
training:	Epoch: [72][8/233]	Loss 0.0091 (0.0035)	
training:	Epoch: [72][9/233]	Loss 0.0022 (0.0034)	
training:	Epoch: [72][10/233]	Loss 0.0636 (0.0094)	
training:	Epoch: [72][11/233]	Loss 0.0016 (0.0087)	
training:	Epoch: [72][12/233]	Loss 0.0019 (0.0081)	
training:	Epoch: [72][13/233]	Loss 0.0056 (0.0079)	
training:	Epoch: [72][14/233]	Loss 0.0318 (0.0096)	
training:	Epoch: [72][15/233]	Loss 0.0115 (0.0098)	
training:	Epoch: [72][16/233]	Loss 0.0232 (0.0106)	
training:	Epoch: [72][17/233]	Loss 0.0062 (0.0103)	
training:	Epoch: [72][18/233]	Loss 0.0020 (0.0099)	
training:	Epoch: [72][19/233]	Loss 0.0124 (0.0100)	
training:	Epoch: [72][20/233]	Loss 0.0026 (0.0096)	
training:	Epoch: [72][21/233]	Loss 0.0381 (0.0110)	
training:	Epoch: [72][22/233]	Loss 0.0023 (0.0106)	
training:	Epoch: [72][23/233]	Loss 0.0018 (0.0102)	
training:	Epoch: [72][24/233]	Loss 0.0022 (0.0099)	
training:	Epoch: [72][25/233]	Loss 0.0103 (0.0099)	
training:	Epoch: [72][26/233]	Loss 0.0096 (0.0099)	
training:	Epoch: [72][27/233]	Loss 0.0023 (0.0096)	
training:	Epoch: [72][28/233]	Loss 0.0022 (0.0093)	
training:	Epoch: [72][29/233]	Loss 0.0034 (0.0091)	
training:	Epoch: [72][30/233]	Loss 0.0033 (0.0089)	
training:	Epoch: [72][31/233]	Loss 0.0095 (0.0090)	
training:	Epoch: [72][32/233]	Loss 0.0017 (0.0087)	
training:	Epoch: [72][33/233]	Loss 0.0020 (0.0085)	
training:	Epoch: [72][34/233]	Loss 0.0017 (0.0083)	
training:	Epoch: [72][35/233]	Loss 0.0952 (0.0108)	
training:	Epoch: [72][36/233]	Loss 0.0019 (0.0106)	
training:	Epoch: [72][37/233]	Loss 0.0017 (0.0103)	
training:	Epoch: [72][38/233]	Loss 0.0036 (0.0101)	
training:	Epoch: [72][39/233]	Loss 0.0019 (0.0099)	
training:	Epoch: [72][40/233]	Loss 0.0018 (0.0097)	
training:	Epoch: [72][41/233]	Loss 0.0022 (0.0095)	
training:	Epoch: [72][42/233]	Loss 0.0016 (0.0094)	
training:	Epoch: [72][43/233]	Loss 0.0017 (0.0092)	
training:	Epoch: [72][44/233]	Loss 0.0018 (0.0090)	
training:	Epoch: [72][45/233]	Loss 0.0029 (0.0089)	
training:	Epoch: [72][46/233]	Loss 0.0106 (0.0089)	
training:	Epoch: [72][47/233]	Loss 0.0023 (0.0088)	
training:	Epoch: [72][48/233]	Loss 0.0120 (0.0088)	
training:	Epoch: [72][49/233]	Loss 0.0059 (0.0088)	
training:	Epoch: [72][50/233]	Loss 0.0029 (0.0087)	
training:	Epoch: [72][51/233]	Loss 0.0020 (0.0085)	
training:	Epoch: [72][52/233]	Loss 0.0020 (0.0084)	
training:	Epoch: [72][53/233]	Loss 0.0022 (0.0083)	
training:	Epoch: [72][54/233]	Loss 0.0024 (0.0082)	
training:	Epoch: [72][55/233]	Loss 0.0500 (0.0089)	
training:	Epoch: [72][56/233]	Loss 0.0019 (0.0088)	
training:	Epoch: [72][57/233]	Loss 0.2113 (0.0124)	
training:	Epoch: [72][58/233]	Loss 0.0038 (0.0122)	
training:	Epoch: [72][59/233]	Loss 0.0019 (0.0120)	
training:	Epoch: [72][60/233]	Loss 0.0024 (0.0119)	
training:	Epoch: [72][61/233]	Loss 0.0026 (0.0117)	
training:	Epoch: [72][62/233]	Loss 0.0022 (0.0116)	
training:	Epoch: [72][63/233]	Loss 0.0020 (0.0114)	
training:	Epoch: [72][64/233]	Loss 0.0056 (0.0113)	
training:	Epoch: [72][65/233]	Loss 0.0296 (0.0116)	
training:	Epoch: [72][66/233]	Loss 0.0022 (0.0115)	
training:	Epoch: [72][67/233]	Loss 0.0015 (0.0113)	
training:	Epoch: [72][68/233]	Loss 0.0042 (0.0112)	
training:	Epoch: [72][69/233]	Loss 0.0039 (0.0111)	
training:	Epoch: [72][70/233]	Loss 0.0025 (0.0110)	
training:	Epoch: [72][71/233]	Loss 0.0052 (0.0109)	
training:	Epoch: [72][72/233]	Loss 0.0221 (0.0111)	
training:	Epoch: [72][73/233]	Loss 0.0051 (0.0110)	
training:	Epoch: [72][74/233]	Loss 0.0019 (0.0109)	
training:	Epoch: [72][75/233]	Loss 0.0028 (0.0107)	
training:	Epoch: [72][76/233]	Loss 0.2283 (0.0136)	
training:	Epoch: [72][77/233]	Loss 0.0021 (0.0135)	
training:	Epoch: [72][78/233]	Loss 0.0153 (0.0135)	
training:	Epoch: [72][79/233]	Loss 0.0031 (0.0134)	
training:	Epoch: [72][80/233]	Loss 0.0023 (0.0132)	
training:	Epoch: [72][81/233]	Loss 0.0048 (0.0131)	
training:	Epoch: [72][82/233]	Loss 0.0043 (0.0130)	
training:	Epoch: [72][83/233]	Loss 0.0018 (0.0129)	
training:	Epoch: [72][84/233]	Loss 0.1983 (0.0151)	
training:	Epoch: [72][85/233]	Loss 0.0040 (0.0149)	
training:	Epoch: [72][86/233]	Loss 0.0048 (0.0148)	
training:	Epoch: [72][87/233]	Loss 0.0018 (0.0147)	
training:	Epoch: [72][88/233]	Loss 0.0018 (0.0145)	
training:	Epoch: [72][89/233]	Loss 0.0185 (0.0146)	
training:	Epoch: [72][90/233]	Loss 0.0027 (0.0144)	
training:	Epoch: [72][91/233]	Loss 0.0070 (0.0144)	
training:	Epoch: [72][92/233]	Loss 0.0029 (0.0142)	
training:	Epoch: [72][93/233]	Loss 0.2045 (0.0163)	
training:	Epoch: [72][94/233]	Loss 0.0025 (0.0161)	
training:	Epoch: [72][95/233]	Loss 0.1580 (0.0176)	
training:	Epoch: [72][96/233]	Loss 0.0021 (0.0175)	
training:	Epoch: [72][97/233]	Loss 0.0020 (0.0173)	
training:	Epoch: [72][98/233]	Loss 0.0026 (0.0172)	
training:	Epoch: [72][99/233]	Loss 0.0025 (0.0170)	
training:	Epoch: [72][100/233]	Loss 0.0019 (0.0169)	
training:	Epoch: [72][101/233]	Loss 0.0021 (0.0167)	
training:	Epoch: [72][102/233]	Loss 0.0017 (0.0166)	
training:	Epoch: [72][103/233]	Loss 0.0020 (0.0164)	
training:	Epoch: [72][104/233]	Loss 0.0019 (0.0163)	
training:	Epoch: [72][105/233]	Loss 0.0063 (0.0162)	
training:	Epoch: [72][106/233]	Loss 0.0040 (0.0161)	
training:	Epoch: [72][107/233]	Loss 0.0020 (0.0159)	
training:	Epoch: [72][108/233]	Loss 0.0018 (0.0158)	
training:	Epoch: [72][109/233]	Loss 0.0016 (0.0157)	
training:	Epoch: [72][110/233]	Loss 0.0017 (0.0156)	
training:	Epoch: [72][111/233]	Loss 0.0022 (0.0154)	
training:	Epoch: [72][112/233]	Loss 0.0017 (0.0153)	
training:	Epoch: [72][113/233]	Loss 0.0021 (0.0152)	
training:	Epoch: [72][114/233]	Loss 0.0021 (0.0151)	
training:	Epoch: [72][115/233]	Loss 0.0024 (0.0150)	
training:	Epoch: [72][116/233]	Loss 0.0026 (0.0149)	
training:	Epoch: [72][117/233]	Loss 0.0075 (0.0148)	
training:	Epoch: [72][118/233]	Loss 0.0024 (0.0147)	
training:	Epoch: [72][119/233]	Loss 0.0020 (0.0146)	
training:	Epoch: [72][120/233]	Loss 0.0019 (0.0145)	
training:	Epoch: [72][121/233]	Loss 0.0325 (0.0146)	
training:	Epoch: [72][122/233]	Loss 0.0021 (0.0145)	
training:	Epoch: [72][123/233]	Loss 0.0024 (0.0144)	
training:	Epoch: [72][124/233]	Loss 0.0153 (0.0144)	
training:	Epoch: [72][125/233]	Loss 0.0022 (0.0143)	
training:	Epoch: [72][126/233]	Loss 0.0016 (0.0142)	
training:	Epoch: [72][127/233]	Loss 0.0024 (0.0141)	
training:	Epoch: [72][128/233]	Loss 0.0034 (0.0141)	
training:	Epoch: [72][129/233]	Loss 0.0017 (0.0140)	
training:	Epoch: [72][130/233]	Loss 0.0022 (0.0139)	
training:	Epoch: [72][131/233]	Loss 0.0016 (0.0138)	
training:	Epoch: [72][132/233]	Loss 0.1841 (0.0151)	
training:	Epoch: [72][133/233]	Loss 0.0018 (0.0150)	
training:	Epoch: [72][134/233]	Loss 0.0027 (0.0149)	
training:	Epoch: [72][135/233]	Loss 0.0017 (0.0148)	
training:	Epoch: [72][136/233]	Loss 0.0901 (0.0153)	
training:	Epoch: [72][137/233]	Loss 0.1818 (0.0166)	
training:	Epoch: [72][138/233]	Loss 0.0016 (0.0164)	
training:	Epoch: [72][139/233]	Loss 0.0017 (0.0163)	
training:	Epoch: [72][140/233]	Loss 0.0017 (0.0162)	
training:	Epoch: [72][141/233]	Loss 0.0030 (0.0161)	
training:	Epoch: [72][142/233]	Loss 0.0019 (0.0160)	
training:	Epoch: [72][143/233]	Loss 0.0018 (0.0159)	
training:	Epoch: [72][144/233]	Loss 0.0033 (0.0159)	
training:	Epoch: [72][145/233]	Loss 0.0072 (0.0158)	
training:	Epoch: [72][146/233]	Loss 0.0088 (0.0157)	
training:	Epoch: [72][147/233]	Loss 0.0078 (0.0157)	
training:	Epoch: [72][148/233]	Loss 0.0028 (0.0156)	
training:	Epoch: [72][149/233]	Loss 0.0022 (0.0155)	
training:	Epoch: [72][150/233]	Loss 0.0035 (0.0154)	
training:	Epoch: [72][151/233]	Loss 0.0220 (0.0155)	
training:	Epoch: [72][152/233]	Loss 0.0044 (0.0154)	
training:	Epoch: [72][153/233]	Loss 0.0019 (0.0153)	
training:	Epoch: [72][154/233]	Loss 0.0021 (0.0152)	
training:	Epoch: [72][155/233]	Loss 0.0033 (0.0152)	
training:	Epoch: [72][156/233]	Loss 0.0071 (0.0151)	
training:	Epoch: [72][157/233]	Loss 0.0017 (0.0150)	
training:	Epoch: [72][158/233]	Loss 0.0022 (0.0149)	
training:	Epoch: [72][159/233]	Loss 0.0022 (0.0149)	
training:	Epoch: [72][160/233]	Loss 0.0024 (0.0148)	
training:	Epoch: [72][161/233]	Loss 0.0022 (0.0147)	
training:	Epoch: [72][162/233]	Loss 0.0033 (0.0146)	
training:	Epoch: [72][163/233]	Loss 0.0022 (0.0146)	
training:	Epoch: [72][164/233]	Loss 0.0024 (0.0145)	
training:	Epoch: [72][165/233]	Loss 0.0030 (0.0144)	
training:	Epoch: [72][166/233]	Loss 0.0483 (0.0146)	
training:	Epoch: [72][167/233]	Loss 0.0020 (0.0145)	
training:	Epoch: [72][168/233]	Loss 0.0022 (0.0145)	
training:	Epoch: [72][169/233]	Loss 0.0016 (0.0144)	
training:	Epoch: [72][170/233]	Loss 0.0029 (0.0143)	
training:	Epoch: [72][171/233]	Loss 0.0033 (0.0143)	
training:	Epoch: [72][172/233]	Loss 0.1320 (0.0149)	
training:	Epoch: [72][173/233]	Loss 0.0039 (0.0149)	
training:	Epoch: [72][174/233]	Loss 0.0028 (0.0148)	
training:	Epoch: [72][175/233]	Loss 0.0054 (0.0148)	
training:	Epoch: [72][176/233]	Loss 0.0034 (0.0147)	
training:	Epoch: [72][177/233]	Loss 0.0038 (0.0146)	
training:	Epoch: [72][178/233]	Loss 0.0047 (0.0146)	
training:	Epoch: [72][179/233]	Loss 0.0016 (0.0145)	
training:	Epoch: [72][180/233]	Loss 0.0020 (0.0144)	
training:	Epoch: [72][181/233]	Loss 0.0018 (0.0144)	
training:	Epoch: [72][182/233]	Loss 0.0057 (0.0143)	
training:	Epoch: [72][183/233]	Loss 0.0021 (0.0142)	
training:	Epoch: [72][184/233]	Loss 0.0485 (0.0144)	
training:	Epoch: [72][185/233]	Loss 0.0132 (0.0144)	
training:	Epoch: [72][186/233]	Loss 0.0025 (0.0144)	
training:	Epoch: [72][187/233]	Loss 0.0017 (0.0143)	
training:	Epoch: [72][188/233]	Loss 0.0424 (0.0144)	
training:	Epoch: [72][189/233]	Loss 0.0030 (0.0144)	
training:	Epoch: [72][190/233]	Loss 0.0030 (0.0143)	
training:	Epoch: [72][191/233]	Loss 0.0099 (0.0143)	
training:	Epoch: [72][192/233]	Loss 0.0020 (0.0142)	
training:	Epoch: [72][193/233]	Loss 0.0313 (0.0143)	
training:	Epoch: [72][194/233]	Loss 0.0023 (0.0143)	
training:	Epoch: [72][195/233]	Loss 0.0018 (0.0142)	
training:	Epoch: [72][196/233]	Loss 0.0022 (0.0141)	
training:	Epoch: [72][197/233]	Loss 0.0770 (0.0145)	
training:	Epoch: [72][198/233]	Loss 0.0018 (0.0144)	
training:	Epoch: [72][199/233]	Loss 0.0060 (0.0143)	
training:	Epoch: [72][200/233]	Loss 0.0019 (0.0143)	
training:	Epoch: [72][201/233]	Loss 0.0017 (0.0142)	
training:	Epoch: [72][202/233]	Loss 0.2061 (0.0152)	
training:	Epoch: [72][203/233]	Loss 0.0023 (0.0151)	
training:	Epoch: [72][204/233]	Loss 0.0041 (0.0151)	
training:	Epoch: [72][205/233]	Loss 0.0226 (0.0151)	
training:	Epoch: [72][206/233]	Loss 0.0023 (0.0150)	
training:	Epoch: [72][207/233]	Loss 0.0152 (0.0150)	
training:	Epoch: [72][208/233]	Loss 0.0093 (0.0150)	
training:	Epoch: [72][209/233]	Loss 0.0756 (0.0153)	
training:	Epoch: [72][210/233]	Loss 0.0035 (0.0152)	
training:	Epoch: [72][211/233]	Loss 0.0023 (0.0152)	
training:	Epoch: [72][212/233]	Loss 0.0211 (0.0152)	
training:	Epoch: [72][213/233]	Loss 0.0027 (0.0151)	
training:	Epoch: [72][214/233]	Loss 0.0017 (0.0151)	
training:	Epoch: [72][215/233]	Loss 0.0032 (0.0150)	
training:	Epoch: [72][216/233]	Loss 0.0019 (0.0150)	
training:	Epoch: [72][217/233]	Loss 0.0099 (0.0149)	
training:	Epoch: [72][218/233]	Loss 0.0022 (0.0149)	
training:	Epoch: [72][219/233]	Loss 0.0018 (0.0148)	
training:	Epoch: [72][220/233]	Loss 0.0017 (0.0148)	
training:	Epoch: [72][221/233]	Loss 0.0022 (0.0147)	
training:	Epoch: [72][222/233]	Loss 0.0040 (0.0147)	
training:	Epoch: [72][223/233]	Loss 0.0060 (0.0146)	
training:	Epoch: [72][224/233]	Loss 0.0027 (0.0146)	
training:	Epoch: [72][225/233]	Loss 0.0018 (0.0145)	
training:	Epoch: [72][226/233]	Loss 0.0020 (0.0145)	
training:	Epoch: [72][227/233]	Loss 0.0092 (0.0144)	
training:	Epoch: [72][228/233]	Loss 0.0017 (0.0144)	
training:	Epoch: [72][229/233]	Loss 0.0260 (0.0144)	
training:	Epoch: [72][230/233]	Loss 0.0025 (0.0144)	
training:	Epoch: [72][231/233]	Loss 0.0060 (0.0143)	
training:	Epoch: [72][232/233]	Loss 0.0019 (0.0143)	
training:	Epoch: [72][233/233]	Loss 0.0071 (0.0143)	
Training:	 Loss: 0.0142

Training:	 ACC: 0.9992 0.9992 0.9992 0.9992
Validation:	 ACC: 0.7897 0.7887 0.7681 0.8112
Validation:	 Best_BACC: 0.8162 0.8154 0.7998 0.8326
Validation:	 Loss: 1.0407
Pretraining:	Epoch 73/200
----------
training:	Epoch: [73][1/233]	Loss 0.0025 (0.0025)	
training:	Epoch: [73][2/233]	Loss 0.0015 (0.0020)	
training:	Epoch: [73][3/233]	Loss 0.0019 (0.0019)	
training:	Epoch: [73][4/233]	Loss 0.0016 (0.0019)	
training:	Epoch: [73][5/233]	Loss 0.0025 (0.0020)	
training:	Epoch: [73][6/233]	Loss 0.0035 (0.0022)	
training:	Epoch: [73][7/233]	Loss 0.0019 (0.0022)	
training:	Epoch: [73][8/233]	Loss 0.0021 (0.0022)	
training:	Epoch: [73][9/233]	Loss 0.0019 (0.0021)	
training:	Epoch: [73][10/233]	Loss 0.0018 (0.0021)	
training:	Epoch: [73][11/233]	Loss 0.0033 (0.0022)	
training:	Epoch: [73][12/233]	Loss 0.0016 (0.0022)	
training:	Epoch: [73][13/233]	Loss 0.0133 (0.0030)	
training:	Epoch: [73][14/233]	Loss 0.0018 (0.0029)	
training:	Epoch: [73][15/233]	Loss 0.0019 (0.0029)	
training:	Epoch: [73][16/233]	Loss 0.0018 (0.0028)	
training:	Epoch: [73][17/233]	Loss 0.0020 (0.0028)	
training:	Epoch: [73][18/233]	Loss 0.0016 (0.0027)	
training:	Epoch: [73][19/233]	Loss 0.0016 (0.0026)	
training:	Epoch: [73][20/233]	Loss 0.0255 (0.0038)	
training:	Epoch: [73][21/233]	Loss 0.0025 (0.0037)	
training:	Epoch: [73][22/233]	Loss 0.0529 (0.0060)	
training:	Epoch: [73][23/233]	Loss 0.0024 (0.0058)	
training:	Epoch: [73][24/233]	Loss 0.0018 (0.0056)	
training:	Epoch: [73][25/233]	Loss 0.0020 (0.0055)	
training:	Epoch: [73][26/233]	Loss 0.0058 (0.0055)	
training:	Epoch: [73][27/233]	Loss 0.0015 (0.0054)	
training:	Epoch: [73][28/233]	Loss 0.0016 (0.0052)	
training:	Epoch: [73][29/233]	Loss 0.0021 (0.0051)	
training:	Epoch: [73][30/233]	Loss 0.0022 (0.0050)	
training:	Epoch: [73][31/233]	Loss 0.0027 (0.0049)	
training:	Epoch: [73][32/233]	Loss 0.0109 (0.0051)	
training:	Epoch: [73][33/233]	Loss 0.0016 (0.0050)	
training:	Epoch: [73][34/233]	Loss 0.0030 (0.0050)	
training:	Epoch: [73][35/233]	Loss 0.0017 (0.0049)	
training:	Epoch: [73][36/233]	Loss 0.0019 (0.0048)	
training:	Epoch: [73][37/233]	Loss 0.0059 (0.0048)	
training:	Epoch: [73][38/233]	Loss 0.0029 (0.0048)	
training:	Epoch: [73][39/233]	Loss 0.0020 (0.0047)	
training:	Epoch: [73][40/233]	Loss 0.0072 (0.0048)	
training:	Epoch: [73][41/233]	Loss 0.0022 (0.0047)	
training:	Epoch: [73][42/233]	Loss 0.0015 (0.0046)	
training:	Epoch: [73][43/233]	Loss 0.0078 (0.0047)	
training:	Epoch: [73][44/233]	Loss 0.0033 (0.0047)	
training:	Epoch: [73][45/233]	Loss 0.0017 (0.0046)	
training:	Epoch: [73][46/233]	Loss 0.0016 (0.0045)	
training:	Epoch: [73][47/233]	Loss 0.0017 (0.0045)	
training:	Epoch: [73][48/233]	Loss 0.2121 (0.0088)	
training:	Epoch: [73][49/233]	Loss 0.0017 (0.0086)	
training:	Epoch: [73][50/233]	Loss 0.0020 (0.0085)	
training:	Epoch: [73][51/233]	Loss 0.0018 (0.0084)	
training:	Epoch: [73][52/233]	Loss 0.0016 (0.0083)	
training:	Epoch: [73][53/233]	Loss 0.0017 (0.0081)	
training:	Epoch: [73][54/233]	Loss 0.0018 (0.0080)	
training:	Epoch: [73][55/233]	Loss 0.0022 (0.0079)	
training:	Epoch: [73][56/233]	Loss 0.0021 (0.0078)	
training:	Epoch: [73][57/233]	Loss 0.0019 (0.0077)	
training:	Epoch: [73][58/233]	Loss 0.0045 (0.0076)	
training:	Epoch: [73][59/233]	Loss 0.1110 (0.0094)	
training:	Epoch: [73][60/233]	Loss 0.0015 (0.0093)	
training:	Epoch: [73][61/233]	Loss 0.0076 (0.0092)	
training:	Epoch: [73][62/233]	Loss 0.0016 (0.0091)	
training:	Epoch: [73][63/233]	Loss 0.0015 (0.0090)	
training:	Epoch: [73][64/233]	Loss 0.0017 (0.0089)	
training:	Epoch: [73][65/233]	Loss 0.1237 (0.0106)	
training:	Epoch: [73][66/233]	Loss 0.0027 (0.0105)	
training:	Epoch: [73][67/233]	Loss 0.0016 (0.0104)	
training:	Epoch: [73][68/233]	Loss 0.0036 (0.0103)	
training:	Epoch: [73][69/233]	Loss 0.2034 (0.0131)	
training:	Epoch: [73][70/233]	Loss 0.0022 (0.0129)	
training:	Epoch: [73][71/233]	Loss 0.0023 (0.0128)	
training:	Epoch: [73][72/233]	Loss 0.0016 (0.0126)	
training:	Epoch: [73][73/233]	Loss 0.0452 (0.0131)	
training:	Epoch: [73][74/233]	Loss 0.0037 (0.0129)	
training:	Epoch: [73][75/233]	Loss 0.0052 (0.0128)	
training:	Epoch: [73][76/233]	Loss 0.0093 (0.0128)	
training:	Epoch: [73][77/233]	Loss 0.0785 (0.0136)	
training:	Epoch: [73][78/233]	Loss 0.0014 (0.0135)	
training:	Epoch: [73][79/233]	Loss 0.0796 (0.0143)	
training:	Epoch: [73][80/233]	Loss 0.0018 (0.0142)	
training:	Epoch: [73][81/233]	Loss 0.0024 (0.0140)	
training:	Epoch: [73][82/233]	Loss 0.0041 (0.0139)	
training:	Epoch: [73][83/233]	Loss 0.1724 (0.0158)	
training:	Epoch: [73][84/233]	Loss 0.0020 (0.0157)	
training:	Epoch: [73][85/233]	Loss 0.0020 (0.0155)	
training:	Epoch: [73][86/233]	Loss 0.0108 (0.0154)	
training:	Epoch: [73][87/233]	Loss 0.0037 (0.0153)	
training:	Epoch: [73][88/233]	Loss 0.0018 (0.0151)	
training:	Epoch: [73][89/233]	Loss 0.0058 (0.0150)	
training:	Epoch: [73][90/233]	Loss 0.0021 (0.0149)	
training:	Epoch: [73][91/233]	Loss 0.0018 (0.0148)	
training:	Epoch: [73][92/233]	Loss 0.0045 (0.0146)	
training:	Epoch: [73][93/233]	Loss 0.0188 (0.0147)	
training:	Epoch: [73][94/233]	Loss 0.0081 (0.0146)	
training:	Epoch: [73][95/233]	Loss 0.0017 (0.0145)	
training:	Epoch: [73][96/233]	Loss 0.0050 (0.0144)	
training:	Epoch: [73][97/233]	Loss 0.0028 (0.0143)	
training:	Epoch: [73][98/233]	Loss 0.0019 (0.0141)	
training:	Epoch: [73][99/233]	Loss 0.0098 (0.0141)	
training:	Epoch: [73][100/233]	Loss 0.0017 (0.0140)	
training:	Epoch: [73][101/233]	Loss 0.0016 (0.0138)	
training:	Epoch: [73][102/233]	Loss 0.0179 (0.0139)	
training:	Epoch: [73][103/233]	Loss 0.0017 (0.0138)	
training:	Epoch: [73][104/233]	Loss 0.0197 (0.0138)	
training:	Epoch: [73][105/233]	Loss 0.0017 (0.0137)	
training:	Epoch: [73][106/233]	Loss 0.0018 (0.0136)	
training:	Epoch: [73][107/233]	Loss 0.0043 (0.0135)	
training:	Epoch: [73][108/233]	Loss 0.0137 (0.0135)	
training:	Epoch: [73][109/233]	Loss 0.0028 (0.0134)	
training:	Epoch: [73][110/233]	Loss 0.0019 (0.0133)	
training:	Epoch: [73][111/233]	Loss 0.0017 (0.0132)	
training:	Epoch: [73][112/233]	Loss 0.0016 (0.0131)	
training:	Epoch: [73][113/233]	Loss 0.0039 (0.0130)	
training:	Epoch: [73][114/233]	Loss 0.0015 (0.0129)	
training:	Epoch: [73][115/233]	Loss 0.0016 (0.0128)	
training:	Epoch: [73][116/233]	Loss 0.0015 (0.0127)	
training:	Epoch: [73][117/233]	Loss 0.0024 (0.0126)	
training:	Epoch: [73][118/233]	Loss 0.0765 (0.0132)	
training:	Epoch: [73][119/233]	Loss 0.0036 (0.0131)	
training:	Epoch: [73][120/233]	Loss 0.0016 (0.0130)	
training:	Epoch: [73][121/233]	Loss 0.0027 (0.0129)	
training:	Epoch: [73][122/233]	Loss 0.0246 (0.0130)	
training:	Epoch: [73][123/233]	Loss 0.0031 (0.0129)	
training:	Epoch: [73][124/233]	Loss 0.0018 (0.0128)	
training:	Epoch: [73][125/233]	Loss 0.0018 (0.0128)	
training:	Epoch: [73][126/233]	Loss 0.0029 (0.0127)	
training:	Epoch: [73][127/233]	Loss 0.0061 (0.0126)	
training:	Epoch: [73][128/233]	Loss 0.0027 (0.0125)	
training:	Epoch: [73][129/233]	Loss 0.0024 (0.0125)	
training:	Epoch: [73][130/233]	Loss 0.0030 (0.0124)	
training:	Epoch: [73][131/233]	Loss 0.0610 (0.0128)	
training:	Epoch: [73][132/233]	Loss 0.0022 (0.0127)	
training:	Epoch: [73][133/233]	Loss 0.0019 (0.0126)	
training:	Epoch: [73][134/233]	Loss 0.0180 (0.0126)	
training:	Epoch: [73][135/233]	Loss 0.3697 (0.0153)	
training:	Epoch: [73][136/233]	Loss 0.0034 (0.0152)	
training:	Epoch: [73][137/233]	Loss 0.0020 (0.0151)	
training:	Epoch: [73][138/233]	Loss 0.0047 (0.0150)	
training:	Epoch: [73][139/233]	Loss 0.0113 (0.0150)	
training:	Epoch: [73][140/233]	Loss 0.0021 (0.0149)	
training:	Epoch: [73][141/233]	Loss 0.0022 (0.0148)	
training:	Epoch: [73][142/233]	Loss 0.0143 (0.0148)	
training:	Epoch: [73][143/233]	Loss 0.0237 (0.0149)	
training:	Epoch: [73][144/233]	Loss 0.0022 (0.0148)	
training:	Epoch: [73][145/233]	Loss 0.0040 (0.0147)	
training:	Epoch: [73][146/233]	Loss 0.0030 (0.0146)	
training:	Epoch: [73][147/233]	Loss 0.2054 (0.0159)	
training:	Epoch: [73][148/233]	Loss 0.0027 (0.0158)	
training:	Epoch: [73][149/233]	Loss 0.0018 (0.0157)	
training:	Epoch: [73][150/233]	Loss 0.0015 (0.0157)	
training:	Epoch: [73][151/233]	Loss 0.0023 (0.0156)	
training:	Epoch: [73][152/233]	Loss 0.0039 (0.0155)	
training:	Epoch: [73][153/233]	Loss 0.0018 (0.0154)	
training:	Epoch: [73][154/233]	Loss 0.0022 (0.0153)	
training:	Epoch: [73][155/233]	Loss 0.0017 (0.0152)	
training:	Epoch: [73][156/233]	Loss 0.0023 (0.0151)	
training:	Epoch: [73][157/233]	Loss 0.0021 (0.0151)	
training:	Epoch: [73][158/233]	Loss 0.0060 (0.0150)	
training:	Epoch: [73][159/233]	Loss 0.0014 (0.0149)	
training:	Epoch: [73][160/233]	Loss 0.0030 (0.0148)	
training:	Epoch: [73][161/233]	Loss 0.0408 (0.0150)	
training:	Epoch: [73][162/233]	Loss 0.0026 (0.0149)	
training:	Epoch: [73][163/233]	Loss 0.0477 (0.0151)	
training:	Epoch: [73][164/233]	Loss 0.0019 (0.0150)	
training:	Epoch: [73][165/233]	Loss 0.0022 (0.0150)	
training:	Epoch: [73][166/233]	Loss 0.0027 (0.0149)	
training:	Epoch: [73][167/233]	Loss 0.0023 (0.0148)	
training:	Epoch: [73][168/233]	Loss 0.0020 (0.0147)	
training:	Epoch: [73][169/233]	Loss 0.0021 (0.0147)	
training:	Epoch: [73][170/233]	Loss 0.0270 (0.0147)	
training:	Epoch: [73][171/233]	Loss 0.0269 (0.0148)	
training:	Epoch: [73][172/233]	Loss 0.0602 (0.0151)	
training:	Epoch: [73][173/233]	Loss 0.0016 (0.0150)	
training:	Epoch: [73][174/233]	Loss 0.0192 (0.0150)	
training:	Epoch: [73][175/233]	Loss 0.0039 (0.0150)	
training:	Epoch: [73][176/233]	Loss 0.0015 (0.0149)	
training:	Epoch: [73][177/233]	Loss 0.0020 (0.0148)	
training:	Epoch: [73][178/233]	Loss 0.0684 (0.0151)	
training:	Epoch: [73][179/233]	Loss 0.0016 (0.0150)	
training:	Epoch: [73][180/233]	Loss 0.0064 (0.0150)	
training:	Epoch: [73][181/233]	Loss 0.0017 (0.0149)	
training:	Epoch: [73][182/233]	Loss 0.1988 (0.0159)	
training:	Epoch: [73][183/233]	Loss 0.0020 (0.0158)	
training:	Epoch: [73][184/233]	Loss 0.0021 (0.0158)	
training:	Epoch: [73][185/233]	Loss 0.0019 (0.0157)	
training:	Epoch: [73][186/233]	Loss 0.0035 (0.0156)	
training:	Epoch: [73][187/233]	Loss 0.0023 (0.0156)	
training:	Epoch: [73][188/233]	Loss 0.0016 (0.0155)	
training:	Epoch: [73][189/233]	Loss 0.0201 (0.0155)	
training:	Epoch: [73][190/233]	Loss 0.0043 (0.0155)	
training:	Epoch: [73][191/233]	Loss 0.0017 (0.0154)	
training:	Epoch: [73][192/233]	Loss 0.0014 (0.0153)	
training:	Epoch: [73][193/233]	Loss 0.0065 (0.0153)	
training:	Epoch: [73][194/233]	Loss 0.0096 (0.0152)	
training:	Epoch: [73][195/233]	Loss 0.0018 (0.0152)	
training:	Epoch: [73][196/233]	Loss 0.0116 (0.0151)	
training:	Epoch: [73][197/233]	Loss 0.0018 (0.0151)	
training:	Epoch: [73][198/233]	Loss 0.0059 (0.0150)	
training:	Epoch: [73][199/233]	Loss 0.0019 (0.0150)	
training:	Epoch: [73][200/233]	Loss 0.0036 (0.0149)	
training:	Epoch: [73][201/233]	Loss 0.0022 (0.0148)	
training:	Epoch: [73][202/233]	Loss 0.0024 (0.0148)	
training:	Epoch: [73][203/233]	Loss 0.0024 (0.0147)	
training:	Epoch: [73][204/233]	Loss 0.0016 (0.0147)	
training:	Epoch: [73][205/233]	Loss 0.0020 (0.0146)	
training:	Epoch: [73][206/233]	Loss 0.0022 (0.0145)	
training:	Epoch: [73][207/233]	Loss 0.0033 (0.0145)	
training:	Epoch: [73][208/233]	Loss 0.0018 (0.0144)	
training:	Epoch: [73][209/233]	Loss 0.0022 (0.0144)	
training:	Epoch: [73][210/233]	Loss 0.0050 (0.0143)	
training:	Epoch: [73][211/233]	Loss 0.0060 (0.0143)	
training:	Epoch: [73][212/233]	Loss 0.0048 (0.0142)	
training:	Epoch: [73][213/233]	Loss 0.0020 (0.0142)	
training:	Epoch: [73][214/233]	Loss 0.0017 (0.0141)	
training:	Epoch: [73][215/233]	Loss 0.0017 (0.0141)	
training:	Epoch: [73][216/233]	Loss 0.0078 (0.0140)	
training:	Epoch: [73][217/233]	Loss 0.0015 (0.0140)	
training:	Epoch: [73][218/233]	Loss 0.0936 (0.0143)	
training:	Epoch: [73][219/233]	Loss 0.0016 (0.0143)	
training:	Epoch: [73][220/233]	Loss 0.0023 (0.0142)	
training:	Epoch: [73][221/233]	Loss 0.0029 (0.0142)	
training:	Epoch: [73][222/233]	Loss 0.1864 (0.0150)	
training:	Epoch: [73][223/233]	Loss 0.0018 (0.0149)	
training:	Epoch: [73][224/233]	Loss 0.0321 (0.0150)	
training:	Epoch: [73][225/233]	Loss 0.0022 (0.0149)	
training:	Epoch: [73][226/233]	Loss 0.0027 (0.0149)	
training:	Epoch: [73][227/233]	Loss 0.0315 (0.0149)	
training:	Epoch: [73][228/233]	Loss 0.0038 (0.0149)	
training:	Epoch: [73][229/233]	Loss 0.0021 (0.0148)	
training:	Epoch: [73][230/233]	Loss 0.0069 (0.0148)	
training:	Epoch: [73][231/233]	Loss 0.0015 (0.0147)	
training:	Epoch: [73][232/233]	Loss 0.0020 (0.0147)	
training:	Epoch: [73][233/233]	Loss 0.0017 (0.0146)	
Training:	 Loss: 0.0146

Training:	 ACC: 0.9992 0.9992 0.9992 0.9992
Validation:	 ACC: 0.7829 0.7828 0.7804 0.7854
Validation:	 Best_BACC: 0.8162 0.8154 0.7998 0.8326
Validation:	 Loss: 1.0569
Pretraining:	Epoch 74/200
----------
training:	Epoch: [74][1/233]	Loss 0.0016 (0.0016)	
training:	Epoch: [74][2/233]	Loss 0.0021 (0.0018)	
training:	Epoch: [74][3/233]	Loss 0.0018 (0.0018)	
training:	Epoch: [74][4/233]	Loss 0.0102 (0.0039)	
training:	Epoch: [74][5/233]	Loss 0.0022 (0.0036)	
training:	Epoch: [74][6/233]	Loss 0.0063 (0.0040)	
training:	Epoch: [74][7/233]	Loss 0.0128 (0.0053)	
training:	Epoch: [74][8/233]	Loss 0.0023 (0.0049)	
training:	Epoch: [74][9/233]	Loss 0.0022 (0.0046)	
training:	Epoch: [74][10/233]	Loss 0.0019 (0.0043)	
training:	Epoch: [74][11/233]	Loss 0.0027 (0.0042)	
training:	Epoch: [74][12/233]	Loss 0.0055 (0.0043)	
training:	Epoch: [74][13/233]	Loss 0.0021 (0.0041)	
training:	Epoch: [74][14/233]	Loss 0.0017 (0.0039)	
training:	Epoch: [74][15/233]	Loss 0.0020 (0.0038)	
training:	Epoch: [74][16/233]	Loss 0.0222 (0.0050)	
training:	Epoch: [74][17/233]	Loss 0.0030 (0.0048)	
training:	Epoch: [74][18/233]	Loss 0.0113 (0.0052)	
training:	Epoch: [74][19/233]	Loss 0.0017 (0.0050)	
training:	Epoch: [74][20/233]	Loss 0.0017 (0.0049)	
training:	Epoch: [74][21/233]	Loss 0.1741 (0.0129)	
training:	Epoch: [74][22/233]	Loss 0.0027 (0.0125)	
training:	Epoch: [74][23/233]	Loss 0.0919 (0.0159)	
training:	Epoch: [74][24/233]	Loss 0.0021 (0.0153)	
training:	Epoch: [74][25/233]	Loss 0.0046 (0.0149)	
training:	Epoch: [74][26/233]	Loss 0.0016 (0.0144)	
training:	Epoch: [74][27/233]	Loss 0.0018 (0.0139)	
training:	Epoch: [74][28/233]	Loss 0.0217 (0.0142)	
training:	Epoch: [74][29/233]	Loss 0.0029 (0.0138)	
training:	Epoch: [74][30/233]	Loss 0.0042 (0.0135)	
training:	Epoch: [74][31/233]	Loss 0.0033 (0.0132)	
training:	Epoch: [74][32/233]	Loss 0.0022 (0.0128)	
training:	Epoch: [74][33/233]	Loss 0.0021 (0.0125)	
training:	Epoch: [74][34/233]	Loss 0.0030 (0.0122)	
training:	Epoch: [74][35/233]	Loss 0.0021 (0.0119)	
training:	Epoch: [74][36/233]	Loss 0.0015 (0.0116)	
training:	Epoch: [74][37/233]	Loss 0.0021 (0.0114)	
training:	Epoch: [74][38/233]	Loss 0.0017 (0.0111)	
training:	Epoch: [74][39/233]	Loss 0.0019 (0.0109)	
training:	Epoch: [74][40/233]	Loss 0.0028 (0.0107)	
training:	Epoch: [74][41/233]	Loss 0.0021 (0.0105)	
training:	Epoch: [74][42/233]	Loss 0.0019 (0.0103)	
training:	Epoch: [74][43/233]	Loss 0.0029 (0.0101)	
training:	Epoch: [74][44/233]	Loss 0.0016 (0.0099)	
training:	Epoch: [74][45/233]	Loss 0.0025 (0.0097)	
training:	Epoch: [74][46/233]	Loss 0.0069 (0.0097)	
training:	Epoch: [74][47/233]	Loss 0.0019 (0.0095)	
training:	Epoch: [74][48/233]	Loss 0.0023 (0.0094)	
training:	Epoch: [74][49/233]	Loss 0.1194 (0.0116)	
training:	Epoch: [74][50/233]	Loss 0.2124 (0.0156)	
training:	Epoch: [74][51/233]	Loss 0.0018 (0.0154)	
training:	Epoch: [74][52/233]	Loss 0.0021 (0.0151)	
training:	Epoch: [74][53/233]	Loss 0.0017 (0.0149)	
training:	Epoch: [74][54/233]	Loss 0.0031 (0.0146)	
training:	Epoch: [74][55/233]	Loss 0.0022 (0.0144)	
training:	Epoch: [74][56/233]	Loss 0.0580 (0.0152)	
training:	Epoch: [74][57/233]	Loss 0.1938 (0.0183)	
training:	Epoch: [74][58/233]	Loss 0.0023 (0.0180)	
training:	Epoch: [74][59/233]	Loss 0.0019 (0.0178)	
training:	Epoch: [74][60/233]	Loss 0.0020 (0.0175)	
training:	Epoch: [74][61/233]	Loss 0.0015 (0.0172)	
training:	Epoch: [74][62/233]	Loss 0.0017 (0.0170)	
training:	Epoch: [74][63/233]	Loss 0.0025 (0.0168)	
training:	Epoch: [74][64/233]	Loss 0.0016 (0.0165)	
training:	Epoch: [74][65/233]	Loss 0.0036 (0.0163)	
training:	Epoch: [74][66/233]	Loss 0.0024 (0.0161)	
training:	Epoch: [74][67/233]	Loss 0.0184 (0.0162)	
training:	Epoch: [74][68/233]	Loss 0.0110 (0.0161)	
training:	Epoch: [74][69/233]	Loss 0.0016 (0.0159)	
training:	Epoch: [74][70/233]	Loss 0.0018 (0.0157)	
training:	Epoch: [74][71/233]	Loss 0.0018 (0.0155)	
training:	Epoch: [74][72/233]	Loss 0.0149 (0.0155)	
training:	Epoch: [74][73/233]	Loss 0.0055 (0.0153)	
training:	Epoch: [74][74/233]	Loss 0.0016 (0.0151)	
training:	Epoch: [74][75/233]	Loss 0.0023 (0.0150)	
training:	Epoch: [74][76/233]	Loss 0.0061 (0.0149)	
training:	Epoch: [74][77/233]	Loss 0.0027 (0.0147)	
training:	Epoch: [74][78/233]	Loss 0.0017 (0.0145)	
training:	Epoch: [74][79/233]	Loss 0.0022 (0.0144)	
training:	Epoch: [74][80/233]	Loss 0.0017 (0.0142)	
training:	Epoch: [74][81/233]	Loss 0.0015 (0.0141)	
training:	Epoch: [74][82/233]	Loss 0.0016 (0.0139)	
training:	Epoch: [74][83/233]	Loss 0.0019 (0.0138)	
training:	Epoch: [74][84/233]	Loss 0.0017 (0.0136)	
training:	Epoch: [74][85/233]	Loss 0.0030 (0.0135)	
training:	Epoch: [74][86/233]	Loss 0.0026 (0.0134)	
training:	Epoch: [74][87/233]	Loss 0.0030 (0.0132)	
training:	Epoch: [74][88/233]	Loss 0.0016 (0.0131)	
training:	Epoch: [74][89/233]	Loss 0.0015 (0.0130)	
training:	Epoch: [74][90/233]	Loss 0.0017 (0.0129)	
training:	Epoch: [74][91/233]	Loss 0.0019 (0.0127)	
training:	Epoch: [74][92/233]	Loss 0.0020 (0.0126)	
training:	Epoch: [74][93/233]	Loss 0.0020 (0.0125)	
training:	Epoch: [74][94/233]	Loss 0.1093 (0.0135)	
training:	Epoch: [74][95/233]	Loss 0.0020 (0.0134)	
training:	Epoch: [74][96/233]	Loss 0.0564 (0.0139)	
training:	Epoch: [74][97/233]	Loss 0.0017 (0.0137)	
training:	Epoch: [74][98/233]	Loss 0.0026 (0.0136)	
training:	Epoch: [74][99/233]	Loss 0.0033 (0.0135)	
training:	Epoch: [74][100/233]	Loss 0.1956 (0.0153)	
training:	Epoch: [74][101/233]	Loss 0.0037 (0.0152)	
training:	Epoch: [74][102/233]	Loss 0.0024 (0.0151)	
training:	Epoch: [74][103/233]	Loss 0.0487 (0.0154)	
training:	Epoch: [74][104/233]	Loss 0.0021 (0.0153)	
training:	Epoch: [74][105/233]	Loss 0.0016 (0.0152)	
training:	Epoch: [74][106/233]	Loss 0.0122 (0.0151)	
training:	Epoch: [74][107/233]	Loss 0.0021 (0.0150)	
training:	Epoch: [74][108/233]	Loss 0.0022 (0.0149)	
training:	Epoch: [74][109/233]	Loss 0.0092 (0.0148)	
training:	Epoch: [74][110/233]	Loss 0.0034 (0.0147)	
training:	Epoch: [74][111/233]	Loss 0.0022 (0.0146)	
training:	Epoch: [74][112/233]	Loss 0.0041 (0.0145)	
training:	Epoch: [74][113/233]	Loss 0.0017 (0.0144)	
training:	Epoch: [74][114/233]	Loss 0.0016 (0.0143)	
training:	Epoch: [74][115/233]	Loss 0.0040 (0.0142)	
training:	Epoch: [74][116/233]	Loss 0.0181 (0.0143)	
training:	Epoch: [74][117/233]	Loss 0.0024 (0.0142)	
training:	Epoch: [74][118/233]	Loss 0.0021 (0.0140)	
training:	Epoch: [74][119/233]	Loss 0.0019 (0.0139)	
training:	Epoch: [74][120/233]	Loss 0.0026 (0.0139)	
training:	Epoch: [74][121/233]	Loss 0.0021 (0.0138)	
training:	Epoch: [74][122/233]	Loss 0.0019 (0.0137)	
training:	Epoch: [74][123/233]	Loss 0.0019 (0.0136)	
training:	Epoch: [74][124/233]	Loss 0.0364 (0.0137)	
training:	Epoch: [74][125/233]	Loss 0.1995 (0.0152)	
training:	Epoch: [74][126/233]	Loss 0.0030 (0.0151)	
training:	Epoch: [74][127/233]	Loss 0.0185 (0.0152)	
training:	Epoch: [74][128/233]	Loss 0.0039 (0.0151)	
training:	Epoch: [74][129/233]	Loss 0.0022 (0.0150)	
training:	Epoch: [74][130/233]	Loss 0.0037 (0.0149)	
training:	Epoch: [74][131/233]	Loss 0.0023 (0.0148)	
training:	Epoch: [74][132/233]	Loss 0.0017 (0.0147)	
training:	Epoch: [74][133/233]	Loss 0.0016 (0.0146)	
training:	Epoch: [74][134/233]	Loss 0.0069 (0.0145)	
training:	Epoch: [74][135/233]	Loss 0.0018 (0.0144)	
training:	Epoch: [74][136/233]	Loss 0.0018 (0.0144)	
training:	Epoch: [74][137/233]	Loss 0.0021 (0.0143)	
training:	Epoch: [74][138/233]	Loss 0.0025 (0.0142)	
training:	Epoch: [74][139/233]	Loss 0.0034 (0.0141)	
training:	Epoch: [74][140/233]	Loss 0.0016 (0.0140)	
training:	Epoch: [74][141/233]	Loss 0.0018 (0.0139)	
training:	Epoch: [74][142/233]	Loss 0.0020 (0.0138)	
training:	Epoch: [74][143/233]	Loss 0.0028 (0.0138)	
training:	Epoch: [74][144/233]	Loss 0.0016 (0.0137)	
training:	Epoch: [74][145/233]	Loss 0.0017 (0.0136)	
training:	Epoch: [74][146/233]	Loss 0.0016 (0.0135)	
training:	Epoch: [74][147/233]	Loss 0.0017 (0.0134)	
training:	Epoch: [74][148/233]	Loss 0.0026 (0.0134)	
training:	Epoch: [74][149/233]	Loss 0.0071 (0.0133)	
training:	Epoch: [74][150/233]	Loss 0.0017 (0.0132)	
training:	Epoch: [74][151/233]	Loss 0.0018 (0.0132)	
training:	Epoch: [74][152/233]	Loss 0.0122 (0.0132)	
training:	Epoch: [74][153/233]	Loss 0.0028 (0.0131)	
training:	Epoch: [74][154/233]	Loss 0.0018 (0.0130)	
training:	Epoch: [74][155/233]	Loss 0.0017 (0.0129)	
training:	Epoch: [74][156/233]	Loss 0.0018 (0.0129)	
training:	Epoch: [74][157/233]	Loss 0.0014 (0.0128)	
training:	Epoch: [74][158/233]	Loss 0.0017 (0.0127)	
training:	Epoch: [74][159/233]	Loss 0.0015 (0.0127)	
training:	Epoch: [74][160/233]	Loss 0.0025 (0.0126)	
training:	Epoch: [74][161/233]	Loss 0.0079 (0.0126)	
training:	Epoch: [74][162/233]	Loss 0.2046 (0.0138)	
training:	Epoch: [74][163/233]	Loss 0.0016 (0.0137)	
training:	Epoch: [74][164/233]	Loss 0.0028 (0.0136)	
training:	Epoch: [74][165/233]	Loss 0.0025 (0.0135)	
training:	Epoch: [74][166/233]	Loss 0.0016 (0.0135)	
training:	Epoch: [74][167/233]	Loss 0.0016 (0.0134)	
training:	Epoch: [74][168/233]	Loss 0.0016 (0.0133)	
training:	Epoch: [74][169/233]	Loss 0.0016 (0.0133)	
training:	Epoch: [74][170/233]	Loss 0.0028 (0.0132)	
training:	Epoch: [74][171/233]	Loss 0.0024 (0.0131)	
training:	Epoch: [74][172/233]	Loss 0.0016 (0.0131)	
training:	Epoch: [74][173/233]	Loss 0.0017 (0.0130)	
training:	Epoch: [74][174/233]	Loss 0.0029 (0.0129)	
training:	Epoch: [74][175/233]	Loss 0.0016 (0.0129)	
training:	Epoch: [74][176/233]	Loss 0.0027 (0.0128)	
training:	Epoch: [74][177/233]	Loss 0.0029 (0.0128)	
training:	Epoch: [74][178/233]	Loss 0.0020 (0.0127)	
training:	Epoch: [74][179/233]	Loss 0.0023 (0.0126)	
training:	Epoch: [74][180/233]	Loss 0.0019 (0.0126)	
training:	Epoch: [74][181/233]	Loss 0.0017 (0.0125)	
training:	Epoch: [74][182/233]	Loss 0.0018 (0.0125)	
training:	Epoch: [74][183/233]	Loss 0.0017 (0.0124)	
training:	Epoch: [74][184/233]	Loss 0.0042 (0.0124)	
training:	Epoch: [74][185/233]	Loss 0.0029 (0.0123)	
training:	Epoch: [74][186/233]	Loss 0.0083 (0.0123)	
training:	Epoch: [74][187/233]	Loss 0.0019 (0.0122)	
training:	Epoch: [74][188/233]	Loss 0.0105 (0.0122)	
training:	Epoch: [74][189/233]	Loss 0.0923 (0.0127)	
training:	Epoch: [74][190/233]	Loss 0.0016 (0.0126)	
training:	Epoch: [74][191/233]	Loss 0.0017 (0.0125)	
training:	Epoch: [74][192/233]	Loss 0.0021 (0.0125)	
training:	Epoch: [74][193/233]	Loss 0.2047 (0.0135)	
training:	Epoch: [74][194/233]	Loss 0.0015 (0.0134)	
training:	Epoch: [74][195/233]	Loss 0.0016 (0.0134)	
training:	Epoch: [74][196/233]	Loss 0.0020 (0.0133)	
training:	Epoch: [74][197/233]	Loss 0.0021 (0.0132)	
training:	Epoch: [74][198/233]	Loss 0.0016 (0.0132)	
training:	Epoch: [74][199/233]	Loss 0.0017 (0.0131)	
training:	Epoch: [74][200/233]	Loss 0.0017 (0.0131)	
training:	Epoch: [74][201/233]	Loss 0.0018 (0.0130)	
training:	Epoch: [74][202/233]	Loss 0.0021 (0.0130)	
training:	Epoch: [74][203/233]	Loss 0.0273 (0.0130)	
training:	Epoch: [74][204/233]	Loss 0.1767 (0.0138)	
training:	Epoch: [74][205/233]	Loss 0.0565 (0.0140)	
training:	Epoch: [74][206/233]	Loss 0.0019 (0.0140)	
training:	Epoch: [74][207/233]	Loss 0.0017 (0.0139)	
training:	Epoch: [74][208/233]	Loss 0.0014 (0.0139)	
training:	Epoch: [74][209/233]	Loss 0.0017 (0.0138)	
training:	Epoch: [74][210/233]	Loss 0.0024 (0.0137)	
training:	Epoch: [74][211/233]	Loss 0.0028 (0.0137)	
training:	Epoch: [74][212/233]	Loss 0.0113 (0.0137)	
training:	Epoch: [74][213/233]	Loss 0.0016 (0.0136)	
training:	Epoch: [74][214/233]	Loss 0.0020 (0.0136)	
training:	Epoch: [74][215/233]	Loss 0.0032 (0.0135)	
training:	Epoch: [74][216/233]	Loss 0.0017 (0.0135)	
training:	Epoch: [74][217/233]	Loss 0.0097 (0.0135)	
training:	Epoch: [74][218/233]	Loss 0.0018 (0.0134)	
training:	Epoch: [74][219/233]	Loss 0.0021 (0.0133)	
training:	Epoch: [74][220/233]	Loss 0.0031 (0.0133)	
training:	Epoch: [74][221/233]	Loss 0.0017 (0.0132)	
training:	Epoch: [74][222/233]	Loss 0.0018 (0.0132)	
training:	Epoch: [74][223/233]	Loss 0.0019 (0.0131)	
training:	Epoch: [74][224/233]	Loss 0.0038 (0.0131)	
training:	Epoch: [74][225/233]	Loss 0.0022 (0.0131)	
training:	Epoch: [74][226/233]	Loss 0.0017 (0.0130)	
training:	Epoch: [74][227/233]	Loss 0.0019 (0.0130)	
training:	Epoch: [74][228/233]	Loss 0.0025 (0.0129)	
training:	Epoch: [74][229/233]	Loss 0.0082 (0.0129)	
training:	Epoch: [74][230/233]	Loss 0.0037 (0.0129)	
training:	Epoch: [74][231/233]	Loss 0.0490 (0.0130)	
training:	Epoch: [74][232/233]	Loss 0.0020 (0.0130)	
training:	Epoch: [74][233/233]	Loss 0.0022 (0.0129)	
Training:	 Loss: 0.0129

Training:	 ACC: 0.9988 0.9988 0.9985 0.9992
Validation:	 ACC: 0.7913 0.7892 0.7467 0.8360
Validation:	 Best_BACC: 0.8162 0.8154 0.7998 0.8326
Validation:	 Loss: 1.0563
Pretraining:	Epoch 75/200
----------
training:	Epoch: [75][1/233]	Loss 0.0017 (0.0017)	
training:	Epoch: [75][2/233]	Loss 0.0824 (0.0420)	
training:	Epoch: [75][3/233]	Loss 0.0058 (0.0300)	
training:	Epoch: [75][4/233]	Loss 0.0027 (0.0231)	
training:	Epoch: [75][5/233]	Loss 0.0055 (0.0196)	
training:	Epoch: [75][6/233]	Loss 0.0031 (0.0169)	
training:	Epoch: [75][7/233]	Loss 0.0023 (0.0148)	
training:	Epoch: [75][8/233]	Loss 0.0016 (0.0131)	
training:	Epoch: [75][9/233]	Loss 0.0018 (0.0119)	
training:	Epoch: [75][10/233]	Loss 0.0015 (0.0108)	
training:	Epoch: [75][11/233]	Loss 0.1738 (0.0257)	
training:	Epoch: [75][12/233]	Loss 0.0052 (0.0240)	
training:	Epoch: [75][13/233]	Loss 0.0187 (0.0235)	
training:	Epoch: [75][14/233]	Loss 0.0017 (0.0220)	
training:	Epoch: [75][15/233]	Loss 0.0022 (0.0207)	
training:	Epoch: [75][16/233]	Loss 0.0016 (0.0195)	
training:	Epoch: [75][17/233]	Loss 0.0016 (0.0184)	
training:	Epoch: [75][18/233]	Loss 0.0016 (0.0175)	
training:	Epoch: [75][19/233]	Loss 0.0331 (0.0183)	
training:	Epoch: [75][20/233]	Loss 0.0030 (0.0175)	
training:	Epoch: [75][21/233]	Loss 0.0016 (0.0168)	
training:	Epoch: [75][22/233]	Loss 0.0020 (0.0161)	
training:	Epoch: [75][23/233]	Loss 0.0027 (0.0155)	
training:	Epoch: [75][24/233]	Loss 0.0021 (0.0150)	
training:	Epoch: [75][25/233]	Loss 0.0016 (0.0144)	
training:	Epoch: [75][26/233]	Loss 0.0016 (0.0139)	
training:	Epoch: [75][27/233]	Loss 0.0024 (0.0135)	
training:	Epoch: [75][28/233]	Loss 0.0024 (0.0131)	
training:	Epoch: [75][29/233]	Loss 0.0024 (0.0127)	
training:	Epoch: [75][30/233]	Loss 0.0144 (0.0128)	
training:	Epoch: [75][31/233]	Loss 0.0017 (0.0124)	
training:	Epoch: [75][32/233]	Loss 0.0020 (0.0121)	
training:	Epoch: [75][33/233]	Loss 0.0116 (0.0121)	
training:	Epoch: [75][34/233]	Loss 0.0016 (0.0118)	
training:	Epoch: [75][35/233]	Loss 0.0320 (0.0124)	
training:	Epoch: [75][36/233]	Loss 0.0015 (0.0121)	
training:	Epoch: [75][37/233]	Loss 0.0017 (0.0118)	
training:	Epoch: [75][38/233]	Loss 0.0032 (0.0116)	
training:	Epoch: [75][39/233]	Loss 0.0019 (0.0113)	
training:	Epoch: [75][40/233]	Loss 0.0067 (0.0112)	
training:	Epoch: [75][41/233]	Loss 0.0017 (0.0110)	
training:	Epoch: [75][42/233]	Loss 0.0019 (0.0107)	
training:	Epoch: [75][43/233]	Loss 0.1963 (0.0151)	
training:	Epoch: [75][44/233]	Loss 0.0018 (0.0148)	
training:	Epoch: [75][45/233]	Loss 0.0041 (0.0145)	
training:	Epoch: [75][46/233]	Loss 0.0016 (0.0142)	
training:	Epoch: [75][47/233]	Loss 0.0016 (0.0140)	
training:	Epoch: [75][48/233]	Loss 0.0017 (0.0137)	
training:	Epoch: [75][49/233]	Loss 0.0016 (0.0135)	
training:	Epoch: [75][50/233]	Loss 0.0016 (0.0132)	
training:	Epoch: [75][51/233]	Loss 0.0016 (0.0130)	
training:	Epoch: [75][52/233]	Loss 0.0017 (0.0128)	
training:	Epoch: [75][53/233]	Loss 0.1807 (0.0160)	
training:	Epoch: [75][54/233]	Loss 0.0015 (0.0157)	
training:	Epoch: [75][55/233]	Loss 0.0016 (0.0154)	
training:	Epoch: [75][56/233]	Loss 0.0018 (0.0152)	
training:	Epoch: [75][57/233]	Loss 0.0091 (0.0151)	
training:	Epoch: [75][58/233]	Loss 0.0017 (0.0149)	
training:	Epoch: [75][59/233]	Loss 0.0040 (0.0147)	
training:	Epoch: [75][60/233]	Loss 0.0030 (0.0145)	
training:	Epoch: [75][61/233]	Loss 0.0017 (0.0143)	
training:	Epoch: [75][62/233]	Loss 0.0037 (0.0141)	
training:	Epoch: [75][63/233]	Loss 0.0879 (0.0153)	
training:	Epoch: [75][64/233]	Loss 0.0023 (0.0151)	
training:	Epoch: [75][65/233]	Loss 0.2052 (0.0180)	
training:	Epoch: [75][66/233]	Loss 0.0022 (0.0178)	
training:	Epoch: [75][67/233]	Loss 0.0017 (0.0175)	
training:	Epoch: [75][68/233]	Loss 0.0027 (0.0173)	
training:	Epoch: [75][69/233]	Loss 0.1772 (0.0196)	
training:	Epoch: [75][70/233]	Loss 0.0038 (0.0194)	
training:	Epoch: [75][71/233]	Loss 0.0089 (0.0192)	
training:	Epoch: [75][72/233]	Loss 0.0038 (0.0190)	
training:	Epoch: [75][73/233]	Loss 0.0019 (0.0188)	
training:	Epoch: [75][74/233]	Loss 0.0015 (0.0186)	
training:	Epoch: [75][75/233]	Loss 0.0031 (0.0183)	
training:	Epoch: [75][76/233]	Loss 0.0018 (0.0181)	
training:	Epoch: [75][77/233]	Loss 0.0019 (0.0179)	
training:	Epoch: [75][78/233]	Loss 0.0017 (0.0177)	
training:	Epoch: [75][79/233]	Loss 0.0020 (0.0175)	
training:	Epoch: [75][80/233]	Loss 0.0018 (0.0173)	
training:	Epoch: [75][81/233]	Loss 0.0019 (0.0171)	
training:	Epoch: [75][82/233]	Loss 0.1378 (0.0186)	
training:	Epoch: [75][83/233]	Loss 0.0016 (0.0184)	
training:	Epoch: [75][84/233]	Loss 0.0020 (0.0182)	
training:	Epoch: [75][85/233]	Loss 0.0041 (0.0180)	
training:	Epoch: [75][86/233]	Loss 0.0014 (0.0178)	
training:	Epoch: [75][87/233]	Loss 0.0054 (0.0177)	
training:	Epoch: [75][88/233]	Loss 0.0099 (0.0176)	
training:	Epoch: [75][89/233]	Loss 0.0019 (0.0174)	
training:	Epoch: [75][90/233]	Loss 0.0017 (0.0173)	
training:	Epoch: [75][91/233]	Loss 0.0017 (0.0171)	
training:	Epoch: [75][92/233]	Loss 0.0019 (0.0169)	
training:	Epoch: [75][93/233]	Loss 0.0018 (0.0168)	
training:	Epoch: [75][94/233]	Loss 0.0088 (0.0167)	
training:	Epoch: [75][95/233]	Loss 0.0119 (0.0166)	
training:	Epoch: [75][96/233]	Loss 0.0980 (0.0175)	
training:	Epoch: [75][97/233]	Loss 0.0046 (0.0173)	
training:	Epoch: [75][98/233]	Loss 0.0017 (0.0172)	
training:	Epoch: [75][99/233]	Loss 0.2066 (0.0191)	
training:	Epoch: [75][100/233]	Loss 0.0027 (0.0189)	
training:	Epoch: [75][101/233]	Loss 0.0018 (0.0188)	
training:	Epoch: [75][102/233]	Loss 0.0035 (0.0186)	
training:	Epoch: [75][103/233]	Loss 0.0015 (0.0184)	
training:	Epoch: [75][104/233]	Loss 0.0328 (0.0186)	
training:	Epoch: [75][105/233]	Loss 0.0018 (0.0184)	
training:	Epoch: [75][106/233]	Loss 0.2139 (0.0203)	
training:	Epoch: [75][107/233]	Loss 0.1820 (0.0218)	
training:	Epoch: [75][108/233]	Loss 0.1439 (0.0229)	
training:	Epoch: [75][109/233]	Loss 0.0019 (0.0227)	
training:	Epoch: [75][110/233]	Loss 0.0021 (0.0225)	
training:	Epoch: [75][111/233]	Loss 0.0022 (0.0223)	
training:	Epoch: [75][112/233]	Loss 0.0536 (0.0226)	
training:	Epoch: [75][113/233]	Loss 0.0016 (0.0224)	
training:	Epoch: [75][114/233]	Loss 0.0018 (0.0223)	
training:	Epoch: [75][115/233]	Loss 0.0124 (0.0222)	
training:	Epoch: [75][116/233]	Loss 0.0021 (0.0220)	
training:	Epoch: [75][117/233]	Loss 0.0088 (0.0219)	
training:	Epoch: [75][118/233]	Loss 0.0021 (0.0217)	
training:	Epoch: [75][119/233]	Loss 0.0018 (0.0215)	
training:	Epoch: [75][120/233]	Loss 0.0017 (0.0214)	
training:	Epoch: [75][121/233]	Loss 0.0021 (0.0212)	
training:	Epoch: [75][122/233]	Loss 0.0070 (0.0211)	
training:	Epoch: [75][123/233]	Loss 0.0031 (0.0210)	
training:	Epoch: [75][124/233]	Loss 0.0099 (0.0209)	
training:	Epoch: [75][125/233]	Loss 0.0020 (0.0207)	
training:	Epoch: [75][126/233]	Loss 0.0024 (0.0206)	
training:	Epoch: [75][127/233]	Loss 0.0024 (0.0204)	
training:	Epoch: [75][128/233]	Loss 0.0030 (0.0203)	
training:	Epoch: [75][129/233]	Loss 0.0019 (0.0201)	
training:	Epoch: [75][130/233]	Loss 0.0041 (0.0200)	
training:	Epoch: [75][131/233]	Loss 0.0014 (0.0199)	
training:	Epoch: [75][132/233]	Loss 0.0018 (0.0197)	
training:	Epoch: [75][133/233]	Loss 0.0017 (0.0196)	
training:	Epoch: [75][134/233]	Loss 0.0036 (0.0195)	
training:	Epoch: [75][135/233]	Loss 0.0035 (0.0194)	
training:	Epoch: [75][136/233]	Loss 0.0025 (0.0192)	
training:	Epoch: [75][137/233]	Loss 0.0018 (0.0191)	
training:	Epoch: [75][138/233]	Loss 0.0019 (0.0190)	
training:	Epoch: [75][139/233]	Loss 0.0478 (0.0192)	
training:	Epoch: [75][140/233]	Loss 0.0093 (0.0191)	
training:	Epoch: [75][141/233]	Loss 0.0018 (0.0190)	
training:	Epoch: [75][142/233]	Loss 0.0068 (0.0189)	
training:	Epoch: [75][143/233]	Loss 0.0032 (0.0188)	
training:	Epoch: [75][144/233]	Loss 0.0019 (0.0187)	
training:	Epoch: [75][145/233]	Loss 0.0134 (0.0187)	
training:	Epoch: [75][146/233]	Loss 0.0020 (0.0185)	
training:	Epoch: [75][147/233]	Loss 0.0017 (0.0184)	
training:	Epoch: [75][148/233]	Loss 0.0020 (0.0183)	
training:	Epoch: [75][149/233]	Loss 0.0018 (0.0182)	
training:	Epoch: [75][150/233]	Loss 0.2138 (0.0195)	
training:	Epoch: [75][151/233]	Loss 0.0045 (0.0194)	
training:	Epoch: [75][152/233]	Loss 0.0025 (0.0193)	
training:	Epoch: [75][153/233]	Loss 0.0018 (0.0192)	
training:	Epoch: [75][154/233]	Loss 0.0020 (0.0191)	
training:	Epoch: [75][155/233]	Loss 0.0021 (0.0190)	
training:	Epoch: [75][156/233]	Loss 0.0021 (0.0189)	
training:	Epoch: [75][157/233]	Loss 0.0019 (0.0188)	
training:	Epoch: [75][158/233]	Loss 0.0015 (0.0186)	
training:	Epoch: [75][159/233]	Loss 0.0031 (0.0185)	
training:	Epoch: [75][160/233]	Loss 0.0018 (0.0184)	
training:	Epoch: [75][161/233]	Loss 0.0022 (0.0183)	
training:	Epoch: [75][162/233]	Loss 0.0016 (0.0182)	
training:	Epoch: [75][163/233]	Loss 0.0017 (0.0181)	
training:	Epoch: [75][164/233]	Loss 0.0021 (0.0180)	
training:	Epoch: [75][165/233]	Loss 0.0015 (0.0179)	
training:	Epoch: [75][166/233]	Loss 0.0021 (0.0178)	
training:	Epoch: [75][167/233]	Loss 0.0034 (0.0178)	
training:	Epoch: [75][168/233]	Loss 0.0016 (0.0177)	
training:	Epoch: [75][169/233]	Loss 0.0018 (0.0176)	
training:	Epoch: [75][170/233]	Loss 0.0019 (0.0175)	
training:	Epoch: [75][171/233]	Loss 0.0018 (0.0174)	
training:	Epoch: [75][172/233]	Loss 0.0328 (0.0175)	
training:	Epoch: [75][173/233]	Loss 0.0025 (0.0174)	
training:	Epoch: [75][174/233]	Loss 0.0016 (0.0173)	
training:	Epoch: [75][175/233]	Loss 0.0022 (0.0172)	
training:	Epoch: [75][176/233]	Loss 0.0015 (0.0171)	
training:	Epoch: [75][177/233]	Loss 0.0037 (0.0170)	
training:	Epoch: [75][178/233]	Loss 0.0016 (0.0170)	
training:	Epoch: [75][179/233]	Loss 0.0512 (0.0171)	
training:	Epoch: [75][180/233]	Loss 0.0016 (0.0171)	
training:	Epoch: [75][181/233]	Loss 0.0020 (0.0170)	
training:	Epoch: [75][182/233]	Loss 0.0032 (0.0169)	
training:	Epoch: [75][183/233]	Loss 0.0019 (0.0168)	
training:	Epoch: [75][184/233]	Loss 0.0018 (0.0167)	
training:	Epoch: [75][185/233]	Loss 0.0019 (0.0167)	
training:	Epoch: [75][186/233]	Loss 0.0016 (0.0166)	
training:	Epoch: [75][187/233]	Loss 0.0015 (0.0165)	
training:	Epoch: [75][188/233]	Loss 0.0019 (0.0164)	
training:	Epoch: [75][189/233]	Loss 0.0028 (0.0163)	
training:	Epoch: [75][190/233]	Loss 0.0018 (0.0163)	
training:	Epoch: [75][191/233]	Loss 0.0049 (0.0162)	
training:	Epoch: [75][192/233]	Loss 0.0022 (0.0161)	
training:	Epoch: [75][193/233]	Loss 0.0318 (0.0162)	
training:	Epoch: [75][194/233]	Loss 0.0020 (0.0161)	
training:	Epoch: [75][195/233]	Loss 0.0016 (0.0161)	
training:	Epoch: [75][196/233]	Loss 0.0019 (0.0160)	
training:	Epoch: [75][197/233]	Loss 0.0016 (0.0159)	
training:	Epoch: [75][198/233]	Loss 0.0029 (0.0159)	
training:	Epoch: [75][199/233]	Loss 0.0018 (0.0158)	
training:	Epoch: [75][200/233]	Loss 0.0039 (0.0157)	
training:	Epoch: [75][201/233]	Loss 0.0040 (0.0157)	
training:	Epoch: [75][202/233]	Loss 0.0270 (0.0157)	
training:	Epoch: [75][203/233]	Loss 0.0018 (0.0157)	
training:	Epoch: [75][204/233]	Loss 0.0020 (0.0156)	
training:	Epoch: [75][205/233]	Loss 0.0191 (0.0156)	
training:	Epoch: [75][206/233]	Loss 0.0016 (0.0155)	
training:	Epoch: [75][207/233]	Loss 0.0017 (0.0155)	
training:	Epoch: [75][208/233]	Loss 0.0019 (0.0154)	
training:	Epoch: [75][209/233]	Loss 0.0022 (0.0153)	
training:	Epoch: [75][210/233]	Loss 0.0052 (0.0153)	
training:	Epoch: [75][211/233]	Loss 0.0018 (0.0152)	
training:	Epoch: [75][212/233]	Loss 0.0432 (0.0154)	
training:	Epoch: [75][213/233]	Loss 0.0017 (0.0153)	
training:	Epoch: [75][214/233]	Loss 0.0016 (0.0152)	
training:	Epoch: [75][215/233]	Loss 0.0015 (0.0152)	
training:	Epoch: [75][216/233]	Loss 0.0570 (0.0154)	
training:	Epoch: [75][217/233]	Loss 0.0070 (0.0153)	
training:	Epoch: [75][218/233]	Loss 0.0038 (0.0153)	
training:	Epoch: [75][219/233]	Loss 0.0509 (0.0154)	
training:	Epoch: [75][220/233]	Loss 0.1346 (0.0160)	
training:	Epoch: [75][221/233]	Loss 0.0017 (0.0159)	
training:	Epoch: [75][222/233]	Loss 0.0019 (0.0159)	
training:	Epoch: [75][223/233]	Loss 0.0022 (0.0158)	
training:	Epoch: [75][224/233]	Loss 0.0017 (0.0157)	
training:	Epoch: [75][225/233]	Loss 0.0028 (0.0157)	
training:	Epoch: [75][226/233]	Loss 0.0016 (0.0156)	
training:	Epoch: [75][227/233]	Loss 0.0175 (0.0156)	
training:	Epoch: [75][228/233]	Loss 0.0018 (0.0156)	
training:	Epoch: [75][229/233]	Loss 0.1081 (0.0160)	
training:	Epoch: [75][230/233]	Loss 0.0021 (0.0159)	
training:	Epoch: [75][231/233]	Loss 0.0028 (0.0158)	
training:	Epoch: [75][232/233]	Loss 0.0030 (0.0158)	
training:	Epoch: [75][233/233]	Loss 0.0023 (0.0157)	
Training:	 Loss: 0.0157

Training:	 ACC: 0.9992 0.9992 0.9992 0.9992
Validation:	 ACC: 0.7967 0.7945 0.7508 0.8427
Validation:	 Best_BACC: 0.8162 0.8154 0.7998 0.8326
Validation:	 Loss: 1.0403
Pretraining:	Epoch 76/200
----------
training:	Epoch: [76][1/233]	Loss 0.0025 (0.0025)	
training:	Epoch: [76][2/233]	Loss 0.0120 (0.0073)	
training:	Epoch: [76][3/233]	Loss 0.0019 (0.0055)	
training:	Epoch: [76][4/233]	Loss 0.0018 (0.0045)	
training:	Epoch: [76][5/233]	Loss 0.0019 (0.0040)	
training:	Epoch: [76][6/233]	Loss 0.0014 (0.0036)	
training:	Epoch: [76][7/233]	Loss 0.0554 (0.0110)	
training:	Epoch: [76][8/233]	Loss 0.0026 (0.0099)	
training:	Epoch: [76][9/233]	Loss 0.0017 (0.0090)	
training:	Epoch: [76][10/233]	Loss 0.0191 (0.0100)	
training:	Epoch: [76][11/233]	Loss 0.0017 (0.0093)	
training:	Epoch: [76][12/233]	Loss 0.0031 (0.0088)	
training:	Epoch: [76][13/233]	Loss 0.0025 (0.0083)	
training:	Epoch: [76][14/233]	Loss 0.0015 (0.0078)	
training:	Epoch: [76][15/233]	Loss 0.0179 (0.0085)	
training:	Epoch: [76][16/233]	Loss 0.0019 (0.0081)	
training:	Epoch: [76][17/233]	Loss 0.0019 (0.0077)	
training:	Epoch: [76][18/233]	Loss 0.0017 (0.0074)	
training:	Epoch: [76][19/233]	Loss 0.0017 (0.0071)	
training:	Epoch: [76][20/233]	Loss 0.2113 (0.0173)	
training:	Epoch: [76][21/233]	Loss 0.0016 (0.0165)	
training:	Epoch: [76][22/233]	Loss 0.0023 (0.0159)	
training:	Epoch: [76][23/233]	Loss 0.0019 (0.0153)	
training:	Epoch: [76][24/233]	Loss 0.0097 (0.0150)	
training:	Epoch: [76][25/233]	Loss 0.0016 (0.0145)	
training:	Epoch: [76][26/233]	Loss 0.0053 (0.0141)	
training:	Epoch: [76][27/233]	Loss 0.0015 (0.0137)	
training:	Epoch: [76][28/233]	Loss 0.0018 (0.0133)	
training:	Epoch: [76][29/233]	Loss 0.0016 (0.0129)	
training:	Epoch: [76][30/233]	Loss 0.0018 (0.0125)	
training:	Epoch: [76][31/233]	Loss 0.0050 (0.0122)	
training:	Epoch: [76][32/233]	Loss 0.0020 (0.0119)	
training:	Epoch: [76][33/233]	Loss 0.0293 (0.0125)	
training:	Epoch: [76][34/233]	Loss 0.0015 (0.0121)	
training:	Epoch: [76][35/233]	Loss 0.0016 (0.0118)	
training:	Epoch: [76][36/233]	Loss 0.0016 (0.0115)	
training:	Epoch: [76][37/233]	Loss 0.0017 (0.0113)	
training:	Epoch: [76][38/233]	Loss 0.0090 (0.0112)	
training:	Epoch: [76][39/233]	Loss 0.0017 (0.0110)	
training:	Epoch: [76][40/233]	Loss 0.0022 (0.0108)	
training:	Epoch: [76][41/233]	Loss 0.0017 (0.0105)	
training:	Epoch: [76][42/233]	Loss 0.0026 (0.0103)	
training:	Epoch: [76][43/233]	Loss 0.0113 (0.0104)	
training:	Epoch: [76][44/233]	Loss 0.0021 (0.0102)	
training:	Epoch: [76][45/233]	Loss 0.0017 (0.0100)	
training:	Epoch: [76][46/233]	Loss 0.0019 (0.0098)	
training:	Epoch: [76][47/233]	Loss 0.0016 (0.0096)	
training:	Epoch: [76][48/233]	Loss 0.0020 (0.0095)	
training:	Epoch: [76][49/233]	Loss 0.0026 (0.0093)	
training:	Epoch: [76][50/233]	Loss 0.0018 (0.0092)	
training:	Epoch: [76][51/233]	Loss 0.0018 (0.0090)	
training:	Epoch: [76][52/233]	Loss 0.0016 (0.0089)	
training:	Epoch: [76][53/233]	Loss 0.0017 (0.0088)	
training:	Epoch: [76][54/233]	Loss 0.0029 (0.0087)	
training:	Epoch: [76][55/233]	Loss 0.0021 (0.0085)	
training:	Epoch: [76][56/233]	Loss 0.0015 (0.0084)	
training:	Epoch: [76][57/233]	Loss 0.0095 (0.0084)	
training:	Epoch: [76][58/233]	Loss 0.0020 (0.0083)	
training:	Epoch: [76][59/233]	Loss 0.0021 (0.0082)	
training:	Epoch: [76][60/233]	Loss 0.0014 (0.0081)	
training:	Epoch: [76][61/233]	Loss 0.0034 (0.0080)	
training:	Epoch: [76][62/233]	Loss 0.0102 (0.0081)	
training:	Epoch: [76][63/233]	Loss 0.0017 (0.0080)	
training:	Epoch: [76][64/233]	Loss 0.0320 (0.0083)	
training:	Epoch: [76][65/233]	Loss 0.0024 (0.0082)	
training:	Epoch: [76][66/233]	Loss 0.0020 (0.0082)	
training:	Epoch: [76][67/233]	Loss 0.0017 (0.0081)	
training:	Epoch: [76][68/233]	Loss 0.0020 (0.0080)	
training:	Epoch: [76][69/233]	Loss 0.0054 (0.0079)	
training:	Epoch: [76][70/233]	Loss 0.0016 (0.0078)	
training:	Epoch: [76][71/233]	Loss 0.0355 (0.0082)	
training:	Epoch: [76][72/233]	Loss 0.0023 (0.0081)	
training:	Epoch: [76][73/233]	Loss 0.0098 (0.0082)	
training:	Epoch: [76][74/233]	Loss 0.0020 (0.0081)	
training:	Epoch: [76][75/233]	Loss 0.0015 (0.0080)	
training:	Epoch: [76][76/233]	Loss 0.0348 (0.0084)	
training:	Epoch: [76][77/233]	Loss 0.0017 (0.0083)	
training:	Epoch: [76][78/233]	Loss 0.0026 (0.0082)	
training:	Epoch: [76][79/233]	Loss 0.0015 (0.0081)	
training:	Epoch: [76][80/233]	Loss 0.0018 (0.0080)	
training:	Epoch: [76][81/233]	Loss 0.0038 (0.0080)	
training:	Epoch: [76][82/233]	Loss 0.0021 (0.0079)	
training:	Epoch: [76][83/233]	Loss 0.0015 (0.0078)	
training:	Epoch: [76][84/233]	Loss 0.0018 (0.0078)	
training:	Epoch: [76][85/233]	Loss 0.0024 (0.0077)	
training:	Epoch: [76][86/233]	Loss 0.0020 (0.0076)	
training:	Epoch: [76][87/233]	Loss 0.0026 (0.0076)	
training:	Epoch: [76][88/233]	Loss 0.0068 (0.0076)	
training:	Epoch: [76][89/233]	Loss 0.0020 (0.0075)	
training:	Epoch: [76][90/233]	Loss 0.0239 (0.0077)	
training:	Epoch: [76][91/233]	Loss 0.0024 (0.0076)	
training:	Epoch: [76][92/233]	Loss 0.0015 (0.0076)	
training:	Epoch: [76][93/233]	Loss 0.0039 (0.0075)	
training:	Epoch: [76][94/233]	Loss 0.0046 (0.0075)	
training:	Epoch: [76][95/233]	Loss 0.1971 (0.0095)	
training:	Epoch: [76][96/233]	Loss 0.0029 (0.0094)	
training:	Epoch: [76][97/233]	Loss 0.0034 (0.0093)	
training:	Epoch: [76][98/233]	Loss 0.0021 (0.0093)	
training:	Epoch: [76][99/233]	Loss 0.0018 (0.0092)	
training:	Epoch: [76][100/233]	Loss 0.0016 (0.0091)	
training:	Epoch: [76][101/233]	Loss 0.0281 (0.0093)	
training:	Epoch: [76][102/233]	Loss 0.0016 (0.0092)	
training:	Epoch: [76][103/233]	Loss 0.0021 (0.0092)	
training:	Epoch: [76][104/233]	Loss 0.0079 (0.0092)	
training:	Epoch: [76][105/233]	Loss 0.0015 (0.0091)	
training:	Epoch: [76][106/233]	Loss 0.0025 (0.0090)	
training:	Epoch: [76][107/233]	Loss 0.1030 (0.0099)	
training:	Epoch: [76][108/233]	Loss 0.0018 (0.0098)	
training:	Epoch: [76][109/233]	Loss 0.1240 (0.0109)	
training:	Epoch: [76][110/233]	Loss 0.0021 (0.0108)	
training:	Epoch: [76][111/233]	Loss 0.0015 (0.0107)	
training:	Epoch: [76][112/233]	Loss 0.0169 (0.0108)	
training:	Epoch: [76][113/233]	Loss 0.0026 (0.0107)	
training:	Epoch: [76][114/233]	Loss 0.0016 (0.0106)	
training:	Epoch: [76][115/233]	Loss 0.0031 (0.0105)	
training:	Epoch: [76][116/233]	Loss 0.0467 (0.0109)	
training:	Epoch: [76][117/233]	Loss 0.0016 (0.0108)	
training:	Epoch: [76][118/233]	Loss 0.0019 (0.0107)	
training:	Epoch: [76][119/233]	Loss 0.0018 (0.0106)	
training:	Epoch: [76][120/233]	Loss 0.0083 (0.0106)	
training:	Epoch: [76][121/233]	Loss 0.0022 (0.0105)	
training:	Epoch: [76][122/233]	Loss 0.0015 (0.0105)	
training:	Epoch: [76][123/233]	Loss 0.0140 (0.0105)	
training:	Epoch: [76][124/233]	Loss 0.0016 (0.0104)	
training:	Epoch: [76][125/233]	Loss 0.0043 (0.0104)	
training:	Epoch: [76][126/233]	Loss 0.1641 (0.0116)	
training:	Epoch: [76][127/233]	Loss 0.0229 (0.0117)	
training:	Epoch: [76][128/233]	Loss 0.0015 (0.0116)	
training:	Epoch: [76][129/233]	Loss 0.0016 (0.0115)	
training:	Epoch: [76][130/233]	Loss 0.0014 (0.0114)	
training:	Epoch: [76][131/233]	Loss 0.0024 (0.0114)	
training:	Epoch: [76][132/233]	Loss 0.0022 (0.0113)	
training:	Epoch: [76][133/233]	Loss 0.0248 (0.0114)	
training:	Epoch: [76][134/233]	Loss 0.0027 (0.0113)	
training:	Epoch: [76][135/233]	Loss 0.0145 (0.0114)	
training:	Epoch: [76][136/233]	Loss 0.0342 (0.0115)	
training:	Epoch: [76][137/233]	Loss 0.0019 (0.0115)	
training:	Epoch: [76][138/233]	Loss 0.0027 (0.0114)	
training:	Epoch: [76][139/233]	Loss 0.0277 (0.0115)	
training:	Epoch: [76][140/233]	Loss 0.0016 (0.0114)	
training:	Epoch: [76][141/233]	Loss 0.0018 (0.0114)	
training:	Epoch: [76][142/233]	Loss 0.0059 (0.0113)	
training:	Epoch: [76][143/233]	Loss 0.0018 (0.0113)	
training:	Epoch: [76][144/233]	Loss 0.0016 (0.0112)	
training:	Epoch: [76][145/233]	Loss 0.0018 (0.0111)	
training:	Epoch: [76][146/233]	Loss 0.0057 (0.0111)	
training:	Epoch: [76][147/233]	Loss 0.0054 (0.0111)	
training:	Epoch: [76][148/233]	Loss 0.2041 (0.0124)	
training:	Epoch: [76][149/233]	Loss 0.0025 (0.0123)	
training:	Epoch: [76][150/233]	Loss 0.0158 (0.0123)	
training:	Epoch: [76][151/233]	Loss 0.2101 (0.0136)	
training:	Epoch: [76][152/233]	Loss 0.0025 (0.0136)	
training:	Epoch: [76][153/233]	Loss 0.0017 (0.0135)	
training:	Epoch: [76][154/233]	Loss 0.0882 (0.0140)	
training:	Epoch: [76][155/233]	Loss 0.0016 (0.0139)	
training:	Epoch: [76][156/233]	Loss 0.0600 (0.0142)	
training:	Epoch: [76][157/233]	Loss 0.0611 (0.0145)	
training:	Epoch: [76][158/233]	Loss 0.0047 (0.0144)	
training:	Epoch: [76][159/233]	Loss 0.0566 (0.0147)	
training:	Epoch: [76][160/233]	Loss 0.0057 (0.0146)	
training:	Epoch: [76][161/233]	Loss 0.1314 (0.0154)	
training:	Epoch: [76][162/233]	Loss 0.0019 (0.0153)	
training:	Epoch: [76][163/233]	Loss 0.0020 (0.0152)	
training:	Epoch: [76][164/233]	Loss 0.0017 (0.0151)	
training:	Epoch: [76][165/233]	Loss 0.2047 (0.0163)	
training:	Epoch: [76][166/233]	Loss 0.0014 (0.0162)	
training:	Epoch: [76][167/233]	Loss 0.0019 (0.0161)	
training:	Epoch: [76][168/233]	Loss 0.0035 (0.0160)	
training:	Epoch: [76][169/233]	Loss 0.0017 (0.0159)	
training:	Epoch: [76][170/233]	Loss 0.0017 (0.0158)	
training:	Epoch: [76][171/233]	Loss 0.0021 (0.0158)	
training:	Epoch: [76][172/233]	Loss 0.0115 (0.0157)	
training:	Epoch: [76][173/233]	Loss 0.0024 (0.0157)	
training:	Epoch: [76][174/233]	Loss 0.0020 (0.0156)	
training:	Epoch: [76][175/233]	Loss 0.0016 (0.0155)	
training:	Epoch: [76][176/233]	Loss 0.0019 (0.0154)	
training:	Epoch: [76][177/233]	Loss 0.0018 (0.0153)	
training:	Epoch: [76][178/233]	Loss 0.0017 (0.0153)	
training:	Epoch: [76][179/233]	Loss 0.0017 (0.0152)	
training:	Epoch: [76][180/233]	Loss 0.0357 (0.0153)	
training:	Epoch: [76][181/233]	Loss 0.0020 (0.0152)	
training:	Epoch: [76][182/233]	Loss 0.0018 (0.0152)	
training:	Epoch: [76][183/233]	Loss 0.0015 (0.0151)	
training:	Epoch: [76][184/233]	Loss 0.0029 (0.0150)	
training:	Epoch: [76][185/233]	Loss 0.0147 (0.0150)	
training:	Epoch: [76][186/233]	Loss 0.0047 (0.0150)	
training:	Epoch: [76][187/233]	Loss 0.0015 (0.0149)	
training:	Epoch: [76][188/233]	Loss 0.0017 (0.0148)	
training:	Epoch: [76][189/233]	Loss 0.0017 (0.0148)	
training:	Epoch: [76][190/233]	Loss 0.0020 (0.0147)	
training:	Epoch: [76][191/233]	Loss 0.0028 (0.0146)	
training:	Epoch: [76][192/233]	Loss 0.0022 (0.0146)	
training:	Epoch: [76][193/233]	Loss 0.0023 (0.0145)	
training:	Epoch: [76][194/233]	Loss 0.0017 (0.0144)	
training:	Epoch: [76][195/233]	Loss 0.0024 (0.0144)	
training:	Epoch: [76][196/233]	Loss 0.0056 (0.0143)	
training:	Epoch: [76][197/233]	Loss 0.0653 (0.0146)	
training:	Epoch: [76][198/233]	Loss 0.0016 (0.0145)	
training:	Epoch: [76][199/233]	Loss 0.0642 (0.0148)	
training:	Epoch: [76][200/233]	Loss 0.0990 (0.0152)	
training:	Epoch: [76][201/233]	Loss 0.0040 (0.0151)	
training:	Epoch: [76][202/233]	Loss 0.0054 (0.0151)	
training:	Epoch: [76][203/233]	Loss 0.0022 (0.0150)	
training:	Epoch: [76][204/233]	Loss 0.0018 (0.0150)	
training:	Epoch: [76][205/233]	Loss 0.0022 (0.0149)	
training:	Epoch: [76][206/233]	Loss 0.0018 (0.0148)	
training:	Epoch: [76][207/233]	Loss 0.0060 (0.0148)	
training:	Epoch: [76][208/233]	Loss 0.0033 (0.0147)	
training:	Epoch: [76][209/233]	Loss 0.0054 (0.0147)	
training:	Epoch: [76][210/233]	Loss 0.0059 (0.0146)	
training:	Epoch: [76][211/233]	Loss 0.0038 (0.0146)	
training:	Epoch: [76][212/233]	Loss 0.0132 (0.0146)	
training:	Epoch: [76][213/233]	Loss 0.0019 (0.0145)	
training:	Epoch: [76][214/233]	Loss 0.0022 (0.0145)	
training:	Epoch: [76][215/233]	Loss 0.0027 (0.0144)	
training:	Epoch: [76][216/233]	Loss 0.0015 (0.0144)	
training:	Epoch: [76][217/233]	Loss 0.0024 (0.0143)	
training:	Epoch: [76][218/233]	Loss 0.0098 (0.0143)	
training:	Epoch: [76][219/233]	Loss 0.1692 (0.0150)	
training:	Epoch: [76][220/233]	Loss 0.0033 (0.0149)	
training:	Epoch: [76][221/233]	Loss 0.0261 (0.0150)	
training:	Epoch: [76][222/233]	Loss 0.0618 (0.0152)	
training:	Epoch: [76][223/233]	Loss 0.0181 (0.0152)	
training:	Epoch: [76][224/233]	Loss 0.0020 (0.0151)	
training:	Epoch: [76][225/233]	Loss 0.0026 (0.0151)	
training:	Epoch: [76][226/233]	Loss 0.0447 (0.0152)	
training:	Epoch: [76][227/233]	Loss 0.0018 (0.0152)	
training:	Epoch: [76][228/233]	Loss 0.0158 (0.0152)	
training:	Epoch: [76][229/233]	Loss 0.0475 (0.0153)	
training:	Epoch: [76][230/233]	Loss 0.0019 (0.0152)	
training:	Epoch: [76][231/233]	Loss 0.0016 (0.0152)	
training:	Epoch: [76][232/233]	Loss 0.0203 (0.0152)	
training:	Epoch: [76][233/233]	Loss 0.1999 (0.0160)	
Training:	 Loss: 0.0160

Training:	 ACC: 0.9988 0.9988 0.9985 0.9992
Validation:	 ACC: 0.7855 0.7828 0.7273 0.8438
Validation:	 Best_BACC: 0.8162 0.8154 0.7998 0.8326
Validation:	 Loss: 1.0788
Pretraining:	Epoch 77/200
----------
training:	Epoch: [77][1/233]	Loss 0.0021 (0.0021)	
training:	Epoch: [77][2/233]	Loss 0.0025 (0.0023)	
training:	Epoch: [77][3/233]	Loss 0.0017 (0.0021)	
training:	Epoch: [77][4/233]	Loss 0.0027 (0.0023)	
training:	Epoch: [77][5/233]	Loss 0.0055 (0.0029)	
training:	Epoch: [77][6/233]	Loss 0.0187 (0.0055)	
training:	Epoch: [77][7/233]	Loss 0.1306 (0.0234)	
training:	Epoch: [77][8/233]	Loss 0.0023 (0.0208)	
training:	Epoch: [77][9/233]	Loss 0.0109 (0.0197)	
training:	Epoch: [77][10/233]	Loss 0.0037 (0.0181)	
training:	Epoch: [77][11/233]	Loss 0.0110 (0.0174)	
training:	Epoch: [77][12/233]	Loss 0.0028 (0.0162)	
training:	Epoch: [77][13/233]	Loss 0.1923 (0.0298)	
training:	Epoch: [77][14/233]	Loss 0.0025 (0.0278)	
training:	Epoch: [77][15/233]	Loss 0.0021 (0.0261)	
training:	Epoch: [77][16/233]	Loss 0.0072 (0.0249)	
training:	Epoch: [77][17/233]	Loss 0.0042 (0.0237)	
training:	Epoch: [77][18/233]	Loss 0.0015 (0.0225)	
training:	Epoch: [77][19/233]	Loss 0.0017 (0.0214)	
training:	Epoch: [77][20/233]	Loss 0.0019 (0.0204)	
training:	Epoch: [77][21/233]	Loss 0.0027 (0.0196)	
training:	Epoch: [77][22/233]	Loss 0.0028 (0.0188)	
training:	Epoch: [77][23/233]	Loss 0.0025 (0.0181)	
training:	Epoch: [77][24/233]	Loss 0.0049 (0.0175)	
training:	Epoch: [77][25/233]	Loss 0.0015 (0.0169)	
training:	Epoch: [77][26/233]	Loss 0.0124 (0.0167)	
training:	Epoch: [77][27/233]	Loss 0.0022 (0.0162)	
training:	Epoch: [77][28/233]	Loss 0.0041 (0.0158)	
training:	Epoch: [77][29/233]	Loss 0.0063 (0.0154)	
training:	Epoch: [77][30/233]	Loss 0.0031 (0.0150)	
training:	Epoch: [77][31/233]	Loss 0.0134 (0.0150)	
training:	Epoch: [77][32/233]	Loss 0.0018 (0.0146)	
training:	Epoch: [77][33/233]	Loss 0.0024 (0.0142)	
training:	Epoch: [77][34/233]	Loss 0.0020 (0.0138)	
training:	Epoch: [77][35/233]	Loss 0.0049 (0.0136)	
training:	Epoch: [77][36/233]	Loss 0.0016 (0.0132)	
training:	Epoch: [77][37/233]	Loss 0.0030 (0.0130)	
training:	Epoch: [77][38/233]	Loss 0.1358 (0.0162)	
training:	Epoch: [77][39/233]	Loss 0.0017 (0.0158)	
training:	Epoch: [77][40/233]	Loss 0.0250 (0.0160)	
training:	Epoch: [77][41/233]	Loss 0.0027 (0.0157)	
training:	Epoch: [77][42/233]	Loss 0.0058 (0.0155)	
training:	Epoch: [77][43/233]	Loss 0.0025 (0.0152)	
training:	Epoch: [77][44/233]	Loss 0.0025 (0.0149)	
training:	Epoch: [77][45/233]	Loss 0.0025 (0.0146)	
training:	Epoch: [77][46/233]	Loss 0.0784 (0.0160)	
training:	Epoch: [77][47/233]	Loss 0.0019 (0.0157)	
training:	Epoch: [77][48/233]	Loss 0.0016 (0.0154)	
training:	Epoch: [77][49/233]	Loss 0.0017 (0.0151)	
training:	Epoch: [77][50/233]	Loss 0.0020 (0.0149)	
training:	Epoch: [77][51/233]	Loss 0.0026 (0.0146)	
training:	Epoch: [77][52/233]	Loss 0.0021 (0.0144)	
training:	Epoch: [77][53/233]	Loss 0.0019 (0.0142)	
training:	Epoch: [77][54/233]	Loss 0.0028 (0.0139)	
training:	Epoch: [77][55/233]	Loss 0.0017 (0.0137)	
training:	Epoch: [77][56/233]	Loss 0.0070 (0.0136)	
training:	Epoch: [77][57/233]	Loss 0.0079 (0.0135)	
training:	Epoch: [77][58/233]	Loss 0.0025 (0.0133)	
training:	Epoch: [77][59/233]	Loss 0.0641 (0.0142)	
training:	Epoch: [77][60/233]	Loss 0.0015 (0.0140)	
training:	Epoch: [77][61/233]	Loss 0.0697 (0.0149)	
training:	Epoch: [77][62/233]	Loss 0.0020 (0.0147)	
training:	Epoch: [77][63/233]	Loss 0.0018 (0.0145)	
training:	Epoch: [77][64/233]	Loss 0.0021 (0.0143)	
training:	Epoch: [77][65/233]	Loss 0.0035 (0.0141)	
training:	Epoch: [77][66/233]	Loss 0.0020 (0.0139)	
training:	Epoch: [77][67/233]	Loss 0.0017 (0.0137)	
training:	Epoch: [77][68/233]	Loss 0.0025 (0.0136)	
training:	Epoch: [77][69/233]	Loss 0.0018 (0.0134)	
training:	Epoch: [77][70/233]	Loss 0.0016 (0.0132)	
training:	Epoch: [77][71/233]	Loss 0.0016 (0.0131)	
training:	Epoch: [77][72/233]	Loss 0.0029 (0.0129)	
training:	Epoch: [77][73/233]	Loss 0.0022 (0.0128)	
training:	Epoch: [77][74/233]	Loss 0.0022 (0.0126)	
training:	Epoch: [77][75/233]	Loss 0.0093 (0.0126)	
training:	Epoch: [77][76/233]	Loss 0.0413 (0.0130)	
training:	Epoch: [77][77/233]	Loss 0.0017 (0.0128)	
training:	Epoch: [77][78/233]	Loss 0.0035 (0.0127)	
training:	Epoch: [77][79/233]	Loss 0.2016 (0.0151)	
training:	Epoch: [77][80/233]	Loss 0.0016 (0.0149)	
training:	Epoch: [77][81/233]	Loss 0.1082 (0.0161)	
training:	Epoch: [77][82/233]	Loss 0.0018 (0.0159)	
training:	Epoch: [77][83/233]	Loss 0.0029 (0.0158)	
training:	Epoch: [77][84/233]	Loss 0.0788 (0.0165)	
training:	Epoch: [77][85/233]	Loss 0.0017 (0.0163)	
training:	Epoch: [77][86/233]	Loss 0.0161 (0.0163)	
training:	Epoch: [77][87/233]	Loss 0.0084 (0.0162)	
training:	Epoch: [77][88/233]	Loss 0.0017 (0.0161)	
training:	Epoch: [77][89/233]	Loss 0.0018 (0.0159)	
training:	Epoch: [77][90/233]	Loss 0.0017 (0.0158)	
training:	Epoch: [77][91/233]	Loss 0.0022 (0.0156)	
training:	Epoch: [77][92/233]	Loss 0.0022 (0.0155)	
training:	Epoch: [77][93/233]	Loss 0.0203 (0.0155)	
training:	Epoch: [77][94/233]	Loss 0.0117 (0.0155)	
training:	Epoch: [77][95/233]	Loss 0.0016 (0.0153)	
training:	Epoch: [77][96/233]	Loss 0.0017 (0.0152)	
training:	Epoch: [77][97/233]	Loss 0.0025 (0.0151)	
training:	Epoch: [77][98/233]	Loss 0.0049 (0.0149)	
training:	Epoch: [77][99/233]	Loss 0.0020 (0.0148)	
training:	Epoch: [77][100/233]	Loss 0.0029 (0.0147)	
training:	Epoch: [77][101/233]	Loss 0.0015 (0.0146)	
training:	Epoch: [77][102/233]	Loss 0.0019 (0.0144)	
training:	Epoch: [77][103/233]	Loss 0.0028 (0.0143)	
training:	Epoch: [77][104/233]	Loss 0.0024 (0.0142)	
training:	Epoch: [77][105/233]	Loss 0.0017 (0.0141)	
training:	Epoch: [77][106/233]	Loss 0.0021 (0.0140)	
training:	Epoch: [77][107/233]	Loss 0.0015 (0.0139)	
training:	Epoch: [77][108/233]	Loss 0.0016 (0.0138)	
training:	Epoch: [77][109/233]	Loss 0.0019 (0.0136)	
training:	Epoch: [77][110/233]	Loss 0.2158 (0.0155)	
training:	Epoch: [77][111/233]	Loss 0.0022 (0.0154)	
training:	Epoch: [77][112/233]	Loss 0.0017 (0.0152)	
training:	Epoch: [77][113/233]	Loss 0.0018 (0.0151)	
training:	Epoch: [77][114/233]	Loss 0.0022 (0.0150)	
training:	Epoch: [77][115/233]	Loss 0.0020 (0.0149)	
training:	Epoch: [77][116/233]	Loss 0.0037 (0.0148)	
training:	Epoch: [77][117/233]	Loss 0.0033 (0.0147)	
training:	Epoch: [77][118/233]	Loss 0.0017 (0.0146)	
training:	Epoch: [77][119/233]	Loss 0.0298 (0.0147)	
training:	Epoch: [77][120/233]	Loss 0.0016 (0.0146)	
training:	Epoch: [77][121/233]	Loss 0.0015 (0.0145)	
training:	Epoch: [77][122/233]	Loss 0.0347 (0.0147)	
training:	Epoch: [77][123/233]	Loss 0.0018 (0.0146)	
training:	Epoch: [77][124/233]	Loss 0.0018 (0.0145)	
training:	Epoch: [77][125/233]	Loss 0.0016 (0.0144)	
training:	Epoch: [77][126/233]	Loss 0.0024 (0.0143)	
training:	Epoch: [77][127/233]	Loss 0.0020 (0.0142)	
training:	Epoch: [77][128/233]	Loss 0.0017 (0.0141)	
training:	Epoch: [77][129/233]	Loss 0.0019 (0.0140)	
training:	Epoch: [77][130/233]	Loss 0.0019 (0.0139)	
training:	Epoch: [77][131/233]	Loss 0.0054 (0.0138)	
training:	Epoch: [77][132/233]	Loss 0.0024 (0.0137)	
training:	Epoch: [77][133/233]	Loss 0.2012 (0.0151)	
training:	Epoch: [77][134/233]	Loss 0.0017 (0.0150)	
training:	Epoch: [77][135/233]	Loss 0.0248 (0.0151)	
training:	Epoch: [77][136/233]	Loss 0.0409 (0.0153)	
training:	Epoch: [77][137/233]	Loss 0.0032 (0.0152)	
training:	Epoch: [77][138/233]	Loss 0.0055 (0.0151)	
training:	Epoch: [77][139/233]	Loss 0.0016 (0.0150)	
training:	Epoch: [77][140/233]	Loss 0.0023 (0.0150)	
training:	Epoch: [77][141/233]	Loss 0.0021 (0.0149)	
training:	Epoch: [77][142/233]	Loss 0.0071 (0.0148)	
training:	Epoch: [77][143/233]	Loss 0.0016 (0.0147)	
training:	Epoch: [77][144/233]	Loss 0.0016 (0.0146)	
training:	Epoch: [77][145/233]	Loss 0.0016 (0.0145)	
training:	Epoch: [77][146/233]	Loss 0.0015 (0.0144)	
training:	Epoch: [77][147/233]	Loss 0.0032 (0.0144)	
training:	Epoch: [77][148/233]	Loss 0.0014 (0.0143)	
training:	Epoch: [77][149/233]	Loss 0.0036 (0.0142)	
training:	Epoch: [77][150/233]	Loss 0.0032 (0.0141)	
training:	Epoch: [77][151/233]	Loss 0.0015 (0.0140)	
training:	Epoch: [77][152/233]	Loss 0.0016 (0.0140)	
training:	Epoch: [77][153/233]	Loss 0.0017 (0.0139)	
training:	Epoch: [77][154/233]	Loss 0.0014 (0.0138)	
training:	Epoch: [77][155/233]	Loss 0.0029 (0.0137)	
training:	Epoch: [77][156/233]	Loss 0.0018 (0.0137)	
training:	Epoch: [77][157/233]	Loss 0.0018 (0.0136)	
training:	Epoch: [77][158/233]	Loss 0.0041 (0.0135)	
training:	Epoch: [77][159/233]	Loss 0.0025 (0.0135)	
training:	Epoch: [77][160/233]	Loss 0.0022 (0.0134)	
training:	Epoch: [77][161/233]	Loss 0.0018 (0.0133)	
training:	Epoch: [77][162/233]	Loss 0.0019 (0.0132)	
training:	Epoch: [77][163/233]	Loss 0.0033 (0.0132)	
training:	Epoch: [77][164/233]	Loss 0.0017 (0.0131)	
training:	Epoch: [77][165/233]	Loss 0.0782 (0.0135)	
training:	Epoch: [77][166/233]	Loss 0.0025 (0.0134)	
training:	Epoch: [77][167/233]	Loss 0.0021 (0.0134)	
training:	Epoch: [77][168/233]	Loss 0.0066 (0.0133)	
training:	Epoch: [77][169/233]	Loss 0.0018 (0.0133)	
training:	Epoch: [77][170/233]	Loss 0.0019 (0.0132)	
training:	Epoch: [77][171/233]	Loss 0.0042 (0.0131)	
training:	Epoch: [77][172/233]	Loss 0.0021 (0.0131)	
training:	Epoch: [77][173/233]	Loss 0.0017 (0.0130)	
training:	Epoch: [77][174/233]	Loss 0.0026 (0.0130)	
training:	Epoch: [77][175/233]	Loss 0.0278 (0.0130)	
training:	Epoch: [77][176/233]	Loss 0.0018 (0.0130)	
training:	Epoch: [77][177/233]	Loss 0.0018 (0.0129)	
training:	Epoch: [77][178/233]	Loss 0.0069 (0.0129)	
training:	Epoch: [77][179/233]	Loss 0.0049 (0.0128)	
training:	Epoch: [77][180/233]	Loss 0.0531 (0.0131)	
training:	Epoch: [77][181/233]	Loss 0.0035 (0.0130)	
training:	Epoch: [77][182/233]	Loss 0.0018 (0.0129)	
training:	Epoch: [77][183/233]	Loss 0.0017 (0.0129)	
training:	Epoch: [77][184/233]	Loss 0.0777 (0.0132)	
training:	Epoch: [77][185/233]	Loss 0.0019 (0.0132)	
training:	Epoch: [77][186/233]	Loss 0.0017 (0.0131)	
training:	Epoch: [77][187/233]	Loss 0.0018 (0.0130)	
training:	Epoch: [77][188/233]	Loss 0.0018 (0.0130)	
training:	Epoch: [77][189/233]	Loss 0.0016 (0.0129)	
training:	Epoch: [77][190/233]	Loss 0.0017 (0.0129)	
training:	Epoch: [77][191/233]	Loss 0.0019 (0.0128)	
training:	Epoch: [77][192/233]	Loss 0.0019 (0.0128)	
training:	Epoch: [77][193/233]	Loss 0.0042 (0.0127)	
training:	Epoch: [77][194/233]	Loss 0.0015 (0.0127)	
training:	Epoch: [77][195/233]	Loss 0.0022 (0.0126)	
training:	Epoch: [77][196/233]	Loss 0.0016 (0.0125)	
training:	Epoch: [77][197/233]	Loss 0.0015 (0.0125)	
training:	Epoch: [77][198/233]	Loss 0.0024 (0.0124)	
training:	Epoch: [77][199/233]	Loss 0.0228 (0.0125)	
training:	Epoch: [77][200/233]	Loss 0.0040 (0.0124)	
training:	Epoch: [77][201/233]	Loss 0.0018 (0.0124)	
training:	Epoch: [77][202/233]	Loss 0.0076 (0.0124)	
training:	Epoch: [77][203/233]	Loss 0.0203 (0.0124)	
training:	Epoch: [77][204/233]	Loss 0.0021 (0.0124)	
training:	Epoch: [77][205/233]	Loss 0.0017 (0.0123)	
training:	Epoch: [77][206/233]	Loss 0.0021 (0.0123)	
training:	Epoch: [77][207/233]	Loss 0.0015 (0.0122)	
training:	Epoch: [77][208/233]	Loss 0.0040 (0.0122)	
training:	Epoch: [77][209/233]	Loss 0.0019 (0.0121)	
training:	Epoch: [77][210/233]	Loss 0.0019 (0.0121)	
training:	Epoch: [77][211/233]	Loss 0.1778 (0.0129)	
training:	Epoch: [77][212/233]	Loss 0.0016 (0.0128)	
training:	Epoch: [77][213/233]	Loss 0.0058 (0.0128)	
training:	Epoch: [77][214/233]	Loss 0.0034 (0.0127)	
training:	Epoch: [77][215/233]	Loss 0.0017 (0.0127)	
training:	Epoch: [77][216/233]	Loss 0.0015 (0.0126)	
training:	Epoch: [77][217/233]	Loss 0.0024 (0.0126)	
training:	Epoch: [77][218/233]	Loss 0.0014 (0.0125)	
training:	Epoch: [77][219/233]	Loss 0.0019 (0.0125)	
training:	Epoch: [77][220/233]	Loss 0.0124 (0.0125)	
training:	Epoch: [77][221/233]	Loss 0.0159 (0.0125)	
training:	Epoch: [77][222/233]	Loss 0.0017 (0.0124)	
training:	Epoch: [77][223/233]	Loss 0.0022 (0.0124)	
training:	Epoch: [77][224/233]	Loss 0.0035 (0.0124)	
training:	Epoch: [77][225/233]	Loss 0.0017 (0.0123)	
training:	Epoch: [77][226/233]	Loss 0.0019 (0.0123)	
training:	Epoch: [77][227/233]	Loss 0.0015 (0.0122)	
training:	Epoch: [77][228/233]	Loss 0.1043 (0.0126)	
training:	Epoch: [77][229/233]	Loss 0.0048 (0.0126)	
training:	Epoch: [77][230/233]	Loss 0.0048 (0.0126)	
training:	Epoch: [77][231/233]	Loss 0.0055 (0.0125)	
training:	Epoch: [77][232/233]	Loss 0.0016 (0.0125)	
training:	Epoch: [77][233/233]	Loss 0.0032 (0.0124)	
Training:	 Loss: 0.0124

Training:	 ACC: 0.9993 0.9993 0.9995 0.9992
Validation:	 ACC: 0.7966 0.7972 0.8100 0.7831
Validation:	 Best_BACC: 0.8162 0.8154 0.7998 0.8326
Validation:	 Loss: 1.0280
Pretraining:	Epoch 78/200
----------
training:	Epoch: [78][1/233]	Loss 0.0021 (0.0021)	
training:	Epoch: [78][2/233]	Loss 0.0077 (0.0049)	
training:	Epoch: [78][3/233]	Loss 0.0027 (0.0042)	
training:	Epoch: [78][4/233]	Loss 0.0030 (0.0039)	
training:	Epoch: [78][5/233]	Loss 0.0055 (0.0042)	
training:	Epoch: [78][6/233]	Loss 0.0310 (0.0087)	
training:	Epoch: [78][7/233]	Loss 0.0015 (0.0076)	
training:	Epoch: [78][8/233]	Loss 0.0092 (0.0078)	
training:	Epoch: [78][9/233]	Loss 0.0021 (0.0072)	
training:	Epoch: [78][10/233]	Loss 0.0017 (0.0066)	
training:	Epoch: [78][11/233]	Loss 0.0016 (0.0062)	
training:	Epoch: [78][12/233]	Loss 0.0637 (0.0110)	
training:	Epoch: [78][13/233]	Loss 0.0133 (0.0112)	
training:	Epoch: [78][14/233]	Loss 0.0018 (0.0105)	
training:	Epoch: [78][15/233]	Loss 0.0014 (0.0099)	
training:	Epoch: [78][16/233]	Loss 0.0041 (0.0095)	
training:	Epoch: [78][17/233]	Loss 0.0017 (0.0091)	
training:	Epoch: [78][18/233]	Loss 0.0027 (0.0087)	
training:	Epoch: [78][19/233]	Loss 0.0015 (0.0083)	
training:	Epoch: [78][20/233]	Loss 0.0067 (0.0083)	
training:	Epoch: [78][21/233]	Loss 0.0038 (0.0080)	
training:	Epoch: [78][22/233]	Loss 0.1578 (0.0148)	
training:	Epoch: [78][23/233]	Loss 0.0015 (0.0143)	
training:	Epoch: [78][24/233]	Loss 0.0018 (0.0138)	
training:	Epoch: [78][25/233]	Loss 0.0048 (0.0134)	
training:	Epoch: [78][26/233]	Loss 0.0016 (0.0129)	
training:	Epoch: [78][27/233]	Loss 0.0231 (0.0133)	
training:	Epoch: [78][28/233]	Loss 0.0016 (0.0129)	
training:	Epoch: [78][29/233]	Loss 0.0032 (0.0126)	
training:	Epoch: [78][30/233]	Loss 0.0112 (0.0125)	
training:	Epoch: [78][31/233]	Loss 0.0018 (0.0122)	
training:	Epoch: [78][32/233]	Loss 0.0016 (0.0118)	
training:	Epoch: [78][33/233]	Loss 0.0017 (0.0115)	
training:	Epoch: [78][34/233]	Loss 0.0019 (0.0113)	
training:	Epoch: [78][35/233]	Loss 0.0043 (0.0111)	
training:	Epoch: [78][36/233]	Loss 0.0037 (0.0108)	
training:	Epoch: [78][37/233]	Loss 0.0019 (0.0106)	
training:	Epoch: [78][38/233]	Loss 0.0021 (0.0104)	
training:	Epoch: [78][39/233]	Loss 0.0015 (0.0102)	
training:	Epoch: [78][40/233]	Loss 0.0019 (0.0100)	
training:	Epoch: [78][41/233]	Loss 0.0018 (0.0098)	
training:	Epoch: [78][42/233]	Loss 0.0018 (0.0096)	
training:	Epoch: [78][43/233]	Loss 0.0018 (0.0094)	
training:	Epoch: [78][44/233]	Loss 0.0035 (0.0092)	
training:	Epoch: [78][45/233]	Loss 0.0016 (0.0091)	
training:	Epoch: [78][46/233]	Loss 0.1137 (0.0114)	
training:	Epoch: [78][47/233]	Loss 0.0018 (0.0111)	
training:	Epoch: [78][48/233]	Loss 0.0017 (0.0110)	
training:	Epoch: [78][49/233]	Loss 0.0018 (0.0108)	
training:	Epoch: [78][50/233]	Loss 0.0028 (0.0106)	
training:	Epoch: [78][51/233]	Loss 0.0062 (0.0105)	
training:	Epoch: [78][52/233]	Loss 0.0017 (0.0103)	
training:	Epoch: [78][53/233]	Loss 0.0018 (0.0102)	
training:	Epoch: [78][54/233]	Loss 0.0015 (0.0100)	
training:	Epoch: [78][55/233]	Loss 0.0016 (0.0099)	
training:	Epoch: [78][56/233]	Loss 0.0015 (0.0097)	
training:	Epoch: [78][57/233]	Loss 0.0090 (0.0097)	
training:	Epoch: [78][58/233]	Loss 0.0017 (0.0096)	
training:	Epoch: [78][59/233]	Loss 0.0622 (0.0105)	
training:	Epoch: [78][60/233]	Loss 0.0079 (0.0104)	
training:	Epoch: [78][61/233]	Loss 0.0263 (0.0107)	
training:	Epoch: [78][62/233]	Loss 0.0022 (0.0105)	
training:	Epoch: [78][63/233]	Loss 0.0048 (0.0105)	
training:	Epoch: [78][64/233]	Loss 0.0016 (0.0103)	
training:	Epoch: [78][65/233]	Loss 0.0016 (0.0102)	
training:	Epoch: [78][66/233]	Loss 0.0047 (0.0101)	
training:	Epoch: [78][67/233]	Loss 0.0073 (0.0101)	
training:	Epoch: [78][68/233]	Loss 0.0016 (0.0099)	
training:	Epoch: [78][69/233]	Loss 0.0017 (0.0098)	
training:	Epoch: [78][70/233]	Loss 0.0019 (0.0097)	
training:	Epoch: [78][71/233]	Loss 0.0018 (0.0096)	
training:	Epoch: [78][72/233]	Loss 0.1419 (0.0114)	
training:	Epoch: [78][73/233]	Loss 0.0020 (0.0113)	
training:	Epoch: [78][74/233]	Loss 0.0028 (0.0112)	
training:	Epoch: [78][75/233]	Loss 0.0028 (0.0111)	
training:	Epoch: [78][76/233]	Loss 0.0018 (0.0110)	
training:	Epoch: [78][77/233]	Loss 0.0015 (0.0108)	
training:	Epoch: [78][78/233]	Loss 0.0022 (0.0107)	
training:	Epoch: [78][79/233]	Loss 0.0020 (0.0106)	
training:	Epoch: [78][80/233]	Loss 0.0049 (0.0105)	
training:	Epoch: [78][81/233]	Loss 0.0020 (0.0104)	
training:	Epoch: [78][82/233]	Loss 0.0048 (0.0104)	
training:	Epoch: [78][83/233]	Loss 0.0028 (0.0103)	
training:	Epoch: [78][84/233]	Loss 0.0018 (0.0102)	
training:	Epoch: [78][85/233]	Loss 0.0021 (0.0101)	
training:	Epoch: [78][86/233]	Loss 0.0021 (0.0100)	
training:	Epoch: [78][87/233]	Loss 0.0074 (0.0100)	
training:	Epoch: [78][88/233]	Loss 0.2008 (0.0121)	
training:	Epoch: [78][89/233]	Loss 0.0020 (0.0120)	
training:	Epoch: [78][90/233]	Loss 0.0017 (0.0119)	
training:	Epoch: [78][91/233]	Loss 0.0020 (0.0118)	
training:	Epoch: [78][92/233]	Loss 0.0039 (0.0117)	
training:	Epoch: [78][93/233]	Loss 0.0034 (0.0116)	
training:	Epoch: [78][94/233]	Loss 0.0036 (0.0115)	
training:	Epoch: [78][95/233]	Loss 0.0019 (0.0114)	
training:	Epoch: [78][96/233]	Loss 0.0014 (0.0113)	
training:	Epoch: [78][97/233]	Loss 0.0094 (0.0113)	
training:	Epoch: [78][98/233]	Loss 0.0022 (0.0112)	
training:	Epoch: [78][99/233]	Loss 0.0019 (0.0111)	
training:	Epoch: [78][100/233]	Loss 0.0022 (0.0110)	
training:	Epoch: [78][101/233]	Loss 0.0021 (0.0109)	
training:	Epoch: [78][102/233]	Loss 0.0018 (0.0108)	
training:	Epoch: [78][103/233]	Loss 0.0014 (0.0108)	
training:	Epoch: [78][104/233]	Loss 0.0016 (0.0107)	
training:	Epoch: [78][105/233]	Loss 0.0014 (0.0106)	
training:	Epoch: [78][106/233]	Loss 0.0124 (0.0106)	
training:	Epoch: [78][107/233]	Loss 0.0021 (0.0105)	
training:	Epoch: [78][108/233]	Loss 0.0059 (0.0105)	
training:	Epoch: [78][109/233]	Loss 0.0023 (0.0104)	
training:	Epoch: [78][110/233]	Loss 0.0262 (0.0105)	
training:	Epoch: [78][111/233]	Loss 0.0018 (0.0105)	
training:	Epoch: [78][112/233]	Loss 0.0612 (0.0109)	
training:	Epoch: [78][113/233]	Loss 0.0028 (0.0108)	
training:	Epoch: [78][114/233]	Loss 0.0026 (0.0108)	
training:	Epoch: [78][115/233]	Loss 0.0021 (0.0107)	
training:	Epoch: [78][116/233]	Loss 0.0026 (0.0106)	
training:	Epoch: [78][117/233]	Loss 0.0020 (0.0106)	
training:	Epoch: [78][118/233]	Loss 0.0018 (0.0105)	
training:	Epoch: [78][119/233]	Loss 0.0074 (0.0105)	
training:	Epoch: [78][120/233]	Loss 0.0015 (0.0104)	
training:	Epoch: [78][121/233]	Loss 0.0024 (0.0103)	
training:	Epoch: [78][122/233]	Loss 0.0238 (0.0104)	
training:	Epoch: [78][123/233]	Loss 0.0018 (0.0104)	
training:	Epoch: [78][124/233]	Loss 0.0067 (0.0103)	
training:	Epoch: [78][125/233]	Loss 0.0016 (0.0103)	
training:	Epoch: [78][126/233]	Loss 0.0017 (0.0102)	
training:	Epoch: [78][127/233]	Loss 0.0024 (0.0101)	
training:	Epoch: [78][128/233]	Loss 0.0018 (0.0101)	
training:	Epoch: [78][129/233]	Loss 0.0067 (0.0100)	
training:	Epoch: [78][130/233]	Loss 0.0015 (0.0100)	
training:	Epoch: [78][131/233]	Loss 0.0017 (0.0099)	
training:	Epoch: [78][132/233]	Loss 0.0021 (0.0098)	
training:	Epoch: [78][133/233]	Loss 0.0022 (0.0098)	
training:	Epoch: [78][134/233]	Loss 0.1746 (0.0110)	
training:	Epoch: [78][135/233]	Loss 0.0015 (0.0109)	
training:	Epoch: [78][136/233]	Loss 0.0027 (0.0109)	
training:	Epoch: [78][137/233]	Loss 0.0026 (0.0108)	
training:	Epoch: [78][138/233]	Loss 0.0014 (0.0108)	
training:	Epoch: [78][139/233]	Loss 0.0021 (0.0107)	
training:	Epoch: [78][140/233]	Loss 0.0015 (0.0106)	
training:	Epoch: [78][141/233]	Loss 0.0026 (0.0106)	
training:	Epoch: [78][142/233]	Loss 0.0017 (0.0105)	
training:	Epoch: [78][143/233]	Loss 0.0036 (0.0105)	
training:	Epoch: [78][144/233]	Loss 0.0015 (0.0104)	
training:	Epoch: [78][145/233]	Loss 0.0247 (0.0105)	
training:	Epoch: [78][146/233]	Loss 0.0015 (0.0104)	
training:	Epoch: [78][147/233]	Loss 0.0016 (0.0104)	
training:	Epoch: [78][148/233]	Loss 0.0024 (0.0103)	
training:	Epoch: [78][149/233]	Loss 0.0020 (0.0103)	
training:	Epoch: [78][150/233]	Loss 0.0015 (0.0102)	
training:	Epoch: [78][151/233]	Loss 0.0014 (0.0101)	
training:	Epoch: [78][152/233]	Loss 0.0019 (0.0101)	
training:	Epoch: [78][153/233]	Loss 0.2043 (0.0114)	
training:	Epoch: [78][154/233]	Loss 0.0025 (0.0113)	
training:	Epoch: [78][155/233]	Loss 0.0018 (0.0112)	
training:	Epoch: [78][156/233]	Loss 0.0045 (0.0112)	
training:	Epoch: [78][157/233]	Loss 0.0025 (0.0111)	
training:	Epoch: [78][158/233]	Loss 0.0034 (0.0111)	
training:	Epoch: [78][159/233]	Loss 0.0028 (0.0110)	
training:	Epoch: [78][160/233]	Loss 0.0015 (0.0110)	
training:	Epoch: [78][161/233]	Loss 0.0027 (0.0109)	
training:	Epoch: [78][162/233]	Loss 0.1845 (0.0120)	
training:	Epoch: [78][163/233]	Loss 0.0016 (0.0119)	
training:	Epoch: [78][164/233]	Loss 0.0017 (0.0119)	
training:	Epoch: [78][165/233]	Loss 0.0014 (0.0118)	
training:	Epoch: [78][166/233]	Loss 0.0018 (0.0118)	
training:	Epoch: [78][167/233]	Loss 0.2034 (0.0129)	
training:	Epoch: [78][168/233]	Loss 0.0025 (0.0128)	
training:	Epoch: [78][169/233]	Loss 0.0018 (0.0128)	
training:	Epoch: [78][170/233]	Loss 0.0017 (0.0127)	
training:	Epoch: [78][171/233]	Loss 0.0016 (0.0126)	
training:	Epoch: [78][172/233]	Loss 0.0040 (0.0126)	
training:	Epoch: [78][173/233]	Loss 0.0050 (0.0126)	
training:	Epoch: [78][174/233]	Loss 0.0024 (0.0125)	
training:	Epoch: [78][175/233]	Loss 0.0027 (0.0124)	
training:	Epoch: [78][176/233]	Loss 0.0027 (0.0124)	
training:	Epoch: [78][177/233]	Loss 0.0016 (0.0123)	
training:	Epoch: [78][178/233]	Loss 0.0016 (0.0123)	
training:	Epoch: [78][179/233]	Loss 0.0020 (0.0122)	
training:	Epoch: [78][180/233]	Loss 0.0026 (0.0121)	
training:	Epoch: [78][181/233]	Loss 0.0290 (0.0122)	
training:	Epoch: [78][182/233]	Loss 0.0021 (0.0122)	
training:	Epoch: [78][183/233]	Loss 0.0019 (0.0121)	
training:	Epoch: [78][184/233]	Loss 0.0034 (0.0121)	
training:	Epoch: [78][185/233]	Loss 0.0022 (0.0120)	
training:	Epoch: [78][186/233]	Loss 0.0019 (0.0120)	
training:	Epoch: [78][187/233]	Loss 0.0055 (0.0119)	
training:	Epoch: [78][188/233]	Loss 0.0016 (0.0119)	
training:	Epoch: [78][189/233]	Loss 0.0019 (0.0118)	
training:	Epoch: [78][190/233]	Loss 0.0017 (0.0118)	
training:	Epoch: [78][191/233]	Loss 0.0022 (0.0117)	
training:	Epoch: [78][192/233]	Loss 0.0016 (0.0117)	
training:	Epoch: [78][193/233]	Loss 0.0041 (0.0116)	
training:	Epoch: [78][194/233]	Loss 0.0033 (0.0116)	
training:	Epoch: [78][195/233]	Loss 0.0057 (0.0116)	
training:	Epoch: [78][196/233]	Loss 0.0018 (0.0115)	
training:	Epoch: [78][197/233]	Loss 0.2158 (0.0126)	
training:	Epoch: [78][198/233]	Loss 0.0040 (0.0125)	
training:	Epoch: [78][199/233]	Loss 0.3466 (0.0142)	
training:	Epoch: [78][200/233]	Loss 0.0051 (0.0141)	
training:	Epoch: [78][201/233]	Loss 0.0016 (0.0141)	
training:	Epoch: [78][202/233]	Loss 0.0152 (0.0141)	
training:	Epoch: [78][203/233]	Loss 0.0014 (0.0140)	
training:	Epoch: [78][204/233]	Loss 0.0073 (0.0140)	
training:	Epoch: [78][205/233]	Loss 0.0015 (0.0139)	
training:	Epoch: [78][206/233]	Loss 0.0019 (0.0139)	
training:	Epoch: [78][207/233]	Loss 0.0019 (0.0138)	
training:	Epoch: [78][208/233]	Loss 0.0014 (0.0138)	
training:	Epoch: [78][209/233]	Loss 0.0016 (0.0137)	
training:	Epoch: [78][210/233]	Loss 0.0014 (0.0136)	
training:	Epoch: [78][211/233]	Loss 0.0018 (0.0136)	
training:	Epoch: [78][212/233]	Loss 0.0021 (0.0135)	
training:	Epoch: [78][213/233]	Loss 0.0019 (0.0135)	
training:	Epoch: [78][214/233]	Loss 0.0014 (0.0134)	
training:	Epoch: [78][215/233]	Loss 0.0696 (0.0137)	
training:	Epoch: [78][216/233]	Loss 0.0019 (0.0136)	
training:	Epoch: [78][217/233]	Loss 0.0022 (0.0136)	
training:	Epoch: [78][218/233]	Loss 0.0040 (0.0135)	
training:	Epoch: [78][219/233]	Loss 0.0019 (0.0135)	
training:	Epoch: [78][220/233]	Loss 0.0017 (0.0134)	
training:	Epoch: [78][221/233]	Loss 0.0015 (0.0134)	
training:	Epoch: [78][222/233]	Loss 0.0038 (0.0133)	
training:	Epoch: [78][223/233]	Loss 0.0018 (0.0133)	
training:	Epoch: [78][224/233]	Loss 0.0024 (0.0132)	
training:	Epoch: [78][225/233]	Loss 0.0017 (0.0132)	
training:	Epoch: [78][226/233]	Loss 0.0016 (0.0131)	
training:	Epoch: [78][227/233]	Loss 0.0022 (0.0131)	
training:	Epoch: [78][228/233]	Loss 0.0016 (0.0130)	
training:	Epoch: [78][229/233]	Loss 0.0056 (0.0130)	
training:	Epoch: [78][230/233]	Loss 0.0043 (0.0129)	
training:	Epoch: [78][231/233]	Loss 0.0026 (0.0129)	
training:	Epoch: [78][232/233]	Loss 0.0016 (0.0129)	
training:	Epoch: [78][233/233]	Loss 0.0023 (0.0128)	
Training:	 Loss: 0.0128

Training:	 ACC: 0.9993 0.9993 0.9995 0.9992
Validation:	 ACC: 0.7952 0.7945 0.7814 0.8090
Validation:	 Best_BACC: 0.8162 0.8154 0.7998 0.8326
Validation:	 Loss: 1.0213
Pretraining:	Epoch 79/200
----------
training:	Epoch: [79][1/233]	Loss 0.0014 (0.0014)	
training:	Epoch: [79][2/233]	Loss 0.0017 (0.0016)	
training:	Epoch: [79][3/233]	Loss 0.0015 (0.0015)	
training:	Epoch: [79][4/233]	Loss 0.0022 (0.0017)	
training:	Epoch: [79][5/233]	Loss 0.0016 (0.0017)	
training:	Epoch: [79][6/233]	Loss 0.0022 (0.0018)	
training:	Epoch: [79][7/233]	Loss 0.0016 (0.0017)	
training:	Epoch: [79][8/233]	Loss 0.0014 (0.0017)	
training:	Epoch: [79][9/233]	Loss 0.0020 (0.0017)	
training:	Epoch: [79][10/233]	Loss 0.0031 (0.0019)	
training:	Epoch: [79][11/233]	Loss 0.0019 (0.0019)	
training:	Epoch: [79][12/233]	Loss 0.0282 (0.0041)	
training:	Epoch: [79][13/233]	Loss 0.1315 (0.0139)	
training:	Epoch: [79][14/233]	Loss 0.0016 (0.0130)	
training:	Epoch: [79][15/233]	Loss 0.0015 (0.0122)	
training:	Epoch: [79][16/233]	Loss 0.0017 (0.0116)	
training:	Epoch: [79][17/233]	Loss 0.0015 (0.0110)	
training:	Epoch: [79][18/233]	Loss 0.0014 (0.0104)	
training:	Epoch: [79][19/233]	Loss 0.0063 (0.0102)	
training:	Epoch: [79][20/233]	Loss 0.0015 (0.0098)	
training:	Epoch: [79][21/233]	Loss 0.0016 (0.0094)	
training:	Epoch: [79][22/233]	Loss 0.0017 (0.0091)	
training:	Epoch: [79][23/233]	Loss 0.0019 (0.0087)	
training:	Epoch: [79][24/233]	Loss 0.0013 (0.0084)	
training:	Epoch: [79][25/233]	Loss 0.0015 (0.0082)	
training:	Epoch: [79][26/233]	Loss 0.0614 (0.0102)	
training:	Epoch: [79][27/233]	Loss 0.0020 (0.0099)	
training:	Epoch: [79][28/233]	Loss 0.0073 (0.0098)	
training:	Epoch: [79][29/233]	Loss 0.0017 (0.0095)	
training:	Epoch: [79][30/233]	Loss 0.0017 (0.0093)	
training:	Epoch: [79][31/233]	Loss 0.0016 (0.0090)	
training:	Epoch: [79][32/233]	Loss 0.0034 (0.0088)	
training:	Epoch: [79][33/233]	Loss 0.0015 (0.0086)	
training:	Epoch: [79][34/233]	Loss 0.0015 (0.0084)	
training:	Epoch: [79][35/233]	Loss 0.0022 (0.0082)	
training:	Epoch: [79][36/233]	Loss 0.0015 (0.0080)	
training:	Epoch: [79][37/233]	Loss 0.0021 (0.0079)	
training:	Epoch: [79][38/233]	Loss 0.0013 (0.0077)	
training:	Epoch: [79][39/233]	Loss 0.0021 (0.0076)	
training:	Epoch: [79][40/233]	Loss 0.0013 (0.0074)	
training:	Epoch: [79][41/233]	Loss 0.0392 (0.0082)	
training:	Epoch: [79][42/233]	Loss 0.0017 (0.0080)	
training:	Epoch: [79][43/233]	Loss 0.0023 (0.0079)	
training:	Epoch: [79][44/233]	Loss 0.0506 (0.0089)	
training:	Epoch: [79][45/233]	Loss 0.0031 (0.0087)	
training:	Epoch: [79][46/233]	Loss 0.0017 (0.0086)	
training:	Epoch: [79][47/233]	Loss 0.0021 (0.0084)	
training:	Epoch: [79][48/233]	Loss 0.0026 (0.0083)	
training:	Epoch: [79][49/233]	Loss 0.0014 (0.0082)	
training:	Epoch: [79][50/233]	Loss 0.0197 (0.0084)	
training:	Epoch: [79][51/233]	Loss 0.0058 (0.0084)	
training:	Epoch: [79][52/233]	Loss 0.0015 (0.0082)	
training:	Epoch: [79][53/233]	Loss 0.0016 (0.0081)	
training:	Epoch: [79][54/233]	Loss 0.0018 (0.0080)	
training:	Epoch: [79][55/233]	Loss 0.0027 (0.0079)	
training:	Epoch: [79][56/233]	Loss 0.0014 (0.0078)	
training:	Epoch: [79][57/233]	Loss 0.0014 (0.0077)	
training:	Epoch: [79][58/233]	Loss 0.0026 (0.0076)	
training:	Epoch: [79][59/233]	Loss 0.0019 (0.0075)	
training:	Epoch: [79][60/233]	Loss 0.0019 (0.0074)	
training:	Epoch: [79][61/233]	Loss 0.0020 (0.0073)	
training:	Epoch: [79][62/233]	Loss 0.0014 (0.0072)	
training:	Epoch: [79][63/233]	Loss 0.0121 (0.0073)	
training:	Epoch: [79][64/233]	Loss 0.0017 (0.0072)	
training:	Epoch: [79][65/233]	Loss 0.0015 (0.0071)	
training:	Epoch: [79][66/233]	Loss 0.0163 (0.0072)	
training:	Epoch: [79][67/233]	Loss 0.0027 (0.0072)	
training:	Epoch: [79][68/233]	Loss 0.0140 (0.0073)	
training:	Epoch: [79][69/233]	Loss 0.0131 (0.0074)	
training:	Epoch: [79][70/233]	Loss 0.0080 (0.0074)	
training:	Epoch: [79][71/233]	Loss 0.0017 (0.0073)	
training:	Epoch: [79][72/233]	Loss 0.0028 (0.0072)	
training:	Epoch: [79][73/233]	Loss 0.2013 (0.0099)	
training:	Epoch: [79][74/233]	Loss 0.0022 (0.0098)	
training:	Epoch: [79][75/233]	Loss 0.2035 (0.0124)	
training:	Epoch: [79][76/233]	Loss 0.0015 (0.0122)	
training:	Epoch: [79][77/233]	Loss 0.0021 (0.0121)	
training:	Epoch: [79][78/233]	Loss 0.0014 (0.0120)	
training:	Epoch: [79][79/233]	Loss 0.0017 (0.0118)	
training:	Epoch: [79][80/233]	Loss 0.0809 (0.0127)	
training:	Epoch: [79][81/233]	Loss 0.0017 (0.0126)	
training:	Epoch: [79][82/233]	Loss 0.0013 (0.0124)	
training:	Epoch: [79][83/233]	Loss 0.0020 (0.0123)	
training:	Epoch: [79][84/233]	Loss 0.0017 (0.0122)	
training:	Epoch: [79][85/233]	Loss 0.0022 (0.0120)	
training:	Epoch: [79][86/233]	Loss 0.0018 (0.0119)	
training:	Epoch: [79][87/233]	Loss 0.0241 (0.0121)	
training:	Epoch: [79][88/233]	Loss 0.0017 (0.0120)	
training:	Epoch: [79][89/233]	Loss 0.0031 (0.0119)	
training:	Epoch: [79][90/233]	Loss 0.0035 (0.0118)	
training:	Epoch: [79][91/233]	Loss 0.0020 (0.0117)	
training:	Epoch: [79][92/233]	Loss 0.0019 (0.0115)	
training:	Epoch: [79][93/233]	Loss 0.0221 (0.0117)	
training:	Epoch: [79][94/233]	Loss 0.0244 (0.0118)	
training:	Epoch: [79][95/233]	Loss 0.0028 (0.0117)	
training:	Epoch: [79][96/233]	Loss 0.0014 (0.0116)	
training:	Epoch: [79][97/233]	Loss 0.0020 (0.0115)	
training:	Epoch: [79][98/233]	Loss 0.0016 (0.0114)	
training:	Epoch: [79][99/233]	Loss 0.0027 (0.0113)	
training:	Epoch: [79][100/233]	Loss 0.0031 (0.0112)	
training:	Epoch: [79][101/233]	Loss 0.0014 (0.0111)	
training:	Epoch: [79][102/233]	Loss 0.0065 (0.0111)	
training:	Epoch: [79][103/233]	Loss 0.0013 (0.0110)	
training:	Epoch: [79][104/233]	Loss 0.0025 (0.0109)	
training:	Epoch: [79][105/233]	Loss 0.0019 (0.0108)	
training:	Epoch: [79][106/233]	Loss 0.0065 (0.0108)	
training:	Epoch: [79][107/233]	Loss 0.0028 (0.0107)	
training:	Epoch: [79][108/233]	Loss 0.0254 (0.0108)	
training:	Epoch: [79][109/233]	Loss 0.0014 (0.0108)	
training:	Epoch: [79][110/233]	Loss 0.0014 (0.0107)	
training:	Epoch: [79][111/233]	Loss 0.0017 (0.0106)	
training:	Epoch: [79][112/233]	Loss 0.0071 (0.0106)	
training:	Epoch: [79][113/233]	Loss 0.0080 (0.0105)	
training:	Epoch: [79][114/233]	Loss 0.0024 (0.0105)	
training:	Epoch: [79][115/233]	Loss 0.0020 (0.0104)	
training:	Epoch: [79][116/233]	Loss 0.0017 (0.0103)	
training:	Epoch: [79][117/233]	Loss 0.0131 (0.0103)	
training:	Epoch: [79][118/233]	Loss 0.0456 (0.0106)	
training:	Epoch: [79][119/233]	Loss 0.0016 (0.0106)	
training:	Epoch: [79][120/233]	Loss 0.0031 (0.0105)	
training:	Epoch: [79][121/233]	Loss 0.0018 (0.0104)	
training:	Epoch: [79][122/233]	Loss 0.0025 (0.0104)	
training:	Epoch: [79][123/233]	Loss 0.0015 (0.0103)	
training:	Epoch: [79][124/233]	Loss 0.0030 (0.0102)	
training:	Epoch: [79][125/233]	Loss 0.0018 (0.0102)	
training:	Epoch: [79][126/233]	Loss 0.1027 (0.0109)	
training:	Epoch: [79][127/233]	Loss 0.0030 (0.0108)	
training:	Epoch: [79][128/233]	Loss 0.0215 (0.0109)	
training:	Epoch: [79][129/233]	Loss 0.0020 (0.0109)	
training:	Epoch: [79][130/233]	Loss 0.0015 (0.0108)	
training:	Epoch: [79][131/233]	Loss 0.0028 (0.0107)	
training:	Epoch: [79][132/233]	Loss 0.0015 (0.0107)	
training:	Epoch: [79][133/233]	Loss 0.0015 (0.0106)	
training:	Epoch: [79][134/233]	Loss 0.0014 (0.0105)	
training:	Epoch: [79][135/233]	Loss 0.0015 (0.0104)	
training:	Epoch: [79][136/233]	Loss 0.0045 (0.0104)	
training:	Epoch: [79][137/233]	Loss 0.0580 (0.0107)	
training:	Epoch: [79][138/233]	Loss 0.0019 (0.0107)	
training:	Epoch: [79][139/233]	Loss 0.0171 (0.0107)	
training:	Epoch: [79][140/233]	Loss 0.1728 (0.0119)	
training:	Epoch: [79][141/233]	Loss 0.0015 (0.0118)	
training:	Epoch: [79][142/233]	Loss 0.0108 (0.0118)	
training:	Epoch: [79][143/233]	Loss 0.0076 (0.0118)	
training:	Epoch: [79][144/233]	Loss 0.0016 (0.0117)	
training:	Epoch: [79][145/233]	Loss 0.0553 (0.0120)	
training:	Epoch: [79][146/233]	Loss 0.0081 (0.0120)	
training:	Epoch: [79][147/233]	Loss 0.0018 (0.0119)	
training:	Epoch: [79][148/233]	Loss 0.0014 (0.0118)	
training:	Epoch: [79][149/233]	Loss 0.0025 (0.0118)	
training:	Epoch: [79][150/233]	Loss 0.1989 (0.0130)	
training:	Epoch: [79][151/233]	Loss 0.0017 (0.0130)	
training:	Epoch: [79][152/233]	Loss 0.0109 (0.0129)	
training:	Epoch: [79][153/233]	Loss 0.0055 (0.0129)	
training:	Epoch: [79][154/233]	Loss 0.0020 (0.0128)	
training:	Epoch: [79][155/233]	Loss 0.0014 (0.0127)	
training:	Epoch: [79][156/233]	Loss 0.0016 (0.0127)	
training:	Epoch: [79][157/233]	Loss 0.0022 (0.0126)	
training:	Epoch: [79][158/233]	Loss 0.0025 (0.0125)	
training:	Epoch: [79][159/233]	Loss 0.0248 (0.0126)	
training:	Epoch: [79][160/233]	Loss 0.0030 (0.0126)	
training:	Epoch: [79][161/233]	Loss 0.0012 (0.0125)	
training:	Epoch: [79][162/233]	Loss 0.0027 (0.0124)	
training:	Epoch: [79][163/233]	Loss 0.0015 (0.0124)	
training:	Epoch: [79][164/233]	Loss 0.0022 (0.0123)	
training:	Epoch: [79][165/233]	Loss 0.0128 (0.0123)	
training:	Epoch: [79][166/233]	Loss 0.0014 (0.0122)	
training:	Epoch: [79][167/233]	Loss 0.0271 (0.0123)	
training:	Epoch: [79][168/233]	Loss 0.0034 (0.0123)	
training:	Epoch: [79][169/233]	Loss 0.1405 (0.0130)	
training:	Epoch: [79][170/233]	Loss 0.0016 (0.0130)	
training:	Epoch: [79][171/233]	Loss 0.0013 (0.0129)	
training:	Epoch: [79][172/233]	Loss 0.0016 (0.0128)	
training:	Epoch: [79][173/233]	Loss 0.0045 (0.0128)	
training:	Epoch: [79][174/233]	Loss 0.0040 (0.0127)	
training:	Epoch: [79][175/233]	Loss 0.0018 (0.0127)	
training:	Epoch: [79][176/233]	Loss 0.0016 (0.0126)	
training:	Epoch: [79][177/233]	Loss 0.0022 (0.0125)	
training:	Epoch: [79][178/233]	Loss 0.0037 (0.0125)	
training:	Epoch: [79][179/233]	Loss 0.0016 (0.0124)	
training:	Epoch: [79][180/233]	Loss 0.0030 (0.0124)	
training:	Epoch: [79][181/233]	Loss 0.0136 (0.0124)	
training:	Epoch: [79][182/233]	Loss 0.0025 (0.0123)	
training:	Epoch: [79][183/233]	Loss 0.0022 (0.0123)	
training:	Epoch: [79][184/233]	Loss 0.0015 (0.0122)	
training:	Epoch: [79][185/233]	Loss 0.0020 (0.0122)	
training:	Epoch: [79][186/233]	Loss 0.0016 (0.0121)	
training:	Epoch: [79][187/233]	Loss 0.0018 (0.0121)	
training:	Epoch: [79][188/233]	Loss 0.0015 (0.0120)	
training:	Epoch: [79][189/233]	Loss 0.0036 (0.0120)	
training:	Epoch: [79][190/233]	Loss 0.0015 (0.0119)	
training:	Epoch: [79][191/233]	Loss 0.0017 (0.0118)	
training:	Epoch: [79][192/233]	Loss 0.0015 (0.0118)	
training:	Epoch: [79][193/233]	Loss 0.0019 (0.0117)	
training:	Epoch: [79][194/233]	Loss 0.0020 (0.0117)	
training:	Epoch: [79][195/233]	Loss 0.0023 (0.0116)	
training:	Epoch: [79][196/233]	Loss 0.0015 (0.0116)	
training:	Epoch: [79][197/233]	Loss 0.0014 (0.0115)	
training:	Epoch: [79][198/233]	Loss 0.0014 (0.0115)	
training:	Epoch: [79][199/233]	Loss 0.0024 (0.0114)	
training:	Epoch: [79][200/233]	Loss 0.0018 (0.0114)	
training:	Epoch: [79][201/233]	Loss 0.0017 (0.0113)	
training:	Epoch: [79][202/233]	Loss 0.0014 (0.0113)	
training:	Epoch: [79][203/233]	Loss 0.0012 (0.0112)	
training:	Epoch: [79][204/233]	Loss 0.0015 (0.0112)	
training:	Epoch: [79][205/233]	Loss 0.0014 (0.0112)	
training:	Epoch: [79][206/233]	Loss 0.0047 (0.0111)	
training:	Epoch: [79][207/233]	Loss 0.0022 (0.0111)	
training:	Epoch: [79][208/233]	Loss 0.0015 (0.0110)	
training:	Epoch: [79][209/233]	Loss 0.0016 (0.0110)	
training:	Epoch: [79][210/233]	Loss 0.0015 (0.0109)	
training:	Epoch: [79][211/233]	Loss 0.0013 (0.0109)	
training:	Epoch: [79][212/233]	Loss 0.0018 (0.0109)	
training:	Epoch: [79][213/233]	Loss 0.0014 (0.0108)	
training:	Epoch: [79][214/233]	Loss 0.0061 (0.0108)	
training:	Epoch: [79][215/233]	Loss 0.0019 (0.0107)	
training:	Epoch: [79][216/233]	Loss 0.0022 (0.0107)	
training:	Epoch: [79][217/233]	Loss 0.2155 (0.0116)	
training:	Epoch: [79][218/233]	Loss 0.0013 (0.0116)	
training:	Epoch: [79][219/233]	Loss 0.0016 (0.0116)	
training:	Epoch: [79][220/233]	Loss 0.0018 (0.0115)	
training:	Epoch: [79][221/233]	Loss 0.0018 (0.0115)	
training:	Epoch: [79][222/233]	Loss 0.0013 (0.0114)	
training:	Epoch: [79][223/233]	Loss 0.0014 (0.0114)	
training:	Epoch: [79][224/233]	Loss 0.0014 (0.0113)	
training:	Epoch: [79][225/233]	Loss 0.0028 (0.0113)	
training:	Epoch: [79][226/233]	Loss 0.0042 (0.0113)	
training:	Epoch: [79][227/233]	Loss 0.0014 (0.0112)	
training:	Epoch: [79][228/233]	Loss 0.0019 (0.0112)	
training:	Epoch: [79][229/233]	Loss 0.0057 (0.0112)	
training:	Epoch: [79][230/233]	Loss 0.0017 (0.0111)	
training:	Epoch: [79][231/233]	Loss 0.0014 (0.0111)	
training:	Epoch: [79][232/233]	Loss 0.0059 (0.0110)	
training:	Epoch: [79][233/233]	Loss 0.0032 (0.0110)	
Training:	 Loss: 0.0110

Training:	 ACC: 0.9993 0.9993 0.9995 0.9992
Validation:	 ACC: 0.7956 0.7951 0.7855 0.8056
Validation:	 Best_BACC: 0.8162 0.8154 0.7998 0.8326
Validation:	 Loss: 1.0384
Pretraining:	Epoch 80/200
----------
training:	Epoch: [80][1/233]	Loss 0.0017 (0.0017)	
training:	Epoch: [80][2/233]	Loss 0.0016 (0.0016)	
training:	Epoch: [80][3/233]	Loss 0.0017 (0.0016)	
training:	Epoch: [80][4/233]	Loss 0.2161 (0.0553)	
training:	Epoch: [80][5/233]	Loss 0.0015 (0.0445)	
training:	Epoch: [80][6/233]	Loss 0.1775 (0.0667)	
training:	Epoch: [80][7/233]	Loss 0.0019 (0.0574)	
training:	Epoch: [80][8/233]	Loss 0.0015 (0.0504)	
training:	Epoch: [80][9/233]	Loss 0.0086 (0.0458)	
training:	Epoch: [80][10/233]	Loss 0.0022 (0.0414)	
training:	Epoch: [80][11/233]	Loss 0.0048 (0.0381)	
training:	Epoch: [80][12/233]	Loss 0.0237 (0.0369)	
training:	Epoch: [80][13/233]	Loss 0.0015 (0.0342)	
training:	Epoch: [80][14/233]	Loss 0.0021 (0.0319)	
training:	Epoch: [80][15/233]	Loss 0.0027 (0.0299)	
training:	Epoch: [80][16/233]	Loss 0.0018 (0.0282)	
training:	Epoch: [80][17/233]	Loss 0.0015 (0.0266)	
training:	Epoch: [80][18/233]	Loss 0.0028 (0.0253)	
training:	Epoch: [80][19/233]	Loss 0.0014 (0.0240)	
training:	Epoch: [80][20/233]	Loss 0.0016 (0.0229)	
training:	Epoch: [80][21/233]	Loss 0.0014 (0.0219)	
training:	Epoch: [80][22/233]	Loss 0.0014 (0.0210)	
training:	Epoch: [80][23/233]	Loss 0.0016 (0.0201)	
training:	Epoch: [80][24/233]	Loss 0.0017 (0.0193)	
training:	Epoch: [80][25/233]	Loss 0.0013 (0.0186)	
training:	Epoch: [80][26/233]	Loss 0.0014 (0.0180)	
training:	Epoch: [80][27/233]	Loss 0.0017 (0.0174)	
training:	Epoch: [80][28/233]	Loss 0.0084 (0.0170)	
training:	Epoch: [80][29/233]	Loss 0.0020 (0.0165)	
training:	Epoch: [80][30/233]	Loss 0.0015 (0.0160)	
training:	Epoch: [80][31/233]	Loss 0.0526 (0.0172)	
training:	Epoch: [80][32/233]	Loss 0.0019 (0.0167)	
training:	Epoch: [80][33/233]	Loss 0.0015 (0.0163)	
training:	Epoch: [80][34/233]	Loss 0.0239 (0.0165)	
training:	Epoch: [80][35/233]	Loss 0.0017 (0.0161)	
training:	Epoch: [80][36/233]	Loss 0.0015 (0.0157)	
training:	Epoch: [80][37/233]	Loss 0.0015 (0.0153)	
training:	Epoch: [80][38/233]	Loss 0.0013 (0.0149)	
training:	Epoch: [80][39/233]	Loss 0.0013 (0.0146)	
training:	Epoch: [80][40/233]	Loss 0.0013 (0.0142)	
training:	Epoch: [80][41/233]	Loss 0.0029 (0.0140)	
training:	Epoch: [80][42/233]	Loss 0.0064 (0.0138)	
training:	Epoch: [80][43/233]	Loss 0.0018 (0.0135)	
training:	Epoch: [80][44/233]	Loss 0.0018 (0.0132)	
training:	Epoch: [80][45/233]	Loss 0.0035 (0.0130)	
training:	Epoch: [80][46/233]	Loss 0.0015 (0.0128)	
training:	Epoch: [80][47/233]	Loss 0.0014 (0.0125)	
training:	Epoch: [80][48/233]	Loss 0.0019 (0.0123)	
training:	Epoch: [80][49/233]	Loss 0.0118 (0.0123)	
training:	Epoch: [80][50/233]	Loss 0.1834 (0.0157)	
training:	Epoch: [80][51/233]	Loss 0.0042 (0.0155)	
training:	Epoch: [80][52/233]	Loss 0.0286 (0.0157)	
training:	Epoch: [80][53/233]	Loss 0.0015 (0.0155)	
training:	Epoch: [80][54/233]	Loss 0.0016 (0.0152)	
training:	Epoch: [80][55/233]	Loss 0.0349 (0.0156)	
training:	Epoch: [80][56/233]	Loss 0.0015 (0.0153)	
training:	Epoch: [80][57/233]	Loss 0.0041 (0.0151)	
training:	Epoch: [80][58/233]	Loss 0.0032 (0.0149)	
training:	Epoch: [80][59/233]	Loss 0.0016 (0.0147)	
training:	Epoch: [80][60/233]	Loss 0.0014 (0.0145)	
training:	Epoch: [80][61/233]	Loss 0.0329 (0.0148)	
training:	Epoch: [80][62/233]	Loss 0.0022 (0.0146)	
training:	Epoch: [80][63/233]	Loss 0.0013 (0.0144)	
training:	Epoch: [80][64/233]	Loss 0.0014 (0.0142)	
training:	Epoch: [80][65/233]	Loss 0.0017 (0.0140)	
training:	Epoch: [80][66/233]	Loss 0.0014 (0.0138)	
training:	Epoch: [80][67/233]	Loss 0.0013 (0.0136)	
training:	Epoch: [80][68/233]	Loss 0.0013 (0.0134)	
training:	Epoch: [80][69/233]	Loss 0.0031 (0.0133)	
training:	Epoch: [80][70/233]	Loss 0.0018 (0.0131)	
training:	Epoch: [80][71/233]	Loss 0.0014 (0.0129)	
training:	Epoch: [80][72/233]	Loss 0.0016 (0.0128)	
training:	Epoch: [80][73/233]	Loss 0.0037 (0.0126)	
training:	Epoch: [80][74/233]	Loss 0.0016 (0.0125)	
training:	Epoch: [80][75/233]	Loss 0.0015 (0.0124)	
training:	Epoch: [80][76/233]	Loss 0.0023 (0.0122)	
training:	Epoch: [80][77/233]	Loss 0.0014 (0.0121)	
training:	Epoch: [80][78/233]	Loss 0.0021 (0.0119)	
training:	Epoch: [80][79/233]	Loss 0.0035 (0.0118)	
training:	Epoch: [80][80/233]	Loss 0.0015 (0.0117)	
training:	Epoch: [80][81/233]	Loss 0.0015 (0.0116)	
training:	Epoch: [80][82/233]	Loss 0.0015 (0.0115)	
training:	Epoch: [80][83/233]	Loss 0.0089 (0.0114)	
training:	Epoch: [80][84/233]	Loss 0.0016 (0.0113)	
training:	Epoch: [80][85/233]	Loss 0.0015 (0.0112)	
training:	Epoch: [80][86/233]	Loss 0.0014 (0.0111)	
training:	Epoch: [80][87/233]	Loss 0.0026 (0.0110)	
training:	Epoch: [80][88/233]	Loss 0.0013 (0.0109)	
training:	Epoch: [80][89/233]	Loss 0.0027 (0.0108)	
training:	Epoch: [80][90/233]	Loss 0.0016 (0.0107)	
training:	Epoch: [80][91/233]	Loss 0.0065 (0.0106)	
training:	Epoch: [80][92/233]	Loss 0.0015 (0.0105)	
training:	Epoch: [80][93/233]	Loss 0.0352 (0.0108)	
training:	Epoch: [80][94/233]	Loss 0.0015 (0.0107)	
training:	Epoch: [80][95/233]	Loss 0.0014 (0.0106)	
training:	Epoch: [80][96/233]	Loss 0.0014 (0.0105)	
training:	Epoch: [80][97/233]	Loss 0.0021 (0.0104)	
training:	Epoch: [80][98/233]	Loss 0.0014 (0.0103)	
training:	Epoch: [80][99/233]	Loss 0.0016 (0.0102)	
training:	Epoch: [80][100/233]	Loss 0.0021 (0.0102)	
training:	Epoch: [80][101/233]	Loss 0.0021 (0.0101)	
training:	Epoch: [80][102/233]	Loss 0.0015 (0.0100)	
training:	Epoch: [80][103/233]	Loss 0.0014 (0.0099)	
training:	Epoch: [80][104/233]	Loss 0.0038 (0.0099)	
training:	Epoch: [80][105/233]	Loss 0.0045 (0.0098)	
training:	Epoch: [80][106/233]	Loss 0.0021 (0.0097)	
training:	Epoch: [80][107/233]	Loss 0.0033 (0.0097)	
training:	Epoch: [80][108/233]	Loss 0.0019 (0.0096)	
training:	Epoch: [80][109/233]	Loss 0.1268 (0.0107)	
training:	Epoch: [80][110/233]	Loss 0.0018 (0.0106)	
training:	Epoch: [80][111/233]	Loss 0.0030 (0.0105)	
training:	Epoch: [80][112/233]	Loss 0.0014 (0.0104)	
training:	Epoch: [80][113/233]	Loss 0.0018 (0.0104)	
training:	Epoch: [80][114/233]	Loss 0.0016 (0.0103)	
training:	Epoch: [80][115/233]	Loss 0.0013 (0.0102)	
training:	Epoch: [80][116/233]	Loss 0.0016 (0.0101)	
training:	Epoch: [80][117/233]	Loss 0.0017 (0.0101)	
training:	Epoch: [80][118/233]	Loss 0.0014 (0.0100)	
training:	Epoch: [80][119/233]	Loss 0.0016 (0.0099)	
training:	Epoch: [80][120/233]	Loss 0.0027 (0.0099)	
training:	Epoch: [80][121/233]	Loss 0.0021 (0.0098)	
training:	Epoch: [80][122/233]	Loss 0.0013 (0.0097)	
training:	Epoch: [80][123/233]	Loss 0.0017 (0.0097)	
training:	Epoch: [80][124/233]	Loss 0.0144 (0.0097)	
training:	Epoch: [80][125/233]	Loss 0.0057 (0.0097)	
training:	Epoch: [80][126/233]	Loss 0.0014 (0.0096)	
training:	Epoch: [80][127/233]	Loss 0.0026 (0.0096)	
training:	Epoch: [80][128/233]	Loss 0.0014 (0.0095)	
training:	Epoch: [80][129/233]	Loss 0.0020 (0.0094)	
training:	Epoch: [80][130/233]	Loss 0.0013 (0.0094)	
training:	Epoch: [80][131/233]	Loss 0.0258 (0.0095)	
training:	Epoch: [80][132/233]	Loss 0.0013 (0.0094)	
training:	Epoch: [80][133/233]	Loss 0.0032 (0.0094)	
training:	Epoch: [80][134/233]	Loss 0.0019 (0.0093)	
training:	Epoch: [80][135/233]	Loss 0.0016 (0.0093)	
training:	Epoch: [80][136/233]	Loss 0.0013 (0.0092)	
training:	Epoch: [80][137/233]	Loss 0.0016 (0.0092)	
training:	Epoch: [80][138/233]	Loss 0.0014 (0.0091)	
training:	Epoch: [80][139/233]	Loss 0.0023 (0.0091)	
training:	Epoch: [80][140/233]	Loss 0.0024 (0.0090)	
training:	Epoch: [80][141/233]	Loss 0.0016 (0.0090)	
training:	Epoch: [80][142/233]	Loss 0.0013 (0.0089)	
training:	Epoch: [80][143/233]	Loss 0.0074 (0.0089)	
training:	Epoch: [80][144/233]	Loss 0.0066 (0.0089)	
training:	Epoch: [80][145/233]	Loss 0.0014 (0.0088)	
training:	Epoch: [80][146/233]	Loss 0.0154 (0.0089)	
training:	Epoch: [80][147/233]	Loss 0.0014 (0.0088)	
training:	Epoch: [80][148/233]	Loss 0.0013 (0.0088)	
training:	Epoch: [80][149/233]	Loss 0.0014 (0.0087)	
training:	Epoch: [80][150/233]	Loss 0.0012 (0.0087)	
training:	Epoch: [80][151/233]	Loss 0.0026 (0.0086)	
training:	Epoch: [80][152/233]	Loss 0.0014 (0.0086)	
training:	Epoch: [80][153/233]	Loss 0.0012 (0.0085)	
training:	Epoch: [80][154/233]	Loss 0.0023 (0.0085)	
training:	Epoch: [80][155/233]	Loss 0.0015 (0.0084)	
training:	Epoch: [80][156/233]	Loss 0.0022 (0.0084)	
training:	Epoch: [80][157/233]	Loss 0.0043 (0.0084)	
training:	Epoch: [80][158/233]	Loss 0.0668 (0.0087)	
training:	Epoch: [80][159/233]	Loss 0.0017 (0.0087)	
training:	Epoch: [80][160/233]	Loss 0.0027 (0.0087)	
training:	Epoch: [80][161/233]	Loss 0.0015 (0.0086)	
training:	Epoch: [80][162/233]	Loss 0.0014 (0.0086)	
training:	Epoch: [80][163/233]	Loss 0.0013 (0.0085)	
training:	Epoch: [80][164/233]	Loss 0.0485 (0.0088)	
training:	Epoch: [80][165/233]	Loss 0.0049 (0.0088)	
training:	Epoch: [80][166/233]	Loss 0.0029 (0.0087)	
training:	Epoch: [80][167/233]	Loss 0.0014 (0.0087)	
training:	Epoch: [80][168/233]	Loss 0.0017 (0.0086)	
training:	Epoch: [80][169/233]	Loss 0.0013 (0.0086)	
training:	Epoch: [80][170/233]	Loss 0.0015 (0.0085)	
training:	Epoch: [80][171/233]	Loss 0.0018 (0.0085)	
training:	Epoch: [80][172/233]	Loss 0.1778 (0.0095)	
training:	Epoch: [80][173/233]	Loss 0.2089 (0.0106)	
training:	Epoch: [80][174/233]	Loss 0.0022 (0.0106)	
training:	Epoch: [80][175/233]	Loss 0.0015 (0.0105)	
training:	Epoch: [80][176/233]	Loss 0.0014 (0.0105)	
training:	Epoch: [80][177/233]	Loss 0.0036 (0.0105)	
training:	Epoch: [80][178/233]	Loss 0.0015 (0.0104)	
training:	Epoch: [80][179/233]	Loss 0.0014 (0.0104)	
training:	Epoch: [80][180/233]	Loss 0.0045 (0.0103)	
training:	Epoch: [80][181/233]	Loss 0.0190 (0.0104)	
training:	Epoch: [80][182/233]	Loss 0.0017 (0.0103)	
training:	Epoch: [80][183/233]	Loss 0.2097 (0.0114)	
training:	Epoch: [80][184/233]	Loss 0.2183 (0.0125)	
training:	Epoch: [80][185/233]	Loss 0.0051 (0.0125)	
training:	Epoch: [80][186/233]	Loss 0.0015 (0.0124)	
training:	Epoch: [80][187/233]	Loss 0.0020 (0.0124)	
training:	Epoch: [80][188/233]	Loss 0.0221 (0.0124)	
training:	Epoch: [80][189/233]	Loss 0.0012 (0.0124)	
training:	Epoch: [80][190/233]	Loss 0.0024 (0.0123)	
training:	Epoch: [80][191/233]	Loss 0.0049 (0.0123)	
training:	Epoch: [80][192/233]	Loss 0.0265 (0.0124)	
training:	Epoch: [80][193/233]	Loss 0.0014 (0.0123)	
training:	Epoch: [80][194/233]	Loss 0.0012 (0.0122)	
training:	Epoch: [80][195/233]	Loss 0.0016 (0.0122)	
training:	Epoch: [80][196/233]	Loss 0.0032 (0.0121)	
training:	Epoch: [80][197/233]	Loss 0.0015 (0.0121)	
training:	Epoch: [80][198/233]	Loss 0.0014 (0.0120)	
training:	Epoch: [80][199/233]	Loss 0.0128 (0.0120)	
training:	Epoch: [80][200/233]	Loss 0.0021 (0.0120)	
training:	Epoch: [80][201/233]	Loss 0.0012 (0.0119)	
training:	Epoch: [80][202/233]	Loss 0.0012 (0.0119)	
training:	Epoch: [80][203/233]	Loss 0.0016 (0.0118)	
training:	Epoch: [80][204/233]	Loss 0.0015 (0.0118)	
training:	Epoch: [80][205/233]	Loss 0.0023 (0.0117)	
training:	Epoch: [80][206/233]	Loss 0.0012 (0.0117)	
training:	Epoch: [80][207/233]	Loss 0.1910 (0.0125)	
training:	Epoch: [80][208/233]	Loss 0.0018 (0.0125)	
training:	Epoch: [80][209/233]	Loss 0.0014 (0.0124)	
training:	Epoch: [80][210/233]	Loss 0.0013 (0.0124)	
training:	Epoch: [80][211/233]	Loss 0.0020 (0.0123)	
training:	Epoch: [80][212/233]	Loss 0.0014 (0.0123)	
training:	Epoch: [80][213/233]	Loss 0.0014 (0.0122)	
training:	Epoch: [80][214/233]	Loss 0.0018 (0.0122)	
training:	Epoch: [80][215/233]	Loss 0.0014 (0.0121)	
training:	Epoch: [80][216/233]	Loss 0.0016 (0.0121)	
training:	Epoch: [80][217/233]	Loss 0.0079 (0.0121)	
training:	Epoch: [80][218/233]	Loss 0.0019 (0.0120)	
training:	Epoch: [80][219/233]	Loss 0.0015 (0.0120)	
training:	Epoch: [80][220/233]	Loss 0.0015 (0.0119)	
training:	Epoch: [80][221/233]	Loss 0.1447 (0.0125)	
training:	Epoch: [80][222/233]	Loss 0.0014 (0.0125)	
training:	Epoch: [80][223/233]	Loss 0.0016 (0.0124)	
training:	Epoch: [80][224/233]	Loss 0.0022 (0.0124)	
training:	Epoch: [80][225/233]	Loss 0.0012 (0.0123)	
training:	Epoch: [80][226/233]	Loss 0.0015 (0.0123)	
training:	Epoch: [80][227/233]	Loss 0.0018 (0.0122)	
training:	Epoch: [80][228/233]	Loss 0.0016 (0.0122)	
training:	Epoch: [80][229/233]	Loss 0.0016 (0.0121)	
training:	Epoch: [80][230/233]	Loss 0.0017 (0.0121)	
training:	Epoch: [80][231/233]	Loss 0.0015 (0.0121)	
training:	Epoch: [80][232/233]	Loss 0.0017 (0.0120)	
training:	Epoch: [80][233/233]	Loss 0.2284 (0.0129)	
Training:	 Loss: 0.0129

Training:	 ACC: 0.9993 0.9993 0.9995 0.9992
Validation:	 ACC: 0.8031 0.8031 0.8029 0.8034
Validation:	 Best_BACC: 0.8162 0.8154 0.7998 0.8326
Validation:	 Loss: 1.0264
Pretraining:	Epoch 81/200
----------
training:	Epoch: [81][1/233]	Loss 0.0013 (0.0013)	
training:	Epoch: [81][2/233]	Loss 0.0017 (0.0015)	
training:	Epoch: [81][3/233]	Loss 0.0016 (0.0015)	
training:	Epoch: [81][4/233]	Loss 0.0022 (0.0017)	
training:	Epoch: [81][5/233]	Loss 0.0017 (0.0017)	
training:	Epoch: [81][6/233]	Loss 0.0016 (0.0017)	
training:	Epoch: [81][7/233]	Loss 0.0014 (0.0016)	
training:	Epoch: [81][8/233]	Loss 0.0304 (0.0052)	
training:	Epoch: [81][9/233]	Loss 0.0015 (0.0048)	
training:	Epoch: [81][10/233]	Loss 0.0021 (0.0045)	
training:	Epoch: [81][11/233]	Loss 0.0015 (0.0043)	
training:	Epoch: [81][12/233]	Loss 0.0018 (0.0041)	
training:	Epoch: [81][13/233]	Loss 0.0015 (0.0039)	
training:	Epoch: [81][14/233]	Loss 0.0069 (0.0041)	
training:	Epoch: [81][15/233]	Loss 0.0031 (0.0040)	
training:	Epoch: [81][16/233]	Loss 0.0190 (0.0050)	
training:	Epoch: [81][17/233]	Loss 0.0017 (0.0048)	
training:	Epoch: [81][18/233]	Loss 0.0083 (0.0050)	
training:	Epoch: [81][19/233]	Loss 0.0019 (0.0048)	
training:	Epoch: [81][20/233]	Loss 0.0016 (0.0046)	
training:	Epoch: [81][21/233]	Loss 0.0016 (0.0045)	
training:	Epoch: [81][22/233]	Loss 0.0057 (0.0046)	
training:	Epoch: [81][23/233]	Loss 0.0014 (0.0044)	
training:	Epoch: [81][24/233]	Loss 0.0032 (0.0044)	
training:	Epoch: [81][25/233]	Loss 0.0026 (0.0043)	
training:	Epoch: [81][26/233]	Loss 0.0020 (0.0042)	
training:	Epoch: [81][27/233]	Loss 0.0029 (0.0042)	
training:	Epoch: [81][28/233]	Loss 0.0015 (0.0041)	
training:	Epoch: [81][29/233]	Loss 0.0019 (0.0040)	
training:	Epoch: [81][30/233]	Loss 0.0018 (0.0039)	
training:	Epoch: [81][31/233]	Loss 0.1896 (0.0099)	
training:	Epoch: [81][32/233]	Loss 0.0013 (0.0096)	
training:	Epoch: [81][33/233]	Loss 0.0015 (0.0094)	
training:	Epoch: [81][34/233]	Loss 0.0022 (0.0092)	
training:	Epoch: [81][35/233]	Loss 0.0031 (0.0090)	
training:	Epoch: [81][36/233]	Loss 0.0018 (0.0088)	
training:	Epoch: [81][37/233]	Loss 0.0016 (0.0086)	
training:	Epoch: [81][38/233]	Loss 0.0013 (0.0084)	
training:	Epoch: [81][39/233]	Loss 0.0091 (0.0084)	
training:	Epoch: [81][40/233]	Loss 0.0015 (0.0083)	
training:	Epoch: [81][41/233]	Loss 0.0018 (0.0081)	
training:	Epoch: [81][42/233]	Loss 0.0065 (0.0081)	
training:	Epoch: [81][43/233]	Loss 0.0043 (0.0080)	
training:	Epoch: [81][44/233]	Loss 0.0015 (0.0078)	
training:	Epoch: [81][45/233]	Loss 0.0022 (0.0077)	
training:	Epoch: [81][46/233]	Loss 0.0016 (0.0076)	
training:	Epoch: [81][47/233]	Loss 0.0014 (0.0074)	
training:	Epoch: [81][48/233]	Loss 0.0017 (0.0073)	
training:	Epoch: [81][49/233]	Loss 0.0014 (0.0072)	
training:	Epoch: [81][50/233]	Loss 0.0094 (0.0072)	
training:	Epoch: [81][51/233]	Loss 0.0019 (0.0071)	
training:	Epoch: [81][52/233]	Loss 0.0014 (0.0070)	
training:	Epoch: [81][53/233]	Loss 0.0015 (0.0069)	
training:	Epoch: [81][54/233]	Loss 0.0016 (0.0068)	
training:	Epoch: [81][55/233]	Loss 0.0021 (0.0067)	
training:	Epoch: [81][56/233]	Loss 0.0017 (0.0066)	
training:	Epoch: [81][57/233]	Loss 0.0015 (0.0066)	
training:	Epoch: [81][58/233]	Loss 0.0025 (0.0065)	
training:	Epoch: [81][59/233]	Loss 0.0015 (0.0064)	
training:	Epoch: [81][60/233]	Loss 0.0014 (0.0063)	
training:	Epoch: [81][61/233]	Loss 0.0030 (0.0063)	
training:	Epoch: [81][62/233]	Loss 0.0015 (0.0062)	
training:	Epoch: [81][63/233]	Loss 0.0026 (0.0061)	
training:	Epoch: [81][64/233]	Loss 0.0019 (0.0061)	
training:	Epoch: [81][65/233]	Loss 0.0015 (0.0060)	
training:	Epoch: [81][66/233]	Loss 0.0013 (0.0059)	
training:	Epoch: [81][67/233]	Loss 0.0013 (0.0059)	
training:	Epoch: [81][68/233]	Loss 0.0015 (0.0058)	
training:	Epoch: [81][69/233]	Loss 0.0018 (0.0057)	
training:	Epoch: [81][70/233]	Loss 0.0014 (0.0057)	
training:	Epoch: [81][71/233]	Loss 0.0014 (0.0056)	
training:	Epoch: [81][72/233]	Loss 0.0036 (0.0056)	
training:	Epoch: [81][73/233]	Loss 0.0052 (0.0056)	
training:	Epoch: [81][74/233]	Loss 0.0017 (0.0055)	
training:	Epoch: [81][75/233]	Loss 0.0016 (0.0055)	
training:	Epoch: [81][76/233]	Loss 0.0380 (0.0059)	
training:	Epoch: [81][77/233]	Loss 0.0371 (0.0063)	
training:	Epoch: [81][78/233]	Loss 0.0013 (0.0062)	
training:	Epoch: [81][79/233]	Loss 0.0021 (0.0062)	
training:	Epoch: [81][80/233]	Loss 0.0138 (0.0063)	
training:	Epoch: [81][81/233]	Loss 0.0049 (0.0063)	
training:	Epoch: [81][82/233]	Loss 0.0586 (0.0069)	
training:	Epoch: [81][83/233]	Loss 0.0024 (0.0069)	
training:	Epoch: [81][84/233]	Loss 0.0018 (0.0068)	
training:	Epoch: [81][85/233]	Loss 0.0013 (0.0067)	
training:	Epoch: [81][86/233]	Loss 0.0014 (0.0067)	
training:	Epoch: [81][87/233]	Loss 0.0013 (0.0066)	
training:	Epoch: [81][88/233]	Loss 0.0105 (0.0066)	
training:	Epoch: [81][89/233]	Loss 0.0433 (0.0071)	
training:	Epoch: [81][90/233]	Loss 0.0538 (0.0076)	
training:	Epoch: [81][91/233]	Loss 0.0032 (0.0075)	
training:	Epoch: [81][92/233]	Loss 0.0019 (0.0075)	
training:	Epoch: [81][93/233]	Loss 0.0069 (0.0075)	
training:	Epoch: [81][94/233]	Loss 0.0018 (0.0074)	
training:	Epoch: [81][95/233]	Loss 0.0025 (0.0074)	
training:	Epoch: [81][96/233]	Loss 0.0013 (0.0073)	
training:	Epoch: [81][97/233]	Loss 0.0048 (0.0073)	
training:	Epoch: [81][98/233]	Loss 0.0017 (0.0072)	
training:	Epoch: [81][99/233]	Loss 0.0060 (0.0072)	
training:	Epoch: [81][100/233]	Loss 0.0013 (0.0071)	
training:	Epoch: [81][101/233]	Loss 0.0013 (0.0071)	
training:	Epoch: [81][102/233]	Loss 0.0064 (0.0071)	
training:	Epoch: [81][103/233]	Loss 0.2159 (0.0091)	
training:	Epoch: [81][104/233]	Loss 0.0018 (0.0090)	
training:	Epoch: [81][105/233]	Loss 0.0013 (0.0090)	
training:	Epoch: [81][106/233]	Loss 0.0014 (0.0089)	
training:	Epoch: [81][107/233]	Loss 0.0013 (0.0088)	
training:	Epoch: [81][108/233]	Loss 0.0013 (0.0087)	
training:	Epoch: [81][109/233]	Loss 0.0012 (0.0087)	
training:	Epoch: [81][110/233]	Loss 0.1868 (0.0103)	
training:	Epoch: [81][111/233]	Loss 0.0018 (0.0102)	
training:	Epoch: [81][112/233]	Loss 0.0017 (0.0101)	
training:	Epoch: [81][113/233]	Loss 0.0015 (0.0101)	
training:	Epoch: [81][114/233]	Loss 0.0016 (0.0100)	
training:	Epoch: [81][115/233]	Loss 0.0013 (0.0099)	
training:	Epoch: [81][116/233]	Loss 0.0466 (0.0102)	
training:	Epoch: [81][117/233]	Loss 0.0024 (0.0102)	
training:	Epoch: [81][118/233]	Loss 0.0013 (0.0101)	
training:	Epoch: [81][119/233]	Loss 0.0016 (0.0100)	
training:	Epoch: [81][120/233]	Loss 0.0014 (0.0099)	
training:	Epoch: [81][121/233]	Loss 0.0013 (0.0099)	
training:	Epoch: [81][122/233]	Loss 0.0053 (0.0098)	
training:	Epoch: [81][123/233]	Loss 0.0015 (0.0098)	
training:	Epoch: [81][124/233]	Loss 0.0013 (0.0097)	
training:	Epoch: [81][125/233]	Loss 0.0015 (0.0096)	
training:	Epoch: [81][126/233]	Loss 0.0016 (0.0096)	
training:	Epoch: [81][127/233]	Loss 0.0017 (0.0095)	
training:	Epoch: [81][128/233]	Loss 0.0013 (0.0094)	
training:	Epoch: [81][129/233]	Loss 0.0122 (0.0095)	
training:	Epoch: [81][130/233]	Loss 0.1373 (0.0105)	
training:	Epoch: [81][131/233]	Loss 0.0025 (0.0104)	
training:	Epoch: [81][132/233]	Loss 0.0022 (0.0103)	
training:	Epoch: [81][133/233]	Loss 0.0014 (0.0103)	
training:	Epoch: [81][134/233]	Loss 0.0053 (0.0102)	
training:	Epoch: [81][135/233]	Loss 0.0013 (0.0102)	
training:	Epoch: [81][136/233]	Loss 0.0023 (0.0101)	
training:	Epoch: [81][137/233]	Loss 0.0019 (0.0100)	
training:	Epoch: [81][138/233]	Loss 0.2081 (0.0115)	
training:	Epoch: [81][139/233]	Loss 0.0014 (0.0114)	
training:	Epoch: [81][140/233]	Loss 0.0027 (0.0113)	
training:	Epoch: [81][141/233]	Loss 0.0014 (0.0113)	
training:	Epoch: [81][142/233]	Loss 0.0017 (0.0112)	
training:	Epoch: [81][143/233]	Loss 0.0014 (0.0111)	
training:	Epoch: [81][144/233]	Loss 0.0017 (0.0111)	
training:	Epoch: [81][145/233]	Loss 0.0091 (0.0111)	
training:	Epoch: [81][146/233]	Loss 0.0013 (0.0110)	
training:	Epoch: [81][147/233]	Loss 0.0097 (0.0110)	
training:	Epoch: [81][148/233]	Loss 0.0014 (0.0109)	
training:	Epoch: [81][149/233]	Loss 0.0132 (0.0109)	
training:	Epoch: [81][150/233]	Loss 0.0013 (0.0109)	
training:	Epoch: [81][151/233]	Loss 0.0755 (0.0113)	
training:	Epoch: [81][152/233]	Loss 0.0018 (0.0112)	
training:	Epoch: [81][153/233]	Loss 0.0019 (0.0112)	
training:	Epoch: [81][154/233]	Loss 0.0037 (0.0111)	
training:	Epoch: [81][155/233]	Loss 0.0015 (0.0111)	
training:	Epoch: [81][156/233]	Loss 0.0015 (0.0110)	
training:	Epoch: [81][157/233]	Loss 0.1277 (0.0117)	
training:	Epoch: [81][158/233]	Loss 0.0016 (0.0117)	
training:	Epoch: [81][159/233]	Loss 0.0013 (0.0116)	
training:	Epoch: [81][160/233]	Loss 0.0018 (0.0116)	
training:	Epoch: [81][161/233]	Loss 0.0017 (0.0115)	
training:	Epoch: [81][162/233]	Loss 0.0019 (0.0114)	
training:	Epoch: [81][163/233]	Loss 0.1149 (0.0121)	
training:	Epoch: [81][164/233]	Loss 0.0018 (0.0120)	
training:	Epoch: [81][165/233]	Loss 0.0017 (0.0119)	
training:	Epoch: [81][166/233]	Loss 0.0020 (0.0119)	
training:	Epoch: [81][167/233]	Loss 0.0016 (0.0118)	
training:	Epoch: [81][168/233]	Loss 0.0014 (0.0118)	
training:	Epoch: [81][169/233]	Loss 0.0015 (0.0117)	
training:	Epoch: [81][170/233]	Loss 0.0019 (0.0116)	
training:	Epoch: [81][171/233]	Loss 0.0025 (0.0116)	
training:	Epoch: [81][172/233]	Loss 0.0015 (0.0115)	
training:	Epoch: [81][173/233]	Loss 0.0022 (0.0115)	
training:	Epoch: [81][174/233]	Loss 0.0033 (0.0114)	
training:	Epoch: [81][175/233]	Loss 0.0047 (0.0114)	
training:	Epoch: [81][176/233]	Loss 0.0014 (0.0113)	
training:	Epoch: [81][177/233]	Loss 0.0018 (0.0113)	
training:	Epoch: [81][178/233]	Loss 0.0013 (0.0112)	
training:	Epoch: [81][179/233]	Loss 0.0016 (0.0112)	
training:	Epoch: [81][180/233]	Loss 0.0019 (0.0111)	
training:	Epoch: [81][181/233]	Loss 0.0017 (0.0111)	
training:	Epoch: [81][182/233]	Loss 0.0013 (0.0110)	
training:	Epoch: [81][183/233]	Loss 0.0024 (0.0110)	
training:	Epoch: [81][184/233]	Loss 0.0019 (0.0109)	
training:	Epoch: [81][185/233]	Loss 0.0015 (0.0109)	
training:	Epoch: [81][186/233]	Loss 0.0031 (0.0108)	
training:	Epoch: [81][187/233]	Loss 0.0013 (0.0108)	
training:	Epoch: [81][188/233]	Loss 0.0016 (0.0107)	
training:	Epoch: [81][189/233]	Loss 0.0022 (0.0107)	
training:	Epoch: [81][190/233]	Loss 0.0021 (0.0106)	
training:	Epoch: [81][191/233]	Loss 0.0014 (0.0106)	
training:	Epoch: [81][192/233]	Loss 0.0036 (0.0105)	
training:	Epoch: [81][193/233]	Loss 0.0012 (0.0105)	
training:	Epoch: [81][194/233]	Loss 0.0015 (0.0105)	
training:	Epoch: [81][195/233]	Loss 0.0015 (0.0104)	
training:	Epoch: [81][196/233]	Loss 0.2054 (0.0114)	
training:	Epoch: [81][197/233]	Loss 0.0017 (0.0114)	
training:	Epoch: [81][198/233]	Loss 0.0017 (0.0113)	
training:	Epoch: [81][199/233]	Loss 0.0037 (0.0113)	
training:	Epoch: [81][200/233]	Loss 0.0779 (0.0116)	
training:	Epoch: [81][201/233]	Loss 0.0026 (0.0116)	
training:	Epoch: [81][202/233]	Loss 0.0020 (0.0115)	
training:	Epoch: [81][203/233]	Loss 0.0014 (0.0115)	
training:	Epoch: [81][204/233]	Loss 0.0021 (0.0114)	
training:	Epoch: [81][205/233]	Loss 0.0017 (0.0114)	
training:	Epoch: [81][206/233]	Loss 0.0019 (0.0113)	
training:	Epoch: [81][207/233]	Loss 0.0017 (0.0113)	
training:	Epoch: [81][208/233]	Loss 0.0015 (0.0112)	
training:	Epoch: [81][209/233]	Loss 0.0019 (0.0112)	
training:	Epoch: [81][210/233]	Loss 0.0014 (0.0111)	
training:	Epoch: [81][211/233]	Loss 0.0016 (0.0111)	
training:	Epoch: [81][212/233]	Loss 0.0014 (0.0110)	
training:	Epoch: [81][213/233]	Loss 0.0493 (0.0112)	
training:	Epoch: [81][214/233]	Loss 0.0016 (0.0112)	
training:	Epoch: [81][215/233]	Loss 0.0026 (0.0111)	
training:	Epoch: [81][216/233]	Loss 0.0013 (0.0111)	
training:	Epoch: [81][217/233]	Loss 0.0018 (0.0110)	
training:	Epoch: [81][218/233]	Loss 0.0022 (0.0110)	
training:	Epoch: [81][219/233]	Loss 0.0027 (0.0110)	
training:	Epoch: [81][220/233]	Loss 0.0014 (0.0109)	
training:	Epoch: [81][221/233]	Loss 0.0015 (0.0109)	
training:	Epoch: [81][222/233]	Loss 0.0018 (0.0108)	
training:	Epoch: [81][223/233]	Loss 0.0054 (0.0108)	
training:	Epoch: [81][224/233]	Loss 0.0020 (0.0108)	
training:	Epoch: [81][225/233]	Loss 0.0014 (0.0107)	
training:	Epoch: [81][226/233]	Loss 0.0022 (0.0107)	
training:	Epoch: [81][227/233]	Loss 0.0038 (0.0107)	
training:	Epoch: [81][228/233]	Loss 0.0071 (0.0107)	
training:	Epoch: [81][229/233]	Loss 0.0021 (0.0106)	
training:	Epoch: [81][230/233]	Loss 0.0014 (0.0106)	
training:	Epoch: [81][231/233]	Loss 0.0021 (0.0105)	
training:	Epoch: [81][232/233]	Loss 0.0056 (0.0105)	
training:	Epoch: [81][233/233]	Loss 0.0047 (0.0105)	
Training:	 Loss: 0.0105

Training:	 ACC: 0.9993 0.9993 0.9995 0.9992
Validation:	 ACC: 0.7983 0.7978 0.7875 0.8090
Validation:	 Best_BACC: 0.8162 0.8154 0.7998 0.8326
Validation:	 Loss: 1.0440
Pretraining:	Epoch 82/200
----------
training:	Epoch: [82][1/233]	Loss 0.0174 (0.0174)	
training:	Epoch: [82][2/233]	Loss 0.0024 (0.0099)	
training:	Epoch: [82][3/233]	Loss 0.0062 (0.0087)	
training:	Epoch: [82][4/233]	Loss 0.0014 (0.0068)	
training:	Epoch: [82][5/233]	Loss 0.0105 (0.0076)	
training:	Epoch: [82][6/233]	Loss 0.0014 (0.0066)	
training:	Epoch: [82][7/233]	Loss 0.0023 (0.0060)	
training:	Epoch: [82][8/233]	Loss 0.0027 (0.0056)	
training:	Epoch: [82][9/233]	Loss 0.0013 (0.0051)	
training:	Epoch: [82][10/233]	Loss 0.0046 (0.0050)	
training:	Epoch: [82][11/233]	Loss 0.0027 (0.0048)	
training:	Epoch: [82][12/233]	Loss 0.0218 (0.0062)	
training:	Epoch: [82][13/233]	Loss 0.0017 (0.0059)	
training:	Epoch: [82][14/233]	Loss 0.0088 (0.0061)	
training:	Epoch: [82][15/233]	Loss 0.0985 (0.0123)	
training:	Epoch: [82][16/233]	Loss 0.0015 (0.0116)	
training:	Epoch: [82][17/233]	Loss 0.0016 (0.0110)	
training:	Epoch: [82][18/233]	Loss 0.0018 (0.0105)	
training:	Epoch: [82][19/233]	Loss 0.0013 (0.0100)	
training:	Epoch: [82][20/233]	Loss 0.0023 (0.0096)	
training:	Epoch: [82][21/233]	Loss 0.0520 (0.0116)	
training:	Epoch: [82][22/233]	Loss 0.0015 (0.0112)	
training:	Epoch: [82][23/233]	Loss 0.0022 (0.0108)	
training:	Epoch: [82][24/233]	Loss 0.0142 (0.0109)	
training:	Epoch: [82][25/233]	Loss 0.0016 (0.0105)	
training:	Epoch: [82][26/233]	Loss 0.0014 (0.0102)	
training:	Epoch: [82][27/233]	Loss 0.0014 (0.0099)	
training:	Epoch: [82][28/233]	Loss 0.0015 (0.0096)	
training:	Epoch: [82][29/233]	Loss 0.0013 (0.0093)	
training:	Epoch: [82][30/233]	Loss 0.1105 (0.0127)	
training:	Epoch: [82][31/233]	Loss 0.0019 (0.0123)	
training:	Epoch: [82][32/233]	Loss 0.0018 (0.0120)	
training:	Epoch: [82][33/233]	Loss 0.0015 (0.0117)	
training:	Epoch: [82][34/233]	Loss 0.0014 (0.0114)	
training:	Epoch: [82][35/233]	Loss 0.0026 (0.0111)	
training:	Epoch: [82][36/233]	Loss 0.0014 (0.0108)	
training:	Epoch: [82][37/233]	Loss 0.0035 (0.0106)	
training:	Epoch: [82][38/233]	Loss 0.0016 (0.0104)	
training:	Epoch: [82][39/233]	Loss 0.0021 (0.0102)	
training:	Epoch: [82][40/233]	Loss 0.0022 (0.0100)	
training:	Epoch: [82][41/233]	Loss 0.0016 (0.0098)	
training:	Epoch: [82][42/233]	Loss 0.0016 (0.0096)	
training:	Epoch: [82][43/233]	Loss 0.0020 (0.0094)	
training:	Epoch: [82][44/233]	Loss 0.0015 (0.0092)	
training:	Epoch: [82][45/233]	Loss 0.2112 (0.0137)	
training:	Epoch: [82][46/233]	Loss 0.0019 (0.0135)	
training:	Epoch: [82][47/233]	Loss 0.0015 (0.0132)	
training:	Epoch: [82][48/233]	Loss 0.0015 (0.0130)	
training:	Epoch: [82][49/233]	Loss 0.0014 (0.0127)	
training:	Epoch: [82][50/233]	Loss 0.0058 (0.0126)	
training:	Epoch: [82][51/233]	Loss 0.0157 (0.0127)	
training:	Epoch: [82][52/233]	Loss 0.0018 (0.0124)	
training:	Epoch: [82][53/233]	Loss 0.0027 (0.0123)	
training:	Epoch: [82][54/233]	Loss 0.0093 (0.0122)	
training:	Epoch: [82][55/233]	Loss 0.0013 (0.0120)	
training:	Epoch: [82][56/233]	Loss 0.0019 (0.0118)	
training:	Epoch: [82][57/233]	Loss 0.0019 (0.0117)	
training:	Epoch: [82][58/233]	Loss 0.0013 (0.0115)	
training:	Epoch: [82][59/233]	Loss 0.0012 (0.0113)	
training:	Epoch: [82][60/233]	Loss 0.0014 (0.0111)	
training:	Epoch: [82][61/233]	Loss 0.0013 (0.0110)	
training:	Epoch: [82][62/233]	Loss 0.1187 (0.0127)	
training:	Epoch: [82][63/233]	Loss 0.2022 (0.0157)	
training:	Epoch: [82][64/233]	Loss 0.0025 (0.0155)	
training:	Epoch: [82][65/233]	Loss 0.0023 (0.0153)	
training:	Epoch: [82][66/233]	Loss 0.0014 (0.0151)	
training:	Epoch: [82][67/233]	Loss 0.0150 (0.0151)	
training:	Epoch: [82][68/233]	Loss 0.0019 (0.0149)	
training:	Epoch: [82][69/233]	Loss 0.0022 (0.0147)	
training:	Epoch: [82][70/233]	Loss 0.0020 (0.0145)	
training:	Epoch: [82][71/233]	Loss 0.0020 (0.0144)	
training:	Epoch: [82][72/233]	Loss 0.0014 (0.0142)	
training:	Epoch: [82][73/233]	Loss 0.0014 (0.0140)	
training:	Epoch: [82][74/233]	Loss 0.0048 (0.0139)	
training:	Epoch: [82][75/233]	Loss 0.1685 (0.0159)	
training:	Epoch: [82][76/233]	Loss 0.0020 (0.0158)	
training:	Epoch: [82][77/233]	Loss 0.0033 (0.0156)	
training:	Epoch: [82][78/233]	Loss 0.0017 (0.0154)	
training:	Epoch: [82][79/233]	Loss 0.0018 (0.0153)	
training:	Epoch: [82][80/233]	Loss 0.0013 (0.0151)	
training:	Epoch: [82][81/233]	Loss 0.0028 (0.0149)	
training:	Epoch: [82][82/233]	Loss 0.0012 (0.0148)	
training:	Epoch: [82][83/233]	Loss 0.0020 (0.0146)	
training:	Epoch: [82][84/233]	Loss 0.0015 (0.0144)	
training:	Epoch: [82][85/233]	Loss 0.0018 (0.0143)	
training:	Epoch: [82][86/233]	Loss 0.0016 (0.0142)	
training:	Epoch: [82][87/233]	Loss 0.0021 (0.0140)	
training:	Epoch: [82][88/233]	Loss 0.0075 (0.0139)	
training:	Epoch: [82][89/233]	Loss 0.0016 (0.0138)	
training:	Epoch: [82][90/233]	Loss 0.0066 (0.0137)	
training:	Epoch: [82][91/233]	Loss 0.0015 (0.0136)	
training:	Epoch: [82][92/233]	Loss 0.0021 (0.0135)	
training:	Epoch: [82][93/233]	Loss 0.0014 (0.0133)	
training:	Epoch: [82][94/233]	Loss 0.0024 (0.0132)	
training:	Epoch: [82][95/233]	Loss 0.0015 (0.0131)	
training:	Epoch: [82][96/233]	Loss 0.1561 (0.0146)	
training:	Epoch: [82][97/233]	Loss 0.0017 (0.0144)	
training:	Epoch: [82][98/233]	Loss 0.0155 (0.0145)	
training:	Epoch: [82][99/233]	Loss 0.0019 (0.0143)	
training:	Epoch: [82][100/233]	Loss 0.0015 (0.0142)	
training:	Epoch: [82][101/233]	Loss 0.0015 (0.0141)	
training:	Epoch: [82][102/233]	Loss 0.0047 (0.0140)	
training:	Epoch: [82][103/233]	Loss 0.0013 (0.0139)	
training:	Epoch: [82][104/233]	Loss 0.0020 (0.0137)	
training:	Epoch: [82][105/233]	Loss 0.0047 (0.0137)	
training:	Epoch: [82][106/233]	Loss 0.0068 (0.0136)	
training:	Epoch: [82][107/233]	Loss 0.0020 (0.0135)	
training:	Epoch: [82][108/233]	Loss 0.0015 (0.0134)	
training:	Epoch: [82][109/233]	Loss 0.0019 (0.0133)	
training:	Epoch: [82][110/233]	Loss 0.1048 (0.0141)	
training:	Epoch: [82][111/233]	Loss 0.0013 (0.0140)	
training:	Epoch: [82][112/233]	Loss 0.0022 (0.0139)	
training:	Epoch: [82][113/233]	Loss 0.0024 (0.0138)	
training:	Epoch: [82][114/233]	Loss 0.0029 (0.0137)	
training:	Epoch: [82][115/233]	Loss 0.0020 (0.0136)	
training:	Epoch: [82][116/233]	Loss 0.0015 (0.0135)	
training:	Epoch: [82][117/233]	Loss 0.0013 (0.0134)	
training:	Epoch: [82][118/233]	Loss 0.0014 (0.0133)	
training:	Epoch: [82][119/233]	Loss 0.0016 (0.0132)	
training:	Epoch: [82][120/233]	Loss 0.0016 (0.0131)	
training:	Epoch: [82][121/233]	Loss 0.0022 (0.0130)	
training:	Epoch: [82][122/233]	Loss 0.0099 (0.0130)	
training:	Epoch: [82][123/233]	Loss 0.0012 (0.0129)	
training:	Epoch: [82][124/233]	Loss 0.0128 (0.0129)	
training:	Epoch: [82][125/233]	Loss 0.0015 (0.0128)	
training:	Epoch: [82][126/233]	Loss 0.0017 (0.0127)	
training:	Epoch: [82][127/233]	Loss 0.0042 (0.0126)	
training:	Epoch: [82][128/233]	Loss 0.0017 (0.0125)	
training:	Epoch: [82][129/233]	Loss 0.0013 (0.0125)	
training:	Epoch: [82][130/233]	Loss 0.0013 (0.0124)	
training:	Epoch: [82][131/233]	Loss 0.0014 (0.0123)	
training:	Epoch: [82][132/233]	Loss 0.0012 (0.0122)	
training:	Epoch: [82][133/233]	Loss 0.0018 (0.0121)	
training:	Epoch: [82][134/233]	Loss 0.2082 (0.0136)	
training:	Epoch: [82][135/233]	Loss 0.0020 (0.0135)	
training:	Epoch: [82][136/233]	Loss 0.0049 (0.0134)	
training:	Epoch: [82][137/233]	Loss 0.0030 (0.0134)	
training:	Epoch: [82][138/233]	Loss 0.0012 (0.0133)	
training:	Epoch: [82][139/233]	Loss 0.0013 (0.0132)	
training:	Epoch: [82][140/233]	Loss 0.0040 (0.0131)	
training:	Epoch: [82][141/233]	Loss 0.0198 (0.0132)	
training:	Epoch: [82][142/233]	Loss 0.0014 (0.0131)	
training:	Epoch: [82][143/233]	Loss 0.0015 (0.0130)	
training:	Epoch: [82][144/233]	Loss 0.0012 (0.0129)	
training:	Epoch: [82][145/233]	Loss 0.2143 (0.0143)	
training:	Epoch: [82][146/233]	Loss 0.0015 (0.0142)	
training:	Epoch: [82][147/233]	Loss 0.0077 (0.0142)	
training:	Epoch: [82][148/233]	Loss 0.0017 (0.0141)	
training:	Epoch: [82][149/233]	Loss 0.0015 (0.0140)	
training:	Epoch: [82][150/233]	Loss 0.0016 (0.0139)	
training:	Epoch: [82][151/233]	Loss 0.0014 (0.0138)	
training:	Epoch: [82][152/233]	Loss 0.0021 (0.0138)	
training:	Epoch: [82][153/233]	Loss 0.0800 (0.0142)	
training:	Epoch: [82][154/233]	Loss 0.0410 (0.0144)	
training:	Epoch: [82][155/233]	Loss 0.0042 (0.0143)	
training:	Epoch: [82][156/233]	Loss 0.0069 (0.0143)	
training:	Epoch: [82][157/233]	Loss 0.0019 (0.0142)	
training:	Epoch: [82][158/233]	Loss 0.0013 (0.0141)	
training:	Epoch: [82][159/233]	Loss 0.1532 (0.0150)	
training:	Epoch: [82][160/233]	Loss 0.0050 (0.0149)	
training:	Epoch: [82][161/233]	Loss 0.0799 (0.0153)	
training:	Epoch: [82][162/233]	Loss 0.0017 (0.0152)	
training:	Epoch: [82][163/233]	Loss 0.0031 (0.0152)	
training:	Epoch: [82][164/233]	Loss 0.0013 (0.0151)	
training:	Epoch: [82][165/233]	Loss 0.0038 (0.0150)	
training:	Epoch: [82][166/233]	Loss 0.0019 (0.0149)	
training:	Epoch: [82][167/233]	Loss 0.0016 (0.0148)	
training:	Epoch: [82][168/233]	Loss 0.0013 (0.0148)	
training:	Epoch: [82][169/233]	Loss 0.0016 (0.0147)	
training:	Epoch: [82][170/233]	Loss 0.0014 (0.0146)	
training:	Epoch: [82][171/233]	Loss 0.0025 (0.0145)	
training:	Epoch: [82][172/233]	Loss 0.0144 (0.0145)	
training:	Epoch: [82][173/233]	Loss 0.0353 (0.0147)	
training:	Epoch: [82][174/233]	Loss 0.0049 (0.0146)	
training:	Epoch: [82][175/233]	Loss 0.0013 (0.0145)	
training:	Epoch: [82][176/233]	Loss 0.0944 (0.0150)	
training:	Epoch: [82][177/233]	Loss 0.0023 (0.0149)	
training:	Epoch: [82][178/233]	Loss 0.0322 (0.0150)	
training:	Epoch: [82][179/233]	Loss 0.0039 (0.0149)	
training:	Epoch: [82][180/233]	Loss 0.0356 (0.0151)	
training:	Epoch: [82][181/233]	Loss 0.0015 (0.0150)	
training:	Epoch: [82][182/233]	Loss 0.0014 (0.0149)	
training:	Epoch: [82][183/233]	Loss 0.0071 (0.0149)	
training:	Epoch: [82][184/233]	Loss 0.0146 (0.0149)	
training:	Epoch: [82][185/233]	Loss 0.0016 (0.0148)	
training:	Epoch: [82][186/233]	Loss 0.0016 (0.0147)	
training:	Epoch: [82][187/233]	Loss 0.0032 (0.0147)	
training:	Epoch: [82][188/233]	Loss 0.0098 (0.0146)	
training:	Epoch: [82][189/233]	Loss 0.0018 (0.0146)	
training:	Epoch: [82][190/233]	Loss 0.0018 (0.0145)	
training:	Epoch: [82][191/233]	Loss 0.0014 (0.0144)	
training:	Epoch: [82][192/233]	Loss 0.0014 (0.0144)	
training:	Epoch: [82][193/233]	Loss 0.0013 (0.0143)	
training:	Epoch: [82][194/233]	Loss 0.1055 (0.0148)	
training:	Epoch: [82][195/233]	Loss 0.0013 (0.0147)	
training:	Epoch: [82][196/233]	Loss 0.0020 (0.0146)	
training:	Epoch: [82][197/233]	Loss 0.0017 (0.0146)	
training:	Epoch: [82][198/233]	Loss 0.0018 (0.0145)	
training:	Epoch: [82][199/233]	Loss 0.0084 (0.0145)	
training:	Epoch: [82][200/233]	Loss 0.0013 (0.0144)	
training:	Epoch: [82][201/233]	Loss 0.0013 (0.0143)	
training:	Epoch: [82][202/233]	Loss 0.0016 (0.0143)	
training:	Epoch: [82][203/233]	Loss 0.0017 (0.0142)	
training:	Epoch: [82][204/233]	Loss 0.0012 (0.0141)	
training:	Epoch: [82][205/233]	Loss 0.0014 (0.0141)	
training:	Epoch: [82][206/233]	Loss 0.0027 (0.0140)	
training:	Epoch: [82][207/233]	Loss 0.0016 (0.0140)	
training:	Epoch: [82][208/233]	Loss 0.1854 (0.0148)	
training:	Epoch: [82][209/233]	Loss 0.0013 (0.0147)	
training:	Epoch: [82][210/233]	Loss 0.0021 (0.0147)	
training:	Epoch: [82][211/233]	Loss 0.0015 (0.0146)	
training:	Epoch: [82][212/233]	Loss 0.0018 (0.0145)	
training:	Epoch: [82][213/233]	Loss 0.0426 (0.0147)	
training:	Epoch: [82][214/233]	Loss 0.1101 (0.0151)	
training:	Epoch: [82][215/233]	Loss 0.0027 (0.0151)	
training:	Epoch: [82][216/233]	Loss 0.0050 (0.0150)	
training:	Epoch: [82][217/233]	Loss 0.0017 (0.0150)	
training:	Epoch: [82][218/233]	Loss 0.0016 (0.0149)	
training:	Epoch: [82][219/233]	Loss 0.0018 (0.0148)	
training:	Epoch: [82][220/233]	Loss 0.0043 (0.0148)	
training:	Epoch: [82][221/233]	Loss 0.0017 (0.0147)	
training:	Epoch: [82][222/233]	Loss 0.0015 (0.0147)	
training:	Epoch: [82][223/233]	Loss 0.0610 (0.0149)	
training:	Epoch: [82][224/233]	Loss 0.0224 (0.0149)	
training:	Epoch: [82][225/233]	Loss 0.0019 (0.0149)	
training:	Epoch: [82][226/233]	Loss 0.0039 (0.0148)	
training:	Epoch: [82][227/233]	Loss 0.0050 (0.0148)	
training:	Epoch: [82][228/233]	Loss 0.0014 (0.0147)	
training:	Epoch: [82][229/233]	Loss 0.0015 (0.0146)	
training:	Epoch: [82][230/233]	Loss 0.0015 (0.0146)	
training:	Epoch: [82][231/233]	Loss 0.0032 (0.0145)	
training:	Epoch: [82][232/233]	Loss 0.1119 (0.0150)	
training:	Epoch: [82][233/233]	Loss 0.0017 (0.0149)	
Training:	 Loss: 0.0149

Training:	 ACC: 0.9993 0.9993 0.9995 0.9992
Validation:	 ACC: 0.7830 0.7817 0.7549 0.8112
Validation:	 Best_BACC: 0.8162 0.8154 0.7998 0.8326
Validation:	 Loss: 1.0643
Pretraining:	Epoch 83/200
----------
training:	Epoch: [83][1/233]	Loss 0.0019 (0.0019)	
training:	Epoch: [83][2/233]	Loss 0.0018 (0.0018)	
training:	Epoch: [83][3/233]	Loss 0.0019 (0.0018)	
training:	Epoch: [83][4/233]	Loss 0.0015 (0.0018)	
training:	Epoch: [83][5/233]	Loss 0.0341 (0.0082)	
training:	Epoch: [83][6/233]	Loss 0.0018 (0.0071)	
training:	Epoch: [83][7/233]	Loss 0.0014 (0.0063)	
training:	Epoch: [83][8/233]	Loss 0.2070 (0.0314)	
training:	Epoch: [83][9/233]	Loss 0.0071 (0.0287)	
training:	Epoch: [83][10/233]	Loss 0.0041 (0.0263)	
training:	Epoch: [83][11/233]	Loss 0.0040 (0.0242)	
training:	Epoch: [83][12/233]	Loss 0.0032 (0.0225)	
training:	Epoch: [83][13/233]	Loss 0.0013 (0.0209)	
training:	Epoch: [83][14/233]	Loss 0.0367 (0.0220)	
training:	Epoch: [83][15/233]	Loss 0.0015 (0.0206)	
training:	Epoch: [83][16/233]	Loss 0.0050 (0.0196)	
training:	Epoch: [83][17/233]	Loss 0.0018 (0.0186)	
training:	Epoch: [83][18/233]	Loss 0.0016 (0.0176)	
training:	Epoch: [83][19/233]	Loss 0.0034 (0.0169)	
training:	Epoch: [83][20/233]	Loss 0.0013 (0.0161)	
training:	Epoch: [83][21/233]	Loss 0.0033 (0.0155)	
training:	Epoch: [83][22/233]	Loss 0.0014 (0.0149)	
training:	Epoch: [83][23/233]	Loss 0.0019 (0.0143)	
training:	Epoch: [83][24/233]	Loss 0.0017 (0.0138)	
training:	Epoch: [83][25/233]	Loss 0.0018 (0.0133)	
training:	Epoch: [83][26/233]	Loss 0.0029 (0.0129)	
training:	Epoch: [83][27/233]	Loss 0.0449 (0.0141)	
training:	Epoch: [83][28/233]	Loss 0.0015 (0.0136)	
training:	Epoch: [83][29/233]	Loss 0.0160 (0.0137)	
training:	Epoch: [83][30/233]	Loss 0.0015 (0.0133)	
training:	Epoch: [83][31/233]	Loss 0.0013 (0.0129)	
training:	Epoch: [83][32/233]	Loss 0.0014 (0.0126)	
training:	Epoch: [83][33/233]	Loss 0.0027 (0.0123)	
training:	Epoch: [83][34/233]	Loss 0.0033 (0.0120)	
training:	Epoch: [83][35/233]	Loss 0.0016 (0.0117)	
training:	Epoch: [83][36/233]	Loss 0.0018 (0.0114)	
training:	Epoch: [83][37/233]	Loss 0.0014 (0.0112)	
training:	Epoch: [83][38/233]	Loss 0.0015 (0.0109)	
training:	Epoch: [83][39/233]	Loss 0.1784 (0.0152)	
training:	Epoch: [83][40/233]	Loss 0.0017 (0.0149)	
training:	Epoch: [83][41/233]	Loss 0.1140 (0.0173)	
training:	Epoch: [83][42/233]	Loss 0.0021 (0.0169)	
training:	Epoch: [83][43/233]	Loss 0.0016 (0.0166)	
training:	Epoch: [83][44/233]	Loss 0.0041 (0.0163)	
training:	Epoch: [83][45/233]	Loss 0.0015 (0.0159)	
training:	Epoch: [83][46/233]	Loss 0.0038 (0.0157)	
training:	Epoch: [83][47/233]	Loss 0.0015 (0.0154)	
training:	Epoch: [83][48/233]	Loss 0.0015 (0.0151)	
training:	Epoch: [83][49/233]	Loss 0.0039 (0.0149)	
training:	Epoch: [83][50/233]	Loss 0.0016 (0.0146)	
training:	Epoch: [83][51/233]	Loss 0.0013 (0.0143)	
training:	Epoch: [83][52/233]	Loss 0.0019 (0.0141)	
training:	Epoch: [83][53/233]	Loss 0.0014 (0.0139)	
training:	Epoch: [83][54/233]	Loss 0.0019 (0.0136)	
training:	Epoch: [83][55/233]	Loss 0.0012 (0.0134)	
training:	Epoch: [83][56/233]	Loss 0.0021 (0.0132)	
training:	Epoch: [83][57/233]	Loss 0.0018 (0.0130)	
training:	Epoch: [83][58/233]	Loss 0.0015 (0.0128)	
training:	Epoch: [83][59/233]	Loss 0.0017 (0.0126)	
training:	Epoch: [83][60/233]	Loss 0.0030 (0.0125)	
training:	Epoch: [83][61/233]	Loss 0.1189 (0.0142)	
training:	Epoch: [83][62/233]	Loss 0.0014 (0.0140)	
training:	Epoch: [83][63/233]	Loss 0.0015 (0.0138)	
training:	Epoch: [83][64/233]	Loss 0.0016 (0.0136)	
training:	Epoch: [83][65/233]	Loss 0.0016 (0.0134)	
training:	Epoch: [83][66/233]	Loss 0.0016 (0.0132)	
training:	Epoch: [83][67/233]	Loss 0.0043 (0.0131)	
training:	Epoch: [83][68/233]	Loss 0.2144 (0.0161)	
training:	Epoch: [83][69/233]	Loss 0.0426 (0.0165)	
training:	Epoch: [83][70/233]	Loss 0.0018 (0.0162)	
training:	Epoch: [83][71/233]	Loss 0.0030 (0.0161)	
training:	Epoch: [83][72/233]	Loss 0.0016 (0.0159)	
training:	Epoch: [83][73/233]	Loss 0.0017 (0.0157)	
training:	Epoch: [83][74/233]	Loss 0.0017 (0.0155)	
training:	Epoch: [83][75/233]	Loss 0.0020 (0.0153)	
training:	Epoch: [83][76/233]	Loss 0.0815 (0.0162)	
training:	Epoch: [83][77/233]	Loss 0.0014 (0.0160)	
training:	Epoch: [83][78/233]	Loss 0.0028 (0.0158)	
training:	Epoch: [83][79/233]	Loss 0.1761 (0.0178)	
training:	Epoch: [83][80/233]	Loss 0.0025 (0.0176)	
training:	Epoch: [83][81/233]	Loss 0.0017 (0.0174)	
training:	Epoch: [83][82/233]	Loss 0.0016 (0.0173)	
training:	Epoch: [83][83/233]	Loss 0.0014 (0.0171)	
training:	Epoch: [83][84/233]	Loss 0.0018 (0.0169)	
training:	Epoch: [83][85/233]	Loss 0.0016 (0.0167)	
training:	Epoch: [83][86/233]	Loss 0.0020 (0.0165)	
training:	Epoch: [83][87/233]	Loss 0.0015 (0.0164)	
training:	Epoch: [83][88/233]	Loss 0.0018 (0.0162)	
training:	Epoch: [83][89/233]	Loss 0.0061 (0.0161)	
training:	Epoch: [83][90/233]	Loss 0.0035 (0.0159)	
training:	Epoch: [83][91/233]	Loss 0.0030 (0.0158)	
training:	Epoch: [83][92/233]	Loss 0.0012 (0.0156)	
training:	Epoch: [83][93/233]	Loss 0.0016 (0.0155)	
training:	Epoch: [83][94/233]	Loss 0.0019 (0.0153)	
training:	Epoch: [83][95/233]	Loss 0.0015 (0.0152)	
training:	Epoch: [83][96/233]	Loss 0.0017 (0.0151)	
training:	Epoch: [83][97/233]	Loss 0.0015 (0.0149)	
training:	Epoch: [83][98/233]	Loss 0.0030 (0.0148)	
training:	Epoch: [83][99/233]	Loss 0.0029 (0.0147)	
training:	Epoch: [83][100/233]	Loss 0.0018 (0.0145)	
training:	Epoch: [83][101/233]	Loss 0.0014 (0.0144)	
training:	Epoch: [83][102/233]	Loss 0.0047 (0.0143)	
training:	Epoch: [83][103/233]	Loss 0.0518 (0.0147)	
training:	Epoch: [83][104/233]	Loss 0.0016 (0.0146)	
training:	Epoch: [83][105/233]	Loss 0.0016 (0.0144)	
training:	Epoch: [83][106/233]	Loss 0.0035 (0.0143)	
training:	Epoch: [83][107/233]	Loss 0.0013 (0.0142)	
training:	Epoch: [83][108/233]	Loss 0.0032 (0.0141)	
training:	Epoch: [83][109/233]	Loss 0.0014 (0.0140)	
training:	Epoch: [83][110/233]	Loss 0.0025 (0.0139)	
training:	Epoch: [83][111/233]	Loss 0.0018 (0.0138)	
training:	Epoch: [83][112/233]	Loss 0.0030 (0.0137)	
training:	Epoch: [83][113/233]	Loss 0.2180 (0.0155)	
training:	Epoch: [83][114/233]	Loss 0.1030 (0.0163)	
training:	Epoch: [83][115/233]	Loss 0.0033 (0.0161)	
training:	Epoch: [83][116/233]	Loss 0.0016 (0.0160)	
training:	Epoch: [83][117/233]	Loss 0.0019 (0.0159)	
training:	Epoch: [83][118/233]	Loss 0.0017 (0.0158)	
training:	Epoch: [83][119/233]	Loss 0.0018 (0.0157)	
training:	Epoch: [83][120/233]	Loss 0.0013 (0.0155)	
training:	Epoch: [83][121/233]	Loss 0.0023 (0.0154)	
training:	Epoch: [83][122/233]	Loss 0.0023 (0.0153)	
training:	Epoch: [83][123/233]	Loss 0.0012 (0.0152)	
training:	Epoch: [83][124/233]	Loss 0.0042 (0.0151)	
training:	Epoch: [83][125/233]	Loss 0.0016 (0.0150)	
training:	Epoch: [83][126/233]	Loss 0.0012 (0.0149)	
training:	Epoch: [83][127/233]	Loss 0.0028 (0.0148)	
training:	Epoch: [83][128/233]	Loss 0.0136 (0.0148)	
training:	Epoch: [83][129/233]	Loss 0.0016 (0.0147)	
training:	Epoch: [83][130/233]	Loss 0.0021 (0.0146)	
training:	Epoch: [83][131/233]	Loss 0.0015 (0.0145)	
training:	Epoch: [83][132/233]	Loss 0.0019 (0.0144)	
training:	Epoch: [83][133/233]	Loss 0.0079 (0.0144)	
training:	Epoch: [83][134/233]	Loss 0.0015 (0.0143)	
training:	Epoch: [83][135/233]	Loss 0.0708 (0.0147)	
training:	Epoch: [83][136/233]	Loss 0.0032 (0.0146)	
training:	Epoch: [83][137/233]	Loss 0.0047 (0.0145)	
training:	Epoch: [83][138/233]	Loss 0.0016 (0.0144)	
training:	Epoch: [83][139/233]	Loss 0.0020 (0.0143)	
training:	Epoch: [83][140/233]	Loss 0.0083 (0.0143)	
training:	Epoch: [83][141/233]	Loss 0.0028 (0.0142)	
training:	Epoch: [83][142/233]	Loss 0.0134 (0.0142)	
training:	Epoch: [83][143/233]	Loss 0.0029 (0.0141)	
training:	Epoch: [83][144/233]	Loss 0.0018 (0.0140)	
training:	Epoch: [83][145/233]	Loss 0.0023 (0.0140)	
training:	Epoch: [83][146/233]	Loss 0.0279 (0.0141)	
training:	Epoch: [83][147/233]	Loss 0.0017 (0.0140)	
training:	Epoch: [83][148/233]	Loss 0.0050 (0.0139)	
training:	Epoch: [83][149/233]	Loss 0.0017 (0.0138)	
training:	Epoch: [83][150/233]	Loss 0.0034 (0.0138)	
training:	Epoch: [83][151/233]	Loss 0.0020 (0.0137)	
training:	Epoch: [83][152/233]	Loss 0.0712 (0.0141)	
training:	Epoch: [83][153/233]	Loss 0.0014 (0.0140)	
training:	Epoch: [83][154/233]	Loss 0.0090 (0.0139)	
training:	Epoch: [83][155/233]	Loss 0.0020 (0.0139)	
training:	Epoch: [83][156/233]	Loss 0.0017 (0.0138)	
training:	Epoch: [83][157/233]	Loss 0.1648 (0.0148)	
training:	Epoch: [83][158/233]	Loss 0.0014 (0.0147)	
training:	Epoch: [83][159/233]	Loss 0.0017 (0.0146)	
training:	Epoch: [83][160/233]	Loss 0.0022 (0.0145)	
training:	Epoch: [83][161/233]	Loss 0.0016 (0.0144)	
training:	Epoch: [83][162/233]	Loss 0.0014 (0.0143)	
training:	Epoch: [83][163/233]	Loss 0.0040 (0.0143)	
training:	Epoch: [83][164/233]	Loss 0.0023 (0.0142)	
training:	Epoch: [83][165/233]	Loss 0.0019 (0.0141)	
training:	Epoch: [83][166/233]	Loss 0.0016 (0.0141)	
training:	Epoch: [83][167/233]	Loss 0.0033 (0.0140)	
training:	Epoch: [83][168/233]	Loss 0.0025 (0.0139)	
training:	Epoch: [83][169/233]	Loss 0.0328 (0.0140)	
training:	Epoch: [83][170/233]	Loss 0.0023 (0.0140)	
training:	Epoch: [83][171/233]	Loss 0.0020 (0.0139)	
training:	Epoch: [83][172/233]	Loss 0.0017 (0.0138)	
training:	Epoch: [83][173/233]	Loss 0.0055 (0.0138)	
training:	Epoch: [83][174/233]	Loss 0.0013 (0.0137)	
training:	Epoch: [83][175/233]	Loss 0.0015 (0.0136)	
training:	Epoch: [83][176/233]	Loss 0.0024 (0.0136)	
training:	Epoch: [83][177/233]	Loss 0.0014 (0.0135)	
training:	Epoch: [83][178/233]	Loss 0.0049 (0.0135)	
training:	Epoch: [83][179/233]	Loss 0.0022 (0.0134)	
training:	Epoch: [83][180/233]	Loss 0.0022 (0.0133)	
training:	Epoch: [83][181/233]	Loss 0.1396 (0.0140)	
training:	Epoch: [83][182/233]	Loss 0.0017 (0.0140)	
training:	Epoch: [83][183/233]	Loss 0.0317 (0.0141)	
training:	Epoch: [83][184/233]	Loss 0.0017 (0.0140)	
training:	Epoch: [83][185/233]	Loss 0.1090 (0.0145)	
training:	Epoch: [83][186/233]	Loss 0.2359 (0.0157)	
training:	Epoch: [83][187/233]	Loss 0.0020 (0.0156)	
training:	Epoch: [83][188/233]	Loss 0.0026 (0.0156)	
training:	Epoch: [83][189/233]	Loss 0.0018 (0.0155)	
training:	Epoch: [83][190/233]	Loss 0.0020 (0.0154)	
training:	Epoch: [83][191/233]	Loss 0.0079 (0.0154)	
training:	Epoch: [83][192/233]	Loss 0.0067 (0.0153)	
training:	Epoch: [83][193/233]	Loss 0.0052 (0.0153)	
training:	Epoch: [83][194/233]	Loss 0.0161 (0.0153)	
training:	Epoch: [83][195/233]	Loss 0.0107 (0.0153)	
training:	Epoch: [83][196/233]	Loss 0.0037 (0.0152)	
training:	Epoch: [83][197/233]	Loss 0.0041 (0.0151)	
training:	Epoch: [83][198/233]	Loss 0.0131 (0.0151)	
training:	Epoch: [83][199/233]	Loss 0.0078 (0.0151)	
training:	Epoch: [83][200/233]	Loss 0.0078 (0.0151)	
training:	Epoch: [83][201/233]	Loss 0.0290 (0.0151)	
training:	Epoch: [83][202/233]	Loss 0.0675 (0.0154)	
training:	Epoch: [83][203/233]	Loss 0.0085 (0.0154)	
training:	Epoch: [83][204/233]	Loss 0.0015 (0.0153)	
training:	Epoch: [83][205/233]	Loss 0.0457 (0.0154)	
training:	Epoch: [83][206/233]	Loss 0.0021 (0.0154)	
training:	Epoch: [83][207/233]	Loss 0.0013 (0.0153)	
training:	Epoch: [83][208/233]	Loss 0.0024 (0.0152)	
training:	Epoch: [83][209/233]	Loss 0.0028 (0.0152)	
training:	Epoch: [83][210/233]	Loss 0.0050 (0.0151)	
training:	Epoch: [83][211/233]	Loss 0.0017 (0.0151)	
training:	Epoch: [83][212/233]	Loss 0.0015 (0.0150)	
training:	Epoch: [83][213/233]	Loss 0.1249 (0.0155)	
training:	Epoch: [83][214/233]	Loss 0.0048 (0.0155)	
training:	Epoch: [83][215/233]	Loss 0.0031 (0.0154)	
training:	Epoch: [83][216/233]	Loss 0.0024 (0.0154)	
training:	Epoch: [83][217/233]	Loss 0.0020 (0.0153)	
training:	Epoch: [83][218/233]	Loss 0.0032 (0.0152)	
training:	Epoch: [83][219/233]	Loss 0.0020 (0.0152)	
training:	Epoch: [83][220/233]	Loss 0.0029 (0.0151)	
training:	Epoch: [83][221/233]	Loss 0.0032 (0.0151)	
training:	Epoch: [83][222/233]	Loss 0.0121 (0.0151)	
training:	Epoch: [83][223/233]	Loss 0.0035 (0.0150)	
training:	Epoch: [83][224/233]	Loss 0.0045 (0.0150)	
training:	Epoch: [83][225/233]	Loss 0.0052 (0.0149)	
training:	Epoch: [83][226/233]	Loss 0.0384 (0.0150)	
training:	Epoch: [83][227/233]	Loss 0.0032 (0.0150)	
training:	Epoch: [83][228/233]	Loss 0.0042 (0.0149)	
training:	Epoch: [83][229/233]	Loss 0.0016 (0.0149)	
training:	Epoch: [83][230/233]	Loss 0.0038 (0.0148)	
training:	Epoch: [83][231/233]	Loss 0.0017 (0.0147)	
training:	Epoch: [83][232/233]	Loss 0.0017 (0.0147)	
training:	Epoch: [83][233/233]	Loss 0.0014 (0.0146)	
Training:	 Loss: 0.0146

Training:	 ACC: 0.9993 0.9993 0.9995 0.9992
Validation:	 ACC: 0.7873 0.7871 0.7814 0.7933
Validation:	 Best_BACC: 0.8162 0.8154 0.7998 0.8326
Validation:	 Loss: 1.0586
Pretraining:	Epoch 84/200
----------
training:	Epoch: [84][1/233]	Loss 0.0016 (0.0016)	
training:	Epoch: [84][2/233]	Loss 0.0014 (0.0015)	
training:	Epoch: [84][3/233]	Loss 0.0096 (0.0042)	
training:	Epoch: [84][4/233]	Loss 0.0019 (0.0036)	
training:	Epoch: [84][5/233]	Loss 0.0014 (0.0032)	
training:	Epoch: [84][6/233]	Loss 0.0020 (0.0030)	
training:	Epoch: [84][7/233]	Loss 0.0031 (0.0030)	
training:	Epoch: [84][8/233]	Loss 0.0029 (0.0030)	
training:	Epoch: [84][9/233]	Loss 0.0175 (0.0046)	
training:	Epoch: [84][10/233]	Loss 0.0016 (0.0043)	
training:	Epoch: [84][11/233]	Loss 0.0021 (0.0041)	
training:	Epoch: [84][12/233]	Loss 0.0013 (0.0039)	
training:	Epoch: [84][13/233]	Loss 0.0024 (0.0038)	
training:	Epoch: [84][14/233]	Loss 0.0012 (0.0036)	
training:	Epoch: [84][15/233]	Loss 0.0279 (0.0052)	
training:	Epoch: [84][16/233]	Loss 0.0019 (0.0050)	
training:	Epoch: [84][17/233]	Loss 0.0023 (0.0048)	
training:	Epoch: [84][18/233]	Loss 0.0025 (0.0047)	
training:	Epoch: [84][19/233]	Loss 0.0038 (0.0047)	
training:	Epoch: [84][20/233]	Loss 0.0032 (0.0046)	
training:	Epoch: [84][21/233]	Loss 0.0024 (0.0045)	
training:	Epoch: [84][22/233]	Loss 0.0137 (0.0049)	
training:	Epoch: [84][23/233]	Loss 0.0214 (0.0056)	
training:	Epoch: [84][24/233]	Loss 0.0014 (0.0054)	
training:	Epoch: [84][25/233]	Loss 0.0023 (0.0053)	
training:	Epoch: [84][26/233]	Loss 0.0017 (0.0052)	
training:	Epoch: [84][27/233]	Loss 0.0017 (0.0050)	
training:	Epoch: [84][28/233]	Loss 0.0015 (0.0049)	
training:	Epoch: [84][29/233]	Loss 0.0016 (0.0048)	
training:	Epoch: [84][30/233]	Loss 0.0014 (0.0047)	
training:	Epoch: [84][31/233]	Loss 0.0013 (0.0046)	
training:	Epoch: [84][32/233]	Loss 0.0017 (0.0045)	
training:	Epoch: [84][33/233]	Loss 0.0048 (0.0045)	
training:	Epoch: [84][34/233]	Loss 0.0018 (0.0044)	
training:	Epoch: [84][35/233]	Loss 0.0151 (0.0047)	
training:	Epoch: [84][36/233]	Loss 0.0017 (0.0046)	
training:	Epoch: [84][37/233]	Loss 0.0031 (0.0046)	
training:	Epoch: [84][38/233]	Loss 0.0019 (0.0045)	
training:	Epoch: [84][39/233]	Loss 0.0021 (0.0045)	
training:	Epoch: [84][40/233]	Loss 0.0032 (0.0044)	
training:	Epoch: [84][41/233]	Loss 0.0016 (0.0044)	
training:	Epoch: [84][42/233]	Loss 0.0021 (0.0043)	
training:	Epoch: [84][43/233]	Loss 0.0013 (0.0042)	
training:	Epoch: [84][44/233]	Loss 0.0409 (0.0051)	
training:	Epoch: [84][45/233]	Loss 0.0025 (0.0050)	
training:	Epoch: [84][46/233]	Loss 0.0013 (0.0049)	
training:	Epoch: [84][47/233]	Loss 0.0014 (0.0049)	
training:	Epoch: [84][48/233]	Loss 0.0122 (0.0050)	
training:	Epoch: [84][49/233]	Loss 0.0043 (0.0050)	
training:	Epoch: [84][50/233]	Loss 0.0016 (0.0049)	
training:	Epoch: [84][51/233]	Loss 0.0020 (0.0049)	
training:	Epoch: [84][52/233]	Loss 0.0019 (0.0048)	
training:	Epoch: [84][53/233]	Loss 0.0025 (0.0048)	
training:	Epoch: [84][54/233]	Loss 0.0093 (0.0049)	
training:	Epoch: [84][55/233]	Loss 0.0057 (0.0049)	
training:	Epoch: [84][56/233]	Loss 0.0014 (0.0048)	
training:	Epoch: [84][57/233]	Loss 0.0015 (0.0048)	
training:	Epoch: [84][58/233]	Loss 0.0551 (0.0056)	
training:	Epoch: [84][59/233]	Loss 0.0018 (0.0056)	
training:	Epoch: [84][60/233]	Loss 0.0016 (0.0055)	
training:	Epoch: [84][61/233]	Loss 0.0016 (0.0054)	
training:	Epoch: [84][62/233]	Loss 0.0056 (0.0054)	
training:	Epoch: [84][63/233]	Loss 0.0014 (0.0054)	
training:	Epoch: [84][64/233]	Loss 0.0557 (0.0062)	
training:	Epoch: [84][65/233]	Loss 0.0018 (0.0061)	
training:	Epoch: [84][66/233]	Loss 0.0022 (0.0060)	
training:	Epoch: [84][67/233]	Loss 0.0014 (0.0060)	
training:	Epoch: [84][68/233]	Loss 0.0012 (0.0059)	
training:	Epoch: [84][69/233]	Loss 0.0276 (0.0062)	
training:	Epoch: [84][70/233]	Loss 0.0013 (0.0061)	
training:	Epoch: [84][71/233]	Loss 0.0014 (0.0061)	
training:	Epoch: [84][72/233]	Loss 0.0020 (0.0060)	
training:	Epoch: [84][73/233]	Loss 0.0865 (0.0071)	
training:	Epoch: [84][74/233]	Loss 0.0017 (0.0070)	
training:	Epoch: [84][75/233]	Loss 0.0020 (0.0070)	
training:	Epoch: [84][76/233]	Loss 0.0033 (0.0069)	
training:	Epoch: [84][77/233]	Loss 0.0033 (0.0069)	
training:	Epoch: [84][78/233]	Loss 0.0022 (0.0068)	
training:	Epoch: [84][79/233]	Loss 0.0018 (0.0068)	
training:	Epoch: [84][80/233]	Loss 0.0016 (0.0067)	
training:	Epoch: [84][81/233]	Loss 0.1786 (0.0088)	
training:	Epoch: [84][82/233]	Loss 0.0020 (0.0087)	
training:	Epoch: [84][83/233]	Loss 0.0014 (0.0086)	
training:	Epoch: [84][84/233]	Loss 0.0019 (0.0086)	
training:	Epoch: [84][85/233]	Loss 0.0015 (0.0085)	
training:	Epoch: [84][86/233]	Loss 0.0832 (0.0093)	
training:	Epoch: [84][87/233]	Loss 0.0027 (0.0093)	
training:	Epoch: [84][88/233]	Loss 0.0013 (0.0092)	
training:	Epoch: [84][89/233]	Loss 0.0019 (0.0091)	
training:	Epoch: [84][90/233]	Loss 0.0014 (0.0090)	
training:	Epoch: [84][91/233]	Loss 0.0092 (0.0090)	
training:	Epoch: [84][92/233]	Loss 0.0015 (0.0089)	
training:	Epoch: [84][93/233]	Loss 0.0748 (0.0096)	
training:	Epoch: [84][94/233]	Loss 0.0032 (0.0096)	
training:	Epoch: [84][95/233]	Loss 0.0055 (0.0095)	
training:	Epoch: [84][96/233]	Loss 0.0027 (0.0095)	
training:	Epoch: [84][97/233]	Loss 0.0039 (0.0094)	
training:	Epoch: [84][98/233]	Loss 0.0070 (0.0094)	
training:	Epoch: [84][99/233]	Loss 0.0019 (0.0093)	
training:	Epoch: [84][100/233]	Loss 0.0016 (0.0092)	
training:	Epoch: [84][101/233]	Loss 0.0023 (0.0092)	
training:	Epoch: [84][102/233]	Loss 0.0032 (0.0091)	
training:	Epoch: [84][103/233]	Loss 0.0124 (0.0091)	
training:	Epoch: [84][104/233]	Loss 0.0057 (0.0091)	
training:	Epoch: [84][105/233]	Loss 0.0017 (0.0090)	
training:	Epoch: [84][106/233]	Loss 0.0479 (0.0094)	
training:	Epoch: [84][107/233]	Loss 0.0021 (0.0093)	
training:	Epoch: [84][108/233]	Loss 0.0016 (0.0093)	
training:	Epoch: [84][109/233]	Loss 0.0014 (0.0092)	
training:	Epoch: [84][110/233]	Loss 0.0037 (0.0091)	
training:	Epoch: [84][111/233]	Loss 0.0018 (0.0091)	
training:	Epoch: [84][112/233]	Loss 0.0060 (0.0090)	
training:	Epoch: [84][113/233]	Loss 0.2110 (0.0108)	
training:	Epoch: [84][114/233]	Loss 0.0032 (0.0108)	
training:	Epoch: [84][115/233]	Loss 0.0014 (0.0107)	
training:	Epoch: [84][116/233]	Loss 0.0016 (0.0106)	
training:	Epoch: [84][117/233]	Loss 0.0034 (0.0105)	
training:	Epoch: [84][118/233]	Loss 0.0017 (0.0105)	
training:	Epoch: [84][119/233]	Loss 0.0045 (0.0104)	
training:	Epoch: [84][120/233]	Loss 0.0021 (0.0103)	
training:	Epoch: [84][121/233]	Loss 0.0022 (0.0103)	
training:	Epoch: [84][122/233]	Loss 0.0014 (0.0102)	
training:	Epoch: [84][123/233]	Loss 0.0016 (0.0101)	
training:	Epoch: [84][124/233]	Loss 0.0029 (0.0101)	
training:	Epoch: [84][125/233]	Loss 0.0027 (0.0100)	
training:	Epoch: [84][126/233]	Loss 0.0013 (0.0099)	
training:	Epoch: [84][127/233]	Loss 0.0051 (0.0099)	
training:	Epoch: [84][128/233]	Loss 0.0030 (0.0099)	
training:	Epoch: [84][129/233]	Loss 0.0012 (0.0098)	
training:	Epoch: [84][130/233]	Loss 0.0017 (0.0097)	
training:	Epoch: [84][131/233]	Loss 0.0013 (0.0097)	
training:	Epoch: [84][132/233]	Loss 0.0079 (0.0096)	
training:	Epoch: [84][133/233]	Loss 0.0014 (0.0096)	
training:	Epoch: [84][134/233]	Loss 0.0015 (0.0095)	
training:	Epoch: [84][135/233]	Loss 0.0047 (0.0095)	
training:	Epoch: [84][136/233]	Loss 0.0013 (0.0094)	
training:	Epoch: [84][137/233]	Loss 0.0013 (0.0094)	
training:	Epoch: [84][138/233]	Loss 0.0018 (0.0093)	
training:	Epoch: [84][139/233]	Loss 0.0020 (0.0093)	
training:	Epoch: [84][140/233]	Loss 0.0011 (0.0092)	
training:	Epoch: [84][141/233]	Loss 0.0013 (0.0091)	
training:	Epoch: [84][142/233]	Loss 0.0033 (0.0091)	
training:	Epoch: [84][143/233]	Loss 0.0018 (0.0091)	
training:	Epoch: [84][144/233]	Loss 0.0037 (0.0090)	
training:	Epoch: [84][145/233]	Loss 0.0031 (0.0090)	
training:	Epoch: [84][146/233]	Loss 0.0013 (0.0089)	
training:	Epoch: [84][147/233]	Loss 0.0013 (0.0089)	
training:	Epoch: [84][148/233]	Loss 0.0015 (0.0088)	
training:	Epoch: [84][149/233]	Loss 0.0019 (0.0088)	
training:	Epoch: [84][150/233]	Loss 0.0019 (0.0087)	
training:	Epoch: [84][151/233]	Loss 0.0026 (0.0087)	
training:	Epoch: [84][152/233]	Loss 0.0017 (0.0086)	
training:	Epoch: [84][153/233]	Loss 0.0018 (0.0086)	
training:	Epoch: [84][154/233]	Loss 0.0014 (0.0086)	
training:	Epoch: [84][155/233]	Loss 0.0030 (0.0085)	
training:	Epoch: [84][156/233]	Loss 0.0031 (0.0085)	
training:	Epoch: [84][157/233]	Loss 0.0017 (0.0084)	
training:	Epoch: [84][158/233]	Loss 0.0019 (0.0084)	
training:	Epoch: [84][159/233]	Loss 0.0017 (0.0084)	
training:	Epoch: [84][160/233]	Loss 0.0057 (0.0083)	
training:	Epoch: [84][161/233]	Loss 0.0017 (0.0083)	
training:	Epoch: [84][162/233]	Loss 0.0154 (0.0083)	
training:	Epoch: [84][163/233]	Loss 0.0046 (0.0083)	
training:	Epoch: [84][164/233]	Loss 0.0025 (0.0083)	
training:	Epoch: [84][165/233]	Loss 0.2082 (0.0095)	
training:	Epoch: [84][166/233]	Loss 0.0016 (0.0094)	
training:	Epoch: [84][167/233]	Loss 0.0019 (0.0094)	
training:	Epoch: [84][168/233]	Loss 0.0036 (0.0094)	
training:	Epoch: [84][169/233]	Loss 0.0019 (0.0093)	
training:	Epoch: [84][170/233]	Loss 0.0036 (0.0093)	
training:	Epoch: [84][171/233]	Loss 0.0024 (0.0092)	
training:	Epoch: [84][172/233]	Loss 0.0013 (0.0092)	
training:	Epoch: [84][173/233]	Loss 0.0071 (0.0092)	
training:	Epoch: [84][174/233]	Loss 0.0028 (0.0092)	
training:	Epoch: [84][175/233]	Loss 0.0042 (0.0091)	
training:	Epoch: [84][176/233]	Loss 0.0017 (0.0091)	
training:	Epoch: [84][177/233]	Loss 0.0021 (0.0090)	
training:	Epoch: [84][178/233]	Loss 0.0027 (0.0090)	
training:	Epoch: [84][179/233]	Loss 0.0015 (0.0090)	
training:	Epoch: [84][180/233]	Loss 0.0012 (0.0089)	
training:	Epoch: [84][181/233]	Loss 0.0013 (0.0089)	
training:	Epoch: [84][182/233]	Loss 0.0016 (0.0088)	
training:	Epoch: [84][183/233]	Loss 0.0013 (0.0088)	
training:	Epoch: [84][184/233]	Loss 0.0014 (0.0088)	
training:	Epoch: [84][185/233]	Loss 0.1186 (0.0094)	
training:	Epoch: [84][186/233]	Loss 0.0018 (0.0093)	
training:	Epoch: [84][187/233]	Loss 0.2191 (0.0104)	
training:	Epoch: [84][188/233]	Loss 0.1963 (0.0114)	
training:	Epoch: [84][189/233]	Loss 0.0019 (0.0114)	
training:	Epoch: [84][190/233]	Loss 0.0035 (0.0113)	
training:	Epoch: [84][191/233]	Loss 0.0029 (0.0113)	
training:	Epoch: [84][192/233]	Loss 0.0018 (0.0112)	
training:	Epoch: [84][193/233]	Loss 0.0024 (0.0112)	
training:	Epoch: [84][194/233]	Loss 0.0018 (0.0111)	
training:	Epoch: [84][195/233]	Loss 0.0056 (0.0111)	
training:	Epoch: [84][196/233]	Loss 0.0019 (0.0111)	
training:	Epoch: [84][197/233]	Loss 0.0015 (0.0110)	
training:	Epoch: [84][198/233]	Loss 0.0014 (0.0110)	
training:	Epoch: [84][199/233]	Loss 0.0985 (0.0114)	
training:	Epoch: [84][200/233]	Loss 0.0014 (0.0114)	
training:	Epoch: [84][201/233]	Loss 0.0027 (0.0113)	
training:	Epoch: [84][202/233]	Loss 0.0033 (0.0113)	
training:	Epoch: [84][203/233]	Loss 0.0595 (0.0115)	
training:	Epoch: [84][204/233]	Loss 0.0018 (0.0115)	
training:	Epoch: [84][205/233]	Loss 0.0043 (0.0114)	
training:	Epoch: [84][206/233]	Loss 0.0034 (0.0114)	
training:	Epoch: [84][207/233]	Loss 0.0034 (0.0114)	
training:	Epoch: [84][208/233]	Loss 0.0026 (0.0113)	
training:	Epoch: [84][209/233]	Loss 0.0013 (0.0113)	
training:	Epoch: [84][210/233]	Loss 0.0013 (0.0112)	
training:	Epoch: [84][211/233]	Loss 0.0117 (0.0112)	
training:	Epoch: [84][212/233]	Loss 0.0015 (0.0112)	
training:	Epoch: [84][213/233]	Loss 0.1381 (0.0118)	
training:	Epoch: [84][214/233]	Loss 0.0018 (0.0117)	
training:	Epoch: [84][215/233]	Loss 0.0013 (0.0117)	
training:	Epoch: [84][216/233]	Loss 0.0032 (0.0116)	
training:	Epoch: [84][217/233]	Loss 0.0075 (0.0116)	
training:	Epoch: [84][218/233]	Loss 0.0022 (0.0116)	
training:	Epoch: [84][219/233]	Loss 0.0017 (0.0115)	
training:	Epoch: [84][220/233]	Loss 0.0017 (0.0115)	
training:	Epoch: [84][221/233]	Loss 0.0012 (0.0114)	
training:	Epoch: [84][222/233]	Loss 0.0034 (0.0114)	
training:	Epoch: [84][223/233]	Loss 0.0014 (0.0114)	
training:	Epoch: [84][224/233]	Loss 0.0658 (0.0116)	
training:	Epoch: [84][225/233]	Loss 0.0043 (0.0116)	
training:	Epoch: [84][226/233]	Loss 0.0014 (0.0115)	
training:	Epoch: [84][227/233]	Loss 0.0022 (0.0115)	
training:	Epoch: [84][228/233]	Loss 0.0014 (0.0114)	
training:	Epoch: [84][229/233]	Loss 0.0027 (0.0114)	
training:	Epoch: [84][230/233]	Loss 0.0016 (0.0114)	
training:	Epoch: [84][231/233]	Loss 0.0021 (0.0113)	
training:	Epoch: [84][232/233]	Loss 0.0018 (0.0113)	
training:	Epoch: [84][233/233]	Loss 0.0015 (0.0112)	
Training:	 Loss: 0.0112

Training:	 ACC: 0.9993 0.9993 0.9995 0.9992
Validation:	 ACC: 0.7884 0.7876 0.7712 0.8056
Validation:	 Best_BACC: 0.8162 0.8154 0.7998 0.8326
Validation:	 Loss: 1.0659
Pretraining:	Epoch 85/200
----------
training:	Epoch: [85][1/233]	Loss 0.0016 (0.0016)	
training:	Epoch: [85][2/233]	Loss 0.0033 (0.0025)	
training:	Epoch: [85][3/233]	Loss 0.0022 (0.0024)	
training:	Epoch: [85][4/233]	Loss 0.0025 (0.0024)	
training:	Epoch: [85][5/233]	Loss 0.0020 (0.0023)	
training:	Epoch: [85][6/233]	Loss 0.0014 (0.0022)	
training:	Epoch: [85][7/233]	Loss 0.0016 (0.0021)	
training:	Epoch: [85][8/233]	Loss 0.0014 (0.0020)	
training:	Epoch: [85][9/233]	Loss 0.0017 (0.0020)	
training:	Epoch: [85][10/233]	Loss 0.0023 (0.0020)	
training:	Epoch: [85][11/233]	Loss 0.0115 (0.0029)	
training:	Epoch: [85][12/233]	Loss 0.0029 (0.0029)	
training:	Epoch: [85][13/233]	Loss 0.1213 (0.0120)	
training:	Epoch: [85][14/233]	Loss 0.0026 (0.0113)	
training:	Epoch: [85][15/233]	Loss 0.0015 (0.0106)	
training:	Epoch: [85][16/233]	Loss 0.0015 (0.0101)	
training:	Epoch: [85][17/233]	Loss 0.0063 (0.0099)	
training:	Epoch: [85][18/233]	Loss 0.0020 (0.0094)	
training:	Epoch: [85][19/233]	Loss 0.0030 (0.0091)	
training:	Epoch: [85][20/233]	Loss 0.0013 (0.0087)	
training:	Epoch: [85][21/233]	Loss 0.0519 (0.0107)	
training:	Epoch: [85][22/233]	Loss 0.0015 (0.0103)	
training:	Epoch: [85][23/233]	Loss 0.0038 (0.0100)	
training:	Epoch: [85][24/233]	Loss 0.0025 (0.0097)	
training:	Epoch: [85][25/233]	Loss 0.0016 (0.0094)	
training:	Epoch: [85][26/233]	Loss 0.0050 (0.0092)	
training:	Epoch: [85][27/233]	Loss 0.0247 (0.0098)	
training:	Epoch: [85][28/233]	Loss 0.0030 (0.0096)	
training:	Epoch: [85][29/233]	Loss 0.0020 (0.0093)	
training:	Epoch: [85][30/233]	Loss 0.0409 (0.0104)	
training:	Epoch: [85][31/233]	Loss 0.0019 (0.0101)	
training:	Epoch: [85][32/233]	Loss 0.0081 (0.0100)	
training:	Epoch: [85][33/233]	Loss 0.0017 (0.0098)	
training:	Epoch: [85][34/233]	Loss 0.0032 (0.0096)	
training:	Epoch: [85][35/233]	Loss 0.0016 (0.0093)	
training:	Epoch: [85][36/233]	Loss 0.0020 (0.0091)	
training:	Epoch: [85][37/233]	Loss 0.0025 (0.0090)	
training:	Epoch: [85][38/233]	Loss 0.0019 (0.0088)	
training:	Epoch: [85][39/233]	Loss 0.0064 (0.0087)	
training:	Epoch: [85][40/233]	Loss 0.0063 (0.0086)	
training:	Epoch: [85][41/233]	Loss 0.0022 (0.0085)	
training:	Epoch: [85][42/233]	Loss 0.0034 (0.0084)	
training:	Epoch: [85][43/233]	Loss 0.0277 (0.0088)	
training:	Epoch: [85][44/233]	Loss 0.0167 (0.0090)	
training:	Epoch: [85][45/233]	Loss 0.0017 (0.0088)	
training:	Epoch: [85][46/233]	Loss 0.0014 (0.0087)	
training:	Epoch: [85][47/233]	Loss 0.0021 (0.0085)	
training:	Epoch: [85][48/233]	Loss 0.0082 (0.0085)	
training:	Epoch: [85][49/233]	Loss 0.0101 (0.0086)	
training:	Epoch: [85][50/233]	Loss 0.0018 (0.0084)	
training:	Epoch: [85][51/233]	Loss 0.0025 (0.0083)	
training:	Epoch: [85][52/233]	Loss 0.0018 (0.0082)	
training:	Epoch: [85][53/233]	Loss 0.0017 (0.0081)	
training:	Epoch: [85][54/233]	Loss 0.0028 (0.0080)	
training:	Epoch: [85][55/233]	Loss 0.0015 (0.0078)	
training:	Epoch: [85][56/233]	Loss 0.0015 (0.0077)	
training:	Epoch: [85][57/233]	Loss 0.0156 (0.0079)	
training:	Epoch: [85][58/233]	Loss 0.0119 (0.0079)	
training:	Epoch: [85][59/233]	Loss 0.0017 (0.0078)	
training:	Epoch: [85][60/233]	Loss 0.0105 (0.0079)	
training:	Epoch: [85][61/233]	Loss 0.0020 (0.0078)	
training:	Epoch: [85][62/233]	Loss 0.0015 (0.0077)	
training:	Epoch: [85][63/233]	Loss 0.0018 (0.0076)	
training:	Epoch: [85][64/233]	Loss 0.0041 (0.0075)	
training:	Epoch: [85][65/233]	Loss 0.0014 (0.0074)	
training:	Epoch: [85][66/233]	Loss 0.0022 (0.0074)	
training:	Epoch: [85][67/233]	Loss 0.0028 (0.0073)	
training:	Epoch: [85][68/233]	Loss 0.0025 (0.0072)	
training:	Epoch: [85][69/233]	Loss 0.0022 (0.0071)	
training:	Epoch: [85][70/233]	Loss 0.0012 (0.0071)	
training:	Epoch: [85][71/233]	Loss 0.0028 (0.0070)	
training:	Epoch: [85][72/233]	Loss 0.0076 (0.0070)	
training:	Epoch: [85][73/233]	Loss 0.0016 (0.0069)	
training:	Epoch: [85][74/233]	Loss 0.0016 (0.0069)	
training:	Epoch: [85][75/233]	Loss 0.0017 (0.0068)	
training:	Epoch: [85][76/233]	Loss 0.0023 (0.0067)	
training:	Epoch: [85][77/233]	Loss 0.0014 (0.0067)	
training:	Epoch: [85][78/233]	Loss 0.0025 (0.0066)	
training:	Epoch: [85][79/233]	Loss 0.0016 (0.0066)	
training:	Epoch: [85][80/233]	Loss 0.0083 (0.0066)	
training:	Epoch: [85][81/233]	Loss 0.0062 (0.0066)	
training:	Epoch: [85][82/233]	Loss 0.0015 (0.0065)	
training:	Epoch: [85][83/233]	Loss 0.0014 (0.0064)	
training:	Epoch: [85][84/233]	Loss 0.0022 (0.0064)	
training:	Epoch: [85][85/233]	Loss 0.0025 (0.0063)	
training:	Epoch: [85][86/233]	Loss 0.0014 (0.0063)	
training:	Epoch: [85][87/233]	Loss 0.0013 (0.0062)	
training:	Epoch: [85][88/233]	Loss 0.0012 (0.0062)	
training:	Epoch: [85][89/233]	Loss 0.0024 (0.0061)	
training:	Epoch: [85][90/233]	Loss 0.0064 (0.0061)	
training:	Epoch: [85][91/233]	Loss 0.0262 (0.0064)	
training:	Epoch: [85][92/233]	Loss 0.0013 (0.0063)	
training:	Epoch: [85][93/233]	Loss 0.0015 (0.0063)	
training:	Epoch: [85][94/233]	Loss 0.0015 (0.0062)	
training:	Epoch: [85][95/233]	Loss 0.0019 (0.0062)	
training:	Epoch: [85][96/233]	Loss 0.0017 (0.0061)	
training:	Epoch: [85][97/233]	Loss 0.0013 (0.0061)	
training:	Epoch: [85][98/233]	Loss 0.0024 (0.0060)	
training:	Epoch: [85][99/233]	Loss 0.0015 (0.0060)	
training:	Epoch: [85][100/233]	Loss 0.0014 (0.0059)	
training:	Epoch: [85][101/233]	Loss 0.0045 (0.0059)	
training:	Epoch: [85][102/233]	Loss 0.0018 (0.0059)	
training:	Epoch: [85][103/233]	Loss 0.0013 (0.0058)	
training:	Epoch: [85][104/233]	Loss 0.0027 (0.0058)	
training:	Epoch: [85][105/233]	Loss 0.0022 (0.0058)	
training:	Epoch: [85][106/233]	Loss 0.0031 (0.0057)	
training:	Epoch: [85][107/233]	Loss 0.0027 (0.0057)	
training:	Epoch: [85][108/233]	Loss 0.0013 (0.0057)	
training:	Epoch: [85][109/233]	Loss 0.0016 (0.0056)	
training:	Epoch: [85][110/233]	Loss 0.0048 (0.0056)	
training:	Epoch: [85][111/233]	Loss 0.0016 (0.0056)	
training:	Epoch: [85][112/233]	Loss 0.0012 (0.0056)	
training:	Epoch: [85][113/233]	Loss 0.0019 (0.0055)	
training:	Epoch: [85][114/233]	Loss 0.0013 (0.0055)	
training:	Epoch: [85][115/233]	Loss 0.0021 (0.0055)	
training:	Epoch: [85][116/233]	Loss 0.0052 (0.0055)	
training:	Epoch: [85][117/233]	Loss 0.0405 (0.0058)	
training:	Epoch: [85][118/233]	Loss 0.0016 (0.0057)	
training:	Epoch: [85][119/233]	Loss 0.0013 (0.0057)	
training:	Epoch: [85][120/233]	Loss 0.0026 (0.0057)	
training:	Epoch: [85][121/233]	Loss 0.0016 (0.0056)	
training:	Epoch: [85][122/233]	Loss 0.0015 (0.0056)	
training:	Epoch: [85][123/233]	Loss 0.0014 (0.0056)	
training:	Epoch: [85][124/233]	Loss 0.0013 (0.0055)	
training:	Epoch: [85][125/233]	Loss 0.0077 (0.0055)	
training:	Epoch: [85][126/233]	Loss 0.0013 (0.0055)	
training:	Epoch: [85][127/233]	Loss 0.0016 (0.0055)	
training:	Epoch: [85][128/233]	Loss 0.0018 (0.0054)	
training:	Epoch: [85][129/233]	Loss 0.0031 (0.0054)	
training:	Epoch: [85][130/233]	Loss 0.0012 (0.0054)	
training:	Epoch: [85][131/233]	Loss 0.0013 (0.0054)	
training:	Epoch: [85][132/233]	Loss 0.0014 (0.0053)	
training:	Epoch: [85][133/233]	Loss 0.0033 (0.0053)	
training:	Epoch: [85][134/233]	Loss 0.0394 (0.0056)	
training:	Epoch: [85][135/233]	Loss 0.0013 (0.0055)	
training:	Epoch: [85][136/233]	Loss 0.0065 (0.0055)	
training:	Epoch: [85][137/233]	Loss 0.0016 (0.0055)	
training:	Epoch: [85][138/233]	Loss 0.0013 (0.0055)	
training:	Epoch: [85][139/233]	Loss 0.0015 (0.0055)	
training:	Epoch: [85][140/233]	Loss 0.0011 (0.0054)	
training:	Epoch: [85][141/233]	Loss 0.0014 (0.0054)	
training:	Epoch: [85][142/233]	Loss 0.0013 (0.0054)	
training:	Epoch: [85][143/233]	Loss 0.2190 (0.0069)	
training:	Epoch: [85][144/233]	Loss 0.0160 (0.0069)	
training:	Epoch: [85][145/233]	Loss 0.0020 (0.0069)	
training:	Epoch: [85][146/233]	Loss 0.0049 (0.0069)	
training:	Epoch: [85][147/233]	Loss 0.0014 (0.0068)	
training:	Epoch: [85][148/233]	Loss 0.0017 (0.0068)	
training:	Epoch: [85][149/233]	Loss 0.0017 (0.0068)	
training:	Epoch: [85][150/233]	Loss 0.0034 (0.0068)	
training:	Epoch: [85][151/233]	Loss 0.0024 (0.0067)	
training:	Epoch: [85][152/233]	Loss 0.0018 (0.0067)	
training:	Epoch: [85][153/233]	Loss 0.0015 (0.0067)	
training:	Epoch: [85][154/233]	Loss 0.0013 (0.0066)	
training:	Epoch: [85][155/233]	Loss 0.0070 (0.0066)	
training:	Epoch: [85][156/233]	Loss 0.0014 (0.0066)	
training:	Epoch: [85][157/233]	Loss 0.0011 (0.0066)	
training:	Epoch: [85][158/233]	Loss 0.0013 (0.0065)	
training:	Epoch: [85][159/233]	Loss 0.0016 (0.0065)	
training:	Epoch: [85][160/233]	Loss 0.0016 (0.0065)	
training:	Epoch: [85][161/233]	Loss 0.0015 (0.0064)	
training:	Epoch: [85][162/233]	Loss 0.0526 (0.0067)	
training:	Epoch: [85][163/233]	Loss 0.0011 (0.0067)	
training:	Epoch: [85][164/233]	Loss 0.0061 (0.0067)	
training:	Epoch: [85][165/233]	Loss 0.0018 (0.0066)	
training:	Epoch: [85][166/233]	Loss 0.0013 (0.0066)	
training:	Epoch: [85][167/233]	Loss 0.0026 (0.0066)	
training:	Epoch: [85][168/233]	Loss 0.0015 (0.0066)	
training:	Epoch: [85][169/233]	Loss 0.0024 (0.0065)	
training:	Epoch: [85][170/233]	Loss 0.0012 (0.0065)	
training:	Epoch: [85][171/233]	Loss 0.0022 (0.0065)	
training:	Epoch: [85][172/233]	Loss 0.0024 (0.0065)	
training:	Epoch: [85][173/233]	Loss 0.0011 (0.0064)	
training:	Epoch: [85][174/233]	Loss 0.0034 (0.0064)	
training:	Epoch: [85][175/233]	Loss 0.0027 (0.0064)	
training:	Epoch: [85][176/233]	Loss 0.0020 (0.0064)	
training:	Epoch: [85][177/233]	Loss 0.0038 (0.0063)	
training:	Epoch: [85][178/233]	Loss 0.0012 (0.0063)	
training:	Epoch: [85][179/233]	Loss 0.0013 (0.0063)	
training:	Epoch: [85][180/233]	Loss 0.0013 (0.0063)	
training:	Epoch: [85][181/233]	Loss 0.0015 (0.0062)	
training:	Epoch: [85][182/233]	Loss 0.0014 (0.0062)	
training:	Epoch: [85][183/233]	Loss 0.0016 (0.0062)	
training:	Epoch: [85][184/233]	Loss 0.0021 (0.0062)	
training:	Epoch: [85][185/233]	Loss 0.0083 (0.0062)	
training:	Epoch: [85][186/233]	Loss 0.0017 (0.0061)	
training:	Epoch: [85][187/233]	Loss 0.0014 (0.0061)	
training:	Epoch: [85][188/233]	Loss 0.2302 (0.0073)	
training:	Epoch: [85][189/233]	Loss 0.0023 (0.0073)	
training:	Epoch: [85][190/233]	Loss 0.0012 (0.0073)	
training:	Epoch: [85][191/233]	Loss 0.0013 (0.0072)	
training:	Epoch: [85][192/233]	Loss 0.0027 (0.0072)	
training:	Epoch: [85][193/233]	Loss 0.0022 (0.0072)	
training:	Epoch: [85][194/233]	Loss 0.0297 (0.0073)	
training:	Epoch: [85][195/233]	Loss 0.0018 (0.0073)	
training:	Epoch: [85][196/233]	Loss 0.0014 (0.0072)	
training:	Epoch: [85][197/233]	Loss 0.0019 (0.0072)	
training:	Epoch: [85][198/233]	Loss 0.0014 (0.0072)	
training:	Epoch: [85][199/233]	Loss 0.0045 (0.0072)	
training:	Epoch: [85][200/233]	Loss 0.1386 (0.0078)	
training:	Epoch: [85][201/233]	Loss 0.0014 (0.0078)	
training:	Epoch: [85][202/233]	Loss 0.0012 (0.0078)	
training:	Epoch: [85][203/233]	Loss 0.0040 (0.0077)	
training:	Epoch: [85][204/233]	Loss 0.0018 (0.0077)	
training:	Epoch: [85][205/233]	Loss 0.0011 (0.0077)	
training:	Epoch: [85][206/233]	Loss 0.0026 (0.0077)	
training:	Epoch: [85][207/233]	Loss 0.0014 (0.0076)	
training:	Epoch: [85][208/233]	Loss 0.0014 (0.0076)	
training:	Epoch: [85][209/233]	Loss 0.0026 (0.0076)	
training:	Epoch: [85][210/233]	Loss 0.0012 (0.0075)	
training:	Epoch: [85][211/233]	Loss 0.0019 (0.0075)	
training:	Epoch: [85][212/233]	Loss 0.0011 (0.0075)	
training:	Epoch: [85][213/233]	Loss 0.0012 (0.0075)	
training:	Epoch: [85][214/233]	Loss 0.0347 (0.0076)	
training:	Epoch: [85][215/233]	Loss 0.0011 (0.0075)	
training:	Epoch: [85][216/233]	Loss 0.0013 (0.0075)	
training:	Epoch: [85][217/233]	Loss 0.0042 (0.0075)	
training:	Epoch: [85][218/233]	Loss 0.0014 (0.0075)	
training:	Epoch: [85][219/233]	Loss 0.0012 (0.0074)	
training:	Epoch: [85][220/233]	Loss 0.0016 (0.0074)	
training:	Epoch: [85][221/233]	Loss 0.0011 (0.0074)	
training:	Epoch: [85][222/233]	Loss 0.0013 (0.0074)	
training:	Epoch: [85][223/233]	Loss 0.0035 (0.0073)	
training:	Epoch: [85][224/233]	Loss 0.1828 (0.0081)	
training:	Epoch: [85][225/233]	Loss 0.2111 (0.0090)	
training:	Epoch: [85][226/233]	Loss 0.0012 (0.0090)	
training:	Epoch: [85][227/233]	Loss 0.0012 (0.0090)	
training:	Epoch: [85][228/233]	Loss 0.0022 (0.0089)	
training:	Epoch: [85][229/233]	Loss 0.0012 (0.0089)	
training:	Epoch: [85][230/233]	Loss 0.0016 (0.0089)	
training:	Epoch: [85][231/233]	Loss 0.1299 (0.0094)	
training:	Epoch: [85][232/233]	Loss 0.0013 (0.0094)	
training:	Epoch: [85][233/233]	Loss 0.0015 (0.0093)	
Training:	 Loss: 0.0093

Training:	 ACC: 0.9995 0.9995 0.9995 0.9994
Validation:	 ACC: 0.7908 0.7913 0.8018 0.7798
Validation:	 Best_BACC: 0.8162 0.8154 0.7998 0.8326
Validation:	 Loss: 1.0877
Pretraining:	Epoch 86/200
----------
training:	Epoch: [86][1/233]	Loss 0.0120 (0.0120)	
training:	Epoch: [86][2/233]	Loss 0.0012 (0.0066)	
training:	Epoch: [86][3/233]	Loss 0.0015 (0.0049)	
training:	Epoch: [86][4/233]	Loss 0.0035 (0.0045)	
training:	Epoch: [86][5/233]	Loss 0.0572 (0.0151)	
training:	Epoch: [86][6/233]	Loss 0.0064 (0.0136)	
training:	Epoch: [86][7/233]	Loss 0.0017 (0.0119)	
training:	Epoch: [86][8/233]	Loss 0.0014 (0.0106)	
training:	Epoch: [86][9/233]	Loss 0.0020 (0.0096)	
training:	Epoch: [86][10/233]	Loss 0.0015 (0.0088)	
training:	Epoch: [86][11/233]	Loss 0.0012 (0.0081)	
training:	Epoch: [86][12/233]	Loss 0.0013 (0.0076)	
training:	Epoch: [86][13/233]	Loss 0.0021 (0.0072)	
training:	Epoch: [86][14/233]	Loss 0.0015 (0.0067)	
training:	Epoch: [86][15/233]	Loss 0.0016 (0.0064)	
training:	Epoch: [86][16/233]	Loss 0.0014 (0.0061)	
training:	Epoch: [86][17/233]	Loss 0.0020 (0.0058)	
training:	Epoch: [86][18/233]	Loss 0.0077 (0.0060)	
training:	Epoch: [86][19/233]	Loss 0.0264 (0.0070)	
training:	Epoch: [86][20/233]	Loss 0.0031 (0.0068)	
training:	Epoch: [86][21/233]	Loss 0.0019 (0.0066)	
training:	Epoch: [86][22/233]	Loss 0.0012 (0.0063)	
training:	Epoch: [86][23/233]	Loss 0.0042 (0.0063)	
training:	Epoch: [86][24/233]	Loss 0.0013 (0.0060)	
training:	Epoch: [86][25/233]	Loss 0.0012 (0.0058)	
training:	Epoch: [86][26/233]	Loss 0.0013 (0.0057)	
training:	Epoch: [86][27/233]	Loss 0.0014 (0.0055)	
training:	Epoch: [86][28/233]	Loss 0.0021 (0.0054)	
training:	Epoch: [86][29/233]	Loss 0.0014 (0.0053)	
training:	Epoch: [86][30/233]	Loss 0.0016 (0.0051)	
training:	Epoch: [86][31/233]	Loss 0.0028 (0.0051)	
training:	Epoch: [86][32/233]	Loss 0.0020 (0.0050)	
training:	Epoch: [86][33/233]	Loss 0.0014 (0.0049)	
training:	Epoch: [86][34/233]	Loss 0.0035 (0.0048)	
training:	Epoch: [86][35/233]	Loss 0.0017 (0.0047)	
training:	Epoch: [86][36/233]	Loss 0.0016 (0.0046)	
training:	Epoch: [86][37/233]	Loss 0.0022 (0.0046)	
training:	Epoch: [86][38/233]	Loss 0.0021 (0.0045)	
training:	Epoch: [86][39/233]	Loss 0.0014 (0.0044)	
training:	Epoch: [86][40/233]	Loss 0.0022 (0.0044)	
training:	Epoch: [86][41/233]	Loss 0.0014 (0.0043)	
training:	Epoch: [86][42/233]	Loss 0.0013 (0.0042)	
training:	Epoch: [86][43/233]	Loss 0.0025 (0.0042)	
training:	Epoch: [86][44/233]	Loss 0.0044 (0.0042)	
training:	Epoch: [86][45/233]	Loss 0.0018 (0.0041)	
training:	Epoch: [86][46/233]	Loss 0.2173 (0.0088)	
training:	Epoch: [86][47/233]	Loss 0.0014 (0.0086)	
training:	Epoch: [86][48/233]	Loss 0.0011 (0.0085)	
training:	Epoch: [86][49/233]	Loss 0.0013 (0.0083)	
training:	Epoch: [86][50/233]	Loss 0.0026 (0.0082)	
training:	Epoch: [86][51/233]	Loss 0.0014 (0.0081)	
training:	Epoch: [86][52/233]	Loss 0.0835 (0.0095)	
training:	Epoch: [86][53/233]	Loss 0.0013 (0.0094)	
training:	Epoch: [86][54/233]	Loss 0.2041 (0.0130)	
training:	Epoch: [86][55/233]	Loss 0.0022 (0.0128)	
training:	Epoch: [86][56/233]	Loss 0.0017 (0.0126)	
training:	Epoch: [86][57/233]	Loss 0.0013 (0.0124)	
training:	Epoch: [86][58/233]	Loss 0.0013 (0.0122)	
training:	Epoch: [86][59/233]	Loss 0.0116 (0.0122)	
training:	Epoch: [86][60/233]	Loss 0.0108 (0.0122)	
training:	Epoch: [86][61/233]	Loss 0.0165 (0.0122)	
training:	Epoch: [86][62/233]	Loss 0.0012 (0.0120)	
training:	Epoch: [86][63/233]	Loss 0.0238 (0.0122)	
training:	Epoch: [86][64/233]	Loss 0.0013 (0.0121)	
training:	Epoch: [86][65/233]	Loss 0.0015 (0.0119)	
training:	Epoch: [86][66/233]	Loss 0.0015 (0.0117)	
training:	Epoch: [86][67/233]	Loss 0.0011 (0.0116)	
training:	Epoch: [86][68/233]	Loss 0.0016 (0.0114)	
training:	Epoch: [86][69/233]	Loss 0.0016 (0.0113)	
training:	Epoch: [86][70/233]	Loss 0.0012 (0.0112)	
training:	Epoch: [86][71/233]	Loss 0.0014 (0.0110)	
training:	Epoch: [86][72/233]	Loss 0.0017 (0.0109)	
training:	Epoch: [86][73/233]	Loss 0.0022 (0.0108)	
training:	Epoch: [86][74/233]	Loss 0.1665 (0.0129)	
training:	Epoch: [86][75/233]	Loss 0.0078 (0.0128)	
training:	Epoch: [86][76/233]	Loss 0.1275 (0.0143)	
training:	Epoch: [86][77/233]	Loss 0.0014 (0.0141)	
training:	Epoch: [86][78/233]	Loss 0.0017 (0.0140)	
training:	Epoch: [86][79/233]	Loss 0.0018 (0.0138)	
training:	Epoch: [86][80/233]	Loss 0.0018 (0.0137)	
training:	Epoch: [86][81/233]	Loss 0.0015 (0.0135)	
training:	Epoch: [86][82/233]	Loss 0.0020 (0.0134)	
training:	Epoch: [86][83/233]	Loss 0.0013 (0.0132)	
training:	Epoch: [86][84/233]	Loss 0.0020 (0.0131)	
training:	Epoch: [86][85/233]	Loss 0.0013 (0.0130)	
training:	Epoch: [86][86/233]	Loss 0.0016 (0.0128)	
training:	Epoch: [86][87/233]	Loss 0.0013 (0.0127)	
training:	Epoch: [86][88/233]	Loss 0.0013 (0.0126)	
training:	Epoch: [86][89/233]	Loss 0.0016 (0.0125)	
training:	Epoch: [86][90/233]	Loss 0.0015 (0.0123)	
training:	Epoch: [86][91/233]	Loss 0.0022 (0.0122)	
training:	Epoch: [86][92/233]	Loss 0.0016 (0.0121)	
training:	Epoch: [86][93/233]	Loss 0.0023 (0.0120)	
training:	Epoch: [86][94/233]	Loss 0.0020 (0.0119)	
training:	Epoch: [86][95/233]	Loss 0.0015 (0.0118)	
training:	Epoch: [86][96/233]	Loss 0.0025 (0.0117)	
training:	Epoch: [86][97/233]	Loss 0.0014 (0.0116)	
training:	Epoch: [86][98/233]	Loss 0.0486 (0.0120)	
training:	Epoch: [86][99/233]	Loss 0.0026 (0.0119)	
training:	Epoch: [86][100/233]	Loss 0.0022 (0.0118)	
training:	Epoch: [86][101/233]	Loss 0.0021 (0.0117)	
training:	Epoch: [86][102/233]	Loss 0.0014 (0.0116)	
training:	Epoch: [86][103/233]	Loss 0.0015 (0.0115)	
training:	Epoch: [86][104/233]	Loss 0.0015 (0.0114)	
training:	Epoch: [86][105/233]	Loss 0.0015 (0.0113)	
training:	Epoch: [86][106/233]	Loss 0.0021 (0.0112)	
training:	Epoch: [86][107/233]	Loss 0.0024 (0.0111)	
training:	Epoch: [86][108/233]	Loss 0.0020 (0.0110)	
training:	Epoch: [86][109/233]	Loss 0.0014 (0.0109)	
training:	Epoch: [86][110/233]	Loss 0.0032 (0.0109)	
training:	Epoch: [86][111/233]	Loss 0.0012 (0.0108)	
training:	Epoch: [86][112/233]	Loss 0.0734 (0.0113)	
training:	Epoch: [86][113/233]	Loss 0.0024 (0.0113)	
training:	Epoch: [86][114/233]	Loss 0.0023 (0.0112)	
training:	Epoch: [86][115/233]	Loss 0.0018 (0.0111)	
training:	Epoch: [86][116/233]	Loss 0.0041 (0.0110)	
training:	Epoch: [86][117/233]	Loss 0.0015 (0.0110)	
training:	Epoch: [86][118/233]	Loss 0.0036 (0.0109)	
training:	Epoch: [86][119/233]	Loss 0.0070 (0.0109)	
training:	Epoch: [86][120/233]	Loss 0.0014 (0.0108)	
training:	Epoch: [86][121/233]	Loss 0.0013 (0.0107)	
training:	Epoch: [86][122/233]	Loss 0.0015 (0.0106)	
training:	Epoch: [86][123/233]	Loss 0.0018 (0.0106)	
training:	Epoch: [86][124/233]	Loss 0.0018 (0.0105)	
training:	Epoch: [86][125/233]	Loss 0.0018 (0.0104)	
training:	Epoch: [86][126/233]	Loss 0.0029 (0.0104)	
training:	Epoch: [86][127/233]	Loss 0.0012 (0.0103)	
training:	Epoch: [86][128/233]	Loss 0.0012 (0.0102)	
training:	Epoch: [86][129/233]	Loss 0.0022 (0.0102)	
training:	Epoch: [86][130/233]	Loss 0.0014 (0.0101)	
training:	Epoch: [86][131/233]	Loss 0.0050 (0.0100)	
training:	Epoch: [86][132/233]	Loss 0.0035 (0.0100)	
training:	Epoch: [86][133/233]	Loss 0.0018 (0.0099)	
training:	Epoch: [86][134/233]	Loss 0.0129 (0.0100)	
training:	Epoch: [86][135/233]	Loss 0.0018 (0.0099)	
training:	Epoch: [86][136/233]	Loss 0.0036 (0.0099)	
training:	Epoch: [86][137/233]	Loss 0.0020 (0.0098)	
training:	Epoch: [86][138/233]	Loss 0.0015 (0.0097)	
training:	Epoch: [86][139/233]	Loss 0.0515 (0.0100)	
training:	Epoch: [86][140/233]	Loss 0.0018 (0.0100)	
training:	Epoch: [86][141/233]	Loss 0.0018 (0.0099)	
training:	Epoch: [86][142/233]	Loss 0.0822 (0.0104)	
training:	Epoch: [86][143/233]	Loss 0.0015 (0.0104)	
training:	Epoch: [86][144/233]	Loss 0.0011 (0.0103)	
training:	Epoch: [86][145/233]	Loss 0.0022 (0.0102)	
training:	Epoch: [86][146/233]	Loss 0.0013 (0.0102)	
training:	Epoch: [86][147/233]	Loss 0.0011 (0.0101)	
training:	Epoch: [86][148/233]	Loss 0.0016 (0.0101)	
training:	Epoch: [86][149/233]	Loss 0.0014 (0.0100)	
training:	Epoch: [86][150/233]	Loss 0.0045 (0.0100)	
training:	Epoch: [86][151/233]	Loss 0.0070 (0.0099)	
training:	Epoch: [86][152/233]	Loss 0.1788 (0.0111)	
training:	Epoch: [86][153/233]	Loss 0.0442 (0.0113)	
training:	Epoch: [86][154/233]	Loss 0.0019 (0.0112)	
training:	Epoch: [86][155/233]	Loss 0.0011 (0.0111)	
training:	Epoch: [86][156/233]	Loss 0.0017 (0.0111)	
training:	Epoch: [86][157/233]	Loss 0.0132 (0.0111)	
training:	Epoch: [86][158/233]	Loss 0.0013 (0.0110)	
training:	Epoch: [86][159/233]	Loss 0.0031 (0.0110)	
training:	Epoch: [86][160/233]	Loss 0.0029 (0.0109)	
training:	Epoch: [86][161/233]	Loss 0.0026 (0.0109)	
training:	Epoch: [86][162/233]	Loss 0.0035 (0.0108)	
training:	Epoch: [86][163/233]	Loss 0.0016 (0.0108)	
training:	Epoch: [86][164/233]	Loss 0.1835 (0.0118)	
training:	Epoch: [86][165/233]	Loss 0.0019 (0.0118)	
training:	Epoch: [86][166/233]	Loss 0.0018 (0.0117)	
training:	Epoch: [86][167/233]	Loss 0.0015 (0.0117)	
training:	Epoch: [86][168/233]	Loss 0.0014 (0.0116)	
training:	Epoch: [86][169/233]	Loss 0.0025 (0.0115)	
training:	Epoch: [86][170/233]	Loss 0.0014 (0.0115)	
training:	Epoch: [86][171/233]	Loss 0.0016 (0.0114)	
training:	Epoch: [86][172/233]	Loss 0.0114 (0.0114)	
training:	Epoch: [86][173/233]	Loss 0.0013 (0.0114)	
training:	Epoch: [86][174/233]	Loss 0.0087 (0.0114)	
training:	Epoch: [86][175/233]	Loss 0.0012 (0.0113)	
training:	Epoch: [86][176/233]	Loss 0.0014 (0.0112)	
training:	Epoch: [86][177/233]	Loss 0.0386 (0.0114)	
training:	Epoch: [86][178/233]	Loss 0.0017 (0.0113)	
training:	Epoch: [86][179/233]	Loss 0.0013 (0.0113)	
training:	Epoch: [86][180/233]	Loss 0.0012 (0.0112)	
training:	Epoch: [86][181/233]	Loss 0.0013 (0.0112)	
training:	Epoch: [86][182/233]	Loss 0.0015 (0.0111)	
training:	Epoch: [86][183/233]	Loss 0.0014 (0.0111)	
training:	Epoch: [86][184/233]	Loss 0.0023 (0.0110)	
training:	Epoch: [86][185/233]	Loss 0.0015 (0.0110)	
training:	Epoch: [86][186/233]	Loss 0.0022 (0.0109)	
training:	Epoch: [86][187/233]	Loss 0.0014 (0.0109)	
training:	Epoch: [86][188/233]	Loss 0.0012 (0.0108)	
training:	Epoch: [86][189/233]	Loss 0.0030 (0.0108)	
training:	Epoch: [86][190/233]	Loss 0.0015 (0.0107)	
training:	Epoch: [86][191/233]	Loss 0.0610 (0.0110)	
training:	Epoch: [86][192/233]	Loss 0.0014 (0.0109)	
training:	Epoch: [86][193/233]	Loss 0.0037 (0.0109)	
training:	Epoch: [86][194/233]	Loss 0.0029 (0.0109)	
training:	Epoch: [86][195/233]	Loss 0.0012 (0.0108)	
training:	Epoch: [86][196/233]	Loss 0.0014 (0.0108)	
training:	Epoch: [86][197/233]	Loss 0.0015 (0.0107)	
training:	Epoch: [86][198/233]	Loss 0.0018 (0.0107)	
training:	Epoch: [86][199/233]	Loss 0.0056 (0.0106)	
training:	Epoch: [86][200/233]	Loss 0.0016 (0.0106)	
training:	Epoch: [86][201/233]	Loss 0.0374 (0.0107)	
training:	Epoch: [86][202/233]	Loss 0.0015 (0.0107)	
training:	Epoch: [86][203/233]	Loss 0.0014 (0.0106)	
training:	Epoch: [86][204/233]	Loss 0.0015 (0.0106)	
training:	Epoch: [86][205/233]	Loss 0.2220 (0.0116)	
training:	Epoch: [86][206/233]	Loss 0.0025 (0.0116)	
training:	Epoch: [86][207/233]	Loss 0.0036 (0.0115)	
training:	Epoch: [86][208/233]	Loss 0.0016 (0.0115)	
training:	Epoch: [86][209/233]	Loss 0.0029 (0.0115)	
training:	Epoch: [86][210/233]	Loss 0.0822 (0.0118)	
training:	Epoch: [86][211/233]	Loss 0.0013 (0.0117)	
training:	Epoch: [86][212/233]	Loss 0.0011 (0.0117)	
training:	Epoch: [86][213/233]	Loss 0.0014 (0.0116)	
training:	Epoch: [86][214/233]	Loss 0.0122 (0.0116)	
training:	Epoch: [86][215/233]	Loss 0.0017 (0.0116)	
training:	Epoch: [86][216/233]	Loss 0.0013 (0.0116)	
training:	Epoch: [86][217/233]	Loss 0.0026 (0.0115)	
training:	Epoch: [86][218/233]	Loss 0.0084 (0.0115)	
training:	Epoch: [86][219/233]	Loss 0.0013 (0.0114)	
training:	Epoch: [86][220/233]	Loss 0.0015 (0.0114)	
training:	Epoch: [86][221/233]	Loss 0.0021 (0.0114)	
training:	Epoch: [86][222/233]	Loss 0.0013 (0.0113)	
training:	Epoch: [86][223/233]	Loss 0.0014 (0.0113)	
training:	Epoch: [86][224/233]	Loss 0.0014 (0.0112)	
training:	Epoch: [86][225/233]	Loss 0.0012 (0.0112)	
training:	Epoch: [86][226/233]	Loss 0.0067 (0.0112)	
training:	Epoch: [86][227/233]	Loss 0.0018 (0.0111)	
training:	Epoch: [86][228/233]	Loss 0.0012 (0.0111)	
training:	Epoch: [86][229/233]	Loss 0.0014 (0.0110)	
training:	Epoch: [86][230/233]	Loss 0.0014 (0.0110)	
training:	Epoch: [86][231/233]	Loss 0.0022 (0.0110)	
training:	Epoch: [86][232/233]	Loss 0.0022 (0.0109)	
training:	Epoch: [86][233/233]	Loss 0.0022 (0.0109)	
Training:	 Loss: 0.0109

Training:	 ACC: 0.9995 0.9995 0.9995 0.9994
Validation:	 ACC: 0.7913 0.7903 0.7692 0.8135
Validation:	 Best_BACC: 0.8162 0.8154 0.7998 0.8326
Validation:	 Loss: 1.0773
Pretraining:	Epoch 87/200
----------
training:	Epoch: [87][1/233]	Loss 0.0087 (0.0087)	
training:	Epoch: [87][2/233]	Loss 0.0013 (0.0050)	
training:	Epoch: [87][3/233]	Loss 0.0035 (0.0045)	
training:	Epoch: [87][4/233]	Loss 0.0045 (0.0045)	
training:	Epoch: [87][5/233]	Loss 0.0013 (0.0038)	
training:	Epoch: [87][6/233]	Loss 0.0012 (0.0034)	
training:	Epoch: [87][7/233]	Loss 0.0426 (0.0090)	
training:	Epoch: [87][8/233]	Loss 0.0011 (0.0080)	
training:	Epoch: [87][9/233]	Loss 0.0013 (0.0073)	
training:	Epoch: [87][10/233]	Loss 0.0014 (0.0067)	
training:	Epoch: [87][11/233]	Loss 0.0020 (0.0062)	
training:	Epoch: [87][12/233]	Loss 0.0019 (0.0059)	
training:	Epoch: [87][13/233]	Loss 0.0015 (0.0055)	
training:	Epoch: [87][14/233]	Loss 0.0032 (0.0054)	
training:	Epoch: [87][15/233]	Loss 0.0015 (0.0051)	
training:	Epoch: [87][16/233]	Loss 0.0013 (0.0049)	
training:	Epoch: [87][17/233]	Loss 0.0017 (0.0047)	
training:	Epoch: [87][18/233]	Loss 0.0013 (0.0045)	
training:	Epoch: [87][19/233]	Loss 0.0012 (0.0043)	
training:	Epoch: [87][20/233]	Loss 0.0018 (0.0042)	
training:	Epoch: [87][21/233]	Loss 0.0182 (0.0049)	
training:	Epoch: [87][22/233]	Loss 0.0025 (0.0048)	
training:	Epoch: [87][23/233]	Loss 0.0014 (0.0046)	
training:	Epoch: [87][24/233]	Loss 0.0021 (0.0045)	
training:	Epoch: [87][25/233]	Loss 0.0013 (0.0044)	
training:	Epoch: [87][26/233]	Loss 0.0014 (0.0043)	
training:	Epoch: [87][27/233]	Loss 0.0029 (0.0042)	
training:	Epoch: [87][28/233]	Loss 0.0014 (0.0041)	
training:	Epoch: [87][29/233]	Loss 0.0110 (0.0044)	
training:	Epoch: [87][30/233]	Loss 0.0022 (0.0043)	
training:	Epoch: [87][31/233]	Loss 0.0120 (0.0045)	
training:	Epoch: [87][32/233]	Loss 0.0010 (0.0044)	
training:	Epoch: [87][33/233]	Loss 0.0652 (0.0063)	
training:	Epoch: [87][34/233]	Loss 0.0017 (0.0061)	
training:	Epoch: [87][35/233]	Loss 0.0012 (0.0060)	
training:	Epoch: [87][36/233]	Loss 0.0017 (0.0059)	
training:	Epoch: [87][37/233]	Loss 0.0017 (0.0058)	
training:	Epoch: [87][38/233]	Loss 0.0021 (0.0057)	
training:	Epoch: [87][39/233]	Loss 0.0025 (0.0056)	
training:	Epoch: [87][40/233]	Loss 0.0013 (0.0055)	
training:	Epoch: [87][41/233]	Loss 0.0014 (0.0054)	
training:	Epoch: [87][42/233]	Loss 0.0449 (0.0063)	
training:	Epoch: [87][43/233]	Loss 0.0029 (0.0062)	
training:	Epoch: [87][44/233]	Loss 0.0027 (0.0062)	
training:	Epoch: [87][45/233]	Loss 0.0013 (0.0060)	
training:	Epoch: [87][46/233]	Loss 0.0011 (0.0059)	
training:	Epoch: [87][47/233]	Loss 0.0018 (0.0059)	
training:	Epoch: [87][48/233]	Loss 0.0015 (0.0058)	
training:	Epoch: [87][49/233]	Loss 0.0016 (0.0057)	
training:	Epoch: [87][50/233]	Loss 0.0013 (0.0056)	
training:	Epoch: [87][51/233]	Loss 0.0015 (0.0055)	
training:	Epoch: [87][52/233]	Loss 0.0020 (0.0054)	
training:	Epoch: [87][53/233]	Loss 0.0043 (0.0054)	
training:	Epoch: [87][54/233]	Loss 0.0013 (0.0053)	
training:	Epoch: [87][55/233]	Loss 0.2141 (0.0091)	
training:	Epoch: [87][56/233]	Loss 0.0026 (0.0090)	
training:	Epoch: [87][57/233]	Loss 0.0023 (0.0089)	
training:	Epoch: [87][58/233]	Loss 0.1666 (0.0116)	
training:	Epoch: [87][59/233]	Loss 0.0014 (0.0114)	
training:	Epoch: [87][60/233]	Loss 0.0014 (0.0113)	
training:	Epoch: [87][61/233]	Loss 0.0038 (0.0112)	
training:	Epoch: [87][62/233]	Loss 0.0013 (0.0110)	
training:	Epoch: [87][63/233]	Loss 0.0014 (0.0108)	
training:	Epoch: [87][64/233]	Loss 0.0016 (0.0107)	
training:	Epoch: [87][65/233]	Loss 0.0015 (0.0106)	
training:	Epoch: [87][66/233]	Loss 0.0018 (0.0104)	
training:	Epoch: [87][67/233]	Loss 0.0392 (0.0109)	
training:	Epoch: [87][68/233]	Loss 0.0013 (0.0107)	
training:	Epoch: [87][69/233]	Loss 0.0013 (0.0106)	
training:	Epoch: [87][70/233]	Loss 0.0016 (0.0105)	
training:	Epoch: [87][71/233]	Loss 0.0014 (0.0103)	
training:	Epoch: [87][72/233]	Loss 0.0021 (0.0102)	
training:	Epoch: [87][73/233]	Loss 0.0021 (0.0101)	
training:	Epoch: [87][74/233]	Loss 0.0206 (0.0102)	
training:	Epoch: [87][75/233]	Loss 0.0021 (0.0101)	
training:	Epoch: [87][76/233]	Loss 0.0016 (0.0100)	
training:	Epoch: [87][77/233]	Loss 0.0014 (0.0099)	
training:	Epoch: [87][78/233]	Loss 0.0012 (0.0098)	
training:	Epoch: [87][79/233]	Loss 0.0015 (0.0097)	
training:	Epoch: [87][80/233]	Loss 0.0022 (0.0096)	
training:	Epoch: [87][81/233]	Loss 0.0413 (0.0100)	
training:	Epoch: [87][82/233]	Loss 0.0013 (0.0099)	
training:	Epoch: [87][83/233]	Loss 0.0018 (0.0098)	
training:	Epoch: [87][84/233]	Loss 0.0012 (0.0097)	
training:	Epoch: [87][85/233]	Loss 0.0032 (0.0096)	
training:	Epoch: [87][86/233]	Loss 0.0097 (0.0096)	
training:	Epoch: [87][87/233]	Loss 0.0013 (0.0095)	
training:	Epoch: [87][88/233]	Loss 0.0303 (0.0098)	
training:	Epoch: [87][89/233]	Loss 0.0015 (0.0097)	
training:	Epoch: [87][90/233]	Loss 0.0415 (0.0100)	
training:	Epoch: [87][91/233]	Loss 0.0021 (0.0099)	
training:	Epoch: [87][92/233]	Loss 0.0049 (0.0099)	
training:	Epoch: [87][93/233]	Loss 0.0011 (0.0098)	
training:	Epoch: [87][94/233]	Loss 0.0013 (0.0097)	
training:	Epoch: [87][95/233]	Loss 0.0014 (0.0096)	
training:	Epoch: [87][96/233]	Loss 0.0012 (0.0095)	
training:	Epoch: [87][97/233]	Loss 0.0013 (0.0094)	
training:	Epoch: [87][98/233]	Loss 0.0020 (0.0094)	
training:	Epoch: [87][99/233]	Loss 0.0014 (0.0093)	
training:	Epoch: [87][100/233]	Loss 0.0015 (0.0092)	
training:	Epoch: [87][101/233]	Loss 0.1167 (0.0103)	
training:	Epoch: [87][102/233]	Loss 0.0013 (0.0102)	
training:	Epoch: [87][103/233]	Loss 0.0035 (0.0101)	
training:	Epoch: [87][104/233]	Loss 0.0015 (0.0100)	
training:	Epoch: [87][105/233]	Loss 0.0015 (0.0099)	
training:	Epoch: [87][106/233]	Loss 0.0012 (0.0099)	
training:	Epoch: [87][107/233]	Loss 0.0015 (0.0098)	
training:	Epoch: [87][108/233]	Loss 0.0117 (0.0098)	
training:	Epoch: [87][109/233]	Loss 0.0013 (0.0097)	
training:	Epoch: [87][110/233]	Loss 0.0013 (0.0096)	
training:	Epoch: [87][111/233]	Loss 0.0189 (0.0097)	
training:	Epoch: [87][112/233]	Loss 0.0026 (0.0097)	
training:	Epoch: [87][113/233]	Loss 0.0130 (0.0097)	
training:	Epoch: [87][114/233]	Loss 0.0013 (0.0096)	
training:	Epoch: [87][115/233]	Loss 0.0013 (0.0095)	
training:	Epoch: [87][116/233]	Loss 0.0052 (0.0095)	
training:	Epoch: [87][117/233]	Loss 0.0013 (0.0094)	
training:	Epoch: [87][118/233]	Loss 0.0013 (0.0094)	
training:	Epoch: [87][119/233]	Loss 0.0015 (0.0093)	
training:	Epoch: [87][120/233]	Loss 0.0057 (0.0093)	
training:	Epoch: [87][121/233]	Loss 0.0014 (0.0092)	
training:	Epoch: [87][122/233]	Loss 0.0584 (0.0096)	
training:	Epoch: [87][123/233]	Loss 0.0612 (0.0100)	
training:	Epoch: [87][124/233]	Loss 0.1482 (0.0111)	
training:	Epoch: [87][125/233]	Loss 0.0012 (0.0111)	
training:	Epoch: [87][126/233]	Loss 0.0013 (0.0110)	
training:	Epoch: [87][127/233]	Loss 0.0029 (0.0109)	
training:	Epoch: [87][128/233]	Loss 0.0014 (0.0109)	
training:	Epoch: [87][129/233]	Loss 0.0013 (0.0108)	
training:	Epoch: [87][130/233]	Loss 0.0018 (0.0107)	
training:	Epoch: [87][131/233]	Loss 0.0015 (0.0106)	
training:	Epoch: [87][132/233]	Loss 0.0014 (0.0106)	
training:	Epoch: [87][133/233]	Loss 0.0017 (0.0105)	
training:	Epoch: [87][134/233]	Loss 0.0016 (0.0104)	
training:	Epoch: [87][135/233]	Loss 0.0062 (0.0104)	
training:	Epoch: [87][136/233]	Loss 0.0278 (0.0105)	
training:	Epoch: [87][137/233]	Loss 0.0031 (0.0105)	
training:	Epoch: [87][138/233]	Loss 0.0017 (0.0104)	
training:	Epoch: [87][139/233]	Loss 0.0012 (0.0103)	
training:	Epoch: [87][140/233]	Loss 0.0013 (0.0103)	
training:	Epoch: [87][141/233]	Loss 0.0013 (0.0102)	
training:	Epoch: [87][142/233]	Loss 0.0038 (0.0102)	
training:	Epoch: [87][143/233]	Loss 0.0154 (0.0102)	
training:	Epoch: [87][144/233]	Loss 0.0014 (0.0101)	
training:	Epoch: [87][145/233]	Loss 0.0012 (0.0101)	
training:	Epoch: [87][146/233]	Loss 0.0646 (0.0105)	
training:	Epoch: [87][147/233]	Loss 0.0990 (0.0111)	
training:	Epoch: [87][148/233]	Loss 0.2143 (0.0124)	
training:	Epoch: [87][149/233]	Loss 0.0016 (0.0124)	
training:	Epoch: [87][150/233]	Loss 0.0015 (0.0123)	
training:	Epoch: [87][151/233]	Loss 0.0017 (0.0122)	
training:	Epoch: [87][152/233]	Loss 0.0026 (0.0122)	
training:	Epoch: [87][153/233]	Loss 0.0014 (0.0121)	
training:	Epoch: [87][154/233]	Loss 0.0015 (0.0120)	
training:	Epoch: [87][155/233]	Loss 0.0061 (0.0120)	
training:	Epoch: [87][156/233]	Loss 0.0015 (0.0119)	
training:	Epoch: [87][157/233]	Loss 0.0013 (0.0118)	
training:	Epoch: [87][158/233]	Loss 0.0016 (0.0118)	
training:	Epoch: [87][159/233]	Loss 0.0982 (0.0123)	
training:	Epoch: [87][160/233]	Loss 0.0015 (0.0123)	
training:	Epoch: [87][161/233]	Loss 0.0017 (0.0122)	
training:	Epoch: [87][162/233]	Loss 0.0016 (0.0121)	
training:	Epoch: [87][163/233]	Loss 0.0074 (0.0121)	
training:	Epoch: [87][164/233]	Loss 0.0143 (0.0121)	
training:	Epoch: [87][165/233]	Loss 0.0019 (0.0120)	
training:	Epoch: [87][166/233]	Loss 0.0014 (0.0120)	
training:	Epoch: [87][167/233]	Loss 0.0014 (0.0119)	
training:	Epoch: [87][168/233]	Loss 0.0028 (0.0119)	
training:	Epoch: [87][169/233]	Loss 0.0018 (0.0118)	
training:	Epoch: [87][170/233]	Loss 0.0921 (0.0123)	
training:	Epoch: [87][171/233]	Loss 0.0012 (0.0122)	
training:	Epoch: [87][172/233]	Loss 0.0137 (0.0122)	
training:	Epoch: [87][173/233]	Loss 0.0052 (0.0122)	
training:	Epoch: [87][174/233]	Loss 0.0014 (0.0121)	
training:	Epoch: [87][175/233]	Loss 0.2098 (0.0132)	
training:	Epoch: [87][176/233]	Loss 0.0052 (0.0132)	
training:	Epoch: [87][177/233]	Loss 0.0135 (0.0132)	
training:	Epoch: [87][178/233]	Loss 0.0016 (0.0131)	
training:	Epoch: [87][179/233]	Loss 0.0022 (0.0131)	
training:	Epoch: [87][180/233]	Loss 0.0017 (0.0130)	
training:	Epoch: [87][181/233]	Loss 0.0015 (0.0130)	
training:	Epoch: [87][182/233]	Loss 0.0016 (0.0129)	
training:	Epoch: [87][183/233]	Loss 0.0016 (0.0128)	
training:	Epoch: [87][184/233]	Loss 0.0025 (0.0128)	
training:	Epoch: [87][185/233]	Loss 0.0015 (0.0127)	
training:	Epoch: [87][186/233]	Loss 0.0029 (0.0127)	
training:	Epoch: [87][187/233]	Loss 0.0047 (0.0126)	
training:	Epoch: [87][188/233]	Loss 0.0095 (0.0126)	
training:	Epoch: [87][189/233]	Loss 0.0016 (0.0125)	
training:	Epoch: [87][190/233]	Loss 0.0221 (0.0126)	
training:	Epoch: [87][191/233]	Loss 0.0012 (0.0125)	
training:	Epoch: [87][192/233]	Loss 0.0017 (0.0125)	
training:	Epoch: [87][193/233]	Loss 0.0066 (0.0124)	
training:	Epoch: [87][194/233]	Loss 0.0012 (0.0124)	
training:	Epoch: [87][195/233]	Loss 0.0214 (0.0124)	
training:	Epoch: [87][196/233]	Loss 0.0011 (0.0124)	
training:	Epoch: [87][197/233]	Loss 0.0012 (0.0123)	
training:	Epoch: [87][198/233]	Loss 0.0034 (0.0123)	
training:	Epoch: [87][199/233]	Loss 0.0012 (0.0122)	
training:	Epoch: [87][200/233]	Loss 0.0041 (0.0122)	
training:	Epoch: [87][201/233]	Loss 0.0016 (0.0121)	
training:	Epoch: [87][202/233]	Loss 0.0012 (0.0121)	
training:	Epoch: [87][203/233]	Loss 0.0021 (0.0120)	
training:	Epoch: [87][204/233]	Loss 0.0030 (0.0120)	
training:	Epoch: [87][205/233]	Loss 0.0057 (0.0119)	
training:	Epoch: [87][206/233]	Loss 0.0131 (0.0120)	
training:	Epoch: [87][207/233]	Loss 0.0019 (0.0119)	
training:	Epoch: [87][208/233]	Loss 0.0017 (0.0119)	
training:	Epoch: [87][209/233]	Loss 0.0017 (0.0118)	
training:	Epoch: [87][210/233]	Loss 0.0016 (0.0118)	
training:	Epoch: [87][211/233]	Loss 0.0043 (0.0117)	
training:	Epoch: [87][212/233]	Loss 0.0021 (0.0117)	
training:	Epoch: [87][213/233]	Loss 0.0015 (0.0116)	
training:	Epoch: [87][214/233]	Loss 0.0022 (0.0116)	
training:	Epoch: [87][215/233]	Loss 0.0035 (0.0115)	
training:	Epoch: [87][216/233]	Loss 0.0018 (0.0115)	
training:	Epoch: [87][217/233]	Loss 0.0013 (0.0115)	
training:	Epoch: [87][218/233]	Loss 0.0014 (0.0114)	
training:	Epoch: [87][219/233]	Loss 0.0021 (0.0114)	
training:	Epoch: [87][220/233]	Loss 0.0100 (0.0114)	
training:	Epoch: [87][221/233]	Loss 0.0015 (0.0113)	
training:	Epoch: [87][222/233]	Loss 0.0013 (0.0113)	
training:	Epoch: [87][223/233]	Loss 0.0016 (0.0112)	
training:	Epoch: [87][224/233]	Loss 0.0013 (0.0112)	
training:	Epoch: [87][225/233]	Loss 0.0013 (0.0111)	
training:	Epoch: [87][226/233]	Loss 0.0086 (0.0111)	
training:	Epoch: [87][227/233]	Loss 0.0017 (0.0111)	
training:	Epoch: [87][228/233]	Loss 0.0043 (0.0111)	
training:	Epoch: [87][229/233]	Loss 0.0014 (0.0110)	
training:	Epoch: [87][230/233]	Loss 0.0015 (0.0110)	
training:	Epoch: [87][231/233]	Loss 0.0022 (0.0109)	
training:	Epoch: [87][232/233]	Loss 0.0188 (0.0110)	
training:	Epoch: [87][233/233]	Loss 0.0019 (0.0109)	
Training:	 Loss: 0.0109

Training:	 ACC: 0.9993 0.9993 0.9995 0.9992
Validation:	 ACC: 0.7869 0.7881 0.8121 0.7618
Validation:	 Best_BACC: 0.8162 0.8154 0.7998 0.8326
Validation:	 Loss: 1.0907
Pretraining:	Epoch 88/200
----------
training:	Epoch: [88][1/233]	Loss 0.0016 (0.0016)	
training:	Epoch: [88][2/233]	Loss 0.0012 (0.0014)	
training:	Epoch: [88][3/233]	Loss 0.0015 (0.0014)	
training:	Epoch: [88][4/233]	Loss 0.0015 (0.0015)	
training:	Epoch: [88][5/233]	Loss 0.0013 (0.0014)	
training:	Epoch: [88][6/233]	Loss 0.0022 (0.0016)	
training:	Epoch: [88][7/233]	Loss 0.0013 (0.0015)	
training:	Epoch: [88][8/233]	Loss 0.0013 (0.0015)	
training:	Epoch: [88][9/233]	Loss 0.0012 (0.0015)	
training:	Epoch: [88][10/233]	Loss 0.0015 (0.0015)	
training:	Epoch: [88][11/233]	Loss 0.0097 (0.0022)	
training:	Epoch: [88][12/233]	Loss 0.0094 (0.0028)	
training:	Epoch: [88][13/233]	Loss 0.0024 (0.0028)	
training:	Epoch: [88][14/233]	Loss 0.0019 (0.0027)	
training:	Epoch: [88][15/233]	Loss 0.0064 (0.0030)	
training:	Epoch: [88][16/233]	Loss 0.0014 (0.0029)	
training:	Epoch: [88][17/233]	Loss 0.0015 (0.0028)	
training:	Epoch: [88][18/233]	Loss 0.0013 (0.0027)	
training:	Epoch: [88][19/233]	Loss 0.0332 (0.0043)	
training:	Epoch: [88][20/233]	Loss 0.0012 (0.0042)	
training:	Epoch: [88][21/233]	Loss 0.0016 (0.0040)	
training:	Epoch: [88][22/233]	Loss 0.0012 (0.0039)	
training:	Epoch: [88][23/233]	Loss 0.0016 (0.0038)	
training:	Epoch: [88][24/233]	Loss 0.0014 (0.0037)	
training:	Epoch: [88][25/233]	Loss 0.0018 (0.0036)	
training:	Epoch: [88][26/233]	Loss 0.0102 (0.0039)	
training:	Epoch: [88][27/233]	Loss 0.0020 (0.0038)	
training:	Epoch: [88][28/233]	Loss 0.0011 (0.0037)	
training:	Epoch: [88][29/233]	Loss 0.2057 (0.0107)	
training:	Epoch: [88][30/233]	Loss 0.0012 (0.0104)	
training:	Epoch: [88][31/233]	Loss 0.0014 (0.0101)	
training:	Epoch: [88][32/233]	Loss 0.0012 (0.0098)	
training:	Epoch: [88][33/233]	Loss 0.0013 (0.0095)	
training:	Epoch: [88][34/233]	Loss 0.0011 (0.0093)	
training:	Epoch: [88][35/233]	Loss 0.0015 (0.0091)	
training:	Epoch: [88][36/233]	Loss 0.0028 (0.0089)	
training:	Epoch: [88][37/233]	Loss 0.0012 (0.0087)	
training:	Epoch: [88][38/233]	Loss 0.0031 (0.0085)	
training:	Epoch: [88][39/233]	Loss 0.0013 (0.0084)	
training:	Epoch: [88][40/233]	Loss 0.0026 (0.0082)	
training:	Epoch: [88][41/233]	Loss 0.0011 (0.0080)	
training:	Epoch: [88][42/233]	Loss 0.0096 (0.0081)	
training:	Epoch: [88][43/233]	Loss 0.0020 (0.0079)	
training:	Epoch: [88][44/233]	Loss 0.0014 (0.0078)	
training:	Epoch: [88][45/233]	Loss 0.0013 (0.0076)	
training:	Epoch: [88][46/233]	Loss 0.0056 (0.0076)	
training:	Epoch: [88][47/233]	Loss 0.0014 (0.0075)	
training:	Epoch: [88][48/233]	Loss 0.0013 (0.0073)	
training:	Epoch: [88][49/233]	Loss 0.0014 (0.0072)	
training:	Epoch: [88][50/233]	Loss 0.0021 (0.0071)	
training:	Epoch: [88][51/233]	Loss 0.0017 (0.0070)	
training:	Epoch: [88][52/233]	Loss 0.0013 (0.0069)	
training:	Epoch: [88][53/233]	Loss 0.0015 (0.0068)	
training:	Epoch: [88][54/233]	Loss 0.0017 (0.0067)	
training:	Epoch: [88][55/233]	Loss 0.0032 (0.0066)	
training:	Epoch: [88][56/233]	Loss 0.0013 (0.0065)	
training:	Epoch: [88][57/233]	Loss 0.0016 (0.0065)	
training:	Epoch: [88][58/233]	Loss 0.0013 (0.0064)	
training:	Epoch: [88][59/233]	Loss 0.0015 (0.0063)	
training:	Epoch: [88][60/233]	Loss 0.0013 (0.0062)	
training:	Epoch: [88][61/233]	Loss 0.0023 (0.0061)	
training:	Epoch: [88][62/233]	Loss 0.0014 (0.0061)	
training:	Epoch: [88][63/233]	Loss 0.0013 (0.0060)	
training:	Epoch: [88][64/233]	Loss 0.0012 (0.0059)	
training:	Epoch: [88][65/233]	Loss 0.0028 (0.0059)	
training:	Epoch: [88][66/233]	Loss 0.0012 (0.0058)	
training:	Epoch: [88][67/233]	Loss 0.0011 (0.0057)	
training:	Epoch: [88][68/233]	Loss 0.0013 (0.0057)	
training:	Epoch: [88][69/233]	Loss 0.0021 (0.0056)	
training:	Epoch: [88][70/233]	Loss 0.0017 (0.0056)	
training:	Epoch: [88][71/233]	Loss 0.0055 (0.0056)	
training:	Epoch: [88][72/233]	Loss 0.0012 (0.0055)	
training:	Epoch: [88][73/233]	Loss 0.0023 (0.0055)	
training:	Epoch: [88][74/233]	Loss 0.0009 (0.0054)	
training:	Epoch: [88][75/233]	Loss 0.0116 (0.0055)	
training:	Epoch: [88][76/233]	Loss 0.0012 (0.0054)	
training:	Epoch: [88][77/233]	Loss 0.0010 (0.0054)	
training:	Epoch: [88][78/233]	Loss 0.0024 (0.0053)	
training:	Epoch: [88][79/233]	Loss 0.0020 (0.0053)	
training:	Epoch: [88][80/233]	Loss 0.0013 (0.0052)	
training:	Epoch: [88][81/233]	Loss 0.0017 (0.0052)	
training:	Epoch: [88][82/233]	Loss 0.0012 (0.0051)	
training:	Epoch: [88][83/233]	Loss 0.0013 (0.0051)	
training:	Epoch: [88][84/233]	Loss 0.0013 (0.0050)	
training:	Epoch: [88][85/233]	Loss 0.0012 (0.0050)	
training:	Epoch: [88][86/233]	Loss 0.0015 (0.0050)	
training:	Epoch: [88][87/233]	Loss 0.0024 (0.0049)	
training:	Epoch: [88][88/233]	Loss 0.0014 (0.0049)	
training:	Epoch: [88][89/233]	Loss 0.0011 (0.0048)	
training:	Epoch: [88][90/233]	Loss 0.0010 (0.0048)	
training:	Epoch: [88][91/233]	Loss 0.0011 (0.0048)	
training:	Epoch: [88][92/233]	Loss 0.0014 (0.0047)	
training:	Epoch: [88][93/233]	Loss 0.1742 (0.0065)	
training:	Epoch: [88][94/233]	Loss 0.0015 (0.0065)	
training:	Epoch: [88][95/233]	Loss 0.0012 (0.0064)	
training:	Epoch: [88][96/233]	Loss 0.0013 (0.0064)	
training:	Epoch: [88][97/233]	Loss 0.0013 (0.0063)	
training:	Epoch: [88][98/233]	Loss 0.0122 (0.0064)	
training:	Epoch: [88][99/233]	Loss 0.0041 (0.0064)	
training:	Epoch: [88][100/233]	Loss 0.0012 (0.0063)	
training:	Epoch: [88][101/233]	Loss 0.0072 (0.0063)	
training:	Epoch: [88][102/233]	Loss 0.0015 (0.0063)	
training:	Epoch: [88][103/233]	Loss 0.0096 (0.0063)	
training:	Epoch: [88][104/233]	Loss 0.0010 (0.0063)	
training:	Epoch: [88][105/233]	Loss 0.0019 (0.0062)	
training:	Epoch: [88][106/233]	Loss 0.0012 (0.0062)	
training:	Epoch: [88][107/233]	Loss 0.0023 (0.0061)	
training:	Epoch: [88][108/233]	Loss 0.0012 (0.0061)	
training:	Epoch: [88][109/233]	Loss 0.0015 (0.0060)	
training:	Epoch: [88][110/233]	Loss 0.0012 (0.0060)	
training:	Epoch: [88][111/233]	Loss 0.1086 (0.0069)	
training:	Epoch: [88][112/233]	Loss 0.0014 (0.0069)	
training:	Epoch: [88][113/233]	Loss 0.0011 (0.0068)	
training:	Epoch: [88][114/233]	Loss 0.0089 (0.0068)	
training:	Epoch: [88][115/233]	Loss 0.0061 (0.0068)	
training:	Epoch: [88][116/233]	Loss 0.0012 (0.0068)	
training:	Epoch: [88][117/233]	Loss 0.0012 (0.0067)	
training:	Epoch: [88][118/233]	Loss 0.0012 (0.0067)	
training:	Epoch: [88][119/233]	Loss 0.0011 (0.0067)	
training:	Epoch: [88][120/233]	Loss 0.0028 (0.0066)	
training:	Epoch: [88][121/233]	Loss 0.0011 (0.0066)	
training:	Epoch: [88][122/233]	Loss 0.0014 (0.0065)	
training:	Epoch: [88][123/233]	Loss 0.0013 (0.0065)	
training:	Epoch: [88][124/233]	Loss 0.0011 (0.0064)	
training:	Epoch: [88][125/233]	Loss 0.0022 (0.0064)	
training:	Epoch: [88][126/233]	Loss 0.0011 (0.0064)	
training:	Epoch: [88][127/233]	Loss 0.0124 (0.0064)	
training:	Epoch: [88][128/233]	Loss 0.0018 (0.0064)	
training:	Epoch: [88][129/233]	Loss 0.0015 (0.0063)	
training:	Epoch: [88][130/233]	Loss 0.0011 (0.0063)	
training:	Epoch: [88][131/233]	Loss 0.0014 (0.0063)	
training:	Epoch: [88][132/233]	Loss 0.0018 (0.0062)	
training:	Epoch: [88][133/233]	Loss 0.0014 (0.0062)	
training:	Epoch: [88][134/233]	Loss 0.0012 (0.0062)	
training:	Epoch: [88][135/233]	Loss 0.0011 (0.0061)	
training:	Epoch: [88][136/233]	Loss 0.0013 (0.0061)	
training:	Epoch: [88][137/233]	Loss 0.0011 (0.0060)	
training:	Epoch: [88][138/233]	Loss 0.0012 (0.0060)	
training:	Epoch: [88][139/233]	Loss 0.0012 (0.0060)	
training:	Epoch: [88][140/233]	Loss 0.0020 (0.0060)	
training:	Epoch: [88][141/233]	Loss 0.0011 (0.0059)	
training:	Epoch: [88][142/233]	Loss 0.0204 (0.0060)	
training:	Epoch: [88][143/233]	Loss 0.0029 (0.0060)	
training:	Epoch: [88][144/233]	Loss 0.0011 (0.0060)	
training:	Epoch: [88][145/233]	Loss 0.0769 (0.0065)	
training:	Epoch: [88][146/233]	Loss 0.2201 (0.0079)	
training:	Epoch: [88][147/233]	Loss 0.0012 (0.0079)	
training:	Epoch: [88][148/233]	Loss 0.0085 (0.0079)	
training:	Epoch: [88][149/233]	Loss 0.0016 (0.0078)	
training:	Epoch: [88][150/233]	Loss 0.0021 (0.0078)	
training:	Epoch: [88][151/233]	Loss 0.0012 (0.0078)	
training:	Epoch: [88][152/233]	Loss 0.0012 (0.0077)	
training:	Epoch: [88][153/233]	Loss 0.0012 (0.0077)	
training:	Epoch: [88][154/233]	Loss 0.0029 (0.0076)	
training:	Epoch: [88][155/233]	Loss 0.0025 (0.0076)	
training:	Epoch: [88][156/233]	Loss 0.0032 (0.0076)	
training:	Epoch: [88][157/233]	Loss 0.0138 (0.0076)	
training:	Epoch: [88][158/233]	Loss 0.0021 (0.0076)	
training:	Epoch: [88][159/233]	Loss 0.0021 (0.0075)	
training:	Epoch: [88][160/233]	Loss 0.0018 (0.0075)	
training:	Epoch: [88][161/233]	Loss 0.0011 (0.0075)	
training:	Epoch: [88][162/233]	Loss 0.0020 (0.0074)	
training:	Epoch: [88][163/233]	Loss 0.0039 (0.0074)	
training:	Epoch: [88][164/233]	Loss 0.0019 (0.0074)	
training:	Epoch: [88][165/233]	Loss 0.0012 (0.0073)	
training:	Epoch: [88][166/233]	Loss 0.0024 (0.0073)	
training:	Epoch: [88][167/233]	Loss 0.1073 (0.0079)	
training:	Epoch: [88][168/233]	Loss 0.0022 (0.0079)	
training:	Epoch: [88][169/233]	Loss 0.0012 (0.0078)	
training:	Epoch: [88][170/233]	Loss 0.0021 (0.0078)	
training:	Epoch: [88][171/233]	Loss 0.0014 (0.0078)	
training:	Epoch: [88][172/233]	Loss 0.0011 (0.0077)	
training:	Epoch: [88][173/233]	Loss 0.0013 (0.0077)	
training:	Epoch: [88][174/233]	Loss 0.1460 (0.0085)	
training:	Epoch: [88][175/233]	Loss 0.0011 (0.0084)	
training:	Epoch: [88][176/233]	Loss 0.0011 (0.0084)	
training:	Epoch: [88][177/233]	Loss 0.0366 (0.0086)	
training:	Epoch: [88][178/233]	Loss 0.0012 (0.0085)	
training:	Epoch: [88][179/233]	Loss 0.0017 (0.0085)	
training:	Epoch: [88][180/233]	Loss 0.0029 (0.0084)	
training:	Epoch: [88][181/233]	Loss 0.0030 (0.0084)	
training:	Epoch: [88][182/233]	Loss 0.0015 (0.0084)	
training:	Epoch: [88][183/233]	Loss 0.0014 (0.0083)	
training:	Epoch: [88][184/233]	Loss 0.0015 (0.0083)	
training:	Epoch: [88][185/233]	Loss 0.0015 (0.0083)	
training:	Epoch: [88][186/233]	Loss 0.0013 (0.0082)	
training:	Epoch: [88][187/233]	Loss 0.0023 (0.0082)	
training:	Epoch: [88][188/233]	Loss 0.0105 (0.0082)	
training:	Epoch: [88][189/233]	Loss 0.0014 (0.0082)	
training:	Epoch: [88][190/233]	Loss 0.0013 (0.0081)	
training:	Epoch: [88][191/233]	Loss 0.0011 (0.0081)	
training:	Epoch: [88][192/233]	Loss 0.0018 (0.0081)	
training:	Epoch: [88][193/233]	Loss 0.0374 (0.0082)	
training:	Epoch: [88][194/233]	Loss 0.0151 (0.0083)	
training:	Epoch: [88][195/233]	Loss 0.0147 (0.0083)	
training:	Epoch: [88][196/233]	Loss 0.0012 (0.0083)	
training:	Epoch: [88][197/233]	Loss 0.0019 (0.0082)	
training:	Epoch: [88][198/233]	Loss 0.0083 (0.0082)	
training:	Epoch: [88][199/233]	Loss 0.0011 (0.0082)	
training:	Epoch: [88][200/233]	Loss 0.0013 (0.0082)	
training:	Epoch: [88][201/233]	Loss 0.0168 (0.0082)	
training:	Epoch: [88][202/233]	Loss 0.0011 (0.0082)	
training:	Epoch: [88][203/233]	Loss 0.0429 (0.0083)	
training:	Epoch: [88][204/233]	Loss 0.0011 (0.0083)	
training:	Epoch: [88][205/233]	Loss 0.0164 (0.0083)	
training:	Epoch: [88][206/233]	Loss 0.0011 (0.0083)	
training:	Epoch: [88][207/233]	Loss 0.0012 (0.0083)	
training:	Epoch: [88][208/233]	Loss 0.0013 (0.0082)	
training:	Epoch: [88][209/233]	Loss 0.0215 (0.0083)	
training:	Epoch: [88][210/233]	Loss 0.0018 (0.0083)	
training:	Epoch: [88][211/233]	Loss 0.0012 (0.0082)	
training:	Epoch: [88][212/233]	Loss 0.0027 (0.0082)	
training:	Epoch: [88][213/233]	Loss 0.0011 (0.0082)	
training:	Epoch: [88][214/233]	Loss 0.0013 (0.0081)	
training:	Epoch: [88][215/233]	Loss 0.0013 (0.0081)	
training:	Epoch: [88][216/233]	Loss 0.0015 (0.0081)	
training:	Epoch: [88][217/233]	Loss 0.0015 (0.0080)	
training:	Epoch: [88][218/233]	Loss 0.0032 (0.0080)	
training:	Epoch: [88][219/233]	Loss 0.0011 (0.0080)	
training:	Epoch: [88][220/233]	Loss 0.0394 (0.0081)	
training:	Epoch: [88][221/233]	Loss 0.0012 (0.0081)	
training:	Epoch: [88][222/233]	Loss 0.0014 (0.0081)	
training:	Epoch: [88][223/233]	Loss 0.0011 (0.0080)	
training:	Epoch: [88][224/233]	Loss 0.0012 (0.0080)	
training:	Epoch: [88][225/233]	Loss 0.0016 (0.0080)	
training:	Epoch: [88][226/233]	Loss 0.2145 (0.0089)	
training:	Epoch: [88][227/233]	Loss 0.0012 (0.0089)	
training:	Epoch: [88][228/233]	Loss 0.0016 (0.0088)	
training:	Epoch: [88][229/233]	Loss 0.0074 (0.0088)	
training:	Epoch: [88][230/233]	Loss 0.0012 (0.0088)	
training:	Epoch: [88][231/233]	Loss 0.0014 (0.0088)	
training:	Epoch: [88][232/233]	Loss 0.0160 (0.0088)	
training:	Epoch: [88][233/233]	Loss 0.0056 (0.0088)	
Training:	 Loss: 0.0088

Training:	 ACC: 0.9995 0.9995 0.9995 0.9994
Validation:	 ACC: 0.7793 0.7774 0.7395 0.8191
Validation:	 Best_BACC: 0.8162 0.8154 0.7998 0.8326
Validation:	 Loss: 1.1455
Pretraining:	Epoch 89/200
----------
training:	Epoch: [89][1/233]	Loss 0.0012 (0.0012)	
training:	Epoch: [89][2/233]	Loss 0.0013 (0.0013)	
training:	Epoch: [89][3/233]	Loss 0.0017 (0.0014)	
training:	Epoch: [89][4/233]	Loss 0.1123 (0.0291)	
training:	Epoch: [89][5/233]	Loss 0.0808 (0.0395)	
training:	Epoch: [89][6/233]	Loss 0.0011 (0.0331)	
training:	Epoch: [89][7/233]	Loss 0.0014 (0.0285)	
training:	Epoch: [89][8/233]	Loss 0.0134 (0.0266)	
training:	Epoch: [89][9/233]	Loss 0.0015 (0.0239)	
training:	Epoch: [89][10/233]	Loss 0.0016 (0.0216)	
training:	Epoch: [89][11/233]	Loss 0.0092 (0.0205)	
training:	Epoch: [89][12/233]	Loss 0.0021 (0.0190)	
training:	Epoch: [89][13/233]	Loss 0.0017 (0.0176)	
training:	Epoch: [89][14/233]	Loss 0.0011 (0.0165)	
training:	Epoch: [89][15/233]	Loss 0.0013 (0.0155)	
training:	Epoch: [89][16/233]	Loss 0.2092 (0.0276)	
training:	Epoch: [89][17/233]	Loss 0.0022 (0.0261)	
training:	Epoch: [89][18/233]	Loss 0.0019 (0.0247)	
training:	Epoch: [89][19/233]	Loss 0.0011 (0.0235)	
training:	Epoch: [89][20/233]	Loss 0.0010 (0.0224)	
training:	Epoch: [89][21/233]	Loss 0.0013 (0.0214)	
training:	Epoch: [89][22/233]	Loss 0.0013 (0.0204)	
training:	Epoch: [89][23/233]	Loss 0.0015 (0.0196)	
training:	Epoch: [89][24/233]	Loss 0.0054 (0.0190)	
training:	Epoch: [89][25/233]	Loss 0.0065 (0.0185)	
training:	Epoch: [89][26/233]	Loss 0.0015 (0.0179)	
training:	Epoch: [89][27/233]	Loss 0.0025 (0.0173)	
training:	Epoch: [89][28/233]	Loss 0.0014 (0.0167)	
training:	Epoch: [89][29/233]	Loss 0.0011 (0.0162)	
training:	Epoch: [89][30/233]	Loss 0.0284 (0.0166)	
training:	Epoch: [89][31/233]	Loss 0.0021 (0.0161)	
training:	Epoch: [89][32/233]	Loss 0.0077 (0.0159)	
training:	Epoch: [89][33/233]	Loss 0.0014 (0.0154)	
training:	Epoch: [89][34/233]	Loss 0.0014 (0.0150)	
training:	Epoch: [89][35/233]	Loss 0.1642 (0.0193)	
training:	Epoch: [89][36/233]	Loss 0.0023 (0.0188)	
training:	Epoch: [89][37/233]	Loss 0.0014 (0.0183)	
training:	Epoch: [89][38/233]	Loss 0.0229 (0.0185)	
training:	Epoch: [89][39/233]	Loss 0.0014 (0.0180)	
training:	Epoch: [89][40/233]	Loss 0.0011 (0.0176)	
training:	Epoch: [89][41/233]	Loss 0.0011 (0.0172)	
training:	Epoch: [89][42/233]	Loss 0.0011 (0.0168)	
training:	Epoch: [89][43/233]	Loss 0.0014 (0.0165)	
training:	Epoch: [89][44/233]	Loss 0.0017 (0.0161)	
training:	Epoch: [89][45/233]	Loss 0.2123 (0.0205)	
training:	Epoch: [89][46/233]	Loss 0.0011 (0.0201)	
training:	Epoch: [89][47/233]	Loss 0.0038 (0.0197)	
training:	Epoch: [89][48/233]	Loss 0.0011 (0.0193)	
training:	Epoch: [89][49/233]	Loss 0.0043 (0.0190)	
training:	Epoch: [89][50/233]	Loss 0.0023 (0.0187)	
training:	Epoch: [89][51/233]	Loss 0.2107 (0.0224)	
training:	Epoch: [89][52/233]	Loss 0.0010 (0.0220)	
training:	Epoch: [89][53/233]	Loss 0.0026 (0.0217)	
training:	Epoch: [89][54/233]	Loss 0.0013 (0.0213)	
training:	Epoch: [89][55/233]	Loss 0.0012 (0.0209)	
training:	Epoch: [89][56/233]	Loss 0.0011 (0.0206)	
training:	Epoch: [89][57/233]	Loss 0.0016 (0.0202)	
training:	Epoch: [89][58/233]	Loss 0.0017 (0.0199)	
training:	Epoch: [89][59/233]	Loss 0.0013 (0.0196)	
training:	Epoch: [89][60/233]	Loss 0.0015 (0.0193)	
training:	Epoch: [89][61/233]	Loss 0.0012 (0.0190)	
training:	Epoch: [89][62/233]	Loss 0.0044 (0.0188)	
training:	Epoch: [89][63/233]	Loss 0.0213 (0.0188)	
training:	Epoch: [89][64/233]	Loss 0.0013 (0.0185)	
training:	Epoch: [89][65/233]	Loss 0.0010 (0.0183)	
training:	Epoch: [89][66/233]	Loss 0.0014 (0.0180)	
training:	Epoch: [89][67/233]	Loss 0.0151 (0.0180)	
training:	Epoch: [89][68/233]	Loss 0.0019 (0.0177)	
training:	Epoch: [89][69/233]	Loss 0.0013 (0.0175)	
training:	Epoch: [89][70/233]	Loss 0.0017 (0.0173)	
training:	Epoch: [89][71/233]	Loss 0.0012 (0.0170)	
training:	Epoch: [89][72/233]	Loss 0.0011 (0.0168)	
training:	Epoch: [89][73/233]	Loss 0.0011 (0.0166)	
training:	Epoch: [89][74/233]	Loss 0.0012 (0.0164)	
training:	Epoch: [89][75/233]	Loss 0.0015 (0.0162)	
training:	Epoch: [89][76/233]	Loss 0.0016 (0.0160)	
training:	Epoch: [89][77/233]	Loss 0.0016 (0.0158)	
training:	Epoch: [89][78/233]	Loss 0.0015 (0.0156)	
training:	Epoch: [89][79/233]	Loss 0.0012 (0.0155)	
training:	Epoch: [89][80/233]	Loss 0.0012 (0.0153)	
training:	Epoch: [89][81/233]	Loss 0.0136 (0.0153)	
training:	Epoch: [89][82/233]	Loss 0.0016 (0.0151)	
training:	Epoch: [89][83/233]	Loss 0.0891 (0.0160)	
training:	Epoch: [89][84/233]	Loss 0.0016 (0.0158)	
training:	Epoch: [89][85/233]	Loss 0.0206 (0.0159)	
training:	Epoch: [89][86/233]	Loss 0.0019 (0.0157)	
training:	Epoch: [89][87/233]	Loss 0.0010 (0.0155)	
training:	Epoch: [89][88/233]	Loss 0.0015 (0.0154)	
training:	Epoch: [89][89/233]	Loss 0.0012 (0.0152)	
training:	Epoch: [89][90/233]	Loss 0.0012 (0.0151)	
training:	Epoch: [89][91/233]	Loss 0.0029 (0.0149)	
training:	Epoch: [89][92/233]	Loss 0.0869 (0.0157)	
training:	Epoch: [89][93/233]	Loss 0.0081 (0.0156)	
training:	Epoch: [89][94/233]	Loss 0.0010 (0.0155)	
training:	Epoch: [89][95/233]	Loss 0.0044 (0.0154)	
training:	Epoch: [89][96/233]	Loss 0.0089 (0.0153)	
training:	Epoch: [89][97/233]	Loss 0.0683 (0.0158)	
training:	Epoch: [89][98/233]	Loss 0.0015 (0.0157)	
training:	Epoch: [89][99/233]	Loss 0.0117 (0.0156)	
training:	Epoch: [89][100/233]	Loss 0.0014 (0.0155)	
training:	Epoch: [89][101/233]	Loss 0.0011 (0.0154)	
training:	Epoch: [89][102/233]	Loss 0.0021 (0.0152)	
training:	Epoch: [89][103/233]	Loss 0.0012 (0.0151)	
training:	Epoch: [89][104/233]	Loss 0.0084 (0.0150)	
training:	Epoch: [89][105/233]	Loss 0.0340 (0.0152)	
training:	Epoch: [89][106/233]	Loss 0.0012 (0.0151)	
training:	Epoch: [89][107/233]	Loss 0.0017 (0.0150)	
training:	Epoch: [89][108/233]	Loss 0.0011 (0.0148)	
training:	Epoch: [89][109/233]	Loss 0.0015 (0.0147)	
training:	Epoch: [89][110/233]	Loss 0.0017 (0.0146)	
training:	Epoch: [89][111/233]	Loss 0.0016 (0.0145)	
training:	Epoch: [89][112/233]	Loss 0.0019 (0.0144)	
training:	Epoch: [89][113/233]	Loss 0.0019 (0.0142)	
training:	Epoch: [89][114/233]	Loss 0.0048 (0.0142)	
training:	Epoch: [89][115/233]	Loss 0.0011 (0.0140)	
training:	Epoch: [89][116/233]	Loss 0.0024 (0.0139)	
training:	Epoch: [89][117/233]	Loss 0.0011 (0.0138)	
training:	Epoch: [89][118/233]	Loss 0.0012 (0.0137)	
training:	Epoch: [89][119/233]	Loss 0.0028 (0.0136)	
training:	Epoch: [89][120/233]	Loss 0.0014 (0.0135)	
training:	Epoch: [89][121/233]	Loss 0.0089 (0.0135)	
training:	Epoch: [89][122/233]	Loss 0.0016 (0.0134)	
training:	Epoch: [89][123/233]	Loss 0.0036 (0.0133)	
training:	Epoch: [89][124/233]	Loss 0.0027 (0.0132)	
training:	Epoch: [89][125/233]	Loss 0.1038 (0.0140)	
training:	Epoch: [89][126/233]	Loss 0.0013 (0.0139)	
training:	Epoch: [89][127/233]	Loss 0.0024 (0.0138)	
training:	Epoch: [89][128/233]	Loss 0.0013 (0.0137)	
training:	Epoch: [89][129/233]	Loss 0.0014 (0.0136)	
training:	Epoch: [89][130/233]	Loss 0.0013 (0.0135)	
training:	Epoch: [89][131/233]	Loss 0.0018 (0.0134)	
training:	Epoch: [89][132/233]	Loss 0.0014 (0.0133)	
training:	Epoch: [89][133/233]	Loss 0.0026 (0.0132)	
training:	Epoch: [89][134/233]	Loss 0.0013 (0.0131)	
training:	Epoch: [89][135/233]	Loss 0.0631 (0.0135)	
training:	Epoch: [89][136/233]	Loss 0.1039 (0.0142)	
training:	Epoch: [89][137/233]	Loss 0.0012 (0.0141)	
training:	Epoch: [89][138/233]	Loss 0.0022 (0.0140)	
training:	Epoch: [89][139/233]	Loss 0.0014 (0.0139)	
training:	Epoch: [89][140/233]	Loss 0.1228 (0.0147)	
training:	Epoch: [89][141/233]	Loss 0.0041 (0.0146)	
training:	Epoch: [89][142/233]	Loss 0.1049 (0.0152)	
training:	Epoch: [89][143/233]	Loss 0.0334 (0.0154)	
training:	Epoch: [89][144/233]	Loss 0.0013 (0.0153)	
training:	Epoch: [89][145/233]	Loss 0.0100 (0.0152)	
training:	Epoch: [89][146/233]	Loss 0.0021 (0.0151)	
training:	Epoch: [89][147/233]	Loss 0.0042 (0.0151)	
training:	Epoch: [89][148/233]	Loss 0.0012 (0.0150)	
training:	Epoch: [89][149/233]	Loss 0.0025 (0.0149)	
training:	Epoch: [89][150/233]	Loss 0.2197 (0.0163)	
training:	Epoch: [89][151/233]	Loss 0.0017 (0.0162)	
training:	Epoch: [89][152/233]	Loss 0.0024 (0.0161)	
training:	Epoch: [89][153/233]	Loss 0.0089 (0.0160)	
training:	Epoch: [89][154/233]	Loss 0.0013 (0.0159)	
training:	Epoch: [89][155/233]	Loss 0.0015 (0.0158)	
training:	Epoch: [89][156/233]	Loss 0.0048 (0.0158)	
training:	Epoch: [89][157/233]	Loss 0.0036 (0.0157)	
training:	Epoch: [89][158/233]	Loss 0.0122 (0.0157)	
training:	Epoch: [89][159/233]	Loss 0.0014 (0.0156)	
training:	Epoch: [89][160/233]	Loss 0.1482 (0.0164)	
training:	Epoch: [89][161/233]	Loss 0.0018 (0.0163)	
training:	Epoch: [89][162/233]	Loss 0.0010 (0.0162)	
training:	Epoch: [89][163/233]	Loss 0.0023 (0.0161)	
training:	Epoch: [89][164/233]	Loss 0.0125 (0.0161)	
training:	Epoch: [89][165/233]	Loss 0.0013 (0.0160)	
training:	Epoch: [89][166/233]	Loss 0.0012 (0.0159)	
training:	Epoch: [89][167/233]	Loss 0.0015 (0.0158)	
training:	Epoch: [89][168/233]	Loss 0.0015 (0.0158)	
training:	Epoch: [89][169/233]	Loss 0.0015 (0.0157)	
training:	Epoch: [89][170/233]	Loss 0.0029 (0.0156)	
training:	Epoch: [89][171/233]	Loss 0.0012 (0.0155)	
training:	Epoch: [89][172/233]	Loss 0.0016 (0.0154)	
training:	Epoch: [89][173/233]	Loss 0.0035 (0.0154)	
training:	Epoch: [89][174/233]	Loss 0.0053 (0.0153)	
training:	Epoch: [89][175/233]	Loss 0.0024 (0.0152)	
training:	Epoch: [89][176/233]	Loss 0.0021 (0.0152)	
training:	Epoch: [89][177/233]	Loss 0.1703 (0.0160)	
training:	Epoch: [89][178/233]	Loss 0.0039 (0.0160)	
training:	Epoch: [89][179/233]	Loss 0.0110 (0.0159)	
training:	Epoch: [89][180/233]	Loss 0.0017 (0.0159)	
training:	Epoch: [89][181/233]	Loss 0.0033 (0.0158)	
training:	Epoch: [89][182/233]	Loss 0.0015 (0.0157)	
training:	Epoch: [89][183/233]	Loss 0.0080 (0.0157)	
training:	Epoch: [89][184/233]	Loss 0.0084 (0.0156)	
training:	Epoch: [89][185/233]	Loss 0.0021 (0.0156)	
training:	Epoch: [89][186/233]	Loss 0.0012 (0.0155)	
training:	Epoch: [89][187/233]	Loss 0.0632 (0.0157)	
training:	Epoch: [89][188/233]	Loss 0.0012 (0.0157)	
training:	Epoch: [89][189/233]	Loss 0.0020 (0.0156)	
training:	Epoch: [89][190/233]	Loss 0.0015 (0.0155)	
training:	Epoch: [89][191/233]	Loss 0.0013 (0.0154)	
training:	Epoch: [89][192/233]	Loss 0.0194 (0.0155)	
training:	Epoch: [89][193/233]	Loss 0.0018 (0.0154)	
training:	Epoch: [89][194/233]	Loss 0.0019 (0.0153)	
training:	Epoch: [89][195/233]	Loss 0.0037 (0.0153)	
training:	Epoch: [89][196/233]	Loss 0.0126 (0.0152)	
training:	Epoch: [89][197/233]	Loss 0.0605 (0.0155)	
training:	Epoch: [89][198/233]	Loss 0.0013 (0.0154)	
training:	Epoch: [89][199/233]	Loss 0.0320 (0.0155)	
training:	Epoch: [89][200/233]	Loss 0.0011 (0.0154)	
training:	Epoch: [89][201/233]	Loss 0.0011 (0.0153)	
training:	Epoch: [89][202/233]	Loss 0.0085 (0.0153)	
training:	Epoch: [89][203/233]	Loss 0.0222 (0.0153)	
training:	Epoch: [89][204/233]	Loss 0.0040 (0.0153)	
training:	Epoch: [89][205/233]	Loss 0.0026 (0.0152)	
training:	Epoch: [89][206/233]	Loss 0.0041 (0.0152)	
training:	Epoch: [89][207/233]	Loss 0.0789 (0.0155)	
training:	Epoch: [89][208/233]	Loss 0.0012 (0.0154)	
training:	Epoch: [89][209/233]	Loss 0.0012 (0.0153)	
training:	Epoch: [89][210/233]	Loss 0.0021 (0.0153)	
training:	Epoch: [89][211/233]	Loss 0.0019 (0.0152)	
training:	Epoch: [89][212/233]	Loss 0.0068 (0.0152)	
training:	Epoch: [89][213/233]	Loss 0.0013 (0.0151)	
training:	Epoch: [89][214/233]	Loss 0.0045 (0.0151)	
training:	Epoch: [89][215/233]	Loss 0.0026 (0.0150)	
training:	Epoch: [89][216/233]	Loss 0.0013 (0.0149)	
training:	Epoch: [89][217/233]	Loss 0.0733 (0.0152)	
training:	Epoch: [89][218/233]	Loss 0.0184 (0.0152)	
training:	Epoch: [89][219/233]	Loss 0.0025 (0.0152)	
training:	Epoch: [89][220/233]	Loss 0.0011 (0.0151)	
training:	Epoch: [89][221/233]	Loss 0.0021 (0.0150)	
training:	Epoch: [89][222/233]	Loss 0.0012 (0.0150)	
training:	Epoch: [89][223/233]	Loss 0.0015 (0.0149)	
training:	Epoch: [89][224/233]	Loss 0.0012 (0.0149)	
training:	Epoch: [89][225/233]	Loss 0.1882 (0.0156)	
training:	Epoch: [89][226/233]	Loss 0.0023 (0.0156)	
training:	Epoch: [89][227/233]	Loss 0.0016 (0.0155)	
training:	Epoch: [89][228/233]	Loss 0.0014 (0.0154)	
training:	Epoch: [89][229/233]	Loss 0.0155 (0.0154)	
training:	Epoch: [89][230/233]	Loss 0.0014 (0.0154)	
training:	Epoch: [89][231/233]	Loss 0.0013 (0.0153)	
training:	Epoch: [89][232/233]	Loss 0.0230 (0.0154)	
training:	Epoch: [89][233/233]	Loss 0.0267 (0.0154)	
Training:	 Loss: 0.0154

Training:	 ACC: 0.9995 0.9995 0.9995 0.9994
Validation:	 ACC: 0.7944 0.7956 0.8192 0.7697
Validation:	 Best_BACC: 0.8162 0.8154 0.7998 0.8326
Validation:	 Loss: 1.0360
Pretraining:	Epoch 90/200
----------
training:	Epoch: [90][1/233]	Loss 0.0011 (0.0011)	
training:	Epoch: [90][2/233]	Loss 0.0011 (0.0011)	
training:	Epoch: [90][3/233]	Loss 0.0656 (0.0226)	
training:	Epoch: [90][4/233]	Loss 0.0011 (0.0172)	
training:	Epoch: [90][5/233]	Loss 0.0012 (0.0140)	
training:	Epoch: [90][6/233]	Loss 0.0038 (0.0123)	
training:	Epoch: [90][7/233]	Loss 0.0165 (0.0129)	
training:	Epoch: [90][8/233]	Loss 0.0728 (0.0204)	
training:	Epoch: [90][9/233]	Loss 0.0010 (0.0183)	
training:	Epoch: [90][10/233]	Loss 0.0010 (0.0165)	
training:	Epoch: [90][11/233]	Loss 0.0013 (0.0151)	
training:	Epoch: [90][12/233]	Loss 0.2119 (0.0315)	
training:	Epoch: [90][13/233]	Loss 0.0125 (0.0301)	
training:	Epoch: [90][14/233]	Loss 0.0064 (0.0284)	
training:	Epoch: [90][15/233]	Loss 0.2257 (0.0415)	
training:	Epoch: [90][16/233]	Loss 0.0030 (0.0391)	
training:	Epoch: [90][17/233]	Loss 0.0095 (0.0374)	
training:	Epoch: [90][18/233]	Loss 0.0023 (0.0354)	
training:	Epoch: [90][19/233]	Loss 0.0018 (0.0337)	
training:	Epoch: [90][20/233]	Loss 0.0014 (0.0321)	
training:	Epoch: [90][21/233]	Loss 0.0022 (0.0306)	
training:	Epoch: [90][22/233]	Loss 0.0028 (0.0294)	
training:	Epoch: [90][23/233]	Loss 0.0011 (0.0281)	
training:	Epoch: [90][24/233]	Loss 0.0020 (0.0270)	
training:	Epoch: [90][25/233]	Loss 0.0021 (0.0260)	
training:	Epoch: [90][26/233]	Loss 0.0106 (0.0255)	
training:	Epoch: [90][27/233]	Loss 0.0013 (0.0246)	
training:	Epoch: [90][28/233]	Loss 0.0035 (0.0238)	
training:	Epoch: [90][29/233]	Loss 0.0014 (0.0230)	
training:	Epoch: [90][30/233]	Loss 0.0014 (0.0223)	
training:	Epoch: [90][31/233]	Loss 0.0013 (0.0216)	
training:	Epoch: [90][32/233]	Loss 0.0019 (0.0210)	
training:	Epoch: [90][33/233]	Loss 0.0015 (0.0204)	
training:	Epoch: [90][34/233]	Loss 0.0033 (0.0199)	
training:	Epoch: [90][35/233]	Loss 0.0043 (0.0195)	
training:	Epoch: [90][36/233]	Loss 0.0044 (0.0191)	
training:	Epoch: [90][37/233]	Loss 0.0018 (0.0186)	
training:	Epoch: [90][38/233]	Loss 0.0016 (0.0181)	
training:	Epoch: [90][39/233]	Loss 0.0013 (0.0177)	
training:	Epoch: [90][40/233]	Loss 0.0016 (0.0173)	
training:	Epoch: [90][41/233]	Loss 0.0013 (0.0169)	
training:	Epoch: [90][42/233]	Loss 0.0013 (0.0165)	
training:	Epoch: [90][43/233]	Loss 0.0031 (0.0162)	
training:	Epoch: [90][44/233]	Loss 0.0019 (0.0159)	
training:	Epoch: [90][45/233]	Loss 0.0013 (0.0156)	
training:	Epoch: [90][46/233]	Loss 0.0021 (0.0153)	
training:	Epoch: [90][47/233]	Loss 0.0012 (0.0150)	
training:	Epoch: [90][48/233]	Loss 0.0019 (0.0147)	
training:	Epoch: [90][49/233]	Loss 0.0982 (0.0164)	
training:	Epoch: [90][50/233]	Loss 0.0820 (0.0177)	
training:	Epoch: [90][51/233]	Loss 0.0018 (0.0174)	
training:	Epoch: [90][52/233]	Loss 0.0012 (0.0171)	
training:	Epoch: [90][53/233]	Loss 0.0012 (0.0168)	
training:	Epoch: [90][54/233]	Loss 0.0012 (0.0165)	
training:	Epoch: [90][55/233]	Loss 0.0016 (0.0162)	
training:	Epoch: [90][56/233]	Loss 0.0030 (0.0160)	
training:	Epoch: [90][57/233]	Loss 0.0013 (0.0158)	
training:	Epoch: [90][58/233]	Loss 0.0067 (0.0156)	
training:	Epoch: [90][59/233]	Loss 0.0153 (0.0156)	
training:	Epoch: [90][60/233]	Loss 0.0024 (0.0154)	
training:	Epoch: [90][61/233]	Loss 0.0015 (0.0151)	
training:	Epoch: [90][62/233]	Loss 0.0869 (0.0163)	
training:	Epoch: [90][63/233]	Loss 0.0025 (0.0161)	
training:	Epoch: [90][64/233]	Loss 0.0134 (0.0160)	
training:	Epoch: [90][65/233]	Loss 0.0015 (0.0158)	
training:	Epoch: [90][66/233]	Loss 0.0019 (0.0156)	
training:	Epoch: [90][67/233]	Loss 0.0013 (0.0154)	
training:	Epoch: [90][68/233]	Loss 0.0303 (0.0156)	
training:	Epoch: [90][69/233]	Loss 0.0016 (0.0154)	
training:	Epoch: [90][70/233]	Loss 0.0040 (0.0152)	
training:	Epoch: [90][71/233]	Loss 0.0028 (0.0151)	
training:	Epoch: [90][72/233]	Loss 0.0015 (0.0149)	
training:	Epoch: [90][73/233]	Loss 0.0017 (0.0147)	
training:	Epoch: [90][74/233]	Loss 0.0048 (0.0146)	
training:	Epoch: [90][75/233]	Loss 0.0016 (0.0144)	
training:	Epoch: [90][76/233]	Loss 0.0011 (0.0142)	
training:	Epoch: [90][77/233]	Loss 0.0013 (0.0141)	
training:	Epoch: [90][78/233]	Loss 0.0012 (0.0139)	
training:	Epoch: [90][79/233]	Loss 0.0088 (0.0138)	
training:	Epoch: [90][80/233]	Loss 0.0017 (0.0137)	
training:	Epoch: [90][81/233]	Loss 0.0029 (0.0135)	
training:	Epoch: [90][82/233]	Loss 0.0013 (0.0134)	
training:	Epoch: [90][83/233]	Loss 0.0016 (0.0132)	
training:	Epoch: [90][84/233]	Loss 0.0097 (0.0132)	
training:	Epoch: [90][85/233]	Loss 0.0016 (0.0131)	
training:	Epoch: [90][86/233]	Loss 0.0016 (0.0129)	
training:	Epoch: [90][87/233]	Loss 0.0022 (0.0128)	
training:	Epoch: [90][88/233]	Loss 0.0022 (0.0127)	
training:	Epoch: [90][89/233]	Loss 0.0280 (0.0129)	
training:	Epoch: [90][90/233]	Loss 0.0011 (0.0127)	
training:	Epoch: [90][91/233]	Loss 0.0041 (0.0126)	
training:	Epoch: [90][92/233]	Loss 0.0013 (0.0125)	
training:	Epoch: [90][93/233]	Loss 0.0012 (0.0124)	
training:	Epoch: [90][94/233]	Loss 0.0021 (0.0123)	
training:	Epoch: [90][95/233]	Loss 0.1219 (0.0134)	
training:	Epoch: [90][96/233]	Loss 0.0042 (0.0133)	
training:	Epoch: [90][97/233]	Loss 0.0011 (0.0132)	
training:	Epoch: [90][98/233]	Loss 0.0015 (0.0131)	
training:	Epoch: [90][99/233]	Loss 0.0159 (0.0131)	
training:	Epoch: [90][100/233]	Loss 0.0030 (0.0130)	
training:	Epoch: [90][101/233]	Loss 0.0034 (0.0129)	
training:	Epoch: [90][102/233]	Loss 0.0025 (0.0128)	
training:	Epoch: [90][103/233]	Loss 0.0016 (0.0127)	
training:	Epoch: [90][104/233]	Loss 0.0018 (0.0126)	
training:	Epoch: [90][105/233]	Loss 0.0014 (0.0125)	
training:	Epoch: [90][106/233]	Loss 0.0015 (0.0124)	
training:	Epoch: [90][107/233]	Loss 0.0033 (0.0123)	
training:	Epoch: [90][108/233]	Loss 0.0018 (0.0122)	
training:	Epoch: [90][109/233]	Loss 0.0047 (0.0121)	
training:	Epoch: [90][110/233]	Loss 0.0024 (0.0121)	
training:	Epoch: [90][111/233]	Loss 0.0017 (0.0120)	
training:	Epoch: [90][112/233]	Loss 0.0218 (0.0121)	
training:	Epoch: [90][113/233]	Loss 0.0016 (0.0120)	
training:	Epoch: [90][114/233]	Loss 0.0012 (0.0119)	
training:	Epoch: [90][115/233]	Loss 0.0067 (0.0118)	
training:	Epoch: [90][116/233]	Loss 0.0163 (0.0119)	
training:	Epoch: [90][117/233]	Loss 0.0025 (0.0118)	
training:	Epoch: [90][118/233]	Loss 0.0012 (0.0117)	
training:	Epoch: [90][119/233]	Loss 0.0015 (0.0116)	
training:	Epoch: [90][120/233]	Loss 0.0021 (0.0115)	
training:	Epoch: [90][121/233]	Loss 0.0012 (0.0114)	
training:	Epoch: [90][122/233]	Loss 0.2118 (0.0131)	
training:	Epoch: [90][123/233]	Loss 0.0014 (0.0130)	
training:	Epoch: [90][124/233]	Loss 0.0016 (0.0129)	
training:	Epoch: [90][125/233]	Loss 0.0011 (0.0128)	
training:	Epoch: [90][126/233]	Loss 0.0014 (0.0127)	
training:	Epoch: [90][127/233]	Loss 0.0020 (0.0126)	
training:	Epoch: [90][128/233]	Loss 0.0013 (0.0125)	
training:	Epoch: [90][129/233]	Loss 0.0014 (0.0125)	
training:	Epoch: [90][130/233]	Loss 0.0016 (0.0124)	
training:	Epoch: [90][131/233]	Loss 0.2204 (0.0140)	
training:	Epoch: [90][132/233]	Loss 0.1033 (0.0146)	
training:	Epoch: [90][133/233]	Loss 0.0015 (0.0145)	
training:	Epoch: [90][134/233]	Loss 0.0016 (0.0144)	
training:	Epoch: [90][135/233]	Loss 0.0838 (0.0150)	
training:	Epoch: [90][136/233]	Loss 0.0013 (0.0149)	
training:	Epoch: [90][137/233]	Loss 0.0036 (0.0148)	
training:	Epoch: [90][138/233]	Loss 0.0013 (0.0147)	
training:	Epoch: [90][139/233]	Loss 0.0021 (0.0146)	
training:	Epoch: [90][140/233]	Loss 0.0020 (0.0145)	
training:	Epoch: [90][141/233]	Loss 0.0011 (0.0144)	
training:	Epoch: [90][142/233]	Loss 0.0079 (0.0143)	
training:	Epoch: [90][143/233]	Loss 0.0640 (0.0147)	
training:	Epoch: [90][144/233]	Loss 0.0305 (0.0148)	
training:	Epoch: [90][145/233]	Loss 0.0020 (0.0147)	
training:	Epoch: [90][146/233]	Loss 0.0296 (0.0148)	
training:	Epoch: [90][147/233]	Loss 0.0014 (0.0147)	
training:	Epoch: [90][148/233]	Loss 0.0027 (0.0146)	
training:	Epoch: [90][149/233]	Loss 0.0389 (0.0148)	
training:	Epoch: [90][150/233]	Loss 0.0016 (0.0147)	
training:	Epoch: [90][151/233]	Loss 0.0019 (0.0146)	
training:	Epoch: [90][152/233]	Loss 0.0012 (0.0145)	
training:	Epoch: [90][153/233]	Loss 0.0025 (0.0145)	
training:	Epoch: [90][154/233]	Loss 0.0019 (0.0144)	
training:	Epoch: [90][155/233]	Loss 0.0027 (0.0143)	
training:	Epoch: [90][156/233]	Loss 0.0041 (0.0142)	
training:	Epoch: [90][157/233]	Loss 0.0016 (0.0142)	
training:	Epoch: [90][158/233]	Loss 0.0032 (0.0141)	
training:	Epoch: [90][159/233]	Loss 0.0021 (0.0140)	
training:	Epoch: [90][160/233]	Loss 0.0013 (0.0139)	
training:	Epoch: [90][161/233]	Loss 0.0022 (0.0139)	
training:	Epoch: [90][162/233]	Loss 0.0015 (0.0138)	
training:	Epoch: [90][163/233]	Loss 0.0012 (0.0137)	
training:	Epoch: [90][164/233]	Loss 0.0013 (0.0136)	
training:	Epoch: [90][165/233]	Loss 0.0013 (0.0136)	
training:	Epoch: [90][166/233]	Loss 0.0019 (0.0135)	
training:	Epoch: [90][167/233]	Loss 0.0021 (0.0134)	
training:	Epoch: [90][168/233]	Loss 0.0021 (0.0134)	
training:	Epoch: [90][169/233]	Loss 0.0017 (0.0133)	
training:	Epoch: [90][170/233]	Loss 0.0126 (0.0133)	
training:	Epoch: [90][171/233]	Loss 0.0016 (0.0132)	
training:	Epoch: [90][172/233]	Loss 0.0043 (0.0132)	
training:	Epoch: [90][173/233]	Loss 0.0019 (0.0131)	
training:	Epoch: [90][174/233]	Loss 0.0023 (0.0130)	
training:	Epoch: [90][175/233]	Loss 0.0025 (0.0130)	
training:	Epoch: [90][176/233]	Loss 0.0019 (0.0129)	
training:	Epoch: [90][177/233]	Loss 0.0014 (0.0129)	
training:	Epoch: [90][178/233]	Loss 0.0101 (0.0128)	
training:	Epoch: [90][179/233]	Loss 0.0020 (0.0128)	
training:	Epoch: [90][180/233]	Loss 0.0014 (0.0127)	
training:	Epoch: [90][181/233]	Loss 0.0014 (0.0126)	
training:	Epoch: [90][182/233]	Loss 0.0029 (0.0126)	
training:	Epoch: [90][183/233]	Loss 0.0021 (0.0125)	
training:	Epoch: [90][184/233]	Loss 0.0770 (0.0129)	
training:	Epoch: [90][185/233]	Loss 0.0018 (0.0128)	
training:	Epoch: [90][186/233]	Loss 0.0025 (0.0128)	
training:	Epoch: [90][187/233]	Loss 0.0012 (0.0127)	
training:	Epoch: [90][188/233]	Loss 0.0036 (0.0127)	
training:	Epoch: [90][189/233]	Loss 0.0013 (0.0126)	
training:	Epoch: [90][190/233]	Loss 0.0027 (0.0126)	
training:	Epoch: [90][191/233]	Loss 0.0015 (0.0125)	
training:	Epoch: [90][192/233]	Loss 0.0021 (0.0124)	
training:	Epoch: [90][193/233]	Loss 0.0013 (0.0124)	
training:	Epoch: [90][194/233]	Loss 0.0020 (0.0123)	
training:	Epoch: [90][195/233]	Loss 0.0011 (0.0123)	
training:	Epoch: [90][196/233]	Loss 0.0031 (0.0122)	
training:	Epoch: [90][197/233]	Loss 0.0013 (0.0122)	
training:	Epoch: [90][198/233]	Loss 0.0025 (0.0121)	
training:	Epoch: [90][199/233]	Loss 0.0020 (0.0121)	
training:	Epoch: [90][200/233]	Loss 0.0020 (0.0120)	
training:	Epoch: [90][201/233]	Loss 0.0021 (0.0120)	
training:	Epoch: [90][202/233]	Loss 0.0016 (0.0119)	
training:	Epoch: [90][203/233]	Loss 0.0015 (0.0119)	
training:	Epoch: [90][204/233]	Loss 0.0036 (0.0118)	
training:	Epoch: [90][205/233]	Loss 0.0012 (0.0118)	
training:	Epoch: [90][206/233]	Loss 0.0017 (0.0117)	
training:	Epoch: [90][207/233]	Loss 0.0014 (0.0117)	
training:	Epoch: [90][208/233]	Loss 0.0037 (0.0116)	
training:	Epoch: [90][209/233]	Loss 0.0029 (0.0116)	
training:	Epoch: [90][210/233]	Loss 0.0015 (0.0115)	
training:	Epoch: [90][211/233]	Loss 0.0013 (0.0115)	
training:	Epoch: [90][212/233]	Loss 0.0086 (0.0115)	
training:	Epoch: [90][213/233]	Loss 0.0016 (0.0114)	
training:	Epoch: [90][214/233]	Loss 0.0034 (0.0114)	
training:	Epoch: [90][215/233]	Loss 0.0011 (0.0114)	
training:	Epoch: [90][216/233]	Loss 0.0020 (0.0113)	
training:	Epoch: [90][217/233]	Loss 0.0014 (0.0113)	
training:	Epoch: [90][218/233]	Loss 0.0014 (0.0112)	
training:	Epoch: [90][219/233]	Loss 0.0015 (0.0112)	
training:	Epoch: [90][220/233]	Loss 0.0094 (0.0112)	
training:	Epoch: [90][221/233]	Loss 0.0011 (0.0111)	
training:	Epoch: [90][222/233]	Loss 0.0011 (0.0111)	
training:	Epoch: [90][223/233]	Loss 0.0012 (0.0110)	
training:	Epoch: [90][224/233]	Loss 0.0015 (0.0110)	
training:	Epoch: [90][225/233]	Loss 0.0010 (0.0109)	
training:	Epoch: [90][226/233]	Loss 0.0197 (0.0110)	
training:	Epoch: [90][227/233]	Loss 0.0395 (0.0111)	
training:	Epoch: [90][228/233]	Loss 0.0014 (0.0111)	
training:	Epoch: [90][229/233]	Loss 0.0012 (0.0110)	
training:	Epoch: [90][230/233]	Loss 0.0011 (0.0110)	
training:	Epoch: [90][231/233]	Loss 0.0013 (0.0109)	
training:	Epoch: [90][232/233]	Loss 0.0025 (0.0109)	
training:	Epoch: [90][233/233]	Loss 0.0016 (0.0109)	
Training:	 Loss: 0.0108

Training:	 ACC: 0.9996 0.9996 0.9995 0.9997
Validation:	 ACC: 0.7962 0.7956 0.7845 0.8079
Validation:	 Best_BACC: 0.8162 0.8154 0.7998 0.8326
Validation:	 Loss: 1.0497
Pretraining:	Epoch 91/200
----------
training:	Epoch: [91][1/233]	Loss 0.0027 (0.0027)	
training:	Epoch: [91][2/233]	Loss 0.0017 (0.0022)	
training:	Epoch: [91][3/233]	Loss 0.0015 (0.0020)	
training:	Epoch: [91][4/233]	Loss 0.0013 (0.0018)	
training:	Epoch: [91][5/233]	Loss 0.0013 (0.0017)	
training:	Epoch: [91][6/233]	Loss 0.0019 (0.0017)	
training:	Epoch: [91][7/233]	Loss 0.0017 (0.0017)	
training:	Epoch: [91][8/233]	Loss 0.0017 (0.0017)	
training:	Epoch: [91][9/233]	Loss 0.0011 (0.0017)	
training:	Epoch: [91][10/233]	Loss 0.0017 (0.0017)	
training:	Epoch: [91][11/233]	Loss 0.0224 (0.0035)	
training:	Epoch: [91][12/233]	Loss 0.0012 (0.0033)	
training:	Epoch: [91][13/233]	Loss 0.0031 (0.0033)	
training:	Epoch: [91][14/233]	Loss 0.0014 (0.0032)	
training:	Epoch: [91][15/233]	Loss 0.0015 (0.0031)	
training:	Epoch: [91][16/233]	Loss 0.0013 (0.0030)	
training:	Epoch: [91][17/233]	Loss 0.0050 (0.0031)	
training:	Epoch: [91][18/233]	Loss 0.0015 (0.0030)	
training:	Epoch: [91][19/233]	Loss 0.0105 (0.0034)	
training:	Epoch: [91][20/233]	Loss 0.1859 (0.0125)	
training:	Epoch: [91][21/233]	Loss 0.0018 (0.0120)	
training:	Epoch: [91][22/233]	Loss 0.0017 (0.0115)	
training:	Epoch: [91][23/233]	Loss 0.0019 (0.0111)	
training:	Epoch: [91][24/233]	Loss 0.0024 (0.0108)	
training:	Epoch: [91][25/233]	Loss 0.0016 (0.0104)	
training:	Epoch: [91][26/233]	Loss 0.0030 (0.0101)	
training:	Epoch: [91][27/233]	Loss 0.0015 (0.0098)	
training:	Epoch: [91][28/233]	Loss 0.0012 (0.0095)	
training:	Epoch: [91][29/233]	Loss 0.0013 (0.0092)	
training:	Epoch: [91][30/233]	Loss 0.0015 (0.0089)	
training:	Epoch: [91][31/233]	Loss 0.0011 (0.0087)	
training:	Epoch: [91][32/233]	Loss 0.0143 (0.0089)	
training:	Epoch: [91][33/233]	Loss 0.0034 (0.0087)	
training:	Epoch: [91][34/233]	Loss 0.0011 (0.0085)	
training:	Epoch: [91][35/233]	Loss 0.0013 (0.0083)	
training:	Epoch: [91][36/233]	Loss 0.0011 (0.0081)	
training:	Epoch: [91][37/233]	Loss 0.0014 (0.0079)	
training:	Epoch: [91][38/233]	Loss 0.0012 (0.0077)	
training:	Epoch: [91][39/233]	Loss 0.0013 (0.0076)	
training:	Epoch: [91][40/233]	Loss 0.0012 (0.0074)	
training:	Epoch: [91][41/233]	Loss 0.0015 (0.0072)	
training:	Epoch: [91][42/233]	Loss 0.0011 (0.0071)	
training:	Epoch: [91][43/233]	Loss 0.0015 (0.0070)	
training:	Epoch: [91][44/233]	Loss 0.0011 (0.0068)	
training:	Epoch: [91][45/233]	Loss 0.0032 (0.0068)	
training:	Epoch: [91][46/233]	Loss 0.0010 (0.0066)	
training:	Epoch: [91][47/233]	Loss 0.0019 (0.0065)	
training:	Epoch: [91][48/233]	Loss 0.0017 (0.0064)	
training:	Epoch: [91][49/233]	Loss 0.0016 (0.0063)	
training:	Epoch: [91][50/233]	Loss 0.0013 (0.0062)	
training:	Epoch: [91][51/233]	Loss 0.0019 (0.0061)	
training:	Epoch: [91][52/233]	Loss 0.0012 (0.0060)	
training:	Epoch: [91][53/233]	Loss 0.0203 (0.0063)	
training:	Epoch: [91][54/233]	Loss 0.0017 (0.0062)	
training:	Epoch: [91][55/233]	Loss 0.0014 (0.0061)	
training:	Epoch: [91][56/233]	Loss 0.0016 (0.0061)	
training:	Epoch: [91][57/233]	Loss 0.0011 (0.0060)	
training:	Epoch: [91][58/233]	Loss 0.0014 (0.0059)	
training:	Epoch: [91][59/233]	Loss 0.0010 (0.0058)	
training:	Epoch: [91][60/233]	Loss 0.0064 (0.0058)	
training:	Epoch: [91][61/233]	Loss 0.0013 (0.0057)	
training:	Epoch: [91][62/233]	Loss 0.0011 (0.0057)	
training:	Epoch: [91][63/233]	Loss 0.0012 (0.0056)	
training:	Epoch: [91][64/233]	Loss 0.0013 (0.0055)	
training:	Epoch: [91][65/233]	Loss 0.0017 (0.0055)	
training:	Epoch: [91][66/233]	Loss 0.0012 (0.0054)	
training:	Epoch: [91][67/233]	Loss 0.0014 (0.0054)	
training:	Epoch: [91][68/233]	Loss 0.0026 (0.0053)	
training:	Epoch: [91][69/233]	Loss 0.0019 (0.0053)	
training:	Epoch: [91][70/233]	Loss 0.0020 (0.0052)	
training:	Epoch: [91][71/233]	Loss 0.0027 (0.0052)	
training:	Epoch: [91][72/233]	Loss 0.0014 (0.0051)	
training:	Epoch: [91][73/233]	Loss 0.0010 (0.0051)	
training:	Epoch: [91][74/233]	Loss 0.0012 (0.0050)	
training:	Epoch: [91][75/233]	Loss 0.0018 (0.0050)	
training:	Epoch: [91][76/233]	Loss 0.0012 (0.0049)	
training:	Epoch: [91][77/233]	Loss 0.0013 (0.0049)	
training:	Epoch: [91][78/233]	Loss 0.0015 (0.0048)	
training:	Epoch: [91][79/233]	Loss 0.0012 (0.0048)	
training:	Epoch: [91][80/233]	Loss 0.0011 (0.0047)	
training:	Epoch: [91][81/233]	Loss 0.0019 (0.0047)	
training:	Epoch: [91][82/233]	Loss 0.0037 (0.0047)	
training:	Epoch: [91][83/233]	Loss 0.0011 (0.0047)	
training:	Epoch: [91][84/233]	Loss 0.0020 (0.0046)	
training:	Epoch: [91][85/233]	Loss 0.0042 (0.0046)	
training:	Epoch: [91][86/233]	Loss 0.0012 (0.0046)	
training:	Epoch: [91][87/233]	Loss 0.0013 (0.0045)	
training:	Epoch: [91][88/233]	Loss 0.0030 (0.0045)	
training:	Epoch: [91][89/233]	Loss 0.0010 (0.0045)	
training:	Epoch: [91][90/233]	Loss 0.0252 (0.0047)	
training:	Epoch: [91][91/233]	Loss 0.0015 (0.0047)	
training:	Epoch: [91][92/233]	Loss 0.0010 (0.0046)	
training:	Epoch: [91][93/233]	Loss 0.0012 (0.0046)	
training:	Epoch: [91][94/233]	Loss 0.0017 (0.0046)	
training:	Epoch: [91][95/233]	Loss 0.0032 (0.0046)	
training:	Epoch: [91][96/233]	Loss 0.0018 (0.0045)	
training:	Epoch: [91][97/233]	Loss 0.1952 (0.0065)	
training:	Epoch: [91][98/233]	Loss 0.0010 (0.0064)	
training:	Epoch: [91][99/233]	Loss 0.0014 (0.0064)	
training:	Epoch: [91][100/233]	Loss 0.2145 (0.0085)	
training:	Epoch: [91][101/233]	Loss 0.0011 (0.0084)	
training:	Epoch: [91][102/233]	Loss 0.0022 (0.0083)	
training:	Epoch: [91][103/233]	Loss 0.0012 (0.0083)	
training:	Epoch: [91][104/233]	Loss 0.0706 (0.0089)	
training:	Epoch: [91][105/233]	Loss 0.0014 (0.0088)	
training:	Epoch: [91][106/233]	Loss 0.0011 (0.0087)	
training:	Epoch: [91][107/233]	Loss 0.1849 (0.0104)	
training:	Epoch: [91][108/233]	Loss 0.0011 (0.0103)	
training:	Epoch: [91][109/233]	Loss 0.0011 (0.0102)	
training:	Epoch: [91][110/233]	Loss 0.0012 (0.0101)	
training:	Epoch: [91][111/233]	Loss 0.0012 (0.0100)	
training:	Epoch: [91][112/233]	Loss 0.0011 (0.0100)	
training:	Epoch: [91][113/233]	Loss 0.0022 (0.0099)	
training:	Epoch: [91][114/233]	Loss 0.0024 (0.0098)	
training:	Epoch: [91][115/233]	Loss 0.0133 (0.0099)	
training:	Epoch: [91][116/233]	Loss 0.0032 (0.0098)	
training:	Epoch: [91][117/233]	Loss 0.0042 (0.0097)	
training:	Epoch: [91][118/233]	Loss 0.0010 (0.0097)	
training:	Epoch: [91][119/233]	Loss 0.0015 (0.0096)	
training:	Epoch: [91][120/233]	Loss 0.0017 (0.0095)	
training:	Epoch: [91][121/233]	Loss 0.0011 (0.0095)	
training:	Epoch: [91][122/233]	Loss 0.0013 (0.0094)	
training:	Epoch: [91][123/233]	Loss 0.0014 (0.0093)	
training:	Epoch: [91][124/233]	Loss 0.0013 (0.0093)	
training:	Epoch: [91][125/233]	Loss 0.0019 (0.0092)	
training:	Epoch: [91][126/233]	Loss 0.0014 (0.0092)	
training:	Epoch: [91][127/233]	Loss 0.0032 (0.0091)	
training:	Epoch: [91][128/233]	Loss 0.0011 (0.0090)	
training:	Epoch: [91][129/233]	Loss 0.0013 (0.0090)	
training:	Epoch: [91][130/233]	Loss 0.0023 (0.0089)	
training:	Epoch: [91][131/233]	Loss 0.0042 (0.0089)	
training:	Epoch: [91][132/233]	Loss 0.0015 (0.0088)	
training:	Epoch: [91][133/233]	Loss 0.0014 (0.0088)	
training:	Epoch: [91][134/233]	Loss 0.0011 (0.0087)	
training:	Epoch: [91][135/233]	Loss 0.0013 (0.0087)	
training:	Epoch: [91][136/233]	Loss 0.0018 (0.0086)	
training:	Epoch: [91][137/233]	Loss 0.0013 (0.0086)	
training:	Epoch: [91][138/233]	Loss 0.2027 (0.0100)	
training:	Epoch: [91][139/233]	Loss 0.0038 (0.0099)	
training:	Epoch: [91][140/233]	Loss 0.0022 (0.0099)	
training:	Epoch: [91][141/233]	Loss 0.0011 (0.0098)	
training:	Epoch: [91][142/233]	Loss 0.2223 (0.0113)	
training:	Epoch: [91][143/233]	Loss 0.0012 (0.0112)	
training:	Epoch: [91][144/233]	Loss 0.0028 (0.0112)	
training:	Epoch: [91][145/233]	Loss 0.0011 (0.0111)	
training:	Epoch: [91][146/233]	Loss 0.0011 (0.0110)	
training:	Epoch: [91][147/233]	Loss 0.0015 (0.0110)	
training:	Epoch: [91][148/233]	Loss 0.0010 (0.0109)	
training:	Epoch: [91][149/233]	Loss 0.0074 (0.0109)	
training:	Epoch: [91][150/233]	Loss 0.0013 (0.0108)	
training:	Epoch: [91][151/233]	Loss 0.0012 (0.0108)	
training:	Epoch: [91][152/233]	Loss 0.0012 (0.0107)	
training:	Epoch: [91][153/233]	Loss 0.0012 (0.0106)	
training:	Epoch: [91][154/233]	Loss 0.0639 (0.0110)	
training:	Epoch: [91][155/233]	Loss 0.0012 (0.0109)	
training:	Epoch: [91][156/233]	Loss 0.0043 (0.0109)	
training:	Epoch: [91][157/233]	Loss 0.0058 (0.0108)	
training:	Epoch: [91][158/233]	Loss 0.1460 (0.0117)	
training:	Epoch: [91][159/233]	Loss 0.0016 (0.0116)	
training:	Epoch: [91][160/233]	Loss 0.0021 (0.0116)	
training:	Epoch: [91][161/233]	Loss 0.0013 (0.0115)	
training:	Epoch: [91][162/233]	Loss 0.0015 (0.0114)	
training:	Epoch: [91][163/233]	Loss 0.0052 (0.0114)	
training:	Epoch: [91][164/233]	Loss 0.0021 (0.0114)	
training:	Epoch: [91][165/233]	Loss 0.0010 (0.0113)	
training:	Epoch: [91][166/233]	Loss 0.0029 (0.0112)	
training:	Epoch: [91][167/233]	Loss 0.0021 (0.0112)	
training:	Epoch: [91][168/233]	Loss 0.0012 (0.0111)	
training:	Epoch: [91][169/233]	Loss 0.0022 (0.0111)	
training:	Epoch: [91][170/233]	Loss 0.0022 (0.0110)	
training:	Epoch: [91][171/233]	Loss 0.0023 (0.0110)	
training:	Epoch: [91][172/233]	Loss 0.0016 (0.0109)	
training:	Epoch: [91][173/233]	Loss 0.0016 (0.0109)	
training:	Epoch: [91][174/233]	Loss 0.0015 (0.0108)	
training:	Epoch: [91][175/233]	Loss 0.0017 (0.0108)	
training:	Epoch: [91][176/233]	Loss 0.0013 (0.0107)	
training:	Epoch: [91][177/233]	Loss 0.0015 (0.0106)	
training:	Epoch: [91][178/233]	Loss 0.0047 (0.0106)	
training:	Epoch: [91][179/233]	Loss 0.0013 (0.0106)	
training:	Epoch: [91][180/233]	Loss 0.0013 (0.0105)	
training:	Epoch: [91][181/233]	Loss 0.0042 (0.0105)	
training:	Epoch: [91][182/233]	Loss 0.0029 (0.0104)	
training:	Epoch: [91][183/233]	Loss 0.0014 (0.0104)	
training:	Epoch: [91][184/233]	Loss 0.0111 (0.0104)	
training:	Epoch: [91][185/233]	Loss 0.0026 (0.0103)	
training:	Epoch: [91][186/233]	Loss 0.0039 (0.0103)	
training:	Epoch: [91][187/233]	Loss 0.0015 (0.0103)	
training:	Epoch: [91][188/233]	Loss 0.0238 (0.0103)	
training:	Epoch: [91][189/233]	Loss 0.0022 (0.0103)	
training:	Epoch: [91][190/233]	Loss 0.0014 (0.0102)	
training:	Epoch: [91][191/233]	Loss 0.0012 (0.0102)	
training:	Epoch: [91][192/233]	Loss 0.0012 (0.0102)	
training:	Epoch: [91][193/233]	Loss 0.0026 (0.0101)	
training:	Epoch: [91][194/233]	Loss 0.0012 (0.0101)	
training:	Epoch: [91][195/233]	Loss 0.0014 (0.0100)	
training:	Epoch: [91][196/233]	Loss 0.1178 (0.0106)	
training:	Epoch: [91][197/233]	Loss 0.0014 (0.0105)	
training:	Epoch: [91][198/233]	Loss 0.0011 (0.0105)	
training:	Epoch: [91][199/233]	Loss 0.0012 (0.0104)	
training:	Epoch: [91][200/233]	Loss 0.0028 (0.0104)	
training:	Epoch: [91][201/233]	Loss 0.0287 (0.0105)	
training:	Epoch: [91][202/233]	Loss 0.0012 (0.0104)	
training:	Epoch: [91][203/233]	Loss 0.0016 (0.0104)	
training:	Epoch: [91][204/233]	Loss 0.0013 (0.0104)	
training:	Epoch: [91][205/233]	Loss 0.0248 (0.0104)	
training:	Epoch: [91][206/233]	Loss 0.0105 (0.0104)	
training:	Epoch: [91][207/233]	Loss 0.0011 (0.0104)	
training:	Epoch: [91][208/233]	Loss 0.0035 (0.0103)	
training:	Epoch: [91][209/233]	Loss 0.0156 (0.0104)	
training:	Epoch: [91][210/233]	Loss 0.0012 (0.0103)	
training:	Epoch: [91][211/233]	Loss 0.0013 (0.0103)	
training:	Epoch: [91][212/233]	Loss 0.0048 (0.0103)	
training:	Epoch: [91][213/233]	Loss 0.0685 (0.0105)	
training:	Epoch: [91][214/233]	Loss 0.0019 (0.0105)	
training:	Epoch: [91][215/233]	Loss 0.0011 (0.0104)	
training:	Epoch: [91][216/233]	Loss 0.0012 (0.0104)	
training:	Epoch: [91][217/233]	Loss 0.0016 (0.0104)	
training:	Epoch: [91][218/233]	Loss 0.0017 (0.0103)	
training:	Epoch: [91][219/233]	Loss 0.0022 (0.0103)	
training:	Epoch: [91][220/233]	Loss 0.0101 (0.0103)	
training:	Epoch: [91][221/233]	Loss 0.0012 (0.0102)	
training:	Epoch: [91][222/233]	Loss 0.0016 (0.0102)	
training:	Epoch: [91][223/233]	Loss 0.0013 (0.0102)	
training:	Epoch: [91][224/233]	Loss 0.0019 (0.0101)	
training:	Epoch: [91][225/233]	Loss 0.0033 (0.0101)	
training:	Epoch: [91][226/233]	Loss 0.0015 (0.0101)	
training:	Epoch: [91][227/233]	Loss 0.0013 (0.0100)	
training:	Epoch: [91][228/233]	Loss 0.0016 (0.0100)	
training:	Epoch: [91][229/233]	Loss 0.0016 (0.0100)	
training:	Epoch: [91][230/233]	Loss 0.0015 (0.0099)	
training:	Epoch: [91][231/233]	Loss 0.0014 (0.0099)	
training:	Epoch: [91][232/233]	Loss 0.0023 (0.0098)	
training:	Epoch: [91][233/233]	Loss 0.0013 (0.0098)	
Training:	 Loss: 0.0098

Training:	 ACC: 0.9995 0.9995 0.9995 0.9994
Validation:	 ACC: 0.7868 0.7881 0.8151 0.7584
Validation:	 Best_BACC: 0.8162 0.8154 0.7998 0.8326
Validation:	 Loss: 1.0834
Pretraining:	Epoch 92/200
----------
training:	Epoch: [92][1/233]	Loss 0.0024 (0.0024)	
training:	Epoch: [92][2/233]	Loss 0.0012 (0.0018)	
training:	Epoch: [92][3/233]	Loss 0.0050 (0.0029)	
training:	Epoch: [92][4/233]	Loss 0.0132 (0.0054)	
training:	Epoch: [92][5/233]	Loss 0.0015 (0.0047)	
training:	Epoch: [92][6/233]	Loss 0.0023 (0.0043)	
training:	Epoch: [92][7/233]	Loss 0.0033 (0.0041)	
training:	Epoch: [92][8/233]	Loss 0.0034 (0.0040)	
training:	Epoch: [92][9/233]	Loss 0.0013 (0.0037)	
training:	Epoch: [92][10/233]	Loss 0.0016 (0.0035)	
training:	Epoch: [92][11/233]	Loss 0.0070 (0.0038)	
training:	Epoch: [92][12/233]	Loss 0.0014 (0.0036)	
training:	Epoch: [92][13/233]	Loss 0.0054 (0.0038)	
training:	Epoch: [92][14/233]	Loss 0.0326 (0.0058)	
training:	Epoch: [92][15/233]	Loss 0.0138 (0.0064)	
training:	Epoch: [92][16/233]	Loss 0.0014 (0.0060)	
training:	Epoch: [92][17/233]	Loss 0.0012 (0.0058)	
training:	Epoch: [92][18/233]	Loss 0.0017 (0.0055)	
training:	Epoch: [92][19/233]	Loss 0.0070 (0.0056)	
training:	Epoch: [92][20/233]	Loss 0.0010 (0.0054)	
training:	Epoch: [92][21/233]	Loss 0.0010 (0.0052)	
training:	Epoch: [92][22/233]	Loss 0.0031 (0.0051)	
training:	Epoch: [92][23/233]	Loss 0.0011 (0.0049)	
training:	Epoch: [92][24/233]	Loss 0.0012 (0.0048)	
training:	Epoch: [92][25/233]	Loss 0.0021 (0.0046)	
training:	Epoch: [92][26/233]	Loss 0.0022 (0.0046)	
training:	Epoch: [92][27/233]	Loss 0.0521 (0.0063)	
training:	Epoch: [92][28/233]	Loss 0.0013 (0.0061)	
training:	Epoch: [92][29/233]	Loss 0.0014 (0.0060)	
training:	Epoch: [92][30/233]	Loss 0.0012 (0.0058)	
training:	Epoch: [92][31/233]	Loss 0.0063 (0.0058)	
training:	Epoch: [92][32/233]	Loss 0.0015 (0.0057)	
training:	Epoch: [92][33/233]	Loss 0.0197 (0.0061)	
training:	Epoch: [92][34/233]	Loss 0.0013 (0.0060)	
training:	Epoch: [92][35/233]	Loss 0.0009 (0.0058)	
training:	Epoch: [92][36/233]	Loss 0.0019 (0.0057)	
training:	Epoch: [92][37/233]	Loss 0.0014 (0.0056)	
training:	Epoch: [92][38/233]	Loss 0.0011 (0.0055)	
training:	Epoch: [92][39/233]	Loss 0.0011 (0.0054)	
training:	Epoch: [92][40/233]	Loss 0.0016 (0.0053)	
training:	Epoch: [92][41/233]	Loss 0.0013 (0.0052)	
training:	Epoch: [92][42/233]	Loss 0.0013 (0.0051)	
training:	Epoch: [92][43/233]	Loss 0.0012 (0.0050)	
training:	Epoch: [92][44/233]	Loss 0.0016 (0.0049)	
training:	Epoch: [92][45/233]	Loss 0.0011 (0.0048)	
training:	Epoch: [92][46/233]	Loss 0.0242 (0.0053)	
training:	Epoch: [92][47/233]	Loss 0.0012 (0.0052)	
training:	Epoch: [92][48/233]	Loss 0.0011 (0.0051)	
training:	Epoch: [92][49/233]	Loss 0.0012 (0.0050)	
training:	Epoch: [92][50/233]	Loss 0.0013 (0.0049)	
training:	Epoch: [92][51/233]	Loss 0.0011 (0.0049)	
training:	Epoch: [92][52/233]	Loss 0.0012 (0.0048)	
training:	Epoch: [92][53/233]	Loss 0.0010 (0.0047)	
training:	Epoch: [92][54/233]	Loss 0.0077 (0.0048)	
training:	Epoch: [92][55/233]	Loss 0.0019 (0.0047)	
training:	Epoch: [92][56/233]	Loss 0.0061 (0.0047)	
training:	Epoch: [92][57/233]	Loss 0.0016 (0.0047)	
training:	Epoch: [92][58/233]	Loss 0.0015 (0.0046)	
training:	Epoch: [92][59/233]	Loss 0.0021 (0.0046)	
training:	Epoch: [92][60/233]	Loss 0.0092 (0.0047)	
training:	Epoch: [92][61/233]	Loss 0.0015 (0.0046)	
training:	Epoch: [92][62/233]	Loss 0.0024 (0.0046)	
training:	Epoch: [92][63/233]	Loss 0.0015 (0.0045)	
training:	Epoch: [92][64/233]	Loss 0.0017 (0.0045)	
training:	Epoch: [92][65/233]	Loss 0.0010 (0.0044)	
training:	Epoch: [92][66/233]	Loss 0.0022 (0.0044)	
training:	Epoch: [92][67/233]	Loss 0.0011 (0.0044)	
training:	Epoch: [92][68/233]	Loss 0.0013 (0.0043)	
training:	Epoch: [92][69/233]	Loss 0.0010 (0.0043)	
training:	Epoch: [92][70/233]	Loss 0.0011 (0.0042)	
training:	Epoch: [92][71/233]	Loss 0.0011 (0.0042)	
training:	Epoch: [92][72/233]	Loss 0.0011 (0.0041)	
training:	Epoch: [92][73/233]	Loss 0.0011 (0.0041)	
training:	Epoch: [92][74/233]	Loss 0.0032 (0.0041)	
training:	Epoch: [92][75/233]	Loss 0.0017 (0.0040)	
training:	Epoch: [92][76/233]	Loss 0.0010 (0.0040)	
training:	Epoch: [92][77/233]	Loss 0.0009 (0.0040)	
training:	Epoch: [92][78/233]	Loss 0.0009 (0.0039)	
training:	Epoch: [92][79/233]	Loss 0.0010 (0.0039)	
training:	Epoch: [92][80/233]	Loss 0.0021 (0.0039)	
training:	Epoch: [92][81/233]	Loss 0.0010 (0.0038)	
training:	Epoch: [92][82/233]	Loss 0.0012 (0.0038)	
training:	Epoch: [92][83/233]	Loss 0.0014 (0.0038)	
training:	Epoch: [92][84/233]	Loss 0.0478 (0.0043)	
training:	Epoch: [92][85/233]	Loss 0.0013 (0.0043)	
training:	Epoch: [92][86/233]	Loss 0.1099 (0.0055)	
training:	Epoch: [92][87/233]	Loss 0.0014 (0.0054)	
training:	Epoch: [92][88/233]	Loss 0.0010 (0.0054)	
training:	Epoch: [92][89/233]	Loss 0.0024 (0.0054)	
training:	Epoch: [92][90/233]	Loss 0.0167 (0.0055)	
training:	Epoch: [92][91/233]	Loss 0.0033 (0.0055)	
training:	Epoch: [92][92/233]	Loss 0.0216 (0.0056)	
training:	Epoch: [92][93/233]	Loss 0.0014 (0.0056)	
training:	Epoch: [92][94/233]	Loss 0.0009 (0.0055)	
training:	Epoch: [92][95/233]	Loss 0.0013 (0.0055)	
training:	Epoch: [92][96/233]	Loss 0.0011 (0.0054)	
training:	Epoch: [92][97/233]	Loss 0.0020 (0.0054)	
training:	Epoch: [92][98/233]	Loss 0.0018 (0.0054)	
training:	Epoch: [92][99/233]	Loss 0.0010 (0.0053)	
training:	Epoch: [92][100/233]	Loss 0.0018 (0.0053)	
training:	Epoch: [92][101/233]	Loss 0.0013 (0.0053)	
training:	Epoch: [92][102/233]	Loss 0.0009 (0.0052)	
training:	Epoch: [92][103/233]	Loss 0.0073 (0.0052)	
training:	Epoch: [92][104/233]	Loss 0.0011 (0.0052)	
training:	Epoch: [92][105/233]	Loss 0.0015 (0.0052)	
training:	Epoch: [92][106/233]	Loss 0.2139 (0.0071)	
training:	Epoch: [92][107/233]	Loss 0.1061 (0.0081)	
training:	Epoch: [92][108/233]	Loss 0.0036 (0.0080)	
training:	Epoch: [92][109/233]	Loss 0.0010 (0.0079)	
training:	Epoch: [92][110/233]	Loss 0.0010 (0.0079)	
training:	Epoch: [92][111/233]	Loss 0.0012 (0.0078)	
training:	Epoch: [92][112/233]	Loss 0.0013 (0.0078)	
training:	Epoch: [92][113/233]	Loss 0.0011 (0.0077)	
training:	Epoch: [92][114/233]	Loss 0.0016 (0.0077)	
training:	Epoch: [92][115/233]	Loss 0.0017 (0.0076)	
training:	Epoch: [92][116/233]	Loss 0.0040 (0.0076)	
training:	Epoch: [92][117/233]	Loss 0.0010 (0.0075)	
training:	Epoch: [92][118/233]	Loss 0.0011 (0.0075)	
training:	Epoch: [92][119/233]	Loss 0.0783 (0.0081)	
training:	Epoch: [92][120/233]	Loss 0.0039 (0.0080)	
training:	Epoch: [92][121/233]	Loss 0.0010 (0.0080)	
training:	Epoch: [92][122/233]	Loss 0.0013 (0.0079)	
training:	Epoch: [92][123/233]	Loss 0.0036 (0.0079)	
training:	Epoch: [92][124/233]	Loss 0.0051 (0.0079)	
training:	Epoch: [92][125/233]	Loss 0.0011 (0.0078)	
training:	Epoch: [92][126/233]	Loss 0.0058 (0.0078)	
training:	Epoch: [92][127/233]	Loss 0.0020 (0.0077)	
training:	Epoch: [92][128/233]	Loss 0.0011 (0.0077)	
training:	Epoch: [92][129/233]	Loss 0.0010 (0.0076)	
training:	Epoch: [92][130/233]	Loss 0.0016 (0.0076)	
training:	Epoch: [92][131/233]	Loss 0.0027 (0.0075)	
training:	Epoch: [92][132/233]	Loss 0.0014 (0.0075)	
training:	Epoch: [92][133/233]	Loss 0.0012 (0.0075)	
training:	Epoch: [92][134/233]	Loss 0.0047 (0.0074)	
training:	Epoch: [92][135/233]	Loss 0.0055 (0.0074)	
training:	Epoch: [92][136/233]	Loss 0.0010 (0.0074)	
training:	Epoch: [92][137/233]	Loss 0.0016 (0.0073)	
training:	Epoch: [92][138/233]	Loss 0.0012 (0.0073)	
training:	Epoch: [92][139/233]	Loss 0.0030 (0.0073)	
training:	Epoch: [92][140/233]	Loss 0.0022 (0.0072)	
training:	Epoch: [92][141/233]	Loss 0.0078 (0.0072)	
training:	Epoch: [92][142/233]	Loss 0.0011 (0.0072)	
training:	Epoch: [92][143/233]	Loss 0.0017 (0.0071)	
training:	Epoch: [92][144/233]	Loss 0.0045 (0.0071)	
training:	Epoch: [92][145/233]	Loss 0.0013 (0.0071)	
training:	Epoch: [92][146/233]	Loss 0.0010 (0.0070)	
training:	Epoch: [92][147/233]	Loss 0.0014 (0.0070)	
training:	Epoch: [92][148/233]	Loss 0.0060 (0.0070)	
training:	Epoch: [92][149/233]	Loss 0.0015 (0.0070)	
training:	Epoch: [92][150/233]	Loss 0.0013 (0.0069)	
training:	Epoch: [92][151/233]	Loss 0.0037 (0.0069)	
training:	Epoch: [92][152/233]	Loss 0.0013 (0.0069)	
training:	Epoch: [92][153/233]	Loss 0.0011 (0.0068)	
training:	Epoch: [92][154/233]	Loss 0.0012 (0.0068)	
training:	Epoch: [92][155/233]	Loss 0.0012 (0.0068)	
training:	Epoch: [92][156/233]	Loss 0.0010 (0.0067)	
training:	Epoch: [92][157/233]	Loss 0.0010 (0.0067)	
training:	Epoch: [92][158/233]	Loss 0.0027 (0.0067)	
training:	Epoch: [92][159/233]	Loss 0.0011 (0.0066)	
training:	Epoch: [92][160/233]	Loss 0.0011 (0.0066)	
training:	Epoch: [92][161/233]	Loss 0.0013 (0.0066)	
training:	Epoch: [92][162/233]	Loss 0.0016 (0.0065)	
training:	Epoch: [92][163/233]	Loss 0.0012 (0.0065)	
training:	Epoch: [92][164/233]	Loss 0.0010 (0.0065)	
training:	Epoch: [92][165/233]	Loss 0.0010 (0.0064)	
training:	Epoch: [92][166/233]	Loss 0.0060 (0.0064)	
training:	Epoch: [92][167/233]	Loss 0.0545 (0.0067)	
training:	Epoch: [92][168/233]	Loss 0.0014 (0.0067)	
training:	Epoch: [92][169/233]	Loss 0.0016 (0.0066)	
training:	Epoch: [92][170/233]	Loss 0.0018 (0.0066)	
training:	Epoch: [92][171/233]	Loss 0.0012 (0.0066)	
training:	Epoch: [92][172/233]	Loss 0.0011 (0.0066)	
training:	Epoch: [92][173/233]	Loss 0.0011 (0.0065)	
training:	Epoch: [92][174/233]	Loss 0.1530 (0.0074)	
training:	Epoch: [92][175/233]	Loss 0.0009 (0.0073)	
training:	Epoch: [92][176/233]	Loss 0.0039 (0.0073)	
training:	Epoch: [92][177/233]	Loss 0.0010 (0.0073)	
training:	Epoch: [92][178/233]	Loss 0.0012 (0.0072)	
training:	Epoch: [92][179/233]	Loss 0.0012 (0.0072)	
training:	Epoch: [92][180/233]	Loss 0.0011 (0.0072)	
training:	Epoch: [92][181/233]	Loss 0.0010 (0.0071)	
training:	Epoch: [92][182/233]	Loss 0.0017 (0.0071)	
training:	Epoch: [92][183/233]	Loss 0.0010 (0.0071)	
training:	Epoch: [92][184/233]	Loss 0.0011 (0.0070)	
training:	Epoch: [92][185/233]	Loss 0.1963 (0.0081)	
training:	Epoch: [92][186/233]	Loss 0.0013 (0.0080)	
training:	Epoch: [92][187/233]	Loss 0.0071 (0.0080)	
training:	Epoch: [92][188/233]	Loss 0.0012 (0.0080)	
training:	Epoch: [92][189/233]	Loss 0.0017 (0.0080)	
training:	Epoch: [92][190/233]	Loss 0.0012 (0.0079)	
training:	Epoch: [92][191/233]	Loss 0.0011 (0.0079)	
training:	Epoch: [92][192/233]	Loss 0.0338 (0.0080)	
training:	Epoch: [92][193/233]	Loss 0.0019 (0.0080)	
training:	Epoch: [92][194/233]	Loss 0.0014 (0.0080)	
training:	Epoch: [92][195/233]	Loss 0.0081 (0.0080)	
training:	Epoch: [92][196/233]	Loss 0.0013 (0.0079)	
training:	Epoch: [92][197/233]	Loss 0.0012 (0.0079)	
training:	Epoch: [92][198/233]	Loss 0.0018 (0.0079)	
training:	Epoch: [92][199/233]	Loss 0.0031 (0.0078)	
training:	Epoch: [92][200/233]	Loss 0.0016 (0.0078)	
training:	Epoch: [92][201/233]	Loss 0.0487 (0.0080)	
training:	Epoch: [92][202/233]	Loss 0.0029 (0.0080)	
training:	Epoch: [92][203/233]	Loss 0.0058 (0.0080)	
training:	Epoch: [92][204/233]	Loss 0.0013 (0.0079)	
training:	Epoch: [92][205/233]	Loss 0.0021 (0.0079)	
training:	Epoch: [92][206/233]	Loss 0.0012 (0.0079)	
training:	Epoch: [92][207/233]	Loss 0.0010 (0.0078)	
training:	Epoch: [92][208/233]	Loss 0.0010 (0.0078)	
training:	Epoch: [92][209/233]	Loss 0.0093 (0.0078)	
training:	Epoch: [92][210/233]	Loss 0.0669 (0.0081)	
training:	Epoch: [92][211/233]	Loss 0.0021 (0.0081)	
training:	Epoch: [92][212/233]	Loss 0.0011 (0.0080)	
training:	Epoch: [92][213/233]	Loss 0.0075 (0.0080)	
training:	Epoch: [92][214/233]	Loss 0.2243 (0.0090)	
training:	Epoch: [92][215/233]	Loss 0.0080 (0.0090)	
training:	Epoch: [92][216/233]	Loss 0.0018 (0.0090)	
training:	Epoch: [92][217/233]	Loss 0.0014 (0.0090)	
training:	Epoch: [92][218/233]	Loss 0.0021 (0.0089)	
training:	Epoch: [92][219/233]	Loss 0.0011 (0.0089)	
training:	Epoch: [92][220/233]	Loss 0.0011 (0.0089)	
training:	Epoch: [92][221/233]	Loss 0.0014 (0.0088)	
training:	Epoch: [92][222/233]	Loss 0.0011 (0.0088)	
training:	Epoch: [92][223/233]	Loss 0.0018 (0.0088)	
training:	Epoch: [92][224/233]	Loss 0.0010 (0.0087)	
training:	Epoch: [92][225/233]	Loss 0.0013 (0.0087)	
training:	Epoch: [92][226/233]	Loss 0.0009 (0.0087)	
training:	Epoch: [92][227/233]	Loss 0.0010 (0.0086)	
training:	Epoch: [92][228/233]	Loss 0.0018 (0.0086)	
training:	Epoch: [92][229/233]	Loss 0.0011 (0.0086)	
training:	Epoch: [92][230/233]	Loss 0.0012 (0.0085)	
training:	Epoch: [92][231/233]	Loss 0.0104 (0.0085)	
training:	Epoch: [92][232/233]	Loss 0.0025 (0.0085)	
training:	Epoch: [92][233/233]	Loss 0.0014 (0.0085)	
Training:	 Loss: 0.0085

Training:	 ACC: 0.9996 0.9996 0.9995 0.9997
Validation:	 ACC: 0.7904 0.7892 0.7651 0.8157
Validation:	 Best_BACC: 0.8162 0.8154 0.7998 0.8326
Validation:	 Loss: 1.1069
Pretraining:	Epoch 93/200
----------
training:	Epoch: [93][1/233]	Loss 0.0012 (0.0012)	
training:	Epoch: [93][2/233]	Loss 0.0011 (0.0011)	
training:	Epoch: [93][3/233]	Loss 0.0010 (0.0011)	
training:	Epoch: [93][4/233]	Loss 0.0011 (0.0011)	
training:	Epoch: [93][5/233]	Loss 0.0015 (0.0012)	
training:	Epoch: [93][6/233]	Loss 0.0013 (0.0012)	
training:	Epoch: [93][7/233]	Loss 0.0010 (0.0012)	
training:	Epoch: [93][8/233]	Loss 0.0015 (0.0012)	
training:	Epoch: [93][9/233]	Loss 0.0011 (0.0012)	
training:	Epoch: [93][10/233]	Loss 0.0014 (0.0012)	
training:	Epoch: [93][11/233]	Loss 0.0033 (0.0014)	
training:	Epoch: [93][12/233]	Loss 0.0015 (0.0014)	
training:	Epoch: [93][13/233]	Loss 0.0022 (0.0015)	
training:	Epoch: [93][14/233]	Loss 0.0018 (0.0015)	
training:	Epoch: [93][15/233]	Loss 0.0012 (0.0015)	
training:	Epoch: [93][16/233]	Loss 0.0012 (0.0015)	
training:	Epoch: [93][17/233]	Loss 0.0153 (0.0023)	
training:	Epoch: [93][18/233]	Loss 0.2118 (0.0139)	
training:	Epoch: [93][19/233]	Loss 0.0012 (0.0133)	
training:	Epoch: [93][20/233]	Loss 0.0014 (0.0127)	
training:	Epoch: [93][21/233]	Loss 0.0009 (0.0121)	
training:	Epoch: [93][22/233]	Loss 0.0015 (0.0116)	
training:	Epoch: [93][23/233]	Loss 0.0011 (0.0112)	
training:	Epoch: [93][24/233]	Loss 0.0022 (0.0108)	
training:	Epoch: [93][25/233]	Loss 0.0011 (0.0104)	
training:	Epoch: [93][26/233]	Loss 0.0010 (0.0100)	
training:	Epoch: [93][27/233]	Loss 0.0026 (0.0098)	
training:	Epoch: [93][28/233]	Loss 0.0013 (0.0095)	
training:	Epoch: [93][29/233]	Loss 0.2248 (0.0169)	
training:	Epoch: [93][30/233]	Loss 0.0011 (0.0164)	
training:	Epoch: [93][31/233]	Loss 0.0010 (0.0159)	
training:	Epoch: [93][32/233]	Loss 0.0013 (0.0154)	
training:	Epoch: [93][33/233]	Loss 0.0012 (0.0150)	
training:	Epoch: [93][34/233]	Loss 0.0012 (0.0146)	
training:	Epoch: [93][35/233]	Loss 0.0019 (0.0142)	
training:	Epoch: [93][36/233]	Loss 0.0024 (0.0139)	
training:	Epoch: [93][37/233]	Loss 0.0013 (0.0135)	
training:	Epoch: [93][38/233]	Loss 0.0021 (0.0132)	
training:	Epoch: [93][39/233]	Loss 0.0013 (0.0129)	
training:	Epoch: [93][40/233]	Loss 0.0011 (0.0126)	
training:	Epoch: [93][41/233]	Loss 0.0010 (0.0124)	
training:	Epoch: [93][42/233]	Loss 0.0012 (0.0121)	
training:	Epoch: [93][43/233]	Loss 0.0012 (0.0118)	
training:	Epoch: [93][44/233]	Loss 0.0013 (0.0116)	
training:	Epoch: [93][45/233]	Loss 0.0011 (0.0114)	
training:	Epoch: [93][46/233]	Loss 0.0011 (0.0111)	
training:	Epoch: [93][47/233]	Loss 0.0016 (0.0109)	
training:	Epoch: [93][48/233]	Loss 0.0035 (0.0108)	
training:	Epoch: [93][49/233]	Loss 0.0012 (0.0106)	
training:	Epoch: [93][50/233]	Loss 0.0092 (0.0106)	
training:	Epoch: [93][51/233]	Loss 0.0020 (0.0104)	
training:	Epoch: [93][52/233]	Loss 0.0011 (0.0102)	
training:	Epoch: [93][53/233]	Loss 0.0011 (0.0100)	
training:	Epoch: [93][54/233]	Loss 0.0017 (0.0099)	
training:	Epoch: [93][55/233]	Loss 0.0010 (0.0097)	
training:	Epoch: [93][56/233]	Loss 0.0011 (0.0096)	
training:	Epoch: [93][57/233]	Loss 0.0018 (0.0094)	
training:	Epoch: [93][58/233]	Loss 0.0011 (0.0093)	
training:	Epoch: [93][59/233]	Loss 0.0010 (0.0092)	
training:	Epoch: [93][60/233]	Loss 0.0014 (0.0090)	
training:	Epoch: [93][61/233]	Loss 0.0043 (0.0089)	
training:	Epoch: [93][62/233]	Loss 0.0013 (0.0088)	
training:	Epoch: [93][63/233]	Loss 0.0011 (0.0087)	
training:	Epoch: [93][64/233]	Loss 0.0133 (0.0088)	
training:	Epoch: [93][65/233]	Loss 0.0009 (0.0086)	
training:	Epoch: [93][66/233]	Loss 0.0011 (0.0085)	
training:	Epoch: [93][67/233]	Loss 0.0100 (0.0086)	
training:	Epoch: [93][68/233]	Loss 0.0016 (0.0085)	
training:	Epoch: [93][69/233]	Loss 0.0027 (0.0084)	
training:	Epoch: [93][70/233]	Loss 0.0013 (0.0083)	
training:	Epoch: [93][71/233]	Loss 0.0010 (0.0082)	
training:	Epoch: [93][72/233]	Loss 0.0012 (0.0081)	
training:	Epoch: [93][73/233]	Loss 0.0014 (0.0080)	
training:	Epoch: [93][74/233]	Loss 0.0012 (0.0079)	
training:	Epoch: [93][75/233]	Loss 0.0013 (0.0078)	
training:	Epoch: [93][76/233]	Loss 0.0011 (0.0077)	
training:	Epoch: [93][77/233]	Loss 0.0010 (0.0076)	
training:	Epoch: [93][78/233]	Loss 0.0009 (0.0075)	
training:	Epoch: [93][79/233]	Loss 0.0015 (0.0075)	
training:	Epoch: [93][80/233]	Loss 0.0010 (0.0074)	
training:	Epoch: [93][81/233]	Loss 0.0012 (0.0073)	
training:	Epoch: [93][82/233]	Loss 0.0009 (0.0072)	
training:	Epoch: [93][83/233]	Loss 0.0010 (0.0072)	
training:	Epoch: [93][84/233]	Loss 0.0014 (0.0071)	
training:	Epoch: [93][85/233]	Loss 0.0010 (0.0070)	
training:	Epoch: [93][86/233]	Loss 0.0012 (0.0069)	
training:	Epoch: [93][87/233]	Loss 0.0014 (0.0069)	
training:	Epoch: [93][88/233]	Loss 0.0014 (0.0068)	
training:	Epoch: [93][89/233]	Loss 0.0048 (0.0068)	
training:	Epoch: [93][90/233]	Loss 0.0017 (0.0067)	
training:	Epoch: [93][91/233]	Loss 0.0018 (0.0067)	
training:	Epoch: [93][92/233]	Loss 0.0016 (0.0066)	
training:	Epoch: [93][93/233]	Loss 0.0014 (0.0066)	
training:	Epoch: [93][94/233]	Loss 0.0011 (0.0065)	
training:	Epoch: [93][95/233]	Loss 0.0012 (0.0065)	
training:	Epoch: [93][96/233]	Loss 0.0017 (0.0064)	
training:	Epoch: [93][97/233]	Loss 0.0009 (0.0064)	
training:	Epoch: [93][98/233]	Loss 0.0015 (0.0063)	
training:	Epoch: [93][99/233]	Loss 0.0009 (0.0062)	
training:	Epoch: [93][100/233]	Loss 0.0011 (0.0062)	
training:	Epoch: [93][101/233]	Loss 0.0026 (0.0062)	
training:	Epoch: [93][102/233]	Loss 0.0012 (0.0061)	
training:	Epoch: [93][103/233]	Loss 0.0011 (0.0061)	
training:	Epoch: [93][104/233]	Loss 0.0021 (0.0060)	
training:	Epoch: [93][105/233]	Loss 0.1096 (0.0070)	
training:	Epoch: [93][106/233]	Loss 0.0011 (0.0070)	
training:	Epoch: [93][107/233]	Loss 0.0012 (0.0069)	
training:	Epoch: [93][108/233]	Loss 0.0016 (0.0069)	
training:	Epoch: [93][109/233]	Loss 0.0012 (0.0068)	
training:	Epoch: [93][110/233]	Loss 0.0168 (0.0069)	
training:	Epoch: [93][111/233]	Loss 0.0015 (0.0068)	
training:	Epoch: [93][112/233]	Loss 0.0013 (0.0068)	
training:	Epoch: [93][113/233]	Loss 0.0011 (0.0067)	
training:	Epoch: [93][114/233]	Loss 0.0013 (0.0067)	
training:	Epoch: [93][115/233]	Loss 0.0013 (0.0066)	
training:	Epoch: [93][116/233]	Loss 0.0016 (0.0066)	
training:	Epoch: [93][117/233]	Loss 0.0013 (0.0066)	
training:	Epoch: [93][118/233]	Loss 0.0018 (0.0065)	
training:	Epoch: [93][119/233]	Loss 0.0013 (0.0065)	
training:	Epoch: [93][120/233]	Loss 0.0010 (0.0064)	
training:	Epoch: [93][121/233]	Loss 0.0010 (0.0064)	
training:	Epoch: [93][122/233]	Loss 0.0025 (0.0064)	
training:	Epoch: [93][123/233]	Loss 0.0010 (0.0063)	
training:	Epoch: [93][124/233]	Loss 0.0012 (0.0063)	
training:	Epoch: [93][125/233]	Loss 0.1116 (0.0071)	
training:	Epoch: [93][126/233]	Loss 0.0011 (0.0071)	
training:	Epoch: [93][127/233]	Loss 0.0015 (0.0070)	
training:	Epoch: [93][128/233]	Loss 0.0018 (0.0070)	
training:	Epoch: [93][129/233]	Loss 0.0013 (0.0069)	
training:	Epoch: [93][130/233]	Loss 0.0022 (0.0069)	
training:	Epoch: [93][131/233]	Loss 0.0010 (0.0069)	
training:	Epoch: [93][132/233]	Loss 0.0014 (0.0068)	
training:	Epoch: [93][133/233]	Loss 0.0015 (0.0068)	
training:	Epoch: [93][134/233]	Loss 0.0011 (0.0067)	
training:	Epoch: [93][135/233]	Loss 0.0011 (0.0067)	
training:	Epoch: [93][136/233]	Loss 0.0043 (0.0067)	
training:	Epoch: [93][137/233]	Loss 0.0014 (0.0066)	
training:	Epoch: [93][138/233]	Loss 0.0012 (0.0066)	
training:	Epoch: [93][139/233]	Loss 0.0045 (0.0066)	
training:	Epoch: [93][140/233]	Loss 0.0013 (0.0065)	
training:	Epoch: [93][141/233]	Loss 0.0025 (0.0065)	
training:	Epoch: [93][142/233]	Loss 0.0013 (0.0065)	
training:	Epoch: [93][143/233]	Loss 0.0016 (0.0064)	
training:	Epoch: [93][144/233]	Loss 0.0024 (0.0064)	
training:	Epoch: [93][145/233]	Loss 0.0010 (0.0064)	
training:	Epoch: [93][146/233]	Loss 0.0013 (0.0063)	
training:	Epoch: [93][147/233]	Loss 0.0011 (0.0063)	
training:	Epoch: [93][148/233]	Loss 0.0012 (0.0063)	
training:	Epoch: [93][149/233]	Loss 0.0013 (0.0062)	
training:	Epoch: [93][150/233]	Loss 0.0117 (0.0063)	
training:	Epoch: [93][151/233]	Loss 0.0010 (0.0062)	
training:	Epoch: [93][152/233]	Loss 0.0073 (0.0062)	
training:	Epoch: [93][153/233]	Loss 0.0012 (0.0062)	
training:	Epoch: [93][154/233]	Loss 0.0012 (0.0062)	
training:	Epoch: [93][155/233]	Loss 0.0009 (0.0061)	
training:	Epoch: [93][156/233]	Loss 0.0037 (0.0061)	
training:	Epoch: [93][157/233]	Loss 0.0015 (0.0061)	
training:	Epoch: [93][158/233]	Loss 0.0022 (0.0061)	
training:	Epoch: [93][159/233]	Loss 0.0014 (0.0060)	
training:	Epoch: [93][160/233]	Loss 0.0014 (0.0060)	
training:	Epoch: [93][161/233]	Loss 0.0015 (0.0060)	
training:	Epoch: [93][162/233]	Loss 0.0010 (0.0060)	
training:	Epoch: [93][163/233]	Loss 0.0011 (0.0059)	
training:	Epoch: [93][164/233]	Loss 0.0011 (0.0059)	
training:	Epoch: [93][165/233]	Loss 0.0044 (0.0059)	
training:	Epoch: [93][166/233]	Loss 0.0014 (0.0059)	
training:	Epoch: [93][167/233]	Loss 0.0010 (0.0058)	
training:	Epoch: [93][168/233]	Loss 0.0026 (0.0058)	
training:	Epoch: [93][169/233]	Loss 0.0068 (0.0058)	
training:	Epoch: [93][170/233]	Loss 0.0011 (0.0058)	
training:	Epoch: [93][171/233]	Loss 0.0010 (0.0058)	
training:	Epoch: [93][172/233]	Loss 0.0010 (0.0057)	
training:	Epoch: [93][173/233]	Loss 0.0025 (0.0057)	
training:	Epoch: [93][174/233]	Loss 0.0010 (0.0057)	
training:	Epoch: [93][175/233]	Loss 0.0009 (0.0057)	
training:	Epoch: [93][176/233]	Loss 0.0837 (0.0061)	
training:	Epoch: [93][177/233]	Loss 0.0010 (0.0061)	
training:	Epoch: [93][178/233]	Loss 0.0011 (0.0060)	
training:	Epoch: [93][179/233]	Loss 0.0022 (0.0060)	
training:	Epoch: [93][180/233]	Loss 0.0010 (0.0060)	
training:	Epoch: [93][181/233]	Loss 0.0289 (0.0061)	
training:	Epoch: [93][182/233]	Loss 0.0240 (0.0062)	
training:	Epoch: [93][183/233]	Loss 0.0024 (0.0062)	
training:	Epoch: [93][184/233]	Loss 0.0009 (0.0062)	
training:	Epoch: [93][185/233]	Loss 0.0010 (0.0061)	
training:	Epoch: [93][186/233]	Loss 0.0011 (0.0061)	
training:	Epoch: [93][187/233]	Loss 0.0011 (0.0061)	
training:	Epoch: [93][188/233]	Loss 0.0010 (0.0061)	
training:	Epoch: [93][189/233]	Loss 0.0011 (0.0060)	
training:	Epoch: [93][190/233]	Loss 0.0015 (0.0060)	
training:	Epoch: [93][191/233]	Loss 0.0012 (0.0060)	
training:	Epoch: [93][192/233]	Loss 0.0014 (0.0060)	
training:	Epoch: [93][193/233]	Loss 0.0011 (0.0059)	
training:	Epoch: [93][194/233]	Loss 0.0010 (0.0059)	
training:	Epoch: [93][195/233]	Loss 0.0012 (0.0059)	
training:	Epoch: [93][196/233]	Loss 0.0020 (0.0059)	
training:	Epoch: [93][197/233]	Loss 0.0010 (0.0058)	
training:	Epoch: [93][198/233]	Loss 0.0022 (0.0058)	
training:	Epoch: [93][199/233]	Loss 0.0011 (0.0058)	
training:	Epoch: [93][200/233]	Loss 0.0022 (0.0058)	
training:	Epoch: [93][201/233]	Loss 0.0029 (0.0058)	
training:	Epoch: [93][202/233]	Loss 0.0010 (0.0057)	
training:	Epoch: [93][203/233]	Loss 0.0013 (0.0057)	
training:	Epoch: [93][204/233]	Loss 0.0009 (0.0057)	
training:	Epoch: [93][205/233]	Loss 0.0018 (0.0057)	
training:	Epoch: [93][206/233]	Loss 0.0021 (0.0057)	
training:	Epoch: [93][207/233]	Loss 0.0012 (0.0056)	
training:	Epoch: [93][208/233]	Loss 0.0009 (0.0056)	
training:	Epoch: [93][209/233]	Loss 0.0011 (0.0056)	
training:	Epoch: [93][210/233]	Loss 0.0012 (0.0056)	
training:	Epoch: [93][211/233]	Loss 0.0009 (0.0056)	
training:	Epoch: [93][212/233]	Loss 0.0011 (0.0055)	
training:	Epoch: [93][213/233]	Loss 0.0010 (0.0055)	
training:	Epoch: [93][214/233]	Loss 0.0013 (0.0055)	
training:	Epoch: [93][215/233]	Loss 0.0012 (0.0055)	
training:	Epoch: [93][216/233]	Loss 0.0010 (0.0055)	
training:	Epoch: [93][217/233]	Loss 0.0009 (0.0054)	
training:	Epoch: [93][218/233]	Loss 0.1994 (0.0063)	
training:	Epoch: [93][219/233]	Loss 0.0012 (0.0063)	
training:	Epoch: [93][220/233]	Loss 0.0023 (0.0063)	
training:	Epoch: [93][221/233]	Loss 0.0024 (0.0063)	
training:	Epoch: [93][222/233]	Loss 0.0014 (0.0062)	
training:	Epoch: [93][223/233]	Loss 0.0009 (0.0062)	
training:	Epoch: [93][224/233]	Loss 0.0010 (0.0062)	
training:	Epoch: [93][225/233]	Loss 0.0009 (0.0062)	
training:	Epoch: [93][226/233]	Loss 0.0015 (0.0062)	
training:	Epoch: [93][227/233]	Loss 0.0010 (0.0061)	
training:	Epoch: [93][228/233]	Loss 0.0065 (0.0061)	
training:	Epoch: [93][229/233]	Loss 0.0020 (0.0061)	
training:	Epoch: [93][230/233]	Loss 0.0010 (0.0061)	
training:	Epoch: [93][231/233]	Loss 0.0009 (0.0061)	
training:	Epoch: [93][232/233]	Loss 0.0010 (0.0060)	
training:	Epoch: [93][233/233]	Loss 0.0054 (0.0060)	
Training:	 Loss: 0.0060

Training:	 ACC: 0.9996 0.9996 0.9995 0.9997
Validation:	 ACC: 0.7906 0.7903 0.7845 0.7966
Validation:	 Best_BACC: 0.8162 0.8154 0.7998 0.8326
Validation:	 Loss: 1.1107
Pretraining:	Epoch 94/200
----------
training:	Epoch: [94][1/233]	Loss 0.0011 (0.0011)	
training:	Epoch: [94][2/233]	Loss 0.0010 (0.0010)	
training:	Epoch: [94][3/233]	Loss 0.0011 (0.0011)	
training:	Epoch: [94][4/233]	Loss 0.0011 (0.0011)	
training:	Epoch: [94][5/233]	Loss 0.0024 (0.0013)	
training:	Epoch: [94][6/233]	Loss 0.0017 (0.0014)	
training:	Epoch: [94][7/233]	Loss 0.0010 (0.0014)	
training:	Epoch: [94][8/233]	Loss 0.0011 (0.0013)	
training:	Epoch: [94][9/233]	Loss 0.0013 (0.0013)	
training:	Epoch: [94][10/233]	Loss 0.0009 (0.0013)	
training:	Epoch: [94][11/233]	Loss 0.0032 (0.0014)	
training:	Epoch: [94][12/233]	Loss 0.0014 (0.0014)	
training:	Epoch: [94][13/233]	Loss 0.0012 (0.0014)	
training:	Epoch: [94][14/233]	Loss 0.0009 (0.0014)	
training:	Epoch: [94][15/233]	Loss 0.0012 (0.0014)	
training:	Epoch: [94][16/233]	Loss 0.0010 (0.0014)	
training:	Epoch: [94][17/233]	Loss 0.0009 (0.0013)	
training:	Epoch: [94][18/233]	Loss 0.0077 (0.0017)	
training:	Epoch: [94][19/233]	Loss 0.0009 (0.0016)	
training:	Epoch: [94][20/233]	Loss 0.0011 (0.0016)	
training:	Epoch: [94][21/233]	Loss 0.0009 (0.0016)	
training:	Epoch: [94][22/233]	Loss 0.0016 (0.0016)	
training:	Epoch: [94][23/233]	Loss 0.0009 (0.0016)	
training:	Epoch: [94][24/233]	Loss 0.0017 (0.0016)	
training:	Epoch: [94][25/233]	Loss 0.0013 (0.0016)	
training:	Epoch: [94][26/233]	Loss 0.0021 (0.0016)	
training:	Epoch: [94][27/233]	Loss 0.0038 (0.0017)	
training:	Epoch: [94][28/233]	Loss 0.0009 (0.0016)	
training:	Epoch: [94][29/233]	Loss 0.0010 (0.0016)	
training:	Epoch: [94][30/233]	Loss 0.0011 (0.0016)	
training:	Epoch: [94][31/233]	Loss 0.0010 (0.0016)	
training:	Epoch: [94][32/233]	Loss 0.0017 (0.0016)	
training:	Epoch: [94][33/233]	Loss 0.0012 (0.0016)	
training:	Epoch: [94][34/233]	Loss 0.0012 (0.0016)	
training:	Epoch: [94][35/233]	Loss 0.0018 (0.0016)	
training:	Epoch: [94][36/233]	Loss 0.0012 (0.0015)	
training:	Epoch: [94][37/233]	Loss 0.0013 (0.0015)	
training:	Epoch: [94][38/233]	Loss 0.0010 (0.0015)	
training:	Epoch: [94][39/233]	Loss 0.0014 (0.0015)	
training:	Epoch: [94][40/233]	Loss 0.0021 (0.0015)	
training:	Epoch: [94][41/233]	Loss 0.0010 (0.0015)	
training:	Epoch: [94][42/233]	Loss 0.0009 (0.0015)	
training:	Epoch: [94][43/233]	Loss 0.0009 (0.0015)	
training:	Epoch: [94][44/233]	Loss 0.0011 (0.0015)	
training:	Epoch: [94][45/233]	Loss 0.0031 (0.0015)	
training:	Epoch: [94][46/233]	Loss 0.0011 (0.0015)	
training:	Epoch: [94][47/233]	Loss 0.0009 (0.0015)	
training:	Epoch: [94][48/233]	Loss 0.0009 (0.0015)	
training:	Epoch: [94][49/233]	Loss 0.0009 (0.0015)	
training:	Epoch: [94][50/233]	Loss 0.0358 (0.0022)	
training:	Epoch: [94][51/233]	Loss 0.0009 (0.0021)	
training:	Epoch: [94][52/233]	Loss 0.0031 (0.0022)	
training:	Epoch: [94][53/233]	Loss 0.0011 (0.0021)	
training:	Epoch: [94][54/233]	Loss 0.0014 (0.0021)	
training:	Epoch: [94][55/233]	Loss 0.0011 (0.0021)	
training:	Epoch: [94][56/233]	Loss 0.0010 (0.0021)	
training:	Epoch: [94][57/233]	Loss 0.0014 (0.0021)	
training:	Epoch: [94][58/233]	Loss 0.0009 (0.0021)	
training:	Epoch: [94][59/233]	Loss 0.0010 (0.0020)	
training:	Epoch: [94][60/233]	Loss 0.2100 (0.0055)	
training:	Epoch: [94][61/233]	Loss 0.0012 (0.0054)	
training:	Epoch: [94][62/233]	Loss 0.0049 (0.0054)	
training:	Epoch: [94][63/233]	Loss 0.0009 (0.0053)	
training:	Epoch: [94][64/233]	Loss 0.0011 (0.0053)	
training:	Epoch: [94][65/233]	Loss 0.0009 (0.0052)	
training:	Epoch: [94][66/233]	Loss 0.0009 (0.0051)	
training:	Epoch: [94][67/233]	Loss 0.0013 (0.0051)	
training:	Epoch: [94][68/233]	Loss 0.0014 (0.0050)	
training:	Epoch: [94][69/233]	Loss 0.0013 (0.0050)	
training:	Epoch: [94][70/233]	Loss 0.0011 (0.0049)	
training:	Epoch: [94][71/233]	Loss 0.0015 (0.0049)	
training:	Epoch: [94][72/233]	Loss 0.0032 (0.0049)	
training:	Epoch: [94][73/233]	Loss 0.0012 (0.0048)	
training:	Epoch: [94][74/233]	Loss 0.0014 (0.0048)	
training:	Epoch: [94][75/233]	Loss 0.2277 (0.0077)	
training:	Epoch: [94][76/233]	Loss 0.0009 (0.0076)	
training:	Epoch: [94][77/233]	Loss 0.0016 (0.0076)	
training:	Epoch: [94][78/233]	Loss 0.0010 (0.0075)	
training:	Epoch: [94][79/233]	Loss 0.0288 (0.0078)	
training:	Epoch: [94][80/233]	Loss 0.0011 (0.0077)	
training:	Epoch: [94][81/233]	Loss 0.0016 (0.0076)	
training:	Epoch: [94][82/233]	Loss 0.0012 (0.0075)	
training:	Epoch: [94][83/233]	Loss 0.0009 (0.0074)	
training:	Epoch: [94][84/233]	Loss 0.0077 (0.0074)	
training:	Epoch: [94][85/233]	Loss 0.0165 (0.0075)	
training:	Epoch: [94][86/233]	Loss 0.0011 (0.0075)	
training:	Epoch: [94][87/233]	Loss 0.0011 (0.0074)	
training:	Epoch: [94][88/233]	Loss 0.0011 (0.0073)	
training:	Epoch: [94][89/233]	Loss 0.0028 (0.0073)	
training:	Epoch: [94][90/233]	Loss 0.0377 (0.0076)	
training:	Epoch: [94][91/233]	Loss 0.1636 (0.0093)	
training:	Epoch: [94][92/233]	Loss 0.0010 (0.0092)	
training:	Epoch: [94][93/233]	Loss 0.0010 (0.0091)	
training:	Epoch: [94][94/233]	Loss 0.0015 (0.0091)	
training:	Epoch: [94][95/233]	Loss 0.0022 (0.0090)	
training:	Epoch: [94][96/233]	Loss 0.0010 (0.0089)	
training:	Epoch: [94][97/233]	Loss 0.0010 (0.0088)	
training:	Epoch: [94][98/233]	Loss 0.0010 (0.0088)	
training:	Epoch: [94][99/233]	Loss 0.0029 (0.0087)	
training:	Epoch: [94][100/233]	Loss 0.0072 (0.0087)	
training:	Epoch: [94][101/233]	Loss 0.0015 (0.0086)	
training:	Epoch: [94][102/233]	Loss 0.0010 (0.0085)	
training:	Epoch: [94][103/233]	Loss 0.0009 (0.0085)	
training:	Epoch: [94][104/233]	Loss 0.0019 (0.0084)	
training:	Epoch: [94][105/233]	Loss 0.0010 (0.0083)	
training:	Epoch: [94][106/233]	Loss 0.0021 (0.0083)	
training:	Epoch: [94][107/233]	Loss 0.0013 (0.0082)	
training:	Epoch: [94][108/233]	Loss 0.0030 (0.0082)	
training:	Epoch: [94][109/233]	Loss 0.0024 (0.0081)	
training:	Epoch: [94][110/233]	Loss 0.0009 (0.0080)	
training:	Epoch: [94][111/233]	Loss 0.0142 (0.0081)	
training:	Epoch: [94][112/233]	Loss 0.0010 (0.0080)	
training:	Epoch: [94][113/233]	Loss 0.0010 (0.0080)	
training:	Epoch: [94][114/233]	Loss 0.0012 (0.0079)	
training:	Epoch: [94][115/233]	Loss 0.0009 (0.0078)	
training:	Epoch: [94][116/233]	Loss 0.0010 (0.0078)	
training:	Epoch: [94][117/233]	Loss 0.1670 (0.0091)	
training:	Epoch: [94][118/233]	Loss 0.0011 (0.0091)	
training:	Epoch: [94][119/233]	Loss 0.0052 (0.0090)	
training:	Epoch: [94][120/233]	Loss 0.0016 (0.0090)	
training:	Epoch: [94][121/233]	Loss 0.0024 (0.0089)	
training:	Epoch: [94][122/233]	Loss 0.0042 (0.0089)	
training:	Epoch: [94][123/233]	Loss 0.0008 (0.0088)	
training:	Epoch: [94][124/233]	Loss 0.0010 (0.0088)	
training:	Epoch: [94][125/233]	Loss 0.0010 (0.0087)	
training:	Epoch: [94][126/233]	Loss 0.0009 (0.0086)	
training:	Epoch: [94][127/233]	Loss 0.0622 (0.0091)	
training:	Epoch: [94][128/233]	Loss 0.0090 (0.0091)	
training:	Epoch: [94][129/233]	Loss 0.0013 (0.0090)	
training:	Epoch: [94][130/233]	Loss 0.0076 (0.0090)	
training:	Epoch: [94][131/233]	Loss 0.0012 (0.0089)	
training:	Epoch: [94][132/233]	Loss 0.0010 (0.0089)	
training:	Epoch: [94][133/233]	Loss 0.0009 (0.0088)	
training:	Epoch: [94][134/233]	Loss 0.0009 (0.0087)	
training:	Epoch: [94][135/233]	Loss 0.0011 (0.0087)	
training:	Epoch: [94][136/233]	Loss 0.0218 (0.0088)	
training:	Epoch: [94][137/233]	Loss 0.0067 (0.0088)	
training:	Epoch: [94][138/233]	Loss 0.0076 (0.0088)	
training:	Epoch: [94][139/233]	Loss 0.2126 (0.0102)	
training:	Epoch: [94][140/233]	Loss 0.0015 (0.0102)	
training:	Epoch: [94][141/233]	Loss 0.0034 (0.0101)	
training:	Epoch: [94][142/233]	Loss 0.0009 (0.0101)	
training:	Epoch: [94][143/233]	Loss 0.0014 (0.0100)	
training:	Epoch: [94][144/233]	Loss 0.0112 (0.0100)	
training:	Epoch: [94][145/233]	Loss 0.0011 (0.0099)	
training:	Epoch: [94][146/233]	Loss 0.0012 (0.0099)	
training:	Epoch: [94][147/233]	Loss 0.0010 (0.0098)	
training:	Epoch: [94][148/233]	Loss 0.0009 (0.0098)	
training:	Epoch: [94][149/233]	Loss 0.0060 (0.0097)	
training:	Epoch: [94][150/233]	Loss 0.0025 (0.0097)	
training:	Epoch: [94][151/233]	Loss 0.0433 (0.0099)	
training:	Epoch: [94][152/233]	Loss 0.0032 (0.0099)	
training:	Epoch: [94][153/233]	Loss 0.0012 (0.0098)	
training:	Epoch: [94][154/233]	Loss 0.0009 (0.0098)	
training:	Epoch: [94][155/233]	Loss 0.0013 (0.0097)	
training:	Epoch: [94][156/233]	Loss 0.0011 (0.0096)	
training:	Epoch: [94][157/233]	Loss 0.0078 (0.0096)	
training:	Epoch: [94][158/233]	Loss 0.0010 (0.0096)	
training:	Epoch: [94][159/233]	Loss 0.0681 (0.0099)	
training:	Epoch: [94][160/233]	Loss 0.0013 (0.0099)	
training:	Epoch: [94][161/233]	Loss 0.0011 (0.0098)	
training:	Epoch: [94][162/233]	Loss 0.0010 (0.0098)	
training:	Epoch: [94][163/233]	Loss 0.0018 (0.0097)	
training:	Epoch: [94][164/233]	Loss 0.0011 (0.0097)	
training:	Epoch: [94][165/233]	Loss 0.0013 (0.0096)	
training:	Epoch: [94][166/233]	Loss 0.0012 (0.0096)	
training:	Epoch: [94][167/233]	Loss 0.0009 (0.0095)	
training:	Epoch: [94][168/233]	Loss 0.0423 (0.0097)	
training:	Epoch: [94][169/233]	Loss 0.0019 (0.0097)	
training:	Epoch: [94][170/233]	Loss 0.0018 (0.0096)	
training:	Epoch: [94][171/233]	Loss 0.0014 (0.0096)	
training:	Epoch: [94][172/233]	Loss 0.0011 (0.0095)	
training:	Epoch: [94][173/233]	Loss 0.0008 (0.0095)	
training:	Epoch: [94][174/233]	Loss 0.0064 (0.0095)	
training:	Epoch: [94][175/233]	Loss 0.0015 (0.0094)	
training:	Epoch: [94][176/233]	Loss 0.0022 (0.0094)	
training:	Epoch: [94][177/233]	Loss 0.0056 (0.0094)	
training:	Epoch: [94][178/233]	Loss 0.0010 (0.0093)	
training:	Epoch: [94][179/233]	Loss 0.0010 (0.0093)	
training:	Epoch: [94][180/233]	Loss 0.0013 (0.0092)	
training:	Epoch: [94][181/233]	Loss 0.0012 (0.0092)	
training:	Epoch: [94][182/233]	Loss 0.0015 (0.0091)	
training:	Epoch: [94][183/233]	Loss 0.0009 (0.0091)	
training:	Epoch: [94][184/233]	Loss 0.0011 (0.0090)	
training:	Epoch: [94][185/233]	Loss 0.0060 (0.0090)	
training:	Epoch: [94][186/233]	Loss 0.0010 (0.0090)	
training:	Epoch: [94][187/233]	Loss 0.0012 (0.0089)	
training:	Epoch: [94][188/233]	Loss 0.0013 (0.0089)	
training:	Epoch: [94][189/233]	Loss 0.0011 (0.0089)	
training:	Epoch: [94][190/233]	Loss 0.0011 (0.0088)	
training:	Epoch: [94][191/233]	Loss 0.0011 (0.0088)	
training:	Epoch: [94][192/233]	Loss 0.0009 (0.0087)	
training:	Epoch: [94][193/233]	Loss 0.0010 (0.0087)	
training:	Epoch: [94][194/233]	Loss 0.0012 (0.0087)	
training:	Epoch: [94][195/233]	Loss 0.0010 (0.0086)	
training:	Epoch: [94][196/233]	Loss 0.0046 (0.0086)	
training:	Epoch: [94][197/233]	Loss 0.0010 (0.0086)	
training:	Epoch: [94][198/233]	Loss 0.0420 (0.0087)	
training:	Epoch: [94][199/233]	Loss 0.0013 (0.0087)	
training:	Epoch: [94][200/233]	Loss 0.0014 (0.0087)	
training:	Epoch: [94][201/233]	Loss 0.0047 (0.0086)	
training:	Epoch: [94][202/233]	Loss 0.0011 (0.0086)	
training:	Epoch: [94][203/233]	Loss 0.0010 (0.0086)	
training:	Epoch: [94][204/233]	Loss 0.0009 (0.0085)	
training:	Epoch: [94][205/233]	Loss 0.0013 (0.0085)	
training:	Epoch: [94][206/233]	Loss 0.0010 (0.0085)	
training:	Epoch: [94][207/233]	Loss 0.0010 (0.0084)	
training:	Epoch: [94][208/233]	Loss 0.0534 (0.0086)	
training:	Epoch: [94][209/233]	Loss 0.0022 (0.0086)	
training:	Epoch: [94][210/233]	Loss 0.0014 (0.0086)	
training:	Epoch: [94][211/233]	Loss 0.0011 (0.0085)	
training:	Epoch: [94][212/233]	Loss 0.0057 (0.0085)	
training:	Epoch: [94][213/233]	Loss 0.0015 (0.0085)	
training:	Epoch: [94][214/233]	Loss 0.0013 (0.0085)	
training:	Epoch: [94][215/233]	Loss 0.0011 (0.0084)	
training:	Epoch: [94][216/233]	Loss 0.0009 (0.0084)	
training:	Epoch: [94][217/233]	Loss 0.0015 (0.0084)	
training:	Epoch: [94][218/233]	Loss 0.0012 (0.0083)	
training:	Epoch: [94][219/233]	Loss 0.0010 (0.0083)	
training:	Epoch: [94][220/233]	Loss 0.0010 (0.0083)	
training:	Epoch: [94][221/233]	Loss 0.0020 (0.0082)	
training:	Epoch: [94][222/233]	Loss 0.0009 (0.0082)	
training:	Epoch: [94][223/233]	Loss 0.0009 (0.0082)	
training:	Epoch: [94][224/233]	Loss 0.0009 (0.0081)	
training:	Epoch: [94][225/233]	Loss 0.0011 (0.0081)	
training:	Epoch: [94][226/233]	Loss 0.0020 (0.0081)	
training:	Epoch: [94][227/233]	Loss 0.0010 (0.0080)	
training:	Epoch: [94][228/233]	Loss 0.0009 (0.0080)	
training:	Epoch: [94][229/233]	Loss 0.0013 (0.0080)	
training:	Epoch: [94][230/233]	Loss 0.0012 (0.0079)	
training:	Epoch: [94][231/233]	Loss 0.0011 (0.0079)	
training:	Epoch: [94][232/233]	Loss 0.0011 (0.0079)	
training:	Epoch: [94][233/233]	Loss 0.0953 (0.0083)	
Training:	 Loss: 0.0082

Training:	 ACC: 0.9996 0.9996 0.9995 0.9997
Validation:	 ACC: 0.7933 0.7919 0.7630 0.8236
Validation:	 Best_BACC: 0.8162 0.8154 0.7998 0.8326
Validation:	 Loss: 1.1392
Pretraining:	Epoch 95/200
----------
training:	Epoch: [95][1/233]	Loss 0.0013 (0.0013)	
training:	Epoch: [95][2/233]	Loss 0.0010 (0.0012)	
training:	Epoch: [95][3/233]	Loss 0.0011 (0.0011)	
training:	Epoch: [95][4/233]	Loss 0.0011 (0.0011)	
training:	Epoch: [95][5/233]	Loss 0.0018 (0.0013)	
training:	Epoch: [95][6/233]	Loss 0.0013 (0.0013)	
training:	Epoch: [95][7/233]	Loss 0.0018 (0.0013)	
training:	Epoch: [95][8/233]	Loss 0.0015 (0.0014)	
training:	Epoch: [95][9/233]	Loss 0.0010 (0.0013)	
training:	Epoch: [95][10/233]	Loss 0.0009 (0.0013)	
training:	Epoch: [95][11/233]	Loss 0.0019 (0.0013)	
training:	Epoch: [95][12/233]	Loss 0.0012 (0.0013)	
training:	Epoch: [95][13/233]	Loss 0.0034 (0.0015)	
training:	Epoch: [95][14/233]	Loss 0.0016 (0.0015)	
training:	Epoch: [95][15/233]	Loss 0.0010 (0.0015)	
training:	Epoch: [95][16/233]	Loss 0.0035 (0.0016)	
training:	Epoch: [95][17/233]	Loss 0.0009 (0.0016)	
training:	Epoch: [95][18/233]	Loss 0.0221 (0.0027)	
training:	Epoch: [95][19/233]	Loss 0.0018 (0.0026)	
training:	Epoch: [95][20/233]	Loss 0.0013 (0.0026)	
training:	Epoch: [95][21/233]	Loss 0.0009 (0.0025)	
training:	Epoch: [95][22/233]	Loss 0.0011 (0.0024)	
training:	Epoch: [95][23/233]	Loss 0.0011 (0.0024)	
training:	Epoch: [95][24/233]	Loss 0.0010 (0.0023)	
training:	Epoch: [95][25/233]	Loss 0.0030 (0.0024)	
training:	Epoch: [95][26/233]	Loss 0.0010 (0.0023)	
training:	Epoch: [95][27/233]	Loss 0.0011 (0.0023)	
training:	Epoch: [95][28/233]	Loss 0.0010 (0.0022)	
training:	Epoch: [95][29/233]	Loss 0.0013 (0.0022)	
training:	Epoch: [95][30/233]	Loss 0.0010 (0.0021)	
training:	Epoch: [95][31/233]	Loss 0.0012 (0.0021)	
training:	Epoch: [95][32/233]	Loss 0.0009 (0.0021)	
training:	Epoch: [95][33/233]	Loss 0.0012 (0.0020)	
training:	Epoch: [95][34/233]	Loss 0.0030 (0.0021)	
training:	Epoch: [95][35/233]	Loss 0.0013 (0.0021)	
training:	Epoch: [95][36/233]	Loss 0.0010 (0.0020)	
training:	Epoch: [95][37/233]	Loss 0.0018 (0.0020)	
training:	Epoch: [95][38/233]	Loss 0.0014 (0.0020)	
training:	Epoch: [95][39/233]	Loss 0.0012 (0.0020)	
training:	Epoch: [95][40/233]	Loss 0.0037 (0.0020)	
training:	Epoch: [95][41/233]	Loss 0.0032 (0.0020)	
training:	Epoch: [95][42/233]	Loss 0.0011 (0.0020)	
training:	Epoch: [95][43/233]	Loss 0.0009 (0.0020)	
training:	Epoch: [95][44/233]	Loss 0.0012 (0.0020)	
training:	Epoch: [95][45/233]	Loss 0.0009 (0.0020)	
training:	Epoch: [95][46/233]	Loss 0.0009 (0.0019)	
training:	Epoch: [95][47/233]	Loss 0.0071 (0.0020)	
training:	Epoch: [95][48/233]	Loss 0.0011 (0.0020)	
training:	Epoch: [95][49/233]	Loss 0.0010 (0.0020)	
training:	Epoch: [95][50/233]	Loss 0.0011 (0.0020)	
training:	Epoch: [95][51/233]	Loss 0.0015 (0.0020)	
training:	Epoch: [95][52/233]	Loss 0.0013 (0.0020)	
training:	Epoch: [95][53/233]	Loss 0.0009 (0.0019)	
training:	Epoch: [95][54/233]	Loss 0.0010 (0.0019)	
training:	Epoch: [95][55/233]	Loss 0.0011 (0.0019)	
training:	Epoch: [95][56/233]	Loss 0.0009 (0.0019)	
training:	Epoch: [95][57/233]	Loss 0.0011 (0.0019)	
training:	Epoch: [95][58/233]	Loss 0.0023 (0.0019)	
training:	Epoch: [95][59/233]	Loss 0.0008 (0.0019)	
training:	Epoch: [95][60/233]	Loss 0.0008 (0.0019)	
training:	Epoch: [95][61/233]	Loss 0.0019 (0.0019)	
training:	Epoch: [95][62/233]	Loss 0.0145 (0.0021)	
training:	Epoch: [95][63/233]	Loss 0.0009 (0.0020)	
training:	Epoch: [95][64/233]	Loss 0.0011 (0.0020)	
training:	Epoch: [95][65/233]	Loss 0.0018 (0.0020)	
training:	Epoch: [95][66/233]	Loss 0.0010 (0.0020)	
training:	Epoch: [95][67/233]	Loss 0.0010 (0.0020)	
training:	Epoch: [95][68/233]	Loss 0.0011 (0.0020)	
training:	Epoch: [95][69/233]	Loss 0.0010 (0.0020)	
training:	Epoch: [95][70/233]	Loss 0.0013 (0.0020)	
training:	Epoch: [95][71/233]	Loss 0.2104 (0.0049)	
training:	Epoch: [95][72/233]	Loss 0.0009 (0.0048)	
training:	Epoch: [95][73/233]	Loss 0.0178 (0.0050)	
training:	Epoch: [95][74/233]	Loss 0.0009 (0.0050)	
training:	Epoch: [95][75/233]	Loss 0.0010 (0.0049)	
training:	Epoch: [95][76/233]	Loss 0.0012 (0.0049)	
training:	Epoch: [95][77/233]	Loss 0.0009 (0.0048)	
training:	Epoch: [95][78/233]	Loss 0.0010 (0.0048)	
training:	Epoch: [95][79/233]	Loss 0.0036 (0.0047)	
training:	Epoch: [95][80/233]	Loss 0.0018 (0.0047)	
training:	Epoch: [95][81/233]	Loss 0.0009 (0.0047)	
training:	Epoch: [95][82/233]	Loss 0.0009 (0.0046)	
training:	Epoch: [95][83/233]	Loss 0.0011 (0.0046)	
training:	Epoch: [95][84/233]	Loss 0.0012 (0.0045)	
training:	Epoch: [95][85/233]	Loss 0.0010 (0.0045)	
training:	Epoch: [95][86/233]	Loss 0.0012 (0.0044)	
training:	Epoch: [95][87/233]	Loss 0.0059 (0.0045)	
training:	Epoch: [95][88/233]	Loss 0.0011 (0.0044)	
training:	Epoch: [95][89/233]	Loss 0.0205 (0.0046)	
training:	Epoch: [95][90/233]	Loss 0.0040 (0.0046)	
training:	Epoch: [95][91/233]	Loss 0.0010 (0.0046)	
training:	Epoch: [95][92/233]	Loss 0.0010 (0.0045)	
training:	Epoch: [95][93/233]	Loss 0.0010 (0.0045)	
training:	Epoch: [95][94/233]	Loss 0.0011 (0.0044)	
training:	Epoch: [95][95/233]	Loss 0.0010 (0.0044)	
training:	Epoch: [95][96/233]	Loss 0.0013 (0.0044)	
training:	Epoch: [95][97/233]	Loss 0.0010 (0.0043)	
training:	Epoch: [95][98/233]	Loss 0.0028 (0.0043)	
training:	Epoch: [95][99/233]	Loss 0.0009 (0.0043)	
training:	Epoch: [95][100/233]	Loss 0.0134 (0.0044)	
training:	Epoch: [95][101/233]	Loss 0.0013 (0.0044)	
training:	Epoch: [95][102/233]	Loss 0.0050 (0.0044)	
training:	Epoch: [95][103/233]	Loss 0.0008 (0.0043)	
training:	Epoch: [95][104/233]	Loss 0.0009 (0.0043)	
training:	Epoch: [95][105/233]	Loss 0.0010 (0.0043)	
training:	Epoch: [95][106/233]	Loss 0.0028 (0.0042)	
training:	Epoch: [95][107/233]	Loss 0.0010 (0.0042)	
training:	Epoch: [95][108/233]	Loss 0.0009 (0.0042)	
training:	Epoch: [95][109/233]	Loss 0.0010 (0.0042)	
training:	Epoch: [95][110/233]	Loss 0.0027 (0.0041)	
training:	Epoch: [95][111/233]	Loss 0.0009 (0.0041)	
training:	Epoch: [95][112/233]	Loss 0.0010 (0.0041)	
training:	Epoch: [95][113/233]	Loss 0.0082 (0.0041)	
training:	Epoch: [95][114/233]	Loss 0.0009 (0.0041)	
training:	Epoch: [95][115/233]	Loss 0.0012 (0.0041)	
training:	Epoch: [95][116/233]	Loss 0.0019 (0.0040)	
training:	Epoch: [95][117/233]	Loss 0.0032 (0.0040)	
training:	Epoch: [95][118/233]	Loss 0.0011 (0.0040)	
training:	Epoch: [95][119/233]	Loss 0.0010 (0.0040)	
training:	Epoch: [95][120/233]	Loss 0.0015 (0.0040)	
training:	Epoch: [95][121/233]	Loss 0.0008 (0.0039)	
training:	Epoch: [95][122/233]	Loss 0.0025 (0.0039)	
training:	Epoch: [95][123/233]	Loss 0.0009 (0.0039)	
training:	Epoch: [95][124/233]	Loss 0.0012 (0.0039)	
training:	Epoch: [95][125/233]	Loss 0.0009 (0.0039)	
training:	Epoch: [95][126/233]	Loss 0.0017 (0.0038)	
training:	Epoch: [95][127/233]	Loss 0.0011 (0.0038)	
training:	Epoch: [95][128/233]	Loss 0.0010 (0.0038)	
training:	Epoch: [95][129/233]	Loss 0.0011 (0.0038)	
training:	Epoch: [95][130/233]	Loss 0.0009 (0.0038)	
training:	Epoch: [95][131/233]	Loss 0.0025 (0.0037)	
training:	Epoch: [95][132/233]	Loss 0.0011 (0.0037)	
training:	Epoch: [95][133/233]	Loss 0.0013 (0.0037)	
training:	Epoch: [95][134/233]	Loss 0.0011 (0.0037)	
training:	Epoch: [95][135/233]	Loss 0.0010 (0.0037)	
training:	Epoch: [95][136/233]	Loss 0.0010 (0.0037)	
training:	Epoch: [95][137/233]	Loss 0.0033 (0.0036)	
training:	Epoch: [95][138/233]	Loss 0.0011 (0.0036)	
training:	Epoch: [95][139/233]	Loss 0.0010 (0.0036)	
training:	Epoch: [95][140/233]	Loss 0.0011 (0.0036)	
training:	Epoch: [95][141/233]	Loss 0.0465 (0.0039)	
training:	Epoch: [95][142/233]	Loss 0.0010 (0.0039)	
training:	Epoch: [95][143/233]	Loss 0.0010 (0.0039)	
training:	Epoch: [95][144/233]	Loss 0.2112 (0.0053)	
training:	Epoch: [95][145/233]	Loss 0.0142 (0.0054)	
training:	Epoch: [95][146/233]	Loss 0.0009 (0.0053)	
training:	Epoch: [95][147/233]	Loss 0.0009 (0.0053)	
training:	Epoch: [95][148/233]	Loss 0.2289 (0.0068)	
training:	Epoch: [95][149/233]	Loss 0.0010 (0.0068)	
training:	Epoch: [95][150/233]	Loss 0.0009 (0.0067)	
training:	Epoch: [95][151/233]	Loss 0.1126 (0.0074)	
training:	Epoch: [95][152/233]	Loss 0.0009 (0.0074)	
training:	Epoch: [95][153/233]	Loss 0.0009 (0.0073)	
training:	Epoch: [95][154/233]	Loss 0.0010 (0.0073)	
training:	Epoch: [95][155/233]	Loss 0.0026 (0.0073)	
training:	Epoch: [95][156/233]	Loss 0.0012 (0.0072)	
training:	Epoch: [95][157/233]	Loss 0.0012 (0.0072)	
training:	Epoch: [95][158/233]	Loss 0.0012 (0.0072)	
training:	Epoch: [95][159/233]	Loss 0.0032 (0.0071)	
training:	Epoch: [95][160/233]	Loss 0.0011 (0.0071)	
training:	Epoch: [95][161/233]	Loss 0.0029 (0.0071)	
training:	Epoch: [95][162/233]	Loss 0.0032 (0.0070)	
training:	Epoch: [95][163/233]	Loss 0.0009 (0.0070)	
training:	Epoch: [95][164/233]	Loss 0.0014 (0.0070)	
training:	Epoch: [95][165/233]	Loss 0.0015 (0.0069)	
training:	Epoch: [95][166/233]	Loss 0.0406 (0.0071)	
training:	Epoch: [95][167/233]	Loss 0.0030 (0.0071)	
training:	Epoch: [95][168/233]	Loss 0.0014 (0.0071)	
training:	Epoch: [95][169/233]	Loss 0.0014 (0.0071)	
training:	Epoch: [95][170/233]	Loss 0.0010 (0.0070)	
training:	Epoch: [95][171/233]	Loss 0.0015 (0.0070)	
training:	Epoch: [95][172/233]	Loss 0.0013 (0.0070)	
training:	Epoch: [95][173/233]	Loss 0.0009 (0.0069)	
training:	Epoch: [95][174/233]	Loss 0.0068 (0.0069)	
training:	Epoch: [95][175/233]	Loss 0.0035 (0.0069)	
training:	Epoch: [95][176/233]	Loss 0.0011 (0.0069)	
training:	Epoch: [95][177/233]	Loss 0.0902 (0.0073)	
training:	Epoch: [95][178/233]	Loss 0.0010 (0.0073)	
training:	Epoch: [95][179/233]	Loss 0.0016 (0.0073)	
training:	Epoch: [95][180/233]	Loss 0.0015 (0.0072)	
training:	Epoch: [95][181/233]	Loss 0.0037 (0.0072)	
training:	Epoch: [95][182/233]	Loss 0.0011 (0.0072)	
training:	Epoch: [95][183/233]	Loss 0.0993 (0.0077)	
training:	Epoch: [95][184/233]	Loss 0.0010 (0.0076)	
training:	Epoch: [95][185/233]	Loss 0.0012 (0.0076)	
training:	Epoch: [95][186/233]	Loss 0.2034 (0.0087)	
training:	Epoch: [95][187/233]	Loss 0.0009 (0.0086)	
training:	Epoch: [95][188/233]	Loss 0.0011 (0.0086)	
training:	Epoch: [95][189/233]	Loss 0.0011 (0.0085)	
training:	Epoch: [95][190/233]	Loss 0.0022 (0.0085)	
training:	Epoch: [95][191/233]	Loss 0.0030 (0.0085)	
training:	Epoch: [95][192/233]	Loss 0.0036 (0.0085)	
training:	Epoch: [95][193/233]	Loss 0.0010 (0.0084)	
training:	Epoch: [95][194/233]	Loss 0.0009 (0.0084)	
training:	Epoch: [95][195/233]	Loss 0.0149 (0.0084)	
training:	Epoch: [95][196/233]	Loss 0.0015 (0.0084)	
training:	Epoch: [95][197/233]	Loss 0.0010 (0.0083)	
training:	Epoch: [95][198/233]	Loss 0.0009 (0.0083)	
training:	Epoch: [95][199/233]	Loss 0.0011 (0.0083)	
training:	Epoch: [95][200/233]	Loss 0.0013 (0.0082)	
training:	Epoch: [95][201/233]	Loss 0.0048 (0.0082)	
training:	Epoch: [95][202/233]	Loss 0.0013 (0.0082)	
training:	Epoch: [95][203/233]	Loss 0.0012 (0.0081)	
training:	Epoch: [95][204/233]	Loss 0.0014 (0.0081)	
training:	Epoch: [95][205/233]	Loss 0.0013 (0.0081)	
training:	Epoch: [95][206/233]	Loss 0.0011 (0.0080)	
training:	Epoch: [95][207/233]	Loss 0.0052 (0.0080)	
training:	Epoch: [95][208/233]	Loss 0.0019 (0.0080)	
training:	Epoch: [95][209/233]	Loss 0.0009 (0.0080)	
training:	Epoch: [95][210/233]	Loss 0.0009 (0.0079)	
training:	Epoch: [95][211/233]	Loss 0.0012 (0.0079)	
training:	Epoch: [95][212/233]	Loss 0.0016 (0.0079)	
training:	Epoch: [95][213/233]	Loss 0.0010 (0.0078)	
training:	Epoch: [95][214/233]	Loss 0.0024 (0.0078)	
training:	Epoch: [95][215/233]	Loss 0.0010 (0.0078)	
training:	Epoch: [95][216/233]	Loss 0.0009 (0.0078)	
training:	Epoch: [95][217/233]	Loss 0.0010 (0.0077)	
training:	Epoch: [95][218/233]	Loss 0.0010 (0.0077)	
training:	Epoch: [95][219/233]	Loss 0.0014 (0.0077)	
training:	Epoch: [95][220/233]	Loss 0.0012 (0.0076)	
training:	Epoch: [95][221/233]	Loss 0.0089 (0.0076)	
training:	Epoch: [95][222/233]	Loss 0.0011 (0.0076)	
training:	Epoch: [95][223/233]	Loss 0.0011 (0.0076)	
training:	Epoch: [95][224/233]	Loss 0.0009 (0.0075)	
training:	Epoch: [95][225/233]	Loss 0.0009 (0.0075)	
training:	Epoch: [95][226/233]	Loss 0.0010 (0.0075)	
training:	Epoch: [95][227/233]	Loss 0.0012 (0.0075)	
training:	Epoch: [95][228/233]	Loss 0.0013 (0.0074)	
training:	Epoch: [95][229/233]	Loss 0.0015 (0.0074)	
training:	Epoch: [95][230/233]	Loss 0.0015 (0.0074)	
training:	Epoch: [95][231/233]	Loss 0.0129 (0.0074)	
training:	Epoch: [95][232/233]	Loss 0.0514 (0.0076)	
training:	Epoch: [95][233/233]	Loss 0.0008 (0.0076)	
Training:	 Loss: 0.0076

Training:	 ACC: 0.9996 0.9996 0.9995 0.9997
Validation:	 ACC: 0.7934 0.7924 0.7732 0.8135
Validation:	 Best_BACC: 0.8162 0.8154 0.7998 0.8326
Validation:	 Loss: 1.1294
Pretraining:	Epoch 96/200
----------
training:	Epoch: [96][1/233]	Loss 0.0759 (0.0759)	
training:	Epoch: [96][2/233]	Loss 0.0008 (0.0384)	
training:	Epoch: [96][3/233]	Loss 0.0012 (0.0260)	
training:	Epoch: [96][4/233]	Loss 0.0009 (0.0197)	
training:	Epoch: [96][5/233]	Loss 0.0090 (0.0176)	
training:	Epoch: [96][6/233]	Loss 0.0011 (0.0148)	
training:	Epoch: [96][7/233]	Loss 0.0011 (0.0129)	
training:	Epoch: [96][8/233]	Loss 0.0009 (0.0114)	
training:	Epoch: [96][9/233]	Loss 0.0009 (0.0102)	
training:	Epoch: [96][10/233]	Loss 0.0014 (0.0093)	
training:	Epoch: [96][11/233]	Loss 0.0011 (0.0086)	
training:	Epoch: [96][12/233]	Loss 0.0010 (0.0079)	
training:	Epoch: [96][13/233]	Loss 0.0009 (0.0074)	
training:	Epoch: [96][14/233]	Loss 0.0010 (0.0069)	
training:	Epoch: [96][15/233]	Loss 0.2942 (0.0261)	
training:	Epoch: [96][16/233]	Loss 0.0010 (0.0245)	
training:	Epoch: [96][17/233]	Loss 0.0031 (0.0233)	
training:	Epoch: [96][18/233]	Loss 0.0012 (0.0220)	
training:	Epoch: [96][19/233]	Loss 0.0009 (0.0209)	
training:	Epoch: [96][20/233]	Loss 0.0009 (0.0199)	
training:	Epoch: [96][21/233]	Loss 0.0042 (0.0192)	
training:	Epoch: [96][22/233]	Loss 0.0035 (0.0185)	
training:	Epoch: [96][23/233]	Loss 0.0013 (0.0177)	
training:	Epoch: [96][24/233]	Loss 0.0018 (0.0170)	
training:	Epoch: [96][25/233]	Loss 0.0009 (0.0164)	
training:	Epoch: [96][26/233]	Loss 0.0061 (0.0160)	
training:	Epoch: [96][27/233]	Loss 0.0013 (0.0155)	
training:	Epoch: [96][28/233]	Loss 0.0009 (0.0149)	
training:	Epoch: [96][29/233]	Loss 0.0009 (0.0145)	
training:	Epoch: [96][30/233]	Loss 0.0061 (0.0142)	
training:	Epoch: [96][31/233]	Loss 0.0393 (0.0150)	
training:	Epoch: [96][32/233]	Loss 0.0018 (0.0146)	
training:	Epoch: [96][33/233]	Loss 0.2034 (0.0203)	
training:	Epoch: [96][34/233]	Loss 0.0010 (0.0197)	
training:	Epoch: [96][35/233]	Loss 0.0014 (0.0192)	
training:	Epoch: [96][36/233]	Loss 0.0018 (0.0187)	
training:	Epoch: [96][37/233]	Loss 0.0011 (0.0182)	
training:	Epoch: [96][38/233]	Loss 0.0011 (0.0178)	
training:	Epoch: [96][39/233]	Loss 0.0044 (0.0174)	
training:	Epoch: [96][40/233]	Loss 0.0024 (0.0171)	
training:	Epoch: [96][41/233]	Loss 0.0017 (0.0167)	
training:	Epoch: [96][42/233]	Loss 0.0055 (0.0164)	
training:	Epoch: [96][43/233]	Loss 0.0025 (0.0161)	
training:	Epoch: [96][44/233]	Loss 0.0031 (0.0158)	
training:	Epoch: [96][45/233]	Loss 0.0033 (0.0155)	
training:	Epoch: [96][46/233]	Loss 0.0020 (0.0152)	
training:	Epoch: [96][47/233]	Loss 0.0017 (0.0150)	
training:	Epoch: [96][48/233]	Loss 0.0097 (0.0148)	
training:	Epoch: [96][49/233]	Loss 0.0046 (0.0146)	
training:	Epoch: [96][50/233]	Loss 0.0017 (0.0144)	
training:	Epoch: [96][51/233]	Loss 0.0011 (0.0141)	
training:	Epoch: [96][52/233]	Loss 0.0019 (0.0139)	
training:	Epoch: [96][53/233]	Loss 0.0486 (0.0145)	
training:	Epoch: [96][54/233]	Loss 0.0008 (0.0143)	
training:	Epoch: [96][55/233]	Loss 0.0016 (0.0141)	
training:	Epoch: [96][56/233]	Loss 0.0184 (0.0141)	
training:	Epoch: [96][57/233]	Loss 0.0022 (0.0139)	
training:	Epoch: [96][58/233]	Loss 0.0018 (0.0137)	
training:	Epoch: [96][59/233]	Loss 0.0013 (0.0135)	
training:	Epoch: [96][60/233]	Loss 0.0364 (0.0139)	
training:	Epoch: [96][61/233]	Loss 0.0012 (0.0137)	
training:	Epoch: [96][62/233]	Loss 0.0012 (0.0135)	
training:	Epoch: [96][63/233]	Loss 0.0379 (0.0139)	
training:	Epoch: [96][64/233]	Loss 0.0010 (0.0137)	
training:	Epoch: [96][65/233]	Loss 0.0014 (0.0135)	
training:	Epoch: [96][66/233]	Loss 0.0101 (0.0134)	
training:	Epoch: [96][67/233]	Loss 0.0010 (0.0132)	
training:	Epoch: [96][68/233]	Loss 0.0020 (0.0131)	
training:	Epoch: [96][69/233]	Loss 0.0012 (0.0129)	
training:	Epoch: [96][70/233]	Loss 0.0010 (0.0127)	
training:	Epoch: [96][71/233]	Loss 0.0010 (0.0126)	
training:	Epoch: [96][72/233]	Loss 0.0012 (0.0124)	
training:	Epoch: [96][73/233]	Loss 0.0015 (0.0123)	
training:	Epoch: [96][74/233]	Loss 0.0045 (0.0122)	
training:	Epoch: [96][75/233]	Loss 0.0014 (0.0120)	
training:	Epoch: [96][76/233]	Loss 0.0017 (0.0119)	
training:	Epoch: [96][77/233]	Loss 0.0009 (0.0117)	
training:	Epoch: [96][78/233]	Loss 0.0014 (0.0116)	
training:	Epoch: [96][79/233]	Loss 0.0011 (0.0115)	
training:	Epoch: [96][80/233]	Loss 0.0011 (0.0113)	
training:	Epoch: [96][81/233]	Loss 0.0011 (0.0112)	
training:	Epoch: [96][82/233]	Loss 0.0039 (0.0111)	
training:	Epoch: [96][83/233]	Loss 0.0010 (0.0110)	
training:	Epoch: [96][84/233]	Loss 0.0028 (0.0109)	
training:	Epoch: [96][85/233]	Loss 0.0132 (0.0109)	
training:	Epoch: [96][86/233]	Loss 0.0010 (0.0108)	
training:	Epoch: [96][87/233]	Loss 0.1506 (0.0124)	
training:	Epoch: [96][88/233]	Loss 0.0012 (0.0123)	
training:	Epoch: [96][89/233]	Loss 0.0116 (0.0123)	
training:	Epoch: [96][90/233]	Loss 0.0011 (0.0122)	
training:	Epoch: [96][91/233]	Loss 0.0017 (0.0120)	
training:	Epoch: [96][92/233]	Loss 0.0013 (0.0119)	
training:	Epoch: [96][93/233]	Loss 0.0013 (0.0118)	
training:	Epoch: [96][94/233]	Loss 0.0018 (0.0117)	
training:	Epoch: [96][95/233]	Loss 0.0010 (0.0116)	
training:	Epoch: [96][96/233]	Loss 0.0010 (0.0115)	
training:	Epoch: [96][97/233]	Loss 0.0034 (0.0114)	
training:	Epoch: [96][98/233]	Loss 0.0010 (0.0113)	
training:	Epoch: [96][99/233]	Loss 0.0305 (0.0115)	
training:	Epoch: [96][100/233]	Loss 0.0011 (0.0114)	
training:	Epoch: [96][101/233]	Loss 0.0871 (0.0121)	
training:	Epoch: [96][102/233]	Loss 0.0653 (0.0127)	
training:	Epoch: [96][103/233]	Loss 0.0011 (0.0125)	
training:	Epoch: [96][104/233]	Loss 0.0026 (0.0124)	
training:	Epoch: [96][105/233]	Loss 0.0012 (0.0123)	
training:	Epoch: [96][106/233]	Loss 0.0010 (0.0122)	
training:	Epoch: [96][107/233]	Loss 0.0010 (0.0121)	
training:	Epoch: [96][108/233]	Loss 0.0073 (0.0121)	
training:	Epoch: [96][109/233]	Loss 0.0018 (0.0120)	
training:	Epoch: [96][110/233]	Loss 0.0011 (0.0119)	
training:	Epoch: [96][111/233]	Loss 0.0033 (0.0118)	
training:	Epoch: [96][112/233]	Loss 0.0010 (0.0117)	
training:	Epoch: [96][113/233]	Loss 0.0008 (0.0116)	
training:	Epoch: [96][114/233]	Loss 0.0018 (0.0115)	
training:	Epoch: [96][115/233]	Loss 0.0018 (0.0114)	
training:	Epoch: [96][116/233]	Loss 0.0011 (0.0114)	
training:	Epoch: [96][117/233]	Loss 0.0011 (0.0113)	
training:	Epoch: [96][118/233]	Loss 0.0011 (0.0112)	
training:	Epoch: [96][119/233]	Loss 0.0011 (0.0111)	
training:	Epoch: [96][120/233]	Loss 0.0008 (0.0110)	
training:	Epoch: [96][121/233]	Loss 0.0014 (0.0109)	
training:	Epoch: [96][122/233]	Loss 0.0099 (0.0109)	
training:	Epoch: [96][123/233]	Loss 0.0011 (0.0108)	
training:	Epoch: [96][124/233]	Loss 0.0018 (0.0108)	
training:	Epoch: [96][125/233]	Loss 0.0019 (0.0107)	
training:	Epoch: [96][126/233]	Loss 0.0075 (0.0107)	
training:	Epoch: [96][127/233]	Loss 0.0013 (0.0106)	
training:	Epoch: [96][128/233]	Loss 0.0015 (0.0105)	
training:	Epoch: [96][129/233]	Loss 0.0015 (0.0105)	
training:	Epoch: [96][130/233]	Loss 0.0018 (0.0104)	
training:	Epoch: [96][131/233]	Loss 0.0014 (0.0103)	
training:	Epoch: [96][132/233]	Loss 0.0325 (0.0105)	
training:	Epoch: [96][133/233]	Loss 0.0012 (0.0104)	
training:	Epoch: [96][134/233]	Loss 0.0011 (0.0104)	
training:	Epoch: [96][135/233]	Loss 0.0012 (0.0103)	
training:	Epoch: [96][136/233]	Loss 0.0015 (0.0102)	
training:	Epoch: [96][137/233]	Loss 0.0024 (0.0102)	
training:	Epoch: [96][138/233]	Loss 0.0015 (0.0101)	
training:	Epoch: [96][139/233]	Loss 0.1454 (0.0111)	
training:	Epoch: [96][140/233]	Loss 0.0057 (0.0110)	
training:	Epoch: [96][141/233]	Loss 0.0043 (0.0110)	
training:	Epoch: [96][142/233]	Loss 0.0012 (0.0109)	
training:	Epoch: [96][143/233]	Loss 0.0460 (0.0112)	
training:	Epoch: [96][144/233]	Loss 0.0016 (0.0111)	
training:	Epoch: [96][145/233]	Loss 0.0008 (0.0110)	
training:	Epoch: [96][146/233]	Loss 0.0011 (0.0110)	
training:	Epoch: [96][147/233]	Loss 0.0292 (0.0111)	
training:	Epoch: [96][148/233]	Loss 0.0010 (0.0110)	
training:	Epoch: [96][149/233]	Loss 0.0031 (0.0110)	
training:	Epoch: [96][150/233]	Loss 0.0012 (0.0109)	
training:	Epoch: [96][151/233]	Loss 0.0010 (0.0108)	
training:	Epoch: [96][152/233]	Loss 0.0010 (0.0108)	
training:	Epoch: [96][153/233]	Loss 0.0019 (0.0107)	
training:	Epoch: [96][154/233]	Loss 0.0481 (0.0110)	
training:	Epoch: [96][155/233]	Loss 0.0010 (0.0109)	
training:	Epoch: [96][156/233]	Loss 0.0010 (0.0108)	
training:	Epoch: [96][157/233]	Loss 0.0013 (0.0108)	
training:	Epoch: [96][158/233]	Loss 0.0010 (0.0107)	
training:	Epoch: [96][159/233]	Loss 0.0011 (0.0106)	
training:	Epoch: [96][160/233]	Loss 0.0022 (0.0106)	
training:	Epoch: [96][161/233]	Loss 0.0010 (0.0105)	
training:	Epoch: [96][162/233]	Loss 0.0014 (0.0105)	
training:	Epoch: [96][163/233]	Loss 0.0010 (0.0104)	
training:	Epoch: [96][164/233]	Loss 0.0025 (0.0104)	
training:	Epoch: [96][165/233]	Loss 0.0009 (0.0103)	
training:	Epoch: [96][166/233]	Loss 0.0013 (0.0103)	
training:	Epoch: [96][167/233]	Loss 0.0036 (0.0102)	
training:	Epoch: [96][168/233]	Loss 0.0010 (0.0102)	
training:	Epoch: [96][169/233]	Loss 0.0022 (0.0101)	
training:	Epoch: [96][170/233]	Loss 0.0059 (0.0101)	
training:	Epoch: [96][171/233]	Loss 0.0010 (0.0100)	
training:	Epoch: [96][172/233]	Loss 0.0015 (0.0100)	
training:	Epoch: [96][173/233]	Loss 0.0008 (0.0099)	
training:	Epoch: [96][174/233]	Loss 0.0013 (0.0099)	
training:	Epoch: [96][175/233]	Loss 0.0015 (0.0098)	
training:	Epoch: [96][176/233]	Loss 0.0036 (0.0098)	
training:	Epoch: [96][177/233]	Loss 0.0011 (0.0098)	
training:	Epoch: [96][178/233]	Loss 0.0010 (0.0097)	
training:	Epoch: [96][179/233]	Loss 0.0014 (0.0097)	
training:	Epoch: [96][180/233]	Loss 0.0016 (0.0096)	
training:	Epoch: [96][181/233]	Loss 0.0017 (0.0096)	
training:	Epoch: [96][182/233]	Loss 0.0015 (0.0095)	
training:	Epoch: [96][183/233]	Loss 0.0012 (0.0095)	
training:	Epoch: [96][184/233]	Loss 0.0555 (0.0097)	
training:	Epoch: [96][185/233]	Loss 0.0014 (0.0097)	
training:	Epoch: [96][186/233]	Loss 0.0028 (0.0096)	
training:	Epoch: [96][187/233]	Loss 0.0016 (0.0096)	
training:	Epoch: [96][188/233]	Loss 0.0121 (0.0096)	
training:	Epoch: [96][189/233]	Loss 0.0021 (0.0096)	
training:	Epoch: [96][190/233]	Loss 0.0010 (0.0095)	
training:	Epoch: [96][191/233]	Loss 0.0721 (0.0099)	
training:	Epoch: [96][192/233]	Loss 0.0195 (0.0099)	
training:	Epoch: [96][193/233]	Loss 0.0008 (0.0099)	
training:	Epoch: [96][194/233]	Loss 0.0012 (0.0098)	
training:	Epoch: [96][195/233]	Loss 0.0010 (0.0098)	
training:	Epoch: [96][196/233]	Loss 0.0013 (0.0097)	
training:	Epoch: [96][197/233]	Loss 0.0010 (0.0097)	
training:	Epoch: [96][198/233]	Loss 0.0014 (0.0096)	
training:	Epoch: [96][199/233]	Loss 0.0016 (0.0096)	
training:	Epoch: [96][200/233]	Loss 0.0022 (0.0096)	
training:	Epoch: [96][201/233]	Loss 0.0010 (0.0095)	
training:	Epoch: [96][202/233]	Loss 0.0013 (0.0095)	
training:	Epoch: [96][203/233]	Loss 0.0010 (0.0094)	
training:	Epoch: [96][204/233]	Loss 0.1598 (0.0102)	
training:	Epoch: [96][205/233]	Loss 0.2128 (0.0112)	
training:	Epoch: [96][206/233]	Loss 0.0012 (0.0111)	
training:	Epoch: [96][207/233]	Loss 0.0009 (0.0111)	
training:	Epoch: [96][208/233]	Loss 0.0011 (0.0110)	
training:	Epoch: [96][209/233]	Loss 0.0029 (0.0110)	
training:	Epoch: [96][210/233]	Loss 0.0010 (0.0109)	
training:	Epoch: [96][211/233]	Loss 0.0024 (0.0109)	
training:	Epoch: [96][212/233]	Loss 0.0016 (0.0108)	
training:	Epoch: [96][213/233]	Loss 0.0010 (0.0108)	
training:	Epoch: [96][214/233]	Loss 0.0010 (0.0108)	
training:	Epoch: [96][215/233]	Loss 0.0010 (0.0107)	
training:	Epoch: [96][216/233]	Loss 0.0009 (0.0107)	
training:	Epoch: [96][217/233]	Loss 0.0025 (0.0106)	
training:	Epoch: [96][218/233]	Loss 0.0008 (0.0106)	
training:	Epoch: [96][219/233]	Loss 0.0012 (0.0105)	
training:	Epoch: [96][220/233]	Loss 0.0033 (0.0105)	
training:	Epoch: [96][221/233]	Loss 0.0012 (0.0105)	
training:	Epoch: [96][222/233]	Loss 0.0013 (0.0104)	
training:	Epoch: [96][223/233]	Loss 0.0014 (0.0104)	
training:	Epoch: [96][224/233]	Loss 0.0012 (0.0103)	
training:	Epoch: [96][225/233]	Loss 0.1039 (0.0108)	
training:	Epoch: [96][226/233]	Loss 0.0068 (0.0107)	
training:	Epoch: [96][227/233]	Loss 0.0015 (0.0107)	
training:	Epoch: [96][228/233]	Loss 0.0025 (0.0107)	
training:	Epoch: [96][229/233]	Loss 0.0015 (0.0106)	
training:	Epoch: [96][230/233]	Loss 0.0092 (0.0106)	
training:	Epoch: [96][231/233]	Loss 0.0009 (0.0106)	
training:	Epoch: [96][232/233]	Loss 0.0009 (0.0105)	
training:	Epoch: [96][233/233]	Loss 0.0017 (0.0105)	
Training:	 Loss: 0.0105

Training:	 ACC: 0.9996 0.9996 0.9995 0.9997
Validation:	 ACC: 0.8020 0.8020 0.8018 0.8022
Validation:	 Best_BACC: 0.8162 0.8154 0.7998 0.8326
Validation:	 Loss: 1.0873
Pretraining:	Epoch 97/200
----------
training:	Epoch: [97][1/233]	Loss 0.0014 (0.0014)	
training:	Epoch: [97][2/233]	Loss 0.0010 (0.0012)	
training:	Epoch: [97][3/233]	Loss 0.0009 (0.0011)	
training:	Epoch: [97][4/233]	Loss 0.0080 (0.0028)	
training:	Epoch: [97][5/233]	Loss 0.0023 (0.0027)	
training:	Epoch: [97][6/233]	Loss 0.0069 (0.0034)	
training:	Epoch: [97][7/233]	Loss 0.0015 (0.0031)	
training:	Epoch: [97][8/233]	Loss 0.0039 (0.0032)	
training:	Epoch: [97][9/233]	Loss 0.0019 (0.0031)	
training:	Epoch: [97][10/233]	Loss 0.0017 (0.0030)	
training:	Epoch: [97][11/233]	Loss 0.0014 (0.0028)	
training:	Epoch: [97][12/233]	Loss 0.0019 (0.0027)	
training:	Epoch: [97][13/233]	Loss 0.0009 (0.0026)	
training:	Epoch: [97][14/233]	Loss 0.0016 (0.0025)	
training:	Epoch: [97][15/233]	Loss 0.0017 (0.0025)	
training:	Epoch: [97][16/233]	Loss 0.0010 (0.0024)	
training:	Epoch: [97][17/233]	Loss 0.0013 (0.0023)	
training:	Epoch: [97][18/233]	Loss 0.0043 (0.0024)	
training:	Epoch: [97][19/233]	Loss 0.0009 (0.0023)	
training:	Epoch: [97][20/233]	Loss 0.0011 (0.0023)	
training:	Epoch: [97][21/233]	Loss 0.0029 (0.0023)	
training:	Epoch: [97][22/233]	Loss 0.0011 (0.0023)	
training:	Epoch: [97][23/233]	Loss 0.0011 (0.0022)	
training:	Epoch: [97][24/233]	Loss 0.0009 (0.0022)	
training:	Epoch: [97][25/233]	Loss 0.0018 (0.0021)	
training:	Epoch: [97][26/233]	Loss 0.0029 (0.0022)	
training:	Epoch: [97][27/233]	Loss 0.0620 (0.0044)	
training:	Epoch: [97][28/233]	Loss 0.0353 (0.0055)	
training:	Epoch: [97][29/233]	Loss 0.0010 (0.0053)	
training:	Epoch: [97][30/233]	Loss 0.0011 (0.0052)	
training:	Epoch: [97][31/233]	Loss 0.0011 (0.0051)	
training:	Epoch: [97][32/233]	Loss 0.0012 (0.0049)	
training:	Epoch: [97][33/233]	Loss 0.0254 (0.0056)	
training:	Epoch: [97][34/233]	Loss 0.0009 (0.0054)	
training:	Epoch: [97][35/233]	Loss 0.2325 (0.0119)	
training:	Epoch: [97][36/233]	Loss 0.0011 (0.0116)	
training:	Epoch: [97][37/233]	Loss 0.0009 (0.0113)	
training:	Epoch: [97][38/233]	Loss 0.0007 (0.0110)	
training:	Epoch: [97][39/233]	Loss 0.0023 (0.0108)	
training:	Epoch: [97][40/233]	Loss 0.0017 (0.0106)	
training:	Epoch: [97][41/233]	Loss 0.0008 (0.0104)	
training:	Epoch: [97][42/233]	Loss 0.0013 (0.0101)	
training:	Epoch: [97][43/233]	Loss 0.0386 (0.0108)	
training:	Epoch: [97][44/233]	Loss 0.0055 (0.0107)	
training:	Epoch: [97][45/233]	Loss 0.0012 (0.0105)	
training:	Epoch: [97][46/233]	Loss 0.0008 (0.0103)	
training:	Epoch: [97][47/233]	Loss 0.0009 (0.0101)	
training:	Epoch: [97][48/233]	Loss 0.0015 (0.0099)	
training:	Epoch: [97][49/233]	Loss 0.1159 (0.0120)	
training:	Epoch: [97][50/233]	Loss 0.0155 (0.0121)	
training:	Epoch: [97][51/233]	Loss 0.0009 (0.0119)	
training:	Epoch: [97][52/233]	Loss 0.0010 (0.0117)	
training:	Epoch: [97][53/233]	Loss 0.0013 (0.0115)	
training:	Epoch: [97][54/233]	Loss 0.0040 (0.0113)	
training:	Epoch: [97][55/233]	Loss 0.0010 (0.0112)	
training:	Epoch: [97][56/233]	Loss 0.0011 (0.0110)	
training:	Epoch: [97][57/233]	Loss 0.0010 (0.0108)	
training:	Epoch: [97][58/233]	Loss 0.0010 (0.0106)	
training:	Epoch: [97][59/233]	Loss 0.0008 (0.0105)	
training:	Epoch: [97][60/233]	Loss 0.0008 (0.0103)	
training:	Epoch: [97][61/233]	Loss 0.0013 (0.0102)	
training:	Epoch: [97][62/233]	Loss 0.0011 (0.0100)	
training:	Epoch: [97][63/233]	Loss 0.0015 (0.0099)	
training:	Epoch: [97][64/233]	Loss 0.0010 (0.0097)	
training:	Epoch: [97][65/233]	Loss 0.0492 (0.0104)	
training:	Epoch: [97][66/233]	Loss 0.0025 (0.0102)	
training:	Epoch: [97][67/233]	Loss 0.0011 (0.0101)	
training:	Epoch: [97][68/233]	Loss 0.0009 (0.0100)	
training:	Epoch: [97][69/233]	Loss 0.0010 (0.0098)	
training:	Epoch: [97][70/233]	Loss 0.0009 (0.0097)	
training:	Epoch: [97][71/233]	Loss 0.0011 (0.0096)	
training:	Epoch: [97][72/233]	Loss 0.0032 (0.0095)	
training:	Epoch: [97][73/233]	Loss 0.0010 (0.0094)	
training:	Epoch: [97][74/233]	Loss 0.0009 (0.0093)	
training:	Epoch: [97][75/233]	Loss 0.0029 (0.0092)	
training:	Epoch: [97][76/233]	Loss 0.0009 (0.0091)	
training:	Epoch: [97][77/233]	Loss 0.0035 (0.0090)	
training:	Epoch: [97][78/233]	Loss 0.0011 (0.0089)	
training:	Epoch: [97][79/233]	Loss 0.1827 (0.0111)	
training:	Epoch: [97][80/233]	Loss 0.0161 (0.0112)	
training:	Epoch: [97][81/233]	Loss 0.0009 (0.0110)	
training:	Epoch: [97][82/233]	Loss 0.0010 (0.0109)	
training:	Epoch: [97][83/233]	Loss 0.0025 (0.0108)	
training:	Epoch: [97][84/233]	Loss 0.0008 (0.0107)	
training:	Epoch: [97][85/233]	Loss 0.0011 (0.0106)	
training:	Epoch: [97][86/233]	Loss 0.0032 (0.0105)	
training:	Epoch: [97][87/233]	Loss 0.0009 (0.0104)	
training:	Epoch: [97][88/233]	Loss 0.0009 (0.0103)	
training:	Epoch: [97][89/233]	Loss 0.0009 (0.0102)	
training:	Epoch: [97][90/233]	Loss 0.0012 (0.0101)	
training:	Epoch: [97][91/233]	Loss 0.0013 (0.0100)	
training:	Epoch: [97][92/233]	Loss 0.0011 (0.0099)	
training:	Epoch: [97][93/233]	Loss 0.0016 (0.0098)	
training:	Epoch: [97][94/233]	Loss 0.0012 (0.0097)	
training:	Epoch: [97][95/233]	Loss 0.0010 (0.0096)	
training:	Epoch: [97][96/233]	Loss 0.0010 (0.0095)	
training:	Epoch: [97][97/233]	Loss 0.0008 (0.0094)	
training:	Epoch: [97][98/233]	Loss 0.0012 (0.0093)	
training:	Epoch: [97][99/233]	Loss 0.0011 (0.0093)	
training:	Epoch: [97][100/233]	Loss 0.0012 (0.0092)	
training:	Epoch: [97][101/233]	Loss 0.0056 (0.0091)	
training:	Epoch: [97][102/233]	Loss 0.0012 (0.0091)	
training:	Epoch: [97][103/233]	Loss 0.0010 (0.0090)	
training:	Epoch: [97][104/233]	Loss 0.0013 (0.0089)	
training:	Epoch: [97][105/233]	Loss 0.0010 (0.0088)	
training:	Epoch: [97][106/233]	Loss 0.0009 (0.0088)	
training:	Epoch: [97][107/233]	Loss 0.0010 (0.0087)	
training:	Epoch: [97][108/233]	Loss 0.0017 (0.0086)	
training:	Epoch: [97][109/233]	Loss 0.0017 (0.0086)	
training:	Epoch: [97][110/233]	Loss 0.0009 (0.0085)	
training:	Epoch: [97][111/233]	Loss 0.0009 (0.0084)	
training:	Epoch: [97][112/233]	Loss 0.0008 (0.0084)	
training:	Epoch: [97][113/233]	Loss 0.0010 (0.0083)	
training:	Epoch: [97][114/233]	Loss 0.2087 (0.0100)	
training:	Epoch: [97][115/233]	Loss 0.0072 (0.0100)	
training:	Epoch: [97][116/233]	Loss 0.0009 (0.0099)	
training:	Epoch: [97][117/233]	Loss 0.0013 (0.0099)	
training:	Epoch: [97][118/233]	Loss 0.0011 (0.0098)	
training:	Epoch: [97][119/233]	Loss 0.0018 (0.0097)	
training:	Epoch: [97][120/233]	Loss 0.0010 (0.0097)	
training:	Epoch: [97][121/233]	Loss 0.0010 (0.0096)	
training:	Epoch: [97][122/233]	Loss 0.0010 (0.0095)	
training:	Epoch: [97][123/233]	Loss 0.0100 (0.0095)	
training:	Epoch: [97][124/233]	Loss 0.0013 (0.0095)	
training:	Epoch: [97][125/233]	Loss 0.0011 (0.0094)	
training:	Epoch: [97][126/233]	Loss 0.0009 (0.0093)	
training:	Epoch: [97][127/233]	Loss 0.0026 (0.0093)	
training:	Epoch: [97][128/233]	Loss 0.0009 (0.0092)	
training:	Epoch: [97][129/233]	Loss 0.0075 (0.0092)	
training:	Epoch: [97][130/233]	Loss 0.0008 (0.0091)	
training:	Epoch: [97][131/233]	Loss 0.0011 (0.0091)	
training:	Epoch: [97][132/233]	Loss 0.0011 (0.0090)	
training:	Epoch: [97][133/233]	Loss 0.0013 (0.0089)	
training:	Epoch: [97][134/233]	Loss 0.0015 (0.0089)	
training:	Epoch: [97][135/233]	Loss 0.0015 (0.0088)	
training:	Epoch: [97][136/233]	Loss 0.0010 (0.0088)	
training:	Epoch: [97][137/233]	Loss 0.0009 (0.0087)	
training:	Epoch: [97][138/233]	Loss 0.0009 (0.0087)	
training:	Epoch: [97][139/233]	Loss 0.0010 (0.0086)	
training:	Epoch: [97][140/233]	Loss 0.0009 (0.0086)	
training:	Epoch: [97][141/233]	Loss 0.0010 (0.0085)	
training:	Epoch: [97][142/233]	Loss 0.0018 (0.0084)	
training:	Epoch: [97][143/233]	Loss 0.0022 (0.0084)	
training:	Epoch: [97][144/233]	Loss 0.0010 (0.0084)	
training:	Epoch: [97][145/233]	Loss 0.0010 (0.0083)	
training:	Epoch: [97][146/233]	Loss 0.0012 (0.0083)	
training:	Epoch: [97][147/233]	Loss 0.0051 (0.0082)	
training:	Epoch: [97][148/233]	Loss 0.0011 (0.0082)	
training:	Epoch: [97][149/233]	Loss 0.0008 (0.0081)	
training:	Epoch: [97][150/233]	Loss 0.0018 (0.0081)	
training:	Epoch: [97][151/233]	Loss 0.0019 (0.0081)	
training:	Epoch: [97][152/233]	Loss 0.0009 (0.0080)	
training:	Epoch: [97][153/233]	Loss 0.0010 (0.0080)	
training:	Epoch: [97][154/233]	Loss 0.0018 (0.0079)	
training:	Epoch: [97][155/233]	Loss 0.0009 (0.0079)	
training:	Epoch: [97][156/233]	Loss 0.0022 (0.0078)	
training:	Epoch: [97][157/233]	Loss 0.0018 (0.0078)	
training:	Epoch: [97][158/233]	Loss 0.0009 (0.0078)	
training:	Epoch: [97][159/233]	Loss 0.0012 (0.0077)	
training:	Epoch: [97][160/233]	Loss 0.0090 (0.0077)	
training:	Epoch: [97][161/233]	Loss 0.0013 (0.0077)	
training:	Epoch: [97][162/233]	Loss 0.0010 (0.0076)	
training:	Epoch: [97][163/233]	Loss 0.0008 (0.0076)	
training:	Epoch: [97][164/233]	Loss 0.0018 (0.0076)	
training:	Epoch: [97][165/233]	Loss 0.0015 (0.0075)	
training:	Epoch: [97][166/233]	Loss 0.0014 (0.0075)	
training:	Epoch: [97][167/233]	Loss 0.0011 (0.0075)	
training:	Epoch: [97][168/233]	Loss 0.0009 (0.0074)	
training:	Epoch: [97][169/233]	Loss 0.0017 (0.0074)	
training:	Epoch: [97][170/233]	Loss 0.0009 (0.0073)	
training:	Epoch: [97][171/233]	Loss 0.0049 (0.0073)	
training:	Epoch: [97][172/233]	Loss 0.0010 (0.0073)	
training:	Epoch: [97][173/233]	Loss 0.0019 (0.0073)	
training:	Epoch: [97][174/233]	Loss 0.0009 (0.0072)	
training:	Epoch: [97][175/233]	Loss 0.0009 (0.0072)	
training:	Epoch: [97][176/233]	Loss 0.0009 (0.0072)	
training:	Epoch: [97][177/233]	Loss 0.0009 (0.0071)	
training:	Epoch: [97][178/233]	Loss 0.0010 (0.0071)	
training:	Epoch: [97][179/233]	Loss 0.0010 (0.0070)	
training:	Epoch: [97][180/233]	Loss 0.0009 (0.0070)	
training:	Epoch: [97][181/233]	Loss 0.0011 (0.0070)	
training:	Epoch: [97][182/233]	Loss 0.0012 (0.0069)	
training:	Epoch: [97][183/233]	Loss 0.0029 (0.0069)	
training:	Epoch: [97][184/233]	Loss 0.0012 (0.0069)	
training:	Epoch: [97][185/233]	Loss 0.0028 (0.0069)	
training:	Epoch: [97][186/233]	Loss 0.0416 (0.0071)	
training:	Epoch: [97][187/233]	Loss 0.0009 (0.0070)	
training:	Epoch: [97][188/233]	Loss 0.0008 (0.0070)	
training:	Epoch: [97][189/233]	Loss 0.0022 (0.0070)	
training:	Epoch: [97][190/233]	Loss 0.0009 (0.0069)	
training:	Epoch: [97][191/233]	Loss 0.0016 (0.0069)	
training:	Epoch: [97][192/233]	Loss 0.0010 (0.0069)	
training:	Epoch: [97][193/233]	Loss 0.0010 (0.0068)	
training:	Epoch: [97][194/233]	Loss 0.0009 (0.0068)	
training:	Epoch: [97][195/233]	Loss 0.0010 (0.0068)	
training:	Epoch: [97][196/233]	Loss 0.0009 (0.0068)	
training:	Epoch: [97][197/233]	Loss 0.0009 (0.0067)	
training:	Epoch: [97][198/233]	Loss 0.0448 (0.0069)	
training:	Epoch: [97][199/233]	Loss 0.0015 (0.0069)	
training:	Epoch: [97][200/233]	Loss 0.0031 (0.0069)	
training:	Epoch: [97][201/233]	Loss 0.0008 (0.0068)	
training:	Epoch: [97][202/233]	Loss 0.0008 (0.0068)	
training:	Epoch: [97][203/233]	Loss 0.0009 (0.0068)	
training:	Epoch: [97][204/233]	Loss 0.2165 (0.0078)	
training:	Epoch: [97][205/233]	Loss 0.0012 (0.0078)	
training:	Epoch: [97][206/233]	Loss 0.0016 (0.0077)	
training:	Epoch: [97][207/233]	Loss 0.0019 (0.0077)	
training:	Epoch: [97][208/233]	Loss 0.0009 (0.0077)	
training:	Epoch: [97][209/233]	Loss 0.0009 (0.0077)	
training:	Epoch: [97][210/233]	Loss 0.0106 (0.0077)	
training:	Epoch: [97][211/233]	Loss 0.0009 (0.0076)	
training:	Epoch: [97][212/233]	Loss 0.0008 (0.0076)	
training:	Epoch: [97][213/233]	Loss 0.0016 (0.0076)	
training:	Epoch: [97][214/233]	Loss 0.0012 (0.0075)	
training:	Epoch: [97][215/233]	Loss 0.0015 (0.0075)	
training:	Epoch: [97][216/233]	Loss 0.0013 (0.0075)	
training:	Epoch: [97][217/233]	Loss 0.0048 (0.0075)	
training:	Epoch: [97][218/233]	Loss 0.0011 (0.0074)	
training:	Epoch: [97][219/233]	Loss 0.0010 (0.0074)	
training:	Epoch: [97][220/233]	Loss 0.0015 (0.0074)	
training:	Epoch: [97][221/233]	Loss 0.0009 (0.0074)	
training:	Epoch: [97][222/233]	Loss 0.0025 (0.0073)	
training:	Epoch: [97][223/233]	Loss 0.0009 (0.0073)	
training:	Epoch: [97][224/233]	Loss 0.0009 (0.0073)	
training:	Epoch: [97][225/233]	Loss 0.0255 (0.0074)	
training:	Epoch: [97][226/233]	Loss 0.0011 (0.0073)	
training:	Epoch: [97][227/233]	Loss 0.0008 (0.0073)	
training:	Epoch: [97][228/233]	Loss 0.0014 (0.0073)	
training:	Epoch: [97][229/233]	Loss 0.0008 (0.0073)	
training:	Epoch: [97][230/233]	Loss 0.0020 (0.0072)	
training:	Epoch: [97][231/233]	Loss 0.0195 (0.0073)	
training:	Epoch: [97][232/233]	Loss 0.0011 (0.0073)	
training:	Epoch: [97][233/233]	Loss 0.0011 (0.0072)	
Training:	 Loss: 0.0072

Training:	 ACC: 0.9996 0.9996 0.9995 0.9997
Validation:	 ACC: 0.8052 0.8058 0.8172 0.7933
Validation:	 Best_BACC: 0.8162 0.8154 0.7998 0.8326
Validation:	 Loss: 1.1015
Pretraining:	Epoch 98/200
----------
training:	Epoch: [98][1/233]	Loss 0.0009 (0.0009)	
training:	Epoch: [98][2/233]	Loss 0.0012 (0.0011)	
training:	Epoch: [98][3/233]	Loss 0.0017 (0.0013)	
training:	Epoch: [98][4/233]	Loss 0.0009 (0.0012)	
training:	Epoch: [98][5/233]	Loss 0.0052 (0.0020)	
training:	Epoch: [98][6/233]	Loss 0.0018 (0.0019)	
training:	Epoch: [98][7/233]	Loss 0.0012 (0.0018)	
training:	Epoch: [98][8/233]	Loss 0.1113 (0.0155)	
training:	Epoch: [98][9/233]	Loss 0.0013 (0.0139)	
training:	Epoch: [98][10/233]	Loss 0.0009 (0.0126)	
training:	Epoch: [98][11/233]	Loss 0.0009 (0.0116)	
training:	Epoch: [98][12/233]	Loss 0.0015 (0.0107)	
training:	Epoch: [98][13/233]	Loss 0.0008 (0.0100)	
training:	Epoch: [98][14/233]	Loss 0.0008 (0.0093)	
training:	Epoch: [98][15/233]	Loss 0.0686 (0.0133)	
training:	Epoch: [98][16/233]	Loss 0.0014 (0.0125)	
training:	Epoch: [98][17/233]	Loss 0.0008 (0.0118)	
training:	Epoch: [98][18/233]	Loss 0.0014 (0.0113)	
training:	Epoch: [98][19/233]	Loss 0.0015 (0.0107)	
training:	Epoch: [98][20/233]	Loss 0.0042 (0.0104)	
training:	Epoch: [98][21/233]	Loss 0.0009 (0.0100)	
training:	Epoch: [98][22/233]	Loss 0.0010 (0.0096)	
training:	Epoch: [98][23/233]	Loss 0.0011 (0.0092)	
training:	Epoch: [98][24/233]	Loss 0.0010 (0.0088)	
training:	Epoch: [98][25/233]	Loss 0.1040 (0.0127)	
training:	Epoch: [98][26/233]	Loss 0.0009 (0.0122)	
training:	Epoch: [98][27/233]	Loss 0.0009 (0.0118)	
training:	Epoch: [98][28/233]	Loss 0.0007 (0.0114)	
training:	Epoch: [98][29/233]	Loss 0.0355 (0.0122)	
training:	Epoch: [98][30/233]	Loss 0.0011 (0.0118)	
training:	Epoch: [98][31/233]	Loss 0.0012 (0.0115)	
training:	Epoch: [98][32/233]	Loss 0.0010 (0.0112)	
training:	Epoch: [98][33/233]	Loss 0.0067 (0.0110)	
training:	Epoch: [98][34/233]	Loss 0.0010 (0.0107)	
training:	Epoch: [98][35/233]	Loss 0.0014 (0.0105)	
training:	Epoch: [98][36/233]	Loss 0.0012 (0.0102)	
training:	Epoch: [98][37/233]	Loss 0.0019 (0.0100)	
training:	Epoch: [98][38/233]	Loss 0.0734 (0.0117)	
training:	Epoch: [98][39/233]	Loss 0.0015 (0.0114)	
training:	Epoch: [98][40/233]	Loss 0.0011 (0.0111)	
training:	Epoch: [98][41/233]	Loss 0.0012 (0.0109)	
training:	Epoch: [98][42/233]	Loss 0.0023 (0.0107)	
training:	Epoch: [98][43/233]	Loss 0.0009 (0.0105)	
training:	Epoch: [98][44/233]	Loss 0.0012 (0.0103)	
training:	Epoch: [98][45/233]	Loss 0.0009 (0.0101)	
training:	Epoch: [98][46/233]	Loss 0.0010 (0.0099)	
training:	Epoch: [98][47/233]	Loss 0.0009 (0.0097)	
training:	Epoch: [98][48/233]	Loss 0.0019 (0.0095)	
training:	Epoch: [98][49/233]	Loss 0.0022 (0.0094)	
training:	Epoch: [98][50/233]	Loss 0.0011 (0.0092)	
training:	Epoch: [98][51/233]	Loss 0.0027 (0.0091)	
training:	Epoch: [98][52/233]	Loss 0.0026 (0.0089)	
training:	Epoch: [98][53/233]	Loss 0.0009 (0.0088)	
training:	Epoch: [98][54/233]	Loss 0.0009 (0.0086)	
training:	Epoch: [98][55/233]	Loss 0.0041 (0.0086)	
training:	Epoch: [98][56/233]	Loss 0.0010 (0.0084)	
training:	Epoch: [98][57/233]	Loss 0.0010 (0.0083)	
training:	Epoch: [98][58/233]	Loss 0.0010 (0.0082)	
training:	Epoch: [98][59/233]	Loss 0.0020 (0.0081)	
training:	Epoch: [98][60/233]	Loss 0.2297 (0.0118)	
training:	Epoch: [98][61/233]	Loss 0.0018 (0.0116)	
training:	Epoch: [98][62/233]	Loss 0.0018 (0.0114)	
training:	Epoch: [98][63/233]	Loss 0.0201 (0.0116)	
training:	Epoch: [98][64/233]	Loss 0.0008 (0.0114)	
training:	Epoch: [98][65/233]	Loss 0.0009 (0.0112)	
training:	Epoch: [98][66/233]	Loss 0.0041 (0.0111)	
training:	Epoch: [98][67/233]	Loss 0.0011 (0.0110)	
training:	Epoch: [98][68/233]	Loss 0.0092 (0.0110)	
training:	Epoch: [98][69/233]	Loss 0.0023 (0.0108)	
training:	Epoch: [98][70/233]	Loss 0.0010 (0.0107)	
training:	Epoch: [98][71/233]	Loss 0.0010 (0.0106)	
training:	Epoch: [98][72/233]	Loss 0.2030 (0.0132)	
training:	Epoch: [98][73/233]	Loss 0.0011 (0.0131)	
training:	Epoch: [98][74/233]	Loss 0.0015 (0.0129)	
training:	Epoch: [98][75/233]	Loss 0.0011 (0.0128)	
training:	Epoch: [98][76/233]	Loss 0.0039 (0.0126)	
training:	Epoch: [98][77/233]	Loss 0.0009 (0.0125)	
training:	Epoch: [98][78/233]	Loss 0.0013 (0.0123)	
training:	Epoch: [98][79/233]	Loss 0.0011 (0.0122)	
training:	Epoch: [98][80/233]	Loss 0.0019 (0.0121)	
training:	Epoch: [98][81/233]	Loss 0.0019 (0.0119)	
training:	Epoch: [98][82/233]	Loss 0.0010 (0.0118)	
training:	Epoch: [98][83/233]	Loss 0.0355 (0.0121)	
training:	Epoch: [98][84/233]	Loss 0.0008 (0.0120)	
training:	Epoch: [98][85/233]	Loss 0.0008 (0.0118)	
training:	Epoch: [98][86/233]	Loss 0.0012 (0.0117)	
training:	Epoch: [98][87/233]	Loss 0.1579 (0.0134)	
training:	Epoch: [98][88/233]	Loss 0.0010 (0.0132)	
training:	Epoch: [98][89/233]	Loss 0.0009 (0.0131)	
training:	Epoch: [98][90/233]	Loss 0.0012 (0.0130)	
training:	Epoch: [98][91/233]	Loss 0.0014 (0.0128)	
training:	Epoch: [98][92/233]	Loss 0.0011 (0.0127)	
training:	Epoch: [98][93/233]	Loss 0.0047 (0.0126)	
training:	Epoch: [98][94/233]	Loss 0.0011 (0.0125)	
training:	Epoch: [98][95/233]	Loss 0.0234 (0.0126)	
training:	Epoch: [98][96/233]	Loss 0.0057 (0.0126)	
training:	Epoch: [98][97/233]	Loss 0.0011 (0.0124)	
training:	Epoch: [98][98/233]	Loss 0.0009 (0.0123)	
training:	Epoch: [98][99/233]	Loss 0.0011 (0.0122)	
training:	Epoch: [98][100/233]	Loss 0.0012 (0.0121)	
training:	Epoch: [98][101/233]	Loss 0.0046 (0.0120)	
training:	Epoch: [98][102/233]	Loss 0.0008 (0.0119)	
training:	Epoch: [98][103/233]	Loss 0.0013 (0.0118)	
training:	Epoch: [98][104/233]	Loss 0.0275 (0.0120)	
training:	Epoch: [98][105/233]	Loss 0.0009 (0.0119)	
training:	Epoch: [98][106/233]	Loss 0.0014 (0.0118)	
training:	Epoch: [98][107/233]	Loss 0.0010 (0.0117)	
training:	Epoch: [98][108/233]	Loss 0.0008 (0.0116)	
training:	Epoch: [98][109/233]	Loss 0.0243 (0.0117)	
training:	Epoch: [98][110/233]	Loss 0.0012 (0.0116)	
training:	Epoch: [98][111/233]	Loss 0.0011 (0.0115)	
training:	Epoch: [98][112/233]	Loss 0.0017 (0.0114)	
training:	Epoch: [98][113/233]	Loss 0.0010 (0.0113)	
training:	Epoch: [98][114/233]	Loss 0.0017 (0.0112)	
training:	Epoch: [98][115/233]	Loss 0.0009 (0.0111)	
training:	Epoch: [98][116/233]	Loss 0.0010 (0.0110)	
training:	Epoch: [98][117/233]	Loss 0.0019 (0.0110)	
training:	Epoch: [98][118/233]	Loss 0.0009 (0.0109)	
training:	Epoch: [98][119/233]	Loss 0.0055 (0.0108)	
training:	Epoch: [98][120/233]	Loss 0.0077 (0.0108)	
training:	Epoch: [98][121/233]	Loss 0.0016 (0.0107)	
training:	Epoch: [98][122/233]	Loss 0.0010 (0.0107)	
training:	Epoch: [98][123/233]	Loss 0.0024 (0.0106)	
training:	Epoch: [98][124/233]	Loss 0.0018 (0.0105)	
training:	Epoch: [98][125/233]	Loss 0.0010 (0.0104)	
training:	Epoch: [98][126/233]	Loss 0.0008 (0.0104)	
training:	Epoch: [98][127/233]	Loss 0.0008 (0.0103)	
training:	Epoch: [98][128/233]	Loss 0.0012 (0.0102)	
training:	Epoch: [98][129/233]	Loss 0.0015 (0.0101)	
training:	Epoch: [98][130/233]	Loss 0.0012 (0.0101)	
training:	Epoch: [98][131/233]	Loss 0.0117 (0.0101)	
training:	Epoch: [98][132/233]	Loss 0.0021 (0.0100)	
training:	Epoch: [98][133/233]	Loss 0.1345 (0.0110)	
training:	Epoch: [98][134/233]	Loss 0.0008 (0.0109)	
training:	Epoch: [98][135/233]	Loss 0.0035 (0.0108)	
training:	Epoch: [98][136/233]	Loss 0.0041 (0.0108)	
training:	Epoch: [98][137/233]	Loss 0.0517 (0.0111)	
training:	Epoch: [98][138/233]	Loss 0.0108 (0.0111)	
training:	Epoch: [98][139/233]	Loss 0.0013 (0.0110)	
training:	Epoch: [98][140/233]	Loss 0.0013 (0.0109)	
training:	Epoch: [98][141/233]	Loss 0.0015 (0.0109)	
training:	Epoch: [98][142/233]	Loss 0.0010 (0.0108)	
training:	Epoch: [98][143/233]	Loss 0.0008 (0.0107)	
training:	Epoch: [98][144/233]	Loss 0.0008 (0.0107)	
training:	Epoch: [98][145/233]	Loss 0.0010 (0.0106)	
training:	Epoch: [98][146/233]	Loss 0.0008 (0.0105)	
training:	Epoch: [98][147/233]	Loss 0.0010 (0.0105)	
training:	Epoch: [98][148/233]	Loss 0.0014 (0.0104)	
training:	Epoch: [98][149/233]	Loss 0.0015 (0.0103)	
training:	Epoch: [98][150/233]	Loss 0.0062 (0.0103)	
training:	Epoch: [98][151/233]	Loss 0.0009 (0.0103)	
training:	Epoch: [98][152/233]	Loss 0.0009 (0.0102)	
training:	Epoch: [98][153/233]	Loss 0.1082 (0.0108)	
training:	Epoch: [98][154/233]	Loss 0.1068 (0.0115)	
training:	Epoch: [98][155/233]	Loss 0.0990 (0.0120)	
training:	Epoch: [98][156/233]	Loss 0.0009 (0.0120)	
training:	Epoch: [98][157/233]	Loss 0.0032 (0.0119)	
training:	Epoch: [98][158/233]	Loss 0.0052 (0.0119)	
training:	Epoch: [98][159/233]	Loss 0.0093 (0.0118)	
training:	Epoch: [98][160/233]	Loss 0.0009 (0.0118)	
training:	Epoch: [98][161/233]	Loss 0.0009 (0.0117)	
training:	Epoch: [98][162/233]	Loss 0.0011 (0.0116)	
training:	Epoch: [98][163/233]	Loss 0.0042 (0.0116)	
training:	Epoch: [98][164/233]	Loss 0.0012 (0.0115)	
training:	Epoch: [98][165/233]	Loss 0.0009 (0.0115)	
training:	Epoch: [98][166/233]	Loss 0.0219 (0.0115)	
training:	Epoch: [98][167/233]	Loss 0.0048 (0.0115)	
training:	Epoch: [98][168/233]	Loss 0.1322 (0.0122)	
training:	Epoch: [98][169/233]	Loss 0.0008 (0.0121)	
training:	Epoch: [98][170/233]	Loss 0.0018 (0.0121)	
training:	Epoch: [98][171/233]	Loss 0.0009 (0.0120)	
training:	Epoch: [98][172/233]	Loss 0.0016 (0.0120)	
training:	Epoch: [98][173/233]	Loss 0.0008 (0.0119)	
training:	Epoch: [98][174/233]	Loss 0.0008 (0.0118)	
training:	Epoch: [98][175/233]	Loss 0.0010 (0.0118)	
training:	Epoch: [98][176/233]	Loss 0.0009 (0.0117)	
training:	Epoch: [98][177/233]	Loss 0.0012 (0.0116)	
training:	Epoch: [98][178/233]	Loss 0.0011 (0.0116)	
training:	Epoch: [98][179/233]	Loss 0.0010 (0.0115)	
training:	Epoch: [98][180/233]	Loss 0.0014 (0.0115)	
training:	Epoch: [98][181/233]	Loss 0.0024 (0.0114)	
training:	Epoch: [98][182/233]	Loss 0.0010 (0.0114)	
training:	Epoch: [98][183/233]	Loss 0.0010 (0.0113)	
training:	Epoch: [98][184/233]	Loss 0.0010 (0.0112)	
training:	Epoch: [98][185/233]	Loss 0.2145 (0.0123)	
training:	Epoch: [98][186/233]	Loss 0.0010 (0.0123)	
training:	Epoch: [98][187/233]	Loss 0.0081 (0.0123)	
training:	Epoch: [98][188/233]	Loss 0.0008 (0.0122)	
training:	Epoch: [98][189/233]	Loss 0.0011 (0.0121)	
training:	Epoch: [98][190/233]	Loss 0.0021 (0.0121)	
training:	Epoch: [98][191/233]	Loss 0.0222 (0.0121)	
training:	Epoch: [98][192/233]	Loss 0.0008 (0.0121)	
training:	Epoch: [98][193/233]	Loss 0.0009 (0.0120)	
training:	Epoch: [98][194/233]	Loss 0.0010 (0.0120)	
training:	Epoch: [98][195/233]	Loss 0.0014 (0.0119)	
training:	Epoch: [98][196/233]	Loss 0.0009 (0.0119)	
training:	Epoch: [98][197/233]	Loss 0.0064 (0.0118)	
training:	Epoch: [98][198/233]	Loss 0.0009 (0.0118)	
training:	Epoch: [98][199/233]	Loss 0.0011 (0.0117)	
training:	Epoch: [98][200/233]	Loss 0.0009 (0.0117)	
training:	Epoch: [98][201/233]	Loss 0.0009 (0.0116)	
training:	Epoch: [98][202/233]	Loss 0.0038 (0.0116)	
training:	Epoch: [98][203/233]	Loss 0.0019 (0.0115)	
training:	Epoch: [98][204/233]	Loss 0.0012 (0.0115)	
training:	Epoch: [98][205/233]	Loss 0.0009 (0.0114)	
training:	Epoch: [98][206/233]	Loss 0.0018 (0.0114)	
training:	Epoch: [98][207/233]	Loss 0.0011 (0.0113)	
training:	Epoch: [98][208/233]	Loss 0.0011 (0.0113)	
training:	Epoch: [98][209/233]	Loss 0.0008 (0.0112)	
training:	Epoch: [98][210/233]	Loss 0.0011 (0.0112)	
training:	Epoch: [98][211/233]	Loss 0.0011 (0.0111)	
training:	Epoch: [98][212/233]	Loss 0.0008 (0.0111)	
training:	Epoch: [98][213/233]	Loss 0.0016 (0.0110)	
training:	Epoch: [98][214/233]	Loss 0.0012 (0.0110)	
training:	Epoch: [98][215/233]	Loss 0.0009 (0.0109)	
training:	Epoch: [98][216/233]	Loss 0.0010 (0.0109)	
training:	Epoch: [98][217/233]	Loss 0.0024 (0.0109)	
training:	Epoch: [98][218/233]	Loss 0.0008 (0.0108)	
training:	Epoch: [98][219/233]	Loss 0.0009 (0.0108)	
training:	Epoch: [98][220/233]	Loss 0.0008 (0.0107)	
training:	Epoch: [98][221/233]	Loss 0.0009 (0.0107)	
training:	Epoch: [98][222/233]	Loss 0.0008 (0.0106)	
training:	Epoch: [98][223/233]	Loss 0.0041 (0.0106)	
training:	Epoch: [98][224/233]	Loss 0.0010 (0.0106)	
training:	Epoch: [98][225/233]	Loss 0.0009 (0.0105)	
training:	Epoch: [98][226/233]	Loss 0.0879 (0.0109)	
training:	Epoch: [98][227/233]	Loss 0.0010 (0.0108)	
training:	Epoch: [98][228/233]	Loss 0.0024 (0.0108)	
training:	Epoch: [98][229/233]	Loss 0.0009 (0.0107)	
training:	Epoch: [98][230/233]	Loss 0.1028 (0.0111)	
training:	Epoch: [98][231/233]	Loss 0.0025 (0.0111)	
training:	Epoch: [98][232/233]	Loss 0.0125 (0.0111)	
training:	Epoch: [98][233/233]	Loss 0.0011 (0.0111)	
Training:	 Loss: 0.0110

Training:	 ACC: 0.9997 0.9997 0.9997 0.9997
Validation:	 ACC: 0.7975 0.7983 0.8141 0.7809
Validation:	 Best_BACC: 0.8162 0.8154 0.7998 0.8326
Validation:	 Loss: 1.1246
Pretraining:	Epoch 99/200
----------
training:	Epoch: [99][1/233]	Loss 0.0009 (0.0009)	
training:	Epoch: [99][2/233]	Loss 0.0049 (0.0029)	
training:	Epoch: [99][3/233]	Loss 0.0009 (0.0022)	
training:	Epoch: [99][4/233]	Loss 0.0011 (0.0019)	
training:	Epoch: [99][5/233]	Loss 0.0008 (0.0017)	
training:	Epoch: [99][6/233]	Loss 0.0048 (0.0022)	
training:	Epoch: [99][7/233]	Loss 0.0009 (0.0020)	
training:	Epoch: [99][8/233]	Loss 0.0009 (0.0019)	
training:	Epoch: [99][9/233]	Loss 0.0703 (0.0095)	
training:	Epoch: [99][10/233]	Loss 0.0009 (0.0086)	
training:	Epoch: [99][11/233]	Loss 0.0008 (0.0079)	
training:	Epoch: [99][12/233]	Loss 0.0011 (0.0073)	
training:	Epoch: [99][13/233]	Loss 0.0047 (0.0071)	
training:	Epoch: [99][14/233]	Loss 0.0008 (0.0067)	
training:	Epoch: [99][15/233]	Loss 0.0009 (0.0063)	
training:	Epoch: [99][16/233]	Loss 0.0827 (0.0111)	
training:	Epoch: [99][17/233]	Loss 0.0011 (0.0105)	
training:	Epoch: [99][18/233]	Loss 0.0154 (0.0108)	
training:	Epoch: [99][19/233]	Loss 0.0113 (0.0108)	
training:	Epoch: [99][20/233]	Loss 0.0050 (0.0105)	
training:	Epoch: [99][21/233]	Loss 0.0012 (0.0101)	
training:	Epoch: [99][22/233]	Loss 0.0093 (0.0100)	
training:	Epoch: [99][23/233]	Loss 0.0010 (0.0096)	
training:	Epoch: [99][24/233]	Loss 0.0013 (0.0093)	
training:	Epoch: [99][25/233]	Loss 0.1357 (0.0143)	
training:	Epoch: [99][26/233]	Loss 0.0010 (0.0138)	
training:	Epoch: [99][27/233]	Loss 0.0065 (0.0136)	
training:	Epoch: [99][28/233]	Loss 0.0085 (0.0134)	
training:	Epoch: [99][29/233]	Loss 0.0009 (0.0130)	
training:	Epoch: [99][30/233]	Loss 0.0008 (0.0125)	
training:	Epoch: [99][31/233]	Loss 0.0011 (0.0122)	
training:	Epoch: [99][32/233]	Loss 0.1233 (0.0156)	
training:	Epoch: [99][33/233]	Loss 0.0008 (0.0152)	
training:	Epoch: [99][34/233]	Loss 0.0010 (0.0148)	
training:	Epoch: [99][35/233]	Loss 0.0009 (0.0144)	
training:	Epoch: [99][36/233]	Loss 0.0008 (0.0140)	
training:	Epoch: [99][37/233]	Loss 0.0010 (0.0137)	
training:	Epoch: [99][38/233]	Loss 0.0016 (0.0133)	
training:	Epoch: [99][39/233]	Loss 0.0009 (0.0130)	
training:	Epoch: [99][40/233]	Loss 0.0014 (0.0127)	
training:	Epoch: [99][41/233]	Loss 0.0021 (0.0125)	
training:	Epoch: [99][42/233]	Loss 0.1834 (0.0165)	
training:	Epoch: [99][43/233]	Loss 0.0010 (0.0162)	
training:	Epoch: [99][44/233]	Loss 0.0068 (0.0160)	
training:	Epoch: [99][45/233]	Loss 0.0008 (0.0156)	
training:	Epoch: [99][46/233]	Loss 0.0017 (0.0153)	
training:	Epoch: [99][47/233]	Loss 0.0014 (0.0150)	
training:	Epoch: [99][48/233]	Loss 0.0013 (0.0147)	
training:	Epoch: [99][49/233]	Loss 0.0009 (0.0145)	
training:	Epoch: [99][50/233]	Loss 0.0008 (0.0142)	
training:	Epoch: [99][51/233]	Loss 0.0016 (0.0139)	
training:	Epoch: [99][52/233]	Loss 0.2122 (0.0178)	
training:	Epoch: [99][53/233]	Loss 0.0030 (0.0175)	
training:	Epoch: [99][54/233]	Loss 0.0016 (0.0172)	
training:	Epoch: [99][55/233]	Loss 0.0064 (0.0170)	
training:	Epoch: [99][56/233]	Loss 0.0008 (0.0167)	
training:	Epoch: [99][57/233]	Loss 0.0079 (0.0165)	
training:	Epoch: [99][58/233]	Loss 0.0007 (0.0163)	
training:	Epoch: [99][59/233]	Loss 0.0011 (0.0160)	
training:	Epoch: [99][60/233]	Loss 0.0014 (0.0158)	
training:	Epoch: [99][61/233]	Loss 0.0010 (0.0155)	
training:	Epoch: [99][62/233]	Loss 0.0008 (0.0153)	
training:	Epoch: [99][63/233]	Loss 0.0011 (0.0151)	
training:	Epoch: [99][64/233]	Loss 0.0019 (0.0149)	
training:	Epoch: [99][65/233]	Loss 0.0010 (0.0146)	
training:	Epoch: [99][66/233]	Loss 0.0012 (0.0144)	
training:	Epoch: [99][67/233]	Loss 0.0014 (0.0142)	
training:	Epoch: [99][68/233]	Loss 0.0020 (0.0141)	
training:	Epoch: [99][69/233]	Loss 0.0009 (0.0139)	
training:	Epoch: [99][70/233]	Loss 0.0009 (0.0137)	
training:	Epoch: [99][71/233]	Loss 0.0011 (0.0135)	
training:	Epoch: [99][72/233]	Loss 0.0012 (0.0133)	
training:	Epoch: [99][73/233]	Loss 0.0013 (0.0132)	
training:	Epoch: [99][74/233]	Loss 0.0011 (0.0130)	
training:	Epoch: [99][75/233]	Loss 0.2319 (0.0159)	
training:	Epoch: [99][76/233]	Loss 0.0052 (0.0158)	
training:	Epoch: [99][77/233]	Loss 0.0010 (0.0156)	
training:	Epoch: [99][78/233]	Loss 0.0009 (0.0154)	
training:	Epoch: [99][79/233]	Loss 0.0010 (0.0152)	
training:	Epoch: [99][80/233]	Loss 0.0011 (0.0151)	
training:	Epoch: [99][81/233]	Loss 0.0011 (0.0149)	
training:	Epoch: [99][82/233]	Loss 0.0016 (0.0147)	
training:	Epoch: [99][83/233]	Loss 0.0010 (0.0146)	
training:	Epoch: [99][84/233]	Loss 0.0007 (0.0144)	
training:	Epoch: [99][85/233]	Loss 0.0014 (0.0142)	
training:	Epoch: [99][86/233]	Loss 0.0182 (0.0143)	
training:	Epoch: [99][87/233]	Loss 0.0013 (0.0141)	
training:	Epoch: [99][88/233]	Loss 0.0009 (0.0140)	
training:	Epoch: [99][89/233]	Loss 0.0065 (0.0139)	
training:	Epoch: [99][90/233]	Loss 0.0165 (0.0139)	
training:	Epoch: [99][91/233]	Loss 0.0016 (0.0138)	
training:	Epoch: [99][92/233]	Loss 0.0011 (0.0137)	
training:	Epoch: [99][93/233]	Loss 0.0009 (0.0135)	
training:	Epoch: [99][94/233]	Loss 0.0009 (0.0134)	
training:	Epoch: [99][95/233]	Loss 0.0040 (0.0133)	
training:	Epoch: [99][96/233]	Loss 0.0013 (0.0132)	
training:	Epoch: [99][97/233]	Loss 0.0009 (0.0130)	
training:	Epoch: [99][98/233]	Loss 0.0011 (0.0129)	
training:	Epoch: [99][99/233]	Loss 0.0047 (0.0128)	
training:	Epoch: [99][100/233]	Loss 0.0007 (0.0127)	
training:	Epoch: [99][101/233]	Loss 0.0165 (0.0127)	
training:	Epoch: [99][102/233]	Loss 0.0008 (0.0126)	
training:	Epoch: [99][103/233]	Loss 0.0094 (0.0126)	
training:	Epoch: [99][104/233]	Loss 0.0009 (0.0125)	
training:	Epoch: [99][105/233]	Loss 0.0009 (0.0124)	
training:	Epoch: [99][106/233]	Loss 0.0009 (0.0123)	
training:	Epoch: [99][107/233]	Loss 0.0009 (0.0122)	
training:	Epoch: [99][108/233]	Loss 0.0010 (0.0121)	
training:	Epoch: [99][109/233]	Loss 0.0009 (0.0120)	
training:	Epoch: [99][110/233]	Loss 0.0010 (0.0119)	
training:	Epoch: [99][111/233]	Loss 0.0008 (0.0118)	
training:	Epoch: [99][112/233]	Loss 0.0012 (0.0117)	
training:	Epoch: [99][113/233]	Loss 0.0010 (0.0116)	
training:	Epoch: [99][114/233]	Loss 0.0008 (0.0115)	
training:	Epoch: [99][115/233]	Loss 0.0009 (0.0114)	
training:	Epoch: [99][116/233]	Loss 0.0148 (0.0114)	
training:	Epoch: [99][117/233]	Loss 0.0090 (0.0114)	
training:	Epoch: [99][118/233]	Loss 0.0010 (0.0113)	
training:	Epoch: [99][119/233]	Loss 0.0008 (0.0112)	
training:	Epoch: [99][120/233]	Loss 0.0012 (0.0111)	
training:	Epoch: [99][121/233]	Loss 0.0011 (0.0110)	
training:	Epoch: [99][122/233]	Loss 0.0009 (0.0110)	
training:	Epoch: [99][123/233]	Loss 0.0010 (0.0109)	
training:	Epoch: [99][124/233]	Loss 0.0012 (0.0108)	
training:	Epoch: [99][125/233]	Loss 0.0011 (0.0107)	
training:	Epoch: [99][126/233]	Loss 0.0016 (0.0107)	
training:	Epoch: [99][127/233]	Loss 0.0008 (0.0106)	
training:	Epoch: [99][128/233]	Loss 0.0013 (0.0105)	
training:	Epoch: [99][129/233]	Loss 0.0169 (0.0106)	
training:	Epoch: [99][130/233]	Loss 0.0012 (0.0105)	
training:	Epoch: [99][131/233]	Loss 0.0008 (0.0104)	
training:	Epoch: [99][132/233]	Loss 0.0009 (0.0103)	
training:	Epoch: [99][133/233]	Loss 0.0022 (0.0103)	
training:	Epoch: [99][134/233]	Loss 0.0020 (0.0102)	
training:	Epoch: [99][135/233]	Loss 0.0009 (0.0101)	
training:	Epoch: [99][136/233]	Loss 0.0011 (0.0101)	
training:	Epoch: [99][137/233]	Loss 0.0014 (0.0100)	
training:	Epoch: [99][138/233]	Loss 0.0012 (0.0099)	
training:	Epoch: [99][139/233]	Loss 0.0020 (0.0099)	
training:	Epoch: [99][140/233]	Loss 0.0008 (0.0098)	
training:	Epoch: [99][141/233]	Loss 0.0008 (0.0098)	
training:	Epoch: [99][142/233]	Loss 0.0050 (0.0097)	
training:	Epoch: [99][143/233]	Loss 0.0546 (0.0100)	
training:	Epoch: [99][144/233]	Loss 0.0342 (0.0102)	
training:	Epoch: [99][145/233]	Loss 0.0013 (0.0102)	
training:	Epoch: [99][146/233]	Loss 0.0010 (0.0101)	
training:	Epoch: [99][147/233]	Loss 0.0009 (0.0100)	
training:	Epoch: [99][148/233]	Loss 0.0010 (0.0100)	
training:	Epoch: [99][149/233]	Loss 0.0014 (0.0099)	
training:	Epoch: [99][150/233]	Loss 0.0016 (0.0099)	
training:	Epoch: [99][151/233]	Loss 0.0012 (0.0098)	
training:	Epoch: [99][152/233]	Loss 0.0015 (0.0097)	
training:	Epoch: [99][153/233]	Loss 0.0007 (0.0097)	
training:	Epoch: [99][154/233]	Loss 0.0009 (0.0096)	
training:	Epoch: [99][155/233]	Loss 0.0011 (0.0096)	
training:	Epoch: [99][156/233]	Loss 0.0012 (0.0095)	
training:	Epoch: [99][157/233]	Loss 0.0012 (0.0095)	
training:	Epoch: [99][158/233]	Loss 0.0015 (0.0094)	
training:	Epoch: [99][159/233]	Loss 0.0008 (0.0094)	
training:	Epoch: [99][160/233]	Loss 0.0014 (0.0093)	
training:	Epoch: [99][161/233]	Loss 0.0009 (0.0093)	
training:	Epoch: [99][162/233]	Loss 0.0009 (0.0092)	
training:	Epoch: [99][163/233]	Loss 0.0011 (0.0092)	
training:	Epoch: [99][164/233]	Loss 0.0009 (0.0091)	
training:	Epoch: [99][165/233]	Loss 0.0009 (0.0091)	
training:	Epoch: [99][166/233]	Loss 0.0026 (0.0090)	
training:	Epoch: [99][167/233]	Loss 0.0286 (0.0091)	
training:	Epoch: [99][168/233]	Loss 0.0023 (0.0091)	
training:	Epoch: [99][169/233]	Loss 0.0008 (0.0090)	
training:	Epoch: [99][170/233]	Loss 0.0013 (0.0090)	
training:	Epoch: [99][171/233]	Loss 0.0009 (0.0090)	
training:	Epoch: [99][172/233]	Loss 0.0546 (0.0092)	
training:	Epoch: [99][173/233]	Loss 0.0019 (0.0092)	
training:	Epoch: [99][174/233]	Loss 0.0012 (0.0091)	
training:	Epoch: [99][175/233]	Loss 0.0084 (0.0091)	
training:	Epoch: [99][176/233]	Loss 0.0017 (0.0091)	
training:	Epoch: [99][177/233]	Loss 0.0012 (0.0090)	
training:	Epoch: [99][178/233]	Loss 0.0040 (0.0090)	
training:	Epoch: [99][179/233]	Loss 0.0040 (0.0090)	
training:	Epoch: [99][180/233]	Loss 0.0015 (0.0089)	
training:	Epoch: [99][181/233]	Loss 0.0012 (0.0089)	
training:	Epoch: [99][182/233]	Loss 0.0010 (0.0089)	
training:	Epoch: [99][183/233]	Loss 0.0014 (0.0088)	
training:	Epoch: [99][184/233]	Loss 0.0010 (0.0088)	
training:	Epoch: [99][185/233]	Loss 0.0016 (0.0087)	
training:	Epoch: [99][186/233]	Loss 0.0009 (0.0087)	
training:	Epoch: [99][187/233]	Loss 0.0021 (0.0087)	
training:	Epoch: [99][188/233]	Loss 0.0014 (0.0086)	
training:	Epoch: [99][189/233]	Loss 0.0011 (0.0086)	
training:	Epoch: [99][190/233]	Loss 0.0009 (0.0085)	
training:	Epoch: [99][191/233]	Loss 0.0010 (0.0085)	
training:	Epoch: [99][192/233]	Loss 0.0013 (0.0085)	
training:	Epoch: [99][193/233]	Loss 0.0075 (0.0085)	
training:	Epoch: [99][194/233]	Loss 0.0008 (0.0084)	
training:	Epoch: [99][195/233]	Loss 0.0019 (0.0084)	
training:	Epoch: [99][196/233]	Loss 0.0116 (0.0084)	
training:	Epoch: [99][197/233]	Loss 0.1360 (0.0090)	
training:	Epoch: [99][198/233]	Loss 0.0012 (0.0090)	
training:	Epoch: [99][199/233]	Loss 0.0010 (0.0090)	
training:	Epoch: [99][200/233]	Loss 0.0015 (0.0089)	
training:	Epoch: [99][201/233]	Loss 0.0017 (0.0089)	
training:	Epoch: [99][202/233]	Loss 0.0016 (0.0089)	
training:	Epoch: [99][203/233]	Loss 0.0009 (0.0088)	
training:	Epoch: [99][204/233]	Loss 0.0019 (0.0088)	
training:	Epoch: [99][205/233]	Loss 0.0015 (0.0087)	
training:	Epoch: [99][206/233]	Loss 0.0008 (0.0087)	
training:	Epoch: [99][207/233]	Loss 0.0009 (0.0087)	
training:	Epoch: [99][208/233]	Loss 0.0122 (0.0087)	
training:	Epoch: [99][209/233]	Loss 0.0236 (0.0088)	
training:	Epoch: [99][210/233]	Loss 0.0012 (0.0087)	
training:	Epoch: [99][211/233]	Loss 0.0011 (0.0087)	
training:	Epoch: [99][212/233]	Loss 0.0010 (0.0086)	
training:	Epoch: [99][213/233]	Loss 0.0010 (0.0086)	
training:	Epoch: [99][214/233]	Loss 0.0038 (0.0086)	
training:	Epoch: [99][215/233]	Loss 0.0022 (0.0086)	
training:	Epoch: [99][216/233]	Loss 0.0011 (0.0085)	
training:	Epoch: [99][217/233]	Loss 0.0008 (0.0085)	
training:	Epoch: [99][218/233]	Loss 0.0012 (0.0085)	
training:	Epoch: [99][219/233]	Loss 0.0010 (0.0084)	
training:	Epoch: [99][220/233]	Loss 0.0586 (0.0087)	
training:	Epoch: [99][221/233]	Loss 0.0022 (0.0086)	
training:	Epoch: [99][222/233]	Loss 0.0013 (0.0086)	
training:	Epoch: [99][223/233]	Loss 0.0016 (0.0086)	
training:	Epoch: [99][224/233]	Loss 0.0012 (0.0085)	
training:	Epoch: [99][225/233]	Loss 0.0011 (0.0085)	
training:	Epoch: [99][226/233]	Loss 0.0740 (0.0088)	
training:	Epoch: [99][227/233]	Loss 0.0020 (0.0088)	
training:	Epoch: [99][228/233]	Loss 0.0009 (0.0087)	
training:	Epoch: [99][229/233]	Loss 0.0066 (0.0087)	
training:	Epoch: [99][230/233]	Loss 0.0019 (0.0087)	
training:	Epoch: [99][231/233]	Loss 0.0049 (0.0087)	
training:	Epoch: [99][232/233]	Loss 0.0011 (0.0086)	
training:	Epoch: [99][233/233]	Loss 0.0012 (0.0086)	
Training:	 Loss: 0.0086

Training:	 ACC: 0.9996 0.9996 0.9995 0.9997
Validation:	 ACC: 0.7871 0.7860 0.7630 0.8112
Validation:	 Best_BACC: 0.8162 0.8154 0.7998 0.8326
Validation:	 Loss: 1.1447
Pretraining:	Epoch 100/200
----------
training:	Epoch: [100][1/233]	Loss 0.0008 (0.0008)	
training:	Epoch: [100][2/233]	Loss 0.0016 (0.0012)	
training:	Epoch: [100][3/233]	Loss 0.0011 (0.0012)	
training:	Epoch: [100][4/233]	Loss 0.0010 (0.0011)	
training:	Epoch: [100][5/233]	Loss 0.0008 (0.0011)	
training:	Epoch: [100][6/233]	Loss 0.0043 (0.0016)	
training:	Epoch: [100][7/233]	Loss 0.0014 (0.0016)	
training:	Epoch: [100][8/233]	Loss 0.0013 (0.0015)	
training:	Epoch: [100][9/233]	Loss 0.0009 (0.0015)	
training:	Epoch: [100][10/233]	Loss 0.0010 (0.0014)	
training:	Epoch: [100][11/233]	Loss 0.0009 (0.0014)	
training:	Epoch: [100][12/233]	Loss 0.0009 (0.0013)	
training:	Epoch: [100][13/233]	Loss 0.0008 (0.0013)	
training:	Epoch: [100][14/233]	Loss 0.0014 (0.0013)	
training:	Epoch: [100][15/233]	Loss 0.0011 (0.0013)	
training:	Epoch: [100][16/233]	Loss 0.0086 (0.0017)	
training:	Epoch: [100][17/233]	Loss 0.0034 (0.0018)	
training:	Epoch: [100][18/233]	Loss 0.0010 (0.0018)	
training:	Epoch: [100][19/233]	Loss 0.0212 (0.0028)	
training:	Epoch: [100][20/233]	Loss 0.0091 (0.0031)	
training:	Epoch: [100][21/233]	Loss 0.0009 (0.0030)	
training:	Epoch: [100][22/233]	Loss 0.0058 (0.0031)	
training:	Epoch: [100][23/233]	Loss 0.0011 (0.0031)	
training:	Epoch: [100][24/233]	Loss 0.0014 (0.0030)	
training:	Epoch: [100][25/233]	Loss 0.0008 (0.0029)	
training:	Epoch: [100][26/233]	Loss 0.0153 (0.0034)	
training:	Epoch: [100][27/233]	Loss 0.0062 (0.0035)	
training:	Epoch: [100][28/233]	Loss 0.0019 (0.0034)	
training:	Epoch: [100][29/233]	Loss 0.0022 (0.0034)	
training:	Epoch: [100][30/233]	Loss 0.0017 (0.0033)	
training:	Epoch: [100][31/233]	Loss 0.0010 (0.0032)	
training:	Epoch: [100][32/233]	Loss 0.0064 (0.0033)	
training:	Epoch: [100][33/233]	Loss 0.0021 (0.0033)	
training:	Epoch: [100][34/233]	Loss 0.0009 (0.0032)	
training:	Epoch: [100][35/233]	Loss 0.0017 (0.0032)	
training:	Epoch: [100][36/233]	Loss 0.0008 (0.0031)	
training:	Epoch: [100][37/233]	Loss 0.0008 (0.0031)	
training:	Epoch: [100][38/233]	Loss 0.0011 (0.0030)	
training:	Epoch: [100][39/233]	Loss 0.0010 (0.0030)	
training:	Epoch: [100][40/233]	Loss 0.0009 (0.0029)	
training:	Epoch: [100][41/233]	Loss 0.0009 (0.0029)	
training:	Epoch: [100][42/233]	Loss 0.0021 (0.0028)	
training:	Epoch: [100][43/233]	Loss 0.0009 (0.0028)	
training:	Epoch: [100][44/233]	Loss 0.0089 (0.0029)	
training:	Epoch: [100][45/233]	Loss 0.0014 (0.0029)	
training:	Epoch: [100][46/233]	Loss 0.0032 (0.0029)	
training:	Epoch: [100][47/233]	Loss 0.0008 (0.0029)	
training:	Epoch: [100][48/233]	Loss 0.0009 (0.0028)	
training:	Epoch: [100][49/233]	Loss 0.0009 (0.0028)	
training:	Epoch: [100][50/233]	Loss 0.0515 (0.0038)	
training:	Epoch: [100][51/233]	Loss 0.0013 (0.0037)	
training:	Epoch: [100][52/233]	Loss 0.0154 (0.0039)	
training:	Epoch: [100][53/233]	Loss 0.1466 (0.0066)	
training:	Epoch: [100][54/233]	Loss 0.0012 (0.0065)	
training:	Epoch: [100][55/233]	Loss 0.0010 (0.0064)	
training:	Epoch: [100][56/233]	Loss 0.0012 (0.0063)	
training:	Epoch: [100][57/233]	Loss 0.0008 (0.0062)	
training:	Epoch: [100][58/233]	Loss 0.0072 (0.0063)	
training:	Epoch: [100][59/233]	Loss 0.0011 (0.0062)	
training:	Epoch: [100][60/233]	Loss 0.0024 (0.0061)	
training:	Epoch: [100][61/233]	Loss 0.0008 (0.0060)	
training:	Epoch: [100][62/233]	Loss 0.0008 (0.0059)	
training:	Epoch: [100][63/233]	Loss 0.0018 (0.0059)	
training:	Epoch: [100][64/233]	Loss 0.0014 (0.0058)	
training:	Epoch: [100][65/233]	Loss 0.0013 (0.0057)	
training:	Epoch: [100][66/233]	Loss 0.0011 (0.0057)	
training:	Epoch: [100][67/233]	Loss 0.0015 (0.0056)	
training:	Epoch: [100][68/233]	Loss 0.0595 (0.0064)	
training:	Epoch: [100][69/233]	Loss 0.0010 (0.0063)	
training:	Epoch: [100][70/233]	Loss 0.0044 (0.0063)	
training:	Epoch: [100][71/233]	Loss 0.0017 (0.0062)	
training:	Epoch: [100][72/233]	Loss 0.0047 (0.0062)	
training:	Epoch: [100][73/233]	Loss 0.0010 (0.0061)	
training:	Epoch: [100][74/233]	Loss 0.0007 (0.0061)	
training:	Epoch: [100][75/233]	Loss 0.0577 (0.0067)	
training:	Epoch: [100][76/233]	Loss 0.0009 (0.0067)	
training:	Epoch: [100][77/233]	Loss 0.0010 (0.0066)	
training:	Epoch: [100][78/233]	Loss 0.0009 (0.0065)	
training:	Epoch: [100][79/233]	Loss 0.0010 (0.0064)	
training:	Epoch: [100][80/233]	Loss 0.0009 (0.0064)	
training:	Epoch: [100][81/233]	Loss 0.0010 (0.0063)	
training:	Epoch: [100][82/233]	Loss 0.0011 (0.0062)	
training:	Epoch: [100][83/233]	Loss 0.0011 (0.0062)	
training:	Epoch: [100][84/233]	Loss 0.0458 (0.0067)	
training:	Epoch: [100][85/233]	Loss 0.0009 (0.0066)	
training:	Epoch: [100][86/233]	Loss 0.0011 (0.0065)	
training:	Epoch: [100][87/233]	Loss 0.0011 (0.0065)	
training:	Epoch: [100][88/233]	Loss 0.0010 (0.0064)	
training:	Epoch: [100][89/233]	Loss 0.0016 (0.0063)	
training:	Epoch: [100][90/233]	Loss 0.0012 (0.0063)	
training:	Epoch: [100][91/233]	Loss 0.0010 (0.0062)	
training:	Epoch: [100][92/233]	Loss 0.0010 (0.0062)	
training:	Epoch: [100][93/233]	Loss 0.0094 (0.0062)	
training:	Epoch: [100][94/233]	Loss 0.0285 (0.0064)	
training:	Epoch: [100][95/233]	Loss 0.0010 (0.0064)	
training:	Epoch: [100][96/233]	Loss 0.0025 (0.0063)	
training:	Epoch: [100][97/233]	Loss 0.0428 (0.0067)	
training:	Epoch: [100][98/233]	Loss 0.0015 (0.0067)	
training:	Epoch: [100][99/233]	Loss 0.0022 (0.0066)	
training:	Epoch: [100][100/233]	Loss 0.0016 (0.0066)	
training:	Epoch: [100][101/233]	Loss 0.0011 (0.0065)	
training:	Epoch: [100][102/233]	Loss 0.0008 (0.0065)	
training:	Epoch: [100][103/233]	Loss 0.0039 (0.0064)	
training:	Epoch: [100][104/233]	Loss 0.0035 (0.0064)	
training:	Epoch: [100][105/233]	Loss 0.0007 (0.0064)	
training:	Epoch: [100][106/233]	Loss 0.0506 (0.0068)	
training:	Epoch: [100][107/233]	Loss 0.0009 (0.0067)	
training:	Epoch: [100][108/233]	Loss 0.0007 (0.0067)	
training:	Epoch: [100][109/233]	Loss 0.0010 (0.0066)	
training:	Epoch: [100][110/233]	Loss 0.0250 (0.0068)	
training:	Epoch: [100][111/233]	Loss 0.0010 (0.0067)	
training:	Epoch: [100][112/233]	Loss 0.0184 (0.0068)	
training:	Epoch: [100][113/233]	Loss 0.0012 (0.0068)	
training:	Epoch: [100][114/233]	Loss 0.0103 (0.0068)	
training:	Epoch: [100][115/233]	Loss 0.0045 (0.0068)	
training:	Epoch: [100][116/233]	Loss 0.0014 (0.0067)	
training:	Epoch: [100][117/233]	Loss 0.0079 (0.0068)	
training:	Epoch: [100][118/233]	Loss 0.0009 (0.0067)	
training:	Epoch: [100][119/233]	Loss 0.0017 (0.0067)	
training:	Epoch: [100][120/233]	Loss 0.0010 (0.0066)	
training:	Epoch: [100][121/233]	Loss 0.0009 (0.0066)	
training:	Epoch: [100][122/233]	Loss 0.0027 (0.0065)	
training:	Epoch: [100][123/233]	Loss 0.0014 (0.0065)	
training:	Epoch: [100][124/233]	Loss 0.0013 (0.0065)	
training:	Epoch: [100][125/233]	Loss 0.0010 (0.0064)	
training:	Epoch: [100][126/233]	Loss 0.0009 (0.0064)	
training:	Epoch: [100][127/233]	Loss 0.0911 (0.0070)	
training:	Epoch: [100][128/233]	Loss 0.0027 (0.0070)	
training:	Epoch: [100][129/233]	Loss 0.0123 (0.0070)	
training:	Epoch: [100][130/233]	Loss 0.0010 (0.0070)	
training:	Epoch: [100][131/233]	Loss 0.0010 (0.0069)	
training:	Epoch: [100][132/233]	Loss 0.0010 (0.0069)	
training:	Epoch: [100][133/233]	Loss 0.0019 (0.0069)	
training:	Epoch: [100][134/233]	Loss 0.0009 (0.0068)	
training:	Epoch: [100][135/233]	Loss 0.0010 (0.0068)	
training:	Epoch: [100][136/233]	Loss 0.0012 (0.0067)	
training:	Epoch: [100][137/233]	Loss 0.0010 (0.0067)	
training:	Epoch: [100][138/233]	Loss 0.0063 (0.0067)	
training:	Epoch: [100][139/233]	Loss 0.0010 (0.0067)	
training:	Epoch: [100][140/233]	Loss 0.0010 (0.0066)	
training:	Epoch: [100][141/233]	Loss 0.0690 (0.0071)	
training:	Epoch: [100][142/233]	Loss 0.0014 (0.0070)	
training:	Epoch: [100][143/233]	Loss 0.0009 (0.0070)	
training:	Epoch: [100][144/233]	Loss 0.0012 (0.0069)	
training:	Epoch: [100][145/233]	Loss 0.0020 (0.0069)	
training:	Epoch: [100][146/233]	Loss 0.0027 (0.0069)	
training:	Epoch: [100][147/233]	Loss 0.2513 (0.0085)	
training:	Epoch: [100][148/233]	Loss 0.0008 (0.0085)	
training:	Epoch: [100][149/233]	Loss 0.0011 (0.0084)	
training:	Epoch: [100][150/233]	Loss 0.0008 (0.0084)	
training:	Epoch: [100][151/233]	Loss 0.0010 (0.0083)	
training:	Epoch: [100][152/233]	Loss 0.0010 (0.0083)	
training:	Epoch: [100][153/233]	Loss 0.0009 (0.0082)	
training:	Epoch: [100][154/233]	Loss 0.0009 (0.0082)	
training:	Epoch: [100][155/233]	Loss 0.0266 (0.0083)	
training:	Epoch: [100][156/233]	Loss 0.0014 (0.0083)	
training:	Epoch: [100][157/233]	Loss 0.0009 (0.0082)	
training:	Epoch: [100][158/233]	Loss 0.0010 (0.0082)	
training:	Epoch: [100][159/233]	Loss 0.0011 (0.0081)	
training:	Epoch: [100][160/233]	Loss 0.0009 (0.0081)	
training:	Epoch: [100][161/233]	Loss 0.0344 (0.0082)	
training:	Epoch: [100][162/233]	Loss 0.0010 (0.0082)	
training:	Epoch: [100][163/233]	Loss 0.0018 (0.0082)	
training:	Epoch: [100][164/233]	Loss 0.0009 (0.0081)	
training:	Epoch: [100][165/233]	Loss 0.0068 (0.0081)	
training:	Epoch: [100][166/233]	Loss 0.0020 (0.0081)	
training:	Epoch: [100][167/233]	Loss 0.0012 (0.0080)	
training:	Epoch: [100][168/233]	Loss 0.0009 (0.0080)	
training:	Epoch: [100][169/233]	Loss 0.0011 (0.0079)	
training:	Epoch: [100][170/233]	Loss 0.0015 (0.0079)	
training:	Epoch: [100][171/233]	Loss 0.0010 (0.0079)	
training:	Epoch: [100][172/233]	Loss 0.0018 (0.0078)	
training:	Epoch: [100][173/233]	Loss 0.0018 (0.0078)	
training:	Epoch: [100][174/233]	Loss 0.0011 (0.0078)	
training:	Epoch: [100][175/233]	Loss 0.0015 (0.0077)	
training:	Epoch: [100][176/233]	Loss 0.0024 (0.0077)	
training:	Epoch: [100][177/233]	Loss 0.0013 (0.0077)	
training:	Epoch: [100][178/233]	Loss 0.0597 (0.0079)	
training:	Epoch: [100][179/233]	Loss 0.0020 (0.0079)	
training:	Epoch: [100][180/233]	Loss 0.0018 (0.0079)	
training:	Epoch: [100][181/233]	Loss 0.0010 (0.0078)	
training:	Epoch: [100][182/233]	Loss 0.0010 (0.0078)	
training:	Epoch: [100][183/233]	Loss 0.0570 (0.0081)	
training:	Epoch: [100][184/233]	Loss 0.0297 (0.0082)	
training:	Epoch: [100][185/233]	Loss 0.0059 (0.0082)	
training:	Epoch: [100][186/233]	Loss 0.0013 (0.0081)	
training:	Epoch: [100][187/233]	Loss 0.0134 (0.0082)	
training:	Epoch: [100][188/233]	Loss 0.0054 (0.0082)	
training:	Epoch: [100][189/233]	Loss 0.2148 (0.0092)	
training:	Epoch: [100][190/233]	Loss 0.0010 (0.0092)	
training:	Epoch: [100][191/233]	Loss 0.1931 (0.0102)	
training:	Epoch: [100][192/233]	Loss 0.0074 (0.0102)	
training:	Epoch: [100][193/233]	Loss 0.0009 (0.0101)	
training:	Epoch: [100][194/233]	Loss 0.0023 (0.0101)	
training:	Epoch: [100][195/233]	Loss 0.0146 (0.0101)	
training:	Epoch: [100][196/233]	Loss 0.0013 (0.0100)	
training:	Epoch: [100][197/233]	Loss 0.0010 (0.0100)	
training:	Epoch: [100][198/233]	Loss 0.0024 (0.0100)	
training:	Epoch: [100][199/233]	Loss 0.0035 (0.0099)	
training:	Epoch: [100][200/233]	Loss 0.0061 (0.0099)	
training:	Epoch: [100][201/233]	Loss 0.0018 (0.0099)	
training:	Epoch: [100][202/233]	Loss 0.0008 (0.0098)	
training:	Epoch: [100][203/233]	Loss 0.0010 (0.0098)	
training:	Epoch: [100][204/233]	Loss 0.0015 (0.0097)	
training:	Epoch: [100][205/233]	Loss 0.0029 (0.0097)	
training:	Epoch: [100][206/233]	Loss 0.0118 (0.0097)	
training:	Epoch: [100][207/233]	Loss 0.0010 (0.0097)	
training:	Epoch: [100][208/233]	Loss 0.0010 (0.0096)	
training:	Epoch: [100][209/233]	Loss 0.0008 (0.0096)	
training:	Epoch: [100][210/233]	Loss 0.0011 (0.0095)	
training:	Epoch: [100][211/233]	Loss 0.0009 (0.0095)	
training:	Epoch: [100][212/233]	Loss 0.0020 (0.0095)	
training:	Epoch: [100][213/233]	Loss 0.0008 (0.0094)	
training:	Epoch: [100][214/233]	Loss 0.0010 (0.0094)	
training:	Epoch: [100][215/233]	Loss 0.0033 (0.0094)	
training:	Epoch: [100][216/233]	Loss 0.0010 (0.0093)	
training:	Epoch: [100][217/233]	Loss 0.0009 (0.0093)	
training:	Epoch: [100][218/233]	Loss 0.0013 (0.0093)	
training:	Epoch: [100][219/233]	Loss 0.0068 (0.0092)	
training:	Epoch: [100][220/233]	Loss 0.0011 (0.0092)	
training:	Epoch: [100][221/233]	Loss 0.0010 (0.0092)	
training:	Epoch: [100][222/233]	Loss 0.0013 (0.0091)	
training:	Epoch: [100][223/233]	Loss 0.0010 (0.0091)	
training:	Epoch: [100][224/233]	Loss 0.0009 (0.0091)	
training:	Epoch: [100][225/233]	Loss 0.0008 (0.0090)	
training:	Epoch: [100][226/233]	Loss 0.0010 (0.0090)	
training:	Epoch: [100][227/233]	Loss 0.0009 (0.0089)	
training:	Epoch: [100][228/233]	Loss 0.0041 (0.0089)	
training:	Epoch: [100][229/233]	Loss 0.0007 (0.0089)	
training:	Epoch: [100][230/233]	Loss 0.0011 (0.0089)	
training:	Epoch: [100][231/233]	Loss 0.0020 (0.0088)	
training:	Epoch: [100][232/233]	Loss 0.0008 (0.0088)	
training:	Epoch: [100][233/233]	Loss 0.0010 (0.0088)	
Training:	 Loss: 0.0087

Training:	 ACC: 0.9997 0.9997 0.9997 0.9997
Validation:	 ACC: 0.7899 0.7892 0.7753 0.8045
Validation:	 Best_BACC: 0.8162 0.8154 0.7998 0.8326
Validation:	 Loss: 1.1424
Pretraining:	Epoch 101/200
----------
training:	Epoch: [101][1/233]	Loss 0.0008 (0.0008)	
training:	Epoch: [101][2/233]	Loss 0.0015 (0.0011)	
training:	Epoch: [101][3/233]	Loss 0.0008 (0.0010)	
training:	Epoch: [101][4/233]	Loss 0.0014 (0.0011)	
training:	Epoch: [101][5/233]	Loss 0.0018 (0.0012)	
training:	Epoch: [101][6/233]	Loss 0.0008 (0.0012)	
training:	Epoch: [101][7/233]	Loss 0.0008 (0.0011)	
training:	Epoch: [101][8/233]	Loss 0.0009 (0.0011)	
training:	Epoch: [101][9/233]	Loss 0.0012 (0.0011)	
training:	Epoch: [101][10/233]	Loss 0.0012 (0.0011)	
training:	Epoch: [101][11/233]	Loss 0.0009 (0.0011)	
training:	Epoch: [101][12/233]	Loss 0.0009 (0.0011)	
training:	Epoch: [101][13/233]	Loss 0.0015 (0.0011)	
training:	Epoch: [101][14/233]	Loss 0.0014 (0.0011)	
training:	Epoch: [101][15/233]	Loss 0.0008 (0.0011)	
training:	Epoch: [101][16/233]	Loss 0.0008 (0.0011)	
training:	Epoch: [101][17/233]	Loss 0.0008 (0.0011)	
training:	Epoch: [101][18/233]	Loss 0.0010 (0.0011)	
training:	Epoch: [101][19/233]	Loss 0.0093 (0.0015)	
training:	Epoch: [101][20/233]	Loss 0.0015 (0.0015)	
training:	Epoch: [101][21/233]	Loss 0.0008 (0.0015)	
training:	Epoch: [101][22/233]	Loss 0.0090 (0.0018)	
training:	Epoch: [101][23/233]	Loss 0.0046 (0.0019)	
training:	Epoch: [101][24/233]	Loss 0.0011 (0.0019)	
training:	Epoch: [101][25/233]	Loss 0.0026 (0.0019)	
training:	Epoch: [101][26/233]	Loss 0.0796 (0.0049)	
training:	Epoch: [101][27/233]	Loss 0.0010 (0.0048)	
training:	Epoch: [101][28/233]	Loss 0.0016 (0.0046)	
training:	Epoch: [101][29/233]	Loss 0.0009 (0.0045)	
training:	Epoch: [101][30/233]	Loss 0.2183 (0.0116)	
training:	Epoch: [101][31/233]	Loss 0.0008 (0.0113)	
training:	Epoch: [101][32/233]	Loss 0.0009 (0.0110)	
training:	Epoch: [101][33/233]	Loss 0.0008 (0.0107)	
training:	Epoch: [101][34/233]	Loss 0.0009 (0.0104)	
training:	Epoch: [101][35/233]	Loss 0.0010 (0.0101)	
training:	Epoch: [101][36/233]	Loss 0.0018 (0.0099)	
training:	Epoch: [101][37/233]	Loss 0.0024 (0.0097)	
training:	Epoch: [101][38/233]	Loss 0.0021 (0.0095)	
training:	Epoch: [101][39/233]	Loss 0.0010 (0.0093)	
training:	Epoch: [101][40/233]	Loss 0.0055 (0.0092)	
training:	Epoch: [101][41/233]	Loss 0.0010 (0.0090)	
training:	Epoch: [101][42/233]	Loss 0.0012 (0.0088)	
training:	Epoch: [101][43/233]	Loss 0.0009 (0.0086)	
training:	Epoch: [101][44/233]	Loss 0.0012 (0.0084)	
training:	Epoch: [101][45/233]	Loss 0.0012 (0.0083)	
training:	Epoch: [101][46/233]	Loss 0.0009 (0.0081)	
training:	Epoch: [101][47/233]	Loss 0.0012 (0.0080)	
training:	Epoch: [101][48/233]	Loss 0.0011 (0.0078)	
training:	Epoch: [101][49/233]	Loss 0.0008 (0.0077)	
training:	Epoch: [101][50/233]	Loss 0.0012 (0.0075)	
training:	Epoch: [101][51/233]	Loss 0.0010 (0.0074)	
training:	Epoch: [101][52/233]	Loss 0.0008 (0.0073)	
training:	Epoch: [101][53/233]	Loss 0.0008 (0.0072)	
training:	Epoch: [101][54/233]	Loss 0.0348 (0.0077)	
training:	Epoch: [101][55/233]	Loss 0.0012 (0.0076)	
training:	Epoch: [101][56/233]	Loss 0.0010 (0.0074)	
training:	Epoch: [101][57/233]	Loss 0.0013 (0.0073)	
training:	Epoch: [101][58/233]	Loss 0.0026 (0.0073)	
training:	Epoch: [101][59/233]	Loss 0.0009 (0.0071)	
training:	Epoch: [101][60/233]	Loss 0.0013 (0.0071)	
training:	Epoch: [101][61/233]	Loss 0.0008 (0.0069)	
training:	Epoch: [101][62/233]	Loss 0.0009 (0.0069)	
training:	Epoch: [101][63/233]	Loss 0.0012 (0.0068)	
training:	Epoch: [101][64/233]	Loss 0.0009 (0.0067)	
training:	Epoch: [101][65/233]	Loss 0.0009 (0.0066)	
training:	Epoch: [101][66/233]	Loss 0.0021 (0.0065)	
training:	Epoch: [101][67/233]	Loss 0.0009 (0.0064)	
training:	Epoch: [101][68/233]	Loss 0.0098 (0.0065)	
training:	Epoch: [101][69/233]	Loss 0.0008 (0.0064)	
training:	Epoch: [101][70/233]	Loss 0.0009 (0.0063)	
training:	Epoch: [101][71/233]	Loss 0.0008 (0.0062)	
training:	Epoch: [101][72/233]	Loss 0.0009 (0.0062)	
training:	Epoch: [101][73/233]	Loss 0.0015 (0.0061)	
training:	Epoch: [101][74/233]	Loss 0.0010 (0.0060)	
training:	Epoch: [101][75/233]	Loss 0.0009 (0.0060)	
training:	Epoch: [101][76/233]	Loss 0.0011 (0.0059)	
training:	Epoch: [101][77/233]	Loss 0.0018 (0.0058)	
training:	Epoch: [101][78/233]	Loss 0.0009 (0.0058)	
training:	Epoch: [101][79/233]	Loss 0.0008 (0.0057)	
training:	Epoch: [101][80/233]	Loss 0.0013 (0.0057)	
training:	Epoch: [101][81/233]	Loss 0.0010 (0.0056)	
training:	Epoch: [101][82/233]	Loss 0.0009 (0.0056)	
training:	Epoch: [101][83/233]	Loss 0.0031 (0.0055)	
training:	Epoch: [101][84/233]	Loss 0.0008 (0.0055)	
training:	Epoch: [101][85/233]	Loss 0.0008 (0.0054)	
training:	Epoch: [101][86/233]	Loss 0.0007 (0.0054)	
training:	Epoch: [101][87/233]	Loss 0.0009 (0.0053)	
training:	Epoch: [101][88/233]	Loss 0.0053 (0.0053)	
training:	Epoch: [101][89/233]	Loss 0.0010 (0.0053)	
training:	Epoch: [101][90/233]	Loss 0.0010 (0.0052)	
training:	Epoch: [101][91/233]	Loss 0.0901 (0.0061)	
training:	Epoch: [101][92/233]	Loss 0.0008 (0.0061)	
training:	Epoch: [101][93/233]	Loss 0.0008 (0.0060)	
training:	Epoch: [101][94/233]	Loss 0.0009 (0.0060)	
training:	Epoch: [101][95/233]	Loss 0.0009 (0.0059)	
training:	Epoch: [101][96/233]	Loss 0.0054 (0.0059)	
training:	Epoch: [101][97/233]	Loss 0.0007 (0.0059)	
training:	Epoch: [101][98/233]	Loss 0.0011 (0.0058)	
training:	Epoch: [101][99/233]	Loss 0.0315 (0.0061)	
training:	Epoch: [101][100/233]	Loss 0.0044 (0.0061)	
training:	Epoch: [101][101/233]	Loss 0.0009 (0.0060)	
training:	Epoch: [101][102/233]	Loss 0.0014 (0.0060)	
training:	Epoch: [101][103/233]	Loss 0.0009 (0.0059)	
training:	Epoch: [101][104/233]	Loss 0.0010 (0.0059)	
training:	Epoch: [101][105/233]	Loss 0.1465 (0.0072)	
training:	Epoch: [101][106/233]	Loss 0.0016 (0.0071)	
training:	Epoch: [101][107/233]	Loss 0.0051 (0.0071)	
training:	Epoch: [101][108/233]	Loss 0.0012 (0.0071)	
training:	Epoch: [101][109/233]	Loss 0.0018 (0.0070)	
training:	Epoch: [101][110/233]	Loss 0.0007 (0.0070)	
training:	Epoch: [101][111/233]	Loss 0.0039 (0.0069)	
training:	Epoch: [101][112/233]	Loss 0.0024 (0.0069)	
training:	Epoch: [101][113/233]	Loss 0.0012 (0.0069)	
training:	Epoch: [101][114/233]	Loss 0.0009 (0.0068)	
training:	Epoch: [101][115/233]	Loss 0.0011 (0.0067)	
training:	Epoch: [101][116/233]	Loss 0.1487 (0.0080)	
training:	Epoch: [101][117/233]	Loss 0.0010 (0.0079)	
training:	Epoch: [101][118/233]	Loss 0.0019 (0.0079)	
training:	Epoch: [101][119/233]	Loss 0.0008 (0.0078)	
training:	Epoch: [101][120/233]	Loss 0.0009 (0.0077)	
training:	Epoch: [101][121/233]	Loss 0.0011 (0.0077)	
training:	Epoch: [101][122/233]	Loss 0.0064 (0.0077)	
training:	Epoch: [101][123/233]	Loss 0.0035 (0.0076)	
training:	Epoch: [101][124/233]	Loss 0.0010 (0.0076)	
training:	Epoch: [101][125/233]	Loss 0.2438 (0.0095)	
training:	Epoch: [101][126/233]	Loss 0.0008 (0.0094)	
training:	Epoch: [101][127/233]	Loss 0.0044 (0.0094)	
training:	Epoch: [101][128/233]	Loss 0.0010 (0.0093)	
training:	Epoch: [101][129/233]	Loss 0.0019 (0.0093)	
training:	Epoch: [101][130/233]	Loss 0.0034 (0.0092)	
training:	Epoch: [101][131/233]	Loss 0.0011 (0.0091)	
training:	Epoch: [101][132/233]	Loss 0.0009 (0.0091)	
training:	Epoch: [101][133/233]	Loss 0.0010 (0.0090)	
training:	Epoch: [101][134/233]	Loss 0.0601 (0.0094)	
training:	Epoch: [101][135/233]	Loss 0.0009 (0.0093)	
training:	Epoch: [101][136/233]	Loss 0.0010 (0.0093)	
training:	Epoch: [101][137/233]	Loss 0.0009 (0.0092)	
training:	Epoch: [101][138/233]	Loss 0.0164 (0.0093)	
training:	Epoch: [101][139/233]	Loss 0.0008 (0.0092)	
training:	Epoch: [101][140/233]	Loss 0.0718 (0.0097)	
training:	Epoch: [101][141/233]	Loss 0.0012 (0.0096)	
training:	Epoch: [101][142/233]	Loss 0.1087 (0.0103)	
training:	Epoch: [101][143/233]	Loss 0.0022 (0.0102)	
training:	Epoch: [101][144/233]	Loss 0.0281 (0.0104)	
training:	Epoch: [101][145/233]	Loss 0.0204 (0.0104)	
training:	Epoch: [101][146/233]	Loss 0.0015 (0.0104)	
training:	Epoch: [101][147/233]	Loss 0.0012 (0.0103)	
training:	Epoch: [101][148/233]	Loss 0.0084 (0.0103)	
training:	Epoch: [101][149/233]	Loss 0.0017 (0.0102)	
training:	Epoch: [101][150/233]	Loss 0.0014 (0.0102)	
training:	Epoch: [101][151/233]	Loss 0.0011 (0.0101)	
training:	Epoch: [101][152/233]	Loss 0.0047 (0.0101)	
training:	Epoch: [101][153/233]	Loss 0.0015 (0.0100)	
training:	Epoch: [101][154/233]	Loss 0.0009 (0.0100)	
training:	Epoch: [101][155/233]	Loss 0.0011 (0.0099)	
training:	Epoch: [101][156/233]	Loss 0.0011 (0.0099)	
training:	Epoch: [101][157/233]	Loss 0.0123 (0.0099)	
training:	Epoch: [101][158/233]	Loss 0.0008 (0.0098)	
training:	Epoch: [101][159/233]	Loss 0.0014 (0.0098)	
training:	Epoch: [101][160/233]	Loss 0.0011 (0.0097)	
training:	Epoch: [101][161/233]	Loss 0.0007 (0.0096)	
training:	Epoch: [101][162/233]	Loss 0.0009 (0.0096)	
training:	Epoch: [101][163/233]	Loss 0.0027 (0.0096)	
training:	Epoch: [101][164/233]	Loss 0.0034 (0.0095)	
training:	Epoch: [101][165/233]	Loss 0.0014 (0.0095)	
training:	Epoch: [101][166/233]	Loss 0.0009 (0.0094)	
training:	Epoch: [101][167/233]	Loss 0.0008 (0.0094)	
training:	Epoch: [101][168/233]	Loss 0.0018 (0.0093)	
training:	Epoch: [101][169/233]	Loss 0.0013 (0.0093)	
training:	Epoch: [101][170/233]	Loss 0.0328 (0.0094)	
training:	Epoch: [101][171/233]	Loss 0.0030 (0.0094)	
training:	Epoch: [101][172/233]	Loss 0.0009 (0.0093)	
training:	Epoch: [101][173/233]	Loss 0.0009 (0.0093)	
training:	Epoch: [101][174/233]	Loss 0.0010 (0.0092)	
training:	Epoch: [101][175/233]	Loss 0.0007 (0.0092)	
training:	Epoch: [101][176/233]	Loss 0.0030 (0.0091)	
training:	Epoch: [101][177/233]	Loss 0.0009 (0.0091)	
training:	Epoch: [101][178/233]	Loss 0.0011 (0.0091)	
training:	Epoch: [101][179/233]	Loss 0.0015 (0.0090)	
training:	Epoch: [101][180/233]	Loss 0.0011 (0.0090)	
training:	Epoch: [101][181/233]	Loss 0.0141 (0.0090)	
training:	Epoch: [101][182/233]	Loss 0.0080 (0.0090)	
training:	Epoch: [101][183/233]	Loss 0.0502 (0.0092)	
training:	Epoch: [101][184/233]	Loss 0.0019 (0.0092)	
training:	Epoch: [101][185/233]	Loss 0.0011 (0.0091)	
training:	Epoch: [101][186/233]	Loss 0.0009 (0.0091)	
training:	Epoch: [101][187/233]	Loss 0.0012 (0.0090)	
training:	Epoch: [101][188/233]	Loss 0.0014 (0.0090)	
training:	Epoch: [101][189/233]	Loss 0.0010 (0.0090)	
training:	Epoch: [101][190/233]	Loss 0.0009 (0.0089)	
training:	Epoch: [101][191/233]	Loss 0.0010 (0.0089)	
training:	Epoch: [101][192/233]	Loss 0.0008 (0.0088)	
training:	Epoch: [101][193/233]	Loss 0.0018 (0.0088)	
training:	Epoch: [101][194/233]	Loss 0.0009 (0.0088)	
training:	Epoch: [101][195/233]	Loss 0.0008 (0.0087)	
training:	Epoch: [101][196/233]	Loss 0.0011 (0.0087)	
training:	Epoch: [101][197/233]	Loss 0.0014 (0.0086)	
training:	Epoch: [101][198/233]	Loss 0.0009 (0.0086)	
training:	Epoch: [101][199/233]	Loss 0.0008 (0.0086)	
training:	Epoch: [101][200/233]	Loss 0.0010 (0.0085)	
training:	Epoch: [101][201/233]	Loss 0.0033 (0.0085)	
training:	Epoch: [101][202/233]	Loss 0.0014 (0.0085)	
training:	Epoch: [101][203/233]	Loss 0.0019 (0.0084)	
training:	Epoch: [101][204/233]	Loss 0.0483 (0.0086)	
training:	Epoch: [101][205/233]	Loss 0.0048 (0.0086)	
training:	Epoch: [101][206/233]	Loss 0.0011 (0.0086)	
training:	Epoch: [101][207/233]	Loss 0.0013 (0.0085)	
training:	Epoch: [101][208/233]	Loss 0.0007 (0.0085)	
training:	Epoch: [101][209/233]	Loss 0.0013 (0.0085)	
training:	Epoch: [101][210/233]	Loss 0.0009 (0.0084)	
training:	Epoch: [101][211/233]	Loss 0.0010 (0.0084)	
training:	Epoch: [101][212/233]	Loss 0.0013 (0.0084)	
training:	Epoch: [101][213/233]	Loss 0.0013 (0.0083)	
training:	Epoch: [101][214/233]	Loss 0.0011 (0.0083)	
training:	Epoch: [101][215/233]	Loss 0.0015 (0.0083)	
training:	Epoch: [101][216/233]	Loss 0.0008 (0.0082)	
training:	Epoch: [101][217/233]	Loss 0.0010 (0.0082)	
training:	Epoch: [101][218/233]	Loss 0.0007 (0.0082)	
training:	Epoch: [101][219/233]	Loss 0.0010 (0.0081)	
training:	Epoch: [101][220/233]	Loss 0.0010 (0.0081)	
training:	Epoch: [101][221/233]	Loss 0.0009 (0.0081)	
training:	Epoch: [101][222/233]	Loss 0.0013 (0.0080)	
training:	Epoch: [101][223/233]	Loss 0.0010 (0.0080)	
training:	Epoch: [101][224/233]	Loss 0.0011 (0.0080)	
training:	Epoch: [101][225/233]	Loss 0.0012 (0.0079)	
training:	Epoch: [101][226/233]	Loss 0.0026 (0.0079)	
training:	Epoch: [101][227/233]	Loss 0.0010 (0.0079)	
training:	Epoch: [101][228/233]	Loss 0.0315 (0.0080)	
training:	Epoch: [101][229/233]	Loss 0.0011 (0.0080)	
training:	Epoch: [101][230/233]	Loss 0.0022 (0.0079)	
training:	Epoch: [101][231/233]	Loss 0.0008 (0.0079)	
training:	Epoch: [101][232/233]	Loss 0.0038 (0.0079)	
training:	Epoch: [101][233/233]	Loss 0.0009 (0.0079)	
Training:	 Loss: 0.0078

Training:	 ACC: 0.9996 0.9996 0.9997 0.9994
Validation:	 ACC: 0.7910 0.7929 0.8315 0.7506
Validation:	 Best_BACC: 0.8162 0.8154 0.7998 0.8326
Validation:	 Loss: 1.1403
Pretraining:	Epoch 102/200
----------
training:	Epoch: [102][1/233]	Loss 0.0205 (0.0205)	
training:	Epoch: [102][2/233]	Loss 0.0034 (0.0120)	
training:	Epoch: [102][3/233]	Loss 0.0206 (0.0148)	
training:	Epoch: [102][4/233]	Loss 0.1840 (0.0571)	
training:	Epoch: [102][5/233]	Loss 0.0115 (0.0480)	
training:	Epoch: [102][6/233]	Loss 0.0229 (0.0438)	
training:	Epoch: [102][7/233]	Loss 0.0032 (0.0380)	
training:	Epoch: [102][8/233]	Loss 0.0009 (0.0334)	
training:	Epoch: [102][9/233]	Loss 0.0008 (0.0298)	
training:	Epoch: [102][10/233]	Loss 0.0010 (0.0269)	
training:	Epoch: [102][11/233]	Loss 0.0012 (0.0245)	
training:	Epoch: [102][12/233]	Loss 0.0013 (0.0226)	
training:	Epoch: [102][13/233]	Loss 0.0008 (0.0209)	
training:	Epoch: [102][14/233]	Loss 0.0009 (0.0195)	
training:	Epoch: [102][15/233]	Loss 0.0022 (0.0183)	
training:	Epoch: [102][16/233]	Loss 0.0009 (0.0173)	
training:	Epoch: [102][17/233]	Loss 0.0009 (0.0163)	
training:	Epoch: [102][18/233]	Loss 0.0010 (0.0154)	
training:	Epoch: [102][19/233]	Loss 0.0009 (0.0147)	
training:	Epoch: [102][20/233]	Loss 0.0009 (0.0140)	
training:	Epoch: [102][21/233]	Loss 0.0010 (0.0134)	
training:	Epoch: [102][22/233]	Loss 0.0012 (0.0128)	
training:	Epoch: [102][23/233]	Loss 0.0007 (0.0123)	
training:	Epoch: [102][24/233]	Loss 0.0010 (0.0118)	
training:	Epoch: [102][25/233]	Loss 0.0011 (0.0114)	
training:	Epoch: [102][26/233]	Loss 0.0023 (0.0110)	
training:	Epoch: [102][27/233]	Loss 0.0009 (0.0107)	
training:	Epoch: [102][28/233]	Loss 0.0024 (0.0104)	
training:	Epoch: [102][29/233]	Loss 0.0012 (0.0101)	
training:	Epoch: [102][30/233]	Loss 0.0009 (0.0097)	
training:	Epoch: [102][31/233]	Loss 0.2089 (0.0162)	
training:	Epoch: [102][32/233]	Loss 0.0012 (0.0157)	
training:	Epoch: [102][33/233]	Loss 0.0025 (0.0153)	
training:	Epoch: [102][34/233]	Loss 0.0008 (0.0149)	
training:	Epoch: [102][35/233]	Loss 0.0010 (0.0145)	
training:	Epoch: [102][36/233]	Loss 0.0013 (0.0141)	
training:	Epoch: [102][37/233]	Loss 0.0034 (0.0138)	
training:	Epoch: [102][38/233]	Loss 0.1734 (0.0180)	
training:	Epoch: [102][39/233]	Loss 0.0063 (0.0177)	
training:	Epoch: [102][40/233]	Loss 0.0015 (0.0173)	
training:	Epoch: [102][41/233]	Loss 0.0028 (0.0170)	
training:	Epoch: [102][42/233]	Loss 0.0013 (0.0166)	
training:	Epoch: [102][43/233]	Loss 0.0150 (0.0166)	
training:	Epoch: [102][44/233]	Loss 0.1144 (0.0188)	
training:	Epoch: [102][45/233]	Loss 0.0046 (0.0185)	
training:	Epoch: [102][46/233]	Loss 0.0023 (0.0181)	
training:	Epoch: [102][47/233]	Loss 0.0020 (0.0178)	
training:	Epoch: [102][48/233]	Loss 0.0008 (0.0174)	
training:	Epoch: [102][49/233]	Loss 0.0009 (0.0171)	
training:	Epoch: [102][50/233]	Loss 0.0014 (0.0168)	
training:	Epoch: [102][51/233]	Loss 0.0012 (0.0165)	
training:	Epoch: [102][52/233]	Loss 0.0011 (0.0162)	
training:	Epoch: [102][53/233]	Loss 0.0756 (0.0173)	
training:	Epoch: [102][54/233]	Loss 0.0011 (0.0170)	
training:	Epoch: [102][55/233]	Loss 0.0007 (0.0167)	
training:	Epoch: [102][56/233]	Loss 0.0009 (0.0164)	
training:	Epoch: [102][57/233]	Loss 0.0009 (0.0161)	
training:	Epoch: [102][58/233]	Loss 0.0040 (0.0159)	
training:	Epoch: [102][59/233]	Loss 0.0018 (0.0157)	
training:	Epoch: [102][60/233]	Loss 0.0009 (0.0154)	
training:	Epoch: [102][61/233]	Loss 0.0007 (0.0152)	
training:	Epoch: [102][62/233]	Loss 0.0014 (0.0150)	
training:	Epoch: [102][63/233]	Loss 0.0010 (0.0148)	
training:	Epoch: [102][64/233]	Loss 0.0007 (0.0145)	
training:	Epoch: [102][65/233]	Loss 0.0011 (0.0143)	
training:	Epoch: [102][66/233]	Loss 0.0009 (0.0141)	
training:	Epoch: [102][67/233]	Loss 0.0012 (0.0139)	
training:	Epoch: [102][68/233]	Loss 0.0011 (0.0137)	
training:	Epoch: [102][69/233]	Loss 0.0009 (0.0136)	
training:	Epoch: [102][70/233]	Loss 0.0007 (0.0134)	
training:	Epoch: [102][71/233]	Loss 0.0008 (0.0132)	
training:	Epoch: [102][72/233]	Loss 0.0011 (0.0130)	
training:	Epoch: [102][73/233]	Loss 0.0009 (0.0129)	
training:	Epoch: [102][74/233]	Loss 0.0009 (0.0127)	
training:	Epoch: [102][75/233]	Loss 0.0009 (0.0125)	
training:	Epoch: [102][76/233]	Loss 0.0007 (0.0124)	
training:	Epoch: [102][77/233]	Loss 0.0009 (0.0122)	
training:	Epoch: [102][78/233]	Loss 0.0008 (0.0121)	
training:	Epoch: [102][79/233]	Loss 0.0008 (0.0119)	
training:	Epoch: [102][80/233]	Loss 0.0017 (0.0118)	
training:	Epoch: [102][81/233]	Loss 0.0048 (0.0117)	
training:	Epoch: [102][82/233]	Loss 0.0008 (0.0116)	
training:	Epoch: [102][83/233]	Loss 0.0008 (0.0115)	
training:	Epoch: [102][84/233]	Loss 0.0011 (0.0113)	
training:	Epoch: [102][85/233]	Loss 0.0009 (0.0112)	
training:	Epoch: [102][86/233]	Loss 0.0007 (0.0111)	
training:	Epoch: [102][87/233]	Loss 0.0080 (0.0111)	
training:	Epoch: [102][88/233]	Loss 0.0010 (0.0110)	
training:	Epoch: [102][89/233]	Loss 0.0014 (0.0108)	
training:	Epoch: [102][90/233]	Loss 0.0008 (0.0107)	
training:	Epoch: [102][91/233]	Loss 0.0113 (0.0107)	
training:	Epoch: [102][92/233]	Loss 0.0008 (0.0106)	
training:	Epoch: [102][93/233]	Loss 0.0008 (0.0105)	
training:	Epoch: [102][94/233]	Loss 0.0083 (0.0105)	
training:	Epoch: [102][95/233]	Loss 0.0011 (0.0104)	
training:	Epoch: [102][96/233]	Loss 0.0053 (0.0104)	
training:	Epoch: [102][97/233]	Loss 0.0458 (0.0107)	
training:	Epoch: [102][98/233]	Loss 0.0009 (0.0106)	
training:	Epoch: [102][99/233]	Loss 0.0009 (0.0105)	
training:	Epoch: [102][100/233]	Loss 0.0010 (0.0104)	
training:	Epoch: [102][101/233]	Loss 0.1047 (0.0114)	
training:	Epoch: [102][102/233]	Loss 0.0011 (0.0113)	
training:	Epoch: [102][103/233]	Loss 0.0008 (0.0112)	
training:	Epoch: [102][104/233]	Loss 0.1074 (0.0121)	
training:	Epoch: [102][105/233]	Loss 0.0258 (0.0122)	
training:	Epoch: [102][106/233]	Loss 0.0028 (0.0121)	
training:	Epoch: [102][107/233]	Loss 0.0009 (0.0120)	
training:	Epoch: [102][108/233]	Loss 0.0008 (0.0119)	
training:	Epoch: [102][109/233]	Loss 0.0026 (0.0118)	
training:	Epoch: [102][110/233]	Loss 0.0010 (0.0117)	
training:	Epoch: [102][111/233]	Loss 0.0010 (0.0116)	
training:	Epoch: [102][112/233]	Loss 0.0011 (0.0115)	
training:	Epoch: [102][113/233]	Loss 0.0024 (0.0115)	
training:	Epoch: [102][114/233]	Loss 0.0010 (0.0114)	
training:	Epoch: [102][115/233]	Loss 0.0015 (0.0113)	
training:	Epoch: [102][116/233]	Loss 0.0009 (0.0112)	
training:	Epoch: [102][117/233]	Loss 0.0029 (0.0111)	
training:	Epoch: [102][118/233]	Loss 0.0011 (0.0110)	
training:	Epoch: [102][119/233]	Loss 0.0011 (0.0110)	
training:	Epoch: [102][120/233]	Loss 0.0016 (0.0109)	
training:	Epoch: [102][121/233]	Loss 0.2316 (0.0127)	
training:	Epoch: [102][122/233]	Loss 0.0009 (0.0126)	
training:	Epoch: [102][123/233]	Loss 0.0010 (0.0125)	
training:	Epoch: [102][124/233]	Loss 0.0079 (0.0125)	
training:	Epoch: [102][125/233]	Loss 0.0016 (0.0124)	
training:	Epoch: [102][126/233]	Loss 0.0701 (0.0128)	
training:	Epoch: [102][127/233]	Loss 0.0018 (0.0128)	
training:	Epoch: [102][128/233]	Loss 0.0029 (0.0127)	
training:	Epoch: [102][129/233]	Loss 0.0024 (0.0126)	
training:	Epoch: [102][130/233]	Loss 0.0009 (0.0125)	
training:	Epoch: [102][131/233]	Loss 0.0011 (0.0124)	
training:	Epoch: [102][132/233]	Loss 0.0008 (0.0123)	
training:	Epoch: [102][133/233]	Loss 0.0009 (0.0122)	
training:	Epoch: [102][134/233]	Loss 0.0029 (0.0122)	
training:	Epoch: [102][135/233]	Loss 0.0012 (0.0121)	
training:	Epoch: [102][136/233]	Loss 0.0026 (0.0120)	
training:	Epoch: [102][137/233]	Loss 0.0017 (0.0120)	
training:	Epoch: [102][138/233]	Loss 0.0010 (0.0119)	
training:	Epoch: [102][139/233]	Loss 0.0014 (0.0118)	
training:	Epoch: [102][140/233]	Loss 0.0008 (0.0117)	
training:	Epoch: [102][141/233]	Loss 0.0013 (0.0116)	
training:	Epoch: [102][142/233]	Loss 0.0014 (0.0116)	
training:	Epoch: [102][143/233]	Loss 0.0009 (0.0115)	
training:	Epoch: [102][144/233]	Loss 0.0016 (0.0114)	
training:	Epoch: [102][145/233]	Loss 0.0008 (0.0114)	
training:	Epoch: [102][146/233]	Loss 0.0011 (0.0113)	
training:	Epoch: [102][147/233]	Loss 0.0012 (0.0112)	
training:	Epoch: [102][148/233]	Loss 0.0009 (0.0111)	
training:	Epoch: [102][149/233]	Loss 0.0008 (0.0111)	
training:	Epoch: [102][150/233]	Loss 0.0008 (0.0110)	
training:	Epoch: [102][151/233]	Loss 0.0014 (0.0109)	
training:	Epoch: [102][152/233]	Loss 0.0009 (0.0109)	
training:	Epoch: [102][153/233]	Loss 0.0170 (0.0109)	
training:	Epoch: [102][154/233]	Loss 0.0028 (0.0109)	
training:	Epoch: [102][155/233]	Loss 0.0038 (0.0108)	
training:	Epoch: [102][156/233]	Loss 0.0009 (0.0108)	
training:	Epoch: [102][157/233]	Loss 0.0461 (0.0110)	
training:	Epoch: [102][158/233]	Loss 0.0014 (0.0109)	
training:	Epoch: [102][159/233]	Loss 0.0015 (0.0109)	
training:	Epoch: [102][160/233]	Loss 0.0020 (0.0108)	
training:	Epoch: [102][161/233]	Loss 0.0011 (0.0107)	
training:	Epoch: [102][162/233]	Loss 0.0013 (0.0107)	
training:	Epoch: [102][163/233]	Loss 0.0014 (0.0106)	
training:	Epoch: [102][164/233]	Loss 0.0009 (0.0106)	
training:	Epoch: [102][165/233]	Loss 0.0040 (0.0105)	
training:	Epoch: [102][166/233]	Loss 0.1424 (0.0113)	
training:	Epoch: [102][167/233]	Loss 0.0008 (0.0113)	
training:	Epoch: [102][168/233]	Loss 0.1784 (0.0123)	
training:	Epoch: [102][169/233]	Loss 0.0010 (0.0122)	
training:	Epoch: [102][170/233]	Loss 0.0012 (0.0121)	
training:	Epoch: [102][171/233]	Loss 0.0010 (0.0121)	
training:	Epoch: [102][172/233]	Loss 0.0011 (0.0120)	
training:	Epoch: [102][173/233]	Loss 0.0013 (0.0119)	
training:	Epoch: [102][174/233]	Loss 0.0015 (0.0119)	
training:	Epoch: [102][175/233]	Loss 0.0016 (0.0118)	
training:	Epoch: [102][176/233]	Loss 0.0011 (0.0118)	
training:	Epoch: [102][177/233]	Loss 0.0020 (0.0117)	
training:	Epoch: [102][178/233]	Loss 0.0009 (0.0116)	
training:	Epoch: [102][179/233]	Loss 0.0009 (0.0116)	
training:	Epoch: [102][180/233]	Loss 0.0016 (0.0115)	
training:	Epoch: [102][181/233]	Loss 0.0049 (0.0115)	
training:	Epoch: [102][182/233]	Loss 0.0011 (0.0114)	
training:	Epoch: [102][183/233]	Loss 0.0016 (0.0114)	
training:	Epoch: [102][184/233]	Loss 0.0012 (0.0113)	
training:	Epoch: [102][185/233]	Loss 0.0017 (0.0113)	
training:	Epoch: [102][186/233]	Loss 0.2127 (0.0124)	
training:	Epoch: [102][187/233]	Loss 0.0011 (0.0123)	
training:	Epoch: [102][188/233]	Loss 0.0010 (0.0122)	
training:	Epoch: [102][189/233]	Loss 0.0013 (0.0122)	
training:	Epoch: [102][190/233]	Loss 0.0008 (0.0121)	
training:	Epoch: [102][191/233]	Loss 0.0031 (0.0121)	
training:	Epoch: [102][192/233]	Loss 0.0009 (0.0120)	
training:	Epoch: [102][193/233]	Loss 0.0009 (0.0120)	
training:	Epoch: [102][194/233]	Loss 0.0009 (0.0119)	
training:	Epoch: [102][195/233]	Loss 0.0942 (0.0123)	
training:	Epoch: [102][196/233]	Loss 0.0021 (0.0123)	
training:	Epoch: [102][197/233]	Loss 0.0010 (0.0122)	
training:	Epoch: [102][198/233]	Loss 0.0009 (0.0122)	
training:	Epoch: [102][199/233]	Loss 0.0048 (0.0121)	
training:	Epoch: [102][200/233]	Loss 0.0009 (0.0121)	
training:	Epoch: [102][201/233]	Loss 0.0011 (0.0120)	
training:	Epoch: [102][202/233]	Loss 0.0010 (0.0120)	
training:	Epoch: [102][203/233]	Loss 0.0037 (0.0119)	
training:	Epoch: [102][204/233]	Loss 0.0011 (0.0119)	
training:	Epoch: [102][205/233]	Loss 0.0059 (0.0118)	
training:	Epoch: [102][206/233]	Loss 0.0198 (0.0119)	
training:	Epoch: [102][207/233]	Loss 0.0011 (0.0118)	
training:	Epoch: [102][208/233]	Loss 0.0019 (0.0118)	
training:	Epoch: [102][209/233]	Loss 0.0010 (0.0117)	
training:	Epoch: [102][210/233]	Loss 0.0008 (0.0117)	
training:	Epoch: [102][211/233]	Loss 0.0008 (0.0116)	
training:	Epoch: [102][212/233]	Loss 0.0022 (0.0116)	
training:	Epoch: [102][213/233]	Loss 0.0021 (0.0115)	
training:	Epoch: [102][214/233]	Loss 0.0009 (0.0115)	
training:	Epoch: [102][215/233]	Loss 0.0024 (0.0114)	
training:	Epoch: [102][216/233]	Loss 0.0026 (0.0114)	
training:	Epoch: [102][217/233]	Loss 0.0010 (0.0113)	
training:	Epoch: [102][218/233]	Loss 0.0077 (0.0113)	
training:	Epoch: [102][219/233]	Loss 0.0133 (0.0113)	
training:	Epoch: [102][220/233]	Loss 0.0086 (0.0113)	
training:	Epoch: [102][221/233]	Loss 0.0023 (0.0113)	
training:	Epoch: [102][222/233]	Loss 0.0017 (0.0112)	
training:	Epoch: [102][223/233]	Loss 0.0017 (0.0112)	
training:	Epoch: [102][224/233]	Loss 0.0018 (0.0112)	
training:	Epoch: [102][225/233]	Loss 0.0013 (0.0111)	
training:	Epoch: [102][226/233]	Loss 0.0008 (0.0111)	
training:	Epoch: [102][227/233]	Loss 0.0011 (0.0110)	
training:	Epoch: [102][228/233]	Loss 0.0014 (0.0110)	
training:	Epoch: [102][229/233]	Loss 0.0024 (0.0109)	
training:	Epoch: [102][230/233]	Loss 0.0022 (0.0109)	
training:	Epoch: [102][231/233]	Loss 0.0009 (0.0109)	
training:	Epoch: [102][232/233]	Loss 0.0010 (0.0108)	
training:	Epoch: [102][233/233]	Loss 0.0011 (0.0108)	
Training:	 Loss: 0.0108

Training:	 ACC: 0.9997 0.9997 0.9997 0.9997
Validation:	 ACC: 0.7889 0.7887 0.7845 0.7933
Validation:	 Best_BACC: 0.8162 0.8154 0.7998 0.8326
Validation:	 Loss: 1.1135
Pretraining:	Epoch 103/200
----------
training:	Epoch: [103][1/233]	Loss 0.0009 (0.0009)	
training:	Epoch: [103][2/233]	Loss 0.0011 (0.0010)	
training:	Epoch: [103][3/233]	Loss 0.0012 (0.0010)	
training:	Epoch: [103][4/233]	Loss 0.0008 (0.0010)	
training:	Epoch: [103][5/233]	Loss 0.0007 (0.0009)	
training:	Epoch: [103][6/233]	Loss 0.0012 (0.0010)	
training:	Epoch: [103][7/233]	Loss 0.0008 (0.0009)	
training:	Epoch: [103][8/233]	Loss 0.0011 (0.0010)	
training:	Epoch: [103][9/233]	Loss 0.0010 (0.0010)	
training:	Epoch: [103][10/233]	Loss 0.0176 (0.0026)	
training:	Epoch: [103][11/233]	Loss 0.0008 (0.0025)	
training:	Epoch: [103][12/233]	Loss 0.0012 (0.0024)	
training:	Epoch: [103][13/233]	Loss 0.0221 (0.0039)	
training:	Epoch: [103][14/233]	Loss 0.0010 (0.0037)	
training:	Epoch: [103][15/233]	Loss 0.0010 (0.0035)	
training:	Epoch: [103][16/233]	Loss 0.0010 (0.0033)	
training:	Epoch: [103][17/233]	Loss 0.0011 (0.0032)	
training:	Epoch: [103][18/233]	Loss 0.0009 (0.0031)	
training:	Epoch: [103][19/233]	Loss 0.0017 (0.0030)	
training:	Epoch: [103][20/233]	Loss 0.0009 (0.0029)	
training:	Epoch: [103][21/233]	Loss 0.0010 (0.0028)	
training:	Epoch: [103][22/233]	Loss 0.0989 (0.0072)	
training:	Epoch: [103][23/233]	Loss 0.0017 (0.0069)	
training:	Epoch: [103][24/233]	Loss 0.0046 (0.0068)	
training:	Epoch: [103][25/233]	Loss 0.0010 (0.0066)	
training:	Epoch: [103][26/233]	Loss 0.0009 (0.0064)	
training:	Epoch: [103][27/233]	Loss 0.0010 (0.0062)	
training:	Epoch: [103][28/233]	Loss 0.0009 (0.0060)	
training:	Epoch: [103][29/233]	Loss 0.0012 (0.0058)	
training:	Epoch: [103][30/233]	Loss 0.0020 (0.0057)	
training:	Epoch: [103][31/233]	Loss 0.0009 (0.0056)	
training:	Epoch: [103][32/233]	Loss 0.0738 (0.0077)	
training:	Epoch: [103][33/233]	Loss 0.0016 (0.0075)	
training:	Epoch: [103][34/233]	Loss 0.0009 (0.0073)	
training:	Epoch: [103][35/233]	Loss 0.0024 (0.0072)	
training:	Epoch: [103][36/233]	Loss 0.0011 (0.0070)	
training:	Epoch: [103][37/233]	Loss 0.0009 (0.0068)	
training:	Epoch: [103][38/233]	Loss 0.0012 (0.0067)	
training:	Epoch: [103][39/233]	Loss 0.0008 (0.0065)	
training:	Epoch: [103][40/233]	Loss 0.0010 (0.0064)	
training:	Epoch: [103][41/233]	Loss 0.0534 (0.0075)	
training:	Epoch: [103][42/233]	Loss 0.0031 (0.0074)	
training:	Epoch: [103][43/233]	Loss 0.0012 (0.0073)	
training:	Epoch: [103][44/233]	Loss 0.0010 (0.0071)	
training:	Epoch: [103][45/233]	Loss 0.0010 (0.0070)	
training:	Epoch: [103][46/233]	Loss 0.0008 (0.0069)	
training:	Epoch: [103][47/233]	Loss 0.1087 (0.0090)	
training:	Epoch: [103][48/233]	Loss 0.0008 (0.0089)	
training:	Epoch: [103][49/233]	Loss 0.0010 (0.0087)	
training:	Epoch: [103][50/233]	Loss 0.0045 (0.0086)	
training:	Epoch: [103][51/233]	Loss 0.0011 (0.0085)	
training:	Epoch: [103][52/233]	Loss 0.0015 (0.0083)	
training:	Epoch: [103][53/233]	Loss 0.0020 (0.0082)	
training:	Epoch: [103][54/233]	Loss 0.0012 (0.0081)	
training:	Epoch: [103][55/233]	Loss 0.2306 (0.0121)	
training:	Epoch: [103][56/233]	Loss 0.0013 (0.0119)	
training:	Epoch: [103][57/233]	Loss 0.0010 (0.0118)	
training:	Epoch: [103][58/233]	Loss 0.0020 (0.0116)	
training:	Epoch: [103][59/233]	Loss 0.0012 (0.0114)	
training:	Epoch: [103][60/233]	Loss 0.0009 (0.0112)	
training:	Epoch: [103][61/233]	Loss 0.0025 (0.0111)	
training:	Epoch: [103][62/233]	Loss 0.0010 (0.0109)	
training:	Epoch: [103][63/233]	Loss 0.0038 (0.0108)	
training:	Epoch: [103][64/233]	Loss 0.0015 (0.0107)	
training:	Epoch: [103][65/233]	Loss 0.0010 (0.0105)	
training:	Epoch: [103][66/233]	Loss 0.0018 (0.0104)	
training:	Epoch: [103][67/233]	Loss 0.0010 (0.0102)	
training:	Epoch: [103][68/233]	Loss 0.0019 (0.0101)	
training:	Epoch: [103][69/233]	Loss 0.0014 (0.0100)	
training:	Epoch: [103][70/233]	Loss 0.0009 (0.0099)	
training:	Epoch: [103][71/233]	Loss 0.0012 (0.0097)	
training:	Epoch: [103][72/233]	Loss 0.0023 (0.0096)	
training:	Epoch: [103][73/233]	Loss 0.0010 (0.0095)	
training:	Epoch: [103][74/233]	Loss 0.1235 (0.0111)	
training:	Epoch: [103][75/233]	Loss 0.0030 (0.0110)	
training:	Epoch: [103][76/233]	Loss 0.0021 (0.0108)	
training:	Epoch: [103][77/233]	Loss 0.0010 (0.0107)	
training:	Epoch: [103][78/233]	Loss 0.0012 (0.0106)	
training:	Epoch: [103][79/233]	Loss 0.0009 (0.0105)	
training:	Epoch: [103][80/233]	Loss 0.0024 (0.0104)	
training:	Epoch: [103][81/233]	Loss 0.0014 (0.0103)	
training:	Epoch: [103][82/233]	Loss 0.0012 (0.0101)	
training:	Epoch: [103][83/233]	Loss 0.0020 (0.0100)	
training:	Epoch: [103][84/233]	Loss 0.0028 (0.0100)	
training:	Epoch: [103][85/233]	Loss 0.0008 (0.0099)	
training:	Epoch: [103][86/233]	Loss 0.0014 (0.0098)	
training:	Epoch: [103][87/233]	Loss 0.0011 (0.0097)	
training:	Epoch: [103][88/233]	Loss 0.0019 (0.0096)	
training:	Epoch: [103][89/233]	Loss 0.0011 (0.0095)	
training:	Epoch: [103][90/233]	Loss 0.0012 (0.0094)	
training:	Epoch: [103][91/233]	Loss 0.0024 (0.0093)	
training:	Epoch: [103][92/233]	Loss 0.0011 (0.0092)	
training:	Epoch: [103][93/233]	Loss 0.0011 (0.0091)	
training:	Epoch: [103][94/233]	Loss 0.0008 (0.0090)	
training:	Epoch: [103][95/233]	Loss 0.0011 (0.0090)	
training:	Epoch: [103][96/233]	Loss 0.0024 (0.0089)	
training:	Epoch: [103][97/233]	Loss 0.0024 (0.0088)	
training:	Epoch: [103][98/233]	Loss 0.0041 (0.0088)	
training:	Epoch: [103][99/233]	Loss 0.0025 (0.0087)	
training:	Epoch: [103][100/233]	Loss 0.0013 (0.0086)	
training:	Epoch: [103][101/233]	Loss 0.0008 (0.0086)	
training:	Epoch: [103][102/233]	Loss 0.0012 (0.0085)	
training:	Epoch: [103][103/233]	Loss 0.0012 (0.0084)	
training:	Epoch: [103][104/233]	Loss 0.0012 (0.0083)	
training:	Epoch: [103][105/233]	Loss 0.0015 (0.0083)	
training:	Epoch: [103][106/233]	Loss 0.0008 (0.0082)	
training:	Epoch: [103][107/233]	Loss 0.0009 (0.0081)	
training:	Epoch: [103][108/233]	Loss 0.0016 (0.0081)	
training:	Epoch: [103][109/233]	Loss 0.0017 (0.0080)	
training:	Epoch: [103][110/233]	Loss 0.0010 (0.0080)	
training:	Epoch: [103][111/233]	Loss 0.0062 (0.0079)	
training:	Epoch: [103][112/233]	Loss 0.0025 (0.0079)	
training:	Epoch: [103][113/233]	Loss 0.0010 (0.0078)	
training:	Epoch: [103][114/233]	Loss 0.0011 (0.0078)	
training:	Epoch: [103][115/233]	Loss 0.0076 (0.0078)	
training:	Epoch: [103][116/233]	Loss 0.0013 (0.0077)	
training:	Epoch: [103][117/233]	Loss 0.0011 (0.0077)	
training:	Epoch: [103][118/233]	Loss 0.0010 (0.0076)	
training:	Epoch: [103][119/233]	Loss 0.0007 (0.0075)	
training:	Epoch: [103][120/233]	Loss 0.0019 (0.0075)	
training:	Epoch: [103][121/233]	Loss 0.0012 (0.0074)	
training:	Epoch: [103][122/233]	Loss 0.0010 (0.0074)	
training:	Epoch: [103][123/233]	Loss 0.0009 (0.0073)	
training:	Epoch: [103][124/233]	Loss 0.0009 (0.0073)	
training:	Epoch: [103][125/233]	Loss 0.0008 (0.0072)	
training:	Epoch: [103][126/233]	Loss 0.0008 (0.0072)	
training:	Epoch: [103][127/233]	Loss 0.0011 (0.0071)	
training:	Epoch: [103][128/233]	Loss 0.0026 (0.0071)	
training:	Epoch: [103][129/233]	Loss 0.0009 (0.0071)	
training:	Epoch: [103][130/233]	Loss 0.0020 (0.0070)	
training:	Epoch: [103][131/233]	Loss 0.0213 (0.0071)	
training:	Epoch: [103][132/233]	Loss 0.0008 (0.0071)	
training:	Epoch: [103][133/233]	Loss 0.0008 (0.0070)	
training:	Epoch: [103][134/233]	Loss 0.0013 (0.0070)	
training:	Epoch: [103][135/233]	Loss 0.0011 (0.0069)	
training:	Epoch: [103][136/233]	Loss 0.0008 (0.0069)	
training:	Epoch: [103][137/233]	Loss 0.0295 (0.0071)	
training:	Epoch: [103][138/233]	Loss 0.0015 (0.0070)	
training:	Epoch: [103][139/233]	Loss 0.0017 (0.0070)	
training:	Epoch: [103][140/233]	Loss 0.0015 (0.0069)	
training:	Epoch: [103][141/233]	Loss 0.0010 (0.0069)	
training:	Epoch: [103][142/233]	Loss 0.0062 (0.0069)	
training:	Epoch: [103][143/233]	Loss 0.0011 (0.0069)	
training:	Epoch: [103][144/233]	Loss 0.0009 (0.0068)	
training:	Epoch: [103][145/233]	Loss 0.0016 (0.0068)	
training:	Epoch: [103][146/233]	Loss 0.0016 (0.0067)	
training:	Epoch: [103][147/233]	Loss 0.0008 (0.0067)	
training:	Epoch: [103][148/233]	Loss 0.0007 (0.0067)	
training:	Epoch: [103][149/233]	Loss 0.0051 (0.0067)	
training:	Epoch: [103][150/233]	Loss 0.0023 (0.0066)	
training:	Epoch: [103][151/233]	Loss 0.0009 (0.0066)	
training:	Epoch: [103][152/233]	Loss 0.0018 (0.0066)	
training:	Epoch: [103][153/233]	Loss 0.0057 (0.0066)	
training:	Epoch: [103][154/233]	Loss 0.0009 (0.0065)	
training:	Epoch: [103][155/233]	Loss 0.0009 (0.0065)	
training:	Epoch: [103][156/233]	Loss 0.0010 (0.0064)	
training:	Epoch: [103][157/233]	Loss 0.0010 (0.0064)	
training:	Epoch: [103][158/233]	Loss 0.0045 (0.0064)	
training:	Epoch: [103][159/233]	Loss 0.0009 (0.0064)	
training:	Epoch: [103][160/233]	Loss 0.0017 (0.0063)	
training:	Epoch: [103][161/233]	Loss 0.0008 (0.0063)	
training:	Epoch: [103][162/233]	Loss 0.0010 (0.0063)	
training:	Epoch: [103][163/233]	Loss 0.0050 (0.0063)	
training:	Epoch: [103][164/233]	Loss 0.0010 (0.0062)	
training:	Epoch: [103][165/233]	Loss 0.0017 (0.0062)	
training:	Epoch: [103][166/233]	Loss 0.0046 (0.0062)	
training:	Epoch: [103][167/233]	Loss 0.0160 (0.0062)	
training:	Epoch: [103][168/233]	Loss 0.0009 (0.0062)	
training:	Epoch: [103][169/233]	Loss 0.0019 (0.0062)	
training:	Epoch: [103][170/233]	Loss 0.0011 (0.0062)	
training:	Epoch: [103][171/233]	Loss 0.0011 (0.0061)	
training:	Epoch: [103][172/233]	Loss 0.0022 (0.0061)	
training:	Epoch: [103][173/233]	Loss 0.0019 (0.0061)	
training:	Epoch: [103][174/233]	Loss 0.0011 (0.0061)	
training:	Epoch: [103][175/233]	Loss 0.0008 (0.0060)	
training:	Epoch: [103][176/233]	Loss 0.0014 (0.0060)	
training:	Epoch: [103][177/233]	Loss 0.0063 (0.0060)	
training:	Epoch: [103][178/233]	Loss 0.0010 (0.0060)	
training:	Epoch: [103][179/233]	Loss 0.0008 (0.0059)	
training:	Epoch: [103][180/233]	Loss 0.0052 (0.0059)	
training:	Epoch: [103][181/233]	Loss 0.0013 (0.0059)	
training:	Epoch: [103][182/233]	Loss 0.0008 (0.0059)	
training:	Epoch: [103][183/233]	Loss 0.0015 (0.0059)	
training:	Epoch: [103][184/233]	Loss 0.0008 (0.0058)	
training:	Epoch: [103][185/233]	Loss 0.0010 (0.0058)	
training:	Epoch: [103][186/233]	Loss 0.0009 (0.0058)	
training:	Epoch: [103][187/233]	Loss 0.0010 (0.0058)	
training:	Epoch: [103][188/233]	Loss 0.0008 (0.0057)	
training:	Epoch: [103][189/233]	Loss 0.0007 (0.0057)	
training:	Epoch: [103][190/233]	Loss 0.0018 (0.0057)	
training:	Epoch: [103][191/233]	Loss 0.0010 (0.0057)	
training:	Epoch: [103][192/233]	Loss 0.0008 (0.0056)	
training:	Epoch: [103][193/233]	Loss 0.0008 (0.0056)	
training:	Epoch: [103][194/233]	Loss 0.0040 (0.0056)	
training:	Epoch: [103][195/233]	Loss 0.0012 (0.0056)	
training:	Epoch: [103][196/233]	Loss 0.0010 (0.0056)	
training:	Epoch: [103][197/233]	Loss 0.0029 (0.0055)	
training:	Epoch: [103][198/233]	Loss 0.0614 (0.0058)	
training:	Epoch: [103][199/233]	Loss 0.0008 (0.0058)	
training:	Epoch: [103][200/233]	Loss 0.0019 (0.0058)	
training:	Epoch: [103][201/233]	Loss 0.0012 (0.0058)	
training:	Epoch: [103][202/233]	Loss 0.0031 (0.0057)	
training:	Epoch: [103][203/233]	Loss 0.0009 (0.0057)	
training:	Epoch: [103][204/233]	Loss 0.0007 (0.0057)	
training:	Epoch: [103][205/233]	Loss 0.0009 (0.0057)	
training:	Epoch: [103][206/233]	Loss 0.0011 (0.0056)	
training:	Epoch: [103][207/233]	Loss 0.0011 (0.0056)	
training:	Epoch: [103][208/233]	Loss 0.0011 (0.0056)	
training:	Epoch: [103][209/233]	Loss 0.0011 (0.0056)	
training:	Epoch: [103][210/233]	Loss 0.0024 (0.0056)	
training:	Epoch: [103][211/233]	Loss 0.0010 (0.0055)	
training:	Epoch: [103][212/233]	Loss 0.0007 (0.0055)	
training:	Epoch: [103][213/233]	Loss 0.0007 (0.0055)	
training:	Epoch: [103][214/233]	Loss 0.0009 (0.0055)	
training:	Epoch: [103][215/233]	Loss 0.2147 (0.0065)	
training:	Epoch: [103][216/233]	Loss 0.0009 (0.0064)	
training:	Epoch: [103][217/233]	Loss 0.0010 (0.0064)	
training:	Epoch: [103][218/233]	Loss 0.0009 (0.0064)	
training:	Epoch: [103][219/233]	Loss 0.0012 (0.0064)	
training:	Epoch: [103][220/233]	Loss 0.0035 (0.0063)	
training:	Epoch: [103][221/233]	Loss 0.0019 (0.0063)	
training:	Epoch: [103][222/233]	Loss 0.0008 (0.0063)	
training:	Epoch: [103][223/233]	Loss 0.0008 (0.0063)	
training:	Epoch: [103][224/233]	Loss 0.0009 (0.0062)	
training:	Epoch: [103][225/233]	Loss 0.0009 (0.0062)	
training:	Epoch: [103][226/233]	Loss 0.0036 (0.0062)	
training:	Epoch: [103][227/233]	Loss 0.0012 (0.0062)	
training:	Epoch: [103][228/233]	Loss 0.0019 (0.0062)	
training:	Epoch: [103][229/233]	Loss 0.0012 (0.0061)	
training:	Epoch: [103][230/233]	Loss 0.0011 (0.0061)	
training:	Epoch: [103][231/233]	Loss 0.0007 (0.0061)	
training:	Epoch: [103][232/233]	Loss 0.0008 (0.0061)	
training:	Epoch: [103][233/233]	Loss 0.0018 (0.0061)	
Training:	 Loss: 0.0060

Training:	 ACC: 0.9997 0.9997 0.9997 0.9997
Validation:	 ACC: 0.7942 0.7935 0.7783 0.8101
Validation:	 Best_BACC: 0.8162 0.8154 0.7998 0.8326
Validation:	 Loss: 1.1246
Pretraining:	Epoch 104/200
----------
training:	Epoch: [104][1/233]	Loss 0.0009 (0.0009)	
training:	Epoch: [104][2/233]	Loss 0.0011 (0.0010)	
training:	Epoch: [104][3/233]	Loss 0.0008 (0.0009)	
training:	Epoch: [104][4/233]	Loss 0.0009 (0.0009)	
training:	Epoch: [104][5/233]	Loss 0.0009 (0.0009)	
training:	Epoch: [104][6/233]	Loss 0.0013 (0.0010)	
training:	Epoch: [104][7/233]	Loss 0.0009 (0.0010)	
training:	Epoch: [104][8/233]	Loss 0.0007 (0.0009)	
training:	Epoch: [104][9/233]	Loss 0.0007 (0.0009)	
training:	Epoch: [104][10/233]	Loss 0.0009 (0.0009)	
training:	Epoch: [104][11/233]	Loss 0.0008 (0.0009)	
training:	Epoch: [104][12/233]	Loss 0.0008 (0.0009)	
training:	Epoch: [104][13/233]	Loss 0.0319 (0.0033)	
training:	Epoch: [104][14/233]	Loss 0.0010 (0.0031)	
training:	Epoch: [104][15/233]	Loss 0.0008 (0.0030)	
training:	Epoch: [104][16/233]	Loss 0.0014 (0.0029)	
training:	Epoch: [104][17/233]	Loss 0.0010 (0.0028)	
training:	Epoch: [104][18/233]	Loss 0.0015 (0.0027)	
training:	Epoch: [104][19/233]	Loss 0.0013 (0.0026)	
training:	Epoch: [104][20/233]	Loss 0.0012 (0.0025)	
training:	Epoch: [104][21/233]	Loss 0.0011 (0.0025)	
training:	Epoch: [104][22/233]	Loss 0.0007 (0.0024)	
training:	Epoch: [104][23/233]	Loss 0.0035 (0.0024)	
training:	Epoch: [104][24/233]	Loss 0.0012 (0.0024)	
training:	Epoch: [104][25/233]	Loss 0.0009 (0.0023)	
training:	Epoch: [104][26/233]	Loss 0.0008 (0.0023)	
training:	Epoch: [104][27/233]	Loss 0.0050 (0.0024)	
training:	Epoch: [104][28/233]	Loss 0.0034 (0.0024)	
training:	Epoch: [104][29/233]	Loss 0.0007 (0.0024)	
training:	Epoch: [104][30/233]	Loss 0.0009 (0.0023)	
training:	Epoch: [104][31/233]	Loss 0.0038 (0.0024)	
training:	Epoch: [104][32/233]	Loss 0.0017 (0.0023)	
training:	Epoch: [104][33/233]	Loss 0.0014 (0.0023)	
training:	Epoch: [104][34/233]	Loss 0.0007 (0.0023)	
training:	Epoch: [104][35/233]	Loss 0.0007 (0.0022)	
training:	Epoch: [104][36/233]	Loss 0.0010 (0.0022)	
training:	Epoch: [104][37/233]	Loss 0.0016 (0.0022)	
training:	Epoch: [104][38/233]	Loss 0.0010 (0.0021)	
training:	Epoch: [104][39/233]	Loss 0.0011 (0.0021)	
training:	Epoch: [104][40/233]	Loss 0.0041 (0.0022)	
training:	Epoch: [104][41/233]	Loss 0.0008 (0.0021)	
training:	Epoch: [104][42/233]	Loss 0.0008 (0.0021)	
training:	Epoch: [104][43/233]	Loss 0.0009 (0.0021)	
training:	Epoch: [104][44/233]	Loss 0.0009 (0.0020)	
training:	Epoch: [104][45/233]	Loss 0.0008 (0.0020)	
training:	Epoch: [104][46/233]	Loss 0.0016 (0.0020)	
training:	Epoch: [104][47/233]	Loss 0.0052 (0.0021)	
training:	Epoch: [104][48/233]	Loss 0.0009 (0.0020)	
training:	Epoch: [104][49/233]	Loss 0.0008 (0.0020)	
training:	Epoch: [104][50/233]	Loss 0.0011 (0.0020)	
training:	Epoch: [104][51/233]	Loss 0.0104 (0.0022)	
training:	Epoch: [104][52/233]	Loss 0.0007 (0.0021)	
training:	Epoch: [104][53/233]	Loss 0.0019 (0.0021)	
training:	Epoch: [104][54/233]	Loss 0.0011 (0.0021)	
training:	Epoch: [104][55/233]	Loss 0.0008 (0.0021)	
training:	Epoch: [104][56/233]	Loss 0.0008 (0.0021)	
training:	Epoch: [104][57/233]	Loss 0.0008 (0.0020)	
training:	Epoch: [104][58/233]	Loss 0.0013 (0.0020)	
training:	Epoch: [104][59/233]	Loss 0.0014 (0.0020)	
training:	Epoch: [104][60/233]	Loss 0.0010 (0.0020)	
training:	Epoch: [104][61/233]	Loss 0.0008 (0.0020)	
training:	Epoch: [104][62/233]	Loss 0.0420 (0.0026)	
training:	Epoch: [104][63/233]	Loss 0.0008 (0.0026)	
training:	Epoch: [104][64/233]	Loss 0.0039 (0.0026)	
training:	Epoch: [104][65/233]	Loss 0.0007 (0.0026)	
training:	Epoch: [104][66/233]	Loss 0.0009 (0.0026)	
training:	Epoch: [104][67/233]	Loss 0.0009 (0.0025)	
training:	Epoch: [104][68/233]	Loss 0.0008 (0.0025)	
training:	Epoch: [104][69/233]	Loss 0.0008 (0.0025)	
training:	Epoch: [104][70/233]	Loss 0.0009 (0.0025)	
training:	Epoch: [104][71/233]	Loss 0.0009 (0.0024)	
training:	Epoch: [104][72/233]	Loss 0.0008 (0.0024)	
training:	Epoch: [104][73/233]	Loss 0.0016 (0.0024)	
training:	Epoch: [104][74/233]	Loss 0.0009 (0.0024)	
training:	Epoch: [104][75/233]	Loss 0.0011 (0.0024)	
training:	Epoch: [104][76/233]	Loss 0.0007 (0.0024)	
training:	Epoch: [104][77/233]	Loss 0.0008 (0.0023)	
training:	Epoch: [104][78/233]	Loss 0.0007 (0.0023)	
training:	Epoch: [104][79/233]	Loss 0.0023 (0.0023)	
training:	Epoch: [104][80/233]	Loss 0.0007 (0.0023)	
training:	Epoch: [104][81/233]	Loss 0.0007 (0.0023)	
training:	Epoch: [104][82/233]	Loss 0.0184 (0.0025)	
training:	Epoch: [104][83/233]	Loss 0.0010 (0.0024)	
training:	Epoch: [104][84/233]	Loss 0.0088 (0.0025)	
training:	Epoch: [104][85/233]	Loss 0.0009 (0.0025)	
training:	Epoch: [104][86/233]	Loss 0.0007 (0.0025)	
training:	Epoch: [104][87/233]	Loss 0.0008 (0.0025)	
training:	Epoch: [104][88/233]	Loss 0.0008 (0.0024)	
training:	Epoch: [104][89/233]	Loss 0.0011 (0.0024)	
training:	Epoch: [104][90/233]	Loss 0.0010 (0.0024)	
training:	Epoch: [104][91/233]	Loss 0.0011 (0.0024)	
training:	Epoch: [104][92/233]	Loss 0.0008 (0.0024)	
training:	Epoch: [104][93/233]	Loss 0.0008 (0.0024)	
training:	Epoch: [104][94/233]	Loss 0.0008 (0.0024)	
training:	Epoch: [104][95/233]	Loss 0.0008 (0.0023)	
training:	Epoch: [104][96/233]	Loss 0.0238 (0.0026)	
training:	Epoch: [104][97/233]	Loss 0.0007 (0.0025)	
training:	Epoch: [104][98/233]	Loss 0.0096 (0.0026)	
training:	Epoch: [104][99/233]	Loss 0.0009 (0.0026)	
training:	Epoch: [104][100/233]	Loss 0.0028 (0.0026)	
training:	Epoch: [104][101/233]	Loss 0.0008 (0.0026)	
training:	Epoch: [104][102/233]	Loss 0.0008 (0.0026)	
training:	Epoch: [104][103/233]	Loss 0.0010 (0.0025)	
training:	Epoch: [104][104/233]	Loss 0.0007 (0.0025)	
training:	Epoch: [104][105/233]	Loss 0.0010 (0.0025)	
training:	Epoch: [104][106/233]	Loss 0.0048 (0.0025)	
training:	Epoch: [104][107/233]	Loss 0.0007 (0.0025)	
training:	Epoch: [104][108/233]	Loss 0.0015 (0.0025)	
training:	Epoch: [104][109/233]	Loss 0.0007 (0.0025)	
training:	Epoch: [104][110/233]	Loss 0.0006 (0.0025)	
training:	Epoch: [104][111/233]	Loss 0.0049 (0.0025)	
training:	Epoch: [104][112/233]	Loss 0.0008 (0.0025)	
training:	Epoch: [104][113/233]	Loss 0.0014 (0.0025)	
training:	Epoch: [104][114/233]	Loss 0.0007 (0.0025)	
training:	Epoch: [104][115/233]	Loss 0.0007 (0.0024)	
training:	Epoch: [104][116/233]	Loss 0.0008 (0.0024)	
training:	Epoch: [104][117/233]	Loss 0.0010 (0.0024)	
training:	Epoch: [104][118/233]	Loss 0.1504 (0.0037)	
training:	Epoch: [104][119/233]	Loss 0.0065 (0.0037)	
training:	Epoch: [104][120/233]	Loss 0.0008 (0.0037)	
training:	Epoch: [104][121/233]	Loss 0.0007 (0.0036)	
training:	Epoch: [104][122/233]	Loss 0.0009 (0.0036)	
training:	Epoch: [104][123/233]	Loss 0.0008 (0.0036)	
training:	Epoch: [104][124/233]	Loss 0.0009 (0.0036)	
training:	Epoch: [104][125/233]	Loss 0.0009 (0.0036)	
training:	Epoch: [104][126/233]	Loss 0.0007 (0.0035)	
training:	Epoch: [104][127/233]	Loss 0.0013 (0.0035)	
training:	Epoch: [104][128/233]	Loss 0.0116 (0.0036)	
training:	Epoch: [104][129/233]	Loss 0.0015 (0.0036)	
training:	Epoch: [104][130/233]	Loss 0.0012 (0.0035)	
training:	Epoch: [104][131/233]	Loss 0.0011 (0.0035)	
training:	Epoch: [104][132/233]	Loss 0.0009 (0.0035)	
training:	Epoch: [104][133/233]	Loss 0.0009 (0.0035)	
training:	Epoch: [104][134/233]	Loss 0.0007 (0.0035)	
training:	Epoch: [104][135/233]	Loss 0.2130 (0.0050)	
training:	Epoch: [104][136/233]	Loss 0.0009 (0.0050)	
training:	Epoch: [104][137/233]	Loss 0.0029 (0.0050)	
training:	Epoch: [104][138/233]	Loss 0.0009 (0.0049)	
training:	Epoch: [104][139/233]	Loss 0.0008 (0.0049)	
training:	Epoch: [104][140/233]	Loss 0.0010 (0.0049)	
training:	Epoch: [104][141/233]	Loss 0.0008 (0.0049)	
training:	Epoch: [104][142/233]	Loss 0.0009 (0.0048)	
training:	Epoch: [104][143/233]	Loss 0.0010 (0.0048)	
training:	Epoch: [104][144/233]	Loss 0.0020 (0.0048)	
training:	Epoch: [104][145/233]	Loss 0.0007 (0.0048)	
training:	Epoch: [104][146/233]	Loss 0.0009 (0.0047)	
training:	Epoch: [104][147/233]	Loss 0.0008 (0.0047)	
training:	Epoch: [104][148/233]	Loss 0.0008 (0.0047)	
training:	Epoch: [104][149/233]	Loss 0.0008 (0.0046)	
training:	Epoch: [104][150/233]	Loss 0.0007 (0.0046)	
training:	Epoch: [104][151/233]	Loss 0.0006 (0.0046)	
training:	Epoch: [104][152/233]	Loss 0.0008 (0.0046)	
training:	Epoch: [104][153/233]	Loss 0.0009 (0.0045)	
training:	Epoch: [104][154/233]	Loss 0.0014 (0.0045)	
training:	Epoch: [104][155/233]	Loss 0.0009 (0.0045)	
training:	Epoch: [104][156/233]	Loss 0.0008 (0.0045)	
training:	Epoch: [104][157/233]	Loss 0.0007 (0.0045)	
training:	Epoch: [104][158/233]	Loss 0.0011 (0.0044)	
training:	Epoch: [104][159/233]	Loss 0.0009 (0.0044)	
training:	Epoch: [104][160/233]	Loss 0.0020 (0.0044)	
training:	Epoch: [104][161/233]	Loss 0.0009 (0.0044)	
training:	Epoch: [104][162/233]	Loss 0.0020 (0.0044)	
training:	Epoch: [104][163/233]	Loss 0.0010 (0.0043)	
training:	Epoch: [104][164/233]	Loss 0.0009 (0.0043)	
training:	Epoch: [104][165/233]	Loss 0.0011 (0.0043)	
training:	Epoch: [104][166/233]	Loss 0.0009 (0.0043)	
training:	Epoch: [104][167/233]	Loss 0.0007 (0.0043)	
training:	Epoch: [104][168/233]	Loss 0.0022 (0.0042)	
training:	Epoch: [104][169/233]	Loss 0.0009 (0.0042)	
training:	Epoch: [104][170/233]	Loss 0.0008 (0.0042)	
training:	Epoch: [104][171/233]	Loss 0.0008 (0.0042)	
training:	Epoch: [104][172/233]	Loss 0.0007 (0.0042)	
training:	Epoch: [104][173/233]	Loss 0.0013 (0.0041)	
training:	Epoch: [104][174/233]	Loss 0.0012 (0.0041)	
training:	Epoch: [104][175/233]	Loss 0.0008 (0.0041)	
training:	Epoch: [104][176/233]	Loss 0.0025 (0.0041)	
training:	Epoch: [104][177/233]	Loss 0.0008 (0.0041)	
training:	Epoch: [104][178/233]	Loss 0.0042 (0.0041)	
training:	Epoch: [104][179/233]	Loss 0.0008 (0.0041)	
training:	Epoch: [104][180/233]	Loss 0.0008 (0.0040)	
training:	Epoch: [104][181/233]	Loss 0.0007 (0.0040)	
training:	Epoch: [104][182/233]	Loss 0.0008 (0.0040)	
training:	Epoch: [104][183/233]	Loss 0.0017 (0.0040)	
training:	Epoch: [104][184/233]	Loss 0.0008 (0.0040)	
training:	Epoch: [104][185/233]	Loss 0.0010 (0.0040)	
training:	Epoch: [104][186/233]	Loss 0.0280 (0.0041)	
training:	Epoch: [104][187/233]	Loss 0.0008 (0.0041)	
training:	Epoch: [104][188/233]	Loss 0.0010 (0.0041)	
training:	Epoch: [104][189/233]	Loss 0.0007 (0.0040)	
training:	Epoch: [104][190/233]	Loss 0.1770 (0.0050)	
training:	Epoch: [104][191/233]	Loss 0.0007 (0.0049)	
training:	Epoch: [104][192/233]	Loss 0.0173 (0.0050)	
training:	Epoch: [104][193/233]	Loss 0.0007 (0.0050)	
training:	Epoch: [104][194/233]	Loss 0.0008 (0.0049)	
training:	Epoch: [104][195/233]	Loss 0.0014 (0.0049)	
training:	Epoch: [104][196/233]	Loss 0.0009 (0.0049)	
training:	Epoch: [104][197/233]	Loss 0.0008 (0.0049)	
training:	Epoch: [104][198/233]	Loss 0.0011 (0.0049)	
training:	Epoch: [104][199/233]	Loss 0.0008 (0.0049)	
training:	Epoch: [104][200/233]	Loss 0.0007 (0.0048)	
training:	Epoch: [104][201/233]	Loss 0.0024 (0.0048)	
training:	Epoch: [104][202/233]	Loss 0.0008 (0.0048)	
training:	Epoch: [104][203/233]	Loss 0.0018 (0.0048)	
training:	Epoch: [104][204/233]	Loss 0.0008 (0.0048)	
training:	Epoch: [104][205/233]	Loss 0.0013 (0.0047)	
training:	Epoch: [104][206/233]	Loss 0.0007 (0.0047)	
training:	Epoch: [104][207/233]	Loss 0.0007 (0.0047)	
training:	Epoch: [104][208/233]	Loss 0.0136 (0.0047)	
training:	Epoch: [104][209/233]	Loss 0.0008 (0.0047)	
training:	Epoch: [104][210/233]	Loss 0.0009 (0.0047)	
training:	Epoch: [104][211/233]	Loss 0.2361 (0.0058)	
training:	Epoch: [104][212/233]	Loss 0.0015 (0.0058)	
training:	Epoch: [104][213/233]	Loss 0.0011 (0.0058)	
training:	Epoch: [104][214/233]	Loss 0.0199 (0.0058)	
training:	Epoch: [104][215/233]	Loss 0.0009 (0.0058)	
training:	Epoch: [104][216/233]	Loss 0.0008 (0.0058)	
training:	Epoch: [104][217/233]	Loss 0.0008 (0.0058)	
training:	Epoch: [104][218/233]	Loss 0.0008 (0.0057)	
training:	Epoch: [104][219/233]	Loss 0.0048 (0.0057)	
training:	Epoch: [104][220/233]	Loss 0.0138 (0.0058)	
training:	Epoch: [104][221/233]	Loss 0.0044 (0.0058)	
training:	Epoch: [104][222/233]	Loss 0.0009 (0.0057)	
training:	Epoch: [104][223/233]	Loss 0.0007 (0.0057)	
training:	Epoch: [104][224/233]	Loss 0.0010 (0.0057)	
training:	Epoch: [104][225/233]	Loss 0.0016 (0.0057)	
training:	Epoch: [104][226/233]	Loss 0.0008 (0.0057)	
training:	Epoch: [104][227/233]	Loss 0.0012 (0.0056)	
training:	Epoch: [104][228/233]	Loss 0.0013 (0.0056)	
training:	Epoch: [104][229/233]	Loss 0.0009 (0.0056)	
training:	Epoch: [104][230/233]	Loss 0.0598 (0.0058)	
training:	Epoch: [104][231/233]	Loss 0.0059 (0.0058)	
training:	Epoch: [104][232/233]	Loss 0.0007 (0.0058)	
training:	Epoch: [104][233/233]	Loss 0.0009 (0.0058)	
Training:	 Loss: 0.0058

Training:	 ACC: 0.9997 0.9997 0.9997 0.9997
Validation:	 ACC: 0.7920 0.7919 0.7896 0.7944
Validation:	 Best_BACC: 0.8162 0.8154 0.7998 0.8326
Validation:	 Loss: 1.1282
Pretraining:	Epoch 105/200
----------
training:	Epoch: [105][1/233]	Loss 0.0012 (0.0012)	
training:	Epoch: [105][2/233]	Loss 0.0240 (0.0126)	
training:	Epoch: [105][3/233]	Loss 0.0008 (0.0087)	
training:	Epoch: [105][4/233]	Loss 0.0037 (0.0074)	
training:	Epoch: [105][5/233]	Loss 0.0008 (0.0061)	
training:	Epoch: [105][6/233]	Loss 0.0010 (0.0052)	
training:	Epoch: [105][7/233]	Loss 0.0007 (0.0046)	
training:	Epoch: [105][8/233]	Loss 0.0009 (0.0041)	
training:	Epoch: [105][9/233]	Loss 0.0007 (0.0038)	
training:	Epoch: [105][10/233]	Loss 0.0008 (0.0035)	
training:	Epoch: [105][11/233]	Loss 0.2358 (0.0246)	
training:	Epoch: [105][12/233]	Loss 0.0009 (0.0226)	
training:	Epoch: [105][13/233]	Loss 0.0012 (0.0210)	
training:	Epoch: [105][14/233]	Loss 0.0014 (0.0196)	
training:	Epoch: [105][15/233]	Loss 0.0009 (0.0183)	
training:	Epoch: [105][16/233]	Loss 0.0026 (0.0173)	
training:	Epoch: [105][17/233]	Loss 0.0008 (0.0164)	
training:	Epoch: [105][18/233]	Loss 0.0024 (0.0156)	
training:	Epoch: [105][19/233]	Loss 0.0328 (0.0165)	
training:	Epoch: [105][20/233]	Loss 0.0013 (0.0157)	
training:	Epoch: [105][21/233]	Loss 0.0009 (0.0150)	
training:	Epoch: [105][22/233]	Loss 0.0029 (0.0145)	
training:	Epoch: [105][23/233]	Loss 0.1083 (0.0186)	
training:	Epoch: [105][24/233]	Loss 0.0066 (0.0181)	
training:	Epoch: [105][25/233]	Loss 0.0009 (0.0174)	
training:	Epoch: [105][26/233]	Loss 0.0089 (0.0170)	
training:	Epoch: [105][27/233]	Loss 0.0007 (0.0164)	
training:	Epoch: [105][28/233]	Loss 0.0009 (0.0159)	
training:	Epoch: [105][29/233]	Loss 0.0009 (0.0154)	
training:	Epoch: [105][30/233]	Loss 0.0009 (0.0149)	
training:	Epoch: [105][31/233]	Loss 0.0012 (0.0144)	
training:	Epoch: [105][32/233]	Loss 0.0008 (0.0140)	
training:	Epoch: [105][33/233]	Loss 0.0021 (0.0137)	
training:	Epoch: [105][34/233]	Loss 0.0008 (0.0133)	
training:	Epoch: [105][35/233]	Loss 0.0011 (0.0129)	
training:	Epoch: [105][36/233]	Loss 0.0009 (0.0126)	
training:	Epoch: [105][37/233]	Loss 0.0322 (0.0131)	
training:	Epoch: [105][38/233]	Loss 0.0007 (0.0128)	
training:	Epoch: [105][39/233]	Loss 0.0008 (0.0125)	
training:	Epoch: [105][40/233]	Loss 0.0014 (0.0122)	
training:	Epoch: [105][41/233]	Loss 0.0008 (0.0119)	
training:	Epoch: [105][42/233]	Loss 0.0007 (0.0117)	
training:	Epoch: [105][43/233]	Loss 0.0314 (0.0121)	
training:	Epoch: [105][44/233]	Loss 0.0010 (0.0119)	
training:	Epoch: [105][45/233]	Loss 0.0009 (0.0116)	
training:	Epoch: [105][46/233]	Loss 0.0008 (0.0114)	
training:	Epoch: [105][47/233]	Loss 0.0424 (0.0121)	
training:	Epoch: [105][48/233]	Loss 0.0007 (0.0118)	
training:	Epoch: [105][49/233]	Loss 0.0011 (0.0116)	
training:	Epoch: [105][50/233]	Loss 0.0007 (0.0114)	
training:	Epoch: [105][51/233]	Loss 0.0009 (0.0112)	
training:	Epoch: [105][52/233]	Loss 0.0010 (0.0110)	
training:	Epoch: [105][53/233]	Loss 0.0014 (0.0108)	
training:	Epoch: [105][54/233]	Loss 0.0092 (0.0108)	
training:	Epoch: [105][55/233]	Loss 0.2102 (0.0144)	
training:	Epoch: [105][56/233]	Loss 0.0007 (0.0141)	
training:	Epoch: [105][57/233]	Loss 0.0008 (0.0139)	
training:	Epoch: [105][58/233]	Loss 0.0009 (0.0137)	
training:	Epoch: [105][59/233]	Loss 0.0008 (0.0135)	
training:	Epoch: [105][60/233]	Loss 0.0021 (0.0133)	
training:	Epoch: [105][61/233]	Loss 0.0008 (0.0131)	
training:	Epoch: [105][62/233]	Loss 0.0009 (0.0129)	
training:	Epoch: [105][63/233]	Loss 0.0007 (0.0127)	
training:	Epoch: [105][64/233]	Loss 0.0011 (0.0125)	
training:	Epoch: [105][65/233]	Loss 0.0019 (0.0123)	
training:	Epoch: [105][66/233]	Loss 0.0011 (0.0122)	
training:	Epoch: [105][67/233]	Loss 0.0062 (0.0121)	
training:	Epoch: [105][68/233]	Loss 0.0007 (0.0119)	
training:	Epoch: [105][69/233]	Loss 0.0009 (0.0118)	
training:	Epoch: [105][70/233]	Loss 0.0011 (0.0116)	
training:	Epoch: [105][71/233]	Loss 0.0008 (0.0115)	
training:	Epoch: [105][72/233]	Loss 0.0010 (0.0113)	
training:	Epoch: [105][73/233]	Loss 0.0275 (0.0115)	
training:	Epoch: [105][74/233]	Loss 0.0008 (0.0114)	
training:	Epoch: [105][75/233]	Loss 0.0015 (0.0113)	
training:	Epoch: [105][76/233]	Loss 0.0011 (0.0111)	
training:	Epoch: [105][77/233]	Loss 0.0014 (0.0110)	
training:	Epoch: [105][78/233]	Loss 0.0008 (0.0109)	
training:	Epoch: [105][79/233]	Loss 0.0008 (0.0107)	
training:	Epoch: [105][80/233]	Loss 0.0010 (0.0106)	
training:	Epoch: [105][81/233]	Loss 0.0008 (0.0105)	
training:	Epoch: [105][82/233]	Loss 0.0011 (0.0104)	
training:	Epoch: [105][83/233]	Loss 0.0009 (0.0103)	
training:	Epoch: [105][84/233]	Loss 0.0009 (0.0102)	
training:	Epoch: [105][85/233]	Loss 0.0007 (0.0100)	
training:	Epoch: [105][86/233]	Loss 0.0007 (0.0099)	
training:	Epoch: [105][87/233]	Loss 0.0021 (0.0098)	
training:	Epoch: [105][88/233]	Loss 0.0008 (0.0097)	
training:	Epoch: [105][89/233]	Loss 0.0013 (0.0096)	
training:	Epoch: [105][90/233]	Loss 0.0012 (0.0095)	
training:	Epoch: [105][91/233]	Loss 0.0335 (0.0098)	
training:	Epoch: [105][92/233]	Loss 0.0009 (0.0097)	
training:	Epoch: [105][93/233]	Loss 0.0017 (0.0096)	
training:	Epoch: [105][94/233]	Loss 0.0011 (0.0095)	
training:	Epoch: [105][95/233]	Loss 0.0212 (0.0097)	
training:	Epoch: [105][96/233]	Loss 0.0010 (0.0096)	
training:	Epoch: [105][97/233]	Loss 0.0008 (0.0095)	
training:	Epoch: [105][98/233]	Loss 0.0008 (0.0094)	
training:	Epoch: [105][99/233]	Loss 0.0008 (0.0093)	
training:	Epoch: [105][100/233]	Loss 0.0007 (0.0092)	
training:	Epoch: [105][101/233]	Loss 0.0009 (0.0091)	
training:	Epoch: [105][102/233]	Loss 0.0010 (0.0091)	
training:	Epoch: [105][103/233]	Loss 0.0015 (0.0090)	
training:	Epoch: [105][104/233]	Loss 0.0007 (0.0089)	
training:	Epoch: [105][105/233]	Loss 0.0015 (0.0088)	
training:	Epoch: [105][106/233]	Loss 0.0007 (0.0088)	
training:	Epoch: [105][107/233]	Loss 0.0017 (0.0087)	
training:	Epoch: [105][108/233]	Loss 0.0011 (0.0086)	
training:	Epoch: [105][109/233]	Loss 0.0012 (0.0086)	
training:	Epoch: [105][110/233]	Loss 0.0008 (0.0085)	
training:	Epoch: [105][111/233]	Loss 0.0027 (0.0084)	
training:	Epoch: [105][112/233]	Loss 0.0010 (0.0084)	
training:	Epoch: [105][113/233]	Loss 0.0020 (0.0083)	
training:	Epoch: [105][114/233]	Loss 0.0009 (0.0082)	
training:	Epoch: [105][115/233]	Loss 0.0044 (0.0082)	
training:	Epoch: [105][116/233]	Loss 0.0007 (0.0081)	
training:	Epoch: [105][117/233]	Loss 0.0088 (0.0081)	
training:	Epoch: [105][118/233]	Loss 0.0006 (0.0081)	
training:	Epoch: [105][119/233]	Loss 0.0008 (0.0080)	
training:	Epoch: [105][120/233]	Loss 0.0007 (0.0080)	
training:	Epoch: [105][121/233]	Loss 0.0007 (0.0079)	
training:	Epoch: [105][122/233]	Loss 0.0010 (0.0078)	
training:	Epoch: [105][123/233]	Loss 0.0070 (0.0078)	
training:	Epoch: [105][124/233]	Loss 0.0006 (0.0078)	
training:	Epoch: [105][125/233]	Loss 0.0014 (0.0077)	
training:	Epoch: [105][126/233]	Loss 0.0010 (0.0077)	
training:	Epoch: [105][127/233]	Loss 0.0010 (0.0076)	
training:	Epoch: [105][128/233]	Loss 0.1794 (0.0090)	
training:	Epoch: [105][129/233]	Loss 0.0011 (0.0089)	
training:	Epoch: [105][130/233]	Loss 0.0019 (0.0089)	
training:	Epoch: [105][131/233]	Loss 0.0008 (0.0088)	
training:	Epoch: [105][132/233]	Loss 0.0008 (0.0087)	
training:	Epoch: [105][133/233]	Loss 0.0009 (0.0087)	
training:	Epoch: [105][134/233]	Loss 0.0007 (0.0086)	
training:	Epoch: [105][135/233]	Loss 0.0010 (0.0086)	
training:	Epoch: [105][136/233]	Loss 0.0010 (0.0085)	
training:	Epoch: [105][137/233]	Loss 0.0019 (0.0084)	
training:	Epoch: [105][138/233]	Loss 0.0007 (0.0084)	
training:	Epoch: [105][139/233]	Loss 0.0009 (0.0083)	
training:	Epoch: [105][140/233]	Loss 0.0009 (0.0083)	
training:	Epoch: [105][141/233]	Loss 0.0007 (0.0082)	
training:	Epoch: [105][142/233]	Loss 0.0156 (0.0083)	
training:	Epoch: [105][143/233]	Loss 0.0008 (0.0082)	
training:	Epoch: [105][144/233]	Loss 0.0009 (0.0082)	
training:	Epoch: [105][145/233]	Loss 0.0010 (0.0081)	
training:	Epoch: [105][146/233]	Loss 0.0008 (0.0081)	
training:	Epoch: [105][147/233]	Loss 0.0007 (0.0080)	
training:	Epoch: [105][148/233]	Loss 0.0009 (0.0080)	
training:	Epoch: [105][149/233]	Loss 0.0007 (0.0079)	
training:	Epoch: [105][150/233]	Loss 0.0006 (0.0079)	
training:	Epoch: [105][151/233]	Loss 0.0010 (0.0078)	
training:	Epoch: [105][152/233]	Loss 0.0007 (0.0078)	
training:	Epoch: [105][153/233]	Loss 0.0012 (0.0077)	
training:	Epoch: [105][154/233]	Loss 0.0010 (0.0077)	
training:	Epoch: [105][155/233]	Loss 0.0012 (0.0077)	
training:	Epoch: [105][156/233]	Loss 0.0094 (0.0077)	
training:	Epoch: [105][157/233]	Loss 0.0013 (0.0076)	
training:	Epoch: [105][158/233]	Loss 0.0007 (0.0076)	
training:	Epoch: [105][159/233]	Loss 0.0609 (0.0079)	
training:	Epoch: [105][160/233]	Loss 0.0015 (0.0079)	
training:	Epoch: [105][161/233]	Loss 0.0008 (0.0078)	
training:	Epoch: [105][162/233]	Loss 0.0010 (0.0078)	
training:	Epoch: [105][163/233]	Loss 0.0008 (0.0078)	
training:	Epoch: [105][164/233]	Loss 0.0007 (0.0077)	
training:	Epoch: [105][165/233]	Loss 0.0007 (0.0077)	
training:	Epoch: [105][166/233]	Loss 0.0007 (0.0076)	
training:	Epoch: [105][167/233]	Loss 0.0012 (0.0076)	
training:	Epoch: [105][168/233]	Loss 0.0041 (0.0076)	
training:	Epoch: [105][169/233]	Loss 0.0014 (0.0075)	
training:	Epoch: [105][170/233]	Loss 0.0012 (0.0075)	
training:	Epoch: [105][171/233]	Loss 0.0009 (0.0075)	
training:	Epoch: [105][172/233]	Loss 0.0010 (0.0074)	
training:	Epoch: [105][173/233]	Loss 0.0008 (0.0074)	
training:	Epoch: [105][174/233]	Loss 0.0008 (0.0073)	
training:	Epoch: [105][175/233]	Loss 0.0012 (0.0073)	
training:	Epoch: [105][176/233]	Loss 0.0010 (0.0073)	
training:	Epoch: [105][177/233]	Loss 0.0963 (0.0078)	
training:	Epoch: [105][178/233]	Loss 0.0007 (0.0077)	
training:	Epoch: [105][179/233]	Loss 0.0333 (0.0079)	
training:	Epoch: [105][180/233]	Loss 0.0007 (0.0078)	
training:	Epoch: [105][181/233]	Loss 0.0007 (0.0078)	
training:	Epoch: [105][182/233]	Loss 0.0011 (0.0078)	
training:	Epoch: [105][183/233]	Loss 0.0007 (0.0077)	
training:	Epoch: [105][184/233]	Loss 0.0008 (0.0077)	
training:	Epoch: [105][185/233]	Loss 0.0907 (0.0081)	
training:	Epoch: [105][186/233]	Loss 0.0008 (0.0081)	
training:	Epoch: [105][187/233]	Loss 0.0010 (0.0081)	
training:	Epoch: [105][188/233]	Loss 0.0058 (0.0080)	
training:	Epoch: [105][189/233]	Loss 0.0010 (0.0080)	
training:	Epoch: [105][190/233]	Loss 0.0009 (0.0080)	
training:	Epoch: [105][191/233]	Loss 0.0057 (0.0080)	
training:	Epoch: [105][192/233]	Loss 0.0011 (0.0079)	
training:	Epoch: [105][193/233]	Loss 0.0007 (0.0079)	
training:	Epoch: [105][194/233]	Loss 0.0008 (0.0078)	
training:	Epoch: [105][195/233]	Loss 0.0213 (0.0079)	
training:	Epoch: [105][196/233]	Loss 0.2116 (0.0090)	
training:	Epoch: [105][197/233]	Loss 0.0009 (0.0089)	
training:	Epoch: [105][198/233]	Loss 0.0015 (0.0089)	
training:	Epoch: [105][199/233]	Loss 0.0006 (0.0088)	
training:	Epoch: [105][200/233]	Loss 0.0013 (0.0088)	
training:	Epoch: [105][201/233]	Loss 0.0386 (0.0089)	
training:	Epoch: [105][202/233]	Loss 0.0010 (0.0089)	
training:	Epoch: [105][203/233]	Loss 0.0006 (0.0089)	
training:	Epoch: [105][204/233]	Loss 0.0006 (0.0088)	
training:	Epoch: [105][205/233]	Loss 0.0015 (0.0088)	
training:	Epoch: [105][206/233]	Loss 0.0008 (0.0088)	
training:	Epoch: [105][207/233]	Loss 0.0015 (0.0087)	
training:	Epoch: [105][208/233]	Loss 0.0073 (0.0087)	
training:	Epoch: [105][209/233]	Loss 0.0009 (0.0087)	
training:	Epoch: [105][210/233]	Loss 0.0008 (0.0086)	
training:	Epoch: [105][211/233]	Loss 0.0008 (0.0086)	
training:	Epoch: [105][212/233]	Loss 0.0009 (0.0086)	
training:	Epoch: [105][213/233]	Loss 0.0009 (0.0085)	
training:	Epoch: [105][214/233]	Loss 0.0008 (0.0085)	
training:	Epoch: [105][215/233]	Loss 0.0010 (0.0085)	
training:	Epoch: [105][216/233]	Loss 0.0008 (0.0084)	
training:	Epoch: [105][217/233]	Loss 0.0008 (0.0084)	
training:	Epoch: [105][218/233]	Loss 0.0058 (0.0084)	
training:	Epoch: [105][219/233]	Loss 0.0011 (0.0083)	
training:	Epoch: [105][220/233]	Loss 0.0007 (0.0083)	
training:	Epoch: [105][221/233]	Loss 0.0009 (0.0083)	
training:	Epoch: [105][222/233]	Loss 0.0007 (0.0082)	
training:	Epoch: [105][223/233]	Loss 0.0013 (0.0082)	
training:	Epoch: [105][224/233]	Loss 0.0010 (0.0082)	
training:	Epoch: [105][225/233]	Loss 0.0009 (0.0081)	
training:	Epoch: [105][226/233]	Loss 0.0007 (0.0081)	
training:	Epoch: [105][227/233]	Loss 0.0029 (0.0081)	
training:	Epoch: [105][228/233]	Loss 0.0010 (0.0081)	
training:	Epoch: [105][229/233]	Loss 0.0007 (0.0080)	
training:	Epoch: [105][230/233]	Loss 0.0009 (0.0080)	
training:	Epoch: [105][231/233]	Loss 0.2186 (0.0089)	
training:	Epoch: [105][232/233]	Loss 0.0011 (0.0089)	
training:	Epoch: [105][233/233]	Loss 0.0014 (0.0088)	
Training:	 Loss: 0.0088

Training:	 ACC: 0.9997 0.9997 0.9997 0.9997
Validation:	 ACC: 0.7982 0.7978 0.7886 0.8079
Validation:	 Best_BACC: 0.8162 0.8154 0.7998 0.8326
Validation:	 Loss: 1.1310
Pretraining:	Epoch 106/200
----------
training:	Epoch: [106][1/233]	Loss 0.0007 (0.0007)	
training:	Epoch: [106][2/233]	Loss 0.0008 (0.0008)	
training:	Epoch: [106][3/233]	Loss 0.0009 (0.0008)	
training:	Epoch: [106][4/233]	Loss 0.0009 (0.0008)	
training:	Epoch: [106][5/233]	Loss 0.0012 (0.0009)	
training:	Epoch: [106][6/233]	Loss 0.0018 (0.0011)	
training:	Epoch: [106][7/233]	Loss 0.0007 (0.0010)	
training:	Epoch: [106][8/233]	Loss 0.0007 (0.0010)	
training:	Epoch: [106][9/233]	Loss 0.0007 (0.0009)	
training:	Epoch: [106][10/233]	Loss 0.0011 (0.0010)	
training:	Epoch: [106][11/233]	Loss 0.0016 (0.0010)	
training:	Epoch: [106][12/233]	Loss 0.0009 (0.0010)	
training:	Epoch: [106][13/233]	Loss 0.0012 (0.0010)	
training:	Epoch: [106][14/233]	Loss 0.2358 (0.0178)	
training:	Epoch: [106][15/233]	Loss 0.0008 (0.0167)	
training:	Epoch: [106][16/233]	Loss 0.0012 (0.0157)	
training:	Epoch: [106][17/233]	Loss 0.0034 (0.0150)	
training:	Epoch: [106][18/233]	Loss 0.0021 (0.0142)	
training:	Epoch: [106][19/233]	Loss 0.0017 (0.0136)	
training:	Epoch: [106][20/233]	Loss 0.0007 (0.0129)	
training:	Epoch: [106][21/233]	Loss 0.0007 (0.0124)	
training:	Epoch: [106][22/233]	Loss 0.0017 (0.0119)	
training:	Epoch: [106][23/233]	Loss 0.0022 (0.0115)	
training:	Epoch: [106][24/233]	Loss 0.0028 (0.0111)	
training:	Epoch: [106][25/233]	Loss 0.0008 (0.0107)	
training:	Epoch: [106][26/233]	Loss 0.0010 (0.0103)	
training:	Epoch: [106][27/233]	Loss 0.0009 (0.0100)	
training:	Epoch: [106][28/233]	Loss 0.0008 (0.0096)	
training:	Epoch: [106][29/233]	Loss 0.0007 (0.0093)	
training:	Epoch: [106][30/233]	Loss 0.0704 (0.0114)	
training:	Epoch: [106][31/233]	Loss 0.0014 (0.0110)	
training:	Epoch: [106][32/233]	Loss 0.0010 (0.0107)	
training:	Epoch: [106][33/233]	Loss 0.0009 (0.0104)	
training:	Epoch: [106][34/233]	Loss 0.0008 (0.0101)	
training:	Epoch: [106][35/233]	Loss 0.0048 (0.0100)	
training:	Epoch: [106][36/233]	Loss 0.0008 (0.0097)	
training:	Epoch: [106][37/233]	Loss 0.0009 (0.0095)	
training:	Epoch: [106][38/233]	Loss 0.0012 (0.0093)	
training:	Epoch: [106][39/233]	Loss 0.0007 (0.0091)	
training:	Epoch: [106][40/233]	Loss 0.0007 (0.0089)	
training:	Epoch: [106][41/233]	Loss 0.0009 (0.0087)	
training:	Epoch: [106][42/233]	Loss 0.0008 (0.0085)	
training:	Epoch: [106][43/233]	Loss 0.0008 (0.0083)	
training:	Epoch: [106][44/233]	Loss 0.0007 (0.0081)	
training:	Epoch: [106][45/233]	Loss 0.0058 (0.0081)	
training:	Epoch: [106][46/233]	Loss 0.0008 (0.0079)	
training:	Epoch: [106][47/233]	Loss 0.0007 (0.0078)	
training:	Epoch: [106][48/233]	Loss 0.0016 (0.0076)	
training:	Epoch: [106][49/233]	Loss 0.0010 (0.0075)	
training:	Epoch: [106][50/233]	Loss 0.0007 (0.0074)	
training:	Epoch: [106][51/233]	Loss 0.0007 (0.0072)	
training:	Epoch: [106][52/233]	Loss 0.0024 (0.0071)	
training:	Epoch: [106][53/233]	Loss 0.0010 (0.0070)	
training:	Epoch: [106][54/233]	Loss 0.0084 (0.0070)	
training:	Epoch: [106][55/233]	Loss 0.0007 (0.0069)	
training:	Epoch: [106][56/233]	Loss 0.1240 (0.0090)	
training:	Epoch: [106][57/233]	Loss 0.0007 (0.0089)	
training:	Epoch: [106][58/233]	Loss 0.0011 (0.0087)	
training:	Epoch: [106][59/233]	Loss 0.0008 (0.0086)	
training:	Epoch: [106][60/233]	Loss 0.0007 (0.0085)	
training:	Epoch: [106][61/233]	Loss 0.0019 (0.0084)	
training:	Epoch: [106][62/233]	Loss 0.0008 (0.0082)	
training:	Epoch: [106][63/233]	Loss 0.0031 (0.0082)	
training:	Epoch: [106][64/233]	Loss 0.0007 (0.0080)	
training:	Epoch: [106][65/233]	Loss 0.0007 (0.0079)	
training:	Epoch: [106][66/233]	Loss 0.0009 (0.0078)	
training:	Epoch: [106][67/233]	Loss 0.0008 (0.0077)	
training:	Epoch: [106][68/233]	Loss 0.0008 (0.0076)	
training:	Epoch: [106][69/233]	Loss 0.0010 (0.0075)	
training:	Epoch: [106][70/233]	Loss 0.0007 (0.0074)	
training:	Epoch: [106][71/233]	Loss 0.0008 (0.0073)	
training:	Epoch: [106][72/233]	Loss 0.0007 (0.0072)	
training:	Epoch: [106][73/233]	Loss 0.0014 (0.0072)	
training:	Epoch: [106][74/233]	Loss 0.0008 (0.0071)	
training:	Epoch: [106][75/233]	Loss 0.0008 (0.0070)	
training:	Epoch: [106][76/233]	Loss 0.0008 (0.0069)	
training:	Epoch: [106][77/233]	Loss 0.0010 (0.0068)	
training:	Epoch: [106][78/233]	Loss 0.0007 (0.0067)	
training:	Epoch: [106][79/233]	Loss 0.0009 (0.0067)	
training:	Epoch: [106][80/233]	Loss 0.0012 (0.0066)	
training:	Epoch: [106][81/233]	Loss 0.0015 (0.0065)	
training:	Epoch: [106][82/233]	Loss 0.0008 (0.0065)	
training:	Epoch: [106][83/233]	Loss 0.0009 (0.0064)	
training:	Epoch: [106][84/233]	Loss 0.0027 (0.0064)	
training:	Epoch: [106][85/233]	Loss 0.0008 (0.0063)	
training:	Epoch: [106][86/233]	Loss 0.0014 (0.0062)	
training:	Epoch: [106][87/233]	Loss 0.0040 (0.0062)	
training:	Epoch: [106][88/233]	Loss 0.0015 (0.0062)	
training:	Epoch: [106][89/233]	Loss 0.0025 (0.0061)	
training:	Epoch: [106][90/233]	Loss 0.0008 (0.0061)	
training:	Epoch: [106][91/233]	Loss 0.0009 (0.0060)	
training:	Epoch: [106][92/233]	Loss 0.0011 (0.0060)	
training:	Epoch: [106][93/233]	Loss 0.0040 (0.0059)	
training:	Epoch: [106][94/233]	Loss 0.0540 (0.0064)	
training:	Epoch: [106][95/233]	Loss 0.1104 (0.0075)	
training:	Epoch: [106][96/233]	Loss 0.0007 (0.0075)	
training:	Epoch: [106][97/233]	Loss 0.0007 (0.0074)	
training:	Epoch: [106][98/233]	Loss 0.0008 (0.0073)	
training:	Epoch: [106][99/233]	Loss 0.0007 (0.0073)	
training:	Epoch: [106][100/233]	Loss 0.0039 (0.0072)	
training:	Epoch: [106][101/233]	Loss 0.0037 (0.0072)	
training:	Epoch: [106][102/233]	Loss 0.0008 (0.0071)	
training:	Epoch: [106][103/233]	Loss 0.0008 (0.0071)	
training:	Epoch: [106][104/233]	Loss 0.0007 (0.0070)	
training:	Epoch: [106][105/233]	Loss 0.0009 (0.0069)	
training:	Epoch: [106][106/233]	Loss 0.0011 (0.0069)	
training:	Epoch: [106][107/233]	Loss 0.0010 (0.0068)	
training:	Epoch: [106][108/233]	Loss 0.0007 (0.0068)	
training:	Epoch: [106][109/233]	Loss 0.0210 (0.0069)	
training:	Epoch: [106][110/233]	Loss 0.0008 (0.0069)	
training:	Epoch: [106][111/233]	Loss 0.0008 (0.0068)	
training:	Epoch: [106][112/233]	Loss 0.0010 (0.0067)	
training:	Epoch: [106][113/233]	Loss 0.0018 (0.0067)	
training:	Epoch: [106][114/233]	Loss 0.0008 (0.0067)	
training:	Epoch: [106][115/233]	Loss 0.0009 (0.0066)	
training:	Epoch: [106][116/233]	Loss 0.0007 (0.0066)	
training:	Epoch: [106][117/233]	Loss 0.0006 (0.0065)	
training:	Epoch: [106][118/233]	Loss 0.0007 (0.0065)	
training:	Epoch: [106][119/233]	Loss 0.0008 (0.0064)	
training:	Epoch: [106][120/233]	Loss 0.0031 (0.0064)	
training:	Epoch: [106][121/233]	Loss 0.0008 (0.0063)	
training:	Epoch: [106][122/233]	Loss 0.0009 (0.0063)	
training:	Epoch: [106][123/233]	Loss 0.0011 (0.0062)	
training:	Epoch: [106][124/233]	Loss 0.0015 (0.0062)	
training:	Epoch: [106][125/233]	Loss 0.0421 (0.0065)	
training:	Epoch: [106][126/233]	Loss 0.0015 (0.0065)	
training:	Epoch: [106][127/233]	Loss 0.0010 (0.0064)	
training:	Epoch: [106][128/233]	Loss 0.0010 (0.0064)	
training:	Epoch: [106][129/233]	Loss 0.0007 (0.0063)	
training:	Epoch: [106][130/233]	Loss 0.0010 (0.0063)	
training:	Epoch: [106][131/233]	Loss 0.0009 (0.0062)	
training:	Epoch: [106][132/233]	Loss 0.0048 (0.0062)	
training:	Epoch: [106][133/233]	Loss 0.0008 (0.0062)	
training:	Epoch: [106][134/233]	Loss 0.0007 (0.0062)	
training:	Epoch: [106][135/233]	Loss 0.0047 (0.0061)	
training:	Epoch: [106][136/233]	Loss 0.0008 (0.0061)	
training:	Epoch: [106][137/233]	Loss 0.0014 (0.0061)	
training:	Epoch: [106][138/233]	Loss 0.0009 (0.0060)	
training:	Epoch: [106][139/233]	Loss 0.0012 (0.0060)	
training:	Epoch: [106][140/233]	Loss 0.0016 (0.0060)	
training:	Epoch: [106][141/233]	Loss 0.0008 (0.0059)	
training:	Epoch: [106][142/233]	Loss 0.0007 (0.0059)	
training:	Epoch: [106][143/233]	Loss 0.0007 (0.0059)	
training:	Epoch: [106][144/233]	Loss 0.0010 (0.0058)	
training:	Epoch: [106][145/233]	Loss 0.0379 (0.0060)	
training:	Epoch: [106][146/233]	Loss 0.0012 (0.0060)	
training:	Epoch: [106][147/233]	Loss 0.0008 (0.0060)	
training:	Epoch: [106][148/233]	Loss 0.0007 (0.0059)	
training:	Epoch: [106][149/233]	Loss 0.0019 (0.0059)	
training:	Epoch: [106][150/233]	Loss 0.0563 (0.0062)	
training:	Epoch: [106][151/233]	Loss 0.0019 (0.0062)	
training:	Epoch: [106][152/233]	Loss 0.0007 (0.0062)	
training:	Epoch: [106][153/233]	Loss 0.0008 (0.0061)	
training:	Epoch: [106][154/233]	Loss 0.0014 (0.0061)	
training:	Epoch: [106][155/233]	Loss 0.0061 (0.0061)	
training:	Epoch: [106][156/233]	Loss 0.0009 (0.0061)	
training:	Epoch: [106][157/233]	Loss 0.0014 (0.0061)	
training:	Epoch: [106][158/233]	Loss 0.0011 (0.0060)	
training:	Epoch: [106][159/233]	Loss 0.0010 (0.0060)	
training:	Epoch: [106][160/233]	Loss 0.0011 (0.0060)	
training:	Epoch: [106][161/233]	Loss 0.0009 (0.0059)	
training:	Epoch: [106][162/233]	Loss 0.0010 (0.0059)	
training:	Epoch: [106][163/233]	Loss 0.0130 (0.0059)	
training:	Epoch: [106][164/233]	Loss 0.0012 (0.0059)	
training:	Epoch: [106][165/233]	Loss 0.0012 (0.0059)	
training:	Epoch: [106][166/233]	Loss 0.0008 (0.0059)	
training:	Epoch: [106][167/233]	Loss 0.0007 (0.0058)	
training:	Epoch: [106][168/233]	Loss 0.0011 (0.0058)	
training:	Epoch: [106][169/233]	Loss 0.0042 (0.0058)	
training:	Epoch: [106][170/233]	Loss 0.0016 (0.0058)	
training:	Epoch: [106][171/233]	Loss 0.0007 (0.0057)	
training:	Epoch: [106][172/233]	Loss 0.0016 (0.0057)	
training:	Epoch: [106][173/233]	Loss 0.0027 (0.0057)	
training:	Epoch: [106][174/233]	Loss 0.0099 (0.0057)	
training:	Epoch: [106][175/233]	Loss 0.0014 (0.0057)	
training:	Epoch: [106][176/233]	Loss 0.0013 (0.0057)	
training:	Epoch: [106][177/233]	Loss 0.0057 (0.0057)	
training:	Epoch: [106][178/233]	Loss 0.0020 (0.0056)	
training:	Epoch: [106][179/233]	Loss 0.0008 (0.0056)	
training:	Epoch: [106][180/233]	Loss 0.0113 (0.0056)	
training:	Epoch: [106][181/233]	Loss 0.0011 (0.0056)	
training:	Epoch: [106][182/233]	Loss 0.0022 (0.0056)	
training:	Epoch: [106][183/233]	Loss 0.0007 (0.0056)	
training:	Epoch: [106][184/233]	Loss 0.0009 (0.0056)	
training:	Epoch: [106][185/233]	Loss 0.0010 (0.0055)	
training:	Epoch: [106][186/233]	Loss 0.0008 (0.0055)	
training:	Epoch: [106][187/233]	Loss 0.0030 (0.0055)	
training:	Epoch: [106][188/233]	Loss 0.2186 (0.0066)	
training:	Epoch: [106][189/233]	Loss 0.0020 (0.0066)	
training:	Epoch: [106][190/233]	Loss 0.0009 (0.0066)	
training:	Epoch: [106][191/233]	Loss 0.0007 (0.0065)	
training:	Epoch: [106][192/233]	Loss 0.0008 (0.0065)	
training:	Epoch: [106][193/233]	Loss 0.0079 (0.0065)	
training:	Epoch: [106][194/233]	Loss 0.0007 (0.0065)	
training:	Epoch: [106][195/233]	Loss 0.0010 (0.0065)	
training:	Epoch: [106][196/233]	Loss 0.0312 (0.0066)	
training:	Epoch: [106][197/233]	Loss 0.0007 (0.0066)	
training:	Epoch: [106][198/233]	Loss 0.0633 (0.0068)	
training:	Epoch: [106][199/233]	Loss 0.0009 (0.0068)	
training:	Epoch: [106][200/233]	Loss 0.0038 (0.0068)	
training:	Epoch: [106][201/233]	Loss 0.0007 (0.0068)	
training:	Epoch: [106][202/233]	Loss 0.0021 (0.0067)	
training:	Epoch: [106][203/233]	Loss 0.0007 (0.0067)	
training:	Epoch: [106][204/233]	Loss 0.0171 (0.0068)	
training:	Epoch: [106][205/233]	Loss 0.0013 (0.0067)	
training:	Epoch: [106][206/233]	Loss 0.0008 (0.0067)	
training:	Epoch: [106][207/233]	Loss 0.0008 (0.0067)	
training:	Epoch: [106][208/233]	Loss 0.0007 (0.0066)	
training:	Epoch: [106][209/233]	Loss 0.0008 (0.0066)	
training:	Epoch: [106][210/233]	Loss 0.0007 (0.0066)	
training:	Epoch: [106][211/233]	Loss 0.0096 (0.0066)	
training:	Epoch: [106][212/233]	Loss 0.0019 (0.0066)	
training:	Epoch: [106][213/233]	Loss 0.0009 (0.0066)	
training:	Epoch: [106][214/233]	Loss 0.0008 (0.0065)	
training:	Epoch: [106][215/233]	Loss 0.0008 (0.0065)	
training:	Epoch: [106][216/233]	Loss 0.0007 (0.0065)	
training:	Epoch: [106][217/233]	Loss 0.0371 (0.0066)	
training:	Epoch: [106][218/233]	Loss 0.0008 (0.0066)	
training:	Epoch: [106][219/233]	Loss 0.0021 (0.0066)	
training:	Epoch: [106][220/233]	Loss 0.0018 (0.0065)	
training:	Epoch: [106][221/233]	Loss 0.0009 (0.0065)	
training:	Epoch: [106][222/233]	Loss 0.0008 (0.0065)	
training:	Epoch: [106][223/233]	Loss 0.0016 (0.0065)	
training:	Epoch: [106][224/233]	Loss 0.0013 (0.0065)	
training:	Epoch: [106][225/233]	Loss 0.0024 (0.0064)	
training:	Epoch: [106][226/233]	Loss 0.1112 (0.0069)	
training:	Epoch: [106][227/233]	Loss 0.0017 (0.0069)	
training:	Epoch: [106][228/233]	Loss 0.0007 (0.0068)	
training:	Epoch: [106][229/233]	Loss 0.0015 (0.0068)	
training:	Epoch: [106][230/233]	Loss 0.1065 (0.0073)	
training:	Epoch: [106][231/233]	Loss 0.0019 (0.0072)	
training:	Epoch: [106][232/233]	Loss 0.0009 (0.0072)	
training:	Epoch: [106][233/233]	Loss 0.0010 (0.0072)	
Training:	 Loss: 0.0072

Training:	 ACC: 0.9997 0.9997 0.9997 0.9997
Validation:	 ACC: 0.7982 0.7972 0.7773 0.8191
Validation:	 Best_BACC: 0.8162 0.8154 0.7998 0.8326
Validation:	 Loss: 1.1401
Pretraining:	Epoch 107/200
----------
training:	Epoch: [107][1/233]	Loss 0.0008 (0.0008)	
training:	Epoch: [107][2/233]	Loss 0.0008 (0.0008)	
training:	Epoch: [107][3/233]	Loss 0.0008 (0.0008)	
training:	Epoch: [107][4/233]	Loss 0.0008 (0.0008)	
training:	Epoch: [107][5/233]	Loss 0.0074 (0.0021)	
training:	Epoch: [107][6/233]	Loss 0.0033 (0.0023)	
training:	Epoch: [107][7/233]	Loss 0.0008 (0.0021)	
training:	Epoch: [107][8/233]	Loss 0.0014 (0.0020)	
training:	Epoch: [107][9/233]	Loss 0.0008 (0.0019)	
training:	Epoch: [107][10/233]	Loss 0.0328 (0.0050)	
training:	Epoch: [107][11/233]	Loss 0.0010 (0.0046)	
training:	Epoch: [107][12/233]	Loss 0.0021 (0.0044)	
training:	Epoch: [107][13/233]	Loss 0.0007 (0.0041)	
training:	Epoch: [107][14/233]	Loss 0.0008 (0.0039)	
training:	Epoch: [107][15/233]	Loss 0.0012 (0.0037)	
training:	Epoch: [107][16/233]	Loss 0.0008 (0.0035)	
training:	Epoch: [107][17/233]	Loss 0.0008 (0.0033)	
training:	Epoch: [107][18/233]	Loss 0.0013 (0.0032)	
training:	Epoch: [107][19/233]	Loss 0.0011 (0.0031)	
training:	Epoch: [107][20/233]	Loss 0.0017 (0.0030)	
training:	Epoch: [107][21/233]	Loss 0.0028 (0.0030)	
training:	Epoch: [107][22/233]	Loss 0.0008 (0.0029)	
training:	Epoch: [107][23/233]	Loss 0.0010 (0.0028)	
training:	Epoch: [107][24/233]	Loss 0.0038 (0.0029)	
training:	Epoch: [107][25/233]	Loss 0.0016 (0.0028)	
training:	Epoch: [107][26/233]	Loss 0.0007 (0.0028)	
training:	Epoch: [107][27/233]	Loss 0.0082 (0.0030)	
training:	Epoch: [107][28/233]	Loss 0.0023 (0.0029)	
training:	Epoch: [107][29/233]	Loss 0.0008 (0.0029)	
training:	Epoch: [107][30/233]	Loss 0.0487 (0.0044)	
training:	Epoch: [107][31/233]	Loss 0.0007 (0.0043)	
training:	Epoch: [107][32/233]	Loss 0.0028 (0.0042)	
training:	Epoch: [107][33/233]	Loss 0.0011 (0.0041)	
training:	Epoch: [107][34/233]	Loss 0.0007 (0.0040)	
training:	Epoch: [107][35/233]	Loss 0.0010 (0.0039)	
training:	Epoch: [107][36/233]	Loss 0.0010 (0.0039)	
training:	Epoch: [107][37/233]	Loss 0.0011 (0.0038)	
training:	Epoch: [107][38/233]	Loss 0.0007 (0.0037)	
training:	Epoch: [107][39/233]	Loss 0.0012 (0.0036)	
training:	Epoch: [107][40/233]	Loss 0.0012 (0.0036)	
training:	Epoch: [107][41/233]	Loss 0.0696 (0.0052)	
training:	Epoch: [107][42/233]	Loss 0.0013 (0.0051)	
training:	Epoch: [107][43/233]	Loss 0.0013 (0.0050)	
training:	Epoch: [107][44/233]	Loss 0.0007 (0.0049)	
training:	Epoch: [107][45/233]	Loss 0.0014 (0.0048)	
training:	Epoch: [107][46/233]	Loss 0.0007 (0.0047)	
training:	Epoch: [107][47/233]	Loss 0.2442 (0.0098)	
training:	Epoch: [107][48/233]	Loss 0.0029 (0.0097)	
training:	Epoch: [107][49/233]	Loss 0.0015 (0.0095)	
training:	Epoch: [107][50/233]	Loss 0.2242 (0.0138)	
training:	Epoch: [107][51/233]	Loss 0.0062 (0.0137)	
training:	Epoch: [107][52/233]	Loss 0.0046 (0.0135)	
training:	Epoch: [107][53/233]	Loss 0.0013 (0.0133)	
training:	Epoch: [107][54/233]	Loss 0.0035 (0.0131)	
training:	Epoch: [107][55/233]	Loss 0.0008 (0.0129)	
training:	Epoch: [107][56/233]	Loss 0.0011 (0.0126)	
training:	Epoch: [107][57/233]	Loss 0.0010 (0.0124)	
training:	Epoch: [107][58/233]	Loss 0.0012 (0.0122)	
training:	Epoch: [107][59/233]	Loss 0.0007 (0.0121)	
training:	Epoch: [107][60/233]	Loss 0.0009 (0.0119)	
training:	Epoch: [107][61/233]	Loss 0.0009 (0.0117)	
training:	Epoch: [107][62/233]	Loss 0.0795 (0.0128)	
training:	Epoch: [107][63/233]	Loss 0.0233 (0.0129)	
training:	Epoch: [107][64/233]	Loss 0.0922 (0.0142)	
training:	Epoch: [107][65/233]	Loss 0.0008 (0.0140)	
training:	Epoch: [107][66/233]	Loss 0.0011 (0.0138)	
training:	Epoch: [107][67/233]	Loss 0.0012 (0.0136)	
training:	Epoch: [107][68/233]	Loss 0.0010 (0.0134)	
training:	Epoch: [107][69/233]	Loss 0.0011 (0.0132)	
training:	Epoch: [107][70/233]	Loss 0.0014 (0.0131)	
training:	Epoch: [107][71/233]	Loss 0.0011 (0.0129)	
training:	Epoch: [107][72/233]	Loss 0.0008 (0.0127)	
training:	Epoch: [107][73/233]	Loss 0.0007 (0.0126)	
training:	Epoch: [107][74/233]	Loss 0.0015 (0.0124)	
training:	Epoch: [107][75/233]	Loss 0.0009 (0.0123)	
training:	Epoch: [107][76/233]	Loss 0.0008 (0.0121)	
training:	Epoch: [107][77/233]	Loss 0.0018 (0.0120)	
training:	Epoch: [107][78/233]	Loss 0.0011 (0.0118)	
training:	Epoch: [107][79/233]	Loss 0.0010 (0.0117)	
training:	Epoch: [107][80/233]	Loss 0.0039 (0.0116)	
training:	Epoch: [107][81/233]	Loss 0.0014 (0.0115)	
training:	Epoch: [107][82/233]	Loss 0.0011 (0.0113)	
training:	Epoch: [107][83/233]	Loss 0.0702 (0.0121)	
training:	Epoch: [107][84/233]	Loss 0.0012 (0.0119)	
training:	Epoch: [107][85/233]	Loss 0.0983 (0.0129)	
training:	Epoch: [107][86/233]	Loss 0.0008 (0.0128)	
training:	Epoch: [107][87/233]	Loss 0.0009 (0.0127)	
training:	Epoch: [107][88/233]	Loss 0.0008 (0.0125)	
training:	Epoch: [107][89/233]	Loss 0.0008 (0.0124)	
training:	Epoch: [107][90/233]	Loss 0.0478 (0.0128)	
training:	Epoch: [107][91/233]	Loss 0.0008 (0.0127)	
training:	Epoch: [107][92/233]	Loss 0.0014 (0.0125)	
training:	Epoch: [107][93/233]	Loss 0.0008 (0.0124)	
training:	Epoch: [107][94/233]	Loss 0.0007 (0.0123)	
training:	Epoch: [107][95/233]	Loss 0.0041 (0.0122)	
training:	Epoch: [107][96/233]	Loss 0.0011 (0.0121)	
training:	Epoch: [107][97/233]	Loss 0.0008 (0.0120)	
training:	Epoch: [107][98/233]	Loss 0.0008 (0.0119)	
training:	Epoch: [107][99/233]	Loss 0.0028 (0.0118)	
training:	Epoch: [107][100/233]	Loss 0.0012 (0.0117)	
training:	Epoch: [107][101/233]	Loss 0.0011 (0.0116)	
training:	Epoch: [107][102/233]	Loss 0.0009 (0.0114)	
training:	Epoch: [107][103/233]	Loss 0.0021 (0.0114)	
training:	Epoch: [107][104/233]	Loss 0.0010 (0.0113)	
training:	Epoch: [107][105/233]	Loss 0.0084 (0.0112)	
training:	Epoch: [107][106/233]	Loss 0.0036 (0.0112)	
training:	Epoch: [107][107/233]	Loss 0.0055 (0.0111)	
training:	Epoch: [107][108/233]	Loss 0.0625 (0.0116)	
training:	Epoch: [107][109/233]	Loss 0.0022 (0.0115)	
training:	Epoch: [107][110/233]	Loss 0.0111 (0.0115)	
training:	Epoch: [107][111/233]	Loss 0.0010 (0.0114)	
training:	Epoch: [107][112/233]	Loss 0.0009 (0.0113)	
training:	Epoch: [107][113/233]	Loss 0.0009 (0.0112)	
training:	Epoch: [107][114/233]	Loss 0.0224 (0.0113)	
training:	Epoch: [107][115/233]	Loss 0.0007 (0.0112)	
training:	Epoch: [107][116/233]	Loss 0.0017 (0.0111)	
training:	Epoch: [107][117/233]	Loss 0.0015 (0.0111)	
training:	Epoch: [107][118/233]	Loss 0.0008 (0.0110)	
training:	Epoch: [107][119/233]	Loss 0.0010 (0.0109)	
training:	Epoch: [107][120/233]	Loss 0.0026 (0.0108)	
training:	Epoch: [107][121/233]	Loss 0.0096 (0.0108)	
training:	Epoch: [107][122/233]	Loss 0.0483 (0.0111)	
training:	Epoch: [107][123/233]	Loss 0.0009 (0.0110)	
training:	Epoch: [107][124/233]	Loss 0.0026 (0.0110)	
training:	Epoch: [107][125/233]	Loss 0.0012 (0.0109)	
training:	Epoch: [107][126/233]	Loss 0.0008 (0.0108)	
training:	Epoch: [107][127/233]	Loss 0.0023 (0.0107)	
training:	Epoch: [107][128/233]	Loss 0.0009 (0.0107)	
training:	Epoch: [107][129/233]	Loss 0.0486 (0.0110)	
training:	Epoch: [107][130/233]	Loss 0.0024 (0.0109)	
training:	Epoch: [107][131/233]	Loss 0.0252 (0.0110)	
training:	Epoch: [107][132/233]	Loss 0.0019 (0.0109)	
training:	Epoch: [107][133/233]	Loss 0.0010 (0.0109)	
training:	Epoch: [107][134/233]	Loss 0.0009 (0.0108)	
training:	Epoch: [107][135/233]	Loss 0.0160 (0.0108)	
training:	Epoch: [107][136/233]	Loss 0.0011 (0.0107)	
training:	Epoch: [107][137/233]	Loss 0.0014 (0.0107)	
training:	Epoch: [107][138/233]	Loss 0.0265 (0.0108)	
training:	Epoch: [107][139/233]	Loss 0.0012 (0.0107)	
training:	Epoch: [107][140/233]	Loss 0.0008 (0.0107)	
training:	Epoch: [107][141/233]	Loss 0.0008 (0.0106)	
training:	Epoch: [107][142/233]	Loss 0.0015 (0.0105)	
training:	Epoch: [107][143/233]	Loss 0.0008 (0.0105)	
training:	Epoch: [107][144/233]	Loss 0.0085 (0.0104)	
training:	Epoch: [107][145/233]	Loss 0.0248 (0.0105)	
training:	Epoch: [107][146/233]	Loss 0.0122 (0.0105)	
training:	Epoch: [107][147/233]	Loss 0.0015 (0.0105)	
training:	Epoch: [107][148/233]	Loss 0.0011 (0.0104)	
training:	Epoch: [107][149/233]	Loss 0.0015 (0.0104)	
training:	Epoch: [107][150/233]	Loss 0.0085 (0.0104)	
training:	Epoch: [107][151/233]	Loss 0.0007 (0.0103)	
training:	Epoch: [107][152/233]	Loss 0.0008 (0.0102)	
training:	Epoch: [107][153/233]	Loss 0.0018 (0.0102)	
training:	Epoch: [107][154/233]	Loss 0.0012 (0.0101)	
training:	Epoch: [107][155/233]	Loss 0.0007 (0.0100)	
training:	Epoch: [107][156/233]	Loss 0.0008 (0.0100)	
training:	Epoch: [107][157/233]	Loss 0.0007 (0.0099)	
training:	Epoch: [107][158/233]	Loss 0.0009 (0.0099)	
training:	Epoch: [107][159/233]	Loss 0.0009 (0.0098)	
training:	Epoch: [107][160/233]	Loss 0.0008 (0.0098)	
training:	Epoch: [107][161/233]	Loss 0.0033 (0.0097)	
training:	Epoch: [107][162/233]	Loss 0.0009 (0.0097)	
training:	Epoch: [107][163/233]	Loss 0.0021 (0.0096)	
training:	Epoch: [107][164/233]	Loss 0.0009 (0.0096)	
training:	Epoch: [107][165/233]	Loss 0.0152 (0.0096)	
training:	Epoch: [107][166/233]	Loss 0.0013 (0.0096)	
training:	Epoch: [107][167/233]	Loss 0.0007 (0.0095)	
training:	Epoch: [107][168/233]	Loss 0.0040 (0.0095)	
training:	Epoch: [107][169/233]	Loss 0.0007 (0.0094)	
training:	Epoch: [107][170/233]	Loss 0.0007 (0.0094)	
training:	Epoch: [107][171/233]	Loss 0.0012 (0.0093)	
training:	Epoch: [107][172/233]	Loss 0.1026 (0.0099)	
training:	Epoch: [107][173/233]	Loss 0.0011 (0.0098)	
training:	Epoch: [107][174/233]	Loss 0.0009 (0.0098)	
training:	Epoch: [107][175/233]	Loss 0.0007 (0.0097)	
training:	Epoch: [107][176/233]	Loss 0.0017 (0.0097)	
training:	Epoch: [107][177/233]	Loss 0.0009 (0.0096)	
training:	Epoch: [107][178/233]	Loss 0.0008 (0.0096)	
training:	Epoch: [107][179/233]	Loss 0.0165 (0.0096)	
training:	Epoch: [107][180/233]	Loss 0.0008 (0.0095)	
training:	Epoch: [107][181/233]	Loss 0.1011 (0.0101)	
training:	Epoch: [107][182/233]	Loss 0.0009 (0.0100)	
training:	Epoch: [107][183/233]	Loss 0.0014 (0.0100)	
training:	Epoch: [107][184/233]	Loss 0.0008 (0.0099)	
training:	Epoch: [107][185/233]	Loss 0.0008 (0.0099)	
training:	Epoch: [107][186/233]	Loss 0.0008 (0.0098)	
training:	Epoch: [107][187/233]	Loss 0.0008 (0.0098)	
training:	Epoch: [107][188/233]	Loss 0.0009 (0.0097)	
training:	Epoch: [107][189/233]	Loss 0.0032 (0.0097)	
training:	Epoch: [107][190/233]	Loss 0.0010 (0.0096)	
training:	Epoch: [107][191/233]	Loss 0.0007 (0.0096)	
training:	Epoch: [107][192/233]	Loss 0.0007 (0.0095)	
training:	Epoch: [107][193/233]	Loss 0.0008 (0.0095)	
training:	Epoch: [107][194/233]	Loss 0.0009 (0.0095)	
training:	Epoch: [107][195/233]	Loss 0.0007 (0.0094)	
training:	Epoch: [107][196/233]	Loss 0.0015 (0.0094)	
training:	Epoch: [107][197/233]	Loss 0.0008 (0.0093)	
training:	Epoch: [107][198/233]	Loss 0.0044 (0.0093)	
training:	Epoch: [107][199/233]	Loss 0.0008 (0.0093)	
training:	Epoch: [107][200/233]	Loss 0.0008 (0.0092)	
training:	Epoch: [107][201/233]	Loss 0.0016 (0.0092)	
training:	Epoch: [107][202/233]	Loss 0.0014 (0.0091)	
training:	Epoch: [107][203/233]	Loss 0.0012 (0.0091)	
training:	Epoch: [107][204/233]	Loss 0.0008 (0.0091)	
training:	Epoch: [107][205/233]	Loss 0.0007 (0.0090)	
training:	Epoch: [107][206/233]	Loss 0.0024 (0.0090)	
training:	Epoch: [107][207/233]	Loss 0.0013 (0.0089)	
training:	Epoch: [107][208/233]	Loss 0.0007 (0.0089)	
training:	Epoch: [107][209/233]	Loss 0.0007 (0.0089)	
training:	Epoch: [107][210/233]	Loss 0.0025 (0.0088)	
training:	Epoch: [107][211/233]	Loss 0.0035 (0.0088)	
training:	Epoch: [107][212/233]	Loss 0.0011 (0.0088)	
training:	Epoch: [107][213/233]	Loss 0.0011 (0.0087)	
training:	Epoch: [107][214/233]	Loss 0.0010 (0.0087)	
training:	Epoch: [107][215/233]	Loss 0.0017 (0.0087)	
training:	Epoch: [107][216/233]	Loss 0.0007 (0.0086)	
training:	Epoch: [107][217/233]	Loss 0.0010 (0.0086)	
training:	Epoch: [107][218/233]	Loss 0.0021 (0.0086)	
training:	Epoch: [107][219/233]	Loss 0.0015 (0.0085)	
training:	Epoch: [107][220/233]	Loss 0.0007 (0.0085)	
training:	Epoch: [107][221/233]	Loss 0.0009 (0.0085)	
training:	Epoch: [107][222/233]	Loss 0.1603 (0.0092)	
training:	Epoch: [107][223/233]	Loss 0.0009 (0.0091)	
training:	Epoch: [107][224/233]	Loss 0.0013 (0.0091)	
training:	Epoch: [107][225/233]	Loss 0.0009 (0.0090)	
training:	Epoch: [107][226/233]	Loss 0.0011 (0.0090)	
training:	Epoch: [107][227/233]	Loss 0.0029 (0.0090)	
training:	Epoch: [107][228/233]	Loss 0.0011 (0.0089)	
training:	Epoch: [107][229/233]	Loss 0.0057 (0.0089)	
training:	Epoch: [107][230/233]	Loss 0.0088 (0.0089)	
training:	Epoch: [107][231/233]	Loss 0.0008 (0.0089)	
training:	Epoch: [107][232/233]	Loss 0.0031 (0.0089)	
training:	Epoch: [107][233/233]	Loss 0.0007 (0.0088)	
Training:	 Loss: 0.0088

Training:	 ACC: 0.9992 0.9992 0.9997 0.9986
Validation:	 ACC: 0.7908 0.7935 0.8478 0.7337
Validation:	 Best_BACC: 0.8162 0.8154 0.7998 0.8326
Validation:	 Loss: 1.1288
Pretraining:	Epoch 108/200
----------
training:	Epoch: [108][1/233]	Loss 0.0205 (0.0205)	
training:	Epoch: [108][2/233]	Loss 0.0207 (0.0206)	
training:	Epoch: [108][3/233]	Loss 0.1637 (0.0683)	
training:	Epoch: [108][4/233]	Loss 0.0021 (0.0517)	
training:	Epoch: [108][5/233]	Loss 0.0007 (0.0415)	
training:	Epoch: [108][6/233]	Loss 0.0036 (0.0352)	
training:	Epoch: [108][7/233]	Loss 0.0013 (0.0304)	
training:	Epoch: [108][8/233]	Loss 0.0008 (0.0267)	
training:	Epoch: [108][9/233]	Loss 0.0008 (0.0238)	
training:	Epoch: [108][10/233]	Loss 0.0009 (0.0215)	
training:	Epoch: [108][11/233]	Loss 0.0008 (0.0196)	
training:	Epoch: [108][12/233]	Loss 0.0017 (0.0181)	
training:	Epoch: [108][13/233]	Loss 0.0007 (0.0168)	
training:	Epoch: [108][14/233]	Loss 0.0008 (0.0156)	
training:	Epoch: [108][15/233]	Loss 0.0006 (0.0146)	
training:	Epoch: [108][16/233]	Loss 0.0011 (0.0138)	
training:	Epoch: [108][17/233]	Loss 0.0013 (0.0131)	
training:	Epoch: [108][18/233]	Loss 0.0030 (0.0125)	
training:	Epoch: [108][19/233]	Loss 0.0027 (0.0120)	
training:	Epoch: [108][20/233]	Loss 0.0008 (0.0114)	
training:	Epoch: [108][21/233]	Loss 0.0007 (0.0109)	
training:	Epoch: [108][22/233]	Loss 0.0008 (0.0105)	
training:	Epoch: [108][23/233]	Loss 0.0009 (0.0100)	
training:	Epoch: [108][24/233]	Loss 0.0009 (0.0097)	
training:	Epoch: [108][25/233]	Loss 0.0007 (0.0093)	
training:	Epoch: [108][26/233]	Loss 0.0680 (0.0116)	
training:	Epoch: [108][27/233]	Loss 0.0008 (0.0112)	
training:	Epoch: [108][28/233]	Loss 0.0009 (0.0108)	
training:	Epoch: [108][29/233]	Loss 0.0009 (0.0104)	
training:	Epoch: [108][30/233]	Loss 0.0010 (0.0101)	
training:	Epoch: [108][31/233]	Loss 0.0009 (0.0098)	
training:	Epoch: [108][32/233]	Loss 0.0008 (0.0096)	
training:	Epoch: [108][33/233]	Loss 0.0100 (0.0096)	
training:	Epoch: [108][34/233]	Loss 0.0008 (0.0093)	
training:	Epoch: [108][35/233]	Loss 0.0038 (0.0091)	
training:	Epoch: [108][36/233]	Loss 0.0010 (0.0089)	
training:	Epoch: [108][37/233]	Loss 0.0007 (0.0087)	
training:	Epoch: [108][38/233]	Loss 0.0007 (0.0085)	
training:	Epoch: [108][39/233]	Loss 0.0013 (0.0083)	
training:	Epoch: [108][40/233]	Loss 0.0010 (0.0081)	
training:	Epoch: [108][41/233]	Loss 0.0016 (0.0080)	
training:	Epoch: [108][42/233]	Loss 0.0033 (0.0079)	
training:	Epoch: [108][43/233]	Loss 0.0015 (0.0077)	
training:	Epoch: [108][44/233]	Loss 0.0011 (0.0076)	
training:	Epoch: [108][45/233]	Loss 0.0022 (0.0074)	
training:	Epoch: [108][46/233]	Loss 0.0010 (0.0073)	
training:	Epoch: [108][47/233]	Loss 0.0008 (0.0072)	
training:	Epoch: [108][48/233]	Loss 0.0012 (0.0070)	
training:	Epoch: [108][49/233]	Loss 0.0012 (0.0069)	
training:	Epoch: [108][50/233]	Loss 0.0140 (0.0071)	
training:	Epoch: [108][51/233]	Loss 0.0008 (0.0069)	
training:	Epoch: [108][52/233]	Loss 0.0016 (0.0068)	
training:	Epoch: [108][53/233]	Loss 0.0006 (0.0067)	
training:	Epoch: [108][54/233]	Loss 0.0035 (0.0067)	
training:	Epoch: [108][55/233]	Loss 0.0008 (0.0065)	
training:	Epoch: [108][56/233]	Loss 0.0008 (0.0064)	
training:	Epoch: [108][57/233]	Loss 0.0015 (0.0064)	
training:	Epoch: [108][58/233]	Loss 0.0010 (0.0063)	
training:	Epoch: [108][59/233]	Loss 0.0527 (0.0071)	
training:	Epoch: [108][60/233]	Loss 0.0245 (0.0073)	
training:	Epoch: [108][61/233]	Loss 0.0017 (0.0073)	
training:	Epoch: [108][62/233]	Loss 0.0020 (0.0072)	
training:	Epoch: [108][63/233]	Loss 0.0009 (0.0071)	
training:	Epoch: [108][64/233]	Loss 0.0007 (0.0070)	
training:	Epoch: [108][65/233]	Loss 0.0360 (0.0074)	
training:	Epoch: [108][66/233]	Loss 0.0012 (0.0073)	
training:	Epoch: [108][67/233]	Loss 0.0061 (0.0073)	
training:	Epoch: [108][68/233]	Loss 0.0006 (0.0072)	
training:	Epoch: [108][69/233]	Loss 0.0008 (0.0071)	
training:	Epoch: [108][70/233]	Loss 0.0420 (0.0076)	
training:	Epoch: [108][71/233]	Loss 0.0007 (0.0075)	
training:	Epoch: [108][72/233]	Loss 0.0010 (0.0074)	
training:	Epoch: [108][73/233]	Loss 0.0007 (0.0073)	
training:	Epoch: [108][74/233]	Loss 0.0020 (0.0073)	
training:	Epoch: [108][75/233]	Loss 0.0041 (0.0072)	
training:	Epoch: [108][76/233]	Loss 0.2369 (0.0102)	
training:	Epoch: [108][77/233]	Loss 0.0012 (0.0101)	
training:	Epoch: [108][78/233]	Loss 0.0039 (0.0100)	
training:	Epoch: [108][79/233]	Loss 0.0006 (0.0099)	
training:	Epoch: [108][80/233]	Loss 0.0014 (0.0098)	
training:	Epoch: [108][81/233]	Loss 0.0021 (0.0097)	
training:	Epoch: [108][82/233]	Loss 0.0020 (0.0096)	
training:	Epoch: [108][83/233]	Loss 0.0006 (0.0095)	
training:	Epoch: [108][84/233]	Loss 0.0022 (0.0094)	
training:	Epoch: [108][85/233]	Loss 0.0007 (0.0093)	
training:	Epoch: [108][86/233]	Loss 0.0021 (0.0092)	
training:	Epoch: [108][87/233]	Loss 0.0089 (0.0092)	
training:	Epoch: [108][88/233]	Loss 0.0050 (0.0092)	
training:	Epoch: [108][89/233]	Loss 0.0010 (0.0091)	
training:	Epoch: [108][90/233]	Loss 0.0007 (0.0090)	
training:	Epoch: [108][91/233]	Loss 0.0013 (0.0089)	
training:	Epoch: [108][92/233]	Loss 0.1963 (0.0110)	
training:	Epoch: [108][93/233]	Loss 0.0013 (0.0109)	
training:	Epoch: [108][94/233]	Loss 0.0012 (0.0108)	
training:	Epoch: [108][95/233]	Loss 0.0017 (0.0107)	
training:	Epoch: [108][96/233]	Loss 0.0085 (0.0106)	
training:	Epoch: [108][97/233]	Loss 0.0008 (0.0105)	
training:	Epoch: [108][98/233]	Loss 0.0017 (0.0104)	
training:	Epoch: [108][99/233]	Loss 0.0008 (0.0103)	
training:	Epoch: [108][100/233]	Loss 0.0008 (0.0103)	
training:	Epoch: [108][101/233]	Loss 0.0007 (0.0102)	
training:	Epoch: [108][102/233]	Loss 0.0114 (0.0102)	
training:	Epoch: [108][103/233]	Loss 0.0009 (0.0101)	
training:	Epoch: [108][104/233]	Loss 0.0006 (0.0100)	
training:	Epoch: [108][105/233]	Loss 0.0055 (0.0099)	
training:	Epoch: [108][106/233]	Loss 0.0017 (0.0099)	
training:	Epoch: [108][107/233]	Loss 0.0009 (0.0098)	
training:	Epoch: [108][108/233]	Loss 0.0013 (0.0097)	
training:	Epoch: [108][109/233]	Loss 0.0010 (0.0096)	
training:	Epoch: [108][110/233]	Loss 0.0012 (0.0095)	
training:	Epoch: [108][111/233]	Loss 0.0031 (0.0095)	
training:	Epoch: [108][112/233]	Loss 0.0548 (0.0099)	
training:	Epoch: [108][113/233]	Loss 0.0008 (0.0098)	
training:	Epoch: [108][114/233]	Loss 0.0010 (0.0097)	
training:	Epoch: [108][115/233]	Loss 0.0017 (0.0097)	
training:	Epoch: [108][116/233]	Loss 0.0010 (0.0096)	
training:	Epoch: [108][117/233]	Loss 0.0045 (0.0095)	
training:	Epoch: [108][118/233]	Loss 0.0019 (0.0095)	
training:	Epoch: [108][119/233]	Loss 0.0007 (0.0094)	
training:	Epoch: [108][120/233]	Loss 0.0023 (0.0093)	
training:	Epoch: [108][121/233]	Loss 0.0035 (0.0093)	
training:	Epoch: [108][122/233]	Loss 0.0018 (0.0092)	
training:	Epoch: [108][123/233]	Loss 0.0007 (0.0092)	
training:	Epoch: [108][124/233]	Loss 0.0110 (0.0092)	
training:	Epoch: [108][125/233]	Loss 0.0009 (0.0091)	
training:	Epoch: [108][126/233]	Loss 0.0023 (0.0091)	
training:	Epoch: [108][127/233]	Loss 0.0008 (0.0090)	
training:	Epoch: [108][128/233]	Loss 0.0010 (0.0089)	
training:	Epoch: [108][129/233]	Loss 0.0016 (0.0089)	
training:	Epoch: [108][130/233]	Loss 0.0008 (0.0088)	
training:	Epoch: [108][131/233]	Loss 0.0008 (0.0088)	
training:	Epoch: [108][132/233]	Loss 0.0008 (0.0087)	
training:	Epoch: [108][133/233]	Loss 0.0013 (0.0086)	
training:	Epoch: [108][134/233]	Loss 0.0008 (0.0086)	
training:	Epoch: [108][135/233]	Loss 0.0008 (0.0085)	
training:	Epoch: [108][136/233]	Loss 0.0007 (0.0085)	
training:	Epoch: [108][137/233]	Loss 0.0008 (0.0084)	
training:	Epoch: [108][138/233]	Loss 0.0007 (0.0084)	
training:	Epoch: [108][139/233]	Loss 0.0014 (0.0083)	
training:	Epoch: [108][140/233]	Loss 0.0264 (0.0084)	
training:	Epoch: [108][141/233]	Loss 0.0007 (0.0084)	
training:	Epoch: [108][142/233]	Loss 0.1023 (0.0090)	
training:	Epoch: [108][143/233]	Loss 0.0022 (0.0090)	
training:	Epoch: [108][144/233]	Loss 0.0008 (0.0089)	
training:	Epoch: [108][145/233]	Loss 0.0018 (0.0089)	
training:	Epoch: [108][146/233]	Loss 0.0007 (0.0088)	
training:	Epoch: [108][147/233]	Loss 0.0007 (0.0088)	
training:	Epoch: [108][148/233]	Loss 0.0008 (0.0087)	
training:	Epoch: [108][149/233]	Loss 0.0009 (0.0087)	
training:	Epoch: [108][150/233]	Loss 0.0008 (0.0086)	
training:	Epoch: [108][151/233]	Loss 0.0011 (0.0086)	
training:	Epoch: [108][152/233]	Loss 0.0155 (0.0086)	
training:	Epoch: [108][153/233]	Loss 0.0012 (0.0086)	
training:	Epoch: [108][154/233]	Loss 0.0081 (0.0086)	
training:	Epoch: [108][155/233]	Loss 0.0009 (0.0085)	
training:	Epoch: [108][156/233]	Loss 0.0011 (0.0085)	
training:	Epoch: [108][157/233]	Loss 0.1016 (0.0091)	
training:	Epoch: [108][158/233]	Loss 0.0542 (0.0093)	
training:	Epoch: [108][159/233]	Loss 0.0007 (0.0093)	
training:	Epoch: [108][160/233]	Loss 0.0007 (0.0092)	
training:	Epoch: [108][161/233]	Loss 0.0023 (0.0092)	
training:	Epoch: [108][162/233]	Loss 0.0008 (0.0091)	
training:	Epoch: [108][163/233]	Loss 0.0010 (0.0091)	
training:	Epoch: [108][164/233]	Loss 0.0008 (0.0090)	
training:	Epoch: [108][165/233]	Loss 0.0020 (0.0090)	
training:	Epoch: [108][166/233]	Loss 0.0018 (0.0090)	
training:	Epoch: [108][167/233]	Loss 0.0279 (0.0091)	
training:	Epoch: [108][168/233]	Loss 0.0008 (0.0090)	
training:	Epoch: [108][169/233]	Loss 0.0007 (0.0090)	
training:	Epoch: [108][170/233]	Loss 0.0009 (0.0089)	
training:	Epoch: [108][171/233]	Loss 0.0010 (0.0089)	
training:	Epoch: [108][172/233]	Loss 0.0011 (0.0088)	
training:	Epoch: [108][173/233]	Loss 0.0009 (0.0088)	
training:	Epoch: [108][174/233]	Loss 0.0008 (0.0087)	
training:	Epoch: [108][175/233]	Loss 0.0007 (0.0087)	
training:	Epoch: [108][176/233]	Loss 0.0047 (0.0087)	
training:	Epoch: [108][177/233]	Loss 0.0007 (0.0086)	
training:	Epoch: [108][178/233]	Loss 0.0037 (0.0086)	
training:	Epoch: [108][179/233]	Loss 0.0007 (0.0086)	
training:	Epoch: [108][180/233]	Loss 0.0064 (0.0085)	
training:	Epoch: [108][181/233]	Loss 0.0103 (0.0086)	
training:	Epoch: [108][182/233]	Loss 0.0086 (0.0086)	
training:	Epoch: [108][183/233]	Loss 0.0008 (0.0085)	
training:	Epoch: [108][184/233]	Loss 0.0007 (0.0085)	
training:	Epoch: [108][185/233]	Loss 0.0009 (0.0084)	
training:	Epoch: [108][186/233]	Loss 0.0009 (0.0084)	
training:	Epoch: [108][187/233]	Loss 0.0024 (0.0084)	
training:	Epoch: [108][188/233]	Loss 0.0008 (0.0083)	
training:	Epoch: [108][189/233]	Loss 0.0007 (0.0083)	
training:	Epoch: [108][190/233]	Loss 0.0008 (0.0082)	
training:	Epoch: [108][191/233]	Loss 0.0007 (0.0082)	
training:	Epoch: [108][192/233]	Loss 0.0010 (0.0082)	
training:	Epoch: [108][193/233]	Loss 0.0016 (0.0081)	
training:	Epoch: [108][194/233]	Loss 0.0015 (0.0081)	
training:	Epoch: [108][195/233]	Loss 0.0008 (0.0080)	
training:	Epoch: [108][196/233]	Loss 0.0011 (0.0080)	
training:	Epoch: [108][197/233]	Loss 0.0008 (0.0080)	
training:	Epoch: [108][198/233]	Loss 0.0008 (0.0079)	
training:	Epoch: [108][199/233]	Loss 0.1685 (0.0087)	
training:	Epoch: [108][200/233]	Loss 0.0010 (0.0087)	
training:	Epoch: [108][201/233]	Loss 0.0076 (0.0087)	
training:	Epoch: [108][202/233]	Loss 0.0010 (0.0087)	
training:	Epoch: [108][203/233]	Loss 0.0011 (0.0086)	
training:	Epoch: [108][204/233]	Loss 0.0006 (0.0086)	
training:	Epoch: [108][205/233]	Loss 0.0008 (0.0086)	
training:	Epoch: [108][206/233]	Loss 0.0008 (0.0085)	
training:	Epoch: [108][207/233]	Loss 0.0014 (0.0085)	
training:	Epoch: [108][208/233]	Loss 0.0010 (0.0084)	
training:	Epoch: [108][209/233]	Loss 0.0006 (0.0084)	
training:	Epoch: [108][210/233]	Loss 0.0008 (0.0084)	
training:	Epoch: [108][211/233]	Loss 0.0449 (0.0085)	
training:	Epoch: [108][212/233]	Loss 0.1499 (0.0092)	
training:	Epoch: [108][213/233]	Loss 0.0010 (0.0092)	
training:	Epoch: [108][214/233]	Loss 0.0044 (0.0091)	
training:	Epoch: [108][215/233]	Loss 0.0012 (0.0091)	
training:	Epoch: [108][216/233]	Loss 0.0009 (0.0091)	
training:	Epoch: [108][217/233]	Loss 0.0008 (0.0090)	
training:	Epoch: [108][218/233]	Loss 0.0009 (0.0090)	
training:	Epoch: [108][219/233]	Loss 0.0007 (0.0090)	
training:	Epoch: [108][220/233]	Loss 0.0010 (0.0089)	
training:	Epoch: [108][221/233]	Loss 0.0008 (0.0089)	
training:	Epoch: [108][222/233]	Loss 0.0008 (0.0089)	
training:	Epoch: [108][223/233]	Loss 0.0008 (0.0088)	
training:	Epoch: [108][224/233]	Loss 0.0009 (0.0088)	
training:	Epoch: [108][225/233]	Loss 0.0008 (0.0087)	
training:	Epoch: [108][226/233]	Loss 0.0011 (0.0087)	
training:	Epoch: [108][227/233]	Loss 0.0016 (0.0087)	
training:	Epoch: [108][228/233]	Loss 0.0009 (0.0086)	
training:	Epoch: [108][229/233]	Loss 0.0010 (0.0086)	
training:	Epoch: [108][230/233]	Loss 0.0008 (0.0086)	
training:	Epoch: [108][231/233]	Loss 0.0009 (0.0085)	
training:	Epoch: [108][232/233]	Loss 0.0011 (0.0085)	
training:	Epoch: [108][233/233]	Loss 0.0012 (0.0085)	
Training:	 Loss: 0.0085

Training:	 ACC: 0.9997 0.9997 0.9997 0.9997
Validation:	 ACC: 0.7864 0.7860 0.7773 0.7955
Validation:	 Best_BACC: 0.8162 0.8154 0.7998 0.8326
Validation:	 Loss: 1.1121
Pretraining:	Epoch 109/200
----------
training:	Epoch: [109][1/233]	Loss 0.0009 (0.0009)	
training:	Epoch: [109][2/233]	Loss 0.0007 (0.0008)	
training:	Epoch: [109][3/233]	Loss 0.0009 (0.0009)	
training:	Epoch: [109][4/233]	Loss 0.0011 (0.0009)	
training:	Epoch: [109][5/233]	Loss 0.0019 (0.0011)	
training:	Epoch: [109][6/233]	Loss 0.0011 (0.0011)	
training:	Epoch: [109][7/233]	Loss 0.0008 (0.0011)	
training:	Epoch: [109][8/233]	Loss 0.0008 (0.0010)	
training:	Epoch: [109][9/233]	Loss 0.0009 (0.0010)	
training:	Epoch: [109][10/233]	Loss 0.0008 (0.0010)	
training:	Epoch: [109][11/233]	Loss 0.0007 (0.0010)	
training:	Epoch: [109][12/233]	Loss 0.0009 (0.0010)	
training:	Epoch: [109][13/233]	Loss 0.0010 (0.0010)	
training:	Epoch: [109][14/233]	Loss 0.0011 (0.0010)	
training:	Epoch: [109][15/233]	Loss 0.0010 (0.0010)	
training:	Epoch: [109][16/233]	Loss 0.0034 (0.0011)	
training:	Epoch: [109][17/233]	Loss 0.0108 (0.0017)	
training:	Epoch: [109][18/233]	Loss 0.0008 (0.0016)	
training:	Epoch: [109][19/233]	Loss 0.0007 (0.0016)	
training:	Epoch: [109][20/233]	Loss 0.0007 (0.0015)	
training:	Epoch: [109][21/233]	Loss 0.0009 (0.0015)	
training:	Epoch: [109][22/233]	Loss 0.0009 (0.0015)	
training:	Epoch: [109][23/233]	Loss 0.0008 (0.0015)	
training:	Epoch: [109][24/233]	Loss 0.0009 (0.0014)	
training:	Epoch: [109][25/233]	Loss 0.0057 (0.0016)	
training:	Epoch: [109][26/233]	Loss 0.0007 (0.0016)	
training:	Epoch: [109][27/233]	Loss 0.0009 (0.0015)	
training:	Epoch: [109][28/233]	Loss 0.0010 (0.0015)	
training:	Epoch: [109][29/233]	Loss 0.0263 (0.0024)	
training:	Epoch: [109][30/233]	Loss 0.0009 (0.0023)	
training:	Epoch: [109][31/233]	Loss 0.0008 (0.0023)	
training:	Epoch: [109][32/233]	Loss 0.0009 (0.0022)	
training:	Epoch: [109][33/233]	Loss 0.0011 (0.0022)	
training:	Epoch: [109][34/233]	Loss 0.0008 (0.0022)	
training:	Epoch: [109][35/233]	Loss 0.0017 (0.0021)	
training:	Epoch: [109][36/233]	Loss 0.0007 (0.0021)	
training:	Epoch: [109][37/233]	Loss 0.0041 (0.0022)	
training:	Epoch: [109][38/233]	Loss 0.0010 (0.0021)	
training:	Epoch: [109][39/233]	Loss 0.0024 (0.0021)	
training:	Epoch: [109][40/233]	Loss 0.0009 (0.0021)	
training:	Epoch: [109][41/233]	Loss 0.2349 (0.0078)	
training:	Epoch: [109][42/233]	Loss 0.0009 (0.0076)	
training:	Epoch: [109][43/233]	Loss 0.0020 (0.0075)	
training:	Epoch: [109][44/233]	Loss 0.0007 (0.0073)	
training:	Epoch: [109][45/233]	Loss 0.0011 (0.0072)	
training:	Epoch: [109][46/233]	Loss 0.0010 (0.0071)	
training:	Epoch: [109][47/233]	Loss 0.0008 (0.0069)	
training:	Epoch: [109][48/233]	Loss 0.0011 (0.0068)	
training:	Epoch: [109][49/233]	Loss 0.0013 (0.0067)	
training:	Epoch: [109][50/233]	Loss 0.0009 (0.0066)	
training:	Epoch: [109][51/233]	Loss 0.1910 (0.0102)	
training:	Epoch: [109][52/233]	Loss 0.0009 (0.0100)	
training:	Epoch: [109][53/233]	Loss 0.0220 (0.0102)	
training:	Epoch: [109][54/233]	Loss 0.0009 (0.0101)	
training:	Epoch: [109][55/233]	Loss 0.0016 (0.0099)	
training:	Epoch: [109][56/233]	Loss 0.0069 (0.0099)	
training:	Epoch: [109][57/233]	Loss 0.0027 (0.0097)	
training:	Epoch: [109][58/233]	Loss 0.0012 (0.0096)	
training:	Epoch: [109][59/233]	Loss 0.0007 (0.0094)	
training:	Epoch: [109][60/233]	Loss 0.0018 (0.0093)	
training:	Epoch: [109][61/233]	Loss 0.0009 (0.0092)	
training:	Epoch: [109][62/233]	Loss 0.0015 (0.0091)	
training:	Epoch: [109][63/233]	Loss 0.0011 (0.0089)	
training:	Epoch: [109][64/233]	Loss 0.0020 (0.0088)	
training:	Epoch: [109][65/233]	Loss 0.0008 (0.0087)	
training:	Epoch: [109][66/233]	Loss 0.0009 (0.0086)	
training:	Epoch: [109][67/233]	Loss 0.0017 (0.0085)	
training:	Epoch: [109][68/233]	Loss 0.0009 (0.0084)	
training:	Epoch: [109][69/233]	Loss 0.0011 (0.0083)	
training:	Epoch: [109][70/233]	Loss 0.0007 (0.0081)	
training:	Epoch: [109][71/233]	Loss 0.0011 (0.0080)	
training:	Epoch: [109][72/233]	Loss 0.0010 (0.0080)	
training:	Epoch: [109][73/233]	Loss 0.0015 (0.0079)	
training:	Epoch: [109][74/233]	Loss 0.0010 (0.0078)	
training:	Epoch: [109][75/233]	Loss 0.0007 (0.0077)	
training:	Epoch: [109][76/233]	Loss 0.0006 (0.0076)	
training:	Epoch: [109][77/233]	Loss 0.0291 (0.0079)	
training:	Epoch: [109][78/233]	Loss 0.0008 (0.0078)	
training:	Epoch: [109][79/233]	Loss 0.0011 (0.0077)	
training:	Epoch: [109][80/233]	Loss 0.0008 (0.0076)	
training:	Epoch: [109][81/233]	Loss 0.0006 (0.0075)	
training:	Epoch: [109][82/233]	Loss 0.0011 (0.0074)	
training:	Epoch: [109][83/233]	Loss 0.0012 (0.0074)	
training:	Epoch: [109][84/233]	Loss 0.0008 (0.0073)	
training:	Epoch: [109][85/233]	Loss 0.0007 (0.0072)	
training:	Epoch: [109][86/233]	Loss 0.0012 (0.0071)	
training:	Epoch: [109][87/233]	Loss 0.0014 (0.0071)	
training:	Epoch: [109][88/233]	Loss 0.0007 (0.0070)	
training:	Epoch: [109][89/233]	Loss 0.0025 (0.0069)	
training:	Epoch: [109][90/233]	Loss 0.0820 (0.0078)	
training:	Epoch: [109][91/233]	Loss 0.0024 (0.0077)	
training:	Epoch: [109][92/233]	Loss 0.0008 (0.0076)	
training:	Epoch: [109][93/233]	Loss 0.0008 (0.0076)	
training:	Epoch: [109][94/233]	Loss 0.0010 (0.0075)	
training:	Epoch: [109][95/233]	Loss 0.0007 (0.0074)	
training:	Epoch: [109][96/233]	Loss 0.0010 (0.0074)	
training:	Epoch: [109][97/233]	Loss 0.0007 (0.0073)	
training:	Epoch: [109][98/233]	Loss 0.0008 (0.0072)	
training:	Epoch: [109][99/233]	Loss 0.0008 (0.0072)	
training:	Epoch: [109][100/233]	Loss 0.0008 (0.0071)	
training:	Epoch: [109][101/233]	Loss 0.0010 (0.0070)	
training:	Epoch: [109][102/233]	Loss 0.0048 (0.0070)	
training:	Epoch: [109][103/233]	Loss 0.0010 (0.0070)	
training:	Epoch: [109][104/233]	Loss 0.0015 (0.0069)	
training:	Epoch: [109][105/233]	Loss 0.0008 (0.0069)	
training:	Epoch: [109][106/233]	Loss 0.0007 (0.0068)	
training:	Epoch: [109][107/233]	Loss 0.0007 (0.0067)	
training:	Epoch: [109][108/233]	Loss 0.0017 (0.0067)	
training:	Epoch: [109][109/233]	Loss 0.0012 (0.0066)	
training:	Epoch: [109][110/233]	Loss 0.0008 (0.0066)	
training:	Epoch: [109][111/233]	Loss 0.0008 (0.0065)	
training:	Epoch: [109][112/233]	Loss 0.0015 (0.0065)	
training:	Epoch: [109][113/233]	Loss 0.0008 (0.0064)	
training:	Epoch: [109][114/233]	Loss 0.0012 (0.0064)	
training:	Epoch: [109][115/233]	Loss 0.0010 (0.0063)	
training:	Epoch: [109][116/233]	Loss 0.0007 (0.0063)	
training:	Epoch: [109][117/233]	Loss 0.0008 (0.0062)	
training:	Epoch: [109][118/233]	Loss 0.0008 (0.0062)	
training:	Epoch: [109][119/233]	Loss 0.0008 (0.0062)	
training:	Epoch: [109][120/233]	Loss 0.0019 (0.0061)	
training:	Epoch: [109][121/233]	Loss 0.0010 (0.0061)	
training:	Epoch: [109][122/233]	Loss 0.0007 (0.0060)	
training:	Epoch: [109][123/233]	Loss 0.0009 (0.0060)	
training:	Epoch: [109][124/233]	Loss 0.0009 (0.0060)	
training:	Epoch: [109][125/233]	Loss 0.0436 (0.0063)	
training:	Epoch: [109][126/233]	Loss 0.0041 (0.0062)	
training:	Epoch: [109][127/233]	Loss 0.0279 (0.0064)	
training:	Epoch: [109][128/233]	Loss 0.0015 (0.0064)	
training:	Epoch: [109][129/233]	Loss 0.0055 (0.0064)	
training:	Epoch: [109][130/233]	Loss 0.0007 (0.0063)	
training:	Epoch: [109][131/233]	Loss 0.0006 (0.0063)	
training:	Epoch: [109][132/233]	Loss 0.0012 (0.0062)	
training:	Epoch: [109][133/233]	Loss 0.0007 (0.0062)	
training:	Epoch: [109][134/233]	Loss 0.0008 (0.0062)	
training:	Epoch: [109][135/233]	Loss 0.0007 (0.0061)	
training:	Epoch: [109][136/233]	Loss 0.0011 (0.0061)	
training:	Epoch: [109][137/233]	Loss 0.0025 (0.0061)	
training:	Epoch: [109][138/233]	Loss 0.0007 (0.0060)	
training:	Epoch: [109][139/233]	Loss 0.0008 (0.0060)	
training:	Epoch: [109][140/233]	Loss 0.0016 (0.0059)	
training:	Epoch: [109][141/233]	Loss 0.0008 (0.0059)	
training:	Epoch: [109][142/233]	Loss 0.0335 (0.0061)	
training:	Epoch: [109][143/233]	Loss 0.0009 (0.0061)	
training:	Epoch: [109][144/233]	Loss 0.0008 (0.0060)	
training:	Epoch: [109][145/233]	Loss 0.0095 (0.0061)	
training:	Epoch: [109][146/233]	Loss 0.0008 (0.0060)	
training:	Epoch: [109][147/233]	Loss 0.0008 (0.0060)	
training:	Epoch: [109][148/233]	Loss 0.0009 (0.0059)	
training:	Epoch: [109][149/233]	Loss 0.0008 (0.0059)	
training:	Epoch: [109][150/233]	Loss 0.0008 (0.0059)	
training:	Epoch: [109][151/233]	Loss 0.0012 (0.0058)	
training:	Epoch: [109][152/233]	Loss 0.0891 (0.0064)	
training:	Epoch: [109][153/233]	Loss 0.0013 (0.0064)	
training:	Epoch: [109][154/233]	Loss 0.0007 (0.0063)	
training:	Epoch: [109][155/233]	Loss 0.0007 (0.0063)	
training:	Epoch: [109][156/233]	Loss 0.0008 (0.0063)	
training:	Epoch: [109][157/233]	Loss 0.0847 (0.0068)	
training:	Epoch: [109][158/233]	Loss 0.0017 (0.0067)	
training:	Epoch: [109][159/233]	Loss 0.0008 (0.0067)	
training:	Epoch: [109][160/233]	Loss 0.0016 (0.0067)	
training:	Epoch: [109][161/233]	Loss 0.0011 (0.0066)	
training:	Epoch: [109][162/233]	Loss 0.0040 (0.0066)	
training:	Epoch: [109][163/233]	Loss 0.0012 (0.0066)	
training:	Epoch: [109][164/233]	Loss 0.0009 (0.0065)	
training:	Epoch: [109][165/233]	Loss 0.0017 (0.0065)	
training:	Epoch: [109][166/233]	Loss 0.0008 (0.0065)	
training:	Epoch: [109][167/233]	Loss 0.0017 (0.0064)	
training:	Epoch: [109][168/233]	Loss 0.0143 (0.0065)	
training:	Epoch: [109][169/233]	Loss 0.0025 (0.0065)	
training:	Epoch: [109][170/233]	Loss 0.0008 (0.0064)	
training:	Epoch: [109][171/233]	Loss 0.0011 (0.0064)	
training:	Epoch: [109][172/233]	Loss 0.0065 (0.0064)	
training:	Epoch: [109][173/233]	Loss 0.0081 (0.0064)	
training:	Epoch: [109][174/233]	Loss 0.0614 (0.0067)	
training:	Epoch: [109][175/233]	Loss 0.0010 (0.0067)	
training:	Epoch: [109][176/233]	Loss 0.0009 (0.0067)	
training:	Epoch: [109][177/233]	Loss 0.0007 (0.0066)	
training:	Epoch: [109][178/233]	Loss 0.0009 (0.0066)	
training:	Epoch: [109][179/233]	Loss 0.0007 (0.0066)	
training:	Epoch: [109][180/233]	Loss 0.0008 (0.0065)	
training:	Epoch: [109][181/233]	Loss 0.0030 (0.0065)	
training:	Epoch: [109][182/233]	Loss 0.0012 (0.0065)	
training:	Epoch: [109][183/233]	Loss 0.0008 (0.0065)	
training:	Epoch: [109][184/233]	Loss 0.0008 (0.0064)	
training:	Epoch: [109][185/233]	Loss 0.0010 (0.0064)	
training:	Epoch: [109][186/233]	Loss 0.0011 (0.0064)	
training:	Epoch: [109][187/233]	Loss 0.0020 (0.0063)	
training:	Epoch: [109][188/233]	Loss 0.0019 (0.0063)	
training:	Epoch: [109][189/233]	Loss 0.0008 (0.0063)	
training:	Epoch: [109][190/233]	Loss 0.0008 (0.0063)	
training:	Epoch: [109][191/233]	Loss 0.0019 (0.0062)	
training:	Epoch: [109][192/233]	Loss 0.0006 (0.0062)	
training:	Epoch: [109][193/233]	Loss 0.0012 (0.0062)	
training:	Epoch: [109][194/233]	Loss 0.0178 (0.0062)	
training:	Epoch: [109][195/233]	Loss 0.0013 (0.0062)	
training:	Epoch: [109][196/233]	Loss 0.0012 (0.0062)	
training:	Epoch: [109][197/233]	Loss 0.0009 (0.0062)	
training:	Epoch: [109][198/233]	Loss 0.0007 (0.0061)	
training:	Epoch: [109][199/233]	Loss 0.0014 (0.0061)	
training:	Epoch: [109][200/233]	Loss 0.0022 (0.0061)	
training:	Epoch: [109][201/233]	Loss 0.0011 (0.0061)	
training:	Epoch: [109][202/233]	Loss 0.0007 (0.0060)	
training:	Epoch: [109][203/233]	Loss 0.0034 (0.0060)	
training:	Epoch: [109][204/233]	Loss 0.0009 (0.0060)	
training:	Epoch: [109][205/233]	Loss 0.0008 (0.0060)	
training:	Epoch: [109][206/233]	Loss 0.0043 (0.0060)	
training:	Epoch: [109][207/233]	Loss 0.0012 (0.0059)	
training:	Epoch: [109][208/233]	Loss 0.0013 (0.0059)	
training:	Epoch: [109][209/233]	Loss 0.0120 (0.0060)	
training:	Epoch: [109][210/233]	Loss 0.0007 (0.0059)	
training:	Epoch: [109][211/233]	Loss 0.0009 (0.0059)	
training:	Epoch: [109][212/233]	Loss 0.0030 (0.0059)	
training:	Epoch: [109][213/233]	Loss 0.0009 (0.0059)	
training:	Epoch: [109][214/233]	Loss 0.0009 (0.0058)	
training:	Epoch: [109][215/233]	Loss 0.0007 (0.0058)	
training:	Epoch: [109][216/233]	Loss 0.0013 (0.0058)	
training:	Epoch: [109][217/233]	Loss 0.0011 (0.0058)	
training:	Epoch: [109][218/233]	Loss 0.0007 (0.0058)	
training:	Epoch: [109][219/233]	Loss 0.0009 (0.0057)	
training:	Epoch: [109][220/233]	Loss 0.0310 (0.0058)	
training:	Epoch: [109][221/233]	Loss 0.0010 (0.0058)	
training:	Epoch: [109][222/233]	Loss 0.0006 (0.0058)	
training:	Epoch: [109][223/233]	Loss 0.0007 (0.0058)	
training:	Epoch: [109][224/233]	Loss 0.0024 (0.0058)	
training:	Epoch: [109][225/233]	Loss 0.0007 (0.0057)	
training:	Epoch: [109][226/233]	Loss 0.0009 (0.0057)	
training:	Epoch: [109][227/233]	Loss 0.0156 (0.0058)	
training:	Epoch: [109][228/233]	Loss 0.0013 (0.0057)	
training:	Epoch: [109][229/233]	Loss 0.0056 (0.0057)	
training:	Epoch: [109][230/233]	Loss 0.0049 (0.0057)	
training:	Epoch: [109][231/233]	Loss 0.0030 (0.0057)	
training:	Epoch: [109][232/233]	Loss 0.0035 (0.0057)	
training:	Epoch: [109][233/233]	Loss 0.0016 (0.0057)	
Training:	 Loss: 0.0057

Training:	 ACC: 0.9995 0.9995 0.9992 0.9997
Validation:	 ACC: 0.7871 0.7849 0.7406 0.8337
Validation:	 Best_BACC: 0.8162 0.8154 0.7998 0.8326
Validation:	 Loss: 1.1603
Pretraining:	Epoch 110/200
----------
training:	Epoch: [110][1/233]	Loss 0.0629 (0.0629)	
training:	Epoch: [110][2/233]	Loss 0.0007 (0.0318)	
training:	Epoch: [110][3/233]	Loss 0.0007 (0.0214)	
training:	Epoch: [110][4/233]	Loss 0.0010 (0.0163)	
training:	Epoch: [110][5/233]	Loss 0.0058 (0.0142)	
training:	Epoch: [110][6/233]	Loss 0.0008 (0.0120)	
training:	Epoch: [110][7/233]	Loss 0.0033 (0.0107)	
training:	Epoch: [110][8/233]	Loss 0.0058 (0.0101)	
training:	Epoch: [110][9/233]	Loss 0.0007 (0.0091)	
training:	Epoch: [110][10/233]	Loss 0.0007 (0.0082)	
training:	Epoch: [110][11/233]	Loss 0.0007 (0.0075)	
training:	Epoch: [110][12/233]	Loss 0.0007 (0.0070)	
training:	Epoch: [110][13/233]	Loss 0.0012 (0.0065)	
training:	Epoch: [110][14/233]	Loss 0.0013 (0.0062)	
training:	Epoch: [110][15/233]	Loss 0.0011 (0.0058)	
training:	Epoch: [110][16/233]	Loss 0.0009 (0.0055)	
training:	Epoch: [110][17/233]	Loss 0.0017 (0.0053)	
training:	Epoch: [110][18/233]	Loss 0.0014 (0.0051)	
training:	Epoch: [110][19/233]	Loss 0.0007 (0.0048)	
training:	Epoch: [110][20/233]	Loss 0.1042 (0.0098)	
training:	Epoch: [110][21/233]	Loss 0.0008 (0.0094)	
training:	Epoch: [110][22/233]	Loss 0.0012 (0.0090)	
training:	Epoch: [110][23/233]	Loss 0.0015 (0.0087)	
training:	Epoch: [110][24/233]	Loss 0.0008 (0.0084)	
training:	Epoch: [110][25/233]	Loss 0.0007 (0.0080)	
training:	Epoch: [110][26/233]	Loss 0.1193 (0.0123)	
training:	Epoch: [110][27/233]	Loss 0.0008 (0.0119)	
training:	Epoch: [110][28/233]	Loss 0.0025 (0.0116)	
training:	Epoch: [110][29/233]	Loss 0.0084 (0.0115)	
training:	Epoch: [110][30/233]	Loss 0.0009 (0.0111)	
training:	Epoch: [110][31/233]	Loss 0.0008 (0.0108)	
training:	Epoch: [110][32/233]	Loss 0.0011 (0.0105)	
training:	Epoch: [110][33/233]	Loss 0.0011 (0.0102)	
training:	Epoch: [110][34/233]	Loss 0.0011 (0.0099)	
training:	Epoch: [110][35/233]	Loss 0.0097 (0.0099)	
training:	Epoch: [110][36/233]	Loss 0.0020 (0.0097)	
training:	Epoch: [110][37/233]	Loss 0.0008 (0.0095)	
training:	Epoch: [110][38/233]	Loss 0.0007 (0.0092)	
training:	Epoch: [110][39/233]	Loss 0.0014 (0.0090)	
training:	Epoch: [110][40/233]	Loss 0.0011 (0.0088)	
training:	Epoch: [110][41/233]	Loss 0.0008 (0.0086)	
training:	Epoch: [110][42/233]	Loss 0.0007 (0.0084)	
training:	Epoch: [110][43/233]	Loss 0.0015 (0.0083)	
training:	Epoch: [110][44/233]	Loss 0.0006 (0.0081)	
training:	Epoch: [110][45/233]	Loss 0.0011 (0.0079)	
training:	Epoch: [110][46/233]	Loss 0.0008 (0.0078)	
training:	Epoch: [110][47/233]	Loss 0.0008 (0.0076)	
training:	Epoch: [110][48/233]	Loss 0.0007 (0.0075)	
training:	Epoch: [110][49/233]	Loss 0.0008 (0.0074)	
training:	Epoch: [110][50/233]	Loss 0.0009 (0.0072)	
training:	Epoch: [110][51/233]	Loss 0.0010 (0.0071)	
training:	Epoch: [110][52/233]	Loss 0.0010 (0.0070)	
training:	Epoch: [110][53/233]	Loss 0.0007 (0.0069)	
training:	Epoch: [110][54/233]	Loss 0.0008 (0.0068)	
training:	Epoch: [110][55/233]	Loss 0.0011 (0.0067)	
training:	Epoch: [110][56/233]	Loss 0.0024 (0.0066)	
training:	Epoch: [110][57/233]	Loss 0.0006 (0.0065)	
training:	Epoch: [110][58/233]	Loss 0.0008 (0.0064)	
training:	Epoch: [110][59/233]	Loss 0.0007 (0.0063)	
training:	Epoch: [110][60/233]	Loss 0.1892 (0.0093)	
training:	Epoch: [110][61/233]	Loss 0.0009 (0.0092)	
training:	Epoch: [110][62/233]	Loss 0.0054 (0.0091)	
training:	Epoch: [110][63/233]	Loss 0.0064 (0.0091)	
training:	Epoch: [110][64/233]	Loss 0.0009 (0.0090)	
training:	Epoch: [110][65/233]	Loss 0.0017 (0.0089)	
training:	Epoch: [110][66/233]	Loss 0.0013 (0.0087)	
training:	Epoch: [110][67/233]	Loss 0.0044 (0.0087)	
training:	Epoch: [110][68/233]	Loss 0.0009 (0.0086)	
training:	Epoch: [110][69/233]	Loss 0.0108 (0.0086)	
training:	Epoch: [110][70/233]	Loss 0.0006 (0.0085)	
training:	Epoch: [110][71/233]	Loss 0.0992 (0.0098)	
training:	Epoch: [110][72/233]	Loss 0.0010 (0.0096)	
training:	Epoch: [110][73/233]	Loss 0.0013 (0.0095)	
training:	Epoch: [110][74/233]	Loss 0.0010 (0.0094)	
training:	Epoch: [110][75/233]	Loss 0.0014 (0.0093)	
training:	Epoch: [110][76/233]	Loss 0.0016 (0.0092)	
training:	Epoch: [110][77/233]	Loss 0.0010 (0.0091)	
training:	Epoch: [110][78/233]	Loss 0.0007 (0.0090)	
training:	Epoch: [110][79/233]	Loss 0.0183 (0.0091)	
training:	Epoch: [110][80/233]	Loss 0.0012 (0.0090)	
training:	Epoch: [110][81/233]	Loss 0.0006 (0.0089)	
training:	Epoch: [110][82/233]	Loss 0.0010 (0.0088)	
training:	Epoch: [110][83/233]	Loss 0.0288 (0.0090)	
training:	Epoch: [110][84/233]	Loss 0.0010 (0.0089)	
training:	Epoch: [110][85/233]	Loss 0.0034 (0.0089)	
training:	Epoch: [110][86/233]	Loss 0.0017 (0.0088)	
training:	Epoch: [110][87/233]	Loss 0.0373 (0.0091)	
training:	Epoch: [110][88/233]	Loss 0.0127 (0.0092)	
training:	Epoch: [110][89/233]	Loss 0.0013 (0.0091)	
training:	Epoch: [110][90/233]	Loss 0.0008 (0.0090)	
training:	Epoch: [110][91/233]	Loss 0.0008 (0.0089)	
training:	Epoch: [110][92/233]	Loss 0.0007 (0.0088)	
training:	Epoch: [110][93/233]	Loss 0.0008 (0.0087)	
training:	Epoch: [110][94/233]	Loss 0.0017 (0.0086)	
training:	Epoch: [110][95/233]	Loss 0.0024 (0.0086)	
training:	Epoch: [110][96/233]	Loss 0.0115 (0.0086)	
training:	Epoch: [110][97/233]	Loss 0.0007 (0.0085)	
training:	Epoch: [110][98/233]	Loss 0.0008 (0.0085)	
training:	Epoch: [110][99/233]	Loss 0.0006 (0.0084)	
training:	Epoch: [110][100/233]	Loss 0.0106 (0.0084)	
training:	Epoch: [110][101/233]	Loss 0.0007 (0.0083)	
training:	Epoch: [110][102/233]	Loss 0.0007 (0.0082)	
training:	Epoch: [110][103/233]	Loss 0.0402 (0.0086)	
training:	Epoch: [110][104/233]	Loss 0.0013 (0.0085)	
training:	Epoch: [110][105/233]	Loss 0.0008 (0.0084)	
training:	Epoch: [110][106/233]	Loss 0.0007 (0.0083)	
training:	Epoch: [110][107/233]	Loss 0.0010 (0.0083)	
training:	Epoch: [110][108/233]	Loss 0.0015 (0.0082)	
training:	Epoch: [110][109/233]	Loss 0.0009 (0.0081)	
training:	Epoch: [110][110/233]	Loss 0.0013 (0.0081)	
training:	Epoch: [110][111/233]	Loss 0.0741 (0.0087)	
training:	Epoch: [110][112/233]	Loss 0.0008 (0.0086)	
training:	Epoch: [110][113/233]	Loss 0.0007 (0.0085)	
training:	Epoch: [110][114/233]	Loss 0.0008 (0.0085)	
training:	Epoch: [110][115/233]	Loss 0.0006 (0.0084)	
training:	Epoch: [110][116/233]	Loss 0.0010 (0.0083)	
training:	Epoch: [110][117/233]	Loss 0.0011 (0.0083)	
training:	Epoch: [110][118/233]	Loss 0.0015 (0.0082)	
training:	Epoch: [110][119/233]	Loss 0.0008 (0.0082)	
training:	Epoch: [110][120/233]	Loss 0.0008 (0.0081)	
training:	Epoch: [110][121/233]	Loss 0.0069 (0.0081)	
training:	Epoch: [110][122/233]	Loss 0.0031 (0.0080)	
training:	Epoch: [110][123/233]	Loss 0.0008 (0.0080)	
training:	Epoch: [110][124/233]	Loss 0.0017 (0.0079)	
training:	Epoch: [110][125/233]	Loss 0.0371 (0.0082)	
training:	Epoch: [110][126/233]	Loss 0.0010 (0.0081)	
training:	Epoch: [110][127/233]	Loss 0.0023 (0.0081)	
training:	Epoch: [110][128/233]	Loss 0.0046 (0.0080)	
training:	Epoch: [110][129/233]	Loss 0.0203 (0.0081)	
training:	Epoch: [110][130/233]	Loss 0.0007 (0.0081)	
training:	Epoch: [110][131/233]	Loss 0.0013 (0.0080)	
training:	Epoch: [110][132/233]	Loss 0.0007 (0.0080)	
training:	Epoch: [110][133/233]	Loss 0.0009 (0.0079)	
training:	Epoch: [110][134/233]	Loss 0.0007 (0.0079)	
training:	Epoch: [110][135/233]	Loss 0.0007 (0.0078)	
training:	Epoch: [110][136/233]	Loss 0.0008 (0.0078)	
training:	Epoch: [110][137/233]	Loss 0.0013 (0.0077)	
training:	Epoch: [110][138/233]	Loss 0.0007 (0.0077)	
training:	Epoch: [110][139/233]	Loss 0.0010 (0.0076)	
training:	Epoch: [110][140/233]	Loss 0.0010 (0.0076)	
training:	Epoch: [110][141/233]	Loss 0.0007 (0.0075)	
training:	Epoch: [110][142/233]	Loss 0.0010 (0.0075)	
training:	Epoch: [110][143/233]	Loss 0.0007 (0.0074)	
training:	Epoch: [110][144/233]	Loss 0.0009 (0.0074)	
training:	Epoch: [110][145/233]	Loss 0.0007 (0.0073)	
training:	Epoch: [110][146/233]	Loss 0.0008 (0.0073)	
training:	Epoch: [110][147/233]	Loss 0.0024 (0.0072)	
training:	Epoch: [110][148/233]	Loss 0.0009 (0.0072)	
training:	Epoch: [110][149/233]	Loss 0.2379 (0.0088)	
training:	Epoch: [110][150/233]	Loss 0.0008 (0.0087)	
training:	Epoch: [110][151/233]	Loss 0.0021 (0.0087)	
training:	Epoch: [110][152/233]	Loss 0.0006 (0.0086)	
training:	Epoch: [110][153/233]	Loss 0.0007 (0.0086)	
training:	Epoch: [110][154/233]	Loss 0.0364 (0.0087)	
training:	Epoch: [110][155/233]	Loss 0.0007 (0.0087)	
training:	Epoch: [110][156/233]	Loss 0.0007 (0.0086)	
training:	Epoch: [110][157/233]	Loss 0.0019 (0.0086)	
training:	Epoch: [110][158/233]	Loss 0.0144 (0.0086)	
training:	Epoch: [110][159/233]	Loss 0.0025 (0.0086)	
training:	Epoch: [110][160/233]	Loss 0.0014 (0.0085)	
training:	Epoch: [110][161/233]	Loss 0.0024 (0.0085)	
training:	Epoch: [110][162/233]	Loss 0.0018 (0.0085)	
training:	Epoch: [110][163/233]	Loss 0.0009 (0.0084)	
training:	Epoch: [110][164/233]	Loss 0.0008 (0.0084)	
training:	Epoch: [110][165/233]	Loss 0.0007 (0.0083)	
training:	Epoch: [110][166/233]	Loss 0.0007 (0.0083)	
training:	Epoch: [110][167/233]	Loss 0.0008 (0.0082)	
training:	Epoch: [110][168/233]	Loss 0.0009 (0.0082)	
training:	Epoch: [110][169/233]	Loss 0.0014 (0.0081)	
training:	Epoch: [110][170/233]	Loss 0.0008 (0.0081)	
training:	Epoch: [110][171/233]	Loss 0.0012 (0.0081)	
training:	Epoch: [110][172/233]	Loss 0.0229 (0.0082)	
training:	Epoch: [110][173/233]	Loss 0.0007 (0.0081)	
training:	Epoch: [110][174/233]	Loss 0.0009 (0.0081)	
training:	Epoch: [110][175/233]	Loss 0.0022 (0.0080)	
training:	Epoch: [110][176/233]	Loss 0.0012 (0.0080)	
training:	Epoch: [110][177/233]	Loss 0.0169 (0.0080)	
training:	Epoch: [110][178/233]	Loss 0.0014 (0.0080)	
training:	Epoch: [110][179/233]	Loss 0.0008 (0.0080)	
training:	Epoch: [110][180/233]	Loss 0.0008 (0.0079)	
training:	Epoch: [110][181/233]	Loss 0.0132 (0.0080)	
training:	Epoch: [110][182/233]	Loss 0.0018 (0.0079)	
training:	Epoch: [110][183/233]	Loss 0.0151 (0.0080)	
training:	Epoch: [110][184/233]	Loss 0.0007 (0.0079)	
training:	Epoch: [110][185/233]	Loss 0.0007 (0.0079)	
training:	Epoch: [110][186/233]	Loss 0.0128 (0.0079)	
training:	Epoch: [110][187/233]	Loss 0.0014 (0.0079)	
training:	Epoch: [110][188/233]	Loss 0.0007 (0.0078)	
training:	Epoch: [110][189/233]	Loss 0.0006 (0.0078)	
training:	Epoch: [110][190/233]	Loss 0.0008 (0.0078)	
training:	Epoch: [110][191/233]	Loss 0.0008 (0.0077)	
training:	Epoch: [110][192/233]	Loss 0.0008 (0.0077)	
training:	Epoch: [110][193/233]	Loss 0.0007 (0.0077)	
training:	Epoch: [110][194/233]	Loss 0.0008 (0.0076)	
training:	Epoch: [110][195/233]	Loss 0.0009 (0.0076)	
training:	Epoch: [110][196/233]	Loss 0.0012 (0.0076)	
training:	Epoch: [110][197/233]	Loss 0.0006 (0.0075)	
training:	Epoch: [110][198/233]	Loss 0.0026 (0.0075)	
training:	Epoch: [110][199/233]	Loss 0.0013 (0.0075)	
training:	Epoch: [110][200/233]	Loss 0.0060 (0.0075)	
training:	Epoch: [110][201/233]	Loss 0.0006 (0.0074)	
training:	Epoch: [110][202/233]	Loss 0.0008 (0.0074)	
training:	Epoch: [110][203/233]	Loss 0.0017 (0.0074)	
training:	Epoch: [110][204/233]	Loss 0.0006 (0.0073)	
training:	Epoch: [110][205/233]	Loss 0.0007 (0.0073)	
training:	Epoch: [110][206/233]	Loss 0.0014 (0.0073)	
training:	Epoch: [110][207/233]	Loss 0.0009 (0.0072)	
training:	Epoch: [110][208/233]	Loss 0.0007 (0.0072)	
training:	Epoch: [110][209/233]	Loss 0.0009 (0.0072)	
training:	Epoch: [110][210/233]	Loss 0.0006 (0.0071)	
training:	Epoch: [110][211/233]	Loss 0.0008 (0.0071)	
training:	Epoch: [110][212/233]	Loss 0.0009 (0.0071)	
training:	Epoch: [110][213/233]	Loss 0.0007 (0.0070)	
training:	Epoch: [110][214/233]	Loss 0.0011 (0.0070)	
training:	Epoch: [110][215/233]	Loss 0.0006 (0.0070)	
training:	Epoch: [110][216/233]	Loss 0.0008 (0.0070)	
training:	Epoch: [110][217/233]	Loss 0.0066 (0.0070)	
training:	Epoch: [110][218/233]	Loss 0.0902 (0.0073)	
training:	Epoch: [110][219/233]	Loss 0.0006 (0.0073)	
training:	Epoch: [110][220/233]	Loss 0.0006 (0.0073)	
training:	Epoch: [110][221/233]	Loss 0.0019 (0.0073)	
training:	Epoch: [110][222/233]	Loss 0.0008 (0.0072)	
training:	Epoch: [110][223/233]	Loss 0.0008 (0.0072)	
training:	Epoch: [110][224/233]	Loss 0.0097 (0.0072)	
training:	Epoch: [110][225/233]	Loss 0.0013 (0.0072)	
training:	Epoch: [110][226/233]	Loss 0.1176 (0.0077)	
training:	Epoch: [110][227/233]	Loss 0.0007 (0.0076)	
training:	Epoch: [110][228/233]	Loss 0.0008 (0.0076)	
training:	Epoch: [110][229/233]	Loss 0.0021 (0.0076)	
training:	Epoch: [110][230/233]	Loss 0.0006 (0.0076)	
training:	Epoch: [110][231/233]	Loss 0.0096 (0.0076)	
training:	Epoch: [110][232/233]	Loss 0.0008 (0.0075)	
training:	Epoch: [110][233/233]	Loss 0.0024 (0.0075)	
Training:	 Loss: 0.0075

Training:	 ACC: 0.9997 0.9997 0.9997 0.9997
Validation:	 ACC: 0.7888 0.7881 0.7753 0.8022
Validation:	 Best_BACC: 0.8162 0.8154 0.7998 0.8326
Validation:	 Loss: 1.1392
Pretraining:	Epoch 111/200
----------
training:	Epoch: [111][1/233]	Loss 0.0008 (0.0008)	
training:	Epoch: [111][2/233]	Loss 0.0019 (0.0013)	
training:	Epoch: [111][3/233]	Loss 0.0006 (0.0011)	
training:	Epoch: [111][4/233]	Loss 0.0007 (0.0010)	
training:	Epoch: [111][5/233]	Loss 0.0006 (0.0009)	
training:	Epoch: [111][6/233]	Loss 0.0011 (0.0009)	
training:	Epoch: [111][7/233]	Loss 0.0009 (0.0009)	
training:	Epoch: [111][8/233]	Loss 0.0007 (0.0009)	
training:	Epoch: [111][9/233]	Loss 0.0034 (0.0012)	
training:	Epoch: [111][10/233]	Loss 0.0013 (0.0012)	
training:	Epoch: [111][11/233]	Loss 0.0626 (0.0068)	
training:	Epoch: [111][12/233]	Loss 0.0017 (0.0064)	
training:	Epoch: [111][13/233]	Loss 0.0007 (0.0059)	
training:	Epoch: [111][14/233]	Loss 0.2391 (0.0226)	
training:	Epoch: [111][15/233]	Loss 0.0007 (0.0211)	
training:	Epoch: [111][16/233]	Loss 0.0011 (0.0199)	
training:	Epoch: [111][17/233]	Loss 0.0011 (0.0188)	
training:	Epoch: [111][18/233]	Loss 0.0015 (0.0178)	
training:	Epoch: [111][19/233]	Loss 0.0013 (0.0169)	
training:	Epoch: [111][20/233]	Loss 0.0051 (0.0164)	
training:	Epoch: [111][21/233]	Loss 0.0012 (0.0156)	
training:	Epoch: [111][22/233]	Loss 0.1921 (0.0237)	
training:	Epoch: [111][23/233]	Loss 0.0011 (0.0227)	
training:	Epoch: [111][24/233]	Loss 0.0007 (0.0218)	
training:	Epoch: [111][25/233]	Loss 0.0013 (0.0209)	
training:	Epoch: [111][26/233]	Loss 0.0019 (0.0202)	
training:	Epoch: [111][27/233]	Loss 0.0258 (0.0204)	
training:	Epoch: [111][28/233]	Loss 0.0011 (0.0197)	
training:	Epoch: [111][29/233]	Loss 0.0015 (0.0191)	
training:	Epoch: [111][30/233]	Loss 0.0035 (0.0186)	
training:	Epoch: [111][31/233]	Loss 0.0311 (0.0190)	
training:	Epoch: [111][32/233]	Loss 0.0057 (0.0186)	
training:	Epoch: [111][33/233]	Loss 0.0007 (0.0180)	
training:	Epoch: [111][34/233]	Loss 0.0016 (0.0175)	
training:	Epoch: [111][35/233]	Loss 0.0008 (0.0171)	
training:	Epoch: [111][36/233]	Loss 0.0007 (0.0166)	
training:	Epoch: [111][37/233]	Loss 0.0012 (0.0162)	
training:	Epoch: [111][38/233]	Loss 0.0007 (0.0158)	
training:	Epoch: [111][39/233]	Loss 0.0007 (0.0154)	
training:	Epoch: [111][40/233]	Loss 0.0009 (0.0150)	
training:	Epoch: [111][41/233]	Loss 0.0452 (0.0158)	
training:	Epoch: [111][42/233]	Loss 0.0076 (0.0156)	
training:	Epoch: [111][43/233]	Loss 0.0126 (0.0155)	
training:	Epoch: [111][44/233]	Loss 0.0264 (0.0158)	
training:	Epoch: [111][45/233]	Loss 0.0015 (0.0154)	
training:	Epoch: [111][46/233]	Loss 0.3101 (0.0218)	
training:	Epoch: [111][47/233]	Loss 0.0018 (0.0214)	
training:	Epoch: [111][48/233]	Loss 0.0008 (0.0210)	
training:	Epoch: [111][49/233]	Loss 0.0072 (0.0207)	
training:	Epoch: [111][50/233]	Loss 0.0011 (0.0203)	
training:	Epoch: [111][51/233]	Loss 0.0008 (0.0199)	
training:	Epoch: [111][52/233]	Loss 0.0013 (0.0196)	
training:	Epoch: [111][53/233]	Loss 0.0009 (0.0192)	
training:	Epoch: [111][54/233]	Loss 0.0008 (0.0189)	
training:	Epoch: [111][55/233]	Loss 0.0010 (0.0186)	
training:	Epoch: [111][56/233]	Loss 0.0009 (0.0182)	
training:	Epoch: [111][57/233]	Loss 0.0011 (0.0179)	
training:	Epoch: [111][58/233]	Loss 0.0008 (0.0176)	
training:	Epoch: [111][59/233]	Loss 0.0120 (0.0175)	
training:	Epoch: [111][60/233]	Loss 0.0018 (0.0173)	
training:	Epoch: [111][61/233]	Loss 0.0057 (0.0171)	
training:	Epoch: [111][62/233]	Loss 0.0007 (0.0168)	
training:	Epoch: [111][63/233]	Loss 0.0010 (0.0166)	
training:	Epoch: [111][64/233]	Loss 0.0008 (0.0163)	
training:	Epoch: [111][65/233]	Loss 0.0007 (0.0161)	
training:	Epoch: [111][66/233]	Loss 0.0023 (0.0159)	
training:	Epoch: [111][67/233]	Loss 0.0008 (0.0157)	
training:	Epoch: [111][68/233]	Loss 0.0007 (0.0154)	
training:	Epoch: [111][69/233]	Loss 0.0018 (0.0152)	
training:	Epoch: [111][70/233]	Loss 0.0008 (0.0150)	
training:	Epoch: [111][71/233]	Loss 0.0016 (0.0148)	
training:	Epoch: [111][72/233]	Loss 0.0010 (0.0147)	
training:	Epoch: [111][73/233]	Loss 0.0007 (0.0145)	
training:	Epoch: [111][74/233]	Loss 0.0007 (0.0143)	
training:	Epoch: [111][75/233]	Loss 0.0018 (0.0141)	
training:	Epoch: [111][76/233]	Loss 0.0009 (0.0139)	
training:	Epoch: [111][77/233]	Loss 0.0006 (0.0138)	
training:	Epoch: [111][78/233]	Loss 0.0008 (0.0136)	
training:	Epoch: [111][79/233]	Loss 0.0010 (0.0134)	
training:	Epoch: [111][80/233]	Loss 0.0008 (0.0133)	
training:	Epoch: [111][81/233]	Loss 0.0028 (0.0132)	
training:	Epoch: [111][82/233]	Loss 0.0111 (0.0131)	
training:	Epoch: [111][83/233]	Loss 0.0006 (0.0130)	
training:	Epoch: [111][84/233]	Loss 0.0013 (0.0128)	
training:	Epoch: [111][85/233]	Loss 0.0008 (0.0127)	
training:	Epoch: [111][86/233]	Loss 0.0007 (0.0126)	
training:	Epoch: [111][87/233]	Loss 0.0014 (0.0124)	
training:	Epoch: [111][88/233]	Loss 0.0032 (0.0123)	
training:	Epoch: [111][89/233]	Loss 0.0008 (0.0122)	
training:	Epoch: [111][90/233]	Loss 0.0010 (0.0121)	
training:	Epoch: [111][91/233]	Loss 0.0006 (0.0119)	
training:	Epoch: [111][92/233]	Loss 0.0007 (0.0118)	
training:	Epoch: [111][93/233]	Loss 0.0012 (0.0117)	
training:	Epoch: [111][94/233]	Loss 0.0090 (0.0117)	
training:	Epoch: [111][95/233]	Loss 0.0006 (0.0116)	
training:	Epoch: [111][96/233]	Loss 0.0009 (0.0114)	
training:	Epoch: [111][97/233]	Loss 0.0007 (0.0113)	
training:	Epoch: [111][98/233]	Loss 0.0015 (0.0112)	
training:	Epoch: [111][99/233]	Loss 0.0067 (0.0112)	
training:	Epoch: [111][100/233]	Loss 0.0009 (0.0111)	
training:	Epoch: [111][101/233]	Loss 0.0009 (0.0110)	
training:	Epoch: [111][102/233]	Loss 0.0007 (0.0109)	
training:	Epoch: [111][103/233]	Loss 0.0074 (0.0109)	
training:	Epoch: [111][104/233]	Loss 0.0009 (0.0108)	
training:	Epoch: [111][105/233]	Loss 0.0015 (0.0107)	
training:	Epoch: [111][106/233]	Loss 0.0008 (0.0106)	
training:	Epoch: [111][107/233]	Loss 0.0015 (0.0105)	
training:	Epoch: [111][108/233]	Loss 0.0006 (0.0104)	
training:	Epoch: [111][109/233]	Loss 0.0007 (0.0103)	
training:	Epoch: [111][110/233]	Loss 0.0013 (0.0102)	
training:	Epoch: [111][111/233]	Loss 0.0042 (0.0102)	
training:	Epoch: [111][112/233]	Loss 0.0006 (0.0101)	
training:	Epoch: [111][113/233]	Loss 0.0036 (0.0100)	
training:	Epoch: [111][114/233]	Loss 0.0007 (0.0100)	
training:	Epoch: [111][115/233]	Loss 0.0007 (0.0099)	
training:	Epoch: [111][116/233]	Loss 0.0062 (0.0098)	
training:	Epoch: [111][117/233]	Loss 0.0008 (0.0098)	
training:	Epoch: [111][118/233]	Loss 0.0009 (0.0097)	
training:	Epoch: [111][119/233]	Loss 0.0019 (0.0096)	
training:	Epoch: [111][120/233]	Loss 0.0012 (0.0096)	
training:	Epoch: [111][121/233]	Loss 0.0016 (0.0095)	
training:	Epoch: [111][122/233]	Loss 0.0018 (0.0094)	
training:	Epoch: [111][123/233]	Loss 0.0012 (0.0094)	
training:	Epoch: [111][124/233]	Loss 0.0007 (0.0093)	
training:	Epoch: [111][125/233]	Loss 0.0008 (0.0092)	
training:	Epoch: [111][126/233]	Loss 0.0006 (0.0091)	
training:	Epoch: [111][127/233]	Loss 0.0007 (0.0091)	
training:	Epoch: [111][128/233]	Loss 0.0007 (0.0090)	
training:	Epoch: [111][129/233]	Loss 0.0097 (0.0090)	
training:	Epoch: [111][130/233]	Loss 0.0007 (0.0090)	
training:	Epoch: [111][131/233]	Loss 0.0013 (0.0089)	
training:	Epoch: [111][132/233]	Loss 0.0012 (0.0088)	
training:	Epoch: [111][133/233]	Loss 0.0007 (0.0088)	
training:	Epoch: [111][134/233]	Loss 0.0009 (0.0087)	
training:	Epoch: [111][135/233]	Loss 0.0006 (0.0087)	
training:	Epoch: [111][136/233]	Loss 0.0008 (0.0086)	
training:	Epoch: [111][137/233]	Loss 0.0008 (0.0085)	
training:	Epoch: [111][138/233]	Loss 0.0009 (0.0085)	
training:	Epoch: [111][139/233]	Loss 0.0007 (0.0084)	
training:	Epoch: [111][140/233]	Loss 0.0009 (0.0084)	
training:	Epoch: [111][141/233]	Loss 0.0007 (0.0083)	
training:	Epoch: [111][142/233]	Loss 0.0008 (0.0083)	
training:	Epoch: [111][143/233]	Loss 0.0021 (0.0082)	
training:	Epoch: [111][144/233]	Loss 0.0007 (0.0082)	
training:	Epoch: [111][145/233]	Loss 0.0008 (0.0081)	
training:	Epoch: [111][146/233]	Loss 0.0007 (0.0081)	
training:	Epoch: [111][147/233]	Loss 0.0007 (0.0080)	
training:	Epoch: [111][148/233]	Loss 0.0006 (0.0080)	
training:	Epoch: [111][149/233]	Loss 0.0009 (0.0079)	
training:	Epoch: [111][150/233]	Loss 0.0024 (0.0079)	
training:	Epoch: [111][151/233]	Loss 0.0007 (0.0078)	
training:	Epoch: [111][152/233]	Loss 0.0087 (0.0079)	
training:	Epoch: [111][153/233]	Loss 0.0011 (0.0078)	
training:	Epoch: [111][154/233]	Loss 0.0008 (0.0078)	
training:	Epoch: [111][155/233]	Loss 0.0007 (0.0077)	
training:	Epoch: [111][156/233]	Loss 0.0242 (0.0078)	
training:	Epoch: [111][157/233]	Loss 0.0008 (0.0078)	
training:	Epoch: [111][158/233]	Loss 0.0006 (0.0077)	
training:	Epoch: [111][159/233]	Loss 0.0007 (0.0077)	
training:	Epoch: [111][160/233]	Loss 0.0010 (0.0076)	
training:	Epoch: [111][161/233]	Loss 0.0007 (0.0076)	
training:	Epoch: [111][162/233]	Loss 0.0007 (0.0076)	
training:	Epoch: [111][163/233]	Loss 0.0046 (0.0075)	
training:	Epoch: [111][164/233]	Loss 0.0006 (0.0075)	
training:	Epoch: [111][165/233]	Loss 0.0035 (0.0075)	
training:	Epoch: [111][166/233]	Loss 0.0118 (0.0075)	
training:	Epoch: [111][167/233]	Loss 0.0007 (0.0075)	
training:	Epoch: [111][168/233]	Loss 0.0006 (0.0074)	
training:	Epoch: [111][169/233]	Loss 0.0007 (0.0074)	
training:	Epoch: [111][170/233]	Loss 0.0007 (0.0073)	
training:	Epoch: [111][171/233]	Loss 0.0011 (0.0073)	
training:	Epoch: [111][172/233]	Loss 0.0006 (0.0073)	
training:	Epoch: [111][173/233]	Loss 0.0006 (0.0072)	
training:	Epoch: [111][174/233]	Loss 0.0007 (0.0072)	
training:	Epoch: [111][175/233]	Loss 0.0010 (0.0072)	
training:	Epoch: [111][176/233]	Loss 0.0009 (0.0071)	
training:	Epoch: [111][177/233]	Loss 0.0007 (0.0071)	
training:	Epoch: [111][178/233]	Loss 0.0228 (0.0072)	
training:	Epoch: [111][179/233]	Loss 0.0009 (0.0071)	
training:	Epoch: [111][180/233]	Loss 0.0006 (0.0071)	
training:	Epoch: [111][181/233]	Loss 0.0007 (0.0071)	
training:	Epoch: [111][182/233]	Loss 0.0123 (0.0071)	
training:	Epoch: [111][183/233]	Loss 0.0007 (0.0071)	
training:	Epoch: [111][184/233]	Loss 0.0011 (0.0070)	
training:	Epoch: [111][185/233]	Loss 0.0008 (0.0070)	
training:	Epoch: [111][186/233]	Loss 0.0008 (0.0070)	
training:	Epoch: [111][187/233]	Loss 0.0048 (0.0069)	
training:	Epoch: [111][188/233]	Loss 0.0007 (0.0069)	
training:	Epoch: [111][189/233]	Loss 0.0007 (0.0069)	
training:	Epoch: [111][190/233]	Loss 0.0006 (0.0068)	
training:	Epoch: [111][191/233]	Loss 0.0010 (0.0068)	
training:	Epoch: [111][192/233]	Loss 0.0007 (0.0068)	
training:	Epoch: [111][193/233]	Loss 0.0008 (0.0068)	
training:	Epoch: [111][194/233]	Loss 0.0008 (0.0067)	
training:	Epoch: [111][195/233]	Loss 0.0010 (0.0067)	
training:	Epoch: [111][196/233]	Loss 0.0013 (0.0067)	
training:	Epoch: [111][197/233]	Loss 0.0009 (0.0066)	
training:	Epoch: [111][198/233]	Loss 0.0008 (0.0066)	
training:	Epoch: [111][199/233]	Loss 0.0016 (0.0066)	
training:	Epoch: [111][200/233]	Loss 0.0006 (0.0066)	
training:	Epoch: [111][201/233]	Loss 0.0017 (0.0065)	
training:	Epoch: [111][202/233]	Loss 0.0006 (0.0065)	
training:	Epoch: [111][203/233]	Loss 0.0009 (0.0065)	
training:	Epoch: [111][204/233]	Loss 0.0006 (0.0064)	
training:	Epoch: [111][205/233]	Loss 0.0010 (0.0064)	
training:	Epoch: [111][206/233]	Loss 0.0124 (0.0064)	
training:	Epoch: [111][207/233]	Loss 0.0008 (0.0064)	
training:	Epoch: [111][208/233]	Loss 0.0008 (0.0064)	
training:	Epoch: [111][209/233]	Loss 0.0007 (0.0064)	
training:	Epoch: [111][210/233]	Loss 0.0006 (0.0063)	
training:	Epoch: [111][211/233]	Loss 0.0009 (0.0063)	
training:	Epoch: [111][212/233]	Loss 0.0006 (0.0063)	
training:	Epoch: [111][213/233]	Loss 0.0007 (0.0063)	
training:	Epoch: [111][214/233]	Loss 0.0016 (0.0062)	
training:	Epoch: [111][215/233]	Loss 0.0237 (0.0063)	
training:	Epoch: [111][216/233]	Loss 0.0449 (0.0065)	
training:	Epoch: [111][217/233]	Loss 0.0006 (0.0065)	
training:	Epoch: [111][218/233]	Loss 0.0006 (0.0064)	
training:	Epoch: [111][219/233]	Loss 0.0007 (0.0064)	
training:	Epoch: [111][220/233]	Loss 0.0008 (0.0064)	
training:	Epoch: [111][221/233]	Loss 0.0007 (0.0064)	
training:	Epoch: [111][222/233]	Loss 0.0007 (0.0063)	
training:	Epoch: [111][223/233]	Loss 0.0008 (0.0063)	
training:	Epoch: [111][224/233]	Loss 0.0011 (0.0063)	
training:	Epoch: [111][225/233]	Loss 0.0007 (0.0063)	
training:	Epoch: [111][226/233]	Loss 0.0006 (0.0062)	
training:	Epoch: [111][227/233]	Loss 0.0006 (0.0062)	
training:	Epoch: [111][228/233]	Loss 0.0006 (0.0062)	
training:	Epoch: [111][229/233]	Loss 0.0009 (0.0062)	
training:	Epoch: [111][230/233]	Loss 0.1903 (0.0070)	
training:	Epoch: [111][231/233]	Loss 0.0013 (0.0069)	
training:	Epoch: [111][232/233]	Loss 0.0020 (0.0069)	
training:	Epoch: [111][233/233]	Loss 0.0006 (0.0069)	
Training:	 Loss: 0.0069

Training:	 ACC: 0.9997 0.9997 0.9997 0.9997
Validation:	 ACC: 0.7925 0.7897 0.7344 0.8506
Validation:	 Best_BACC: 0.8162 0.8154 0.7998 0.8326
Validation:	 Loss: 1.1799
Pretraining:	Epoch 112/200
----------
training:	Epoch: [112][1/233]	Loss 0.0020 (0.0020)	
training:	Epoch: [112][2/233]	Loss 0.0346 (0.0183)	
training:	Epoch: [112][3/233]	Loss 0.0008 (0.0124)	
training:	Epoch: [112][4/233]	Loss 0.0006 (0.0095)	
training:	Epoch: [112][5/233]	Loss 0.0006 (0.0077)	
training:	Epoch: [112][6/233]	Loss 0.0007 (0.0065)	
training:	Epoch: [112][7/233]	Loss 0.0011 (0.0058)	
training:	Epoch: [112][8/233]	Loss 0.0007 (0.0051)	
training:	Epoch: [112][9/233]	Loss 0.0006 (0.0046)	
training:	Epoch: [112][10/233]	Loss 0.0007 (0.0042)	
training:	Epoch: [112][11/233]	Loss 0.0035 (0.0042)	
training:	Epoch: [112][12/233]	Loss 0.0036 (0.0041)	
training:	Epoch: [112][13/233]	Loss 0.0029 (0.0040)	
training:	Epoch: [112][14/233]	Loss 0.0006 (0.0038)	
training:	Epoch: [112][15/233]	Loss 0.0008 (0.0036)	
training:	Epoch: [112][16/233]	Loss 0.0627 (0.0073)	
training:	Epoch: [112][17/233]	Loss 0.0007 (0.0069)	
training:	Epoch: [112][18/233]	Loss 0.0006 (0.0065)	
training:	Epoch: [112][19/233]	Loss 0.0010 (0.0062)	
training:	Epoch: [112][20/233]	Loss 0.0006 (0.0060)	
training:	Epoch: [112][21/233]	Loss 0.0008 (0.0057)	
training:	Epoch: [112][22/233]	Loss 0.0178 (0.0063)	
training:	Epoch: [112][23/233]	Loss 0.0020 (0.0061)	
training:	Epoch: [112][24/233]	Loss 0.0471 (0.0078)	
training:	Epoch: [112][25/233]	Loss 0.0009 (0.0075)	
training:	Epoch: [112][26/233]	Loss 0.0006 (0.0073)	
training:	Epoch: [112][27/233]	Loss 0.0013 (0.0070)	
training:	Epoch: [112][28/233]	Loss 0.0006 (0.0068)	
training:	Epoch: [112][29/233]	Loss 0.2392 (0.0148)	
training:	Epoch: [112][30/233]	Loss 0.0007 (0.0143)	
training:	Epoch: [112][31/233]	Loss 0.0005 (0.0139)	
training:	Epoch: [112][32/233]	Loss 0.0014 (0.0135)	
training:	Epoch: [112][33/233]	Loss 0.0007 (0.0131)	
training:	Epoch: [112][34/233]	Loss 0.1076 (0.0159)	
training:	Epoch: [112][35/233]	Loss 0.0010 (0.0155)	
training:	Epoch: [112][36/233]	Loss 0.0008 (0.0151)	
training:	Epoch: [112][37/233]	Loss 0.0011 (0.0147)	
training:	Epoch: [112][38/233]	Loss 0.0007 (0.0143)	
training:	Epoch: [112][39/233]	Loss 0.0006 (0.0140)	
training:	Epoch: [112][40/233]	Loss 0.0007 (0.0136)	
training:	Epoch: [112][41/233]	Loss 0.0014 (0.0133)	
training:	Epoch: [112][42/233]	Loss 0.0015 (0.0131)	
training:	Epoch: [112][43/233]	Loss 0.0009 (0.0128)	
training:	Epoch: [112][44/233]	Loss 0.0007 (0.0125)	
training:	Epoch: [112][45/233]	Loss 0.0010 (0.0122)	
training:	Epoch: [112][46/233]	Loss 0.0027 (0.0120)	
training:	Epoch: [112][47/233]	Loss 0.0006 (0.0118)	
training:	Epoch: [112][48/233]	Loss 0.0023 (0.0116)	
training:	Epoch: [112][49/233]	Loss 0.0020 (0.0114)	
training:	Epoch: [112][50/233]	Loss 0.0009 (0.0112)	
training:	Epoch: [112][51/233]	Loss 0.0011 (0.0110)	
training:	Epoch: [112][52/233]	Loss 0.0007 (0.0108)	
training:	Epoch: [112][53/233]	Loss 0.0007 (0.0106)	
training:	Epoch: [112][54/233]	Loss 0.0008 (0.0104)	
training:	Epoch: [112][55/233]	Loss 0.0006 (0.0102)	
training:	Epoch: [112][56/233]	Loss 0.0019 (0.0101)	
training:	Epoch: [112][57/233]	Loss 0.0007 (0.0099)	
training:	Epoch: [112][58/233]	Loss 0.0006 (0.0098)	
training:	Epoch: [112][59/233]	Loss 0.0022 (0.0096)	
training:	Epoch: [112][60/233]	Loss 0.0140 (0.0097)	
training:	Epoch: [112][61/233]	Loss 0.0007 (0.0096)	
training:	Epoch: [112][62/233]	Loss 0.0007 (0.0094)	
training:	Epoch: [112][63/233]	Loss 0.0008 (0.0093)	
training:	Epoch: [112][64/233]	Loss 0.0006 (0.0091)	
training:	Epoch: [112][65/233]	Loss 0.0014 (0.0090)	
training:	Epoch: [112][66/233]	Loss 0.0007 (0.0089)	
training:	Epoch: [112][67/233]	Loss 0.0009 (0.0088)	
training:	Epoch: [112][68/233]	Loss 0.0008 (0.0087)	
training:	Epoch: [112][69/233]	Loss 0.0006 (0.0085)	
training:	Epoch: [112][70/233]	Loss 0.0006 (0.0084)	
training:	Epoch: [112][71/233]	Loss 0.0340 (0.0088)	
training:	Epoch: [112][72/233]	Loss 0.0009 (0.0087)	
training:	Epoch: [112][73/233]	Loss 0.0007 (0.0086)	
training:	Epoch: [112][74/233]	Loss 0.0007 (0.0085)	
training:	Epoch: [112][75/233]	Loss 0.0008 (0.0084)	
training:	Epoch: [112][76/233]	Loss 0.0030 (0.0083)	
training:	Epoch: [112][77/233]	Loss 0.0007 (0.0082)	
training:	Epoch: [112][78/233]	Loss 0.0012 (0.0081)	
training:	Epoch: [112][79/233]	Loss 0.0040 (0.0081)	
training:	Epoch: [112][80/233]	Loss 0.0006 (0.0080)	
training:	Epoch: [112][81/233]	Loss 0.0006 (0.0079)	
training:	Epoch: [112][82/233]	Loss 0.0012 (0.0078)	
training:	Epoch: [112][83/233]	Loss 0.0008 (0.0077)	
training:	Epoch: [112][84/233]	Loss 0.0007 (0.0076)	
training:	Epoch: [112][85/233]	Loss 0.2113 (0.0100)	
training:	Epoch: [112][86/233]	Loss 0.0014 (0.0099)	
training:	Epoch: [112][87/233]	Loss 0.0007 (0.0098)	
training:	Epoch: [112][88/233]	Loss 0.0007 (0.0097)	
training:	Epoch: [112][89/233]	Loss 0.0007 (0.0096)	
training:	Epoch: [112][90/233]	Loss 0.1088 (0.0107)	
training:	Epoch: [112][91/233]	Loss 0.0028 (0.0106)	
training:	Epoch: [112][92/233]	Loss 0.0008 (0.0105)	
training:	Epoch: [112][93/233]	Loss 0.2298 (0.0129)	
training:	Epoch: [112][94/233]	Loss 0.0007 (0.0127)	
training:	Epoch: [112][95/233]	Loss 0.0008 (0.0126)	
training:	Epoch: [112][96/233]	Loss 0.0008 (0.0125)	
training:	Epoch: [112][97/233]	Loss 0.0011 (0.0124)	
training:	Epoch: [112][98/233]	Loss 0.0011 (0.0123)	
training:	Epoch: [112][99/233]	Loss 0.0008 (0.0121)	
training:	Epoch: [112][100/233]	Loss 0.0009 (0.0120)	
training:	Epoch: [112][101/233]	Loss 0.0013 (0.0119)	
training:	Epoch: [112][102/233]	Loss 0.0008 (0.0118)	
training:	Epoch: [112][103/233]	Loss 0.0008 (0.0117)	
training:	Epoch: [112][104/233]	Loss 0.0007 (0.0116)	
training:	Epoch: [112][105/233]	Loss 0.0007 (0.0115)	
training:	Epoch: [112][106/233]	Loss 0.0006 (0.0114)	
training:	Epoch: [112][107/233]	Loss 0.0007 (0.0113)	
training:	Epoch: [112][108/233]	Loss 0.0008 (0.0112)	
training:	Epoch: [112][109/233]	Loss 0.0008 (0.0111)	
training:	Epoch: [112][110/233]	Loss 0.0019 (0.0110)	
training:	Epoch: [112][111/233]	Loss 0.0039 (0.0110)	
training:	Epoch: [112][112/233]	Loss 0.0009 (0.0109)	
training:	Epoch: [112][113/233]	Loss 0.0007 (0.0108)	
training:	Epoch: [112][114/233]	Loss 0.0045 (0.0107)	
training:	Epoch: [112][115/233]	Loss 0.0007 (0.0106)	
training:	Epoch: [112][116/233]	Loss 0.0013 (0.0106)	
training:	Epoch: [112][117/233]	Loss 0.0053 (0.0105)	
training:	Epoch: [112][118/233]	Loss 0.0008 (0.0104)	
training:	Epoch: [112][119/233]	Loss 0.0017 (0.0104)	
training:	Epoch: [112][120/233]	Loss 0.0006 (0.0103)	
training:	Epoch: [112][121/233]	Loss 0.0009 (0.0102)	
training:	Epoch: [112][122/233]	Loss 0.0007 (0.0101)	
training:	Epoch: [112][123/233]	Loss 0.0038 (0.0101)	
training:	Epoch: [112][124/233]	Loss 0.0084 (0.0101)	
training:	Epoch: [112][125/233]	Loss 0.0008 (0.0100)	
training:	Epoch: [112][126/233]	Loss 0.0007 (0.0099)	
training:	Epoch: [112][127/233]	Loss 0.0008 (0.0098)	
training:	Epoch: [112][128/233]	Loss 0.0007 (0.0098)	
training:	Epoch: [112][129/233]	Loss 0.0889 (0.0104)	
training:	Epoch: [112][130/233]	Loss 0.0008 (0.0103)	
training:	Epoch: [112][131/233]	Loss 0.0116 (0.0103)	
training:	Epoch: [112][132/233]	Loss 0.0108 (0.0103)	
training:	Epoch: [112][133/233]	Loss 0.0007 (0.0102)	
training:	Epoch: [112][134/233]	Loss 0.0008 (0.0102)	
training:	Epoch: [112][135/233]	Loss 0.0285 (0.0103)	
training:	Epoch: [112][136/233]	Loss 0.0009 (0.0102)	
training:	Epoch: [112][137/233]	Loss 0.0013 (0.0102)	
training:	Epoch: [112][138/233]	Loss 0.0137 (0.0102)	
training:	Epoch: [112][139/233]	Loss 0.0007 (0.0101)	
training:	Epoch: [112][140/233]	Loss 0.0017 (0.0101)	
training:	Epoch: [112][141/233]	Loss 0.0010 (0.0100)	
training:	Epoch: [112][142/233]	Loss 0.0007 (0.0099)	
training:	Epoch: [112][143/233]	Loss 0.0049 (0.0099)	
training:	Epoch: [112][144/233]	Loss 0.0009 (0.0098)	
training:	Epoch: [112][145/233]	Loss 0.0018 (0.0098)	
training:	Epoch: [112][146/233]	Loss 0.0007 (0.0097)	
training:	Epoch: [112][147/233]	Loss 0.0009 (0.0097)	
training:	Epoch: [112][148/233]	Loss 0.0008 (0.0096)	
training:	Epoch: [112][149/233]	Loss 0.0010 (0.0095)	
training:	Epoch: [112][150/233]	Loss 0.0009 (0.0095)	
training:	Epoch: [112][151/233]	Loss 0.0008 (0.0094)	
training:	Epoch: [112][152/233]	Loss 0.0006 (0.0094)	
training:	Epoch: [112][153/233]	Loss 0.0007 (0.0093)	
training:	Epoch: [112][154/233]	Loss 0.0006 (0.0093)	
training:	Epoch: [112][155/233]	Loss 0.0038 (0.0092)	
training:	Epoch: [112][156/233]	Loss 0.0008 (0.0092)	
training:	Epoch: [112][157/233]	Loss 0.0007 (0.0091)	
training:	Epoch: [112][158/233]	Loss 0.0007 (0.0091)	
training:	Epoch: [112][159/233]	Loss 0.0014 (0.0090)	
training:	Epoch: [112][160/233]	Loss 0.0007 (0.0090)	
training:	Epoch: [112][161/233]	Loss 0.0007 (0.0089)	
training:	Epoch: [112][162/233]	Loss 0.0011 (0.0089)	
training:	Epoch: [112][163/233]	Loss 0.0006 (0.0088)	
training:	Epoch: [112][164/233]	Loss 0.0013 (0.0088)	
training:	Epoch: [112][165/233]	Loss 0.0012 (0.0087)	
training:	Epoch: [112][166/233]	Loss 0.0006 (0.0087)	
training:	Epoch: [112][167/233]	Loss 0.0009 (0.0086)	
training:	Epoch: [112][168/233]	Loss 0.0082 (0.0086)	
training:	Epoch: [112][169/233]	Loss 0.0008 (0.0086)	
training:	Epoch: [112][170/233]	Loss 0.0006 (0.0085)	
training:	Epoch: [112][171/233]	Loss 0.0008 (0.0085)	
training:	Epoch: [112][172/233]	Loss 0.0023 (0.0085)	
training:	Epoch: [112][173/233]	Loss 0.0006 (0.0084)	
training:	Epoch: [112][174/233]	Loss 0.0007 (0.0084)	
training:	Epoch: [112][175/233]	Loss 0.0009 (0.0083)	
training:	Epoch: [112][176/233]	Loss 0.0008 (0.0083)	
training:	Epoch: [112][177/233]	Loss 0.0006 (0.0082)	
training:	Epoch: [112][178/233]	Loss 0.0011 (0.0082)	
training:	Epoch: [112][179/233]	Loss 0.2984 (0.0098)	
training:	Epoch: [112][180/233]	Loss 0.0009 (0.0098)	
training:	Epoch: [112][181/233]	Loss 0.0011 (0.0097)	
training:	Epoch: [112][182/233]	Loss 0.0006 (0.0097)	
training:	Epoch: [112][183/233]	Loss 0.0007 (0.0096)	
training:	Epoch: [112][184/233]	Loss 0.0009 (0.0096)	
training:	Epoch: [112][185/233]	Loss 0.0008 (0.0095)	
training:	Epoch: [112][186/233]	Loss 0.0009 (0.0095)	
training:	Epoch: [112][187/233]	Loss 0.0009 (0.0094)	
training:	Epoch: [112][188/233]	Loss 0.0008 (0.0094)	
training:	Epoch: [112][189/233]	Loss 0.0008 (0.0093)	
training:	Epoch: [112][190/233]	Loss 0.0008 (0.0093)	
training:	Epoch: [112][191/233]	Loss 0.0014 (0.0093)	
training:	Epoch: [112][192/233]	Loss 0.0007 (0.0092)	
training:	Epoch: [112][193/233]	Loss 0.0010 (0.0092)	
training:	Epoch: [112][194/233]	Loss 0.0013 (0.0091)	
training:	Epoch: [112][195/233]	Loss 0.0007 (0.0091)	
training:	Epoch: [112][196/233]	Loss 0.0008 (0.0090)	
training:	Epoch: [112][197/233]	Loss 0.0008 (0.0090)	
training:	Epoch: [112][198/233]	Loss 0.0007 (0.0090)	
training:	Epoch: [112][199/233]	Loss 0.0014 (0.0089)	
training:	Epoch: [112][200/233]	Loss 0.0006 (0.0089)	
training:	Epoch: [112][201/233]	Loss 0.0013 (0.0088)	
training:	Epoch: [112][202/233]	Loss 0.1395 (0.0095)	
training:	Epoch: [112][203/233]	Loss 0.0006 (0.0094)	
training:	Epoch: [112][204/233]	Loss 0.0008 (0.0094)	
training:	Epoch: [112][205/233]	Loss 0.0007 (0.0094)	
training:	Epoch: [112][206/233]	Loss 0.0026 (0.0093)	
training:	Epoch: [112][207/233]	Loss 0.0006 (0.0093)	
training:	Epoch: [112][208/233]	Loss 0.0267 (0.0094)	
training:	Epoch: [112][209/233]	Loss 0.0011 (0.0093)	
training:	Epoch: [112][210/233]	Loss 0.0010 (0.0093)	
training:	Epoch: [112][211/233]	Loss 0.0009 (0.0092)	
training:	Epoch: [112][212/233]	Loss 0.0007 (0.0092)	
training:	Epoch: [112][213/233]	Loss 0.0007 (0.0092)	
training:	Epoch: [112][214/233]	Loss 0.0007 (0.0091)	
training:	Epoch: [112][215/233]	Loss 0.0008 (0.0091)	
training:	Epoch: [112][216/233]	Loss 0.0008 (0.0090)	
training:	Epoch: [112][217/233]	Loss 0.0008 (0.0090)	
training:	Epoch: [112][218/233]	Loss 0.0009 (0.0090)	
training:	Epoch: [112][219/233]	Loss 0.0009 (0.0089)	
training:	Epoch: [112][220/233]	Loss 0.0007 (0.0089)	
training:	Epoch: [112][221/233]	Loss 0.0007 (0.0089)	
training:	Epoch: [112][222/233]	Loss 0.0007 (0.0088)	
training:	Epoch: [112][223/233]	Loss 0.0007 (0.0088)	
training:	Epoch: [112][224/233]	Loss 0.0009 (0.0088)	
training:	Epoch: [112][225/233]	Loss 0.0013 (0.0087)	
training:	Epoch: [112][226/233]	Loss 0.0010 (0.0087)	
training:	Epoch: [112][227/233]	Loss 0.0008 (0.0087)	
training:	Epoch: [112][228/233]	Loss 0.0543 (0.0089)	
training:	Epoch: [112][229/233]	Loss 0.0009 (0.0088)	
training:	Epoch: [112][230/233]	Loss 0.0006 (0.0088)	
training:	Epoch: [112][231/233]	Loss 0.0660 (0.0090)	
training:	Epoch: [112][232/233]	Loss 0.0007 (0.0090)	
training:	Epoch: [112][233/233]	Loss 0.0010 (0.0090)	
Training:	 Loss: 0.0089

Training:	 ACC: 0.9997 0.9997 0.9997 0.9997
Validation:	 ACC: 0.7901 0.7881 0.7487 0.8315
Validation:	 Best_BACC: 0.8162 0.8154 0.7998 0.8326
Validation:	 Loss: 1.1752
Pretraining:	Epoch 113/200
----------
training:	Epoch: [113][1/233]	Loss 0.0007 (0.0007)	
training:	Epoch: [113][2/233]	Loss 0.0008 (0.0007)	
training:	Epoch: [113][3/233]	Loss 0.0007 (0.0007)	
training:	Epoch: [113][4/233]	Loss 0.0008 (0.0007)	
training:	Epoch: [113][5/233]	Loss 0.0007 (0.0007)	
training:	Epoch: [113][6/233]	Loss 0.0008 (0.0007)	
training:	Epoch: [113][7/233]	Loss 0.0007 (0.0007)	
training:	Epoch: [113][8/233]	Loss 0.0020 (0.0009)	
training:	Epoch: [113][9/233]	Loss 0.0009 (0.0009)	
training:	Epoch: [113][10/233]	Loss 0.0008 (0.0009)	
training:	Epoch: [113][11/233]	Loss 0.0007 (0.0009)	
training:	Epoch: [113][12/233]	Loss 0.0008 (0.0009)	
training:	Epoch: [113][13/233]	Loss 0.0008 (0.0009)	
training:	Epoch: [113][14/233]	Loss 0.0026 (0.0010)	
training:	Epoch: [113][15/233]	Loss 0.2425 (0.0171)	
training:	Epoch: [113][16/233]	Loss 0.0016 (0.0161)	
training:	Epoch: [113][17/233]	Loss 0.0187 (0.0163)	
training:	Epoch: [113][18/233]	Loss 0.0037 (0.0156)	
training:	Epoch: [113][19/233]	Loss 0.0303 (0.0163)	
training:	Epoch: [113][20/233]	Loss 0.0082 (0.0159)	
training:	Epoch: [113][21/233]	Loss 0.0008 (0.0152)	
training:	Epoch: [113][22/233]	Loss 0.0006 (0.0145)	
training:	Epoch: [113][23/233]	Loss 0.0007 (0.0139)	
training:	Epoch: [113][24/233]	Loss 0.0007 (0.0134)	
training:	Epoch: [113][25/233]	Loss 0.0007 (0.0129)	
training:	Epoch: [113][26/233]	Loss 0.0009 (0.0124)	
training:	Epoch: [113][27/233]	Loss 0.0014 (0.0120)	
training:	Epoch: [113][28/233]	Loss 0.0006 (0.0116)	
training:	Epoch: [113][29/233]	Loss 0.0007 (0.0112)	
training:	Epoch: [113][30/233]	Loss 0.0007 (0.0109)	
training:	Epoch: [113][31/233]	Loss 0.0151 (0.0110)	
training:	Epoch: [113][32/233]	Loss 0.0007 (0.0107)	
training:	Epoch: [113][33/233]	Loss 0.0007 (0.0104)	
training:	Epoch: [113][34/233]	Loss 0.0013 (0.0101)	
training:	Epoch: [113][35/233]	Loss 0.0026 (0.0099)	
training:	Epoch: [113][36/233]	Loss 0.0014 (0.0097)	
training:	Epoch: [113][37/233]	Loss 0.0022 (0.0095)	
training:	Epoch: [113][38/233]	Loss 0.0031 (0.0093)	
training:	Epoch: [113][39/233]	Loss 0.0008 (0.0091)	
training:	Epoch: [113][40/233]	Loss 0.0084 (0.0091)	
training:	Epoch: [113][41/233]	Loss 0.0012 (0.0089)	
training:	Epoch: [113][42/233]	Loss 0.0010 (0.0087)	
training:	Epoch: [113][43/233]	Loss 0.0166 (0.0089)	
training:	Epoch: [113][44/233]	Loss 0.0010 (0.0087)	
training:	Epoch: [113][45/233]	Loss 0.0009 (0.0085)	
training:	Epoch: [113][46/233]	Loss 0.0009 (0.0084)	
training:	Epoch: [113][47/233]	Loss 0.0008 (0.0082)	
training:	Epoch: [113][48/233]	Loss 0.0008 (0.0080)	
training:	Epoch: [113][49/233]	Loss 0.0011 (0.0079)	
training:	Epoch: [113][50/233]	Loss 0.0010 (0.0078)	
training:	Epoch: [113][51/233]	Loss 0.0010 (0.0076)	
training:	Epoch: [113][52/233]	Loss 0.0009 (0.0075)	
training:	Epoch: [113][53/233]	Loss 0.0015 (0.0074)	
training:	Epoch: [113][54/233]	Loss 0.0010 (0.0073)	
training:	Epoch: [113][55/233]	Loss 0.2103 (0.0110)	
training:	Epoch: [113][56/233]	Loss 0.0011 (0.0108)	
training:	Epoch: [113][57/233]	Loss 0.0007 (0.0106)	
training:	Epoch: [113][58/233]	Loss 0.0018 (0.0105)	
training:	Epoch: [113][59/233]	Loss 0.0009 (0.0103)	
training:	Epoch: [113][60/233]	Loss 0.0035 (0.0102)	
training:	Epoch: [113][61/233]	Loss 0.0007 (0.0100)	
training:	Epoch: [113][62/233]	Loss 0.0008 (0.0099)	
training:	Epoch: [113][63/233]	Loss 0.0011 (0.0097)	
training:	Epoch: [113][64/233]	Loss 0.0013 (0.0096)	
training:	Epoch: [113][65/233]	Loss 0.0011 (0.0095)	
training:	Epoch: [113][66/233]	Loss 0.0064 (0.0094)	
training:	Epoch: [113][67/233]	Loss 0.0011 (0.0093)	
training:	Epoch: [113][68/233]	Loss 0.0008 (0.0092)	
training:	Epoch: [113][69/233]	Loss 0.0008 (0.0091)	
training:	Epoch: [113][70/233]	Loss 0.0011 (0.0089)	
training:	Epoch: [113][71/233]	Loss 0.0009 (0.0088)	
training:	Epoch: [113][72/233]	Loss 0.0007 (0.0087)	
training:	Epoch: [113][73/233]	Loss 0.0007 (0.0086)	
training:	Epoch: [113][74/233]	Loss 0.0108 (0.0086)	
training:	Epoch: [113][75/233]	Loss 0.0010 (0.0085)	
training:	Epoch: [113][76/233]	Loss 0.0007 (0.0084)	
training:	Epoch: [113][77/233]	Loss 0.0029 (0.0084)	
training:	Epoch: [113][78/233]	Loss 0.0006 (0.0083)	
training:	Epoch: [113][79/233]	Loss 0.0013 (0.0082)	
training:	Epoch: [113][80/233]	Loss 0.0017 (0.0081)	
training:	Epoch: [113][81/233]	Loss 0.0007 (0.0080)	
training:	Epoch: [113][82/233]	Loss 0.0008 (0.0079)	
training:	Epoch: [113][83/233]	Loss 0.0008 (0.0078)	
training:	Epoch: [113][84/233]	Loss 0.0827 (0.0087)	
training:	Epoch: [113][85/233]	Loss 0.0390 (0.0091)	
training:	Epoch: [113][86/233]	Loss 0.0007 (0.0090)	
training:	Epoch: [113][87/233]	Loss 0.0065 (0.0089)	
training:	Epoch: [113][88/233]	Loss 0.0008 (0.0089)	
training:	Epoch: [113][89/233]	Loss 0.0007 (0.0088)	
training:	Epoch: [113][90/233]	Loss 0.0012 (0.0087)	
training:	Epoch: [113][91/233]	Loss 0.0008 (0.0086)	
training:	Epoch: [113][92/233]	Loss 0.0007 (0.0085)	
training:	Epoch: [113][93/233]	Loss 0.0007 (0.0084)	
training:	Epoch: [113][94/233]	Loss 0.0006 (0.0083)	
training:	Epoch: [113][95/233]	Loss 0.0007 (0.0083)	
training:	Epoch: [113][96/233]	Loss 0.0007 (0.0082)	
training:	Epoch: [113][97/233]	Loss 0.0007 (0.0081)	
training:	Epoch: [113][98/233]	Loss 0.0009 (0.0080)	
training:	Epoch: [113][99/233]	Loss 0.0007 (0.0080)	
training:	Epoch: [113][100/233]	Loss 0.0222 (0.0081)	
training:	Epoch: [113][101/233]	Loss 0.0015 (0.0080)	
training:	Epoch: [113][102/233]	Loss 0.0007 (0.0080)	
training:	Epoch: [113][103/233]	Loss 0.0027 (0.0079)	
training:	Epoch: [113][104/233]	Loss 0.0008 (0.0078)	
training:	Epoch: [113][105/233]	Loss 0.0017 (0.0078)	
training:	Epoch: [113][106/233]	Loss 0.0009 (0.0077)	
training:	Epoch: [113][107/233]	Loss 0.0017 (0.0077)	
training:	Epoch: [113][108/233]	Loss 0.0030 (0.0076)	
training:	Epoch: [113][109/233]	Loss 0.0012 (0.0076)	
training:	Epoch: [113][110/233]	Loss 0.0019 (0.0075)	
training:	Epoch: [113][111/233]	Loss 0.0029 (0.0075)	
training:	Epoch: [113][112/233]	Loss 0.0007 (0.0074)	
training:	Epoch: [113][113/233]	Loss 0.0010 (0.0073)	
training:	Epoch: [113][114/233]	Loss 0.0008 (0.0073)	
training:	Epoch: [113][115/233]	Loss 0.0015 (0.0072)	
training:	Epoch: [113][116/233]	Loss 0.0045 (0.0072)	
training:	Epoch: [113][117/233]	Loss 0.0006 (0.0072)	
training:	Epoch: [113][118/233]	Loss 0.0019 (0.0071)	
training:	Epoch: [113][119/233]	Loss 0.0021 (0.0071)	
training:	Epoch: [113][120/233]	Loss 0.0008 (0.0070)	
training:	Epoch: [113][121/233]	Loss 0.0022 (0.0070)	
training:	Epoch: [113][122/233]	Loss 0.0009 (0.0069)	
training:	Epoch: [113][123/233]	Loss 0.0008 (0.0069)	
training:	Epoch: [113][124/233]	Loss 0.0093 (0.0069)	
training:	Epoch: [113][125/233]	Loss 0.0007 (0.0068)	
training:	Epoch: [113][126/233]	Loss 0.0070 (0.0069)	
training:	Epoch: [113][127/233]	Loss 0.0008 (0.0068)	
training:	Epoch: [113][128/233]	Loss 0.0008 (0.0068)	
training:	Epoch: [113][129/233]	Loss 0.0014 (0.0067)	
training:	Epoch: [113][130/233]	Loss 0.0008 (0.0067)	
training:	Epoch: [113][131/233]	Loss 0.0007 (0.0066)	
training:	Epoch: [113][132/233]	Loss 0.0011 (0.0066)	
training:	Epoch: [113][133/233]	Loss 0.0009 (0.0065)	
training:	Epoch: [113][134/233]	Loss 0.0010 (0.0065)	
training:	Epoch: [113][135/233]	Loss 0.0008 (0.0065)	
training:	Epoch: [113][136/233]	Loss 0.0009 (0.0064)	
training:	Epoch: [113][137/233]	Loss 0.0006 (0.0064)	
training:	Epoch: [113][138/233]	Loss 0.0011 (0.0063)	
training:	Epoch: [113][139/233]	Loss 0.0006 (0.0063)	
training:	Epoch: [113][140/233]	Loss 0.0009 (0.0063)	
training:	Epoch: [113][141/233]	Loss 0.0010 (0.0062)	
training:	Epoch: [113][142/233]	Loss 0.0220 (0.0063)	
training:	Epoch: [113][143/233]	Loss 0.0008 (0.0063)	
training:	Epoch: [113][144/233]	Loss 0.0007 (0.0063)	
training:	Epoch: [113][145/233]	Loss 0.1119 (0.0070)	
training:	Epoch: [113][146/233]	Loss 0.0006 (0.0069)	
training:	Epoch: [113][147/233]	Loss 0.0006 (0.0069)	
training:	Epoch: [113][148/233]	Loss 0.0010 (0.0069)	
training:	Epoch: [113][149/233]	Loss 0.0012 (0.0068)	
training:	Epoch: [113][150/233]	Loss 0.0006 (0.0068)	
training:	Epoch: [113][151/233]	Loss 0.0011 (0.0067)	
training:	Epoch: [113][152/233]	Loss 0.0008 (0.0067)	
training:	Epoch: [113][153/233]	Loss 0.0011 (0.0067)	
training:	Epoch: [113][154/233]	Loss 0.0010 (0.0066)	
training:	Epoch: [113][155/233]	Loss 0.0010 (0.0066)	
training:	Epoch: [113][156/233]	Loss 0.0007 (0.0065)	
training:	Epoch: [113][157/233]	Loss 0.0009 (0.0065)	
training:	Epoch: [113][158/233]	Loss 0.0012 (0.0065)	
training:	Epoch: [113][159/233]	Loss 0.0006 (0.0064)	
training:	Epoch: [113][160/233]	Loss 0.0008 (0.0064)	
training:	Epoch: [113][161/233]	Loss 0.0007 (0.0064)	
training:	Epoch: [113][162/233]	Loss 0.0007 (0.0063)	
training:	Epoch: [113][163/233]	Loss 0.0006 (0.0063)	
training:	Epoch: [113][164/233]	Loss 0.0058 (0.0063)	
training:	Epoch: [113][165/233]	Loss 0.0006 (0.0063)	
training:	Epoch: [113][166/233]	Loss 0.0008 (0.0062)	
training:	Epoch: [113][167/233]	Loss 0.0007 (0.0062)	
training:	Epoch: [113][168/233]	Loss 0.0008 (0.0062)	
training:	Epoch: [113][169/233]	Loss 0.0009 (0.0061)	
training:	Epoch: [113][170/233]	Loss 0.0006 (0.0061)	
training:	Epoch: [113][171/233]	Loss 0.0006 (0.0061)	
training:	Epoch: [113][172/233]	Loss 0.0010 (0.0060)	
training:	Epoch: [113][173/233]	Loss 0.0041 (0.0060)	
training:	Epoch: [113][174/233]	Loss 0.0010 (0.0060)	
training:	Epoch: [113][175/233]	Loss 0.0014 (0.0060)	
training:	Epoch: [113][176/233]	Loss 0.0007 (0.0059)	
training:	Epoch: [113][177/233]	Loss 0.0006 (0.0059)	
training:	Epoch: [113][178/233]	Loss 0.0010 (0.0059)	
training:	Epoch: [113][179/233]	Loss 0.0006 (0.0059)	
training:	Epoch: [113][180/233]	Loss 0.0006 (0.0058)	
training:	Epoch: [113][181/233]	Loss 0.0010 (0.0058)	
training:	Epoch: [113][182/233]	Loss 0.0014 (0.0058)	
training:	Epoch: [113][183/233]	Loss 0.0007 (0.0057)	
training:	Epoch: [113][184/233]	Loss 0.0006 (0.0057)	
training:	Epoch: [113][185/233]	Loss 0.0008 (0.0057)	
training:	Epoch: [113][186/233]	Loss 0.0008 (0.0057)	
training:	Epoch: [113][187/233]	Loss 0.0006 (0.0056)	
training:	Epoch: [113][188/233]	Loss 0.0008 (0.0056)	
training:	Epoch: [113][189/233]	Loss 0.0009 (0.0056)	
training:	Epoch: [113][190/233]	Loss 0.0008 (0.0056)	
training:	Epoch: [113][191/233]	Loss 0.0028 (0.0056)	
training:	Epoch: [113][192/233]	Loss 0.0007 (0.0055)	
training:	Epoch: [113][193/233]	Loss 0.0006 (0.0055)	
training:	Epoch: [113][194/233]	Loss 0.0013 (0.0055)	
training:	Epoch: [113][195/233]	Loss 0.0006 (0.0055)	
training:	Epoch: [113][196/233]	Loss 0.0006 (0.0054)	
training:	Epoch: [113][197/233]	Loss 0.0008 (0.0054)	
training:	Epoch: [113][198/233]	Loss 0.0008 (0.0054)	
training:	Epoch: [113][199/233]	Loss 0.0007 (0.0054)	
training:	Epoch: [113][200/233]	Loss 0.0007 (0.0053)	
training:	Epoch: [113][201/233]	Loss 0.0008 (0.0053)	
training:	Epoch: [113][202/233]	Loss 0.0009 (0.0053)	
training:	Epoch: [113][203/233]	Loss 0.0014 (0.0053)	
training:	Epoch: [113][204/233]	Loss 0.0011 (0.0053)	
training:	Epoch: [113][205/233]	Loss 0.0010 (0.0052)	
training:	Epoch: [113][206/233]	Loss 0.0008 (0.0052)	
training:	Epoch: [113][207/233]	Loss 0.0010 (0.0052)	
training:	Epoch: [113][208/233]	Loss 0.0006 (0.0052)	
training:	Epoch: [113][209/233]	Loss 0.0008 (0.0051)	
training:	Epoch: [113][210/233]	Loss 0.0008 (0.0051)	
training:	Epoch: [113][211/233]	Loss 0.0006 (0.0051)	
training:	Epoch: [113][212/233]	Loss 0.0009 (0.0051)	
training:	Epoch: [113][213/233]	Loss 0.0009 (0.0051)	
training:	Epoch: [113][214/233]	Loss 0.0010 (0.0050)	
training:	Epoch: [113][215/233]	Loss 0.0006 (0.0050)	
training:	Epoch: [113][216/233]	Loss 0.0015 (0.0050)	
training:	Epoch: [113][217/233]	Loss 0.0009 (0.0050)	
training:	Epoch: [113][218/233]	Loss 0.0375 (0.0051)	
training:	Epoch: [113][219/233]	Loss 0.0010 (0.0051)	
training:	Epoch: [113][220/233]	Loss 0.0008 (0.0051)	
training:	Epoch: [113][221/233]	Loss 0.0006 (0.0051)	
training:	Epoch: [113][222/233]	Loss 0.0015 (0.0051)	
training:	Epoch: [113][223/233]	Loss 0.0007 (0.0050)	
training:	Epoch: [113][224/233]	Loss 0.0006 (0.0050)	
training:	Epoch: [113][225/233]	Loss 0.0064 (0.0050)	
training:	Epoch: [113][226/233]	Loss 0.0007 (0.0050)	
training:	Epoch: [113][227/233]	Loss 0.0007 (0.0050)	
training:	Epoch: [113][228/233]	Loss 0.0007 (0.0050)	
training:	Epoch: [113][229/233]	Loss 0.0010 (0.0050)	
training:	Epoch: [113][230/233]	Loss 0.0006 (0.0049)	
training:	Epoch: [113][231/233]	Loss 0.0006 (0.0049)	
training:	Epoch: [113][232/233]	Loss 0.0007 (0.0049)	
training:	Epoch: [113][233/233]	Loss 0.0006 (0.0049)	
Training:	 Loss: 0.0049

Training:	 ACC: 0.9997 0.9997 0.9997 0.9997
Validation:	 ACC: 0.7928 0.7924 0.7845 0.8011
Validation:	 Best_BACC: 0.8162 0.8154 0.7998 0.8326
Validation:	 Loss: 1.1466
Pretraining:	Epoch 114/200
----------
training:	Epoch: [114][1/233]	Loss 0.0006 (0.0006)	
training:	Epoch: [114][2/233]	Loss 0.0007 (0.0007)	
training:	Epoch: [114][3/233]	Loss 0.0006 (0.0007)	
training:	Epoch: [114][4/233]	Loss 0.0006 (0.0007)	
training:	Epoch: [114][5/233]	Loss 0.0231 (0.0051)	
training:	Epoch: [114][6/233]	Loss 0.0007 (0.0044)	
training:	Epoch: [114][7/233]	Loss 0.0006 (0.0039)	
training:	Epoch: [114][8/233]	Loss 0.0006 (0.0035)	
training:	Epoch: [114][9/233]	Loss 0.0007 (0.0031)	
training:	Epoch: [114][10/233]	Loss 0.0006 (0.0029)	
training:	Epoch: [114][11/233]	Loss 0.0041 (0.0030)	
training:	Epoch: [114][12/233]	Loss 0.0009 (0.0028)	
training:	Epoch: [114][13/233]	Loss 0.0006 (0.0027)	
training:	Epoch: [114][14/233]	Loss 0.2033 (0.0170)	
training:	Epoch: [114][15/233]	Loss 0.0009 (0.0159)	
training:	Epoch: [114][16/233]	Loss 0.0009 (0.0150)	
training:	Epoch: [114][17/233]	Loss 0.0007 (0.0141)	
training:	Epoch: [114][18/233]	Loss 0.0007 (0.0134)	
training:	Epoch: [114][19/233]	Loss 0.0011 (0.0127)	
training:	Epoch: [114][20/233]	Loss 0.0009 (0.0121)	
training:	Epoch: [114][21/233]	Loss 0.2466 (0.0233)	
training:	Epoch: [114][22/233]	Loss 0.0007 (0.0223)	
training:	Epoch: [114][23/233]	Loss 0.0034 (0.0215)	
training:	Epoch: [114][24/233]	Loss 0.0101 (0.0210)	
training:	Epoch: [114][25/233]	Loss 0.0006 (0.0202)	
training:	Epoch: [114][26/233]	Loss 0.0006 (0.0194)	
training:	Epoch: [114][27/233]	Loss 0.0009 (0.0187)	
training:	Epoch: [114][28/233]	Loss 0.0010 (0.0181)	
training:	Epoch: [114][29/233]	Loss 0.0006 (0.0175)	
training:	Epoch: [114][30/233]	Loss 0.0345 (0.0181)	
training:	Epoch: [114][31/233]	Loss 0.0006 (0.0175)	
training:	Epoch: [114][32/233]	Loss 0.0027 (0.0170)	
training:	Epoch: [114][33/233]	Loss 0.0007 (0.0165)	
training:	Epoch: [114][34/233]	Loss 0.0008 (0.0161)	
training:	Epoch: [114][35/233]	Loss 0.0007 (0.0156)	
training:	Epoch: [114][36/233]	Loss 0.0006 (0.0152)	
training:	Epoch: [114][37/233]	Loss 0.0021 (0.0149)	
training:	Epoch: [114][38/233]	Loss 0.0006 (0.0145)	
training:	Epoch: [114][39/233]	Loss 0.0009 (0.0141)	
training:	Epoch: [114][40/233]	Loss 0.0013 (0.0138)	
training:	Epoch: [114][41/233]	Loss 0.0014 (0.0135)	
training:	Epoch: [114][42/233]	Loss 0.0007 (0.0132)	
training:	Epoch: [114][43/233]	Loss 0.0041 (0.0130)	
training:	Epoch: [114][44/233]	Loss 0.0011 (0.0127)	
training:	Epoch: [114][45/233]	Loss 0.0019 (0.0125)	
training:	Epoch: [114][46/233]	Loss 0.0027 (0.0123)	
training:	Epoch: [114][47/233]	Loss 0.0007 (0.0120)	
training:	Epoch: [114][48/233]	Loss 0.0013 (0.0118)	
training:	Epoch: [114][49/233]	Loss 0.0007 (0.0116)	
training:	Epoch: [114][50/233]	Loss 0.0006 (0.0114)	
training:	Epoch: [114][51/233]	Loss 0.0006 (0.0112)	
training:	Epoch: [114][52/233]	Loss 0.0008 (0.0110)	
training:	Epoch: [114][53/233]	Loss 0.0007 (0.0108)	
training:	Epoch: [114][54/233]	Loss 0.0007 (0.0106)	
training:	Epoch: [114][55/233]	Loss 0.0009 (0.0104)	
training:	Epoch: [114][56/233]	Loss 0.0013 (0.0102)	
training:	Epoch: [114][57/233]	Loss 0.0009 (0.0101)	
training:	Epoch: [114][58/233]	Loss 0.0016 (0.0099)	
training:	Epoch: [114][59/233]	Loss 0.0037 (0.0098)	
training:	Epoch: [114][60/233]	Loss 0.0006 (0.0097)	
training:	Epoch: [114][61/233]	Loss 0.0007 (0.0095)	
training:	Epoch: [114][62/233]	Loss 0.0030 (0.0094)	
training:	Epoch: [114][63/233]	Loss 0.0010 (0.0093)	
training:	Epoch: [114][64/233]	Loss 0.0038 (0.0092)	
training:	Epoch: [114][65/233]	Loss 0.0006 (0.0091)	
training:	Epoch: [114][66/233]	Loss 0.0008 (0.0089)	
training:	Epoch: [114][67/233]	Loss 0.0009 (0.0088)	
training:	Epoch: [114][68/233]	Loss 0.0007 (0.0087)	
training:	Epoch: [114][69/233]	Loss 0.0007 (0.0086)	
training:	Epoch: [114][70/233]	Loss 0.0011 (0.0085)	
training:	Epoch: [114][71/233]	Loss 0.0008 (0.0084)	
training:	Epoch: [114][72/233]	Loss 0.0007 (0.0083)	
training:	Epoch: [114][73/233]	Loss 0.0006 (0.0082)	
training:	Epoch: [114][74/233]	Loss 0.0017 (0.0081)	
training:	Epoch: [114][75/233]	Loss 0.0007 (0.0080)	
training:	Epoch: [114][76/233]	Loss 0.0007 (0.0079)	
training:	Epoch: [114][77/233]	Loss 0.0082 (0.0079)	
training:	Epoch: [114][78/233]	Loss 0.0006 (0.0078)	
training:	Epoch: [114][79/233]	Loss 0.0637 (0.0085)	
training:	Epoch: [114][80/233]	Loss 0.0006 (0.0084)	
training:	Epoch: [114][81/233]	Loss 0.0007 (0.0083)	
training:	Epoch: [114][82/233]	Loss 0.0006 (0.0082)	
training:	Epoch: [114][83/233]	Loss 0.0007 (0.0081)	
training:	Epoch: [114][84/233]	Loss 0.0009 (0.0080)	
training:	Epoch: [114][85/233]	Loss 0.2084 (0.0104)	
training:	Epoch: [114][86/233]	Loss 0.0017 (0.0103)	
training:	Epoch: [114][87/233]	Loss 0.0006 (0.0102)	
training:	Epoch: [114][88/233]	Loss 0.0008 (0.0101)	
training:	Epoch: [114][89/233]	Loss 0.0006 (0.0100)	
training:	Epoch: [114][90/233]	Loss 0.0024 (0.0099)	
training:	Epoch: [114][91/233]	Loss 0.0007 (0.0098)	
training:	Epoch: [114][92/233]	Loss 0.0008 (0.0097)	
training:	Epoch: [114][93/233]	Loss 0.0010 (0.0096)	
training:	Epoch: [114][94/233]	Loss 0.0013 (0.0095)	
training:	Epoch: [114][95/233]	Loss 0.0011 (0.0094)	
training:	Epoch: [114][96/233]	Loss 0.0007 (0.0093)	
training:	Epoch: [114][97/233]	Loss 0.0119 (0.0093)	
training:	Epoch: [114][98/233]	Loss 0.0007 (0.0093)	
training:	Epoch: [114][99/233]	Loss 0.0010 (0.0092)	
training:	Epoch: [114][100/233]	Loss 0.0007 (0.0091)	
training:	Epoch: [114][101/233]	Loss 0.0006 (0.0090)	
training:	Epoch: [114][102/233]	Loss 0.0018 (0.0089)	
training:	Epoch: [114][103/233]	Loss 0.0012 (0.0089)	
training:	Epoch: [114][104/233]	Loss 0.0008 (0.0088)	
training:	Epoch: [114][105/233]	Loss 0.0008 (0.0087)	
training:	Epoch: [114][106/233]	Loss 0.0007 (0.0086)	
training:	Epoch: [114][107/233]	Loss 0.0007 (0.0086)	
training:	Epoch: [114][108/233]	Loss 0.0008 (0.0085)	
training:	Epoch: [114][109/233]	Loss 0.0007 (0.0084)	
training:	Epoch: [114][110/233]	Loss 0.0165 (0.0085)	
training:	Epoch: [114][111/233]	Loss 0.0007 (0.0084)	
training:	Epoch: [114][112/233]	Loss 0.0008 (0.0083)	
training:	Epoch: [114][113/233]	Loss 0.0006 (0.0083)	
training:	Epoch: [114][114/233]	Loss 0.0006 (0.0082)	
training:	Epoch: [114][115/233]	Loss 0.0007 (0.0081)	
training:	Epoch: [114][116/233]	Loss 0.0007 (0.0081)	
training:	Epoch: [114][117/233]	Loss 0.0084 (0.0081)	
training:	Epoch: [114][118/233]	Loss 0.0007 (0.0080)	
training:	Epoch: [114][119/233]	Loss 0.0026 (0.0080)	
training:	Epoch: [114][120/233]	Loss 0.0008 (0.0079)	
training:	Epoch: [114][121/233]	Loss 0.0007 (0.0079)	
training:	Epoch: [114][122/233]	Loss 0.0801 (0.0084)	
training:	Epoch: [114][123/233]	Loss 0.0009 (0.0084)	
training:	Epoch: [114][124/233]	Loss 0.0006 (0.0083)	
training:	Epoch: [114][125/233]	Loss 0.0009 (0.0083)	
training:	Epoch: [114][126/233]	Loss 0.0042 (0.0082)	
training:	Epoch: [114][127/233]	Loss 0.0007 (0.0082)	
training:	Epoch: [114][128/233]	Loss 0.0007 (0.0081)	
training:	Epoch: [114][129/233]	Loss 0.0007 (0.0081)	
training:	Epoch: [114][130/233]	Loss 0.0049 (0.0080)	
training:	Epoch: [114][131/233]	Loss 0.0008 (0.0080)	
training:	Epoch: [114][132/233]	Loss 0.0007 (0.0079)	
training:	Epoch: [114][133/233]	Loss 0.0006 (0.0079)	
training:	Epoch: [114][134/233]	Loss 0.0009 (0.0078)	
training:	Epoch: [114][135/233]	Loss 0.0012 (0.0078)	
training:	Epoch: [114][136/233]	Loss 0.0006 (0.0077)	
training:	Epoch: [114][137/233]	Loss 0.0006 (0.0077)	
training:	Epoch: [114][138/233]	Loss 0.0008 (0.0076)	
training:	Epoch: [114][139/233]	Loss 0.0007 (0.0076)	
training:	Epoch: [114][140/233]	Loss 0.0007 (0.0075)	
training:	Epoch: [114][141/233]	Loss 0.0007 (0.0075)	
training:	Epoch: [114][142/233]	Loss 0.0006 (0.0074)	
training:	Epoch: [114][143/233]	Loss 0.0008 (0.0074)	
training:	Epoch: [114][144/233]	Loss 0.0007 (0.0073)	
training:	Epoch: [114][145/233]	Loss 0.0013 (0.0073)	
training:	Epoch: [114][146/233]	Loss 0.0007 (0.0072)	
training:	Epoch: [114][147/233]	Loss 0.0040 (0.0072)	
training:	Epoch: [114][148/233]	Loss 0.0015 (0.0072)	
training:	Epoch: [114][149/233]	Loss 0.0010 (0.0071)	
training:	Epoch: [114][150/233]	Loss 0.0006 (0.0071)	
training:	Epoch: [114][151/233]	Loss 0.0006 (0.0070)	
training:	Epoch: [114][152/233]	Loss 0.0008 (0.0070)	
training:	Epoch: [114][153/233]	Loss 0.0009 (0.0070)	
training:	Epoch: [114][154/233]	Loss 0.0008 (0.0069)	
training:	Epoch: [114][155/233]	Loss 0.0006 (0.0069)	
training:	Epoch: [114][156/233]	Loss 0.0010 (0.0068)	
training:	Epoch: [114][157/233]	Loss 0.0020 (0.0068)	
training:	Epoch: [114][158/233]	Loss 0.0008 (0.0068)	
training:	Epoch: [114][159/233]	Loss 0.0007 (0.0067)	
training:	Epoch: [114][160/233]	Loss 0.0007 (0.0067)	
training:	Epoch: [114][161/233]	Loss 0.0007 (0.0067)	
training:	Epoch: [114][162/233]	Loss 0.0008 (0.0066)	
training:	Epoch: [114][163/233]	Loss 0.0008 (0.0066)	
training:	Epoch: [114][164/233]	Loss 0.0043 (0.0066)	
training:	Epoch: [114][165/233]	Loss 0.0006 (0.0065)	
training:	Epoch: [114][166/233]	Loss 0.0043 (0.0065)	
training:	Epoch: [114][167/233]	Loss 0.0054 (0.0065)	
training:	Epoch: [114][168/233]	Loss 0.0007 (0.0065)	
training:	Epoch: [114][169/233]	Loss 0.0008 (0.0065)	
training:	Epoch: [114][170/233]	Loss 0.0009 (0.0064)	
training:	Epoch: [114][171/233]	Loss 0.0007 (0.0064)	
training:	Epoch: [114][172/233]	Loss 0.0008 (0.0064)	
training:	Epoch: [114][173/233]	Loss 0.0008 (0.0063)	
training:	Epoch: [114][174/233]	Loss 0.0007 (0.0063)	
training:	Epoch: [114][175/233]	Loss 0.0013 (0.0063)	
training:	Epoch: [114][176/233]	Loss 0.0006 (0.0062)	
training:	Epoch: [114][177/233]	Loss 0.0006 (0.0062)	
training:	Epoch: [114][178/233]	Loss 0.0006 (0.0062)	
training:	Epoch: [114][179/233]	Loss 0.0012 (0.0061)	
training:	Epoch: [114][180/233]	Loss 0.0007 (0.0061)	
training:	Epoch: [114][181/233]	Loss 0.0015 (0.0061)	
training:	Epoch: [114][182/233]	Loss 0.0011 (0.0061)	
training:	Epoch: [114][183/233]	Loss 0.0009 (0.0060)	
training:	Epoch: [114][184/233]	Loss 0.0010 (0.0060)	
training:	Epoch: [114][185/233]	Loss 0.0007 (0.0060)	
training:	Epoch: [114][186/233]	Loss 0.0006 (0.0059)	
training:	Epoch: [114][187/233]	Loss 0.0007 (0.0059)	
training:	Epoch: [114][188/233]	Loss 0.0052 (0.0059)	
training:	Epoch: [114][189/233]	Loss 0.0008 (0.0059)	
training:	Epoch: [114][190/233]	Loss 0.0007 (0.0059)	
training:	Epoch: [114][191/233]	Loss 0.0011 (0.0058)	
training:	Epoch: [114][192/233]	Loss 0.0009 (0.0058)	
training:	Epoch: [114][193/233]	Loss 0.0007 (0.0058)	
training:	Epoch: [114][194/233]	Loss 0.0005 (0.0058)	
training:	Epoch: [114][195/233]	Loss 0.0006 (0.0057)	
training:	Epoch: [114][196/233]	Loss 0.0006 (0.0057)	
training:	Epoch: [114][197/233]	Loss 0.0006 (0.0057)	
training:	Epoch: [114][198/233]	Loss 0.0008 (0.0056)	
training:	Epoch: [114][199/233]	Loss 0.0008 (0.0056)	
training:	Epoch: [114][200/233]	Loss 0.0020 (0.0056)	
training:	Epoch: [114][201/233]	Loss 0.0010 (0.0056)	
training:	Epoch: [114][202/233]	Loss 0.0007 (0.0056)	
training:	Epoch: [114][203/233]	Loss 0.0049 (0.0056)	
training:	Epoch: [114][204/233]	Loss 0.0008 (0.0055)	
training:	Epoch: [114][205/233]	Loss 0.1423 (0.0062)	
training:	Epoch: [114][206/233]	Loss 0.0006 (0.0062)	
training:	Epoch: [114][207/233]	Loss 0.0008 (0.0061)	
training:	Epoch: [114][208/233]	Loss 0.0008 (0.0061)	
training:	Epoch: [114][209/233]	Loss 0.2460 (0.0073)	
training:	Epoch: [114][210/233]	Loss 0.0007 (0.0072)	
training:	Epoch: [114][211/233]	Loss 0.0007 (0.0072)	
training:	Epoch: [114][212/233]	Loss 0.0007 (0.0072)	
training:	Epoch: [114][213/233]	Loss 0.0006 (0.0071)	
training:	Epoch: [114][214/233]	Loss 0.0007 (0.0071)	
training:	Epoch: [114][215/233]	Loss 0.0006 (0.0071)	
training:	Epoch: [114][216/233]	Loss 0.0007 (0.0071)	
training:	Epoch: [114][217/233]	Loss 0.0006 (0.0070)	
training:	Epoch: [114][218/233]	Loss 0.0015 (0.0070)	
training:	Epoch: [114][219/233]	Loss 0.0041 (0.0070)	
training:	Epoch: [114][220/233]	Loss 0.0008 (0.0070)	
training:	Epoch: [114][221/233]	Loss 0.0006 (0.0069)	
training:	Epoch: [114][222/233]	Loss 0.0029 (0.0069)	
training:	Epoch: [114][223/233]	Loss 0.0008 (0.0069)	
training:	Epoch: [114][224/233]	Loss 0.1214 (0.0074)	
training:	Epoch: [114][225/233]	Loss 0.0011 (0.0074)	
training:	Epoch: [114][226/233]	Loss 0.0011 (0.0073)	
training:	Epoch: [114][227/233]	Loss 0.0022 (0.0073)	
training:	Epoch: [114][228/233]	Loss 0.0007 (0.0073)	
training:	Epoch: [114][229/233]	Loss 0.2292 (0.0083)	
training:	Epoch: [114][230/233]	Loss 0.0006 (0.0082)	
training:	Epoch: [114][231/233]	Loss 0.0087 (0.0082)	
training:	Epoch: [114][232/233]	Loss 0.0075 (0.0082)	
training:	Epoch: [114][233/233]	Loss 0.0007 (0.0082)	
Training:	 Loss: 0.0082

Training:	 ACC: 0.9997 0.9997 0.9997 0.9997
Validation:	 ACC: 0.7919 0.7913 0.7794 0.8045
Validation:	 Best_BACC: 0.8162 0.8154 0.7998 0.8326
Validation:	 Loss: 1.1570
Pretraining:	Epoch 115/200
----------
training:	Epoch: [115][1/233]	Loss 0.0008 (0.0008)	
training:	Epoch: [115][2/233]	Loss 0.0006 (0.0007)	
training:	Epoch: [115][3/233]	Loss 0.0007 (0.0007)	
training:	Epoch: [115][4/233]	Loss 0.0008 (0.0007)	
training:	Epoch: [115][5/233]	Loss 0.0015 (0.0009)	
training:	Epoch: [115][6/233]	Loss 0.0006 (0.0008)	
training:	Epoch: [115][7/233]	Loss 0.0006 (0.0008)	
training:	Epoch: [115][8/233]	Loss 0.0007 (0.0008)	
training:	Epoch: [115][9/233]	Loss 0.0009 (0.0008)	
training:	Epoch: [115][10/233]	Loss 0.0006 (0.0008)	
training:	Epoch: [115][11/233]	Loss 0.0007 (0.0008)	
training:	Epoch: [115][12/233]	Loss 0.0011 (0.0008)	
training:	Epoch: [115][13/233]	Loss 0.0006 (0.0008)	
training:	Epoch: [115][14/233]	Loss 0.0008 (0.0008)	
training:	Epoch: [115][15/233]	Loss 0.0008 (0.0008)	
training:	Epoch: [115][16/233]	Loss 0.0008 (0.0008)	
training:	Epoch: [115][17/233]	Loss 0.0007 (0.0008)	
training:	Epoch: [115][18/233]	Loss 0.0068 (0.0011)	
training:	Epoch: [115][19/233]	Loss 0.0019 (0.0012)	
training:	Epoch: [115][20/233]	Loss 0.0008 (0.0011)	
training:	Epoch: [115][21/233]	Loss 0.0006 (0.0011)	
training:	Epoch: [115][22/233]	Loss 0.0006 (0.0011)	
training:	Epoch: [115][23/233]	Loss 0.0008 (0.0011)	
training:	Epoch: [115][24/233]	Loss 0.0006 (0.0011)	
training:	Epoch: [115][25/233]	Loss 0.0006 (0.0010)	
training:	Epoch: [115][26/233]	Loss 0.0007 (0.0010)	
training:	Epoch: [115][27/233]	Loss 0.0007 (0.0010)	
training:	Epoch: [115][28/233]	Loss 0.0006 (0.0010)	
training:	Epoch: [115][29/233]	Loss 0.0006 (0.0010)	
training:	Epoch: [115][30/233]	Loss 0.0007 (0.0010)	
training:	Epoch: [115][31/233]	Loss 0.0013 (0.0010)	
training:	Epoch: [115][32/233]	Loss 0.0006 (0.0010)	
training:	Epoch: [115][33/233]	Loss 0.0008 (0.0010)	
training:	Epoch: [115][34/233]	Loss 0.0016 (0.0010)	
training:	Epoch: [115][35/233]	Loss 0.0006 (0.0010)	
training:	Epoch: [115][36/233]	Loss 0.0006 (0.0010)	
training:	Epoch: [115][37/233]	Loss 0.0007 (0.0010)	
training:	Epoch: [115][38/233]	Loss 0.0010 (0.0010)	
training:	Epoch: [115][39/233]	Loss 0.0006 (0.0010)	
training:	Epoch: [115][40/233]	Loss 0.0011 (0.0010)	
training:	Epoch: [115][41/233]	Loss 0.0005 (0.0009)	
training:	Epoch: [115][42/233]	Loss 0.0010 (0.0009)	
training:	Epoch: [115][43/233]	Loss 0.0007 (0.0009)	
training:	Epoch: [115][44/233]	Loss 0.0007 (0.0009)	
training:	Epoch: [115][45/233]	Loss 0.0005 (0.0009)	
training:	Epoch: [115][46/233]	Loss 0.0006 (0.0009)	
training:	Epoch: [115][47/233]	Loss 0.0007 (0.0009)	
training:	Epoch: [115][48/233]	Loss 0.0012 (0.0009)	
training:	Epoch: [115][49/233]	Loss 0.0007 (0.0009)	
training:	Epoch: [115][50/233]	Loss 0.0008 (0.0009)	
training:	Epoch: [115][51/233]	Loss 0.0007 (0.0009)	
training:	Epoch: [115][52/233]	Loss 0.0014 (0.0009)	
training:	Epoch: [115][53/233]	Loss 0.0012 (0.0009)	
training:	Epoch: [115][54/233]	Loss 0.0045 (0.0010)	
training:	Epoch: [115][55/233]	Loss 0.0008 (0.0010)	
training:	Epoch: [115][56/233]	Loss 0.0011 (0.0010)	
training:	Epoch: [115][57/233]	Loss 0.0011 (0.0010)	
training:	Epoch: [115][58/233]	Loss 0.0019 (0.0010)	
training:	Epoch: [115][59/233]	Loss 0.0006 (0.0010)	
training:	Epoch: [115][60/233]	Loss 0.0014 (0.0010)	
training:	Epoch: [115][61/233]	Loss 0.0033 (0.0010)	
training:	Epoch: [115][62/233]	Loss 0.0008 (0.0010)	
training:	Epoch: [115][63/233]	Loss 0.0009 (0.0010)	
training:	Epoch: [115][64/233]	Loss 0.0006 (0.0010)	
training:	Epoch: [115][65/233]	Loss 0.0007 (0.0010)	
training:	Epoch: [115][66/233]	Loss 0.0023 (0.0010)	
training:	Epoch: [115][67/233]	Loss 0.0006 (0.0010)	
training:	Epoch: [115][68/233]	Loss 0.0005 (0.0010)	
training:	Epoch: [115][69/233]	Loss 0.0006 (0.0010)	
training:	Epoch: [115][70/233]	Loss 0.0014 (0.0010)	
training:	Epoch: [115][71/233]	Loss 0.0021 (0.0010)	
training:	Epoch: [115][72/233]	Loss 0.0018 (0.0011)	
training:	Epoch: [115][73/233]	Loss 0.0006 (0.0011)	
training:	Epoch: [115][74/233]	Loss 0.0007 (0.0010)	
training:	Epoch: [115][75/233]	Loss 0.0007 (0.0010)	
training:	Epoch: [115][76/233]	Loss 0.0007 (0.0010)	
training:	Epoch: [115][77/233]	Loss 0.0028 (0.0011)	
training:	Epoch: [115][78/233]	Loss 0.0006 (0.0011)	
training:	Epoch: [115][79/233]	Loss 0.0221 (0.0013)	
training:	Epoch: [115][80/233]	Loss 0.0007 (0.0013)	
training:	Epoch: [115][81/233]	Loss 0.0011 (0.0013)	
training:	Epoch: [115][82/233]	Loss 0.0006 (0.0013)	
training:	Epoch: [115][83/233]	Loss 0.0006 (0.0013)	
training:	Epoch: [115][84/233]	Loss 0.0005 (0.0013)	
training:	Epoch: [115][85/233]	Loss 0.0012 (0.0013)	
training:	Epoch: [115][86/233]	Loss 0.0263 (0.0016)	
training:	Epoch: [115][87/233]	Loss 0.0020 (0.0016)	
training:	Epoch: [115][88/233]	Loss 0.0006 (0.0016)	
training:	Epoch: [115][89/233]	Loss 0.0006 (0.0016)	
training:	Epoch: [115][90/233]	Loss 0.0253 (0.0018)	
training:	Epoch: [115][91/233]	Loss 0.0017 (0.0018)	
training:	Epoch: [115][92/233]	Loss 0.0007 (0.0018)	
training:	Epoch: [115][93/233]	Loss 0.0013 (0.0018)	
training:	Epoch: [115][94/233]	Loss 0.0007 (0.0018)	
training:	Epoch: [115][95/233]	Loss 0.0006 (0.0018)	
training:	Epoch: [115][96/233]	Loss 0.0006 (0.0018)	
training:	Epoch: [115][97/233]	Loss 0.0021 (0.0018)	
training:	Epoch: [115][98/233]	Loss 0.0085 (0.0018)	
training:	Epoch: [115][99/233]	Loss 0.0006 (0.0018)	
training:	Epoch: [115][100/233]	Loss 0.0016 (0.0018)	
training:	Epoch: [115][101/233]	Loss 0.0007 (0.0018)	
training:	Epoch: [115][102/233]	Loss 0.0014 (0.0018)	
training:	Epoch: [115][103/233]	Loss 0.0007 (0.0018)	
training:	Epoch: [115][104/233]	Loss 0.0136 (0.0019)	
training:	Epoch: [115][105/233]	Loss 0.0013 (0.0019)	
training:	Epoch: [115][106/233]	Loss 0.0009 (0.0019)	
training:	Epoch: [115][107/233]	Loss 0.0008 (0.0019)	
training:	Epoch: [115][108/233]	Loss 0.0006 (0.0019)	
training:	Epoch: [115][109/233]	Loss 0.0030 (0.0019)	
training:	Epoch: [115][110/233]	Loss 0.0006 (0.0019)	
training:	Epoch: [115][111/233]	Loss 0.0035 (0.0019)	
training:	Epoch: [115][112/233]	Loss 0.0006 (0.0019)	
training:	Epoch: [115][113/233]	Loss 0.0009 (0.0019)	
training:	Epoch: [115][114/233]	Loss 0.0006 (0.0019)	
training:	Epoch: [115][115/233]	Loss 0.0006 (0.0018)	
training:	Epoch: [115][116/233]	Loss 0.0006 (0.0018)	
training:	Epoch: [115][117/233]	Loss 0.0009 (0.0018)	
training:	Epoch: [115][118/233]	Loss 0.0807 (0.0025)	
training:	Epoch: [115][119/233]	Loss 0.0007 (0.0025)	
training:	Epoch: [115][120/233]	Loss 0.0006 (0.0025)	
training:	Epoch: [115][121/233]	Loss 0.0005 (0.0024)	
training:	Epoch: [115][122/233]	Loss 0.0008 (0.0024)	
training:	Epoch: [115][123/233]	Loss 0.0007 (0.0024)	
training:	Epoch: [115][124/233]	Loss 0.0010 (0.0024)	
training:	Epoch: [115][125/233]	Loss 0.0007 (0.0024)	
training:	Epoch: [115][126/233]	Loss 0.0006 (0.0024)	
training:	Epoch: [115][127/233]	Loss 0.0008 (0.0024)	
training:	Epoch: [115][128/233]	Loss 0.0006 (0.0024)	
training:	Epoch: [115][129/233]	Loss 0.0007 (0.0023)	
training:	Epoch: [115][130/233]	Loss 0.0009 (0.0023)	
training:	Epoch: [115][131/233]	Loss 0.0095 (0.0024)	
training:	Epoch: [115][132/233]	Loss 0.0006 (0.0024)	
training:	Epoch: [115][133/233]	Loss 0.0006 (0.0024)	
training:	Epoch: [115][134/233]	Loss 0.0006 (0.0023)	
training:	Epoch: [115][135/233]	Loss 0.0173 (0.0025)	
training:	Epoch: [115][136/233]	Loss 0.0008 (0.0024)	
training:	Epoch: [115][137/233]	Loss 0.0014 (0.0024)	
training:	Epoch: [115][138/233]	Loss 0.0009 (0.0024)	
training:	Epoch: [115][139/233]	Loss 0.0007 (0.0024)	
training:	Epoch: [115][140/233]	Loss 0.0006 (0.0024)	
training:	Epoch: [115][141/233]	Loss 0.0007 (0.0024)	
training:	Epoch: [115][142/233]	Loss 0.1180 (0.0032)	
training:	Epoch: [115][143/233]	Loss 0.0007 (0.0032)	
training:	Epoch: [115][144/233]	Loss 0.0011 (0.0032)	
training:	Epoch: [115][145/233]	Loss 0.0007 (0.0032)	
training:	Epoch: [115][146/233]	Loss 0.0014 (0.0031)	
training:	Epoch: [115][147/233]	Loss 0.0007 (0.0031)	
training:	Epoch: [115][148/233]	Loss 0.0016 (0.0031)	
training:	Epoch: [115][149/233]	Loss 0.0006 (0.0031)	
training:	Epoch: [115][150/233]	Loss 0.0011 (0.0031)	
training:	Epoch: [115][151/233]	Loss 0.0008 (0.0031)	
training:	Epoch: [115][152/233]	Loss 0.2127 (0.0044)	
training:	Epoch: [115][153/233]	Loss 0.0007 (0.0044)	
training:	Epoch: [115][154/233]	Loss 0.0007 (0.0044)	
training:	Epoch: [115][155/233]	Loss 0.0009 (0.0044)	
training:	Epoch: [115][156/233]	Loss 0.0007 (0.0044)	
training:	Epoch: [115][157/233]	Loss 0.0007 (0.0043)	
training:	Epoch: [115][158/233]	Loss 0.0009 (0.0043)	
training:	Epoch: [115][159/233]	Loss 0.0008 (0.0043)	
training:	Epoch: [115][160/233]	Loss 0.0007 (0.0043)	
training:	Epoch: [115][161/233]	Loss 0.0006 (0.0042)	
training:	Epoch: [115][162/233]	Loss 0.0006 (0.0042)	
training:	Epoch: [115][163/233]	Loss 0.0007 (0.0042)	
training:	Epoch: [115][164/233]	Loss 0.0008 (0.0042)	
training:	Epoch: [115][165/233]	Loss 0.0007 (0.0042)	
training:	Epoch: [115][166/233]	Loss 0.0376 (0.0044)	
training:	Epoch: [115][167/233]	Loss 0.0006 (0.0043)	
training:	Epoch: [115][168/233]	Loss 0.0006 (0.0043)	
training:	Epoch: [115][169/233]	Loss 0.0008 (0.0043)	
training:	Epoch: [115][170/233]	Loss 0.0008 (0.0043)	
training:	Epoch: [115][171/233]	Loss 0.0006 (0.0042)	
training:	Epoch: [115][172/233]	Loss 0.0019 (0.0042)	
training:	Epoch: [115][173/233]	Loss 0.0006 (0.0042)	
training:	Epoch: [115][174/233]	Loss 0.0007 (0.0042)	
training:	Epoch: [115][175/233]	Loss 0.0006 (0.0042)	
training:	Epoch: [115][176/233]	Loss 0.0006 (0.0042)	
training:	Epoch: [115][177/233]	Loss 0.0156 (0.0042)	
training:	Epoch: [115][178/233]	Loss 0.0007 (0.0042)	
training:	Epoch: [115][179/233]	Loss 0.0008 (0.0042)	
training:	Epoch: [115][180/233]	Loss 0.0011 (0.0042)	
training:	Epoch: [115][181/233]	Loss 0.0010 (0.0041)	
training:	Epoch: [115][182/233]	Loss 0.0016 (0.0041)	
training:	Epoch: [115][183/233]	Loss 0.0006 (0.0041)	
training:	Epoch: [115][184/233]	Loss 0.0006 (0.0041)	
training:	Epoch: [115][185/233]	Loss 0.0015 (0.0041)	
training:	Epoch: [115][186/233]	Loss 0.0009 (0.0041)	
training:	Epoch: [115][187/233]	Loss 0.0007 (0.0040)	
training:	Epoch: [115][188/233]	Loss 0.0009 (0.0040)	
training:	Epoch: [115][189/233]	Loss 0.0007 (0.0040)	
training:	Epoch: [115][190/233]	Loss 0.0006 (0.0040)	
training:	Epoch: [115][191/233]	Loss 0.0054 (0.0040)	
training:	Epoch: [115][192/233]	Loss 0.0021 (0.0040)	
training:	Epoch: [115][193/233]	Loss 0.1182 (0.0046)	
training:	Epoch: [115][194/233]	Loss 0.0005 (0.0046)	
training:	Epoch: [115][195/233]	Loss 0.0007 (0.0045)	
training:	Epoch: [115][196/233]	Loss 0.0022 (0.0045)	
training:	Epoch: [115][197/233]	Loss 0.0010 (0.0045)	
training:	Epoch: [115][198/233]	Loss 0.0183 (0.0046)	
training:	Epoch: [115][199/233]	Loss 0.0012 (0.0046)	
training:	Epoch: [115][200/233]	Loss 0.0016 (0.0045)	
training:	Epoch: [115][201/233]	Loss 0.0009 (0.0045)	
training:	Epoch: [115][202/233]	Loss 0.0046 (0.0045)	
training:	Epoch: [115][203/233]	Loss 0.0011 (0.0045)	
training:	Epoch: [115][204/233]	Loss 0.0012 (0.0045)	
training:	Epoch: [115][205/233]	Loss 0.0008 (0.0045)	
training:	Epoch: [115][206/233]	Loss 0.0007 (0.0045)	
training:	Epoch: [115][207/233]	Loss 0.0019 (0.0044)	
training:	Epoch: [115][208/233]	Loss 0.0009 (0.0044)	
training:	Epoch: [115][209/233]	Loss 0.0006 (0.0044)	
training:	Epoch: [115][210/233]	Loss 0.0006 (0.0044)	
training:	Epoch: [115][211/233]	Loss 0.0007 (0.0044)	
training:	Epoch: [115][212/233]	Loss 0.0042 (0.0044)	
training:	Epoch: [115][213/233]	Loss 0.0012 (0.0044)	
training:	Epoch: [115][214/233]	Loss 0.0007 (0.0043)	
training:	Epoch: [115][215/233]	Loss 0.0039 (0.0043)	
training:	Epoch: [115][216/233]	Loss 0.0006 (0.0043)	
training:	Epoch: [115][217/233]	Loss 0.0006 (0.0043)	
training:	Epoch: [115][218/233]	Loss 0.0006 (0.0043)	
training:	Epoch: [115][219/233]	Loss 0.0006 (0.0043)	
training:	Epoch: [115][220/233]	Loss 0.0006 (0.0043)	
training:	Epoch: [115][221/233]	Loss 0.0007 (0.0042)	
training:	Epoch: [115][222/233]	Loss 0.2383 (0.0053)	
training:	Epoch: [115][223/233]	Loss 0.0007 (0.0053)	
training:	Epoch: [115][224/233]	Loss 0.0009 (0.0053)	
training:	Epoch: [115][225/233]	Loss 0.0006 (0.0052)	
training:	Epoch: [115][226/233]	Loss 0.0006 (0.0052)	
training:	Epoch: [115][227/233]	Loss 0.0006 (0.0052)	
training:	Epoch: [115][228/233]	Loss 0.0011 (0.0052)	
training:	Epoch: [115][229/233]	Loss 0.0080 (0.0052)	
training:	Epoch: [115][230/233]	Loss 0.0007 (0.0052)	
training:	Epoch: [115][231/233]	Loss 0.0157 (0.0052)	
training:	Epoch: [115][232/233]	Loss 0.0006 (0.0052)	
training:	Epoch: [115][233/233]	Loss 0.0012 (0.0052)	
Training:	 Loss: 0.0052

Training:	 ACC: 0.9997 0.9997 0.9997 0.9997
Validation:	 ACC: 0.7941 0.7940 0.7916 0.7966
Validation:	 Best_BACC: 0.8162 0.8154 0.7998 0.8326
Validation:	 Loss: 1.1191
Pretraining:	Epoch 116/200
----------
training:	Epoch: [116][1/233]	Loss 0.0007 (0.0007)	
training:	Epoch: [116][2/233]	Loss 0.0007 (0.0007)	
training:	Epoch: [116][3/233]	Loss 0.0007 (0.0007)	
training:	Epoch: [116][4/233]	Loss 0.0010 (0.0008)	
training:	Epoch: [116][5/233]	Loss 0.0007 (0.0008)	
training:	Epoch: [116][6/233]	Loss 0.0006 (0.0007)	
training:	Epoch: [116][7/233]	Loss 0.0012 (0.0008)	
training:	Epoch: [116][8/233]	Loss 0.0082 (0.0017)	
training:	Epoch: [116][9/233]	Loss 0.0006 (0.0016)	
training:	Epoch: [116][10/233]	Loss 0.0993 (0.0114)	
training:	Epoch: [116][11/233]	Loss 0.0006 (0.0104)	
training:	Epoch: [116][12/233]	Loss 0.0010 (0.0096)	
training:	Epoch: [116][13/233]	Loss 0.0007 (0.0089)	
training:	Epoch: [116][14/233]	Loss 0.0006 (0.0083)	
training:	Epoch: [116][15/233]	Loss 0.0009 (0.0078)	
training:	Epoch: [116][16/233]	Loss 0.0011 (0.0074)	
training:	Epoch: [116][17/233]	Loss 0.0006 (0.0070)	
training:	Epoch: [116][18/233]	Loss 0.2373 (0.0198)	
training:	Epoch: [116][19/233]	Loss 0.0006 (0.0188)	
training:	Epoch: [116][20/233]	Loss 0.0008 (0.0179)	
training:	Epoch: [116][21/233]	Loss 0.0008 (0.0171)	
training:	Epoch: [116][22/233]	Loss 0.0010 (0.0163)	
training:	Epoch: [116][23/233]	Loss 0.0010 (0.0157)	
training:	Epoch: [116][24/233]	Loss 0.0009 (0.0151)	
training:	Epoch: [116][25/233]	Loss 0.0007 (0.0145)	
training:	Epoch: [116][26/233]	Loss 0.0008 (0.0140)	
training:	Epoch: [116][27/233]	Loss 0.0009 (0.0135)	
training:	Epoch: [116][28/233]	Loss 0.0008 (0.0130)	
training:	Epoch: [116][29/233]	Loss 0.0010 (0.0126)	
training:	Epoch: [116][30/233]	Loss 0.0007 (0.0122)	
training:	Epoch: [116][31/233]	Loss 0.0006 (0.0118)	
training:	Epoch: [116][32/233]	Loss 0.0008 (0.0115)	
training:	Epoch: [116][33/233]	Loss 0.0005 (0.0112)	
training:	Epoch: [116][34/233]	Loss 0.0007 (0.0109)	
training:	Epoch: [116][35/233]	Loss 0.0007 (0.0106)	
training:	Epoch: [116][36/233]	Loss 0.0011 (0.0103)	
training:	Epoch: [116][37/233]	Loss 0.0007 (0.0100)	
training:	Epoch: [116][38/233]	Loss 0.0006 (0.0098)	
training:	Epoch: [116][39/233]	Loss 0.1980 (0.0146)	
training:	Epoch: [116][40/233]	Loss 0.0008 (0.0143)	
training:	Epoch: [116][41/233]	Loss 0.0007 (0.0139)	
training:	Epoch: [116][42/233]	Loss 0.0007 (0.0136)	
training:	Epoch: [116][43/233]	Loss 0.0055 (0.0134)	
training:	Epoch: [116][44/233]	Loss 0.0011 (0.0132)	
training:	Epoch: [116][45/233]	Loss 0.0010 (0.0129)	
training:	Epoch: [116][46/233]	Loss 0.0010 (0.0126)	
training:	Epoch: [116][47/233]	Loss 0.0006 (0.0124)	
training:	Epoch: [116][48/233]	Loss 0.0006 (0.0121)	
training:	Epoch: [116][49/233]	Loss 0.0007 (0.0119)	
training:	Epoch: [116][50/233]	Loss 0.0007 (0.0117)	
training:	Epoch: [116][51/233]	Loss 0.0006 (0.0115)	
training:	Epoch: [116][52/233]	Loss 0.0006 (0.0112)	
training:	Epoch: [116][53/233]	Loss 0.0014 (0.0111)	
training:	Epoch: [116][54/233]	Loss 0.0007 (0.0109)	
training:	Epoch: [116][55/233]	Loss 0.0008 (0.0107)	
training:	Epoch: [116][56/233]	Loss 0.0006 (0.0105)	
training:	Epoch: [116][57/233]	Loss 0.0052 (0.0104)	
training:	Epoch: [116][58/233]	Loss 0.0007 (0.0102)	
training:	Epoch: [116][59/233]	Loss 0.0007 (0.0101)	
training:	Epoch: [116][60/233]	Loss 0.0014 (0.0099)	
training:	Epoch: [116][61/233]	Loss 0.0008 (0.0098)	
training:	Epoch: [116][62/233]	Loss 0.0006 (0.0096)	
training:	Epoch: [116][63/233]	Loss 0.0060 (0.0096)	
training:	Epoch: [116][64/233]	Loss 0.0006 (0.0094)	
training:	Epoch: [116][65/233]	Loss 0.0166 (0.0096)	
training:	Epoch: [116][66/233]	Loss 0.0006 (0.0094)	
training:	Epoch: [116][67/233]	Loss 0.0006 (0.0093)	
training:	Epoch: [116][68/233]	Loss 0.0009 (0.0092)	
training:	Epoch: [116][69/233]	Loss 0.0012 (0.0090)	
training:	Epoch: [116][70/233]	Loss 0.0008 (0.0089)	
training:	Epoch: [116][71/233]	Loss 0.0049 (0.0089)	
training:	Epoch: [116][72/233]	Loss 0.0009 (0.0088)	
training:	Epoch: [116][73/233]	Loss 0.0006 (0.0086)	
training:	Epoch: [116][74/233]	Loss 0.0008 (0.0085)	
training:	Epoch: [116][75/233]	Loss 0.0006 (0.0084)	
training:	Epoch: [116][76/233]	Loss 0.0007 (0.0083)	
training:	Epoch: [116][77/233]	Loss 0.0010 (0.0082)	
training:	Epoch: [116][78/233]	Loss 0.0006 (0.0081)	
training:	Epoch: [116][79/233]	Loss 0.0006 (0.0080)	
training:	Epoch: [116][80/233]	Loss 0.0142 (0.0081)	
training:	Epoch: [116][81/233]	Loss 0.0008 (0.0080)	
training:	Epoch: [116][82/233]	Loss 0.0008 (0.0079)	
training:	Epoch: [116][83/233]	Loss 0.0012 (0.0079)	
training:	Epoch: [116][84/233]	Loss 0.0006 (0.0078)	
training:	Epoch: [116][85/233]	Loss 0.0007 (0.0077)	
training:	Epoch: [116][86/233]	Loss 0.0007 (0.0076)	
training:	Epoch: [116][87/233]	Loss 0.0013 (0.0075)	
training:	Epoch: [116][88/233]	Loss 0.0006 (0.0075)	
training:	Epoch: [116][89/233]	Loss 0.0006 (0.0074)	
training:	Epoch: [116][90/233]	Loss 0.0008 (0.0073)	
training:	Epoch: [116][91/233]	Loss 0.0008 (0.0072)	
training:	Epoch: [116][92/233]	Loss 0.0006 (0.0072)	
training:	Epoch: [116][93/233]	Loss 0.0009 (0.0071)	
training:	Epoch: [116][94/233]	Loss 0.0008 (0.0070)	
training:	Epoch: [116][95/233]	Loss 0.0006 (0.0070)	
training:	Epoch: [116][96/233]	Loss 0.0010 (0.0069)	
training:	Epoch: [116][97/233]	Loss 0.0008 (0.0068)	
training:	Epoch: [116][98/233]	Loss 0.0009 (0.0068)	
training:	Epoch: [116][99/233]	Loss 0.0006 (0.0067)	
training:	Epoch: [116][100/233]	Loss 0.0008 (0.0067)	
training:	Epoch: [116][101/233]	Loss 0.0013 (0.0066)	
training:	Epoch: [116][102/233]	Loss 0.0010 (0.0066)	
training:	Epoch: [116][103/233]	Loss 0.0006 (0.0065)	
training:	Epoch: [116][104/233]	Loss 0.0012 (0.0064)	
training:	Epoch: [116][105/233]	Loss 0.0015 (0.0064)	
training:	Epoch: [116][106/233]	Loss 0.0021 (0.0064)	
training:	Epoch: [116][107/233]	Loss 0.0006 (0.0063)	
training:	Epoch: [116][108/233]	Loss 0.0007 (0.0062)	
training:	Epoch: [116][109/233]	Loss 0.0008 (0.0062)	
training:	Epoch: [116][110/233]	Loss 0.0007 (0.0061)	
training:	Epoch: [116][111/233]	Loss 0.0007 (0.0061)	
training:	Epoch: [116][112/233]	Loss 0.0021 (0.0061)	
training:	Epoch: [116][113/233]	Loss 0.0007 (0.0060)	
training:	Epoch: [116][114/233]	Loss 0.0009 (0.0060)	
training:	Epoch: [116][115/233]	Loss 0.0008 (0.0059)	
training:	Epoch: [116][116/233]	Loss 0.0008 (0.0059)	
training:	Epoch: [116][117/233]	Loss 0.0006 (0.0058)	
training:	Epoch: [116][118/233]	Loss 0.0621 (0.0063)	
training:	Epoch: [116][119/233]	Loss 0.0006 (0.0063)	
training:	Epoch: [116][120/233]	Loss 0.0007 (0.0062)	
training:	Epoch: [116][121/233]	Loss 0.0008 (0.0062)	
training:	Epoch: [116][122/233]	Loss 0.0022 (0.0061)	
training:	Epoch: [116][123/233]	Loss 0.0008 (0.0061)	
training:	Epoch: [116][124/233]	Loss 0.0010 (0.0061)	
training:	Epoch: [116][125/233]	Loss 0.0006 (0.0060)	
training:	Epoch: [116][126/233]	Loss 0.0030 (0.0060)	
training:	Epoch: [116][127/233]	Loss 0.0010 (0.0060)	
training:	Epoch: [116][128/233]	Loss 0.0006 (0.0059)	
training:	Epoch: [116][129/233]	Loss 0.0007 (0.0059)	
training:	Epoch: [116][130/233]	Loss 0.0207 (0.0060)	
training:	Epoch: [116][131/233]	Loss 0.0006 (0.0059)	
training:	Epoch: [116][132/233]	Loss 0.0007 (0.0059)	
training:	Epoch: [116][133/233]	Loss 0.0013 (0.0059)	
training:	Epoch: [116][134/233]	Loss 0.0015 (0.0058)	
training:	Epoch: [116][135/233]	Loss 0.0186 (0.0059)	
training:	Epoch: [116][136/233]	Loss 0.0025 (0.0059)	
training:	Epoch: [116][137/233]	Loss 0.0007 (0.0059)	
training:	Epoch: [116][138/233]	Loss 0.0126 (0.0059)	
training:	Epoch: [116][139/233]	Loss 0.0007 (0.0059)	
training:	Epoch: [116][140/233]	Loss 0.0023 (0.0059)	
training:	Epoch: [116][141/233]	Loss 0.0031 (0.0058)	
training:	Epoch: [116][142/233]	Loss 0.0010 (0.0058)	
training:	Epoch: [116][143/233]	Loss 0.0009 (0.0058)	
training:	Epoch: [116][144/233]	Loss 0.0010 (0.0057)	
training:	Epoch: [116][145/233]	Loss 0.0006 (0.0057)	
training:	Epoch: [116][146/233]	Loss 0.0014 (0.0057)	
training:	Epoch: [116][147/233]	Loss 0.0006 (0.0056)	
training:	Epoch: [116][148/233]	Loss 0.0007 (0.0056)	
training:	Epoch: [116][149/233]	Loss 0.0007 (0.0056)	
training:	Epoch: [116][150/233]	Loss 0.0121 (0.0056)	
training:	Epoch: [116][151/233]	Loss 0.0005 (0.0056)	
training:	Epoch: [116][152/233]	Loss 0.0008 (0.0055)	
training:	Epoch: [116][153/233]	Loss 0.0007 (0.0055)	
training:	Epoch: [116][154/233]	Loss 0.0023 (0.0055)	
training:	Epoch: [116][155/233]	Loss 0.0006 (0.0055)	
training:	Epoch: [116][156/233]	Loss 0.0017 (0.0054)	
training:	Epoch: [116][157/233]	Loss 0.0011 (0.0054)	
training:	Epoch: [116][158/233]	Loss 0.0007 (0.0054)	
training:	Epoch: [116][159/233]	Loss 0.0009 (0.0053)	
training:	Epoch: [116][160/233]	Loss 0.0006 (0.0053)	
training:	Epoch: [116][161/233]	Loss 0.0006 (0.0053)	
training:	Epoch: [116][162/233]	Loss 0.0005 (0.0053)	
training:	Epoch: [116][163/233]	Loss 0.0019 (0.0052)	
training:	Epoch: [116][164/233]	Loss 0.0006 (0.0052)	
training:	Epoch: [116][165/233]	Loss 0.0006 (0.0052)	
training:	Epoch: [116][166/233]	Loss 0.0006 (0.0052)	
training:	Epoch: [116][167/233]	Loss 0.0006 (0.0051)	
training:	Epoch: [116][168/233]	Loss 0.0006 (0.0051)	
training:	Epoch: [116][169/233]	Loss 0.0010 (0.0051)	
training:	Epoch: [116][170/233]	Loss 0.0007 (0.0051)	
training:	Epoch: [116][171/233]	Loss 0.0006 (0.0050)	
training:	Epoch: [116][172/233]	Loss 0.0028 (0.0050)	
training:	Epoch: [116][173/233]	Loss 0.0006 (0.0050)	
training:	Epoch: [116][174/233]	Loss 0.0005 (0.0050)	
training:	Epoch: [116][175/233]	Loss 0.0033 (0.0050)	
training:	Epoch: [116][176/233]	Loss 0.0007 (0.0049)	
training:	Epoch: [116][177/233]	Loss 0.0008 (0.0049)	
training:	Epoch: [116][178/233]	Loss 0.0006 (0.0049)	
training:	Epoch: [116][179/233]	Loss 0.0011 (0.0049)	
training:	Epoch: [116][180/233]	Loss 0.0007 (0.0048)	
training:	Epoch: [116][181/233]	Loss 0.0008 (0.0048)	
training:	Epoch: [116][182/233]	Loss 0.0006 (0.0048)	
training:	Epoch: [116][183/233]	Loss 0.0007 (0.0048)	
training:	Epoch: [116][184/233]	Loss 0.0006 (0.0047)	
training:	Epoch: [116][185/233]	Loss 0.0035 (0.0047)	
training:	Epoch: [116][186/233]	Loss 0.0008 (0.0047)	
training:	Epoch: [116][187/233]	Loss 0.0009 (0.0047)	
training:	Epoch: [116][188/233]	Loss 0.0006 (0.0047)	
training:	Epoch: [116][189/233]	Loss 0.0006 (0.0047)	
training:	Epoch: [116][190/233]	Loss 0.0007 (0.0046)	
training:	Epoch: [116][191/233]	Loss 0.0007 (0.0046)	
training:	Epoch: [116][192/233]	Loss 0.0006 (0.0046)	
training:	Epoch: [116][193/233]	Loss 0.0008 (0.0046)	
training:	Epoch: [116][194/233]	Loss 0.0009 (0.0046)	
training:	Epoch: [116][195/233]	Loss 0.0007 (0.0045)	
training:	Epoch: [116][196/233]	Loss 0.0006 (0.0045)	
training:	Epoch: [116][197/233]	Loss 0.0006 (0.0045)	
training:	Epoch: [116][198/233]	Loss 0.0007 (0.0045)	
training:	Epoch: [116][199/233]	Loss 0.0006 (0.0045)	
training:	Epoch: [116][200/233]	Loss 0.0007 (0.0044)	
training:	Epoch: [116][201/233]	Loss 0.0008 (0.0044)	
training:	Epoch: [116][202/233]	Loss 0.0006 (0.0044)	
training:	Epoch: [116][203/233]	Loss 0.0062 (0.0044)	
training:	Epoch: [116][204/233]	Loss 0.0009 (0.0044)	
training:	Epoch: [116][205/233]	Loss 0.0007 (0.0044)	
training:	Epoch: [116][206/233]	Loss 0.0007 (0.0044)	
training:	Epoch: [116][207/233]	Loss 0.0006 (0.0043)	
training:	Epoch: [116][208/233]	Loss 0.0006 (0.0043)	
training:	Epoch: [116][209/233]	Loss 0.0016 (0.0043)	
training:	Epoch: [116][210/233]	Loss 0.0006 (0.0043)	
training:	Epoch: [116][211/233]	Loss 0.0075 (0.0043)	
training:	Epoch: [116][212/233]	Loss 0.0008 (0.0043)	
training:	Epoch: [116][213/233]	Loss 0.0009 (0.0043)	
training:	Epoch: [116][214/233]	Loss 0.0006 (0.0043)	
training:	Epoch: [116][215/233]	Loss 0.0006 (0.0042)	
training:	Epoch: [116][216/233]	Loss 0.0012 (0.0042)	
training:	Epoch: [116][217/233]	Loss 0.0013 (0.0042)	
training:	Epoch: [116][218/233]	Loss 0.0005 (0.0042)	
training:	Epoch: [116][219/233]	Loss 0.0006 (0.0042)	
training:	Epoch: [116][220/233]	Loss 0.0009 (0.0042)	
training:	Epoch: [116][221/233]	Loss 0.0372 (0.0043)	
training:	Epoch: [116][222/233]	Loss 0.0007 (0.0043)	
training:	Epoch: [116][223/233]	Loss 0.0006 (0.0043)	
training:	Epoch: [116][224/233]	Loss 0.0009 (0.0043)	
training:	Epoch: [116][225/233]	Loss 0.0007 (0.0042)	
training:	Epoch: [116][226/233]	Loss 0.0010 (0.0042)	
training:	Epoch: [116][227/233]	Loss 0.0007 (0.0042)	
training:	Epoch: [116][228/233]	Loss 0.0006 (0.0042)	
training:	Epoch: [116][229/233]	Loss 0.0006 (0.0042)	
training:	Epoch: [116][230/233]	Loss 0.0007 (0.0042)	
training:	Epoch: [116][231/233]	Loss 0.0006 (0.0042)	
training:	Epoch: [116][232/233]	Loss 0.0012 (0.0041)	
training:	Epoch: [116][233/233]	Loss 0.0550 (0.0044)	
Training:	 Loss: 0.0043

Training:	 ACC: 0.9997 0.9997 0.9997 0.9997
Validation:	 ACC: 0.7953 0.7961 0.8141 0.7764
Validation:	 Best_BACC: 0.8162 0.8154 0.7998 0.8326
Validation:	 Loss: 1.1511
Pretraining:	Epoch 117/200
----------
training:	Epoch: [117][1/233]	Loss 0.0006 (0.0006)	
training:	Epoch: [117][2/233]	Loss 0.0007 (0.0006)	
training:	Epoch: [117][3/233]	Loss 0.0005 (0.0006)	
training:	Epoch: [117][4/233]	Loss 0.0006 (0.0006)	
training:	Epoch: [117][5/233]	Loss 0.0017 (0.0008)	
training:	Epoch: [117][6/233]	Loss 0.0036 (0.0013)	
training:	Epoch: [117][7/233]	Loss 0.0011 (0.0013)	
training:	Epoch: [117][8/233]	Loss 0.0009 (0.0012)	
training:	Epoch: [117][9/233]	Loss 0.0006 (0.0012)	
training:	Epoch: [117][10/233]	Loss 0.0009 (0.0011)	
training:	Epoch: [117][11/233]	Loss 0.0006 (0.0011)	
training:	Epoch: [117][12/233]	Loss 0.0013 (0.0011)	
training:	Epoch: [117][13/233]	Loss 0.0009 (0.0011)	
training:	Epoch: [117][14/233]	Loss 0.0007 (0.0011)	
training:	Epoch: [117][15/233]	Loss 0.0006 (0.0010)	
training:	Epoch: [117][16/233]	Loss 0.0006 (0.0010)	
training:	Epoch: [117][17/233]	Loss 0.0008 (0.0010)	
training:	Epoch: [117][18/233]	Loss 0.0033 (0.0011)	
training:	Epoch: [117][19/233]	Loss 0.0006 (0.0011)	
training:	Epoch: [117][20/233]	Loss 0.0007 (0.0011)	
training:	Epoch: [117][21/233]	Loss 0.0007 (0.0011)	
training:	Epoch: [117][22/233]	Loss 0.0005 (0.0010)	
training:	Epoch: [117][23/233]	Loss 0.0006 (0.0010)	
training:	Epoch: [117][24/233]	Loss 0.0006 (0.0010)	
training:	Epoch: [117][25/233]	Loss 0.0009 (0.0010)	
training:	Epoch: [117][26/233]	Loss 0.0015 (0.0010)	
training:	Epoch: [117][27/233]	Loss 0.0006 (0.0010)	
training:	Epoch: [117][28/233]	Loss 0.0006 (0.0010)	
training:	Epoch: [117][29/233]	Loss 0.0007 (0.0010)	
training:	Epoch: [117][30/233]	Loss 0.0008 (0.0010)	
training:	Epoch: [117][31/233]	Loss 0.0011 (0.0010)	
training:	Epoch: [117][32/233]	Loss 0.0006 (0.0010)	
training:	Epoch: [117][33/233]	Loss 0.0014 (0.0010)	
training:	Epoch: [117][34/233]	Loss 0.0009 (0.0010)	
training:	Epoch: [117][35/233]	Loss 0.0006 (0.0010)	
training:	Epoch: [117][36/233]	Loss 0.2372 (0.0075)	
training:	Epoch: [117][37/233]	Loss 0.0051 (0.0075)	
training:	Epoch: [117][38/233]	Loss 0.0007 (0.0073)	
training:	Epoch: [117][39/233]	Loss 0.0006 (0.0071)	
training:	Epoch: [117][40/233]	Loss 0.0006 (0.0069)	
training:	Epoch: [117][41/233]	Loss 0.2048 (0.0118)	
training:	Epoch: [117][42/233]	Loss 0.0006 (0.0115)	
training:	Epoch: [117][43/233]	Loss 0.0006 (0.0112)	
training:	Epoch: [117][44/233]	Loss 0.0006 (0.0110)	
training:	Epoch: [117][45/233]	Loss 0.0007 (0.0108)	
training:	Epoch: [117][46/233]	Loss 0.0005 (0.0106)	
training:	Epoch: [117][47/233]	Loss 0.0005 (0.0103)	
training:	Epoch: [117][48/233]	Loss 0.0006 (0.0101)	
training:	Epoch: [117][49/233]	Loss 0.0006 (0.0099)	
training:	Epoch: [117][50/233]	Loss 0.0006 (0.0098)	
training:	Epoch: [117][51/233]	Loss 0.0008 (0.0096)	
training:	Epoch: [117][52/233]	Loss 0.0007 (0.0094)	
training:	Epoch: [117][53/233]	Loss 0.0006 (0.0092)	
training:	Epoch: [117][54/233]	Loss 0.0085 (0.0092)	
training:	Epoch: [117][55/233]	Loss 0.0006 (0.0091)	
training:	Epoch: [117][56/233]	Loss 0.0668 (0.0101)	
training:	Epoch: [117][57/233]	Loss 0.0007 (0.0099)	
training:	Epoch: [117][58/233]	Loss 0.0009 (0.0098)	
training:	Epoch: [117][59/233]	Loss 0.0007 (0.0096)	
training:	Epoch: [117][60/233]	Loss 0.0015 (0.0095)	
training:	Epoch: [117][61/233]	Loss 0.0006 (0.0093)	
training:	Epoch: [117][62/233]	Loss 0.0012 (0.0092)	
training:	Epoch: [117][63/233]	Loss 0.0005 (0.0091)	
training:	Epoch: [117][64/233]	Loss 0.0006 (0.0089)	
training:	Epoch: [117][65/233]	Loss 0.0028 (0.0088)	
training:	Epoch: [117][66/233]	Loss 0.0010 (0.0087)	
training:	Epoch: [117][67/233]	Loss 0.0006 (0.0086)	
training:	Epoch: [117][68/233]	Loss 0.0016 (0.0085)	
training:	Epoch: [117][69/233]	Loss 0.0014 (0.0084)	
training:	Epoch: [117][70/233]	Loss 0.0006 (0.0083)	
training:	Epoch: [117][71/233]	Loss 0.0006 (0.0082)	
training:	Epoch: [117][72/233]	Loss 0.0005 (0.0081)	
training:	Epoch: [117][73/233]	Loss 0.0011 (0.0080)	
training:	Epoch: [117][74/233]	Loss 0.0006 (0.0079)	
training:	Epoch: [117][75/233]	Loss 0.0007 (0.0078)	
training:	Epoch: [117][76/233]	Loss 0.1091 (0.0091)	
training:	Epoch: [117][77/233]	Loss 0.0006 (0.0090)	
training:	Epoch: [117][78/233]	Loss 0.0025 (0.0089)	
training:	Epoch: [117][79/233]	Loss 0.0020 (0.0088)	
training:	Epoch: [117][80/233]	Loss 0.0659 (0.0095)	
training:	Epoch: [117][81/233]	Loss 0.0726 (0.0103)	
training:	Epoch: [117][82/233]	Loss 0.0009 (0.0102)	
training:	Epoch: [117][83/233]	Loss 0.0009 (0.0101)	
training:	Epoch: [117][84/233]	Loss 0.0106 (0.0101)	
training:	Epoch: [117][85/233]	Loss 0.0007 (0.0100)	
training:	Epoch: [117][86/233]	Loss 0.0006 (0.0099)	
training:	Epoch: [117][87/233]	Loss 0.0010 (0.0098)	
training:	Epoch: [117][88/233]	Loss 0.0117 (0.0098)	
training:	Epoch: [117][89/233]	Loss 0.1624 (0.0115)	
training:	Epoch: [117][90/233]	Loss 0.0020 (0.0114)	
training:	Epoch: [117][91/233]	Loss 0.0110 (0.0114)	
training:	Epoch: [117][92/233]	Loss 0.0642 (0.0120)	
training:	Epoch: [117][93/233]	Loss 0.0030 (0.0119)	
training:	Epoch: [117][94/233]	Loss 0.0006 (0.0118)	
training:	Epoch: [117][95/233]	Loss 0.0013 (0.0117)	
training:	Epoch: [117][96/233]	Loss 0.0009 (0.0115)	
training:	Epoch: [117][97/233]	Loss 0.0014 (0.0114)	
training:	Epoch: [117][98/233]	Loss 0.0007 (0.0113)	
training:	Epoch: [117][99/233]	Loss 0.0006 (0.0112)	
training:	Epoch: [117][100/233]	Loss 0.0008 (0.0111)	
training:	Epoch: [117][101/233]	Loss 0.0006 (0.0110)	
training:	Epoch: [117][102/233]	Loss 0.0006 (0.0109)	
training:	Epoch: [117][103/233]	Loss 0.0076 (0.0109)	
training:	Epoch: [117][104/233]	Loss 0.0006 (0.0108)	
training:	Epoch: [117][105/233]	Loss 0.0018 (0.0107)	
training:	Epoch: [117][106/233]	Loss 0.0006 (0.0106)	
training:	Epoch: [117][107/233]	Loss 0.0007 (0.0105)	
training:	Epoch: [117][108/233]	Loss 0.0007 (0.0104)	
training:	Epoch: [117][109/233]	Loss 0.0005 (0.0103)	
training:	Epoch: [117][110/233]	Loss 0.0627 (0.0108)	
training:	Epoch: [117][111/233]	Loss 0.0007 (0.0107)	
training:	Epoch: [117][112/233]	Loss 0.0007 (0.0106)	
training:	Epoch: [117][113/233]	Loss 0.0045 (0.0106)	
training:	Epoch: [117][114/233]	Loss 0.0007 (0.0105)	
training:	Epoch: [117][115/233]	Loss 0.0031 (0.0104)	
training:	Epoch: [117][116/233]	Loss 0.0007 (0.0103)	
training:	Epoch: [117][117/233]	Loss 0.0017 (0.0103)	
training:	Epoch: [117][118/233]	Loss 0.0010 (0.0102)	
training:	Epoch: [117][119/233]	Loss 0.0071 (0.0102)	
training:	Epoch: [117][120/233]	Loss 0.0008 (0.0101)	
training:	Epoch: [117][121/233]	Loss 0.0008 (0.0100)	
training:	Epoch: [117][122/233]	Loss 0.0007 (0.0099)	
training:	Epoch: [117][123/233]	Loss 0.0010 (0.0099)	
training:	Epoch: [117][124/233]	Loss 0.0010 (0.0098)	
training:	Epoch: [117][125/233]	Loss 0.0008 (0.0097)	
training:	Epoch: [117][126/233]	Loss 0.0038 (0.0097)	
training:	Epoch: [117][127/233]	Loss 0.0008 (0.0096)	
training:	Epoch: [117][128/233]	Loss 0.0007 (0.0095)	
training:	Epoch: [117][129/233]	Loss 0.0006 (0.0095)	
training:	Epoch: [117][130/233]	Loss 0.0020 (0.0094)	
training:	Epoch: [117][131/233]	Loss 0.0036 (0.0094)	
training:	Epoch: [117][132/233]	Loss 0.0006 (0.0093)	
training:	Epoch: [117][133/233]	Loss 0.0015 (0.0092)	
training:	Epoch: [117][134/233]	Loss 0.0006 (0.0092)	
training:	Epoch: [117][135/233]	Loss 0.0012 (0.0091)	
training:	Epoch: [117][136/233]	Loss 0.0006 (0.0090)	
training:	Epoch: [117][137/233]	Loss 0.0008 (0.0090)	
training:	Epoch: [117][138/233]	Loss 0.0008 (0.0089)	
training:	Epoch: [117][139/233]	Loss 0.0007 (0.0089)	
training:	Epoch: [117][140/233]	Loss 0.0015 (0.0088)	
training:	Epoch: [117][141/233]	Loss 0.0007 (0.0088)	
training:	Epoch: [117][142/233]	Loss 0.0684 (0.0092)	
training:	Epoch: [117][143/233]	Loss 0.0007 (0.0091)	
training:	Epoch: [117][144/233]	Loss 0.0018 (0.0091)	
training:	Epoch: [117][145/233]	Loss 0.0036 (0.0090)	
training:	Epoch: [117][146/233]	Loss 0.0008 (0.0090)	
training:	Epoch: [117][147/233]	Loss 0.0012 (0.0089)	
training:	Epoch: [117][148/233]	Loss 0.0016 (0.0089)	
training:	Epoch: [117][149/233]	Loss 0.0009 (0.0088)	
training:	Epoch: [117][150/233]	Loss 0.0008 (0.0088)	
training:	Epoch: [117][151/233]	Loss 0.0010 (0.0087)	
training:	Epoch: [117][152/233]	Loss 0.0007 (0.0087)	
training:	Epoch: [117][153/233]	Loss 0.0006 (0.0086)	
training:	Epoch: [117][154/233]	Loss 0.0016 (0.0086)	
training:	Epoch: [117][155/233]	Loss 0.0007 (0.0085)	
training:	Epoch: [117][156/233]	Loss 0.0007 (0.0085)	
training:	Epoch: [117][157/233]	Loss 0.0009 (0.0084)	
training:	Epoch: [117][158/233]	Loss 0.0029 (0.0084)	
training:	Epoch: [117][159/233]	Loss 0.0007 (0.0083)	
training:	Epoch: [117][160/233]	Loss 0.0022 (0.0083)	
training:	Epoch: [117][161/233]	Loss 0.0022 (0.0082)	
training:	Epoch: [117][162/233]	Loss 0.0006 (0.0082)	
training:	Epoch: [117][163/233]	Loss 0.0008 (0.0082)	
training:	Epoch: [117][164/233]	Loss 0.0008 (0.0081)	
training:	Epoch: [117][165/233]	Loss 0.0015 (0.0081)	
training:	Epoch: [117][166/233]	Loss 0.0007 (0.0080)	
training:	Epoch: [117][167/233]	Loss 0.0028 (0.0080)	
training:	Epoch: [117][168/233]	Loss 0.0008 (0.0080)	
training:	Epoch: [117][169/233]	Loss 0.0006 (0.0079)	
training:	Epoch: [117][170/233]	Loss 0.0014 (0.0079)	
training:	Epoch: [117][171/233]	Loss 0.0105 (0.0079)	
training:	Epoch: [117][172/233]	Loss 0.0057 (0.0079)	
training:	Epoch: [117][173/233]	Loss 0.0012 (0.0078)	
training:	Epoch: [117][174/233]	Loss 0.0458 (0.0081)	
training:	Epoch: [117][175/233]	Loss 0.0008 (0.0080)	
training:	Epoch: [117][176/233]	Loss 0.0013 (0.0080)	
training:	Epoch: [117][177/233]	Loss 0.0052 (0.0080)	
training:	Epoch: [117][178/233]	Loss 0.0097 (0.0080)	
training:	Epoch: [117][179/233]	Loss 0.0021 (0.0079)	
training:	Epoch: [117][180/233]	Loss 0.0007 (0.0079)	
training:	Epoch: [117][181/233]	Loss 0.0009 (0.0079)	
training:	Epoch: [117][182/233]	Loss 0.0018 (0.0078)	
training:	Epoch: [117][183/233]	Loss 0.0006 (0.0078)	
training:	Epoch: [117][184/233]	Loss 0.0079 (0.0078)	
training:	Epoch: [117][185/233]	Loss 0.0007 (0.0077)	
training:	Epoch: [117][186/233]	Loss 0.0037 (0.0077)	
training:	Epoch: [117][187/233]	Loss 0.0013 (0.0077)	
training:	Epoch: [117][188/233]	Loss 0.0020 (0.0077)	
training:	Epoch: [117][189/233]	Loss 0.0010 (0.0076)	
training:	Epoch: [117][190/233]	Loss 0.0009 (0.0076)	
training:	Epoch: [117][191/233]	Loss 0.1692 (0.0084)	
training:	Epoch: [117][192/233]	Loss 0.0007 (0.0084)	
training:	Epoch: [117][193/233]	Loss 0.0008 (0.0084)	
training:	Epoch: [117][194/233]	Loss 0.0049 (0.0083)	
training:	Epoch: [117][195/233]	Loss 0.0008 (0.0083)	
training:	Epoch: [117][196/233]	Loss 0.0008 (0.0083)	
training:	Epoch: [117][197/233]	Loss 0.0008 (0.0082)	
training:	Epoch: [117][198/233]	Loss 0.0012 (0.0082)	
training:	Epoch: [117][199/233]	Loss 0.0008 (0.0082)	
training:	Epoch: [117][200/233]	Loss 0.0011 (0.0081)	
training:	Epoch: [117][201/233]	Loss 0.0019 (0.0081)	
training:	Epoch: [117][202/233]	Loss 0.0007 (0.0080)	
training:	Epoch: [117][203/233]	Loss 0.0007 (0.0080)	
training:	Epoch: [117][204/233]	Loss 0.0555 (0.0082)	
training:	Epoch: [117][205/233]	Loss 0.0006 (0.0082)	
training:	Epoch: [117][206/233]	Loss 0.0007 (0.0082)	
training:	Epoch: [117][207/233]	Loss 0.0007 (0.0081)	
training:	Epoch: [117][208/233]	Loss 0.0008 (0.0081)	
training:	Epoch: [117][209/233]	Loss 0.0006 (0.0081)	
training:	Epoch: [117][210/233]	Loss 0.1674 (0.0088)	
training:	Epoch: [117][211/233]	Loss 0.0022 (0.0088)	
training:	Epoch: [117][212/233]	Loss 0.0009 (0.0088)	
training:	Epoch: [117][213/233]	Loss 0.0007 (0.0087)	
training:	Epoch: [117][214/233]	Loss 0.0009 (0.0087)	
training:	Epoch: [117][215/233]	Loss 0.0011 (0.0086)	
training:	Epoch: [117][216/233]	Loss 0.0081 (0.0086)	
training:	Epoch: [117][217/233]	Loss 0.0204 (0.0087)	
training:	Epoch: [117][218/233]	Loss 0.0009 (0.0087)	
training:	Epoch: [117][219/233]	Loss 0.0006 (0.0086)	
training:	Epoch: [117][220/233]	Loss 0.0019 (0.0086)	
training:	Epoch: [117][221/233]	Loss 0.0007 (0.0086)	
training:	Epoch: [117][222/233]	Loss 0.0007 (0.0085)	
training:	Epoch: [117][223/233]	Loss 0.0009 (0.0085)	
training:	Epoch: [117][224/233]	Loss 0.0007 (0.0085)	
training:	Epoch: [117][225/233]	Loss 0.0011 (0.0084)	
training:	Epoch: [117][226/233]	Loss 0.0006 (0.0084)	
training:	Epoch: [117][227/233]	Loss 0.0024 (0.0084)	
training:	Epoch: [117][228/233]	Loss 0.0028 (0.0083)	
training:	Epoch: [117][229/233]	Loss 0.0025 (0.0083)	
training:	Epoch: [117][230/233]	Loss 0.0006 (0.0083)	
training:	Epoch: [117][231/233]	Loss 0.0017 (0.0082)	
training:	Epoch: [117][232/233]	Loss 0.0006 (0.0082)	
training:	Epoch: [117][233/233]	Loss 0.0019 (0.0082)	
Training:	 Loss: 0.0082

Training:	 ACC: 0.9996 0.9996 0.9997 0.9994
Validation:	 ACC: 0.7979 0.7972 0.7835 0.8124
Validation:	 Best_BACC: 0.8162 0.8154 0.7998 0.8326
Validation:	 Loss: 1.1749
Pretraining:	Epoch 118/200
----------
training:	Epoch: [118][1/233]	Loss 0.0008 (0.0008)	
training:	Epoch: [118][2/233]	Loss 0.0009 (0.0008)	
training:	Epoch: [118][3/233]	Loss 0.0015 (0.0011)	
training:	Epoch: [118][4/233]	Loss 0.0283 (0.0079)	
training:	Epoch: [118][5/233]	Loss 0.0081 (0.0079)	
training:	Epoch: [118][6/233]	Loss 0.0007 (0.0067)	
training:	Epoch: [118][7/233]	Loss 0.0007 (0.0059)	
training:	Epoch: [118][8/233]	Loss 0.0007 (0.0052)	
training:	Epoch: [118][9/233]	Loss 0.0012 (0.0048)	
training:	Epoch: [118][10/233]	Loss 0.0012 (0.0044)	
training:	Epoch: [118][11/233]	Loss 0.0006 (0.0041)	
training:	Epoch: [118][12/233]	Loss 0.0010 (0.0038)	
training:	Epoch: [118][13/233]	Loss 0.0122 (0.0045)	
training:	Epoch: [118][14/233]	Loss 0.0007 (0.0042)	
training:	Epoch: [118][15/233]	Loss 0.0006 (0.0040)	
training:	Epoch: [118][16/233]	Loss 0.0018 (0.0038)	
training:	Epoch: [118][17/233]	Loss 0.0012 (0.0037)	
training:	Epoch: [118][18/233]	Loss 0.0006 (0.0035)	
training:	Epoch: [118][19/233]	Loss 0.0006 (0.0033)	
training:	Epoch: [118][20/233]	Loss 0.0092 (0.0036)	
training:	Epoch: [118][21/233]	Loss 0.0006 (0.0035)	
training:	Epoch: [118][22/233]	Loss 0.0946 (0.0076)	
training:	Epoch: [118][23/233]	Loss 0.0007 (0.0073)	
training:	Epoch: [118][24/233]	Loss 0.0005 (0.0070)	
training:	Epoch: [118][25/233]	Loss 0.0006 (0.0068)	
training:	Epoch: [118][26/233]	Loss 0.0007 (0.0066)	
training:	Epoch: [118][27/233]	Loss 0.0008 (0.0063)	
training:	Epoch: [118][28/233]	Loss 0.0007 (0.0061)	
training:	Epoch: [118][29/233]	Loss 0.0033 (0.0060)	
training:	Epoch: [118][30/233]	Loss 0.0008 (0.0059)	
training:	Epoch: [118][31/233]	Loss 0.0009 (0.0057)	
training:	Epoch: [118][32/233]	Loss 0.0007 (0.0056)	
training:	Epoch: [118][33/233]	Loss 0.0049 (0.0055)	
training:	Epoch: [118][34/233]	Loss 0.0009 (0.0054)	
training:	Epoch: [118][35/233]	Loss 0.0008 (0.0053)	
training:	Epoch: [118][36/233]	Loss 0.0009 (0.0051)	
training:	Epoch: [118][37/233]	Loss 0.0006 (0.0050)	
training:	Epoch: [118][38/233]	Loss 0.0008 (0.0049)	
training:	Epoch: [118][39/233]	Loss 0.0006 (0.0048)	
training:	Epoch: [118][40/233]	Loss 0.0161 (0.0051)	
training:	Epoch: [118][41/233]	Loss 0.0008 (0.0050)	
training:	Epoch: [118][42/233]	Loss 0.0012 (0.0049)	
training:	Epoch: [118][43/233]	Loss 0.0009 (0.0048)	
training:	Epoch: [118][44/233]	Loss 0.0010 (0.0047)	
training:	Epoch: [118][45/233]	Loss 0.0015 (0.0046)	
training:	Epoch: [118][46/233]	Loss 0.0010 (0.0046)	
training:	Epoch: [118][47/233]	Loss 0.0008 (0.0045)	
training:	Epoch: [118][48/233]	Loss 0.0007 (0.0044)	
training:	Epoch: [118][49/233]	Loss 0.0008 (0.0043)	
training:	Epoch: [118][50/233]	Loss 0.0042 (0.0043)	
training:	Epoch: [118][51/233]	Loss 0.0028 (0.0043)	
training:	Epoch: [118][52/233]	Loss 0.0008 (0.0042)	
training:	Epoch: [118][53/233]	Loss 0.0009 (0.0042)	
training:	Epoch: [118][54/233]	Loss 0.0007 (0.0041)	
training:	Epoch: [118][55/233]	Loss 0.0006 (0.0040)	
training:	Epoch: [118][56/233]	Loss 0.0020 (0.0040)	
training:	Epoch: [118][57/233]	Loss 0.0005 (0.0039)	
training:	Epoch: [118][58/233]	Loss 0.0007 (0.0039)	
training:	Epoch: [118][59/233]	Loss 0.0134 (0.0040)	
training:	Epoch: [118][60/233]	Loss 0.0007 (0.0040)	
training:	Epoch: [118][61/233]	Loss 0.0008 (0.0039)	
training:	Epoch: [118][62/233]	Loss 0.0024 (0.0039)	
training:	Epoch: [118][63/233]	Loss 0.0010 (0.0039)	
training:	Epoch: [118][64/233]	Loss 0.0007 (0.0038)	
training:	Epoch: [118][65/233]	Loss 0.0711 (0.0049)	
training:	Epoch: [118][66/233]	Loss 0.0007 (0.0048)	
training:	Epoch: [118][67/233]	Loss 0.0006 (0.0047)	
training:	Epoch: [118][68/233]	Loss 0.0894 (0.0060)	
training:	Epoch: [118][69/233]	Loss 0.0008 (0.0059)	
training:	Epoch: [118][70/233]	Loss 0.0009 (0.0058)	
training:	Epoch: [118][71/233]	Loss 0.0008 (0.0058)	
training:	Epoch: [118][72/233]	Loss 0.0008 (0.0057)	
training:	Epoch: [118][73/233]	Loss 0.0006 (0.0056)	
training:	Epoch: [118][74/233]	Loss 0.0007 (0.0055)	
training:	Epoch: [118][75/233]	Loss 0.0009 (0.0055)	
training:	Epoch: [118][76/233]	Loss 0.0007 (0.0054)	
training:	Epoch: [118][77/233]	Loss 0.0008 (0.0054)	
training:	Epoch: [118][78/233]	Loss 0.0008 (0.0053)	
training:	Epoch: [118][79/233]	Loss 0.2075 (0.0079)	
training:	Epoch: [118][80/233]	Loss 0.0007 (0.0078)	
training:	Epoch: [118][81/233]	Loss 0.0009 (0.0077)	
training:	Epoch: [118][82/233]	Loss 0.0010 (0.0076)	
training:	Epoch: [118][83/233]	Loss 0.0010 (0.0075)	
training:	Epoch: [118][84/233]	Loss 0.0008 (0.0074)	
training:	Epoch: [118][85/233]	Loss 0.0006 (0.0074)	
training:	Epoch: [118][86/233]	Loss 0.0006 (0.0073)	
training:	Epoch: [118][87/233]	Loss 0.0007 (0.0072)	
training:	Epoch: [118][88/233]	Loss 0.0026 (0.0072)	
training:	Epoch: [118][89/233]	Loss 0.0125 (0.0072)	
training:	Epoch: [118][90/233]	Loss 0.0006 (0.0071)	
training:	Epoch: [118][91/233]	Loss 0.0010 (0.0071)	
training:	Epoch: [118][92/233]	Loss 0.0012 (0.0070)	
training:	Epoch: [118][93/233]	Loss 0.0010 (0.0070)	
training:	Epoch: [118][94/233]	Loss 0.0006 (0.0069)	
training:	Epoch: [118][95/233]	Loss 0.0012 (0.0068)	
training:	Epoch: [118][96/233]	Loss 0.0016 (0.0068)	
training:	Epoch: [118][97/233]	Loss 0.0007 (0.0067)	
training:	Epoch: [118][98/233]	Loss 0.0012 (0.0067)	
training:	Epoch: [118][99/233]	Loss 0.0007 (0.0066)	
training:	Epoch: [118][100/233]	Loss 0.0007 (0.0065)	
training:	Epoch: [118][101/233]	Loss 0.0026 (0.0065)	
training:	Epoch: [118][102/233]	Loss 0.0006 (0.0064)	
training:	Epoch: [118][103/233]	Loss 0.0015 (0.0064)	
training:	Epoch: [118][104/233]	Loss 0.0008 (0.0063)	
training:	Epoch: [118][105/233]	Loss 0.0008 (0.0063)	
training:	Epoch: [118][106/233]	Loss 0.0008 (0.0062)	
training:	Epoch: [118][107/233]	Loss 0.0025 (0.0062)	
training:	Epoch: [118][108/233]	Loss 0.0045 (0.0062)	
training:	Epoch: [118][109/233]	Loss 0.0007 (0.0061)	
training:	Epoch: [118][110/233]	Loss 0.0006 (0.0061)	
training:	Epoch: [118][111/233]	Loss 0.0006 (0.0060)	
training:	Epoch: [118][112/233]	Loss 0.0012 (0.0060)	
training:	Epoch: [118][113/233]	Loss 0.0006 (0.0059)	
training:	Epoch: [118][114/233]	Loss 0.0013 (0.0059)	
training:	Epoch: [118][115/233]	Loss 0.0007 (0.0059)	
training:	Epoch: [118][116/233]	Loss 0.1562 (0.0071)	
training:	Epoch: [118][117/233]	Loss 0.0008 (0.0071)	
training:	Epoch: [118][118/233]	Loss 0.0010 (0.0070)	
training:	Epoch: [118][119/233]	Loss 0.0007 (0.0070)	
training:	Epoch: [118][120/233]	Loss 0.0005 (0.0069)	
training:	Epoch: [118][121/233]	Loss 0.0007 (0.0069)	
training:	Epoch: [118][122/233]	Loss 0.0020 (0.0068)	
training:	Epoch: [118][123/233]	Loss 0.0007 (0.0068)	
training:	Epoch: [118][124/233]	Loss 0.0008 (0.0067)	
training:	Epoch: [118][125/233]	Loss 0.0008 (0.0067)	
training:	Epoch: [118][126/233]	Loss 0.0008 (0.0067)	
training:	Epoch: [118][127/233]	Loss 0.0009 (0.0066)	
training:	Epoch: [118][128/233]	Loss 0.0007 (0.0066)	
training:	Epoch: [118][129/233]	Loss 0.0404 (0.0068)	
training:	Epoch: [118][130/233]	Loss 0.0008 (0.0068)	
training:	Epoch: [118][131/233]	Loss 0.0054 (0.0068)	
training:	Epoch: [118][132/233]	Loss 0.0007 (0.0067)	
training:	Epoch: [118][133/233]	Loss 0.0007 (0.0067)	
training:	Epoch: [118][134/233]	Loss 0.0020 (0.0066)	
training:	Epoch: [118][135/233]	Loss 0.0012 (0.0066)	
training:	Epoch: [118][136/233]	Loss 0.0006 (0.0066)	
training:	Epoch: [118][137/233]	Loss 0.0013 (0.0065)	
training:	Epoch: [118][138/233]	Loss 0.0010 (0.0065)	
training:	Epoch: [118][139/233]	Loss 0.1066 (0.0072)	
training:	Epoch: [118][140/233]	Loss 0.0008 (0.0072)	
training:	Epoch: [118][141/233]	Loss 0.0008 (0.0071)	
training:	Epoch: [118][142/233]	Loss 0.0006 (0.0071)	
training:	Epoch: [118][143/233]	Loss 0.0007 (0.0070)	
training:	Epoch: [118][144/233]	Loss 0.0007 (0.0070)	
training:	Epoch: [118][145/233]	Loss 0.0007 (0.0069)	
training:	Epoch: [118][146/233]	Loss 0.0007 (0.0069)	
training:	Epoch: [118][147/233]	Loss 0.0008 (0.0068)	
training:	Epoch: [118][148/233]	Loss 0.0010 (0.0068)	
training:	Epoch: [118][149/233]	Loss 0.0010 (0.0068)	
training:	Epoch: [118][150/233]	Loss 0.0009 (0.0067)	
training:	Epoch: [118][151/233]	Loss 0.0089 (0.0067)	
training:	Epoch: [118][152/233]	Loss 0.0008 (0.0067)	
training:	Epoch: [118][153/233]	Loss 0.0007 (0.0067)	
training:	Epoch: [118][154/233]	Loss 0.0007 (0.0066)	
training:	Epoch: [118][155/233]	Loss 0.0008 (0.0066)	
training:	Epoch: [118][156/233]	Loss 0.0007 (0.0065)	
training:	Epoch: [118][157/233]	Loss 0.0007 (0.0065)	
training:	Epoch: [118][158/233]	Loss 0.0012 (0.0065)	
training:	Epoch: [118][159/233]	Loss 0.0009 (0.0064)	
training:	Epoch: [118][160/233]	Loss 0.0028 (0.0064)	
training:	Epoch: [118][161/233]	Loss 0.0009 (0.0064)	
training:	Epoch: [118][162/233]	Loss 0.0009 (0.0064)	
training:	Epoch: [118][163/233]	Loss 0.1182 (0.0070)	
training:	Epoch: [118][164/233]	Loss 0.0006 (0.0070)	
training:	Epoch: [118][165/233]	Loss 0.0006 (0.0070)	
training:	Epoch: [118][166/233]	Loss 0.0006 (0.0069)	
training:	Epoch: [118][167/233]	Loss 0.0010 (0.0069)	
training:	Epoch: [118][168/233]	Loss 0.0009 (0.0069)	
training:	Epoch: [118][169/233]	Loss 0.0661 (0.0072)	
training:	Epoch: [118][170/233]	Loss 0.0008 (0.0072)	
training:	Epoch: [118][171/233]	Loss 0.0022 (0.0071)	
training:	Epoch: [118][172/233]	Loss 0.0013 (0.0071)	
training:	Epoch: [118][173/233]	Loss 0.0421 (0.0073)	
training:	Epoch: [118][174/233]	Loss 0.0005 (0.0073)	
training:	Epoch: [118][175/233]	Loss 0.0016 (0.0072)	
training:	Epoch: [118][176/233]	Loss 0.0011 (0.0072)	
training:	Epoch: [118][177/233]	Loss 0.0005 (0.0072)	
training:	Epoch: [118][178/233]	Loss 0.0007 (0.0071)	
training:	Epoch: [118][179/233]	Loss 0.2416 (0.0084)	
training:	Epoch: [118][180/233]	Loss 0.0009 (0.0084)	
training:	Epoch: [118][181/233]	Loss 0.0011 (0.0084)	
training:	Epoch: [118][182/233]	Loss 0.1690 (0.0092)	
training:	Epoch: [118][183/233]	Loss 0.0013 (0.0092)	
training:	Epoch: [118][184/233]	Loss 0.0008 (0.0091)	
training:	Epoch: [118][185/233]	Loss 0.0023 (0.0091)	
training:	Epoch: [118][186/233]	Loss 0.0013 (0.0091)	
training:	Epoch: [118][187/233]	Loss 0.0012 (0.0090)	
training:	Epoch: [118][188/233]	Loss 0.0016 (0.0090)	
training:	Epoch: [118][189/233]	Loss 0.0028 (0.0090)	
training:	Epoch: [118][190/233]	Loss 0.0007 (0.0089)	
training:	Epoch: [118][191/233]	Loss 0.0009 (0.0089)	
training:	Epoch: [118][192/233]	Loss 0.0011 (0.0088)	
training:	Epoch: [118][193/233]	Loss 0.0008 (0.0088)	
training:	Epoch: [118][194/233]	Loss 0.0014 (0.0087)	
training:	Epoch: [118][195/233]	Loss 0.0145 (0.0088)	
training:	Epoch: [118][196/233]	Loss 0.0179 (0.0088)	
training:	Epoch: [118][197/233]	Loss 0.0609 (0.0091)	
training:	Epoch: [118][198/233]	Loss 0.0008 (0.0090)	
training:	Epoch: [118][199/233]	Loss 0.0007 (0.0090)	
training:	Epoch: [118][200/233]	Loss 0.0523 (0.0092)	
training:	Epoch: [118][201/233]	Loss 0.0007 (0.0092)	
training:	Epoch: [118][202/233]	Loss 0.0136 (0.0092)	
training:	Epoch: [118][203/233]	Loss 0.0082 (0.0092)	
training:	Epoch: [118][204/233]	Loss 0.0121 (0.0092)	
training:	Epoch: [118][205/233]	Loss 0.0007 (0.0092)	
training:	Epoch: [118][206/233]	Loss 0.0009 (0.0091)	
training:	Epoch: [118][207/233]	Loss 0.0088 (0.0091)	
training:	Epoch: [118][208/233]	Loss 0.0009 (0.0091)	
training:	Epoch: [118][209/233]	Loss 0.0006 (0.0090)	
training:	Epoch: [118][210/233]	Loss 0.0010 (0.0090)	
training:	Epoch: [118][211/233]	Loss 0.0011 (0.0090)	
training:	Epoch: [118][212/233]	Loss 0.0006 (0.0089)	
training:	Epoch: [118][213/233]	Loss 0.0008 (0.0089)	
training:	Epoch: [118][214/233]	Loss 0.0012 (0.0089)	
training:	Epoch: [118][215/233]	Loss 0.0008 (0.0088)	
training:	Epoch: [118][216/233]	Loss 0.0009 (0.0088)	
training:	Epoch: [118][217/233]	Loss 0.0010 (0.0087)	
training:	Epoch: [118][218/233]	Loss 0.1466 (0.0094)	
training:	Epoch: [118][219/233]	Loss 0.0027 (0.0093)	
training:	Epoch: [118][220/233]	Loss 0.0008 (0.0093)	
training:	Epoch: [118][221/233]	Loss 0.0007 (0.0093)	
training:	Epoch: [118][222/233]	Loss 0.0007 (0.0092)	
training:	Epoch: [118][223/233]	Loss 0.0007 (0.0092)	
training:	Epoch: [118][224/233]	Loss 0.0041 (0.0092)	
training:	Epoch: [118][225/233]	Loss 0.0008 (0.0091)	
training:	Epoch: [118][226/233]	Loss 0.0009 (0.0091)	
training:	Epoch: [118][227/233]	Loss 0.0014 (0.0091)	
training:	Epoch: [118][228/233]	Loss 0.0184 (0.0091)	
training:	Epoch: [118][229/233]	Loss 0.0011 (0.0091)	
training:	Epoch: [118][230/233]	Loss 0.0056 (0.0091)	
training:	Epoch: [118][231/233]	Loss 0.0012 (0.0090)	
training:	Epoch: [118][232/233]	Loss 0.0007 (0.0090)	
training:	Epoch: [118][233/233]	Loss 0.0025 (0.0090)	
Training:	 Loss: 0.0089

Training:	 ACC: 0.9997 0.9997 0.9997 0.9997
Validation:	 ACC: 0.7973 0.7983 0.8172 0.7775
Validation:	 Best_BACC: 0.8162 0.8154 0.7998 0.8326
Validation:	 Loss: 1.0799
Pretraining:	Epoch 119/200
----------
training:	Epoch: [119][1/233]	Loss 0.0008 (0.0008)	
training:	Epoch: [119][2/233]	Loss 0.0009 (0.0009)	
training:	Epoch: [119][3/233]	Loss 0.0006 (0.0008)	
training:	Epoch: [119][4/233]	Loss 0.0007 (0.0008)	
training:	Epoch: [119][5/233]	Loss 0.0011 (0.0008)	
training:	Epoch: [119][6/233]	Loss 0.0093 (0.0022)	
training:	Epoch: [119][7/233]	Loss 0.0031 (0.0024)	
training:	Epoch: [119][8/233]	Loss 0.0008 (0.0022)	
training:	Epoch: [119][9/233]	Loss 0.0011 (0.0020)	
training:	Epoch: [119][10/233]	Loss 0.0013 (0.0020)	
training:	Epoch: [119][11/233]	Loss 0.0010 (0.0019)	
training:	Epoch: [119][12/233]	Loss 0.0013 (0.0018)	
training:	Epoch: [119][13/233]	Loss 0.0010 (0.0018)	
training:	Epoch: [119][14/233]	Loss 0.0007 (0.0017)	
training:	Epoch: [119][15/233]	Loss 0.0008 (0.0016)	
training:	Epoch: [119][16/233]	Loss 0.0007 (0.0016)	
training:	Epoch: [119][17/233]	Loss 0.0108 (0.0021)	
training:	Epoch: [119][18/233]	Loss 0.0014 (0.0021)	
training:	Epoch: [119][19/233]	Loss 0.0005 (0.0020)	
training:	Epoch: [119][20/233]	Loss 0.0124 (0.0025)	
training:	Epoch: [119][21/233]	Loss 0.0009 (0.0024)	
training:	Epoch: [119][22/233]	Loss 0.0007 (0.0024)	
training:	Epoch: [119][23/233]	Loss 0.0024 (0.0024)	
training:	Epoch: [119][24/233]	Loss 0.0008 (0.0023)	
training:	Epoch: [119][25/233]	Loss 0.0016 (0.0023)	
training:	Epoch: [119][26/233]	Loss 0.0006 (0.0022)	
training:	Epoch: [119][27/233]	Loss 0.0008 (0.0022)	
training:	Epoch: [119][28/233]	Loss 0.0012 (0.0021)	
training:	Epoch: [119][29/233]	Loss 0.0010 (0.0021)	
training:	Epoch: [119][30/233]	Loss 0.0008 (0.0020)	
training:	Epoch: [119][31/233]	Loss 0.1028 (0.0053)	
training:	Epoch: [119][32/233]	Loss 0.0170 (0.0057)	
training:	Epoch: [119][33/233]	Loss 0.0008 (0.0055)	
training:	Epoch: [119][34/233]	Loss 0.0009 (0.0054)	
training:	Epoch: [119][35/233]	Loss 0.0017 (0.0053)	
training:	Epoch: [119][36/233]	Loss 0.0157 (0.0056)	
training:	Epoch: [119][37/233]	Loss 0.0011 (0.0054)	
training:	Epoch: [119][38/233]	Loss 0.0008 (0.0053)	
training:	Epoch: [119][39/233]	Loss 0.0007 (0.0052)	
training:	Epoch: [119][40/233]	Loss 0.0096 (0.0053)	
training:	Epoch: [119][41/233]	Loss 0.0008 (0.0052)	
training:	Epoch: [119][42/233]	Loss 0.0015 (0.0051)	
training:	Epoch: [119][43/233]	Loss 0.0024 (0.0050)	
training:	Epoch: [119][44/233]	Loss 0.0008 (0.0049)	
training:	Epoch: [119][45/233]	Loss 0.0008 (0.0049)	
training:	Epoch: [119][46/233]	Loss 0.0013 (0.0048)	
training:	Epoch: [119][47/233]	Loss 0.0011 (0.0047)	
training:	Epoch: [119][48/233]	Loss 0.0061 (0.0047)	
training:	Epoch: [119][49/233]	Loss 0.0058 (0.0048)	
training:	Epoch: [119][50/233]	Loss 0.0010 (0.0047)	
training:	Epoch: [119][51/233]	Loss 0.0023 (0.0046)	
training:	Epoch: [119][52/233]	Loss 0.0008 (0.0046)	
training:	Epoch: [119][53/233]	Loss 0.0015 (0.0045)	
training:	Epoch: [119][54/233]	Loss 0.0009 (0.0044)	
training:	Epoch: [119][55/233]	Loss 0.0008 (0.0044)	
training:	Epoch: [119][56/233]	Loss 0.0009 (0.0043)	
training:	Epoch: [119][57/233]	Loss 0.0030 (0.0043)	
training:	Epoch: [119][58/233]	Loss 0.0008 (0.0042)	
training:	Epoch: [119][59/233]	Loss 0.0006 (0.0042)	
training:	Epoch: [119][60/233]	Loss 0.0013 (0.0041)	
training:	Epoch: [119][61/233]	Loss 0.0008 (0.0041)	
training:	Epoch: [119][62/233]	Loss 0.0007 (0.0040)	
training:	Epoch: [119][63/233]	Loss 0.0008 (0.0040)	
training:	Epoch: [119][64/233]	Loss 0.0008 (0.0039)	
training:	Epoch: [119][65/233]	Loss 0.0016 (0.0039)	
training:	Epoch: [119][66/233]	Loss 0.0018 (0.0038)	
training:	Epoch: [119][67/233]	Loss 0.0007 (0.0038)	
training:	Epoch: [119][68/233]	Loss 0.0020 (0.0038)	
training:	Epoch: [119][69/233]	Loss 0.0012 (0.0037)	
training:	Epoch: [119][70/233]	Loss 0.0027 (0.0037)	
training:	Epoch: [119][71/233]	Loss 0.0007 (0.0037)	
training:	Epoch: [119][72/233]	Loss 0.0023 (0.0037)	
training:	Epoch: [119][73/233]	Loss 0.0013 (0.0036)	
training:	Epoch: [119][74/233]	Loss 0.0009 (0.0036)	
training:	Epoch: [119][75/233]	Loss 0.0007 (0.0035)	
training:	Epoch: [119][76/233]	Loss 0.0021 (0.0035)	
training:	Epoch: [119][77/233]	Loss 0.0046 (0.0035)	
training:	Epoch: [119][78/233]	Loss 0.0007 (0.0035)	
training:	Epoch: [119][79/233]	Loss 0.0046 (0.0035)	
training:	Epoch: [119][80/233]	Loss 0.0006 (0.0035)	
training:	Epoch: [119][81/233]	Loss 0.0384 (0.0039)	
training:	Epoch: [119][82/233]	Loss 0.0008 (0.0039)	
training:	Epoch: [119][83/233]	Loss 0.0326 (0.0042)	
training:	Epoch: [119][84/233]	Loss 0.0007 (0.0042)	
training:	Epoch: [119][85/233]	Loss 0.0008 (0.0041)	
training:	Epoch: [119][86/233]	Loss 0.0013 (0.0041)	
training:	Epoch: [119][87/233]	Loss 0.0008 (0.0041)	
training:	Epoch: [119][88/233]	Loss 0.0298 (0.0044)	
training:	Epoch: [119][89/233]	Loss 0.0007 (0.0043)	
training:	Epoch: [119][90/233]	Loss 0.0294 (0.0046)	
training:	Epoch: [119][91/233]	Loss 0.0018 (0.0046)	
training:	Epoch: [119][92/233]	Loss 0.0007 (0.0045)	
training:	Epoch: [119][93/233]	Loss 0.0011 (0.0045)	
training:	Epoch: [119][94/233]	Loss 0.0007 (0.0044)	
training:	Epoch: [119][95/233]	Loss 0.0006 (0.0044)	
training:	Epoch: [119][96/233]	Loss 0.1445 (0.0059)	
training:	Epoch: [119][97/233]	Loss 0.0009 (0.0058)	
training:	Epoch: [119][98/233]	Loss 0.0008 (0.0058)	
training:	Epoch: [119][99/233]	Loss 0.0201 (0.0059)	
training:	Epoch: [119][100/233]	Loss 0.0009 (0.0059)	
training:	Epoch: [119][101/233]	Loss 0.0007 (0.0058)	
training:	Epoch: [119][102/233]	Loss 0.0006 (0.0058)	
training:	Epoch: [119][103/233]	Loss 0.0016 (0.0057)	
training:	Epoch: [119][104/233]	Loss 0.0012 (0.0057)	
training:	Epoch: [119][105/233]	Loss 0.0010 (0.0056)	
training:	Epoch: [119][106/233]	Loss 0.0018 (0.0056)	
training:	Epoch: [119][107/233]	Loss 0.0010 (0.0055)	
training:	Epoch: [119][108/233]	Loss 0.0007 (0.0055)	
training:	Epoch: [119][109/233]	Loss 0.0086 (0.0055)	
training:	Epoch: [119][110/233]	Loss 0.0009 (0.0055)	
training:	Epoch: [119][111/233]	Loss 0.0007 (0.0054)	
training:	Epoch: [119][112/233]	Loss 0.0023 (0.0054)	
training:	Epoch: [119][113/233]	Loss 0.0015 (0.0054)	
training:	Epoch: [119][114/233]	Loss 0.0017 (0.0054)	
training:	Epoch: [119][115/233]	Loss 0.0006 (0.0053)	
training:	Epoch: [119][116/233]	Loss 0.0007 (0.0053)	
training:	Epoch: [119][117/233]	Loss 0.0066 (0.0053)	
training:	Epoch: [119][118/233]	Loss 0.0009 (0.0052)	
training:	Epoch: [119][119/233]	Loss 0.2367 (0.0072)	
training:	Epoch: [119][120/233]	Loss 0.0013 (0.0071)	
training:	Epoch: [119][121/233]	Loss 0.0020 (0.0071)	
training:	Epoch: [119][122/233]	Loss 0.0007 (0.0070)	
training:	Epoch: [119][123/233]	Loss 0.0007 (0.0070)	
training:	Epoch: [119][124/233]	Loss 0.0010 (0.0069)	
training:	Epoch: [119][125/233]	Loss 0.0007 (0.0069)	
training:	Epoch: [119][126/233]	Loss 0.0165 (0.0070)	
training:	Epoch: [119][127/233]	Loss 0.0012 (0.0069)	
training:	Epoch: [119][128/233]	Loss 0.0007 (0.0069)	
training:	Epoch: [119][129/233]	Loss 0.0007 (0.0068)	
training:	Epoch: [119][130/233]	Loss 0.0014 (0.0068)	
training:	Epoch: [119][131/233]	Loss 0.0018 (0.0067)	
training:	Epoch: [119][132/233]	Loss 0.0007 (0.0067)	
training:	Epoch: [119][133/233]	Loss 0.0009 (0.0067)	
training:	Epoch: [119][134/233]	Loss 0.0007 (0.0066)	
training:	Epoch: [119][135/233]	Loss 0.0008 (0.0066)	
training:	Epoch: [119][136/233]	Loss 0.0009 (0.0065)	
training:	Epoch: [119][137/233]	Loss 0.0006 (0.0065)	
training:	Epoch: [119][138/233]	Loss 0.0007 (0.0064)	
training:	Epoch: [119][139/233]	Loss 0.0010 (0.0064)	
training:	Epoch: [119][140/233]	Loss 0.0006 (0.0064)	
training:	Epoch: [119][141/233]	Loss 0.0007 (0.0063)	
training:	Epoch: [119][142/233]	Loss 0.0009 (0.0063)	
training:	Epoch: [119][143/233]	Loss 0.0006 (0.0062)	
training:	Epoch: [119][144/233]	Loss 0.1761 (0.0074)	
training:	Epoch: [119][145/233]	Loss 0.0006 (0.0074)	
training:	Epoch: [119][146/233]	Loss 0.0015 (0.0073)	
training:	Epoch: [119][147/233]	Loss 0.0014 (0.0073)	
training:	Epoch: [119][148/233]	Loss 0.0011 (0.0073)	
training:	Epoch: [119][149/233]	Loss 0.0015 (0.0072)	
training:	Epoch: [119][150/233]	Loss 0.0058 (0.0072)	
training:	Epoch: [119][151/233]	Loss 0.0008 (0.0072)	
training:	Epoch: [119][152/233]	Loss 0.0298 (0.0073)	
training:	Epoch: [119][153/233]	Loss 0.0008 (0.0073)	
training:	Epoch: [119][154/233]	Loss 0.0006 (0.0072)	
training:	Epoch: [119][155/233]	Loss 0.0013 (0.0072)	
training:	Epoch: [119][156/233]	Loss 0.0033 (0.0072)	
training:	Epoch: [119][157/233]	Loss 0.0009 (0.0071)	
training:	Epoch: [119][158/233]	Loss 0.0162 (0.0072)	
training:	Epoch: [119][159/233]	Loss 0.0008 (0.0071)	
training:	Epoch: [119][160/233]	Loss 0.0007 (0.0071)	
training:	Epoch: [119][161/233]	Loss 0.0018 (0.0071)	
training:	Epoch: [119][162/233]	Loss 0.0018 (0.0070)	
training:	Epoch: [119][163/233]	Loss 0.0011 (0.0070)	
training:	Epoch: [119][164/233]	Loss 0.0011 (0.0070)	
training:	Epoch: [119][165/233]	Loss 0.0008 (0.0069)	
training:	Epoch: [119][166/233]	Loss 0.0008 (0.0069)	
training:	Epoch: [119][167/233]	Loss 0.0037 (0.0069)	
training:	Epoch: [119][168/233]	Loss 0.0006 (0.0068)	
training:	Epoch: [119][169/233]	Loss 0.0007 (0.0068)	
training:	Epoch: [119][170/233]	Loss 0.0017 (0.0068)	
training:	Epoch: [119][171/233]	Loss 0.0007 (0.0067)	
training:	Epoch: [119][172/233]	Loss 0.0007 (0.0067)	
training:	Epoch: [119][173/233]	Loss 0.0012 (0.0067)	
training:	Epoch: [119][174/233]	Loss 0.0009 (0.0066)	
training:	Epoch: [119][175/233]	Loss 0.0023 (0.0066)	
training:	Epoch: [119][176/233]	Loss 0.0008 (0.0066)	
training:	Epoch: [119][177/233]	Loss 0.0010 (0.0065)	
training:	Epoch: [119][178/233]	Loss 0.0013 (0.0065)	
training:	Epoch: [119][179/233]	Loss 0.0012 (0.0065)	
training:	Epoch: [119][180/233]	Loss 0.0007 (0.0065)	
training:	Epoch: [119][181/233]	Loss 0.0010 (0.0064)	
training:	Epoch: [119][182/233]	Loss 0.0015 (0.0064)	
training:	Epoch: [119][183/233]	Loss 0.0010 (0.0064)	
training:	Epoch: [119][184/233]	Loss 0.0006 (0.0063)	
training:	Epoch: [119][185/233]	Loss 0.0008 (0.0063)	
training:	Epoch: [119][186/233]	Loss 0.0006 (0.0063)	
training:	Epoch: [119][187/233]	Loss 0.1468 (0.0070)	
training:	Epoch: [119][188/233]	Loss 0.0009 (0.0070)	
training:	Epoch: [119][189/233]	Loss 0.0006 (0.0070)	
training:	Epoch: [119][190/233]	Loss 0.0010 (0.0069)	
training:	Epoch: [119][191/233]	Loss 0.0006 (0.0069)	
training:	Epoch: [119][192/233]	Loss 0.0010 (0.0069)	
training:	Epoch: [119][193/233]	Loss 0.0024 (0.0068)	
training:	Epoch: [119][194/233]	Loss 0.0017 (0.0068)	
training:	Epoch: [119][195/233]	Loss 0.0007 (0.0068)	
training:	Epoch: [119][196/233]	Loss 0.0012 (0.0068)	
training:	Epoch: [119][197/233]	Loss 0.0011 (0.0067)	
training:	Epoch: [119][198/233]	Loss 0.0007 (0.0067)	
training:	Epoch: [119][199/233]	Loss 0.0009 (0.0067)	
training:	Epoch: [119][200/233]	Loss 0.0009 (0.0066)	
training:	Epoch: [119][201/233]	Loss 0.0006 (0.0066)	
training:	Epoch: [119][202/233]	Loss 0.0006 (0.0066)	
training:	Epoch: [119][203/233]	Loss 0.0014 (0.0066)	
training:	Epoch: [119][204/233]	Loss 0.0014 (0.0065)	
training:	Epoch: [119][205/233]	Loss 0.0008 (0.0065)	
training:	Epoch: [119][206/233]	Loss 0.0010 (0.0065)	
training:	Epoch: [119][207/233]	Loss 0.0046 (0.0065)	
training:	Epoch: [119][208/233]	Loss 0.0026 (0.0064)	
training:	Epoch: [119][209/233]	Loss 0.0005 (0.0064)	
training:	Epoch: [119][210/233]	Loss 0.0007 (0.0064)	
training:	Epoch: [119][211/233]	Loss 0.0031 (0.0064)	
training:	Epoch: [119][212/233]	Loss 0.0008 (0.0063)	
training:	Epoch: [119][213/233]	Loss 0.0007 (0.0063)	
training:	Epoch: [119][214/233]	Loss 0.0006 (0.0063)	
training:	Epoch: [119][215/233]	Loss 0.0007 (0.0063)	
training:	Epoch: [119][216/233]	Loss 0.0007 (0.0062)	
training:	Epoch: [119][217/233]	Loss 0.0022 (0.0062)	
training:	Epoch: [119][218/233]	Loss 0.0010 (0.0062)	
training:	Epoch: [119][219/233]	Loss 0.0163 (0.0062)	
training:	Epoch: [119][220/233]	Loss 0.0007 (0.0062)	
training:	Epoch: [119][221/233]	Loss 0.0010 (0.0062)	
training:	Epoch: [119][222/233]	Loss 0.0743 (0.0065)	
training:	Epoch: [119][223/233]	Loss 0.0011 (0.0065)	
training:	Epoch: [119][224/233]	Loss 0.0009 (0.0065)	
training:	Epoch: [119][225/233]	Loss 0.0137 (0.0065)	
training:	Epoch: [119][226/233]	Loss 0.0006 (0.0065)	
training:	Epoch: [119][227/233]	Loss 0.0008 (0.0064)	
training:	Epoch: [119][228/233]	Loss 0.0009 (0.0064)	
training:	Epoch: [119][229/233]	Loss 0.0018 (0.0064)	
training:	Epoch: [119][230/233]	Loss 0.0568 (0.0066)	
training:	Epoch: [119][231/233]	Loss 0.0006 (0.0066)	
training:	Epoch: [119][232/233]	Loss 0.0007 (0.0066)	
training:	Epoch: [119][233/233]	Loss 0.0007 (0.0065)	
Training:	 Loss: 0.0065

Training:	 ACC: 0.9997 0.9997 0.9997 0.9997
Validation:	 ACC: 0.7964 0.7945 0.7579 0.8348
Validation:	 Best_BACC: 0.8162 0.8154 0.7998 0.8326
Validation:	 Loss: 1.1492
Pretraining:	Epoch 120/200
----------
training:	Epoch: [120][1/233]	Loss 0.0007 (0.0007)	
training:	Epoch: [120][2/233]	Loss 0.0008 (0.0007)	
training:	Epoch: [120][3/233]	Loss 0.0970 (0.0328)	
training:	Epoch: [120][4/233]	Loss 0.0007 (0.0248)	
training:	Epoch: [120][5/233]	Loss 0.0007 (0.0200)	
training:	Epoch: [120][6/233]	Loss 0.0041 (0.0173)	
training:	Epoch: [120][7/233]	Loss 0.0006 (0.0149)	
training:	Epoch: [120][8/233]	Loss 0.0008 (0.0132)	
training:	Epoch: [120][9/233]	Loss 0.0007 (0.0118)	
training:	Epoch: [120][10/233]	Loss 0.0023 (0.0108)	
training:	Epoch: [120][11/233]	Loss 0.0010 (0.0099)	
training:	Epoch: [120][12/233]	Loss 0.0012 (0.0092)	
training:	Epoch: [120][13/233]	Loss 0.0013 (0.0086)	
training:	Epoch: [120][14/233]	Loss 0.0183 (0.0093)	
training:	Epoch: [120][15/233]	Loss 0.0034 (0.0089)	
training:	Epoch: [120][16/233]	Loss 0.0013 (0.0084)	
training:	Epoch: [120][17/233]	Loss 0.0010 (0.0080)	
training:	Epoch: [120][18/233]	Loss 0.0008 (0.0076)	
training:	Epoch: [120][19/233]	Loss 0.0010 (0.0072)	
training:	Epoch: [120][20/233]	Loss 0.0008 (0.0069)	
training:	Epoch: [120][21/233]	Loss 0.0008 (0.0066)	
training:	Epoch: [120][22/233]	Loss 0.0008 (0.0064)	
training:	Epoch: [120][23/233]	Loss 0.0059 (0.0063)	
training:	Epoch: [120][24/233]	Loss 0.0006 (0.0061)	
training:	Epoch: [120][25/233]	Loss 0.0011 (0.0059)	
training:	Epoch: [120][26/233]	Loss 0.0112 (0.0061)	
training:	Epoch: [120][27/233]	Loss 0.0009 (0.0059)	
training:	Epoch: [120][28/233]	Loss 0.0009 (0.0057)	
training:	Epoch: [120][29/233]	Loss 0.0012 (0.0056)	
training:	Epoch: [120][30/233]	Loss 0.0008 (0.0054)	
training:	Epoch: [120][31/233]	Loss 0.0017 (0.0053)	
training:	Epoch: [120][32/233]	Loss 0.0007 (0.0052)	
training:	Epoch: [120][33/233]	Loss 0.0012 (0.0050)	
training:	Epoch: [120][34/233]	Loss 0.0008 (0.0049)	
training:	Epoch: [120][35/233]	Loss 0.0011 (0.0048)	
training:	Epoch: [120][36/233]	Loss 0.0015 (0.0047)	
training:	Epoch: [120][37/233]	Loss 0.0951 (0.0072)	
training:	Epoch: [120][38/233]	Loss 0.0008 (0.0070)	
training:	Epoch: [120][39/233]	Loss 0.0009 (0.0068)	
training:	Epoch: [120][40/233]	Loss 0.0015 (0.0067)	
training:	Epoch: [120][41/233]	Loss 0.0010 (0.0066)	
training:	Epoch: [120][42/233]	Loss 0.0006 (0.0064)	
training:	Epoch: [120][43/233]	Loss 0.0945 (0.0085)	
training:	Epoch: [120][44/233]	Loss 0.0009 (0.0083)	
training:	Epoch: [120][45/233]	Loss 0.0006 (0.0081)	
training:	Epoch: [120][46/233]	Loss 0.0023 (0.0080)	
training:	Epoch: [120][47/233]	Loss 0.0006 (0.0078)	
training:	Epoch: [120][48/233]	Loss 0.0008 (0.0077)	
training:	Epoch: [120][49/233]	Loss 0.0008 (0.0076)	
training:	Epoch: [120][50/233]	Loss 0.0005 (0.0074)	
training:	Epoch: [120][51/233]	Loss 0.0008 (0.0073)	
training:	Epoch: [120][52/233]	Loss 0.0008 (0.0072)	
training:	Epoch: [120][53/233]	Loss 0.0007 (0.0070)	
training:	Epoch: [120][54/233]	Loss 0.0007 (0.0069)	
training:	Epoch: [120][55/233]	Loss 0.0011 (0.0068)	
training:	Epoch: [120][56/233]	Loss 0.0007 (0.0067)	
training:	Epoch: [120][57/233]	Loss 0.0008 (0.0066)	
training:	Epoch: [120][58/233]	Loss 0.0009 (0.0065)	
training:	Epoch: [120][59/233]	Loss 0.0007 (0.0064)	
training:	Epoch: [120][60/233]	Loss 0.0027 (0.0063)	
training:	Epoch: [120][61/233]	Loss 0.0008 (0.0063)	
training:	Epoch: [120][62/233]	Loss 0.2349 (0.0099)	
training:	Epoch: [120][63/233]	Loss 0.0644 (0.0108)	
training:	Epoch: [120][64/233]	Loss 0.0018 (0.0107)	
training:	Epoch: [120][65/233]	Loss 0.0020 (0.0105)	
training:	Epoch: [120][66/233]	Loss 0.0006 (0.0104)	
training:	Epoch: [120][67/233]	Loss 0.0027 (0.0103)	
training:	Epoch: [120][68/233]	Loss 0.0006 (0.0101)	
training:	Epoch: [120][69/233]	Loss 0.0005 (0.0100)	
training:	Epoch: [120][70/233]	Loss 0.0006 (0.0098)	
training:	Epoch: [120][71/233]	Loss 0.0007 (0.0097)	
training:	Epoch: [120][72/233]	Loss 0.0006 (0.0096)	
training:	Epoch: [120][73/233]	Loss 0.0007 (0.0095)	
training:	Epoch: [120][74/233]	Loss 0.0008 (0.0094)	
training:	Epoch: [120][75/233]	Loss 0.0006 (0.0092)	
training:	Epoch: [120][76/233]	Loss 0.0018 (0.0091)	
training:	Epoch: [120][77/233]	Loss 0.0009 (0.0090)	
training:	Epoch: [120][78/233]	Loss 0.0023 (0.0089)	
training:	Epoch: [120][79/233]	Loss 0.0007 (0.0088)	
training:	Epoch: [120][80/233]	Loss 0.0008 (0.0087)	
training:	Epoch: [120][81/233]	Loss 0.0007 (0.0086)	
training:	Epoch: [120][82/233]	Loss 0.0017 (0.0086)	
training:	Epoch: [120][83/233]	Loss 0.0012 (0.0085)	
training:	Epoch: [120][84/233]	Loss 0.0115 (0.0085)	
training:	Epoch: [120][85/233]	Loss 0.0007 (0.0084)	
training:	Epoch: [120][86/233]	Loss 0.0045 (0.0084)	
training:	Epoch: [120][87/233]	Loss 0.0005 (0.0083)	
training:	Epoch: [120][88/233]	Loss 0.0021 (0.0082)	
training:	Epoch: [120][89/233]	Loss 0.0006 (0.0081)	
training:	Epoch: [120][90/233]	Loss 0.0006 (0.0080)	
training:	Epoch: [120][91/233]	Loss 0.0007 (0.0080)	
training:	Epoch: [120][92/233]	Loss 0.0006 (0.0079)	
training:	Epoch: [120][93/233]	Loss 0.0257 (0.0081)	
training:	Epoch: [120][94/233]	Loss 0.0006 (0.0080)	
training:	Epoch: [120][95/233]	Loss 0.0006 (0.0079)	
training:	Epoch: [120][96/233]	Loss 0.0008 (0.0078)	
training:	Epoch: [120][97/233]	Loss 0.0036 (0.0078)	
training:	Epoch: [120][98/233]	Loss 0.0007 (0.0077)	
training:	Epoch: [120][99/233]	Loss 0.0005 (0.0076)	
training:	Epoch: [120][100/233]	Loss 0.0107 (0.0077)	
training:	Epoch: [120][101/233]	Loss 0.0018 (0.0076)	
training:	Epoch: [120][102/233]	Loss 0.0005 (0.0076)	
training:	Epoch: [120][103/233]	Loss 0.0006 (0.0075)	
training:	Epoch: [120][104/233]	Loss 0.0045 (0.0075)	
training:	Epoch: [120][105/233]	Loss 0.0243 (0.0076)	
training:	Epoch: [120][106/233]	Loss 0.0010 (0.0076)	
training:	Epoch: [120][107/233]	Loss 0.0006 (0.0075)	
training:	Epoch: [120][108/233]	Loss 0.0014 (0.0074)	
training:	Epoch: [120][109/233]	Loss 0.0007 (0.0074)	
training:	Epoch: [120][110/233]	Loss 0.0007 (0.0073)	
training:	Epoch: [120][111/233]	Loss 0.0011 (0.0073)	
training:	Epoch: [120][112/233]	Loss 0.0013 (0.0072)	
training:	Epoch: [120][113/233]	Loss 0.0005 (0.0071)	
training:	Epoch: [120][114/233]	Loss 0.0005 (0.0071)	
training:	Epoch: [120][115/233]	Loss 0.0011 (0.0070)	
training:	Epoch: [120][116/233]	Loss 0.0007 (0.0070)	
training:	Epoch: [120][117/233]	Loss 0.0008 (0.0069)	
training:	Epoch: [120][118/233]	Loss 0.0006 (0.0069)	
training:	Epoch: [120][119/233]	Loss 0.0013 (0.0068)	
training:	Epoch: [120][120/233]	Loss 0.0019 (0.0068)	
training:	Epoch: [120][121/233]	Loss 0.0006 (0.0067)	
training:	Epoch: [120][122/233]	Loss 0.0005 (0.0067)	
training:	Epoch: [120][123/233]	Loss 0.0201 (0.0068)	
training:	Epoch: [120][124/233]	Loss 0.0009 (0.0067)	
training:	Epoch: [120][125/233]	Loss 0.0011 (0.0067)	
training:	Epoch: [120][126/233]	Loss 0.0006 (0.0066)	
training:	Epoch: [120][127/233]	Loss 0.0007 (0.0066)	
training:	Epoch: [120][128/233]	Loss 0.0007 (0.0066)	
training:	Epoch: [120][129/233]	Loss 0.0007 (0.0065)	
training:	Epoch: [120][130/233]	Loss 0.0007 (0.0065)	
training:	Epoch: [120][131/233]	Loss 0.0006 (0.0064)	
training:	Epoch: [120][132/233]	Loss 0.0012 (0.0064)	
training:	Epoch: [120][133/233]	Loss 0.0006 (0.0063)	
training:	Epoch: [120][134/233]	Loss 0.0006 (0.0063)	
training:	Epoch: [120][135/233]	Loss 0.0006 (0.0063)	
training:	Epoch: [120][136/233]	Loss 0.0147 (0.0063)	
training:	Epoch: [120][137/233]	Loss 0.0007 (0.0063)	
training:	Epoch: [120][138/233]	Loss 0.0007 (0.0062)	
training:	Epoch: [120][139/233]	Loss 0.0009 (0.0062)	
training:	Epoch: [120][140/233]	Loss 0.0007 (0.0062)	
training:	Epoch: [120][141/233]	Loss 0.0006 (0.0061)	
training:	Epoch: [120][142/233]	Loss 0.0006 (0.0061)	
training:	Epoch: [120][143/233]	Loss 0.0006 (0.0060)	
training:	Epoch: [120][144/233]	Loss 0.0007 (0.0060)	
training:	Epoch: [120][145/233]	Loss 0.0008 (0.0060)	
training:	Epoch: [120][146/233]	Loss 0.0014 (0.0059)	
training:	Epoch: [120][147/233]	Loss 0.0006 (0.0059)	
training:	Epoch: [120][148/233]	Loss 0.0007 (0.0059)	
training:	Epoch: [120][149/233]	Loss 0.0014 (0.0058)	
training:	Epoch: [120][150/233]	Loss 0.0064 (0.0058)	
training:	Epoch: [120][151/233]	Loss 0.0008 (0.0058)	
training:	Epoch: [120][152/233]	Loss 0.0007 (0.0058)	
training:	Epoch: [120][153/233]	Loss 0.0005 (0.0057)	
training:	Epoch: [120][154/233]	Loss 0.0016 (0.0057)	
training:	Epoch: [120][155/233]	Loss 0.0008 (0.0057)	
training:	Epoch: [120][156/233]	Loss 0.1388 (0.0065)	
training:	Epoch: [120][157/233]	Loss 0.0007 (0.0065)	
training:	Epoch: [120][158/233]	Loss 0.0017 (0.0065)	
training:	Epoch: [120][159/233]	Loss 0.0011 (0.0064)	
training:	Epoch: [120][160/233]	Loss 0.0006 (0.0064)	
training:	Epoch: [120][161/233]	Loss 0.0011 (0.0064)	
training:	Epoch: [120][162/233]	Loss 0.0068 (0.0064)	
training:	Epoch: [120][163/233]	Loss 0.0007 (0.0063)	
training:	Epoch: [120][164/233]	Loss 0.0006 (0.0063)	
training:	Epoch: [120][165/233]	Loss 0.0006 (0.0063)	
training:	Epoch: [120][166/233]	Loss 0.2191 (0.0075)	
training:	Epoch: [120][167/233]	Loss 0.0008 (0.0075)	
training:	Epoch: [120][168/233]	Loss 0.0006 (0.0075)	
training:	Epoch: [120][169/233]	Loss 0.0045 (0.0074)	
training:	Epoch: [120][170/233]	Loss 0.0007 (0.0074)	
training:	Epoch: [120][171/233]	Loss 0.0006 (0.0074)	
training:	Epoch: [120][172/233]	Loss 0.0009 (0.0073)	
training:	Epoch: [120][173/233]	Loss 0.0006 (0.0073)	
training:	Epoch: [120][174/233]	Loss 0.0009 (0.0072)	
training:	Epoch: [120][175/233]	Loss 0.0008 (0.0072)	
training:	Epoch: [120][176/233]	Loss 0.0006 (0.0072)	
training:	Epoch: [120][177/233]	Loss 0.0006 (0.0071)	
training:	Epoch: [120][178/233]	Loss 0.0006 (0.0071)	
training:	Epoch: [120][179/233]	Loss 0.0010 (0.0071)	
training:	Epoch: [120][180/233]	Loss 0.0008 (0.0070)	
training:	Epoch: [120][181/233]	Loss 0.0006 (0.0070)	
training:	Epoch: [120][182/233]	Loss 0.0006 (0.0070)	
training:	Epoch: [120][183/233]	Loss 0.0011 (0.0069)	
training:	Epoch: [120][184/233]	Loss 0.0007 (0.0069)	
training:	Epoch: [120][185/233]	Loss 0.0009 (0.0069)	
training:	Epoch: [120][186/233]	Loss 0.0006 (0.0068)	
training:	Epoch: [120][187/233]	Loss 0.0042 (0.0068)	
training:	Epoch: [120][188/233]	Loss 0.0008 (0.0068)	
training:	Epoch: [120][189/233]	Loss 0.0006 (0.0068)	
training:	Epoch: [120][190/233]	Loss 0.0008 (0.0067)	
training:	Epoch: [120][191/233]	Loss 0.0018 (0.0067)	
training:	Epoch: [120][192/233]	Loss 0.0008 (0.0067)	
training:	Epoch: [120][193/233]	Loss 0.0006 (0.0066)	
training:	Epoch: [120][194/233]	Loss 0.0008 (0.0066)	
training:	Epoch: [120][195/233]	Loss 0.0006 (0.0066)	
training:	Epoch: [120][196/233]	Loss 0.0008 (0.0065)	
training:	Epoch: [120][197/233]	Loss 0.0007 (0.0065)	
training:	Epoch: [120][198/233]	Loss 0.0005 (0.0065)	
training:	Epoch: [120][199/233]	Loss 0.0007 (0.0065)	
training:	Epoch: [120][200/233]	Loss 0.0006 (0.0064)	
training:	Epoch: [120][201/233]	Loss 0.0007 (0.0064)	
training:	Epoch: [120][202/233]	Loss 0.0008 (0.0064)	
training:	Epoch: [120][203/233]	Loss 0.0007 (0.0063)	
training:	Epoch: [120][204/233]	Loss 0.0005 (0.0063)	
training:	Epoch: [120][205/233]	Loss 0.0008 (0.0063)	
training:	Epoch: [120][206/233]	Loss 0.0006 (0.0063)	
training:	Epoch: [120][207/233]	Loss 0.0014 (0.0062)	
training:	Epoch: [120][208/233]	Loss 0.0027 (0.0062)	
training:	Epoch: [120][209/233]	Loss 0.0006 (0.0062)	
training:	Epoch: [120][210/233]	Loss 0.0302 (0.0063)	
training:	Epoch: [120][211/233]	Loss 0.0012 (0.0063)	
training:	Epoch: [120][212/233]	Loss 0.0011 (0.0063)	
training:	Epoch: [120][213/233]	Loss 0.0005 (0.0062)	
training:	Epoch: [120][214/233]	Loss 0.0008 (0.0062)	
training:	Epoch: [120][215/233]	Loss 0.0006 (0.0062)	
training:	Epoch: [120][216/233]	Loss 0.0007 (0.0062)	
training:	Epoch: [120][217/233]	Loss 0.0006 (0.0061)	
training:	Epoch: [120][218/233]	Loss 0.0006 (0.0061)	
training:	Epoch: [120][219/233]	Loss 0.0010 (0.0061)	
training:	Epoch: [120][220/233]	Loss 0.0006 (0.0061)	
training:	Epoch: [120][221/233]	Loss 0.0006 (0.0060)	
training:	Epoch: [120][222/233]	Loss 0.0008 (0.0060)	
training:	Epoch: [120][223/233]	Loss 0.0006 (0.0060)	
training:	Epoch: [120][224/233]	Loss 0.0013 (0.0060)	
training:	Epoch: [120][225/233]	Loss 0.0007 (0.0059)	
training:	Epoch: [120][226/233]	Loss 0.0011 (0.0059)	
training:	Epoch: [120][227/233]	Loss 0.0006 (0.0059)	
training:	Epoch: [120][228/233]	Loss 0.0008 (0.0059)	
training:	Epoch: [120][229/233]	Loss 0.0032 (0.0059)	
training:	Epoch: [120][230/233]	Loss 0.0006 (0.0058)	
training:	Epoch: [120][231/233]	Loss 0.0013 (0.0058)	
training:	Epoch: [120][232/233]	Loss 0.0007 (0.0058)	
training:	Epoch: [120][233/233]	Loss 0.0025 (0.0058)	
Training:	 Loss: 0.0058

Training:	 ACC: 0.9997 0.9997 0.9997 0.9997
Validation:	 ACC: 0.7941 0.7919 0.7467 0.8416
Validation:	 Best_BACC: 0.8162 0.8154 0.7998 0.8326
Validation:	 Loss: 1.1827
Pretraining:	Epoch 121/200
----------
training:	Epoch: [121][1/233]	Loss 0.0066 (0.0066)	
training:	Epoch: [121][2/233]	Loss 0.1308 (0.0687)	
training:	Epoch: [121][3/233]	Loss 0.0008 (0.0461)	
training:	Epoch: [121][4/233]	Loss 0.0008 (0.0348)	
training:	Epoch: [121][5/233]	Loss 0.0066 (0.0291)	
training:	Epoch: [121][6/233]	Loss 0.0033 (0.0248)	
training:	Epoch: [121][7/233]	Loss 0.0005 (0.0213)	
training:	Epoch: [121][8/233]	Loss 0.0006 (0.0188)	
training:	Epoch: [121][9/233]	Loss 0.0021 (0.0169)	
training:	Epoch: [121][10/233]	Loss 0.0007 (0.0153)	
training:	Epoch: [121][11/233]	Loss 0.0006 (0.0139)	
training:	Epoch: [121][12/233]	Loss 0.0006 (0.0128)	
training:	Epoch: [121][13/233]	Loss 0.0107 (0.0127)	
training:	Epoch: [121][14/233]	Loss 0.0006 (0.0118)	
training:	Epoch: [121][15/233]	Loss 0.2338 (0.0266)	
training:	Epoch: [121][16/233]	Loss 0.0007 (0.0250)	
training:	Epoch: [121][17/233]	Loss 0.0009 (0.0236)	
training:	Epoch: [121][18/233]	Loss 0.0102 (0.0228)	
training:	Epoch: [121][19/233]	Loss 0.0007 (0.0217)	
training:	Epoch: [121][20/233]	Loss 0.0009 (0.0206)	
training:	Epoch: [121][21/233]	Loss 0.0013 (0.0197)	
training:	Epoch: [121][22/233]	Loss 0.0007 (0.0188)	
training:	Epoch: [121][23/233]	Loss 0.0011 (0.0181)	
training:	Epoch: [121][24/233]	Loss 0.0007 (0.0173)	
training:	Epoch: [121][25/233]	Loss 0.0006 (0.0167)	
training:	Epoch: [121][26/233]	Loss 0.0010 (0.0161)	
training:	Epoch: [121][27/233]	Loss 0.0007 (0.0155)	
training:	Epoch: [121][28/233]	Loss 0.0028 (0.0150)	
training:	Epoch: [121][29/233]	Loss 0.0006 (0.0146)	
training:	Epoch: [121][30/233]	Loss 0.0006 (0.0141)	
training:	Epoch: [121][31/233]	Loss 0.0006 (0.0137)	
training:	Epoch: [121][32/233]	Loss 0.0009 (0.0133)	
training:	Epoch: [121][33/233]	Loss 0.0008 (0.0129)	
training:	Epoch: [121][34/233]	Loss 0.0007 (0.0125)	
training:	Epoch: [121][35/233]	Loss 0.0007 (0.0122)	
training:	Epoch: [121][36/233]	Loss 0.0005 (0.0119)	
training:	Epoch: [121][37/233]	Loss 0.0007 (0.0116)	
training:	Epoch: [121][38/233]	Loss 0.0023 (0.0113)	
training:	Epoch: [121][39/233]	Loss 0.0010 (0.0110)	
training:	Epoch: [121][40/233]	Loss 0.0033 (0.0109)	
training:	Epoch: [121][41/233]	Loss 0.0008 (0.0106)	
training:	Epoch: [121][42/233]	Loss 0.0005 (0.0104)	
training:	Epoch: [121][43/233]	Loss 0.0007 (0.0101)	
training:	Epoch: [121][44/233]	Loss 0.0007 (0.0099)	
training:	Epoch: [121][45/233]	Loss 0.0093 (0.0099)	
training:	Epoch: [121][46/233]	Loss 0.0008 (0.0097)	
training:	Epoch: [121][47/233]	Loss 0.0007 (0.0095)	
training:	Epoch: [121][48/233]	Loss 0.0819 (0.0110)	
training:	Epoch: [121][49/233]	Loss 0.0008 (0.0108)	
training:	Epoch: [121][50/233]	Loss 0.0006 (0.0106)	
training:	Epoch: [121][51/233]	Loss 0.0009 (0.0104)	
training:	Epoch: [121][52/233]	Loss 0.0007 (0.0102)	
training:	Epoch: [121][53/233]	Loss 0.0006 (0.0101)	
training:	Epoch: [121][54/233]	Loss 0.0006 (0.0099)	
training:	Epoch: [121][55/233]	Loss 0.0008 (0.0097)	
training:	Epoch: [121][56/233]	Loss 0.0007 (0.0096)	
training:	Epoch: [121][57/233]	Loss 0.0043 (0.0095)	
training:	Epoch: [121][58/233]	Loss 0.0005 (0.0093)	
training:	Epoch: [121][59/233]	Loss 0.0034 (0.0092)	
training:	Epoch: [121][60/233]	Loss 0.0182 (0.0094)	
training:	Epoch: [121][61/233]	Loss 0.0007 (0.0092)	
training:	Epoch: [121][62/233]	Loss 0.0021 (0.0091)	
training:	Epoch: [121][63/233]	Loss 0.0023 (0.0090)	
training:	Epoch: [121][64/233]	Loss 0.0054 (0.0089)	
training:	Epoch: [121][65/233]	Loss 0.0007 (0.0088)	
training:	Epoch: [121][66/233]	Loss 0.0028 (0.0087)	
training:	Epoch: [121][67/233]	Loss 0.0006 (0.0086)	
training:	Epoch: [121][68/233]	Loss 0.0006 (0.0085)	
training:	Epoch: [121][69/233]	Loss 0.0007 (0.0084)	
training:	Epoch: [121][70/233]	Loss 0.0007 (0.0083)	
training:	Epoch: [121][71/233]	Loss 0.0011 (0.0082)	
training:	Epoch: [121][72/233]	Loss 0.0006 (0.0081)	
training:	Epoch: [121][73/233]	Loss 0.0007 (0.0079)	
training:	Epoch: [121][74/233]	Loss 0.0012 (0.0079)	
training:	Epoch: [121][75/233]	Loss 0.0016 (0.0078)	
training:	Epoch: [121][76/233]	Loss 0.0010 (0.0077)	
training:	Epoch: [121][77/233]	Loss 0.0006 (0.0076)	
training:	Epoch: [121][78/233]	Loss 0.0008 (0.0075)	
training:	Epoch: [121][79/233]	Loss 0.0302 (0.0078)	
training:	Epoch: [121][80/233]	Loss 0.0008 (0.0077)	
training:	Epoch: [121][81/233]	Loss 0.0009 (0.0076)	
training:	Epoch: [121][82/233]	Loss 0.0006 (0.0075)	
training:	Epoch: [121][83/233]	Loss 0.0008 (0.0075)	
training:	Epoch: [121][84/233]	Loss 0.0007 (0.0074)	
training:	Epoch: [121][85/233]	Loss 0.0017 (0.0073)	
training:	Epoch: [121][86/233]	Loss 0.0012 (0.0072)	
training:	Epoch: [121][87/233]	Loss 0.0010 (0.0072)	
training:	Epoch: [121][88/233]	Loss 0.0065 (0.0072)	
training:	Epoch: [121][89/233]	Loss 0.0008 (0.0071)	
training:	Epoch: [121][90/233]	Loss 0.0173 (0.0072)	
training:	Epoch: [121][91/233]	Loss 0.0017 (0.0071)	
training:	Epoch: [121][92/233]	Loss 0.0007 (0.0071)	
training:	Epoch: [121][93/233]	Loss 0.0008 (0.0070)	
training:	Epoch: [121][94/233]	Loss 0.0016 (0.0069)	
training:	Epoch: [121][95/233]	Loss 0.0013 (0.0069)	
training:	Epoch: [121][96/233]	Loss 0.0007 (0.0068)	
training:	Epoch: [121][97/233]	Loss 0.0014 (0.0068)	
training:	Epoch: [121][98/233]	Loss 0.0007 (0.0067)	
training:	Epoch: [121][99/233]	Loss 0.0013 (0.0066)	
training:	Epoch: [121][100/233]	Loss 0.0017 (0.0066)	
training:	Epoch: [121][101/233]	Loss 0.0008 (0.0065)	
training:	Epoch: [121][102/233]	Loss 0.0007 (0.0065)	
training:	Epoch: [121][103/233]	Loss 0.0008 (0.0064)	
training:	Epoch: [121][104/233]	Loss 0.0007 (0.0064)	
training:	Epoch: [121][105/233]	Loss 0.0009 (0.0063)	
training:	Epoch: [121][106/233]	Loss 0.0012 (0.0063)	
training:	Epoch: [121][107/233]	Loss 0.0006 (0.0062)	
training:	Epoch: [121][108/233]	Loss 0.0007 (0.0062)	
training:	Epoch: [121][109/233]	Loss 0.0017 (0.0061)	
training:	Epoch: [121][110/233]	Loss 0.0008 (0.0061)	
training:	Epoch: [121][111/233]	Loss 0.0008 (0.0060)	
training:	Epoch: [121][112/233]	Loss 0.0012 (0.0060)	
training:	Epoch: [121][113/233]	Loss 0.0008 (0.0059)	
training:	Epoch: [121][114/233]	Loss 0.0008 (0.0059)	
training:	Epoch: [121][115/233]	Loss 0.0009 (0.0059)	
training:	Epoch: [121][116/233]	Loss 0.0007 (0.0058)	
training:	Epoch: [121][117/233]	Loss 0.0006 (0.0058)	
training:	Epoch: [121][118/233]	Loss 0.0008 (0.0057)	
training:	Epoch: [121][119/233]	Loss 0.0009 (0.0057)	
training:	Epoch: [121][120/233]	Loss 0.0177 (0.0058)	
training:	Epoch: [121][121/233]	Loss 0.0006 (0.0057)	
training:	Epoch: [121][122/233]	Loss 0.0941 (0.0065)	
training:	Epoch: [121][123/233]	Loss 0.0010 (0.0064)	
training:	Epoch: [121][124/233]	Loss 0.0006 (0.0064)	
training:	Epoch: [121][125/233]	Loss 0.0015 (0.0063)	
training:	Epoch: [121][126/233]	Loss 0.0007 (0.0063)	
training:	Epoch: [121][127/233]	Loss 0.0008 (0.0062)	
training:	Epoch: [121][128/233]	Loss 0.0007 (0.0062)	
training:	Epoch: [121][129/233]	Loss 0.0006 (0.0062)	
training:	Epoch: [121][130/233]	Loss 0.0019 (0.0061)	
training:	Epoch: [121][131/233]	Loss 0.0007 (0.0061)	
training:	Epoch: [121][132/233]	Loss 0.0010 (0.0060)	
training:	Epoch: [121][133/233]	Loss 0.0012 (0.0060)	
training:	Epoch: [121][134/233]	Loss 0.0011 (0.0060)	
training:	Epoch: [121][135/233]	Loss 0.0009 (0.0059)	
training:	Epoch: [121][136/233]	Loss 0.0337 (0.0061)	
training:	Epoch: [121][137/233]	Loss 0.0007 (0.0061)	
training:	Epoch: [121][138/233]	Loss 0.0006 (0.0061)	
training:	Epoch: [121][139/233]	Loss 0.0005 (0.0060)	
training:	Epoch: [121][140/233]	Loss 0.0007 (0.0060)	
training:	Epoch: [121][141/233]	Loss 0.0007 (0.0059)	
training:	Epoch: [121][142/233]	Loss 0.0006 (0.0059)	
training:	Epoch: [121][143/233]	Loss 0.0010 (0.0059)	
training:	Epoch: [121][144/233]	Loss 0.0008 (0.0058)	
training:	Epoch: [121][145/233]	Loss 0.0008 (0.0058)	
training:	Epoch: [121][146/233]	Loss 0.0006 (0.0058)	
training:	Epoch: [121][147/233]	Loss 0.0452 (0.0060)	
training:	Epoch: [121][148/233]	Loss 0.0015 (0.0060)	
training:	Epoch: [121][149/233]	Loss 0.0007 (0.0060)	
training:	Epoch: [121][150/233]	Loss 0.0007 (0.0059)	
training:	Epoch: [121][151/233]	Loss 0.0008 (0.0059)	
training:	Epoch: [121][152/233]	Loss 0.0005 (0.0059)	
training:	Epoch: [121][153/233]	Loss 0.0042 (0.0059)	
training:	Epoch: [121][154/233]	Loss 0.0007 (0.0058)	
training:	Epoch: [121][155/233]	Loss 0.0005 (0.0058)	
training:	Epoch: [121][156/233]	Loss 0.0007 (0.0058)	
training:	Epoch: [121][157/233]	Loss 0.0006 (0.0057)	
training:	Epoch: [121][158/233]	Loss 0.0008 (0.0057)	
training:	Epoch: [121][159/233]	Loss 0.0014 (0.0057)	
training:	Epoch: [121][160/233]	Loss 0.0010 (0.0056)	
training:	Epoch: [121][161/233]	Loss 0.0028 (0.0056)	
training:	Epoch: [121][162/233]	Loss 0.0007 (0.0056)	
training:	Epoch: [121][163/233]	Loss 0.0007 (0.0056)	
training:	Epoch: [121][164/233]	Loss 0.0012 (0.0055)	
training:	Epoch: [121][165/233]	Loss 0.0006 (0.0055)	
training:	Epoch: [121][166/233]	Loss 0.0005 (0.0055)	
training:	Epoch: [121][167/233]	Loss 0.1097 (0.0061)	
training:	Epoch: [121][168/233]	Loss 0.0007 (0.0061)	
training:	Epoch: [121][169/233]	Loss 0.0005 (0.0060)	
training:	Epoch: [121][170/233]	Loss 0.0007 (0.0060)	
training:	Epoch: [121][171/233]	Loss 0.0006 (0.0060)	
training:	Epoch: [121][172/233]	Loss 0.0016 (0.0059)	
training:	Epoch: [121][173/233]	Loss 0.0010 (0.0059)	
training:	Epoch: [121][174/233]	Loss 0.0007 (0.0059)	
training:	Epoch: [121][175/233]	Loss 0.0282 (0.0060)	
training:	Epoch: [121][176/233]	Loss 0.0006 (0.0060)	
training:	Epoch: [121][177/233]	Loss 0.0008 (0.0059)	
training:	Epoch: [121][178/233]	Loss 0.0047 (0.0059)	
training:	Epoch: [121][179/233]	Loss 0.0013 (0.0059)	
training:	Epoch: [121][180/233]	Loss 0.0203 (0.0060)	
training:	Epoch: [121][181/233]	Loss 0.1292 (0.0067)	
training:	Epoch: [121][182/233]	Loss 0.0480 (0.0069)	
training:	Epoch: [121][183/233]	Loss 0.0484 (0.0071)	
training:	Epoch: [121][184/233]	Loss 0.0009 (0.0071)	
training:	Epoch: [121][185/233]	Loss 0.0021 (0.0071)	
training:	Epoch: [121][186/233]	Loss 0.0005 (0.0070)	
training:	Epoch: [121][187/233]	Loss 0.0006 (0.0070)	
training:	Epoch: [121][188/233]	Loss 0.0007 (0.0070)	
training:	Epoch: [121][189/233]	Loss 0.0006 (0.0069)	
training:	Epoch: [121][190/233]	Loss 0.0009 (0.0069)	
training:	Epoch: [121][191/233]	Loss 0.0035 (0.0069)	
training:	Epoch: [121][192/233]	Loss 0.0063 (0.0069)	
training:	Epoch: [121][193/233]	Loss 0.0006 (0.0068)	
training:	Epoch: [121][194/233]	Loss 0.0362 (0.0070)	
training:	Epoch: [121][195/233]	Loss 0.0014 (0.0070)	
training:	Epoch: [121][196/233]	Loss 0.0011 (0.0069)	
training:	Epoch: [121][197/233]	Loss 0.0015 (0.0069)	
training:	Epoch: [121][198/233]	Loss 0.0006 (0.0069)	
training:	Epoch: [121][199/233]	Loss 0.0007 (0.0068)	
training:	Epoch: [121][200/233]	Loss 0.0006 (0.0068)	
training:	Epoch: [121][201/233]	Loss 0.0007 (0.0068)	
training:	Epoch: [121][202/233]	Loss 0.0007 (0.0068)	
training:	Epoch: [121][203/233]	Loss 0.0006 (0.0067)	
training:	Epoch: [121][204/233]	Loss 0.0006 (0.0067)	
training:	Epoch: [121][205/233]	Loss 0.0056 (0.0067)	
training:	Epoch: [121][206/233]	Loss 0.0007 (0.0067)	
training:	Epoch: [121][207/233]	Loss 0.0034 (0.0066)	
training:	Epoch: [121][208/233]	Loss 0.0081 (0.0067)	
training:	Epoch: [121][209/233]	Loss 0.0008 (0.0066)	
training:	Epoch: [121][210/233]	Loss 0.0013 (0.0066)	
training:	Epoch: [121][211/233]	Loss 0.0007 (0.0066)	
training:	Epoch: [121][212/233]	Loss 0.0009 (0.0065)	
training:	Epoch: [121][213/233]	Loss 0.0126 (0.0066)	
training:	Epoch: [121][214/233]	Loss 0.0008 (0.0065)	
training:	Epoch: [121][215/233]	Loss 0.0006 (0.0065)	
training:	Epoch: [121][216/233]	Loss 0.0194 (0.0066)	
training:	Epoch: [121][217/233]	Loss 0.0007 (0.0066)	
training:	Epoch: [121][218/233]	Loss 0.0007 (0.0065)	
training:	Epoch: [121][219/233]	Loss 0.0006 (0.0065)	
training:	Epoch: [121][220/233]	Loss 0.0007 (0.0065)	
training:	Epoch: [121][221/233]	Loss 0.0006 (0.0064)	
training:	Epoch: [121][222/233]	Loss 0.0006 (0.0064)	
training:	Epoch: [121][223/233]	Loss 0.0007 (0.0064)	
training:	Epoch: [121][224/233]	Loss 0.0006 (0.0064)	
training:	Epoch: [121][225/233]	Loss 0.0010 (0.0063)	
training:	Epoch: [121][226/233]	Loss 0.0013 (0.0063)	
training:	Epoch: [121][227/233]	Loss 0.0006 (0.0063)	
training:	Epoch: [121][228/233]	Loss 0.0005 (0.0063)	
training:	Epoch: [121][229/233]	Loss 0.0005 (0.0062)	
training:	Epoch: [121][230/233]	Loss 0.0007 (0.0062)	
training:	Epoch: [121][231/233]	Loss 0.0006 (0.0062)	
training:	Epoch: [121][232/233]	Loss 0.0011 (0.0062)	
training:	Epoch: [121][233/233]	Loss 0.0009 (0.0062)	
Training:	 Loss: 0.0061

Training:	 ACC: 0.9997 0.9997 0.9997 0.9997
Validation:	 ACC: 0.7928 0.7919 0.7732 0.8124
Validation:	 Best_BACC: 0.8162 0.8154 0.7998 0.8326
Validation:	 Loss: 1.1730
Pretraining:	Epoch 122/200
----------
training:	Epoch: [122][1/233]	Loss 0.0010 (0.0010)	
training:	Epoch: [122][2/233]	Loss 0.0006 (0.0008)	
training:	Epoch: [122][3/233]	Loss 0.0005 (0.0007)	
training:	Epoch: [122][4/233]	Loss 0.0007 (0.0007)	
training:	Epoch: [122][5/233]	Loss 0.0006 (0.0007)	
training:	Epoch: [122][6/233]	Loss 0.0007 (0.0007)	
training:	Epoch: [122][7/233]	Loss 0.0006 (0.0007)	
training:	Epoch: [122][8/233]	Loss 0.0006 (0.0007)	
training:	Epoch: [122][9/233]	Loss 0.0011 (0.0007)	
training:	Epoch: [122][10/233]	Loss 0.0008 (0.0007)	
training:	Epoch: [122][11/233]	Loss 0.0010 (0.0007)	
training:	Epoch: [122][12/233]	Loss 0.0006 (0.0007)	
training:	Epoch: [122][13/233]	Loss 0.0037 (0.0010)	
training:	Epoch: [122][14/233]	Loss 0.0006 (0.0009)	
training:	Epoch: [122][15/233]	Loss 0.0007 (0.0009)	
training:	Epoch: [122][16/233]	Loss 0.0020 (0.0010)	
training:	Epoch: [122][17/233]	Loss 0.0017 (0.0010)	
training:	Epoch: [122][18/233]	Loss 0.0006 (0.0010)	
training:	Epoch: [122][19/233]	Loss 0.0007 (0.0010)	
training:	Epoch: [122][20/233]	Loss 0.0010 (0.0010)	
training:	Epoch: [122][21/233]	Loss 0.0289 (0.0023)	
training:	Epoch: [122][22/233]	Loss 0.0009 (0.0022)	
training:	Epoch: [122][23/233]	Loss 0.1870 (0.0103)	
training:	Epoch: [122][24/233]	Loss 0.0144 (0.0105)	
training:	Epoch: [122][25/233]	Loss 0.0006 (0.0101)	
training:	Epoch: [122][26/233]	Loss 0.0006 (0.0097)	
training:	Epoch: [122][27/233]	Loss 0.0006 (0.0094)	
training:	Epoch: [122][28/233]	Loss 0.0005 (0.0090)	
training:	Epoch: [122][29/233]	Loss 0.0006 (0.0088)	
training:	Epoch: [122][30/233]	Loss 0.0005 (0.0085)	
training:	Epoch: [122][31/233]	Loss 0.0006 (0.0082)	
training:	Epoch: [122][32/233]	Loss 0.0008 (0.0080)	
training:	Epoch: [122][33/233]	Loss 0.0005 (0.0078)	
training:	Epoch: [122][34/233]	Loss 0.0520 (0.0091)	
training:	Epoch: [122][35/233]	Loss 0.0011 (0.0088)	
training:	Epoch: [122][36/233]	Loss 0.0010 (0.0086)	
training:	Epoch: [122][37/233]	Loss 0.0029 (0.0085)	
training:	Epoch: [122][38/233]	Loss 0.0007 (0.0083)	
training:	Epoch: [122][39/233]	Loss 0.0025 (0.0081)	
training:	Epoch: [122][40/233]	Loss 0.0008 (0.0079)	
training:	Epoch: [122][41/233]	Loss 0.0009 (0.0078)	
training:	Epoch: [122][42/233]	Loss 0.0012 (0.0076)	
training:	Epoch: [122][43/233]	Loss 0.0010 (0.0075)	
training:	Epoch: [122][44/233]	Loss 0.0012 (0.0073)	
training:	Epoch: [122][45/233]	Loss 0.0014 (0.0072)	
training:	Epoch: [122][46/233]	Loss 0.0012 (0.0070)	
training:	Epoch: [122][47/233]	Loss 0.0229 (0.0074)	
training:	Epoch: [122][48/233]	Loss 0.0007 (0.0072)	
training:	Epoch: [122][49/233]	Loss 0.0006 (0.0071)	
training:	Epoch: [122][50/233]	Loss 0.0008 (0.0070)	
training:	Epoch: [122][51/233]	Loss 0.0007 (0.0069)	
training:	Epoch: [122][52/233]	Loss 0.0016 (0.0068)	
training:	Epoch: [122][53/233]	Loss 0.0007 (0.0066)	
training:	Epoch: [122][54/233]	Loss 0.0007 (0.0065)	
training:	Epoch: [122][55/233]	Loss 0.0007 (0.0064)	
training:	Epoch: [122][56/233]	Loss 0.0005 (0.0063)	
training:	Epoch: [122][57/233]	Loss 0.0006 (0.0062)	
training:	Epoch: [122][58/233]	Loss 0.0007 (0.0061)	
training:	Epoch: [122][59/233]	Loss 0.0082 (0.0062)	
training:	Epoch: [122][60/233]	Loss 0.0007 (0.0061)	
training:	Epoch: [122][61/233]	Loss 0.0006 (0.0060)	
training:	Epoch: [122][62/233]	Loss 0.0005 (0.0059)	
training:	Epoch: [122][63/233]	Loss 0.0130 (0.0060)	
training:	Epoch: [122][64/233]	Loss 0.0016 (0.0059)	
training:	Epoch: [122][65/233]	Loss 0.0006 (0.0059)	
training:	Epoch: [122][66/233]	Loss 0.0006 (0.0058)	
training:	Epoch: [122][67/233]	Loss 0.0039 (0.0057)	
training:	Epoch: [122][68/233]	Loss 0.0035 (0.0057)	
training:	Epoch: [122][69/233]	Loss 0.1180 (0.0073)	
training:	Epoch: [122][70/233]	Loss 0.0010 (0.0073)	
training:	Epoch: [122][71/233]	Loss 0.0005 (0.0072)	
training:	Epoch: [122][72/233]	Loss 0.0007 (0.0071)	
training:	Epoch: [122][73/233]	Loss 0.0007 (0.0070)	
training:	Epoch: [122][74/233]	Loss 0.0008 (0.0069)	
training:	Epoch: [122][75/233]	Loss 0.0008 (0.0068)	
training:	Epoch: [122][76/233]	Loss 0.0007 (0.0067)	
training:	Epoch: [122][77/233]	Loss 0.0006 (0.0067)	
training:	Epoch: [122][78/233]	Loss 0.0012 (0.0066)	
training:	Epoch: [122][79/233]	Loss 0.0005 (0.0065)	
training:	Epoch: [122][80/233]	Loss 0.0009 (0.0064)	
training:	Epoch: [122][81/233]	Loss 0.0005 (0.0064)	
training:	Epoch: [122][82/233]	Loss 0.0009 (0.0063)	
training:	Epoch: [122][83/233]	Loss 0.0011 (0.0062)	
training:	Epoch: [122][84/233]	Loss 0.0006 (0.0062)	
training:	Epoch: [122][85/233]	Loss 0.0005 (0.0061)	
training:	Epoch: [122][86/233]	Loss 0.0006 (0.0060)	
training:	Epoch: [122][87/233]	Loss 0.0005 (0.0060)	
training:	Epoch: [122][88/233]	Loss 0.0013 (0.0059)	
training:	Epoch: [122][89/233]	Loss 0.0008 (0.0059)	
training:	Epoch: [122][90/233]	Loss 0.0005 (0.0058)	
training:	Epoch: [122][91/233]	Loss 0.0007 (0.0058)	
training:	Epoch: [122][92/233]	Loss 0.0008 (0.0057)	
training:	Epoch: [122][93/233]	Loss 0.0006 (0.0056)	
training:	Epoch: [122][94/233]	Loss 0.0011 (0.0056)	
training:	Epoch: [122][95/233]	Loss 0.0008 (0.0055)	
training:	Epoch: [122][96/233]	Loss 0.0006 (0.0055)	
training:	Epoch: [122][97/233]	Loss 0.0007 (0.0054)	
training:	Epoch: [122][98/233]	Loss 0.0006 (0.0054)	
training:	Epoch: [122][99/233]	Loss 0.0010 (0.0053)	
training:	Epoch: [122][100/233]	Loss 0.0008 (0.0053)	
training:	Epoch: [122][101/233]	Loss 0.0005 (0.0053)	
training:	Epoch: [122][102/233]	Loss 0.0007 (0.0052)	
training:	Epoch: [122][103/233]	Loss 0.0006 (0.0052)	
training:	Epoch: [122][104/233]	Loss 0.0008 (0.0051)	
training:	Epoch: [122][105/233]	Loss 0.0007 (0.0051)	
training:	Epoch: [122][106/233]	Loss 0.0018 (0.0051)	
training:	Epoch: [122][107/233]	Loss 0.0006 (0.0050)	
training:	Epoch: [122][108/233]	Loss 0.0014 (0.0050)	
training:	Epoch: [122][109/233]	Loss 0.0320 (0.0052)	
training:	Epoch: [122][110/233]	Loss 0.0005 (0.0052)	
training:	Epoch: [122][111/233]	Loss 0.0008 (0.0051)	
training:	Epoch: [122][112/233]	Loss 0.0006 (0.0051)	
training:	Epoch: [122][113/233]	Loss 0.0010 (0.0051)	
training:	Epoch: [122][114/233]	Loss 0.0031 (0.0050)	
training:	Epoch: [122][115/233]	Loss 0.0005 (0.0050)	
training:	Epoch: [122][116/233]	Loss 0.0038 (0.0050)	
training:	Epoch: [122][117/233]	Loss 0.0007 (0.0050)	
training:	Epoch: [122][118/233]	Loss 0.0009 (0.0049)	
training:	Epoch: [122][119/233]	Loss 0.0041 (0.0049)	
training:	Epoch: [122][120/233]	Loss 0.0007 (0.0049)	
training:	Epoch: [122][121/233]	Loss 0.1291 (0.0059)	
training:	Epoch: [122][122/233]	Loss 0.0007 (0.0059)	
training:	Epoch: [122][123/233]	Loss 0.0009 (0.0058)	
training:	Epoch: [122][124/233]	Loss 0.0016 (0.0058)	
training:	Epoch: [122][125/233]	Loss 0.0007 (0.0058)	
training:	Epoch: [122][126/233]	Loss 0.0009 (0.0057)	
training:	Epoch: [122][127/233]	Loss 0.0007 (0.0057)	
training:	Epoch: [122][128/233]	Loss 0.0014 (0.0056)	
training:	Epoch: [122][129/233]	Loss 0.0024 (0.0056)	
training:	Epoch: [122][130/233]	Loss 0.0009 (0.0056)	
training:	Epoch: [122][131/233]	Loss 0.0005 (0.0055)	
training:	Epoch: [122][132/233]	Loss 0.0006 (0.0055)	
training:	Epoch: [122][133/233]	Loss 0.0309 (0.0057)	
training:	Epoch: [122][134/233]	Loss 0.0406 (0.0060)	
training:	Epoch: [122][135/233]	Loss 0.0041 (0.0059)	
training:	Epoch: [122][136/233]	Loss 0.0855 (0.0065)	
training:	Epoch: [122][137/233]	Loss 0.0006 (0.0065)	
training:	Epoch: [122][138/233]	Loss 0.0007 (0.0064)	
training:	Epoch: [122][139/233]	Loss 0.0007 (0.0064)	
training:	Epoch: [122][140/233]	Loss 0.0021 (0.0064)	
training:	Epoch: [122][141/233]	Loss 0.0006 (0.0063)	
training:	Epoch: [122][142/233]	Loss 0.2375 (0.0080)	
training:	Epoch: [122][143/233]	Loss 0.0123 (0.0080)	
training:	Epoch: [122][144/233]	Loss 0.0007 (0.0079)	
training:	Epoch: [122][145/233]	Loss 0.0007 (0.0079)	
training:	Epoch: [122][146/233]	Loss 0.0155 (0.0079)	
training:	Epoch: [122][147/233]	Loss 0.0012 (0.0079)	
training:	Epoch: [122][148/233]	Loss 0.0008 (0.0078)	
training:	Epoch: [122][149/233]	Loss 0.0012 (0.0078)	
training:	Epoch: [122][150/233]	Loss 0.0108 (0.0078)	
training:	Epoch: [122][151/233]	Loss 0.0032 (0.0078)	
training:	Epoch: [122][152/233]	Loss 0.0015 (0.0077)	
training:	Epoch: [122][153/233]	Loss 0.0007 (0.0077)	
training:	Epoch: [122][154/233]	Loss 0.0009 (0.0077)	
training:	Epoch: [122][155/233]	Loss 0.0006 (0.0076)	
training:	Epoch: [122][156/233]	Loss 0.0472 (0.0079)	
training:	Epoch: [122][157/233]	Loss 0.0012 (0.0078)	
training:	Epoch: [122][158/233]	Loss 0.0006 (0.0078)	
training:	Epoch: [122][159/233]	Loss 0.0399 (0.0080)	
training:	Epoch: [122][160/233]	Loss 0.0006 (0.0079)	
training:	Epoch: [122][161/233]	Loss 0.0085 (0.0079)	
training:	Epoch: [122][162/233]	Loss 0.1041 (0.0085)	
training:	Epoch: [122][163/233]	Loss 0.0006 (0.0085)	
training:	Epoch: [122][164/233]	Loss 0.0044 (0.0085)	
training:	Epoch: [122][165/233]	Loss 0.0062 (0.0084)	
training:	Epoch: [122][166/233]	Loss 0.0006 (0.0084)	
training:	Epoch: [122][167/233]	Loss 0.0007 (0.0083)	
training:	Epoch: [122][168/233]	Loss 0.0056 (0.0083)	
training:	Epoch: [122][169/233]	Loss 0.0013 (0.0083)	
training:	Epoch: [122][170/233]	Loss 0.0120 (0.0083)	
training:	Epoch: [122][171/233]	Loss 0.0005 (0.0083)	
training:	Epoch: [122][172/233]	Loss 0.0012 (0.0082)	
training:	Epoch: [122][173/233]	Loss 0.0006 (0.0082)	
training:	Epoch: [122][174/233]	Loss 0.0006 (0.0081)	
training:	Epoch: [122][175/233]	Loss 0.0009 (0.0081)	
training:	Epoch: [122][176/233]	Loss 0.0011 (0.0081)	
training:	Epoch: [122][177/233]	Loss 0.0008 (0.0080)	
training:	Epoch: [122][178/233]	Loss 0.0216 (0.0081)	
training:	Epoch: [122][179/233]	Loss 0.0009 (0.0081)	
training:	Epoch: [122][180/233]	Loss 0.0008 (0.0080)	
training:	Epoch: [122][181/233]	Loss 0.0006 (0.0080)	
training:	Epoch: [122][182/233]	Loss 0.0008 (0.0079)	
training:	Epoch: [122][183/233]	Loss 0.0007 (0.0079)	
training:	Epoch: [122][184/233]	Loss 0.0007 (0.0079)	
training:	Epoch: [122][185/233]	Loss 0.0007 (0.0078)	
training:	Epoch: [122][186/233]	Loss 0.0009 (0.0078)	
training:	Epoch: [122][187/233]	Loss 0.0040 (0.0078)	
training:	Epoch: [122][188/233]	Loss 0.0009 (0.0077)	
training:	Epoch: [122][189/233]	Loss 0.0027 (0.0077)	
training:	Epoch: [122][190/233]	Loss 0.0009 (0.0077)	
training:	Epoch: [122][191/233]	Loss 0.0006 (0.0076)	
training:	Epoch: [122][192/233]	Loss 0.0007 (0.0076)	
training:	Epoch: [122][193/233]	Loss 0.0007 (0.0075)	
training:	Epoch: [122][194/233]	Loss 0.0007 (0.0075)	
training:	Epoch: [122][195/233]	Loss 0.0005 (0.0075)	
training:	Epoch: [122][196/233]	Loss 0.0007 (0.0074)	
training:	Epoch: [122][197/233]	Loss 0.0005 (0.0074)	
training:	Epoch: [122][198/233]	Loss 0.0015 (0.0074)	
training:	Epoch: [122][199/233]	Loss 0.0006 (0.0073)	
training:	Epoch: [122][200/233]	Loss 0.0008 (0.0073)	
training:	Epoch: [122][201/233]	Loss 0.0007 (0.0073)	
training:	Epoch: [122][202/233]	Loss 0.0007 (0.0072)	
training:	Epoch: [122][203/233]	Loss 0.0008 (0.0072)	
training:	Epoch: [122][204/233]	Loss 0.0025 (0.0072)	
training:	Epoch: [122][205/233]	Loss 0.0006 (0.0072)	
training:	Epoch: [122][206/233]	Loss 0.0007 (0.0071)	
training:	Epoch: [122][207/233]	Loss 0.0009 (0.0071)	
training:	Epoch: [122][208/233]	Loss 0.0010 (0.0071)	
training:	Epoch: [122][209/233]	Loss 0.0008 (0.0070)	
training:	Epoch: [122][210/233]	Loss 0.1345 (0.0076)	
training:	Epoch: [122][211/233]	Loss 0.0005 (0.0076)	
training:	Epoch: [122][212/233]	Loss 0.0026 (0.0076)	
training:	Epoch: [122][213/233]	Loss 0.0019 (0.0076)	
training:	Epoch: [122][214/233]	Loss 0.0005 (0.0075)	
training:	Epoch: [122][215/233]	Loss 0.0011 (0.0075)	
training:	Epoch: [122][216/233]	Loss 0.0008 (0.0075)	
training:	Epoch: [122][217/233]	Loss 0.0008 (0.0074)	
training:	Epoch: [122][218/233]	Loss 0.0009 (0.0074)	
training:	Epoch: [122][219/233]	Loss 0.0043 (0.0074)	
training:	Epoch: [122][220/233]	Loss 0.0016 (0.0074)	
training:	Epoch: [122][221/233]	Loss 0.0042 (0.0074)	
training:	Epoch: [122][222/233]	Loss 0.0007 (0.0073)	
training:	Epoch: [122][223/233]	Loss 0.0006 (0.0073)	
training:	Epoch: [122][224/233]	Loss 0.0012 (0.0073)	
training:	Epoch: [122][225/233]	Loss 0.0012 (0.0072)	
training:	Epoch: [122][226/233]	Loss 0.0007 (0.0072)	
training:	Epoch: [122][227/233]	Loss 0.0011 (0.0072)	
training:	Epoch: [122][228/233]	Loss 0.0005 (0.0072)	
training:	Epoch: [122][229/233]	Loss 0.0006 (0.0071)	
training:	Epoch: [122][230/233]	Loss 0.0019 (0.0071)	
training:	Epoch: [122][231/233]	Loss 0.0006 (0.0071)	
training:	Epoch: [122][232/233]	Loss 0.0009 (0.0070)	
training:	Epoch: [122][233/233]	Loss 0.0083 (0.0071)	
Training:	 Loss: 0.0070

Training:	 ACC: 0.9997 0.9997 0.9997 0.9997
Validation:	 ACC: 0.7986 0.7972 0.7692 0.8281
Validation:	 Best_BACC: 0.8162 0.8154 0.7998 0.8326
Validation:	 Loss: 1.1597
Pretraining:	Epoch 123/200
----------
training:	Epoch: [123][1/233]	Loss 0.0006 (0.0006)	
training:	Epoch: [123][2/233]	Loss 0.0007 (0.0007)	
training:	Epoch: [123][3/233]	Loss 0.0007 (0.0007)	
training:	Epoch: [123][4/233]	Loss 0.0006 (0.0006)	
training:	Epoch: [123][5/233]	Loss 0.0013 (0.0008)	
training:	Epoch: [123][6/233]	Loss 0.0008 (0.0008)	
training:	Epoch: [123][7/233]	Loss 0.0194 (0.0034)	
training:	Epoch: [123][8/233]	Loss 0.0006 (0.0031)	
training:	Epoch: [123][9/233]	Loss 0.0006 (0.0028)	
training:	Epoch: [123][10/233]	Loss 0.0010 (0.0026)	
training:	Epoch: [123][11/233]	Loss 0.0009 (0.0025)	
training:	Epoch: [123][12/233]	Loss 0.0005 (0.0023)	
training:	Epoch: [123][13/233]	Loss 0.0006 (0.0022)	
training:	Epoch: [123][14/233]	Loss 0.0011 (0.0021)	
training:	Epoch: [123][15/233]	Loss 0.0006 (0.0020)	
training:	Epoch: [123][16/233]	Loss 0.0013 (0.0020)	
training:	Epoch: [123][17/233]	Loss 0.0035 (0.0020)	
training:	Epoch: [123][18/233]	Loss 0.0006 (0.0020)	
training:	Epoch: [123][19/233]	Loss 0.0010 (0.0019)	
training:	Epoch: [123][20/233]	Loss 0.0007 (0.0019)	
training:	Epoch: [123][21/233]	Loss 0.0006 (0.0018)	
training:	Epoch: [123][22/233]	Loss 0.0011 (0.0018)	
training:	Epoch: [123][23/233]	Loss 0.0006 (0.0017)	
training:	Epoch: [123][24/233]	Loss 0.0012 (0.0017)	
training:	Epoch: [123][25/233]	Loss 0.0440 (0.0034)	
training:	Epoch: [123][26/233]	Loss 0.0033 (0.0034)	
training:	Epoch: [123][27/233]	Loss 0.0008 (0.0033)	
training:	Epoch: [123][28/233]	Loss 0.0006 (0.0032)	
training:	Epoch: [123][29/233]	Loss 0.0007 (0.0031)	
training:	Epoch: [123][30/233]	Loss 0.0009 (0.0030)	
training:	Epoch: [123][31/233]	Loss 0.0433 (0.0043)	
training:	Epoch: [123][32/233]	Loss 0.0199 (0.0048)	
training:	Epoch: [123][33/233]	Loss 0.0062 (0.0049)	
training:	Epoch: [123][34/233]	Loss 0.0008 (0.0047)	
training:	Epoch: [123][35/233]	Loss 0.0020 (0.0047)	
training:	Epoch: [123][36/233]	Loss 0.0027 (0.0046)	
training:	Epoch: [123][37/233]	Loss 0.0005 (0.0045)	
training:	Epoch: [123][38/233]	Loss 0.0006 (0.0044)	
training:	Epoch: [123][39/233]	Loss 0.0006 (0.0043)	
training:	Epoch: [123][40/233]	Loss 0.0146 (0.0046)	
training:	Epoch: [123][41/233]	Loss 0.0009 (0.0045)	
training:	Epoch: [123][42/233]	Loss 0.0005 (0.0044)	
training:	Epoch: [123][43/233]	Loss 0.0978 (0.0065)	
training:	Epoch: [123][44/233]	Loss 0.0006 (0.0064)	
training:	Epoch: [123][45/233]	Loss 0.0010 (0.0063)	
training:	Epoch: [123][46/233]	Loss 0.0007 (0.0062)	
training:	Epoch: [123][47/233]	Loss 0.0008 (0.0060)	
training:	Epoch: [123][48/233]	Loss 0.0005 (0.0059)	
training:	Epoch: [123][49/233]	Loss 0.0007 (0.0058)	
training:	Epoch: [123][50/233]	Loss 0.0015 (0.0057)	
training:	Epoch: [123][51/233]	Loss 0.0006 (0.0056)	
training:	Epoch: [123][52/233]	Loss 0.0007 (0.0055)	
training:	Epoch: [123][53/233]	Loss 0.0006 (0.0055)	
training:	Epoch: [123][54/233]	Loss 0.0012 (0.0054)	
training:	Epoch: [123][55/233]	Loss 0.0006 (0.0053)	
training:	Epoch: [123][56/233]	Loss 0.0012 (0.0052)	
training:	Epoch: [123][57/233]	Loss 0.0015 (0.0051)	
training:	Epoch: [123][58/233]	Loss 0.0007 (0.0051)	
training:	Epoch: [123][59/233]	Loss 0.0006 (0.0050)	
training:	Epoch: [123][60/233]	Loss 0.0008 (0.0049)	
training:	Epoch: [123][61/233]	Loss 0.0007 (0.0049)	
training:	Epoch: [123][62/233]	Loss 0.0007 (0.0048)	
training:	Epoch: [123][63/233]	Loss 0.0006 (0.0047)	
training:	Epoch: [123][64/233]	Loss 0.0007 (0.0047)	
training:	Epoch: [123][65/233]	Loss 0.0009 (0.0046)	
training:	Epoch: [123][66/233]	Loss 0.0014 (0.0046)	
training:	Epoch: [123][67/233]	Loss 0.0009 (0.0045)	
training:	Epoch: [123][68/233]	Loss 0.0007 (0.0044)	
training:	Epoch: [123][69/233]	Loss 0.0008 (0.0044)	
training:	Epoch: [123][70/233]	Loss 0.0011 (0.0043)	
training:	Epoch: [123][71/233]	Loss 0.0008 (0.0043)	
training:	Epoch: [123][72/233]	Loss 0.0010 (0.0042)	
training:	Epoch: [123][73/233]	Loss 0.2377 (0.0074)	
training:	Epoch: [123][74/233]	Loss 0.0008 (0.0074)	
training:	Epoch: [123][75/233]	Loss 0.0013 (0.0073)	
training:	Epoch: [123][76/233]	Loss 0.0006 (0.0072)	
training:	Epoch: [123][77/233]	Loss 0.0006 (0.0071)	
training:	Epoch: [123][78/233]	Loss 0.0007 (0.0070)	
training:	Epoch: [123][79/233]	Loss 0.0007 (0.0069)	
training:	Epoch: [123][80/233]	Loss 0.0007 (0.0069)	
training:	Epoch: [123][81/233]	Loss 0.0006 (0.0068)	
training:	Epoch: [123][82/233]	Loss 0.0005 (0.0067)	
training:	Epoch: [123][83/233]	Loss 0.0006 (0.0066)	
training:	Epoch: [123][84/233]	Loss 0.0006 (0.0066)	
training:	Epoch: [123][85/233]	Loss 0.0006 (0.0065)	
training:	Epoch: [123][86/233]	Loss 0.0009 (0.0064)	
training:	Epoch: [123][87/233]	Loss 0.0007 (0.0064)	
training:	Epoch: [123][88/233]	Loss 0.0016 (0.0063)	
training:	Epoch: [123][89/233]	Loss 0.1148 (0.0075)	
training:	Epoch: [123][90/233]	Loss 0.0007 (0.0075)	
training:	Epoch: [123][91/233]	Loss 0.0006 (0.0074)	
training:	Epoch: [123][92/233]	Loss 0.0006 (0.0073)	
training:	Epoch: [123][93/233]	Loss 0.0006 (0.0072)	
training:	Epoch: [123][94/233]	Loss 0.0010 (0.0072)	
training:	Epoch: [123][95/233]	Loss 0.0007 (0.0071)	
training:	Epoch: [123][96/233]	Loss 0.0009 (0.0070)	
training:	Epoch: [123][97/233]	Loss 0.0007 (0.0070)	
training:	Epoch: [123][98/233]	Loss 0.0075 (0.0070)	
training:	Epoch: [123][99/233]	Loss 0.0005 (0.0069)	
training:	Epoch: [123][100/233]	Loss 0.0007 (0.0068)	
training:	Epoch: [123][101/233]	Loss 0.0006 (0.0068)	
training:	Epoch: [123][102/233]	Loss 0.0006 (0.0067)	
training:	Epoch: [123][103/233]	Loss 0.0008 (0.0067)	
training:	Epoch: [123][104/233]	Loss 0.0005 (0.0066)	
training:	Epoch: [123][105/233]	Loss 0.0013 (0.0066)	
training:	Epoch: [123][106/233]	Loss 0.1115 (0.0075)	
training:	Epoch: [123][107/233]	Loss 0.0019 (0.0075)	
training:	Epoch: [123][108/233]	Loss 0.0010 (0.0074)	
training:	Epoch: [123][109/233]	Loss 0.0006 (0.0074)	
training:	Epoch: [123][110/233]	Loss 0.1773 (0.0089)	
training:	Epoch: [123][111/233]	Loss 0.0011 (0.0088)	
training:	Epoch: [123][112/233]	Loss 0.0007 (0.0088)	
training:	Epoch: [123][113/233]	Loss 0.0008 (0.0087)	
training:	Epoch: [123][114/233]	Loss 0.0011 (0.0086)	
training:	Epoch: [123][115/233]	Loss 0.0007 (0.0086)	
training:	Epoch: [123][116/233]	Loss 0.0020 (0.0085)	
training:	Epoch: [123][117/233]	Loss 0.0016 (0.0084)	
training:	Epoch: [123][118/233]	Loss 0.0006 (0.0084)	
training:	Epoch: [123][119/233]	Loss 0.0005 (0.0083)	
training:	Epoch: [123][120/233]	Loss 0.0127 (0.0084)	
training:	Epoch: [123][121/233]	Loss 0.0021 (0.0083)	
training:	Epoch: [123][122/233]	Loss 0.0008 (0.0082)	
training:	Epoch: [123][123/233]	Loss 0.0110 (0.0083)	
training:	Epoch: [123][124/233]	Loss 0.0007 (0.0082)	
training:	Epoch: [123][125/233]	Loss 0.0011 (0.0081)	
training:	Epoch: [123][126/233]	Loss 0.0008 (0.0081)	
training:	Epoch: [123][127/233]	Loss 0.0007 (0.0080)	
training:	Epoch: [123][128/233]	Loss 0.0009 (0.0080)	
training:	Epoch: [123][129/233]	Loss 0.0014 (0.0079)	
training:	Epoch: [123][130/233]	Loss 0.0006 (0.0079)	
training:	Epoch: [123][131/233]	Loss 0.0006 (0.0078)	
training:	Epoch: [123][132/233]	Loss 0.0074 (0.0078)	
training:	Epoch: [123][133/233]	Loss 0.0007 (0.0078)	
training:	Epoch: [123][134/233]	Loss 0.0302 (0.0079)	
training:	Epoch: [123][135/233]	Loss 0.0020 (0.0079)	
training:	Epoch: [123][136/233]	Loss 0.0007 (0.0078)	
training:	Epoch: [123][137/233]	Loss 0.0601 (0.0082)	
training:	Epoch: [123][138/233]	Loss 0.0010 (0.0082)	
training:	Epoch: [123][139/233]	Loss 0.0060 (0.0081)	
training:	Epoch: [123][140/233]	Loss 0.0009 (0.0081)	
training:	Epoch: [123][141/233]	Loss 0.0008 (0.0080)	
training:	Epoch: [123][142/233]	Loss 0.0007 (0.0080)	
training:	Epoch: [123][143/233]	Loss 0.0964 (0.0086)	
training:	Epoch: [123][144/233]	Loss 0.0006 (0.0085)	
training:	Epoch: [123][145/233]	Loss 0.0006 (0.0085)	
training:	Epoch: [123][146/233]	Loss 0.0008 (0.0084)	
training:	Epoch: [123][147/233]	Loss 0.0007 (0.0084)	
training:	Epoch: [123][148/233]	Loss 0.0006 (0.0083)	
training:	Epoch: [123][149/233]	Loss 0.0008 (0.0083)	
training:	Epoch: [123][150/233]	Loss 0.0007 (0.0082)	
training:	Epoch: [123][151/233]	Loss 0.0005 (0.0082)	
training:	Epoch: [123][152/233]	Loss 0.0009 (0.0081)	
training:	Epoch: [123][153/233]	Loss 0.0020 (0.0081)	
training:	Epoch: [123][154/233]	Loss 0.0009 (0.0080)	
training:	Epoch: [123][155/233]	Loss 0.0010 (0.0080)	
training:	Epoch: [123][156/233]	Loss 0.0006 (0.0080)	
training:	Epoch: [123][157/233]	Loss 0.0006 (0.0079)	
training:	Epoch: [123][158/233]	Loss 0.0007 (0.0079)	
training:	Epoch: [123][159/233]	Loss 0.0028 (0.0078)	
training:	Epoch: [123][160/233]	Loss 0.0008 (0.0078)	
training:	Epoch: [123][161/233]	Loss 0.0013 (0.0077)	
training:	Epoch: [123][162/233]	Loss 0.0006 (0.0077)	
training:	Epoch: [123][163/233]	Loss 0.0009 (0.0077)	
training:	Epoch: [123][164/233]	Loss 0.0037 (0.0076)	
training:	Epoch: [123][165/233]	Loss 0.0006 (0.0076)	
training:	Epoch: [123][166/233]	Loss 0.0009 (0.0075)	
training:	Epoch: [123][167/233]	Loss 0.0010 (0.0075)	
training:	Epoch: [123][168/233]	Loss 0.0008 (0.0075)	
training:	Epoch: [123][169/233]	Loss 0.0006 (0.0074)	
training:	Epoch: [123][170/233]	Loss 0.0008 (0.0074)	
training:	Epoch: [123][171/233]	Loss 0.0047 (0.0074)	
training:	Epoch: [123][172/233]	Loss 0.0005 (0.0073)	
training:	Epoch: [123][173/233]	Loss 0.0011 (0.0073)	
training:	Epoch: [123][174/233]	Loss 0.0069 (0.0073)	
training:	Epoch: [123][175/233]	Loss 0.0007 (0.0073)	
training:	Epoch: [123][176/233]	Loss 0.0223 (0.0073)	
training:	Epoch: [123][177/233]	Loss 0.0171 (0.0074)	
training:	Epoch: [123][178/233]	Loss 0.0006 (0.0074)	
training:	Epoch: [123][179/233]	Loss 0.0007 (0.0073)	
training:	Epoch: [123][180/233]	Loss 0.0007 (0.0073)	
training:	Epoch: [123][181/233]	Loss 0.0007 (0.0073)	
training:	Epoch: [123][182/233]	Loss 0.0006 (0.0072)	
training:	Epoch: [123][183/233]	Loss 0.0015 (0.0072)	
training:	Epoch: [123][184/233]	Loss 0.0005 (0.0071)	
training:	Epoch: [123][185/233]	Loss 0.0006 (0.0071)	
training:	Epoch: [123][186/233]	Loss 0.0008 (0.0071)	
training:	Epoch: [123][187/233]	Loss 0.0007 (0.0070)	
training:	Epoch: [123][188/233]	Loss 0.0010 (0.0070)	
training:	Epoch: [123][189/233]	Loss 0.0007 (0.0070)	
training:	Epoch: [123][190/233]	Loss 0.0007 (0.0069)	
training:	Epoch: [123][191/233]	Loss 0.0006 (0.0069)	
training:	Epoch: [123][192/233]	Loss 0.0013 (0.0069)	
training:	Epoch: [123][193/233]	Loss 0.0014 (0.0069)	
training:	Epoch: [123][194/233]	Loss 0.0020 (0.0068)	
training:	Epoch: [123][195/233]	Loss 0.0020 (0.0068)	
training:	Epoch: [123][196/233]	Loss 0.0039 (0.0068)	
training:	Epoch: [123][197/233]	Loss 0.0230 (0.0069)	
training:	Epoch: [123][198/233]	Loss 0.0008 (0.0068)	
training:	Epoch: [123][199/233]	Loss 0.0007 (0.0068)	
training:	Epoch: [123][200/233]	Loss 0.0005 (0.0068)	
training:	Epoch: [123][201/233]	Loss 0.0008 (0.0067)	
training:	Epoch: [123][202/233]	Loss 0.0007 (0.0067)	
training:	Epoch: [123][203/233]	Loss 0.0554 (0.0070)	
training:	Epoch: [123][204/233]	Loss 0.0006 (0.0069)	
training:	Epoch: [123][205/233]	Loss 0.0007 (0.0069)	
training:	Epoch: [123][206/233]	Loss 0.0006 (0.0069)	
training:	Epoch: [123][207/233]	Loss 0.0006 (0.0068)	
training:	Epoch: [123][208/233]	Loss 0.0005 (0.0068)	
training:	Epoch: [123][209/233]	Loss 0.0010 (0.0068)	
training:	Epoch: [123][210/233]	Loss 0.0007 (0.0068)	
training:	Epoch: [123][211/233]	Loss 0.0006 (0.0067)	
training:	Epoch: [123][212/233]	Loss 0.0009 (0.0067)	
training:	Epoch: [123][213/233]	Loss 0.0011 (0.0067)	
training:	Epoch: [123][214/233]	Loss 0.0008 (0.0066)	
training:	Epoch: [123][215/233]	Loss 0.0048 (0.0066)	
training:	Epoch: [123][216/233]	Loss 0.0006 (0.0066)	
training:	Epoch: [123][217/233]	Loss 0.0006 (0.0066)	
training:	Epoch: [123][218/233]	Loss 0.0006 (0.0065)	
training:	Epoch: [123][219/233]	Loss 0.0006 (0.0065)	
training:	Epoch: [123][220/233]	Loss 0.0006 (0.0065)	
training:	Epoch: [123][221/233]	Loss 0.0218 (0.0066)	
training:	Epoch: [123][222/233]	Loss 0.0008 (0.0065)	
training:	Epoch: [123][223/233]	Loss 0.0014 (0.0065)	
training:	Epoch: [123][224/233]	Loss 0.0012 (0.0065)	
training:	Epoch: [123][225/233]	Loss 0.0008 (0.0065)	
training:	Epoch: [123][226/233]	Loss 0.0005 (0.0064)	
training:	Epoch: [123][227/233]	Loss 0.0013 (0.0064)	
training:	Epoch: [123][228/233]	Loss 0.0006 (0.0064)	
training:	Epoch: [123][229/233]	Loss 0.0007 (0.0064)	
training:	Epoch: [123][230/233]	Loss 0.0005 (0.0063)	
training:	Epoch: [123][231/233]	Loss 0.1007 (0.0067)	
training:	Epoch: [123][232/233]	Loss 0.0007 (0.0067)	
training:	Epoch: [123][233/233]	Loss 0.0005 (0.0067)	
Training:	 Loss: 0.0067

Training:	 ACC: 0.9997 0.9997 0.9997 0.9997
Validation:	 ACC: 0.7926 0.7919 0.7763 0.8090
Validation:	 Best_BACC: 0.8162 0.8154 0.7998 0.8326
Validation:	 Loss: 1.1445
Pretraining:	Epoch 124/200
----------
training:	Epoch: [124][1/233]	Loss 0.0006 (0.0006)	
training:	Epoch: [124][2/233]	Loss 0.0018 (0.0012)	
training:	Epoch: [124][3/233]	Loss 0.0008 (0.0011)	
training:	Epoch: [124][4/233]	Loss 0.0018 (0.0013)	
training:	Epoch: [124][5/233]	Loss 0.0010 (0.0012)	
training:	Epoch: [124][6/233]	Loss 0.0007 (0.0011)	
training:	Epoch: [124][7/233]	Loss 0.0010 (0.0011)	
training:	Epoch: [124][8/233]	Loss 0.0015 (0.0012)	
training:	Epoch: [124][9/233]	Loss 0.0008 (0.0011)	
training:	Epoch: [124][10/233]	Loss 0.0007 (0.0011)	
training:	Epoch: [124][11/233]	Loss 0.0006 (0.0010)	
training:	Epoch: [124][12/233]	Loss 0.0012 (0.0010)	
training:	Epoch: [124][13/233]	Loss 0.0007 (0.0010)	
training:	Epoch: [124][14/233]	Loss 0.0012 (0.0010)	
training:	Epoch: [124][15/233]	Loss 0.0005 (0.0010)	
training:	Epoch: [124][16/233]	Loss 0.0007 (0.0010)	
training:	Epoch: [124][17/233]	Loss 0.0006 (0.0010)	
training:	Epoch: [124][18/233]	Loss 0.0260 (0.0024)	
training:	Epoch: [124][19/233]	Loss 0.0007 (0.0023)	
training:	Epoch: [124][20/233]	Loss 0.0006 (0.0022)	
training:	Epoch: [124][21/233]	Loss 0.0005 (0.0021)	
training:	Epoch: [124][22/233]	Loss 0.0024 (0.0021)	
training:	Epoch: [124][23/233]	Loss 0.0010 (0.0021)	
training:	Epoch: [124][24/233]	Loss 0.0006 (0.0020)	
training:	Epoch: [124][25/233]	Loss 0.0012 (0.0020)	
training:	Epoch: [124][26/233]	Loss 0.0043 (0.0021)	
training:	Epoch: [124][27/233]	Loss 0.0009 (0.0020)	
training:	Epoch: [124][28/233]	Loss 0.0006 (0.0020)	
training:	Epoch: [124][29/233]	Loss 0.0010 (0.0019)	
training:	Epoch: [124][30/233]	Loss 0.0018 (0.0019)	
training:	Epoch: [124][31/233]	Loss 0.0006 (0.0019)	
training:	Epoch: [124][32/233]	Loss 0.0005 (0.0019)	
training:	Epoch: [124][33/233]	Loss 0.0009 (0.0018)	
training:	Epoch: [124][34/233]	Loss 0.0006 (0.0018)	
training:	Epoch: [124][35/233]	Loss 0.0013 (0.0018)	
training:	Epoch: [124][36/233]	Loss 0.0007 (0.0017)	
training:	Epoch: [124][37/233]	Loss 0.0007 (0.0017)	
training:	Epoch: [124][38/233]	Loss 0.0005 (0.0017)	
training:	Epoch: [124][39/233]	Loss 0.0006 (0.0017)	
training:	Epoch: [124][40/233]	Loss 0.0006 (0.0016)	
training:	Epoch: [124][41/233]	Loss 0.0005 (0.0016)	
training:	Epoch: [124][42/233]	Loss 0.0006 (0.0016)	
training:	Epoch: [124][43/233]	Loss 0.0020 (0.0016)	
training:	Epoch: [124][44/233]	Loss 0.0009 (0.0016)	
training:	Epoch: [124][45/233]	Loss 0.0008 (0.0016)	
training:	Epoch: [124][46/233]	Loss 0.0010 (0.0015)	
training:	Epoch: [124][47/233]	Loss 0.0007 (0.0015)	
training:	Epoch: [124][48/233]	Loss 0.0007 (0.0015)	
training:	Epoch: [124][49/233]	Loss 0.0007 (0.0015)	
training:	Epoch: [124][50/233]	Loss 0.0008 (0.0015)	
training:	Epoch: [124][51/233]	Loss 0.0797 (0.0030)	
training:	Epoch: [124][52/233]	Loss 0.0012 (0.0030)	
training:	Epoch: [124][53/233]	Loss 0.0006 (0.0029)	
training:	Epoch: [124][54/233]	Loss 0.0005 (0.0029)	
training:	Epoch: [124][55/233]	Loss 0.0009 (0.0029)	
training:	Epoch: [124][56/233]	Loss 0.0008 (0.0028)	
training:	Epoch: [124][57/233]	Loss 0.0006 (0.0028)	
training:	Epoch: [124][58/233]	Loss 0.0005 (0.0027)	
training:	Epoch: [124][59/233]	Loss 0.0006 (0.0027)	
training:	Epoch: [124][60/233]	Loss 0.0006 (0.0027)	
training:	Epoch: [124][61/233]	Loss 0.0006 (0.0026)	
training:	Epoch: [124][62/233]	Loss 0.0006 (0.0026)	
training:	Epoch: [124][63/233]	Loss 0.0022 (0.0026)	
training:	Epoch: [124][64/233]	Loss 0.0005 (0.0026)	
training:	Epoch: [124][65/233]	Loss 0.0017 (0.0025)	
training:	Epoch: [124][66/233]	Loss 0.0005 (0.0025)	
training:	Epoch: [124][67/233]	Loss 0.0050 (0.0026)	
training:	Epoch: [124][68/233]	Loss 0.0006 (0.0025)	
training:	Epoch: [124][69/233]	Loss 0.0007 (0.0025)	
training:	Epoch: [124][70/233]	Loss 0.0006 (0.0025)	
training:	Epoch: [124][71/233]	Loss 0.0008 (0.0024)	
training:	Epoch: [124][72/233]	Loss 0.0019 (0.0024)	
training:	Epoch: [124][73/233]	Loss 0.0008 (0.0024)	
training:	Epoch: [124][74/233]	Loss 0.0005 (0.0024)	
training:	Epoch: [124][75/233]	Loss 0.0005 (0.0024)	
training:	Epoch: [124][76/233]	Loss 0.0040 (0.0024)	
training:	Epoch: [124][77/233]	Loss 0.0006 (0.0024)	
training:	Epoch: [124][78/233]	Loss 0.0007 (0.0023)	
training:	Epoch: [124][79/233]	Loss 0.0005 (0.0023)	
training:	Epoch: [124][80/233]	Loss 0.0008 (0.0023)	
training:	Epoch: [124][81/233]	Loss 0.0020 (0.0023)	
training:	Epoch: [124][82/233]	Loss 0.0005 (0.0023)	
training:	Epoch: [124][83/233]	Loss 0.0010 (0.0023)	
training:	Epoch: [124][84/233]	Loss 0.0005 (0.0022)	
training:	Epoch: [124][85/233]	Loss 0.0005 (0.0022)	
training:	Epoch: [124][86/233]	Loss 0.0006 (0.0022)	
training:	Epoch: [124][87/233]	Loss 0.0005 (0.0022)	
training:	Epoch: [124][88/233]	Loss 0.0007 (0.0022)	
training:	Epoch: [124][89/233]	Loss 0.1115 (0.0034)	
training:	Epoch: [124][90/233]	Loss 0.0037 (0.0034)	
training:	Epoch: [124][91/233]	Loss 0.0028 (0.0034)	
training:	Epoch: [124][92/233]	Loss 0.0008 (0.0034)	
training:	Epoch: [124][93/233]	Loss 0.0007 (0.0033)	
training:	Epoch: [124][94/233]	Loss 0.0010 (0.0033)	
training:	Epoch: [124][95/233]	Loss 0.0007 (0.0033)	
training:	Epoch: [124][96/233]	Loss 0.0019 (0.0033)	
training:	Epoch: [124][97/233]	Loss 0.0050 (0.0033)	
training:	Epoch: [124][98/233]	Loss 0.0006 (0.0033)	
training:	Epoch: [124][99/233]	Loss 0.0006 (0.0032)	
training:	Epoch: [124][100/233]	Loss 0.0007 (0.0032)	
training:	Epoch: [124][101/233]	Loss 0.0009 (0.0032)	
training:	Epoch: [124][102/233]	Loss 0.0005 (0.0032)	
training:	Epoch: [124][103/233]	Loss 0.0005 (0.0031)	
training:	Epoch: [124][104/233]	Loss 0.0008 (0.0031)	
training:	Epoch: [124][105/233]	Loss 0.0008 (0.0031)	
training:	Epoch: [124][106/233]	Loss 0.0011 (0.0031)	
training:	Epoch: [124][107/233]	Loss 0.0012 (0.0030)	
training:	Epoch: [124][108/233]	Loss 0.0024 (0.0030)	
training:	Epoch: [124][109/233]	Loss 0.0853 (0.0038)	
training:	Epoch: [124][110/233]	Loss 0.0006 (0.0038)	
training:	Epoch: [124][111/233]	Loss 0.0006 (0.0037)	
training:	Epoch: [124][112/233]	Loss 0.0005 (0.0037)	
training:	Epoch: [124][113/233]	Loss 0.0006 (0.0037)	
training:	Epoch: [124][114/233]	Loss 0.0009 (0.0037)	
training:	Epoch: [124][115/233]	Loss 0.0029 (0.0037)	
training:	Epoch: [124][116/233]	Loss 0.0009 (0.0036)	
training:	Epoch: [124][117/233]	Loss 0.0006 (0.0036)	
training:	Epoch: [124][118/233]	Loss 0.0011 (0.0036)	
training:	Epoch: [124][119/233]	Loss 0.0005 (0.0036)	
training:	Epoch: [124][120/233]	Loss 0.2334 (0.0055)	
training:	Epoch: [124][121/233]	Loss 0.0005 (0.0054)	
training:	Epoch: [124][122/233]	Loss 0.0376 (0.0057)	
training:	Epoch: [124][123/233]	Loss 0.0007 (0.0057)	
training:	Epoch: [124][124/233]	Loss 0.0007 (0.0056)	
training:	Epoch: [124][125/233]	Loss 0.0040 (0.0056)	
training:	Epoch: [124][126/233]	Loss 0.0008 (0.0056)	
training:	Epoch: [124][127/233]	Loss 0.0007 (0.0055)	
training:	Epoch: [124][128/233]	Loss 0.0065 (0.0055)	
training:	Epoch: [124][129/233]	Loss 0.0012 (0.0055)	
training:	Epoch: [124][130/233]	Loss 0.0008 (0.0055)	
training:	Epoch: [124][131/233]	Loss 0.0005 (0.0054)	
training:	Epoch: [124][132/233]	Loss 0.0005 (0.0054)	
training:	Epoch: [124][133/233]	Loss 0.0006 (0.0054)	
training:	Epoch: [124][134/233]	Loss 0.0007 (0.0053)	
training:	Epoch: [124][135/233]	Loss 0.0007 (0.0053)	
training:	Epoch: [124][136/233]	Loss 0.0008 (0.0052)	
training:	Epoch: [124][137/233]	Loss 0.0014 (0.0052)	
training:	Epoch: [124][138/233]	Loss 0.0010 (0.0052)	
training:	Epoch: [124][139/233]	Loss 0.0007 (0.0052)	
training:	Epoch: [124][140/233]	Loss 0.0077 (0.0052)	
training:	Epoch: [124][141/233]	Loss 0.0012 (0.0051)	
training:	Epoch: [124][142/233]	Loss 0.0006 (0.0051)	
training:	Epoch: [124][143/233]	Loss 0.0008 (0.0051)	
training:	Epoch: [124][144/233]	Loss 0.0018 (0.0051)	
training:	Epoch: [124][145/233]	Loss 0.0020 (0.0050)	
training:	Epoch: [124][146/233]	Loss 0.0147 (0.0051)	
training:	Epoch: [124][147/233]	Loss 0.0010 (0.0051)	
training:	Epoch: [124][148/233]	Loss 0.0009 (0.0051)	
training:	Epoch: [124][149/233]	Loss 0.0007 (0.0050)	
training:	Epoch: [124][150/233]	Loss 0.0010 (0.0050)	
training:	Epoch: [124][151/233]	Loss 0.0008 (0.0050)	
training:	Epoch: [124][152/233]	Loss 0.0041 (0.0050)	
training:	Epoch: [124][153/233]	Loss 0.0007 (0.0049)	
training:	Epoch: [124][154/233]	Loss 0.2319 (0.0064)	
training:	Epoch: [124][155/233]	Loss 0.0006 (0.0064)	
training:	Epoch: [124][156/233]	Loss 0.0047 (0.0064)	
training:	Epoch: [124][157/233]	Loss 0.0006 (0.0063)	
training:	Epoch: [124][158/233]	Loss 0.0005 (0.0063)	
training:	Epoch: [124][159/233]	Loss 0.0006 (0.0063)	
training:	Epoch: [124][160/233]	Loss 0.0008 (0.0062)	
training:	Epoch: [124][161/233]	Loss 0.0033 (0.0062)	
training:	Epoch: [124][162/233]	Loss 0.0767 (0.0066)	
training:	Epoch: [124][163/233]	Loss 0.0008 (0.0066)	
training:	Epoch: [124][164/233]	Loss 0.0006 (0.0066)	
training:	Epoch: [124][165/233]	Loss 0.0008 (0.0065)	
training:	Epoch: [124][166/233]	Loss 0.0006 (0.0065)	
training:	Epoch: [124][167/233]	Loss 0.0007 (0.0065)	
training:	Epoch: [124][168/233]	Loss 0.0007 (0.0064)	
training:	Epoch: [124][169/233]	Loss 0.0010 (0.0064)	
training:	Epoch: [124][170/233]	Loss 0.0012 (0.0064)	
training:	Epoch: [124][171/233]	Loss 0.0023 (0.0063)	
training:	Epoch: [124][172/233]	Loss 0.0013 (0.0063)	
training:	Epoch: [124][173/233]	Loss 0.0038 (0.0063)	
training:	Epoch: [124][174/233]	Loss 0.0005 (0.0063)	
training:	Epoch: [124][175/233]	Loss 0.0005 (0.0062)	
training:	Epoch: [124][176/233]	Loss 0.0008 (0.0062)	
training:	Epoch: [124][177/233]	Loss 0.0078 (0.0062)	
training:	Epoch: [124][178/233]	Loss 0.0007 (0.0062)	
training:	Epoch: [124][179/233]	Loss 0.0006 (0.0061)	
training:	Epoch: [124][180/233]	Loss 0.0015 (0.0061)	
training:	Epoch: [124][181/233]	Loss 0.0006 (0.0061)	
training:	Epoch: [124][182/233]	Loss 0.0012 (0.0061)	
training:	Epoch: [124][183/233]	Loss 0.0006 (0.0060)	
training:	Epoch: [124][184/233]	Loss 0.0006 (0.0060)	
training:	Epoch: [124][185/233]	Loss 0.0008 (0.0060)	
training:	Epoch: [124][186/233]	Loss 0.0005 (0.0059)	
training:	Epoch: [124][187/233]	Loss 0.0028 (0.0059)	
training:	Epoch: [124][188/233]	Loss 0.0009 (0.0059)	
training:	Epoch: [124][189/233]	Loss 0.0409 (0.0061)	
training:	Epoch: [124][190/233]	Loss 0.0006 (0.0061)	
training:	Epoch: [124][191/233]	Loss 0.0006 (0.0060)	
training:	Epoch: [124][192/233]	Loss 0.0007 (0.0060)	
training:	Epoch: [124][193/233]	Loss 0.0010 (0.0060)	
training:	Epoch: [124][194/233]	Loss 0.0012 (0.0059)	
training:	Epoch: [124][195/233]	Loss 0.0006 (0.0059)	
training:	Epoch: [124][196/233]	Loss 0.0007 (0.0059)	
training:	Epoch: [124][197/233]	Loss 0.1610 (0.0067)	
training:	Epoch: [124][198/233]	Loss 0.0005 (0.0067)	
training:	Epoch: [124][199/233]	Loss 0.0008 (0.0066)	
training:	Epoch: [124][200/233]	Loss 0.0010 (0.0066)	
training:	Epoch: [124][201/233]	Loss 0.0008 (0.0066)	
training:	Epoch: [124][202/233]	Loss 0.0008 (0.0065)	
training:	Epoch: [124][203/233]	Loss 0.0006 (0.0065)	
training:	Epoch: [124][204/233]	Loss 0.0009 (0.0065)	
training:	Epoch: [124][205/233]	Loss 0.0254 (0.0066)	
training:	Epoch: [124][206/233]	Loss 0.0028 (0.0066)	
training:	Epoch: [124][207/233]	Loss 0.0007 (0.0065)	
training:	Epoch: [124][208/233]	Loss 0.0333 (0.0067)	
training:	Epoch: [124][209/233]	Loss 0.0007 (0.0066)	
training:	Epoch: [124][210/233]	Loss 0.0007 (0.0066)	
training:	Epoch: [124][211/233]	Loss 0.0009 (0.0066)	
training:	Epoch: [124][212/233]	Loss 0.0006 (0.0065)	
training:	Epoch: [124][213/233]	Loss 0.0006 (0.0065)	
training:	Epoch: [124][214/233]	Loss 0.0005 (0.0065)	
training:	Epoch: [124][215/233]	Loss 0.0006 (0.0065)	
training:	Epoch: [124][216/233]	Loss 0.0006 (0.0064)	
training:	Epoch: [124][217/233]	Loss 0.0006 (0.0064)	
training:	Epoch: [124][218/233]	Loss 0.0007 (0.0064)	
training:	Epoch: [124][219/233]	Loss 0.0006 (0.0064)	
training:	Epoch: [124][220/233]	Loss 0.0009 (0.0063)	
training:	Epoch: [124][221/233]	Loss 0.0006 (0.0063)	
training:	Epoch: [124][222/233]	Loss 0.0006 (0.0063)	
training:	Epoch: [124][223/233]	Loss 0.0006 (0.0062)	
training:	Epoch: [124][224/233]	Loss 0.0006 (0.0062)	
training:	Epoch: [124][225/233]	Loss 0.0017 (0.0062)	
training:	Epoch: [124][226/233]	Loss 0.0046 (0.0062)	
training:	Epoch: [124][227/233]	Loss 0.0026 (0.0062)	
training:	Epoch: [124][228/233]	Loss 0.0006 (0.0062)	
training:	Epoch: [124][229/233]	Loss 0.0007 (0.0061)	
training:	Epoch: [124][230/233]	Loss 0.0006 (0.0061)	
training:	Epoch: [124][231/233]	Loss 0.0009 (0.0061)	
training:	Epoch: [124][232/233]	Loss 0.0012 (0.0061)	
training:	Epoch: [124][233/233]	Loss 0.0007 (0.0060)	
Training:	 Loss: 0.0060

Training:	 ACC: 0.9999 0.9999 1.0000 0.9997
Validation:	 ACC: 0.7949 0.7951 0.7988 0.7910
Validation:	 Best_BACC: 0.8162 0.8154 0.7998 0.8326
Validation:	 Loss: 1.1267
Pretraining:	Epoch 125/200
----------
training:	Epoch: [125][1/233]	Loss 0.0006 (0.0006)	
training:	Epoch: [125][2/233]	Loss 0.0014 (0.0010)	
training:	Epoch: [125][3/233]	Loss 0.0010 (0.0010)	
training:	Epoch: [125][4/233]	Loss 0.0006 (0.0009)	
training:	Epoch: [125][5/233]	Loss 0.0009 (0.0009)	
training:	Epoch: [125][6/233]	Loss 0.0009 (0.0009)	
training:	Epoch: [125][7/233]	Loss 0.0007 (0.0009)	
training:	Epoch: [125][8/233]	Loss 0.0007 (0.0008)	
training:	Epoch: [125][9/233]	Loss 0.0025 (0.0010)	
training:	Epoch: [125][10/233]	Loss 0.0007 (0.0010)	
training:	Epoch: [125][11/233]	Loss 0.0027 (0.0011)	
training:	Epoch: [125][12/233]	Loss 0.0020 (0.0012)	
training:	Epoch: [125][13/233]	Loss 0.0008 (0.0012)	
training:	Epoch: [125][14/233]	Loss 0.0021 (0.0012)	
training:	Epoch: [125][15/233]	Loss 0.0005 (0.0012)	
training:	Epoch: [125][16/233]	Loss 0.0011 (0.0012)	
training:	Epoch: [125][17/233]	Loss 0.0005 (0.0012)	
training:	Epoch: [125][18/233]	Loss 0.2205 (0.0133)	
training:	Epoch: [125][19/233]	Loss 0.0006 (0.0127)	
training:	Epoch: [125][20/233]	Loss 0.0009 (0.0121)	
training:	Epoch: [125][21/233]	Loss 0.1984 (0.0209)	
training:	Epoch: [125][22/233]	Loss 0.0020 (0.0201)	
training:	Epoch: [125][23/233]	Loss 0.0007 (0.0192)	
training:	Epoch: [125][24/233]	Loss 0.0013 (0.0185)	
training:	Epoch: [125][25/233]	Loss 0.0006 (0.0178)	
training:	Epoch: [125][26/233]	Loss 0.0013 (0.0171)	
training:	Epoch: [125][27/233]	Loss 0.0012 (0.0166)	
training:	Epoch: [125][28/233]	Loss 0.0014 (0.0160)	
training:	Epoch: [125][29/233]	Loss 0.0009 (0.0155)	
training:	Epoch: [125][30/233]	Loss 0.0031 (0.0151)	
training:	Epoch: [125][31/233]	Loss 0.0006 (0.0146)	
training:	Epoch: [125][32/233]	Loss 0.0058 (0.0143)	
training:	Epoch: [125][33/233]	Loss 0.0010 (0.0139)	
training:	Epoch: [125][34/233]	Loss 0.0006 (0.0135)	
training:	Epoch: [125][35/233]	Loss 0.0005 (0.0132)	
training:	Epoch: [125][36/233]	Loss 0.0011 (0.0128)	
training:	Epoch: [125][37/233]	Loss 0.0007 (0.0125)	
training:	Epoch: [125][38/233]	Loss 0.0007 (0.0122)	
training:	Epoch: [125][39/233]	Loss 0.0006 (0.0119)	
training:	Epoch: [125][40/233]	Loss 0.0006 (0.0116)	
training:	Epoch: [125][41/233]	Loss 0.0035 (0.0114)	
training:	Epoch: [125][42/233]	Loss 0.0009 (0.0112)	
training:	Epoch: [125][43/233]	Loss 0.0009 (0.0109)	
training:	Epoch: [125][44/233]	Loss 0.0011 (0.0107)	
training:	Epoch: [125][45/233]	Loss 0.0019 (0.0105)	
training:	Epoch: [125][46/233]	Loss 0.0025 (0.0103)	
training:	Epoch: [125][47/233]	Loss 0.0899 (0.0120)	
training:	Epoch: [125][48/233]	Loss 0.0010 (0.0118)	
training:	Epoch: [125][49/233]	Loss 0.2283 (0.0162)	
training:	Epoch: [125][50/233]	Loss 0.0038 (0.0160)	
training:	Epoch: [125][51/233]	Loss 0.0005 (0.0157)	
training:	Epoch: [125][52/233]	Loss 0.0006 (0.0154)	
training:	Epoch: [125][53/233]	Loss 0.0009 (0.0151)	
training:	Epoch: [125][54/233]	Loss 0.0005 (0.0148)	
training:	Epoch: [125][55/233]	Loss 0.0006 (0.0146)	
training:	Epoch: [125][56/233]	Loss 0.0023 (0.0144)	
training:	Epoch: [125][57/233]	Loss 0.0006 (0.0141)	
training:	Epoch: [125][58/233]	Loss 0.0008 (0.0139)	
training:	Epoch: [125][59/233]	Loss 0.0008 (0.0137)	
training:	Epoch: [125][60/233]	Loss 0.0008 (0.0135)	
training:	Epoch: [125][61/233]	Loss 0.0008 (0.0132)	
training:	Epoch: [125][62/233]	Loss 0.0009 (0.0130)	
training:	Epoch: [125][63/233]	Loss 0.0008 (0.0128)	
training:	Epoch: [125][64/233]	Loss 0.0011 (0.0127)	
training:	Epoch: [125][65/233]	Loss 0.0027 (0.0125)	
training:	Epoch: [125][66/233]	Loss 0.0023 (0.0124)	
training:	Epoch: [125][67/233]	Loss 0.0094 (0.0123)	
training:	Epoch: [125][68/233]	Loss 0.0014 (0.0122)	
training:	Epoch: [125][69/233]	Loss 0.0019 (0.0120)	
training:	Epoch: [125][70/233]	Loss 0.0015 (0.0119)	
training:	Epoch: [125][71/233]	Loss 0.0011 (0.0117)	
training:	Epoch: [125][72/233]	Loss 0.0313 (0.0120)	
training:	Epoch: [125][73/233]	Loss 0.0009 (0.0118)	
training:	Epoch: [125][74/233]	Loss 0.0165 (0.0119)	
training:	Epoch: [125][75/233]	Loss 0.0009 (0.0117)	
training:	Epoch: [125][76/233]	Loss 0.0020 (0.0116)	
training:	Epoch: [125][77/233]	Loss 0.0008 (0.0115)	
training:	Epoch: [125][78/233]	Loss 0.0012 (0.0113)	
training:	Epoch: [125][79/233]	Loss 0.0006 (0.0112)	
training:	Epoch: [125][80/233]	Loss 0.0008 (0.0111)	
training:	Epoch: [125][81/233]	Loss 0.0005 (0.0109)	
training:	Epoch: [125][82/233]	Loss 0.0007 (0.0108)	
training:	Epoch: [125][83/233]	Loss 0.0021 (0.0107)	
training:	Epoch: [125][84/233]	Loss 0.0007 (0.0106)	
training:	Epoch: [125][85/233]	Loss 0.0009 (0.0105)	
training:	Epoch: [125][86/233]	Loss 0.0007 (0.0104)	
training:	Epoch: [125][87/233]	Loss 0.0007 (0.0103)	
training:	Epoch: [125][88/233]	Loss 0.0008 (0.0101)	
training:	Epoch: [125][89/233]	Loss 0.0059 (0.0101)	
training:	Epoch: [125][90/233]	Loss 0.0007 (0.0100)	
training:	Epoch: [125][91/233]	Loss 0.0280 (0.0102)	
training:	Epoch: [125][92/233]	Loss 0.0006 (0.0101)	
training:	Epoch: [125][93/233]	Loss 0.0022 (0.0100)	
training:	Epoch: [125][94/233]	Loss 0.0006 (0.0099)	
training:	Epoch: [125][95/233]	Loss 0.0008 (0.0098)	
training:	Epoch: [125][96/233]	Loss 0.0006 (0.0097)	
training:	Epoch: [125][97/233]	Loss 0.0006 (0.0096)	
training:	Epoch: [125][98/233]	Loss 0.0007 (0.0095)	
training:	Epoch: [125][99/233]	Loss 0.0007 (0.0094)	
training:	Epoch: [125][100/233]	Loss 0.0009 (0.0094)	
training:	Epoch: [125][101/233]	Loss 0.0008 (0.0093)	
training:	Epoch: [125][102/233]	Loss 0.0005 (0.0092)	
training:	Epoch: [125][103/233]	Loss 0.0096 (0.0092)	
training:	Epoch: [125][104/233]	Loss 0.0015 (0.0091)	
training:	Epoch: [125][105/233]	Loss 0.0006 (0.0090)	
training:	Epoch: [125][106/233]	Loss 0.0006 (0.0090)	
training:	Epoch: [125][107/233]	Loss 0.0016 (0.0089)	
training:	Epoch: [125][108/233]	Loss 0.0005 (0.0088)	
training:	Epoch: [125][109/233]	Loss 0.0008 (0.0087)	
training:	Epoch: [125][110/233]	Loss 0.0007 (0.0087)	
training:	Epoch: [125][111/233]	Loss 0.0005 (0.0086)	
training:	Epoch: [125][112/233]	Loss 0.0006 (0.0085)	
training:	Epoch: [125][113/233]	Loss 0.0006 (0.0084)	
training:	Epoch: [125][114/233]	Loss 0.0007 (0.0084)	
training:	Epoch: [125][115/233]	Loss 0.0073 (0.0084)	
training:	Epoch: [125][116/233]	Loss 0.0006 (0.0083)	
training:	Epoch: [125][117/233]	Loss 0.0005 (0.0082)	
training:	Epoch: [125][118/233]	Loss 0.0015 (0.0082)	
training:	Epoch: [125][119/233]	Loss 0.0008 (0.0081)	
training:	Epoch: [125][120/233]	Loss 0.0005 (0.0080)	
training:	Epoch: [125][121/233]	Loss 0.0007 (0.0080)	
training:	Epoch: [125][122/233]	Loss 0.0006 (0.0079)	
training:	Epoch: [125][123/233]	Loss 0.0012 (0.0079)	
training:	Epoch: [125][124/233]	Loss 0.0598 (0.0083)	
training:	Epoch: [125][125/233]	Loss 0.0005 (0.0082)	
training:	Epoch: [125][126/233]	Loss 0.0006 (0.0082)	
training:	Epoch: [125][127/233]	Loss 0.0014 (0.0081)	
training:	Epoch: [125][128/233]	Loss 0.0013 (0.0081)	
training:	Epoch: [125][129/233]	Loss 0.0006 (0.0080)	
training:	Epoch: [125][130/233]	Loss 0.0019 (0.0080)	
training:	Epoch: [125][131/233]	Loss 0.0007 (0.0079)	
training:	Epoch: [125][132/233]	Loss 0.0006 (0.0078)	
training:	Epoch: [125][133/233]	Loss 0.0510 (0.0082)	
training:	Epoch: [125][134/233]	Loss 0.0008 (0.0081)	
training:	Epoch: [125][135/233]	Loss 0.0015 (0.0081)	
training:	Epoch: [125][136/233]	Loss 0.0673 (0.0085)	
training:	Epoch: [125][137/233]	Loss 0.0007 (0.0084)	
training:	Epoch: [125][138/233]	Loss 0.0943 (0.0091)	
training:	Epoch: [125][139/233]	Loss 0.0006 (0.0090)	
training:	Epoch: [125][140/233]	Loss 0.0017 (0.0090)	
training:	Epoch: [125][141/233]	Loss 0.0005 (0.0089)	
training:	Epoch: [125][142/233]	Loss 0.0018 (0.0088)	
training:	Epoch: [125][143/233]	Loss 0.0149 (0.0089)	
training:	Epoch: [125][144/233]	Loss 0.0006 (0.0088)	
training:	Epoch: [125][145/233]	Loss 0.0006 (0.0088)	
training:	Epoch: [125][146/233]	Loss 0.0006 (0.0087)	
training:	Epoch: [125][147/233]	Loss 0.0021 (0.0087)	
training:	Epoch: [125][148/233]	Loss 0.0007 (0.0086)	
training:	Epoch: [125][149/233]	Loss 0.0016 (0.0086)	
training:	Epoch: [125][150/233]	Loss 0.0007 (0.0085)	
training:	Epoch: [125][151/233]	Loss 0.0009 (0.0085)	
training:	Epoch: [125][152/233]	Loss 0.0008 (0.0084)	
training:	Epoch: [125][153/233]	Loss 0.0011 (0.0084)	
training:	Epoch: [125][154/233]	Loss 0.0011 (0.0083)	
training:	Epoch: [125][155/233]	Loss 0.0006 (0.0083)	
training:	Epoch: [125][156/233]	Loss 0.0007 (0.0082)	
training:	Epoch: [125][157/233]	Loss 0.0008 (0.0082)	
training:	Epoch: [125][158/233]	Loss 0.0010 (0.0081)	
training:	Epoch: [125][159/233]	Loss 0.0023 (0.0081)	
training:	Epoch: [125][160/233]	Loss 0.0008 (0.0080)	
training:	Epoch: [125][161/233]	Loss 0.0010 (0.0080)	
training:	Epoch: [125][162/233]	Loss 0.0008 (0.0080)	
training:	Epoch: [125][163/233]	Loss 0.0005 (0.0079)	
training:	Epoch: [125][164/233]	Loss 0.0013 (0.0079)	
training:	Epoch: [125][165/233]	Loss 0.0029 (0.0078)	
training:	Epoch: [125][166/233]	Loss 0.0007 (0.0078)	
training:	Epoch: [125][167/233]	Loss 0.0007 (0.0078)	
training:	Epoch: [125][168/233]	Loss 0.0006 (0.0077)	
training:	Epoch: [125][169/233]	Loss 0.0044 (0.0077)	
training:	Epoch: [125][170/233]	Loss 0.0005 (0.0077)	
training:	Epoch: [125][171/233]	Loss 0.0007 (0.0076)	
training:	Epoch: [125][172/233]	Loss 0.0007 (0.0076)	
training:	Epoch: [125][173/233]	Loss 0.0005 (0.0075)	
training:	Epoch: [125][174/233]	Loss 0.0278 (0.0076)	
training:	Epoch: [125][175/233]	Loss 0.0277 (0.0078)	
training:	Epoch: [125][176/233]	Loss 0.0009 (0.0077)	
training:	Epoch: [125][177/233]	Loss 0.0009 (0.0077)	
training:	Epoch: [125][178/233]	Loss 0.0006 (0.0076)	
training:	Epoch: [125][179/233]	Loss 0.0007 (0.0076)	
training:	Epoch: [125][180/233]	Loss 0.0007 (0.0076)	
training:	Epoch: [125][181/233]	Loss 0.0100 (0.0076)	
training:	Epoch: [125][182/233]	Loss 0.0010 (0.0075)	
training:	Epoch: [125][183/233]	Loss 0.0006 (0.0075)	
training:	Epoch: [125][184/233]	Loss 0.0008 (0.0075)	
training:	Epoch: [125][185/233]	Loss 0.0024 (0.0074)	
training:	Epoch: [125][186/233]	Loss 0.0008 (0.0074)	
training:	Epoch: [125][187/233]	Loss 0.0007 (0.0074)	
training:	Epoch: [125][188/233]	Loss 0.0015 (0.0073)	
training:	Epoch: [125][189/233]	Loss 0.0009 (0.0073)	
training:	Epoch: [125][190/233]	Loss 0.0007 (0.0073)	
training:	Epoch: [125][191/233]	Loss 0.0012 (0.0072)	
training:	Epoch: [125][192/233]	Loss 0.0018 (0.0072)	
training:	Epoch: [125][193/233]	Loss 0.0008 (0.0072)	
training:	Epoch: [125][194/233]	Loss 0.0005 (0.0071)	
training:	Epoch: [125][195/233]	Loss 0.0006 (0.0071)	
training:	Epoch: [125][196/233]	Loss 0.0006 (0.0071)	
training:	Epoch: [125][197/233]	Loss 0.0011 (0.0070)	
training:	Epoch: [125][198/233]	Loss 0.0006 (0.0070)	
training:	Epoch: [125][199/233]	Loss 0.0008 (0.0070)	
training:	Epoch: [125][200/233]	Loss 0.0112 (0.0070)	
training:	Epoch: [125][201/233]	Loss 0.0005 (0.0070)	
training:	Epoch: [125][202/233]	Loss 0.0008 (0.0069)	
training:	Epoch: [125][203/233]	Loss 0.0007 (0.0069)	
training:	Epoch: [125][204/233]	Loss 0.0007 (0.0069)	
training:	Epoch: [125][205/233]	Loss 0.0007 (0.0069)	
training:	Epoch: [125][206/233]	Loss 0.0018 (0.0068)	
training:	Epoch: [125][207/233]	Loss 0.0008 (0.0068)	
training:	Epoch: [125][208/233]	Loss 0.0014 (0.0068)	
training:	Epoch: [125][209/233]	Loss 0.0007 (0.0067)	
training:	Epoch: [125][210/233]	Loss 0.0115 (0.0068)	
training:	Epoch: [125][211/233]	Loss 0.0007 (0.0067)	
training:	Epoch: [125][212/233]	Loss 0.0005 (0.0067)	
training:	Epoch: [125][213/233]	Loss 0.0036 (0.0067)	
training:	Epoch: [125][214/233]	Loss 0.0006 (0.0067)	
training:	Epoch: [125][215/233]	Loss 0.0060 (0.0067)	
training:	Epoch: [125][216/233]	Loss 0.0008 (0.0066)	
training:	Epoch: [125][217/233]	Loss 0.0005 (0.0066)	
training:	Epoch: [125][218/233]	Loss 0.0044 (0.0066)	
training:	Epoch: [125][219/233]	Loss 0.0006 (0.0066)	
training:	Epoch: [125][220/233]	Loss 0.0009 (0.0065)	
training:	Epoch: [125][221/233]	Loss 0.0006 (0.0065)	
training:	Epoch: [125][222/233]	Loss 0.0006 (0.0065)	
training:	Epoch: [125][223/233]	Loss 0.0008 (0.0065)	
training:	Epoch: [125][224/233]	Loss 0.0006 (0.0064)	
training:	Epoch: [125][225/233]	Loss 0.0007 (0.0064)	
training:	Epoch: [125][226/233]	Loss 0.1468 (0.0070)	
training:	Epoch: [125][227/233]	Loss 0.0008 (0.0070)	
training:	Epoch: [125][228/233]	Loss 0.0005 (0.0070)	
training:	Epoch: [125][229/233]	Loss 0.0010 (0.0070)	
training:	Epoch: [125][230/233]	Loss 0.0006 (0.0069)	
training:	Epoch: [125][231/233]	Loss 0.0009 (0.0069)	
training:	Epoch: [125][232/233]	Loss 0.0006 (0.0069)	
training:	Epoch: [125][233/233]	Loss 0.0006 (0.0068)	
Training:	 Loss: 0.0068

Training:	 ACC: 0.9999 0.9999 1.0000 0.9997
Validation:	 ACC: 0.7907 0.7908 0.7937 0.7876
Validation:	 Best_BACC: 0.8162 0.8154 0.7998 0.8326
Validation:	 Loss: 1.1287
Pretraining:	Epoch 126/200
----------
training:	Epoch: [126][1/233]	Loss 0.0007 (0.0007)	
training:	Epoch: [126][2/233]	Loss 0.0006 (0.0007)	
training:	Epoch: [126][3/233]	Loss 0.0007 (0.0007)	
training:	Epoch: [126][4/233]	Loss 0.0288 (0.0077)	
training:	Epoch: [126][5/233]	Loss 0.0007 (0.0063)	
training:	Epoch: [126][6/233]	Loss 0.0010 (0.0054)	
training:	Epoch: [126][7/233]	Loss 0.0008 (0.0048)	
training:	Epoch: [126][8/233]	Loss 0.0005 (0.0042)	
training:	Epoch: [126][9/233]	Loss 0.0010 (0.0039)	
training:	Epoch: [126][10/233]	Loss 0.0006 (0.0036)	
training:	Epoch: [126][11/233]	Loss 0.0007 (0.0033)	
training:	Epoch: [126][12/233]	Loss 0.0006 (0.0031)	
training:	Epoch: [126][13/233]	Loss 0.0007 (0.0029)	
training:	Epoch: [126][14/233]	Loss 0.0012 (0.0028)	
training:	Epoch: [126][15/233]	Loss 0.0007 (0.0026)	
training:	Epoch: [126][16/233]	Loss 0.0066 (0.0029)	
training:	Epoch: [126][17/233]	Loss 0.0157 (0.0036)	
training:	Epoch: [126][18/233]	Loss 0.0006 (0.0035)	
training:	Epoch: [126][19/233]	Loss 0.0017 (0.0034)	
training:	Epoch: [126][20/233]	Loss 0.0006 (0.0032)	
training:	Epoch: [126][21/233]	Loss 0.0089 (0.0035)	
training:	Epoch: [126][22/233]	Loss 0.0046 (0.0036)	
training:	Epoch: [126][23/233]	Loss 0.0010 (0.0034)	
training:	Epoch: [126][24/233]	Loss 0.0019 (0.0034)	
training:	Epoch: [126][25/233]	Loss 0.0008 (0.0033)	
training:	Epoch: [126][26/233]	Loss 0.0025 (0.0032)	
training:	Epoch: [126][27/233]	Loss 0.0006 (0.0031)	
training:	Epoch: [126][28/233]	Loss 0.0006 (0.0031)	
training:	Epoch: [126][29/233]	Loss 0.0006 (0.0030)	
training:	Epoch: [126][30/233]	Loss 0.0008 (0.0029)	
training:	Epoch: [126][31/233]	Loss 0.0381 (0.0040)	
training:	Epoch: [126][32/233]	Loss 0.0006 (0.0039)	
training:	Epoch: [126][33/233]	Loss 0.0009 (0.0038)	
training:	Epoch: [126][34/233]	Loss 0.0004 (0.0037)	
training:	Epoch: [126][35/233]	Loss 0.0110 (0.0039)	
training:	Epoch: [126][36/233]	Loss 0.0044 (0.0040)	
training:	Epoch: [126][37/233]	Loss 0.0006 (0.0039)	
training:	Epoch: [126][38/233]	Loss 0.0008 (0.0038)	
training:	Epoch: [126][39/233]	Loss 0.0007 (0.0037)	
training:	Epoch: [126][40/233]	Loss 0.0014 (0.0036)	
training:	Epoch: [126][41/233]	Loss 0.0006 (0.0036)	
training:	Epoch: [126][42/233]	Loss 0.0027 (0.0035)	
training:	Epoch: [126][43/233]	Loss 0.0006 (0.0035)	
training:	Epoch: [126][44/233]	Loss 0.2328 (0.0087)	
training:	Epoch: [126][45/233]	Loss 0.0007 (0.0085)	
training:	Epoch: [126][46/233]	Loss 0.0005 (0.0083)	
training:	Epoch: [126][47/233]	Loss 0.0008 (0.0082)	
training:	Epoch: [126][48/233]	Loss 0.0006 (0.0080)	
training:	Epoch: [126][49/233]	Loss 0.0112 (0.0081)	
training:	Epoch: [126][50/233]	Loss 0.0008 (0.0079)	
training:	Epoch: [126][51/233]	Loss 0.0006 (0.0078)	
training:	Epoch: [126][52/233]	Loss 0.0009 (0.0077)	
training:	Epoch: [126][53/233]	Loss 0.0015 (0.0075)	
training:	Epoch: [126][54/233]	Loss 0.0005 (0.0074)	
training:	Epoch: [126][55/233]	Loss 0.0008 (0.0073)	
training:	Epoch: [126][56/233]	Loss 0.0087 (0.0073)	
training:	Epoch: [126][57/233]	Loss 0.0006 (0.0072)	
training:	Epoch: [126][58/233]	Loss 0.0006 (0.0071)	
training:	Epoch: [126][59/233]	Loss 0.0006 (0.0070)	
training:	Epoch: [126][60/233]	Loss 0.0006 (0.0069)	
training:	Epoch: [126][61/233]	Loss 0.0009 (0.0068)	
training:	Epoch: [126][62/233]	Loss 0.0006 (0.0067)	
training:	Epoch: [126][63/233]	Loss 0.0007 (0.0066)	
training:	Epoch: [126][64/233]	Loss 0.0007 (0.0065)	
training:	Epoch: [126][65/233]	Loss 0.0008 (0.0064)	
training:	Epoch: [126][66/233]	Loss 0.0006 (0.0063)	
training:	Epoch: [126][67/233]	Loss 0.0014 (0.0062)	
training:	Epoch: [126][68/233]	Loss 0.0014 (0.0062)	
training:	Epoch: [126][69/233]	Loss 0.0006 (0.0061)	
training:	Epoch: [126][70/233]	Loss 0.0005 (0.0060)	
training:	Epoch: [126][71/233]	Loss 0.0009 (0.0059)	
training:	Epoch: [126][72/233]	Loss 0.0005 (0.0059)	
training:	Epoch: [126][73/233]	Loss 0.0187 (0.0060)	
training:	Epoch: [126][74/233]	Loss 0.0008 (0.0060)	
training:	Epoch: [126][75/233]	Loss 0.0007 (0.0059)	
training:	Epoch: [126][76/233]	Loss 0.0011 (0.0058)	
training:	Epoch: [126][77/233]	Loss 0.0009 (0.0058)	
training:	Epoch: [126][78/233]	Loss 0.0028 (0.0057)	
training:	Epoch: [126][79/233]	Loss 0.0005 (0.0057)	
training:	Epoch: [126][80/233]	Loss 0.0010 (0.0056)	
training:	Epoch: [126][81/233]	Loss 0.0013 (0.0056)	
training:	Epoch: [126][82/233]	Loss 0.0008 (0.0055)	
training:	Epoch: [126][83/233]	Loss 0.0020 (0.0055)	
training:	Epoch: [126][84/233]	Loss 0.0005 (0.0054)	
training:	Epoch: [126][85/233]	Loss 0.0008 (0.0053)	
training:	Epoch: [126][86/233]	Loss 0.0007 (0.0053)	
training:	Epoch: [126][87/233]	Loss 0.0005 (0.0052)	
training:	Epoch: [126][88/233]	Loss 0.0008 (0.0052)	
training:	Epoch: [126][89/233]	Loss 0.0051 (0.0052)	
training:	Epoch: [126][90/233]	Loss 0.0011 (0.0051)	
training:	Epoch: [126][91/233]	Loss 0.0007 (0.0051)	
training:	Epoch: [126][92/233]	Loss 0.1540 (0.0067)	
training:	Epoch: [126][93/233]	Loss 0.0006 (0.0066)	
training:	Epoch: [126][94/233]	Loss 0.0008 (0.0066)	
training:	Epoch: [126][95/233]	Loss 0.0131 (0.0066)	
training:	Epoch: [126][96/233]	Loss 0.0009 (0.0066)	
training:	Epoch: [126][97/233]	Loss 0.0026 (0.0065)	
training:	Epoch: [126][98/233]	Loss 0.0005 (0.0065)	
training:	Epoch: [126][99/233]	Loss 0.0006 (0.0064)	
training:	Epoch: [126][100/233]	Loss 0.0021 (0.0064)	
training:	Epoch: [126][101/233]	Loss 0.0007 (0.0063)	
training:	Epoch: [126][102/233]	Loss 0.1874 (0.0081)	
training:	Epoch: [126][103/233]	Loss 0.0006 (0.0080)	
training:	Epoch: [126][104/233]	Loss 0.0010 (0.0080)	
training:	Epoch: [126][105/233]	Loss 0.0013 (0.0079)	
training:	Epoch: [126][106/233]	Loss 0.0011 (0.0078)	
training:	Epoch: [126][107/233]	Loss 0.0026 (0.0078)	
training:	Epoch: [126][108/233]	Loss 0.0007 (0.0077)	
training:	Epoch: [126][109/233]	Loss 0.0008 (0.0077)	
training:	Epoch: [126][110/233]	Loss 0.0006 (0.0076)	
training:	Epoch: [126][111/233]	Loss 0.0021 (0.0075)	
training:	Epoch: [126][112/233]	Loss 0.0005 (0.0075)	
training:	Epoch: [126][113/233]	Loss 0.0007 (0.0074)	
training:	Epoch: [126][114/233]	Loss 0.0013 (0.0074)	
training:	Epoch: [126][115/233]	Loss 0.0006 (0.0073)	
training:	Epoch: [126][116/233]	Loss 0.0006 (0.0072)	
training:	Epoch: [126][117/233]	Loss 0.0005 (0.0072)	
training:	Epoch: [126][118/233]	Loss 0.0006 (0.0071)	
training:	Epoch: [126][119/233]	Loss 0.0007 (0.0071)	
training:	Epoch: [126][120/233]	Loss 0.0010 (0.0070)	
training:	Epoch: [126][121/233]	Loss 0.0006 (0.0070)	
training:	Epoch: [126][122/233]	Loss 0.0011 (0.0069)	
training:	Epoch: [126][123/233]	Loss 0.0008 (0.0069)	
training:	Epoch: [126][124/233]	Loss 0.0024 (0.0068)	
training:	Epoch: [126][125/233]	Loss 0.0006 (0.0068)	
training:	Epoch: [126][126/233]	Loss 0.0006 (0.0067)	
training:	Epoch: [126][127/233]	Loss 0.0007 (0.0067)	
training:	Epoch: [126][128/233]	Loss 0.0009 (0.0067)	
training:	Epoch: [126][129/233]	Loss 0.0006 (0.0066)	
training:	Epoch: [126][130/233]	Loss 0.0006 (0.0066)	
training:	Epoch: [126][131/233]	Loss 0.0005 (0.0065)	
training:	Epoch: [126][132/233]	Loss 0.0009 (0.0065)	
training:	Epoch: [126][133/233]	Loss 0.0006 (0.0064)	
training:	Epoch: [126][134/233]	Loss 0.0006 (0.0064)	
training:	Epoch: [126][135/233]	Loss 0.0036 (0.0064)	
training:	Epoch: [126][136/233]	Loss 0.0009 (0.0063)	
training:	Epoch: [126][137/233]	Loss 0.0006 (0.0063)	
training:	Epoch: [126][138/233]	Loss 0.0011 (0.0062)	
training:	Epoch: [126][139/233]	Loss 0.0006 (0.0062)	
training:	Epoch: [126][140/233]	Loss 0.0005 (0.0062)	
training:	Epoch: [126][141/233]	Loss 0.0023 (0.0061)	
training:	Epoch: [126][142/233]	Loss 0.0005 (0.0061)	
training:	Epoch: [126][143/233]	Loss 0.0005 (0.0061)	
training:	Epoch: [126][144/233]	Loss 0.0011 (0.0060)	
training:	Epoch: [126][145/233]	Loss 0.0006 (0.0060)	
training:	Epoch: [126][146/233]	Loss 0.0006 (0.0059)	
training:	Epoch: [126][147/233]	Loss 0.0007 (0.0059)	
training:	Epoch: [126][148/233]	Loss 0.0007 (0.0059)	
training:	Epoch: [126][149/233]	Loss 0.0005 (0.0058)	
training:	Epoch: [126][150/233]	Loss 0.0005 (0.0058)	
training:	Epoch: [126][151/233]	Loss 0.0016 (0.0058)	
training:	Epoch: [126][152/233]	Loss 0.0011 (0.0057)	
training:	Epoch: [126][153/233]	Loss 0.0006 (0.0057)	
training:	Epoch: [126][154/233]	Loss 0.0006 (0.0057)	
training:	Epoch: [126][155/233]	Loss 0.0005 (0.0056)	
training:	Epoch: [126][156/233]	Loss 0.0005 (0.0056)	
training:	Epoch: [126][157/233]	Loss 0.0023 (0.0056)	
training:	Epoch: [126][158/233]	Loss 0.0006 (0.0056)	
training:	Epoch: [126][159/233]	Loss 0.0005 (0.0055)	
training:	Epoch: [126][160/233]	Loss 0.0007 (0.0055)	
training:	Epoch: [126][161/233]	Loss 0.0009 (0.0055)	
training:	Epoch: [126][162/233]	Loss 0.0005 (0.0054)	
training:	Epoch: [126][163/233]	Loss 0.0005 (0.0054)	
training:	Epoch: [126][164/233]	Loss 0.0987 (0.0060)	
training:	Epoch: [126][165/233]	Loss 0.0006 (0.0059)	
training:	Epoch: [126][166/233]	Loss 0.0006 (0.0059)	
training:	Epoch: [126][167/233]	Loss 0.0005 (0.0059)	
training:	Epoch: [126][168/233]	Loss 0.0005 (0.0058)	
training:	Epoch: [126][169/233]	Loss 0.0008 (0.0058)	
training:	Epoch: [126][170/233]	Loss 0.0018 (0.0058)	
training:	Epoch: [126][171/233]	Loss 0.0006 (0.0058)	
training:	Epoch: [126][172/233]	Loss 0.0014 (0.0057)	
training:	Epoch: [126][173/233]	Loss 0.0008 (0.0057)	
training:	Epoch: [126][174/233]	Loss 0.0030 (0.0057)	
training:	Epoch: [126][175/233]	Loss 0.0005 (0.0057)	
training:	Epoch: [126][176/233]	Loss 0.0006 (0.0056)	
training:	Epoch: [126][177/233]	Loss 0.0006 (0.0056)	
training:	Epoch: [126][178/233]	Loss 0.0087 (0.0056)	
training:	Epoch: [126][179/233]	Loss 0.0010 (0.0056)	
training:	Epoch: [126][180/233]	Loss 0.0005 (0.0056)	
training:	Epoch: [126][181/233]	Loss 0.0987 (0.0061)	
training:	Epoch: [126][182/233]	Loss 0.0005 (0.0061)	
training:	Epoch: [126][183/233]	Loss 0.0176 (0.0061)	
training:	Epoch: [126][184/233]	Loss 0.0082 (0.0061)	
training:	Epoch: [126][185/233]	Loss 0.0016 (0.0061)	
training:	Epoch: [126][186/233]	Loss 0.0014 (0.0061)	
training:	Epoch: [126][187/233]	Loss 0.0007 (0.0061)	
training:	Epoch: [126][188/233]	Loss 0.0005 (0.0060)	
training:	Epoch: [126][189/233]	Loss 0.0008 (0.0060)	
training:	Epoch: [126][190/233]	Loss 0.0008 (0.0060)	
training:	Epoch: [126][191/233]	Loss 0.0006 (0.0059)	
training:	Epoch: [126][192/233]	Loss 0.0007 (0.0059)	
training:	Epoch: [126][193/233]	Loss 0.0005 (0.0059)	
training:	Epoch: [126][194/233]	Loss 0.0006 (0.0059)	
training:	Epoch: [126][195/233]	Loss 0.0016 (0.0058)	
training:	Epoch: [126][196/233]	Loss 0.0005 (0.0058)	
training:	Epoch: [126][197/233]	Loss 0.0043 (0.0058)	
training:	Epoch: [126][198/233]	Loss 0.0007 (0.0058)	
training:	Epoch: [126][199/233]	Loss 0.0030 (0.0058)	
training:	Epoch: [126][200/233]	Loss 0.0108 (0.0058)	
training:	Epoch: [126][201/233]	Loss 0.0010 (0.0058)	
training:	Epoch: [126][202/233]	Loss 0.0010 (0.0057)	
training:	Epoch: [126][203/233]	Loss 0.0006 (0.0057)	
training:	Epoch: [126][204/233]	Loss 0.0029 (0.0057)	
training:	Epoch: [126][205/233]	Loss 0.0026 (0.0057)	
training:	Epoch: [126][206/233]	Loss 0.0006 (0.0057)	
training:	Epoch: [126][207/233]	Loss 0.0006 (0.0056)	
training:	Epoch: [126][208/233]	Loss 0.0007 (0.0056)	
training:	Epoch: [126][209/233]	Loss 0.0051 (0.0056)	
training:	Epoch: [126][210/233]	Loss 0.0020 (0.0056)	
training:	Epoch: [126][211/233]	Loss 0.0077 (0.0056)	
training:	Epoch: [126][212/233]	Loss 0.0008 (0.0056)	
training:	Epoch: [126][213/233]	Loss 0.0006 (0.0056)	
training:	Epoch: [126][214/233]	Loss 0.0006 (0.0055)	
training:	Epoch: [126][215/233]	Loss 0.0028 (0.0055)	
training:	Epoch: [126][216/233]	Loss 0.0006 (0.0055)	
training:	Epoch: [126][217/233]	Loss 0.0006 (0.0055)	
training:	Epoch: [126][218/233]	Loss 0.0006 (0.0055)	
training:	Epoch: [126][219/233]	Loss 0.1180 (0.0060)	
training:	Epoch: [126][220/233]	Loss 0.0005 (0.0059)	
training:	Epoch: [126][221/233]	Loss 0.0006 (0.0059)	
training:	Epoch: [126][222/233]	Loss 0.0017 (0.0059)	
training:	Epoch: [126][223/233]	Loss 0.0006 (0.0059)	
training:	Epoch: [126][224/233]	Loss 0.0008 (0.0059)	
training:	Epoch: [126][225/233]	Loss 0.0009 (0.0058)	
training:	Epoch: [126][226/233]	Loss 0.0008 (0.0058)	
training:	Epoch: [126][227/233]	Loss 0.0005 (0.0058)	
training:	Epoch: [126][228/233]	Loss 0.0013 (0.0058)	
training:	Epoch: [126][229/233]	Loss 0.0094 (0.0058)	
training:	Epoch: [126][230/233]	Loss 0.0020 (0.0058)	
training:	Epoch: [126][231/233]	Loss 0.0010 (0.0057)	
training:	Epoch: [126][232/233]	Loss 0.0005 (0.0057)	
training:	Epoch: [126][233/233]	Loss 0.0100 (0.0057)	
Training:	 Loss: 0.0057

Training:	 ACC: 0.9999 0.9999 1.0000 0.9997
Validation:	 ACC: 0.7936 0.7940 0.8029 0.7843
Validation:	 Best_BACC: 0.8162 0.8154 0.7998 0.8326
Validation:	 Loss: 1.1507
Pretraining:	Epoch 127/200
----------
training:	Epoch: [127][1/233]	Loss 0.0006 (0.0006)	
training:	Epoch: [127][2/233]	Loss 0.0005 (0.0005)	
training:	Epoch: [127][3/233]	Loss 0.0054 (0.0022)	
training:	Epoch: [127][4/233]	Loss 0.0011 (0.0019)	
training:	Epoch: [127][5/233]	Loss 0.0008 (0.0017)	
training:	Epoch: [127][6/233]	Loss 0.0054 (0.0023)	
training:	Epoch: [127][7/233]	Loss 0.0012 (0.0021)	
training:	Epoch: [127][8/233]	Loss 0.0005 (0.0019)	
training:	Epoch: [127][9/233]	Loss 0.0011 (0.0018)	
training:	Epoch: [127][10/233]	Loss 0.0021 (0.0019)	
training:	Epoch: [127][11/233]	Loss 0.0005 (0.0017)	
training:	Epoch: [127][12/233]	Loss 0.0009 (0.0017)	
training:	Epoch: [127][13/233]	Loss 0.0005 (0.0016)	
training:	Epoch: [127][14/233]	Loss 0.0010 (0.0015)	
training:	Epoch: [127][15/233]	Loss 0.0006 (0.0015)	
training:	Epoch: [127][16/233]	Loss 0.0006 (0.0014)	
training:	Epoch: [127][17/233]	Loss 0.0026 (0.0015)	
training:	Epoch: [127][18/233]	Loss 0.0006 (0.0014)	
training:	Epoch: [127][19/233]	Loss 0.0007 (0.0014)	
training:	Epoch: [127][20/233]	Loss 0.0406 (0.0034)	
training:	Epoch: [127][21/233]	Loss 0.0008 (0.0032)	
training:	Epoch: [127][22/233]	Loss 0.0006 (0.0031)	
training:	Epoch: [127][23/233]	Loss 0.0013 (0.0030)	
training:	Epoch: [127][24/233]	Loss 0.0007 (0.0029)	
training:	Epoch: [127][25/233]	Loss 0.0013 (0.0029)	
training:	Epoch: [127][26/233]	Loss 0.0010 (0.0028)	
training:	Epoch: [127][27/233]	Loss 0.0007 (0.0027)	
training:	Epoch: [127][28/233]	Loss 0.0006 (0.0026)	
training:	Epoch: [127][29/233]	Loss 0.0005 (0.0026)	
training:	Epoch: [127][30/233]	Loss 0.0005 (0.0025)	
training:	Epoch: [127][31/233]	Loss 0.0007 (0.0024)	
training:	Epoch: [127][32/233]	Loss 0.0012 (0.0024)	
training:	Epoch: [127][33/233]	Loss 0.0016 (0.0024)	
training:	Epoch: [127][34/233]	Loss 0.0008 (0.0023)	
training:	Epoch: [127][35/233]	Loss 0.0010 (0.0023)	
training:	Epoch: [127][36/233]	Loss 0.0007 (0.0023)	
training:	Epoch: [127][37/233]	Loss 0.2248 (0.0083)	
training:	Epoch: [127][38/233]	Loss 0.0009 (0.0081)	
training:	Epoch: [127][39/233]	Loss 0.0008 (0.0079)	
training:	Epoch: [127][40/233]	Loss 0.0005 (0.0077)	
training:	Epoch: [127][41/233]	Loss 0.0102 (0.0078)	
training:	Epoch: [127][42/233]	Loss 0.0006 (0.0076)	
training:	Epoch: [127][43/233]	Loss 0.0006 (0.0074)	
training:	Epoch: [127][44/233]	Loss 0.0007 (0.0073)	
training:	Epoch: [127][45/233]	Loss 0.0018 (0.0072)	
training:	Epoch: [127][46/233]	Loss 0.0006 (0.0070)	
training:	Epoch: [127][47/233]	Loss 0.0006 (0.0069)	
training:	Epoch: [127][48/233]	Loss 0.0006 (0.0067)	
training:	Epoch: [127][49/233]	Loss 0.0012 (0.0066)	
training:	Epoch: [127][50/233]	Loss 0.0006 (0.0065)	
training:	Epoch: [127][51/233]	Loss 0.0005 (0.0064)	
training:	Epoch: [127][52/233]	Loss 0.0007 (0.0063)	
training:	Epoch: [127][53/233]	Loss 0.0010 (0.0062)	
training:	Epoch: [127][54/233]	Loss 0.0006 (0.0061)	
training:	Epoch: [127][55/233]	Loss 0.0008 (0.0060)	
training:	Epoch: [127][56/233]	Loss 0.0005 (0.0059)	
training:	Epoch: [127][57/233]	Loss 0.0272 (0.0063)	
training:	Epoch: [127][58/233]	Loss 0.0018 (0.0062)	
training:	Epoch: [127][59/233]	Loss 0.0006 (0.0061)	
training:	Epoch: [127][60/233]	Loss 0.0018 (0.0060)	
training:	Epoch: [127][61/233]	Loss 0.0024 (0.0060)	
training:	Epoch: [127][62/233]	Loss 0.0006 (0.0059)	
training:	Epoch: [127][63/233]	Loss 0.0005 (0.0058)	
training:	Epoch: [127][64/233]	Loss 0.0006 (0.0057)	
training:	Epoch: [127][65/233]	Loss 0.0019 (0.0056)	
training:	Epoch: [127][66/233]	Loss 0.0009 (0.0056)	
training:	Epoch: [127][67/233]	Loss 0.0023 (0.0055)	
training:	Epoch: [127][68/233]	Loss 0.0012 (0.0055)	
training:	Epoch: [127][69/233]	Loss 0.0005 (0.0054)	
training:	Epoch: [127][70/233]	Loss 0.0006 (0.0053)	
training:	Epoch: [127][71/233]	Loss 0.0006 (0.0053)	
training:	Epoch: [127][72/233]	Loss 0.0007 (0.0052)	
training:	Epoch: [127][73/233]	Loss 0.0004 (0.0051)	
training:	Epoch: [127][74/233]	Loss 0.0005 (0.0051)	
training:	Epoch: [127][75/233]	Loss 0.0008 (0.0050)	
training:	Epoch: [127][76/233]	Loss 0.0011 (0.0050)	
training:	Epoch: [127][77/233]	Loss 0.0007 (0.0049)	
training:	Epoch: [127][78/233]	Loss 0.0006 (0.0048)	
training:	Epoch: [127][79/233]	Loss 0.0005 (0.0048)	
training:	Epoch: [127][80/233]	Loss 0.0006 (0.0047)	
training:	Epoch: [127][81/233]	Loss 0.0006 (0.0047)	
training:	Epoch: [127][82/233]	Loss 0.0124 (0.0048)	
training:	Epoch: [127][83/233]	Loss 0.0006 (0.0047)	
training:	Epoch: [127][84/233]	Loss 0.0006 (0.0047)	
training:	Epoch: [127][85/233]	Loss 0.0194 (0.0049)	
training:	Epoch: [127][86/233]	Loss 0.0006 (0.0048)	
training:	Epoch: [127][87/233]	Loss 0.0009 (0.0048)	
training:	Epoch: [127][88/233]	Loss 0.0006 (0.0047)	
training:	Epoch: [127][89/233]	Loss 0.0005 (0.0047)	
training:	Epoch: [127][90/233]	Loss 0.0015 (0.0046)	
training:	Epoch: [127][91/233]	Loss 0.0006 (0.0046)	
training:	Epoch: [127][92/233]	Loss 0.0005 (0.0045)	
training:	Epoch: [127][93/233]	Loss 0.0005 (0.0045)	
training:	Epoch: [127][94/233]	Loss 0.0005 (0.0045)	
training:	Epoch: [127][95/233]	Loss 0.0102 (0.0045)	
training:	Epoch: [127][96/233]	Loss 0.0019 (0.0045)	
training:	Epoch: [127][97/233]	Loss 0.0006 (0.0045)	
training:	Epoch: [127][98/233]	Loss 0.0006 (0.0044)	
training:	Epoch: [127][99/233]	Loss 0.0008 (0.0044)	
training:	Epoch: [127][100/233]	Loss 0.0006 (0.0043)	
training:	Epoch: [127][101/233]	Loss 0.0005 (0.0043)	
training:	Epoch: [127][102/233]	Loss 0.0046 (0.0043)	
training:	Epoch: [127][103/233]	Loss 0.0005 (0.0043)	
training:	Epoch: [127][104/233]	Loss 0.0005 (0.0042)	
training:	Epoch: [127][105/233]	Loss 0.0005 (0.0042)	
training:	Epoch: [127][106/233]	Loss 0.0009 (0.0042)	
training:	Epoch: [127][107/233]	Loss 0.0005 (0.0041)	
training:	Epoch: [127][108/233]	Loss 0.0005 (0.0041)	
training:	Epoch: [127][109/233]	Loss 0.0007 (0.0041)	
training:	Epoch: [127][110/233]	Loss 0.0006 (0.0040)	
training:	Epoch: [127][111/233]	Loss 0.0138 (0.0041)	
training:	Epoch: [127][112/233]	Loss 0.0011 (0.0041)	
training:	Epoch: [127][113/233]	Loss 0.0008 (0.0041)	
training:	Epoch: [127][114/233]	Loss 0.0008 (0.0040)	
training:	Epoch: [127][115/233]	Loss 0.0005 (0.0040)	
training:	Epoch: [127][116/233]	Loss 0.0006 (0.0040)	
training:	Epoch: [127][117/233]	Loss 0.0012 (0.0040)	
training:	Epoch: [127][118/233]	Loss 0.0007 (0.0039)	
training:	Epoch: [127][119/233]	Loss 0.0073 (0.0040)	
training:	Epoch: [127][120/233]	Loss 0.0011 (0.0039)	
training:	Epoch: [127][121/233]	Loss 0.0007 (0.0039)	
training:	Epoch: [127][122/233]	Loss 0.0005 (0.0039)	
training:	Epoch: [127][123/233]	Loss 0.0005 (0.0038)	
training:	Epoch: [127][124/233]	Loss 0.0005 (0.0038)	
training:	Epoch: [127][125/233]	Loss 0.0006 (0.0038)	
training:	Epoch: [127][126/233]	Loss 0.0005 (0.0038)	
training:	Epoch: [127][127/233]	Loss 0.0005 (0.0037)	
training:	Epoch: [127][128/233]	Loss 0.0006 (0.0037)	
training:	Epoch: [127][129/233]	Loss 0.0006 (0.0037)	
training:	Epoch: [127][130/233]	Loss 0.0044 (0.0037)	
training:	Epoch: [127][131/233]	Loss 0.0007 (0.0037)	
training:	Epoch: [127][132/233]	Loss 0.0019 (0.0037)	
training:	Epoch: [127][133/233]	Loss 0.0005 (0.0036)	
training:	Epoch: [127][134/233]	Loss 0.0006 (0.0036)	
training:	Epoch: [127][135/233]	Loss 0.0005 (0.0036)	
training:	Epoch: [127][136/233]	Loss 0.0008 (0.0036)	
training:	Epoch: [127][137/233]	Loss 0.0005 (0.0035)	
training:	Epoch: [127][138/233]	Loss 0.0006 (0.0035)	
training:	Epoch: [127][139/233]	Loss 0.0004 (0.0035)	
training:	Epoch: [127][140/233]	Loss 0.0975 (0.0042)	
training:	Epoch: [127][141/233]	Loss 0.0006 (0.0042)	
training:	Epoch: [127][142/233]	Loss 0.0006 (0.0041)	
training:	Epoch: [127][143/233]	Loss 0.0029 (0.0041)	
training:	Epoch: [127][144/233]	Loss 0.0007 (0.0041)	
training:	Epoch: [127][145/233]	Loss 0.0012 (0.0041)	
training:	Epoch: [127][146/233]	Loss 0.0008 (0.0041)	
training:	Epoch: [127][147/233]	Loss 0.0007 (0.0040)	
training:	Epoch: [127][148/233]	Loss 0.0007 (0.0040)	
training:	Epoch: [127][149/233]	Loss 0.0006 (0.0040)	
training:	Epoch: [127][150/233]	Loss 0.0005 (0.0040)	
training:	Epoch: [127][151/233]	Loss 0.0005 (0.0039)	
training:	Epoch: [127][152/233]	Loss 0.0005 (0.0039)	
training:	Epoch: [127][153/233]	Loss 0.0008 (0.0039)	
training:	Epoch: [127][154/233]	Loss 0.0016 (0.0039)	
training:	Epoch: [127][155/233]	Loss 0.0008 (0.0039)	
training:	Epoch: [127][156/233]	Loss 0.0007 (0.0038)	
training:	Epoch: [127][157/233]	Loss 0.0008 (0.0038)	
training:	Epoch: [127][158/233]	Loss 0.0004 (0.0038)	
training:	Epoch: [127][159/233]	Loss 0.0008 (0.0038)	
training:	Epoch: [127][160/233]	Loss 0.0005 (0.0038)	
training:	Epoch: [127][161/233]	Loss 0.0010 (0.0037)	
training:	Epoch: [127][162/233]	Loss 0.0005 (0.0037)	
training:	Epoch: [127][163/233]	Loss 0.0006 (0.0037)	
training:	Epoch: [127][164/233]	Loss 0.0013 (0.0037)	
training:	Epoch: [127][165/233]	Loss 0.0008 (0.0037)	
training:	Epoch: [127][166/233]	Loss 0.0049 (0.0037)	
training:	Epoch: [127][167/233]	Loss 0.0014 (0.0037)	
training:	Epoch: [127][168/233]	Loss 0.0006 (0.0036)	
training:	Epoch: [127][169/233]	Loss 0.0005 (0.0036)	
training:	Epoch: [127][170/233]	Loss 0.0005 (0.0036)	
training:	Epoch: [127][171/233]	Loss 0.0005 (0.0036)	
training:	Epoch: [127][172/233]	Loss 0.0007 (0.0036)	
training:	Epoch: [127][173/233]	Loss 0.0179 (0.0037)	
training:	Epoch: [127][174/233]	Loss 0.1198 (0.0043)	
training:	Epoch: [127][175/233]	Loss 0.0024 (0.0043)	
training:	Epoch: [127][176/233]	Loss 0.0006 (0.0043)	
training:	Epoch: [127][177/233]	Loss 0.0006 (0.0043)	
training:	Epoch: [127][178/233]	Loss 0.0007 (0.0043)	
training:	Epoch: [127][179/233]	Loss 0.0006 (0.0042)	
training:	Epoch: [127][180/233]	Loss 0.0008 (0.0042)	
training:	Epoch: [127][181/233]	Loss 0.0006 (0.0042)	
training:	Epoch: [127][182/233]	Loss 0.0006 (0.0042)	
training:	Epoch: [127][183/233]	Loss 0.0004 (0.0042)	
training:	Epoch: [127][184/233]	Loss 0.0007 (0.0041)	
training:	Epoch: [127][185/233]	Loss 0.0161 (0.0042)	
training:	Epoch: [127][186/233]	Loss 0.0011 (0.0042)	
training:	Epoch: [127][187/233]	Loss 0.0006 (0.0042)	
training:	Epoch: [127][188/233]	Loss 0.0005 (0.0041)	
training:	Epoch: [127][189/233]	Loss 0.0014 (0.0041)	
training:	Epoch: [127][190/233]	Loss 0.0032 (0.0041)	
training:	Epoch: [127][191/233]	Loss 0.0008 (0.0041)	
training:	Epoch: [127][192/233]	Loss 0.0007 (0.0041)	
training:	Epoch: [127][193/233]	Loss 0.0012 (0.0041)	
training:	Epoch: [127][194/233]	Loss 0.0008 (0.0041)	
training:	Epoch: [127][195/233]	Loss 0.0011 (0.0040)	
training:	Epoch: [127][196/233]	Loss 0.0005 (0.0040)	
training:	Epoch: [127][197/233]	Loss 0.0005 (0.0040)	
training:	Epoch: [127][198/233]	Loss 0.0353 (0.0042)	
training:	Epoch: [127][199/233]	Loss 0.0007 (0.0041)	
training:	Epoch: [127][200/233]	Loss 0.0006 (0.0041)	
training:	Epoch: [127][201/233]	Loss 0.0007 (0.0041)	
training:	Epoch: [127][202/233]	Loss 0.0022 (0.0041)	
training:	Epoch: [127][203/233]	Loss 0.0023 (0.0041)	
training:	Epoch: [127][204/233]	Loss 0.0005 (0.0041)	
training:	Epoch: [127][205/233]	Loss 0.0005 (0.0041)	
training:	Epoch: [127][206/233]	Loss 0.0008 (0.0040)	
training:	Epoch: [127][207/233]	Loss 0.0005 (0.0040)	
training:	Epoch: [127][208/233]	Loss 0.0005 (0.0040)	
training:	Epoch: [127][209/233]	Loss 0.0006 (0.0040)	
training:	Epoch: [127][210/233]	Loss 0.0005 (0.0040)	
training:	Epoch: [127][211/233]	Loss 0.0006 (0.0040)	
training:	Epoch: [127][212/233]	Loss 0.0014 (0.0039)	
training:	Epoch: [127][213/233]	Loss 0.0005 (0.0039)	
training:	Epoch: [127][214/233]	Loss 0.0005 (0.0039)	
training:	Epoch: [127][215/233]	Loss 0.0007 (0.0039)	
training:	Epoch: [127][216/233]	Loss 0.0073 (0.0039)	
training:	Epoch: [127][217/233]	Loss 0.0005 (0.0039)	
training:	Epoch: [127][218/233]	Loss 0.0010 (0.0039)	
training:	Epoch: [127][219/233]	Loss 0.0006 (0.0039)	
training:	Epoch: [127][220/233]	Loss 0.0005 (0.0039)	
training:	Epoch: [127][221/233]	Loss 0.0007 (0.0038)	
training:	Epoch: [127][222/233]	Loss 0.0010 (0.0038)	
training:	Epoch: [127][223/233]	Loss 0.0005 (0.0038)	
training:	Epoch: [127][224/233]	Loss 0.0005 (0.0038)	
training:	Epoch: [127][225/233]	Loss 0.0006 (0.0038)	
training:	Epoch: [127][226/233]	Loss 0.0006 (0.0038)	
training:	Epoch: [127][227/233]	Loss 0.0011 (0.0038)	
training:	Epoch: [127][228/233]	Loss 0.0010 (0.0037)	
training:	Epoch: [127][229/233]	Loss 0.0005 (0.0037)	
training:	Epoch: [127][230/233]	Loss 0.0775 (0.0041)	
training:	Epoch: [127][231/233]	Loss 0.0011 (0.0040)	
training:	Epoch: [127][232/233]	Loss 0.0005 (0.0040)	
training:	Epoch: [127][233/233]	Loss 0.0096 (0.0040)	
Training:	 Loss: 0.0040

Training:	 ACC: 0.9999 0.9999 1.0000 0.9997
Validation:	 ACC: 0.7871 0.7865 0.7753 0.7989
Validation:	 Best_BACC: 0.8162 0.8154 0.7998 0.8326
Validation:	 Loss: 1.1694
Pretraining:	Epoch 128/200
----------
training:	Epoch: [128][1/233]	Loss 0.0007 (0.0007)	
training:	Epoch: [128][2/233]	Loss 0.0005 (0.0006)	
training:	Epoch: [128][3/233]	Loss 0.0006 (0.0006)	
training:	Epoch: [128][4/233]	Loss 0.0005 (0.0006)	
training:	Epoch: [128][5/233]	Loss 0.0006 (0.0006)	
training:	Epoch: [128][6/233]	Loss 0.0006 (0.0006)	
training:	Epoch: [128][7/233]	Loss 0.0006 (0.0006)	
training:	Epoch: [128][8/233]	Loss 0.0170 (0.0026)	
training:	Epoch: [128][9/233]	Loss 0.0005 (0.0024)	
training:	Epoch: [128][10/233]	Loss 0.0005 (0.0022)	
training:	Epoch: [128][11/233]	Loss 0.0006 (0.0021)	
training:	Epoch: [128][12/233]	Loss 0.0007 (0.0020)	
training:	Epoch: [128][13/233]	Loss 0.0005 (0.0018)	
training:	Epoch: [128][14/233]	Loss 0.0006 (0.0018)	
training:	Epoch: [128][15/233]	Loss 0.0005 (0.0017)	
training:	Epoch: [128][16/233]	Loss 0.0006 (0.0016)	
training:	Epoch: [128][17/233]	Loss 0.0005 (0.0015)	
training:	Epoch: [128][18/233]	Loss 0.0009 (0.0015)	
training:	Epoch: [128][19/233]	Loss 0.0007 (0.0015)	
training:	Epoch: [128][20/233]	Loss 0.0008 (0.0014)	
training:	Epoch: [128][21/233]	Loss 0.0007 (0.0014)	
training:	Epoch: [128][22/233]	Loss 0.0005 (0.0014)	
training:	Epoch: [128][23/233]	Loss 0.0006 (0.0013)	
training:	Epoch: [128][24/233]	Loss 0.0005 (0.0013)	
training:	Epoch: [128][25/233]	Loss 0.0005 (0.0013)	
training:	Epoch: [128][26/233]	Loss 0.0007 (0.0012)	
training:	Epoch: [128][27/233]	Loss 0.0005 (0.0012)	
training:	Epoch: [128][28/233]	Loss 0.0008 (0.0012)	
training:	Epoch: [128][29/233]	Loss 0.0008 (0.0012)	
training:	Epoch: [128][30/233]	Loss 0.0006 (0.0012)	
training:	Epoch: [128][31/233]	Loss 0.0005 (0.0011)	
training:	Epoch: [128][32/233]	Loss 0.0005 (0.0011)	
training:	Epoch: [128][33/233]	Loss 0.0006 (0.0011)	
training:	Epoch: [128][34/233]	Loss 0.0006 (0.0011)	
training:	Epoch: [128][35/233]	Loss 0.0006 (0.0011)	
training:	Epoch: [128][36/233]	Loss 0.0005 (0.0011)	
training:	Epoch: [128][37/233]	Loss 0.1124 (0.0041)	
training:	Epoch: [128][38/233]	Loss 0.0005 (0.0040)	
training:	Epoch: [128][39/233]	Loss 0.0006 (0.0039)	
training:	Epoch: [128][40/233]	Loss 0.0007 (0.0038)	
training:	Epoch: [128][41/233]	Loss 0.0005 (0.0037)	
training:	Epoch: [128][42/233]	Loss 0.0005 (0.0036)	
training:	Epoch: [128][43/233]	Loss 0.0006 (0.0036)	
training:	Epoch: [128][44/233]	Loss 0.0007 (0.0035)	
training:	Epoch: [128][45/233]	Loss 0.0005 (0.0034)	
training:	Epoch: [128][46/233]	Loss 0.0005 (0.0034)	
training:	Epoch: [128][47/233]	Loss 0.0272 (0.0039)	
training:	Epoch: [128][48/233]	Loss 0.0006 (0.0038)	
training:	Epoch: [128][49/233]	Loss 0.0005 (0.0038)	
training:	Epoch: [128][50/233]	Loss 0.0006 (0.0037)	
training:	Epoch: [128][51/233]	Loss 0.0007 (0.0036)	
training:	Epoch: [128][52/233]	Loss 0.0009 (0.0036)	
training:	Epoch: [128][53/233]	Loss 0.0007 (0.0035)	
training:	Epoch: [128][54/233]	Loss 0.0005 (0.0035)	
training:	Epoch: [128][55/233]	Loss 0.0004 (0.0034)	
training:	Epoch: [128][56/233]	Loss 0.0004 (0.0034)	
training:	Epoch: [128][57/233]	Loss 0.0022 (0.0033)	
training:	Epoch: [128][58/233]	Loss 0.0005 (0.0033)	
training:	Epoch: [128][59/233]	Loss 0.0006 (0.0032)	
training:	Epoch: [128][60/233]	Loss 0.0007 (0.0032)	
training:	Epoch: [128][61/233]	Loss 0.0006 (0.0032)	
training:	Epoch: [128][62/233]	Loss 0.0008 (0.0031)	
training:	Epoch: [128][63/233]	Loss 0.0012 (0.0031)	
training:	Epoch: [128][64/233]	Loss 0.0007 (0.0031)	
training:	Epoch: [128][65/233]	Loss 0.0006 (0.0030)	
training:	Epoch: [128][66/233]	Loss 0.0006 (0.0030)	
training:	Epoch: [128][67/233]	Loss 0.0006 (0.0029)	
training:	Epoch: [128][68/233]	Loss 0.0007 (0.0029)	
training:	Epoch: [128][69/233]	Loss 0.0007 (0.0029)	
training:	Epoch: [128][70/233]	Loss 0.0009 (0.0029)	
training:	Epoch: [128][71/233]	Loss 0.0005 (0.0028)	
training:	Epoch: [128][72/233]	Loss 0.0039 (0.0028)	
training:	Epoch: [128][73/233]	Loss 0.0007 (0.0028)	
training:	Epoch: [128][74/233]	Loss 0.0006 (0.0028)	
training:	Epoch: [128][75/233]	Loss 0.0005 (0.0027)	
training:	Epoch: [128][76/233]	Loss 0.0005 (0.0027)	
training:	Epoch: [128][77/233]	Loss 0.0007 (0.0027)	
training:	Epoch: [128][78/233]	Loss 0.0310 (0.0031)	
training:	Epoch: [128][79/233]	Loss 0.0004 (0.0030)	
training:	Epoch: [128][80/233]	Loss 0.0009 (0.0030)	
training:	Epoch: [128][81/233]	Loss 0.0008 (0.0030)	
training:	Epoch: [128][82/233]	Loss 0.0005 (0.0029)	
training:	Epoch: [128][83/233]	Loss 0.0008 (0.0029)	
training:	Epoch: [128][84/233]	Loss 0.0007 (0.0029)	
training:	Epoch: [128][85/233]	Loss 0.0006 (0.0029)	
training:	Epoch: [128][86/233]	Loss 0.0005 (0.0028)	
training:	Epoch: [128][87/233]	Loss 0.0007 (0.0028)	
training:	Epoch: [128][88/233]	Loss 0.0006 (0.0028)	
training:	Epoch: [128][89/233]	Loss 0.0006 (0.0028)	
training:	Epoch: [128][90/233]	Loss 0.0004 (0.0027)	
training:	Epoch: [128][91/233]	Loss 0.0005 (0.0027)	
training:	Epoch: [128][92/233]	Loss 0.0018 (0.0027)	
training:	Epoch: [128][93/233]	Loss 0.0006 (0.0027)	
training:	Epoch: [128][94/233]	Loss 0.0012 (0.0027)	
training:	Epoch: [128][95/233]	Loss 0.0007 (0.0026)	
training:	Epoch: [128][96/233]	Loss 0.0009 (0.0026)	
training:	Epoch: [128][97/233]	Loss 0.0019 (0.0026)	
training:	Epoch: [128][98/233]	Loss 0.0007 (0.0026)	
training:	Epoch: [128][99/233]	Loss 0.0011 (0.0026)	
training:	Epoch: [128][100/233]	Loss 0.0006 (0.0026)	
training:	Epoch: [128][101/233]	Loss 0.0442 (0.0030)	
training:	Epoch: [128][102/233]	Loss 0.0007 (0.0029)	
training:	Epoch: [128][103/233]	Loss 0.2770 (0.0056)	
training:	Epoch: [128][104/233]	Loss 0.0005 (0.0056)	
training:	Epoch: [128][105/233]	Loss 0.0006 (0.0055)	
training:	Epoch: [128][106/233]	Loss 0.0012 (0.0055)	
training:	Epoch: [128][107/233]	Loss 0.0007 (0.0054)	
training:	Epoch: [128][108/233]	Loss 0.0059 (0.0054)	
training:	Epoch: [128][109/233]	Loss 0.0006 (0.0054)	
training:	Epoch: [128][110/233]	Loss 0.0007 (0.0053)	
training:	Epoch: [128][111/233]	Loss 0.0005 (0.0053)	
training:	Epoch: [128][112/233]	Loss 0.0012 (0.0053)	
training:	Epoch: [128][113/233]	Loss 0.0006 (0.0052)	
training:	Epoch: [128][114/233]	Loss 0.0004 (0.0052)	
training:	Epoch: [128][115/233]	Loss 0.0006 (0.0051)	
training:	Epoch: [128][116/233]	Loss 0.0032 (0.0051)	
training:	Epoch: [128][117/233]	Loss 0.0025 (0.0051)	
training:	Epoch: [128][118/233]	Loss 0.0006 (0.0051)	
training:	Epoch: [128][119/233]	Loss 0.0074 (0.0051)	
training:	Epoch: [128][120/233]	Loss 0.0005 (0.0050)	
training:	Epoch: [128][121/233]	Loss 0.0008 (0.0050)	
training:	Epoch: [128][122/233]	Loss 0.0029 (0.0050)	
training:	Epoch: [128][123/233]	Loss 0.0005 (0.0050)	
training:	Epoch: [128][124/233]	Loss 0.0007 (0.0049)	
training:	Epoch: [128][125/233]	Loss 0.0011 (0.0049)	
training:	Epoch: [128][126/233]	Loss 0.0013 (0.0049)	
training:	Epoch: [128][127/233]	Loss 0.0122 (0.0049)	
training:	Epoch: [128][128/233]	Loss 0.0005 (0.0049)	
training:	Epoch: [128][129/233]	Loss 0.0162 (0.0050)	
training:	Epoch: [128][130/233]	Loss 0.0025 (0.0050)	
training:	Epoch: [128][131/233]	Loss 0.0011 (0.0049)	
training:	Epoch: [128][132/233]	Loss 0.0005 (0.0049)	
training:	Epoch: [128][133/233]	Loss 0.0005 (0.0049)	
training:	Epoch: [128][134/233]	Loss 0.0005 (0.0048)	
training:	Epoch: [128][135/233]	Loss 0.0008 (0.0048)	
training:	Epoch: [128][136/233]	Loss 0.0008 (0.0048)	
training:	Epoch: [128][137/233]	Loss 0.0007 (0.0047)	
training:	Epoch: [128][138/233]	Loss 0.0005 (0.0047)	
training:	Epoch: [128][139/233]	Loss 0.0005 (0.0047)	
training:	Epoch: [128][140/233]	Loss 0.0030 (0.0047)	
training:	Epoch: [128][141/233]	Loss 0.0005 (0.0046)	
training:	Epoch: [128][142/233]	Loss 0.0009 (0.0046)	
training:	Epoch: [128][143/233]	Loss 0.0008 (0.0046)	
training:	Epoch: [128][144/233]	Loss 0.0006 (0.0046)	
training:	Epoch: [128][145/233]	Loss 0.0005 (0.0045)	
training:	Epoch: [128][146/233]	Loss 0.0046 (0.0045)	
training:	Epoch: [128][147/233]	Loss 0.0006 (0.0045)	
training:	Epoch: [128][148/233]	Loss 0.0263 (0.0046)	
training:	Epoch: [128][149/233]	Loss 0.0005 (0.0046)	
training:	Epoch: [128][150/233]	Loss 0.0007 (0.0046)	
training:	Epoch: [128][151/233]	Loss 0.0007 (0.0046)	
training:	Epoch: [128][152/233]	Loss 0.0502 (0.0049)	
training:	Epoch: [128][153/233]	Loss 0.0004 (0.0048)	
training:	Epoch: [128][154/233]	Loss 0.0010 (0.0048)	
training:	Epoch: [128][155/233]	Loss 0.0006 (0.0048)	
training:	Epoch: [128][156/233]	Loss 0.0005 (0.0048)	
training:	Epoch: [128][157/233]	Loss 0.0007 (0.0047)	
training:	Epoch: [128][158/233]	Loss 0.0006 (0.0047)	
training:	Epoch: [128][159/233]	Loss 0.0005 (0.0047)	
training:	Epoch: [128][160/233]	Loss 0.0016 (0.0047)	
training:	Epoch: [128][161/233]	Loss 0.0021 (0.0046)	
training:	Epoch: [128][162/233]	Loss 0.0009 (0.0046)	
training:	Epoch: [128][163/233]	Loss 0.0008 (0.0046)	
training:	Epoch: [128][164/233]	Loss 0.0006 (0.0046)	
training:	Epoch: [128][165/233]	Loss 0.0007 (0.0046)	
training:	Epoch: [128][166/233]	Loss 0.0009 (0.0045)	
training:	Epoch: [128][167/233]	Loss 0.0004 (0.0045)	
training:	Epoch: [128][168/233]	Loss 0.0007 (0.0045)	
training:	Epoch: [128][169/233]	Loss 0.0005 (0.0045)	
training:	Epoch: [128][170/233]	Loss 0.0005 (0.0044)	
training:	Epoch: [128][171/233]	Loss 0.0006 (0.0044)	
training:	Epoch: [128][172/233]	Loss 0.0011 (0.0044)	
training:	Epoch: [128][173/233]	Loss 0.0005 (0.0044)	
training:	Epoch: [128][174/233]	Loss 0.0010 (0.0044)	
training:	Epoch: [128][175/233]	Loss 0.0005 (0.0043)	
training:	Epoch: [128][176/233]	Loss 0.0006 (0.0043)	
training:	Epoch: [128][177/233]	Loss 0.0144 (0.0044)	
training:	Epoch: [128][178/233]	Loss 0.0006 (0.0043)	
training:	Epoch: [128][179/233]	Loss 0.0028 (0.0043)	
training:	Epoch: [128][180/233]	Loss 0.0006 (0.0043)	
training:	Epoch: [128][181/233]	Loss 0.0007 (0.0043)	
training:	Epoch: [128][182/233]	Loss 0.0028 (0.0043)	
training:	Epoch: [128][183/233]	Loss 0.0008 (0.0043)	
training:	Epoch: [128][184/233]	Loss 0.0005 (0.0042)	
training:	Epoch: [128][185/233]	Loss 0.0006 (0.0042)	
training:	Epoch: [128][186/233]	Loss 0.0076 (0.0042)	
training:	Epoch: [128][187/233]	Loss 0.0006 (0.0042)	
training:	Epoch: [128][188/233]	Loss 0.0007 (0.0042)	
training:	Epoch: [128][189/233]	Loss 0.0005 (0.0042)	
training:	Epoch: [128][190/233]	Loss 0.0006 (0.0042)	
training:	Epoch: [128][191/233]	Loss 0.0247 (0.0043)	
training:	Epoch: [128][192/233]	Loss 0.0010 (0.0043)	
training:	Epoch: [128][193/233]	Loss 0.0025 (0.0042)	
training:	Epoch: [128][194/233]	Loss 0.0011 (0.0042)	
training:	Epoch: [128][195/233]	Loss 0.0005 (0.0042)	
training:	Epoch: [128][196/233]	Loss 0.0013 (0.0042)	
training:	Epoch: [128][197/233]	Loss 0.0008 (0.0042)	
training:	Epoch: [128][198/233]	Loss 0.0020 (0.0042)	
training:	Epoch: [128][199/233]	Loss 0.0008 (0.0042)	
training:	Epoch: [128][200/233]	Loss 0.0005 (0.0041)	
training:	Epoch: [128][201/233]	Loss 0.0006 (0.0041)	
training:	Epoch: [128][202/233]	Loss 0.0008 (0.0041)	
training:	Epoch: [128][203/233]	Loss 0.0005 (0.0041)	
training:	Epoch: [128][204/233]	Loss 0.0006 (0.0041)	
training:	Epoch: [128][205/233]	Loss 0.0049 (0.0041)	
training:	Epoch: [128][206/233]	Loss 0.0006 (0.0041)	
training:	Epoch: [128][207/233]	Loss 0.0693 (0.0044)	
training:	Epoch: [128][208/233]	Loss 0.0005 (0.0044)	
training:	Epoch: [128][209/233]	Loss 0.0074 (0.0044)	
training:	Epoch: [128][210/233]	Loss 0.0005 (0.0043)	
training:	Epoch: [128][211/233]	Loss 0.0005 (0.0043)	
training:	Epoch: [128][212/233]	Loss 0.0005 (0.0043)	
training:	Epoch: [128][213/233]	Loss 0.0007 (0.0043)	
training:	Epoch: [128][214/233]	Loss 0.0005 (0.0043)	
training:	Epoch: [128][215/233]	Loss 0.0005 (0.0043)	
training:	Epoch: [128][216/233]	Loss 0.0019 (0.0042)	
training:	Epoch: [128][217/233]	Loss 0.0006 (0.0042)	
training:	Epoch: [128][218/233]	Loss 0.0016 (0.0042)	
training:	Epoch: [128][219/233]	Loss 0.0005 (0.0042)	
training:	Epoch: [128][220/233]	Loss 0.0012 (0.0042)	
training:	Epoch: [128][221/233]	Loss 0.0024 (0.0042)	
training:	Epoch: [128][222/233]	Loss 0.0081 (0.0042)	
training:	Epoch: [128][223/233]	Loss 0.0155 (0.0042)	
training:	Epoch: [128][224/233]	Loss 0.0006 (0.0042)	
training:	Epoch: [128][225/233]	Loss 0.0006 (0.0042)	
training:	Epoch: [128][226/233]	Loss 0.0006 (0.0042)	
training:	Epoch: [128][227/233]	Loss 0.0005 (0.0042)	
training:	Epoch: [128][228/233]	Loss 0.0007 (0.0042)	
training:	Epoch: [128][229/233]	Loss 0.0037 (0.0042)	
training:	Epoch: [128][230/233]	Loss 0.0008 (0.0042)	
training:	Epoch: [128][231/233]	Loss 0.0005 (0.0041)	
training:	Epoch: [128][232/233]	Loss 0.0004 (0.0041)	
training:	Epoch: [128][233/233]	Loss 0.0519 (0.0043)	
Training:	 Loss: 0.0043

Training:	 ACC: 0.9999 0.9999 1.0000 0.9997
Validation:	 ACC: 0.7943 0.7945 0.7988 0.7899
Validation:	 Best_BACC: 0.8162 0.8154 0.7998 0.8326
Validation:	 Loss: 1.1872
Pretraining:	Epoch 129/200
----------
training:	Epoch: [129][1/233]	Loss 0.0005 (0.0005)	
training:	Epoch: [129][2/233]	Loss 0.0016 (0.0011)	
training:	Epoch: [129][3/233]	Loss 0.0021 (0.0014)	
training:	Epoch: [129][4/233]	Loss 0.0005 (0.0012)	
training:	Epoch: [129][5/233]	Loss 0.0014 (0.0012)	
training:	Epoch: [129][6/233]	Loss 0.0006 (0.0011)	
training:	Epoch: [129][7/233]	Loss 0.2191 (0.0323)	
training:	Epoch: [129][8/233]	Loss 0.0009 (0.0283)	
training:	Epoch: [129][9/233]	Loss 0.0006 (0.0253)	
training:	Epoch: [129][10/233]	Loss 0.0253 (0.0253)	
training:	Epoch: [129][11/233]	Loss 0.0015 (0.0231)	
training:	Epoch: [129][12/233]	Loss 0.0005 (0.0212)	
training:	Epoch: [129][13/233]	Loss 0.0006 (0.0196)	
training:	Epoch: [129][14/233]	Loss 0.0093 (0.0189)	
training:	Epoch: [129][15/233]	Loss 0.0006 (0.0177)	
training:	Epoch: [129][16/233]	Loss 0.0009 (0.0166)	
training:	Epoch: [129][17/233]	Loss 0.0013 (0.0157)	
training:	Epoch: [129][18/233]	Loss 0.0009 (0.0149)	
training:	Epoch: [129][19/233]	Loss 0.0005 (0.0142)	
training:	Epoch: [129][20/233]	Loss 0.0045 (0.0137)	
training:	Epoch: [129][21/233]	Loss 0.0016 (0.0131)	
training:	Epoch: [129][22/233]	Loss 0.0006 (0.0125)	
training:	Epoch: [129][23/233]	Loss 0.0008 (0.0120)	
training:	Epoch: [129][24/233]	Loss 0.0009 (0.0116)	
training:	Epoch: [129][25/233]	Loss 0.0020 (0.0112)	
training:	Epoch: [129][26/233]	Loss 0.0006 (0.0108)	
training:	Epoch: [129][27/233]	Loss 0.0009 (0.0104)	
training:	Epoch: [129][28/233]	Loss 0.0245 (0.0109)	
training:	Epoch: [129][29/233]	Loss 0.0008 (0.0106)	
training:	Epoch: [129][30/233]	Loss 0.0005 (0.0102)	
training:	Epoch: [129][31/233]	Loss 0.0005 (0.0099)	
training:	Epoch: [129][32/233]	Loss 0.0005 (0.0096)	
training:	Epoch: [129][33/233]	Loss 0.0461 (0.0107)	
training:	Epoch: [129][34/233]	Loss 0.0009 (0.0104)	
training:	Epoch: [129][35/233]	Loss 0.0005 (0.0101)	
training:	Epoch: [129][36/233]	Loss 0.0005 (0.0099)	
training:	Epoch: [129][37/233]	Loss 0.0005 (0.0096)	
training:	Epoch: [129][38/233]	Loss 0.0009 (0.0094)	
training:	Epoch: [129][39/233]	Loss 0.0007 (0.0092)	
training:	Epoch: [129][40/233]	Loss 0.0006 (0.0090)	
training:	Epoch: [129][41/233]	Loss 0.0008 (0.0088)	
training:	Epoch: [129][42/233]	Loss 0.0005 (0.0086)	
training:	Epoch: [129][43/233]	Loss 0.0009 (0.0084)	
training:	Epoch: [129][44/233]	Loss 0.0016 (0.0082)	
training:	Epoch: [129][45/233]	Loss 0.0008 (0.0081)	
training:	Epoch: [129][46/233]	Loss 0.0011 (0.0079)	
training:	Epoch: [129][47/233]	Loss 0.0005 (0.0078)	
training:	Epoch: [129][48/233]	Loss 0.0006 (0.0076)	
training:	Epoch: [129][49/233]	Loss 0.0007 (0.0075)	
training:	Epoch: [129][50/233]	Loss 0.0005 (0.0073)	
training:	Epoch: [129][51/233]	Loss 0.0004 (0.0072)	
training:	Epoch: [129][52/233]	Loss 0.0005 (0.0071)	
training:	Epoch: [129][53/233]	Loss 0.0007 (0.0069)	
training:	Epoch: [129][54/233]	Loss 0.0006 (0.0068)	
training:	Epoch: [129][55/233]	Loss 0.0081 (0.0068)	
training:	Epoch: [129][56/233]	Loss 0.0440 (0.0075)	
training:	Epoch: [129][57/233]	Loss 0.0015 (0.0074)	
training:	Epoch: [129][58/233]	Loss 0.0009 (0.0073)	
training:	Epoch: [129][59/233]	Loss 0.0006 (0.0072)	
training:	Epoch: [129][60/233]	Loss 0.0107 (0.0072)	
training:	Epoch: [129][61/233]	Loss 0.0005 (0.0071)	
training:	Epoch: [129][62/233]	Loss 0.0006 (0.0070)	
training:	Epoch: [129][63/233]	Loss 0.0006 (0.0069)	
training:	Epoch: [129][64/233]	Loss 0.0006 (0.0068)	
training:	Epoch: [129][65/233]	Loss 0.0005 (0.0067)	
training:	Epoch: [129][66/233]	Loss 0.0005 (0.0066)	
training:	Epoch: [129][67/233]	Loss 0.0021 (0.0066)	
training:	Epoch: [129][68/233]	Loss 0.0007 (0.0065)	
training:	Epoch: [129][69/233]	Loss 0.0008 (0.0064)	
training:	Epoch: [129][70/233]	Loss 0.0006 (0.0063)	
training:	Epoch: [129][71/233]	Loss 0.0006 (0.0062)	
training:	Epoch: [129][72/233]	Loss 0.0008 (0.0062)	
training:	Epoch: [129][73/233]	Loss 0.0006 (0.0061)	
training:	Epoch: [129][74/233]	Loss 0.0006 (0.0060)	
training:	Epoch: [129][75/233]	Loss 0.0006 (0.0059)	
training:	Epoch: [129][76/233]	Loss 0.0005 (0.0059)	
training:	Epoch: [129][77/233]	Loss 0.0484 (0.0064)	
training:	Epoch: [129][78/233]	Loss 0.0007 (0.0063)	
training:	Epoch: [129][79/233]	Loss 0.0006 (0.0063)	
training:	Epoch: [129][80/233]	Loss 0.0011 (0.0062)	
training:	Epoch: [129][81/233]	Loss 0.0007 (0.0061)	
training:	Epoch: [129][82/233]	Loss 0.0005 (0.0061)	
training:	Epoch: [129][83/233]	Loss 0.0008 (0.0060)	
training:	Epoch: [129][84/233]	Loss 0.0012 (0.0059)	
training:	Epoch: [129][85/233]	Loss 0.0148 (0.0060)	
training:	Epoch: [129][86/233]	Loss 0.0005 (0.0060)	
training:	Epoch: [129][87/233]	Loss 0.0008 (0.0059)	
training:	Epoch: [129][88/233]	Loss 0.0057 (0.0059)	
training:	Epoch: [129][89/233]	Loss 0.0006 (0.0059)	
training:	Epoch: [129][90/233]	Loss 0.0005 (0.0058)	
training:	Epoch: [129][91/233]	Loss 0.0006 (0.0057)	
training:	Epoch: [129][92/233]	Loss 0.0009 (0.0057)	
training:	Epoch: [129][93/233]	Loss 0.0007 (0.0056)	
training:	Epoch: [129][94/233]	Loss 0.0007 (0.0056)	
training:	Epoch: [129][95/233]	Loss 0.0008 (0.0055)	
training:	Epoch: [129][96/233]	Loss 0.0011 (0.0055)	
training:	Epoch: [129][97/233]	Loss 0.0006 (0.0054)	
training:	Epoch: [129][98/233]	Loss 0.0004 (0.0054)	
training:	Epoch: [129][99/233]	Loss 0.0005 (0.0053)	
training:	Epoch: [129][100/233]	Loss 0.0005 (0.0053)	
training:	Epoch: [129][101/233]	Loss 0.0005 (0.0052)	
training:	Epoch: [129][102/233]	Loss 0.0005 (0.0052)	
training:	Epoch: [129][103/233]	Loss 0.0007 (0.0052)	
training:	Epoch: [129][104/233]	Loss 0.0006 (0.0051)	
training:	Epoch: [129][105/233]	Loss 0.0005 (0.0051)	
training:	Epoch: [129][106/233]	Loss 0.0012 (0.0050)	
training:	Epoch: [129][107/233]	Loss 0.0006 (0.0050)	
training:	Epoch: [129][108/233]	Loss 0.0005 (0.0049)	
training:	Epoch: [129][109/233]	Loss 0.0004 (0.0049)	
training:	Epoch: [129][110/233]	Loss 0.0028 (0.0049)	
training:	Epoch: [129][111/233]	Loss 0.0026 (0.0049)	
training:	Epoch: [129][112/233]	Loss 0.0011 (0.0048)	
training:	Epoch: [129][113/233]	Loss 0.0023 (0.0048)	
training:	Epoch: [129][114/233]	Loss 0.0005 (0.0048)	
training:	Epoch: [129][115/233]	Loss 0.0019 (0.0047)	
training:	Epoch: [129][116/233]	Loss 0.0020 (0.0047)	
training:	Epoch: [129][117/233]	Loss 0.0009 (0.0047)	
training:	Epoch: [129][118/233]	Loss 0.0006 (0.0047)	
training:	Epoch: [129][119/233]	Loss 0.0011 (0.0046)	
training:	Epoch: [129][120/233]	Loss 0.0006 (0.0046)	
training:	Epoch: [129][121/233]	Loss 0.0005 (0.0046)	
training:	Epoch: [129][122/233]	Loss 0.0009 (0.0045)	
training:	Epoch: [129][123/233]	Loss 0.0007 (0.0045)	
training:	Epoch: [129][124/233]	Loss 0.0008 (0.0045)	
training:	Epoch: [129][125/233]	Loss 0.0006 (0.0044)	
training:	Epoch: [129][126/233]	Loss 0.0006 (0.0044)	
training:	Epoch: [129][127/233]	Loss 0.0006 (0.0044)	
training:	Epoch: [129][128/233]	Loss 0.0005 (0.0043)	
training:	Epoch: [129][129/233]	Loss 0.0019 (0.0043)	
training:	Epoch: [129][130/233]	Loss 0.0015 (0.0043)	
training:	Epoch: [129][131/233]	Loss 0.0005 (0.0043)	
training:	Epoch: [129][132/233]	Loss 0.0006 (0.0042)	
training:	Epoch: [129][133/233]	Loss 0.0005 (0.0042)	
training:	Epoch: [129][134/233]	Loss 0.0005 (0.0042)	
training:	Epoch: [129][135/233]	Loss 0.0011 (0.0042)	
training:	Epoch: [129][136/233]	Loss 0.0007 (0.0041)	
training:	Epoch: [129][137/233]	Loss 0.0007 (0.0041)	
training:	Epoch: [129][138/233]	Loss 0.0013 (0.0041)	
training:	Epoch: [129][139/233]	Loss 0.0023 (0.0041)	
training:	Epoch: [129][140/233]	Loss 0.0005 (0.0041)	
training:	Epoch: [129][141/233]	Loss 0.0005 (0.0040)	
training:	Epoch: [129][142/233]	Loss 0.0006 (0.0040)	
training:	Epoch: [129][143/233]	Loss 0.0007 (0.0040)	
training:	Epoch: [129][144/233]	Loss 0.0007 (0.0040)	
training:	Epoch: [129][145/233]	Loss 0.0005 (0.0039)	
training:	Epoch: [129][146/233]	Loss 0.0028 (0.0039)	
training:	Epoch: [129][147/233]	Loss 0.0006 (0.0039)	
training:	Epoch: [129][148/233]	Loss 0.0004 (0.0039)	
training:	Epoch: [129][149/233]	Loss 0.0004 (0.0039)	
training:	Epoch: [129][150/233]	Loss 0.0007 (0.0038)	
training:	Epoch: [129][151/233]	Loss 0.0005 (0.0038)	
training:	Epoch: [129][152/233]	Loss 0.0006 (0.0038)	
training:	Epoch: [129][153/233]	Loss 0.0007 (0.0038)	
training:	Epoch: [129][154/233]	Loss 0.0006 (0.0038)	
training:	Epoch: [129][155/233]	Loss 0.0009 (0.0037)	
training:	Epoch: [129][156/233]	Loss 0.0005 (0.0037)	
training:	Epoch: [129][157/233]	Loss 0.0004 (0.0037)	
training:	Epoch: [129][158/233]	Loss 0.0014 (0.0037)	
training:	Epoch: [129][159/233]	Loss 0.0027 (0.0037)	
training:	Epoch: [129][160/233]	Loss 0.0012 (0.0037)	
training:	Epoch: [129][161/233]	Loss 0.0011 (0.0036)	
training:	Epoch: [129][162/233]	Loss 0.0005 (0.0036)	
training:	Epoch: [129][163/233]	Loss 0.0008 (0.0036)	
training:	Epoch: [129][164/233]	Loss 0.0006 (0.0036)	
training:	Epoch: [129][165/233]	Loss 0.0149 (0.0037)	
training:	Epoch: [129][166/233]	Loss 0.0005 (0.0036)	
training:	Epoch: [129][167/233]	Loss 0.0004 (0.0036)	
training:	Epoch: [129][168/233]	Loss 0.0006 (0.0036)	
training:	Epoch: [129][169/233]	Loss 0.0005 (0.0036)	
training:	Epoch: [129][170/233]	Loss 0.0009 (0.0036)	
training:	Epoch: [129][171/233]	Loss 0.0007 (0.0035)	
training:	Epoch: [129][172/233]	Loss 0.0004 (0.0035)	
training:	Epoch: [129][173/233]	Loss 0.0016 (0.0035)	
training:	Epoch: [129][174/233]	Loss 0.0009 (0.0035)	
training:	Epoch: [129][175/233]	Loss 0.0006 (0.0035)	
training:	Epoch: [129][176/233]	Loss 0.0004 (0.0035)	
training:	Epoch: [129][177/233]	Loss 0.0006 (0.0035)	
training:	Epoch: [129][178/233]	Loss 0.0006 (0.0034)	
training:	Epoch: [129][179/233]	Loss 0.0012 (0.0034)	
training:	Epoch: [129][180/233]	Loss 0.1776 (0.0044)	
training:	Epoch: [129][181/233]	Loss 0.0004 (0.0044)	
training:	Epoch: [129][182/233]	Loss 0.0005 (0.0043)	
training:	Epoch: [129][183/233]	Loss 0.0006 (0.0043)	
training:	Epoch: [129][184/233]	Loss 0.0006 (0.0043)	
training:	Epoch: [129][185/233]	Loss 0.0010 (0.0043)	
training:	Epoch: [129][186/233]	Loss 0.0006 (0.0043)	
training:	Epoch: [129][187/233]	Loss 0.0018 (0.0043)	
training:	Epoch: [129][188/233]	Loss 0.0005 (0.0042)	
training:	Epoch: [129][189/233]	Loss 0.0005 (0.0042)	
training:	Epoch: [129][190/233]	Loss 0.0007 (0.0042)	
training:	Epoch: [129][191/233]	Loss 0.0005 (0.0042)	
training:	Epoch: [129][192/233]	Loss 0.0004 (0.0042)	
training:	Epoch: [129][193/233]	Loss 0.0005 (0.0041)	
training:	Epoch: [129][194/233]	Loss 0.0244 (0.0042)	
training:	Epoch: [129][195/233]	Loss 0.0008 (0.0042)	
training:	Epoch: [129][196/233]	Loss 0.0009 (0.0042)	
training:	Epoch: [129][197/233]	Loss 0.0005 (0.0042)	
training:	Epoch: [129][198/233]	Loss 0.0008 (0.0042)	
training:	Epoch: [129][199/233]	Loss 0.0014 (0.0042)	
training:	Epoch: [129][200/233]	Loss 0.0005 (0.0041)	
training:	Epoch: [129][201/233]	Loss 0.0006 (0.0041)	
training:	Epoch: [129][202/233]	Loss 0.0007 (0.0041)	
training:	Epoch: [129][203/233]	Loss 0.0971 (0.0046)	
training:	Epoch: [129][204/233]	Loss 0.0032 (0.0046)	
training:	Epoch: [129][205/233]	Loss 0.0006 (0.0045)	
training:	Epoch: [129][206/233]	Loss 0.0006 (0.0045)	
training:	Epoch: [129][207/233]	Loss 0.0048 (0.0045)	
training:	Epoch: [129][208/233]	Loss 0.0922 (0.0049)	
training:	Epoch: [129][209/233]	Loss 0.0004 (0.0049)	
training:	Epoch: [129][210/233]	Loss 0.0117 (0.0050)	
training:	Epoch: [129][211/233]	Loss 0.0005 (0.0049)	
training:	Epoch: [129][212/233]	Loss 0.0005 (0.0049)	
training:	Epoch: [129][213/233]	Loss 0.0006 (0.0049)	
training:	Epoch: [129][214/233]	Loss 0.0005 (0.0049)	
training:	Epoch: [129][215/233]	Loss 0.0006 (0.0049)	
training:	Epoch: [129][216/233]	Loss 0.0005 (0.0048)	
training:	Epoch: [129][217/233]	Loss 0.0012 (0.0048)	
training:	Epoch: [129][218/233]	Loss 0.0011 (0.0048)	
training:	Epoch: [129][219/233]	Loss 0.0040 (0.0048)	
training:	Epoch: [129][220/233]	Loss 0.0005 (0.0048)	
training:	Epoch: [129][221/233]	Loss 0.0010 (0.0048)	
training:	Epoch: [129][222/233]	Loss 0.0006 (0.0047)	
training:	Epoch: [129][223/233]	Loss 0.0005 (0.0047)	
training:	Epoch: [129][224/233]	Loss 0.0005 (0.0047)	
training:	Epoch: [129][225/233]	Loss 0.0046 (0.0047)	
training:	Epoch: [129][226/233]	Loss 0.0006 (0.0047)	
training:	Epoch: [129][227/233]	Loss 0.0009 (0.0047)	
training:	Epoch: [129][228/233]	Loss 0.0004 (0.0046)	
training:	Epoch: [129][229/233]	Loss 0.0005 (0.0046)	
training:	Epoch: [129][230/233]	Loss 0.0004 (0.0046)	
training:	Epoch: [129][231/233]	Loss 0.0008 (0.0046)	
training:	Epoch: [129][232/233]	Loss 0.0009 (0.0046)	
training:	Epoch: [129][233/233]	Loss 0.0004 (0.0046)	
Training:	 Loss: 0.0046

Training:	 ACC: 0.9999 0.9999 1.0000 0.9997
Validation:	 ACC: 0.7872 0.7871 0.7845 0.7899
Validation:	 Best_BACC: 0.8162 0.8154 0.7998 0.8326
Validation:	 Loss: 1.1923
Pretraining:	Epoch 130/200
----------
training:	Epoch: [130][1/233]	Loss 0.0006 (0.0006)	
training:	Epoch: [130][2/233]	Loss 0.0006 (0.0006)	
training:	Epoch: [130][3/233]	Loss 0.0004 (0.0005)	
training:	Epoch: [130][4/233]	Loss 0.0005 (0.0005)	
training:	Epoch: [130][5/233]	Loss 0.0008 (0.0006)	
training:	Epoch: [130][6/233]	Loss 0.0006 (0.0006)	
training:	Epoch: [130][7/233]	Loss 0.0006 (0.0006)	
training:	Epoch: [130][8/233]	Loss 0.0006 (0.0006)	
training:	Epoch: [130][9/233]	Loss 0.0223 (0.0030)	
training:	Epoch: [130][10/233]	Loss 0.0005 (0.0028)	
training:	Epoch: [130][11/233]	Loss 0.0005 (0.0025)	
training:	Epoch: [130][12/233]	Loss 0.0007 (0.0024)	
training:	Epoch: [130][13/233]	Loss 0.0004 (0.0022)	
training:	Epoch: [130][14/233]	Loss 0.0004 (0.0021)	
training:	Epoch: [130][15/233]	Loss 0.0017 (0.0021)	
training:	Epoch: [130][16/233]	Loss 0.0006 (0.0020)	
training:	Epoch: [130][17/233]	Loss 0.0006 (0.0019)	
training:	Epoch: [130][18/233]	Loss 0.0011 (0.0019)	
training:	Epoch: [130][19/233]	Loss 0.0006 (0.0018)	
training:	Epoch: [130][20/233]	Loss 0.0019 (0.0018)	
training:	Epoch: [130][21/233]	Loss 0.0032 (0.0019)	
training:	Epoch: [130][22/233]	Loss 0.0005 (0.0018)	
training:	Epoch: [130][23/233]	Loss 0.0006 (0.0018)	
training:	Epoch: [130][24/233]	Loss 0.0007 (0.0017)	
training:	Epoch: [130][25/233]	Loss 0.0576 (0.0040)	
training:	Epoch: [130][26/233]	Loss 0.0004 (0.0038)	
training:	Epoch: [130][27/233]	Loss 0.0004 (0.0037)	
training:	Epoch: [130][28/233]	Loss 0.0016 (0.0036)	
training:	Epoch: [130][29/233]	Loss 0.0043 (0.0036)	
training:	Epoch: [130][30/233]	Loss 0.0025 (0.0036)	
training:	Epoch: [130][31/233]	Loss 0.0007 (0.0035)	
training:	Epoch: [130][32/233]	Loss 0.0006 (0.0034)	
training:	Epoch: [130][33/233]	Loss 0.0008 (0.0033)	
training:	Epoch: [130][34/233]	Loss 0.0005 (0.0033)	
training:	Epoch: [130][35/233]	Loss 0.0005 (0.0032)	
training:	Epoch: [130][36/233]	Loss 0.0007 (0.0031)	
training:	Epoch: [130][37/233]	Loss 0.0010 (0.0030)	
training:	Epoch: [130][38/233]	Loss 0.0004 (0.0030)	
training:	Epoch: [130][39/233]	Loss 0.0005 (0.0029)	
training:	Epoch: [130][40/233]	Loss 0.0006 (0.0029)	
training:	Epoch: [130][41/233]	Loss 0.0005 (0.0028)	
training:	Epoch: [130][42/233]	Loss 0.0007 (0.0027)	
training:	Epoch: [130][43/233]	Loss 0.0008 (0.0027)	
training:	Epoch: [130][44/233]	Loss 0.0007 (0.0027)	
training:	Epoch: [130][45/233]	Loss 0.0005 (0.0026)	
training:	Epoch: [130][46/233]	Loss 0.0004 (0.0026)	
training:	Epoch: [130][47/233]	Loss 0.0014 (0.0025)	
training:	Epoch: [130][48/233]	Loss 0.0009 (0.0025)	
training:	Epoch: [130][49/233]	Loss 0.0426 (0.0033)	
training:	Epoch: [130][50/233]	Loss 0.0015 (0.0033)	
training:	Epoch: [130][51/233]	Loss 0.0008 (0.0032)	
training:	Epoch: [130][52/233]	Loss 0.0005 (0.0032)	
training:	Epoch: [130][53/233]	Loss 0.0006 (0.0031)	
training:	Epoch: [130][54/233]	Loss 0.0004 (0.0031)	
training:	Epoch: [130][55/233]	Loss 0.0005 (0.0030)	
training:	Epoch: [130][56/233]	Loss 0.0005 (0.0030)	
training:	Epoch: [130][57/233]	Loss 0.0005 (0.0029)	
training:	Epoch: [130][58/233]	Loss 0.0022 (0.0029)	
training:	Epoch: [130][59/233]	Loss 0.0036 (0.0029)	
training:	Epoch: [130][60/233]	Loss 0.0006 (0.0029)	
training:	Epoch: [130][61/233]	Loss 0.0006 (0.0029)	
training:	Epoch: [130][62/233]	Loss 0.0005 (0.0028)	
training:	Epoch: [130][63/233]	Loss 0.0005 (0.0028)	
training:	Epoch: [130][64/233]	Loss 0.0005 (0.0028)	
training:	Epoch: [130][65/233]	Loss 0.0008 (0.0027)	
training:	Epoch: [130][66/233]	Loss 0.0022 (0.0027)	
training:	Epoch: [130][67/233]	Loss 0.0005 (0.0027)	
training:	Epoch: [130][68/233]	Loss 0.0021 (0.0027)	
training:	Epoch: [130][69/233]	Loss 0.0005 (0.0026)	
training:	Epoch: [130][70/233]	Loss 0.0007 (0.0026)	
training:	Epoch: [130][71/233]	Loss 0.0008 (0.0026)	
training:	Epoch: [130][72/233]	Loss 0.0006 (0.0026)	
training:	Epoch: [130][73/233]	Loss 0.0072 (0.0026)	
training:	Epoch: [130][74/233]	Loss 0.0005 (0.0026)	
training:	Epoch: [130][75/233]	Loss 0.0564 (0.0033)	
training:	Epoch: [130][76/233]	Loss 0.2329 (0.0063)	
training:	Epoch: [130][77/233]	Loss 0.0004 (0.0063)	
training:	Epoch: [130][78/233]	Loss 0.0012 (0.0062)	
training:	Epoch: [130][79/233]	Loss 0.0009 (0.0061)	
training:	Epoch: [130][80/233]	Loss 0.0006 (0.0061)	
training:	Epoch: [130][81/233]	Loss 0.0005 (0.0060)	
training:	Epoch: [130][82/233]	Loss 0.0004 (0.0059)	
training:	Epoch: [130][83/233]	Loss 0.0012 (0.0059)	
training:	Epoch: [130][84/233]	Loss 0.0055 (0.0059)	
training:	Epoch: [130][85/233]	Loss 0.0004 (0.0058)	
training:	Epoch: [130][86/233]	Loss 0.0030 (0.0058)	
training:	Epoch: [130][87/233]	Loss 0.0004 (0.0057)	
training:	Epoch: [130][88/233]	Loss 0.0007 (0.0056)	
training:	Epoch: [130][89/233]	Loss 0.0021 (0.0056)	
training:	Epoch: [130][90/233]	Loss 0.0007 (0.0056)	
training:	Epoch: [130][91/233]	Loss 0.0006 (0.0055)	
training:	Epoch: [130][92/233]	Loss 0.0005 (0.0054)	
training:	Epoch: [130][93/233]	Loss 0.0804 (0.0063)	
training:	Epoch: [130][94/233]	Loss 0.0008 (0.0062)	
training:	Epoch: [130][95/233]	Loss 0.0007 (0.0061)	
training:	Epoch: [130][96/233]	Loss 0.0005 (0.0061)	
training:	Epoch: [130][97/233]	Loss 0.0009 (0.0060)	
training:	Epoch: [130][98/233]	Loss 0.0007 (0.0060)	
training:	Epoch: [130][99/233]	Loss 0.0005 (0.0059)	
training:	Epoch: [130][100/233]	Loss 0.0009 (0.0059)	
training:	Epoch: [130][101/233]	Loss 0.0004 (0.0058)	
training:	Epoch: [130][102/233]	Loss 0.0009 (0.0058)	
training:	Epoch: [130][103/233]	Loss 0.0009 (0.0057)	
training:	Epoch: [130][104/233]	Loss 0.0005 (0.0057)	
training:	Epoch: [130][105/233]	Loss 0.0005 (0.0056)	
training:	Epoch: [130][106/233]	Loss 0.0009 (0.0056)	
training:	Epoch: [130][107/233]	Loss 0.0312 (0.0058)	
training:	Epoch: [130][108/233]	Loss 0.0005 (0.0058)	
training:	Epoch: [130][109/233]	Loss 0.0005 (0.0057)	
training:	Epoch: [130][110/233]	Loss 0.0461 (0.0061)	
training:	Epoch: [130][111/233]	Loss 0.0008 (0.0060)	
training:	Epoch: [130][112/233]	Loss 0.0006 (0.0060)	
training:	Epoch: [130][113/233]	Loss 0.0006 (0.0059)	
training:	Epoch: [130][114/233]	Loss 0.0044 (0.0059)	
training:	Epoch: [130][115/233]	Loss 0.0006 (0.0059)	
training:	Epoch: [130][116/233]	Loss 0.0005 (0.0058)	
training:	Epoch: [130][117/233]	Loss 0.0009 (0.0058)	
training:	Epoch: [130][118/233]	Loss 0.0006 (0.0057)	
training:	Epoch: [130][119/233]	Loss 0.0005 (0.0057)	
training:	Epoch: [130][120/233]	Loss 0.0005 (0.0057)	
training:	Epoch: [130][121/233]	Loss 0.0007 (0.0056)	
training:	Epoch: [130][122/233]	Loss 0.0005 (0.0056)	
training:	Epoch: [130][123/233]	Loss 0.0005 (0.0055)	
training:	Epoch: [130][124/233]	Loss 0.0005 (0.0055)	
training:	Epoch: [130][125/233]	Loss 0.0005 (0.0055)	
training:	Epoch: [130][126/233]	Loss 0.0026 (0.0054)	
training:	Epoch: [130][127/233]	Loss 0.0005 (0.0054)	
training:	Epoch: [130][128/233]	Loss 0.0006 (0.0054)	
training:	Epoch: [130][129/233]	Loss 0.0016 (0.0053)	
training:	Epoch: [130][130/233]	Loss 0.0006 (0.0053)	
training:	Epoch: [130][131/233]	Loss 0.0005 (0.0053)	
training:	Epoch: [130][132/233]	Loss 0.0007 (0.0052)	
training:	Epoch: [130][133/233]	Loss 0.0005 (0.0052)	
training:	Epoch: [130][134/233]	Loss 0.0005 (0.0051)	
training:	Epoch: [130][135/233]	Loss 0.0009 (0.0051)	
training:	Epoch: [130][136/233]	Loss 0.0057 (0.0051)	
training:	Epoch: [130][137/233]	Loss 0.0005 (0.0051)	
training:	Epoch: [130][138/233]	Loss 0.0012 (0.0051)	
training:	Epoch: [130][139/233]	Loss 0.0005 (0.0050)	
training:	Epoch: [130][140/233]	Loss 0.0020 (0.0050)	
training:	Epoch: [130][141/233]	Loss 0.0006 (0.0050)	
training:	Epoch: [130][142/233]	Loss 0.0006 (0.0049)	
training:	Epoch: [130][143/233]	Loss 0.0005 (0.0049)	
training:	Epoch: [130][144/233]	Loss 0.0004 (0.0049)	
training:	Epoch: [130][145/233]	Loss 0.0009 (0.0049)	
training:	Epoch: [130][146/233]	Loss 0.0005 (0.0048)	
training:	Epoch: [130][147/233]	Loss 0.0069 (0.0048)	
training:	Epoch: [130][148/233]	Loss 0.0005 (0.0048)	
training:	Epoch: [130][149/233]	Loss 0.0017 (0.0048)	
training:	Epoch: [130][150/233]	Loss 0.0009 (0.0048)	
training:	Epoch: [130][151/233]	Loss 0.2344 (0.0063)	
training:	Epoch: [130][152/233]	Loss 0.0004 (0.0062)	
training:	Epoch: [130][153/233]	Loss 0.0006 (0.0062)	
training:	Epoch: [130][154/233]	Loss 0.0006 (0.0062)	
training:	Epoch: [130][155/233]	Loss 0.0016 (0.0061)	
training:	Epoch: [130][156/233]	Loss 0.0004 (0.0061)	
training:	Epoch: [130][157/233]	Loss 0.0006 (0.0061)	
training:	Epoch: [130][158/233]	Loss 0.0005 (0.0060)	
training:	Epoch: [130][159/233]	Loss 0.0005 (0.0060)	
training:	Epoch: [130][160/233]	Loss 0.0007 (0.0060)	
training:	Epoch: [130][161/233]	Loss 0.0020 (0.0059)	
training:	Epoch: [130][162/233]	Loss 0.0124 (0.0060)	
training:	Epoch: [130][163/233]	Loss 0.0005 (0.0059)	
training:	Epoch: [130][164/233]	Loss 0.0011 (0.0059)	
training:	Epoch: [130][165/233]	Loss 0.0005 (0.0059)	
training:	Epoch: [130][166/233]	Loss 0.0008 (0.0059)	
training:	Epoch: [130][167/233]	Loss 0.0006 (0.0058)	
training:	Epoch: [130][168/233]	Loss 0.0005 (0.0058)	
training:	Epoch: [130][169/233]	Loss 0.0017 (0.0058)	
training:	Epoch: [130][170/233]	Loss 0.0005 (0.0057)	
training:	Epoch: [130][171/233]	Loss 0.0005 (0.0057)	
training:	Epoch: [130][172/233]	Loss 0.0006 (0.0057)	
training:	Epoch: [130][173/233]	Loss 0.0005 (0.0056)	
training:	Epoch: [130][174/233]	Loss 0.0011 (0.0056)	
training:	Epoch: [130][175/233]	Loss 0.0006 (0.0056)	
training:	Epoch: [130][176/233]	Loss 0.0005 (0.0056)	
training:	Epoch: [130][177/233]	Loss 0.0004 (0.0055)	
training:	Epoch: [130][178/233]	Loss 0.0017 (0.0055)	
training:	Epoch: [130][179/233]	Loss 0.0005 (0.0055)	
training:	Epoch: [130][180/233]	Loss 0.0005 (0.0055)	
training:	Epoch: [130][181/233]	Loss 0.0023 (0.0054)	
training:	Epoch: [130][182/233]	Loss 0.0006 (0.0054)	
training:	Epoch: [130][183/233]	Loss 0.0022 (0.0054)	
training:	Epoch: [130][184/233]	Loss 0.0007 (0.0054)	
training:	Epoch: [130][185/233]	Loss 0.0013 (0.0053)	
training:	Epoch: [130][186/233]	Loss 0.0005 (0.0053)	
training:	Epoch: [130][187/233]	Loss 0.0005 (0.0053)	
training:	Epoch: [130][188/233]	Loss 0.0007 (0.0053)	
training:	Epoch: [130][189/233]	Loss 0.0061 (0.0053)	
training:	Epoch: [130][190/233]	Loss 0.0007 (0.0052)	
training:	Epoch: [130][191/233]	Loss 0.0005 (0.0052)	
training:	Epoch: [130][192/233]	Loss 0.0012 (0.0052)	
training:	Epoch: [130][193/233]	Loss 0.0006 (0.0052)	
training:	Epoch: [130][194/233]	Loss 0.0005 (0.0052)	
training:	Epoch: [130][195/233]	Loss 0.0060 (0.0052)	
training:	Epoch: [130][196/233]	Loss 0.0084 (0.0052)	
training:	Epoch: [130][197/233]	Loss 0.0005 (0.0052)	
training:	Epoch: [130][198/233]	Loss 0.0005 (0.0051)	
training:	Epoch: [130][199/233]	Loss 0.0006 (0.0051)	
training:	Epoch: [130][200/233]	Loss 0.0004 (0.0051)	
training:	Epoch: [130][201/233]	Loss 0.0008 (0.0051)	
training:	Epoch: [130][202/233]	Loss 0.0021 (0.0050)	
training:	Epoch: [130][203/233]	Loss 0.0005 (0.0050)	
training:	Epoch: [130][204/233]	Loss 0.0005 (0.0050)	
training:	Epoch: [130][205/233]	Loss 0.0004 (0.0050)	
training:	Epoch: [130][206/233]	Loss 0.0005 (0.0050)	
training:	Epoch: [130][207/233]	Loss 0.0006 (0.0049)	
training:	Epoch: [130][208/233]	Loss 0.0007 (0.0049)	
training:	Epoch: [130][209/233]	Loss 0.0004 (0.0049)	
training:	Epoch: [130][210/233]	Loss 0.0033 (0.0049)	
training:	Epoch: [130][211/233]	Loss 0.0005 (0.0049)	
training:	Epoch: [130][212/233]	Loss 0.0060 (0.0049)	
training:	Epoch: [130][213/233]	Loss 0.0007 (0.0049)	
training:	Epoch: [130][214/233]	Loss 0.0005 (0.0048)	
training:	Epoch: [130][215/233]	Loss 0.0048 (0.0048)	
training:	Epoch: [130][216/233]	Loss 0.0020 (0.0048)	
training:	Epoch: [130][217/233]	Loss 0.0005 (0.0048)	
training:	Epoch: [130][218/233]	Loss 0.0013 (0.0048)	
training:	Epoch: [130][219/233]	Loss 0.0006 (0.0048)	
training:	Epoch: [130][220/233]	Loss 0.0004 (0.0047)	
training:	Epoch: [130][221/233]	Loss 0.0005 (0.0047)	
training:	Epoch: [130][222/233]	Loss 0.0004 (0.0047)	
training:	Epoch: [130][223/233]	Loss 0.0005 (0.0047)	
training:	Epoch: [130][224/233]	Loss 0.0006 (0.0047)	
training:	Epoch: [130][225/233]	Loss 0.0383 (0.0048)	
training:	Epoch: [130][226/233]	Loss 0.0005 (0.0048)	
training:	Epoch: [130][227/233]	Loss 0.0009 (0.0048)	
training:	Epoch: [130][228/233]	Loss 0.0004 (0.0048)	
training:	Epoch: [130][229/233]	Loss 0.0004 (0.0047)	
training:	Epoch: [130][230/233]	Loss 0.0007 (0.0047)	
training:	Epoch: [130][231/233]	Loss 0.0005 (0.0047)	
training:	Epoch: [130][232/233]	Loss 0.0005 (0.0047)	
training:	Epoch: [130][233/233]	Loss 0.0021 (0.0047)	
Training:	 Loss: 0.0047

Training:	 ACC: 0.9999 0.9999 1.0000 0.9997
Validation:	 ACC: 0.7960 0.7940 0.7538 0.8382
Validation:	 Best_BACC: 0.8162 0.8154 0.7998 0.8326
Validation:	 Loss: 1.2220
Pretraining:	Epoch 131/200
----------
training:	Epoch: [131][1/233]	Loss 0.0007 (0.0007)	
training:	Epoch: [131][2/233]	Loss 0.0049 (0.0028)	
training:	Epoch: [131][3/233]	Loss 0.0006 (0.0021)	
training:	Epoch: [131][4/233]	Loss 0.0008 (0.0017)	
training:	Epoch: [131][5/233]	Loss 0.0102 (0.0034)	
training:	Epoch: [131][6/233]	Loss 0.0005 (0.0029)	
training:	Epoch: [131][7/233]	Loss 0.0005 (0.0026)	
training:	Epoch: [131][8/233]	Loss 0.0005 (0.0023)	
training:	Epoch: [131][9/233]	Loss 0.0006 (0.0021)	
training:	Epoch: [131][10/233]	Loss 0.0018 (0.0021)	
training:	Epoch: [131][11/233]	Loss 0.0008 (0.0020)	
training:	Epoch: [131][12/233]	Loss 0.0005 (0.0019)	
training:	Epoch: [131][13/233]	Loss 0.0010 (0.0018)	
training:	Epoch: [131][14/233]	Loss 0.0005 (0.0017)	
training:	Epoch: [131][15/233]	Loss 0.0005 (0.0016)	
training:	Epoch: [131][16/233]	Loss 0.0004 (0.0015)	
training:	Epoch: [131][17/233]	Loss 0.0005 (0.0015)	
training:	Epoch: [131][18/233]	Loss 0.0004 (0.0014)	
training:	Epoch: [131][19/233]	Loss 0.0005 (0.0014)	
training:	Epoch: [131][20/233]	Loss 0.0005 (0.0013)	
training:	Epoch: [131][21/233]	Loss 0.0005 (0.0013)	
training:	Epoch: [131][22/233]	Loss 0.0004 (0.0013)	
training:	Epoch: [131][23/233]	Loss 0.0004 (0.0012)	
training:	Epoch: [131][24/233]	Loss 0.0006 (0.0012)	
training:	Epoch: [131][25/233]	Loss 0.0280 (0.0023)	
training:	Epoch: [131][26/233]	Loss 0.0008 (0.0022)	
training:	Epoch: [131][27/233]	Loss 0.0006 (0.0021)	
training:	Epoch: [131][28/233]	Loss 0.0006 (0.0021)	
training:	Epoch: [131][29/233]	Loss 0.0006 (0.0020)	
training:	Epoch: [131][30/233]	Loss 0.0011 (0.0020)	
training:	Epoch: [131][31/233]	Loss 0.0004 (0.0020)	
training:	Epoch: [131][32/233]	Loss 0.0004 (0.0019)	
training:	Epoch: [131][33/233]	Loss 0.0006 (0.0019)	
training:	Epoch: [131][34/233]	Loss 0.0009 (0.0018)	
training:	Epoch: [131][35/233]	Loss 0.0005 (0.0018)	
training:	Epoch: [131][36/233]	Loss 0.0011 (0.0018)	
training:	Epoch: [131][37/233]	Loss 0.0004 (0.0018)	
training:	Epoch: [131][38/233]	Loss 0.0004 (0.0017)	
training:	Epoch: [131][39/233]	Loss 0.0005 (0.0017)	
training:	Epoch: [131][40/233]	Loss 0.0065 (0.0018)	
training:	Epoch: [131][41/233]	Loss 0.0052 (0.0019)	
training:	Epoch: [131][42/233]	Loss 0.0008 (0.0019)	
training:	Epoch: [131][43/233]	Loss 0.0007 (0.0018)	
training:	Epoch: [131][44/233]	Loss 0.0015 (0.0018)	
training:	Epoch: [131][45/233]	Loss 0.0005 (0.0018)	
training:	Epoch: [131][46/233]	Loss 0.0004 (0.0018)	
training:	Epoch: [131][47/233]	Loss 0.0005 (0.0017)	
training:	Epoch: [131][48/233]	Loss 0.0009 (0.0017)	
training:	Epoch: [131][49/233]	Loss 0.0005 (0.0017)	
training:	Epoch: [131][50/233]	Loss 0.0005 (0.0017)	
training:	Epoch: [131][51/233]	Loss 0.0004 (0.0017)	
training:	Epoch: [131][52/233]	Loss 0.0005 (0.0016)	
training:	Epoch: [131][53/233]	Loss 0.0009 (0.0016)	
training:	Epoch: [131][54/233]	Loss 0.0005 (0.0016)	
training:	Epoch: [131][55/233]	Loss 0.0008 (0.0016)	
training:	Epoch: [131][56/233]	Loss 0.0006 (0.0016)	
training:	Epoch: [131][57/233]	Loss 0.0005 (0.0015)	
training:	Epoch: [131][58/233]	Loss 0.0004 (0.0015)	
training:	Epoch: [131][59/233]	Loss 0.0004 (0.0015)	
training:	Epoch: [131][60/233]	Loss 0.0008 (0.0015)	
training:	Epoch: [131][61/233]	Loss 0.0009 (0.0015)	
training:	Epoch: [131][62/233]	Loss 0.0012 (0.0015)	
training:	Epoch: [131][63/233]	Loss 0.0004 (0.0015)	
training:	Epoch: [131][64/233]	Loss 0.0005 (0.0014)	
training:	Epoch: [131][65/233]	Loss 0.0005 (0.0014)	
training:	Epoch: [131][66/233]	Loss 0.0005 (0.0014)	
training:	Epoch: [131][67/233]	Loss 0.0005 (0.0014)	
training:	Epoch: [131][68/233]	Loss 0.0007 (0.0014)	
training:	Epoch: [131][69/233]	Loss 0.0004 (0.0014)	
training:	Epoch: [131][70/233]	Loss 0.0007 (0.0014)	
training:	Epoch: [131][71/233]	Loss 0.0005 (0.0014)	
training:	Epoch: [131][72/233]	Loss 0.0007 (0.0013)	
training:	Epoch: [131][73/233]	Loss 0.0013 (0.0013)	
training:	Epoch: [131][74/233]	Loss 0.0005 (0.0013)	
training:	Epoch: [131][75/233]	Loss 0.0005 (0.0013)	
training:	Epoch: [131][76/233]	Loss 0.0007 (0.0013)	
training:	Epoch: [131][77/233]	Loss 0.0005 (0.0013)	
training:	Epoch: [131][78/233]	Loss 0.0005 (0.0013)	
training:	Epoch: [131][79/233]	Loss 0.0007 (0.0013)	
training:	Epoch: [131][80/233]	Loss 0.0007 (0.0013)	
training:	Epoch: [131][81/233]	Loss 0.0005 (0.0013)	
training:	Epoch: [131][82/233]	Loss 0.0006 (0.0013)	
training:	Epoch: [131][83/233]	Loss 0.0005 (0.0013)	
training:	Epoch: [131][84/233]	Loss 0.0005 (0.0012)	
training:	Epoch: [131][85/233]	Loss 0.0004 (0.0012)	
training:	Epoch: [131][86/233]	Loss 0.2139 (0.0037)	
training:	Epoch: [131][87/233]	Loss 0.0004 (0.0037)	
training:	Epoch: [131][88/233]	Loss 0.0006 (0.0036)	
training:	Epoch: [131][89/233]	Loss 0.0004 (0.0036)	
training:	Epoch: [131][90/233]	Loss 0.0007 (0.0036)	
training:	Epoch: [131][91/233]	Loss 0.0004 (0.0035)	
training:	Epoch: [131][92/233]	Loss 0.0005 (0.0035)	
training:	Epoch: [131][93/233]	Loss 0.0013 (0.0035)	
training:	Epoch: [131][94/233]	Loss 0.0005 (0.0034)	
training:	Epoch: [131][95/233]	Loss 0.0004 (0.0034)	
training:	Epoch: [131][96/233]	Loss 0.0005 (0.0034)	
training:	Epoch: [131][97/233]	Loss 0.0004 (0.0034)	
training:	Epoch: [131][98/233]	Loss 0.0005 (0.0033)	
training:	Epoch: [131][99/233]	Loss 0.0007 (0.0033)	
training:	Epoch: [131][100/233]	Loss 0.0015 (0.0033)	
training:	Epoch: [131][101/233]	Loss 0.0035 (0.0033)	
training:	Epoch: [131][102/233]	Loss 0.0005 (0.0033)	
training:	Epoch: [131][103/233]	Loss 0.0005 (0.0032)	
training:	Epoch: [131][104/233]	Loss 0.0004 (0.0032)	
training:	Epoch: [131][105/233]	Loss 0.0004 (0.0032)	
training:	Epoch: [131][106/233]	Loss 0.0007 (0.0031)	
training:	Epoch: [131][107/233]	Loss 0.0022 (0.0031)	
training:	Epoch: [131][108/233]	Loss 0.0052 (0.0032)	
training:	Epoch: [131][109/233]	Loss 0.0156 (0.0033)	
training:	Epoch: [131][110/233]	Loss 0.0653 (0.0038)	
training:	Epoch: [131][111/233]	Loss 0.1208 (0.0049)	
training:	Epoch: [131][112/233]	Loss 0.0006 (0.0049)	
training:	Epoch: [131][113/233]	Loss 0.0021 (0.0048)	
training:	Epoch: [131][114/233]	Loss 0.0009 (0.0048)	
training:	Epoch: [131][115/233]	Loss 0.0010 (0.0048)	
training:	Epoch: [131][116/233]	Loss 0.0027 (0.0047)	
training:	Epoch: [131][117/233]	Loss 0.1284 (0.0058)	
training:	Epoch: [131][118/233]	Loss 0.0017 (0.0058)	
training:	Epoch: [131][119/233]	Loss 0.0004 (0.0057)	
training:	Epoch: [131][120/233]	Loss 0.0090 (0.0057)	
training:	Epoch: [131][121/233]	Loss 0.0080 (0.0058)	
training:	Epoch: [131][122/233]	Loss 0.0064 (0.0058)	
training:	Epoch: [131][123/233]	Loss 0.0008 (0.0057)	
training:	Epoch: [131][124/233]	Loss 0.0007 (0.0057)	
training:	Epoch: [131][125/233]	Loss 0.0005 (0.0057)	
training:	Epoch: [131][126/233]	Loss 0.0005 (0.0056)	
training:	Epoch: [131][127/233]	Loss 0.0823 (0.0062)	
training:	Epoch: [131][128/233]	Loss 0.0012 (0.0062)	
training:	Epoch: [131][129/233]	Loss 0.0011 (0.0061)	
training:	Epoch: [131][130/233]	Loss 0.0237 (0.0063)	
training:	Epoch: [131][131/233]	Loss 0.0006 (0.0062)	
training:	Epoch: [131][132/233]	Loss 0.0010 (0.0062)	
training:	Epoch: [131][133/233]	Loss 0.0006 (0.0061)	
training:	Epoch: [131][134/233]	Loss 0.0048 (0.0061)	
training:	Epoch: [131][135/233]	Loss 0.0008 (0.0061)	
training:	Epoch: [131][136/233]	Loss 0.0026 (0.0061)	
training:	Epoch: [131][137/233]	Loss 0.0194 (0.0062)	
training:	Epoch: [131][138/233]	Loss 0.0006 (0.0061)	
training:	Epoch: [131][139/233]	Loss 0.0011 (0.0061)	
training:	Epoch: [131][140/233]	Loss 0.0045 (0.0061)	
training:	Epoch: [131][141/233]	Loss 0.0013 (0.0060)	
training:	Epoch: [131][142/233]	Loss 0.0007 (0.0060)	
training:	Epoch: [131][143/233]	Loss 0.0016 (0.0060)	
training:	Epoch: [131][144/233]	Loss 0.0004 (0.0059)	
training:	Epoch: [131][145/233]	Loss 0.0007 (0.0059)	
training:	Epoch: [131][146/233]	Loss 0.0007 (0.0059)	
training:	Epoch: [131][147/233]	Loss 0.0023 (0.0058)	
training:	Epoch: [131][148/233]	Loss 0.0005 (0.0058)	
training:	Epoch: [131][149/233]	Loss 0.0055 (0.0058)	
training:	Epoch: [131][150/233]	Loss 0.0007 (0.0058)	
training:	Epoch: [131][151/233]	Loss 0.0009 (0.0057)	
training:	Epoch: [131][152/233]	Loss 0.0006 (0.0057)	
training:	Epoch: [131][153/233]	Loss 0.0176 (0.0058)	
training:	Epoch: [131][154/233]	Loss 0.0006 (0.0057)	
training:	Epoch: [131][155/233]	Loss 0.0007 (0.0057)	
training:	Epoch: [131][156/233]	Loss 0.0009 (0.0057)	
training:	Epoch: [131][157/233]	Loss 0.0004 (0.0057)	
training:	Epoch: [131][158/233]	Loss 0.0017 (0.0056)	
training:	Epoch: [131][159/233]	Loss 0.0007 (0.0056)	
training:	Epoch: [131][160/233]	Loss 0.0005 (0.0056)	
training:	Epoch: [131][161/233]	Loss 0.0006 (0.0055)	
training:	Epoch: [131][162/233]	Loss 0.0006 (0.0055)	
training:	Epoch: [131][163/233]	Loss 0.0010 (0.0055)	
training:	Epoch: [131][164/233]	Loss 0.0005 (0.0054)	
training:	Epoch: [131][165/233]	Loss 0.0054 (0.0054)	
training:	Epoch: [131][166/233]	Loss 0.0921 (0.0060)	
training:	Epoch: [131][167/233]	Loss 0.0005 (0.0059)	
training:	Epoch: [131][168/233]	Loss 0.0005 (0.0059)	
training:	Epoch: [131][169/233]	Loss 0.0007 (0.0059)	
training:	Epoch: [131][170/233]	Loss 0.0005 (0.0058)	
training:	Epoch: [131][171/233]	Loss 0.0004 (0.0058)	
training:	Epoch: [131][172/233]	Loss 0.0005 (0.0058)	
training:	Epoch: [131][173/233]	Loss 0.0045 (0.0058)	
training:	Epoch: [131][174/233]	Loss 0.0035 (0.0058)	
training:	Epoch: [131][175/233]	Loss 0.0006 (0.0057)	
training:	Epoch: [131][176/233]	Loss 0.0005 (0.0057)	
training:	Epoch: [131][177/233]	Loss 0.2469 (0.0071)	
training:	Epoch: [131][178/233]	Loss 0.0011 (0.0070)	
training:	Epoch: [131][179/233]	Loss 0.0009 (0.0070)	
training:	Epoch: [131][180/233]	Loss 0.0005 (0.0070)	
training:	Epoch: [131][181/233]	Loss 0.0277 (0.0071)	
training:	Epoch: [131][182/233]	Loss 0.0013 (0.0070)	
training:	Epoch: [131][183/233]	Loss 0.0005 (0.0070)	
training:	Epoch: [131][184/233]	Loss 0.0005 (0.0070)	
training:	Epoch: [131][185/233]	Loss 0.0008 (0.0069)	
training:	Epoch: [131][186/233]	Loss 0.0044 (0.0069)	
training:	Epoch: [131][187/233]	Loss 0.0378 (0.0071)	
training:	Epoch: [131][188/233]	Loss 0.0005 (0.0071)	
training:	Epoch: [131][189/233]	Loss 0.0006 (0.0070)	
training:	Epoch: [131][190/233]	Loss 0.0020 (0.0070)	
training:	Epoch: [131][191/233]	Loss 0.0014 (0.0070)	
training:	Epoch: [131][192/233]	Loss 0.0005 (0.0069)	
training:	Epoch: [131][193/233]	Loss 0.0005 (0.0069)	
training:	Epoch: [131][194/233]	Loss 0.0631 (0.0072)	
training:	Epoch: [131][195/233]	Loss 0.0006 (0.0072)	
training:	Epoch: [131][196/233]	Loss 0.0007 (0.0071)	
training:	Epoch: [131][197/233]	Loss 0.0013 (0.0071)	
training:	Epoch: [131][198/233]	Loss 0.0005 (0.0071)	
training:	Epoch: [131][199/233]	Loss 0.0006 (0.0070)	
training:	Epoch: [131][200/233]	Loss 0.0004 (0.0070)	
training:	Epoch: [131][201/233]	Loss 0.0005 (0.0070)	
training:	Epoch: [131][202/233]	Loss 0.0011 (0.0069)	
training:	Epoch: [131][203/233]	Loss 0.0027 (0.0069)	
training:	Epoch: [131][204/233]	Loss 0.0006 (0.0069)	
training:	Epoch: [131][205/233]	Loss 0.0010 (0.0068)	
training:	Epoch: [131][206/233]	Loss 0.0016 (0.0068)	
training:	Epoch: [131][207/233]	Loss 0.0007 (0.0068)	
training:	Epoch: [131][208/233]	Loss 0.0005 (0.0068)	
training:	Epoch: [131][209/233]	Loss 0.0006 (0.0067)	
training:	Epoch: [131][210/233]	Loss 0.0082 (0.0067)	
training:	Epoch: [131][211/233]	Loss 0.0007 (0.0067)	
training:	Epoch: [131][212/233]	Loss 0.0041 (0.0067)	
training:	Epoch: [131][213/233]	Loss 0.0007 (0.0067)	
training:	Epoch: [131][214/233]	Loss 0.0008 (0.0066)	
training:	Epoch: [131][215/233]	Loss 0.0006 (0.0066)	
training:	Epoch: [131][216/233]	Loss 0.0004 (0.0066)	
training:	Epoch: [131][217/233]	Loss 0.0006 (0.0066)	
training:	Epoch: [131][218/233]	Loss 0.0007 (0.0065)	
training:	Epoch: [131][219/233]	Loss 0.0007 (0.0065)	
training:	Epoch: [131][220/233]	Loss 0.0820 (0.0068)	
training:	Epoch: [131][221/233]	Loss 0.0007 (0.0068)	
training:	Epoch: [131][222/233]	Loss 0.0187 (0.0069)	
training:	Epoch: [131][223/233]	Loss 0.0017 (0.0069)	
training:	Epoch: [131][224/233]	Loss 0.0006 (0.0068)	
training:	Epoch: [131][225/233]	Loss 0.0005 (0.0068)	
training:	Epoch: [131][226/233]	Loss 0.0011 (0.0068)	
training:	Epoch: [131][227/233]	Loss 0.1627 (0.0075)	
training:	Epoch: [131][228/233]	Loss 0.0018 (0.0074)	
training:	Epoch: [131][229/233]	Loss 0.0005 (0.0074)	
training:	Epoch: [131][230/233]	Loss 0.0081 (0.0074)	
training:	Epoch: [131][231/233]	Loss 0.0006 (0.0074)	
training:	Epoch: [131][232/233]	Loss 0.0008 (0.0073)	
training:	Epoch: [131][233/233]	Loss 0.0012 (0.0073)	
Training:	 Loss: 0.0073

Training:	 ACC: 0.9996 0.9996 1.0000 0.9992
Validation:	 ACC: 0.7917 0.7940 0.8407 0.7427
Validation:	 Best_BACC: 0.8162 0.8154 0.7998 0.8326
Validation:	 Loss: 1.1545
Pretraining:	Epoch 132/200
----------
training:	Epoch: [132][1/233]	Loss 0.0274 (0.0274)	
training:	Epoch: [132][2/233]	Loss 0.0005 (0.0139)	
training:	Epoch: [132][3/233]	Loss 0.0011 (0.0096)	
training:	Epoch: [132][4/233]	Loss 0.0009 (0.0075)	
training:	Epoch: [132][5/233]	Loss 0.0004 (0.0061)	
training:	Epoch: [132][6/233]	Loss 0.0005 (0.0051)	
training:	Epoch: [132][7/233]	Loss 0.0004 (0.0044)	
training:	Epoch: [132][8/233]	Loss 0.0008 (0.0040)	
training:	Epoch: [132][9/233]	Loss 0.0005 (0.0036)	
training:	Epoch: [132][10/233]	Loss 0.0008 (0.0033)	
training:	Epoch: [132][11/233]	Loss 0.0005 (0.0031)	
training:	Epoch: [132][12/233]	Loss 0.0007 (0.0029)	
training:	Epoch: [132][13/233]	Loss 0.0006 (0.0027)	
training:	Epoch: [132][14/233]	Loss 0.0007 (0.0026)	
training:	Epoch: [132][15/233]	Loss 0.1273 (0.0109)	
training:	Epoch: [132][16/233]	Loss 0.0008 (0.0103)	
training:	Epoch: [132][17/233]	Loss 0.0011 (0.0097)	
training:	Epoch: [132][18/233]	Loss 0.0006 (0.0092)	
training:	Epoch: [132][19/233]	Loss 0.0010 (0.0088)	
training:	Epoch: [132][20/233]	Loss 0.0005 (0.0084)	
training:	Epoch: [132][21/233]	Loss 0.0006 (0.0080)	
training:	Epoch: [132][22/233]	Loss 0.0013 (0.0077)	
training:	Epoch: [132][23/233]	Loss 0.0008 (0.0074)	
training:	Epoch: [132][24/233]	Loss 0.0004 (0.0071)	
training:	Epoch: [132][25/233]	Loss 0.0018 (0.0069)	
training:	Epoch: [132][26/233]	Loss 0.0008 (0.0067)	
training:	Epoch: [132][27/233]	Loss 0.0007 (0.0064)	
training:	Epoch: [132][28/233]	Loss 0.0007 (0.0062)	
training:	Epoch: [132][29/233]	Loss 0.0040 (0.0062)	
training:	Epoch: [132][30/233]	Loss 0.0029 (0.0060)	
training:	Epoch: [132][31/233]	Loss 0.0068 (0.0061)	
training:	Epoch: [132][32/233]	Loss 0.0034 (0.0060)	
training:	Epoch: [132][33/233]	Loss 0.0012 (0.0058)	
training:	Epoch: [132][34/233]	Loss 0.0013 (0.0057)	
training:	Epoch: [132][35/233]	Loss 0.0008 (0.0056)	
training:	Epoch: [132][36/233]	Loss 0.0005 (0.0054)	
training:	Epoch: [132][37/233]	Loss 0.0011 (0.0053)	
training:	Epoch: [132][38/233]	Loss 0.0005 (0.0052)	
training:	Epoch: [132][39/233]	Loss 0.0010 (0.0051)	
training:	Epoch: [132][40/233]	Loss 0.0006 (0.0050)	
training:	Epoch: [132][41/233]	Loss 0.0008 (0.0049)	
training:	Epoch: [132][42/233]	Loss 0.0070 (0.0049)	
training:	Epoch: [132][43/233]	Loss 0.0005 (0.0048)	
training:	Epoch: [132][44/233]	Loss 0.0395 (0.0056)	
training:	Epoch: [132][45/233]	Loss 0.0060 (0.0056)	
training:	Epoch: [132][46/233]	Loss 0.0005 (0.0055)	
training:	Epoch: [132][47/233]	Loss 0.0006 (0.0054)	
training:	Epoch: [132][48/233]	Loss 0.0024 (0.0053)	
training:	Epoch: [132][49/233]	Loss 0.0005 (0.0052)	
training:	Epoch: [132][50/233]	Loss 0.0007 (0.0051)	
training:	Epoch: [132][51/233]	Loss 0.0006 (0.0051)	
training:	Epoch: [132][52/233]	Loss 0.0005 (0.0050)	
training:	Epoch: [132][53/233]	Loss 0.0006 (0.0049)	
training:	Epoch: [132][54/233]	Loss 0.0005 (0.0048)	
training:	Epoch: [132][55/233]	Loss 0.0097 (0.0049)	
training:	Epoch: [132][56/233]	Loss 0.0005 (0.0048)	
training:	Epoch: [132][57/233]	Loss 0.0012 (0.0047)	
training:	Epoch: [132][58/233]	Loss 0.0012 (0.0047)	
training:	Epoch: [132][59/233]	Loss 0.0007 (0.0046)	
training:	Epoch: [132][60/233]	Loss 0.0010 (0.0046)	
training:	Epoch: [132][61/233]	Loss 0.0006 (0.0045)	
training:	Epoch: [132][62/233]	Loss 0.0006 (0.0044)	
training:	Epoch: [132][63/233]	Loss 0.0009 (0.0044)	
training:	Epoch: [132][64/233]	Loss 0.0006 (0.0043)	
training:	Epoch: [132][65/233]	Loss 0.0006 (0.0043)	
training:	Epoch: [132][66/233]	Loss 0.0009 (0.0042)	
training:	Epoch: [132][67/233]	Loss 0.0005 (0.0042)	
training:	Epoch: [132][68/233]	Loss 0.0170 (0.0043)	
training:	Epoch: [132][69/233]	Loss 0.0005 (0.0043)	
training:	Epoch: [132][70/233]	Loss 0.0007 (0.0042)	
training:	Epoch: [132][71/233]	Loss 0.0007 (0.0042)	
training:	Epoch: [132][72/233]	Loss 0.0006 (0.0041)	
training:	Epoch: [132][73/233]	Loss 0.0006 (0.0041)	
training:	Epoch: [132][74/233]	Loss 0.0016 (0.0041)	
training:	Epoch: [132][75/233]	Loss 0.0005 (0.0040)	
training:	Epoch: [132][76/233]	Loss 0.0006 (0.0040)	
training:	Epoch: [132][77/233]	Loss 0.0807 (0.0050)	
training:	Epoch: [132][78/233]	Loss 0.0005 (0.0049)	
training:	Epoch: [132][79/233]	Loss 0.0007 (0.0048)	
training:	Epoch: [132][80/233]	Loss 0.0005 (0.0048)	
training:	Epoch: [132][81/233]	Loss 0.0004 (0.0047)	
training:	Epoch: [132][82/233]	Loss 0.0006 (0.0047)	
training:	Epoch: [132][83/233]	Loss 0.0024 (0.0047)	
training:	Epoch: [132][84/233]	Loss 0.0019 (0.0046)	
training:	Epoch: [132][85/233]	Loss 0.0006 (0.0046)	
training:	Epoch: [132][86/233]	Loss 0.0007 (0.0045)	
training:	Epoch: [132][87/233]	Loss 0.0265 (0.0048)	
training:	Epoch: [132][88/233]	Loss 0.1473 (0.0064)	
training:	Epoch: [132][89/233]	Loss 0.0012 (0.0063)	
training:	Epoch: [132][90/233]	Loss 0.0076 (0.0064)	
training:	Epoch: [132][91/233]	Loss 0.0007 (0.0063)	
training:	Epoch: [132][92/233]	Loss 0.0068 (0.0063)	
training:	Epoch: [132][93/233]	Loss 0.0023 (0.0063)	
training:	Epoch: [132][94/233]	Loss 0.0005 (0.0062)	
training:	Epoch: [132][95/233]	Loss 0.0490 (0.0067)	
training:	Epoch: [132][96/233]	Loss 0.0006 (0.0066)	
training:	Epoch: [132][97/233]	Loss 0.0008 (0.0065)	
training:	Epoch: [132][98/233]	Loss 0.0067 (0.0065)	
training:	Epoch: [132][99/233]	Loss 0.0007 (0.0065)	
training:	Epoch: [132][100/233]	Loss 0.0005 (0.0064)	
training:	Epoch: [132][101/233]	Loss 0.0007 (0.0064)	
training:	Epoch: [132][102/233]	Loss 0.0008 (0.0063)	
training:	Epoch: [132][103/233]	Loss 0.0069 (0.0063)	
training:	Epoch: [132][104/233]	Loss 0.0007 (0.0063)	
training:	Epoch: [132][105/233]	Loss 0.0022 (0.0062)	
training:	Epoch: [132][106/233]	Loss 0.0007 (0.0062)	
training:	Epoch: [132][107/233]	Loss 0.0007 (0.0061)	
training:	Epoch: [132][108/233]	Loss 0.0018 (0.0061)	
training:	Epoch: [132][109/233]	Loss 0.0019 (0.0060)	
training:	Epoch: [132][110/233]	Loss 0.0030 (0.0060)	
training:	Epoch: [132][111/233]	Loss 0.2324 (0.0080)	
training:	Epoch: [132][112/233]	Loss 0.0027 (0.0080)	
training:	Epoch: [132][113/233]	Loss 0.0025 (0.0079)	
training:	Epoch: [132][114/233]	Loss 0.0756 (0.0085)	
training:	Epoch: [132][115/233]	Loss 0.0007 (0.0085)	
training:	Epoch: [132][116/233]	Loss 0.0711 (0.0090)	
training:	Epoch: [132][117/233]	Loss 0.0069 (0.0090)	
training:	Epoch: [132][118/233]	Loss 0.0006 (0.0089)	
training:	Epoch: [132][119/233]	Loss 0.0005 (0.0089)	
training:	Epoch: [132][120/233]	Loss 0.0016 (0.0088)	
training:	Epoch: [132][121/233]	Loss 0.0006 (0.0087)	
training:	Epoch: [132][122/233]	Loss 0.0005 (0.0087)	
training:	Epoch: [132][123/233]	Loss 0.0300 (0.0088)	
training:	Epoch: [132][124/233]	Loss 0.0010 (0.0088)	
training:	Epoch: [132][125/233]	Loss 0.0011 (0.0087)	
training:	Epoch: [132][126/233]	Loss 0.0005 (0.0086)	
training:	Epoch: [132][127/233]	Loss 0.0005 (0.0086)	
training:	Epoch: [132][128/233]	Loss 0.0023 (0.0085)	
training:	Epoch: [132][129/233]	Loss 0.0327 (0.0087)	
training:	Epoch: [132][130/233]	Loss 0.0172 (0.0088)	
training:	Epoch: [132][131/233]	Loss 0.0011 (0.0087)	
training:	Epoch: [132][132/233]	Loss 0.0212 (0.0088)	
training:	Epoch: [132][133/233]	Loss 0.0006 (0.0088)	
training:	Epoch: [132][134/233]	Loss 0.0010 (0.0087)	
training:	Epoch: [132][135/233]	Loss 0.0010 (0.0086)	
training:	Epoch: [132][136/233]	Loss 0.0007 (0.0086)	
training:	Epoch: [132][137/233]	Loss 0.0005 (0.0085)	
training:	Epoch: [132][138/233]	Loss 0.0007 (0.0085)	
training:	Epoch: [132][139/233]	Loss 0.0014 (0.0084)	
training:	Epoch: [132][140/233]	Loss 0.0006 (0.0084)	
training:	Epoch: [132][141/233]	Loss 0.0006 (0.0083)	
training:	Epoch: [132][142/233]	Loss 0.0024 (0.0083)	
training:	Epoch: [132][143/233]	Loss 0.0004 (0.0082)	
training:	Epoch: [132][144/233]	Loss 0.0023 (0.0082)	
training:	Epoch: [132][145/233]	Loss 0.0005 (0.0081)	
training:	Epoch: [132][146/233]	Loss 0.0007 (0.0081)	
training:	Epoch: [132][147/233]	Loss 0.0005 (0.0080)	
training:	Epoch: [132][148/233]	Loss 0.0005 (0.0080)	
training:	Epoch: [132][149/233]	Loss 0.0007 (0.0079)	
training:	Epoch: [132][150/233]	Loss 0.0009 (0.0079)	
training:	Epoch: [132][151/233]	Loss 0.0009 (0.0078)	
training:	Epoch: [132][152/233]	Loss 0.0007 (0.0078)	
training:	Epoch: [132][153/233]	Loss 0.0032 (0.0077)	
training:	Epoch: [132][154/233]	Loss 0.0016 (0.0077)	
training:	Epoch: [132][155/233]	Loss 0.0005 (0.0077)	
training:	Epoch: [132][156/233]	Loss 0.0006 (0.0076)	
training:	Epoch: [132][157/233]	Loss 0.0017 (0.0076)	
training:	Epoch: [132][158/233]	Loss 0.0008 (0.0075)	
training:	Epoch: [132][159/233]	Loss 0.0026 (0.0075)	
training:	Epoch: [132][160/233]	Loss 0.0009 (0.0075)	
training:	Epoch: [132][161/233]	Loss 0.0006 (0.0074)	
training:	Epoch: [132][162/233]	Loss 0.0006 (0.0074)	
training:	Epoch: [132][163/233]	Loss 0.0020 (0.0073)	
training:	Epoch: [132][164/233]	Loss 0.0009 (0.0073)	
training:	Epoch: [132][165/233]	Loss 0.0005 (0.0073)	
training:	Epoch: [132][166/233]	Loss 0.0007 (0.0072)	
training:	Epoch: [132][167/233]	Loss 0.0020 (0.0072)	
training:	Epoch: [132][168/233]	Loss 0.0009 (0.0072)	
training:	Epoch: [132][169/233]	Loss 0.0005 (0.0071)	
training:	Epoch: [132][170/233]	Loss 0.0015 (0.0071)	
training:	Epoch: [132][171/233]	Loss 0.0069 (0.0071)	
training:	Epoch: [132][172/233]	Loss 0.0006 (0.0070)	
training:	Epoch: [132][173/233]	Loss 0.0007 (0.0070)	
training:	Epoch: [132][174/233]	Loss 0.0007 (0.0070)	
training:	Epoch: [132][175/233]	Loss 0.0005 (0.0069)	
training:	Epoch: [132][176/233]	Loss 0.0005 (0.0069)	
training:	Epoch: [132][177/233]	Loss 0.0018 (0.0069)	
training:	Epoch: [132][178/233]	Loss 0.0005 (0.0068)	
training:	Epoch: [132][179/233]	Loss 0.0005 (0.0068)	
training:	Epoch: [132][180/233]	Loss 0.0005 (0.0068)	
training:	Epoch: [132][181/233]	Loss 0.0017 (0.0067)	
training:	Epoch: [132][182/233]	Loss 0.0054 (0.0067)	
training:	Epoch: [132][183/233]	Loss 0.0006 (0.0067)	
training:	Epoch: [132][184/233]	Loss 0.0004 (0.0067)	
training:	Epoch: [132][185/233]	Loss 0.0007 (0.0066)	
training:	Epoch: [132][186/233]	Loss 0.0010 (0.0066)	
training:	Epoch: [132][187/233]	Loss 0.0013 (0.0066)	
training:	Epoch: [132][188/233]	Loss 0.0009 (0.0065)	
training:	Epoch: [132][189/233]	Loss 0.0023 (0.0065)	
training:	Epoch: [132][190/233]	Loss 0.0007 (0.0065)	
training:	Epoch: [132][191/233]	Loss 0.0008 (0.0065)	
training:	Epoch: [132][192/233]	Loss 0.0007 (0.0064)	
training:	Epoch: [132][193/233]	Loss 0.0012 (0.0064)	
training:	Epoch: [132][194/233]	Loss 0.0032 (0.0064)	
training:	Epoch: [132][195/233]	Loss 0.0005 (0.0064)	
training:	Epoch: [132][196/233]	Loss 0.0007 (0.0063)	
training:	Epoch: [132][197/233]	Loss 0.0169 (0.0064)	
training:	Epoch: [132][198/233]	Loss 0.0014 (0.0064)	
training:	Epoch: [132][199/233]	Loss 0.0005 (0.0063)	
training:	Epoch: [132][200/233]	Loss 0.0018 (0.0063)	
training:	Epoch: [132][201/233]	Loss 0.0009 (0.0063)	
training:	Epoch: [132][202/233]	Loss 0.0017 (0.0062)	
training:	Epoch: [132][203/233]	Loss 0.0006 (0.0062)	
training:	Epoch: [132][204/233]	Loss 0.0006 (0.0062)	
training:	Epoch: [132][205/233]	Loss 0.0004 (0.0062)	
training:	Epoch: [132][206/233]	Loss 0.0005 (0.0061)	
training:	Epoch: [132][207/233]	Loss 0.0009 (0.0061)	
training:	Epoch: [132][208/233]	Loss 0.0004 (0.0061)	
training:	Epoch: [132][209/233]	Loss 0.0005 (0.0061)	
training:	Epoch: [132][210/233]	Loss 0.0006 (0.0060)	
training:	Epoch: [132][211/233]	Loss 0.0010 (0.0060)	
training:	Epoch: [132][212/233]	Loss 0.0357 (0.0061)	
training:	Epoch: [132][213/233]	Loss 0.0025 (0.0061)	
training:	Epoch: [132][214/233]	Loss 0.0057 (0.0061)	
training:	Epoch: [132][215/233]	Loss 0.0007 (0.0061)	
training:	Epoch: [132][216/233]	Loss 0.0006 (0.0061)	
training:	Epoch: [132][217/233]	Loss 0.0029 (0.0061)	
training:	Epoch: [132][218/233]	Loss 0.0014 (0.0060)	
training:	Epoch: [132][219/233]	Loss 0.0007 (0.0060)	
training:	Epoch: [132][220/233]	Loss 0.0004 (0.0060)	
training:	Epoch: [132][221/233]	Loss 0.0008 (0.0060)	
training:	Epoch: [132][222/233]	Loss 0.0012 (0.0059)	
training:	Epoch: [132][223/233]	Loss 0.0006 (0.0059)	
training:	Epoch: [132][224/233]	Loss 0.0005 (0.0059)	
training:	Epoch: [132][225/233]	Loss 0.0030 (0.0059)	
training:	Epoch: [132][226/233]	Loss 0.0121 (0.0059)	
training:	Epoch: [132][227/233]	Loss 0.0007 (0.0059)	
training:	Epoch: [132][228/233]	Loss 0.0050 (0.0059)	
training:	Epoch: [132][229/233]	Loss 0.0005 (0.0059)	
training:	Epoch: [132][230/233]	Loss 0.0004 (0.0058)	
training:	Epoch: [132][231/233]	Loss 0.0006 (0.0058)	
training:	Epoch: [132][232/233]	Loss 0.0007 (0.0058)	
training:	Epoch: [132][233/233]	Loss 0.0007 (0.0058)	
Training:	 Loss: 0.0058

Training:	 ACC: 1.0000 1.0000 1.0000 1.0000
Validation:	 ACC: 0.7889 0.7871 0.7497 0.8281
Validation:	 Best_BACC: 0.8162 0.8154 0.7998 0.8326
Validation:	 Loss: 1.1965
Pretraining:	Epoch 133/200
----------
training:	Epoch: [133][1/233]	Loss 0.0005 (0.0005)	
training:	Epoch: [133][2/233]	Loss 0.0017 (0.0011)	
training:	Epoch: [133][3/233]	Loss 0.0007 (0.0010)	
training:	Epoch: [133][4/233]	Loss 0.0114 (0.0036)	
training:	Epoch: [133][5/233]	Loss 0.0008 (0.0030)	
training:	Epoch: [133][6/233]	Loss 0.0008 (0.0026)	
training:	Epoch: [133][7/233]	Loss 0.0006 (0.0024)	
training:	Epoch: [133][8/233]	Loss 0.0019 (0.0023)	
training:	Epoch: [133][9/233]	Loss 0.0008 (0.0021)	
training:	Epoch: [133][10/233]	Loss 0.0005 (0.0020)	
training:	Epoch: [133][11/233]	Loss 0.0005 (0.0018)	
training:	Epoch: [133][12/233]	Loss 0.0005 (0.0017)	
training:	Epoch: [133][13/233]	Loss 0.0011 (0.0017)	
training:	Epoch: [133][14/233]	Loss 0.0005 (0.0016)	
training:	Epoch: [133][15/233]	Loss 0.0006 (0.0015)	
training:	Epoch: [133][16/233]	Loss 0.0021 (0.0016)	
training:	Epoch: [133][17/233]	Loss 0.0012 (0.0015)	
training:	Epoch: [133][18/233]	Loss 0.0005 (0.0015)	
training:	Epoch: [133][19/233]	Loss 0.0022 (0.0015)	
training:	Epoch: [133][20/233]	Loss 0.0017 (0.0015)	
training:	Epoch: [133][21/233]	Loss 0.0006 (0.0015)	
training:	Epoch: [133][22/233]	Loss 0.0008 (0.0015)	
training:	Epoch: [133][23/233]	Loss 0.0004 (0.0014)	
training:	Epoch: [133][24/233]	Loss 0.0005 (0.0014)	
training:	Epoch: [133][25/233]	Loss 0.0155 (0.0019)	
training:	Epoch: [133][26/233]	Loss 0.0009 (0.0019)	
training:	Epoch: [133][27/233]	Loss 0.0004 (0.0018)	
training:	Epoch: [133][28/233]	Loss 0.0005 (0.0018)	
training:	Epoch: [133][29/233]	Loss 0.0009 (0.0018)	
training:	Epoch: [133][30/233]	Loss 0.0004 (0.0017)	
training:	Epoch: [133][31/233]	Loss 0.0007 (0.0017)	
training:	Epoch: [133][32/233]	Loss 0.0006 (0.0017)	
training:	Epoch: [133][33/233]	Loss 0.0006 (0.0016)	
training:	Epoch: [133][34/233]	Loss 0.0004 (0.0016)	
training:	Epoch: [133][35/233]	Loss 0.0008 (0.0016)	
training:	Epoch: [133][36/233]	Loss 0.0005 (0.0015)	
training:	Epoch: [133][37/233]	Loss 0.0008 (0.0015)	
training:	Epoch: [133][38/233]	Loss 0.0074 (0.0017)	
training:	Epoch: [133][39/233]	Loss 0.0004 (0.0016)	
training:	Epoch: [133][40/233]	Loss 0.0004 (0.0016)	
training:	Epoch: [133][41/233]	Loss 0.0011 (0.0016)	
training:	Epoch: [133][42/233]	Loss 0.0006 (0.0016)	
training:	Epoch: [133][43/233]	Loss 0.0009 (0.0016)	
training:	Epoch: [133][44/233]	Loss 0.0004 (0.0015)	
training:	Epoch: [133][45/233]	Loss 0.0005 (0.0015)	
training:	Epoch: [133][46/233]	Loss 0.0004 (0.0015)	
training:	Epoch: [133][47/233]	Loss 0.0009 (0.0015)	
training:	Epoch: [133][48/233]	Loss 0.0006 (0.0015)	
training:	Epoch: [133][49/233]	Loss 0.0005 (0.0014)	
training:	Epoch: [133][50/233]	Loss 0.0006 (0.0014)	
training:	Epoch: [133][51/233]	Loss 0.0005 (0.0014)	
training:	Epoch: [133][52/233]	Loss 0.0006 (0.0014)	
training:	Epoch: [133][53/233]	Loss 0.0006 (0.0014)	
training:	Epoch: [133][54/233]	Loss 0.0004 (0.0013)	
training:	Epoch: [133][55/233]	Loss 0.0004 (0.0013)	
training:	Epoch: [133][56/233]	Loss 0.0005 (0.0013)	
training:	Epoch: [133][57/233]	Loss 0.0005 (0.0013)	
training:	Epoch: [133][58/233]	Loss 0.0005 (0.0013)	
training:	Epoch: [133][59/233]	Loss 0.0004 (0.0013)	
training:	Epoch: [133][60/233]	Loss 0.0005 (0.0013)	
training:	Epoch: [133][61/233]	Loss 0.0031 (0.0013)	
training:	Epoch: [133][62/233]	Loss 0.0007 (0.0013)	
training:	Epoch: [133][63/233]	Loss 0.0006 (0.0013)	
training:	Epoch: [133][64/233]	Loss 0.0007 (0.0013)	
training:	Epoch: [133][65/233]	Loss 0.0012 (0.0013)	
training:	Epoch: [133][66/233]	Loss 0.0166 (0.0015)	
training:	Epoch: [133][67/233]	Loss 0.0004 (0.0015)	
training:	Epoch: [133][68/233]	Loss 0.0006 (0.0015)	
training:	Epoch: [133][69/233]	Loss 0.0005 (0.0014)	
training:	Epoch: [133][70/233]	Loss 0.0004 (0.0014)	
training:	Epoch: [133][71/233]	Loss 0.0022 (0.0014)	
training:	Epoch: [133][72/233]	Loss 0.0004 (0.0014)	
training:	Epoch: [133][73/233]	Loss 0.0008 (0.0014)	
training:	Epoch: [133][74/233]	Loss 0.0015 (0.0014)	
training:	Epoch: [133][75/233]	Loss 0.0005 (0.0014)	
training:	Epoch: [133][76/233]	Loss 0.0004 (0.0014)	
training:	Epoch: [133][77/233]	Loss 0.0005 (0.0014)	
training:	Epoch: [133][78/233]	Loss 0.0005 (0.0014)	
training:	Epoch: [133][79/233]	Loss 0.0004 (0.0014)	
training:	Epoch: [133][80/233]	Loss 0.0010 (0.0014)	
training:	Epoch: [133][81/233]	Loss 0.0005 (0.0013)	
training:	Epoch: [133][82/233]	Loss 0.0006 (0.0013)	
training:	Epoch: [133][83/233]	Loss 0.0005 (0.0013)	
training:	Epoch: [133][84/233]	Loss 0.0005 (0.0013)	
training:	Epoch: [133][85/233]	Loss 0.0005 (0.0013)	
training:	Epoch: [133][86/233]	Loss 0.0004 (0.0013)	
training:	Epoch: [133][87/233]	Loss 0.0005 (0.0013)	
training:	Epoch: [133][88/233]	Loss 0.0008 (0.0013)	
training:	Epoch: [133][89/233]	Loss 0.0005 (0.0013)	
training:	Epoch: [133][90/233]	Loss 0.0005 (0.0013)	
training:	Epoch: [133][91/233]	Loss 0.0005 (0.0013)	
training:	Epoch: [133][92/233]	Loss 0.0005 (0.0013)	
training:	Epoch: [133][93/233]	Loss 0.0006 (0.0012)	
training:	Epoch: [133][94/233]	Loss 0.0011 (0.0012)	
training:	Epoch: [133][95/233]	Loss 0.0005 (0.0012)	
training:	Epoch: [133][96/233]	Loss 0.0085 (0.0013)	
training:	Epoch: [133][97/233]	Loss 0.0005 (0.0013)	
training:	Epoch: [133][98/233]	Loss 0.0007 (0.0013)	
training:	Epoch: [133][99/233]	Loss 0.0005 (0.0013)	
training:	Epoch: [133][100/233]	Loss 0.0004 (0.0013)	
training:	Epoch: [133][101/233]	Loss 0.0005 (0.0013)	
training:	Epoch: [133][102/233]	Loss 0.0006 (0.0013)	
training:	Epoch: [133][103/233]	Loss 0.0005 (0.0013)	
training:	Epoch: [133][104/233]	Loss 0.0005 (0.0012)	
training:	Epoch: [133][105/233]	Loss 0.0004 (0.0012)	
training:	Epoch: [133][106/233]	Loss 0.0011 (0.0012)	
training:	Epoch: [133][107/233]	Loss 0.0006 (0.0012)	
training:	Epoch: [133][108/233]	Loss 0.0006 (0.0012)	
training:	Epoch: [133][109/233]	Loss 0.0010 (0.0012)	
training:	Epoch: [133][110/233]	Loss 0.0004 (0.0012)	
training:	Epoch: [133][111/233]	Loss 0.0104 (0.0013)	
training:	Epoch: [133][112/233]	Loss 0.0005 (0.0013)	
training:	Epoch: [133][113/233]	Loss 0.0004 (0.0013)	
training:	Epoch: [133][114/233]	Loss 0.0004 (0.0013)	
training:	Epoch: [133][115/233]	Loss 0.0004 (0.0013)	
training:	Epoch: [133][116/233]	Loss 0.0006 (0.0013)	
training:	Epoch: [133][117/233]	Loss 0.0034 (0.0013)	
training:	Epoch: [133][118/233]	Loss 0.0006 (0.0013)	
training:	Epoch: [133][119/233]	Loss 0.0008 (0.0013)	
training:	Epoch: [133][120/233]	Loss 0.0004 (0.0013)	
training:	Epoch: [133][121/233]	Loss 0.0005 (0.0013)	
training:	Epoch: [133][122/233]	Loss 0.0009 (0.0013)	
training:	Epoch: [133][123/233]	Loss 0.0184 (0.0014)	
training:	Epoch: [133][124/233]	Loss 0.0153 (0.0015)	
training:	Epoch: [133][125/233]	Loss 0.0004 (0.0015)	
training:	Epoch: [133][126/233]	Loss 0.0008 (0.0015)	
training:	Epoch: [133][127/233]	Loss 0.0005 (0.0015)	
training:	Epoch: [133][128/233]	Loss 0.0008 (0.0015)	
training:	Epoch: [133][129/233]	Loss 0.0005 (0.0015)	
training:	Epoch: [133][130/233]	Loss 0.0044 (0.0015)	
training:	Epoch: [133][131/233]	Loss 0.0008 (0.0015)	
training:	Epoch: [133][132/233]	Loss 0.0005 (0.0015)	
training:	Epoch: [133][133/233]	Loss 0.0015 (0.0015)	
training:	Epoch: [133][134/233]	Loss 0.0005 (0.0015)	
training:	Epoch: [133][135/233]	Loss 0.0005 (0.0015)	
training:	Epoch: [133][136/233]	Loss 0.0005 (0.0015)	
training:	Epoch: [133][137/233]	Loss 0.0009 (0.0015)	
training:	Epoch: [133][138/233]	Loss 0.0004 (0.0014)	
training:	Epoch: [133][139/233]	Loss 0.0008 (0.0014)	
training:	Epoch: [133][140/233]	Loss 0.0006 (0.0014)	
training:	Epoch: [133][141/233]	Loss 0.0024 (0.0014)	
training:	Epoch: [133][142/233]	Loss 0.0005 (0.0014)	
training:	Epoch: [133][143/233]	Loss 0.0038 (0.0015)	
training:	Epoch: [133][144/233]	Loss 0.0005 (0.0014)	
training:	Epoch: [133][145/233]	Loss 0.0005 (0.0014)	
training:	Epoch: [133][146/233]	Loss 0.0091 (0.0015)	
training:	Epoch: [133][147/233]	Loss 0.0010 (0.0015)	
training:	Epoch: [133][148/233]	Loss 0.0005 (0.0015)	
training:	Epoch: [133][149/233]	Loss 0.0004 (0.0015)	
training:	Epoch: [133][150/233]	Loss 0.0004 (0.0015)	
training:	Epoch: [133][151/233]	Loss 0.0005 (0.0015)	
training:	Epoch: [133][152/233]	Loss 0.0010 (0.0015)	
training:	Epoch: [133][153/233]	Loss 0.0006 (0.0015)	
training:	Epoch: [133][154/233]	Loss 0.0013 (0.0015)	
training:	Epoch: [133][155/233]	Loss 0.0005 (0.0014)	
training:	Epoch: [133][156/233]	Loss 0.0005 (0.0014)	
training:	Epoch: [133][157/233]	Loss 0.0004 (0.0014)	
training:	Epoch: [133][158/233]	Loss 0.0004 (0.0014)	
training:	Epoch: [133][159/233]	Loss 0.0004 (0.0014)	
training:	Epoch: [133][160/233]	Loss 0.0004 (0.0014)	
training:	Epoch: [133][161/233]	Loss 0.0005 (0.0014)	
training:	Epoch: [133][162/233]	Loss 0.0004 (0.0014)	
training:	Epoch: [133][163/233]	Loss 0.0004 (0.0014)	
training:	Epoch: [133][164/233]	Loss 0.0009 (0.0014)	
training:	Epoch: [133][165/233]	Loss 0.0004 (0.0014)	
training:	Epoch: [133][166/233]	Loss 0.0005 (0.0014)	
training:	Epoch: [133][167/233]	Loss 0.0005 (0.0014)	
training:	Epoch: [133][168/233]	Loss 0.0012 (0.0014)	
training:	Epoch: [133][169/233]	Loss 0.0011 (0.0014)	
training:	Epoch: [133][170/233]	Loss 0.0005 (0.0014)	
training:	Epoch: [133][171/233]	Loss 0.0005 (0.0014)	
training:	Epoch: [133][172/233]	Loss 0.0005 (0.0014)	
training:	Epoch: [133][173/233]	Loss 0.0065 (0.0014)	
training:	Epoch: [133][174/233]	Loss 0.0004 (0.0014)	
training:	Epoch: [133][175/233]	Loss 0.0004 (0.0014)	
training:	Epoch: [133][176/233]	Loss 0.0010 (0.0014)	
training:	Epoch: [133][177/233]	Loss 0.0005 (0.0014)	
training:	Epoch: [133][178/233]	Loss 0.0005 (0.0014)	
training:	Epoch: [133][179/233]	Loss 0.0004 (0.0014)	
training:	Epoch: [133][180/233]	Loss 0.0005 (0.0014)	
training:	Epoch: [133][181/233]	Loss 0.0004 (0.0014)	
training:	Epoch: [133][182/233]	Loss 0.0004 (0.0013)	
training:	Epoch: [133][183/233]	Loss 0.0006 (0.0013)	
training:	Epoch: [133][184/233]	Loss 0.0639 (0.0017)	
training:	Epoch: [133][185/233]	Loss 0.0004 (0.0017)	
training:	Epoch: [133][186/233]	Loss 0.0004 (0.0017)	
training:	Epoch: [133][187/233]	Loss 0.0044 (0.0017)	
training:	Epoch: [133][188/233]	Loss 0.0009 (0.0017)	
training:	Epoch: [133][189/233]	Loss 0.0005 (0.0017)	
training:	Epoch: [133][190/233]	Loss 0.0005 (0.0017)	
training:	Epoch: [133][191/233]	Loss 0.0006 (0.0017)	
training:	Epoch: [133][192/233]	Loss 0.0236 (0.0018)	
training:	Epoch: [133][193/233]	Loss 0.0005 (0.0018)	
training:	Epoch: [133][194/233]	Loss 0.0005 (0.0018)	
training:	Epoch: [133][195/233]	Loss 0.1026 (0.0023)	
training:	Epoch: [133][196/233]	Loss 0.0005 (0.0023)	
training:	Epoch: [133][197/233]	Loss 0.0005 (0.0023)	
training:	Epoch: [133][198/233]	Loss 0.0005 (0.0023)	
training:	Epoch: [133][199/233]	Loss 0.0003 (0.0022)	
training:	Epoch: [133][200/233]	Loss 0.0029 (0.0022)	
training:	Epoch: [133][201/233]	Loss 0.0014 (0.0022)	
training:	Epoch: [133][202/233]	Loss 0.0007 (0.0022)	
training:	Epoch: [133][203/233]	Loss 0.0007 (0.0022)	
training:	Epoch: [133][204/233]	Loss 0.0232 (0.0023)	
training:	Epoch: [133][205/233]	Loss 0.0005 (0.0023)	
training:	Epoch: [133][206/233]	Loss 0.0005 (0.0023)	
training:	Epoch: [133][207/233]	Loss 0.0012 (0.0023)	
training:	Epoch: [133][208/233]	Loss 0.0004 (0.0023)	
training:	Epoch: [133][209/233]	Loss 0.0009 (0.0023)	
training:	Epoch: [133][210/233]	Loss 0.0012 (0.0023)	
training:	Epoch: [133][211/233]	Loss 0.0008 (0.0023)	
training:	Epoch: [133][212/233]	Loss 0.0069 (0.0023)	
training:	Epoch: [133][213/233]	Loss 0.0007 (0.0023)	
training:	Epoch: [133][214/233]	Loss 0.0298 (0.0024)	
training:	Epoch: [133][215/233]	Loss 0.0076 (0.0024)	
training:	Epoch: [133][216/233]	Loss 0.0007 (0.0024)	
training:	Epoch: [133][217/233]	Loss 0.0004 (0.0024)	
training:	Epoch: [133][218/233]	Loss 0.0004 (0.0024)	
training:	Epoch: [133][219/233]	Loss 0.0005 (0.0024)	
training:	Epoch: [133][220/233]	Loss 0.0007 (0.0024)	
training:	Epoch: [133][221/233]	Loss 0.0004 (0.0024)	
training:	Epoch: [133][222/233]	Loss 0.0006 (0.0024)	
training:	Epoch: [133][223/233]	Loss 0.1330 (0.0030)	
training:	Epoch: [133][224/233]	Loss 0.0004 (0.0030)	
training:	Epoch: [133][225/233]	Loss 0.0004 (0.0029)	
training:	Epoch: [133][226/233]	Loss 0.0006 (0.0029)	
training:	Epoch: [133][227/233]	Loss 0.0004 (0.0029)	
training:	Epoch: [133][228/233]	Loss 0.0311 (0.0030)	
training:	Epoch: [133][229/233]	Loss 0.0006 (0.0030)	
training:	Epoch: [133][230/233]	Loss 0.0011 (0.0030)	
training:	Epoch: [133][231/233]	Loss 0.0006 (0.0030)	
training:	Epoch: [133][232/233]	Loss 0.0005 (0.0030)	
training:	Epoch: [133][233/233]	Loss 0.0005 (0.0030)	
Training:	 Loss: 0.0030

Training:	 ACC: 1.0000 1.0000 1.0000 1.0000
Validation:	 ACC: 0.7948 0.7951 0.7998 0.7899
Validation:	 Best_BACC: 0.8162 0.8154 0.7998 0.8326
Validation:	 Loss: 1.1932
Pretraining:	Epoch 134/200
----------
training:	Epoch: [134][1/233]	Loss 0.0007 (0.0007)	
training:	Epoch: [134][2/233]	Loss 0.0004 (0.0005)	
training:	Epoch: [134][3/233]	Loss 0.0004 (0.0005)	
training:	Epoch: [134][4/233]	Loss 0.0005 (0.0005)	
training:	Epoch: [134][5/233]	Loss 0.0005 (0.0005)	
training:	Epoch: [134][6/233]	Loss 0.0005 (0.0005)	
training:	Epoch: [134][7/233]	Loss 0.0011 (0.0006)	
training:	Epoch: [134][8/233]	Loss 0.0005 (0.0006)	
training:	Epoch: [134][9/233]	Loss 0.0004 (0.0006)	
training:	Epoch: [134][10/233]	Loss 0.0004 (0.0005)	
training:	Epoch: [134][11/233]	Loss 0.0006 (0.0005)	
training:	Epoch: [134][12/233]	Loss 0.0010 (0.0006)	
training:	Epoch: [134][13/233]	Loss 0.0005 (0.0006)	
training:	Epoch: [134][14/233]	Loss 0.0004 (0.0006)	
training:	Epoch: [134][15/233]	Loss 0.0014 (0.0006)	
training:	Epoch: [134][16/233]	Loss 0.0018 (0.0007)	
training:	Epoch: [134][17/233]	Loss 0.0004 (0.0007)	
training:	Epoch: [134][18/233]	Loss 0.0005 (0.0007)	
training:	Epoch: [134][19/233]	Loss 0.0007 (0.0007)	
training:	Epoch: [134][20/233]	Loss 0.0004 (0.0007)	
training:	Epoch: [134][21/233]	Loss 0.0006 (0.0007)	
training:	Epoch: [134][22/233]	Loss 0.0005 (0.0006)	
training:	Epoch: [134][23/233]	Loss 0.0006 (0.0006)	
training:	Epoch: [134][24/233]	Loss 0.0004 (0.0006)	
training:	Epoch: [134][25/233]	Loss 0.0040 (0.0008)	
training:	Epoch: [134][26/233]	Loss 0.0013 (0.0008)	
training:	Epoch: [134][27/233]	Loss 0.0086 (0.0011)	
training:	Epoch: [134][28/233]	Loss 0.0008 (0.0011)	
training:	Epoch: [134][29/233]	Loss 0.0004 (0.0010)	
training:	Epoch: [134][30/233]	Loss 0.0046 (0.0012)	
training:	Epoch: [134][31/233]	Loss 0.0005 (0.0011)	
training:	Epoch: [134][32/233]	Loss 0.0006 (0.0011)	
training:	Epoch: [134][33/233]	Loss 0.0579 (0.0028)	
training:	Epoch: [134][34/233]	Loss 0.0014 (0.0028)	
training:	Epoch: [134][35/233]	Loss 0.0004 (0.0027)	
training:	Epoch: [134][36/233]	Loss 0.0054 (0.0028)	
training:	Epoch: [134][37/233]	Loss 0.0007 (0.0028)	
training:	Epoch: [134][38/233]	Loss 0.0005 (0.0027)	
training:	Epoch: [134][39/233]	Loss 0.0005 (0.0026)	
training:	Epoch: [134][40/233]	Loss 0.0015 (0.0026)	
training:	Epoch: [134][41/233]	Loss 0.0004 (0.0026)	
training:	Epoch: [134][42/233]	Loss 0.0004 (0.0025)	
training:	Epoch: [134][43/233]	Loss 0.0145 (0.0028)	
training:	Epoch: [134][44/233]	Loss 0.0008 (0.0027)	
training:	Epoch: [134][45/233]	Loss 0.0089 (0.0029)	
training:	Epoch: [134][46/233]	Loss 0.0005 (0.0028)	
training:	Epoch: [134][47/233]	Loss 0.0005 (0.0028)	
training:	Epoch: [134][48/233]	Loss 0.0004 (0.0027)	
training:	Epoch: [134][49/233]	Loss 0.0004 (0.0027)	
training:	Epoch: [134][50/233]	Loss 0.0005 (0.0026)	
training:	Epoch: [134][51/233]	Loss 0.0005 (0.0026)	
training:	Epoch: [134][52/233]	Loss 0.0005 (0.0026)	
training:	Epoch: [134][53/233]	Loss 0.0005 (0.0025)	
training:	Epoch: [134][54/233]	Loss 0.0005 (0.0025)	
training:	Epoch: [134][55/233]	Loss 0.0004 (0.0024)	
training:	Epoch: [134][56/233]	Loss 0.0005 (0.0024)	
training:	Epoch: [134][57/233]	Loss 0.0004 (0.0024)	
training:	Epoch: [134][58/233]	Loss 0.0028 (0.0024)	
training:	Epoch: [134][59/233]	Loss 0.0005 (0.0023)	
training:	Epoch: [134][60/233]	Loss 0.0086 (0.0024)	
training:	Epoch: [134][61/233]	Loss 0.0004 (0.0024)	
training:	Epoch: [134][62/233]	Loss 0.0004 (0.0024)	
training:	Epoch: [134][63/233]	Loss 0.0005 (0.0024)	
training:	Epoch: [134][64/233]	Loss 0.0004 (0.0023)	
training:	Epoch: [134][65/233]	Loss 0.0004 (0.0023)	
training:	Epoch: [134][66/233]	Loss 0.0012 (0.0023)	
training:	Epoch: [134][67/233]	Loss 0.0040 (0.0023)	
training:	Epoch: [134][68/233]	Loss 0.0004 (0.0023)	
training:	Epoch: [134][69/233]	Loss 0.0006 (0.0023)	
training:	Epoch: [134][70/233]	Loss 0.0005 (0.0022)	
training:	Epoch: [134][71/233]	Loss 0.0005 (0.0022)	
training:	Epoch: [134][72/233]	Loss 0.0011 (0.0022)	
training:	Epoch: [134][73/233]	Loss 0.0016 (0.0022)	
training:	Epoch: [134][74/233]	Loss 0.0080 (0.0023)	
training:	Epoch: [134][75/233]	Loss 0.0005 (0.0022)	
training:	Epoch: [134][76/233]	Loss 0.0006 (0.0022)	
training:	Epoch: [134][77/233]	Loss 0.0005 (0.0022)	
training:	Epoch: [134][78/233]	Loss 0.0006 (0.0022)	
training:	Epoch: [134][79/233]	Loss 0.0003 (0.0021)	
training:	Epoch: [134][80/233]	Loss 0.0005 (0.0021)	
training:	Epoch: [134][81/233]	Loss 0.0004 (0.0021)	
training:	Epoch: [134][82/233]	Loss 0.0005 (0.0021)	
training:	Epoch: [134][83/233]	Loss 0.0005 (0.0021)	
training:	Epoch: [134][84/233]	Loss 0.0005 (0.0020)	
training:	Epoch: [134][85/233]	Loss 0.0004 (0.0020)	
training:	Epoch: [134][86/233]	Loss 0.0027 (0.0020)	
training:	Epoch: [134][87/233]	Loss 0.0016 (0.0020)	
training:	Epoch: [134][88/233]	Loss 0.0004 (0.0020)	
training:	Epoch: [134][89/233]	Loss 0.0008 (0.0020)	
training:	Epoch: [134][90/233]	Loss 0.0152 (0.0021)	
training:	Epoch: [134][91/233]	Loss 0.0273 (0.0024)	
training:	Epoch: [134][92/233]	Loss 0.0005 (0.0024)	
training:	Epoch: [134][93/233]	Loss 0.0004 (0.0024)	
training:	Epoch: [134][94/233]	Loss 0.0032 (0.0024)	
training:	Epoch: [134][95/233]	Loss 0.0005 (0.0024)	
training:	Epoch: [134][96/233]	Loss 0.0004 (0.0023)	
training:	Epoch: [134][97/233]	Loss 0.0005 (0.0023)	
training:	Epoch: [134][98/233]	Loss 0.0004 (0.0023)	
training:	Epoch: [134][99/233]	Loss 0.0036 (0.0023)	
training:	Epoch: [134][100/233]	Loss 0.0004 (0.0023)	
training:	Epoch: [134][101/233]	Loss 0.0004 (0.0023)	
training:	Epoch: [134][102/233]	Loss 0.0005 (0.0023)	
training:	Epoch: [134][103/233]	Loss 0.0005 (0.0022)	
training:	Epoch: [134][104/233]	Loss 0.0004 (0.0022)	
training:	Epoch: [134][105/233]	Loss 0.0004 (0.0022)	
training:	Epoch: [134][106/233]	Loss 0.0004 (0.0022)	
training:	Epoch: [134][107/233]	Loss 0.0006 (0.0022)	
training:	Epoch: [134][108/233]	Loss 0.0005 (0.0022)	
training:	Epoch: [134][109/233]	Loss 0.0010 (0.0022)	
training:	Epoch: [134][110/233]	Loss 0.0007 (0.0021)	
training:	Epoch: [134][111/233]	Loss 0.0004 (0.0021)	
training:	Epoch: [134][112/233]	Loss 0.0004 (0.0021)	
training:	Epoch: [134][113/233]	Loss 0.0004 (0.0021)	
training:	Epoch: [134][114/233]	Loss 0.0007 (0.0021)	
training:	Epoch: [134][115/233]	Loss 0.0004 (0.0021)	
training:	Epoch: [134][116/233]	Loss 0.0004 (0.0021)	
training:	Epoch: [134][117/233]	Loss 0.0008 (0.0020)	
training:	Epoch: [134][118/233]	Loss 0.0005 (0.0020)	
training:	Epoch: [134][119/233]	Loss 0.0024 (0.0020)	
training:	Epoch: [134][120/233]	Loss 0.0078 (0.0021)	
training:	Epoch: [134][121/233]	Loss 0.0004 (0.0021)	
training:	Epoch: [134][122/233]	Loss 0.0005 (0.0021)	
training:	Epoch: [134][123/233]	Loss 0.0005 (0.0020)	
training:	Epoch: [134][124/233]	Loss 0.0004 (0.0020)	
training:	Epoch: [134][125/233]	Loss 0.0010 (0.0020)	
training:	Epoch: [134][126/233]	Loss 0.0006 (0.0020)	
training:	Epoch: [134][127/233]	Loss 0.0004 (0.0020)	
training:	Epoch: [134][128/233]	Loss 0.0004 (0.0020)	
training:	Epoch: [134][129/233]	Loss 0.0070 (0.0020)	
training:	Epoch: [134][130/233]	Loss 0.0006 (0.0020)	
training:	Epoch: [134][131/233]	Loss 0.0005 (0.0020)	
training:	Epoch: [134][132/233]	Loss 0.0006 (0.0020)	
training:	Epoch: [134][133/233]	Loss 0.0005 (0.0020)	
training:	Epoch: [134][134/233]	Loss 0.0005 (0.0020)	
training:	Epoch: [134][135/233]	Loss 0.0004 (0.0020)	
training:	Epoch: [134][136/233]	Loss 0.0024 (0.0020)	
training:	Epoch: [134][137/233]	Loss 0.0005 (0.0019)	
training:	Epoch: [134][138/233]	Loss 0.0005 (0.0019)	
training:	Epoch: [134][139/233]	Loss 0.0004 (0.0019)	
training:	Epoch: [134][140/233]	Loss 0.0004 (0.0019)	
training:	Epoch: [134][141/233]	Loss 0.0004 (0.0019)	
training:	Epoch: [134][142/233]	Loss 0.0011 (0.0019)	
training:	Epoch: [134][143/233]	Loss 0.0005 (0.0019)	
training:	Epoch: [134][144/233]	Loss 0.0004 (0.0019)	
training:	Epoch: [134][145/233]	Loss 0.0005 (0.0019)	
training:	Epoch: [134][146/233]	Loss 0.0004 (0.0019)	
training:	Epoch: [134][147/233]	Loss 0.0005 (0.0018)	
training:	Epoch: [134][148/233]	Loss 0.0004 (0.0018)	
training:	Epoch: [134][149/233]	Loss 0.0005 (0.0018)	
training:	Epoch: [134][150/233]	Loss 0.0201 (0.0020)	
training:	Epoch: [134][151/233]	Loss 0.0038 (0.0020)	
training:	Epoch: [134][152/233]	Loss 0.0009 (0.0020)	
training:	Epoch: [134][153/233]	Loss 0.0008 (0.0020)	
training:	Epoch: [134][154/233]	Loss 0.0004 (0.0019)	
training:	Epoch: [134][155/233]	Loss 0.0012 (0.0019)	
training:	Epoch: [134][156/233]	Loss 0.0004 (0.0019)	
training:	Epoch: [134][157/233]	Loss 0.0004 (0.0019)	
training:	Epoch: [134][158/233]	Loss 0.0963 (0.0025)	
training:	Epoch: [134][159/233]	Loss 0.0006 (0.0025)	
training:	Epoch: [134][160/233]	Loss 0.0005 (0.0025)	
training:	Epoch: [134][161/233]	Loss 0.0007 (0.0025)	
training:	Epoch: [134][162/233]	Loss 0.0005 (0.0025)	
training:	Epoch: [134][163/233]	Loss 0.0005 (0.0025)	
training:	Epoch: [134][164/233]	Loss 0.0010 (0.0024)	
training:	Epoch: [134][165/233]	Loss 0.0005 (0.0024)	
training:	Epoch: [134][166/233]	Loss 0.0004 (0.0024)	
training:	Epoch: [134][167/233]	Loss 0.0004 (0.0024)	
training:	Epoch: [134][168/233]	Loss 0.0038 (0.0024)	
training:	Epoch: [134][169/233]	Loss 0.0004 (0.0024)	
training:	Epoch: [134][170/233]	Loss 0.0005 (0.0024)	
training:	Epoch: [134][171/233]	Loss 0.0014 (0.0024)	
training:	Epoch: [134][172/233]	Loss 0.0026 (0.0024)	
training:	Epoch: [134][173/233]	Loss 0.0008 (0.0024)	
training:	Epoch: [134][174/233]	Loss 0.0302 (0.0025)	
training:	Epoch: [134][175/233]	Loss 0.0004 (0.0025)	
training:	Epoch: [134][176/233]	Loss 0.0004 (0.0025)	
training:	Epoch: [134][177/233]	Loss 0.0005 (0.0025)	
training:	Epoch: [134][178/233]	Loss 0.0005 (0.0025)	
training:	Epoch: [134][179/233]	Loss 0.0005 (0.0025)	
training:	Epoch: [134][180/233]	Loss 0.0004 (0.0025)	
training:	Epoch: [134][181/233]	Loss 0.0006 (0.0025)	
training:	Epoch: [134][182/233]	Loss 0.0004 (0.0024)	
training:	Epoch: [134][183/233]	Loss 0.0004 (0.0024)	
training:	Epoch: [134][184/233]	Loss 0.0004 (0.0024)	
training:	Epoch: [134][185/233]	Loss 0.0004 (0.0024)	
training:	Epoch: [134][186/233]	Loss 0.0006 (0.0024)	
training:	Epoch: [134][187/233]	Loss 0.0005 (0.0024)	
training:	Epoch: [134][188/233]	Loss 0.0007 (0.0024)	
training:	Epoch: [134][189/233]	Loss 0.0004 (0.0024)	
training:	Epoch: [134][190/233]	Loss 0.0010 (0.0024)	
training:	Epoch: [134][191/233]	Loss 0.0004 (0.0024)	
training:	Epoch: [134][192/233]	Loss 0.0005 (0.0023)	
training:	Epoch: [134][193/233]	Loss 0.0004 (0.0023)	
training:	Epoch: [134][194/233]	Loss 0.0006 (0.0023)	
training:	Epoch: [134][195/233]	Loss 0.0006 (0.0023)	
training:	Epoch: [134][196/233]	Loss 0.0011 (0.0023)	
training:	Epoch: [134][197/233]	Loss 0.0006 (0.0023)	
training:	Epoch: [134][198/233]	Loss 0.0005 (0.0023)	
training:	Epoch: [134][199/233]	Loss 0.1062 (0.0028)	
training:	Epoch: [134][200/233]	Loss 0.0005 (0.0028)	
training:	Epoch: [134][201/233]	Loss 0.0004 (0.0028)	
training:	Epoch: [134][202/233]	Loss 0.0005 (0.0028)	
training:	Epoch: [134][203/233]	Loss 0.0004 (0.0028)	
training:	Epoch: [134][204/233]	Loss 0.0004 (0.0028)	
training:	Epoch: [134][205/233]	Loss 0.0004 (0.0027)	
training:	Epoch: [134][206/233]	Loss 0.0033 (0.0028)	
training:	Epoch: [134][207/233]	Loss 0.0005 (0.0027)	
training:	Epoch: [134][208/233]	Loss 0.0010 (0.0027)	
training:	Epoch: [134][209/233]	Loss 0.0008 (0.0027)	
training:	Epoch: [134][210/233]	Loss 0.0004 (0.0027)	
training:	Epoch: [134][211/233]	Loss 0.0005 (0.0027)	
training:	Epoch: [134][212/233]	Loss 0.0004 (0.0027)	
training:	Epoch: [134][213/233]	Loss 0.0004 (0.0027)	
training:	Epoch: [134][214/233]	Loss 0.0006 (0.0027)	
training:	Epoch: [134][215/233]	Loss 0.0006 (0.0027)	
training:	Epoch: [134][216/233]	Loss 0.0005 (0.0027)	
training:	Epoch: [134][217/233]	Loss 0.0006 (0.0026)	
training:	Epoch: [134][218/233]	Loss 0.0044 (0.0026)	
training:	Epoch: [134][219/233]	Loss 0.0005 (0.0026)	
training:	Epoch: [134][220/233]	Loss 0.0005 (0.0026)	
training:	Epoch: [134][221/233]	Loss 0.0004 (0.0026)	
training:	Epoch: [134][222/233]	Loss 0.0009 (0.0026)	
training:	Epoch: [134][223/233]	Loss 0.0011 (0.0026)	
training:	Epoch: [134][224/233]	Loss 0.0011 (0.0026)	
training:	Epoch: [134][225/233]	Loss 0.0004 (0.0026)	
training:	Epoch: [134][226/233]	Loss 0.0018 (0.0026)	
training:	Epoch: [134][227/233]	Loss 0.0006 (0.0026)	
training:	Epoch: [134][228/233]	Loss 0.0007 (0.0026)	
training:	Epoch: [134][229/233]	Loss 0.0005 (0.0026)	
training:	Epoch: [134][230/233]	Loss 0.0004 (0.0025)	
training:	Epoch: [134][231/233]	Loss 0.0006 (0.0025)	
training:	Epoch: [134][232/233]	Loss 0.0004 (0.0025)	
training:	Epoch: [134][233/233]	Loss 0.0004 (0.0025)	
Training:	 Loss: 0.0025

Training:	 ACC: 1.0000 1.0000 1.0000 1.0000
Validation:	 ACC: 0.7949 0.7935 0.7640 0.8258
Validation:	 Best_BACC: 0.8162 0.8154 0.7998 0.8326
Validation:	 Loss: 1.2256
Pretraining:	Epoch 135/200
----------
training:	Epoch: [135][1/233]	Loss 0.0010 (0.0010)	
training:	Epoch: [135][2/233]	Loss 0.0005 (0.0008)	
training:	Epoch: [135][3/233]	Loss 0.0007 (0.0007)	
training:	Epoch: [135][4/233]	Loss 0.0005 (0.0007)	
training:	Epoch: [135][5/233]	Loss 0.0005 (0.0006)	
training:	Epoch: [135][6/233]	Loss 0.0009 (0.0007)	
training:	Epoch: [135][7/233]	Loss 0.0006 (0.0007)	
training:	Epoch: [135][8/233]	Loss 0.0006 (0.0007)	
training:	Epoch: [135][9/233]	Loss 0.0027 (0.0009)	
training:	Epoch: [135][10/233]	Loss 0.0009 (0.0009)	
training:	Epoch: [135][11/233]	Loss 0.0004 (0.0008)	
training:	Epoch: [135][12/233]	Loss 0.0004 (0.0008)	
training:	Epoch: [135][13/233]	Loss 0.0004 (0.0008)	
training:	Epoch: [135][14/233]	Loss 0.0083 (0.0013)	
training:	Epoch: [135][15/233]	Loss 0.0004 (0.0013)	
training:	Epoch: [135][16/233]	Loss 0.0007 (0.0012)	
training:	Epoch: [135][17/233]	Loss 0.0005 (0.0012)	
training:	Epoch: [135][18/233]	Loss 0.0018 (0.0012)	
training:	Epoch: [135][19/233]	Loss 0.0009 (0.0012)	
training:	Epoch: [135][20/233]	Loss 0.0011 (0.0012)	
training:	Epoch: [135][21/233]	Loss 0.0017 (0.0012)	
training:	Epoch: [135][22/233]	Loss 0.0205 (0.0021)	
training:	Epoch: [135][23/233]	Loss 0.0060 (0.0023)	
training:	Epoch: [135][24/233]	Loss 0.0005 (0.0022)	
training:	Epoch: [135][25/233]	Loss 0.0005 (0.0021)	
training:	Epoch: [135][26/233]	Loss 0.0004 (0.0020)	
training:	Epoch: [135][27/233]	Loss 0.0004 (0.0020)	
training:	Epoch: [135][28/233]	Loss 0.0004 (0.0019)	
training:	Epoch: [135][29/233]	Loss 0.0015 (0.0019)	
training:	Epoch: [135][30/233]	Loss 0.0004 (0.0019)	
training:	Epoch: [135][31/233]	Loss 0.0005 (0.0018)	
training:	Epoch: [135][32/233]	Loss 0.0005 (0.0018)	
training:	Epoch: [135][33/233]	Loss 0.0005 (0.0017)	
training:	Epoch: [135][34/233]	Loss 0.0156 (0.0021)	
training:	Epoch: [135][35/233]	Loss 0.0004 (0.0021)	
training:	Epoch: [135][36/233]	Loss 0.0004 (0.0020)	
training:	Epoch: [135][37/233]	Loss 0.0006 (0.0020)	
training:	Epoch: [135][38/233]	Loss 0.0003 (0.0020)	
training:	Epoch: [135][39/233]	Loss 0.0011 (0.0019)	
training:	Epoch: [135][40/233]	Loss 0.0005 (0.0019)	
training:	Epoch: [135][41/233]	Loss 0.0006 (0.0019)	
training:	Epoch: [135][42/233]	Loss 0.0004 (0.0018)	
training:	Epoch: [135][43/233]	Loss 0.0009 (0.0018)	
training:	Epoch: [135][44/233]	Loss 0.0004 (0.0018)	
training:	Epoch: [135][45/233]	Loss 0.0004 (0.0018)	
training:	Epoch: [135][46/233]	Loss 0.0004 (0.0017)	
training:	Epoch: [135][47/233]	Loss 0.0004 (0.0017)	
training:	Epoch: [135][48/233]	Loss 0.0006 (0.0017)	
training:	Epoch: [135][49/233]	Loss 0.0005 (0.0017)	
training:	Epoch: [135][50/233]	Loss 0.0004 (0.0016)	
training:	Epoch: [135][51/233]	Loss 0.0017 (0.0016)	
training:	Epoch: [135][52/233]	Loss 0.0005 (0.0016)	
training:	Epoch: [135][53/233]	Loss 0.0006 (0.0016)	
training:	Epoch: [135][54/233]	Loss 0.0004 (0.0016)	
training:	Epoch: [135][55/233]	Loss 0.0004 (0.0015)	
training:	Epoch: [135][56/233]	Loss 0.0004 (0.0015)	
training:	Epoch: [135][57/233]	Loss 0.0179 (0.0018)	
training:	Epoch: [135][58/233]	Loss 0.0005 (0.0018)	
training:	Epoch: [135][59/233]	Loss 0.0006 (0.0018)	
training:	Epoch: [135][60/233]	Loss 0.0004 (0.0017)	
training:	Epoch: [135][61/233]	Loss 0.0004 (0.0017)	
training:	Epoch: [135][62/233]	Loss 0.0005 (0.0017)	
training:	Epoch: [135][63/233]	Loss 0.0005 (0.0017)	
training:	Epoch: [135][64/233]	Loss 0.0005 (0.0017)	
training:	Epoch: [135][65/233]	Loss 0.0005 (0.0016)	
training:	Epoch: [135][66/233]	Loss 0.0005 (0.0016)	
training:	Epoch: [135][67/233]	Loss 0.0005 (0.0016)	
training:	Epoch: [135][68/233]	Loss 0.0011 (0.0016)	
training:	Epoch: [135][69/233]	Loss 0.0005 (0.0016)	
training:	Epoch: [135][70/233]	Loss 0.0004 (0.0016)	
training:	Epoch: [135][71/233]	Loss 0.0005 (0.0016)	
training:	Epoch: [135][72/233]	Loss 0.0005 (0.0015)	
training:	Epoch: [135][73/233]	Loss 0.0004 (0.0015)	
training:	Epoch: [135][74/233]	Loss 0.0010 (0.0015)	
training:	Epoch: [135][75/233]	Loss 0.0005 (0.0015)	
training:	Epoch: [135][76/233]	Loss 0.0006 (0.0015)	
training:	Epoch: [135][77/233]	Loss 0.0005 (0.0015)	
training:	Epoch: [135][78/233]	Loss 0.0004 (0.0015)	
training:	Epoch: [135][79/233]	Loss 0.0005 (0.0015)	
training:	Epoch: [135][80/233]	Loss 0.0004 (0.0014)	
training:	Epoch: [135][81/233]	Loss 0.0004 (0.0014)	
training:	Epoch: [135][82/233]	Loss 0.0064 (0.0015)	
training:	Epoch: [135][83/233]	Loss 0.0005 (0.0015)	
training:	Epoch: [135][84/233]	Loss 0.0004 (0.0015)	
training:	Epoch: [135][85/233]	Loss 0.0014 (0.0015)	
training:	Epoch: [135][86/233]	Loss 0.0005 (0.0015)	
training:	Epoch: [135][87/233]	Loss 0.0004 (0.0014)	
training:	Epoch: [135][88/233]	Loss 0.0004 (0.0014)	
training:	Epoch: [135][89/233]	Loss 0.0004 (0.0014)	
training:	Epoch: [135][90/233]	Loss 0.0006 (0.0014)	
training:	Epoch: [135][91/233]	Loss 0.0004 (0.0014)	
training:	Epoch: [135][92/233]	Loss 0.0007 (0.0014)	
training:	Epoch: [135][93/233]	Loss 0.0006 (0.0014)	
training:	Epoch: [135][94/233]	Loss 0.0039 (0.0014)	
training:	Epoch: [135][95/233]	Loss 0.0005 (0.0014)	
training:	Epoch: [135][96/233]	Loss 0.0005 (0.0014)	
training:	Epoch: [135][97/233]	Loss 0.0005 (0.0014)	
training:	Epoch: [135][98/233]	Loss 0.0004 (0.0014)	
training:	Epoch: [135][99/233]	Loss 0.0048 (0.0014)	
training:	Epoch: [135][100/233]	Loss 0.0004 (0.0014)	
training:	Epoch: [135][101/233]	Loss 0.0007 (0.0014)	
training:	Epoch: [135][102/233]	Loss 0.0005 (0.0014)	
training:	Epoch: [135][103/233]	Loss 0.0005 (0.0014)	
training:	Epoch: [135][104/233]	Loss 0.0008 (0.0014)	
training:	Epoch: [135][105/233]	Loss 0.0004 (0.0014)	
training:	Epoch: [135][106/233]	Loss 0.0005 (0.0013)	
training:	Epoch: [135][107/233]	Loss 0.0647 (0.0019)	
training:	Epoch: [135][108/233]	Loss 0.0006 (0.0019)	
training:	Epoch: [135][109/233]	Loss 0.0004 (0.0019)	
training:	Epoch: [135][110/233]	Loss 0.0004 (0.0019)	
training:	Epoch: [135][111/233]	Loss 0.0005 (0.0019)	
training:	Epoch: [135][112/233]	Loss 0.0005 (0.0019)	
training:	Epoch: [135][113/233]	Loss 0.0004 (0.0019)	
training:	Epoch: [135][114/233]	Loss 0.1144 (0.0028)	
training:	Epoch: [135][115/233]	Loss 0.0004 (0.0028)	
training:	Epoch: [135][116/233]	Loss 0.0004 (0.0028)	
training:	Epoch: [135][117/233]	Loss 0.0003 (0.0028)	
training:	Epoch: [135][118/233]	Loss 0.0009 (0.0028)	
training:	Epoch: [135][119/233]	Loss 0.0004 (0.0027)	
training:	Epoch: [135][120/233]	Loss 0.0013 (0.0027)	
training:	Epoch: [135][121/233]	Loss 0.0004 (0.0027)	
training:	Epoch: [135][122/233]	Loss 0.0004 (0.0027)	
training:	Epoch: [135][123/233]	Loss 0.0004 (0.0027)	
training:	Epoch: [135][124/233]	Loss 0.0004 (0.0027)	
training:	Epoch: [135][125/233]	Loss 0.0005 (0.0026)	
training:	Epoch: [135][126/233]	Loss 0.0009 (0.0026)	
training:	Epoch: [135][127/233]	Loss 0.0092 (0.0027)	
training:	Epoch: [135][128/233]	Loss 0.0005 (0.0027)	
training:	Epoch: [135][129/233]	Loss 0.0004 (0.0026)	
training:	Epoch: [135][130/233]	Loss 0.0005 (0.0026)	
training:	Epoch: [135][131/233]	Loss 0.0004 (0.0026)	
training:	Epoch: [135][132/233]	Loss 0.0004 (0.0026)	
training:	Epoch: [135][133/233]	Loss 0.0005 (0.0026)	
training:	Epoch: [135][134/233]	Loss 0.0003 (0.0026)	
training:	Epoch: [135][135/233]	Loss 0.0005 (0.0026)	
training:	Epoch: [135][136/233]	Loss 0.0005 (0.0025)	
training:	Epoch: [135][137/233]	Loss 0.0004 (0.0025)	
training:	Epoch: [135][138/233]	Loss 0.0005 (0.0025)	
training:	Epoch: [135][139/233]	Loss 0.0004 (0.0025)	
training:	Epoch: [135][140/233]	Loss 0.0005 (0.0025)	
training:	Epoch: [135][141/233]	Loss 0.0005 (0.0025)	
training:	Epoch: [135][142/233]	Loss 0.0005 (0.0024)	
training:	Epoch: [135][143/233]	Loss 0.0007 (0.0024)	
training:	Epoch: [135][144/233]	Loss 0.0109 (0.0025)	
training:	Epoch: [135][145/233]	Loss 0.0009 (0.0025)	
training:	Epoch: [135][146/233]	Loss 0.0004 (0.0025)	
training:	Epoch: [135][147/233]	Loss 0.0004 (0.0025)	
training:	Epoch: [135][148/233]	Loss 0.0004 (0.0024)	
training:	Epoch: [135][149/233]	Loss 0.0004 (0.0024)	
training:	Epoch: [135][150/233]	Loss 0.0020 (0.0024)	
training:	Epoch: [135][151/233]	Loss 0.0005 (0.0024)	
training:	Epoch: [135][152/233]	Loss 0.0004 (0.0024)	
training:	Epoch: [135][153/233]	Loss 0.0005 (0.0024)	
training:	Epoch: [135][154/233]	Loss 0.0742 (0.0029)	
training:	Epoch: [135][155/233]	Loss 0.0014 (0.0028)	
training:	Epoch: [135][156/233]	Loss 0.0007 (0.0028)	
training:	Epoch: [135][157/233]	Loss 0.0004 (0.0028)	
training:	Epoch: [135][158/233]	Loss 0.0004 (0.0028)	
training:	Epoch: [135][159/233]	Loss 0.0004 (0.0028)	
training:	Epoch: [135][160/233]	Loss 0.0010 (0.0028)	
training:	Epoch: [135][161/233]	Loss 0.0004 (0.0028)	
training:	Epoch: [135][162/233]	Loss 0.0006 (0.0027)	
training:	Epoch: [135][163/233]	Loss 0.0016 (0.0027)	
training:	Epoch: [135][164/233]	Loss 0.0090 (0.0028)	
training:	Epoch: [135][165/233]	Loss 0.0006 (0.0028)	
training:	Epoch: [135][166/233]	Loss 0.0031 (0.0028)	
training:	Epoch: [135][167/233]	Loss 0.0005 (0.0027)	
training:	Epoch: [135][168/233]	Loss 0.0005 (0.0027)	
training:	Epoch: [135][169/233]	Loss 0.0009 (0.0027)	
training:	Epoch: [135][170/233]	Loss 0.0008 (0.0027)	
training:	Epoch: [135][171/233]	Loss 0.0448 (0.0030)	
training:	Epoch: [135][172/233]	Loss 0.0006 (0.0029)	
training:	Epoch: [135][173/233]	Loss 0.0005 (0.0029)	
training:	Epoch: [135][174/233]	Loss 0.0006 (0.0029)	
training:	Epoch: [135][175/233]	Loss 0.0004 (0.0029)	
training:	Epoch: [135][176/233]	Loss 0.0008 (0.0029)	
training:	Epoch: [135][177/233]	Loss 0.0004 (0.0029)	
training:	Epoch: [135][178/233]	Loss 0.0013 (0.0029)	
training:	Epoch: [135][179/233]	Loss 0.0004 (0.0029)	
training:	Epoch: [135][180/233]	Loss 0.0005 (0.0028)	
training:	Epoch: [135][181/233]	Loss 0.0004 (0.0028)	
training:	Epoch: [135][182/233]	Loss 0.0038 (0.0028)	
training:	Epoch: [135][183/233]	Loss 0.0005 (0.0028)	
training:	Epoch: [135][184/233]	Loss 0.0006 (0.0028)	
training:	Epoch: [135][185/233]	Loss 0.0004 (0.0028)	
training:	Epoch: [135][186/233]	Loss 0.0005 (0.0028)	
training:	Epoch: [135][187/233]	Loss 0.0004 (0.0028)	
training:	Epoch: [135][188/233]	Loss 0.0008 (0.0028)	
training:	Epoch: [135][189/233]	Loss 0.0005 (0.0027)	
training:	Epoch: [135][190/233]	Loss 0.0011 (0.0027)	
training:	Epoch: [135][191/233]	Loss 0.0011 (0.0027)	
training:	Epoch: [135][192/233]	Loss 0.0004 (0.0027)	
training:	Epoch: [135][193/233]	Loss 0.0013 (0.0027)	
training:	Epoch: [135][194/233]	Loss 0.0005 (0.0027)	
training:	Epoch: [135][195/233]	Loss 0.0022 (0.0027)	
training:	Epoch: [135][196/233]	Loss 0.0358 (0.0029)	
training:	Epoch: [135][197/233]	Loss 0.0004 (0.0029)	
training:	Epoch: [135][198/233]	Loss 0.0004 (0.0028)	
training:	Epoch: [135][199/233]	Loss 0.0006 (0.0028)	
training:	Epoch: [135][200/233]	Loss 0.0004 (0.0028)	
training:	Epoch: [135][201/233]	Loss 0.0004 (0.0028)	
training:	Epoch: [135][202/233]	Loss 0.0007 (0.0028)	
training:	Epoch: [135][203/233]	Loss 0.0004 (0.0028)	
training:	Epoch: [135][204/233]	Loss 0.0005 (0.0028)	
training:	Epoch: [135][205/233]	Loss 0.0005 (0.0028)	
training:	Epoch: [135][206/233]	Loss 0.0004 (0.0028)	
training:	Epoch: [135][207/233]	Loss 0.0004 (0.0027)	
training:	Epoch: [135][208/233]	Loss 0.0004 (0.0027)	
training:	Epoch: [135][209/233]	Loss 0.0040 (0.0027)	
training:	Epoch: [135][210/233]	Loss 0.0007 (0.0027)	
training:	Epoch: [135][211/233]	Loss 0.0531 (0.0030)	
training:	Epoch: [135][212/233]	Loss 0.0010 (0.0030)	
training:	Epoch: [135][213/233]	Loss 0.0019 (0.0030)	
training:	Epoch: [135][214/233]	Loss 0.0004 (0.0029)	
training:	Epoch: [135][215/233]	Loss 0.0006 (0.0029)	
training:	Epoch: [135][216/233]	Loss 0.0004 (0.0029)	
training:	Epoch: [135][217/233]	Loss 0.0004 (0.0029)	
training:	Epoch: [135][218/233]	Loss 0.0009 (0.0029)	
training:	Epoch: [135][219/233]	Loss 0.0004 (0.0029)	
training:	Epoch: [135][220/233]	Loss 0.0027 (0.0029)	
training:	Epoch: [135][221/233]	Loss 0.0018 (0.0029)	
training:	Epoch: [135][222/233]	Loss 0.0165 (0.0029)	
training:	Epoch: [135][223/233]	Loss 0.0004 (0.0029)	
training:	Epoch: [135][224/233]	Loss 0.0065 (0.0029)	
training:	Epoch: [135][225/233]	Loss 0.0005 (0.0029)	
training:	Epoch: [135][226/233]	Loss 0.0064 (0.0029)	
training:	Epoch: [135][227/233]	Loss 0.0005 (0.0029)	
training:	Epoch: [135][228/233]	Loss 0.0004 (0.0029)	
training:	Epoch: [135][229/233]	Loss 0.0005 (0.0029)	
training:	Epoch: [135][230/233]	Loss 0.0004 (0.0029)	
training:	Epoch: [135][231/233]	Loss 0.0004 (0.0029)	
training:	Epoch: [135][232/233]	Loss 0.0098 (0.0029)	
training:	Epoch: [135][233/233]	Loss 0.0004 (0.0029)	
Training:	 Loss: 0.0029

Training:	 ACC: 1.0000 1.0000 1.0000 1.0000
Validation:	 ACC: 0.7875 0.7892 0.8233 0.7517
Validation:	 Best_BACC: 0.8162 0.8154 0.7998 0.8326
Validation:	 Loss: 1.2439
Pretraining:	Epoch 136/200
----------
training:	Epoch: [136][1/233]	Loss 0.0005 (0.0005)	
training:	Epoch: [136][2/233]	Loss 0.0004 (0.0004)	
training:	Epoch: [136][3/233]	Loss 0.0224 (0.0077)	
training:	Epoch: [136][4/233]	Loss 0.0004 (0.0059)	
training:	Epoch: [136][5/233]	Loss 0.0006 (0.0049)	
training:	Epoch: [136][6/233]	Loss 0.0011 (0.0042)	
training:	Epoch: [136][7/233]	Loss 0.0006 (0.0037)	
training:	Epoch: [136][8/233]	Loss 0.0013 (0.0034)	
training:	Epoch: [136][9/233]	Loss 0.0005 (0.0031)	
training:	Epoch: [136][10/233]	Loss 0.0005 (0.0028)	
training:	Epoch: [136][11/233]	Loss 0.0004 (0.0026)	
training:	Epoch: [136][12/233]	Loss 0.0005 (0.0024)	
training:	Epoch: [136][13/233]	Loss 0.0006 (0.0023)	
training:	Epoch: [136][14/233]	Loss 0.0007 (0.0022)	
training:	Epoch: [136][15/233]	Loss 0.0004 (0.0021)	
training:	Epoch: [136][16/233]	Loss 0.0004 (0.0020)	
training:	Epoch: [136][17/233]	Loss 0.0005 (0.0019)	
training:	Epoch: [136][18/233]	Loss 0.0004 (0.0018)	
training:	Epoch: [136][19/233]	Loss 0.0005 (0.0017)	
training:	Epoch: [136][20/233]	Loss 0.1070 (0.0070)	
training:	Epoch: [136][21/233]	Loss 0.0005 (0.0067)	
training:	Epoch: [136][22/233]	Loss 0.0006 (0.0064)	
training:	Epoch: [136][23/233]	Loss 0.0004 (0.0061)	
training:	Epoch: [136][24/233]	Loss 0.0004 (0.0059)	
training:	Epoch: [136][25/233]	Loss 0.0390 (0.0072)	
training:	Epoch: [136][26/233]	Loss 0.0004 (0.0070)	
training:	Epoch: [136][27/233]	Loss 0.0005 (0.0067)	
training:	Epoch: [136][28/233]	Loss 0.0009 (0.0065)	
training:	Epoch: [136][29/233]	Loss 0.0006 (0.0063)	
training:	Epoch: [136][30/233]	Loss 0.0015 (0.0061)	
training:	Epoch: [136][31/233]	Loss 0.0012 (0.0060)	
training:	Epoch: [136][32/233]	Loss 0.0006 (0.0058)	
training:	Epoch: [136][33/233]	Loss 0.0018 (0.0057)	
training:	Epoch: [136][34/233]	Loss 0.0006 (0.0055)	
training:	Epoch: [136][35/233]	Loss 0.0004 (0.0054)	
training:	Epoch: [136][36/233]	Loss 0.0006 (0.0053)	
training:	Epoch: [136][37/233]	Loss 0.0007 (0.0051)	
training:	Epoch: [136][38/233]	Loss 0.0004 (0.0050)	
training:	Epoch: [136][39/233]	Loss 0.0789 (0.0069)	
training:	Epoch: [136][40/233]	Loss 0.0005 (0.0068)	
training:	Epoch: [136][41/233]	Loss 0.0004 (0.0066)	
training:	Epoch: [136][42/233]	Loss 0.0009 (0.0065)	
training:	Epoch: [136][43/233]	Loss 0.0007 (0.0063)	
training:	Epoch: [136][44/233]	Loss 0.0158 (0.0065)	
training:	Epoch: [136][45/233]	Loss 0.0012 (0.0064)	
training:	Epoch: [136][46/233]	Loss 0.0004 (0.0063)	
training:	Epoch: [136][47/233]	Loss 0.0004 (0.0062)	
training:	Epoch: [136][48/233]	Loss 0.0004 (0.0060)	
training:	Epoch: [136][49/233]	Loss 0.0007 (0.0059)	
training:	Epoch: [136][50/233]	Loss 0.0004 (0.0058)	
training:	Epoch: [136][51/233]	Loss 0.0005 (0.0057)	
training:	Epoch: [136][52/233]	Loss 0.0009 (0.0056)	
training:	Epoch: [136][53/233]	Loss 0.0006 (0.0055)	
training:	Epoch: [136][54/233]	Loss 0.0013 (0.0055)	
training:	Epoch: [136][55/233]	Loss 0.0017 (0.0054)	
training:	Epoch: [136][56/233]	Loss 0.0006 (0.0053)	
training:	Epoch: [136][57/233]	Loss 0.0004 (0.0052)	
training:	Epoch: [136][58/233]	Loss 0.0006 (0.0051)	
training:	Epoch: [136][59/233]	Loss 0.0643 (0.0061)	
training:	Epoch: [136][60/233]	Loss 0.0022 (0.0061)	
training:	Epoch: [136][61/233]	Loss 0.0007 (0.0060)	
training:	Epoch: [136][62/233]	Loss 0.0004 (0.0059)	
training:	Epoch: [136][63/233]	Loss 0.0004 (0.0058)	
training:	Epoch: [136][64/233]	Loss 0.0009 (0.0057)	
training:	Epoch: [136][65/233]	Loss 0.0006 (0.0057)	
training:	Epoch: [136][66/233]	Loss 0.0004 (0.0056)	
training:	Epoch: [136][67/233]	Loss 0.0004 (0.0055)	
training:	Epoch: [136][68/233]	Loss 0.0009 (0.0054)	
training:	Epoch: [136][69/233]	Loss 0.0054 (0.0054)	
training:	Epoch: [136][70/233]	Loss 0.0054 (0.0054)	
training:	Epoch: [136][71/233]	Loss 0.0014 (0.0054)	
training:	Epoch: [136][72/233]	Loss 0.0015 (0.0053)	
training:	Epoch: [136][73/233]	Loss 0.0007 (0.0053)	
training:	Epoch: [136][74/233]	Loss 0.0016 (0.0052)	
training:	Epoch: [136][75/233]	Loss 0.0042 (0.0052)	
training:	Epoch: [136][76/233]	Loss 0.0031 (0.0052)	
training:	Epoch: [136][77/233]	Loss 0.0867 (0.0062)	
training:	Epoch: [136][78/233]	Loss 0.0394 (0.0066)	
training:	Epoch: [136][79/233]	Loss 0.0004 (0.0066)	
training:	Epoch: [136][80/233]	Loss 0.0004 (0.0065)	
training:	Epoch: [136][81/233]	Loss 0.0005 (0.0064)	
training:	Epoch: [136][82/233]	Loss 0.0005 (0.0063)	
training:	Epoch: [136][83/233]	Loss 0.0007 (0.0063)	
training:	Epoch: [136][84/233]	Loss 0.0004 (0.0062)	
training:	Epoch: [136][85/233]	Loss 0.0007 (0.0061)	
training:	Epoch: [136][86/233]	Loss 0.0005 (0.0061)	
training:	Epoch: [136][87/233]	Loss 0.0006 (0.0060)	
training:	Epoch: [136][88/233]	Loss 0.0006 (0.0060)	
training:	Epoch: [136][89/233]	Loss 0.0004 (0.0059)	
training:	Epoch: [136][90/233]	Loss 0.0010 (0.0058)	
training:	Epoch: [136][91/233]	Loss 0.0004 (0.0058)	
training:	Epoch: [136][92/233]	Loss 0.0010 (0.0057)	
training:	Epoch: [136][93/233]	Loss 0.0006 (0.0057)	
training:	Epoch: [136][94/233]	Loss 0.0288 (0.0059)	
training:	Epoch: [136][95/233]	Loss 0.0008 (0.0059)	
training:	Epoch: [136][96/233]	Loss 0.0013 (0.0058)	
training:	Epoch: [136][97/233]	Loss 0.0962 (0.0067)	
training:	Epoch: [136][98/233]	Loss 0.0087 (0.0068)	
training:	Epoch: [136][99/233]	Loss 0.0012 (0.0067)	
training:	Epoch: [136][100/233]	Loss 0.0005 (0.0067)	
training:	Epoch: [136][101/233]	Loss 0.0004 (0.0066)	
training:	Epoch: [136][102/233]	Loss 0.0005 (0.0065)	
training:	Epoch: [136][103/233]	Loss 0.0004 (0.0065)	
training:	Epoch: [136][104/233]	Loss 0.0005 (0.0064)	
training:	Epoch: [136][105/233]	Loss 0.0014 (0.0064)	
training:	Epoch: [136][106/233]	Loss 0.0007 (0.0063)	
training:	Epoch: [136][107/233]	Loss 0.0034 (0.0063)	
training:	Epoch: [136][108/233]	Loss 0.0003 (0.0062)	
training:	Epoch: [136][109/233]	Loss 0.0006 (0.0062)	
training:	Epoch: [136][110/233]	Loss 0.0006 (0.0061)	
training:	Epoch: [136][111/233]	Loss 0.0009 (0.0061)	
training:	Epoch: [136][112/233]	Loss 0.0008 (0.0060)	
training:	Epoch: [136][113/233]	Loss 0.0005 (0.0060)	
training:	Epoch: [136][114/233]	Loss 0.0004 (0.0059)	
training:	Epoch: [136][115/233]	Loss 0.0007 (0.0059)	
training:	Epoch: [136][116/233]	Loss 0.0012 (0.0058)	
training:	Epoch: [136][117/233]	Loss 0.0005 (0.0058)	
training:	Epoch: [136][118/233]	Loss 0.0172 (0.0059)	
training:	Epoch: [136][119/233]	Loss 0.0017 (0.0059)	
training:	Epoch: [136][120/233]	Loss 0.0007 (0.0058)	
training:	Epoch: [136][121/233]	Loss 0.0004 (0.0058)	
training:	Epoch: [136][122/233]	Loss 0.0007 (0.0057)	
training:	Epoch: [136][123/233]	Loss 0.0006 (0.0057)	
training:	Epoch: [136][124/233]	Loss 0.0009 (0.0057)	
training:	Epoch: [136][125/233]	Loss 0.0022 (0.0056)	
training:	Epoch: [136][126/233]	Loss 0.0024 (0.0056)	
training:	Epoch: [136][127/233]	Loss 0.0005 (0.0056)	
training:	Epoch: [136][128/233]	Loss 0.0005 (0.0055)	
training:	Epoch: [136][129/233]	Loss 0.0013 (0.0055)	
training:	Epoch: [136][130/233]	Loss 0.0004 (0.0054)	
training:	Epoch: [136][131/233]	Loss 0.0006 (0.0054)	
training:	Epoch: [136][132/233]	Loss 0.0007 (0.0054)	
training:	Epoch: [136][133/233]	Loss 0.0008 (0.0053)	
training:	Epoch: [136][134/233]	Loss 0.0023 (0.0053)	
training:	Epoch: [136][135/233]	Loss 0.0004 (0.0053)	
training:	Epoch: [136][136/233]	Loss 0.0004 (0.0052)	
training:	Epoch: [136][137/233]	Loss 0.0004 (0.0052)	
training:	Epoch: [136][138/233]	Loss 0.0004 (0.0052)	
training:	Epoch: [136][139/233]	Loss 0.0004 (0.0051)	
training:	Epoch: [136][140/233]	Loss 0.0004 (0.0051)	
training:	Epoch: [136][141/233]	Loss 0.0005 (0.0051)	
training:	Epoch: [136][142/233]	Loss 0.0008 (0.0050)	
training:	Epoch: [136][143/233]	Loss 0.0017 (0.0050)	
training:	Epoch: [136][144/233]	Loss 0.0006 (0.0050)	
training:	Epoch: [136][145/233]	Loss 0.0004 (0.0050)	
training:	Epoch: [136][146/233]	Loss 0.0023 (0.0049)	
training:	Epoch: [136][147/233]	Loss 0.0006 (0.0049)	
training:	Epoch: [136][148/233]	Loss 0.0027 (0.0049)	
training:	Epoch: [136][149/233]	Loss 0.0048 (0.0049)	
training:	Epoch: [136][150/233]	Loss 0.1669 (0.0060)	
training:	Epoch: [136][151/233]	Loss 0.0006 (0.0059)	
training:	Epoch: [136][152/233]	Loss 0.0003 (0.0059)	
training:	Epoch: [136][153/233]	Loss 0.0005 (0.0059)	
training:	Epoch: [136][154/233]	Loss 0.0009 (0.0058)	
training:	Epoch: [136][155/233]	Loss 0.0005 (0.0058)	
training:	Epoch: [136][156/233]	Loss 0.0036 (0.0058)	
training:	Epoch: [136][157/233]	Loss 0.1897 (0.0070)	
training:	Epoch: [136][158/233]	Loss 0.0004 (0.0069)	
training:	Epoch: [136][159/233]	Loss 0.0004 (0.0069)	
training:	Epoch: [136][160/233]	Loss 0.0004 (0.0068)	
training:	Epoch: [136][161/233]	Loss 0.0007 (0.0068)	
training:	Epoch: [136][162/233]	Loss 0.0005 (0.0068)	
training:	Epoch: [136][163/233]	Loss 0.0013 (0.0067)	
training:	Epoch: [136][164/233]	Loss 0.2339 (0.0081)	
training:	Epoch: [136][165/233]	Loss 0.0011 (0.0081)	
training:	Epoch: [136][166/233]	Loss 0.0092 (0.0081)	
training:	Epoch: [136][167/233]	Loss 0.0007 (0.0080)	
training:	Epoch: [136][168/233]	Loss 0.0008 (0.0080)	
training:	Epoch: [136][169/233]	Loss 0.0006 (0.0079)	
training:	Epoch: [136][170/233]	Loss 0.0005 (0.0079)	
training:	Epoch: [136][171/233]	Loss 0.0009 (0.0079)	
training:	Epoch: [136][172/233]	Loss 0.0004 (0.0078)	
training:	Epoch: [136][173/233]	Loss 0.0004 (0.0078)	
training:	Epoch: [136][174/233]	Loss 0.0069 (0.0078)	
training:	Epoch: [136][175/233]	Loss 0.0009 (0.0077)	
training:	Epoch: [136][176/233]	Loss 0.0004 (0.0077)	
training:	Epoch: [136][177/233]	Loss 0.0005 (0.0076)	
training:	Epoch: [136][178/233]	Loss 0.0005 (0.0076)	
training:	Epoch: [136][179/233]	Loss 0.0005 (0.0076)	
training:	Epoch: [136][180/233]	Loss 0.0010 (0.0075)	
training:	Epoch: [136][181/233]	Loss 0.0008 (0.0075)	
training:	Epoch: [136][182/233]	Loss 0.0009 (0.0075)	
training:	Epoch: [136][183/233]	Loss 0.0006 (0.0074)	
training:	Epoch: [136][184/233]	Loss 0.0004 (0.0074)	
training:	Epoch: [136][185/233]	Loss 0.0005 (0.0073)	
training:	Epoch: [136][186/233]	Loss 0.0053 (0.0073)	
training:	Epoch: [136][187/233]	Loss 0.0011 (0.0073)	
training:	Epoch: [136][188/233]	Loss 0.0005 (0.0073)	
training:	Epoch: [136][189/233]	Loss 0.0004 (0.0072)	
training:	Epoch: [136][190/233]	Loss 0.0009 (0.0072)	
training:	Epoch: [136][191/233]	Loss 0.0049 (0.0072)	
training:	Epoch: [136][192/233]	Loss 0.0009 (0.0071)	
training:	Epoch: [136][193/233]	Loss 0.0006 (0.0071)	
training:	Epoch: [136][194/233]	Loss 0.0006 (0.0071)	
training:	Epoch: [136][195/233]	Loss 0.0017 (0.0071)	
training:	Epoch: [136][196/233]	Loss 0.0004 (0.0070)	
training:	Epoch: [136][197/233]	Loss 0.0005 (0.0070)	
training:	Epoch: [136][198/233]	Loss 0.0012 (0.0070)	
training:	Epoch: [136][199/233]	Loss 0.0005 (0.0069)	
training:	Epoch: [136][200/233]	Loss 0.0006 (0.0069)	
training:	Epoch: [136][201/233]	Loss 0.0004 (0.0069)	
training:	Epoch: [136][202/233]	Loss 0.0004 (0.0068)	
training:	Epoch: [136][203/233]	Loss 0.0004 (0.0068)	
training:	Epoch: [136][204/233]	Loss 0.0005 (0.0068)	
training:	Epoch: [136][205/233]	Loss 0.0004 (0.0067)	
training:	Epoch: [136][206/233]	Loss 0.0004 (0.0067)	
training:	Epoch: [136][207/233]	Loss 0.0005 (0.0067)	
training:	Epoch: [136][208/233]	Loss 0.0005 (0.0066)	
training:	Epoch: [136][209/233]	Loss 0.0005 (0.0066)	
training:	Epoch: [136][210/233]	Loss 0.0004 (0.0066)	
training:	Epoch: [136][211/233]	Loss 0.0004 (0.0066)	
training:	Epoch: [136][212/233]	Loss 0.0006 (0.0065)	
training:	Epoch: [136][213/233]	Loss 0.0009 (0.0065)	
training:	Epoch: [136][214/233]	Loss 0.0005 (0.0065)	
training:	Epoch: [136][215/233]	Loss 0.0004 (0.0064)	
training:	Epoch: [136][216/233]	Loss 0.0004 (0.0064)	
training:	Epoch: [136][217/233]	Loss 0.0005 (0.0064)	
training:	Epoch: [136][218/233]	Loss 0.0007 (0.0064)	
training:	Epoch: [136][219/233]	Loss 0.0004 (0.0063)	
training:	Epoch: [136][220/233]	Loss 0.0006 (0.0063)	
training:	Epoch: [136][221/233]	Loss 0.0005 (0.0063)	
training:	Epoch: [136][222/233]	Loss 0.0006 (0.0063)	
training:	Epoch: [136][223/233]	Loss 0.0005 (0.0062)	
training:	Epoch: [136][224/233]	Loss 0.0003 (0.0062)	
training:	Epoch: [136][225/233]	Loss 0.0006 (0.0062)	
training:	Epoch: [136][226/233]	Loss 0.0007 (0.0062)	
training:	Epoch: [136][227/233]	Loss 0.0005 (0.0061)	
training:	Epoch: [136][228/233]	Loss 0.0007 (0.0061)	
training:	Epoch: [136][229/233]	Loss 0.0004 (0.0061)	
training:	Epoch: [136][230/233]	Loss 0.0004 (0.0061)	
training:	Epoch: [136][231/233]	Loss 0.0007 (0.0060)	
training:	Epoch: [136][232/233]	Loss 0.0004 (0.0060)	
training:	Epoch: [136][233/233]	Loss 0.0004 (0.0060)	
Training:	 Loss: 0.0060

Training:	 ACC: 1.0000 1.0000 1.0000 1.0000
Validation:	 ACC: 0.7934 0.7940 0.8069 0.7798
Validation:	 Best_BACC: 0.8162 0.8154 0.7998 0.8326
Validation:	 Loss: 1.2057
Pretraining:	Epoch 137/200
----------
training:	Epoch: [137][1/233]	Loss 0.0009 (0.0009)	
training:	Epoch: [137][2/233]	Loss 0.0004 (0.0006)	
training:	Epoch: [137][3/233]	Loss 0.0014 (0.0009)	
training:	Epoch: [137][4/233]	Loss 0.0004 (0.0008)	
training:	Epoch: [137][5/233]	Loss 0.0004 (0.0007)	
training:	Epoch: [137][6/233]	Loss 0.0004 (0.0007)	
training:	Epoch: [137][7/233]	Loss 0.0004 (0.0006)	
training:	Epoch: [137][8/233]	Loss 0.0004 (0.0006)	
training:	Epoch: [137][9/233]	Loss 0.0005 (0.0006)	
training:	Epoch: [137][10/233]	Loss 0.0009 (0.0006)	
training:	Epoch: [137][11/233]	Loss 0.0004 (0.0006)	
training:	Epoch: [137][12/233]	Loss 0.0006 (0.0006)	
training:	Epoch: [137][13/233]	Loss 0.0005 (0.0006)	
training:	Epoch: [137][14/233]	Loss 0.0008 (0.0006)	
training:	Epoch: [137][15/233]	Loss 0.0004 (0.0006)	
training:	Epoch: [137][16/233]	Loss 0.0003 (0.0006)	
training:	Epoch: [137][17/233]	Loss 0.0004 (0.0006)	
training:	Epoch: [137][18/233]	Loss 0.0014 (0.0006)	
training:	Epoch: [137][19/233]	Loss 0.0023 (0.0007)	
training:	Epoch: [137][20/233]	Loss 0.0003 (0.0007)	
training:	Epoch: [137][21/233]	Loss 0.0039 (0.0008)	
training:	Epoch: [137][22/233]	Loss 0.0005 (0.0008)	
training:	Epoch: [137][23/233]	Loss 0.0005 (0.0008)	
training:	Epoch: [137][24/233]	Loss 0.0004 (0.0008)	
training:	Epoch: [137][25/233]	Loss 0.0004 (0.0008)	
training:	Epoch: [137][26/233]	Loss 0.0004 (0.0008)	
training:	Epoch: [137][27/233]	Loss 0.0004 (0.0008)	
training:	Epoch: [137][28/233]	Loss 0.0004 (0.0007)	
training:	Epoch: [137][29/233]	Loss 0.0005 (0.0007)	
training:	Epoch: [137][30/233]	Loss 0.0004 (0.0007)	
training:	Epoch: [137][31/233]	Loss 0.0005 (0.0007)	
training:	Epoch: [137][32/233]	Loss 0.0005 (0.0007)	
training:	Epoch: [137][33/233]	Loss 0.0010 (0.0007)	
training:	Epoch: [137][34/233]	Loss 0.0006 (0.0007)	
training:	Epoch: [137][35/233]	Loss 0.0005 (0.0007)	
training:	Epoch: [137][36/233]	Loss 0.0004 (0.0007)	
training:	Epoch: [137][37/233]	Loss 0.0006 (0.0007)	
training:	Epoch: [137][38/233]	Loss 0.0004 (0.0007)	
training:	Epoch: [137][39/233]	Loss 0.0004 (0.0007)	
training:	Epoch: [137][40/233]	Loss 0.0006 (0.0007)	
training:	Epoch: [137][41/233]	Loss 0.0004 (0.0007)	
training:	Epoch: [137][42/233]	Loss 0.0004 (0.0007)	
training:	Epoch: [137][43/233]	Loss 0.2295 (0.0060)	
training:	Epoch: [137][44/233]	Loss 0.0096 (0.0061)	
training:	Epoch: [137][45/233]	Loss 0.0004 (0.0059)	
training:	Epoch: [137][46/233]	Loss 0.0005 (0.0058)	
training:	Epoch: [137][47/233]	Loss 0.0028 (0.0058)	
training:	Epoch: [137][48/233]	Loss 0.0005 (0.0057)	
training:	Epoch: [137][49/233]	Loss 0.0005 (0.0055)	
training:	Epoch: [137][50/233]	Loss 0.0005 (0.0054)	
training:	Epoch: [137][51/233]	Loss 0.0004 (0.0053)	
training:	Epoch: [137][52/233]	Loss 0.0014 (0.0053)	
training:	Epoch: [137][53/233]	Loss 0.0004 (0.0052)	
training:	Epoch: [137][54/233]	Loss 0.0005 (0.0051)	
training:	Epoch: [137][55/233]	Loss 0.0004 (0.0050)	
training:	Epoch: [137][56/233]	Loss 0.1020 (0.0067)	
training:	Epoch: [137][57/233]	Loss 0.0004 (0.0066)	
training:	Epoch: [137][58/233]	Loss 0.0004 (0.0065)	
training:	Epoch: [137][59/233]	Loss 0.0004 (0.0064)	
training:	Epoch: [137][60/233]	Loss 0.0005 (0.0063)	
training:	Epoch: [137][61/233]	Loss 0.0006 (0.0062)	
training:	Epoch: [137][62/233]	Loss 0.0004 (0.0061)	
training:	Epoch: [137][63/233]	Loss 0.0010 (0.0060)	
training:	Epoch: [137][64/233]	Loss 0.0025 (0.0060)	
training:	Epoch: [137][65/233]	Loss 0.0004 (0.0059)	
training:	Epoch: [137][66/233]	Loss 0.0006 (0.0058)	
training:	Epoch: [137][67/233]	Loss 0.0004 (0.0057)	
training:	Epoch: [137][68/233]	Loss 0.0007 (0.0057)	
training:	Epoch: [137][69/233]	Loss 0.0004 (0.0056)	
training:	Epoch: [137][70/233]	Loss 0.0004 (0.0055)	
training:	Epoch: [137][71/233]	Loss 0.0032 (0.0055)	
training:	Epoch: [137][72/233]	Loss 0.0007 (0.0054)	
training:	Epoch: [137][73/233]	Loss 0.0004 (0.0054)	
training:	Epoch: [137][74/233]	Loss 0.0004 (0.0053)	
training:	Epoch: [137][75/233]	Loss 0.0005 (0.0052)	
training:	Epoch: [137][76/233]	Loss 0.0007 (0.0052)	
training:	Epoch: [137][77/233]	Loss 0.0004 (0.0051)	
training:	Epoch: [137][78/233]	Loss 0.0004 (0.0050)	
training:	Epoch: [137][79/233]	Loss 0.0004 (0.0050)	
training:	Epoch: [137][80/233]	Loss 0.0007 (0.0049)	
training:	Epoch: [137][81/233]	Loss 0.0006 (0.0049)	
training:	Epoch: [137][82/233]	Loss 0.0006 (0.0048)	
training:	Epoch: [137][83/233]	Loss 0.0005 (0.0048)	
training:	Epoch: [137][84/233]	Loss 0.0004 (0.0047)	
training:	Epoch: [137][85/233]	Loss 0.0009 (0.0047)	
training:	Epoch: [137][86/233]	Loss 0.0028 (0.0047)	
training:	Epoch: [137][87/233]	Loss 0.0008 (0.0046)	
training:	Epoch: [137][88/233]	Loss 0.0004 (0.0046)	
training:	Epoch: [137][89/233]	Loss 0.0004 (0.0045)	
training:	Epoch: [137][90/233]	Loss 0.0004 (0.0045)	
training:	Epoch: [137][91/233]	Loss 0.0006 (0.0044)	
training:	Epoch: [137][92/233]	Loss 0.0004 (0.0044)	
training:	Epoch: [137][93/233]	Loss 0.0004 (0.0043)	
training:	Epoch: [137][94/233]	Loss 0.0005 (0.0043)	
training:	Epoch: [137][95/233]	Loss 0.0004 (0.0043)	
training:	Epoch: [137][96/233]	Loss 0.0211 (0.0044)	
training:	Epoch: [137][97/233]	Loss 0.0008 (0.0044)	
training:	Epoch: [137][98/233]	Loss 0.0004 (0.0044)	
training:	Epoch: [137][99/233]	Loss 0.0006 (0.0043)	
training:	Epoch: [137][100/233]	Loss 0.0004 (0.0043)	
training:	Epoch: [137][101/233]	Loss 0.0004 (0.0042)	
training:	Epoch: [137][102/233]	Loss 0.0010 (0.0042)	
training:	Epoch: [137][103/233]	Loss 0.0004 (0.0042)	
training:	Epoch: [137][104/233]	Loss 0.0005 (0.0041)	
training:	Epoch: [137][105/233]	Loss 0.0054 (0.0041)	
training:	Epoch: [137][106/233]	Loss 0.0269 (0.0044)	
training:	Epoch: [137][107/233]	Loss 0.0005 (0.0043)	
training:	Epoch: [137][108/233]	Loss 0.0005 (0.0043)	
training:	Epoch: [137][109/233]	Loss 0.0004 (0.0043)	
training:	Epoch: [137][110/233]	Loss 0.0059 (0.0043)	
training:	Epoch: [137][111/233]	Loss 0.0099 (0.0043)	
training:	Epoch: [137][112/233]	Loss 0.0007 (0.0043)	
training:	Epoch: [137][113/233]	Loss 0.0005 (0.0043)	
training:	Epoch: [137][114/233]	Loss 0.0005 (0.0042)	
training:	Epoch: [137][115/233]	Loss 0.0006 (0.0042)	
training:	Epoch: [137][116/233]	Loss 0.0005 (0.0042)	
training:	Epoch: [137][117/233]	Loss 0.0006 (0.0041)	
training:	Epoch: [137][118/233]	Loss 0.0005 (0.0041)	
training:	Epoch: [137][119/233]	Loss 0.0010 (0.0041)	
training:	Epoch: [137][120/233]	Loss 0.0004 (0.0040)	
training:	Epoch: [137][121/233]	Loss 0.0006 (0.0040)	
training:	Epoch: [137][122/233]	Loss 0.0015 (0.0040)	
training:	Epoch: [137][123/233]	Loss 0.0005 (0.0040)	
training:	Epoch: [137][124/233]	Loss 0.0004 (0.0039)	
training:	Epoch: [137][125/233]	Loss 0.0004 (0.0039)	
training:	Epoch: [137][126/233]	Loss 0.0006 (0.0039)	
training:	Epoch: [137][127/233]	Loss 0.0005 (0.0039)	
training:	Epoch: [137][128/233]	Loss 0.0004 (0.0038)	
training:	Epoch: [137][129/233]	Loss 0.0004 (0.0038)	
training:	Epoch: [137][130/233]	Loss 0.0007 (0.0038)	
training:	Epoch: [137][131/233]	Loss 0.0008 (0.0038)	
training:	Epoch: [137][132/233]	Loss 0.0004 (0.0037)	
training:	Epoch: [137][133/233]	Loss 0.0004 (0.0037)	
training:	Epoch: [137][134/233]	Loss 0.0010 (0.0037)	
training:	Epoch: [137][135/233]	Loss 0.0004 (0.0037)	
training:	Epoch: [137][136/233]	Loss 0.0004 (0.0036)	
training:	Epoch: [137][137/233]	Loss 0.0010 (0.0036)	
training:	Epoch: [137][138/233]	Loss 0.0004 (0.0036)	
training:	Epoch: [137][139/233]	Loss 0.0011 (0.0036)	
training:	Epoch: [137][140/233]	Loss 0.0007 (0.0036)	
training:	Epoch: [137][141/233]	Loss 0.0018 (0.0035)	
training:	Epoch: [137][142/233]	Loss 0.0005 (0.0035)	
training:	Epoch: [137][143/233]	Loss 0.0092 (0.0036)	
training:	Epoch: [137][144/233]	Loss 0.0017 (0.0035)	
training:	Epoch: [137][145/233]	Loss 0.0008 (0.0035)	
training:	Epoch: [137][146/233]	Loss 0.0004 (0.0035)	
training:	Epoch: [137][147/233]	Loss 0.0004 (0.0035)	
training:	Epoch: [137][148/233]	Loss 0.0052 (0.0035)	
training:	Epoch: [137][149/233]	Loss 0.0033 (0.0035)	
training:	Epoch: [137][150/233]	Loss 0.0004 (0.0035)	
training:	Epoch: [137][151/233]	Loss 0.0005 (0.0035)	
training:	Epoch: [137][152/233]	Loss 0.0004 (0.0034)	
training:	Epoch: [137][153/233]	Loss 0.0004 (0.0034)	
training:	Epoch: [137][154/233]	Loss 0.0008 (0.0034)	
training:	Epoch: [137][155/233]	Loss 0.0004 (0.0034)	
training:	Epoch: [137][156/233]	Loss 0.0004 (0.0034)	
training:	Epoch: [137][157/233]	Loss 0.0004 (0.0033)	
training:	Epoch: [137][158/233]	Loss 0.0004 (0.0033)	
training:	Epoch: [137][159/233]	Loss 0.0004 (0.0033)	
training:	Epoch: [137][160/233]	Loss 0.0033 (0.0033)	
training:	Epoch: [137][161/233]	Loss 0.0005 (0.0033)	
training:	Epoch: [137][162/233]	Loss 0.0028 (0.0033)	
training:	Epoch: [137][163/233]	Loss 0.0008 (0.0033)	
training:	Epoch: [137][164/233]	Loss 0.0140 (0.0033)	
training:	Epoch: [137][165/233]	Loss 0.0004 (0.0033)	
training:	Epoch: [137][166/233]	Loss 0.0004 (0.0033)	
training:	Epoch: [137][167/233]	Loss 0.0004 (0.0033)	
training:	Epoch: [137][168/233]	Loss 0.0004 (0.0033)	
training:	Epoch: [137][169/233]	Loss 0.0005 (0.0032)	
training:	Epoch: [137][170/233]	Loss 0.0005 (0.0032)	
training:	Epoch: [137][171/233]	Loss 0.1635 (0.0042)	
training:	Epoch: [137][172/233]	Loss 0.0007 (0.0041)	
training:	Epoch: [137][173/233]	Loss 0.0004 (0.0041)	
training:	Epoch: [137][174/233]	Loss 0.0009 (0.0041)	
training:	Epoch: [137][175/233]	Loss 0.0004 (0.0041)	
training:	Epoch: [137][176/233]	Loss 0.0008 (0.0041)	
training:	Epoch: [137][177/233]	Loss 0.0004 (0.0040)	
training:	Epoch: [137][178/233]	Loss 0.0004 (0.0040)	
training:	Epoch: [137][179/233]	Loss 0.0004 (0.0040)	
training:	Epoch: [137][180/233]	Loss 0.0004 (0.0040)	
training:	Epoch: [137][181/233]	Loss 0.0008 (0.0040)	
training:	Epoch: [137][182/233]	Loss 0.0005 (0.0039)	
training:	Epoch: [137][183/233]	Loss 0.0181 (0.0040)	
training:	Epoch: [137][184/233]	Loss 0.0004 (0.0040)	
training:	Epoch: [137][185/233]	Loss 0.0006 (0.0040)	
training:	Epoch: [137][186/233]	Loss 0.0005 (0.0040)	
training:	Epoch: [137][187/233]	Loss 0.0004 (0.0039)	
training:	Epoch: [137][188/233]	Loss 0.0006 (0.0039)	
training:	Epoch: [137][189/233]	Loss 0.0004 (0.0039)	
training:	Epoch: [137][190/233]	Loss 0.0010 (0.0039)	
training:	Epoch: [137][191/233]	Loss 0.0012 (0.0039)	
training:	Epoch: [137][192/233]	Loss 0.0028 (0.0039)	
training:	Epoch: [137][193/233]	Loss 0.0010 (0.0039)	
training:	Epoch: [137][194/233]	Loss 0.0004 (0.0038)	
training:	Epoch: [137][195/233]	Loss 0.0012 (0.0038)	
training:	Epoch: [137][196/233]	Loss 0.0024 (0.0038)	
training:	Epoch: [137][197/233]	Loss 0.0004 (0.0038)	
training:	Epoch: [137][198/233]	Loss 0.0004 (0.0038)	
training:	Epoch: [137][199/233]	Loss 0.0006 (0.0038)	
training:	Epoch: [137][200/233]	Loss 0.0027 (0.0038)	
training:	Epoch: [137][201/233]	Loss 0.0005 (0.0038)	
training:	Epoch: [137][202/233]	Loss 0.0009 (0.0037)	
training:	Epoch: [137][203/233]	Loss 0.0009 (0.0037)	
training:	Epoch: [137][204/233]	Loss 0.0005 (0.0037)	
training:	Epoch: [137][205/233]	Loss 0.0004 (0.0037)	
training:	Epoch: [137][206/233]	Loss 0.0008 (0.0037)	
training:	Epoch: [137][207/233]	Loss 0.0004 (0.0037)	
training:	Epoch: [137][208/233]	Loss 0.0007 (0.0036)	
training:	Epoch: [137][209/233]	Loss 0.0186 (0.0037)	
training:	Epoch: [137][210/233]	Loss 0.0006 (0.0037)	
training:	Epoch: [137][211/233]	Loss 0.0004 (0.0037)	
training:	Epoch: [137][212/233]	Loss 0.0004 (0.0037)	
training:	Epoch: [137][213/233]	Loss 0.0011 (0.0037)	
training:	Epoch: [137][214/233]	Loss 0.0020 (0.0037)	
training:	Epoch: [137][215/233]	Loss 0.0004 (0.0036)	
training:	Epoch: [137][216/233]	Loss 0.0009 (0.0036)	
training:	Epoch: [137][217/233]	Loss 0.0004 (0.0036)	
training:	Epoch: [137][218/233]	Loss 0.0044 (0.0036)	
training:	Epoch: [137][219/233]	Loss 0.0003 (0.0036)	
training:	Epoch: [137][220/233]	Loss 0.0005 (0.0036)	
training:	Epoch: [137][221/233]	Loss 0.0010 (0.0036)	
training:	Epoch: [137][222/233]	Loss 0.0106 (0.0036)	
training:	Epoch: [137][223/233]	Loss 0.0021 (0.0036)	
training:	Epoch: [137][224/233]	Loss 0.0004 (0.0036)	
training:	Epoch: [137][225/233]	Loss 0.0004 (0.0036)	
training:	Epoch: [137][226/233]	Loss 0.1194 (0.0041)	
training:	Epoch: [137][227/233]	Loss 0.0017 (0.0041)	
training:	Epoch: [137][228/233]	Loss 0.0008 (0.0041)	
training:	Epoch: [137][229/233]	Loss 0.0005 (0.0040)	
training:	Epoch: [137][230/233]	Loss 0.0004 (0.0040)	
training:	Epoch: [137][231/233]	Loss 0.0004 (0.0040)	
training:	Epoch: [137][232/233]	Loss 0.0004 (0.0040)	
training:	Epoch: [137][233/233]	Loss 0.0004 (0.0040)	
Training:	 Loss: 0.0040

Training:	 ACC: 1.0000 1.0000 1.0000 1.0000
Validation:	 ACC: 0.7935 0.7919 0.7600 0.8270
Validation:	 Best_BACC: 0.8162 0.8154 0.7998 0.8326
Validation:	 Loss: 1.2537
Pretraining:	Epoch 138/200
----------
training:	Epoch: [138][1/233]	Loss 0.0005 (0.0005)	
training:	Epoch: [138][2/233]	Loss 0.0006 (0.0006)	
training:	Epoch: [138][3/233]	Loss 0.0007 (0.0006)	
training:	Epoch: [138][4/233]	Loss 0.0004 (0.0006)	
training:	Epoch: [138][5/233]	Loss 0.0012 (0.0007)	
training:	Epoch: [138][6/233]	Loss 0.0007 (0.0007)	
training:	Epoch: [138][7/233]	Loss 0.0005 (0.0007)	
training:	Epoch: [138][8/233]	Loss 0.0006 (0.0007)	
training:	Epoch: [138][9/233]	Loss 0.0004 (0.0006)	
training:	Epoch: [138][10/233]	Loss 0.0004 (0.0006)	
training:	Epoch: [138][11/233]	Loss 0.0011 (0.0007)	
training:	Epoch: [138][12/233]	Loss 0.0006 (0.0006)	
training:	Epoch: [138][13/233]	Loss 0.0006 (0.0006)	
training:	Epoch: [138][14/233]	Loss 0.0004 (0.0006)	
training:	Epoch: [138][15/233]	Loss 0.0006 (0.0006)	
training:	Epoch: [138][16/233]	Loss 0.0009 (0.0006)	
training:	Epoch: [138][17/233]	Loss 0.0007 (0.0006)	
training:	Epoch: [138][18/233]	Loss 0.0004 (0.0006)	
training:	Epoch: [138][19/233]	Loss 0.0004 (0.0006)	
training:	Epoch: [138][20/233]	Loss 0.0004 (0.0006)	
training:	Epoch: [138][21/233]	Loss 0.0021 (0.0007)	
training:	Epoch: [138][22/233]	Loss 0.0003 (0.0007)	
training:	Epoch: [138][23/233]	Loss 0.0005 (0.0007)	
training:	Epoch: [138][24/233]	Loss 0.0081 (0.0010)	
training:	Epoch: [138][25/233]	Loss 0.0007 (0.0010)	
training:	Epoch: [138][26/233]	Loss 0.0005 (0.0009)	
training:	Epoch: [138][27/233]	Loss 0.0008 (0.0009)	
training:	Epoch: [138][28/233]	Loss 0.0004 (0.0009)	
training:	Epoch: [138][29/233]	Loss 0.0011 (0.0009)	
training:	Epoch: [138][30/233]	Loss 0.0003 (0.0009)	
training:	Epoch: [138][31/233]	Loss 0.0015 (0.0009)	
training:	Epoch: [138][32/233]	Loss 0.0004 (0.0009)	
training:	Epoch: [138][33/233]	Loss 0.0195 (0.0015)	
training:	Epoch: [138][34/233]	Loss 0.0004 (0.0014)	
training:	Epoch: [138][35/233]	Loss 0.0004 (0.0014)	
training:	Epoch: [138][36/233]	Loss 0.0004 (0.0014)	
training:	Epoch: [138][37/233]	Loss 0.0010 (0.0014)	
training:	Epoch: [138][38/233]	Loss 0.0004 (0.0013)	
training:	Epoch: [138][39/233]	Loss 0.0025 (0.0014)	
training:	Epoch: [138][40/233]	Loss 0.0004 (0.0013)	
training:	Epoch: [138][41/233]	Loss 0.0007 (0.0013)	
training:	Epoch: [138][42/233]	Loss 0.0004 (0.0013)	
training:	Epoch: [138][43/233]	Loss 0.0004 (0.0013)	
training:	Epoch: [138][44/233]	Loss 0.0020 (0.0013)	
training:	Epoch: [138][45/233]	Loss 0.0145 (0.0016)	
training:	Epoch: [138][46/233]	Loss 0.0003 (0.0016)	
training:	Epoch: [138][47/233]	Loss 0.0017 (0.0016)	
training:	Epoch: [138][48/233]	Loss 0.0004 (0.0015)	
training:	Epoch: [138][49/233]	Loss 0.0005 (0.0015)	
training:	Epoch: [138][50/233]	Loss 0.0007 (0.0015)	
training:	Epoch: [138][51/233]	Loss 0.0003 (0.0015)	
training:	Epoch: [138][52/233]	Loss 0.0004 (0.0015)	
training:	Epoch: [138][53/233]	Loss 0.0004 (0.0014)	
training:	Epoch: [138][54/233]	Loss 0.0004 (0.0014)	
training:	Epoch: [138][55/233]	Loss 0.0004 (0.0014)	
training:	Epoch: [138][56/233]	Loss 0.0008 (0.0014)	
training:	Epoch: [138][57/233]	Loss 0.0005 (0.0014)	
training:	Epoch: [138][58/233]	Loss 0.0004 (0.0014)	
training:	Epoch: [138][59/233]	Loss 0.0008 (0.0014)	
training:	Epoch: [138][60/233]	Loss 0.0004 (0.0013)	
training:	Epoch: [138][61/233]	Loss 0.0004 (0.0013)	
training:	Epoch: [138][62/233]	Loss 0.0008 (0.0013)	
training:	Epoch: [138][63/233]	Loss 0.0008 (0.0013)	
training:	Epoch: [138][64/233]	Loss 0.0004 (0.0013)	
training:	Epoch: [138][65/233]	Loss 0.0065 (0.0014)	
training:	Epoch: [138][66/233]	Loss 0.0005 (0.0014)	
training:	Epoch: [138][67/233]	Loss 0.0005 (0.0013)	
training:	Epoch: [138][68/233]	Loss 0.0004 (0.0013)	
training:	Epoch: [138][69/233]	Loss 0.0004 (0.0013)	
training:	Epoch: [138][70/233]	Loss 0.0003 (0.0013)	
training:	Epoch: [138][71/233]	Loss 0.0004 (0.0013)	
training:	Epoch: [138][72/233]	Loss 0.0004 (0.0013)	
training:	Epoch: [138][73/233]	Loss 0.0005 (0.0013)	
training:	Epoch: [138][74/233]	Loss 0.0008 (0.0013)	
training:	Epoch: [138][75/233]	Loss 0.0004 (0.0012)	
training:	Epoch: [138][76/233]	Loss 0.0009 (0.0012)	
training:	Epoch: [138][77/233]	Loss 0.0005 (0.0012)	
training:	Epoch: [138][78/233]	Loss 0.0005 (0.0012)	
training:	Epoch: [138][79/233]	Loss 0.0003 (0.0012)	
training:	Epoch: [138][80/233]	Loss 0.0006 (0.0012)	
training:	Epoch: [138][81/233]	Loss 0.0004 (0.0012)	
training:	Epoch: [138][82/233]	Loss 0.0005 (0.0012)	
training:	Epoch: [138][83/233]	Loss 0.0005 (0.0012)	
training:	Epoch: [138][84/233]	Loss 0.0004 (0.0012)	
training:	Epoch: [138][85/233]	Loss 0.0004 (0.0012)	
training:	Epoch: [138][86/233]	Loss 0.0003 (0.0012)	
training:	Epoch: [138][87/233]	Loss 0.0004 (0.0011)	
training:	Epoch: [138][88/233]	Loss 0.0004 (0.0011)	
training:	Epoch: [138][89/233]	Loss 0.0021 (0.0011)	
training:	Epoch: [138][90/233]	Loss 0.0004 (0.0011)	
training:	Epoch: [138][91/233]	Loss 0.0004 (0.0011)	
training:	Epoch: [138][92/233]	Loss 0.0005 (0.0011)	
training:	Epoch: [138][93/233]	Loss 0.0004 (0.0011)	
training:	Epoch: [138][94/233]	Loss 0.0004 (0.0011)	
training:	Epoch: [138][95/233]	Loss 0.0003 (0.0011)	
training:	Epoch: [138][96/233]	Loss 0.0008 (0.0011)	
training:	Epoch: [138][97/233]	Loss 0.0004 (0.0011)	
training:	Epoch: [138][98/233]	Loss 0.0005 (0.0011)	
training:	Epoch: [138][99/233]	Loss 0.0006 (0.0011)	
training:	Epoch: [138][100/233]	Loss 0.0004 (0.0011)	
training:	Epoch: [138][101/233]	Loss 0.0003 (0.0011)	
training:	Epoch: [138][102/233]	Loss 0.0004 (0.0011)	
training:	Epoch: [138][103/233]	Loss 0.0004 (0.0010)	
training:	Epoch: [138][104/233]	Loss 0.0258 (0.0013)	
training:	Epoch: [138][105/233]	Loss 0.0003 (0.0013)	
training:	Epoch: [138][106/233]	Loss 0.0021 (0.0013)	
training:	Epoch: [138][107/233]	Loss 0.0004 (0.0013)	
training:	Epoch: [138][108/233]	Loss 0.0005 (0.0013)	
training:	Epoch: [138][109/233]	Loss 0.0004 (0.0013)	
training:	Epoch: [138][110/233]	Loss 0.0004 (0.0013)	
training:	Epoch: [138][111/233]	Loss 0.0003 (0.0012)	
training:	Epoch: [138][112/233]	Loss 0.0005 (0.0012)	
training:	Epoch: [138][113/233]	Loss 0.0004 (0.0012)	
training:	Epoch: [138][114/233]	Loss 0.0004 (0.0012)	
training:	Epoch: [138][115/233]	Loss 0.0003 (0.0012)	
training:	Epoch: [138][116/233]	Loss 0.0004 (0.0012)	
training:	Epoch: [138][117/233]	Loss 0.0006 (0.0012)	
training:	Epoch: [138][118/233]	Loss 0.0005 (0.0012)	
training:	Epoch: [138][119/233]	Loss 0.0003 (0.0012)	
training:	Epoch: [138][120/233]	Loss 0.0004 (0.0012)	
training:	Epoch: [138][121/233]	Loss 0.0007 (0.0012)	
training:	Epoch: [138][122/233]	Loss 0.0003 (0.0012)	
training:	Epoch: [138][123/233]	Loss 0.0323 (0.0014)	
training:	Epoch: [138][124/233]	Loss 0.0004 (0.0014)	
training:	Epoch: [138][125/233]	Loss 0.0016 (0.0014)	
training:	Epoch: [138][126/233]	Loss 0.0363 (0.0017)	
training:	Epoch: [138][127/233]	Loss 0.0030 (0.0017)	
training:	Epoch: [138][128/233]	Loss 0.0015 (0.0017)	
training:	Epoch: [138][129/233]	Loss 0.0006 (0.0017)	
training:	Epoch: [138][130/233]	Loss 0.0004 (0.0017)	
training:	Epoch: [138][131/233]	Loss 0.0005 (0.0017)	
training:	Epoch: [138][132/233]	Loss 0.0004 (0.0017)	
training:	Epoch: [138][133/233]	Loss 0.0003 (0.0017)	
training:	Epoch: [138][134/233]	Loss 0.0003 (0.0016)	
training:	Epoch: [138][135/233]	Loss 0.0150 (0.0017)	
training:	Epoch: [138][136/233]	Loss 0.0085 (0.0018)	
training:	Epoch: [138][137/233]	Loss 0.0004 (0.0018)	
training:	Epoch: [138][138/233]	Loss 0.0004 (0.0018)	
training:	Epoch: [138][139/233]	Loss 0.0003 (0.0018)	
training:	Epoch: [138][140/233]	Loss 0.0003 (0.0018)	
training:	Epoch: [138][141/233]	Loss 0.0004 (0.0017)	
training:	Epoch: [138][142/233]	Loss 0.0004 (0.0017)	
training:	Epoch: [138][143/233]	Loss 0.0003 (0.0017)	
training:	Epoch: [138][144/233]	Loss 0.0006 (0.0017)	
training:	Epoch: [138][145/233]	Loss 0.0004 (0.0017)	
training:	Epoch: [138][146/233]	Loss 0.0005 (0.0017)	
training:	Epoch: [138][147/233]	Loss 0.0010 (0.0017)	
training:	Epoch: [138][148/233]	Loss 0.0007 (0.0017)	
training:	Epoch: [138][149/233]	Loss 0.0004 (0.0017)	
training:	Epoch: [138][150/233]	Loss 0.0004 (0.0017)	
training:	Epoch: [138][151/233]	Loss 0.0004 (0.0017)	
training:	Epoch: [138][152/233]	Loss 0.0004 (0.0017)	
training:	Epoch: [138][153/233]	Loss 0.0003 (0.0016)	
training:	Epoch: [138][154/233]	Loss 0.0030 (0.0017)	
training:	Epoch: [138][155/233]	Loss 0.0006 (0.0017)	
training:	Epoch: [138][156/233]	Loss 0.0003 (0.0016)	
training:	Epoch: [138][157/233]	Loss 0.0004 (0.0016)	
training:	Epoch: [138][158/233]	Loss 0.0003 (0.0016)	
training:	Epoch: [138][159/233]	Loss 0.0032 (0.0016)	
training:	Epoch: [138][160/233]	Loss 0.0012 (0.0016)	
training:	Epoch: [138][161/233]	Loss 0.0006 (0.0016)	
training:	Epoch: [138][162/233]	Loss 0.0005 (0.0016)	
training:	Epoch: [138][163/233]	Loss 0.0004 (0.0016)	
training:	Epoch: [138][164/233]	Loss 0.0008 (0.0016)	
training:	Epoch: [138][165/233]	Loss 0.0003 (0.0016)	
training:	Epoch: [138][166/233]	Loss 0.0005 (0.0016)	
training:	Epoch: [138][167/233]	Loss 0.0004 (0.0016)	
training:	Epoch: [138][168/233]	Loss 0.0003 (0.0016)	
training:	Epoch: [138][169/233]	Loss 0.0004 (0.0016)	
training:	Epoch: [138][170/233]	Loss 0.0005 (0.0016)	
training:	Epoch: [138][171/233]	Loss 0.0004 (0.0016)	
training:	Epoch: [138][172/233]	Loss 0.0003 (0.0016)	
training:	Epoch: [138][173/233]	Loss 0.0004 (0.0015)	
training:	Epoch: [138][174/233]	Loss 0.0005 (0.0015)	
training:	Epoch: [138][175/233]	Loss 0.0004 (0.0015)	
training:	Epoch: [138][176/233]	Loss 0.0005 (0.0015)	
training:	Epoch: [138][177/233]	Loss 0.0003 (0.0015)	
training:	Epoch: [138][178/233]	Loss 0.0007 (0.0015)	
training:	Epoch: [138][179/233]	Loss 0.0007 (0.0015)	
training:	Epoch: [138][180/233]	Loss 0.0004 (0.0015)	
training:	Epoch: [138][181/233]	Loss 0.0004 (0.0015)	
training:	Epoch: [138][182/233]	Loss 0.0014 (0.0015)	
training:	Epoch: [138][183/233]	Loss 0.0007 (0.0015)	
training:	Epoch: [138][184/233]	Loss 0.0015 (0.0015)	
training:	Epoch: [138][185/233]	Loss 0.0004 (0.0015)	
training:	Epoch: [138][186/233]	Loss 0.0016 (0.0015)	
training:	Epoch: [138][187/233]	Loss 0.0005 (0.0015)	
training:	Epoch: [138][188/233]	Loss 0.0004 (0.0015)	
training:	Epoch: [138][189/233]	Loss 0.0004 (0.0015)	
training:	Epoch: [138][190/233]	Loss 0.0013 (0.0015)	
training:	Epoch: [138][191/233]	Loss 0.0004 (0.0015)	
training:	Epoch: [138][192/233]	Loss 0.0004 (0.0015)	
training:	Epoch: [138][193/233]	Loss 0.0011 (0.0015)	
training:	Epoch: [138][194/233]	Loss 0.0003 (0.0014)	
training:	Epoch: [138][195/233]	Loss 0.0003 (0.0014)	
training:	Epoch: [138][196/233]	Loss 0.0007 (0.0014)	
training:	Epoch: [138][197/233]	Loss 0.0005 (0.0014)	
training:	Epoch: [138][198/233]	Loss 0.0005 (0.0014)	
training:	Epoch: [138][199/233]	Loss 0.0007 (0.0014)	
training:	Epoch: [138][200/233]	Loss 0.0335 (0.0016)	
training:	Epoch: [138][201/233]	Loss 0.0003 (0.0016)	
training:	Epoch: [138][202/233]	Loss 0.0004 (0.0016)	
training:	Epoch: [138][203/233]	Loss 0.0004 (0.0016)	
training:	Epoch: [138][204/233]	Loss 0.0004 (0.0016)	
training:	Epoch: [138][205/233]	Loss 0.0012 (0.0016)	
training:	Epoch: [138][206/233]	Loss 0.0003 (0.0016)	
training:	Epoch: [138][207/233]	Loss 0.0060 (0.0016)	
training:	Epoch: [138][208/233]	Loss 0.0003 (0.0016)	
training:	Epoch: [138][209/233]	Loss 0.0004 (0.0016)	
training:	Epoch: [138][210/233]	Loss 0.0005 (0.0016)	
training:	Epoch: [138][211/233]	Loss 0.0004 (0.0016)	
training:	Epoch: [138][212/233]	Loss 0.0003 (0.0015)	
training:	Epoch: [138][213/233]	Loss 0.0016 (0.0016)	
training:	Epoch: [138][214/233]	Loss 0.0009 (0.0015)	
training:	Epoch: [138][215/233]	Loss 0.0005 (0.0015)	
training:	Epoch: [138][216/233]	Loss 0.0006 (0.0015)	
training:	Epoch: [138][217/233]	Loss 0.0026 (0.0015)	
training:	Epoch: [138][218/233]	Loss 0.0005 (0.0015)	
training:	Epoch: [138][219/233]	Loss 0.0004 (0.0015)	
training:	Epoch: [138][220/233]	Loss 0.0004 (0.0015)	
training:	Epoch: [138][221/233]	Loss 0.0006 (0.0015)	
training:	Epoch: [138][222/233]	Loss 0.0007 (0.0015)	
training:	Epoch: [138][223/233]	Loss 0.0004 (0.0015)	
training:	Epoch: [138][224/233]	Loss 0.0004 (0.0015)	
training:	Epoch: [138][225/233]	Loss 0.0003 (0.0015)	
training:	Epoch: [138][226/233]	Loss 0.0023 (0.0015)	
training:	Epoch: [138][227/233]	Loss 0.0005 (0.0015)	
training:	Epoch: [138][228/233]	Loss 0.0004 (0.0015)	
training:	Epoch: [138][229/233]	Loss 0.0018 (0.0015)	
training:	Epoch: [138][230/233]	Loss 0.0004 (0.0015)	
training:	Epoch: [138][231/233]	Loss 0.0004 (0.0015)	
training:	Epoch: [138][232/233]	Loss 0.0003 (0.0015)	
training:	Epoch: [138][233/233]	Loss 0.0004 (0.0015)	
Training:	 Loss: 0.0015

Training:	 ACC: 1.0000 1.0000 1.0000 1.0000
Validation:	 ACC: 0.7967 0.7951 0.7620 0.8315
Validation:	 Best_BACC: 0.8162 0.8154 0.7998 0.8326
Validation:	 Loss: 1.2696
Pretraining:	Epoch 139/200
----------
training:	Epoch: [139][1/233]	Loss 0.0008 (0.0008)	
training:	Epoch: [139][2/233]	Loss 0.0018 (0.0013)	
training:	Epoch: [139][3/233]	Loss 0.0004 (0.0010)	
training:	Epoch: [139][4/233]	Loss 0.0003 (0.0008)	
training:	Epoch: [139][5/233]	Loss 0.0003 (0.0007)	
training:	Epoch: [139][6/233]	Loss 0.0004 (0.0007)	
training:	Epoch: [139][7/233]	Loss 0.0004 (0.0006)	
training:	Epoch: [139][8/233]	Loss 0.0006 (0.0006)	
training:	Epoch: [139][9/233]	Loss 0.0005 (0.0006)	
training:	Epoch: [139][10/233]	Loss 0.0003 (0.0006)	
training:	Epoch: [139][11/233]	Loss 0.0005 (0.0006)	
training:	Epoch: [139][12/233]	Loss 0.0004 (0.0006)	
training:	Epoch: [139][13/233]	Loss 0.0004 (0.0005)	
training:	Epoch: [139][14/233]	Loss 0.0005 (0.0005)	
training:	Epoch: [139][15/233]	Loss 0.0003 (0.0005)	
training:	Epoch: [139][16/233]	Loss 0.0004 (0.0005)	
training:	Epoch: [139][17/233]	Loss 0.0004 (0.0005)	
training:	Epoch: [139][18/233]	Loss 0.0004 (0.0005)	
training:	Epoch: [139][19/233]	Loss 0.0004 (0.0005)	
training:	Epoch: [139][20/233]	Loss 0.0022 (0.0006)	
training:	Epoch: [139][21/233]	Loss 0.0003 (0.0006)	
training:	Epoch: [139][22/233]	Loss 0.0003 (0.0006)	
training:	Epoch: [139][23/233]	Loss 0.0071 (0.0008)	
training:	Epoch: [139][24/233]	Loss 0.0280 (0.0020)	
training:	Epoch: [139][25/233]	Loss 0.0003 (0.0019)	
training:	Epoch: [139][26/233]	Loss 0.0004 (0.0019)	
training:	Epoch: [139][27/233]	Loss 0.0004 (0.0018)	
training:	Epoch: [139][28/233]	Loss 0.0004 (0.0017)	
training:	Epoch: [139][29/233]	Loss 0.0005 (0.0017)	
training:	Epoch: [139][30/233]	Loss 0.0005 (0.0017)	
training:	Epoch: [139][31/233]	Loss 0.0004 (0.0016)	
training:	Epoch: [139][32/233]	Loss 0.0004 (0.0016)	
training:	Epoch: [139][33/233]	Loss 0.0003 (0.0015)	
training:	Epoch: [139][34/233]	Loss 0.0003 (0.0015)	
training:	Epoch: [139][35/233]	Loss 0.0005 (0.0015)	
training:	Epoch: [139][36/233]	Loss 0.0004 (0.0015)	
training:	Epoch: [139][37/233]	Loss 0.0013 (0.0015)	
training:	Epoch: [139][38/233]	Loss 0.0005 (0.0014)	
training:	Epoch: [139][39/233]	Loss 0.0003 (0.0014)	
training:	Epoch: [139][40/233]	Loss 0.0003 (0.0014)	
training:	Epoch: [139][41/233]	Loss 0.0105 (0.0016)	
training:	Epoch: [139][42/233]	Loss 0.0006 (0.0016)	
training:	Epoch: [139][43/233]	Loss 0.0003 (0.0015)	
training:	Epoch: [139][44/233]	Loss 0.0004 (0.0015)	
training:	Epoch: [139][45/233]	Loss 0.0003 (0.0015)	
training:	Epoch: [139][46/233]	Loss 0.0003 (0.0015)	
training:	Epoch: [139][47/233]	Loss 0.0005 (0.0014)	
training:	Epoch: [139][48/233]	Loss 0.0008 (0.0014)	
training:	Epoch: [139][49/233]	Loss 0.0010 (0.0014)	
training:	Epoch: [139][50/233]	Loss 0.0004 (0.0014)	
training:	Epoch: [139][51/233]	Loss 0.0006 (0.0014)	
training:	Epoch: [139][52/233]	Loss 0.0004 (0.0014)	
training:	Epoch: [139][53/233]	Loss 0.0004 (0.0013)	
training:	Epoch: [139][54/233]	Loss 0.0003 (0.0013)	
training:	Epoch: [139][55/233]	Loss 0.0005 (0.0013)	
training:	Epoch: [139][56/233]	Loss 0.0005 (0.0013)	
training:	Epoch: [139][57/233]	Loss 0.0005 (0.0013)	
training:	Epoch: [139][58/233]	Loss 0.0004 (0.0013)	
training:	Epoch: [139][59/233]	Loss 0.0004 (0.0012)	
training:	Epoch: [139][60/233]	Loss 0.0003 (0.0012)	
training:	Epoch: [139][61/233]	Loss 0.0004 (0.0012)	
training:	Epoch: [139][62/233]	Loss 0.0004 (0.0012)	
training:	Epoch: [139][63/233]	Loss 0.0004 (0.0012)	
training:	Epoch: [139][64/233]	Loss 0.0004 (0.0012)	
training:	Epoch: [139][65/233]	Loss 0.0008 (0.0012)	
training:	Epoch: [139][66/233]	Loss 0.0004 (0.0012)	
training:	Epoch: [139][67/233]	Loss 0.0013 (0.0012)	
training:	Epoch: [139][68/233]	Loss 0.0003 (0.0012)	
training:	Epoch: [139][69/233]	Loss 0.0005 (0.0011)	
training:	Epoch: [139][70/233]	Loss 0.0004 (0.0011)	
training:	Epoch: [139][71/233]	Loss 0.0003 (0.0011)	
training:	Epoch: [139][72/233]	Loss 0.0004 (0.0011)	
training:	Epoch: [139][73/233]	Loss 0.0005 (0.0011)	
training:	Epoch: [139][74/233]	Loss 0.0003 (0.0011)	
training:	Epoch: [139][75/233]	Loss 0.0008 (0.0011)	
training:	Epoch: [139][76/233]	Loss 0.0004 (0.0011)	
training:	Epoch: [139][77/233]	Loss 0.0004 (0.0011)	
training:	Epoch: [139][78/233]	Loss 0.0004 (0.0011)	
training:	Epoch: [139][79/233]	Loss 0.0007 (0.0011)	
training:	Epoch: [139][80/233]	Loss 0.0566 (0.0017)	
training:	Epoch: [139][81/233]	Loss 0.0003 (0.0017)	
training:	Epoch: [139][82/233]	Loss 0.0329 (0.0021)	
training:	Epoch: [139][83/233]	Loss 0.0005 (0.0021)	
training:	Epoch: [139][84/233]	Loss 0.0011 (0.0021)	
training:	Epoch: [139][85/233]	Loss 0.0005 (0.0021)	
training:	Epoch: [139][86/233]	Loss 0.0003 (0.0020)	
training:	Epoch: [139][87/233]	Loss 0.0008 (0.0020)	
training:	Epoch: [139][88/233]	Loss 0.0005 (0.0020)	
training:	Epoch: [139][89/233]	Loss 0.0003 (0.0020)	
training:	Epoch: [139][90/233]	Loss 0.0005 (0.0020)	
training:	Epoch: [139][91/233]	Loss 0.0020 (0.0020)	
training:	Epoch: [139][92/233]	Loss 0.0003 (0.0020)	
training:	Epoch: [139][93/233]	Loss 0.0005 (0.0019)	
training:	Epoch: [139][94/233]	Loss 0.0009 (0.0019)	
training:	Epoch: [139][95/233]	Loss 0.0003 (0.0019)	
training:	Epoch: [139][96/233]	Loss 0.0004 (0.0019)	
training:	Epoch: [139][97/233]	Loss 0.0003 (0.0019)	
training:	Epoch: [139][98/233]	Loss 0.0005 (0.0019)	
training:	Epoch: [139][99/233]	Loss 0.0038 (0.0019)	
training:	Epoch: [139][100/233]	Loss 0.0003 (0.0019)	
training:	Epoch: [139][101/233]	Loss 0.0004 (0.0019)	
training:	Epoch: [139][102/233]	Loss 0.0004 (0.0018)	
training:	Epoch: [139][103/233]	Loss 0.0113 (0.0019)	
training:	Epoch: [139][104/233]	Loss 0.0005 (0.0019)	
training:	Epoch: [139][105/233]	Loss 0.0004 (0.0019)	
training:	Epoch: [139][106/233]	Loss 0.0006 (0.0019)	
training:	Epoch: [139][107/233]	Loss 0.0019 (0.0019)	
training:	Epoch: [139][108/233]	Loss 0.0003 (0.0019)	
training:	Epoch: [139][109/233]	Loss 0.0004 (0.0019)	
training:	Epoch: [139][110/233]	Loss 0.0057 (0.0019)	
training:	Epoch: [139][111/233]	Loss 0.0007 (0.0019)	
training:	Epoch: [139][112/233]	Loss 0.0003 (0.0019)	
training:	Epoch: [139][113/233]	Loss 0.0019 (0.0019)	
training:	Epoch: [139][114/233]	Loss 0.0003 (0.0019)	
training:	Epoch: [139][115/233]	Loss 0.0003 (0.0018)	
training:	Epoch: [139][116/233]	Loss 0.0003 (0.0018)	
training:	Epoch: [139][117/233]	Loss 0.0004 (0.0018)	
training:	Epoch: [139][118/233]	Loss 0.0003 (0.0018)	
training:	Epoch: [139][119/233]	Loss 0.0008 (0.0018)	
training:	Epoch: [139][120/233]	Loss 0.0032 (0.0018)	
training:	Epoch: [139][121/233]	Loss 0.0003 (0.0018)	
training:	Epoch: [139][122/233]	Loss 0.0004 (0.0018)	
training:	Epoch: [139][123/233]	Loss 0.0004 (0.0018)	
training:	Epoch: [139][124/233]	Loss 0.0003 (0.0018)	
training:	Epoch: [139][125/233]	Loss 0.0004 (0.0018)	
training:	Epoch: [139][126/233]	Loss 0.0008 (0.0017)	
training:	Epoch: [139][127/233]	Loss 0.0005 (0.0017)	
training:	Epoch: [139][128/233]	Loss 0.0003 (0.0017)	
training:	Epoch: [139][129/233]	Loss 0.0004 (0.0017)	
training:	Epoch: [139][130/233]	Loss 0.0003 (0.0017)	
training:	Epoch: [139][131/233]	Loss 0.0019 (0.0017)	
training:	Epoch: [139][132/233]	Loss 0.0033 (0.0017)	
training:	Epoch: [139][133/233]	Loss 0.0003 (0.0017)	
training:	Epoch: [139][134/233]	Loss 0.0009 (0.0017)	
training:	Epoch: [139][135/233]	Loss 0.0004 (0.0017)	
training:	Epoch: [139][136/233]	Loss 0.0389 (0.0020)	
training:	Epoch: [139][137/233]	Loss 0.0005 (0.0020)	
training:	Epoch: [139][138/233]	Loss 0.0003 (0.0019)	
training:	Epoch: [139][139/233]	Loss 0.0004 (0.0019)	
training:	Epoch: [139][140/233]	Loss 0.0045 (0.0020)	
training:	Epoch: [139][141/233]	Loss 0.0005 (0.0019)	
training:	Epoch: [139][142/233]	Loss 0.0005 (0.0019)	
training:	Epoch: [139][143/233]	Loss 0.0003 (0.0019)	
training:	Epoch: [139][144/233]	Loss 0.0005 (0.0019)	
training:	Epoch: [139][145/233]	Loss 0.0006 (0.0019)	
training:	Epoch: [139][146/233]	Loss 0.0005 (0.0019)	
training:	Epoch: [139][147/233]	Loss 0.0003 (0.0019)	
training:	Epoch: [139][148/233]	Loss 0.0003 (0.0019)	
training:	Epoch: [139][149/233]	Loss 0.0003 (0.0019)	
training:	Epoch: [139][150/233]	Loss 0.0004 (0.0019)	
training:	Epoch: [139][151/233]	Loss 0.0003 (0.0018)	
training:	Epoch: [139][152/233]	Loss 0.0005 (0.0018)	
training:	Epoch: [139][153/233]	Loss 0.0004 (0.0018)	
training:	Epoch: [139][154/233]	Loss 0.0011 (0.0018)	
training:	Epoch: [139][155/233]	Loss 0.0005 (0.0018)	
training:	Epoch: [139][156/233]	Loss 0.0004 (0.0018)	
training:	Epoch: [139][157/233]	Loss 0.0007 (0.0018)	
training:	Epoch: [139][158/233]	Loss 0.0006 (0.0018)	
training:	Epoch: [139][159/233]	Loss 0.0004 (0.0018)	
training:	Epoch: [139][160/233]	Loss 0.0005 (0.0018)	
training:	Epoch: [139][161/233]	Loss 0.0004 (0.0018)	
training:	Epoch: [139][162/233]	Loss 0.0004 (0.0018)	
training:	Epoch: [139][163/233]	Loss 0.0005 (0.0017)	
training:	Epoch: [139][164/233]	Loss 0.0003 (0.0017)	
training:	Epoch: [139][165/233]	Loss 0.0005 (0.0017)	
training:	Epoch: [139][166/233]	Loss 0.0004 (0.0017)	
training:	Epoch: [139][167/233]	Loss 0.0003 (0.0017)	
training:	Epoch: [139][168/233]	Loss 0.0004 (0.0017)	
training:	Epoch: [139][169/233]	Loss 0.1727 (0.0027)	
training:	Epoch: [139][170/233]	Loss 0.0004 (0.0027)	
training:	Epoch: [139][171/233]	Loss 0.0003 (0.0027)	
training:	Epoch: [139][172/233]	Loss 0.0005 (0.0027)	
training:	Epoch: [139][173/233]	Loss 0.0004 (0.0027)	
training:	Epoch: [139][174/233]	Loss 0.0006 (0.0026)	
training:	Epoch: [139][175/233]	Loss 0.0005 (0.0026)	
training:	Epoch: [139][176/233]	Loss 0.0004 (0.0026)	
training:	Epoch: [139][177/233]	Loss 0.0004 (0.0026)	
training:	Epoch: [139][178/233]	Loss 0.0003 (0.0026)	
training:	Epoch: [139][179/233]	Loss 0.0017 (0.0026)	
training:	Epoch: [139][180/233]	Loss 0.0009 (0.0026)	
training:	Epoch: [139][181/233]	Loss 0.0008 (0.0026)	
training:	Epoch: [139][182/233]	Loss 0.0003 (0.0026)	
training:	Epoch: [139][183/233]	Loss 0.0006 (0.0026)	
training:	Epoch: [139][184/233]	Loss 0.0005 (0.0025)	
training:	Epoch: [139][185/233]	Loss 0.0137 (0.0026)	
training:	Epoch: [139][186/233]	Loss 0.0004 (0.0026)	
training:	Epoch: [139][187/233]	Loss 0.0086 (0.0026)	
training:	Epoch: [139][188/233]	Loss 0.0005 (0.0026)	
training:	Epoch: [139][189/233]	Loss 0.0003 (0.0026)	
training:	Epoch: [139][190/233]	Loss 0.0004 (0.0026)	
training:	Epoch: [139][191/233]	Loss 0.0007 (0.0026)	
training:	Epoch: [139][192/233]	Loss 0.0005 (0.0026)	
training:	Epoch: [139][193/233]	Loss 0.0006 (0.0026)	
training:	Epoch: [139][194/233]	Loss 0.0015 (0.0025)	
training:	Epoch: [139][195/233]	Loss 0.0003 (0.0025)	
training:	Epoch: [139][196/233]	Loss 0.0007 (0.0025)	
training:	Epoch: [139][197/233]	Loss 0.0005 (0.0025)	
training:	Epoch: [139][198/233]	Loss 0.0004 (0.0025)	
training:	Epoch: [139][199/233]	Loss 0.0004 (0.0025)	
training:	Epoch: [139][200/233]	Loss 0.0004 (0.0025)	
training:	Epoch: [139][201/233]	Loss 0.0003 (0.0025)	
training:	Epoch: [139][202/233]	Loss 0.0003 (0.0025)	
training:	Epoch: [139][203/233]	Loss 0.0006 (0.0025)	
training:	Epoch: [139][204/233]	Loss 0.0004 (0.0024)	
training:	Epoch: [139][205/233]	Loss 0.0008 (0.0024)	
training:	Epoch: [139][206/233]	Loss 0.0003 (0.0024)	
training:	Epoch: [139][207/233]	Loss 0.0003 (0.0024)	
training:	Epoch: [139][208/233]	Loss 0.0636 (0.0027)	
training:	Epoch: [139][209/233]	Loss 0.0004 (0.0027)	
training:	Epoch: [139][210/233]	Loss 0.0145 (0.0028)	
training:	Epoch: [139][211/233]	Loss 0.0004 (0.0027)	
training:	Epoch: [139][212/233]	Loss 0.0003 (0.0027)	
training:	Epoch: [139][213/233]	Loss 0.0006 (0.0027)	
training:	Epoch: [139][214/233]	Loss 0.0004 (0.0027)	
training:	Epoch: [139][215/233]	Loss 0.0004 (0.0027)	
training:	Epoch: [139][216/233]	Loss 0.0337 (0.0028)	
training:	Epoch: [139][217/233]	Loss 0.0006 (0.0028)	
training:	Epoch: [139][218/233]	Loss 0.0012 (0.0028)	
training:	Epoch: [139][219/233]	Loss 0.0006 (0.0028)	
training:	Epoch: [139][220/233]	Loss 0.0004 (0.0028)	
training:	Epoch: [139][221/233]	Loss 0.0004 (0.0028)	
training:	Epoch: [139][222/233]	Loss 0.0003 (0.0028)	
training:	Epoch: [139][223/233]	Loss 0.0006 (0.0028)	
training:	Epoch: [139][224/233]	Loss 0.0008 (0.0028)	
training:	Epoch: [139][225/233]	Loss 0.0616 (0.0030)	
training:	Epoch: [139][226/233]	Loss 0.0003 (0.0030)	
training:	Epoch: [139][227/233]	Loss 0.0003 (0.0030)	
training:	Epoch: [139][228/233]	Loss 0.0003 (0.0030)	
training:	Epoch: [139][229/233]	Loss 0.0003 (0.0030)	
training:	Epoch: [139][230/233]	Loss 0.0003 (0.0030)	
training:	Epoch: [139][231/233]	Loss 0.0003 (0.0030)	
training:	Epoch: [139][232/233]	Loss 0.0004 (0.0029)	
training:	Epoch: [139][233/233]	Loss 0.0003 (0.0029)	
Training:	 Loss: 0.0029

Training:	 ACC: 1.0000 1.0000 1.0000 1.0000
Validation:	 ACC: 0.7911 0.7897 0.7620 0.8202
Validation:	 Best_BACC: 0.8162 0.8154 0.7998 0.8326
Validation:	 Loss: 1.3083
Pretraining:	Epoch 140/200
----------
training:	Epoch: [140][1/233]	Loss 0.0004 (0.0004)	
training:	Epoch: [140][2/233]	Loss 0.0015 (0.0009)	
training:	Epoch: [140][3/233]	Loss 0.0003 (0.0007)	
training:	Epoch: [140][4/233]	Loss 0.0003 (0.0006)	
training:	Epoch: [140][5/233]	Loss 0.0609 (0.0127)	
training:	Epoch: [140][6/233]	Loss 0.0003 (0.0106)	
training:	Epoch: [140][7/233]	Loss 0.0008 (0.0092)	
training:	Epoch: [140][8/233]	Loss 0.0003 (0.0081)	
training:	Epoch: [140][9/233]	Loss 0.0056 (0.0078)	
training:	Epoch: [140][10/233]	Loss 0.0004 (0.0071)	
training:	Epoch: [140][11/233]	Loss 0.0004 (0.0065)	
training:	Epoch: [140][12/233]	Loss 0.0033 (0.0062)	
training:	Epoch: [140][13/233]	Loss 0.1947 (0.0207)	
training:	Epoch: [140][14/233]	Loss 0.0012 (0.0193)	
training:	Epoch: [140][15/233]	Loss 0.0004 (0.0181)	
training:	Epoch: [140][16/233]	Loss 0.0005 (0.0170)	
training:	Epoch: [140][17/233]	Loss 0.0006 (0.0160)	
training:	Epoch: [140][18/233]	Loss 0.0059 (0.0154)	
training:	Epoch: [140][19/233]	Loss 0.0005 (0.0147)	
training:	Epoch: [140][20/233]	Loss 0.0011 (0.0140)	
training:	Epoch: [140][21/233]	Loss 0.0003 (0.0133)	
training:	Epoch: [140][22/233]	Loss 0.0003 (0.0127)	
training:	Epoch: [140][23/233]	Loss 0.0004 (0.0122)	
training:	Epoch: [140][24/233]	Loss 0.0003 (0.0117)	
training:	Epoch: [140][25/233]	Loss 0.0003 (0.0112)	
training:	Epoch: [140][26/233]	Loss 0.0004 (0.0108)	
training:	Epoch: [140][27/233]	Loss 0.0006 (0.0105)	
training:	Epoch: [140][28/233]	Loss 0.0004 (0.0101)	
training:	Epoch: [140][29/233]	Loss 0.0003 (0.0098)	
training:	Epoch: [140][30/233]	Loss 0.0004 (0.0094)	
training:	Epoch: [140][31/233]	Loss 0.0003 (0.0092)	
training:	Epoch: [140][32/233]	Loss 0.0003 (0.0089)	
training:	Epoch: [140][33/233]	Loss 0.0004 (0.0086)	
training:	Epoch: [140][34/233]	Loss 0.1219 (0.0119)	
training:	Epoch: [140][35/233]	Loss 0.0007 (0.0116)	
training:	Epoch: [140][36/233]	Loss 0.0004 (0.0113)	
training:	Epoch: [140][37/233]	Loss 0.0138 (0.0114)	
training:	Epoch: [140][38/233]	Loss 0.0012 (0.0111)	
training:	Epoch: [140][39/233]	Loss 0.0005 (0.0108)	
training:	Epoch: [140][40/233]	Loss 0.0003 (0.0106)	
training:	Epoch: [140][41/233]	Loss 0.0003 (0.0103)	
training:	Epoch: [140][42/233]	Loss 0.0004 (0.0101)	
training:	Epoch: [140][43/233]	Loss 0.0004 (0.0099)	
training:	Epoch: [140][44/233]	Loss 0.0003 (0.0096)	
training:	Epoch: [140][45/233]	Loss 0.0003 (0.0094)	
training:	Epoch: [140][46/233]	Loss 0.0003 (0.0092)	
training:	Epoch: [140][47/233]	Loss 0.0007 (0.0091)	
training:	Epoch: [140][48/233]	Loss 0.0003 (0.0089)	
training:	Epoch: [140][49/233]	Loss 0.0003 (0.0087)	
training:	Epoch: [140][50/233]	Loss 0.0016 (0.0086)	
training:	Epoch: [140][51/233]	Loss 0.0012 (0.0084)	
training:	Epoch: [140][52/233]	Loss 0.0003 (0.0083)	
training:	Epoch: [140][53/233]	Loss 0.0005 (0.0081)	
training:	Epoch: [140][54/233]	Loss 0.0004 (0.0080)	
training:	Epoch: [140][55/233]	Loss 0.0003 (0.0078)	
training:	Epoch: [140][56/233]	Loss 0.0033 (0.0078)	
training:	Epoch: [140][57/233]	Loss 0.0007 (0.0076)	
training:	Epoch: [140][58/233]	Loss 0.0004 (0.0075)	
training:	Epoch: [140][59/233]	Loss 0.0007 (0.0074)	
training:	Epoch: [140][60/233]	Loss 0.0008 (0.0073)	
training:	Epoch: [140][61/233]	Loss 0.0011 (0.0072)	
training:	Epoch: [140][62/233]	Loss 0.0004 (0.0071)	
training:	Epoch: [140][63/233]	Loss 0.0005 (0.0070)	
training:	Epoch: [140][64/233]	Loss 0.0003 (0.0069)	
training:	Epoch: [140][65/233]	Loss 0.0045 (0.0068)	
training:	Epoch: [140][66/233]	Loss 0.0005 (0.0067)	
training:	Epoch: [140][67/233]	Loss 0.0022 (0.0067)	
training:	Epoch: [140][68/233]	Loss 0.0004 (0.0066)	
training:	Epoch: [140][69/233]	Loss 0.0004 (0.0065)	
training:	Epoch: [140][70/233]	Loss 0.0003 (0.0064)	
training:	Epoch: [140][71/233]	Loss 0.0023 (0.0063)	
training:	Epoch: [140][72/233]	Loss 0.0003 (0.0062)	
training:	Epoch: [140][73/233]	Loss 0.0005 (0.0062)	
training:	Epoch: [140][74/233]	Loss 0.0005 (0.0061)	
training:	Epoch: [140][75/233]	Loss 0.0004 (0.0060)	
training:	Epoch: [140][76/233]	Loss 0.0005 (0.0059)	
training:	Epoch: [140][77/233]	Loss 0.0016 (0.0059)	
training:	Epoch: [140][78/233]	Loss 0.0010 (0.0058)	
training:	Epoch: [140][79/233]	Loss 0.0004 (0.0058)	
training:	Epoch: [140][80/233]	Loss 0.0004 (0.0057)	
training:	Epoch: [140][81/233]	Loss 0.0003 (0.0056)	
training:	Epoch: [140][82/233]	Loss 0.0004 (0.0056)	
training:	Epoch: [140][83/233]	Loss 0.0005 (0.0055)	
training:	Epoch: [140][84/233]	Loss 0.0560 (0.0061)	
training:	Epoch: [140][85/233]	Loss 0.0003 (0.0060)	
training:	Epoch: [140][86/233]	Loss 0.0004 (0.0060)	
training:	Epoch: [140][87/233]	Loss 0.0006 (0.0059)	
training:	Epoch: [140][88/233]	Loss 0.0003 (0.0058)	
training:	Epoch: [140][89/233]	Loss 0.0195 (0.0060)	
training:	Epoch: [140][90/233]	Loss 0.0003 (0.0059)	
training:	Epoch: [140][91/233]	Loss 0.0003 (0.0059)	
training:	Epoch: [140][92/233]	Loss 0.0003 (0.0058)	
training:	Epoch: [140][93/233]	Loss 0.0006 (0.0058)	
training:	Epoch: [140][94/233]	Loss 0.0118 (0.0058)	
training:	Epoch: [140][95/233]	Loss 0.0006 (0.0058)	
training:	Epoch: [140][96/233]	Loss 0.0016 (0.0057)	
training:	Epoch: [140][97/233]	Loss 0.0848 (0.0065)	
training:	Epoch: [140][98/233]	Loss 0.0069 (0.0065)	
training:	Epoch: [140][99/233]	Loss 0.0392 (0.0069)	
training:	Epoch: [140][100/233]	Loss 0.0003 (0.0068)	
training:	Epoch: [140][101/233]	Loss 0.0003 (0.0067)	
training:	Epoch: [140][102/233]	Loss 0.0004 (0.0067)	
training:	Epoch: [140][103/233]	Loss 0.1565 (0.0081)	
training:	Epoch: [140][104/233]	Loss 0.0017 (0.0081)	
training:	Epoch: [140][105/233]	Loss 0.0006 (0.0080)	
training:	Epoch: [140][106/233]	Loss 0.0040 (0.0080)	
training:	Epoch: [140][107/233]	Loss 0.0007 (0.0079)	
training:	Epoch: [140][108/233]	Loss 0.0003 (0.0078)	
training:	Epoch: [140][109/233]	Loss 0.0183 (0.0079)	
training:	Epoch: [140][110/233]	Loss 0.0030 (0.0079)	
training:	Epoch: [140][111/233]	Loss 0.0033 (0.0078)	
training:	Epoch: [140][112/233]	Loss 0.0005 (0.0078)	
training:	Epoch: [140][113/233]	Loss 0.0004 (0.0077)	
training:	Epoch: [140][114/233]	Loss 0.0012 (0.0076)	
training:	Epoch: [140][115/233]	Loss 0.0600 (0.0081)	
training:	Epoch: [140][116/233]	Loss 0.0004 (0.0080)	
training:	Epoch: [140][117/233]	Loss 0.0004 (0.0080)	
training:	Epoch: [140][118/233]	Loss 0.0094 (0.0080)	
training:	Epoch: [140][119/233]	Loss 0.0006 (0.0079)	
training:	Epoch: [140][120/233]	Loss 0.0004 (0.0079)	
training:	Epoch: [140][121/233]	Loss 0.0005 (0.0078)	
training:	Epoch: [140][122/233]	Loss 0.0003 (0.0077)	
training:	Epoch: [140][123/233]	Loss 0.0003 (0.0077)	
training:	Epoch: [140][124/233]	Loss 0.0478 (0.0080)	
training:	Epoch: [140][125/233]	Loss 0.0004 (0.0079)	
training:	Epoch: [140][126/233]	Loss 0.0004 (0.0079)	
training:	Epoch: [140][127/233]	Loss 0.0005 (0.0078)	
training:	Epoch: [140][128/233]	Loss 0.0007 (0.0078)	
training:	Epoch: [140][129/233]	Loss 0.0009 (0.0077)	
training:	Epoch: [140][130/233]	Loss 0.0005 (0.0076)	
training:	Epoch: [140][131/233]	Loss 0.0004 (0.0076)	
training:	Epoch: [140][132/233]	Loss 0.0003 (0.0075)	
training:	Epoch: [140][133/233]	Loss 0.0005 (0.0075)	
training:	Epoch: [140][134/233]	Loss 0.0003 (0.0074)	
training:	Epoch: [140][135/233]	Loss 0.0009 (0.0074)	
training:	Epoch: [140][136/233]	Loss 0.0004 (0.0073)	
training:	Epoch: [140][137/233]	Loss 0.0004 (0.0073)	
training:	Epoch: [140][138/233]	Loss 0.0004 (0.0072)	
training:	Epoch: [140][139/233]	Loss 0.0012 (0.0072)	
training:	Epoch: [140][140/233]	Loss 0.0003 (0.0071)	
training:	Epoch: [140][141/233]	Loss 0.0005 (0.0071)	
training:	Epoch: [140][142/233]	Loss 0.0005 (0.0070)	
training:	Epoch: [140][143/233]	Loss 0.0005 (0.0070)	
training:	Epoch: [140][144/233]	Loss 0.0004 (0.0070)	
training:	Epoch: [140][145/233]	Loss 0.0004 (0.0069)	
training:	Epoch: [140][146/233]	Loss 0.0005 (0.0069)	
training:	Epoch: [140][147/233]	Loss 0.0003 (0.0068)	
training:	Epoch: [140][148/233]	Loss 0.0007 (0.0068)	
training:	Epoch: [140][149/233]	Loss 0.0705 (0.0072)	
training:	Epoch: [140][150/233]	Loss 0.0003 (0.0072)	
training:	Epoch: [140][151/233]	Loss 0.0004 (0.0071)	
training:	Epoch: [140][152/233]	Loss 0.0122 (0.0071)	
training:	Epoch: [140][153/233]	Loss 0.0003 (0.0071)	
training:	Epoch: [140][154/233]	Loss 0.0007 (0.0071)	
training:	Epoch: [140][155/233]	Loss 0.0004 (0.0070)	
training:	Epoch: [140][156/233]	Loss 0.0004 (0.0070)	
training:	Epoch: [140][157/233]	Loss 0.0005 (0.0069)	
training:	Epoch: [140][158/233]	Loss 0.0005 (0.0069)	
training:	Epoch: [140][159/233]	Loss 0.0035 (0.0069)	
training:	Epoch: [140][160/233]	Loss 0.0294 (0.0070)	
training:	Epoch: [140][161/233]	Loss 0.0003 (0.0070)	
training:	Epoch: [140][162/233]	Loss 0.0005 (0.0069)	
training:	Epoch: [140][163/233]	Loss 0.0003 (0.0069)	
training:	Epoch: [140][164/233]	Loss 0.0004 (0.0069)	
training:	Epoch: [140][165/233]	Loss 0.0004 (0.0068)	
training:	Epoch: [140][166/233]	Loss 0.0003 (0.0068)	
training:	Epoch: [140][167/233]	Loss 0.0015 (0.0067)	
training:	Epoch: [140][168/233]	Loss 0.0248 (0.0069)	
training:	Epoch: [140][169/233]	Loss 0.0036 (0.0068)	
training:	Epoch: [140][170/233]	Loss 0.0004 (0.0068)	
training:	Epoch: [140][171/233]	Loss 0.0004 (0.0068)	
training:	Epoch: [140][172/233]	Loss 0.0004 (0.0067)	
training:	Epoch: [140][173/233]	Loss 0.0003 (0.0067)	
training:	Epoch: [140][174/233]	Loss 0.0003 (0.0066)	
training:	Epoch: [140][175/233]	Loss 0.0004 (0.0066)	
training:	Epoch: [140][176/233]	Loss 0.0004 (0.0066)	
training:	Epoch: [140][177/233]	Loss 0.0004 (0.0065)	
training:	Epoch: [140][178/233]	Loss 0.0018 (0.0065)	
training:	Epoch: [140][179/233]	Loss 0.0004 (0.0065)	
training:	Epoch: [140][180/233]	Loss 0.0003 (0.0064)	
training:	Epoch: [140][181/233]	Loss 0.0961 (0.0069)	
training:	Epoch: [140][182/233]	Loss 0.0005 (0.0069)	
training:	Epoch: [140][183/233]	Loss 0.0005 (0.0069)	
training:	Epoch: [140][184/233]	Loss 0.1535 (0.0077)	
training:	Epoch: [140][185/233]	Loss 0.0134 (0.0077)	
training:	Epoch: [140][186/233]	Loss 0.0004 (0.0077)	
training:	Epoch: [140][187/233]	Loss 0.0004 (0.0076)	
training:	Epoch: [140][188/233]	Loss 0.0038 (0.0076)	
training:	Epoch: [140][189/233]	Loss 0.0021 (0.0076)	
training:	Epoch: [140][190/233]	Loss 0.0010 (0.0075)	
training:	Epoch: [140][191/233]	Loss 0.0005 (0.0075)	
training:	Epoch: [140][192/233]	Loss 0.0003 (0.0075)	
training:	Epoch: [140][193/233]	Loss 0.0003 (0.0074)	
training:	Epoch: [140][194/233]	Loss 0.0066 (0.0074)	
training:	Epoch: [140][195/233]	Loss 0.0003 (0.0074)	
training:	Epoch: [140][196/233]	Loss 0.0004 (0.0073)	
training:	Epoch: [140][197/233]	Loss 0.0315 (0.0075)	
training:	Epoch: [140][198/233]	Loss 0.0009 (0.0074)	
training:	Epoch: [140][199/233]	Loss 0.0019 (0.0074)	
training:	Epoch: [140][200/233]	Loss 0.0410 (0.0076)	
training:	Epoch: [140][201/233]	Loss 0.0005 (0.0075)	
training:	Epoch: [140][202/233]	Loss 0.0009 (0.0075)	
training:	Epoch: [140][203/233]	Loss 0.0003 (0.0075)	
training:	Epoch: [140][204/233]	Loss 0.0005 (0.0074)	
training:	Epoch: [140][205/233]	Loss 0.0006 (0.0074)	
training:	Epoch: [140][206/233]	Loss 0.0005 (0.0074)	
training:	Epoch: [140][207/233]	Loss 0.0037 (0.0074)	
training:	Epoch: [140][208/233]	Loss 0.0010 (0.0073)	
training:	Epoch: [140][209/233]	Loss 0.0005 (0.0073)	
training:	Epoch: [140][210/233]	Loss 0.0017 (0.0073)	
training:	Epoch: [140][211/233]	Loss 0.0003 (0.0072)	
training:	Epoch: [140][212/233]	Loss 0.0004 (0.0072)	
training:	Epoch: [140][213/233]	Loss 0.1101 (0.0077)	
training:	Epoch: [140][214/233]	Loss 0.0003 (0.0076)	
training:	Epoch: [140][215/233]	Loss 0.0011 (0.0076)	
training:	Epoch: [140][216/233]	Loss 0.0004 (0.0076)	
training:	Epoch: [140][217/233]	Loss 0.0295 (0.0077)	
training:	Epoch: [140][218/233]	Loss 0.0004 (0.0077)	
training:	Epoch: [140][219/233]	Loss 0.0026 (0.0076)	
training:	Epoch: [140][220/233]	Loss 0.0003 (0.0076)	
training:	Epoch: [140][221/233]	Loss 0.0006 (0.0076)	
training:	Epoch: [140][222/233]	Loss 0.0016 (0.0075)	
training:	Epoch: [140][223/233]	Loss 0.0010 (0.0075)	
training:	Epoch: [140][224/233]	Loss 0.0003 (0.0075)	
training:	Epoch: [140][225/233]	Loss 0.0041 (0.0075)	
training:	Epoch: [140][226/233]	Loss 0.0012 (0.0074)	
training:	Epoch: [140][227/233]	Loss 0.0010 (0.0074)	
training:	Epoch: [140][228/233]	Loss 0.0007 (0.0074)	
training:	Epoch: [140][229/233]	Loss 0.0008 (0.0073)	
training:	Epoch: [140][230/233]	Loss 0.0193 (0.0074)	
training:	Epoch: [140][231/233]	Loss 0.0007 (0.0074)	
training:	Epoch: [140][232/233]	Loss 0.0008 (0.0073)	
training:	Epoch: [140][233/233]	Loss 0.0005 (0.0073)	
Training:	 Loss: 0.0073

Training:	 ACC: 1.0000 1.0000 1.0000 1.0000
Validation:	 ACC: 0.7960 0.7972 0.8212 0.7708
Validation:	 Best_BACC: 0.8162 0.8154 0.7998 0.8326
Validation:	 Loss: 1.2107
Pretraining:	Epoch 141/200
----------
training:	Epoch: [141][1/233]	Loss 0.0004 (0.0004)	
training:	Epoch: [141][2/233]	Loss 0.0003 (0.0004)	
training:	Epoch: [141][3/233]	Loss 0.0003 (0.0003)	
training:	Epoch: [141][4/233]	Loss 0.0004 (0.0003)	
training:	Epoch: [141][5/233]	Loss 0.0005 (0.0004)	
training:	Epoch: [141][6/233]	Loss 0.0003 (0.0004)	
training:	Epoch: [141][7/233]	Loss 0.0004 (0.0004)	
training:	Epoch: [141][8/233]	Loss 0.0057 (0.0011)	
training:	Epoch: [141][9/233]	Loss 0.0004 (0.0010)	
training:	Epoch: [141][10/233]	Loss 0.0007 (0.0009)	
training:	Epoch: [141][11/233]	Loss 0.0005 (0.0009)	
training:	Epoch: [141][12/233]	Loss 0.0005 (0.0009)	
training:	Epoch: [141][13/233]	Loss 0.0004 (0.0008)	
training:	Epoch: [141][14/233]	Loss 0.0005 (0.0008)	
training:	Epoch: [141][15/233]	Loss 0.0004 (0.0008)	
training:	Epoch: [141][16/233]	Loss 0.0003 (0.0008)	
training:	Epoch: [141][17/233]	Loss 0.0004 (0.0007)	
training:	Epoch: [141][18/233]	Loss 0.0004 (0.0007)	
training:	Epoch: [141][19/233]	Loss 0.0004 (0.0007)	
training:	Epoch: [141][20/233]	Loss 0.0009 (0.0007)	
training:	Epoch: [141][21/233]	Loss 0.0005 (0.0007)	
training:	Epoch: [141][22/233]	Loss 0.0005 (0.0007)	
training:	Epoch: [141][23/233]	Loss 0.0006 (0.0007)	
training:	Epoch: [141][24/233]	Loss 0.0006 (0.0007)	
training:	Epoch: [141][25/233]	Loss 0.0016 (0.0007)	
training:	Epoch: [141][26/233]	Loss 0.0021 (0.0008)	
training:	Epoch: [141][27/233]	Loss 0.0004 (0.0008)	
training:	Epoch: [141][28/233]	Loss 0.0006 (0.0008)	
training:	Epoch: [141][29/233]	Loss 0.0007 (0.0007)	
training:	Epoch: [141][30/233]	Loss 0.0004 (0.0007)	
training:	Epoch: [141][31/233]	Loss 0.0014 (0.0008)	
training:	Epoch: [141][32/233]	Loss 0.0016 (0.0008)	
training:	Epoch: [141][33/233]	Loss 0.0004 (0.0008)	
training:	Epoch: [141][34/233]	Loss 0.0006 (0.0008)	
training:	Epoch: [141][35/233]	Loss 0.0004 (0.0008)	
training:	Epoch: [141][36/233]	Loss 0.0004 (0.0007)	
training:	Epoch: [141][37/233]	Loss 0.0004 (0.0007)	
training:	Epoch: [141][38/233]	Loss 0.0004 (0.0007)	
training:	Epoch: [141][39/233]	Loss 0.0004 (0.0007)	
training:	Epoch: [141][40/233]	Loss 0.0017 (0.0007)	
training:	Epoch: [141][41/233]	Loss 0.0004 (0.0007)	
training:	Epoch: [141][42/233]	Loss 0.0004 (0.0007)	
training:	Epoch: [141][43/233]	Loss 0.0005 (0.0007)	
training:	Epoch: [141][44/233]	Loss 0.0004 (0.0007)	
training:	Epoch: [141][45/233]	Loss 0.0006 (0.0007)	
training:	Epoch: [141][46/233]	Loss 0.0004 (0.0007)	
training:	Epoch: [141][47/233]	Loss 0.0017 (0.0007)	
training:	Epoch: [141][48/233]	Loss 0.0003 (0.0007)	
training:	Epoch: [141][49/233]	Loss 0.0008 (0.0007)	
training:	Epoch: [141][50/233]	Loss 0.0006 (0.0007)	
training:	Epoch: [141][51/233]	Loss 0.0006 (0.0007)	
training:	Epoch: [141][52/233]	Loss 0.0005 (0.0007)	
training:	Epoch: [141][53/233]	Loss 0.0004 (0.0007)	
training:	Epoch: [141][54/233]	Loss 0.0004 (0.0007)	
training:	Epoch: [141][55/233]	Loss 0.0005 (0.0007)	
training:	Epoch: [141][56/233]	Loss 0.0003 (0.0007)	
training:	Epoch: [141][57/233]	Loss 0.0004 (0.0007)	
training:	Epoch: [141][58/233]	Loss 0.0732 (0.0019)	
training:	Epoch: [141][59/233]	Loss 0.0004 (0.0019)	
training:	Epoch: [141][60/233]	Loss 0.0004 (0.0019)	
training:	Epoch: [141][61/233]	Loss 0.0015 (0.0019)	
training:	Epoch: [141][62/233]	Loss 0.0007 (0.0019)	
training:	Epoch: [141][63/233]	Loss 0.0004 (0.0018)	
training:	Epoch: [141][64/233]	Loss 0.0015 (0.0018)	
training:	Epoch: [141][65/233]	Loss 0.0006 (0.0018)	
training:	Epoch: [141][66/233]	Loss 0.0004 (0.0018)	
training:	Epoch: [141][67/233]	Loss 0.1263 (0.0036)	
training:	Epoch: [141][68/233]	Loss 0.0152 (0.0038)	
training:	Epoch: [141][69/233]	Loss 0.0011 (0.0038)	
training:	Epoch: [141][70/233]	Loss 0.0608 (0.0046)	
training:	Epoch: [141][71/233]	Loss 0.0003 (0.0045)	
training:	Epoch: [141][72/233]	Loss 0.0004 (0.0045)	
training:	Epoch: [141][73/233]	Loss 0.0006 (0.0044)	
training:	Epoch: [141][74/233]	Loss 0.0003 (0.0044)	
training:	Epoch: [141][75/233]	Loss 0.0004 (0.0043)	
training:	Epoch: [141][76/233]	Loss 0.0004 (0.0043)	
training:	Epoch: [141][77/233]	Loss 0.0004 (0.0042)	
training:	Epoch: [141][78/233]	Loss 0.0005 (0.0042)	
training:	Epoch: [141][79/233]	Loss 0.0004 (0.0041)	
training:	Epoch: [141][80/233]	Loss 0.0009 (0.0041)	
training:	Epoch: [141][81/233]	Loss 0.0004 (0.0040)	
training:	Epoch: [141][82/233]	Loss 0.0003 (0.0040)	
training:	Epoch: [141][83/233]	Loss 0.0003 (0.0039)	
training:	Epoch: [141][84/233]	Loss 0.0006 (0.0039)	
training:	Epoch: [141][85/233]	Loss 0.0125 (0.0040)	
training:	Epoch: [141][86/233]	Loss 0.0004 (0.0040)	
training:	Epoch: [141][87/233]	Loss 0.0005 (0.0039)	
training:	Epoch: [141][88/233]	Loss 0.0008 (0.0039)	
training:	Epoch: [141][89/233]	Loss 0.0004 (0.0038)	
training:	Epoch: [141][90/233]	Loss 0.0003 (0.0038)	
training:	Epoch: [141][91/233]	Loss 0.0003 (0.0038)	
training:	Epoch: [141][92/233]	Loss 0.0020 (0.0037)	
training:	Epoch: [141][93/233]	Loss 0.0003 (0.0037)	
training:	Epoch: [141][94/233]	Loss 0.0008 (0.0037)	
training:	Epoch: [141][95/233]	Loss 0.0006 (0.0036)	
training:	Epoch: [141][96/233]	Loss 0.0004 (0.0036)	
training:	Epoch: [141][97/233]	Loss 0.0005 (0.0036)	
training:	Epoch: [141][98/233]	Loss 0.0007 (0.0036)	
training:	Epoch: [141][99/233]	Loss 0.0005 (0.0035)	
training:	Epoch: [141][100/233]	Loss 0.0005 (0.0035)	
training:	Epoch: [141][101/233]	Loss 0.0015 (0.0035)	
training:	Epoch: [141][102/233]	Loss 0.0006 (0.0034)	
training:	Epoch: [141][103/233]	Loss 0.0005 (0.0034)	
training:	Epoch: [141][104/233]	Loss 0.0004 (0.0034)	
training:	Epoch: [141][105/233]	Loss 0.0004 (0.0034)	
training:	Epoch: [141][106/233]	Loss 0.0009 (0.0033)	
training:	Epoch: [141][107/233]	Loss 0.0003 (0.0033)	
training:	Epoch: [141][108/233]	Loss 0.0047 (0.0033)	
training:	Epoch: [141][109/233]	Loss 0.0395 (0.0036)	
training:	Epoch: [141][110/233]	Loss 0.0008 (0.0036)	
training:	Epoch: [141][111/233]	Loss 0.0004 (0.0036)	
training:	Epoch: [141][112/233]	Loss 0.0005 (0.0036)	
training:	Epoch: [141][113/233]	Loss 0.0004 (0.0035)	
training:	Epoch: [141][114/233]	Loss 0.0004 (0.0035)	
training:	Epoch: [141][115/233]	Loss 0.0004 (0.0035)	
training:	Epoch: [141][116/233]	Loss 0.0007 (0.0035)	
training:	Epoch: [141][117/233]	Loss 0.0004 (0.0034)	
training:	Epoch: [141][118/233]	Loss 0.0005 (0.0034)	
training:	Epoch: [141][119/233]	Loss 0.0004 (0.0034)	
training:	Epoch: [141][120/233]	Loss 0.0003 (0.0034)	
training:	Epoch: [141][121/233]	Loss 0.0027 (0.0034)	
training:	Epoch: [141][122/233]	Loss 0.0008 (0.0033)	
training:	Epoch: [141][123/233]	Loss 0.1520 (0.0045)	
training:	Epoch: [141][124/233]	Loss 0.0004 (0.0045)	
training:	Epoch: [141][125/233]	Loss 0.0006 (0.0045)	
training:	Epoch: [141][126/233]	Loss 0.0048 (0.0045)	
training:	Epoch: [141][127/233]	Loss 0.0009 (0.0044)	
training:	Epoch: [141][128/233]	Loss 0.0006 (0.0044)	
training:	Epoch: [141][129/233]	Loss 0.0003 (0.0044)	
training:	Epoch: [141][130/233]	Loss 0.0004 (0.0044)	
training:	Epoch: [141][131/233]	Loss 0.0005 (0.0043)	
training:	Epoch: [141][132/233]	Loss 0.0005 (0.0043)	
training:	Epoch: [141][133/233]	Loss 0.0017 (0.0043)	
training:	Epoch: [141][134/233]	Loss 0.0003 (0.0042)	
training:	Epoch: [141][135/233]	Loss 0.0013 (0.0042)	
training:	Epoch: [141][136/233]	Loss 0.0004 (0.0042)	
training:	Epoch: [141][137/233]	Loss 0.0004 (0.0042)	
training:	Epoch: [141][138/233]	Loss 0.0003 (0.0041)	
training:	Epoch: [141][139/233]	Loss 0.0006 (0.0041)	
training:	Epoch: [141][140/233]	Loss 0.0004 (0.0041)	
training:	Epoch: [141][141/233]	Loss 0.0004 (0.0041)	
training:	Epoch: [141][142/233]	Loss 0.0003 (0.0040)	
training:	Epoch: [141][143/233]	Loss 0.0003 (0.0040)	
training:	Epoch: [141][144/233]	Loss 0.0004 (0.0040)	
training:	Epoch: [141][145/233]	Loss 0.0003 (0.0040)	
training:	Epoch: [141][146/233]	Loss 0.0015 (0.0039)	
training:	Epoch: [141][147/233]	Loss 0.0004 (0.0039)	
training:	Epoch: [141][148/233]	Loss 0.0003 (0.0039)	
training:	Epoch: [141][149/233]	Loss 0.0004 (0.0039)	
training:	Epoch: [141][150/233]	Loss 0.0004 (0.0038)	
training:	Epoch: [141][151/233]	Loss 0.0007 (0.0038)	
training:	Epoch: [141][152/233]	Loss 0.0004 (0.0038)	
training:	Epoch: [141][153/233]	Loss 0.0005 (0.0038)	
training:	Epoch: [141][154/233]	Loss 0.0004 (0.0038)	
training:	Epoch: [141][155/233]	Loss 0.0004 (0.0037)	
training:	Epoch: [141][156/233]	Loss 0.0005 (0.0037)	
training:	Epoch: [141][157/233]	Loss 0.0011 (0.0037)	
training:	Epoch: [141][158/233]	Loss 0.0003 (0.0037)	
training:	Epoch: [141][159/233]	Loss 0.0004 (0.0037)	
training:	Epoch: [141][160/233]	Loss 0.0009 (0.0036)	
training:	Epoch: [141][161/233]	Loss 0.0008 (0.0036)	
training:	Epoch: [141][162/233]	Loss 0.0004 (0.0036)	
training:	Epoch: [141][163/233]	Loss 0.0005 (0.0036)	
training:	Epoch: [141][164/233]	Loss 0.0003 (0.0036)	
training:	Epoch: [141][165/233]	Loss 0.0004 (0.0035)	
training:	Epoch: [141][166/233]	Loss 0.0005 (0.0035)	
training:	Epoch: [141][167/233]	Loss 0.0004 (0.0035)	
training:	Epoch: [141][168/233]	Loss 0.0006 (0.0035)	
training:	Epoch: [141][169/233]	Loss 0.0006 (0.0035)	
training:	Epoch: [141][170/233]	Loss 0.0008 (0.0035)	
training:	Epoch: [141][171/233]	Loss 0.0012 (0.0034)	
training:	Epoch: [141][172/233]	Loss 0.0010 (0.0034)	
training:	Epoch: [141][173/233]	Loss 0.0464 (0.0037)	
training:	Epoch: [141][174/233]	Loss 0.0014 (0.0037)	
training:	Epoch: [141][175/233]	Loss 0.0004 (0.0037)	
training:	Epoch: [141][176/233]	Loss 0.0004 (0.0036)	
training:	Epoch: [141][177/233]	Loss 0.0048 (0.0036)	
training:	Epoch: [141][178/233]	Loss 0.0005 (0.0036)	
training:	Epoch: [141][179/233]	Loss 0.0005 (0.0036)	
training:	Epoch: [141][180/233]	Loss 0.0008 (0.0036)	
training:	Epoch: [141][181/233]	Loss 0.0004 (0.0036)	
training:	Epoch: [141][182/233]	Loss 0.0004 (0.0036)	
training:	Epoch: [141][183/233]	Loss 0.0015 (0.0035)	
training:	Epoch: [141][184/233]	Loss 0.0003 (0.0035)	
training:	Epoch: [141][185/233]	Loss 0.0005 (0.0035)	
training:	Epoch: [141][186/233]	Loss 0.0003 (0.0035)	
training:	Epoch: [141][187/233]	Loss 0.0008 (0.0035)	
training:	Epoch: [141][188/233]	Loss 0.0006 (0.0035)	
training:	Epoch: [141][189/233]	Loss 0.0004 (0.0034)	
training:	Epoch: [141][190/233]	Loss 0.0009 (0.0034)	
training:	Epoch: [141][191/233]	Loss 0.0004 (0.0034)	
training:	Epoch: [141][192/233]	Loss 0.0005 (0.0034)	
training:	Epoch: [141][193/233]	Loss 0.0004 (0.0034)	
training:	Epoch: [141][194/233]	Loss 0.0004 (0.0034)	
training:	Epoch: [141][195/233]	Loss 0.0006 (0.0034)	
training:	Epoch: [141][196/233]	Loss 0.0006 (0.0033)	
training:	Epoch: [141][197/233]	Loss 0.0008 (0.0033)	
training:	Epoch: [141][198/233]	Loss 0.0008 (0.0033)	
training:	Epoch: [141][199/233]	Loss 0.0004 (0.0033)	
training:	Epoch: [141][200/233]	Loss 0.0004 (0.0033)	
training:	Epoch: [141][201/233]	Loss 0.0006 (0.0033)	
training:	Epoch: [141][202/233]	Loss 0.0003 (0.0033)	
training:	Epoch: [141][203/233]	Loss 0.0011 (0.0032)	
training:	Epoch: [141][204/233]	Loss 0.0127 (0.0033)	
training:	Epoch: [141][205/233]	Loss 0.0018 (0.0033)	
training:	Epoch: [141][206/233]	Loss 0.0003 (0.0033)	
training:	Epoch: [141][207/233]	Loss 0.0008 (0.0033)	
training:	Epoch: [141][208/233]	Loss 0.0028 (0.0033)	
training:	Epoch: [141][209/233]	Loss 0.0005 (0.0032)	
training:	Epoch: [141][210/233]	Loss 0.0004 (0.0032)	
training:	Epoch: [141][211/233]	Loss 0.0400 (0.0034)	
training:	Epoch: [141][212/233]	Loss 0.0008 (0.0034)	
training:	Epoch: [141][213/233]	Loss 0.0013 (0.0034)	
training:	Epoch: [141][214/233]	Loss 0.0011 (0.0034)	
training:	Epoch: [141][215/233]	Loss 0.0477 (0.0036)	
training:	Epoch: [141][216/233]	Loss 0.0007 (0.0036)	
training:	Epoch: [141][217/233]	Loss 0.0011 (0.0036)	
training:	Epoch: [141][218/233]	Loss 0.0182 (0.0036)	
training:	Epoch: [141][219/233]	Loss 0.0010 (0.0036)	
training:	Epoch: [141][220/233]	Loss 0.0004 (0.0036)	
training:	Epoch: [141][221/233]	Loss 0.0011 (0.0036)	
training:	Epoch: [141][222/233]	Loss 0.0004 (0.0036)	
training:	Epoch: [141][223/233]	Loss 0.0005 (0.0036)	
training:	Epoch: [141][224/233]	Loss 0.0004 (0.0035)	
training:	Epoch: [141][225/233]	Loss 0.0856 (0.0039)	
training:	Epoch: [141][226/233]	Loss 0.0233 (0.0040)	
training:	Epoch: [141][227/233]	Loss 0.0011 (0.0040)	
training:	Epoch: [141][228/233]	Loss 0.0011 (0.0040)	
training:	Epoch: [141][229/233]	Loss 0.0126 (0.0040)	
training:	Epoch: [141][230/233]	Loss 0.0022 (0.0040)	
training:	Epoch: [141][231/233]	Loss 0.0282 (0.0041)	
training:	Epoch: [141][232/233]	Loss 0.1038 (0.0045)	
training:	Epoch: [141][233/233]	Loss 0.0004 (0.0045)	
Training:	 Loss: 0.0045

Training:	 ACC: 0.9999 0.9999 0.9997 1.0000
Validation:	 ACC: 0.7941 0.7929 0.7702 0.8180
Validation:	 Best_BACC: 0.8162 0.8154 0.7998 0.8326
Validation:	 Loss: 1.1954
Pretraining:	Epoch 142/200
----------
training:	Epoch: [142][1/233]	Loss 0.0193 (0.0193)	
training:	Epoch: [142][2/233]	Loss 0.0003 (0.0098)	
training:	Epoch: [142][3/233]	Loss 0.0032 (0.0076)	
training:	Epoch: [142][4/233]	Loss 0.0005 (0.0058)	
training:	Epoch: [142][5/233]	Loss 0.0044 (0.0055)	
training:	Epoch: [142][6/233]	Loss 0.0006 (0.0047)	
training:	Epoch: [142][7/233]	Loss 0.0005 (0.0041)	
training:	Epoch: [142][8/233]	Loss 0.0012 (0.0038)	
training:	Epoch: [142][9/233]	Loss 0.0004 (0.0034)	
training:	Epoch: [142][10/233]	Loss 0.0003 (0.0031)	
training:	Epoch: [142][11/233]	Loss 0.0003 (0.0028)	
training:	Epoch: [142][12/233]	Loss 0.0004 (0.0026)	
training:	Epoch: [142][13/233]	Loss 0.0022 (0.0026)	
training:	Epoch: [142][14/233]	Loss 0.0005 (0.0024)	
training:	Epoch: [142][15/233]	Loss 0.0028 (0.0025)	
training:	Epoch: [142][16/233]	Loss 0.0101 (0.0029)	
training:	Epoch: [142][17/233]	Loss 0.0015 (0.0029)	
training:	Epoch: [142][18/233]	Loss 0.0043 (0.0029)	
training:	Epoch: [142][19/233]	Loss 0.0260 (0.0042)	
training:	Epoch: [142][20/233]	Loss 0.0007 (0.0040)	
training:	Epoch: [142][21/233]	Loss 0.0014 (0.0039)	
training:	Epoch: [142][22/233]	Loss 0.0028 (0.0038)	
training:	Epoch: [142][23/233]	Loss 0.0005 (0.0037)	
training:	Epoch: [142][24/233]	Loss 0.0004 (0.0035)	
training:	Epoch: [142][25/233]	Loss 0.0065 (0.0037)	
training:	Epoch: [142][26/233]	Loss 0.0003 (0.0035)	
training:	Epoch: [142][27/233]	Loss 0.0015 (0.0034)	
training:	Epoch: [142][28/233]	Loss 0.0004 (0.0033)	
training:	Epoch: [142][29/233]	Loss 0.0022 (0.0033)	
training:	Epoch: [142][30/233]	Loss 0.0014 (0.0032)	
training:	Epoch: [142][31/233]	Loss 0.0006 (0.0032)	
training:	Epoch: [142][32/233]	Loss 0.0005 (0.0031)	
training:	Epoch: [142][33/233]	Loss 0.0004 (0.0030)	
training:	Epoch: [142][34/233]	Loss 0.0017 (0.0030)	
training:	Epoch: [142][35/233]	Loss 0.0026 (0.0029)	
training:	Epoch: [142][36/233]	Loss 0.0004 (0.0029)	
training:	Epoch: [142][37/233]	Loss 0.0032 (0.0029)	
training:	Epoch: [142][38/233]	Loss 0.0004 (0.0028)	
training:	Epoch: [142][39/233]	Loss 0.0125 (0.0031)	
training:	Epoch: [142][40/233]	Loss 0.0003 (0.0030)	
training:	Epoch: [142][41/233]	Loss 0.0008 (0.0029)	
training:	Epoch: [142][42/233]	Loss 0.0283 (0.0035)	
training:	Epoch: [142][43/233]	Loss 0.0003 (0.0035)	
training:	Epoch: [142][44/233]	Loss 0.0004 (0.0034)	
training:	Epoch: [142][45/233]	Loss 0.0004 (0.0033)	
training:	Epoch: [142][46/233]	Loss 0.0004 (0.0033)	
training:	Epoch: [142][47/233]	Loss 0.0003 (0.0032)	
training:	Epoch: [142][48/233]	Loss 0.0004 (0.0032)	
training:	Epoch: [142][49/233]	Loss 0.0004 (0.0031)	
training:	Epoch: [142][50/233]	Loss 0.0015 (0.0031)	
training:	Epoch: [142][51/233]	Loss 0.0067 (0.0031)	
training:	Epoch: [142][52/233]	Loss 0.0004 (0.0031)	
training:	Epoch: [142][53/233]	Loss 0.0013 (0.0030)	
training:	Epoch: [142][54/233]	Loss 0.0007 (0.0030)	
training:	Epoch: [142][55/233]	Loss 0.0005 (0.0030)	
training:	Epoch: [142][56/233]	Loss 0.0004 (0.0029)	
training:	Epoch: [142][57/233]	Loss 0.0003 (0.0029)	
training:	Epoch: [142][58/233]	Loss 0.0005 (0.0028)	
training:	Epoch: [142][59/233]	Loss 0.0005 (0.0028)	
training:	Epoch: [142][60/233]	Loss 0.0003 (0.0027)	
training:	Epoch: [142][61/233]	Loss 0.0019 (0.0027)	
training:	Epoch: [142][62/233]	Loss 0.0008 (0.0027)	
training:	Epoch: [142][63/233]	Loss 0.0521 (0.0035)	
training:	Epoch: [142][64/233]	Loss 0.0003 (0.0034)	
training:	Epoch: [142][65/233]	Loss 0.0003 (0.0034)	
training:	Epoch: [142][66/233]	Loss 0.0007 (0.0033)	
training:	Epoch: [142][67/233]	Loss 0.0004 (0.0033)	
training:	Epoch: [142][68/233]	Loss 0.0003 (0.0033)	
training:	Epoch: [142][69/233]	Loss 0.0003 (0.0032)	
training:	Epoch: [142][70/233]	Loss 0.0003 (0.0032)	
training:	Epoch: [142][71/233]	Loss 0.0006 (0.0031)	
training:	Epoch: [142][72/233]	Loss 0.0006 (0.0031)	
training:	Epoch: [142][73/233]	Loss 0.0005 (0.0031)	
training:	Epoch: [142][74/233]	Loss 0.0003 (0.0030)	
training:	Epoch: [142][75/233]	Loss 0.0005 (0.0030)	
training:	Epoch: [142][76/233]	Loss 0.0007 (0.0030)	
training:	Epoch: [142][77/233]	Loss 0.0004 (0.0029)	
training:	Epoch: [142][78/233]	Loss 0.0003 (0.0029)	
training:	Epoch: [142][79/233]	Loss 0.0005 (0.0029)	
training:	Epoch: [142][80/233]	Loss 0.0011 (0.0028)	
training:	Epoch: [142][81/233]	Loss 0.0004 (0.0028)	
training:	Epoch: [142][82/233]	Loss 0.0004 (0.0028)	
training:	Epoch: [142][83/233]	Loss 0.0005 (0.0028)	
training:	Epoch: [142][84/233]	Loss 0.0004 (0.0027)	
training:	Epoch: [142][85/233]	Loss 0.0025 (0.0027)	
training:	Epoch: [142][86/233]	Loss 0.0003 (0.0027)	
training:	Epoch: [142][87/233]	Loss 0.0006 (0.0027)	
training:	Epoch: [142][88/233]	Loss 0.0004 (0.0027)	
training:	Epoch: [142][89/233]	Loss 0.0003 (0.0026)	
training:	Epoch: [142][90/233]	Loss 0.0003 (0.0026)	
training:	Epoch: [142][91/233]	Loss 0.0004 (0.0026)	
training:	Epoch: [142][92/233]	Loss 0.0003 (0.0026)	
training:	Epoch: [142][93/233]	Loss 0.0004 (0.0025)	
training:	Epoch: [142][94/233]	Loss 0.0003 (0.0025)	
training:	Epoch: [142][95/233]	Loss 0.0003 (0.0025)	
training:	Epoch: [142][96/233]	Loss 0.0018 (0.0025)	
training:	Epoch: [142][97/233]	Loss 0.0003 (0.0025)	
training:	Epoch: [142][98/233]	Loss 0.0008 (0.0024)	
training:	Epoch: [142][99/233]	Loss 0.0006 (0.0024)	
training:	Epoch: [142][100/233]	Loss 0.0060 (0.0025)	
training:	Epoch: [142][101/233]	Loss 0.0008 (0.0024)	
training:	Epoch: [142][102/233]	Loss 0.0003 (0.0024)	
training:	Epoch: [142][103/233]	Loss 0.0004 (0.0024)	
training:	Epoch: [142][104/233]	Loss 0.0007 (0.0024)	
training:	Epoch: [142][105/233]	Loss 0.0006 (0.0024)	
training:	Epoch: [142][106/233]	Loss 0.0009 (0.0023)	
training:	Epoch: [142][107/233]	Loss 0.0006 (0.0023)	
training:	Epoch: [142][108/233]	Loss 0.0050 (0.0024)	
training:	Epoch: [142][109/233]	Loss 0.0003 (0.0023)	
training:	Epoch: [142][110/233]	Loss 0.0005 (0.0023)	
training:	Epoch: [142][111/233]	Loss 0.0004 (0.0023)	
training:	Epoch: [142][112/233]	Loss 0.0004 (0.0023)	
training:	Epoch: [142][113/233]	Loss 0.0009 (0.0023)	
training:	Epoch: [142][114/233]	Loss 0.0004 (0.0023)	
training:	Epoch: [142][115/233]	Loss 0.0004 (0.0022)	
training:	Epoch: [142][116/233]	Loss 0.0005 (0.0022)	
training:	Epoch: [142][117/233]	Loss 0.0021 (0.0022)	
training:	Epoch: [142][118/233]	Loss 0.0006 (0.0022)	
training:	Epoch: [142][119/233]	Loss 0.0004 (0.0022)	
training:	Epoch: [142][120/233]	Loss 0.0004 (0.0022)	
training:	Epoch: [142][121/233]	Loss 0.0004 (0.0022)	
training:	Epoch: [142][122/233]	Loss 0.0089 (0.0022)	
training:	Epoch: [142][123/233]	Loss 0.0003 (0.0022)	
training:	Epoch: [142][124/233]	Loss 0.0004 (0.0022)	
training:	Epoch: [142][125/233]	Loss 0.0003 (0.0022)	
training:	Epoch: [142][126/233]	Loss 0.0003 (0.0022)	
training:	Epoch: [142][127/233]	Loss 0.0003 (0.0021)	
training:	Epoch: [142][128/233]	Loss 0.0008 (0.0021)	
training:	Epoch: [142][129/233]	Loss 0.0016 (0.0021)	
training:	Epoch: [142][130/233]	Loss 0.0005 (0.0021)	
training:	Epoch: [142][131/233]	Loss 0.0003 (0.0021)	
training:	Epoch: [142][132/233]	Loss 0.0004 (0.0021)	
training:	Epoch: [142][133/233]	Loss 0.0003 (0.0021)	
training:	Epoch: [142][134/233]	Loss 0.0004 (0.0021)	
training:	Epoch: [142][135/233]	Loss 0.0020 (0.0021)	
training:	Epoch: [142][136/233]	Loss 0.0030 (0.0021)	
training:	Epoch: [142][137/233]	Loss 0.0004 (0.0021)	
training:	Epoch: [142][138/233]	Loss 0.0005 (0.0020)	
training:	Epoch: [142][139/233]	Loss 0.0003 (0.0020)	
training:	Epoch: [142][140/233]	Loss 0.0005 (0.0020)	
training:	Epoch: [142][141/233]	Loss 0.0003 (0.0020)	
training:	Epoch: [142][142/233]	Loss 0.0006 (0.0020)	
training:	Epoch: [142][143/233]	Loss 0.0005 (0.0020)	
training:	Epoch: [142][144/233]	Loss 0.0004 (0.0020)	
training:	Epoch: [142][145/233]	Loss 0.0003 (0.0020)	
training:	Epoch: [142][146/233]	Loss 0.0005 (0.0020)	
training:	Epoch: [142][147/233]	Loss 0.0003 (0.0019)	
training:	Epoch: [142][148/233]	Loss 0.0009 (0.0019)	
training:	Epoch: [142][149/233]	Loss 0.0013 (0.0019)	
training:	Epoch: [142][150/233]	Loss 0.0003 (0.0019)	
training:	Epoch: [142][151/233]	Loss 0.0003 (0.0019)	
training:	Epoch: [142][152/233]	Loss 0.0005 (0.0019)	
training:	Epoch: [142][153/233]	Loss 0.0004 (0.0019)	
training:	Epoch: [142][154/233]	Loss 0.0006 (0.0019)	
training:	Epoch: [142][155/233]	Loss 0.0003 (0.0019)	
training:	Epoch: [142][156/233]	Loss 0.0003 (0.0019)	
training:	Epoch: [142][157/233]	Loss 0.0004 (0.0019)	
training:	Epoch: [142][158/233]	Loss 0.0003 (0.0018)	
training:	Epoch: [142][159/233]	Loss 0.0004 (0.0018)	
training:	Epoch: [142][160/233]	Loss 0.0007 (0.0018)	
training:	Epoch: [142][161/233]	Loss 0.0005 (0.0018)	
training:	Epoch: [142][162/233]	Loss 0.0005 (0.0018)	
training:	Epoch: [142][163/233]	Loss 0.0003 (0.0018)	
training:	Epoch: [142][164/233]	Loss 0.0032 (0.0018)	
training:	Epoch: [142][165/233]	Loss 0.0005 (0.0018)	
training:	Epoch: [142][166/233]	Loss 0.0004 (0.0018)	
training:	Epoch: [142][167/233]	Loss 0.0006 (0.0018)	
training:	Epoch: [142][168/233]	Loss 0.0008 (0.0018)	
training:	Epoch: [142][169/233]	Loss 0.0003 (0.0018)	
training:	Epoch: [142][170/233]	Loss 0.0003 (0.0018)	
training:	Epoch: [142][171/233]	Loss 0.0003 (0.0018)	
training:	Epoch: [142][172/233]	Loss 0.0013 (0.0018)	
training:	Epoch: [142][173/233]	Loss 0.0004 (0.0017)	
training:	Epoch: [142][174/233]	Loss 0.2113 (0.0030)	
training:	Epoch: [142][175/233]	Loss 0.0003 (0.0029)	
training:	Epoch: [142][176/233]	Loss 0.0007 (0.0029)	
training:	Epoch: [142][177/233]	Loss 0.0003 (0.0029)	
training:	Epoch: [142][178/233]	Loss 0.0004 (0.0029)	
training:	Epoch: [142][179/233]	Loss 0.0009 (0.0029)	
training:	Epoch: [142][180/233]	Loss 0.0366 (0.0031)	
training:	Epoch: [142][181/233]	Loss 0.0003 (0.0031)	
training:	Epoch: [142][182/233]	Loss 0.0013 (0.0030)	
training:	Epoch: [142][183/233]	Loss 0.0003 (0.0030)	
training:	Epoch: [142][184/233]	Loss 0.0010 (0.0030)	
training:	Epoch: [142][185/233]	Loss 0.0003 (0.0030)	
training:	Epoch: [142][186/233]	Loss 0.0005 (0.0030)	
training:	Epoch: [142][187/233]	Loss 0.0004 (0.0030)	
training:	Epoch: [142][188/233]	Loss 0.0013 (0.0030)	
training:	Epoch: [142][189/233]	Loss 0.0005 (0.0030)	
training:	Epoch: [142][190/233]	Loss 0.0003 (0.0029)	
training:	Epoch: [142][191/233]	Loss 0.0003 (0.0029)	
training:	Epoch: [142][192/233]	Loss 0.0003 (0.0029)	
training:	Epoch: [142][193/233]	Loss 0.0027 (0.0029)	
training:	Epoch: [142][194/233]	Loss 0.0003 (0.0029)	
training:	Epoch: [142][195/233]	Loss 0.0100 (0.0029)	
training:	Epoch: [142][196/233]	Loss 0.0003 (0.0029)	
training:	Epoch: [142][197/233]	Loss 0.0014 (0.0029)	
training:	Epoch: [142][198/233]	Loss 0.0004 (0.0029)	
training:	Epoch: [142][199/233]	Loss 0.0004 (0.0029)	
training:	Epoch: [142][200/233]	Loss 0.0003 (0.0029)	
training:	Epoch: [142][201/233]	Loss 0.0005 (0.0029)	
training:	Epoch: [142][202/233]	Loss 0.0004 (0.0029)	
training:	Epoch: [142][203/233]	Loss 0.0003 (0.0028)	
training:	Epoch: [142][204/233]	Loss 0.0010 (0.0028)	
training:	Epoch: [142][205/233]	Loss 0.0003 (0.0028)	
training:	Epoch: [142][206/233]	Loss 0.0006 (0.0028)	
training:	Epoch: [142][207/233]	Loss 0.0003 (0.0028)	
training:	Epoch: [142][208/233]	Loss 0.0004 (0.0028)	
training:	Epoch: [142][209/233]	Loss 0.0061 (0.0028)	
training:	Epoch: [142][210/233]	Loss 0.0007 (0.0028)	
training:	Epoch: [142][211/233]	Loss 0.0065 (0.0028)	
training:	Epoch: [142][212/233]	Loss 0.0003 (0.0028)	
training:	Epoch: [142][213/233]	Loss 0.0003 (0.0028)	
training:	Epoch: [142][214/233]	Loss 0.0004 (0.0028)	
training:	Epoch: [142][215/233]	Loss 0.0011 (0.0028)	
training:	Epoch: [142][216/233]	Loss 0.0003 (0.0028)	
training:	Epoch: [142][217/233]	Loss 0.0004 (0.0027)	
training:	Epoch: [142][218/233]	Loss 0.0003 (0.0027)	
training:	Epoch: [142][219/233]	Loss 0.0004 (0.0027)	
training:	Epoch: [142][220/233]	Loss 0.0004 (0.0027)	
training:	Epoch: [142][221/233]	Loss 0.0003 (0.0027)	
training:	Epoch: [142][222/233]	Loss 0.0010 (0.0027)	
training:	Epoch: [142][223/233]	Loss 0.0005 (0.0027)	
training:	Epoch: [142][224/233]	Loss 0.0003 (0.0027)	
training:	Epoch: [142][225/233]	Loss 0.0003 (0.0027)	
training:	Epoch: [142][226/233]	Loss 0.0003 (0.0027)	
training:	Epoch: [142][227/233]	Loss 0.0005 (0.0026)	
training:	Epoch: [142][228/233]	Loss 0.0004 (0.0026)	
training:	Epoch: [142][229/233]	Loss 0.0003 (0.0026)	
training:	Epoch: [142][230/233]	Loss 0.0019 (0.0026)	
training:	Epoch: [142][231/233]	Loss 0.0004 (0.0026)	
training:	Epoch: [142][232/233]	Loss 0.0003 (0.0026)	
training:	Epoch: [142][233/233]	Loss 0.0004 (0.0026)	
Training:	 Loss: 0.0026

Training:	 ACC: 1.0000 1.0000 1.0000 1.0000
Validation:	 ACC: 0.8005 0.7999 0.7886 0.8124
Validation:	 Best_BACC: 0.8162 0.8154 0.7998 0.8326
Validation:	 Loss: 1.2106
Pretraining:	Epoch 143/200
----------
training:	Epoch: [143][1/233]	Loss 0.0003 (0.0003)	
training:	Epoch: [143][2/233]	Loss 0.0003 (0.0003)	
training:	Epoch: [143][3/233]	Loss 0.0004 (0.0003)	
training:	Epoch: [143][4/233]	Loss 0.0011 (0.0005)	
training:	Epoch: [143][5/233]	Loss 0.0003 (0.0005)	
training:	Epoch: [143][6/233]	Loss 0.0047 (0.0012)	
training:	Epoch: [143][7/233]	Loss 0.0003 (0.0011)	
training:	Epoch: [143][8/233]	Loss 0.0004 (0.0010)	
training:	Epoch: [143][9/233]	Loss 0.0003 (0.0009)	
training:	Epoch: [143][10/233]	Loss 0.0008 (0.0009)	
training:	Epoch: [143][11/233]	Loss 0.0003 (0.0008)	
training:	Epoch: [143][12/233]	Loss 0.0004 (0.0008)	
training:	Epoch: [143][13/233]	Loss 0.0006 (0.0008)	
training:	Epoch: [143][14/233]	Loss 0.0004 (0.0007)	
training:	Epoch: [143][15/233]	Loss 0.2490 (0.0173)	
training:	Epoch: [143][16/233]	Loss 0.0007 (0.0163)	
training:	Epoch: [143][17/233]	Loss 0.0003 (0.0153)	
training:	Epoch: [143][18/233]	Loss 0.0003 (0.0145)	
training:	Epoch: [143][19/233]	Loss 0.0006 (0.0138)	
training:	Epoch: [143][20/233]	Loss 0.0006 (0.0131)	
training:	Epoch: [143][21/233]	Loss 0.0006 (0.0125)	
training:	Epoch: [143][22/233]	Loss 0.0003 (0.0119)	
training:	Epoch: [143][23/233]	Loss 0.0003 (0.0114)	
training:	Epoch: [143][24/233]	Loss 0.0012 (0.0110)	
training:	Epoch: [143][25/233]	Loss 0.0003 (0.0106)	
training:	Epoch: [143][26/233]	Loss 0.0008 (0.0102)	
training:	Epoch: [143][27/233]	Loss 0.0003 (0.0098)	
training:	Epoch: [143][28/233]	Loss 0.0006 (0.0095)	
training:	Epoch: [143][29/233]	Loss 0.0004 (0.0092)	
training:	Epoch: [143][30/233]	Loss 0.0010 (0.0089)	
training:	Epoch: [143][31/233]	Loss 0.0004 (0.0086)	
training:	Epoch: [143][32/233]	Loss 0.0006 (0.0084)	
training:	Epoch: [143][33/233]	Loss 0.0004 (0.0082)	
training:	Epoch: [143][34/233]	Loss 0.0003 (0.0079)	
training:	Epoch: [143][35/233]	Loss 0.0004 (0.0077)	
training:	Epoch: [143][36/233]	Loss 0.0004 (0.0075)	
training:	Epoch: [143][37/233]	Loss 0.0014 (0.0073)	
training:	Epoch: [143][38/233]	Loss 0.0003 (0.0072)	
training:	Epoch: [143][39/233]	Loss 0.0004 (0.0070)	
training:	Epoch: [143][40/233]	Loss 0.0006 (0.0068)	
training:	Epoch: [143][41/233]	Loss 0.0007 (0.0067)	
training:	Epoch: [143][42/233]	Loss 0.0005 (0.0065)	
training:	Epoch: [143][43/233]	Loss 0.0003 (0.0064)	
training:	Epoch: [143][44/233]	Loss 0.0003 (0.0062)	
training:	Epoch: [143][45/233]	Loss 0.0005 (0.0061)	
training:	Epoch: [143][46/233]	Loss 0.0005 (0.0060)	
training:	Epoch: [143][47/233]	Loss 0.0004 (0.0059)	
training:	Epoch: [143][48/233]	Loss 0.0007 (0.0058)	
training:	Epoch: [143][49/233]	Loss 0.0044 (0.0057)	
training:	Epoch: [143][50/233]	Loss 0.0004 (0.0056)	
training:	Epoch: [143][51/233]	Loss 0.0011 (0.0055)	
training:	Epoch: [143][52/233]	Loss 0.0003 (0.0054)	
training:	Epoch: [143][53/233]	Loss 0.0005 (0.0053)	
training:	Epoch: [143][54/233]	Loss 0.0012 (0.0053)	
training:	Epoch: [143][55/233]	Loss 0.0003 (0.0052)	
training:	Epoch: [143][56/233]	Loss 0.0009 (0.0051)	
training:	Epoch: [143][57/233]	Loss 0.0008 (0.0050)	
training:	Epoch: [143][58/233]	Loss 0.0053 (0.0050)	
training:	Epoch: [143][59/233]	Loss 0.0004 (0.0050)	
training:	Epoch: [143][60/233]	Loss 0.0008 (0.0049)	
training:	Epoch: [143][61/233]	Loss 0.0003 (0.0048)	
training:	Epoch: [143][62/233]	Loss 0.0003 (0.0047)	
training:	Epoch: [143][63/233]	Loss 0.0004 (0.0047)	
training:	Epoch: [143][64/233]	Loss 0.0003 (0.0046)	
training:	Epoch: [143][65/233]	Loss 0.0003 (0.0045)	
training:	Epoch: [143][66/233]	Loss 0.0003 (0.0045)	
training:	Epoch: [143][67/233]	Loss 0.0004 (0.0044)	
training:	Epoch: [143][68/233]	Loss 0.0005 (0.0044)	
training:	Epoch: [143][69/233]	Loss 0.0006 (0.0043)	
training:	Epoch: [143][70/233]	Loss 0.0003 (0.0042)	
training:	Epoch: [143][71/233]	Loss 0.0029 (0.0042)	
training:	Epoch: [143][72/233]	Loss 0.0004 (0.0042)	
training:	Epoch: [143][73/233]	Loss 0.0004 (0.0041)	
training:	Epoch: [143][74/233]	Loss 0.0004 (0.0041)	
training:	Epoch: [143][75/233]	Loss 0.0004 (0.0040)	
training:	Epoch: [143][76/233]	Loss 0.0003 (0.0040)	
training:	Epoch: [143][77/233]	Loss 0.0003 (0.0039)	
training:	Epoch: [143][78/233]	Loss 0.0302 (0.0043)	
training:	Epoch: [143][79/233]	Loss 0.0003 (0.0042)	
training:	Epoch: [143][80/233]	Loss 0.0005 (0.0042)	
training:	Epoch: [143][81/233]	Loss 0.0005 (0.0041)	
training:	Epoch: [143][82/233]	Loss 0.0004 (0.0041)	
training:	Epoch: [143][83/233]	Loss 0.0005 (0.0040)	
training:	Epoch: [143][84/233]	Loss 0.0003 (0.0040)	
training:	Epoch: [143][85/233]	Loss 0.0008 (0.0039)	
training:	Epoch: [143][86/233]	Loss 0.0005 (0.0039)	
training:	Epoch: [143][87/233]	Loss 0.0004 (0.0039)	
training:	Epoch: [143][88/233]	Loss 0.0023 (0.0038)	
training:	Epoch: [143][89/233]	Loss 0.0007 (0.0038)	
training:	Epoch: [143][90/233]	Loss 0.0004 (0.0038)	
training:	Epoch: [143][91/233]	Loss 0.1346 (0.0052)	
training:	Epoch: [143][92/233]	Loss 0.0003 (0.0052)	
training:	Epoch: [143][93/233]	Loss 0.0014 (0.0051)	
training:	Epoch: [143][94/233]	Loss 0.0003 (0.0051)	
training:	Epoch: [143][95/233]	Loss 0.0003 (0.0050)	
training:	Epoch: [143][96/233]	Loss 0.0003 (0.0050)	
training:	Epoch: [143][97/233]	Loss 0.0005 (0.0049)	
training:	Epoch: [143][98/233]	Loss 0.0004 (0.0049)	
training:	Epoch: [143][99/233]	Loss 0.0003 (0.0048)	
training:	Epoch: [143][100/233]	Loss 0.0003 (0.0048)	
training:	Epoch: [143][101/233]	Loss 0.0003 (0.0047)	
training:	Epoch: [143][102/233]	Loss 0.0008 (0.0047)	
training:	Epoch: [143][103/233]	Loss 0.0003 (0.0047)	
training:	Epoch: [143][104/233]	Loss 0.0004 (0.0046)	
training:	Epoch: [143][105/233]	Loss 0.0004 (0.0046)	
training:	Epoch: [143][106/233]	Loss 0.0006 (0.0045)	
training:	Epoch: [143][107/233]	Loss 0.0003 (0.0045)	
training:	Epoch: [143][108/233]	Loss 0.0014 (0.0045)	
training:	Epoch: [143][109/233]	Loss 0.0005 (0.0044)	
training:	Epoch: [143][110/233]	Loss 0.0003 (0.0044)	
training:	Epoch: [143][111/233]	Loss 0.0004 (0.0044)	
training:	Epoch: [143][112/233]	Loss 0.0158 (0.0045)	
training:	Epoch: [143][113/233]	Loss 0.0130 (0.0045)	
training:	Epoch: [143][114/233]	Loss 0.0145 (0.0046)	
training:	Epoch: [143][115/233]	Loss 0.0006 (0.0046)	
training:	Epoch: [143][116/233]	Loss 0.0007 (0.0046)	
training:	Epoch: [143][117/233]	Loss 0.0003 (0.0045)	
training:	Epoch: [143][118/233]	Loss 0.0004 (0.0045)	
training:	Epoch: [143][119/233]	Loss 0.0004 (0.0045)	
training:	Epoch: [143][120/233]	Loss 0.0003 (0.0044)	
training:	Epoch: [143][121/233]	Loss 0.0005 (0.0044)	
training:	Epoch: [143][122/233]	Loss 0.0005 (0.0044)	
training:	Epoch: [143][123/233]	Loss 0.0005 (0.0043)	
training:	Epoch: [143][124/233]	Loss 0.0003 (0.0043)	
training:	Epoch: [143][125/233]	Loss 0.0004 (0.0043)	
training:	Epoch: [143][126/233]	Loss 0.0003 (0.0042)	
training:	Epoch: [143][127/233]	Loss 0.0005 (0.0042)	
training:	Epoch: [143][128/233]	Loss 0.0009 (0.0042)	
training:	Epoch: [143][129/233]	Loss 0.0007 (0.0041)	
training:	Epoch: [143][130/233]	Loss 0.0003 (0.0041)	
training:	Epoch: [143][131/233]	Loss 0.0003 (0.0041)	
training:	Epoch: [143][132/233]	Loss 0.0007 (0.0041)	
training:	Epoch: [143][133/233]	Loss 0.0010 (0.0040)	
training:	Epoch: [143][134/233]	Loss 0.0004 (0.0040)	
training:	Epoch: [143][135/233]	Loss 0.0005 (0.0040)	
training:	Epoch: [143][136/233]	Loss 0.0003 (0.0040)	
training:	Epoch: [143][137/233]	Loss 0.0003 (0.0039)	
training:	Epoch: [143][138/233]	Loss 0.0006 (0.0039)	
training:	Epoch: [143][139/233]	Loss 0.0003 (0.0039)	
training:	Epoch: [143][140/233]	Loss 0.0037 (0.0039)	
training:	Epoch: [143][141/233]	Loss 0.0003 (0.0039)	
training:	Epoch: [143][142/233]	Loss 0.0003 (0.0038)	
training:	Epoch: [143][143/233]	Loss 0.0004 (0.0038)	
training:	Epoch: [143][144/233]	Loss 0.0158 (0.0039)	
training:	Epoch: [143][145/233]	Loss 0.0004 (0.0039)	
training:	Epoch: [143][146/233]	Loss 0.0014 (0.0038)	
training:	Epoch: [143][147/233]	Loss 0.0003 (0.0038)	
training:	Epoch: [143][148/233]	Loss 0.0003 (0.0038)	
training:	Epoch: [143][149/233]	Loss 0.0004 (0.0038)	
training:	Epoch: [143][150/233]	Loss 0.0006 (0.0038)	
training:	Epoch: [143][151/233]	Loss 0.0005 (0.0037)	
training:	Epoch: [143][152/233]	Loss 0.0003 (0.0037)	
training:	Epoch: [143][153/233]	Loss 0.0023 (0.0037)	
training:	Epoch: [143][154/233]	Loss 0.0004 (0.0037)	
training:	Epoch: [143][155/233]	Loss 0.0003 (0.0037)	
training:	Epoch: [143][156/233]	Loss 0.0005 (0.0036)	
training:	Epoch: [143][157/233]	Loss 0.0038 (0.0036)	
training:	Epoch: [143][158/233]	Loss 0.0003 (0.0036)	
training:	Epoch: [143][159/233]	Loss 0.0005 (0.0036)	
training:	Epoch: [143][160/233]	Loss 0.0004 (0.0036)	
training:	Epoch: [143][161/233]	Loss 0.0005 (0.0036)	
training:	Epoch: [143][162/233]	Loss 0.0004 (0.0035)	
training:	Epoch: [143][163/233]	Loss 0.0004 (0.0035)	
training:	Epoch: [143][164/233]	Loss 0.0003 (0.0035)	
training:	Epoch: [143][165/233]	Loss 0.0031 (0.0035)	
training:	Epoch: [143][166/233]	Loss 0.0003 (0.0035)	
training:	Epoch: [143][167/233]	Loss 0.0003 (0.0035)	
training:	Epoch: [143][168/233]	Loss 0.0003 (0.0034)	
training:	Epoch: [143][169/233]	Loss 0.0005 (0.0034)	
training:	Epoch: [143][170/233]	Loss 0.0004 (0.0034)	
training:	Epoch: [143][171/233]	Loss 0.0004 (0.0034)	
training:	Epoch: [143][172/233]	Loss 0.0027 (0.0034)	
training:	Epoch: [143][173/233]	Loss 0.1199 (0.0041)	
training:	Epoch: [143][174/233]	Loss 0.0003 (0.0040)	
training:	Epoch: [143][175/233]	Loss 0.0005 (0.0040)	
training:	Epoch: [143][176/233]	Loss 0.0005 (0.0040)	
training:	Epoch: [143][177/233]	Loss 0.0003 (0.0040)	
training:	Epoch: [143][178/233]	Loss 0.0004 (0.0040)	
training:	Epoch: [143][179/233]	Loss 0.0005 (0.0039)	
training:	Epoch: [143][180/233]	Loss 0.0007 (0.0039)	
training:	Epoch: [143][181/233]	Loss 0.0007 (0.0039)	
training:	Epoch: [143][182/233]	Loss 0.0005 (0.0039)	
training:	Epoch: [143][183/233]	Loss 0.0005 (0.0039)	
training:	Epoch: [143][184/233]	Loss 0.0012 (0.0038)	
training:	Epoch: [143][185/233]	Loss 0.0006 (0.0038)	
training:	Epoch: [143][186/233]	Loss 0.0012 (0.0038)	
training:	Epoch: [143][187/233]	Loss 0.0429 (0.0040)	
training:	Epoch: [143][188/233]	Loss 0.0200 (0.0041)	
training:	Epoch: [143][189/233]	Loss 0.0004 (0.0041)	
training:	Epoch: [143][190/233]	Loss 0.0004 (0.0041)	
training:	Epoch: [143][191/233]	Loss 0.0572 (0.0044)	
training:	Epoch: [143][192/233]	Loss 0.0007 (0.0043)	
training:	Epoch: [143][193/233]	Loss 0.0003 (0.0043)	
training:	Epoch: [143][194/233]	Loss 0.0112 (0.0043)	
training:	Epoch: [143][195/233]	Loss 0.0004 (0.0043)	
training:	Epoch: [143][196/233]	Loss 0.0003 (0.0043)	
training:	Epoch: [143][197/233]	Loss 0.0003 (0.0043)	
training:	Epoch: [143][198/233]	Loss 0.0058 (0.0043)	
training:	Epoch: [143][199/233]	Loss 0.0003 (0.0043)	
training:	Epoch: [143][200/233]	Loss 0.0029 (0.0043)	
training:	Epoch: [143][201/233]	Loss 0.0017 (0.0043)	
training:	Epoch: [143][202/233]	Loss 0.0215 (0.0043)	
training:	Epoch: [143][203/233]	Loss 0.0141 (0.0044)	
training:	Epoch: [143][204/233]	Loss 0.0020 (0.0044)	
training:	Epoch: [143][205/233]	Loss 0.0005 (0.0044)	
training:	Epoch: [143][206/233]	Loss 0.0004 (0.0043)	
training:	Epoch: [143][207/233]	Loss 0.0005 (0.0043)	
training:	Epoch: [143][208/233]	Loss 0.0003 (0.0043)	
training:	Epoch: [143][209/233]	Loss 0.0009 (0.0043)	
training:	Epoch: [143][210/233]	Loss 0.0003 (0.0043)	
training:	Epoch: [143][211/233]	Loss 0.0003 (0.0042)	
training:	Epoch: [143][212/233]	Loss 0.0004 (0.0042)	
training:	Epoch: [143][213/233]	Loss 0.0003 (0.0042)	
training:	Epoch: [143][214/233]	Loss 0.0018 (0.0042)	
training:	Epoch: [143][215/233]	Loss 0.0005 (0.0042)	
training:	Epoch: [143][216/233]	Loss 0.0030 (0.0042)	
training:	Epoch: [143][217/233]	Loss 0.0031 (0.0042)	
training:	Epoch: [143][218/233]	Loss 0.0130 (0.0042)	
training:	Epoch: [143][219/233]	Loss 0.0003 (0.0042)	
training:	Epoch: [143][220/233]	Loss 0.0003 (0.0042)	
training:	Epoch: [143][221/233]	Loss 0.0003 (0.0042)	
training:	Epoch: [143][222/233]	Loss 0.0045 (0.0042)	
training:	Epoch: [143][223/233]	Loss 0.0006 (0.0041)	
training:	Epoch: [143][224/233]	Loss 0.0008 (0.0041)	
training:	Epoch: [143][225/233]	Loss 0.0003 (0.0041)	
training:	Epoch: [143][226/233]	Loss 0.0004 (0.0041)	
training:	Epoch: [143][227/233]	Loss 0.0007 (0.0041)	
training:	Epoch: [143][228/233]	Loss 0.0003 (0.0041)	
training:	Epoch: [143][229/233]	Loss 0.0007 (0.0040)	
training:	Epoch: [143][230/233]	Loss 0.0016 (0.0040)	
training:	Epoch: [143][231/233]	Loss 0.0003 (0.0040)	
training:	Epoch: [143][232/233]	Loss 0.0006 (0.0040)	
training:	Epoch: [143][233/233]	Loss 0.0004 (0.0040)	
Training:	 Loss: 0.0040

Training:	 ACC: 1.0000 1.0000 1.0000 1.0000
Validation:	 ACC: 0.7959 0.7945 0.7671 0.8247
Validation:	 Best_BACC: 0.8162 0.8154 0.7998 0.8326
Validation:	 Loss: 1.2339
Pretraining:	Epoch 144/200
----------
training:	Epoch: [144][1/233]	Loss 0.0004 (0.0004)	
training:	Epoch: [144][2/233]	Loss 0.0003 (0.0003)	
training:	Epoch: [144][3/233]	Loss 0.0003 (0.0003)	
training:	Epoch: [144][4/233]	Loss 0.0018 (0.0007)	
training:	Epoch: [144][5/233]	Loss 0.0004 (0.0006)	
training:	Epoch: [144][6/233]	Loss 0.0003 (0.0006)	
training:	Epoch: [144][7/233]	Loss 0.0005 (0.0006)	
training:	Epoch: [144][8/233]	Loss 0.0015 (0.0007)	
training:	Epoch: [144][9/233]	Loss 0.0004 (0.0007)	
training:	Epoch: [144][10/233]	Loss 0.0004 (0.0006)	
training:	Epoch: [144][11/233]	Loss 0.0003 (0.0006)	
training:	Epoch: [144][12/233]	Loss 0.0003 (0.0006)	
training:	Epoch: [144][13/233]	Loss 0.0003 (0.0006)	
training:	Epoch: [144][14/233]	Loss 0.0018 (0.0006)	
training:	Epoch: [144][15/233]	Loss 0.0003 (0.0006)	
training:	Epoch: [144][16/233]	Loss 0.0004 (0.0006)	
training:	Epoch: [144][17/233]	Loss 0.0004 (0.0006)	
training:	Epoch: [144][18/233]	Loss 0.0005 (0.0006)	
training:	Epoch: [144][19/233]	Loss 0.0026 (0.0007)	
training:	Epoch: [144][20/233]	Loss 0.0036 (0.0008)	
training:	Epoch: [144][21/233]	Loss 0.0004 (0.0008)	
training:	Epoch: [144][22/233]	Loss 0.0456 (0.0028)	
training:	Epoch: [144][23/233]	Loss 0.0069 (0.0030)	
training:	Epoch: [144][24/233]	Loss 0.0005 (0.0029)	
training:	Epoch: [144][25/233]	Loss 0.0004 (0.0028)	
training:	Epoch: [144][26/233]	Loss 0.0003 (0.0027)	
training:	Epoch: [144][27/233]	Loss 0.0004 (0.0026)	
training:	Epoch: [144][28/233]	Loss 0.0004 (0.0026)	
training:	Epoch: [144][29/233]	Loss 0.0003 (0.0025)	
training:	Epoch: [144][30/233]	Loss 0.0004 (0.0024)	
training:	Epoch: [144][31/233]	Loss 0.0004 (0.0023)	
training:	Epoch: [144][32/233]	Loss 0.0004 (0.0023)	
training:	Epoch: [144][33/233]	Loss 0.0004 (0.0022)	
training:	Epoch: [144][34/233]	Loss 0.0185 (0.0027)	
training:	Epoch: [144][35/233]	Loss 0.0004 (0.0026)	
training:	Epoch: [144][36/233]	Loss 0.0003 (0.0026)	
training:	Epoch: [144][37/233]	Loss 0.0005 (0.0025)	
training:	Epoch: [144][38/233]	Loss 0.0003 (0.0025)	
training:	Epoch: [144][39/233]	Loss 0.0005 (0.0024)	
training:	Epoch: [144][40/233]	Loss 0.0004 (0.0024)	
training:	Epoch: [144][41/233]	Loss 0.0003 (0.0023)	
training:	Epoch: [144][42/233]	Loss 0.0003 (0.0023)	
training:	Epoch: [144][43/233]	Loss 0.0003 (0.0022)	
training:	Epoch: [144][44/233]	Loss 0.0027 (0.0022)	
training:	Epoch: [144][45/233]	Loss 0.0003 (0.0022)	
training:	Epoch: [144][46/233]	Loss 0.0003 (0.0021)	
training:	Epoch: [144][47/233]	Loss 0.0003 (0.0021)	
training:	Epoch: [144][48/233]	Loss 0.0005 (0.0021)	
training:	Epoch: [144][49/233]	Loss 0.0003 (0.0020)	
training:	Epoch: [144][50/233]	Loss 0.0005 (0.0020)	
training:	Epoch: [144][51/233]	Loss 0.0013 (0.0020)	
training:	Epoch: [144][52/233]	Loss 0.0003 (0.0020)	
training:	Epoch: [144][53/233]	Loss 0.0004 (0.0019)	
training:	Epoch: [144][54/233]	Loss 0.0010 (0.0019)	
training:	Epoch: [144][55/233]	Loss 0.0003 (0.0019)	
training:	Epoch: [144][56/233]	Loss 0.0007 (0.0019)	
training:	Epoch: [144][57/233]	Loss 0.0578 (0.0028)	
training:	Epoch: [144][58/233]	Loss 0.0003 (0.0028)	
training:	Epoch: [144][59/233]	Loss 0.0007 (0.0028)	
training:	Epoch: [144][60/233]	Loss 0.0003 (0.0027)	
training:	Epoch: [144][61/233]	Loss 0.0003 (0.0027)	
training:	Epoch: [144][62/233]	Loss 0.0003 (0.0026)	
training:	Epoch: [144][63/233]	Loss 0.0009 (0.0026)	
training:	Epoch: [144][64/233]	Loss 0.0005 (0.0026)	
training:	Epoch: [144][65/233]	Loss 0.0004 (0.0026)	
training:	Epoch: [144][66/233]	Loss 0.0004 (0.0025)	
training:	Epoch: [144][67/233]	Loss 0.0003 (0.0025)	
training:	Epoch: [144][68/233]	Loss 0.0003 (0.0025)	
training:	Epoch: [144][69/233]	Loss 0.0003 (0.0024)	
training:	Epoch: [144][70/233]	Loss 0.0003 (0.0024)	
training:	Epoch: [144][71/233]	Loss 0.0003 (0.0024)	
training:	Epoch: [144][72/233]	Loss 0.0003 (0.0023)	
training:	Epoch: [144][73/233]	Loss 0.0005 (0.0023)	
training:	Epoch: [144][74/233]	Loss 0.0004 (0.0023)	
training:	Epoch: [144][75/233]	Loss 0.0003 (0.0023)	
training:	Epoch: [144][76/233]	Loss 0.0003 (0.0022)	
training:	Epoch: [144][77/233]	Loss 0.0003 (0.0022)	
training:	Epoch: [144][78/233]	Loss 0.0003 (0.0022)	
training:	Epoch: [144][79/233]	Loss 0.0004 (0.0022)	
training:	Epoch: [144][80/233]	Loss 0.0004 (0.0021)	
training:	Epoch: [144][81/233]	Loss 0.0004 (0.0021)	
training:	Epoch: [144][82/233]	Loss 0.0005 (0.0021)	
training:	Epoch: [144][83/233]	Loss 0.0003 (0.0021)	
training:	Epoch: [144][84/233]	Loss 0.0037 (0.0021)	
training:	Epoch: [144][85/233]	Loss 0.0003 (0.0021)	
training:	Epoch: [144][86/233]	Loss 0.0004 (0.0021)	
training:	Epoch: [144][87/233]	Loss 0.0004 (0.0020)	
training:	Epoch: [144][88/233]	Loss 0.0003 (0.0020)	
training:	Epoch: [144][89/233]	Loss 0.0003 (0.0020)	
training:	Epoch: [144][90/233]	Loss 0.0003 (0.0020)	
training:	Epoch: [144][91/233]	Loss 0.0004 (0.0020)	
training:	Epoch: [144][92/233]	Loss 0.0319 (0.0023)	
training:	Epoch: [144][93/233]	Loss 0.0003 (0.0023)	
training:	Epoch: [144][94/233]	Loss 0.0004 (0.0022)	
training:	Epoch: [144][95/233]	Loss 0.0003 (0.0022)	
training:	Epoch: [144][96/233]	Loss 0.0616 (0.0028)	
training:	Epoch: [144][97/233]	Loss 0.0003 (0.0028)	
training:	Epoch: [144][98/233]	Loss 0.0003 (0.0028)	
training:	Epoch: [144][99/233]	Loss 0.0003 (0.0028)	
training:	Epoch: [144][100/233]	Loss 0.0003 (0.0027)	
training:	Epoch: [144][101/233]	Loss 0.0004 (0.0027)	
training:	Epoch: [144][102/233]	Loss 0.0007 (0.0027)	
training:	Epoch: [144][103/233]	Loss 0.0005 (0.0027)	
training:	Epoch: [144][104/233]	Loss 0.0003 (0.0027)	
training:	Epoch: [144][105/233]	Loss 0.0003 (0.0026)	
training:	Epoch: [144][106/233]	Loss 0.0003 (0.0026)	
training:	Epoch: [144][107/233]	Loss 0.0082 (0.0027)	
training:	Epoch: [144][108/233]	Loss 0.0005 (0.0026)	
training:	Epoch: [144][109/233]	Loss 0.0014 (0.0026)	
training:	Epoch: [144][110/233]	Loss 0.0005 (0.0026)	
training:	Epoch: [144][111/233]	Loss 0.0003 (0.0026)	
training:	Epoch: [144][112/233]	Loss 0.0004 (0.0026)	
training:	Epoch: [144][113/233]	Loss 0.0008 (0.0026)	
training:	Epoch: [144][114/233]	Loss 0.0016 (0.0025)	
training:	Epoch: [144][115/233]	Loss 0.0006 (0.0025)	
training:	Epoch: [144][116/233]	Loss 0.0004 (0.0025)	
training:	Epoch: [144][117/233]	Loss 0.0005 (0.0025)	
training:	Epoch: [144][118/233]	Loss 0.0004 (0.0025)	
training:	Epoch: [144][119/233]	Loss 0.0004 (0.0025)	
training:	Epoch: [144][120/233]	Loss 0.0005 (0.0024)	
training:	Epoch: [144][121/233]	Loss 0.0004 (0.0024)	
training:	Epoch: [144][122/233]	Loss 0.0004 (0.0024)	
training:	Epoch: [144][123/233]	Loss 0.0042 (0.0024)	
training:	Epoch: [144][124/233]	Loss 0.0003 (0.0024)	
training:	Epoch: [144][125/233]	Loss 0.0004 (0.0024)	
training:	Epoch: [144][126/233]	Loss 0.0010 (0.0024)	
training:	Epoch: [144][127/233]	Loss 0.0015 (0.0024)	
training:	Epoch: [144][128/233]	Loss 0.0003 (0.0024)	
training:	Epoch: [144][129/233]	Loss 0.0003 (0.0023)	
training:	Epoch: [144][130/233]	Loss 0.0003 (0.0023)	
training:	Epoch: [144][131/233]	Loss 0.0003 (0.0023)	
training:	Epoch: [144][132/233]	Loss 0.0009 (0.0023)	
training:	Epoch: [144][133/233]	Loss 0.0003 (0.0023)	
training:	Epoch: [144][134/233]	Loss 0.0004 (0.0023)	
training:	Epoch: [144][135/233]	Loss 0.0005 (0.0023)	
training:	Epoch: [144][136/233]	Loss 0.0003 (0.0022)	
training:	Epoch: [144][137/233]	Loss 0.0004 (0.0022)	
training:	Epoch: [144][138/233]	Loss 0.0003 (0.0022)	
training:	Epoch: [144][139/233]	Loss 0.0014 (0.0022)	
training:	Epoch: [144][140/233]	Loss 0.0003 (0.0022)	
training:	Epoch: [144][141/233]	Loss 0.0003 (0.0022)	
training:	Epoch: [144][142/233]	Loss 0.0004 (0.0022)	
training:	Epoch: [144][143/233]	Loss 0.0004 (0.0022)	
training:	Epoch: [144][144/233]	Loss 0.0003 (0.0021)	
training:	Epoch: [144][145/233]	Loss 0.0004 (0.0021)	
training:	Epoch: [144][146/233]	Loss 0.0003 (0.0021)	
training:	Epoch: [144][147/233]	Loss 0.1143 (0.0029)	
training:	Epoch: [144][148/233]	Loss 0.0003 (0.0029)	
training:	Epoch: [144][149/233]	Loss 0.1025 (0.0035)	
training:	Epoch: [144][150/233]	Loss 0.0006 (0.0035)	
training:	Epoch: [144][151/233]	Loss 0.0598 (0.0039)	
training:	Epoch: [144][152/233]	Loss 0.0009 (0.0039)	
training:	Epoch: [144][153/233]	Loss 0.0003 (0.0038)	
training:	Epoch: [144][154/233]	Loss 0.0004 (0.0038)	
training:	Epoch: [144][155/233]	Loss 0.0003 (0.0038)	
training:	Epoch: [144][156/233]	Loss 0.0004 (0.0038)	
training:	Epoch: [144][157/233]	Loss 0.0003 (0.0038)	
training:	Epoch: [144][158/233]	Loss 0.0004 (0.0037)	
training:	Epoch: [144][159/233]	Loss 0.0003 (0.0037)	
training:	Epoch: [144][160/233]	Loss 0.0004 (0.0037)	
training:	Epoch: [144][161/233]	Loss 0.0003 (0.0037)	
training:	Epoch: [144][162/233]	Loss 0.0003 (0.0036)	
training:	Epoch: [144][163/233]	Loss 0.0003 (0.0036)	
training:	Epoch: [144][164/233]	Loss 0.0005 (0.0036)	
training:	Epoch: [144][165/233]	Loss 0.0003 (0.0036)	
training:	Epoch: [144][166/233]	Loss 0.0128 (0.0036)	
training:	Epoch: [144][167/233]	Loss 0.0005 (0.0036)	
training:	Epoch: [144][168/233]	Loss 0.0003 (0.0036)	
training:	Epoch: [144][169/233]	Loss 0.0004 (0.0036)	
training:	Epoch: [144][170/233]	Loss 0.0006 (0.0036)	
training:	Epoch: [144][171/233]	Loss 0.0003 (0.0036)	
training:	Epoch: [144][172/233]	Loss 0.0004 (0.0035)	
training:	Epoch: [144][173/233]	Loss 0.0012 (0.0035)	
training:	Epoch: [144][174/233]	Loss 0.0405 (0.0037)	
training:	Epoch: [144][175/233]	Loss 0.0004 (0.0037)	
training:	Epoch: [144][176/233]	Loss 0.0003 (0.0037)	
training:	Epoch: [144][177/233]	Loss 0.0004 (0.0037)	
training:	Epoch: [144][178/233]	Loss 0.0008 (0.0037)	
training:	Epoch: [144][179/233]	Loss 0.0003 (0.0036)	
training:	Epoch: [144][180/233]	Loss 0.0004 (0.0036)	
training:	Epoch: [144][181/233]	Loss 0.0003 (0.0036)	
training:	Epoch: [144][182/233]	Loss 0.0003 (0.0036)	
training:	Epoch: [144][183/233]	Loss 0.0003 (0.0036)	
training:	Epoch: [144][184/233]	Loss 0.0003 (0.0035)	
training:	Epoch: [144][185/233]	Loss 0.0006 (0.0035)	
training:	Epoch: [144][186/233]	Loss 0.0003 (0.0035)	
training:	Epoch: [144][187/233]	Loss 0.0003 (0.0035)	
training:	Epoch: [144][188/233]	Loss 0.0015 (0.0035)	
training:	Epoch: [144][189/233]	Loss 0.0624 (0.0038)	
training:	Epoch: [144][190/233]	Loss 0.0003 (0.0038)	
training:	Epoch: [144][191/233]	Loss 0.0003 (0.0038)	
training:	Epoch: [144][192/233]	Loss 0.0004 (0.0037)	
training:	Epoch: [144][193/233]	Loss 0.0005 (0.0037)	
training:	Epoch: [144][194/233]	Loss 0.0003 (0.0037)	
training:	Epoch: [144][195/233]	Loss 0.0004 (0.0037)	
training:	Epoch: [144][196/233]	Loss 0.0013 (0.0037)	
training:	Epoch: [144][197/233]	Loss 0.0005 (0.0037)	
training:	Epoch: [144][198/233]	Loss 0.0003 (0.0036)	
training:	Epoch: [144][199/233]	Loss 0.0003 (0.0036)	
training:	Epoch: [144][200/233]	Loss 0.0004 (0.0036)	
training:	Epoch: [144][201/233]	Loss 0.0006 (0.0036)	
training:	Epoch: [144][202/233]	Loss 0.0004 (0.0036)	
training:	Epoch: [144][203/233]	Loss 0.0007 (0.0036)	
training:	Epoch: [144][204/233]	Loss 0.0003 (0.0036)	
training:	Epoch: [144][205/233]	Loss 0.0010 (0.0035)	
training:	Epoch: [144][206/233]	Loss 0.0003 (0.0035)	
training:	Epoch: [144][207/233]	Loss 0.0007 (0.0035)	
training:	Epoch: [144][208/233]	Loss 0.0004 (0.0035)	
training:	Epoch: [144][209/233]	Loss 0.0004 (0.0035)	
training:	Epoch: [144][210/233]	Loss 0.0071 (0.0035)	
training:	Epoch: [144][211/233]	Loss 0.0007 (0.0035)	
training:	Epoch: [144][212/233]	Loss 0.0003 (0.0035)	
training:	Epoch: [144][213/233]	Loss 0.0003 (0.0035)	
training:	Epoch: [144][214/233]	Loss 0.0004 (0.0034)	
training:	Epoch: [144][215/233]	Loss 0.0003 (0.0034)	
training:	Epoch: [144][216/233]	Loss 0.0003 (0.0034)	
training:	Epoch: [144][217/233]	Loss 0.0004 (0.0034)	
training:	Epoch: [144][218/233]	Loss 0.0003 (0.0034)	
training:	Epoch: [144][219/233]	Loss 0.0004 (0.0034)	
training:	Epoch: [144][220/233]	Loss 0.0010 (0.0034)	
training:	Epoch: [144][221/233]	Loss 0.0003 (0.0033)	
training:	Epoch: [144][222/233]	Loss 0.0003 (0.0033)	
training:	Epoch: [144][223/233]	Loss 0.0003 (0.0033)	
training:	Epoch: [144][224/233]	Loss 0.0004 (0.0033)	
training:	Epoch: [144][225/233]	Loss 0.0003 (0.0033)	
training:	Epoch: [144][226/233]	Loss 0.0005 (0.0033)	
training:	Epoch: [144][227/233]	Loss 0.0004 (0.0033)	
training:	Epoch: [144][228/233]	Loss 0.0004 (0.0033)	
training:	Epoch: [144][229/233]	Loss 0.0004 (0.0032)	
training:	Epoch: [144][230/233]	Loss 0.0003 (0.0032)	
training:	Epoch: [144][231/233]	Loss 0.0004 (0.0032)	
training:	Epoch: [144][232/233]	Loss 0.0013 (0.0032)	
training:	Epoch: [144][233/233]	Loss 0.0017 (0.0032)	
Training:	 Loss: 0.0032

Training:	 ACC: 1.0000 1.0000 1.0000 1.0000
Validation:	 ACC: 0.7933 0.7929 0.7855 0.8011
Validation:	 Best_BACC: 0.8162 0.8154 0.7998 0.8326
Validation:	 Loss: 1.1840
Pretraining:	Epoch 145/200
----------
training:	Epoch: [145][1/233]	Loss 0.0004 (0.0004)	
training:	Epoch: [145][2/233]	Loss 0.0003 (0.0003)	
training:	Epoch: [145][3/233]	Loss 0.0004 (0.0004)	
training:	Epoch: [145][4/233]	Loss 0.0112 (0.0031)	
training:	Epoch: [145][5/233]	Loss 0.0004 (0.0025)	
training:	Epoch: [145][6/233]	Loss 0.0005 (0.0022)	
training:	Epoch: [145][7/233]	Loss 0.0005 (0.0019)	
training:	Epoch: [145][8/233]	Loss 0.0004 (0.0017)	
training:	Epoch: [145][9/233]	Loss 0.0007 (0.0016)	
training:	Epoch: [145][10/233]	Loss 0.0006 (0.0015)	
training:	Epoch: [145][11/233]	Loss 0.0003 (0.0014)	
training:	Epoch: [145][12/233]	Loss 0.0007 (0.0014)	
training:	Epoch: [145][13/233]	Loss 0.0008 (0.0013)	
training:	Epoch: [145][14/233]	Loss 0.0003 (0.0012)	
training:	Epoch: [145][15/233]	Loss 0.0003 (0.0012)	
training:	Epoch: [145][16/233]	Loss 0.0003 (0.0011)	
training:	Epoch: [145][17/233]	Loss 0.0004 (0.0011)	
training:	Epoch: [145][18/233]	Loss 0.0009 (0.0011)	
training:	Epoch: [145][19/233]	Loss 0.0003 (0.0010)	
training:	Epoch: [145][20/233]	Loss 0.0005 (0.0010)	
training:	Epoch: [145][21/233]	Loss 0.0004 (0.0010)	
training:	Epoch: [145][22/233]	Loss 0.0003 (0.0009)	
training:	Epoch: [145][23/233]	Loss 0.0004 (0.0009)	
training:	Epoch: [145][24/233]	Loss 0.0006 (0.0009)	
training:	Epoch: [145][25/233]	Loss 0.0005 (0.0009)	
training:	Epoch: [145][26/233]	Loss 0.0009 (0.0009)	
training:	Epoch: [145][27/233]	Loss 0.0025 (0.0010)	
training:	Epoch: [145][28/233]	Loss 0.0007 (0.0009)	
training:	Epoch: [145][29/233]	Loss 0.0004 (0.0009)	
training:	Epoch: [145][30/233]	Loss 0.0008 (0.0009)	
training:	Epoch: [145][31/233]	Loss 0.0003 (0.0009)	
training:	Epoch: [145][32/233]	Loss 0.0004 (0.0009)	
training:	Epoch: [145][33/233]	Loss 0.0005 (0.0009)	
training:	Epoch: [145][34/233]	Loss 0.0003 (0.0009)	
training:	Epoch: [145][35/233]	Loss 0.0005 (0.0008)	
training:	Epoch: [145][36/233]	Loss 0.0006 (0.0008)	
training:	Epoch: [145][37/233]	Loss 0.0008 (0.0008)	
training:	Epoch: [145][38/233]	Loss 0.0004 (0.0008)	
training:	Epoch: [145][39/233]	Loss 0.0004 (0.0008)	
training:	Epoch: [145][40/233]	Loss 0.0004 (0.0008)	
training:	Epoch: [145][41/233]	Loss 0.0004 (0.0008)	
training:	Epoch: [145][42/233]	Loss 0.0010 (0.0008)	
training:	Epoch: [145][43/233]	Loss 0.0007 (0.0008)	
training:	Epoch: [145][44/233]	Loss 0.0003 (0.0008)	
training:	Epoch: [145][45/233]	Loss 0.0003 (0.0008)	
training:	Epoch: [145][46/233]	Loss 0.0006 (0.0008)	
training:	Epoch: [145][47/233]	Loss 0.0004 (0.0008)	
training:	Epoch: [145][48/233]	Loss 0.0070 (0.0009)	
training:	Epoch: [145][49/233]	Loss 0.0006 (0.0009)	
training:	Epoch: [145][50/233]	Loss 0.0003 (0.0009)	
training:	Epoch: [145][51/233]	Loss 0.0004 (0.0009)	
training:	Epoch: [145][52/233]	Loss 0.0003 (0.0009)	
training:	Epoch: [145][53/233]	Loss 0.0003 (0.0008)	
training:	Epoch: [145][54/233]	Loss 0.0006 (0.0008)	
training:	Epoch: [145][55/233]	Loss 0.0006 (0.0008)	
training:	Epoch: [145][56/233]	Loss 0.0007 (0.0008)	
training:	Epoch: [145][57/233]	Loss 0.0019 (0.0009)	
training:	Epoch: [145][58/233]	Loss 0.0009 (0.0009)	
training:	Epoch: [145][59/233]	Loss 0.0015 (0.0009)	
training:	Epoch: [145][60/233]	Loss 0.0009 (0.0009)	
training:	Epoch: [145][61/233]	Loss 0.0004 (0.0009)	
training:	Epoch: [145][62/233]	Loss 0.0019 (0.0009)	
training:	Epoch: [145][63/233]	Loss 0.0004 (0.0009)	
training:	Epoch: [145][64/233]	Loss 0.0003 (0.0009)	
training:	Epoch: [145][65/233]	Loss 0.0003 (0.0009)	
training:	Epoch: [145][66/233]	Loss 0.0066 (0.0009)	
training:	Epoch: [145][67/233]	Loss 0.0003 (0.0009)	
training:	Epoch: [145][68/233]	Loss 0.0004 (0.0009)	
training:	Epoch: [145][69/233]	Loss 0.0003 (0.0009)	
training:	Epoch: [145][70/233]	Loss 0.0121 (0.0011)	
training:	Epoch: [145][71/233]	Loss 0.0034 (0.0011)	
training:	Epoch: [145][72/233]	Loss 0.2615 (0.0047)	
training:	Epoch: [145][73/233]	Loss 0.0003 (0.0047)	
training:	Epoch: [145][74/233]	Loss 0.0004 (0.0046)	
training:	Epoch: [145][75/233]	Loss 0.0003 (0.0045)	
training:	Epoch: [145][76/233]	Loss 0.0292 (0.0049)	
training:	Epoch: [145][77/233]	Loss 0.0005 (0.0048)	
training:	Epoch: [145][78/233]	Loss 0.0003 (0.0048)	
training:	Epoch: [145][79/233]	Loss 0.0003 (0.0047)	
training:	Epoch: [145][80/233]	Loss 0.0005 (0.0046)	
training:	Epoch: [145][81/233]	Loss 0.0004 (0.0046)	
training:	Epoch: [145][82/233]	Loss 0.0028 (0.0046)	
training:	Epoch: [145][83/233]	Loss 0.0004 (0.0045)	
training:	Epoch: [145][84/233]	Loss 0.0046 (0.0045)	
training:	Epoch: [145][85/233]	Loss 0.0003 (0.0045)	
training:	Epoch: [145][86/233]	Loss 0.0015 (0.0044)	
training:	Epoch: [145][87/233]	Loss 0.0003 (0.0044)	
training:	Epoch: [145][88/233]	Loss 0.0004 (0.0043)	
training:	Epoch: [145][89/233]	Loss 0.0003 (0.0043)	
training:	Epoch: [145][90/233]	Loss 0.0051 (0.0043)	
training:	Epoch: [145][91/233]	Loss 0.0669 (0.0050)	
training:	Epoch: [145][92/233]	Loss 0.0003 (0.0049)	
training:	Epoch: [145][93/233]	Loss 0.0004 (0.0049)	
training:	Epoch: [145][94/233]	Loss 0.0052 (0.0049)	
training:	Epoch: [145][95/233]	Loss 0.0004 (0.0049)	
training:	Epoch: [145][96/233]	Loss 0.0004 (0.0048)	
training:	Epoch: [145][97/233]	Loss 0.0091 (0.0049)	
training:	Epoch: [145][98/233]	Loss 0.0006 (0.0048)	
training:	Epoch: [145][99/233]	Loss 0.0003 (0.0048)	
training:	Epoch: [145][100/233]	Loss 0.0003 (0.0047)	
training:	Epoch: [145][101/233]	Loss 0.0004 (0.0047)	
training:	Epoch: [145][102/233]	Loss 0.0003 (0.0046)	
training:	Epoch: [145][103/233]	Loss 0.0004 (0.0046)	
training:	Epoch: [145][104/233]	Loss 0.1428 (0.0059)	
training:	Epoch: [145][105/233]	Loss 0.1030 (0.0068)	
training:	Epoch: [145][106/233]	Loss 0.0003 (0.0068)	
training:	Epoch: [145][107/233]	Loss 0.0006 (0.0067)	
training:	Epoch: [145][108/233]	Loss 0.0003 (0.0067)	
training:	Epoch: [145][109/233]	Loss 0.0016 (0.0066)	
training:	Epoch: [145][110/233]	Loss 0.0006 (0.0066)	
training:	Epoch: [145][111/233]	Loss 0.0004 (0.0065)	
training:	Epoch: [145][112/233]	Loss 0.0005 (0.0065)	
training:	Epoch: [145][113/233]	Loss 0.0205 (0.0066)	
training:	Epoch: [145][114/233]	Loss 0.0003 (0.0065)	
training:	Epoch: [145][115/233]	Loss 0.0021 (0.0065)	
training:	Epoch: [145][116/233]	Loss 0.0006 (0.0064)	
training:	Epoch: [145][117/233]	Loss 0.0004 (0.0064)	
training:	Epoch: [145][118/233]	Loss 0.2307 (0.0083)	
training:	Epoch: [145][119/233]	Loss 0.0005 (0.0082)	
training:	Epoch: [145][120/233]	Loss 0.0004 (0.0082)	
training:	Epoch: [145][121/233]	Loss 0.0004 (0.0081)	
training:	Epoch: [145][122/233]	Loss 0.0003 (0.0080)	
training:	Epoch: [145][123/233]	Loss 0.0004 (0.0080)	
training:	Epoch: [145][124/233]	Loss 0.0004 (0.0079)	
training:	Epoch: [145][125/233]	Loss 0.0003 (0.0078)	
training:	Epoch: [145][126/233]	Loss 0.0004 (0.0078)	
training:	Epoch: [145][127/233]	Loss 0.0003 (0.0077)	
training:	Epoch: [145][128/233]	Loss 0.0005 (0.0077)	
training:	Epoch: [145][129/233]	Loss 0.0006 (0.0076)	
training:	Epoch: [145][130/233]	Loss 0.0031 (0.0076)	
training:	Epoch: [145][131/233]	Loss 0.0018 (0.0075)	
training:	Epoch: [145][132/233]	Loss 0.0004 (0.0075)	
training:	Epoch: [145][133/233]	Loss 0.0003 (0.0074)	
training:	Epoch: [145][134/233]	Loss 0.0008 (0.0074)	
training:	Epoch: [145][135/233]	Loss 0.0033 (0.0073)	
training:	Epoch: [145][136/233]	Loss 0.0004 (0.0073)	
training:	Epoch: [145][137/233]	Loss 0.0003 (0.0072)	
training:	Epoch: [145][138/233]	Loss 0.0004 (0.0072)	
training:	Epoch: [145][139/233]	Loss 0.0004 (0.0071)	
training:	Epoch: [145][140/233]	Loss 0.0003 (0.0071)	
training:	Epoch: [145][141/233]	Loss 0.0006 (0.0071)	
training:	Epoch: [145][142/233]	Loss 0.0008 (0.0070)	
training:	Epoch: [145][143/233]	Loss 0.0674 (0.0074)	
training:	Epoch: [145][144/233]	Loss 0.0019 (0.0074)	
training:	Epoch: [145][145/233]	Loss 0.0005 (0.0073)	
training:	Epoch: [145][146/233]	Loss 0.0028 (0.0073)	
training:	Epoch: [145][147/233]	Loss 0.0004 (0.0073)	
training:	Epoch: [145][148/233]	Loss 0.0003 (0.0072)	
training:	Epoch: [145][149/233]	Loss 0.0003 (0.0072)	
training:	Epoch: [145][150/233]	Loss 0.0003 (0.0071)	
training:	Epoch: [145][151/233]	Loss 0.0004 (0.0071)	
training:	Epoch: [145][152/233]	Loss 0.0004 (0.0070)	
training:	Epoch: [145][153/233]	Loss 0.0003 (0.0070)	
training:	Epoch: [145][154/233]	Loss 0.0005 (0.0070)	
training:	Epoch: [145][155/233]	Loss 0.0003 (0.0069)	
training:	Epoch: [145][156/233]	Loss 0.0004 (0.0069)	
training:	Epoch: [145][157/233]	Loss 0.0003 (0.0068)	
training:	Epoch: [145][158/233]	Loss 0.0003 (0.0068)	
training:	Epoch: [145][159/233]	Loss 0.0007 (0.0067)	
training:	Epoch: [145][160/233]	Loss 0.0007 (0.0067)	
training:	Epoch: [145][161/233]	Loss 0.0006 (0.0067)	
training:	Epoch: [145][162/233]	Loss 0.0039 (0.0067)	
training:	Epoch: [145][163/233]	Loss 0.0005 (0.0066)	
training:	Epoch: [145][164/233]	Loss 0.0003 (0.0066)	
training:	Epoch: [145][165/233]	Loss 0.0005 (0.0065)	
training:	Epoch: [145][166/233]	Loss 0.0006 (0.0065)	
training:	Epoch: [145][167/233]	Loss 0.0003 (0.0065)	
training:	Epoch: [145][168/233]	Loss 0.0025 (0.0064)	
training:	Epoch: [145][169/233]	Loss 0.0062 (0.0064)	
training:	Epoch: [145][170/233]	Loss 0.0004 (0.0064)	
training:	Epoch: [145][171/233]	Loss 0.0003 (0.0064)	
training:	Epoch: [145][172/233]	Loss 0.0005 (0.0063)	
training:	Epoch: [145][173/233]	Loss 0.0004 (0.0063)	
training:	Epoch: [145][174/233]	Loss 0.0004 (0.0063)	
training:	Epoch: [145][175/233]	Loss 0.0004 (0.0062)	
training:	Epoch: [145][176/233]	Loss 0.0003 (0.0062)	
training:	Epoch: [145][177/233]	Loss 0.0005 (0.0062)	
training:	Epoch: [145][178/233]	Loss 0.0010 (0.0061)	
training:	Epoch: [145][179/233]	Loss 0.0003 (0.0061)	
training:	Epoch: [145][180/233]	Loss 0.0003 (0.0061)	
training:	Epoch: [145][181/233]	Loss 0.0083 (0.0061)	
training:	Epoch: [145][182/233]	Loss 0.0004 (0.0061)	
training:	Epoch: [145][183/233]	Loss 0.0003 (0.0060)	
training:	Epoch: [145][184/233]	Loss 0.0004 (0.0060)	
training:	Epoch: [145][185/233]	Loss 0.0003 (0.0060)	
training:	Epoch: [145][186/233]	Loss 0.0066 (0.0060)	
training:	Epoch: [145][187/233]	Loss 0.0003 (0.0059)	
training:	Epoch: [145][188/233]	Loss 0.0003 (0.0059)	
training:	Epoch: [145][189/233]	Loss 0.0004 (0.0059)	
training:	Epoch: [145][190/233]	Loss 0.0003 (0.0058)	
training:	Epoch: [145][191/233]	Loss 0.0015 (0.0058)	
training:	Epoch: [145][192/233]	Loss 0.0006 (0.0058)	
training:	Epoch: [145][193/233]	Loss 0.0004 (0.0058)	
training:	Epoch: [145][194/233]	Loss 0.0003 (0.0057)	
training:	Epoch: [145][195/233]	Loss 0.0003 (0.0057)	
training:	Epoch: [145][196/233]	Loss 0.0003 (0.0057)	
training:	Epoch: [145][197/233]	Loss 0.0003 (0.0057)	
training:	Epoch: [145][198/233]	Loss 0.0004 (0.0056)	
training:	Epoch: [145][199/233]	Loss 0.0003 (0.0056)	
training:	Epoch: [145][200/233]	Loss 0.0008 (0.0056)	
training:	Epoch: [145][201/233]	Loss 0.0003 (0.0056)	
training:	Epoch: [145][202/233]	Loss 0.0004 (0.0055)	
training:	Epoch: [145][203/233]	Loss 0.0004 (0.0055)	
training:	Epoch: [145][204/233]	Loss 0.0006 (0.0055)	
training:	Epoch: [145][205/233]	Loss 0.0003 (0.0055)	
training:	Epoch: [145][206/233]	Loss 0.0014 (0.0054)	
training:	Epoch: [145][207/233]	Loss 0.0004 (0.0054)	
training:	Epoch: [145][208/233]	Loss 0.0004 (0.0054)	
training:	Epoch: [145][209/233]	Loss 0.0003 (0.0054)	
training:	Epoch: [145][210/233]	Loss 0.0003 (0.0053)	
training:	Epoch: [145][211/233]	Loss 0.0003 (0.0053)	
training:	Epoch: [145][212/233]	Loss 0.0004 (0.0053)	
training:	Epoch: [145][213/233]	Loss 0.0003 (0.0053)	
training:	Epoch: [145][214/233]	Loss 0.0004 (0.0052)	
training:	Epoch: [145][215/233]	Loss 0.0005 (0.0052)	
training:	Epoch: [145][216/233]	Loss 0.0008 (0.0052)	
training:	Epoch: [145][217/233]	Loss 0.0003 (0.0052)	
training:	Epoch: [145][218/233]	Loss 0.0029 (0.0052)	
training:	Epoch: [145][219/233]	Loss 0.0005 (0.0051)	
training:	Epoch: [145][220/233]	Loss 0.0050 (0.0051)	
training:	Epoch: [145][221/233]	Loss 0.0003 (0.0051)	
training:	Epoch: [145][222/233]	Loss 0.0005 (0.0051)	
training:	Epoch: [145][223/233]	Loss 0.0003 (0.0051)	
training:	Epoch: [145][224/233]	Loss 0.0004 (0.0051)	
training:	Epoch: [145][225/233]	Loss 0.0004 (0.0050)	
training:	Epoch: [145][226/233]	Loss 0.0003 (0.0050)	
training:	Epoch: [145][227/233]	Loss 0.0003 (0.0050)	
training:	Epoch: [145][228/233]	Loss 0.0003 (0.0050)	
training:	Epoch: [145][229/233]	Loss 0.0004 (0.0050)	
training:	Epoch: [145][230/233]	Loss 0.0006 (0.0049)	
training:	Epoch: [145][231/233]	Loss 0.0019 (0.0049)	
training:	Epoch: [145][232/233]	Loss 0.0004 (0.0049)	
training:	Epoch: [145][233/233]	Loss 0.0003 (0.0049)	
Training:	 Loss: 0.0049

Training:	 ACC: 1.0000 1.0000 1.0000 1.0000
Validation:	 ACC: 0.7958 0.7956 0.7916 0.8000
Validation:	 Best_BACC: 0.8162 0.8154 0.7998 0.8326
Validation:	 Loss: 1.2378
Pretraining:	Epoch 146/200
----------
training:	Epoch: [146][1/233]	Loss 0.0004 (0.0004)	
training:	Epoch: [146][2/233]	Loss 0.0004 (0.0004)	
training:	Epoch: [146][3/233]	Loss 0.0004 (0.0004)	
training:	Epoch: [146][4/233]	Loss 0.0003 (0.0004)	
training:	Epoch: [146][5/233]	Loss 0.0004 (0.0004)	
training:	Epoch: [146][6/233]	Loss 0.0005 (0.0004)	
training:	Epoch: [146][7/233]	Loss 0.0003 (0.0004)	
training:	Epoch: [146][8/233]	Loss 0.0003 (0.0004)	
training:	Epoch: [146][9/233]	Loss 0.0005 (0.0004)	
training:	Epoch: [146][10/233]	Loss 0.0003 (0.0004)	
training:	Epoch: [146][11/233]	Loss 0.0006 (0.0004)	
training:	Epoch: [146][12/233]	Loss 0.0003 (0.0004)	
training:	Epoch: [146][13/233]	Loss 0.0003 (0.0004)	
training:	Epoch: [146][14/233]	Loss 0.0003 (0.0004)	
training:	Epoch: [146][15/233]	Loss 0.0003 (0.0004)	
training:	Epoch: [146][16/233]	Loss 0.0004 (0.0004)	
training:	Epoch: [146][17/233]	Loss 0.0004 (0.0004)	
training:	Epoch: [146][18/233]	Loss 0.0003 (0.0004)	
training:	Epoch: [146][19/233]	Loss 0.0021 (0.0005)	
training:	Epoch: [146][20/233]	Loss 0.0006 (0.0005)	
training:	Epoch: [146][21/233]	Loss 0.0005 (0.0005)	
training:	Epoch: [146][22/233]	Loss 0.0003 (0.0005)	
training:	Epoch: [146][23/233]	Loss 0.0004 (0.0005)	
training:	Epoch: [146][24/233]	Loss 0.0003 (0.0005)	
training:	Epoch: [146][25/233]	Loss 0.0003 (0.0005)	
training:	Epoch: [146][26/233]	Loss 0.0003 (0.0004)	
training:	Epoch: [146][27/233]	Loss 0.0005 (0.0005)	
training:	Epoch: [146][28/233]	Loss 0.0015 (0.0005)	
training:	Epoch: [146][29/233]	Loss 0.0003 (0.0005)	
training:	Epoch: [146][30/233]	Loss 0.0003 (0.0005)	
training:	Epoch: [146][31/233]	Loss 0.0003 (0.0005)	
training:	Epoch: [146][32/233]	Loss 0.0005 (0.0005)	
training:	Epoch: [146][33/233]	Loss 0.0004 (0.0005)	
training:	Epoch: [146][34/233]	Loss 0.0009 (0.0005)	
training:	Epoch: [146][35/233]	Loss 0.0008 (0.0005)	
training:	Epoch: [146][36/233]	Loss 0.0005 (0.0005)	
training:	Epoch: [146][37/233]	Loss 0.0027 (0.0006)	
training:	Epoch: [146][38/233]	Loss 0.0005 (0.0006)	
training:	Epoch: [146][39/233]	Loss 0.0007 (0.0006)	
training:	Epoch: [146][40/233]	Loss 0.0143 (0.0009)	
training:	Epoch: [146][41/233]	Loss 0.0007 (0.0009)	
training:	Epoch: [146][42/233]	Loss 0.0005 (0.0009)	
training:	Epoch: [146][43/233]	Loss 0.0004 (0.0009)	
training:	Epoch: [146][44/233]	Loss 0.0003 (0.0009)	
training:	Epoch: [146][45/233]	Loss 0.0003 (0.0008)	
training:	Epoch: [146][46/233]	Loss 0.0005 (0.0008)	
training:	Epoch: [146][47/233]	Loss 0.0007 (0.0008)	
training:	Epoch: [146][48/233]	Loss 0.0009 (0.0008)	
training:	Epoch: [146][49/233]	Loss 0.1755 (0.0044)	
training:	Epoch: [146][50/233]	Loss 0.0003 (0.0043)	
training:	Epoch: [146][51/233]	Loss 0.0003 (0.0042)	
training:	Epoch: [146][52/233]	Loss 0.0004 (0.0042)	
training:	Epoch: [146][53/233]	Loss 0.0003 (0.0041)	
training:	Epoch: [146][54/233]	Loss 0.0003 (0.0040)	
training:	Epoch: [146][55/233]	Loss 0.0003 (0.0040)	
training:	Epoch: [146][56/233]	Loss 0.0004 (0.0039)	
training:	Epoch: [146][57/233]	Loss 0.0003 (0.0038)	
training:	Epoch: [146][58/233]	Loss 0.0006 (0.0038)	
training:	Epoch: [146][59/233]	Loss 0.0006 (0.0037)	
training:	Epoch: [146][60/233]	Loss 0.0004 (0.0037)	
training:	Epoch: [146][61/233]	Loss 0.0004 (0.0036)	
training:	Epoch: [146][62/233]	Loss 0.0004 (0.0036)	
training:	Epoch: [146][63/233]	Loss 0.0003 (0.0035)	
training:	Epoch: [146][64/233]	Loss 0.0005 (0.0035)	
training:	Epoch: [146][65/233]	Loss 0.1778 (0.0061)	
training:	Epoch: [146][66/233]	Loss 0.0004 (0.0061)	
training:	Epoch: [146][67/233]	Loss 0.0021 (0.0060)	
training:	Epoch: [146][68/233]	Loss 0.0004 (0.0059)	
training:	Epoch: [146][69/233]	Loss 0.0003 (0.0058)	
training:	Epoch: [146][70/233]	Loss 0.2478 (0.0093)	
training:	Epoch: [146][71/233]	Loss 0.0003 (0.0092)	
training:	Epoch: [146][72/233]	Loss 0.0003 (0.0090)	
training:	Epoch: [146][73/233]	Loss 0.0003 (0.0089)	
training:	Epoch: [146][74/233]	Loss 0.0004 (0.0088)	
training:	Epoch: [146][75/233]	Loss 0.0008 (0.0087)	
training:	Epoch: [146][76/233]	Loss 0.0003 (0.0086)	
training:	Epoch: [146][77/233]	Loss 0.0003 (0.0085)	
training:	Epoch: [146][78/233]	Loss 0.0006 (0.0084)	
training:	Epoch: [146][79/233]	Loss 0.0004 (0.0083)	
training:	Epoch: [146][80/233]	Loss 0.0007 (0.0082)	
training:	Epoch: [146][81/233]	Loss 0.0005 (0.0081)	
training:	Epoch: [146][82/233]	Loss 0.0008 (0.0080)	
training:	Epoch: [146][83/233]	Loss 0.0009 (0.0079)	
training:	Epoch: [146][84/233]	Loss 0.0004 (0.0078)	
training:	Epoch: [146][85/233]	Loss 0.0004 (0.0077)	
training:	Epoch: [146][86/233]	Loss 0.0005 (0.0077)	
training:	Epoch: [146][87/233]	Loss 0.2170 (0.0101)	
training:	Epoch: [146][88/233]	Loss 0.0003 (0.0099)	
training:	Epoch: [146][89/233]	Loss 0.0003 (0.0098)	
training:	Epoch: [146][90/233]	Loss 0.0004 (0.0097)	
training:	Epoch: [146][91/233]	Loss 0.0014 (0.0096)	
training:	Epoch: [146][92/233]	Loss 0.0679 (0.0103)	
training:	Epoch: [146][93/233]	Loss 0.0003 (0.0102)	
training:	Epoch: [146][94/233]	Loss 0.0003 (0.0101)	
training:	Epoch: [146][95/233]	Loss 0.0004 (0.0100)	
training:	Epoch: [146][96/233]	Loss 0.0005 (0.0099)	
training:	Epoch: [146][97/233]	Loss 0.0004 (0.0098)	
training:	Epoch: [146][98/233]	Loss 0.0005 (0.0097)	
training:	Epoch: [146][99/233]	Loss 0.0006 (0.0096)	
training:	Epoch: [146][100/233]	Loss 0.0005 (0.0095)	
training:	Epoch: [146][101/233]	Loss 0.0003 (0.0094)	
training:	Epoch: [146][102/233]	Loss 0.0010 (0.0093)	
training:	Epoch: [146][103/233]	Loss 0.0004 (0.0092)	
training:	Epoch: [146][104/233]	Loss 0.0003 (0.0091)	
training:	Epoch: [146][105/233]	Loss 0.0004 (0.0091)	
training:	Epoch: [146][106/233]	Loss 0.0003 (0.0090)	
training:	Epoch: [146][107/233]	Loss 0.0003 (0.0089)	
training:	Epoch: [146][108/233]	Loss 0.0024 (0.0088)	
training:	Epoch: [146][109/233]	Loss 0.0011 (0.0088)	
training:	Epoch: [146][110/233]	Loss 0.0007 (0.0087)	
training:	Epoch: [146][111/233]	Loss 0.0004 (0.0086)	
training:	Epoch: [146][112/233]	Loss 0.0012 (0.0086)	
training:	Epoch: [146][113/233]	Loss 0.0018 (0.0085)	
training:	Epoch: [146][114/233]	Loss 0.0003 (0.0084)	
training:	Epoch: [146][115/233]	Loss 0.0012 (0.0084)	
training:	Epoch: [146][116/233]	Loss 0.0005 (0.0083)	
training:	Epoch: [146][117/233]	Loss 0.0003 (0.0082)	
training:	Epoch: [146][118/233]	Loss 0.0010 (0.0082)	
training:	Epoch: [146][119/233]	Loss 0.0004 (0.0081)	
training:	Epoch: [146][120/233]	Loss 0.0005 (0.0080)	
training:	Epoch: [146][121/233]	Loss 0.0005 (0.0080)	
training:	Epoch: [146][122/233]	Loss 0.0004 (0.0079)	
training:	Epoch: [146][123/233]	Loss 0.0003 (0.0078)	
training:	Epoch: [146][124/233]	Loss 0.0004 (0.0078)	
training:	Epoch: [146][125/233]	Loss 0.0004 (0.0077)	
training:	Epoch: [146][126/233]	Loss 0.0006 (0.0077)	
training:	Epoch: [146][127/233]	Loss 0.0004 (0.0076)	
training:	Epoch: [146][128/233]	Loss 0.0005 (0.0076)	
training:	Epoch: [146][129/233]	Loss 0.0005 (0.0075)	
training:	Epoch: [146][130/233]	Loss 0.0004 (0.0074)	
training:	Epoch: [146][131/233]	Loss 0.0003 (0.0074)	
training:	Epoch: [146][132/233]	Loss 0.0008 (0.0073)	
training:	Epoch: [146][133/233]	Loss 0.0004 (0.0073)	
training:	Epoch: [146][134/233]	Loss 0.0876 (0.0079)	
training:	Epoch: [146][135/233]	Loss 0.0011 (0.0078)	
training:	Epoch: [146][136/233]	Loss 0.0039 (0.0078)	
training:	Epoch: [146][137/233]	Loss 0.0003 (0.0078)	
training:	Epoch: [146][138/233]	Loss 0.0010 (0.0077)	
training:	Epoch: [146][139/233]	Loss 0.0018 (0.0077)	
training:	Epoch: [146][140/233]	Loss 0.0004 (0.0076)	
training:	Epoch: [146][141/233]	Loss 0.0088 (0.0076)	
training:	Epoch: [146][142/233]	Loss 0.0003 (0.0076)	
training:	Epoch: [146][143/233]	Loss 0.0018 (0.0075)	
training:	Epoch: [146][144/233]	Loss 0.0003 (0.0075)	
training:	Epoch: [146][145/233]	Loss 0.0004 (0.0074)	
training:	Epoch: [146][146/233]	Loss 0.0003 (0.0074)	
training:	Epoch: [146][147/233]	Loss 0.0008 (0.0073)	
training:	Epoch: [146][148/233]	Loss 0.0005 (0.0073)	
training:	Epoch: [146][149/233]	Loss 0.0010 (0.0072)	
training:	Epoch: [146][150/233]	Loss 0.0009 (0.0072)	
training:	Epoch: [146][151/233]	Loss 0.0010 (0.0072)	
training:	Epoch: [146][152/233]	Loss 0.0003 (0.0071)	
training:	Epoch: [146][153/233]	Loss 0.0004 (0.0071)	
training:	Epoch: [146][154/233]	Loss 0.0007 (0.0070)	
training:	Epoch: [146][155/233]	Loss 0.0005 (0.0070)	
training:	Epoch: [146][156/233]	Loss 0.0008 (0.0070)	
training:	Epoch: [146][157/233]	Loss 0.0004 (0.0069)	
training:	Epoch: [146][158/233]	Loss 0.0003 (0.0069)	
training:	Epoch: [146][159/233]	Loss 0.0004 (0.0068)	
training:	Epoch: [146][160/233]	Loss 0.0006 (0.0068)	
training:	Epoch: [146][161/233]	Loss 0.1439 (0.0076)	
training:	Epoch: [146][162/233]	Loss 0.0003 (0.0076)	
training:	Epoch: [146][163/233]	Loss 0.0006 (0.0076)	
training:	Epoch: [146][164/233]	Loss 0.0016 (0.0075)	
training:	Epoch: [146][165/233]	Loss 0.0006 (0.0075)	
training:	Epoch: [146][166/233]	Loss 0.0003 (0.0074)	
training:	Epoch: [146][167/233]	Loss 0.0005 (0.0074)	
training:	Epoch: [146][168/233]	Loss 0.0004 (0.0073)	
training:	Epoch: [146][169/233]	Loss 0.0003 (0.0073)	
training:	Epoch: [146][170/233]	Loss 0.0005 (0.0073)	
training:	Epoch: [146][171/233]	Loss 0.0003 (0.0072)	
training:	Epoch: [146][172/233]	Loss 0.0009 (0.0072)	
training:	Epoch: [146][173/233]	Loss 0.0009 (0.0072)	
training:	Epoch: [146][174/233]	Loss 0.0003 (0.0071)	
training:	Epoch: [146][175/233]	Loss 0.0154 (0.0072)	
training:	Epoch: [146][176/233]	Loss 0.0004 (0.0071)	
training:	Epoch: [146][177/233]	Loss 0.0015 (0.0071)	
training:	Epoch: [146][178/233]	Loss 0.0003 (0.0071)	
training:	Epoch: [146][179/233]	Loss 0.0004 (0.0070)	
training:	Epoch: [146][180/233]	Loss 0.0004 (0.0070)	
training:	Epoch: [146][181/233]	Loss 0.0004 (0.0069)	
training:	Epoch: [146][182/233]	Loss 0.0005 (0.0069)	
training:	Epoch: [146][183/233]	Loss 0.0005 (0.0069)	
training:	Epoch: [146][184/233]	Loss 0.0005 (0.0068)	
training:	Epoch: [146][185/233]	Loss 0.0138 (0.0069)	
training:	Epoch: [146][186/233]	Loss 0.0974 (0.0074)	
training:	Epoch: [146][187/233]	Loss 0.0004 (0.0073)	
training:	Epoch: [146][188/233]	Loss 0.0004 (0.0073)	
training:	Epoch: [146][189/233]	Loss 0.0066 (0.0073)	
training:	Epoch: [146][190/233]	Loss 0.0004 (0.0072)	
training:	Epoch: [146][191/233]	Loss 0.0004 (0.0072)	
training:	Epoch: [146][192/233]	Loss 0.0005 (0.0072)	
training:	Epoch: [146][193/233]	Loss 0.0004 (0.0071)	
training:	Epoch: [146][194/233]	Loss 0.0038 (0.0071)	
training:	Epoch: [146][195/233]	Loss 0.0003 (0.0071)	
training:	Epoch: [146][196/233]	Loss 0.0013 (0.0071)	
training:	Epoch: [146][197/233]	Loss 0.0880 (0.0075)	
training:	Epoch: [146][198/233]	Loss 0.0004 (0.0074)	
training:	Epoch: [146][199/233]	Loss 0.0009 (0.0074)	
training:	Epoch: [146][200/233]	Loss 0.0007 (0.0074)	
training:	Epoch: [146][201/233]	Loss 0.0009 (0.0073)	
training:	Epoch: [146][202/233]	Loss 0.0005 (0.0073)	
training:	Epoch: [146][203/233]	Loss 0.0009 (0.0073)	
training:	Epoch: [146][204/233]	Loss 0.0006 (0.0072)	
training:	Epoch: [146][205/233]	Loss 0.0003 (0.0072)	
training:	Epoch: [146][206/233]	Loss 0.0004 (0.0072)	
training:	Epoch: [146][207/233]	Loss 0.0013 (0.0071)	
training:	Epoch: [146][208/233]	Loss 0.0005 (0.0071)	
training:	Epoch: [146][209/233]	Loss 0.0007 (0.0071)	
training:	Epoch: [146][210/233]	Loss 0.0004 (0.0071)	
training:	Epoch: [146][211/233]	Loss 0.0006 (0.0070)	
training:	Epoch: [146][212/233]	Loss 0.0014 (0.0070)	
training:	Epoch: [146][213/233]	Loss 0.0013 (0.0070)	
training:	Epoch: [146][214/233]	Loss 0.0004 (0.0069)	
training:	Epoch: [146][215/233]	Loss 0.0003 (0.0069)	
training:	Epoch: [146][216/233]	Loss 0.0006 (0.0069)	
training:	Epoch: [146][217/233]	Loss 0.0006 (0.0068)	
training:	Epoch: [146][218/233]	Loss 0.0010 (0.0068)	
training:	Epoch: [146][219/233]	Loss 0.0006 (0.0068)	
training:	Epoch: [146][220/233]	Loss 0.0092 (0.0068)	
training:	Epoch: [146][221/233]	Loss 0.0025 (0.0068)	
training:	Epoch: [146][222/233]	Loss 0.0059 (0.0068)	
training:	Epoch: [146][223/233]	Loss 0.0049 (0.0068)	
training:	Epoch: [146][224/233]	Loss 0.0005 (0.0067)	
training:	Epoch: [146][225/233]	Loss 0.0005 (0.0067)	
training:	Epoch: [146][226/233]	Loss 0.0004 (0.0067)	
training:	Epoch: [146][227/233]	Loss 0.0182 (0.0067)	
training:	Epoch: [146][228/233]	Loss 0.0012 (0.0067)	
training:	Epoch: [146][229/233]	Loss 0.0012 (0.0067)	
training:	Epoch: [146][230/233]	Loss 0.0009 (0.0067)	
training:	Epoch: [146][231/233]	Loss 0.0006 (0.0066)	
training:	Epoch: [146][232/233]	Loss 0.0006 (0.0066)	
training:	Epoch: [146][233/233]	Loss 0.0005 (0.0066)	
Training:	 Loss: 0.0066

Training:	 ACC: 1.0000 1.0000 1.0000 1.0000
Validation:	 ACC: 0.7912 0.7913 0.7947 0.7876
Validation:	 Best_BACC: 0.8162 0.8154 0.7998 0.8326
Validation:	 Loss: 1.1966
Pretraining:	Epoch 147/200
----------
training:	Epoch: [147][1/233]	Loss 0.0005 (0.0005)	
training:	Epoch: [147][2/233]	Loss 0.0003 (0.0004)	
training:	Epoch: [147][3/233]	Loss 0.0005 (0.0005)	
training:	Epoch: [147][4/233]	Loss 0.0004 (0.0004)	
training:	Epoch: [147][5/233]	Loss 0.0046 (0.0013)	
training:	Epoch: [147][6/233]	Loss 0.0007 (0.0012)	
training:	Epoch: [147][7/233]	Loss 0.0015 (0.0012)	
training:	Epoch: [147][8/233]	Loss 0.0004 (0.0011)	
training:	Epoch: [147][9/233]	Loss 0.0004 (0.0010)	
training:	Epoch: [147][10/233]	Loss 0.0009 (0.0010)	
training:	Epoch: [147][11/233]	Loss 0.0123 (0.0021)	
training:	Epoch: [147][12/233]	Loss 0.0006 (0.0019)	
training:	Epoch: [147][13/233]	Loss 0.0003 (0.0018)	
training:	Epoch: [147][14/233]	Loss 0.0003 (0.0017)	
training:	Epoch: [147][15/233]	Loss 0.0004 (0.0016)	
training:	Epoch: [147][16/233]	Loss 0.0004 (0.0015)	
training:	Epoch: [147][17/233]	Loss 0.0016 (0.0015)	
training:	Epoch: [147][18/233]	Loss 0.0004 (0.0015)	
training:	Epoch: [147][19/233]	Loss 0.0006 (0.0014)	
training:	Epoch: [147][20/233]	Loss 0.0011 (0.0014)	
training:	Epoch: [147][21/233]	Loss 0.0059 (0.0016)	
training:	Epoch: [147][22/233]	Loss 0.0005 (0.0016)	
training:	Epoch: [147][23/233]	Loss 0.0005 (0.0015)	
training:	Epoch: [147][24/233]	Loss 0.0004 (0.0015)	
training:	Epoch: [147][25/233]	Loss 0.0003 (0.0014)	
training:	Epoch: [147][26/233]	Loss 0.0006 (0.0014)	
training:	Epoch: [147][27/233]	Loss 0.0003 (0.0014)	
training:	Epoch: [147][28/233]	Loss 0.0004 (0.0013)	
training:	Epoch: [147][29/233]	Loss 0.0038 (0.0014)	
training:	Epoch: [147][30/233]	Loss 0.0070 (0.0016)	
training:	Epoch: [147][31/233]	Loss 0.0004 (0.0016)	
training:	Epoch: [147][32/233]	Loss 0.0004 (0.0015)	
training:	Epoch: [147][33/233]	Loss 0.0003 (0.0015)	
training:	Epoch: [147][34/233]	Loss 0.0004 (0.0015)	
training:	Epoch: [147][35/233]	Loss 0.0004 (0.0014)	
training:	Epoch: [147][36/233]	Loss 0.0007 (0.0014)	
training:	Epoch: [147][37/233]	Loss 0.0004 (0.0014)	
training:	Epoch: [147][38/233]	Loss 0.0005 (0.0014)	
training:	Epoch: [147][39/233]	Loss 0.0008 (0.0013)	
training:	Epoch: [147][40/233]	Loss 0.0005 (0.0013)	
training:	Epoch: [147][41/233]	Loss 0.0004 (0.0013)	
training:	Epoch: [147][42/233]	Loss 0.0004 (0.0013)	
training:	Epoch: [147][43/233]	Loss 0.0005 (0.0013)	
training:	Epoch: [147][44/233]	Loss 0.0004 (0.0012)	
training:	Epoch: [147][45/233]	Loss 0.0009 (0.0012)	
training:	Epoch: [147][46/233]	Loss 0.0004 (0.0012)	
training:	Epoch: [147][47/233]	Loss 0.0003 (0.0012)	
training:	Epoch: [147][48/233]	Loss 0.0004 (0.0012)	
training:	Epoch: [147][49/233]	Loss 0.0003 (0.0012)	
training:	Epoch: [147][50/233]	Loss 0.0047 (0.0012)	
training:	Epoch: [147][51/233]	Loss 0.0005 (0.0012)	
training:	Epoch: [147][52/233]	Loss 0.0027 (0.0012)	
training:	Epoch: [147][53/233]	Loss 0.0004 (0.0012)	
training:	Epoch: [147][54/233]	Loss 0.0007 (0.0012)	
training:	Epoch: [147][55/233]	Loss 0.0005 (0.0012)	
training:	Epoch: [147][56/233]	Loss 0.0103 (0.0014)	
training:	Epoch: [147][57/233]	Loss 0.0003 (0.0014)	
training:	Epoch: [147][58/233]	Loss 0.0006 (0.0013)	
training:	Epoch: [147][59/233]	Loss 0.0006 (0.0013)	
training:	Epoch: [147][60/233]	Loss 0.0004 (0.0013)	
training:	Epoch: [147][61/233]	Loss 0.0004 (0.0013)	
training:	Epoch: [147][62/233]	Loss 0.0003 (0.0013)	
training:	Epoch: [147][63/233]	Loss 0.0015 (0.0013)	
training:	Epoch: [147][64/233]	Loss 0.0094 (0.0014)	
training:	Epoch: [147][65/233]	Loss 0.0005 (0.0014)	
training:	Epoch: [147][66/233]	Loss 0.0004 (0.0014)	
training:	Epoch: [147][67/233]	Loss 0.0004 (0.0014)	
training:	Epoch: [147][68/233]	Loss 0.0007 (0.0014)	
training:	Epoch: [147][69/233]	Loss 0.0012 (0.0014)	
training:	Epoch: [147][70/233]	Loss 0.0003 (0.0013)	
training:	Epoch: [147][71/233]	Loss 0.0003 (0.0013)	
training:	Epoch: [147][72/233]	Loss 0.0005 (0.0013)	
training:	Epoch: [147][73/233]	Loss 0.0007 (0.0013)	
training:	Epoch: [147][74/233]	Loss 0.0004 (0.0013)	
training:	Epoch: [147][75/233]	Loss 0.0004 (0.0013)	
training:	Epoch: [147][76/233]	Loss 0.0005 (0.0013)	
training:	Epoch: [147][77/233]	Loss 0.0027 (0.0013)	
training:	Epoch: [147][78/233]	Loss 0.0007 (0.0013)	
training:	Epoch: [147][79/233]	Loss 0.0003 (0.0013)	
training:	Epoch: [147][80/233]	Loss 0.0004 (0.0013)	
training:	Epoch: [147][81/233]	Loss 0.0003 (0.0012)	
training:	Epoch: [147][82/233]	Loss 0.0004 (0.0012)	
training:	Epoch: [147][83/233]	Loss 0.0004 (0.0012)	
training:	Epoch: [147][84/233]	Loss 0.0003 (0.0012)	
training:	Epoch: [147][85/233]	Loss 0.0005 (0.0012)	
training:	Epoch: [147][86/233]	Loss 0.0028 (0.0012)	
training:	Epoch: [147][87/233]	Loss 0.0003 (0.0012)	
training:	Epoch: [147][88/233]	Loss 0.0049 (0.0013)	
training:	Epoch: [147][89/233]	Loss 0.0003 (0.0012)	
training:	Epoch: [147][90/233]	Loss 0.0006 (0.0012)	
training:	Epoch: [147][91/233]	Loss 0.0007 (0.0012)	
training:	Epoch: [147][92/233]	Loss 0.0004 (0.0012)	
training:	Epoch: [147][93/233]	Loss 0.0011 (0.0012)	
training:	Epoch: [147][94/233]	Loss 0.0004 (0.0012)	
training:	Epoch: [147][95/233]	Loss 0.0008 (0.0012)	
training:	Epoch: [147][96/233]	Loss 0.0005 (0.0012)	
training:	Epoch: [147][97/233]	Loss 0.0005 (0.0012)	
training:	Epoch: [147][98/233]	Loss 0.0003 (0.0012)	
training:	Epoch: [147][99/233]	Loss 0.0057 (0.0012)	
training:	Epoch: [147][100/233]	Loss 0.0005 (0.0012)	
training:	Epoch: [147][101/233]	Loss 0.0004 (0.0012)	
training:	Epoch: [147][102/233]	Loss 0.0004 (0.0012)	
training:	Epoch: [147][103/233]	Loss 0.0005 (0.0012)	
training:	Epoch: [147][104/233]	Loss 0.1241 (0.0024)	
training:	Epoch: [147][105/233]	Loss 0.0005 (0.0024)	
training:	Epoch: [147][106/233]	Loss 0.0006 (0.0023)	
training:	Epoch: [147][107/233]	Loss 0.0005 (0.0023)	
training:	Epoch: [147][108/233]	Loss 0.0005 (0.0023)	
training:	Epoch: [147][109/233]	Loss 0.0004 (0.0023)	
training:	Epoch: [147][110/233]	Loss 0.0004 (0.0023)	
training:	Epoch: [147][111/233]	Loss 0.0003 (0.0023)	
training:	Epoch: [147][112/233]	Loss 0.0023 (0.0023)	
training:	Epoch: [147][113/233]	Loss 0.0005 (0.0022)	
training:	Epoch: [147][114/233]	Loss 0.0004 (0.0022)	
training:	Epoch: [147][115/233]	Loss 0.0007 (0.0022)	
training:	Epoch: [147][116/233]	Loss 0.0015 (0.0022)	
training:	Epoch: [147][117/233]	Loss 0.0005 (0.0022)	
training:	Epoch: [147][118/233]	Loss 0.0004 (0.0022)	
training:	Epoch: [147][119/233]	Loss 0.0005 (0.0022)	
training:	Epoch: [147][120/233]	Loss 0.0004 (0.0021)	
training:	Epoch: [147][121/233]	Loss 0.0027 (0.0022)	
training:	Epoch: [147][122/233]	Loss 0.0003 (0.0021)	
training:	Epoch: [147][123/233]	Loss 0.0005 (0.0021)	
training:	Epoch: [147][124/233]	Loss 0.0010 (0.0021)	
training:	Epoch: [147][125/233]	Loss 0.0456 (0.0025)	
training:	Epoch: [147][126/233]	Loss 0.0009 (0.0025)	
training:	Epoch: [147][127/233]	Loss 0.0004 (0.0024)	
training:	Epoch: [147][128/233]	Loss 0.0004 (0.0024)	
training:	Epoch: [147][129/233]	Loss 0.0005 (0.0024)	
training:	Epoch: [147][130/233]	Loss 0.0008 (0.0024)	
training:	Epoch: [147][131/233]	Loss 0.0009 (0.0024)	
training:	Epoch: [147][132/233]	Loss 0.0006 (0.0024)	
training:	Epoch: [147][133/233]	Loss 0.0004 (0.0024)	
training:	Epoch: [147][134/233]	Loss 0.0004 (0.0023)	
training:	Epoch: [147][135/233]	Loss 0.0054 (0.0024)	
training:	Epoch: [147][136/233]	Loss 0.0004 (0.0023)	
training:	Epoch: [147][137/233]	Loss 0.0246 (0.0025)	
training:	Epoch: [147][138/233]	Loss 0.0003 (0.0025)	
training:	Epoch: [147][139/233]	Loss 0.0020 (0.0025)	
training:	Epoch: [147][140/233]	Loss 0.0007 (0.0025)	
training:	Epoch: [147][141/233]	Loss 0.0004 (0.0025)	
training:	Epoch: [147][142/233]	Loss 0.0003 (0.0024)	
training:	Epoch: [147][143/233]	Loss 0.0017 (0.0024)	
training:	Epoch: [147][144/233]	Loss 0.0053 (0.0025)	
training:	Epoch: [147][145/233]	Loss 0.0012 (0.0025)	
training:	Epoch: [147][146/233]	Loss 0.0062 (0.0025)	
training:	Epoch: [147][147/233]	Loss 0.0003 (0.0025)	
training:	Epoch: [147][148/233]	Loss 0.0004 (0.0024)	
training:	Epoch: [147][149/233]	Loss 0.0012 (0.0024)	
training:	Epoch: [147][150/233]	Loss 0.0008 (0.0024)	
training:	Epoch: [147][151/233]	Loss 0.0013 (0.0024)	
training:	Epoch: [147][152/233]	Loss 0.0006 (0.0024)	
training:	Epoch: [147][153/233]	Loss 0.0004 (0.0024)	
training:	Epoch: [147][154/233]	Loss 0.0004 (0.0024)	
training:	Epoch: [147][155/233]	Loss 0.0010 (0.0024)	
training:	Epoch: [147][156/233]	Loss 0.0005 (0.0024)	
training:	Epoch: [147][157/233]	Loss 0.0005 (0.0024)	
training:	Epoch: [147][158/233]	Loss 0.0003 (0.0023)	
training:	Epoch: [147][159/233]	Loss 0.0004 (0.0023)	
training:	Epoch: [147][160/233]	Loss 0.0008 (0.0023)	
training:	Epoch: [147][161/233]	Loss 0.0045 (0.0023)	
training:	Epoch: [147][162/233]	Loss 0.0004 (0.0023)	
training:	Epoch: [147][163/233]	Loss 0.0028 (0.0023)	
training:	Epoch: [147][164/233]	Loss 0.0007 (0.0023)	
training:	Epoch: [147][165/233]	Loss 0.0004 (0.0023)	
training:	Epoch: [147][166/233]	Loss 0.0011 (0.0023)	
training:	Epoch: [147][167/233]	Loss 0.0007 (0.0023)	
training:	Epoch: [147][168/233]	Loss 0.0003 (0.0023)	
training:	Epoch: [147][169/233]	Loss 0.0013 (0.0023)	
training:	Epoch: [147][170/233]	Loss 0.0007 (0.0023)	
training:	Epoch: [147][171/233]	Loss 0.0011 (0.0022)	
training:	Epoch: [147][172/233]	Loss 0.0041 (0.0023)	
training:	Epoch: [147][173/233]	Loss 0.0004 (0.0022)	
training:	Epoch: [147][174/233]	Loss 0.0013 (0.0022)	
training:	Epoch: [147][175/233]	Loss 0.0005 (0.0022)	
training:	Epoch: [147][176/233]	Loss 0.0004 (0.0022)	
training:	Epoch: [147][177/233]	Loss 0.0003 (0.0022)	
training:	Epoch: [147][178/233]	Loss 0.0005 (0.0022)	
training:	Epoch: [147][179/233]	Loss 0.0003 (0.0022)	
training:	Epoch: [147][180/233]	Loss 0.0238 (0.0023)	
training:	Epoch: [147][181/233]	Loss 0.0019 (0.0023)	
training:	Epoch: [147][182/233]	Loss 0.0442 (0.0025)	
training:	Epoch: [147][183/233]	Loss 0.0003 (0.0025)	
training:	Epoch: [147][184/233]	Loss 0.0003 (0.0025)	
training:	Epoch: [147][185/233]	Loss 0.0006 (0.0025)	
training:	Epoch: [147][186/233]	Loss 0.0004 (0.0025)	
training:	Epoch: [147][187/233]	Loss 0.0003 (0.0025)	
training:	Epoch: [147][188/233]	Loss 0.0007 (0.0025)	
training:	Epoch: [147][189/233]	Loss 0.0004 (0.0025)	
training:	Epoch: [147][190/233]	Loss 0.0007 (0.0025)	
training:	Epoch: [147][191/233]	Loss 0.0007 (0.0024)	
training:	Epoch: [147][192/233]	Loss 0.0005 (0.0024)	
training:	Epoch: [147][193/233]	Loss 0.0067 (0.0025)	
training:	Epoch: [147][194/233]	Loss 0.0004 (0.0024)	
training:	Epoch: [147][195/233]	Loss 0.0008 (0.0024)	
training:	Epoch: [147][196/233]	Loss 0.0004 (0.0024)	
training:	Epoch: [147][197/233]	Loss 0.0007 (0.0024)	
training:	Epoch: [147][198/233]	Loss 0.0007 (0.0024)	
training:	Epoch: [147][199/233]	Loss 0.0030 (0.0024)	
training:	Epoch: [147][200/233]	Loss 0.0003 (0.0024)	
training:	Epoch: [147][201/233]	Loss 0.0003 (0.0024)	
training:	Epoch: [147][202/233]	Loss 0.0006 (0.0024)	
training:	Epoch: [147][203/233]	Loss 0.0003 (0.0024)	
training:	Epoch: [147][204/233]	Loss 0.0005 (0.0024)	
training:	Epoch: [147][205/233]	Loss 0.0003 (0.0024)	
training:	Epoch: [147][206/233]	Loss 0.0006 (0.0023)	
training:	Epoch: [147][207/233]	Loss 0.0003 (0.0023)	
training:	Epoch: [147][208/233]	Loss 0.0005 (0.0023)	
training:	Epoch: [147][209/233]	Loss 0.0006 (0.0023)	
training:	Epoch: [147][210/233]	Loss 0.0955 (0.0028)	
training:	Epoch: [147][211/233]	Loss 0.0236 (0.0029)	
training:	Epoch: [147][212/233]	Loss 0.0003 (0.0028)	
training:	Epoch: [147][213/233]	Loss 0.0075 (0.0029)	
training:	Epoch: [147][214/233]	Loss 0.0009 (0.0029)	
training:	Epoch: [147][215/233]	Loss 0.0026 (0.0029)	
training:	Epoch: [147][216/233]	Loss 0.0004 (0.0028)	
training:	Epoch: [147][217/233]	Loss 0.0081 (0.0029)	
training:	Epoch: [147][218/233]	Loss 0.0004 (0.0029)	
training:	Epoch: [147][219/233]	Loss 0.0022 (0.0029)	
training:	Epoch: [147][220/233]	Loss 0.0198 (0.0029)	
training:	Epoch: [147][221/233]	Loss 0.0011 (0.0029)	
training:	Epoch: [147][222/233]	Loss 0.0211 (0.0030)	
training:	Epoch: [147][223/233]	Loss 0.0005 (0.0030)	
training:	Epoch: [147][224/233]	Loss 0.0003 (0.0030)	
training:	Epoch: [147][225/233]	Loss 0.0005 (0.0030)	
training:	Epoch: [147][226/233]	Loss 0.0007 (0.0030)	
training:	Epoch: [147][227/233]	Loss 0.0006 (0.0030)	
training:	Epoch: [147][228/233]	Loss 0.0004 (0.0029)	
training:	Epoch: [147][229/233]	Loss 0.0003 (0.0029)	
training:	Epoch: [147][230/233]	Loss 0.0003 (0.0029)	
training:	Epoch: [147][231/233]	Loss 0.0006 (0.0029)	
training:	Epoch: [147][232/233]	Loss 0.0003 (0.0029)	
training:	Epoch: [147][233/233]	Loss 0.0009 (0.0029)	
Training:	 Loss: 0.0029

Training:	 ACC: 1.0000 1.0000 1.0000 1.0000
Validation:	 ACC: 0.7869 0.7892 0.8345 0.7393
Validation:	 Best_BACC: 0.8162 0.8154 0.7998 0.8326
Validation:	 Loss: 1.2616
Pretraining:	Epoch 148/200
----------
training:	Epoch: [148][1/233]	Loss 0.0074 (0.0074)	
training:	Epoch: [148][2/233]	Loss 0.0034 (0.0054)	
training:	Epoch: [148][3/233]	Loss 0.0010 (0.0039)	
training:	Epoch: [148][4/233]	Loss 0.0003 (0.0030)	
training:	Epoch: [148][5/233]	Loss 0.0003 (0.0025)	
training:	Epoch: [148][6/233]	Loss 0.0004 (0.0021)	
training:	Epoch: [148][7/233]	Loss 0.0005 (0.0019)	
training:	Epoch: [148][8/233]	Loss 0.0003 (0.0017)	
training:	Epoch: [148][9/233]	Loss 0.0003 (0.0015)	
training:	Epoch: [148][10/233]	Loss 0.0003 (0.0014)	
training:	Epoch: [148][11/233]	Loss 0.0032 (0.0016)	
training:	Epoch: [148][12/233]	Loss 0.0003 (0.0015)	
training:	Epoch: [148][13/233]	Loss 0.0004 (0.0014)	
training:	Epoch: [148][14/233]	Loss 0.0003 (0.0013)	
training:	Epoch: [148][15/233]	Loss 0.0004 (0.0013)	
training:	Epoch: [148][16/233]	Loss 0.0007 (0.0012)	
training:	Epoch: [148][17/233]	Loss 0.0005 (0.0012)	
training:	Epoch: [148][18/233]	Loss 0.0003 (0.0011)	
training:	Epoch: [148][19/233]	Loss 0.0027 (0.0012)	
training:	Epoch: [148][20/233]	Loss 0.0003 (0.0012)	
training:	Epoch: [148][21/233]	Loss 0.0513 (0.0036)	
training:	Epoch: [148][22/233]	Loss 0.0003 (0.0034)	
training:	Epoch: [148][23/233]	Loss 0.0004 (0.0033)	
training:	Epoch: [148][24/233]	Loss 0.0003 (0.0032)	
training:	Epoch: [148][25/233]	Loss 0.0004 (0.0030)	
training:	Epoch: [148][26/233]	Loss 0.0004 (0.0029)	
training:	Epoch: [148][27/233]	Loss 0.0003 (0.0028)	
training:	Epoch: [148][28/233]	Loss 0.0003 (0.0028)	
training:	Epoch: [148][29/233]	Loss 0.0004 (0.0027)	
training:	Epoch: [148][30/233]	Loss 0.0005 (0.0026)	
training:	Epoch: [148][31/233]	Loss 0.0004 (0.0025)	
training:	Epoch: [148][32/233]	Loss 0.0003 (0.0025)	
training:	Epoch: [148][33/233]	Loss 0.0005 (0.0024)	
training:	Epoch: [148][34/233]	Loss 0.0056 (0.0025)	
training:	Epoch: [148][35/233]	Loss 0.2097 (0.0084)	
training:	Epoch: [148][36/233]	Loss 0.0004 (0.0082)	
training:	Epoch: [148][37/233]	Loss 0.0007 (0.0080)	
training:	Epoch: [148][38/233]	Loss 0.0086 (0.0080)	
training:	Epoch: [148][39/233]	Loss 0.0003 (0.0078)	
training:	Epoch: [148][40/233]	Loss 0.0003 (0.0076)	
training:	Epoch: [148][41/233]	Loss 0.0003 (0.0074)	
training:	Epoch: [148][42/233]	Loss 0.0003 (0.0073)	
training:	Epoch: [148][43/233]	Loss 0.0003 (0.0071)	
training:	Epoch: [148][44/233]	Loss 0.0259 (0.0075)	
training:	Epoch: [148][45/233]	Loss 0.0004 (0.0074)	
training:	Epoch: [148][46/233]	Loss 0.0003 (0.0072)	
training:	Epoch: [148][47/233]	Loss 0.0005 (0.0071)	
training:	Epoch: [148][48/233]	Loss 0.0003 (0.0069)	
training:	Epoch: [148][49/233]	Loss 0.0003 (0.0068)	
training:	Epoch: [148][50/233]	Loss 0.0007 (0.0067)	
training:	Epoch: [148][51/233]	Loss 0.0003 (0.0066)	
training:	Epoch: [148][52/233]	Loss 0.0003 (0.0064)	
training:	Epoch: [148][53/233]	Loss 0.0004 (0.0063)	
training:	Epoch: [148][54/233]	Loss 0.0005 (0.0062)	
training:	Epoch: [148][55/233]	Loss 0.0003 (0.0061)	
training:	Epoch: [148][56/233]	Loss 0.0003 (0.0060)	
training:	Epoch: [148][57/233]	Loss 0.0003 (0.0059)	
training:	Epoch: [148][58/233]	Loss 0.0004 (0.0058)	
training:	Epoch: [148][59/233]	Loss 0.0005 (0.0057)	
training:	Epoch: [148][60/233]	Loss 0.0004 (0.0056)	
training:	Epoch: [148][61/233]	Loss 0.0007 (0.0055)	
training:	Epoch: [148][62/233]	Loss 0.0003 (0.0055)	
training:	Epoch: [148][63/233]	Loss 0.0005 (0.0054)	
training:	Epoch: [148][64/233]	Loss 0.0003 (0.0053)	
training:	Epoch: [148][65/233]	Loss 0.0004 (0.0052)	
training:	Epoch: [148][66/233]	Loss 0.0003 (0.0052)	
training:	Epoch: [148][67/233]	Loss 0.0017 (0.0051)	
training:	Epoch: [148][68/233]	Loss 0.0004 (0.0050)	
training:	Epoch: [148][69/233]	Loss 0.0003 (0.0050)	
training:	Epoch: [148][70/233]	Loss 0.0005 (0.0049)	
training:	Epoch: [148][71/233]	Loss 0.0003 (0.0048)	
training:	Epoch: [148][72/233]	Loss 0.0003 (0.0048)	
training:	Epoch: [148][73/233]	Loss 0.0003 (0.0047)	
training:	Epoch: [148][74/233]	Loss 0.0003 (0.0047)	
training:	Epoch: [148][75/233]	Loss 0.0014 (0.0046)	
training:	Epoch: [148][76/233]	Loss 0.0003 (0.0046)	
training:	Epoch: [148][77/233]	Loss 0.0004 (0.0045)	
training:	Epoch: [148][78/233]	Loss 0.0003 (0.0044)	
training:	Epoch: [148][79/233]	Loss 0.0003 (0.0044)	
training:	Epoch: [148][80/233]	Loss 0.0003 (0.0043)	
training:	Epoch: [148][81/233]	Loss 0.0003 (0.0043)	
training:	Epoch: [148][82/233]	Loss 0.0005 (0.0042)	
training:	Epoch: [148][83/233]	Loss 0.0006 (0.0042)	
training:	Epoch: [148][84/233]	Loss 0.0015 (0.0042)	
training:	Epoch: [148][85/233]	Loss 0.0003 (0.0041)	
training:	Epoch: [148][86/233]	Loss 0.0004 (0.0041)	
training:	Epoch: [148][87/233]	Loss 0.0004 (0.0040)	
training:	Epoch: [148][88/233]	Loss 0.0007 (0.0040)	
training:	Epoch: [148][89/233]	Loss 0.0003 (0.0040)	
training:	Epoch: [148][90/233]	Loss 0.0003 (0.0039)	
training:	Epoch: [148][91/233]	Loss 0.0015 (0.0039)	
training:	Epoch: [148][92/233]	Loss 0.0003 (0.0039)	
training:	Epoch: [148][93/233]	Loss 0.0004 (0.0038)	
training:	Epoch: [148][94/233]	Loss 0.0051 (0.0038)	
training:	Epoch: [148][95/233]	Loss 0.2127 (0.0060)	
training:	Epoch: [148][96/233]	Loss 0.0004 (0.0060)	
training:	Epoch: [148][97/233]	Loss 0.0004 (0.0059)	
training:	Epoch: [148][98/233]	Loss 0.0004 (0.0059)	
training:	Epoch: [148][99/233]	Loss 0.0003 (0.0058)	
training:	Epoch: [148][100/233]	Loss 0.0286 (0.0060)	
training:	Epoch: [148][101/233]	Loss 0.0004 (0.0060)	
training:	Epoch: [148][102/233]	Loss 0.0006 (0.0059)	
training:	Epoch: [148][103/233]	Loss 0.0003 (0.0059)	
training:	Epoch: [148][104/233]	Loss 0.0003 (0.0058)	
training:	Epoch: [148][105/233]	Loss 0.0003 (0.0058)	
training:	Epoch: [148][106/233]	Loss 0.0005 (0.0057)	
training:	Epoch: [148][107/233]	Loss 0.0004 (0.0057)	
training:	Epoch: [148][108/233]	Loss 0.0003 (0.0056)	
training:	Epoch: [148][109/233]	Loss 0.0032 (0.0056)	
training:	Epoch: [148][110/233]	Loss 0.0038 (0.0056)	
training:	Epoch: [148][111/233]	Loss 0.0004 (0.0055)	
training:	Epoch: [148][112/233]	Loss 0.0003 (0.0055)	
training:	Epoch: [148][113/233]	Loss 0.0003 (0.0054)	
training:	Epoch: [148][114/233]	Loss 0.0003 (0.0054)	
training:	Epoch: [148][115/233]	Loss 0.0004 (0.0053)	
training:	Epoch: [148][116/233]	Loss 0.0225 (0.0055)	
training:	Epoch: [148][117/233]	Loss 0.0003 (0.0054)	
training:	Epoch: [148][118/233]	Loss 0.0003 (0.0054)	
training:	Epoch: [148][119/233]	Loss 0.0055 (0.0054)	
training:	Epoch: [148][120/233]	Loss 0.0004 (0.0054)	
training:	Epoch: [148][121/233]	Loss 0.0003 (0.0053)	
training:	Epoch: [148][122/233]	Loss 0.0003 (0.0053)	
training:	Epoch: [148][123/233]	Loss 0.0004 (0.0052)	
training:	Epoch: [148][124/233]	Loss 0.0003 (0.0052)	
training:	Epoch: [148][125/233]	Loss 0.0003 (0.0052)	
training:	Epoch: [148][126/233]	Loss 0.0044 (0.0052)	
training:	Epoch: [148][127/233]	Loss 0.0003 (0.0051)	
training:	Epoch: [148][128/233]	Loss 0.0003 (0.0051)	
training:	Epoch: [148][129/233]	Loss 0.0005 (0.0050)	
training:	Epoch: [148][130/233]	Loss 0.0003 (0.0050)	
training:	Epoch: [148][131/233]	Loss 0.0011 (0.0050)	
training:	Epoch: [148][132/233]	Loss 0.0005 (0.0049)	
training:	Epoch: [148][133/233]	Loss 0.0021 (0.0049)	
training:	Epoch: [148][134/233]	Loss 0.0005 (0.0049)	
training:	Epoch: [148][135/233]	Loss 0.0003 (0.0049)	
training:	Epoch: [148][136/233]	Loss 0.0003 (0.0048)	
training:	Epoch: [148][137/233]	Loss 0.0003 (0.0048)	
training:	Epoch: [148][138/233]	Loss 0.0005 (0.0048)	
training:	Epoch: [148][139/233]	Loss 0.0003 (0.0047)	
training:	Epoch: [148][140/233]	Loss 0.0118 (0.0048)	
training:	Epoch: [148][141/233]	Loss 0.0022 (0.0048)	
training:	Epoch: [148][142/233]	Loss 0.0003 (0.0047)	
training:	Epoch: [148][143/233]	Loss 0.0003 (0.0047)	
training:	Epoch: [148][144/233]	Loss 0.0003 (0.0047)	
training:	Epoch: [148][145/233]	Loss 0.0004 (0.0046)	
training:	Epoch: [148][146/233]	Loss 0.0004 (0.0046)	
training:	Epoch: [148][147/233]	Loss 0.0003 (0.0046)	
training:	Epoch: [148][148/233]	Loss 0.0003 (0.0045)	
training:	Epoch: [148][149/233]	Loss 0.0039 (0.0045)	
training:	Epoch: [148][150/233]	Loss 0.0002 (0.0045)	
training:	Epoch: [148][151/233]	Loss 0.0009 (0.0045)	
training:	Epoch: [148][152/233]	Loss 0.0003 (0.0045)	
training:	Epoch: [148][153/233]	Loss 0.0018 (0.0044)	
training:	Epoch: [148][154/233]	Loss 0.0013 (0.0044)	
training:	Epoch: [148][155/233]	Loss 0.0003 (0.0044)	
training:	Epoch: [148][156/233]	Loss 0.0003 (0.0044)	
training:	Epoch: [148][157/233]	Loss 0.0003 (0.0043)	
training:	Epoch: [148][158/233]	Loss 0.0007 (0.0043)	
training:	Epoch: [148][159/233]	Loss 0.0003 (0.0043)	
training:	Epoch: [148][160/233]	Loss 0.0007 (0.0043)	
training:	Epoch: [148][161/233]	Loss 0.0003 (0.0043)	
training:	Epoch: [148][162/233]	Loss 0.0005 (0.0042)	
training:	Epoch: [148][163/233]	Loss 0.0005 (0.0042)	
training:	Epoch: [148][164/233]	Loss 0.0020 (0.0042)	
training:	Epoch: [148][165/233]	Loss 0.0003 (0.0042)	
training:	Epoch: [148][166/233]	Loss 0.0010 (0.0041)	
training:	Epoch: [148][167/233]	Loss 0.0009 (0.0041)	
training:	Epoch: [148][168/233]	Loss 0.0004 (0.0041)	
training:	Epoch: [148][169/233]	Loss 0.0002 (0.0041)	
training:	Epoch: [148][170/233]	Loss 0.0003 (0.0041)	
training:	Epoch: [148][171/233]	Loss 0.0003 (0.0040)	
training:	Epoch: [148][172/233]	Loss 0.0004 (0.0040)	
training:	Epoch: [148][173/233]	Loss 0.0004 (0.0040)	
training:	Epoch: [148][174/233]	Loss 0.0005 (0.0040)	
training:	Epoch: [148][175/233]	Loss 0.0003 (0.0040)	
training:	Epoch: [148][176/233]	Loss 0.0004 (0.0039)	
training:	Epoch: [148][177/233]	Loss 0.0158 (0.0040)	
training:	Epoch: [148][178/233]	Loss 0.0003 (0.0040)	
training:	Epoch: [148][179/233]	Loss 0.0003 (0.0040)	
training:	Epoch: [148][180/233]	Loss 0.0003 (0.0039)	
training:	Epoch: [148][181/233]	Loss 0.0003 (0.0039)	
training:	Epoch: [148][182/233]	Loss 0.0003 (0.0039)	
training:	Epoch: [148][183/233]	Loss 0.0003 (0.0039)	
training:	Epoch: [148][184/233]	Loss 0.0008 (0.0039)	
training:	Epoch: [148][185/233]	Loss 0.0003 (0.0038)	
training:	Epoch: [148][186/233]	Loss 0.0004 (0.0038)	
training:	Epoch: [148][187/233]	Loss 0.0003 (0.0038)	
training:	Epoch: [148][188/233]	Loss 0.0003 (0.0038)	
training:	Epoch: [148][189/233]	Loss 0.0003 (0.0038)	
training:	Epoch: [148][190/233]	Loss 0.0003 (0.0038)	
training:	Epoch: [148][191/233]	Loss 0.0003 (0.0037)	
training:	Epoch: [148][192/233]	Loss 0.0036 (0.0037)	
training:	Epoch: [148][193/233]	Loss 0.0003 (0.0037)	
training:	Epoch: [148][194/233]	Loss 0.0003 (0.0037)	
training:	Epoch: [148][195/233]	Loss 0.0247 (0.0038)	
training:	Epoch: [148][196/233]	Loss 0.0003 (0.0038)	
training:	Epoch: [148][197/233]	Loss 0.0190 (0.0039)	
training:	Epoch: [148][198/233]	Loss 0.0053 (0.0039)	
training:	Epoch: [148][199/233]	Loss 0.0003 (0.0039)	
training:	Epoch: [148][200/233]	Loss 0.0003 (0.0038)	
training:	Epoch: [148][201/233]	Loss 0.0003 (0.0038)	
training:	Epoch: [148][202/233]	Loss 0.0003 (0.0038)	
training:	Epoch: [148][203/233]	Loss 0.0003 (0.0038)	
training:	Epoch: [148][204/233]	Loss 0.0003 (0.0038)	
training:	Epoch: [148][205/233]	Loss 0.0003 (0.0038)	
training:	Epoch: [148][206/233]	Loss 0.0003 (0.0037)	
training:	Epoch: [148][207/233]	Loss 0.0029 (0.0037)	
training:	Epoch: [148][208/233]	Loss 0.0009 (0.0037)	
training:	Epoch: [148][209/233]	Loss 0.0017 (0.0037)	
training:	Epoch: [148][210/233]	Loss 0.0005 (0.0037)	
training:	Epoch: [148][211/233]	Loss 0.0003 (0.0037)	
training:	Epoch: [148][212/233]	Loss 0.0005 (0.0037)	
training:	Epoch: [148][213/233]	Loss 0.0003 (0.0036)	
training:	Epoch: [148][214/233]	Loss 0.0005 (0.0036)	
training:	Epoch: [148][215/233]	Loss 0.0004 (0.0036)	
training:	Epoch: [148][216/233]	Loss 0.0003 (0.0036)	
training:	Epoch: [148][217/233]	Loss 0.0003 (0.0036)	
training:	Epoch: [148][218/233]	Loss 0.0009 (0.0036)	
training:	Epoch: [148][219/233]	Loss 0.0003 (0.0036)	
training:	Epoch: [148][220/233]	Loss 0.0007 (0.0035)	
training:	Epoch: [148][221/233]	Loss 0.0003 (0.0035)	
training:	Epoch: [148][222/233]	Loss 0.0003 (0.0035)	
training:	Epoch: [148][223/233]	Loss 0.0013 (0.0035)	
training:	Epoch: [148][224/233]	Loss 0.0038 (0.0035)	
training:	Epoch: [148][225/233]	Loss 0.0003 (0.0035)	
training:	Epoch: [148][226/233]	Loss 0.0064 (0.0035)	
training:	Epoch: [148][227/233]	Loss 0.0005 (0.0035)	
training:	Epoch: [148][228/233]	Loss 0.0003 (0.0035)	
training:	Epoch: [148][229/233]	Loss 0.0006 (0.0035)	
training:	Epoch: [148][230/233]	Loss 0.0005 (0.0035)	
training:	Epoch: [148][231/233]	Loss 0.0003 (0.0034)	
training:	Epoch: [148][232/233]	Loss 0.0003 (0.0034)	
training:	Epoch: [148][233/233]	Loss 0.0007 (0.0034)	
Training:	 Loss: 0.0034

Training:	 ACC: 1.0000 1.0000 1.0000 1.0000
Validation:	 ACC: 0.7916 0.7913 0.7865 0.7966
Validation:	 Best_BACC: 0.8162 0.8154 0.7998 0.8326
Validation:	 Loss: 1.3105
Pretraining:	Epoch 149/200
----------
training:	Epoch: [149][1/233]	Loss 0.0003 (0.0003)	
training:	Epoch: [149][2/233]	Loss 0.0003 (0.0003)	
training:	Epoch: [149][3/233]	Loss 0.0003 (0.0003)	
training:	Epoch: [149][4/233]	Loss 0.0390 (0.0100)	
training:	Epoch: [149][5/233]	Loss 0.0007 (0.0081)	
training:	Epoch: [149][6/233]	Loss 0.0003 (0.0068)	
training:	Epoch: [149][7/233]	Loss 0.0004 (0.0059)	
training:	Epoch: [149][8/233]	Loss 0.0002 (0.0052)	
training:	Epoch: [149][9/233]	Loss 0.0006 (0.0047)	
training:	Epoch: [149][10/233]	Loss 0.0006 (0.0043)	
training:	Epoch: [149][11/233]	Loss 0.0009 (0.0040)	
training:	Epoch: [149][12/233]	Loss 0.0005 (0.0037)	
training:	Epoch: [149][13/233]	Loss 0.0003 (0.0034)	
training:	Epoch: [149][14/233]	Loss 0.0003 (0.0032)	
training:	Epoch: [149][15/233]	Loss 0.0008 (0.0030)	
training:	Epoch: [149][16/233]	Loss 0.0004 (0.0029)	
training:	Epoch: [149][17/233]	Loss 0.0022 (0.0028)	
training:	Epoch: [149][18/233]	Loss 0.0005 (0.0027)	
training:	Epoch: [149][19/233]	Loss 0.0003 (0.0026)	
training:	Epoch: [149][20/233]	Loss 0.0017 (0.0025)	
training:	Epoch: [149][21/233]	Loss 0.0003 (0.0024)	
training:	Epoch: [149][22/233]	Loss 0.0003 (0.0023)	
training:	Epoch: [149][23/233]	Loss 0.0003 (0.0022)	
training:	Epoch: [149][24/233]	Loss 0.0005 (0.0022)	
training:	Epoch: [149][25/233]	Loss 0.0004 (0.0021)	
training:	Epoch: [149][26/233]	Loss 0.0003 (0.0020)	
training:	Epoch: [149][27/233]	Loss 0.0003 (0.0020)	
training:	Epoch: [149][28/233]	Loss 0.0004 (0.0019)	
training:	Epoch: [149][29/233]	Loss 0.0003 (0.0019)	
training:	Epoch: [149][30/233]	Loss 0.0005 (0.0018)	
training:	Epoch: [149][31/233]	Loss 0.0003 (0.0018)	
training:	Epoch: [149][32/233]	Loss 0.0004 (0.0017)	
training:	Epoch: [149][33/233]	Loss 0.0003 (0.0017)	
training:	Epoch: [149][34/233]	Loss 0.0004 (0.0016)	
training:	Epoch: [149][35/233]	Loss 0.0009 (0.0016)	
training:	Epoch: [149][36/233]	Loss 0.0003 (0.0016)	
training:	Epoch: [149][37/233]	Loss 0.0003 (0.0015)	
training:	Epoch: [149][38/233]	Loss 0.0003 (0.0015)	
training:	Epoch: [149][39/233]	Loss 0.0040 (0.0016)	
training:	Epoch: [149][40/233]	Loss 0.0004 (0.0015)	
training:	Epoch: [149][41/233]	Loss 0.0003 (0.0015)	
training:	Epoch: [149][42/233]	Loss 0.0006 (0.0015)	
training:	Epoch: [149][43/233]	Loss 0.0008 (0.0015)	
training:	Epoch: [149][44/233]	Loss 0.0005 (0.0015)	
training:	Epoch: [149][45/233]	Loss 0.0248 (0.0020)	
training:	Epoch: [149][46/233]	Loss 0.0003 (0.0019)	
training:	Epoch: [149][47/233]	Loss 0.0003 (0.0019)	
training:	Epoch: [149][48/233]	Loss 0.0003 (0.0019)	
training:	Epoch: [149][49/233]	Loss 0.0004 (0.0018)	
training:	Epoch: [149][50/233]	Loss 0.0003 (0.0018)	
training:	Epoch: [149][51/233]	Loss 0.0051 (0.0019)	
training:	Epoch: [149][52/233]	Loss 0.0003 (0.0018)	
training:	Epoch: [149][53/233]	Loss 0.0003 (0.0018)	
training:	Epoch: [149][54/233]	Loss 0.0002 (0.0018)	
training:	Epoch: [149][55/233]	Loss 0.0003 (0.0018)	
training:	Epoch: [149][56/233]	Loss 0.0003 (0.0017)	
training:	Epoch: [149][57/233]	Loss 0.1347 (0.0041)	
training:	Epoch: [149][58/233]	Loss 0.0003 (0.0040)	
training:	Epoch: [149][59/233]	Loss 0.0066 (0.0040)	
training:	Epoch: [149][60/233]	Loss 0.0013 (0.0040)	
training:	Epoch: [149][61/233]	Loss 0.0003 (0.0039)	
training:	Epoch: [149][62/233]	Loss 0.0020 (0.0039)	
training:	Epoch: [149][63/233]	Loss 0.0009 (0.0039)	
training:	Epoch: [149][64/233]	Loss 0.0004 (0.0038)	
training:	Epoch: [149][65/233]	Loss 0.0003 (0.0038)	
training:	Epoch: [149][66/233]	Loss 0.0005 (0.0037)	
training:	Epoch: [149][67/233]	Loss 0.0003 (0.0037)	
training:	Epoch: [149][68/233]	Loss 0.0687 (0.0046)	
training:	Epoch: [149][69/233]	Loss 0.0185 (0.0048)	
training:	Epoch: [149][70/233]	Loss 0.0420 (0.0053)	
training:	Epoch: [149][71/233]	Loss 0.0003 (0.0053)	
training:	Epoch: [149][72/233]	Loss 0.0004 (0.0052)	
training:	Epoch: [149][73/233]	Loss 0.0004 (0.0051)	
training:	Epoch: [149][74/233]	Loss 0.0005 (0.0051)	
training:	Epoch: [149][75/233]	Loss 0.0003 (0.0050)	
training:	Epoch: [149][76/233]	Loss 0.0003 (0.0049)	
training:	Epoch: [149][77/233]	Loss 0.0200 (0.0051)	
training:	Epoch: [149][78/233]	Loss 0.0008 (0.0051)	
training:	Epoch: [149][79/233]	Loss 0.0076 (0.0051)	
training:	Epoch: [149][80/233]	Loss 0.0003 (0.0051)	
training:	Epoch: [149][81/233]	Loss 0.0003 (0.0050)	
training:	Epoch: [149][82/233]	Loss 0.0003 (0.0049)	
training:	Epoch: [149][83/233]	Loss 0.0005 (0.0049)	
training:	Epoch: [149][84/233]	Loss 0.0010 (0.0048)	
training:	Epoch: [149][85/233]	Loss 0.0003 (0.0048)	
training:	Epoch: [149][86/233]	Loss 0.0003 (0.0047)	
training:	Epoch: [149][87/233]	Loss 0.0005 (0.0047)	
training:	Epoch: [149][88/233]	Loss 0.0003 (0.0046)	
training:	Epoch: [149][89/233]	Loss 0.0003 (0.0046)	
training:	Epoch: [149][90/233]	Loss 0.0025 (0.0046)	
training:	Epoch: [149][91/233]	Loss 0.0005 (0.0045)	
training:	Epoch: [149][92/233]	Loss 0.0004 (0.0045)	
training:	Epoch: [149][93/233]	Loss 0.0003 (0.0044)	
training:	Epoch: [149][94/233]	Loss 0.0100 (0.0045)	
training:	Epoch: [149][95/233]	Loss 0.0049 (0.0045)	
training:	Epoch: [149][96/233]	Loss 0.0005 (0.0045)	
training:	Epoch: [149][97/233]	Loss 0.0003 (0.0044)	
training:	Epoch: [149][98/233]	Loss 0.0003 (0.0044)	
training:	Epoch: [149][99/233]	Loss 0.0003 (0.0043)	
training:	Epoch: [149][100/233]	Loss 0.0003 (0.0043)	
training:	Epoch: [149][101/233]	Loss 0.0004 (0.0042)	
training:	Epoch: [149][102/233]	Loss 0.0003 (0.0042)	
training:	Epoch: [149][103/233]	Loss 0.0003 (0.0042)	
training:	Epoch: [149][104/233]	Loss 0.0004 (0.0041)	
training:	Epoch: [149][105/233]	Loss 0.0003 (0.0041)	
training:	Epoch: [149][106/233]	Loss 0.0003 (0.0041)	
training:	Epoch: [149][107/233]	Loss 0.0004 (0.0040)	
training:	Epoch: [149][108/233]	Loss 0.0414 (0.0044)	
training:	Epoch: [149][109/233]	Loss 0.0003 (0.0043)	
training:	Epoch: [149][110/233]	Loss 0.0003 (0.0043)	
training:	Epoch: [149][111/233]	Loss 0.0004 (0.0043)	
training:	Epoch: [149][112/233]	Loss 0.0003 (0.0042)	
training:	Epoch: [149][113/233]	Loss 0.0004 (0.0042)	
training:	Epoch: [149][114/233]	Loss 0.0003 (0.0042)	
training:	Epoch: [149][115/233]	Loss 0.0016 (0.0041)	
training:	Epoch: [149][116/233]	Loss 0.0004 (0.0041)	
training:	Epoch: [149][117/233]	Loss 0.0003 (0.0041)	
training:	Epoch: [149][118/233]	Loss 0.0013 (0.0041)	
training:	Epoch: [149][119/233]	Loss 0.0042 (0.0041)	
training:	Epoch: [149][120/233]	Loss 0.0004 (0.0040)	
training:	Epoch: [149][121/233]	Loss 0.0005 (0.0040)	
training:	Epoch: [149][122/233]	Loss 0.0003 (0.0040)	
training:	Epoch: [149][123/233]	Loss 0.0003 (0.0039)	
training:	Epoch: [149][124/233]	Loss 0.0003 (0.0039)	
training:	Epoch: [149][125/233]	Loss 0.0003 (0.0039)	
training:	Epoch: [149][126/233]	Loss 0.0003 (0.0038)	
training:	Epoch: [149][127/233]	Loss 0.0004 (0.0038)	
training:	Epoch: [149][128/233]	Loss 0.0003 (0.0038)	
training:	Epoch: [149][129/233]	Loss 0.0003 (0.0038)	
training:	Epoch: [149][130/233]	Loss 0.1595 (0.0050)	
training:	Epoch: [149][131/233]	Loss 0.0007 (0.0049)	
training:	Epoch: [149][132/233]	Loss 0.0008 (0.0049)	
training:	Epoch: [149][133/233]	Loss 0.0003 (0.0049)	
training:	Epoch: [149][134/233]	Loss 0.0009 (0.0048)	
training:	Epoch: [149][135/233]	Loss 0.0003 (0.0048)	
training:	Epoch: [149][136/233]	Loss 0.0004 (0.0048)	
training:	Epoch: [149][137/233]	Loss 0.0023 (0.0047)	
training:	Epoch: [149][138/233]	Loss 0.0003 (0.0047)	
training:	Epoch: [149][139/233]	Loss 0.0004 (0.0047)	
training:	Epoch: [149][140/233]	Loss 0.0003 (0.0047)	
training:	Epoch: [149][141/233]	Loss 0.0007 (0.0046)	
training:	Epoch: [149][142/233]	Loss 0.0004 (0.0046)	
training:	Epoch: [149][143/233]	Loss 0.0008 (0.0046)	
training:	Epoch: [149][144/233]	Loss 0.0003 (0.0045)	
training:	Epoch: [149][145/233]	Loss 0.0003 (0.0045)	
training:	Epoch: [149][146/233]	Loss 0.0003 (0.0045)	
training:	Epoch: [149][147/233]	Loss 0.0004 (0.0045)	
training:	Epoch: [149][148/233]	Loss 0.0004 (0.0044)	
training:	Epoch: [149][149/233]	Loss 0.0024 (0.0044)	
training:	Epoch: [149][150/233]	Loss 0.0006 (0.0044)	
training:	Epoch: [149][151/233]	Loss 0.0004 (0.0044)	
training:	Epoch: [149][152/233]	Loss 0.0005 (0.0043)	
training:	Epoch: [149][153/233]	Loss 0.0003 (0.0043)	
training:	Epoch: [149][154/233]	Loss 0.0003 (0.0043)	
training:	Epoch: [149][155/233]	Loss 0.0003 (0.0043)	
training:	Epoch: [149][156/233]	Loss 0.0007 (0.0042)	
training:	Epoch: [149][157/233]	Loss 0.0004 (0.0042)	
training:	Epoch: [149][158/233]	Loss 0.0003 (0.0042)	
training:	Epoch: [149][159/233]	Loss 0.0004 (0.0042)	
training:	Epoch: [149][160/233]	Loss 0.0005 (0.0041)	
training:	Epoch: [149][161/233]	Loss 0.0055 (0.0041)	
training:	Epoch: [149][162/233]	Loss 0.0003 (0.0041)	
training:	Epoch: [149][163/233]	Loss 0.0013 (0.0041)	
training:	Epoch: [149][164/233]	Loss 0.0008 (0.0041)	
training:	Epoch: [149][165/233]	Loss 0.0007 (0.0041)	
training:	Epoch: [149][166/233]	Loss 0.0042 (0.0041)	
training:	Epoch: [149][167/233]	Loss 0.0004 (0.0040)	
training:	Epoch: [149][168/233]	Loss 0.0003 (0.0040)	
training:	Epoch: [149][169/233]	Loss 0.0012 (0.0040)	
training:	Epoch: [149][170/233]	Loss 0.1719 (0.0050)	
training:	Epoch: [149][171/233]	Loss 0.0003 (0.0050)	
training:	Epoch: [149][172/233]	Loss 0.0004 (0.0049)	
training:	Epoch: [149][173/233]	Loss 0.0689 (0.0053)	
training:	Epoch: [149][174/233]	Loss 0.0009 (0.0053)	
training:	Epoch: [149][175/233]	Loss 0.0003 (0.0053)	
training:	Epoch: [149][176/233]	Loss 0.0007 (0.0052)	
training:	Epoch: [149][177/233]	Loss 0.0005 (0.0052)	
training:	Epoch: [149][178/233]	Loss 0.0041 (0.0052)	
training:	Epoch: [149][179/233]	Loss 0.0007 (0.0052)	
training:	Epoch: [149][180/233]	Loss 0.0009 (0.0051)	
training:	Epoch: [149][181/233]	Loss 0.0003 (0.0051)	
training:	Epoch: [149][182/233]	Loss 0.0003 (0.0051)	
training:	Epoch: [149][183/233]	Loss 0.0044 (0.0051)	
training:	Epoch: [149][184/233]	Loss 0.0015 (0.0051)	
training:	Epoch: [149][185/233]	Loss 0.0003 (0.0050)	
training:	Epoch: [149][186/233]	Loss 0.0004 (0.0050)	
training:	Epoch: [149][187/233]	Loss 0.0011 (0.0050)	
training:	Epoch: [149][188/233]	Loss 0.0006 (0.0050)	
training:	Epoch: [149][189/233]	Loss 0.0017 (0.0050)	
training:	Epoch: [149][190/233]	Loss 0.0015 (0.0049)	
training:	Epoch: [149][191/233]	Loss 0.0018 (0.0049)	
training:	Epoch: [149][192/233]	Loss 0.0003 (0.0049)	
training:	Epoch: [149][193/233]	Loss 0.0339 (0.0051)	
training:	Epoch: [149][194/233]	Loss 0.0010 (0.0050)	
training:	Epoch: [149][195/233]	Loss 0.0079 (0.0050)	
training:	Epoch: [149][196/233]	Loss 0.0024 (0.0050)	
training:	Epoch: [149][197/233]	Loss 0.0260 (0.0051)	
training:	Epoch: [149][198/233]	Loss 0.0003 (0.0051)	
training:	Epoch: [149][199/233]	Loss 0.0005 (0.0051)	
training:	Epoch: [149][200/233]	Loss 0.0008 (0.0051)	
training:	Epoch: [149][201/233]	Loss 0.0006 (0.0050)	
training:	Epoch: [149][202/233]	Loss 0.0004 (0.0050)	
training:	Epoch: [149][203/233]	Loss 0.0006 (0.0050)	
training:	Epoch: [149][204/233]	Loss 0.0003 (0.0050)	
training:	Epoch: [149][205/233]	Loss 0.0003 (0.0050)	
training:	Epoch: [149][206/233]	Loss 0.0005 (0.0049)	
training:	Epoch: [149][207/233]	Loss 0.0003 (0.0049)	
training:	Epoch: [149][208/233]	Loss 0.0006 (0.0049)	
training:	Epoch: [149][209/233]	Loss 0.0003 (0.0049)	
training:	Epoch: [149][210/233]	Loss 0.0003 (0.0048)	
training:	Epoch: [149][211/233]	Loss 0.0003 (0.0048)	
training:	Epoch: [149][212/233]	Loss 0.0003 (0.0048)	
training:	Epoch: [149][213/233]	Loss 0.0563 (0.0050)	
training:	Epoch: [149][214/233]	Loss 0.0003 (0.0050)	
training:	Epoch: [149][215/233]	Loss 0.0005 (0.0050)	
training:	Epoch: [149][216/233]	Loss 0.0003 (0.0050)	
training:	Epoch: [149][217/233]	Loss 0.0040 (0.0050)	
training:	Epoch: [149][218/233]	Loss 0.0004 (0.0050)	
training:	Epoch: [149][219/233]	Loss 0.0010 (0.0049)	
training:	Epoch: [149][220/233]	Loss 0.0025 (0.0049)	
training:	Epoch: [149][221/233]	Loss 0.1788 (0.0057)	
training:	Epoch: [149][222/233]	Loss 0.0003 (0.0057)	
training:	Epoch: [149][223/233]	Loss 0.0003 (0.0057)	
training:	Epoch: [149][224/233]	Loss 0.0003 (0.0056)	
training:	Epoch: [149][225/233]	Loss 0.0005 (0.0056)	
training:	Epoch: [149][226/233]	Loss 0.0004 (0.0056)	
training:	Epoch: [149][227/233]	Loss 0.0003 (0.0056)	
training:	Epoch: [149][228/233]	Loss 0.0013 (0.0056)	
training:	Epoch: [149][229/233]	Loss 0.0013 (0.0055)	
training:	Epoch: [149][230/233]	Loss 0.0011 (0.0055)	
training:	Epoch: [149][231/233]	Loss 0.0004 (0.0055)	
training:	Epoch: [149][232/233]	Loss 0.0003 (0.0055)	
training:	Epoch: [149][233/233]	Loss 0.0007 (0.0054)	
Training:	 Loss: 0.0054

Training:	 ACC: 0.9996 0.9996 1.0000 0.9992
Validation:	 ACC: 0.7956 0.7972 0.8294 0.7618
Validation:	 Best_BACC: 0.8162 0.8154 0.7998 0.8326
Validation:	 Loss: 1.2442
Pretraining:	Epoch 150/200
----------
training:	Epoch: [150][1/233]	Loss 0.0045 (0.0045)	
training:	Epoch: [150][2/233]	Loss 0.0003 (0.0024)	
training:	Epoch: [150][3/233]	Loss 0.0004 (0.0017)	
training:	Epoch: [150][4/233]	Loss 0.0009 (0.0015)	
training:	Epoch: [150][5/233]	Loss 0.0003 (0.0013)	
training:	Epoch: [150][6/233]	Loss 0.0003 (0.0011)	
training:	Epoch: [150][7/233]	Loss 0.0009 (0.0011)	
training:	Epoch: [150][8/233]	Loss 0.0006 (0.0010)	
training:	Epoch: [150][9/233]	Loss 0.0004 (0.0010)	
training:	Epoch: [150][10/233]	Loss 0.0003 (0.0009)	
training:	Epoch: [150][11/233]	Loss 0.0003 (0.0008)	
training:	Epoch: [150][12/233]	Loss 0.0008 (0.0008)	
training:	Epoch: [150][13/233]	Loss 0.0003 (0.0008)	
training:	Epoch: [150][14/233]	Loss 0.0012 (0.0008)	
training:	Epoch: [150][15/233]	Loss 0.0010 (0.0008)	
training:	Epoch: [150][16/233]	Loss 0.0046 (0.0011)	
training:	Epoch: [150][17/233]	Loss 0.0003 (0.0010)	
training:	Epoch: [150][18/233]	Loss 0.0012 (0.0010)	
training:	Epoch: [150][19/233]	Loss 0.0003 (0.0010)	
training:	Epoch: [150][20/233]	Loss 0.0003 (0.0010)	
training:	Epoch: [150][21/233]	Loss 0.0006 (0.0009)	
training:	Epoch: [150][22/233]	Loss 0.0005 (0.0009)	
training:	Epoch: [150][23/233]	Loss 0.0003 (0.0009)	
training:	Epoch: [150][24/233]	Loss 0.0005 (0.0009)	
training:	Epoch: [150][25/233]	Loss 0.0003 (0.0009)	
training:	Epoch: [150][26/233]	Loss 0.0423 (0.0025)	
training:	Epoch: [150][27/233]	Loss 0.0005 (0.0024)	
training:	Epoch: [150][28/233]	Loss 0.1463 (0.0075)	
training:	Epoch: [150][29/233]	Loss 0.0003 (0.0073)	
training:	Epoch: [150][30/233]	Loss 0.0010 (0.0071)	
training:	Epoch: [150][31/233]	Loss 0.0003 (0.0068)	
training:	Epoch: [150][32/233]	Loss 0.0006 (0.0066)	
training:	Epoch: [150][33/233]	Loss 0.0004 (0.0065)	
training:	Epoch: [150][34/233]	Loss 0.0003 (0.0063)	
training:	Epoch: [150][35/233]	Loss 0.0003 (0.0061)	
training:	Epoch: [150][36/233]	Loss 0.0003 (0.0059)	
training:	Epoch: [150][37/233]	Loss 0.0008 (0.0058)	
training:	Epoch: [150][38/233]	Loss 0.0003 (0.0057)	
training:	Epoch: [150][39/233]	Loss 0.0004 (0.0055)	
training:	Epoch: [150][40/233]	Loss 0.0007 (0.0054)	
training:	Epoch: [150][41/233]	Loss 0.0011 (0.0053)	
training:	Epoch: [150][42/233]	Loss 0.0003 (0.0052)	
training:	Epoch: [150][43/233]	Loss 0.0003 (0.0051)	
training:	Epoch: [150][44/233]	Loss 0.0003 (0.0050)	
training:	Epoch: [150][45/233]	Loss 0.0003 (0.0049)	
training:	Epoch: [150][46/233]	Loss 0.0005 (0.0048)	
training:	Epoch: [150][47/233]	Loss 0.0060 (0.0048)	
training:	Epoch: [150][48/233]	Loss 0.0003 (0.0047)	
training:	Epoch: [150][49/233]	Loss 0.0112 (0.0048)	
training:	Epoch: [150][50/233]	Loss 0.0003 (0.0047)	
training:	Epoch: [150][51/233]	Loss 0.0005 (0.0047)	
training:	Epoch: [150][52/233]	Loss 0.0005 (0.0046)	
training:	Epoch: [150][53/233]	Loss 0.0006 (0.0045)	
training:	Epoch: [150][54/233]	Loss 0.0014 (0.0044)	
training:	Epoch: [150][55/233]	Loss 0.0004 (0.0044)	
training:	Epoch: [150][56/233]	Loss 0.1415 (0.0068)	
training:	Epoch: [150][57/233]	Loss 0.0017 (0.0067)	
training:	Epoch: [150][58/233]	Loss 0.0003 (0.0066)	
training:	Epoch: [150][59/233]	Loss 0.0003 (0.0065)	
training:	Epoch: [150][60/233]	Loss 0.0003 (0.0064)	
training:	Epoch: [150][61/233]	Loss 0.0005 (0.0063)	
training:	Epoch: [150][62/233]	Loss 0.0004 (0.0062)	
training:	Epoch: [150][63/233]	Loss 0.0003 (0.0061)	
training:	Epoch: [150][64/233]	Loss 0.0006 (0.0060)	
training:	Epoch: [150][65/233]	Loss 0.0004 (0.0059)	
training:	Epoch: [150][66/233]	Loss 0.0003 (0.0059)	
training:	Epoch: [150][67/233]	Loss 0.0065 (0.0059)	
training:	Epoch: [150][68/233]	Loss 0.0006 (0.0058)	
training:	Epoch: [150][69/233]	Loss 0.0012 (0.0057)	
training:	Epoch: [150][70/233]	Loss 0.0006 (0.0057)	
training:	Epoch: [150][71/233]	Loss 0.0003 (0.0056)	
training:	Epoch: [150][72/233]	Loss 0.0003 (0.0055)	
training:	Epoch: [150][73/233]	Loss 0.0006 (0.0054)	
training:	Epoch: [150][74/233]	Loss 0.0003 (0.0054)	
training:	Epoch: [150][75/233]	Loss 0.0004 (0.0053)	
training:	Epoch: [150][76/233]	Loss 0.0003 (0.0052)	
training:	Epoch: [150][77/233]	Loss 0.0014 (0.0052)	
training:	Epoch: [150][78/233]	Loss 0.0004 (0.0051)	
training:	Epoch: [150][79/233]	Loss 0.0003 (0.0051)	
training:	Epoch: [150][80/233]	Loss 0.0003 (0.0050)	
training:	Epoch: [150][81/233]	Loss 0.0003 (0.0049)	
training:	Epoch: [150][82/233]	Loss 0.0004 (0.0049)	
training:	Epoch: [150][83/233]	Loss 0.0005 (0.0048)	
training:	Epoch: [150][84/233]	Loss 0.0005 (0.0048)	
training:	Epoch: [150][85/233]	Loss 0.0010 (0.0047)	
training:	Epoch: [150][86/233]	Loss 0.0003 (0.0047)	
training:	Epoch: [150][87/233]	Loss 0.0003 (0.0046)	
training:	Epoch: [150][88/233]	Loss 0.0015 (0.0046)	
training:	Epoch: [150][89/233]	Loss 0.0138 (0.0047)	
training:	Epoch: [150][90/233]	Loss 0.0004 (0.0047)	
training:	Epoch: [150][91/233]	Loss 0.0076 (0.0047)	
training:	Epoch: [150][92/233]	Loss 0.0003 (0.0046)	
training:	Epoch: [150][93/233]	Loss 0.0005 (0.0046)	
training:	Epoch: [150][94/233]	Loss 0.0007 (0.0046)	
training:	Epoch: [150][95/233]	Loss 0.0003 (0.0045)	
training:	Epoch: [150][96/233]	Loss 0.0003 (0.0045)	
training:	Epoch: [150][97/233]	Loss 0.0004 (0.0044)	
training:	Epoch: [150][98/233]	Loss 0.0004 (0.0044)	
training:	Epoch: [150][99/233]	Loss 0.0007 (0.0043)	
training:	Epoch: [150][100/233]	Loss 0.0013 (0.0043)	
training:	Epoch: [150][101/233]	Loss 0.0008 (0.0043)	
training:	Epoch: [150][102/233]	Loss 0.0005 (0.0042)	
training:	Epoch: [150][103/233]	Loss 0.0003 (0.0042)	
training:	Epoch: [150][104/233]	Loss 0.0004 (0.0042)	
training:	Epoch: [150][105/233]	Loss 0.0003 (0.0041)	
training:	Epoch: [150][106/233]	Loss 0.0004 (0.0041)	
training:	Epoch: [150][107/233]	Loss 0.0005 (0.0041)	
training:	Epoch: [150][108/233]	Loss 0.0003 (0.0040)	
training:	Epoch: [150][109/233]	Loss 0.0003 (0.0040)	
training:	Epoch: [150][110/233]	Loss 0.0003 (0.0040)	
training:	Epoch: [150][111/233]	Loss 0.0004 (0.0039)	
training:	Epoch: [150][112/233]	Loss 0.0003 (0.0039)	
training:	Epoch: [150][113/233]	Loss 0.0003 (0.0039)	
training:	Epoch: [150][114/233]	Loss 0.0006 (0.0038)	
training:	Epoch: [150][115/233]	Loss 0.0003 (0.0038)	
training:	Epoch: [150][116/233]	Loss 0.0004 (0.0038)	
training:	Epoch: [150][117/233]	Loss 0.0003 (0.0037)	
training:	Epoch: [150][118/233]	Loss 0.0011 (0.0037)	
training:	Epoch: [150][119/233]	Loss 0.0030 (0.0037)	
training:	Epoch: [150][120/233]	Loss 0.0004 (0.0037)	
training:	Epoch: [150][121/233]	Loss 0.0006 (0.0037)	
training:	Epoch: [150][122/233]	Loss 0.0004 (0.0036)	
training:	Epoch: [150][123/233]	Loss 0.0003 (0.0036)	
training:	Epoch: [150][124/233]	Loss 0.0003 (0.0036)	
training:	Epoch: [150][125/233]	Loss 0.0003 (0.0036)	
training:	Epoch: [150][126/233]	Loss 0.0006 (0.0035)	
training:	Epoch: [150][127/233]	Loss 0.0009 (0.0035)	
training:	Epoch: [150][128/233]	Loss 0.0003 (0.0035)	
training:	Epoch: [150][129/233]	Loss 0.0003 (0.0035)	
training:	Epoch: [150][130/233]	Loss 0.0003 (0.0034)	
training:	Epoch: [150][131/233]	Loss 0.0004 (0.0034)	
training:	Epoch: [150][132/233]	Loss 0.0003 (0.0034)	
training:	Epoch: [150][133/233]	Loss 0.0003 (0.0034)	
training:	Epoch: [150][134/233]	Loss 0.0005 (0.0033)	
training:	Epoch: [150][135/233]	Loss 0.0004 (0.0033)	
training:	Epoch: [150][136/233]	Loss 0.0005 (0.0033)	
training:	Epoch: [150][137/233]	Loss 0.0003 (0.0033)	
training:	Epoch: [150][138/233]	Loss 0.0028 (0.0033)	
training:	Epoch: [150][139/233]	Loss 0.0003 (0.0033)	
training:	Epoch: [150][140/233]	Loss 0.0005 (0.0032)	
training:	Epoch: [150][141/233]	Loss 0.0004 (0.0032)	
training:	Epoch: [150][142/233]	Loss 0.0003 (0.0032)	
training:	Epoch: [150][143/233]	Loss 0.0069 (0.0032)	
training:	Epoch: [150][144/233]	Loss 0.0004 (0.0032)	
training:	Epoch: [150][145/233]	Loss 0.0053 (0.0032)	
training:	Epoch: [150][146/233]	Loss 0.0003 (0.0032)	
training:	Epoch: [150][147/233]	Loss 0.0010 (0.0032)	
training:	Epoch: [150][148/233]	Loss 0.0003 (0.0032)	
training:	Epoch: [150][149/233]	Loss 0.0006 (0.0031)	
training:	Epoch: [150][150/233]	Loss 0.0003 (0.0031)	
training:	Epoch: [150][151/233]	Loss 0.0003 (0.0031)	
training:	Epoch: [150][152/233]	Loss 0.0010 (0.0031)	
training:	Epoch: [150][153/233]	Loss 0.0003 (0.0031)	
training:	Epoch: [150][154/233]	Loss 0.0007 (0.0031)	
training:	Epoch: [150][155/233]	Loss 0.0008 (0.0030)	
training:	Epoch: [150][156/233]	Loss 0.0009 (0.0030)	
training:	Epoch: [150][157/233]	Loss 0.0004 (0.0030)	
training:	Epoch: [150][158/233]	Loss 0.0004 (0.0030)	
training:	Epoch: [150][159/233]	Loss 0.0551 (0.0033)	
training:	Epoch: [150][160/233]	Loss 0.0131 (0.0034)	
training:	Epoch: [150][161/233]	Loss 0.0002 (0.0034)	
training:	Epoch: [150][162/233]	Loss 0.0012 (0.0034)	
training:	Epoch: [150][163/233]	Loss 0.0004 (0.0033)	
training:	Epoch: [150][164/233]	Loss 0.0003 (0.0033)	
training:	Epoch: [150][165/233]	Loss 0.0003 (0.0033)	
training:	Epoch: [150][166/233]	Loss 0.0003 (0.0033)	
training:	Epoch: [150][167/233]	Loss 0.0004 (0.0033)	
training:	Epoch: [150][168/233]	Loss 0.0003 (0.0032)	
training:	Epoch: [150][169/233]	Loss 0.0004 (0.0032)	
training:	Epoch: [150][170/233]	Loss 0.0003 (0.0032)	
training:	Epoch: [150][171/233]	Loss 0.0003 (0.0032)	
training:	Epoch: [150][172/233]	Loss 0.0005 (0.0032)	
training:	Epoch: [150][173/233]	Loss 0.0010 (0.0032)	
training:	Epoch: [150][174/233]	Loss 0.0003 (0.0032)	
training:	Epoch: [150][175/233]	Loss 0.0003 (0.0031)	
training:	Epoch: [150][176/233]	Loss 0.0004 (0.0031)	
training:	Epoch: [150][177/233]	Loss 0.0003 (0.0031)	
training:	Epoch: [150][178/233]	Loss 0.0003 (0.0031)	
training:	Epoch: [150][179/233]	Loss 0.0005 (0.0031)	
training:	Epoch: [150][180/233]	Loss 0.0003 (0.0031)	
training:	Epoch: [150][181/233]	Loss 0.0005 (0.0030)	
training:	Epoch: [150][182/233]	Loss 0.0004 (0.0030)	
training:	Epoch: [150][183/233]	Loss 0.0003 (0.0030)	
training:	Epoch: [150][184/233]	Loss 0.0003 (0.0030)	
training:	Epoch: [150][185/233]	Loss 0.0003 (0.0030)	
training:	Epoch: [150][186/233]	Loss 0.0006 (0.0030)	
training:	Epoch: [150][187/233]	Loss 0.0004 (0.0030)	
training:	Epoch: [150][188/233]	Loss 0.0005 (0.0029)	
training:	Epoch: [150][189/233]	Loss 0.0003 (0.0029)	
training:	Epoch: [150][190/233]	Loss 0.0009 (0.0029)	
training:	Epoch: [150][191/233]	Loss 0.0003 (0.0029)	
training:	Epoch: [150][192/233]	Loss 0.0356 (0.0031)	
training:	Epoch: [150][193/233]	Loss 0.0003 (0.0031)	
training:	Epoch: [150][194/233]	Loss 0.0004 (0.0030)	
training:	Epoch: [150][195/233]	Loss 0.0011 (0.0030)	
training:	Epoch: [150][196/233]	Loss 0.0005 (0.0030)	
training:	Epoch: [150][197/233]	Loss 0.0019 (0.0030)	
training:	Epoch: [150][198/233]	Loss 0.0004 (0.0030)	
training:	Epoch: [150][199/233]	Loss 0.0008 (0.0030)	
training:	Epoch: [150][200/233]	Loss 0.0004 (0.0030)	
training:	Epoch: [150][201/233]	Loss 0.0004 (0.0030)	
training:	Epoch: [150][202/233]	Loss 0.0003 (0.0030)	
training:	Epoch: [150][203/233]	Loss 0.0004 (0.0029)	
training:	Epoch: [150][204/233]	Loss 0.0010 (0.0029)	
training:	Epoch: [150][205/233]	Loss 0.0003 (0.0029)	
training:	Epoch: [150][206/233]	Loss 0.0004 (0.0029)	
training:	Epoch: [150][207/233]	Loss 0.0003 (0.0029)	
training:	Epoch: [150][208/233]	Loss 0.0073 (0.0029)	
training:	Epoch: [150][209/233]	Loss 0.0003 (0.0029)	
training:	Epoch: [150][210/233]	Loss 0.0002 (0.0029)	
training:	Epoch: [150][211/233]	Loss 0.0003 (0.0029)	
training:	Epoch: [150][212/233]	Loss 0.0079 (0.0029)	
training:	Epoch: [150][213/233]	Loss 0.0003 (0.0029)	
training:	Epoch: [150][214/233]	Loss 0.0641 (0.0032)	
training:	Epoch: [150][215/233]	Loss 0.0018 (0.0032)	
training:	Epoch: [150][216/233]	Loss 0.0004 (0.0032)	
training:	Epoch: [150][217/233]	Loss 0.0004 (0.0031)	
training:	Epoch: [150][218/233]	Loss 0.0004 (0.0031)	
training:	Epoch: [150][219/233]	Loss 0.0003 (0.0031)	
training:	Epoch: [150][220/233]	Loss 0.0003 (0.0031)	
training:	Epoch: [150][221/233]	Loss 0.0011 (0.0031)	
training:	Epoch: [150][222/233]	Loss 0.0004 (0.0031)	
training:	Epoch: [150][223/233]	Loss 0.0014 (0.0031)	
training:	Epoch: [150][224/233]	Loss 0.0004 (0.0031)	
training:	Epoch: [150][225/233]	Loss 0.0021 (0.0031)	
training:	Epoch: [150][226/233]	Loss 0.0005 (0.0031)	
training:	Epoch: [150][227/233]	Loss 0.0311 (0.0032)	
training:	Epoch: [150][228/233]	Loss 0.0004 (0.0032)	
training:	Epoch: [150][229/233]	Loss 0.0003 (0.0032)	
training:	Epoch: [150][230/233]	Loss 0.0041 (0.0032)	
training:	Epoch: [150][231/233]	Loss 0.0003 (0.0031)	
training:	Epoch: [150][232/233]	Loss 0.0009 (0.0031)	
training:	Epoch: [150][233/233]	Loss 0.0041 (0.0031)	
Training:	 Loss: 0.0031

Training:	 ACC: 1.0000 1.0000 1.0000 1.0000
Validation:	 ACC: 0.7913 0.7903 0.7692 0.8135
Validation:	 Best_BACC: 0.8162 0.8154 0.7998 0.8326
Validation:	 Loss: 1.2879
Pretraining:	Epoch 151/200
----------
training:	Epoch: [151][1/233]	Loss 0.0005 (0.0005)	
training:	Epoch: [151][2/233]	Loss 0.0002 (0.0004)	
training:	Epoch: [151][3/233]	Loss 0.0003 (0.0003)	
training:	Epoch: [151][4/233]	Loss 0.0003 (0.0003)	
training:	Epoch: [151][5/233]	Loss 0.0111 (0.0025)	
training:	Epoch: [151][6/233]	Loss 0.0005 (0.0022)	
training:	Epoch: [151][7/233]	Loss 0.0003 (0.0019)	
training:	Epoch: [151][8/233]	Loss 0.0003 (0.0017)	
training:	Epoch: [151][9/233]	Loss 0.0025 (0.0018)	
training:	Epoch: [151][10/233]	Loss 0.0002 (0.0016)	
training:	Epoch: [151][11/233]	Loss 0.0003 (0.0015)	
training:	Epoch: [151][12/233]	Loss 0.0003 (0.0014)	
training:	Epoch: [151][13/233]	Loss 0.0036 (0.0016)	
training:	Epoch: [151][14/233]	Loss 0.0003 (0.0015)	
training:	Epoch: [151][15/233]	Loss 0.0029 (0.0016)	
training:	Epoch: [151][16/233]	Loss 0.0003 (0.0015)	
training:	Epoch: [151][17/233]	Loss 0.0004 (0.0014)	
training:	Epoch: [151][18/233]	Loss 0.0003 (0.0014)	
training:	Epoch: [151][19/233]	Loss 0.0004 (0.0013)	
training:	Epoch: [151][20/233]	Loss 0.0003 (0.0013)	
training:	Epoch: [151][21/233]	Loss 0.0003 (0.0012)	
training:	Epoch: [151][22/233]	Loss 0.0003 (0.0012)	
training:	Epoch: [151][23/233]	Loss 0.0003 (0.0011)	
training:	Epoch: [151][24/233]	Loss 0.0003 (0.0011)	
training:	Epoch: [151][25/233]	Loss 0.0004 (0.0011)	
training:	Epoch: [151][26/233]	Loss 0.0004 (0.0010)	
training:	Epoch: [151][27/233]	Loss 0.0002 (0.0010)	
training:	Epoch: [151][28/233]	Loss 0.0006 (0.0010)	
training:	Epoch: [151][29/233]	Loss 0.0003 (0.0010)	
training:	Epoch: [151][30/233]	Loss 0.0003 (0.0009)	
training:	Epoch: [151][31/233]	Loss 0.0002 (0.0009)	
training:	Epoch: [151][32/233]	Loss 0.0003 (0.0009)	
training:	Epoch: [151][33/233]	Loss 0.0002 (0.0009)	
training:	Epoch: [151][34/233]	Loss 0.0003 (0.0009)	
training:	Epoch: [151][35/233]	Loss 0.0003 (0.0009)	
training:	Epoch: [151][36/233]	Loss 0.0003 (0.0008)	
training:	Epoch: [151][37/233]	Loss 0.0018 (0.0009)	
training:	Epoch: [151][38/233]	Loss 0.0004 (0.0008)	
training:	Epoch: [151][39/233]	Loss 0.0003 (0.0008)	
training:	Epoch: [151][40/233]	Loss 0.0003 (0.0008)	
training:	Epoch: [151][41/233]	Loss 0.0007 (0.0008)	
training:	Epoch: [151][42/233]	Loss 0.0124 (0.0011)	
training:	Epoch: [151][43/233]	Loss 0.0003 (0.0011)	
training:	Epoch: [151][44/233]	Loss 0.0003 (0.0011)	
training:	Epoch: [151][45/233]	Loss 0.0003 (0.0010)	
training:	Epoch: [151][46/233]	Loss 0.0003 (0.0010)	
training:	Epoch: [151][47/233]	Loss 0.0003 (0.0010)	
training:	Epoch: [151][48/233]	Loss 0.0007 (0.0010)	
training:	Epoch: [151][49/233]	Loss 0.0004 (0.0010)	
training:	Epoch: [151][50/233]	Loss 0.0011 (0.0010)	
training:	Epoch: [151][51/233]	Loss 0.0003 (0.0010)	
training:	Epoch: [151][52/233]	Loss 0.0324 (0.0016)	
training:	Epoch: [151][53/233]	Loss 0.0003 (0.0016)	
training:	Epoch: [151][54/233]	Loss 0.0003 (0.0015)	
training:	Epoch: [151][55/233]	Loss 0.0004 (0.0015)	
training:	Epoch: [151][56/233]	Loss 0.0003 (0.0015)	
training:	Epoch: [151][57/233]	Loss 0.0002 (0.0015)	
training:	Epoch: [151][58/233]	Loss 0.0004 (0.0015)	
training:	Epoch: [151][59/233]	Loss 0.0005 (0.0014)	
training:	Epoch: [151][60/233]	Loss 0.0003 (0.0014)	
training:	Epoch: [151][61/233]	Loss 0.0291 (0.0019)	
training:	Epoch: [151][62/233]	Loss 0.0004 (0.0018)	
training:	Epoch: [151][63/233]	Loss 0.0003 (0.0018)	
training:	Epoch: [151][64/233]	Loss 0.0003 (0.0018)	
training:	Epoch: [151][65/233]	Loss 0.0004 (0.0018)	
training:	Epoch: [151][66/233]	Loss 0.0003 (0.0018)	
training:	Epoch: [151][67/233]	Loss 0.0003 (0.0017)	
training:	Epoch: [151][68/233]	Loss 0.0006 (0.0017)	
training:	Epoch: [151][69/233]	Loss 0.0003 (0.0017)	
training:	Epoch: [151][70/233]	Loss 0.0003 (0.0017)	
training:	Epoch: [151][71/233]	Loss 0.0002 (0.0017)	
training:	Epoch: [151][72/233]	Loss 0.0013 (0.0016)	
training:	Epoch: [151][73/233]	Loss 0.0004 (0.0016)	
training:	Epoch: [151][74/233]	Loss 0.0003 (0.0016)	
training:	Epoch: [151][75/233]	Loss 0.0003 (0.0016)	
training:	Epoch: [151][76/233]	Loss 0.0003 (0.0016)	
training:	Epoch: [151][77/233]	Loss 0.0003 (0.0016)	
training:	Epoch: [151][78/233]	Loss 0.0004 (0.0015)	
training:	Epoch: [151][79/233]	Loss 0.0005 (0.0015)	
training:	Epoch: [151][80/233]	Loss 0.0002 (0.0015)	
training:	Epoch: [151][81/233]	Loss 0.0005 (0.0015)	
training:	Epoch: [151][82/233]	Loss 0.0007 (0.0015)	
training:	Epoch: [151][83/233]	Loss 0.0005 (0.0015)	
training:	Epoch: [151][84/233]	Loss 0.0038 (0.0015)	
training:	Epoch: [151][85/233]	Loss 0.0023 (0.0015)	
training:	Epoch: [151][86/233]	Loss 0.0009 (0.0015)	
training:	Epoch: [151][87/233]	Loss 0.0003 (0.0015)	
training:	Epoch: [151][88/233]	Loss 0.0017 (0.0015)	
training:	Epoch: [151][89/233]	Loss 0.0008 (0.0015)	
training:	Epoch: [151][90/233]	Loss 0.0003 (0.0015)	
training:	Epoch: [151][91/233]	Loss 0.0189 (0.0017)	
training:	Epoch: [151][92/233]	Loss 0.0003 (0.0017)	
training:	Epoch: [151][93/233]	Loss 0.0003 (0.0016)	
training:	Epoch: [151][94/233]	Loss 0.0012 (0.0016)	
training:	Epoch: [151][95/233]	Loss 0.0005 (0.0016)	
training:	Epoch: [151][96/233]	Loss 0.0004 (0.0016)	
training:	Epoch: [151][97/233]	Loss 0.0003 (0.0016)	
training:	Epoch: [151][98/233]	Loss 0.0003 (0.0016)	
training:	Epoch: [151][99/233]	Loss 0.0003 (0.0016)	
training:	Epoch: [151][100/233]	Loss 0.0002 (0.0016)	
training:	Epoch: [151][101/233]	Loss 0.0003 (0.0015)	
training:	Epoch: [151][102/233]	Loss 0.0004 (0.0015)	
training:	Epoch: [151][103/233]	Loss 0.0003 (0.0015)	
training:	Epoch: [151][104/233]	Loss 0.0002 (0.0015)	
training:	Epoch: [151][105/233]	Loss 0.0636 (0.0021)	
training:	Epoch: [151][106/233]	Loss 0.0003 (0.0021)	
training:	Epoch: [151][107/233]	Loss 0.0029 (0.0021)	
training:	Epoch: [151][108/233]	Loss 0.0003 (0.0021)	
training:	Epoch: [151][109/233]	Loss 0.0003 (0.0021)	
training:	Epoch: [151][110/233]	Loss 0.0003 (0.0020)	
training:	Epoch: [151][111/233]	Loss 0.0003 (0.0020)	
training:	Epoch: [151][112/233]	Loss 0.0003 (0.0020)	
training:	Epoch: [151][113/233]	Loss 0.0005 (0.0020)	
training:	Epoch: [151][114/233]	Loss 0.0004 (0.0020)	
training:	Epoch: [151][115/233]	Loss 0.0010 (0.0020)	
training:	Epoch: [151][116/233]	Loss 0.0006 (0.0020)	
training:	Epoch: [151][117/233]	Loss 0.0003 (0.0020)	
training:	Epoch: [151][118/233]	Loss 0.0003 (0.0019)	
training:	Epoch: [151][119/233]	Loss 0.0003 (0.0019)	
training:	Epoch: [151][120/233]	Loss 0.0004 (0.0019)	
training:	Epoch: [151][121/233]	Loss 0.0956 (0.0027)	
training:	Epoch: [151][122/233]	Loss 0.0018 (0.0027)	
training:	Epoch: [151][123/233]	Loss 0.0003 (0.0027)	
training:	Epoch: [151][124/233]	Loss 0.0004 (0.0026)	
training:	Epoch: [151][125/233]	Loss 0.0004 (0.0026)	
training:	Epoch: [151][126/233]	Loss 0.0003 (0.0026)	
training:	Epoch: [151][127/233]	Loss 0.0004 (0.0026)	
training:	Epoch: [151][128/233]	Loss 0.0003 (0.0026)	
training:	Epoch: [151][129/233]	Loss 0.0004 (0.0026)	
training:	Epoch: [151][130/233]	Loss 0.0009 (0.0025)	
training:	Epoch: [151][131/233]	Loss 0.0004 (0.0025)	
training:	Epoch: [151][132/233]	Loss 0.0003 (0.0025)	
training:	Epoch: [151][133/233]	Loss 0.0016 (0.0025)	
training:	Epoch: [151][134/233]	Loss 0.0004 (0.0025)	
training:	Epoch: [151][135/233]	Loss 0.0006 (0.0025)	
training:	Epoch: [151][136/233]	Loss 0.0002 (0.0025)	
training:	Epoch: [151][137/233]	Loss 0.0005 (0.0024)	
training:	Epoch: [151][138/233]	Loss 0.0008 (0.0024)	
training:	Epoch: [151][139/233]	Loss 0.0005 (0.0024)	
training:	Epoch: [151][140/233]	Loss 0.0003 (0.0024)	
training:	Epoch: [151][141/233]	Loss 0.0003 (0.0024)	
training:	Epoch: [151][142/233]	Loss 0.0003 (0.0024)	
training:	Epoch: [151][143/233]	Loss 0.0003 (0.0024)	
training:	Epoch: [151][144/233]	Loss 0.0003 (0.0023)	
training:	Epoch: [151][145/233]	Loss 0.0003 (0.0023)	
training:	Epoch: [151][146/233]	Loss 0.0003 (0.0023)	
training:	Epoch: [151][147/233]	Loss 0.0003 (0.0023)	
training:	Epoch: [151][148/233]	Loss 0.0030 (0.0023)	
training:	Epoch: [151][149/233]	Loss 0.0003 (0.0023)	
training:	Epoch: [151][150/233]	Loss 0.0003 (0.0023)	
training:	Epoch: [151][151/233]	Loss 0.0004 (0.0023)	
training:	Epoch: [151][152/233]	Loss 0.0003 (0.0022)	
training:	Epoch: [151][153/233]	Loss 0.0003 (0.0022)	
training:	Epoch: [151][154/233]	Loss 0.0003 (0.0022)	
training:	Epoch: [151][155/233]	Loss 0.0003 (0.0022)	
training:	Epoch: [151][156/233]	Loss 0.0009 (0.0022)	
training:	Epoch: [151][157/233]	Loss 0.0016 (0.0022)	
training:	Epoch: [151][158/233]	Loss 0.0004 (0.0022)	
training:	Epoch: [151][159/233]	Loss 0.0003 (0.0022)	
training:	Epoch: [151][160/233]	Loss 0.0002 (0.0022)	
training:	Epoch: [151][161/233]	Loss 0.0005 (0.0022)	
training:	Epoch: [151][162/233]	Loss 0.0004 (0.0021)	
training:	Epoch: [151][163/233]	Loss 0.0003 (0.0021)	
training:	Epoch: [151][164/233]	Loss 0.0003 (0.0021)	
training:	Epoch: [151][165/233]	Loss 0.0003 (0.0021)	
training:	Epoch: [151][166/233]	Loss 0.0012 (0.0021)	
training:	Epoch: [151][167/233]	Loss 0.0008 (0.0021)	
training:	Epoch: [151][168/233]	Loss 0.0004 (0.0021)	
training:	Epoch: [151][169/233]	Loss 0.0003 (0.0021)	
training:	Epoch: [151][170/233]	Loss 0.0954 (0.0026)	
training:	Epoch: [151][171/233]	Loss 0.0003 (0.0026)	
training:	Epoch: [151][172/233]	Loss 0.0003 (0.0026)	
training:	Epoch: [151][173/233]	Loss 0.0004 (0.0026)	
training:	Epoch: [151][174/233]	Loss 0.0008 (0.0026)	
training:	Epoch: [151][175/233]	Loss 0.0006 (0.0026)	
training:	Epoch: [151][176/233]	Loss 0.0005 (0.0026)	
training:	Epoch: [151][177/233]	Loss 0.0014 (0.0025)	
training:	Epoch: [151][178/233]	Loss 0.0002 (0.0025)	
training:	Epoch: [151][179/233]	Loss 0.0007 (0.0025)	
training:	Epoch: [151][180/233]	Loss 0.0003 (0.0025)	
training:	Epoch: [151][181/233]	Loss 0.0003 (0.0025)	
training:	Epoch: [151][182/233]	Loss 0.0003 (0.0025)	
training:	Epoch: [151][183/233]	Loss 0.0004 (0.0025)	
training:	Epoch: [151][184/233]	Loss 0.0061 (0.0025)	
training:	Epoch: [151][185/233]	Loss 0.0003 (0.0025)	
training:	Epoch: [151][186/233]	Loss 0.0006 (0.0025)	
training:	Epoch: [151][187/233]	Loss 0.0004 (0.0025)	
training:	Epoch: [151][188/233]	Loss 0.0013 (0.0025)	
training:	Epoch: [151][189/233]	Loss 0.0004 (0.0024)	
training:	Epoch: [151][190/233]	Loss 0.0003 (0.0024)	
training:	Epoch: [151][191/233]	Loss 0.0004 (0.0024)	
training:	Epoch: [151][192/233]	Loss 0.0004 (0.0024)	
training:	Epoch: [151][193/233]	Loss 0.0080 (0.0024)	
training:	Epoch: [151][194/233]	Loss 0.0003 (0.0024)	
training:	Epoch: [151][195/233]	Loss 0.0003 (0.0024)	
training:	Epoch: [151][196/233]	Loss 0.0003 (0.0024)	
training:	Epoch: [151][197/233]	Loss 0.0073 (0.0024)	
training:	Epoch: [151][198/233]	Loss 0.0005 (0.0024)	
training:	Epoch: [151][199/233]	Loss 0.0003 (0.0024)	
training:	Epoch: [151][200/233]	Loss 0.0003 (0.0024)	
training:	Epoch: [151][201/233]	Loss 0.0005 (0.0024)	
training:	Epoch: [151][202/233]	Loss 0.0017 (0.0024)	
training:	Epoch: [151][203/233]	Loss 0.0372 (0.0026)	
training:	Epoch: [151][204/233]	Loss 0.0004 (0.0025)	
training:	Epoch: [151][205/233]	Loss 0.0052 (0.0026)	
training:	Epoch: [151][206/233]	Loss 0.0003 (0.0026)	
training:	Epoch: [151][207/233]	Loss 0.0007 (0.0025)	
training:	Epoch: [151][208/233]	Loss 0.0005 (0.0025)	
training:	Epoch: [151][209/233]	Loss 0.0019 (0.0025)	
training:	Epoch: [151][210/233]	Loss 0.0823 (0.0029)	
training:	Epoch: [151][211/233]	Loss 0.0003 (0.0029)	
training:	Epoch: [151][212/233]	Loss 0.0003 (0.0029)	
training:	Epoch: [151][213/233]	Loss 0.0002 (0.0029)	
training:	Epoch: [151][214/233]	Loss 0.0007 (0.0029)	
training:	Epoch: [151][215/233]	Loss 0.0003 (0.0028)	
training:	Epoch: [151][216/233]	Loss 0.0026 (0.0028)	
training:	Epoch: [151][217/233]	Loss 0.0003 (0.0028)	
training:	Epoch: [151][218/233]	Loss 0.0003 (0.0028)	
training:	Epoch: [151][219/233]	Loss 0.0004 (0.0028)	
training:	Epoch: [151][220/233]	Loss 0.0006 (0.0028)	
training:	Epoch: [151][221/233]	Loss 0.0003 (0.0028)	
training:	Epoch: [151][222/233]	Loss 0.0003 (0.0028)	
training:	Epoch: [151][223/233]	Loss 0.0005 (0.0028)	
training:	Epoch: [151][224/233]	Loss 0.0076 (0.0028)	
training:	Epoch: [151][225/233]	Loss 0.1119 (0.0033)	
training:	Epoch: [151][226/233]	Loss 0.0012 (0.0033)	
training:	Epoch: [151][227/233]	Loss 0.0004 (0.0033)	
training:	Epoch: [151][228/233]	Loss 0.0004 (0.0032)	
training:	Epoch: [151][229/233]	Loss 0.0020 (0.0032)	
training:	Epoch: [151][230/233]	Loss 0.0014 (0.0032)	
training:	Epoch: [151][231/233]	Loss 0.0051 (0.0032)	
training:	Epoch: [151][232/233]	Loss 0.0005 (0.0032)	
training:	Epoch: [151][233/233]	Loss 0.0008 (0.0032)	
Training:	 Loss: 0.0032

Training:	 ACC: 1.0000 1.0000 1.0000 1.0000
Validation:	 ACC: 0.7951 0.7951 0.7947 0.7955
Validation:	 Best_BACC: 0.8162 0.8154 0.7998 0.8326
Validation:	 Loss: 1.2368
Pretraining:	Epoch 152/200
----------
training:	Epoch: [152][1/233]	Loss 0.0003 (0.0003)	
training:	Epoch: [152][2/233]	Loss 0.0003 (0.0003)	
training:	Epoch: [152][3/233]	Loss 0.0004 (0.0003)	
training:	Epoch: [152][4/233]	Loss 0.0005 (0.0004)	
training:	Epoch: [152][5/233]	Loss 0.0008 (0.0005)	
training:	Epoch: [152][6/233]	Loss 0.0003 (0.0004)	
training:	Epoch: [152][7/233]	Loss 0.0003 (0.0004)	
training:	Epoch: [152][8/233]	Loss 0.0126 (0.0019)	
training:	Epoch: [152][9/233]	Loss 0.0003 (0.0018)	
training:	Epoch: [152][10/233]	Loss 0.0003 (0.0016)	
training:	Epoch: [152][11/233]	Loss 0.0497 (0.0060)	
training:	Epoch: [152][12/233]	Loss 0.0003 (0.0055)	
training:	Epoch: [152][13/233]	Loss 0.0010 (0.0052)	
training:	Epoch: [152][14/233]	Loss 0.0007 (0.0048)	
training:	Epoch: [152][15/233]	Loss 0.0003 (0.0045)	
training:	Epoch: [152][16/233]	Loss 0.0005 (0.0043)	
training:	Epoch: [152][17/233]	Loss 0.0002 (0.0041)	
training:	Epoch: [152][18/233]	Loss 0.0004 (0.0038)	
training:	Epoch: [152][19/233]	Loss 0.0007 (0.0037)	
training:	Epoch: [152][20/233]	Loss 0.0183 (0.0044)	
training:	Epoch: [152][21/233]	Loss 0.0005 (0.0042)	
training:	Epoch: [152][22/233]	Loss 0.0003 (0.0040)	
training:	Epoch: [152][23/233]	Loss 0.0004 (0.0039)	
training:	Epoch: [152][24/233]	Loss 0.0005 (0.0037)	
training:	Epoch: [152][25/233]	Loss 0.0004 (0.0036)	
training:	Epoch: [152][26/233]	Loss 0.0012 (0.0035)	
training:	Epoch: [152][27/233]	Loss 0.0038 (0.0035)	
training:	Epoch: [152][28/233]	Loss 0.0007 (0.0034)	
training:	Epoch: [152][29/233]	Loss 0.0006 (0.0033)	
training:	Epoch: [152][30/233]	Loss 0.0014 (0.0033)	
training:	Epoch: [152][31/233]	Loss 0.0008 (0.0032)	
training:	Epoch: [152][32/233]	Loss 0.0005 (0.0031)	
training:	Epoch: [152][33/233]	Loss 0.0003 (0.0030)	
training:	Epoch: [152][34/233]	Loss 0.0003 (0.0029)	
training:	Epoch: [152][35/233]	Loss 0.0011 (0.0029)	
training:	Epoch: [152][36/233]	Loss 0.0003 (0.0028)	
training:	Epoch: [152][37/233]	Loss 0.0003 (0.0027)	
training:	Epoch: [152][38/233]	Loss 0.0002 (0.0027)	
training:	Epoch: [152][39/233]	Loss 0.0003 (0.0026)	
training:	Epoch: [152][40/233]	Loss 0.0003 (0.0026)	
training:	Epoch: [152][41/233]	Loss 0.0011 (0.0025)	
training:	Epoch: [152][42/233]	Loss 0.0004 (0.0025)	
training:	Epoch: [152][43/233]	Loss 0.0007 (0.0024)	
training:	Epoch: [152][44/233]	Loss 0.0003 (0.0024)	
training:	Epoch: [152][45/233]	Loss 0.0310 (0.0030)	
training:	Epoch: [152][46/233]	Loss 0.0005 (0.0030)	
training:	Epoch: [152][47/233]	Loss 0.0005 (0.0029)	
training:	Epoch: [152][48/233]	Loss 0.0002 (0.0029)	
training:	Epoch: [152][49/233]	Loss 0.0043 (0.0029)	
training:	Epoch: [152][50/233]	Loss 0.0009 (0.0028)	
training:	Epoch: [152][51/233]	Loss 0.0004 (0.0028)	
training:	Epoch: [152][52/233]	Loss 0.0003 (0.0028)	
training:	Epoch: [152][53/233]	Loss 0.0004 (0.0027)	
training:	Epoch: [152][54/233]	Loss 0.0003 (0.0027)	
training:	Epoch: [152][55/233]	Loss 0.0004 (0.0026)	
training:	Epoch: [152][56/233]	Loss 0.0006 (0.0026)	
training:	Epoch: [152][57/233]	Loss 0.0003 (0.0025)	
training:	Epoch: [152][58/233]	Loss 0.0003 (0.0025)	
training:	Epoch: [152][59/233]	Loss 0.0067 (0.0026)	
training:	Epoch: [152][60/233]	Loss 0.0010 (0.0026)	
training:	Epoch: [152][61/233]	Loss 0.0004 (0.0025)	
training:	Epoch: [152][62/233]	Loss 0.0004 (0.0025)	
training:	Epoch: [152][63/233]	Loss 0.0004 (0.0024)	
training:	Epoch: [152][64/233]	Loss 0.0003 (0.0024)	
training:	Epoch: [152][65/233]	Loss 0.0004 (0.0024)	
training:	Epoch: [152][66/233]	Loss 0.0004 (0.0024)	
training:	Epoch: [152][67/233]	Loss 0.0003 (0.0023)	
training:	Epoch: [152][68/233]	Loss 0.0004 (0.0023)	
training:	Epoch: [152][69/233]	Loss 0.0003 (0.0023)	
training:	Epoch: [152][70/233]	Loss 0.0010 (0.0022)	
training:	Epoch: [152][71/233]	Loss 0.0003 (0.0022)	
training:	Epoch: [152][72/233]	Loss 0.0013 (0.0022)	
training:	Epoch: [152][73/233]	Loss 0.0003 (0.0022)	
training:	Epoch: [152][74/233]	Loss 0.0003 (0.0022)	
training:	Epoch: [152][75/233]	Loss 0.0003 (0.0021)	
training:	Epoch: [152][76/233]	Loss 0.0021 (0.0021)	
training:	Epoch: [152][77/233]	Loss 0.0002 (0.0021)	
training:	Epoch: [152][78/233]	Loss 0.0010 (0.0021)	
training:	Epoch: [152][79/233]	Loss 0.0006 (0.0021)	
training:	Epoch: [152][80/233]	Loss 0.0003 (0.0020)	
training:	Epoch: [152][81/233]	Loss 0.0003 (0.0020)	
training:	Epoch: [152][82/233]	Loss 0.0005 (0.0020)	
training:	Epoch: [152][83/233]	Loss 0.0004 (0.0020)	
training:	Epoch: [152][84/233]	Loss 0.0003 (0.0020)	
training:	Epoch: [152][85/233]	Loss 0.0003 (0.0019)	
training:	Epoch: [152][86/233]	Loss 0.0003 (0.0019)	
training:	Epoch: [152][87/233]	Loss 0.0003 (0.0019)	
training:	Epoch: [152][88/233]	Loss 0.0003 (0.0019)	
training:	Epoch: [152][89/233]	Loss 0.0003 (0.0019)	
training:	Epoch: [152][90/233]	Loss 0.0002 (0.0019)	
training:	Epoch: [152][91/233]	Loss 0.0008 (0.0018)	
training:	Epoch: [152][92/233]	Loss 0.0003 (0.0018)	
training:	Epoch: [152][93/233]	Loss 0.0003 (0.0018)	
training:	Epoch: [152][94/233]	Loss 0.0005 (0.0018)	
training:	Epoch: [152][95/233]	Loss 0.0006 (0.0018)	
training:	Epoch: [152][96/233]	Loss 0.0027 (0.0018)	
training:	Epoch: [152][97/233]	Loss 0.0003 (0.0018)	
training:	Epoch: [152][98/233]	Loss 0.0424 (0.0022)	
training:	Epoch: [152][99/233]	Loss 0.0527 (0.0027)	
training:	Epoch: [152][100/233]	Loss 0.0008 (0.0027)	
training:	Epoch: [152][101/233]	Loss 0.0014 (0.0027)	
training:	Epoch: [152][102/233]	Loss 0.0009 (0.0027)	
training:	Epoch: [152][103/233]	Loss 0.0004 (0.0026)	
training:	Epoch: [152][104/233]	Loss 0.0004 (0.0026)	
training:	Epoch: [152][105/233]	Loss 0.0005 (0.0026)	
training:	Epoch: [152][106/233]	Loss 0.0012 (0.0026)	
training:	Epoch: [152][107/233]	Loss 0.0080 (0.0026)	
training:	Epoch: [152][108/233]	Loss 0.0004 (0.0026)	
training:	Epoch: [152][109/233]	Loss 0.0044 (0.0026)	
training:	Epoch: [152][110/233]	Loss 0.0003 (0.0026)	
training:	Epoch: [152][111/233]	Loss 0.0006 (0.0026)	
training:	Epoch: [152][112/233]	Loss 0.0003 (0.0026)	
training:	Epoch: [152][113/233]	Loss 0.0004 (0.0025)	
training:	Epoch: [152][114/233]	Loss 0.0006 (0.0025)	
training:	Epoch: [152][115/233]	Loss 0.0019 (0.0025)	
training:	Epoch: [152][116/233]	Loss 0.0019 (0.0025)	
training:	Epoch: [152][117/233]	Loss 0.0051 (0.0025)	
training:	Epoch: [152][118/233]	Loss 0.0010 (0.0025)	
training:	Epoch: [152][119/233]	Loss 0.0023 (0.0025)	
training:	Epoch: [152][120/233]	Loss 0.0007 (0.0025)	
training:	Epoch: [152][121/233]	Loss 0.0101 (0.0026)	
training:	Epoch: [152][122/233]	Loss 0.0003 (0.0026)	
training:	Epoch: [152][123/233]	Loss 0.0003 (0.0025)	
training:	Epoch: [152][124/233]	Loss 0.0003 (0.0025)	
training:	Epoch: [152][125/233]	Loss 0.0002 (0.0025)	
training:	Epoch: [152][126/233]	Loss 0.0003 (0.0025)	
training:	Epoch: [152][127/233]	Loss 0.0017 (0.0025)	
training:	Epoch: [152][128/233]	Loss 0.0039 (0.0025)	
training:	Epoch: [152][129/233]	Loss 0.0012 (0.0025)	
training:	Epoch: [152][130/233]	Loss 0.0004 (0.0025)	
training:	Epoch: [152][131/233]	Loss 0.0003 (0.0024)	
training:	Epoch: [152][132/233]	Loss 0.0003 (0.0024)	
training:	Epoch: [152][133/233]	Loss 0.0005 (0.0024)	
training:	Epoch: [152][134/233]	Loss 0.0008 (0.0024)	
training:	Epoch: [152][135/233]	Loss 0.0006 (0.0024)	
training:	Epoch: [152][136/233]	Loss 0.0004 (0.0024)	
training:	Epoch: [152][137/233]	Loss 0.0004 (0.0024)	
training:	Epoch: [152][138/233]	Loss 0.0004 (0.0023)	
training:	Epoch: [152][139/233]	Loss 0.0003 (0.0023)	
training:	Epoch: [152][140/233]	Loss 0.0003 (0.0023)	
training:	Epoch: [152][141/233]	Loss 0.1204 (0.0032)	
training:	Epoch: [152][142/233]	Loss 0.0003 (0.0031)	
training:	Epoch: [152][143/233]	Loss 0.0003 (0.0031)	
training:	Epoch: [152][144/233]	Loss 0.0003 (0.0031)	
training:	Epoch: [152][145/233]	Loss 0.0125 (0.0032)	
training:	Epoch: [152][146/233]	Loss 0.0003 (0.0031)	
training:	Epoch: [152][147/233]	Loss 0.0003 (0.0031)	
training:	Epoch: [152][148/233]	Loss 0.0003 (0.0031)	
training:	Epoch: [152][149/233]	Loss 0.0008 (0.0031)	
training:	Epoch: [152][150/233]	Loss 0.0004 (0.0031)	
training:	Epoch: [152][151/233]	Loss 0.0004 (0.0030)	
training:	Epoch: [152][152/233]	Loss 0.0003 (0.0030)	
training:	Epoch: [152][153/233]	Loss 0.0003 (0.0030)	
training:	Epoch: [152][154/233]	Loss 0.0003 (0.0030)	
training:	Epoch: [152][155/233]	Loss 0.0005 (0.0030)	
training:	Epoch: [152][156/233]	Loss 0.0003 (0.0030)	
training:	Epoch: [152][157/233]	Loss 0.0061 (0.0030)	
training:	Epoch: [152][158/233]	Loss 0.0003 (0.0030)	
training:	Epoch: [152][159/233]	Loss 0.0003 (0.0029)	
training:	Epoch: [152][160/233]	Loss 0.0019 (0.0029)	
training:	Epoch: [152][161/233]	Loss 0.0202 (0.0030)	
training:	Epoch: [152][162/233]	Loss 0.0003 (0.0030)	
training:	Epoch: [152][163/233]	Loss 0.0004 (0.0030)	
training:	Epoch: [152][164/233]	Loss 0.0004 (0.0030)	
training:	Epoch: [152][165/233]	Loss 0.0006 (0.0030)	
training:	Epoch: [152][166/233]	Loss 0.0003 (0.0030)	
training:	Epoch: [152][167/233]	Loss 0.0003 (0.0030)	
training:	Epoch: [152][168/233]	Loss 0.0003 (0.0029)	
training:	Epoch: [152][169/233]	Loss 0.0013 (0.0029)	
training:	Epoch: [152][170/233]	Loss 0.0003 (0.0029)	
training:	Epoch: [152][171/233]	Loss 0.0003 (0.0029)	
training:	Epoch: [152][172/233]	Loss 0.0004 (0.0029)	
training:	Epoch: [152][173/233]	Loss 0.0003 (0.0029)	
training:	Epoch: [152][174/233]	Loss 0.0003 (0.0029)	
training:	Epoch: [152][175/233]	Loss 0.0002 (0.0028)	
training:	Epoch: [152][176/233]	Loss 0.0004 (0.0028)	
training:	Epoch: [152][177/233]	Loss 0.0005 (0.0028)	
training:	Epoch: [152][178/233]	Loss 0.0004 (0.0028)	
training:	Epoch: [152][179/233]	Loss 0.0004 (0.0028)	
training:	Epoch: [152][180/233]	Loss 0.0003 (0.0028)	
training:	Epoch: [152][181/233]	Loss 0.0003 (0.0028)	
training:	Epoch: [152][182/233]	Loss 0.0004 (0.0027)	
training:	Epoch: [152][183/233]	Loss 0.0003 (0.0027)	
training:	Epoch: [152][184/233]	Loss 0.0002 (0.0027)	
training:	Epoch: [152][185/233]	Loss 0.0015 (0.0027)	
training:	Epoch: [152][186/233]	Loss 0.0002 (0.0027)	
training:	Epoch: [152][187/233]	Loss 0.0003 (0.0027)	
training:	Epoch: [152][188/233]	Loss 0.0003 (0.0027)	
training:	Epoch: [152][189/233]	Loss 0.0005 (0.0027)	
training:	Epoch: [152][190/233]	Loss 0.0003 (0.0026)	
training:	Epoch: [152][191/233]	Loss 0.0003 (0.0026)	
training:	Epoch: [152][192/233]	Loss 0.0003 (0.0026)	
training:	Epoch: [152][193/233]	Loss 0.0003 (0.0026)	
training:	Epoch: [152][194/233]	Loss 0.0003 (0.0026)	
training:	Epoch: [152][195/233]	Loss 0.0002 (0.0026)	
training:	Epoch: [152][196/233]	Loss 0.0003 (0.0026)	
training:	Epoch: [152][197/233]	Loss 0.0003 (0.0026)	
training:	Epoch: [152][198/233]	Loss 0.0003 (0.0025)	
training:	Epoch: [152][199/233]	Loss 0.0008 (0.0025)	
training:	Epoch: [152][200/233]	Loss 0.0003 (0.0025)	
training:	Epoch: [152][201/233]	Loss 0.0002 (0.0025)	
training:	Epoch: [152][202/233]	Loss 0.0003 (0.0025)	
training:	Epoch: [152][203/233]	Loss 0.0004 (0.0025)	
training:	Epoch: [152][204/233]	Loss 0.0003 (0.0025)	
training:	Epoch: [152][205/233]	Loss 0.0003 (0.0025)	
training:	Epoch: [152][206/233]	Loss 0.0006 (0.0025)	
training:	Epoch: [152][207/233]	Loss 0.0002 (0.0025)	
training:	Epoch: [152][208/233]	Loss 0.0003 (0.0024)	
training:	Epoch: [152][209/233]	Loss 0.0004 (0.0024)	
training:	Epoch: [152][210/233]	Loss 0.0006 (0.0024)	
training:	Epoch: [152][211/233]	Loss 0.0004 (0.0024)	
training:	Epoch: [152][212/233]	Loss 0.0003 (0.0024)	
training:	Epoch: [152][213/233]	Loss 0.0003 (0.0024)	
training:	Epoch: [152][214/233]	Loss 0.0002 (0.0024)	
training:	Epoch: [152][215/233]	Loss 0.0003 (0.0024)	
training:	Epoch: [152][216/233]	Loss 0.0003 (0.0024)	
training:	Epoch: [152][217/233]	Loss 0.0004 (0.0024)	
training:	Epoch: [152][218/233]	Loss 0.0018 (0.0024)	
training:	Epoch: [152][219/233]	Loss 0.0008 (0.0023)	
training:	Epoch: [152][220/233]	Loss 0.0010 (0.0023)	
training:	Epoch: [152][221/233]	Loss 0.0003 (0.0023)	
training:	Epoch: [152][222/233]	Loss 0.0009 (0.0023)	
training:	Epoch: [152][223/233]	Loss 0.0003 (0.0023)	
training:	Epoch: [152][224/233]	Loss 0.0003 (0.0023)	
training:	Epoch: [152][225/233]	Loss 0.0047 (0.0023)	
training:	Epoch: [152][226/233]	Loss 0.0006 (0.0023)	
training:	Epoch: [152][227/233]	Loss 0.0003 (0.0023)	
training:	Epoch: [152][228/233]	Loss 0.0003 (0.0023)	
training:	Epoch: [152][229/233]	Loss 0.0002 (0.0023)	
training:	Epoch: [152][230/233]	Loss 0.0002 (0.0023)	
training:	Epoch: [152][231/233]	Loss 0.0003 (0.0023)	
training:	Epoch: [152][232/233]	Loss 0.0008 (0.0023)	
training:	Epoch: [152][233/233]	Loss 0.0011 (0.0023)	
Training:	 Loss: 0.0023

Training:	 ACC: 1.0000 1.0000 1.0000 1.0000
Validation:	 ACC: 0.7957 0.7961 0.8059 0.7854
Validation:	 Best_BACC: 0.8162 0.8154 0.7998 0.8326
Validation:	 Loss: 1.2761
Pretraining:	Epoch 153/200
----------
training:	Epoch: [153][1/233]	Loss 0.0004 (0.0004)	
training:	Epoch: [153][2/233]	Loss 0.0002 (0.0003)	
training:	Epoch: [153][3/233]	Loss 0.0002 (0.0003)	
training:	Epoch: [153][4/233]	Loss 0.0004 (0.0003)	
training:	Epoch: [153][5/233]	Loss 0.0003 (0.0003)	
training:	Epoch: [153][6/233]	Loss 0.0019 (0.0006)	
training:	Epoch: [153][7/233]	Loss 0.0002 (0.0005)	
training:	Epoch: [153][8/233]	Loss 0.0003 (0.0005)	
training:	Epoch: [153][9/233]	Loss 0.0003 (0.0005)	
training:	Epoch: [153][10/233]	Loss 0.0002 (0.0005)	
training:	Epoch: [153][11/233]	Loss 0.0004 (0.0005)	
training:	Epoch: [153][12/233]	Loss 0.0003 (0.0004)	
training:	Epoch: [153][13/233]	Loss 0.0003 (0.0004)	
training:	Epoch: [153][14/233]	Loss 0.0008 (0.0005)	
training:	Epoch: [153][15/233]	Loss 0.0003 (0.0004)	
training:	Epoch: [153][16/233]	Loss 0.0392 (0.0029)	
training:	Epoch: [153][17/233]	Loss 0.0259 (0.0042)	
training:	Epoch: [153][18/233]	Loss 0.0002 (0.0040)	
training:	Epoch: [153][19/233]	Loss 0.0020 (0.0039)	
training:	Epoch: [153][20/233]	Loss 0.0025 (0.0038)	
training:	Epoch: [153][21/233]	Loss 0.0003 (0.0037)	
training:	Epoch: [153][22/233]	Loss 0.0825 (0.0072)	
training:	Epoch: [153][23/233]	Loss 0.0003 (0.0069)	
training:	Epoch: [153][24/233]	Loss 0.0002 (0.0067)	
training:	Epoch: [153][25/233]	Loss 0.0014 (0.0065)	
training:	Epoch: [153][26/233]	Loss 0.0005 (0.0062)	
training:	Epoch: [153][27/233]	Loss 0.0006 (0.0060)	
training:	Epoch: [153][28/233]	Loss 0.0004 (0.0058)	
training:	Epoch: [153][29/233]	Loss 0.0004 (0.0056)	
training:	Epoch: [153][30/233]	Loss 0.0004 (0.0055)	
training:	Epoch: [153][31/233]	Loss 0.0013 (0.0053)	
training:	Epoch: [153][32/233]	Loss 0.1984 (0.0114)	
training:	Epoch: [153][33/233]	Loss 0.0003 (0.0110)	
training:	Epoch: [153][34/233]	Loss 0.0003 (0.0107)	
training:	Epoch: [153][35/233]	Loss 0.0003 (0.0104)	
training:	Epoch: [153][36/233]	Loss 0.0550 (0.0116)	
training:	Epoch: [153][37/233]	Loss 0.0004 (0.0113)	
training:	Epoch: [153][38/233]	Loss 0.0052 (0.0112)	
training:	Epoch: [153][39/233]	Loss 0.1163 (0.0139)	
training:	Epoch: [153][40/233]	Loss 0.0004 (0.0135)	
training:	Epoch: [153][41/233]	Loss 0.0004 (0.0132)	
training:	Epoch: [153][42/233]	Loss 0.0005 (0.0129)	
training:	Epoch: [153][43/233]	Loss 0.0003 (0.0126)	
training:	Epoch: [153][44/233]	Loss 0.0068 (0.0125)	
training:	Epoch: [153][45/233]	Loss 0.0091 (0.0124)	
training:	Epoch: [153][46/233]	Loss 0.0021 (0.0122)	
training:	Epoch: [153][47/233]	Loss 0.0009 (0.0120)	
training:	Epoch: [153][48/233]	Loss 0.0005 (0.0117)	
training:	Epoch: [153][49/233]	Loss 0.0007 (0.0115)	
training:	Epoch: [153][50/233]	Loss 0.0003 (0.0113)	
training:	Epoch: [153][51/233]	Loss 0.0004 (0.0110)	
training:	Epoch: [153][52/233]	Loss 0.0003 (0.0108)	
training:	Epoch: [153][53/233]	Loss 0.0006 (0.0106)	
training:	Epoch: [153][54/233]	Loss 0.0009 (0.0105)	
training:	Epoch: [153][55/233]	Loss 0.0006 (0.0103)	
training:	Epoch: [153][56/233]	Loss 0.0008 (0.0101)	
training:	Epoch: [153][57/233]	Loss 0.1447 (0.0125)	
training:	Epoch: [153][58/233]	Loss 0.0025 (0.0123)	
training:	Epoch: [153][59/233]	Loss 0.1367 (0.0144)	
training:	Epoch: [153][60/233]	Loss 0.0358 (0.0148)	
training:	Epoch: [153][61/233]	Loss 0.0052 (0.0146)	
training:	Epoch: [153][62/233]	Loss 0.0121 (0.0146)	
training:	Epoch: [153][63/233]	Loss 0.0003 (0.0143)	
training:	Epoch: [153][64/233]	Loss 0.0003 (0.0141)	
training:	Epoch: [153][65/233]	Loss 0.0008 (0.0139)	
training:	Epoch: [153][66/233]	Loss 0.0002 (0.0137)	
training:	Epoch: [153][67/233]	Loss 0.0047 (0.0136)	
training:	Epoch: [153][68/233]	Loss 0.0003 (0.0134)	
training:	Epoch: [153][69/233]	Loss 0.0005 (0.0132)	
training:	Epoch: [153][70/233]	Loss 0.0003 (0.0130)	
training:	Epoch: [153][71/233]	Loss 0.0004 (0.0128)	
training:	Epoch: [153][72/233]	Loss 0.0010 (0.0127)	
training:	Epoch: [153][73/233]	Loss 0.0004 (0.0125)	
training:	Epoch: [153][74/233]	Loss 0.0007 (0.0123)	
training:	Epoch: [153][75/233]	Loss 0.0011 (0.0122)	
training:	Epoch: [153][76/233]	Loss 0.0006 (0.0120)	
training:	Epoch: [153][77/233]	Loss 0.0006 (0.0119)	
training:	Epoch: [153][78/233]	Loss 0.0012 (0.0118)	
training:	Epoch: [153][79/233]	Loss 0.0003 (0.0116)	
training:	Epoch: [153][80/233]	Loss 0.1507 (0.0134)	
training:	Epoch: [153][81/233]	Loss 0.0003 (0.0132)	
training:	Epoch: [153][82/233]	Loss 0.0004 (0.0130)	
training:	Epoch: [153][83/233]	Loss 0.0003 (0.0129)	
training:	Epoch: [153][84/233]	Loss 0.0010 (0.0127)	
training:	Epoch: [153][85/233]	Loss 0.0003 (0.0126)	
training:	Epoch: [153][86/233]	Loss 0.0009 (0.0125)	
training:	Epoch: [153][87/233]	Loss 0.0033 (0.0124)	
training:	Epoch: [153][88/233]	Loss 0.0003 (0.0122)	
training:	Epoch: [153][89/233]	Loss 0.0003 (0.0121)	
training:	Epoch: [153][90/233]	Loss 0.0023 (0.0120)	
training:	Epoch: [153][91/233]	Loss 0.0007 (0.0118)	
training:	Epoch: [153][92/233]	Loss 0.0016 (0.0117)	
training:	Epoch: [153][93/233]	Loss 0.0010 (0.0116)	
training:	Epoch: [153][94/233]	Loss 0.0003 (0.0115)	
training:	Epoch: [153][95/233]	Loss 0.0003 (0.0114)	
training:	Epoch: [153][96/233]	Loss 0.0003 (0.0113)	
training:	Epoch: [153][97/233]	Loss 0.0003 (0.0112)	
training:	Epoch: [153][98/233]	Loss 0.0004 (0.0110)	
training:	Epoch: [153][99/233]	Loss 0.0004 (0.0109)	
training:	Epoch: [153][100/233]	Loss 0.0004 (0.0108)	
training:	Epoch: [153][101/233]	Loss 0.0003 (0.0107)	
training:	Epoch: [153][102/233]	Loss 0.0003 (0.0106)	
training:	Epoch: [153][103/233]	Loss 0.0007 (0.0105)	
training:	Epoch: [153][104/233]	Loss 0.0004 (0.0104)	
training:	Epoch: [153][105/233]	Loss 0.0003 (0.0103)	
training:	Epoch: [153][106/233]	Loss 0.0004 (0.0102)	
training:	Epoch: [153][107/233]	Loss 0.0025 (0.0102)	
training:	Epoch: [153][108/233]	Loss 0.0006 (0.0101)	
training:	Epoch: [153][109/233]	Loss 0.0056 (0.0100)	
training:	Epoch: [153][110/233]	Loss 0.0005 (0.0100)	
training:	Epoch: [153][111/233]	Loss 0.0009 (0.0099)	
training:	Epoch: [153][112/233]	Loss 0.0003 (0.0098)	
training:	Epoch: [153][113/233]	Loss 0.0006 (0.0097)	
training:	Epoch: [153][114/233]	Loss 0.0004 (0.0096)	
training:	Epoch: [153][115/233]	Loss 0.0016 (0.0096)	
training:	Epoch: [153][116/233]	Loss 0.0005 (0.0095)	
training:	Epoch: [153][117/233]	Loss 0.0014 (0.0094)	
training:	Epoch: [153][118/233]	Loss 0.0003 (0.0093)	
training:	Epoch: [153][119/233]	Loss 0.0019 (0.0093)	
training:	Epoch: [153][120/233]	Loss 0.0004 (0.0092)	
training:	Epoch: [153][121/233]	Loss 0.0002 (0.0091)	
training:	Epoch: [153][122/233]	Loss 0.0005 (0.0090)	
training:	Epoch: [153][123/233]	Loss 0.0003 (0.0090)	
training:	Epoch: [153][124/233]	Loss 0.0003 (0.0089)	
training:	Epoch: [153][125/233]	Loss 0.1244 (0.0098)	
training:	Epoch: [153][126/233]	Loss 0.0004 (0.0098)	
training:	Epoch: [153][127/233]	Loss 0.0026 (0.0097)	
training:	Epoch: [153][128/233]	Loss 0.0005 (0.0096)	
training:	Epoch: [153][129/233]	Loss 0.0010 (0.0096)	
training:	Epoch: [153][130/233]	Loss 0.0003 (0.0095)	
training:	Epoch: [153][131/233]	Loss 0.0007 (0.0094)	
training:	Epoch: [153][132/233]	Loss 0.0008 (0.0094)	
training:	Epoch: [153][133/233]	Loss 0.0006 (0.0093)	
training:	Epoch: [153][134/233]	Loss 0.0006 (0.0092)	
training:	Epoch: [153][135/233]	Loss 0.0003 (0.0092)	
training:	Epoch: [153][136/233]	Loss 0.0011 (0.0091)	
training:	Epoch: [153][137/233]	Loss 0.0141 (0.0091)	
training:	Epoch: [153][138/233]	Loss 0.0007 (0.0091)	
training:	Epoch: [153][139/233]	Loss 0.0040 (0.0090)	
training:	Epoch: [153][140/233]	Loss 0.0003 (0.0090)	
training:	Epoch: [153][141/233]	Loss 0.0005 (0.0089)	
training:	Epoch: [153][142/233]	Loss 0.0003 (0.0089)	
training:	Epoch: [153][143/233]	Loss 0.0010 (0.0088)	
training:	Epoch: [153][144/233]	Loss 0.0005 (0.0087)	
training:	Epoch: [153][145/233]	Loss 0.0009 (0.0087)	
training:	Epoch: [153][146/233]	Loss 0.0004 (0.0086)	
training:	Epoch: [153][147/233]	Loss 0.0024 (0.0086)	
training:	Epoch: [153][148/233]	Loss 0.0006 (0.0085)	
training:	Epoch: [153][149/233]	Loss 0.0005 (0.0085)	
training:	Epoch: [153][150/233]	Loss 0.0003 (0.0084)	
training:	Epoch: [153][151/233]	Loss 0.0003 (0.0084)	
training:	Epoch: [153][152/233]	Loss 0.0008 (0.0083)	
training:	Epoch: [153][153/233]	Loss 0.0003 (0.0083)	
training:	Epoch: [153][154/233]	Loss 0.0006 (0.0082)	
training:	Epoch: [153][155/233]	Loss 0.1807 (0.0093)	
training:	Epoch: [153][156/233]	Loss 0.0009 (0.0093)	
training:	Epoch: [153][157/233]	Loss 0.0006 (0.0092)	
training:	Epoch: [153][158/233]	Loss 0.0005 (0.0092)	
training:	Epoch: [153][159/233]	Loss 0.0003 (0.0091)	
training:	Epoch: [153][160/233]	Loss 0.0004 (0.0091)	
training:	Epoch: [153][161/233]	Loss 0.0003 (0.0090)	
training:	Epoch: [153][162/233]	Loss 0.0003 (0.0090)	
training:	Epoch: [153][163/233]	Loss 0.0006 (0.0089)	
training:	Epoch: [153][164/233]	Loss 0.0003 (0.0088)	
training:	Epoch: [153][165/233]	Loss 0.0003 (0.0088)	
training:	Epoch: [153][166/233]	Loss 0.0004 (0.0087)	
training:	Epoch: [153][167/233]	Loss 0.0006 (0.0087)	
training:	Epoch: [153][168/233]	Loss 0.0003 (0.0086)	
training:	Epoch: [153][169/233]	Loss 0.0007 (0.0086)	
training:	Epoch: [153][170/233]	Loss 0.0005 (0.0086)	
training:	Epoch: [153][171/233]	Loss 0.0009 (0.0085)	
training:	Epoch: [153][172/233]	Loss 0.0016 (0.0085)	
training:	Epoch: [153][173/233]	Loss 0.0011 (0.0084)	
training:	Epoch: [153][174/233]	Loss 0.0004 (0.0084)	
training:	Epoch: [153][175/233]	Loss 0.0003 (0.0083)	
training:	Epoch: [153][176/233]	Loss 0.0009 (0.0083)	
training:	Epoch: [153][177/233]	Loss 0.0007 (0.0082)	
training:	Epoch: [153][178/233]	Loss 0.0008 (0.0082)	
training:	Epoch: [153][179/233]	Loss 0.0005 (0.0082)	
training:	Epoch: [153][180/233]	Loss 0.0005 (0.0081)	
training:	Epoch: [153][181/233]	Loss 0.0009 (0.0081)	
training:	Epoch: [153][182/233]	Loss 0.0004 (0.0080)	
training:	Epoch: [153][183/233]	Loss 0.0015 (0.0080)	
training:	Epoch: [153][184/233]	Loss 0.0009 (0.0080)	
training:	Epoch: [153][185/233]	Loss 0.0005 (0.0079)	
training:	Epoch: [153][186/233]	Loss 0.0010 (0.0079)	
training:	Epoch: [153][187/233]	Loss 0.0004 (0.0078)	
training:	Epoch: [153][188/233]	Loss 0.0004 (0.0078)	
training:	Epoch: [153][189/233]	Loss 0.0003 (0.0078)	
training:	Epoch: [153][190/233]	Loss 0.0066 (0.0078)	
training:	Epoch: [153][191/233]	Loss 0.0006 (0.0077)	
training:	Epoch: [153][192/233]	Loss 0.0003 (0.0077)	
training:	Epoch: [153][193/233]	Loss 0.0005 (0.0076)	
training:	Epoch: [153][194/233]	Loss 0.0004 (0.0076)	
training:	Epoch: [153][195/233]	Loss 0.0004 (0.0076)	
training:	Epoch: [153][196/233]	Loss 0.0004 (0.0075)	
training:	Epoch: [153][197/233]	Loss 0.0004 (0.0075)	
training:	Epoch: [153][198/233]	Loss 0.0003 (0.0075)	
training:	Epoch: [153][199/233]	Loss 0.0003 (0.0074)	
training:	Epoch: [153][200/233]	Loss 0.0003 (0.0074)	
training:	Epoch: [153][201/233]	Loss 0.0003 (0.0074)	
training:	Epoch: [153][202/233]	Loss 0.0119 (0.0074)	
training:	Epoch: [153][203/233]	Loss 0.0006 (0.0073)	
training:	Epoch: [153][204/233]	Loss 0.0027 (0.0073)	
training:	Epoch: [153][205/233]	Loss 0.0023 (0.0073)	
training:	Epoch: [153][206/233]	Loss 0.0004 (0.0073)	
training:	Epoch: [153][207/233]	Loss 0.0009 (0.0072)	
training:	Epoch: [153][208/233]	Loss 0.0003 (0.0072)	
training:	Epoch: [153][209/233]	Loss 0.0032 (0.0072)	
training:	Epoch: [153][210/233]	Loss 0.0004 (0.0071)	
training:	Epoch: [153][211/233]	Loss 0.0004 (0.0071)	
training:	Epoch: [153][212/233]	Loss 0.0003 (0.0071)	
training:	Epoch: [153][213/233]	Loss 0.0010 (0.0071)	
training:	Epoch: [153][214/233]	Loss 0.0012 (0.0070)	
training:	Epoch: [153][215/233]	Loss 0.0004 (0.0070)	
training:	Epoch: [153][216/233]	Loss 0.0006 (0.0070)	
training:	Epoch: [153][217/233]	Loss 0.0018 (0.0069)	
training:	Epoch: [153][218/233]	Loss 0.0524 (0.0072)	
training:	Epoch: [153][219/233]	Loss 0.0004 (0.0071)	
training:	Epoch: [153][220/233]	Loss 0.0005 (0.0071)	
training:	Epoch: [153][221/233]	Loss 0.0010 (0.0071)	
training:	Epoch: [153][222/233]	Loss 0.0005 (0.0070)	
training:	Epoch: [153][223/233]	Loss 0.0006 (0.0070)	
training:	Epoch: [153][224/233]	Loss 0.0003 (0.0070)	
training:	Epoch: [153][225/233]	Loss 0.0003 (0.0069)	
training:	Epoch: [153][226/233]	Loss 0.0006 (0.0069)	
training:	Epoch: [153][227/233]	Loss 0.0003 (0.0069)	
training:	Epoch: [153][228/233]	Loss 0.0004 (0.0069)	
training:	Epoch: [153][229/233]	Loss 0.0004 (0.0068)	
training:	Epoch: [153][230/233]	Loss 0.0008 (0.0068)	
training:	Epoch: [153][231/233]	Loss 0.0003 (0.0068)	
training:	Epoch: [153][232/233]	Loss 0.0007 (0.0068)	
training:	Epoch: [153][233/233]	Loss 0.0005 (0.0067)	
Training:	 Loss: 0.0067

Training:	 ACC: 1.0000 1.0000 1.0000 1.0000
Validation:	 ACC: 0.7931 0.7940 0.8121 0.7742
Validation:	 Best_BACC: 0.8162 0.8154 0.7998 0.8326
Validation:	 Loss: 1.2150
Pretraining:	Epoch 154/200
----------
training:	Epoch: [154][1/233]	Loss 0.0003 (0.0003)	
training:	Epoch: [154][2/233]	Loss 0.0214 (0.0108)	
training:	Epoch: [154][3/233]	Loss 0.0003 (0.0073)	
training:	Epoch: [154][4/233]	Loss 0.0086 (0.0076)	
training:	Epoch: [154][5/233]	Loss 0.0041 (0.0069)	
training:	Epoch: [154][6/233]	Loss 0.0004 (0.0058)	
training:	Epoch: [154][7/233]	Loss 0.0003 (0.0051)	
training:	Epoch: [154][8/233]	Loss 0.0693 (0.0131)	
training:	Epoch: [154][9/233]	Loss 0.0003 (0.0117)	
training:	Epoch: [154][10/233]	Loss 0.0004 (0.0105)	
training:	Epoch: [154][11/233]	Loss 0.0019 (0.0097)	
training:	Epoch: [154][12/233]	Loss 0.0003 (0.0090)	
training:	Epoch: [154][13/233]	Loss 0.0009 (0.0083)	
training:	Epoch: [154][14/233]	Loss 0.0005 (0.0078)	
training:	Epoch: [154][15/233]	Loss 0.0008 (0.0073)	
training:	Epoch: [154][16/233]	Loss 0.0004 (0.0069)	
training:	Epoch: [154][17/233]	Loss 0.0003 (0.0065)	
training:	Epoch: [154][18/233]	Loss 0.0003 (0.0061)	
training:	Epoch: [154][19/233]	Loss 0.0004 (0.0058)	
training:	Epoch: [154][20/233]	Loss 0.0004 (0.0056)	
training:	Epoch: [154][21/233]	Loss 0.0004 (0.0053)	
training:	Epoch: [154][22/233]	Loss 0.0003 (0.0051)	
training:	Epoch: [154][23/233]	Loss 0.0004 (0.0049)	
training:	Epoch: [154][24/233]	Loss 0.0008 (0.0047)	
training:	Epoch: [154][25/233]	Loss 0.0005 (0.0046)	
training:	Epoch: [154][26/233]	Loss 0.0006 (0.0044)	
training:	Epoch: [154][27/233]	Loss 0.0004 (0.0043)	
training:	Epoch: [154][28/233]	Loss 0.0002 (0.0041)	
training:	Epoch: [154][29/233]	Loss 0.0009 (0.0040)	
training:	Epoch: [154][30/233]	Loss 0.0019 (0.0039)	
training:	Epoch: [154][31/233]	Loss 0.0005 (0.0038)	
training:	Epoch: [154][32/233]	Loss 0.0277 (0.0046)	
training:	Epoch: [154][33/233]	Loss 0.0006 (0.0044)	
training:	Epoch: [154][34/233]	Loss 0.0013 (0.0044)	
training:	Epoch: [154][35/233]	Loss 0.0024 (0.0043)	
training:	Epoch: [154][36/233]	Loss 0.0084 (0.0044)	
training:	Epoch: [154][37/233]	Loss 0.0008 (0.0043)	
training:	Epoch: [154][38/233]	Loss 0.0003 (0.0042)	
training:	Epoch: [154][39/233]	Loss 0.0003 (0.0041)	
training:	Epoch: [154][40/233]	Loss 0.0003 (0.0040)	
training:	Epoch: [154][41/233]	Loss 0.0004 (0.0039)	
training:	Epoch: [154][42/233]	Loss 0.0005 (0.0038)	
training:	Epoch: [154][43/233]	Loss 0.0007 (0.0038)	
training:	Epoch: [154][44/233]	Loss 0.0004 (0.0037)	
training:	Epoch: [154][45/233]	Loss 0.0003 (0.0036)	
training:	Epoch: [154][46/233]	Loss 0.0003 (0.0035)	
training:	Epoch: [154][47/233]	Loss 0.0004 (0.0035)	
training:	Epoch: [154][48/233]	Loss 0.0007 (0.0034)	
training:	Epoch: [154][49/233]	Loss 0.0005 (0.0034)	
training:	Epoch: [154][50/233]	Loss 0.0004 (0.0033)	
training:	Epoch: [154][51/233]	Loss 0.0066 (0.0034)	
training:	Epoch: [154][52/233]	Loss 0.0033 (0.0034)	
training:	Epoch: [154][53/233]	Loss 0.0236 (0.0037)	
training:	Epoch: [154][54/233]	Loss 0.0035 (0.0037)	
training:	Epoch: [154][55/233]	Loss 0.0004 (0.0037)	
training:	Epoch: [154][56/233]	Loss 0.0004 (0.0036)	
training:	Epoch: [154][57/233]	Loss 0.0016 (0.0036)	
training:	Epoch: [154][58/233]	Loss 0.0003 (0.0035)	
training:	Epoch: [154][59/233]	Loss 0.0003 (0.0035)	
training:	Epoch: [154][60/233]	Loss 0.0003 (0.0034)	
training:	Epoch: [154][61/233]	Loss 0.0004 (0.0034)	
training:	Epoch: [154][62/233]	Loss 0.0003 (0.0033)	
training:	Epoch: [154][63/233]	Loss 0.0002 (0.0033)	
training:	Epoch: [154][64/233]	Loss 0.0003 (0.0032)	
training:	Epoch: [154][65/233]	Loss 0.0003 (0.0032)	
training:	Epoch: [154][66/233]	Loss 0.0003 (0.0031)	
training:	Epoch: [154][67/233]	Loss 0.0003 (0.0031)	
training:	Epoch: [154][68/233]	Loss 0.0038 (0.0031)	
training:	Epoch: [154][69/233]	Loss 0.0003 (0.0031)	
training:	Epoch: [154][70/233]	Loss 0.0005 (0.0030)	
training:	Epoch: [154][71/233]	Loss 0.0005 (0.0030)	
training:	Epoch: [154][72/233]	Loss 0.0006 (0.0030)	
training:	Epoch: [154][73/233]	Loss 0.0003 (0.0029)	
training:	Epoch: [154][74/233]	Loss 0.0003 (0.0029)	
training:	Epoch: [154][75/233]	Loss 0.0013 (0.0029)	
training:	Epoch: [154][76/233]	Loss 0.0004 (0.0028)	
training:	Epoch: [154][77/233]	Loss 0.0003 (0.0028)	
training:	Epoch: [154][78/233]	Loss 0.0003 (0.0028)	
training:	Epoch: [154][79/233]	Loss 0.0006 (0.0027)	
training:	Epoch: [154][80/233]	Loss 0.0005 (0.0027)	
training:	Epoch: [154][81/233]	Loss 0.0039 (0.0027)	
training:	Epoch: [154][82/233]	Loss 0.0004 (0.0027)	
training:	Epoch: [154][83/233]	Loss 0.0005 (0.0027)	
training:	Epoch: [154][84/233]	Loss 0.0004 (0.0026)	
training:	Epoch: [154][85/233]	Loss 0.0002 (0.0026)	
training:	Epoch: [154][86/233]	Loss 0.0003 (0.0026)	
training:	Epoch: [154][87/233]	Loss 0.0563 (0.0032)	
training:	Epoch: [154][88/233]	Loss 0.0052 (0.0032)	
training:	Epoch: [154][89/233]	Loss 0.0003 (0.0032)	
training:	Epoch: [154][90/233]	Loss 0.0004 (0.0032)	
training:	Epoch: [154][91/233]	Loss 0.0005 (0.0031)	
training:	Epoch: [154][92/233]	Loss 0.0004 (0.0031)	
training:	Epoch: [154][93/233]	Loss 0.0006 (0.0031)	
training:	Epoch: [154][94/233]	Loss 0.0010 (0.0031)	
training:	Epoch: [154][95/233]	Loss 0.0003 (0.0030)	
training:	Epoch: [154][96/233]	Loss 0.0002 (0.0030)	
training:	Epoch: [154][97/233]	Loss 0.0005 (0.0030)	
training:	Epoch: [154][98/233]	Loss 0.0003 (0.0030)	
training:	Epoch: [154][99/233]	Loss 0.0002 (0.0029)	
training:	Epoch: [154][100/233]	Loss 0.0003 (0.0029)	
training:	Epoch: [154][101/233]	Loss 0.0002 (0.0029)	
training:	Epoch: [154][102/233]	Loss 0.0002 (0.0028)	
training:	Epoch: [154][103/233]	Loss 0.0003 (0.0028)	
training:	Epoch: [154][104/233]	Loss 0.0004 (0.0028)	
training:	Epoch: [154][105/233]	Loss 0.0004 (0.0028)	
training:	Epoch: [154][106/233]	Loss 0.0003 (0.0028)	
training:	Epoch: [154][107/233]	Loss 0.0003 (0.0027)	
training:	Epoch: [154][108/233]	Loss 0.0004 (0.0027)	
training:	Epoch: [154][109/233]	Loss 0.0004 (0.0027)	
training:	Epoch: [154][110/233]	Loss 0.0004 (0.0027)	
training:	Epoch: [154][111/233]	Loss 0.0003 (0.0026)	
training:	Epoch: [154][112/233]	Loss 0.0005 (0.0026)	
training:	Epoch: [154][113/233]	Loss 0.0003 (0.0026)	
training:	Epoch: [154][114/233]	Loss 0.0004 (0.0026)	
training:	Epoch: [154][115/233]	Loss 0.0004 (0.0026)	
training:	Epoch: [154][116/233]	Loss 0.0004 (0.0025)	
training:	Epoch: [154][117/233]	Loss 0.0005 (0.0025)	
training:	Epoch: [154][118/233]	Loss 0.0016 (0.0025)	
training:	Epoch: [154][119/233]	Loss 0.0052 (0.0025)	
training:	Epoch: [154][120/233]	Loss 0.0004 (0.0025)	
training:	Epoch: [154][121/233]	Loss 0.0013 (0.0025)	
training:	Epoch: [154][122/233]	Loss 0.0005 (0.0025)	
training:	Epoch: [154][123/233]	Loss 0.0004 (0.0025)	
training:	Epoch: [154][124/233]	Loss 0.0005 (0.0025)	
training:	Epoch: [154][125/233]	Loss 0.0004 (0.0024)	
training:	Epoch: [154][126/233]	Loss 0.0003 (0.0024)	
training:	Epoch: [154][127/233]	Loss 0.0002 (0.0024)	
training:	Epoch: [154][128/233]	Loss 0.0005 (0.0024)	
training:	Epoch: [154][129/233]	Loss 0.0015 (0.0024)	
training:	Epoch: [154][130/233]	Loss 0.0309 (0.0026)	
training:	Epoch: [154][131/233]	Loss 0.0003 (0.0026)	
training:	Epoch: [154][132/233]	Loss 0.0005 (0.0026)	
training:	Epoch: [154][133/233]	Loss 0.0029 (0.0026)	
training:	Epoch: [154][134/233]	Loss 0.0007 (0.0026)	
training:	Epoch: [154][135/233]	Loss 0.0005 (0.0026)	
training:	Epoch: [154][136/233]	Loss 0.0007 (0.0025)	
training:	Epoch: [154][137/233]	Loss 0.0005 (0.0025)	
training:	Epoch: [154][138/233]	Loss 0.0003 (0.0025)	
training:	Epoch: [154][139/233]	Loss 0.0004 (0.0025)	
training:	Epoch: [154][140/233]	Loss 0.0006 (0.0025)	
training:	Epoch: [154][141/233]	Loss 0.0006 (0.0025)	
training:	Epoch: [154][142/233]	Loss 0.1593 (0.0036)	
training:	Epoch: [154][143/233]	Loss 0.0003 (0.0035)	
training:	Epoch: [154][144/233]	Loss 0.0009 (0.0035)	
training:	Epoch: [154][145/233]	Loss 0.0004 (0.0035)	
training:	Epoch: [154][146/233]	Loss 0.0010 (0.0035)	
training:	Epoch: [154][147/233]	Loss 0.0004 (0.0035)	
training:	Epoch: [154][148/233]	Loss 0.0004 (0.0034)	
training:	Epoch: [154][149/233]	Loss 0.0003 (0.0034)	
training:	Epoch: [154][150/233]	Loss 0.0008 (0.0034)	
training:	Epoch: [154][151/233]	Loss 0.0015 (0.0034)	
training:	Epoch: [154][152/233]	Loss 0.0013 (0.0034)	
training:	Epoch: [154][153/233]	Loss 0.0009 (0.0034)	
training:	Epoch: [154][154/233]	Loss 0.1612 (0.0044)	
training:	Epoch: [154][155/233]	Loss 0.0004 (0.0044)	
training:	Epoch: [154][156/233]	Loss 0.0006 (0.0043)	
training:	Epoch: [154][157/233]	Loss 0.0003 (0.0043)	
training:	Epoch: [154][158/233]	Loss 0.0020 (0.0043)	
training:	Epoch: [154][159/233]	Loss 0.0006 (0.0043)	
training:	Epoch: [154][160/233]	Loss 0.0003 (0.0043)	
training:	Epoch: [154][161/233]	Loss 0.0023 (0.0042)	
training:	Epoch: [154][162/233]	Loss 0.0014 (0.0042)	
training:	Epoch: [154][163/233]	Loss 0.0003 (0.0042)	
training:	Epoch: [154][164/233]	Loss 0.0007 (0.0042)	
training:	Epoch: [154][165/233]	Loss 0.0003 (0.0042)	
training:	Epoch: [154][166/233]	Loss 0.0009 (0.0041)	
training:	Epoch: [154][167/233]	Loss 0.0003 (0.0041)	
training:	Epoch: [154][168/233]	Loss 0.0026 (0.0041)	
training:	Epoch: [154][169/233]	Loss 0.0003 (0.0041)	
training:	Epoch: [154][170/233]	Loss 0.0003 (0.0041)	
training:	Epoch: [154][171/233]	Loss 0.0004 (0.0040)	
training:	Epoch: [154][172/233]	Loss 0.0003 (0.0040)	
training:	Epoch: [154][173/233]	Loss 0.0003 (0.0040)	
training:	Epoch: [154][174/233]	Loss 0.0008 (0.0040)	
training:	Epoch: [154][175/233]	Loss 0.0006 (0.0040)	
training:	Epoch: [154][176/233]	Loss 0.0003 (0.0039)	
training:	Epoch: [154][177/233]	Loss 0.0004 (0.0039)	
training:	Epoch: [154][178/233]	Loss 0.0005 (0.0039)	
training:	Epoch: [154][179/233]	Loss 0.0004 (0.0039)	
training:	Epoch: [154][180/233]	Loss 0.0015 (0.0039)	
training:	Epoch: [154][181/233]	Loss 0.0006 (0.0038)	
training:	Epoch: [154][182/233]	Loss 0.0003 (0.0038)	
training:	Epoch: [154][183/233]	Loss 0.0003 (0.0038)	
training:	Epoch: [154][184/233]	Loss 0.0003 (0.0038)	
training:	Epoch: [154][185/233]	Loss 0.0003 (0.0038)	
training:	Epoch: [154][186/233]	Loss 0.0003 (0.0037)	
training:	Epoch: [154][187/233]	Loss 0.0041 (0.0038)	
training:	Epoch: [154][188/233]	Loss 0.0008 (0.0037)	
training:	Epoch: [154][189/233]	Loss 0.0003 (0.0037)	
training:	Epoch: [154][190/233]	Loss 0.0003 (0.0037)	
training:	Epoch: [154][191/233]	Loss 0.0050 (0.0037)	
training:	Epoch: [154][192/233]	Loss 0.0004 (0.0037)	
training:	Epoch: [154][193/233]	Loss 0.0004 (0.0037)	
training:	Epoch: [154][194/233]	Loss 0.0004 (0.0037)	
training:	Epoch: [154][195/233]	Loss 0.0004 (0.0036)	
training:	Epoch: [154][196/233]	Loss 0.0005 (0.0036)	
training:	Epoch: [154][197/233]	Loss 0.0005 (0.0036)	
training:	Epoch: [154][198/233]	Loss 0.0003 (0.0036)	
training:	Epoch: [154][199/233]	Loss 0.0003 (0.0036)	
training:	Epoch: [154][200/233]	Loss 0.0003 (0.0036)	
training:	Epoch: [154][201/233]	Loss 0.0004 (0.0035)	
training:	Epoch: [154][202/233]	Loss 0.0003 (0.0035)	
training:	Epoch: [154][203/233]	Loss 0.0003 (0.0035)	
training:	Epoch: [154][204/233]	Loss 0.0006 (0.0035)	
training:	Epoch: [154][205/233]	Loss 0.0003 (0.0035)	
training:	Epoch: [154][206/233]	Loss 0.0017 (0.0035)	
training:	Epoch: [154][207/233]	Loss 0.0007 (0.0035)	
training:	Epoch: [154][208/233]	Loss 0.0003 (0.0034)	
training:	Epoch: [154][209/233]	Loss 0.0253 (0.0035)	
training:	Epoch: [154][210/233]	Loss 0.0003 (0.0035)	
training:	Epoch: [154][211/233]	Loss 0.0004 (0.0035)	
training:	Epoch: [154][212/233]	Loss 0.0013 (0.0035)	
training:	Epoch: [154][213/233]	Loss 0.0003 (0.0035)	
training:	Epoch: [154][214/233]	Loss 0.0005 (0.0035)	
training:	Epoch: [154][215/233]	Loss 0.0009 (0.0035)	
training:	Epoch: [154][216/233]	Loss 0.0007 (0.0035)	
training:	Epoch: [154][217/233]	Loss 0.0004 (0.0034)	
training:	Epoch: [154][218/233]	Loss 0.0004 (0.0034)	
training:	Epoch: [154][219/233]	Loss 0.0003 (0.0034)	
training:	Epoch: [154][220/233]	Loss 0.0011 (0.0034)	
training:	Epoch: [154][221/233]	Loss 0.0004 (0.0034)	
training:	Epoch: [154][222/233]	Loss 0.0003 (0.0034)	
training:	Epoch: [154][223/233]	Loss 0.0007 (0.0034)	
training:	Epoch: [154][224/233]	Loss 0.0006 (0.0033)	
training:	Epoch: [154][225/233]	Loss 0.0003 (0.0033)	
training:	Epoch: [154][226/233]	Loss 0.0003 (0.0033)	
training:	Epoch: [154][227/233]	Loss 0.0014 (0.0033)	
training:	Epoch: [154][228/233]	Loss 0.0051 (0.0033)	
training:	Epoch: [154][229/233]	Loss 0.0008 (0.0033)	
training:	Epoch: [154][230/233]	Loss 0.0004 (0.0033)	
training:	Epoch: [154][231/233]	Loss 0.0007 (0.0033)	
training:	Epoch: [154][232/233]	Loss 0.0003 (0.0033)	
training:	Epoch: [154][233/233]	Loss 0.0039 (0.0033)	
Training:	 Loss: 0.0033

Training:	 ACC: 1.0000 1.0000 1.0000 1.0000
Validation:	 ACC: 0.7965 0.7967 0.7998 0.7933
Validation:	 Best_BACC: 0.8162 0.8154 0.7998 0.8326
Validation:	 Loss: 1.2226
Pretraining:	Epoch 155/200
----------
training:	Epoch: [155][1/233]	Loss 0.0003 (0.0003)	
training:	Epoch: [155][2/233]	Loss 0.0003 (0.0003)	
training:	Epoch: [155][3/233]	Loss 0.0003 (0.0003)	
training:	Epoch: [155][4/233]	Loss 0.0007 (0.0004)	
training:	Epoch: [155][5/233]	Loss 0.0067 (0.0017)	
training:	Epoch: [155][6/233]	Loss 0.0003 (0.0014)	
training:	Epoch: [155][7/233]	Loss 0.0003 (0.0013)	
training:	Epoch: [155][8/233]	Loss 0.0113 (0.0025)	
training:	Epoch: [155][9/233]	Loss 0.0003 (0.0023)	
training:	Epoch: [155][10/233]	Loss 0.0004 (0.0021)	
training:	Epoch: [155][11/233]	Loss 0.0002 (0.0019)	
training:	Epoch: [155][12/233]	Loss 0.0003 (0.0018)	
training:	Epoch: [155][13/233]	Loss 0.0005 (0.0017)	
training:	Epoch: [155][14/233]	Loss 0.0003 (0.0016)	
training:	Epoch: [155][15/233]	Loss 0.0004 (0.0015)	
training:	Epoch: [155][16/233]	Loss 0.0004 (0.0014)	
training:	Epoch: [155][17/233]	Loss 0.0003 (0.0014)	
training:	Epoch: [155][18/233]	Loss 0.0003 (0.0013)	
training:	Epoch: [155][19/233]	Loss 0.0003 (0.0013)	
training:	Epoch: [155][20/233]	Loss 0.0007 (0.0012)	
training:	Epoch: [155][21/233]	Loss 0.0052 (0.0014)	
training:	Epoch: [155][22/233]	Loss 0.0216 (0.0023)	
training:	Epoch: [155][23/233]	Loss 0.0003 (0.0022)	
training:	Epoch: [155][24/233]	Loss 0.0005 (0.0022)	
training:	Epoch: [155][25/233]	Loss 0.0002 (0.0021)	
training:	Epoch: [155][26/233]	Loss 0.0005 (0.0020)	
training:	Epoch: [155][27/233]	Loss 0.0005 (0.0020)	
training:	Epoch: [155][28/233]	Loss 0.0010 (0.0019)	
training:	Epoch: [155][29/233]	Loss 0.0004 (0.0019)	
training:	Epoch: [155][30/233]	Loss 0.0003 (0.0018)	
training:	Epoch: [155][31/233]	Loss 0.0003 (0.0018)	
training:	Epoch: [155][32/233]	Loss 0.0005 (0.0017)	
training:	Epoch: [155][33/233]	Loss 0.0003 (0.0017)	
training:	Epoch: [155][34/233]	Loss 0.0012 (0.0017)	
training:	Epoch: [155][35/233]	Loss 0.0011 (0.0017)	
training:	Epoch: [155][36/233]	Loss 0.0003 (0.0016)	
training:	Epoch: [155][37/233]	Loss 0.0003 (0.0016)	
training:	Epoch: [155][38/233]	Loss 0.0014 (0.0016)	
training:	Epoch: [155][39/233]	Loss 0.0003 (0.0016)	
training:	Epoch: [155][40/233]	Loss 0.0022 (0.0016)	
training:	Epoch: [155][41/233]	Loss 0.0003 (0.0015)	
training:	Epoch: [155][42/233]	Loss 0.0400 (0.0025)	
training:	Epoch: [155][43/233]	Loss 0.0002 (0.0024)	
training:	Epoch: [155][44/233]	Loss 0.0003 (0.0024)	
training:	Epoch: [155][45/233]	Loss 0.0005 (0.0023)	
training:	Epoch: [155][46/233]	Loss 0.0011 (0.0023)	
training:	Epoch: [155][47/233]	Loss 0.0003 (0.0022)	
training:	Epoch: [155][48/233]	Loss 0.0003 (0.0022)	
training:	Epoch: [155][49/233]	Loss 0.0004 (0.0022)	
training:	Epoch: [155][50/233]	Loss 0.0003 (0.0021)	
training:	Epoch: [155][51/233]	Loss 0.0657 (0.0034)	
training:	Epoch: [155][52/233]	Loss 0.0003 (0.0033)	
training:	Epoch: [155][53/233]	Loss 0.0003 (0.0033)	
training:	Epoch: [155][54/233]	Loss 0.0005 (0.0032)	
training:	Epoch: [155][55/233]	Loss 0.0003 (0.0032)	
training:	Epoch: [155][56/233]	Loss 0.0003 (0.0031)	
training:	Epoch: [155][57/233]	Loss 0.0031 (0.0031)	
training:	Epoch: [155][58/233]	Loss 0.0003 (0.0031)	
training:	Epoch: [155][59/233]	Loss 0.0008 (0.0030)	
training:	Epoch: [155][60/233]	Loss 0.0004 (0.0030)	
training:	Epoch: [155][61/233]	Loss 0.0005 (0.0029)	
training:	Epoch: [155][62/233]	Loss 0.0005 (0.0029)	
training:	Epoch: [155][63/233]	Loss 0.0003 (0.0029)	
training:	Epoch: [155][64/233]	Loss 0.0003 (0.0028)	
training:	Epoch: [155][65/233]	Loss 0.0003 (0.0028)	
training:	Epoch: [155][66/233]	Loss 0.0005 (0.0027)	
training:	Epoch: [155][67/233]	Loss 0.0011 (0.0027)	
training:	Epoch: [155][68/233]	Loss 0.0003 (0.0027)	
training:	Epoch: [155][69/233]	Loss 0.0059 (0.0027)	
training:	Epoch: [155][70/233]	Loss 0.0003 (0.0027)	
training:	Epoch: [155][71/233]	Loss 0.0058 (0.0027)	
training:	Epoch: [155][72/233]	Loss 0.0005 (0.0027)	
training:	Epoch: [155][73/233]	Loss 0.0007 (0.0027)	
training:	Epoch: [155][74/233]	Loss 0.0003 (0.0026)	
training:	Epoch: [155][75/233]	Loss 0.0002 (0.0026)	
training:	Epoch: [155][76/233]	Loss 0.0004 (0.0026)	
training:	Epoch: [155][77/233]	Loss 0.0002 (0.0026)	
training:	Epoch: [155][78/233]	Loss 0.0003 (0.0025)	
training:	Epoch: [155][79/233]	Loss 0.0003 (0.0025)	
training:	Epoch: [155][80/233]	Loss 0.0003 (0.0025)	
training:	Epoch: [155][81/233]	Loss 0.0003 (0.0024)	
training:	Epoch: [155][82/233]	Loss 0.0011 (0.0024)	
training:	Epoch: [155][83/233]	Loss 0.0003 (0.0024)	
training:	Epoch: [155][84/233]	Loss 0.0003 (0.0024)	
training:	Epoch: [155][85/233]	Loss 0.0003 (0.0024)	
training:	Epoch: [155][86/233]	Loss 0.0003 (0.0023)	
training:	Epoch: [155][87/233]	Loss 0.0008 (0.0023)	
training:	Epoch: [155][88/233]	Loss 0.0004 (0.0023)	
training:	Epoch: [155][89/233]	Loss 0.0003 (0.0023)	
training:	Epoch: [155][90/233]	Loss 0.0007 (0.0022)	
training:	Epoch: [155][91/233]	Loss 0.0033 (0.0023)	
training:	Epoch: [155][92/233]	Loss 0.0003 (0.0022)	
training:	Epoch: [155][93/233]	Loss 0.0002 (0.0022)	
training:	Epoch: [155][94/233]	Loss 0.0003 (0.0022)	
training:	Epoch: [155][95/233]	Loss 0.0003 (0.0022)	
training:	Epoch: [155][96/233]	Loss 0.0003 (0.0022)	
training:	Epoch: [155][97/233]	Loss 0.0003 (0.0021)	
training:	Epoch: [155][98/233]	Loss 0.0004 (0.0021)	
training:	Epoch: [155][99/233]	Loss 0.0003 (0.0021)	
training:	Epoch: [155][100/233]	Loss 0.0006 (0.0021)	
training:	Epoch: [155][101/233]	Loss 0.0003 (0.0021)	
training:	Epoch: [155][102/233]	Loss 0.0003 (0.0021)	
training:	Epoch: [155][103/233]	Loss 0.0003 (0.0020)	
training:	Epoch: [155][104/233]	Loss 0.0031 (0.0020)	
training:	Epoch: [155][105/233]	Loss 0.0006 (0.0020)	
training:	Epoch: [155][106/233]	Loss 0.0002 (0.0020)	
training:	Epoch: [155][107/233]	Loss 0.0008 (0.0020)	
training:	Epoch: [155][108/233]	Loss 0.0005 (0.0020)	
training:	Epoch: [155][109/233]	Loss 0.0004 (0.0020)	
training:	Epoch: [155][110/233]	Loss 0.0002 (0.0020)	
training:	Epoch: [155][111/233]	Loss 0.0003 (0.0019)	
training:	Epoch: [155][112/233]	Loss 0.0006 (0.0019)	
training:	Epoch: [155][113/233]	Loss 0.0003 (0.0019)	
training:	Epoch: [155][114/233]	Loss 0.0003 (0.0019)	
training:	Epoch: [155][115/233]	Loss 0.0003 (0.0019)	
training:	Epoch: [155][116/233]	Loss 0.0009 (0.0019)	
training:	Epoch: [155][117/233]	Loss 0.0002 (0.0019)	
training:	Epoch: [155][118/233]	Loss 0.0002 (0.0019)	
training:	Epoch: [155][119/233]	Loss 0.0013 (0.0018)	
training:	Epoch: [155][120/233]	Loss 0.0002 (0.0018)	
training:	Epoch: [155][121/233]	Loss 0.0028 (0.0018)	
training:	Epoch: [155][122/233]	Loss 0.0002 (0.0018)	
training:	Epoch: [155][123/233]	Loss 0.0003 (0.0018)	
training:	Epoch: [155][124/233]	Loss 0.0003 (0.0018)	
training:	Epoch: [155][125/233]	Loss 0.0004 (0.0018)	
training:	Epoch: [155][126/233]	Loss 0.0003 (0.0018)	
training:	Epoch: [155][127/233]	Loss 0.0004 (0.0018)	
training:	Epoch: [155][128/233]	Loss 0.0003 (0.0018)	
training:	Epoch: [155][129/233]	Loss 0.0004 (0.0017)	
training:	Epoch: [155][130/233]	Loss 0.0003 (0.0017)	
training:	Epoch: [155][131/233]	Loss 0.0003 (0.0017)	
training:	Epoch: [155][132/233]	Loss 0.0004 (0.0017)	
training:	Epoch: [155][133/233]	Loss 0.0003 (0.0017)	
training:	Epoch: [155][134/233]	Loss 0.0003 (0.0017)	
training:	Epoch: [155][135/233]	Loss 0.0493 (0.0020)	
training:	Epoch: [155][136/233]	Loss 0.0003 (0.0020)	
training:	Epoch: [155][137/233]	Loss 0.0005 (0.0020)	
training:	Epoch: [155][138/233]	Loss 0.0002 (0.0020)	
training:	Epoch: [155][139/233]	Loss 0.0007 (0.0020)	
training:	Epoch: [155][140/233]	Loss 0.0006 (0.0020)	
training:	Epoch: [155][141/233]	Loss 0.0004 (0.0020)	
training:	Epoch: [155][142/233]	Loss 0.0158 (0.0021)	
training:	Epoch: [155][143/233]	Loss 0.0082 (0.0021)	
training:	Epoch: [155][144/233]	Loss 0.0002 (0.0021)	
training:	Epoch: [155][145/233]	Loss 0.0003 (0.0021)	
training:	Epoch: [155][146/233]	Loss 0.0007 (0.0021)	
training:	Epoch: [155][147/233]	Loss 0.0286 (0.0023)	
training:	Epoch: [155][148/233]	Loss 0.0005 (0.0023)	
training:	Epoch: [155][149/233]	Loss 0.0006 (0.0022)	
training:	Epoch: [155][150/233]	Loss 0.0002 (0.0022)	
training:	Epoch: [155][151/233]	Loss 0.0003 (0.0022)	
training:	Epoch: [155][152/233]	Loss 0.0035 (0.0022)	
training:	Epoch: [155][153/233]	Loss 0.0005 (0.0022)	
training:	Epoch: [155][154/233]	Loss 0.0003 (0.0022)	
training:	Epoch: [155][155/233]	Loss 0.0005 (0.0022)	
training:	Epoch: [155][156/233]	Loss 0.0580 (0.0025)	
training:	Epoch: [155][157/233]	Loss 0.0003 (0.0025)	
training:	Epoch: [155][158/233]	Loss 0.0004 (0.0025)	
training:	Epoch: [155][159/233]	Loss 0.0002 (0.0025)	
training:	Epoch: [155][160/233]	Loss 0.0002 (0.0025)	
training:	Epoch: [155][161/233]	Loss 0.0005 (0.0025)	
training:	Epoch: [155][162/233]	Loss 0.0003 (0.0025)	
training:	Epoch: [155][163/233]	Loss 0.0004 (0.0025)	
training:	Epoch: [155][164/233]	Loss 0.0003 (0.0024)	
training:	Epoch: [155][165/233]	Loss 0.0004 (0.0024)	
training:	Epoch: [155][166/233]	Loss 0.0003 (0.0024)	
training:	Epoch: [155][167/233]	Loss 0.0003 (0.0024)	
training:	Epoch: [155][168/233]	Loss 0.0002 (0.0024)	
training:	Epoch: [155][169/233]	Loss 0.0028 (0.0024)	
training:	Epoch: [155][170/233]	Loss 0.0061 (0.0024)	
training:	Epoch: [155][171/233]	Loss 0.0003 (0.0024)	
training:	Epoch: [155][172/233]	Loss 0.0003 (0.0024)	
training:	Epoch: [155][173/233]	Loss 0.0018 (0.0024)	
training:	Epoch: [155][174/233]	Loss 0.0006 (0.0024)	
training:	Epoch: [155][175/233]	Loss 0.0004 (0.0024)	
training:	Epoch: [155][176/233]	Loss 0.0003 (0.0024)	
training:	Epoch: [155][177/233]	Loss 0.0003 (0.0023)	
training:	Epoch: [155][178/233]	Loss 0.0003 (0.0023)	
training:	Epoch: [155][179/233]	Loss 0.0003 (0.0023)	
training:	Epoch: [155][180/233]	Loss 0.0004 (0.0023)	
training:	Epoch: [155][181/233]	Loss 0.0003 (0.0023)	
training:	Epoch: [155][182/233]	Loss 0.0039 (0.0023)	
training:	Epoch: [155][183/233]	Loss 0.0005 (0.0023)	
training:	Epoch: [155][184/233]	Loss 0.0063 (0.0023)	
training:	Epoch: [155][185/233]	Loss 0.0003 (0.0023)	
training:	Epoch: [155][186/233]	Loss 0.0006 (0.0023)	
training:	Epoch: [155][187/233]	Loss 0.0002 (0.0023)	
training:	Epoch: [155][188/233]	Loss 0.0003 (0.0023)	
training:	Epoch: [155][189/233]	Loss 0.0003 (0.0023)	
training:	Epoch: [155][190/233]	Loss 0.0004 (0.0023)	
training:	Epoch: [155][191/233]	Loss 0.0006 (0.0022)	
training:	Epoch: [155][192/233]	Loss 0.0004 (0.0022)	
training:	Epoch: [155][193/233]	Loss 0.0003 (0.0022)	
training:	Epoch: [155][194/233]	Loss 0.0004 (0.0022)	
training:	Epoch: [155][195/233]	Loss 0.0003 (0.0022)	
training:	Epoch: [155][196/233]	Loss 0.0003 (0.0022)	
training:	Epoch: [155][197/233]	Loss 0.0003 (0.0022)	
training:	Epoch: [155][198/233]	Loss 0.0005 (0.0022)	
training:	Epoch: [155][199/233]	Loss 0.0003 (0.0022)	
training:	Epoch: [155][200/233]	Loss 0.0003 (0.0022)	
training:	Epoch: [155][201/233]	Loss 0.0003 (0.0022)	
training:	Epoch: [155][202/233]	Loss 0.0005 (0.0021)	
training:	Epoch: [155][203/233]	Loss 0.0002 (0.0021)	
training:	Epoch: [155][204/233]	Loss 0.0004 (0.0021)	
training:	Epoch: [155][205/233]	Loss 0.0005 (0.0021)	
training:	Epoch: [155][206/233]	Loss 0.0002 (0.0021)	
training:	Epoch: [155][207/233]	Loss 0.0004 (0.0021)	
training:	Epoch: [155][208/233]	Loss 0.0002 (0.0021)	
training:	Epoch: [155][209/233]	Loss 0.0002 (0.0021)	
training:	Epoch: [155][210/233]	Loss 0.0448 (0.0023)	
training:	Epoch: [155][211/233]	Loss 0.0003 (0.0023)	
training:	Epoch: [155][212/233]	Loss 0.0003 (0.0023)	
training:	Epoch: [155][213/233]	Loss 0.0004 (0.0023)	
training:	Epoch: [155][214/233]	Loss 0.0002 (0.0022)	
training:	Epoch: [155][215/233]	Loss 0.0011 (0.0022)	
training:	Epoch: [155][216/233]	Loss 0.0002 (0.0022)	
training:	Epoch: [155][217/233]	Loss 0.0003 (0.0022)	
training:	Epoch: [155][218/233]	Loss 0.0005 (0.0022)	
training:	Epoch: [155][219/233]	Loss 0.0061 (0.0022)	
training:	Epoch: [155][220/233]	Loss 0.0003 (0.0022)	
training:	Epoch: [155][221/233]	Loss 0.0003 (0.0022)	
training:	Epoch: [155][222/233]	Loss 0.0003 (0.0022)	
training:	Epoch: [155][223/233]	Loss 0.0003 (0.0022)	
training:	Epoch: [155][224/233]	Loss 0.0003 (0.0022)	
training:	Epoch: [155][225/233]	Loss 0.0258 (0.0023)	
training:	Epoch: [155][226/233]	Loss 0.0003 (0.0023)	
training:	Epoch: [155][227/233]	Loss 0.0004 (0.0023)	
training:	Epoch: [155][228/233]	Loss 0.0003 (0.0023)	
training:	Epoch: [155][229/233]	Loss 0.0013 (0.0023)	
training:	Epoch: [155][230/233]	Loss 0.0006 (0.0023)	
training:	Epoch: [155][231/233]	Loss 0.0008 (0.0023)	
training:	Epoch: [155][232/233]	Loss 0.0003 (0.0022)	
training:	Epoch: [155][233/233]	Loss 0.0005 (0.0022)	
Training:	 Loss: 0.0022

Training:	 ACC: 1.0000 1.0000 1.0000 1.0000
Validation:	 ACC: 0.7953 0.7967 0.8243 0.7663
Validation:	 Best_BACC: 0.8162 0.8154 0.7998 0.8326
Validation:	 Loss: 1.2546
Pretraining:	Epoch 156/200
----------
training:	Epoch: [156][1/233]	Loss 0.0006 (0.0006)	
training:	Epoch: [156][2/233]	Loss 0.0002 (0.0004)	
training:	Epoch: [156][3/233]	Loss 0.0019 (0.0009)	
training:	Epoch: [156][4/233]	Loss 0.0235 (0.0066)	
training:	Epoch: [156][5/233]	Loss 0.0006 (0.0054)	
training:	Epoch: [156][6/233]	Loss 0.0003 (0.0045)	
training:	Epoch: [156][7/233]	Loss 0.0003 (0.0039)	
training:	Epoch: [156][8/233]	Loss 0.0003 (0.0035)	
training:	Epoch: [156][9/233]	Loss 0.0005 (0.0031)	
training:	Epoch: [156][10/233]	Loss 0.0003 (0.0029)	
training:	Epoch: [156][11/233]	Loss 0.0003 (0.0026)	
training:	Epoch: [156][12/233]	Loss 0.0211 (0.0042)	
training:	Epoch: [156][13/233]	Loss 0.0002 (0.0039)	
training:	Epoch: [156][14/233]	Loss 0.0007 (0.0036)	
training:	Epoch: [156][15/233]	Loss 0.0003 (0.0034)	
training:	Epoch: [156][16/233]	Loss 0.0003 (0.0032)	
training:	Epoch: [156][17/233]	Loss 0.0002 (0.0030)	
training:	Epoch: [156][18/233]	Loss 0.0003 (0.0029)	
training:	Epoch: [156][19/233]	Loss 0.0016 (0.0028)	
training:	Epoch: [156][20/233]	Loss 0.0002 (0.0027)	
training:	Epoch: [156][21/233]	Loss 0.0154 (0.0033)	
training:	Epoch: [156][22/233]	Loss 0.0004 (0.0032)	
training:	Epoch: [156][23/233]	Loss 0.0003 (0.0030)	
training:	Epoch: [156][24/233]	Loss 0.0004 (0.0029)	
training:	Epoch: [156][25/233]	Loss 0.0004 (0.0028)	
training:	Epoch: [156][26/233]	Loss 0.0018 (0.0028)	
training:	Epoch: [156][27/233]	Loss 0.0003 (0.0027)	
training:	Epoch: [156][28/233]	Loss 0.0002 (0.0026)	
training:	Epoch: [156][29/233]	Loss 0.0002 (0.0025)	
training:	Epoch: [156][30/233]	Loss 0.0009 (0.0025)	
training:	Epoch: [156][31/233]	Loss 0.0003 (0.0024)	
training:	Epoch: [156][32/233]	Loss 0.0003 (0.0023)	
training:	Epoch: [156][33/233]	Loss 0.0002 (0.0023)	
training:	Epoch: [156][34/233]	Loss 0.0003 (0.0022)	
training:	Epoch: [156][35/233]	Loss 0.0003 (0.0022)	
training:	Epoch: [156][36/233]	Loss 0.0011 (0.0021)	
training:	Epoch: [156][37/233]	Loss 0.0008 (0.0021)	
training:	Epoch: [156][38/233]	Loss 0.0004 (0.0021)	
training:	Epoch: [156][39/233]	Loss 0.0010 (0.0020)	
training:	Epoch: [156][40/233]	Loss 0.0010 (0.0020)	
training:	Epoch: [156][41/233]	Loss 0.0004 (0.0020)	
training:	Epoch: [156][42/233]	Loss 0.0004 (0.0019)	
training:	Epoch: [156][43/233]	Loss 0.0002 (0.0019)	
training:	Epoch: [156][44/233]	Loss 0.0010 (0.0019)	
training:	Epoch: [156][45/233]	Loss 0.0004 (0.0018)	
training:	Epoch: [156][46/233]	Loss 0.0003 (0.0018)	
training:	Epoch: [156][47/233]	Loss 0.0010 (0.0018)	
training:	Epoch: [156][48/233]	Loss 0.0002 (0.0017)	
training:	Epoch: [156][49/233]	Loss 0.0003 (0.0017)	
training:	Epoch: [156][50/233]	Loss 0.0004 (0.0017)	
training:	Epoch: [156][51/233]	Loss 0.0011 (0.0017)	
training:	Epoch: [156][52/233]	Loss 0.0005 (0.0017)	
training:	Epoch: [156][53/233]	Loss 0.0015 (0.0017)	
training:	Epoch: [156][54/233]	Loss 0.0002 (0.0016)	
training:	Epoch: [156][55/233]	Loss 0.0003 (0.0016)	
training:	Epoch: [156][56/233]	Loss 0.0003 (0.0016)	
training:	Epoch: [156][57/233]	Loss 0.0002 (0.0016)	
training:	Epoch: [156][58/233]	Loss 0.0032 (0.0016)	
training:	Epoch: [156][59/233]	Loss 0.0004 (0.0016)	
training:	Epoch: [156][60/233]	Loss 0.0003 (0.0015)	
training:	Epoch: [156][61/233]	Loss 0.0003 (0.0015)	
training:	Epoch: [156][62/233]	Loss 0.0003 (0.0015)	
training:	Epoch: [156][63/233]	Loss 0.0008 (0.0015)	
training:	Epoch: [156][64/233]	Loss 0.0003 (0.0015)	
training:	Epoch: [156][65/233]	Loss 0.0002 (0.0015)	
training:	Epoch: [156][66/233]	Loss 0.0002 (0.0014)	
training:	Epoch: [156][67/233]	Loss 0.0003 (0.0014)	
training:	Epoch: [156][68/233]	Loss 0.0004 (0.0014)	
training:	Epoch: [156][69/233]	Loss 0.0002 (0.0014)	
training:	Epoch: [156][70/233]	Loss 0.0003 (0.0014)	
training:	Epoch: [156][71/233]	Loss 0.0006 (0.0014)	
training:	Epoch: [156][72/233]	Loss 0.0002 (0.0013)	
training:	Epoch: [156][73/233]	Loss 0.0002 (0.0013)	
training:	Epoch: [156][74/233]	Loss 0.0004 (0.0013)	
training:	Epoch: [156][75/233]	Loss 0.0230 (0.0016)	
training:	Epoch: [156][76/233]	Loss 0.0002 (0.0016)	
training:	Epoch: [156][77/233]	Loss 0.0004 (0.0016)	
training:	Epoch: [156][78/233]	Loss 0.0003 (0.0016)	
training:	Epoch: [156][79/233]	Loss 0.0002 (0.0015)	
training:	Epoch: [156][80/233]	Loss 0.0003 (0.0015)	
training:	Epoch: [156][81/233]	Loss 0.0006 (0.0015)	
training:	Epoch: [156][82/233]	Loss 0.0004 (0.0015)	
training:	Epoch: [156][83/233]	Loss 0.0004 (0.0015)	
training:	Epoch: [156][84/233]	Loss 0.0003 (0.0015)	
training:	Epoch: [156][85/233]	Loss 0.0002 (0.0015)	
training:	Epoch: [156][86/233]	Loss 0.0028 (0.0015)	
training:	Epoch: [156][87/233]	Loss 0.0004 (0.0015)	
training:	Epoch: [156][88/233]	Loss 0.0007 (0.0014)	
training:	Epoch: [156][89/233]	Loss 0.0003 (0.0014)	
training:	Epoch: [156][90/233]	Loss 0.0017 (0.0014)	
training:	Epoch: [156][91/233]	Loss 0.0004 (0.0014)	
training:	Epoch: [156][92/233]	Loss 0.0007 (0.0014)	
training:	Epoch: [156][93/233]	Loss 0.0003 (0.0014)	
training:	Epoch: [156][94/233]	Loss 0.0002 (0.0014)	
training:	Epoch: [156][95/233]	Loss 0.0003 (0.0014)	
training:	Epoch: [156][96/233]	Loss 0.0006 (0.0014)	
training:	Epoch: [156][97/233]	Loss 0.0004 (0.0014)	
training:	Epoch: [156][98/233]	Loss 0.0003 (0.0014)	
training:	Epoch: [156][99/233]	Loss 0.0002 (0.0013)	
training:	Epoch: [156][100/233]	Loss 0.0002 (0.0013)	
training:	Epoch: [156][101/233]	Loss 0.0003 (0.0013)	
training:	Epoch: [156][102/233]	Loss 0.1349 (0.0026)	
training:	Epoch: [156][103/233]	Loss 0.0002 (0.0026)	
training:	Epoch: [156][104/233]	Loss 0.0005 (0.0026)	
training:	Epoch: [156][105/233]	Loss 0.0003 (0.0026)	
training:	Epoch: [156][106/233]	Loss 0.0010 (0.0025)	
training:	Epoch: [156][107/233]	Loss 0.0004 (0.0025)	
training:	Epoch: [156][108/233]	Loss 0.0002 (0.0025)	
training:	Epoch: [156][109/233]	Loss 0.0002 (0.0025)	
training:	Epoch: [156][110/233]	Loss 0.0005 (0.0025)	
training:	Epoch: [156][111/233]	Loss 0.0003 (0.0024)	
training:	Epoch: [156][112/233]	Loss 0.0078 (0.0025)	
training:	Epoch: [156][113/233]	Loss 0.0002 (0.0025)	
training:	Epoch: [156][114/233]	Loss 0.0002 (0.0025)	
training:	Epoch: [156][115/233]	Loss 0.0002 (0.0024)	
training:	Epoch: [156][116/233]	Loss 0.0004 (0.0024)	
training:	Epoch: [156][117/233]	Loss 0.0002 (0.0024)	
training:	Epoch: [156][118/233]	Loss 0.0004 (0.0024)	
training:	Epoch: [156][119/233]	Loss 0.0006 (0.0024)	
training:	Epoch: [156][120/233]	Loss 0.0003 (0.0024)	
training:	Epoch: [156][121/233]	Loss 0.0003 (0.0023)	
training:	Epoch: [156][122/233]	Loss 0.0003 (0.0023)	
training:	Epoch: [156][123/233]	Loss 0.0006 (0.0023)	
training:	Epoch: [156][124/233]	Loss 0.0003 (0.0023)	
training:	Epoch: [156][125/233]	Loss 0.0002 (0.0023)	
training:	Epoch: [156][126/233]	Loss 0.0008 (0.0023)	
training:	Epoch: [156][127/233]	Loss 0.0004 (0.0022)	
training:	Epoch: [156][128/233]	Loss 0.0003 (0.0022)	
training:	Epoch: [156][129/233]	Loss 0.0003 (0.0022)	
training:	Epoch: [156][130/233]	Loss 0.0011 (0.0022)	
training:	Epoch: [156][131/233]	Loss 0.0003 (0.0022)	
training:	Epoch: [156][132/233]	Loss 0.0031 (0.0022)	
training:	Epoch: [156][133/233]	Loss 0.0002 (0.0022)	
training:	Epoch: [156][134/233]	Loss 0.0002 (0.0022)	
training:	Epoch: [156][135/233]	Loss 0.0006 (0.0022)	
training:	Epoch: [156][136/233]	Loss 0.0002 (0.0021)	
training:	Epoch: [156][137/233]	Loss 0.0003 (0.0021)	
training:	Epoch: [156][138/233]	Loss 0.0002 (0.0021)	
training:	Epoch: [156][139/233]	Loss 0.0002 (0.0021)	
training:	Epoch: [156][140/233]	Loss 0.0004 (0.0021)	
training:	Epoch: [156][141/233]	Loss 0.0002 (0.0021)	
training:	Epoch: [156][142/233]	Loss 0.0003 (0.0021)	
training:	Epoch: [156][143/233]	Loss 0.0003 (0.0021)	
training:	Epoch: [156][144/233]	Loss 0.0003 (0.0020)	
training:	Epoch: [156][145/233]	Loss 0.0051 (0.0021)	
training:	Epoch: [156][146/233]	Loss 0.0004 (0.0021)	
training:	Epoch: [156][147/233]	Loss 0.0004 (0.0020)	
training:	Epoch: [156][148/233]	Loss 0.0002 (0.0020)	
training:	Epoch: [156][149/233]	Loss 0.0005 (0.0020)	
training:	Epoch: [156][150/233]	Loss 0.0002 (0.0020)	
training:	Epoch: [156][151/233]	Loss 0.0002 (0.0020)	
training:	Epoch: [156][152/233]	Loss 0.0002 (0.0020)	
training:	Epoch: [156][153/233]	Loss 0.0003 (0.0020)	
training:	Epoch: [156][154/233]	Loss 0.0004 (0.0020)	
training:	Epoch: [156][155/233]	Loss 0.0017 (0.0020)	
training:	Epoch: [156][156/233]	Loss 0.0003 (0.0019)	
training:	Epoch: [156][157/233]	Loss 0.0003 (0.0019)	
training:	Epoch: [156][158/233]	Loss 0.0003 (0.0019)	
training:	Epoch: [156][159/233]	Loss 0.0003 (0.0019)	
training:	Epoch: [156][160/233]	Loss 0.0002 (0.0019)	
training:	Epoch: [156][161/233]	Loss 0.0003 (0.0019)	
training:	Epoch: [156][162/233]	Loss 0.0003 (0.0019)	
training:	Epoch: [156][163/233]	Loss 0.0002 (0.0019)	
training:	Epoch: [156][164/233]	Loss 0.0002 (0.0019)	
training:	Epoch: [156][165/233]	Loss 0.0003 (0.0019)	
training:	Epoch: [156][166/233]	Loss 0.0003 (0.0018)	
training:	Epoch: [156][167/233]	Loss 0.0009 (0.0018)	
training:	Epoch: [156][168/233]	Loss 0.0004 (0.0018)	
training:	Epoch: [156][169/233]	Loss 0.0002 (0.0018)	
training:	Epoch: [156][170/233]	Loss 0.0003 (0.0018)	
training:	Epoch: [156][171/233]	Loss 0.0002 (0.0018)	
training:	Epoch: [156][172/233]	Loss 0.0037 (0.0018)	
training:	Epoch: [156][173/233]	Loss 0.0004 (0.0018)	
training:	Epoch: [156][174/233]	Loss 0.0006 (0.0018)	
training:	Epoch: [156][175/233]	Loss 0.0003 (0.0018)	
training:	Epoch: [156][176/233]	Loss 0.0002 (0.0018)	
training:	Epoch: [156][177/233]	Loss 0.0002 (0.0018)	
training:	Epoch: [156][178/233]	Loss 0.0003 (0.0018)	
training:	Epoch: [156][179/233]	Loss 0.0056 (0.0018)	
training:	Epoch: [156][180/233]	Loss 0.0007 (0.0018)	
training:	Epoch: [156][181/233]	Loss 0.0004 (0.0018)	
training:	Epoch: [156][182/233]	Loss 0.0005 (0.0018)	
training:	Epoch: [156][183/233]	Loss 0.0003 (0.0018)	
training:	Epoch: [156][184/233]	Loss 0.0002 (0.0018)	
training:	Epoch: [156][185/233]	Loss 0.0003 (0.0017)	
training:	Epoch: [156][186/233]	Loss 0.0003 (0.0017)	
training:	Epoch: [156][187/233]	Loss 0.0003 (0.0017)	
training:	Epoch: [156][188/233]	Loss 0.0003 (0.0017)	
training:	Epoch: [156][189/233]	Loss 0.0003 (0.0017)	
training:	Epoch: [156][190/233]	Loss 0.0005 (0.0017)	
training:	Epoch: [156][191/233]	Loss 0.0003 (0.0017)	
training:	Epoch: [156][192/233]	Loss 0.0065 (0.0017)	
training:	Epoch: [156][193/233]	Loss 0.0003 (0.0017)	
training:	Epoch: [156][194/233]	Loss 0.0078 (0.0017)	
training:	Epoch: [156][195/233]	Loss 0.0003 (0.0017)	
training:	Epoch: [156][196/233]	Loss 0.0002 (0.0017)	
training:	Epoch: [156][197/233]	Loss 0.0011 (0.0017)	
training:	Epoch: [156][198/233]	Loss 0.0048 (0.0017)	
training:	Epoch: [156][199/233]	Loss 0.0002 (0.0017)	
training:	Epoch: [156][200/233]	Loss 0.0003 (0.0017)	
training:	Epoch: [156][201/233]	Loss 0.0002 (0.0017)	
training:	Epoch: [156][202/233]	Loss 0.0003 (0.0017)	
training:	Epoch: [156][203/233]	Loss 0.0012 (0.0017)	
training:	Epoch: [156][204/233]	Loss 0.0033 (0.0017)	
training:	Epoch: [156][205/233]	Loss 0.0002 (0.0017)	
training:	Epoch: [156][206/233]	Loss 0.0004 (0.0017)	
training:	Epoch: [156][207/233]	Loss 0.0002 (0.0017)	
training:	Epoch: [156][208/233]	Loss 0.0003 (0.0017)	
training:	Epoch: [156][209/233]	Loss 0.0003 (0.0017)	
training:	Epoch: [156][210/233]	Loss 0.0002 (0.0017)	
training:	Epoch: [156][211/233]	Loss 0.0003 (0.0017)	
training:	Epoch: [156][212/233]	Loss 0.0002 (0.0017)	
training:	Epoch: [156][213/233]	Loss 0.0003 (0.0017)	
training:	Epoch: [156][214/233]	Loss 0.0003 (0.0017)	
training:	Epoch: [156][215/233]	Loss 0.0003 (0.0016)	
training:	Epoch: [156][216/233]	Loss 0.0002 (0.0016)	
training:	Epoch: [156][217/233]	Loss 0.0032 (0.0016)	
training:	Epoch: [156][218/233]	Loss 0.0011 (0.0016)	
training:	Epoch: [156][219/233]	Loss 0.0003 (0.0016)	
training:	Epoch: [156][220/233]	Loss 0.0002 (0.0016)	
training:	Epoch: [156][221/233]	Loss 0.0004 (0.0016)	
training:	Epoch: [156][222/233]	Loss 0.0002 (0.0016)	
training:	Epoch: [156][223/233]	Loss 0.0002 (0.0016)	
training:	Epoch: [156][224/233]	Loss 0.0005 (0.0016)	
training:	Epoch: [156][225/233]	Loss 0.0011 (0.0016)	
training:	Epoch: [156][226/233]	Loss 0.0016 (0.0016)	
training:	Epoch: [156][227/233]	Loss 0.0011 (0.0016)	
training:	Epoch: [156][228/233]	Loss 0.0002 (0.0016)	
training:	Epoch: [156][229/233]	Loss 0.0002 (0.0016)	
training:	Epoch: [156][230/233]	Loss 0.0003 (0.0016)	
training:	Epoch: [156][231/233]	Loss 0.0002 (0.0016)	
training:	Epoch: [156][232/233]	Loss 0.0005 (0.0016)	
training:	Epoch: [156][233/233]	Loss 0.0002 (0.0016)	
Training:	 Loss: 0.0016

Training:	 ACC: 1.0000 1.0000 1.0000 1.0000
Validation:	 ACC: 0.7944 0.7945 0.7967 0.7921
Validation:	 Best_BACC: 0.8162 0.8154 0.7998 0.8326
Validation:	 Loss: 1.2853
Pretraining:	Epoch 157/200
----------
training:	Epoch: [157][1/233]	Loss 0.0002 (0.0002)	
training:	Epoch: [157][2/233]	Loss 0.0003 (0.0003)	
training:	Epoch: [157][3/233]	Loss 0.0002 (0.0002)	
training:	Epoch: [157][4/233]	Loss 0.0004 (0.0003)	
training:	Epoch: [157][5/233]	Loss 0.0002 (0.0003)	
training:	Epoch: [157][6/233]	Loss 0.0002 (0.0003)	
training:	Epoch: [157][7/233]	Loss 0.0004 (0.0003)	
training:	Epoch: [157][8/233]	Loss 0.0002 (0.0003)	
training:	Epoch: [157][9/233]	Loss 0.0004 (0.0003)	
training:	Epoch: [157][10/233]	Loss 0.0002 (0.0003)	
training:	Epoch: [157][11/233]	Loss 0.0003 (0.0003)	
training:	Epoch: [157][12/233]	Loss 0.0002 (0.0003)	
training:	Epoch: [157][13/233]	Loss 0.0002 (0.0003)	
training:	Epoch: [157][14/233]	Loss 0.0066 (0.0007)	
training:	Epoch: [157][15/233]	Loss 0.0016 (0.0008)	
training:	Epoch: [157][16/233]	Loss 0.0002 (0.0007)	
training:	Epoch: [157][17/233]	Loss 0.0002 (0.0007)	
training:	Epoch: [157][18/233]	Loss 0.0004 (0.0007)	
training:	Epoch: [157][19/233]	Loss 0.0003 (0.0007)	
training:	Epoch: [157][20/233]	Loss 0.0004 (0.0007)	
training:	Epoch: [157][21/233]	Loss 0.0003 (0.0006)	
training:	Epoch: [157][22/233]	Loss 0.0003 (0.0006)	
training:	Epoch: [157][23/233]	Loss 0.0002 (0.0006)	
training:	Epoch: [157][24/233]	Loss 0.0175 (0.0013)	
training:	Epoch: [157][25/233]	Loss 0.0005 (0.0013)	
training:	Epoch: [157][26/233]	Loss 0.0002 (0.0012)	
training:	Epoch: [157][27/233]	Loss 0.0003 (0.0012)	
training:	Epoch: [157][28/233]	Loss 0.0002 (0.0012)	
training:	Epoch: [157][29/233]	Loss 0.0002 (0.0011)	
training:	Epoch: [157][30/233]	Loss 0.0003 (0.0011)	
training:	Epoch: [157][31/233]	Loss 0.0005 (0.0011)	
training:	Epoch: [157][32/233]	Loss 0.0035 (0.0012)	
training:	Epoch: [157][33/233]	Loss 0.0019 (0.0012)	
training:	Epoch: [157][34/233]	Loss 0.0003 (0.0012)	
training:	Epoch: [157][35/233]	Loss 0.0003 (0.0011)	
training:	Epoch: [157][36/233]	Loss 0.0129 (0.0015)	
training:	Epoch: [157][37/233]	Loss 0.0003 (0.0014)	
training:	Epoch: [157][38/233]	Loss 0.0002 (0.0014)	
training:	Epoch: [157][39/233]	Loss 0.0027 (0.0014)	
training:	Epoch: [157][40/233]	Loss 0.1005 (0.0039)	
training:	Epoch: [157][41/233]	Loss 0.0002 (0.0038)	
training:	Epoch: [157][42/233]	Loss 0.0005 (0.0037)	
training:	Epoch: [157][43/233]	Loss 0.0011 (0.0037)	
training:	Epoch: [157][44/233]	Loss 0.0002 (0.0036)	
training:	Epoch: [157][45/233]	Loss 0.0002 (0.0035)	
training:	Epoch: [157][46/233]	Loss 0.0002 (0.0035)	
training:	Epoch: [157][47/233]	Loss 0.0004 (0.0034)	
training:	Epoch: [157][48/233]	Loss 0.0002 (0.0033)	
training:	Epoch: [157][49/233]	Loss 0.0002 (0.0033)	
training:	Epoch: [157][50/233]	Loss 0.0300 (0.0038)	
training:	Epoch: [157][51/233]	Loss 0.0002 (0.0037)	
training:	Epoch: [157][52/233]	Loss 0.0003 (0.0037)	
training:	Epoch: [157][53/233]	Loss 0.0003 (0.0036)	
training:	Epoch: [157][54/233]	Loss 0.0002 (0.0035)	
training:	Epoch: [157][55/233]	Loss 0.0002 (0.0035)	
training:	Epoch: [157][56/233]	Loss 0.0002 (0.0034)	
training:	Epoch: [157][57/233]	Loss 0.0003 (0.0034)	
training:	Epoch: [157][58/233]	Loss 0.0002 (0.0033)	
training:	Epoch: [157][59/233]	Loss 0.0004 (0.0033)	
training:	Epoch: [157][60/233]	Loss 0.0002 (0.0032)	
training:	Epoch: [157][61/233]	Loss 0.0700 (0.0043)	
training:	Epoch: [157][62/233]	Loss 0.0002 (0.0042)	
training:	Epoch: [157][63/233]	Loss 0.0006 (0.0042)	
training:	Epoch: [157][64/233]	Loss 0.0002 (0.0041)	
training:	Epoch: [157][65/233]	Loss 0.0002 (0.0041)	
training:	Epoch: [157][66/233]	Loss 0.0100 (0.0041)	
training:	Epoch: [157][67/233]	Loss 0.0031 (0.0041)	
training:	Epoch: [157][68/233]	Loss 0.0002 (0.0041)	
training:	Epoch: [157][69/233]	Loss 0.0011 (0.0040)	
training:	Epoch: [157][70/233]	Loss 0.0004 (0.0040)	
training:	Epoch: [157][71/233]	Loss 0.0032 (0.0040)	
training:	Epoch: [157][72/233]	Loss 0.0003 (0.0039)	
training:	Epoch: [157][73/233]	Loss 0.0003 (0.0039)	
training:	Epoch: [157][74/233]	Loss 0.0003 (0.0038)	
training:	Epoch: [157][75/233]	Loss 0.0003 (0.0038)	
training:	Epoch: [157][76/233]	Loss 0.0075 (0.0038)	
training:	Epoch: [157][77/233]	Loss 0.0004 (0.0038)	
training:	Epoch: [157][78/233]	Loss 0.0003 (0.0037)	
training:	Epoch: [157][79/233]	Loss 0.0002 (0.0037)	
training:	Epoch: [157][80/233]	Loss 0.0002 (0.0036)	
training:	Epoch: [157][81/233]	Loss 0.0043 (0.0036)	
training:	Epoch: [157][82/233]	Loss 0.0002 (0.0036)	
training:	Epoch: [157][83/233]	Loss 0.0003 (0.0036)	
training:	Epoch: [157][84/233]	Loss 0.0002 (0.0035)	
training:	Epoch: [157][85/233]	Loss 0.0002 (0.0035)	
training:	Epoch: [157][86/233]	Loss 0.0002 (0.0035)	
training:	Epoch: [157][87/233]	Loss 0.0002 (0.0034)	
training:	Epoch: [157][88/233]	Loss 0.0002 (0.0034)	
training:	Epoch: [157][89/233]	Loss 0.0002 (0.0033)	
training:	Epoch: [157][90/233]	Loss 0.0405 (0.0038)	
training:	Epoch: [157][91/233]	Loss 0.0059 (0.0038)	
training:	Epoch: [157][92/233]	Loss 0.0002 (0.0037)	
training:	Epoch: [157][93/233]	Loss 0.0002 (0.0037)	
training:	Epoch: [157][94/233]	Loss 0.0002 (0.0037)	
training:	Epoch: [157][95/233]	Loss 0.0003 (0.0036)	
training:	Epoch: [157][96/233]	Loss 0.0003 (0.0036)	
training:	Epoch: [157][97/233]	Loss 0.0002 (0.0036)	
training:	Epoch: [157][98/233]	Loss 0.0009 (0.0035)	
training:	Epoch: [157][99/233]	Loss 0.0003 (0.0035)	
training:	Epoch: [157][100/233]	Loss 0.0002 (0.0035)	
training:	Epoch: [157][101/233]	Loss 0.0003 (0.0034)	
training:	Epoch: [157][102/233]	Loss 0.0004 (0.0034)	
training:	Epoch: [157][103/233]	Loss 0.0002 (0.0034)	
training:	Epoch: [157][104/233]	Loss 0.0005 (0.0033)	
training:	Epoch: [157][105/233]	Loss 0.0003 (0.0033)	
training:	Epoch: [157][106/233]	Loss 0.0074 (0.0034)	
training:	Epoch: [157][107/233]	Loss 0.0022 (0.0033)	
training:	Epoch: [157][108/233]	Loss 0.0002 (0.0033)	
training:	Epoch: [157][109/233]	Loss 0.0029 (0.0033)	
training:	Epoch: [157][110/233]	Loss 0.1472 (0.0046)	
training:	Epoch: [157][111/233]	Loss 0.0003 (0.0046)	
training:	Epoch: [157][112/233]	Loss 0.0002 (0.0045)	
training:	Epoch: [157][113/233]	Loss 0.0003 (0.0045)	
training:	Epoch: [157][114/233]	Loss 0.0002 (0.0045)	
training:	Epoch: [157][115/233]	Loss 0.0002 (0.0044)	
training:	Epoch: [157][116/233]	Loss 0.0003 (0.0044)	
training:	Epoch: [157][117/233]	Loss 0.0002 (0.0044)	
training:	Epoch: [157][118/233]	Loss 0.0004 (0.0043)	
training:	Epoch: [157][119/233]	Loss 0.0009 (0.0043)	
training:	Epoch: [157][120/233]	Loss 0.0002 (0.0043)	
training:	Epoch: [157][121/233]	Loss 0.0006 (0.0042)	
training:	Epoch: [157][122/233]	Loss 0.0003 (0.0042)	
training:	Epoch: [157][123/233]	Loss 0.0006 (0.0042)	
training:	Epoch: [157][124/233]	Loss 0.0003 (0.0041)	
training:	Epoch: [157][125/233]	Loss 0.0170 (0.0042)	
training:	Epoch: [157][126/233]	Loss 0.0002 (0.0042)	
training:	Epoch: [157][127/233]	Loss 0.0002 (0.0042)	
training:	Epoch: [157][128/233]	Loss 0.0002 (0.0041)	
training:	Epoch: [157][129/233]	Loss 0.0003 (0.0041)	
training:	Epoch: [157][130/233]	Loss 0.0002 (0.0041)	
training:	Epoch: [157][131/233]	Loss 0.0002 (0.0041)	
training:	Epoch: [157][132/233]	Loss 0.0002 (0.0040)	
training:	Epoch: [157][133/233]	Loss 0.0002 (0.0040)	
training:	Epoch: [157][134/233]	Loss 0.0006 (0.0040)	
training:	Epoch: [157][135/233]	Loss 0.0029 (0.0040)	
training:	Epoch: [157][136/233]	Loss 0.0006 (0.0039)	
training:	Epoch: [157][137/233]	Loss 0.0003 (0.0039)	
training:	Epoch: [157][138/233]	Loss 0.0003 (0.0039)	
training:	Epoch: [157][139/233]	Loss 0.0255 (0.0040)	
training:	Epoch: [157][140/233]	Loss 0.0016 (0.0040)	
training:	Epoch: [157][141/233]	Loss 0.0003 (0.0040)	
training:	Epoch: [157][142/233]	Loss 0.0022 (0.0040)	
training:	Epoch: [157][143/233]	Loss 0.0002 (0.0040)	
training:	Epoch: [157][144/233]	Loss 0.0004 (0.0039)	
training:	Epoch: [157][145/233]	Loss 0.0004 (0.0039)	
training:	Epoch: [157][146/233]	Loss 0.0003 (0.0039)	
training:	Epoch: [157][147/233]	Loss 0.0004 (0.0039)	
training:	Epoch: [157][148/233]	Loss 0.0004 (0.0038)	
training:	Epoch: [157][149/233]	Loss 0.0002 (0.0038)	
training:	Epoch: [157][150/233]	Loss 0.0004 (0.0038)	
training:	Epoch: [157][151/233]	Loss 0.0003 (0.0038)	
training:	Epoch: [157][152/233]	Loss 0.0002 (0.0037)	
training:	Epoch: [157][153/233]	Loss 0.0003 (0.0037)	
training:	Epoch: [157][154/233]	Loss 0.0003 (0.0037)	
training:	Epoch: [157][155/233]	Loss 0.0002 (0.0037)	
training:	Epoch: [157][156/233]	Loss 0.0003 (0.0037)	
training:	Epoch: [157][157/233]	Loss 0.0002 (0.0036)	
training:	Epoch: [157][158/233]	Loss 0.0002 (0.0036)	
training:	Epoch: [157][159/233]	Loss 0.0002 (0.0036)	
training:	Epoch: [157][160/233]	Loss 0.0082 (0.0036)	
training:	Epoch: [157][161/233]	Loss 0.0003 (0.0036)	
training:	Epoch: [157][162/233]	Loss 0.0002 (0.0036)	
training:	Epoch: [157][163/233]	Loss 0.0010 (0.0036)	
training:	Epoch: [157][164/233]	Loss 0.0005 (0.0035)	
training:	Epoch: [157][165/233]	Loss 0.0003 (0.0035)	
training:	Epoch: [157][166/233]	Loss 0.0002 (0.0035)	
training:	Epoch: [157][167/233]	Loss 0.0002 (0.0035)	
training:	Epoch: [157][168/233]	Loss 0.0002 (0.0035)	
training:	Epoch: [157][169/233]	Loss 0.0002 (0.0034)	
training:	Epoch: [157][170/233]	Loss 0.0004 (0.0034)	
training:	Epoch: [157][171/233]	Loss 0.0002 (0.0034)	
training:	Epoch: [157][172/233]	Loss 0.0004 (0.0034)	
training:	Epoch: [157][173/233]	Loss 0.0003 (0.0034)	
training:	Epoch: [157][174/233]	Loss 0.0003 (0.0034)	
training:	Epoch: [157][175/233]	Loss 0.0004 (0.0033)	
training:	Epoch: [157][176/233]	Loss 0.0003 (0.0033)	
training:	Epoch: [157][177/233]	Loss 0.0003 (0.0033)	
training:	Epoch: [157][178/233]	Loss 0.0003 (0.0033)	
training:	Epoch: [157][179/233]	Loss 0.0017 (0.0033)	
training:	Epoch: [157][180/233]	Loss 0.0002 (0.0033)	
training:	Epoch: [157][181/233]	Loss 0.0002 (0.0032)	
training:	Epoch: [157][182/233]	Loss 0.0002 (0.0032)	
training:	Epoch: [157][183/233]	Loss 0.0004 (0.0032)	
training:	Epoch: [157][184/233]	Loss 0.0002 (0.0032)	
training:	Epoch: [157][185/233]	Loss 0.0002 (0.0032)	
training:	Epoch: [157][186/233]	Loss 0.0004 (0.0032)	
training:	Epoch: [157][187/233]	Loss 0.1146 (0.0038)	
training:	Epoch: [157][188/233]	Loss 0.0002 (0.0037)	
training:	Epoch: [157][189/233]	Loss 0.0003 (0.0037)	
training:	Epoch: [157][190/233]	Loss 0.0030 (0.0037)	
training:	Epoch: [157][191/233]	Loss 0.0002 (0.0037)	
training:	Epoch: [157][192/233]	Loss 0.0002 (0.0037)	
training:	Epoch: [157][193/233]	Loss 0.0002 (0.0037)	
training:	Epoch: [157][194/233]	Loss 0.0002 (0.0037)	
training:	Epoch: [157][195/233]	Loss 0.0003 (0.0036)	
training:	Epoch: [157][196/233]	Loss 0.0004 (0.0036)	
training:	Epoch: [157][197/233]	Loss 0.0002 (0.0036)	
training:	Epoch: [157][198/233]	Loss 0.0035 (0.0036)	
training:	Epoch: [157][199/233]	Loss 0.0003 (0.0036)	
training:	Epoch: [157][200/233]	Loss 0.0003 (0.0036)	
training:	Epoch: [157][201/233]	Loss 0.0006 (0.0036)	
training:	Epoch: [157][202/233]	Loss 0.0002 (0.0035)	
training:	Epoch: [157][203/233]	Loss 0.0002 (0.0035)	
training:	Epoch: [157][204/233]	Loss 0.0003 (0.0035)	
training:	Epoch: [157][205/233]	Loss 0.0042 (0.0035)	
training:	Epoch: [157][206/233]	Loss 0.0002 (0.0035)	
training:	Epoch: [157][207/233]	Loss 0.0012 (0.0035)	
training:	Epoch: [157][208/233]	Loss 0.0285 (0.0036)	
training:	Epoch: [157][209/233]	Loss 0.0003 (0.0036)	
training:	Epoch: [157][210/233]	Loss 0.0004 (0.0036)	
training:	Epoch: [157][211/233]	Loss 0.0003 (0.0036)	
training:	Epoch: [157][212/233]	Loss 0.0002 (0.0035)	
training:	Epoch: [157][213/233]	Loss 0.0002 (0.0035)	
training:	Epoch: [157][214/233]	Loss 0.0041 (0.0035)	
training:	Epoch: [157][215/233]	Loss 0.0003 (0.0035)	
training:	Epoch: [157][216/233]	Loss 0.0010 (0.0035)	
training:	Epoch: [157][217/233]	Loss 0.0003 (0.0035)	
training:	Epoch: [157][218/233]	Loss 0.0003 (0.0035)	
training:	Epoch: [157][219/233]	Loss 0.0007 (0.0035)	
training:	Epoch: [157][220/233]	Loss 0.0016 (0.0034)	
training:	Epoch: [157][221/233]	Loss 0.0003 (0.0034)	
training:	Epoch: [157][222/233]	Loss 0.0054 (0.0034)	
training:	Epoch: [157][223/233]	Loss 0.0002 (0.0034)	
training:	Epoch: [157][224/233]	Loss 0.0003 (0.0034)	
training:	Epoch: [157][225/233]	Loss 0.0003 (0.0034)	
training:	Epoch: [157][226/233]	Loss 0.0004 (0.0034)	
training:	Epoch: [157][227/233]	Loss 0.0003 (0.0034)	
training:	Epoch: [157][228/233]	Loss 0.0003 (0.0034)	
training:	Epoch: [157][229/233]	Loss 0.0014 (0.0033)	
training:	Epoch: [157][230/233]	Loss 0.0006 (0.0033)	
training:	Epoch: [157][231/233]	Loss 0.0003 (0.0033)	
training:	Epoch: [157][232/233]	Loss 0.0022 (0.0033)	
training:	Epoch: [157][233/233]	Loss 0.0003 (0.0033)	
Training:	 Loss: 0.0033

Training:	 ACC: 0.9999 0.9999 1.0000 0.9997
Validation:	 ACC: 0.7949 0.7967 0.8315 0.7584
Validation:	 Best_BACC: 0.8162 0.8154 0.7998 0.8326
Validation:	 Loss: 1.2854
Pretraining:	Epoch 158/200
----------
training:	Epoch: [158][1/233]	Loss 0.0004 (0.0004)	
training:	Epoch: [158][2/233]	Loss 0.0003 (0.0003)	
training:	Epoch: [158][3/233]	Loss 0.0013 (0.0007)	
training:	Epoch: [158][4/233]	Loss 0.0002 (0.0006)	
training:	Epoch: [158][5/233]	Loss 0.0021 (0.0009)	
training:	Epoch: [158][6/233]	Loss 0.0002 (0.0008)	
training:	Epoch: [158][7/233]	Loss 0.0077 (0.0018)	
training:	Epoch: [158][8/233]	Loss 0.0002 (0.0016)	
training:	Epoch: [158][9/233]	Loss 0.0002 (0.0014)	
training:	Epoch: [158][10/233]	Loss 0.0004 (0.0013)	
training:	Epoch: [158][11/233]	Loss 0.0008 (0.0013)	
training:	Epoch: [158][12/233]	Loss 0.0004 (0.0012)	
training:	Epoch: [158][13/233]	Loss 0.0006 (0.0011)	
training:	Epoch: [158][14/233]	Loss 0.0002 (0.0011)	
training:	Epoch: [158][15/233]	Loss 0.0004 (0.0010)	
training:	Epoch: [158][16/233]	Loss 0.0013 (0.0010)	
training:	Epoch: [158][17/233]	Loss 0.0365 (0.0031)	
training:	Epoch: [158][18/233]	Loss 0.0081 (0.0034)	
training:	Epoch: [158][19/233]	Loss 0.0004 (0.0033)	
training:	Epoch: [158][20/233]	Loss 0.0002 (0.0031)	
training:	Epoch: [158][21/233]	Loss 0.0004 (0.0030)	
training:	Epoch: [158][22/233]	Loss 0.0003 (0.0028)	
training:	Epoch: [158][23/233]	Loss 0.0002 (0.0027)	
training:	Epoch: [158][24/233]	Loss 0.0003 (0.0026)	
training:	Epoch: [158][25/233]	Loss 0.0019 (0.0026)	
training:	Epoch: [158][26/233]	Loss 0.0002 (0.0025)	
training:	Epoch: [158][27/233]	Loss 0.0003 (0.0024)	
training:	Epoch: [158][28/233]	Loss 0.0002 (0.0024)	
training:	Epoch: [158][29/233]	Loss 0.0003 (0.0023)	
training:	Epoch: [158][30/233]	Loss 0.0005 (0.0022)	
training:	Epoch: [158][31/233]	Loss 0.0002 (0.0022)	
training:	Epoch: [158][32/233]	Loss 0.0002 (0.0021)	
training:	Epoch: [158][33/233]	Loss 0.0002 (0.0020)	
training:	Epoch: [158][34/233]	Loss 0.0002 (0.0020)	
training:	Epoch: [158][35/233]	Loss 0.0003 (0.0019)	
training:	Epoch: [158][36/233]	Loss 0.0003 (0.0019)	
training:	Epoch: [158][37/233]	Loss 0.0002 (0.0018)	
training:	Epoch: [158][38/233]	Loss 0.0002 (0.0018)	
training:	Epoch: [158][39/233]	Loss 0.0003 (0.0018)	
training:	Epoch: [158][40/233]	Loss 0.0408 (0.0027)	
training:	Epoch: [158][41/233]	Loss 0.0003 (0.0027)	
training:	Epoch: [158][42/233]	Loss 0.0003 (0.0026)	
training:	Epoch: [158][43/233]	Loss 0.0006 (0.0026)	
training:	Epoch: [158][44/233]	Loss 0.0002 (0.0025)	
training:	Epoch: [158][45/233]	Loss 0.0003 (0.0025)	
training:	Epoch: [158][46/233]	Loss 0.0045 (0.0025)	
training:	Epoch: [158][47/233]	Loss 0.0005 (0.0025)	
training:	Epoch: [158][48/233]	Loss 0.0006 (0.0024)	
training:	Epoch: [158][49/233]	Loss 0.0004 (0.0024)	
training:	Epoch: [158][50/233]	Loss 0.0007 (0.0024)	
training:	Epoch: [158][51/233]	Loss 0.1991 (0.0062)	
training:	Epoch: [158][52/233]	Loss 0.0003 (0.0061)	
training:	Epoch: [158][53/233]	Loss 0.0004 (0.0060)	
training:	Epoch: [158][54/233]	Loss 0.0003 (0.0059)	
training:	Epoch: [158][55/233]	Loss 0.0004 (0.0058)	
training:	Epoch: [158][56/233]	Loss 0.0032 (0.0057)	
training:	Epoch: [158][57/233]	Loss 0.0019 (0.0057)	
training:	Epoch: [158][58/233]	Loss 0.0002 (0.0056)	
training:	Epoch: [158][59/233]	Loss 0.0005 (0.0055)	
training:	Epoch: [158][60/233]	Loss 0.0002 (0.0054)	
training:	Epoch: [158][61/233]	Loss 0.0002 (0.0053)	
training:	Epoch: [158][62/233]	Loss 0.0002 (0.0052)	
training:	Epoch: [158][63/233]	Loss 0.0002 (0.0052)	
training:	Epoch: [158][64/233]	Loss 0.0002 (0.0051)	
training:	Epoch: [158][65/233]	Loss 0.0003 (0.0050)	
training:	Epoch: [158][66/233]	Loss 0.0039 (0.0050)	
training:	Epoch: [158][67/233]	Loss 0.0003 (0.0049)	
training:	Epoch: [158][68/233]	Loss 0.0002 (0.0049)	
training:	Epoch: [158][69/233]	Loss 0.0003 (0.0048)	
training:	Epoch: [158][70/233]	Loss 0.0003 (0.0047)	
training:	Epoch: [158][71/233]	Loss 0.0003 (0.0047)	
training:	Epoch: [158][72/233]	Loss 0.0036 (0.0046)	
training:	Epoch: [158][73/233]	Loss 0.0002 (0.0046)	
training:	Epoch: [158][74/233]	Loss 0.0003 (0.0045)	
training:	Epoch: [158][75/233]	Loss 0.0033 (0.0045)	
training:	Epoch: [158][76/233]	Loss 0.0006 (0.0045)	
training:	Epoch: [158][77/233]	Loss 0.0007 (0.0044)	
training:	Epoch: [158][78/233]	Loss 0.1299 (0.0060)	
training:	Epoch: [158][79/233]	Loss 0.0002 (0.0059)	
training:	Epoch: [158][80/233]	Loss 0.0005 (0.0059)	
training:	Epoch: [158][81/233]	Loss 0.0004 (0.0058)	
training:	Epoch: [158][82/233]	Loss 0.0004 (0.0057)	
training:	Epoch: [158][83/233]	Loss 0.0003 (0.0057)	
training:	Epoch: [158][84/233]	Loss 0.0005 (0.0056)	
training:	Epoch: [158][85/233]	Loss 0.0002 (0.0056)	
training:	Epoch: [158][86/233]	Loss 0.0003 (0.0055)	
training:	Epoch: [158][87/233]	Loss 0.0003 (0.0054)	
training:	Epoch: [158][88/233]	Loss 0.0003 (0.0054)	
training:	Epoch: [158][89/233]	Loss 0.0173 (0.0055)	
training:	Epoch: [158][90/233]	Loss 0.0002 (0.0054)	
training:	Epoch: [158][91/233]	Loss 0.0006 (0.0054)	
training:	Epoch: [158][92/233]	Loss 0.0003 (0.0053)	
training:	Epoch: [158][93/233]	Loss 0.0004 (0.0053)	
training:	Epoch: [158][94/233]	Loss 0.0004 (0.0052)	
training:	Epoch: [158][95/233]	Loss 0.0003 (0.0052)	
training:	Epoch: [158][96/233]	Loss 0.0003 (0.0051)	
training:	Epoch: [158][97/233]	Loss 0.0005 (0.0051)	
training:	Epoch: [158][98/233]	Loss 0.0002 (0.0050)	
training:	Epoch: [158][99/233]	Loss 0.0010 (0.0050)	
training:	Epoch: [158][100/233]	Loss 0.0003 (0.0049)	
training:	Epoch: [158][101/233]	Loss 0.0003 (0.0049)	
training:	Epoch: [158][102/233]	Loss 0.0016 (0.0049)	
training:	Epoch: [158][103/233]	Loss 0.0022 (0.0048)	
training:	Epoch: [158][104/233]	Loss 0.0002 (0.0048)	
training:	Epoch: [158][105/233]	Loss 0.0017 (0.0048)	
training:	Epoch: [158][106/233]	Loss 0.0059 (0.0048)	
training:	Epoch: [158][107/233]	Loss 0.0003 (0.0047)	
training:	Epoch: [158][108/233]	Loss 0.0002 (0.0047)	
training:	Epoch: [158][109/233]	Loss 0.0005 (0.0047)	
training:	Epoch: [158][110/233]	Loss 0.0013 (0.0046)	
training:	Epoch: [158][111/233]	Loss 0.0011 (0.0046)	
training:	Epoch: [158][112/233]	Loss 0.0002 (0.0046)	
training:	Epoch: [158][113/233]	Loss 0.0007 (0.0045)	
training:	Epoch: [158][114/233]	Loss 0.0002 (0.0045)	
training:	Epoch: [158][115/233]	Loss 0.0416 (0.0048)	
training:	Epoch: [158][116/233]	Loss 0.0004 (0.0048)	
training:	Epoch: [158][117/233]	Loss 0.0013 (0.0047)	
training:	Epoch: [158][118/233]	Loss 0.0004 (0.0047)	
training:	Epoch: [158][119/233]	Loss 0.0003 (0.0047)	
training:	Epoch: [158][120/233]	Loss 0.0021 (0.0046)	
training:	Epoch: [158][121/233]	Loss 0.0086 (0.0047)	
training:	Epoch: [158][122/233]	Loss 0.0004 (0.0046)	
training:	Epoch: [158][123/233]	Loss 0.0003 (0.0046)	
training:	Epoch: [158][124/233]	Loss 0.0003 (0.0046)	
training:	Epoch: [158][125/233]	Loss 0.0016 (0.0045)	
training:	Epoch: [158][126/233]	Loss 0.0002 (0.0045)	
training:	Epoch: [158][127/233]	Loss 0.0003 (0.0045)	
training:	Epoch: [158][128/233]	Loss 0.0003 (0.0044)	
training:	Epoch: [158][129/233]	Loss 0.0003 (0.0044)	
training:	Epoch: [158][130/233]	Loss 0.0011 (0.0044)	
training:	Epoch: [158][131/233]	Loss 0.0002 (0.0044)	
training:	Epoch: [158][132/233]	Loss 0.0003 (0.0043)	
training:	Epoch: [158][133/233]	Loss 0.0006 (0.0043)	
training:	Epoch: [158][134/233]	Loss 0.0002 (0.0043)	
training:	Epoch: [158][135/233]	Loss 0.0006 (0.0042)	
training:	Epoch: [158][136/233]	Loss 0.0003 (0.0042)	
training:	Epoch: [158][137/233]	Loss 0.0003 (0.0042)	
training:	Epoch: [158][138/233]	Loss 0.0002 (0.0042)	
training:	Epoch: [158][139/233]	Loss 0.0003 (0.0041)	
training:	Epoch: [158][140/233]	Loss 0.0021 (0.0041)	
training:	Epoch: [158][141/233]	Loss 0.0809 (0.0047)	
training:	Epoch: [158][142/233]	Loss 0.1037 (0.0054)	
training:	Epoch: [158][143/233]	Loss 0.0005 (0.0053)	
training:	Epoch: [158][144/233]	Loss 0.0003 (0.0053)	
training:	Epoch: [158][145/233]	Loss 0.0003 (0.0053)	
training:	Epoch: [158][146/233]	Loss 0.0009 (0.0052)	
training:	Epoch: [158][147/233]	Loss 0.0152 (0.0053)	
training:	Epoch: [158][148/233]	Loss 0.0004 (0.0053)	
training:	Epoch: [158][149/233]	Loss 0.0003 (0.0052)	
training:	Epoch: [158][150/233]	Loss 0.0009 (0.0052)	
training:	Epoch: [158][151/233]	Loss 0.0081 (0.0052)	
training:	Epoch: [158][152/233]	Loss 0.0002 (0.0052)	
training:	Epoch: [158][153/233]	Loss 0.0008 (0.0052)	
training:	Epoch: [158][154/233]	Loss 0.0034 (0.0051)	
training:	Epoch: [158][155/233]	Loss 0.0014 (0.0051)	
training:	Epoch: [158][156/233]	Loss 0.0002 (0.0051)	
training:	Epoch: [158][157/233]	Loss 0.0004 (0.0051)	
training:	Epoch: [158][158/233]	Loss 0.0003 (0.0050)	
training:	Epoch: [158][159/233]	Loss 0.0003 (0.0050)	
training:	Epoch: [158][160/233]	Loss 0.0024 (0.0050)	
training:	Epoch: [158][161/233]	Loss 0.0003 (0.0049)	
training:	Epoch: [158][162/233]	Loss 0.0002 (0.0049)	
training:	Epoch: [158][163/233]	Loss 0.0005 (0.0049)	
training:	Epoch: [158][164/233]	Loss 0.0002 (0.0049)	
training:	Epoch: [158][165/233]	Loss 0.0004 (0.0048)	
training:	Epoch: [158][166/233]	Loss 0.1031 (0.0054)	
training:	Epoch: [158][167/233]	Loss 0.0004 (0.0054)	
training:	Epoch: [158][168/233]	Loss 0.0003 (0.0054)	
training:	Epoch: [158][169/233]	Loss 0.0007 (0.0053)	
training:	Epoch: [158][170/233]	Loss 0.0003 (0.0053)	
training:	Epoch: [158][171/233]	Loss 0.0004 (0.0053)	
training:	Epoch: [158][172/233]	Loss 0.0006 (0.0053)	
training:	Epoch: [158][173/233]	Loss 0.0004 (0.0052)	
training:	Epoch: [158][174/233]	Loss 0.0003 (0.0052)	
training:	Epoch: [158][175/233]	Loss 0.0011 (0.0052)	
training:	Epoch: [158][176/233]	Loss 0.0007 (0.0052)	
training:	Epoch: [158][177/233]	Loss 0.0003 (0.0051)	
training:	Epoch: [158][178/233]	Loss 0.0010 (0.0051)	
training:	Epoch: [158][179/233]	Loss 0.0003 (0.0051)	
training:	Epoch: [158][180/233]	Loss 0.0003 (0.0050)	
training:	Epoch: [158][181/233]	Loss 0.0003 (0.0050)	
training:	Epoch: [158][182/233]	Loss 0.0002 (0.0050)	
training:	Epoch: [158][183/233]	Loss 0.0003 (0.0050)	
training:	Epoch: [158][184/233]	Loss 0.0007 (0.0049)	
training:	Epoch: [158][185/233]	Loss 0.0002 (0.0049)	
training:	Epoch: [158][186/233]	Loss 0.0003 (0.0049)	
training:	Epoch: [158][187/233]	Loss 0.0018 (0.0049)	
training:	Epoch: [158][188/233]	Loss 0.0011 (0.0049)	
training:	Epoch: [158][189/233]	Loss 0.0004 (0.0048)	
training:	Epoch: [158][190/233]	Loss 0.0007 (0.0048)	
training:	Epoch: [158][191/233]	Loss 0.0005 (0.0048)	
training:	Epoch: [158][192/233]	Loss 0.0031 (0.0048)	
training:	Epoch: [158][193/233]	Loss 0.0008 (0.0048)	
training:	Epoch: [158][194/233]	Loss 0.0002 (0.0047)	
training:	Epoch: [158][195/233]	Loss 0.0061 (0.0047)	
training:	Epoch: [158][196/233]	Loss 0.0003 (0.0047)	
training:	Epoch: [158][197/233]	Loss 0.0007 (0.0047)	
training:	Epoch: [158][198/233]	Loss 0.0002 (0.0047)	
training:	Epoch: [158][199/233]	Loss 0.0002 (0.0047)	
training:	Epoch: [158][200/233]	Loss 0.0158 (0.0047)	
training:	Epoch: [158][201/233]	Loss 0.0018 (0.0047)	
training:	Epoch: [158][202/233]	Loss 0.0003 (0.0047)	
training:	Epoch: [158][203/233]	Loss 0.0003 (0.0047)	
training:	Epoch: [158][204/233]	Loss 0.0005 (0.0046)	
training:	Epoch: [158][205/233]	Loss 0.0015 (0.0046)	
training:	Epoch: [158][206/233]	Loss 0.0009 (0.0046)	
training:	Epoch: [158][207/233]	Loss 0.0003 (0.0046)	
training:	Epoch: [158][208/233]	Loss 0.0003 (0.0046)	
training:	Epoch: [158][209/233]	Loss 0.0003 (0.0045)	
training:	Epoch: [158][210/233]	Loss 0.0132 (0.0046)	
training:	Epoch: [158][211/233]	Loss 0.0075 (0.0046)	
training:	Epoch: [158][212/233]	Loss 0.2347 (0.0057)	
training:	Epoch: [158][213/233]	Loss 0.0715 (0.0060)	
training:	Epoch: [158][214/233]	Loss 0.0002 (0.0060)	
training:	Epoch: [158][215/233]	Loss 0.0004 (0.0059)	
training:	Epoch: [158][216/233]	Loss 0.0003 (0.0059)	
training:	Epoch: [158][217/233]	Loss 0.0070 (0.0059)	
training:	Epoch: [158][218/233]	Loss 0.0002 (0.0059)	
training:	Epoch: [158][219/233]	Loss 0.0023 (0.0059)	
training:	Epoch: [158][220/233]	Loss 0.0003 (0.0058)	
training:	Epoch: [158][221/233]	Loss 0.0003 (0.0058)	
training:	Epoch: [158][222/233]	Loss 0.0004 (0.0058)	
training:	Epoch: [158][223/233]	Loss 0.0004 (0.0058)	
training:	Epoch: [158][224/233]	Loss 0.0135 (0.0058)	
training:	Epoch: [158][225/233]	Loss 0.0005 (0.0058)	
training:	Epoch: [158][226/233]	Loss 0.0002 (0.0058)	
training:	Epoch: [158][227/233]	Loss 0.0023 (0.0057)	
training:	Epoch: [158][228/233]	Loss 0.0003 (0.0057)	
training:	Epoch: [158][229/233]	Loss 0.0004 (0.0057)	
training:	Epoch: [158][230/233]	Loss 0.0008 (0.0057)	
training:	Epoch: [158][231/233]	Loss 0.0003 (0.0057)	
training:	Epoch: [158][232/233]	Loss 0.0003 (0.0056)	
training:	Epoch: [158][233/233]	Loss 0.0005 (0.0056)	
Training:	 Loss: 0.0056

Training:	 ACC: 1.0000 1.0000 1.0000 1.0000
Validation:	 ACC: 0.7935 0.7924 0.7702 0.8169
Validation:	 Best_BACC: 0.8162 0.8154 0.7998 0.8326
Validation:	 Loss: 1.2460
Pretraining:	Epoch 159/200
----------
training:	Epoch: [159][1/233]	Loss 0.0024 (0.0024)	
training:	Epoch: [159][2/233]	Loss 0.0003 (0.0013)	
training:	Epoch: [159][3/233]	Loss 0.0029 (0.0018)	
training:	Epoch: [159][4/233]	Loss 0.0003 (0.0015)	
training:	Epoch: [159][5/233]	Loss 0.0008 (0.0013)	
training:	Epoch: [159][6/233]	Loss 0.0003 (0.0011)	
training:	Epoch: [159][7/233]	Loss 0.0002 (0.0010)	
training:	Epoch: [159][8/233]	Loss 0.0017 (0.0011)	
training:	Epoch: [159][9/233]	Loss 0.0004 (0.0010)	
training:	Epoch: [159][10/233]	Loss 0.0009 (0.0010)	
training:	Epoch: [159][11/233]	Loss 0.0081 (0.0017)	
training:	Epoch: [159][12/233]	Loss 0.0003 (0.0015)	
training:	Epoch: [159][13/233]	Loss 0.0004 (0.0014)	
training:	Epoch: [159][14/233]	Loss 0.0004 (0.0014)	
training:	Epoch: [159][15/233]	Loss 0.0003 (0.0013)	
training:	Epoch: [159][16/233]	Loss 0.0003 (0.0012)	
training:	Epoch: [159][17/233]	Loss 0.0003 (0.0012)	
training:	Epoch: [159][18/233]	Loss 0.0002 (0.0011)	
training:	Epoch: [159][19/233]	Loss 0.0003 (0.0011)	
training:	Epoch: [159][20/233]	Loss 0.0006 (0.0011)	
training:	Epoch: [159][21/233]	Loss 0.0003 (0.0010)	
training:	Epoch: [159][22/233]	Loss 0.0004 (0.0010)	
training:	Epoch: [159][23/233]	Loss 0.0009 (0.0010)	
training:	Epoch: [159][24/233]	Loss 0.0007 (0.0010)	
training:	Epoch: [159][25/233]	Loss 0.0004 (0.0010)	
training:	Epoch: [159][26/233]	Loss 0.0003 (0.0009)	
training:	Epoch: [159][27/233]	Loss 0.0007 (0.0009)	
training:	Epoch: [159][28/233]	Loss 0.0003 (0.0009)	
training:	Epoch: [159][29/233]	Loss 0.0003 (0.0009)	
training:	Epoch: [159][30/233]	Loss 0.0002 (0.0009)	
training:	Epoch: [159][31/233]	Loss 0.0015 (0.0009)	
training:	Epoch: [159][32/233]	Loss 0.0004 (0.0009)	
training:	Epoch: [159][33/233]	Loss 0.0002 (0.0008)	
training:	Epoch: [159][34/233]	Loss 0.0013 (0.0009)	
training:	Epoch: [159][35/233]	Loss 0.0003 (0.0008)	
training:	Epoch: [159][36/233]	Loss 0.0003 (0.0008)	
training:	Epoch: [159][37/233]	Loss 0.0003 (0.0008)	
training:	Epoch: [159][38/233]	Loss 0.0003 (0.0008)	
training:	Epoch: [159][39/233]	Loss 0.0037 (0.0009)	
training:	Epoch: [159][40/233]	Loss 0.0003 (0.0009)	
training:	Epoch: [159][41/233]	Loss 0.0003 (0.0008)	
training:	Epoch: [159][42/233]	Loss 0.0003 (0.0008)	
training:	Epoch: [159][43/233]	Loss 0.0043 (0.0009)	
training:	Epoch: [159][44/233]	Loss 0.0013 (0.0009)	
training:	Epoch: [159][45/233]	Loss 0.0003 (0.0009)	
training:	Epoch: [159][46/233]	Loss 0.0014 (0.0009)	
training:	Epoch: [159][47/233]	Loss 0.0002 (0.0009)	
training:	Epoch: [159][48/233]	Loss 0.0013 (0.0009)	
training:	Epoch: [159][49/233]	Loss 0.0003 (0.0009)	
training:	Epoch: [159][50/233]	Loss 0.0005 (0.0009)	
training:	Epoch: [159][51/233]	Loss 0.0007 (0.0009)	
training:	Epoch: [159][52/233]	Loss 0.0002 (0.0009)	
training:	Epoch: [159][53/233]	Loss 0.0003 (0.0009)	
training:	Epoch: [159][54/233]	Loss 0.0002 (0.0008)	
training:	Epoch: [159][55/233]	Loss 0.0003 (0.0008)	
training:	Epoch: [159][56/233]	Loss 0.0003 (0.0008)	
training:	Epoch: [159][57/233]	Loss 0.0003 (0.0008)	
training:	Epoch: [159][58/233]	Loss 0.0004 (0.0008)	
training:	Epoch: [159][59/233]	Loss 0.0002 (0.0008)	
training:	Epoch: [159][60/233]	Loss 0.0006 (0.0008)	
training:	Epoch: [159][61/233]	Loss 0.0003 (0.0008)	
training:	Epoch: [159][62/233]	Loss 0.0004 (0.0008)	
training:	Epoch: [159][63/233]	Loss 0.0005 (0.0008)	
training:	Epoch: [159][64/233]	Loss 0.0003 (0.0008)	
training:	Epoch: [159][65/233]	Loss 0.0003 (0.0008)	
training:	Epoch: [159][66/233]	Loss 0.0003 (0.0008)	
training:	Epoch: [159][67/233]	Loss 0.0002 (0.0007)	
training:	Epoch: [159][68/233]	Loss 0.0003 (0.0007)	
training:	Epoch: [159][69/233]	Loss 0.0005 (0.0007)	
training:	Epoch: [159][70/233]	Loss 0.0004 (0.0007)	
training:	Epoch: [159][71/233]	Loss 0.0003 (0.0007)	
training:	Epoch: [159][72/233]	Loss 0.0002 (0.0007)	
training:	Epoch: [159][73/233]	Loss 0.0003 (0.0007)	
training:	Epoch: [159][74/233]	Loss 0.0066 (0.0008)	
training:	Epoch: [159][75/233]	Loss 0.0011 (0.0008)	
training:	Epoch: [159][76/233]	Loss 0.0006 (0.0008)	
training:	Epoch: [159][77/233]	Loss 0.0007 (0.0008)	
training:	Epoch: [159][78/233]	Loss 0.0007 (0.0008)	
training:	Epoch: [159][79/233]	Loss 0.0003 (0.0008)	
training:	Epoch: [159][80/233]	Loss 0.0003 (0.0008)	
training:	Epoch: [159][81/233]	Loss 0.0012 (0.0008)	
training:	Epoch: [159][82/233]	Loss 0.0002 (0.0008)	
training:	Epoch: [159][83/233]	Loss 0.0002 (0.0008)	
training:	Epoch: [159][84/233]	Loss 0.0002 (0.0008)	
training:	Epoch: [159][85/233]	Loss 0.0003 (0.0008)	
training:	Epoch: [159][86/233]	Loss 0.0002 (0.0008)	
training:	Epoch: [159][87/233]	Loss 0.0004 (0.0007)	
training:	Epoch: [159][88/233]	Loss 0.0003 (0.0007)	
training:	Epoch: [159][89/233]	Loss 0.0003 (0.0007)	
training:	Epoch: [159][90/233]	Loss 0.0006 (0.0007)	
training:	Epoch: [159][91/233]	Loss 0.0003 (0.0007)	
training:	Epoch: [159][92/233]	Loss 0.0002 (0.0007)	
training:	Epoch: [159][93/233]	Loss 0.0004 (0.0007)	
training:	Epoch: [159][94/233]	Loss 0.0022 (0.0007)	
training:	Epoch: [159][95/233]	Loss 0.0002 (0.0007)	
training:	Epoch: [159][96/233]	Loss 0.0003 (0.0007)	
training:	Epoch: [159][97/233]	Loss 0.0004 (0.0007)	
training:	Epoch: [159][98/233]	Loss 0.0003 (0.0007)	
training:	Epoch: [159][99/233]	Loss 0.0005 (0.0007)	
training:	Epoch: [159][100/233]	Loss 0.0752 (0.0015)	
training:	Epoch: [159][101/233]	Loss 0.0002 (0.0014)	
training:	Epoch: [159][102/233]	Loss 0.0019 (0.0015)	
training:	Epoch: [159][103/233]	Loss 0.0005 (0.0014)	
training:	Epoch: [159][104/233]	Loss 0.0003 (0.0014)	
training:	Epoch: [159][105/233]	Loss 0.0059 (0.0015)	
training:	Epoch: [159][106/233]	Loss 0.0089 (0.0015)	
training:	Epoch: [159][107/233]	Loss 0.0003 (0.0015)	
training:	Epoch: [159][108/233]	Loss 0.0006 (0.0015)	
training:	Epoch: [159][109/233]	Loss 0.0004 (0.0015)	
training:	Epoch: [159][110/233]	Loss 0.0002 (0.0015)	
training:	Epoch: [159][111/233]	Loss 0.0003 (0.0015)	
training:	Epoch: [159][112/233]	Loss 0.0003 (0.0015)	
training:	Epoch: [159][113/233]	Loss 0.0003 (0.0015)	
training:	Epoch: [159][114/233]	Loss 0.0002 (0.0015)	
training:	Epoch: [159][115/233]	Loss 0.0011 (0.0015)	
training:	Epoch: [159][116/233]	Loss 0.0002 (0.0014)	
training:	Epoch: [159][117/233]	Loss 0.0005 (0.0014)	
training:	Epoch: [159][118/233]	Loss 0.0006 (0.0014)	
training:	Epoch: [159][119/233]	Loss 0.0006 (0.0014)	
training:	Epoch: [159][120/233]	Loss 0.0005 (0.0014)	
training:	Epoch: [159][121/233]	Loss 0.0005 (0.0014)	
training:	Epoch: [159][122/233]	Loss 0.0007 (0.0014)	
training:	Epoch: [159][123/233]	Loss 0.0003 (0.0014)	
training:	Epoch: [159][124/233]	Loss 0.0002 (0.0014)	
training:	Epoch: [159][125/233]	Loss 0.0023 (0.0014)	
training:	Epoch: [159][126/233]	Loss 0.0003 (0.0014)	
training:	Epoch: [159][127/233]	Loss 0.0002 (0.0014)	
training:	Epoch: [159][128/233]	Loss 0.0003 (0.0014)	
training:	Epoch: [159][129/233]	Loss 0.0003 (0.0014)	
training:	Epoch: [159][130/233]	Loss 0.0004 (0.0013)	
training:	Epoch: [159][131/233]	Loss 0.0003 (0.0013)	
training:	Epoch: [159][132/233]	Loss 0.0003 (0.0013)	
training:	Epoch: [159][133/233]	Loss 0.0005 (0.0013)	
training:	Epoch: [159][134/233]	Loss 0.0002 (0.0013)	
training:	Epoch: [159][135/233]	Loss 0.0004 (0.0013)	
training:	Epoch: [159][136/233]	Loss 0.0010 (0.0013)	
training:	Epoch: [159][137/233]	Loss 0.0026 (0.0013)	
training:	Epoch: [159][138/233]	Loss 0.0005 (0.0013)	
training:	Epoch: [159][139/233]	Loss 0.1006 (0.0020)	
training:	Epoch: [159][140/233]	Loss 0.0011 (0.0020)	
training:	Epoch: [159][141/233]	Loss 0.0002 (0.0020)	
training:	Epoch: [159][142/233]	Loss 0.0004 (0.0020)	
training:	Epoch: [159][143/233]	Loss 0.0005 (0.0020)	
training:	Epoch: [159][144/233]	Loss 0.0006 (0.0020)	
training:	Epoch: [159][145/233]	Loss 0.0003 (0.0020)	
training:	Epoch: [159][146/233]	Loss 0.0003 (0.0020)	
training:	Epoch: [159][147/233]	Loss 0.0005 (0.0019)	
training:	Epoch: [159][148/233]	Loss 0.0002 (0.0019)	
training:	Epoch: [159][149/233]	Loss 0.0004 (0.0019)	
training:	Epoch: [159][150/233]	Loss 0.0004 (0.0019)	
training:	Epoch: [159][151/233]	Loss 0.0002 (0.0019)	
training:	Epoch: [159][152/233]	Loss 0.0005 (0.0019)	
training:	Epoch: [159][153/233]	Loss 0.0003 (0.0019)	
training:	Epoch: [159][154/233]	Loss 0.0003 (0.0019)	
training:	Epoch: [159][155/233]	Loss 0.0003 (0.0019)	
training:	Epoch: [159][156/233]	Loss 0.0010 (0.0019)	
training:	Epoch: [159][157/233]	Loss 0.0004 (0.0018)	
training:	Epoch: [159][158/233]	Loss 0.0002 (0.0018)	
training:	Epoch: [159][159/233]	Loss 0.0002 (0.0018)	
training:	Epoch: [159][160/233]	Loss 0.0002 (0.0018)	
training:	Epoch: [159][161/233]	Loss 0.0003 (0.0018)	
training:	Epoch: [159][162/233]	Loss 0.0003 (0.0018)	
training:	Epoch: [159][163/233]	Loss 0.0002 (0.0018)	
training:	Epoch: [159][164/233]	Loss 0.0144 (0.0019)	
training:	Epoch: [159][165/233]	Loss 0.0004 (0.0019)	
training:	Epoch: [159][166/233]	Loss 0.0003 (0.0018)	
training:	Epoch: [159][167/233]	Loss 0.0824 (0.0023)	
training:	Epoch: [159][168/233]	Loss 0.0003 (0.0023)	
training:	Epoch: [159][169/233]	Loss 0.0006 (0.0023)	
training:	Epoch: [159][170/233]	Loss 0.0003 (0.0023)	
training:	Epoch: [159][171/233]	Loss 0.0006 (0.0023)	
training:	Epoch: [159][172/233]	Loss 0.0003 (0.0023)	
training:	Epoch: [159][173/233]	Loss 0.0006 (0.0023)	
training:	Epoch: [159][174/233]	Loss 0.0003 (0.0022)	
training:	Epoch: [159][175/233]	Loss 0.0003 (0.0022)	
training:	Epoch: [159][176/233]	Loss 0.0005 (0.0022)	
training:	Epoch: [159][177/233]	Loss 0.0003 (0.0022)	
training:	Epoch: [159][178/233]	Loss 0.0003 (0.0022)	
training:	Epoch: [159][179/233]	Loss 0.0003 (0.0022)	
training:	Epoch: [159][180/233]	Loss 0.0010 (0.0022)	
training:	Epoch: [159][181/233]	Loss 0.0002 (0.0022)	
training:	Epoch: [159][182/233]	Loss 0.0002 (0.0022)	
training:	Epoch: [159][183/233]	Loss 0.0003 (0.0022)	
training:	Epoch: [159][184/233]	Loss 0.0011 (0.0022)	
training:	Epoch: [159][185/233]	Loss 0.0003 (0.0021)	
training:	Epoch: [159][186/233]	Loss 0.0025 (0.0021)	
training:	Epoch: [159][187/233]	Loss 0.0002 (0.0021)	
training:	Epoch: [159][188/233]	Loss 0.0002 (0.0021)	
training:	Epoch: [159][189/233]	Loss 0.0005 (0.0021)	
training:	Epoch: [159][190/233]	Loss 0.0003 (0.0021)	
training:	Epoch: [159][191/233]	Loss 0.0010 (0.0021)	
training:	Epoch: [159][192/233]	Loss 0.0018 (0.0021)	
training:	Epoch: [159][193/233]	Loss 0.0009 (0.0021)	
training:	Epoch: [159][194/233]	Loss 0.0005 (0.0021)	
training:	Epoch: [159][195/233]	Loss 0.0003 (0.0021)	
training:	Epoch: [159][196/233]	Loss 0.0003 (0.0021)	
training:	Epoch: [159][197/233]	Loss 0.0002 (0.0021)	
training:	Epoch: [159][198/233]	Loss 0.0005 (0.0020)	
training:	Epoch: [159][199/233]	Loss 0.0215 (0.0021)	
training:	Epoch: [159][200/233]	Loss 0.0010 (0.0021)	
training:	Epoch: [159][201/233]	Loss 0.0002 (0.0021)	
training:	Epoch: [159][202/233]	Loss 0.0002 (0.0021)	
training:	Epoch: [159][203/233]	Loss 0.0003 (0.0021)	
training:	Epoch: [159][204/233]	Loss 0.0004 (0.0021)	
training:	Epoch: [159][205/233]	Loss 0.0002 (0.0021)	
training:	Epoch: [159][206/233]	Loss 0.0004 (0.0021)	
training:	Epoch: [159][207/233]	Loss 0.0003 (0.0021)	
training:	Epoch: [159][208/233]	Loss 0.0002 (0.0021)	
training:	Epoch: [159][209/233]	Loss 0.0006 (0.0021)	
training:	Epoch: [159][210/233]	Loss 0.0002 (0.0021)	
training:	Epoch: [159][211/233]	Loss 0.0006 (0.0020)	
training:	Epoch: [159][212/233]	Loss 0.0007 (0.0020)	
training:	Epoch: [159][213/233]	Loss 0.0003 (0.0020)	
training:	Epoch: [159][214/233]	Loss 0.0002 (0.0020)	
training:	Epoch: [159][215/233]	Loss 0.2201 (0.0030)	
training:	Epoch: [159][216/233]	Loss 0.0002 (0.0030)	
training:	Epoch: [159][217/233]	Loss 0.0010 (0.0030)	
training:	Epoch: [159][218/233]	Loss 0.0003 (0.0030)	
training:	Epoch: [159][219/233]	Loss 0.0009 (0.0030)	
training:	Epoch: [159][220/233]	Loss 0.0003 (0.0030)	
training:	Epoch: [159][221/233]	Loss 0.0003 (0.0030)	
training:	Epoch: [159][222/233]	Loss 0.0005 (0.0030)	
training:	Epoch: [159][223/233]	Loss 0.0005 (0.0029)	
training:	Epoch: [159][224/233]	Loss 0.0004 (0.0029)	
training:	Epoch: [159][225/233]	Loss 0.0005 (0.0029)	
training:	Epoch: [159][226/233]	Loss 0.0035 (0.0029)	
training:	Epoch: [159][227/233]	Loss 0.0002 (0.0029)	
training:	Epoch: [159][228/233]	Loss 0.0002 (0.0029)	
training:	Epoch: [159][229/233]	Loss 0.0003 (0.0029)	
training:	Epoch: [159][230/233]	Loss 0.0003 (0.0029)	
training:	Epoch: [159][231/233]	Loss 0.0003 (0.0029)	
training:	Epoch: [159][232/233]	Loss 0.0003 (0.0029)	
training:	Epoch: [159][233/233]	Loss 0.0002 (0.0028)	
Training:	 Loss: 0.0028

Training:	 ACC: 1.0000 1.0000 1.0000 1.0000
Validation:	 ACC: 0.7971 0.7978 0.8100 0.7843
Validation:	 Best_BACC: 0.8162 0.8154 0.7998 0.8326
Validation:	 Loss: 1.2660
Pretraining:	Epoch 160/200
----------
training:	Epoch: [160][1/233]	Loss 0.0002 (0.0002)	
training:	Epoch: [160][2/233]	Loss 0.0006 (0.0004)	
training:	Epoch: [160][3/233]	Loss 0.0002 (0.0004)	
training:	Epoch: [160][4/233]	Loss 0.0004 (0.0004)	
training:	Epoch: [160][5/233]	Loss 0.0004 (0.0004)	
training:	Epoch: [160][6/233]	Loss 0.0029 (0.0008)	
training:	Epoch: [160][7/233]	Loss 0.0003 (0.0007)	
training:	Epoch: [160][8/233]	Loss 0.0003 (0.0007)	
training:	Epoch: [160][9/233]	Loss 0.0003 (0.0006)	
training:	Epoch: [160][10/233]	Loss 0.0002 (0.0006)	
training:	Epoch: [160][11/233]	Loss 0.0002 (0.0005)	
training:	Epoch: [160][12/233]	Loss 0.0007 (0.0006)	
training:	Epoch: [160][13/233]	Loss 0.0016 (0.0006)	
training:	Epoch: [160][14/233]	Loss 0.0009 (0.0007)	
training:	Epoch: [160][15/233]	Loss 0.0007 (0.0007)	
training:	Epoch: [160][16/233]	Loss 0.0003 (0.0006)	
training:	Epoch: [160][17/233]	Loss 0.0012 (0.0007)	
training:	Epoch: [160][18/233]	Loss 0.0002 (0.0006)	
training:	Epoch: [160][19/233]	Loss 0.0007 (0.0006)	
training:	Epoch: [160][20/233]	Loss 0.0003 (0.0006)	
training:	Epoch: [160][21/233]	Loss 0.0002 (0.0006)	
training:	Epoch: [160][22/233]	Loss 0.0003 (0.0006)	
training:	Epoch: [160][23/233]	Loss 0.0005 (0.0006)	
training:	Epoch: [160][24/233]	Loss 0.0005 (0.0006)	
training:	Epoch: [160][25/233]	Loss 0.0002 (0.0006)	
training:	Epoch: [160][26/233]	Loss 0.0002 (0.0006)	
training:	Epoch: [160][27/233]	Loss 0.0008 (0.0006)	
training:	Epoch: [160][28/233]	Loss 0.0003 (0.0006)	
training:	Epoch: [160][29/233]	Loss 0.0002 (0.0005)	
training:	Epoch: [160][30/233]	Loss 0.0002 (0.0005)	
training:	Epoch: [160][31/233]	Loss 0.0002 (0.0005)	
training:	Epoch: [160][32/233]	Loss 0.0003 (0.0005)	
training:	Epoch: [160][33/233]	Loss 0.0003 (0.0005)	
training:	Epoch: [160][34/233]	Loss 0.0066 (0.0007)	
training:	Epoch: [160][35/233]	Loss 0.0007 (0.0007)	
training:	Epoch: [160][36/233]	Loss 0.0002 (0.0007)	
training:	Epoch: [160][37/233]	Loss 0.0003 (0.0007)	
training:	Epoch: [160][38/233]	Loss 0.0004 (0.0007)	
training:	Epoch: [160][39/233]	Loss 0.0003 (0.0007)	
training:	Epoch: [160][40/233]	Loss 0.0003 (0.0006)	
training:	Epoch: [160][41/233]	Loss 0.0002 (0.0006)	
training:	Epoch: [160][42/233]	Loss 0.0003 (0.0006)	
training:	Epoch: [160][43/233]	Loss 0.0026 (0.0007)	
training:	Epoch: [160][44/233]	Loss 0.0003 (0.0007)	
training:	Epoch: [160][45/233]	Loss 0.0002 (0.0007)	
training:	Epoch: [160][46/233]	Loss 0.0004 (0.0006)	
training:	Epoch: [160][47/233]	Loss 0.0003 (0.0006)	
training:	Epoch: [160][48/233]	Loss 0.0003 (0.0006)	
training:	Epoch: [160][49/233]	Loss 0.0005 (0.0006)	
training:	Epoch: [160][50/233]	Loss 0.0004 (0.0006)	
training:	Epoch: [160][51/233]	Loss 0.0002 (0.0006)	
training:	Epoch: [160][52/233]	Loss 0.0003 (0.0006)	
training:	Epoch: [160][53/233]	Loss 0.0002 (0.0006)	
training:	Epoch: [160][54/233]	Loss 0.0112 (0.0008)	
training:	Epoch: [160][55/233]	Loss 0.0002 (0.0008)	
training:	Epoch: [160][56/233]	Loss 0.0003 (0.0008)	
training:	Epoch: [160][57/233]	Loss 0.0058 (0.0009)	
training:	Epoch: [160][58/233]	Loss 0.1069 (0.0027)	
training:	Epoch: [160][59/233]	Loss 0.0006 (0.0027)	
training:	Epoch: [160][60/233]	Loss 0.0003 (0.0026)	
training:	Epoch: [160][61/233]	Loss 0.0002 (0.0026)	
training:	Epoch: [160][62/233]	Loss 0.0002 (0.0025)	
training:	Epoch: [160][63/233]	Loss 0.0011 (0.0025)	
training:	Epoch: [160][64/233]	Loss 0.0003 (0.0025)	
training:	Epoch: [160][65/233]	Loss 0.0002 (0.0024)	
training:	Epoch: [160][66/233]	Loss 0.0003 (0.0024)	
training:	Epoch: [160][67/233]	Loss 0.0045 (0.0024)	
training:	Epoch: [160][68/233]	Loss 0.0004 (0.0024)	
training:	Epoch: [160][69/233]	Loss 0.0002 (0.0024)	
training:	Epoch: [160][70/233]	Loss 0.0002 (0.0024)	
training:	Epoch: [160][71/233]	Loss 0.0050 (0.0024)	
training:	Epoch: [160][72/233]	Loss 0.0005 (0.0024)	
training:	Epoch: [160][73/233]	Loss 0.0002 (0.0023)	
training:	Epoch: [160][74/233]	Loss 0.0004 (0.0023)	
training:	Epoch: [160][75/233]	Loss 0.0009 (0.0023)	
training:	Epoch: [160][76/233]	Loss 0.0197 (0.0025)	
training:	Epoch: [160][77/233]	Loss 0.0013 (0.0025)	
training:	Epoch: [160][78/233]	Loss 0.0006 (0.0025)	
training:	Epoch: [160][79/233]	Loss 0.0006 (0.0025)	
training:	Epoch: [160][80/233]	Loss 0.0002 (0.0024)	
training:	Epoch: [160][81/233]	Loss 0.0003 (0.0024)	
training:	Epoch: [160][82/233]	Loss 0.0003 (0.0024)	
training:	Epoch: [160][83/233]	Loss 0.0007 (0.0024)	
training:	Epoch: [160][84/233]	Loss 0.0002 (0.0023)	
training:	Epoch: [160][85/233]	Loss 0.0002 (0.0023)	
training:	Epoch: [160][86/233]	Loss 0.0005 (0.0023)	
training:	Epoch: [160][87/233]	Loss 0.0002 (0.0023)	
training:	Epoch: [160][88/233]	Loss 0.0004 (0.0022)	
training:	Epoch: [160][89/233]	Loss 0.0021 (0.0022)	
training:	Epoch: [160][90/233]	Loss 0.0002 (0.0022)	
training:	Epoch: [160][91/233]	Loss 0.0003 (0.0022)	
training:	Epoch: [160][92/233]	Loss 0.0002 (0.0022)	
training:	Epoch: [160][93/233]	Loss 0.0004 (0.0022)	
training:	Epoch: [160][94/233]	Loss 0.0003 (0.0021)	
training:	Epoch: [160][95/233]	Loss 0.0157 (0.0023)	
training:	Epoch: [160][96/233]	Loss 0.0019 (0.0023)	
training:	Epoch: [160][97/233]	Loss 0.0003 (0.0023)	
training:	Epoch: [160][98/233]	Loss 0.0003 (0.0022)	
training:	Epoch: [160][99/233]	Loss 0.0002 (0.0022)	
training:	Epoch: [160][100/233]	Loss 0.0002 (0.0022)	
training:	Epoch: [160][101/233]	Loss 0.0004 (0.0022)	
training:	Epoch: [160][102/233]	Loss 0.0012 (0.0022)	
training:	Epoch: [160][103/233]	Loss 0.0003 (0.0021)	
training:	Epoch: [160][104/233]	Loss 0.0002 (0.0021)	
training:	Epoch: [160][105/233]	Loss 0.0006 (0.0021)	
training:	Epoch: [160][106/233]	Loss 0.0003 (0.0021)	
training:	Epoch: [160][107/233]	Loss 0.0002 (0.0021)	
training:	Epoch: [160][108/233]	Loss 0.0002 (0.0021)	
training:	Epoch: [160][109/233]	Loss 0.0004 (0.0020)	
training:	Epoch: [160][110/233]	Loss 0.0003 (0.0020)	
training:	Epoch: [160][111/233]	Loss 0.0008 (0.0020)	
training:	Epoch: [160][112/233]	Loss 0.0006 (0.0020)	
training:	Epoch: [160][113/233]	Loss 0.0003 (0.0020)	
training:	Epoch: [160][114/233]	Loss 0.0011 (0.0020)	
training:	Epoch: [160][115/233]	Loss 0.0002 (0.0020)	
training:	Epoch: [160][116/233]	Loss 0.0003 (0.0020)	
training:	Epoch: [160][117/233]	Loss 0.0006 (0.0019)	
training:	Epoch: [160][118/233]	Loss 0.0004 (0.0019)	
training:	Epoch: [160][119/233]	Loss 0.0002 (0.0019)	
training:	Epoch: [160][120/233]	Loss 0.0003 (0.0019)	
training:	Epoch: [160][121/233]	Loss 0.0040 (0.0019)	
training:	Epoch: [160][122/233]	Loss 0.0002 (0.0019)	
training:	Epoch: [160][123/233]	Loss 0.0003 (0.0019)	
training:	Epoch: [160][124/233]	Loss 0.0003 (0.0019)	
training:	Epoch: [160][125/233]	Loss 0.0004 (0.0019)	
training:	Epoch: [160][126/233]	Loss 0.0002 (0.0019)	
training:	Epoch: [160][127/233]	Loss 0.0003 (0.0018)	
training:	Epoch: [160][128/233]	Loss 0.0003 (0.0018)	
training:	Epoch: [160][129/233]	Loss 0.0002 (0.0018)	
training:	Epoch: [160][130/233]	Loss 0.0002 (0.0018)	
training:	Epoch: [160][131/233]	Loss 0.0004 (0.0018)	
training:	Epoch: [160][132/233]	Loss 0.0002 (0.0018)	
training:	Epoch: [160][133/233]	Loss 0.0011 (0.0018)	
training:	Epoch: [160][134/233]	Loss 0.0004 (0.0018)	
training:	Epoch: [160][135/233]	Loss 0.0286 (0.0020)	
training:	Epoch: [160][136/233]	Loss 0.0005 (0.0020)	
training:	Epoch: [160][137/233]	Loss 0.0011 (0.0019)	
training:	Epoch: [160][138/233]	Loss 0.0003 (0.0019)	
training:	Epoch: [160][139/233]	Loss 0.0002 (0.0019)	
training:	Epoch: [160][140/233]	Loss 0.0004 (0.0019)	
training:	Epoch: [160][141/233]	Loss 0.0003 (0.0019)	
training:	Epoch: [160][142/233]	Loss 0.0003 (0.0019)	
training:	Epoch: [160][143/233]	Loss 0.0003 (0.0019)	
training:	Epoch: [160][144/233]	Loss 0.0004 (0.0019)	
training:	Epoch: [160][145/233]	Loss 0.0002 (0.0019)	
training:	Epoch: [160][146/233]	Loss 0.0007 (0.0019)	
training:	Epoch: [160][147/233]	Loss 0.0002 (0.0018)	
training:	Epoch: [160][148/233]	Loss 0.0003 (0.0018)	
training:	Epoch: [160][149/233]	Loss 0.0002 (0.0018)	
training:	Epoch: [160][150/233]	Loss 0.0003 (0.0018)	
training:	Epoch: [160][151/233]	Loss 0.0139 (0.0019)	
training:	Epoch: [160][152/233]	Loss 0.0002 (0.0019)	
training:	Epoch: [160][153/233]	Loss 0.1072 (0.0026)	
training:	Epoch: [160][154/233]	Loss 0.0002 (0.0026)	
training:	Epoch: [160][155/233]	Loss 0.0003 (0.0025)	
training:	Epoch: [160][156/233]	Loss 0.0002 (0.0025)	
training:	Epoch: [160][157/233]	Loss 0.0028 (0.0025)	
training:	Epoch: [160][158/233]	Loss 0.0002 (0.0025)	
training:	Epoch: [160][159/233]	Loss 0.0015 (0.0025)	
training:	Epoch: [160][160/233]	Loss 0.0002 (0.0025)	
training:	Epoch: [160][161/233]	Loss 0.0007 (0.0025)	
training:	Epoch: [160][162/233]	Loss 0.0004 (0.0025)	
training:	Epoch: [160][163/233]	Loss 0.0005 (0.0025)	
training:	Epoch: [160][164/233]	Loss 0.0004 (0.0024)	
training:	Epoch: [160][165/233]	Loss 0.0003 (0.0024)	
training:	Epoch: [160][166/233]	Loss 0.0950 (0.0030)	
training:	Epoch: [160][167/233]	Loss 0.0002 (0.0030)	
training:	Epoch: [160][168/233]	Loss 0.0004 (0.0030)	
training:	Epoch: [160][169/233]	Loss 0.0002 (0.0029)	
training:	Epoch: [160][170/233]	Loss 0.0002 (0.0029)	
training:	Epoch: [160][171/233]	Loss 0.0002 (0.0029)	
training:	Epoch: [160][172/233]	Loss 0.0002 (0.0029)	
training:	Epoch: [160][173/233]	Loss 0.0003 (0.0029)	
training:	Epoch: [160][174/233]	Loss 0.0002 (0.0029)	
training:	Epoch: [160][175/233]	Loss 0.0003 (0.0028)	
training:	Epoch: [160][176/233]	Loss 0.0006 (0.0028)	
training:	Epoch: [160][177/233]	Loss 0.0076 (0.0029)	
training:	Epoch: [160][178/233]	Loss 0.0002 (0.0028)	
training:	Epoch: [160][179/233]	Loss 0.0003 (0.0028)	
training:	Epoch: [160][180/233]	Loss 0.0008 (0.0028)	
training:	Epoch: [160][181/233]	Loss 0.0002 (0.0028)	
training:	Epoch: [160][182/233]	Loss 0.0026 (0.0028)	
training:	Epoch: [160][183/233]	Loss 0.0003 (0.0028)	
training:	Epoch: [160][184/233]	Loss 0.0005 (0.0028)	
training:	Epoch: [160][185/233]	Loss 0.0004 (0.0028)	
training:	Epoch: [160][186/233]	Loss 0.0002 (0.0027)	
training:	Epoch: [160][187/233]	Loss 0.0003 (0.0027)	
training:	Epoch: [160][188/233]	Loss 0.0003 (0.0027)	
training:	Epoch: [160][189/233]	Loss 0.0004 (0.0027)	
training:	Epoch: [160][190/233]	Loss 0.0022 (0.0027)	
training:	Epoch: [160][191/233]	Loss 0.0003 (0.0027)	
training:	Epoch: [160][192/233]	Loss 0.0002 (0.0027)	
training:	Epoch: [160][193/233]	Loss 0.0004 (0.0027)	
training:	Epoch: [160][194/233]	Loss 0.0003 (0.0027)	
training:	Epoch: [160][195/233]	Loss 0.0003 (0.0026)	
training:	Epoch: [160][196/233]	Loss 0.0156 (0.0027)	
training:	Epoch: [160][197/233]	Loss 0.0002 (0.0027)	
training:	Epoch: [160][198/233]	Loss 0.0021 (0.0027)	
training:	Epoch: [160][199/233]	Loss 0.0012 (0.0027)	
training:	Epoch: [160][200/233]	Loss 0.0003 (0.0027)	
training:	Epoch: [160][201/233]	Loss 0.0003 (0.0027)	
training:	Epoch: [160][202/233]	Loss 0.0003 (0.0027)	
training:	Epoch: [160][203/233]	Loss 0.0003 (0.0026)	
training:	Epoch: [160][204/233]	Loss 0.0004 (0.0026)	
training:	Epoch: [160][205/233]	Loss 0.0003 (0.0026)	
training:	Epoch: [160][206/233]	Loss 0.0002 (0.0026)	
training:	Epoch: [160][207/233]	Loss 0.0002 (0.0026)	
training:	Epoch: [160][208/233]	Loss 0.0024 (0.0026)	
training:	Epoch: [160][209/233]	Loss 0.0003 (0.0026)	
training:	Epoch: [160][210/233]	Loss 0.0002 (0.0026)	
training:	Epoch: [160][211/233]	Loss 0.0003 (0.0026)	
training:	Epoch: [160][212/233]	Loss 0.0002 (0.0026)	
training:	Epoch: [160][213/233]	Loss 0.0003 (0.0025)	
training:	Epoch: [160][214/233]	Loss 0.0002 (0.0025)	
training:	Epoch: [160][215/233]	Loss 0.0004 (0.0025)	
training:	Epoch: [160][216/233]	Loss 0.0054 (0.0025)	
training:	Epoch: [160][217/233]	Loss 0.0004 (0.0025)	
training:	Epoch: [160][218/233]	Loss 0.0012 (0.0025)	
training:	Epoch: [160][219/233]	Loss 0.0011 (0.0025)	
training:	Epoch: [160][220/233]	Loss 0.0002 (0.0025)	
training:	Epoch: [160][221/233]	Loss 0.0003 (0.0025)	
training:	Epoch: [160][222/233]	Loss 0.0081 (0.0025)	
training:	Epoch: [160][223/233]	Loss 0.0002 (0.0025)	
training:	Epoch: [160][224/233]	Loss 0.0568 (0.0027)	
training:	Epoch: [160][225/233]	Loss 0.0003 (0.0027)	
training:	Epoch: [160][226/233]	Loss 0.0006 (0.0027)	
training:	Epoch: [160][227/233]	Loss 0.0003 (0.0027)	
training:	Epoch: [160][228/233]	Loss 0.0003 (0.0027)	
training:	Epoch: [160][229/233]	Loss 0.0002 (0.0027)	
training:	Epoch: [160][230/233]	Loss 0.0172 (0.0028)	
training:	Epoch: [160][231/233]	Loss 0.0101 (0.0028)	
training:	Epoch: [160][232/233]	Loss 0.0014 (0.0028)	
training:	Epoch: [160][233/233]	Loss 0.0003 (0.0028)	
Training:	 Loss: 0.0028

Training:	 ACC: 1.0000 1.0000 1.0000 1.0000
Validation:	 ACC: 0.7906 0.7903 0.7835 0.7978
Validation:	 Best_BACC: 0.8162 0.8154 0.7998 0.8326
Validation:	 Loss: 1.2967
Pretraining:	Epoch 161/200
----------
training:	Epoch: [161][1/233]	Loss 0.0003 (0.0003)	
training:	Epoch: [161][2/233]	Loss 0.0003 (0.0003)	
training:	Epoch: [161][3/233]	Loss 0.0003 (0.0003)	
training:	Epoch: [161][4/233]	Loss 0.0014 (0.0006)	
training:	Epoch: [161][5/233]	Loss 0.0002 (0.0005)	
training:	Epoch: [161][6/233]	Loss 0.0005 (0.0005)	
training:	Epoch: [161][7/233]	Loss 0.0003 (0.0005)	
training:	Epoch: [161][8/233]	Loss 0.0004 (0.0004)	
training:	Epoch: [161][9/233]	Loss 0.0002 (0.0004)	
training:	Epoch: [161][10/233]	Loss 0.0003 (0.0004)	
training:	Epoch: [161][11/233]	Loss 0.0003 (0.0004)	
training:	Epoch: [161][12/233]	Loss 0.0003 (0.0004)	
training:	Epoch: [161][13/233]	Loss 0.0018 (0.0005)	
training:	Epoch: [161][14/233]	Loss 0.0003 (0.0005)	
training:	Epoch: [161][15/233]	Loss 0.0003 (0.0005)	
training:	Epoch: [161][16/233]	Loss 0.0002 (0.0005)	
training:	Epoch: [161][17/233]	Loss 0.0002 (0.0004)	
training:	Epoch: [161][18/233]	Loss 0.0004 (0.0004)	
training:	Epoch: [161][19/233]	Loss 0.0033 (0.0006)	
training:	Epoch: [161][20/233]	Loss 0.0002 (0.0006)	
training:	Epoch: [161][21/233]	Loss 0.0004 (0.0006)	
training:	Epoch: [161][22/233]	Loss 0.0006 (0.0006)	
training:	Epoch: [161][23/233]	Loss 0.0002 (0.0005)	
training:	Epoch: [161][24/233]	Loss 0.0003 (0.0005)	
training:	Epoch: [161][25/233]	Loss 0.0002 (0.0005)	
training:	Epoch: [161][26/233]	Loss 0.0004 (0.0005)	
training:	Epoch: [161][27/233]	Loss 0.0003 (0.0005)	
training:	Epoch: [161][28/233]	Loss 0.0002 (0.0005)	
training:	Epoch: [161][29/233]	Loss 0.0002 (0.0005)	
training:	Epoch: [161][30/233]	Loss 0.0004 (0.0005)	
training:	Epoch: [161][31/233]	Loss 0.0792 (0.0030)	
training:	Epoch: [161][32/233]	Loss 0.0003 (0.0029)	
training:	Epoch: [161][33/233]	Loss 0.0003 (0.0029)	
training:	Epoch: [161][34/233]	Loss 0.0004 (0.0028)	
training:	Epoch: [161][35/233]	Loss 0.0027 (0.0028)	
training:	Epoch: [161][36/233]	Loss 0.0007 (0.0027)	
training:	Epoch: [161][37/233]	Loss 0.0011 (0.0027)	
training:	Epoch: [161][38/233]	Loss 0.0004 (0.0026)	
training:	Epoch: [161][39/233]	Loss 0.0003 (0.0026)	
training:	Epoch: [161][40/233]	Loss 0.0002 (0.0025)	
training:	Epoch: [161][41/233]	Loss 0.0003 (0.0025)	
training:	Epoch: [161][42/233]	Loss 0.0005 (0.0024)	
training:	Epoch: [161][43/233]	Loss 0.0058 (0.0025)	
training:	Epoch: [161][44/233]	Loss 0.0007 (0.0024)	
training:	Epoch: [161][45/233]	Loss 0.0005 (0.0024)	
training:	Epoch: [161][46/233]	Loss 0.0006 (0.0024)	
training:	Epoch: [161][47/233]	Loss 0.0005 (0.0023)	
training:	Epoch: [161][48/233]	Loss 0.0010 (0.0023)	
training:	Epoch: [161][49/233]	Loss 0.0006 (0.0023)	
training:	Epoch: [161][50/233]	Loss 0.0005 (0.0022)	
training:	Epoch: [161][51/233]	Loss 0.0003 (0.0022)	
training:	Epoch: [161][52/233]	Loss 0.0204 (0.0025)	
training:	Epoch: [161][53/233]	Loss 0.0002 (0.0025)	
training:	Epoch: [161][54/233]	Loss 0.0002 (0.0025)	
training:	Epoch: [161][55/233]	Loss 0.0002 (0.0024)	
training:	Epoch: [161][56/233]	Loss 0.0002 (0.0024)	
training:	Epoch: [161][57/233]	Loss 0.0002 (0.0023)	
training:	Epoch: [161][58/233]	Loss 0.0004 (0.0023)	
training:	Epoch: [161][59/233]	Loss 0.0003 (0.0023)	
training:	Epoch: [161][60/233]	Loss 0.0002 (0.0022)	
training:	Epoch: [161][61/233]	Loss 0.0002 (0.0022)	
training:	Epoch: [161][62/233]	Loss 0.0002 (0.0022)	
training:	Epoch: [161][63/233]	Loss 0.0002 (0.0021)	
training:	Epoch: [161][64/233]	Loss 0.0002 (0.0021)	
training:	Epoch: [161][65/233]	Loss 0.0009 (0.0021)	
training:	Epoch: [161][66/233]	Loss 0.0002 (0.0021)	
training:	Epoch: [161][67/233]	Loss 0.0004 (0.0020)	
training:	Epoch: [161][68/233]	Loss 0.0002 (0.0020)	
training:	Epoch: [161][69/233]	Loss 0.0005 (0.0020)	
training:	Epoch: [161][70/233]	Loss 0.0003 (0.0020)	
training:	Epoch: [161][71/233]	Loss 0.0011 (0.0019)	
training:	Epoch: [161][72/233]	Loss 0.0003 (0.0019)	
training:	Epoch: [161][73/233]	Loss 0.0003 (0.0019)	
training:	Epoch: [161][74/233]	Loss 0.0003 (0.0019)	
training:	Epoch: [161][75/233]	Loss 0.0002 (0.0019)	
training:	Epoch: [161][76/233]	Loss 0.0029 (0.0019)	
training:	Epoch: [161][77/233]	Loss 0.0002 (0.0019)	
training:	Epoch: [161][78/233]	Loss 0.0002 (0.0018)	
training:	Epoch: [161][79/233]	Loss 0.0002 (0.0018)	
training:	Epoch: [161][80/233]	Loss 0.0035 (0.0018)	
training:	Epoch: [161][81/233]	Loss 0.0003 (0.0018)	
training:	Epoch: [161][82/233]	Loss 0.0003 (0.0018)	
training:	Epoch: [161][83/233]	Loss 0.0002 (0.0018)	
training:	Epoch: [161][84/233]	Loss 0.0002 (0.0018)	
training:	Epoch: [161][85/233]	Loss 0.0004 (0.0017)	
training:	Epoch: [161][86/233]	Loss 0.0002 (0.0017)	
training:	Epoch: [161][87/233]	Loss 0.0004 (0.0017)	
training:	Epoch: [161][88/233]	Loss 0.0003 (0.0017)	
training:	Epoch: [161][89/233]	Loss 0.0002 (0.0017)	
training:	Epoch: [161][90/233]	Loss 0.0002 (0.0017)	
training:	Epoch: [161][91/233]	Loss 0.0002 (0.0016)	
training:	Epoch: [161][92/233]	Loss 0.0002 (0.0016)	
training:	Epoch: [161][93/233]	Loss 0.0002 (0.0016)	
training:	Epoch: [161][94/233]	Loss 0.0004 (0.0016)	
training:	Epoch: [161][95/233]	Loss 0.0005 (0.0016)	
training:	Epoch: [161][96/233]	Loss 0.0003 (0.0016)	
training:	Epoch: [161][97/233]	Loss 0.0002 (0.0016)	
training:	Epoch: [161][98/233]	Loss 0.0002 (0.0015)	
training:	Epoch: [161][99/233]	Loss 0.0002 (0.0015)	
training:	Epoch: [161][100/233]	Loss 0.0002 (0.0015)	
training:	Epoch: [161][101/233]	Loss 0.0003 (0.0015)	
training:	Epoch: [161][102/233]	Loss 0.0016 (0.0015)	
training:	Epoch: [161][103/233]	Loss 0.0003 (0.0015)	
training:	Epoch: [161][104/233]	Loss 0.0002 (0.0015)	
training:	Epoch: [161][105/233]	Loss 0.0003 (0.0015)	
training:	Epoch: [161][106/233]	Loss 0.0002 (0.0015)	
training:	Epoch: [161][107/233]	Loss 0.0002 (0.0015)	
training:	Epoch: [161][108/233]	Loss 0.0045 (0.0015)	
training:	Epoch: [161][109/233]	Loss 0.0322 (0.0018)	
training:	Epoch: [161][110/233]	Loss 0.0002 (0.0017)	
training:	Epoch: [161][111/233]	Loss 0.0002 (0.0017)	
training:	Epoch: [161][112/233]	Loss 0.0004 (0.0017)	
training:	Epoch: [161][113/233]	Loss 0.0002 (0.0017)	
training:	Epoch: [161][114/233]	Loss 0.0003 (0.0017)	
training:	Epoch: [161][115/233]	Loss 0.0006 (0.0017)	
training:	Epoch: [161][116/233]	Loss 0.0003 (0.0017)	
training:	Epoch: [161][117/233]	Loss 0.0002 (0.0017)	
training:	Epoch: [161][118/233]	Loss 0.0005 (0.0017)	
training:	Epoch: [161][119/233]	Loss 0.0003 (0.0016)	
training:	Epoch: [161][120/233]	Loss 0.0018 (0.0016)	
training:	Epoch: [161][121/233]	Loss 0.0019 (0.0016)	
training:	Epoch: [161][122/233]	Loss 0.0003 (0.0016)	
training:	Epoch: [161][123/233]	Loss 0.0002 (0.0016)	
training:	Epoch: [161][124/233]	Loss 0.0017 (0.0016)	
training:	Epoch: [161][125/233]	Loss 0.0003 (0.0016)	
training:	Epoch: [161][126/233]	Loss 0.0004 (0.0016)	
training:	Epoch: [161][127/233]	Loss 0.0004 (0.0016)	
training:	Epoch: [161][128/233]	Loss 0.0007 (0.0016)	
training:	Epoch: [161][129/233]	Loss 0.0002 (0.0016)	
training:	Epoch: [161][130/233]	Loss 0.0003 (0.0016)	
training:	Epoch: [161][131/233]	Loss 0.0003 (0.0016)	
training:	Epoch: [161][132/233]	Loss 0.0005 (0.0015)	
training:	Epoch: [161][133/233]	Loss 0.0036 (0.0016)	
training:	Epoch: [161][134/233]	Loss 0.0002 (0.0016)	
training:	Epoch: [161][135/233]	Loss 0.0004 (0.0015)	
training:	Epoch: [161][136/233]	Loss 0.0009 (0.0015)	
training:	Epoch: [161][137/233]	Loss 0.0003 (0.0015)	
training:	Epoch: [161][138/233]	Loss 0.0025 (0.0015)	
training:	Epoch: [161][139/233]	Loss 0.0002 (0.0015)	
training:	Epoch: [161][140/233]	Loss 0.0002 (0.0015)	
training:	Epoch: [161][141/233]	Loss 0.0002 (0.0015)	
training:	Epoch: [161][142/233]	Loss 0.0003 (0.0015)	
training:	Epoch: [161][143/233]	Loss 0.0005 (0.0015)	
training:	Epoch: [161][144/233]	Loss 0.0003 (0.0015)	
training:	Epoch: [161][145/233]	Loss 0.0003 (0.0015)	
training:	Epoch: [161][146/233]	Loss 0.0002 (0.0015)	
training:	Epoch: [161][147/233]	Loss 0.0002 (0.0015)	
training:	Epoch: [161][148/233]	Loss 0.0002 (0.0015)	
training:	Epoch: [161][149/233]	Loss 0.0003 (0.0014)	
training:	Epoch: [161][150/233]	Loss 0.0002 (0.0014)	
training:	Epoch: [161][151/233]	Loss 0.0002 (0.0014)	
training:	Epoch: [161][152/233]	Loss 0.0004 (0.0014)	
training:	Epoch: [161][153/233]	Loss 0.0044 (0.0014)	
training:	Epoch: [161][154/233]	Loss 0.0002 (0.0014)	
training:	Epoch: [161][155/233]	Loss 0.0002 (0.0014)	
training:	Epoch: [161][156/233]	Loss 0.0002 (0.0014)	
training:	Epoch: [161][157/233]	Loss 0.0002 (0.0014)	
training:	Epoch: [161][158/233]	Loss 0.0002 (0.0014)	
training:	Epoch: [161][159/233]	Loss 0.0210 (0.0015)	
training:	Epoch: [161][160/233]	Loss 0.0053 (0.0015)	
training:	Epoch: [161][161/233]	Loss 0.0008 (0.0015)	
training:	Epoch: [161][162/233]	Loss 0.0003 (0.0015)	
training:	Epoch: [161][163/233]	Loss 0.0003 (0.0015)	
training:	Epoch: [161][164/233]	Loss 0.0003 (0.0015)	
training:	Epoch: [161][165/233]	Loss 0.0003 (0.0015)	
training:	Epoch: [161][166/233]	Loss 0.0116 (0.0016)	
training:	Epoch: [161][167/233]	Loss 0.0003 (0.0016)	
training:	Epoch: [161][168/233]	Loss 0.0007 (0.0016)	
training:	Epoch: [161][169/233]	Loss 0.0002 (0.0016)	
training:	Epoch: [161][170/233]	Loss 0.0004 (0.0015)	
training:	Epoch: [161][171/233]	Loss 0.0002 (0.0015)	
training:	Epoch: [161][172/233]	Loss 0.0002 (0.0015)	
training:	Epoch: [161][173/233]	Loss 0.0003 (0.0015)	
training:	Epoch: [161][174/233]	Loss 0.0004 (0.0015)	
training:	Epoch: [161][175/233]	Loss 0.0003 (0.0015)	
training:	Epoch: [161][176/233]	Loss 0.0003 (0.0015)	
training:	Epoch: [161][177/233]	Loss 0.0003 (0.0015)	
training:	Epoch: [161][178/233]	Loss 0.0003 (0.0015)	
training:	Epoch: [161][179/233]	Loss 0.0088 (0.0015)	
training:	Epoch: [161][180/233]	Loss 0.0002 (0.0015)	
training:	Epoch: [161][181/233]	Loss 0.0004 (0.0015)	
training:	Epoch: [161][182/233]	Loss 0.0006 (0.0015)	
training:	Epoch: [161][183/233]	Loss 0.0173 (0.0016)	
training:	Epoch: [161][184/233]	Loss 0.0011 (0.0016)	
training:	Epoch: [161][185/233]	Loss 0.0002 (0.0016)	
training:	Epoch: [161][186/233]	Loss 0.0003 (0.0016)	
training:	Epoch: [161][187/233]	Loss 0.0002 (0.0016)	
training:	Epoch: [161][188/233]	Loss 0.0002 (0.0016)	
training:	Epoch: [161][189/233]	Loss 0.0002 (0.0016)	
training:	Epoch: [161][190/233]	Loss 0.0002 (0.0016)	
training:	Epoch: [161][191/233]	Loss 0.0007 (0.0015)	
training:	Epoch: [161][192/233]	Loss 0.0005 (0.0015)	
training:	Epoch: [161][193/233]	Loss 0.0002 (0.0015)	
training:	Epoch: [161][194/233]	Loss 0.0096 (0.0016)	
training:	Epoch: [161][195/233]	Loss 0.0914 (0.0020)	
training:	Epoch: [161][196/233]	Loss 0.0002 (0.0020)	
training:	Epoch: [161][197/233]	Loss 0.0002 (0.0020)	
training:	Epoch: [161][198/233]	Loss 0.0004 (0.0020)	
training:	Epoch: [161][199/233]	Loss 0.0004 (0.0020)	
training:	Epoch: [161][200/233]	Loss 0.0002 (0.0020)	
training:	Epoch: [161][201/233]	Loss 0.0002 (0.0020)	
training:	Epoch: [161][202/233]	Loss 0.0033 (0.0020)	
training:	Epoch: [161][203/233]	Loss 0.0002 (0.0020)	
training:	Epoch: [161][204/233]	Loss 0.0002 (0.0020)	
training:	Epoch: [161][205/233]	Loss 0.0003 (0.0020)	
training:	Epoch: [161][206/233]	Loss 0.0010 (0.0020)	
training:	Epoch: [161][207/233]	Loss 0.0002 (0.0020)	
training:	Epoch: [161][208/233]	Loss 0.0002 (0.0019)	
training:	Epoch: [161][209/233]	Loss 0.0002 (0.0019)	
training:	Epoch: [161][210/233]	Loss 0.0004 (0.0019)	
training:	Epoch: [161][211/233]	Loss 0.0003 (0.0019)	
training:	Epoch: [161][212/233]	Loss 0.0024 (0.0019)	
training:	Epoch: [161][213/233]	Loss 0.0003 (0.0019)	
training:	Epoch: [161][214/233]	Loss 0.0002 (0.0019)	
training:	Epoch: [161][215/233]	Loss 0.0029 (0.0019)	
training:	Epoch: [161][216/233]	Loss 0.0010 (0.0019)	
training:	Epoch: [161][217/233]	Loss 0.0245 (0.0020)	
training:	Epoch: [161][218/233]	Loss 0.0003 (0.0020)	
training:	Epoch: [161][219/233]	Loss 0.0003 (0.0020)	
training:	Epoch: [161][220/233]	Loss 0.0002 (0.0020)	
training:	Epoch: [161][221/233]	Loss 0.0002 (0.0020)	
training:	Epoch: [161][222/233]	Loss 0.0006 (0.0020)	
training:	Epoch: [161][223/233]	Loss 0.0003 (0.0020)	
training:	Epoch: [161][224/233]	Loss 0.0002 (0.0020)	
training:	Epoch: [161][225/233]	Loss 0.0002 (0.0019)	
training:	Epoch: [161][226/233]	Loss 0.0003 (0.0019)	
training:	Epoch: [161][227/233]	Loss 0.0007 (0.0019)	
training:	Epoch: [161][228/233]	Loss 0.0002 (0.0019)	
training:	Epoch: [161][229/233]	Loss 0.0039 (0.0019)	
training:	Epoch: [161][230/233]	Loss 0.0002 (0.0019)	
training:	Epoch: [161][231/233]	Loss 0.0024 (0.0019)	
training:	Epoch: [161][232/233]	Loss 0.0008 (0.0019)	
training:	Epoch: [161][233/233]	Loss 0.0002 (0.0019)	
Training:	 Loss: 0.0019

Training:	 ACC: 1.0000 1.0000 1.0000 1.0000
Validation:	 ACC: 0.7937 0.7935 0.7886 0.7989
Validation:	 Best_BACC: 0.8162 0.8154 0.7998 0.8326
Validation:	 Loss: 1.3039
Pretraining:	Epoch 162/200
----------
training:	Epoch: [162][1/233]	Loss 0.0003 (0.0003)	
training:	Epoch: [162][2/233]	Loss 0.0002 (0.0003)	
training:	Epoch: [162][3/233]	Loss 0.0004 (0.0003)	
training:	Epoch: [162][4/233]	Loss 0.0002 (0.0003)	
training:	Epoch: [162][5/233]	Loss 0.0048 (0.0012)	
training:	Epoch: [162][6/233]	Loss 0.0003 (0.0010)	
training:	Epoch: [162][7/233]	Loss 0.0002 (0.0009)	
training:	Epoch: [162][8/233]	Loss 0.0147 (0.0027)	
training:	Epoch: [162][9/233]	Loss 0.0003 (0.0024)	
training:	Epoch: [162][10/233]	Loss 0.0002 (0.0022)	
training:	Epoch: [162][11/233]	Loss 0.0003 (0.0020)	
training:	Epoch: [162][12/233]	Loss 0.0002 (0.0018)	
training:	Epoch: [162][13/233]	Loss 0.0002 (0.0017)	
training:	Epoch: [162][14/233]	Loss 0.0003 (0.0016)	
training:	Epoch: [162][15/233]	Loss 0.0003 (0.0015)	
training:	Epoch: [162][16/233]	Loss 0.0005 (0.0015)	
training:	Epoch: [162][17/233]	Loss 0.0002 (0.0014)	
training:	Epoch: [162][18/233]	Loss 0.0002 (0.0013)	
training:	Epoch: [162][19/233]	Loss 0.0003 (0.0013)	
training:	Epoch: [162][20/233]	Loss 0.0002 (0.0012)	
training:	Epoch: [162][21/233]	Loss 0.0003 (0.0012)	
training:	Epoch: [162][22/233]	Loss 0.0002 (0.0011)	
training:	Epoch: [162][23/233]	Loss 0.0004 (0.0011)	
training:	Epoch: [162][24/233]	Loss 0.0004 (0.0011)	
training:	Epoch: [162][25/233]	Loss 0.0002 (0.0010)	
training:	Epoch: [162][26/233]	Loss 0.0002 (0.0010)	
training:	Epoch: [162][27/233]	Loss 0.0003 (0.0010)	
training:	Epoch: [162][28/233]	Loss 0.0005 (0.0010)	
training:	Epoch: [162][29/233]	Loss 0.0003 (0.0009)	
training:	Epoch: [162][30/233]	Loss 0.0265 (0.0018)	
training:	Epoch: [162][31/233]	Loss 0.0002 (0.0017)	
training:	Epoch: [162][32/233]	Loss 0.0013 (0.0017)	
training:	Epoch: [162][33/233]	Loss 0.0003 (0.0017)	
training:	Epoch: [162][34/233]	Loss 0.0004 (0.0016)	
training:	Epoch: [162][35/233]	Loss 0.0002 (0.0016)	
training:	Epoch: [162][36/233]	Loss 0.0057 (0.0017)	
training:	Epoch: [162][37/233]	Loss 0.0012 (0.0017)	
training:	Epoch: [162][38/233]	Loss 0.0002 (0.0017)	
training:	Epoch: [162][39/233]	Loss 0.0003 (0.0016)	
training:	Epoch: [162][40/233]	Loss 0.0004 (0.0016)	
training:	Epoch: [162][41/233]	Loss 0.0002 (0.0016)	
training:	Epoch: [162][42/233]	Loss 0.0028 (0.0016)	
training:	Epoch: [162][43/233]	Loss 0.0005 (0.0016)	
training:	Epoch: [162][44/233]	Loss 0.0014 (0.0016)	
training:	Epoch: [162][45/233]	Loss 0.0002 (0.0015)	
training:	Epoch: [162][46/233]	Loss 0.0002 (0.0015)	
training:	Epoch: [162][47/233]	Loss 0.0002 (0.0015)	
training:	Epoch: [162][48/233]	Loss 0.0002 (0.0015)	
training:	Epoch: [162][49/233]	Loss 0.0046 (0.0015)	
training:	Epoch: [162][50/233]	Loss 0.0004 (0.0015)	
training:	Epoch: [162][51/233]	Loss 0.0002 (0.0015)	
training:	Epoch: [162][52/233]	Loss 0.0046 (0.0015)	
training:	Epoch: [162][53/233]	Loss 0.0003 (0.0015)	
training:	Epoch: [162][54/233]	Loss 0.0003 (0.0015)	
training:	Epoch: [162][55/233]	Loss 0.0003 (0.0015)	
training:	Epoch: [162][56/233]	Loss 0.0002 (0.0014)	
training:	Epoch: [162][57/233]	Loss 0.0002 (0.0014)	
training:	Epoch: [162][58/233]	Loss 0.0004 (0.0014)	
training:	Epoch: [162][59/233]	Loss 0.0003 (0.0014)	
training:	Epoch: [162][60/233]	Loss 0.0020 (0.0014)	
training:	Epoch: [162][61/233]	Loss 0.0003 (0.0014)	
training:	Epoch: [162][62/233]	Loss 0.0012 (0.0014)	
training:	Epoch: [162][63/233]	Loss 0.0002 (0.0014)	
training:	Epoch: [162][64/233]	Loss 0.0002 (0.0013)	
training:	Epoch: [162][65/233]	Loss 0.0002 (0.0013)	
training:	Epoch: [162][66/233]	Loss 0.0002 (0.0013)	
training:	Epoch: [162][67/233]	Loss 0.0002 (0.0013)	
training:	Epoch: [162][68/233]	Loss 0.0002 (0.0013)	
training:	Epoch: [162][69/233]	Loss 0.0003 (0.0013)	
training:	Epoch: [162][70/233]	Loss 0.0003 (0.0012)	
training:	Epoch: [162][71/233]	Loss 0.0002 (0.0012)	
training:	Epoch: [162][72/233]	Loss 0.0055 (0.0013)	
training:	Epoch: [162][73/233]	Loss 0.0002 (0.0013)	
training:	Epoch: [162][74/233]	Loss 0.0002 (0.0013)	
training:	Epoch: [162][75/233]	Loss 0.0002 (0.0012)	
training:	Epoch: [162][76/233]	Loss 0.0004 (0.0012)	
training:	Epoch: [162][77/233]	Loss 0.0004 (0.0012)	
training:	Epoch: [162][78/233]	Loss 0.0003 (0.0012)	
training:	Epoch: [162][79/233]	Loss 0.0056 (0.0013)	
training:	Epoch: [162][80/233]	Loss 0.0002 (0.0012)	
training:	Epoch: [162][81/233]	Loss 0.0002 (0.0012)	
training:	Epoch: [162][82/233]	Loss 0.0003 (0.0012)	
training:	Epoch: [162][83/233]	Loss 0.0002 (0.0012)	
training:	Epoch: [162][84/233]	Loss 0.0006 (0.0012)	
training:	Epoch: [162][85/233]	Loss 0.0004 (0.0012)	
training:	Epoch: [162][86/233]	Loss 0.0005 (0.0012)	
training:	Epoch: [162][87/233]	Loss 0.0003 (0.0012)	
training:	Epoch: [162][88/233]	Loss 0.0002 (0.0012)	
training:	Epoch: [162][89/233]	Loss 0.0003 (0.0012)	
training:	Epoch: [162][90/233]	Loss 0.0003 (0.0011)	
training:	Epoch: [162][91/233]	Loss 0.0003 (0.0011)	
training:	Epoch: [162][92/233]	Loss 0.0002 (0.0011)	
training:	Epoch: [162][93/233]	Loss 0.0003 (0.0011)	
training:	Epoch: [162][94/233]	Loss 0.0003 (0.0011)	
training:	Epoch: [162][95/233]	Loss 0.0003 (0.0011)	
training:	Epoch: [162][96/233]	Loss 0.0003 (0.0011)	
training:	Epoch: [162][97/233]	Loss 0.0002 (0.0011)	
training:	Epoch: [162][98/233]	Loss 0.0002 (0.0011)	
training:	Epoch: [162][99/233]	Loss 0.0002 (0.0011)	
training:	Epoch: [162][100/233]	Loss 0.0002 (0.0011)	
training:	Epoch: [162][101/233]	Loss 0.0006 (0.0011)	
training:	Epoch: [162][102/233]	Loss 0.0029 (0.0011)	
training:	Epoch: [162][103/233]	Loss 0.0016 (0.0011)	
training:	Epoch: [162][104/233]	Loss 0.0002 (0.0011)	
training:	Epoch: [162][105/233]	Loss 0.0002 (0.0011)	
training:	Epoch: [162][106/233]	Loss 0.0007 (0.0011)	
training:	Epoch: [162][107/233]	Loss 0.0002 (0.0010)	
training:	Epoch: [162][108/233]	Loss 0.0002 (0.0010)	
training:	Epoch: [162][109/233]	Loss 0.0018 (0.0010)	
training:	Epoch: [162][110/233]	Loss 0.0002 (0.0010)	
training:	Epoch: [162][111/233]	Loss 0.0002 (0.0010)	
training:	Epoch: [162][112/233]	Loss 0.0003 (0.0010)	
training:	Epoch: [162][113/233]	Loss 0.0019 (0.0010)	
training:	Epoch: [162][114/233]	Loss 0.0003 (0.0010)	
training:	Epoch: [162][115/233]	Loss 0.0002 (0.0010)	
training:	Epoch: [162][116/233]	Loss 0.0002 (0.0010)	
training:	Epoch: [162][117/233]	Loss 0.0002 (0.0010)	
training:	Epoch: [162][118/233]	Loss 0.0004 (0.0010)	
training:	Epoch: [162][119/233]	Loss 0.0187 (0.0012)	
training:	Epoch: [162][120/233]	Loss 0.0002 (0.0011)	
training:	Epoch: [162][121/233]	Loss 0.0003 (0.0011)	
training:	Epoch: [162][122/233]	Loss 0.0004 (0.0011)	
training:	Epoch: [162][123/233]	Loss 0.0002 (0.0011)	
training:	Epoch: [162][124/233]	Loss 0.0003 (0.0011)	
training:	Epoch: [162][125/233]	Loss 0.0003 (0.0011)	
training:	Epoch: [162][126/233]	Loss 0.0003 (0.0011)	
training:	Epoch: [162][127/233]	Loss 0.0003 (0.0011)	
training:	Epoch: [162][128/233]	Loss 0.0494 (0.0015)	
training:	Epoch: [162][129/233]	Loss 0.0002 (0.0015)	
training:	Epoch: [162][130/233]	Loss 0.0003 (0.0015)	
training:	Epoch: [162][131/233]	Loss 0.0002 (0.0014)	
training:	Epoch: [162][132/233]	Loss 0.0023 (0.0015)	
training:	Epoch: [162][133/233]	Loss 0.0002 (0.0014)	
training:	Epoch: [162][134/233]	Loss 0.0002 (0.0014)	
training:	Epoch: [162][135/233]	Loss 0.0002 (0.0014)	
training:	Epoch: [162][136/233]	Loss 0.1705 (0.0027)	
training:	Epoch: [162][137/233]	Loss 0.0006 (0.0027)	
training:	Epoch: [162][138/233]	Loss 0.0003 (0.0026)	
training:	Epoch: [162][139/233]	Loss 0.0003 (0.0026)	
training:	Epoch: [162][140/233]	Loss 0.0003 (0.0026)	
training:	Epoch: [162][141/233]	Loss 0.0004 (0.0026)	
training:	Epoch: [162][142/233]	Loss 0.0002 (0.0026)	
training:	Epoch: [162][143/233]	Loss 0.0002 (0.0026)	
training:	Epoch: [162][144/233]	Loss 0.0013 (0.0025)	
training:	Epoch: [162][145/233]	Loss 0.0004 (0.0025)	
training:	Epoch: [162][146/233]	Loss 0.0004 (0.0025)	
training:	Epoch: [162][147/233]	Loss 0.0004 (0.0025)	
training:	Epoch: [162][148/233]	Loss 0.0002 (0.0025)	
training:	Epoch: [162][149/233]	Loss 0.0020 (0.0025)	
training:	Epoch: [162][150/233]	Loss 0.0005 (0.0025)	
training:	Epoch: [162][151/233]	Loss 0.0006 (0.0025)	
training:	Epoch: [162][152/233]	Loss 0.0009 (0.0024)	
training:	Epoch: [162][153/233]	Loss 0.0002 (0.0024)	
training:	Epoch: [162][154/233]	Loss 0.0003 (0.0024)	
training:	Epoch: [162][155/233]	Loss 0.0009 (0.0024)	
training:	Epoch: [162][156/233]	Loss 0.0003 (0.0024)	
training:	Epoch: [162][157/233]	Loss 0.2310 (0.0039)	
training:	Epoch: [162][158/233]	Loss 0.0003 (0.0038)	
training:	Epoch: [162][159/233]	Loss 0.0004 (0.0038)	
training:	Epoch: [162][160/233]	Loss 0.0002 (0.0038)	
training:	Epoch: [162][161/233]	Loss 0.0003 (0.0038)	
training:	Epoch: [162][162/233]	Loss 0.0064 (0.0038)	
training:	Epoch: [162][163/233]	Loss 0.0003 (0.0038)	
training:	Epoch: [162][164/233]	Loss 0.0011 (0.0037)	
training:	Epoch: [162][165/233]	Loss 0.0048 (0.0037)	
training:	Epoch: [162][166/233]	Loss 0.0002 (0.0037)	
training:	Epoch: [162][167/233]	Loss 0.0002 (0.0037)	
training:	Epoch: [162][168/233]	Loss 0.0005 (0.0037)	
training:	Epoch: [162][169/233]	Loss 0.0012 (0.0037)	
training:	Epoch: [162][170/233]	Loss 0.0129 (0.0037)	
training:	Epoch: [162][171/233]	Loss 0.0002 (0.0037)	
training:	Epoch: [162][172/233]	Loss 0.0002 (0.0037)	
training:	Epoch: [162][173/233]	Loss 0.0004 (0.0037)	
training:	Epoch: [162][174/233]	Loss 0.0002 (0.0036)	
training:	Epoch: [162][175/233]	Loss 0.0010 (0.0036)	
training:	Epoch: [162][176/233]	Loss 0.0002 (0.0036)	
training:	Epoch: [162][177/233]	Loss 0.0002 (0.0036)	
training:	Epoch: [162][178/233]	Loss 0.0002 (0.0036)	
training:	Epoch: [162][179/233]	Loss 0.0008 (0.0036)	
training:	Epoch: [162][180/233]	Loss 0.0002 (0.0035)	
training:	Epoch: [162][181/233]	Loss 0.0004 (0.0035)	
training:	Epoch: [162][182/233]	Loss 0.0003 (0.0035)	
training:	Epoch: [162][183/233]	Loss 0.0002 (0.0035)	
training:	Epoch: [162][184/233]	Loss 0.0002 (0.0035)	
training:	Epoch: [162][185/233]	Loss 0.0003 (0.0035)	
training:	Epoch: [162][186/233]	Loss 0.0002 (0.0034)	
training:	Epoch: [162][187/233]	Loss 0.0002 (0.0034)	
training:	Epoch: [162][188/233]	Loss 0.0002 (0.0034)	
training:	Epoch: [162][189/233]	Loss 0.0003 (0.0034)	
training:	Epoch: [162][190/233]	Loss 0.0004 (0.0034)	
training:	Epoch: [162][191/233]	Loss 0.0002 (0.0034)	
training:	Epoch: [162][192/233]	Loss 0.0004 (0.0033)	
training:	Epoch: [162][193/233]	Loss 0.0002 (0.0033)	
training:	Epoch: [162][194/233]	Loss 0.0002 (0.0033)	
training:	Epoch: [162][195/233]	Loss 0.0002 (0.0033)	
training:	Epoch: [162][196/233]	Loss 0.0003 (0.0033)	
training:	Epoch: [162][197/233]	Loss 0.0002 (0.0033)	
training:	Epoch: [162][198/233]	Loss 0.0002 (0.0032)	
training:	Epoch: [162][199/233]	Loss 0.0006 (0.0032)	
training:	Epoch: [162][200/233]	Loss 0.0002 (0.0032)	
training:	Epoch: [162][201/233]	Loss 0.0003 (0.0032)	
training:	Epoch: [162][202/233]	Loss 0.0002 (0.0032)	
training:	Epoch: [162][203/233]	Loss 0.0002 (0.0032)	
training:	Epoch: [162][204/233]	Loss 0.0003 (0.0032)	
training:	Epoch: [162][205/233]	Loss 0.0002 (0.0031)	
training:	Epoch: [162][206/233]	Loss 0.0011 (0.0031)	
training:	Epoch: [162][207/233]	Loss 0.0004 (0.0031)	
training:	Epoch: [162][208/233]	Loss 0.0002 (0.0031)	
training:	Epoch: [162][209/233]	Loss 0.0004 (0.0031)	
training:	Epoch: [162][210/233]	Loss 0.0003 (0.0031)	
training:	Epoch: [162][211/233]	Loss 0.0025 (0.0031)	
training:	Epoch: [162][212/233]	Loss 0.0002 (0.0031)	
training:	Epoch: [162][213/233]	Loss 0.0004 (0.0030)	
training:	Epoch: [162][214/233]	Loss 0.0002 (0.0030)	
training:	Epoch: [162][215/233]	Loss 0.0002 (0.0030)	
training:	Epoch: [162][216/233]	Loss 0.0002 (0.0030)	
training:	Epoch: [162][217/233]	Loss 0.0002 (0.0030)	
training:	Epoch: [162][218/233]	Loss 0.0002 (0.0030)	
training:	Epoch: [162][219/233]	Loss 0.0002 (0.0030)	
training:	Epoch: [162][220/233]	Loss 0.0076 (0.0030)	
training:	Epoch: [162][221/233]	Loss 0.0002 (0.0030)	
training:	Epoch: [162][222/233]	Loss 0.0002 (0.0030)	
training:	Epoch: [162][223/233]	Loss 0.0003 (0.0030)	
training:	Epoch: [162][224/233]	Loss 0.0002 (0.0029)	
training:	Epoch: [162][225/233]	Loss 0.0003 (0.0029)	
training:	Epoch: [162][226/233]	Loss 0.0004 (0.0029)	
training:	Epoch: [162][227/233]	Loss 0.0002 (0.0029)	
training:	Epoch: [162][228/233]	Loss 0.0002 (0.0029)	
training:	Epoch: [162][229/233]	Loss 0.0005 (0.0029)	
training:	Epoch: [162][230/233]	Loss 0.0002 (0.0029)	
training:	Epoch: [162][231/233]	Loss 0.0002 (0.0029)	
training:	Epoch: [162][232/233]	Loss 0.0002 (0.0029)	
training:	Epoch: [162][233/233]	Loss 0.0002 (0.0028)	
Training:	 Loss: 0.0028

Training:	 ACC: 1.0000 1.0000 1.0000 1.0000
Validation:	 ACC: 0.7933 0.7940 0.8080 0.7787
Validation:	 Best_BACC: 0.8162 0.8154 0.7998 0.8326
Validation:	 Loss: 1.3435
Pretraining:	Epoch 163/200
----------
training:	Epoch: [163][1/233]	Loss 0.0002 (0.0002)	
training:	Epoch: [163][2/233]	Loss 0.0002 (0.0002)	
training:	Epoch: [163][3/233]	Loss 0.0002 (0.0002)	
training:	Epoch: [163][4/233]	Loss 0.0014 (0.0005)	
training:	Epoch: [163][5/233]	Loss 0.0002 (0.0005)	
training:	Epoch: [163][6/233]	Loss 0.0003 (0.0004)	
training:	Epoch: [163][7/233]	Loss 0.0003 (0.0004)	
training:	Epoch: [163][8/233]	Loss 0.0002 (0.0004)	
training:	Epoch: [163][9/233]	Loss 0.0002 (0.0004)	
training:	Epoch: [163][10/233]	Loss 0.0003 (0.0003)	
training:	Epoch: [163][11/233]	Loss 0.0002 (0.0003)	
training:	Epoch: [163][12/233]	Loss 0.0002 (0.0003)	
training:	Epoch: [163][13/233]	Loss 0.0002 (0.0003)	
training:	Epoch: [163][14/233]	Loss 0.0003 (0.0003)	
training:	Epoch: [163][15/233]	Loss 0.0002 (0.0003)	
training:	Epoch: [163][16/233]	Loss 0.0003 (0.0003)	
training:	Epoch: [163][17/233]	Loss 0.0002 (0.0003)	
training:	Epoch: [163][18/233]	Loss 0.0014 (0.0004)	
training:	Epoch: [163][19/233]	Loss 0.0002 (0.0004)	
training:	Epoch: [163][20/233]	Loss 0.0002 (0.0003)	
training:	Epoch: [163][21/233]	Loss 0.0002 (0.0003)	
training:	Epoch: [163][22/233]	Loss 0.0022 (0.0004)	
training:	Epoch: [163][23/233]	Loss 0.0005 (0.0004)	
training:	Epoch: [163][24/233]	Loss 0.0002 (0.0004)	
training:	Epoch: [163][25/233]	Loss 0.0003 (0.0004)	
training:	Epoch: [163][26/233]	Loss 0.0002 (0.0004)	
training:	Epoch: [163][27/233]	Loss 0.0002 (0.0004)	
training:	Epoch: [163][28/233]	Loss 0.0002 (0.0004)	
training:	Epoch: [163][29/233]	Loss 0.0003 (0.0004)	
training:	Epoch: [163][30/233]	Loss 0.0239 (0.0012)	
training:	Epoch: [163][31/233]	Loss 0.0002 (0.0011)	
training:	Epoch: [163][32/233]	Loss 0.0006 (0.0011)	
training:	Epoch: [163][33/233]	Loss 0.0002 (0.0011)	
training:	Epoch: [163][34/233]	Loss 0.0003 (0.0011)	
training:	Epoch: [163][35/233]	Loss 0.0002 (0.0011)	
training:	Epoch: [163][36/233]	Loss 0.0002 (0.0010)	
training:	Epoch: [163][37/233]	Loss 0.0003 (0.0010)	
training:	Epoch: [163][38/233]	Loss 0.0002 (0.0010)	
training:	Epoch: [163][39/233]	Loss 0.0002 (0.0010)	
training:	Epoch: [163][40/233]	Loss 0.0020 (0.0010)	
training:	Epoch: [163][41/233]	Loss 0.0002 (0.0010)	
training:	Epoch: [163][42/233]	Loss 0.0003 (0.0010)	
training:	Epoch: [163][43/233]	Loss 0.0002 (0.0009)	
training:	Epoch: [163][44/233]	Loss 0.0002 (0.0009)	
training:	Epoch: [163][45/233]	Loss 0.0003 (0.0009)	
training:	Epoch: [163][46/233]	Loss 0.0002 (0.0009)	
training:	Epoch: [163][47/233]	Loss 0.0002 (0.0009)	
training:	Epoch: [163][48/233]	Loss 0.0002 (0.0009)	
training:	Epoch: [163][49/233]	Loss 0.0002 (0.0009)	
training:	Epoch: [163][50/233]	Loss 0.0002 (0.0008)	
training:	Epoch: [163][51/233]	Loss 0.0002 (0.0008)	
training:	Epoch: [163][52/233]	Loss 0.0005 (0.0008)	
training:	Epoch: [163][53/233]	Loss 0.0002 (0.0008)	
training:	Epoch: [163][54/233]	Loss 0.0008 (0.0008)	
training:	Epoch: [163][55/233]	Loss 0.0002 (0.0008)	
training:	Epoch: [163][56/233]	Loss 0.0002 (0.0008)	
training:	Epoch: [163][57/233]	Loss 0.0002 (0.0008)	
training:	Epoch: [163][58/233]	Loss 0.0002 (0.0008)	
training:	Epoch: [163][59/233]	Loss 0.2061 (0.0042)	
training:	Epoch: [163][60/233]	Loss 0.0305 (0.0047)	
training:	Epoch: [163][61/233]	Loss 0.0002 (0.0046)	
training:	Epoch: [163][62/233]	Loss 0.0339 (0.0051)	
training:	Epoch: [163][63/233]	Loss 0.0002 (0.0050)	
training:	Epoch: [163][64/233]	Loss 0.0003 (0.0049)	
training:	Epoch: [163][65/233]	Loss 0.0004 (0.0049)	
training:	Epoch: [163][66/233]	Loss 0.0042 (0.0049)	
training:	Epoch: [163][67/233]	Loss 0.0003 (0.0048)	
training:	Epoch: [163][68/233]	Loss 0.0005 (0.0047)	
training:	Epoch: [163][69/233]	Loss 0.0007 (0.0047)	
training:	Epoch: [163][70/233]	Loss 0.0003 (0.0046)	
training:	Epoch: [163][71/233]	Loss 0.0002 (0.0045)	
training:	Epoch: [163][72/233]	Loss 0.0002 (0.0045)	
training:	Epoch: [163][73/233]	Loss 0.0009 (0.0044)	
training:	Epoch: [163][74/233]	Loss 0.0070 (0.0045)	
training:	Epoch: [163][75/233]	Loss 0.0119 (0.0046)	
training:	Epoch: [163][76/233]	Loss 0.0002 (0.0045)	
training:	Epoch: [163][77/233]	Loss 0.0005 (0.0045)	
training:	Epoch: [163][78/233]	Loss 0.0010 (0.0044)	
training:	Epoch: [163][79/233]	Loss 0.0005 (0.0044)	
training:	Epoch: [163][80/233]	Loss 0.0002 (0.0043)	
training:	Epoch: [163][81/233]	Loss 0.0004 (0.0043)	
training:	Epoch: [163][82/233]	Loss 0.0005 (0.0042)	
training:	Epoch: [163][83/233]	Loss 0.0002 (0.0042)	
training:	Epoch: [163][84/233]	Loss 0.0003 (0.0041)	
training:	Epoch: [163][85/233]	Loss 0.0002 (0.0041)	
training:	Epoch: [163][86/233]	Loss 0.0002 (0.0040)	
training:	Epoch: [163][87/233]	Loss 0.0006 (0.0040)	
training:	Epoch: [163][88/233]	Loss 0.0003 (0.0039)	
training:	Epoch: [163][89/233]	Loss 0.0002 (0.0039)	
training:	Epoch: [163][90/233]	Loss 0.0051 (0.0039)	
training:	Epoch: [163][91/233]	Loss 0.0002 (0.0039)	
training:	Epoch: [163][92/233]	Loss 0.0021 (0.0039)	
training:	Epoch: [163][93/233]	Loss 0.0004 (0.0038)	
training:	Epoch: [163][94/233]	Loss 0.0002 (0.0038)	
training:	Epoch: [163][95/233]	Loss 0.0002 (0.0037)	
training:	Epoch: [163][96/233]	Loss 0.0003 (0.0037)	
training:	Epoch: [163][97/233]	Loss 0.0002 (0.0037)	
training:	Epoch: [163][98/233]	Loss 0.0004 (0.0036)	
training:	Epoch: [163][99/233]	Loss 0.0003 (0.0036)	
training:	Epoch: [163][100/233]	Loss 0.0002 (0.0036)	
training:	Epoch: [163][101/233]	Loss 0.0003 (0.0035)	
training:	Epoch: [163][102/233]	Loss 0.0002 (0.0035)	
training:	Epoch: [163][103/233]	Loss 0.0003 (0.0035)	
training:	Epoch: [163][104/233]	Loss 0.0018 (0.0035)	
training:	Epoch: [163][105/233]	Loss 0.0004 (0.0034)	
training:	Epoch: [163][106/233]	Loss 0.0003 (0.0034)	
training:	Epoch: [163][107/233]	Loss 0.0002 (0.0034)	
training:	Epoch: [163][108/233]	Loss 0.0006 (0.0033)	
training:	Epoch: [163][109/233]	Loss 0.0002 (0.0033)	
training:	Epoch: [163][110/233]	Loss 0.0005 (0.0033)	
training:	Epoch: [163][111/233]	Loss 0.0002 (0.0033)	
training:	Epoch: [163][112/233]	Loss 0.0003 (0.0032)	
training:	Epoch: [163][113/233]	Loss 0.0009 (0.0032)	
training:	Epoch: [163][114/233]	Loss 0.0002 (0.0032)	
training:	Epoch: [163][115/233]	Loss 0.0003 (0.0032)	
training:	Epoch: [163][116/233]	Loss 0.0016 (0.0032)	
training:	Epoch: [163][117/233]	Loss 0.0003 (0.0031)	
training:	Epoch: [163][118/233]	Loss 0.0003 (0.0031)	
training:	Epoch: [163][119/233]	Loss 0.0002 (0.0031)	
training:	Epoch: [163][120/233]	Loss 0.0003 (0.0031)	
training:	Epoch: [163][121/233]	Loss 0.0003 (0.0030)	
training:	Epoch: [163][122/233]	Loss 0.0002 (0.0030)	
training:	Epoch: [163][123/233]	Loss 0.0006 (0.0030)	
training:	Epoch: [163][124/233]	Loss 0.0005 (0.0030)	
training:	Epoch: [163][125/233]	Loss 0.0003 (0.0029)	
training:	Epoch: [163][126/233]	Loss 0.0002 (0.0029)	
training:	Epoch: [163][127/233]	Loss 0.0002 (0.0029)	
training:	Epoch: [163][128/233]	Loss 0.0002 (0.0029)	
training:	Epoch: [163][129/233]	Loss 0.0005 (0.0029)	
training:	Epoch: [163][130/233]	Loss 0.0002 (0.0028)	
training:	Epoch: [163][131/233]	Loss 0.0002 (0.0028)	
training:	Epoch: [163][132/233]	Loss 0.0002 (0.0028)	
training:	Epoch: [163][133/233]	Loss 0.0022 (0.0028)	
training:	Epoch: [163][134/233]	Loss 0.0192 (0.0029)	
training:	Epoch: [163][135/233]	Loss 0.0166 (0.0030)	
training:	Epoch: [163][136/233]	Loss 0.0003 (0.0030)	
training:	Epoch: [163][137/233]	Loss 0.0004 (0.0030)	
training:	Epoch: [163][138/233]	Loss 0.0003 (0.0030)	
training:	Epoch: [163][139/233]	Loss 0.0002 (0.0029)	
training:	Epoch: [163][140/233]	Loss 0.0002 (0.0029)	
training:	Epoch: [163][141/233]	Loss 0.0002 (0.0029)	
training:	Epoch: [163][142/233]	Loss 0.0003 (0.0029)	
training:	Epoch: [163][143/233]	Loss 0.0004 (0.0029)	
training:	Epoch: [163][144/233]	Loss 0.0002 (0.0029)	
training:	Epoch: [163][145/233]	Loss 0.0002 (0.0028)	
training:	Epoch: [163][146/233]	Loss 0.0296 (0.0030)	
training:	Epoch: [163][147/233]	Loss 0.0005 (0.0030)	
training:	Epoch: [163][148/233]	Loss 0.0002 (0.0030)	
training:	Epoch: [163][149/233]	Loss 0.0002 (0.0030)	
training:	Epoch: [163][150/233]	Loss 0.0003 (0.0029)	
training:	Epoch: [163][151/233]	Loss 0.0002 (0.0029)	
training:	Epoch: [163][152/233]	Loss 0.0002 (0.0029)	
training:	Epoch: [163][153/233]	Loss 0.0024 (0.0029)	
training:	Epoch: [163][154/233]	Loss 0.0002 (0.0029)	
training:	Epoch: [163][155/233]	Loss 0.0002 (0.0029)	
training:	Epoch: [163][156/233]	Loss 0.0003 (0.0029)	
training:	Epoch: [163][157/233]	Loss 0.0003 (0.0028)	
training:	Epoch: [163][158/233]	Loss 0.0002 (0.0028)	
training:	Epoch: [163][159/233]	Loss 0.0002 (0.0028)	
training:	Epoch: [163][160/233]	Loss 0.0035 (0.0028)	
training:	Epoch: [163][161/233]	Loss 0.0002 (0.0028)	
training:	Epoch: [163][162/233]	Loss 0.0002 (0.0028)	
training:	Epoch: [163][163/233]	Loss 0.0003 (0.0028)	
training:	Epoch: [163][164/233]	Loss 0.0009 (0.0028)	
training:	Epoch: [163][165/233]	Loss 0.0003 (0.0027)	
training:	Epoch: [163][166/233]	Loss 0.1535 (0.0036)	
training:	Epoch: [163][167/233]	Loss 0.0008 (0.0036)	
training:	Epoch: [163][168/233]	Loss 0.0003 (0.0036)	
training:	Epoch: [163][169/233]	Loss 0.0003 (0.0036)	
training:	Epoch: [163][170/233]	Loss 0.0004 (0.0036)	
training:	Epoch: [163][171/233]	Loss 0.0015 (0.0036)	
training:	Epoch: [163][172/233]	Loss 0.0070 (0.0036)	
training:	Epoch: [163][173/233]	Loss 0.0002 (0.0036)	
training:	Epoch: [163][174/233]	Loss 0.0006 (0.0035)	
training:	Epoch: [163][175/233]	Loss 0.0002 (0.0035)	
training:	Epoch: [163][176/233]	Loss 0.0005 (0.0035)	
training:	Epoch: [163][177/233]	Loss 0.0002 (0.0035)	
training:	Epoch: [163][178/233]	Loss 0.0019 (0.0035)	
training:	Epoch: [163][179/233]	Loss 0.0003 (0.0035)	
training:	Epoch: [163][180/233]	Loss 0.0002 (0.0034)	
training:	Epoch: [163][181/233]	Loss 0.0002 (0.0034)	
training:	Epoch: [163][182/233]	Loss 0.0003 (0.0034)	
training:	Epoch: [163][183/233]	Loss 0.0003 (0.0034)	
training:	Epoch: [163][184/233]	Loss 0.0002 (0.0034)	
training:	Epoch: [163][185/233]	Loss 0.0003 (0.0034)	
training:	Epoch: [163][186/233]	Loss 0.0002 (0.0033)	
training:	Epoch: [163][187/233]	Loss 0.0002 (0.0033)	
training:	Epoch: [163][188/233]	Loss 0.0003 (0.0033)	
training:	Epoch: [163][189/233]	Loss 0.0002 (0.0033)	
training:	Epoch: [163][190/233]	Loss 0.0002 (0.0033)	
training:	Epoch: [163][191/233]	Loss 0.0005 (0.0033)	
training:	Epoch: [163][192/233]	Loss 0.0017 (0.0032)	
training:	Epoch: [163][193/233]	Loss 0.0002 (0.0032)	
training:	Epoch: [163][194/233]	Loss 0.0003 (0.0032)	
training:	Epoch: [163][195/233]	Loss 0.0002 (0.0032)	
training:	Epoch: [163][196/233]	Loss 0.0006 (0.0032)	
training:	Epoch: [163][197/233]	Loss 0.0160 (0.0033)	
training:	Epoch: [163][198/233]	Loss 0.0002 (0.0032)	
training:	Epoch: [163][199/233]	Loss 0.0002 (0.0032)	
training:	Epoch: [163][200/233]	Loss 0.0003 (0.0032)	
training:	Epoch: [163][201/233]	Loss 0.0003 (0.0032)	
training:	Epoch: [163][202/233]	Loss 0.0002 (0.0032)	
training:	Epoch: [163][203/233]	Loss 0.0002 (0.0032)	
training:	Epoch: [163][204/233]	Loss 0.0003 (0.0032)	
training:	Epoch: [163][205/233]	Loss 0.0005 (0.0031)	
training:	Epoch: [163][206/233]	Loss 0.0003 (0.0031)	
training:	Epoch: [163][207/233]	Loss 0.0002 (0.0031)	
training:	Epoch: [163][208/233]	Loss 0.0002 (0.0031)	
training:	Epoch: [163][209/233]	Loss 0.0003 (0.0031)	
training:	Epoch: [163][210/233]	Loss 0.0032 (0.0031)	
training:	Epoch: [163][211/233]	Loss 0.0005 (0.0031)	
training:	Epoch: [163][212/233]	Loss 0.0004 (0.0031)	
training:	Epoch: [163][213/233]	Loss 0.0014 (0.0031)	
training:	Epoch: [163][214/233]	Loss 0.0002 (0.0030)	
training:	Epoch: [163][215/233]	Loss 0.0002 (0.0030)	
training:	Epoch: [163][216/233]	Loss 0.0002 (0.0030)	
training:	Epoch: [163][217/233]	Loss 0.0004 (0.0030)	
training:	Epoch: [163][218/233]	Loss 0.0014 (0.0030)	
training:	Epoch: [163][219/233]	Loss 0.0002 (0.0030)	
training:	Epoch: [163][220/233]	Loss 0.0002 (0.0030)	
training:	Epoch: [163][221/233]	Loss 0.0002 (0.0030)	
training:	Epoch: [163][222/233]	Loss 0.0002 (0.0029)	
training:	Epoch: [163][223/233]	Loss 0.0003 (0.0029)	
training:	Epoch: [163][224/233]	Loss 0.0003 (0.0029)	
training:	Epoch: [163][225/233]	Loss 0.0003 (0.0029)	
training:	Epoch: [163][226/233]	Loss 0.0002 (0.0029)	
training:	Epoch: [163][227/233]	Loss 0.0015 (0.0029)	
training:	Epoch: [163][228/233]	Loss 0.0004 (0.0029)	
training:	Epoch: [163][229/233]	Loss 0.0002 (0.0029)	
training:	Epoch: [163][230/233]	Loss 0.0002 (0.0029)	
training:	Epoch: [163][231/233]	Loss 0.0003 (0.0028)	
training:	Epoch: [163][232/233]	Loss 0.0002 (0.0028)	
training:	Epoch: [163][233/233]	Loss 0.0007 (0.0028)	
Training:	 Loss: 0.0028

Training:	 ACC: 1.0000 1.0000 1.0000 1.0000
Validation:	 ACC: 0.7896 0.7887 0.7692 0.8101
Validation:	 Best_BACC: 0.8162 0.8154 0.7998 0.8326
Validation:	 Loss: 1.3540
Pretraining:	Epoch 164/200
----------
training:	Epoch: [164][1/233]	Loss 0.0002 (0.0002)	
training:	Epoch: [164][2/233]	Loss 0.0002 (0.0002)	
training:	Epoch: [164][3/233]	Loss 0.0002 (0.0002)	
training:	Epoch: [164][4/233]	Loss 0.0003 (0.0002)	
training:	Epoch: [164][5/233]	Loss 0.0713 (0.0145)	
training:	Epoch: [164][6/233]	Loss 0.0502 (0.0204)	
training:	Epoch: [164][7/233]	Loss 0.0002 (0.0175)	
training:	Epoch: [164][8/233]	Loss 0.0002 (0.0154)	
training:	Epoch: [164][9/233]	Loss 0.0010 (0.0138)	
training:	Epoch: [164][10/233]	Loss 0.0002 (0.0124)	
training:	Epoch: [164][11/233]	Loss 0.0011 (0.0114)	
training:	Epoch: [164][12/233]	Loss 0.0004 (0.0105)	
training:	Epoch: [164][13/233]	Loss 0.0003 (0.0097)	
training:	Epoch: [164][14/233]	Loss 0.0002 (0.0090)	
training:	Epoch: [164][15/233]	Loss 0.0028 (0.0086)	
training:	Epoch: [164][16/233]	Loss 0.0002 (0.0081)	
training:	Epoch: [164][17/233]	Loss 0.0003 (0.0076)	
training:	Epoch: [164][18/233]	Loss 0.0002 (0.0072)	
training:	Epoch: [164][19/233]	Loss 0.0008 (0.0069)	
training:	Epoch: [164][20/233]	Loss 0.0003 (0.0065)	
training:	Epoch: [164][21/233]	Loss 0.0002 (0.0062)	
training:	Epoch: [164][22/233]	Loss 0.0003 (0.0060)	
training:	Epoch: [164][23/233]	Loss 0.1208 (0.0110)	
training:	Epoch: [164][24/233]	Loss 0.0003 (0.0105)	
training:	Epoch: [164][25/233]	Loss 0.0003 (0.0101)	
training:	Epoch: [164][26/233]	Loss 0.0003 (0.0097)	
training:	Epoch: [164][27/233]	Loss 0.0587 (0.0115)	
training:	Epoch: [164][28/233]	Loss 0.0004 (0.0111)	
training:	Epoch: [164][29/233]	Loss 0.0006 (0.0108)	
training:	Epoch: [164][30/233]	Loss 0.0007 (0.0104)	
training:	Epoch: [164][31/233]	Loss 0.0002 (0.0101)	
training:	Epoch: [164][32/233]	Loss 0.0004 (0.0098)	
training:	Epoch: [164][33/233]	Loss 0.0196 (0.0101)	
training:	Epoch: [164][34/233]	Loss 0.0006 (0.0098)	
training:	Epoch: [164][35/233]	Loss 0.0058 (0.0097)	
training:	Epoch: [164][36/233]	Loss 0.0002 (0.0094)	
training:	Epoch: [164][37/233]	Loss 0.0006 (0.0092)	
training:	Epoch: [164][38/233]	Loss 0.0013 (0.0090)	
training:	Epoch: [164][39/233]	Loss 0.0002 (0.0088)	
training:	Epoch: [164][40/233]	Loss 0.0016 (0.0086)	
training:	Epoch: [164][41/233]	Loss 0.0002 (0.0084)	
training:	Epoch: [164][42/233]	Loss 0.0019 (0.0082)	
training:	Epoch: [164][43/233]	Loss 0.0009 (0.0081)	
training:	Epoch: [164][44/233]	Loss 0.0003 (0.0079)	
training:	Epoch: [164][45/233]	Loss 0.0004 (0.0077)	
training:	Epoch: [164][46/233]	Loss 0.0002 (0.0076)	
training:	Epoch: [164][47/233]	Loss 0.0004 (0.0074)	
training:	Epoch: [164][48/233]	Loss 0.0007 (0.0073)	
training:	Epoch: [164][49/233]	Loss 0.0002 (0.0071)	
training:	Epoch: [164][50/233]	Loss 0.0429 (0.0078)	
training:	Epoch: [164][51/233]	Loss 0.0020 (0.0077)	
training:	Epoch: [164][52/233]	Loss 0.0010 (0.0076)	
training:	Epoch: [164][53/233]	Loss 0.0002 (0.0075)	
training:	Epoch: [164][54/233]	Loss 0.0002 (0.0073)	
training:	Epoch: [164][55/233]	Loss 0.0002 (0.0072)	
training:	Epoch: [164][56/233]	Loss 0.0004 (0.0071)	
training:	Epoch: [164][57/233]	Loss 0.0008 (0.0070)	
training:	Epoch: [164][58/233]	Loss 0.0004 (0.0068)	
training:	Epoch: [164][59/233]	Loss 0.0003 (0.0067)	
training:	Epoch: [164][60/233]	Loss 0.0004 (0.0066)	
training:	Epoch: [164][61/233]	Loss 0.0002 (0.0065)	
training:	Epoch: [164][62/233]	Loss 0.0032 (0.0065)	
training:	Epoch: [164][63/233]	Loss 0.0015 (0.0064)	
training:	Epoch: [164][64/233]	Loss 0.1826 (0.0091)	
training:	Epoch: [164][65/233]	Loss 0.0003 (0.0090)	
training:	Epoch: [164][66/233]	Loss 0.0003 (0.0089)	
training:	Epoch: [164][67/233]	Loss 0.0002 (0.0087)	
training:	Epoch: [164][68/233]	Loss 0.0002 (0.0086)	
training:	Epoch: [164][69/233]	Loss 0.0009 (0.0085)	
training:	Epoch: [164][70/233]	Loss 0.0006 (0.0084)	
training:	Epoch: [164][71/233]	Loss 0.0002 (0.0083)	
training:	Epoch: [164][72/233]	Loss 0.0002 (0.0082)	
training:	Epoch: [164][73/233]	Loss 0.0002 (0.0081)	
training:	Epoch: [164][74/233]	Loss 0.0004 (0.0080)	
training:	Epoch: [164][75/233]	Loss 0.0020 (0.0079)	
training:	Epoch: [164][76/233]	Loss 0.0002 (0.0078)	
training:	Epoch: [164][77/233]	Loss 0.0061 (0.0078)	
training:	Epoch: [164][78/233]	Loss 0.0003 (0.0077)	
training:	Epoch: [164][79/233]	Loss 0.0003 (0.0076)	
training:	Epoch: [164][80/233]	Loss 0.0002 (0.0075)	
training:	Epoch: [164][81/233]	Loss 0.0002 (0.0074)	
training:	Epoch: [164][82/233]	Loss 0.0115 (0.0074)	
training:	Epoch: [164][83/233]	Loss 0.0003 (0.0073)	
training:	Epoch: [164][84/233]	Loss 0.0004 (0.0073)	
training:	Epoch: [164][85/233]	Loss 0.0008 (0.0072)	
training:	Epoch: [164][86/233]	Loss 0.0003 (0.0071)	
training:	Epoch: [164][87/233]	Loss 0.0004 (0.0070)	
training:	Epoch: [164][88/233]	Loss 0.0002 (0.0070)	
training:	Epoch: [164][89/233]	Loss 0.0003 (0.0069)	
training:	Epoch: [164][90/233]	Loss 0.0003 (0.0068)	
training:	Epoch: [164][91/233]	Loss 0.0015 (0.0067)	
training:	Epoch: [164][92/233]	Loss 0.0003 (0.0067)	
training:	Epoch: [164][93/233]	Loss 0.0008 (0.0066)	
training:	Epoch: [164][94/233]	Loss 0.0003 (0.0065)	
training:	Epoch: [164][95/233]	Loss 0.0010 (0.0065)	
training:	Epoch: [164][96/233]	Loss 0.0006 (0.0064)	
training:	Epoch: [164][97/233]	Loss 0.0003 (0.0064)	
training:	Epoch: [164][98/233]	Loss 0.0005 (0.0063)	
training:	Epoch: [164][99/233]	Loss 0.0004 (0.0062)	
training:	Epoch: [164][100/233]	Loss 0.0002 (0.0062)	
training:	Epoch: [164][101/233]	Loss 0.0004 (0.0061)	
training:	Epoch: [164][102/233]	Loss 0.0004 (0.0061)	
training:	Epoch: [164][103/233]	Loss 0.0002 (0.0060)	
training:	Epoch: [164][104/233]	Loss 0.0003 (0.0060)	
training:	Epoch: [164][105/233]	Loss 0.0002 (0.0059)	
training:	Epoch: [164][106/233]	Loss 0.0078 (0.0059)	
training:	Epoch: [164][107/233]	Loss 0.0002 (0.0059)	
training:	Epoch: [164][108/233]	Loss 0.0002 (0.0058)	
training:	Epoch: [164][109/233]	Loss 0.0004 (0.0058)	
training:	Epoch: [164][110/233]	Loss 0.0003 (0.0057)	
training:	Epoch: [164][111/233]	Loss 0.0003 (0.0057)	
training:	Epoch: [164][112/233]	Loss 0.0003 (0.0056)	
training:	Epoch: [164][113/233]	Loss 0.0239 (0.0058)	
training:	Epoch: [164][114/233]	Loss 0.0016 (0.0057)	
training:	Epoch: [164][115/233]	Loss 0.0002 (0.0057)	
training:	Epoch: [164][116/233]	Loss 0.0067 (0.0057)	
training:	Epoch: [164][117/233]	Loss 0.0008 (0.0057)	
training:	Epoch: [164][118/233]	Loss 0.0002 (0.0056)	
training:	Epoch: [164][119/233]	Loss 0.0002 (0.0056)	
training:	Epoch: [164][120/233]	Loss 0.0003 (0.0055)	
training:	Epoch: [164][121/233]	Loss 0.0002 (0.0055)	
training:	Epoch: [164][122/233]	Loss 0.0002 (0.0054)	
training:	Epoch: [164][123/233]	Loss 0.0002 (0.0054)	
training:	Epoch: [164][124/233]	Loss 0.0057 (0.0054)	
training:	Epoch: [164][125/233]	Loss 0.0002 (0.0054)	
training:	Epoch: [164][126/233]	Loss 0.0004 (0.0053)	
training:	Epoch: [164][127/233]	Loss 0.0002 (0.0053)	
training:	Epoch: [164][128/233]	Loss 0.0002 (0.0052)	
training:	Epoch: [164][129/233]	Loss 0.2008 (0.0068)	
training:	Epoch: [164][130/233]	Loss 0.0003 (0.0067)	
training:	Epoch: [164][131/233]	Loss 0.0003 (0.0067)	
training:	Epoch: [164][132/233]	Loss 0.0002 (0.0066)	
training:	Epoch: [164][133/233]	Loss 0.0003 (0.0066)	
training:	Epoch: [164][134/233]	Loss 0.0393 (0.0068)	
training:	Epoch: [164][135/233]	Loss 0.0002 (0.0068)	
training:	Epoch: [164][136/233]	Loss 0.0002 (0.0067)	
training:	Epoch: [164][137/233]	Loss 0.0002 (0.0067)	
training:	Epoch: [164][138/233]	Loss 0.0002 (0.0066)	
training:	Epoch: [164][139/233]	Loss 0.0002 (0.0066)	
training:	Epoch: [164][140/233]	Loss 0.0002 (0.0065)	
training:	Epoch: [164][141/233]	Loss 0.0003 (0.0065)	
training:	Epoch: [164][142/233]	Loss 0.0004 (0.0064)	
training:	Epoch: [164][143/233]	Loss 0.0006 (0.0064)	
training:	Epoch: [164][144/233]	Loss 0.0590 (0.0068)	
training:	Epoch: [164][145/233]	Loss 0.0062 (0.0068)	
training:	Epoch: [164][146/233]	Loss 0.0006 (0.0067)	
training:	Epoch: [164][147/233]	Loss 0.0007 (0.0067)	
training:	Epoch: [164][148/233]	Loss 0.0302 (0.0068)	
training:	Epoch: [164][149/233]	Loss 0.0003 (0.0068)	
training:	Epoch: [164][150/233]	Loss 0.0003 (0.0067)	
training:	Epoch: [164][151/233]	Loss 0.0009 (0.0067)	
training:	Epoch: [164][152/233]	Loss 0.0003 (0.0067)	
training:	Epoch: [164][153/233]	Loss 0.0003 (0.0066)	
training:	Epoch: [164][154/233]	Loss 0.0050 (0.0066)	
training:	Epoch: [164][155/233]	Loss 0.0003 (0.0066)	
training:	Epoch: [164][156/233]	Loss 0.0020 (0.0065)	
training:	Epoch: [164][157/233]	Loss 0.0003 (0.0065)	
training:	Epoch: [164][158/233]	Loss 0.0002 (0.0065)	
training:	Epoch: [164][159/233]	Loss 0.0002 (0.0064)	
training:	Epoch: [164][160/233]	Loss 0.0004 (0.0064)	
training:	Epoch: [164][161/233]	Loss 0.0010 (0.0064)	
training:	Epoch: [164][162/233]	Loss 0.0014 (0.0063)	
training:	Epoch: [164][163/233]	Loss 0.0003 (0.0063)	
training:	Epoch: [164][164/233]	Loss 0.0002 (0.0062)	
training:	Epoch: [164][165/233]	Loss 0.0008 (0.0062)	
training:	Epoch: [164][166/233]	Loss 0.0003 (0.0062)	
training:	Epoch: [164][167/233]	Loss 0.0002 (0.0061)	
training:	Epoch: [164][168/233]	Loss 0.0005 (0.0061)	
training:	Epoch: [164][169/233]	Loss 0.0242 (0.0062)	
training:	Epoch: [164][170/233]	Loss 0.0015 (0.0062)	
training:	Epoch: [164][171/233]	Loss 0.0015 (0.0062)	
training:	Epoch: [164][172/233]	Loss 0.0006 (0.0061)	
training:	Epoch: [164][173/233]	Loss 0.0002 (0.0061)	
training:	Epoch: [164][174/233]	Loss 0.0005 (0.0061)	
training:	Epoch: [164][175/233]	Loss 0.0004 (0.0060)	
training:	Epoch: [164][176/233]	Loss 0.0003 (0.0060)	
training:	Epoch: [164][177/233]	Loss 0.0002 (0.0060)	
training:	Epoch: [164][178/233]	Loss 0.0004 (0.0059)	
training:	Epoch: [164][179/233]	Loss 0.0003 (0.0059)	
training:	Epoch: [164][180/233]	Loss 0.0009 (0.0059)	
training:	Epoch: [164][181/233]	Loss 0.0002 (0.0058)	
training:	Epoch: [164][182/233]	Loss 0.0002 (0.0058)	
training:	Epoch: [164][183/233]	Loss 0.0006 (0.0058)	
training:	Epoch: [164][184/233]	Loss 0.0002 (0.0058)	
training:	Epoch: [164][185/233]	Loss 0.0003 (0.0057)	
training:	Epoch: [164][186/233]	Loss 0.0191 (0.0058)	
training:	Epoch: [164][187/233]	Loss 0.0002 (0.0058)	
training:	Epoch: [164][188/233]	Loss 0.0031 (0.0058)	
training:	Epoch: [164][189/233]	Loss 0.0003 (0.0057)	
training:	Epoch: [164][190/233]	Loss 0.0002 (0.0057)	
training:	Epoch: [164][191/233]	Loss 0.0004 (0.0057)	
training:	Epoch: [164][192/233]	Loss 0.0005 (0.0056)	
training:	Epoch: [164][193/233]	Loss 0.0004 (0.0056)	
training:	Epoch: [164][194/233]	Loss 0.0011 (0.0056)	
training:	Epoch: [164][195/233]	Loss 0.0008 (0.0056)	
training:	Epoch: [164][196/233]	Loss 0.0028 (0.0055)	
training:	Epoch: [164][197/233]	Loss 0.0039 (0.0055)	
training:	Epoch: [164][198/233]	Loss 0.0002 (0.0055)	
training:	Epoch: [164][199/233]	Loss 0.0005 (0.0055)	
training:	Epoch: [164][200/233]	Loss 0.0002 (0.0055)	
training:	Epoch: [164][201/233]	Loss 0.0010 (0.0054)	
training:	Epoch: [164][202/233]	Loss 0.0003 (0.0054)	
training:	Epoch: [164][203/233]	Loss 0.0086 (0.0054)	
training:	Epoch: [164][204/233]	Loss 0.0003 (0.0054)	
training:	Epoch: [164][205/233]	Loss 0.0002 (0.0054)	
training:	Epoch: [164][206/233]	Loss 0.0045 (0.0054)	
training:	Epoch: [164][207/233]	Loss 0.0003 (0.0054)	
training:	Epoch: [164][208/233]	Loss 0.0003 (0.0053)	
training:	Epoch: [164][209/233]	Loss 0.0014 (0.0053)	
training:	Epoch: [164][210/233]	Loss 0.0003 (0.0053)	
training:	Epoch: [164][211/233]	Loss 0.0002 (0.0053)	
training:	Epoch: [164][212/233]	Loss 0.0030 (0.0053)	
training:	Epoch: [164][213/233]	Loss 0.0004 (0.0052)	
training:	Epoch: [164][214/233]	Loss 0.0002 (0.0052)	
training:	Epoch: [164][215/233]	Loss 0.0003 (0.0052)	
training:	Epoch: [164][216/233]	Loss 0.0003 (0.0052)	
training:	Epoch: [164][217/233]	Loss 0.0002 (0.0051)	
training:	Epoch: [164][218/233]	Loss 0.0002 (0.0051)	
training:	Epoch: [164][219/233]	Loss 0.0003 (0.0051)	
training:	Epoch: [164][220/233]	Loss 0.0002 (0.0051)	
training:	Epoch: [164][221/233]	Loss 0.0005 (0.0050)	
training:	Epoch: [164][222/233]	Loss 0.0002 (0.0050)	
training:	Epoch: [164][223/233]	Loss 0.0002 (0.0050)	
training:	Epoch: [164][224/233]	Loss 0.0041 (0.0050)	
training:	Epoch: [164][225/233]	Loss 0.0004 (0.0050)	
training:	Epoch: [164][226/233]	Loss 0.0002 (0.0050)	
training:	Epoch: [164][227/233]	Loss 0.0002 (0.0049)	
training:	Epoch: [164][228/233]	Loss 0.0002 (0.0049)	
training:	Epoch: [164][229/233]	Loss 0.0114 (0.0049)	
training:	Epoch: [164][230/233]	Loss 0.0003 (0.0049)	
training:	Epoch: [164][231/233]	Loss 0.0003 (0.0049)	
training:	Epoch: [164][232/233]	Loss 0.0002 (0.0049)	
training:	Epoch: [164][233/233]	Loss 0.0002 (0.0049)	
Training:	 Loss: 0.0049

Training:	 ACC: 1.0000 1.0000 1.0000 1.0000
Validation:	 ACC: 0.7923 0.7929 0.8049 0.7798
Validation:	 Best_BACC: 0.8162 0.8154 0.7998 0.8326
Validation:	 Loss: 1.3003
Pretraining:	Epoch 165/200
----------
training:	Epoch: [165][1/233]	Loss 0.0006 (0.0006)	
training:	Epoch: [165][2/233]	Loss 0.0003 (0.0005)	
training:	Epoch: [165][3/233]	Loss 0.0035 (0.0015)	
training:	Epoch: [165][4/233]	Loss 0.0002 (0.0012)	
training:	Epoch: [165][5/233]	Loss 0.0416 (0.0093)	
training:	Epoch: [165][6/233]	Loss 0.0002 (0.0077)	
training:	Epoch: [165][7/233]	Loss 0.0041 (0.0072)	
training:	Epoch: [165][8/233]	Loss 0.0003 (0.0064)	
training:	Epoch: [165][9/233]	Loss 0.0002 (0.0057)	
training:	Epoch: [165][10/233]	Loss 0.0003 (0.0051)	
training:	Epoch: [165][11/233]	Loss 0.0003 (0.0047)	
training:	Epoch: [165][12/233]	Loss 0.0004 (0.0043)	
training:	Epoch: [165][13/233]	Loss 0.0002 (0.0040)	
training:	Epoch: [165][14/233]	Loss 0.0003 (0.0038)	
training:	Epoch: [165][15/233]	Loss 0.0002 (0.0035)	
training:	Epoch: [165][16/233]	Loss 0.0002 (0.0033)	
training:	Epoch: [165][17/233]	Loss 0.0002 (0.0031)	
training:	Epoch: [165][18/233]	Loss 0.0013 (0.0030)	
training:	Epoch: [165][19/233]	Loss 0.0002 (0.0029)	
training:	Epoch: [165][20/233]	Loss 0.0005 (0.0028)	
training:	Epoch: [165][21/233]	Loss 0.0002 (0.0026)	
training:	Epoch: [165][22/233]	Loss 0.0004 (0.0025)	
training:	Epoch: [165][23/233]	Loss 0.0002 (0.0024)	
training:	Epoch: [165][24/233]	Loss 0.0002 (0.0023)	
training:	Epoch: [165][25/233]	Loss 0.0003 (0.0023)	
training:	Epoch: [165][26/233]	Loss 0.0002 (0.0022)	
training:	Epoch: [165][27/233]	Loss 0.0005 (0.0021)	
training:	Epoch: [165][28/233]	Loss 0.0010 (0.0021)	
training:	Epoch: [165][29/233]	Loss 0.0002 (0.0020)	
training:	Epoch: [165][30/233]	Loss 0.0008 (0.0020)	
training:	Epoch: [165][31/233]	Loss 0.0008 (0.0019)	
training:	Epoch: [165][32/233]	Loss 0.0002 (0.0019)	
training:	Epoch: [165][33/233]	Loss 0.0003 (0.0018)	
training:	Epoch: [165][34/233]	Loss 0.0034 (0.0019)	
training:	Epoch: [165][35/233]	Loss 0.0002 (0.0018)	
training:	Epoch: [165][36/233]	Loss 0.0002 (0.0018)	
training:	Epoch: [165][37/233]	Loss 0.0002 (0.0017)	
training:	Epoch: [165][38/233]	Loss 0.0012 (0.0017)	
training:	Epoch: [165][39/233]	Loss 0.0002 (0.0017)	
training:	Epoch: [165][40/233]	Loss 0.0002 (0.0017)	
training:	Epoch: [165][41/233]	Loss 0.0003 (0.0016)	
training:	Epoch: [165][42/233]	Loss 0.0003 (0.0016)	
training:	Epoch: [165][43/233]	Loss 0.0002 (0.0016)	
training:	Epoch: [165][44/233]	Loss 0.0003 (0.0015)	
training:	Epoch: [165][45/233]	Loss 0.0002 (0.0015)	
training:	Epoch: [165][46/233]	Loss 0.0002 (0.0015)	
training:	Epoch: [165][47/233]	Loss 0.0002 (0.0014)	
training:	Epoch: [165][48/233]	Loss 0.0002 (0.0014)	
training:	Epoch: [165][49/233]	Loss 0.0007 (0.0014)	
training:	Epoch: [165][50/233]	Loss 0.0003 (0.0014)	
training:	Epoch: [165][51/233]	Loss 0.0002 (0.0014)	
training:	Epoch: [165][52/233]	Loss 0.0003 (0.0013)	
training:	Epoch: [165][53/233]	Loss 0.1134 (0.0035)	
training:	Epoch: [165][54/233]	Loss 0.0007 (0.0034)	
training:	Epoch: [165][55/233]	Loss 0.0003 (0.0033)	
training:	Epoch: [165][56/233]	Loss 0.0003 (0.0033)	
training:	Epoch: [165][57/233]	Loss 0.0012 (0.0033)	
training:	Epoch: [165][58/233]	Loss 0.0010 (0.0032)	
training:	Epoch: [165][59/233]	Loss 0.0002 (0.0032)	
training:	Epoch: [165][60/233]	Loss 0.0044 (0.0032)	
training:	Epoch: [165][61/233]	Loss 0.0002 (0.0031)	
training:	Epoch: [165][62/233]	Loss 0.0008 (0.0031)	
training:	Epoch: [165][63/233]	Loss 0.0004 (0.0031)	
training:	Epoch: [165][64/233]	Loss 0.0003 (0.0030)	
training:	Epoch: [165][65/233]	Loss 0.0005 (0.0030)	
training:	Epoch: [165][66/233]	Loss 0.0002 (0.0029)	
training:	Epoch: [165][67/233]	Loss 0.0004 (0.0029)	
training:	Epoch: [165][68/233]	Loss 0.0016 (0.0029)	
training:	Epoch: [165][69/233]	Loss 0.0002 (0.0028)	
training:	Epoch: [165][70/233]	Loss 0.0003 (0.0028)	
training:	Epoch: [165][71/233]	Loss 0.0004 (0.0028)	
training:	Epoch: [165][72/233]	Loss 0.0003 (0.0027)	
training:	Epoch: [165][73/233]	Loss 0.0004 (0.0027)	
training:	Epoch: [165][74/233]	Loss 0.0003 (0.0027)	
training:	Epoch: [165][75/233]	Loss 0.0002 (0.0026)	
training:	Epoch: [165][76/233]	Loss 0.0004 (0.0026)	
training:	Epoch: [165][77/233]	Loss 0.0003 (0.0026)	
training:	Epoch: [165][78/233]	Loss 0.0002 (0.0025)	
training:	Epoch: [165][79/233]	Loss 0.0002 (0.0025)	
training:	Epoch: [165][80/233]	Loss 0.0006 (0.0025)	
training:	Epoch: [165][81/233]	Loss 0.0003 (0.0025)	
training:	Epoch: [165][82/233]	Loss 0.0002 (0.0024)	
training:	Epoch: [165][83/233]	Loss 0.0004 (0.0024)	
training:	Epoch: [165][84/233]	Loss 0.0004 (0.0024)	
training:	Epoch: [165][85/233]	Loss 0.0003 (0.0024)	
training:	Epoch: [165][86/233]	Loss 0.0003 (0.0023)	
training:	Epoch: [165][87/233]	Loss 0.0003 (0.0023)	
training:	Epoch: [165][88/233]	Loss 0.0002 (0.0023)	
training:	Epoch: [165][89/233]	Loss 0.0103 (0.0024)	
training:	Epoch: [165][90/233]	Loss 0.0002 (0.0024)	
training:	Epoch: [165][91/233]	Loss 0.0330 (0.0027)	
training:	Epoch: [165][92/233]	Loss 0.0002 (0.0027)	
training:	Epoch: [165][93/233]	Loss 0.0003 (0.0026)	
training:	Epoch: [165][94/233]	Loss 0.0003 (0.0026)	
training:	Epoch: [165][95/233]	Loss 0.0003 (0.0026)	
training:	Epoch: [165][96/233]	Loss 0.0006 (0.0026)	
training:	Epoch: [165][97/233]	Loss 0.0002 (0.0025)	
training:	Epoch: [165][98/233]	Loss 0.0003 (0.0025)	
training:	Epoch: [165][99/233]	Loss 0.0012 (0.0025)	
training:	Epoch: [165][100/233]	Loss 0.0003 (0.0025)	
training:	Epoch: [165][101/233]	Loss 0.0003 (0.0025)	
training:	Epoch: [165][102/233]	Loss 0.0002 (0.0024)	
training:	Epoch: [165][103/233]	Loss 0.0002 (0.0024)	
training:	Epoch: [165][104/233]	Loss 0.0002 (0.0024)	
training:	Epoch: [165][105/233]	Loss 0.0003 (0.0024)	
training:	Epoch: [165][106/233]	Loss 0.0020 (0.0024)	
training:	Epoch: [165][107/233]	Loss 0.0002 (0.0024)	
training:	Epoch: [165][108/233]	Loss 0.0003 (0.0023)	
training:	Epoch: [165][109/233]	Loss 0.0002 (0.0023)	
training:	Epoch: [165][110/233]	Loss 0.0002 (0.0023)	
training:	Epoch: [165][111/233]	Loss 0.0002 (0.0023)	
training:	Epoch: [165][112/233]	Loss 0.0003 (0.0023)	
training:	Epoch: [165][113/233]	Loss 0.0487 (0.0027)	
training:	Epoch: [165][114/233]	Loss 0.0002 (0.0027)	
training:	Epoch: [165][115/233]	Loss 0.0002 (0.0026)	
training:	Epoch: [165][116/233]	Loss 0.0002 (0.0026)	
training:	Epoch: [165][117/233]	Loss 0.0002 (0.0026)	
training:	Epoch: [165][118/233]	Loss 0.0002 (0.0026)	
training:	Epoch: [165][119/233]	Loss 0.0002 (0.0026)	
training:	Epoch: [165][120/233]	Loss 0.0030 (0.0026)	
training:	Epoch: [165][121/233]	Loss 0.0002 (0.0025)	
training:	Epoch: [165][122/233]	Loss 0.0002 (0.0025)	
training:	Epoch: [165][123/233]	Loss 0.0002 (0.0025)	
training:	Epoch: [165][124/233]	Loss 0.0124 (0.0026)	
training:	Epoch: [165][125/233]	Loss 0.0002 (0.0026)	
training:	Epoch: [165][126/233]	Loss 0.0014 (0.0025)	
training:	Epoch: [165][127/233]	Loss 0.0002 (0.0025)	
training:	Epoch: [165][128/233]	Loss 0.0002 (0.0025)	
training:	Epoch: [165][129/233]	Loss 0.0002 (0.0025)	
training:	Epoch: [165][130/233]	Loss 0.0002 (0.0025)	
training:	Epoch: [165][131/233]	Loss 0.0009 (0.0025)	
training:	Epoch: [165][132/233]	Loss 0.0003 (0.0024)	
training:	Epoch: [165][133/233]	Loss 0.0003 (0.0024)	
training:	Epoch: [165][134/233]	Loss 0.0007 (0.0024)	
training:	Epoch: [165][135/233]	Loss 0.0002 (0.0024)	
training:	Epoch: [165][136/233]	Loss 0.0004 (0.0024)	
training:	Epoch: [165][137/233]	Loss 0.0003 (0.0024)	
training:	Epoch: [165][138/233]	Loss 0.0002 (0.0024)	
training:	Epoch: [165][139/233]	Loss 0.0007 (0.0023)	
training:	Epoch: [165][140/233]	Loss 0.0043 (0.0024)	
training:	Epoch: [165][141/233]	Loss 0.0002 (0.0023)	
training:	Epoch: [165][142/233]	Loss 0.0038 (0.0024)	
training:	Epoch: [165][143/233]	Loss 0.0003 (0.0023)	
training:	Epoch: [165][144/233]	Loss 0.0002 (0.0023)	
training:	Epoch: [165][145/233]	Loss 0.0002 (0.0023)	
training:	Epoch: [165][146/233]	Loss 0.0004 (0.0023)	
training:	Epoch: [165][147/233]	Loss 0.0003 (0.0023)	
training:	Epoch: [165][148/233]	Loss 0.0002 (0.0023)	
training:	Epoch: [165][149/233]	Loss 0.0002 (0.0023)	
training:	Epoch: [165][150/233]	Loss 0.0002 (0.0022)	
training:	Epoch: [165][151/233]	Loss 0.0003 (0.0022)	
training:	Epoch: [165][152/233]	Loss 0.0002 (0.0022)	
training:	Epoch: [165][153/233]	Loss 0.0002 (0.0022)	
training:	Epoch: [165][154/233]	Loss 0.0016 (0.0022)	
training:	Epoch: [165][155/233]	Loss 0.0003 (0.0022)	
training:	Epoch: [165][156/233]	Loss 0.0002 (0.0022)	
training:	Epoch: [165][157/233]	Loss 0.0003 (0.0022)	
training:	Epoch: [165][158/233]	Loss 0.0009 (0.0022)	
training:	Epoch: [165][159/233]	Loss 0.0003 (0.0021)	
training:	Epoch: [165][160/233]	Loss 0.0007 (0.0021)	
training:	Epoch: [165][161/233]	Loss 0.0002 (0.0021)	
training:	Epoch: [165][162/233]	Loss 0.0003 (0.0021)	
training:	Epoch: [165][163/233]	Loss 0.0002 (0.0021)	
training:	Epoch: [165][164/233]	Loss 0.0012 (0.0021)	
training:	Epoch: [165][165/233]	Loss 0.0002 (0.0021)	
training:	Epoch: [165][166/233]	Loss 0.0002 (0.0021)	
training:	Epoch: [165][167/233]	Loss 0.0003 (0.0021)	
training:	Epoch: [165][168/233]	Loss 0.0002 (0.0020)	
training:	Epoch: [165][169/233]	Loss 0.0003 (0.0020)	
training:	Epoch: [165][170/233]	Loss 0.0002 (0.0020)	
training:	Epoch: [165][171/233]	Loss 0.0004 (0.0020)	
training:	Epoch: [165][172/233]	Loss 0.0028 (0.0020)	
training:	Epoch: [165][173/233]	Loss 0.0003 (0.0020)	
training:	Epoch: [165][174/233]	Loss 0.0005 (0.0020)	
training:	Epoch: [165][175/233]	Loss 0.0003 (0.0020)	
training:	Epoch: [165][176/233]	Loss 0.0002 (0.0020)	
training:	Epoch: [165][177/233]	Loss 0.0003 (0.0020)	
training:	Epoch: [165][178/233]	Loss 0.0002 (0.0020)	
training:	Epoch: [165][179/233]	Loss 0.0005 (0.0020)	
training:	Epoch: [165][180/233]	Loss 0.0002 (0.0019)	
training:	Epoch: [165][181/233]	Loss 0.0003 (0.0019)	
training:	Epoch: [165][182/233]	Loss 0.0002 (0.0019)	
training:	Epoch: [165][183/233]	Loss 0.0002 (0.0019)	
training:	Epoch: [165][184/233]	Loss 0.0002 (0.0019)	
training:	Epoch: [165][185/233]	Loss 0.0004 (0.0019)	
training:	Epoch: [165][186/233]	Loss 0.0008 (0.0019)	
training:	Epoch: [165][187/233]	Loss 0.0002 (0.0019)	
training:	Epoch: [165][188/233]	Loss 0.0010 (0.0019)	
training:	Epoch: [165][189/233]	Loss 0.0002 (0.0019)	
training:	Epoch: [165][190/233]	Loss 0.0003 (0.0019)	
training:	Epoch: [165][191/233]	Loss 0.0010 (0.0019)	
training:	Epoch: [165][192/233]	Loss 0.0105 (0.0019)	
training:	Epoch: [165][193/233]	Loss 0.0003 (0.0019)	
training:	Epoch: [165][194/233]	Loss 0.0086 (0.0019)	
training:	Epoch: [165][195/233]	Loss 0.0003 (0.0019)	
training:	Epoch: [165][196/233]	Loss 0.0005 (0.0019)	
training:	Epoch: [165][197/233]	Loss 0.0011 (0.0019)	
training:	Epoch: [165][198/233]	Loss 0.0002 (0.0019)	
training:	Epoch: [165][199/233]	Loss 0.1108 (0.0025)	
training:	Epoch: [165][200/233]	Loss 0.0015 (0.0024)	
training:	Epoch: [165][201/233]	Loss 0.0006 (0.0024)	
training:	Epoch: [165][202/233]	Loss 0.0003 (0.0024)	
training:	Epoch: [165][203/233]	Loss 0.0002 (0.0024)	
training:	Epoch: [165][204/233]	Loss 0.0003 (0.0024)	
training:	Epoch: [165][205/233]	Loss 0.0002 (0.0024)	
training:	Epoch: [165][206/233]	Loss 0.0003 (0.0024)	
training:	Epoch: [165][207/233]	Loss 0.0007 (0.0024)	
training:	Epoch: [165][208/233]	Loss 0.0002 (0.0024)	
training:	Epoch: [165][209/233]	Loss 0.0002 (0.0024)	
training:	Epoch: [165][210/233]	Loss 0.0016 (0.0024)	
training:	Epoch: [165][211/233]	Loss 0.0005 (0.0023)	
training:	Epoch: [165][212/233]	Loss 0.0002 (0.0023)	
training:	Epoch: [165][213/233]	Loss 0.0003 (0.0023)	
training:	Epoch: [165][214/233]	Loss 0.0002 (0.0023)	
training:	Epoch: [165][215/233]	Loss 0.0002 (0.0023)	
training:	Epoch: [165][216/233]	Loss 0.0003 (0.0023)	
training:	Epoch: [165][217/233]	Loss 0.0002 (0.0023)	
training:	Epoch: [165][218/233]	Loss 0.0004 (0.0023)	
training:	Epoch: [165][219/233]	Loss 0.0003 (0.0023)	
training:	Epoch: [165][220/233]	Loss 0.0002 (0.0023)	
training:	Epoch: [165][221/233]	Loss 0.0002 (0.0022)	
training:	Epoch: [165][222/233]	Loss 0.0003 (0.0022)	
training:	Epoch: [165][223/233]	Loss 0.0003 (0.0022)	
training:	Epoch: [165][224/233]	Loss 0.0002 (0.0022)	
training:	Epoch: [165][225/233]	Loss 0.0002 (0.0022)	
training:	Epoch: [165][226/233]	Loss 0.0002 (0.0022)	
training:	Epoch: [165][227/233]	Loss 0.0003 (0.0022)	
training:	Epoch: [165][228/233]	Loss 0.0002 (0.0022)	
training:	Epoch: [165][229/233]	Loss 0.0002 (0.0022)	
training:	Epoch: [165][230/233]	Loss 0.0002 (0.0022)	
training:	Epoch: [165][231/233]	Loss 0.0004 (0.0022)	
training:	Epoch: [165][232/233]	Loss 0.0008 (0.0022)	
training:	Epoch: [165][233/233]	Loss 0.0018 (0.0022)	
Training:	 Loss: 0.0021

Training:	 ACC: 1.0000 1.0000 1.0000 1.0000
Validation:	 ACC: 0.7868 0.7849 0.7467 0.8270
Validation:	 Best_BACC: 0.8162 0.8154 0.7998 0.8326
Validation:	 Loss: 1.3423
Pretraining:	Epoch 166/200
----------
training:	Epoch: [166][1/233]	Loss 0.0007 (0.0007)	
training:	Epoch: [166][2/233]	Loss 0.0094 (0.0050)	
training:	Epoch: [166][3/233]	Loss 0.0002 (0.0034)	
training:	Epoch: [166][4/233]	Loss 0.0002 (0.0026)	
training:	Epoch: [166][5/233]	Loss 0.0002 (0.0021)	
training:	Epoch: [166][6/233]	Loss 0.0011 (0.0020)	
training:	Epoch: [166][7/233]	Loss 0.0002 (0.0017)	
training:	Epoch: [166][8/233]	Loss 0.0003 (0.0015)	
training:	Epoch: [166][9/233]	Loss 0.0002 (0.0014)	
training:	Epoch: [166][10/233]	Loss 0.0002 (0.0013)	
training:	Epoch: [166][11/233]	Loss 0.0002 (0.0012)	
training:	Epoch: [166][12/233]	Loss 0.0003 (0.0011)	
training:	Epoch: [166][13/233]	Loss 0.0324 (0.0035)	
training:	Epoch: [166][14/233]	Loss 0.0003 (0.0033)	
training:	Epoch: [166][15/233]	Loss 0.0004 (0.0031)	
training:	Epoch: [166][16/233]	Loss 0.0002 (0.0029)	
training:	Epoch: [166][17/233]	Loss 0.0002 (0.0027)	
training:	Epoch: [166][18/233]	Loss 0.0002 (0.0026)	
training:	Epoch: [166][19/233]	Loss 0.0002 (0.0025)	
training:	Epoch: [166][20/233]	Loss 0.0002 (0.0024)	
training:	Epoch: [166][21/233]	Loss 0.0002 (0.0023)	
training:	Epoch: [166][22/233]	Loss 0.0011 (0.0022)	
training:	Epoch: [166][23/233]	Loss 0.0003 (0.0021)	
training:	Epoch: [166][24/233]	Loss 0.0002 (0.0020)	
training:	Epoch: [166][25/233]	Loss 0.0002 (0.0020)	
training:	Epoch: [166][26/233]	Loss 0.0002 (0.0019)	
training:	Epoch: [166][27/233]	Loss 0.0003 (0.0018)	
training:	Epoch: [166][28/233]	Loss 0.0003 (0.0018)	
training:	Epoch: [166][29/233]	Loss 0.0003 (0.0017)	
training:	Epoch: [166][30/233]	Loss 0.0002 (0.0017)	
training:	Epoch: [166][31/233]	Loss 0.0002 (0.0016)	
training:	Epoch: [166][32/233]	Loss 0.0002 (0.0016)	
training:	Epoch: [166][33/233]	Loss 0.2141 (0.0080)	
training:	Epoch: [166][34/233]	Loss 0.0002 (0.0078)	
training:	Epoch: [166][35/233]	Loss 0.0002 (0.0076)	
training:	Epoch: [166][36/233]	Loss 0.0003 (0.0074)	
training:	Epoch: [166][37/233]	Loss 0.0004 (0.0072)	
training:	Epoch: [166][38/233]	Loss 0.0018 (0.0070)	
training:	Epoch: [166][39/233]	Loss 0.0004 (0.0069)	
training:	Epoch: [166][40/233]	Loss 0.0002 (0.0067)	
training:	Epoch: [166][41/233]	Loss 0.0003 (0.0066)	
training:	Epoch: [166][42/233]	Loss 0.0002 (0.0064)	
training:	Epoch: [166][43/233]	Loss 0.0005 (0.0063)	
training:	Epoch: [166][44/233]	Loss 0.0003 (0.0061)	
training:	Epoch: [166][45/233]	Loss 0.0003 (0.0060)	
training:	Epoch: [166][46/233]	Loss 0.0003 (0.0059)	
training:	Epoch: [166][47/233]	Loss 0.0003 (0.0058)	
training:	Epoch: [166][48/233]	Loss 0.2294 (0.0104)	
training:	Epoch: [166][49/233]	Loss 0.0004 (0.0102)	
training:	Epoch: [166][50/233]	Loss 0.0003 (0.0100)	
training:	Epoch: [166][51/233]	Loss 0.0004 (0.0098)	
training:	Epoch: [166][52/233]	Loss 0.0073 (0.0098)	
training:	Epoch: [166][53/233]	Loss 0.0002 (0.0096)	
training:	Epoch: [166][54/233]	Loss 0.0003 (0.0094)	
training:	Epoch: [166][55/233]	Loss 0.0002 (0.0093)	
training:	Epoch: [166][56/233]	Loss 0.0002 (0.0091)	
training:	Epoch: [166][57/233]	Loss 0.0005 (0.0089)	
training:	Epoch: [166][58/233]	Loss 0.0002 (0.0088)	
training:	Epoch: [166][59/233]	Loss 0.0003 (0.0086)	
training:	Epoch: [166][60/233]	Loss 0.0004 (0.0085)	
training:	Epoch: [166][61/233]	Loss 0.0002 (0.0084)	
training:	Epoch: [166][62/233]	Loss 0.0004 (0.0082)	
training:	Epoch: [166][63/233]	Loss 0.0003 (0.0081)	
training:	Epoch: [166][64/233]	Loss 0.0002 (0.0080)	
training:	Epoch: [166][65/233]	Loss 0.0004 (0.0079)	
training:	Epoch: [166][66/233]	Loss 0.0002 (0.0078)	
training:	Epoch: [166][67/233]	Loss 0.0002 (0.0077)	
training:	Epoch: [166][68/233]	Loss 0.0002 (0.0075)	
training:	Epoch: [166][69/233]	Loss 0.0010 (0.0074)	
training:	Epoch: [166][70/233]	Loss 0.0002 (0.0073)	
training:	Epoch: [166][71/233]	Loss 0.1269 (0.0090)	
training:	Epoch: [166][72/233]	Loss 0.0090 (0.0090)	
training:	Epoch: [166][73/233]	Loss 0.0004 (0.0089)	
training:	Epoch: [166][74/233]	Loss 0.0010 (0.0088)	
training:	Epoch: [166][75/233]	Loss 0.0003 (0.0087)	
training:	Epoch: [166][76/233]	Loss 0.0002 (0.0086)	
training:	Epoch: [166][77/233]	Loss 0.0003 (0.0085)	
training:	Epoch: [166][78/233]	Loss 0.0002 (0.0084)	
training:	Epoch: [166][79/233]	Loss 0.0002 (0.0083)	
training:	Epoch: [166][80/233]	Loss 0.0990 (0.0094)	
training:	Epoch: [166][81/233]	Loss 0.0002 (0.0093)	
training:	Epoch: [166][82/233]	Loss 0.0003 (0.0092)	
training:	Epoch: [166][83/233]	Loss 0.0002 (0.0091)	
training:	Epoch: [166][84/233]	Loss 0.0003 (0.0090)	
training:	Epoch: [166][85/233]	Loss 0.0003 (0.0089)	
training:	Epoch: [166][86/233]	Loss 0.1236 (0.0102)	
training:	Epoch: [166][87/233]	Loss 0.0004 (0.0101)	
training:	Epoch: [166][88/233]	Loss 0.0006 (0.0100)	
training:	Epoch: [166][89/233]	Loss 0.0005 (0.0099)	
training:	Epoch: [166][90/233]	Loss 0.0003 (0.0098)	
training:	Epoch: [166][91/233]	Loss 0.0005 (0.0097)	
training:	Epoch: [166][92/233]	Loss 0.0002 (0.0096)	
training:	Epoch: [166][93/233]	Loss 0.0003 (0.0095)	
training:	Epoch: [166][94/233]	Loss 0.0010 (0.0094)	
training:	Epoch: [166][95/233]	Loss 0.0003 (0.0093)	
training:	Epoch: [166][96/233]	Loss 0.0002 (0.0092)	
training:	Epoch: [166][97/233]	Loss 0.0003 (0.0091)	
training:	Epoch: [166][98/233]	Loss 0.0002 (0.0090)	
training:	Epoch: [166][99/233]	Loss 0.0004 (0.0089)	
training:	Epoch: [166][100/233]	Loss 0.0005 (0.0088)	
training:	Epoch: [166][101/233]	Loss 0.0008 (0.0087)	
training:	Epoch: [166][102/233]	Loss 0.0002 (0.0087)	
training:	Epoch: [166][103/233]	Loss 0.0003 (0.0086)	
training:	Epoch: [166][104/233]	Loss 0.0004 (0.0085)	
training:	Epoch: [166][105/233]	Loss 0.0002 (0.0084)	
training:	Epoch: [166][106/233]	Loss 0.0003 (0.0083)	
training:	Epoch: [166][107/233]	Loss 0.0005 (0.0083)	
training:	Epoch: [166][108/233]	Loss 0.0002 (0.0082)	
training:	Epoch: [166][109/233]	Loss 0.0003 (0.0081)	
training:	Epoch: [166][110/233]	Loss 0.0044 (0.0081)	
training:	Epoch: [166][111/233]	Loss 0.0003 (0.0080)	
training:	Epoch: [166][112/233]	Loss 0.0008 (0.0080)	
training:	Epoch: [166][113/233]	Loss 0.0003 (0.0079)	
training:	Epoch: [166][114/233]	Loss 0.0003 (0.0078)	
training:	Epoch: [166][115/233]	Loss 0.0002 (0.0078)	
training:	Epoch: [166][116/233]	Loss 0.0008 (0.0077)	
training:	Epoch: [166][117/233]	Loss 0.0002 (0.0076)	
training:	Epoch: [166][118/233]	Loss 0.0011 (0.0076)	
training:	Epoch: [166][119/233]	Loss 0.0011 (0.0075)	
training:	Epoch: [166][120/233]	Loss 0.0002 (0.0075)	
training:	Epoch: [166][121/233]	Loss 0.0006 (0.0074)	
training:	Epoch: [166][122/233]	Loss 0.0002 (0.0073)	
training:	Epoch: [166][123/233]	Loss 0.0003 (0.0073)	
training:	Epoch: [166][124/233]	Loss 0.0003 (0.0072)	
training:	Epoch: [166][125/233]	Loss 0.0005 (0.0072)	
training:	Epoch: [166][126/233]	Loss 0.0007 (0.0071)	
training:	Epoch: [166][127/233]	Loss 0.0002 (0.0071)	
training:	Epoch: [166][128/233]	Loss 0.0002 (0.0070)	
training:	Epoch: [166][129/233]	Loss 0.0003 (0.0070)	
training:	Epoch: [166][130/233]	Loss 0.0017 (0.0069)	
training:	Epoch: [166][131/233]	Loss 0.0002 (0.0069)	
training:	Epoch: [166][132/233]	Loss 0.0003 (0.0068)	
training:	Epoch: [166][133/233]	Loss 0.0003 (0.0068)	
training:	Epoch: [166][134/233]	Loss 0.0002 (0.0067)	
training:	Epoch: [166][135/233]	Loss 0.0003 (0.0067)	
training:	Epoch: [166][136/233]	Loss 0.0002 (0.0066)	
training:	Epoch: [166][137/233]	Loss 0.0003 (0.0066)	
training:	Epoch: [166][138/233]	Loss 0.0003 (0.0065)	
training:	Epoch: [166][139/233]	Loss 0.0002 (0.0065)	
training:	Epoch: [166][140/233]	Loss 0.0003 (0.0064)	
training:	Epoch: [166][141/233]	Loss 0.0002 (0.0064)	
training:	Epoch: [166][142/233]	Loss 0.0002 (0.0064)	
training:	Epoch: [166][143/233]	Loss 0.0002 (0.0063)	
training:	Epoch: [166][144/233]	Loss 0.0003 (0.0063)	
training:	Epoch: [166][145/233]	Loss 0.0004 (0.0062)	
training:	Epoch: [166][146/233]	Loss 0.0003 (0.0062)	
training:	Epoch: [166][147/233]	Loss 0.0003 (0.0062)	
training:	Epoch: [166][148/233]	Loss 0.0002 (0.0061)	
training:	Epoch: [166][149/233]	Loss 0.0007 (0.0061)	
training:	Epoch: [166][150/233]	Loss 0.0002 (0.0060)	
training:	Epoch: [166][151/233]	Loss 0.0062 (0.0060)	
training:	Epoch: [166][152/233]	Loss 0.0003 (0.0060)	
training:	Epoch: [166][153/233]	Loss 0.0009 (0.0060)	
training:	Epoch: [166][154/233]	Loss 0.0002 (0.0059)	
training:	Epoch: [166][155/233]	Loss 0.0004 (0.0059)	
training:	Epoch: [166][156/233]	Loss 0.0015 (0.0059)	
training:	Epoch: [166][157/233]	Loss 0.0002 (0.0058)	
training:	Epoch: [166][158/233]	Loss 0.0003 (0.0058)	
training:	Epoch: [166][159/233]	Loss 0.0002 (0.0058)	
training:	Epoch: [166][160/233]	Loss 0.0003 (0.0057)	
training:	Epoch: [166][161/233]	Loss 0.0004 (0.0057)	
training:	Epoch: [166][162/233]	Loss 0.0012 (0.0057)	
training:	Epoch: [166][163/233]	Loss 0.0005 (0.0056)	
training:	Epoch: [166][164/233]	Loss 0.0004 (0.0056)	
training:	Epoch: [166][165/233]	Loss 0.0003 (0.0056)	
training:	Epoch: [166][166/233]	Loss 0.0003 (0.0055)	
training:	Epoch: [166][167/233]	Loss 0.0307 (0.0057)	
training:	Epoch: [166][168/233]	Loss 0.0006 (0.0057)	
training:	Epoch: [166][169/233]	Loss 0.0123 (0.0057)	
training:	Epoch: [166][170/233]	Loss 0.0002 (0.0057)	
training:	Epoch: [166][171/233]	Loss 0.0004 (0.0056)	
training:	Epoch: [166][172/233]	Loss 0.0002 (0.0056)	
training:	Epoch: [166][173/233]	Loss 0.0003 (0.0056)	
training:	Epoch: [166][174/233]	Loss 0.0377 (0.0058)	
training:	Epoch: [166][175/233]	Loss 0.0003 (0.0057)	
training:	Epoch: [166][176/233]	Loss 0.0002 (0.0057)	
training:	Epoch: [166][177/233]	Loss 0.0003 (0.0057)	
training:	Epoch: [166][178/233]	Loss 0.0002 (0.0056)	
training:	Epoch: [166][179/233]	Loss 0.0006 (0.0056)	
training:	Epoch: [166][180/233]	Loss 0.0018 (0.0056)	
training:	Epoch: [166][181/233]	Loss 0.0002 (0.0056)	
training:	Epoch: [166][182/233]	Loss 0.0002 (0.0055)	
training:	Epoch: [166][183/233]	Loss 0.0002 (0.0055)	
training:	Epoch: [166][184/233]	Loss 0.0004 (0.0055)	
training:	Epoch: [166][185/233]	Loss 0.0011 (0.0054)	
training:	Epoch: [166][186/233]	Loss 0.0005 (0.0054)	
training:	Epoch: [166][187/233]	Loss 0.0004 (0.0054)	
training:	Epoch: [166][188/233]	Loss 0.0003 (0.0054)	
training:	Epoch: [166][189/233]	Loss 0.1244 (0.0060)	
training:	Epoch: [166][190/233]	Loss 0.0110 (0.0060)	
training:	Epoch: [166][191/233]	Loss 0.0135 (0.0061)	
training:	Epoch: [166][192/233]	Loss 0.0005 (0.0060)	
training:	Epoch: [166][193/233]	Loss 0.0005 (0.0060)	
training:	Epoch: [166][194/233]	Loss 0.0007 (0.0060)	
training:	Epoch: [166][195/233]	Loss 0.0002 (0.0059)	
training:	Epoch: [166][196/233]	Loss 0.0002 (0.0059)	
training:	Epoch: [166][197/233]	Loss 0.0002 (0.0059)	
training:	Epoch: [166][198/233]	Loss 0.0003 (0.0059)	
training:	Epoch: [166][199/233]	Loss 0.0058 (0.0059)	
training:	Epoch: [166][200/233]	Loss 0.0005 (0.0058)	
training:	Epoch: [166][201/233]	Loss 0.0005 (0.0058)	
training:	Epoch: [166][202/233]	Loss 0.0002 (0.0058)	
training:	Epoch: [166][203/233]	Loss 0.0018 (0.0058)	
training:	Epoch: [166][204/233]	Loss 0.0005 (0.0057)	
training:	Epoch: [166][205/233]	Loss 0.0029 (0.0057)	
training:	Epoch: [166][206/233]	Loss 0.0003 (0.0057)	
training:	Epoch: [166][207/233]	Loss 0.0003 (0.0057)	
training:	Epoch: [166][208/233]	Loss 0.0002 (0.0056)	
training:	Epoch: [166][209/233]	Loss 0.0007 (0.0056)	
training:	Epoch: [166][210/233]	Loss 0.0006 (0.0056)	
training:	Epoch: [166][211/233]	Loss 0.0004 (0.0056)	
training:	Epoch: [166][212/233]	Loss 0.0006 (0.0055)	
training:	Epoch: [166][213/233]	Loss 0.0002 (0.0055)	
training:	Epoch: [166][214/233]	Loss 0.0006 (0.0055)	
training:	Epoch: [166][215/233]	Loss 0.0120 (0.0055)	
training:	Epoch: [166][216/233]	Loss 0.0003 (0.0055)	
training:	Epoch: [166][217/233]	Loss 0.0048 (0.0055)	
training:	Epoch: [166][218/233]	Loss 0.0006 (0.0055)	
training:	Epoch: [166][219/233]	Loss 0.0008 (0.0055)	
training:	Epoch: [166][220/233]	Loss 0.0004 (0.0054)	
training:	Epoch: [166][221/233]	Loss 0.0015 (0.0054)	
training:	Epoch: [166][222/233]	Loss 0.0003 (0.0054)	
training:	Epoch: [166][223/233]	Loss 0.0467 (0.0056)	
training:	Epoch: [166][224/233]	Loss 0.0004 (0.0056)	
training:	Epoch: [166][225/233]	Loss 0.0002 (0.0055)	
training:	Epoch: [166][226/233]	Loss 0.0003 (0.0055)	
training:	Epoch: [166][227/233]	Loss 0.0020 (0.0055)	
training:	Epoch: [166][228/233]	Loss 0.0002 (0.0055)	
training:	Epoch: [166][229/233]	Loss 0.0003 (0.0054)	
training:	Epoch: [166][230/233]	Loss 0.0003 (0.0054)	
training:	Epoch: [166][231/233]	Loss 0.0004 (0.0054)	
training:	Epoch: [166][232/233]	Loss 0.0007 (0.0054)	
training:	Epoch: [166][233/233]	Loss 0.0003 (0.0054)	
Training:	 Loss: 0.0053

Training:	 ACC: 1.0000 1.0000 1.0000 1.0000
Validation:	 ACC: 0.7867 0.7860 0.7722 0.8011
Validation:	 Best_BACC: 0.8162 0.8154 0.7998 0.8326
Validation:	 Loss: 1.2861
Pretraining:	Epoch 167/200
----------
training:	Epoch: [167][1/233]	Loss 0.0002 (0.0002)	
training:	Epoch: [167][2/233]	Loss 0.0018 (0.0010)	
training:	Epoch: [167][3/233]	Loss 0.0026 (0.0015)	
training:	Epoch: [167][4/233]	Loss 0.0003 (0.0012)	
training:	Epoch: [167][5/233]	Loss 0.0005 (0.0011)	
training:	Epoch: [167][6/233]	Loss 0.0003 (0.0009)	
training:	Epoch: [167][7/233]	Loss 0.0002 (0.0008)	
training:	Epoch: [167][8/233]	Loss 0.0003 (0.0008)	
training:	Epoch: [167][9/233]	Loss 0.0003 (0.0007)	
training:	Epoch: [167][10/233]	Loss 0.0009 (0.0007)	
training:	Epoch: [167][11/233]	Loss 0.0002 (0.0007)	
training:	Epoch: [167][12/233]	Loss 0.0002 (0.0007)	
training:	Epoch: [167][13/233]	Loss 0.0002 (0.0006)	
training:	Epoch: [167][14/233]	Loss 0.0002 (0.0006)	
training:	Epoch: [167][15/233]	Loss 0.0002 (0.0006)	
training:	Epoch: [167][16/233]	Loss 0.0105 (0.0012)	
training:	Epoch: [167][17/233]	Loss 0.0019 (0.0012)	
training:	Epoch: [167][18/233]	Loss 0.0006 (0.0012)	
training:	Epoch: [167][19/233]	Loss 0.0003 (0.0012)	
training:	Epoch: [167][20/233]	Loss 0.0104 (0.0016)	
training:	Epoch: [167][21/233]	Loss 0.0002 (0.0015)	
training:	Epoch: [167][22/233]	Loss 0.0029 (0.0016)	
training:	Epoch: [167][23/233]	Loss 0.0003 (0.0016)	
training:	Epoch: [167][24/233]	Loss 0.0033 (0.0016)	
training:	Epoch: [167][25/233]	Loss 0.0006 (0.0016)	
training:	Epoch: [167][26/233]	Loss 0.0002 (0.0015)	
training:	Epoch: [167][27/233]	Loss 0.0003 (0.0015)	
training:	Epoch: [167][28/233]	Loss 0.0003 (0.0014)	
training:	Epoch: [167][29/233]	Loss 0.0004 (0.0014)	
training:	Epoch: [167][30/233]	Loss 0.0003 (0.0014)	
training:	Epoch: [167][31/233]	Loss 0.0010 (0.0014)	
training:	Epoch: [167][32/233]	Loss 0.0003 (0.0013)	
training:	Epoch: [167][33/233]	Loss 0.0002 (0.0013)	
training:	Epoch: [167][34/233]	Loss 0.0005 (0.0013)	
training:	Epoch: [167][35/233]	Loss 0.0003 (0.0012)	
training:	Epoch: [167][36/233]	Loss 0.0002 (0.0012)	
training:	Epoch: [167][37/233]	Loss 0.0002 (0.0012)	
training:	Epoch: [167][38/233]	Loss 0.0003 (0.0012)	
training:	Epoch: [167][39/233]	Loss 0.0003 (0.0011)	
training:	Epoch: [167][40/233]	Loss 0.0002 (0.0011)	
training:	Epoch: [167][41/233]	Loss 0.0011 (0.0011)	
training:	Epoch: [167][42/233]	Loss 0.0002 (0.0011)	
training:	Epoch: [167][43/233]	Loss 0.0002 (0.0011)	
training:	Epoch: [167][44/233]	Loss 0.0004 (0.0011)	
training:	Epoch: [167][45/233]	Loss 0.0003 (0.0010)	
training:	Epoch: [167][46/233]	Loss 0.0047 (0.0011)	
training:	Epoch: [167][47/233]	Loss 0.0002 (0.0011)	
training:	Epoch: [167][48/233]	Loss 0.0002 (0.0011)	
training:	Epoch: [167][49/233]	Loss 0.0005 (0.0011)	
training:	Epoch: [167][50/233]	Loss 0.0022 (0.0011)	
training:	Epoch: [167][51/233]	Loss 0.0002 (0.0011)	
training:	Epoch: [167][52/233]	Loss 0.0002 (0.0011)	
training:	Epoch: [167][53/233]	Loss 0.0002 (0.0010)	
training:	Epoch: [167][54/233]	Loss 0.0019 (0.0011)	
training:	Epoch: [167][55/233]	Loss 0.0002 (0.0010)	
training:	Epoch: [167][56/233]	Loss 0.0003 (0.0010)	
training:	Epoch: [167][57/233]	Loss 0.0003 (0.0010)	
training:	Epoch: [167][58/233]	Loss 0.0002 (0.0010)	
training:	Epoch: [167][59/233]	Loss 0.0002 (0.0010)	
training:	Epoch: [167][60/233]	Loss 0.0002 (0.0010)	
training:	Epoch: [167][61/233]	Loss 0.0002 (0.0010)	
training:	Epoch: [167][62/233]	Loss 0.0007 (0.0010)	
training:	Epoch: [167][63/233]	Loss 0.0002 (0.0009)	
training:	Epoch: [167][64/233]	Loss 0.0002 (0.0009)	
training:	Epoch: [167][65/233]	Loss 0.0020 (0.0010)	
training:	Epoch: [167][66/233]	Loss 0.0003 (0.0009)	
training:	Epoch: [167][67/233]	Loss 0.0008 (0.0009)	
training:	Epoch: [167][68/233]	Loss 0.0004 (0.0009)	
training:	Epoch: [167][69/233]	Loss 0.0002 (0.0009)	
training:	Epoch: [167][70/233]	Loss 0.0003 (0.0009)	
training:	Epoch: [167][71/233]	Loss 0.0003 (0.0009)	
training:	Epoch: [167][72/233]	Loss 0.0002 (0.0009)	
training:	Epoch: [167][73/233]	Loss 0.0004 (0.0009)	
training:	Epoch: [167][74/233]	Loss 0.0003 (0.0009)	
training:	Epoch: [167][75/233]	Loss 0.0003 (0.0009)	
training:	Epoch: [167][76/233]	Loss 0.0002 (0.0009)	
training:	Epoch: [167][77/233]	Loss 0.0003 (0.0009)	
training:	Epoch: [167][78/233]	Loss 0.0007 (0.0009)	
training:	Epoch: [167][79/233]	Loss 0.0008 (0.0009)	
training:	Epoch: [167][80/233]	Loss 0.0002 (0.0008)	
training:	Epoch: [167][81/233]	Loss 0.0011 (0.0008)	
training:	Epoch: [167][82/233]	Loss 0.0002 (0.0008)	
training:	Epoch: [167][83/233]	Loss 0.0003 (0.0008)	
training:	Epoch: [167][84/233]	Loss 0.0002 (0.0008)	
training:	Epoch: [167][85/233]	Loss 0.0002 (0.0008)	
training:	Epoch: [167][86/233]	Loss 0.0003 (0.0008)	
training:	Epoch: [167][87/233]	Loss 0.0003 (0.0008)	
training:	Epoch: [167][88/233]	Loss 0.0003 (0.0008)	
training:	Epoch: [167][89/233]	Loss 0.0002 (0.0008)	
training:	Epoch: [167][90/233]	Loss 0.0004 (0.0008)	
training:	Epoch: [167][91/233]	Loss 0.0003 (0.0008)	
training:	Epoch: [167][92/233]	Loss 0.0037 (0.0008)	
training:	Epoch: [167][93/233]	Loss 0.0002 (0.0008)	
training:	Epoch: [167][94/233]	Loss 0.0005 (0.0008)	
training:	Epoch: [167][95/233]	Loss 0.0003 (0.0008)	
training:	Epoch: [167][96/233]	Loss 0.0002 (0.0008)	
training:	Epoch: [167][97/233]	Loss 0.0003 (0.0008)	
training:	Epoch: [167][98/233]	Loss 0.0002 (0.0008)	
training:	Epoch: [167][99/233]	Loss 0.0002 (0.0008)	
training:	Epoch: [167][100/233]	Loss 0.0003 (0.0008)	
training:	Epoch: [167][101/233]	Loss 0.0002 (0.0008)	
training:	Epoch: [167][102/233]	Loss 0.0003 (0.0008)	
training:	Epoch: [167][103/233]	Loss 0.0002 (0.0008)	
training:	Epoch: [167][104/233]	Loss 0.0003 (0.0008)	
training:	Epoch: [167][105/233]	Loss 0.0003 (0.0007)	
training:	Epoch: [167][106/233]	Loss 0.0003 (0.0007)	
training:	Epoch: [167][107/233]	Loss 0.0002 (0.0007)	
training:	Epoch: [167][108/233]	Loss 0.0003 (0.0007)	
training:	Epoch: [167][109/233]	Loss 0.0002 (0.0007)	
training:	Epoch: [167][110/233]	Loss 0.0002 (0.0007)	
training:	Epoch: [167][111/233]	Loss 0.0005 (0.0007)	
training:	Epoch: [167][112/233]	Loss 0.0002 (0.0007)	
training:	Epoch: [167][113/233]	Loss 0.0014 (0.0007)	
training:	Epoch: [167][114/233]	Loss 0.0006 (0.0007)	
training:	Epoch: [167][115/233]	Loss 0.0002 (0.0007)	
training:	Epoch: [167][116/233]	Loss 0.0003 (0.0007)	
training:	Epoch: [167][117/233]	Loss 0.0004 (0.0007)	
training:	Epoch: [167][118/233]	Loss 0.0194 (0.0009)	
training:	Epoch: [167][119/233]	Loss 0.0026 (0.0009)	
training:	Epoch: [167][120/233]	Loss 0.0002 (0.0009)	
training:	Epoch: [167][121/233]	Loss 0.0002 (0.0009)	
training:	Epoch: [167][122/233]	Loss 0.0003 (0.0009)	
training:	Epoch: [167][123/233]	Loss 0.0002 (0.0009)	
training:	Epoch: [167][124/233]	Loss 0.0004 (0.0009)	
training:	Epoch: [167][125/233]	Loss 0.0003 (0.0009)	
training:	Epoch: [167][126/233]	Loss 0.0007 (0.0009)	
training:	Epoch: [167][127/233]	Loss 0.0006 (0.0009)	
training:	Epoch: [167][128/233]	Loss 0.0003 (0.0008)	
training:	Epoch: [167][129/233]	Loss 0.0004 (0.0008)	
training:	Epoch: [167][130/233]	Loss 0.0004 (0.0008)	
training:	Epoch: [167][131/233]	Loss 0.0002 (0.0008)	
training:	Epoch: [167][132/233]	Loss 0.0318 (0.0011)	
training:	Epoch: [167][133/233]	Loss 0.0002 (0.0011)	
training:	Epoch: [167][134/233]	Loss 0.0002 (0.0011)	
training:	Epoch: [167][135/233]	Loss 0.0016 (0.0011)	
training:	Epoch: [167][136/233]	Loss 0.0126 (0.0011)	
training:	Epoch: [167][137/233]	Loss 0.0013 (0.0011)	
training:	Epoch: [167][138/233]	Loss 0.0004 (0.0011)	
training:	Epoch: [167][139/233]	Loss 0.0002 (0.0011)	
training:	Epoch: [167][140/233]	Loss 0.0002 (0.0011)	
training:	Epoch: [167][141/233]	Loss 0.0002 (0.0011)	
training:	Epoch: [167][142/233]	Loss 0.0002 (0.0011)	
training:	Epoch: [167][143/233]	Loss 0.0141 (0.0012)	
training:	Epoch: [167][144/233]	Loss 0.0038 (0.0012)	
training:	Epoch: [167][145/233]	Loss 0.0050 (0.0013)	
training:	Epoch: [167][146/233]	Loss 0.0012 (0.0013)	
training:	Epoch: [167][147/233]	Loss 0.0002 (0.0012)	
training:	Epoch: [167][148/233]	Loss 0.0002 (0.0012)	
training:	Epoch: [167][149/233]	Loss 0.0003 (0.0012)	
training:	Epoch: [167][150/233]	Loss 0.0002 (0.0012)	
training:	Epoch: [167][151/233]	Loss 0.0004 (0.0012)	
training:	Epoch: [167][152/233]	Loss 0.0004 (0.0012)	
training:	Epoch: [167][153/233]	Loss 0.0003 (0.0012)	
training:	Epoch: [167][154/233]	Loss 0.0003 (0.0012)	
training:	Epoch: [167][155/233]	Loss 0.0250 (0.0014)	
training:	Epoch: [167][156/233]	Loss 0.0002 (0.0013)	
training:	Epoch: [167][157/233]	Loss 0.0002 (0.0013)	
training:	Epoch: [167][158/233]	Loss 0.0026 (0.0013)	
training:	Epoch: [167][159/233]	Loss 0.0003 (0.0013)	
training:	Epoch: [167][160/233]	Loss 0.0002 (0.0013)	
training:	Epoch: [167][161/233]	Loss 0.0002 (0.0013)	
training:	Epoch: [167][162/233]	Loss 0.0005 (0.0013)	
training:	Epoch: [167][163/233]	Loss 0.0002 (0.0013)	
training:	Epoch: [167][164/233]	Loss 0.0009 (0.0013)	
training:	Epoch: [167][165/233]	Loss 0.0003 (0.0013)	
training:	Epoch: [167][166/233]	Loss 0.0003 (0.0013)	
training:	Epoch: [167][167/233]	Loss 0.0003 (0.0013)	
training:	Epoch: [167][168/233]	Loss 0.0002 (0.0013)	
training:	Epoch: [167][169/233]	Loss 0.0002 (0.0013)	
training:	Epoch: [167][170/233]	Loss 0.0007 (0.0013)	
training:	Epoch: [167][171/233]	Loss 0.0006 (0.0013)	
training:	Epoch: [167][172/233]	Loss 0.0002 (0.0013)	
training:	Epoch: [167][173/233]	Loss 0.0002 (0.0013)	
training:	Epoch: [167][174/233]	Loss 0.0010 (0.0013)	
training:	Epoch: [167][175/233]	Loss 0.0007 (0.0013)	
training:	Epoch: [167][176/233]	Loss 0.0003 (0.0012)	
training:	Epoch: [167][177/233]	Loss 0.0104 (0.0013)	
training:	Epoch: [167][178/233]	Loss 0.0008 (0.0013)	
training:	Epoch: [167][179/233]	Loss 0.0007 (0.0013)	
training:	Epoch: [167][180/233]	Loss 0.0002 (0.0013)	
training:	Epoch: [167][181/233]	Loss 0.0004 (0.0013)	
training:	Epoch: [167][182/233]	Loss 0.0005 (0.0013)	
training:	Epoch: [167][183/233]	Loss 0.0002 (0.0013)	
training:	Epoch: [167][184/233]	Loss 0.0004 (0.0013)	
training:	Epoch: [167][185/233]	Loss 0.0002 (0.0013)	
training:	Epoch: [167][186/233]	Loss 0.0003 (0.0013)	
training:	Epoch: [167][187/233]	Loss 0.0003 (0.0013)	
training:	Epoch: [167][188/233]	Loss 0.0005 (0.0013)	
training:	Epoch: [167][189/233]	Loss 0.0002 (0.0012)	
training:	Epoch: [167][190/233]	Loss 0.0003 (0.0012)	
training:	Epoch: [167][191/233]	Loss 0.0003 (0.0012)	
training:	Epoch: [167][192/233]	Loss 0.0005 (0.0012)	
training:	Epoch: [167][193/233]	Loss 0.0002 (0.0012)	
training:	Epoch: [167][194/233]	Loss 0.0005 (0.0012)	
training:	Epoch: [167][195/233]	Loss 0.0002 (0.0012)	
training:	Epoch: [167][196/233]	Loss 0.0002 (0.0012)	
training:	Epoch: [167][197/233]	Loss 0.0002 (0.0012)	
training:	Epoch: [167][198/233]	Loss 0.0003 (0.0012)	
training:	Epoch: [167][199/233]	Loss 0.0002 (0.0012)	
training:	Epoch: [167][200/233]	Loss 0.0005 (0.0012)	
training:	Epoch: [167][201/233]	Loss 0.0088 (0.0012)	
training:	Epoch: [167][202/233]	Loss 0.0004 (0.0012)	
training:	Epoch: [167][203/233]	Loss 0.0003 (0.0012)	
training:	Epoch: [167][204/233]	Loss 0.0003 (0.0012)	
training:	Epoch: [167][205/233]	Loss 0.0003 (0.0012)	
training:	Epoch: [167][206/233]	Loss 0.0008 (0.0012)	
training:	Epoch: [167][207/233]	Loss 0.0003 (0.0012)	
training:	Epoch: [167][208/233]	Loss 0.0002 (0.0012)	
training:	Epoch: [167][209/233]	Loss 0.0002 (0.0012)	
training:	Epoch: [167][210/233]	Loss 0.0002 (0.0012)	
training:	Epoch: [167][211/233]	Loss 0.0003 (0.0012)	
training:	Epoch: [167][212/233]	Loss 0.0002 (0.0012)	
training:	Epoch: [167][213/233]	Loss 0.0004 (0.0012)	
training:	Epoch: [167][214/233]	Loss 0.0002 (0.0012)	
training:	Epoch: [167][215/233]	Loss 0.0002 (0.0012)	
training:	Epoch: [167][216/233]	Loss 0.0002 (0.0012)	
training:	Epoch: [167][217/233]	Loss 0.0002 (0.0012)	
training:	Epoch: [167][218/233]	Loss 0.0019 (0.0012)	
training:	Epoch: [167][219/233]	Loss 0.0002 (0.0012)	
training:	Epoch: [167][220/233]	Loss 0.0049 (0.0012)	
training:	Epoch: [167][221/233]	Loss 0.0003 (0.0012)	
training:	Epoch: [167][222/233]	Loss 0.0003 (0.0012)	
training:	Epoch: [167][223/233]	Loss 0.0002 (0.0012)	
training:	Epoch: [167][224/233]	Loss 0.0005 (0.0012)	
training:	Epoch: [167][225/233]	Loss 0.0002 (0.0012)	
training:	Epoch: [167][226/233]	Loss 0.0004 (0.0012)	
training:	Epoch: [167][227/233]	Loss 0.0002 (0.0012)	
training:	Epoch: [167][228/233]	Loss 0.0003 (0.0011)	
training:	Epoch: [167][229/233]	Loss 0.0002 (0.0011)	
training:	Epoch: [167][230/233]	Loss 0.0005 (0.0011)	
training:	Epoch: [167][231/233]	Loss 0.0002 (0.0011)	
training:	Epoch: [167][232/233]	Loss 0.0002 (0.0011)	
training:	Epoch: [167][233/233]	Loss 0.0299 (0.0013)	
Training:	 Loss: 0.0013

Training:	 ACC: 1.0000 1.0000 1.0000 1.0000
Validation:	 ACC: 0.7885 0.7881 0.7804 0.7966
Validation:	 Best_BACC: 0.8162 0.8154 0.7998 0.8326
Validation:	 Loss: 1.3619
Pretraining:	Epoch 168/200
----------
training:	Epoch: [168][1/233]	Loss 0.0002 (0.0002)	
training:	Epoch: [168][2/233]	Loss 0.0002 (0.0002)	
training:	Epoch: [168][3/233]	Loss 0.0002 (0.0002)	
training:	Epoch: [168][4/233]	Loss 0.0002 (0.0002)	
training:	Epoch: [168][5/233]	Loss 0.0006 (0.0003)	
training:	Epoch: [168][6/233]	Loss 0.0017 (0.0005)	
training:	Epoch: [168][7/233]	Loss 0.0004 (0.0005)	
training:	Epoch: [168][8/233]	Loss 0.0002 (0.0005)	
training:	Epoch: [168][9/233]	Loss 0.0004 (0.0005)	
training:	Epoch: [168][10/233]	Loss 0.0007 (0.0005)	
training:	Epoch: [168][11/233]	Loss 0.0006 (0.0005)	
training:	Epoch: [168][12/233]	Loss 0.0002 (0.0005)	
training:	Epoch: [168][13/233]	Loss 0.0003 (0.0005)	
training:	Epoch: [168][14/233]	Loss 0.0009 (0.0005)	
training:	Epoch: [168][15/233]	Loss 0.0002 (0.0005)	
training:	Epoch: [168][16/233]	Loss 0.0005 (0.0005)	
training:	Epoch: [168][17/233]	Loss 0.0006 (0.0005)	
training:	Epoch: [168][18/233]	Loss 0.0006 (0.0005)	
training:	Epoch: [168][19/233]	Loss 0.0006 (0.0005)	
training:	Epoch: [168][20/233]	Loss 0.0003 (0.0005)	
training:	Epoch: [168][21/233]	Loss 0.0006 (0.0005)	
training:	Epoch: [168][22/233]	Loss 0.0002 (0.0005)	
training:	Epoch: [168][23/233]	Loss 0.0002 (0.0005)	
training:	Epoch: [168][24/233]	Loss 0.0030 (0.0006)	
training:	Epoch: [168][25/233]	Loss 0.0578 (0.0029)	
training:	Epoch: [168][26/233]	Loss 0.0015 (0.0028)	
training:	Epoch: [168][27/233]	Loss 0.0002 (0.0027)	
training:	Epoch: [168][28/233]	Loss 0.0003 (0.0026)	
training:	Epoch: [168][29/233]	Loss 0.0002 (0.0025)	
training:	Epoch: [168][30/233]	Loss 0.0058 (0.0026)	
training:	Epoch: [168][31/233]	Loss 0.0002 (0.0026)	
training:	Epoch: [168][32/233]	Loss 0.0002 (0.0025)	
training:	Epoch: [168][33/233]	Loss 0.0003 (0.0024)	
training:	Epoch: [168][34/233]	Loss 0.0002 (0.0024)	
training:	Epoch: [168][35/233]	Loss 0.0005 (0.0023)	
training:	Epoch: [168][36/233]	Loss 0.0002 (0.0022)	
training:	Epoch: [168][37/233]	Loss 0.0003 (0.0022)	
training:	Epoch: [168][38/233]	Loss 0.0002 (0.0021)	
training:	Epoch: [168][39/233]	Loss 0.0002 (0.0021)	
training:	Epoch: [168][40/233]	Loss 0.0003 (0.0020)	
training:	Epoch: [168][41/233]	Loss 0.0002 (0.0020)	
training:	Epoch: [168][42/233]	Loss 0.0006 (0.0020)	
training:	Epoch: [168][43/233]	Loss 0.0002 (0.0019)	
training:	Epoch: [168][44/233]	Loss 0.0003 (0.0019)	
training:	Epoch: [168][45/233]	Loss 0.0003 (0.0019)	
training:	Epoch: [168][46/233]	Loss 0.0003 (0.0018)	
training:	Epoch: [168][47/233]	Loss 0.0608 (0.0031)	
training:	Epoch: [168][48/233]	Loss 0.0196 (0.0034)	
training:	Epoch: [168][49/233]	Loss 0.0003 (0.0034)	
training:	Epoch: [168][50/233]	Loss 0.1374 (0.0060)	
training:	Epoch: [168][51/233]	Loss 0.0002 (0.0059)	
training:	Epoch: [168][52/233]	Loss 0.0002 (0.0058)	
training:	Epoch: [168][53/233]	Loss 0.0004 (0.0057)	
training:	Epoch: [168][54/233]	Loss 0.0002 (0.0056)	
training:	Epoch: [168][55/233]	Loss 0.0014 (0.0055)	
training:	Epoch: [168][56/233]	Loss 0.0005 (0.0054)	
training:	Epoch: [168][57/233]	Loss 0.0003 (0.0053)	
training:	Epoch: [168][58/233]	Loss 0.0002 (0.0053)	
training:	Epoch: [168][59/233]	Loss 0.0002 (0.0052)	
training:	Epoch: [168][60/233]	Loss 0.0004 (0.0051)	
training:	Epoch: [168][61/233]	Loss 0.0002 (0.0050)	
training:	Epoch: [168][62/233]	Loss 0.0003 (0.0049)	
training:	Epoch: [168][63/233]	Loss 0.0003 (0.0049)	
training:	Epoch: [168][64/233]	Loss 0.0004 (0.0048)	
training:	Epoch: [168][65/233]	Loss 0.0628 (0.0057)	
training:	Epoch: [168][66/233]	Loss 0.0002 (0.0056)	
training:	Epoch: [168][67/233]	Loss 0.0024 (0.0056)	
training:	Epoch: [168][68/233]	Loss 0.0002 (0.0055)	
training:	Epoch: [168][69/233]	Loss 0.0003 (0.0054)	
training:	Epoch: [168][70/233]	Loss 0.0003 (0.0053)	
training:	Epoch: [168][71/233]	Loss 0.0007 (0.0053)	
training:	Epoch: [168][72/233]	Loss 0.0002 (0.0052)	
training:	Epoch: [168][73/233]	Loss 0.0008 (0.0051)	
training:	Epoch: [168][74/233]	Loss 0.0002 (0.0051)	
training:	Epoch: [168][75/233]	Loss 0.0002 (0.0050)	
training:	Epoch: [168][76/233]	Loss 0.0004 (0.0049)	
training:	Epoch: [168][77/233]	Loss 0.0016 (0.0049)	
training:	Epoch: [168][78/233]	Loss 0.0003 (0.0048)	
training:	Epoch: [168][79/233]	Loss 0.0003 (0.0048)	
training:	Epoch: [168][80/233]	Loss 0.0003 (0.0047)	
training:	Epoch: [168][81/233]	Loss 0.0003 (0.0047)	
training:	Epoch: [168][82/233]	Loss 0.0003 (0.0046)	
training:	Epoch: [168][83/233]	Loss 0.0002 (0.0046)	
training:	Epoch: [168][84/233]	Loss 0.0002 (0.0045)	
training:	Epoch: [168][85/233]	Loss 0.0009 (0.0045)	
training:	Epoch: [168][86/233]	Loss 0.0003 (0.0044)	
training:	Epoch: [168][87/233]	Loss 0.0002 (0.0044)	
training:	Epoch: [168][88/233]	Loss 0.0003 (0.0043)	
training:	Epoch: [168][89/233]	Loss 0.0002 (0.0043)	
training:	Epoch: [168][90/233]	Loss 0.1316 (0.0057)	
training:	Epoch: [168][91/233]	Loss 0.0008 (0.0056)	
training:	Epoch: [168][92/233]	Loss 0.0002 (0.0056)	
training:	Epoch: [168][93/233]	Loss 0.0002 (0.0055)	
training:	Epoch: [168][94/233]	Loss 0.0003 (0.0055)	
training:	Epoch: [168][95/233]	Loss 0.0002 (0.0054)	
training:	Epoch: [168][96/233]	Loss 0.0002 (0.0054)	
training:	Epoch: [168][97/233]	Loss 0.0002 (0.0053)	
training:	Epoch: [168][98/233]	Loss 0.0003 (0.0053)	
training:	Epoch: [168][99/233]	Loss 0.0005 (0.0052)	
training:	Epoch: [168][100/233]	Loss 0.0004 (0.0052)	
training:	Epoch: [168][101/233]	Loss 0.0002 (0.0051)	
training:	Epoch: [168][102/233]	Loss 0.0006 (0.0051)	
training:	Epoch: [168][103/233]	Loss 0.0002 (0.0050)	
training:	Epoch: [168][104/233]	Loss 0.0003 (0.0050)	
training:	Epoch: [168][105/233]	Loss 0.0012 (0.0049)	
training:	Epoch: [168][106/233]	Loss 0.0006 (0.0049)	
training:	Epoch: [168][107/233]	Loss 0.0002 (0.0049)	
training:	Epoch: [168][108/233]	Loss 0.0002 (0.0048)	
training:	Epoch: [168][109/233]	Loss 0.0005 (0.0048)	
training:	Epoch: [168][110/233]	Loss 0.0003 (0.0047)	
training:	Epoch: [168][111/233]	Loss 0.0003 (0.0047)	
training:	Epoch: [168][112/233]	Loss 0.0032 (0.0047)	
training:	Epoch: [168][113/233]	Loss 0.0302 (0.0049)	
training:	Epoch: [168][114/233]	Loss 0.0003 (0.0049)	
training:	Epoch: [168][115/233]	Loss 0.0003 (0.0048)	
training:	Epoch: [168][116/233]	Loss 0.0003 (0.0048)	
training:	Epoch: [168][117/233]	Loss 0.0406 (0.0051)	
training:	Epoch: [168][118/233]	Loss 0.0002 (0.0050)	
training:	Epoch: [168][119/233]	Loss 0.0014 (0.0050)	
training:	Epoch: [168][120/233]	Loss 0.0019 (0.0050)	
training:	Epoch: [168][121/233]	Loss 0.0002 (0.0050)	
training:	Epoch: [168][122/233]	Loss 0.0006 (0.0049)	
training:	Epoch: [168][123/233]	Loss 0.0002 (0.0049)	
training:	Epoch: [168][124/233]	Loss 0.0029 (0.0049)	
training:	Epoch: [168][125/233]	Loss 0.0004 (0.0048)	
training:	Epoch: [168][126/233]	Loss 0.0295 (0.0050)	
training:	Epoch: [168][127/233]	Loss 0.0006 (0.0050)	
training:	Epoch: [168][128/233]	Loss 0.0002 (0.0049)	
training:	Epoch: [168][129/233]	Loss 0.0004 (0.0049)	
training:	Epoch: [168][130/233]	Loss 0.0002 (0.0049)	
training:	Epoch: [168][131/233]	Loss 0.0002 (0.0048)	
training:	Epoch: [168][132/233]	Loss 0.0002 (0.0048)	
training:	Epoch: [168][133/233]	Loss 0.0023 (0.0048)	
training:	Epoch: [168][134/233]	Loss 0.0002 (0.0048)	
training:	Epoch: [168][135/233]	Loss 0.0002 (0.0047)	
training:	Epoch: [168][136/233]	Loss 0.0003 (0.0047)	
training:	Epoch: [168][137/233]	Loss 0.0003 (0.0047)	
training:	Epoch: [168][138/233]	Loss 0.0002 (0.0046)	
training:	Epoch: [168][139/233]	Loss 0.0006 (0.0046)	
training:	Epoch: [168][140/233]	Loss 0.0195 (0.0047)	
training:	Epoch: [168][141/233]	Loss 0.0002 (0.0047)	
training:	Epoch: [168][142/233]	Loss 0.0002 (0.0046)	
training:	Epoch: [168][143/233]	Loss 0.0002 (0.0046)	
training:	Epoch: [168][144/233]	Loss 0.0002 (0.0046)	
training:	Epoch: [168][145/233]	Loss 0.0002 (0.0045)	
training:	Epoch: [168][146/233]	Loss 0.0188 (0.0046)	
training:	Epoch: [168][147/233]	Loss 0.0002 (0.0046)	
training:	Epoch: [168][148/233]	Loss 0.0226 (0.0047)	
training:	Epoch: [168][149/233]	Loss 0.0003 (0.0047)	
training:	Epoch: [168][150/233]	Loss 0.0004 (0.0047)	
training:	Epoch: [168][151/233]	Loss 0.0003 (0.0046)	
training:	Epoch: [168][152/233]	Loss 0.0004 (0.0046)	
training:	Epoch: [168][153/233]	Loss 0.0004 (0.0046)	
training:	Epoch: [168][154/233]	Loss 0.0035 (0.0046)	
training:	Epoch: [168][155/233]	Loss 0.0002 (0.0046)	
training:	Epoch: [168][156/233]	Loss 0.0024 (0.0045)	
training:	Epoch: [168][157/233]	Loss 0.0002 (0.0045)	
training:	Epoch: [168][158/233]	Loss 0.0003 (0.0045)	
training:	Epoch: [168][159/233]	Loss 0.0023 (0.0045)	
training:	Epoch: [168][160/233]	Loss 0.0010 (0.0045)	
training:	Epoch: [168][161/233]	Loss 0.0002 (0.0044)	
training:	Epoch: [168][162/233]	Loss 0.0003 (0.0044)	
training:	Epoch: [168][163/233]	Loss 0.0003 (0.0044)	
training:	Epoch: [168][164/233]	Loss 0.0003 (0.0044)	
training:	Epoch: [168][165/233]	Loss 0.0003 (0.0043)	
training:	Epoch: [168][166/233]	Loss 0.0003 (0.0043)	
training:	Epoch: [168][167/233]	Loss 0.0002 (0.0043)	
training:	Epoch: [168][168/233]	Loss 0.0002 (0.0043)	
training:	Epoch: [168][169/233]	Loss 0.0002 (0.0042)	
training:	Epoch: [168][170/233]	Loss 0.0002 (0.0042)	
training:	Epoch: [168][171/233]	Loss 0.0002 (0.0042)	
training:	Epoch: [168][172/233]	Loss 0.0013 (0.0042)	
training:	Epoch: [168][173/233]	Loss 0.0002 (0.0041)	
training:	Epoch: [168][174/233]	Loss 0.0004 (0.0041)	
training:	Epoch: [168][175/233]	Loss 0.0003 (0.0041)	
training:	Epoch: [168][176/233]	Loss 0.0035 (0.0041)	
training:	Epoch: [168][177/233]	Loss 0.0196 (0.0042)	
training:	Epoch: [168][178/233]	Loss 0.0008 (0.0042)	
training:	Epoch: [168][179/233]	Loss 0.0003 (0.0041)	
training:	Epoch: [168][180/233]	Loss 0.0005 (0.0041)	
training:	Epoch: [168][181/233]	Loss 0.0004 (0.0041)	
training:	Epoch: [168][182/233]	Loss 0.0005 (0.0041)	
training:	Epoch: [168][183/233]	Loss 0.0003 (0.0041)	
training:	Epoch: [168][184/233]	Loss 0.0002 (0.0040)	
training:	Epoch: [168][185/233]	Loss 0.0003 (0.0040)	
training:	Epoch: [168][186/233]	Loss 0.0002 (0.0040)	
training:	Epoch: [168][187/233]	Loss 0.0002 (0.0040)	
training:	Epoch: [168][188/233]	Loss 0.0004 (0.0040)	
training:	Epoch: [168][189/233]	Loss 0.0002 (0.0039)	
training:	Epoch: [168][190/233]	Loss 0.0013 (0.0039)	
training:	Epoch: [168][191/233]	Loss 0.0002 (0.0039)	
training:	Epoch: [168][192/233]	Loss 0.0002 (0.0039)	
training:	Epoch: [168][193/233]	Loss 0.0002 (0.0039)	
training:	Epoch: [168][194/233]	Loss 0.0245 (0.0040)	
training:	Epoch: [168][195/233]	Loss 0.0002 (0.0040)	
training:	Epoch: [168][196/233]	Loss 0.0006 (0.0039)	
training:	Epoch: [168][197/233]	Loss 0.0002 (0.0039)	
training:	Epoch: [168][198/233]	Loss 0.0014 (0.0039)	
training:	Epoch: [168][199/233]	Loss 0.0448 (0.0041)	
training:	Epoch: [168][200/233]	Loss 0.0003 (0.0041)	
training:	Epoch: [168][201/233]	Loss 0.0053 (0.0041)	
training:	Epoch: [168][202/233]	Loss 0.0005 (0.0041)	
training:	Epoch: [168][203/233]	Loss 0.0003 (0.0041)	
training:	Epoch: [168][204/233]	Loss 0.0002 (0.0040)	
training:	Epoch: [168][205/233]	Loss 0.0002 (0.0040)	
training:	Epoch: [168][206/233]	Loss 0.0003 (0.0040)	
training:	Epoch: [168][207/233]	Loss 0.0009 (0.0040)	
training:	Epoch: [168][208/233]	Loss 0.0003 (0.0040)	
training:	Epoch: [168][209/233]	Loss 0.0002 (0.0040)	
training:	Epoch: [168][210/233]	Loss 0.0041 (0.0040)	
training:	Epoch: [168][211/233]	Loss 0.0002 (0.0039)	
training:	Epoch: [168][212/233]	Loss 0.0002 (0.0039)	
training:	Epoch: [168][213/233]	Loss 0.0002 (0.0039)	
training:	Epoch: [168][214/233]	Loss 0.0005 (0.0039)	
training:	Epoch: [168][215/233]	Loss 0.0002 (0.0039)	
training:	Epoch: [168][216/233]	Loss 0.0002 (0.0039)	
training:	Epoch: [168][217/233]	Loss 0.0010 (0.0038)	
training:	Epoch: [168][218/233]	Loss 0.0004 (0.0038)	
training:	Epoch: [168][219/233]	Loss 0.0002 (0.0038)	
training:	Epoch: [168][220/233]	Loss 0.0083 (0.0038)	
training:	Epoch: [168][221/233]	Loss 0.0002 (0.0038)	
training:	Epoch: [168][222/233]	Loss 0.0029 (0.0038)	
training:	Epoch: [168][223/233]	Loss 0.0002 (0.0038)	
training:	Epoch: [168][224/233]	Loss 0.0015 (0.0038)	
training:	Epoch: [168][225/233]	Loss 0.0003 (0.0038)	
training:	Epoch: [168][226/233]	Loss 0.0002 (0.0038)	
training:	Epoch: [168][227/233]	Loss 0.0003 (0.0037)	
training:	Epoch: [168][228/233]	Loss 0.0007 (0.0037)	
training:	Epoch: [168][229/233]	Loss 0.0003 (0.0037)	
training:	Epoch: [168][230/233]	Loss 0.0002 (0.0037)	
training:	Epoch: [168][231/233]	Loss 0.0019 (0.0037)	
training:	Epoch: [168][232/233]	Loss 0.0002 (0.0037)	
training:	Epoch: [168][233/233]	Loss 0.0002 (0.0037)	
Training:	 Loss: 0.0036

Training:	 ACC: 1.0000 1.0000 1.0000 1.0000
Validation:	 ACC: 0.7920 0.7924 0.8008 0.7831
Validation:	 Best_BACC: 0.8162 0.8154 0.7998 0.8326
Validation:	 Loss: 1.3495
Pretraining:	Epoch 169/200
----------
training:	Epoch: [169][1/233]	Loss 0.0002 (0.0002)	
training:	Epoch: [169][2/233]	Loss 0.0002 (0.0002)	
training:	Epoch: [169][3/233]	Loss 0.0002 (0.0002)	
training:	Epoch: [169][4/233]	Loss 0.0038 (0.0011)	
training:	Epoch: [169][5/233]	Loss 0.0002 (0.0009)	
training:	Epoch: [169][6/233]	Loss 0.0003 (0.0008)	
training:	Epoch: [169][7/233]	Loss 0.0004 (0.0008)	
training:	Epoch: [169][8/233]	Loss 0.0002 (0.0007)	
training:	Epoch: [169][9/233]	Loss 0.0002 (0.0006)	
training:	Epoch: [169][10/233]	Loss 0.0002 (0.0006)	
training:	Epoch: [169][11/233]	Loss 0.0002 (0.0006)	
training:	Epoch: [169][12/233]	Loss 0.0038 (0.0008)	
training:	Epoch: [169][13/233]	Loss 0.0002 (0.0008)	
training:	Epoch: [169][14/233]	Loss 0.0004 (0.0008)	
training:	Epoch: [169][15/233]	Loss 0.0002 (0.0007)	
training:	Epoch: [169][16/233]	Loss 0.0002 (0.0007)	
training:	Epoch: [169][17/233]	Loss 0.0015 (0.0007)	
training:	Epoch: [169][18/233]	Loss 0.0003 (0.0007)	
training:	Epoch: [169][19/233]	Loss 0.0002 (0.0007)	
training:	Epoch: [169][20/233]	Loss 0.0002 (0.0007)	
training:	Epoch: [169][21/233]	Loss 0.0002 (0.0006)	
training:	Epoch: [169][22/233]	Loss 0.0007 (0.0006)	
training:	Epoch: [169][23/233]	Loss 0.0002 (0.0006)	
training:	Epoch: [169][24/233]	Loss 0.0002 (0.0006)	
training:	Epoch: [169][25/233]	Loss 0.0002 (0.0006)	
training:	Epoch: [169][26/233]	Loss 0.0010 (0.0006)	
training:	Epoch: [169][27/233]	Loss 0.0003 (0.0006)	
training:	Epoch: [169][28/233]	Loss 0.0005 (0.0006)	
training:	Epoch: [169][29/233]	Loss 0.0003 (0.0006)	
training:	Epoch: [169][30/233]	Loss 0.0004 (0.0006)	
training:	Epoch: [169][31/233]	Loss 0.0003 (0.0006)	
training:	Epoch: [169][32/233]	Loss 0.0005 (0.0006)	
training:	Epoch: [169][33/233]	Loss 0.0008 (0.0006)	
training:	Epoch: [169][34/233]	Loss 0.0003 (0.0006)	
training:	Epoch: [169][35/233]	Loss 0.0003 (0.0005)	
training:	Epoch: [169][36/233]	Loss 0.0002 (0.0005)	
training:	Epoch: [169][37/233]	Loss 0.0168 (0.0010)	
training:	Epoch: [169][38/233]	Loss 0.0004 (0.0010)	
training:	Epoch: [169][39/233]	Loss 0.0002 (0.0009)	
training:	Epoch: [169][40/233]	Loss 0.0003 (0.0009)	
training:	Epoch: [169][41/233]	Loss 0.0826 (0.0029)	
training:	Epoch: [169][42/233]	Loss 0.0002 (0.0028)	
training:	Epoch: [169][43/233]	Loss 0.0005 (0.0028)	
training:	Epoch: [169][44/233]	Loss 0.0003 (0.0027)	
training:	Epoch: [169][45/233]	Loss 0.0003 (0.0027)	
training:	Epoch: [169][46/233]	Loss 0.0005 (0.0026)	
training:	Epoch: [169][47/233]	Loss 0.0007 (0.0026)	
training:	Epoch: [169][48/233]	Loss 0.0003 (0.0025)	
training:	Epoch: [169][49/233]	Loss 0.0009 (0.0025)	
training:	Epoch: [169][50/233]	Loss 0.0002 (0.0025)	
training:	Epoch: [169][51/233]	Loss 0.0002 (0.0024)	
training:	Epoch: [169][52/233]	Loss 0.0004 (0.0024)	
training:	Epoch: [169][53/233]	Loss 0.0002 (0.0023)	
training:	Epoch: [169][54/233]	Loss 0.0003 (0.0023)	
training:	Epoch: [169][55/233]	Loss 0.0011 (0.0023)	
training:	Epoch: [169][56/233]	Loss 0.0012 (0.0023)	
training:	Epoch: [169][57/233]	Loss 0.0015 (0.0022)	
training:	Epoch: [169][58/233]	Loss 0.0097 (0.0024)	
training:	Epoch: [169][59/233]	Loss 0.0024 (0.0024)	
training:	Epoch: [169][60/233]	Loss 0.0025 (0.0024)	
training:	Epoch: [169][61/233]	Loss 0.1201 (0.0043)	
training:	Epoch: [169][62/233]	Loss 0.0010 (0.0043)	
training:	Epoch: [169][63/233]	Loss 0.0004 (0.0042)	
training:	Epoch: [169][64/233]	Loss 0.0044 (0.0042)	
training:	Epoch: [169][65/233]	Loss 0.0004 (0.0041)	
training:	Epoch: [169][66/233]	Loss 0.0002 (0.0041)	
training:	Epoch: [169][67/233]	Loss 0.0234 (0.0044)	
training:	Epoch: [169][68/233]	Loss 0.0130 (0.0045)	
training:	Epoch: [169][69/233]	Loss 0.0002 (0.0044)	
training:	Epoch: [169][70/233]	Loss 0.0002 (0.0044)	
training:	Epoch: [169][71/233]	Loss 0.0003 (0.0043)	
training:	Epoch: [169][72/233]	Loss 0.0002 (0.0043)	
training:	Epoch: [169][73/233]	Loss 0.0003 (0.0042)	
training:	Epoch: [169][74/233]	Loss 0.0003 (0.0042)	
training:	Epoch: [169][75/233]	Loss 0.0002 (0.0041)	
training:	Epoch: [169][76/233]	Loss 0.0002 (0.0040)	
training:	Epoch: [169][77/233]	Loss 0.0003 (0.0040)	
training:	Epoch: [169][78/233]	Loss 0.0005 (0.0040)	
training:	Epoch: [169][79/233]	Loss 0.0116 (0.0041)	
training:	Epoch: [169][80/233]	Loss 0.0002 (0.0040)	
training:	Epoch: [169][81/233]	Loss 0.0036 (0.0040)	
training:	Epoch: [169][82/233]	Loss 0.0006 (0.0040)	
training:	Epoch: [169][83/233]	Loss 0.0016 (0.0039)	
training:	Epoch: [169][84/233]	Loss 0.0004 (0.0039)	
training:	Epoch: [169][85/233]	Loss 0.0006 (0.0038)	
training:	Epoch: [169][86/233]	Loss 0.0002 (0.0038)	
training:	Epoch: [169][87/233]	Loss 0.0013 (0.0038)	
training:	Epoch: [169][88/233]	Loss 0.0002 (0.0037)	
training:	Epoch: [169][89/233]	Loss 0.0002 (0.0037)	
training:	Epoch: [169][90/233]	Loss 0.0002 (0.0037)	
training:	Epoch: [169][91/233]	Loss 0.0003 (0.0036)	
training:	Epoch: [169][92/233]	Loss 0.0002 (0.0036)	
training:	Epoch: [169][93/233]	Loss 0.0004 (0.0035)	
training:	Epoch: [169][94/233]	Loss 0.0003 (0.0035)	
training:	Epoch: [169][95/233]	Loss 0.0003 (0.0035)	
training:	Epoch: [169][96/233]	Loss 0.0002 (0.0034)	
training:	Epoch: [169][97/233]	Loss 0.0080 (0.0035)	
training:	Epoch: [169][98/233]	Loss 0.0014 (0.0035)	
training:	Epoch: [169][99/233]	Loss 0.0002 (0.0034)	
training:	Epoch: [169][100/233]	Loss 0.0003 (0.0034)	
training:	Epoch: [169][101/233]	Loss 0.0003 (0.0034)	
training:	Epoch: [169][102/233]	Loss 0.0005 (0.0033)	
training:	Epoch: [169][103/233]	Loss 0.0004 (0.0033)	
training:	Epoch: [169][104/233]	Loss 0.0003 (0.0033)	
training:	Epoch: [169][105/233]	Loss 0.0002 (0.0033)	
training:	Epoch: [169][106/233]	Loss 0.0004 (0.0032)	
training:	Epoch: [169][107/233]	Loss 0.0003 (0.0032)	
training:	Epoch: [169][108/233]	Loss 0.0002 (0.0032)	
training:	Epoch: [169][109/233]	Loss 0.0400 (0.0035)	
training:	Epoch: [169][110/233]	Loss 0.0004 (0.0035)	
training:	Epoch: [169][111/233]	Loss 0.0260 (0.0037)	
training:	Epoch: [169][112/233]	Loss 0.0002 (0.0037)	
training:	Epoch: [169][113/233]	Loss 0.0004 (0.0036)	
training:	Epoch: [169][114/233]	Loss 0.0002 (0.0036)	
training:	Epoch: [169][115/233]	Loss 0.0002 (0.0036)	
training:	Epoch: [169][116/233]	Loss 0.0002 (0.0035)	
training:	Epoch: [169][117/233]	Loss 0.0002 (0.0035)	
training:	Epoch: [169][118/233]	Loss 0.0002 (0.0035)	
training:	Epoch: [169][119/233]	Loss 0.0002 (0.0035)	
training:	Epoch: [169][120/233]	Loss 0.0002 (0.0034)	
training:	Epoch: [169][121/233]	Loss 0.0002 (0.0034)	
training:	Epoch: [169][122/233]	Loss 0.0003 (0.0034)	
training:	Epoch: [169][123/233]	Loss 0.0004 (0.0034)	
training:	Epoch: [169][124/233]	Loss 0.0002 (0.0033)	
training:	Epoch: [169][125/233]	Loss 0.0007 (0.0033)	
training:	Epoch: [169][126/233]	Loss 0.0002 (0.0033)	
training:	Epoch: [169][127/233]	Loss 0.0002 (0.0033)	
training:	Epoch: [169][128/233]	Loss 0.0002 (0.0032)	
training:	Epoch: [169][129/233]	Loss 0.0002 (0.0032)	
training:	Epoch: [169][130/233]	Loss 0.0002 (0.0032)	
training:	Epoch: [169][131/233]	Loss 0.0003 (0.0032)	
training:	Epoch: [169][132/233]	Loss 0.0006 (0.0031)	
training:	Epoch: [169][133/233]	Loss 0.0002 (0.0031)	
training:	Epoch: [169][134/233]	Loss 0.0004 (0.0031)	
training:	Epoch: [169][135/233]	Loss 0.0002 (0.0031)	
training:	Epoch: [169][136/233]	Loss 0.0016 (0.0031)	
training:	Epoch: [169][137/233]	Loss 0.0003 (0.0031)	
training:	Epoch: [169][138/233]	Loss 0.0002 (0.0030)	
training:	Epoch: [169][139/233]	Loss 0.0152 (0.0031)	
training:	Epoch: [169][140/233]	Loss 0.0003 (0.0031)	
training:	Epoch: [169][141/233]	Loss 0.0015 (0.0031)	
training:	Epoch: [169][142/233]	Loss 0.0006 (0.0031)	
training:	Epoch: [169][143/233]	Loss 0.0006 (0.0031)	
training:	Epoch: [169][144/233]	Loss 0.0007 (0.0030)	
training:	Epoch: [169][145/233]	Loss 0.0002 (0.0030)	
training:	Epoch: [169][146/233]	Loss 0.0002 (0.0030)	
training:	Epoch: [169][147/233]	Loss 0.0003 (0.0030)	
training:	Epoch: [169][148/233]	Loss 0.0025 (0.0030)	
training:	Epoch: [169][149/233]	Loss 0.0005 (0.0030)	
training:	Epoch: [169][150/233]	Loss 0.0002 (0.0029)	
training:	Epoch: [169][151/233]	Loss 0.0002 (0.0029)	
training:	Epoch: [169][152/233]	Loss 0.0002 (0.0029)	
training:	Epoch: [169][153/233]	Loss 0.0002 (0.0029)	
training:	Epoch: [169][154/233]	Loss 0.0089 (0.0029)	
training:	Epoch: [169][155/233]	Loss 0.0004 (0.0029)	
training:	Epoch: [169][156/233]	Loss 0.0002 (0.0029)	
training:	Epoch: [169][157/233]	Loss 0.0002 (0.0029)	
training:	Epoch: [169][158/233]	Loss 0.0002 (0.0029)	
training:	Epoch: [169][159/233]	Loss 0.0007 (0.0028)	
training:	Epoch: [169][160/233]	Loss 0.0005 (0.0028)	
training:	Epoch: [169][161/233]	Loss 0.0002 (0.0028)	
training:	Epoch: [169][162/233]	Loss 0.0002 (0.0028)	
training:	Epoch: [169][163/233]	Loss 0.0007 (0.0028)	
training:	Epoch: [169][164/233]	Loss 0.0002 (0.0028)	
training:	Epoch: [169][165/233]	Loss 0.0004 (0.0028)	
training:	Epoch: [169][166/233]	Loss 0.0002 (0.0027)	
training:	Epoch: [169][167/233]	Loss 0.0004 (0.0027)	
training:	Epoch: [169][168/233]	Loss 0.0002 (0.0027)	
training:	Epoch: [169][169/233]	Loss 0.0002 (0.0027)	
training:	Epoch: [169][170/233]	Loss 0.0009 (0.0027)	
training:	Epoch: [169][171/233]	Loss 0.0002 (0.0027)	
training:	Epoch: [169][172/233]	Loss 0.0003 (0.0027)	
training:	Epoch: [169][173/233]	Loss 0.0004 (0.0026)	
training:	Epoch: [169][174/233]	Loss 0.0002 (0.0026)	
training:	Epoch: [169][175/233]	Loss 0.0009 (0.0026)	
training:	Epoch: [169][176/233]	Loss 0.0002 (0.0026)	
training:	Epoch: [169][177/233]	Loss 0.0002 (0.0026)	
training:	Epoch: [169][178/233]	Loss 0.0002 (0.0026)	
training:	Epoch: [169][179/233]	Loss 0.0024 (0.0026)	
training:	Epoch: [169][180/233]	Loss 0.0004 (0.0026)	
training:	Epoch: [169][181/233]	Loss 0.0002 (0.0026)	
training:	Epoch: [169][182/233]	Loss 0.0002 (0.0025)	
training:	Epoch: [169][183/233]	Loss 0.0009 (0.0025)	
training:	Epoch: [169][184/233]	Loss 0.0002 (0.0025)	
training:	Epoch: [169][185/233]	Loss 0.0002 (0.0025)	
training:	Epoch: [169][186/233]	Loss 0.0002 (0.0025)	
training:	Epoch: [169][187/233]	Loss 0.0002 (0.0025)	
training:	Epoch: [169][188/233]	Loss 0.0002 (0.0025)	
training:	Epoch: [169][189/233]	Loss 0.0003 (0.0025)	
training:	Epoch: [169][190/233]	Loss 0.0003 (0.0024)	
training:	Epoch: [169][191/233]	Loss 0.0002 (0.0024)	
training:	Epoch: [169][192/233]	Loss 0.0003 (0.0024)	
training:	Epoch: [169][193/233]	Loss 0.0005 (0.0024)	
training:	Epoch: [169][194/233]	Loss 0.0003 (0.0024)	
training:	Epoch: [169][195/233]	Loss 0.0004 (0.0024)	
training:	Epoch: [169][196/233]	Loss 0.0936 (0.0029)	
training:	Epoch: [169][197/233]	Loss 0.0003 (0.0028)	
training:	Epoch: [169][198/233]	Loss 0.0002 (0.0028)	
training:	Epoch: [169][199/233]	Loss 0.0002 (0.0028)	
training:	Epoch: [169][200/233]	Loss 0.0002 (0.0028)	
training:	Epoch: [169][201/233]	Loss 0.0002 (0.0028)	
training:	Epoch: [169][202/233]	Loss 0.0003 (0.0028)	
training:	Epoch: [169][203/233]	Loss 0.0002 (0.0028)	
training:	Epoch: [169][204/233]	Loss 0.0003 (0.0028)	
training:	Epoch: [169][205/233]	Loss 0.0009 (0.0027)	
training:	Epoch: [169][206/233]	Loss 0.0007 (0.0027)	
training:	Epoch: [169][207/233]	Loss 0.0002 (0.0027)	
training:	Epoch: [169][208/233]	Loss 0.0002 (0.0027)	
training:	Epoch: [169][209/233]	Loss 0.0002 (0.0027)	
training:	Epoch: [169][210/233]	Loss 0.0005 (0.0027)	
training:	Epoch: [169][211/233]	Loss 0.0002 (0.0027)	
training:	Epoch: [169][212/233]	Loss 0.0003 (0.0027)	
training:	Epoch: [169][213/233]	Loss 0.0002 (0.0027)	
training:	Epoch: [169][214/233]	Loss 0.0002 (0.0026)	
training:	Epoch: [169][215/233]	Loss 0.0031 (0.0026)	
training:	Epoch: [169][216/233]	Loss 0.0004 (0.0026)	
training:	Epoch: [169][217/233]	Loss 0.0059 (0.0026)	
training:	Epoch: [169][218/233]	Loss 0.0002 (0.0026)	
training:	Epoch: [169][219/233]	Loss 0.0002 (0.0026)	
training:	Epoch: [169][220/233]	Loss 0.0002 (0.0026)	
training:	Epoch: [169][221/233]	Loss 0.0003 (0.0026)	
training:	Epoch: [169][222/233]	Loss 0.0003 (0.0026)	
training:	Epoch: [169][223/233]	Loss 0.0007 (0.0026)	
training:	Epoch: [169][224/233]	Loss 0.0002 (0.0026)	
training:	Epoch: [169][225/233]	Loss 0.0003 (0.0026)	
training:	Epoch: [169][226/233]	Loss 0.0002 (0.0026)	
training:	Epoch: [169][227/233]	Loss 0.0002 (0.0025)	
training:	Epoch: [169][228/233]	Loss 0.0003 (0.0025)	
training:	Epoch: [169][229/233]	Loss 0.0002 (0.0025)	
training:	Epoch: [169][230/233]	Loss 0.0002 (0.0025)	
training:	Epoch: [169][231/233]	Loss 0.0079 (0.0025)	
training:	Epoch: [169][232/233]	Loss 0.0002 (0.0025)	
training:	Epoch: [169][233/233]	Loss 0.0002 (0.0025)	
Training:	 Loss: 0.0025

Training:	 ACC: 1.0000 1.0000 1.0000 1.0000
Validation:	 ACC: 0.7890 0.7876 0.7600 0.8180
Validation:	 Best_BACC: 0.8162 0.8154 0.7998 0.8326
Validation:	 Loss: 1.3933
Pretraining:	Epoch 170/200
----------
training:	Epoch: [170][1/233]	Loss 0.0050 (0.0050)	
training:	Epoch: [170][2/233]	Loss 0.0006 (0.0028)	
training:	Epoch: [170][3/233]	Loss 0.0003 (0.0020)	
training:	Epoch: [170][4/233]	Loss 0.0003 (0.0016)	
training:	Epoch: [170][5/233]	Loss 0.0002 (0.0013)	
training:	Epoch: [170][6/233]	Loss 0.0002 (0.0011)	
training:	Epoch: [170][7/233]	Loss 0.0004 (0.0010)	
training:	Epoch: [170][8/233]	Loss 0.0002 (0.0009)	
training:	Epoch: [170][9/233]	Loss 0.0002 (0.0008)	
training:	Epoch: [170][10/233]	Loss 0.0002 (0.0008)	
training:	Epoch: [170][11/233]	Loss 0.0060 (0.0012)	
training:	Epoch: [170][12/233]	Loss 0.0006 (0.0012)	
training:	Epoch: [170][13/233]	Loss 0.0002 (0.0011)	
training:	Epoch: [170][14/233]	Loss 0.0003 (0.0010)	
training:	Epoch: [170][15/233]	Loss 0.0002 (0.0010)	
training:	Epoch: [170][16/233]	Loss 0.0002 (0.0009)	
training:	Epoch: [170][17/233]	Loss 0.0002 (0.0009)	
training:	Epoch: [170][18/233]	Loss 0.0002 (0.0009)	
training:	Epoch: [170][19/233]	Loss 0.0002 (0.0008)	
training:	Epoch: [170][20/233]	Loss 0.0002 (0.0008)	
training:	Epoch: [170][21/233]	Loss 0.0002 (0.0008)	
training:	Epoch: [170][22/233]	Loss 0.0003 (0.0007)	
training:	Epoch: [170][23/233]	Loss 0.0015 (0.0008)	
training:	Epoch: [170][24/233]	Loss 0.0003 (0.0008)	
training:	Epoch: [170][25/233]	Loss 0.0004 (0.0007)	
training:	Epoch: [170][26/233]	Loss 0.0002 (0.0007)	
training:	Epoch: [170][27/233]	Loss 0.0002 (0.0007)	
training:	Epoch: [170][28/233]	Loss 0.0002 (0.0007)	
training:	Epoch: [170][29/233]	Loss 0.0008 (0.0007)	
training:	Epoch: [170][30/233]	Loss 0.0002 (0.0007)	
training:	Epoch: [170][31/233]	Loss 0.0002 (0.0006)	
training:	Epoch: [170][32/233]	Loss 0.0002 (0.0006)	
training:	Epoch: [170][33/233]	Loss 0.0002 (0.0006)	
training:	Epoch: [170][34/233]	Loss 0.0002 (0.0006)	
training:	Epoch: [170][35/233]	Loss 0.0003 (0.0006)	
training:	Epoch: [170][36/233]	Loss 0.0002 (0.0006)	
training:	Epoch: [170][37/233]	Loss 0.0002 (0.0006)	
training:	Epoch: [170][38/233]	Loss 0.0004 (0.0006)	
training:	Epoch: [170][39/233]	Loss 0.0002 (0.0006)	
training:	Epoch: [170][40/233]	Loss 0.0011 (0.0006)	
training:	Epoch: [170][41/233]	Loss 0.0002 (0.0006)	
training:	Epoch: [170][42/233]	Loss 0.0002 (0.0006)	
training:	Epoch: [170][43/233]	Loss 0.0003 (0.0005)	
training:	Epoch: [170][44/233]	Loss 0.0001 (0.0005)	
training:	Epoch: [170][45/233]	Loss 0.0768 (0.0022)	
training:	Epoch: [170][46/233]	Loss 0.0034 (0.0023)	
training:	Epoch: [170][47/233]	Loss 0.0003 (0.0022)	
training:	Epoch: [170][48/233]	Loss 0.0002 (0.0022)	
training:	Epoch: [170][49/233]	Loss 0.0004 (0.0021)	
training:	Epoch: [170][50/233]	Loss 0.0002 (0.0021)	
training:	Epoch: [170][51/233]	Loss 0.0002 (0.0021)	
training:	Epoch: [170][52/233]	Loss 0.0002 (0.0020)	
training:	Epoch: [170][53/233]	Loss 0.0007 (0.0020)	
training:	Epoch: [170][54/233]	Loss 0.0380 (0.0027)	
training:	Epoch: [170][55/233]	Loss 0.0002 (0.0026)	
training:	Epoch: [170][56/233]	Loss 0.0002 (0.0026)	
training:	Epoch: [170][57/233]	Loss 0.0002 (0.0025)	
training:	Epoch: [170][58/233]	Loss 0.0002 (0.0025)	
training:	Epoch: [170][59/233]	Loss 0.0002 (0.0025)	
training:	Epoch: [170][60/233]	Loss 0.0002 (0.0024)	
training:	Epoch: [170][61/233]	Loss 0.0005 (0.0024)	
training:	Epoch: [170][62/233]	Loss 0.0002 (0.0024)	
training:	Epoch: [170][63/233]	Loss 0.0002 (0.0023)	
training:	Epoch: [170][64/233]	Loss 0.0002 (0.0023)	
training:	Epoch: [170][65/233]	Loss 0.0002 (0.0023)	
training:	Epoch: [170][66/233]	Loss 0.0076 (0.0023)	
training:	Epoch: [170][67/233]	Loss 0.0002 (0.0023)	
training:	Epoch: [170][68/233]	Loss 0.0003 (0.0023)	
training:	Epoch: [170][69/233]	Loss 0.0002 (0.0022)	
training:	Epoch: [170][70/233]	Loss 0.0003 (0.0022)	
training:	Epoch: [170][71/233]	Loss 0.0002 (0.0022)	
training:	Epoch: [170][72/233]	Loss 0.0002 (0.0022)	
training:	Epoch: [170][73/233]	Loss 0.0008 (0.0021)	
training:	Epoch: [170][74/233]	Loss 0.0002 (0.0021)	
training:	Epoch: [170][75/233]	Loss 0.0005 (0.0021)	
training:	Epoch: [170][76/233]	Loss 0.0003 (0.0021)	
training:	Epoch: [170][77/233]	Loss 0.0003 (0.0020)	
training:	Epoch: [170][78/233]	Loss 0.0003 (0.0020)	
training:	Epoch: [170][79/233]	Loss 0.0002 (0.0020)	
training:	Epoch: [170][80/233]	Loss 0.0007 (0.0020)	
training:	Epoch: [170][81/233]	Loss 0.0004 (0.0020)	
training:	Epoch: [170][82/233]	Loss 0.0004 (0.0019)	
training:	Epoch: [170][83/233]	Loss 0.0002 (0.0019)	
training:	Epoch: [170][84/233]	Loss 0.0002 (0.0019)	
training:	Epoch: [170][85/233]	Loss 0.0028 (0.0019)	
training:	Epoch: [170][86/233]	Loss 0.0002 (0.0019)	
training:	Epoch: [170][87/233]	Loss 0.0003 (0.0019)	
training:	Epoch: [170][88/233]	Loss 0.0003 (0.0019)	
training:	Epoch: [170][89/233]	Loss 0.0003 (0.0018)	
training:	Epoch: [170][90/233]	Loss 0.0003 (0.0018)	
training:	Epoch: [170][91/233]	Loss 0.0002 (0.0018)	
training:	Epoch: [170][92/233]	Loss 0.1285 (0.0032)	
training:	Epoch: [170][93/233]	Loss 0.0004 (0.0032)	
training:	Epoch: [170][94/233]	Loss 0.0003 (0.0031)	
training:	Epoch: [170][95/233]	Loss 0.0003 (0.0031)	
training:	Epoch: [170][96/233]	Loss 0.0017 (0.0031)	
training:	Epoch: [170][97/233]	Loss 0.0058 (0.0031)	
training:	Epoch: [170][98/233]	Loss 0.0002 (0.0031)	
training:	Epoch: [170][99/233]	Loss 0.0043 (0.0031)	
training:	Epoch: [170][100/233]	Loss 0.0002 (0.0031)	
training:	Epoch: [170][101/233]	Loss 0.0142 (0.0032)	
training:	Epoch: [170][102/233]	Loss 0.0002 (0.0031)	
training:	Epoch: [170][103/233]	Loss 0.0004 (0.0031)	
training:	Epoch: [170][104/233]	Loss 0.0009 (0.0031)	
training:	Epoch: [170][105/233]	Loss 0.0022 (0.0031)	
training:	Epoch: [170][106/233]	Loss 0.0176 (0.0032)	
training:	Epoch: [170][107/233]	Loss 0.0003 (0.0032)	
training:	Epoch: [170][108/233]	Loss 0.0004 (0.0032)	
training:	Epoch: [170][109/233]	Loss 0.0002 (0.0031)	
training:	Epoch: [170][110/233]	Loss 0.0003 (0.0031)	
training:	Epoch: [170][111/233]	Loss 0.0002 (0.0031)	
training:	Epoch: [170][112/233]	Loss 0.0001 (0.0031)	
training:	Epoch: [170][113/233]	Loss 0.0036 (0.0031)	
training:	Epoch: [170][114/233]	Loss 0.0002 (0.0030)	
training:	Epoch: [170][115/233]	Loss 0.0214 (0.0032)	
training:	Epoch: [170][116/233]	Loss 0.0004 (0.0032)	
training:	Epoch: [170][117/233]	Loss 0.0002 (0.0032)	
training:	Epoch: [170][118/233]	Loss 0.0002 (0.0031)	
training:	Epoch: [170][119/233]	Loss 0.0002 (0.0031)	
training:	Epoch: [170][120/233]	Loss 0.0008 (0.0031)	
training:	Epoch: [170][121/233]	Loss 0.0013 (0.0031)	
training:	Epoch: [170][122/233]	Loss 0.0002 (0.0030)	
training:	Epoch: [170][123/233]	Loss 0.0014 (0.0030)	
training:	Epoch: [170][124/233]	Loss 0.0002 (0.0030)	
training:	Epoch: [170][125/233]	Loss 0.0010 (0.0030)	
training:	Epoch: [170][126/233]	Loss 0.0027 (0.0030)	
training:	Epoch: [170][127/233]	Loss 0.0004 (0.0030)	
training:	Epoch: [170][128/233]	Loss 0.0006 (0.0030)	
training:	Epoch: [170][129/233]	Loss 0.0004 (0.0029)	
training:	Epoch: [170][130/233]	Loss 0.0002 (0.0029)	
training:	Epoch: [170][131/233]	Loss 0.0005 (0.0029)	
training:	Epoch: [170][132/233]	Loss 0.0003 (0.0029)	
training:	Epoch: [170][133/233]	Loss 0.0006 (0.0029)	
training:	Epoch: [170][134/233]	Loss 0.0105 (0.0029)	
training:	Epoch: [170][135/233]	Loss 0.0005 (0.0029)	
training:	Epoch: [170][136/233]	Loss 0.0002 (0.0029)	
training:	Epoch: [170][137/233]	Loss 0.0003 (0.0029)	
training:	Epoch: [170][138/233]	Loss 0.0002 (0.0028)	
training:	Epoch: [170][139/233]	Loss 0.0070 (0.0029)	
training:	Epoch: [170][140/233]	Loss 0.0002 (0.0028)	
training:	Epoch: [170][141/233]	Loss 0.0003 (0.0028)	
training:	Epoch: [170][142/233]	Loss 0.0004 (0.0028)	
training:	Epoch: [170][143/233]	Loss 0.0002 (0.0028)	
training:	Epoch: [170][144/233]	Loss 0.0002 (0.0028)	
training:	Epoch: [170][145/233]	Loss 0.0051 (0.0028)	
training:	Epoch: [170][146/233]	Loss 0.0004 (0.0028)	
training:	Epoch: [170][147/233]	Loss 0.0002 (0.0028)	
training:	Epoch: [170][148/233]	Loss 0.0002 (0.0027)	
training:	Epoch: [170][149/233]	Loss 0.0002 (0.0027)	
training:	Epoch: [170][150/233]	Loss 0.0003 (0.0027)	
training:	Epoch: [170][151/233]	Loss 0.0002 (0.0027)	
training:	Epoch: [170][152/233]	Loss 0.0003 (0.0027)	
training:	Epoch: [170][153/233]	Loss 0.0002 (0.0027)	
training:	Epoch: [170][154/233]	Loss 0.0002 (0.0026)	
training:	Epoch: [170][155/233]	Loss 0.0006 (0.0026)	
training:	Epoch: [170][156/233]	Loss 0.0002 (0.0026)	
training:	Epoch: [170][157/233]	Loss 0.0006 (0.0026)	
training:	Epoch: [170][158/233]	Loss 0.0002 (0.0026)	
training:	Epoch: [170][159/233]	Loss 0.0002 (0.0026)	
training:	Epoch: [170][160/233]	Loss 0.0002 (0.0026)	
training:	Epoch: [170][161/233]	Loss 0.0002 (0.0025)	
training:	Epoch: [170][162/233]	Loss 0.0636 (0.0029)	
training:	Epoch: [170][163/233]	Loss 0.0002 (0.0029)	
training:	Epoch: [170][164/233]	Loss 0.0002 (0.0029)	
training:	Epoch: [170][165/233]	Loss 0.0002 (0.0029)	
training:	Epoch: [170][166/233]	Loss 0.0002 (0.0029)	
training:	Epoch: [170][167/233]	Loss 0.0003 (0.0028)	
training:	Epoch: [170][168/233]	Loss 0.0002 (0.0028)	
training:	Epoch: [170][169/233]	Loss 0.0002 (0.0028)	
training:	Epoch: [170][170/233]	Loss 0.0002 (0.0028)	
training:	Epoch: [170][171/233]	Loss 0.0002 (0.0028)	
training:	Epoch: [170][172/233]	Loss 0.0002 (0.0028)	
training:	Epoch: [170][173/233]	Loss 0.0002 (0.0027)	
training:	Epoch: [170][174/233]	Loss 0.0002 (0.0027)	
training:	Epoch: [170][175/233]	Loss 0.0002 (0.0027)	
training:	Epoch: [170][176/233]	Loss 0.0002 (0.0027)	
training:	Epoch: [170][177/233]	Loss 0.0020 (0.0027)	
training:	Epoch: [170][178/233]	Loss 0.0002 (0.0027)	
training:	Epoch: [170][179/233]	Loss 0.0007 (0.0027)	
training:	Epoch: [170][180/233]	Loss 0.0006 (0.0027)	
training:	Epoch: [170][181/233]	Loss 0.0002 (0.0026)	
training:	Epoch: [170][182/233]	Loss 0.0003 (0.0026)	
training:	Epoch: [170][183/233]	Loss 0.0002 (0.0026)	
training:	Epoch: [170][184/233]	Loss 0.0003 (0.0026)	
training:	Epoch: [170][185/233]	Loss 0.0003 (0.0026)	
training:	Epoch: [170][186/233]	Loss 0.0002 (0.0026)	
training:	Epoch: [170][187/233]	Loss 0.0003 (0.0026)	
training:	Epoch: [170][188/233]	Loss 0.0002 (0.0026)	
training:	Epoch: [170][189/233]	Loss 0.0002 (0.0025)	
training:	Epoch: [170][190/233]	Loss 0.0037 (0.0026)	
training:	Epoch: [170][191/233]	Loss 0.0002 (0.0025)	
training:	Epoch: [170][192/233]	Loss 0.0003 (0.0025)	
training:	Epoch: [170][193/233]	Loss 0.0004 (0.0025)	
training:	Epoch: [170][194/233]	Loss 0.0006 (0.0025)	
training:	Epoch: [170][195/233]	Loss 0.0004 (0.0025)	
training:	Epoch: [170][196/233]	Loss 0.0004 (0.0025)	
training:	Epoch: [170][197/233]	Loss 0.0002 (0.0025)	
training:	Epoch: [170][198/233]	Loss 0.0002 (0.0025)	
training:	Epoch: [170][199/233]	Loss 0.0002 (0.0025)	
training:	Epoch: [170][200/233]	Loss 0.0003 (0.0024)	
training:	Epoch: [170][201/233]	Loss 0.0003 (0.0024)	
training:	Epoch: [170][202/233]	Loss 0.0003 (0.0024)	
training:	Epoch: [170][203/233]	Loss 0.0002 (0.0024)	
training:	Epoch: [170][204/233]	Loss 0.0008 (0.0024)	
training:	Epoch: [170][205/233]	Loss 0.0004 (0.0024)	
training:	Epoch: [170][206/233]	Loss 0.0002 (0.0024)	
training:	Epoch: [170][207/233]	Loss 0.0002 (0.0024)	
training:	Epoch: [170][208/233]	Loss 0.0009 (0.0024)	
training:	Epoch: [170][209/233]	Loss 0.0004 (0.0024)	
training:	Epoch: [170][210/233]	Loss 0.0003 (0.0023)	
training:	Epoch: [170][211/233]	Loss 0.0076 (0.0024)	
training:	Epoch: [170][212/233]	Loss 0.0002 (0.0024)	
training:	Epoch: [170][213/233]	Loss 0.0018 (0.0024)	
training:	Epoch: [170][214/233]	Loss 0.0002 (0.0023)	
training:	Epoch: [170][215/233]	Loss 0.0002 (0.0023)	
training:	Epoch: [170][216/233]	Loss 0.0003 (0.0023)	
training:	Epoch: [170][217/233]	Loss 0.0002 (0.0023)	
training:	Epoch: [170][218/233]	Loss 0.0002 (0.0023)	
training:	Epoch: [170][219/233]	Loss 0.0003 (0.0023)	
training:	Epoch: [170][220/233]	Loss 0.0002 (0.0023)	
training:	Epoch: [170][221/233]	Loss 0.0101 (0.0023)	
training:	Epoch: [170][222/233]	Loss 0.0002 (0.0023)	
training:	Epoch: [170][223/233]	Loss 0.0003 (0.0023)	
training:	Epoch: [170][224/233]	Loss 0.0002 (0.0023)	
training:	Epoch: [170][225/233]	Loss 0.0002 (0.0023)	
training:	Epoch: [170][226/233]	Loss 0.0002 (0.0023)	
training:	Epoch: [170][227/233]	Loss 0.0003 (0.0023)	
training:	Epoch: [170][228/233]	Loss 0.0002 (0.0023)	
training:	Epoch: [170][229/233]	Loss 0.0003 (0.0022)	
training:	Epoch: [170][230/233]	Loss 0.0002 (0.0022)	
training:	Epoch: [170][231/233]	Loss 0.0004 (0.0022)	
training:	Epoch: [170][232/233]	Loss 0.0002 (0.0022)	
training:	Epoch: [170][233/233]	Loss 0.0002 (0.0022)	
Training:	 Loss: 0.0022

Training:	 ACC: 1.0000 1.0000 1.0000 1.0000
Validation:	 ACC: 0.7915 0.7913 0.7886 0.7944
Validation:	 Best_BACC: 0.8162 0.8154 0.7998 0.8326
Validation:	 Loss: 1.3929
Pretraining:	Epoch 171/200
----------
training:	Epoch: [171][1/233]	Loss 0.0005 (0.0005)	
training:	Epoch: [171][2/233]	Loss 0.0002 (0.0004)	
training:	Epoch: [171][3/233]	Loss 0.0002 (0.0003)	
training:	Epoch: [171][4/233]	Loss 0.0002 (0.0003)	
training:	Epoch: [171][5/233]	Loss 0.0002 (0.0003)	
training:	Epoch: [171][6/233]	Loss 0.0002 (0.0002)	
training:	Epoch: [171][7/233]	Loss 0.0038 (0.0008)	
training:	Epoch: [171][8/233]	Loss 0.0002 (0.0007)	
training:	Epoch: [171][9/233]	Loss 0.0002 (0.0006)	
training:	Epoch: [171][10/233]	Loss 0.0003 (0.0006)	
training:	Epoch: [171][11/233]	Loss 0.0002 (0.0006)	
training:	Epoch: [171][12/233]	Loss 0.0002 (0.0005)	
training:	Epoch: [171][13/233]	Loss 0.0008 (0.0005)	
training:	Epoch: [171][14/233]	Loss 0.0011 (0.0006)	
training:	Epoch: [171][15/233]	Loss 0.0002 (0.0006)	
training:	Epoch: [171][16/233]	Loss 0.0013 (0.0006)	
training:	Epoch: [171][17/233]	Loss 0.0002 (0.0006)	
training:	Epoch: [171][18/233]	Loss 0.0003 (0.0006)	
training:	Epoch: [171][19/233]	Loss 0.0010 (0.0006)	
training:	Epoch: [171][20/233]	Loss 0.0004 (0.0006)	
training:	Epoch: [171][21/233]	Loss 0.0003 (0.0006)	
training:	Epoch: [171][22/233]	Loss 0.0159 (0.0013)	
training:	Epoch: [171][23/233]	Loss 0.0002 (0.0012)	
training:	Epoch: [171][24/233]	Loss 0.0002 (0.0012)	
training:	Epoch: [171][25/233]	Loss 0.0005 (0.0011)	
training:	Epoch: [171][26/233]	Loss 0.0002 (0.0011)	
training:	Epoch: [171][27/233]	Loss 0.0002 (0.0011)	
training:	Epoch: [171][28/233]	Loss 0.0002 (0.0010)	
training:	Epoch: [171][29/233]	Loss 0.0002 (0.0010)	
training:	Epoch: [171][30/233]	Loss 0.0066 (0.0012)	
training:	Epoch: [171][31/233]	Loss 0.0002 (0.0012)	
training:	Epoch: [171][32/233]	Loss 0.0002 (0.0011)	
training:	Epoch: [171][33/233]	Loss 0.0002 (0.0011)	
training:	Epoch: [171][34/233]	Loss 0.0002 (0.0011)	
training:	Epoch: [171][35/233]	Loss 0.0002 (0.0011)	
training:	Epoch: [171][36/233]	Loss 0.0008 (0.0010)	
training:	Epoch: [171][37/233]	Loss 0.0002 (0.0010)	
training:	Epoch: [171][38/233]	Loss 0.0002 (0.0010)	
training:	Epoch: [171][39/233]	Loss 0.0002 (0.0010)	
training:	Epoch: [171][40/233]	Loss 0.0525 (0.0023)	
training:	Epoch: [171][41/233]	Loss 0.0002 (0.0022)	
training:	Epoch: [171][42/233]	Loss 0.0002 (0.0022)	
training:	Epoch: [171][43/233]	Loss 0.0003 (0.0021)	
training:	Epoch: [171][44/233]	Loss 0.0002 (0.0021)	
training:	Epoch: [171][45/233]	Loss 0.0002 (0.0020)	
training:	Epoch: [171][46/233]	Loss 0.0002 (0.0020)	
training:	Epoch: [171][47/233]	Loss 0.0002 (0.0020)	
training:	Epoch: [171][48/233]	Loss 0.0002 (0.0019)	
training:	Epoch: [171][49/233]	Loss 0.0015 (0.0019)	
training:	Epoch: [171][50/233]	Loss 0.0002 (0.0019)	
training:	Epoch: [171][51/233]	Loss 0.0001 (0.0018)	
training:	Epoch: [171][52/233]	Loss 0.0002 (0.0018)	
training:	Epoch: [171][53/233]	Loss 0.0002 (0.0018)	
training:	Epoch: [171][54/233]	Loss 0.0006 (0.0018)	
training:	Epoch: [171][55/233]	Loss 0.0425 (0.0025)	
training:	Epoch: [171][56/233]	Loss 0.0002 (0.0025)	
training:	Epoch: [171][57/233]	Loss 0.0002 (0.0024)	
training:	Epoch: [171][58/233]	Loss 0.0002 (0.0024)	
training:	Epoch: [171][59/233]	Loss 0.0004 (0.0024)	
training:	Epoch: [171][60/233]	Loss 0.0029 (0.0024)	
training:	Epoch: [171][61/233]	Loss 0.0002 (0.0023)	
training:	Epoch: [171][62/233]	Loss 0.1066 (0.0040)	
training:	Epoch: [171][63/233]	Loss 0.0002 (0.0039)	
training:	Epoch: [171][64/233]	Loss 0.0002 (0.0039)	
training:	Epoch: [171][65/233]	Loss 0.0002 (0.0038)	
training:	Epoch: [171][66/233]	Loss 0.0006 (0.0038)	
training:	Epoch: [171][67/233]	Loss 0.0002 (0.0037)	
training:	Epoch: [171][68/233]	Loss 0.0002 (0.0037)	
training:	Epoch: [171][69/233]	Loss 0.0002 (0.0036)	
training:	Epoch: [171][70/233]	Loss 0.0002 (0.0036)	
training:	Epoch: [171][71/233]	Loss 0.0002 (0.0035)	
training:	Epoch: [171][72/233]	Loss 0.0002 (0.0035)	
training:	Epoch: [171][73/233]	Loss 0.0005 (0.0034)	
training:	Epoch: [171][74/233]	Loss 0.0011 (0.0034)	
training:	Epoch: [171][75/233]	Loss 0.0002 (0.0034)	
training:	Epoch: [171][76/233]	Loss 0.0002 (0.0033)	
training:	Epoch: [171][77/233]	Loss 0.0007 (0.0033)	
training:	Epoch: [171][78/233]	Loss 0.0022 (0.0033)	
training:	Epoch: [171][79/233]	Loss 0.0010 (0.0032)	
training:	Epoch: [171][80/233]	Loss 0.0002 (0.0032)	
training:	Epoch: [171][81/233]	Loss 0.0079 (0.0033)	
training:	Epoch: [171][82/233]	Loss 0.0002 (0.0032)	
training:	Epoch: [171][83/233]	Loss 0.0002 (0.0032)	
training:	Epoch: [171][84/233]	Loss 0.0002 (0.0032)	
training:	Epoch: [171][85/233]	Loss 0.0004 (0.0031)	
training:	Epoch: [171][86/233]	Loss 0.0002 (0.0031)	
training:	Epoch: [171][87/233]	Loss 0.0009 (0.0031)	
training:	Epoch: [171][88/233]	Loss 0.0582 (0.0037)	
training:	Epoch: [171][89/233]	Loss 0.0002 (0.0036)	
training:	Epoch: [171][90/233]	Loss 0.0002 (0.0036)	
training:	Epoch: [171][91/233]	Loss 0.0011 (0.0036)	
training:	Epoch: [171][92/233]	Loss 0.0004 (0.0035)	
training:	Epoch: [171][93/233]	Loss 0.0002 (0.0035)	
training:	Epoch: [171][94/233]	Loss 0.0003 (0.0035)	
training:	Epoch: [171][95/233]	Loss 0.0002 (0.0034)	
training:	Epoch: [171][96/233]	Loss 0.0002 (0.0034)	
training:	Epoch: [171][97/233]	Loss 0.0005 (0.0034)	
training:	Epoch: [171][98/233]	Loss 0.0002 (0.0033)	
training:	Epoch: [171][99/233]	Loss 0.0002 (0.0033)	
training:	Epoch: [171][100/233]	Loss 0.0075 (0.0034)	
training:	Epoch: [171][101/233]	Loss 0.0004 (0.0033)	
training:	Epoch: [171][102/233]	Loss 0.0002 (0.0033)	
training:	Epoch: [171][103/233]	Loss 0.0002 (0.0033)	
training:	Epoch: [171][104/233]	Loss 0.0002 (0.0032)	
training:	Epoch: [171][105/233]	Loss 0.0002 (0.0032)	
training:	Epoch: [171][106/233]	Loss 0.0010 (0.0032)	
training:	Epoch: [171][107/233]	Loss 0.0005 (0.0032)	
training:	Epoch: [171][108/233]	Loss 0.0003 (0.0031)	
training:	Epoch: [171][109/233]	Loss 0.0008 (0.0031)	
training:	Epoch: [171][110/233]	Loss 0.0002 (0.0031)	
training:	Epoch: [171][111/233]	Loss 0.0004 (0.0031)	
training:	Epoch: [171][112/233]	Loss 0.0004 (0.0030)	
training:	Epoch: [171][113/233]	Loss 0.0003 (0.0030)	
training:	Epoch: [171][114/233]	Loss 0.0138 (0.0031)	
training:	Epoch: [171][115/233]	Loss 0.0003 (0.0031)	
training:	Epoch: [171][116/233]	Loss 0.0002 (0.0031)	
training:	Epoch: [171][117/233]	Loss 0.0002 (0.0030)	
training:	Epoch: [171][118/233]	Loss 0.0002 (0.0030)	
training:	Epoch: [171][119/233]	Loss 0.0002 (0.0030)	
training:	Epoch: [171][120/233]	Loss 0.0001 (0.0030)	
training:	Epoch: [171][121/233]	Loss 0.0006 (0.0029)	
training:	Epoch: [171][122/233]	Loss 0.0003 (0.0029)	
training:	Epoch: [171][123/233]	Loss 0.0013 (0.0029)	
training:	Epoch: [171][124/233]	Loss 0.0002 (0.0029)	
training:	Epoch: [171][125/233]	Loss 0.0002 (0.0029)	
training:	Epoch: [171][126/233]	Loss 0.0002 (0.0028)	
training:	Epoch: [171][127/233]	Loss 0.0003 (0.0028)	
training:	Epoch: [171][128/233]	Loss 0.0003 (0.0028)	
training:	Epoch: [171][129/233]	Loss 0.0002 (0.0028)	
training:	Epoch: [171][130/233]	Loss 0.0003 (0.0028)	
training:	Epoch: [171][131/233]	Loss 0.0002 (0.0027)	
training:	Epoch: [171][132/233]	Loss 0.0013 (0.0027)	
training:	Epoch: [171][133/233]	Loss 0.0048 (0.0028)	
training:	Epoch: [171][134/233]	Loss 0.0003 (0.0027)	
training:	Epoch: [171][135/233]	Loss 0.0002 (0.0027)	
training:	Epoch: [171][136/233]	Loss 0.0002 (0.0027)	
training:	Epoch: [171][137/233]	Loss 0.0040 (0.0027)	
training:	Epoch: [171][138/233]	Loss 0.0002 (0.0027)	
training:	Epoch: [171][139/233]	Loss 0.0002 (0.0027)	
training:	Epoch: [171][140/233]	Loss 0.0001 (0.0027)	
training:	Epoch: [171][141/233]	Loss 0.0002 (0.0026)	
training:	Epoch: [171][142/233]	Loss 0.0002 (0.0026)	
training:	Epoch: [171][143/233]	Loss 0.0021 (0.0026)	
training:	Epoch: [171][144/233]	Loss 0.0002 (0.0026)	
training:	Epoch: [171][145/233]	Loss 0.0002 (0.0026)	
training:	Epoch: [171][146/233]	Loss 0.0002 (0.0026)	
training:	Epoch: [171][147/233]	Loss 0.0001 (0.0025)	
training:	Epoch: [171][148/233]	Loss 0.0003 (0.0025)	
training:	Epoch: [171][149/233]	Loss 0.0003 (0.0025)	
training:	Epoch: [171][150/233]	Loss 0.0002 (0.0025)	
training:	Epoch: [171][151/233]	Loss 0.0011 (0.0025)	
training:	Epoch: [171][152/233]	Loss 0.0042 (0.0025)	
training:	Epoch: [171][153/233]	Loss 0.0004 (0.0025)	
training:	Epoch: [171][154/233]	Loss 0.0015 (0.0025)	
training:	Epoch: [171][155/233]	Loss 0.0002 (0.0025)	
training:	Epoch: [171][156/233]	Loss 0.0010 (0.0025)	
training:	Epoch: [171][157/233]	Loss 0.0002 (0.0024)	
training:	Epoch: [171][158/233]	Loss 0.0002 (0.0024)	
training:	Epoch: [171][159/233]	Loss 0.0004 (0.0024)	
training:	Epoch: [171][160/233]	Loss 0.0002 (0.0024)	
training:	Epoch: [171][161/233]	Loss 0.0002 (0.0024)	
training:	Epoch: [171][162/233]	Loss 0.0002 (0.0024)	
training:	Epoch: [171][163/233]	Loss 0.0002 (0.0024)	
training:	Epoch: [171][164/233]	Loss 0.0001 (0.0023)	
training:	Epoch: [171][165/233]	Loss 0.0013 (0.0023)	
training:	Epoch: [171][166/233]	Loss 0.0005 (0.0023)	
training:	Epoch: [171][167/233]	Loss 0.0002 (0.0023)	
training:	Epoch: [171][168/233]	Loss 0.0002 (0.0023)	
training:	Epoch: [171][169/233]	Loss 0.0002 (0.0023)	
training:	Epoch: [171][170/233]	Loss 0.0007 (0.0023)	
training:	Epoch: [171][171/233]	Loss 0.0002 (0.0023)	
training:	Epoch: [171][172/233]	Loss 0.0010 (0.0023)	
training:	Epoch: [171][173/233]	Loss 0.0002 (0.0023)	
training:	Epoch: [171][174/233]	Loss 0.0010 (0.0022)	
training:	Epoch: [171][175/233]	Loss 0.0065 (0.0023)	
training:	Epoch: [171][176/233]	Loss 0.0002 (0.0023)	
training:	Epoch: [171][177/233]	Loss 0.0002 (0.0022)	
training:	Epoch: [171][178/233]	Loss 0.0002 (0.0022)	
training:	Epoch: [171][179/233]	Loss 0.0002 (0.0022)	
training:	Epoch: [171][180/233]	Loss 0.0002 (0.0022)	
training:	Epoch: [171][181/233]	Loss 0.0001 (0.0022)	
training:	Epoch: [171][182/233]	Loss 0.0004 (0.0022)	
training:	Epoch: [171][183/233]	Loss 0.0064 (0.0022)	
training:	Epoch: [171][184/233]	Loss 0.0003 (0.0022)	
training:	Epoch: [171][185/233]	Loss 0.0002 (0.0022)	
training:	Epoch: [171][186/233]	Loss 0.0016 (0.0022)	
training:	Epoch: [171][187/233]	Loss 0.0005 (0.0022)	
training:	Epoch: [171][188/233]	Loss 0.0002 (0.0022)	
training:	Epoch: [171][189/233]	Loss 0.0003 (0.0022)	
training:	Epoch: [171][190/233]	Loss 0.0002 (0.0021)	
training:	Epoch: [171][191/233]	Loss 0.0002 (0.0021)	
training:	Epoch: [171][192/233]	Loss 0.0004 (0.0021)	
training:	Epoch: [171][193/233]	Loss 0.0434 (0.0023)	
training:	Epoch: [171][194/233]	Loss 0.0002 (0.0023)	
training:	Epoch: [171][195/233]	Loss 0.0004 (0.0023)	
training:	Epoch: [171][196/233]	Loss 0.0002 (0.0023)	
training:	Epoch: [171][197/233]	Loss 0.0002 (0.0023)	
training:	Epoch: [171][198/233]	Loss 0.0012 (0.0023)	
training:	Epoch: [171][199/233]	Loss 0.0004 (0.0023)	
training:	Epoch: [171][200/233]	Loss 0.0025 (0.0023)	
training:	Epoch: [171][201/233]	Loss 0.0012 (0.0023)	
training:	Epoch: [171][202/233]	Loss 0.0011 (0.0023)	
training:	Epoch: [171][203/233]	Loss 0.0001 (0.0023)	
training:	Epoch: [171][204/233]	Loss 0.0003 (0.0023)	
training:	Epoch: [171][205/233]	Loss 0.0027 (0.0023)	
training:	Epoch: [171][206/233]	Loss 0.0002 (0.0022)	
training:	Epoch: [171][207/233]	Loss 0.0003 (0.0022)	
training:	Epoch: [171][208/233]	Loss 0.0209 (0.0023)	
training:	Epoch: [171][209/233]	Loss 0.0076 (0.0024)	
training:	Epoch: [171][210/233]	Loss 0.0073 (0.0024)	
training:	Epoch: [171][211/233]	Loss 0.0004 (0.0024)	
training:	Epoch: [171][212/233]	Loss 0.0002 (0.0024)	
training:	Epoch: [171][213/233]	Loss 0.0003 (0.0023)	
training:	Epoch: [171][214/233]	Loss 0.0009 (0.0023)	
training:	Epoch: [171][215/233]	Loss 0.0009 (0.0023)	
training:	Epoch: [171][216/233]	Loss 0.0060 (0.0023)	
training:	Epoch: [171][217/233]	Loss 0.0082 (0.0024)	
training:	Epoch: [171][218/233]	Loss 0.0002 (0.0024)	
training:	Epoch: [171][219/233]	Loss 0.0002 (0.0024)	
training:	Epoch: [171][220/233]	Loss 0.0003 (0.0023)	
training:	Epoch: [171][221/233]	Loss 0.0002 (0.0023)	
training:	Epoch: [171][222/233]	Loss 0.0002 (0.0023)	
training:	Epoch: [171][223/233]	Loss 0.0004 (0.0023)	
training:	Epoch: [171][224/233]	Loss 0.0002 (0.0023)	
training:	Epoch: [171][225/233]	Loss 0.0002 (0.0023)	
training:	Epoch: [171][226/233]	Loss 0.0004 (0.0023)	
training:	Epoch: [171][227/233]	Loss 0.0028 (0.0023)	
training:	Epoch: [171][228/233]	Loss 0.0002 (0.0023)	
training:	Epoch: [171][229/233]	Loss 0.0004 (0.0023)	
training:	Epoch: [171][230/233]	Loss 0.0002 (0.0023)	
training:	Epoch: [171][231/233]	Loss 0.0003 (0.0023)	
training:	Epoch: [171][232/233]	Loss 0.0375 (0.0024)	
training:	Epoch: [171][233/233]	Loss 0.0002 (0.0024)	
Training:	 Loss: 0.0024

Training:	 ACC: 1.0000 1.0000 1.0000 1.0000
Validation:	 ACC: 0.7940 0.7935 0.7835 0.8045
Validation:	 Best_BACC: 0.8162 0.8154 0.7998 0.8326
Validation:	 Loss: 1.3926
Pretraining:	Epoch 172/200
----------
training:	Epoch: [172][1/233]	Loss 0.0002 (0.0002)	
training:	Epoch: [172][2/233]	Loss 0.0001 (0.0002)	
training:	Epoch: [172][3/233]	Loss 0.0002 (0.0002)	
training:	Epoch: [172][4/233]	Loss 0.0002 (0.0002)	
training:	Epoch: [172][5/233]	Loss 0.0004 (0.0002)	
training:	Epoch: [172][6/233]	Loss 0.0032 (0.0007)	
training:	Epoch: [172][7/233]	Loss 0.0299 (0.0049)	
training:	Epoch: [172][8/233]	Loss 0.0001 (0.0043)	
training:	Epoch: [172][9/233]	Loss 0.0002 (0.0038)	
training:	Epoch: [172][10/233]	Loss 0.0002 (0.0035)	
training:	Epoch: [172][11/233]	Loss 0.0204 (0.0050)	
training:	Epoch: [172][12/233]	Loss 0.0001 (0.0046)	
training:	Epoch: [172][13/233]	Loss 0.0001 (0.0043)	
training:	Epoch: [172][14/233]	Loss 0.0003 (0.0040)	
training:	Epoch: [172][15/233]	Loss 0.0006 (0.0038)	
training:	Epoch: [172][16/233]	Loss 0.0002 (0.0035)	
training:	Epoch: [172][17/233]	Loss 0.0002 (0.0033)	
training:	Epoch: [172][18/233]	Loss 0.0002 (0.0032)	
training:	Epoch: [172][19/233]	Loss 0.0003 (0.0030)	
training:	Epoch: [172][20/233]	Loss 0.0002 (0.0029)	
training:	Epoch: [172][21/233]	Loss 0.0255 (0.0039)	
training:	Epoch: [172][22/233]	Loss 0.0004 (0.0038)	
training:	Epoch: [172][23/233]	Loss 0.0002 (0.0036)	
training:	Epoch: [172][24/233]	Loss 0.0002 (0.0035)	
training:	Epoch: [172][25/233]	Loss 0.0002 (0.0033)	
training:	Epoch: [172][26/233]	Loss 0.0002 (0.0032)	
training:	Epoch: [172][27/233]	Loss 0.0003 (0.0031)	
training:	Epoch: [172][28/233]	Loss 0.0001 (0.0030)	
training:	Epoch: [172][29/233]	Loss 0.0012 (0.0029)	
training:	Epoch: [172][30/233]	Loss 0.0002 (0.0029)	
training:	Epoch: [172][31/233]	Loss 0.0019 (0.0028)	
training:	Epoch: [172][32/233]	Loss 0.0008 (0.0028)	
training:	Epoch: [172][33/233]	Loss 0.0003 (0.0027)	
training:	Epoch: [172][34/233]	Loss 0.0002 (0.0026)	
training:	Epoch: [172][35/233]	Loss 0.0002 (0.0025)	
training:	Epoch: [172][36/233]	Loss 0.0001 (0.0025)	
training:	Epoch: [172][37/233]	Loss 0.0002 (0.0024)	
training:	Epoch: [172][38/233]	Loss 0.0005 (0.0024)	
training:	Epoch: [172][39/233]	Loss 0.0003 (0.0023)	
training:	Epoch: [172][40/233]	Loss 0.0002 (0.0023)	
training:	Epoch: [172][41/233]	Loss 0.0002 (0.0022)	
training:	Epoch: [172][42/233]	Loss 0.0002 (0.0022)	
training:	Epoch: [172][43/233]	Loss 0.0002 (0.0021)	
training:	Epoch: [172][44/233]	Loss 0.0002 (0.0021)	
training:	Epoch: [172][45/233]	Loss 0.0002 (0.0020)	
training:	Epoch: [172][46/233]	Loss 0.0002 (0.0020)	
training:	Epoch: [172][47/233]	Loss 0.0002 (0.0019)	
training:	Epoch: [172][48/233]	Loss 0.0002 (0.0019)	
training:	Epoch: [172][49/233]	Loss 0.0002 (0.0019)	
training:	Epoch: [172][50/233]	Loss 0.0003 (0.0018)	
training:	Epoch: [172][51/233]	Loss 0.0002 (0.0018)	
training:	Epoch: [172][52/233]	Loss 0.0002 (0.0018)	
training:	Epoch: [172][53/233]	Loss 0.0004 (0.0018)	
training:	Epoch: [172][54/233]	Loss 0.0033 (0.0018)	
training:	Epoch: [172][55/233]	Loss 0.0002 (0.0018)	
training:	Epoch: [172][56/233]	Loss 0.0004 (0.0017)	
training:	Epoch: [172][57/233]	Loss 0.0002 (0.0017)	
training:	Epoch: [172][58/233]	Loss 0.0002 (0.0017)	
training:	Epoch: [172][59/233]	Loss 0.0017 (0.0017)	
training:	Epoch: [172][60/233]	Loss 0.0006 (0.0017)	
training:	Epoch: [172][61/233]	Loss 0.0002 (0.0016)	
training:	Epoch: [172][62/233]	Loss 0.0002 (0.0016)	
training:	Epoch: [172][63/233]	Loss 0.0002 (0.0016)	
training:	Epoch: [172][64/233]	Loss 0.0022 (0.0016)	
training:	Epoch: [172][65/233]	Loss 0.0003 (0.0016)	
training:	Epoch: [172][66/233]	Loss 0.0003 (0.0016)	
training:	Epoch: [172][67/233]	Loss 0.0002 (0.0015)	
training:	Epoch: [172][68/233]	Loss 0.0002 (0.0015)	
training:	Epoch: [172][69/233]	Loss 0.0020 (0.0015)	
training:	Epoch: [172][70/233]	Loss 0.0001 (0.0015)	
training:	Epoch: [172][71/233]	Loss 0.0016 (0.0015)	
training:	Epoch: [172][72/233]	Loss 0.0049 (0.0016)	
training:	Epoch: [172][73/233]	Loss 0.0002 (0.0015)	
training:	Epoch: [172][74/233]	Loss 0.0002 (0.0015)	
training:	Epoch: [172][75/233]	Loss 0.0002 (0.0015)	
training:	Epoch: [172][76/233]	Loss 0.0091 (0.0016)	
training:	Epoch: [172][77/233]	Loss 0.0002 (0.0016)	
training:	Epoch: [172][78/233]	Loss 0.0004 (0.0016)	
training:	Epoch: [172][79/233]	Loss 0.0212 (0.0018)	
training:	Epoch: [172][80/233]	Loss 0.0005 (0.0018)	
training:	Epoch: [172][81/233]	Loss 0.0002 (0.0018)	
training:	Epoch: [172][82/233]	Loss 0.0004 (0.0018)	
training:	Epoch: [172][83/233]	Loss 0.0002 (0.0017)	
training:	Epoch: [172][84/233]	Loss 0.0017 (0.0017)	
training:	Epoch: [172][85/233]	Loss 0.0002 (0.0017)	
training:	Epoch: [172][86/233]	Loss 0.0002 (0.0017)	
training:	Epoch: [172][87/233]	Loss 0.0003 (0.0017)	
training:	Epoch: [172][88/233]	Loss 0.0353 (0.0021)	
training:	Epoch: [172][89/233]	Loss 0.0329 (0.0024)	
training:	Epoch: [172][90/233]	Loss 0.0002 (0.0024)	
training:	Epoch: [172][91/233]	Loss 0.0005 (0.0024)	
training:	Epoch: [172][92/233]	Loss 0.0004 (0.0023)	
training:	Epoch: [172][93/233]	Loss 0.0002 (0.0023)	
training:	Epoch: [172][94/233]	Loss 0.0013 (0.0023)	
training:	Epoch: [172][95/233]	Loss 0.0010 (0.0023)	
training:	Epoch: [172][96/233]	Loss 0.0002 (0.0023)	
training:	Epoch: [172][97/233]	Loss 0.0006 (0.0023)	
training:	Epoch: [172][98/233]	Loss 0.0002 (0.0022)	
training:	Epoch: [172][99/233]	Loss 0.0001 (0.0022)	
training:	Epoch: [172][100/233]	Loss 0.0002 (0.0022)	
training:	Epoch: [172][101/233]	Loss 0.0002 (0.0022)	
training:	Epoch: [172][102/233]	Loss 0.0100 (0.0023)	
training:	Epoch: [172][103/233]	Loss 0.0002 (0.0022)	
training:	Epoch: [172][104/233]	Loss 0.0002 (0.0022)	
training:	Epoch: [172][105/233]	Loss 0.0002 (0.0022)	
training:	Epoch: [172][106/233]	Loss 0.0003 (0.0022)	
training:	Epoch: [172][107/233]	Loss 0.0003 (0.0022)	
training:	Epoch: [172][108/233]	Loss 0.0007 (0.0021)	
training:	Epoch: [172][109/233]	Loss 0.0072 (0.0022)	
training:	Epoch: [172][110/233]	Loss 0.0002 (0.0022)	
training:	Epoch: [172][111/233]	Loss 0.0004 (0.0022)	
training:	Epoch: [172][112/233]	Loss 0.0017 (0.0022)	
training:	Epoch: [172][113/233]	Loss 0.0004 (0.0021)	
training:	Epoch: [172][114/233]	Loss 0.0002 (0.0021)	
training:	Epoch: [172][115/233]	Loss 0.0006 (0.0021)	
training:	Epoch: [172][116/233]	Loss 0.0002 (0.0021)	
training:	Epoch: [172][117/233]	Loss 0.0083 (0.0021)	
training:	Epoch: [172][118/233]	Loss 0.0002 (0.0021)	
training:	Epoch: [172][119/233]	Loss 0.0004 (0.0021)	
training:	Epoch: [172][120/233]	Loss 0.0002 (0.0021)	
training:	Epoch: [172][121/233]	Loss 0.0002 (0.0021)	
training:	Epoch: [172][122/233]	Loss 0.0003 (0.0021)	
training:	Epoch: [172][123/233]	Loss 0.0002 (0.0021)	
training:	Epoch: [172][124/233]	Loss 0.0023 (0.0021)	
training:	Epoch: [172][125/233]	Loss 0.0002 (0.0020)	
training:	Epoch: [172][126/233]	Loss 0.0007 (0.0020)	
training:	Epoch: [172][127/233]	Loss 0.0058 (0.0021)	
training:	Epoch: [172][128/233]	Loss 0.0002 (0.0020)	
training:	Epoch: [172][129/233]	Loss 0.0070 (0.0021)	
training:	Epoch: [172][130/233]	Loss 0.0002 (0.0021)	
training:	Epoch: [172][131/233]	Loss 0.0260 (0.0022)	
training:	Epoch: [172][132/233]	Loss 0.0002 (0.0022)	
training:	Epoch: [172][133/233]	Loss 0.0002 (0.0022)	
training:	Epoch: [172][134/233]	Loss 0.0002 (0.0022)	
training:	Epoch: [172][135/233]	Loss 0.0002 (0.0022)	
training:	Epoch: [172][136/233]	Loss 0.0002 (0.0022)	
training:	Epoch: [172][137/233]	Loss 0.0002 (0.0022)	
training:	Epoch: [172][138/233]	Loss 0.0004 (0.0021)	
training:	Epoch: [172][139/233]	Loss 0.0002 (0.0021)	
training:	Epoch: [172][140/233]	Loss 0.0002 (0.0021)	
training:	Epoch: [172][141/233]	Loss 0.0003 (0.0021)	
training:	Epoch: [172][142/233]	Loss 0.0002 (0.0021)	
training:	Epoch: [172][143/233]	Loss 0.0003 (0.0021)	
training:	Epoch: [172][144/233]	Loss 0.0002 (0.0021)	
training:	Epoch: [172][145/233]	Loss 0.0002 (0.0021)	
training:	Epoch: [172][146/233]	Loss 0.0002 (0.0020)	
training:	Epoch: [172][147/233]	Loss 0.0027 (0.0020)	
training:	Epoch: [172][148/233]	Loss 0.0002 (0.0020)	
training:	Epoch: [172][149/233]	Loss 0.0002 (0.0020)	
training:	Epoch: [172][150/233]	Loss 0.0005 (0.0020)	
training:	Epoch: [172][151/233]	Loss 0.0012 (0.0020)	
training:	Epoch: [172][152/233]	Loss 0.0003 (0.0020)	
training:	Epoch: [172][153/233]	Loss 0.0002 (0.0020)	
training:	Epoch: [172][154/233]	Loss 0.0002 (0.0020)	
training:	Epoch: [172][155/233]	Loss 0.0002 (0.0020)	
training:	Epoch: [172][156/233]	Loss 0.0207 (0.0021)	
training:	Epoch: [172][157/233]	Loss 0.0020 (0.0021)	
training:	Epoch: [172][158/233]	Loss 0.0002 (0.0021)	
training:	Epoch: [172][159/233]	Loss 0.0001 (0.0021)	
training:	Epoch: [172][160/233]	Loss 0.0002 (0.0020)	
training:	Epoch: [172][161/233]	Loss 0.0002 (0.0020)	
training:	Epoch: [172][162/233]	Loss 0.0002 (0.0020)	
training:	Epoch: [172][163/233]	Loss 0.0002 (0.0020)	
training:	Epoch: [172][164/233]	Loss 0.0002 (0.0020)	
training:	Epoch: [172][165/233]	Loss 0.0002 (0.0020)	
training:	Epoch: [172][166/233]	Loss 0.0002 (0.0020)	
training:	Epoch: [172][167/233]	Loss 0.0004 (0.0020)	
training:	Epoch: [172][168/233]	Loss 0.0002 (0.0020)	
training:	Epoch: [172][169/233]	Loss 0.0002 (0.0019)	
training:	Epoch: [172][170/233]	Loss 0.0002 (0.0019)	
training:	Epoch: [172][171/233]	Loss 0.0005 (0.0019)	
training:	Epoch: [172][172/233]	Loss 0.0005 (0.0019)	
training:	Epoch: [172][173/233]	Loss 0.0005 (0.0019)	
training:	Epoch: [172][174/233]	Loss 0.0006 (0.0019)	
training:	Epoch: [172][175/233]	Loss 0.0002 (0.0019)	
training:	Epoch: [172][176/233]	Loss 0.0002 (0.0019)	
training:	Epoch: [172][177/233]	Loss 0.0006 (0.0019)	
training:	Epoch: [172][178/233]	Loss 0.0002 (0.0019)	
training:	Epoch: [172][179/233]	Loss 0.0002 (0.0019)	
training:	Epoch: [172][180/233]	Loss 0.0137 (0.0019)	
training:	Epoch: [172][181/233]	Loss 0.0002 (0.0019)	
training:	Epoch: [172][182/233]	Loss 0.0070 (0.0019)	
training:	Epoch: [172][183/233]	Loss 0.0004 (0.0019)	
training:	Epoch: [172][184/233]	Loss 0.0037 (0.0019)	
training:	Epoch: [172][185/233]	Loss 0.0002 (0.0019)	
training:	Epoch: [172][186/233]	Loss 0.0006 (0.0019)	
training:	Epoch: [172][187/233]	Loss 0.0002 (0.0019)	
training:	Epoch: [172][188/233]	Loss 0.0004 (0.0019)	
training:	Epoch: [172][189/233]	Loss 0.0003 (0.0019)	
training:	Epoch: [172][190/233]	Loss 0.0002 (0.0019)	
training:	Epoch: [172][191/233]	Loss 0.0002 (0.0019)	
training:	Epoch: [172][192/233]	Loss 0.0002 (0.0019)	
training:	Epoch: [172][193/233]	Loss 0.0002 (0.0019)	
training:	Epoch: [172][194/233]	Loss 0.1646 (0.0027)	
training:	Epoch: [172][195/233]	Loss 0.0002 (0.0027)	
training:	Epoch: [172][196/233]	Loss 0.0001 (0.0027)	
training:	Epoch: [172][197/233]	Loss 0.0003 (0.0027)	
training:	Epoch: [172][198/233]	Loss 0.0002 (0.0027)	
training:	Epoch: [172][199/233]	Loss 0.0002 (0.0026)	
training:	Epoch: [172][200/233]	Loss 0.0002 (0.0026)	
training:	Epoch: [172][201/233]	Loss 0.0024 (0.0026)	
training:	Epoch: [172][202/233]	Loss 0.0002 (0.0026)	
training:	Epoch: [172][203/233]	Loss 0.0002 (0.0026)	
training:	Epoch: [172][204/233]	Loss 0.0010 (0.0026)	
training:	Epoch: [172][205/233]	Loss 0.0002 (0.0026)	
training:	Epoch: [172][206/233]	Loss 0.0163 (0.0026)	
training:	Epoch: [172][207/233]	Loss 0.0070 (0.0027)	
training:	Epoch: [172][208/233]	Loss 0.0002 (0.0027)	
training:	Epoch: [172][209/233]	Loss 0.0002 (0.0026)	
training:	Epoch: [172][210/233]	Loss 0.0002 (0.0026)	
training:	Epoch: [172][211/233]	Loss 0.0002 (0.0026)	
training:	Epoch: [172][212/233]	Loss 0.0005 (0.0026)	
training:	Epoch: [172][213/233]	Loss 0.0007 (0.0026)	
training:	Epoch: [172][214/233]	Loss 0.0002 (0.0026)	
training:	Epoch: [172][215/233]	Loss 0.0002 (0.0026)	
training:	Epoch: [172][216/233]	Loss 0.0057 (0.0026)	
training:	Epoch: [172][217/233]	Loss 0.0001 (0.0026)	
training:	Epoch: [172][218/233]	Loss 0.0002 (0.0026)	
training:	Epoch: [172][219/233]	Loss 0.0002 (0.0026)	
training:	Epoch: [172][220/233]	Loss 0.0002 (0.0026)	
training:	Epoch: [172][221/233]	Loss 0.0006 (0.0025)	
training:	Epoch: [172][222/233]	Loss 0.0002 (0.0025)	
training:	Epoch: [172][223/233]	Loss 0.0002 (0.0025)	
training:	Epoch: [172][224/233]	Loss 0.0002 (0.0025)	
training:	Epoch: [172][225/233]	Loss 0.0002 (0.0025)	
training:	Epoch: [172][226/233]	Loss 0.0002 (0.0025)	
training:	Epoch: [172][227/233]	Loss 0.0097 (0.0025)	
training:	Epoch: [172][228/233]	Loss 0.0003 (0.0025)	
training:	Epoch: [172][229/233]	Loss 0.0002 (0.0025)	
training:	Epoch: [172][230/233]	Loss 0.0003 (0.0025)	
training:	Epoch: [172][231/233]	Loss 0.0002 (0.0025)	
training:	Epoch: [172][232/233]	Loss 0.0024 (0.0025)	
training:	Epoch: [172][233/233]	Loss 0.0004 (0.0025)	
Training:	 Loss: 0.0025

Training:	 ACC: 0.9999 0.9999 1.0000 0.9997
Validation:	 ACC: 0.7943 0.7940 0.7886 0.8000
Validation:	 Best_BACC: 0.8162 0.8154 0.7998 0.8326
Validation:	 Loss: 1.4113
Pretraining:	Epoch 173/200
----------
training:	Epoch: [173][1/233]	Loss 0.0003 (0.0003)	
training:	Epoch: [173][2/233]	Loss 0.0002 (0.0002)	
training:	Epoch: [173][3/233]	Loss 0.0002 (0.0002)	
training:	Epoch: [173][4/233]	Loss 0.0002 (0.0002)	
training:	Epoch: [173][5/233]	Loss 0.0004 (0.0002)	
training:	Epoch: [173][6/233]	Loss 0.0002 (0.0002)	
training:	Epoch: [173][7/233]	Loss 0.0003 (0.0002)	
training:	Epoch: [173][8/233]	Loss 0.0002 (0.0002)	
training:	Epoch: [173][9/233]	Loss 0.0002 (0.0002)	
training:	Epoch: [173][10/233]	Loss 0.0001 (0.0002)	
training:	Epoch: [173][11/233]	Loss 0.0002 (0.0002)	
training:	Epoch: [173][12/233]	Loss 0.0002 (0.0002)	
training:	Epoch: [173][13/233]	Loss 0.0003 (0.0002)	
training:	Epoch: [173][14/233]	Loss 0.0002 (0.0002)	
training:	Epoch: [173][15/233]	Loss 0.0002 (0.0002)	
training:	Epoch: [173][16/233]	Loss 0.0002 (0.0002)	
training:	Epoch: [173][17/233]	Loss 0.0002 (0.0002)	
training:	Epoch: [173][18/233]	Loss 0.0005 (0.0002)	
training:	Epoch: [173][19/233]	Loss 0.0003 (0.0002)	
training:	Epoch: [173][20/233]	Loss 0.0002 (0.0002)	
training:	Epoch: [173][21/233]	Loss 0.0001 (0.0002)	
training:	Epoch: [173][22/233]	Loss 0.0097 (0.0007)	
training:	Epoch: [173][23/233]	Loss 0.0002 (0.0006)	
training:	Epoch: [173][24/233]	Loss 0.0002 (0.0006)	
training:	Epoch: [173][25/233]	Loss 0.0002 (0.0006)	
training:	Epoch: [173][26/233]	Loss 0.0009 (0.0006)	
training:	Epoch: [173][27/233]	Loss 0.0274 (0.0016)	
training:	Epoch: [173][28/233]	Loss 0.0002 (0.0016)	
training:	Epoch: [173][29/233]	Loss 0.0002 (0.0015)	
training:	Epoch: [173][30/233]	Loss 0.0001 (0.0015)	
training:	Epoch: [173][31/233]	Loss 0.0021 (0.0015)	
training:	Epoch: [173][32/233]	Loss 0.0008 (0.0015)	
training:	Epoch: [173][33/233]	Loss 0.0003 (0.0014)	
training:	Epoch: [173][34/233]	Loss 0.0042 (0.0015)	
training:	Epoch: [173][35/233]	Loss 0.0604 (0.0032)	
training:	Epoch: [173][36/233]	Loss 0.0005 (0.0031)	
training:	Epoch: [173][37/233]	Loss 0.0001 (0.0030)	
training:	Epoch: [173][38/233]	Loss 0.1686 (0.0074)	
training:	Epoch: [173][39/233]	Loss 0.0008 (0.0072)	
training:	Epoch: [173][40/233]	Loss 0.0002 (0.0071)	
training:	Epoch: [173][41/233]	Loss 0.0002 (0.0069)	
training:	Epoch: [173][42/233]	Loss 0.0002 (0.0067)	
training:	Epoch: [173][43/233]	Loss 0.0001 (0.0066)	
training:	Epoch: [173][44/233]	Loss 0.0002 (0.0064)	
training:	Epoch: [173][45/233]	Loss 0.0004 (0.0063)	
training:	Epoch: [173][46/233]	Loss 0.0003 (0.0062)	
training:	Epoch: [173][47/233]	Loss 0.0002 (0.0060)	
training:	Epoch: [173][48/233]	Loss 0.0002 (0.0059)	
training:	Epoch: [173][49/233]	Loss 0.0068 (0.0059)	
training:	Epoch: [173][50/233]	Loss 0.0015 (0.0058)	
training:	Epoch: [173][51/233]	Loss 0.0002 (0.0057)	
training:	Epoch: [173][52/233]	Loss 0.0002 (0.0056)	
training:	Epoch: [173][53/233]	Loss 0.0002 (0.0055)	
training:	Epoch: [173][54/233]	Loss 0.0002 (0.0054)	
training:	Epoch: [173][55/233]	Loss 0.0002 (0.0053)	
training:	Epoch: [173][56/233]	Loss 0.0002 (0.0052)	
training:	Epoch: [173][57/233]	Loss 0.0105 (0.0053)	
training:	Epoch: [173][58/233]	Loss 0.0003 (0.0052)	
training:	Epoch: [173][59/233]	Loss 0.0468 (0.0059)	
training:	Epoch: [173][60/233]	Loss 0.0003 (0.0059)	
training:	Epoch: [173][61/233]	Loss 0.0002 (0.0058)	
training:	Epoch: [173][62/233]	Loss 0.0002 (0.0057)	
training:	Epoch: [173][63/233]	Loss 0.1186 (0.0075)	
training:	Epoch: [173][64/233]	Loss 0.0001 (0.0073)	
training:	Epoch: [173][65/233]	Loss 0.0002 (0.0072)	
training:	Epoch: [173][66/233]	Loss 0.0002 (0.0071)	
training:	Epoch: [173][67/233]	Loss 0.0002 (0.0070)	
training:	Epoch: [173][68/233]	Loss 0.0002 (0.0069)	
training:	Epoch: [173][69/233]	Loss 0.0002 (0.0068)	
training:	Epoch: [173][70/233]	Loss 0.0724 (0.0078)	
training:	Epoch: [173][71/233]	Loss 0.0002 (0.0077)	
training:	Epoch: [173][72/233]	Loss 0.0002 (0.0076)	
training:	Epoch: [173][73/233]	Loss 0.0002 (0.0075)	
training:	Epoch: [173][74/233]	Loss 0.0002 (0.0074)	
training:	Epoch: [173][75/233]	Loss 0.0006 (0.0073)	
training:	Epoch: [173][76/233]	Loss 0.0004 (0.0072)	
training:	Epoch: [173][77/233]	Loss 0.0002 (0.0071)	
training:	Epoch: [173][78/233]	Loss 0.0002 (0.0070)	
training:	Epoch: [173][79/233]	Loss 0.0002 (0.0069)	
training:	Epoch: [173][80/233]	Loss 0.0005 (0.0068)	
training:	Epoch: [173][81/233]	Loss 0.0002 (0.0067)	
training:	Epoch: [173][82/233]	Loss 0.0002 (0.0067)	
training:	Epoch: [173][83/233]	Loss 0.0001 (0.0066)	
training:	Epoch: [173][84/233]	Loss 0.0003 (0.0065)	
training:	Epoch: [173][85/233]	Loss 0.0005 (0.0064)	
training:	Epoch: [173][86/233]	Loss 0.0002 (0.0064)	
training:	Epoch: [173][87/233]	Loss 0.0003 (0.0063)	
training:	Epoch: [173][88/233]	Loss 0.0002 (0.0062)	
training:	Epoch: [173][89/233]	Loss 0.0002 (0.0062)	
training:	Epoch: [173][90/233]	Loss 0.0005 (0.0061)	
training:	Epoch: [173][91/233]	Loss 0.0001 (0.0060)	
training:	Epoch: [173][92/233]	Loss 0.0002 (0.0060)	
training:	Epoch: [173][93/233]	Loss 0.0277 (0.0062)	
training:	Epoch: [173][94/233]	Loss 0.0002 (0.0061)	
training:	Epoch: [173][95/233]	Loss 0.0061 (0.0061)	
training:	Epoch: [173][96/233]	Loss 0.0002 (0.0061)	
training:	Epoch: [173][97/233]	Loss 0.0002 (0.0060)	
training:	Epoch: [173][98/233]	Loss 0.0019 (0.0060)	
training:	Epoch: [173][99/233]	Loss 0.0002 (0.0059)	
training:	Epoch: [173][100/233]	Loss 0.0002 (0.0059)	
training:	Epoch: [173][101/233]	Loss 0.0002 (0.0058)	
training:	Epoch: [173][102/233]	Loss 0.0002 (0.0057)	
training:	Epoch: [173][103/233]	Loss 0.0002 (0.0057)	
training:	Epoch: [173][104/233]	Loss 0.0003 (0.0056)	
training:	Epoch: [173][105/233]	Loss 0.0001 (0.0056)	
training:	Epoch: [173][106/233]	Loss 0.0004 (0.0055)	
training:	Epoch: [173][107/233]	Loss 0.0002 (0.0055)	
training:	Epoch: [173][108/233]	Loss 0.0002 (0.0054)	
training:	Epoch: [173][109/233]	Loss 0.0002 (0.0054)	
training:	Epoch: [173][110/233]	Loss 0.0737 (0.0060)	
training:	Epoch: [173][111/233]	Loss 0.0003 (0.0060)	
training:	Epoch: [173][112/233]	Loss 0.0002 (0.0059)	
training:	Epoch: [173][113/233]	Loss 0.0002 (0.0059)	
training:	Epoch: [173][114/233]	Loss 0.0003 (0.0058)	
training:	Epoch: [173][115/233]	Loss 0.0002 (0.0058)	
training:	Epoch: [173][116/233]	Loss 0.0001 (0.0057)	
training:	Epoch: [173][117/233]	Loss 0.0002 (0.0057)	
training:	Epoch: [173][118/233]	Loss 0.0002 (0.0056)	
training:	Epoch: [173][119/233]	Loss 0.0002 (0.0056)	
training:	Epoch: [173][120/233]	Loss 0.0023 (0.0055)	
training:	Epoch: [173][121/233]	Loss 0.0002 (0.0055)	
training:	Epoch: [173][122/233]	Loss 0.0002 (0.0055)	
training:	Epoch: [173][123/233]	Loss 0.0002 (0.0054)	
training:	Epoch: [173][124/233]	Loss 0.0009 (0.0054)	
training:	Epoch: [173][125/233]	Loss 0.0002 (0.0053)	
training:	Epoch: [173][126/233]	Loss 0.0001 (0.0053)	
training:	Epoch: [173][127/233]	Loss 0.0005 (0.0053)	
training:	Epoch: [173][128/233]	Loss 0.0013 (0.0052)	
training:	Epoch: [173][129/233]	Loss 0.0129 (0.0053)	
training:	Epoch: [173][130/233]	Loss 0.0002 (0.0052)	
training:	Epoch: [173][131/233]	Loss 0.0002 (0.0052)	
training:	Epoch: [173][132/233]	Loss 0.0003 (0.0052)	
training:	Epoch: [173][133/233]	Loss 0.0003 (0.0051)	
training:	Epoch: [173][134/233]	Loss 0.0002 (0.0051)	
training:	Epoch: [173][135/233]	Loss 0.0002 (0.0051)	
training:	Epoch: [173][136/233]	Loss 0.0038 (0.0051)	
training:	Epoch: [173][137/233]	Loss 0.0002 (0.0050)	
training:	Epoch: [173][138/233]	Loss 0.0004 (0.0050)	
training:	Epoch: [173][139/233]	Loss 0.0002 (0.0049)	
training:	Epoch: [173][140/233]	Loss 0.0003 (0.0049)	
training:	Epoch: [173][141/233]	Loss 0.0002 (0.0049)	
training:	Epoch: [173][142/233]	Loss 0.0002 (0.0048)	
training:	Epoch: [173][143/233]	Loss 0.0002 (0.0048)	
training:	Epoch: [173][144/233]	Loss 0.0083 (0.0048)	
training:	Epoch: [173][145/233]	Loss 0.0002 (0.0048)	
training:	Epoch: [173][146/233]	Loss 0.0002 (0.0048)	
training:	Epoch: [173][147/233]	Loss 0.0004 (0.0047)	
training:	Epoch: [173][148/233]	Loss 0.0004 (0.0047)	
training:	Epoch: [173][149/233]	Loss 0.0002 (0.0047)	
training:	Epoch: [173][150/233]	Loss 0.0043 (0.0047)	
training:	Epoch: [173][151/233]	Loss 0.0001 (0.0047)	
training:	Epoch: [173][152/233]	Loss 0.0006 (0.0046)	
training:	Epoch: [173][153/233]	Loss 0.0002 (0.0046)	
training:	Epoch: [173][154/233]	Loss 0.0003 (0.0046)	
training:	Epoch: [173][155/233]	Loss 0.0003 (0.0045)	
training:	Epoch: [173][156/233]	Loss 0.0002 (0.0045)	
training:	Epoch: [173][157/233]	Loss 0.0008 (0.0045)	
training:	Epoch: [173][158/233]	Loss 0.0002 (0.0045)	
training:	Epoch: [173][159/233]	Loss 0.0002 (0.0044)	
training:	Epoch: [173][160/233]	Loss 0.0002 (0.0044)	
training:	Epoch: [173][161/233]	Loss 0.0132 (0.0045)	
training:	Epoch: [173][162/233]	Loss 0.0002 (0.0044)	
training:	Epoch: [173][163/233]	Loss 0.0002 (0.0044)	
training:	Epoch: [173][164/233]	Loss 0.0002 (0.0044)	
training:	Epoch: [173][165/233]	Loss 0.0004 (0.0044)	
training:	Epoch: [173][166/233]	Loss 0.0002 (0.0043)	
training:	Epoch: [173][167/233]	Loss 0.0002 (0.0043)	
training:	Epoch: [173][168/233]	Loss 0.0011 (0.0043)	
training:	Epoch: [173][169/233]	Loss 0.0002 (0.0043)	
training:	Epoch: [173][170/233]	Loss 0.0001 (0.0042)	
training:	Epoch: [173][171/233]	Loss 0.0020 (0.0042)	
training:	Epoch: [173][172/233]	Loss 0.0001 (0.0042)	
training:	Epoch: [173][173/233]	Loss 0.0002 (0.0042)	
training:	Epoch: [173][174/233]	Loss 0.0002 (0.0042)	
training:	Epoch: [173][175/233]	Loss 0.0006 (0.0041)	
training:	Epoch: [173][176/233]	Loss 0.0032 (0.0041)	
training:	Epoch: [173][177/233]	Loss 0.0004 (0.0041)	
training:	Epoch: [173][178/233]	Loss 0.0012 (0.0041)	
training:	Epoch: [173][179/233]	Loss 0.0002 (0.0041)	
training:	Epoch: [173][180/233]	Loss 0.0004 (0.0041)	
training:	Epoch: [173][181/233]	Loss 0.0005 (0.0040)	
training:	Epoch: [173][182/233]	Loss 0.0002 (0.0040)	
training:	Epoch: [173][183/233]	Loss 0.0002 (0.0040)	
training:	Epoch: [173][184/233]	Loss 0.0002 (0.0040)	
training:	Epoch: [173][185/233]	Loss 0.1289 (0.0046)	
training:	Epoch: [173][186/233]	Loss 0.0002 (0.0046)	
training:	Epoch: [173][187/233]	Loss 0.0002 (0.0046)	
training:	Epoch: [173][188/233]	Loss 0.0002 (0.0046)	
training:	Epoch: [173][189/233]	Loss 0.0002 (0.0046)	
training:	Epoch: [173][190/233]	Loss 0.1353 (0.0052)	
training:	Epoch: [173][191/233]	Loss 0.0030 (0.0052)	
training:	Epoch: [173][192/233]	Loss 0.0006 (0.0052)	
training:	Epoch: [173][193/233]	Loss 0.0002 (0.0052)	
training:	Epoch: [173][194/233]	Loss 0.0004 (0.0052)	
training:	Epoch: [173][195/233]	Loss 0.0004 (0.0051)	
training:	Epoch: [173][196/233]	Loss 0.0002 (0.0051)	
training:	Epoch: [173][197/233]	Loss 0.1147 (0.0057)	
training:	Epoch: [173][198/233]	Loss 0.0002 (0.0056)	
training:	Epoch: [173][199/233]	Loss 0.0002 (0.0056)	
training:	Epoch: [173][200/233]	Loss 0.0004 (0.0056)	
training:	Epoch: [173][201/233]	Loss 0.0013 (0.0056)	
training:	Epoch: [173][202/233]	Loss 0.0002 (0.0055)	
training:	Epoch: [173][203/233]	Loss 0.0003 (0.0055)	
training:	Epoch: [173][204/233]	Loss 0.0012 (0.0055)	
training:	Epoch: [173][205/233]	Loss 0.0003 (0.0055)	
training:	Epoch: [173][206/233]	Loss 0.0029 (0.0054)	
training:	Epoch: [173][207/233]	Loss 0.0002 (0.0054)	
training:	Epoch: [173][208/233]	Loss 0.0002 (0.0054)	
training:	Epoch: [173][209/233]	Loss 0.0004 (0.0054)	
training:	Epoch: [173][210/233]	Loss 0.0005 (0.0054)	
training:	Epoch: [173][211/233]	Loss 0.0062 (0.0054)	
training:	Epoch: [173][212/233]	Loss 0.0005 (0.0053)	
training:	Epoch: [173][213/233]	Loss 0.0002 (0.0053)	
training:	Epoch: [173][214/233]	Loss 0.0002 (0.0053)	
training:	Epoch: [173][215/233]	Loss 0.0002 (0.0053)	
training:	Epoch: [173][216/233]	Loss 0.0038 (0.0053)	
training:	Epoch: [173][217/233]	Loss 0.0007 (0.0052)	
training:	Epoch: [173][218/233]	Loss 0.0002 (0.0052)	
training:	Epoch: [173][219/233]	Loss 0.0002 (0.0052)	
training:	Epoch: [173][220/233]	Loss 0.0002 (0.0052)	
training:	Epoch: [173][221/233]	Loss 0.0002 (0.0051)	
training:	Epoch: [173][222/233]	Loss 0.0092 (0.0052)	
training:	Epoch: [173][223/233]	Loss 0.0003 (0.0051)	
training:	Epoch: [173][224/233]	Loss 0.0006 (0.0051)	
training:	Epoch: [173][225/233]	Loss 0.0004 (0.0051)	
training:	Epoch: [173][226/233]	Loss 0.0002 (0.0051)	
training:	Epoch: [173][227/233]	Loss 0.0002 (0.0051)	
training:	Epoch: [173][228/233]	Loss 0.0002 (0.0050)	
training:	Epoch: [173][229/233]	Loss 0.0005 (0.0050)	
training:	Epoch: [173][230/233]	Loss 0.0002 (0.0050)	
training:	Epoch: [173][231/233]	Loss 0.0004 (0.0050)	
training:	Epoch: [173][232/233]	Loss 0.0003 (0.0050)	
training:	Epoch: [173][233/233]	Loss 0.0004 (0.0049)	
Training:	 Loss: 0.0049

Training:	 ACC: 0.9996 0.9996 0.9992 1.0000
Validation:	 ACC: 0.8008 0.7994 0.7712 0.8303
Validation:	 Best_BACC: 0.8162 0.8154 0.7998 0.8326
Validation:	 Loss: 1.3243
Pretraining:	Epoch 174/200
----------
training:	Epoch: [174][1/233]	Loss 0.0006 (0.0006)	
training:	Epoch: [174][2/233]	Loss 0.0002 (0.0004)	
training:	Epoch: [174][3/233]	Loss 0.0002 (0.0004)	
training:	Epoch: [174][4/233]	Loss 0.0003 (0.0004)	
training:	Epoch: [174][5/233]	Loss 0.0003 (0.0003)	
training:	Epoch: [174][6/233]	Loss 0.0005 (0.0004)	
training:	Epoch: [174][7/233]	Loss 0.0002 (0.0003)	
training:	Epoch: [174][8/233]	Loss 0.0002 (0.0003)	
training:	Epoch: [174][9/233]	Loss 0.0002 (0.0003)	
training:	Epoch: [174][10/233]	Loss 0.0002 (0.0003)	
training:	Epoch: [174][11/233]	Loss 0.0002 (0.0003)	
training:	Epoch: [174][12/233]	Loss 0.0003 (0.0003)	
training:	Epoch: [174][13/233]	Loss 0.0002 (0.0003)	
training:	Epoch: [174][14/233]	Loss 0.0002 (0.0003)	
training:	Epoch: [174][15/233]	Loss 0.1300 (0.0089)	
training:	Epoch: [174][16/233]	Loss 0.0218 (0.0097)	
training:	Epoch: [174][17/233]	Loss 0.0002 (0.0092)	
training:	Epoch: [174][18/233]	Loss 0.0002 (0.0087)	
training:	Epoch: [174][19/233]	Loss 0.0003 (0.0082)	
training:	Epoch: [174][20/233]	Loss 0.0002 (0.0078)	
training:	Epoch: [174][21/233]	Loss 0.0003 (0.0075)	
training:	Epoch: [174][22/233]	Loss 0.0005 (0.0072)	
training:	Epoch: [174][23/233]	Loss 0.0004 (0.0069)	
training:	Epoch: [174][24/233]	Loss 0.0007 (0.0066)	
training:	Epoch: [174][25/233]	Loss 0.0002 (0.0063)	
training:	Epoch: [174][26/233]	Loss 0.0002 (0.0061)	
training:	Epoch: [174][27/233]	Loss 0.0047 (0.0061)	
training:	Epoch: [174][28/233]	Loss 0.0003 (0.0059)	
training:	Epoch: [174][29/233]	Loss 0.0054 (0.0058)	
training:	Epoch: [174][30/233]	Loss 0.0002 (0.0057)	
training:	Epoch: [174][31/233]	Loss 0.0003 (0.0055)	
training:	Epoch: [174][32/233]	Loss 0.0002 (0.0053)	
training:	Epoch: [174][33/233]	Loss 0.0002 (0.0052)	
training:	Epoch: [174][34/233]	Loss 0.0002 (0.0050)	
training:	Epoch: [174][35/233]	Loss 0.0043 (0.0050)	
training:	Epoch: [174][36/233]	Loss 0.0002 (0.0049)	
training:	Epoch: [174][37/233]	Loss 0.0004 (0.0047)	
training:	Epoch: [174][38/233]	Loss 0.0002 (0.0046)	
training:	Epoch: [174][39/233]	Loss 0.0003 (0.0045)	
training:	Epoch: [174][40/233]	Loss 0.0002 (0.0044)	
training:	Epoch: [174][41/233]	Loss 0.0022 (0.0043)	
training:	Epoch: [174][42/233]	Loss 0.0003 (0.0042)	
training:	Epoch: [174][43/233]	Loss 0.0002 (0.0042)	
training:	Epoch: [174][44/233]	Loss 0.0002 (0.0041)	
training:	Epoch: [174][45/233]	Loss 0.0002 (0.0040)	
training:	Epoch: [174][46/233]	Loss 0.0002 (0.0039)	
training:	Epoch: [174][47/233]	Loss 0.0002 (0.0038)	
training:	Epoch: [174][48/233]	Loss 0.1351 (0.0066)	
training:	Epoch: [174][49/233]	Loss 0.0002 (0.0064)	
training:	Epoch: [174][50/233]	Loss 0.0001 (0.0063)	
training:	Epoch: [174][51/233]	Loss 0.0002 (0.0062)	
training:	Epoch: [174][52/233]	Loss 0.0002 (0.0061)	
training:	Epoch: [174][53/233]	Loss 0.0002 (0.0060)	
training:	Epoch: [174][54/233]	Loss 0.0035 (0.0059)	
training:	Epoch: [174][55/233]	Loss 0.0004 (0.0058)	
training:	Epoch: [174][56/233]	Loss 0.0002 (0.0057)	
training:	Epoch: [174][57/233]	Loss 0.0058 (0.0057)	
training:	Epoch: [174][58/233]	Loss 0.0004 (0.0056)	
training:	Epoch: [174][59/233]	Loss 0.0002 (0.0055)	
training:	Epoch: [174][60/233]	Loss 0.0004 (0.0054)	
training:	Epoch: [174][61/233]	Loss 0.0011 (0.0054)	
training:	Epoch: [174][62/233]	Loss 0.0002 (0.0053)	
training:	Epoch: [174][63/233]	Loss 0.0003 (0.0052)	
training:	Epoch: [174][64/233]	Loss 0.0002 (0.0051)	
training:	Epoch: [174][65/233]	Loss 0.0024 (0.0051)	
training:	Epoch: [174][66/233]	Loss 0.0002 (0.0050)	
training:	Epoch: [174][67/233]	Loss 0.0003 (0.0049)	
training:	Epoch: [174][68/233]	Loss 0.0002 (0.0049)	
training:	Epoch: [174][69/233]	Loss 0.0002 (0.0048)	
training:	Epoch: [174][70/233]	Loss 0.0009 (0.0047)	
training:	Epoch: [174][71/233]	Loss 0.0003 (0.0047)	
training:	Epoch: [174][72/233]	Loss 0.0003 (0.0046)	
training:	Epoch: [174][73/233]	Loss 0.0002 (0.0046)	
training:	Epoch: [174][74/233]	Loss 0.0002 (0.0045)	
training:	Epoch: [174][75/233]	Loss 0.0054 (0.0045)	
training:	Epoch: [174][76/233]	Loss 0.0003 (0.0045)	
training:	Epoch: [174][77/233]	Loss 0.0002 (0.0044)	
training:	Epoch: [174][78/233]	Loss 0.0004 (0.0044)	
training:	Epoch: [174][79/233]	Loss 0.0002 (0.0043)	
training:	Epoch: [174][80/233]	Loss 0.0002 (0.0042)	
training:	Epoch: [174][81/233]	Loss 0.0003 (0.0042)	
training:	Epoch: [174][82/233]	Loss 0.0002 (0.0041)	
training:	Epoch: [174][83/233]	Loss 0.0012 (0.0041)	
training:	Epoch: [174][84/233]	Loss 0.0010 (0.0041)	
training:	Epoch: [174][85/233]	Loss 0.0004 (0.0040)	
training:	Epoch: [174][86/233]	Loss 0.0004 (0.0040)	
training:	Epoch: [174][87/233]	Loss 0.0008 (0.0040)	
training:	Epoch: [174][88/233]	Loss 0.0003 (0.0039)	
training:	Epoch: [174][89/233]	Loss 0.0002 (0.0039)	
training:	Epoch: [174][90/233]	Loss 0.0002 (0.0038)	
training:	Epoch: [174][91/233]	Loss 0.0001 (0.0038)	
training:	Epoch: [174][92/233]	Loss 0.0030 (0.0038)	
training:	Epoch: [174][93/233]	Loss 0.0002 (0.0037)	
training:	Epoch: [174][94/233]	Loss 0.0002 (0.0037)	
training:	Epoch: [174][95/233]	Loss 0.0002 (0.0037)	
training:	Epoch: [174][96/233]	Loss 0.0003 (0.0036)	
training:	Epoch: [174][97/233]	Loss 0.0002 (0.0036)	
training:	Epoch: [174][98/233]	Loss 0.0017 (0.0036)	
training:	Epoch: [174][99/233]	Loss 0.0002 (0.0035)	
training:	Epoch: [174][100/233]	Loss 0.1073 (0.0046)	
training:	Epoch: [174][101/233]	Loss 0.0002 (0.0045)	
training:	Epoch: [174][102/233]	Loss 0.0003 (0.0045)	
training:	Epoch: [174][103/233]	Loss 0.0501 (0.0049)	
training:	Epoch: [174][104/233]	Loss 0.0002 (0.0049)	
training:	Epoch: [174][105/233]	Loss 0.0003 (0.0048)	
training:	Epoch: [174][106/233]	Loss 0.0002 (0.0048)	
training:	Epoch: [174][107/233]	Loss 0.0003 (0.0048)	
training:	Epoch: [174][108/233]	Loss 0.0002 (0.0047)	
training:	Epoch: [174][109/233]	Loss 0.0046 (0.0047)	
training:	Epoch: [174][110/233]	Loss 0.0003 (0.0047)	
training:	Epoch: [174][111/233]	Loss 0.0009 (0.0046)	
training:	Epoch: [174][112/233]	Loss 0.0002 (0.0046)	
training:	Epoch: [174][113/233]	Loss 0.0006 (0.0046)	
training:	Epoch: [174][114/233]	Loss 0.0002 (0.0045)	
training:	Epoch: [174][115/233]	Loss 0.0002 (0.0045)	
training:	Epoch: [174][116/233]	Loss 0.0002 (0.0045)	
training:	Epoch: [174][117/233]	Loss 0.0004 (0.0044)	
training:	Epoch: [174][118/233]	Loss 0.0016 (0.0044)	
training:	Epoch: [174][119/233]	Loss 0.0002 (0.0044)	
training:	Epoch: [174][120/233]	Loss 0.0007 (0.0043)	
training:	Epoch: [174][121/233]	Loss 0.0002 (0.0043)	
training:	Epoch: [174][122/233]	Loss 0.0016 (0.0043)	
training:	Epoch: [174][123/233]	Loss 0.0002 (0.0042)	
training:	Epoch: [174][124/233]	Loss 0.0003 (0.0042)	
training:	Epoch: [174][125/233]	Loss 0.0003 (0.0042)	
training:	Epoch: [174][126/233]	Loss 0.0003 (0.0041)	
training:	Epoch: [174][127/233]	Loss 0.0006 (0.0041)	
training:	Epoch: [174][128/233]	Loss 0.0002 (0.0041)	
training:	Epoch: [174][129/233]	Loss 0.0003 (0.0041)	
training:	Epoch: [174][130/233]	Loss 0.0002 (0.0040)	
training:	Epoch: [174][131/233]	Loss 0.0002 (0.0040)	
training:	Epoch: [174][132/233]	Loss 0.0045 (0.0040)	
training:	Epoch: [174][133/233]	Loss 0.0002 (0.0040)	
training:	Epoch: [174][134/233]	Loss 0.0007 (0.0040)	
training:	Epoch: [174][135/233]	Loss 0.0002 (0.0039)	
training:	Epoch: [174][136/233]	Loss 0.0003 (0.0039)	
training:	Epoch: [174][137/233]	Loss 0.0037 (0.0039)	
training:	Epoch: [174][138/233]	Loss 0.0002 (0.0039)	
training:	Epoch: [174][139/233]	Loss 0.0002 (0.0038)	
training:	Epoch: [174][140/233]	Loss 0.0001 (0.0038)	
training:	Epoch: [174][141/233]	Loss 0.0002 (0.0038)	
training:	Epoch: [174][142/233]	Loss 0.0005 (0.0038)	
training:	Epoch: [174][143/233]	Loss 0.0003 (0.0037)	
training:	Epoch: [174][144/233]	Loss 0.0002 (0.0037)	
training:	Epoch: [174][145/233]	Loss 0.0010 (0.0037)	
training:	Epoch: [174][146/233]	Loss 0.0002 (0.0037)	
training:	Epoch: [174][147/233]	Loss 0.0002 (0.0037)	
training:	Epoch: [174][148/233]	Loss 0.0004 (0.0036)	
training:	Epoch: [174][149/233]	Loss 0.0002 (0.0036)	
training:	Epoch: [174][150/233]	Loss 0.0327 (0.0038)	
training:	Epoch: [174][151/233]	Loss 0.0014 (0.0038)	
training:	Epoch: [174][152/233]	Loss 0.0004 (0.0038)	
training:	Epoch: [174][153/233]	Loss 0.0002 (0.0037)	
training:	Epoch: [174][154/233]	Loss 0.0002 (0.0037)	
training:	Epoch: [174][155/233]	Loss 0.0005 (0.0037)	
training:	Epoch: [174][156/233]	Loss 0.0003 (0.0037)	
training:	Epoch: [174][157/233]	Loss 0.0002 (0.0037)	
training:	Epoch: [174][158/233]	Loss 0.0015 (0.0036)	
training:	Epoch: [174][159/233]	Loss 0.0002 (0.0036)	
training:	Epoch: [174][160/233]	Loss 0.0002 (0.0036)	
training:	Epoch: [174][161/233]	Loss 0.0006 (0.0036)	
training:	Epoch: [174][162/233]	Loss 0.0002 (0.0036)	
training:	Epoch: [174][163/233]	Loss 0.0002 (0.0035)	
training:	Epoch: [174][164/233]	Loss 0.0002 (0.0035)	
training:	Epoch: [174][165/233]	Loss 0.0006 (0.0035)	
training:	Epoch: [174][166/233]	Loss 0.0002 (0.0035)	
training:	Epoch: [174][167/233]	Loss 0.0004 (0.0035)	
training:	Epoch: [174][168/233]	Loss 0.0002 (0.0034)	
training:	Epoch: [174][169/233]	Loss 0.0002 (0.0034)	
training:	Epoch: [174][170/233]	Loss 0.0004 (0.0034)	
training:	Epoch: [174][171/233]	Loss 0.0010 (0.0034)	
training:	Epoch: [174][172/233]	Loss 0.0002 (0.0034)	
training:	Epoch: [174][173/233]	Loss 0.0005 (0.0034)	
training:	Epoch: [174][174/233]	Loss 0.0003 (0.0033)	
training:	Epoch: [174][175/233]	Loss 0.0002 (0.0033)	
training:	Epoch: [174][176/233]	Loss 0.0003 (0.0033)	
training:	Epoch: [174][177/233]	Loss 0.0002 (0.0033)	
training:	Epoch: [174][178/233]	Loss 0.0002 (0.0033)	
training:	Epoch: [174][179/233]	Loss 0.0006 (0.0033)	
training:	Epoch: [174][180/233]	Loss 0.0003 (0.0032)	
training:	Epoch: [174][181/233]	Loss 0.0003 (0.0032)	
training:	Epoch: [174][182/233]	Loss 0.0005 (0.0032)	
training:	Epoch: [174][183/233]	Loss 0.0005 (0.0032)	
training:	Epoch: [174][184/233]	Loss 0.0002 (0.0032)	
training:	Epoch: [174][185/233]	Loss 0.0004 (0.0032)	
training:	Epoch: [174][186/233]	Loss 0.0615 (0.0035)	
training:	Epoch: [174][187/233]	Loss 0.0002 (0.0035)	
training:	Epoch: [174][188/233]	Loss 0.0002 (0.0034)	
training:	Epoch: [174][189/233]	Loss 0.0002 (0.0034)	
training:	Epoch: [174][190/233]	Loss 0.0002 (0.0034)	
training:	Epoch: [174][191/233]	Loss 0.0029 (0.0034)	
training:	Epoch: [174][192/233]	Loss 0.0002 (0.0034)	
training:	Epoch: [174][193/233]	Loss 0.0002 (0.0034)	
training:	Epoch: [174][194/233]	Loss 0.0003 (0.0034)	
training:	Epoch: [174][195/233]	Loss 0.0002 (0.0033)	
training:	Epoch: [174][196/233]	Loss 0.0003 (0.0033)	
training:	Epoch: [174][197/233]	Loss 0.0005 (0.0033)	
training:	Epoch: [174][198/233]	Loss 0.0007 (0.0033)	
training:	Epoch: [174][199/233]	Loss 0.0345 (0.0034)	
training:	Epoch: [174][200/233]	Loss 0.0002 (0.0034)	
training:	Epoch: [174][201/233]	Loss 0.0002 (0.0034)	
training:	Epoch: [174][202/233]	Loss 0.0002 (0.0034)	
training:	Epoch: [174][203/233]	Loss 0.0003 (0.0034)	
training:	Epoch: [174][204/233]	Loss 0.0002 (0.0034)	
training:	Epoch: [174][205/233]	Loss 0.0067 (0.0034)	
training:	Epoch: [174][206/233]	Loss 0.0002 (0.0034)	
training:	Epoch: [174][207/233]	Loss 0.0031 (0.0034)	
training:	Epoch: [174][208/233]	Loss 0.0002 (0.0034)	
training:	Epoch: [174][209/233]	Loss 0.0002 (0.0033)	
training:	Epoch: [174][210/233]	Loss 0.0003 (0.0033)	
training:	Epoch: [174][211/233]	Loss 0.0007 (0.0033)	
training:	Epoch: [174][212/233]	Loss 0.0002 (0.0033)	
training:	Epoch: [174][213/233]	Loss 0.0002 (0.0033)	
training:	Epoch: [174][214/233]	Loss 0.0003 (0.0033)	
training:	Epoch: [174][215/233]	Loss 0.0002 (0.0033)	
training:	Epoch: [174][216/233]	Loss 0.0055 (0.0033)	
training:	Epoch: [174][217/233]	Loss 0.0007 (0.0033)	
training:	Epoch: [174][218/233]	Loss 0.0002 (0.0032)	
training:	Epoch: [174][219/233]	Loss 0.0002 (0.0032)	
training:	Epoch: [174][220/233]	Loss 0.0002 (0.0032)	
training:	Epoch: [174][221/233]	Loss 0.0003 (0.0032)	
training:	Epoch: [174][222/233]	Loss 0.0002 (0.0032)	
training:	Epoch: [174][223/233]	Loss 0.0005 (0.0032)	
training:	Epoch: [174][224/233]	Loss 0.0003 (0.0032)	
training:	Epoch: [174][225/233]	Loss 0.0002 (0.0031)	
training:	Epoch: [174][226/233]	Loss 0.0002 (0.0031)	
training:	Epoch: [174][227/233]	Loss 0.0003 (0.0031)	
training:	Epoch: [174][228/233]	Loss 0.0003 (0.0031)	
training:	Epoch: [174][229/233]	Loss 0.0002 (0.0031)	
training:	Epoch: [174][230/233]	Loss 0.0002 (0.0031)	
training:	Epoch: [174][231/233]	Loss 0.0002 (0.0031)	
training:	Epoch: [174][232/233]	Loss 0.0019 (0.0031)	
training:	Epoch: [174][233/233]	Loss 0.0004 (0.0031)	
Training:	 Loss: 0.0030

Training:	 ACC: 1.0000 1.0000 1.0000 1.0000
Validation:	 ACC: 0.7986 0.7988 0.8029 0.7944
Validation:	 Best_BACC: 0.8162 0.8154 0.7998 0.8326
Validation:	 Loss: 1.2983
Pretraining:	Epoch 175/200
----------
training:	Epoch: [175][1/233]	Loss 0.0002 (0.0002)	
training:	Epoch: [175][2/233]	Loss 0.0005 (0.0003)	
training:	Epoch: [175][3/233]	Loss 0.0180 (0.0062)	
training:	Epoch: [175][4/233]	Loss 0.0018 (0.0051)	
training:	Epoch: [175][5/233]	Loss 0.0002 (0.0041)	
training:	Epoch: [175][6/233]	Loss 0.0002 (0.0035)	
training:	Epoch: [175][7/233]	Loss 0.0002 (0.0030)	
training:	Epoch: [175][8/233]	Loss 0.0002 (0.0026)	
training:	Epoch: [175][9/233]	Loss 0.0002 (0.0024)	
training:	Epoch: [175][10/233]	Loss 0.0024 (0.0024)	
training:	Epoch: [175][11/233]	Loss 0.0004 (0.0022)	
training:	Epoch: [175][12/233]	Loss 0.0011 (0.0021)	
training:	Epoch: [175][13/233]	Loss 0.0004 (0.0020)	
training:	Epoch: [175][14/233]	Loss 0.0046 (0.0022)	
training:	Epoch: [175][15/233]	Loss 0.0003 (0.0020)	
training:	Epoch: [175][16/233]	Loss 0.0004 (0.0019)	
training:	Epoch: [175][17/233]	Loss 0.0002 (0.0018)	
training:	Epoch: [175][18/233]	Loss 0.0002 (0.0017)	
training:	Epoch: [175][19/233]	Loss 0.0002 (0.0017)	
training:	Epoch: [175][20/233]	Loss 0.0002 (0.0016)	
training:	Epoch: [175][21/233]	Loss 0.0002 (0.0015)	
training:	Epoch: [175][22/233]	Loss 0.0063 (0.0017)	
training:	Epoch: [175][23/233]	Loss 0.0002 (0.0017)	
training:	Epoch: [175][24/233]	Loss 0.0103 (0.0020)	
training:	Epoch: [175][25/233]	Loss 0.0003 (0.0020)	
training:	Epoch: [175][26/233]	Loss 0.0002 (0.0019)	
training:	Epoch: [175][27/233]	Loss 0.0018 (0.0019)	
training:	Epoch: [175][28/233]	Loss 0.0003 (0.0018)	
training:	Epoch: [175][29/233]	Loss 0.0002 (0.0018)	
training:	Epoch: [175][30/233]	Loss 0.0002 (0.0017)	
training:	Epoch: [175][31/233]	Loss 0.0002 (0.0017)	
training:	Epoch: [175][32/233]	Loss 0.0006 (0.0016)	
training:	Epoch: [175][33/233]	Loss 0.0002 (0.0016)	
training:	Epoch: [175][34/233]	Loss 0.0002 (0.0015)	
training:	Epoch: [175][35/233]	Loss 0.0002 (0.0015)	
training:	Epoch: [175][36/233]	Loss 0.0002 (0.0015)	
training:	Epoch: [175][37/233]	Loss 0.0002 (0.0014)	
training:	Epoch: [175][38/233]	Loss 0.0002 (0.0014)	
training:	Epoch: [175][39/233]	Loss 0.0002 (0.0014)	
training:	Epoch: [175][40/233]	Loss 0.0002 (0.0013)	
training:	Epoch: [175][41/233]	Loss 0.0185 (0.0018)	
training:	Epoch: [175][42/233]	Loss 0.0002 (0.0017)	
training:	Epoch: [175][43/233]	Loss 0.0002 (0.0017)	
training:	Epoch: [175][44/233]	Loss 0.0002 (0.0017)	
training:	Epoch: [175][45/233]	Loss 0.0002 (0.0016)	
training:	Epoch: [175][46/233]	Loss 0.0002 (0.0016)	
training:	Epoch: [175][47/233]	Loss 0.0002 (0.0016)	
training:	Epoch: [175][48/233]	Loss 0.0002 (0.0015)	
training:	Epoch: [175][49/233]	Loss 0.0002 (0.0015)	
training:	Epoch: [175][50/233]	Loss 0.0002 (0.0015)	
training:	Epoch: [175][51/233]	Loss 0.0002 (0.0015)	
training:	Epoch: [175][52/233]	Loss 0.0007 (0.0014)	
training:	Epoch: [175][53/233]	Loss 0.0002 (0.0014)	
training:	Epoch: [175][54/233]	Loss 0.0002 (0.0014)	
training:	Epoch: [175][55/233]	Loss 0.0004 (0.0014)	
training:	Epoch: [175][56/233]	Loss 0.0022 (0.0014)	
training:	Epoch: [175][57/233]	Loss 0.0002 (0.0014)	
training:	Epoch: [175][58/233]	Loss 0.0002 (0.0014)	
training:	Epoch: [175][59/233]	Loss 0.0005 (0.0013)	
training:	Epoch: [175][60/233]	Loss 0.0205 (0.0017)	
training:	Epoch: [175][61/233]	Loss 0.0015 (0.0017)	
training:	Epoch: [175][62/233]	Loss 0.0002 (0.0016)	
training:	Epoch: [175][63/233]	Loss 0.0002 (0.0016)	
training:	Epoch: [175][64/233]	Loss 0.0002 (0.0016)	
training:	Epoch: [175][65/233]	Loss 0.0002 (0.0016)	
training:	Epoch: [175][66/233]	Loss 0.0002 (0.0015)	
training:	Epoch: [175][67/233]	Loss 0.0002 (0.0015)	
training:	Epoch: [175][68/233]	Loss 0.0002 (0.0015)	
training:	Epoch: [175][69/233]	Loss 0.0010 (0.0015)	
training:	Epoch: [175][70/233]	Loss 0.0003 (0.0015)	
training:	Epoch: [175][71/233]	Loss 0.0003 (0.0015)	
training:	Epoch: [175][72/233]	Loss 0.0002 (0.0014)	
training:	Epoch: [175][73/233]	Loss 0.0002 (0.0014)	
training:	Epoch: [175][74/233]	Loss 0.0021 (0.0014)	
training:	Epoch: [175][75/233]	Loss 0.0002 (0.0014)	
training:	Epoch: [175][76/233]	Loss 0.0002 (0.0014)	
training:	Epoch: [175][77/233]	Loss 0.0001 (0.0014)	
training:	Epoch: [175][78/233]	Loss 0.0008 (0.0014)	
training:	Epoch: [175][79/233]	Loss 0.0002 (0.0014)	
training:	Epoch: [175][80/233]	Loss 0.0002 (0.0014)	
training:	Epoch: [175][81/233]	Loss 0.0002 (0.0013)	
training:	Epoch: [175][82/233]	Loss 0.0009 (0.0013)	
training:	Epoch: [175][83/233]	Loss 0.0005 (0.0013)	
training:	Epoch: [175][84/233]	Loss 0.0051 (0.0014)	
training:	Epoch: [175][85/233]	Loss 0.0002 (0.0014)	
training:	Epoch: [175][86/233]	Loss 0.0002 (0.0013)	
training:	Epoch: [175][87/233]	Loss 0.0006 (0.0013)	
training:	Epoch: [175][88/233]	Loss 0.0017 (0.0013)	
training:	Epoch: [175][89/233]	Loss 0.0002 (0.0013)	
training:	Epoch: [175][90/233]	Loss 0.0002 (0.0013)	
training:	Epoch: [175][91/233]	Loss 0.0002 (0.0013)	
training:	Epoch: [175][92/233]	Loss 0.0002 (0.0013)	
training:	Epoch: [175][93/233]	Loss 0.0008 (0.0013)	
training:	Epoch: [175][94/233]	Loss 0.0003 (0.0013)	
training:	Epoch: [175][95/233]	Loss 0.0002 (0.0013)	
training:	Epoch: [175][96/233]	Loss 0.0002 (0.0012)	
training:	Epoch: [175][97/233]	Loss 0.0013 (0.0012)	
training:	Epoch: [175][98/233]	Loss 0.0190 (0.0014)	
training:	Epoch: [175][99/233]	Loss 0.0002 (0.0014)	
training:	Epoch: [175][100/233]	Loss 0.0066 (0.0015)	
training:	Epoch: [175][101/233]	Loss 0.0002 (0.0015)	
training:	Epoch: [175][102/233]	Loss 0.0003 (0.0014)	
training:	Epoch: [175][103/233]	Loss 0.0002 (0.0014)	
training:	Epoch: [175][104/233]	Loss 0.0042 (0.0015)	
training:	Epoch: [175][105/233]	Loss 0.0004 (0.0014)	
training:	Epoch: [175][106/233]	Loss 0.0006 (0.0014)	
training:	Epoch: [175][107/233]	Loss 0.0003 (0.0014)	
training:	Epoch: [175][108/233]	Loss 0.0003 (0.0014)	
training:	Epoch: [175][109/233]	Loss 0.0002 (0.0014)	
training:	Epoch: [175][110/233]	Loss 0.0001 (0.0014)	
training:	Epoch: [175][111/233]	Loss 0.0019 (0.0014)	
training:	Epoch: [175][112/233]	Loss 0.0018 (0.0014)	
training:	Epoch: [175][113/233]	Loss 0.0002 (0.0014)	
training:	Epoch: [175][114/233]	Loss 0.0002 (0.0014)	
training:	Epoch: [175][115/233]	Loss 0.0002 (0.0014)	
training:	Epoch: [175][116/233]	Loss 0.0002 (0.0014)	
training:	Epoch: [175][117/233]	Loss 0.0002 (0.0014)	
training:	Epoch: [175][118/233]	Loss 0.0002 (0.0013)	
training:	Epoch: [175][119/233]	Loss 0.0004 (0.0013)	
training:	Epoch: [175][120/233]	Loss 0.0002 (0.0013)	
training:	Epoch: [175][121/233]	Loss 0.0212 (0.0015)	
training:	Epoch: [175][122/233]	Loss 0.0002 (0.0015)	
training:	Epoch: [175][123/233]	Loss 0.0003 (0.0015)	
training:	Epoch: [175][124/233]	Loss 0.0002 (0.0015)	
training:	Epoch: [175][125/233]	Loss 0.0002 (0.0014)	
training:	Epoch: [175][126/233]	Loss 0.0002 (0.0014)	
training:	Epoch: [175][127/233]	Loss 0.0002 (0.0014)	
training:	Epoch: [175][128/233]	Loss 0.0002 (0.0014)	
training:	Epoch: [175][129/233]	Loss 0.0003 (0.0014)	
training:	Epoch: [175][130/233]	Loss 0.0003 (0.0014)	
training:	Epoch: [175][131/233]	Loss 0.0013 (0.0014)	
training:	Epoch: [175][132/233]	Loss 0.0002 (0.0014)	
training:	Epoch: [175][133/233]	Loss 0.0001 (0.0014)	
training:	Epoch: [175][134/233]	Loss 0.0014 (0.0014)	
training:	Epoch: [175][135/233]	Loss 0.0001 (0.0014)	
training:	Epoch: [175][136/233]	Loss 0.0001 (0.0014)	
training:	Epoch: [175][137/233]	Loss 0.0002 (0.0014)	
training:	Epoch: [175][138/233]	Loss 0.0086 (0.0014)	
training:	Epoch: [175][139/233]	Loss 0.0002 (0.0014)	
training:	Epoch: [175][140/233]	Loss 0.0005 (0.0014)	
training:	Epoch: [175][141/233]	Loss 0.0001 (0.0014)	
training:	Epoch: [175][142/233]	Loss 0.0002 (0.0014)	
training:	Epoch: [175][143/233]	Loss 0.0004 (0.0014)	
training:	Epoch: [175][144/233]	Loss 0.0002 (0.0014)	
training:	Epoch: [175][145/233]	Loss 0.0002 (0.0014)	
training:	Epoch: [175][146/233]	Loss 0.0002 (0.0013)	
training:	Epoch: [175][147/233]	Loss 0.0002 (0.0013)	
training:	Epoch: [175][148/233]	Loss 0.0001 (0.0013)	
training:	Epoch: [175][149/233]	Loss 0.0194 (0.0014)	
training:	Epoch: [175][150/233]	Loss 0.0004 (0.0014)	
training:	Epoch: [175][151/233]	Loss 0.0002 (0.0014)	
training:	Epoch: [175][152/233]	Loss 0.0006 (0.0014)	
training:	Epoch: [175][153/233]	Loss 0.0002 (0.0014)	
training:	Epoch: [175][154/233]	Loss 0.0002 (0.0014)	
training:	Epoch: [175][155/233]	Loss 0.0002 (0.0014)	
training:	Epoch: [175][156/233]	Loss 0.0002 (0.0014)	
training:	Epoch: [175][157/233]	Loss 0.0001 (0.0014)	
training:	Epoch: [175][158/233]	Loss 0.0002 (0.0014)	
training:	Epoch: [175][159/233]	Loss 0.0002 (0.0014)	
training:	Epoch: [175][160/233]	Loss 0.0002 (0.0014)	
training:	Epoch: [175][161/233]	Loss 0.0002 (0.0014)	
training:	Epoch: [175][162/233]	Loss 0.0002 (0.0014)	
training:	Epoch: [175][163/233]	Loss 0.0002 (0.0013)	
training:	Epoch: [175][164/233]	Loss 0.0002 (0.0013)	
training:	Epoch: [175][165/233]	Loss 0.0002 (0.0013)	
training:	Epoch: [175][166/233]	Loss 0.0007 (0.0013)	
training:	Epoch: [175][167/233]	Loss 0.0003 (0.0013)	
training:	Epoch: [175][168/233]	Loss 0.0005 (0.0013)	
training:	Epoch: [175][169/233]	Loss 0.0002 (0.0013)	
training:	Epoch: [175][170/233]	Loss 0.0007 (0.0013)	
training:	Epoch: [175][171/233]	Loss 0.0002 (0.0013)	
training:	Epoch: [175][172/233]	Loss 0.0009 (0.0013)	
training:	Epoch: [175][173/233]	Loss 0.0002 (0.0013)	
training:	Epoch: [175][174/233]	Loss 0.0002 (0.0013)	
training:	Epoch: [175][175/233]	Loss 0.0004 (0.0013)	
training:	Epoch: [175][176/233]	Loss 0.0002 (0.0013)	
training:	Epoch: [175][177/233]	Loss 0.0001 (0.0013)	
training:	Epoch: [175][178/233]	Loss 0.0002 (0.0013)	
training:	Epoch: [175][179/233]	Loss 0.0003 (0.0013)	
training:	Epoch: [175][180/233]	Loss 0.0002 (0.0012)	
training:	Epoch: [175][181/233]	Loss 0.0010 (0.0012)	
training:	Epoch: [175][182/233]	Loss 0.0002 (0.0012)	
training:	Epoch: [175][183/233]	Loss 0.0002 (0.0012)	
training:	Epoch: [175][184/233]	Loss 0.0002 (0.0012)	
training:	Epoch: [175][185/233]	Loss 0.0002 (0.0012)	
training:	Epoch: [175][186/233]	Loss 0.0002 (0.0012)	
training:	Epoch: [175][187/233]	Loss 0.0003 (0.0012)	
training:	Epoch: [175][188/233]	Loss 0.0002 (0.0012)	
training:	Epoch: [175][189/233]	Loss 0.0002 (0.0012)	
training:	Epoch: [175][190/233]	Loss 0.0002 (0.0012)	
training:	Epoch: [175][191/233]	Loss 0.0002 (0.0012)	
training:	Epoch: [175][192/233]	Loss 0.0002 (0.0012)	
training:	Epoch: [175][193/233]	Loss 0.1227 (0.0018)	
training:	Epoch: [175][194/233]	Loss 0.0002 (0.0018)	
training:	Epoch: [175][195/233]	Loss 0.0002 (0.0018)	
training:	Epoch: [175][196/233]	Loss 0.0002 (0.0018)	
training:	Epoch: [175][197/233]	Loss 0.0001 (0.0018)	
training:	Epoch: [175][198/233]	Loss 0.0003 (0.0018)	
training:	Epoch: [175][199/233]	Loss 0.0002 (0.0018)	
training:	Epoch: [175][200/233]	Loss 0.0002 (0.0018)	
training:	Epoch: [175][201/233]	Loss 0.0007 (0.0018)	
training:	Epoch: [175][202/233]	Loss 0.0005 (0.0017)	
training:	Epoch: [175][203/233]	Loss 0.0223 (0.0019)	
training:	Epoch: [175][204/233]	Loss 0.0002 (0.0018)	
training:	Epoch: [175][205/233]	Loss 0.0001 (0.0018)	
training:	Epoch: [175][206/233]	Loss 0.0003 (0.0018)	
training:	Epoch: [175][207/233]	Loss 0.0002 (0.0018)	
training:	Epoch: [175][208/233]	Loss 0.0002 (0.0018)	
training:	Epoch: [175][209/233]	Loss 0.0001 (0.0018)	
training:	Epoch: [175][210/233]	Loss 0.0002 (0.0018)	
training:	Epoch: [175][211/233]	Loss 0.0029 (0.0018)	
training:	Epoch: [175][212/233]	Loss 0.0001 (0.0018)	
training:	Epoch: [175][213/233]	Loss 0.0191 (0.0019)	
training:	Epoch: [175][214/233]	Loss 0.0007 (0.0019)	
training:	Epoch: [175][215/233]	Loss 0.0013 (0.0019)	
training:	Epoch: [175][216/233]	Loss 0.0002 (0.0019)	
training:	Epoch: [175][217/233]	Loss 0.0002 (0.0018)	
training:	Epoch: [175][218/233]	Loss 0.0002 (0.0018)	
training:	Epoch: [175][219/233]	Loss 0.0002 (0.0018)	
training:	Epoch: [175][220/233]	Loss 0.0002 (0.0018)	
training:	Epoch: [175][221/233]	Loss 0.0001 (0.0018)	
training:	Epoch: [175][222/233]	Loss 0.0001 (0.0018)	
training:	Epoch: [175][223/233]	Loss 0.0002 (0.0018)	
training:	Epoch: [175][224/233]	Loss 0.0002 (0.0018)	
training:	Epoch: [175][225/233]	Loss 0.0038 (0.0018)	
training:	Epoch: [175][226/233]	Loss 0.0006 (0.0018)	
training:	Epoch: [175][227/233]	Loss 0.0002 (0.0018)	
training:	Epoch: [175][228/233]	Loss 0.0003 (0.0018)	
training:	Epoch: [175][229/233]	Loss 0.0083 (0.0018)	
training:	Epoch: [175][230/233]	Loss 0.0002 (0.0018)	
training:	Epoch: [175][231/233]	Loss 0.0002 (0.0018)	
training:	Epoch: [175][232/233]	Loss 0.0002 (0.0018)	
training:	Epoch: [175][233/233]	Loss 0.0001 (0.0018)	
Training:	 Loss: 0.0018

Training:	 ACC: 1.0000 1.0000 1.0000 1.0000
Validation:	 ACC: 0.7984 0.7988 0.8080 0.7888
Validation:	 Best_BACC: 0.8162 0.8154 0.7998 0.8326
Validation:	 Loss: 1.3495
Pretraining:	Epoch 176/200
----------
training:	Epoch: [176][1/233]	Loss 0.0006 (0.0006)	
training:	Epoch: [176][2/233]	Loss 0.0008 (0.0007)	
training:	Epoch: [176][3/233]	Loss 0.0032 (0.0015)	
training:	Epoch: [176][4/233]	Loss 0.0002 (0.0012)	
training:	Epoch: [176][5/233]	Loss 0.0041 (0.0018)	
training:	Epoch: [176][6/233]	Loss 0.0001 (0.0015)	
training:	Epoch: [176][7/233]	Loss 0.0002 (0.0013)	
training:	Epoch: [176][8/233]	Loss 0.0001 (0.0012)	
training:	Epoch: [176][9/233]	Loss 0.0015 (0.0012)	
training:	Epoch: [176][10/233]	Loss 0.0001 (0.0011)	
training:	Epoch: [176][11/233]	Loss 0.0002 (0.0010)	
training:	Epoch: [176][12/233]	Loss 0.0001 (0.0009)	
training:	Epoch: [176][13/233]	Loss 0.0003 (0.0009)	
training:	Epoch: [176][14/233]	Loss 0.0002 (0.0008)	
training:	Epoch: [176][15/233]	Loss 0.0003 (0.0008)	
training:	Epoch: [176][16/233]	Loss 0.0002 (0.0008)	
training:	Epoch: [176][17/233]	Loss 0.0002 (0.0007)	
training:	Epoch: [176][18/233]	Loss 0.0727 (0.0047)	
training:	Epoch: [176][19/233]	Loss 0.0002 (0.0045)	
training:	Epoch: [176][20/233]	Loss 0.0002 (0.0043)	
training:	Epoch: [176][21/233]	Loss 0.0002 (0.0041)	
training:	Epoch: [176][22/233]	Loss 0.0001 (0.0039)	
training:	Epoch: [176][23/233]	Loss 0.0002 (0.0037)	
training:	Epoch: [176][24/233]	Loss 0.0003 (0.0036)	
training:	Epoch: [176][25/233]	Loss 0.0002 (0.0035)	
training:	Epoch: [176][26/233]	Loss 0.0001 (0.0033)	
training:	Epoch: [176][27/233]	Loss 0.0001 (0.0032)	
training:	Epoch: [176][28/233]	Loss 0.0001 (0.0031)	
training:	Epoch: [176][29/233]	Loss 0.0002 (0.0030)	
training:	Epoch: [176][30/233]	Loss 0.0002 (0.0029)	
training:	Epoch: [176][31/233]	Loss 0.0003 (0.0028)	
training:	Epoch: [176][32/233]	Loss 0.0002 (0.0027)	
training:	Epoch: [176][33/233]	Loss 0.0005 (0.0027)	
training:	Epoch: [176][34/233]	Loss 0.0012 (0.0026)	
training:	Epoch: [176][35/233]	Loss 0.0007 (0.0026)	
training:	Epoch: [176][36/233]	Loss 0.0003 (0.0025)	
training:	Epoch: [176][37/233]	Loss 0.0002 (0.0024)	
training:	Epoch: [176][38/233]	Loss 0.0001 (0.0024)	
training:	Epoch: [176][39/233]	Loss 0.0002 (0.0023)	
training:	Epoch: [176][40/233]	Loss 0.0002 (0.0023)	
training:	Epoch: [176][41/233]	Loss 0.0002 (0.0022)	
training:	Epoch: [176][42/233]	Loss 0.0002 (0.0022)	
training:	Epoch: [176][43/233]	Loss 0.0020 (0.0022)	
training:	Epoch: [176][44/233]	Loss 0.0362 (0.0029)	
training:	Epoch: [176][45/233]	Loss 0.0001 (0.0029)	
training:	Epoch: [176][46/233]	Loss 0.0001 (0.0028)	
training:	Epoch: [176][47/233]	Loss 0.0003 (0.0028)	
training:	Epoch: [176][48/233]	Loss 0.0002 (0.0027)	
training:	Epoch: [176][49/233]	Loss 0.0006 (0.0027)	
training:	Epoch: [176][50/233]	Loss 0.0002 (0.0026)	
training:	Epoch: [176][51/233]	Loss 0.0023 (0.0026)	
training:	Epoch: [176][52/233]	Loss 0.0001 (0.0026)	
training:	Epoch: [176][53/233]	Loss 0.0002 (0.0025)	
training:	Epoch: [176][54/233]	Loss 0.0351 (0.0031)	
training:	Epoch: [176][55/233]	Loss 0.0002 (0.0031)	
training:	Epoch: [176][56/233]	Loss 0.0003 (0.0030)	
training:	Epoch: [176][57/233]	Loss 0.0001 (0.0030)	
training:	Epoch: [176][58/233]	Loss 0.0033 (0.0030)	
training:	Epoch: [176][59/233]	Loss 0.0003 (0.0029)	
training:	Epoch: [176][60/233]	Loss 0.0002 (0.0029)	
training:	Epoch: [176][61/233]	Loss 0.0001 (0.0028)	
training:	Epoch: [176][62/233]	Loss 0.0002 (0.0028)	
training:	Epoch: [176][63/233]	Loss 0.0017 (0.0028)	
training:	Epoch: [176][64/233]	Loss 0.0003 (0.0027)	
training:	Epoch: [176][65/233]	Loss 0.0002 (0.0027)	
training:	Epoch: [176][66/233]	Loss 0.0001 (0.0027)	
training:	Epoch: [176][67/233]	Loss 0.0001 (0.0026)	
training:	Epoch: [176][68/233]	Loss 0.0002 (0.0026)	
training:	Epoch: [176][69/233]	Loss 0.0002 (0.0026)	
training:	Epoch: [176][70/233]	Loss 0.0002 (0.0025)	
training:	Epoch: [176][71/233]	Loss 0.0007 (0.0025)	
training:	Epoch: [176][72/233]	Loss 0.0001 (0.0025)	
training:	Epoch: [176][73/233]	Loss 0.0002 (0.0024)	
training:	Epoch: [176][74/233]	Loss 0.0009 (0.0024)	
training:	Epoch: [176][75/233]	Loss 0.0002 (0.0024)	
training:	Epoch: [176][76/233]	Loss 0.0571 (0.0031)	
training:	Epoch: [176][77/233]	Loss 0.0009 (0.0031)	
training:	Epoch: [176][78/233]	Loss 0.0002 (0.0030)	
training:	Epoch: [176][79/233]	Loss 0.0001 (0.0030)	
training:	Epoch: [176][80/233]	Loss 0.0002 (0.0030)	
training:	Epoch: [176][81/233]	Loss 0.0003 (0.0029)	
training:	Epoch: [176][82/233]	Loss 0.0001 (0.0029)	
training:	Epoch: [176][83/233]	Loss 0.0002 (0.0029)	
training:	Epoch: [176][84/233]	Loss 0.0003 (0.0028)	
training:	Epoch: [176][85/233]	Loss 0.0002 (0.0028)	
training:	Epoch: [176][86/233]	Loss 0.0001 (0.0028)	
training:	Epoch: [176][87/233]	Loss 0.0001 (0.0027)	
training:	Epoch: [176][88/233]	Loss 0.0002 (0.0027)	
training:	Epoch: [176][89/233]	Loss 0.0003 (0.0027)	
training:	Epoch: [176][90/233]	Loss 0.0002 (0.0027)	
training:	Epoch: [176][91/233]	Loss 0.0003 (0.0026)	
training:	Epoch: [176][92/233]	Loss 0.0042 (0.0027)	
training:	Epoch: [176][93/233]	Loss 0.0002 (0.0026)	
training:	Epoch: [176][94/233]	Loss 0.0002 (0.0026)	
training:	Epoch: [176][95/233]	Loss 0.0002 (0.0026)	
training:	Epoch: [176][96/233]	Loss 0.0002 (0.0025)	
training:	Epoch: [176][97/233]	Loss 0.0003 (0.0025)	
training:	Epoch: [176][98/233]	Loss 0.0002 (0.0025)	
training:	Epoch: [176][99/233]	Loss 0.0001 (0.0025)	
training:	Epoch: [176][100/233]	Loss 0.0004 (0.0025)	
training:	Epoch: [176][101/233]	Loss 0.0002 (0.0024)	
training:	Epoch: [176][102/233]	Loss 0.0002 (0.0024)	
training:	Epoch: [176][103/233]	Loss 0.0002 (0.0024)	
training:	Epoch: [176][104/233]	Loss 0.0002 (0.0024)	
training:	Epoch: [176][105/233]	Loss 0.0001 (0.0023)	
training:	Epoch: [176][106/233]	Loss 0.0002 (0.0023)	
training:	Epoch: [176][107/233]	Loss 0.0001 (0.0023)	
training:	Epoch: [176][108/233]	Loss 0.0004 (0.0023)	
training:	Epoch: [176][109/233]	Loss 0.0004 (0.0023)	
training:	Epoch: [176][110/233]	Loss 0.0002 (0.0023)	
training:	Epoch: [176][111/233]	Loss 0.0002 (0.0022)	
training:	Epoch: [176][112/233]	Loss 0.0005 (0.0022)	
training:	Epoch: [176][113/233]	Loss 0.0002 (0.0022)	
training:	Epoch: [176][114/233]	Loss 0.0002 (0.0022)	
training:	Epoch: [176][115/233]	Loss 0.0001 (0.0022)	
training:	Epoch: [176][116/233]	Loss 0.0004 (0.0021)	
training:	Epoch: [176][117/233]	Loss 0.0003 (0.0021)	
training:	Epoch: [176][118/233]	Loss 0.0002 (0.0021)	
training:	Epoch: [176][119/233]	Loss 0.0001 (0.0021)	
training:	Epoch: [176][120/233]	Loss 0.0002 (0.0021)	
training:	Epoch: [176][121/233]	Loss 0.0009 (0.0021)	
training:	Epoch: [176][122/233]	Loss 0.0002 (0.0021)	
training:	Epoch: [176][123/233]	Loss 0.0008 (0.0020)	
training:	Epoch: [176][124/233]	Loss 0.0001 (0.0020)	
training:	Epoch: [176][125/233]	Loss 0.0005 (0.0020)	
training:	Epoch: [176][126/233]	Loss 0.0003 (0.0020)	
training:	Epoch: [176][127/233]	Loss 0.0020 (0.0020)	
training:	Epoch: [176][128/233]	Loss 0.0001 (0.0020)	
training:	Epoch: [176][129/233]	Loss 0.0003 (0.0020)	
training:	Epoch: [176][130/233]	Loss 0.0002 (0.0020)	
training:	Epoch: [176][131/233]	Loss 0.0002 (0.0020)	
training:	Epoch: [176][132/233]	Loss 0.0016 (0.0019)	
training:	Epoch: [176][133/233]	Loss 0.0003 (0.0019)	
training:	Epoch: [176][134/233]	Loss 0.0002 (0.0019)	
training:	Epoch: [176][135/233]	Loss 0.0002 (0.0019)	
training:	Epoch: [176][136/233]	Loss 0.0002 (0.0019)	
training:	Epoch: [176][137/233]	Loss 0.0002 (0.0019)	
training:	Epoch: [176][138/233]	Loss 0.0003 (0.0019)	
training:	Epoch: [176][139/233]	Loss 0.0002 (0.0019)	
training:	Epoch: [176][140/233]	Loss 0.0002 (0.0018)	
training:	Epoch: [176][141/233]	Loss 0.0341 (0.0021)	
training:	Epoch: [176][142/233]	Loss 0.0002 (0.0021)	
training:	Epoch: [176][143/233]	Loss 0.0002 (0.0021)	
training:	Epoch: [176][144/233]	Loss 0.0005 (0.0020)	
training:	Epoch: [176][145/233]	Loss 0.0002 (0.0020)	
training:	Epoch: [176][146/233]	Loss 0.0002 (0.0020)	
training:	Epoch: [176][147/233]	Loss 0.0002 (0.0020)	
training:	Epoch: [176][148/233]	Loss 0.0002 (0.0020)	
training:	Epoch: [176][149/233]	Loss 0.0002 (0.0020)	
training:	Epoch: [176][150/233]	Loss 0.0002 (0.0020)	
training:	Epoch: [176][151/233]	Loss 0.0013 (0.0020)	
training:	Epoch: [176][152/233]	Loss 0.0002 (0.0020)	
training:	Epoch: [176][153/233]	Loss 0.0004 (0.0019)	
training:	Epoch: [176][154/233]	Loss 0.0002 (0.0019)	
training:	Epoch: [176][155/233]	Loss 0.0002 (0.0019)	
training:	Epoch: [176][156/233]	Loss 0.0002 (0.0019)	
training:	Epoch: [176][157/233]	Loss 0.0002 (0.0019)	
training:	Epoch: [176][158/233]	Loss 0.1760 (0.0030)	
training:	Epoch: [176][159/233]	Loss 0.0002 (0.0030)	
training:	Epoch: [176][160/233]	Loss 0.0002 (0.0030)	
training:	Epoch: [176][161/233]	Loss 0.0002 (0.0029)	
training:	Epoch: [176][162/233]	Loss 0.0009 (0.0029)	
training:	Epoch: [176][163/233]	Loss 0.0001 (0.0029)	
training:	Epoch: [176][164/233]	Loss 0.0002 (0.0029)	
training:	Epoch: [176][165/233]	Loss 0.0002 (0.0029)	
training:	Epoch: [176][166/233]	Loss 0.0002 (0.0029)	
training:	Epoch: [176][167/233]	Loss 0.0011 (0.0029)	
training:	Epoch: [176][168/233]	Loss 0.0002 (0.0028)	
training:	Epoch: [176][169/233]	Loss 0.0001 (0.0028)	
training:	Epoch: [176][170/233]	Loss 0.0002 (0.0028)	
training:	Epoch: [176][171/233]	Loss 0.0002 (0.0028)	
training:	Epoch: [176][172/233]	Loss 0.0135 (0.0029)	
training:	Epoch: [176][173/233]	Loss 0.0002 (0.0028)	
training:	Epoch: [176][174/233]	Loss 0.0001 (0.0028)	
training:	Epoch: [176][175/233]	Loss 0.0002 (0.0028)	
training:	Epoch: [176][176/233]	Loss 0.0002 (0.0028)	
training:	Epoch: [176][177/233]	Loss 0.0002 (0.0028)	
training:	Epoch: [176][178/233]	Loss 0.0002 (0.0028)	
training:	Epoch: [176][179/233]	Loss 0.0006 (0.0028)	
training:	Epoch: [176][180/233]	Loss 0.0002 (0.0027)	
training:	Epoch: [176][181/233]	Loss 0.0002 (0.0027)	
training:	Epoch: [176][182/233]	Loss 0.0002 (0.0027)	
training:	Epoch: [176][183/233]	Loss 0.0001 (0.0027)	
training:	Epoch: [176][184/233]	Loss 0.0043 (0.0027)	
training:	Epoch: [176][185/233]	Loss 0.0002 (0.0027)	
training:	Epoch: [176][186/233]	Loss 0.0047 (0.0027)	
training:	Epoch: [176][187/233]	Loss 0.0010 (0.0027)	
training:	Epoch: [176][188/233]	Loss 0.0002 (0.0027)	
training:	Epoch: [176][189/233]	Loss 0.0001 (0.0027)	
training:	Epoch: [176][190/233]	Loss 0.0002 (0.0027)	
training:	Epoch: [176][191/233]	Loss 0.0002 (0.0026)	
training:	Epoch: [176][192/233]	Loss 0.0002 (0.0026)	
training:	Epoch: [176][193/233]	Loss 0.0002 (0.0026)	
training:	Epoch: [176][194/233]	Loss 0.0003 (0.0026)	
training:	Epoch: [176][195/233]	Loss 0.0001 (0.0026)	
training:	Epoch: [176][196/233]	Loss 0.0002 (0.0026)	
training:	Epoch: [176][197/233]	Loss 0.0003 (0.0026)	
training:	Epoch: [176][198/233]	Loss 0.0002 (0.0026)	
training:	Epoch: [176][199/233]	Loss 0.0001 (0.0025)	
training:	Epoch: [176][200/233]	Loss 0.0002 (0.0025)	
training:	Epoch: [176][201/233]	Loss 0.0002 (0.0025)	
training:	Epoch: [176][202/233]	Loss 0.0002 (0.0025)	
training:	Epoch: [176][203/233]	Loss 0.0004 (0.0025)	
training:	Epoch: [176][204/233]	Loss 0.0002 (0.0025)	
training:	Epoch: [176][205/233]	Loss 0.0002 (0.0025)	
training:	Epoch: [176][206/233]	Loss 0.0003 (0.0025)	
training:	Epoch: [176][207/233]	Loss 0.0002 (0.0025)	
training:	Epoch: [176][208/233]	Loss 0.0002 (0.0024)	
training:	Epoch: [176][209/233]	Loss 0.0003 (0.0024)	
training:	Epoch: [176][210/233]	Loss 0.0002 (0.0024)	
training:	Epoch: [176][211/233]	Loss 0.0002 (0.0024)	
training:	Epoch: [176][212/233]	Loss 0.0004 (0.0024)	
training:	Epoch: [176][213/233]	Loss 0.0002 (0.0024)	
training:	Epoch: [176][214/233]	Loss 0.0002 (0.0024)	
training:	Epoch: [176][215/233]	Loss 0.0001 (0.0024)	
training:	Epoch: [176][216/233]	Loss 0.0005 (0.0024)	
training:	Epoch: [176][217/233]	Loss 0.0002 (0.0023)	
training:	Epoch: [176][218/233]	Loss 0.0002 (0.0023)	
training:	Epoch: [176][219/233]	Loss 0.0002 (0.0023)	
training:	Epoch: [176][220/233]	Loss 0.0002 (0.0023)	
training:	Epoch: [176][221/233]	Loss 0.0002 (0.0023)	
training:	Epoch: [176][222/233]	Loss 0.0002 (0.0023)	
training:	Epoch: [176][223/233]	Loss 0.0012 (0.0023)	
training:	Epoch: [176][224/233]	Loss 0.0002 (0.0023)	
training:	Epoch: [176][225/233]	Loss 0.0002 (0.0023)	
training:	Epoch: [176][226/233]	Loss 0.0002 (0.0023)	
training:	Epoch: [176][227/233]	Loss 0.0002 (0.0023)	
training:	Epoch: [176][228/233]	Loss 0.0001 (0.0022)	
training:	Epoch: [176][229/233]	Loss 0.0001 (0.0022)	
training:	Epoch: [176][230/233]	Loss 0.0002 (0.0022)	
training:	Epoch: [176][231/233]	Loss 0.0002 (0.0022)	
training:	Epoch: [176][232/233]	Loss 0.0002 (0.0022)	
training:	Epoch: [176][233/233]	Loss 0.0002 (0.0022)	
Training:	 Loss: 0.0022

Training:	 ACC: 1.0000 1.0000 1.0000 1.0000
Validation:	 ACC: 0.7966 0.7967 0.7988 0.7944
Validation:	 Best_BACC: 0.8162 0.8154 0.7998 0.8326
Validation:	 Loss: 1.3575
Pretraining:	Epoch 177/200
----------
training:	Epoch: [177][1/233]	Loss 0.0006 (0.0006)	
training:	Epoch: [177][2/233]	Loss 0.0002 (0.0004)	
training:	Epoch: [177][3/233]	Loss 0.0001 (0.0003)	
training:	Epoch: [177][4/233]	Loss 0.0002 (0.0003)	
training:	Epoch: [177][5/233]	Loss 0.0002 (0.0003)	
training:	Epoch: [177][6/233]	Loss 0.0002 (0.0002)	
training:	Epoch: [177][7/233]	Loss 0.0149 (0.0023)	
training:	Epoch: [177][8/233]	Loss 0.0002 (0.0021)	
training:	Epoch: [177][9/233]	Loss 0.0008 (0.0019)	
training:	Epoch: [177][10/233]	Loss 0.0002 (0.0018)	
training:	Epoch: [177][11/233]	Loss 0.0002 (0.0016)	
training:	Epoch: [177][12/233]	Loss 0.0001 (0.0015)	
training:	Epoch: [177][13/233]	Loss 0.0002 (0.0014)	
training:	Epoch: [177][14/233]	Loss 0.0002 (0.0013)	
training:	Epoch: [177][15/233]	Loss 0.0002 (0.0012)	
training:	Epoch: [177][16/233]	Loss 0.0002 (0.0012)	
training:	Epoch: [177][17/233]	Loss 0.0005 (0.0011)	
training:	Epoch: [177][18/233]	Loss 0.0001 (0.0011)	
training:	Epoch: [177][19/233]	Loss 0.0005 (0.0010)	
training:	Epoch: [177][20/233]	Loss 0.0002 (0.0010)	
training:	Epoch: [177][21/233]	Loss 0.0002 (0.0010)	
training:	Epoch: [177][22/233]	Loss 0.0002 (0.0009)	
training:	Epoch: [177][23/233]	Loss 0.0002 (0.0009)	
training:	Epoch: [177][24/233]	Loss 0.0001 (0.0009)	
training:	Epoch: [177][25/233]	Loss 0.0006 (0.0009)	
training:	Epoch: [177][26/233]	Loss 0.0002 (0.0008)	
training:	Epoch: [177][27/233]	Loss 0.0002 (0.0008)	
training:	Epoch: [177][28/233]	Loss 0.0002 (0.0008)	
training:	Epoch: [177][29/233]	Loss 0.0002 (0.0008)	
training:	Epoch: [177][30/233]	Loss 0.0010 (0.0008)	
training:	Epoch: [177][31/233]	Loss 0.0002 (0.0007)	
training:	Epoch: [177][32/233]	Loss 0.0002 (0.0007)	
training:	Epoch: [177][33/233]	Loss 0.0001 (0.0007)	
training:	Epoch: [177][34/233]	Loss 0.0002 (0.0007)	
training:	Epoch: [177][35/233]	Loss 0.0002 (0.0007)	
training:	Epoch: [177][36/233]	Loss 0.0005 (0.0007)	
training:	Epoch: [177][37/233]	Loss 0.0088 (0.0009)	
training:	Epoch: [177][38/233]	Loss 0.0626 (0.0025)	
training:	Epoch: [177][39/233]	Loss 0.0002 (0.0025)	
training:	Epoch: [177][40/233]	Loss 0.0002 (0.0024)	
training:	Epoch: [177][41/233]	Loss 0.1280 (0.0055)	
training:	Epoch: [177][42/233]	Loss 0.0001 (0.0053)	
training:	Epoch: [177][43/233]	Loss 0.0001 (0.0052)	
training:	Epoch: [177][44/233]	Loss 0.0003 (0.0051)	
training:	Epoch: [177][45/233]	Loss 0.0002 (0.0050)	
training:	Epoch: [177][46/233]	Loss 0.0071 (0.0050)	
training:	Epoch: [177][47/233]	Loss 0.0002 (0.0049)	
training:	Epoch: [177][48/233]	Loss 0.0001 (0.0048)	
training:	Epoch: [177][49/233]	Loss 0.0002 (0.0047)	
training:	Epoch: [177][50/233]	Loss 0.0004 (0.0047)	
training:	Epoch: [177][51/233]	Loss 0.0002 (0.0046)	
training:	Epoch: [177][52/233]	Loss 0.0002 (0.0045)	
training:	Epoch: [177][53/233]	Loss 0.0006 (0.0044)	
training:	Epoch: [177][54/233]	Loss 0.0002 (0.0043)	
training:	Epoch: [177][55/233]	Loss 0.0004 (0.0043)	
training:	Epoch: [177][56/233]	Loss 0.0002 (0.0042)	
training:	Epoch: [177][57/233]	Loss 0.0002 (0.0041)	
training:	Epoch: [177][58/233]	Loss 0.0005 (0.0041)	
training:	Epoch: [177][59/233]	Loss 0.0001 (0.0040)	
training:	Epoch: [177][60/233]	Loss 0.0001 (0.0039)	
training:	Epoch: [177][61/233]	Loss 0.0002 (0.0039)	
training:	Epoch: [177][62/233]	Loss 0.0001 (0.0038)	
training:	Epoch: [177][63/233]	Loss 0.0002 (0.0038)	
training:	Epoch: [177][64/233]	Loss 0.0003 (0.0037)	
training:	Epoch: [177][65/233]	Loss 0.0005 (0.0036)	
training:	Epoch: [177][66/233]	Loss 0.0002 (0.0036)	
training:	Epoch: [177][67/233]	Loss 0.0213 (0.0039)	
training:	Epoch: [177][68/233]	Loss 0.0002 (0.0038)	
training:	Epoch: [177][69/233]	Loss 0.0002 (0.0038)	
training:	Epoch: [177][70/233]	Loss 0.0002 (0.0037)	
training:	Epoch: [177][71/233]	Loss 0.0004 (0.0037)	
training:	Epoch: [177][72/233]	Loss 0.0001 (0.0036)	
training:	Epoch: [177][73/233]	Loss 0.0002 (0.0036)	
training:	Epoch: [177][74/233]	Loss 0.0002 (0.0035)	
training:	Epoch: [177][75/233]	Loss 0.0002 (0.0035)	
training:	Epoch: [177][76/233]	Loss 0.0001 (0.0034)	
training:	Epoch: [177][77/233]	Loss 0.0003 (0.0034)	
training:	Epoch: [177][78/233]	Loss 0.0003 (0.0033)	
training:	Epoch: [177][79/233]	Loss 0.0002 (0.0033)	
training:	Epoch: [177][80/233]	Loss 0.0002 (0.0033)	
training:	Epoch: [177][81/233]	Loss 0.0002 (0.0032)	
training:	Epoch: [177][82/233]	Loss 0.0005 (0.0032)	
training:	Epoch: [177][83/233]	Loss 0.0002 (0.0032)	
training:	Epoch: [177][84/233]	Loss 0.0002 (0.0031)	
training:	Epoch: [177][85/233]	Loss 0.0003 (0.0031)	
training:	Epoch: [177][86/233]	Loss 0.0002 (0.0031)	
training:	Epoch: [177][87/233]	Loss 0.0002 (0.0030)	
training:	Epoch: [177][88/233]	Loss 0.0002 (0.0030)	
training:	Epoch: [177][89/233]	Loss 0.1122 (0.0042)	
training:	Epoch: [177][90/233]	Loss 0.0002 (0.0042)	
training:	Epoch: [177][91/233]	Loss 0.0002 (0.0041)	
training:	Epoch: [177][92/233]	Loss 0.0003 (0.0041)	
training:	Epoch: [177][93/233]	Loss 0.0004 (0.0040)	
training:	Epoch: [177][94/233]	Loss 0.0002 (0.0040)	
training:	Epoch: [177][95/233]	Loss 0.0004 (0.0040)	
training:	Epoch: [177][96/233]	Loss 0.0001 (0.0039)	
training:	Epoch: [177][97/233]	Loss 0.0002 (0.0039)	
training:	Epoch: [177][98/233]	Loss 0.0002 (0.0039)	
training:	Epoch: [177][99/233]	Loss 0.0001 (0.0038)	
training:	Epoch: [177][100/233]	Loss 0.0003 (0.0038)	
training:	Epoch: [177][101/233]	Loss 0.0002 (0.0037)	
training:	Epoch: [177][102/233]	Loss 0.0002 (0.0037)	
training:	Epoch: [177][103/233]	Loss 0.0002 (0.0037)	
training:	Epoch: [177][104/233]	Loss 0.0004 (0.0036)	
training:	Epoch: [177][105/233]	Loss 0.0002 (0.0036)	
training:	Epoch: [177][106/233]	Loss 0.0594 (0.0041)	
training:	Epoch: [177][107/233]	Loss 0.0239 (0.0043)	
training:	Epoch: [177][108/233]	Loss 0.0002 (0.0043)	
training:	Epoch: [177][109/233]	Loss 0.0001 (0.0042)	
training:	Epoch: [177][110/233]	Loss 0.0004 (0.0042)	
training:	Epoch: [177][111/233]	Loss 0.0002 (0.0042)	
training:	Epoch: [177][112/233]	Loss 0.0002 (0.0041)	
training:	Epoch: [177][113/233]	Loss 0.0009 (0.0041)	
training:	Epoch: [177][114/233]	Loss 0.0025 (0.0041)	
training:	Epoch: [177][115/233]	Loss 0.0691 (0.0047)	
training:	Epoch: [177][116/233]	Loss 0.0002 (0.0046)	
training:	Epoch: [177][117/233]	Loss 0.0002 (0.0046)	
training:	Epoch: [177][118/233]	Loss 0.0003 (0.0045)	
training:	Epoch: [177][119/233]	Loss 0.0002 (0.0045)	
training:	Epoch: [177][120/233]	Loss 0.0002 (0.0045)	
training:	Epoch: [177][121/233]	Loss 0.0010 (0.0044)	
training:	Epoch: [177][122/233]	Loss 0.0005 (0.0044)	
training:	Epoch: [177][123/233]	Loss 0.1645 (0.0057)	
training:	Epoch: [177][124/233]	Loss 0.0003 (0.0057)	
training:	Epoch: [177][125/233]	Loss 0.0004 (0.0056)	
training:	Epoch: [177][126/233]	Loss 0.0012 (0.0056)	
training:	Epoch: [177][127/233]	Loss 0.0004 (0.0056)	
training:	Epoch: [177][128/233]	Loss 0.0008 (0.0055)	
training:	Epoch: [177][129/233]	Loss 0.0006 (0.0055)	
training:	Epoch: [177][130/233]	Loss 0.0006 (0.0054)	
training:	Epoch: [177][131/233]	Loss 0.0006 (0.0054)	
training:	Epoch: [177][132/233]	Loss 0.0002 (0.0054)	
training:	Epoch: [177][133/233]	Loss 0.0002 (0.0053)	
training:	Epoch: [177][134/233]	Loss 0.0004 (0.0053)	
training:	Epoch: [177][135/233]	Loss 0.0009 (0.0053)	
training:	Epoch: [177][136/233]	Loss 0.0006 (0.0052)	
training:	Epoch: [177][137/233]	Loss 0.0038 (0.0052)	
training:	Epoch: [177][138/233]	Loss 0.0002 (0.0052)	
training:	Epoch: [177][139/233]	Loss 0.0002 (0.0051)	
training:	Epoch: [177][140/233]	Loss 0.0002 (0.0051)	
training:	Epoch: [177][141/233]	Loss 0.0005 (0.0051)	
training:	Epoch: [177][142/233]	Loss 0.0002 (0.0050)	
training:	Epoch: [177][143/233]	Loss 0.0009 (0.0050)	
training:	Epoch: [177][144/233]	Loss 0.0003 (0.0050)	
training:	Epoch: [177][145/233]	Loss 0.0002 (0.0049)	
training:	Epoch: [177][146/233]	Loss 0.0012 (0.0049)	
training:	Epoch: [177][147/233]	Loss 0.0002 (0.0049)	
training:	Epoch: [177][148/233]	Loss 0.0005 (0.0049)	
training:	Epoch: [177][149/233]	Loss 0.0032 (0.0048)	
training:	Epoch: [177][150/233]	Loss 0.0002 (0.0048)	
training:	Epoch: [177][151/233]	Loss 0.0002 (0.0048)	
training:	Epoch: [177][152/233]	Loss 0.0006 (0.0048)	
training:	Epoch: [177][153/233]	Loss 0.0004 (0.0047)	
training:	Epoch: [177][154/233]	Loss 0.0002 (0.0047)	
training:	Epoch: [177][155/233]	Loss 0.0002 (0.0047)	
training:	Epoch: [177][156/233]	Loss 0.0002 (0.0046)	
training:	Epoch: [177][157/233]	Loss 0.0004 (0.0046)	
training:	Epoch: [177][158/233]	Loss 0.0002 (0.0046)	
training:	Epoch: [177][159/233]	Loss 0.0002 (0.0046)	
training:	Epoch: [177][160/233]	Loss 0.0002 (0.0045)	
training:	Epoch: [177][161/233]	Loss 0.0002 (0.0045)	
training:	Epoch: [177][162/233]	Loss 0.0002 (0.0045)	
training:	Epoch: [177][163/233]	Loss 0.0004 (0.0044)	
training:	Epoch: [177][164/233]	Loss 0.0002 (0.0044)	
training:	Epoch: [177][165/233]	Loss 0.0003 (0.0044)	
training:	Epoch: [177][166/233]	Loss 0.0002 (0.0044)	
training:	Epoch: [177][167/233]	Loss 0.0002 (0.0043)	
training:	Epoch: [177][168/233]	Loss 0.0002 (0.0043)	
training:	Epoch: [177][169/233]	Loss 0.0002 (0.0043)	
training:	Epoch: [177][170/233]	Loss 0.0002 (0.0043)	
training:	Epoch: [177][171/233]	Loss 0.0003 (0.0042)	
training:	Epoch: [177][172/233]	Loss 0.0003 (0.0042)	
training:	Epoch: [177][173/233]	Loss 0.1881 (0.0053)	
training:	Epoch: [177][174/233]	Loss 0.0048 (0.0053)	
training:	Epoch: [177][175/233]	Loss 0.0002 (0.0053)	
training:	Epoch: [177][176/233]	Loss 0.0003 (0.0052)	
training:	Epoch: [177][177/233]	Loss 0.0002 (0.0052)	
training:	Epoch: [177][178/233]	Loss 0.0003 (0.0052)	
training:	Epoch: [177][179/233]	Loss 0.0002 (0.0051)	
training:	Epoch: [177][180/233]	Loss 0.0002 (0.0051)	
training:	Epoch: [177][181/233]	Loss 0.0002 (0.0051)	
training:	Epoch: [177][182/233]	Loss 0.0001 (0.0051)	
training:	Epoch: [177][183/233]	Loss 0.0002 (0.0050)	
training:	Epoch: [177][184/233]	Loss 0.0002 (0.0050)	
training:	Epoch: [177][185/233]	Loss 0.0002 (0.0050)	
training:	Epoch: [177][186/233]	Loss 0.0036 (0.0050)	
training:	Epoch: [177][187/233]	Loss 0.0008 (0.0050)	
training:	Epoch: [177][188/233]	Loss 0.0009 (0.0049)	
training:	Epoch: [177][189/233]	Loss 0.0002 (0.0049)	
training:	Epoch: [177][190/233]	Loss 0.0002 (0.0049)	
training:	Epoch: [177][191/233]	Loss 0.0003 (0.0049)	
training:	Epoch: [177][192/233]	Loss 0.0002 (0.0048)	
training:	Epoch: [177][193/233]	Loss 0.0002 (0.0048)	
training:	Epoch: [177][194/233]	Loss 0.0002 (0.0048)	
training:	Epoch: [177][195/233]	Loss 0.0002 (0.0048)	
training:	Epoch: [177][196/233]	Loss 0.0770 (0.0051)	
training:	Epoch: [177][197/233]	Loss 0.0017 (0.0051)	
training:	Epoch: [177][198/233]	Loss 0.0002 (0.0051)	
training:	Epoch: [177][199/233]	Loss 0.0003 (0.0051)	
training:	Epoch: [177][200/233]	Loss 0.0002 (0.0050)	
training:	Epoch: [177][201/233]	Loss 0.0003 (0.0050)	
training:	Epoch: [177][202/233]	Loss 0.0003 (0.0050)	
training:	Epoch: [177][203/233]	Loss 0.0007 (0.0050)	
training:	Epoch: [177][204/233]	Loss 0.0006 (0.0050)	
training:	Epoch: [177][205/233]	Loss 0.0022 (0.0049)	
training:	Epoch: [177][206/233]	Loss 0.0003 (0.0049)	
training:	Epoch: [177][207/233]	Loss 0.0002 (0.0049)	
training:	Epoch: [177][208/233]	Loss 0.0003 (0.0049)	
training:	Epoch: [177][209/233]	Loss 0.0004 (0.0048)	
training:	Epoch: [177][210/233]	Loss 0.0002 (0.0048)	
training:	Epoch: [177][211/233]	Loss 0.0001 (0.0048)	
training:	Epoch: [177][212/233]	Loss 0.0002 (0.0048)	
training:	Epoch: [177][213/233]	Loss 0.0002 (0.0048)	
training:	Epoch: [177][214/233]	Loss 0.0224 (0.0048)	
training:	Epoch: [177][215/233]	Loss 0.0002 (0.0048)	
training:	Epoch: [177][216/233]	Loss 0.0002 (0.0048)	
training:	Epoch: [177][217/233]	Loss 0.0002 (0.0048)	
training:	Epoch: [177][218/233]	Loss 0.0002 (0.0048)	
training:	Epoch: [177][219/233]	Loss 0.0002 (0.0047)	
training:	Epoch: [177][220/233]	Loss 0.0002 (0.0047)	
training:	Epoch: [177][221/233]	Loss 0.0002 (0.0047)	
training:	Epoch: [177][222/233]	Loss 0.0002 (0.0047)	
training:	Epoch: [177][223/233]	Loss 0.0002 (0.0047)	
training:	Epoch: [177][224/233]	Loss 0.0003 (0.0046)	
training:	Epoch: [177][225/233]	Loss 0.0001 (0.0046)	
training:	Epoch: [177][226/233]	Loss 0.0002 (0.0046)	
training:	Epoch: [177][227/233]	Loss 0.0001 (0.0046)	
training:	Epoch: [177][228/233]	Loss 0.0002 (0.0046)	
training:	Epoch: [177][229/233]	Loss 0.0002 (0.0045)	
training:	Epoch: [177][230/233]	Loss 0.0001 (0.0045)	
training:	Epoch: [177][231/233]	Loss 0.0002 (0.0045)	
training:	Epoch: [177][232/233]	Loss 0.0262 (0.0046)	
training:	Epoch: [177][233/233]	Loss 0.0002 (0.0046)	
Training:	 Loss: 0.0046

Training:	 ACC: 1.0000 1.0000 1.0000 1.0000
Validation:	 ACC: 0.7948 0.7940 0.7783 0.8112
Validation:	 Best_BACC: 0.8162 0.8154 0.7998 0.8326
Validation:	 Loss: 1.3421
Pretraining:	Epoch 178/200
----------
training:	Epoch: [178][1/233]	Loss 0.0003 (0.0003)	
training:	Epoch: [178][2/233]	Loss 0.0002 (0.0002)	
training:	Epoch: [178][3/233]	Loss 0.0002 (0.0002)	
training:	Epoch: [178][4/233]	Loss 0.0002 (0.0002)	
training:	Epoch: [178][5/233]	Loss 0.0002 (0.0002)	
training:	Epoch: [178][6/233]	Loss 0.0004 (0.0002)	
training:	Epoch: [178][7/233]	Loss 0.0002 (0.0002)	
training:	Epoch: [178][8/233]	Loss 0.0019 (0.0004)	
training:	Epoch: [178][9/233]	Loss 0.0002 (0.0004)	
training:	Epoch: [178][10/233]	Loss 0.0002 (0.0004)	
training:	Epoch: [178][11/233]	Loss 0.0002 (0.0004)	
training:	Epoch: [178][12/233]	Loss 0.0011 (0.0004)	
training:	Epoch: [178][13/233]	Loss 0.0003 (0.0004)	
training:	Epoch: [178][14/233]	Loss 0.0002 (0.0004)	
training:	Epoch: [178][15/233]	Loss 0.0010 (0.0004)	
training:	Epoch: [178][16/233]	Loss 0.0044 (0.0007)	
training:	Epoch: [178][17/233]	Loss 0.0216 (0.0019)	
training:	Epoch: [178][18/233]	Loss 0.0002 (0.0018)	
training:	Epoch: [178][19/233]	Loss 0.0002 (0.0017)	
training:	Epoch: [178][20/233]	Loss 0.0002 (0.0017)	
training:	Epoch: [178][21/233]	Loss 0.0002 (0.0016)	
training:	Epoch: [178][22/233]	Loss 0.0002 (0.0015)	
training:	Epoch: [178][23/233]	Loss 0.0002 (0.0015)	
training:	Epoch: [178][24/233]	Loss 0.0135 (0.0020)	
training:	Epoch: [178][25/233]	Loss 0.0002 (0.0019)	
training:	Epoch: [178][26/233]	Loss 0.0001 (0.0018)	
training:	Epoch: [178][27/233]	Loss 0.0002 (0.0018)	
training:	Epoch: [178][28/233]	Loss 0.0002 (0.0017)	
training:	Epoch: [178][29/233]	Loss 0.0075 (0.0019)	
training:	Epoch: [178][30/233]	Loss 0.0002 (0.0018)	
training:	Epoch: [178][31/233]	Loss 0.0003 (0.0018)	
training:	Epoch: [178][32/233]	Loss 0.0002 (0.0017)	
training:	Epoch: [178][33/233]	Loss 0.0002 (0.0017)	
training:	Epoch: [178][34/233]	Loss 0.0009 (0.0017)	
training:	Epoch: [178][35/233]	Loss 0.0002 (0.0016)	
training:	Epoch: [178][36/233]	Loss 0.0003 (0.0016)	
training:	Epoch: [178][37/233]	Loss 0.0003 (0.0016)	
training:	Epoch: [178][38/233]	Loss 0.0002 (0.0015)	
training:	Epoch: [178][39/233]	Loss 0.0002 (0.0015)	
training:	Epoch: [178][40/233]	Loss 0.0002 (0.0015)	
training:	Epoch: [178][41/233]	Loss 0.0006 (0.0014)	
training:	Epoch: [178][42/233]	Loss 0.0002 (0.0014)	
training:	Epoch: [178][43/233]	Loss 0.0002 (0.0014)	
training:	Epoch: [178][44/233]	Loss 0.0002 (0.0014)	
training:	Epoch: [178][45/233]	Loss 0.0003 (0.0013)	
training:	Epoch: [178][46/233]	Loss 0.0022 (0.0014)	
training:	Epoch: [178][47/233]	Loss 0.0002 (0.0013)	
training:	Epoch: [178][48/233]	Loss 0.0002 (0.0013)	
training:	Epoch: [178][49/233]	Loss 0.0002 (0.0013)	
training:	Epoch: [178][50/233]	Loss 0.0004 (0.0013)	
training:	Epoch: [178][51/233]	Loss 0.0002 (0.0012)	
training:	Epoch: [178][52/233]	Loss 0.0005 (0.0012)	
training:	Epoch: [178][53/233]	Loss 0.0004 (0.0012)	
training:	Epoch: [178][54/233]	Loss 0.0002 (0.0012)	
training:	Epoch: [178][55/233]	Loss 0.0002 (0.0012)	
training:	Epoch: [178][56/233]	Loss 0.0001 (0.0012)	
training:	Epoch: [178][57/233]	Loss 0.0002 (0.0011)	
training:	Epoch: [178][58/233]	Loss 0.0002 (0.0011)	
training:	Epoch: [178][59/233]	Loss 0.0046 (0.0012)	
training:	Epoch: [178][60/233]	Loss 0.0002 (0.0012)	
training:	Epoch: [178][61/233]	Loss 0.0002 (0.0011)	
training:	Epoch: [178][62/233]	Loss 0.0002 (0.0011)	
training:	Epoch: [178][63/233]	Loss 0.0003 (0.0011)	
training:	Epoch: [178][64/233]	Loss 0.0028 (0.0011)	
training:	Epoch: [178][65/233]	Loss 0.0002 (0.0011)	
training:	Epoch: [178][66/233]	Loss 0.0010 (0.0011)	
training:	Epoch: [178][67/233]	Loss 0.0007 (0.0011)	
training:	Epoch: [178][68/233]	Loss 0.0003 (0.0011)	
training:	Epoch: [178][69/233]	Loss 0.0004 (0.0011)	
training:	Epoch: [178][70/233]	Loss 0.0002 (0.0011)	
training:	Epoch: [178][71/233]	Loss 0.0002 (0.0011)	
training:	Epoch: [178][72/233]	Loss 0.0002 (0.0011)	
training:	Epoch: [178][73/233]	Loss 0.0005 (0.0011)	
training:	Epoch: [178][74/233]	Loss 0.0002 (0.0010)	
training:	Epoch: [178][75/233]	Loss 0.0036 (0.0011)	
training:	Epoch: [178][76/233]	Loss 0.0003 (0.0011)	
training:	Epoch: [178][77/233]	Loss 0.0002 (0.0011)	
training:	Epoch: [178][78/233]	Loss 0.0003 (0.0010)	
training:	Epoch: [178][79/233]	Loss 0.0447 (0.0016)	
training:	Epoch: [178][80/233]	Loss 0.0005 (0.0016)	
training:	Epoch: [178][81/233]	Loss 0.0001 (0.0016)	
training:	Epoch: [178][82/233]	Loss 0.0011 (0.0016)	
training:	Epoch: [178][83/233]	Loss 0.0002 (0.0015)	
training:	Epoch: [178][84/233]	Loss 0.0003 (0.0015)	
training:	Epoch: [178][85/233]	Loss 0.0002 (0.0015)	
training:	Epoch: [178][86/233]	Loss 0.0002 (0.0015)	
training:	Epoch: [178][87/233]	Loss 0.0013 (0.0015)	
training:	Epoch: [178][88/233]	Loss 0.0002 (0.0015)	
training:	Epoch: [178][89/233]	Loss 0.0004 (0.0015)	
training:	Epoch: [178][90/233]	Loss 0.0002 (0.0015)	
training:	Epoch: [178][91/233]	Loss 0.0002 (0.0014)	
training:	Epoch: [178][92/233]	Loss 0.0003 (0.0014)	
training:	Epoch: [178][93/233]	Loss 0.0002 (0.0014)	
training:	Epoch: [178][94/233]	Loss 0.0002 (0.0014)	
training:	Epoch: [178][95/233]	Loss 0.0003 (0.0014)	
training:	Epoch: [178][96/233]	Loss 0.0003 (0.0014)	
training:	Epoch: [178][97/233]	Loss 0.0003 (0.0014)	
training:	Epoch: [178][98/233]	Loss 0.0002 (0.0014)	
training:	Epoch: [178][99/233]	Loss 0.0088 (0.0014)	
training:	Epoch: [178][100/233]	Loss 0.0005 (0.0014)	
training:	Epoch: [178][101/233]	Loss 0.0001 (0.0014)	
training:	Epoch: [178][102/233]	Loss 0.1077 (0.0024)	
training:	Epoch: [178][103/233]	Loss 0.0467 (0.0029)	
training:	Epoch: [178][104/233]	Loss 0.0546 (0.0034)	
training:	Epoch: [178][105/233]	Loss 0.0007 (0.0033)	
training:	Epoch: [178][106/233]	Loss 0.0005 (0.0033)	
training:	Epoch: [178][107/233]	Loss 0.0002 (0.0033)	
training:	Epoch: [178][108/233]	Loss 0.0002 (0.0033)	
training:	Epoch: [178][109/233]	Loss 0.0002 (0.0032)	
training:	Epoch: [178][110/233]	Loss 0.0002 (0.0032)	
training:	Epoch: [178][111/233]	Loss 0.0005 (0.0032)	
training:	Epoch: [178][112/233]	Loss 0.0004 (0.0032)	
training:	Epoch: [178][113/233]	Loss 0.0001 (0.0031)	
training:	Epoch: [178][114/233]	Loss 0.0002 (0.0031)	
training:	Epoch: [178][115/233]	Loss 0.0003 (0.0031)	
training:	Epoch: [178][116/233]	Loss 0.0002 (0.0031)	
training:	Epoch: [178][117/233]	Loss 0.0001 (0.0030)	
training:	Epoch: [178][118/233]	Loss 0.0001 (0.0030)	
training:	Epoch: [178][119/233]	Loss 0.0002 (0.0030)	
training:	Epoch: [178][120/233]	Loss 0.0003 (0.0030)	
training:	Epoch: [178][121/233]	Loss 0.0002 (0.0029)	
training:	Epoch: [178][122/233]	Loss 0.0002 (0.0029)	
training:	Epoch: [178][123/233]	Loss 0.0030 (0.0029)	
training:	Epoch: [178][124/233]	Loss 0.0005 (0.0029)	
training:	Epoch: [178][125/233]	Loss 0.0686 (0.0034)	
training:	Epoch: [178][126/233]	Loss 0.0016 (0.0034)	
training:	Epoch: [178][127/233]	Loss 0.0005 (0.0034)	
training:	Epoch: [178][128/233]	Loss 0.0049 (0.0034)	
training:	Epoch: [178][129/233]	Loss 0.1206 (0.0043)	
training:	Epoch: [178][130/233]	Loss 0.0003 (0.0043)	
training:	Epoch: [178][131/233]	Loss 0.0002 (0.0042)	
training:	Epoch: [178][132/233]	Loss 0.0002 (0.0042)	
training:	Epoch: [178][133/233]	Loss 0.0003 (0.0042)	
training:	Epoch: [178][134/233]	Loss 0.0003 (0.0042)	
training:	Epoch: [178][135/233]	Loss 0.0002 (0.0041)	
training:	Epoch: [178][136/233]	Loss 0.0002 (0.0041)	
training:	Epoch: [178][137/233]	Loss 0.0002 (0.0041)	
training:	Epoch: [178][138/233]	Loss 0.0284 (0.0042)	
training:	Epoch: [178][139/233]	Loss 0.0002 (0.0042)	
training:	Epoch: [178][140/233]	Loss 0.0023 (0.0042)	
training:	Epoch: [178][141/233]	Loss 0.0002 (0.0042)	
training:	Epoch: [178][142/233]	Loss 0.0008 (0.0042)	
training:	Epoch: [178][143/233]	Loss 0.0004 (0.0041)	
training:	Epoch: [178][144/233]	Loss 0.0002 (0.0041)	
training:	Epoch: [178][145/233]	Loss 0.0178 (0.0042)	
training:	Epoch: [178][146/233]	Loss 0.0943 (0.0048)	
training:	Epoch: [178][147/233]	Loss 0.0003 (0.0048)	
training:	Epoch: [178][148/233]	Loss 0.0002 (0.0047)	
training:	Epoch: [178][149/233]	Loss 0.0003 (0.0047)	
training:	Epoch: [178][150/233]	Loss 0.0003 (0.0047)	
training:	Epoch: [178][151/233]	Loss 0.0006 (0.0047)	
training:	Epoch: [178][152/233]	Loss 0.0008 (0.0046)	
training:	Epoch: [178][153/233]	Loss 0.0004 (0.0046)	
training:	Epoch: [178][154/233]	Loss 0.0002 (0.0046)	
training:	Epoch: [178][155/233]	Loss 0.0009 (0.0046)	
training:	Epoch: [178][156/233]	Loss 0.0002 (0.0045)	
training:	Epoch: [178][157/233]	Loss 0.0001 (0.0045)	
training:	Epoch: [178][158/233]	Loss 0.0031 (0.0045)	
training:	Epoch: [178][159/233]	Loss 0.0009 (0.0045)	
training:	Epoch: [178][160/233]	Loss 0.0002 (0.0044)	
training:	Epoch: [178][161/233]	Loss 0.0005 (0.0044)	
training:	Epoch: [178][162/233]	Loss 0.0003 (0.0044)	
training:	Epoch: [178][163/233]	Loss 0.0002 (0.0044)	
training:	Epoch: [178][164/233]	Loss 0.0002 (0.0043)	
training:	Epoch: [178][165/233]	Loss 0.0003 (0.0043)	
training:	Epoch: [178][166/233]	Loss 0.0002 (0.0043)	
training:	Epoch: [178][167/233]	Loss 0.0010 (0.0043)	
training:	Epoch: [178][168/233]	Loss 0.0004 (0.0042)	
training:	Epoch: [178][169/233]	Loss 0.0003 (0.0042)	
training:	Epoch: [178][170/233]	Loss 0.0015 (0.0042)	
training:	Epoch: [178][171/233]	Loss 0.0008 (0.0042)	
training:	Epoch: [178][172/233]	Loss 0.0089 (0.0042)	
training:	Epoch: [178][173/233]	Loss 0.0014 (0.0042)	
training:	Epoch: [178][174/233]	Loss 0.0003 (0.0042)	
training:	Epoch: [178][175/233]	Loss 0.0017 (0.0042)	
training:	Epoch: [178][176/233]	Loss 0.0003 (0.0041)	
training:	Epoch: [178][177/233]	Loss 0.0004 (0.0041)	
training:	Epoch: [178][178/233]	Loss 0.0032 (0.0041)	
training:	Epoch: [178][179/233]	Loss 0.0002 (0.0041)	
training:	Epoch: [178][180/233]	Loss 0.0124 (0.0041)	
training:	Epoch: [178][181/233]	Loss 0.0005 (0.0041)	
training:	Epoch: [178][182/233]	Loss 0.0003 (0.0041)	
training:	Epoch: [178][183/233]	Loss 0.0012 (0.0041)	
training:	Epoch: [178][184/233]	Loss 0.0007 (0.0041)	
training:	Epoch: [178][185/233]	Loss 0.0002 (0.0040)	
training:	Epoch: [178][186/233]	Loss 0.0003 (0.0040)	
training:	Epoch: [178][187/233]	Loss 0.0003 (0.0040)	
training:	Epoch: [178][188/233]	Loss 0.0012 (0.0040)	
training:	Epoch: [178][189/233]	Loss 0.0005 (0.0040)	
training:	Epoch: [178][190/233]	Loss 0.0007 (0.0040)	
training:	Epoch: [178][191/233]	Loss 0.0003 (0.0039)	
training:	Epoch: [178][192/233]	Loss 0.0002 (0.0039)	
training:	Epoch: [178][193/233]	Loss 0.0052 (0.0039)	
training:	Epoch: [178][194/233]	Loss 0.0045 (0.0039)	
training:	Epoch: [178][195/233]	Loss 0.0005 (0.0039)	
training:	Epoch: [178][196/233]	Loss 0.0001 (0.0039)	
training:	Epoch: [178][197/233]	Loss 0.0032 (0.0039)	
training:	Epoch: [178][198/233]	Loss 0.0003 (0.0039)	
training:	Epoch: [178][199/233]	Loss 0.0002 (0.0038)	
training:	Epoch: [178][200/233]	Loss 0.0002 (0.0038)	
training:	Epoch: [178][201/233]	Loss 0.0020 (0.0038)	
training:	Epoch: [178][202/233]	Loss 0.0005 (0.0038)	
training:	Epoch: [178][203/233]	Loss 0.0002 (0.0038)	
training:	Epoch: [178][204/233]	Loss 0.0007 (0.0038)	
training:	Epoch: [178][205/233]	Loss 0.0002 (0.0038)	
training:	Epoch: [178][206/233]	Loss 0.0004 (0.0037)	
training:	Epoch: [178][207/233]	Loss 0.0107 (0.0038)	
training:	Epoch: [178][208/233]	Loss 0.0002 (0.0038)	
training:	Epoch: [178][209/233]	Loss 0.0037 (0.0038)	
training:	Epoch: [178][210/233]	Loss 0.0002 (0.0037)	
training:	Epoch: [178][211/233]	Loss 0.0012 (0.0037)	
training:	Epoch: [178][212/233]	Loss 0.0005 (0.0037)	
training:	Epoch: [178][213/233]	Loss 0.0002 (0.0037)	
training:	Epoch: [178][214/233]	Loss 0.0004 (0.0037)	
training:	Epoch: [178][215/233]	Loss 0.0002 (0.0037)	
training:	Epoch: [178][216/233]	Loss 0.0012 (0.0036)	
training:	Epoch: [178][217/233]	Loss 0.0003 (0.0036)	
training:	Epoch: [178][218/233]	Loss 0.0022 (0.0036)	
training:	Epoch: [178][219/233]	Loss 0.0002 (0.0036)	
training:	Epoch: [178][220/233]	Loss 0.0023 (0.0036)	
training:	Epoch: [178][221/233]	Loss 0.0002 (0.0036)	
training:	Epoch: [178][222/233]	Loss 0.0009 (0.0036)	
training:	Epoch: [178][223/233]	Loss 0.0004 (0.0036)	
training:	Epoch: [178][224/233]	Loss 0.0002 (0.0035)	
training:	Epoch: [178][225/233]	Loss 0.0002 (0.0035)	
training:	Epoch: [178][226/233]	Loss 0.0002 (0.0035)	
training:	Epoch: [178][227/233]	Loss 0.0002 (0.0035)	
training:	Epoch: [178][228/233]	Loss 0.0004 (0.0035)	
training:	Epoch: [178][229/233]	Loss 0.0002 (0.0035)	
training:	Epoch: [178][230/233]	Loss 0.0007 (0.0035)	
training:	Epoch: [178][231/233]	Loss 0.0002 (0.0035)	
training:	Epoch: [178][232/233]	Loss 0.0002 (0.0034)	
training:	Epoch: [178][233/233]	Loss 0.0008 (0.0034)	
Training:	 Loss: 0.0034

Training:	 ACC: 1.0000 1.0000 1.0000 1.0000
Validation:	 ACC: 0.7988 0.7978 0.7773 0.8202
Validation:	 Best_BACC: 0.8162 0.8154 0.7998 0.8326
Validation:	 Loss: 1.3325
Pretraining:	Epoch 179/200
----------
training:	Epoch: [179][1/233]	Loss 0.0002 (0.0002)	
training:	Epoch: [179][2/233]	Loss 0.0003 (0.0002)	
training:	Epoch: [179][3/233]	Loss 0.0003 (0.0002)	
training:	Epoch: [179][4/233]	Loss 0.0002 (0.0002)	
training:	Epoch: [179][5/233]	Loss 0.0002 (0.0002)	
training:	Epoch: [179][6/233]	Loss 0.0002 (0.0002)	
training:	Epoch: [179][7/233]	Loss 0.0002 (0.0002)	
training:	Epoch: [179][8/233]	Loss 0.0003 (0.0002)	
training:	Epoch: [179][9/233]	Loss 0.0001 (0.0002)	
training:	Epoch: [179][10/233]	Loss 0.0002 (0.0002)	
training:	Epoch: [179][11/233]	Loss 0.0022 (0.0004)	
training:	Epoch: [179][12/233]	Loss 0.0007 (0.0004)	
training:	Epoch: [179][13/233]	Loss 0.0025 (0.0006)	
training:	Epoch: [179][14/233]	Loss 0.0005 (0.0006)	
training:	Epoch: [179][15/233]	Loss 0.0002 (0.0005)	
training:	Epoch: [179][16/233]	Loss 0.0002 (0.0005)	
training:	Epoch: [179][17/233]	Loss 0.0009 (0.0005)	
training:	Epoch: [179][18/233]	Loss 0.0002 (0.0005)	
training:	Epoch: [179][19/233]	Loss 0.0001 (0.0005)	
training:	Epoch: [179][20/233]	Loss 0.0003 (0.0005)	
training:	Epoch: [179][21/233]	Loss 0.0002 (0.0005)	
training:	Epoch: [179][22/233]	Loss 0.0001 (0.0005)	
training:	Epoch: [179][23/233]	Loss 0.0002 (0.0004)	
training:	Epoch: [179][24/233]	Loss 0.0007 (0.0005)	
training:	Epoch: [179][25/233]	Loss 0.0002 (0.0004)	
training:	Epoch: [179][26/233]	Loss 0.0002 (0.0004)	
training:	Epoch: [179][27/233]	Loss 0.0004 (0.0004)	
training:	Epoch: [179][28/233]	Loss 0.0001 (0.0004)	
training:	Epoch: [179][29/233]	Loss 0.0002 (0.0004)	
training:	Epoch: [179][30/233]	Loss 0.0002 (0.0004)	
training:	Epoch: [179][31/233]	Loss 0.0002 (0.0004)	
training:	Epoch: [179][32/233]	Loss 0.0002 (0.0004)	
training:	Epoch: [179][33/233]	Loss 0.0002 (0.0004)	
training:	Epoch: [179][34/233]	Loss 0.0002 (0.0004)	
training:	Epoch: [179][35/233]	Loss 0.0002 (0.0004)	
training:	Epoch: [179][36/233]	Loss 0.0001 (0.0004)	
training:	Epoch: [179][37/233]	Loss 0.0002 (0.0004)	
training:	Epoch: [179][38/233]	Loss 0.0005 (0.0004)	
training:	Epoch: [179][39/233]	Loss 0.0002 (0.0004)	
training:	Epoch: [179][40/233]	Loss 0.0002 (0.0004)	
training:	Epoch: [179][41/233]	Loss 0.0004 (0.0004)	
training:	Epoch: [179][42/233]	Loss 0.0002 (0.0004)	
training:	Epoch: [179][43/233]	Loss 0.0002 (0.0004)	
training:	Epoch: [179][44/233]	Loss 0.0005 (0.0004)	
training:	Epoch: [179][45/233]	Loss 0.0002 (0.0004)	
training:	Epoch: [179][46/233]	Loss 0.0002 (0.0004)	
training:	Epoch: [179][47/233]	Loss 0.0002 (0.0003)	
training:	Epoch: [179][48/233]	Loss 0.0003 (0.0003)	
training:	Epoch: [179][49/233]	Loss 0.0002 (0.0003)	
training:	Epoch: [179][50/233]	Loss 0.0002 (0.0003)	
training:	Epoch: [179][51/233]	Loss 0.0001 (0.0003)	
training:	Epoch: [179][52/233]	Loss 0.0003 (0.0003)	
training:	Epoch: [179][53/233]	Loss 0.0004 (0.0003)	
training:	Epoch: [179][54/233]	Loss 0.0002 (0.0003)	
training:	Epoch: [179][55/233]	Loss 0.0002 (0.0003)	
training:	Epoch: [179][56/233]	Loss 0.0003 (0.0003)	
training:	Epoch: [179][57/233]	Loss 0.0003 (0.0003)	
training:	Epoch: [179][58/233]	Loss 0.0003 (0.0003)	
training:	Epoch: [179][59/233]	Loss 0.0002 (0.0003)	
training:	Epoch: [179][60/233]	Loss 0.0003 (0.0003)	
training:	Epoch: [179][61/233]	Loss 0.0002 (0.0003)	
training:	Epoch: [179][62/233]	Loss 0.0002 (0.0003)	
training:	Epoch: [179][63/233]	Loss 0.0048 (0.0004)	
training:	Epoch: [179][64/233]	Loss 0.0008 (0.0004)	
training:	Epoch: [179][65/233]	Loss 0.0002 (0.0004)	
training:	Epoch: [179][66/233]	Loss 0.0001 (0.0004)	
training:	Epoch: [179][67/233]	Loss 0.0002 (0.0004)	
training:	Epoch: [179][68/233]	Loss 0.0002 (0.0004)	
training:	Epoch: [179][69/233]	Loss 0.0003 (0.0004)	
training:	Epoch: [179][70/233]	Loss 0.0002 (0.0004)	
training:	Epoch: [179][71/233]	Loss 0.0002 (0.0004)	
training:	Epoch: [179][72/233]	Loss 0.0002 (0.0004)	
training:	Epoch: [179][73/233]	Loss 0.0034 (0.0004)	
training:	Epoch: [179][74/233]	Loss 0.0005 (0.0004)	
training:	Epoch: [179][75/233]	Loss 0.0003 (0.0004)	
training:	Epoch: [179][76/233]	Loss 0.0002 (0.0004)	
training:	Epoch: [179][77/233]	Loss 0.0002 (0.0004)	
training:	Epoch: [179][78/233]	Loss 0.0002 (0.0004)	
training:	Epoch: [179][79/233]	Loss 0.0120 (0.0006)	
training:	Epoch: [179][80/233]	Loss 0.0003 (0.0006)	
training:	Epoch: [179][81/233]	Loss 0.0002 (0.0006)	
training:	Epoch: [179][82/233]	Loss 0.0002 (0.0005)	
training:	Epoch: [179][83/233]	Loss 0.0002 (0.0005)	
training:	Epoch: [179][84/233]	Loss 0.0002 (0.0005)	
training:	Epoch: [179][85/233]	Loss 0.0004 (0.0005)	
training:	Epoch: [179][86/233]	Loss 0.0002 (0.0005)	
training:	Epoch: [179][87/233]	Loss 0.0002 (0.0005)	
training:	Epoch: [179][88/233]	Loss 0.0003 (0.0005)	
training:	Epoch: [179][89/233]	Loss 0.0002 (0.0005)	
training:	Epoch: [179][90/233]	Loss 0.0001 (0.0005)	
training:	Epoch: [179][91/233]	Loss 0.0002 (0.0005)	
training:	Epoch: [179][92/233]	Loss 0.0003 (0.0005)	
training:	Epoch: [179][93/233]	Loss 0.0006 (0.0005)	
training:	Epoch: [179][94/233]	Loss 0.0003 (0.0005)	
training:	Epoch: [179][95/233]	Loss 0.0106 (0.0006)	
training:	Epoch: [179][96/233]	Loss 0.0002 (0.0006)	
training:	Epoch: [179][97/233]	Loss 0.0002 (0.0006)	
training:	Epoch: [179][98/233]	Loss 0.0002 (0.0006)	
training:	Epoch: [179][99/233]	Loss 0.0002 (0.0006)	
training:	Epoch: [179][100/233]	Loss 0.0002 (0.0006)	
training:	Epoch: [179][101/233]	Loss 0.0002 (0.0006)	
training:	Epoch: [179][102/233]	Loss 0.0033 (0.0006)	
training:	Epoch: [179][103/233]	Loss 0.0002 (0.0006)	
training:	Epoch: [179][104/233]	Loss 0.0002 (0.0006)	
training:	Epoch: [179][105/233]	Loss 0.0002 (0.0006)	
training:	Epoch: [179][106/233]	Loss 0.0003 (0.0006)	
training:	Epoch: [179][107/233]	Loss 0.0004 (0.0006)	
training:	Epoch: [179][108/233]	Loss 0.0002 (0.0006)	
training:	Epoch: [179][109/233]	Loss 0.0002 (0.0006)	
training:	Epoch: [179][110/233]	Loss 0.0003 (0.0006)	
training:	Epoch: [179][111/233]	Loss 0.0002 (0.0006)	
training:	Epoch: [179][112/233]	Loss 0.0002 (0.0006)	
training:	Epoch: [179][113/233]	Loss 0.0001 (0.0006)	
training:	Epoch: [179][114/233]	Loss 0.0002 (0.0006)	
training:	Epoch: [179][115/233]	Loss 0.0002 (0.0006)	
training:	Epoch: [179][116/233]	Loss 0.0003 (0.0006)	
training:	Epoch: [179][117/233]	Loss 0.0002 (0.0006)	
training:	Epoch: [179][118/233]	Loss 0.0002 (0.0006)	
training:	Epoch: [179][119/233]	Loss 0.0003 (0.0006)	
training:	Epoch: [179][120/233]	Loss 0.0002 (0.0006)	
training:	Epoch: [179][121/233]	Loss 0.0003 (0.0006)	
training:	Epoch: [179][122/233]	Loss 0.0002 (0.0006)	
training:	Epoch: [179][123/233]	Loss 0.0019 (0.0006)	
training:	Epoch: [179][124/233]	Loss 0.0002 (0.0006)	
training:	Epoch: [179][125/233]	Loss 0.0003 (0.0006)	
training:	Epoch: [179][126/233]	Loss 0.0002 (0.0006)	
training:	Epoch: [179][127/233]	Loss 0.0002 (0.0006)	
training:	Epoch: [179][128/233]	Loss 0.0003 (0.0005)	
training:	Epoch: [179][129/233]	Loss 0.0019 (0.0006)	
training:	Epoch: [179][130/233]	Loss 0.0002 (0.0006)	
training:	Epoch: [179][131/233]	Loss 0.0004 (0.0006)	
training:	Epoch: [179][132/233]	Loss 0.0003 (0.0006)	
training:	Epoch: [179][133/233]	Loss 0.0002 (0.0006)	
training:	Epoch: [179][134/233]	Loss 0.0001 (0.0005)	
training:	Epoch: [179][135/233]	Loss 0.0002 (0.0005)	
training:	Epoch: [179][136/233]	Loss 0.0002 (0.0005)	
training:	Epoch: [179][137/233]	Loss 0.0282 (0.0007)	
training:	Epoch: [179][138/233]	Loss 0.0002 (0.0007)	
training:	Epoch: [179][139/233]	Loss 0.0002 (0.0007)	
training:	Epoch: [179][140/233]	Loss 0.0002 (0.0007)	
training:	Epoch: [179][141/233]	Loss 0.0003 (0.0007)	
training:	Epoch: [179][142/233]	Loss 0.0003 (0.0007)	
training:	Epoch: [179][143/233]	Loss 0.0002 (0.0007)	
training:	Epoch: [179][144/233]	Loss 0.0002 (0.0007)	
training:	Epoch: [179][145/233]	Loss 0.0001 (0.0007)	
training:	Epoch: [179][146/233]	Loss 0.0002 (0.0007)	
training:	Epoch: [179][147/233]	Loss 0.0002 (0.0007)	
training:	Epoch: [179][148/233]	Loss 0.0002 (0.0007)	
training:	Epoch: [179][149/233]	Loss 0.0002 (0.0007)	
training:	Epoch: [179][150/233]	Loss 0.0002 (0.0007)	
training:	Epoch: [179][151/233]	Loss 0.0002 (0.0007)	
training:	Epoch: [179][152/233]	Loss 0.0003 (0.0007)	
training:	Epoch: [179][153/233]	Loss 0.0003 (0.0007)	
training:	Epoch: [179][154/233]	Loss 0.0082 (0.0007)	
training:	Epoch: [179][155/233]	Loss 0.0001 (0.0007)	
training:	Epoch: [179][156/233]	Loss 0.0002 (0.0007)	
training:	Epoch: [179][157/233]	Loss 0.0006 (0.0007)	
training:	Epoch: [179][158/233]	Loss 0.0002 (0.0007)	
training:	Epoch: [179][159/233]	Loss 0.0846 (0.0013)	
training:	Epoch: [179][160/233]	Loss 0.0011 (0.0013)	
training:	Epoch: [179][161/233]	Loss 0.0002 (0.0012)	
training:	Epoch: [179][162/233]	Loss 0.0002 (0.0012)	
training:	Epoch: [179][163/233]	Loss 0.0001 (0.0012)	
training:	Epoch: [179][164/233]	Loss 0.0007 (0.0012)	
training:	Epoch: [179][165/233]	Loss 0.0013 (0.0012)	
training:	Epoch: [179][166/233]	Loss 0.0009 (0.0012)	
training:	Epoch: [179][167/233]	Loss 0.0005 (0.0012)	
training:	Epoch: [179][168/233]	Loss 0.0002 (0.0012)	
training:	Epoch: [179][169/233]	Loss 0.0002 (0.0012)	
training:	Epoch: [179][170/233]	Loss 0.0002 (0.0012)	
training:	Epoch: [179][171/233]	Loss 0.0002 (0.0012)	
training:	Epoch: [179][172/233]	Loss 0.0055 (0.0012)	
training:	Epoch: [179][173/233]	Loss 0.0002 (0.0012)	
training:	Epoch: [179][174/233]	Loss 0.0003 (0.0012)	
training:	Epoch: [179][175/233]	Loss 0.0012 (0.0012)	
training:	Epoch: [179][176/233]	Loss 0.0002 (0.0012)	
training:	Epoch: [179][177/233]	Loss 0.0002 (0.0012)	
training:	Epoch: [179][178/233]	Loss 0.0002 (0.0012)	
training:	Epoch: [179][179/233]	Loss 0.0003 (0.0012)	
training:	Epoch: [179][180/233]	Loss 0.0002 (0.0012)	
training:	Epoch: [179][181/233]	Loss 0.0056 (0.0012)	
training:	Epoch: [179][182/233]	Loss 0.0004 (0.0012)	
training:	Epoch: [179][183/233]	Loss 0.0003 (0.0012)	
training:	Epoch: [179][184/233]	Loss 0.0002 (0.0012)	
training:	Epoch: [179][185/233]	Loss 0.0002 (0.0012)	
training:	Epoch: [179][186/233]	Loss 0.0002 (0.0012)	
training:	Epoch: [179][187/233]	Loss 0.0002 (0.0012)	
training:	Epoch: [179][188/233]	Loss 0.0003 (0.0012)	
training:	Epoch: [179][189/233]	Loss 0.0002 (0.0012)	
training:	Epoch: [179][190/233]	Loss 0.0002 (0.0012)	
training:	Epoch: [179][191/233]	Loss 0.0003 (0.0012)	
training:	Epoch: [179][192/233]	Loss 0.0002 (0.0012)	
training:	Epoch: [179][193/233]	Loss 0.0013 (0.0012)	
training:	Epoch: [179][194/233]	Loss 0.0002 (0.0011)	
training:	Epoch: [179][195/233]	Loss 0.0001 (0.0011)	
training:	Epoch: [179][196/233]	Loss 0.0053 (0.0012)	
training:	Epoch: [179][197/233]	Loss 0.0002 (0.0012)	
training:	Epoch: [179][198/233]	Loss 0.0007 (0.0012)	
training:	Epoch: [179][199/233]	Loss 0.0002 (0.0012)	
training:	Epoch: [179][200/233]	Loss 0.0002 (0.0011)	
training:	Epoch: [179][201/233]	Loss 0.0002 (0.0011)	
training:	Epoch: [179][202/233]	Loss 0.0002 (0.0011)	
training:	Epoch: [179][203/233]	Loss 0.0002 (0.0011)	
training:	Epoch: [179][204/233]	Loss 0.0002 (0.0011)	
training:	Epoch: [179][205/233]	Loss 0.0009 (0.0011)	
training:	Epoch: [179][206/233]	Loss 0.0002 (0.0011)	
training:	Epoch: [179][207/233]	Loss 0.0002 (0.0011)	
training:	Epoch: [179][208/233]	Loss 0.0002 (0.0011)	
training:	Epoch: [179][209/233]	Loss 0.0001 (0.0011)	
training:	Epoch: [179][210/233]	Loss 0.0002 (0.0011)	
training:	Epoch: [179][211/233]	Loss 0.0001 (0.0011)	
training:	Epoch: [179][212/233]	Loss 0.0002 (0.0011)	
training:	Epoch: [179][213/233]	Loss 0.0002 (0.0011)	
training:	Epoch: [179][214/233]	Loss 0.0002 (0.0011)	
training:	Epoch: [179][215/233]	Loss 0.0002 (0.0011)	
training:	Epoch: [179][216/233]	Loss 0.0002 (0.0011)	
training:	Epoch: [179][217/233]	Loss 0.0002 (0.0011)	
training:	Epoch: [179][218/233]	Loss 0.0001 (0.0011)	
training:	Epoch: [179][219/233]	Loss 0.0001 (0.0011)	
training:	Epoch: [179][220/233]	Loss 0.0001 (0.0011)	
training:	Epoch: [179][221/233]	Loss 0.0002 (0.0011)	
training:	Epoch: [179][222/233]	Loss 0.0002 (0.0011)	
training:	Epoch: [179][223/233]	Loss 0.0007 (0.0011)	
training:	Epoch: [179][224/233]	Loss 0.0002 (0.0010)	
training:	Epoch: [179][225/233]	Loss 0.0001 (0.0010)	
training:	Epoch: [179][226/233]	Loss 0.0061 (0.0011)	
training:	Epoch: [179][227/233]	Loss 0.0002 (0.0011)	
training:	Epoch: [179][228/233]	Loss 0.0002 (0.0011)	
training:	Epoch: [179][229/233]	Loss 0.0003 (0.0011)	
training:	Epoch: [179][230/233]	Loss 0.0003 (0.0011)	
training:	Epoch: [179][231/233]	Loss 0.0002 (0.0010)	
training:	Epoch: [179][232/233]	Loss 0.0004 (0.0010)	
training:	Epoch: [179][233/233]	Loss 0.0003 (0.0010)	
Training:	 Loss: 0.0010

Training:	 ACC: 1.0000 1.0000 1.0000 1.0000
Validation:	 ACC: 0.7960 0.7951 0.7773 0.8146
Validation:	 Best_BACC: 0.8162 0.8154 0.7998 0.8326
Validation:	 Loss: 1.3702
Pretraining:	Epoch 180/200
----------
training:	Epoch: [180][1/233]	Loss 0.0004 (0.0004)	
training:	Epoch: [180][2/233]	Loss 0.0002 (0.0003)	
training:	Epoch: [180][3/233]	Loss 0.0002 (0.0003)	
training:	Epoch: [180][4/233]	Loss 0.0001 (0.0002)	
training:	Epoch: [180][5/233]	Loss 0.0002 (0.0002)	
training:	Epoch: [180][6/233]	Loss 0.0003 (0.0002)	
training:	Epoch: [180][7/233]	Loss 0.0001 (0.0002)	
training:	Epoch: [180][8/233]	Loss 0.0007 (0.0003)	
training:	Epoch: [180][9/233]	Loss 0.0002 (0.0003)	
training:	Epoch: [180][10/233]	Loss 0.0002 (0.0003)	
training:	Epoch: [180][11/233]	Loss 0.0002 (0.0003)	
training:	Epoch: [180][12/233]	Loss 0.0002 (0.0002)	
training:	Epoch: [180][13/233]	Loss 0.0002 (0.0002)	
training:	Epoch: [180][14/233]	Loss 0.0002 (0.0002)	
training:	Epoch: [180][15/233]	Loss 0.0002 (0.0002)	
training:	Epoch: [180][16/233]	Loss 0.0010 (0.0003)	
training:	Epoch: [180][17/233]	Loss 0.0002 (0.0003)	
training:	Epoch: [180][18/233]	Loss 0.0002 (0.0003)	
training:	Epoch: [180][19/233]	Loss 0.0005 (0.0003)	
training:	Epoch: [180][20/233]	Loss 0.0002 (0.0003)	
training:	Epoch: [180][21/233]	Loss 0.0002 (0.0003)	
training:	Epoch: [180][22/233]	Loss 0.0002 (0.0003)	
training:	Epoch: [180][23/233]	Loss 0.0004 (0.0003)	
training:	Epoch: [180][24/233]	Loss 0.0001 (0.0003)	
training:	Epoch: [180][25/233]	Loss 0.0001 (0.0003)	
training:	Epoch: [180][26/233]	Loss 0.0016 (0.0003)	
training:	Epoch: [180][27/233]	Loss 0.0002 (0.0003)	
training:	Epoch: [180][28/233]	Loss 0.0002 (0.0003)	
training:	Epoch: [180][29/233]	Loss 0.0002 (0.0003)	
training:	Epoch: [180][30/233]	Loss 0.0003 (0.0003)	
training:	Epoch: [180][31/233]	Loss 0.0001 (0.0003)	
training:	Epoch: [180][32/233]	Loss 0.0001 (0.0003)	
training:	Epoch: [180][33/233]	Loss 0.0002 (0.0003)	
training:	Epoch: [180][34/233]	Loss 0.0020 (0.0003)	
training:	Epoch: [180][35/233]	Loss 0.0003 (0.0003)	
training:	Epoch: [180][36/233]	Loss 0.0002 (0.0003)	
training:	Epoch: [180][37/233]	Loss 0.0003 (0.0003)	
training:	Epoch: [180][38/233]	Loss 0.0001 (0.0003)	
training:	Epoch: [180][39/233]	Loss 0.0002 (0.0003)	
training:	Epoch: [180][40/233]	Loss 0.0002 (0.0003)	
training:	Epoch: [180][41/233]	Loss 0.0002 (0.0003)	
training:	Epoch: [180][42/233]	Loss 0.0002 (0.0003)	
training:	Epoch: [180][43/233]	Loss 0.0003 (0.0003)	
training:	Epoch: [180][44/233]	Loss 0.0006 (0.0003)	
training:	Epoch: [180][45/233]	Loss 0.0004 (0.0003)	
training:	Epoch: [180][46/233]	Loss 0.0001 (0.0003)	
training:	Epoch: [180][47/233]	Loss 0.0001 (0.0003)	
training:	Epoch: [180][48/233]	Loss 0.0002 (0.0003)	
training:	Epoch: [180][49/233]	Loss 0.0001 (0.0003)	
training:	Epoch: [180][50/233]	Loss 0.0002 (0.0003)	
training:	Epoch: [180][51/233]	Loss 0.0003 (0.0003)	
training:	Epoch: [180][52/233]	Loss 0.0005 (0.0003)	
training:	Epoch: [180][53/233]	Loss 0.0004 (0.0003)	
training:	Epoch: [180][54/233]	Loss 0.0002 (0.0003)	
training:	Epoch: [180][55/233]	Loss 0.0002 (0.0003)	
training:	Epoch: [180][56/233]	Loss 0.0002 (0.0003)	
training:	Epoch: [180][57/233]	Loss 0.0004 (0.0003)	
training:	Epoch: [180][58/233]	Loss 0.0003 (0.0003)	
training:	Epoch: [180][59/233]	Loss 0.0002 (0.0003)	
training:	Epoch: [180][60/233]	Loss 0.0002 (0.0003)	
training:	Epoch: [180][61/233]	Loss 0.0001 (0.0003)	
training:	Epoch: [180][62/233]	Loss 0.0001 (0.0003)	
training:	Epoch: [180][63/233]	Loss 0.0006 (0.0003)	
training:	Epoch: [180][64/233]	Loss 0.0035 (0.0004)	
training:	Epoch: [180][65/233]	Loss 0.0002 (0.0004)	
training:	Epoch: [180][66/233]	Loss 0.0002 (0.0003)	
training:	Epoch: [180][67/233]	Loss 0.0003 (0.0003)	
training:	Epoch: [180][68/233]	Loss 0.0018 (0.0004)	
training:	Epoch: [180][69/233]	Loss 0.0006 (0.0004)	
training:	Epoch: [180][70/233]	Loss 0.0002 (0.0004)	
training:	Epoch: [180][71/233]	Loss 0.0002 (0.0004)	
training:	Epoch: [180][72/233]	Loss 0.0003 (0.0004)	
training:	Epoch: [180][73/233]	Loss 0.0003 (0.0004)	
training:	Epoch: [180][74/233]	Loss 0.0006 (0.0004)	
training:	Epoch: [180][75/233]	Loss 0.0008 (0.0004)	
training:	Epoch: [180][76/233]	Loss 0.0002 (0.0004)	
training:	Epoch: [180][77/233]	Loss 0.0002 (0.0004)	
training:	Epoch: [180][78/233]	Loss 0.0024 (0.0004)	
training:	Epoch: [180][79/233]	Loss 0.0003 (0.0004)	
training:	Epoch: [180][80/233]	Loss 0.0001 (0.0004)	
training:	Epoch: [180][81/233]	Loss 0.0003 (0.0004)	
training:	Epoch: [180][82/233]	Loss 0.0003 (0.0004)	
training:	Epoch: [180][83/233]	Loss 0.0001 (0.0004)	
training:	Epoch: [180][84/233]	Loss 0.0002 (0.0004)	
training:	Epoch: [180][85/233]	Loss 0.0210 (0.0006)	
training:	Epoch: [180][86/233]	Loss 0.0001 (0.0006)	
training:	Epoch: [180][87/233]	Loss 0.0002 (0.0006)	
training:	Epoch: [180][88/233]	Loss 0.1597 (0.0024)	
training:	Epoch: [180][89/233]	Loss 0.0002 (0.0024)	
training:	Epoch: [180][90/233]	Loss 0.0001 (0.0024)	
training:	Epoch: [180][91/233]	Loss 0.0002 (0.0023)	
training:	Epoch: [180][92/233]	Loss 0.0001 (0.0023)	
training:	Epoch: [180][93/233]	Loss 0.0002 (0.0023)	
training:	Epoch: [180][94/233]	Loss 0.0002 (0.0023)	
training:	Epoch: [180][95/233]	Loss 0.0002 (0.0023)	
training:	Epoch: [180][96/233]	Loss 0.0002 (0.0022)	
training:	Epoch: [180][97/233]	Loss 0.0006 (0.0022)	
training:	Epoch: [180][98/233]	Loss 0.0847 (0.0031)	
training:	Epoch: [180][99/233]	Loss 0.0002 (0.0030)	
training:	Epoch: [180][100/233]	Loss 0.0002 (0.0030)	
training:	Epoch: [180][101/233]	Loss 0.0002 (0.0030)	
training:	Epoch: [180][102/233]	Loss 0.0002 (0.0029)	
training:	Epoch: [180][103/233]	Loss 0.0003 (0.0029)	
training:	Epoch: [180][104/233]	Loss 0.0002 (0.0029)	
training:	Epoch: [180][105/233]	Loss 0.0001 (0.0029)	
training:	Epoch: [180][106/233]	Loss 0.0002 (0.0028)	
training:	Epoch: [180][107/233]	Loss 0.0001 (0.0028)	
training:	Epoch: [180][108/233]	Loss 0.0001 (0.0028)	
training:	Epoch: [180][109/233]	Loss 0.0002 (0.0028)	
training:	Epoch: [180][110/233]	Loss 0.0002 (0.0027)	
training:	Epoch: [180][111/233]	Loss 0.0025 (0.0027)	
training:	Epoch: [180][112/233]	Loss 0.0002 (0.0027)	
training:	Epoch: [180][113/233]	Loss 0.0002 (0.0027)	
training:	Epoch: [180][114/233]	Loss 0.0009 (0.0027)	
training:	Epoch: [180][115/233]	Loss 0.0022 (0.0027)	
training:	Epoch: [180][116/233]	Loss 0.0003 (0.0027)	
training:	Epoch: [180][117/233]	Loss 0.0008 (0.0026)	
training:	Epoch: [180][118/233]	Loss 0.0024 (0.0026)	
training:	Epoch: [180][119/233]	Loss 0.0002 (0.0026)	
training:	Epoch: [180][120/233]	Loss 0.0003 (0.0026)	
training:	Epoch: [180][121/233]	Loss 0.0003 (0.0026)	
training:	Epoch: [180][122/233]	Loss 0.0004 (0.0026)	
training:	Epoch: [180][123/233]	Loss 0.0004 (0.0025)	
training:	Epoch: [180][124/233]	Loss 0.0002 (0.0025)	
training:	Epoch: [180][125/233]	Loss 0.0002 (0.0025)	
training:	Epoch: [180][126/233]	Loss 0.0339 (0.0028)	
training:	Epoch: [180][127/233]	Loss 0.0002 (0.0027)	
training:	Epoch: [180][128/233]	Loss 0.0002 (0.0027)	
training:	Epoch: [180][129/233]	Loss 0.0007 (0.0027)	
training:	Epoch: [180][130/233]	Loss 0.0001 (0.0027)	
training:	Epoch: [180][131/233]	Loss 0.0001 (0.0027)	
training:	Epoch: [180][132/233]	Loss 0.0002 (0.0026)	
training:	Epoch: [180][133/233]	Loss 0.0001 (0.0026)	
training:	Epoch: [180][134/233]	Loss 0.0004 (0.0026)	
training:	Epoch: [180][135/233]	Loss 0.0001 (0.0026)	
training:	Epoch: [180][136/233]	Loss 0.0002 (0.0026)	
training:	Epoch: [180][137/233]	Loss 0.1271 (0.0035)	
training:	Epoch: [180][138/233]	Loss 0.0003 (0.0035)	
training:	Epoch: [180][139/233]	Loss 0.0001 (0.0034)	
training:	Epoch: [180][140/233]	Loss 0.0002 (0.0034)	
training:	Epoch: [180][141/233]	Loss 0.0002 (0.0034)	
training:	Epoch: [180][142/233]	Loss 0.0002 (0.0034)	
training:	Epoch: [180][143/233]	Loss 0.0002 (0.0033)	
training:	Epoch: [180][144/233]	Loss 0.0006 (0.0033)	
training:	Epoch: [180][145/233]	Loss 0.0003 (0.0033)	
training:	Epoch: [180][146/233]	Loss 0.0003 (0.0033)	
training:	Epoch: [180][147/233]	Loss 0.0002 (0.0033)	
training:	Epoch: [180][148/233]	Loss 0.0002 (0.0032)	
training:	Epoch: [180][149/233]	Loss 0.1118 (0.0040)	
training:	Epoch: [180][150/233]	Loss 0.0002 (0.0039)	
training:	Epoch: [180][151/233]	Loss 0.0007 (0.0039)	
training:	Epoch: [180][152/233]	Loss 0.0002 (0.0039)	
training:	Epoch: [180][153/233]	Loss 0.0002 (0.0039)	
training:	Epoch: [180][154/233]	Loss 0.0002 (0.0039)	
training:	Epoch: [180][155/233]	Loss 0.0007 (0.0038)	
training:	Epoch: [180][156/233]	Loss 0.0002 (0.0038)	
training:	Epoch: [180][157/233]	Loss 0.0002 (0.0038)	
training:	Epoch: [180][158/233]	Loss 0.0157 (0.0039)	
training:	Epoch: [180][159/233]	Loss 0.0001 (0.0038)	
training:	Epoch: [180][160/233]	Loss 0.0002 (0.0038)	
training:	Epoch: [180][161/233]	Loss 0.0003 (0.0038)	
training:	Epoch: [180][162/233]	Loss 0.0001 (0.0038)	
training:	Epoch: [180][163/233]	Loss 0.0002 (0.0037)	
training:	Epoch: [180][164/233]	Loss 0.0003 (0.0037)	
training:	Epoch: [180][165/233]	Loss 0.0003 (0.0037)	
training:	Epoch: [180][166/233]	Loss 0.0002 (0.0037)	
training:	Epoch: [180][167/233]	Loss 0.0003 (0.0037)	
training:	Epoch: [180][168/233]	Loss 0.0002 (0.0036)	
training:	Epoch: [180][169/233]	Loss 0.0007 (0.0036)	
training:	Epoch: [180][170/233]	Loss 0.0061 (0.0036)	
training:	Epoch: [180][171/233]	Loss 0.0002 (0.0036)	
training:	Epoch: [180][172/233]	Loss 0.0002 (0.0036)	
training:	Epoch: [180][173/233]	Loss 0.0005 (0.0036)	
training:	Epoch: [180][174/233]	Loss 0.0011 (0.0036)	
training:	Epoch: [180][175/233]	Loss 0.0010 (0.0036)	
training:	Epoch: [180][176/233]	Loss 0.0012 (0.0035)	
training:	Epoch: [180][177/233]	Loss 0.0002 (0.0035)	
training:	Epoch: [180][178/233]	Loss 0.0005 (0.0035)	
training:	Epoch: [180][179/233]	Loss 0.0002 (0.0035)	
training:	Epoch: [180][180/233]	Loss 0.0002 (0.0035)	
training:	Epoch: [180][181/233]	Loss 0.0200 (0.0036)	
training:	Epoch: [180][182/233]	Loss 0.0033 (0.0036)	
training:	Epoch: [180][183/233]	Loss 0.0002 (0.0035)	
training:	Epoch: [180][184/233]	Loss 0.0006 (0.0035)	
training:	Epoch: [180][185/233]	Loss 0.0005 (0.0035)	
training:	Epoch: [180][186/233]	Loss 0.0002 (0.0035)	
training:	Epoch: [180][187/233]	Loss 0.0178 (0.0036)	
training:	Epoch: [180][188/233]	Loss 0.0007 (0.0036)	
training:	Epoch: [180][189/233]	Loss 0.0002 (0.0035)	
training:	Epoch: [180][190/233]	Loss 0.0004 (0.0035)	
training:	Epoch: [180][191/233]	Loss 0.0013 (0.0035)	
training:	Epoch: [180][192/233]	Loss 0.0002 (0.0035)	
training:	Epoch: [180][193/233]	Loss 0.0004 (0.0035)	
training:	Epoch: [180][194/233]	Loss 0.0002 (0.0035)	
training:	Epoch: [180][195/233]	Loss 0.0003 (0.0034)	
training:	Epoch: [180][196/233]	Loss 0.0005 (0.0034)	
training:	Epoch: [180][197/233]	Loss 0.0002 (0.0034)	
training:	Epoch: [180][198/233]	Loss 0.0002 (0.0034)	
training:	Epoch: [180][199/233]	Loss 0.0003 (0.0034)	
training:	Epoch: [180][200/233]	Loss 0.0001 (0.0034)	
training:	Epoch: [180][201/233]	Loss 0.0003 (0.0033)	
training:	Epoch: [180][202/233]	Loss 0.0008 (0.0033)	
training:	Epoch: [180][203/233]	Loss 0.0003 (0.0033)	
training:	Epoch: [180][204/233]	Loss 0.0002 (0.0033)	
training:	Epoch: [180][205/233]	Loss 0.0002 (0.0033)	
training:	Epoch: [180][206/233]	Loss 0.0002 (0.0033)	
training:	Epoch: [180][207/233]	Loss 0.0002 (0.0033)	
training:	Epoch: [180][208/233]	Loss 0.0002 (0.0032)	
training:	Epoch: [180][209/233]	Loss 0.0002 (0.0032)	
training:	Epoch: [180][210/233]	Loss 0.0001 (0.0032)	
training:	Epoch: [180][211/233]	Loss 0.0003 (0.0032)	
training:	Epoch: [180][212/233]	Loss 0.0002 (0.0032)	
training:	Epoch: [180][213/233]	Loss 0.0012 (0.0032)	
training:	Epoch: [180][214/233]	Loss 0.0002 (0.0032)	
training:	Epoch: [180][215/233]	Loss 0.0004 (0.0031)	
training:	Epoch: [180][216/233]	Loss 0.0002 (0.0031)	
training:	Epoch: [180][217/233]	Loss 0.0002 (0.0031)	
training:	Epoch: [180][218/233]	Loss 0.0005 (0.0031)	
training:	Epoch: [180][219/233]	Loss 0.0002 (0.0031)	
training:	Epoch: [180][220/233]	Loss 0.0006 (0.0031)	
training:	Epoch: [180][221/233]	Loss 0.0002 (0.0031)	
training:	Epoch: [180][222/233]	Loss 0.0003 (0.0031)	
training:	Epoch: [180][223/233]	Loss 0.0003 (0.0030)	
training:	Epoch: [180][224/233]	Loss 0.0002 (0.0030)	
training:	Epoch: [180][225/233]	Loss 0.0002 (0.0030)	
training:	Epoch: [180][226/233]	Loss 0.0001 (0.0030)	
training:	Epoch: [180][227/233]	Loss 0.0002 (0.0030)	
training:	Epoch: [180][228/233]	Loss 0.0002 (0.0030)	
training:	Epoch: [180][229/233]	Loss 0.0002 (0.0030)	
training:	Epoch: [180][230/233]	Loss 0.0002 (0.0030)	
training:	Epoch: [180][231/233]	Loss 0.0014 (0.0030)	
training:	Epoch: [180][232/233]	Loss 0.0002 (0.0029)	
training:	Epoch: [180][233/233]	Loss 0.0002 (0.0029)	
Training:	 Loss: 0.0029

Training:	 ACC: 1.0000 1.0000 1.0000 1.0000
Validation:	 ACC: 0.7943 0.7940 0.7875 0.8011
Validation:	 Best_BACC: 0.8162 0.8154 0.7998 0.8326
Validation:	 Loss: 1.3549
Pretraining:	Epoch 181/200
----------
training:	Epoch: [181][1/233]	Loss 0.0002 (0.0002)	
training:	Epoch: [181][2/233]	Loss 0.0005 (0.0004)	
training:	Epoch: [181][3/233]	Loss 0.0002 (0.0003)	
training:	Epoch: [181][4/233]	Loss 0.0008 (0.0004)	
training:	Epoch: [181][5/233]	Loss 0.0002 (0.0004)	
training:	Epoch: [181][6/233]	Loss 0.0012 (0.0005)	
training:	Epoch: [181][7/233]	Loss 0.0004 (0.0005)	
training:	Epoch: [181][8/233]	Loss 0.0002 (0.0005)	
training:	Epoch: [181][9/233]	Loss 0.0004 (0.0005)	
training:	Epoch: [181][10/233]	Loss 0.0003 (0.0004)	
training:	Epoch: [181][11/233]	Loss 0.0002 (0.0004)	
training:	Epoch: [181][12/233]	Loss 0.0002 (0.0004)	
training:	Epoch: [181][13/233]	Loss 0.0002 (0.0004)	
training:	Epoch: [181][14/233]	Loss 0.0002 (0.0004)	
training:	Epoch: [181][15/233]	Loss 0.0002 (0.0004)	
training:	Epoch: [181][16/233]	Loss 0.0009 (0.0004)	
training:	Epoch: [181][17/233]	Loss 0.0002 (0.0004)	
training:	Epoch: [181][18/233]	Loss 0.0002 (0.0004)	
training:	Epoch: [181][19/233]	Loss 0.0003 (0.0004)	
training:	Epoch: [181][20/233]	Loss 0.0001 (0.0003)	
training:	Epoch: [181][21/233]	Loss 0.0003 (0.0003)	
training:	Epoch: [181][22/233]	Loss 0.0005 (0.0004)	
training:	Epoch: [181][23/233]	Loss 0.0002 (0.0003)	
training:	Epoch: [181][24/233]	Loss 0.0004 (0.0003)	
training:	Epoch: [181][25/233]	Loss 0.0002 (0.0003)	
training:	Epoch: [181][26/233]	Loss 0.0002 (0.0003)	
training:	Epoch: [181][27/233]	Loss 0.0002 (0.0003)	
training:	Epoch: [181][28/233]	Loss 0.0002 (0.0003)	
training:	Epoch: [181][29/233]	Loss 0.0002 (0.0003)	
training:	Epoch: [181][30/233]	Loss 0.0002 (0.0003)	
training:	Epoch: [181][31/233]	Loss 0.0004 (0.0003)	
training:	Epoch: [181][32/233]	Loss 0.0002 (0.0003)	
training:	Epoch: [181][33/233]	Loss 0.0003 (0.0003)	
training:	Epoch: [181][34/233]	Loss 0.0002 (0.0003)	
training:	Epoch: [181][35/233]	Loss 0.0007 (0.0003)	
training:	Epoch: [181][36/233]	Loss 0.0007 (0.0003)	
training:	Epoch: [181][37/233]	Loss 0.0184 (0.0008)	
training:	Epoch: [181][38/233]	Loss 0.0004 (0.0008)	
training:	Epoch: [181][39/233]	Loss 0.0002 (0.0008)	
training:	Epoch: [181][40/233]	Loss 0.0001 (0.0008)	
training:	Epoch: [181][41/233]	Loss 0.0012 (0.0008)	
training:	Epoch: [181][42/233]	Loss 0.0001 (0.0008)	
training:	Epoch: [181][43/233]	Loss 0.0002 (0.0008)	
training:	Epoch: [181][44/233]	Loss 0.0749 (0.0024)	
training:	Epoch: [181][45/233]	Loss 0.0005 (0.0024)	
training:	Epoch: [181][46/233]	Loss 0.0002 (0.0023)	
training:	Epoch: [181][47/233]	Loss 0.0002 (0.0023)	
training:	Epoch: [181][48/233]	Loss 0.0003 (0.0023)	
training:	Epoch: [181][49/233]	Loss 0.0002 (0.0022)	
training:	Epoch: [181][50/233]	Loss 0.0003 (0.0022)	
training:	Epoch: [181][51/233]	Loss 0.0003 (0.0021)	
training:	Epoch: [181][52/233]	Loss 0.0002 (0.0021)	
training:	Epoch: [181][53/233]	Loss 0.0001 (0.0021)	
training:	Epoch: [181][54/233]	Loss 0.0917 (0.0037)	
training:	Epoch: [181][55/233]	Loss 0.0044 (0.0037)	
training:	Epoch: [181][56/233]	Loss 0.0004 (0.0037)	
training:	Epoch: [181][57/233]	Loss 0.0002 (0.0036)	
training:	Epoch: [181][58/233]	Loss 0.0097 (0.0037)	
training:	Epoch: [181][59/233]	Loss 0.0002 (0.0037)	
training:	Epoch: [181][60/233]	Loss 0.0006 (0.0036)	
training:	Epoch: [181][61/233]	Loss 0.0002 (0.0036)	
training:	Epoch: [181][62/233]	Loss 0.0004 (0.0035)	
training:	Epoch: [181][63/233]	Loss 0.0003 (0.0035)	
training:	Epoch: [181][64/233]	Loss 0.0004 (0.0034)	
training:	Epoch: [181][65/233]	Loss 0.0002 (0.0034)	
training:	Epoch: [181][66/233]	Loss 0.0002 (0.0033)	
training:	Epoch: [181][67/233]	Loss 0.0265 (0.0037)	
training:	Epoch: [181][68/233]	Loss 0.0190 (0.0039)	
training:	Epoch: [181][69/233]	Loss 0.0449 (0.0045)	
training:	Epoch: [181][70/233]	Loss 0.0007 (0.0044)	
training:	Epoch: [181][71/233]	Loss 0.1080 (0.0059)	
training:	Epoch: [181][72/233]	Loss 0.0002 (0.0058)	
training:	Epoch: [181][73/233]	Loss 0.0002 (0.0057)	
training:	Epoch: [181][74/233]	Loss 0.0002 (0.0056)	
training:	Epoch: [181][75/233]	Loss 0.0002 (0.0056)	
training:	Epoch: [181][76/233]	Loss 0.0002 (0.0055)	
training:	Epoch: [181][77/233]	Loss 0.0003 (0.0054)	
training:	Epoch: [181][78/233]	Loss 0.0020 (0.0054)	
training:	Epoch: [181][79/233]	Loss 0.0058 (0.0054)	
training:	Epoch: [181][80/233]	Loss 0.0002 (0.0053)	
training:	Epoch: [181][81/233]	Loss 0.0034 (0.0053)	
training:	Epoch: [181][82/233]	Loss 0.0002 (0.0052)	
training:	Epoch: [181][83/233]	Loss 0.0003 (0.0052)	
training:	Epoch: [181][84/233]	Loss 0.0002 (0.0051)	
training:	Epoch: [181][85/233]	Loss 0.0007 (0.0051)	
training:	Epoch: [181][86/233]	Loss 0.0003 (0.0050)	
training:	Epoch: [181][87/233]	Loss 0.0002 (0.0050)	
training:	Epoch: [181][88/233]	Loss 0.0004 (0.0049)	
training:	Epoch: [181][89/233]	Loss 0.0003 (0.0049)	
training:	Epoch: [181][90/233]	Loss 0.0002 (0.0048)	
training:	Epoch: [181][91/233]	Loss 0.0004 (0.0048)	
training:	Epoch: [181][92/233]	Loss 0.0008 (0.0047)	
training:	Epoch: [181][93/233]	Loss 0.0006 (0.0047)	
training:	Epoch: [181][94/233]	Loss 0.0006 (0.0046)	
training:	Epoch: [181][95/233]	Loss 0.0002 (0.0046)	
training:	Epoch: [181][96/233]	Loss 0.0009 (0.0045)	
training:	Epoch: [181][97/233]	Loss 0.0003 (0.0045)	
training:	Epoch: [181][98/233]	Loss 0.0077 (0.0045)	
training:	Epoch: [181][99/233]	Loss 0.0002 (0.0045)	
training:	Epoch: [181][100/233]	Loss 0.0115 (0.0046)	
training:	Epoch: [181][101/233]	Loss 0.0002 (0.0045)	
training:	Epoch: [181][102/233]	Loss 0.0003 (0.0045)	
training:	Epoch: [181][103/233]	Loss 0.0003 (0.0044)	
training:	Epoch: [181][104/233]	Loss 0.0002 (0.0044)	
training:	Epoch: [181][105/233]	Loss 0.0010 (0.0044)	
training:	Epoch: [181][106/233]	Loss 0.0010 (0.0043)	
training:	Epoch: [181][107/233]	Loss 0.0037 (0.0043)	
training:	Epoch: [181][108/233]	Loss 0.0003 (0.0043)	
training:	Epoch: [181][109/233]	Loss 0.0015 (0.0043)	
training:	Epoch: [181][110/233]	Loss 0.0002 (0.0042)	
training:	Epoch: [181][111/233]	Loss 0.0004 (0.0042)	
training:	Epoch: [181][112/233]	Loss 0.0005 (0.0042)	
training:	Epoch: [181][113/233]	Loss 0.0003 (0.0041)	
training:	Epoch: [181][114/233]	Loss 0.0004 (0.0041)	
training:	Epoch: [181][115/233]	Loss 0.0003 (0.0041)	
training:	Epoch: [181][116/233]	Loss 0.0042 (0.0041)	
training:	Epoch: [181][117/233]	Loss 0.0006 (0.0040)	
training:	Epoch: [181][118/233]	Loss 0.0003 (0.0040)	
training:	Epoch: [181][119/233]	Loss 0.0066 (0.0040)	
training:	Epoch: [181][120/233]	Loss 0.0033 (0.0040)	
training:	Epoch: [181][121/233]	Loss 0.0002 (0.0040)	
training:	Epoch: [181][122/233]	Loss 0.0004 (0.0040)	
training:	Epoch: [181][123/233]	Loss 0.0005 (0.0039)	
training:	Epoch: [181][124/233]	Loss 0.0002 (0.0039)	
training:	Epoch: [181][125/233]	Loss 0.0024 (0.0039)	
training:	Epoch: [181][126/233]	Loss 0.0004 (0.0039)	
training:	Epoch: [181][127/233]	Loss 0.0002 (0.0038)	
training:	Epoch: [181][128/233]	Loss 0.0002 (0.0038)	
training:	Epoch: [181][129/233]	Loss 0.0009 (0.0038)	
training:	Epoch: [181][130/233]	Loss 0.0002 (0.0037)	
training:	Epoch: [181][131/233]	Loss 0.0049 (0.0038)	
training:	Epoch: [181][132/233]	Loss 0.0003 (0.0037)	
training:	Epoch: [181][133/233]	Loss 0.0001 (0.0037)	
training:	Epoch: [181][134/233]	Loss 0.0003 (0.0037)	
training:	Epoch: [181][135/233]	Loss 0.0003 (0.0037)	
training:	Epoch: [181][136/233]	Loss 0.0003 (0.0036)	
training:	Epoch: [181][137/233]	Loss 0.0002 (0.0036)	
training:	Epoch: [181][138/233]	Loss 0.0007 (0.0036)	
training:	Epoch: [181][139/233]	Loss 0.0001 (0.0036)	
training:	Epoch: [181][140/233]	Loss 0.0003 (0.0035)	
training:	Epoch: [181][141/233]	Loss 0.0004 (0.0035)	
training:	Epoch: [181][142/233]	Loss 0.0009 (0.0035)	
training:	Epoch: [181][143/233]	Loss 0.0002 (0.0035)	
training:	Epoch: [181][144/233]	Loss 0.0002 (0.0034)	
training:	Epoch: [181][145/233]	Loss 0.0004 (0.0034)	
training:	Epoch: [181][146/233]	Loss 0.0003 (0.0034)	
training:	Epoch: [181][147/233]	Loss 0.0002 (0.0034)	
training:	Epoch: [181][148/233]	Loss 0.0002 (0.0034)	
training:	Epoch: [181][149/233]	Loss 0.0002 (0.0033)	
training:	Epoch: [181][150/233]	Loss 0.0002 (0.0033)	
training:	Epoch: [181][151/233]	Loss 0.0002 (0.0033)	
training:	Epoch: [181][152/233]	Loss 0.0002 (0.0033)	
training:	Epoch: [181][153/233]	Loss 0.0002 (0.0033)	
training:	Epoch: [181][154/233]	Loss 0.0003 (0.0032)	
training:	Epoch: [181][155/233]	Loss 0.0002 (0.0032)	
training:	Epoch: [181][156/233]	Loss 0.0003 (0.0032)	
training:	Epoch: [181][157/233]	Loss 0.0002 (0.0032)	
training:	Epoch: [181][158/233]	Loss 0.0003 (0.0032)	
training:	Epoch: [181][159/233]	Loss 0.0002 (0.0031)	
training:	Epoch: [181][160/233]	Loss 0.0003 (0.0031)	
training:	Epoch: [181][161/233]	Loss 0.0002 (0.0031)	
training:	Epoch: [181][162/233]	Loss 0.0002 (0.0031)	
training:	Epoch: [181][163/233]	Loss 0.0002 (0.0031)	
training:	Epoch: [181][164/233]	Loss 0.0002 (0.0031)	
training:	Epoch: [181][165/233]	Loss 0.0016 (0.0030)	
training:	Epoch: [181][166/233]	Loss 0.0004 (0.0030)	
training:	Epoch: [181][167/233]	Loss 0.0014 (0.0030)	
training:	Epoch: [181][168/233]	Loss 0.0004 (0.0030)	
training:	Epoch: [181][169/233]	Loss 0.0002 (0.0030)	
training:	Epoch: [181][170/233]	Loss 0.0003 (0.0030)	
training:	Epoch: [181][171/233]	Loss 0.0069 (0.0030)	
training:	Epoch: [181][172/233]	Loss 0.0005 (0.0030)	
training:	Epoch: [181][173/233]	Loss 0.0002 (0.0030)	
training:	Epoch: [181][174/233]	Loss 0.0009 (0.0030)	
training:	Epoch: [181][175/233]	Loss 0.0001 (0.0029)	
training:	Epoch: [181][176/233]	Loss 0.0002 (0.0029)	
training:	Epoch: [181][177/233]	Loss 0.0002 (0.0029)	
training:	Epoch: [181][178/233]	Loss 0.0003 (0.0029)	
training:	Epoch: [181][179/233]	Loss 0.0003 (0.0029)	
training:	Epoch: [181][180/233]	Loss 0.0002 (0.0029)	
training:	Epoch: [181][181/233]	Loss 0.0002 (0.0028)	
training:	Epoch: [181][182/233]	Loss 0.0038 (0.0029)	
training:	Epoch: [181][183/233]	Loss 0.0037 (0.0029)	
training:	Epoch: [181][184/233]	Loss 0.0002 (0.0028)	
training:	Epoch: [181][185/233]	Loss 0.0003 (0.0028)	
training:	Epoch: [181][186/233]	Loss 0.0002 (0.0028)	
training:	Epoch: [181][187/233]	Loss 0.0001 (0.0028)	
training:	Epoch: [181][188/233]	Loss 0.0003 (0.0028)	
training:	Epoch: [181][189/233]	Loss 0.0002 (0.0028)	
training:	Epoch: [181][190/233]	Loss 0.0001 (0.0028)	
training:	Epoch: [181][191/233]	Loss 0.0002 (0.0027)	
training:	Epoch: [181][192/233]	Loss 0.0002 (0.0027)	
training:	Epoch: [181][193/233]	Loss 0.0002 (0.0027)	
training:	Epoch: [181][194/233]	Loss 0.0005 (0.0027)	
training:	Epoch: [181][195/233]	Loss 0.0002 (0.0027)	
training:	Epoch: [181][196/233]	Loss 0.0002 (0.0027)	
training:	Epoch: [181][197/233]	Loss 0.0002 (0.0027)	
training:	Epoch: [181][198/233]	Loss 0.0002 (0.0027)	
training:	Epoch: [181][199/233]	Loss 0.0002 (0.0026)	
training:	Epoch: [181][200/233]	Loss 0.0003 (0.0026)	
training:	Epoch: [181][201/233]	Loss 0.0001 (0.0026)	
training:	Epoch: [181][202/233]	Loss 0.0001 (0.0026)	
training:	Epoch: [181][203/233]	Loss 0.0020 (0.0026)	
training:	Epoch: [181][204/233]	Loss 0.0002 (0.0026)	
training:	Epoch: [181][205/233]	Loss 0.0004 (0.0026)	
training:	Epoch: [181][206/233]	Loss 0.0019 (0.0026)	
training:	Epoch: [181][207/233]	Loss 0.0002 (0.0026)	
training:	Epoch: [181][208/233]	Loss 0.0003 (0.0026)	
training:	Epoch: [181][209/233]	Loss 0.0007 (0.0025)	
training:	Epoch: [181][210/233]	Loss 0.0002 (0.0025)	
training:	Epoch: [181][211/233]	Loss 0.0004 (0.0025)	
training:	Epoch: [181][212/233]	Loss 0.0002 (0.0025)	
training:	Epoch: [181][213/233]	Loss 0.0002 (0.0025)	
training:	Epoch: [181][214/233]	Loss 0.0001 (0.0025)	
training:	Epoch: [181][215/233]	Loss 0.0002 (0.0025)	
training:	Epoch: [181][216/233]	Loss 0.0004 (0.0025)	
training:	Epoch: [181][217/233]	Loss 0.0002 (0.0025)	
training:	Epoch: [181][218/233]	Loss 0.0002 (0.0025)	
training:	Epoch: [181][219/233]	Loss 0.0273 (0.0026)	
training:	Epoch: [181][220/233]	Loss 0.0003 (0.0026)	
training:	Epoch: [181][221/233]	Loss 0.0032 (0.0026)	
training:	Epoch: [181][222/233]	Loss 0.0002 (0.0025)	
training:	Epoch: [181][223/233]	Loss 0.0004 (0.0025)	
training:	Epoch: [181][224/233]	Loss 0.0002 (0.0025)	
training:	Epoch: [181][225/233]	Loss 0.0001 (0.0025)	
training:	Epoch: [181][226/233]	Loss 0.0008 (0.0025)	
training:	Epoch: [181][227/233]	Loss 0.0002 (0.0025)	
training:	Epoch: [181][228/233]	Loss 0.0002 (0.0025)	
training:	Epoch: [181][229/233]	Loss 0.0311 (0.0026)	
training:	Epoch: [181][230/233]	Loss 0.0002 (0.0026)	
training:	Epoch: [181][231/233]	Loss 0.0002 (0.0026)	
training:	Epoch: [181][232/233]	Loss 0.0001 (0.0026)	
training:	Epoch: [181][233/233]	Loss 0.0002 (0.0026)	
Training:	 Loss: 0.0026

Training:	 ACC: 1.0000 1.0000 1.0000 1.0000
Validation:	 ACC: 0.7927 0.7919 0.7743 0.8112
Validation:	 Best_BACC: 0.8162 0.8154 0.7998 0.8326
Validation:	 Loss: 1.3713
Pretraining:	Epoch 182/200
----------
training:	Epoch: [182][1/233]	Loss 0.0014 (0.0014)	
training:	Epoch: [182][2/233]	Loss 0.0003 (0.0009)	
training:	Epoch: [182][3/233]	Loss 0.0004 (0.0007)	
training:	Epoch: [182][4/233]	Loss 0.0002 (0.0006)	
training:	Epoch: [182][5/233]	Loss 0.0002 (0.0005)	
training:	Epoch: [182][6/233]	Loss 0.0003 (0.0005)	
training:	Epoch: [182][7/233]	Loss 0.0002 (0.0004)	
training:	Epoch: [182][8/233]	Loss 0.0001 (0.0004)	
training:	Epoch: [182][9/233]	Loss 0.0002 (0.0004)	
training:	Epoch: [182][10/233]	Loss 0.0002 (0.0003)	
training:	Epoch: [182][11/233]	Loss 0.0002 (0.0003)	
training:	Epoch: [182][12/233]	Loss 0.0004 (0.0003)	
training:	Epoch: [182][13/233]	Loss 0.0001 (0.0003)	
training:	Epoch: [182][14/233]	Loss 0.0002 (0.0003)	
training:	Epoch: [182][15/233]	Loss 0.0002 (0.0003)	
training:	Epoch: [182][16/233]	Loss 0.0294 (0.0021)	
training:	Epoch: [182][17/233]	Loss 0.0001 (0.0020)	
training:	Epoch: [182][18/233]	Loss 0.0002 (0.0019)	
training:	Epoch: [182][19/233]	Loss 0.0002 (0.0018)	
training:	Epoch: [182][20/233]	Loss 0.0001 (0.0017)	
training:	Epoch: [182][21/233]	Loss 0.0191 (0.0026)	
training:	Epoch: [182][22/233]	Loss 0.0056 (0.0027)	
training:	Epoch: [182][23/233]	Loss 0.0002 (0.0026)	
training:	Epoch: [182][24/233]	Loss 0.0005 (0.0025)	
training:	Epoch: [182][25/233]	Loss 0.0001 (0.0024)	
training:	Epoch: [182][26/233]	Loss 0.0002 (0.0023)	
training:	Epoch: [182][27/233]	Loss 0.0002 (0.0022)	
training:	Epoch: [182][28/233]	Loss 0.0003 (0.0022)	
training:	Epoch: [182][29/233]	Loss 0.0024 (0.0022)	
training:	Epoch: [182][30/233]	Loss 0.0001 (0.0021)	
training:	Epoch: [182][31/233]	Loss 0.0003 (0.0021)	
training:	Epoch: [182][32/233]	Loss 0.0002 (0.0020)	
training:	Epoch: [182][33/233]	Loss 0.0118 (0.0023)	
training:	Epoch: [182][34/233]	Loss 0.0290 (0.0031)	
training:	Epoch: [182][35/233]	Loss 0.0056 (0.0031)	
training:	Epoch: [182][36/233]	Loss 0.0003 (0.0031)	
training:	Epoch: [182][37/233]	Loss 0.0013 (0.0030)	
training:	Epoch: [182][38/233]	Loss 0.0004 (0.0029)	
training:	Epoch: [182][39/233]	Loss 0.0003 (0.0029)	
training:	Epoch: [182][40/233]	Loss 0.0003 (0.0028)	
training:	Epoch: [182][41/233]	Loss 0.0002 (0.0028)	
training:	Epoch: [182][42/233]	Loss 0.0011 (0.0027)	
training:	Epoch: [182][43/233]	Loss 0.0002 (0.0027)	
training:	Epoch: [182][44/233]	Loss 0.0168 (0.0030)	
training:	Epoch: [182][45/233]	Loss 0.0002 (0.0029)	
training:	Epoch: [182][46/233]	Loss 0.0002 (0.0029)	
training:	Epoch: [182][47/233]	Loss 0.0001 (0.0028)	
training:	Epoch: [182][48/233]	Loss 0.0003 (0.0027)	
training:	Epoch: [182][49/233]	Loss 0.0002 (0.0027)	
training:	Epoch: [182][50/233]	Loss 0.0002 (0.0026)	
training:	Epoch: [182][51/233]	Loss 0.0007 (0.0026)	
training:	Epoch: [182][52/233]	Loss 0.0002 (0.0026)	
training:	Epoch: [182][53/233]	Loss 0.0002 (0.0025)	
training:	Epoch: [182][54/233]	Loss 0.0002 (0.0025)	
training:	Epoch: [182][55/233]	Loss 0.0002 (0.0024)	
training:	Epoch: [182][56/233]	Loss 0.0001 (0.0024)	
training:	Epoch: [182][57/233]	Loss 0.0002 (0.0023)	
training:	Epoch: [182][58/233]	Loss 0.0001 (0.0023)	
training:	Epoch: [182][59/233]	Loss 0.0003 (0.0023)	
training:	Epoch: [182][60/233]	Loss 0.0002 (0.0022)	
training:	Epoch: [182][61/233]	Loss 0.0004 (0.0022)	
training:	Epoch: [182][62/233]	Loss 0.0002 (0.0022)	
training:	Epoch: [182][63/233]	Loss 0.0003 (0.0021)	
training:	Epoch: [182][64/233]	Loss 0.0013 (0.0021)	
training:	Epoch: [182][65/233]	Loss 0.0005 (0.0021)	
training:	Epoch: [182][66/233]	Loss 0.0003 (0.0021)	
training:	Epoch: [182][67/233]	Loss 0.0003 (0.0021)	
training:	Epoch: [182][68/233]	Loss 0.0005 (0.0020)	
training:	Epoch: [182][69/233]	Loss 0.0005 (0.0020)	
training:	Epoch: [182][70/233]	Loss 0.0003 (0.0020)	
training:	Epoch: [182][71/233]	Loss 0.0001 (0.0020)	
training:	Epoch: [182][72/233]	Loss 0.0002 (0.0019)	
training:	Epoch: [182][73/233]	Loss 0.0002 (0.0019)	
training:	Epoch: [182][74/233]	Loss 0.0002 (0.0019)	
training:	Epoch: [182][75/233]	Loss 0.0003 (0.0019)	
training:	Epoch: [182][76/233]	Loss 0.0002 (0.0018)	
training:	Epoch: [182][77/233]	Loss 0.0001 (0.0018)	
training:	Epoch: [182][78/233]	Loss 0.0004 (0.0018)	
training:	Epoch: [182][79/233]	Loss 0.0003 (0.0018)	
training:	Epoch: [182][80/233]	Loss 0.0005 (0.0018)	
training:	Epoch: [182][81/233]	Loss 0.0018 (0.0018)	
training:	Epoch: [182][82/233]	Loss 0.0001 (0.0017)	
training:	Epoch: [182][83/233]	Loss 0.0002 (0.0017)	
training:	Epoch: [182][84/233]	Loss 0.0005 (0.0017)	
training:	Epoch: [182][85/233]	Loss 0.0002 (0.0017)	
training:	Epoch: [182][86/233]	Loss 0.0011 (0.0017)	
training:	Epoch: [182][87/233]	Loss 0.0002 (0.0017)	
training:	Epoch: [182][88/233]	Loss 0.0002 (0.0017)	
training:	Epoch: [182][89/233]	Loss 0.0002 (0.0016)	
training:	Epoch: [182][90/233]	Loss 0.0002 (0.0016)	
training:	Epoch: [182][91/233]	Loss 0.0002 (0.0016)	
training:	Epoch: [182][92/233]	Loss 0.0004 (0.0016)	
training:	Epoch: [182][93/233]	Loss 0.0096 (0.0017)	
training:	Epoch: [182][94/233]	Loss 0.0060 (0.0017)	
training:	Epoch: [182][95/233]	Loss 0.0002 (0.0017)	
training:	Epoch: [182][96/233]	Loss 0.0001 (0.0017)	
training:	Epoch: [182][97/233]	Loss 0.0001 (0.0017)	
training:	Epoch: [182][98/233]	Loss 0.0004 (0.0017)	
training:	Epoch: [182][99/233]	Loss 0.0013 (0.0017)	
training:	Epoch: [182][100/233]	Loss 0.0002 (0.0016)	
training:	Epoch: [182][101/233]	Loss 0.0002 (0.0016)	
training:	Epoch: [182][102/233]	Loss 0.0970 (0.0026)	
training:	Epoch: [182][103/233]	Loss 0.0002 (0.0025)	
training:	Epoch: [182][104/233]	Loss 0.0001 (0.0025)	
training:	Epoch: [182][105/233]	Loss 0.0002 (0.0025)	
training:	Epoch: [182][106/233]	Loss 0.0002 (0.0025)	
training:	Epoch: [182][107/233]	Loss 0.0002 (0.0025)	
training:	Epoch: [182][108/233]	Loss 0.0001 (0.0024)	
training:	Epoch: [182][109/233]	Loss 0.0002 (0.0024)	
training:	Epoch: [182][110/233]	Loss 0.0002 (0.0024)	
training:	Epoch: [182][111/233]	Loss 0.0005 (0.0024)	
training:	Epoch: [182][112/233]	Loss 0.0002 (0.0024)	
training:	Epoch: [182][113/233]	Loss 0.0002 (0.0023)	
training:	Epoch: [182][114/233]	Loss 0.0002 (0.0023)	
training:	Epoch: [182][115/233]	Loss 0.0002 (0.0023)	
training:	Epoch: [182][116/233]	Loss 0.0002 (0.0023)	
training:	Epoch: [182][117/233]	Loss 0.0005 (0.0023)	
training:	Epoch: [182][118/233]	Loss 0.0001 (0.0022)	
training:	Epoch: [182][119/233]	Loss 0.0003 (0.0022)	
training:	Epoch: [182][120/233]	Loss 0.0001 (0.0022)	
training:	Epoch: [182][121/233]	Loss 0.0002 (0.0022)	
training:	Epoch: [182][122/233]	Loss 0.0001 (0.0022)	
training:	Epoch: [182][123/233]	Loss 0.0067 (0.0022)	
training:	Epoch: [182][124/233]	Loss 0.0001 (0.0022)	
training:	Epoch: [182][125/233]	Loss 0.0001 (0.0022)	
training:	Epoch: [182][126/233]	Loss 0.0003 (0.0022)	
training:	Epoch: [182][127/233]	Loss 0.0002 (0.0022)	
training:	Epoch: [182][128/233]	Loss 0.0002 (0.0021)	
training:	Epoch: [182][129/233]	Loss 0.0002 (0.0021)	
training:	Epoch: [182][130/233]	Loss 0.0003 (0.0021)	
training:	Epoch: [182][131/233]	Loss 0.0002 (0.0021)	
training:	Epoch: [182][132/233]	Loss 0.0002 (0.0021)	
training:	Epoch: [182][133/233]	Loss 0.0005 (0.0021)	
training:	Epoch: [182][134/233]	Loss 0.0032 (0.0021)	
training:	Epoch: [182][135/233]	Loss 0.0001 (0.0021)	
training:	Epoch: [182][136/233]	Loss 0.0016 (0.0021)	
training:	Epoch: [182][137/233]	Loss 0.0002 (0.0020)	
training:	Epoch: [182][138/233]	Loss 0.0002 (0.0020)	
training:	Epoch: [182][139/233]	Loss 0.0001 (0.0020)	
training:	Epoch: [182][140/233]	Loss 0.0001 (0.0020)	
training:	Epoch: [182][141/233]	Loss 0.0005 (0.0020)	
training:	Epoch: [182][142/233]	Loss 0.0002 (0.0020)	
training:	Epoch: [182][143/233]	Loss 0.0002 (0.0020)	
training:	Epoch: [182][144/233]	Loss 0.0003 (0.0020)	
training:	Epoch: [182][145/233]	Loss 0.0002 (0.0019)	
training:	Epoch: [182][146/233]	Loss 0.0002 (0.0019)	
training:	Epoch: [182][147/233]	Loss 0.0002 (0.0019)	
training:	Epoch: [182][148/233]	Loss 0.0002 (0.0019)	
training:	Epoch: [182][149/233]	Loss 0.0621 (0.0023)	
training:	Epoch: [182][150/233]	Loss 0.0002 (0.0023)	
training:	Epoch: [182][151/233]	Loss 0.0003 (0.0023)	
training:	Epoch: [182][152/233]	Loss 0.0001 (0.0023)	
training:	Epoch: [182][153/233]	Loss 0.0002 (0.0023)	
training:	Epoch: [182][154/233]	Loss 0.0002 (0.0022)	
training:	Epoch: [182][155/233]	Loss 0.0002 (0.0022)	
training:	Epoch: [182][156/233]	Loss 0.0002 (0.0022)	
training:	Epoch: [182][157/233]	Loss 0.0026 (0.0022)	
training:	Epoch: [182][158/233]	Loss 0.0003 (0.0022)	
training:	Epoch: [182][159/233]	Loss 0.0002 (0.0022)	
training:	Epoch: [182][160/233]	Loss 0.0002 (0.0022)	
training:	Epoch: [182][161/233]	Loss 0.0002 (0.0022)	
training:	Epoch: [182][162/233]	Loss 0.0003 (0.0022)	
training:	Epoch: [182][163/233]	Loss 0.0053 (0.0022)	
training:	Epoch: [182][164/233]	Loss 0.0002 (0.0022)	
training:	Epoch: [182][165/233]	Loss 0.0013 (0.0022)	
training:	Epoch: [182][166/233]	Loss 0.0002 (0.0021)	
training:	Epoch: [182][167/233]	Loss 0.0002 (0.0021)	
training:	Epoch: [182][168/233]	Loss 0.0002 (0.0021)	
training:	Epoch: [182][169/233]	Loss 0.0002 (0.0021)	
training:	Epoch: [182][170/233]	Loss 0.0004 (0.0021)	
training:	Epoch: [182][171/233]	Loss 0.0002 (0.0021)	
training:	Epoch: [182][172/233]	Loss 0.0002 (0.0021)	
training:	Epoch: [182][173/233]	Loss 0.0005 (0.0021)	
training:	Epoch: [182][174/233]	Loss 0.0115 (0.0021)	
training:	Epoch: [182][175/233]	Loss 0.0003 (0.0021)	
training:	Epoch: [182][176/233]	Loss 0.0042 (0.0021)	
training:	Epoch: [182][177/233]	Loss 0.0005 (0.0021)	
training:	Epoch: [182][178/233]	Loss 0.0002 (0.0021)	
training:	Epoch: [182][179/233]	Loss 0.0001 (0.0021)	
training:	Epoch: [182][180/233]	Loss 0.0003 (0.0021)	
training:	Epoch: [182][181/233]	Loss 0.0001 (0.0021)	
training:	Epoch: [182][182/233]	Loss 0.0005 (0.0021)	
training:	Epoch: [182][183/233]	Loss 0.0002 (0.0021)	
training:	Epoch: [182][184/233]	Loss 0.0001 (0.0020)	
training:	Epoch: [182][185/233]	Loss 0.0047 (0.0021)	
training:	Epoch: [182][186/233]	Loss 0.0002 (0.0020)	
training:	Epoch: [182][187/233]	Loss 0.0001 (0.0020)	
training:	Epoch: [182][188/233]	Loss 0.0002 (0.0020)	
training:	Epoch: [182][189/233]	Loss 0.0003 (0.0020)	
training:	Epoch: [182][190/233]	Loss 0.0002 (0.0020)	
training:	Epoch: [182][191/233]	Loss 0.0005 (0.0020)	
training:	Epoch: [182][192/233]	Loss 0.0001 (0.0020)	
training:	Epoch: [182][193/233]	Loss 0.0001 (0.0020)	
training:	Epoch: [182][194/233]	Loss 0.0002 (0.0020)	
training:	Epoch: [182][195/233]	Loss 0.0001 (0.0020)	
training:	Epoch: [182][196/233]	Loss 0.0005 (0.0020)	
training:	Epoch: [182][197/233]	Loss 0.0004 (0.0019)	
training:	Epoch: [182][198/233]	Loss 0.0001 (0.0019)	
training:	Epoch: [182][199/233]	Loss 0.0002 (0.0019)	
training:	Epoch: [182][200/233]	Loss 0.0002 (0.0019)	
training:	Epoch: [182][201/233]	Loss 0.0006 (0.0019)	
training:	Epoch: [182][202/233]	Loss 0.0001 (0.0019)	
training:	Epoch: [182][203/233]	Loss 0.0001 (0.0019)	
training:	Epoch: [182][204/233]	Loss 0.0004 (0.0019)	
training:	Epoch: [182][205/233]	Loss 0.0001 (0.0019)	
training:	Epoch: [182][206/233]	Loss 0.0002 (0.0019)	
training:	Epoch: [182][207/233]	Loss 0.0002 (0.0019)	
training:	Epoch: [182][208/233]	Loss 0.0002 (0.0019)	
training:	Epoch: [182][209/233]	Loss 0.0020 (0.0019)	
training:	Epoch: [182][210/233]	Loss 0.0032 (0.0019)	
training:	Epoch: [182][211/233]	Loss 0.0010 (0.0019)	
training:	Epoch: [182][212/233]	Loss 0.0002 (0.0019)	
training:	Epoch: [182][213/233]	Loss 0.0002 (0.0018)	
training:	Epoch: [182][214/233]	Loss 0.0001 (0.0018)	
training:	Epoch: [182][215/233]	Loss 0.0002 (0.0018)	
training:	Epoch: [182][216/233]	Loss 0.0001 (0.0018)	
training:	Epoch: [182][217/233]	Loss 0.0002 (0.0018)	
training:	Epoch: [182][218/233]	Loss 0.0002 (0.0018)	
training:	Epoch: [182][219/233]	Loss 0.0002 (0.0018)	
training:	Epoch: [182][220/233]	Loss 0.0002 (0.0018)	
training:	Epoch: [182][221/233]	Loss 0.0002 (0.0018)	
training:	Epoch: [182][222/233]	Loss 0.0002 (0.0018)	
training:	Epoch: [182][223/233]	Loss 0.0001 (0.0018)	
training:	Epoch: [182][224/233]	Loss 0.0001 (0.0018)	
training:	Epoch: [182][225/233]	Loss 0.0036 (0.0018)	
training:	Epoch: [182][226/233]	Loss 0.0002 (0.0018)	
training:	Epoch: [182][227/233]	Loss 0.0002 (0.0018)	
training:	Epoch: [182][228/233]	Loss 0.0002 (0.0017)	
training:	Epoch: [182][229/233]	Loss 0.0002 (0.0017)	
training:	Epoch: [182][230/233]	Loss 0.0002 (0.0017)	
training:	Epoch: [182][231/233]	Loss 0.0002 (0.0017)	
training:	Epoch: [182][232/233]	Loss 0.0054 (0.0017)	
training:	Epoch: [182][233/233]	Loss 0.0002 (0.0017)	
Training:	 Loss: 0.0017

Training:	 ACC: 1.0000 1.0000 1.0000 1.0000
Validation:	 ACC: 0.7997 0.7988 0.7814 0.8180
Validation:	 Best_BACC: 0.8162 0.8154 0.7998 0.8326
Validation:	 Loss: 1.3501
Pretraining:	Epoch 183/200
----------
training:	Epoch: [183][1/233]	Loss 0.0002 (0.0002)	
training:	Epoch: [183][2/233]	Loss 0.0002 (0.0002)	
training:	Epoch: [183][3/233]	Loss 0.0003 (0.0002)	
training:	Epoch: [183][4/233]	Loss 0.0002 (0.0002)	
training:	Epoch: [183][5/233]	Loss 0.0002 (0.0002)	
training:	Epoch: [183][6/233]	Loss 0.0002 (0.0002)	
training:	Epoch: [183][7/233]	Loss 0.0001 (0.0002)	
training:	Epoch: [183][8/233]	Loss 0.0003 (0.0002)	
training:	Epoch: [183][9/233]	Loss 0.0001 (0.0002)	
training:	Epoch: [183][10/233]	Loss 0.0003 (0.0002)	
training:	Epoch: [183][11/233]	Loss 0.0002 (0.0002)	
training:	Epoch: [183][12/233]	Loss 0.0002 (0.0002)	
training:	Epoch: [183][13/233]	Loss 0.0002 (0.0002)	
training:	Epoch: [183][14/233]	Loss 0.0003 (0.0002)	
training:	Epoch: [183][15/233]	Loss 0.0001 (0.0002)	
training:	Epoch: [183][16/233]	Loss 0.0002 (0.0002)	
training:	Epoch: [183][17/233]	Loss 0.0001 (0.0002)	
training:	Epoch: [183][18/233]	Loss 0.0001 (0.0002)	
training:	Epoch: [183][19/233]	Loss 0.0002 (0.0002)	
training:	Epoch: [183][20/233]	Loss 0.0002 (0.0002)	
training:	Epoch: [183][21/233]	Loss 0.0002 (0.0002)	
training:	Epoch: [183][22/233]	Loss 0.0002 (0.0002)	
training:	Epoch: [183][23/233]	Loss 0.0001 (0.0002)	
training:	Epoch: [183][24/233]	Loss 0.0002 (0.0002)	
training:	Epoch: [183][25/233]	Loss 0.0002 (0.0002)	
training:	Epoch: [183][26/233]	Loss 0.0005 (0.0002)	
training:	Epoch: [183][27/233]	Loss 0.0003 (0.0002)	
training:	Epoch: [183][28/233]	Loss 0.0002 (0.0002)	
training:	Epoch: [183][29/233]	Loss 0.0002 (0.0002)	
training:	Epoch: [183][30/233]	Loss 0.0002 (0.0002)	
training:	Epoch: [183][31/233]	Loss 0.0002 (0.0002)	
training:	Epoch: [183][32/233]	Loss 0.0002 (0.0002)	
training:	Epoch: [183][33/233]	Loss 0.0056 (0.0004)	
training:	Epoch: [183][34/233]	Loss 0.0002 (0.0004)	
training:	Epoch: [183][35/233]	Loss 0.0001 (0.0004)	
training:	Epoch: [183][36/233]	Loss 0.0001 (0.0003)	
training:	Epoch: [183][37/233]	Loss 0.0002 (0.0003)	
training:	Epoch: [183][38/233]	Loss 0.0002 (0.0003)	
training:	Epoch: [183][39/233]	Loss 0.0001 (0.0003)	
training:	Epoch: [183][40/233]	Loss 0.0001 (0.0003)	
training:	Epoch: [183][41/233]	Loss 0.0005 (0.0003)	
training:	Epoch: [183][42/233]	Loss 0.0002 (0.0003)	
training:	Epoch: [183][43/233]	Loss 0.0002 (0.0003)	
training:	Epoch: [183][44/233]	Loss 0.0003 (0.0003)	
training:	Epoch: [183][45/233]	Loss 0.0002 (0.0003)	
training:	Epoch: [183][46/233]	Loss 0.0004 (0.0003)	
training:	Epoch: [183][47/233]	Loss 0.0002 (0.0003)	
training:	Epoch: [183][48/233]	Loss 0.0003 (0.0003)	
training:	Epoch: [183][49/233]	Loss 0.0002 (0.0003)	
training:	Epoch: [183][50/233]	Loss 0.0026 (0.0004)	
training:	Epoch: [183][51/233]	Loss 0.0002 (0.0004)	
training:	Epoch: [183][52/233]	Loss 0.0001 (0.0004)	
training:	Epoch: [183][53/233]	Loss 0.0004 (0.0004)	
training:	Epoch: [183][54/233]	Loss 0.0002 (0.0004)	
training:	Epoch: [183][55/233]	Loss 0.0001 (0.0003)	
training:	Epoch: [183][56/233]	Loss 0.0003 (0.0003)	
training:	Epoch: [183][57/233]	Loss 0.0002 (0.0003)	
training:	Epoch: [183][58/233]	Loss 0.0002 (0.0003)	
training:	Epoch: [183][59/233]	Loss 0.0003 (0.0003)	
training:	Epoch: [183][60/233]	Loss 0.0002 (0.0003)	
training:	Epoch: [183][61/233]	Loss 0.0002 (0.0003)	
training:	Epoch: [183][62/233]	Loss 0.0003 (0.0003)	
training:	Epoch: [183][63/233]	Loss 0.0002 (0.0003)	
training:	Epoch: [183][64/233]	Loss 0.0002 (0.0003)	
training:	Epoch: [183][65/233]	Loss 0.0324 (0.0008)	
training:	Epoch: [183][66/233]	Loss 0.0004 (0.0008)	
training:	Epoch: [183][67/233]	Loss 0.0003 (0.0008)	
training:	Epoch: [183][68/233]	Loss 0.0002 (0.0008)	
training:	Epoch: [183][69/233]	Loss 0.0001 (0.0008)	
training:	Epoch: [183][70/233]	Loss 0.0001 (0.0008)	
training:	Epoch: [183][71/233]	Loss 0.0001 (0.0008)	
training:	Epoch: [183][72/233]	Loss 0.0001 (0.0008)	
training:	Epoch: [183][73/233]	Loss 0.0002 (0.0008)	
training:	Epoch: [183][74/233]	Loss 0.0001 (0.0007)	
training:	Epoch: [183][75/233]	Loss 0.0002 (0.0007)	
training:	Epoch: [183][76/233]	Loss 0.0001 (0.0007)	
training:	Epoch: [183][77/233]	Loss 0.0001 (0.0007)	
training:	Epoch: [183][78/233]	Loss 0.0003 (0.0007)	
training:	Epoch: [183][79/233]	Loss 0.0002 (0.0007)	
training:	Epoch: [183][80/233]	Loss 0.0027 (0.0007)	
training:	Epoch: [183][81/233]	Loss 0.0002 (0.0007)	
training:	Epoch: [183][82/233]	Loss 0.0003 (0.0007)	
training:	Epoch: [183][83/233]	Loss 0.0126 (0.0009)	
training:	Epoch: [183][84/233]	Loss 0.0003 (0.0009)	
training:	Epoch: [183][85/233]	Loss 0.1329 (0.0024)	
training:	Epoch: [183][86/233]	Loss 0.0001 (0.0024)	
training:	Epoch: [183][87/233]	Loss 0.0002 (0.0024)	
training:	Epoch: [183][88/233]	Loss 0.0003 (0.0023)	
training:	Epoch: [183][89/233]	Loss 0.0003 (0.0023)	
training:	Epoch: [183][90/233]	Loss 0.0002 (0.0023)	
training:	Epoch: [183][91/233]	Loss 0.0002 (0.0023)	
training:	Epoch: [183][92/233]	Loss 0.0002 (0.0022)	
training:	Epoch: [183][93/233]	Loss 0.0002 (0.0022)	
training:	Epoch: [183][94/233]	Loss 0.0002 (0.0022)	
training:	Epoch: [183][95/233]	Loss 0.0001 (0.0022)	
training:	Epoch: [183][96/233]	Loss 0.0002 (0.0022)	
training:	Epoch: [183][97/233]	Loss 0.0008 (0.0021)	
training:	Epoch: [183][98/233]	Loss 0.0001 (0.0021)	
training:	Epoch: [183][99/233]	Loss 0.0002 (0.0021)	
training:	Epoch: [183][100/233]	Loss 0.0001 (0.0021)	
training:	Epoch: [183][101/233]	Loss 0.0001 (0.0021)	
training:	Epoch: [183][102/233]	Loss 0.0056 (0.0021)	
training:	Epoch: [183][103/233]	Loss 0.0003 (0.0021)	
training:	Epoch: [183][104/233]	Loss 0.0004 (0.0021)	
training:	Epoch: [183][105/233]	Loss 0.0002 (0.0020)	
training:	Epoch: [183][106/233]	Loss 0.0003 (0.0020)	
training:	Epoch: [183][107/233]	Loss 0.0001 (0.0020)	
training:	Epoch: [183][108/233]	Loss 0.0002 (0.0020)	
training:	Epoch: [183][109/233]	Loss 0.0002 (0.0020)	
training:	Epoch: [183][110/233]	Loss 0.0002 (0.0020)	
training:	Epoch: [183][111/233]	Loss 0.0001 (0.0019)	
training:	Epoch: [183][112/233]	Loss 0.0002 (0.0019)	
training:	Epoch: [183][113/233]	Loss 0.0037 (0.0019)	
training:	Epoch: [183][114/233]	Loss 0.0001 (0.0019)	
training:	Epoch: [183][115/233]	Loss 0.0001 (0.0019)	
training:	Epoch: [183][116/233]	Loss 0.0002 (0.0019)	
training:	Epoch: [183][117/233]	Loss 0.0001 (0.0019)	
training:	Epoch: [183][118/233]	Loss 0.0001 (0.0019)	
training:	Epoch: [183][119/233]	Loss 0.0002 (0.0019)	
training:	Epoch: [183][120/233]	Loss 0.0002 (0.0018)	
training:	Epoch: [183][121/233]	Loss 0.0001 (0.0018)	
training:	Epoch: [183][122/233]	Loss 0.0003 (0.0018)	
training:	Epoch: [183][123/233]	Loss 0.0002 (0.0018)	
training:	Epoch: [183][124/233]	Loss 0.0001 (0.0018)	
training:	Epoch: [183][125/233]	Loss 0.0003 (0.0018)	
training:	Epoch: [183][126/233]	Loss 0.0002 (0.0018)	
training:	Epoch: [183][127/233]	Loss 0.0002 (0.0018)	
training:	Epoch: [183][128/233]	Loss 0.0005 (0.0017)	
training:	Epoch: [183][129/233]	Loss 0.0002 (0.0017)	
training:	Epoch: [183][130/233]	Loss 0.0002 (0.0017)	
training:	Epoch: [183][131/233]	Loss 0.0018 (0.0017)	
training:	Epoch: [183][132/233]	Loss 0.0002 (0.0017)	
training:	Epoch: [183][133/233]	Loss 0.0001 (0.0017)	
training:	Epoch: [183][134/233]	Loss 0.0004 (0.0017)	
training:	Epoch: [183][135/233]	Loss 0.0001 (0.0017)	
training:	Epoch: [183][136/233]	Loss 0.0002 (0.0017)	
training:	Epoch: [183][137/233]	Loss 0.0003 (0.0017)	
training:	Epoch: [183][138/233]	Loss 0.0002 (0.0016)	
training:	Epoch: [183][139/233]	Loss 0.0001 (0.0016)	
training:	Epoch: [183][140/233]	Loss 0.0005 (0.0016)	
training:	Epoch: [183][141/233]	Loss 0.0002 (0.0016)	
training:	Epoch: [183][142/233]	Loss 0.0002 (0.0016)	
training:	Epoch: [183][143/233]	Loss 0.0003 (0.0016)	
training:	Epoch: [183][144/233]	Loss 0.0001 (0.0016)	
training:	Epoch: [183][145/233]	Loss 0.0002 (0.0016)	
training:	Epoch: [183][146/233]	Loss 0.0002 (0.0016)	
training:	Epoch: [183][147/233]	Loss 0.0002 (0.0016)	
training:	Epoch: [183][148/233]	Loss 0.0001 (0.0015)	
training:	Epoch: [183][149/233]	Loss 0.0004 (0.0015)	
training:	Epoch: [183][150/233]	Loss 0.0001 (0.0015)	
training:	Epoch: [183][151/233]	Loss 0.0007 (0.0015)	
training:	Epoch: [183][152/233]	Loss 0.0003 (0.0015)	
training:	Epoch: [183][153/233]	Loss 0.0001 (0.0015)	
training:	Epoch: [183][154/233]	Loss 0.0001 (0.0015)	
training:	Epoch: [183][155/233]	Loss 0.0001 (0.0015)	
training:	Epoch: [183][156/233]	Loss 0.0023 (0.0015)	
training:	Epoch: [183][157/233]	Loss 0.0001 (0.0015)	
training:	Epoch: [183][158/233]	Loss 0.0001 (0.0015)	
training:	Epoch: [183][159/233]	Loss 0.0079 (0.0015)	
training:	Epoch: [183][160/233]	Loss 0.0002 (0.0015)	
training:	Epoch: [183][161/233]	Loss 0.0003 (0.0015)	
training:	Epoch: [183][162/233]	Loss 0.0002 (0.0015)	
training:	Epoch: [183][163/233]	Loss 0.0001 (0.0015)	
training:	Epoch: [183][164/233]	Loss 0.0003 (0.0015)	
training:	Epoch: [183][165/233]	Loss 0.0029 (0.0015)	
training:	Epoch: [183][166/233]	Loss 0.0008 (0.0015)	
training:	Epoch: [183][167/233]	Loss 0.0003 (0.0015)	
training:	Epoch: [183][168/233]	Loss 0.0148 (0.0016)	
training:	Epoch: [183][169/233]	Loss 0.0002 (0.0015)	
training:	Epoch: [183][170/233]	Loss 0.0002 (0.0015)	
training:	Epoch: [183][171/233]	Loss 0.0002 (0.0015)	
training:	Epoch: [183][172/233]	Loss 0.0001 (0.0015)	
training:	Epoch: [183][173/233]	Loss 0.0001 (0.0015)	
training:	Epoch: [183][174/233]	Loss 0.0001 (0.0015)	
training:	Epoch: [183][175/233]	Loss 0.0005 (0.0015)	
training:	Epoch: [183][176/233]	Loss 0.0002 (0.0015)	
training:	Epoch: [183][177/233]	Loss 0.0001 (0.0015)	
training:	Epoch: [183][178/233]	Loss 0.0002 (0.0015)	
training:	Epoch: [183][179/233]	Loss 0.0002 (0.0015)	
training:	Epoch: [183][180/233]	Loss 0.0004 (0.0015)	
training:	Epoch: [183][181/233]	Loss 0.0001 (0.0015)	
training:	Epoch: [183][182/233]	Loss 0.0002 (0.0015)	
training:	Epoch: [183][183/233]	Loss 0.0002 (0.0014)	
training:	Epoch: [183][184/233]	Loss 0.0002 (0.0014)	
training:	Epoch: [183][185/233]	Loss 0.0002 (0.0014)	
training:	Epoch: [183][186/233]	Loss 0.0002 (0.0014)	
training:	Epoch: [183][187/233]	Loss 0.0001 (0.0014)	
training:	Epoch: [183][188/233]	Loss 0.0001 (0.0014)	
training:	Epoch: [183][189/233]	Loss 0.0002 (0.0014)	
training:	Epoch: [183][190/233]	Loss 0.0002 (0.0014)	
training:	Epoch: [183][191/233]	Loss 0.0002 (0.0014)	
training:	Epoch: [183][192/233]	Loss 0.0001 (0.0014)	
training:	Epoch: [183][193/233]	Loss 0.0001 (0.0014)	
training:	Epoch: [183][194/233]	Loss 0.0002 (0.0014)	
training:	Epoch: [183][195/233]	Loss 0.0006 (0.0014)	
training:	Epoch: [183][196/233]	Loss 0.0010 (0.0014)	
training:	Epoch: [183][197/233]	Loss 0.0006 (0.0014)	
training:	Epoch: [183][198/233]	Loss 0.0002 (0.0014)	
training:	Epoch: [183][199/233]	Loss 0.0002 (0.0014)	
training:	Epoch: [183][200/233]	Loss 0.0002 (0.0013)	
training:	Epoch: [183][201/233]	Loss 0.0001 (0.0013)	
training:	Epoch: [183][202/233]	Loss 0.0003 (0.0013)	
training:	Epoch: [183][203/233]	Loss 0.0002 (0.0013)	
training:	Epoch: [183][204/233]	Loss 0.0001 (0.0013)	
training:	Epoch: [183][205/233]	Loss 0.0001 (0.0013)	
training:	Epoch: [183][206/233]	Loss 0.0002 (0.0013)	
training:	Epoch: [183][207/233]	Loss 0.0001 (0.0013)	
training:	Epoch: [183][208/233]	Loss 0.0002 (0.0013)	
training:	Epoch: [183][209/233]	Loss 0.0002 (0.0013)	
training:	Epoch: [183][210/233]	Loss 0.0002 (0.0013)	
training:	Epoch: [183][211/233]	Loss 0.0001 (0.0013)	
training:	Epoch: [183][212/233]	Loss 0.0001 (0.0013)	
training:	Epoch: [183][213/233]	Loss 0.0005 (0.0013)	
training:	Epoch: [183][214/233]	Loss 0.0001 (0.0013)	
training:	Epoch: [183][215/233]	Loss 0.0002 (0.0013)	
training:	Epoch: [183][216/233]	Loss 0.0002 (0.0013)	
training:	Epoch: [183][217/233]	Loss 0.0003 (0.0013)	
training:	Epoch: [183][218/233]	Loss 0.0028 (0.0013)	
training:	Epoch: [183][219/233]	Loss 0.0003 (0.0013)	
training:	Epoch: [183][220/233]	Loss 0.0006 (0.0013)	
training:	Epoch: [183][221/233]	Loss 0.0001 (0.0012)	
training:	Epoch: [183][222/233]	Loss 0.0073 (0.0013)	
training:	Epoch: [183][223/233]	Loss 0.0001 (0.0013)	
training:	Epoch: [183][224/233]	Loss 0.0015 (0.0013)	
training:	Epoch: [183][225/233]	Loss 0.0004 (0.0013)	
training:	Epoch: [183][226/233]	Loss 0.0002 (0.0013)	
training:	Epoch: [183][227/233]	Loss 0.0004 (0.0013)	
training:	Epoch: [183][228/233]	Loss 0.0002 (0.0013)	
training:	Epoch: [183][229/233]	Loss 0.0002 (0.0012)	
training:	Epoch: [183][230/233]	Loss 0.0009 (0.0012)	
training:	Epoch: [183][231/233]	Loss 0.0005 (0.0012)	
training:	Epoch: [183][232/233]	Loss 0.0008 (0.0012)	
training:	Epoch: [183][233/233]	Loss 0.0001 (0.0012)	
Training:	 Loss: 0.0012

Training:	 ACC: 1.0000 1.0000 1.0000 1.0000
Validation:	 ACC: 0.8011 0.8010 0.7988 0.8034
Validation:	 Best_BACC: 0.8162 0.8154 0.7998 0.8326
Validation:	 Loss: 1.3692
Pretraining:	Epoch 184/200
----------
training:	Epoch: [184][1/233]	Loss 0.0001 (0.0001)	
training:	Epoch: [184][2/233]	Loss 0.0002 (0.0002)	
training:	Epoch: [184][3/233]	Loss 0.0005 (0.0003)	
training:	Epoch: [184][4/233]	Loss 0.0001 (0.0002)	
training:	Epoch: [184][5/233]	Loss 0.0002 (0.0002)	
training:	Epoch: [184][6/233]	Loss 0.0002 (0.0002)	
training:	Epoch: [184][7/233]	Loss 0.0002 (0.0002)	
training:	Epoch: [184][8/233]	Loss 0.0034 (0.0006)	
training:	Epoch: [184][9/233]	Loss 0.0008 (0.0006)	
training:	Epoch: [184][10/233]	Loss 0.0002 (0.0006)	
training:	Epoch: [184][11/233]	Loss 0.0002 (0.0005)	
training:	Epoch: [184][12/233]	Loss 0.0001 (0.0005)	
training:	Epoch: [184][13/233]	Loss 0.0002 (0.0005)	
training:	Epoch: [184][14/233]	Loss 0.0002 (0.0005)	
training:	Epoch: [184][15/233]	Loss 0.0002 (0.0004)	
training:	Epoch: [184][16/233]	Loss 0.0001 (0.0004)	
training:	Epoch: [184][17/233]	Loss 0.0001 (0.0004)	
training:	Epoch: [184][18/233]	Loss 0.0001 (0.0004)	
training:	Epoch: [184][19/233]	Loss 0.0021 (0.0005)	
training:	Epoch: [184][20/233]	Loss 0.0001 (0.0005)	
training:	Epoch: [184][21/233]	Loss 0.0001 (0.0004)	
training:	Epoch: [184][22/233]	Loss 0.0001 (0.0004)	
training:	Epoch: [184][23/233]	Loss 0.0002 (0.0004)	
training:	Epoch: [184][24/233]	Loss 0.0071 (0.0007)	
training:	Epoch: [184][25/233]	Loss 0.0002 (0.0007)	
training:	Epoch: [184][26/233]	Loss 0.0003 (0.0007)	
training:	Epoch: [184][27/233]	Loss 0.0003 (0.0007)	
training:	Epoch: [184][28/233]	Loss 0.0001 (0.0006)	
training:	Epoch: [184][29/233]	Loss 0.0001 (0.0006)	
training:	Epoch: [184][30/233]	Loss 0.0001 (0.0006)	
training:	Epoch: [184][31/233]	Loss 0.0001 (0.0006)	
training:	Epoch: [184][32/233]	Loss 0.0002 (0.0006)	
training:	Epoch: [184][33/233]	Loss 0.0001 (0.0006)	
training:	Epoch: [184][34/233]	Loss 0.0002 (0.0005)	
training:	Epoch: [184][35/233]	Loss 0.0024 (0.0006)	
training:	Epoch: [184][36/233]	Loss 0.0001 (0.0006)	
training:	Epoch: [184][37/233]	Loss 0.0002 (0.0006)	
training:	Epoch: [184][38/233]	Loss 0.0001 (0.0006)	
training:	Epoch: [184][39/233]	Loss 0.0002 (0.0006)	
training:	Epoch: [184][40/233]	Loss 0.0002 (0.0005)	
training:	Epoch: [184][41/233]	Loss 0.0002 (0.0005)	
training:	Epoch: [184][42/233]	Loss 0.0002 (0.0005)	
training:	Epoch: [184][43/233]	Loss 0.0001 (0.0005)	
training:	Epoch: [184][44/233]	Loss 0.0012 (0.0005)	
training:	Epoch: [184][45/233]	Loss 0.0001 (0.0005)	
training:	Epoch: [184][46/233]	Loss 0.0002 (0.0005)	
training:	Epoch: [184][47/233]	Loss 0.0001 (0.0005)	
training:	Epoch: [184][48/233]	Loss 0.0001 (0.0005)	
training:	Epoch: [184][49/233]	Loss 0.0008 (0.0005)	
training:	Epoch: [184][50/233]	Loss 0.0002 (0.0005)	
training:	Epoch: [184][51/233]	Loss 0.0002 (0.0005)	
training:	Epoch: [184][52/233]	Loss 0.0012 (0.0005)	
training:	Epoch: [184][53/233]	Loss 0.0002 (0.0005)	
training:	Epoch: [184][54/233]	Loss 0.0001 (0.0005)	
training:	Epoch: [184][55/233]	Loss 0.0001 (0.0005)	
training:	Epoch: [184][56/233]	Loss 0.0002 (0.0005)	
training:	Epoch: [184][57/233]	Loss 0.0001 (0.0005)	
training:	Epoch: [184][58/233]	Loss 0.0001 (0.0005)	
training:	Epoch: [184][59/233]	Loss 0.0003 (0.0005)	
training:	Epoch: [184][60/233]	Loss 0.0002 (0.0005)	
training:	Epoch: [184][61/233]	Loss 0.0001 (0.0005)	
training:	Epoch: [184][62/233]	Loss 0.0002 (0.0005)	
training:	Epoch: [184][63/233]	Loss 0.0002 (0.0005)	
training:	Epoch: [184][64/233]	Loss 0.0001 (0.0004)	
training:	Epoch: [184][65/233]	Loss 0.0001 (0.0004)	
training:	Epoch: [184][66/233]	Loss 0.0005 (0.0004)	
training:	Epoch: [184][67/233]	Loss 0.0001 (0.0004)	
training:	Epoch: [184][68/233]	Loss 0.0068 (0.0005)	
training:	Epoch: [184][69/233]	Loss 0.0002 (0.0005)	
training:	Epoch: [184][70/233]	Loss 0.0001 (0.0005)	
training:	Epoch: [184][71/233]	Loss 0.0004 (0.0005)	
training:	Epoch: [184][72/233]	Loss 0.0001 (0.0005)	
training:	Epoch: [184][73/233]	Loss 0.0001 (0.0005)	
training:	Epoch: [184][74/233]	Loss 0.0002 (0.0005)	
training:	Epoch: [184][75/233]	Loss 0.0002 (0.0005)	
training:	Epoch: [184][76/233]	Loss 0.0001 (0.0005)	
training:	Epoch: [184][77/233]	Loss 0.0002 (0.0005)	
training:	Epoch: [184][78/233]	Loss 0.0001 (0.0005)	
training:	Epoch: [184][79/233]	Loss 0.0003 (0.0005)	
training:	Epoch: [184][80/233]	Loss 0.0001 (0.0005)	
training:	Epoch: [184][81/233]	Loss 0.0001 (0.0005)	
training:	Epoch: [184][82/233]	Loss 0.0001 (0.0005)	
training:	Epoch: [184][83/233]	Loss 0.0001 (0.0005)	
training:	Epoch: [184][84/233]	Loss 0.0001 (0.0005)	
training:	Epoch: [184][85/233]	Loss 0.0012 (0.0005)	
training:	Epoch: [184][86/233]	Loss 0.0001 (0.0005)	
training:	Epoch: [184][87/233]	Loss 0.0001 (0.0005)	
training:	Epoch: [184][88/233]	Loss 0.0003 (0.0005)	
training:	Epoch: [184][89/233]	Loss 0.0001 (0.0005)	
training:	Epoch: [184][90/233]	Loss 0.0001 (0.0005)	
training:	Epoch: [184][91/233]	Loss 0.0007 (0.0005)	
training:	Epoch: [184][92/233]	Loss 0.0002 (0.0005)	
training:	Epoch: [184][93/233]	Loss 0.0029 (0.0005)	
training:	Epoch: [184][94/233]	Loss 0.0005 (0.0005)	
training:	Epoch: [184][95/233]	Loss 0.0001 (0.0005)	
training:	Epoch: [184][96/233]	Loss 0.0002 (0.0005)	
training:	Epoch: [184][97/233]	Loss 0.0003 (0.0005)	
training:	Epoch: [184][98/233]	Loss 0.0001 (0.0005)	
training:	Epoch: [184][99/233]	Loss 0.0039 (0.0005)	
training:	Epoch: [184][100/233]	Loss 0.0003 (0.0005)	
training:	Epoch: [184][101/233]	Loss 0.0001 (0.0005)	
training:	Epoch: [184][102/233]	Loss 0.0001 (0.0005)	
training:	Epoch: [184][103/233]	Loss 0.0001 (0.0005)	
training:	Epoch: [184][104/233]	Loss 0.0001 (0.0005)	
training:	Epoch: [184][105/233]	Loss 0.0001 (0.0005)	
training:	Epoch: [184][106/233]	Loss 0.0002 (0.0005)	
training:	Epoch: [184][107/233]	Loss 0.0001 (0.0005)	
training:	Epoch: [184][108/233]	Loss 0.0002 (0.0005)	
training:	Epoch: [184][109/233]	Loss 0.0001 (0.0005)	
training:	Epoch: [184][110/233]	Loss 0.0002 (0.0005)	
training:	Epoch: [184][111/233]	Loss 0.0002 (0.0005)	
training:	Epoch: [184][112/233]	Loss 0.0002 (0.0005)	
training:	Epoch: [184][113/233]	Loss 0.0002 (0.0005)	
training:	Epoch: [184][114/233]	Loss 0.0003 (0.0005)	
training:	Epoch: [184][115/233]	Loss 0.0003 (0.0005)	
training:	Epoch: [184][116/233]	Loss 0.0002 (0.0005)	
training:	Epoch: [184][117/233]	Loss 0.0002 (0.0005)	
training:	Epoch: [184][118/233]	Loss 0.0001 (0.0005)	
training:	Epoch: [184][119/233]	Loss 0.0002 (0.0005)	
training:	Epoch: [184][120/233]	Loss 0.0002 (0.0004)	
training:	Epoch: [184][121/233]	Loss 0.0001 (0.0004)	
training:	Epoch: [184][122/233]	Loss 0.0009 (0.0005)	
training:	Epoch: [184][123/233]	Loss 0.0002 (0.0004)	
training:	Epoch: [184][124/233]	Loss 0.0002 (0.0004)	
training:	Epoch: [184][125/233]	Loss 0.0001 (0.0004)	
training:	Epoch: [184][126/233]	Loss 0.0002 (0.0004)	
training:	Epoch: [184][127/233]	Loss 0.0002 (0.0004)	
training:	Epoch: [184][128/233]	Loss 0.0002 (0.0004)	
training:	Epoch: [184][129/233]	Loss 0.0031 (0.0005)	
training:	Epoch: [184][130/233]	Loss 0.0002 (0.0005)	
training:	Epoch: [184][131/233]	Loss 0.0001 (0.0005)	
training:	Epoch: [184][132/233]	Loss 0.0001 (0.0005)	
training:	Epoch: [184][133/233]	Loss 0.0006 (0.0005)	
training:	Epoch: [184][134/233]	Loss 0.0002 (0.0004)	
training:	Epoch: [184][135/233]	Loss 0.0001 (0.0004)	
training:	Epoch: [184][136/233]	Loss 0.0001 (0.0004)	
training:	Epoch: [184][137/233]	Loss 0.0002 (0.0004)	
training:	Epoch: [184][138/233]	Loss 0.0002 (0.0004)	
training:	Epoch: [184][139/233]	Loss 0.0003 (0.0004)	
training:	Epoch: [184][140/233]	Loss 0.0002 (0.0004)	
training:	Epoch: [184][141/233]	Loss 0.0001 (0.0004)	
training:	Epoch: [184][142/233]	Loss 0.0001 (0.0004)	
training:	Epoch: [184][143/233]	Loss 0.0003 (0.0004)	
training:	Epoch: [184][144/233]	Loss 0.0002 (0.0004)	
training:	Epoch: [184][145/233]	Loss 0.0001 (0.0004)	
training:	Epoch: [184][146/233]	Loss 0.0002 (0.0004)	
training:	Epoch: [184][147/233]	Loss 0.0001 (0.0004)	
training:	Epoch: [184][148/233]	Loss 0.0001 (0.0004)	
training:	Epoch: [184][149/233]	Loss 0.0001 (0.0004)	
training:	Epoch: [184][150/233]	Loss 0.0001 (0.0004)	
training:	Epoch: [184][151/233]	Loss 0.0001 (0.0004)	
training:	Epoch: [184][152/233]	Loss 0.0001 (0.0004)	
training:	Epoch: [184][153/233]	Loss 0.0002 (0.0004)	
training:	Epoch: [184][154/233]	Loss 0.0001 (0.0004)	
training:	Epoch: [184][155/233]	Loss 0.0001 (0.0004)	
training:	Epoch: [184][156/233]	Loss 0.0001 (0.0004)	
training:	Epoch: [184][157/233]	Loss 0.0001 (0.0004)	
training:	Epoch: [184][158/233]	Loss 0.0001 (0.0004)	
training:	Epoch: [184][159/233]	Loss 0.0001 (0.0004)	
training:	Epoch: [184][160/233]	Loss 0.0002 (0.0004)	
training:	Epoch: [184][161/233]	Loss 0.0002 (0.0004)	
training:	Epoch: [184][162/233]	Loss 0.0001 (0.0004)	
training:	Epoch: [184][163/233]	Loss 0.0002 (0.0004)	
training:	Epoch: [184][164/233]	Loss 0.0001 (0.0004)	
training:	Epoch: [184][165/233]	Loss 0.0001 (0.0004)	
training:	Epoch: [184][166/233]	Loss 0.0001 (0.0004)	
training:	Epoch: [184][167/233]	Loss 0.0001 (0.0004)	
training:	Epoch: [184][168/233]	Loss 0.0001 (0.0004)	
training:	Epoch: [184][169/233]	Loss 0.0001 (0.0004)	
training:	Epoch: [184][170/233]	Loss 0.0002 (0.0004)	
training:	Epoch: [184][171/233]	Loss 0.0001 (0.0004)	
training:	Epoch: [184][172/233]	Loss 0.0001 (0.0004)	
training:	Epoch: [184][173/233]	Loss 0.0001 (0.0004)	
training:	Epoch: [184][174/233]	Loss 0.0001 (0.0004)	
training:	Epoch: [184][175/233]	Loss 0.0001 (0.0004)	
training:	Epoch: [184][176/233]	Loss 0.0002 (0.0004)	
training:	Epoch: [184][177/233]	Loss 0.0002 (0.0004)	
training:	Epoch: [184][178/233]	Loss 0.0001 (0.0004)	
training:	Epoch: [184][179/233]	Loss 0.0001 (0.0004)	
training:	Epoch: [184][180/233]	Loss 0.0002 (0.0004)	
training:	Epoch: [184][181/233]	Loss 0.0001 (0.0004)	
training:	Epoch: [184][182/233]	Loss 0.0001 (0.0004)	
training:	Epoch: [184][183/233]	Loss 0.0002 (0.0004)	
training:	Epoch: [184][184/233]	Loss 0.0001 (0.0004)	
training:	Epoch: [184][185/233]	Loss 0.0006 (0.0004)	
training:	Epoch: [184][186/233]	Loss 0.0001 (0.0004)	
training:	Epoch: [184][187/233]	Loss 0.0002 (0.0004)	
training:	Epoch: [184][188/233]	Loss 0.0001 (0.0004)	
training:	Epoch: [184][189/233]	Loss 0.0004 (0.0004)	
training:	Epoch: [184][190/233]	Loss 0.0002 (0.0004)	
training:	Epoch: [184][191/233]	Loss 0.0001 (0.0004)	
training:	Epoch: [184][192/233]	Loss 0.0019 (0.0004)	
training:	Epoch: [184][193/233]	Loss 0.0001 (0.0004)	
training:	Epoch: [184][194/233]	Loss 0.0001 (0.0004)	
training:	Epoch: [184][195/233]	Loss 0.0002 (0.0004)	
training:	Epoch: [184][196/233]	Loss 0.0003 (0.0004)	
training:	Epoch: [184][197/233]	Loss 0.0001 (0.0004)	
training:	Epoch: [184][198/233]	Loss 0.0002 (0.0004)	
training:	Epoch: [184][199/233]	Loss 0.0162 (0.0004)	
training:	Epoch: [184][200/233]	Loss 0.0001 (0.0004)	
training:	Epoch: [184][201/233]	Loss 0.0004 (0.0004)	
training:	Epoch: [184][202/233]	Loss 0.0002 (0.0004)	
training:	Epoch: [184][203/233]	Loss 0.0002 (0.0004)	
training:	Epoch: [184][204/233]	Loss 0.0001 (0.0004)	
training:	Epoch: [184][205/233]	Loss 0.0002 (0.0004)	
training:	Epoch: [184][206/233]	Loss 0.0002 (0.0004)	
training:	Epoch: [184][207/233]	Loss 0.0003 (0.0004)	
training:	Epoch: [184][208/233]	Loss 0.0002 (0.0004)	
training:	Epoch: [184][209/233]	Loss 0.0003 (0.0004)	
training:	Epoch: [184][210/233]	Loss 0.0004 (0.0004)	
training:	Epoch: [184][211/233]	Loss 0.0001 (0.0004)	
training:	Epoch: [184][212/233]	Loss 0.0002 (0.0004)	
training:	Epoch: [184][213/233]	Loss 0.0001 (0.0004)	
training:	Epoch: [184][214/233]	Loss 0.0011 (0.0004)	
training:	Epoch: [184][215/233]	Loss 0.0002 (0.0004)	
training:	Epoch: [184][216/233]	Loss 0.0001 (0.0004)	
training:	Epoch: [184][217/233]	Loss 0.0001 (0.0004)	
training:	Epoch: [184][218/233]	Loss 0.0001 (0.0004)	
training:	Epoch: [184][219/233]	Loss 0.0001 (0.0004)	
training:	Epoch: [184][220/233]	Loss 0.0002 (0.0004)	
training:	Epoch: [184][221/233]	Loss 0.0001 (0.0004)	
training:	Epoch: [184][222/233]	Loss 0.0001 (0.0004)	
training:	Epoch: [184][223/233]	Loss 0.0002 (0.0004)	
training:	Epoch: [184][224/233]	Loss 0.0001 (0.0004)	
training:	Epoch: [184][225/233]	Loss 0.0001 (0.0004)	
training:	Epoch: [184][226/233]	Loss 0.0001 (0.0004)	
training:	Epoch: [184][227/233]	Loss 0.0006 (0.0004)	
training:	Epoch: [184][228/233]	Loss 0.0001 (0.0004)	
training:	Epoch: [184][229/233]	Loss 0.0001 (0.0004)	
training:	Epoch: [184][230/233]	Loss 0.0001 (0.0004)	
training:	Epoch: [184][231/233]	Loss 0.0001 (0.0004)	
training:	Epoch: [184][232/233]	Loss 0.0001 (0.0004)	
training:	Epoch: [184][233/233]	Loss 0.0001 (0.0004)	
Training:	 Loss: 0.0004

Training:	 ACC: 1.0000 1.0000 1.0000 1.0000
Validation:	 ACC: 0.7919 0.7903 0.7569 0.8270
Validation:	 Best_BACC: 0.8162 0.8154 0.7998 0.8326
Validation:	 Loss: 1.4343
Pretraining:	Epoch 185/200
----------
training:	Epoch: [185][1/233]	Loss 0.0002 (0.0002)	
training:	Epoch: [185][2/233]	Loss 0.0001 (0.0002)	
training:	Epoch: [185][3/233]	Loss 0.0001 (0.0002)	
training:	Epoch: [185][4/233]	Loss 0.0012 (0.0004)	
training:	Epoch: [185][5/233]	Loss 0.0004 (0.0004)	
training:	Epoch: [185][6/233]	Loss 0.0001 (0.0004)	
training:	Epoch: [185][7/233]	Loss 0.0002 (0.0003)	
training:	Epoch: [185][8/233]	Loss 0.0003 (0.0003)	
training:	Epoch: [185][9/233]	Loss 0.0004 (0.0003)	
training:	Epoch: [185][10/233]	Loss 0.0002 (0.0003)	
training:	Epoch: [185][11/233]	Loss 0.0001 (0.0003)	
training:	Epoch: [185][12/233]	Loss 0.0001 (0.0003)	
training:	Epoch: [185][13/233]	Loss 0.0001 (0.0003)	
training:	Epoch: [185][14/233]	Loss 0.0027 (0.0004)	
training:	Epoch: [185][15/233]	Loss 0.0001 (0.0004)	
training:	Epoch: [185][16/233]	Loss 0.0562 (0.0039)	
training:	Epoch: [185][17/233]	Loss 0.0002 (0.0037)	
training:	Epoch: [185][18/233]	Loss 0.0001 (0.0035)	
training:	Epoch: [185][19/233]	Loss 0.0001 (0.0033)	
training:	Epoch: [185][20/233]	Loss 0.0011 (0.0032)	
training:	Epoch: [185][21/233]	Loss 0.0001 (0.0031)	
training:	Epoch: [185][22/233]	Loss 0.0001 (0.0029)	
training:	Epoch: [185][23/233]	Loss 0.0001 (0.0028)	
training:	Epoch: [185][24/233]	Loss 0.0001 (0.0027)	
training:	Epoch: [185][25/233]	Loss 0.0001 (0.0026)	
training:	Epoch: [185][26/233]	Loss 0.0002 (0.0025)	
training:	Epoch: [185][27/233]	Loss 0.0001 (0.0024)	
training:	Epoch: [185][28/233]	Loss 0.0005 (0.0023)	
training:	Epoch: [185][29/233]	Loss 0.0001 (0.0023)	
training:	Epoch: [185][30/233]	Loss 0.0001 (0.0022)	
training:	Epoch: [185][31/233]	Loss 0.0001 (0.0021)	
training:	Epoch: [185][32/233]	Loss 0.0001 (0.0021)	
training:	Epoch: [185][33/233]	Loss 0.0005 (0.0020)	
training:	Epoch: [185][34/233]	Loss 0.0001 (0.0020)	
training:	Epoch: [185][35/233]	Loss 0.0005 (0.0019)	
training:	Epoch: [185][36/233]	Loss 0.0004 (0.0019)	
training:	Epoch: [185][37/233]	Loss 0.0003 (0.0018)	
training:	Epoch: [185][38/233]	Loss 0.0001 (0.0018)	
training:	Epoch: [185][39/233]	Loss 0.0001 (0.0017)	
training:	Epoch: [185][40/233]	Loss 0.0002 (0.0017)	
training:	Epoch: [185][41/233]	Loss 0.0001 (0.0017)	
training:	Epoch: [185][42/233]	Loss 0.0002 (0.0016)	
training:	Epoch: [185][43/233]	Loss 0.0002 (0.0016)	
training:	Epoch: [185][44/233]	Loss 0.0019 (0.0016)	
training:	Epoch: [185][45/233]	Loss 0.0006 (0.0016)	
training:	Epoch: [185][46/233]	Loss 0.0005 (0.0016)	
training:	Epoch: [185][47/233]	Loss 0.0024 (0.0016)	
training:	Epoch: [185][48/233]	Loss 0.0002 (0.0016)	
training:	Epoch: [185][49/233]	Loss 0.0003 (0.0015)	
training:	Epoch: [185][50/233]	Loss 0.0004 (0.0015)	
training:	Epoch: [185][51/233]	Loss 0.0354 (0.0022)	
training:	Epoch: [185][52/233]	Loss 0.0002 (0.0021)	
training:	Epoch: [185][53/233]	Loss 0.0002 (0.0021)	
training:	Epoch: [185][54/233]	Loss 0.0002 (0.0021)	
training:	Epoch: [185][55/233]	Loss 0.0002 (0.0020)	
training:	Epoch: [185][56/233]	Loss 0.0001 (0.0020)	
training:	Epoch: [185][57/233]	Loss 0.0001 (0.0020)	
training:	Epoch: [185][58/233]	Loss 0.0001 (0.0019)	
training:	Epoch: [185][59/233]	Loss 0.0001 (0.0019)	
training:	Epoch: [185][60/233]	Loss 0.0001 (0.0019)	
training:	Epoch: [185][61/233]	Loss 0.0001 (0.0018)	
training:	Epoch: [185][62/233]	Loss 0.0006 (0.0018)	
training:	Epoch: [185][63/233]	Loss 0.0005 (0.0018)	
training:	Epoch: [185][64/233]	Loss 0.0017 (0.0018)	
training:	Epoch: [185][65/233]	Loss 0.0001 (0.0018)	
training:	Epoch: [185][66/233]	Loss 0.0001 (0.0017)	
training:	Epoch: [185][67/233]	Loss 0.1085 (0.0033)	
training:	Epoch: [185][68/233]	Loss 0.0001 (0.0033)	
training:	Epoch: [185][69/233]	Loss 0.0002 (0.0032)	
training:	Epoch: [185][70/233]	Loss 0.0002 (0.0032)	
training:	Epoch: [185][71/233]	Loss 0.0001 (0.0032)	
training:	Epoch: [185][72/233]	Loss 0.0008 (0.0031)	
training:	Epoch: [185][73/233]	Loss 0.0007 (0.0031)	
training:	Epoch: [185][74/233]	Loss 0.0002 (0.0031)	
training:	Epoch: [185][75/233]	Loss 0.0001 (0.0030)	
training:	Epoch: [185][76/233]	Loss 0.0001 (0.0030)	
training:	Epoch: [185][77/233]	Loss 0.0001 (0.0029)	
training:	Epoch: [185][78/233]	Loss 0.0003 (0.0029)	
training:	Epoch: [185][79/233]	Loss 0.0001 (0.0029)	
training:	Epoch: [185][80/233]	Loss 0.0003 (0.0028)	
training:	Epoch: [185][81/233]	Loss 0.0002 (0.0028)	
training:	Epoch: [185][82/233]	Loss 0.0001 (0.0028)	
training:	Epoch: [185][83/233]	Loss 0.0001 (0.0027)	
training:	Epoch: [185][84/233]	Loss 0.0002 (0.0027)	
training:	Epoch: [185][85/233]	Loss 0.0012 (0.0027)	
training:	Epoch: [185][86/233]	Loss 0.0005 (0.0027)	
training:	Epoch: [185][87/233]	Loss 0.0001 (0.0026)	
training:	Epoch: [185][88/233]	Loss 0.0002 (0.0026)	
training:	Epoch: [185][89/233]	Loss 0.0002 (0.0026)	
training:	Epoch: [185][90/233]	Loss 0.0002 (0.0026)	
training:	Epoch: [185][91/233]	Loss 0.0001 (0.0025)	
training:	Epoch: [185][92/233]	Loss 0.0004 (0.0025)	
training:	Epoch: [185][93/233]	Loss 0.0001 (0.0025)	
training:	Epoch: [185][94/233]	Loss 0.0005 (0.0025)	
training:	Epoch: [185][95/233]	Loss 0.0010 (0.0024)	
training:	Epoch: [185][96/233]	Loss 0.0001 (0.0024)	
training:	Epoch: [185][97/233]	Loss 0.0001 (0.0024)	
training:	Epoch: [185][98/233]	Loss 0.0006 (0.0024)	
training:	Epoch: [185][99/233]	Loss 0.0002 (0.0024)	
training:	Epoch: [185][100/233]	Loss 0.0001 (0.0023)	
training:	Epoch: [185][101/233]	Loss 0.0020 (0.0023)	
training:	Epoch: [185][102/233]	Loss 0.0002 (0.0023)	
training:	Epoch: [185][103/233]	Loss 0.0069 (0.0024)	
training:	Epoch: [185][104/233]	Loss 0.0028 (0.0024)	
training:	Epoch: [185][105/233]	Loss 0.0009 (0.0023)	
training:	Epoch: [185][106/233]	Loss 0.0033 (0.0024)	
training:	Epoch: [185][107/233]	Loss 0.0003 (0.0023)	
training:	Epoch: [185][108/233]	Loss 0.0004 (0.0023)	
training:	Epoch: [185][109/233]	Loss 0.1571 (0.0037)	
training:	Epoch: [185][110/233]	Loss 0.0003 (0.0037)	
training:	Epoch: [185][111/233]	Loss 0.0002 (0.0037)	
training:	Epoch: [185][112/233]	Loss 0.0002 (0.0036)	
training:	Epoch: [185][113/233]	Loss 0.0003 (0.0036)	
training:	Epoch: [185][114/233]	Loss 0.0001 (0.0036)	
training:	Epoch: [185][115/233]	Loss 0.0001 (0.0036)	
training:	Epoch: [185][116/233]	Loss 0.0591 (0.0040)	
training:	Epoch: [185][117/233]	Loss 0.0002 (0.0040)	
training:	Epoch: [185][118/233]	Loss 0.0005 (0.0040)	
training:	Epoch: [185][119/233]	Loss 0.0001 (0.0039)	
training:	Epoch: [185][120/233]	Loss 0.0002 (0.0039)	
training:	Epoch: [185][121/233]	Loss 0.0001 (0.0039)	
training:	Epoch: [185][122/233]	Loss 0.0002 (0.0038)	
training:	Epoch: [185][123/233]	Loss 0.0011 (0.0038)	
training:	Epoch: [185][124/233]	Loss 0.0049 (0.0038)	
training:	Epoch: [185][125/233]	Loss 0.0293 (0.0040)	
training:	Epoch: [185][126/233]	Loss 0.0001 (0.0040)	
training:	Epoch: [185][127/233]	Loss 0.0001 (0.0040)	
training:	Epoch: [185][128/233]	Loss 0.0091 (0.0040)	
training:	Epoch: [185][129/233]	Loss 0.0001 (0.0040)	
training:	Epoch: [185][130/233]	Loss 0.0210 (0.0041)	
training:	Epoch: [185][131/233]	Loss 0.0002 (0.0041)	
training:	Epoch: [185][132/233]	Loss 0.0002 (0.0041)	
training:	Epoch: [185][133/233]	Loss 0.0001 (0.0040)	
training:	Epoch: [185][134/233]	Loss 0.0005 (0.0040)	
training:	Epoch: [185][135/233]	Loss 0.0001 (0.0040)	
training:	Epoch: [185][136/233]	Loss 0.0009 (0.0039)	
training:	Epoch: [185][137/233]	Loss 0.0002 (0.0039)	
training:	Epoch: [185][138/233]	Loss 0.0057 (0.0039)	
training:	Epoch: [185][139/233]	Loss 0.0001 (0.0039)	
training:	Epoch: [185][140/233]	Loss 0.0002 (0.0039)	
training:	Epoch: [185][141/233]	Loss 0.0002 (0.0039)	
training:	Epoch: [185][142/233]	Loss 0.0001 (0.0038)	
training:	Epoch: [185][143/233]	Loss 0.0001 (0.0038)	
training:	Epoch: [185][144/233]	Loss 0.0002 (0.0038)	
training:	Epoch: [185][145/233]	Loss 0.0002 (0.0037)	
training:	Epoch: [185][146/233]	Loss 0.0002 (0.0037)	
training:	Epoch: [185][147/233]	Loss 0.0001 (0.0037)	
training:	Epoch: [185][148/233]	Loss 0.0172 (0.0038)	
training:	Epoch: [185][149/233]	Loss 0.0007 (0.0038)	
training:	Epoch: [185][150/233]	Loss 0.0002 (0.0037)	
training:	Epoch: [185][151/233]	Loss 0.0002 (0.0037)	
training:	Epoch: [185][152/233]	Loss 0.0001 (0.0037)	
training:	Epoch: [185][153/233]	Loss 0.0009 (0.0037)	
training:	Epoch: [185][154/233]	Loss 0.0003 (0.0037)	
training:	Epoch: [185][155/233]	Loss 0.0014 (0.0036)	
training:	Epoch: [185][156/233]	Loss 0.0002 (0.0036)	
training:	Epoch: [185][157/233]	Loss 0.0001 (0.0036)	
training:	Epoch: [185][158/233]	Loss 0.0001 (0.0036)	
training:	Epoch: [185][159/233]	Loss 0.0001 (0.0036)	
training:	Epoch: [185][160/233]	Loss 0.0005 (0.0035)	
training:	Epoch: [185][161/233]	Loss 0.0005 (0.0035)	
training:	Epoch: [185][162/233]	Loss 0.0002 (0.0035)	
training:	Epoch: [185][163/233]	Loss 0.0001 (0.0035)	
training:	Epoch: [185][164/233]	Loss 0.0001 (0.0035)	
training:	Epoch: [185][165/233]	Loss 0.0002 (0.0034)	
training:	Epoch: [185][166/233]	Loss 0.0007 (0.0034)	
training:	Epoch: [185][167/233]	Loss 0.0002 (0.0034)	
training:	Epoch: [185][168/233]	Loss 0.0002 (0.0034)	
training:	Epoch: [185][169/233]	Loss 0.0003 (0.0034)	
training:	Epoch: [185][170/233]	Loss 0.0001 (0.0033)	
training:	Epoch: [185][171/233]	Loss 0.0001 (0.0033)	
training:	Epoch: [185][172/233]	Loss 0.0002 (0.0033)	
training:	Epoch: [185][173/233]	Loss 0.0001 (0.0033)	
training:	Epoch: [185][174/233]	Loss 0.0007 (0.0033)	
training:	Epoch: [185][175/233]	Loss 0.0002 (0.0033)	
training:	Epoch: [185][176/233]	Loss 0.0018 (0.0032)	
training:	Epoch: [185][177/233]	Loss 0.0002 (0.0032)	
training:	Epoch: [185][178/233]	Loss 0.0003 (0.0032)	
training:	Epoch: [185][179/233]	Loss 0.0002 (0.0032)	
training:	Epoch: [185][180/233]	Loss 0.0090 (0.0032)	
training:	Epoch: [185][181/233]	Loss 0.0003 (0.0032)	
training:	Epoch: [185][182/233]	Loss 0.0002 (0.0032)	
training:	Epoch: [185][183/233]	Loss 0.0001 (0.0032)	
training:	Epoch: [185][184/233]	Loss 0.0001 (0.0032)	
training:	Epoch: [185][185/233]	Loss 0.0001 (0.0031)	
training:	Epoch: [185][186/233]	Loss 0.0002 (0.0031)	
training:	Epoch: [185][187/233]	Loss 0.0002 (0.0031)	
training:	Epoch: [185][188/233]	Loss 0.0002 (0.0031)	
training:	Epoch: [185][189/233]	Loss 0.0002 (0.0031)	
training:	Epoch: [185][190/233]	Loss 0.0001 (0.0031)	
training:	Epoch: [185][191/233]	Loss 0.0002 (0.0031)	
training:	Epoch: [185][192/233]	Loss 0.0001 (0.0030)	
training:	Epoch: [185][193/233]	Loss 0.0009 (0.0030)	
training:	Epoch: [185][194/233]	Loss 0.0744 (0.0034)	
training:	Epoch: [185][195/233]	Loss 0.0001 (0.0034)	
training:	Epoch: [185][196/233]	Loss 0.0001 (0.0034)	
training:	Epoch: [185][197/233]	Loss 0.0001 (0.0033)	
training:	Epoch: [185][198/233]	Loss 0.0005 (0.0033)	
training:	Epoch: [185][199/233]	Loss 0.0002 (0.0033)	
training:	Epoch: [185][200/233]	Loss 0.0002 (0.0033)	
training:	Epoch: [185][201/233]	Loss 0.0002 (0.0033)	
training:	Epoch: [185][202/233]	Loss 0.0001 (0.0033)	
training:	Epoch: [185][203/233]	Loss 0.0004 (0.0033)	
training:	Epoch: [185][204/233]	Loss 0.0002 (0.0032)	
training:	Epoch: [185][205/233]	Loss 0.0273 (0.0034)	
training:	Epoch: [185][206/233]	Loss 0.0002 (0.0033)	
training:	Epoch: [185][207/233]	Loss 0.0054 (0.0033)	
training:	Epoch: [185][208/233]	Loss 0.0002 (0.0033)	
training:	Epoch: [185][209/233]	Loss 0.0004 (0.0033)	
training:	Epoch: [185][210/233]	Loss 0.0004 (0.0033)	
training:	Epoch: [185][211/233]	Loss 0.0001 (0.0033)	
training:	Epoch: [185][212/233]	Loss 0.0002 (0.0033)	
training:	Epoch: [185][213/233]	Loss 0.0002 (0.0033)	
training:	Epoch: [185][214/233]	Loss 0.0002 (0.0032)	
training:	Epoch: [185][215/233]	Loss 0.0002 (0.0032)	
training:	Epoch: [185][216/233]	Loss 0.0002 (0.0032)	
training:	Epoch: [185][217/233]	Loss 0.0087 (0.0032)	
training:	Epoch: [185][218/233]	Loss 0.1009 (0.0037)	
training:	Epoch: [185][219/233]	Loss 0.0001 (0.0037)	
training:	Epoch: [185][220/233]	Loss 0.0001 (0.0037)	
training:	Epoch: [185][221/233]	Loss 0.0002 (0.0036)	
training:	Epoch: [185][222/233]	Loss 0.0001 (0.0036)	
training:	Epoch: [185][223/233]	Loss 0.0001 (0.0036)	
training:	Epoch: [185][224/233]	Loss 0.0001 (0.0036)	
training:	Epoch: [185][225/233]	Loss 0.0001 (0.0036)	
training:	Epoch: [185][226/233]	Loss 0.0001 (0.0036)	
training:	Epoch: [185][227/233]	Loss 0.0431 (0.0037)	
training:	Epoch: [185][228/233]	Loss 0.0002 (0.0037)	
training:	Epoch: [185][229/233]	Loss 0.0002 (0.0037)	
training:	Epoch: [185][230/233]	Loss 0.0001 (0.0037)	
training:	Epoch: [185][231/233]	Loss 0.0001 (0.0037)	
training:	Epoch: [185][232/233]	Loss 0.0002 (0.0037)	
training:	Epoch: [185][233/233]	Loss 0.0002 (0.0036)	
Training:	 Loss: 0.0036

Training:	 ACC: 1.0000 1.0000 1.0000 1.0000
Validation:	 ACC: 0.7950 0.7940 0.7732 0.8169
Validation:	 Best_BACC: 0.8162 0.8154 0.7998 0.8326
Validation:	 Loss: 1.4057
Pretraining:	Epoch 186/200
----------
training:	Epoch: [186][1/233]	Loss 0.0002 (0.0002)	
training:	Epoch: [186][2/233]	Loss 0.1115 (0.0559)	
training:	Epoch: [186][3/233]	Loss 0.0001 (0.0373)	
training:	Epoch: [186][4/233]	Loss 0.0001 (0.0280)	
training:	Epoch: [186][5/233]	Loss 0.0003 (0.0225)	
training:	Epoch: [186][6/233]	Loss 0.0002 (0.0187)	
training:	Epoch: [186][7/233]	Loss 0.0001 (0.0161)	
training:	Epoch: [186][8/233]	Loss 0.0002 (0.0141)	
training:	Epoch: [186][9/233]	Loss 0.0002 (0.0126)	
training:	Epoch: [186][10/233]	Loss 0.0001 (0.0113)	
training:	Epoch: [186][11/233]	Loss 0.0004 (0.0103)	
training:	Epoch: [186][12/233]	Loss 0.0015 (0.0096)	
training:	Epoch: [186][13/233]	Loss 0.0001 (0.0089)	
training:	Epoch: [186][14/233]	Loss 0.0002 (0.0082)	
training:	Epoch: [186][15/233]	Loss 0.0015 (0.0078)	
training:	Epoch: [186][16/233]	Loss 0.0002 (0.0073)	
training:	Epoch: [186][17/233]	Loss 0.0001 (0.0069)	
training:	Epoch: [186][18/233]	Loss 0.0002 (0.0065)	
training:	Epoch: [186][19/233]	Loss 0.0002 (0.0062)	
training:	Epoch: [186][20/233]	Loss 0.0002 (0.0059)	
training:	Epoch: [186][21/233]	Loss 0.0087 (0.0060)	
training:	Epoch: [186][22/233]	Loss 0.0001 (0.0058)	
training:	Epoch: [186][23/233]	Loss 0.0007 (0.0055)	
training:	Epoch: [186][24/233]	Loss 0.0002 (0.0053)	
training:	Epoch: [186][25/233]	Loss 0.0002 (0.0051)	
training:	Epoch: [186][26/233]	Loss 0.0061 (0.0051)	
training:	Epoch: [186][27/233]	Loss 0.0010 (0.0050)	
training:	Epoch: [186][28/233]	Loss 0.1269 (0.0093)	
training:	Epoch: [186][29/233]	Loss 0.0001 (0.0090)	
training:	Epoch: [186][30/233]	Loss 0.0001 (0.0087)	
training:	Epoch: [186][31/233]	Loss 0.0001 (0.0084)	
training:	Epoch: [186][32/233]	Loss 0.0001 (0.0082)	
training:	Epoch: [186][33/233]	Loss 0.0001 (0.0079)	
training:	Epoch: [186][34/233]	Loss 0.0003 (0.0077)	
training:	Epoch: [186][35/233]	Loss 0.0001 (0.0075)	
training:	Epoch: [186][36/233]	Loss 0.0001 (0.0073)	
training:	Epoch: [186][37/233]	Loss 0.0016 (0.0071)	
training:	Epoch: [186][38/233]	Loss 0.0002 (0.0070)	
training:	Epoch: [186][39/233]	Loss 0.0002 (0.0068)	
training:	Epoch: [186][40/233]	Loss 0.0004 (0.0066)	
training:	Epoch: [186][41/233]	Loss 0.0001 (0.0065)	
training:	Epoch: [186][42/233]	Loss 0.0002 (0.0063)	
training:	Epoch: [186][43/233]	Loss 0.0101 (0.0064)	
training:	Epoch: [186][44/233]	Loss 0.0002 (0.0063)	
training:	Epoch: [186][45/233]	Loss 0.0004 (0.0061)	
training:	Epoch: [186][46/233]	Loss 0.0002 (0.0060)	
training:	Epoch: [186][47/233]	Loss 0.0002 (0.0059)	
training:	Epoch: [186][48/233]	Loss 0.0119 (0.0060)	
training:	Epoch: [186][49/233]	Loss 0.0002 (0.0059)	
training:	Epoch: [186][50/233]	Loss 0.0002 (0.0058)	
training:	Epoch: [186][51/233]	Loss 0.0004 (0.0057)	
training:	Epoch: [186][52/233]	Loss 0.0005 (0.0056)	
training:	Epoch: [186][53/233]	Loss 0.0002 (0.0055)	
training:	Epoch: [186][54/233]	Loss 0.0002 (0.0054)	
training:	Epoch: [186][55/233]	Loss 0.0008 (0.0053)	
training:	Epoch: [186][56/233]	Loss 0.0002 (0.0052)	
training:	Epoch: [186][57/233]	Loss 0.0001 (0.0051)	
training:	Epoch: [186][58/233]	Loss 0.0001 (0.0050)	
training:	Epoch: [186][59/233]	Loss 0.0002 (0.0049)	
training:	Epoch: [186][60/233]	Loss 0.0002 (0.0049)	
training:	Epoch: [186][61/233]	Loss 0.0002 (0.0048)	
training:	Epoch: [186][62/233]	Loss 0.0003 (0.0047)	
training:	Epoch: [186][63/233]	Loss 0.0002 (0.0046)	
training:	Epoch: [186][64/233]	Loss 0.0001 (0.0046)	
training:	Epoch: [186][65/233]	Loss 0.0001 (0.0045)	
training:	Epoch: [186][66/233]	Loss 0.0002 (0.0044)	
training:	Epoch: [186][67/233]	Loss 0.0002 (0.0044)	
training:	Epoch: [186][68/233]	Loss 0.0002 (0.0043)	
training:	Epoch: [186][69/233]	Loss 0.0021 (0.0043)	
training:	Epoch: [186][70/233]	Loss 0.0004 (0.0042)	
training:	Epoch: [186][71/233]	Loss 0.0002 (0.0042)	
training:	Epoch: [186][72/233]	Loss 0.0002 (0.0041)	
training:	Epoch: [186][73/233]	Loss 0.0002 (0.0041)	
training:	Epoch: [186][74/233]	Loss 0.0001 (0.0040)	
training:	Epoch: [186][75/233]	Loss 0.0025 (0.0040)	
training:	Epoch: [186][76/233]	Loss 0.0002 (0.0039)	
training:	Epoch: [186][77/233]	Loss 0.0010 (0.0039)	
training:	Epoch: [186][78/233]	Loss 0.0005 (0.0039)	
training:	Epoch: [186][79/233]	Loss 0.0002 (0.0038)	
training:	Epoch: [186][80/233]	Loss 0.0002 (0.0038)	
training:	Epoch: [186][81/233]	Loss 0.0001 (0.0037)	
training:	Epoch: [186][82/233]	Loss 0.0002 (0.0037)	
training:	Epoch: [186][83/233]	Loss 0.0001 (0.0036)	
training:	Epoch: [186][84/233]	Loss 0.0002 (0.0036)	
training:	Epoch: [186][85/233]	Loss 0.0002 (0.0035)	
training:	Epoch: [186][86/233]	Loss 0.0001 (0.0035)	
training:	Epoch: [186][87/233]	Loss 0.0002 (0.0035)	
training:	Epoch: [186][88/233]	Loss 0.0002 (0.0034)	
training:	Epoch: [186][89/233]	Loss 0.0004 (0.0034)	
training:	Epoch: [186][90/233]	Loss 0.0003 (0.0034)	
training:	Epoch: [186][91/233]	Loss 0.0245 (0.0036)	
training:	Epoch: [186][92/233]	Loss 0.0037 (0.0036)	
training:	Epoch: [186][93/233]	Loss 0.0002 (0.0036)	
training:	Epoch: [186][94/233]	Loss 0.0010 (0.0035)	
training:	Epoch: [186][95/233]	Loss 0.0272 (0.0038)	
training:	Epoch: [186][96/233]	Loss 0.0025 (0.0038)	
training:	Epoch: [186][97/233]	Loss 0.0013 (0.0037)	
training:	Epoch: [186][98/233]	Loss 0.0003 (0.0037)	
training:	Epoch: [186][99/233]	Loss 0.0002 (0.0037)	
training:	Epoch: [186][100/233]	Loss 0.0006 (0.0036)	
training:	Epoch: [186][101/233]	Loss 0.0002 (0.0036)	
training:	Epoch: [186][102/233]	Loss 0.0032 (0.0036)	
training:	Epoch: [186][103/233]	Loss 0.0002 (0.0036)	
training:	Epoch: [186][104/233]	Loss 0.0024 (0.0036)	
training:	Epoch: [186][105/233]	Loss 0.0006 (0.0035)	
training:	Epoch: [186][106/233]	Loss 0.0014 (0.0035)	
training:	Epoch: [186][107/233]	Loss 0.0047 (0.0035)	
training:	Epoch: [186][108/233]	Loss 0.0005 (0.0035)	
training:	Epoch: [186][109/233]	Loss 0.0001 (0.0035)	
training:	Epoch: [186][110/233]	Loss 0.0002 (0.0034)	
training:	Epoch: [186][111/233]	Loss 0.0005 (0.0034)	
training:	Epoch: [186][112/233]	Loss 0.0009 (0.0034)	
training:	Epoch: [186][113/233]	Loss 0.0002 (0.0034)	
training:	Epoch: [186][114/233]	Loss 0.0004 (0.0033)	
training:	Epoch: [186][115/233]	Loss 0.0005 (0.0033)	
training:	Epoch: [186][116/233]	Loss 0.0481 (0.0037)	
training:	Epoch: [186][117/233]	Loss 0.0001 (0.0037)	
training:	Epoch: [186][118/233]	Loss 0.0004 (0.0036)	
training:	Epoch: [186][119/233]	Loss 0.0002 (0.0036)	
training:	Epoch: [186][120/233]	Loss 0.0001 (0.0036)	
training:	Epoch: [186][121/233]	Loss 0.0784 (0.0042)	
training:	Epoch: [186][122/233]	Loss 0.0001 (0.0042)	
training:	Epoch: [186][123/233]	Loss 0.0011 (0.0041)	
training:	Epoch: [186][124/233]	Loss 0.0002 (0.0041)	
training:	Epoch: [186][125/233]	Loss 0.0002 (0.0041)	
training:	Epoch: [186][126/233]	Loss 0.0001 (0.0040)	
training:	Epoch: [186][127/233]	Loss 0.0005 (0.0040)	
training:	Epoch: [186][128/233]	Loss 0.0002 (0.0040)	
training:	Epoch: [186][129/233]	Loss 0.0003 (0.0040)	
training:	Epoch: [186][130/233]	Loss 0.0001 (0.0039)	
training:	Epoch: [186][131/233]	Loss 0.0042 (0.0039)	
training:	Epoch: [186][132/233]	Loss 0.0003 (0.0039)	
training:	Epoch: [186][133/233]	Loss 0.0030 (0.0039)	
training:	Epoch: [186][134/233]	Loss 0.0001 (0.0039)	
training:	Epoch: [186][135/233]	Loss 0.0003 (0.0038)	
training:	Epoch: [186][136/233]	Loss 0.0018 (0.0038)	
training:	Epoch: [186][137/233]	Loss 0.0077 (0.0039)	
training:	Epoch: [186][138/233]	Loss 0.0001 (0.0038)	
training:	Epoch: [186][139/233]	Loss 0.0002 (0.0038)	
training:	Epoch: [186][140/233]	Loss 0.0002 (0.0038)	
training:	Epoch: [186][141/233]	Loss 0.0002 (0.0037)	
training:	Epoch: [186][142/233]	Loss 0.0002 (0.0037)	
training:	Epoch: [186][143/233]	Loss 0.0003 (0.0037)	
training:	Epoch: [186][144/233]	Loss 0.0002 (0.0037)	
training:	Epoch: [186][145/233]	Loss 0.0005 (0.0037)	
training:	Epoch: [186][146/233]	Loss 0.0002 (0.0036)	
training:	Epoch: [186][147/233]	Loss 0.0011 (0.0036)	
training:	Epoch: [186][148/233]	Loss 0.0148 (0.0037)	
training:	Epoch: [186][149/233]	Loss 0.0006 (0.0037)	
training:	Epoch: [186][150/233]	Loss 0.0001 (0.0036)	
training:	Epoch: [186][151/233]	Loss 0.0126 (0.0037)	
training:	Epoch: [186][152/233]	Loss 0.0002 (0.0037)	
training:	Epoch: [186][153/233]	Loss 0.0984 (0.0043)	
training:	Epoch: [186][154/233]	Loss 0.0002 (0.0043)	
training:	Epoch: [186][155/233]	Loss 0.0001 (0.0042)	
training:	Epoch: [186][156/233]	Loss 0.0003 (0.0042)	
training:	Epoch: [186][157/233]	Loss 0.0002 (0.0042)	
training:	Epoch: [186][158/233]	Loss 0.0002 (0.0042)	
training:	Epoch: [186][159/233]	Loss 0.0001 (0.0041)	
training:	Epoch: [186][160/233]	Loss 0.0001 (0.0041)	
training:	Epoch: [186][161/233]	Loss 0.0002 (0.0041)	
training:	Epoch: [186][162/233]	Loss 0.0002 (0.0041)	
training:	Epoch: [186][163/233]	Loss 0.0001 (0.0040)	
training:	Epoch: [186][164/233]	Loss 0.0001 (0.0040)	
training:	Epoch: [186][165/233]	Loss 0.0001 (0.0040)	
training:	Epoch: [186][166/233]	Loss 0.0003 (0.0040)	
training:	Epoch: [186][167/233]	Loss 0.0029 (0.0040)	
training:	Epoch: [186][168/233]	Loss 0.0002 (0.0039)	
training:	Epoch: [186][169/233]	Loss 0.0002 (0.0039)	
training:	Epoch: [186][170/233]	Loss 0.0002 (0.0039)	
training:	Epoch: [186][171/233]	Loss 0.0011 (0.0039)	
training:	Epoch: [186][172/233]	Loss 0.0346 (0.0041)	
training:	Epoch: [186][173/233]	Loss 0.0002 (0.0040)	
training:	Epoch: [186][174/233]	Loss 0.0014 (0.0040)	
training:	Epoch: [186][175/233]	Loss 0.0002 (0.0040)	
training:	Epoch: [186][176/233]	Loss 0.0001 (0.0040)	
training:	Epoch: [186][177/233]	Loss 0.0002 (0.0040)	
training:	Epoch: [186][178/233]	Loss 0.0002 (0.0039)	
training:	Epoch: [186][179/233]	Loss 0.0005 (0.0039)	
training:	Epoch: [186][180/233]	Loss 0.0002 (0.0039)	
training:	Epoch: [186][181/233]	Loss 0.0003 (0.0039)	
training:	Epoch: [186][182/233]	Loss 0.1435 (0.0046)	
training:	Epoch: [186][183/233]	Loss 0.0003 (0.0046)	
training:	Epoch: [186][184/233]	Loss 0.0003 (0.0046)	
training:	Epoch: [186][185/233]	Loss 0.0004 (0.0046)	
training:	Epoch: [186][186/233]	Loss 0.0005 (0.0046)	
training:	Epoch: [186][187/233]	Loss 0.0001 (0.0045)	
training:	Epoch: [186][188/233]	Loss 0.0003 (0.0045)	
training:	Epoch: [186][189/233]	Loss 0.0009 (0.0045)	
training:	Epoch: [186][190/233]	Loss 0.0002 (0.0045)	
training:	Epoch: [186][191/233]	Loss 0.0109 (0.0045)	
training:	Epoch: [186][192/233]	Loss 0.0016 (0.0045)	
training:	Epoch: [186][193/233]	Loss 0.0002 (0.0045)	
training:	Epoch: [186][194/233]	Loss 0.0003 (0.0044)	
training:	Epoch: [186][195/233]	Loss 0.0002 (0.0044)	
training:	Epoch: [186][196/233]	Loss 0.0616 (0.0047)	
training:	Epoch: [186][197/233]	Loss 0.0003 (0.0047)	
training:	Epoch: [186][198/233]	Loss 0.0009 (0.0047)	
training:	Epoch: [186][199/233]	Loss 0.0004 (0.0046)	
training:	Epoch: [186][200/233]	Loss 0.0005 (0.0046)	
training:	Epoch: [186][201/233]	Loss 0.0001 (0.0046)	
training:	Epoch: [186][202/233]	Loss 0.0005 (0.0046)	
training:	Epoch: [186][203/233]	Loss 0.0002 (0.0046)	
training:	Epoch: [186][204/233]	Loss 0.0038 (0.0046)	
training:	Epoch: [186][205/233]	Loss 0.0002 (0.0045)	
training:	Epoch: [186][206/233]	Loss 0.0003 (0.0045)	
training:	Epoch: [186][207/233]	Loss 0.0001 (0.0045)	
training:	Epoch: [186][208/233]	Loss 0.0004 (0.0045)	
training:	Epoch: [186][209/233]	Loss 0.0002 (0.0045)	
training:	Epoch: [186][210/233]	Loss 0.0002 (0.0044)	
training:	Epoch: [186][211/233]	Loss 0.0003 (0.0044)	
training:	Epoch: [186][212/233]	Loss 0.0001 (0.0044)	
training:	Epoch: [186][213/233]	Loss 0.0092 (0.0044)	
training:	Epoch: [186][214/233]	Loss 0.0002 (0.0044)	
training:	Epoch: [186][215/233]	Loss 0.0019 (0.0044)	
training:	Epoch: [186][216/233]	Loss 0.0002 (0.0044)	
training:	Epoch: [186][217/233]	Loss 0.0005 (0.0044)	
training:	Epoch: [186][218/233]	Loss 0.0003 (0.0043)	
training:	Epoch: [186][219/233]	Loss 0.0001 (0.0043)	
training:	Epoch: [186][220/233]	Loss 0.0002 (0.0043)	
training:	Epoch: [186][221/233]	Loss 0.0048 (0.0043)	
training:	Epoch: [186][222/233]	Loss 0.0027 (0.0043)	
training:	Epoch: [186][223/233]	Loss 0.0041 (0.0043)	
training:	Epoch: [186][224/233]	Loss 0.0027 (0.0043)	
training:	Epoch: [186][225/233]	Loss 0.0001 (0.0043)	
training:	Epoch: [186][226/233]	Loss 0.0002 (0.0042)	
training:	Epoch: [186][227/233]	Loss 0.0004 (0.0042)	
training:	Epoch: [186][228/233]	Loss 0.0001 (0.0042)	
training:	Epoch: [186][229/233]	Loss 0.0003 (0.0042)	
training:	Epoch: [186][230/233]	Loss 0.0025 (0.0042)	
training:	Epoch: [186][231/233]	Loss 0.0026 (0.0042)	
training:	Epoch: [186][232/233]	Loss 0.0007 (0.0042)	
training:	Epoch: [186][233/233]	Loss 0.0002 (0.0041)	
Training:	 Loss: 0.0041

Training:	 ACC: 1.0000 1.0000 1.0000 1.0000
Validation:	 ACC: 0.7891 0.7881 0.7692 0.8090
Validation:	 Best_BACC: 0.8162 0.8154 0.7998 0.8326
Validation:	 Loss: 1.3818
Pretraining:	Epoch 187/200
----------
training:	Epoch: [187][1/233]	Loss 0.0002 (0.0002)	
training:	Epoch: [187][2/233]	Loss 0.0002 (0.0002)	
training:	Epoch: [187][3/233]	Loss 0.0002 (0.0002)	
training:	Epoch: [187][4/233]	Loss 0.0001 (0.0002)	
training:	Epoch: [187][5/233]	Loss 0.0001 (0.0002)	
training:	Epoch: [187][6/233]	Loss 0.0002 (0.0002)	
training:	Epoch: [187][7/233]	Loss 0.0002 (0.0002)	
training:	Epoch: [187][8/233]	Loss 0.0004 (0.0002)	
training:	Epoch: [187][9/233]	Loss 0.0017 (0.0004)	
training:	Epoch: [187][10/233]	Loss 0.0370 (0.0040)	
training:	Epoch: [187][11/233]	Loss 0.0003 (0.0037)	
training:	Epoch: [187][12/233]	Loss 0.0004 (0.0034)	
training:	Epoch: [187][13/233]	Loss 0.0021 (0.0033)	
training:	Epoch: [187][14/233]	Loss 0.0002 (0.0031)	
training:	Epoch: [187][15/233]	Loss 0.0002 (0.0029)	
training:	Epoch: [187][16/233]	Loss 0.0008 (0.0028)	
training:	Epoch: [187][17/233]	Loss 0.0002 (0.0026)	
training:	Epoch: [187][18/233]	Loss 0.0002 (0.0025)	
training:	Epoch: [187][19/233]	Loss 0.0006 (0.0024)	
training:	Epoch: [187][20/233]	Loss 0.0002 (0.0023)	
training:	Epoch: [187][21/233]	Loss 0.0002 (0.0022)	
training:	Epoch: [187][22/233]	Loss 0.0003 (0.0021)	
training:	Epoch: [187][23/233]	Loss 0.0002 (0.0020)	
training:	Epoch: [187][24/233]	Loss 0.0001 (0.0019)	
training:	Epoch: [187][25/233]	Loss 0.0002 (0.0019)	
training:	Epoch: [187][26/233]	Loss 0.0001 (0.0018)	
training:	Epoch: [187][27/233]	Loss 0.0002 (0.0017)	
training:	Epoch: [187][28/233]	Loss 0.0002 (0.0017)	
training:	Epoch: [187][29/233]	Loss 0.0129 (0.0021)	
training:	Epoch: [187][30/233]	Loss 0.0002 (0.0020)	
training:	Epoch: [187][31/233]	Loss 0.0004 (0.0020)	
training:	Epoch: [187][32/233]	Loss 0.0008 (0.0019)	
training:	Epoch: [187][33/233]	Loss 0.0008 (0.0019)	
training:	Epoch: [187][34/233]	Loss 0.0004 (0.0018)	
training:	Epoch: [187][35/233]	Loss 0.0001 (0.0018)	
training:	Epoch: [187][36/233]	Loss 0.0026 (0.0018)	
training:	Epoch: [187][37/233]	Loss 0.0074 (0.0020)	
training:	Epoch: [187][38/233]	Loss 0.0002 (0.0019)	
training:	Epoch: [187][39/233]	Loss 0.0006 (0.0019)	
training:	Epoch: [187][40/233]	Loss 0.0309 (0.0026)	
training:	Epoch: [187][41/233]	Loss 0.0326 (0.0033)	
training:	Epoch: [187][42/233]	Loss 0.0001 (0.0033)	
training:	Epoch: [187][43/233]	Loss 0.0003 (0.0032)	
training:	Epoch: [187][44/233]	Loss 0.0003 (0.0031)	
training:	Epoch: [187][45/233]	Loss 0.0001 (0.0031)	
training:	Epoch: [187][46/233]	Loss 0.0100 (0.0032)	
training:	Epoch: [187][47/233]	Loss 0.0001 (0.0032)	
training:	Epoch: [187][48/233]	Loss 0.0004 (0.0031)	
training:	Epoch: [187][49/233]	Loss 0.0002 (0.0030)	
training:	Epoch: [187][50/233]	Loss 0.0003 (0.0030)	
training:	Epoch: [187][51/233]	Loss 0.0002 (0.0029)	
training:	Epoch: [187][52/233]	Loss 0.0008 (0.0029)	
training:	Epoch: [187][53/233]	Loss 0.0002 (0.0028)	
training:	Epoch: [187][54/233]	Loss 0.0002 (0.0028)	
training:	Epoch: [187][55/233]	Loss 0.0001 (0.0027)	
training:	Epoch: [187][56/233]	Loss 0.0014 (0.0027)	
training:	Epoch: [187][57/233]	Loss 0.0002 (0.0027)	
training:	Epoch: [187][58/233]	Loss 0.0001 (0.0026)	
training:	Epoch: [187][59/233]	Loss 0.0001 (0.0026)	
training:	Epoch: [187][60/233]	Loss 0.0002 (0.0025)	
training:	Epoch: [187][61/233]	Loss 0.0002 (0.0025)	
training:	Epoch: [187][62/233]	Loss 0.0001 (0.0025)	
training:	Epoch: [187][63/233]	Loss 0.0002 (0.0024)	
training:	Epoch: [187][64/233]	Loss 0.0001 (0.0024)	
training:	Epoch: [187][65/233]	Loss 0.0002 (0.0024)	
training:	Epoch: [187][66/233]	Loss 0.0002 (0.0023)	
training:	Epoch: [187][67/233]	Loss 0.0001 (0.0023)	
training:	Epoch: [187][68/233]	Loss 0.0002 (0.0023)	
training:	Epoch: [187][69/233]	Loss 0.0001 (0.0022)	
training:	Epoch: [187][70/233]	Loss 0.0002 (0.0022)	
training:	Epoch: [187][71/233]	Loss 0.0005 (0.0022)	
training:	Epoch: [187][72/233]	Loss 0.0008 (0.0022)	
training:	Epoch: [187][73/233]	Loss 0.0325 (0.0026)	
training:	Epoch: [187][74/233]	Loss 0.0002 (0.0025)	
training:	Epoch: [187][75/233]	Loss 0.0002 (0.0025)	
training:	Epoch: [187][76/233]	Loss 0.0011 (0.0025)	
training:	Epoch: [187][77/233]	Loss 0.0001 (0.0025)	
training:	Epoch: [187][78/233]	Loss 0.0001 (0.0024)	
training:	Epoch: [187][79/233]	Loss 0.0007 (0.0024)	
training:	Epoch: [187][80/233]	Loss 0.0026 (0.0024)	
training:	Epoch: [187][81/233]	Loss 0.0002 (0.0024)	
training:	Epoch: [187][82/233]	Loss 0.0005 (0.0024)	
training:	Epoch: [187][83/233]	Loss 0.0012 (0.0023)	
training:	Epoch: [187][84/233]	Loss 0.0001 (0.0023)	
training:	Epoch: [187][85/233]	Loss 0.0003 (0.0023)	
training:	Epoch: [187][86/233]	Loss 0.0038 (0.0023)	
training:	Epoch: [187][87/233]	Loss 0.0004 (0.0023)	
training:	Epoch: [187][88/233]	Loss 0.0004 (0.0023)	
training:	Epoch: [187][89/233]	Loss 0.0001 (0.0022)	
training:	Epoch: [187][90/233]	Loss 0.0005 (0.0022)	
training:	Epoch: [187][91/233]	Loss 0.0131 (0.0023)	
training:	Epoch: [187][92/233]	Loss 0.0001 (0.0023)	
training:	Epoch: [187][93/233]	Loss 0.0003 (0.0023)	
training:	Epoch: [187][94/233]	Loss 0.0002 (0.0023)	
training:	Epoch: [187][95/233]	Loss 0.0011 (0.0023)	
training:	Epoch: [187][96/233]	Loss 0.0001 (0.0022)	
training:	Epoch: [187][97/233]	Loss 0.0004 (0.0022)	
training:	Epoch: [187][98/233]	Loss 0.0004 (0.0022)	
training:	Epoch: [187][99/233]	Loss 0.0001 (0.0022)	
training:	Epoch: [187][100/233]	Loss 0.0002 (0.0022)	
training:	Epoch: [187][101/233]	Loss 0.0004 (0.0021)	
training:	Epoch: [187][102/233]	Loss 0.0034 (0.0022)	
training:	Epoch: [187][103/233]	Loss 0.0002 (0.0021)	
training:	Epoch: [187][104/233]	Loss 0.0001 (0.0021)	
training:	Epoch: [187][105/233]	Loss 0.0002 (0.0021)	
training:	Epoch: [187][106/233]	Loss 0.0208 (0.0023)	
training:	Epoch: [187][107/233]	Loss 0.0002 (0.0023)	
training:	Epoch: [187][108/233]	Loss 0.0017 (0.0023)	
training:	Epoch: [187][109/233]	Loss 0.0001 (0.0022)	
training:	Epoch: [187][110/233]	Loss 0.0001 (0.0022)	
training:	Epoch: [187][111/233]	Loss 0.0001 (0.0022)	
training:	Epoch: [187][112/233]	Loss 0.0079 (0.0022)	
training:	Epoch: [187][113/233]	Loss 0.0001 (0.0022)	
training:	Epoch: [187][114/233]	Loss 0.0003 (0.0022)	
training:	Epoch: [187][115/233]	Loss 0.0018 (0.0022)	
training:	Epoch: [187][116/233]	Loss 0.0007 (0.0022)	
training:	Epoch: [187][117/233]	Loss 0.0002 (0.0022)	
training:	Epoch: [187][118/233]	Loss 0.0002 (0.0022)	
training:	Epoch: [187][119/233]	Loss 0.0003 (0.0021)	
training:	Epoch: [187][120/233]	Loss 0.0003 (0.0021)	
training:	Epoch: [187][121/233]	Loss 0.0002 (0.0021)	
training:	Epoch: [187][122/233]	Loss 0.0002 (0.0021)	
training:	Epoch: [187][123/233]	Loss 0.0005 (0.0021)	
training:	Epoch: [187][124/233]	Loss 0.0002 (0.0021)	
training:	Epoch: [187][125/233]	Loss 0.0001 (0.0021)	
training:	Epoch: [187][126/233]	Loss 0.0001 (0.0020)	
training:	Epoch: [187][127/233]	Loss 0.0001 (0.0020)	
training:	Epoch: [187][128/233]	Loss 0.0001 (0.0020)	
training:	Epoch: [187][129/233]	Loss 0.0001 (0.0020)	
training:	Epoch: [187][130/233]	Loss 0.0001 (0.0020)	
training:	Epoch: [187][131/233]	Loss 0.0002 (0.0020)	
training:	Epoch: [187][132/233]	Loss 0.0002 (0.0020)	
training:	Epoch: [187][133/233]	Loss 0.0002 (0.0019)	
training:	Epoch: [187][134/233]	Loss 0.0006 (0.0019)	
training:	Epoch: [187][135/233]	Loss 0.0001 (0.0019)	
training:	Epoch: [187][136/233]	Loss 0.0001 (0.0019)	
training:	Epoch: [187][137/233]	Loss 0.0002 (0.0019)	
training:	Epoch: [187][138/233]	Loss 0.0002 (0.0019)	
training:	Epoch: [187][139/233]	Loss 0.0001 (0.0019)	
training:	Epoch: [187][140/233]	Loss 0.0001 (0.0019)	
training:	Epoch: [187][141/233]	Loss 0.0001 (0.0018)	
training:	Epoch: [187][142/233]	Loss 0.0002 (0.0018)	
training:	Epoch: [187][143/233]	Loss 0.0003 (0.0018)	
training:	Epoch: [187][144/233]	Loss 0.0003 (0.0018)	
training:	Epoch: [187][145/233]	Loss 0.0001 (0.0018)	
training:	Epoch: [187][146/233]	Loss 0.0003 (0.0018)	
training:	Epoch: [187][147/233]	Loss 0.0002 (0.0018)	
training:	Epoch: [187][148/233]	Loss 0.0002 (0.0018)	
training:	Epoch: [187][149/233]	Loss 0.0002 (0.0018)	
training:	Epoch: [187][150/233]	Loss 0.0001 (0.0017)	
training:	Epoch: [187][151/233]	Loss 0.0001 (0.0017)	
training:	Epoch: [187][152/233]	Loss 0.0007 (0.0017)	
training:	Epoch: [187][153/233]	Loss 0.0001 (0.0017)	
training:	Epoch: [187][154/233]	Loss 0.0108 (0.0018)	
training:	Epoch: [187][155/233]	Loss 0.0001 (0.0018)	
training:	Epoch: [187][156/233]	Loss 0.0004 (0.0018)	
training:	Epoch: [187][157/233]	Loss 0.0001 (0.0017)	
training:	Epoch: [187][158/233]	Loss 0.0001 (0.0017)	
training:	Epoch: [187][159/233]	Loss 0.0001 (0.0017)	
training:	Epoch: [187][160/233]	Loss 0.0001 (0.0017)	
training:	Epoch: [187][161/233]	Loss 0.0002 (0.0017)	
training:	Epoch: [187][162/233]	Loss 0.0001 (0.0017)	
training:	Epoch: [187][163/233]	Loss 0.0001 (0.0017)	
training:	Epoch: [187][164/233]	Loss 0.0001 (0.0017)	
training:	Epoch: [187][165/233]	Loss 0.0005 (0.0017)	
training:	Epoch: [187][166/233]	Loss 0.0023 (0.0017)	
training:	Epoch: [187][167/233]	Loss 0.0006 (0.0017)	
training:	Epoch: [187][168/233]	Loss 0.0001 (0.0017)	
training:	Epoch: [187][169/233]	Loss 0.0002 (0.0016)	
training:	Epoch: [187][170/233]	Loss 0.0001 (0.0016)	
training:	Epoch: [187][171/233]	Loss 0.0001 (0.0016)	
training:	Epoch: [187][172/233]	Loss 0.0001 (0.0016)	
training:	Epoch: [187][173/233]	Loss 0.0006 (0.0016)	
training:	Epoch: [187][174/233]	Loss 0.0002 (0.0016)	
training:	Epoch: [187][175/233]	Loss 0.0001 (0.0016)	
training:	Epoch: [187][176/233]	Loss 0.0001 (0.0016)	
training:	Epoch: [187][177/233]	Loss 0.0001 (0.0016)	
training:	Epoch: [187][178/233]	Loss 0.0006 (0.0016)	
training:	Epoch: [187][179/233]	Loss 0.0001 (0.0016)	
training:	Epoch: [187][180/233]	Loss 0.0006 (0.0016)	
training:	Epoch: [187][181/233]	Loss 0.0001 (0.0016)	
training:	Epoch: [187][182/233]	Loss 0.0003 (0.0015)	
training:	Epoch: [187][183/233]	Loss 0.0041 (0.0016)	
training:	Epoch: [187][184/233]	Loss 0.0002 (0.0016)	
training:	Epoch: [187][185/233]	Loss 0.0002 (0.0015)	
training:	Epoch: [187][186/233]	Loss 0.0003 (0.0015)	
training:	Epoch: [187][187/233]	Loss 0.0002 (0.0015)	
training:	Epoch: [187][188/233]	Loss 0.0002 (0.0015)	
training:	Epoch: [187][189/233]	Loss 0.0018 (0.0015)	
training:	Epoch: [187][190/233]	Loss 0.0001 (0.0015)	
training:	Epoch: [187][191/233]	Loss 0.0001 (0.0015)	
training:	Epoch: [187][192/233]	Loss 0.0001 (0.0015)	
training:	Epoch: [187][193/233]	Loss 0.0001 (0.0015)	
training:	Epoch: [187][194/233]	Loss 0.0001 (0.0015)	
training:	Epoch: [187][195/233]	Loss 0.0002 (0.0015)	
training:	Epoch: [187][196/233]	Loss 0.0001 (0.0015)	
training:	Epoch: [187][197/233]	Loss 0.0001 (0.0015)	
training:	Epoch: [187][198/233]	Loss 0.0001 (0.0015)	
training:	Epoch: [187][199/233]	Loss 0.0113 (0.0015)	
training:	Epoch: [187][200/233]	Loss 0.0001 (0.0015)	
training:	Epoch: [187][201/233]	Loss 0.0001 (0.0015)	
training:	Epoch: [187][202/233]	Loss 0.0003 (0.0015)	
training:	Epoch: [187][203/233]	Loss 0.0001 (0.0015)	
training:	Epoch: [187][204/233]	Loss 0.0010 (0.0015)	
training:	Epoch: [187][205/233]	Loss 0.0003 (0.0015)	
training:	Epoch: [187][206/233]	Loss 0.0001 (0.0015)	
training:	Epoch: [187][207/233]	Loss 0.0002 (0.0015)	
training:	Epoch: [187][208/233]	Loss 0.0005 (0.0015)	
training:	Epoch: [187][209/233]	Loss 0.0003 (0.0015)	
training:	Epoch: [187][210/233]	Loss 0.0001 (0.0015)	
training:	Epoch: [187][211/233]	Loss 0.0003 (0.0014)	
training:	Epoch: [187][212/233]	Loss 0.0013 (0.0014)	
training:	Epoch: [187][213/233]	Loss 0.0001 (0.0014)	
training:	Epoch: [187][214/233]	Loss 0.0002 (0.0014)	
training:	Epoch: [187][215/233]	Loss 0.0001 (0.0014)	
training:	Epoch: [187][216/233]	Loss 0.0001 (0.0014)	
training:	Epoch: [187][217/233]	Loss 0.0001 (0.0014)	
training:	Epoch: [187][218/233]	Loss 0.0001 (0.0014)	
training:	Epoch: [187][219/233]	Loss 0.0003 (0.0014)	
training:	Epoch: [187][220/233]	Loss 0.0001 (0.0014)	
training:	Epoch: [187][221/233]	Loss 0.0002 (0.0014)	
training:	Epoch: [187][222/233]	Loss 0.0002 (0.0014)	
training:	Epoch: [187][223/233]	Loss 0.0002 (0.0014)	
training:	Epoch: [187][224/233]	Loss 0.0089 (0.0014)	
training:	Epoch: [187][225/233]	Loss 0.0001 (0.0014)	
training:	Epoch: [187][226/233]	Loss 0.0335 (0.0016)	
training:	Epoch: [187][227/233]	Loss 0.0012 (0.0015)	
training:	Epoch: [187][228/233]	Loss 0.0002 (0.0015)	
training:	Epoch: [187][229/233]	Loss 0.0001 (0.0015)	
training:	Epoch: [187][230/233]	Loss 0.0002 (0.0015)	
training:	Epoch: [187][231/233]	Loss 0.0001 (0.0015)	
training:	Epoch: [187][232/233]	Loss 0.0002 (0.0015)	
training:	Epoch: [187][233/233]	Loss 0.0002 (0.0015)	
Training:	 Loss: 0.0015

Training:	 ACC: 1.0000 1.0000 1.0000 1.0000
Validation:	 ACC: 0.7944 0.7940 0.7855 0.8034
Validation:	 Best_BACC: 0.8162 0.8154 0.7998 0.8326
Validation:	 Loss: 1.4213
Pretraining:	Epoch 188/200
----------
training:	Epoch: [188][1/233]	Loss 0.0002 (0.0002)	
training:	Epoch: [188][2/233]	Loss 0.0014 (0.0008)	
training:	Epoch: [188][3/233]	Loss 0.0002 (0.0006)	
training:	Epoch: [188][4/233]	Loss 0.0001 (0.0005)	
training:	Epoch: [188][5/233]	Loss 0.0010 (0.0006)	
training:	Epoch: [188][6/233]	Loss 0.0001 (0.0005)	
training:	Epoch: [188][7/233]	Loss 0.0001 (0.0004)	
training:	Epoch: [188][8/233]	Loss 0.0002 (0.0004)	
training:	Epoch: [188][9/233]	Loss 0.0001 (0.0004)	
training:	Epoch: [188][10/233]	Loss 0.0002 (0.0004)	
training:	Epoch: [188][11/233]	Loss 0.0001 (0.0003)	
training:	Epoch: [188][12/233]	Loss 0.0001 (0.0003)	
training:	Epoch: [188][13/233]	Loss 0.0004 (0.0003)	
training:	Epoch: [188][14/233]	Loss 0.0001 (0.0003)	
training:	Epoch: [188][15/233]	Loss 0.0002 (0.0003)	
training:	Epoch: [188][16/233]	Loss 0.0001 (0.0003)	
training:	Epoch: [188][17/233]	Loss 0.0001 (0.0003)	
training:	Epoch: [188][18/233]	Loss 0.0002 (0.0003)	
training:	Epoch: [188][19/233]	Loss 0.0002 (0.0003)	
training:	Epoch: [188][20/233]	Loss 0.0002 (0.0003)	
training:	Epoch: [188][21/233]	Loss 0.0003 (0.0003)	
training:	Epoch: [188][22/233]	Loss 0.0033 (0.0004)	
training:	Epoch: [188][23/233]	Loss 0.0001 (0.0004)	
training:	Epoch: [188][24/233]	Loss 0.0001 (0.0004)	
training:	Epoch: [188][25/233]	Loss 0.0002 (0.0004)	
training:	Epoch: [188][26/233]	Loss 0.0003 (0.0004)	
training:	Epoch: [188][27/233]	Loss 0.0003 (0.0004)	
training:	Epoch: [188][28/233]	Loss 0.0001 (0.0004)	
training:	Epoch: [188][29/233]	Loss 0.0003 (0.0004)	
training:	Epoch: [188][30/233]	Loss 0.0001 (0.0004)	
training:	Epoch: [188][31/233]	Loss 0.0001 (0.0003)	
training:	Epoch: [188][32/233]	Loss 0.0001 (0.0003)	
training:	Epoch: [188][33/233]	Loss 0.0001 (0.0003)	
training:	Epoch: [188][34/233]	Loss 0.0001 (0.0003)	
training:	Epoch: [188][35/233]	Loss 0.0001 (0.0003)	
training:	Epoch: [188][36/233]	Loss 0.0002 (0.0003)	
training:	Epoch: [188][37/233]	Loss 0.0001 (0.0003)	
training:	Epoch: [188][38/233]	Loss 0.0013 (0.0003)	
training:	Epoch: [188][39/233]	Loss 0.0001 (0.0003)	
training:	Epoch: [188][40/233]	Loss 0.0003 (0.0003)	
training:	Epoch: [188][41/233]	Loss 0.0002 (0.0003)	
training:	Epoch: [188][42/233]	Loss 0.0007 (0.0003)	
training:	Epoch: [188][43/233]	Loss 0.0002 (0.0003)	
training:	Epoch: [188][44/233]	Loss 0.0022 (0.0004)	
training:	Epoch: [188][45/233]	Loss 0.0001 (0.0004)	
training:	Epoch: [188][46/233]	Loss 0.0001 (0.0004)	
training:	Epoch: [188][47/233]	Loss 0.0001 (0.0004)	
training:	Epoch: [188][48/233]	Loss 0.0001 (0.0004)	
training:	Epoch: [188][49/233]	Loss 0.0001 (0.0003)	
training:	Epoch: [188][50/233]	Loss 0.0002 (0.0003)	
training:	Epoch: [188][51/233]	Loss 0.0002 (0.0003)	
training:	Epoch: [188][52/233]	Loss 0.0352 (0.0010)	
training:	Epoch: [188][53/233]	Loss 0.0002 (0.0010)	
training:	Epoch: [188][54/233]	Loss 0.0003 (0.0010)	
training:	Epoch: [188][55/233]	Loss 0.0002 (0.0010)	
training:	Epoch: [188][56/233]	Loss 0.0001 (0.0010)	
training:	Epoch: [188][57/233]	Loss 0.0003 (0.0009)	
training:	Epoch: [188][58/233]	Loss 0.0001 (0.0009)	
training:	Epoch: [188][59/233]	Loss 0.0001 (0.0009)	
training:	Epoch: [188][60/233]	Loss 0.0002 (0.0009)	
training:	Epoch: [188][61/233]	Loss 0.0003 (0.0009)	
training:	Epoch: [188][62/233]	Loss 0.0001 (0.0009)	
training:	Epoch: [188][63/233]	Loss 0.0001 (0.0009)	
training:	Epoch: [188][64/233]	Loss 0.0006 (0.0009)	
training:	Epoch: [188][65/233]	Loss 0.0008 (0.0009)	
training:	Epoch: [188][66/233]	Loss 0.0001 (0.0009)	
training:	Epoch: [188][67/233]	Loss 0.0001 (0.0008)	
training:	Epoch: [188][68/233]	Loss 0.0008 (0.0008)	
training:	Epoch: [188][69/233]	Loss 0.0001 (0.0008)	
training:	Epoch: [188][70/233]	Loss 0.0001 (0.0008)	
training:	Epoch: [188][71/233]	Loss 0.0002 (0.0008)	
training:	Epoch: [188][72/233]	Loss 0.0002 (0.0008)	
training:	Epoch: [188][73/233]	Loss 0.0001 (0.0008)	
training:	Epoch: [188][74/233]	Loss 0.0001 (0.0008)	
training:	Epoch: [188][75/233]	Loss 0.0001 (0.0008)	
training:	Epoch: [188][76/233]	Loss 0.0001 (0.0008)	
training:	Epoch: [188][77/233]	Loss 0.0001 (0.0008)	
training:	Epoch: [188][78/233]	Loss 0.0001 (0.0007)	
training:	Epoch: [188][79/233]	Loss 0.0153 (0.0009)	
training:	Epoch: [188][80/233]	Loss 0.0001 (0.0009)	
training:	Epoch: [188][81/233]	Loss 0.0002 (0.0009)	
training:	Epoch: [188][82/233]	Loss 0.0001 (0.0009)	
training:	Epoch: [188][83/233]	Loss 0.0004 (0.0009)	
training:	Epoch: [188][84/233]	Loss 0.0001 (0.0009)	
training:	Epoch: [188][85/233]	Loss 0.0016 (0.0009)	
training:	Epoch: [188][86/233]	Loss 0.0003 (0.0009)	
training:	Epoch: [188][87/233]	Loss 0.0003 (0.0009)	
training:	Epoch: [188][88/233]	Loss 0.0002 (0.0009)	
training:	Epoch: [188][89/233]	Loss 0.0010 (0.0009)	
training:	Epoch: [188][90/233]	Loss 0.0001 (0.0009)	
training:	Epoch: [188][91/233]	Loss 0.0002 (0.0009)	
training:	Epoch: [188][92/233]	Loss 0.0001 (0.0009)	
training:	Epoch: [188][93/233]	Loss 0.0001 (0.0008)	
training:	Epoch: [188][94/233]	Loss 0.0004 (0.0008)	
training:	Epoch: [188][95/233]	Loss 0.0001 (0.0008)	
training:	Epoch: [188][96/233]	Loss 0.0001 (0.0008)	
training:	Epoch: [188][97/233]	Loss 0.0003 (0.0008)	
training:	Epoch: [188][98/233]	Loss 0.0001 (0.0008)	
training:	Epoch: [188][99/233]	Loss 0.0001 (0.0008)	
training:	Epoch: [188][100/233]	Loss 0.0053 (0.0009)	
training:	Epoch: [188][101/233]	Loss 0.0003 (0.0008)	
training:	Epoch: [188][102/233]	Loss 0.0013 (0.0009)	
training:	Epoch: [188][103/233]	Loss 0.0002 (0.0008)	
training:	Epoch: [188][104/233]	Loss 0.0002 (0.0008)	
training:	Epoch: [188][105/233]	Loss 0.0001 (0.0008)	
training:	Epoch: [188][106/233]	Loss 0.0002 (0.0008)	
training:	Epoch: [188][107/233]	Loss 0.0002 (0.0008)	
training:	Epoch: [188][108/233]	Loss 0.0001 (0.0008)	
training:	Epoch: [188][109/233]	Loss 0.0001 (0.0008)	
training:	Epoch: [188][110/233]	Loss 0.0001 (0.0008)	
training:	Epoch: [188][111/233]	Loss 0.0001 (0.0008)	
training:	Epoch: [188][112/233]	Loss 0.0001 (0.0008)	
training:	Epoch: [188][113/233]	Loss 0.0002 (0.0008)	
training:	Epoch: [188][114/233]	Loss 0.0001 (0.0008)	
training:	Epoch: [188][115/233]	Loss 0.0006 (0.0008)	
training:	Epoch: [188][116/233]	Loss 0.0001 (0.0008)	
training:	Epoch: [188][117/233]	Loss 0.0002 (0.0008)	
training:	Epoch: [188][118/233]	Loss 0.0010 (0.0008)	
training:	Epoch: [188][119/233]	Loss 0.0001 (0.0008)	
training:	Epoch: [188][120/233]	Loss 0.0001 (0.0008)	
training:	Epoch: [188][121/233]	Loss 0.0002 (0.0008)	
training:	Epoch: [188][122/233]	Loss 0.0001 (0.0007)	
training:	Epoch: [188][123/233]	Loss 0.0002 (0.0007)	
training:	Epoch: [188][124/233]	Loss 0.0095 (0.0008)	
training:	Epoch: [188][125/233]	Loss 0.0001 (0.0008)	
training:	Epoch: [188][126/233]	Loss 0.0002 (0.0008)	
training:	Epoch: [188][127/233]	Loss 0.0033 (0.0008)	
training:	Epoch: [188][128/233]	Loss 0.0001 (0.0008)	
training:	Epoch: [188][129/233]	Loss 0.0001 (0.0008)	
training:	Epoch: [188][130/233]	Loss 0.0001 (0.0008)	
training:	Epoch: [188][131/233]	Loss 0.0134 (0.0009)	
training:	Epoch: [188][132/233]	Loss 0.0006 (0.0009)	
training:	Epoch: [188][133/233]	Loss 0.0001 (0.0009)	
training:	Epoch: [188][134/233]	Loss 0.0002 (0.0009)	
training:	Epoch: [188][135/233]	Loss 0.0001 (0.0009)	
training:	Epoch: [188][136/233]	Loss 0.0001 (0.0009)	
training:	Epoch: [188][137/233]	Loss 0.0001 (0.0009)	
training:	Epoch: [188][138/233]	Loss 0.0001 (0.0009)	
training:	Epoch: [188][139/233]	Loss 0.0001 (0.0009)	
training:	Epoch: [188][140/233]	Loss 0.0001 (0.0009)	
training:	Epoch: [188][141/233]	Loss 0.0002 (0.0009)	
training:	Epoch: [188][142/233]	Loss 0.0002 (0.0008)	
training:	Epoch: [188][143/233]	Loss 0.0001 (0.0008)	
training:	Epoch: [188][144/233]	Loss 0.0028 (0.0009)	
training:	Epoch: [188][145/233]	Loss 0.0002 (0.0009)	
training:	Epoch: [188][146/233]	Loss 0.0005 (0.0008)	
training:	Epoch: [188][147/233]	Loss 0.0001 (0.0008)	
training:	Epoch: [188][148/233]	Loss 0.0001 (0.0008)	
training:	Epoch: [188][149/233]	Loss 0.0001 (0.0008)	
training:	Epoch: [188][150/233]	Loss 0.0002 (0.0008)	
training:	Epoch: [188][151/233]	Loss 0.0066 (0.0009)	
training:	Epoch: [188][152/233]	Loss 0.0001 (0.0009)	
training:	Epoch: [188][153/233]	Loss 0.0002 (0.0009)	
training:	Epoch: [188][154/233]	Loss 0.0002 (0.0009)	
training:	Epoch: [188][155/233]	Loss 0.0932 (0.0014)	
training:	Epoch: [188][156/233]	Loss 0.0001 (0.0014)	
training:	Epoch: [188][157/233]	Loss 0.0001 (0.0014)	
training:	Epoch: [188][158/233]	Loss 0.0001 (0.0014)	
training:	Epoch: [188][159/233]	Loss 0.0001 (0.0014)	
training:	Epoch: [188][160/233]	Loss 0.0001 (0.0014)	
training:	Epoch: [188][161/233]	Loss 0.0001 (0.0014)	
training:	Epoch: [188][162/233]	Loss 0.0001 (0.0014)	
training:	Epoch: [188][163/233]	Loss 0.0001 (0.0014)	
training:	Epoch: [188][164/233]	Loss 0.0025 (0.0014)	
training:	Epoch: [188][165/233]	Loss 0.0003 (0.0014)	
training:	Epoch: [188][166/233]	Loss 0.0002 (0.0014)	
training:	Epoch: [188][167/233]	Loss 0.0003 (0.0014)	
training:	Epoch: [188][168/233]	Loss 0.0001 (0.0014)	
training:	Epoch: [188][169/233]	Loss 0.0001 (0.0014)	
training:	Epoch: [188][170/233]	Loss 0.0001 (0.0013)	
training:	Epoch: [188][171/233]	Loss 0.0001 (0.0013)	
training:	Epoch: [188][172/233]	Loss 0.0004 (0.0013)	
training:	Epoch: [188][173/233]	Loss 0.0002 (0.0013)	
training:	Epoch: [188][174/233]	Loss 0.0002 (0.0013)	
training:	Epoch: [188][175/233]	Loss 0.0001 (0.0013)	
training:	Epoch: [188][176/233]	Loss 0.0002 (0.0013)	
training:	Epoch: [188][177/233]	Loss 0.0001 (0.0013)	
training:	Epoch: [188][178/233]	Loss 0.0025 (0.0013)	
training:	Epoch: [188][179/233]	Loss 0.0002 (0.0013)	
training:	Epoch: [188][180/233]	Loss 0.0230 (0.0014)	
training:	Epoch: [188][181/233]	Loss 0.0002 (0.0014)	
training:	Epoch: [188][182/233]	Loss 0.0002 (0.0014)	
training:	Epoch: [188][183/233]	Loss 0.0001 (0.0014)	
training:	Epoch: [188][184/233]	Loss 0.0001 (0.0014)	
training:	Epoch: [188][185/233]	Loss 0.0002 (0.0014)	
training:	Epoch: [188][186/233]	Loss 0.0001 (0.0014)	
training:	Epoch: [188][187/233]	Loss 0.0001 (0.0014)	
training:	Epoch: [188][188/233]	Loss 0.0001 (0.0014)	
training:	Epoch: [188][189/233]	Loss 0.0001 (0.0014)	
training:	Epoch: [188][190/233]	Loss 0.0006 (0.0014)	
training:	Epoch: [188][191/233]	Loss 0.0020 (0.0014)	
training:	Epoch: [188][192/233]	Loss 0.0001 (0.0014)	
training:	Epoch: [188][193/233]	Loss 0.0006 (0.0014)	
training:	Epoch: [188][194/233]	Loss 0.0001 (0.0013)	
training:	Epoch: [188][195/233]	Loss 0.0001 (0.0013)	
training:	Epoch: [188][196/233]	Loss 0.0001 (0.0013)	
training:	Epoch: [188][197/233]	Loss 0.0001 (0.0013)	
training:	Epoch: [188][198/233]	Loss 0.0002 (0.0013)	
training:	Epoch: [188][199/233]	Loss 0.0001 (0.0013)	
training:	Epoch: [188][200/233]	Loss 0.0020 (0.0013)	
training:	Epoch: [188][201/233]	Loss 0.0002 (0.0013)	
training:	Epoch: [188][202/233]	Loss 0.0002 (0.0013)	
training:	Epoch: [188][203/233]	Loss 0.0002 (0.0013)	
training:	Epoch: [188][204/233]	Loss 0.0002 (0.0013)	
training:	Epoch: [188][205/233]	Loss 0.0001 (0.0013)	
training:	Epoch: [188][206/233]	Loss 0.0003 (0.0013)	
training:	Epoch: [188][207/233]	Loss 0.0002 (0.0013)	
training:	Epoch: [188][208/233]	Loss 0.0001 (0.0013)	
training:	Epoch: [188][209/233]	Loss 0.0002 (0.0013)	
training:	Epoch: [188][210/233]	Loss 0.0001 (0.0013)	
training:	Epoch: [188][211/233]	Loss 0.0002 (0.0013)	
training:	Epoch: [188][212/233]	Loss 0.0001 (0.0013)	
training:	Epoch: [188][213/233]	Loss 0.0001 (0.0012)	
training:	Epoch: [188][214/233]	Loss 0.0001 (0.0012)	
training:	Epoch: [188][215/233]	Loss 0.0001 (0.0012)	
training:	Epoch: [188][216/233]	Loss 0.0002 (0.0012)	
training:	Epoch: [188][217/233]	Loss 0.0003 (0.0012)	
training:	Epoch: [188][218/233]	Loss 0.0001 (0.0012)	
training:	Epoch: [188][219/233]	Loss 0.0028 (0.0012)	
training:	Epoch: [188][220/233]	Loss 0.0002 (0.0012)	
training:	Epoch: [188][221/233]	Loss 0.0001 (0.0012)	
training:	Epoch: [188][222/233]	Loss 0.0001 (0.0012)	
training:	Epoch: [188][223/233]	Loss 0.0005 (0.0012)	
training:	Epoch: [188][224/233]	Loss 0.0002 (0.0012)	
training:	Epoch: [188][225/233]	Loss 0.0001 (0.0012)	
training:	Epoch: [188][226/233]	Loss 0.0001 (0.0012)	
training:	Epoch: [188][227/233]	Loss 0.0002 (0.0012)	
training:	Epoch: [188][228/233]	Loss 0.0002 (0.0012)	
training:	Epoch: [188][229/233]	Loss 0.0001 (0.0012)	
training:	Epoch: [188][230/233]	Loss 0.0013 (0.0012)	
training:	Epoch: [188][231/233]	Loss 0.0002 (0.0012)	
training:	Epoch: [188][232/233]	Loss 0.0001 (0.0012)	
training:	Epoch: [188][233/233]	Loss 0.0001 (0.0012)	
Training:	 Loss: 0.0012

Training:	 ACC: 1.0000 1.0000 1.0000 1.0000
Validation:	 ACC: 0.7906 0.7897 0.7732 0.8079
Validation:	 Best_BACC: 0.8162 0.8154 0.7998 0.8326
Validation:	 Loss: 1.4288
Pretraining:	Epoch 189/200
----------
training:	Epoch: [189][1/233]	Loss 0.0001 (0.0001)	
training:	Epoch: [189][2/233]	Loss 0.0005 (0.0003)	
training:	Epoch: [189][3/233]	Loss 0.0001 (0.0003)	
training:	Epoch: [189][4/233]	Loss 0.0001 (0.0002)	
training:	Epoch: [189][5/233]	Loss 0.0002 (0.0002)	
training:	Epoch: [189][6/233]	Loss 0.0002 (0.0002)	
training:	Epoch: [189][7/233]	Loss 0.0002 (0.0002)	
training:	Epoch: [189][8/233]	Loss 0.0001 (0.0002)	
training:	Epoch: [189][9/233]	Loss 0.0001 (0.0002)	
training:	Epoch: [189][10/233]	Loss 0.0001 (0.0002)	
training:	Epoch: [189][11/233]	Loss 0.0001 (0.0002)	
training:	Epoch: [189][12/233]	Loss 0.0001 (0.0002)	
training:	Epoch: [189][13/233]	Loss 0.0001 (0.0002)	
training:	Epoch: [189][14/233]	Loss 0.0004 (0.0002)	
training:	Epoch: [189][15/233]	Loss 0.0001 (0.0002)	
training:	Epoch: [189][16/233]	Loss 0.0003 (0.0002)	
training:	Epoch: [189][17/233]	Loss 0.0002 (0.0002)	
training:	Epoch: [189][18/233]	Loss 0.0004 (0.0002)	
training:	Epoch: [189][19/233]	Loss 0.0001 (0.0002)	
training:	Epoch: [189][20/233]	Loss 0.0002 (0.0002)	
training:	Epoch: [189][21/233]	Loss 0.0002 (0.0002)	
training:	Epoch: [189][22/233]	Loss 0.0001 (0.0002)	
training:	Epoch: [189][23/233]	Loss 0.0001 (0.0002)	
training:	Epoch: [189][24/233]	Loss 0.0001 (0.0002)	
training:	Epoch: [189][25/233]	Loss 0.0003 (0.0002)	
training:	Epoch: [189][26/233]	Loss 0.0001 (0.0002)	
training:	Epoch: [189][27/233]	Loss 0.0003 (0.0002)	
training:	Epoch: [189][28/233]	Loss 0.0001 (0.0002)	
training:	Epoch: [189][29/233]	Loss 0.0001 (0.0002)	
training:	Epoch: [189][30/233]	Loss 0.0001 (0.0002)	
training:	Epoch: [189][31/233]	Loss 0.0001 (0.0002)	
training:	Epoch: [189][32/233]	Loss 0.0001 (0.0002)	
training:	Epoch: [189][33/233]	Loss 0.0004 (0.0002)	
training:	Epoch: [189][34/233]	Loss 0.0001 (0.0002)	
training:	Epoch: [189][35/233]	Loss 0.0004 (0.0002)	
training:	Epoch: [189][36/233]	Loss 0.0001 (0.0002)	
training:	Epoch: [189][37/233]	Loss 0.0001 (0.0002)	
training:	Epoch: [189][38/233]	Loss 0.0001 (0.0002)	
training:	Epoch: [189][39/233]	Loss 0.0001 (0.0002)	
training:	Epoch: [189][40/233]	Loss 0.0001 (0.0002)	
training:	Epoch: [189][41/233]	Loss 0.0002 (0.0002)	
training:	Epoch: [189][42/233]	Loss 0.0002 (0.0002)	
training:	Epoch: [189][43/233]	Loss 0.0001 (0.0002)	
training:	Epoch: [189][44/233]	Loss 0.0001 (0.0002)	
training:	Epoch: [189][45/233]	Loss 0.0010 (0.0002)	
training:	Epoch: [189][46/233]	Loss 0.0006 (0.0002)	
training:	Epoch: [189][47/233]	Loss 0.0001 (0.0002)	
training:	Epoch: [189][48/233]	Loss 0.0001 (0.0002)	
training:	Epoch: [189][49/233]	Loss 0.0001 (0.0002)	
training:	Epoch: [189][50/233]	Loss 0.0002 (0.0002)	
training:	Epoch: [189][51/233]	Loss 0.0001 (0.0002)	
training:	Epoch: [189][52/233]	Loss 0.0002 (0.0002)	
training:	Epoch: [189][53/233]	Loss 0.0001 (0.0002)	
training:	Epoch: [189][54/233]	Loss 0.0001 (0.0002)	
training:	Epoch: [189][55/233]	Loss 0.0001 (0.0002)	
training:	Epoch: [189][56/233]	Loss 0.0003 (0.0002)	
training:	Epoch: [189][57/233]	Loss 0.0002 (0.0002)	
training:	Epoch: [189][58/233]	Loss 0.0001 (0.0002)	
training:	Epoch: [189][59/233]	Loss 0.0001 (0.0002)	
training:	Epoch: [189][60/233]	Loss 0.0001 (0.0002)	
training:	Epoch: [189][61/233]	Loss 0.0004 (0.0002)	
training:	Epoch: [189][62/233]	Loss 0.0001 (0.0002)	
training:	Epoch: [189][63/233]	Loss 0.0001 (0.0002)	
training:	Epoch: [189][64/233]	Loss 0.0002 (0.0002)	
training:	Epoch: [189][65/233]	Loss 0.0002 (0.0002)	
training:	Epoch: [189][66/233]	Loss 0.0051 (0.0003)	
training:	Epoch: [189][67/233]	Loss 0.0001 (0.0003)	
training:	Epoch: [189][68/233]	Loss 0.0002 (0.0003)	
training:	Epoch: [189][69/233]	Loss 0.0002 (0.0003)	
training:	Epoch: [189][70/233]	Loss 0.0002 (0.0003)	
training:	Epoch: [189][71/233]	Loss 0.0758 (0.0013)	
training:	Epoch: [189][72/233]	Loss 0.0001 (0.0013)	
training:	Epoch: [189][73/233]	Loss 0.0002 (0.0013)	
training:	Epoch: [189][74/233]	Loss 0.0002 (0.0013)	
training:	Epoch: [189][75/233]	Loss 0.0003 (0.0013)	
training:	Epoch: [189][76/233]	Loss 0.0008 (0.0013)	
training:	Epoch: [189][77/233]	Loss 0.0008 (0.0013)	
training:	Epoch: [189][78/233]	Loss 0.0001 (0.0012)	
training:	Epoch: [189][79/233]	Loss 0.0002 (0.0012)	
training:	Epoch: [189][80/233]	Loss 0.0004 (0.0012)	
training:	Epoch: [189][81/233]	Loss 0.0692 (0.0021)	
training:	Epoch: [189][82/233]	Loss 0.0001 (0.0020)	
training:	Epoch: [189][83/233]	Loss 0.0001 (0.0020)	
training:	Epoch: [189][84/233]	Loss 0.0002 (0.0020)	
training:	Epoch: [189][85/233]	Loss 0.0001 (0.0020)	
training:	Epoch: [189][86/233]	Loss 0.0001 (0.0019)	
training:	Epoch: [189][87/233]	Loss 0.0001 (0.0019)	
training:	Epoch: [189][88/233]	Loss 0.0001 (0.0019)	
training:	Epoch: [189][89/233]	Loss 0.0004 (0.0019)	
training:	Epoch: [189][90/233]	Loss 0.0001 (0.0019)	
training:	Epoch: [189][91/233]	Loss 0.0003 (0.0019)	
training:	Epoch: [189][92/233]	Loss 0.0002 (0.0018)	
training:	Epoch: [189][93/233]	Loss 0.0001 (0.0018)	
training:	Epoch: [189][94/233]	Loss 0.0002 (0.0018)	
training:	Epoch: [189][95/233]	Loss 0.0002 (0.0018)	
training:	Epoch: [189][96/233]	Loss 0.0002 (0.0018)	
training:	Epoch: [189][97/233]	Loss 0.0002 (0.0017)	
training:	Epoch: [189][98/233]	Loss 0.0002 (0.0017)	
training:	Epoch: [189][99/233]	Loss 0.0001 (0.0017)	
training:	Epoch: [189][100/233]	Loss 0.0001 (0.0017)	
training:	Epoch: [189][101/233]	Loss 0.0002 (0.0017)	
training:	Epoch: [189][102/233]	Loss 0.0001 (0.0017)	
training:	Epoch: [189][103/233]	Loss 0.0001 (0.0017)	
training:	Epoch: [189][104/233]	Loss 0.0001 (0.0016)	
training:	Epoch: [189][105/233]	Loss 0.0001 (0.0016)	
training:	Epoch: [189][106/233]	Loss 0.0011 (0.0016)	
training:	Epoch: [189][107/233]	Loss 0.0002 (0.0016)	
training:	Epoch: [189][108/233]	Loss 0.0002 (0.0016)	
training:	Epoch: [189][109/233]	Loss 0.0001 (0.0016)	
training:	Epoch: [189][110/233]	Loss 0.0052 (0.0016)	
training:	Epoch: [189][111/233]	Loss 0.0002 (0.0016)	
training:	Epoch: [189][112/233]	Loss 0.0001 (0.0016)	
training:	Epoch: [189][113/233]	Loss 0.0003 (0.0016)	
training:	Epoch: [189][114/233]	Loss 0.0001 (0.0016)	
training:	Epoch: [189][115/233]	Loss 0.0003 (0.0016)	
training:	Epoch: [189][116/233]	Loss 0.0002 (0.0015)	
training:	Epoch: [189][117/233]	Loss 0.0001 (0.0015)	
training:	Epoch: [189][118/233]	Loss 0.0001 (0.0015)	
training:	Epoch: [189][119/233]	Loss 0.0001 (0.0015)	
training:	Epoch: [189][120/233]	Loss 0.0002 (0.0015)	
training:	Epoch: [189][121/233]	Loss 0.0007 (0.0015)	
training:	Epoch: [189][122/233]	Loss 0.0001 (0.0015)	
training:	Epoch: [189][123/233]	Loss 0.0020 (0.0015)	
training:	Epoch: [189][124/233]	Loss 0.0002 (0.0015)	
training:	Epoch: [189][125/233]	Loss 0.0001 (0.0015)	
training:	Epoch: [189][126/233]	Loss 0.0002 (0.0014)	
training:	Epoch: [189][127/233]	Loss 0.0002 (0.0014)	
training:	Epoch: [189][128/233]	Loss 0.0002 (0.0014)	
training:	Epoch: [189][129/233]	Loss 0.0002 (0.0014)	
training:	Epoch: [189][130/233]	Loss 0.0001 (0.0014)	
training:	Epoch: [189][131/233]	Loss 0.0002 (0.0014)	
training:	Epoch: [189][132/233]	Loss 0.0001 (0.0014)	
training:	Epoch: [189][133/233]	Loss 0.0002 (0.0014)	
training:	Epoch: [189][134/233]	Loss 0.0001 (0.0014)	
training:	Epoch: [189][135/233]	Loss 0.0001 (0.0014)	
training:	Epoch: [189][136/233]	Loss 0.0001 (0.0014)	
training:	Epoch: [189][137/233]	Loss 0.0002 (0.0013)	
training:	Epoch: [189][138/233]	Loss 0.0002 (0.0013)	
training:	Epoch: [189][139/233]	Loss 0.0001 (0.0013)	
training:	Epoch: [189][140/233]	Loss 0.0001 (0.0013)	
training:	Epoch: [189][141/233]	Loss 0.0001 (0.0013)	
training:	Epoch: [189][142/233]	Loss 0.0001 (0.0013)	
training:	Epoch: [189][143/233]	Loss 0.0018 (0.0013)	
training:	Epoch: [189][144/233]	Loss 0.0001 (0.0013)	
training:	Epoch: [189][145/233]	Loss 0.0002 (0.0013)	
training:	Epoch: [189][146/233]	Loss 0.0002 (0.0013)	
training:	Epoch: [189][147/233]	Loss 0.0001 (0.0013)	
training:	Epoch: [189][148/233]	Loss 0.0002 (0.0013)	
training:	Epoch: [189][149/233]	Loss 0.0001 (0.0013)	
training:	Epoch: [189][150/233]	Loss 0.0005 (0.0013)	
training:	Epoch: [189][151/233]	Loss 0.0005 (0.0012)	
training:	Epoch: [189][152/233]	Loss 0.0001 (0.0012)	
training:	Epoch: [189][153/233]	Loss 0.0002 (0.0012)	
training:	Epoch: [189][154/233]	Loss 0.0002 (0.0012)	
training:	Epoch: [189][155/233]	Loss 0.0001 (0.0012)	
training:	Epoch: [189][156/233]	Loss 0.0001 (0.0012)	
training:	Epoch: [189][157/233]	Loss 0.0001 (0.0012)	
training:	Epoch: [189][158/233]	Loss 0.0002 (0.0012)	
training:	Epoch: [189][159/233]	Loss 0.0001 (0.0012)	
training:	Epoch: [189][160/233]	Loss 0.0001 (0.0012)	
training:	Epoch: [189][161/233]	Loss 0.0001 (0.0012)	
training:	Epoch: [189][162/233]	Loss 0.1192 (0.0019)	
training:	Epoch: [189][163/233]	Loss 0.0001 (0.0019)	
training:	Epoch: [189][164/233]	Loss 0.0004 (0.0019)	
training:	Epoch: [189][165/233]	Loss 0.0002 (0.0019)	
training:	Epoch: [189][166/233]	Loss 0.0003 (0.0019)	
training:	Epoch: [189][167/233]	Loss 0.0002 (0.0019)	
training:	Epoch: [189][168/233]	Loss 0.0003 (0.0018)	
training:	Epoch: [189][169/233]	Loss 0.0008 (0.0018)	
training:	Epoch: [189][170/233]	Loss 0.0001 (0.0018)	
training:	Epoch: [189][171/233]	Loss 0.0001 (0.0018)	
training:	Epoch: [189][172/233]	Loss 0.0001 (0.0018)	
training:	Epoch: [189][173/233]	Loss 0.0002 (0.0018)	
training:	Epoch: [189][174/233]	Loss 0.0006 (0.0018)	
training:	Epoch: [189][175/233]	Loss 0.0001 (0.0018)	
training:	Epoch: [189][176/233]	Loss 0.0002 (0.0018)	
training:	Epoch: [189][177/233]	Loss 0.0006 (0.0018)	
training:	Epoch: [189][178/233]	Loss 0.0001 (0.0018)	
training:	Epoch: [189][179/233]	Loss 0.0002 (0.0018)	
training:	Epoch: [189][180/233]	Loss 0.0001 (0.0017)	
training:	Epoch: [189][181/233]	Loss 0.0058 (0.0018)	
training:	Epoch: [189][182/233]	Loss 0.1839 (0.0028)	
training:	Epoch: [189][183/233]	Loss 0.0002 (0.0028)	
training:	Epoch: [189][184/233]	Loss 0.0002 (0.0027)	
training:	Epoch: [189][185/233]	Loss 0.0002 (0.0027)	
training:	Epoch: [189][186/233]	Loss 0.0002 (0.0027)	
training:	Epoch: [189][187/233]	Loss 0.0001 (0.0027)	
training:	Epoch: [189][188/233]	Loss 0.0001 (0.0027)	
training:	Epoch: [189][189/233]	Loss 0.0002 (0.0027)	
training:	Epoch: [189][190/233]	Loss 0.0001 (0.0027)	
training:	Epoch: [189][191/233]	Loss 0.0001 (0.0026)	
training:	Epoch: [189][192/233]	Loss 0.0001 (0.0026)	
training:	Epoch: [189][193/233]	Loss 0.0001 (0.0026)	
training:	Epoch: [189][194/233]	Loss 0.0002 (0.0026)	
training:	Epoch: [189][195/233]	Loss 0.0002 (0.0026)	
training:	Epoch: [189][196/233]	Loss 0.0001 (0.0026)	
training:	Epoch: [189][197/233]	Loss 0.0004 (0.0026)	
training:	Epoch: [189][198/233]	Loss 0.0225 (0.0027)	
training:	Epoch: [189][199/233]	Loss 0.0004 (0.0027)	
training:	Epoch: [189][200/233]	Loss 0.0002 (0.0026)	
training:	Epoch: [189][201/233]	Loss 0.0099 (0.0027)	
training:	Epoch: [189][202/233]	Loss 0.0001 (0.0027)	
training:	Epoch: [189][203/233]	Loss 0.0005 (0.0027)	
training:	Epoch: [189][204/233]	Loss 0.0001 (0.0026)	
training:	Epoch: [189][205/233]	Loss 0.0038 (0.0027)	
training:	Epoch: [189][206/233]	Loss 0.0001 (0.0026)	
training:	Epoch: [189][207/233]	Loss 0.0001 (0.0026)	
training:	Epoch: [189][208/233]	Loss 0.0002 (0.0026)	
training:	Epoch: [189][209/233]	Loss 0.0077 (0.0026)	
training:	Epoch: [189][210/233]	Loss 0.0002 (0.0026)	
training:	Epoch: [189][211/233]	Loss 0.0002 (0.0026)	
training:	Epoch: [189][212/233]	Loss 0.0004 (0.0026)	
training:	Epoch: [189][213/233]	Loss 0.0002 (0.0026)	
training:	Epoch: [189][214/233]	Loss 0.0011 (0.0026)	
training:	Epoch: [189][215/233]	Loss 0.0001 (0.0026)	
training:	Epoch: [189][216/233]	Loss 0.0002 (0.0026)	
training:	Epoch: [189][217/233]	Loss 0.0004 (0.0026)	
training:	Epoch: [189][218/233]	Loss 0.0001 (0.0025)	
training:	Epoch: [189][219/233]	Loss 0.0002 (0.0025)	
training:	Epoch: [189][220/233]	Loss 0.0002 (0.0025)	
training:	Epoch: [189][221/233]	Loss 0.0002 (0.0025)	
training:	Epoch: [189][222/233]	Loss 0.0001 (0.0025)	
training:	Epoch: [189][223/233]	Loss 0.0002 (0.0025)	
training:	Epoch: [189][224/233]	Loss 0.0011 (0.0025)	
training:	Epoch: [189][225/233]	Loss 0.0467 (0.0027)	
training:	Epoch: [189][226/233]	Loss 0.0034 (0.0027)	
training:	Epoch: [189][227/233]	Loss 0.0001 (0.0027)	
training:	Epoch: [189][228/233]	Loss 0.0002 (0.0027)	
training:	Epoch: [189][229/233]	Loss 0.0005 (0.0027)	
training:	Epoch: [189][230/233]	Loss 0.0001 (0.0026)	
training:	Epoch: [189][231/233]	Loss 0.0002 (0.0026)	
training:	Epoch: [189][232/233]	Loss 0.0002 (0.0026)	
training:	Epoch: [189][233/233]	Loss 0.0006 (0.0026)	
Training:	 Loss: 0.0026

Training:	 ACC: 1.0000 1.0000 1.0000 1.0000
Validation:	 ACC: 0.7915 0.7908 0.7773 0.8056
Validation:	 Best_BACC: 0.8162 0.8154 0.7998 0.8326
Validation:	 Loss: 1.4257
Pretraining:	Epoch 190/200
----------
training:	Epoch: [190][1/233]	Loss 0.0001 (0.0001)	
training:	Epoch: [190][2/233]	Loss 0.0024 (0.0012)	
training:	Epoch: [190][3/233]	Loss 0.0002 (0.0009)	
training:	Epoch: [190][4/233]	Loss 0.0002 (0.0007)	
training:	Epoch: [190][5/233]	Loss 0.0001 (0.0006)	
training:	Epoch: [190][6/233]	Loss 0.0140 (0.0028)	
training:	Epoch: [190][7/233]	Loss 0.0001 (0.0025)	
training:	Epoch: [190][8/233]	Loss 0.0002 (0.0022)	
training:	Epoch: [190][9/233]	Loss 0.0001 (0.0019)	
training:	Epoch: [190][10/233]	Loss 0.0001 (0.0018)	
training:	Epoch: [190][11/233]	Loss 0.0001 (0.0016)	
training:	Epoch: [190][12/233]	Loss 0.0001 (0.0015)	
training:	Epoch: [190][13/233]	Loss 0.0071 (0.0019)	
training:	Epoch: [190][14/233]	Loss 0.0001 (0.0018)	
training:	Epoch: [190][15/233]	Loss 0.0004 (0.0017)	
training:	Epoch: [190][16/233]	Loss 0.0031 (0.0018)	
training:	Epoch: [190][17/233]	Loss 0.0002 (0.0017)	
training:	Epoch: [190][18/233]	Loss 0.0001 (0.0016)	
training:	Epoch: [190][19/233]	Loss 0.0003 (0.0015)	
training:	Epoch: [190][20/233]	Loss 0.0001 (0.0015)	
training:	Epoch: [190][21/233]	Loss 0.0001 (0.0014)	
training:	Epoch: [190][22/233]	Loss 0.0012 (0.0014)	
training:	Epoch: [190][23/233]	Loss 0.0001 (0.0013)	
training:	Epoch: [190][24/233]	Loss 0.0001 (0.0013)	
training:	Epoch: [190][25/233]	Loss 0.0001 (0.0012)	
training:	Epoch: [190][26/233]	Loss 0.0002 (0.0012)	
training:	Epoch: [190][27/233]	Loss 0.0002 (0.0012)	
training:	Epoch: [190][28/233]	Loss 0.0056 (0.0013)	
training:	Epoch: [190][29/233]	Loss 0.0002 (0.0013)	
training:	Epoch: [190][30/233]	Loss 0.0001 (0.0012)	
training:	Epoch: [190][31/233]	Loss 0.0001 (0.0012)	
training:	Epoch: [190][32/233]	Loss 0.0001 (0.0012)	
training:	Epoch: [190][33/233]	Loss 0.0001 (0.0011)	
training:	Epoch: [190][34/233]	Loss 0.0001 (0.0011)	
training:	Epoch: [190][35/233]	Loss 0.0002 (0.0011)	
training:	Epoch: [190][36/233]	Loss 0.0001 (0.0011)	
training:	Epoch: [190][37/233]	Loss 0.0001 (0.0010)	
training:	Epoch: [190][38/233]	Loss 0.0002 (0.0010)	
training:	Epoch: [190][39/233]	Loss 0.0002 (0.0010)	
training:	Epoch: [190][40/233]	Loss 0.0001 (0.0010)	
training:	Epoch: [190][41/233]	Loss 0.0001 (0.0009)	
training:	Epoch: [190][42/233]	Loss 0.0005 (0.0009)	
training:	Epoch: [190][43/233]	Loss 0.0001 (0.0009)	
training:	Epoch: [190][44/233]	Loss 0.0002 (0.0009)	
training:	Epoch: [190][45/233]	Loss 0.0002 (0.0009)	
training:	Epoch: [190][46/233]	Loss 0.0041 (0.0010)	
training:	Epoch: [190][47/233]	Loss 0.0002 (0.0009)	
training:	Epoch: [190][48/233]	Loss 0.0002 (0.0009)	
training:	Epoch: [190][49/233]	Loss 0.0002 (0.0009)	
training:	Epoch: [190][50/233]	Loss 0.0015 (0.0009)	
training:	Epoch: [190][51/233]	Loss 0.0002 (0.0009)	
training:	Epoch: [190][52/233]	Loss 0.0001 (0.0009)	
training:	Epoch: [190][53/233]	Loss 0.0001 (0.0009)	
training:	Epoch: [190][54/233]	Loss 0.0001 (0.0009)	
training:	Epoch: [190][55/233]	Loss 0.0001 (0.0008)	
training:	Epoch: [190][56/233]	Loss 0.0001 (0.0008)	
training:	Epoch: [190][57/233]	Loss 0.0001 (0.0008)	
training:	Epoch: [190][58/233]	Loss 0.0001 (0.0008)	
training:	Epoch: [190][59/233]	Loss 0.0001 (0.0008)	
training:	Epoch: [190][60/233]	Loss 0.0001 (0.0008)	
training:	Epoch: [190][61/233]	Loss 0.0001 (0.0008)	
training:	Epoch: [190][62/233]	Loss 0.0668 (0.0018)	
training:	Epoch: [190][63/233]	Loss 0.0001 (0.0018)	
training:	Epoch: [190][64/233]	Loss 0.0001 (0.0018)	
training:	Epoch: [190][65/233]	Loss 0.1118 (0.0035)	
training:	Epoch: [190][66/233]	Loss 0.0001 (0.0034)	
training:	Epoch: [190][67/233]	Loss 0.0028 (0.0034)	
training:	Epoch: [190][68/233]	Loss 0.0001 (0.0034)	
training:	Epoch: [190][69/233]	Loss 0.0002 (0.0033)	
training:	Epoch: [190][70/233]	Loss 0.0001 (0.0033)	
training:	Epoch: [190][71/233]	Loss 0.0002 (0.0032)	
training:	Epoch: [190][72/233]	Loss 0.0001 (0.0032)	
training:	Epoch: [190][73/233]	Loss 0.0002 (0.0031)	
training:	Epoch: [190][74/233]	Loss 0.0011 (0.0031)	
training:	Epoch: [190][75/233]	Loss 0.0001 (0.0031)	
training:	Epoch: [190][76/233]	Loss 0.0001 (0.0030)	
training:	Epoch: [190][77/233]	Loss 0.0002 (0.0030)	
training:	Epoch: [190][78/233]	Loss 0.0001 (0.0030)	
training:	Epoch: [190][79/233]	Loss 0.0002 (0.0029)	
training:	Epoch: [190][80/233]	Loss 0.0003 (0.0029)	
training:	Epoch: [190][81/233]	Loss 0.0002 (0.0029)	
training:	Epoch: [190][82/233]	Loss 0.0008 (0.0028)	
training:	Epoch: [190][83/233]	Loss 0.0009 (0.0028)	
training:	Epoch: [190][84/233]	Loss 0.0003 (0.0028)	
training:	Epoch: [190][85/233]	Loss 0.0003 (0.0028)	
training:	Epoch: [190][86/233]	Loss 0.0005 (0.0027)	
training:	Epoch: [190][87/233]	Loss 0.0021 (0.0027)	
training:	Epoch: [190][88/233]	Loss 0.0003 (0.0027)	
training:	Epoch: [190][89/233]	Loss 0.0011 (0.0027)	
training:	Epoch: [190][90/233]	Loss 0.0003 (0.0027)	
training:	Epoch: [190][91/233]	Loss 0.0001 (0.0026)	
training:	Epoch: [190][92/233]	Loss 0.0003 (0.0026)	
training:	Epoch: [190][93/233]	Loss 0.0002 (0.0026)	
training:	Epoch: [190][94/233]	Loss 0.0002 (0.0025)	
training:	Epoch: [190][95/233]	Loss 0.0013 (0.0025)	
training:	Epoch: [190][96/233]	Loss 0.0044 (0.0026)	
training:	Epoch: [190][97/233]	Loss 0.0005 (0.0025)	
training:	Epoch: [190][98/233]	Loss 0.0013 (0.0025)	
training:	Epoch: [190][99/233]	Loss 0.0002 (0.0025)	
training:	Epoch: [190][100/233]	Loss 0.0001 (0.0025)	
training:	Epoch: [190][101/233]	Loss 0.0006 (0.0025)	
training:	Epoch: [190][102/233]	Loss 0.0399 (0.0028)	
training:	Epoch: [190][103/233]	Loss 0.0015 (0.0028)	
training:	Epoch: [190][104/233]	Loss 0.0003 (0.0028)	
training:	Epoch: [190][105/233]	Loss 0.0001 (0.0028)	
training:	Epoch: [190][106/233]	Loss 0.0002 (0.0027)	
training:	Epoch: [190][107/233]	Loss 0.0002 (0.0027)	
training:	Epoch: [190][108/233]	Loss 0.0066 (0.0027)	
training:	Epoch: [190][109/233]	Loss 0.0001 (0.0027)	
training:	Epoch: [190][110/233]	Loss 0.0014 (0.0027)	
training:	Epoch: [190][111/233]	Loss 0.0001 (0.0027)	
training:	Epoch: [190][112/233]	Loss 0.0001 (0.0027)	
training:	Epoch: [190][113/233]	Loss 0.0001 (0.0026)	
training:	Epoch: [190][114/233]	Loss 0.0003 (0.0026)	
training:	Epoch: [190][115/233]	Loss 0.0001 (0.0026)	
training:	Epoch: [190][116/233]	Loss 0.0002 (0.0026)	
training:	Epoch: [190][117/233]	Loss 0.0002 (0.0026)	
training:	Epoch: [190][118/233]	Loss 0.0012 (0.0025)	
training:	Epoch: [190][119/233]	Loss 0.0001 (0.0025)	
training:	Epoch: [190][120/233]	Loss 0.0006 (0.0025)	
training:	Epoch: [190][121/233]	Loss 0.0002 (0.0025)	
training:	Epoch: [190][122/233]	Loss 0.0024 (0.0025)	
training:	Epoch: [190][123/233]	Loss 0.0001 (0.0025)	
training:	Epoch: [190][124/233]	Loss 0.0003 (0.0025)	
training:	Epoch: [190][125/233]	Loss 0.0006 (0.0024)	
training:	Epoch: [190][126/233]	Loss 0.0002 (0.0024)	
training:	Epoch: [190][127/233]	Loss 0.0001 (0.0024)	
training:	Epoch: [190][128/233]	Loss 0.0060 (0.0024)	
training:	Epoch: [190][129/233]	Loss 0.0008 (0.0024)	
training:	Epoch: [190][130/233]	Loss 0.0001 (0.0024)	
training:	Epoch: [190][131/233]	Loss 0.0001 (0.0024)	
training:	Epoch: [190][132/233]	Loss 0.0001 (0.0024)	
training:	Epoch: [190][133/233]	Loss 0.0001 (0.0024)	
training:	Epoch: [190][134/233]	Loss 0.1840 (0.0037)	
training:	Epoch: [190][135/233]	Loss 0.0002 (0.0037)	
training:	Epoch: [190][136/233]	Loss 0.0001 (0.0037)	
training:	Epoch: [190][137/233]	Loss 0.0004 (0.0036)	
training:	Epoch: [190][138/233]	Loss 0.0007 (0.0036)	
training:	Epoch: [190][139/233]	Loss 0.0002 (0.0036)	
training:	Epoch: [190][140/233]	Loss 0.0002 (0.0036)	
training:	Epoch: [190][141/233]	Loss 0.0001 (0.0035)	
training:	Epoch: [190][142/233]	Loss 0.1336 (0.0045)	
training:	Epoch: [190][143/233]	Loss 0.0001 (0.0044)	
training:	Epoch: [190][144/233]	Loss 0.0002 (0.0044)	
training:	Epoch: [190][145/233]	Loss 0.0015 (0.0044)	
training:	Epoch: [190][146/233]	Loss 0.0002 (0.0043)	
training:	Epoch: [190][147/233]	Loss 0.0002 (0.0043)	
training:	Epoch: [190][148/233]	Loss 0.0001 (0.0043)	
training:	Epoch: [190][149/233]	Loss 0.0010 (0.0043)	
training:	Epoch: [190][150/233]	Loss 0.0010 (0.0042)	
training:	Epoch: [190][151/233]	Loss 0.0001 (0.0042)	
training:	Epoch: [190][152/233]	Loss 0.0001 (0.0042)	
training:	Epoch: [190][153/233]	Loss 0.0004 (0.0042)	
training:	Epoch: [190][154/233]	Loss 0.0012 (0.0041)	
training:	Epoch: [190][155/233]	Loss 0.0002 (0.0041)	
training:	Epoch: [190][156/233]	Loss 0.0002 (0.0041)	
training:	Epoch: [190][157/233]	Loss 0.0033 (0.0041)	
training:	Epoch: [190][158/233]	Loss 0.0001 (0.0041)	
training:	Epoch: [190][159/233]	Loss 0.0002 (0.0040)	
training:	Epoch: [190][160/233]	Loss 0.0001 (0.0040)	
training:	Epoch: [190][161/233]	Loss 0.0010 (0.0040)	
training:	Epoch: [190][162/233]	Loss 0.0104 (0.0040)	
training:	Epoch: [190][163/233]	Loss 0.0009 (0.0040)	
training:	Epoch: [190][164/233]	Loss 0.0001 (0.0040)	
training:	Epoch: [190][165/233]	Loss 0.0241 (0.0041)	
training:	Epoch: [190][166/233]	Loss 0.0513 (0.0044)	
training:	Epoch: [190][167/233]	Loss 0.0002 (0.0044)	
training:	Epoch: [190][168/233]	Loss 0.0001 (0.0043)	
training:	Epoch: [190][169/233]	Loss 0.0003 (0.0043)	
training:	Epoch: [190][170/233]	Loss 0.0003 (0.0043)	
training:	Epoch: [190][171/233]	Loss 0.0006 (0.0043)	
training:	Epoch: [190][172/233]	Loss 0.0022 (0.0043)	
training:	Epoch: [190][173/233]	Loss 0.0002 (0.0042)	
training:	Epoch: [190][174/233]	Loss 0.0002 (0.0042)	
training:	Epoch: [190][175/233]	Loss 0.0004 (0.0042)	
training:	Epoch: [190][176/233]	Loss 0.0001 (0.0042)	
training:	Epoch: [190][177/233]	Loss 0.0001 (0.0042)	
training:	Epoch: [190][178/233]	Loss 0.0003 (0.0041)	
training:	Epoch: [190][179/233]	Loss 0.0003 (0.0041)	
training:	Epoch: [190][180/233]	Loss 0.0318 (0.0043)	
training:	Epoch: [190][181/233]	Loss 0.0001 (0.0042)	
training:	Epoch: [190][182/233]	Loss 0.0001 (0.0042)	
training:	Epoch: [190][183/233]	Loss 0.0004 (0.0042)	
training:	Epoch: [190][184/233]	Loss 0.0001 (0.0042)	
training:	Epoch: [190][185/233]	Loss 0.0332 (0.0043)	
training:	Epoch: [190][186/233]	Loss 0.0295 (0.0045)	
training:	Epoch: [190][187/233]	Loss 0.0001 (0.0044)	
training:	Epoch: [190][188/233]	Loss 0.0002 (0.0044)	
training:	Epoch: [190][189/233]	Loss 0.0002 (0.0044)	
training:	Epoch: [190][190/233]	Loss 0.0004 (0.0044)	
training:	Epoch: [190][191/233]	Loss 0.0003 (0.0044)	
training:	Epoch: [190][192/233]	Loss 0.0002 (0.0043)	
training:	Epoch: [190][193/233]	Loss 0.0001 (0.0043)	
training:	Epoch: [190][194/233]	Loss 0.0002 (0.0043)	
training:	Epoch: [190][195/233]	Loss 0.0015 (0.0043)	
training:	Epoch: [190][196/233]	Loss 0.0003 (0.0043)	
training:	Epoch: [190][197/233]	Loss 0.0001 (0.0042)	
training:	Epoch: [190][198/233]	Loss 0.0002 (0.0042)	
training:	Epoch: [190][199/233]	Loss 0.0001 (0.0042)	
training:	Epoch: [190][200/233]	Loss 0.0003 (0.0042)	
training:	Epoch: [190][201/233]	Loss 0.0002 (0.0042)	
training:	Epoch: [190][202/233]	Loss 0.0001 (0.0041)	
training:	Epoch: [190][203/233]	Loss 0.0002 (0.0041)	
training:	Epoch: [190][204/233]	Loss 0.0030 (0.0041)	
training:	Epoch: [190][205/233]	Loss 0.0002 (0.0041)	
training:	Epoch: [190][206/233]	Loss 0.0019 (0.0041)	
training:	Epoch: [190][207/233]	Loss 0.0320 (0.0042)	
training:	Epoch: [190][208/233]	Loss 0.0003 (0.0042)	
training:	Epoch: [190][209/233]	Loss 0.0090 (0.0042)	
training:	Epoch: [190][210/233]	Loss 0.0001 (0.0042)	
training:	Epoch: [190][211/233]	Loss 0.0003 (0.0042)	
training:	Epoch: [190][212/233]	Loss 0.0001 (0.0042)	
training:	Epoch: [190][213/233]	Loss 0.0002 (0.0041)	
training:	Epoch: [190][214/233]	Loss 0.0003 (0.0041)	
training:	Epoch: [190][215/233]	Loss 0.0002 (0.0041)	
training:	Epoch: [190][216/233]	Loss 0.0002 (0.0041)	
training:	Epoch: [190][217/233]	Loss 0.0002 (0.0041)	
training:	Epoch: [190][218/233]	Loss 0.0050 (0.0041)	
training:	Epoch: [190][219/233]	Loss 0.0012 (0.0041)	
training:	Epoch: [190][220/233]	Loss 0.0011 (0.0040)	
training:	Epoch: [190][221/233]	Loss 0.0206 (0.0041)	
training:	Epoch: [190][222/233]	Loss 0.0005 (0.0041)	
training:	Epoch: [190][223/233]	Loss 0.0004 (0.0041)	
training:	Epoch: [190][224/233]	Loss 0.0002 (0.0041)	
training:	Epoch: [190][225/233]	Loss 0.0026 (0.0041)	
training:	Epoch: [190][226/233]	Loss 0.0002 (0.0040)	
training:	Epoch: [190][227/233]	Loss 0.0005 (0.0040)	
training:	Epoch: [190][228/233]	Loss 0.0001 (0.0040)	
training:	Epoch: [190][229/233]	Loss 0.0005 (0.0040)	
training:	Epoch: [190][230/233]	Loss 0.0057 (0.0040)	
training:	Epoch: [190][231/233]	Loss 0.0019 (0.0040)	
training:	Epoch: [190][232/233]	Loss 0.0006 (0.0040)	
training:	Epoch: [190][233/233]	Loss 0.0002 (0.0040)	
Training:	 Loss: 0.0040

Training:	 ACC: 0.9999 0.9999 0.9997 1.0000
Validation:	 ACC: 0.7865 0.7844 0.7416 0.8315
Validation:	 Best_BACC: 0.8162 0.8154 0.7998 0.8326
Validation:	 Loss: 1.4725
Pretraining:	Epoch 191/200
----------
training:	Epoch: [191][1/233]	Loss 0.0209 (0.0209)	
training:	Epoch: [191][2/233]	Loss 0.0251 (0.0230)	
training:	Epoch: [191][3/233]	Loss 0.0003 (0.0154)	
training:	Epoch: [191][4/233]	Loss 0.0004 (0.0117)	
training:	Epoch: [191][5/233]	Loss 0.0002 (0.0094)	
training:	Epoch: [191][6/233]	Loss 0.0006 (0.0079)	
training:	Epoch: [191][7/233]	Loss 0.0001 (0.0068)	
training:	Epoch: [191][8/233]	Loss 0.0002 (0.0060)	
training:	Epoch: [191][9/233]	Loss 0.0006 (0.0054)	
training:	Epoch: [191][10/233]	Loss 0.0004 (0.0049)	
training:	Epoch: [191][11/233]	Loss 0.0001 (0.0044)	
training:	Epoch: [191][12/233]	Loss 0.0005 (0.0041)	
training:	Epoch: [191][13/233]	Loss 0.0002 (0.0038)	
training:	Epoch: [191][14/233]	Loss 0.0079 (0.0041)	
training:	Epoch: [191][15/233]	Loss 0.0001 (0.0038)	
training:	Epoch: [191][16/233]	Loss 0.0001 (0.0036)	
training:	Epoch: [191][17/233]	Loss 0.0001 (0.0034)	
training:	Epoch: [191][18/233]	Loss 0.0001 (0.0032)	
training:	Epoch: [191][19/233]	Loss 0.0002 (0.0031)	
training:	Epoch: [191][20/233]	Loss 0.0001 (0.0029)	
training:	Epoch: [191][21/233]	Loss 0.0001 (0.0028)	
training:	Epoch: [191][22/233]	Loss 0.0001 (0.0027)	
training:	Epoch: [191][23/233]	Loss 0.0001 (0.0026)	
training:	Epoch: [191][24/233]	Loss 0.0002 (0.0025)	
training:	Epoch: [191][25/233]	Loss 0.0001 (0.0024)	
training:	Epoch: [191][26/233]	Loss 0.0001 (0.0023)	
training:	Epoch: [191][27/233]	Loss 0.0001 (0.0022)	
training:	Epoch: [191][28/233]	Loss 0.0002 (0.0021)	
training:	Epoch: [191][29/233]	Loss 0.0002 (0.0021)	
training:	Epoch: [191][30/233]	Loss 0.0002 (0.0020)	
training:	Epoch: [191][31/233]	Loss 0.0002 (0.0019)	
training:	Epoch: [191][32/233]	Loss 0.0002 (0.0019)	
training:	Epoch: [191][33/233]	Loss 0.0002 (0.0018)	
training:	Epoch: [191][34/233]	Loss 0.0001 (0.0018)	
training:	Epoch: [191][35/233]	Loss 0.0001 (0.0017)	
training:	Epoch: [191][36/233]	Loss 0.0001 (0.0017)	
training:	Epoch: [191][37/233]	Loss 0.0003 (0.0016)	
training:	Epoch: [191][38/233]	Loss 0.0001 (0.0016)	
training:	Epoch: [191][39/233]	Loss 0.0002 (0.0016)	
training:	Epoch: [191][40/233]	Loss 0.0001 (0.0015)	
training:	Epoch: [191][41/233]	Loss 0.0001 (0.0015)	
training:	Epoch: [191][42/233]	Loss 0.0005 (0.0015)	
training:	Epoch: [191][43/233]	Loss 0.0001 (0.0014)	
training:	Epoch: [191][44/233]	Loss 0.0002 (0.0014)	
training:	Epoch: [191][45/233]	Loss 0.0001 (0.0014)	
training:	Epoch: [191][46/233]	Loss 0.0001 (0.0014)	
training:	Epoch: [191][47/233]	Loss 0.0001 (0.0013)	
training:	Epoch: [191][48/233]	Loss 0.0001 (0.0013)	
training:	Epoch: [191][49/233]	Loss 0.0002 (0.0013)	
training:	Epoch: [191][50/233]	Loss 0.0001 (0.0013)	
training:	Epoch: [191][51/233]	Loss 0.0001 (0.0012)	
training:	Epoch: [191][52/233]	Loss 0.0002 (0.0012)	
training:	Epoch: [191][53/233]	Loss 0.0003 (0.0012)	
training:	Epoch: [191][54/233]	Loss 0.0028 (0.0012)	
training:	Epoch: [191][55/233]	Loss 0.0001 (0.0012)	
training:	Epoch: [191][56/233]	Loss 0.0044 (0.0013)	
training:	Epoch: [191][57/233]	Loss 0.1409 (0.0037)	
training:	Epoch: [191][58/233]	Loss 0.0001 (0.0037)	
training:	Epoch: [191][59/233]	Loss 0.0002 (0.0036)	
training:	Epoch: [191][60/233]	Loss 0.0009 (0.0036)	
training:	Epoch: [191][61/233]	Loss 0.0002 (0.0035)	
training:	Epoch: [191][62/233]	Loss 0.0002 (0.0034)	
training:	Epoch: [191][63/233]	Loss 0.0002 (0.0034)	
training:	Epoch: [191][64/233]	Loss 0.0105 (0.0035)	
training:	Epoch: [191][65/233]	Loss 0.0001 (0.0035)	
training:	Epoch: [191][66/233]	Loss 0.0002 (0.0034)	
training:	Epoch: [191][67/233]	Loss 0.0001 (0.0034)	
training:	Epoch: [191][68/233]	Loss 0.0001 (0.0033)	
training:	Epoch: [191][69/233]	Loss 0.0714 (0.0043)	
training:	Epoch: [191][70/233]	Loss 0.0001 (0.0042)	
training:	Epoch: [191][71/233]	Loss 0.0001 (0.0042)	
training:	Epoch: [191][72/233]	Loss 0.0002 (0.0041)	
training:	Epoch: [191][73/233]	Loss 0.0003 (0.0041)	
training:	Epoch: [191][74/233]	Loss 0.0001 (0.0040)	
training:	Epoch: [191][75/233]	Loss 0.0002 (0.0040)	
training:	Epoch: [191][76/233]	Loss 0.0002 (0.0039)	
training:	Epoch: [191][77/233]	Loss 0.0007 (0.0039)	
training:	Epoch: [191][78/233]	Loss 0.0002 (0.0038)	
training:	Epoch: [191][79/233]	Loss 0.0002 (0.0038)	
training:	Epoch: [191][80/233]	Loss 0.0001 (0.0037)	
training:	Epoch: [191][81/233]	Loss 0.0001 (0.0037)	
training:	Epoch: [191][82/233]	Loss 0.0003 (0.0036)	
training:	Epoch: [191][83/233]	Loss 0.0001 (0.0036)	
training:	Epoch: [191][84/233]	Loss 0.0004 (0.0036)	
training:	Epoch: [191][85/233]	Loss 0.0003 (0.0035)	
training:	Epoch: [191][86/233]	Loss 0.0001 (0.0035)	
training:	Epoch: [191][87/233]	Loss 0.0138 (0.0036)	
training:	Epoch: [191][88/233]	Loss 0.0051 (0.0036)	
training:	Epoch: [191][89/233]	Loss 0.0001 (0.0036)	
training:	Epoch: [191][90/233]	Loss 0.0002 (0.0035)	
training:	Epoch: [191][91/233]	Loss 0.0001 (0.0035)	
training:	Epoch: [191][92/233]	Loss 0.0002 (0.0035)	
training:	Epoch: [191][93/233]	Loss 0.0001 (0.0034)	
training:	Epoch: [191][94/233]	Loss 0.0001 (0.0034)	
training:	Epoch: [191][95/233]	Loss 0.0002 (0.0034)	
training:	Epoch: [191][96/233]	Loss 0.0001 (0.0033)	
training:	Epoch: [191][97/233]	Loss 0.0003 (0.0033)	
training:	Epoch: [191][98/233]	Loss 0.0001 (0.0033)	
training:	Epoch: [191][99/233]	Loss 0.0004 (0.0032)	
training:	Epoch: [191][100/233]	Loss 0.0002 (0.0032)	
training:	Epoch: [191][101/233]	Loss 0.0001 (0.0032)	
training:	Epoch: [191][102/233]	Loss 0.0002 (0.0031)	
training:	Epoch: [191][103/233]	Loss 0.0002 (0.0031)	
training:	Epoch: [191][104/233]	Loss 0.0001 (0.0031)	
training:	Epoch: [191][105/233]	Loss 0.0001 (0.0031)	
training:	Epoch: [191][106/233]	Loss 0.0001 (0.0030)	
training:	Epoch: [191][107/233]	Loss 0.0001 (0.0030)	
training:	Epoch: [191][108/233]	Loss 0.0003 (0.0030)	
training:	Epoch: [191][109/233]	Loss 0.0002 (0.0030)	
training:	Epoch: [191][110/233]	Loss 0.0001 (0.0029)	
training:	Epoch: [191][111/233]	Loss 0.0001 (0.0029)	
training:	Epoch: [191][112/233]	Loss 0.0001 (0.0029)	
training:	Epoch: [191][113/233]	Loss 0.0001 (0.0029)	
training:	Epoch: [191][114/233]	Loss 0.0001 (0.0028)	
training:	Epoch: [191][115/233]	Loss 0.0002 (0.0028)	
training:	Epoch: [191][116/233]	Loss 0.0002 (0.0028)	
training:	Epoch: [191][117/233]	Loss 0.0001 (0.0028)	
training:	Epoch: [191][118/233]	Loss 0.0002 (0.0027)	
training:	Epoch: [191][119/233]	Loss 0.0001 (0.0027)	
training:	Epoch: [191][120/233]	Loss 0.0004 (0.0027)	
training:	Epoch: [191][121/233]	Loss 0.0002 (0.0027)	
training:	Epoch: [191][122/233]	Loss 0.0002 (0.0027)	
training:	Epoch: [191][123/233]	Loss 0.0001 (0.0026)	
training:	Epoch: [191][124/233]	Loss 0.0001 (0.0026)	
training:	Epoch: [191][125/233]	Loss 0.0003 (0.0026)	
training:	Epoch: [191][126/233]	Loss 0.0001 (0.0026)	
training:	Epoch: [191][127/233]	Loss 0.0001 (0.0026)	
training:	Epoch: [191][128/233]	Loss 0.0001 (0.0025)	
training:	Epoch: [191][129/233]	Loss 0.0001 (0.0025)	
training:	Epoch: [191][130/233]	Loss 0.0001 (0.0025)	
training:	Epoch: [191][131/233]	Loss 0.0004 (0.0025)	
training:	Epoch: [191][132/233]	Loss 0.0002 (0.0025)	
training:	Epoch: [191][133/233]	Loss 0.0003 (0.0025)	
training:	Epoch: [191][134/233]	Loss 0.0002 (0.0024)	
training:	Epoch: [191][135/233]	Loss 0.0002 (0.0024)	
training:	Epoch: [191][136/233]	Loss 0.0002 (0.0024)	
training:	Epoch: [191][137/233]	Loss 0.0001 (0.0024)	
training:	Epoch: [191][138/233]	Loss 0.0001 (0.0024)	
training:	Epoch: [191][139/233]	Loss 0.0001 (0.0024)	
training:	Epoch: [191][140/233]	Loss 0.0002 (0.0023)	
training:	Epoch: [191][141/233]	Loss 0.0104 (0.0024)	
training:	Epoch: [191][142/233]	Loss 0.0001 (0.0024)	
training:	Epoch: [191][143/233]	Loss 0.0002 (0.0024)	
training:	Epoch: [191][144/233]	Loss 0.0001 (0.0024)	
training:	Epoch: [191][145/233]	Loss 0.0002 (0.0023)	
training:	Epoch: [191][146/233]	Loss 0.0002 (0.0023)	
training:	Epoch: [191][147/233]	Loss 0.0008 (0.0023)	
training:	Epoch: [191][148/233]	Loss 0.0001 (0.0023)	
training:	Epoch: [191][149/233]	Loss 0.0011 (0.0023)	
training:	Epoch: [191][150/233]	Loss 0.0003 (0.0023)	
training:	Epoch: [191][151/233]	Loss 0.0056 (0.0023)	
training:	Epoch: [191][152/233]	Loss 0.0001 (0.0023)	
training:	Epoch: [191][153/233]	Loss 0.0001 (0.0023)	
training:	Epoch: [191][154/233]	Loss 0.0003 (0.0023)	
training:	Epoch: [191][155/233]	Loss 0.0002 (0.0022)	
training:	Epoch: [191][156/233]	Loss 0.0002 (0.0022)	
training:	Epoch: [191][157/233]	Loss 0.0002 (0.0022)	
training:	Epoch: [191][158/233]	Loss 0.0005 (0.0022)	
training:	Epoch: [191][159/233]	Loss 0.0001 (0.0022)	
training:	Epoch: [191][160/233]	Loss 0.0001 (0.0022)	
training:	Epoch: [191][161/233]	Loss 0.0011 (0.0022)	
training:	Epoch: [191][162/233]	Loss 0.0001 (0.0022)	
training:	Epoch: [191][163/233]	Loss 0.0002 (0.0022)	
training:	Epoch: [191][164/233]	Loss 0.0001 (0.0021)	
training:	Epoch: [191][165/233]	Loss 0.0033 (0.0021)	
training:	Epoch: [191][166/233]	Loss 0.0002 (0.0021)	
training:	Epoch: [191][167/233]	Loss 0.0002 (0.0021)	
training:	Epoch: [191][168/233]	Loss 0.0001 (0.0021)	
training:	Epoch: [191][169/233]	Loss 0.0001 (0.0021)	
training:	Epoch: [191][170/233]	Loss 0.0002 (0.0021)	
training:	Epoch: [191][171/233]	Loss 0.0045 (0.0021)	
training:	Epoch: [191][172/233]	Loss 0.0002 (0.0021)	
training:	Epoch: [191][173/233]	Loss 0.0001 (0.0021)	
training:	Epoch: [191][174/233]	Loss 0.0001 (0.0021)	
training:	Epoch: [191][175/233]	Loss 0.0007 (0.0021)	
training:	Epoch: [191][176/233]	Loss 0.0001 (0.0020)	
training:	Epoch: [191][177/233]	Loss 0.0001 (0.0020)	
training:	Epoch: [191][178/233]	Loss 0.0001 (0.0020)	
training:	Epoch: [191][179/233]	Loss 0.0001 (0.0020)	
training:	Epoch: [191][180/233]	Loss 0.0001 (0.0020)	
training:	Epoch: [191][181/233]	Loss 0.0001 (0.0020)	
training:	Epoch: [191][182/233]	Loss 0.0002 (0.0020)	
training:	Epoch: [191][183/233]	Loss 0.0001 (0.0020)	
training:	Epoch: [191][184/233]	Loss 0.0001 (0.0020)	
training:	Epoch: [191][185/233]	Loss 0.0004 (0.0020)	
training:	Epoch: [191][186/233]	Loss 0.0001 (0.0019)	
training:	Epoch: [191][187/233]	Loss 0.0002 (0.0019)	
training:	Epoch: [191][188/233]	Loss 0.0001 (0.0019)	
training:	Epoch: [191][189/233]	Loss 0.0003 (0.0019)	
training:	Epoch: [191][190/233]	Loss 0.0001 (0.0019)	
training:	Epoch: [191][191/233]	Loss 0.0002 (0.0019)	
training:	Epoch: [191][192/233]	Loss 0.0004 (0.0019)	
training:	Epoch: [191][193/233]	Loss 0.0006 (0.0019)	
training:	Epoch: [191][194/233]	Loss 0.0001 (0.0019)	
training:	Epoch: [191][195/233]	Loss 0.0004 (0.0019)	
training:	Epoch: [191][196/233]	Loss 0.0002 (0.0019)	
training:	Epoch: [191][197/233]	Loss 0.0002 (0.0019)	
training:	Epoch: [191][198/233]	Loss 0.0002 (0.0018)	
training:	Epoch: [191][199/233]	Loss 0.0002 (0.0018)	
training:	Epoch: [191][200/233]	Loss 0.0001 (0.0018)	
training:	Epoch: [191][201/233]	Loss 0.0004 (0.0018)	
training:	Epoch: [191][202/233]	Loss 0.0002 (0.0018)	
training:	Epoch: [191][203/233]	Loss 0.0198 (0.0019)	
training:	Epoch: [191][204/233]	Loss 0.0001 (0.0019)	
training:	Epoch: [191][205/233]	Loss 0.0001 (0.0019)	
training:	Epoch: [191][206/233]	Loss 0.0001 (0.0019)	
training:	Epoch: [191][207/233]	Loss 0.0002 (0.0019)	
training:	Epoch: [191][208/233]	Loss 0.0105 (0.0019)	
training:	Epoch: [191][209/233]	Loss 0.0001 (0.0019)	
training:	Epoch: [191][210/233]	Loss 0.0005 (0.0019)	
training:	Epoch: [191][211/233]	Loss 0.0002 (0.0019)	
training:	Epoch: [191][212/233]	Loss 0.0008 (0.0019)	
training:	Epoch: [191][213/233]	Loss 0.0001 (0.0019)	
training:	Epoch: [191][214/233]	Loss 0.0002 (0.0019)	
training:	Epoch: [191][215/233]	Loss 0.0005 (0.0019)	
training:	Epoch: [191][216/233]	Loss 0.0007 (0.0019)	
training:	Epoch: [191][217/233]	Loss 0.0001 (0.0018)	
training:	Epoch: [191][218/233]	Loss 0.1460 (0.0025)	
training:	Epoch: [191][219/233]	Loss 0.0009 (0.0025)	
training:	Epoch: [191][220/233]	Loss 0.0001 (0.0025)	
training:	Epoch: [191][221/233]	Loss 0.0036 (0.0025)	
training:	Epoch: [191][222/233]	Loss 0.0001 (0.0025)	
training:	Epoch: [191][223/233]	Loss 0.0001 (0.0025)	
training:	Epoch: [191][224/233]	Loss 0.0005 (0.0025)	
training:	Epoch: [191][225/233]	Loss 0.0001 (0.0025)	
training:	Epoch: [191][226/233]	Loss 0.0002 (0.0024)	
training:	Epoch: [191][227/233]	Loss 0.0002 (0.0024)	
training:	Epoch: [191][228/233]	Loss 0.0003 (0.0024)	
training:	Epoch: [191][229/233]	Loss 0.0002 (0.0024)	
training:	Epoch: [191][230/233]	Loss 0.0002 (0.0024)	
training:	Epoch: [191][231/233]	Loss 0.0001 (0.0024)	
training:	Epoch: [191][232/233]	Loss 0.0001 (0.0024)	
training:	Epoch: [191][233/233]	Loss 0.0002 (0.0024)	
Training:	 Loss: 0.0024

Training:	 ACC: 1.0000 1.0000 1.0000 1.0000
Validation:	 ACC: 0.7908 0.7903 0.7804 0.8011
Validation:	 Best_BACC: 0.8162 0.8154 0.7998 0.8326
Validation:	 Loss: 1.4318
Pretraining:	Epoch 192/200
----------
training:	Epoch: [192][1/233]	Loss 0.0001 (0.0001)	
training:	Epoch: [192][2/233]	Loss 0.0014 (0.0008)	
training:	Epoch: [192][3/233]	Loss 0.0001 (0.0005)	
training:	Epoch: [192][4/233]	Loss 0.0001 (0.0004)	
training:	Epoch: [192][5/233]	Loss 0.0002 (0.0004)	
training:	Epoch: [192][6/233]	Loss 0.0001 (0.0003)	
training:	Epoch: [192][7/233]	Loss 0.0034 (0.0008)	
training:	Epoch: [192][8/233]	Loss 0.0002 (0.0007)	
training:	Epoch: [192][9/233]	Loss 0.0001 (0.0006)	
training:	Epoch: [192][10/233]	Loss 0.0004 (0.0006)	
training:	Epoch: [192][11/233]	Loss 0.0002 (0.0006)	
training:	Epoch: [192][12/233]	Loss 0.0016 (0.0007)	
training:	Epoch: [192][13/233]	Loss 0.0001 (0.0006)	
training:	Epoch: [192][14/233]	Loss 0.0002 (0.0006)	
training:	Epoch: [192][15/233]	Loss 0.0002 (0.0006)	
training:	Epoch: [192][16/233]	Loss 0.0002 (0.0005)	
training:	Epoch: [192][17/233]	Loss 0.0003 (0.0005)	
training:	Epoch: [192][18/233]	Loss 0.0002 (0.0005)	
training:	Epoch: [192][19/233]	Loss 0.0001 (0.0005)	
training:	Epoch: [192][20/233]	Loss 0.0002 (0.0005)	
training:	Epoch: [192][21/233]	Loss 0.0001 (0.0005)	
training:	Epoch: [192][22/233]	Loss 0.0001 (0.0004)	
training:	Epoch: [192][23/233]	Loss 0.0002 (0.0004)	
training:	Epoch: [192][24/233]	Loss 0.0002 (0.0004)	
training:	Epoch: [192][25/233]	Loss 0.0002 (0.0004)	
training:	Epoch: [192][26/233]	Loss 0.0014 (0.0004)	
training:	Epoch: [192][27/233]	Loss 0.0001 (0.0004)	
training:	Epoch: [192][28/233]	Loss 0.0001 (0.0004)	
training:	Epoch: [192][29/233]	Loss 0.0003 (0.0004)	
training:	Epoch: [192][30/233]	Loss 0.0007 (0.0004)	
training:	Epoch: [192][31/233]	Loss 0.0002 (0.0004)	
training:	Epoch: [192][32/233]	Loss 0.0002 (0.0004)	
training:	Epoch: [192][33/233]	Loss 0.0015 (0.0004)	
training:	Epoch: [192][34/233]	Loss 0.0002 (0.0004)	
training:	Epoch: [192][35/233]	Loss 0.0002 (0.0004)	
training:	Epoch: [192][36/233]	Loss 0.0001 (0.0004)	
training:	Epoch: [192][37/233]	Loss 0.0001 (0.0004)	
training:	Epoch: [192][38/233]	Loss 0.0001 (0.0004)	
training:	Epoch: [192][39/233]	Loss 0.0007 (0.0004)	
training:	Epoch: [192][40/233]	Loss 0.0001 (0.0004)	
training:	Epoch: [192][41/233]	Loss 0.0001 (0.0004)	
training:	Epoch: [192][42/233]	Loss 0.0001 (0.0004)	
training:	Epoch: [192][43/233]	Loss 0.0002 (0.0004)	
training:	Epoch: [192][44/233]	Loss 0.0013 (0.0004)	
training:	Epoch: [192][45/233]	Loss 0.0001 (0.0004)	
training:	Epoch: [192][46/233]	Loss 0.0003 (0.0004)	
training:	Epoch: [192][47/233]	Loss 0.0001 (0.0004)	
training:	Epoch: [192][48/233]	Loss 0.0002 (0.0004)	
training:	Epoch: [192][49/233]	Loss 0.0002 (0.0004)	
training:	Epoch: [192][50/233]	Loss 0.0001 (0.0004)	
training:	Epoch: [192][51/233]	Loss 0.1496 (0.0033)	
training:	Epoch: [192][52/233]	Loss 0.0001 (0.0032)	
training:	Epoch: [192][53/233]	Loss 0.0001 (0.0032)	
training:	Epoch: [192][54/233]	Loss 0.0001 (0.0031)	
training:	Epoch: [192][55/233]	Loss 0.0001 (0.0031)	
training:	Epoch: [192][56/233]	Loss 0.0001 (0.0030)	
training:	Epoch: [192][57/233]	Loss 0.0002 (0.0030)	
training:	Epoch: [192][58/233]	Loss 0.0002 (0.0029)	
training:	Epoch: [192][59/233]	Loss 0.0001 (0.0029)	
training:	Epoch: [192][60/233]	Loss 0.0001 (0.0028)	
training:	Epoch: [192][61/233]	Loss 0.0001 (0.0028)	
training:	Epoch: [192][62/233]	Loss 0.0050 (0.0028)	
training:	Epoch: [192][63/233]	Loss 0.0001 (0.0028)	
training:	Epoch: [192][64/233]	Loss 0.1286 (0.0047)	
training:	Epoch: [192][65/233]	Loss 0.0038 (0.0047)	
training:	Epoch: [192][66/233]	Loss 0.0002 (0.0047)	
training:	Epoch: [192][67/233]	Loss 0.0006 (0.0046)	
training:	Epoch: [192][68/233]	Loss 0.0001 (0.0045)	
training:	Epoch: [192][69/233]	Loss 0.0002 (0.0045)	
training:	Epoch: [192][70/233]	Loss 0.0002 (0.0044)	
training:	Epoch: [192][71/233]	Loss 0.0006 (0.0044)	
training:	Epoch: [192][72/233]	Loss 0.0001 (0.0043)	
training:	Epoch: [192][73/233]	Loss 0.0002 (0.0042)	
training:	Epoch: [192][74/233]	Loss 0.0001 (0.0042)	
training:	Epoch: [192][75/233]	Loss 0.0028 (0.0042)	
training:	Epoch: [192][76/233]	Loss 0.0012 (0.0041)	
training:	Epoch: [192][77/233]	Loss 0.0006 (0.0041)	
training:	Epoch: [192][78/233]	Loss 0.0014 (0.0041)	
training:	Epoch: [192][79/233]	Loss 0.0055 (0.0041)	
training:	Epoch: [192][80/233]	Loss 0.0004 (0.0040)	
training:	Epoch: [192][81/233]	Loss 0.0100 (0.0041)	
training:	Epoch: [192][82/233]	Loss 0.0002 (0.0040)	
training:	Epoch: [192][83/233]	Loss 0.0002 (0.0040)	
training:	Epoch: [192][84/233]	Loss 0.0006 (0.0040)	
training:	Epoch: [192][85/233]	Loss 0.0001 (0.0039)	
training:	Epoch: [192][86/233]	Loss 0.0002 (0.0039)	
training:	Epoch: [192][87/233]	Loss 0.0001 (0.0038)	
training:	Epoch: [192][88/233]	Loss 0.0013 (0.0038)	
training:	Epoch: [192][89/233]	Loss 0.0813 (0.0047)	
training:	Epoch: [192][90/233]	Loss 0.0002 (0.0046)	
training:	Epoch: [192][91/233]	Loss 0.0003 (0.0046)	
training:	Epoch: [192][92/233]	Loss 0.0002 (0.0045)	
training:	Epoch: [192][93/233]	Loss 0.0002 (0.0045)	
training:	Epoch: [192][94/233]	Loss 0.0002 (0.0044)	
training:	Epoch: [192][95/233]	Loss 0.0002 (0.0044)	
training:	Epoch: [192][96/233]	Loss 0.0002 (0.0043)	
training:	Epoch: [192][97/233]	Loss 0.0003 (0.0043)	
training:	Epoch: [192][98/233]	Loss 0.0001 (0.0043)	
training:	Epoch: [192][99/233]	Loss 0.0001 (0.0042)	
training:	Epoch: [192][100/233]	Loss 0.0002 (0.0042)	
training:	Epoch: [192][101/233]	Loss 0.0001 (0.0041)	
training:	Epoch: [192][102/233]	Loss 0.0007 (0.0041)	
training:	Epoch: [192][103/233]	Loss 0.0001 (0.0041)	
training:	Epoch: [192][104/233]	Loss 0.0003 (0.0040)	
training:	Epoch: [192][105/233]	Loss 0.0002 (0.0040)	
training:	Epoch: [192][106/233]	Loss 0.0004 (0.0040)	
training:	Epoch: [192][107/233]	Loss 0.0032 (0.0040)	
training:	Epoch: [192][108/233]	Loss 0.0003 (0.0039)	
training:	Epoch: [192][109/233]	Loss 0.0011 (0.0039)	
training:	Epoch: [192][110/233]	Loss 0.0007 (0.0039)	
training:	Epoch: [192][111/233]	Loss 0.0002 (0.0038)	
training:	Epoch: [192][112/233]	Loss 0.0003 (0.0038)	
training:	Epoch: [192][113/233]	Loss 0.0033 (0.0038)	
training:	Epoch: [192][114/233]	Loss 0.0001 (0.0038)	
training:	Epoch: [192][115/233]	Loss 0.0001 (0.0037)	
training:	Epoch: [192][116/233]	Loss 0.0035 (0.0037)	
training:	Epoch: [192][117/233]	Loss 0.0002 (0.0037)	
training:	Epoch: [192][118/233]	Loss 0.0001 (0.0037)	
training:	Epoch: [192][119/233]	Loss 0.0320 (0.0039)	
training:	Epoch: [192][120/233]	Loss 0.0565 (0.0043)	
training:	Epoch: [192][121/233]	Loss 0.0001 (0.0043)	
training:	Epoch: [192][122/233]	Loss 0.0118 (0.0044)	
training:	Epoch: [192][123/233]	Loss 0.0001 (0.0043)	
training:	Epoch: [192][124/233]	Loss 0.0090 (0.0044)	
training:	Epoch: [192][125/233]	Loss 0.0002 (0.0043)	
training:	Epoch: [192][126/233]	Loss 0.0011 (0.0043)	
training:	Epoch: [192][127/233]	Loss 0.0001 (0.0043)	
training:	Epoch: [192][128/233]	Loss 0.0003 (0.0043)	
training:	Epoch: [192][129/233]	Loss 0.0002 (0.0042)	
training:	Epoch: [192][130/233]	Loss 0.0005 (0.0042)	
training:	Epoch: [192][131/233]	Loss 0.1230 (0.0051)	
training:	Epoch: [192][132/233]	Loss 0.0001 (0.0051)	
training:	Epoch: [192][133/233]	Loss 0.0004 (0.0050)	
training:	Epoch: [192][134/233]	Loss 0.0002 (0.0050)	
training:	Epoch: [192][135/233]	Loss 0.0001 (0.0050)	
training:	Epoch: [192][136/233]	Loss 0.0007 (0.0049)	
training:	Epoch: [192][137/233]	Loss 0.0002 (0.0049)	
training:	Epoch: [192][138/233]	Loss 0.0004 (0.0049)	
training:	Epoch: [192][139/233]	Loss 0.0002 (0.0048)	
training:	Epoch: [192][140/233]	Loss 0.0001 (0.0048)	
training:	Epoch: [192][141/233]	Loss 0.0004 (0.0048)	
training:	Epoch: [192][142/233]	Loss 0.0021 (0.0047)	
training:	Epoch: [192][143/233]	Loss 0.0003 (0.0047)	
training:	Epoch: [192][144/233]	Loss 0.0002 (0.0047)	
training:	Epoch: [192][145/233]	Loss 0.0002 (0.0046)	
training:	Epoch: [192][146/233]	Loss 0.0001 (0.0046)	
training:	Epoch: [192][147/233]	Loss 0.0002 (0.0046)	
training:	Epoch: [192][148/233]	Loss 0.0040 (0.0046)	
training:	Epoch: [192][149/233]	Loss 0.0002 (0.0045)	
training:	Epoch: [192][150/233]	Loss 0.0002 (0.0045)	
training:	Epoch: [192][151/233]	Loss 0.0001 (0.0045)	
training:	Epoch: [192][152/233]	Loss 0.0008 (0.0045)	
training:	Epoch: [192][153/233]	Loss 0.0002 (0.0044)	
training:	Epoch: [192][154/233]	Loss 0.0001 (0.0044)	
training:	Epoch: [192][155/233]	Loss 0.0001 (0.0044)	
training:	Epoch: [192][156/233]	Loss 0.0004 (0.0044)	
training:	Epoch: [192][157/233]	Loss 0.0002 (0.0043)	
training:	Epoch: [192][158/233]	Loss 0.0004 (0.0043)	
training:	Epoch: [192][159/233]	Loss 0.0002 (0.0043)	
training:	Epoch: [192][160/233]	Loss 0.0001 (0.0043)	
training:	Epoch: [192][161/233]	Loss 0.0007 (0.0042)	
training:	Epoch: [192][162/233]	Loss 0.0002 (0.0042)	
training:	Epoch: [192][163/233]	Loss 0.0129 (0.0043)	
training:	Epoch: [192][164/233]	Loss 0.0002 (0.0042)	
training:	Epoch: [192][165/233]	Loss 0.0017 (0.0042)	
training:	Epoch: [192][166/233]	Loss 0.0002 (0.0042)	
training:	Epoch: [192][167/233]	Loss 0.0722 (0.0046)	
training:	Epoch: [192][168/233]	Loss 0.0001 (0.0046)	
training:	Epoch: [192][169/233]	Loss 0.0041 (0.0046)	
training:	Epoch: [192][170/233]	Loss 0.0002 (0.0045)	
training:	Epoch: [192][171/233]	Loss 0.0002 (0.0045)	
training:	Epoch: [192][172/233]	Loss 0.0006 (0.0045)	
training:	Epoch: [192][173/233]	Loss 0.0002 (0.0045)	
training:	Epoch: [192][174/233]	Loss 0.0002 (0.0045)	
training:	Epoch: [192][175/233]	Loss 0.0002 (0.0044)	
training:	Epoch: [192][176/233]	Loss 0.0001 (0.0044)	
training:	Epoch: [192][177/233]	Loss 0.0002 (0.0044)	
training:	Epoch: [192][178/233]	Loss 0.0002 (0.0044)	
training:	Epoch: [192][179/233]	Loss 0.0009 (0.0043)	
training:	Epoch: [192][180/233]	Loss 0.0019 (0.0043)	
training:	Epoch: [192][181/233]	Loss 0.0002 (0.0043)	
training:	Epoch: [192][182/233]	Loss 0.0343 (0.0045)	
training:	Epoch: [192][183/233]	Loss 0.0001 (0.0044)	
training:	Epoch: [192][184/233]	Loss 0.0001 (0.0044)	
training:	Epoch: [192][185/233]	Loss 0.0031 (0.0044)	
training:	Epoch: [192][186/233]	Loss 0.0001 (0.0044)	
training:	Epoch: [192][187/233]	Loss 0.0002 (0.0044)	
training:	Epoch: [192][188/233]	Loss 0.0002 (0.0043)	
training:	Epoch: [192][189/233]	Loss 0.0002 (0.0043)	
training:	Epoch: [192][190/233]	Loss 0.0002 (0.0043)	
training:	Epoch: [192][191/233]	Loss 0.0002 (0.0043)	
training:	Epoch: [192][192/233]	Loss 0.0001 (0.0043)	
training:	Epoch: [192][193/233]	Loss 0.0002 (0.0042)	
training:	Epoch: [192][194/233]	Loss 0.0270 (0.0044)	
training:	Epoch: [192][195/233]	Loss 0.0020 (0.0043)	
training:	Epoch: [192][196/233]	Loss 0.0001 (0.0043)	
training:	Epoch: [192][197/233]	Loss 0.0002 (0.0043)	
training:	Epoch: [192][198/233]	Loss 0.0002 (0.0043)	
training:	Epoch: [192][199/233]	Loss 0.0002 (0.0043)	
training:	Epoch: [192][200/233]	Loss 0.0001 (0.0042)	
training:	Epoch: [192][201/233]	Loss 0.0002 (0.0042)	
training:	Epoch: [192][202/233]	Loss 0.0009 (0.0042)	
training:	Epoch: [192][203/233]	Loss 0.0002 (0.0042)	
training:	Epoch: [192][204/233]	Loss 0.0002 (0.0042)	
training:	Epoch: [192][205/233]	Loss 0.0001 (0.0041)	
training:	Epoch: [192][206/233]	Loss 0.0002 (0.0041)	
training:	Epoch: [192][207/233]	Loss 0.0001 (0.0041)	
training:	Epoch: [192][208/233]	Loss 0.0003 (0.0041)	
training:	Epoch: [192][209/233]	Loss 0.0002 (0.0041)	
training:	Epoch: [192][210/233]	Loss 0.0001 (0.0040)	
training:	Epoch: [192][211/233]	Loss 0.0001 (0.0040)	
training:	Epoch: [192][212/233]	Loss 0.0002 (0.0040)	
training:	Epoch: [192][213/233]	Loss 0.0013 (0.0040)	
training:	Epoch: [192][214/233]	Loss 0.0002 (0.0040)	
training:	Epoch: [192][215/233]	Loss 0.0001 (0.0040)	
training:	Epoch: [192][216/233]	Loss 0.0001 (0.0039)	
training:	Epoch: [192][217/233]	Loss 0.0005 (0.0039)	
training:	Epoch: [192][218/233]	Loss 0.0001 (0.0039)	
training:	Epoch: [192][219/233]	Loss 0.0001 (0.0039)	
training:	Epoch: [192][220/233]	Loss 0.0001 (0.0039)	
training:	Epoch: [192][221/233]	Loss 0.0002 (0.0039)	
training:	Epoch: [192][222/233]	Loss 0.0001 (0.0038)	
training:	Epoch: [192][223/233]	Loss 0.0001 (0.0038)	
training:	Epoch: [192][224/233]	Loss 0.0001 (0.0038)	
training:	Epoch: [192][225/233]	Loss 0.0002 (0.0038)	
training:	Epoch: [192][226/233]	Loss 0.0001 (0.0038)	
training:	Epoch: [192][227/233]	Loss 0.0001 (0.0038)	
training:	Epoch: [192][228/233]	Loss 0.0019 (0.0038)	
training:	Epoch: [192][229/233]	Loss 0.0010 (0.0037)	
training:	Epoch: [192][230/233]	Loss 0.0008 (0.0037)	
training:	Epoch: [192][231/233]	Loss 0.0001 (0.0037)	
training:	Epoch: [192][232/233]	Loss 0.0001 (0.0037)	
training:	Epoch: [192][233/233]	Loss 0.0001 (0.0037)	
Training:	 Loss: 0.0037

Training:	 ACC: 1.0000 1.0000 1.0000 1.0000
Validation:	 ACC: 0.7906 0.7892 0.7610 0.8202
Validation:	 Best_BACC: 0.8162 0.8154 0.7998 0.8326
Validation:	 Loss: 1.3979
Pretraining:	Epoch 193/200
----------
training:	Epoch: [193][1/233]	Loss 0.0002 (0.0002)	
training:	Epoch: [193][2/233]	Loss 0.0002 (0.0002)	
training:	Epoch: [193][3/233]	Loss 0.0002 (0.0002)	
training:	Epoch: [193][4/233]	Loss 0.0002 (0.0002)	
training:	Epoch: [193][5/233]	Loss 0.0014 (0.0004)	
training:	Epoch: [193][6/233]	Loss 0.0005 (0.0004)	
training:	Epoch: [193][7/233]	Loss 0.0574 (0.0086)	
training:	Epoch: [193][8/233]	Loss 0.0001 (0.0075)	
training:	Epoch: [193][9/233]	Loss 0.0004 (0.0067)	
training:	Epoch: [193][10/233]	Loss 0.0002 (0.0061)	
training:	Epoch: [193][11/233]	Loss 0.0005 (0.0056)	
training:	Epoch: [193][12/233]	Loss 0.0003 (0.0051)	
training:	Epoch: [193][13/233]	Loss 0.0200 (0.0063)	
training:	Epoch: [193][14/233]	Loss 0.0001 (0.0058)	
training:	Epoch: [193][15/233]	Loss 0.0001 (0.0055)	
training:	Epoch: [193][16/233]	Loss 0.0060 (0.0055)	
training:	Epoch: [193][17/233]	Loss 0.0040 (0.0054)	
training:	Epoch: [193][18/233]	Loss 0.0001 (0.0051)	
training:	Epoch: [193][19/233]	Loss 0.0002 (0.0048)	
training:	Epoch: [193][20/233]	Loss 0.0001 (0.0046)	
training:	Epoch: [193][21/233]	Loss 0.0016 (0.0045)	
training:	Epoch: [193][22/233]	Loss 0.0011 (0.0043)	
training:	Epoch: [193][23/233]	Loss 0.0005 (0.0042)	
training:	Epoch: [193][24/233]	Loss 0.0003 (0.0040)	
training:	Epoch: [193][25/233]	Loss 0.0002 (0.0038)	
training:	Epoch: [193][26/233]	Loss 0.0002 (0.0037)	
training:	Epoch: [193][27/233]	Loss 0.0002 (0.0036)	
training:	Epoch: [193][28/233]	Loss 0.0003 (0.0035)	
training:	Epoch: [193][29/233]	Loss 0.0003 (0.0033)	
training:	Epoch: [193][30/233]	Loss 0.0002 (0.0032)	
training:	Epoch: [193][31/233]	Loss 0.0015 (0.0032)	
training:	Epoch: [193][32/233]	Loss 0.0001 (0.0031)	
training:	Epoch: [193][33/233]	Loss 0.0002 (0.0030)	
training:	Epoch: [193][34/233]	Loss 0.0002 (0.0029)	
training:	Epoch: [193][35/233]	Loss 0.0008 (0.0029)	
training:	Epoch: [193][36/233]	Loss 0.0001 (0.0028)	
training:	Epoch: [193][37/233]	Loss 0.0001 (0.0027)	
training:	Epoch: [193][38/233]	Loss 0.0002 (0.0026)	
training:	Epoch: [193][39/233]	Loss 0.0002 (0.0026)	
training:	Epoch: [193][40/233]	Loss 0.0002 (0.0025)	
training:	Epoch: [193][41/233]	Loss 0.0002 (0.0025)	
training:	Epoch: [193][42/233]	Loss 0.0002 (0.0024)	
training:	Epoch: [193][43/233]	Loss 0.0001 (0.0024)	
training:	Epoch: [193][44/233]	Loss 0.0001 (0.0023)	
training:	Epoch: [193][45/233]	Loss 0.0001 (0.0023)	
training:	Epoch: [193][46/233]	Loss 0.0001 (0.0022)	
training:	Epoch: [193][47/233]	Loss 0.0001 (0.0022)	
training:	Epoch: [193][48/233]	Loss 0.0002 (0.0021)	
training:	Epoch: [193][49/233]	Loss 0.0001 (0.0021)	
training:	Epoch: [193][50/233]	Loss 0.0002 (0.0020)	
training:	Epoch: [193][51/233]	Loss 0.0003 (0.0020)	
training:	Epoch: [193][52/233]	Loss 0.0004 (0.0020)	
training:	Epoch: [193][53/233]	Loss 0.0001 (0.0019)	
training:	Epoch: [193][54/233]	Loss 0.0002 (0.0019)	
training:	Epoch: [193][55/233]	Loss 0.0001 (0.0019)	
training:	Epoch: [193][56/233]	Loss 0.0003 (0.0018)	
training:	Epoch: [193][57/233]	Loss 0.0002 (0.0018)	
training:	Epoch: [193][58/233]	Loss 0.0001 (0.0018)	
training:	Epoch: [193][59/233]	Loss 0.0012 (0.0018)	
training:	Epoch: [193][60/233]	Loss 0.0001 (0.0018)	
training:	Epoch: [193][61/233]	Loss 0.0001 (0.0017)	
training:	Epoch: [193][62/233]	Loss 0.0008 (0.0017)	
training:	Epoch: [193][63/233]	Loss 0.0001 (0.0017)	
training:	Epoch: [193][64/233]	Loss 0.0001 (0.0017)	
training:	Epoch: [193][65/233]	Loss 0.0002 (0.0016)	
training:	Epoch: [193][66/233]	Loss 0.0001 (0.0016)	
training:	Epoch: [193][67/233]	Loss 0.0002 (0.0016)	
training:	Epoch: [193][68/233]	Loss 0.0001 (0.0016)	
training:	Epoch: [193][69/233]	Loss 0.0002 (0.0016)	
training:	Epoch: [193][70/233]	Loss 0.0001 (0.0015)	
training:	Epoch: [193][71/233]	Loss 0.0001 (0.0015)	
training:	Epoch: [193][72/233]	Loss 0.0002 (0.0015)	
training:	Epoch: [193][73/233]	Loss 0.0010 (0.0015)	
training:	Epoch: [193][74/233]	Loss 0.0001 (0.0015)	
training:	Epoch: [193][75/233]	Loss 0.0001 (0.0015)	
training:	Epoch: [193][76/233]	Loss 0.0001 (0.0014)	
training:	Epoch: [193][77/233]	Loss 0.0005 (0.0014)	
training:	Epoch: [193][78/233]	Loss 0.0001 (0.0014)	
training:	Epoch: [193][79/233]	Loss 0.0005 (0.0014)	
training:	Epoch: [193][80/233]	Loss 0.0001 (0.0014)	
training:	Epoch: [193][81/233]	Loss 0.0002 (0.0014)	
training:	Epoch: [193][82/233]	Loss 0.0002 (0.0013)	
training:	Epoch: [193][83/233]	Loss 0.0003 (0.0013)	
training:	Epoch: [193][84/233]	Loss 0.0001 (0.0013)	
training:	Epoch: [193][85/233]	Loss 0.0002 (0.0013)	
training:	Epoch: [193][86/233]	Loss 0.0586 (0.0020)	
training:	Epoch: [193][87/233]	Loss 0.0003 (0.0020)	
training:	Epoch: [193][88/233]	Loss 0.0001 (0.0019)	
training:	Epoch: [193][89/233]	Loss 0.0001 (0.0019)	
training:	Epoch: [193][90/233]	Loss 0.0001 (0.0019)	
training:	Epoch: [193][91/233]	Loss 0.0002 (0.0019)	
training:	Epoch: [193][92/233]	Loss 0.0001 (0.0019)	
training:	Epoch: [193][93/233]	Loss 0.0002 (0.0018)	
training:	Epoch: [193][94/233]	Loss 0.0002 (0.0018)	
training:	Epoch: [193][95/233]	Loss 0.0002 (0.0018)	
training:	Epoch: [193][96/233]	Loss 0.0002 (0.0018)	
training:	Epoch: [193][97/233]	Loss 0.0001 (0.0018)	
training:	Epoch: [193][98/233]	Loss 0.0001 (0.0018)	
training:	Epoch: [193][99/233]	Loss 0.0001 (0.0017)	
training:	Epoch: [193][100/233]	Loss 0.0002 (0.0017)	
training:	Epoch: [193][101/233]	Loss 0.0002 (0.0017)	
training:	Epoch: [193][102/233]	Loss 0.0002 (0.0017)	
training:	Epoch: [193][103/233]	Loss 0.0002 (0.0017)	
training:	Epoch: [193][104/233]	Loss 0.0001 (0.0017)	
training:	Epoch: [193][105/233]	Loss 0.0001 (0.0016)	
training:	Epoch: [193][106/233]	Loss 0.0001 (0.0016)	
training:	Epoch: [193][107/233]	Loss 0.0002 (0.0016)	
training:	Epoch: [193][108/233]	Loss 0.0001 (0.0016)	
training:	Epoch: [193][109/233]	Loss 0.0002 (0.0016)	
training:	Epoch: [193][110/233]	Loss 0.0003 (0.0016)	
training:	Epoch: [193][111/233]	Loss 0.0001 (0.0016)	
training:	Epoch: [193][112/233]	Loss 0.0002 (0.0016)	
training:	Epoch: [193][113/233]	Loss 0.0002 (0.0015)	
training:	Epoch: [193][114/233]	Loss 0.0008 (0.0015)	
training:	Epoch: [193][115/233]	Loss 0.0115 (0.0016)	
training:	Epoch: [193][116/233]	Loss 0.0002 (0.0016)	
training:	Epoch: [193][117/233]	Loss 0.0001 (0.0016)	
training:	Epoch: [193][118/233]	Loss 0.0002 (0.0016)	
training:	Epoch: [193][119/233]	Loss 0.0001 (0.0016)	
training:	Epoch: [193][120/233]	Loss 0.0003 (0.0016)	
training:	Epoch: [193][121/233]	Loss 0.0005 (0.0016)	
training:	Epoch: [193][122/233]	Loss 0.0001 (0.0015)	
training:	Epoch: [193][123/233]	Loss 0.0002 (0.0015)	
training:	Epoch: [193][124/233]	Loss 0.0002 (0.0015)	
training:	Epoch: [193][125/233]	Loss 0.0002 (0.0015)	
training:	Epoch: [193][126/233]	Loss 0.0114 (0.0016)	
training:	Epoch: [193][127/233]	Loss 0.0001 (0.0016)	
training:	Epoch: [193][128/233]	Loss 0.0005 (0.0016)	
training:	Epoch: [193][129/233]	Loss 0.0002 (0.0016)	
training:	Epoch: [193][130/233]	Loss 0.0001 (0.0015)	
training:	Epoch: [193][131/233]	Loss 0.0001 (0.0015)	
training:	Epoch: [193][132/233]	Loss 0.0073 (0.0016)	
training:	Epoch: [193][133/233]	Loss 0.0002 (0.0016)	
training:	Epoch: [193][134/233]	Loss 0.0001 (0.0016)	
training:	Epoch: [193][135/233]	Loss 0.0001 (0.0015)	
training:	Epoch: [193][136/233]	Loss 0.0008 (0.0015)	
training:	Epoch: [193][137/233]	Loss 0.0002 (0.0015)	
training:	Epoch: [193][138/233]	Loss 0.0006 (0.0015)	
training:	Epoch: [193][139/233]	Loss 0.0001 (0.0015)	
training:	Epoch: [193][140/233]	Loss 0.0002 (0.0015)	
training:	Epoch: [193][141/233]	Loss 0.0001 (0.0015)	
training:	Epoch: [193][142/233]	Loss 0.0001 (0.0015)	
training:	Epoch: [193][143/233]	Loss 0.0060 (0.0015)	
training:	Epoch: [193][144/233]	Loss 0.0002 (0.0015)	
training:	Epoch: [193][145/233]	Loss 0.0003 (0.0015)	
training:	Epoch: [193][146/233]	Loss 0.0002 (0.0015)	
training:	Epoch: [193][147/233]	Loss 0.0002 (0.0015)	
training:	Epoch: [193][148/233]	Loss 0.0004 (0.0015)	
training:	Epoch: [193][149/233]	Loss 0.0002 (0.0015)	
training:	Epoch: [193][150/233]	Loss 0.0001 (0.0015)	
training:	Epoch: [193][151/233]	Loss 0.0001 (0.0014)	
training:	Epoch: [193][152/233]	Loss 0.0002 (0.0014)	
training:	Epoch: [193][153/233]	Loss 0.0002 (0.0014)	
training:	Epoch: [193][154/233]	Loss 0.0001 (0.0014)	
training:	Epoch: [193][155/233]	Loss 0.0002 (0.0014)	
training:	Epoch: [193][156/233]	Loss 0.0002 (0.0014)	
training:	Epoch: [193][157/233]	Loss 0.0001 (0.0014)	
training:	Epoch: [193][158/233]	Loss 0.0027 (0.0014)	
training:	Epoch: [193][159/233]	Loss 0.0002 (0.0014)	
training:	Epoch: [193][160/233]	Loss 0.0001 (0.0014)	
training:	Epoch: [193][161/233]	Loss 0.0001 (0.0014)	
training:	Epoch: [193][162/233]	Loss 0.0002 (0.0014)	
training:	Epoch: [193][163/233]	Loss 0.0002 (0.0014)	
training:	Epoch: [193][164/233]	Loss 0.0001 (0.0014)	
training:	Epoch: [193][165/233]	Loss 0.0003 (0.0014)	
training:	Epoch: [193][166/233]	Loss 0.0001 (0.0013)	
training:	Epoch: [193][167/233]	Loss 0.0001 (0.0013)	
training:	Epoch: [193][168/233]	Loss 0.0001 (0.0013)	
training:	Epoch: [193][169/233]	Loss 0.0785 (0.0018)	
training:	Epoch: [193][170/233]	Loss 0.0001 (0.0018)	
training:	Epoch: [193][171/233]	Loss 0.0002 (0.0018)	
training:	Epoch: [193][172/233]	Loss 0.0002 (0.0018)	
training:	Epoch: [193][173/233]	Loss 0.0012 (0.0018)	
training:	Epoch: [193][174/233]	Loss 0.0008 (0.0018)	
training:	Epoch: [193][175/233]	Loss 0.0002 (0.0017)	
training:	Epoch: [193][176/233]	Loss 0.0002 (0.0017)	
training:	Epoch: [193][177/233]	Loss 0.0003 (0.0017)	
training:	Epoch: [193][178/233]	Loss 0.0002 (0.0017)	
training:	Epoch: [193][179/233]	Loss 0.0002 (0.0017)	
training:	Epoch: [193][180/233]	Loss 0.0002 (0.0017)	
training:	Epoch: [193][181/233]	Loss 0.0001 (0.0017)	
training:	Epoch: [193][182/233]	Loss 0.0002 (0.0017)	
training:	Epoch: [193][183/233]	Loss 0.0001 (0.0017)	
training:	Epoch: [193][184/233]	Loss 0.0001 (0.0017)	
training:	Epoch: [193][185/233]	Loss 0.0002 (0.0017)	
training:	Epoch: [193][186/233]	Loss 0.0009 (0.0017)	
training:	Epoch: [193][187/233]	Loss 0.0003 (0.0016)	
training:	Epoch: [193][188/233]	Loss 0.0001 (0.0016)	
training:	Epoch: [193][189/233]	Loss 0.0002 (0.0016)	
training:	Epoch: [193][190/233]	Loss 0.0002 (0.0016)	
training:	Epoch: [193][191/233]	Loss 0.0002 (0.0016)	
training:	Epoch: [193][192/233]	Loss 0.0004 (0.0016)	
training:	Epoch: [193][193/233]	Loss 0.0001 (0.0016)	
training:	Epoch: [193][194/233]	Loss 0.0003 (0.0016)	
training:	Epoch: [193][195/233]	Loss 0.0006 (0.0016)	
training:	Epoch: [193][196/233]	Loss 0.0002 (0.0016)	
training:	Epoch: [193][197/233]	Loss 0.0005 (0.0016)	
training:	Epoch: [193][198/233]	Loss 0.0005 (0.0016)	
training:	Epoch: [193][199/233]	Loss 0.0002 (0.0016)	
training:	Epoch: [193][200/233]	Loss 0.0001 (0.0016)	
training:	Epoch: [193][201/233]	Loss 0.0001 (0.0016)	
training:	Epoch: [193][202/233]	Loss 0.0015 (0.0016)	
training:	Epoch: [193][203/233]	Loss 0.0027 (0.0016)	
training:	Epoch: [193][204/233]	Loss 0.0001 (0.0015)	
training:	Epoch: [193][205/233]	Loss 0.0023 (0.0016)	
training:	Epoch: [193][206/233]	Loss 0.0004 (0.0015)	
training:	Epoch: [193][207/233]	Loss 0.0002 (0.0015)	
training:	Epoch: [193][208/233]	Loss 0.0001 (0.0015)	
training:	Epoch: [193][209/233]	Loss 0.0505 (0.0018)	
training:	Epoch: [193][210/233]	Loss 0.0001 (0.0018)	
training:	Epoch: [193][211/233]	Loss 0.0306 (0.0019)	
training:	Epoch: [193][212/233]	Loss 0.0001 (0.0019)	
training:	Epoch: [193][213/233]	Loss 0.0001 (0.0019)	
training:	Epoch: [193][214/233]	Loss 0.0001 (0.0019)	
training:	Epoch: [193][215/233]	Loss 0.0002 (0.0019)	
training:	Epoch: [193][216/233]	Loss 0.0001 (0.0019)	
training:	Epoch: [193][217/233]	Loss 0.0002 (0.0018)	
training:	Epoch: [193][218/233]	Loss 0.0001 (0.0018)	
training:	Epoch: [193][219/233]	Loss 0.0001 (0.0018)	
training:	Epoch: [193][220/233]	Loss 0.0030 (0.0018)	
training:	Epoch: [193][221/233]	Loss 0.0002 (0.0018)	
training:	Epoch: [193][222/233]	Loss 0.0002 (0.0018)	
training:	Epoch: [193][223/233]	Loss 0.0002 (0.0018)	
training:	Epoch: [193][224/233]	Loss 0.0030 (0.0018)	
training:	Epoch: [193][225/233]	Loss 0.0002 (0.0018)	
training:	Epoch: [193][226/233]	Loss 0.0003 (0.0018)	
training:	Epoch: [193][227/233]	Loss 0.0002 (0.0018)	
training:	Epoch: [193][228/233]	Loss 0.0002 (0.0018)	
training:	Epoch: [193][229/233]	Loss 0.0001 (0.0018)	
training:	Epoch: [193][230/233]	Loss 0.0003 (0.0018)	
training:	Epoch: [193][231/233]	Loss 0.0001 (0.0018)	
training:	Epoch: [193][232/233]	Loss 0.0005 (0.0018)	
training:	Epoch: [193][233/233]	Loss 0.0020 (0.0018)	
Training:	 Loss: 0.0018

Training:	 ACC: 1.0000 1.0000 1.0000 1.0000
Validation:	 ACC: 0.7858 0.7833 0.7334 0.8382
Validation:	 Best_BACC: 0.8162 0.8154 0.7998 0.8326
Validation:	 Loss: 1.4498
Pretraining:	Epoch 194/200
----------
training:	Epoch: [194][1/233]	Loss 0.0001 (0.0001)	
training:	Epoch: [194][2/233]	Loss 0.0001 (0.0001)	
training:	Epoch: [194][3/233]	Loss 0.0002 (0.0002)	
training:	Epoch: [194][4/233]	Loss 0.0002 (0.0002)	
training:	Epoch: [194][5/233]	Loss 0.0002 (0.0002)	
training:	Epoch: [194][6/233]	Loss 0.0005 (0.0002)	
training:	Epoch: [194][7/233]	Loss 0.0001 (0.0002)	
training:	Epoch: [194][8/233]	Loss 0.0002 (0.0002)	
training:	Epoch: [194][9/233]	Loss 0.0036 (0.0006)	
training:	Epoch: [194][10/233]	Loss 0.0001 (0.0005)	
training:	Epoch: [194][11/233]	Loss 0.0476 (0.0048)	
training:	Epoch: [194][12/233]	Loss 0.0003 (0.0044)	
training:	Epoch: [194][13/233]	Loss 0.0001 (0.0041)	
training:	Epoch: [194][14/233]	Loss 0.0001 (0.0038)	
training:	Epoch: [194][15/233]	Loss 0.0003 (0.0036)	
training:	Epoch: [194][16/233]	Loss 0.0005 (0.0034)	
training:	Epoch: [194][17/233]	Loss 0.0001 (0.0032)	
training:	Epoch: [194][18/233]	Loss 0.0002 (0.0030)	
training:	Epoch: [194][19/233]	Loss 0.0003 (0.0029)	
training:	Epoch: [194][20/233]	Loss 0.0002 (0.0028)	
training:	Epoch: [194][21/233]	Loss 0.0012 (0.0027)	
training:	Epoch: [194][22/233]	Loss 0.0001 (0.0026)	
training:	Epoch: [194][23/233]	Loss 0.0001 (0.0025)	
training:	Epoch: [194][24/233]	Loss 0.0002 (0.0024)	
training:	Epoch: [194][25/233]	Loss 0.0001 (0.0023)	
training:	Epoch: [194][26/233]	Loss 0.0017 (0.0023)	
training:	Epoch: [194][27/233]	Loss 0.0001 (0.0022)	
training:	Epoch: [194][28/233]	Loss 0.0002 (0.0021)	
training:	Epoch: [194][29/233]	Loss 0.0002 (0.0020)	
training:	Epoch: [194][30/233]	Loss 0.0005 (0.0020)	
training:	Epoch: [194][31/233]	Loss 0.0001 (0.0019)	
training:	Epoch: [194][32/233]	Loss 0.0001 (0.0019)	
training:	Epoch: [194][33/233]	Loss 0.0002 (0.0018)	
training:	Epoch: [194][34/233]	Loss 0.0002 (0.0018)	
training:	Epoch: [194][35/233]	Loss 0.0004 (0.0017)	
training:	Epoch: [194][36/233]	Loss 0.0002 (0.0017)	
training:	Epoch: [194][37/233]	Loss 0.0005 (0.0017)	
training:	Epoch: [194][38/233]	Loss 0.0002 (0.0016)	
training:	Epoch: [194][39/233]	Loss 0.0002 (0.0016)	
training:	Epoch: [194][40/233]	Loss 0.0031 (0.0016)	
training:	Epoch: [194][41/233]	Loss 0.0001 (0.0016)	
training:	Epoch: [194][42/233]	Loss 0.0001 (0.0016)	
training:	Epoch: [194][43/233]	Loss 0.0003 (0.0015)	
training:	Epoch: [194][44/233]	Loss 0.0001 (0.0015)	
training:	Epoch: [194][45/233]	Loss 0.0001 (0.0015)	
training:	Epoch: [194][46/233]	Loss 0.0001 (0.0014)	
training:	Epoch: [194][47/233]	Loss 0.0002 (0.0014)	
training:	Epoch: [194][48/233]	Loss 0.0002 (0.0014)	
training:	Epoch: [194][49/233]	Loss 0.0001 (0.0014)	
training:	Epoch: [194][50/233]	Loss 0.0002 (0.0013)	
training:	Epoch: [194][51/233]	Loss 0.0014 (0.0013)	
training:	Epoch: [194][52/233]	Loss 0.0001 (0.0013)	
training:	Epoch: [194][53/233]	Loss 0.0001 (0.0013)	
training:	Epoch: [194][54/233]	Loss 0.0001 (0.0013)	
training:	Epoch: [194][55/233]	Loss 0.0001 (0.0012)	
training:	Epoch: [194][56/233]	Loss 0.0030 (0.0013)	
training:	Epoch: [194][57/233]	Loss 0.0001 (0.0013)	
training:	Epoch: [194][58/233]	Loss 0.0002 (0.0012)	
training:	Epoch: [194][59/233]	Loss 0.0037 (0.0013)	
training:	Epoch: [194][60/233]	Loss 0.0001 (0.0013)	
training:	Epoch: [194][61/233]	Loss 0.0001 (0.0012)	
training:	Epoch: [194][62/233]	Loss 0.0001 (0.0012)	
training:	Epoch: [194][63/233]	Loss 0.0001 (0.0012)	
training:	Epoch: [194][64/233]	Loss 0.0002 (0.0012)	
training:	Epoch: [194][65/233]	Loss 0.0001 (0.0012)	
training:	Epoch: [194][66/233]	Loss 0.0002 (0.0012)	
training:	Epoch: [194][67/233]	Loss 0.0003 (0.0011)	
training:	Epoch: [194][68/233]	Loss 0.0001 (0.0011)	
training:	Epoch: [194][69/233]	Loss 0.0040 (0.0012)	
training:	Epoch: [194][70/233]	Loss 0.0002 (0.0012)	
training:	Epoch: [194][71/233]	Loss 0.0002 (0.0011)	
training:	Epoch: [194][72/233]	Loss 0.0006 (0.0011)	
training:	Epoch: [194][73/233]	Loss 0.0001 (0.0011)	
training:	Epoch: [194][74/233]	Loss 0.1076 (0.0026)	
training:	Epoch: [194][75/233]	Loss 0.0001 (0.0025)	
training:	Epoch: [194][76/233]	Loss 0.0005 (0.0025)	
training:	Epoch: [194][77/233]	Loss 0.0001 (0.0025)	
training:	Epoch: [194][78/233]	Loss 0.0005 (0.0024)	
training:	Epoch: [194][79/233]	Loss 0.0002 (0.0024)	
training:	Epoch: [194][80/233]	Loss 0.0002 (0.0024)	
training:	Epoch: [194][81/233]	Loss 0.0001 (0.0024)	
training:	Epoch: [194][82/233]	Loss 0.0009 (0.0023)	
training:	Epoch: [194][83/233]	Loss 0.0005 (0.0023)	
training:	Epoch: [194][84/233]	Loss 0.0015 (0.0023)	
training:	Epoch: [194][85/233]	Loss 0.0002 (0.0023)	
training:	Epoch: [194][86/233]	Loss 0.0003 (0.0023)	
training:	Epoch: [194][87/233]	Loss 0.0005 (0.0022)	
training:	Epoch: [194][88/233]	Loss 0.0001 (0.0022)	
training:	Epoch: [194][89/233]	Loss 0.0002 (0.0022)	
training:	Epoch: [194][90/233]	Loss 0.0001 (0.0022)	
training:	Epoch: [194][91/233]	Loss 0.0002 (0.0022)	
training:	Epoch: [194][92/233]	Loss 0.0002 (0.0021)	
training:	Epoch: [194][93/233]	Loss 0.0005 (0.0021)	
training:	Epoch: [194][94/233]	Loss 0.0001 (0.0021)	
training:	Epoch: [194][95/233]	Loss 0.0007 (0.0021)	
training:	Epoch: [194][96/233]	Loss 0.0018 (0.0021)	
training:	Epoch: [194][97/233]	Loss 0.0001 (0.0021)	
training:	Epoch: [194][98/233]	Loss 0.0003 (0.0020)	
training:	Epoch: [194][99/233]	Loss 0.0002 (0.0020)	
training:	Epoch: [194][100/233]	Loss 0.0002 (0.0020)	
training:	Epoch: [194][101/233]	Loss 0.0003 (0.0020)	
training:	Epoch: [194][102/233]	Loss 0.0050 (0.0020)	
training:	Epoch: [194][103/233]	Loss 0.0001 (0.0020)	
training:	Epoch: [194][104/233]	Loss 0.0002 (0.0020)	
training:	Epoch: [194][105/233]	Loss 0.0002 (0.0020)	
training:	Epoch: [194][106/233]	Loss 0.0001 (0.0019)	
training:	Epoch: [194][107/233]	Loss 0.0010 (0.0019)	
training:	Epoch: [194][108/233]	Loss 0.0036 (0.0020)	
training:	Epoch: [194][109/233]	Loss 0.0001 (0.0019)	
training:	Epoch: [194][110/233]	Loss 0.0007 (0.0019)	
training:	Epoch: [194][111/233]	Loss 0.0005 (0.0019)	
training:	Epoch: [194][112/233]	Loss 0.0002 (0.0019)	
training:	Epoch: [194][113/233]	Loss 0.0001 (0.0019)	
training:	Epoch: [194][114/233]	Loss 0.0002 (0.0019)	
training:	Epoch: [194][115/233]	Loss 0.0007 (0.0019)	
training:	Epoch: [194][116/233]	Loss 0.0022 (0.0019)	
training:	Epoch: [194][117/233]	Loss 0.0001 (0.0018)	
training:	Epoch: [194][118/233]	Loss 0.0002 (0.0018)	
training:	Epoch: [194][119/233]	Loss 0.0002 (0.0018)	
training:	Epoch: [194][120/233]	Loss 0.0006 (0.0018)	
training:	Epoch: [194][121/233]	Loss 0.0001 (0.0018)	
training:	Epoch: [194][122/233]	Loss 0.0003 (0.0018)	
training:	Epoch: [194][123/233]	Loss 0.0005 (0.0018)	
training:	Epoch: [194][124/233]	Loss 0.0001 (0.0018)	
training:	Epoch: [194][125/233]	Loss 0.0001 (0.0017)	
training:	Epoch: [194][126/233]	Loss 0.0002 (0.0017)	
training:	Epoch: [194][127/233]	Loss 0.0002 (0.0017)	
training:	Epoch: [194][128/233]	Loss 0.0002 (0.0017)	
training:	Epoch: [194][129/233]	Loss 0.0002 (0.0017)	
training:	Epoch: [194][130/233]	Loss 0.0002 (0.0017)	
training:	Epoch: [194][131/233]	Loss 0.0002 (0.0017)	
training:	Epoch: [194][132/233]	Loss 0.0001 (0.0017)	
training:	Epoch: [194][133/233]	Loss 0.0002 (0.0016)	
training:	Epoch: [194][134/233]	Loss 0.0001 (0.0016)	
training:	Epoch: [194][135/233]	Loss 0.0002 (0.0016)	
training:	Epoch: [194][136/233]	Loss 0.0001 (0.0016)	
training:	Epoch: [194][137/233]	Loss 0.0653 (0.0021)	
training:	Epoch: [194][138/233]	Loss 0.0001 (0.0021)	
training:	Epoch: [194][139/233]	Loss 0.0002 (0.0021)	
training:	Epoch: [194][140/233]	Loss 0.0001 (0.0020)	
training:	Epoch: [194][141/233]	Loss 0.0001 (0.0020)	
training:	Epoch: [194][142/233]	Loss 0.0003 (0.0020)	
training:	Epoch: [194][143/233]	Loss 0.0002 (0.0020)	
training:	Epoch: [194][144/233]	Loss 0.0293 (0.0022)	
training:	Epoch: [194][145/233]	Loss 0.0002 (0.0022)	
training:	Epoch: [194][146/233]	Loss 0.0004 (0.0022)	
training:	Epoch: [194][147/233]	Loss 0.0060 (0.0022)	
training:	Epoch: [194][148/233]	Loss 0.0002 (0.0022)	
training:	Epoch: [194][149/233]	Loss 0.0003 (0.0022)	
training:	Epoch: [194][150/233]	Loss 0.0001 (0.0021)	
training:	Epoch: [194][151/233]	Loss 0.0001 (0.0021)	
training:	Epoch: [194][152/233]	Loss 0.0001 (0.0021)	
training:	Epoch: [194][153/233]	Loss 0.0056 (0.0021)	
training:	Epoch: [194][154/233]	Loss 0.0003 (0.0021)	
training:	Epoch: [194][155/233]	Loss 0.0048 (0.0021)	
training:	Epoch: [194][156/233]	Loss 0.0001 (0.0021)	
training:	Epoch: [194][157/233]	Loss 0.0001 (0.0021)	
training:	Epoch: [194][158/233]	Loss 0.0003 (0.0021)	
training:	Epoch: [194][159/233]	Loss 0.0002 (0.0021)	
training:	Epoch: [194][160/233]	Loss 0.0001 (0.0021)	
training:	Epoch: [194][161/233]	Loss 0.0001 (0.0021)	
training:	Epoch: [194][162/233]	Loss 0.0001 (0.0021)	
training:	Epoch: [194][163/233]	Loss 0.0003 (0.0021)	
training:	Epoch: [194][164/233]	Loss 0.0001 (0.0020)	
training:	Epoch: [194][165/233]	Loss 0.0003 (0.0020)	
training:	Epoch: [194][166/233]	Loss 0.0001 (0.0020)	
training:	Epoch: [194][167/233]	Loss 0.0008 (0.0020)	
training:	Epoch: [194][168/233]	Loss 0.0001 (0.0020)	
training:	Epoch: [194][169/233]	Loss 0.0001 (0.0020)	
training:	Epoch: [194][170/233]	Loss 0.0174 (0.0021)	
training:	Epoch: [194][171/233]	Loss 0.0002 (0.0021)	
training:	Epoch: [194][172/233]	Loss 0.0003 (0.0021)	
training:	Epoch: [194][173/233]	Loss 0.0002 (0.0020)	
training:	Epoch: [194][174/233]	Loss 0.0004 (0.0020)	
training:	Epoch: [194][175/233]	Loss 0.0003 (0.0020)	
training:	Epoch: [194][176/233]	Loss 0.0002 (0.0020)	
training:	Epoch: [194][177/233]	Loss 0.0002 (0.0020)	
training:	Epoch: [194][178/233]	Loss 0.0011 (0.0020)	
training:	Epoch: [194][179/233]	Loss 0.0001 (0.0020)	
training:	Epoch: [194][180/233]	Loss 0.0002 (0.0020)	
training:	Epoch: [194][181/233]	Loss 0.0006 (0.0020)	
training:	Epoch: [194][182/233]	Loss 0.0001 (0.0020)	
training:	Epoch: [194][183/233]	Loss 0.0007 (0.0020)	
training:	Epoch: [194][184/233]	Loss 0.0002 (0.0019)	
training:	Epoch: [194][185/233]	Loss 0.0001 (0.0019)	
training:	Epoch: [194][186/233]	Loss 0.0004 (0.0019)	
training:	Epoch: [194][187/233]	Loss 0.0001 (0.0019)	
training:	Epoch: [194][188/233]	Loss 0.0001 (0.0019)	
training:	Epoch: [194][189/233]	Loss 0.0001 (0.0019)	
training:	Epoch: [194][190/233]	Loss 0.0002 (0.0019)	
training:	Epoch: [194][191/233]	Loss 0.0001 (0.0019)	
training:	Epoch: [194][192/233]	Loss 0.0001 (0.0019)	
training:	Epoch: [194][193/233]	Loss 0.0002 (0.0019)	
training:	Epoch: [194][194/233]	Loss 0.0002 (0.0019)	
training:	Epoch: [194][195/233]	Loss 0.0002 (0.0018)	
training:	Epoch: [194][196/233]	Loss 0.0002 (0.0018)	
training:	Epoch: [194][197/233]	Loss 0.0001 (0.0018)	
training:	Epoch: [194][198/233]	Loss 0.0001 (0.0018)	
training:	Epoch: [194][199/233]	Loss 0.0003 (0.0018)	
training:	Epoch: [194][200/233]	Loss 0.0001 (0.0018)	
training:	Epoch: [194][201/233]	Loss 0.0004 (0.0018)	
training:	Epoch: [194][202/233]	Loss 0.0002 (0.0018)	
training:	Epoch: [194][203/233]	Loss 0.0002 (0.0018)	
training:	Epoch: [194][204/233]	Loss 0.0001 (0.0018)	
training:	Epoch: [194][205/233]	Loss 0.0002 (0.0018)	
training:	Epoch: [194][206/233]	Loss 0.0001 (0.0018)	
training:	Epoch: [194][207/233]	Loss 0.0001 (0.0018)	
training:	Epoch: [194][208/233]	Loss 0.0003 (0.0017)	
training:	Epoch: [194][209/233]	Loss 0.0003 (0.0017)	
training:	Epoch: [194][210/233]	Loss 0.0003 (0.0017)	
training:	Epoch: [194][211/233]	Loss 0.0001 (0.0017)	
training:	Epoch: [194][212/233]	Loss 0.0001 (0.0017)	
training:	Epoch: [194][213/233]	Loss 0.0001 (0.0017)	
training:	Epoch: [194][214/233]	Loss 0.0003 (0.0017)	
training:	Epoch: [194][215/233]	Loss 0.0001 (0.0017)	
training:	Epoch: [194][216/233]	Loss 0.0002 (0.0017)	
training:	Epoch: [194][217/233]	Loss 0.0002 (0.0017)	
training:	Epoch: [194][218/233]	Loss 0.0003 (0.0017)	
training:	Epoch: [194][219/233]	Loss 0.0001 (0.0017)	
training:	Epoch: [194][220/233]	Loss 0.0002 (0.0017)	
training:	Epoch: [194][221/233]	Loss 0.0002 (0.0017)	
training:	Epoch: [194][222/233]	Loss 0.0031 (0.0017)	
training:	Epoch: [194][223/233]	Loss 0.0002 (0.0017)	
training:	Epoch: [194][224/233]	Loss 0.0013 (0.0017)	
training:	Epoch: [194][225/233]	Loss 0.0002 (0.0016)	
training:	Epoch: [194][226/233]	Loss 0.0002 (0.0016)	
training:	Epoch: [194][227/233]	Loss 0.0002 (0.0016)	
training:	Epoch: [194][228/233]	Loss 0.0001 (0.0016)	
training:	Epoch: [194][229/233]	Loss 0.0001 (0.0016)	
training:	Epoch: [194][230/233]	Loss 0.0002 (0.0016)	
training:	Epoch: [194][231/233]	Loss 0.0001 (0.0016)	
training:	Epoch: [194][232/233]	Loss 0.0145 (0.0017)	
training:	Epoch: [194][233/233]	Loss 0.0001 (0.0017)	
Training:	 Loss: 0.0017

Training:	 ACC: 1.0000 1.0000 1.0000 1.0000
Validation:	 ACC: 0.7866 0.7865 0.7845 0.7888
Validation:	 Best_BACC: 0.8162 0.8154 0.7998 0.8326
Validation:	 Loss: 1.4135
Pretraining:	Epoch 195/200
----------
training:	Epoch: [195][1/233]	Loss 0.0002 (0.0002)	
training:	Epoch: [195][2/233]	Loss 0.0001 (0.0001)	
training:	Epoch: [195][3/233]	Loss 0.0001 (0.0001)	
training:	Epoch: [195][4/233]	Loss 0.0001 (0.0001)	
training:	Epoch: [195][5/233]	Loss 0.0002 (0.0001)	
training:	Epoch: [195][6/233]	Loss 0.0001 (0.0001)	
training:	Epoch: [195][7/233]	Loss 0.0005 (0.0002)	
training:	Epoch: [195][8/233]	Loss 0.0129 (0.0018)	
training:	Epoch: [195][9/233]	Loss 0.0093 (0.0026)	
training:	Epoch: [195][10/233]	Loss 0.0002 (0.0024)	
training:	Epoch: [195][11/233]	Loss 0.0001 (0.0022)	
training:	Epoch: [195][12/233]	Loss 0.0001 (0.0020)	
training:	Epoch: [195][13/233]	Loss 0.0002 (0.0019)	
training:	Epoch: [195][14/233]	Loss 0.0098 (0.0024)	
training:	Epoch: [195][15/233]	Loss 0.0001 (0.0023)	
training:	Epoch: [195][16/233]	Loss 0.0001 (0.0021)	
training:	Epoch: [195][17/233]	Loss 0.0002 (0.0020)	
training:	Epoch: [195][18/233]	Loss 0.0001 (0.0019)	
training:	Epoch: [195][19/233]	Loss 0.0002 (0.0018)	
training:	Epoch: [195][20/233]	Loss 0.0830 (0.0059)	
training:	Epoch: [195][21/233]	Loss 0.0002 (0.0056)	
training:	Epoch: [195][22/233]	Loss 0.0016 (0.0054)	
training:	Epoch: [195][23/233]	Loss 0.0002 (0.0052)	
training:	Epoch: [195][24/233]	Loss 0.0001 (0.0050)	
training:	Epoch: [195][25/233]	Loss 0.0003 (0.0048)	
training:	Epoch: [195][26/233]	Loss 0.0002 (0.0046)	
training:	Epoch: [195][27/233]	Loss 0.0006 (0.0045)	
training:	Epoch: [195][28/233]	Loss 0.0001 (0.0043)	
training:	Epoch: [195][29/233]	Loss 0.0007 (0.0042)	
training:	Epoch: [195][30/233]	Loss 0.0002 (0.0041)	
training:	Epoch: [195][31/233]	Loss 0.0001 (0.0039)	
training:	Epoch: [195][32/233]	Loss 0.0002 (0.0038)	
training:	Epoch: [195][33/233]	Loss 0.0001 (0.0037)	
training:	Epoch: [195][34/233]	Loss 0.0001 (0.0036)	
training:	Epoch: [195][35/233]	Loss 0.0001 (0.0035)	
training:	Epoch: [195][36/233]	Loss 0.0001 (0.0034)	
training:	Epoch: [195][37/233]	Loss 0.0003 (0.0033)	
training:	Epoch: [195][38/233]	Loss 0.0001 (0.0032)	
training:	Epoch: [195][39/233]	Loss 0.0001 (0.0032)	
training:	Epoch: [195][40/233]	Loss 0.0002 (0.0031)	
training:	Epoch: [195][41/233]	Loss 0.0014 (0.0030)	
training:	Epoch: [195][42/233]	Loss 0.0001 (0.0030)	
training:	Epoch: [195][43/233]	Loss 0.0003 (0.0029)	
training:	Epoch: [195][44/233]	Loss 0.0003 (0.0029)	
training:	Epoch: [195][45/233]	Loss 0.0003 (0.0028)	
training:	Epoch: [195][46/233]	Loss 0.0002 (0.0027)	
training:	Epoch: [195][47/233]	Loss 0.0006 (0.0027)	
training:	Epoch: [195][48/233]	Loss 0.0019 (0.0027)	
training:	Epoch: [195][49/233]	Loss 0.0002 (0.0026)	
training:	Epoch: [195][50/233]	Loss 0.0002 (0.0026)	
training:	Epoch: [195][51/233]	Loss 0.0001 (0.0025)	
training:	Epoch: [195][52/233]	Loss 0.0001 (0.0025)	
training:	Epoch: [195][53/233]	Loss 0.0001 (0.0024)	
training:	Epoch: [195][54/233]	Loss 0.0002 (0.0024)	
training:	Epoch: [195][55/233]	Loss 0.0003 (0.0024)	
training:	Epoch: [195][56/233]	Loss 0.0001 (0.0023)	
training:	Epoch: [195][57/233]	Loss 0.0009 (0.0023)	
training:	Epoch: [195][58/233]	Loss 0.0002 (0.0023)	
training:	Epoch: [195][59/233]	Loss 0.0001 (0.0022)	
training:	Epoch: [195][60/233]	Loss 0.0001 (0.0022)	
training:	Epoch: [195][61/233]	Loss 0.0001 (0.0022)	
training:	Epoch: [195][62/233]	Loss 0.0002 (0.0021)	
training:	Epoch: [195][63/233]	Loss 0.0001 (0.0021)	
training:	Epoch: [195][64/233]	Loss 0.0001 (0.0021)	
training:	Epoch: [195][65/233]	Loss 0.0002 (0.0020)	
training:	Epoch: [195][66/233]	Loss 0.0002 (0.0020)	
training:	Epoch: [195][67/233]	Loss 0.0002 (0.0020)	
training:	Epoch: [195][68/233]	Loss 0.0007 (0.0020)	
training:	Epoch: [195][69/233]	Loss 0.0001 (0.0019)	
training:	Epoch: [195][70/233]	Loss 0.0002 (0.0019)	
training:	Epoch: [195][71/233]	Loss 0.0001 (0.0019)	
training:	Epoch: [195][72/233]	Loss 0.0001 (0.0019)	
training:	Epoch: [195][73/233]	Loss 0.0001 (0.0018)	
training:	Epoch: [195][74/233]	Loss 0.0001 (0.0018)	
training:	Epoch: [195][75/233]	Loss 0.0001 (0.0018)	
training:	Epoch: [195][76/233]	Loss 0.0001 (0.0018)	
training:	Epoch: [195][77/233]	Loss 0.0002 (0.0017)	
training:	Epoch: [195][78/233]	Loss 0.0001 (0.0017)	
training:	Epoch: [195][79/233]	Loss 0.0001 (0.0017)	
training:	Epoch: [195][80/233]	Loss 0.0002 (0.0017)	
training:	Epoch: [195][81/233]	Loss 0.0001 (0.0017)	
training:	Epoch: [195][82/233]	Loss 0.0001 (0.0016)	
training:	Epoch: [195][83/233]	Loss 0.0001 (0.0016)	
training:	Epoch: [195][84/233]	Loss 0.0001 (0.0016)	
training:	Epoch: [195][85/233]	Loss 0.0002 (0.0016)	
training:	Epoch: [195][86/233]	Loss 0.0001 (0.0016)	
training:	Epoch: [195][87/233]	Loss 0.0002 (0.0016)	
training:	Epoch: [195][88/233]	Loss 0.0002 (0.0015)	
training:	Epoch: [195][89/233]	Loss 0.0002 (0.0015)	
training:	Epoch: [195][90/233]	Loss 0.0002 (0.0015)	
training:	Epoch: [195][91/233]	Loss 0.0001 (0.0015)	
training:	Epoch: [195][92/233]	Loss 0.0001 (0.0015)	
training:	Epoch: [195][93/233]	Loss 0.0002 (0.0015)	
training:	Epoch: [195][94/233]	Loss 0.0002 (0.0015)	
training:	Epoch: [195][95/233]	Loss 0.0001 (0.0014)	
training:	Epoch: [195][96/233]	Loss 0.0373 (0.0018)	
training:	Epoch: [195][97/233]	Loss 0.0001 (0.0018)	
training:	Epoch: [195][98/233]	Loss 0.0002 (0.0018)	
training:	Epoch: [195][99/233]	Loss 0.0002 (0.0018)	
training:	Epoch: [195][100/233]	Loss 0.0001 (0.0018)	
training:	Epoch: [195][101/233]	Loss 0.0001 (0.0017)	
training:	Epoch: [195][102/233]	Loss 0.0047 (0.0018)	
training:	Epoch: [195][103/233]	Loss 0.0002 (0.0017)	
training:	Epoch: [195][104/233]	Loss 0.0001 (0.0017)	
training:	Epoch: [195][105/233]	Loss 0.0001 (0.0017)	
training:	Epoch: [195][106/233]	Loss 0.0002 (0.0017)	
training:	Epoch: [195][107/233]	Loss 0.0002 (0.0017)	
training:	Epoch: [195][108/233]	Loss 0.0003 (0.0017)	
training:	Epoch: [195][109/233]	Loss 0.0001 (0.0017)	
training:	Epoch: [195][110/233]	Loss 0.0001 (0.0016)	
training:	Epoch: [195][111/233]	Loss 0.0004 (0.0016)	
training:	Epoch: [195][112/233]	Loss 0.0001 (0.0016)	
training:	Epoch: [195][113/233]	Loss 0.0001 (0.0016)	
training:	Epoch: [195][114/233]	Loss 0.0002 (0.0016)	
training:	Epoch: [195][115/233]	Loss 0.0001 (0.0016)	
training:	Epoch: [195][116/233]	Loss 0.0001 (0.0016)	
training:	Epoch: [195][117/233]	Loss 0.0002 (0.0016)	
training:	Epoch: [195][118/233]	Loss 0.0003 (0.0015)	
training:	Epoch: [195][119/233]	Loss 0.0004 (0.0015)	
training:	Epoch: [195][120/233]	Loss 0.0002 (0.0015)	
training:	Epoch: [195][121/233]	Loss 0.0005 (0.0015)	
training:	Epoch: [195][122/233]	Loss 0.0002 (0.0015)	
training:	Epoch: [195][123/233]	Loss 0.0001 (0.0015)	
training:	Epoch: [195][124/233]	Loss 0.0001 (0.0015)	
training:	Epoch: [195][125/233]	Loss 0.0004 (0.0015)	
training:	Epoch: [195][126/233]	Loss 0.0002 (0.0015)	
training:	Epoch: [195][127/233]	Loss 0.0006 (0.0015)	
training:	Epoch: [195][128/233]	Loss 0.0002 (0.0015)	
training:	Epoch: [195][129/233]	Loss 0.0001 (0.0014)	
training:	Epoch: [195][130/233]	Loss 0.0002 (0.0014)	
training:	Epoch: [195][131/233]	Loss 0.0595 (0.0019)	
training:	Epoch: [195][132/233]	Loss 0.0006 (0.0019)	
training:	Epoch: [195][133/233]	Loss 0.0001 (0.0019)	
training:	Epoch: [195][134/233]	Loss 0.0002 (0.0018)	
training:	Epoch: [195][135/233]	Loss 0.0007 (0.0018)	
training:	Epoch: [195][136/233]	Loss 0.0006 (0.0018)	
training:	Epoch: [195][137/233]	Loss 0.0005 (0.0018)	
training:	Epoch: [195][138/233]	Loss 0.0068 (0.0018)	
training:	Epoch: [195][139/233]	Loss 0.0021 (0.0018)	
training:	Epoch: [195][140/233]	Loss 0.0001 (0.0018)	
training:	Epoch: [195][141/233]	Loss 0.0014 (0.0018)	
training:	Epoch: [195][142/233]	Loss 0.0003 (0.0018)	
training:	Epoch: [195][143/233]	Loss 0.0003 (0.0018)	
training:	Epoch: [195][144/233]	Loss 0.0035 (0.0018)	
training:	Epoch: [195][145/233]	Loss 0.0001 (0.0018)	
training:	Epoch: [195][146/233]	Loss 0.0001 (0.0018)	
training:	Epoch: [195][147/233]	Loss 0.0347 (0.0020)	
training:	Epoch: [195][148/233]	Loss 0.0002 (0.0020)	
training:	Epoch: [195][149/233]	Loss 0.0004 (0.0020)	
training:	Epoch: [195][150/233]	Loss 0.0001 (0.0020)	
training:	Epoch: [195][151/233]	Loss 0.0001 (0.0020)	
training:	Epoch: [195][152/233]	Loss 0.0005 (0.0020)	
training:	Epoch: [195][153/233]	Loss 0.0001 (0.0020)	
training:	Epoch: [195][154/233]	Loss 0.0002 (0.0019)	
training:	Epoch: [195][155/233]	Loss 0.0002 (0.0019)	
training:	Epoch: [195][156/233]	Loss 0.0001 (0.0019)	
training:	Epoch: [195][157/233]	Loss 0.0002 (0.0019)	
training:	Epoch: [195][158/233]	Loss 0.0003 (0.0019)	
training:	Epoch: [195][159/233]	Loss 0.0061 (0.0019)	
training:	Epoch: [195][160/233]	Loss 0.0004 (0.0019)	
training:	Epoch: [195][161/233]	Loss 0.0002 (0.0019)	
training:	Epoch: [195][162/233]	Loss 0.0002 (0.0019)	
training:	Epoch: [195][163/233]	Loss 0.0001 (0.0019)	
training:	Epoch: [195][164/233]	Loss 0.0001 (0.0019)	
training:	Epoch: [195][165/233]	Loss 0.0027 (0.0019)	
training:	Epoch: [195][166/233]	Loss 0.0001 (0.0019)	
training:	Epoch: [195][167/233]	Loss 0.0003 (0.0019)	
training:	Epoch: [195][168/233]	Loss 0.0015 (0.0019)	
training:	Epoch: [195][169/233]	Loss 0.0001 (0.0018)	
training:	Epoch: [195][170/233]	Loss 0.0006 (0.0018)	
training:	Epoch: [195][171/233]	Loss 0.0002 (0.0018)	
training:	Epoch: [195][172/233]	Loss 0.0004 (0.0018)	
training:	Epoch: [195][173/233]	Loss 0.0003 (0.0018)	
training:	Epoch: [195][174/233]	Loss 0.0003 (0.0018)	
training:	Epoch: [195][175/233]	Loss 0.0001 (0.0018)	
training:	Epoch: [195][176/233]	Loss 0.0002 (0.0018)	
training:	Epoch: [195][177/233]	Loss 0.0001 (0.0018)	
training:	Epoch: [195][178/233]	Loss 0.0002 (0.0018)	
training:	Epoch: [195][179/233]	Loss 0.0003 (0.0018)	
training:	Epoch: [195][180/233]	Loss 0.0002 (0.0017)	
training:	Epoch: [195][181/233]	Loss 0.0098 (0.0018)	
training:	Epoch: [195][182/233]	Loss 0.0035 (0.0018)	
training:	Epoch: [195][183/233]	Loss 0.0001 (0.0018)	
training:	Epoch: [195][184/233]	Loss 0.0002 (0.0018)	
training:	Epoch: [195][185/233]	Loss 0.0013 (0.0018)	
training:	Epoch: [195][186/233]	Loss 0.0001 (0.0018)	
training:	Epoch: [195][187/233]	Loss 0.0001 (0.0018)	
training:	Epoch: [195][188/233]	Loss 0.0010 (0.0018)	
training:	Epoch: [195][189/233]	Loss 0.0002 (0.0018)	
training:	Epoch: [195][190/233]	Loss 0.0001 (0.0017)	
training:	Epoch: [195][191/233]	Loss 0.0001 (0.0017)	
training:	Epoch: [195][192/233]	Loss 0.0001 (0.0017)	
training:	Epoch: [195][193/233]	Loss 0.0001 (0.0017)	
training:	Epoch: [195][194/233]	Loss 0.0001 (0.0017)	
training:	Epoch: [195][195/233]	Loss 0.0002 (0.0017)	
training:	Epoch: [195][196/233]	Loss 0.0002 (0.0017)	
training:	Epoch: [195][197/233]	Loss 0.0001 (0.0017)	
training:	Epoch: [195][198/233]	Loss 0.0011 (0.0017)	
training:	Epoch: [195][199/233]	Loss 0.0002 (0.0017)	
training:	Epoch: [195][200/233]	Loss 0.0002 (0.0017)	
training:	Epoch: [195][201/233]	Loss 0.0001 (0.0017)	
training:	Epoch: [195][202/233]	Loss 0.0001 (0.0017)	
training:	Epoch: [195][203/233]	Loss 0.0002 (0.0016)	
training:	Epoch: [195][204/233]	Loss 0.0001 (0.0016)	
training:	Epoch: [195][205/233]	Loss 0.0001 (0.0016)	
training:	Epoch: [195][206/233]	Loss 0.0001 (0.0016)	
training:	Epoch: [195][207/233]	Loss 0.0001 (0.0016)	
training:	Epoch: [195][208/233]	Loss 0.0002 (0.0016)	
training:	Epoch: [195][209/233]	Loss 0.0001 (0.0016)	
training:	Epoch: [195][210/233]	Loss 0.0001 (0.0016)	
training:	Epoch: [195][211/233]	Loss 0.0001 (0.0016)	
training:	Epoch: [195][212/233]	Loss 0.0002 (0.0016)	
training:	Epoch: [195][213/233]	Loss 0.0004 (0.0016)	
training:	Epoch: [195][214/233]	Loss 0.0005 (0.0016)	
training:	Epoch: [195][215/233]	Loss 0.0002 (0.0016)	
training:	Epoch: [195][216/233]	Loss 0.0011 (0.0016)	
training:	Epoch: [195][217/233]	Loss 0.0003 (0.0016)	
training:	Epoch: [195][218/233]	Loss 0.0001 (0.0016)	
training:	Epoch: [195][219/233]	Loss 0.0002 (0.0015)	
training:	Epoch: [195][220/233]	Loss 0.0002 (0.0015)	
training:	Epoch: [195][221/233]	Loss 0.0002 (0.0015)	
training:	Epoch: [195][222/233]	Loss 0.0001 (0.0015)	
training:	Epoch: [195][223/233]	Loss 0.0002 (0.0015)	
training:	Epoch: [195][224/233]	Loss 0.0001 (0.0015)	
training:	Epoch: [195][225/233]	Loss 0.0004 (0.0015)	
training:	Epoch: [195][226/233]	Loss 0.0006 (0.0015)	
training:	Epoch: [195][227/233]	Loss 0.0321 (0.0016)	
training:	Epoch: [195][228/233]	Loss 0.0004 (0.0016)	
training:	Epoch: [195][229/233]	Loss 0.0001 (0.0016)	
training:	Epoch: [195][230/233]	Loss 0.0002 (0.0016)	
training:	Epoch: [195][231/233]	Loss 0.0001 (0.0016)	
training:	Epoch: [195][232/233]	Loss 0.0129 (0.0017)	
training:	Epoch: [195][233/233]	Loss 0.0002 (0.0017)	
Training:	 Loss: 0.0017

Training:	 ACC: 1.0000 1.0000 1.0000 1.0000
Validation:	 ACC: 0.7926 0.7924 0.7875 0.7978
Validation:	 Best_BACC: 0.8162 0.8154 0.7998 0.8326
Validation:	 Loss: 1.4034
Pretraining:	Epoch 196/200
----------
training:	Epoch: [196][1/233]	Loss 0.0001 (0.0001)	
training:	Epoch: [196][2/233]	Loss 0.0002 (0.0001)	
training:	Epoch: [196][3/233]	Loss 0.0001 (0.0001)	
training:	Epoch: [196][4/233]	Loss 0.0001 (0.0001)	
training:	Epoch: [196][5/233]	Loss 0.0001 (0.0001)	
training:	Epoch: [196][6/233]	Loss 0.0001 (0.0001)	
training:	Epoch: [196][7/233]	Loss 0.0001 (0.0001)	
training:	Epoch: [196][8/233]	Loss 0.0001 (0.0001)	
training:	Epoch: [196][9/233]	Loss 0.0002 (0.0001)	
training:	Epoch: [196][10/233]	Loss 0.0001 (0.0001)	
training:	Epoch: [196][11/233]	Loss 0.0001 (0.0001)	
training:	Epoch: [196][12/233]	Loss 0.0001 (0.0001)	
training:	Epoch: [196][13/233]	Loss 0.0001 (0.0001)	
training:	Epoch: [196][14/233]	Loss 0.0001 (0.0001)	
training:	Epoch: [196][15/233]	Loss 0.0001 (0.0001)	
training:	Epoch: [196][16/233]	Loss 0.0002 (0.0001)	
training:	Epoch: [196][17/233]	Loss 0.0001 (0.0001)	
training:	Epoch: [196][18/233]	Loss 0.0001 (0.0001)	
training:	Epoch: [196][19/233]	Loss 0.0001 (0.0001)	
training:	Epoch: [196][20/233]	Loss 0.0001 (0.0001)	
training:	Epoch: [196][21/233]	Loss 0.0014 (0.0002)	
training:	Epoch: [196][22/233]	Loss 0.0001 (0.0002)	
training:	Epoch: [196][23/233]	Loss 0.0001 (0.0002)	
training:	Epoch: [196][24/233]	Loss 0.0002 (0.0002)	
training:	Epoch: [196][25/233]	Loss 0.0001 (0.0002)	
training:	Epoch: [196][26/233]	Loss 0.0001 (0.0002)	
training:	Epoch: [196][27/233]	Loss 0.0001 (0.0002)	
training:	Epoch: [196][28/233]	Loss 0.0010 (0.0002)	
training:	Epoch: [196][29/233]	Loss 0.0016 (0.0003)	
training:	Epoch: [196][30/233]	Loss 0.0003 (0.0003)	
training:	Epoch: [196][31/233]	Loss 0.0001 (0.0003)	
training:	Epoch: [196][32/233]	Loss 0.0002 (0.0003)	
training:	Epoch: [196][33/233]	Loss 0.0001 (0.0002)	
training:	Epoch: [196][34/233]	Loss 0.0001 (0.0002)	
training:	Epoch: [196][35/233]	Loss 0.0002 (0.0002)	
training:	Epoch: [196][36/233]	Loss 0.0003 (0.0002)	
training:	Epoch: [196][37/233]	Loss 0.0002 (0.0002)	
training:	Epoch: [196][38/233]	Loss 0.0001 (0.0002)	
training:	Epoch: [196][39/233]	Loss 0.0001 (0.0002)	
training:	Epoch: [196][40/233]	Loss 0.0002 (0.0002)	
training:	Epoch: [196][41/233]	Loss 0.0001 (0.0002)	
training:	Epoch: [196][42/233]	Loss 0.0001 (0.0002)	
training:	Epoch: [196][43/233]	Loss 0.0001 (0.0002)	
training:	Epoch: [196][44/233]	Loss 0.0001 (0.0002)	
training:	Epoch: [196][45/233]	Loss 0.0001 (0.0002)	
training:	Epoch: [196][46/233]	Loss 0.0001 (0.0002)	
training:	Epoch: [196][47/233]	Loss 0.0001 (0.0002)	
training:	Epoch: [196][48/233]	Loss 0.0014 (0.0002)	
training:	Epoch: [196][49/233]	Loss 0.0002 (0.0002)	
training:	Epoch: [196][50/233]	Loss 0.0002 (0.0002)	
training:	Epoch: [196][51/233]	Loss 0.0001 (0.0002)	
training:	Epoch: [196][52/233]	Loss 0.0001 (0.0002)	
training:	Epoch: [196][53/233]	Loss 0.0182 (0.0006)	
training:	Epoch: [196][54/233]	Loss 0.0001 (0.0006)	
training:	Epoch: [196][55/233]	Loss 0.0001 (0.0006)	
training:	Epoch: [196][56/233]	Loss 0.0001 (0.0005)	
training:	Epoch: [196][57/233]	Loss 0.0004 (0.0005)	
training:	Epoch: [196][58/233]	Loss 0.0004 (0.0005)	
training:	Epoch: [196][59/233]	Loss 0.0001 (0.0005)	
training:	Epoch: [196][60/233]	Loss 0.0002 (0.0005)	
training:	Epoch: [196][61/233]	Loss 0.0001 (0.0005)	
training:	Epoch: [196][62/233]	Loss 0.0001 (0.0005)	
training:	Epoch: [196][63/233]	Loss 0.0001 (0.0005)	
training:	Epoch: [196][64/233]	Loss 0.0001 (0.0005)	
training:	Epoch: [196][65/233]	Loss 0.0098 (0.0006)	
training:	Epoch: [196][66/233]	Loss 0.0001 (0.0006)	
training:	Epoch: [196][67/233]	Loss 0.0001 (0.0006)	
training:	Epoch: [196][68/233]	Loss 0.0002 (0.0006)	
training:	Epoch: [196][69/233]	Loss 0.0001 (0.0006)	
training:	Epoch: [196][70/233]	Loss 0.0001 (0.0006)	
training:	Epoch: [196][71/233]	Loss 0.0001 (0.0006)	
training:	Epoch: [196][72/233]	Loss 0.0002 (0.0006)	
training:	Epoch: [196][73/233]	Loss 0.0002 (0.0006)	
training:	Epoch: [196][74/233]	Loss 0.0003 (0.0006)	
training:	Epoch: [196][75/233]	Loss 0.0001 (0.0006)	
training:	Epoch: [196][76/233]	Loss 0.0002 (0.0006)	
training:	Epoch: [196][77/233]	Loss 0.0001 (0.0006)	
training:	Epoch: [196][78/233]	Loss 0.0003 (0.0006)	
training:	Epoch: [196][79/233]	Loss 0.0001 (0.0006)	
training:	Epoch: [196][80/233]	Loss 0.0003 (0.0006)	
training:	Epoch: [196][81/233]	Loss 0.0002 (0.0006)	
training:	Epoch: [196][82/233]	Loss 0.0012 (0.0006)	
training:	Epoch: [196][83/233]	Loss 0.0002 (0.0006)	
training:	Epoch: [196][84/233]	Loss 0.0001 (0.0006)	
training:	Epoch: [196][85/233]	Loss 0.0002 (0.0005)	
training:	Epoch: [196][86/233]	Loss 0.0001 (0.0005)	
training:	Epoch: [196][87/233]	Loss 0.0003 (0.0005)	
training:	Epoch: [196][88/233]	Loss 0.0084 (0.0006)	
training:	Epoch: [196][89/233]	Loss 0.0002 (0.0006)	
training:	Epoch: [196][90/233]	Loss 0.0001 (0.0006)	
training:	Epoch: [196][91/233]	Loss 0.0005 (0.0006)	
training:	Epoch: [196][92/233]	Loss 0.0001 (0.0006)	
training:	Epoch: [196][93/233]	Loss 0.0018 (0.0006)	
training:	Epoch: [196][94/233]	Loss 0.0016 (0.0006)	
training:	Epoch: [196][95/233]	Loss 0.0001 (0.0006)	
training:	Epoch: [196][96/233]	Loss 0.0001 (0.0006)	
training:	Epoch: [196][97/233]	Loss 0.0060 (0.0007)	
training:	Epoch: [196][98/233]	Loss 0.0002 (0.0007)	
training:	Epoch: [196][99/233]	Loss 0.0042 (0.0007)	
training:	Epoch: [196][100/233]	Loss 0.0004 (0.0007)	
training:	Epoch: [196][101/233]	Loss 0.0005 (0.0007)	
training:	Epoch: [196][102/233]	Loss 0.0001 (0.0007)	
training:	Epoch: [196][103/233]	Loss 0.0001 (0.0007)	
training:	Epoch: [196][104/233]	Loss 0.0001 (0.0007)	
training:	Epoch: [196][105/233]	Loss 0.0001 (0.0007)	
training:	Epoch: [196][106/233]	Loss 0.0001 (0.0007)	
training:	Epoch: [196][107/233]	Loss 0.0001 (0.0007)	
training:	Epoch: [196][108/233]	Loss 0.0001 (0.0007)	
training:	Epoch: [196][109/233]	Loss 0.0001 (0.0007)	
training:	Epoch: [196][110/233]	Loss 0.0001 (0.0007)	
training:	Epoch: [196][111/233]	Loss 0.0002 (0.0007)	
training:	Epoch: [196][112/233]	Loss 0.0001 (0.0007)	
training:	Epoch: [196][113/233]	Loss 0.0001 (0.0006)	
training:	Epoch: [196][114/233]	Loss 0.0002 (0.0006)	
training:	Epoch: [196][115/233]	Loss 0.0001 (0.0006)	
training:	Epoch: [196][116/233]	Loss 0.0001 (0.0006)	
training:	Epoch: [196][117/233]	Loss 0.0001 (0.0006)	
training:	Epoch: [196][118/233]	Loss 0.0002 (0.0006)	
training:	Epoch: [196][119/233]	Loss 0.0002 (0.0006)	
training:	Epoch: [196][120/233]	Loss 0.0002 (0.0006)	
training:	Epoch: [196][121/233]	Loss 0.0001 (0.0006)	
training:	Epoch: [196][122/233]	Loss 0.0001 (0.0006)	
training:	Epoch: [196][123/233]	Loss 0.0001 (0.0006)	
training:	Epoch: [196][124/233]	Loss 0.0002 (0.0006)	
training:	Epoch: [196][125/233]	Loss 0.0049 (0.0006)	
training:	Epoch: [196][126/233]	Loss 0.0004 (0.0006)	
training:	Epoch: [196][127/233]	Loss 0.0003 (0.0006)	
training:	Epoch: [196][128/233]	Loss 0.0006 (0.0006)	
training:	Epoch: [196][129/233]	Loss 0.0001 (0.0006)	
training:	Epoch: [196][130/233]	Loss 0.0227 (0.0008)	
training:	Epoch: [196][131/233]	Loss 0.0001 (0.0008)	
training:	Epoch: [196][132/233]	Loss 0.0002 (0.0008)	
training:	Epoch: [196][133/233]	Loss 0.0003 (0.0008)	
training:	Epoch: [196][134/233]	Loss 0.0042 (0.0008)	
training:	Epoch: [196][135/233]	Loss 0.0002 (0.0008)	
training:	Epoch: [196][136/233]	Loss 0.0001 (0.0008)	
training:	Epoch: [196][137/233]	Loss 0.0015 (0.0008)	
training:	Epoch: [196][138/233]	Loss 0.0001 (0.0008)	
training:	Epoch: [196][139/233]	Loss 0.0001 (0.0008)	
training:	Epoch: [196][140/233]	Loss 0.0002 (0.0008)	
training:	Epoch: [196][141/233]	Loss 0.0005 (0.0008)	
training:	Epoch: [196][142/233]	Loss 0.0001 (0.0008)	
training:	Epoch: [196][143/233]	Loss 0.0001 (0.0008)	
training:	Epoch: [196][144/233]	Loss 0.0002 (0.0008)	
training:	Epoch: [196][145/233]	Loss 0.0002 (0.0008)	
training:	Epoch: [196][146/233]	Loss 0.0016 (0.0008)	
training:	Epoch: [196][147/233]	Loss 0.0001 (0.0008)	
training:	Epoch: [196][148/233]	Loss 0.0002 (0.0008)	
training:	Epoch: [196][149/233]	Loss 0.0020 (0.0008)	
training:	Epoch: [196][150/233]	Loss 0.0001 (0.0008)	
training:	Epoch: [196][151/233]	Loss 0.0002 (0.0008)	
training:	Epoch: [196][152/233]	Loss 0.0002 (0.0008)	
training:	Epoch: [196][153/233]	Loss 0.0006 (0.0008)	
training:	Epoch: [196][154/233]	Loss 0.0001 (0.0008)	
training:	Epoch: [196][155/233]	Loss 0.0013 (0.0008)	
training:	Epoch: [196][156/233]	Loss 0.0001 (0.0008)	
training:	Epoch: [196][157/233]	Loss 0.0002 (0.0008)	
training:	Epoch: [196][158/233]	Loss 0.0006 (0.0008)	
training:	Epoch: [196][159/233]	Loss 0.0003 (0.0007)	
training:	Epoch: [196][160/233]	Loss 0.0001 (0.0007)	
training:	Epoch: [196][161/233]	Loss 0.0001 (0.0007)	
training:	Epoch: [196][162/233]	Loss 0.0001 (0.0007)	
training:	Epoch: [196][163/233]	Loss 0.0002 (0.0007)	
training:	Epoch: [196][164/233]	Loss 0.0005 (0.0007)	
training:	Epoch: [196][165/233]	Loss 0.0001 (0.0007)	
training:	Epoch: [196][166/233]	Loss 0.0003 (0.0007)	
training:	Epoch: [196][167/233]	Loss 0.0001 (0.0007)	
training:	Epoch: [196][168/233]	Loss 0.0001 (0.0007)	
training:	Epoch: [196][169/233]	Loss 0.0001 (0.0007)	
training:	Epoch: [196][170/233]	Loss 0.0002 (0.0007)	
training:	Epoch: [196][171/233]	Loss 0.0012 (0.0007)	
training:	Epoch: [196][172/233]	Loss 0.0001 (0.0007)	
training:	Epoch: [196][173/233]	Loss 0.0002 (0.0007)	
training:	Epoch: [196][174/233]	Loss 0.0001 (0.0007)	
training:	Epoch: [196][175/233]	Loss 0.0005 (0.0007)	
training:	Epoch: [196][176/233]	Loss 0.0001 (0.0007)	
training:	Epoch: [196][177/233]	Loss 0.0002 (0.0007)	
training:	Epoch: [196][178/233]	Loss 0.0027 (0.0007)	
training:	Epoch: [196][179/233]	Loss 0.0001 (0.0007)	
training:	Epoch: [196][180/233]	Loss 0.0005 (0.0007)	
training:	Epoch: [196][181/233]	Loss 0.0602 (0.0010)	
training:	Epoch: [196][182/233]	Loss 0.0001 (0.0010)	
training:	Epoch: [196][183/233]	Loss 0.0001 (0.0010)	
training:	Epoch: [196][184/233]	Loss 0.0001 (0.0010)	
training:	Epoch: [196][185/233]	Loss 0.0001 (0.0010)	
training:	Epoch: [196][186/233]	Loss 0.0001 (0.0010)	
training:	Epoch: [196][187/233]	Loss 0.0001 (0.0010)	
training:	Epoch: [196][188/233]	Loss 0.0001 (0.0010)	
training:	Epoch: [196][189/233]	Loss 0.0001 (0.0010)	
training:	Epoch: [196][190/233]	Loss 0.0001 (0.0010)	
training:	Epoch: [196][191/233]	Loss 0.0002 (0.0010)	
training:	Epoch: [196][192/233]	Loss 0.0026 (0.0010)	
training:	Epoch: [196][193/233]	Loss 0.0079 (0.0010)	
training:	Epoch: [196][194/233]	Loss 0.0001 (0.0010)	
training:	Epoch: [196][195/233]	Loss 0.0014 (0.0010)	
training:	Epoch: [196][196/233]	Loss 0.0001 (0.0010)	
training:	Epoch: [196][197/233]	Loss 0.0158 (0.0011)	
training:	Epoch: [196][198/233]	Loss 0.0002 (0.0011)	
training:	Epoch: [196][199/233]	Loss 0.0007 (0.0011)	
training:	Epoch: [196][200/233]	Loss 0.0001 (0.0011)	
training:	Epoch: [196][201/233]	Loss 0.0003 (0.0011)	
training:	Epoch: [196][202/233]	Loss 0.0003 (0.0011)	
training:	Epoch: [196][203/233]	Loss 0.0011 (0.0011)	
training:	Epoch: [196][204/233]	Loss 0.0001 (0.0011)	
training:	Epoch: [196][205/233]	Loss 0.0002 (0.0011)	
training:	Epoch: [196][206/233]	Loss 0.0001 (0.0011)	
training:	Epoch: [196][207/233]	Loss 0.0001 (0.0011)	
training:	Epoch: [196][208/233]	Loss 0.0002 (0.0011)	
training:	Epoch: [196][209/233]	Loss 0.0001 (0.0011)	
training:	Epoch: [196][210/233]	Loss 0.0047 (0.0011)	
training:	Epoch: [196][211/233]	Loss 0.0001 (0.0011)	
training:	Epoch: [196][212/233]	Loss 0.0001 (0.0011)	
training:	Epoch: [196][213/233]	Loss 0.0001 (0.0011)	
training:	Epoch: [196][214/233]	Loss 0.0001 (0.0011)	
training:	Epoch: [196][215/233]	Loss 0.0001 (0.0010)	
training:	Epoch: [196][216/233]	Loss 0.0001 (0.0010)	
training:	Epoch: [196][217/233]	Loss 0.0001 (0.0010)	
training:	Epoch: [196][218/233]	Loss 0.0214 (0.0011)	
training:	Epoch: [196][219/233]	Loss 0.0005 (0.0011)	
training:	Epoch: [196][220/233]	Loss 0.0002 (0.0011)	
training:	Epoch: [196][221/233]	Loss 0.0001 (0.0011)	
training:	Epoch: [196][222/233]	Loss 0.0001 (0.0011)	
training:	Epoch: [196][223/233]	Loss 0.0003 (0.0011)	
training:	Epoch: [196][224/233]	Loss 0.0002 (0.0011)	
training:	Epoch: [196][225/233]	Loss 0.0002 (0.0011)	
training:	Epoch: [196][226/233]	Loss 0.0002 (0.0011)	
training:	Epoch: [196][227/233]	Loss 0.0001 (0.0011)	
training:	Epoch: [196][228/233]	Loss 0.0001 (0.0011)	
training:	Epoch: [196][229/233]	Loss 0.0004 (0.0011)	
training:	Epoch: [196][230/233]	Loss 0.0001 (0.0011)	
training:	Epoch: [196][231/233]	Loss 0.0036 (0.0011)	
training:	Epoch: [196][232/233]	Loss 0.0001 (0.0011)	
training:	Epoch: [196][233/233]	Loss 0.0001 (0.0011)	
Training:	 Loss: 0.0011

Training:	 ACC: 1.0000 1.0000 1.0000 1.0000
Validation:	 ACC: 0.7982 0.7983 0.8008 0.7955
Validation:	 Best_BACC: 0.8162 0.8154 0.7998 0.8326
Validation:	 Loss: 1.4208
Pretraining:	Epoch 197/200
----------
training:	Epoch: [197][1/233]	Loss 0.0001 (0.0001)	
training:	Epoch: [197][2/233]	Loss 0.0001 (0.0001)	
training:	Epoch: [197][3/233]	Loss 0.0007 (0.0003)	
training:	Epoch: [197][4/233]	Loss 0.0001 (0.0003)	
training:	Epoch: [197][5/233]	Loss 0.0001 (0.0002)	
training:	Epoch: [197][6/233]	Loss 0.0010 (0.0004)	
training:	Epoch: [197][7/233]	Loss 0.1374 (0.0199)	
training:	Epoch: [197][8/233]	Loss 0.0001 (0.0175)	
training:	Epoch: [197][9/233]	Loss 0.0001 (0.0155)	
training:	Epoch: [197][10/233]	Loss 0.0001 (0.0140)	
training:	Epoch: [197][11/233]	Loss 0.0001 (0.0127)	
training:	Epoch: [197][12/233]	Loss 0.0021 (0.0118)	
training:	Epoch: [197][13/233]	Loss 0.0001 (0.0109)	
training:	Epoch: [197][14/233]	Loss 0.0001 (0.0102)	
training:	Epoch: [197][15/233]	Loss 0.0067 (0.0099)	
training:	Epoch: [197][16/233]	Loss 0.0001 (0.0093)	
training:	Epoch: [197][17/233]	Loss 0.0001 (0.0088)	
training:	Epoch: [197][18/233]	Loss 0.0002 (0.0083)	
training:	Epoch: [197][19/233]	Loss 0.0001 (0.0079)	
training:	Epoch: [197][20/233]	Loss 0.0001 (0.0075)	
training:	Epoch: [197][21/233]	Loss 0.0001 (0.0071)	
training:	Epoch: [197][22/233]	Loss 0.0001 (0.0068)	
training:	Epoch: [197][23/233]	Loss 0.0002 (0.0065)	
training:	Epoch: [197][24/233]	Loss 0.0002 (0.0063)	
training:	Epoch: [197][25/233]	Loss 0.0001 (0.0060)	
training:	Epoch: [197][26/233]	Loss 0.0010 (0.0058)	
training:	Epoch: [197][27/233]	Loss 0.0003 (0.0056)	
training:	Epoch: [197][28/233]	Loss 0.0074 (0.0057)	
training:	Epoch: [197][29/233]	Loss 0.0001 (0.0055)	
training:	Epoch: [197][30/233]	Loss 0.0002 (0.0053)	
training:	Epoch: [197][31/233]	Loss 0.0004 (0.0052)	
training:	Epoch: [197][32/233]	Loss 0.0001 (0.0050)	
training:	Epoch: [197][33/233]	Loss 0.0001 (0.0049)	
training:	Epoch: [197][34/233]	Loss 0.0003 (0.0047)	
training:	Epoch: [197][35/233]	Loss 0.0003 (0.0046)	
training:	Epoch: [197][36/233]	Loss 0.0008 (0.0045)	
training:	Epoch: [197][37/233]	Loss 0.0001 (0.0044)	
training:	Epoch: [197][38/233]	Loss 0.0852 (0.0065)	
training:	Epoch: [197][39/233]	Loss 0.0001 (0.0063)	
training:	Epoch: [197][40/233]	Loss 0.0001 (0.0062)	
training:	Epoch: [197][41/233]	Loss 0.0001 (0.0060)	
training:	Epoch: [197][42/233]	Loss 0.0001 (0.0059)	
training:	Epoch: [197][43/233]	Loss 0.0003 (0.0058)	
training:	Epoch: [197][44/233]	Loss 0.0001 (0.0056)	
training:	Epoch: [197][45/233]	Loss 0.0001 (0.0055)	
training:	Epoch: [197][46/233]	Loss 0.0001 (0.0054)	
training:	Epoch: [197][47/233]	Loss 0.0001 (0.0053)	
training:	Epoch: [197][48/233]	Loss 0.0003 (0.0052)	
training:	Epoch: [197][49/233]	Loss 0.0001 (0.0051)	
training:	Epoch: [197][50/233]	Loss 0.0001 (0.0050)	
training:	Epoch: [197][51/233]	Loss 0.0001 (0.0049)	
training:	Epoch: [197][52/233]	Loss 0.0001 (0.0048)	
training:	Epoch: [197][53/233]	Loss 0.0202 (0.0051)	
training:	Epoch: [197][54/233]	Loss 0.0001 (0.0050)	
training:	Epoch: [197][55/233]	Loss 0.0001 (0.0049)	
training:	Epoch: [197][56/233]	Loss 0.0001 (0.0048)	
training:	Epoch: [197][57/233]	Loss 0.0002 (0.0047)	
training:	Epoch: [197][58/233]	Loss 0.0011 (0.0047)	
training:	Epoch: [197][59/233]	Loss 0.0001 (0.0046)	
training:	Epoch: [197][60/233]	Loss 0.0007 (0.0045)	
training:	Epoch: [197][61/233]	Loss 0.0012 (0.0045)	
training:	Epoch: [197][62/233]	Loss 0.0001 (0.0044)	
training:	Epoch: [197][63/233]	Loss 0.0005 (0.0043)	
training:	Epoch: [197][64/233]	Loss 0.0001 (0.0043)	
training:	Epoch: [197][65/233]	Loss 0.0001 (0.0042)	
training:	Epoch: [197][66/233]	Loss 0.0001 (0.0041)	
training:	Epoch: [197][67/233]	Loss 0.0009 (0.0041)	
training:	Epoch: [197][68/233]	Loss 0.0002 (0.0040)	
training:	Epoch: [197][69/233]	Loss 0.0001 (0.0040)	
training:	Epoch: [197][70/233]	Loss 0.0002 (0.0039)	
training:	Epoch: [197][71/233]	Loss 0.0009 (0.0039)	
training:	Epoch: [197][72/233]	Loss 0.0002 (0.0038)	
training:	Epoch: [197][73/233]	Loss 0.0002 (0.0038)	
training:	Epoch: [197][74/233]	Loss 0.0001 (0.0037)	
training:	Epoch: [197][75/233]	Loss 0.0002 (0.0037)	
training:	Epoch: [197][76/233]	Loss 0.0001 (0.0036)	
training:	Epoch: [197][77/233]	Loss 0.0001 (0.0036)	
training:	Epoch: [197][78/233]	Loss 0.0002 (0.0036)	
training:	Epoch: [197][79/233]	Loss 0.0001 (0.0035)	
training:	Epoch: [197][80/233]	Loss 0.0001 (0.0035)	
training:	Epoch: [197][81/233]	Loss 0.0001 (0.0034)	
training:	Epoch: [197][82/233]	Loss 0.0001 (0.0034)	
training:	Epoch: [197][83/233]	Loss 0.0014 (0.0034)	
training:	Epoch: [197][84/233]	Loss 0.0001 (0.0033)	
training:	Epoch: [197][85/233]	Loss 0.0001 (0.0033)	
training:	Epoch: [197][86/233]	Loss 0.0003 (0.0033)	
training:	Epoch: [197][87/233]	Loss 0.0001 (0.0032)	
training:	Epoch: [197][88/233]	Loss 0.0001 (0.0032)	
training:	Epoch: [197][89/233]	Loss 0.0001 (0.0031)	
training:	Epoch: [197][90/233]	Loss 0.0261 (0.0034)	
training:	Epoch: [197][91/233]	Loss 0.0002 (0.0034)	
training:	Epoch: [197][92/233]	Loss 0.0001 (0.0033)	
training:	Epoch: [197][93/233]	Loss 0.0004 (0.0033)	
training:	Epoch: [197][94/233]	Loss 0.0001 (0.0033)	
training:	Epoch: [197][95/233]	Loss 0.0001 (0.0032)	
training:	Epoch: [197][96/233]	Loss 0.0001 (0.0032)	
training:	Epoch: [197][97/233]	Loss 0.0001 (0.0032)	
training:	Epoch: [197][98/233]	Loss 0.0001 (0.0031)	
training:	Epoch: [197][99/233]	Loss 0.0001 (0.0031)	
training:	Epoch: [197][100/233]	Loss 0.0739 (0.0038)	
training:	Epoch: [197][101/233]	Loss 0.0001 (0.0038)	
training:	Epoch: [197][102/233]	Loss 0.0002 (0.0037)	
training:	Epoch: [197][103/233]	Loss 0.0001 (0.0037)	
training:	Epoch: [197][104/233]	Loss 0.0001 (0.0037)	
training:	Epoch: [197][105/233]	Loss 0.0001 (0.0036)	
training:	Epoch: [197][106/233]	Loss 0.0001 (0.0036)	
training:	Epoch: [197][107/233]	Loss 0.0001 (0.0036)	
training:	Epoch: [197][108/233]	Loss 0.0001 (0.0035)	
training:	Epoch: [197][109/233]	Loss 0.0002 (0.0035)	
training:	Epoch: [197][110/233]	Loss 0.0001 (0.0035)	
training:	Epoch: [197][111/233]	Loss 0.0001 (0.0034)	
training:	Epoch: [197][112/233]	Loss 0.0018 (0.0034)	
training:	Epoch: [197][113/233]	Loss 0.0002 (0.0034)	
training:	Epoch: [197][114/233]	Loss 0.0002 (0.0034)	
training:	Epoch: [197][115/233]	Loss 0.0001 (0.0033)	
training:	Epoch: [197][116/233]	Loss 0.0001 (0.0033)	
training:	Epoch: [197][117/233]	Loss 0.0001 (0.0033)	
training:	Epoch: [197][118/233]	Loss 0.0001 (0.0033)	
training:	Epoch: [197][119/233]	Loss 0.0007 (0.0032)	
training:	Epoch: [197][120/233]	Loss 0.0004 (0.0032)	
training:	Epoch: [197][121/233]	Loss 0.0001 (0.0032)	
training:	Epoch: [197][122/233]	Loss 0.0029 (0.0032)	
training:	Epoch: [197][123/233]	Loss 0.0001 (0.0032)	
training:	Epoch: [197][124/233]	Loss 0.0002 (0.0031)	
training:	Epoch: [197][125/233]	Loss 0.0001 (0.0031)	
training:	Epoch: [197][126/233]	Loss 0.0004 (0.0031)	
training:	Epoch: [197][127/233]	Loss 0.0001 (0.0031)	
training:	Epoch: [197][128/233]	Loss 0.0001 (0.0031)	
training:	Epoch: [197][129/233]	Loss 0.0067 (0.0031)	
training:	Epoch: [197][130/233]	Loss 0.0009 (0.0031)	
training:	Epoch: [197][131/233]	Loss 0.0001 (0.0030)	
training:	Epoch: [197][132/233]	Loss 0.0001 (0.0030)	
training:	Epoch: [197][133/233]	Loss 0.0001 (0.0030)	
training:	Epoch: [197][134/233]	Loss 0.0012 (0.0030)	
training:	Epoch: [197][135/233]	Loss 0.0001 (0.0030)	
training:	Epoch: [197][136/233]	Loss 0.0801 (0.0035)	
training:	Epoch: [197][137/233]	Loss 0.0265 (0.0037)	
training:	Epoch: [197][138/233]	Loss 0.0001 (0.0037)	
training:	Epoch: [197][139/233]	Loss 0.0001 (0.0036)	
training:	Epoch: [197][140/233]	Loss 0.0002 (0.0036)	
training:	Epoch: [197][141/233]	Loss 0.0001 (0.0036)	
training:	Epoch: [197][142/233]	Loss 0.0144 (0.0037)	
training:	Epoch: [197][143/233]	Loss 0.0001 (0.0036)	
training:	Epoch: [197][144/233]	Loss 0.0054 (0.0037)	
training:	Epoch: [197][145/233]	Loss 0.0018 (0.0036)	
training:	Epoch: [197][146/233]	Loss 0.0003 (0.0036)	
training:	Epoch: [197][147/233]	Loss 0.0001 (0.0036)	
training:	Epoch: [197][148/233]	Loss 0.0002 (0.0036)	
training:	Epoch: [197][149/233]	Loss 0.0019 (0.0036)	
training:	Epoch: [197][150/233]	Loss 0.0001 (0.0035)	
training:	Epoch: [197][151/233]	Loss 0.0003 (0.0035)	
training:	Epoch: [197][152/233]	Loss 0.0002 (0.0035)	
training:	Epoch: [197][153/233]	Loss 0.0001 (0.0035)	
training:	Epoch: [197][154/233]	Loss 0.0056 (0.0035)	
training:	Epoch: [197][155/233]	Loss 0.0004 (0.0035)	
training:	Epoch: [197][156/233]	Loss 0.0001 (0.0034)	
training:	Epoch: [197][157/233]	Loss 0.0001 (0.0034)	
training:	Epoch: [197][158/233]	Loss 0.0003 (0.0034)	
training:	Epoch: [197][159/233]	Loss 0.0001 (0.0034)	
training:	Epoch: [197][160/233]	Loss 0.0006 (0.0034)	
training:	Epoch: [197][161/233]	Loss 0.0001 (0.0033)	
training:	Epoch: [197][162/233]	Loss 0.0001 (0.0033)	
training:	Epoch: [197][163/233]	Loss 0.0001 (0.0033)	
training:	Epoch: [197][164/233]	Loss 0.0021 (0.0033)	
training:	Epoch: [197][165/233]	Loss 0.0001 (0.0033)	
training:	Epoch: [197][166/233]	Loss 0.0001 (0.0033)	
training:	Epoch: [197][167/233]	Loss 0.0004 (0.0032)	
training:	Epoch: [197][168/233]	Loss 0.0002 (0.0032)	
training:	Epoch: [197][169/233]	Loss 0.0001 (0.0032)	
training:	Epoch: [197][170/233]	Loss 0.0003 (0.0032)	
training:	Epoch: [197][171/233]	Loss 0.0002 (0.0032)	
training:	Epoch: [197][172/233]	Loss 0.0055 (0.0032)	
training:	Epoch: [197][173/233]	Loss 0.0041 (0.0032)	
training:	Epoch: [197][174/233]	Loss 0.0003 (0.0032)	
training:	Epoch: [197][175/233]	Loss 0.0001 (0.0032)	
training:	Epoch: [197][176/233]	Loss 0.0002 (0.0031)	
training:	Epoch: [197][177/233]	Loss 0.0003 (0.0031)	
training:	Epoch: [197][178/233]	Loss 0.0002 (0.0031)	
training:	Epoch: [197][179/233]	Loss 0.0001 (0.0031)	
training:	Epoch: [197][180/233]	Loss 0.0001 (0.0031)	
training:	Epoch: [197][181/233]	Loss 0.0001 (0.0031)	
training:	Epoch: [197][182/233]	Loss 0.0001 (0.0030)	
training:	Epoch: [197][183/233]	Loss 0.0001 (0.0030)	
training:	Epoch: [197][184/233]	Loss 0.0001 (0.0030)	
training:	Epoch: [197][185/233]	Loss 0.0002 (0.0030)	
training:	Epoch: [197][186/233]	Loss 0.0001 (0.0030)	
training:	Epoch: [197][187/233]	Loss 0.0001 (0.0030)	
training:	Epoch: [197][188/233]	Loss 0.0001 (0.0030)	
training:	Epoch: [197][189/233]	Loss 0.0001 (0.0029)	
training:	Epoch: [197][190/233]	Loss 0.0002 (0.0029)	
training:	Epoch: [197][191/233]	Loss 0.0001 (0.0029)	
training:	Epoch: [197][192/233]	Loss 0.0810 (0.0033)	
training:	Epoch: [197][193/233]	Loss 0.0002 (0.0033)	
training:	Epoch: [197][194/233]	Loss 0.0004 (0.0033)	
training:	Epoch: [197][195/233]	Loss 0.0576 (0.0036)	
training:	Epoch: [197][196/233]	Loss 0.0001 (0.0035)	
training:	Epoch: [197][197/233]	Loss 0.0002 (0.0035)	
training:	Epoch: [197][198/233]	Loss 0.0001 (0.0035)	
training:	Epoch: [197][199/233]	Loss 0.0001 (0.0035)	
training:	Epoch: [197][200/233]	Loss 0.0001 (0.0035)	
training:	Epoch: [197][201/233]	Loss 0.0001 (0.0035)	
training:	Epoch: [197][202/233]	Loss 0.0001 (0.0034)	
training:	Epoch: [197][203/233]	Loss 0.0015 (0.0034)	
training:	Epoch: [197][204/233]	Loss 0.0008 (0.0034)	
training:	Epoch: [197][205/233]	Loss 0.0001 (0.0034)	
training:	Epoch: [197][206/233]	Loss 0.0001 (0.0034)	
training:	Epoch: [197][207/233]	Loss 0.0035 (0.0034)	
training:	Epoch: [197][208/233]	Loss 0.0001 (0.0034)	
training:	Epoch: [197][209/233]	Loss 0.0002 (0.0034)	
training:	Epoch: [197][210/233]	Loss 0.0001 (0.0033)	
training:	Epoch: [197][211/233]	Loss 0.0148 (0.0034)	
training:	Epoch: [197][212/233]	Loss 0.0002 (0.0034)	
training:	Epoch: [197][213/233]	Loss 0.0001 (0.0034)	
training:	Epoch: [197][214/233]	Loss 0.0010 (0.0034)	
training:	Epoch: [197][215/233]	Loss 0.0012 (0.0033)	
training:	Epoch: [197][216/233]	Loss 0.0002 (0.0033)	
training:	Epoch: [197][217/233]	Loss 0.0006 (0.0033)	
training:	Epoch: [197][218/233]	Loss 0.0001 (0.0033)	
training:	Epoch: [197][219/233]	Loss 0.0001 (0.0033)	
training:	Epoch: [197][220/233]	Loss 0.0001 (0.0033)	
training:	Epoch: [197][221/233]	Loss 0.0002 (0.0033)	
training:	Epoch: [197][222/233]	Loss 0.0002 (0.0032)	
training:	Epoch: [197][223/233]	Loss 0.0001 (0.0032)	
training:	Epoch: [197][224/233]	Loss 0.0001 (0.0032)	
training:	Epoch: [197][225/233]	Loss 0.0002 (0.0032)	
training:	Epoch: [197][226/233]	Loss 0.0001 (0.0032)	
training:	Epoch: [197][227/233]	Loss 0.0001 (0.0032)	
training:	Epoch: [197][228/233]	Loss 0.0001 (0.0032)	
training:	Epoch: [197][229/233]	Loss 0.0001 (0.0031)	
training:	Epoch: [197][230/233]	Loss 0.0007 (0.0031)	
training:	Epoch: [197][231/233]	Loss 0.0003 (0.0031)	
training:	Epoch: [197][232/233]	Loss 0.0005 (0.0031)	
training:	Epoch: [197][233/233]	Loss 0.0008 (0.0031)	
Training:	 Loss: 0.0031

Training:	 ACC: 1.0000 1.0000 1.0000 1.0000
Validation:	 ACC: 0.7882 0.7897 0.8202 0.7562
Validation:	 Best_BACC: 0.8162 0.8154 0.7998 0.8326
Validation:	 Loss: 1.4504
Pretraining:	Epoch 198/200
----------
training:	Epoch: [198][1/233]	Loss 0.0106 (0.0106)	
training:	Epoch: [198][2/233]	Loss 0.0002 (0.0054)	
training:	Epoch: [198][3/233]	Loss 0.0002 (0.0037)	
training:	Epoch: [198][4/233]	Loss 0.0003 (0.0028)	
training:	Epoch: [198][5/233]	Loss 0.0001 (0.0023)	
training:	Epoch: [198][6/233]	Loss 0.0001 (0.0019)	
training:	Epoch: [198][7/233]	Loss 0.0001 (0.0017)	
training:	Epoch: [198][8/233]	Loss 0.0200 (0.0040)	
training:	Epoch: [198][9/233]	Loss 0.0002 (0.0035)	
training:	Epoch: [198][10/233]	Loss 0.0001 (0.0032)	
training:	Epoch: [198][11/233]	Loss 0.0002 (0.0029)	
training:	Epoch: [198][12/233]	Loss 0.0001 (0.0027)	
training:	Epoch: [198][13/233]	Loss 0.0001 (0.0025)	
training:	Epoch: [198][14/233]	Loss 0.0002 (0.0023)	
training:	Epoch: [198][15/233]	Loss 0.0003 (0.0022)	
training:	Epoch: [198][16/233]	Loss 0.0001 (0.0021)	
training:	Epoch: [198][17/233]	Loss 0.0001 (0.0020)	
training:	Epoch: [198][18/233]	Loss 0.0003 (0.0019)	
training:	Epoch: [198][19/233]	Loss 0.0001 (0.0018)	
training:	Epoch: [198][20/233]	Loss 0.0002 (0.0017)	
training:	Epoch: [198][21/233]	Loss 0.0016 (0.0017)	
training:	Epoch: [198][22/233]	Loss 0.0058 (0.0019)	
training:	Epoch: [198][23/233]	Loss 0.0001 (0.0018)	
training:	Epoch: [198][24/233]	Loss 0.0001 (0.0017)	
training:	Epoch: [198][25/233]	Loss 0.0002 (0.0017)	
training:	Epoch: [198][26/233]	Loss 0.0002 (0.0016)	
training:	Epoch: [198][27/233]	Loss 0.0003 (0.0016)	
training:	Epoch: [198][28/233]	Loss 0.0001 (0.0015)	
training:	Epoch: [198][29/233]	Loss 0.0003 (0.0015)	
training:	Epoch: [198][30/233]	Loss 0.0001 (0.0014)	
training:	Epoch: [198][31/233]	Loss 0.0003 (0.0014)	
training:	Epoch: [198][32/233]	Loss 0.0005 (0.0014)	
training:	Epoch: [198][33/233]	Loss 0.0002 (0.0013)	
training:	Epoch: [198][34/233]	Loss 0.0001 (0.0013)	
training:	Epoch: [198][35/233]	Loss 0.0003 (0.0013)	
training:	Epoch: [198][36/233]	Loss 0.0001 (0.0012)	
training:	Epoch: [198][37/233]	Loss 0.0001 (0.0012)	
training:	Epoch: [198][38/233]	Loss 0.0001 (0.0012)	
training:	Epoch: [198][39/233]	Loss 0.0001 (0.0011)	
training:	Epoch: [198][40/233]	Loss 0.0001 (0.0011)	
training:	Epoch: [198][41/233]	Loss 0.0001 (0.0011)	
training:	Epoch: [198][42/233]	Loss 0.0003 (0.0011)	
training:	Epoch: [198][43/233]	Loss 0.0001 (0.0011)	
training:	Epoch: [198][44/233]	Loss 0.0001 (0.0010)	
training:	Epoch: [198][45/233]	Loss 0.0001 (0.0010)	
training:	Epoch: [198][46/233]	Loss 0.0001 (0.0010)	
training:	Epoch: [198][47/233]	Loss 0.0001 (0.0010)	
training:	Epoch: [198][48/233]	Loss 0.0001 (0.0010)	
training:	Epoch: [198][49/233]	Loss 0.0004 (0.0009)	
training:	Epoch: [198][50/233]	Loss 0.0007 (0.0009)	
training:	Epoch: [198][51/233]	Loss 0.0001 (0.0009)	
training:	Epoch: [198][52/233]	Loss 0.0002 (0.0009)	
training:	Epoch: [198][53/233]	Loss 0.0001 (0.0009)	
training:	Epoch: [198][54/233]	Loss 0.0001 (0.0009)	
training:	Epoch: [198][55/233]	Loss 0.0001 (0.0009)	
training:	Epoch: [198][56/233]	Loss 0.0001 (0.0009)	
training:	Epoch: [198][57/233]	Loss 0.0016 (0.0009)	
training:	Epoch: [198][58/233]	Loss 0.0001 (0.0009)	
training:	Epoch: [198][59/233]	Loss 0.0005 (0.0008)	
training:	Epoch: [198][60/233]	Loss 0.0004 (0.0008)	
training:	Epoch: [198][61/233]	Loss 0.0001 (0.0008)	
training:	Epoch: [198][62/233]	Loss 0.0001 (0.0008)	
training:	Epoch: [198][63/233]	Loss 0.0001 (0.0008)	
training:	Epoch: [198][64/233]	Loss 0.0002 (0.0008)	
training:	Epoch: [198][65/233]	Loss 0.0004 (0.0008)	
training:	Epoch: [198][66/233]	Loss 0.0001 (0.0008)	
training:	Epoch: [198][67/233]	Loss 0.0002 (0.0008)	
training:	Epoch: [198][68/233]	Loss 0.0001 (0.0008)	
training:	Epoch: [198][69/233]	Loss 0.0001 (0.0008)	
training:	Epoch: [198][70/233]	Loss 0.0002 (0.0007)	
training:	Epoch: [198][71/233]	Loss 0.0003 (0.0007)	
training:	Epoch: [198][72/233]	Loss 0.0001 (0.0007)	
training:	Epoch: [198][73/233]	Loss 0.0001 (0.0007)	
training:	Epoch: [198][74/233]	Loss 0.0001 (0.0007)	
training:	Epoch: [198][75/233]	Loss 0.0002 (0.0007)	
training:	Epoch: [198][76/233]	Loss 0.0001 (0.0007)	
training:	Epoch: [198][77/233]	Loss 0.0001 (0.0007)	
training:	Epoch: [198][78/233]	Loss 0.0001 (0.0007)	
training:	Epoch: [198][79/233]	Loss 0.0001 (0.0007)	
training:	Epoch: [198][80/233]	Loss 0.0002 (0.0007)	
training:	Epoch: [198][81/233]	Loss 0.0001 (0.0007)	
training:	Epoch: [198][82/233]	Loss 0.0002 (0.0007)	
training:	Epoch: [198][83/233]	Loss 0.0001 (0.0006)	
training:	Epoch: [198][84/233]	Loss 0.0005 (0.0006)	
training:	Epoch: [198][85/233]	Loss 0.0001 (0.0006)	
training:	Epoch: [198][86/233]	Loss 0.0002 (0.0006)	
training:	Epoch: [198][87/233]	Loss 0.0001 (0.0006)	
training:	Epoch: [198][88/233]	Loss 0.0001 (0.0006)	
training:	Epoch: [198][89/233]	Loss 0.0001 (0.0006)	
training:	Epoch: [198][90/233]	Loss 0.0202 (0.0008)	
training:	Epoch: [198][91/233]	Loss 0.0002 (0.0008)	
training:	Epoch: [198][92/233]	Loss 0.0002 (0.0008)	
training:	Epoch: [198][93/233]	Loss 0.0001 (0.0008)	
training:	Epoch: [198][94/233]	Loss 0.0014 (0.0008)	
training:	Epoch: [198][95/233]	Loss 0.0001 (0.0008)	
training:	Epoch: [198][96/233]	Loss 0.0001 (0.0008)	
training:	Epoch: [198][97/233]	Loss 0.0001 (0.0008)	
training:	Epoch: [198][98/233]	Loss 0.0002 (0.0008)	
training:	Epoch: [198][99/233]	Loss 0.0018 (0.0008)	
training:	Epoch: [198][100/233]	Loss 0.0002 (0.0008)	
training:	Epoch: [198][101/233]	Loss 0.0001 (0.0008)	
training:	Epoch: [198][102/233]	Loss 0.0001 (0.0008)	
training:	Epoch: [198][103/233]	Loss 0.0001 (0.0008)	
training:	Epoch: [198][104/233]	Loss 0.0004 (0.0008)	
training:	Epoch: [198][105/233]	Loss 0.0048 (0.0008)	
training:	Epoch: [198][106/233]	Loss 0.0001 (0.0008)	
training:	Epoch: [198][107/233]	Loss 0.0031 (0.0008)	
training:	Epoch: [198][108/233]	Loss 0.0002 (0.0008)	
training:	Epoch: [198][109/233]	Loss 0.0001 (0.0008)	
training:	Epoch: [198][110/233]	Loss 0.0004 (0.0008)	
training:	Epoch: [198][111/233]	Loss 0.0001 (0.0008)	
training:	Epoch: [198][112/233]	Loss 0.0001 (0.0008)	
training:	Epoch: [198][113/233]	Loss 0.0001 (0.0008)	
training:	Epoch: [198][114/233]	Loss 0.0203 (0.0010)	
training:	Epoch: [198][115/233]	Loss 0.0013 (0.0010)	
training:	Epoch: [198][116/233]	Loss 0.0002 (0.0010)	
training:	Epoch: [198][117/233]	Loss 0.0001 (0.0010)	
training:	Epoch: [198][118/233]	Loss 0.0002 (0.0009)	
training:	Epoch: [198][119/233]	Loss 0.0001 (0.0009)	
training:	Epoch: [198][120/233]	Loss 0.0001 (0.0009)	
training:	Epoch: [198][121/233]	Loss 0.0001 (0.0009)	
training:	Epoch: [198][122/233]	Loss 0.0011 (0.0009)	
training:	Epoch: [198][123/233]	Loss 0.0001 (0.0009)	
training:	Epoch: [198][124/233]	Loss 0.0003 (0.0009)	
training:	Epoch: [198][125/233]	Loss 0.0116 (0.0010)	
training:	Epoch: [198][126/233]	Loss 0.0003 (0.0010)	
training:	Epoch: [198][127/233]	Loss 0.0002 (0.0010)	
training:	Epoch: [198][128/233]	Loss 0.0001 (0.0010)	
training:	Epoch: [198][129/233]	Loss 0.0002 (0.0010)	
training:	Epoch: [198][130/233]	Loss 0.0002 (0.0010)	
training:	Epoch: [198][131/233]	Loss 0.0003 (0.0010)	
training:	Epoch: [198][132/233]	Loss 0.0002 (0.0010)	
training:	Epoch: [198][133/233]	Loss 0.0002 (0.0010)	
training:	Epoch: [198][134/233]	Loss 0.0001 (0.0009)	
training:	Epoch: [198][135/233]	Loss 0.0001 (0.0009)	
training:	Epoch: [198][136/233]	Loss 0.0001 (0.0009)	
training:	Epoch: [198][137/233]	Loss 0.0018 (0.0009)	
training:	Epoch: [198][138/233]	Loss 0.0002 (0.0009)	
training:	Epoch: [198][139/233]	Loss 0.0001 (0.0009)	
training:	Epoch: [198][140/233]	Loss 0.0001 (0.0009)	
training:	Epoch: [198][141/233]	Loss 0.0347 (0.0012)	
training:	Epoch: [198][142/233]	Loss 0.0001 (0.0012)	
training:	Epoch: [198][143/233]	Loss 0.0001 (0.0011)	
training:	Epoch: [198][144/233]	Loss 0.0001 (0.0011)	
training:	Epoch: [198][145/233]	Loss 0.0001 (0.0011)	
training:	Epoch: [198][146/233]	Loss 0.0002 (0.0011)	
training:	Epoch: [198][147/233]	Loss 0.0001 (0.0011)	
training:	Epoch: [198][148/233]	Loss 0.0025 (0.0011)	
training:	Epoch: [198][149/233]	Loss 0.0002 (0.0011)	
training:	Epoch: [198][150/233]	Loss 0.0001 (0.0011)	
training:	Epoch: [198][151/233]	Loss 0.0001 (0.0011)	
training:	Epoch: [198][152/233]	Loss 0.0002 (0.0011)	
training:	Epoch: [198][153/233]	Loss 0.0001 (0.0011)	
training:	Epoch: [198][154/233]	Loss 0.0001 (0.0011)	
training:	Epoch: [198][155/233]	Loss 0.0002 (0.0011)	
training:	Epoch: [198][156/233]	Loss 0.0002 (0.0011)	
training:	Epoch: [198][157/233]	Loss 0.0002 (0.0011)	
training:	Epoch: [198][158/233]	Loss 0.1455 (0.0020)	
training:	Epoch: [198][159/233]	Loss 0.0001 (0.0020)	
training:	Epoch: [198][160/233]	Loss 0.0001 (0.0020)	
training:	Epoch: [198][161/233]	Loss 0.0001 (0.0020)	
training:	Epoch: [198][162/233]	Loss 0.0001 (0.0019)	
training:	Epoch: [198][163/233]	Loss 0.0001 (0.0019)	
training:	Epoch: [198][164/233]	Loss 0.0012 (0.0019)	
training:	Epoch: [198][165/233]	Loss 0.0002 (0.0019)	
training:	Epoch: [198][166/233]	Loss 0.0001 (0.0019)	
training:	Epoch: [198][167/233]	Loss 0.0004 (0.0019)	
training:	Epoch: [198][168/233]	Loss 0.0001 (0.0019)	
training:	Epoch: [198][169/233]	Loss 0.0002 (0.0019)	
training:	Epoch: [198][170/233]	Loss 0.0001 (0.0019)	
training:	Epoch: [198][171/233]	Loss 0.0001 (0.0019)	
training:	Epoch: [198][172/233]	Loss 0.0002 (0.0018)	
training:	Epoch: [198][173/233]	Loss 0.0001 (0.0018)	
training:	Epoch: [198][174/233]	Loss 0.0001 (0.0018)	
training:	Epoch: [198][175/233]	Loss 0.0001 (0.0018)	
training:	Epoch: [198][176/233]	Loss 0.0001 (0.0018)	
training:	Epoch: [198][177/233]	Loss 0.0001 (0.0018)	
training:	Epoch: [198][178/233]	Loss 0.0001 (0.0018)	
training:	Epoch: [198][179/233]	Loss 0.0001 (0.0018)	
training:	Epoch: [198][180/233]	Loss 0.0004 (0.0018)	
training:	Epoch: [198][181/233]	Loss 0.0313 (0.0019)	
training:	Epoch: [198][182/233]	Loss 0.0002 (0.0019)	
training:	Epoch: [198][183/233]	Loss 0.0001 (0.0019)	
training:	Epoch: [198][184/233]	Loss 0.0001 (0.0019)	
training:	Epoch: [198][185/233]	Loss 0.0001 (0.0019)	
training:	Epoch: [198][186/233]	Loss 0.0001 (0.0019)	
training:	Epoch: [198][187/233]	Loss 0.0001 (0.0019)	
training:	Epoch: [198][188/233]	Loss 0.0006 (0.0019)	
training:	Epoch: [198][189/233]	Loss 0.0001 (0.0019)	
training:	Epoch: [198][190/233]	Loss 0.0001 (0.0019)	
training:	Epoch: [198][191/233]	Loss 0.0001 (0.0018)	
training:	Epoch: [198][192/233]	Loss 0.0003 (0.0018)	
training:	Epoch: [198][193/233]	Loss 0.0011 (0.0018)	
training:	Epoch: [198][194/233]	Loss 0.0001 (0.0018)	
training:	Epoch: [198][195/233]	Loss 0.0002 (0.0018)	
training:	Epoch: [198][196/233]	Loss 0.0018 (0.0018)	
training:	Epoch: [198][197/233]	Loss 0.0001 (0.0018)	
training:	Epoch: [198][198/233]	Loss 0.0002 (0.0018)	
training:	Epoch: [198][199/233]	Loss 0.0001 (0.0018)	
training:	Epoch: [198][200/233]	Loss 0.0002 (0.0018)	
training:	Epoch: [198][201/233]	Loss 0.0001 (0.0018)	
training:	Epoch: [198][202/233]	Loss 0.0008 (0.0018)	
training:	Epoch: [198][203/233]	Loss 0.0002 (0.0018)	
training:	Epoch: [198][204/233]	Loss 0.0001 (0.0018)	
training:	Epoch: [198][205/233]	Loss 0.0001 (0.0017)	
training:	Epoch: [198][206/233]	Loss 0.0001 (0.0017)	
training:	Epoch: [198][207/233]	Loss 0.0001 (0.0017)	
training:	Epoch: [198][208/233]	Loss 0.0001 (0.0017)	
training:	Epoch: [198][209/233]	Loss 0.0024 (0.0017)	
training:	Epoch: [198][210/233]	Loss 0.0001 (0.0017)	
training:	Epoch: [198][211/233]	Loss 0.0001 (0.0017)	
training:	Epoch: [198][212/233]	Loss 0.0001 (0.0017)	
training:	Epoch: [198][213/233]	Loss 0.0001 (0.0017)	
training:	Epoch: [198][214/233]	Loss 0.0004 (0.0017)	
training:	Epoch: [198][215/233]	Loss 0.0002 (0.0017)	
training:	Epoch: [198][216/233]	Loss 0.0001 (0.0017)	
training:	Epoch: [198][217/233]	Loss 0.0001 (0.0017)	
training:	Epoch: [198][218/233]	Loss 0.1540 (0.0024)	
training:	Epoch: [198][219/233]	Loss 0.0001 (0.0024)	
training:	Epoch: [198][220/233]	Loss 0.0001 (0.0023)	
training:	Epoch: [198][221/233]	Loss 0.0011 (0.0023)	
training:	Epoch: [198][222/233]	Loss 0.0001 (0.0023)	
training:	Epoch: [198][223/233]	Loss 0.0001 (0.0023)	
training:	Epoch: [198][224/233]	Loss 0.0001 (0.0023)	
training:	Epoch: [198][225/233]	Loss 0.0003 (0.0023)	
training:	Epoch: [198][226/233]	Loss 0.0006 (0.0023)	
training:	Epoch: [198][227/233]	Loss 0.0001 (0.0023)	
training:	Epoch: [198][228/233]	Loss 0.0001 (0.0023)	
training:	Epoch: [198][229/233]	Loss 0.0100 (0.0023)	
training:	Epoch: [198][230/233]	Loss 0.0001 (0.0023)	
training:	Epoch: [198][231/233]	Loss 0.0001 (0.0023)	
training:	Epoch: [198][232/233]	Loss 0.0020 (0.0023)	
training:	Epoch: [198][233/233]	Loss 0.0002 (0.0023)	
Training:	 Loss: 0.0023

Training:	 ACC: 1.0000 1.0000 1.0000 1.0000
Validation:	 ACC: 0.7961 0.7972 0.8192 0.7730
Validation:	 Best_BACC: 0.8162 0.8154 0.7998 0.8326
Validation:	 Loss: 1.3986
Pretraining:	Epoch 199/200
----------
training:	Epoch: [199][1/233]	Loss 0.0017 (0.0017)	
training:	Epoch: [199][2/233]	Loss 0.0008 (0.0012)	
training:	Epoch: [199][3/233]	Loss 0.0001 (0.0009)	
training:	Epoch: [199][4/233]	Loss 0.0428 (0.0113)	
training:	Epoch: [199][5/233]	Loss 0.0003 (0.0091)	
training:	Epoch: [199][6/233]	Loss 0.0063 (0.0087)	
training:	Epoch: [199][7/233]	Loss 0.0001 (0.0074)	
training:	Epoch: [199][8/233]	Loss 0.0003 (0.0065)	
training:	Epoch: [199][9/233]	Loss 0.0002 (0.0058)	
training:	Epoch: [199][10/233]	Loss 0.0002 (0.0053)	
training:	Epoch: [199][11/233]	Loss 0.0001 (0.0048)	
training:	Epoch: [199][12/233]	Loss 0.0002 (0.0044)	
training:	Epoch: [199][13/233]	Loss 0.0002 (0.0041)	
training:	Epoch: [199][14/233]	Loss 0.0001 (0.0038)	
training:	Epoch: [199][15/233]	Loss 0.0003 (0.0036)	
training:	Epoch: [199][16/233]	Loss 0.0001 (0.0034)	
training:	Epoch: [199][17/233]	Loss 0.0001 (0.0032)	
training:	Epoch: [199][18/233]	Loss 0.0001 (0.0030)	
training:	Epoch: [199][19/233]	Loss 0.0001 (0.0028)	
training:	Epoch: [199][20/233]	Loss 0.0047 (0.0029)	
training:	Epoch: [199][21/233]	Loss 0.0001 (0.0028)	
training:	Epoch: [199][22/233]	Loss 0.0001 (0.0027)	
training:	Epoch: [199][23/233]	Loss 0.0002 (0.0026)	
training:	Epoch: [199][24/233]	Loss 0.0001 (0.0025)	
training:	Epoch: [199][25/233]	Loss 0.0001 (0.0024)	
training:	Epoch: [199][26/233]	Loss 0.0001 (0.0023)	
training:	Epoch: [199][27/233]	Loss 0.0047 (0.0024)	
training:	Epoch: [199][28/233]	Loss 0.0001 (0.0023)	
training:	Epoch: [199][29/233]	Loss 0.0001 (0.0022)	
training:	Epoch: [199][30/233]	Loss 0.0003 (0.0022)	
training:	Epoch: [199][31/233]	Loss 0.0001 (0.0021)	
training:	Epoch: [199][32/233]	Loss 0.0001 (0.0020)	
training:	Epoch: [199][33/233]	Loss 0.0001 (0.0020)	
training:	Epoch: [199][34/233]	Loss 0.0002 (0.0019)	
training:	Epoch: [199][35/233]	Loss 0.0001 (0.0019)	
training:	Epoch: [199][36/233]	Loss 0.0006 (0.0018)	
training:	Epoch: [199][37/233]	Loss 0.0001 (0.0018)	
training:	Epoch: [199][38/233]	Loss 0.0002 (0.0017)	
training:	Epoch: [199][39/233]	Loss 0.0001 (0.0017)	
training:	Epoch: [199][40/233]	Loss 0.0001 (0.0017)	
training:	Epoch: [199][41/233]	Loss 0.0001 (0.0016)	
training:	Epoch: [199][42/233]	Loss 0.0002 (0.0016)	
training:	Epoch: [199][43/233]	Loss 0.0296 (0.0022)	
training:	Epoch: [199][44/233]	Loss 0.0002 (0.0022)	
training:	Epoch: [199][45/233]	Loss 0.0001 (0.0021)	
training:	Epoch: [199][46/233]	Loss 0.0001 (0.0021)	
training:	Epoch: [199][47/233]	Loss 0.0001 (0.0021)	
training:	Epoch: [199][48/233]	Loss 0.0001 (0.0020)	
training:	Epoch: [199][49/233]	Loss 0.0001 (0.0020)	
training:	Epoch: [199][50/233]	Loss 0.0002 (0.0019)	
training:	Epoch: [199][51/233]	Loss 0.0001 (0.0019)	
training:	Epoch: [199][52/233]	Loss 0.0002 (0.0019)	
training:	Epoch: [199][53/233]	Loss 0.0001 (0.0018)	
training:	Epoch: [199][54/233]	Loss 0.0001 (0.0018)	
training:	Epoch: [199][55/233]	Loss 0.0001 (0.0018)	
training:	Epoch: [199][56/233]	Loss 0.0001 (0.0018)	
training:	Epoch: [199][57/233]	Loss 0.0001 (0.0017)	
training:	Epoch: [199][58/233]	Loss 0.0001 (0.0017)	
training:	Epoch: [199][59/233]	Loss 0.0003 (0.0017)	
training:	Epoch: [199][60/233]	Loss 0.0003 (0.0016)	
training:	Epoch: [199][61/233]	Loss 0.0003 (0.0016)	
training:	Epoch: [199][62/233]	Loss 0.0001 (0.0016)	
training:	Epoch: [199][63/233]	Loss 0.0002 (0.0016)	
training:	Epoch: [199][64/233]	Loss 0.0048 (0.0016)	
training:	Epoch: [199][65/233]	Loss 0.0014 (0.0016)	
training:	Epoch: [199][66/233]	Loss 0.0002 (0.0016)	
training:	Epoch: [199][67/233]	Loss 0.0002 (0.0016)	
training:	Epoch: [199][68/233]	Loss 0.0001 (0.0016)	
training:	Epoch: [199][69/233]	Loss 0.0001 (0.0015)	
training:	Epoch: [199][70/233]	Loss 0.0001 (0.0015)	
training:	Epoch: [199][71/233]	Loss 0.0013 (0.0015)	
training:	Epoch: [199][72/233]	Loss 0.0026 (0.0015)	
training:	Epoch: [199][73/233]	Loss 0.0005 (0.0015)	
training:	Epoch: [199][74/233]	Loss 0.0002 (0.0015)	
training:	Epoch: [199][75/233]	Loss 0.0004 (0.0015)	
training:	Epoch: [199][76/233]	Loss 0.0017 (0.0015)	
training:	Epoch: [199][77/233]	Loss 0.0001 (0.0015)	
training:	Epoch: [199][78/233]	Loss 0.0001 (0.0015)	
training:	Epoch: [199][79/233]	Loss 0.0003 (0.0014)	
training:	Epoch: [199][80/233]	Loss 0.0001 (0.0014)	
training:	Epoch: [199][81/233]	Loss 0.0001 (0.0014)	
training:	Epoch: [199][82/233]	Loss 0.0001 (0.0014)	
training:	Epoch: [199][83/233]	Loss 0.0001 (0.0014)	
training:	Epoch: [199][84/233]	Loss 0.0003 (0.0014)	
training:	Epoch: [199][85/233]	Loss 0.0001 (0.0013)	
training:	Epoch: [199][86/233]	Loss 0.0001 (0.0013)	
training:	Epoch: [199][87/233]	Loss 0.0002 (0.0013)	
training:	Epoch: [199][88/233]	Loss 0.0001 (0.0013)	
training:	Epoch: [199][89/233]	Loss 0.0001 (0.0013)	
training:	Epoch: [199][90/233]	Loss 0.0001 (0.0013)	
training:	Epoch: [199][91/233]	Loss 0.0002 (0.0013)	
training:	Epoch: [199][92/233]	Loss 0.0002 (0.0013)	
training:	Epoch: [199][93/233]	Loss 0.0003 (0.0012)	
training:	Epoch: [199][94/233]	Loss 0.0001 (0.0012)	
training:	Epoch: [199][95/233]	Loss 0.0001 (0.0012)	
training:	Epoch: [199][96/233]	Loss 0.0001 (0.0012)	
training:	Epoch: [199][97/233]	Loss 0.0001 (0.0012)	
training:	Epoch: [199][98/233]	Loss 0.0001 (0.0012)	
training:	Epoch: [199][99/233]	Loss 0.0001 (0.0012)	
training:	Epoch: [199][100/233]	Loss 0.0002 (0.0012)	
training:	Epoch: [199][101/233]	Loss 0.0001 (0.0012)	
training:	Epoch: [199][102/233]	Loss 0.0002 (0.0011)	
training:	Epoch: [199][103/233]	Loss 0.0001 (0.0011)	
training:	Epoch: [199][104/233]	Loss 0.0001 (0.0011)	
training:	Epoch: [199][105/233]	Loss 0.0001 (0.0011)	
training:	Epoch: [199][106/233]	Loss 0.0017 (0.0011)	
training:	Epoch: [199][107/233]	Loss 0.0001 (0.0011)	
training:	Epoch: [199][108/233]	Loss 0.0004 (0.0011)	
training:	Epoch: [199][109/233]	Loss 0.0002 (0.0011)	
training:	Epoch: [199][110/233]	Loss 0.0003 (0.0011)	
training:	Epoch: [199][111/233]	Loss 0.0002 (0.0011)	
training:	Epoch: [199][112/233]	Loss 0.0001 (0.0011)	
training:	Epoch: [199][113/233]	Loss 0.0007 (0.0011)	
training:	Epoch: [199][114/233]	Loss 0.0001 (0.0011)	
training:	Epoch: [199][115/233]	Loss 0.0002 (0.0011)	
training:	Epoch: [199][116/233]	Loss 0.0001 (0.0010)	
training:	Epoch: [199][117/233]	Loss 0.0004 (0.0010)	
training:	Epoch: [199][118/233]	Loss 0.0001 (0.0010)	
training:	Epoch: [199][119/233]	Loss 0.0001 (0.0010)	
training:	Epoch: [199][120/233]	Loss 0.0003 (0.0010)	
training:	Epoch: [199][121/233]	Loss 0.0001 (0.0010)	
training:	Epoch: [199][122/233]	Loss 0.0001 (0.0010)	
training:	Epoch: [199][123/233]	Loss 0.0003 (0.0010)	
training:	Epoch: [199][124/233]	Loss 0.0001 (0.0010)	
training:	Epoch: [199][125/233]	Loss 0.0002 (0.0010)	
training:	Epoch: [199][126/233]	Loss 0.0001 (0.0010)	
training:	Epoch: [199][127/233]	Loss 0.0001 (0.0010)	
training:	Epoch: [199][128/233]	Loss 0.0003 (0.0010)	
training:	Epoch: [199][129/233]	Loss 0.0001 (0.0010)	
training:	Epoch: [199][130/233]	Loss 0.0005 (0.0010)	
training:	Epoch: [199][131/233]	Loss 0.0002 (0.0009)	
training:	Epoch: [199][132/233]	Loss 0.0002 (0.0009)	
training:	Epoch: [199][133/233]	Loss 0.0060 (0.0010)	
training:	Epoch: [199][134/233]	Loss 0.0002 (0.0010)	
training:	Epoch: [199][135/233]	Loss 0.0002 (0.0010)	
training:	Epoch: [199][136/233]	Loss 0.0004 (0.0010)	
training:	Epoch: [199][137/233]	Loss 0.0001 (0.0010)	
training:	Epoch: [199][138/233]	Loss 0.0001 (0.0010)	
training:	Epoch: [199][139/233]	Loss 0.0001 (0.0009)	
training:	Epoch: [199][140/233]	Loss 0.0001 (0.0009)	
training:	Epoch: [199][141/233]	Loss 0.0002 (0.0009)	
training:	Epoch: [199][142/233]	Loss 0.0001 (0.0009)	
training:	Epoch: [199][143/233]	Loss 0.0002 (0.0009)	
training:	Epoch: [199][144/233]	Loss 0.0001 (0.0009)	
training:	Epoch: [199][145/233]	Loss 0.0002 (0.0009)	
training:	Epoch: [199][146/233]	Loss 0.0002 (0.0009)	
training:	Epoch: [199][147/233]	Loss 0.0001 (0.0009)	
training:	Epoch: [199][148/233]	Loss 0.0001 (0.0009)	
training:	Epoch: [199][149/233]	Loss 0.0001 (0.0009)	
training:	Epoch: [199][150/233]	Loss 0.0003 (0.0009)	
training:	Epoch: [199][151/233]	Loss 0.0003 (0.0009)	
training:	Epoch: [199][152/233]	Loss 0.0001 (0.0009)	
training:	Epoch: [199][153/233]	Loss 0.0096 (0.0009)	
training:	Epoch: [199][154/233]	Loss 0.0082 (0.0010)	
training:	Epoch: [199][155/233]	Loss 0.0149 (0.0011)	
training:	Epoch: [199][156/233]	Loss 0.0004 (0.0011)	
training:	Epoch: [199][157/233]	Loss 0.0001 (0.0011)	
training:	Epoch: [199][158/233]	Loss 0.0001 (0.0011)	
training:	Epoch: [199][159/233]	Loss 0.0001 (0.0011)	
training:	Epoch: [199][160/233]	Loss 0.0002 (0.0010)	
training:	Epoch: [199][161/233]	Loss 0.0002 (0.0010)	
training:	Epoch: [199][162/233]	Loss 0.0002 (0.0010)	
training:	Epoch: [199][163/233]	Loss 0.0001 (0.0010)	
training:	Epoch: [199][164/233]	Loss 0.0001 (0.0010)	
training:	Epoch: [199][165/233]	Loss 0.0001 (0.0010)	
training:	Epoch: [199][166/233]	Loss 0.0079 (0.0011)	
training:	Epoch: [199][167/233]	Loss 0.0003 (0.0011)	
training:	Epoch: [199][168/233]	Loss 0.0032 (0.0011)	
training:	Epoch: [199][169/233]	Loss 0.0002 (0.0011)	
training:	Epoch: [199][170/233]	Loss 0.0745 (0.0015)	
training:	Epoch: [199][171/233]	Loss 0.0001 (0.0015)	
training:	Epoch: [199][172/233]	Loss 0.0006 (0.0015)	
training:	Epoch: [199][173/233]	Loss 0.0001 (0.0015)	
training:	Epoch: [199][174/233]	Loss 0.0003 (0.0015)	
training:	Epoch: [199][175/233]	Loss 0.0006 (0.0015)	
training:	Epoch: [199][176/233]	Loss 0.0007 (0.0015)	
training:	Epoch: [199][177/233]	Loss 0.0001 (0.0015)	
training:	Epoch: [199][178/233]	Loss 0.0005 (0.0014)	
training:	Epoch: [199][179/233]	Loss 0.0007 (0.0014)	
training:	Epoch: [199][180/233]	Loss 0.0020 (0.0014)	
training:	Epoch: [199][181/233]	Loss 0.0001 (0.0014)	
training:	Epoch: [199][182/233]	Loss 0.1029 (0.0020)	
training:	Epoch: [199][183/233]	Loss 0.0002 (0.0020)	
training:	Epoch: [199][184/233]	Loss 0.0003 (0.0020)	
training:	Epoch: [199][185/233]	Loss 0.0003 (0.0020)	
training:	Epoch: [199][186/233]	Loss 0.0118 (0.0020)	
training:	Epoch: [199][187/233]	Loss 0.0002 (0.0020)	
training:	Epoch: [199][188/233]	Loss 0.0008 (0.0020)	
training:	Epoch: [199][189/233]	Loss 0.0001 (0.0020)	
training:	Epoch: [199][190/233]	Loss 0.0001 (0.0020)	
training:	Epoch: [199][191/233]	Loss 0.0003 (0.0020)	
training:	Epoch: [199][192/233]	Loss 0.0003 (0.0020)	
training:	Epoch: [199][193/233]	Loss 0.0002 (0.0020)	
training:	Epoch: [199][194/233]	Loss 0.0002 (0.0019)	
training:	Epoch: [199][195/233]	Loss 0.0003 (0.0019)	
training:	Epoch: [199][196/233]	Loss 0.0001 (0.0019)	
training:	Epoch: [199][197/233]	Loss 0.0002 (0.0019)	
training:	Epoch: [199][198/233]	Loss 0.0004 (0.0019)	
training:	Epoch: [199][199/233]	Loss 0.0002 (0.0019)	
training:	Epoch: [199][200/233]	Loss 0.0033 (0.0019)	
training:	Epoch: [199][201/233]	Loss 0.0004 (0.0019)	
training:	Epoch: [199][202/233]	Loss 0.0001 (0.0019)	
training:	Epoch: [199][203/233]	Loss 0.0004 (0.0019)	
training:	Epoch: [199][204/233]	Loss 0.0001 (0.0019)	
training:	Epoch: [199][205/233]	Loss 0.0001 (0.0019)	
training:	Epoch: [199][206/233]	Loss 0.0001 (0.0019)	
training:	Epoch: [199][207/233]	Loss 0.0001 (0.0019)	
training:	Epoch: [199][208/233]	Loss 0.0001 (0.0018)	
training:	Epoch: [199][209/233]	Loss 0.0001 (0.0018)	
training:	Epoch: [199][210/233]	Loss 0.0003 (0.0018)	
training:	Epoch: [199][211/233]	Loss 0.0005 (0.0018)	
training:	Epoch: [199][212/233]	Loss 0.0002 (0.0018)	
training:	Epoch: [199][213/233]	Loss 0.0038 (0.0018)	
training:	Epoch: [199][214/233]	Loss 0.0002 (0.0018)	
training:	Epoch: [199][215/233]	Loss 0.0003 (0.0018)	
training:	Epoch: [199][216/233]	Loss 0.0001 (0.0018)	
training:	Epoch: [199][217/233]	Loss 0.0002 (0.0018)	
training:	Epoch: [199][218/233]	Loss 0.0001 (0.0018)	
training:	Epoch: [199][219/233]	Loss 0.0001 (0.0018)	
training:	Epoch: [199][220/233]	Loss 0.0002 (0.0018)	
training:	Epoch: [199][221/233]	Loss 0.0009 (0.0018)	
training:	Epoch: [199][222/233]	Loss 0.0002 (0.0018)	
training:	Epoch: [199][223/233]	Loss 0.0002 (0.0018)	
training:	Epoch: [199][224/233]	Loss 0.0001 (0.0017)	
training:	Epoch: [199][225/233]	Loss 0.0001 (0.0017)	
training:	Epoch: [199][226/233]	Loss 0.0005 (0.0017)	
training:	Epoch: [199][227/233]	Loss 0.0001 (0.0017)	
training:	Epoch: [199][228/233]	Loss 0.0001 (0.0017)	
training:	Epoch: [199][229/233]	Loss 0.0001 (0.0017)	
training:	Epoch: [199][230/233]	Loss 0.0386 (0.0019)	
training:	Epoch: [199][231/233]	Loss 0.0001 (0.0019)	
training:	Epoch: [199][232/233]	Loss 0.0001 (0.0019)	
training:	Epoch: [199][233/233]	Loss 0.0002 (0.0019)	
Training:	 Loss: 0.0018

Training:	 ACC: 1.0000 1.0000 1.0000 1.0000
Validation:	 ACC: 0.7975 0.7983 0.8141 0.7809
Validation:	 Best_BACC: 0.8162 0.8154 0.7998 0.8326
Validation:	 Loss: 1.4278
Pretraining:	Epoch 200/200
----------
training:	Epoch: [200][1/233]	Loss 0.0001 (0.0001)	
training:	Epoch: [200][2/233]	Loss 0.0001 (0.0001)	
training:	Epoch: [200][3/233]	Loss 0.0001 (0.0001)	
training:	Epoch: [200][4/233]	Loss 0.0006 (0.0003)	
training:	Epoch: [200][5/233]	Loss 0.0001 (0.0002)	
training:	Epoch: [200][6/233]	Loss 0.0001 (0.0002)	
training:	Epoch: [200][7/233]	Loss 0.0001 (0.0002)	
training:	Epoch: [200][8/233]	Loss 0.0001 (0.0002)	
training:	Epoch: [200][9/233]	Loss 0.0005 (0.0002)	
training:	Epoch: [200][10/233]	Loss 0.0002 (0.0002)	
training:	Epoch: [200][11/233]	Loss 0.0002 (0.0002)	
training:	Epoch: [200][12/233]	Loss 0.0001 (0.0002)	
training:	Epoch: [200][13/233]	Loss 0.0001 (0.0002)	
training:	Epoch: [200][14/233]	Loss 0.0001 (0.0002)	
training:	Epoch: [200][15/233]	Loss 0.0001 (0.0002)	
training:	Epoch: [200][16/233]	Loss 0.0003 (0.0002)	
training:	Epoch: [200][17/233]	Loss 0.0001 (0.0002)	
training:	Epoch: [200][18/233]	Loss 0.0001 (0.0002)	
training:	Epoch: [200][19/233]	Loss 0.0013 (0.0002)	
training:	Epoch: [200][20/233]	Loss 0.0001 (0.0002)	
training:	Epoch: [200][21/233]	Loss 0.0002 (0.0002)	
training:	Epoch: [200][22/233]	Loss 0.0001 (0.0002)	
training:	Epoch: [200][23/233]	Loss 0.0002 (0.0002)	
training:	Epoch: [200][24/233]	Loss 0.0508 (0.0023)	
training:	Epoch: [200][25/233]	Loss 0.0001 (0.0022)	
training:	Epoch: [200][26/233]	Loss 0.0003 (0.0022)	
training:	Epoch: [200][27/233]	Loss 0.0023 (0.0022)	
training:	Epoch: [200][28/233]	Loss 0.0002 (0.0021)	
training:	Epoch: [200][29/233]	Loss 0.0001 (0.0020)	
training:	Epoch: [200][30/233]	Loss 0.0002 (0.0020)	
training:	Epoch: [200][31/233]	Loss 0.0001 (0.0019)	
training:	Epoch: [200][32/233]	Loss 0.0003 (0.0019)	
training:	Epoch: [200][33/233]	Loss 0.0001 (0.0018)	
training:	Epoch: [200][34/233]	Loss 0.0002 (0.0018)	
training:	Epoch: [200][35/233]	Loss 0.0002 (0.0017)	
training:	Epoch: [200][36/233]	Loss 0.0002 (0.0017)	
training:	Epoch: [200][37/233]	Loss 0.0057 (0.0018)	
training:	Epoch: [200][38/233]	Loss 0.0001 (0.0017)	
training:	Epoch: [200][39/233]	Loss 0.0002 (0.0017)	
training:	Epoch: [200][40/233]	Loss 0.0001 (0.0017)	
training:	Epoch: [200][41/233]	Loss 0.0002 (0.0016)	
training:	Epoch: [200][42/233]	Loss 0.0001 (0.0016)	
training:	Epoch: [200][43/233]	Loss 0.0001 (0.0016)	
training:	Epoch: [200][44/233]	Loss 0.0001 (0.0015)	
training:	Epoch: [200][45/233]	Loss 0.1049 (0.0038)	
training:	Epoch: [200][46/233]	Loss 0.0001 (0.0037)	
training:	Epoch: [200][47/233]	Loss 0.0001 (0.0037)	
training:	Epoch: [200][48/233]	Loss 0.0004 (0.0036)	
training:	Epoch: [200][49/233]	Loss 0.0019 (0.0036)	
training:	Epoch: [200][50/233]	Loss 0.0002 (0.0035)	
training:	Epoch: [200][51/233]	Loss 0.0001 (0.0034)	
training:	Epoch: [200][52/233]	Loss 0.0001 (0.0034)	
training:	Epoch: [200][53/233]	Loss 0.0012 (0.0033)	
training:	Epoch: [200][54/233]	Loss 0.0022 (0.0033)	
training:	Epoch: [200][55/233]	Loss 0.0001 (0.0032)	
training:	Epoch: [200][56/233]	Loss 0.0002 (0.0032)	
training:	Epoch: [200][57/233]	Loss 0.0001 (0.0031)	
training:	Epoch: [200][58/233]	Loss 0.0467 (0.0039)	
training:	Epoch: [200][59/233]	Loss 0.0001 (0.0038)	
training:	Epoch: [200][60/233]	Loss 0.0006 (0.0038)	
training:	Epoch: [200][61/233]	Loss 0.0001 (0.0037)	
training:	Epoch: [200][62/233]	Loss 0.0003 (0.0037)	
training:	Epoch: [200][63/233]	Loss 0.0046 (0.0037)	
training:	Epoch: [200][64/233]	Loss 0.0119 (0.0038)	
training:	Epoch: [200][65/233]	Loss 0.0002 (0.0037)	
training:	Epoch: [200][66/233]	Loss 0.0001 (0.0037)	
training:	Epoch: [200][67/233]	Loss 0.0001 (0.0036)	
training:	Epoch: [200][68/233]	Loss 0.0230 (0.0039)	
training:	Epoch: [200][69/233]	Loss 0.0030 (0.0039)	
training:	Epoch: [200][70/233]	Loss 0.0003 (0.0039)	
training:	Epoch: [200][71/233]	Loss 0.0001 (0.0038)	
training:	Epoch: [200][72/233]	Loss 0.0001 (0.0037)	
training:	Epoch: [200][73/233]	Loss 0.0002 (0.0037)	
training:	Epoch: [200][74/233]	Loss 0.0008 (0.0037)	
training:	Epoch: [200][75/233]	Loss 0.0002 (0.0036)	
training:	Epoch: [200][76/233]	Loss 0.0002 (0.0036)	
training:	Epoch: [200][77/233]	Loss 0.0002 (0.0035)	
training:	Epoch: [200][78/233]	Loss 0.0001 (0.0035)	
training:	Epoch: [200][79/233]	Loss 0.0002 (0.0034)	
training:	Epoch: [200][80/233]	Loss 0.0001 (0.0034)	
training:	Epoch: [200][81/233]	Loss 0.0001 (0.0034)	
training:	Epoch: [200][82/233]	Loss 0.0003 (0.0033)	
training:	Epoch: [200][83/233]	Loss 0.0160 (0.0035)	
training:	Epoch: [200][84/233]	Loss 0.0002 (0.0034)	
training:	Epoch: [200][85/233]	Loss 0.0002 (0.0034)	
training:	Epoch: [200][86/233]	Loss 0.0002 (0.0034)	
training:	Epoch: [200][87/233]	Loss 0.0002 (0.0033)	
training:	Epoch: [200][88/233]	Loss 0.0001 (0.0033)	
training:	Epoch: [200][89/233]	Loss 0.0012 (0.0033)	
training:	Epoch: [200][90/233]	Loss 0.0001 (0.0032)	
training:	Epoch: [200][91/233]	Loss 0.0002 (0.0032)	
training:	Epoch: [200][92/233]	Loss 0.0001 (0.0032)	
training:	Epoch: [200][93/233]	Loss 0.0002 (0.0031)	
training:	Epoch: [200][94/233]	Loss 0.0011 (0.0031)	
training:	Epoch: [200][95/233]	Loss 0.0001 (0.0031)	
training:	Epoch: [200][96/233]	Loss 0.0002 (0.0030)	
training:	Epoch: [200][97/233]	Loss 0.0001 (0.0030)	
training:	Epoch: [200][98/233]	Loss 0.0001 (0.0030)	
training:	Epoch: [200][99/233]	Loss 0.0001 (0.0030)	
training:	Epoch: [200][100/233]	Loss 0.0002 (0.0029)	
training:	Epoch: [200][101/233]	Loss 0.0002 (0.0029)	
training:	Epoch: [200][102/233]	Loss 0.0001 (0.0029)	
training:	Epoch: [200][103/233]	Loss 0.0002 (0.0028)	
training:	Epoch: [200][104/233]	Loss 0.0009 (0.0028)	
training:	Epoch: [200][105/233]	Loss 0.0002 (0.0028)	
training:	Epoch: [200][106/233]	Loss 0.0007 (0.0028)	
training:	Epoch: [200][107/233]	Loss 0.0001 (0.0028)	
training:	Epoch: [200][108/233]	Loss 0.0001 (0.0027)	
training:	Epoch: [200][109/233]	Loss 0.0001 (0.0027)	
training:	Epoch: [200][110/233]	Loss 0.0068 (0.0027)	
training:	Epoch: [200][111/233]	Loss 0.0001 (0.0027)	
training:	Epoch: [200][112/233]	Loss 0.0798 (0.0034)	
training:	Epoch: [200][113/233]	Loss 0.0002 (0.0034)	
training:	Epoch: [200][114/233]	Loss 0.0003 (0.0034)	
training:	Epoch: [200][115/233]	Loss 0.0001 (0.0033)	
training:	Epoch: [200][116/233]	Loss 0.0002 (0.0033)	
training:	Epoch: [200][117/233]	Loss 0.0001 (0.0033)	
training:	Epoch: [200][118/233]	Loss 0.0007 (0.0032)	
training:	Epoch: [200][119/233]	Loss 0.0001 (0.0032)	
training:	Epoch: [200][120/233]	Loss 0.0002 (0.0032)	
training:	Epoch: [200][121/233]	Loss 0.0002 (0.0032)	
training:	Epoch: [200][122/233]	Loss 0.0001 (0.0031)	
training:	Epoch: [200][123/233]	Loss 0.0004 (0.0031)	
training:	Epoch: [200][124/233]	Loss 0.0001 (0.0031)	
training:	Epoch: [200][125/233]	Loss 0.0008 (0.0031)	
training:	Epoch: [200][126/233]	Loss 0.0001 (0.0031)	
training:	Epoch: [200][127/233]	Loss 0.0004 (0.0030)	
training:	Epoch: [200][128/233]	Loss 0.1095 (0.0039)	
training:	Epoch: [200][129/233]	Loss 0.0001 (0.0038)	
training:	Epoch: [200][130/233]	Loss 0.0003 (0.0038)	
training:	Epoch: [200][131/233]	Loss 0.0001 (0.0038)	
training:	Epoch: [200][132/233]	Loss 0.0015 (0.0038)	
training:	Epoch: [200][133/233]	Loss 0.0002 (0.0037)	
training:	Epoch: [200][134/233]	Loss 0.0001 (0.0037)	
training:	Epoch: [200][135/233]	Loss 0.0001 (0.0037)	
training:	Epoch: [200][136/233]	Loss 0.0001 (0.0037)	
training:	Epoch: [200][137/233]	Loss 0.0001 (0.0036)	
training:	Epoch: [200][138/233]	Loss 0.0010 (0.0036)	
training:	Epoch: [200][139/233]	Loss 0.0001 (0.0036)	
training:	Epoch: [200][140/233]	Loss 0.0001 (0.0036)	
training:	Epoch: [200][141/233]	Loss 0.0059 (0.0036)	
training:	Epoch: [200][142/233]	Loss 0.0001 (0.0036)	
training:	Epoch: [200][143/233]	Loss 0.0001 (0.0035)	
training:	Epoch: [200][144/233]	Loss 0.0002 (0.0035)	
training:	Epoch: [200][145/233]	Loss 0.0001 (0.0035)	
training:	Epoch: [200][146/233]	Loss 0.0007 (0.0035)	
training:	Epoch: [200][147/233]	Loss 0.0001 (0.0034)	
training:	Epoch: [200][148/233]	Loss 0.0001 (0.0034)	
training:	Epoch: [200][149/233]	Loss 0.0005 (0.0034)	
training:	Epoch: [200][150/233]	Loss 0.0001 (0.0034)	
training:	Epoch: [200][151/233]	Loss 0.0011 (0.0034)	
training:	Epoch: [200][152/233]	Loss 0.0002 (0.0033)	
training:	Epoch: [200][153/233]	Loss 0.0001 (0.0033)	
training:	Epoch: [200][154/233]	Loss 0.0006 (0.0033)	
training:	Epoch: [200][155/233]	Loss 0.0001 (0.0033)	
training:	Epoch: [200][156/233]	Loss 0.0001 (0.0033)	
training:	Epoch: [200][157/233]	Loss 0.0002 (0.0032)	
training:	Epoch: [200][158/233]	Loss 0.0001 (0.0032)	
training:	Epoch: [200][159/233]	Loss 0.0003 (0.0032)	
training:	Epoch: [200][160/233]	Loss 0.0001 (0.0032)	
training:	Epoch: [200][161/233]	Loss 0.0001 (0.0032)	
training:	Epoch: [200][162/233]	Loss 0.0003 (0.0032)	
training:	Epoch: [200][163/233]	Loss 0.0002 (0.0031)	
training:	Epoch: [200][164/233]	Loss 0.0001 (0.0031)	
training:	Epoch: [200][165/233]	Loss 0.0001 (0.0031)	
training:	Epoch: [200][166/233]	Loss 0.0002 (0.0031)	
training:	Epoch: [200][167/233]	Loss 0.0001 (0.0031)	
training:	Epoch: [200][168/233]	Loss 0.0005 (0.0030)	
training:	Epoch: [200][169/233]	Loss 0.0002 (0.0030)	
training:	Epoch: [200][170/233]	Loss 0.0075 (0.0031)	
training:	Epoch: [200][171/233]	Loss 0.0003 (0.0030)	
training:	Epoch: [200][172/233]	Loss 0.0001 (0.0030)	
training:	Epoch: [200][173/233]	Loss 0.0001 (0.0030)	
training:	Epoch: [200][174/233]	Loss 0.0002 (0.0030)	
training:	Epoch: [200][175/233]	Loss 0.0001 (0.0030)	
training:	Epoch: [200][176/233]	Loss 0.0001 (0.0030)	
training:	Epoch: [200][177/233]	Loss 0.0001 (0.0029)	
training:	Epoch: [200][178/233]	Loss 0.0014 (0.0029)	
training:	Epoch: [200][179/233]	Loss 0.0004 (0.0029)	
training:	Epoch: [200][180/233]	Loss 0.0002 (0.0029)	
training:	Epoch: [200][181/233]	Loss 0.0001 (0.0029)	
training:	Epoch: [200][182/233]	Loss 0.0001 (0.0029)	
training:	Epoch: [200][183/233]	Loss 0.0001 (0.0029)	
training:	Epoch: [200][184/233]	Loss 0.0002 (0.0028)	
training:	Epoch: [200][185/233]	Loss 0.0002 (0.0028)	
training:	Epoch: [200][186/233]	Loss 0.0001 (0.0028)	
training:	Epoch: [200][187/233]	Loss 0.0021 (0.0028)	
training:	Epoch: [200][188/233]	Loss 0.0001 (0.0028)	
training:	Epoch: [200][189/233]	Loss 0.0002 (0.0028)	
training:	Epoch: [200][190/233]	Loss 0.0002 (0.0028)	
training:	Epoch: [200][191/233]	Loss 0.0002 (0.0028)	
training:	Epoch: [200][192/233]	Loss 0.0001 (0.0027)	
training:	Epoch: [200][193/233]	Loss 0.0001 (0.0027)	
training:	Epoch: [200][194/233]	Loss 0.0001 (0.0027)	
training:	Epoch: [200][195/233]	Loss 0.0004 (0.0027)	
training:	Epoch: [200][196/233]	Loss 0.0002 (0.0027)	
training:	Epoch: [200][197/233]	Loss 0.0001 (0.0027)	
training:	Epoch: [200][198/233]	Loss 0.0001 (0.0027)	
training:	Epoch: [200][199/233]	Loss 0.0005 (0.0027)	
training:	Epoch: [200][200/233]	Loss 0.0004 (0.0026)	
training:	Epoch: [200][201/233]	Loss 0.0001 (0.0026)	
training:	Epoch: [200][202/233]	Loss 0.0002 (0.0026)	
training:	Epoch: [200][203/233]	Loss 0.0002 (0.0026)	
training:	Epoch: [200][204/233]	Loss 0.0006 (0.0026)	
training:	Epoch: [200][205/233]	Loss 0.0001 (0.0026)	
training:	Epoch: [200][206/233]	Loss 0.0002 (0.0026)	
training:	Epoch: [200][207/233]	Loss 0.0001 (0.0026)	
training:	Epoch: [200][208/233]	Loss 0.0001 (0.0026)	
training:	Epoch: [200][209/233]	Loss 0.0002 (0.0025)	
training:	Epoch: [200][210/233]	Loss 0.0001 (0.0025)	
training:	Epoch: [200][211/233]	Loss 0.0001 (0.0025)	
training:	Epoch: [200][212/233]	Loss 0.0007 (0.0025)	
training:	Epoch: [200][213/233]	Loss 0.0001 (0.0025)	
training:	Epoch: [200][214/233]	Loss 0.0003 (0.0025)	
training:	Epoch: [200][215/233]	Loss 0.0002 (0.0025)	
training:	Epoch: [200][216/233]	Loss 0.0001 (0.0025)	
training:	Epoch: [200][217/233]	Loss 0.0357 (0.0026)	
training:	Epoch: [200][218/233]	Loss 0.0002 (0.0026)	
training:	Epoch: [200][219/233]	Loss 0.0250 (0.0027)	
training:	Epoch: [200][220/233]	Loss 0.0002 (0.0027)	
training:	Epoch: [200][221/233]	Loss 0.0002 (0.0027)	
training:	Epoch: [200][222/233]	Loss 0.0008 (0.0027)	
training:	Epoch: [200][223/233]	Loss 0.0001 (0.0027)	
training:	Epoch: [200][224/233]	Loss 0.0001 (0.0027)	
training:	Epoch: [200][225/233]	Loss 0.0001 (0.0026)	
training:	Epoch: [200][226/233]	Loss 0.0001 (0.0026)	
training:	Epoch: [200][227/233]	Loss 0.0001 (0.0026)	
training:	Epoch: [200][228/233]	Loss 0.0001 (0.0026)	
training:	Epoch: [200][229/233]	Loss 0.0001 (0.0026)	
training:	Epoch: [200][230/233]	Loss 0.0001 (0.0026)	
training:	Epoch: [200][231/233]	Loss 0.0001 (0.0026)	
training:	Epoch: [200][232/233]	Loss 0.0001 (0.0026)	
training:	Epoch: [200][233/233]	Loss 0.0002 (0.0026)	
Training:	 Loss: 0.0026

Training:	 ACC: 1.0000 1.0000 1.0000 1.0000
Validation:	 ACC: 0.7990 0.7999 0.8172 0.7809
Validation:	 Best_BACC: 0.8162 0.8154 0.7998 0.8326
Validation:	 Loss: 1.4279
Training complete in 37m 52s
