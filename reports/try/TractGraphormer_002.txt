Namespace(inputDirectory='data', outputDirectory='try', DisFile='data/dis_sort_roi2dis2000.npy', task='sex', type='binary', feature=['Nos', 'FA1'], CUDA_id='0', data_id='0', dataset='Classification', norm=True, channels=1, epochs=500, tensorboard=True, net_architecture='TractGraphormer', batch_size=32, rate=1e-06, weight=0.0, sched_step=300, sched_gamma=0.1, printing_frequency=1, seed=1, alpha=0, remix_kappa=0, remix_tau=0, loss='CE', sigma=0, k=20)
Training the 'TractGraphormer' architecture

The following parameters are used:
Batch size:	32
Number of workers:	0
Learning rate:	1e-06
Weight decay:	0.0
Scheduler steps:	300
Scheduler gamma:	0.1
Number of epochs of training:	500
Number of input channels:	1

Data preparation
Reading data from:	./data
Training set size:	6538
Validation set size:	1869

Performing calculations on:	cuda:0

Pretraining:	Epoch 1/500
----------
training:	Epoch: [1][1/204]	Loss 0.6857 (0.6857)	
training:	Epoch: [1][2/204]	Loss 0.7173 (0.7015)	
training:	Epoch: [1][3/204]	Loss 0.7071 (0.7034)	
training:	Epoch: [1][4/204]	Loss 0.6971 (0.7018)	
training:	Epoch: [1][5/204]	Loss 0.7318 (0.7078)	
training:	Epoch: [1][6/204]	Loss 0.7211 (0.7100)	
training:	Epoch: [1][7/204]	Loss 0.7213 (0.7116)	
training:	Epoch: [1][8/204]	Loss 0.7043 (0.7107)	
training:	Epoch: [1][9/204]	Loss 0.6972 (0.7092)	
training:	Epoch: [1][10/204]	Loss 0.6996 (0.7082)	
training:	Epoch: [1][11/204]	Loss 0.6871 (0.7063)	
training:	Epoch: [1][12/204]	Loss 0.7375 (0.7089)	
training:	Epoch: [1][13/204]	Loss 0.7023 (0.7084)	
training:	Epoch: [1][14/204]	Loss 0.7137 (0.7088)	
training:	Epoch: [1][15/204]	Loss 0.7208 (0.7096)	
training:	Epoch: [1][16/204]	Loss 0.6657 (0.7068)	
training:	Epoch: [1][17/204]	Loss 0.7106 (0.7071)	
training:	Epoch: [1][18/204]	Loss 0.7190 (0.7077)	
training:	Epoch: [1][19/204]	Loss 0.7306 (0.7089)	
training:	Epoch: [1][20/204]	Loss 0.7288 (0.7099)	
training:	Epoch: [1][21/204]	Loss 0.7137 (0.7101)	
training:	Epoch: [1][22/204]	Loss 0.6962 (0.7095)	
training:	Epoch: [1][23/204]	Loss 0.6872 (0.7085)	
training:	Epoch: [1][24/204]	Loss 0.6752 (0.7071)	
training:	Epoch: [1][25/204]	Loss 0.7209 (0.7077)	
training:	Epoch: [1][26/204]	Loss 0.7034 (0.7075)	
training:	Epoch: [1][27/204]	Loss 0.6971 (0.7071)	
training:	Epoch: [1][28/204]	Loss 0.6921 (0.7066)	
training:	Epoch: [1][29/204]	Loss 0.6964 (0.7062)	
training:	Epoch: [1][30/204]	Loss 0.6850 (0.7055)	
training:	Epoch: [1][31/204]	Loss 0.6842 (0.7048)	
training:	Epoch: [1][32/204]	Loss 0.6882 (0.7043)	
training:	Epoch: [1][33/204]	Loss 0.6774 (0.7035)	
training:	Epoch: [1][34/204]	Loss 0.7023 (0.7035)	
training:	Epoch: [1][35/204]	Loss 0.6822 (0.7029)	
training:	Epoch: [1][36/204]	Loss 0.7146 (0.7032)	
training:	Epoch: [1][37/204]	Loss 0.7149 (0.7035)	
training:	Epoch: [1][38/204]	Loss 0.7125 (0.7037)	
training:	Epoch: [1][39/204]	Loss 0.7181 (0.7041)	
training:	Epoch: [1][40/204]	Loss 0.6750 (0.7034)	
training:	Epoch: [1][41/204]	Loss 0.6678 (0.7025)	
training:	Epoch: [1][42/204]	Loss 0.7049 (0.7026)	
training:	Epoch: [1][43/204]	Loss 0.6713 (0.7018)	
training:	Epoch: [1][44/204]	Loss 0.7141 (0.7021)	
training:	Epoch: [1][45/204]	Loss 0.7259 (0.7026)	
training:	Epoch: [1][46/204]	Loss 0.7255 (0.7031)	
training:	Epoch: [1][47/204]	Loss 0.6874 (0.7028)	
training:	Epoch: [1][48/204]	Loss 0.6862 (0.7025)	
training:	Epoch: [1][49/204]	Loss 0.7044 (0.7025)	
training:	Epoch: [1][50/204]	Loss 0.6686 (0.7018)	
training:	Epoch: [1][51/204]	Loss 0.6932 (0.7017)	
training:	Epoch: [1][52/204]	Loss 0.7194 (0.7020)	
training:	Epoch: [1][53/204]	Loss 0.7178 (0.7023)	
training:	Epoch: [1][54/204]	Loss 0.7231 (0.7027)	
training:	Epoch: [1][55/204]	Loss 0.7178 (0.7030)	
training:	Epoch: [1][56/204]	Loss 0.7030 (0.7030)	
training:	Epoch: [1][57/204]	Loss 0.6976 (0.7029)	
training:	Epoch: [1][58/204]	Loss 0.6996 (0.7028)	
training:	Epoch: [1][59/204]	Loss 0.6846 (0.7025)	
training:	Epoch: [1][60/204]	Loss 0.7134 (0.7027)	
training:	Epoch: [1][61/204]	Loss 0.6820 (0.7023)	
training:	Epoch: [1][62/204]	Loss 0.7236 (0.7027)	
training:	Epoch: [1][63/204]	Loss 0.7080 (0.7028)	
training:	Epoch: [1][64/204]	Loss 0.7149 (0.7030)	
training:	Epoch: [1][65/204]	Loss 0.6760 (0.7025)	
training:	Epoch: [1][66/204]	Loss 0.7129 (0.7027)	
training:	Epoch: [1][67/204]	Loss 0.7071 (0.7028)	
training:	Epoch: [1][68/204]	Loss 0.6964 (0.7027)	
training:	Epoch: [1][69/204]	Loss 0.7043 (0.7027)	
training:	Epoch: [1][70/204]	Loss 0.7064 (0.7027)	
training:	Epoch: [1][71/204]	Loss 0.6937 (0.7026)	
training:	Epoch: [1][72/204]	Loss 0.6951 (0.7025)	
training:	Epoch: [1][73/204]	Loss 0.7101 (0.7026)	
training:	Epoch: [1][74/204]	Loss 0.6990 (0.7026)	
training:	Epoch: [1][75/204]	Loss 0.7204 (0.7028)	
training:	Epoch: [1][76/204]	Loss 0.6790 (0.7025)	
training:	Epoch: [1][77/204]	Loss 0.7336 (0.7029)	
training:	Epoch: [1][78/204]	Loss 0.6982 (0.7028)	
training:	Epoch: [1][79/204]	Loss 0.7249 (0.7031)	
training:	Epoch: [1][80/204]	Loss 0.6941 (0.7030)	
training:	Epoch: [1][81/204]	Loss 0.6971 (0.7029)	
training:	Epoch: [1][82/204]	Loss 0.6919 (0.7028)	
training:	Epoch: [1][83/204]	Loss 0.6883 (0.7026)	
training:	Epoch: [1][84/204]	Loss 0.7155 (0.7028)	
training:	Epoch: [1][85/204]	Loss 0.7205 (0.7030)	
training:	Epoch: [1][86/204]	Loss 0.7078 (0.7030)	
training:	Epoch: [1][87/204]	Loss 0.6823 (0.7028)	
training:	Epoch: [1][88/204]	Loss 0.6860 (0.7026)	
training:	Epoch: [1][89/204]	Loss 0.6945 (0.7025)	
training:	Epoch: [1][90/204]	Loss 0.7017 (0.7025)	
training:	Epoch: [1][91/204]	Loss 0.6943 (0.7024)	
training:	Epoch: [1][92/204]	Loss 0.7168 (0.7026)	
training:	Epoch: [1][93/204]	Loss 0.6937 (0.7025)	
training:	Epoch: [1][94/204]	Loss 0.7132 (0.7026)	
training:	Epoch: [1][95/204]	Loss 0.7225 (0.7028)	
training:	Epoch: [1][96/204]	Loss 0.7223 (0.7030)	
training:	Epoch: [1][97/204]	Loss 0.7030 (0.7030)	
training:	Epoch: [1][98/204]	Loss 0.7136 (0.7031)	
training:	Epoch: [1][99/204]	Loss 0.6733 (0.7028)	
training:	Epoch: [1][100/204]	Loss 0.7332 (0.7031)	
training:	Epoch: [1][101/204]	Loss 0.6853 (0.7029)	
training:	Epoch: [1][102/204]	Loss 0.6797 (0.7027)	
training:	Epoch: [1][103/204]	Loss 0.6876 (0.7026)	
training:	Epoch: [1][104/204]	Loss 0.6974 (0.7025)	
training:	Epoch: [1][105/204]	Loss 0.6686 (0.7022)	
training:	Epoch: [1][106/204]	Loss 0.7184 (0.7023)	
training:	Epoch: [1][107/204]	Loss 0.7044 (0.7024)	
training:	Epoch: [1][108/204]	Loss 0.7070 (0.7024)	
training:	Epoch: [1][109/204]	Loss 0.6597 (0.7020)	
training:	Epoch: [1][110/204]	Loss 0.7023 (0.7020)	
training:	Epoch: [1][111/204]	Loss 0.6969 (0.7020)	
training:	Epoch: [1][112/204]	Loss 0.7227 (0.7022)	
training:	Epoch: [1][113/204]	Loss 0.6894 (0.7020)	
training:	Epoch: [1][114/204]	Loss 0.6940 (0.7020)	
training:	Epoch: [1][115/204]	Loss 0.7131 (0.7021)	
training:	Epoch: [1][116/204]	Loss 0.6952 (0.7020)	
training:	Epoch: [1][117/204]	Loss 0.7308 (0.7023)	
training:	Epoch: [1][118/204]	Loss 0.7178 (0.7024)	
training:	Epoch: [1][119/204]	Loss 0.7220 (0.7026)	
training:	Epoch: [1][120/204]	Loss 0.6952 (0.7025)	
training:	Epoch: [1][121/204]	Loss 0.7215 (0.7027)	
training:	Epoch: [1][122/204]	Loss 0.7107 (0.7027)	
training:	Epoch: [1][123/204]	Loss 0.6809 (0.7025)	
training:	Epoch: [1][124/204]	Loss 0.6864 (0.7024)	
training:	Epoch: [1][125/204]	Loss 0.6955 (0.7024)	
training:	Epoch: [1][126/204]	Loss 0.6948 (0.7023)	
training:	Epoch: [1][127/204]	Loss 0.6637 (0.7020)	
training:	Epoch: [1][128/204]	Loss 0.6657 (0.7017)	
training:	Epoch: [1][129/204]	Loss 0.6882 (0.7016)	
training:	Epoch: [1][130/204]	Loss 0.6858 (0.7015)	
training:	Epoch: [1][131/204]	Loss 0.7110 (0.7016)	
training:	Epoch: [1][132/204]	Loss 0.7002 (0.7015)	
training:	Epoch: [1][133/204]	Loss 0.6718 (0.7013)	
training:	Epoch: [1][134/204]	Loss 0.7152 (0.7014)	
training:	Epoch: [1][135/204]	Loss 0.7189 (0.7016)	
training:	Epoch: [1][136/204]	Loss 0.7119 (0.7016)	
training:	Epoch: [1][137/204]	Loss 0.7058 (0.7017)	
training:	Epoch: [1][138/204]	Loss 0.6908 (0.7016)	
training:	Epoch: [1][139/204]	Loss 0.7181 (0.7017)	
training:	Epoch: [1][140/204]	Loss 0.6761 (0.7015)	
training:	Epoch: [1][141/204]	Loss 0.6810 (0.7014)	
training:	Epoch: [1][142/204]	Loss 0.6903 (0.7013)	
training:	Epoch: [1][143/204]	Loss 0.7048 (0.7013)	
training:	Epoch: [1][144/204]	Loss 0.7083 (0.7014)	
training:	Epoch: [1][145/204]	Loss 0.6963 (0.7013)	
training:	Epoch: [1][146/204]	Loss 0.6998 (0.7013)	
training:	Epoch: [1][147/204]	Loss 0.7166 (0.7014)	
training:	Epoch: [1][148/204]	Loss 0.7233 (0.7016)	
training:	Epoch: [1][149/204]	Loss 0.6870 (0.7015)	
training:	Epoch: [1][150/204]	Loss 0.6919 (0.7014)	
training:	Epoch: [1][151/204]	Loss 0.6793 (0.7013)	
training:	Epoch: [1][152/204]	Loss 0.6967 (0.7012)	
training:	Epoch: [1][153/204]	Loss 0.6695 (0.7010)	
training:	Epoch: [1][154/204]	Loss 0.6994 (0.7010)	
training:	Epoch: [1][155/204]	Loss 0.6770 (0.7009)	
training:	Epoch: [1][156/204]	Loss 0.7000 (0.7009)	
training:	Epoch: [1][157/204]	Loss 0.6996 (0.7008)	
training:	Epoch: [1][158/204]	Loss 0.7124 (0.7009)	
training:	Epoch: [1][159/204]	Loss 0.7035 (0.7009)	
training:	Epoch: [1][160/204]	Loss 0.7146 (0.7010)	
training:	Epoch: [1][161/204]	Loss 0.6933 (0.7010)	
training:	Epoch: [1][162/204]	Loss 0.6892 (0.7009)	
training:	Epoch: [1][163/204]	Loss 0.7272 (0.7011)	
training:	Epoch: [1][164/204]	Loss 0.6721 (0.7009)	
training:	Epoch: [1][165/204]	Loss 0.6770 (0.7007)	
training:	Epoch: [1][166/204]	Loss 0.6803 (0.7006)	
training:	Epoch: [1][167/204]	Loss 0.6829 (0.7005)	
training:	Epoch: [1][168/204]	Loss 0.6922 (0.7005)	
training:	Epoch: [1][169/204]	Loss 0.6992 (0.7005)	
training:	Epoch: [1][170/204]	Loss 0.6974 (0.7004)	
training:	Epoch: [1][171/204]	Loss 0.7048 (0.7005)	
training:	Epoch: [1][172/204]	Loss 0.7174 (0.7006)	
training:	Epoch: [1][173/204]	Loss 0.6656 (0.7004)	
training:	Epoch: [1][174/204]	Loss 0.7018 (0.7004)	
training:	Epoch: [1][175/204]	Loss 0.7107 (0.7004)	
training:	Epoch: [1][176/204]	Loss 0.6935 (0.7004)	
training:	Epoch: [1][177/204]	Loss 0.6895 (0.7003)	
training:	Epoch: [1][178/204]	Loss 0.7063 (0.7004)	
training:	Epoch: [1][179/204]	Loss 0.7028 (0.7004)	
training:	Epoch: [1][180/204]	Loss 0.6883 (0.7003)	
training:	Epoch: [1][181/204]	Loss 0.7042 (0.7003)	
training:	Epoch: [1][182/204]	Loss 0.7229 (0.7005)	
training:	Epoch: [1][183/204]	Loss 0.7052 (0.7005)	
training:	Epoch: [1][184/204]	Loss 0.6884 (0.7004)	
training:	Epoch: [1][185/204]	Loss 0.7008 (0.7004)	
training:	Epoch: [1][186/204]	Loss 0.7015 (0.7004)	
training:	Epoch: [1][187/204]	Loss 0.6899 (0.7004)	
training:	Epoch: [1][188/204]	Loss 0.7001 (0.7004)	
training:	Epoch: [1][189/204]	Loss 0.7102 (0.7004)	
training:	Epoch: [1][190/204]	Loss 0.6844 (0.7003)	
training:	Epoch: [1][191/204]	Loss 0.7167 (0.7004)	
training:	Epoch: [1][192/204]	Loss 0.7056 (0.7004)	
training:	Epoch: [1][193/204]	Loss 0.7084 (0.7005)	
training:	Epoch: [1][194/204]	Loss 0.7255 (0.7006)	
training:	Epoch: [1][195/204]	Loss 0.6953 (0.7006)	
training:	Epoch: [1][196/204]	Loss 0.6942 (0.7006)	
training:	Epoch: [1][197/204]	Loss 0.6918 (0.7005)	
training:	Epoch: [1][198/204]	Loss 0.7020 (0.7005)	
training:	Epoch: [1][199/204]	Loss 0.7016 (0.7005)	
training:	Epoch: [1][200/204]	Loss 0.6857 (0.7004)	
training:	Epoch: [1][201/204]	Loss 0.6803 (0.7003)	
training:	Epoch: [1][202/204]	Loss 0.7270 (0.7005)	
training:	Epoch: [1][203/204]	Loss 0.6851 (0.7004)	
training:	Epoch: [1][204/204]	Loss 0.6975 (0.7004)	
Training:	 Loss: 0.6993

