Namespace(inputDirectory='data', outputDirectory='try', DisFile='data/dis_sort_roi2dis2000.npy', task='sex', type='binary', feature=['all'], CUDA_id='0', data_id='0', dataset='Classification', norm=True, epochs=400, tensorboard=True, net_architecture='TractGraphormer', batch_size=16, rate=0.0001, weight=0.0, sched_step=200, sched_gamma=0.1, printing_frequency=1, seed=0, alpha=0, remix_kappa=0, remix_tau=0, loss='CE', sigma=0, k=22, fl=64, nh=1)
Training the 'TractGraphormer' architecture
Batch size:	16
Number of workers:	0
Learning rate:	0.0001
Weight decay:	0.0
Scheduler steps:	200
Scheduler gamma:	0.1
Number of epochs of training:	400
Number of input channels:	1

Data preparation
Reading data from:	./data
Training set size:	6538
Validation set size:	1869

Performing calculations on:	cuda:0


Total number of parameters:7970834  Trainable:7970834
Pretraining:	Epoch 1/400
----------
training:	Epoch: [1][1/408]	Loss 0.6868 (0.6868)	
training:	Epoch: [1][2/408]	Loss 0.7908 (0.7388)	
training:	Epoch: [1][3/408]	Loss 0.6719 (0.7165)	
training:	Epoch: [1][4/408]	Loss 0.6873 (0.7092)	
training:	Epoch: [1][5/408]	Loss 0.7173 (0.7108)	
training:	Epoch: [1][6/408]	Loss 0.7093 (0.7106)	
training:	Epoch: [1][7/408]	Loss 0.7154 (0.7113)	
training:	Epoch: [1][8/408]	Loss 0.6938 (0.7091)	
training:	Epoch: [1][9/408]	Loss 0.7108 (0.7093)	
training:	Epoch: [1][10/408]	Loss 0.7021 (0.7086)	
training:	Epoch: [1][11/408]	Loss 0.6862 (0.7065)	
training:	Epoch: [1][12/408]	Loss 0.6436 (0.7013)	
training:	Epoch: [1][13/408]	Loss 0.6879 (0.7003)	
training:	Epoch: [1][14/408]	Loss 0.6373 (0.6958)	
training:	Epoch: [1][15/408]	Loss 0.6976 (0.6959)	
training:	Epoch: [1][16/408]	Loss 0.6730 (0.6945)	
training:	Epoch: [1][17/408]	Loss 0.6201 (0.6901)	
training:	Epoch: [1][18/408]	Loss 0.7142 (0.6914)	
training:	Epoch: [1][19/408]	Loss 0.7311 (0.6935)	
training:	Epoch: [1][20/408]	Loss 0.7394 (0.6958)	
training:	Epoch: [1][21/408]	Loss 0.6400 (0.6931)	
training:	Epoch: [1][22/408]	Loss 0.7041 (0.6936)	
training:	Epoch: [1][23/408]	Loss 0.6673 (0.6925)	
training:	Epoch: [1][24/408]	Loss 0.7061 (0.6931)	
training:	Epoch: [1][25/408]	Loss 0.6450 (0.6911)	
training:	Epoch: [1][26/408]	Loss 0.6673 (0.6902)	
training:	Epoch: [1][27/408]	Loss 0.6425 (0.6885)	
training:	Epoch: [1][28/408]	Loss 0.6550 (0.6873)	
training:	Epoch: [1][29/408]	Loss 0.5846 (0.6837)	
training:	Epoch: [1][30/408]	Loss 0.6810 (0.6836)	
training:	Epoch: [1][31/408]	Loss 0.7559 (0.6860)	
training:	Epoch: [1][32/408]	Loss 0.6718 (0.6855)	
training:	Epoch: [1][33/408]	Loss 0.6323 (0.6839)	
training:	Epoch: [1][34/408]	Loss 0.6322 (0.6824)	
training:	Epoch: [1][35/408]	Loss 0.5827 (0.6795)	
training:	Epoch: [1][36/408]	Loss 0.6937 (0.6799)	
training:	Epoch: [1][37/408]	Loss 0.6408 (0.6789)	
training:	Epoch: [1][38/408]	Loss 0.6009 (0.6768)	
training:	Epoch: [1][39/408]	Loss 0.6576 (0.6763)	
training:	Epoch: [1][40/408]	Loss 0.5745 (0.6738)	
training:	Epoch: [1][41/408]	Loss 0.5381 (0.6705)	
training:	Epoch: [1][42/408]	Loss 0.5660 (0.6680)	
training:	Epoch: [1][43/408]	Loss 0.7641 (0.6702)	
training:	Epoch: [1][44/408]	Loss 0.7837 (0.6728)	
training:	Epoch: [1][45/408]	Loss 0.6978 (0.6734)	
training:	Epoch: [1][46/408]	Loss 0.6106 (0.6720)	
training:	Epoch: [1][47/408]	Loss 0.5131 (0.6686)	
training:	Epoch: [1][48/408]	Loss 0.6615 (0.6685)	
training:	Epoch: [1][49/408]	Loss 0.6323 (0.6677)	
training:	Epoch: [1][50/408]	Loss 0.5386 (0.6651)	
training:	Epoch: [1][51/408]	Loss 0.7882 (0.6676)	
training:	Epoch: [1][52/408]	Loss 0.6264 (0.6668)	
training:	Epoch: [1][53/408]	Loss 0.5146 (0.6639)	
training:	Epoch: [1][54/408]	Loss 0.6609 (0.6638)	
training:	Epoch: [1][55/408]	Loss 0.6611 (0.6638)	
training:	Epoch: [1][56/408]	Loss 0.6502 (0.6635)	
training:	Epoch: [1][57/408]	Loss 0.6513 (0.6633)	
training:	Epoch: [1][58/408]	Loss 0.5363 (0.6611)	
training:	Epoch: [1][59/408]	Loss 0.8129 (0.6637)	
training:	Epoch: [1][60/408]	Loss 0.6783 (0.6640)	
training:	Epoch: [1][61/408]	Loss 0.5439 (0.6620)	
training:	Epoch: [1][62/408]	Loss 0.5987 (0.6610)	
training:	Epoch: [1][63/408]	Loss 0.6443 (0.6607)	
training:	Epoch: [1][64/408]	Loss 0.6356 (0.6603)	
training:	Epoch: [1][65/408]	Loss 0.6744 (0.6605)	
training:	Epoch: [1][66/408]	Loss 0.5247 (0.6585)	
training:	Epoch: [1][67/408]	Loss 0.7652 (0.6601)	
training:	Epoch: [1][68/408]	Loss 0.5864 (0.6590)	
training:	Epoch: [1][69/408]	Loss 0.6334 (0.6586)	
training:	Epoch: [1][70/408]	Loss 0.6097 (0.6579)	
training:	Epoch: [1][71/408]	Loss 0.5682 (0.6566)	
training:	Epoch: [1][72/408]	Loss 0.5727 (0.6555)	
training:	Epoch: [1][73/408]	Loss 0.6125 (0.6549)	
training:	Epoch: [1][74/408]	Loss 0.7163 (0.6557)	
training:	Epoch: [1][75/408]	Loss 0.6802 (0.6561)	
training:	Epoch: [1][76/408]	Loss 0.6374 (0.6558)	
training:	Epoch: [1][77/408]	Loss 0.6634 (0.6559)	
training:	Epoch: [1][78/408]	Loss 0.6723 (0.6561)	
training:	Epoch: [1][79/408]	Loss 0.6334 (0.6558)	
training:	Epoch: [1][80/408]	Loss 0.7710 (0.6573)	
training:	Epoch: [1][81/408]	Loss 0.5561 (0.6560)	
training:	Epoch: [1][82/408]	Loss 0.6716 (0.6562)	
training:	Epoch: [1][83/408]	Loss 0.6356 (0.6560)	
training:	Epoch: [1][84/408]	Loss 0.7153 (0.6567)	
training:	Epoch: [1][85/408]	Loss 0.6352 (0.6564)	
training:	Epoch: [1][86/408]	Loss 0.7238 (0.6572)	
training:	Epoch: [1][87/408]	Loss 0.6241 (0.6568)	
training:	Epoch: [1][88/408]	Loss 0.5373 (0.6555)	
training:	Epoch: [1][89/408]	Loss 0.6964 (0.6559)	
training:	Epoch: [1][90/408]	Loss 0.6015 (0.6553)	
training:	Epoch: [1][91/408]	Loss 0.6878 (0.6557)	
training:	Epoch: [1][92/408]	Loss 0.7144 (0.6563)	
training:	Epoch: [1][93/408]	Loss 0.5911 (0.6556)	
training:	Epoch: [1][94/408]	Loss 0.5848 (0.6549)	
training:	Epoch: [1][95/408]	Loss 0.7458 (0.6558)	
training:	Epoch: [1][96/408]	Loss 0.5536 (0.6547)	
training:	Epoch: [1][97/408]	Loss 0.4898 (0.6530)	
training:	Epoch: [1][98/408]	Loss 0.6866 (0.6534)	
training:	Epoch: [1][99/408]	Loss 0.6260 (0.6531)	
training:	Epoch: [1][100/408]	Loss 0.7371 (0.6540)	
training:	Epoch: [1][101/408]	Loss 0.8220 (0.6556)	
training:	Epoch: [1][102/408]	Loss 0.7246 (0.6563)	
training:	Epoch: [1][103/408]	Loss 0.8215 (0.6579)	
training:	Epoch: [1][104/408]	Loss 0.7869 (0.6591)	
training:	Epoch: [1][105/408]	Loss 0.4833 (0.6575)	
training:	Epoch: [1][106/408]	Loss 0.5635 (0.6566)	
training:	Epoch: [1][107/408]	Loss 0.7010 (0.6570)	
training:	Epoch: [1][108/408]	Loss 0.7215 (0.6576)	
training:	Epoch: [1][109/408]	Loss 0.5886 (0.6570)	
training:	Epoch: [1][110/408]	Loss 0.7640 (0.6579)	
training:	Epoch: [1][111/408]	Loss 0.6917 (0.6582)	
training:	Epoch: [1][112/408]	Loss 0.6090 (0.6578)	
training:	Epoch: [1][113/408]	Loss 0.6179 (0.6574)	
training:	Epoch: [1][114/408]	Loss 0.6130 (0.6570)	
training:	Epoch: [1][115/408]	Loss 0.5794 (0.6564)	
training:	Epoch: [1][116/408]	Loss 0.6333 (0.6562)	
training:	Epoch: [1][117/408]	Loss 0.7312 (0.6568)	
training:	Epoch: [1][118/408]	Loss 0.7071 (0.6572)	
training:	Epoch: [1][119/408]	Loss 0.6144 (0.6569)	
training:	Epoch: [1][120/408]	Loss 0.5719 (0.6562)	
training:	Epoch: [1][121/408]	Loss 0.6512 (0.6561)	
training:	Epoch: [1][122/408]	Loss 0.5839 (0.6555)	
training:	Epoch: [1][123/408]	Loss 0.6321 (0.6553)	
training:	Epoch: [1][124/408]	Loss 0.6151 (0.6550)	
training:	Epoch: [1][125/408]	Loss 0.4991 (0.6538)	
training:	Epoch: [1][126/408]	Loss 0.6132 (0.6535)	
training:	Epoch: [1][127/408]	Loss 0.5903 (0.6530)	
training:	Epoch: [1][128/408]	Loss 0.6123 (0.6526)	
training:	Epoch: [1][129/408]	Loss 0.6828 (0.6529)	
training:	Epoch: [1][130/408]	Loss 0.6524 (0.6529)	
training:	Epoch: [1][131/408]	Loss 0.7565 (0.6537)	
training:	Epoch: [1][132/408]	Loss 0.7074 (0.6541)	
training:	Epoch: [1][133/408]	Loss 0.6739 (0.6542)	
training:	Epoch: [1][134/408]	Loss 0.6443 (0.6541)	
training:	Epoch: [1][135/408]	Loss 0.6793 (0.6543)	
training:	Epoch: [1][136/408]	Loss 0.5578 (0.6536)	
training:	Epoch: [1][137/408]	Loss 0.6258 (0.6534)	
training:	Epoch: [1][138/408]	Loss 0.6494 (0.6534)	
training:	Epoch: [1][139/408]	Loss 0.6399 (0.6533)	
training:	Epoch: [1][140/408]	Loss 0.6842 (0.6535)	
training:	Epoch: [1][141/408]	Loss 0.6669 (0.6536)	
training:	Epoch: [1][142/408]	Loss 0.5119 (0.6526)	
training:	Epoch: [1][143/408]	Loss 0.5443 (0.6519)	
training:	Epoch: [1][144/408]	Loss 0.6211 (0.6516)	
training:	Epoch: [1][145/408]	Loss 0.7103 (0.6520)	
training:	Epoch: [1][146/408]	Loss 0.7896 (0.6530)	
training:	Epoch: [1][147/408]	Loss 0.6281 (0.6528)	
training:	Epoch: [1][148/408]	Loss 0.5985 (0.6524)	
training:	Epoch: [1][149/408]	Loss 0.6986 (0.6528)	
training:	Epoch: [1][150/408]	Loss 0.6279 (0.6526)	
training:	Epoch: [1][151/408]	Loss 0.6808 (0.6528)	
training:	Epoch: [1][152/408]	Loss 0.5761 (0.6523)	
training:	Epoch: [1][153/408]	Loss 0.6476 (0.6522)	
training:	Epoch: [1][154/408]	Loss 0.7431 (0.6528)	
training:	Epoch: [1][155/408]	Loss 0.7083 (0.6532)	
training:	Epoch: [1][156/408]	Loss 0.6950 (0.6535)	
training:	Epoch: [1][157/408]	Loss 0.5400 (0.6527)	
training:	Epoch: [1][158/408]	Loss 0.5807 (0.6523)	
training:	Epoch: [1][159/408]	Loss 0.6273 (0.6521)	
training:	Epoch: [1][160/408]	Loss 0.6959 (0.6524)	
training:	Epoch: [1][161/408]	Loss 0.6902 (0.6526)	
training:	Epoch: [1][162/408]	Loss 0.5956 (0.6523)	
training:	Epoch: [1][163/408]	Loss 0.6763 (0.6524)	
training:	Epoch: [1][164/408]	Loss 0.6853 (0.6526)	
training:	Epoch: [1][165/408]	Loss 0.6769 (0.6528)	
training:	Epoch: [1][166/408]	Loss 0.5051 (0.6519)	
training:	Epoch: [1][167/408]	Loss 0.5148 (0.6511)	
training:	Epoch: [1][168/408]	Loss 0.6752 (0.6512)	
training:	Epoch: [1][169/408]	Loss 0.7363 (0.6517)	
training:	Epoch: [1][170/408]	Loss 0.6047 (0.6514)	
training:	Epoch: [1][171/408]	Loss 0.4756 (0.6504)	
training:	Epoch: [1][172/408]	Loss 0.7426 (0.6509)	
training:	Epoch: [1][173/408]	Loss 0.5374 (0.6503)	
training:	Epoch: [1][174/408]	Loss 0.5438 (0.6497)	
training:	Epoch: [1][175/408]	Loss 0.6157 (0.6495)	
training:	Epoch: [1][176/408]	Loss 0.5774 (0.6491)	
training:	Epoch: [1][177/408]	Loss 0.5146 (0.6483)	
training:	Epoch: [1][178/408]	Loss 0.4839 (0.6474)	
training:	Epoch: [1][179/408]	Loss 0.6795 (0.6476)	
training:	Epoch: [1][180/408]	Loss 0.6015 (0.6473)	
training:	Epoch: [1][181/408]	Loss 0.5681 (0.6469)	
training:	Epoch: [1][182/408]	Loss 0.5081 (0.6461)	
training:	Epoch: [1][183/408]	Loss 0.7089 (0.6465)	
training:	Epoch: [1][184/408]	Loss 0.6599 (0.6465)	
training:	Epoch: [1][185/408]	Loss 0.4880 (0.6457)	
training:	Epoch: [1][186/408]	Loss 0.7299 (0.6461)	
training:	Epoch: [1][187/408]	Loss 0.6074 (0.6459)	
training:	Epoch: [1][188/408]	Loss 0.6032 (0.6457)	
training:	Epoch: [1][189/408]	Loss 0.6666 (0.6458)	
training:	Epoch: [1][190/408]	Loss 0.5411 (0.6452)	
training:	Epoch: [1][191/408]	Loss 0.6514 (0.6453)	
training:	Epoch: [1][192/408]	Loss 0.7723 (0.6459)	
training:	Epoch: [1][193/408]	Loss 0.5101 (0.6452)	
training:	Epoch: [1][194/408]	Loss 0.6907 (0.6455)	
training:	Epoch: [1][195/408]	Loss 0.7311 (0.6459)	
training:	Epoch: [1][196/408]	Loss 0.4712 (0.6450)	
training:	Epoch: [1][197/408]	Loss 0.6508 (0.6450)	
training:	Epoch: [1][198/408]	Loss 0.5209 (0.6444)	
training:	Epoch: [1][199/408]	Loss 0.6867 (0.6446)	
training:	Epoch: [1][200/408]	Loss 0.7672 (0.6452)	
training:	Epoch: [1][201/408]	Loss 0.7490 (0.6458)	
training:	Epoch: [1][202/408]	Loss 0.7096 (0.6461)	
training:	Epoch: [1][203/408]	Loss 0.5795 (0.6458)	
training:	Epoch: [1][204/408]	Loss 0.5516 (0.6453)	
training:	Epoch: [1][205/408]	Loss 0.5070 (0.6446)	
training:	Epoch: [1][206/408]	Loss 0.6881 (0.6448)	
training:	Epoch: [1][207/408]	Loss 0.5950 (0.6446)	
training:	Epoch: [1][208/408]	Loss 0.6054 (0.6444)	
training:	Epoch: [1][209/408]	Loss 0.5962 (0.6442)	
training:	Epoch: [1][210/408]	Loss 0.7514 (0.6447)	
training:	Epoch: [1][211/408]	Loss 0.6138 (0.6445)	
training:	Epoch: [1][212/408]	Loss 0.6259 (0.6444)	
training:	Epoch: [1][213/408]	Loss 0.6892 (0.6447)	
training:	Epoch: [1][214/408]	Loss 0.5532 (0.6442)	
training:	Epoch: [1][215/408]	Loss 0.6725 (0.6444)	
training:	Epoch: [1][216/408]	Loss 0.7263 (0.6447)	
training:	Epoch: [1][217/408]	Loss 0.4929 (0.6440)	
training:	Epoch: [1][218/408]	Loss 0.5761 (0.6437)	
training:	Epoch: [1][219/408]	Loss 0.6274 (0.6437)	
training:	Epoch: [1][220/408]	Loss 0.6974 (0.6439)	
training:	Epoch: [1][221/408]	Loss 0.5953 (0.6437)	
training:	Epoch: [1][222/408]	Loss 0.7903 (0.6443)	
training:	Epoch: [1][223/408]	Loss 0.6605 (0.6444)	
training:	Epoch: [1][224/408]	Loss 0.6216 (0.6443)	
training:	Epoch: [1][225/408]	Loss 0.5386 (0.6438)	
training:	Epoch: [1][226/408]	Loss 0.6094 (0.6437)	
training:	Epoch: [1][227/408]	Loss 0.5183 (0.6431)	
training:	Epoch: [1][228/408]	Loss 0.5622 (0.6428)	
training:	Epoch: [1][229/408]	Loss 0.6016 (0.6426)	
training:	Epoch: [1][230/408]	Loss 0.6898 (0.6428)	
training:	Epoch: [1][231/408]	Loss 0.5810 (0.6425)	
training:	Epoch: [1][232/408]	Loss 0.6963 (0.6428)	
training:	Epoch: [1][233/408]	Loss 0.6558 (0.6428)	
training:	Epoch: [1][234/408]	Loss 0.7593 (0.6433)	
training:	Epoch: [1][235/408]	Loss 0.5676 (0.6430)	
training:	Epoch: [1][236/408]	Loss 0.5695 (0.6427)	
training:	Epoch: [1][237/408]	Loss 0.6241 (0.6426)	
training:	Epoch: [1][238/408]	Loss 0.5940 (0.6424)	
training:	Epoch: [1][239/408]	Loss 0.6645 (0.6425)	
training:	Epoch: [1][240/408]	Loss 0.6834 (0.6427)	
training:	Epoch: [1][241/408]	Loss 0.7161 (0.6430)	
training:	Epoch: [1][242/408]	Loss 0.6403 (0.6430)	
training:	Epoch: [1][243/408]	Loss 0.6239 (0.6429)	
training:	Epoch: [1][244/408]	Loss 0.5641 (0.6426)	
training:	Epoch: [1][245/408]	Loss 0.5518 (0.6422)	
training:	Epoch: [1][246/408]	Loss 0.5801 (0.6419)	
training:	Epoch: [1][247/408]	Loss 0.5695 (0.6416)	
training:	Epoch: [1][248/408]	Loss 0.6490 (0.6417)	
training:	Epoch: [1][249/408]	Loss 0.6030 (0.6415)	
training:	Epoch: [1][250/408]	Loss 0.6927 (0.6417)	
training:	Epoch: [1][251/408]	Loss 0.6111 (0.6416)	
training:	Epoch: [1][252/408]	Loss 0.4267 (0.6407)	
training:	Epoch: [1][253/408]	Loss 0.6401 (0.6407)	
training:	Epoch: [1][254/408]	Loss 0.6047 (0.6406)	
training:	Epoch: [1][255/408]	Loss 0.6311 (0.6406)	
training:	Epoch: [1][256/408]	Loss 0.5822 (0.6403)	
training:	Epoch: [1][257/408]	Loss 0.6486 (0.6404)	
training:	Epoch: [1][258/408]	Loss 0.7648 (0.6409)	
training:	Epoch: [1][259/408]	Loss 0.6520 (0.6409)	
training:	Epoch: [1][260/408]	Loss 0.6346 (0.6409)	
training:	Epoch: [1][261/408]	Loss 0.7783 (0.6414)	
training:	Epoch: [1][262/408]	Loss 0.5200 (0.6409)	
training:	Epoch: [1][263/408]	Loss 0.7026 (0.6412)	
training:	Epoch: [1][264/408]	Loss 0.5883 (0.6410)	
training:	Epoch: [1][265/408]	Loss 0.6125 (0.6409)	
training:	Epoch: [1][266/408]	Loss 0.5933 (0.6407)	
training:	Epoch: [1][267/408]	Loss 0.6315 (0.6406)	
training:	Epoch: [1][268/408]	Loss 0.5434 (0.6403)	
training:	Epoch: [1][269/408]	Loss 0.6222 (0.6402)	
training:	Epoch: [1][270/408]	Loss 0.5814 (0.6400)	
training:	Epoch: [1][271/408]	Loss 0.4979 (0.6395)	
training:	Epoch: [1][272/408]	Loss 0.6162 (0.6394)	
training:	Epoch: [1][273/408]	Loss 0.4728 (0.6388)	
training:	Epoch: [1][274/408]	Loss 0.5112 (0.6383)	
training:	Epoch: [1][275/408]	Loss 0.5634 (0.6380)	
training:	Epoch: [1][276/408]	Loss 0.6571 (0.6381)	
training:	Epoch: [1][277/408]	Loss 0.5998 (0.6380)	
training:	Epoch: [1][278/408]	Loss 0.6893 (0.6382)	
training:	Epoch: [1][279/408]	Loss 0.5454 (0.6378)	
training:	Epoch: [1][280/408]	Loss 0.5964 (0.6377)	
training:	Epoch: [1][281/408]	Loss 0.5005 (0.6372)	
training:	Epoch: [1][282/408]	Loss 0.5184 (0.6368)	
training:	Epoch: [1][283/408]	Loss 0.8693 (0.6376)	
training:	Epoch: [1][284/408]	Loss 0.6293 (0.6376)	
training:	Epoch: [1][285/408]	Loss 0.5828 (0.6374)	
training:	Epoch: [1][286/408]	Loss 0.7157 (0.6376)	
training:	Epoch: [1][287/408]	Loss 0.5532 (0.6373)	
training:	Epoch: [1][288/408]	Loss 0.7684 (0.6378)	
training:	Epoch: [1][289/408]	Loss 0.5734 (0.6376)	
training:	Epoch: [1][290/408]	Loss 0.5454 (0.6373)	
training:	Epoch: [1][291/408]	Loss 0.5232 (0.6369)	
training:	Epoch: [1][292/408]	Loss 0.5470 (0.6366)	
training:	Epoch: [1][293/408]	Loss 0.6765 (0.6367)	
training:	Epoch: [1][294/408]	Loss 0.3870 (0.6358)	
training:	Epoch: [1][295/408]	Loss 0.5179 (0.6354)	
training:	Epoch: [1][296/408]	Loss 0.5664 (0.6352)	
training:	Epoch: [1][297/408]	Loss 0.6165 (0.6352)	
training:	Epoch: [1][298/408]	Loss 0.5862 (0.6350)	
training:	Epoch: [1][299/408]	Loss 0.4667 (0.6344)	
training:	Epoch: [1][300/408]	Loss 0.4769 (0.6339)	
training:	Epoch: [1][301/408]	Loss 0.6557 (0.6340)	
training:	Epoch: [1][302/408]	Loss 0.4557 (0.6334)	
training:	Epoch: [1][303/408]	Loss 0.5613 (0.6331)	
training:	Epoch: [1][304/408]	Loss 0.8429 (0.6338)	
training:	Epoch: [1][305/408]	Loss 0.5195 (0.6335)	
training:	Epoch: [1][306/408]	Loss 0.5080 (0.6330)	
training:	Epoch: [1][307/408]	Loss 0.4421 (0.6324)	
training:	Epoch: [1][308/408]	Loss 0.5098 (0.6320)	
training:	Epoch: [1][309/408]	Loss 0.5699 (0.6318)	
training:	Epoch: [1][310/408]	Loss 0.5317 (0.6315)	
training:	Epoch: [1][311/408]	Loss 0.5440 (0.6312)	
training:	Epoch: [1][312/408]	Loss 0.6503 (0.6313)	
training:	Epoch: [1][313/408]	Loss 0.6171 (0.6312)	
training:	Epoch: [1][314/408]	Loss 0.5411 (0.6310)	
training:	Epoch: [1][315/408]	Loss 0.5932 (0.6308)	
training:	Epoch: [1][316/408]	Loss 0.6790 (0.6310)	
training:	Epoch: [1][317/408]	Loss 0.4837 (0.6305)	
training:	Epoch: [1][318/408]	Loss 0.5217 (0.6302)	
training:	Epoch: [1][319/408]	Loss 0.5248 (0.6298)	
training:	Epoch: [1][320/408]	Loss 0.6560 (0.6299)	
training:	Epoch: [1][321/408]	Loss 0.4723 (0.6294)	
training:	Epoch: [1][322/408]	Loss 0.6330 (0.6294)	
training:	Epoch: [1][323/408]	Loss 0.5346 (0.6292)	
training:	Epoch: [1][324/408]	Loss 0.5951 (0.6291)	
training:	Epoch: [1][325/408]	Loss 0.7131 (0.6293)	
training:	Epoch: [1][326/408]	Loss 0.6029 (0.6292)	
training:	Epoch: [1][327/408]	Loss 0.5195 (0.6289)	
training:	Epoch: [1][328/408]	Loss 0.4445 (0.6283)	
training:	Epoch: [1][329/408]	Loss 0.5405 (0.6281)	
training:	Epoch: [1][330/408]	Loss 0.4362 (0.6275)	
training:	Epoch: [1][331/408]	Loss 0.5134 (0.6271)	
training:	Epoch: [1][332/408]	Loss 0.4552 (0.6266)	
training:	Epoch: [1][333/408]	Loss 0.4046 (0.6260)	
training:	Epoch: [1][334/408]	Loss 0.4161 (0.6253)	
training:	Epoch: [1][335/408]	Loss 0.5656 (0.6251)	
training:	Epoch: [1][336/408]	Loss 0.5741 (0.6250)	
training:	Epoch: [1][337/408]	Loss 0.5164 (0.6247)	
training:	Epoch: [1][338/408]	Loss 0.3900 (0.6240)	
training:	Epoch: [1][339/408]	Loss 0.7042 (0.6242)	
training:	Epoch: [1][340/408]	Loss 0.8074 (0.6248)	
training:	Epoch: [1][341/408]	Loss 0.7026 (0.6250)	
training:	Epoch: [1][342/408]	Loss 0.6826 (0.6252)	
training:	Epoch: [1][343/408]	Loss 0.5096 (0.6248)	
training:	Epoch: [1][344/408]	Loss 0.4081 (0.6242)	
training:	Epoch: [1][345/408]	Loss 0.4773 (0.6238)	
training:	Epoch: [1][346/408]	Loss 0.5782 (0.6236)	
training:	Epoch: [1][347/408]	Loss 0.5777 (0.6235)	
training:	Epoch: [1][348/408]	Loss 0.4030 (0.6229)	
training:	Epoch: [1][349/408]	Loss 0.5177 (0.6226)	
training:	Epoch: [1][350/408]	Loss 0.6262 (0.6226)	
training:	Epoch: [1][351/408]	Loss 0.4401 (0.6220)	
training:	Epoch: [1][352/408]	Loss 0.5136 (0.6217)	
training:	Epoch: [1][353/408]	Loss 0.4459 (0.6212)	
training:	Epoch: [1][354/408]	Loss 0.5801 (0.6211)	
training:	Epoch: [1][355/408]	Loss 0.6492 (0.6212)	
training:	Epoch: [1][356/408]	Loss 0.6011 (0.6211)	
training:	Epoch: [1][357/408]	Loss 0.6171 (0.6211)	
training:	Epoch: [1][358/408]	Loss 0.4425 (0.6206)	
training:	Epoch: [1][359/408]	Loss 0.5405 (0.6204)	
training:	Epoch: [1][360/408]	Loss 0.5103 (0.6201)	
training:	Epoch: [1][361/408]	Loss 0.6033 (0.6201)	
training:	Epoch: [1][362/408]	Loss 0.5978 (0.6200)	
training:	Epoch: [1][363/408]	Loss 0.6024 (0.6200)	
training:	Epoch: [1][364/408]	Loss 0.4418 (0.6195)	
training:	Epoch: [1][365/408]	Loss 0.4747 (0.6191)	
training:	Epoch: [1][366/408]	Loss 0.3321 (0.6183)	
training:	Epoch: [1][367/408]	Loss 0.5277 (0.6180)	
training:	Epoch: [1][368/408]	Loss 0.4189 (0.6175)	
training:	Epoch: [1][369/408]	Loss 0.6460 (0.6176)	
training:	Epoch: [1][370/408]	Loss 0.4432 (0.6171)	
training:	Epoch: [1][371/408]	Loss 0.6580 (0.6172)	
training:	Epoch: [1][372/408]	Loss 0.4287 (0.6167)	
training:	Epoch: [1][373/408]	Loss 0.5058 (0.6164)	
training:	Epoch: [1][374/408]	Loss 0.5019 (0.6161)	
training:	Epoch: [1][375/408]	Loss 0.5067 (0.6158)	
training:	Epoch: [1][376/408]	Loss 0.5749 (0.6157)	
training:	Epoch: [1][377/408]	Loss 0.4355 (0.6152)	
training:	Epoch: [1][378/408]	Loss 0.5038 (0.6149)	
training:	Epoch: [1][379/408]	Loss 0.5269 (0.6147)	
training:	Epoch: [1][380/408]	Loss 0.6366 (0.6148)	
training:	Epoch: [1][381/408]	Loss 0.6888 (0.6149)	
training:	Epoch: [1][382/408]	Loss 1.0436 (0.6161)	
training:	Epoch: [1][383/408]	Loss 0.3978 (0.6155)	
training:	Epoch: [1][384/408]	Loss 0.6091 (0.6155)	
training:	Epoch: [1][385/408]	Loss 0.5697 (0.6154)	
training:	Epoch: [1][386/408]	Loss 0.7050 (0.6156)	
training:	Epoch: [1][387/408]	Loss 0.4539 (0.6152)	
training:	Epoch: [1][388/408]	Loss 0.5719 (0.6151)	
training:	Epoch: [1][389/408]	Loss 0.5213 (0.6148)	
training:	Epoch: [1][390/408]	Loss 0.6574 (0.6149)	
training:	Epoch: [1][391/408]	Loss 0.4467 (0.6145)	
training:	Epoch: [1][392/408]	Loss 0.6005 (0.6145)	
training:	Epoch: [1][393/408]	Loss 0.6837 (0.6146)	
training:	Epoch: [1][394/408]	Loss 0.5345 (0.6144)	
training:	Epoch: [1][395/408]	Loss 0.5694 (0.6143)	
training:	Epoch: [1][396/408]	Loss 0.5897 (0.6143)	
training:	Epoch: [1][397/408]	Loss 0.4067 (0.6137)	
training:	Epoch: [1][398/408]	Loss 0.5906 (0.6137)	
training:	Epoch: [1][399/408]	Loss 0.5899 (0.6136)	
training:	Epoch: [1][400/408]	Loss 0.4493 (0.6132)	
training:	Epoch: [1][401/408]	Loss 0.5605 (0.6131)	
training:	Epoch: [1][402/408]	Loss 0.5751 (0.6130)	
training:	Epoch: [1][403/408]	Loss 0.6190 (0.6130)	
training:	Epoch: [1][404/408]	Loss 0.5262 (0.6128)	
training:	Epoch: [1][405/408]	Loss 0.6617 (0.6129)	
training:	Epoch: [1][406/408]	Loss 0.6536 (0.6130)	
training:	Epoch: [1][407/408]	Loss 0.6517 (0.6131)	
training:	Epoch: [1][408/408]	Loss 0.5452 (0.6129)	
Training:	 Loss: 0.6120

Training:	 ACC: 0.7402 0.7342 0.6026 0.8778
Validation:	 ACC: 0.7100 0.7025 0.5536 0.8663
Validation:	 Best_BACC: 0.7100 0.7025 0.5536 0.8663
Validation:	 Loss: 0.5752
Pretraining:	Epoch 2/400
----------
training:	Epoch: [2][1/408]	Loss 0.5474 (0.5474)	
training:	Epoch: [2][2/408]	Loss 0.5458 (0.5466)	
training:	Epoch: [2][3/408]	Loss 0.4994 (0.5309)	
training:	Epoch: [2][4/408]	Loss 0.5003 (0.5232)	
training:	Epoch: [2][5/408]	Loss 0.6685 (0.5523)	
training:	Epoch: [2][6/408]	Loss 0.3895 (0.5251)	
training:	Epoch: [2][7/408]	Loss 0.6672 (0.5454)	
training:	Epoch: [2][8/408]	Loss 0.4132 (0.5289)	
training:	Epoch: [2][9/408]	Loss 0.4444 (0.5195)	
training:	Epoch: [2][10/408]	Loss 0.3478 (0.5023)	
training:	Epoch: [2][11/408]	Loss 0.5528 (0.5069)	
training:	Epoch: [2][12/408]	Loss 0.6887 (0.5221)	
training:	Epoch: [2][13/408]	Loss 0.6157 (0.5293)	
training:	Epoch: [2][14/408]	Loss 0.7710 (0.5466)	
training:	Epoch: [2][15/408]	Loss 0.8072 (0.5639)	
training:	Epoch: [2][16/408]	Loss 0.4171 (0.5548)	
training:	Epoch: [2][17/408]	Loss 0.4450 (0.5483)	
training:	Epoch: [2][18/408]	Loss 0.8002 (0.5623)	
training:	Epoch: [2][19/408]	Loss 0.4214 (0.5549)	
training:	Epoch: [2][20/408]	Loss 0.5685 (0.5556)	
training:	Epoch: [2][21/408]	Loss 0.5256 (0.5541)	
training:	Epoch: [2][22/408]	Loss 0.6849 (0.5601)	
training:	Epoch: [2][23/408]	Loss 0.6751 (0.5651)	
training:	Epoch: [2][24/408]	Loss 0.2923 (0.5537)	
training:	Epoch: [2][25/408]	Loss 0.4573 (0.5499)	
training:	Epoch: [2][26/408]	Loss 0.5413 (0.5495)	
training:	Epoch: [2][27/408]	Loss 0.6191 (0.5521)	
training:	Epoch: [2][28/408]	Loss 0.5223 (0.5510)	
training:	Epoch: [2][29/408]	Loss 0.5914 (0.5524)	
training:	Epoch: [2][30/408]	Loss 0.3817 (0.5467)	
training:	Epoch: [2][31/408]	Loss 0.4691 (0.5442)	
training:	Epoch: [2][32/408]	Loss 0.5471 (0.5443)	
training:	Epoch: [2][33/408]	Loss 0.5127 (0.5434)	
training:	Epoch: [2][34/408]	Loss 0.5769 (0.5444)	
training:	Epoch: [2][35/408]	Loss 0.5027 (0.5432)	
training:	Epoch: [2][36/408]	Loss 0.4024 (0.5393)	
training:	Epoch: [2][37/408]	Loss 0.3903 (0.5352)	
training:	Epoch: [2][38/408]	Loss 0.3441 (0.5302)	
training:	Epoch: [2][39/408]	Loss 0.6329 (0.5328)	
training:	Epoch: [2][40/408]	Loss 0.6009 (0.5345)	
training:	Epoch: [2][41/408]	Loss 0.4288 (0.5320)	
training:	Epoch: [2][42/408]	Loss 0.6902 (0.5357)	
training:	Epoch: [2][43/408]	Loss 0.5217 (0.5354)	
training:	Epoch: [2][44/408]	Loss 0.4290 (0.5330)	
training:	Epoch: [2][45/408]	Loss 0.5923 (0.5343)	
training:	Epoch: [2][46/408]	Loss 0.6055 (0.5358)	
training:	Epoch: [2][47/408]	Loss 0.5862 (0.5369)	
training:	Epoch: [2][48/408]	Loss 0.4782 (0.5357)	
training:	Epoch: [2][49/408]	Loss 0.4461 (0.5339)	
training:	Epoch: [2][50/408]	Loss 0.3797 (0.5308)	
training:	Epoch: [2][51/408]	Loss 0.6350 (0.5328)	
training:	Epoch: [2][52/408]	Loss 0.4160 (0.5306)	
training:	Epoch: [2][53/408]	Loss 0.4527 (0.5291)	
training:	Epoch: [2][54/408]	Loss 0.4288 (0.5272)	
training:	Epoch: [2][55/408]	Loss 0.5158 (0.5270)	
training:	Epoch: [2][56/408]	Loss 0.5792 (0.5280)	
training:	Epoch: [2][57/408]	Loss 0.4087 (0.5259)	
training:	Epoch: [2][58/408]	Loss 0.4530 (0.5246)	
training:	Epoch: [2][59/408]	Loss 0.6027 (0.5259)	
training:	Epoch: [2][60/408]	Loss 0.5722 (0.5267)	
training:	Epoch: [2][61/408]	Loss 0.3859 (0.5244)	
training:	Epoch: [2][62/408]	Loss 0.2772 (0.5204)	
training:	Epoch: [2][63/408]	Loss 0.5042 (0.5202)	
training:	Epoch: [2][64/408]	Loss 0.6274 (0.5218)	
training:	Epoch: [2][65/408]	Loss 0.2554 (0.5177)	
training:	Epoch: [2][66/408]	Loss 0.4899 (0.5173)	
training:	Epoch: [2][67/408]	Loss 0.6208 (0.5189)	
training:	Epoch: [2][68/408]	Loss 0.6559 (0.5209)	
training:	Epoch: [2][69/408]	Loss 0.3234 (0.5180)	
training:	Epoch: [2][70/408]	Loss 0.3971 (0.5163)	
training:	Epoch: [2][71/408]	Loss 0.6225 (0.5178)	
training:	Epoch: [2][72/408]	Loss 0.4306 (0.5166)	
training:	Epoch: [2][73/408]	Loss 0.6769 (0.5188)	
training:	Epoch: [2][74/408]	Loss 0.3714 (0.5168)	
training:	Epoch: [2][75/408]	Loss 0.5051 (0.5166)	
training:	Epoch: [2][76/408]	Loss 0.3802 (0.5148)	
training:	Epoch: [2][77/408]	Loss 0.4898 (0.5145)	
training:	Epoch: [2][78/408]	Loss 0.5284 (0.5147)	
training:	Epoch: [2][79/408]	Loss 0.5856 (0.5156)	
training:	Epoch: [2][80/408]	Loss 0.5677 (0.5162)	
training:	Epoch: [2][81/408]	Loss 0.4445 (0.5153)	
training:	Epoch: [2][82/408]	Loss 0.6608 (0.5171)	
training:	Epoch: [2][83/408]	Loss 0.4288 (0.5161)	
training:	Epoch: [2][84/408]	Loss 0.4189 (0.5149)	
training:	Epoch: [2][85/408]	Loss 0.5335 (0.5151)	
training:	Epoch: [2][86/408]	Loss 0.4085 (0.5139)	
training:	Epoch: [2][87/408]	Loss 0.4264 (0.5129)	
training:	Epoch: [2][88/408]	Loss 0.5586 (0.5134)	
training:	Epoch: [2][89/408]	Loss 0.6748 (0.5152)	
training:	Epoch: [2][90/408]	Loss 0.5043 (0.5151)	
training:	Epoch: [2][91/408]	Loss 0.4817 (0.5147)	
training:	Epoch: [2][92/408]	Loss 0.5682 (0.5153)	
training:	Epoch: [2][93/408]	Loss 0.6093 (0.5163)	
training:	Epoch: [2][94/408]	Loss 0.6144 (0.5174)	
training:	Epoch: [2][95/408]	Loss 0.4674 (0.5168)	
training:	Epoch: [2][96/408]	Loss 0.3231 (0.5148)	
training:	Epoch: [2][97/408]	Loss 0.2934 (0.5125)	
training:	Epoch: [2][98/408]	Loss 0.3135 (0.5105)	
training:	Epoch: [2][99/408]	Loss 0.5712 (0.5111)	
training:	Epoch: [2][100/408]	Loss 0.5011 (0.5110)	
training:	Epoch: [2][101/408]	Loss 0.3888 (0.5098)	
training:	Epoch: [2][102/408]	Loss 0.5837 (0.5105)	
training:	Epoch: [2][103/408]	Loss 0.4539 (0.5100)	
training:	Epoch: [2][104/408]	Loss 0.4171 (0.5091)	
training:	Epoch: [2][105/408]	Loss 0.4360 (0.5084)	
training:	Epoch: [2][106/408]	Loss 0.4423 (0.5078)	
training:	Epoch: [2][107/408]	Loss 0.5057 (0.5077)	
training:	Epoch: [2][108/408]	Loss 0.5372 (0.5080)	
training:	Epoch: [2][109/408]	Loss 0.3826 (0.5069)	
training:	Epoch: [2][110/408]	Loss 0.3869 (0.5058)	
training:	Epoch: [2][111/408]	Loss 0.4568 (0.5053)	
training:	Epoch: [2][112/408]	Loss 0.5386 (0.5056)	
training:	Epoch: [2][113/408]	Loss 0.5808 (0.5063)	
training:	Epoch: [2][114/408]	Loss 0.5005 (0.5062)	
training:	Epoch: [2][115/408]	Loss 0.3925 (0.5053)	
training:	Epoch: [2][116/408]	Loss 0.5401 (0.5056)	
training:	Epoch: [2][117/408]	Loss 0.5785 (0.5062)	
training:	Epoch: [2][118/408]	Loss 0.7260 (0.5080)	
training:	Epoch: [2][119/408]	Loss 0.4274 (0.5074)	
training:	Epoch: [2][120/408]	Loss 0.4440 (0.5068)	
