Namespace(inputDirectory='data', outputDirectory='try', DisFile='data/dis_sort_roi2dis2000.npy', task='sex', type='binary', feature=['Nos', 'FA1'], CUDA_id='0', data_id='0', dataset='Classification', norm=True, epochs=200, tensorboard=True, net_architecture='TractGraphormer', batch_size=32, rate=1e-05, weight=0.0, sched_step=300, sched_gamma=0.1, printing_frequency=1, seed=1, alpha=0, remix_kappa=0, remix_tau=0, loss='CE', sigma=0, k=20, fl=64, nh=1)
Training the 'TractGraphormer' architecture

The following parameters are used:
Batch size:	32
Number of workers:	0
Learning rate:	1e-05
Weight decay:	0.0
Scheduler steps:	300
Scheduler gamma:	0.1
Number of epochs of training:	200
Number of input channels:	2

Data preparation
Reading data from:	./data
Training set size:	6538
Validation set size:	1869

Performing calculations on:	cuda:0

Pretraining:	Epoch 1/200
----------
training:	Epoch: [1][1/204]	Loss 0.7371 (0.7371)	
training:	Epoch: [1][2/204]	Loss 0.7143 (0.7257)	
training:	Epoch: [1][3/204]	Loss 0.7776 (0.7430)	
training:	Epoch: [1][4/204]	Loss 0.6874 (0.7291)	
training:	Epoch: [1][5/204]	Loss 0.6887 (0.7210)	
training:	Epoch: [1][6/204]	Loss 0.6817 (0.7145)	
training:	Epoch: [1][7/204]	Loss 0.6972 (0.7120)	
training:	Epoch: [1][8/204]	Loss 0.6711 (0.7069)	
training:	Epoch: [1][9/204]	Loss 0.6766 (0.7035)	
training:	Epoch: [1][10/204]	Loss 0.7067 (0.7038)	
training:	Epoch: [1][11/204]	Loss 0.6802 (0.7017)	
training:	Epoch: [1][12/204]	Loss 0.6807 (0.6999)	
training:	Epoch: [1][13/204]	Loss 0.6870 (0.6989)	
training:	Epoch: [1][14/204]	Loss 0.6947 (0.6986)	
training:	Epoch: [1][15/204]	Loss 0.7505 (0.7021)	
training:	Epoch: [1][16/204]	Loss 0.7182 (0.7031)	
training:	Epoch: [1][17/204]	Loss 0.7201 (0.7041)	
training:	Epoch: [1][18/204]	Loss 0.6856 (0.7031)	
training:	Epoch: [1][19/204]	Loss 0.7440 (0.7052)	
training:	Epoch: [1][20/204]	Loss 0.6919 (0.7046)	
training:	Epoch: [1][21/204]	Loss 0.7094 (0.7048)	
training:	Epoch: [1][22/204]	Loss 0.6852 (0.7039)	
training:	Epoch: [1][23/204]	Loss 0.7031 (0.7039)	
training:	Epoch: [1][24/204]	Loss 0.6941 (0.7035)	
training:	Epoch: [1][25/204]	Loss 0.6856 (0.7027)	
training:	Epoch: [1][26/204]	Loss 0.7175 (0.7033)	
training:	Epoch: [1][27/204]	Loss 0.6988 (0.7031)	
training:	Epoch: [1][28/204]	Loss 0.7216 (0.7038)	
training:	Epoch: [1][29/204]	Loss 0.6898 (0.7033)	
training:	Epoch: [1][30/204]	Loss 0.7098 (0.7035)	
training:	Epoch: [1][31/204]	Loss 0.6952 (0.7033)	
training:	Epoch: [1][32/204]	Loss 0.7217 (0.7038)	
training:	Epoch: [1][33/204]	Loss 0.7340 (0.7048)	
training:	Epoch: [1][34/204]	Loss 0.7507 (0.7061)	
training:	Epoch: [1][35/204]	Loss 0.7066 (0.7061)	
training:	Epoch: [1][36/204]	Loss 0.6886 (0.7056)	
training:	Epoch: [1][37/204]	Loss 0.7140 (0.7059)	
training:	Epoch: [1][38/204]	Loss 0.7126 (0.7060)	
training:	Epoch: [1][39/204]	Loss 0.6802 (0.7054)	
training:	Epoch: [1][40/204]	Loss 0.6879 (0.7049)	
training:	Epoch: [1][41/204]	Loss 0.7157 (0.7052)	
training:	Epoch: [1][42/204]	Loss 0.6876 (0.7048)	
training:	Epoch: [1][43/204]	Loss 0.7202 (0.7051)	
training:	Epoch: [1][44/204]	Loss 0.6754 (0.7045)	
training:	Epoch: [1][45/204]	Loss 0.6985 (0.7043)	
training:	Epoch: [1][46/204]	Loss 0.7236 (0.7048)	
training:	Epoch: [1][47/204]	Loss 0.6638 (0.7039)	
training:	Epoch: [1][48/204]	Loss 0.7046 (0.7039)	
training:	Epoch: [1][49/204]	Loss 0.7057 (0.7039)	
training:	Epoch: [1][50/204]	Loss 0.6710 (0.7033)	
training:	Epoch: [1][51/204]	Loss 0.7423 (0.7040)	
training:	Epoch: [1][52/204]	Loss 0.6976 (0.7039)	
training:	Epoch: [1][53/204]	Loss 0.6754 (0.7034)	
training:	Epoch: [1][54/204]	Loss 0.7010 (0.7033)	
training:	Epoch: [1][55/204]	Loss 0.7224 (0.7037)	
training:	Epoch: [1][56/204]	Loss 0.6575 (0.7029)	
training:	Epoch: [1][57/204]	Loss 0.7298 (0.7033)	
training:	Epoch: [1][58/204]	Loss 0.6882 (0.7031)	
training:	Epoch: [1][59/204]	Loss 0.7031 (0.7031)	
training:	Epoch: [1][60/204]	Loss 0.7041 (0.7031)	
training:	Epoch: [1][61/204]	Loss 0.6700 (0.7025)	
training:	Epoch: [1][62/204]	Loss 0.7155 (0.7028)	
training:	Epoch: [1][63/204]	Loss 0.6880 (0.7025)	
training:	Epoch: [1][64/204]	Loss 0.6805 (0.7022)	
training:	Epoch: [1][65/204]	Loss 0.6543 (0.7014)	
training:	Epoch: [1][66/204]	Loss 0.6773 (0.7011)	
training:	Epoch: [1][67/204]	Loss 0.6534 (0.7004)	
training:	Epoch: [1][68/204]	Loss 0.7329 (0.7008)	
training:	Epoch: [1][69/204]	Loss 0.6795 (0.7005)	
training:	Epoch: [1][70/204]	Loss 0.6749 (0.7002)	
training:	Epoch: [1][71/204]	Loss 0.6796 (0.6999)	
training:	Epoch: [1][72/204]	Loss 0.6989 (0.6999)	
training:	Epoch: [1][73/204]	Loss 0.6783 (0.6996)	
training:	Epoch: [1][74/204]	Loss 0.6761 (0.6992)	
training:	Epoch: [1][75/204]	Loss 0.7059 (0.6993)	
training:	Epoch: [1][76/204]	Loss 0.7176 (0.6996)	
training:	Epoch: [1][77/204]	Loss 0.6828 (0.6994)	
training:	Epoch: [1][78/204]	Loss 0.6977 (0.6993)	
training:	Epoch: [1][79/204]	Loss 0.6878 (0.6992)	
training:	Epoch: [1][80/204]	Loss 0.7154 (0.6994)	
training:	Epoch: [1][81/204]	Loss 0.6574 (0.6989)	
training:	Epoch: [1][82/204]	Loss 0.7002 (0.6989)	
training:	Epoch: [1][83/204]	Loss 0.6253 (0.6980)	
training:	Epoch: [1][84/204]	Loss 0.7209 (0.6983)	
training:	Epoch: [1][85/204]	Loss 0.6794 (0.6981)	
training:	Epoch: [1][86/204]	Loss 0.7154 (0.6983)	
training:	Epoch: [1][87/204]	Loss 0.7106 (0.6984)	
training:	Epoch: [1][88/204]	Loss 0.6969 (0.6984)	
training:	Epoch: [1][89/204]	Loss 0.6908 (0.6983)	
training:	Epoch: [1][90/204]	Loss 0.7429 (0.6988)	
training:	Epoch: [1][91/204]	Loss 0.6452 (0.6982)	
training:	Epoch: [1][92/204]	Loss 0.7102 (0.6983)	
training:	Epoch: [1][93/204]	Loss 0.6402 (0.6977)	
training:	Epoch: [1][94/204]	Loss 0.6817 (0.6975)	
training:	Epoch: [1][95/204]	Loss 0.6791 (0.6973)	
training:	Epoch: [1][96/204]	Loss 0.6771 (0.6971)	
training:	Epoch: [1][97/204]	Loss 0.6999 (0.6972)	
training:	Epoch: [1][98/204]	Loss 0.6966 (0.6972)	
training:	Epoch: [1][99/204]	Loss 0.6834 (0.6970)	
training:	Epoch: [1][100/204]	Loss 0.6688 (0.6967)	
training:	Epoch: [1][101/204]	Loss 0.6910 (0.6967)	
training:	Epoch: [1][102/204]	Loss 0.6746 (0.6965)	
training:	Epoch: [1][103/204]	Loss 0.7148 (0.6966)	
training:	Epoch: [1][104/204]	Loss 0.6717 (0.6964)	
training:	Epoch: [1][105/204]	Loss 0.6627 (0.6961)	
training:	Epoch: [1][106/204]	Loss 0.6921 (0.6960)	
training:	Epoch: [1][107/204]	Loss 0.7184 (0.6963)	
training:	Epoch: [1][108/204]	Loss 0.6860 (0.6962)	
training:	Epoch: [1][109/204]	Loss 0.6750 (0.6960)	
training:	Epoch: [1][110/204]	Loss 0.6675 (0.6957)	
training:	Epoch: [1][111/204]	Loss 0.7733 (0.6964)	
training:	Epoch: [1][112/204]	Loss 0.6480 (0.6960)	
training:	Epoch: [1][113/204]	Loss 0.6891 (0.6959)	
training:	Epoch: [1][114/204]	Loss 0.6422 (0.6954)	
training:	Epoch: [1][115/204]	Loss 0.7187 (0.6956)	
training:	Epoch: [1][116/204]	Loss 0.6836 (0.6955)	
training:	Epoch: [1][117/204]	Loss 0.6504 (0.6952)	
training:	Epoch: [1][118/204]	Loss 0.7015 (0.6952)	
training:	Epoch: [1][119/204]	Loss 0.6385 (0.6947)	
training:	Epoch: [1][120/204]	Loss 0.6557 (0.6944)	
training:	Epoch: [1][121/204]	Loss 0.6801 (0.6943)	
training:	Epoch: [1][122/204]	Loss 0.6503 (0.6939)	
training:	Epoch: [1][123/204]	Loss 0.7003 (0.6940)	
training:	Epoch: [1][124/204]	Loss 0.7015 (0.6940)	
training:	Epoch: [1][125/204]	Loss 0.7128 (0.6942)	
training:	Epoch: [1][126/204]	Loss 0.6751 (0.6940)	
training:	Epoch: [1][127/204]	Loss 0.7215 (0.6943)	
training:	Epoch: [1][128/204]	Loss 0.7190 (0.6944)	
training:	Epoch: [1][129/204]	Loss 0.6616 (0.6942)	
training:	Epoch: [1][130/204]	Loss 0.6870 (0.6941)	
training:	Epoch: [1][131/204]	Loss 0.7236 (0.6944)	
training:	Epoch: [1][132/204]	Loss 0.6894 (0.6943)	
training:	Epoch: [1][133/204]	Loss 0.6650 (0.6941)	
training:	Epoch: [1][134/204]	Loss 0.7095 (0.6942)	
training:	Epoch: [1][135/204]	Loss 0.6613 (0.6940)	
training:	Epoch: [1][136/204]	Loss 0.6828 (0.6939)	
training:	Epoch: [1][137/204]	Loss 0.6608 (0.6936)	
training:	Epoch: [1][138/204]	Loss 0.6645 (0.6934)	
training:	Epoch: [1][139/204]	Loss 0.7258 (0.6937)	
training:	Epoch: [1][140/204]	Loss 0.7001 (0.6937)	
training:	Epoch: [1][141/204]	Loss 0.6310 (0.6933)	
training:	Epoch: [1][142/204]	Loss 0.6844 (0.6932)	
training:	Epoch: [1][143/204]	Loss 0.6502 (0.6929)	
training:	Epoch: [1][144/204]	Loss 0.6676 (0.6927)	
training:	Epoch: [1][145/204]	Loss 0.6896 (0.6927)	
training:	Epoch: [1][146/204]	Loss 0.6769 (0.6926)	
training:	Epoch: [1][147/204]	Loss 0.7102 (0.6927)	
training:	Epoch: [1][148/204]	Loss 0.6901 (0.6927)	
training:	Epoch: [1][149/204]	Loss 0.6298 (0.6923)	
training:	Epoch: [1][150/204]	Loss 0.6753 (0.6922)	
training:	Epoch: [1][151/204]	Loss 0.6934 (0.6922)	
training:	Epoch: [1][152/204]	Loss 0.6869 (0.6921)	
training:	Epoch: [1][153/204]	Loss 0.7069 (0.6922)	
training:	Epoch: [1][154/204]	Loss 0.7063 (0.6923)	
training:	Epoch: [1][155/204]	Loss 0.6752 (0.6922)	
training:	Epoch: [1][156/204]	Loss 0.6868 (0.6922)	
training:	Epoch: [1][157/204]	Loss 0.6775 (0.6921)	
training:	Epoch: [1][158/204]	Loss 0.6983 (0.6921)	
training:	Epoch: [1][159/204]	Loss 0.7147 (0.6923)	
training:	Epoch: [1][160/204]	Loss 0.6874 (0.6922)	
training:	Epoch: [1][161/204]	Loss 0.6528 (0.6920)	
training:	Epoch: [1][162/204]	Loss 0.7077 (0.6921)	
training:	Epoch: [1][163/204]	Loss 0.6534 (0.6919)	
training:	Epoch: [1][164/204]	Loss 0.6488 (0.6916)	
training:	Epoch: [1][165/204]	Loss 0.6601 (0.6914)	
training:	Epoch: [1][166/204]	Loss 0.7028 (0.6915)	
training:	Epoch: [1][167/204]	Loss 0.6824 (0.6914)	
training:	Epoch: [1][168/204]	Loss 0.6432 (0.6911)	
training:	Epoch: [1][169/204]	Loss 0.6836 (0.6911)	
training:	Epoch: [1][170/204]	Loss 0.6763 (0.6910)	
training:	Epoch: [1][171/204]	Loss 0.6937 (0.6910)	
training:	Epoch: [1][172/204]	Loss 0.6917 (0.6910)	
training:	Epoch: [1][173/204]	Loss 0.6381 (0.6907)	
training:	Epoch: [1][174/204]	Loss 0.6250 (0.6903)	
training:	Epoch: [1][175/204]	Loss 0.6942 (0.6904)	
training:	Epoch: [1][176/204]	Loss 0.6753 (0.6903)	
training:	Epoch: [1][177/204]	Loss 0.7059 (0.6904)	
training:	Epoch: [1][178/204]	Loss 0.6929 (0.6904)	
training:	Epoch: [1][179/204]	Loss 0.6438 (0.6901)	
training:	Epoch: [1][180/204]	Loss 0.6904 (0.6901)	
training:	Epoch: [1][181/204]	Loss 0.6633 (0.6900)	
training:	Epoch: [1][182/204]	Loss 0.6740 (0.6899)	
training:	Epoch: [1][183/204]	Loss 0.6561 (0.6897)	
training:	Epoch: [1][184/204]	Loss 0.6725 (0.6896)	
training:	Epoch: [1][185/204]	Loss 0.7370 (0.6899)	
training:	Epoch: [1][186/204]	Loss 0.6529 (0.6897)	
training:	Epoch: [1][187/204]	Loss 0.6742 (0.6896)	
training:	Epoch: [1][188/204]	Loss 0.6591 (0.6894)	
training:	Epoch: [1][189/204]	Loss 0.6773 (0.6894)	
training:	Epoch: [1][190/204]	Loss 0.7549 (0.6897)	
training:	Epoch: [1][191/204]	Loss 0.6862 (0.6897)	
training:	Epoch: [1][192/204]	Loss 0.6651 (0.6895)	
training:	Epoch: [1][193/204]	Loss 0.6776 (0.6895)	
training:	Epoch: [1][194/204]	Loss 0.6816 (0.6894)	
training:	Epoch: [1][195/204]	Loss 0.6849 (0.6894)	
training:	Epoch: [1][196/204]	Loss 0.7212 (0.6896)	
training:	Epoch: [1][197/204]	Loss 0.6867 (0.6896)	
training:	Epoch: [1][198/204]	Loss 0.6801 (0.6895)	
training:	Epoch: [1][199/204]	Loss 0.6539 (0.6893)	
training:	Epoch: [1][200/204]	Loss 0.6598 (0.6892)	
training:	Epoch: [1][201/204]	Loss 0.6899 (0.6892)	
training:	Epoch: [1][202/204]	Loss 0.6720 (0.6891)	
training:	Epoch: [1][203/204]	Loss 0.6465 (0.6889)	
training:	Epoch: [1][204/204]	Loss 0.6623 (0.6888)	
Training:	 Loss: 0.6877

Training:	 ACC: 0.5814 0.5855 0.6822 0.4805
Validation:	 ACC: 0.5764 0.5816 0.6909 0.4619
Validation:	 Best_BACC: 0.5764 0.5816 0.6909 0.4619
Validation:	 Loss: 0.6743
Pretraining:	Epoch 2/200
----------
training:	Epoch: [2][1/204]	Loss 0.6251 (0.6251)	
training:	Epoch: [2][2/204]	Loss 0.6476 (0.6363)	
training:	Epoch: [2][3/204]	Loss 0.6353 (0.6360)	
training:	Epoch: [2][4/204]	Loss 0.7010 (0.6522)	
training:	Epoch: [2][5/204]	Loss 0.6480 (0.6514)	
training:	Epoch: [2][6/204]	Loss 0.6649 (0.6537)	
training:	Epoch: [2][7/204]	Loss 0.6888 (0.6587)	
training:	Epoch: [2][8/204]	Loss 0.6741 (0.6606)	
training:	Epoch: [2][9/204]	Loss 0.6414 (0.6585)	
training:	Epoch: [2][10/204]	Loss 0.6810 (0.6607)	
training:	Epoch: [2][11/204]	Loss 0.6623 (0.6609)	
training:	Epoch: [2][12/204]	Loss 0.7209 (0.6659)	
training:	Epoch: [2][13/204]	Loss 0.6805 (0.6670)	
training:	Epoch: [2][14/204]	Loss 0.6280 (0.6642)	
training:	Epoch: [2][15/204]	Loss 0.7344 (0.6689)	
training:	Epoch: [2][16/204]	Loss 0.6521 (0.6678)	
training:	Epoch: [2][17/204]	Loss 0.6391 (0.6661)	
training:	Epoch: [2][18/204]	Loss 0.6794 (0.6669)	
training:	Epoch: [2][19/204]	Loss 0.6878 (0.6680)	
training:	Epoch: [2][20/204]	Loss 0.6561 (0.6674)	
training:	Epoch: [2][21/204]	Loss 0.6857 (0.6683)	
training:	Epoch: [2][22/204]	Loss 0.6910 (0.6693)	
training:	Epoch: [2][23/204]	Loss 0.6980 (0.6705)	
training:	Epoch: [2][24/204]	Loss 0.7288 (0.6730)	
training:	Epoch: [2][25/204]	Loss 0.6397 (0.6716)	
training:	Epoch: [2][26/204]	Loss 0.6737 (0.6717)	
training:	Epoch: [2][27/204]	Loss 0.6144 (0.6696)	
training:	Epoch: [2][28/204]	Loss 0.6439 (0.6687)	
training:	Epoch: [2][29/204]	Loss 0.6713 (0.6688)	
training:	Epoch: [2][30/204]	Loss 0.6140 (0.6669)	
training:	Epoch: [2][31/204]	Loss 0.6688 (0.6670)	
training:	Epoch: [2][32/204]	Loss 0.6627 (0.6669)	
training:	Epoch: [2][33/204]	Loss 0.6731 (0.6671)	
training:	Epoch: [2][34/204]	Loss 0.7028 (0.6681)	
training:	Epoch: [2][35/204]	Loss 0.6895 (0.6687)	
training:	Epoch: [2][36/204]	Loss 0.6492 (0.6682)	
training:	Epoch: [2][37/204]	Loss 0.6620 (0.6680)	
training:	Epoch: [2][38/204]	Loss 0.6787 (0.6683)	
training:	Epoch: [2][39/204]	Loss 0.7045 (0.6692)	
training:	Epoch: [2][40/204]	Loss 0.6916 (0.6698)	
training:	Epoch: [2][41/204]	Loss 0.6573 (0.6695)	
training:	Epoch: [2][42/204]	Loss 0.6656 (0.6694)	
training:	Epoch: [2][43/204]	Loss 0.6879 (0.6698)	
training:	Epoch: [2][44/204]	Loss 0.6744 (0.6699)	
training:	Epoch: [2][45/204]	Loss 0.6690 (0.6699)	
training:	Epoch: [2][46/204]	Loss 0.6981 (0.6705)	
training:	Epoch: [2][47/204]	Loss 0.6649 (0.6704)	
training:	Epoch: [2][48/204]	Loss 0.6665 (0.6703)	
training:	Epoch: [2][49/204]	Loss 0.7068 (0.6711)	
training:	Epoch: [2][50/204]	Loss 0.7399 (0.6724)	
training:	Epoch: [2][51/204]	Loss 0.7071 (0.6731)	
training:	Epoch: [2][52/204]	Loss 0.6630 (0.6729)	
training:	Epoch: [2][53/204]	Loss 0.6926 (0.6733)	
training:	Epoch: [2][54/204]	Loss 0.7043 (0.6739)	
training:	Epoch: [2][55/204]	Loss 0.6461 (0.6734)	
training:	Epoch: [2][56/204]	Loss 0.6703 (0.6733)	
training:	Epoch: [2][57/204]	Loss 0.6468 (0.6728)	
training:	Epoch: [2][58/204]	Loss 0.7065 (0.6734)	
training:	Epoch: [2][59/204]	Loss 0.6710 (0.6734)	
training:	Epoch: [2][60/204]	Loss 0.6425 (0.6729)	
training:	Epoch: [2][61/204]	Loss 0.6976 (0.6733)	
training:	Epoch: [2][62/204]	Loss 0.6827 (0.6734)	
training:	Epoch: [2][63/204]	Loss 0.7272 (0.6743)	
training:	Epoch: [2][64/204]	Loss 0.6560 (0.6740)	
training:	Epoch: [2][65/204]	Loss 0.6414 (0.6735)	
training:	Epoch: [2][66/204]	Loss 0.6654 (0.6734)	
training:	Epoch: [2][67/204]	Loss 0.6822 (0.6735)	
training:	Epoch: [2][68/204]	Loss 0.7108 (0.6740)	
training:	Epoch: [2][69/204]	Loss 0.6011 (0.6730)	
training:	Epoch: [2][70/204]	Loss 0.6837 (0.6731)	
training:	Epoch: [2][71/204]	Loss 0.6573 (0.6729)	
training:	Epoch: [2][72/204]	Loss 0.7502 (0.6740)	
training:	Epoch: [2][73/204]	Loss 0.6645 (0.6739)	
training:	Epoch: [2][74/204]	Loss 0.6988 (0.6742)	
training:	Epoch: [2][75/204]	Loss 0.6541 (0.6739)	
training:	Epoch: [2][76/204]	Loss 0.6589 (0.6737)	
training:	Epoch: [2][77/204]	Loss 0.6994 (0.6741)	
training:	Epoch: [2][78/204]	Loss 0.6963 (0.6744)	
training:	Epoch: [2][79/204]	Loss 0.6578 (0.6741)	
training:	Epoch: [2][80/204]	Loss 0.6143 (0.6734)	
training:	Epoch: [2][81/204]	Loss 0.6909 (0.6736)	
training:	Epoch: [2][82/204]	Loss 0.7184 (0.6742)	
training:	Epoch: [2][83/204]	Loss 0.6934 (0.6744)	
training:	Epoch: [2][84/204]	Loss 0.6790 (0.6744)	
training:	Epoch: [2][85/204]	Loss 0.6394 (0.6740)	
training:	Epoch: [2][86/204]	Loss 0.7077 (0.6744)	
training:	Epoch: [2][87/204]	Loss 0.6470 (0.6741)	
training:	Epoch: [2][88/204]	Loss 0.6551 (0.6739)	
training:	Epoch: [2][89/204]	Loss 0.6696 (0.6738)	
training:	Epoch: [2][90/204]	Loss 0.7114 (0.6743)	
training:	Epoch: [2][91/204]	Loss 0.6538 (0.6740)	
training:	Epoch: [2][92/204]	Loss 0.6680 (0.6740)	
training:	Epoch: [2][93/204]	Loss 0.6114 (0.6733)	
training:	Epoch: [2][94/204]	Loss 0.6950 (0.6735)	
training:	Epoch: [2][95/204]	Loss 0.6661 (0.6734)	
training:	Epoch: [2][96/204]	Loss 0.6533 (0.6732)	
training:	Epoch: [2][97/204]	Loss 0.6598 (0.6731)	
training:	Epoch: [2][98/204]	Loss 0.6690 (0.6731)	
training:	Epoch: [2][99/204]	Loss 0.7318 (0.6737)	
training:	Epoch: [2][100/204]	Loss 0.7027 (0.6739)	
training:	Epoch: [2][101/204]	Loss 0.6510 (0.6737)	
training:	Epoch: [2][102/204]	Loss 0.6783 (0.6738)	
training:	Epoch: [2][103/204]	Loss 0.6884 (0.6739)	
training:	Epoch: [2][104/204]	Loss 0.6347 (0.6735)	
training:	Epoch: [2][105/204]	Loss 0.6783 (0.6736)	
training:	Epoch: [2][106/204]	Loss 0.6416 (0.6733)	
training:	Epoch: [2][107/204]	Loss 0.6819 (0.6734)	
training:	Epoch: [2][108/204]	Loss 0.7014 (0.6736)	
training:	Epoch: [2][109/204]	Loss 0.6360 (0.6733)	
training:	Epoch: [2][110/204]	Loss 0.6293 (0.6729)	
training:	Epoch: [2][111/204]	Loss 0.6454 (0.6726)	
training:	Epoch: [2][112/204]	Loss 0.6332 (0.6723)	
training:	Epoch: [2][113/204]	Loss 0.6492 (0.6721)	
training:	Epoch: [2][114/204]	Loss 0.6077 (0.6715)	
training:	Epoch: [2][115/204]	Loss 0.7167 (0.6719)	
training:	Epoch: [2][116/204]	Loss 0.7320 (0.6724)	
training:	Epoch: [2][117/204]	Loss 0.6670 (0.6724)	
training:	Epoch: [2][118/204]	Loss 0.6866 (0.6725)	
training:	Epoch: [2][119/204]	Loss 0.6348 (0.6722)	
training:	Epoch: [2][120/204]	Loss 0.6776 (0.6722)	
training:	Epoch: [2][121/204]	Loss 0.7273 (0.6727)	
training:	Epoch: [2][122/204]	Loss 0.6704 (0.6726)	
training:	Epoch: [2][123/204]	Loss 0.6589 (0.6725)	
training:	Epoch: [2][124/204]	Loss 0.6798 (0.6726)	
training:	Epoch: [2][125/204]	Loss 0.6773 (0.6726)	
training:	Epoch: [2][126/204]	Loss 0.6337 (0.6723)	
training:	Epoch: [2][127/204]	Loss 0.6654 (0.6723)	
training:	Epoch: [2][128/204]	Loss 0.6382 (0.6720)	
training:	Epoch: [2][129/204]	Loss 0.6603 (0.6719)	
training:	Epoch: [2][130/204]	Loss 0.6689 (0.6719)	
training:	Epoch: [2][131/204]	Loss 0.6904 (0.6720)	
training:	Epoch: [2][132/204]	Loss 0.6629 (0.6720)	
training:	Epoch: [2][133/204]	Loss 0.6381 (0.6717)	
training:	Epoch: [2][134/204]	Loss 0.5903 (0.6711)	
training:	Epoch: [2][135/204]	Loss 0.6740 (0.6711)	
training:	Epoch: [2][136/204]	Loss 0.6038 (0.6706)	
training:	Epoch: [2][137/204]	Loss 0.6703 (0.6706)	
training:	Epoch: [2][138/204]	Loss 0.6961 (0.6708)	
training:	Epoch: [2][139/204]	Loss 0.6517 (0.6707)	
training:	Epoch: [2][140/204]	Loss 0.6418 (0.6705)	
training:	Epoch: [2][141/204]	Loss 0.6463 (0.6703)	
training:	Epoch: [2][142/204]	Loss 0.6476 (0.6701)	
training:	Epoch: [2][143/204]	Loss 0.7137 (0.6704)	
training:	Epoch: [2][144/204]	Loss 0.7098 (0.6707)	
training:	Epoch: [2][145/204]	Loss 0.6508 (0.6706)	
training:	Epoch: [2][146/204]	Loss 0.6485 (0.6704)	
training:	Epoch: [2][147/204]	Loss 0.6673 (0.6704)	
training:	Epoch: [2][148/204]	Loss 0.6619 (0.6703)	
training:	Epoch: [2][149/204]	Loss 0.6444 (0.6702)	
training:	Epoch: [2][150/204]	Loss 0.6395 (0.6700)	
training:	Epoch: [2][151/204]	Loss 0.6568 (0.6699)	
training:	Epoch: [2][152/204]	Loss 0.6995 (0.6701)	
training:	Epoch: [2][153/204]	Loss 0.7093 (0.6703)	
training:	Epoch: [2][154/204]	Loss 0.6550 (0.6702)	
training:	Epoch: [2][155/204]	Loss 0.7118 (0.6705)	
training:	Epoch: [2][156/204]	Loss 0.7058 (0.6707)	
training:	Epoch: [2][157/204]	Loss 0.6821 (0.6708)	
training:	Epoch: [2][158/204]	Loss 0.6301 (0.6705)	
training:	Epoch: [2][159/204]	Loss 0.7021 (0.6707)	
training:	Epoch: [2][160/204]	Loss 0.5857 (0.6702)	
training:	Epoch: [2][161/204]	Loss 0.6867 (0.6703)	
training:	Epoch: [2][162/204]	Loss 0.6518 (0.6702)	
training:	Epoch: [2][163/204]	Loss 0.6642 (0.6702)	
training:	Epoch: [2][164/204]	Loss 0.6866 (0.6703)	
training:	Epoch: [2][165/204]	Loss 0.6556 (0.6702)	
training:	Epoch: [2][166/204]	Loss 0.6429 (0.6700)	
training:	Epoch: [2][167/204]	Loss 0.5988 (0.6696)	
training:	Epoch: [2][168/204]	Loss 0.6643 (0.6695)	
training:	Epoch: [2][169/204]	Loss 0.6549 (0.6695)	
training:	Epoch: [2][170/204]	Loss 0.6453 (0.6693)	
training:	Epoch: [2][171/204]	Loss 0.7150 (0.6696)	
training:	Epoch: [2][172/204]	Loss 0.5922 (0.6691)	
training:	Epoch: [2][173/204]	Loss 0.6316 (0.6689)	
training:	Epoch: [2][174/204]	Loss 0.6561 (0.6688)	
training:	Epoch: [2][175/204]	Loss 0.6446 (0.6687)	
training:	Epoch: [2][176/204]	Loss 0.6884 (0.6688)	
training:	Epoch: [2][177/204]	Loss 0.6624 (0.6688)	
training:	Epoch: [2][178/204]	Loss 0.6694 (0.6688)	
training:	Epoch: [2][179/204]	Loss 0.6475 (0.6687)	
training:	Epoch: [2][180/204]	Loss 0.6819 (0.6687)	
training:	Epoch: [2][181/204]	Loss 0.6650 (0.6687)	
training:	Epoch: [2][182/204]	Loss 0.5984 (0.6683)	
training:	Epoch: [2][183/204]	Loss 0.6751 (0.6684)	
training:	Epoch: [2][184/204]	Loss 0.6996 (0.6685)	
training:	Epoch: [2][185/204]	Loss 0.6877 (0.6686)	
training:	Epoch: [2][186/204]	Loss 0.6419 (0.6685)	
training:	Epoch: [2][187/204]	Loss 0.6665 (0.6685)	
training:	Epoch: [2][188/204]	Loss 0.6425 (0.6684)	
training:	Epoch: [2][189/204]	Loss 0.6695 (0.6684)	
training:	Epoch: [2][190/204]	Loss 0.7341 (0.6687)	
training:	Epoch: [2][191/204]	Loss 0.6008 (0.6683)	
training:	Epoch: [2][192/204]	Loss 0.6678 (0.6683)	
training:	Epoch: [2][193/204]	Loss 0.6620 (0.6683)	
training:	Epoch: [2][194/204]	Loss 0.6820 (0.6684)	
training:	Epoch: [2][195/204]	Loss 0.6184 (0.6681)	
training:	Epoch: [2][196/204]	Loss 0.6466 (0.6680)	
training:	Epoch: [2][197/204]	Loss 0.6262 (0.6678)	
training:	Epoch: [2][198/204]	Loss 0.6452 (0.6677)	
training:	Epoch: [2][199/204]	Loss 0.7048 (0.6679)	
training:	Epoch: [2][200/204]	Loss 0.6865 (0.6680)	
training:	Epoch: [2][201/204]	Loss 0.6498 (0.6679)	
training:	Epoch: [2][202/204]	Loss 0.6319 (0.6677)	
training:	Epoch: [2][203/204]	Loss 0.6509 (0.6676)	
training:	Epoch: [2][204/204]	Loss 0.6950 (0.6678)	
Training:	 Loss: 0.6667

Training:	 ACC: 0.6168 0.6176 0.6379 0.5957
Validation:	 ACC: 0.6129 0.6137 0.6295 0.5964
Validation:	 Best_BACC: 0.6129 0.6137 0.6295 0.5964
Validation:	 Loss: 0.6566
Pretraining:	Epoch 3/200
----------
training:	Epoch: [3][1/204]	Loss 0.6682 (0.6682)	
training:	Epoch: [3][2/204]	Loss 0.5952 (0.6317)	
training:	Epoch: [3][3/204]	Loss 0.6550 (0.6395)	
training:	Epoch: [3][4/204]	Loss 0.6907 (0.6523)	
training:	Epoch: [3][5/204]	Loss 0.7049 (0.6628)	
training:	Epoch: [3][6/204]	Loss 0.6407 (0.6591)	
training:	Epoch: [3][7/204]	Loss 0.6589 (0.6591)	
training:	Epoch: [3][8/204]	Loss 0.6673 (0.6601)	
training:	Epoch: [3][9/204]	Loss 0.6435 (0.6583)	
training:	Epoch: [3][10/204]	Loss 0.6518 (0.6576)	
training:	Epoch: [3][11/204]	Loss 0.6106 (0.6533)	
training:	Epoch: [3][12/204]	Loss 0.7426 (0.6608)	
training:	Epoch: [3][13/204]	Loss 0.6313 (0.6585)	
training:	Epoch: [3][14/204]	Loss 0.6636 (0.6589)	
training:	Epoch: [3][15/204]	Loss 0.6493 (0.6582)	
training:	Epoch: [3][16/204]	Loss 0.6578 (0.6582)	
training:	Epoch: [3][17/204]	Loss 0.6962 (0.6605)	
training:	Epoch: [3][18/204]	Loss 0.6854 (0.6618)	
training:	Epoch: [3][19/204]	Loss 0.7186 (0.6648)	
training:	Epoch: [3][20/204]	Loss 0.6535 (0.6643)	
training:	Epoch: [3][21/204]	Loss 0.6472 (0.6634)	
training:	Epoch: [3][22/204]	Loss 0.6401 (0.6624)	
training:	Epoch: [3][23/204]	Loss 0.7066 (0.6643)	
training:	Epoch: [3][24/204]	Loss 0.6524 (0.6638)	
training:	Epoch: [3][25/204]	Loss 0.7138 (0.6658)	
training:	Epoch: [3][26/204]	Loss 0.6754 (0.6662)	
training:	Epoch: [3][27/204]	Loss 0.6388 (0.6652)	
training:	Epoch: [3][28/204]	Loss 0.6079 (0.6631)	
training:	Epoch: [3][29/204]	Loss 0.6957 (0.6642)	
training:	Epoch: [3][30/204]	Loss 0.6692 (0.6644)	
training:	Epoch: [3][31/204]	Loss 0.6357 (0.6635)	
training:	Epoch: [3][32/204]	Loss 0.6663 (0.6636)	
training:	Epoch: [3][33/204]	Loss 0.5938 (0.6615)	
training:	Epoch: [3][34/204]	Loss 0.6432 (0.6609)	
training:	Epoch: [3][35/204]	Loss 0.6890 (0.6617)	
training:	Epoch: [3][36/204]	Loss 0.6187 (0.6605)	
training:	Epoch: [3][37/204]	Loss 0.6407 (0.6600)	
training:	Epoch: [3][38/204]	Loss 0.6574 (0.6599)	
training:	Epoch: [3][39/204]	Loss 0.6480 (0.6596)	
training:	Epoch: [3][40/204]	Loss 0.6212 (0.6587)	
training:	Epoch: [3][41/204]	Loss 0.6643 (0.6588)	
training:	Epoch: [3][42/204]	Loss 0.6279 (0.6581)	
training:	Epoch: [3][43/204]	Loss 0.6584 (0.6581)	
training:	Epoch: [3][44/204]	Loss 0.7355 (0.6598)	
training:	Epoch: [3][45/204]	Loss 0.6841 (0.6604)	
training:	Epoch: [3][46/204]	Loss 0.6768 (0.6607)	
training:	Epoch: [3][47/204]	Loss 0.6459 (0.6604)	
training:	Epoch: [3][48/204]	Loss 0.6704 (0.6606)	
training:	Epoch: [3][49/204]	Loss 0.6634 (0.6607)	
training:	Epoch: [3][50/204]	Loss 0.6459 (0.6604)	
training:	Epoch: [3][51/204]	Loss 0.6496 (0.6602)	
training:	Epoch: [3][52/204]	Loss 0.5678 (0.6584)	
training:	Epoch: [3][53/204]	Loss 0.6186 (0.6576)	
training:	Epoch: [3][54/204]	Loss 0.6838 (0.6581)	
training:	Epoch: [3][55/204]	Loss 0.6391 (0.6578)	
training:	Epoch: [3][56/204]	Loss 0.6918 (0.6584)	
training:	Epoch: [3][57/204]	Loss 0.6596 (0.6584)	
training:	Epoch: [3][58/204]	Loss 0.7212 (0.6595)	
training:	Epoch: [3][59/204]	Loss 0.6979 (0.6601)	
training:	Epoch: [3][60/204]	Loss 0.6940 (0.6607)	
training:	Epoch: [3][61/204]	Loss 0.7172 (0.6616)	
training:	Epoch: [3][62/204]	Loss 0.6457 (0.6614)	
training:	Epoch: [3][63/204]	Loss 0.7044 (0.6621)	
training:	Epoch: [3][64/204]	Loss 0.6779 (0.6623)	
training:	Epoch: [3][65/204]	Loss 0.6731 (0.6625)	
training:	Epoch: [3][66/204]	Loss 0.6342 (0.6620)	
training:	Epoch: [3][67/204]	Loss 0.6981 (0.6626)	
training:	Epoch: [3][68/204]	Loss 0.6163 (0.6619)	
training:	Epoch: [3][69/204]	Loss 0.6548 (0.6618)	
training:	Epoch: [3][70/204]	Loss 0.6037 (0.6610)	
training:	Epoch: [3][71/204]	Loss 0.6133 (0.6603)	
training:	Epoch: [3][72/204]	Loss 0.6690 (0.6604)	
training:	Epoch: [3][73/204]	Loss 0.6644 (0.6605)	
training:	Epoch: [3][74/204]	Loss 0.6331 (0.6601)	
training:	Epoch: [3][75/204]	Loss 0.7105 (0.6608)	
training:	Epoch: [3][76/204]	Loss 0.6710 (0.6609)	
training:	Epoch: [3][77/204]	Loss 0.6399 (0.6606)	
training:	Epoch: [3][78/204]	Loss 0.7054 (0.6612)	
training:	Epoch: [3][79/204]	Loss 0.6768 (0.6614)	
training:	Epoch: [3][80/204]	Loss 0.6159 (0.6608)	
training:	Epoch: [3][81/204]	Loss 0.6544 (0.6608)	
training:	Epoch: [3][82/204]	Loss 0.6859 (0.6611)	
