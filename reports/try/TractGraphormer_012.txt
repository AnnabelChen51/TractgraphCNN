Namespace(inputDirectory='data', outputDirectory='try', DisFile='data/dis_sort_roi2dis2000.npy', task='sex', type='binary', feature=['all'], CUDA_id='0', data_id='0', dataset='Classification', norm=True, epochs=400, tensorboard=True, net_architecture='TractGraphormer', batch_size=16, rate=0.0001, weight=0.0, sched_step=200, sched_gamma=0.1, printing_frequency=1, seed=0, alpha=0, remix_kappa=0, remix_tau=0, loss='CE', sigma=0, k=22, fl=64, nh=1)
Training the 'TractGraphormer' architecture

The following parameters are used:
Batch size:	16
Number of workers:	0
Learning rate:	0.0001
Weight decay:	0.0
Scheduler steps:	200
Scheduler gamma:	0.1
Number of epochs of training:	400
Number of input channels:	1

Data preparation
Reading data from:	./data
Training set size:	6538
Validation set size:	1869

Performing calculations on:	cuda:0

Pretraining:	Epoch 1/400
----------
training:	Epoch: [1][1/408]	Loss 0.7194 (0.7194)	
training:	Epoch: [1][2/408]	Loss 0.6989 (0.7091)	
training:	Epoch: [1][3/408]	Loss 0.7855 (0.7346)	
training:	Epoch: [1][4/408]	Loss 0.6988 (0.7257)	
training:	Epoch: [1][5/408]	Loss 0.6768 (0.7159)	
training:	Epoch: [1][6/408]	Loss 0.9696 (0.7582)	
training:	Epoch: [1][7/408]	Loss 0.9284 (0.7825)	
training:	Epoch: [1][8/408]	Loss 0.7487 (0.7783)	
training:	Epoch: [1][9/408]	Loss 0.6123 (0.7598)	
training:	Epoch: [1][10/408]	Loss 0.7266 (0.7565)	
training:	Epoch: [1][11/408]	Loss 0.7345 (0.7545)	
training:	Epoch: [1][12/408]	Loss 0.7253 (0.7521)	
training:	Epoch: [1][13/408]	Loss 0.5652 (0.7377)	
training:	Epoch: [1][14/408]	Loss 0.7712 (0.7401)	
training:	Epoch: [1][15/408]	Loss 0.6152 (0.7318)	
training:	Epoch: [1][16/408]	Loss 0.8749 (0.7407)	
training:	Epoch: [1][17/408]	Loss 0.3901 (0.7201)	
training:	Epoch: [1][18/408]	Loss 0.5726 (0.7119)	
training:	Epoch: [1][19/408]	Loss 0.6526 (0.7088)	
training:	Epoch: [1][20/408]	Loss 0.7336 (0.7100)	
training:	Epoch: [1][21/408]	Loss 0.5319 (0.7015)	
training:	Epoch: [1][22/408]	Loss 0.7867 (0.7054)	
training:	Epoch: [1][23/408]	Loss 0.6282 (0.7021)	
training:	Epoch: [1][24/408]	Loss 0.7204 (0.7028)	
training:	Epoch: [1][25/408]	Loss 0.5013 (0.6948)	
training:	Epoch: [1][26/408]	Loss 0.5911 (0.6908)	
training:	Epoch: [1][27/408]	Loss 0.6610 (0.6897)	
training:	Epoch: [1][28/408]	Loss 0.6821 (0.6894)	
training:	Epoch: [1][29/408]	Loss 0.8526 (0.6950)	
training:	Epoch: [1][30/408]	Loss 0.5256 (0.6894)	
training:	Epoch: [1][31/408]	Loss 0.5627 (0.6853)	
training:	Epoch: [1][32/408]	Loss 0.4971 (0.6794)	
training:	Epoch: [1][33/408]	Loss 0.6481 (0.6785)	
training:	Epoch: [1][34/408]	Loss 0.4838 (0.6727)	
training:	Epoch: [1][35/408]	Loss 0.6088 (0.6709)	
training:	Epoch: [1][36/408]	Loss 0.4647 (0.6652)	
training:	Epoch: [1][37/408]	Loss 0.7738 (0.6681)	
training:	Epoch: [1][38/408]	Loss 0.4806 (0.6632)	
training:	Epoch: [1][39/408]	Loss 0.7086 (0.6643)	
training:	Epoch: [1][40/408]	Loss 0.4978 (0.6602)	
training:	Epoch: [1][41/408]	Loss 0.5200 (0.6568)	
training:	Epoch: [1][42/408]	Loss 0.6205 (0.6559)	
training:	Epoch: [1][43/408]	Loss 0.5365 (0.6531)	
training:	Epoch: [1][44/408]	Loss 0.4616 (0.6488)	
training:	Epoch: [1][45/408]	Loss 0.4188 (0.6437)	
training:	Epoch: [1][46/408]	Loss 0.6023 (0.6428)	
training:	Epoch: [1][47/408]	Loss 0.6221 (0.6423)	
training:	Epoch: [1][48/408]	Loss 0.5782 (0.6410)	
training:	Epoch: [1][49/408]	Loss 0.5063 (0.6382)	
training:	Epoch: [1][50/408]	Loss 0.6420 (0.6383)	
training:	Epoch: [1][51/408]	Loss 0.7977 (0.6414)	
training:	Epoch: [1][52/408]	Loss 0.4580 (0.6379)	
training:	Epoch: [1][53/408]	Loss 0.5762 (0.6367)	
training:	Epoch: [1][54/408]	Loss 0.4002 (0.6324)	
training:	Epoch: [1][55/408]	Loss 0.5201 (0.6303)	
training:	Epoch: [1][56/408]	Loss 0.4799 (0.6276)	
training:	Epoch: [1][57/408]	Loss 0.4642 (0.6248)	
training:	Epoch: [1][58/408]	Loss 0.4244 (0.6213)	
training:	Epoch: [1][59/408]	Loss 0.6189 (0.6213)	
training:	Epoch: [1][60/408]	Loss 0.4383 (0.6182)	
training:	Epoch: [1][61/408]	Loss 0.5368 (0.6169)	
training:	Epoch: [1][62/408]	Loss 0.5252 (0.6154)	
training:	Epoch: [1][63/408]	Loss 0.4271 (0.6124)	
training:	Epoch: [1][64/408]	Loss 0.4273 (0.6095)	
training:	Epoch: [1][65/408]	Loss 0.2671 (0.6043)	
training:	Epoch: [1][66/408]	Loss 0.5754 (0.6038)	
training:	Epoch: [1][67/408]	Loss 0.5894 (0.6036)	
training:	Epoch: [1][68/408]	Loss 0.5280 (0.6025)	
training:	Epoch: [1][69/408]	Loss 0.5753 (0.6021)	
training:	Epoch: [1][70/408]	Loss 0.6258 (0.6024)	
training:	Epoch: [1][71/408]	Loss 0.4120 (0.5998)	
training:	Epoch: [1][72/408]	Loss 0.5239 (0.5987)	
training:	Epoch: [1][73/408]	Loss 0.5486 (0.5980)	
training:	Epoch: [1][74/408]	Loss 0.6420 (0.5986)	
training:	Epoch: [1][75/408]	Loss 0.6041 (0.5987)	
training:	Epoch: [1][76/408]	Loss 0.3399 (0.5953)	
training:	Epoch: [1][77/408]	Loss 0.3895 (0.5926)	
training:	Epoch: [1][78/408]	Loss 0.4877 (0.5913)	
training:	Epoch: [1][79/408]	Loss 0.5827 (0.5912)	
training:	Epoch: [1][80/408]	Loss 0.9032 (0.5951)	
training:	Epoch: [1][81/408]	Loss 0.4309 (0.5930)	
training:	Epoch: [1][82/408]	Loss 0.4308 (0.5911)	
training:	Epoch: [1][83/408]	Loss 0.3672 (0.5884)	
training:	Epoch: [1][84/408]	Loss 0.5272 (0.5876)	
training:	Epoch: [1][85/408]	Loss 0.7268 (0.5893)	
training:	Epoch: [1][86/408]	Loss 0.7394 (0.5910)	
training:	Epoch: [1][87/408]	Loss 0.4729 (0.5897)	
training:	Epoch: [1][88/408]	Loss 0.5785 (0.5895)	
training:	Epoch: [1][89/408]	Loss 0.5088 (0.5886)	
training:	Epoch: [1][90/408]	Loss 0.4727 (0.5873)	
training:	Epoch: [1][91/408]	Loss 0.4889 (0.5863)	
training:	Epoch: [1][92/408]	Loss 0.6427 (0.5869)	
training:	Epoch: [1][93/408]	Loss 0.6327 (0.5874)	
training:	Epoch: [1][94/408]	Loss 0.5022 (0.5865)	
training:	Epoch: [1][95/408]	Loss 0.7687 (0.5884)	
training:	Epoch: [1][96/408]	Loss 0.3960 (0.5864)	
training:	Epoch: [1][97/408]	Loss 0.4131 (0.5846)	
training:	Epoch: [1][98/408]	Loss 0.4172 (0.5829)	
training:	Epoch: [1][99/408]	Loss 0.6296 (0.5833)	
training:	Epoch: [1][100/408]	Loss 0.3522 (0.5810)	
training:	Epoch: [1][101/408]	Loss 0.4627 (0.5799)	
training:	Epoch: [1][102/408]	Loss 0.5271 (0.5793)	
training:	Epoch: [1][103/408]	Loss 0.5858 (0.5794)	
training:	Epoch: [1][104/408]	Loss 0.3880 (0.5776)	
