Namespace(inputDirectory='data', outputDirectory='try', DisFile='data/dis_sort_roi2dis2000.npy', task='sex', type='binary', feature=['Nos', 'FA1'], CUDA_id='0', data_id='0', dataset='Classification', norm=True, channels=1, epochs=200, tensorboard=True, net_architecture='DGCNN', batch_size=32, rate=1e-06, weight=0.0, sched_step=100, sched_gamma=0.1, printing_frequency=1, seed=1, alpha=0, remix_kappa=0, remix_tau=0, loss='CE', sigma=0, k=20)
Training the 'DGCNN' architecture

The following parameters are used:
Batch size:	32
Number of workers:	0
Learning rate:	1e-06
Weight decay:	0.0
Scheduler steps:	100
Scheduler gamma:	0.1
Number of epochs of training:	200
Number of input channels:	1

Data preparation
Reading data from:	./data
Training set size:	7473
Validation set size:	1869

Performing calculations on:	cuda:0

Pretraining:	Epoch 1/200
----------
training:	Epoch: [1][1/233]	Loss 0.7859 (0.7859)	
training:	Epoch: [1][2/233]	Loss 0.7052 (0.7455)	
training:	Epoch: [1][3/233]	Loss 0.6361 (0.7091)	
training:	Epoch: [1][4/233]	Loss 0.6671 (0.6986)	
training:	Epoch: [1][5/233]	Loss 0.7433 (0.7075)	
training:	Epoch: [1][6/233]	Loss 0.7798 (0.7196)	
training:	Epoch: [1][7/233]	Loss 0.5838 (0.7002)	
training:	Epoch: [1][8/233]	Loss 0.7248 (0.7033)	
training:	Epoch: [1][9/233]	Loss 0.7402 (0.7074)	
training:	Epoch: [1][10/233]	Loss 0.6779 (0.7044)	
training:	Epoch: [1][11/233]	Loss 0.6342 (0.6980)	
training:	Epoch: [1][12/233]	Loss 0.6973 (0.6980)	
training:	Epoch: [1][13/233]	Loss 0.8104 (0.7066)	
training:	Epoch: [1][14/233]	Loss 0.7003 (0.7062)	
training:	Epoch: [1][15/233]	Loss 0.7930 (0.7120)	
training:	Epoch: [1][16/233]	Loss 0.6213 (0.7063)	
training:	Epoch: [1][17/233]	Loss 0.7343 (0.7079)	
training:	Epoch: [1][18/233]	Loss 0.7560 (0.7106)	
training:	Epoch: [1][19/233]	Loss 0.7707 (0.7138)	
training:	Epoch: [1][20/233]	Loss 0.6172 (0.7089)	
training:	Epoch: [1][21/233]	Loss 0.7365 (0.7103)	
training:	Epoch: [1][22/233]	Loss 0.6269 (0.7065)	
training:	Epoch: [1][23/233]	Loss 0.6848 (0.7055)	
training:	Epoch: [1][24/233]	Loss 0.6303 (0.7024)	
training:	Epoch: [1][25/233]	Loss 0.6368 (0.6998)	
training:	Epoch: [1][26/233]	Loss 0.6020 (0.6960)	
training:	Epoch: [1][27/233]	Loss 0.7137 (0.6967)	
training:	Epoch: [1][28/233]	Loss 0.6212 (0.6940)	
training:	Epoch: [1][29/233]	Loss 0.7490 (0.6959)	
training:	Epoch: [1][30/233]	Loss 0.7222 (0.6967)	
training:	Epoch: [1][31/233]	Loss 0.7279 (0.6977)	
training:	Epoch: [1][32/233]	Loss 0.7486 (0.6993)	
training:	Epoch: [1][33/233]	Loss 0.6384 (0.6975)	
training:	Epoch: [1][34/233]	Loss 0.6384 (0.6958)	
training:	Epoch: [1][35/233]	Loss 0.7654 (0.6977)	
training:	Epoch: [1][36/233]	Loss 0.5948 (0.6949)	
training:	Epoch: [1][37/233]	Loss 0.5930 (0.6921)	
training:	Epoch: [1][38/233]	Loss 0.6631 (0.6914)	
training:	Epoch: [1][39/233]	Loss 0.6396 (0.6900)	
training:	Epoch: [1][40/233]	Loss 0.6868 (0.6900)	
training:	Epoch: [1][41/233]	Loss 0.7500 (0.6914)	
training:	Epoch: [1][42/233]	Loss 0.6062 (0.6894)	
training:	Epoch: [1][43/233]	Loss 0.6676 (0.6889)	
training:	Epoch: [1][44/233]	Loss 0.6878 (0.6889)	
training:	Epoch: [1][45/233]	Loss 0.6940 (0.6890)	
training:	Epoch: [1][46/233]	Loss 0.6516 (0.6882)	
training:	Epoch: [1][47/233]	Loss 0.6208 (0.6867)	
training:	Epoch: [1][48/233]	Loss 0.6623 (0.6862)	
training:	Epoch: [1][49/233]	Loss 0.6966 (0.6864)	
training:	Epoch: [1][50/233]	Loss 0.6897 (0.6865)	
training:	Epoch: [1][51/233]	Loss 0.7606 (0.6880)	
training:	Epoch: [1][52/233]	Loss 0.5626 (0.6855)	
training:	Epoch: [1][53/233]	Loss 0.6351 (0.6846)	
training:	Epoch: [1][54/233]	Loss 0.5605 (0.6823)	
training:	Epoch: [1][55/233]	Loss 0.5754 (0.6803)	
training:	Epoch: [1][56/233]	Loss 0.6097 (0.6791)	
training:	Epoch: [1][57/233]	Loss 0.5604 (0.6770)	
training:	Epoch: [1][58/233]	Loss 0.6104 (0.6759)	
training:	Epoch: [1][59/233]	Loss 0.6069 (0.6747)	
training:	Epoch: [1][60/233]	Loss 0.6954 (0.6750)	
training:	Epoch: [1][61/233]	Loss 0.6173 (0.6741)	
training:	Epoch: [1][62/233]	Loss 0.6055 (0.6730)	
training:	Epoch: [1][63/233]	Loss 0.5883 (0.6716)	
training:	Epoch: [1][64/233]	Loss 0.7749 (0.6732)	
training:	Epoch: [1][65/233]	Loss 0.6369 (0.6727)	
training:	Epoch: [1][66/233]	Loss 0.5666 (0.6711)	
training:	Epoch: [1][67/233]	Loss 0.5675 (0.6695)	
training:	Epoch: [1][68/233]	Loss 0.5591 (0.6679)	
training:	Epoch: [1][69/233]	Loss 0.5993 (0.6669)	
training:	Epoch: [1][70/233]	Loss 0.7386 (0.6679)	
training:	Epoch: [1][71/233]	Loss 0.6243 (0.6673)	
training:	Epoch: [1][72/233]	Loss 0.7503 (0.6685)	
training:	Epoch: [1][73/233]	Loss 0.5919 (0.6674)	
training:	Epoch: [1][74/233]	Loss 0.6038 (0.6666)	
training:	Epoch: [1][75/233]	Loss 0.5352 (0.6648)	
training:	Epoch: [1][76/233]	Loss 0.6341 (0.6644)	
training:	Epoch: [1][77/233]	Loss 0.6776 (0.6646)	
training:	Epoch: [1][78/233]	Loss 0.6270 (0.6641)	
training:	Epoch: [1][79/233]	Loss 0.7224 (0.6648)	
training:	Epoch: [1][80/233]	Loss 0.5916 (0.6639)	
training:	Epoch: [1][81/233]	Loss 0.5892 (0.6630)	
training:	Epoch: [1][82/233]	Loss 0.7009 (0.6635)	
training:	Epoch: [1][83/233]	Loss 0.6559 (0.6634)	
training:	Epoch: [1][84/233]	Loss 0.5892 (0.6625)	
training:	Epoch: [1][85/233]	Loss 0.5976 (0.6617)	
training:	Epoch: [1][86/233]	Loss 0.5421 (0.6603)	
training:	Epoch: [1][87/233]	Loss 0.6662 (0.6604)	
training:	Epoch: [1][88/233]	Loss 0.6937 (0.6608)	
training:	Epoch: [1][89/233]	Loss 0.6757 (0.6609)	
training:	Epoch: [1][90/233]	Loss 0.5712 (0.6600)	
training:	Epoch: [1][91/233]	Loss 0.6346 (0.6597)	
training:	Epoch: [1][92/233]	Loss 0.6870 (0.6600)	
training:	Epoch: [1][93/233]	Loss 0.6065 (0.6594)	
training:	Epoch: [1][94/233]	Loss 0.6967 (0.6598)	
training:	Epoch: [1][95/233]	Loss 0.6349 (0.6595)	
training:	Epoch: [1][96/233]	Loss 0.5963 (0.6589)	
training:	Epoch: [1][97/233]	Loss 0.7273 (0.6596)	
training:	Epoch: [1][98/233]	Loss 0.5853 (0.6588)	
training:	Epoch: [1][99/233]	Loss 0.5893 (0.6581)	
training:	Epoch: [1][100/233]	Loss 0.5812 (0.6573)	
training:	Epoch: [1][101/233]	Loss 0.5505 (0.6563)	
training:	Epoch: [1][102/233]	Loss 0.7412 (0.6571)	
training:	Epoch: [1][103/233]	Loss 0.6454 (0.6570)	
training:	Epoch: [1][104/233]	Loss 0.6635 (0.6571)	
training:	Epoch: [1][105/233]	Loss 0.5965 (0.6565)	
training:	Epoch: [1][106/233]	Loss 0.7336 (0.6572)	
training:	Epoch: [1][107/233]	Loss 0.7107 (0.6577)	
training:	Epoch: [1][108/233]	Loss 0.6956 (0.6581)	
training:	Epoch: [1][109/233]	Loss 0.6083 (0.6576)	
training:	Epoch: [1][110/233]	Loss 0.6545 (0.6576)	
training:	Epoch: [1][111/233]	Loss 0.4516 (0.6557)	
training:	Epoch: [1][112/233]	Loss 0.6185 (0.6554)	
training:	Epoch: [1][113/233]	Loss 0.6991 (0.6558)	
training:	Epoch: [1][114/233]	Loss 0.5766 (0.6551)	
training:	Epoch: [1][115/233]	Loss 0.5663 (0.6543)	
training:	Epoch: [1][116/233]	Loss 0.6861 (0.6546)	
training:	Epoch: [1][117/233]	Loss 0.5594 (0.6538)	
training:	Epoch: [1][118/233]	Loss 0.6634 (0.6539)	
training:	Epoch: [1][119/233]	Loss 0.6195 (0.6536)	
training:	Epoch: [1][120/233]	Loss 0.7572 (0.6544)	
training:	Epoch: [1][121/233]	Loss 0.6296 (0.6542)	
training:	Epoch: [1][122/233]	Loss 0.5618 (0.6535)	
training:	Epoch: [1][123/233]	Loss 0.5675 (0.6528)	
training:	Epoch: [1][124/233]	Loss 0.5956 (0.6523)	
training:	Epoch: [1][125/233]	Loss 0.5606 (0.6516)	
training:	Epoch: [1][126/233]	Loss 0.5631 (0.6509)	
training:	Epoch: [1][127/233]	Loss 0.7210 (0.6514)	
training:	Epoch: [1][128/233]	Loss 0.6303 (0.6513)	
training:	Epoch: [1][129/233]	Loss 0.5071 (0.6501)	
training:	Epoch: [1][130/233]	Loss 0.6764 (0.6503)	
training:	Epoch: [1][131/233]	Loss 0.5860 (0.6499)	
training:	Epoch: [1][132/233]	Loss 0.5874 (0.6494)	
training:	Epoch: [1][133/233]	Loss 0.6476 (0.6494)	
training:	Epoch: [1][134/233]	Loss 0.5940 (0.6490)	
training:	Epoch: [1][135/233]	Loss 0.7148 (0.6494)	
training:	Epoch: [1][136/233]	Loss 0.6600 (0.6495)	
training:	Epoch: [1][137/233]	Loss 0.6269 (0.6494)	
training:	Epoch: [1][138/233]	Loss 0.5170 (0.6484)	
training:	Epoch: [1][139/233]	Loss 0.6623 (0.6485)	
training:	Epoch: [1][140/233]	Loss 0.5557 (0.6478)	
training:	Epoch: [1][141/233]	Loss 0.5478 (0.6471)	
training:	Epoch: [1][142/233]	Loss 0.5432 (0.6464)	
training:	Epoch: [1][143/233]	Loss 0.5200 (0.6455)	
training:	Epoch: [1][144/233]	Loss 0.6649 (0.6456)	
training:	Epoch: [1][145/233]	Loss 0.5710 (0.6451)	
training:	Epoch: [1][146/233]	Loss 0.5504 (0.6445)	
training:	Epoch: [1][147/233]	Loss 0.4932 (0.6435)	
training:	Epoch: [1][148/233]	Loss 0.6328 (0.6434)	
training:	Epoch: [1][149/233]	Loss 0.6126 (0.6432)	
training:	Epoch: [1][150/233]	Loss 0.5743 (0.6427)	
training:	Epoch: [1][151/233]	Loss 0.6364 (0.6427)	
training:	Epoch: [1][152/233]	Loss 0.6330 (0.6426)	
training:	Epoch: [1][153/233]	Loss 0.6805 (0.6429)	
training:	Epoch: [1][154/233]	Loss 0.4924 (0.6419)	
training:	Epoch: [1][155/233]	Loss 0.6031 (0.6416)	
training:	Epoch: [1][156/233]	Loss 0.6733 (0.6418)	
training:	Epoch: [1][157/233]	Loss 0.5381 (0.6412)	
training:	Epoch: [1][158/233]	Loss 0.5498 (0.6406)	
training:	Epoch: [1][159/233]	Loss 0.5723 (0.6402)	
training:	Epoch: [1][160/233]	Loss 0.6762 (0.6404)	
training:	Epoch: [1][161/233]	Loss 0.6576 (0.6405)	
training:	Epoch: [1][162/233]	Loss 0.5905 (0.6402)	
training:	Epoch: [1][163/233]	Loss 0.5297 (0.6395)	
training:	Epoch: [1][164/233]	Loss 0.5666 (0.6391)	
training:	Epoch: [1][165/233]	Loss 0.6015 (0.6388)	
training:	Epoch: [1][166/233]	Loss 0.5632 (0.6384)	
training:	Epoch: [1][167/233]	Loss 0.5671 (0.6380)	
training:	Epoch: [1][168/233]	Loss 0.5191 (0.6372)	
training:	Epoch: [1][169/233]	Loss 0.5371 (0.6367)	
training:	Epoch: [1][170/233]	Loss 0.5842 (0.6363)	
training:	Epoch: [1][171/233]	Loss 0.5472 (0.6358)	
training:	Epoch: [1][172/233]	Loss 0.7154 (0.6363)	
training:	Epoch: [1][173/233]	Loss 0.6058 (0.6361)	
training:	Epoch: [1][174/233]	Loss 0.5426 (0.6356)	
training:	Epoch: [1][175/233]	Loss 0.5695 (0.6352)	
training:	Epoch: [1][176/233]	Loss 0.6747 (0.6354)	
training:	Epoch: [1][177/233]	Loss 0.6134 (0.6353)	
training:	Epoch: [1][178/233]	Loss 0.6070 (0.6351)	
training:	Epoch: [1][179/233]	Loss 0.5642 (0.6347)	
training:	Epoch: [1][180/233]	Loss 0.6112 (0.6346)	
training:	Epoch: [1][181/233]	Loss 0.6623 (0.6348)	
training:	Epoch: [1][182/233]	Loss 0.5834 (0.6345)	
training:	Epoch: [1][183/233]	Loss 0.6945 (0.6348)	
training:	Epoch: [1][184/233]	Loss 0.5148 (0.6342)	
training:	Epoch: [1][185/233]	Loss 0.4370 (0.6331)	
training:	Epoch: [1][186/233]	Loss 0.5552 (0.6327)	
training:	Epoch: [1][187/233]	Loss 0.6596 (0.6328)	
training:	Epoch: [1][188/233]	Loss 0.6959 (0.6332)	
training:	Epoch: [1][189/233]	Loss 0.5386 (0.6327)	
training:	Epoch: [1][190/233]	Loss 0.5685 (0.6323)	
training:	Epoch: [1][191/233]	Loss 0.5396 (0.6318)	
training:	Epoch: [1][192/233]	Loss 0.6150 (0.6317)	
training:	Epoch: [1][193/233]	Loss 0.5490 (0.6313)	
training:	Epoch: [1][194/233]	Loss 0.5632 (0.6310)	
training:	Epoch: [1][195/233]	Loss 0.5383 (0.6305)	
training:	Epoch: [1][196/233]	Loss 0.8001 (0.6314)	
training:	Epoch: [1][197/233]	Loss 0.6278 (0.6313)	
training:	Epoch: [1][198/233]	Loss 0.4983 (0.6307)	
training:	Epoch: [1][199/233]	Loss 0.5319 (0.6302)	
training:	Epoch: [1][200/233]	Loss 0.6751 (0.6304)	
training:	Epoch: [1][201/233]	Loss 0.5392 (0.6299)	
training:	Epoch: [1][202/233]	Loss 0.5915 (0.6297)	
training:	Epoch: [1][203/233]	Loss 0.4875 (0.6290)	
training:	Epoch: [1][204/233]	Loss 0.5398 (0.6286)	
training:	Epoch: [1][205/233]	Loss 0.5126 (0.6280)	
training:	Epoch: [1][206/233]	Loss 0.5659 (0.6277)	
training:	Epoch: [1][207/233]	Loss 0.7036 (0.6281)	
training:	Epoch: [1][208/233]	Loss 0.5336 (0.6277)	
training:	Epoch: [1][209/233]	Loss 0.6457 (0.6277)	
training:	Epoch: [1][210/233]	Loss 0.5453 (0.6273)	
training:	Epoch: [1][211/233]	Loss 0.5899 (0.6272)	
training:	Epoch: [1][212/233]	Loss 0.6203 (0.6271)	
training:	Epoch: [1][213/233]	Loss 0.5085 (0.6266)	
training:	Epoch: [1][214/233]	Loss 0.6464 (0.6267)	
training:	Epoch: [1][215/233]	Loss 0.6292 (0.6267)	
training:	Epoch: [1][216/233]	Loss 0.6053 (0.6266)	
training:	Epoch: [1][217/233]	Loss 0.5348 (0.6262)	
training:	Epoch: [1][218/233]	Loss 0.5520 (0.6258)	
training:	Epoch: [1][219/233]	Loss 0.6247 (0.6258)	
training:	Epoch: [1][220/233]	Loss 0.4982 (0.6252)	
training:	Epoch: [1][221/233]	Loss 0.4122 (0.6243)	
training:	Epoch: [1][222/233]	Loss 0.6888 (0.6246)	
training:	Epoch: [1][223/233]	Loss 0.5244 (0.6241)	
training:	Epoch: [1][224/233]	Loss 0.6372 (0.6242)	
training:	Epoch: [1][225/233]	Loss 0.5726 (0.6239)	
training:	Epoch: [1][226/233]	Loss 0.6511 (0.6241)	
training:	Epoch: [1][227/233]	Loss 0.5350 (0.6237)	
training:	Epoch: [1][228/233]	Loss 0.6294 (0.6237)	
training:	Epoch: [1][229/233]	Loss 0.5045 (0.6232)	
training:	Epoch: [1][230/233]	Loss 0.7834 (0.6239)	
training:	Epoch: [1][231/233]	Loss 0.5578 (0.6236)	
training:	Epoch: [1][232/233]	Loss 0.5372 (0.6232)	
training:	Epoch: [1][233/233]	Loss 0.6589 (0.6234)	
Training:	 Loss: 0.6219

Training:	 ACC: 0.7195 0.7267 0.8834 0.5556
Validation:	 ACC: 0.7024 0.7105 0.8813 0.5235
Validation:	 Best_BACC: 0.7024 0.7105 0.8813 0.5235
Validation:	 Loss: 0.5671
Pretraining:	Epoch 2/200
----------
training:	Epoch: [2][1/233]	Loss 0.5508 (0.5508)	
training:	Epoch: [2][2/233]	Loss 0.6868 (0.6188)	
training:	Epoch: [2][3/233]	Loss 0.5588 (0.5988)	
training:	Epoch: [2][4/233]	Loss 0.5802 (0.5942)	
training:	Epoch: [2][5/233]	Loss 0.4865 (0.5726)	
training:	Epoch: [2][6/233]	Loss 0.4416 (0.5508)	
training:	Epoch: [2][7/233]	Loss 0.5868 (0.5559)	
training:	Epoch: [2][8/233]	Loss 0.4843 (0.5470)	
training:	Epoch: [2][9/233]	Loss 0.4507 (0.5363)	
training:	Epoch: [2][10/233]	Loss 0.5459 (0.5373)	
training:	Epoch: [2][11/233]	Loss 0.5499 (0.5384)	
training:	Epoch: [2][12/233]	Loss 0.4450 (0.5306)	
training:	Epoch: [2][13/233]	Loss 0.5918 (0.5353)	
training:	Epoch: [2][14/233]	Loss 0.5986 (0.5398)	
training:	Epoch: [2][15/233]	Loss 0.5053 (0.5375)	
training:	Epoch: [2][16/233]	Loss 0.6100 (0.5421)	
training:	Epoch: [2][17/233]	Loss 0.4212 (0.5350)	
training:	Epoch: [2][18/233]	Loss 0.4756 (0.5317)	
training:	Epoch: [2][19/233]	Loss 0.5771 (0.5341)	
training:	Epoch: [2][20/233]	Loss 0.4989 (0.5323)	
training:	Epoch: [2][21/233]	Loss 0.5199 (0.5317)	
training:	Epoch: [2][22/233]	Loss 0.6010 (0.5349)	
training:	Epoch: [2][23/233]	Loss 0.5014 (0.5334)	
training:	Epoch: [2][24/233]	Loss 0.5713 (0.5350)	
training:	Epoch: [2][25/233]	Loss 0.4435 (0.5313)	
training:	Epoch: [2][26/233]	Loss 0.6162 (0.5346)	
training:	Epoch: [2][27/233]	Loss 0.6668 (0.5395)	
training:	Epoch: [2][28/233]	Loss 0.4221 (0.5353)	
training:	Epoch: [2][29/233]	Loss 0.5572 (0.5360)	
training:	Epoch: [2][30/233]	Loss 0.5880 (0.5378)	
training:	Epoch: [2][31/233]	Loss 0.5293 (0.5375)	
training:	Epoch: [2][32/233]	Loss 0.5829 (0.5389)	
training:	Epoch: [2][33/233]	Loss 0.4614 (0.5366)	
training:	Epoch: [2][34/233]	Loss 0.4152 (0.5330)	
training:	Epoch: [2][35/233]	Loss 0.6517 (0.5364)	
training:	Epoch: [2][36/233]	Loss 0.4710 (0.5346)	
training:	Epoch: [2][37/233]	Loss 0.4594 (0.5325)	
training:	Epoch: [2][38/233]	Loss 0.4938 (0.5315)	
training:	Epoch: [2][39/233]	Loss 0.5964 (0.5332)	
training:	Epoch: [2][40/233]	Loss 0.5826 (0.5344)	
training:	Epoch: [2][41/233]	Loss 0.4349 (0.5320)	
training:	Epoch: [2][42/233]	Loss 0.4964 (0.5312)	
training:	Epoch: [2][43/233]	Loss 0.4655 (0.5296)	
training:	Epoch: [2][44/233]	Loss 0.4633 (0.5281)	
training:	Epoch: [2][45/233]	Loss 0.5187 (0.5279)	
training:	Epoch: [2][46/233]	Loss 0.3912 (0.5249)	
training:	Epoch: [2][47/233]	Loss 0.4633 (0.5236)	
training:	Epoch: [2][48/233]	Loss 0.5682 (0.5246)	
training:	Epoch: [2][49/233]	Loss 0.5292 (0.5246)	
training:	Epoch: [2][50/233]	Loss 0.4689 (0.5235)	
training:	Epoch: [2][51/233]	Loss 0.5386 (0.5238)	
training:	Epoch: [2][52/233]	Loss 0.4512 (0.5224)	
training:	Epoch: [2][53/233]	Loss 0.5277 (0.5225)	
training:	Epoch: [2][54/233]	Loss 0.6702 (0.5253)	
training:	Epoch: [2][55/233]	Loss 0.6260 (0.5271)	
training:	Epoch: [2][56/233]	Loss 0.5226 (0.5270)	
training:	Epoch: [2][57/233]	Loss 0.6776 (0.5297)	
training:	Epoch: [2][58/233]	Loss 0.4742 (0.5287)	
training:	Epoch: [2][59/233]	Loss 0.4851 (0.5280)	
training:	Epoch: [2][60/233]	Loss 0.6238 (0.5296)	
training:	Epoch: [2][61/233]	Loss 0.4799 (0.5287)	
training:	Epoch: [2][62/233]	Loss 0.5153 (0.5285)	
training:	Epoch: [2][63/233]	Loss 0.5230 (0.5284)	
training:	Epoch: [2][64/233]	Loss 0.5378 (0.5286)	
training:	Epoch: [2][65/233]	Loss 0.6029 (0.5297)	
training:	Epoch: [2][66/233]	Loss 0.6991 (0.5323)	
training:	Epoch: [2][67/233]	Loss 0.6170 (0.5336)	
training:	Epoch: [2][68/233]	Loss 0.4048 (0.5317)	
training:	Epoch: [2][69/233]	Loss 0.4798 (0.5309)	
training:	Epoch: [2][70/233]	Loss 0.6083 (0.5320)	
training:	Epoch: [2][71/233]	Loss 0.3848 (0.5299)	
training:	Epoch: [2][72/233]	Loss 0.4182 (0.5284)	
training:	Epoch: [2][73/233]	Loss 0.4714 (0.5276)	
training:	Epoch: [2][74/233]	Loss 0.4847 (0.5270)	
training:	Epoch: [2][75/233]	Loss 0.5595 (0.5275)	
training:	Epoch: [2][76/233]	Loss 0.7189 (0.5300)	
training:	Epoch: [2][77/233]	Loss 0.5710 (0.5305)	
training:	Epoch: [2][78/233]	Loss 0.6172 (0.5316)	
training:	Epoch: [2][79/233]	Loss 0.4266 (0.5303)	
training:	Epoch: [2][80/233]	Loss 0.4803 (0.5297)	
training:	Epoch: [2][81/233]	Loss 0.5895 (0.5304)	
training:	Epoch: [2][82/233]	Loss 0.6007 (0.5313)	
training:	Epoch: [2][83/233]	Loss 0.5379 (0.5314)	
training:	Epoch: [2][84/233]	Loss 0.5266 (0.5313)	
training:	Epoch: [2][85/233]	Loss 0.5275 (0.5313)	
training:	Epoch: [2][86/233]	Loss 0.5936 (0.5320)	
training:	Epoch: [2][87/233]	Loss 0.6405 (0.5332)	
training:	Epoch: [2][88/233]	Loss 0.5456 (0.5334)	
training:	Epoch: [2][89/233]	Loss 0.6001 (0.5341)	
training:	Epoch: [2][90/233]	Loss 0.5872 (0.5347)	
training:	Epoch: [2][91/233]	Loss 0.5441 (0.5348)	
training:	Epoch: [2][92/233]	Loss 0.5200 (0.5346)	
training:	Epoch: [2][93/233]	Loss 0.5558 (0.5349)	
training:	Epoch: [2][94/233]	Loss 0.4995 (0.5345)	
training:	Epoch: [2][95/233]	Loss 0.4776 (0.5339)	
training:	Epoch: [2][96/233]	Loss 0.5780 (0.5344)	
training:	Epoch: [2][97/233]	Loss 0.4941 (0.5339)	
training:	Epoch: [2][98/233]	Loss 0.4038 (0.5326)	
training:	Epoch: [2][99/233]	Loss 0.5063 (0.5323)	
training:	Epoch: [2][100/233]	Loss 0.4703 (0.5317)	
training:	Epoch: [2][101/233]	Loss 0.4435 (0.5309)	
training:	Epoch: [2][102/233]	Loss 0.4852 (0.5304)	
training:	Epoch: [2][103/233]	Loss 0.5208 (0.5303)	
training:	Epoch: [2][104/233]	Loss 0.5714 (0.5307)	
training:	Epoch: [2][105/233]	Loss 0.5545 (0.5309)	
training:	Epoch: [2][106/233]	Loss 0.4721 (0.5304)	
training:	Epoch: [2][107/233]	Loss 0.5827 (0.5309)	
training:	Epoch: [2][108/233]	Loss 0.5441 (0.5310)	
training:	Epoch: [2][109/233]	Loss 0.5217 (0.5309)	
training:	Epoch: [2][110/233]	Loss 0.6759 (0.5322)	
training:	Epoch: [2][111/233]	Loss 0.4927 (0.5319)	
training:	Epoch: [2][112/233]	Loss 0.4063 (0.5307)	
training:	Epoch: [2][113/233]	Loss 0.4883 (0.5304)	
training:	Epoch: [2][114/233]	Loss 0.5275 (0.5303)	
training:	Epoch: [2][115/233]	Loss 0.7034 (0.5319)	
training:	Epoch: [2][116/233]	Loss 0.5076 (0.5316)	
training:	Epoch: [2][117/233]	Loss 0.4966 (0.5313)	
training:	Epoch: [2][118/233]	Loss 0.4448 (0.5306)	
training:	Epoch: [2][119/233]	Loss 0.5374 (0.5307)	
training:	Epoch: [2][120/233]	Loss 0.4863 (0.5303)	
training:	Epoch: [2][121/233]	Loss 0.5252 (0.5303)	
training:	Epoch: [2][122/233]	Loss 0.5965 (0.5308)	
training:	Epoch: [2][123/233]	Loss 0.5047 (0.5306)	
training:	Epoch: [2][124/233]	Loss 0.5827 (0.5310)	
training:	Epoch: [2][125/233]	Loss 0.4576 (0.5304)	
training:	Epoch: [2][126/233]	Loss 0.4861 (0.5301)	
training:	Epoch: [2][127/233]	Loss 0.4937 (0.5298)	
training:	Epoch: [2][128/233]	Loss 0.5493 (0.5299)	
training:	Epoch: [2][129/233]	Loss 0.4638 (0.5294)	
training:	Epoch: [2][130/233]	Loss 0.4908 (0.5291)	
training:	Epoch: [2][131/233]	Loss 0.4937 (0.5289)	
training:	Epoch: [2][132/233]	Loss 0.5219 (0.5288)	
training:	Epoch: [2][133/233]	Loss 0.4547 (0.5282)	
training:	Epoch: [2][134/233]	Loss 0.4854 (0.5279)	
training:	Epoch: [2][135/233]	Loss 0.5433 (0.5280)	
training:	Epoch: [2][136/233]	Loss 0.5873 (0.5285)	
training:	Epoch: [2][137/233]	Loss 0.5441 (0.5286)	
training:	Epoch: [2][138/233]	Loss 0.5085 (0.5284)	
training:	Epoch: [2][139/233]	Loss 0.4680 (0.5280)	
training:	Epoch: [2][140/233]	Loss 0.4810 (0.5277)	
training:	Epoch: [2][141/233]	Loss 0.6262 (0.5284)	
training:	Epoch: [2][142/233]	Loss 0.4361 (0.5277)	
training:	Epoch: [2][143/233]	Loss 0.5253 (0.5277)	
training:	Epoch: [2][144/233]	Loss 0.4441 (0.5271)	
training:	Epoch: [2][145/233]	Loss 0.4646 (0.5267)	
training:	Epoch: [2][146/233]	Loss 0.5974 (0.5272)	
training:	Epoch: [2][147/233]	Loss 0.7203 (0.5285)	
training:	Epoch: [2][148/233]	Loss 0.4950 (0.5283)	
training:	Epoch: [2][149/233]	Loss 0.3856 (0.5273)	
training:	Epoch: [2][150/233]	Loss 0.5477 (0.5274)	
training:	Epoch: [2][151/233]	Loss 0.5901 (0.5279)	
training:	Epoch: [2][152/233]	Loss 0.5835 (0.5282)	
training:	Epoch: [2][153/233]	Loss 0.4553 (0.5277)	
training:	Epoch: [2][154/233]	Loss 0.3673 (0.5267)	
training:	Epoch: [2][155/233]	Loss 0.4650 (0.5263)	
training:	Epoch: [2][156/233]	Loss 0.5043 (0.5262)	
training:	Epoch: [2][157/233]	Loss 0.5705 (0.5264)	
training:	Epoch: [2][158/233]	Loss 0.4689 (0.5261)	
training:	Epoch: [2][159/233]	Loss 0.5826 (0.5264)	
training:	Epoch: [2][160/233]	Loss 0.5297 (0.5265)	
training:	Epoch: [2][161/233]	Loss 0.4509 (0.5260)	
training:	Epoch: [2][162/233]	Loss 0.5401 (0.5261)	
training:	Epoch: [2][163/233]	Loss 0.4692 (0.5257)	
training:	Epoch: [2][164/233]	Loss 0.5515 (0.5259)	
training:	Epoch: [2][165/233]	Loss 0.4619 (0.5255)	
training:	Epoch: [2][166/233]	Loss 0.5677 (0.5258)	
training:	Epoch: [2][167/233]	Loss 0.5252 (0.5257)	
training:	Epoch: [2][168/233]	Loss 0.4703 (0.5254)	
training:	Epoch: [2][169/233]	Loss 0.5075 (0.5253)	
training:	Epoch: [2][170/233]	Loss 0.3780 (0.5244)	
training:	Epoch: [2][171/233]	Loss 0.4349 (0.5239)	
training:	Epoch: [2][172/233]	Loss 0.5320 (0.5240)	
training:	Epoch: [2][173/233]	Loss 0.5382 (0.5241)	
training:	Epoch: [2][174/233]	Loss 0.5691 (0.5243)	
training:	Epoch: [2][175/233]	Loss 0.4947 (0.5241)	
training:	Epoch: [2][176/233]	Loss 0.5599 (0.5243)	
training:	Epoch: [2][177/233]	Loss 0.4594 (0.5240)	
training:	Epoch: [2][178/233]	Loss 0.4735 (0.5237)	
training:	Epoch: [2][179/233]	Loss 0.5377 (0.5238)	
training:	Epoch: [2][180/233]	Loss 0.5583 (0.5240)	
training:	Epoch: [2][181/233]	Loss 0.4989 (0.5238)	
training:	Epoch: [2][182/233]	Loss 0.4377 (0.5234)	
training:	Epoch: [2][183/233]	Loss 0.4935 (0.5232)	
training:	Epoch: [2][184/233]	Loss 0.4657 (0.5229)	
training:	Epoch: [2][185/233]	Loss 0.4927 (0.5227)	
training:	Epoch: [2][186/233]	Loss 0.4028 (0.5221)	
training:	Epoch: [2][187/233]	Loss 0.5912 (0.5224)	
training:	Epoch: [2][188/233]	Loss 0.4622 (0.5221)	
training:	Epoch: [2][189/233]	Loss 0.4989 (0.5220)	
training:	Epoch: [2][190/233]	Loss 0.3826 (0.5213)	
training:	Epoch: [2][191/233]	Loss 0.4522 (0.5209)	
training:	Epoch: [2][192/233]	Loss 0.5585 (0.5211)	
training:	Epoch: [2][193/233]	Loss 0.5189 (0.5211)	
training:	Epoch: [2][194/233]	Loss 0.5173 (0.5211)	
training:	Epoch: [2][195/233]	Loss 0.5441 (0.5212)	
training:	Epoch: [2][196/233]	Loss 0.4033 (0.5206)	
training:	Epoch: [2][197/233]	Loss 0.6118 (0.5210)	
training:	Epoch: [2][198/233]	Loss 0.5273 (0.5211)	
training:	Epoch: [2][199/233]	Loss 0.6651 (0.5218)	
training:	Epoch: [2][200/233]	Loss 0.4467 (0.5214)	
training:	Epoch: [2][201/233]	Loss 0.5001 (0.5213)	
training:	Epoch: [2][202/233]	Loss 0.4420 (0.5209)	
training:	Epoch: [2][203/233]	Loss 0.4268 (0.5205)	
training:	Epoch: [2][204/233]	Loss 0.4561 (0.5201)	
training:	Epoch: [2][205/233]	Loss 0.4622 (0.5199)	
training:	Epoch: [2][206/233]	Loss 0.4181 (0.5194)	
training:	Epoch: [2][207/233]	Loss 0.4185 (0.5189)	
training:	Epoch: [2][208/233]	Loss 0.4848 (0.5187)	
training:	Epoch: [2][209/233]	Loss 0.4081 (0.5182)	
training:	Epoch: [2][210/233]	Loss 0.6453 (0.5188)	
training:	Epoch: [2][211/233]	Loss 0.6670 (0.5195)	
training:	Epoch: [2][212/233]	Loss 0.5588 (0.5197)	
training:	Epoch: [2][213/233]	Loss 0.5805 (0.5200)	
training:	Epoch: [2][214/233]	Loss 0.6800 (0.5207)	
training:	Epoch: [2][215/233]	Loss 0.6509 (0.5213)	
training:	Epoch: [2][216/233]	Loss 0.4484 (0.5210)	
training:	Epoch: [2][217/233]	Loss 0.4407 (0.5206)	
training:	Epoch: [2][218/233]	Loss 0.4785 (0.5204)	
training:	Epoch: [2][219/233]	Loss 0.5649 (0.5206)	
training:	Epoch: [2][220/233]	Loss 0.4531 (0.5203)	
training:	Epoch: [2][221/233]	Loss 0.4600 (0.5200)	
training:	Epoch: [2][222/233]	Loss 0.4031 (0.5195)	
training:	Epoch: [2][223/233]	Loss 0.4233 (0.5191)	
training:	Epoch: [2][224/233]	Loss 0.3917 (0.5185)	
training:	Epoch: [2][225/233]	Loss 0.4876 (0.5184)	
training:	Epoch: [2][226/233]	Loss 0.3462 (0.5176)	
training:	Epoch: [2][227/233]	Loss 0.5195 (0.5176)	
training:	Epoch: [2][228/233]	Loss 0.5284 (0.5177)	
training:	Epoch: [2][229/233]	Loss 0.6327 (0.5182)	
training:	Epoch: [2][230/233]	Loss 0.5730 (0.5184)	
training:	Epoch: [2][231/233]	Loss 0.6433 (0.5190)	
training:	Epoch: [2][232/233]	Loss 0.4915 (0.5188)	
training:	Epoch: [2][233/233]	Loss 0.5124 (0.5188)	
Training:	 Loss: 0.5176

Training:	 ACC: 0.7829 0.7874 0.8836 0.6822
Validation:	 ACC: 0.7519 0.7571 0.8669 0.6368
Validation:	 Best_BACC: 0.7519 0.7571 0.8669 0.6368
Validation:	 Loss: 0.5162
Pretraining:	Epoch 3/200
----------
training:	Epoch: [3][1/233]	Loss 0.4921 (0.4921)	
training:	Epoch: [3][2/233]	Loss 0.4497 (0.4709)	
training:	Epoch: [3][3/233]	Loss 0.4396 (0.4605)	
training:	Epoch: [3][4/233]	Loss 0.6238 (0.5013)	
training:	Epoch: [3][5/233]	Loss 0.5763 (0.5163)	
training:	Epoch: [3][6/233]	Loss 0.4355 (0.5028)	
training:	Epoch: [3][7/233]	Loss 0.5261 (0.5062)	
training:	Epoch: [3][8/233]	Loss 0.4636 (0.5008)	
training:	Epoch: [3][9/233]	Loss 0.5188 (0.5028)	
training:	Epoch: [3][10/233]	Loss 0.4167 (0.4942)	
training:	Epoch: [3][11/233]	Loss 0.4935 (0.4942)	
training:	Epoch: [3][12/233]	Loss 0.4962 (0.4943)	
training:	Epoch: [3][13/233]	Loss 0.4608 (0.4917)	
training:	Epoch: [3][14/233]	Loss 0.4384 (0.4879)	
training:	Epoch: [3][15/233]	Loss 0.4355 (0.4844)	
training:	Epoch: [3][16/233]	Loss 0.5225 (0.4868)	
training:	Epoch: [3][17/233]	Loss 0.4394 (0.4840)	
training:	Epoch: [3][18/233]	Loss 0.4618 (0.4828)	
training:	Epoch: [3][19/233]	Loss 0.4637 (0.4818)	
training:	Epoch: [3][20/233]	Loss 0.5564 (0.4855)	
training:	Epoch: [3][21/233]	Loss 0.4362 (0.4832)	
training:	Epoch: [3][22/233]	Loss 0.5043 (0.4841)	
training:	Epoch: [3][23/233]	Loss 0.4580 (0.4830)	
training:	Epoch: [3][24/233]	Loss 0.4284 (0.4807)	
training:	Epoch: [3][25/233]	Loss 0.5244 (0.4825)	
training:	Epoch: [3][26/233]	Loss 0.4496 (0.4812)	
training:	Epoch: [3][27/233]	Loss 0.4825 (0.4812)	
training:	Epoch: [3][28/233]	Loss 0.4504 (0.4801)	
training:	Epoch: [3][29/233]	Loss 0.4716 (0.4799)	
training:	Epoch: [3][30/233]	Loss 0.5372 (0.4818)	
training:	Epoch: [3][31/233]	Loss 0.4423 (0.4805)	
training:	Epoch: [3][32/233]	Loss 0.5140 (0.4815)	
training:	Epoch: [3][33/233]	Loss 0.4070 (0.4793)	
training:	Epoch: [3][34/233]	Loss 0.4679 (0.4789)	
training:	Epoch: [3][35/233]	Loss 0.4049 (0.4768)	
training:	Epoch: [3][36/233]	Loss 0.6158 (0.4807)	
training:	Epoch: [3][37/233]	Loss 0.4034 (0.4786)	
training:	Epoch: [3][38/233]	Loss 0.4263 (0.4772)	
training:	Epoch: [3][39/233]	Loss 0.3905 (0.4750)	
training:	Epoch: [3][40/233]	Loss 0.4918 (0.4754)	
training:	Epoch: [3][41/233]	Loss 0.4219 (0.4741)	
training:	Epoch: [3][42/233]	Loss 0.5329 (0.4755)	
training:	Epoch: [3][43/233]	Loss 0.5118 (0.4764)	
training:	Epoch: [3][44/233]	Loss 0.4410 (0.4756)	
training:	Epoch: [3][45/233]	Loss 0.3977 (0.4738)	
training:	Epoch: [3][46/233]	Loss 0.4456 (0.4732)	
training:	Epoch: [3][47/233]	Loss 0.5755 (0.4754)	
training:	Epoch: [3][48/233]	Loss 0.4366 (0.4746)	
training:	Epoch: [3][49/233]	Loss 0.5306 (0.4757)	
training:	Epoch: [3][50/233]	Loss 0.4145 (0.4745)	
training:	Epoch: [3][51/233]	Loss 0.4573 (0.4742)	
training:	Epoch: [3][52/233]	Loss 0.5125 (0.4749)	
training:	Epoch: [3][53/233]	Loss 0.5395 (0.4761)	
training:	Epoch: [3][54/233]	Loss 0.4149 (0.4750)	
training:	Epoch: [3][55/233]	Loss 0.4885 (0.4752)	
training:	Epoch: [3][56/233]	Loss 0.4051 (0.4740)	
training:	Epoch: [3][57/233]	Loss 0.3724 (0.4722)	
training:	Epoch: [3][58/233]	Loss 0.4822 (0.4724)	
training:	Epoch: [3][59/233]	Loss 0.4700 (0.4723)	
training:	Epoch: [3][60/233]	Loss 0.5335 (0.4733)	
training:	Epoch: [3][61/233]	Loss 0.4923 (0.4737)	
training:	Epoch: [3][62/233]	Loss 0.4507 (0.4733)	
training:	Epoch: [3][63/233]	Loss 0.4592 (0.4731)	
training:	Epoch: [3][64/233]	Loss 0.4410 (0.4726)	
training:	Epoch: [3][65/233]	Loss 0.4705 (0.4725)	
training:	Epoch: [3][66/233]	Loss 0.4556 (0.4723)	
training:	Epoch: [3][67/233]	Loss 0.5217 (0.4730)	
training:	Epoch: [3][68/233]	Loss 0.6004 (0.4749)	
training:	Epoch: [3][69/233]	Loss 0.4851 (0.4750)	
training:	Epoch: [3][70/233]	Loss 0.4602 (0.4748)	
training:	Epoch: [3][71/233]	Loss 0.3816 (0.4735)	
training:	Epoch: [3][72/233]	Loss 0.4751 (0.4735)	
training:	Epoch: [3][73/233]	Loss 0.4671 (0.4734)	
training:	Epoch: [3][74/233]	Loss 0.5878 (0.4750)	
training:	Epoch: [3][75/233]	Loss 0.5084 (0.4754)	
training:	Epoch: [3][76/233]	Loss 0.4651 (0.4753)	
training:	Epoch: [3][77/233]	Loss 0.5455 (0.4762)	
training:	Epoch: [3][78/233]	Loss 0.4575 (0.4760)	
training:	Epoch: [3][79/233]	Loss 0.4401 (0.4755)	
training:	Epoch: [3][80/233]	Loss 0.5289 (0.4762)	
training:	Epoch: [3][81/233]	Loss 0.5956 (0.4777)	
training:	Epoch: [3][82/233]	Loss 0.4296 (0.4771)	
training:	Epoch: [3][83/233]	Loss 0.5782 (0.4783)	
training:	Epoch: [3][84/233]	Loss 0.4153 (0.4775)	
training:	Epoch: [3][85/233]	Loss 0.4934 (0.4777)	
training:	Epoch: [3][86/233]	Loss 0.4348 (0.4772)	
training:	Epoch: [3][87/233]	Loss 0.4117 (0.4765)	
training:	Epoch: [3][88/233]	Loss 0.4831 (0.4765)	
training:	Epoch: [3][89/233]	Loss 0.4816 (0.4766)	
training:	Epoch: [3][90/233]	Loss 0.4621 (0.4764)	
training:	Epoch: [3][91/233]	Loss 0.4482 (0.4761)	
training:	Epoch: [3][92/233]	Loss 0.3865 (0.4752)	
training:	Epoch: [3][93/233]	Loss 0.3273 (0.4736)	
training:	Epoch: [3][94/233]	Loss 0.3681 (0.4724)	
training:	Epoch: [3][95/233]	Loss 0.5388 (0.4731)	
training:	Epoch: [3][96/233]	Loss 0.4430 (0.4728)	
training:	Epoch: [3][97/233]	Loss 0.4377 (0.4725)	
training:	Epoch: [3][98/233]	Loss 0.3826 (0.4716)	
training:	Epoch: [3][99/233]	Loss 0.6272 (0.4731)	
training:	Epoch: [3][100/233]	Loss 0.4670 (0.4731)	
training:	Epoch: [3][101/233]	Loss 0.4035 (0.4724)	
training:	Epoch: [3][102/233]	Loss 0.4716 (0.4724)	
training:	Epoch: [3][103/233]	Loss 0.3873 (0.4715)	
training:	Epoch: [3][104/233]	Loss 0.5412 (0.4722)	
training:	Epoch: [3][105/233]	Loss 0.4053 (0.4716)	
training:	Epoch: [3][106/233]	Loss 0.4826 (0.4717)	
training:	Epoch: [3][107/233]	Loss 0.4061 (0.4711)	
training:	Epoch: [3][108/233]	Loss 0.4683 (0.4710)	
training:	Epoch: [3][109/233]	Loss 0.5248 (0.4715)	
training:	Epoch: [3][110/233]	Loss 0.5804 (0.4725)	
training:	Epoch: [3][111/233]	Loss 0.3841 (0.4717)	
training:	Epoch: [3][112/233]	Loss 0.4375 (0.4714)	
training:	Epoch: [3][113/233]	Loss 0.3970 (0.4708)	
training:	Epoch: [3][114/233]	Loss 0.4657 (0.4707)	
training:	Epoch: [3][115/233]	Loss 0.4557 (0.4706)	
training:	Epoch: [3][116/233]	Loss 0.4872 (0.4707)	
training:	Epoch: [3][117/233]	Loss 0.5604 (0.4715)	
training:	Epoch: [3][118/233]	Loss 0.5427 (0.4721)	
training:	Epoch: [3][119/233]	Loss 0.5542 (0.4728)	
training:	Epoch: [3][120/233]	Loss 0.4936 (0.4730)	
training:	Epoch: [3][121/233]	Loss 0.4600 (0.4729)	
training:	Epoch: [3][122/233]	Loss 0.4361 (0.4726)	
training:	Epoch: [3][123/233]	Loss 0.5245 (0.4730)	
training:	Epoch: [3][124/233]	Loss 0.3787 (0.4722)	
training:	Epoch: [3][125/233]	Loss 0.4830 (0.4723)	
training:	Epoch: [3][126/233]	Loss 0.4322 (0.4720)	
training:	Epoch: [3][127/233]	Loss 0.6174 (0.4731)	
training:	Epoch: [3][128/233]	Loss 0.4351 (0.4728)	
training:	Epoch: [3][129/233]	Loss 0.3807 (0.4721)	
training:	Epoch: [3][130/233]	Loss 0.3859 (0.4715)	
training:	Epoch: [3][131/233]	Loss 0.5774 (0.4723)	
training:	Epoch: [3][132/233]	Loss 0.5817 (0.4731)	
training:	Epoch: [3][133/233]	Loss 0.4647 (0.4730)	
training:	Epoch: [3][134/233]	Loss 0.5088 (0.4733)	
training:	Epoch: [3][135/233]	Loss 0.3974 (0.4727)	
training:	Epoch: [3][136/233]	Loss 0.4938 (0.4729)	
training:	Epoch: [3][137/233]	Loss 0.3293 (0.4718)	
training:	Epoch: [3][138/233]	Loss 0.5695 (0.4726)	
training:	Epoch: [3][139/233]	Loss 0.4030 (0.4721)	
training:	Epoch: [3][140/233]	Loss 0.5375 (0.4725)	
training:	Epoch: [3][141/233]	Loss 0.4781 (0.4726)	
training:	Epoch: [3][142/233]	Loss 0.4112 (0.4721)	
training:	Epoch: [3][143/233]	Loss 0.3301 (0.4711)	
training:	Epoch: [3][144/233]	Loss 0.4528 (0.4710)	
training:	Epoch: [3][145/233]	Loss 0.4341 (0.4708)	
training:	Epoch: [3][146/233]	Loss 0.4247 (0.4704)	
training:	Epoch: [3][147/233]	Loss 0.4709 (0.4704)	
training:	Epoch: [3][148/233]	Loss 0.4758 (0.4705)	
training:	Epoch: [3][149/233]	Loss 0.5982 (0.4713)	
training:	Epoch: [3][150/233]	Loss 0.4223 (0.4710)	
training:	Epoch: [3][151/233]	Loss 0.4127 (0.4706)	
training:	Epoch: [3][152/233]	Loss 0.4451 (0.4705)	
training:	Epoch: [3][153/233]	Loss 0.3519 (0.4697)	
training:	Epoch: [3][154/233]	Loss 0.5555 (0.4702)	
training:	Epoch: [3][155/233]	Loss 0.4512 (0.4701)	
training:	Epoch: [3][156/233]	Loss 0.5019 (0.4703)	
training:	Epoch: [3][157/233]	Loss 0.4517 (0.4702)	
training:	Epoch: [3][158/233]	Loss 0.4559 (0.4701)	
training:	Epoch: [3][159/233]	Loss 0.4698 (0.4701)	
training:	Epoch: [3][160/233]	Loss 0.4608 (0.4700)	
training:	Epoch: [3][161/233]	Loss 0.4576 (0.4700)	
training:	Epoch: [3][162/233]	Loss 0.4319 (0.4697)	
training:	Epoch: [3][163/233]	Loss 0.4406 (0.4696)	
training:	Epoch: [3][164/233]	Loss 0.4265 (0.4693)	
training:	Epoch: [3][165/233]	Loss 0.5267 (0.4696)	
training:	Epoch: [3][166/233]	Loss 0.4559 (0.4696)	
training:	Epoch: [3][167/233]	Loss 0.4236 (0.4693)	
training:	Epoch: [3][168/233]	Loss 0.4931 (0.4694)	
training:	Epoch: [3][169/233]	Loss 0.4941 (0.4696)	
training:	Epoch: [3][170/233]	Loss 0.4028 (0.4692)	
training:	Epoch: [3][171/233]	Loss 0.5693 (0.4698)	
training:	Epoch: [3][172/233]	Loss 0.3141 (0.4689)	
training:	Epoch: [3][173/233]	Loss 0.4754 (0.4689)	
training:	Epoch: [3][174/233]	Loss 0.4677 (0.4689)	
training:	Epoch: [3][175/233]	Loss 0.3314 (0.4681)	
training:	Epoch: [3][176/233]	Loss 0.5923 (0.4688)	
training:	Epoch: [3][177/233]	Loss 0.4087 (0.4685)	
training:	Epoch: [3][178/233]	Loss 0.4259 (0.4682)	
training:	Epoch: [3][179/233]	Loss 0.4717 (0.4683)	
training:	Epoch: [3][180/233]	Loss 0.4449 (0.4681)	
training:	Epoch: [3][181/233]	Loss 0.4688 (0.4681)	
training:	Epoch: [3][182/233]	Loss 0.3764 (0.4676)	
training:	Epoch: [3][183/233]	Loss 0.4336 (0.4674)	
training:	Epoch: [3][184/233]	Loss 0.5510 (0.4679)	
training:	Epoch: [3][185/233]	Loss 0.3608 (0.4673)	
training:	Epoch: [3][186/233]	Loss 0.4047 (0.4670)	
training:	Epoch: [3][187/233]	Loss 0.4873 (0.4671)	
training:	Epoch: [3][188/233]	Loss 0.4322 (0.4669)	
training:	Epoch: [3][189/233]	Loss 0.4454 (0.4668)	
training:	Epoch: [3][190/233]	Loss 0.3290 (0.4661)	
training:	Epoch: [3][191/233]	Loss 0.3713 (0.4656)	
training:	Epoch: [3][192/233]	Loss 0.4513 (0.4655)	
training:	Epoch: [3][193/233]	Loss 0.4835 (0.4656)	
training:	Epoch: [3][194/233]	Loss 0.3459 (0.4650)	
training:	Epoch: [3][195/233]	Loss 0.4392 (0.4648)	
training:	Epoch: [3][196/233]	Loss 0.4400 (0.4647)	
training:	Epoch: [3][197/233]	Loss 0.3911 (0.4643)	
training:	Epoch: [3][198/233]	Loss 0.4113 (0.4641)	
training:	Epoch: [3][199/233]	Loss 0.4344 (0.4639)	
training:	Epoch: [3][200/233]	Loss 0.4229 (0.4637)	
training:	Epoch: [3][201/233]	Loss 0.5199 (0.4640)	
training:	Epoch: [3][202/233]	Loss 0.3938 (0.4636)	
training:	Epoch: [3][203/233]	Loss 0.3652 (0.4632)	
training:	Epoch: [3][204/233]	Loss 0.4467 (0.4631)	
training:	Epoch: [3][205/233]	Loss 0.4424 (0.4630)	
training:	Epoch: [3][206/233]	Loss 0.4695 (0.4630)	
training:	Epoch: [3][207/233]	Loss 0.3908 (0.4627)	
training:	Epoch: [3][208/233]	Loss 0.4658 (0.4627)	
training:	Epoch: [3][209/233]	Loss 0.4478 (0.4626)	
training:	Epoch: [3][210/233]	Loss 0.4191 (0.4624)	
training:	Epoch: [3][211/233]	Loss 0.5234 (0.4627)	
training:	Epoch: [3][212/233]	Loss 0.4911 (0.4628)	
training:	Epoch: [3][213/233]	Loss 0.4771 (0.4629)	
training:	Epoch: [3][214/233]	Loss 0.5201 (0.4632)	
training:	Epoch: [3][215/233]	Loss 0.3818 (0.4628)	
training:	Epoch: [3][216/233]	Loss 0.4954 (0.4629)	
training:	Epoch: [3][217/233]	Loss 0.4701 (0.4630)	
training:	Epoch: [3][218/233]	Loss 0.4438 (0.4629)	
training:	Epoch: [3][219/233]	Loss 0.4588 (0.4629)	
training:	Epoch: [3][220/233]	Loss 0.4592 (0.4628)	
training:	Epoch: [3][221/233]	Loss 0.5579 (0.4633)	
training:	Epoch: [3][222/233]	Loss 0.5674 (0.4637)	
training:	Epoch: [3][223/233]	Loss 0.4065 (0.4635)	
training:	Epoch: [3][224/233]	Loss 0.4318 (0.4633)	
training:	Epoch: [3][225/233]	Loss 0.4951 (0.4635)	
training:	Epoch: [3][226/233]	Loss 0.5429 (0.4638)	
training:	Epoch: [3][227/233]	Loss 0.6045 (0.4644)	
training:	Epoch: [3][228/233]	Loss 0.5785 (0.4649)	
training:	Epoch: [3][229/233]	Loss 0.5071 (0.4651)	
training:	Epoch: [3][230/233]	Loss 0.4173 (0.4649)	
training:	Epoch: [3][231/233]	Loss 0.5326 (0.4652)	
training:	Epoch: [3][232/233]	Loss 0.4999 (0.4654)	
training:	Epoch: [3][233/233]	Loss 0.4162 (0.4652)	
Training:	 Loss: 0.4641

Training:	 ACC: 0.8169 0.8208 0.9065 0.7272
Validation:	 ACC: 0.7621 0.7673 0.8751 0.6491
Validation:	 Best_BACC: 0.7621 0.7673 0.8751 0.6491
Validation:	 Loss: 0.4924
Pretraining:	Epoch 4/200
----------
training:	Epoch: [4][1/233]	Loss 0.3615 (0.3615)	
training:	Epoch: [4][2/233]	Loss 0.5013 (0.4314)	
training:	Epoch: [4][3/233]	Loss 0.4085 (0.4238)	
training:	Epoch: [4][4/233]	Loss 0.4091 (0.4201)	
training:	Epoch: [4][5/233]	Loss 0.5055 (0.4372)	
training:	Epoch: [4][6/233]	Loss 0.4515 (0.4396)	
training:	Epoch: [4][7/233]	Loss 0.4587 (0.4423)	
training:	Epoch: [4][8/233]	Loss 0.4772 (0.4467)	
training:	Epoch: [4][9/233]	Loss 0.3921 (0.4406)	
training:	Epoch: [4][10/233]	Loss 0.4139 (0.4379)	
training:	Epoch: [4][11/233]	Loss 0.4265 (0.4369)	
training:	Epoch: [4][12/233]	Loss 0.3762 (0.4318)	
training:	Epoch: [4][13/233]	Loss 0.4413 (0.4326)	
training:	Epoch: [4][14/233]	Loss 0.3977 (0.4301)	
training:	Epoch: [4][15/233]	Loss 0.3306 (0.4234)	
training:	Epoch: [4][16/233]	Loss 0.4626 (0.4259)	
training:	Epoch: [4][17/233]	Loss 0.3661 (0.4224)	
training:	Epoch: [4][18/233]	Loss 0.5020 (0.4268)	
training:	Epoch: [4][19/233]	Loss 0.4220 (0.4265)	
training:	Epoch: [4][20/233]	Loss 0.5422 (0.4323)	
training:	Epoch: [4][21/233]	Loss 0.3943 (0.4305)	
training:	Epoch: [4][22/233]	Loss 0.3713 (0.4278)	
training:	Epoch: [4][23/233]	Loss 0.4018 (0.4267)	
training:	Epoch: [4][24/233]	Loss 0.3539 (0.4237)	
training:	Epoch: [4][25/233]	Loss 0.4891 (0.4263)	
training:	Epoch: [4][26/233]	Loss 0.4537 (0.4273)	
training:	Epoch: [4][27/233]	Loss 0.4829 (0.4294)	
training:	Epoch: [4][28/233]	Loss 0.4974 (0.4318)	
training:	Epoch: [4][29/233]	Loss 0.3333 (0.4284)	
training:	Epoch: [4][30/233]	Loss 0.4190 (0.4281)	
training:	Epoch: [4][31/233]	Loss 0.6733 (0.4360)	
training:	Epoch: [4][32/233]	Loss 0.4724 (0.4372)	
training:	Epoch: [4][33/233]	Loss 0.3736 (0.4352)	
training:	Epoch: [4][34/233]	Loss 0.4336 (0.4352)	
training:	Epoch: [4][35/233]	Loss 0.4675 (0.4361)	
training:	Epoch: [4][36/233]	Loss 0.4186 (0.4356)	
training:	Epoch: [4][37/233]	Loss 0.4268 (0.4354)	
training:	Epoch: [4][38/233]	Loss 0.4727 (0.4364)	
training:	Epoch: [4][39/233]	Loss 0.4251 (0.4361)	
training:	Epoch: [4][40/233]	Loss 0.6765 (0.4421)	
training:	Epoch: [4][41/233]	Loss 0.4019 (0.4411)	
training:	Epoch: [4][42/233]	Loss 0.3179 (0.4382)	
training:	Epoch: [4][43/233]	Loss 0.4580 (0.4386)	
training:	Epoch: [4][44/233]	Loss 0.3775 (0.4372)	
training:	Epoch: [4][45/233]	Loss 0.4587 (0.4377)	
training:	Epoch: [4][46/233]	Loss 0.4352 (0.4377)	
training:	Epoch: [4][47/233]	Loss 0.3906 (0.4367)	
training:	Epoch: [4][48/233]	Loss 0.4508 (0.4370)	
training:	Epoch: [4][49/233]	Loss 0.5095 (0.4384)	
training:	Epoch: [4][50/233]	Loss 0.3857 (0.4374)	
training:	Epoch: [4][51/233]	Loss 0.5624 (0.4398)	
training:	Epoch: [4][52/233]	Loss 0.4052 (0.4392)	
training:	Epoch: [4][53/233]	Loss 0.3900 (0.4382)	
training:	Epoch: [4][54/233]	Loss 0.4052 (0.4376)	
training:	Epoch: [4][55/233]	Loss 0.4211 (0.4373)	
training:	Epoch: [4][56/233]	Loss 0.3865 (0.4364)	
training:	Epoch: [4][57/233]	Loss 0.4913 (0.4374)	
training:	Epoch: [4][58/233]	Loss 0.3890 (0.4365)	
training:	Epoch: [4][59/233]	Loss 0.3457 (0.4350)	
training:	Epoch: [4][60/233]	Loss 0.3943 (0.4343)	
training:	Epoch: [4][61/233]	Loss 0.3219 (0.4325)	
training:	Epoch: [4][62/233]	Loss 0.4418 (0.4326)	
training:	Epoch: [4][63/233]	Loss 0.4202 (0.4324)	
training:	Epoch: [4][64/233]	Loss 0.4354 (0.4325)	
training:	Epoch: [4][65/233]	Loss 0.3633 (0.4314)	
training:	Epoch: [4][66/233]	Loss 0.3288 (0.4299)	
training:	Epoch: [4][67/233]	Loss 0.3283 (0.4283)	
training:	Epoch: [4][68/233]	Loss 0.4248 (0.4283)	
training:	Epoch: [4][69/233]	Loss 0.4482 (0.4286)	
training:	Epoch: [4][70/233]	Loss 0.3819 (0.4279)	
training:	Epoch: [4][71/233]	Loss 0.4459 (0.4282)	
training:	Epoch: [4][72/233]	Loss 0.3413 (0.4270)	
training:	Epoch: [4][73/233]	Loss 0.4474 (0.4272)	
training:	Epoch: [4][74/233]	Loss 0.4143 (0.4271)	
training:	Epoch: [4][75/233]	Loss 0.3415 (0.4259)	
training:	Epoch: [4][76/233]	Loss 0.4466 (0.4262)	
training:	Epoch: [4][77/233]	Loss 0.4645 (0.4267)	
training:	Epoch: [4][78/233]	Loss 0.3689 (0.4260)	
training:	Epoch: [4][79/233]	Loss 0.4464 (0.4262)	
training:	Epoch: [4][80/233]	Loss 0.4438 (0.4264)	
training:	Epoch: [4][81/233]	Loss 0.4854 (0.4272)	
training:	Epoch: [4][82/233]	Loss 0.5244 (0.4283)	
training:	Epoch: [4][83/233]	Loss 0.3297 (0.4272)	
training:	Epoch: [4][84/233]	Loss 0.4029 (0.4269)	
training:	Epoch: [4][85/233]	Loss 0.4992 (0.4277)	
training:	Epoch: [4][86/233]	Loss 0.4201 (0.4276)	
training:	Epoch: [4][87/233]	Loss 0.3398 (0.4266)	
training:	Epoch: [4][88/233]	Loss 0.3493 (0.4257)	
training:	Epoch: [4][89/233]	Loss 0.3991 (0.4254)	
training:	Epoch: [4][90/233]	Loss 0.3493 (0.4246)	
training:	Epoch: [4][91/233]	Loss 0.4202 (0.4245)	
training:	Epoch: [4][92/233]	Loss 0.4314 (0.4246)	
training:	Epoch: [4][93/233]	Loss 0.3907 (0.4243)	
training:	Epoch: [4][94/233]	Loss 0.3625 (0.4236)	
training:	Epoch: [4][95/233]	Loss 0.4172 (0.4235)	
training:	Epoch: [4][96/233]	Loss 0.4393 (0.4237)	
training:	Epoch: [4][97/233]	Loss 0.4528 (0.4240)	
training:	Epoch: [4][98/233]	Loss 0.4919 (0.4247)	
training:	Epoch: [4][99/233]	Loss 0.4606 (0.4251)	
training:	Epoch: [4][100/233]	Loss 0.4724 (0.4255)	
training:	Epoch: [4][101/233]	Loss 0.3778 (0.4251)	
training:	Epoch: [4][102/233]	Loss 0.3328 (0.4242)	
training:	Epoch: [4][103/233]	Loss 0.4514 (0.4244)	
training:	Epoch: [4][104/233]	Loss 0.4011 (0.4242)	
training:	Epoch: [4][105/233]	Loss 0.3720 (0.4237)	
training:	Epoch: [4][106/233]	Loss 0.3866 (0.4233)	
training:	Epoch: [4][107/233]	Loss 0.4167 (0.4233)	
training:	Epoch: [4][108/233]	Loss 0.4153 (0.4232)	
training:	Epoch: [4][109/233]	Loss 0.4188 (0.4232)	
training:	Epoch: [4][110/233]	Loss 0.4244 (0.4232)	
training:	Epoch: [4][111/233]	Loss 0.4194 (0.4231)	
training:	Epoch: [4][112/233]	Loss 0.3918 (0.4229)	
training:	Epoch: [4][113/233]	Loss 0.5398 (0.4239)	
training:	Epoch: [4][114/233]	Loss 0.4373 (0.4240)	
training:	Epoch: [4][115/233]	Loss 0.3989 (0.4238)	
training:	Epoch: [4][116/233]	Loss 0.4572 (0.4241)	
training:	Epoch: [4][117/233]	Loss 0.4014 (0.4239)	
training:	Epoch: [4][118/233]	Loss 0.5549 (0.4250)	
training:	Epoch: [4][119/233]	Loss 0.4292 (0.4250)	
training:	Epoch: [4][120/233]	Loss 0.4368 (0.4251)	
training:	Epoch: [4][121/233]	Loss 0.4915 (0.4257)	
training:	Epoch: [4][122/233]	Loss 0.4451 (0.4258)	
training:	Epoch: [4][123/233]	Loss 0.4191 (0.4258)	
training:	Epoch: [4][124/233]	Loss 0.4204 (0.4257)	
training:	Epoch: [4][125/233]	Loss 0.5813 (0.4270)	
training:	Epoch: [4][126/233]	Loss 0.4799 (0.4274)	
training:	Epoch: [4][127/233]	Loss 0.5842 (0.4286)	
training:	Epoch: [4][128/233]	Loss 0.3520 (0.4280)	
training:	Epoch: [4][129/233]	Loss 0.3828 (0.4277)	
training:	Epoch: [4][130/233]	Loss 0.3980 (0.4275)	
training:	Epoch: [4][131/233]	Loss 0.3717 (0.4270)	
training:	Epoch: [4][132/233]	Loss 0.3917 (0.4268)	
training:	Epoch: [4][133/233]	Loss 0.4170 (0.4267)	
training:	Epoch: [4][134/233]	Loss 0.4205 (0.4267)	
training:	Epoch: [4][135/233]	Loss 0.4570 (0.4269)	
training:	Epoch: [4][136/233]	Loss 0.3326 (0.4262)	
training:	Epoch: [4][137/233]	Loss 0.5173 (0.4269)	
training:	Epoch: [4][138/233]	Loss 0.3612 (0.4264)	
training:	Epoch: [4][139/233]	Loss 0.3295 (0.4257)	
training:	Epoch: [4][140/233]	Loss 0.4052 (0.4255)	
training:	Epoch: [4][141/233]	Loss 0.4519 (0.4257)	
training:	Epoch: [4][142/233]	Loss 0.3658 (0.4253)	
training:	Epoch: [4][143/233]	Loss 0.3360 (0.4247)	
training:	Epoch: [4][144/233]	Loss 0.4741 (0.4250)	
training:	Epoch: [4][145/233]	Loss 0.4248 (0.4250)	
training:	Epoch: [4][146/233]	Loss 0.3109 (0.4242)	
training:	Epoch: [4][147/233]	Loss 0.4739 (0.4246)	
training:	Epoch: [4][148/233]	Loss 0.5870 (0.4257)	
training:	Epoch: [4][149/233]	Loss 0.4592 (0.4259)	
training:	Epoch: [4][150/233]	Loss 0.4491 (0.4260)	
training:	Epoch: [4][151/233]	Loss 0.3308 (0.4254)	
training:	Epoch: [4][152/233]	Loss 0.4491 (0.4256)	
training:	Epoch: [4][153/233]	Loss 0.4383 (0.4257)	
training:	Epoch: [4][154/233]	Loss 0.3756 (0.4253)	
training:	Epoch: [4][155/233]	Loss 0.4031 (0.4252)	
training:	Epoch: [4][156/233]	Loss 0.4157 (0.4251)	
training:	Epoch: [4][157/233]	Loss 0.4368 (0.4252)	
training:	Epoch: [4][158/233]	Loss 0.3949 (0.4250)	
training:	Epoch: [4][159/233]	Loss 0.5745 (0.4260)	
training:	Epoch: [4][160/233]	Loss 0.3417 (0.4254)	
training:	Epoch: [4][161/233]	Loss 0.4063 (0.4253)	
training:	Epoch: [4][162/233]	Loss 0.3660 (0.4249)	
training:	Epoch: [4][163/233]	Loss 0.3487 (0.4245)	
training:	Epoch: [4][164/233]	Loss 0.3753 (0.4242)	
training:	Epoch: [4][165/233]	Loss 0.4125 (0.4241)	
training:	Epoch: [4][166/233]	Loss 0.4428 (0.4242)	
training:	Epoch: [4][167/233]	Loss 0.4374 (0.4243)	
training:	Epoch: [4][168/233]	Loss 0.5269 (0.4249)	
training:	Epoch: [4][169/233]	Loss 0.3394 (0.4244)	
training:	Epoch: [4][170/233]	Loss 0.3241 (0.4238)	
training:	Epoch: [4][171/233]	Loss 0.3589 (0.4234)	
training:	Epoch: [4][172/233]	Loss 0.3310 (0.4229)	
training:	Epoch: [4][173/233]	Loss 0.4247 (0.4229)	
training:	Epoch: [4][174/233]	Loss 0.4645 (0.4231)	
training:	Epoch: [4][175/233]	Loss 0.4243 (0.4231)	
training:	Epoch: [4][176/233]	Loss 0.3442 (0.4227)	
training:	Epoch: [4][177/233]	Loss 0.4035 (0.4226)	
training:	Epoch: [4][178/233]	Loss 0.3475 (0.4222)	
training:	Epoch: [4][179/233]	Loss 0.4459 (0.4223)	
training:	Epoch: [4][180/233]	Loss 0.4222 (0.4223)	
training:	Epoch: [4][181/233]	Loss 0.3882 (0.4221)	
training:	Epoch: [4][182/233]	Loss 0.4821 (0.4224)	
training:	Epoch: [4][183/233]	Loss 0.3689 (0.4221)	
training:	Epoch: [4][184/233]	Loss 0.3608 (0.4218)	
training:	Epoch: [4][185/233]	Loss 0.3794 (0.4216)	
training:	Epoch: [4][186/233]	Loss 0.3231 (0.4211)	
training:	Epoch: [4][187/233]	Loss 0.3889 (0.4209)	
training:	Epoch: [4][188/233]	Loss 0.4005 (0.4208)	
training:	Epoch: [4][189/233]	Loss 0.3553 (0.4204)	
training:	Epoch: [4][190/233]	Loss 0.5461 (0.4211)	
training:	Epoch: [4][191/233]	Loss 0.4477 (0.4212)	
training:	Epoch: [4][192/233]	Loss 0.4347 (0.4213)	
training:	Epoch: [4][193/233]	Loss 0.4831 (0.4216)	
training:	Epoch: [4][194/233]	Loss 0.3826 (0.4214)	
training:	Epoch: [4][195/233]	Loss 0.5142 (0.4219)	
training:	Epoch: [4][196/233]	Loss 0.3729 (0.4216)	
training:	Epoch: [4][197/233]	Loss 0.3733 (0.4214)	
training:	Epoch: [4][198/233]	Loss 0.4781 (0.4217)	
training:	Epoch: [4][199/233]	Loss 0.3961 (0.4216)	
training:	Epoch: [4][200/233]	Loss 0.3998 (0.4214)	
training:	Epoch: [4][201/233]	Loss 0.3957 (0.4213)	
training:	Epoch: [4][202/233]	Loss 0.4075 (0.4213)	
training:	Epoch: [4][203/233]	Loss 0.4644 (0.4215)	
training:	Epoch: [4][204/233]	Loss 0.4276 (0.4215)	
training:	Epoch: [4][205/233]	Loss 0.3315 (0.4211)	
training:	Epoch: [4][206/233]	Loss 0.5476 (0.4217)	
training:	Epoch: [4][207/233]	Loss 0.4577 (0.4218)	
training:	Epoch: [4][208/233]	Loss 0.4557 (0.4220)	
training:	Epoch: [4][209/233]	Loss 0.5026 (0.4224)	
training:	Epoch: [4][210/233]	Loss 0.5903 (0.4232)	
training:	Epoch: [4][211/233]	Loss 0.3931 (0.4231)	
training:	Epoch: [4][212/233]	Loss 0.3749 (0.4228)	
training:	Epoch: [4][213/233]	Loss 0.4115 (0.4228)	
training:	Epoch: [4][214/233]	Loss 0.3525 (0.4224)	
training:	Epoch: [4][215/233]	Loss 0.3279 (0.4220)	
training:	Epoch: [4][216/233]	Loss 0.3985 (0.4219)	
training:	Epoch: [4][217/233]	Loss 0.4393 (0.4220)	
training:	Epoch: [4][218/233]	Loss 0.4044 (0.4219)	
training:	Epoch: [4][219/233]	Loss 0.4398 (0.4220)	
training:	Epoch: [4][220/233]	Loss 0.4536 (0.4221)	
training:	Epoch: [4][221/233]	Loss 0.3536 (0.4218)	
training:	Epoch: [4][222/233]	Loss 0.3713 (0.4216)	
training:	Epoch: [4][223/233]	Loss 0.4894 (0.4219)	
training:	Epoch: [4][224/233]	Loss 0.3853 (0.4217)	
training:	Epoch: [4][225/233]	Loss 0.4283 (0.4218)	
training:	Epoch: [4][226/233]	Loss 0.3444 (0.4214)	
training:	Epoch: [4][227/233]	Loss 0.4135 (0.4214)	
training:	Epoch: [4][228/233]	Loss 0.4439 (0.4215)	
training:	Epoch: [4][229/233]	Loss 0.4312 (0.4215)	
training:	Epoch: [4][230/233]	Loss 0.3919 (0.4214)	
training:	Epoch: [4][231/233]	Loss 0.3145 (0.4209)	
training:	Epoch: [4][232/233]	Loss 0.4275 (0.4210)	
training:	Epoch: [4][233/233]	Loss 0.3384 (0.4206)	
Training:	 Loss: 0.4196

Training:	 ACC: 0.8504 0.8533 0.9167 0.7841
Validation:	 ACC: 0.7765 0.7806 0.8669 0.6861
Validation:	 Best_BACC: 0.7765 0.7806 0.8669 0.6861
Validation:	 Loss: 0.4712
Pretraining:	Epoch 5/200
----------
training:	Epoch: [5][1/233]	Loss 0.3967 (0.3967)	
training:	Epoch: [5][2/233]	Loss 0.3825 (0.3896)	
training:	Epoch: [5][3/233]	Loss 0.3456 (0.3749)	
training:	Epoch: [5][4/233]	Loss 0.3101 (0.3587)	
training:	Epoch: [5][5/233]	Loss 0.3565 (0.3583)	
training:	Epoch: [5][6/233]	Loss 0.4476 (0.3732)	
training:	Epoch: [5][7/233]	Loss 0.4324 (0.3816)	
training:	Epoch: [5][8/233]	Loss 0.3376 (0.3761)	
training:	Epoch: [5][9/233]	Loss 0.3485 (0.3731)	
training:	Epoch: [5][10/233]	Loss 0.4265 (0.3784)	
training:	Epoch: [5][11/233]	Loss 0.4411 (0.3841)	
training:	Epoch: [5][12/233]	Loss 0.3637 (0.3824)	
training:	Epoch: [5][13/233]	Loss 0.3572 (0.3805)	
training:	Epoch: [5][14/233]	Loss 0.4057 (0.3823)	
training:	Epoch: [5][15/233]	Loss 0.3922 (0.3829)	
training:	Epoch: [5][16/233]	Loss 0.4378 (0.3864)	
training:	Epoch: [5][17/233]	Loss 0.3348 (0.3833)	
training:	Epoch: [5][18/233]	Loss 0.3586 (0.3820)	
training:	Epoch: [5][19/233]	Loss 0.3792 (0.3818)	
training:	Epoch: [5][20/233]	Loss 0.4278 (0.3841)	
training:	Epoch: [5][21/233]	Loss 0.4337 (0.3865)	
training:	Epoch: [5][22/233]	Loss 0.3156 (0.3832)	
training:	Epoch: [5][23/233]	Loss 0.2961 (0.3795)	
training:	Epoch: [5][24/233]	Loss 0.4639 (0.3830)	
training:	Epoch: [5][25/233]	Loss 0.5911 (0.3913)	
training:	Epoch: [5][26/233]	Loss 0.3314 (0.3890)	
training:	Epoch: [5][27/233]	Loss 0.4192 (0.3901)	
training:	Epoch: [5][28/233]	Loss 0.3713 (0.3894)	
training:	Epoch: [5][29/233]	Loss 0.5233 (0.3941)	
training:	Epoch: [5][30/233]	Loss 0.3322 (0.3920)	
training:	Epoch: [5][31/233]	Loss 0.3365 (0.3902)	
training:	Epoch: [5][32/233]	Loss 0.4740 (0.3928)	
training:	Epoch: [5][33/233]	Loss 0.3010 (0.3900)	
training:	Epoch: [5][34/233]	Loss 0.3737 (0.3896)	
training:	Epoch: [5][35/233]	Loss 0.3426 (0.3882)	
training:	Epoch: [5][36/233]	Loss 0.4356 (0.3895)	
training:	Epoch: [5][37/233]	Loss 0.4167 (0.3903)	
training:	Epoch: [5][38/233]	Loss 0.3742 (0.3898)	
training:	Epoch: [5][39/233]	Loss 0.4346 (0.3910)	
training:	Epoch: [5][40/233]	Loss 0.3650 (0.3903)	
training:	Epoch: [5][41/233]	Loss 0.3463 (0.3893)	
training:	Epoch: [5][42/233]	Loss 0.3669 (0.3887)	
training:	Epoch: [5][43/233]	Loss 0.4146 (0.3893)	
training:	Epoch: [5][44/233]	Loss 0.4114 (0.3898)	
training:	Epoch: [5][45/233]	Loss 0.4561 (0.3913)	
training:	Epoch: [5][46/233]	Loss 0.4096 (0.3917)	
training:	Epoch: [5][47/233]	Loss 0.3719 (0.3913)	
training:	Epoch: [5][48/233]	Loss 0.3883 (0.3912)	
training:	Epoch: [5][49/233]	Loss 0.3827 (0.3911)	
training:	Epoch: [5][50/233]	Loss 0.3393 (0.3900)	
training:	Epoch: [5][51/233]	Loss 0.3361 (0.3890)	
training:	Epoch: [5][52/233]	Loss 0.4513 (0.3902)	
training:	Epoch: [5][53/233]	Loss 0.3049 (0.3886)	
training:	Epoch: [5][54/233]	Loss 0.4887 (0.3904)	
training:	Epoch: [5][55/233]	Loss 0.3898 (0.3904)	
training:	Epoch: [5][56/233]	Loss 0.3064 (0.3889)	
training:	Epoch: [5][57/233]	Loss 0.3737 (0.3886)	
training:	Epoch: [5][58/233]	Loss 0.4411 (0.3895)	
training:	Epoch: [5][59/233]	Loss 0.4398 (0.3904)	
training:	Epoch: [5][60/233]	Loss 0.4489 (0.3914)	
training:	Epoch: [5][61/233]	Loss 0.3965 (0.3914)	
training:	Epoch: [5][62/233]	Loss 0.3011 (0.3900)	
training:	Epoch: [5][63/233]	Loss 0.3823 (0.3899)	
training:	Epoch: [5][64/233]	Loss 0.3598 (0.3894)	
training:	Epoch: [5][65/233]	Loss 0.3935 (0.3895)	
training:	Epoch: [5][66/233]	Loss 0.3490 (0.3888)	
training:	Epoch: [5][67/233]	Loss 0.3289 (0.3879)	
training:	Epoch: [5][68/233]	Loss 0.3429 (0.3873)	
training:	Epoch: [5][69/233]	Loss 0.4726 (0.3885)	
training:	Epoch: [5][70/233]	Loss 0.3906 (0.3886)	
training:	Epoch: [5][71/233]	Loss 0.3287 (0.3877)	
training:	Epoch: [5][72/233]	Loss 0.4061 (0.3880)	
training:	Epoch: [5][73/233]	Loss 0.3628 (0.3876)	
training:	Epoch: [5][74/233]	Loss 0.3718 (0.3874)	
training:	Epoch: [5][75/233]	Loss 0.4296 (0.3880)	
training:	Epoch: [5][76/233]	Loss 0.3329 (0.3872)	
training:	Epoch: [5][77/233]	Loss 0.3723 (0.3871)	
training:	Epoch: [5][78/233]	Loss 0.4311 (0.3876)	
training:	Epoch: [5][79/233]	Loss 0.2773 (0.3862)	
training:	Epoch: [5][80/233]	Loss 0.4500 (0.3870)	
training:	Epoch: [5][81/233]	Loss 0.5266 (0.3887)	
training:	Epoch: [5][82/233]	Loss 0.5309 (0.3905)	
training:	Epoch: [5][83/233]	Loss 0.4008 (0.3906)	
training:	Epoch: [5][84/233]	Loss 0.3429 (0.3900)	
training:	Epoch: [5][85/233]	Loss 0.4154 (0.3903)	
training:	Epoch: [5][86/233]	Loss 0.4046 (0.3905)	
training:	Epoch: [5][87/233]	Loss 0.3854 (0.3904)	
training:	Epoch: [5][88/233]	Loss 0.3646 (0.3901)	
training:	Epoch: [5][89/233]	Loss 0.4078 (0.3903)	
training:	Epoch: [5][90/233]	Loss 0.2870 (0.3892)	
training:	Epoch: [5][91/233]	Loss 0.4051 (0.3894)	
training:	Epoch: [5][92/233]	Loss 0.4082 (0.3896)	
training:	Epoch: [5][93/233]	Loss 0.2532 (0.3881)	
training:	Epoch: [5][94/233]	Loss 0.3508 (0.3877)	
training:	Epoch: [5][95/233]	Loss 0.4547 (0.3884)	
training:	Epoch: [5][96/233]	Loss 0.3961 (0.3885)	
training:	Epoch: [5][97/233]	Loss 0.2993 (0.3876)	
training:	Epoch: [5][98/233]	Loss 0.4057 (0.3878)	
training:	Epoch: [5][99/233]	Loss 0.4303 (0.3882)	
training:	Epoch: [5][100/233]	Loss 0.3728 (0.3880)	
training:	Epoch: [5][101/233]	Loss 0.4038 (0.3882)	
training:	Epoch: [5][102/233]	Loss 0.4148 (0.3885)	
training:	Epoch: [5][103/233]	Loss 0.3530 (0.3881)	
training:	Epoch: [5][104/233]	Loss 0.2774 (0.3870)	
training:	Epoch: [5][105/233]	Loss 0.3798 (0.3870)	
training:	Epoch: [5][106/233]	Loss 0.4006 (0.3871)	
training:	Epoch: [5][107/233]	Loss 0.4569 (0.3878)	
training:	Epoch: [5][108/233]	Loss 0.3253 (0.3872)	
training:	Epoch: [5][109/233]	Loss 0.4069 (0.3874)	
training:	Epoch: [5][110/233]	Loss 0.3412 (0.3869)	
training:	Epoch: [5][111/233]	Loss 0.3980 (0.3870)	
training:	Epoch: [5][112/233]	Loss 0.3796 (0.3870)	
training:	Epoch: [5][113/233]	Loss 0.2837 (0.3861)	
training:	Epoch: [5][114/233]	Loss 0.3543 (0.3858)	
training:	Epoch: [5][115/233]	Loss 0.3838 (0.3858)	
training:	Epoch: [5][116/233]	Loss 0.4361 (0.3862)	
training:	Epoch: [5][117/233]	Loss 0.4459 (0.3867)	
training:	Epoch: [5][118/233]	Loss 0.3206 (0.3861)	
training:	Epoch: [5][119/233]	Loss 0.3364 (0.3857)	
training:	Epoch: [5][120/233]	Loss 0.4332 (0.3861)	
training:	Epoch: [5][121/233]	Loss 0.1802 (0.3844)	
training:	Epoch: [5][122/233]	Loss 0.4244 (0.3848)	
training:	Epoch: [5][123/233]	Loss 0.4118 (0.3850)	
training:	Epoch: [5][124/233]	Loss 0.2887 (0.3842)	
training:	Epoch: [5][125/233]	Loss 0.3278 (0.3837)	
training:	Epoch: [5][126/233]	Loss 0.4512 (0.3843)	
training:	Epoch: [5][127/233]	Loss 0.4107 (0.3845)	
training:	Epoch: [5][128/233]	Loss 0.3570 (0.3843)	
training:	Epoch: [5][129/233]	Loss 0.3506 (0.3840)	
training:	Epoch: [5][130/233]	Loss 0.3945 (0.3841)	
training:	Epoch: [5][131/233]	Loss 0.4257 (0.3844)	
training:	Epoch: [5][132/233]	Loss 0.3542 (0.3842)	
training:	Epoch: [5][133/233]	Loss 0.4816 (0.3849)	
training:	Epoch: [5][134/233]	Loss 0.3567 (0.3847)	
training:	Epoch: [5][135/233]	Loss 0.4060 (0.3849)	
training:	Epoch: [5][136/233]	Loss 0.5904 (0.3864)	
training:	Epoch: [5][137/233]	Loss 0.3300 (0.3860)	
training:	Epoch: [5][138/233]	Loss 0.3627 (0.3858)	
training:	Epoch: [5][139/233]	Loss 0.4650 (0.3864)	
training:	Epoch: [5][140/233]	Loss 0.3823 (0.3863)	
training:	Epoch: [5][141/233]	Loss 0.5038 (0.3872)	
training:	Epoch: [5][142/233]	Loss 0.4175 (0.3874)	
training:	Epoch: [5][143/233]	Loss 0.4201 (0.3876)	
training:	Epoch: [5][144/233]	Loss 0.5000 (0.3884)	
training:	Epoch: [5][145/233]	Loss 0.3077 (0.3878)	
training:	Epoch: [5][146/233]	Loss 0.3595 (0.3876)	
training:	Epoch: [5][147/233]	Loss 0.3457 (0.3874)	
training:	Epoch: [5][148/233]	Loss 0.2549 (0.3865)	
training:	Epoch: [5][149/233]	Loss 0.3761 (0.3864)	
training:	Epoch: [5][150/233]	Loss 0.3976 (0.3865)	
training:	Epoch: [5][151/233]	Loss 0.3903 (0.3865)	
training:	Epoch: [5][152/233]	Loss 0.4666 (0.3870)	
training:	Epoch: [5][153/233]	Loss 0.3666 (0.3869)	
training:	Epoch: [5][154/233]	Loss 0.4799 (0.3875)	
training:	Epoch: [5][155/233]	Loss 0.3617 (0.3873)	
training:	Epoch: [5][156/233]	Loss 0.4380 (0.3876)	
training:	Epoch: [5][157/233]	Loss 0.4714 (0.3882)	
training:	Epoch: [5][158/233]	Loss 0.3771 (0.3881)	
training:	Epoch: [5][159/233]	Loss 0.3449 (0.3878)	
training:	Epoch: [5][160/233]	Loss 0.4802 (0.3884)	
training:	Epoch: [5][161/233]	Loss 0.4529 (0.3888)	
training:	Epoch: [5][162/233]	Loss 0.3342 (0.3885)	
training:	Epoch: [5][163/233]	Loss 0.3017 (0.3879)	
training:	Epoch: [5][164/233]	Loss 0.3743 (0.3879)	
training:	Epoch: [5][165/233]	Loss 0.4084 (0.3880)	
training:	Epoch: [5][166/233]	Loss 0.3870 (0.3880)	
training:	Epoch: [5][167/233]	Loss 0.3678 (0.3879)	
training:	Epoch: [5][168/233]	Loss 0.3986 (0.3879)	
training:	Epoch: [5][169/233]	Loss 0.4151 (0.3881)	
training:	Epoch: [5][170/233]	Loss 0.2938 (0.3875)	
training:	Epoch: [5][171/233]	Loss 0.3936 (0.3876)	
training:	Epoch: [5][172/233]	Loss 0.4999 (0.3882)	
training:	Epoch: [5][173/233]	Loss 0.3485 (0.3880)	
training:	Epoch: [5][174/233]	Loss 0.4925 (0.3886)	
training:	Epoch: [5][175/233]	Loss 0.2736 (0.3879)	
training:	Epoch: [5][176/233]	Loss 0.5120 (0.3886)	
training:	Epoch: [5][177/233]	Loss 0.3978 (0.3887)	
training:	Epoch: [5][178/233]	Loss 0.3085 (0.3882)	
training:	Epoch: [5][179/233]	Loss 0.4362 (0.3885)	
training:	Epoch: [5][180/233]	Loss 0.3278 (0.3882)	
training:	Epoch: [5][181/233]	Loss 0.3745 (0.3881)	
training:	Epoch: [5][182/233]	Loss 0.4461 (0.3884)	
training:	Epoch: [5][183/233]	Loss 0.2791 (0.3878)	
training:	Epoch: [5][184/233]	Loss 0.2741 (0.3872)	
training:	Epoch: [5][185/233]	Loss 0.3778 (0.3871)	
training:	Epoch: [5][186/233]	Loss 0.4269 (0.3874)	
training:	Epoch: [5][187/233]	Loss 0.2955 (0.3869)	
training:	Epoch: [5][188/233]	Loss 0.4397 (0.3871)	
training:	Epoch: [5][189/233]	Loss 0.4804 (0.3876)	
training:	Epoch: [5][190/233]	Loss 0.4329 (0.3879)	
training:	Epoch: [5][191/233]	Loss 0.3158 (0.3875)	
training:	Epoch: [5][192/233]	Loss 0.3606 (0.3874)	
training:	Epoch: [5][193/233]	Loss 0.3786 (0.3873)	
training:	Epoch: [5][194/233]	Loss 0.3265 (0.3870)	
training:	Epoch: [5][195/233]	Loss 0.4013 (0.3871)	
training:	Epoch: [5][196/233]	Loss 0.3684 (0.3870)	
training:	Epoch: [5][197/233]	Loss 0.3308 (0.3867)	
training:	Epoch: [5][198/233]	Loss 0.2818 (0.3862)	
training:	Epoch: [5][199/233]	Loss 0.4105 (0.3863)	
training:	Epoch: [5][200/233]	Loss 0.4541 (0.3866)	
training:	Epoch: [5][201/233]	Loss 0.3906 (0.3866)	
training:	Epoch: [5][202/233]	Loss 0.5107 (0.3873)	
training:	Epoch: [5][203/233]	Loss 0.3601 (0.3871)	
training:	Epoch: [5][204/233]	Loss 0.3333 (0.3869)	
training:	Epoch: [5][205/233]	Loss 0.4067 (0.3870)	
training:	Epoch: [5][206/233]	Loss 0.3078 (0.3866)	
training:	Epoch: [5][207/233]	Loss 0.2637 (0.3860)	
training:	Epoch: [5][208/233]	Loss 0.3640 (0.3859)	
training:	Epoch: [5][209/233]	Loss 0.3046 (0.3855)	
training:	Epoch: [5][210/233]	Loss 0.4540 (0.3858)	
training:	Epoch: [5][211/233]	Loss 0.4767 (0.3862)	
training:	Epoch: [5][212/233]	Loss 0.3730 (0.3862)	
training:	Epoch: [5][213/233]	Loss 0.3155 (0.3858)	
training:	Epoch: [5][214/233]	Loss 0.3204 (0.3855)	
training:	Epoch: [5][215/233]	Loss 0.5820 (0.3865)	
training:	Epoch: [5][216/233]	Loss 0.4623 (0.3868)	
training:	Epoch: [5][217/233]	Loss 0.4673 (0.3872)	
training:	Epoch: [5][218/233]	Loss 0.3802 (0.3871)	
training:	Epoch: [5][219/233]	Loss 0.4015 (0.3872)	
training:	Epoch: [5][220/233]	Loss 0.2315 (0.3865)	
training:	Epoch: [5][221/233]	Loss 0.4183 (0.3866)	
training:	Epoch: [5][222/233]	Loss 0.4729 (0.3870)	
training:	Epoch: [5][223/233]	Loss 0.4396 (0.3873)	
training:	Epoch: [5][224/233]	Loss 0.3627 (0.3872)	
training:	Epoch: [5][225/233]	Loss 0.3112 (0.3868)	
training:	Epoch: [5][226/233]	Loss 0.3599 (0.3867)	
training:	Epoch: [5][227/233]	Loss 0.4143 (0.3868)	
training:	Epoch: [5][228/233]	Loss 0.5036 (0.3873)	
training:	Epoch: [5][229/233]	Loss 0.4067 (0.3874)	
training:	Epoch: [5][230/233]	Loss 0.2513 (0.3868)	
training:	Epoch: [5][231/233]	Loss 0.4187 (0.3870)	
training:	Epoch: [5][232/233]	Loss 0.4736 (0.3873)	
training:	Epoch: [5][233/233]	Loss 0.3369 (0.3871)	
Training:	 Loss: 0.3862

Training:	 ACC: 0.8799 0.8816 0.9185 0.8412
Validation:	 ACC: 0.7841 0.7871 0.8495 0.7186
Validation:	 Best_BACC: 0.7841 0.7871 0.8495 0.7186
Validation:	 Loss: 0.4544
Pretraining:	Epoch 6/200
----------
training:	Epoch: [6][1/233]	Loss 0.2471 (0.2471)	
training:	Epoch: [6][2/233]	Loss 0.3855 (0.3163)	
training:	Epoch: [6][3/233]	Loss 0.4128 (0.3485)	
training:	Epoch: [6][4/233]	Loss 0.3917 (0.3593)	
training:	Epoch: [6][5/233]	Loss 0.3486 (0.3571)	
training:	Epoch: [6][6/233]	Loss 0.3453 (0.3552)	
training:	Epoch: [6][7/233]	Loss 0.2984 (0.3470)	
training:	Epoch: [6][8/233]	Loss 0.3389 (0.3460)	
training:	Epoch: [6][9/233]	Loss 0.3787 (0.3497)	
training:	Epoch: [6][10/233]	Loss 0.3464 (0.3493)	
training:	Epoch: [6][11/233]	Loss 0.3888 (0.3529)	
training:	Epoch: [6][12/233]	Loss 0.2901 (0.3477)	
training:	Epoch: [6][13/233]	Loss 0.4172 (0.3530)	
training:	Epoch: [6][14/233]	Loss 0.3842 (0.3553)	
training:	Epoch: [6][15/233]	Loss 0.3660 (0.3560)	
training:	Epoch: [6][16/233]	Loss 0.2960 (0.3522)	
training:	Epoch: [6][17/233]	Loss 0.3655 (0.3530)	
training:	Epoch: [6][18/233]	Loss 0.3551 (0.3531)	
training:	Epoch: [6][19/233]	Loss 0.5426 (0.3631)	
training:	Epoch: [6][20/233]	Loss 0.4562 (0.3678)	
training:	Epoch: [6][21/233]	Loss 0.5002 (0.3741)	
training:	Epoch: [6][22/233]	Loss 0.3563 (0.3732)	
training:	Epoch: [6][23/233]	Loss 0.3778 (0.3734)	
training:	Epoch: [6][24/233]	Loss 0.3835 (0.3739)	
training:	Epoch: [6][25/233]	Loss 0.4146 (0.3755)	
training:	Epoch: [6][26/233]	Loss 0.4058 (0.3767)	
training:	Epoch: [6][27/233]	Loss 0.3360 (0.3752)	
training:	Epoch: [6][28/233]	Loss 0.4592 (0.3782)	
training:	Epoch: [6][29/233]	Loss 0.2882 (0.3751)	
training:	Epoch: [6][30/233]	Loss 0.5135 (0.3797)	
training:	Epoch: [6][31/233]	Loss 0.3479 (0.3786)	
training:	Epoch: [6][32/233]	Loss 0.3075 (0.3764)	
training:	Epoch: [6][33/233]	Loss 0.4017 (0.3772)	
training:	Epoch: [6][34/233]	Loss 0.3204 (0.3755)	
training:	Epoch: [6][35/233]	Loss 0.2888 (0.3730)	
training:	Epoch: [6][36/233]	Loss 0.4004 (0.3738)	
training:	Epoch: [6][37/233]	Loss 0.3902 (0.3742)	
training:	Epoch: [6][38/233]	Loss 0.3226 (0.3729)	
training:	Epoch: [6][39/233]	Loss 0.3958 (0.3735)	
training:	Epoch: [6][40/233]	Loss 0.3053 (0.3718)	
training:	Epoch: [6][41/233]	Loss 0.3751 (0.3718)	
training:	Epoch: [6][42/233]	Loss 0.3617 (0.3716)	
training:	Epoch: [6][43/233]	Loss 0.2343 (0.3684)	
training:	Epoch: [6][44/233]	Loss 0.3518 (0.3680)	
training:	Epoch: [6][45/233]	Loss 0.3274 (0.3671)	
training:	Epoch: [6][46/233]	Loss 0.4106 (0.3681)	
training:	Epoch: [6][47/233]	Loss 0.3230 (0.3671)	
training:	Epoch: [6][48/233]	Loss 0.3687 (0.3672)	
training:	Epoch: [6][49/233]	Loss 0.4028 (0.3679)	
training:	Epoch: [6][50/233]	Loss 0.2446 (0.3654)	
training:	Epoch: [6][51/233]	Loss 0.3330 (0.3648)	
training:	Epoch: [6][52/233]	Loss 0.3274 (0.3641)	
training:	Epoch: [6][53/233]	Loss 0.2543 (0.3620)	
training:	Epoch: [6][54/233]	Loss 0.3309 (0.3614)	
training:	Epoch: [6][55/233]	Loss 0.4101 (0.3623)	
training:	Epoch: [6][56/233]	Loss 0.3629 (0.3623)	
training:	Epoch: [6][57/233]	Loss 0.2859 (0.3610)	
training:	Epoch: [6][58/233]	Loss 0.3703 (0.3611)	
training:	Epoch: [6][59/233]	Loss 0.3704 (0.3613)	
training:	Epoch: [6][60/233]	Loss 0.5029 (0.3636)	
training:	Epoch: [6][61/233]	Loss 0.3431 (0.3633)	
training:	Epoch: [6][62/233]	Loss 0.4101 (0.3641)	
training:	Epoch: [6][63/233]	Loss 0.4488 (0.3654)	
training:	Epoch: [6][64/233]	Loss 0.3342 (0.3649)	
training:	Epoch: [6][65/233]	Loss 0.4097 (0.3656)	
training:	Epoch: [6][66/233]	Loss 0.2987 (0.3646)	
training:	Epoch: [6][67/233]	Loss 0.2922 (0.3635)	
training:	Epoch: [6][68/233]	Loss 0.4369 (0.3646)	
training:	Epoch: [6][69/233]	Loss 0.3140 (0.3639)	
training:	Epoch: [6][70/233]	Loss 0.3364 (0.3635)	
training:	Epoch: [6][71/233]	Loss 0.3258 (0.3629)	
training:	Epoch: [6][72/233]	Loss 0.3684 (0.3630)	
training:	Epoch: [6][73/233]	Loss 0.3446 (0.3628)	
training:	Epoch: [6][74/233]	Loss 0.3108 (0.3621)	
training:	Epoch: [6][75/233]	Loss 0.2633 (0.3607)	
training:	Epoch: [6][76/233]	Loss 0.4051 (0.3613)	
training:	Epoch: [6][77/233]	Loss 0.3699 (0.3614)	
training:	Epoch: [6][78/233]	Loss 0.2460 (0.3600)	
training:	Epoch: [6][79/233]	Loss 0.3018 (0.3592)	
training:	Epoch: [6][80/233]	Loss 0.5435 (0.3615)	
training:	Epoch: [6][81/233]	Loss 0.2732 (0.3604)	
training:	Epoch: [6][82/233]	Loss 0.3389 (0.3602)	
training:	Epoch: [6][83/233]	Loss 0.3267 (0.3598)	
training:	Epoch: [6][84/233]	Loss 0.2851 (0.3589)	
training:	Epoch: [6][85/233]	Loss 0.3787 (0.3591)	
training:	Epoch: [6][86/233]	Loss 0.4184 (0.3598)	
training:	Epoch: [6][87/233]	Loss 0.3055 (0.3592)	
training:	Epoch: [6][88/233]	Loss 0.2951 (0.3584)	
training:	Epoch: [6][89/233]	Loss 0.4353 (0.3593)	
training:	Epoch: [6][90/233]	Loss 0.4247 (0.3600)	
training:	Epoch: [6][91/233]	Loss 0.3961 (0.3604)	
training:	Epoch: [6][92/233]	Loss 0.4762 (0.3617)	
training:	Epoch: [6][93/233]	Loss 0.3967 (0.3621)	
training:	Epoch: [6][94/233]	Loss 0.3242 (0.3617)	
training:	Epoch: [6][95/233]	Loss 0.3093 (0.3611)	
training:	Epoch: [6][96/233]	Loss 0.2545 (0.3600)	
training:	Epoch: [6][97/233]	Loss 0.3343 (0.3597)	
training:	Epoch: [6][98/233]	Loss 0.3261 (0.3594)	
training:	Epoch: [6][99/233]	Loss 0.2762 (0.3586)	
training:	Epoch: [6][100/233]	Loss 0.2960 (0.3579)	
training:	Epoch: [6][101/233]	Loss 0.3783 (0.3581)	
training:	Epoch: [6][102/233]	Loss 0.3612 (0.3582)	
training:	Epoch: [6][103/233]	Loss 0.4894 (0.3594)	
training:	Epoch: [6][104/233]	Loss 0.2776 (0.3586)	
training:	Epoch: [6][105/233]	Loss 0.4230 (0.3593)	
training:	Epoch: [6][106/233]	Loss 0.2860 (0.3586)	
training:	Epoch: [6][107/233]	Loss 0.4224 (0.3592)	
training:	Epoch: [6][108/233]	Loss 0.2974 (0.3586)	
training:	Epoch: [6][109/233]	Loss 0.2909 (0.3580)	
training:	Epoch: [6][110/233]	Loss 0.4642 (0.3589)	
training:	Epoch: [6][111/233]	Loss 0.3683 (0.3590)	
training:	Epoch: [6][112/233]	Loss 0.3135 (0.3586)	
training:	Epoch: [6][113/233]	Loss 0.3817 (0.3588)	
training:	Epoch: [6][114/233]	Loss 0.3216 (0.3585)	
training:	Epoch: [6][115/233]	Loss 0.3147 (0.3581)	
training:	Epoch: [6][116/233]	Loss 0.2858 (0.3575)	
training:	Epoch: [6][117/233]	Loss 0.3359 (0.3573)	
training:	Epoch: [6][118/233]	Loss 0.5273 (0.3587)	
training:	Epoch: [6][119/233]	Loss 0.3618 (0.3588)	
training:	Epoch: [6][120/233]	Loss 0.4033 (0.3591)	
training:	Epoch: [6][121/233]	Loss 0.3522 (0.3591)	
training:	Epoch: [6][122/233]	Loss 0.3113 (0.3587)	
training:	Epoch: [6][123/233]	Loss 0.5097 (0.3599)	
training:	Epoch: [6][124/233]	Loss 0.3181 (0.3596)	
training:	Epoch: [6][125/233]	Loss 0.3373 (0.3594)	
training:	Epoch: [6][126/233]	Loss 0.3451 (0.3593)	
training:	Epoch: [6][127/233]	Loss 0.3287 (0.3591)	
training:	Epoch: [6][128/233]	Loss 0.3834 (0.3592)	
training:	Epoch: [6][129/233]	Loss 0.2659 (0.3585)	
training:	Epoch: [6][130/233]	Loss 0.3361 (0.3583)	
training:	Epoch: [6][131/233]	Loss 0.4013 (0.3587)	
training:	Epoch: [6][132/233]	Loss 0.4151 (0.3591)	
training:	Epoch: [6][133/233]	Loss 0.3144 (0.3588)	
training:	Epoch: [6][134/233]	Loss 0.4287 (0.3593)	
training:	Epoch: [6][135/233]	Loss 0.3146 (0.3590)	
training:	Epoch: [6][136/233]	Loss 0.2651 (0.3583)	
training:	Epoch: [6][137/233]	Loss 0.3960 (0.3585)	
training:	Epoch: [6][138/233]	Loss 0.2863 (0.3580)	
training:	Epoch: [6][139/233]	Loss 0.2343 (0.3571)	
training:	Epoch: [6][140/233]	Loss 0.4470 (0.3578)	
training:	Epoch: [6][141/233]	Loss 0.3142 (0.3575)	
training:	Epoch: [6][142/233]	Loss 0.2121 (0.3564)	
training:	Epoch: [6][143/233]	Loss 0.4839 (0.3573)	
training:	Epoch: [6][144/233]	Loss 0.4254 (0.3578)	
training:	Epoch: [6][145/233]	Loss 0.3980 (0.3581)	
training:	Epoch: [6][146/233]	Loss 0.3412 (0.3580)	
training:	Epoch: [6][147/233]	Loss 0.3151 (0.3577)	
training:	Epoch: [6][148/233]	Loss 0.2921 (0.3572)	
training:	Epoch: [6][149/233]	Loss 0.4333 (0.3577)	
training:	Epoch: [6][150/233]	Loss 0.2733 (0.3572)	
training:	Epoch: [6][151/233]	Loss 0.4737 (0.3579)	
training:	Epoch: [6][152/233]	Loss 0.3235 (0.3577)	
training:	Epoch: [6][153/233]	Loss 0.4155 (0.3581)	
training:	Epoch: [6][154/233]	Loss 0.2953 (0.3577)	
training:	Epoch: [6][155/233]	Loss 0.3593 (0.3577)	
training:	Epoch: [6][156/233]	Loss 0.4455 (0.3583)	
training:	Epoch: [6][157/233]	Loss 0.3534 (0.3582)	
training:	Epoch: [6][158/233]	Loss 0.3085 (0.3579)	
training:	Epoch: [6][159/233]	Loss 0.3950 (0.3582)	
training:	Epoch: [6][160/233]	Loss 0.3957 (0.3584)	
training:	Epoch: [6][161/233]	Loss 0.2965 (0.3580)	
training:	Epoch: [6][162/233]	Loss 0.4021 (0.3583)	
training:	Epoch: [6][163/233]	Loss 0.3409 (0.3582)	
training:	Epoch: [6][164/233]	Loss 0.3801 (0.3583)	
training:	Epoch: [6][165/233]	Loss 0.3450 (0.3582)	
training:	Epoch: [6][166/233]	Loss 0.4159 (0.3586)	
training:	Epoch: [6][167/233]	Loss 0.3592 (0.3586)	
training:	Epoch: [6][168/233]	Loss 0.3707 (0.3586)	
training:	Epoch: [6][169/233]	Loss 0.4169 (0.3590)	
training:	Epoch: [6][170/233]	Loss 0.2914 (0.3586)	
training:	Epoch: [6][171/233]	Loss 0.3707 (0.3587)	
training:	Epoch: [6][172/233]	Loss 0.3180 (0.3584)	
training:	Epoch: [6][173/233]	Loss 0.3897 (0.3586)	
training:	Epoch: [6][174/233]	Loss 0.3649 (0.3586)	
training:	Epoch: [6][175/233]	Loss 0.3644 (0.3587)	
training:	Epoch: [6][176/233]	Loss 0.3934 (0.3589)	
training:	Epoch: [6][177/233]	Loss 0.2926 (0.3585)	
training:	Epoch: [6][178/233]	Loss 0.3083 (0.3582)	
training:	Epoch: [6][179/233]	Loss 0.4971 (0.3590)	
training:	Epoch: [6][180/233]	Loss 0.2849 (0.3586)	
training:	Epoch: [6][181/233]	Loss 0.4215 (0.3589)	
training:	Epoch: [6][182/233]	Loss 0.3483 (0.3589)	
training:	Epoch: [6][183/233]	Loss 0.5138 (0.3597)	
training:	Epoch: [6][184/233]	Loss 0.2755 (0.3593)	
training:	Epoch: [6][185/233]	Loss 0.4250 (0.3596)	
training:	Epoch: [6][186/233]	Loss 0.3271 (0.3594)	
training:	Epoch: [6][187/233]	Loss 0.5197 (0.3603)	
training:	Epoch: [6][188/233]	Loss 0.3592 (0.3603)	
training:	Epoch: [6][189/233]	Loss 0.3048 (0.3600)	
training:	Epoch: [6][190/233]	Loss 0.3832 (0.3601)	
training:	Epoch: [6][191/233]	Loss 0.3698 (0.3602)	
training:	Epoch: [6][192/233]	Loss 0.3378 (0.3601)	
training:	Epoch: [6][193/233]	Loss 0.3995 (0.3603)	
training:	Epoch: [6][194/233]	Loss 0.3113 (0.3600)	
training:	Epoch: [6][195/233]	Loss 0.4026 (0.3602)	
training:	Epoch: [6][196/233]	Loss 0.3434 (0.3601)	
training:	Epoch: [6][197/233]	Loss 0.4198 (0.3604)	
training:	Epoch: [6][198/233]	Loss 0.3945 (0.3606)	
training:	Epoch: [6][199/233]	Loss 0.4977 (0.3613)	
training:	Epoch: [6][200/233]	Loss 0.4324 (0.3617)	
training:	Epoch: [6][201/233]	Loss 0.3151 (0.3614)	
training:	Epoch: [6][202/233]	Loss 0.4671 (0.3619)	
training:	Epoch: [6][203/233]	Loss 0.4633 (0.3624)	
training:	Epoch: [6][204/233]	Loss 0.3471 (0.3624)	
training:	Epoch: [6][205/233]	Loss 0.3302 (0.3622)	
training:	Epoch: [6][206/233]	Loss 0.2431 (0.3616)	
training:	Epoch: [6][207/233]	Loss 0.3075 (0.3614)	
training:	Epoch: [6][208/233]	Loss 0.3316 (0.3612)	
training:	Epoch: [6][209/233]	Loss 0.3885 (0.3614)	
training:	Epoch: [6][210/233]	Loss 0.3100 (0.3611)	
training:	Epoch: [6][211/233]	Loss 0.3267 (0.3610)	
training:	Epoch: [6][212/233]	Loss 0.3819 (0.3611)	
training:	Epoch: [6][213/233]	Loss 0.3904 (0.3612)	
training:	Epoch: [6][214/233]	Loss 0.3333 (0.3611)	
training:	Epoch: [6][215/233]	Loss 0.2341 (0.3605)	
training:	Epoch: [6][216/233]	Loss 0.3204 (0.3603)	
training:	Epoch: [6][217/233]	Loss 0.4493 (0.3607)	
training:	Epoch: [6][218/233]	Loss 0.3724 (0.3608)	
training:	Epoch: [6][219/233]	Loss 0.3868 (0.3609)	
training:	Epoch: [6][220/233]	Loss 0.4581 (0.3613)	
training:	Epoch: [6][221/233]	Loss 0.4635 (0.3618)	
training:	Epoch: [6][222/233]	Loss 0.4708 (0.3623)	
training:	Epoch: [6][223/233]	Loss 0.2696 (0.3618)	
training:	Epoch: [6][224/233]	Loss 0.3298 (0.3617)	
training:	Epoch: [6][225/233]	Loss 0.2881 (0.3614)	
training:	Epoch: [6][226/233]	Loss 0.3998 (0.3615)	
training:	Epoch: [6][227/233]	Loss 0.2888 (0.3612)	
training:	Epoch: [6][228/233]	Loss 0.2860 (0.3609)	
training:	Epoch: [6][229/233]	Loss 0.5221 (0.3616)	
training:	Epoch: [6][230/233]	Loss 0.4679 (0.3621)	
training:	Epoch: [6][231/233]	Loss 0.3133 (0.3619)	
training:	Epoch: [6][232/233]	Loss 0.4981 (0.3624)	
training:	Epoch: [6][233/233]	Loss 0.3992 (0.3626)	
Training:	 Loss: 0.3618

Training:	 ACC: 0.8883 0.8904 0.9359 0.8407
Validation:	 ACC: 0.7877 0.7913 0.8669 0.7085
Validation:	 Best_BACC: 0.7877 0.7913 0.8669 0.7085
Validation:	 Loss: 0.4471
Pretraining:	Epoch 7/200
----------
training:	Epoch: [7][1/233]	Loss 0.3352 (0.3352)	
training:	Epoch: [7][2/233]	Loss 0.4320 (0.3836)	
training:	Epoch: [7][3/233]	Loss 0.3614 (0.3762)	
training:	Epoch: [7][4/233]	Loss 0.2968 (0.3564)	
training:	Epoch: [7][5/233]	Loss 0.3863 (0.3624)	
training:	Epoch: [7][6/233]	Loss 0.4226 (0.3724)	
training:	Epoch: [7][7/233]	Loss 0.3682 (0.3718)	
training:	Epoch: [7][8/233]	Loss 0.3448 (0.3684)	
training:	Epoch: [7][9/233]	Loss 0.2442 (0.3546)	
training:	Epoch: [7][10/233]	Loss 0.3800 (0.3571)	
training:	Epoch: [7][11/233]	Loss 0.3847 (0.3597)	
training:	Epoch: [7][12/233]	Loss 0.4376 (0.3661)	
training:	Epoch: [7][13/233]	Loss 0.3359 (0.3638)	
training:	Epoch: [7][14/233]	Loss 0.3301 (0.3614)	
training:	Epoch: [7][15/233]	Loss 0.3049 (0.3576)	
training:	Epoch: [7][16/233]	Loss 0.3286 (0.3558)	
training:	Epoch: [7][17/233]	Loss 0.3234 (0.3539)	
training:	Epoch: [7][18/233]	Loss 0.3575 (0.3541)	
training:	Epoch: [7][19/233]	Loss 0.3185 (0.3522)	
training:	Epoch: [7][20/233]	Loss 0.2516 (0.3472)	
training:	Epoch: [7][21/233]	Loss 0.3360 (0.3467)	
training:	Epoch: [7][22/233]	Loss 0.3436 (0.3465)	
training:	Epoch: [7][23/233]	Loss 0.3070 (0.3448)	
training:	Epoch: [7][24/233]	Loss 0.2364 (0.3403)	
training:	Epoch: [7][25/233]	Loss 0.3665 (0.3414)	
training:	Epoch: [7][26/233]	Loss 0.2265 (0.3369)	
training:	Epoch: [7][27/233]	Loss 0.3139 (0.3361)	
training:	Epoch: [7][28/233]	Loss 0.2789 (0.3340)	
training:	Epoch: [7][29/233]	Loss 0.4003 (0.3363)	
training:	Epoch: [7][30/233]	Loss 0.3631 (0.3372)	
training:	Epoch: [7][31/233]	Loss 0.3188 (0.3366)	
training:	Epoch: [7][32/233]	Loss 0.2813 (0.3349)	
training:	Epoch: [7][33/233]	Loss 0.3747 (0.3361)	
training:	Epoch: [7][34/233]	Loss 0.3754 (0.3373)	
training:	Epoch: [7][35/233]	Loss 0.4088 (0.3393)	
training:	Epoch: [7][36/233]	Loss 0.3705 (0.3402)	
training:	Epoch: [7][37/233]	Loss 0.3172 (0.3395)	
training:	Epoch: [7][38/233]	Loss 0.3217 (0.3391)	
training:	Epoch: [7][39/233]	Loss 0.3760 (0.3400)	
training:	Epoch: [7][40/233]	Loss 0.3292 (0.3398)	
training:	Epoch: [7][41/233]	Loss 0.2996 (0.3388)	
training:	Epoch: [7][42/233]	Loss 0.3088 (0.3381)	
training:	Epoch: [7][43/233]	Loss 0.2769 (0.3366)	
training:	Epoch: [7][44/233]	Loss 0.2850 (0.3355)	
training:	Epoch: [7][45/233]	Loss 0.3079 (0.3349)	
training:	Epoch: [7][46/233]	Loss 0.3413 (0.3350)	
training:	Epoch: [7][47/233]	Loss 0.3975 (0.3363)	
training:	Epoch: [7][48/233]	Loss 0.3692 (0.3370)	
training:	Epoch: [7][49/233]	Loss 0.4785 (0.3399)	
training:	Epoch: [7][50/233]	Loss 0.2640 (0.3384)	
training:	Epoch: [7][51/233]	Loss 0.3151 (0.3379)	
training:	Epoch: [7][52/233]	Loss 0.2568 (0.3364)	
training:	Epoch: [7][53/233]	Loss 0.3980 (0.3375)	
training:	Epoch: [7][54/233]	Loss 0.3011 (0.3368)	
training:	Epoch: [7][55/233]	Loss 0.3065 (0.3363)	
training:	Epoch: [7][56/233]	Loss 0.3627 (0.3368)	
training:	Epoch: [7][57/233]	Loss 0.2777 (0.3357)	
training:	Epoch: [7][58/233]	Loss 0.3850 (0.3366)	
training:	Epoch: [7][59/233]	Loss 0.3362 (0.3366)	
training:	Epoch: [7][60/233]	Loss 0.3054 (0.3361)	
training:	Epoch: [7][61/233]	Loss 0.3320 (0.3360)	
training:	Epoch: [7][62/233]	Loss 0.3537 (0.3363)	
training:	Epoch: [7][63/233]	Loss 0.3561 (0.3366)	
training:	Epoch: [7][64/233]	Loss 0.3586 (0.3369)	
training:	Epoch: [7][65/233]	Loss 0.3449 (0.3371)	
training:	Epoch: [7][66/233]	Loss 0.3574 (0.3374)	
training:	Epoch: [7][67/233]	Loss 0.2944 (0.3367)	
training:	Epoch: [7][68/233]	Loss 0.3453 (0.3369)	
training:	Epoch: [7][69/233]	Loss 0.4374 (0.3383)	
training:	Epoch: [7][70/233]	Loss 0.3296 (0.3382)	
training:	Epoch: [7][71/233]	Loss 0.3629 (0.3385)	
training:	Epoch: [7][72/233]	Loss 0.3390 (0.3385)	
training:	Epoch: [7][73/233]	Loss 0.3856 (0.3392)	
training:	Epoch: [7][74/233]	Loss 0.2616 (0.3381)	
training:	Epoch: [7][75/233]	Loss 0.2468 (0.3369)	
training:	Epoch: [7][76/233]	Loss 0.3958 (0.3377)	
training:	Epoch: [7][77/233]	Loss 0.3094 (0.3373)	
training:	Epoch: [7][78/233]	Loss 0.4129 (0.3383)	
training:	Epoch: [7][79/233]	Loss 0.3157 (0.3380)	
training:	Epoch: [7][80/233]	Loss 0.2903 (0.3374)	
training:	Epoch: [7][81/233]	Loss 0.3384 (0.3374)	
training:	Epoch: [7][82/233]	Loss 0.3133 (0.3371)	
training:	Epoch: [7][83/233]	Loss 0.3202 (0.3369)	
training:	Epoch: [7][84/233]	Loss 0.3073 (0.3366)	
training:	Epoch: [7][85/233]	Loss 0.3531 (0.3368)	
training:	Epoch: [7][86/233]	Loss 0.2808 (0.3361)	
training:	Epoch: [7][87/233]	Loss 0.2942 (0.3356)	
training:	Epoch: [7][88/233]	Loss 0.3663 (0.3360)	
training:	Epoch: [7][89/233]	Loss 0.3520 (0.3362)	
training:	Epoch: [7][90/233]	Loss 0.3353 (0.3362)	
training:	Epoch: [7][91/233]	Loss 0.3602 (0.3364)	
training:	Epoch: [7][92/233]	Loss 0.3558 (0.3366)	
training:	Epoch: [7][93/233]	Loss 0.3285 (0.3365)	
training:	Epoch: [7][94/233]	Loss 0.2774 (0.3359)	
training:	Epoch: [7][95/233]	Loss 0.3723 (0.3363)	
training:	Epoch: [7][96/233]	Loss 0.3350 (0.3363)	
training:	Epoch: [7][97/233]	Loss 0.4398 (0.3373)	
training:	Epoch: [7][98/233]	Loss 0.3654 (0.3376)	
training:	Epoch: [7][99/233]	Loss 0.3235 (0.3375)	
training:	Epoch: [7][100/233]	Loss 0.3885 (0.3380)	
training:	Epoch: [7][101/233]	Loss 0.3125 (0.3377)	
training:	Epoch: [7][102/233]	Loss 0.3850 (0.3382)	
training:	Epoch: [7][103/233]	Loss 0.3243 (0.3381)	
training:	Epoch: [7][104/233]	Loss 0.2874 (0.3376)	
training:	Epoch: [7][105/233]	Loss 0.3026 (0.3373)	
training:	Epoch: [7][106/233]	Loss 0.2537 (0.3365)	
training:	Epoch: [7][107/233]	Loss 0.2665 (0.3358)	
training:	Epoch: [7][108/233]	Loss 0.2894 (0.3354)	
training:	Epoch: [7][109/233]	Loss 0.3041 (0.3351)	
training:	Epoch: [7][110/233]	Loss 0.2583 (0.3344)	
training:	Epoch: [7][111/233]	Loss 0.3701 (0.3347)	
training:	Epoch: [7][112/233]	Loss 0.2531 (0.3340)	
training:	Epoch: [7][113/233]	Loss 0.3081 (0.3338)	
training:	Epoch: [7][114/233]	Loss 0.3272 (0.3337)	
training:	Epoch: [7][115/233]	Loss 0.4299 (0.3345)	
training:	Epoch: [7][116/233]	Loss 0.2853 (0.3341)	
training:	Epoch: [7][117/233]	Loss 0.2735 (0.3336)	
training:	Epoch: [7][118/233]	Loss 0.3304 (0.3336)	
training:	Epoch: [7][119/233]	Loss 0.2294 (0.3327)	
training:	Epoch: [7][120/233]	Loss 0.3683 (0.3330)	
training:	Epoch: [7][121/233]	Loss 0.3168 (0.3329)	
training:	Epoch: [7][122/233]	Loss 0.2523 (0.3322)	
training:	Epoch: [7][123/233]	Loss 0.4534 (0.3332)	
training:	Epoch: [7][124/233]	Loss 0.3091 (0.3330)	
training:	Epoch: [7][125/233]	Loss 0.3021 (0.3327)	
training:	Epoch: [7][126/233]	Loss 0.2871 (0.3324)	
training:	Epoch: [7][127/233]	Loss 0.2945 (0.3321)	
training:	Epoch: [7][128/233]	Loss 0.3460 (0.3322)	
training:	Epoch: [7][129/233]	Loss 0.3827 (0.3326)	
training:	Epoch: [7][130/233]	Loss 0.4346 (0.3334)	
training:	Epoch: [7][131/233]	Loss 0.2723 (0.3329)	
training:	Epoch: [7][132/233]	Loss 0.3739 (0.3332)	
training:	Epoch: [7][133/233]	Loss 0.3025 (0.3330)	
training:	Epoch: [7][134/233]	Loss 0.3459 (0.3331)	
training:	Epoch: [7][135/233]	Loss 0.2073 (0.3321)	
training:	Epoch: [7][136/233]	Loss 0.3907 (0.3326)	
training:	Epoch: [7][137/233]	Loss 0.3700 (0.3329)	
training:	Epoch: [7][138/233]	Loss 0.2619 (0.3323)	
training:	Epoch: [7][139/233]	Loss 0.2598 (0.3318)	
training:	Epoch: [7][140/233]	Loss 0.3443 (0.3319)	
training:	Epoch: [7][141/233]	Loss 0.3670 (0.3322)	
training:	Epoch: [7][142/233]	Loss 0.2896 (0.3319)	
training:	Epoch: [7][143/233]	Loss 0.4161 (0.3324)	
training:	Epoch: [7][144/233]	Loss 0.2536 (0.3319)	
training:	Epoch: [7][145/233]	Loss 0.3262 (0.3319)	
training:	Epoch: [7][146/233]	Loss 0.3836 (0.3322)	
training:	Epoch: [7][147/233]	Loss 0.4676 (0.3331)	
training:	Epoch: [7][148/233]	Loss 0.2809 (0.3328)	
training:	Epoch: [7][149/233]	Loss 0.2482 (0.3322)	
training:	Epoch: [7][150/233]	Loss 0.3771 (0.3325)	
training:	Epoch: [7][151/233]	Loss 0.2853 (0.3322)	
training:	Epoch: [7][152/233]	Loss 0.3001 (0.3320)	
training:	Epoch: [7][153/233]	Loss 0.2673 (0.3316)	
training:	Epoch: [7][154/233]	Loss 0.3864 (0.3319)	
training:	Epoch: [7][155/233]	Loss 0.3185 (0.3318)	
training:	Epoch: [7][156/233]	Loss 0.3566 (0.3320)	
training:	Epoch: [7][157/233]	Loss 0.3334 (0.3320)	
training:	Epoch: [7][158/233]	Loss 0.3578 (0.3322)	
training:	Epoch: [7][159/233]	Loss 0.3430 (0.3322)	
training:	Epoch: [7][160/233]	Loss 0.2904 (0.3320)	
training:	Epoch: [7][161/233]	Loss 0.3172 (0.3319)	
training:	Epoch: [7][162/233]	Loss 0.3230 (0.3318)	
training:	Epoch: [7][163/233]	Loss 0.2653 (0.3314)	
training:	Epoch: [7][164/233]	Loss 0.5220 (0.3326)	
training:	Epoch: [7][165/233]	Loss 0.3695 (0.3328)	
training:	Epoch: [7][166/233]	Loss 0.4001 (0.3332)	
training:	Epoch: [7][167/233]	Loss 0.2887 (0.3329)	
training:	Epoch: [7][168/233]	Loss 0.3160 (0.3328)	
training:	Epoch: [7][169/233]	Loss 0.3703 (0.3331)	
training:	Epoch: [7][170/233]	Loss 0.2996 (0.3329)	
training:	Epoch: [7][171/233]	Loss 0.2555 (0.3324)	
training:	Epoch: [7][172/233]	Loss 0.3769 (0.3327)	
training:	Epoch: [7][173/233]	Loss 0.3149 (0.3326)	
training:	Epoch: [7][174/233]	Loss 0.3248 (0.3325)	
training:	Epoch: [7][175/233]	Loss 0.3974 (0.3329)	
training:	Epoch: [7][176/233]	Loss 0.2903 (0.3327)	
training:	Epoch: [7][177/233]	Loss 0.2418 (0.3321)	
training:	Epoch: [7][178/233]	Loss 0.3140 (0.3320)	
training:	Epoch: [7][179/233]	Loss 0.2561 (0.3316)	
training:	Epoch: [7][180/233]	Loss 0.3590 (0.3318)	
training:	Epoch: [7][181/233]	Loss 0.4396 (0.3324)	
training:	Epoch: [7][182/233]	Loss 0.3590 (0.3325)	
training:	Epoch: [7][183/233]	Loss 0.4534 (0.3332)	
training:	Epoch: [7][184/233]	Loss 0.3445 (0.3332)	
training:	Epoch: [7][185/233]	Loss 0.3637 (0.3334)	
training:	Epoch: [7][186/233]	Loss 0.3285 (0.3334)	
training:	Epoch: [7][187/233]	Loss 0.2935 (0.3332)	
training:	Epoch: [7][188/233]	Loss 0.3562 (0.3333)	
training:	Epoch: [7][189/233]	Loss 0.3386 (0.3333)	
training:	Epoch: [7][190/233]	Loss 0.3260 (0.3333)	
training:	Epoch: [7][191/233]	Loss 0.4506 (0.3339)	
training:	Epoch: [7][192/233]	Loss 0.3272 (0.3338)	
training:	Epoch: [7][193/233]	Loss 0.2824 (0.3336)	
training:	Epoch: [7][194/233]	Loss 0.4561 (0.3342)	
training:	Epoch: [7][195/233]	Loss 0.4377 (0.3347)	
training:	Epoch: [7][196/233]	Loss 0.3306 (0.3347)	
training:	Epoch: [7][197/233]	Loss 0.4049 (0.3351)	
training:	Epoch: [7][198/233]	Loss 0.4046 (0.3354)	
training:	Epoch: [7][199/233]	Loss 0.4605 (0.3361)	
training:	Epoch: [7][200/233]	Loss 0.3955 (0.3364)	
training:	Epoch: [7][201/233]	Loss 0.4211 (0.3368)	
training:	Epoch: [7][202/233]	Loss 0.3497 (0.3368)	
training:	Epoch: [7][203/233]	Loss 0.4646 (0.3375)	
training:	Epoch: [7][204/233]	Loss 0.2930 (0.3373)	
training:	Epoch: [7][205/233]	Loss 0.4278 (0.3377)	
training:	Epoch: [7][206/233]	Loss 0.4555 (0.3383)	
training:	Epoch: [7][207/233]	Loss 0.4488 (0.3388)	
training:	Epoch: [7][208/233]	Loss 0.3180 (0.3387)	
training:	Epoch: [7][209/233]	Loss 0.3994 (0.3390)	
training:	Epoch: [7][210/233]	Loss 0.2936 (0.3388)	
training:	Epoch: [7][211/233]	Loss 0.3204 (0.3387)	
training:	Epoch: [7][212/233]	Loss 0.4115 (0.3390)	
training:	Epoch: [7][213/233]	Loss 0.2584 (0.3387)	
training:	Epoch: [7][214/233]	Loss 0.2747 (0.3384)	
training:	Epoch: [7][215/233]	Loss 0.3851 (0.3386)	
training:	Epoch: [7][216/233]	Loss 0.3308 (0.3385)	
training:	Epoch: [7][217/233]	Loss 0.3338 (0.3385)	
training:	Epoch: [7][218/233]	Loss 0.2651 (0.3382)	
training:	Epoch: [7][219/233]	Loss 0.4172 (0.3385)	
training:	Epoch: [7][220/233]	Loss 0.2759 (0.3383)	
training:	Epoch: [7][221/233]	Loss 0.3611 (0.3384)	
training:	Epoch: [7][222/233]	Loss 0.3593 (0.3384)	
training:	Epoch: [7][223/233]	Loss 0.3811 (0.3386)	
training:	Epoch: [7][224/233]	Loss 0.3402 (0.3386)	
training:	Epoch: [7][225/233]	Loss 0.3840 (0.3388)	
training:	Epoch: [7][226/233]	Loss 0.3272 (0.3388)	
training:	Epoch: [7][227/233]	Loss 0.3239 (0.3387)	
training:	Epoch: [7][228/233]	Loss 0.3786 (0.3389)	
training:	Epoch: [7][229/233]	Loss 0.3630 (0.3390)	
training:	Epoch: [7][230/233]	Loss 0.2492 (0.3386)	
training:	Epoch: [7][231/233]	Loss 0.2900 (0.3384)	
training:	Epoch: [7][232/233]	Loss 0.3269 (0.3384)	
training:	Epoch: [7][233/233]	Loss 0.2375 (0.3379)	
Training:	 Loss: 0.3372

Training:	 ACC: 0.9058 0.9074 0.9413 0.8703
Validation:	 ACC: 0.7942 0.7972 0.8608 0.7276
Validation:	 Best_BACC: 0.7942 0.7972 0.8608 0.7276
Validation:	 Loss: 0.4354
Pretraining:	Epoch 8/200
----------
training:	Epoch: [8][1/233]	Loss 0.2300 (0.2300)	
training:	Epoch: [8][2/233]	Loss 0.2730 (0.2515)	
training:	Epoch: [8][3/233]	Loss 0.2948 (0.2660)	
training:	Epoch: [8][4/233]	Loss 0.2149 (0.2532)	
training:	Epoch: [8][5/233]	Loss 0.3677 (0.2761)	
training:	Epoch: [8][6/233]	Loss 0.2750 (0.2759)	
training:	Epoch: [8][7/233]	Loss 0.4413 (0.2995)	
training:	Epoch: [8][8/233]	Loss 0.5181 (0.3269)	
training:	Epoch: [8][9/233]	Loss 0.2686 (0.3204)	
training:	Epoch: [8][10/233]	Loss 0.3621 (0.3246)	
training:	Epoch: [8][11/233]	Loss 0.2738 (0.3199)	
training:	Epoch: [8][12/233]	Loss 0.4555 (0.3312)	
training:	Epoch: [8][13/233]	Loss 0.2277 (0.3233)	
training:	Epoch: [8][14/233]	Loss 0.2938 (0.3212)	
training:	Epoch: [8][15/233]	Loss 0.3883 (0.3256)	
training:	Epoch: [8][16/233]	Loss 0.2949 (0.3237)	
training:	Epoch: [8][17/233]	Loss 0.2531 (0.3196)	
training:	Epoch: [8][18/233]	Loss 0.3690 (0.3223)	
training:	Epoch: [8][19/233]	Loss 0.3672 (0.3247)	
training:	Epoch: [8][20/233]	Loss 0.2962 (0.3232)	
training:	Epoch: [8][21/233]	Loss 0.3563 (0.3248)	
training:	Epoch: [8][22/233]	Loss 0.3002 (0.3237)	
training:	Epoch: [8][23/233]	Loss 0.2092 (0.3187)	
training:	Epoch: [8][24/233]	Loss 0.2735 (0.3168)	
training:	Epoch: [8][25/233]	Loss 0.3676 (0.3189)	
training:	Epoch: [8][26/233]	Loss 0.3416 (0.3197)	
training:	Epoch: [8][27/233]	Loss 0.3882 (0.3223)	
training:	Epoch: [8][28/233]	Loss 0.3512 (0.3233)	
training:	Epoch: [8][29/233]	Loss 0.3783 (0.3252)	
training:	Epoch: [8][30/233]	Loss 0.1977 (0.3210)	
training:	Epoch: [8][31/233]	Loss 0.3700 (0.3225)	
training:	Epoch: [8][32/233]	Loss 0.3720 (0.3241)	
training:	Epoch: [8][33/233]	Loss 0.2898 (0.3231)	
training:	Epoch: [8][34/233]	Loss 0.2654 (0.3214)	
training:	Epoch: [8][35/233]	Loss 0.3072 (0.3210)	
training:	Epoch: [8][36/233]	Loss 0.2604 (0.3193)	
training:	Epoch: [8][37/233]	Loss 0.3345 (0.3197)	
training:	Epoch: [8][38/233]	Loss 0.2332 (0.3174)	
training:	Epoch: [8][39/233]	Loss 0.2575 (0.3159)	
training:	Epoch: [8][40/233]	Loss 0.2322 (0.3138)	
training:	Epoch: [8][41/233]	Loss 0.3607 (0.3149)	
training:	Epoch: [8][42/233]	Loss 0.2662 (0.3138)	
training:	Epoch: [8][43/233]	Loss 0.4310 (0.3165)	
training:	Epoch: [8][44/233]	Loss 0.4370 (0.3192)	
training:	Epoch: [8][45/233]	Loss 0.3564 (0.3201)	
training:	Epoch: [8][46/233]	Loss 0.3885 (0.3215)	
training:	Epoch: [8][47/233]	Loss 0.4392 (0.3240)	
training:	Epoch: [8][48/233]	Loss 0.3457 (0.3245)	
training:	Epoch: [8][49/233]	Loss 0.3204 (0.3244)	
training:	Epoch: [8][50/233]	Loss 0.3331 (0.3246)	
training:	Epoch: [8][51/233]	Loss 0.2912 (0.3239)	
training:	Epoch: [8][52/233]	Loss 0.3016 (0.3235)	
training:	Epoch: [8][53/233]	Loss 0.3206 (0.3234)	
training:	Epoch: [8][54/233]	Loss 0.2505 (0.3221)	
training:	Epoch: [8][55/233]	Loss 0.2848 (0.3214)	
training:	Epoch: [8][56/233]	Loss 0.3542 (0.3220)	
training:	Epoch: [8][57/233]	Loss 0.4566 (0.3244)	
training:	Epoch: [8][58/233]	Loss 0.2796 (0.3236)	
training:	Epoch: [8][59/233]	Loss 0.3293 (0.3237)	
training:	Epoch: [8][60/233]	Loss 0.4220 (0.3253)	
training:	Epoch: [8][61/233]	Loss 0.2700 (0.3244)	
training:	Epoch: [8][62/233]	Loss 0.3225 (0.3244)	
training:	Epoch: [8][63/233]	Loss 0.2968 (0.3240)	
training:	Epoch: [8][64/233]	Loss 0.2798 (0.3233)	
training:	Epoch: [8][65/233]	Loss 0.4368 (0.3250)	
training:	Epoch: [8][66/233]	Loss 0.3043 (0.3247)	
training:	Epoch: [8][67/233]	Loss 0.2557 (0.3237)	
training:	Epoch: [8][68/233]	Loss 0.4293 (0.3252)	
training:	Epoch: [8][69/233]	Loss 0.5362 (0.3283)	
training:	Epoch: [8][70/233]	Loss 0.2964 (0.3278)	
training:	Epoch: [8][71/233]	Loss 0.2958 (0.3274)	
training:	Epoch: [8][72/233]	Loss 0.3067 (0.3271)	
training:	Epoch: [8][73/233]	Loss 0.4095 (0.3282)	
training:	Epoch: [8][74/233]	Loss 0.3554 (0.3286)	
training:	Epoch: [8][75/233]	Loss 0.2970 (0.3282)	
training:	Epoch: [8][76/233]	Loss 0.2813 (0.3275)	
training:	Epoch: [8][77/233]	Loss 0.2797 (0.3269)	
training:	Epoch: [8][78/233]	Loss 0.3305 (0.3270)	
training:	Epoch: [8][79/233]	Loss 0.3114 (0.3268)	
training:	Epoch: [8][80/233]	Loss 0.3096 (0.3266)	
training:	Epoch: [8][81/233]	Loss 0.3183 (0.3265)	
training:	Epoch: [8][82/233]	Loss 0.3347 (0.3266)	
training:	Epoch: [8][83/233]	Loss 0.3431 (0.3268)	
training:	Epoch: [8][84/233]	Loss 0.2451 (0.3258)	
training:	Epoch: [8][85/233]	Loss 0.2538 (0.3249)	
training:	Epoch: [8][86/233]	Loss 0.3381 (0.3251)	
training:	Epoch: [8][87/233]	Loss 0.3567 (0.3254)	
training:	Epoch: [8][88/233]	Loss 0.3378 (0.3256)	
training:	Epoch: [8][89/233]	Loss 0.3523 (0.3259)	
training:	Epoch: [8][90/233]	Loss 0.2690 (0.3253)	
training:	Epoch: [8][91/233]	Loss 0.3520 (0.3256)	
training:	Epoch: [8][92/233]	Loss 0.2809 (0.3251)	
training:	Epoch: [8][93/233]	Loss 0.2494 (0.3243)	
training:	Epoch: [8][94/233]	Loss 0.3284 (0.3243)	
training:	Epoch: [8][95/233]	Loss 0.2826 (0.3239)	
training:	Epoch: [8][96/233]	Loss 0.3966 (0.3246)	
training:	Epoch: [8][97/233]	Loss 0.3016 (0.3244)	
training:	Epoch: [8][98/233]	Loss 0.3088 (0.3242)	
training:	Epoch: [8][99/233]	Loss 0.3164 (0.3241)	
training:	Epoch: [8][100/233]	Loss 0.3432 (0.3243)	
training:	Epoch: [8][101/233]	Loss 0.2504 (0.3236)	
training:	Epoch: [8][102/233]	Loss 0.2916 (0.3233)	
training:	Epoch: [8][103/233]	Loss 0.3279 (0.3233)	
training:	Epoch: [8][104/233]	Loss 0.3467 (0.3236)	
training:	Epoch: [8][105/233]	Loss 0.3414 (0.3237)	
training:	Epoch: [8][106/233]	Loss 0.2735 (0.3233)	
training:	Epoch: [8][107/233]	Loss 0.3480 (0.3235)	
training:	Epoch: [8][108/233]	Loss 0.3847 (0.3241)	
training:	Epoch: [8][109/233]	Loss 0.2355 (0.3232)	
training:	Epoch: [8][110/233]	Loss 0.2719 (0.3228)	
training:	Epoch: [8][111/233]	Loss 0.2777 (0.3224)	
training:	Epoch: [8][112/233]	Loss 0.3331 (0.3225)	
training:	Epoch: [8][113/233]	Loss 0.2756 (0.3220)	
training:	Epoch: [8][114/233]	Loss 0.3278 (0.3221)	
training:	Epoch: [8][115/233]	Loss 0.2375 (0.3214)	
training:	Epoch: [8][116/233]	Loss 0.3012 (0.3212)	
training:	Epoch: [8][117/233]	Loss 0.4229 (0.3221)	
training:	Epoch: [8][118/233]	Loss 0.2563 (0.3215)	
training:	Epoch: [8][119/233]	Loss 0.2688 (0.3211)	
training:	Epoch: [8][120/233]	Loss 0.3503 (0.3213)	
training:	Epoch: [8][121/233]	Loss 0.2445 (0.3207)	
training:	Epoch: [8][122/233]	Loss 0.3013 (0.3205)	
training:	Epoch: [8][123/233]	Loss 0.3567 (0.3208)	
training:	Epoch: [8][124/233]	Loss 0.3661 (0.3212)	
training:	Epoch: [8][125/233]	Loss 0.2625 (0.3207)	
training:	Epoch: [8][126/233]	Loss 0.3398 (0.3208)	
training:	Epoch: [8][127/233]	Loss 0.2559 (0.3203)	
training:	Epoch: [8][128/233]	Loss 0.2876 (0.3201)	
training:	Epoch: [8][129/233]	Loss 0.2640 (0.3196)	
training:	Epoch: [8][130/233]	Loss 0.2707 (0.3193)	
training:	Epoch: [8][131/233]	Loss 0.2810 (0.3190)	
training:	Epoch: [8][132/233]	Loss 0.3675 (0.3193)	
training:	Epoch: [8][133/233]	Loss 0.3870 (0.3199)	
training:	Epoch: [8][134/233]	Loss 0.3246 (0.3199)	
training:	Epoch: [8][135/233]	Loss 0.2897 (0.3197)	
training:	Epoch: [8][136/233]	Loss 0.3065 (0.3196)	
training:	Epoch: [8][137/233]	Loss 0.3905 (0.3201)	
training:	Epoch: [8][138/233]	Loss 0.4201 (0.3208)	
training:	Epoch: [8][139/233]	Loss 0.3434 (0.3210)	
training:	Epoch: [8][140/233]	Loss 0.2763 (0.3207)	
training:	Epoch: [8][141/233]	Loss 0.2831 (0.3204)	
training:	Epoch: [8][142/233]	Loss 0.3716 (0.3208)	
training:	Epoch: [8][143/233]	Loss 0.3490 (0.3209)	
training:	Epoch: [8][144/233]	Loss 0.2709 (0.3206)	
training:	Epoch: [8][145/233]	Loss 0.3170 (0.3206)	
training:	Epoch: [8][146/233]	Loss 0.3036 (0.3205)	
training:	Epoch: [8][147/233]	Loss 0.2954 (0.3203)	
training:	Epoch: [8][148/233]	Loss 0.2636 (0.3199)	
training:	Epoch: [8][149/233]	Loss 0.3269 (0.3200)	
training:	Epoch: [8][150/233]	Loss 0.3097 (0.3199)	
training:	Epoch: [8][151/233]	Loss 0.3574 (0.3201)	
training:	Epoch: [8][152/233]	Loss 0.3496 (0.3203)	
training:	Epoch: [8][153/233]	Loss 0.4114 (0.3209)	
training:	Epoch: [8][154/233]	Loss 0.4297 (0.3216)	
training:	Epoch: [8][155/233]	Loss 0.2110 (0.3209)	
training:	Epoch: [8][156/233]	Loss 0.3708 (0.3212)	
training:	Epoch: [8][157/233]	Loss 0.2470 (0.3208)	
training:	Epoch: [8][158/233]	Loss 0.3057 (0.3207)	
training:	Epoch: [8][159/233]	Loss 0.3181 (0.3207)	
training:	Epoch: [8][160/233]	Loss 0.2676 (0.3203)	
training:	Epoch: [8][161/233]	Loss 0.3299 (0.3204)	
training:	Epoch: [8][162/233]	Loss 0.2782 (0.3201)	
training:	Epoch: [8][163/233]	Loss 0.3367 (0.3202)	
training:	Epoch: [8][164/233]	Loss 0.2678 (0.3199)	
training:	Epoch: [8][165/233]	Loss 0.3311 (0.3200)	
training:	Epoch: [8][166/233]	Loss 0.2723 (0.3197)	
training:	Epoch: [8][167/233]	Loss 0.3097 (0.3196)	
training:	Epoch: [8][168/233]	Loss 0.2370 (0.3191)	
training:	Epoch: [8][169/233]	Loss 0.3101 (0.3191)	
training:	Epoch: [8][170/233]	Loss 0.2819 (0.3189)	
training:	Epoch: [8][171/233]	Loss 0.3448 (0.3190)	
training:	Epoch: [8][172/233]	Loss 0.2809 (0.3188)	
training:	Epoch: [8][173/233]	Loss 0.3049 (0.3187)	
training:	Epoch: [8][174/233]	Loss 0.3581 (0.3189)	
training:	Epoch: [8][175/233]	Loss 0.2489 (0.3185)	
training:	Epoch: [8][176/233]	Loss 0.3899 (0.3189)	
training:	Epoch: [8][177/233]	Loss 0.2843 (0.3187)	
training:	Epoch: [8][178/233]	Loss 0.3444 (0.3189)	
training:	Epoch: [8][179/233]	Loss 0.3759 (0.3192)	
training:	Epoch: [8][180/233]	Loss 0.4828 (0.3201)	
training:	Epoch: [8][181/233]	Loss 0.3801 (0.3204)	
training:	Epoch: [8][182/233]	Loss 0.2299 (0.3199)	
training:	Epoch: [8][183/233]	Loss 0.4005 (0.3204)	
training:	Epoch: [8][184/233]	Loss 0.3733 (0.3207)	
training:	Epoch: [8][185/233]	Loss 0.3949 (0.3211)	
training:	Epoch: [8][186/233]	Loss 0.2729 (0.3208)	
training:	Epoch: [8][187/233]	Loss 0.2989 (0.3207)	
training:	Epoch: [8][188/233]	Loss 0.2245 (0.3202)	
training:	Epoch: [8][189/233]	Loss 0.2793 (0.3200)	
training:	Epoch: [8][190/233]	Loss 0.2287 (0.3195)	
training:	Epoch: [8][191/233]	Loss 0.3635 (0.3197)	
training:	Epoch: [8][192/233]	Loss 0.2492 (0.3194)	
training:	Epoch: [8][193/233]	Loss 0.3926 (0.3197)	
training:	Epoch: [8][194/233]	Loss 0.3792 (0.3200)	
training:	Epoch: [8][195/233]	Loss 0.2875 (0.3199)	
training:	Epoch: [8][196/233]	Loss 0.3682 (0.3201)	
training:	Epoch: [8][197/233]	Loss 0.2505 (0.3198)	
training:	Epoch: [8][198/233]	Loss 0.3099 (0.3197)	
training:	Epoch: [8][199/233]	Loss 0.3424 (0.3198)	
training:	Epoch: [8][200/233]	Loss 0.3296 (0.3199)	
training:	Epoch: [8][201/233]	Loss 0.3170 (0.3199)	
training:	Epoch: [8][202/233]	Loss 0.2079 (0.3193)	
training:	Epoch: [8][203/233]	Loss 0.3470 (0.3194)	
training:	Epoch: [8][204/233]	Loss 0.3597 (0.3196)	
training:	Epoch: [8][205/233]	Loss 0.4332 (0.3202)	
training:	Epoch: [8][206/233]	Loss 0.2838 (0.3200)	
training:	Epoch: [8][207/233]	Loss 0.2818 (0.3198)	
training:	Epoch: [8][208/233]	Loss 0.2744 (0.3196)	
training:	Epoch: [8][209/233]	Loss 0.2908 (0.3195)	
training:	Epoch: [8][210/233]	Loss 0.3067 (0.3194)	
training:	Epoch: [8][211/233]	Loss 0.2579 (0.3191)	
training:	Epoch: [8][212/233]	Loss 0.3174 (0.3191)	
training:	Epoch: [8][213/233]	Loss 0.3316 (0.3192)	
training:	Epoch: [8][214/233]	Loss 0.2525 (0.3189)	
training:	Epoch: [8][215/233]	Loss 0.2342 (0.3185)	
training:	Epoch: [8][216/233]	Loss 0.3816 (0.3188)	
training:	Epoch: [8][217/233]	Loss 0.3037 (0.3187)	
training:	Epoch: [8][218/233]	Loss 0.2621 (0.3184)	
training:	Epoch: [8][219/233]	Loss 0.3640 (0.3186)	
training:	Epoch: [8][220/233]	Loss 0.3398 (0.3187)	
training:	Epoch: [8][221/233]	Loss 0.4134 (0.3192)	
training:	Epoch: [8][222/233]	Loss 0.3479 (0.3193)	
training:	Epoch: [8][223/233]	Loss 0.3336 (0.3194)	
training:	Epoch: [8][224/233]	Loss 0.2648 (0.3191)	
training:	Epoch: [8][225/233]	Loss 0.3196 (0.3191)	
training:	Epoch: [8][226/233]	Loss 0.2623 (0.3189)	
training:	Epoch: [8][227/233]	Loss 0.3476 (0.3190)	
training:	Epoch: [8][228/233]	Loss 0.3320 (0.3191)	
training:	Epoch: [8][229/233]	Loss 0.3102 (0.3190)	
training:	Epoch: [8][230/233]	Loss 0.3905 (0.3193)	
training:	Epoch: [8][231/233]	Loss 0.3447 (0.3194)	
training:	Epoch: [8][232/233]	Loss 0.3200 (0.3194)	
training:	Epoch: [8][233/233]	Loss 0.2560 (0.3192)	
Training:	 Loss: 0.3184

Training:	 ACC: 0.9090 0.9111 0.9580 0.8600
Validation:	 ACC: 0.7990 0.8031 0.8895 0.7085
Validation:	 Best_BACC: 0.7990 0.8031 0.8895 0.7085
Validation:	 Loss: 0.4344
Pretraining:	Epoch 9/200
----------
training:	Epoch: [9][1/233]	Loss 0.3987 (0.3987)	
training:	Epoch: [9][2/233]	Loss 0.2888 (0.3437)	
training:	Epoch: [9][3/233]	Loss 0.2764 (0.3213)	
training:	Epoch: [9][4/233]	Loss 0.2522 (0.3040)	
training:	Epoch: [9][5/233]	Loss 0.3380 (0.3108)	
training:	Epoch: [9][6/233]	Loss 0.2566 (0.3018)	
training:	Epoch: [9][7/233]	Loss 0.2909 (0.3002)	
training:	Epoch: [9][8/233]	Loss 0.2778 (0.2974)	
training:	Epoch: [9][9/233]	Loss 0.2739 (0.2948)	
training:	Epoch: [9][10/233]	Loss 0.1887 (0.2842)	
training:	Epoch: [9][11/233]	Loss 0.3220 (0.2877)	
training:	Epoch: [9][12/233]	Loss 0.2426 (0.2839)	
training:	Epoch: [9][13/233]	Loss 0.3531 (0.2892)	
training:	Epoch: [9][14/233]	Loss 0.4061 (0.2976)	
training:	Epoch: [9][15/233]	Loss 0.2912 (0.2971)	
training:	Epoch: [9][16/233]	Loss 0.3164 (0.2983)	
training:	Epoch: [9][17/233]	Loss 0.3324 (0.3004)	
training:	Epoch: [9][18/233]	Loss 0.3247 (0.3017)	
training:	Epoch: [9][19/233]	Loss 0.2737 (0.3002)	
training:	Epoch: [9][20/233]	Loss 0.3969 (0.3051)	
training:	Epoch: [9][21/233]	Loss 0.3290 (0.3062)	
training:	Epoch: [9][22/233]	Loss 0.2624 (0.3042)	
training:	Epoch: [9][23/233]	Loss 0.3067 (0.3043)	
training:	Epoch: [9][24/233]	Loss 0.3341 (0.3056)	
training:	Epoch: [9][25/233]	Loss 0.2311 (0.3026)	
training:	Epoch: [9][26/233]	Loss 0.3619 (0.3049)	
training:	Epoch: [9][27/233]	Loss 0.3569 (0.3068)	
training:	Epoch: [9][28/233]	Loss 0.3212 (0.3073)	
training:	Epoch: [9][29/233]	Loss 0.2786 (0.3063)	
training:	Epoch: [9][30/233]	Loss 0.1958 (0.3026)	
training:	Epoch: [9][31/233]	Loss 0.1896 (0.2990)	
training:	Epoch: [9][32/233]	Loss 0.3597 (0.3009)	
training:	Epoch: [9][33/233]	Loss 0.3702 (0.3030)	
training:	Epoch: [9][34/233]	Loss 0.3084 (0.3031)	
training:	Epoch: [9][35/233]	Loss 0.2919 (0.3028)	
training:	Epoch: [9][36/233]	Loss 0.2655 (0.3018)	
training:	Epoch: [9][37/233]	Loss 0.2706 (0.3009)	
training:	Epoch: [9][38/233]	Loss 0.2749 (0.3003)	
training:	Epoch: [9][39/233]	Loss 0.3654 (0.3019)	
training:	Epoch: [9][40/233]	Loss 0.2359 (0.3003)	
training:	Epoch: [9][41/233]	Loss 0.3245 (0.3009)	
training:	Epoch: [9][42/233]	Loss 0.3167 (0.3012)	
training:	Epoch: [9][43/233]	Loss 0.2764 (0.3007)	
training:	Epoch: [9][44/233]	Loss 0.3060 (0.3008)	
training:	Epoch: [9][45/233]	Loss 0.3798 (0.3025)	
training:	Epoch: [9][46/233]	Loss 0.3896 (0.3044)	
training:	Epoch: [9][47/233]	Loss 0.2553 (0.3034)	
training:	Epoch: [9][48/233]	Loss 0.2623 (0.3025)	
training:	Epoch: [9][49/233]	Loss 0.2941 (0.3024)	
training:	Epoch: [9][50/233]	Loss 0.2759 (0.3018)	
training:	Epoch: [9][51/233]	Loss 0.2301 (0.3004)	
training:	Epoch: [9][52/233]	Loss 0.2798 (0.3000)	
training:	Epoch: [9][53/233]	Loss 0.2319 (0.2987)	
training:	Epoch: [9][54/233]	Loss 0.2082 (0.2971)	
training:	Epoch: [9][55/233]	Loss 0.2786 (0.2967)	
training:	Epoch: [9][56/233]	Loss 0.3114 (0.2970)	
training:	Epoch: [9][57/233]	Loss 0.2909 (0.2969)	
training:	Epoch: [9][58/233]	Loss 0.3433 (0.2977)	
training:	Epoch: [9][59/233]	Loss 0.3609 (0.2988)	
training:	Epoch: [9][60/233]	Loss 0.2743 (0.2983)	
training:	Epoch: [9][61/233]	Loss 0.3055 (0.2985)	
training:	Epoch: [9][62/233]	Loss 0.2787 (0.2981)	
training:	Epoch: [9][63/233]	Loss 0.2797 (0.2979)	
training:	Epoch: [9][64/233]	Loss 0.3079 (0.2980)	
training:	Epoch: [9][65/233]	Loss 0.2408 (0.2971)	
training:	Epoch: [9][66/233]	Loss 0.2496 (0.2964)	
training:	Epoch: [9][67/233]	Loss 0.3162 (0.2967)	
training:	Epoch: [9][68/233]	Loss 0.3199 (0.2970)	
training:	Epoch: [9][69/233]	Loss 0.2706 (0.2967)	
training:	Epoch: [9][70/233]	Loss 0.2508 (0.2960)	
training:	Epoch: [9][71/233]	Loss 0.3179 (0.2963)	
training:	Epoch: [9][72/233]	Loss 0.3567 (0.2972)	
training:	Epoch: [9][73/233]	Loss 0.3262 (0.2976)	
training:	Epoch: [9][74/233]	Loss 0.2129 (0.2964)	
training:	Epoch: [9][75/233]	Loss 0.3050 (0.2965)	
training:	Epoch: [9][76/233]	Loss 0.2138 (0.2954)	
training:	Epoch: [9][77/233]	Loss 0.2768 (0.2952)	
training:	Epoch: [9][78/233]	Loss 0.2624 (0.2948)	
training:	Epoch: [9][79/233]	Loss 0.3085 (0.2949)	
training:	Epoch: [9][80/233]	Loss 0.4088 (0.2964)	
training:	Epoch: [9][81/233]	Loss 0.3646 (0.2972)	
training:	Epoch: [9][82/233]	Loss 0.2703 (0.2969)	
training:	Epoch: [9][83/233]	Loss 0.3433 (0.2974)	
training:	Epoch: [9][84/233]	Loss 0.3143 (0.2976)	
training:	Epoch: [9][85/233]	Loss 0.3233 (0.2979)	
training:	Epoch: [9][86/233]	Loss 0.2923 (0.2979)	
training:	Epoch: [9][87/233]	Loss 0.3039 (0.2979)	
training:	Epoch: [9][88/233]	Loss 0.2712 (0.2976)	
training:	Epoch: [9][89/233]	Loss 0.3356 (0.2981)	
training:	Epoch: [9][90/233]	Loss 0.2320 (0.2973)	
training:	Epoch: [9][91/233]	Loss 0.2613 (0.2969)	
training:	Epoch: [9][92/233]	Loss 0.3174 (0.2972)	
training:	Epoch: [9][93/233]	Loss 0.3288 (0.2975)	
training:	Epoch: [9][94/233]	Loss 0.3059 (0.2976)	
training:	Epoch: [9][95/233]	Loss 0.2311 (0.2969)	
training:	Epoch: [9][96/233]	Loss 0.3060 (0.2970)	
training:	Epoch: [9][97/233]	Loss 0.5691 (0.2998)	
training:	Epoch: [9][98/233]	Loss 0.2817 (0.2996)	
training:	Epoch: [9][99/233]	Loss 0.2781 (0.2994)	
training:	Epoch: [9][100/233]	Loss 0.2721 (0.2991)	
training:	Epoch: [9][101/233]	Loss 0.2845 (0.2990)	
training:	Epoch: [9][102/233]	Loss 0.3384 (0.2994)	
training:	Epoch: [9][103/233]	Loss 0.2778 (0.2992)	
training:	Epoch: [9][104/233]	Loss 0.2924 (0.2991)	
training:	Epoch: [9][105/233]	Loss 0.3314 (0.2994)	
training:	Epoch: [9][106/233]	Loss 0.3286 (0.2997)	
training:	Epoch: [9][107/233]	Loss 0.2706 (0.2994)	
training:	Epoch: [9][108/233]	Loss 0.3345 (0.2997)	
training:	Epoch: [9][109/233]	Loss 0.3224 (0.2999)	
training:	Epoch: [9][110/233]	Loss 0.3032 (0.3000)	
training:	Epoch: [9][111/233]	Loss 0.2818 (0.2998)	
training:	Epoch: [9][112/233]	Loss 0.3253 (0.3000)	
training:	Epoch: [9][113/233]	Loss 0.2983 (0.3000)	
training:	Epoch: [9][114/233]	Loss 0.3455 (0.3004)	
training:	Epoch: [9][115/233]	Loss 0.2680 (0.3001)	
training:	Epoch: [9][116/233]	Loss 0.4699 (0.3016)	
training:	Epoch: [9][117/233]	Loss 0.2436 (0.3011)	
training:	Epoch: [9][118/233]	Loss 0.3263 (0.3013)	
training:	Epoch: [9][119/233]	Loss 0.3625 (0.3018)	
training:	Epoch: [9][120/233]	Loss 0.2768 (0.3016)	
training:	Epoch: [9][121/233]	Loss 0.2993 (0.3016)	
training:	Epoch: [9][122/233]	Loss 0.2621 (0.3013)	
training:	Epoch: [9][123/233]	Loss 0.2518 (0.3009)	
training:	Epoch: [9][124/233]	Loss 0.3155 (0.3010)	
training:	Epoch: [9][125/233]	Loss 0.2847 (0.3009)	
training:	Epoch: [9][126/233]	Loss 0.3643 (0.3014)	
training:	Epoch: [9][127/233]	Loss 0.3662 (0.3019)	
training:	Epoch: [9][128/233]	Loss 0.3439 (0.3022)	
training:	Epoch: [9][129/233]	Loss 0.3220 (0.3024)	
training:	Epoch: [9][130/233]	Loss 0.2714 (0.3021)	
training:	Epoch: [9][131/233]	Loss 0.3503 (0.3025)	
training:	Epoch: [9][132/233]	Loss 0.3139 (0.3026)	
training:	Epoch: [9][133/233]	Loss 0.2613 (0.3023)	
training:	Epoch: [9][134/233]	Loss 0.3207 (0.3024)	
training:	Epoch: [9][135/233]	Loss 0.2745 (0.3022)	
training:	Epoch: [9][136/233]	Loss 0.2807 (0.3020)	
training:	Epoch: [9][137/233]	Loss 0.2735 (0.3018)	
training:	Epoch: [9][138/233]	Loss 0.2323 (0.3013)	
training:	Epoch: [9][139/233]	Loss 0.3397 (0.3016)	
training:	Epoch: [9][140/233]	Loss 0.3450 (0.3019)	
training:	Epoch: [9][141/233]	Loss 0.3455 (0.3022)	
training:	Epoch: [9][142/233]	Loss 0.2686 (0.3020)	
training:	Epoch: [9][143/233]	Loss 0.2553 (0.3017)	
training:	Epoch: [9][144/233]	Loss 0.2636 (0.3014)	
training:	Epoch: [9][145/233]	Loss 0.2015 (0.3007)	
training:	Epoch: [9][146/233]	Loss 0.2896 (0.3006)	
training:	Epoch: [9][147/233]	Loss 0.2943 (0.3006)	
training:	Epoch: [9][148/233]	Loss 0.2894 (0.3005)	
training:	Epoch: [9][149/233]	Loss 0.2395 (0.3001)	
training:	Epoch: [9][150/233]	Loss 0.2975 (0.3001)	
training:	Epoch: [9][151/233]	Loss 0.4316 (0.3010)	
training:	Epoch: [9][152/233]	Loss 0.2992 (0.3009)	
training:	Epoch: [9][153/233]	Loss 0.3900 (0.3015)	
training:	Epoch: [9][154/233]	Loss 0.2843 (0.3014)	
training:	Epoch: [9][155/233]	Loss 0.3081 (0.3015)	
training:	Epoch: [9][156/233]	Loss 0.3057 (0.3015)	
training:	Epoch: [9][157/233]	Loss 0.3231 (0.3016)	
training:	Epoch: [9][158/233]	Loss 0.2546 (0.3013)	
training:	Epoch: [9][159/233]	Loss 0.2811 (0.3012)	
training:	Epoch: [9][160/233]	Loss 0.2165 (0.3007)	
training:	Epoch: [9][161/233]	Loss 0.2306 (0.3002)	
training:	Epoch: [9][162/233]	Loss 0.2672 (0.3000)	
training:	Epoch: [9][163/233]	Loss 0.2826 (0.2999)	
training:	Epoch: [9][164/233]	Loss 0.2281 (0.2995)	
training:	Epoch: [9][165/233]	Loss 0.3585 (0.2998)	
training:	Epoch: [9][166/233]	Loss 0.2821 (0.2997)	
training:	Epoch: [9][167/233]	Loss 0.2610 (0.2995)	
training:	Epoch: [9][168/233]	Loss 0.4464 (0.3004)	
training:	Epoch: [9][169/233]	Loss 0.3489 (0.3007)	
training:	Epoch: [9][170/233]	Loss 0.3111 (0.3007)	
training:	Epoch: [9][171/233]	Loss 0.2443 (0.3004)	
training:	Epoch: [9][172/233]	Loss 0.2441 (0.3001)	
training:	Epoch: [9][173/233]	Loss 0.3369 (0.3003)	
training:	Epoch: [9][174/233]	Loss 0.4318 (0.3010)	
training:	Epoch: [9][175/233]	Loss 0.2869 (0.3010)	
training:	Epoch: [9][176/233]	Loss 0.2554 (0.3007)	
training:	Epoch: [9][177/233]	Loss 0.3237 (0.3008)	
training:	Epoch: [9][178/233]	Loss 0.2911 (0.3008)	
training:	Epoch: [9][179/233]	Loss 0.2644 (0.3006)	
training:	Epoch: [9][180/233]	Loss 0.3061 (0.3006)	
training:	Epoch: [9][181/233]	Loss 0.2473 (0.3003)	
training:	Epoch: [9][182/233]	Loss 0.4239 (0.3010)	
training:	Epoch: [9][183/233]	Loss 0.3183 (0.3011)	
training:	Epoch: [9][184/233]	Loss 0.2875 (0.3010)	
training:	Epoch: [9][185/233]	Loss 0.2644 (0.3008)	
training:	Epoch: [9][186/233]	Loss 0.3516 (0.3011)	
training:	Epoch: [9][187/233]	Loss 0.3114 (0.3011)	
training:	Epoch: [9][188/233]	Loss 0.2849 (0.3010)	
training:	Epoch: [9][189/233]	Loss 0.2455 (0.3008)	
training:	Epoch: [9][190/233]	Loss 0.2082 (0.3003)	
training:	Epoch: [9][191/233]	Loss 0.3773 (0.3007)	
training:	Epoch: [9][192/233]	Loss 0.2822 (0.3006)	
training:	Epoch: [9][193/233]	Loss 0.3099 (0.3006)	
training:	Epoch: [9][194/233]	Loss 0.3364 (0.3008)	
training:	Epoch: [9][195/233]	Loss 0.2769 (0.3007)	
training:	Epoch: [9][196/233]	Loss 0.2584 (0.3005)	
training:	Epoch: [9][197/233]	Loss 0.3256 (0.3006)	
training:	Epoch: [9][198/233]	Loss 0.3009 (0.3006)	
training:	Epoch: [9][199/233]	Loss 0.3579 (0.3009)	
training:	Epoch: [9][200/233]	Loss 0.2653 (0.3007)	
training:	Epoch: [9][201/233]	Loss 0.3020 (0.3007)	
training:	Epoch: [9][202/233]	Loss 0.3328 (0.3009)	
training:	Epoch: [9][203/233]	Loss 0.2725 (0.3007)	
training:	Epoch: [9][204/233]	Loss 0.2735 (0.3006)	
training:	Epoch: [9][205/233]	Loss 0.3515 (0.3008)	
training:	Epoch: [9][206/233]	Loss 0.2643 (0.3007)	
training:	Epoch: [9][207/233]	Loss 0.2992 (0.3007)	
training:	Epoch: [9][208/233]	Loss 0.2629 (0.3005)	
training:	Epoch: [9][209/233]	Loss 0.3891 (0.3009)	
training:	Epoch: [9][210/233]	Loss 0.2577 (0.3007)	
training:	Epoch: [9][211/233]	Loss 0.2399 (0.3004)	
training:	Epoch: [9][212/233]	Loss 0.3522 (0.3007)	
training:	Epoch: [9][213/233]	Loss 0.2260 (0.3003)	
training:	Epoch: [9][214/233]	Loss 0.3069 (0.3003)	
training:	Epoch: [9][215/233]	Loss 0.2677 (0.3002)	
training:	Epoch: [9][216/233]	Loss 0.3904 (0.3006)	
training:	Epoch: [9][217/233]	Loss 0.2888 (0.3005)	
training:	Epoch: [9][218/233]	Loss 0.3052 (0.3006)	
training:	Epoch: [9][219/233]	Loss 0.3016 (0.3006)	
training:	Epoch: [9][220/233]	Loss 0.2368 (0.3003)	
training:	Epoch: [9][221/233]	Loss 0.3502 (0.3005)	
training:	Epoch: [9][222/233]	Loss 0.3060 (0.3005)	
training:	Epoch: [9][223/233]	Loss 0.2646 (0.3004)	
training:	Epoch: [9][224/233]	Loss 0.4728 (0.3011)	
training:	Epoch: [9][225/233]	Loss 0.2800 (0.3010)	
training:	Epoch: [9][226/233]	Loss 0.3044 (0.3011)	
training:	Epoch: [9][227/233]	Loss 0.3316 (0.3012)	
training:	Epoch: [9][228/233]	Loss 0.2615 (0.3010)	
training:	Epoch: [9][229/233]	Loss 0.2041 (0.3006)	
training:	Epoch: [9][230/233]	Loss 0.4080 (0.3011)	
training:	Epoch: [9][231/233]	Loss 0.3575 (0.3013)	
training:	Epoch: [9][232/233]	Loss 0.2207 (0.3010)	
training:	Epoch: [9][233/233]	Loss 0.2557 (0.3008)	
Training:	 Loss: 0.3001

Training:	 ACC: 0.9309 0.9311 0.9352 0.9266
Validation:	 ACC: 0.8043 0.8058 0.8373 0.7713
Validation:	 Best_BACC: 0.8043 0.8058 0.8373 0.7713
Validation:	 Loss: 0.4158
Pretraining:	Epoch 10/200
----------
training:	Epoch: [10][1/233]	Loss 0.2159 (0.2159)	
training:	Epoch: [10][2/233]	Loss 0.2236 (0.2198)	
training:	Epoch: [10][3/233]	Loss 0.2768 (0.2388)	
training:	Epoch: [10][4/233]	Loss 0.2799 (0.2491)	
training:	Epoch: [10][5/233]	Loss 0.2469 (0.2486)	
training:	Epoch: [10][6/233]	Loss 0.4264 (0.2783)	
training:	Epoch: [10][7/233]	Loss 0.2768 (0.2781)	
training:	Epoch: [10][8/233]	Loss 0.2820 (0.2786)	
training:	Epoch: [10][9/233]	Loss 0.2988 (0.2808)	
training:	Epoch: [10][10/233]	Loss 0.3729 (0.2900)	
training:	Epoch: [10][11/233]	Loss 0.3147 (0.2923)	
training:	Epoch: [10][12/233]	Loss 0.2277 (0.2869)	
training:	Epoch: [10][13/233]	Loss 0.3403 (0.2910)	
training:	Epoch: [10][14/233]	Loss 0.2988 (0.2915)	
training:	Epoch: [10][15/233]	Loss 0.2634 (0.2897)	
training:	Epoch: [10][16/233]	Loss 0.4708 (0.3010)	
training:	Epoch: [10][17/233]	Loss 0.2855 (0.3001)	
training:	Epoch: [10][18/233]	Loss 0.2389 (0.2967)	
training:	Epoch: [10][19/233]	Loss 0.2971 (0.2967)	
training:	Epoch: [10][20/233]	Loss 0.2781 (0.2958)	
training:	Epoch: [10][21/233]	Loss 0.2476 (0.2935)	
training:	Epoch: [10][22/233]	Loss 0.2838 (0.2930)	
training:	Epoch: [10][23/233]	Loss 0.2699 (0.2920)	
training:	Epoch: [10][24/233]	Loss 0.2574 (0.2906)	
training:	Epoch: [10][25/233]	Loss 0.2281 (0.2881)	
training:	Epoch: [10][26/233]	Loss 0.2870 (0.2880)	
training:	Epoch: [10][27/233]	Loss 0.3106 (0.2889)	
training:	Epoch: [10][28/233]	Loss 0.3628 (0.2915)	
training:	Epoch: [10][29/233]	Loss 0.2404 (0.2898)	
training:	Epoch: [10][30/233]	Loss 0.2198 (0.2874)	
training:	Epoch: [10][31/233]	Loss 0.2825 (0.2873)	
training:	Epoch: [10][32/233]	Loss 0.3465 (0.2891)	
training:	Epoch: [10][33/233]	Loss 0.2810 (0.2889)	
training:	Epoch: [10][34/233]	Loss 0.2527 (0.2878)	
training:	Epoch: [10][35/233]	Loss 0.3907 (0.2907)	
training:	Epoch: [10][36/233]	Loss 0.2363 (0.2892)	
training:	Epoch: [10][37/233]	Loss 0.2781 (0.2889)	
training:	Epoch: [10][38/233]	Loss 0.2898 (0.2890)	
training:	Epoch: [10][39/233]	Loss 0.2236 (0.2873)	
training:	Epoch: [10][40/233]	Loss 0.3353 (0.2885)	
training:	Epoch: [10][41/233]	Loss 0.2874 (0.2885)	
training:	Epoch: [10][42/233]	Loss 0.2055 (0.2865)	
training:	Epoch: [10][43/233]	Loss 0.3417 (0.2878)	
training:	Epoch: [10][44/233]	Loss 0.3213 (0.2885)	
training:	Epoch: [10][45/233]	Loss 0.2955 (0.2887)	
training:	Epoch: [10][46/233]	Loss 0.3528 (0.2901)	
training:	Epoch: [10][47/233]	Loss 0.2659 (0.2896)	
training:	Epoch: [10][48/233]	Loss 0.2520 (0.2888)	
training:	Epoch: [10][49/233]	Loss 0.2975 (0.2890)	
training:	Epoch: [10][50/233]	Loss 0.3099 (0.2894)	
training:	Epoch: [10][51/233]	Loss 0.2534 (0.2887)	
training:	Epoch: [10][52/233]	Loss 0.2170 (0.2873)	
training:	Epoch: [10][53/233]	Loss 0.3285 (0.2881)	
training:	Epoch: [10][54/233]	Loss 0.2420 (0.2872)	
training:	Epoch: [10][55/233]	Loss 0.3886 (0.2891)	
training:	Epoch: [10][56/233]	Loss 0.2196 (0.2878)	
training:	Epoch: [10][57/233]	Loss 0.3324 (0.2886)	
training:	Epoch: [10][58/233]	Loss 0.1911 (0.2869)	
training:	Epoch: [10][59/233]	Loss 0.2781 (0.2868)	
training:	Epoch: [10][60/233]	Loss 0.2449 (0.2861)	
training:	Epoch: [10][61/233]	Loss 0.2073 (0.2848)	
training:	Epoch: [10][62/233]	Loss 0.2635 (0.2844)	
training:	Epoch: [10][63/233]	Loss 0.2293 (0.2836)	
training:	Epoch: [10][64/233]	Loss 0.2255 (0.2827)	
training:	Epoch: [10][65/233]	Loss 0.1732 (0.2810)	
training:	Epoch: [10][66/233]	Loss 0.3175 (0.2815)	
training:	Epoch: [10][67/233]	Loss 0.2499 (0.2811)	
training:	Epoch: [10][68/233]	Loss 0.2304 (0.2803)	
training:	Epoch: [10][69/233]	Loss 0.3214 (0.2809)	
training:	Epoch: [10][70/233]	Loss 0.3426 (0.2818)	
training:	Epoch: [10][71/233]	Loss 0.2274 (0.2810)	
training:	Epoch: [10][72/233]	Loss 0.4078 (0.2828)	
training:	Epoch: [10][73/233]	Loss 0.2169 (0.2819)	
training:	Epoch: [10][74/233]	Loss 0.2628 (0.2816)	
training:	Epoch: [10][75/233]	Loss 0.3633 (0.2827)	
training:	Epoch: [10][76/233]	Loss 0.2407 (0.2822)	
training:	Epoch: [10][77/233]	Loss 0.2242 (0.2814)	
training:	Epoch: [10][78/233]	Loss 0.3203 (0.2819)	
training:	Epoch: [10][79/233]	Loss 0.2801 (0.2819)	
training:	Epoch: [10][80/233]	Loss 0.3101 (0.2822)	
training:	Epoch: [10][81/233]	Loss 0.3685 (0.2833)	
training:	Epoch: [10][82/233]	Loss 0.3069 (0.2836)	
training:	Epoch: [10][83/233]	Loss 0.3523 (0.2844)	
training:	Epoch: [10][84/233]	Loss 0.2499 (0.2840)	
training:	Epoch: [10][85/233]	Loss 0.2551 (0.2837)	
training:	Epoch: [10][86/233]	Loss 0.2707 (0.2835)	
training:	Epoch: [10][87/233]	Loss 0.3055 (0.2838)	
training:	Epoch: [10][88/233]	Loss 0.2339 (0.2832)	
training:	Epoch: [10][89/233]	Loss 0.2429 (0.2827)	
training:	Epoch: [10][90/233]	Loss 0.3769 (0.2838)	
training:	Epoch: [10][91/233]	Loss 0.2480 (0.2834)	
training:	Epoch: [10][92/233]	Loss 0.2498 (0.2830)	
training:	Epoch: [10][93/233]	Loss 0.3110 (0.2833)	
training:	Epoch: [10][94/233]	Loss 0.3407 (0.2839)	
training:	Epoch: [10][95/233]	Loss 0.2630 (0.2837)	
training:	Epoch: [10][96/233]	Loss 0.3609 (0.2845)	
training:	Epoch: [10][97/233]	Loss 0.2766 (0.2844)	
training:	Epoch: [10][98/233]	Loss 0.2578 (0.2842)	
training:	Epoch: [10][99/233]	Loss 0.3064 (0.2844)	
training:	Epoch: [10][100/233]	Loss 0.3083 (0.2846)	
training:	Epoch: [10][101/233]	Loss 0.2221 (0.2840)	
training:	Epoch: [10][102/233]	Loss 0.3701 (0.2849)	
training:	Epoch: [10][103/233]	Loss 0.2499 (0.2845)	
training:	Epoch: [10][104/233]	Loss 0.2627 (0.2843)	
training:	Epoch: [10][105/233]	Loss 0.2679 (0.2842)	
training:	Epoch: [10][106/233]	Loss 0.2479 (0.2838)	
training:	Epoch: [10][107/233]	Loss 0.2643 (0.2836)	
training:	Epoch: [10][108/233]	Loss 0.2822 (0.2836)	
training:	Epoch: [10][109/233]	Loss 0.2277 (0.2831)	
training:	Epoch: [10][110/233]	Loss 0.2099 (0.2824)	
training:	Epoch: [10][111/233]	Loss 0.2024 (0.2817)	
training:	Epoch: [10][112/233]	Loss 0.3317 (0.2822)	
training:	Epoch: [10][113/233]	Loss 0.2547 (0.2819)	
training:	Epoch: [10][114/233]	Loss 0.2360 (0.2815)	
training:	Epoch: [10][115/233]	Loss 0.4188 (0.2827)	
training:	Epoch: [10][116/233]	Loss 0.3000 (0.2829)	
training:	Epoch: [10][117/233]	Loss 0.3189 (0.2832)	
training:	Epoch: [10][118/233]	Loss 0.3025 (0.2833)	
training:	Epoch: [10][119/233]	Loss 0.2893 (0.2834)	
training:	Epoch: [10][120/233]	Loss 0.2352 (0.2830)	
training:	Epoch: [10][121/233]	Loss 0.2959 (0.2831)	
training:	Epoch: [10][122/233]	Loss 0.2607 (0.2829)	
training:	Epoch: [10][123/233]	Loss 0.2208 (0.2824)	
training:	Epoch: [10][124/233]	Loss 0.2080 (0.2818)	
training:	Epoch: [10][125/233]	Loss 0.2873 (0.2818)	
training:	Epoch: [10][126/233]	Loss 0.1762 (0.2810)	
training:	Epoch: [10][127/233]	Loss 0.2823 (0.2810)	
training:	Epoch: [10][128/233]	Loss 0.2897 (0.2811)	
training:	Epoch: [10][129/233]	Loss 0.2880 (0.2811)	
training:	Epoch: [10][130/233]	Loss 0.3042 (0.2813)	
training:	Epoch: [10][131/233]	Loss 0.3469 (0.2818)	
training:	Epoch: [10][132/233]	Loss 0.3538 (0.2824)	
training:	Epoch: [10][133/233]	Loss 0.3031 (0.2825)	
training:	Epoch: [10][134/233]	Loss 0.2443 (0.2822)	
training:	Epoch: [10][135/233]	Loss 0.3645 (0.2828)	
training:	Epoch: [10][136/233]	Loss 0.2919 (0.2829)	
training:	Epoch: [10][137/233]	Loss 0.2688 (0.2828)	
training:	Epoch: [10][138/233]	Loss 0.2112 (0.2823)	
training:	Epoch: [10][139/233]	Loss 0.2740 (0.2822)	
training:	Epoch: [10][140/233]	Loss 0.2081 (0.2817)	
training:	Epoch: [10][141/233]	Loss 0.3404 (0.2821)	
training:	Epoch: [10][142/233]	Loss 0.2555 (0.2819)	
training:	Epoch: [10][143/233]	Loss 0.2647 (0.2818)	
training:	Epoch: [10][144/233]	Loss 0.2216 (0.2814)	
training:	Epoch: [10][145/233]	Loss 0.3238 (0.2817)	
training:	Epoch: [10][146/233]	Loss 0.2381 (0.2814)	
training:	Epoch: [10][147/233]	Loss 0.2968 (0.2815)	
training:	Epoch: [10][148/233]	Loss 0.3217 (0.2818)	
training:	Epoch: [10][149/233]	Loss 0.3211 (0.2820)	
training:	Epoch: [10][150/233]	Loss 0.4370 (0.2831)	
training:	Epoch: [10][151/233]	Loss 0.4595 (0.2842)	
training:	Epoch: [10][152/233]	Loss 0.2639 (0.2841)	
training:	Epoch: [10][153/233]	Loss 0.2315 (0.2837)	
training:	Epoch: [10][154/233]	Loss 0.3284 (0.2840)	
training:	Epoch: [10][155/233]	Loss 0.2925 (0.2841)	
training:	Epoch: [10][156/233]	Loss 0.2892 (0.2841)	
training:	Epoch: [10][157/233]	Loss 0.3194 (0.2843)	
training:	Epoch: [10][158/233]	Loss 0.3626 (0.2848)	
training:	Epoch: [10][159/233]	Loss 0.3244 (0.2851)	
training:	Epoch: [10][160/233]	Loss 0.2147 (0.2847)	
training:	Epoch: [10][161/233]	Loss 0.2676 (0.2845)	
training:	Epoch: [10][162/233]	Loss 0.2420 (0.2843)	
training:	Epoch: [10][163/233]	Loss 0.3327 (0.2846)	
training:	Epoch: [10][164/233]	Loss 0.2733 (0.2845)	
training:	Epoch: [10][165/233]	Loss 0.2861 (0.2845)	
training:	Epoch: [10][166/233]	Loss 0.2114 (0.2841)	
training:	Epoch: [10][167/233]	Loss 0.3047 (0.2842)	
training:	Epoch: [10][168/233]	Loss 0.3437 (0.2846)	
training:	Epoch: [10][169/233]	Loss 0.2446 (0.2843)	
training:	Epoch: [10][170/233]	Loss 0.3149 (0.2845)	
training:	Epoch: [10][171/233]	Loss 0.2683 (0.2844)	
training:	Epoch: [10][172/233]	Loss 0.3263 (0.2847)	
training:	Epoch: [10][173/233]	Loss 0.3148 (0.2848)	
training:	Epoch: [10][174/233]	Loss 0.2264 (0.2845)	
training:	Epoch: [10][175/233]	Loss 0.3467 (0.2848)	
training:	Epoch: [10][176/233]	Loss 0.2609 (0.2847)	
training:	Epoch: [10][177/233]	Loss 0.2191 (0.2843)	
training:	Epoch: [10][178/233]	Loss 0.2511 (0.2842)	
training:	Epoch: [10][179/233]	Loss 0.2508 (0.2840)	
training:	Epoch: [10][180/233]	Loss 0.2662 (0.2839)	
training:	Epoch: [10][181/233]	Loss 0.3634 (0.2843)	
training:	Epoch: [10][182/233]	Loss 0.4567 (0.2853)	
training:	Epoch: [10][183/233]	Loss 0.2523 (0.2851)	
training:	Epoch: [10][184/233]	Loss 0.2910 (0.2851)	
training:	Epoch: [10][185/233]	Loss 0.4111 (0.2858)	
training:	Epoch: [10][186/233]	Loss 0.3445 (0.2861)	
training:	Epoch: [10][187/233]	Loss 0.2798 (0.2861)	
training:	Epoch: [10][188/233]	Loss 0.2984 (0.2861)	
training:	Epoch: [10][189/233]	Loss 0.2512 (0.2860)	
training:	Epoch: [10][190/233]	Loss 0.2442 (0.2857)	
training:	Epoch: [10][191/233]	Loss 0.2576 (0.2856)	
training:	Epoch: [10][192/233]	Loss 0.2260 (0.2853)	
training:	Epoch: [10][193/233]	Loss 0.3114 (0.2854)	
training:	Epoch: [10][194/233]	Loss 0.3441 (0.2857)	
training:	Epoch: [10][195/233]	Loss 0.2459 (0.2855)	
training:	Epoch: [10][196/233]	Loss 0.2730 (0.2854)	
training:	Epoch: [10][197/233]	Loss 0.2469 (0.2852)	
training:	Epoch: [10][198/233]	Loss 0.2622 (0.2851)	
training:	Epoch: [10][199/233]	Loss 0.3254 (0.2853)	
training:	Epoch: [10][200/233]	Loss 0.3156 (0.2855)	
training:	Epoch: [10][201/233]	Loss 0.2214 (0.2852)	
training:	Epoch: [10][202/233]	Loss 0.2863 (0.2852)	
training:	Epoch: [10][203/233]	Loss 0.3103 (0.2853)	
training:	Epoch: [10][204/233]	Loss 0.3132 (0.2854)	
training:	Epoch: [10][205/233]	Loss 0.2841 (0.2854)	
training:	Epoch: [10][206/233]	Loss 0.3911 (0.2859)	
training:	Epoch: [10][207/233]	Loss 0.2821 (0.2859)	
training:	Epoch: [10][208/233]	Loss 0.3278 (0.2861)	
training:	Epoch: [10][209/233]	Loss 0.3312 (0.2863)	
training:	Epoch: [10][210/233]	Loss 0.2954 (0.2864)	
training:	Epoch: [10][211/233]	Loss 0.2770 (0.2863)	
training:	Epoch: [10][212/233]	Loss 0.3210 (0.2865)	
training:	Epoch: [10][213/233]	Loss 0.2359 (0.2863)	
training:	Epoch: [10][214/233]	Loss 0.2841 (0.2863)	
training:	Epoch: [10][215/233]	Loss 0.2878 (0.2863)	
training:	Epoch: [10][216/233]	Loss 0.3622 (0.2866)	
training:	Epoch: [10][217/233]	Loss 0.2449 (0.2864)	
training:	Epoch: [10][218/233]	Loss 0.2525 (0.2863)	
training:	Epoch: [10][219/233]	Loss 0.2897 (0.2863)	
training:	Epoch: [10][220/233]	Loss 0.3228 (0.2864)	
training:	Epoch: [10][221/233]	Loss 0.2520 (0.2863)	
training:	Epoch: [10][222/233]	Loss 0.3953 (0.2868)	
training:	Epoch: [10][223/233]	Loss 0.2316 (0.2865)	
training:	Epoch: [10][224/233]	Loss 0.3448 (0.2868)	
training:	Epoch: [10][225/233]	Loss 0.2491 (0.2866)	
training:	Epoch: [10][226/233]	Loss 0.3268 (0.2868)	
training:	Epoch: [10][227/233]	Loss 0.2309 (0.2866)	
training:	Epoch: [10][228/233]	Loss 0.5327 (0.2876)	
training:	Epoch: [10][229/233]	Loss 0.2885 (0.2876)	
training:	Epoch: [10][230/233]	Loss 0.3736 (0.2880)	
training:	Epoch: [10][231/233]	Loss 0.2899 (0.2880)	
training:	Epoch: [10][232/233]	Loss 0.2584 (0.2879)	
training:	Epoch: [10][233/233]	Loss 0.2659 (0.2878)	
Training:	 Loss: 0.2871

Training:	 ACC: 0.9394 0.9403 0.9605 0.9182
Validation:	 ACC: 0.8074 0.8101 0.8659 0.7489
Validation:	 Best_BACC: 0.8074 0.8101 0.8659 0.7489
Validation:	 Loss: 0.4156
Pretraining:	Epoch 11/200
----------
training:	Epoch: [11][1/233]	Loss 0.2047 (0.2047)	
training:	Epoch: [11][2/233]	Loss 0.2900 (0.2473)	
training:	Epoch: [11][3/233]	Loss 0.2289 (0.2412)	
training:	Epoch: [11][4/233]	Loss 0.2308 (0.2386)	
training:	Epoch: [11][5/233]	Loss 0.2530 (0.2415)	
training:	Epoch: [11][6/233]	Loss 0.2797 (0.2479)	
training:	Epoch: [11][7/233]	Loss 0.2964 (0.2548)	
training:	Epoch: [11][8/233]	Loss 0.3387 (0.2653)	
training:	Epoch: [11][9/233]	Loss 0.2199 (0.2602)	
training:	Epoch: [11][10/233]	Loss 0.2858 (0.2628)	
training:	Epoch: [11][11/233]	Loss 0.2289 (0.2597)	
training:	Epoch: [11][12/233]	Loss 0.3538 (0.2676)	
training:	Epoch: [11][13/233]	Loss 0.3073 (0.2706)	
training:	Epoch: [11][14/233]	Loss 0.2361 (0.2681)	
training:	Epoch: [11][15/233]	Loss 0.2766 (0.2687)	
training:	Epoch: [11][16/233]	Loss 0.2559 (0.2679)	
training:	Epoch: [11][17/233]	Loss 0.2458 (0.2666)	
training:	Epoch: [11][18/233]	Loss 0.2837 (0.2675)	
training:	Epoch: [11][19/233]	Loss 0.2648 (0.2674)	
training:	Epoch: [11][20/233]	Loss 0.2193 (0.2650)	
training:	Epoch: [11][21/233]	Loss 0.2282 (0.2632)	
training:	Epoch: [11][22/233]	Loss 0.3602 (0.2677)	
training:	Epoch: [11][23/233]	Loss 0.2082 (0.2651)	
training:	Epoch: [11][24/233]	Loss 0.2870 (0.2660)	
training:	Epoch: [11][25/233]	Loss 0.2046 (0.2635)	
training:	Epoch: [11][26/233]	Loss 0.2404 (0.2626)	
training:	Epoch: [11][27/233]	Loss 0.3766 (0.2669)	
training:	Epoch: [11][28/233]	Loss 0.3347 (0.2693)	
training:	Epoch: [11][29/233]	Loss 0.2259 (0.2678)	
training:	Epoch: [11][30/233]	Loss 0.1827 (0.2650)	
training:	Epoch: [11][31/233]	Loss 0.2682 (0.2651)	
training:	Epoch: [11][32/233]	Loss 0.2809 (0.2656)	
training:	Epoch: [11][33/233]	Loss 0.2355 (0.2646)	
training:	Epoch: [11][34/233]	Loss 0.2310 (0.2637)	
training:	Epoch: [11][35/233]	Loss 0.2408 (0.2630)	
training:	Epoch: [11][36/233]	Loss 0.2164 (0.2617)	
training:	Epoch: [11][37/233]	Loss 0.1997 (0.2600)	
training:	Epoch: [11][38/233]	Loss 0.2348 (0.2594)	
training:	Epoch: [11][39/233]	Loss 0.3240 (0.2610)	
training:	Epoch: [11][40/233]	Loss 0.3098 (0.2622)	
training:	Epoch: [11][41/233]	Loss 0.2905 (0.2629)	
training:	Epoch: [11][42/233]	Loss 0.2380 (0.2623)	
training:	Epoch: [11][43/233]	Loss 0.3504 (0.2644)	
training:	Epoch: [11][44/233]	Loss 0.2233 (0.2635)	
training:	Epoch: [11][45/233]	Loss 0.2054 (0.2622)	
training:	Epoch: [11][46/233]	Loss 0.2704 (0.2623)	
training:	Epoch: [11][47/233]	Loss 0.2492 (0.2621)	
training:	Epoch: [11][48/233]	Loss 0.2733 (0.2623)	
training:	Epoch: [11][49/233]	Loss 0.2939 (0.2629)	
training:	Epoch: [11][50/233]	Loss 0.2360 (0.2624)	
training:	Epoch: [11][51/233]	Loss 0.1556 (0.2603)	
training:	Epoch: [11][52/233]	Loss 0.2288 (0.2597)	
training:	Epoch: [11][53/233]	Loss 0.2687 (0.2599)	
training:	Epoch: [11][54/233]	Loss 0.2355 (0.2594)	
training:	Epoch: [11][55/233]	Loss 0.2051 (0.2584)	
training:	Epoch: [11][56/233]	Loss 0.2434 (0.2582)	
training:	Epoch: [11][57/233]	Loss 0.3505 (0.2598)	
training:	Epoch: [11][58/233]	Loss 0.3092 (0.2606)	
training:	Epoch: [11][59/233]	Loss 0.2519 (0.2605)	
training:	Epoch: [11][60/233]	Loss 0.2335 (0.2600)	
training:	Epoch: [11][61/233]	Loss 0.2370 (0.2597)	
training:	Epoch: [11][62/233]	Loss 0.3143 (0.2605)	
training:	Epoch: [11][63/233]	Loss 0.2882 (0.2610)	
training:	Epoch: [11][64/233]	Loss 0.2902 (0.2614)	
training:	Epoch: [11][65/233]	Loss 0.2602 (0.2614)	
training:	Epoch: [11][66/233]	Loss 0.3308 (0.2625)	
training:	Epoch: [11][67/233]	Loss 0.2205 (0.2618)	
training:	Epoch: [11][68/233]	Loss 0.4236 (0.2642)	
training:	Epoch: [11][69/233]	Loss 0.2254 (0.2637)	
training:	Epoch: [11][70/233]	Loss 0.2122 (0.2629)	
training:	Epoch: [11][71/233]	Loss 0.2255 (0.2624)	
training:	Epoch: [11][72/233]	Loss 0.3046 (0.2630)	
training:	Epoch: [11][73/233]	Loss 0.2810 (0.2632)	
training:	Epoch: [11][74/233]	Loss 0.2320 (0.2628)	
training:	Epoch: [11][75/233]	Loss 0.2659 (0.2629)	
training:	Epoch: [11][76/233]	Loss 0.3332 (0.2638)	
training:	Epoch: [11][77/233]	Loss 0.1993 (0.2629)	
training:	Epoch: [11][78/233]	Loss 0.3006 (0.2634)	
training:	Epoch: [11][79/233]	Loss 0.2581 (0.2634)	
training:	Epoch: [11][80/233]	Loss 0.2732 (0.2635)	
training:	Epoch: [11][81/233]	Loss 0.2598 (0.2634)	
training:	Epoch: [11][82/233]	Loss 0.3290 (0.2642)	
training:	Epoch: [11][83/233]	Loss 0.2269 (0.2638)	
training:	Epoch: [11][84/233]	Loss 0.2585 (0.2637)	
training:	Epoch: [11][85/233]	Loss 0.3002 (0.2642)	
training:	Epoch: [11][86/233]	Loss 0.3079 (0.2647)	
training:	Epoch: [11][87/233]	Loss 0.2409 (0.2644)	
training:	Epoch: [11][88/233]	Loss 0.2242 (0.2639)	
training:	Epoch: [11][89/233]	Loss 0.3484 (0.2649)	
training:	Epoch: [11][90/233]	Loss 0.2694 (0.2649)	
training:	Epoch: [11][91/233]	Loss 0.3016 (0.2653)	
training:	Epoch: [11][92/233]	Loss 0.3372 (0.2661)	
training:	Epoch: [11][93/233]	Loss 0.2823 (0.2663)	
training:	Epoch: [11][94/233]	Loss 0.2816 (0.2665)	
training:	Epoch: [11][95/233]	Loss 0.2966 (0.2668)	
training:	Epoch: [11][96/233]	Loss 0.2528 (0.2666)	
training:	Epoch: [11][97/233]	Loss 0.2487 (0.2664)	
training:	Epoch: [11][98/233]	Loss 0.2370 (0.2661)	
training:	Epoch: [11][99/233]	Loss 0.2310 (0.2658)	
training:	Epoch: [11][100/233]	Loss 0.2068 (0.2652)	
training:	Epoch: [11][101/233]	Loss 0.2942 (0.2655)	
training:	Epoch: [11][102/233]	Loss 0.2573 (0.2654)	
training:	Epoch: [11][103/233]	Loss 0.3399 (0.2661)	
training:	Epoch: [11][104/233]	Loss 0.2316 (0.2658)	
training:	Epoch: [11][105/233]	Loss 0.2436 (0.2656)	
training:	Epoch: [11][106/233]	Loss 0.2628 (0.2656)	
training:	Epoch: [11][107/233]	Loss 0.2792 (0.2657)	
training:	Epoch: [11][108/233]	Loss 0.2667 (0.2657)	
training:	Epoch: [11][109/233]	Loss 0.2192 (0.2653)	
training:	Epoch: [11][110/233]	Loss 0.2335 (0.2650)	
training:	Epoch: [11][111/233]	Loss 0.4562 (0.2667)	
training:	Epoch: [11][112/233]	Loss 0.2434 (0.2665)	
training:	Epoch: [11][113/233]	Loss 0.3818 (0.2675)	
training:	Epoch: [11][114/233]	Loss 0.2593 (0.2674)	
training:	Epoch: [11][115/233]	Loss 0.2298 (0.2671)	
training:	Epoch: [11][116/233]	Loss 0.2081 (0.2666)	
training:	Epoch: [11][117/233]	Loss 0.2288 (0.2663)	
training:	Epoch: [11][118/233]	Loss 0.2636 (0.2663)	
training:	Epoch: [11][119/233]	Loss 0.2328 (0.2660)	
training:	Epoch: [11][120/233]	Loss 0.2384 (0.2657)	
training:	Epoch: [11][121/233]	Loss 0.2897 (0.2659)	
training:	Epoch: [11][122/233]	Loss 0.2559 (0.2659)	
training:	Epoch: [11][123/233]	Loss 0.2785 (0.2660)	
training:	Epoch: [11][124/233]	Loss 0.2806 (0.2661)	
training:	Epoch: [11][125/233]	Loss 0.2264 (0.2658)	
training:	Epoch: [11][126/233]	Loss 0.3484 (0.2664)	
training:	Epoch: [11][127/233]	Loss 0.4121 (0.2676)	
training:	Epoch: [11][128/233]	Loss 0.2966 (0.2678)	
training:	Epoch: [11][129/233]	Loss 0.3149 (0.2682)	
training:	Epoch: [11][130/233]	Loss 0.2651 (0.2681)	
training:	Epoch: [11][131/233]	Loss 0.2644 (0.2681)	
training:	Epoch: [11][132/233]	Loss 0.1911 (0.2675)	
training:	Epoch: [11][133/233]	Loss 0.3761 (0.2683)	
training:	Epoch: [11][134/233]	Loss 0.2690 (0.2683)	
training:	Epoch: [11][135/233]	Loss 0.2667 (0.2683)	
training:	Epoch: [11][136/233]	Loss 0.2775 (0.2684)	
training:	Epoch: [11][137/233]	Loss 0.3570 (0.2690)	
training:	Epoch: [11][138/233]	Loss 0.2795 (0.2691)	
training:	Epoch: [11][139/233]	Loss 0.3322 (0.2696)	
training:	Epoch: [11][140/233]	Loss 0.2856 (0.2697)	
training:	Epoch: [11][141/233]	Loss 0.2360 (0.2695)	
training:	Epoch: [11][142/233]	Loss 0.4537 (0.2708)	
training:	Epoch: [11][143/233]	Loss 0.1880 (0.2702)	
training:	Epoch: [11][144/233]	Loss 0.2960 (0.2704)	
training:	Epoch: [11][145/233]	Loss 0.2940 (0.2705)	
training:	Epoch: [11][146/233]	Loss 0.2782 (0.2706)	
training:	Epoch: [11][147/233]	Loss 0.2935 (0.2707)	
training:	Epoch: [11][148/233]	Loss 0.2139 (0.2703)	
training:	Epoch: [11][149/233]	Loss 0.2113 (0.2699)	
training:	Epoch: [11][150/233]	Loss 0.2579 (0.2699)	
training:	Epoch: [11][151/233]	Loss 0.3212 (0.2702)	
training:	Epoch: [11][152/233]	Loss 0.2722 (0.2702)	
training:	Epoch: [11][153/233]	Loss 0.2414 (0.2700)	
training:	Epoch: [11][154/233]	Loss 0.3010 (0.2702)	
training:	Epoch: [11][155/233]	Loss 0.2341 (0.2700)	
training:	Epoch: [11][156/233]	Loss 0.2575 (0.2699)	
training:	Epoch: [11][157/233]	Loss 0.3108 (0.2702)	
training:	Epoch: [11][158/233]	Loss 0.2421 (0.2700)	
training:	Epoch: [11][159/233]	Loss 0.1507 (0.2692)	
training:	Epoch: [11][160/233]	Loss 0.2719 (0.2693)	
training:	Epoch: [11][161/233]	Loss 0.2745 (0.2693)	
training:	Epoch: [11][162/233]	Loss 0.4054 (0.2701)	
training:	Epoch: [11][163/233]	Loss 0.3190 (0.2704)	
training:	Epoch: [11][164/233]	Loss 0.2028 (0.2700)	
training:	Epoch: [11][165/233]	Loss 0.2377 (0.2698)	
training:	Epoch: [11][166/233]	Loss 0.3212 (0.2701)	
training:	Epoch: [11][167/233]	Loss 0.2934 (0.2703)	
training:	Epoch: [11][168/233]	Loss 0.2259 (0.2700)	
training:	Epoch: [11][169/233]	Loss 0.2607 (0.2700)	
training:	Epoch: [11][170/233]	Loss 0.2864 (0.2701)	
training:	Epoch: [11][171/233]	Loss 0.2408 (0.2699)	
training:	Epoch: [11][172/233]	Loss 0.2612 (0.2698)	
training:	Epoch: [11][173/233]	Loss 0.3037 (0.2700)	
training:	Epoch: [11][174/233]	Loss 0.2843 (0.2701)	
training:	Epoch: [11][175/233]	Loss 0.2607 (0.2701)	
training:	Epoch: [11][176/233]	Loss 0.3332 (0.2704)	
training:	Epoch: [11][177/233]	Loss 0.3510 (0.2709)	
training:	Epoch: [11][178/233]	Loss 0.3389 (0.2713)	
training:	Epoch: [11][179/233]	Loss 0.2312 (0.2710)	
training:	Epoch: [11][180/233]	Loss 0.2254 (0.2708)	
training:	Epoch: [11][181/233]	Loss 0.2065 (0.2704)	
training:	Epoch: [11][182/233]	Loss 0.2520 (0.2703)	
training:	Epoch: [11][183/233]	Loss 0.3359 (0.2707)	
training:	Epoch: [11][184/233]	Loss 0.2800 (0.2707)	
training:	Epoch: [11][185/233]	Loss 0.2578 (0.2707)	
training:	Epoch: [11][186/233]	Loss 0.3194 (0.2709)	
training:	Epoch: [11][187/233]	Loss 0.2701 (0.2709)	
training:	Epoch: [11][188/233]	Loss 0.2166 (0.2706)	
training:	Epoch: [11][189/233]	Loss 0.2948 (0.2708)	
training:	Epoch: [11][190/233]	Loss 0.2982 (0.2709)	
training:	Epoch: [11][191/233]	Loss 0.2365 (0.2707)	
training:	Epoch: [11][192/233]	Loss 0.2209 (0.2705)	
training:	Epoch: [11][193/233]	Loss 0.2197 (0.2702)	
training:	Epoch: [11][194/233]	Loss 0.2749 (0.2702)	
training:	Epoch: [11][195/233]	Loss 0.3328 (0.2705)	
training:	Epoch: [11][196/233]	Loss 0.2365 (0.2704)	
training:	Epoch: [11][197/233]	Loss 0.2893 (0.2705)	
training:	Epoch: [11][198/233]	Loss 0.2644 (0.2704)	
training:	Epoch: [11][199/233]	Loss 0.2276 (0.2702)	
training:	Epoch: [11][200/233]	Loss 0.2354 (0.2700)	
training:	Epoch: [11][201/233]	Loss 0.3012 (0.2702)	
training:	Epoch: [11][202/233]	Loss 0.3182 (0.2704)	
training:	Epoch: [11][203/233]	Loss 0.2675 (0.2704)	
training:	Epoch: [11][204/233]	Loss 0.2870 (0.2705)	
training:	Epoch: [11][205/233]	Loss 0.1749 (0.2700)	
training:	Epoch: [11][206/233]	Loss 0.2343 (0.2699)	
training:	Epoch: [11][207/233]	Loss 0.2147 (0.2696)	
training:	Epoch: [11][208/233]	Loss 0.2434 (0.2695)	
training:	Epoch: [11][209/233]	Loss 0.2437 (0.2693)	
training:	Epoch: [11][210/233]	Loss 0.2953 (0.2695)	
training:	Epoch: [11][211/233]	Loss 0.2172 (0.2692)	
training:	Epoch: [11][212/233]	Loss 0.1954 (0.2689)	
training:	Epoch: [11][213/233]	Loss 0.3085 (0.2691)	
training:	Epoch: [11][214/233]	Loss 0.2477 (0.2690)	
training:	Epoch: [11][215/233]	Loss 0.2565 (0.2689)	
training:	Epoch: [11][216/233]	Loss 0.2669 (0.2689)	
training:	Epoch: [11][217/233]	Loss 0.2480 (0.2688)	
training:	Epoch: [11][218/233]	Loss 0.2691 (0.2688)	
training:	Epoch: [11][219/233]	Loss 0.2683 (0.2688)	
training:	Epoch: [11][220/233]	Loss 0.2883 (0.2689)	
training:	Epoch: [11][221/233]	Loss 0.2414 (0.2688)	
training:	Epoch: [11][222/233]	Loss 0.3016 (0.2689)	
training:	Epoch: [11][223/233]	Loss 0.2339 (0.2688)	
training:	Epoch: [11][224/233]	Loss 0.3371 (0.2691)	
training:	Epoch: [11][225/233]	Loss 0.2361 (0.2689)	
training:	Epoch: [11][226/233]	Loss 0.2342 (0.2688)	
training:	Epoch: [11][227/233]	Loss 0.3078 (0.2689)	
training:	Epoch: [11][228/233]	Loss 0.2045 (0.2686)	
training:	Epoch: [11][229/233]	Loss 0.4342 (0.2694)	
training:	Epoch: [11][230/233]	Loss 0.2834 (0.2694)	
training:	Epoch: [11][231/233]	Loss 0.1993 (0.2691)	
training:	Epoch: [11][232/233]	Loss 0.2613 (0.2691)	
training:	Epoch: [11][233/233]	Loss 0.2869 (0.2692)	
Training:	 Loss: 0.2686

Training:	 ACC: 0.9503 0.9504 0.9508 0.9499
Validation:	 ACC: 0.8104 0.8117 0.8373 0.7836
Validation:	 Best_BACC: 0.8104 0.8117 0.8373 0.7836
Validation:	 Loss: 0.4057
Pretraining:	Epoch 12/200
----------
training:	Epoch: [12][1/233]	Loss 0.2742 (0.2742)	
training:	Epoch: [12][2/233]	Loss 0.2699 (0.2720)	
training:	Epoch: [12][3/233]	Loss 0.1985 (0.2475)	
training:	Epoch: [12][4/233]	Loss 0.2313 (0.2435)	
training:	Epoch: [12][5/233]	Loss 0.2604 (0.2468)	
training:	Epoch: [12][6/233]	Loss 0.2810 (0.2525)	
training:	Epoch: [12][7/233]	Loss 0.2444 (0.2514)	
training:	Epoch: [12][8/233]	Loss 0.2785 (0.2548)	
training:	Epoch: [12][9/233]	Loss 0.2350 (0.2526)	
training:	Epoch: [12][10/233]	Loss 0.2080 (0.2481)	
training:	Epoch: [12][11/233]	Loss 0.1901 (0.2428)	
training:	Epoch: [12][12/233]	Loss 0.2434 (0.2429)	
training:	Epoch: [12][13/233]	Loss 0.2593 (0.2442)	
training:	Epoch: [12][14/233]	Loss 0.1734 (0.2391)	
training:	Epoch: [12][15/233]	Loss 0.1977 (0.2363)	
training:	Epoch: [12][16/233]	Loss 0.3350 (0.2425)	
training:	Epoch: [12][17/233]	Loss 0.2703 (0.2441)	
training:	Epoch: [12][18/233]	Loss 0.2833 (0.2463)	
training:	Epoch: [12][19/233]	Loss 0.1946 (0.2436)	
training:	Epoch: [12][20/233]	Loss 0.2288 (0.2429)	
training:	Epoch: [12][21/233]	Loss 0.1692 (0.2393)	
training:	Epoch: [12][22/233]	Loss 0.2677 (0.2406)	
training:	Epoch: [12][23/233]	Loss 0.2079 (0.2392)	
training:	Epoch: [12][24/233]	Loss 0.3228 (0.2427)	
training:	Epoch: [12][25/233]	Loss 0.3733 (0.2479)	
training:	Epoch: [12][26/233]	Loss 0.1874 (0.2456)	
training:	Epoch: [12][27/233]	Loss 0.2573 (0.2460)	
training:	Epoch: [12][28/233]	Loss 0.3399 (0.2494)	
training:	Epoch: [12][29/233]	Loss 0.2713 (0.2501)	
training:	Epoch: [12][30/233]	Loss 0.2686 (0.2507)	
training:	Epoch: [12][31/233]	Loss 0.2288 (0.2500)	
training:	Epoch: [12][32/233]	Loss 0.1631 (0.2473)	
training:	Epoch: [12][33/233]	Loss 0.2726 (0.2481)	
training:	Epoch: [12][34/233]	Loss 0.3144 (0.2500)	
training:	Epoch: [12][35/233]	Loss 0.2887 (0.2511)	
training:	Epoch: [12][36/233]	Loss 0.2903 (0.2522)	
training:	Epoch: [12][37/233]	Loss 0.2297 (0.2516)	
training:	Epoch: [12][38/233]	Loss 0.3019 (0.2529)	
training:	Epoch: [12][39/233]	Loss 0.1928 (0.2514)	
training:	Epoch: [12][40/233]	Loss 0.1639 (0.2492)	
training:	Epoch: [12][41/233]	Loss 0.3104 (0.2507)	
training:	Epoch: [12][42/233]	Loss 0.2434 (0.2505)	
training:	Epoch: [12][43/233]	Loss 0.3128 (0.2520)	
training:	Epoch: [12][44/233]	Loss 0.1692 (0.2501)	
training:	Epoch: [12][45/233]	Loss 0.2316 (0.2497)	
training:	Epoch: [12][46/233]	Loss 0.3442 (0.2517)	
training:	Epoch: [12][47/233]	Loss 0.2843 (0.2524)	
training:	Epoch: [12][48/233]	Loss 0.2308 (0.2520)	
training:	Epoch: [12][49/233]	Loss 0.2743 (0.2524)	
training:	Epoch: [12][50/233]	Loss 0.1873 (0.2511)	
training:	Epoch: [12][51/233]	Loss 0.2434 (0.2510)	
training:	Epoch: [12][52/233]	Loss 0.2183 (0.2504)	
training:	Epoch: [12][53/233]	Loss 0.2645 (0.2506)	
training:	Epoch: [12][54/233]	Loss 0.3150 (0.2518)	
training:	Epoch: [12][55/233]	Loss 0.2858 (0.2524)	
training:	Epoch: [12][56/233]	Loss 0.2493 (0.2524)	
training:	Epoch: [12][57/233]	Loss 0.2913 (0.2531)	
training:	Epoch: [12][58/233]	Loss 0.3279 (0.2544)	
training:	Epoch: [12][59/233]	Loss 0.2766 (0.2547)	
training:	Epoch: [12][60/233]	Loss 0.3714 (0.2567)	
training:	Epoch: [12][61/233]	Loss 0.3273 (0.2578)	
training:	Epoch: [12][62/233]	Loss 0.2271 (0.2573)	
training:	Epoch: [12][63/233]	Loss 0.2288 (0.2569)	
training:	Epoch: [12][64/233]	Loss 0.2961 (0.2575)	
training:	Epoch: [12][65/233]	Loss 0.1964 (0.2566)	
training:	Epoch: [12][66/233]	Loss 0.2830 (0.2570)	
training:	Epoch: [12][67/233]	Loss 0.2534 (0.2569)	
training:	Epoch: [12][68/233]	Loss 0.2165 (0.2563)	
training:	Epoch: [12][69/233]	Loss 0.2627 (0.2564)	
training:	Epoch: [12][70/233]	Loss 0.1880 (0.2554)	
training:	Epoch: [12][71/233]	Loss 0.2784 (0.2557)	
training:	Epoch: [12][72/233]	Loss 0.4151 (0.2580)	
training:	Epoch: [12][73/233]	Loss 0.2333 (0.2576)	
training:	Epoch: [12][74/233]	Loss 0.3097 (0.2583)	
training:	Epoch: [12][75/233]	Loss 0.2674 (0.2584)	
training:	Epoch: [12][76/233]	Loss 0.2923 (0.2589)	
training:	Epoch: [12][77/233]	Loss 0.2446 (0.2587)	
training:	Epoch: [12][78/233]	Loss 0.2758 (0.2589)	
training:	Epoch: [12][79/233]	Loss 0.2107 (0.2583)	
training:	Epoch: [12][80/233]	Loss 0.2698 (0.2585)	
training:	Epoch: [12][81/233]	Loss 0.2043 (0.2578)	
training:	Epoch: [12][82/233]	Loss 0.2769 (0.2580)	
training:	Epoch: [12][83/233]	Loss 0.2471 (0.2579)	
training:	Epoch: [12][84/233]	Loss 0.2700 (0.2580)	
training:	Epoch: [12][85/233]	Loss 0.2269 (0.2577)	
training:	Epoch: [12][86/233]	Loss 0.1904 (0.2569)	
training:	Epoch: [12][87/233]	Loss 0.3263 (0.2577)	
training:	Epoch: [12][88/233]	Loss 0.3957 (0.2593)	
training:	Epoch: [12][89/233]	Loss 0.2791 (0.2595)	
training:	Epoch: [12][90/233]	Loss 0.2124 (0.2590)	
training:	Epoch: [12][91/233]	Loss 0.1856 (0.2581)	
training:	Epoch: [12][92/233]	Loss 0.2424 (0.2580)	
training:	Epoch: [12][93/233]	Loss 0.2724 (0.2581)	
training:	Epoch: [12][94/233]	Loss 0.3196 (0.2588)	
training:	Epoch: [12][95/233]	Loss 0.2358 (0.2585)	
training:	Epoch: [12][96/233]	Loss 0.2944 (0.2589)	
training:	Epoch: [12][97/233]	Loss 0.2561 (0.2589)	
training:	Epoch: [12][98/233]	Loss 0.2041 (0.2583)	
training:	Epoch: [12][99/233]	Loss 0.2478 (0.2582)	
training:	Epoch: [12][100/233]	Loss 0.3322 (0.2590)	
training:	Epoch: [12][101/233]	Loss 0.2961 (0.2593)	
training:	Epoch: [12][102/233]	Loss 0.2441 (0.2592)	
training:	Epoch: [12][103/233]	Loss 0.2955 (0.2595)	
training:	Epoch: [12][104/233]	Loss 0.2276 (0.2592)	
training:	Epoch: [12][105/233]	Loss 0.2545 (0.2592)	
training:	Epoch: [12][106/233]	Loss 0.2100 (0.2587)	
training:	Epoch: [12][107/233]	Loss 0.2617 (0.2587)	
training:	Epoch: [12][108/233]	Loss 0.3368 (0.2595)	
training:	Epoch: [12][109/233]	Loss 0.2385 (0.2593)	
training:	Epoch: [12][110/233]	Loss 0.2751 (0.2594)	
training:	Epoch: [12][111/233]	Loss 0.3087 (0.2599)	
training:	Epoch: [12][112/233]	Loss 0.2517 (0.2598)	
training:	Epoch: [12][113/233]	Loss 0.2933 (0.2601)	
training:	Epoch: [12][114/233]	Loss 0.2079 (0.2596)	
training:	Epoch: [12][115/233]	Loss 0.2178 (0.2593)	
training:	Epoch: [12][116/233]	Loss 0.2319 (0.2590)	
training:	Epoch: [12][117/233]	Loss 0.2278 (0.2588)	
training:	Epoch: [12][118/233]	Loss 0.2969 (0.2591)	
training:	Epoch: [12][119/233]	Loss 0.1756 (0.2584)	
training:	Epoch: [12][120/233]	Loss 0.2583 (0.2584)	
training:	Epoch: [12][121/233]	Loss 0.2902 (0.2586)	
training:	Epoch: [12][122/233]	Loss 0.3214 (0.2592)	
training:	Epoch: [12][123/233]	Loss 0.2325 (0.2589)	
training:	Epoch: [12][124/233]	Loss 0.3090 (0.2594)	
training:	Epoch: [12][125/233]	Loss 0.2602 (0.2594)	
training:	Epoch: [12][126/233]	Loss 0.3230 (0.2599)	
training:	Epoch: [12][127/233]	Loss 0.2236 (0.2596)	
training:	Epoch: [12][128/233]	Loss 0.2015 (0.2591)	
training:	Epoch: [12][129/233]	Loss 0.2223 (0.2588)	
training:	Epoch: [12][130/233]	Loss 0.2488 (0.2588)	
training:	Epoch: [12][131/233]	Loss 0.2632 (0.2588)	
training:	Epoch: [12][132/233]	Loss 0.2294 (0.2586)	
training:	Epoch: [12][133/233]	Loss 0.2148 (0.2582)	
training:	Epoch: [12][134/233]	Loss 0.2589 (0.2582)	
training:	Epoch: [12][135/233]	Loss 0.2481 (0.2582)	
training:	Epoch: [12][136/233]	Loss 0.2271 (0.2579)	
training:	Epoch: [12][137/233]	Loss 0.1997 (0.2575)	
training:	Epoch: [12][138/233]	Loss 0.2477 (0.2574)	
training:	Epoch: [12][139/233]	Loss 0.2398 (0.2573)	
training:	Epoch: [12][140/233]	Loss 0.2158 (0.2570)	
training:	Epoch: [12][141/233]	Loss 0.2839 (0.2572)	
training:	Epoch: [12][142/233]	Loss 0.4003 (0.2582)	
training:	Epoch: [12][143/233]	Loss 0.2151 (0.2579)	
training:	Epoch: [12][144/233]	Loss 0.2833 (0.2581)	
training:	Epoch: [12][145/233]	Loss 0.2069 (0.2577)	
training:	Epoch: [12][146/233]	Loss 0.3225 (0.2582)	
training:	Epoch: [12][147/233]	Loss 0.2533 (0.2582)	
training:	Epoch: [12][148/233]	Loss 0.3083 (0.2585)	
training:	Epoch: [12][149/233]	Loss 0.2691 (0.2586)	
training:	Epoch: [12][150/233]	Loss 0.3267 (0.2590)	
training:	Epoch: [12][151/233]	Loss 0.2572 (0.2590)	
training:	Epoch: [12][152/233]	Loss 0.2339 (0.2588)	
training:	Epoch: [12][153/233]	Loss 0.2426 (0.2587)	
training:	Epoch: [12][154/233]	Loss 0.2177 (0.2585)	
training:	Epoch: [12][155/233]	Loss 0.2629 (0.2585)	
training:	Epoch: [12][156/233]	Loss 0.2585 (0.2585)	
training:	Epoch: [12][157/233]	Loss 0.2932 (0.2587)	
training:	Epoch: [12][158/233]	Loss 0.4016 (0.2596)	
training:	Epoch: [12][159/233]	Loss 0.2190 (0.2594)	
training:	Epoch: [12][160/233]	Loss 0.2543 (0.2593)	
training:	Epoch: [12][161/233]	Loss 0.1841 (0.2589)	
training:	Epoch: [12][162/233]	Loss 0.2778 (0.2590)	
training:	Epoch: [12][163/233]	Loss 0.2660 (0.2590)	
training:	Epoch: [12][164/233]	Loss 0.2360 (0.2589)	
training:	Epoch: [12][165/233]	Loss 0.2369 (0.2588)	
training:	Epoch: [12][166/233]	Loss 0.2723 (0.2588)	
training:	Epoch: [12][167/233]	Loss 0.1985 (0.2585)	
training:	Epoch: [12][168/233]	Loss 0.2119 (0.2582)	
training:	Epoch: [12][169/233]	Loss 0.2642 (0.2582)	
training:	Epoch: [12][170/233]	Loss 0.2603 (0.2582)	
training:	Epoch: [12][171/233]	Loss 0.2157 (0.2580)	
training:	Epoch: [12][172/233]	Loss 0.2159 (0.2577)	
training:	Epoch: [12][173/233]	Loss 0.2070 (0.2575)	
training:	Epoch: [12][174/233]	Loss 0.1839 (0.2570)	
training:	Epoch: [12][175/233]	Loss 0.1929 (0.2567)	
training:	Epoch: [12][176/233]	Loss 0.2260 (0.2565)	
training:	Epoch: [12][177/233]	Loss 0.2423 (0.2564)	
training:	Epoch: [12][178/233]	Loss 0.2730 (0.2565)	
training:	Epoch: [12][179/233]	Loss 0.2000 (0.2562)	
training:	Epoch: [12][180/233]	Loss 0.1894 (0.2558)	
training:	Epoch: [12][181/233]	Loss 0.2241 (0.2556)	
training:	Epoch: [12][182/233]	Loss 0.1674 (0.2552)	
training:	Epoch: [12][183/233]	Loss 0.2650 (0.2552)	
training:	Epoch: [12][184/233]	Loss 0.2486 (0.2552)	
training:	Epoch: [12][185/233]	Loss 0.2824 (0.2553)	
training:	Epoch: [12][186/233]	Loss 0.1755 (0.2549)	
training:	Epoch: [12][187/233]	Loss 0.2611 (0.2549)	
training:	Epoch: [12][188/233]	Loss 0.3095 (0.2552)	
training:	Epoch: [12][189/233]	Loss 0.3901 (0.2559)	
training:	Epoch: [12][190/233]	Loss 0.2186 (0.2557)	
training:	Epoch: [12][191/233]	Loss 0.2308 (0.2556)	
training:	Epoch: [12][192/233]	Loss 0.2024 (0.2553)	
training:	Epoch: [12][193/233]	Loss 0.3449 (0.2558)	
training:	Epoch: [12][194/233]	Loss 0.2719 (0.2559)	
training:	Epoch: [12][195/233]	Loss 0.3064 (0.2561)	
training:	Epoch: [12][196/233]	Loss 0.2519 (0.2561)	
training:	Epoch: [12][197/233]	Loss 0.1949 (0.2558)	
training:	Epoch: [12][198/233]	Loss 0.1521 (0.2553)	
training:	Epoch: [12][199/233]	Loss 0.2339 (0.2552)	
training:	Epoch: [12][200/233]	Loss 0.2653 (0.2552)	
training:	Epoch: [12][201/233]	Loss 0.2211 (0.2551)	
training:	Epoch: [12][202/233]	Loss 0.2859 (0.2552)	
training:	Epoch: [12][203/233]	Loss 0.2625 (0.2552)	
training:	Epoch: [12][204/233]	Loss 0.2045 (0.2550)	
training:	Epoch: [12][205/233]	Loss 0.2936 (0.2552)	
training:	Epoch: [12][206/233]	Loss 0.2989 (0.2554)	
training:	Epoch: [12][207/233]	Loss 0.2778 (0.2555)	
training:	Epoch: [12][208/233]	Loss 0.3997 (0.2562)	
training:	Epoch: [12][209/233]	Loss 0.2556 (0.2562)	
training:	Epoch: [12][210/233]	Loss 0.2156 (0.2560)	
training:	Epoch: [12][211/233]	Loss 0.2541 (0.2560)	
training:	Epoch: [12][212/233]	Loss 0.2327 (0.2559)	
training:	Epoch: [12][213/233]	Loss 0.2106 (0.2557)	
training:	Epoch: [12][214/233]	Loss 0.2847 (0.2558)	
training:	Epoch: [12][215/233]	Loss 0.4429 (0.2567)	
training:	Epoch: [12][216/233]	Loss 0.2495 (0.2566)	
training:	Epoch: [12][217/233]	Loss 0.1884 (0.2563)	
training:	Epoch: [12][218/233]	Loss 0.2318 (0.2562)	
training:	Epoch: [12][219/233]	Loss 0.4069 (0.2569)	
training:	Epoch: [12][220/233]	Loss 0.2500 (0.2569)	
training:	Epoch: [12][221/233]	Loss 0.2185 (0.2567)	
training:	Epoch: [12][222/233]	Loss 0.2413 (0.2566)	
training:	Epoch: [12][223/233]	Loss 0.1879 (0.2563)	
training:	Epoch: [12][224/233]	Loss 0.3357 (0.2567)	
training:	Epoch: [12][225/233]	Loss 0.2719 (0.2567)	
training:	Epoch: [12][226/233]	Loss 0.2490 (0.2567)	
training:	Epoch: [12][227/233]	Loss 0.2301 (0.2566)	
training:	Epoch: [12][228/233]	Loss 0.2218 (0.2564)	
training:	Epoch: [12][229/233]	Loss 0.2917 (0.2566)	
training:	Epoch: [12][230/233]	Loss 0.2805 (0.2567)	
training:	Epoch: [12][231/233]	Loss 0.4215 (0.2574)	
training:	Epoch: [12][232/233]	Loss 0.2447 (0.2574)	
training:	Epoch: [12][233/233]	Loss 0.2694 (0.2574)	
Training:	 Loss: 0.2568

Training:	 ACC: 0.9585 0.9589 0.9687 0.9482
Validation:	 ACC: 0.8115 0.8138 0.8618 0.7612
Validation:	 Best_BACC: 0.8115 0.8138 0.8618 0.7612
Validation:	 Loss: 0.4055
Pretraining:	Epoch 13/200
----------
training:	Epoch: [13][1/233]	Loss 0.2119 (0.2119)	
training:	Epoch: [13][2/233]	Loss 0.2071 (0.2095)	
training:	Epoch: [13][3/233]	Loss 0.2522 (0.2237)	
training:	Epoch: [13][4/233]	Loss 0.3153 (0.2466)	
training:	Epoch: [13][5/233]	Loss 0.3030 (0.2579)	
training:	Epoch: [13][6/233]	Loss 0.2375 (0.2545)	
training:	Epoch: [13][7/233]	Loss 0.3007 (0.2611)	
training:	Epoch: [13][8/233]	Loss 0.2151 (0.2553)	
training:	Epoch: [13][9/233]	Loss 0.2050 (0.2498)	
training:	Epoch: [13][10/233]	Loss 0.2242 (0.2472)	
training:	Epoch: [13][11/233]	Loss 0.1791 (0.2410)	
training:	Epoch: [13][12/233]	Loss 0.2539 (0.2421)	
training:	Epoch: [13][13/233]	Loss 0.2145 (0.2400)	
training:	Epoch: [13][14/233]	Loss 0.2513 (0.2408)	
training:	Epoch: [13][15/233]	Loss 0.2107 (0.2388)	
training:	Epoch: [13][16/233]	Loss 0.2083 (0.2369)	
training:	Epoch: [13][17/233]	Loss 0.1912 (0.2342)	
training:	Epoch: [13][18/233]	Loss 0.2212 (0.2335)	
training:	Epoch: [13][19/233]	Loss 0.3050 (0.2372)	
training:	Epoch: [13][20/233]	Loss 0.1739 (0.2341)	
training:	Epoch: [13][21/233]	Loss 0.2834 (0.2364)	
training:	Epoch: [13][22/233]	Loss 0.3022 (0.2394)	
training:	Epoch: [13][23/233]	Loss 0.2656 (0.2405)	
training:	Epoch: [13][24/233]	Loss 0.2419 (0.2406)	
training:	Epoch: [13][25/233]	Loss 0.2251 (0.2400)	
training:	Epoch: [13][26/233]	Loss 0.2066 (0.2387)	
training:	Epoch: [13][27/233]	Loss 0.2211 (0.2380)	
training:	Epoch: [13][28/233]	Loss 0.2241 (0.2375)	
training:	Epoch: [13][29/233]	Loss 0.2229 (0.2370)	
training:	Epoch: [13][30/233]	Loss 0.1916 (0.2355)	
training:	Epoch: [13][31/233]	Loss 0.2855 (0.2371)	
training:	Epoch: [13][32/233]	Loss 0.2895 (0.2388)	
training:	Epoch: [13][33/233]	Loss 0.2778 (0.2400)	
training:	Epoch: [13][34/233]	Loss 0.2513 (0.2403)	
training:	Epoch: [13][35/233]	Loss 0.2748 (0.2413)	
training:	Epoch: [13][36/233]	Loss 0.3011 (0.2429)	
training:	Epoch: [13][37/233]	Loss 0.2315 (0.2426)	
training:	Epoch: [13][38/233]	Loss 0.2000 (0.2415)	
training:	Epoch: [13][39/233]	Loss 0.1867 (0.2401)	
training:	Epoch: [13][40/233]	Loss 0.2586 (0.2406)	
training:	Epoch: [13][41/233]	Loss 0.3177 (0.2424)	
training:	Epoch: [13][42/233]	Loss 0.2799 (0.2433)	
training:	Epoch: [13][43/233]	Loss 0.2125 (0.2426)	
training:	Epoch: [13][44/233]	Loss 0.1729 (0.2410)	
training:	Epoch: [13][45/233]	Loss 0.2825 (0.2420)	
training:	Epoch: [13][46/233]	Loss 0.2183 (0.2414)	
training:	Epoch: [13][47/233]	Loss 0.2504 (0.2416)	
training:	Epoch: [13][48/233]	Loss 0.2007 (0.2408)	
training:	Epoch: [13][49/233]	Loss 0.2257 (0.2405)	
training:	Epoch: [13][50/233]	Loss 0.3715 (0.2431)	
training:	Epoch: [13][51/233]	Loss 0.2302 (0.2428)	
training:	Epoch: [13][52/233]	Loss 0.2830 (0.2436)	
training:	Epoch: [13][53/233]	Loss 0.2945 (0.2446)	
training:	Epoch: [13][54/233]	Loss 0.2306 (0.2443)	
training:	Epoch: [13][55/233]	Loss 0.2093 (0.2437)	
training:	Epoch: [13][56/233]	Loss 0.1942 (0.2428)	
training:	Epoch: [13][57/233]	Loss 0.2585 (0.2431)	
training:	Epoch: [13][58/233]	Loss 0.3010 (0.2441)	
training:	Epoch: [13][59/233]	Loss 0.1974 (0.2433)	
training:	Epoch: [13][60/233]	Loss 0.2781 (0.2439)	
training:	Epoch: [13][61/233]	Loss 0.3806 (0.2461)	
training:	Epoch: [13][62/233]	Loss 0.2829 (0.2467)	
training:	Epoch: [13][63/233]	Loss 0.2021 (0.2460)	
training:	Epoch: [13][64/233]	Loss 0.1944 (0.2452)	
training:	Epoch: [13][65/233]	Loss 0.3325 (0.2465)	
training:	Epoch: [13][66/233]	Loss 0.2033 (0.2459)	
training:	Epoch: [13][67/233]	Loss 0.2603 (0.2461)	
training:	Epoch: [13][68/233]	Loss 0.2776 (0.2465)	
training:	Epoch: [13][69/233]	Loss 0.2576 (0.2467)	
training:	Epoch: [13][70/233]	Loss 0.2749 (0.2471)	
training:	Epoch: [13][71/233]	Loss 0.1907 (0.2463)	
training:	Epoch: [13][72/233]	Loss 0.3247 (0.2474)	
training:	Epoch: [13][73/233]	Loss 0.1857 (0.2466)	
training:	Epoch: [13][74/233]	Loss 0.1994 (0.2459)	
training:	Epoch: [13][75/233]	Loss 0.2009 (0.2453)	
training:	Epoch: [13][76/233]	Loss 0.3232 (0.2463)	
training:	Epoch: [13][77/233]	Loss 0.2476 (0.2464)	
training:	Epoch: [13][78/233]	Loss 0.1798 (0.2455)	
training:	Epoch: [13][79/233]	Loss 0.2681 (0.2458)	
training:	Epoch: [13][80/233]	Loss 0.2582 (0.2459)	
training:	Epoch: [13][81/233]	Loss 0.2527 (0.2460)	
training:	Epoch: [13][82/233]	Loss 0.2319 (0.2459)	
training:	Epoch: [13][83/233]	Loss 0.2220 (0.2456)	
training:	Epoch: [13][84/233]	Loss 0.2514 (0.2456)	
training:	Epoch: [13][85/233]	Loss 0.2908 (0.2462)	
training:	Epoch: [13][86/233]	Loss 0.2071 (0.2457)	
training:	Epoch: [13][87/233]	Loss 0.3103 (0.2465)	
training:	Epoch: [13][88/233]	Loss 0.2456 (0.2464)	
training:	Epoch: [13][89/233]	Loss 0.2801 (0.2468)	
training:	Epoch: [13][90/233]	Loss 0.2430 (0.2468)	
training:	Epoch: [13][91/233]	Loss 0.2397 (0.2467)	
training:	Epoch: [13][92/233]	Loss 0.2635 (0.2469)	
training:	Epoch: [13][93/233]	Loss 0.2013 (0.2464)	
training:	Epoch: [13][94/233]	Loss 0.1881 (0.2458)	
training:	Epoch: [13][95/233]	Loss 0.2008 (0.2453)	
training:	Epoch: [13][96/233]	Loss 0.1977 (0.2448)	
training:	Epoch: [13][97/233]	Loss 0.2786 (0.2452)	
training:	Epoch: [13][98/233]	Loss 0.2460 (0.2452)	
training:	Epoch: [13][99/233]	Loss 0.2188 (0.2449)	
training:	Epoch: [13][100/233]	Loss 0.2381 (0.2448)	
training:	Epoch: [13][101/233]	Loss 0.2573 (0.2450)	
training:	Epoch: [13][102/233]	Loss 0.2498 (0.2450)	
training:	Epoch: [13][103/233]	Loss 0.1967 (0.2445)	
training:	Epoch: [13][104/233]	Loss 0.3168 (0.2452)	
training:	Epoch: [13][105/233]	Loss 0.1705 (0.2445)	
training:	Epoch: [13][106/233]	Loss 0.2085 (0.2442)	
training:	Epoch: [13][107/233]	Loss 0.2672 (0.2444)	
training:	Epoch: [13][108/233]	Loss 0.2181 (0.2441)	
training:	Epoch: [13][109/233]	Loss 0.2655 (0.2443)	
training:	Epoch: [13][110/233]	Loss 0.2065 (0.2440)	
training:	Epoch: [13][111/233]	Loss 0.2537 (0.2441)	
training:	Epoch: [13][112/233]	Loss 0.2175 (0.2438)	
training:	Epoch: [13][113/233]	Loss 0.3529 (0.2448)	
training:	Epoch: [13][114/233]	Loss 0.2495 (0.2449)	
training:	Epoch: [13][115/233]	Loss 0.1884 (0.2444)	
training:	Epoch: [13][116/233]	Loss 0.2728 (0.2446)	
training:	Epoch: [13][117/233]	Loss 0.2468 (0.2446)	
training:	Epoch: [13][118/233]	Loss 0.2581 (0.2447)	
training:	Epoch: [13][119/233]	Loss 0.2393 (0.2447)	
training:	Epoch: [13][120/233]	Loss 0.2286 (0.2446)	
training:	Epoch: [13][121/233]	Loss 0.2175 (0.2443)	
training:	Epoch: [13][122/233]	Loss 0.2584 (0.2445)	
training:	Epoch: [13][123/233]	Loss 0.1851 (0.2440)	
training:	Epoch: [13][124/233]	Loss 0.2066 (0.2437)	
training:	Epoch: [13][125/233]	Loss 0.2825 (0.2440)	
training:	Epoch: [13][126/233]	Loss 0.2621 (0.2441)	
training:	Epoch: [13][127/233]	Loss 0.3246 (0.2448)	
training:	Epoch: [13][128/233]	Loss 0.2022 (0.2444)	
training:	Epoch: [13][129/233]	Loss 0.2779 (0.2447)	
training:	Epoch: [13][130/233]	Loss 0.2464 (0.2447)	
training:	Epoch: [13][131/233]	Loss 0.2510 (0.2447)	
training:	Epoch: [13][132/233]	Loss 0.2177 (0.2445)	
training:	Epoch: [13][133/233]	Loss 0.3289 (0.2452)	
training:	Epoch: [13][134/233]	Loss 0.1976 (0.2448)	
training:	Epoch: [13][135/233]	Loss 0.3632 (0.2457)	
training:	Epoch: [13][136/233]	Loss 0.2433 (0.2457)	
training:	Epoch: [13][137/233]	Loss 0.1838 (0.2452)	
training:	Epoch: [13][138/233]	Loss 0.1871 (0.2448)	
training:	Epoch: [13][139/233]	Loss 0.1913 (0.2444)	
training:	Epoch: [13][140/233]	Loss 0.2474 (0.2444)	
training:	Epoch: [13][141/233]	Loss 0.2426 (0.2444)	
training:	Epoch: [13][142/233]	Loss 0.2612 (0.2445)	
training:	Epoch: [13][143/233]	Loss 0.4858 (0.2462)	
training:	Epoch: [13][144/233]	Loss 0.1518 (0.2456)	
training:	Epoch: [13][145/233]	Loss 0.2493 (0.2456)	
training:	Epoch: [13][146/233]	Loss 0.3262 (0.2462)	
training:	Epoch: [13][147/233]	Loss 0.2870 (0.2464)	
training:	Epoch: [13][148/233]	Loss 0.2506 (0.2465)	
training:	Epoch: [13][149/233]	Loss 0.2310 (0.2464)	
training:	Epoch: [13][150/233]	Loss 0.2466 (0.2464)	
training:	Epoch: [13][151/233]	Loss 0.2169 (0.2462)	
training:	Epoch: [13][152/233]	Loss 0.1890 (0.2458)	
training:	Epoch: [13][153/233]	Loss 0.2175 (0.2456)	
training:	Epoch: [13][154/233]	Loss 0.2579 (0.2457)	
training:	Epoch: [13][155/233]	Loss 0.1846 (0.2453)	
training:	Epoch: [13][156/233]	Loss 0.1594 (0.2447)	
training:	Epoch: [13][157/233]	Loss 0.3136 (0.2452)	
training:	Epoch: [13][158/233]	Loss 0.2513 (0.2452)	
training:	Epoch: [13][159/233]	Loss 0.2835 (0.2455)	
training:	Epoch: [13][160/233]	Loss 0.2490 (0.2455)	
training:	Epoch: [13][161/233]	Loss 0.2587 (0.2456)	
training:	Epoch: [13][162/233]	Loss 0.2207 (0.2454)	
training:	Epoch: [13][163/233]	Loss 0.2296 (0.2453)	
training:	Epoch: [13][164/233]	Loss 0.2630 (0.2454)	
training:	Epoch: [13][165/233]	Loss 0.2328 (0.2453)	
training:	Epoch: [13][166/233]	Loss 0.3168 (0.2458)	
training:	Epoch: [13][167/233]	Loss 0.2978 (0.2461)	
training:	Epoch: [13][168/233]	Loss 0.2703 (0.2462)	
training:	Epoch: [13][169/233]	Loss 0.2820 (0.2464)	
training:	Epoch: [13][170/233]	Loss 0.2456 (0.2464)	
training:	Epoch: [13][171/233]	Loss 0.2349 (0.2464)	
training:	Epoch: [13][172/233]	Loss 0.2988 (0.2467)	
training:	Epoch: [13][173/233]	Loss 0.3458 (0.2472)	
training:	Epoch: [13][174/233]	Loss 0.2099 (0.2470)	
training:	Epoch: [13][175/233]	Loss 0.1885 (0.2467)	
training:	Epoch: [13][176/233]	Loss 0.2376 (0.2466)	
training:	Epoch: [13][177/233]	Loss 0.2198 (0.2465)	
training:	Epoch: [13][178/233]	Loss 0.2219 (0.2464)	
training:	Epoch: [13][179/233]	Loss 0.2027 (0.2461)	
training:	Epoch: [13][180/233]	Loss 0.2265 (0.2460)	
training:	Epoch: [13][181/233]	Loss 0.2251 (0.2459)	
training:	Epoch: [13][182/233]	Loss 0.3131 (0.2463)	
training:	Epoch: [13][183/233]	Loss 0.2562 (0.2463)	
training:	Epoch: [13][184/233]	Loss 0.2562 (0.2464)	
training:	Epoch: [13][185/233]	Loss 0.1765 (0.2460)	
training:	Epoch: [13][186/233]	Loss 0.2220 (0.2459)	
training:	Epoch: [13][187/233]	Loss 0.2524 (0.2459)	
training:	Epoch: [13][188/233]	Loss 0.2400 (0.2459)	
training:	Epoch: [13][189/233]	Loss 0.2097 (0.2457)	
training:	Epoch: [13][190/233]	Loss 0.2628 (0.2458)	
training:	Epoch: [13][191/233]	Loss 0.2393 (0.2457)	
training:	Epoch: [13][192/233]	Loss 0.3100 (0.2461)	
training:	Epoch: [13][193/233]	Loss 0.2217 (0.2459)	
training:	Epoch: [13][194/233]	Loss 0.1929 (0.2457)	
training:	Epoch: [13][195/233]	Loss 0.2602 (0.2457)	
training:	Epoch: [13][196/233]	Loss 0.2055 (0.2455)	
training:	Epoch: [13][197/233]	Loss 0.2603 (0.2456)	
training:	Epoch: [13][198/233]	Loss 0.1911 (0.2453)	
training:	Epoch: [13][199/233]	Loss 0.2259 (0.2452)	
training:	Epoch: [13][200/233]	Loss 0.3066 (0.2455)	
training:	Epoch: [13][201/233]	Loss 0.2211 (0.2454)	
training:	Epoch: [13][202/233]	Loss 0.2071 (0.2452)	
training:	Epoch: [13][203/233]	Loss 0.2129 (0.2451)	
training:	Epoch: [13][204/233]	Loss 0.2396 (0.2450)	
training:	Epoch: [13][205/233]	Loss 0.2496 (0.2451)	
training:	Epoch: [13][206/233]	Loss 0.2563 (0.2451)	
training:	Epoch: [13][207/233]	Loss 0.2235 (0.2450)	
training:	Epoch: [13][208/233]	Loss 0.2202 (0.2449)	
training:	Epoch: [13][209/233]	Loss 0.2645 (0.2450)	
training:	Epoch: [13][210/233]	Loss 0.2347 (0.2449)	
training:	Epoch: [13][211/233]	Loss 0.2243 (0.2448)	
training:	Epoch: [13][212/233]	Loss 0.2148 (0.2447)	
training:	Epoch: [13][213/233]	Loss 0.2652 (0.2448)	
training:	Epoch: [13][214/233]	Loss 0.2331 (0.2447)	
training:	Epoch: [13][215/233]	Loss 0.2127 (0.2446)	
training:	Epoch: [13][216/233]	Loss 0.2284 (0.2445)	
training:	Epoch: [13][217/233]	Loss 0.1927 (0.2443)	
training:	Epoch: [13][218/233]	Loss 0.2915 (0.2445)	
training:	Epoch: [13][219/233]	Loss 0.1660 (0.2441)	
training:	Epoch: [13][220/233]	Loss 0.1791 (0.2438)	
training:	Epoch: [13][221/233]	Loss 0.2626 (0.2439)	
training:	Epoch: [13][222/233]	Loss 0.3022 (0.2442)	
training:	Epoch: [13][223/233]	Loss 0.2170 (0.2441)	
training:	Epoch: [13][224/233]	Loss 0.2146 (0.2439)	
training:	Epoch: [13][225/233]	Loss 0.2283 (0.2439)	
training:	Epoch: [13][226/233]	Loss 0.2446 (0.2439)	
training:	Epoch: [13][227/233]	Loss 0.2371 (0.2438)	
training:	Epoch: [13][228/233]	Loss 0.3741 (0.2444)	
training:	Epoch: [13][229/233]	Loss 0.2972 (0.2446)	
training:	Epoch: [13][230/233]	Loss 0.2085 (0.2445)	
training:	Epoch: [13][231/233]	Loss 0.1814 (0.2442)	
training:	Epoch: [13][232/233]	Loss 0.2640 (0.2443)	
training:	Epoch: [13][233/233]	Loss 0.2211 (0.2442)	
Training:	 Loss: 0.2436

Training:	 ACC: 0.9639 0.9640 0.9669 0.9608
Validation:	 ACC: 0.8183 0.8197 0.8485 0.7881
Validation:	 Best_BACC: 0.8183 0.8197 0.8485 0.7881
Validation:	 Loss: 0.3989
Pretraining:	Epoch 14/200
----------
training:	Epoch: [14][1/233]	Loss 0.1871 (0.1871)	
training:	Epoch: [14][2/233]	Loss 0.3297 (0.2584)	
training:	Epoch: [14][3/233]	Loss 0.2057 (0.2408)	
training:	Epoch: [14][4/233]	Loss 0.2372 (0.2399)	
training:	Epoch: [14][5/233]	Loss 0.2481 (0.2415)	
training:	Epoch: [14][6/233]	Loss 0.2283 (0.2393)	
training:	Epoch: [14][7/233]	Loss 0.2432 (0.2399)	
training:	Epoch: [14][8/233]	Loss 0.2863 (0.2457)	
training:	Epoch: [14][9/233]	Loss 0.1973 (0.2403)	
training:	Epoch: [14][10/233]	Loss 0.1817 (0.2344)	
training:	Epoch: [14][11/233]	Loss 0.3408 (0.2441)	
training:	Epoch: [14][12/233]	Loss 0.2195 (0.2421)	
training:	Epoch: [14][13/233]	Loss 0.2359 (0.2416)	
training:	Epoch: [14][14/233]	Loss 0.1958 (0.2383)	
training:	Epoch: [14][15/233]	Loss 0.2748 (0.2408)	
training:	Epoch: [14][16/233]	Loss 0.1845 (0.2372)	
training:	Epoch: [14][17/233]	Loss 0.2655 (0.2389)	
training:	Epoch: [14][18/233]	Loss 0.1932 (0.2364)	
training:	Epoch: [14][19/233]	Loss 0.2399 (0.2365)	
training:	Epoch: [14][20/233]	Loss 0.3060 (0.2400)	
training:	Epoch: [14][21/233]	Loss 0.2592 (0.2409)	
training:	Epoch: [14][22/233]	Loss 0.3228 (0.2447)	
training:	Epoch: [14][23/233]	Loss 0.1890 (0.2422)	
training:	Epoch: [14][24/233]	Loss 0.2164 (0.2412)	
training:	Epoch: [14][25/233]	Loss 0.2818 (0.2428)	
training:	Epoch: [14][26/233]	Loss 0.2083 (0.2415)	
training:	Epoch: [14][27/233]	Loss 0.2014 (0.2400)	
training:	Epoch: [14][28/233]	Loss 0.2125 (0.2390)	
training:	Epoch: [14][29/233]	Loss 0.2186 (0.2383)	
training:	Epoch: [14][30/233]	Loss 0.2530 (0.2388)	
training:	Epoch: [14][31/233]	Loss 0.1942 (0.2373)	
training:	Epoch: [14][32/233]	Loss 0.2679 (0.2383)	
training:	Epoch: [14][33/233]	Loss 0.3324 (0.2411)	
training:	Epoch: [14][34/233]	Loss 0.2485 (0.2414)	
training:	Epoch: [14][35/233]	Loss 0.2182 (0.2407)	
training:	Epoch: [14][36/233]	Loss 0.1950 (0.2394)	
training:	Epoch: [14][37/233]	Loss 0.2017 (0.2384)	
training:	Epoch: [14][38/233]	Loss 0.2411 (0.2385)	
training:	Epoch: [14][39/233]	Loss 0.2635 (0.2391)	
training:	Epoch: [14][40/233]	Loss 0.2360 (0.2390)	
training:	Epoch: [14][41/233]	Loss 0.1806 (0.2376)	
training:	Epoch: [14][42/233]	Loss 0.1927 (0.2366)	
training:	Epoch: [14][43/233]	Loss 0.2407 (0.2366)	
training:	Epoch: [14][44/233]	Loss 0.1694 (0.2351)	
training:	Epoch: [14][45/233]	Loss 0.3099 (0.2368)	
training:	Epoch: [14][46/233]	Loss 0.2517 (0.2371)	
training:	Epoch: [14][47/233]	Loss 0.1556 (0.2354)	
training:	Epoch: [14][48/233]	Loss 0.2466 (0.2356)	
training:	Epoch: [14][49/233]	Loss 0.2559 (0.2360)	
training:	Epoch: [14][50/233]	Loss 0.2230 (0.2358)	
training:	Epoch: [14][51/233]	Loss 0.2020 (0.2351)	
training:	Epoch: [14][52/233]	Loss 0.1954 (0.2343)	
training:	Epoch: [14][53/233]	Loss 0.2797 (0.2352)	
training:	Epoch: [14][54/233]	Loss 0.2390 (0.2353)	
training:	Epoch: [14][55/233]	Loss 0.2154 (0.2349)	
training:	Epoch: [14][56/233]	Loss 0.2607 (0.2354)	
training:	Epoch: [14][57/233]	Loss 0.2291 (0.2352)	
training:	Epoch: [14][58/233]	Loss 0.2490 (0.2355)	
training:	Epoch: [14][59/233]	Loss 0.2944 (0.2365)	
training:	Epoch: [14][60/233]	Loss 0.2277 (0.2363)	
training:	Epoch: [14][61/233]	Loss 0.3344 (0.2379)	
training:	Epoch: [14][62/233]	Loss 0.2078 (0.2375)	
training:	Epoch: [14][63/233]	Loss 0.2200 (0.2372)	
training:	Epoch: [14][64/233]	Loss 0.1928 (0.2365)	
training:	Epoch: [14][65/233]	Loss 0.2097 (0.2361)	
training:	Epoch: [14][66/233]	Loss 0.1757 (0.2352)	
training:	Epoch: [14][67/233]	Loss 0.1636 (0.2341)	
training:	Epoch: [14][68/233]	Loss 0.2000 (0.2336)	
training:	Epoch: [14][69/233]	Loss 0.2252 (0.2335)	
training:	Epoch: [14][70/233]	Loss 0.2263 (0.2334)	
training:	Epoch: [14][71/233]	Loss 0.2775 (0.2340)	
training:	Epoch: [14][72/233]	Loss 0.2128 (0.2337)	
training:	Epoch: [14][73/233]	Loss 0.2093 (0.2334)	
training:	Epoch: [14][74/233]	Loss 0.1968 (0.2329)	
training:	Epoch: [14][75/233]	Loss 0.2889 (0.2336)	
training:	Epoch: [14][76/233]	Loss 0.1821 (0.2329)	
training:	Epoch: [14][77/233]	Loss 0.1769 (0.2322)	
training:	Epoch: [14][78/233]	Loss 0.2010 (0.2318)	
training:	Epoch: [14][79/233]	Loss 0.2281 (0.2318)	
training:	Epoch: [14][80/233]	Loss 0.2700 (0.2322)	
training:	Epoch: [14][81/233]	Loss 0.3932 (0.2342)	
training:	Epoch: [14][82/233]	Loss 0.1989 (0.2338)	
training:	Epoch: [14][83/233]	Loss 0.2245 (0.2337)	
training:	Epoch: [14][84/233]	Loss 0.2423 (0.2338)	
training:	Epoch: [14][85/233]	Loss 0.2888 (0.2344)	
training:	Epoch: [14][86/233]	Loss 0.1746 (0.2337)	
training:	Epoch: [14][87/233]	Loss 0.1989 (0.2333)	
training:	Epoch: [14][88/233]	Loss 0.2178 (0.2332)	
training:	Epoch: [14][89/233]	Loss 0.2367 (0.2332)	
training:	Epoch: [14][90/233]	Loss 0.2435 (0.2333)	
training:	Epoch: [14][91/233]	Loss 0.1634 (0.2325)	
training:	Epoch: [14][92/233]	Loss 0.1817 (0.2320)	
training:	Epoch: [14][93/233]	Loss 0.2830 (0.2325)	
training:	Epoch: [14][94/233]	Loss 0.2184 (0.2324)	
training:	Epoch: [14][95/233]	Loss 0.2150 (0.2322)	
training:	Epoch: [14][96/233]	Loss 0.2027 (0.2319)	
training:	Epoch: [14][97/233]	Loss 0.2401 (0.2320)	
training:	Epoch: [14][98/233]	Loss 0.3444 (0.2331)	
training:	Epoch: [14][99/233]	Loss 0.1808 (0.2326)	
training:	Epoch: [14][100/233]	Loss 0.2692 (0.2330)	
training:	Epoch: [14][101/233]	Loss 0.3235 (0.2339)	
training:	Epoch: [14][102/233]	Loss 0.1816 (0.2334)	
training:	Epoch: [14][103/233]	Loss 0.2323 (0.2333)	
training:	Epoch: [14][104/233]	Loss 0.2491 (0.2335)	
training:	Epoch: [14][105/233]	Loss 0.2170 (0.2333)	
training:	Epoch: [14][106/233]	Loss 0.2364 (0.2334)	
training:	Epoch: [14][107/233]	Loss 0.1953 (0.2330)	
training:	Epoch: [14][108/233]	Loss 0.2818 (0.2335)	
training:	Epoch: [14][109/233]	Loss 0.2249 (0.2334)	
training:	Epoch: [14][110/233]	Loss 0.2098 (0.2332)	
training:	Epoch: [14][111/233]	Loss 0.2321 (0.2332)	
training:	Epoch: [14][112/233]	Loss 0.2746 (0.2335)	
training:	Epoch: [14][113/233]	Loss 0.2514 (0.2337)	
training:	Epoch: [14][114/233]	Loss 0.1681 (0.2331)	
training:	Epoch: [14][115/233]	Loss 0.2545 (0.2333)	
training:	Epoch: [14][116/233]	Loss 0.2639 (0.2336)	
training:	Epoch: [14][117/233]	Loss 0.1802 (0.2331)	
training:	Epoch: [14][118/233]	Loss 0.1867 (0.2327)	
training:	Epoch: [14][119/233]	Loss 0.2208 (0.2326)	
training:	Epoch: [14][120/233]	Loss 0.1779 (0.2322)	
training:	Epoch: [14][121/233]	Loss 0.2086 (0.2320)	
training:	Epoch: [14][122/233]	Loss 0.1976 (0.2317)	
training:	Epoch: [14][123/233]	Loss 0.3289 (0.2325)	
training:	Epoch: [14][124/233]	Loss 0.2757 (0.2328)	
training:	Epoch: [14][125/233]	Loss 0.2000 (0.2326)	
training:	Epoch: [14][126/233]	Loss 0.1840 (0.2322)	
training:	Epoch: [14][127/233]	Loss 0.1758 (0.2317)	
training:	Epoch: [14][128/233]	Loss 0.2344 (0.2317)	
training:	Epoch: [14][129/233]	Loss 0.2266 (0.2317)	
training:	Epoch: [14][130/233]	Loss 0.1975 (0.2314)	
training:	Epoch: [14][131/233]	Loss 0.2330 (0.2315)	
training:	Epoch: [14][132/233]	Loss 0.2037 (0.2312)	
training:	Epoch: [14][133/233]	Loss 0.2377 (0.2313)	
training:	Epoch: [14][134/233]	Loss 0.2171 (0.2312)	
training:	Epoch: [14][135/233]	Loss 0.2684 (0.2315)	
training:	Epoch: [14][136/233]	Loss 0.1982 (0.2312)	
training:	Epoch: [14][137/233]	Loss 0.2222 (0.2312)	
training:	Epoch: [14][138/233]	Loss 0.1650 (0.2307)	
training:	Epoch: [14][139/233]	Loss 0.2108 (0.2305)	
training:	Epoch: [14][140/233]	Loss 0.2349 (0.2306)	
training:	Epoch: [14][141/233]	Loss 0.2925 (0.2310)	
training:	Epoch: [14][142/233]	Loss 0.2539 (0.2312)	
training:	Epoch: [14][143/233]	Loss 0.1867 (0.2309)	
training:	Epoch: [14][144/233]	Loss 0.1856 (0.2305)	
training:	Epoch: [14][145/233]	Loss 0.1720 (0.2301)	
training:	Epoch: [14][146/233]	Loss 0.3733 (0.2311)	
training:	Epoch: [14][147/233]	Loss 0.2090 (0.2310)	
training:	Epoch: [14][148/233]	Loss 0.4038 (0.2321)	
training:	Epoch: [14][149/233]	Loss 0.2324 (0.2321)	
training:	Epoch: [14][150/233]	Loss 0.2232 (0.2321)	
training:	Epoch: [14][151/233]	Loss 0.1988 (0.2319)	
training:	Epoch: [14][152/233]	Loss 0.3298 (0.2325)	
training:	Epoch: [14][153/233]	Loss 0.2360 (0.2325)	
training:	Epoch: [14][154/233]	Loss 0.2109 (0.2324)	
training:	Epoch: [14][155/233]	Loss 0.2636 (0.2326)	
training:	Epoch: [14][156/233]	Loss 0.2380 (0.2326)	
training:	Epoch: [14][157/233]	Loss 0.2434 (0.2327)	
training:	Epoch: [14][158/233]	Loss 0.2036 (0.2325)	
training:	Epoch: [14][159/233]	Loss 0.1943 (0.2323)	
training:	Epoch: [14][160/233]	Loss 0.2116 (0.2321)	
training:	Epoch: [14][161/233]	Loss 0.1914 (0.2319)	
training:	Epoch: [14][162/233]	Loss 0.2478 (0.2320)	
training:	Epoch: [14][163/233]	Loss 0.2820 (0.2323)	
training:	Epoch: [14][164/233]	Loss 0.3576 (0.2330)	
training:	Epoch: [14][165/233]	Loss 0.2899 (0.2334)	
training:	Epoch: [14][166/233]	Loss 0.1973 (0.2332)	
training:	Epoch: [14][167/233]	Loss 0.2071 (0.2330)	
training:	Epoch: [14][168/233]	Loss 0.2530 (0.2331)	
training:	Epoch: [14][169/233]	Loss 0.2973 (0.2335)	
training:	Epoch: [14][170/233]	Loss 0.1581 (0.2331)	
training:	Epoch: [14][171/233]	Loss 0.2074 (0.2329)	
training:	Epoch: [14][172/233]	Loss 0.2086 (0.2328)	
training:	Epoch: [14][173/233]	Loss 0.2020 (0.2326)	
training:	Epoch: [14][174/233]	Loss 0.1958 (0.2324)	
training:	Epoch: [14][175/233]	Loss 0.2890 (0.2327)	
training:	Epoch: [14][176/233]	Loss 0.1930 (0.2325)	
training:	Epoch: [14][177/233]	Loss 0.1904 (0.2323)	
training:	Epoch: [14][178/233]	Loss 0.1570 (0.2318)	
training:	Epoch: [14][179/233]	Loss 0.1891 (0.2316)	
training:	Epoch: [14][180/233]	Loss 0.2061 (0.2315)	
training:	Epoch: [14][181/233]	Loss 0.1974 (0.2313)	
training:	Epoch: [14][182/233]	Loss 0.2173 (0.2312)	
training:	Epoch: [14][183/233]	Loss 0.2321 (0.2312)	
training:	Epoch: [14][184/233]	Loss 0.3182 (0.2317)	
training:	Epoch: [14][185/233]	Loss 0.2485 (0.2318)	
training:	Epoch: [14][186/233]	Loss 0.2529 (0.2319)	
training:	Epoch: [14][187/233]	Loss 0.1664 (0.2315)	
training:	Epoch: [14][188/233]	Loss 0.2241 (0.2315)	
training:	Epoch: [14][189/233]	Loss 0.2752 (0.2317)	
training:	Epoch: [14][190/233]	Loss 0.3135 (0.2321)	
training:	Epoch: [14][191/233]	Loss 0.1940 (0.2319)	
training:	Epoch: [14][192/233]	Loss 0.2662 (0.2321)	
training:	Epoch: [14][193/233]	Loss 0.2400 (0.2322)	
training:	Epoch: [14][194/233]	Loss 0.1613 (0.2318)	
training:	Epoch: [14][195/233]	Loss 0.2987 (0.2321)	
training:	Epoch: [14][196/233]	Loss 0.1998 (0.2320)	
training:	Epoch: [14][197/233]	Loss 0.2607 (0.2321)	
training:	Epoch: [14][198/233]	Loss 0.2607 (0.2323)	
training:	Epoch: [14][199/233]	Loss 0.3439 (0.2328)	
training:	Epoch: [14][200/233]	Loss 0.2615 (0.2330)	
training:	Epoch: [14][201/233]	Loss 0.1439 (0.2325)	
training:	Epoch: [14][202/233]	Loss 0.2228 (0.2325)	
training:	Epoch: [14][203/233]	Loss 0.2003 (0.2323)	
training:	Epoch: [14][204/233]	Loss 0.2011 (0.2322)	
training:	Epoch: [14][205/233]	Loss 0.2480 (0.2322)	
training:	Epoch: [14][206/233]	Loss 0.2248 (0.2322)	
training:	Epoch: [14][207/233]	Loss 0.2461 (0.2323)	
training:	Epoch: [14][208/233]	Loss 0.2032 (0.2321)	
training:	Epoch: [14][209/233]	Loss 0.2034 (0.2320)	
training:	Epoch: [14][210/233]	Loss 0.2477 (0.2321)	
training:	Epoch: [14][211/233]	Loss 0.1655 (0.2318)	
training:	Epoch: [14][212/233]	Loss 0.2753 (0.2320)	
training:	Epoch: [14][213/233]	Loss 0.2323 (0.2320)	
training:	Epoch: [14][214/233]	Loss 0.1991 (0.2318)	
training:	Epoch: [14][215/233]	Loss 0.2436 (0.2319)	
training:	Epoch: [14][216/233]	Loss 0.2892 (0.2321)	
training:	Epoch: [14][217/233]	Loss 0.1808 (0.2319)	
training:	Epoch: [14][218/233]	Loss 0.2158 (0.2318)	
training:	Epoch: [14][219/233]	Loss 0.2946 (0.2321)	
training:	Epoch: [14][220/233]	Loss 0.2756 (0.2323)	
training:	Epoch: [14][221/233]	Loss 0.1814 (0.2321)	
training:	Epoch: [14][222/233]	Loss 0.2485 (0.2321)	
training:	Epoch: [14][223/233]	Loss 0.2333 (0.2322)	
training:	Epoch: [14][224/233]	Loss 0.2336 (0.2322)	
training:	Epoch: [14][225/233]	Loss 0.2001 (0.2320)	
training:	Epoch: [14][226/233]	Loss 0.2627 (0.2322)	
training:	Epoch: [14][227/233]	Loss 0.3025 (0.2325)	
training:	Epoch: [14][228/233]	Loss 0.1757 (0.2322)	
training:	Epoch: [14][229/233]	Loss 0.2412 (0.2323)	
training:	Epoch: [14][230/233]	Loss 0.2143 (0.2322)	
training:	Epoch: [14][231/233]	Loss 0.2050 (0.2321)	
training:	Epoch: [14][232/233]	Loss 0.2441 (0.2321)	
training:	Epoch: [14][233/233]	Loss 0.2715 (0.2323)	
Training:	 Loss: 0.2317

Training:	 ACC: 0.9687 0.9690 0.9736 0.9639
Validation:	 ACC: 0.8170 0.8186 0.8536 0.7803
Validation:	 Best_BACC: 0.8183 0.8197 0.8485 0.7881
Validation:	 Loss: 0.3962
Pretraining:	Epoch 15/200
----------
training:	Epoch: [15][1/233]	Loss 0.2184 (0.2184)	
training:	Epoch: [15][2/233]	Loss 0.1877 (0.2030)	
training:	Epoch: [15][3/233]	Loss 0.2085 (0.2049)	
training:	Epoch: [15][4/233]	Loss 0.2242 (0.2097)	
training:	Epoch: [15][5/233]	Loss 0.2013 (0.2080)	
training:	Epoch: [15][6/233]	Loss 0.3154 (0.2259)	
training:	Epoch: [15][7/233]	Loss 0.1862 (0.2202)	
training:	Epoch: [15][8/233]	Loss 0.2639 (0.2257)	
training:	Epoch: [15][9/233]	Loss 0.2003 (0.2229)	
training:	Epoch: [15][10/233]	Loss 0.1980 (0.2204)	
training:	Epoch: [15][11/233]	Loss 0.3757 (0.2345)	
training:	Epoch: [15][12/233]	Loss 0.2026 (0.2318)	
training:	Epoch: [15][13/233]	Loss 0.1775 (0.2276)	
training:	Epoch: [15][14/233]	Loss 0.2949 (0.2324)	
training:	Epoch: [15][15/233]	Loss 0.2755 (0.2353)	
training:	Epoch: [15][16/233]	Loss 0.1856 (0.2322)	
training:	Epoch: [15][17/233]	Loss 0.2411 (0.2327)	
training:	Epoch: [15][18/233]	Loss 0.2329 (0.2327)	
training:	Epoch: [15][19/233]	Loss 0.2089 (0.2315)	
training:	Epoch: [15][20/233]	Loss 0.1870 (0.2293)	
training:	Epoch: [15][21/233]	Loss 0.2179 (0.2287)	
training:	Epoch: [15][22/233]	Loss 0.2048 (0.2276)	
training:	Epoch: [15][23/233]	Loss 0.2504 (0.2286)	
training:	Epoch: [15][24/233]	Loss 0.1852 (0.2268)	
training:	Epoch: [15][25/233]	Loss 0.2344 (0.2271)	
training:	Epoch: [15][26/233]	Loss 0.1742 (0.2251)	
training:	Epoch: [15][27/233]	Loss 0.1627 (0.2228)	
training:	Epoch: [15][28/233]	Loss 0.2471 (0.2236)	
training:	Epoch: [15][29/233]	Loss 0.1999 (0.2228)	
training:	Epoch: [15][30/233]	Loss 0.1501 (0.2204)	
training:	Epoch: [15][31/233]	Loss 0.2016 (0.2198)	
training:	Epoch: [15][32/233]	Loss 0.2137 (0.2196)	
training:	Epoch: [15][33/233]	Loss 0.2179 (0.2196)	
training:	Epoch: [15][34/233]	Loss 0.2059 (0.2192)	
training:	Epoch: [15][35/233]	Loss 0.2151 (0.2190)	
training:	Epoch: [15][36/233]	Loss 0.2180 (0.2190)	
training:	Epoch: [15][37/233]	Loss 0.1880 (0.2182)	
training:	Epoch: [15][38/233]	Loss 0.2717 (0.2196)	
training:	Epoch: [15][39/233]	Loss 0.1672 (0.2182)	
training:	Epoch: [15][40/233]	Loss 0.1995 (0.2178)	
training:	Epoch: [15][41/233]	Loss 0.2960 (0.2197)	
training:	Epoch: [15][42/233]	Loss 0.2086 (0.2194)	
training:	Epoch: [15][43/233]	Loss 0.1868 (0.2187)	
training:	Epoch: [15][44/233]	Loss 0.1693 (0.2175)	
training:	Epoch: [15][45/233]	Loss 0.2468 (0.2182)	
training:	Epoch: [15][46/233]	Loss 0.2598 (0.2191)	
training:	Epoch: [15][47/233]	Loss 0.2511 (0.2198)	
training:	Epoch: [15][48/233]	Loss 0.2781 (0.2210)	
training:	Epoch: [15][49/233]	Loss 0.2350 (0.2213)	
training:	Epoch: [15][50/233]	Loss 0.2086 (0.2210)	
training:	Epoch: [15][51/233]	Loss 0.2144 (0.2209)	
training:	Epoch: [15][52/233]	Loss 0.2258 (0.2210)	
training:	Epoch: [15][53/233]	Loss 0.2369 (0.2213)	
training:	Epoch: [15][54/233]	Loss 0.2461 (0.2217)	
training:	Epoch: [15][55/233]	Loss 0.2825 (0.2228)	
training:	Epoch: [15][56/233]	Loss 0.2468 (0.2233)	
training:	Epoch: [15][57/233]	Loss 0.2224 (0.2233)	
training:	Epoch: [15][58/233]	Loss 0.2265 (0.2233)	
training:	Epoch: [15][59/233]	Loss 0.2244 (0.2233)	
training:	Epoch: [15][60/233]	Loss 0.1810 (0.2226)	
training:	Epoch: [15][61/233]	Loss 0.2019 (0.2223)	
training:	Epoch: [15][62/233]	Loss 0.2251 (0.2223)	
training:	Epoch: [15][63/233]	Loss 0.2130 (0.2222)	
training:	Epoch: [15][64/233]	Loss 0.1625 (0.2213)	
training:	Epoch: [15][65/233]	Loss 0.2096 (0.2211)	
training:	Epoch: [15][66/233]	Loss 0.2594 (0.2217)	
training:	Epoch: [15][67/233]	Loss 0.2834 (0.2226)	
training:	Epoch: [15][68/233]	Loss 0.2448 (0.2229)	
training:	Epoch: [15][69/233]	Loss 0.2264 (0.2230)	
training:	Epoch: [15][70/233]	Loss 0.3602 (0.2249)	
training:	Epoch: [15][71/233]	Loss 0.1952 (0.2245)	
training:	Epoch: [15][72/233]	Loss 0.1935 (0.2241)	
training:	Epoch: [15][73/233]	Loss 0.2572 (0.2245)	
training:	Epoch: [15][74/233]	Loss 0.2128 (0.2244)	
training:	Epoch: [15][75/233]	Loss 0.1978 (0.2240)	
training:	Epoch: [15][76/233]	Loss 0.1765 (0.2234)	
training:	Epoch: [15][77/233]	Loss 0.1786 (0.2228)	
training:	Epoch: [15][78/233]	Loss 0.2313 (0.2229)	
training:	Epoch: [15][79/233]	Loss 0.1889 (0.2225)	
training:	Epoch: [15][80/233]	Loss 0.2613 (0.2230)	
training:	Epoch: [15][81/233]	Loss 0.2257 (0.2230)	
training:	Epoch: [15][82/233]	Loss 0.2415 (0.2232)	
training:	Epoch: [15][83/233]	Loss 0.2345 (0.2234)	
training:	Epoch: [15][84/233]	Loss 0.2198 (0.2233)	
training:	Epoch: [15][85/233]	Loss 0.1804 (0.2228)	
training:	Epoch: [15][86/233]	Loss 0.3631 (0.2244)	
training:	Epoch: [15][87/233]	Loss 0.2682 (0.2249)	
training:	Epoch: [15][88/233]	Loss 0.1690 (0.2243)	
training:	Epoch: [15][89/233]	Loss 0.2186 (0.2242)	
training:	Epoch: [15][90/233]	Loss 0.1723 (0.2237)	
training:	Epoch: [15][91/233]	Loss 0.3107 (0.2246)	
training:	Epoch: [15][92/233]	Loss 0.1704 (0.2240)	
training:	Epoch: [15][93/233]	Loss 0.1757 (0.2235)	
training:	Epoch: [15][94/233]	Loss 0.1733 (0.2230)	
training:	Epoch: [15][95/233]	Loss 0.2881 (0.2237)	
training:	Epoch: [15][96/233]	Loss 0.1771 (0.2232)	
training:	Epoch: [15][97/233]	Loss 0.2676 (0.2236)	
training:	Epoch: [15][98/233]	Loss 0.1953 (0.2233)	
training:	Epoch: [15][99/233]	Loss 0.2050 (0.2232)	
training:	Epoch: [15][100/233]	Loss 0.2196 (0.2231)	
training:	Epoch: [15][101/233]	Loss 0.2876 (0.2238)	
training:	Epoch: [15][102/233]	Loss 0.2597 (0.2241)	
training:	Epoch: [15][103/233]	Loss 0.2224 (0.2241)	
training:	Epoch: [15][104/233]	Loss 0.2287 (0.2241)	
training:	Epoch: [15][105/233]	Loss 0.2843 (0.2247)	
training:	Epoch: [15][106/233]	Loss 0.1803 (0.2243)	
training:	Epoch: [15][107/233]	Loss 0.2996 (0.2250)	
training:	Epoch: [15][108/233]	Loss 0.2396 (0.2251)	
training:	Epoch: [15][109/233]	Loss 0.1928 (0.2248)	
training:	Epoch: [15][110/233]	Loss 0.1678 (0.2243)	
training:	Epoch: [15][111/233]	Loss 0.2057 (0.2242)	
training:	Epoch: [15][112/233]	Loss 0.2407 (0.2243)	
training:	Epoch: [15][113/233]	Loss 0.2680 (0.2247)	
training:	Epoch: [15][114/233]	Loss 0.2182 (0.2246)	
training:	Epoch: [15][115/233]	Loss 0.2990 (0.2253)	
training:	Epoch: [15][116/233]	Loss 0.2516 (0.2255)	
training:	Epoch: [15][117/233]	Loss 0.1781 (0.2251)	
training:	Epoch: [15][118/233]	Loss 0.1519 (0.2245)	
training:	Epoch: [15][119/233]	Loss 0.2368 (0.2246)	
training:	Epoch: [15][120/233]	Loss 0.1919 (0.2243)	
training:	Epoch: [15][121/233]	Loss 0.2031 (0.2241)	
training:	Epoch: [15][122/233]	Loss 0.2061 (0.2240)	
training:	Epoch: [15][123/233]	Loss 0.1702 (0.2236)	
training:	Epoch: [15][124/233]	Loss 0.2045 (0.2234)	
training:	Epoch: [15][125/233]	Loss 0.1920 (0.2231)	
training:	Epoch: [15][126/233]	Loss 0.2079 (0.2230)	
training:	Epoch: [15][127/233]	Loss 0.2105 (0.2229)	
training:	Epoch: [15][128/233]	Loss 0.1678 (0.2225)	
training:	Epoch: [15][129/233]	Loss 0.1653 (0.2221)	
training:	Epoch: [15][130/233]	Loss 0.2235 (0.2221)	
training:	Epoch: [15][131/233]	Loss 0.2567 (0.2223)	
training:	Epoch: [15][132/233]	Loss 0.1595 (0.2219)	
training:	Epoch: [15][133/233]	Loss 0.2371 (0.2220)	
training:	Epoch: [15][134/233]	Loss 0.1780 (0.2216)	
training:	Epoch: [15][135/233]	Loss 0.2203 (0.2216)	
training:	Epoch: [15][136/233]	Loss 0.1950 (0.2214)	
training:	Epoch: [15][137/233]	Loss 0.2148 (0.2214)	
training:	Epoch: [15][138/233]	Loss 0.2338 (0.2215)	
training:	Epoch: [15][139/233]	Loss 0.2967 (0.2220)	
training:	Epoch: [15][140/233]	Loss 0.3532 (0.2230)	
training:	Epoch: [15][141/233]	Loss 0.1751 (0.2226)	
training:	Epoch: [15][142/233]	Loss 0.2440 (0.2228)	
training:	Epoch: [15][143/233]	Loss 0.1947 (0.2226)	
training:	Epoch: [15][144/233]	Loss 0.1321 (0.2219)	
training:	Epoch: [15][145/233]	Loss 0.2457 (0.2221)	
training:	Epoch: [15][146/233]	Loss 0.2778 (0.2225)	
training:	Epoch: [15][147/233]	Loss 0.2398 (0.2226)	
training:	Epoch: [15][148/233]	Loss 0.2065 (0.2225)	
training:	Epoch: [15][149/233]	Loss 0.1948 (0.2223)	
training:	Epoch: [15][150/233]	Loss 0.1697 (0.2220)	
training:	Epoch: [15][151/233]	Loss 0.1640 (0.2216)	
training:	Epoch: [15][152/233]	Loss 0.2486 (0.2218)	
training:	Epoch: [15][153/233]	Loss 0.2030 (0.2216)	
training:	Epoch: [15][154/233]	Loss 0.2664 (0.2219)	
training:	Epoch: [15][155/233]	Loss 0.1818 (0.2217)	
training:	Epoch: [15][156/233]	Loss 0.1994 (0.2215)	
training:	Epoch: [15][157/233]	Loss 0.2576 (0.2217)	
training:	Epoch: [15][158/233]	Loss 0.1881 (0.2215)	
training:	Epoch: [15][159/233]	Loss 0.1700 (0.2212)	
training:	Epoch: [15][160/233]	Loss 0.1962 (0.2211)	
training:	Epoch: [15][161/233]	Loss 0.1751 (0.2208)	
training:	Epoch: [15][162/233]	Loss 0.2212 (0.2208)	
training:	Epoch: [15][163/233]	Loss 0.2103 (0.2207)	
training:	Epoch: [15][164/233]	Loss 0.3177 (0.2213)	
training:	Epoch: [15][165/233]	Loss 0.2393 (0.2214)	
training:	Epoch: [15][166/233]	Loss 0.2759 (0.2217)	
training:	Epoch: [15][167/233]	Loss 0.2116 (0.2217)	
training:	Epoch: [15][168/233]	Loss 0.1750 (0.2214)	
training:	Epoch: [15][169/233]	Loss 0.2025 (0.2213)	
training:	Epoch: [15][170/233]	Loss 0.2284 (0.2213)	
training:	Epoch: [15][171/233]	Loss 0.2070 (0.2212)	
training:	Epoch: [15][172/233]	Loss 0.2282 (0.2213)	
training:	Epoch: [15][173/233]	Loss 0.2252 (0.2213)	
training:	Epoch: [15][174/233]	Loss 0.2009 (0.2212)	
training:	Epoch: [15][175/233]	Loss 0.1725 (0.2209)	
training:	Epoch: [15][176/233]	Loss 0.1988 (0.2208)	
training:	Epoch: [15][177/233]	Loss 0.2100 (0.2207)	
training:	Epoch: [15][178/233]	Loss 0.2705 (0.2210)	
training:	Epoch: [15][179/233]	Loss 0.2164 (0.2210)	
training:	Epoch: [15][180/233]	Loss 0.1637 (0.2207)	
training:	Epoch: [15][181/233]	Loss 0.1775 (0.2204)	
training:	Epoch: [15][182/233]	Loss 0.3210 (0.2210)	
training:	Epoch: [15][183/233]	Loss 0.2290 (0.2210)	
training:	Epoch: [15][184/233]	Loss 0.1951 (0.2209)	
training:	Epoch: [15][185/233]	Loss 0.2287 (0.2209)	
training:	Epoch: [15][186/233]	Loss 0.2045 (0.2208)	
training:	Epoch: [15][187/233]	Loss 0.1950 (0.2207)	
training:	Epoch: [15][188/233]	Loss 0.2886 (0.2211)	
training:	Epoch: [15][189/233]	Loss 0.2028 (0.2210)	
training:	Epoch: [15][190/233]	Loss 0.2151 (0.2209)	
training:	Epoch: [15][191/233]	Loss 0.1932 (0.2208)	
training:	Epoch: [15][192/233]	Loss 0.1836 (0.2206)	
training:	Epoch: [15][193/233]	Loss 0.2649 (0.2208)	
training:	Epoch: [15][194/233]	Loss 0.2189 (0.2208)	
training:	Epoch: [15][195/233]	Loss 0.2137 (0.2208)	
training:	Epoch: [15][196/233]	Loss 0.2018 (0.2207)	
training:	Epoch: [15][197/233]	Loss 0.2204 (0.2207)	
training:	Epoch: [15][198/233]	Loss 0.2280 (0.2207)	
training:	Epoch: [15][199/233]	Loss 0.1530 (0.2204)	
training:	Epoch: [15][200/233]	Loss 0.2773 (0.2207)	
training:	Epoch: [15][201/233]	Loss 0.1666 (0.2204)	
training:	Epoch: [15][202/233]	Loss 0.2243 (0.2204)	
training:	Epoch: [15][203/233]	Loss 0.2283 (0.2204)	
training:	Epoch: [15][204/233]	Loss 0.2268 (0.2205)	
training:	Epoch: [15][205/233]	Loss 0.2236 (0.2205)	
training:	Epoch: [15][206/233]	Loss 0.2491 (0.2206)	
training:	Epoch: [15][207/233]	Loss 0.2006 (0.2205)	
training:	Epoch: [15][208/233]	Loss 0.2046 (0.2205)	
training:	Epoch: [15][209/233]	Loss 0.1740 (0.2202)	
training:	Epoch: [15][210/233]	Loss 0.1827 (0.2201)	
training:	Epoch: [15][211/233]	Loss 0.1789 (0.2199)	
training:	Epoch: [15][212/233]	Loss 0.1975 (0.2198)	
training:	Epoch: [15][213/233]	Loss 0.2469 (0.2199)	
training:	Epoch: [15][214/233]	Loss 0.2246 (0.2199)	
training:	Epoch: [15][215/233]	Loss 0.2076 (0.2198)	
training:	Epoch: [15][216/233]	Loss 0.2216 (0.2199)	
training:	Epoch: [15][217/233]	Loss 0.1200 (0.2194)	
training:	Epoch: [15][218/233]	Loss 0.1748 (0.2192)	
training:	Epoch: [15][219/233]	Loss 0.3438 (0.2198)	
training:	Epoch: [15][220/233]	Loss 0.2066 (0.2197)	
training:	Epoch: [15][221/233]	Loss 0.1771 (0.2195)	
training:	Epoch: [15][222/233]	Loss 0.5237 (0.2209)	
training:	Epoch: [15][223/233]	Loss 0.1934 (0.2208)	
training:	Epoch: [15][224/233]	Loss 0.2016 (0.2207)	
training:	Epoch: [15][225/233]	Loss 0.2206 (0.2207)	
training:	Epoch: [15][226/233]	Loss 0.3793 (0.2214)	
training:	Epoch: [15][227/233]	Loss 0.2044 (0.2213)	
training:	Epoch: [15][228/233]	Loss 0.1728 (0.2211)	
training:	Epoch: [15][229/233]	Loss 0.1842 (0.2209)	
training:	Epoch: [15][230/233]	Loss 0.2903 (0.2212)	
training:	Epoch: [15][231/233]	Loss 0.2158 (0.2212)	
training:	Epoch: [15][232/233]	Loss 0.1766 (0.2210)	
training:	Epoch: [15][233/233]	Loss 0.1793 (0.2208)	
Training:	 Loss: 0.2203

Training:	 ACC: 0.9720 0.9724 0.9813 0.9628
Validation:	 ACC: 0.8175 0.8197 0.8659 0.7691
Validation:	 Best_BACC: 0.8183 0.8197 0.8485 0.7881
Validation:	 Loss: 0.3953
Pretraining:	Epoch 16/200
----------
training:	Epoch: [16][1/233]	Loss 0.2250 (0.2250)	
training:	Epoch: [16][2/233]	Loss 0.2175 (0.2212)	
training:	Epoch: [16][3/233]	Loss 0.1752 (0.2059)	
training:	Epoch: [16][4/233]	Loss 0.2043 (0.2055)	
training:	Epoch: [16][5/233]	Loss 0.2229 (0.2090)	
training:	Epoch: [16][6/233]	Loss 0.2588 (0.2173)	
training:	Epoch: [16][7/233]	Loss 0.2285 (0.2189)	
training:	Epoch: [16][8/233]	Loss 0.1665 (0.2123)	
training:	Epoch: [16][9/233]	Loss 0.1764 (0.2083)	
training:	Epoch: [16][10/233]	Loss 0.2673 (0.2142)	
training:	Epoch: [16][11/233]	Loss 0.1576 (0.2091)	
training:	Epoch: [16][12/233]	Loss 0.1399 (0.2033)	
training:	Epoch: [16][13/233]	Loss 0.2375 (0.2059)	
training:	Epoch: [16][14/233]	Loss 0.2278 (0.2075)	
training:	Epoch: [16][15/233]	Loss 0.1920 (0.2065)	
training:	Epoch: [16][16/233]	Loss 0.1602 (0.2036)	
training:	Epoch: [16][17/233]	Loss 0.1862 (0.2026)	
training:	Epoch: [16][18/233]	Loss 0.1989 (0.2024)	
training:	Epoch: [16][19/233]	Loss 0.2196 (0.2033)	
training:	Epoch: [16][20/233]	Loss 0.2064 (0.2034)	
training:	Epoch: [16][21/233]	Loss 0.2055 (0.2035)	
training:	Epoch: [16][22/233]	Loss 0.2217 (0.2043)	
training:	Epoch: [16][23/233]	Loss 0.1777 (0.2032)	
training:	Epoch: [16][24/233]	Loss 0.2060 (0.2033)	
training:	Epoch: [16][25/233]	Loss 0.1760 (0.2022)	
training:	Epoch: [16][26/233]	Loss 0.1638 (0.2007)	
training:	Epoch: [16][27/233]	Loss 0.2908 (0.2041)	
training:	Epoch: [16][28/233]	Loss 0.1971 (0.2038)	
training:	Epoch: [16][29/233]	Loss 0.2237 (0.2045)	
training:	Epoch: [16][30/233]	Loss 0.1659 (0.2032)	
training:	Epoch: [16][31/233]	Loss 0.1843 (0.2026)	
training:	Epoch: [16][32/233]	Loss 0.2210 (0.2032)	
training:	Epoch: [16][33/233]	Loss 0.2131 (0.2035)	
training:	Epoch: [16][34/233]	Loss 0.2334 (0.2044)	
training:	Epoch: [16][35/233]	Loss 0.1963 (0.2041)	
training:	Epoch: [16][36/233]	Loss 0.2329 (0.2049)	
training:	Epoch: [16][37/233]	Loss 0.2213 (0.2054)	
training:	Epoch: [16][38/233]	Loss 0.2015 (0.2053)	
training:	Epoch: [16][39/233]	Loss 0.2168 (0.2056)	
training:	Epoch: [16][40/233]	Loss 0.1143 (0.2033)	
training:	Epoch: [16][41/233]	Loss 0.2140 (0.2035)	
training:	Epoch: [16][42/233]	Loss 0.1523 (0.2023)	
training:	Epoch: [16][43/233]	Loss 0.1800 (0.2018)	
training:	Epoch: [16][44/233]	Loss 0.1707 (0.2011)	
training:	Epoch: [16][45/233]	Loss 0.1969 (0.2010)	
training:	Epoch: [16][46/233]	Loss 0.2351 (0.2017)	
training:	Epoch: [16][47/233]	Loss 0.2943 (0.2037)	
training:	Epoch: [16][48/233]	Loss 0.2231 (0.2041)	
training:	Epoch: [16][49/233]	Loss 0.2372 (0.2048)	
training:	Epoch: [16][50/233]	Loss 0.2955 (0.2066)	
training:	Epoch: [16][51/233]	Loss 0.1699 (0.2059)	
training:	Epoch: [16][52/233]	Loss 0.1596 (0.2050)	
training:	Epoch: [16][53/233]	Loss 0.2395 (0.2056)	
training:	Epoch: [16][54/233]	Loss 0.2403 (0.2063)	
training:	Epoch: [16][55/233]	Loss 0.1912 (0.2060)	
training:	Epoch: [16][56/233]	Loss 0.2226 (0.2063)	
training:	Epoch: [16][57/233]	Loss 0.2273 (0.2067)	
training:	Epoch: [16][58/233]	Loss 0.1795 (0.2062)	
training:	Epoch: [16][59/233]	Loss 0.1618 (0.2055)	
training:	Epoch: [16][60/233]	Loss 0.2263 (0.2058)	
training:	Epoch: [16][61/233]	Loss 0.2558 (0.2066)	
training:	Epoch: [16][62/233]	Loss 0.3123 (0.2083)	
training:	Epoch: [16][63/233]	Loss 0.1993 (0.2082)	
training:	Epoch: [16][64/233]	Loss 0.2470 (0.2088)	
training:	Epoch: [16][65/233]	Loss 0.2241 (0.2090)	
training:	Epoch: [16][66/233]	Loss 0.1630 (0.2083)	
training:	Epoch: [16][67/233]	Loss 0.2539 (0.2090)	
training:	Epoch: [16][68/233]	Loss 0.2041 (0.2089)	
training:	Epoch: [16][69/233]	Loss 0.2275 (0.2092)	
training:	Epoch: [16][70/233]	Loss 0.2205 (0.2094)	
training:	Epoch: [16][71/233]	Loss 0.1643 (0.2087)	
training:	Epoch: [16][72/233]	Loss 0.1712 (0.2082)	
training:	Epoch: [16][73/233]	Loss 0.1804 (0.2078)	
training:	Epoch: [16][74/233]	Loss 0.2477 (0.2084)	
training:	Epoch: [16][75/233]	Loss 0.1886 (0.2081)	
training:	Epoch: [16][76/233]	Loss 0.1802 (0.2077)	
training:	Epoch: [16][77/233]	Loss 0.1571 (0.2071)	
training:	Epoch: [16][78/233]	Loss 0.2298 (0.2074)	
training:	Epoch: [16][79/233]	Loss 0.1765 (0.2070)	
training:	Epoch: [16][80/233]	Loss 0.1663 (0.2065)	
training:	Epoch: [16][81/233]	Loss 0.2162 (0.2066)	
training:	Epoch: [16][82/233]	Loss 0.1533 (0.2059)	
training:	Epoch: [16][83/233]	Loss 0.1993 (0.2059)	
training:	Epoch: [16][84/233]	Loss 0.1646 (0.2054)	
training:	Epoch: [16][85/233]	Loss 0.2132 (0.2055)	
training:	Epoch: [16][86/233]	Loss 0.1804 (0.2052)	
training:	Epoch: [16][87/233]	Loss 0.2386 (0.2056)	
training:	Epoch: [16][88/233]	Loss 0.2332 (0.2059)	
training:	Epoch: [16][89/233]	Loss 0.1846 (0.2056)	
training:	Epoch: [16][90/233]	Loss 0.1819 (0.2054)	
training:	Epoch: [16][91/233]	Loss 0.2335 (0.2057)	
training:	Epoch: [16][92/233]	Loss 0.1931 (0.2055)	
training:	Epoch: [16][93/233]	Loss 0.2493 (0.2060)	
training:	Epoch: [16][94/233]	Loss 0.2264 (0.2062)	
training:	Epoch: [16][95/233]	Loss 0.1781 (0.2059)	
training:	Epoch: [16][96/233]	Loss 0.2180 (0.2061)	
training:	Epoch: [16][97/233]	Loss 0.1955 (0.2060)	
training:	Epoch: [16][98/233]	Loss 0.2209 (0.2061)	
training:	Epoch: [16][99/233]	Loss 0.3342 (0.2074)	
training:	Epoch: [16][100/233]	Loss 0.3317 (0.2086)	
training:	Epoch: [16][101/233]	Loss 0.1718 (0.2083)	
training:	Epoch: [16][102/233]	Loss 0.2013 (0.2082)	
training:	Epoch: [16][103/233]	Loss 0.2469 (0.2086)	
training:	Epoch: [16][104/233]	Loss 0.2010 (0.2085)	
training:	Epoch: [16][105/233]	Loss 0.1613 (0.2081)	
training:	Epoch: [16][106/233]	Loss 0.2615 (0.2086)	
training:	Epoch: [16][107/233]	Loss 0.2016 (0.2085)	
training:	Epoch: [16][108/233]	Loss 0.1840 (0.2083)	
training:	Epoch: [16][109/233]	Loss 0.2647 (0.2088)	
training:	Epoch: [16][110/233]	Loss 0.2057 (0.2088)	
training:	Epoch: [16][111/233]	Loss 0.1759 (0.2085)	
training:	Epoch: [16][112/233]	Loss 0.1723 (0.2081)	
training:	Epoch: [16][113/233]	Loss 0.1915 (0.2080)	
training:	Epoch: [16][114/233]	Loss 0.1745 (0.2077)	
training:	Epoch: [16][115/233]	Loss 0.2560 (0.2081)	
training:	Epoch: [16][116/233]	Loss 0.1605 (0.2077)	
training:	Epoch: [16][117/233]	Loss 0.2427 (0.2080)	
training:	Epoch: [16][118/233]	Loss 0.1916 (0.2079)	
training:	Epoch: [16][119/233]	Loss 0.2419 (0.2082)	
training:	Epoch: [16][120/233]	Loss 0.2544 (0.2085)	
training:	Epoch: [16][121/233]	Loss 0.1997 (0.2085)	
training:	Epoch: [16][122/233]	Loss 0.1615 (0.2081)	
training:	Epoch: [16][123/233]	Loss 0.1648 (0.2077)	
training:	Epoch: [16][124/233]	Loss 0.1642 (0.2074)	
training:	Epoch: [16][125/233]	Loss 0.2375 (0.2076)	
training:	Epoch: [16][126/233]	Loss 0.2578 (0.2080)	
training:	Epoch: [16][127/233]	Loss 0.2098 (0.2080)	
training:	Epoch: [16][128/233]	Loss 0.2847 (0.2086)	
training:	Epoch: [16][129/233]	Loss 0.1607 (0.2083)	
training:	Epoch: [16][130/233]	Loss 0.1979 (0.2082)	
training:	Epoch: [16][131/233]	Loss 0.1643 (0.2078)	
training:	Epoch: [16][132/233]	Loss 0.2196 (0.2079)	
training:	Epoch: [16][133/233]	Loss 0.2756 (0.2084)	
training:	Epoch: [16][134/233]	Loss 0.1928 (0.2083)	
training:	Epoch: [16][135/233]	Loss 0.1723 (0.2081)	
training:	Epoch: [16][136/233]	Loss 0.2106 (0.2081)	
training:	Epoch: [16][137/233]	Loss 0.2651 (0.2085)	
training:	Epoch: [16][138/233]	Loss 0.1990 (0.2084)	
training:	Epoch: [16][139/233]	Loss 0.2477 (0.2087)	
training:	Epoch: [16][140/233]	Loss 0.1944 (0.2086)	
training:	Epoch: [16][141/233]	Loss 0.1886 (0.2085)	
training:	Epoch: [16][142/233]	Loss 0.2219 (0.2086)	
training:	Epoch: [16][143/233]	Loss 0.1629 (0.2082)	
training:	Epoch: [16][144/233]	Loss 0.2421 (0.2085)	
training:	Epoch: [16][145/233]	Loss 0.1415 (0.2080)	
training:	Epoch: [16][146/233]	Loss 0.2014 (0.2080)	
training:	Epoch: [16][147/233]	Loss 0.1565 (0.2076)	
training:	Epoch: [16][148/233]	Loss 0.2072 (0.2076)	
training:	Epoch: [16][149/233]	Loss 0.2170 (0.2077)	
training:	Epoch: [16][150/233]	Loss 0.1670 (0.2074)	
training:	Epoch: [16][151/233]	Loss 0.1784 (0.2072)	
training:	Epoch: [16][152/233]	Loss 0.1808 (0.2070)	
training:	Epoch: [16][153/233]	Loss 0.2119 (0.2071)	
training:	Epoch: [16][154/233]	Loss 0.2184 (0.2071)	
training:	Epoch: [16][155/233]	Loss 0.3476 (0.2081)	
training:	Epoch: [16][156/233]	Loss 0.2886 (0.2086)	
training:	Epoch: [16][157/233]	Loss 0.1816 (0.2084)	
training:	Epoch: [16][158/233]	Loss 0.1836 (0.2082)	
training:	Epoch: [16][159/233]	Loss 0.1861 (0.2081)	
training:	Epoch: [16][160/233]	Loss 0.1721 (0.2079)	
training:	Epoch: [16][161/233]	Loss 0.1826 (0.2077)	
training:	Epoch: [16][162/233]	Loss 0.1494 (0.2074)	
training:	Epoch: [16][163/233]	Loss 0.1928 (0.2073)	
training:	Epoch: [16][164/233]	Loss 0.2250 (0.2074)	
training:	Epoch: [16][165/233]	Loss 0.2154 (0.2074)	
training:	Epoch: [16][166/233]	Loss 0.3041 (0.2080)	
training:	Epoch: [16][167/233]	Loss 0.2225 (0.2081)	
training:	Epoch: [16][168/233]	Loss 0.1937 (0.2080)	
training:	Epoch: [16][169/233]	Loss 0.2186 (0.2081)	
training:	Epoch: [16][170/233]	Loss 0.2248 (0.2082)	
training:	Epoch: [16][171/233]	Loss 0.2471 (0.2084)	
training:	Epoch: [16][172/233]	Loss 0.2451 (0.2086)	
training:	Epoch: [16][173/233]	Loss 0.2583 (0.2089)	
training:	Epoch: [16][174/233]	Loss 0.2010 (0.2089)	
training:	Epoch: [16][175/233]	Loss 0.2054 (0.2088)	
training:	Epoch: [16][176/233]	Loss 0.2654 (0.2092)	
training:	Epoch: [16][177/233]	Loss 0.2281 (0.2093)	
training:	Epoch: [16][178/233]	Loss 0.2152 (0.2093)	
training:	Epoch: [16][179/233]	Loss 0.1954 (0.2092)	
training:	Epoch: [16][180/233]	Loss 0.2780 (0.2096)	
training:	Epoch: [16][181/233]	Loss 0.2131 (0.2096)	
training:	Epoch: [16][182/233]	Loss 0.2335 (0.2098)	
training:	Epoch: [16][183/233]	Loss 0.2527 (0.2100)	
training:	Epoch: [16][184/233]	Loss 0.2486 (0.2102)	
training:	Epoch: [16][185/233]	Loss 0.1686 (0.2100)	
training:	Epoch: [16][186/233]	Loss 0.2118 (0.2100)	
training:	Epoch: [16][187/233]	Loss 0.1716 (0.2098)	
training:	Epoch: [16][188/233]	Loss 0.2508 (0.2100)	
training:	Epoch: [16][189/233]	Loss 0.2308 (0.2101)	
training:	Epoch: [16][190/233]	Loss 0.1587 (0.2098)	
training:	Epoch: [16][191/233]	Loss 0.2124 (0.2098)	
training:	Epoch: [16][192/233]	Loss 0.2261 (0.2099)	
training:	Epoch: [16][193/233]	Loss 0.2560 (0.2102)	
training:	Epoch: [16][194/233]	Loss 0.1569 (0.2099)	
training:	Epoch: [16][195/233]	Loss 0.1644 (0.2097)	
training:	Epoch: [16][196/233]	Loss 0.2068 (0.2097)	
training:	Epoch: [16][197/233]	Loss 0.1656 (0.2094)	
training:	Epoch: [16][198/233]	Loss 0.2029 (0.2094)	
training:	Epoch: [16][199/233]	Loss 0.1619 (0.2092)	
training:	Epoch: [16][200/233]	Loss 0.1761 (0.2090)	
training:	Epoch: [16][201/233]	Loss 0.1782 (0.2088)	
training:	Epoch: [16][202/233]	Loss 0.1964 (0.2088)	
training:	Epoch: [16][203/233]	Loss 0.2363 (0.2089)	
training:	Epoch: [16][204/233]	Loss 0.2288 (0.2090)	
training:	Epoch: [16][205/233]	Loss 0.2500 (0.2092)	
training:	Epoch: [16][206/233]	Loss 0.2342 (0.2093)	
training:	Epoch: [16][207/233]	Loss 0.2758 (0.2097)	
training:	Epoch: [16][208/233]	Loss 0.2028 (0.2096)	
training:	Epoch: [16][209/233]	Loss 0.2753 (0.2099)	
training:	Epoch: [16][210/233]	Loss 0.1700 (0.2097)	
training:	Epoch: [16][211/233]	Loss 0.3425 (0.2104)	
training:	Epoch: [16][212/233]	Loss 0.2339 (0.2105)	
training:	Epoch: [16][213/233]	Loss 0.1380 (0.2101)	
training:	Epoch: [16][214/233]	Loss 0.1910 (0.2101)	
training:	Epoch: [16][215/233]	Loss 0.2135 (0.2101)	
training:	Epoch: [16][216/233]	Loss 0.2064 (0.2101)	
training:	Epoch: [16][217/233]	Loss 0.3368 (0.2106)	
training:	Epoch: [16][218/233]	Loss 0.1625 (0.2104)	
training:	Epoch: [16][219/233]	Loss 0.1555 (0.2102)	
training:	Epoch: [16][220/233]	Loss 0.2116 (0.2102)	
training:	Epoch: [16][221/233]	Loss 0.1785 (0.2100)	
training:	Epoch: [16][222/233]	Loss 0.2445 (0.2102)	
training:	Epoch: [16][223/233]	Loss 0.1805 (0.2100)	
training:	Epoch: [16][224/233]	Loss 0.2439 (0.2102)	
training:	Epoch: [16][225/233]	Loss 0.2311 (0.2103)	
training:	Epoch: [16][226/233]	Loss 0.2165 (0.2103)	
training:	Epoch: [16][227/233]	Loss 0.2088 (0.2103)	
training:	Epoch: [16][228/233]	Loss 0.2752 (0.2106)	
training:	Epoch: [16][229/233]	Loss 0.1882 (0.2105)	
training:	Epoch: [16][230/233]	Loss 0.1232 (0.2101)	
training:	Epoch: [16][231/233]	Loss 0.1574 (0.2099)	
training:	Epoch: [16][232/233]	Loss 0.2030 (0.2099)	
training:	Epoch: [16][233/233]	Loss 0.1908 (0.2098)	
Training:	 Loss: 0.2093

Training:	 ACC: 0.9747 0.9751 0.9844 0.9650
Validation:	 ACC: 0.8188 0.8213 0.8731 0.7646
Validation:	 Best_BACC: 0.8188 0.8213 0.8731 0.7646
Validation:	 Loss: 0.3930
Pretraining:	Epoch 17/200
----------
training:	Epoch: [17][1/233]	Loss 0.1305 (0.1305)	
training:	Epoch: [17][2/233]	Loss 0.2807 (0.2056)	
training:	Epoch: [17][3/233]	Loss 0.1470 (0.1861)	
training:	Epoch: [17][4/233]	Loss 0.1715 (0.1824)	
training:	Epoch: [17][5/233]	Loss 0.2251 (0.1909)	
training:	Epoch: [17][6/233]	Loss 0.1719 (0.1878)	
training:	Epoch: [17][7/233]	Loss 0.1616 (0.1840)	
training:	Epoch: [17][8/233]	Loss 0.1570 (0.1806)	
training:	Epoch: [17][9/233]	Loss 0.1584 (0.1782)	
training:	Epoch: [17][10/233]	Loss 0.1518 (0.1755)	
training:	Epoch: [17][11/233]	Loss 0.1908 (0.1769)	
training:	Epoch: [17][12/233]	Loss 0.1908 (0.1781)	
training:	Epoch: [17][13/233]	Loss 0.1711 (0.1775)	
training:	Epoch: [17][14/233]	Loss 0.2399 (0.1820)	
training:	Epoch: [17][15/233]	Loss 0.1535 (0.1801)	
training:	Epoch: [17][16/233]	Loss 0.1466 (0.1780)	
training:	Epoch: [17][17/233]	Loss 0.1767 (0.1779)	
training:	Epoch: [17][18/233]	Loss 0.2227 (0.1804)	
training:	Epoch: [17][19/233]	Loss 0.1499 (0.1788)	
training:	Epoch: [17][20/233]	Loss 0.1927 (0.1795)	
training:	Epoch: [17][21/233]	Loss 0.2365 (0.1822)	
training:	Epoch: [17][22/233]	Loss 0.2420 (0.1849)	
training:	Epoch: [17][23/233]	Loss 0.1330 (0.1827)	
training:	Epoch: [17][24/233]	Loss 0.1766 (0.1824)	
training:	Epoch: [17][25/233]	Loss 0.1725 (0.1820)	
training:	Epoch: [17][26/233]	Loss 0.2277 (0.1838)	
training:	Epoch: [17][27/233]	Loss 0.2297 (0.1855)	
training:	Epoch: [17][28/233]	Loss 0.1894 (0.1856)	
training:	Epoch: [17][29/233]	Loss 0.1977 (0.1860)	
training:	Epoch: [17][30/233]	Loss 0.1840 (0.1860)	
training:	Epoch: [17][31/233]	Loss 0.2483 (0.1880)	
training:	Epoch: [17][32/233]	Loss 0.2129 (0.1888)	
training:	Epoch: [17][33/233]	Loss 0.1471 (0.1875)	
training:	Epoch: [17][34/233]	Loss 0.1727 (0.1871)	
training:	Epoch: [17][35/233]	Loss 0.2264 (0.1882)	
training:	Epoch: [17][36/233]	Loss 0.2059 (0.1887)	
training:	Epoch: [17][37/233]	Loss 0.1418 (0.1874)	
training:	Epoch: [17][38/233]	Loss 0.1450 (0.1863)	
training:	Epoch: [17][39/233]	Loss 0.1706 (0.1859)	
training:	Epoch: [17][40/233]	Loss 0.1703 (0.1855)	
training:	Epoch: [17][41/233]	Loss 0.1999 (0.1859)	
training:	Epoch: [17][42/233]	Loss 0.1785 (0.1857)	
training:	Epoch: [17][43/233]	Loss 0.1737 (0.1854)	
training:	Epoch: [17][44/233]	Loss 0.1823 (0.1853)	
training:	Epoch: [17][45/233]	Loss 0.1660 (0.1849)	
training:	Epoch: [17][46/233]	Loss 0.1917 (0.1850)	
training:	Epoch: [17][47/233]	Loss 0.1793 (0.1849)	
training:	Epoch: [17][48/233]	Loss 0.1606 (0.1844)	
training:	Epoch: [17][49/233]	Loss 0.1944 (0.1846)	
training:	Epoch: [17][50/233]	Loss 0.1294 (0.1835)	
training:	Epoch: [17][51/233]	Loss 0.2616 (0.1851)	
training:	Epoch: [17][52/233]	Loss 0.1587 (0.1845)	
training:	Epoch: [17][53/233]	Loss 0.2029 (0.1849)	
training:	Epoch: [17][54/233]	Loss 0.1689 (0.1846)	
training:	Epoch: [17][55/233]	Loss 0.1978 (0.1848)	
training:	Epoch: [17][56/233]	Loss 0.1624 (0.1844)	
training:	Epoch: [17][57/233]	Loss 0.2085 (0.1849)	
training:	Epoch: [17][58/233]	Loss 0.2213 (0.1855)	
training:	Epoch: [17][59/233]	Loss 0.1801 (0.1854)	
training:	Epoch: [17][60/233]	Loss 0.2040 (0.1857)	
training:	Epoch: [17][61/233]	Loss 0.2248 (0.1863)	
training:	Epoch: [17][62/233]	Loss 0.1591 (0.1859)	
training:	Epoch: [17][63/233]	Loss 0.1596 (0.1855)	
training:	Epoch: [17][64/233]	Loss 0.2375 (0.1863)	
training:	Epoch: [17][65/233]	Loss 0.2164 (0.1868)	
training:	Epoch: [17][66/233]	Loss 0.1795 (0.1867)	
training:	Epoch: [17][67/233]	Loss 0.2595 (0.1877)	
training:	Epoch: [17][68/233]	Loss 0.1797 (0.1876)	
training:	Epoch: [17][69/233]	Loss 0.2648 (0.1887)	
training:	Epoch: [17][70/233]	Loss 0.1733 (0.1885)	
training:	Epoch: [17][71/233]	Loss 0.1842 (0.1885)	
training:	Epoch: [17][72/233]	Loss 0.2812 (0.1897)	
training:	Epoch: [17][73/233]	Loss 0.1879 (0.1897)	
training:	Epoch: [17][74/233]	Loss 0.2122 (0.1900)	
training:	Epoch: [17][75/233]	Loss 0.2380 (0.1907)	
training:	Epoch: [17][76/233]	Loss 0.2039 (0.1908)	
training:	Epoch: [17][77/233]	Loss 0.1906 (0.1908)	
training:	Epoch: [17][78/233]	Loss 0.2169 (0.1912)	
training:	Epoch: [17][79/233]	Loss 0.1987 (0.1913)	
training:	Epoch: [17][80/233]	Loss 0.1930 (0.1913)	
training:	Epoch: [17][81/233]	Loss 0.2419 (0.1919)	
training:	Epoch: [17][82/233]	Loss 0.1792 (0.1918)	
training:	Epoch: [17][83/233]	Loss 0.2496 (0.1925)	
training:	Epoch: [17][84/233]	Loss 0.1643 (0.1921)	
training:	Epoch: [17][85/233]	Loss 0.2124 (0.1924)	
training:	Epoch: [17][86/233]	Loss 0.1853 (0.1923)	
training:	Epoch: [17][87/233]	Loss 0.2403 (0.1928)	
training:	Epoch: [17][88/233]	Loss 0.1687 (0.1926)	
training:	Epoch: [17][89/233]	Loss 0.3105 (0.1939)	
training:	Epoch: [17][90/233]	Loss 0.2112 (0.1941)	
training:	Epoch: [17][91/233]	Loss 0.1861 (0.1940)	
training:	Epoch: [17][92/233]	Loss 0.2377 (0.1945)	
training:	Epoch: [17][93/233]	Loss 0.1846 (0.1944)	
training:	Epoch: [17][94/233]	Loss 0.2839 (0.1953)	
training:	Epoch: [17][95/233]	Loss 0.1929 (0.1953)	
training:	Epoch: [17][96/233]	Loss 0.1569 (0.1949)	
training:	Epoch: [17][97/233]	Loss 0.2248 (0.1952)	
training:	Epoch: [17][98/233]	Loss 0.1950 (0.1952)	
training:	Epoch: [17][99/233]	Loss 0.1580 (0.1948)	
training:	Epoch: [17][100/233]	Loss 0.1980 (0.1948)	
training:	Epoch: [17][101/233]	Loss 0.2081 (0.1950)	
training:	Epoch: [17][102/233]	Loss 0.1783 (0.1948)	
training:	Epoch: [17][103/233]	Loss 0.3013 (0.1958)	
training:	Epoch: [17][104/233]	Loss 0.2011 (0.1959)	
training:	Epoch: [17][105/233]	Loss 0.2076 (0.1960)	
training:	Epoch: [17][106/233]	Loss 0.1947 (0.1960)	
training:	Epoch: [17][107/233]	Loss 0.2423 (0.1964)	
training:	Epoch: [17][108/233]	Loss 0.2620 (0.1970)	
training:	Epoch: [17][109/233]	Loss 0.1733 (0.1968)	
training:	Epoch: [17][110/233]	Loss 0.1900 (0.1968)	
training:	Epoch: [17][111/233]	Loss 0.3590 (0.1982)	
training:	Epoch: [17][112/233]	Loss 0.2382 (0.1986)	
training:	Epoch: [17][113/233]	Loss 0.1834 (0.1984)	
training:	Epoch: [17][114/233]	Loss 0.2044 (0.1985)	
training:	Epoch: [17][115/233]	Loss 0.2116 (0.1986)	
training:	Epoch: [17][116/233]	Loss 0.1692 (0.1984)	
training:	Epoch: [17][117/233]	Loss 0.1556 (0.1980)	
training:	Epoch: [17][118/233]	Loss 0.1782 (0.1978)	
training:	Epoch: [17][119/233]	Loss 0.2216 (0.1980)	
training:	Epoch: [17][120/233]	Loss 0.2915 (0.1988)	
training:	Epoch: [17][121/233]	Loss 0.1734 (0.1986)	
training:	Epoch: [17][122/233]	Loss 0.1786 (0.1984)	
training:	Epoch: [17][123/233]	Loss 0.2701 (0.1990)	
training:	Epoch: [17][124/233]	Loss 0.1362 (0.1985)	
training:	Epoch: [17][125/233]	Loss 0.2058 (0.1986)	
training:	Epoch: [17][126/233]	Loss 0.1612 (0.1983)	
training:	Epoch: [17][127/233]	Loss 0.2673 (0.1988)	
training:	Epoch: [17][128/233]	Loss 0.2334 (0.1991)	
training:	Epoch: [17][129/233]	Loss 0.2034 (0.1991)	
training:	Epoch: [17][130/233]	Loss 0.2071 (0.1992)	
training:	Epoch: [17][131/233]	Loss 0.1513 (0.1988)	
training:	Epoch: [17][132/233]	Loss 0.2055 (0.1989)	
training:	Epoch: [17][133/233]	Loss 0.1766 (0.1987)	
training:	Epoch: [17][134/233]	Loss 0.1659 (0.1984)	
training:	Epoch: [17][135/233]	Loss 0.1893 (0.1984)	
training:	Epoch: [17][136/233]	Loss 0.2461 (0.1987)	
training:	Epoch: [17][137/233]	Loss 0.2052 (0.1988)	
training:	Epoch: [17][138/233]	Loss 0.2087 (0.1988)	
training:	Epoch: [17][139/233]	Loss 0.2383 (0.1991)	
training:	Epoch: [17][140/233]	Loss 0.2145 (0.1992)	
training:	Epoch: [17][141/233]	Loss 0.1737 (0.1991)	
training:	Epoch: [17][142/233]	Loss 0.1886 (0.1990)	
training:	Epoch: [17][143/233]	Loss 0.1801 (0.1989)	
training:	Epoch: [17][144/233]	Loss 0.1832 (0.1987)	
training:	Epoch: [17][145/233]	Loss 0.1940 (0.1987)	
training:	Epoch: [17][146/233]	Loss 0.3855 (0.2000)	
training:	Epoch: [17][147/233]	Loss 0.2157 (0.2001)	
training:	Epoch: [17][148/233]	Loss 0.2495 (0.2004)	
training:	Epoch: [17][149/233]	Loss 0.2499 (0.2008)	
training:	Epoch: [17][150/233]	Loss 0.2122 (0.2008)	
training:	Epoch: [17][151/233]	Loss 0.2021 (0.2008)	
training:	Epoch: [17][152/233]	Loss 0.2497 (0.2012)	
training:	Epoch: [17][153/233]	Loss 0.1751 (0.2010)	
training:	Epoch: [17][154/233]	Loss 0.1825 (0.2009)	
training:	Epoch: [17][155/233]	Loss 0.1785 (0.2007)	
training:	Epoch: [17][156/233]	Loss 0.1946 (0.2007)	
training:	Epoch: [17][157/233]	Loss 0.2617 (0.2011)	
training:	Epoch: [17][158/233]	Loss 0.2627 (0.2015)	
training:	Epoch: [17][159/233]	Loss 0.1529 (0.2012)	
training:	Epoch: [17][160/233]	Loss 0.2053 (0.2012)	
training:	Epoch: [17][161/233]	Loss 0.1922 (0.2011)	
training:	Epoch: [17][162/233]	Loss 0.1628 (0.2009)	
training:	Epoch: [17][163/233]	Loss 0.1881 (0.2008)	
training:	Epoch: [17][164/233]	Loss 0.2134 (0.2009)	
training:	Epoch: [17][165/233]	Loss 0.1764 (0.2007)	
training:	Epoch: [17][166/233]	Loss 0.1730 (0.2006)	
training:	Epoch: [17][167/233]	Loss 0.2223 (0.2007)	
training:	Epoch: [17][168/233]	Loss 0.1773 (0.2006)	
training:	Epoch: [17][169/233]	Loss 0.2651 (0.2010)	
training:	Epoch: [17][170/233]	Loss 0.2315 (0.2011)	
training:	Epoch: [17][171/233]	Loss 0.1520 (0.2008)	
training:	Epoch: [17][172/233]	Loss 0.2528 (0.2011)	
training:	Epoch: [17][173/233]	Loss 0.1856 (0.2011)	
training:	Epoch: [17][174/233]	Loss 0.2131 (0.2011)	
training:	Epoch: [17][175/233]	Loss 0.2228 (0.2013)	
training:	Epoch: [17][176/233]	Loss 0.2627 (0.2016)	
training:	Epoch: [17][177/233]	Loss 0.2948 (0.2021)	
training:	Epoch: [17][178/233]	Loss 0.2149 (0.2022)	
training:	Epoch: [17][179/233]	Loss 0.2006 (0.2022)	
training:	Epoch: [17][180/233]	Loss 0.2105 (0.2022)	
training:	Epoch: [17][181/233]	Loss 0.3091 (0.2028)	
training:	Epoch: [17][182/233]	Loss 0.1595 (0.2026)	
training:	Epoch: [17][183/233]	Loss 0.2000 (0.2026)	
training:	Epoch: [17][184/233]	Loss 0.1605 (0.2023)	
training:	Epoch: [17][185/233]	Loss 0.2570 (0.2026)	
training:	Epoch: [17][186/233]	Loss 0.2120 (0.2027)	
training:	Epoch: [17][187/233]	Loss 0.2475 (0.2029)	
training:	Epoch: [17][188/233]	Loss 0.2838 (0.2034)	
training:	Epoch: [17][189/233]	Loss 0.1881 (0.2033)	
training:	Epoch: [17][190/233]	Loss 0.1543 (0.2030)	
training:	Epoch: [17][191/233]	Loss 0.2347 (0.2032)	
training:	Epoch: [17][192/233]	Loss 0.1750 (0.2030)	
training:	Epoch: [17][193/233]	Loss 0.1667 (0.2029)	
training:	Epoch: [17][194/233]	Loss 0.1684 (0.2027)	
training:	Epoch: [17][195/233]	Loss 0.2436 (0.2029)	
training:	Epoch: [17][196/233]	Loss 0.2051 (0.2029)	
training:	Epoch: [17][197/233]	Loss 0.1783 (0.2028)	
training:	Epoch: [17][198/233]	Loss 0.1764 (0.2026)	
training:	Epoch: [17][199/233]	Loss 0.1328 (0.2023)	
training:	Epoch: [17][200/233]	Loss 0.1429 (0.2020)	
training:	Epoch: [17][201/233]	Loss 0.2233 (0.2021)	
training:	Epoch: [17][202/233]	Loss 0.1581 (0.2019)	
training:	Epoch: [17][203/233]	Loss 0.2322 (0.2020)	
training:	Epoch: [17][204/233]	Loss 0.1523 (0.2018)	
training:	Epoch: [17][205/233]	Loss 0.2342 (0.2019)	
training:	Epoch: [17][206/233]	Loss 0.1815 (0.2018)	
training:	Epoch: [17][207/233]	Loss 0.2527 (0.2021)	
training:	Epoch: [17][208/233]	Loss 0.2003 (0.2021)	
training:	Epoch: [17][209/233]	Loss 0.2428 (0.2023)	
training:	Epoch: [17][210/233]	Loss 0.1878 (0.2022)	
training:	Epoch: [17][211/233]	Loss 0.2031 (0.2022)	
training:	Epoch: [17][212/233]	Loss 0.2184 (0.2023)	
training:	Epoch: [17][213/233]	Loss 0.2105 (0.2023)	
training:	Epoch: [17][214/233]	Loss 0.2224 (0.2024)	
training:	Epoch: [17][215/233]	Loss 0.1937 (0.2024)	
training:	Epoch: [17][216/233]	Loss 0.1676 (0.2022)	
training:	Epoch: [17][217/233]	Loss 0.1870 (0.2021)	
training:	Epoch: [17][218/233]	Loss 0.1583 (0.2019)	
training:	Epoch: [17][219/233]	Loss 0.2995 (0.2024)	
training:	Epoch: [17][220/233]	Loss 0.1952 (0.2024)	
training:	Epoch: [17][221/233]	Loss 0.2360 (0.2025)	
training:	Epoch: [17][222/233]	Loss 0.2182 (0.2026)	
training:	Epoch: [17][223/233]	Loss 0.1856 (0.2025)	
training:	Epoch: [17][224/233]	Loss 0.2085 (0.2025)	
training:	Epoch: [17][225/233]	Loss 0.2466 (0.2027)	
training:	Epoch: [17][226/233]	Loss 0.2037 (0.2027)	
training:	Epoch: [17][227/233]	Loss 0.2199 (0.2028)	
training:	Epoch: [17][228/233]	Loss 0.2061 (0.2028)	
training:	Epoch: [17][229/233]	Loss 0.1522 (0.2026)	
training:	Epoch: [17][230/233]	Loss 0.2445 (0.2028)	
training:	Epoch: [17][231/233]	Loss 0.2056 (0.2028)	
training:	Epoch: [17][232/233]	Loss 0.1344 (0.2025)	
training:	Epoch: [17][233/233]	Loss 0.2316 (0.2026)	
Training:	 Loss: 0.2022

Training:	 ACC: 0.9786 0.9789 0.9854 0.9717
Validation:	 ACC: 0.8215 0.8234 0.8649 0.7780
Validation:	 Best_BACC: 0.8215 0.8234 0.8649 0.7780
Validation:	 Loss: 0.3899
Pretraining:	Epoch 18/200
----------
training:	Epoch: [18][1/233]	Loss 0.1815 (0.1815)	
training:	Epoch: [18][2/233]	Loss 0.1637 (0.1726)	
training:	Epoch: [18][3/233]	Loss 0.1660 (0.1704)	
training:	Epoch: [18][4/233]	Loss 0.1339 (0.1613)	
training:	Epoch: [18][5/233]	Loss 0.2024 (0.1695)	
training:	Epoch: [18][6/233]	Loss 0.1842 (0.1720)	
training:	Epoch: [18][7/233]	Loss 0.2224 (0.1792)	
training:	Epoch: [18][8/233]	Loss 0.1676 (0.1777)	
training:	Epoch: [18][9/233]	Loss 0.1850 (0.1785)	
training:	Epoch: [18][10/233]	Loss 0.2297 (0.1837)	
training:	Epoch: [18][11/233]	Loss 0.1878 (0.1840)	
training:	Epoch: [18][12/233]	Loss 0.2131 (0.1865)	
training:	Epoch: [18][13/233]	Loss 0.1709 (0.1853)	
training:	Epoch: [18][14/233]	Loss 0.2077 (0.1869)	
training:	Epoch: [18][15/233]	Loss 0.1750 (0.1861)	
training:	Epoch: [18][16/233]	Loss 0.1366 (0.1830)	
training:	Epoch: [18][17/233]	Loss 0.1542 (0.1813)	
training:	Epoch: [18][18/233]	Loss 0.1726 (0.1808)	
training:	Epoch: [18][19/233]	Loss 0.2026 (0.1819)	
training:	Epoch: [18][20/233]	Loss 0.1398 (0.1798)	
training:	Epoch: [18][21/233]	Loss 0.2209 (0.1818)	
training:	Epoch: [18][22/233]	Loss 0.1813 (0.1818)	
training:	Epoch: [18][23/233]	Loss 0.1803 (0.1817)	
training:	Epoch: [18][24/233]	Loss 0.1626 (0.1809)	
training:	Epoch: [18][25/233]	Loss 0.1662 (0.1803)	
training:	Epoch: [18][26/233]	Loss 0.1951 (0.1809)	
training:	Epoch: [18][27/233]	Loss 0.2849 (0.1847)	
training:	Epoch: [18][28/233]	Loss 0.1910 (0.1850)	
training:	Epoch: [18][29/233]	Loss 0.1358 (0.1833)	
training:	Epoch: [18][30/233]	Loss 0.1339 (0.1816)	
training:	Epoch: [18][31/233]	Loss 0.1572 (0.1808)	
training:	Epoch: [18][32/233]	Loss 0.1746 (0.1806)	
training:	Epoch: [18][33/233]	Loss 0.1589 (0.1800)	
training:	Epoch: [18][34/233]	Loss 0.1766 (0.1799)	
training:	Epoch: [18][35/233]	Loss 0.1940 (0.1803)	
training:	Epoch: [18][36/233]	Loss 0.1735 (0.1801)	
training:	Epoch: [18][37/233]	Loss 0.1848 (0.1802)	
training:	Epoch: [18][38/233]	Loss 0.2737 (0.1827)	
training:	Epoch: [18][39/233]	Loss 0.2203 (0.1836)	
training:	Epoch: [18][40/233]	Loss 0.1519 (0.1829)	
training:	Epoch: [18][41/233]	Loss 0.1496 (0.1820)	
training:	Epoch: [18][42/233]	Loss 0.2039 (0.1826)	
training:	Epoch: [18][43/233]	Loss 0.2545 (0.1842)	
training:	Epoch: [18][44/233]	Loss 0.2156 (0.1850)	
training:	Epoch: [18][45/233]	Loss 0.2294 (0.1859)	
training:	Epoch: [18][46/233]	Loss 0.1714 (0.1856)	
training:	Epoch: [18][47/233]	Loss 0.2069 (0.1861)	
training:	Epoch: [18][48/233]	Loss 0.1752 (0.1858)	
training:	Epoch: [18][49/233]	Loss 0.2683 (0.1875)	
training:	Epoch: [18][50/233]	Loss 0.1851 (0.1875)	
training:	Epoch: [18][51/233]	Loss 0.1599 (0.1869)	
training:	Epoch: [18][52/233]	Loss 0.1801 (0.1868)	
training:	Epoch: [18][53/233]	Loss 0.2149 (0.1873)	
training:	Epoch: [18][54/233]	Loss 0.2143 (0.1878)	
training:	Epoch: [18][55/233]	Loss 0.1232 (0.1867)	
training:	Epoch: [18][56/233]	Loss 0.1480 (0.1860)	
training:	Epoch: [18][57/233]	Loss 0.1654 (0.1856)	
training:	Epoch: [18][58/233]	Loss 0.1736 (0.1854)	
training:	Epoch: [18][59/233]	Loss 0.1323 (0.1845)	
training:	Epoch: [18][60/233]	Loss 0.2398 (0.1854)	
training:	Epoch: [18][61/233]	Loss 0.2052 (0.1858)	
training:	Epoch: [18][62/233]	Loss 0.2370 (0.1866)	
training:	Epoch: [18][63/233]	Loss 0.2067 (0.1869)	
training:	Epoch: [18][64/233]	Loss 0.1900 (0.1869)	
training:	Epoch: [18][65/233]	Loss 0.1943 (0.1871)	
training:	Epoch: [18][66/233]	Loss 0.1959 (0.1872)	
training:	Epoch: [18][67/233]	Loss 0.2313 (0.1879)	
training:	Epoch: [18][68/233]	Loss 0.1311 (0.1870)	
training:	Epoch: [18][69/233]	Loss 0.1508 (0.1865)	
training:	Epoch: [18][70/233]	Loss 0.1858 (0.1865)	
training:	Epoch: [18][71/233]	Loss 0.2005 (0.1867)	
training:	Epoch: [18][72/233]	Loss 0.2618 (0.1877)	
training:	Epoch: [18][73/233]	Loss 0.1365 (0.1870)	
training:	Epoch: [18][74/233]	Loss 0.1639 (0.1867)	
training:	Epoch: [18][75/233]	Loss 0.1456 (0.1862)	
training:	Epoch: [18][76/233]	Loss 0.2125 (0.1865)	
training:	Epoch: [18][77/233]	Loss 0.2854 (0.1878)	
training:	Epoch: [18][78/233]	Loss 0.1719 (0.1876)	
training:	Epoch: [18][79/233]	Loss 0.2292 (0.1881)	
training:	Epoch: [18][80/233]	Loss 0.1282 (0.1874)	
training:	Epoch: [18][81/233]	Loss 0.1659 (0.1871)	
training:	Epoch: [18][82/233]	Loss 0.2067 (0.1873)	
training:	Epoch: [18][83/233]	Loss 0.1421 (0.1868)	
training:	Epoch: [18][84/233]	Loss 0.2034 (0.1870)	
training:	Epoch: [18][85/233]	Loss 0.1941 (0.1871)	
training:	Epoch: [18][86/233]	Loss 0.1508 (0.1867)	
training:	Epoch: [18][87/233]	Loss 0.2582 (0.1875)	
training:	Epoch: [18][88/233]	Loss 0.1475 (0.1870)	
training:	Epoch: [18][89/233]	Loss 0.1693 (0.1868)	
training:	Epoch: [18][90/233]	Loss 0.1818 (0.1868)	
training:	Epoch: [18][91/233]	Loss 0.2504 (0.1875)	
training:	Epoch: [18][92/233]	Loss 0.1706 (0.1873)	
training:	Epoch: [18][93/233]	Loss 0.1542 (0.1869)	
training:	Epoch: [18][94/233]	Loss 0.1604 (0.1866)	
training:	Epoch: [18][95/233]	Loss 0.2351 (0.1872)	
training:	Epoch: [18][96/233]	Loss 0.1593 (0.1869)	
training:	Epoch: [18][97/233]	Loss 0.1984 (0.1870)	
training:	Epoch: [18][98/233]	Loss 0.1863 (0.1870)	
training:	Epoch: [18][99/233]	Loss 0.2683 (0.1878)	
training:	Epoch: [18][100/233]	Loss 0.1556 (0.1875)	
training:	Epoch: [18][101/233]	Loss 0.1971 (0.1876)	
training:	Epoch: [18][102/233]	Loss 0.1887 (0.1876)	
training:	Epoch: [18][103/233]	Loss 0.1701 (0.1874)	
training:	Epoch: [18][104/233]	Loss 0.1664 (0.1872)	
training:	Epoch: [18][105/233]	Loss 0.1642 (0.1870)	
training:	Epoch: [18][106/233]	Loss 0.2360 (0.1875)	
training:	Epoch: [18][107/233]	Loss 0.2232 (0.1878)	
training:	Epoch: [18][108/233]	Loss 0.2062 (0.1880)	
training:	Epoch: [18][109/233]	Loss 0.1442 (0.1876)	
training:	Epoch: [18][110/233]	Loss 0.2047 (0.1877)	
training:	Epoch: [18][111/233]	Loss 0.1936 (0.1878)	
training:	Epoch: [18][112/233]	Loss 0.2672 (0.1885)	
training:	Epoch: [18][113/233]	Loss 0.2334 (0.1889)	
training:	Epoch: [18][114/233]	Loss 0.2219 (0.1892)	
training:	Epoch: [18][115/233]	Loss 0.2754 (0.1899)	
training:	Epoch: [18][116/233]	Loss 0.2582 (0.1905)	
training:	Epoch: [18][117/233]	Loss 0.2272 (0.1908)	
training:	Epoch: [18][118/233]	Loss 0.1571 (0.1905)	
training:	Epoch: [18][119/233]	Loss 0.1829 (0.1905)	
training:	Epoch: [18][120/233]	Loss 0.2225 (0.1907)	
training:	Epoch: [18][121/233]	Loss 0.1823 (0.1907)	
training:	Epoch: [18][122/233]	Loss 0.2030 (0.1908)	
training:	Epoch: [18][123/233]	Loss 0.3502 (0.1921)	
training:	Epoch: [18][124/233]	Loss 0.2204 (0.1923)	
training:	Epoch: [18][125/233]	Loss 0.1582 (0.1920)	
training:	Epoch: [18][126/233]	Loss 0.1713 (0.1919)	
training:	Epoch: [18][127/233]	Loss 0.1642 (0.1916)	
training:	Epoch: [18][128/233]	Loss 0.1955 (0.1917)	
training:	Epoch: [18][129/233]	Loss 0.2225 (0.1919)	
training:	Epoch: [18][130/233]	Loss 0.1959 (0.1919)	
training:	Epoch: [18][131/233]	Loss 0.2455 (0.1923)	
training:	Epoch: [18][132/233]	Loss 0.1461 (0.1920)	
training:	Epoch: [18][133/233]	Loss 0.1439 (0.1916)	
training:	Epoch: [18][134/233]	Loss 0.1629 (0.1914)	
training:	Epoch: [18][135/233]	Loss 0.1768 (0.1913)	
training:	Epoch: [18][136/233]	Loss 0.2009 (0.1914)	
training:	Epoch: [18][137/233]	Loss 0.1866 (0.1913)	
training:	Epoch: [18][138/233]	Loss 0.1944 (0.1914)	
training:	Epoch: [18][139/233]	Loss 0.1829 (0.1913)	
training:	Epoch: [18][140/233]	Loss 0.2251 (0.1915)	
training:	Epoch: [18][141/233]	Loss 0.1928 (0.1916)	
training:	Epoch: [18][142/233]	Loss 0.1892 (0.1915)	
training:	Epoch: [18][143/233]	Loss 0.1181 (0.1910)	
training:	Epoch: [18][144/233]	Loss 0.2068 (0.1911)	
training:	Epoch: [18][145/233]	Loss 0.1779 (0.1910)	
training:	Epoch: [18][146/233]	Loss 0.1713 (0.1909)	
training:	Epoch: [18][147/233]	Loss 0.1619 (0.1907)	
training:	Epoch: [18][148/233]	Loss 0.1707 (0.1906)	
training:	Epoch: [18][149/233]	Loss 0.1708 (0.1904)	
training:	Epoch: [18][150/233]	Loss 0.1878 (0.1904)	
training:	Epoch: [18][151/233]	Loss 0.1481 (0.1901)	
training:	Epoch: [18][152/233]	Loss 0.1422 (0.1898)	
training:	Epoch: [18][153/233]	Loss 0.1718 (0.1897)	
training:	Epoch: [18][154/233]	Loss 0.1743 (0.1896)	
training:	Epoch: [18][155/233]	Loss 0.2309 (0.1899)	
training:	Epoch: [18][156/233]	Loss 0.1679 (0.1897)	
training:	Epoch: [18][157/233]	Loss 0.1984 (0.1898)	
training:	Epoch: [18][158/233]	Loss 0.1761 (0.1897)	
training:	Epoch: [18][159/233]	Loss 0.2266 (0.1899)	
training:	Epoch: [18][160/233]	Loss 0.2206 (0.1901)	
training:	Epoch: [18][161/233]	Loss 0.1431 (0.1898)	
training:	Epoch: [18][162/233]	Loss 0.1767 (0.1898)	
training:	Epoch: [18][163/233]	Loss 0.2071 (0.1899)	
training:	Epoch: [18][164/233]	Loss 0.1640 (0.1897)	
training:	Epoch: [18][165/233]	Loss 0.1683 (0.1896)	
training:	Epoch: [18][166/233]	Loss 0.2617 (0.1900)	
training:	Epoch: [18][167/233]	Loss 0.1239 (0.1896)	
training:	Epoch: [18][168/233]	Loss 0.2119 (0.1897)	
training:	Epoch: [18][169/233]	Loss 0.1976 (0.1898)	
training:	Epoch: [18][170/233]	Loss 0.2080 (0.1899)	
training:	Epoch: [18][171/233]	Loss 0.2385 (0.1902)	
training:	Epoch: [18][172/233]	Loss 0.1632 (0.1900)	
training:	Epoch: [18][173/233]	Loss 0.2369 (0.1903)	
training:	Epoch: [18][174/233]	Loss 0.2033 (0.1904)	
training:	Epoch: [18][175/233]	Loss 0.1362 (0.1901)	
training:	Epoch: [18][176/233]	Loss 0.2117 (0.1902)	
training:	Epoch: [18][177/233]	Loss 0.2512 (0.1905)	
training:	Epoch: [18][178/233]	Loss 0.1612 (0.1904)	
training:	Epoch: [18][179/233]	Loss 0.2044 (0.1904)	
training:	Epoch: [18][180/233]	Loss 0.2231 (0.1906)	
training:	Epoch: [18][181/233]	Loss 0.2013 (0.1907)	
training:	Epoch: [18][182/233]	Loss 0.1757 (0.1906)	
training:	Epoch: [18][183/233]	Loss 0.1630 (0.1905)	
training:	Epoch: [18][184/233]	Loss 0.1822 (0.1904)	
training:	Epoch: [18][185/233]	Loss 0.1737 (0.1903)	
training:	Epoch: [18][186/233]	Loss 0.2128 (0.1904)	
training:	Epoch: [18][187/233]	Loss 0.2149 (0.1906)	
training:	Epoch: [18][188/233]	Loss 0.1902 (0.1906)	
training:	Epoch: [18][189/233]	Loss 0.2169 (0.1907)	
training:	Epoch: [18][190/233]	Loss 0.2376 (0.1910)	
training:	Epoch: [18][191/233]	Loss 0.1845 (0.1909)	
training:	Epoch: [18][192/233]	Loss 0.2034 (0.1910)	
training:	Epoch: [18][193/233]	Loss 0.1627 (0.1908)	
training:	Epoch: [18][194/233]	Loss 0.1372 (0.1906)	
training:	Epoch: [18][195/233]	Loss 0.1867 (0.1905)	
training:	Epoch: [18][196/233]	Loss 0.1609 (0.1904)	
training:	Epoch: [18][197/233]	Loss 0.2165 (0.1905)	
training:	Epoch: [18][198/233]	Loss 0.1522 (0.1903)	
training:	Epoch: [18][199/233]	Loss 0.2572 (0.1907)	
training:	Epoch: [18][200/233]	Loss 0.2484 (0.1910)	
training:	Epoch: [18][201/233]	Loss 0.1479 (0.1907)	
training:	Epoch: [18][202/233]	Loss 0.2669 (0.1911)	
training:	Epoch: [18][203/233]	Loss 0.1729 (0.1910)	
training:	Epoch: [18][204/233]	Loss 0.1751 (0.1910)	
training:	Epoch: [18][205/233]	Loss 0.2133 (0.1911)	
training:	Epoch: [18][206/233]	Loss 0.1573 (0.1909)	
training:	Epoch: [18][207/233]	Loss 0.1742 (0.1908)	
training:	Epoch: [18][208/233]	Loss 0.1337 (0.1905)	
training:	Epoch: [18][209/233]	Loss 0.1623 (0.1904)	
training:	Epoch: [18][210/233]	Loss 0.1911 (0.1904)	
training:	Epoch: [18][211/233]	Loss 0.1872 (0.1904)	
training:	Epoch: [18][212/233]	Loss 0.1816 (0.1904)	
training:	Epoch: [18][213/233]	Loss 0.1688 (0.1903)	
training:	Epoch: [18][214/233]	Loss 0.2149 (0.1904)	
training:	Epoch: [18][215/233]	Loss 0.2669 (0.1907)	
training:	Epoch: [18][216/233]	Loss 0.1555 (0.1906)	
training:	Epoch: [18][217/233]	Loss 0.2161 (0.1907)	
training:	Epoch: [18][218/233]	Loss 0.1663 (0.1906)	
training:	Epoch: [18][219/233]	Loss 0.1629 (0.1904)	
training:	Epoch: [18][220/233]	Loss 0.2421 (0.1907)	
training:	Epoch: [18][221/233]	Loss 0.1619 (0.1905)	
training:	Epoch: [18][222/233]	Loss 0.1805 (0.1905)	
training:	Epoch: [18][223/233]	Loss 0.1788 (0.1904)	
training:	Epoch: [18][224/233]	Loss 0.2242 (0.1906)	
training:	Epoch: [18][225/233]	Loss 0.2110 (0.1907)	
training:	Epoch: [18][226/233]	Loss 0.1759 (0.1906)	
training:	Epoch: [18][227/233]	Loss 0.1535 (0.1905)	
training:	Epoch: [18][228/233]	Loss 0.1839 (0.1904)	
training:	Epoch: [18][229/233]	Loss 0.1919 (0.1904)	
training:	Epoch: [18][230/233]	Loss 0.1960 (0.1905)	
training:	Epoch: [18][231/233]	Loss 0.1740 (0.1904)	
training:	Epoch: [18][232/233]	Loss 0.2115 (0.1905)	
training:	Epoch: [18][233/233]	Loss 0.1870 (0.1905)	
Training:	 Loss: 0.1900

Training:	 ACC: 0.9825 0.9827 0.9887 0.9762
Validation:	 ACC: 0.8234 0.8256 0.8721 0.7747
Validation:	 Best_BACC: 0.8234 0.8256 0.8721 0.7747
Validation:	 Loss: 0.3873
Pretraining:	Epoch 19/200
----------
training:	Epoch: [19][1/233]	Loss 0.2514 (0.2514)	
training:	Epoch: [19][2/233]	Loss 0.1612 (0.2063)	
training:	Epoch: [19][3/233]	Loss 0.1998 (0.2042)	
training:	Epoch: [19][4/233]	Loss 0.1616 (0.1935)	
training:	Epoch: [19][5/233]	Loss 0.1991 (0.1946)	
training:	Epoch: [19][6/233]	Loss 0.1678 (0.1902)	
training:	Epoch: [19][7/233]	Loss 0.1900 (0.1901)	
training:	Epoch: [19][8/233]	Loss 0.1859 (0.1896)	
training:	Epoch: [19][9/233]	Loss 0.1825 (0.1888)	
training:	Epoch: [19][10/233]	Loss 0.1567 (0.1856)	
training:	Epoch: [19][11/233]	Loss 0.1416 (0.1816)	
training:	Epoch: [19][12/233]	Loss 0.1544 (0.1793)	
training:	Epoch: [19][13/233]	Loss 0.1811 (0.1795)	
training:	Epoch: [19][14/233]	Loss 0.1881 (0.1801)	
training:	Epoch: [19][15/233]	Loss 0.1597 (0.1787)	
training:	Epoch: [19][16/233]	Loss 0.1780 (0.1787)	
training:	Epoch: [19][17/233]	Loss 0.2354 (0.1820)	
training:	Epoch: [19][18/233]	Loss 0.2328 (0.1848)	
training:	Epoch: [19][19/233]	Loss 0.1639 (0.1837)	
training:	Epoch: [19][20/233]	Loss 0.1708 (0.1831)	
training:	Epoch: [19][21/233]	Loss 0.1413 (0.1811)	
training:	Epoch: [19][22/233]	Loss 0.1543 (0.1799)	
training:	Epoch: [19][23/233]	Loss 0.1511 (0.1786)	
training:	Epoch: [19][24/233]	Loss 0.2096 (0.1799)	
training:	Epoch: [19][25/233]	Loss 0.1531 (0.1789)	
training:	Epoch: [19][26/233]	Loss 0.1818 (0.1790)	
training:	Epoch: [19][27/233]	Loss 0.2578 (0.1819)	
training:	Epoch: [19][28/233]	Loss 0.1534 (0.1809)	
training:	Epoch: [19][29/233]	Loss 0.1480 (0.1797)	
training:	Epoch: [19][30/233]	Loss 0.1982 (0.1804)	
training:	Epoch: [19][31/233]	Loss 0.1956 (0.1808)	
training:	Epoch: [19][32/233]	Loss 0.1844 (0.1810)	
training:	Epoch: [19][33/233]	Loss 0.1728 (0.1807)	
training:	Epoch: [19][34/233]	Loss 0.1198 (0.1789)	
training:	Epoch: [19][35/233]	Loss 0.1916 (0.1793)	
training:	Epoch: [19][36/233]	Loss 0.1636 (0.1788)	
training:	Epoch: [19][37/233]	Loss 0.2343 (0.1803)	
training:	Epoch: [19][38/233]	Loss 0.2260 (0.1815)	
training:	Epoch: [19][39/233]	Loss 0.1779 (0.1815)	
training:	Epoch: [19][40/233]	Loss 0.1882 (0.1816)	
training:	Epoch: [19][41/233]	Loss 0.2620 (0.1836)	
training:	Epoch: [19][42/233]	Loss 0.2016 (0.1840)	
training:	Epoch: [19][43/233]	Loss 0.1563 (0.1834)	
training:	Epoch: [19][44/233]	Loss 0.2048 (0.1839)	
training:	Epoch: [19][45/233]	Loss 0.1514 (0.1831)	
training:	Epoch: [19][46/233]	Loss 0.1291 (0.1820)	
training:	Epoch: [19][47/233]	Loss 0.1722 (0.1817)	
training:	Epoch: [19][48/233]	Loss 0.2577 (0.1833)	
training:	Epoch: [19][49/233]	Loss 0.1785 (0.1832)	
training:	Epoch: [19][50/233]	Loss 0.1585 (0.1827)	
training:	Epoch: [19][51/233]	Loss 0.1746 (0.1826)	
training:	Epoch: [19][52/233]	Loss 0.1798 (0.1825)	
training:	Epoch: [19][53/233]	Loss 0.1615 (0.1821)	
training:	Epoch: [19][54/233]	Loss 0.2278 (0.1830)	
training:	Epoch: [19][55/233]	Loss 0.1849 (0.1830)	
training:	Epoch: [19][56/233]	Loss 0.1978 (0.1833)	
training:	Epoch: [19][57/233]	Loss 0.1567 (0.1828)	
training:	Epoch: [19][58/233]	Loss 0.1662 (0.1825)	
training:	Epoch: [19][59/233]	Loss 0.2412 (0.1835)	
training:	Epoch: [19][60/233]	Loss 0.1993 (0.1838)	
training:	Epoch: [19][61/233]	Loss 0.1643 (0.1835)	
training:	Epoch: [19][62/233]	Loss 0.1513 (0.1829)	
training:	Epoch: [19][63/233]	Loss 0.1417 (0.1823)	
training:	Epoch: [19][64/233]	Loss 0.1548 (0.1819)	
training:	Epoch: [19][65/233]	Loss 0.1366 (0.1812)	
training:	Epoch: [19][66/233]	Loss 0.1641 (0.1809)	
training:	Epoch: [19][67/233]	Loss 0.3053 (0.1828)	
training:	Epoch: [19][68/233]	Loss 0.1879 (0.1828)	
training:	Epoch: [19][69/233]	Loss 0.1659 (0.1826)	
training:	Epoch: [19][70/233]	Loss 0.1392 (0.1820)	
training:	Epoch: [19][71/233]	Loss 0.2674 (0.1832)	
training:	Epoch: [19][72/233]	Loss 0.2216 (0.1837)	
training:	Epoch: [19][73/233]	Loss 0.1804 (0.1837)	
training:	Epoch: [19][74/233]	Loss 0.1944 (0.1838)	
training:	Epoch: [19][75/233]	Loss 0.1527 (0.1834)	
training:	Epoch: [19][76/233]	Loss 0.2148 (0.1838)	
training:	Epoch: [19][77/233]	Loss 0.1408 (0.1832)	
training:	Epoch: [19][78/233]	Loss 0.1742 (0.1831)	
training:	Epoch: [19][79/233]	Loss 0.2276 (0.1837)	
training:	Epoch: [19][80/233]	Loss 0.1980 (0.1839)	
training:	Epoch: [19][81/233]	Loss 0.1527 (0.1835)	
training:	Epoch: [19][82/233]	Loss 0.1617 (0.1832)	
training:	Epoch: [19][83/233]	Loss 0.1295 (0.1826)	
training:	Epoch: [19][84/233]	Loss 0.2816 (0.1838)	
training:	Epoch: [19][85/233]	Loss 0.1658 (0.1835)	
training:	Epoch: [19][86/233]	Loss 0.2382 (0.1842)	
training:	Epoch: [19][87/233]	Loss 0.1806 (0.1841)	
training:	Epoch: [19][88/233]	Loss 0.1681 (0.1840)	
training:	Epoch: [19][89/233]	Loss 0.1742 (0.1838)	
training:	Epoch: [19][90/233]	Loss 0.1249 (0.1832)	
training:	Epoch: [19][91/233]	Loss 0.2294 (0.1837)	
training:	Epoch: [19][92/233]	Loss 0.1470 (0.1833)	
training:	Epoch: [19][93/233]	Loss 0.1829 (0.1833)	
training:	Epoch: [19][94/233]	Loss 0.1925 (0.1834)	
training:	Epoch: [19][95/233]	Loss 0.2439 (0.1840)	
training:	Epoch: [19][96/233]	Loss 0.2588 (0.1848)	
training:	Epoch: [19][97/233]	Loss 0.2647 (0.1856)	
training:	Epoch: [19][98/233]	Loss 0.2176 (0.1860)	
training:	Epoch: [19][99/233]	Loss 0.1989 (0.1861)	
training:	Epoch: [19][100/233]	Loss 0.2020 (0.1862)	
training:	Epoch: [19][101/233]	Loss 0.1854 (0.1862)	
training:	Epoch: [19][102/233]	Loss 0.2648 (0.1870)	
training:	Epoch: [19][103/233]	Loss 0.2123 (0.1873)	
training:	Epoch: [19][104/233]	Loss 0.1569 (0.1870)	
training:	Epoch: [19][105/233]	Loss 0.1478 (0.1866)	
training:	Epoch: [19][106/233]	Loss 0.2224 (0.1869)	
training:	Epoch: [19][107/233]	Loss 0.2222 (0.1873)	
training:	Epoch: [19][108/233]	Loss 0.1603 (0.1870)	
training:	Epoch: [19][109/233]	Loss 0.1575 (0.1867)	
training:	Epoch: [19][110/233]	Loss 0.1718 (0.1866)	
training:	Epoch: [19][111/233]	Loss 0.1543 (0.1863)	
training:	Epoch: [19][112/233]	Loss 0.1529 (0.1860)	
training:	Epoch: [19][113/233]	Loss 0.1636 (0.1858)	
training:	Epoch: [19][114/233]	Loss 0.1582 (0.1856)	
training:	Epoch: [19][115/233]	Loss 0.1568 (0.1853)	
training:	Epoch: [19][116/233]	Loss 0.1732 (0.1852)	
training:	Epoch: [19][117/233]	Loss 0.1822 (0.1852)	
training:	Epoch: [19][118/233]	Loss 0.1550 (0.1849)	
training:	Epoch: [19][119/233]	Loss 0.2679 (0.1856)	
training:	Epoch: [19][120/233]	Loss 0.1840 (0.1856)	
training:	Epoch: [19][121/233]	Loss 0.1508 (0.1853)	
training:	Epoch: [19][122/233]	Loss 0.2803 (0.1861)	
training:	Epoch: [19][123/233]	Loss 0.1249 (0.1856)	
training:	Epoch: [19][124/233]	Loss 0.1659 (0.1855)	
training:	Epoch: [19][125/233]	Loss 0.2724 (0.1861)	
training:	Epoch: [19][126/233]	Loss 0.2049 (0.1863)	
training:	Epoch: [19][127/233]	Loss 0.1535 (0.1860)	
training:	Epoch: [19][128/233]	Loss 0.2122 (0.1862)	
training:	Epoch: [19][129/233]	Loss 0.1932 (0.1863)	
training:	Epoch: [19][130/233]	Loss 0.2123 (0.1865)	
training:	Epoch: [19][131/233]	Loss 0.2061 (0.1866)	
training:	Epoch: [19][132/233]	Loss 0.1629 (0.1865)	
training:	Epoch: [19][133/233]	Loss 0.1929 (0.1865)	
training:	Epoch: [19][134/233]	Loss 0.1657 (0.1864)	
training:	Epoch: [19][135/233]	Loss 0.1377 (0.1860)	
training:	Epoch: [19][136/233]	Loss 0.1673 (0.1859)	
training:	Epoch: [19][137/233]	Loss 0.2449 (0.1863)	
training:	Epoch: [19][138/233]	Loss 0.1944 (0.1864)	
training:	Epoch: [19][139/233]	Loss 0.1371 (0.1860)	
training:	Epoch: [19][140/233]	Loss 0.1601 (0.1858)	
training:	Epoch: [19][141/233]	Loss 0.1964 (0.1859)	
training:	Epoch: [19][142/233]	Loss 0.2212 (0.1861)	
training:	Epoch: [19][143/233]	Loss 0.2151 (0.1863)	
training:	Epoch: [19][144/233]	Loss 0.1885 (0.1864)	
training:	Epoch: [19][145/233]	Loss 0.1655 (0.1862)	
training:	Epoch: [19][146/233]	Loss 0.1987 (0.1863)	
training:	Epoch: [19][147/233]	Loss 0.2278 (0.1866)	
training:	Epoch: [19][148/233]	Loss 0.1621 (0.1864)	
training:	Epoch: [19][149/233]	Loss 0.1766 (0.1863)	
training:	Epoch: [19][150/233]	Loss 0.1741 (0.1863)	
training:	Epoch: [19][151/233]	Loss 0.1622 (0.1861)	
training:	Epoch: [19][152/233]	Loss 0.2113 (0.1863)	
training:	Epoch: [19][153/233]	Loss 0.2272 (0.1865)	
training:	Epoch: [19][154/233]	Loss 0.1387 (0.1862)	
training:	Epoch: [19][155/233]	Loss 0.1435 (0.1860)	
training:	Epoch: [19][156/233]	Loss 0.1588 (0.1858)	
training:	Epoch: [19][157/233]	Loss 0.1468 (0.1855)	
training:	Epoch: [19][158/233]	Loss 0.1607 (0.1854)	
training:	Epoch: [19][159/233]	Loss 0.1639 (0.1852)	
training:	Epoch: [19][160/233]	Loss 0.1851 (0.1852)	
training:	Epoch: [19][161/233]	Loss 0.1790 (0.1852)	
training:	Epoch: [19][162/233]	Loss 0.1699 (0.1851)	
training:	Epoch: [19][163/233]	Loss 0.1347 (0.1848)	
training:	Epoch: [19][164/233]	Loss 0.2104 (0.1850)	
training:	Epoch: [19][165/233]	Loss 0.1895 (0.1850)	
training:	Epoch: [19][166/233]	Loss 0.1818 (0.1850)	
training:	Epoch: [19][167/233]	Loss 0.2555 (0.1854)	
training:	Epoch: [19][168/233]	Loss 0.1383 (0.1851)	
training:	Epoch: [19][169/233]	Loss 0.2182 (0.1853)	
training:	Epoch: [19][170/233]	Loss 0.1737 (0.1852)	
training:	Epoch: [19][171/233]	Loss 0.2009 (0.1853)	
training:	Epoch: [19][172/233]	Loss 0.1422 (0.1851)	
training:	Epoch: [19][173/233]	Loss 0.1473 (0.1849)	
training:	Epoch: [19][174/233]	Loss 0.1257 (0.1845)	
training:	Epoch: [19][175/233]	Loss 0.1545 (0.1843)	
training:	Epoch: [19][176/233]	Loss 0.2196 (0.1845)	
training:	Epoch: [19][177/233]	Loss 0.2131 (0.1847)	
training:	Epoch: [19][178/233]	Loss 0.2274 (0.1849)	
training:	Epoch: [19][179/233]	Loss 0.1407 (0.1847)	
training:	Epoch: [19][180/233]	Loss 0.2178 (0.1849)	
training:	Epoch: [19][181/233]	Loss 0.2082 (0.1850)	
training:	Epoch: [19][182/233]	Loss 0.2079 (0.1851)	
training:	Epoch: [19][183/233]	Loss 0.2075 (0.1853)	
training:	Epoch: [19][184/233]	Loss 0.2045 (0.1854)	
training:	Epoch: [19][185/233]	Loss 0.2295 (0.1856)	
training:	Epoch: [19][186/233]	Loss 0.1511 (0.1854)	
training:	Epoch: [19][187/233]	Loss 0.1431 (0.1852)	
training:	Epoch: [19][188/233]	Loss 0.1998 (0.1853)	
training:	Epoch: [19][189/233]	Loss 0.2107 (0.1854)	
training:	Epoch: [19][190/233]	Loss 0.1694 (0.1853)	
training:	Epoch: [19][191/233]	Loss 0.1510 (0.1851)	
training:	Epoch: [19][192/233]	Loss 0.1439 (0.1849)	
training:	Epoch: [19][193/233]	Loss 0.2288 (0.1851)	
training:	Epoch: [19][194/233]	Loss 0.1734 (0.1851)	
training:	Epoch: [19][195/233]	Loss 0.2302 (0.1853)	
training:	Epoch: [19][196/233]	Loss 0.1756 (0.1853)	
training:	Epoch: [19][197/233]	Loss 0.1726 (0.1852)	
training:	Epoch: [19][198/233]	Loss 0.1871 (0.1852)	
training:	Epoch: [19][199/233]	Loss 0.1540 (0.1851)	
training:	Epoch: [19][200/233]	Loss 0.1589 (0.1849)	
training:	Epoch: [19][201/233]	Loss 0.1640 (0.1848)	
training:	Epoch: [19][202/233]	Loss 0.2085 (0.1849)	
training:	Epoch: [19][203/233]	Loss 0.1918 (0.1850)	
training:	Epoch: [19][204/233]	Loss 0.1756 (0.1849)	
training:	Epoch: [19][205/233]	Loss 0.2322 (0.1852)	
training:	Epoch: [19][206/233]	Loss 0.1487 (0.1850)	
training:	Epoch: [19][207/233]	Loss 0.1932 (0.1850)	
training:	Epoch: [19][208/233]	Loss 0.2706 (0.1854)	
training:	Epoch: [19][209/233]	Loss 0.1654 (0.1853)	
training:	Epoch: [19][210/233]	Loss 0.1643 (0.1852)	
training:	Epoch: [19][211/233]	Loss 0.1942 (0.1853)	
training:	Epoch: [19][212/233]	Loss 0.1916 (0.1853)	
training:	Epoch: [19][213/233]	Loss 0.2180 (0.1855)	
training:	Epoch: [19][214/233]	Loss 0.1722 (0.1854)	
training:	Epoch: [19][215/233]	Loss 0.2076 (0.1855)	
training:	Epoch: [19][216/233]	Loss 0.2251 (0.1857)	
training:	Epoch: [19][217/233]	Loss 0.1800 (0.1857)	
training:	Epoch: [19][218/233]	Loss 0.1361 (0.1854)	
training:	Epoch: [19][219/233]	Loss 0.1859 (0.1854)	
training:	Epoch: [19][220/233]	Loss 0.1520 (0.1853)	
training:	Epoch: [19][221/233]	Loss 0.2214 (0.1854)	
training:	Epoch: [19][222/233]	Loss 0.1608 (0.1853)	
training:	Epoch: [19][223/233]	Loss 0.1353 (0.1851)	
training:	Epoch: [19][224/233]	Loss 0.1683 (0.1850)	
training:	Epoch: [19][225/233]	Loss 0.1620 (0.1849)	
training:	Epoch: [19][226/233]	Loss 0.1586 (0.1848)	
training:	Epoch: [19][227/233]	Loss 0.2040 (0.1849)	
training:	Epoch: [19][228/233]	Loss 0.1323 (0.1847)	
training:	Epoch: [19][229/233]	Loss 0.1754 (0.1846)	
training:	Epoch: [19][230/233]	Loss 0.2648 (0.1850)	
training:	Epoch: [19][231/233]	Loss 0.1921 (0.1850)	
training:	Epoch: [19][232/233]	Loss 0.1741 (0.1850)	
training:	Epoch: [19][233/233]	Loss 0.2373 (0.1852)	
Training:	 Loss: 0.1848

Training:	 ACC: 0.9869 0.9869 0.9874 0.9863
Validation:	 ACC: 0.8253 0.8266 0.8557 0.7948
Validation:	 Best_BACC: 0.8253 0.8266 0.8557 0.7948
Validation:	 Loss: 0.3827
Pretraining:	Epoch 20/200
----------
training:	Epoch: [20][1/233]	Loss 0.3919 (0.3919)	
training:	Epoch: [20][2/233]	Loss 0.1764 (0.2841)	
training:	Epoch: [20][3/233]	Loss 0.1916 (0.2533)	
training:	Epoch: [20][4/233]	Loss 0.1241 (0.2210)	
training:	Epoch: [20][5/233]	Loss 0.1200 (0.2008)	
training:	Epoch: [20][6/233]	Loss 0.1206 (0.1874)	
training:	Epoch: [20][7/233]	Loss 0.1732 (0.1854)	
training:	Epoch: [20][8/233]	Loss 0.1322 (0.1787)	
training:	Epoch: [20][9/233]	Loss 0.1274 (0.1730)	
training:	Epoch: [20][10/233]	Loss 0.1269 (0.1684)	
training:	Epoch: [20][11/233]	Loss 0.1823 (0.1697)	
training:	Epoch: [20][12/233]	Loss 0.1371 (0.1670)	
training:	Epoch: [20][13/233]	Loss 0.1249 (0.1637)	
training:	Epoch: [20][14/233]	Loss 0.2476 (0.1697)	
training:	Epoch: [20][15/233]	Loss 0.1499 (0.1684)	
training:	Epoch: [20][16/233]	Loss 0.1560 (0.1676)	
training:	Epoch: [20][17/233]	Loss 0.1800 (0.1683)	
training:	Epoch: [20][18/233]	Loss 0.1372 (0.1666)	
training:	Epoch: [20][19/233]	Loss 0.2073 (0.1688)	
training:	Epoch: [20][20/233]	Loss 0.1475 (0.1677)	
training:	Epoch: [20][21/233]	Loss 0.1811 (0.1683)	
training:	Epoch: [20][22/233]	Loss 0.2371 (0.1715)	
training:	Epoch: [20][23/233]	Loss 0.1950 (0.1725)	
training:	Epoch: [20][24/233]	Loss 0.1381 (0.1710)	
training:	Epoch: [20][25/233]	Loss 0.1574 (0.1705)	
training:	Epoch: [20][26/233]	Loss 0.1630 (0.1702)	
training:	Epoch: [20][27/233]	Loss 0.1340 (0.1689)	
training:	Epoch: [20][28/233]	Loss 0.1513 (0.1682)	
training:	Epoch: [20][29/233]	Loss 0.1908 (0.1690)	
training:	Epoch: [20][30/233]	Loss 0.2082 (0.1703)	
training:	Epoch: [20][31/233]	Loss 0.1175 (0.1686)	
training:	Epoch: [20][32/233]	Loss 0.1664 (0.1686)	
training:	Epoch: [20][33/233]	Loss 0.1707 (0.1686)	
training:	Epoch: [20][34/233]	Loss 0.2274 (0.1703)	
training:	Epoch: [20][35/233]	Loss 0.1364 (0.1694)	
training:	Epoch: [20][36/233]	Loss 0.1702 (0.1694)	
training:	Epoch: [20][37/233]	Loss 0.1353 (0.1685)	
training:	Epoch: [20][38/233]	Loss 0.1472 (0.1679)	
training:	Epoch: [20][39/233]	Loss 0.2123 (0.1691)	
training:	Epoch: [20][40/233]	Loss 0.1791 (0.1693)	
training:	Epoch: [20][41/233]	Loss 0.1917 (0.1699)	
training:	Epoch: [20][42/233]	Loss 0.1318 (0.1690)	
training:	Epoch: [20][43/233]	Loss 0.1522 (0.1686)	
training:	Epoch: [20][44/233]	Loss 0.1960 (0.1692)	
training:	Epoch: [20][45/233]	Loss 0.1768 (0.1694)	
training:	Epoch: [20][46/233]	Loss 0.1980 (0.1700)	
training:	Epoch: [20][47/233]	Loss 0.1388 (0.1693)	
training:	Epoch: [20][48/233]	Loss 0.1596 (0.1691)	
training:	Epoch: [20][49/233]	Loss 0.1290 (0.1683)	
training:	Epoch: [20][50/233]	Loss 0.1834 (0.1686)	
training:	Epoch: [20][51/233]	Loss 0.1766 (0.1688)	
training:	Epoch: [20][52/233]	Loss 0.1623 (0.1686)	
training:	Epoch: [20][53/233]	Loss 0.1719 (0.1687)	
training:	Epoch: [20][54/233]	Loss 0.1573 (0.1685)	
training:	Epoch: [20][55/233]	Loss 0.2066 (0.1692)	
training:	Epoch: [20][56/233]	Loss 0.1632 (0.1691)	
training:	Epoch: [20][57/233]	Loss 0.1424 (0.1686)	
training:	Epoch: [20][58/233]	Loss 0.2168 (0.1694)	
training:	Epoch: [20][59/233]	Loss 0.1313 (0.1688)	
training:	Epoch: [20][60/233]	Loss 0.2311 (0.1698)	
training:	Epoch: [20][61/233]	Loss 0.1396 (0.1693)	
training:	Epoch: [20][62/233]	Loss 0.1815 (0.1695)	
training:	Epoch: [20][63/233]	Loss 0.1411 (0.1691)	
training:	Epoch: [20][64/233]	Loss 0.1494 (0.1688)	
training:	Epoch: [20][65/233]	Loss 0.1730 (0.1688)	
training:	Epoch: [20][66/233]	Loss 0.1402 (0.1684)	
training:	Epoch: [20][67/233]	Loss 0.1581 (0.1682)	
training:	Epoch: [20][68/233]	Loss 0.1481 (0.1679)	
training:	Epoch: [20][69/233]	Loss 0.1375 (0.1675)	
training:	Epoch: [20][70/233]	Loss 0.1459 (0.1672)	
training:	Epoch: [20][71/233]	Loss 0.1850 (0.1674)	
training:	Epoch: [20][72/233]	Loss 0.1892 (0.1677)	
training:	Epoch: [20][73/233]	Loss 0.1784 (0.1679)	
training:	Epoch: [20][74/233]	Loss 0.1534 (0.1677)	
training:	Epoch: [20][75/233]	Loss 0.2239 (0.1684)	
training:	Epoch: [20][76/233]	Loss 0.1650 (0.1684)	
training:	Epoch: [20][77/233]	Loss 0.1208 (0.1678)	
training:	Epoch: [20][78/233]	Loss 0.2266 (0.1685)	
training:	Epoch: [20][79/233]	Loss 0.1679 (0.1685)	
training:	Epoch: [20][80/233]	Loss 0.1588 (0.1684)	
training:	Epoch: [20][81/233]	Loss 0.1471 (0.1681)	
training:	Epoch: [20][82/233]	Loss 0.2145 (0.1687)	
training:	Epoch: [20][83/233]	Loss 0.1516 (0.1685)	
training:	Epoch: [20][84/233]	Loss 0.1890 (0.1687)	
training:	Epoch: [20][85/233]	Loss 0.1397 (0.1684)	
training:	Epoch: [20][86/233]	Loss 0.1563 (0.1683)	
training:	Epoch: [20][87/233]	Loss 0.2273 (0.1689)	
training:	Epoch: [20][88/233]	Loss 0.1717 (0.1690)	
training:	Epoch: [20][89/233]	Loss 0.1756 (0.1691)	
training:	Epoch: [20][90/233]	Loss 0.1586 (0.1689)	
training:	Epoch: [20][91/233]	Loss 0.1501 (0.1687)	
training:	Epoch: [20][92/233]	Loss 0.2013 (0.1691)	
training:	Epoch: [20][93/233]	Loss 0.1916 (0.1693)	
training:	Epoch: [20][94/233]	Loss 0.1495 (0.1691)	
training:	Epoch: [20][95/233]	Loss 0.1668 (0.1691)	
training:	Epoch: [20][96/233]	Loss 0.1093 (0.1685)	
training:	Epoch: [20][97/233]	Loss 0.1585 (0.1684)	
training:	Epoch: [20][98/233]	Loss 0.1960 (0.1686)	
training:	Epoch: [20][99/233]	Loss 0.1332 (0.1683)	
training:	Epoch: [20][100/233]	Loss 0.1517 (0.1681)	
training:	Epoch: [20][101/233]	Loss 0.1583 (0.1680)	
training:	Epoch: [20][102/233]	Loss 0.1570 (0.1679)	
training:	Epoch: [20][103/233]	Loss 0.2395 (0.1686)	
training:	Epoch: [20][104/233]	Loss 0.1413 (0.1683)	
training:	Epoch: [20][105/233]	Loss 0.1354 (0.1680)	
training:	Epoch: [20][106/233]	Loss 0.2242 (0.1686)	
training:	Epoch: [20][107/233]	Loss 0.1431 (0.1683)	
training:	Epoch: [20][108/233]	Loss 0.1629 (0.1683)	
training:	Epoch: [20][109/233]	Loss 0.1953 (0.1685)	
training:	Epoch: [20][110/233]	Loss 0.1796 (0.1686)	
training:	Epoch: [20][111/233]	Loss 0.1406 (0.1684)	
training:	Epoch: [20][112/233]	Loss 0.1995 (0.1686)	
training:	Epoch: [20][113/233]	Loss 0.1699 (0.1687)	
training:	Epoch: [20][114/233]	Loss 0.2471 (0.1693)	
training:	Epoch: [20][115/233]	Loss 0.1571 (0.1692)	
training:	Epoch: [20][116/233]	Loss 0.1643 (0.1692)	
training:	Epoch: [20][117/233]	Loss 0.1731 (0.1692)	
training:	Epoch: [20][118/233]	Loss 0.1321 (0.1689)	
training:	Epoch: [20][119/233]	Loss 0.2038 (0.1692)	
training:	Epoch: [20][120/233]	Loss 0.1810 (0.1693)	
training:	Epoch: [20][121/233]	Loss 0.1528 (0.1692)	
training:	Epoch: [20][122/233]	Loss 0.1912 (0.1694)	
training:	Epoch: [20][123/233]	Loss 0.1765 (0.1694)	
training:	Epoch: [20][124/233]	Loss 0.2002 (0.1697)	
training:	Epoch: [20][125/233]	Loss 0.1647 (0.1696)	
training:	Epoch: [20][126/233]	Loss 0.2889 (0.1706)	
training:	Epoch: [20][127/233]	Loss 0.1576 (0.1705)	
training:	Epoch: [20][128/233]	Loss 0.2371 (0.1710)	
training:	Epoch: [20][129/233]	Loss 0.1714 (0.1710)	
training:	Epoch: [20][130/233]	Loss 0.1356 (0.1707)	
training:	Epoch: [20][131/233]	Loss 0.1898 (0.1709)	
training:	Epoch: [20][132/233]	Loss 0.2326 (0.1713)	
training:	Epoch: [20][133/233]	Loss 0.2034 (0.1716)	
training:	Epoch: [20][134/233]	Loss 0.1597 (0.1715)	
training:	Epoch: [20][135/233]	Loss 0.1948 (0.1717)	
training:	Epoch: [20][136/233]	Loss 0.1779 (0.1717)	
training:	Epoch: [20][137/233]	Loss 0.1936 (0.1719)	
training:	Epoch: [20][138/233]	Loss 0.1931 (0.1720)	
training:	Epoch: [20][139/233]	Loss 0.1985 (0.1722)	
training:	Epoch: [20][140/233]	Loss 0.1511 (0.1721)	
training:	Epoch: [20][141/233]	Loss 0.2038 (0.1723)	
training:	Epoch: [20][142/233]	Loss 0.1694 (0.1723)	
training:	Epoch: [20][143/233]	Loss 0.1945 (0.1724)	
training:	Epoch: [20][144/233]	Loss 0.1773 (0.1724)	
training:	Epoch: [20][145/233]	Loss 0.1643 (0.1724)	
training:	Epoch: [20][146/233]	Loss 0.2049 (0.1726)	
training:	Epoch: [20][147/233]	Loss 0.2197 (0.1729)	
training:	Epoch: [20][148/233]	Loss 0.1363 (0.1727)	
training:	Epoch: [20][149/233]	Loss 0.1275 (0.1724)	
training:	Epoch: [20][150/233]	Loss 0.2493 (0.1729)	
training:	Epoch: [20][151/233]	Loss 0.2189 (0.1732)	
training:	Epoch: [20][152/233]	Loss 0.1448 (0.1730)	
training:	Epoch: [20][153/233]	Loss 0.2627 (0.1736)	
training:	Epoch: [20][154/233]	Loss 0.1338 (0.1733)	
training:	Epoch: [20][155/233]	Loss 0.1477 (0.1732)	
training:	Epoch: [20][156/233]	Loss 0.2017 (0.1734)	
training:	Epoch: [20][157/233]	Loss 0.1872 (0.1734)	
training:	Epoch: [20][158/233]	Loss 0.1635 (0.1734)	
training:	Epoch: [20][159/233]	Loss 0.1647 (0.1733)	
training:	Epoch: [20][160/233]	Loss 0.1725 (0.1733)	
training:	Epoch: [20][161/233]	Loss 0.1416 (0.1731)	
training:	Epoch: [20][162/233]	Loss 0.2316 (0.1735)	
training:	Epoch: [20][163/233]	Loss 0.1633 (0.1734)	
training:	Epoch: [20][164/233]	Loss 0.1380 (0.1732)	
training:	Epoch: [20][165/233]	Loss 0.1409 (0.1730)	
training:	Epoch: [20][166/233]	Loss 0.1616 (0.1729)	
training:	Epoch: [20][167/233]	Loss 0.2161 (0.1732)	
training:	Epoch: [20][168/233]	Loss 0.2183 (0.1735)	
training:	Epoch: [20][169/233]	Loss 0.1692 (0.1734)	
training:	Epoch: [20][170/233]	Loss 0.1895 (0.1735)	
training:	Epoch: [20][171/233]	Loss 0.2108 (0.1738)	
training:	Epoch: [20][172/233]	Loss 0.2493 (0.1742)	
training:	Epoch: [20][173/233]	Loss 0.2090 (0.1744)	
training:	Epoch: [20][174/233]	Loss 0.2062 (0.1746)	
training:	Epoch: [20][175/233]	Loss 0.1492 (0.1744)	
training:	Epoch: [20][176/233]	Loss 0.2389 (0.1748)	
training:	Epoch: [20][177/233]	Loss 0.1479 (0.1747)	
training:	Epoch: [20][178/233]	Loss 0.1881 (0.1747)	
training:	Epoch: [20][179/233]	Loss 0.1544 (0.1746)	
training:	Epoch: [20][180/233]	Loss 0.2496 (0.1750)	
training:	Epoch: [20][181/233]	Loss 0.5699 (0.1772)	
training:	Epoch: [20][182/233]	Loss 0.1668 (0.1772)	
training:	Epoch: [20][183/233]	Loss 0.2011 (0.1773)	
training:	Epoch: [20][184/233]	Loss 0.1348 (0.1771)	
training:	Epoch: [20][185/233]	Loss 0.1539 (0.1769)	
training:	Epoch: [20][186/233]	Loss 0.1868 (0.1770)	
training:	Epoch: [20][187/233]	Loss 0.1858 (0.1770)	
training:	Epoch: [20][188/233]	Loss 0.1367 (0.1768)	
training:	Epoch: [20][189/233]	Loss 0.1558 (0.1767)	
training:	Epoch: [20][190/233]	Loss 0.1456 (0.1765)	
training:	Epoch: [20][191/233]	Loss 0.1480 (0.1764)	
training:	Epoch: [20][192/233]	Loss 0.1638 (0.1763)	
training:	Epoch: [20][193/233]	Loss 0.1611 (0.1762)	
training:	Epoch: [20][194/233]	Loss 0.1572 (0.1761)	
training:	Epoch: [20][195/233]	Loss 0.1461 (0.1760)	
training:	Epoch: [20][196/233]	Loss 0.1244 (0.1757)	
training:	Epoch: [20][197/233]	Loss 0.1698 (0.1757)	
training:	Epoch: [20][198/233]	Loss 0.1906 (0.1758)	
training:	Epoch: [20][199/233]	Loss 0.1866 (0.1758)	
training:	Epoch: [20][200/233]	Loss 0.1808 (0.1759)	
training:	Epoch: [20][201/233]	Loss 0.1997 (0.1760)	
training:	Epoch: [20][202/233]	Loss 0.2250 (0.1762)	
training:	Epoch: [20][203/233]	Loss 0.1798 (0.1762)	
training:	Epoch: [20][204/233]	Loss 0.1588 (0.1761)	
training:	Epoch: [20][205/233]	Loss 0.1722 (0.1761)	
training:	Epoch: [20][206/233]	Loss 0.1765 (0.1761)	
training:	Epoch: [20][207/233]	Loss 0.2163 (0.1763)	
training:	Epoch: [20][208/233]	Loss 0.1820 (0.1764)	
training:	Epoch: [20][209/233]	Loss 0.1644 (0.1763)	
training:	Epoch: [20][210/233]	Loss 0.2123 (0.1765)	
training:	Epoch: [20][211/233]	Loss 0.1819 (0.1765)	
training:	Epoch: [20][212/233]	Loss 0.1678 (0.1765)	
training:	Epoch: [20][213/233]	Loss 0.2127 (0.1766)	
training:	Epoch: [20][214/233]	Loss 0.1967 (0.1767)	
training:	Epoch: [20][215/233]	Loss 0.1181 (0.1764)	
training:	Epoch: [20][216/233]	Loss 0.1475 (0.1763)	
training:	Epoch: [20][217/233]	Loss 0.1938 (0.1764)	
training:	Epoch: [20][218/233]	Loss 0.1346 (0.1762)	
training:	Epoch: [20][219/233]	Loss 0.1823 (0.1762)	
training:	Epoch: [20][220/233]	Loss 0.2554 (0.1766)	
training:	Epoch: [20][221/233]	Loss 0.1644 (0.1765)	
training:	Epoch: [20][222/233]	Loss 0.1280 (0.1763)	
training:	Epoch: [20][223/233]	Loss 0.1650 (0.1763)	
training:	Epoch: [20][224/233]	Loss 0.1701 (0.1762)	
training:	Epoch: [20][225/233]	Loss 0.1536 (0.1761)	
training:	Epoch: [20][226/233]	Loss 0.1927 (0.1762)	
training:	Epoch: [20][227/233]	Loss 0.1221 (0.1760)	
training:	Epoch: [20][228/233]	Loss 0.1772 (0.1760)	
training:	Epoch: [20][229/233]	Loss 0.1830 (0.1760)	
training:	Epoch: [20][230/233]	Loss 0.2260 (0.1762)	
training:	Epoch: [20][231/233]	Loss 0.1606 (0.1762)	
training:	Epoch: [20][232/233]	Loss 0.1385 (0.1760)	
training:	Epoch: [20][233/233]	Loss 0.2252 (0.1762)	
Training:	 Loss: 0.1758

Training:	 ACC: 0.9900 0.9901 0.9923 0.9877
Validation:	 ACC: 0.8281 0.8299 0.8669 0.7892
Validation:	 Best_BACC: 0.8281 0.8299 0.8669 0.7892
Validation:	 Loss: 0.3828
Pretraining:	Epoch 21/200
----------
training:	Epoch: [21][1/233]	Loss 0.1243 (0.1243)	
training:	Epoch: [21][2/233]	Loss 0.1256 (0.1249)	
training:	Epoch: [21][3/233]	Loss 0.1469 (0.1323)	
training:	Epoch: [21][4/233]	Loss 0.1580 (0.1387)	
training:	Epoch: [21][5/233]	Loss 0.1689 (0.1447)	
training:	Epoch: [21][6/233]	Loss 0.1558 (0.1466)	
training:	Epoch: [21][7/233]	Loss 0.1606 (0.1486)	
training:	Epoch: [21][8/233]	Loss 0.2339 (0.1592)	
training:	Epoch: [21][9/233]	Loss 0.1545 (0.1587)	
training:	Epoch: [21][10/233]	Loss 0.2021 (0.1631)	
training:	Epoch: [21][11/233]	Loss 0.1768 (0.1643)	
training:	Epoch: [21][12/233]	Loss 0.1617 (0.1641)	
training:	Epoch: [21][13/233]	Loss 0.1115 (0.1600)	
training:	Epoch: [21][14/233]	Loss 0.1547 (0.1597)	
training:	Epoch: [21][15/233]	Loss 0.2043 (0.1626)	
training:	Epoch: [21][16/233]	Loss 0.2070 (0.1654)	
training:	Epoch: [21][17/233]	Loss 0.1608 (0.1651)	
training:	Epoch: [21][18/233]	Loss 0.1635 (0.1650)	
training:	Epoch: [21][19/233]	Loss 0.1846 (0.1661)	
training:	Epoch: [21][20/233]	Loss 0.3252 (0.1740)	
training:	Epoch: [21][21/233]	Loss 0.1800 (0.1743)	
training:	Epoch: [21][22/233]	Loss 0.1677 (0.1740)	
training:	Epoch: [21][23/233]	Loss 0.1965 (0.1750)	
training:	Epoch: [21][24/233]	Loss 0.1279 (0.1730)	
training:	Epoch: [21][25/233]	Loss 0.1727 (0.1730)	
training:	Epoch: [21][26/233]	Loss 0.1750 (0.1731)	
training:	Epoch: [21][27/233]	Loss 0.1346 (0.1717)	
training:	Epoch: [21][28/233]	Loss 0.1442 (0.1707)	
training:	Epoch: [21][29/233]	Loss 0.1351 (0.1695)	
training:	Epoch: [21][30/233]	Loss 0.1278 (0.1681)	
training:	Epoch: [21][31/233]	Loss 0.1644 (0.1679)	
training:	Epoch: [21][32/233]	Loss 0.1557 (0.1676)	
training:	Epoch: [21][33/233]	Loss 0.1923 (0.1683)	
training:	Epoch: [21][34/233]	Loss 0.2223 (0.1699)	
training:	Epoch: [21][35/233]	Loss 0.1655 (0.1698)	
training:	Epoch: [21][36/233]	Loss 0.1702 (0.1698)	
training:	Epoch: [21][37/233]	Loss 0.1209 (0.1685)	
training:	Epoch: [21][38/233]	Loss 0.1436 (0.1678)	
training:	Epoch: [21][39/233]	Loss 0.1664 (0.1678)	
training:	Epoch: [21][40/233]	Loss 0.1690 (0.1678)	
training:	Epoch: [21][41/233]	Loss 0.1913 (0.1684)	
training:	Epoch: [21][42/233]	Loss 0.1618 (0.1682)	
training:	Epoch: [21][43/233]	Loss 0.1089 (0.1668)	
training:	Epoch: [21][44/233]	Loss 0.1486 (0.1664)	
training:	Epoch: [21][45/233]	Loss 0.2037 (0.1673)	
training:	Epoch: [21][46/233]	Loss 0.1436 (0.1667)	
training:	Epoch: [21][47/233]	Loss 0.1274 (0.1659)	
training:	Epoch: [21][48/233]	Loss 0.1664 (0.1659)	
training:	Epoch: [21][49/233]	Loss 0.2196 (0.1670)	
training:	Epoch: [21][50/233]	Loss 0.1374 (0.1664)	
training:	Epoch: [21][51/233]	Loss 0.2117 (0.1673)	
training:	Epoch: [21][52/233]	Loss 0.1412 (0.1668)	
training:	Epoch: [21][53/233]	Loss 0.1801 (0.1671)	
training:	Epoch: [21][54/233]	Loss 0.1489 (0.1667)	
training:	Epoch: [21][55/233]	Loss 0.1485 (0.1664)	
training:	Epoch: [21][56/233]	Loss 0.1605 (0.1663)	
training:	Epoch: [21][57/233]	Loss 0.1356 (0.1657)	
training:	Epoch: [21][58/233]	Loss 0.1454 (0.1654)	
training:	Epoch: [21][59/233]	Loss 0.1679 (0.1654)	
training:	Epoch: [21][60/233]	Loss 0.1921 (0.1659)	
training:	Epoch: [21][61/233]	Loss 0.1807 (0.1661)	
training:	Epoch: [21][62/233]	Loss 0.1754 (0.1663)	
training:	Epoch: [21][63/233]	Loss 0.2567 (0.1677)	
training:	Epoch: [21][64/233]	Loss 0.1335 (0.1672)	
training:	Epoch: [21][65/233]	Loss 0.1558 (0.1670)	
training:	Epoch: [21][66/233]	Loss 0.1509 (0.1668)	
training:	Epoch: [21][67/233]	Loss 0.1956 (0.1672)	
training:	Epoch: [21][68/233]	Loss 0.1754 (0.1673)	
training:	Epoch: [21][69/233]	Loss 0.1927 (0.1677)	
training:	Epoch: [21][70/233]	Loss 0.1625 (0.1676)	
training:	Epoch: [21][71/233]	Loss 0.1647 (0.1676)	
training:	Epoch: [21][72/233]	Loss 0.1507 (0.1673)	
training:	Epoch: [21][73/233]	Loss 0.2406 (0.1683)	
training:	Epoch: [21][74/233]	Loss 0.1391 (0.1679)	
training:	Epoch: [21][75/233]	Loss 0.2507 (0.1690)	
training:	Epoch: [21][76/233]	Loss 0.1326 (0.1686)	
training:	Epoch: [21][77/233]	Loss 0.1792 (0.1687)	
training:	Epoch: [21][78/233]	Loss 0.1834 (0.1689)	
training:	Epoch: [21][79/233]	Loss 0.1756 (0.1690)	
training:	Epoch: [21][80/233]	Loss 0.2420 (0.1699)	
training:	Epoch: [21][81/233]	Loss 0.1405 (0.1695)	
training:	Epoch: [21][82/233]	Loss 0.1821 (0.1697)	
training:	Epoch: [21][83/233]	Loss 0.1732 (0.1697)	
training:	Epoch: [21][84/233]	Loss 0.1704 (0.1697)	
training:	Epoch: [21][85/233]	Loss 0.2132 (0.1702)	
training:	Epoch: [21][86/233]	Loss 0.1758 (0.1703)	
training:	Epoch: [21][87/233]	Loss 0.1335 (0.1699)	
training:	Epoch: [21][88/233]	Loss 0.1546 (0.1697)	
training:	Epoch: [21][89/233]	Loss 0.1445 (0.1694)	
training:	Epoch: [21][90/233]	Loss 0.1789 (0.1695)	
training:	Epoch: [21][91/233]	Loss 0.1610 (0.1694)	
training:	Epoch: [21][92/233]	Loss 0.1452 (0.1692)	
training:	Epoch: [21][93/233]	Loss 0.1726 (0.1692)	
training:	Epoch: [21][94/233]	Loss 0.1518 (0.1690)	
training:	Epoch: [21][95/233]	Loss 0.2668 (0.1701)	
training:	Epoch: [21][96/233]	Loss 0.1408 (0.1697)	
training:	Epoch: [21][97/233]	Loss 0.1882 (0.1699)	
training:	Epoch: [21][98/233]	Loss 0.1535 (0.1698)	
training:	Epoch: [21][99/233]	Loss 0.1860 (0.1699)	
training:	Epoch: [21][100/233]	Loss 0.1625 (0.1699)	
training:	Epoch: [21][101/233]	Loss 0.1681 (0.1698)	
training:	Epoch: [21][102/233]	Loss 0.1130 (0.1693)	
training:	Epoch: [21][103/233]	Loss 0.1932 (0.1695)	
training:	Epoch: [21][104/233]	Loss 0.1361 (0.1692)	
training:	Epoch: [21][105/233]	Loss 0.1601 (0.1691)	
training:	Epoch: [21][106/233]	Loss 0.2026 (0.1694)	
training:	Epoch: [21][107/233]	Loss 0.2019 (0.1697)	
training:	Epoch: [21][108/233]	Loss 0.1719 (0.1697)	
training:	Epoch: [21][109/233]	Loss 0.1592 (0.1697)	
training:	Epoch: [21][110/233]	Loss 0.1902 (0.1698)	
training:	Epoch: [21][111/233]	Loss 0.1088 (0.1693)	
training:	Epoch: [21][112/233]	Loss 0.2206 (0.1697)	
training:	Epoch: [21][113/233]	Loss 0.1545 (0.1696)	
training:	Epoch: [21][114/233]	Loss 0.1708 (0.1696)	
training:	Epoch: [21][115/233]	Loss 0.0981 (0.1690)	
training:	Epoch: [21][116/233]	Loss 0.1668 (0.1690)	
training:	Epoch: [21][117/233]	Loss 0.1504 (0.1688)	
training:	Epoch: [21][118/233]	Loss 0.1540 (0.1687)	
training:	Epoch: [21][119/233]	Loss 0.1674 (0.1687)	
training:	Epoch: [21][120/233]	Loss 0.1478 (0.1685)	
training:	Epoch: [21][121/233]	Loss 0.1564 (0.1684)	
training:	Epoch: [21][122/233]	Loss 0.1736 (0.1685)	
training:	Epoch: [21][123/233]	Loss 0.1128 (0.1680)	
training:	Epoch: [21][124/233]	Loss 0.1872 (0.1682)	
training:	Epoch: [21][125/233]	Loss 0.1224 (0.1678)	
training:	Epoch: [21][126/233]	Loss 0.1302 (0.1675)	
training:	Epoch: [21][127/233]	Loss 0.1371 (0.1673)	
training:	Epoch: [21][128/233]	Loss 0.1505 (0.1671)	
training:	Epoch: [21][129/233]	Loss 0.1174 (0.1667)	
training:	Epoch: [21][130/233]	Loss 0.1237 (0.1664)	
training:	Epoch: [21][131/233]	Loss 0.1673 (0.1664)	
training:	Epoch: [21][132/233]	Loss 0.1595 (0.1664)	
training:	Epoch: [21][133/233]	Loss 0.1097 (0.1659)	
training:	Epoch: [21][134/233]	Loss 0.1786 (0.1660)	
training:	Epoch: [21][135/233]	Loss 0.2440 (0.1666)	
training:	Epoch: [21][136/233]	Loss 0.1911 (0.1668)	
training:	Epoch: [21][137/233]	Loss 0.1754 (0.1668)	
training:	Epoch: [21][138/233]	Loss 0.1789 (0.1669)	
training:	Epoch: [21][139/233]	Loss 0.2325 (0.1674)	
training:	Epoch: [21][140/233]	Loss 0.1553 (0.1673)	
training:	Epoch: [21][141/233]	Loss 0.1464 (0.1672)	
training:	Epoch: [21][142/233]	Loss 0.1732 (0.1672)	
training:	Epoch: [21][143/233]	Loss 0.1335 (0.1670)	
training:	Epoch: [21][144/233]	Loss 0.1831 (0.1671)	
training:	Epoch: [21][145/233]	Loss 0.1516 (0.1670)	
training:	Epoch: [21][146/233]	Loss 0.1844 (0.1671)	
training:	Epoch: [21][147/233]	Loss 0.2807 (0.1679)	
training:	Epoch: [21][148/233]	Loss 0.1451 (0.1677)	
training:	Epoch: [21][149/233]	Loss 0.2015 (0.1679)	
training:	Epoch: [21][150/233]	Loss 0.2278 (0.1683)	
training:	Epoch: [21][151/233]	Loss 0.2095 (0.1686)	
training:	Epoch: [21][152/233]	Loss 0.1380 (0.1684)	
training:	Epoch: [21][153/233]	Loss 0.1647 (0.1684)	
training:	Epoch: [21][154/233]	Loss 0.1483 (0.1683)	
training:	Epoch: [21][155/233]	Loss 0.1692 (0.1683)	
training:	Epoch: [21][156/233]	Loss 0.1854 (0.1684)	
training:	Epoch: [21][157/233]	Loss 0.1955 (0.1686)	
training:	Epoch: [21][158/233]	Loss 0.2116 (0.1688)	
training:	Epoch: [21][159/233]	Loss 0.2241 (0.1692)	
training:	Epoch: [21][160/233]	Loss 0.2091 (0.1694)	
training:	Epoch: [21][161/233]	Loss 0.1685 (0.1694)	
training:	Epoch: [21][162/233]	Loss 0.1374 (0.1692)	
training:	Epoch: [21][163/233]	Loss 0.1454 (0.1691)	
training:	Epoch: [21][164/233]	Loss 0.1995 (0.1693)	
training:	Epoch: [21][165/233]	Loss 0.1705 (0.1693)	
training:	Epoch: [21][166/233]	Loss 0.1644 (0.1692)	
training:	Epoch: [21][167/233]	Loss 0.1483 (0.1691)	
training:	Epoch: [21][168/233]	Loss 0.1946 (0.1693)	
training:	Epoch: [21][169/233]	Loss 0.1650 (0.1692)	
training:	Epoch: [21][170/233]	Loss 0.1903 (0.1694)	
training:	Epoch: [21][171/233]	Loss 0.1591 (0.1693)	
training:	Epoch: [21][172/233]	Loss 0.1808 (0.1694)	
training:	Epoch: [21][173/233]	Loss 0.1474 (0.1692)	
training:	Epoch: [21][174/233]	Loss 0.1310 (0.1690)	
training:	Epoch: [21][175/233]	Loss 0.1644 (0.1690)	
training:	Epoch: [21][176/233]	Loss 0.1902 (0.1691)	
training:	Epoch: [21][177/233]	Loss 0.1983 (0.1693)	
training:	Epoch: [21][178/233]	Loss 0.1837 (0.1694)	
training:	Epoch: [21][179/233]	Loss 0.1391 (0.1692)	
training:	Epoch: [21][180/233]	Loss 0.1520 (0.1691)	
training:	Epoch: [21][181/233]	Loss 0.1567 (0.1690)	
training:	Epoch: [21][182/233]	Loss 0.1720 (0.1690)	
training:	Epoch: [21][183/233]	Loss 0.1324 (0.1688)	
training:	Epoch: [21][184/233]	Loss 0.1293 (0.1686)	
training:	Epoch: [21][185/233]	Loss 0.1274 (0.1684)	
training:	Epoch: [21][186/233]	Loss 0.1651 (0.1684)	
training:	Epoch: [21][187/233]	Loss 0.1507 (0.1683)	
training:	Epoch: [21][188/233]	Loss 0.2330 (0.1686)	
training:	Epoch: [21][189/233]	Loss 0.1611 (0.1686)	
training:	Epoch: [21][190/233]	Loss 0.2443 (0.1690)	
training:	Epoch: [21][191/233]	Loss 0.1423 (0.1689)	
training:	Epoch: [21][192/233]	Loss 0.1399 (0.1687)	
training:	Epoch: [21][193/233]	Loss 0.1321 (0.1685)	
training:	Epoch: [21][194/233]	Loss 0.1340 (0.1683)	
training:	Epoch: [21][195/233]	Loss 0.1690 (0.1683)	
training:	Epoch: [21][196/233]	Loss 0.1599 (0.1683)	
training:	Epoch: [21][197/233]	Loss 0.1919 (0.1684)	
training:	Epoch: [21][198/233]	Loss 0.1306 (0.1682)	
training:	Epoch: [21][199/233]	Loss 0.2117 (0.1684)	
training:	Epoch: [21][200/233]	Loss 0.1540 (0.1684)	
training:	Epoch: [21][201/233]	Loss 0.1454 (0.1683)	
training:	Epoch: [21][202/233]	Loss 0.1774 (0.1683)	
training:	Epoch: [21][203/233]	Loss 0.1532 (0.1682)	
training:	Epoch: [21][204/233]	Loss 0.1786 (0.1683)	
training:	Epoch: [21][205/233]	Loss 0.0992 (0.1679)	
training:	Epoch: [21][206/233]	Loss 0.1518 (0.1679)	
training:	Epoch: [21][207/233]	Loss 0.2486 (0.1683)	
training:	Epoch: [21][208/233]	Loss 0.2578 (0.1687)	
training:	Epoch: [21][209/233]	Loss 0.1689 (0.1687)	
training:	Epoch: [21][210/233]	Loss 0.1093 (0.1684)	
training:	Epoch: [21][211/233]	Loss 0.1670 (0.1684)	
training:	Epoch: [21][212/233]	Loss 0.1210 (0.1682)	
training:	Epoch: [21][213/233]	Loss 0.1666 (0.1682)	
training:	Epoch: [21][214/233]	Loss 0.1462 (0.1681)	
training:	Epoch: [21][215/233]	Loss 0.1787 (0.1681)	
training:	Epoch: [21][216/233]	Loss 0.2745 (0.1686)	
training:	Epoch: [21][217/233]	Loss 0.1577 (0.1686)	
training:	Epoch: [21][218/233]	Loss 0.1942 (0.1687)	
training:	Epoch: [21][219/233]	Loss 0.1845 (0.1687)	
training:	Epoch: [21][220/233]	Loss 0.1285 (0.1686)	
training:	Epoch: [21][221/233]	Loss 0.1535 (0.1685)	
training:	Epoch: [21][222/233]	Loss 0.2443 (0.1688)	
training:	Epoch: [21][223/233]	Loss 0.2673 (0.1693)	
training:	Epoch: [21][224/233]	Loss 0.1559 (0.1692)	
training:	Epoch: [21][225/233]	Loss 0.1747 (0.1692)	
training:	Epoch: [21][226/233]	Loss 0.1452 (0.1691)	
training:	Epoch: [21][227/233]	Loss 0.2345 (0.1694)	
training:	Epoch: [21][228/233]	Loss 0.2570 (0.1698)	
training:	Epoch: [21][229/233]	Loss 0.2048 (0.1700)	
training:	Epoch: [21][230/233]	Loss 0.1812 (0.1700)	
training:	Epoch: [21][231/233]	Loss 0.1681 (0.1700)	
training:	Epoch: [21][232/233]	Loss 0.1825 (0.1701)	
training:	Epoch: [21][233/233]	Loss 0.1892 (0.1701)	
Training:	 Loss: 0.1698

Training:	 ACC: 0.9890 0.9893 0.9956 0.9824
Validation:	 ACC: 0.8205 0.8234 0.8854 0.7556
Validation:	 Best_BACC: 0.8281 0.8299 0.8669 0.7892
Validation:	 Loss: 0.3889
Pretraining:	Epoch 22/200
----------
training:	Epoch: [22][1/233]	Loss 0.1849 (0.1849)	
training:	Epoch: [22][2/233]	Loss 0.1418 (0.1634)	
training:	Epoch: [22][3/233]	Loss 0.1687 (0.1651)	
training:	Epoch: [22][4/233]	Loss 0.1340 (0.1574)	
training:	Epoch: [22][5/233]	Loss 0.1599 (0.1579)	
training:	Epoch: [22][6/233]	Loss 0.1581 (0.1579)	
training:	Epoch: [22][7/233]	Loss 0.1278 (0.1536)	
training:	Epoch: [22][8/233]	Loss 0.1460 (0.1527)	
training:	Epoch: [22][9/233]	Loss 0.2223 (0.1604)	
training:	Epoch: [22][10/233]	Loss 0.1626 (0.1606)	
training:	Epoch: [22][11/233]	Loss 0.1274 (0.1576)	
training:	Epoch: [22][12/233]	Loss 0.1680 (0.1585)	
training:	Epoch: [22][13/233]	Loss 0.1366 (0.1568)	
training:	Epoch: [22][14/233]	Loss 0.1496 (0.1563)	
training:	Epoch: [22][15/233]	Loss 0.1301 (0.1545)	
training:	Epoch: [22][16/233]	Loss 0.1293 (0.1529)	
training:	Epoch: [22][17/233]	Loss 0.1765 (0.1543)	
training:	Epoch: [22][18/233]	Loss 0.1869 (0.1561)	
training:	Epoch: [22][19/233]	Loss 0.1405 (0.1553)	
training:	Epoch: [22][20/233]	Loss 0.1507 (0.1551)	
training:	Epoch: [22][21/233]	Loss 0.1850 (0.1565)	
training:	Epoch: [22][22/233]	Loss 0.1432 (0.1559)	
training:	Epoch: [22][23/233]	Loss 0.1204 (0.1544)	
training:	Epoch: [22][24/233]	Loss 0.1447 (0.1540)	
training:	Epoch: [22][25/233]	Loss 0.1677 (0.1545)	
training:	Epoch: [22][26/233]	Loss 0.1499 (0.1543)	
training:	Epoch: [22][27/233]	Loss 0.1498 (0.1542)	
training:	Epoch: [22][28/233]	Loss 0.1889 (0.1554)	
training:	Epoch: [22][29/233]	Loss 0.1366 (0.1548)	
training:	Epoch: [22][30/233]	Loss 0.1719 (0.1553)	
training:	Epoch: [22][31/233]	Loss 0.1128 (0.1540)	
training:	Epoch: [22][32/233]	Loss 0.1465 (0.1537)	
training:	Epoch: [22][33/233]	Loss 0.1223 (0.1528)	
training:	Epoch: [22][34/233]	Loss 0.1962 (0.1540)	
training:	Epoch: [22][35/233]	Loss 0.1690 (0.1545)	
training:	Epoch: [22][36/233]	Loss 0.1434 (0.1542)	
training:	Epoch: [22][37/233]	Loss 0.2371 (0.1564)	
training:	Epoch: [22][38/233]	Loss 0.1463 (0.1561)	
training:	Epoch: [22][39/233]	Loss 0.1737 (0.1566)	
training:	Epoch: [22][40/233]	Loss 0.1518 (0.1565)	
training:	Epoch: [22][41/233]	Loss 0.1605 (0.1566)	
training:	Epoch: [22][42/233]	Loss 0.1490 (0.1564)	
training:	Epoch: [22][43/233]	Loss 0.1234 (0.1556)	
training:	Epoch: [22][44/233]	Loss 0.2098 (0.1569)	
training:	Epoch: [22][45/233]	Loss 0.1688 (0.1571)	
training:	Epoch: [22][46/233]	Loss 0.1566 (0.1571)	
training:	Epoch: [22][47/233]	Loss 0.2029 (0.1581)	
training:	Epoch: [22][48/233]	Loss 0.1618 (0.1582)	
training:	Epoch: [22][49/233]	Loss 0.1708 (0.1584)	
training:	Epoch: [22][50/233]	Loss 0.1936 (0.1591)	
training:	Epoch: [22][51/233]	Loss 0.1723 (0.1594)	
training:	Epoch: [22][52/233]	Loss 0.1712 (0.1596)	
training:	Epoch: [22][53/233]	Loss 0.1235 (0.1589)	
training:	Epoch: [22][54/233]	Loss 0.1472 (0.1587)	
training:	Epoch: [22][55/233]	Loss 0.2218 (0.1599)	
training:	Epoch: [22][56/233]	Loss 0.2108 (0.1608)	
training:	Epoch: [22][57/233]	Loss 0.1306 (0.1602)	
training:	Epoch: [22][58/233]	Loss 0.1632 (0.1603)	
training:	Epoch: [22][59/233]	Loss 0.1449 (0.1600)	
training:	Epoch: [22][60/233]	Loss 0.1776 (0.1603)	
training:	Epoch: [22][61/233]	Loss 0.1156 (0.1596)	
training:	Epoch: [22][62/233]	Loss 0.1603 (0.1596)	
training:	Epoch: [22][63/233]	Loss 0.1427 (0.1593)	
training:	Epoch: [22][64/233]	Loss 0.1225 (0.1588)	
training:	Epoch: [22][65/233]	Loss 0.1178 (0.1581)	
training:	Epoch: [22][66/233]	Loss 0.1463 (0.1579)	
training:	Epoch: [22][67/233]	Loss 0.1243 (0.1574)	
training:	Epoch: [22][68/233]	Loss 0.2402 (0.1587)	
training:	Epoch: [22][69/233]	Loss 0.1323 (0.1583)	
training:	Epoch: [22][70/233]	Loss 0.1402 (0.1580)	
training:	Epoch: [22][71/233]	Loss 0.1990 (0.1586)	
training:	Epoch: [22][72/233]	Loss 0.1188 (0.1580)	
training:	Epoch: [22][73/233]	Loss 0.1426 (0.1578)	
training:	Epoch: [22][74/233]	Loss 0.1295 (0.1575)	
training:	Epoch: [22][75/233]	Loss 0.1237 (0.1570)	
training:	Epoch: [22][76/233]	Loss 0.1514 (0.1569)	
training:	Epoch: [22][77/233]	Loss 0.1287 (0.1566)	
training:	Epoch: [22][78/233]	Loss 0.1104 (0.1560)	
training:	Epoch: [22][79/233]	Loss 0.1626 (0.1561)	
training:	Epoch: [22][80/233]	Loss 0.1730 (0.1563)	
training:	Epoch: [22][81/233]	Loss 0.2051 (0.1569)	
training:	Epoch: [22][82/233]	Loss 0.1242 (0.1565)	
training:	Epoch: [22][83/233]	Loss 0.2256 (0.1573)	
training:	Epoch: [22][84/233]	Loss 0.2156 (0.1580)	
training:	Epoch: [22][85/233]	Loss 0.1617 (0.1580)	
training:	Epoch: [22][86/233]	Loss 0.1348 (0.1578)	
training:	Epoch: [22][87/233]	Loss 0.1243 (0.1574)	
training:	Epoch: [22][88/233]	Loss 0.1399 (0.1572)	
training:	Epoch: [22][89/233]	Loss 0.1223 (0.1568)	
training:	Epoch: [22][90/233]	Loss 0.1893 (0.1572)	
training:	Epoch: [22][91/233]	Loss 0.1275 (0.1568)	
training:	Epoch: [22][92/233]	Loss 0.1610 (0.1569)	
training:	Epoch: [22][93/233]	Loss 0.1311 (0.1566)	
training:	Epoch: [22][94/233]	Loss 0.1811 (0.1569)	
training:	Epoch: [22][95/233]	Loss 0.1927 (0.1572)	
training:	Epoch: [22][96/233]	Loss 0.1999 (0.1577)	
training:	Epoch: [22][97/233]	Loss 0.1173 (0.1573)	
training:	Epoch: [22][98/233]	Loss 0.1513 (0.1572)	
training:	Epoch: [22][99/233]	Loss 0.1882 (0.1575)	
training:	Epoch: [22][100/233]	Loss 0.1968 (0.1579)	
training:	Epoch: [22][101/233]	Loss 0.1476 (0.1578)	
training:	Epoch: [22][102/233]	Loss 0.1395 (0.1576)	
training:	Epoch: [22][103/233]	Loss 0.1293 (0.1573)	
training:	Epoch: [22][104/233]	Loss 0.1733 (0.1575)	
training:	Epoch: [22][105/233]	Loss 0.1613 (0.1575)	
training:	Epoch: [22][106/233]	Loss 0.1166 (0.1572)	
training:	Epoch: [22][107/233]	Loss 0.1848 (0.1574)	
training:	Epoch: [22][108/233]	Loss 0.1644 (0.1575)	
training:	Epoch: [22][109/233]	Loss 0.1540 (0.1574)	
training:	Epoch: [22][110/233]	Loss 0.1408 (0.1573)	
training:	Epoch: [22][111/233]	Loss 0.1606 (0.1573)	
training:	Epoch: [22][112/233]	Loss 0.1356 (0.1571)	
training:	Epoch: [22][113/233]	Loss 0.1220 (0.1568)	
training:	Epoch: [22][114/233]	Loss 0.1589 (0.1568)	
training:	Epoch: [22][115/233]	Loss 0.1864 (0.1571)	
training:	Epoch: [22][116/233]	Loss 0.1336 (0.1569)	
training:	Epoch: [22][117/233]	Loss 0.1683 (0.1570)	
training:	Epoch: [22][118/233]	Loss 0.2133 (0.1575)	
training:	Epoch: [22][119/233]	Loss 0.1885 (0.1577)	
training:	Epoch: [22][120/233]	Loss 0.1402 (0.1576)	
training:	Epoch: [22][121/233]	Loss 0.1573 (0.1576)	
training:	Epoch: [22][122/233]	Loss 0.1589 (0.1576)	
training:	Epoch: [22][123/233]	Loss 0.2191 (0.1581)	
training:	Epoch: [22][124/233]	Loss 0.1504 (0.1580)	
training:	Epoch: [22][125/233]	Loss 0.1492 (0.1580)	
training:	Epoch: [22][126/233]	Loss 0.1800 (0.1581)	
training:	Epoch: [22][127/233]	Loss 0.2651 (0.1590)	
training:	Epoch: [22][128/233]	Loss 0.1589 (0.1590)	
training:	Epoch: [22][129/233]	Loss 0.1183 (0.1587)	
training:	Epoch: [22][130/233]	Loss 0.1530 (0.1586)	
training:	Epoch: [22][131/233]	Loss 0.3313 (0.1599)	
training:	Epoch: [22][132/233]	Loss 0.1476 (0.1598)	
training:	Epoch: [22][133/233]	Loss 0.1680 (0.1599)	
training:	Epoch: [22][134/233]	Loss 0.1821 (0.1601)	
training:	Epoch: [22][135/233]	Loss 0.1947 (0.1603)	
training:	Epoch: [22][136/233]	Loss 0.1673 (0.1604)	
training:	Epoch: [22][137/233]	Loss 0.1764 (0.1605)	
training:	Epoch: [22][138/233]	Loss 0.1529 (0.1604)	
training:	Epoch: [22][139/233]	Loss 0.1599 (0.1604)	
training:	Epoch: [22][140/233]	Loss 0.1346 (0.1602)	
training:	Epoch: [22][141/233]	Loss 0.1230 (0.1600)	
training:	Epoch: [22][142/233]	Loss 0.1638 (0.1600)	
training:	Epoch: [22][143/233]	Loss 0.1836 (0.1602)	
training:	Epoch: [22][144/233]	Loss 0.1812 (0.1603)	
training:	Epoch: [22][145/233]	Loss 0.1033 (0.1599)	
training:	Epoch: [22][146/233]	Loss 0.2185 (0.1603)	
training:	Epoch: [22][147/233]	Loss 0.1470 (0.1602)	
training:	Epoch: [22][148/233]	Loss 0.1749 (0.1603)	
training:	Epoch: [22][149/233]	Loss 0.1535 (0.1603)	
training:	Epoch: [22][150/233]	Loss 0.2387 (0.1608)	
training:	Epoch: [22][151/233]	Loss 0.1344 (0.1606)	
training:	Epoch: [22][152/233]	Loss 0.1328 (0.1605)	
training:	Epoch: [22][153/233]	Loss 0.1344 (0.1603)	
training:	Epoch: [22][154/233]	Loss 0.1527 (0.1602)	
training:	Epoch: [22][155/233]	Loss 0.1575 (0.1602)	
training:	Epoch: [22][156/233]	Loss 0.1205 (0.1600)	
training:	Epoch: [22][157/233]	Loss 0.1885 (0.1601)	
training:	Epoch: [22][158/233]	Loss 0.1231 (0.1599)	
training:	Epoch: [22][159/233]	Loss 0.1759 (0.1600)	
training:	Epoch: [22][160/233]	Loss 0.1494 (0.1599)	
training:	Epoch: [22][161/233]	Loss 0.1461 (0.1599)	
training:	Epoch: [22][162/233]	Loss 0.0911 (0.1594)	
training:	Epoch: [22][163/233]	Loss 0.1555 (0.1594)	
training:	Epoch: [22][164/233]	Loss 0.2519 (0.1600)	
training:	Epoch: [22][165/233]	Loss 0.1918 (0.1602)	
training:	Epoch: [22][166/233]	Loss 0.1666 (0.1602)	
training:	Epoch: [22][167/233]	Loss 0.1911 (0.1604)	
training:	Epoch: [22][168/233]	Loss 0.1339 (0.1602)	
training:	Epoch: [22][169/233]	Loss 0.1394 (0.1601)	
training:	Epoch: [22][170/233]	Loss 0.1624 (0.1601)	
training:	Epoch: [22][171/233]	Loss 0.1389 (0.1600)	
training:	Epoch: [22][172/233]	Loss 0.1168 (0.1597)	
training:	Epoch: [22][173/233]	Loss 0.1263 (0.1596)	
training:	Epoch: [22][174/233]	Loss 0.1596 (0.1596)	
training:	Epoch: [22][175/233]	Loss 0.1251 (0.1594)	
training:	Epoch: [22][176/233]	Loss 0.2165 (0.1597)	
training:	Epoch: [22][177/233]	Loss 0.1482 (0.1596)	
training:	Epoch: [22][178/233]	Loss 0.1379 (0.1595)	
training:	Epoch: [22][179/233]	Loss 0.1676 (0.1595)	
training:	Epoch: [22][180/233]	Loss 0.1008 (0.1592)	
training:	Epoch: [22][181/233]	Loss 0.1661 (0.1593)	
training:	Epoch: [22][182/233]	Loss 0.1924 (0.1594)	
training:	Epoch: [22][183/233]	Loss 0.1607 (0.1594)	
training:	Epoch: [22][184/233]	Loss 0.2155 (0.1597)	
training:	Epoch: [22][185/233]	Loss 0.1416 (0.1597)	
training:	Epoch: [22][186/233]	Loss 0.1409 (0.1595)	
training:	Epoch: [22][187/233]	Loss 0.1922 (0.1597)	
training:	Epoch: [22][188/233]	Loss 0.1388 (0.1596)	
training:	Epoch: [22][189/233]	Loss 0.2548 (0.1601)	
training:	Epoch: [22][190/233]	Loss 0.1775 (0.1602)	
training:	Epoch: [22][191/233]	Loss 0.1431 (0.1601)	
training:	Epoch: [22][192/233]	Loss 0.1405 (0.1600)	
training:	Epoch: [22][193/233]	Loss 0.1945 (0.1602)	
training:	Epoch: [22][194/233]	Loss 0.1763 (0.1603)	
training:	Epoch: [22][195/233]	Loss 0.1241 (0.1601)	
training:	Epoch: [22][196/233]	Loss 0.1754 (0.1602)	
training:	Epoch: [22][197/233]	Loss 0.1470 (0.1601)	
training:	Epoch: [22][198/233]	Loss 0.1751 (0.1602)	
training:	Epoch: [22][199/233]	Loss 0.1553 (0.1602)	
training:	Epoch: [22][200/233]	Loss 0.1479 (0.1601)	
training:	Epoch: [22][201/233]	Loss 0.1633 (0.1601)	
training:	Epoch: [22][202/233]	Loss 0.1171 (0.1599)	
training:	Epoch: [22][203/233]	Loss 0.1069 (0.1596)	
training:	Epoch: [22][204/233]	Loss 0.1725 (0.1597)	
training:	Epoch: [22][205/233]	Loss 0.1547 (0.1597)	
training:	Epoch: [22][206/233]	Loss 0.1436 (0.1596)	
training:	Epoch: [22][207/233]	Loss 0.1461 (0.1595)	
training:	Epoch: [22][208/233]	Loss 0.1309 (0.1594)	
training:	Epoch: [22][209/233]	Loss 0.1677 (0.1594)	
training:	Epoch: [22][210/233]	Loss 0.1448 (0.1594)	
training:	Epoch: [22][211/233]	Loss 0.1224 (0.1592)	
training:	Epoch: [22][212/233]	Loss 0.1204 (0.1590)	
training:	Epoch: [22][213/233]	Loss 0.2095 (0.1592)	
training:	Epoch: [22][214/233]	Loss 0.1725 (0.1593)	
training:	Epoch: [22][215/233]	Loss 0.1766 (0.1594)	
training:	Epoch: [22][216/233]	Loss 0.1712 (0.1594)	
training:	Epoch: [22][217/233]	Loss 0.1512 (0.1594)	
training:	Epoch: [22][218/233]	Loss 0.1438 (0.1593)	
training:	Epoch: [22][219/233]	Loss 0.1446 (0.1593)	
training:	Epoch: [22][220/233]	Loss 0.2163 (0.1595)	
training:	Epoch: [22][221/233]	Loss 0.1555 (0.1595)	
training:	Epoch: [22][222/233]	Loss 0.1503 (0.1595)	
training:	Epoch: [22][223/233]	Loss 0.1401 (0.1594)	
training:	Epoch: [22][224/233]	Loss 0.1779 (0.1595)	
training:	Epoch: [22][225/233]	Loss 0.1772 (0.1595)	
training:	Epoch: [22][226/233]	Loss 0.1634 (0.1596)	
training:	Epoch: [22][227/233]	Loss 0.2198 (0.1598)	
training:	Epoch: [22][228/233]	Loss 0.1411 (0.1597)	
training:	Epoch: [22][229/233]	Loss 0.1715 (0.1598)	
training:	Epoch: [22][230/233]	Loss 0.2019 (0.1600)	
training:	Epoch: [22][231/233]	Loss 0.1322 (0.1599)	
training:	Epoch: [22][232/233]	Loss 0.1251 (0.1597)	
training:	Epoch: [22][233/233]	Loss 0.1178 (0.1595)	
Training:	 Loss: 0.1592

Training:	 ACC: 0.9942 0.9942 0.9944 0.9941
Validation:	 ACC: 0.8276 0.8288 0.8547 0.8004
Validation:	 Best_BACC: 0.8281 0.8299 0.8669 0.7892
Validation:	 Loss: 0.3771
Pretraining:	Epoch 23/200
----------
training:	Epoch: [23][1/233]	Loss 0.1496 (0.1496)	
training:	Epoch: [23][2/233]	Loss 0.1725 (0.1611)	
training:	Epoch: [23][3/233]	Loss 0.1624 (0.1615)	
training:	Epoch: [23][4/233]	Loss 0.1564 (0.1602)	
training:	Epoch: [23][5/233]	Loss 0.1688 (0.1619)	
training:	Epoch: [23][6/233]	Loss 0.1227 (0.1554)	
training:	Epoch: [23][7/233]	Loss 0.1726 (0.1579)	
training:	Epoch: [23][8/233]	Loss 0.1330 (0.1547)	
training:	Epoch: [23][9/233]	Loss 0.1435 (0.1535)	
training:	Epoch: [23][10/233]	Loss 0.1621 (0.1544)	
training:	Epoch: [23][11/233]	Loss 0.1373 (0.1528)	
training:	Epoch: [23][12/233]	Loss 0.2120 (0.1577)	
training:	Epoch: [23][13/233]	Loss 0.1492 (0.1571)	
training:	Epoch: [23][14/233]	Loss 0.1806 (0.1588)	
training:	Epoch: [23][15/233]	Loss 0.1284 (0.1567)	
training:	Epoch: [23][16/233]	Loss 0.1194 (0.1544)	
training:	Epoch: [23][17/233]	Loss 0.1793 (0.1559)	
training:	Epoch: [23][18/233]	Loss 0.1931 (0.1579)	
training:	Epoch: [23][19/233]	Loss 0.1289 (0.1564)	
training:	Epoch: [23][20/233]	Loss 0.1616 (0.1567)	
training:	Epoch: [23][21/233]	Loss 0.1587 (0.1568)	
training:	Epoch: [23][22/233]	Loss 0.1214 (0.1552)	
training:	Epoch: [23][23/233]	Loss 0.1184 (0.1536)	
training:	Epoch: [23][24/233]	Loss 0.1408 (0.1530)	
training:	Epoch: [23][25/233]	Loss 0.1345 (0.1523)	
training:	Epoch: [23][26/233]	Loss 0.1650 (0.1528)	
training:	Epoch: [23][27/233]	Loss 0.2018 (0.1546)	
training:	Epoch: [23][28/233]	Loss 0.1272 (0.1536)	
training:	Epoch: [23][29/233]	Loss 0.1900 (0.1549)	
training:	Epoch: [23][30/233]	Loss 0.1177 (0.1536)	
training:	Epoch: [23][31/233]	Loss 0.1654 (0.1540)	
training:	Epoch: [23][32/233]	Loss 0.1559 (0.1541)	
training:	Epoch: [23][33/233]	Loss 0.1686 (0.1545)	
training:	Epoch: [23][34/233]	Loss 0.1249 (0.1536)	
training:	Epoch: [23][35/233]	Loss 0.1247 (0.1528)	
training:	Epoch: [23][36/233]	Loss 0.1479 (0.1527)	
training:	Epoch: [23][37/233]	Loss 0.1191 (0.1518)	
training:	Epoch: [23][38/233]	Loss 0.1435 (0.1515)	
training:	Epoch: [23][39/233]	Loss 0.1431 (0.1513)	
training:	Epoch: [23][40/233]	Loss 0.1753 (0.1519)	
training:	Epoch: [23][41/233]	Loss 0.1490 (0.1519)	
training:	Epoch: [23][42/233]	Loss 0.1555 (0.1519)	
training:	Epoch: [23][43/233]	Loss 0.2186 (0.1535)	
training:	Epoch: [23][44/233]	Loss 0.1505 (0.1534)	
training:	Epoch: [23][45/233]	Loss 0.1221 (0.1527)	
training:	Epoch: [23][46/233]	Loss 0.1571 (0.1528)	
training:	Epoch: [23][47/233]	Loss 0.1314 (0.1524)	
training:	Epoch: [23][48/233]	Loss 0.1686 (0.1527)	
training:	Epoch: [23][49/233]	Loss 0.1995 (0.1537)	
training:	Epoch: [23][50/233]	Loss 0.1646 (0.1539)	
training:	Epoch: [23][51/233]	Loss 0.1667 (0.1541)	
training:	Epoch: [23][52/233]	Loss 0.1411 (0.1539)	
training:	Epoch: [23][53/233]	Loss 0.1977 (0.1547)	
training:	Epoch: [23][54/233]	Loss 0.1341 (0.1543)	
training:	Epoch: [23][55/233]	Loss 0.1365 (0.1540)	
training:	Epoch: [23][56/233]	Loss 0.1355 (0.1537)	
training:	Epoch: [23][57/233]	Loss 0.1327 (0.1533)	
training:	Epoch: [23][58/233]	Loss 0.1072 (0.1525)	
training:	Epoch: [23][59/233]	Loss 0.1592 (0.1526)	
training:	Epoch: [23][60/233]	Loss 0.1408 (0.1524)	
training:	Epoch: [23][61/233]	Loss 0.1482 (0.1524)	
training:	Epoch: [23][62/233]	Loss 0.1124 (0.1517)	
training:	Epoch: [23][63/233]	Loss 0.1344 (0.1514)	
training:	Epoch: [23][64/233]	Loss 0.1370 (0.1512)	
training:	Epoch: [23][65/233]	Loss 0.1464 (0.1511)	
training:	Epoch: [23][66/233]	Loss 0.1347 (0.1509)	
training:	Epoch: [23][67/233]	Loss 0.1304 (0.1506)	
training:	Epoch: [23][68/233]	Loss 0.2001 (0.1513)	
training:	Epoch: [23][69/233]	Loss 0.1853 (0.1518)	
training:	Epoch: [23][70/233]	Loss 0.1585 (0.1519)	
training:	Epoch: [23][71/233]	Loss 0.1394 (0.1517)	
training:	Epoch: [23][72/233]	Loss 0.2073 (0.1525)	
training:	Epoch: [23][73/233]	Loss 0.1457 (0.1524)	
training:	Epoch: [23][74/233]	Loss 0.1319 (0.1521)	
training:	Epoch: [23][75/233]	Loss 0.1483 (0.1521)	
training:	Epoch: [23][76/233]	Loss 0.1361 (0.1519)	
training:	Epoch: [23][77/233]	Loss 0.1790 (0.1522)	
training:	Epoch: [23][78/233]	Loss 0.1387 (0.1520)	
training:	Epoch: [23][79/233]	Loss 0.0972 (0.1513)	
training:	Epoch: [23][80/233]	Loss 0.1567 (0.1514)	
training:	Epoch: [23][81/233]	Loss 0.1515 (0.1514)	
training:	Epoch: [23][82/233]	Loss 0.1344 (0.1512)	
training:	Epoch: [23][83/233]	Loss 0.1181 (0.1508)	
training:	Epoch: [23][84/233]	Loss 0.1616 (0.1509)	
training:	Epoch: [23][85/233]	Loss 0.1500 (0.1509)	
training:	Epoch: [23][86/233]	Loss 0.1295 (0.1507)	
training:	Epoch: [23][87/233]	Loss 0.1487 (0.1507)	
training:	Epoch: [23][88/233]	Loss 0.1705 (0.1509)	
training:	Epoch: [23][89/233]	Loss 0.1706 (0.1511)	
training:	Epoch: [23][90/233]	Loss 0.1643 (0.1512)	
training:	Epoch: [23][91/233]	Loss 0.2782 (0.1526)	
training:	Epoch: [23][92/233]	Loss 0.1336 (0.1524)	
training:	Epoch: [23][93/233]	Loss 0.1226 (0.1521)	
training:	Epoch: [23][94/233]	Loss 0.1640 (0.1522)	
training:	Epoch: [23][95/233]	Loss 0.1130 (0.1518)	
training:	Epoch: [23][96/233]	Loss 0.1199 (0.1515)	
training:	Epoch: [23][97/233]	Loss 0.1598 (0.1516)	
training:	Epoch: [23][98/233]	Loss 0.1349 (0.1514)	
training:	Epoch: [23][99/233]	Loss 0.1175 (0.1511)	
training:	Epoch: [23][100/233]	Loss 0.2357 (0.1519)	
training:	Epoch: [23][101/233]	Loss 0.1729 (0.1521)	
training:	Epoch: [23][102/233]	Loss 0.0932 (0.1515)	
training:	Epoch: [23][103/233]	Loss 0.1289 (0.1513)	
training:	Epoch: [23][104/233]	Loss 0.1679 (0.1515)	
training:	Epoch: [23][105/233]	Loss 0.1749 (0.1517)	
training:	Epoch: [23][106/233]	Loss 0.2049 (0.1522)	
training:	Epoch: [23][107/233]	Loss 0.1594 (0.1523)	
training:	Epoch: [23][108/233]	Loss 0.1525 (0.1523)	
training:	Epoch: [23][109/233]	Loss 0.1562 (0.1523)	
training:	Epoch: [23][110/233]	Loss 0.1568 (0.1524)	
training:	Epoch: [23][111/233]	Loss 0.1729 (0.1525)	
training:	Epoch: [23][112/233]	Loss 0.1657 (0.1527)	
training:	Epoch: [23][113/233]	Loss 0.1754 (0.1529)	
training:	Epoch: [23][114/233]	Loss 0.1829 (0.1531)	
training:	Epoch: [23][115/233]	Loss 0.2023 (0.1536)	
training:	Epoch: [23][116/233]	Loss 0.1677 (0.1537)	
training:	Epoch: [23][117/233]	Loss 0.1523 (0.1537)	
training:	Epoch: [23][118/233]	Loss 0.1811 (0.1539)	
training:	Epoch: [23][119/233]	Loss 0.1208 (0.1536)	
training:	Epoch: [23][120/233]	Loss 0.1572 (0.1536)	
training:	Epoch: [23][121/233]	Loss 0.1412 (0.1535)	
training:	Epoch: [23][122/233]	Loss 0.1320 (0.1534)	
training:	Epoch: [23][123/233]	Loss 0.1408 (0.1533)	
training:	Epoch: [23][124/233]	Loss 0.1313 (0.1531)	
training:	Epoch: [23][125/233]	Loss 0.1818 (0.1533)	
training:	Epoch: [23][126/233]	Loss 0.1383 (0.1532)	
training:	Epoch: [23][127/233]	Loss 0.1470 (0.1532)	
training:	Epoch: [23][128/233]	Loss 0.1666 (0.1533)	
training:	Epoch: [23][129/233]	Loss 0.1227 (0.1530)	
training:	Epoch: [23][130/233]	Loss 0.1161 (0.1527)	
training:	Epoch: [23][131/233]	Loss 0.1625 (0.1528)	
training:	Epoch: [23][132/233]	Loss 0.1459 (0.1528)	
training:	Epoch: [23][133/233]	Loss 0.1538 (0.1528)	
training:	Epoch: [23][134/233]	Loss 0.1792 (0.1530)	
training:	Epoch: [23][135/233]	Loss 0.1488 (0.1529)	
training:	Epoch: [23][136/233]	Loss 0.1636 (0.1530)	
training:	Epoch: [23][137/233]	Loss 0.2594 (0.1538)	
training:	Epoch: [23][138/233]	Loss 0.1436 (0.1537)	
training:	Epoch: [23][139/233]	Loss 0.1201 (0.1535)	
training:	Epoch: [23][140/233]	Loss 0.1469 (0.1534)	
training:	Epoch: [23][141/233]	Loss 0.1456 (0.1534)	
training:	Epoch: [23][142/233]	Loss 0.1296 (0.1532)	
training:	Epoch: [23][143/233]	Loss 0.2300 (0.1537)	
training:	Epoch: [23][144/233]	Loss 0.2420 (0.1544)	
training:	Epoch: [23][145/233]	Loss 0.1775 (0.1545)	
training:	Epoch: [23][146/233]	Loss 0.1331 (0.1544)	
training:	Epoch: [23][147/233]	Loss 0.1532 (0.1544)	
training:	Epoch: [23][148/233]	Loss 0.1363 (0.1542)	
training:	Epoch: [23][149/233]	Loss 0.1551 (0.1542)	
training:	Epoch: [23][150/233]	Loss 0.1211 (0.1540)	
training:	Epoch: [23][151/233]	Loss 0.1232 (0.1538)	
training:	Epoch: [23][152/233]	Loss 0.1585 (0.1538)	
training:	Epoch: [23][153/233]	Loss 0.0987 (0.1535)	
training:	Epoch: [23][154/233]	Loss 0.1088 (0.1532)	
training:	Epoch: [23][155/233]	Loss 0.1324 (0.1531)	
training:	Epoch: [23][156/233]	Loss 0.1673 (0.1532)	
training:	Epoch: [23][157/233]	Loss 0.1728 (0.1533)	
training:	Epoch: [23][158/233]	Loss 0.1590 (0.1533)	
training:	Epoch: [23][159/233]	Loss 0.1527 (0.1533)	
training:	Epoch: [23][160/233]	Loss 0.1271 (0.1531)	
training:	Epoch: [23][161/233]	Loss 0.1544 (0.1532)	
training:	Epoch: [23][162/233]	Loss 0.1875 (0.1534)	
training:	Epoch: [23][163/233]	Loss 0.1604 (0.1534)	
training:	Epoch: [23][164/233]	Loss 0.1845 (0.1536)	
training:	Epoch: [23][165/233]	Loss 0.1278 (0.1534)	
training:	Epoch: [23][166/233]	Loss 0.1526 (0.1534)	
training:	Epoch: [23][167/233]	Loss 0.1481 (0.1534)	
training:	Epoch: [23][168/233]	Loss 0.1540 (0.1534)	
training:	Epoch: [23][169/233]	Loss 0.1834 (0.1536)	
training:	Epoch: [23][170/233]	Loss 0.1043 (0.1533)	
training:	Epoch: [23][171/233]	Loss 0.1457 (0.1533)	
training:	Epoch: [23][172/233]	Loss 0.1502 (0.1532)	
training:	Epoch: [23][173/233]	Loss 0.1435 (0.1532)	
training:	Epoch: [23][174/233]	Loss 0.1248 (0.1530)	
training:	Epoch: [23][175/233]	Loss 0.1718 (0.1531)	
training:	Epoch: [23][176/233]	Loss 0.1595 (0.1532)	
training:	Epoch: [23][177/233]	Loss 0.1694 (0.1533)	
training:	Epoch: [23][178/233]	Loss 0.1546 (0.1533)	
training:	Epoch: [23][179/233]	Loss 0.1958 (0.1535)	
training:	Epoch: [23][180/233]	Loss 0.1519 (0.1535)	
training:	Epoch: [23][181/233]	Loss 0.1751 (0.1536)	
training:	Epoch: [23][182/233]	Loss 0.1057 (0.1533)	
training:	Epoch: [23][183/233]	Loss 0.1272 (0.1532)	
training:	Epoch: [23][184/233]	Loss 0.1749 (0.1533)	
training:	Epoch: [23][185/233]	Loss 0.1290 (0.1532)	
training:	Epoch: [23][186/233]	Loss 0.2480 (0.1537)	
training:	Epoch: [23][187/233]	Loss 0.1615 (0.1537)	
training:	Epoch: [23][188/233]	Loss 0.1290 (0.1536)	
training:	Epoch: [23][189/233]	Loss 0.1807 (0.1538)	
training:	Epoch: [23][190/233]	Loss 0.1296 (0.1536)	
training:	Epoch: [23][191/233]	Loss 0.1357 (0.1535)	
training:	Epoch: [23][192/233]	Loss 0.1952 (0.1537)	
training:	Epoch: [23][193/233]	Loss 0.1282 (0.1536)	
training:	Epoch: [23][194/233]	Loss 0.2103 (0.1539)	
training:	Epoch: [23][195/233]	Loss 0.1183 (0.1537)	
training:	Epoch: [23][196/233]	Loss 0.1501 (0.1537)	
training:	Epoch: [23][197/233]	Loss 0.1646 (0.1538)	
training:	Epoch: [23][198/233]	Loss 0.1521 (0.1538)	
training:	Epoch: [23][199/233]	Loss 0.1414 (0.1537)	
training:	Epoch: [23][200/233]	Loss 0.1364 (0.1536)	
training:	Epoch: [23][201/233]	Loss 0.1171 (0.1534)	
training:	Epoch: [23][202/233]	Loss 0.1570 (0.1534)	
training:	Epoch: [23][203/233]	Loss 0.1101 (0.1532)	
training:	Epoch: [23][204/233]	Loss 0.1440 (0.1532)	
training:	Epoch: [23][205/233]	Loss 0.1607 (0.1532)	
training:	Epoch: [23][206/233]	Loss 0.1676 (0.1533)	
training:	Epoch: [23][207/233]	Loss 0.1351 (0.1532)	
training:	Epoch: [23][208/233]	Loss 0.1846 (0.1534)	
training:	Epoch: [23][209/233]	Loss 0.1419 (0.1533)	
training:	Epoch: [23][210/233]	Loss 0.1560 (0.1533)	
training:	Epoch: [23][211/233]	Loss 0.1531 (0.1533)	
training:	Epoch: [23][212/233]	Loss 0.1296 (0.1532)	
training:	Epoch: [23][213/233]	Loss 0.1643 (0.1532)	
training:	Epoch: [23][214/233]	Loss 0.1737 (0.1533)	
training:	Epoch: [23][215/233]	Loss 0.1108 (0.1531)	
training:	Epoch: [23][216/233]	Loss 0.1488 (0.1531)	
training:	Epoch: [23][217/233]	Loss 0.2488 (0.1536)	
training:	Epoch: [23][218/233]	Loss 0.1326 (0.1535)	
training:	Epoch: [23][219/233]	Loss 0.1805 (0.1536)	
training:	Epoch: [23][220/233]	Loss 0.1513 (0.1536)	
training:	Epoch: [23][221/233]	Loss 0.1827 (0.1537)	
training:	Epoch: [23][222/233]	Loss 0.1642 (0.1538)	
training:	Epoch: [23][223/233]	Loss 0.2420 (0.1542)	
training:	Epoch: [23][224/233]	Loss 0.1522 (0.1542)	
training:	Epoch: [23][225/233]	Loss 0.1917 (0.1543)	
training:	Epoch: [23][226/233]	Loss 0.1624 (0.1544)	
training:	Epoch: [23][227/233]	Loss 0.2009 (0.1546)	
training:	Epoch: [23][228/233]	Loss 0.2347 (0.1549)	
training:	Epoch: [23][229/233]	Loss 0.1445 (0.1549)	
training:	Epoch: [23][230/233]	Loss 0.1327 (0.1548)	
training:	Epoch: [23][231/233]	Loss 0.1962 (0.1549)	
training:	Epoch: [23][232/233]	Loss 0.1237 (0.1548)	
training:	Epoch: [23][233/233]	Loss 0.1376 (0.1547)	
Training:	 Loss: 0.1544

Training:	 ACC: 0.9951 0.9952 0.9969 0.9933
Validation:	 ACC: 0.8284 0.8304 0.8721 0.7848
Validation:	 Best_BACC: 0.8284 0.8304 0.8721 0.7848
Validation:	 Loss: 0.3800
Pretraining:	Epoch 24/200
----------
training:	Epoch: [24][1/233]	Loss 0.1988 (0.1988)	
training:	Epoch: [24][2/233]	Loss 0.1649 (0.1819)	
training:	Epoch: [24][3/233]	Loss 0.1236 (0.1625)	
training:	Epoch: [24][4/233]	Loss 0.1360 (0.1559)	
training:	Epoch: [24][5/233]	Loss 0.1143 (0.1475)	
training:	Epoch: [24][6/233]	Loss 0.1365 (0.1457)	
training:	Epoch: [24][7/233]	Loss 0.1476 (0.1460)	
training:	Epoch: [24][8/233]	Loss 0.1130 (0.1419)	
training:	Epoch: [24][9/233]	Loss 0.1151 (0.1389)	
training:	Epoch: [24][10/233]	Loss 0.1676 (0.1418)	
training:	Epoch: [24][11/233]	Loss 0.1475 (0.1423)	
training:	Epoch: [24][12/233]	Loss 0.1319 (0.1414)	
training:	Epoch: [24][13/233]	Loss 0.2140 (0.1470)	
training:	Epoch: [24][14/233]	Loss 0.1235 (0.1453)	
training:	Epoch: [24][15/233]	Loss 0.1618 (0.1464)	
training:	Epoch: [24][16/233]	Loss 0.1543 (0.1469)	
training:	Epoch: [24][17/233]	Loss 0.1214 (0.1454)	
training:	Epoch: [24][18/233]	Loss 0.1623 (0.1464)	
training:	Epoch: [24][19/233]	Loss 0.1522 (0.1467)	
training:	Epoch: [24][20/233]	Loss 0.1447 (0.1466)	
training:	Epoch: [24][21/233]	Loss 0.1097 (0.1448)	
training:	Epoch: [24][22/233]	Loss 0.1310 (0.1442)	
training:	Epoch: [24][23/233]	Loss 0.1514 (0.1445)	
training:	Epoch: [24][24/233]	Loss 0.1320 (0.1440)	
training:	Epoch: [24][25/233]	Loss 0.1673 (0.1449)	
training:	Epoch: [24][26/233]	Loss 0.1486 (0.1450)	
training:	Epoch: [24][27/233]	Loss 0.1271 (0.1444)	
training:	Epoch: [24][28/233]	Loss 0.1822 (0.1457)	
training:	Epoch: [24][29/233]	Loss 0.1388 (0.1455)	
training:	Epoch: [24][30/233]	Loss 0.1500 (0.1456)	
training:	Epoch: [24][31/233]	Loss 0.1275 (0.1451)	
training:	Epoch: [24][32/233]	Loss 0.1011 (0.1437)	
training:	Epoch: [24][33/233]	Loss 0.2126 (0.1458)	
training:	Epoch: [24][34/233]	Loss 0.1375 (0.1455)	
training:	Epoch: [24][35/233]	Loss 0.1213 (0.1448)	
training:	Epoch: [24][36/233]	Loss 0.1210 (0.1442)	
training:	Epoch: [24][37/233]	Loss 0.1210 (0.1436)	
training:	Epoch: [24][38/233]	Loss 0.1278 (0.1431)	
training:	Epoch: [24][39/233]	Loss 0.1187 (0.1425)	
training:	Epoch: [24][40/233]	Loss 0.1773 (0.1434)	
training:	Epoch: [24][41/233]	Loss 0.1616 (0.1438)	
training:	Epoch: [24][42/233]	Loss 0.1672 (0.1444)	
training:	Epoch: [24][43/233]	Loss 0.1140 (0.1437)	
training:	Epoch: [24][44/233]	Loss 0.1197 (0.1431)	
training:	Epoch: [24][45/233]	Loss 0.1228 (0.1427)	
training:	Epoch: [24][46/233]	Loss 0.1062 (0.1419)	
training:	Epoch: [24][47/233]	Loss 0.1598 (0.1423)	
training:	Epoch: [24][48/233]	Loss 0.1464 (0.1424)	
training:	Epoch: [24][49/233]	Loss 0.1688 (0.1429)	
training:	Epoch: [24][50/233]	Loss 0.1197 (0.1424)	
training:	Epoch: [24][51/233]	Loss 0.1197 (0.1420)	
training:	Epoch: [24][52/233]	Loss 0.1178 (0.1415)	
training:	Epoch: [24][53/233]	Loss 0.1692 (0.1420)	
training:	Epoch: [24][54/233]	Loss 0.1529 (0.1422)	
training:	Epoch: [24][55/233]	Loss 0.1572 (0.1425)	
training:	Epoch: [24][56/233]	Loss 0.1680 (0.1430)	
training:	Epoch: [24][57/233]	Loss 0.1807 (0.1436)	
training:	Epoch: [24][58/233]	Loss 0.1266 (0.1433)	
training:	Epoch: [24][59/233]	Loss 0.1777 (0.1439)	
training:	Epoch: [24][60/233]	Loss 0.1394 (0.1438)	
training:	Epoch: [24][61/233]	Loss 0.1929 (0.1447)	
training:	Epoch: [24][62/233]	Loss 0.2065 (0.1457)	
training:	Epoch: [24][63/233]	Loss 0.1083 (0.1451)	
training:	Epoch: [24][64/233]	Loss 0.1315 (0.1448)	
training:	Epoch: [24][65/233]	Loss 0.1257 (0.1446)	
training:	Epoch: [24][66/233]	Loss 0.0956 (0.1438)	
training:	Epoch: [24][67/233]	Loss 0.1363 (0.1437)	
training:	Epoch: [24][68/233]	Loss 0.1597 (0.1439)	
training:	Epoch: [24][69/233]	Loss 0.1312 (0.1437)	
training:	Epoch: [24][70/233]	Loss 0.1182 (0.1434)	
training:	Epoch: [24][71/233]	Loss 0.2656 (0.1451)	
training:	Epoch: [24][72/233]	Loss 0.1341 (0.1450)	
training:	Epoch: [24][73/233]	Loss 0.1406 (0.1449)	
training:	Epoch: [24][74/233]	Loss 0.1210 (0.1446)	
training:	Epoch: [24][75/233]	Loss 0.1280 (0.1444)	
training:	Epoch: [24][76/233]	Loss 0.1450 (0.1444)	
training:	Epoch: [24][77/233]	Loss 0.1247 (0.1441)	
training:	Epoch: [24][78/233]	Loss 0.1146 (0.1437)	
training:	Epoch: [24][79/233]	Loss 0.1291 (0.1435)	
training:	Epoch: [24][80/233]	Loss 0.1603 (0.1437)	
training:	Epoch: [24][81/233]	Loss 0.1275 (0.1435)	
training:	Epoch: [24][82/233]	Loss 0.1313 (0.1434)	
training:	Epoch: [24][83/233]	Loss 0.1473 (0.1434)	
training:	Epoch: [24][84/233]	Loss 0.1136 (0.1431)	
training:	Epoch: [24][85/233]	Loss 0.1902 (0.1436)	
training:	Epoch: [24][86/233]	Loss 0.1266 (0.1434)	
training:	Epoch: [24][87/233]	Loss 0.1416 (0.1434)	
training:	Epoch: [24][88/233]	Loss 0.1498 (0.1435)	
training:	Epoch: [24][89/233]	Loss 0.1365 (0.1434)	
training:	Epoch: [24][90/233]	Loss 0.1590 (0.1436)	
training:	Epoch: [24][91/233]	Loss 0.2679 (0.1450)	
training:	Epoch: [24][92/233]	Loss 0.1726 (0.1453)	
training:	Epoch: [24][93/233]	Loss 0.1648 (0.1455)	
training:	Epoch: [24][94/233]	Loss 0.1327 (0.1453)	
training:	Epoch: [24][95/233]	Loss 0.2151 (0.1461)	
training:	Epoch: [24][96/233]	Loss 0.1485 (0.1461)	
training:	Epoch: [24][97/233]	Loss 0.1668 (0.1463)	
training:	Epoch: [24][98/233]	Loss 0.1533 (0.1464)	
training:	Epoch: [24][99/233]	Loss 0.1017 (0.1459)	
training:	Epoch: [24][100/233]	Loss 0.1599 (0.1461)	
training:	Epoch: [24][101/233]	Loss 0.1353 (0.1460)	
training:	Epoch: [24][102/233]	Loss 0.1188 (0.1457)	
training:	Epoch: [24][103/233]	Loss 0.1677 (0.1459)	
training:	Epoch: [24][104/233]	Loss 0.1217 (0.1457)	
training:	Epoch: [24][105/233]	Loss 0.1093 (0.1453)	
training:	Epoch: [24][106/233]	Loss 0.1593 (0.1455)	
training:	Epoch: [24][107/233]	Loss 0.1398 (0.1454)	
training:	Epoch: [24][108/233]	Loss 0.1455 (0.1454)	
training:	Epoch: [24][109/233]	Loss 0.1684 (0.1456)	
training:	Epoch: [24][110/233]	Loss 0.1711 (0.1459)	
training:	Epoch: [24][111/233]	Loss 0.1416 (0.1458)	
training:	Epoch: [24][112/233]	Loss 0.1418 (0.1458)	
training:	Epoch: [24][113/233]	Loss 0.2412 (0.1466)	
training:	Epoch: [24][114/233]	Loss 0.1431 (0.1466)	
training:	Epoch: [24][115/233]	Loss 0.2129 (0.1472)	
training:	Epoch: [24][116/233]	Loss 0.1094 (0.1468)	
training:	Epoch: [24][117/233]	Loss 0.1174 (0.1466)	
training:	Epoch: [24][118/233]	Loss 0.1422 (0.1466)	
training:	Epoch: [24][119/233]	Loss 0.1976 (0.1470)	
training:	Epoch: [24][120/233]	Loss 0.1304 (0.1468)	
training:	Epoch: [24][121/233]	Loss 0.1927 (0.1472)	
training:	Epoch: [24][122/233]	Loss 0.1192 (0.1470)	
training:	Epoch: [24][123/233]	Loss 0.1178 (0.1468)	
training:	Epoch: [24][124/233]	Loss 0.1264 (0.1466)	
training:	Epoch: [24][125/233]	Loss 0.1278 (0.1464)	
training:	Epoch: [24][126/233]	Loss 0.1413 (0.1464)	
training:	Epoch: [24][127/233]	Loss 0.1363 (0.1463)	
training:	Epoch: [24][128/233]	Loss 0.1628 (0.1464)	
training:	Epoch: [24][129/233]	Loss 0.1501 (0.1465)	
training:	Epoch: [24][130/233]	Loss 0.1401 (0.1464)	
training:	Epoch: [24][131/233]	Loss 0.1430 (0.1464)	
training:	Epoch: [24][132/233]	Loss 0.1393 (0.1463)	
training:	Epoch: [24][133/233]	Loss 0.1819 (0.1466)	
training:	Epoch: [24][134/233]	Loss 0.1482 (0.1466)	
training:	Epoch: [24][135/233]	Loss 0.1825 (0.1469)	
training:	Epoch: [24][136/233]	Loss 0.1474 (0.1469)	
training:	Epoch: [24][137/233]	Loss 0.1135 (0.1467)	
training:	Epoch: [24][138/233]	Loss 0.1326 (0.1466)	
training:	Epoch: [24][139/233]	Loss 0.1568 (0.1466)	
training:	Epoch: [24][140/233]	Loss 0.1721 (0.1468)	
training:	Epoch: [24][141/233]	Loss 0.1843 (0.1471)	
training:	Epoch: [24][142/233]	Loss 0.1929 (0.1474)	
training:	Epoch: [24][143/233]	Loss 0.2450 (0.1481)	
training:	Epoch: [24][144/233]	Loss 0.1184 (0.1479)	
training:	Epoch: [24][145/233]	Loss 0.1546 (0.1479)	
training:	Epoch: [24][146/233]	Loss 0.2068 (0.1483)	
training:	Epoch: [24][147/233]	Loss 0.1362 (0.1482)	
training:	Epoch: [24][148/233]	Loss 0.1621 (0.1483)	
training:	Epoch: [24][149/233]	Loss 0.1745 (0.1485)	
training:	Epoch: [24][150/233]	Loss 0.1544 (0.1485)	
training:	Epoch: [24][151/233]	Loss 0.1995 (0.1489)	
training:	Epoch: [24][152/233]	Loss 0.1655 (0.1490)	
training:	Epoch: [24][153/233]	Loss 0.1186 (0.1488)	
training:	Epoch: [24][154/233]	Loss 0.1276 (0.1487)	
training:	Epoch: [24][155/233]	Loss 0.1435 (0.1486)	
training:	Epoch: [24][156/233]	Loss 0.1681 (0.1487)	
training:	Epoch: [24][157/233]	Loss 0.2389 (0.1493)	
training:	Epoch: [24][158/233]	Loss 0.1320 (0.1492)	
training:	Epoch: [24][159/233]	Loss 0.1271 (0.1491)	
training:	Epoch: [24][160/233]	Loss 0.1282 (0.1489)	
training:	Epoch: [24][161/233]	Loss 0.1375 (0.1489)	
training:	Epoch: [24][162/233]	Loss 0.1524 (0.1489)	
training:	Epoch: [24][163/233]	Loss 0.1357 (0.1488)	
training:	Epoch: [24][164/233]	Loss 0.1405 (0.1488)	
training:	Epoch: [24][165/233]	Loss 0.1338 (0.1487)	
training:	Epoch: [24][166/233]	Loss 0.1357 (0.1486)	
training:	Epoch: [24][167/233]	Loss 0.1571 (0.1486)	
training:	Epoch: [24][168/233]	Loss 0.1429 (0.1486)	
training:	Epoch: [24][169/233]	Loss 0.1014 (0.1483)	
training:	Epoch: [24][170/233]	Loss 0.1336 (0.1482)	
training:	Epoch: [24][171/233]	Loss 0.1572 (0.1483)	
training:	Epoch: [24][172/233]	Loss 0.1944 (0.1486)	
training:	Epoch: [24][173/233]	Loss 0.1318 (0.1485)	
training:	Epoch: [24][174/233]	Loss 0.1758 (0.1486)	
training:	Epoch: [24][175/233]	Loss 0.1068 (0.1484)	
training:	Epoch: [24][176/233]	Loss 0.1618 (0.1485)	
training:	Epoch: [24][177/233]	Loss 0.1783 (0.1486)	
training:	Epoch: [24][178/233]	Loss 0.2182 (0.1490)	
training:	Epoch: [24][179/233]	Loss 0.1147 (0.1488)	
training:	Epoch: [24][180/233]	Loss 0.1814 (0.1490)	
training:	Epoch: [24][181/233]	Loss 0.1428 (0.1490)	
training:	Epoch: [24][182/233]	Loss 0.1269 (0.1489)	
training:	Epoch: [24][183/233]	Loss 0.1176 (0.1487)	
training:	Epoch: [24][184/233]	Loss 0.1235 (0.1485)	
training:	Epoch: [24][185/233]	Loss 0.1378 (0.1485)	
training:	Epoch: [24][186/233]	Loss 0.1612 (0.1486)	
training:	Epoch: [24][187/233]	Loss 0.1462 (0.1485)	
training:	Epoch: [24][188/233]	Loss 0.1251 (0.1484)	
training:	Epoch: [24][189/233]	Loss 0.1912 (0.1486)	
training:	Epoch: [24][190/233]	Loss 0.1338 (0.1486)	
training:	Epoch: [24][191/233]	Loss 0.1215 (0.1484)	
training:	Epoch: [24][192/233]	Loss 0.1228 (0.1483)	
training:	Epoch: [24][193/233]	Loss 0.1583 (0.1483)	
training:	Epoch: [24][194/233]	Loss 0.1533 (0.1484)	
training:	Epoch: [24][195/233]	Loss 0.1431 (0.1483)	
training:	Epoch: [24][196/233]	Loss 0.1389 (0.1483)	
training:	Epoch: [24][197/233]	Loss 0.1426 (0.1483)	
training:	Epoch: [24][198/233]	Loss 0.1327 (0.1482)	
training:	Epoch: [24][199/233]	Loss 0.1858 (0.1484)	
training:	Epoch: [24][200/233]	Loss 0.0949 (0.1481)	
training:	Epoch: [24][201/233]	Loss 0.1159 (0.1480)	
training:	Epoch: [24][202/233]	Loss 0.1075 (0.1478)	
training:	Epoch: [24][203/233]	Loss 0.1812 (0.1479)	
training:	Epoch: [24][204/233]	Loss 0.1876 (0.1481)	
training:	Epoch: [24][205/233]	Loss 0.1164 (0.1480)	
training:	Epoch: [24][206/233]	Loss 0.1707 (0.1481)	
training:	Epoch: [24][207/233]	Loss 0.1240 (0.1479)	
training:	Epoch: [24][208/233]	Loss 0.0985 (0.1477)	
training:	Epoch: [24][209/233]	Loss 0.1571 (0.1478)	
training:	Epoch: [24][210/233]	Loss 0.1262 (0.1477)	
training:	Epoch: [24][211/233]	Loss 0.1753 (0.1478)	
training:	Epoch: [24][212/233]	Loss 0.1465 (0.1478)	
training:	Epoch: [24][213/233]	Loss 0.1390 (0.1477)	
training:	Epoch: [24][214/233]	Loss 0.1851 (0.1479)	
training:	Epoch: [24][215/233]	Loss 0.2059 (0.1482)	
training:	Epoch: [24][216/233]	Loss 0.1119 (0.1480)	
training:	Epoch: [24][217/233]	Loss 0.1184 (0.1479)	
training:	Epoch: [24][218/233]	Loss 0.1537 (0.1479)	
training:	Epoch: [24][219/233]	Loss 0.1178 (0.1478)	
training:	Epoch: [24][220/233]	Loss 0.1353 (0.1477)	
training:	Epoch: [24][221/233]	Loss 0.1972 (0.1479)	
training:	Epoch: [24][222/233]	Loss 0.1295 (0.1478)	
training:	Epoch: [24][223/233]	Loss 0.1705 (0.1480)	
training:	Epoch: [24][224/233]	Loss 0.1817 (0.1481)	
training:	Epoch: [24][225/233]	Loss 0.1176 (0.1480)	
training:	Epoch: [24][226/233]	Loss 0.2035 (0.1482)	
training:	Epoch: [24][227/233]	Loss 0.1155 (0.1481)	
training:	Epoch: [24][228/233]	Loss 0.1598 (0.1481)	
training:	Epoch: [24][229/233]	Loss 0.2454 (0.1485)	
training:	Epoch: [24][230/233]	Loss 0.1295 (0.1485)	
training:	Epoch: [24][231/233]	Loss 0.1224 (0.1483)	
training:	Epoch: [24][232/233]	Loss 0.1877 (0.1485)	
training:	Epoch: [24][233/233]	Loss 0.1963 (0.1487)	
Training:	 Loss: 0.1484

Training:	 ACC: 0.9966 0.9967 0.9979 0.9952
Validation:	 ACC: 0.8241 0.8266 0.8802 0.7679
Validation:	 Best_BACC: 0.8284 0.8304 0.8721 0.7848
Validation:	 Loss: 0.3810
Pretraining:	Epoch 25/200
----------
training:	Epoch: [25][1/233]	Loss 0.1552 (0.1552)	
training:	Epoch: [25][2/233]	Loss 0.1080 (0.1316)	
training:	Epoch: [25][3/233]	Loss 0.1231 (0.1288)	
training:	Epoch: [25][4/233]	Loss 0.1482 (0.1336)	
training:	Epoch: [25][5/233]	Loss 0.1331 (0.1335)	
training:	Epoch: [25][6/233]	Loss 0.1058 (0.1289)	
training:	Epoch: [25][7/233]	Loss 0.0851 (0.1226)	
training:	Epoch: [25][8/233]	Loss 0.1423 (0.1251)	
training:	Epoch: [25][9/233]	Loss 0.1384 (0.1266)	
training:	Epoch: [25][10/233]	Loss 0.1168 (0.1256)	
training:	Epoch: [25][11/233]	Loss 0.1113 (0.1243)	
training:	Epoch: [25][12/233]	Loss 0.1638 (0.1276)	
training:	Epoch: [25][13/233]	Loss 0.1067 (0.1260)	
training:	Epoch: [25][14/233]	Loss 0.1336 (0.1265)	
training:	Epoch: [25][15/233]	Loss 0.1796 (0.1301)	
training:	Epoch: [25][16/233]	Loss 0.1102 (0.1288)	
training:	Epoch: [25][17/233]	Loss 0.1647 (0.1309)	
training:	Epoch: [25][18/233]	Loss 0.1193 (0.1303)	
training:	Epoch: [25][19/233]	Loss 0.1359 (0.1306)	
training:	Epoch: [25][20/233]	Loss 0.1141 (0.1298)	
training:	Epoch: [25][21/233]	Loss 0.1598 (0.1312)	
training:	Epoch: [25][22/233]	Loss 0.1738 (0.1331)	
training:	Epoch: [25][23/233]	Loss 0.1604 (0.1343)	
training:	Epoch: [25][24/233]	Loss 0.1192 (0.1337)	
training:	Epoch: [25][25/233]	Loss 0.0920 (0.1320)	
training:	Epoch: [25][26/233]	Loss 0.1512 (0.1328)	
training:	Epoch: [25][27/233]	Loss 0.1427 (0.1331)	
training:	Epoch: [25][28/233]	Loss 0.1160 (0.1325)	
training:	Epoch: [25][29/233]	Loss 0.1334 (0.1325)	
training:	Epoch: [25][30/233]	Loss 0.1390 (0.1328)	
training:	Epoch: [25][31/233]	Loss 0.1234 (0.1325)	
training:	Epoch: [25][32/233]	Loss 0.1221 (0.1321)	
training:	Epoch: [25][33/233]	Loss 0.1479 (0.1326)	
training:	Epoch: [25][34/233]	Loss 0.1193 (0.1322)	
training:	Epoch: [25][35/233]	Loss 0.1272 (0.1321)	
training:	Epoch: [25][36/233]	Loss 0.1452 (0.1324)	
training:	Epoch: [25][37/233]	Loss 0.1604 (0.1332)	
training:	Epoch: [25][38/233]	Loss 0.2235 (0.1356)	
training:	Epoch: [25][39/233]	Loss 0.1133 (0.1350)	
training:	Epoch: [25][40/233]	Loss 0.1563 (0.1355)	
training:	Epoch: [25][41/233]	Loss 0.1551 (0.1360)	
training:	Epoch: [25][42/233]	Loss 0.1511 (0.1364)	
training:	Epoch: [25][43/233]	Loss 0.1803 (0.1374)	
training:	Epoch: [25][44/233]	Loss 0.1890 (0.1386)	
training:	Epoch: [25][45/233]	Loss 0.1139 (0.1380)	
training:	Epoch: [25][46/233]	Loss 0.1609 (0.1385)	
training:	Epoch: [25][47/233]	Loss 0.0999 (0.1377)	
training:	Epoch: [25][48/233]	Loss 0.1156 (0.1372)	
training:	Epoch: [25][49/233]	Loss 0.1134 (0.1367)	
training:	Epoch: [25][50/233]	Loss 0.1511 (0.1370)	
training:	Epoch: [25][51/233]	Loss 0.1497 (0.1373)	
training:	Epoch: [25][52/233]	Loss 0.1605 (0.1377)	
training:	Epoch: [25][53/233]	Loss 0.2134 (0.1392)	
training:	Epoch: [25][54/233]	Loss 0.1802 (0.1399)	
training:	Epoch: [25][55/233]	Loss 0.1409 (0.1399)	
training:	Epoch: [25][56/233]	Loss 0.1339 (0.1398)	
training:	Epoch: [25][57/233]	Loss 0.1469 (0.1399)	
training:	Epoch: [25][58/233]	Loss 0.1263 (0.1397)	
training:	Epoch: [25][59/233]	Loss 0.1339 (0.1396)	
training:	Epoch: [25][60/233]	Loss 0.2031 (0.1407)	
training:	Epoch: [25][61/233]	Loss 0.1343 (0.1406)	
training:	Epoch: [25][62/233]	Loss 0.1120 (0.1401)	
training:	Epoch: [25][63/233]	Loss 0.0921 (0.1393)	
training:	Epoch: [25][64/233]	Loss 0.1067 (0.1388)	
training:	Epoch: [25][65/233]	Loss 0.1131 (0.1384)	
training:	Epoch: [25][66/233]	Loss 0.1483 (0.1386)	
training:	Epoch: [25][67/233]	Loss 0.2363 (0.1400)	
training:	Epoch: [25][68/233]	Loss 0.1223 (0.1398)	
training:	Epoch: [25][69/233]	Loss 0.1603 (0.1401)	
training:	Epoch: [25][70/233]	Loss 0.1344 (0.1400)	
training:	Epoch: [25][71/233]	Loss 0.1723 (0.1405)	
training:	Epoch: [25][72/233]	Loss 0.2153 (0.1415)	
training:	Epoch: [25][73/233]	Loss 0.1836 (0.1421)	
training:	Epoch: [25][74/233]	Loss 0.1142 (0.1417)	
training:	Epoch: [25][75/233]	Loss 0.1314 (0.1416)	
training:	Epoch: [25][76/233]	Loss 0.1120 (0.1412)	
training:	Epoch: [25][77/233]	Loss 0.1054 (0.1407)	
training:	Epoch: [25][78/233]	Loss 0.1190 (0.1404)	
training:	Epoch: [25][79/233]	Loss 0.1013 (0.1399)	
training:	Epoch: [25][80/233]	Loss 0.1502 (0.1401)	
training:	Epoch: [25][81/233]	Loss 0.1333 (0.1400)	
training:	Epoch: [25][82/233]	Loss 0.1609 (0.1402)	
training:	Epoch: [25][83/233]	Loss 0.1136 (0.1399)	
training:	Epoch: [25][84/233]	Loss 0.1269 (0.1398)	
training:	Epoch: [25][85/233]	Loss 0.1410 (0.1398)	
training:	Epoch: [25][86/233]	Loss 0.1377 (0.1397)	
training:	Epoch: [25][87/233]	Loss 0.1437 (0.1398)	
training:	Epoch: [25][88/233]	Loss 0.1490 (0.1399)	
training:	Epoch: [25][89/233]	Loss 0.1628 (0.1402)	
training:	Epoch: [25][90/233]	Loss 0.1491 (0.1403)	
training:	Epoch: [25][91/233]	Loss 0.1176 (0.1400)	
training:	Epoch: [25][92/233]	Loss 0.1262 (0.1399)	
training:	Epoch: [25][93/233]	Loss 0.1399 (0.1399)	
training:	Epoch: [25][94/233]	Loss 0.1762 (0.1402)	
training:	Epoch: [25][95/233]	Loss 0.1734 (0.1406)	
training:	Epoch: [25][96/233]	Loss 0.1418 (0.1406)	
training:	Epoch: [25][97/233]	Loss 0.1469 (0.1407)	
training:	Epoch: [25][98/233]	Loss 0.0964 (0.1402)	
training:	Epoch: [25][99/233]	Loss 0.1240 (0.1401)	
training:	Epoch: [25][100/233]	Loss 0.1248 (0.1399)	
training:	Epoch: [25][101/233]	Loss 0.1431 (0.1399)	
training:	Epoch: [25][102/233]	Loss 0.1161 (0.1397)	
training:	Epoch: [25][103/233]	Loss 0.1147 (0.1395)	
training:	Epoch: [25][104/233]	Loss 0.2263 (0.1403)	
training:	Epoch: [25][105/233]	Loss 0.1439 (0.1403)	
training:	Epoch: [25][106/233]	Loss 0.1270 (0.1402)	
training:	Epoch: [25][107/233]	Loss 0.1619 (0.1404)	
training:	Epoch: [25][108/233]	Loss 0.2003 (0.1410)	
training:	Epoch: [25][109/233]	Loss 0.0971 (0.1406)	
training:	Epoch: [25][110/233]	Loss 0.1518 (0.1407)	
training:	Epoch: [25][111/233]	Loss 0.1252 (0.1405)	
training:	Epoch: [25][112/233]	Loss 0.1016 (0.1402)	
training:	Epoch: [25][113/233]	Loss 0.1264 (0.1400)	
training:	Epoch: [25][114/233]	Loss 0.1508 (0.1401)	
training:	Epoch: [25][115/233]	Loss 0.1455 (0.1402)	
training:	Epoch: [25][116/233]	Loss 0.1129 (0.1400)	
training:	Epoch: [25][117/233]	Loss 0.1486 (0.1400)	
training:	Epoch: [25][118/233]	Loss 0.1442 (0.1401)	
training:	Epoch: [25][119/233]	Loss 0.1563 (0.1402)	
training:	Epoch: [25][120/233]	Loss 0.1460 (0.1402)	
training:	Epoch: [25][121/233]	Loss 0.1306 (0.1402)	
training:	Epoch: [25][122/233]	Loss 0.1380 (0.1401)	
training:	Epoch: [25][123/233]	Loss 0.1248 (0.1400)	
training:	Epoch: [25][124/233]	Loss 0.1196 (0.1399)	
training:	Epoch: [25][125/233]	Loss 0.1428 (0.1399)	
training:	Epoch: [25][126/233]	Loss 0.1726 (0.1401)	
training:	Epoch: [25][127/233]	Loss 0.0970 (0.1398)	
training:	Epoch: [25][128/233]	Loss 0.2607 (0.1407)	
training:	Epoch: [25][129/233]	Loss 0.1440 (0.1408)	
training:	Epoch: [25][130/233]	Loss 0.1109 (0.1405)	
training:	Epoch: [25][131/233]	Loss 0.1199 (0.1404)	
training:	Epoch: [25][132/233]	Loss 0.0959 (0.1400)	
training:	Epoch: [25][133/233]	Loss 0.1441 (0.1401)	
training:	Epoch: [25][134/233]	Loss 0.1359 (0.1400)	
training:	Epoch: [25][135/233]	Loss 0.1902 (0.1404)	
training:	Epoch: [25][136/233]	Loss 0.1408 (0.1404)	
training:	Epoch: [25][137/233]	Loss 0.1236 (0.1403)	
training:	Epoch: [25][138/233]	Loss 0.1827 (0.1406)	
training:	Epoch: [25][139/233]	Loss 0.1153 (0.1404)	
training:	Epoch: [25][140/233]	Loss 0.1453 (0.1405)	
training:	Epoch: [25][141/233]	Loss 0.1169 (0.1403)	
training:	Epoch: [25][142/233]	Loss 0.1660 (0.1405)	
training:	Epoch: [25][143/233]	Loss 0.1621 (0.1406)	
training:	Epoch: [25][144/233]	Loss 0.0939 (0.1403)	
training:	Epoch: [25][145/233]	Loss 0.1554 (0.1404)	
training:	Epoch: [25][146/233]	Loss 0.1353 (0.1404)	
training:	Epoch: [25][147/233]	Loss 0.1813 (0.1406)	
training:	Epoch: [25][148/233]	Loss 0.1286 (0.1406)	
training:	Epoch: [25][149/233]	Loss 0.0926 (0.1402)	
training:	Epoch: [25][150/233]	Loss 0.1701 (0.1404)	
training:	Epoch: [25][151/233]	Loss 0.1369 (0.1404)	
training:	Epoch: [25][152/233]	Loss 0.1104 (0.1402)	
training:	Epoch: [25][153/233]	Loss 0.1337 (0.1402)	
training:	Epoch: [25][154/233]	Loss 0.1044 (0.1399)	
training:	Epoch: [25][155/233]	Loss 0.1502 (0.1400)	
training:	Epoch: [25][156/233]	Loss 0.1413 (0.1400)	
training:	Epoch: [25][157/233]	Loss 0.1196 (0.1399)	
training:	Epoch: [25][158/233]	Loss 0.1520 (0.1400)	
training:	Epoch: [25][159/233]	Loss 0.1214 (0.1399)	
training:	Epoch: [25][160/233]	Loss 0.1785 (0.1401)	
training:	Epoch: [25][161/233]	Loss 0.1300 (0.1400)	
training:	Epoch: [25][162/233]	Loss 0.1283 (0.1400)	
training:	Epoch: [25][163/233]	Loss 0.1205 (0.1398)	
training:	Epoch: [25][164/233]	Loss 0.1237 (0.1397)	
training:	Epoch: [25][165/233]	Loss 0.1300 (0.1397)	
training:	Epoch: [25][166/233]	Loss 0.2001 (0.1400)	
training:	Epoch: [25][167/233]	Loss 0.1361 (0.1400)	
training:	Epoch: [25][168/233]	Loss 0.1327 (0.1400)	
training:	Epoch: [25][169/233]	Loss 0.1779 (0.1402)	
training:	Epoch: [25][170/233]	Loss 0.1471 (0.1402)	
training:	Epoch: [25][171/233]	Loss 0.1324 (0.1402)	
training:	Epoch: [25][172/233]	Loss 0.2134 (0.1406)	
training:	Epoch: [25][173/233]	Loss 0.1359 (0.1406)	
training:	Epoch: [25][174/233]	Loss 0.1110 (0.1404)	
training:	Epoch: [25][175/233]	Loss 0.1700 (0.1406)	
training:	Epoch: [25][176/233]	Loss 0.1348 (0.1406)	
training:	Epoch: [25][177/233]	Loss 0.1348 (0.1405)	
training:	Epoch: [25][178/233]	Loss 0.1743 (0.1407)	
training:	Epoch: [25][179/233]	Loss 0.1484 (0.1408)	
training:	Epoch: [25][180/233]	Loss 0.1790 (0.1410)	
training:	Epoch: [25][181/233]	Loss 0.1676 (0.1411)	
training:	Epoch: [25][182/233]	Loss 0.1335 (0.1411)	
training:	Epoch: [25][183/233]	Loss 0.1507 (0.1411)	
training:	Epoch: [25][184/233]	Loss 0.1666 (0.1413)	
training:	Epoch: [25][185/233]	Loss 0.1430 (0.1413)	
training:	Epoch: [25][186/233]	Loss 0.1389 (0.1413)	
training:	Epoch: [25][187/233]	Loss 0.1992 (0.1416)	
training:	Epoch: [25][188/233]	Loss 0.1328 (0.1415)	
training:	Epoch: [25][189/233]	Loss 0.1516 (0.1416)	
training:	Epoch: [25][190/233]	Loss 0.1450 (0.1416)	
training:	Epoch: [25][191/233]	Loss 0.2184 (0.1420)	
training:	Epoch: [25][192/233]	Loss 0.1249 (0.1419)	
training:	Epoch: [25][193/233]	Loss 0.1813 (0.1421)	
training:	Epoch: [25][194/233]	Loss 0.1648 (0.1422)	
training:	Epoch: [25][195/233]	Loss 0.1687 (0.1424)	
training:	Epoch: [25][196/233]	Loss 0.1448 (0.1424)	
training:	Epoch: [25][197/233]	Loss 0.1680 (0.1425)	
training:	Epoch: [25][198/233]	Loss 0.1280 (0.1424)	
training:	Epoch: [25][199/233]	Loss 0.1177 (0.1423)	
training:	Epoch: [25][200/233]	Loss 0.1074 (0.1421)	
training:	Epoch: [25][201/233]	Loss 0.1866 (0.1424)	
training:	Epoch: [25][202/233]	Loss 0.1136 (0.1422)	
training:	Epoch: [25][203/233]	Loss 0.1771 (0.1424)	
training:	Epoch: [25][204/233]	Loss 0.1172 (0.1423)	
training:	Epoch: [25][205/233]	Loss 0.1590 (0.1423)	
training:	Epoch: [25][206/233]	Loss 0.1324 (0.1423)	
training:	Epoch: [25][207/233]	Loss 0.1488 (0.1423)	
training:	Epoch: [25][208/233]	Loss 0.1330 (0.1423)	
training:	Epoch: [25][209/233]	Loss 0.1549 (0.1423)	
training:	Epoch: [25][210/233]	Loss 0.1580 (0.1424)	
training:	Epoch: [25][211/233]	Loss 0.1056 (0.1422)	
training:	Epoch: [25][212/233]	Loss 0.1625 (0.1423)	
training:	Epoch: [25][213/233]	Loss 0.1657 (0.1425)	
training:	Epoch: [25][214/233]	Loss 0.1523 (0.1425)	
training:	Epoch: [25][215/233]	Loss 0.1549 (0.1426)	
training:	Epoch: [25][216/233]	Loss 0.1416 (0.1426)	
training:	Epoch: [25][217/233]	Loss 0.1520 (0.1426)	
training:	Epoch: [25][218/233]	Loss 0.1382 (0.1426)	
training:	Epoch: [25][219/233]	Loss 0.2016 (0.1428)	
training:	Epoch: [25][220/233]	Loss 0.1256 (0.1428)	
training:	Epoch: [25][221/233]	Loss 0.1681 (0.1429)	
training:	Epoch: [25][222/233]	Loss 0.1466 (0.1429)	
training:	Epoch: [25][223/233]	Loss 0.1721 (0.1430)	
training:	Epoch: [25][224/233]	Loss 0.1364 (0.1430)	
training:	Epoch: [25][225/233]	Loss 0.1085 (0.1428)	
training:	Epoch: [25][226/233]	Loss 0.1297 (0.1428)	
training:	Epoch: [25][227/233]	Loss 0.1642 (0.1429)	
training:	Epoch: [25][228/233]	Loss 0.1761 (0.1430)	
training:	Epoch: [25][229/233]	Loss 0.1066 (0.1429)	
training:	Epoch: [25][230/233]	Loss 0.1214 (0.1428)	
training:	Epoch: [25][231/233]	Loss 0.0997 (0.1426)	
training:	Epoch: [25][232/233]	Loss 0.1229 (0.1425)	
training:	Epoch: [25][233/233]	Loss 0.1546 (0.1426)	
Training:	 Loss: 0.1422

Training:	 ACC: 0.9979 0.9979 0.9969 0.9989
Validation:	 ACC: 0.8323 0.8331 0.8485 0.8161
Validation:	 Best_BACC: 0.8323 0.8331 0.8485 0.8161
Validation:	 Loss: 0.3736
Pretraining:	Epoch 26/200
----------
training:	Epoch: [26][1/233]	Loss 0.1298 (0.1298)	
training:	Epoch: [26][2/233]	Loss 0.1177 (0.1238)	
training:	Epoch: [26][3/233]	Loss 0.1296 (0.1257)	
training:	Epoch: [26][4/233]	Loss 0.1463 (0.1309)	
training:	Epoch: [26][5/233]	Loss 0.1156 (0.1278)	
training:	Epoch: [26][6/233]	Loss 0.1265 (0.1276)	
training:	Epoch: [26][7/233]	Loss 0.1093 (0.1250)	
training:	Epoch: [26][8/233]	Loss 0.1406 (0.1269)	
training:	Epoch: [26][9/233]	Loss 0.1401 (0.1284)	
training:	Epoch: [26][10/233]	Loss 0.0928 (0.1248)	
training:	Epoch: [26][11/233]	Loss 0.1300 (0.1253)	
training:	Epoch: [26][12/233]	Loss 0.1702 (0.1291)	
training:	Epoch: [26][13/233]	Loss 0.1422 (0.1301)	
training:	Epoch: [26][14/233]	Loss 0.1141 (0.1289)	
training:	Epoch: [26][15/233]	Loss 0.1552 (0.1307)	
training:	Epoch: [26][16/233]	Loss 0.1532 (0.1321)	
training:	Epoch: [26][17/233]	Loss 0.1340 (0.1322)	
training:	Epoch: [26][18/233]	Loss 0.1457 (0.1330)	
training:	Epoch: [26][19/233]	Loss 0.1144 (0.1320)	
training:	Epoch: [26][20/233]	Loss 0.1298 (0.1319)	
training:	Epoch: [26][21/233]	Loss 0.1348 (0.1320)	
training:	Epoch: [26][22/233]	Loss 0.1494 (0.1328)	
training:	Epoch: [26][23/233]	Loss 0.1281 (0.1326)	
training:	Epoch: [26][24/233]	Loss 0.2139 (0.1360)	
training:	Epoch: [26][25/233]	Loss 0.1214 (0.1354)	
training:	Epoch: [26][26/233]	Loss 0.1425 (0.1357)	
training:	Epoch: [26][27/233]	Loss 0.1096 (0.1347)	
training:	Epoch: [26][28/233]	Loss 0.1382 (0.1348)	
training:	Epoch: [26][29/233]	Loss 0.1452 (0.1352)	
training:	Epoch: [26][30/233]	Loss 0.1398 (0.1353)	
training:	Epoch: [26][31/233]	Loss 0.1126 (0.1346)	
training:	Epoch: [26][32/233]	Loss 0.1445 (0.1349)	
training:	Epoch: [26][33/233]	Loss 0.1401 (0.1351)	
training:	Epoch: [26][34/233]	Loss 0.1512 (0.1355)	
training:	Epoch: [26][35/233]	Loss 0.1574 (0.1362)	
training:	Epoch: [26][36/233]	Loss 0.1693 (0.1371)	
training:	Epoch: [26][37/233]	Loss 0.1244 (0.1367)	
training:	Epoch: [26][38/233]	Loss 0.1251 (0.1364)	
training:	Epoch: [26][39/233]	Loss 0.1319 (0.1363)	
training:	Epoch: [26][40/233]	Loss 0.1391 (0.1364)	
training:	Epoch: [26][41/233]	Loss 0.1733 (0.1373)	
training:	Epoch: [26][42/233]	Loss 0.1664 (0.1380)	
training:	Epoch: [26][43/233]	Loss 0.1007 (0.1371)	
training:	Epoch: [26][44/233]	Loss 0.1078 (0.1365)	
training:	Epoch: [26][45/233]	Loss 0.1209 (0.1361)	
training:	Epoch: [26][46/233]	Loss 0.1859 (0.1372)	
training:	Epoch: [26][47/233]	Loss 0.0994 (0.1364)	
training:	Epoch: [26][48/233]	Loss 0.1225 (0.1361)	
training:	Epoch: [26][49/233]	Loss 0.1267 (0.1359)	
training:	Epoch: [26][50/233]	Loss 0.1975 (0.1371)	
training:	Epoch: [26][51/233]	Loss 0.1227 (0.1369)	
training:	Epoch: [26][52/233]	Loss 0.1300 (0.1367)	
training:	Epoch: [26][53/233]	Loss 0.1097 (0.1362)	
training:	Epoch: [26][54/233]	Loss 0.1102 (0.1357)	
training:	Epoch: [26][55/233]	Loss 0.1132 (0.1353)	
training:	Epoch: [26][56/233]	Loss 0.1397 (0.1354)	
training:	Epoch: [26][57/233]	Loss 0.1714 (0.1360)	
training:	Epoch: [26][58/233]	Loss 0.1343 (0.1360)	
training:	Epoch: [26][59/233]	Loss 0.1833 (0.1368)	
training:	Epoch: [26][60/233]	Loss 0.2038 (0.1379)	
training:	Epoch: [26][61/233]	Loss 0.1299 (0.1378)	
training:	Epoch: [26][62/233]	Loss 0.1075 (0.1373)	
training:	Epoch: [26][63/233]	Loss 0.1543 (0.1376)	
training:	Epoch: [26][64/233]	Loss 0.1303 (0.1375)	
training:	Epoch: [26][65/233]	Loss 0.1880 (0.1382)	
training:	Epoch: [26][66/233]	Loss 0.1106 (0.1378)	
training:	Epoch: [26][67/233]	Loss 0.1336 (0.1378)	
training:	Epoch: [26][68/233]	Loss 0.1758 (0.1383)	
training:	Epoch: [26][69/233]	Loss 0.1306 (0.1382)	
training:	Epoch: [26][70/233]	Loss 0.1974 (0.1390)	
training:	Epoch: [26][71/233]	Loss 0.1294 (0.1389)	
training:	Epoch: [26][72/233]	Loss 0.1305 (0.1388)	
training:	Epoch: [26][73/233]	Loss 0.0858 (0.1381)	
training:	Epoch: [26][74/233]	Loss 0.1184 (0.1378)	
training:	Epoch: [26][75/233]	Loss 0.1194 (0.1376)	
training:	Epoch: [26][76/233]	Loss 0.1204 (0.1373)	
training:	Epoch: [26][77/233]	Loss 0.1353 (0.1373)	
training:	Epoch: [26][78/233]	Loss 0.1272 (0.1372)	
training:	Epoch: [26][79/233]	Loss 0.1234 (0.1370)	
training:	Epoch: [26][80/233]	Loss 0.1380 (0.1370)	
training:	Epoch: [26][81/233]	Loss 0.1453 (0.1371)	
training:	Epoch: [26][82/233]	Loss 0.1652 (0.1375)	
training:	Epoch: [26][83/233]	Loss 0.1195 (0.1372)	
training:	Epoch: [26][84/233]	Loss 0.1356 (0.1372)	
training:	Epoch: [26][85/233]	Loss 0.0976 (0.1368)	
training:	Epoch: [26][86/233]	Loss 0.1288 (0.1367)	
training:	Epoch: [26][87/233]	Loss 0.1629 (0.1370)	
training:	Epoch: [26][88/233]	Loss 0.1874 (0.1375)	
training:	Epoch: [26][89/233]	Loss 0.1384 (0.1375)	
training:	Epoch: [26][90/233]	Loss 0.1346 (0.1375)	
training:	Epoch: [26][91/233]	Loss 0.1046 (0.1372)	
training:	Epoch: [26][92/233]	Loss 0.1350 (0.1371)	
training:	Epoch: [26][93/233]	Loss 0.1403 (0.1372)	
training:	Epoch: [26][94/233]	Loss 0.1016 (0.1368)	
training:	Epoch: [26][95/233]	Loss 0.1618 (0.1370)	
training:	Epoch: [26][96/233]	Loss 0.1736 (0.1374)	
training:	Epoch: [26][97/233]	Loss 0.2257 (0.1383)	
training:	Epoch: [26][98/233]	Loss 0.1261 (0.1382)	
training:	Epoch: [26][99/233]	Loss 0.1770 (0.1386)	
training:	Epoch: [26][100/233]	Loss 0.1259 (0.1385)	
training:	Epoch: [26][101/233]	Loss 0.1588 (0.1387)	
training:	Epoch: [26][102/233]	Loss 0.1320 (0.1386)	
training:	Epoch: [26][103/233]	Loss 0.1351 (0.1386)	
training:	Epoch: [26][104/233]	Loss 0.0929 (0.1381)	
training:	Epoch: [26][105/233]	Loss 0.0985 (0.1378)	
training:	Epoch: [26][106/233]	Loss 0.1674 (0.1380)	
training:	Epoch: [26][107/233]	Loss 0.1346 (0.1380)	
training:	Epoch: [26][108/233]	Loss 0.1633 (0.1382)	
training:	Epoch: [26][109/233]	Loss 0.1305 (0.1382)	
training:	Epoch: [26][110/233]	Loss 0.1246 (0.1381)	
training:	Epoch: [26][111/233]	Loss 0.1377 (0.1380)	
training:	Epoch: [26][112/233]	Loss 0.1363 (0.1380)	
training:	Epoch: [26][113/233]	Loss 0.1016 (0.1377)	
training:	Epoch: [26][114/233]	Loss 0.1077 (0.1374)	
training:	Epoch: [26][115/233]	Loss 0.1120 (0.1372)	
training:	Epoch: [26][116/233]	Loss 0.1357 (0.1372)	
training:	Epoch: [26][117/233]	Loss 0.0945 (0.1368)	
training:	Epoch: [26][118/233]	Loss 0.1360 (0.1368)	
training:	Epoch: [26][119/233]	Loss 0.1223 (0.1367)	
training:	Epoch: [26][120/233]	Loss 0.1531 (0.1369)	
training:	Epoch: [26][121/233]	Loss 0.1122 (0.1367)	
training:	Epoch: [26][122/233]	Loss 0.1230 (0.1365)	
training:	Epoch: [26][123/233]	Loss 0.1347 (0.1365)	
training:	Epoch: [26][124/233]	Loss 0.1127 (0.1363)	
training:	Epoch: [26][125/233]	Loss 0.1389 (0.1364)	
training:	Epoch: [26][126/233]	Loss 0.2034 (0.1369)	
training:	Epoch: [26][127/233]	Loss 0.1192 (0.1367)	
training:	Epoch: [26][128/233]	Loss 0.2008 (0.1372)	
training:	Epoch: [26][129/233]	Loss 0.1511 (0.1374)	
training:	Epoch: [26][130/233]	Loss 0.1529 (0.1375)	
training:	Epoch: [26][131/233]	Loss 0.1884 (0.1379)	
training:	Epoch: [26][132/233]	Loss 0.1328 (0.1378)	
training:	Epoch: [26][133/233]	Loss 0.1368 (0.1378)	
training:	Epoch: [26][134/233]	Loss 0.1224 (0.1377)	
training:	Epoch: [26][135/233]	Loss 0.1334 (0.1377)	
training:	Epoch: [26][136/233]	Loss 0.1266 (0.1376)	
training:	Epoch: [26][137/233]	Loss 0.1767 (0.1379)	
training:	Epoch: [26][138/233]	Loss 0.1079 (0.1377)	
training:	Epoch: [26][139/233]	Loss 0.1318 (0.1376)	
training:	Epoch: [26][140/233]	Loss 0.1525 (0.1377)	
training:	Epoch: [26][141/233]	Loss 0.1138 (0.1376)	
training:	Epoch: [26][142/233]	Loss 0.1337 (0.1375)	
training:	Epoch: [26][143/233]	Loss 0.1470 (0.1376)	
training:	Epoch: [26][144/233]	Loss 0.1191 (0.1375)	
training:	Epoch: [26][145/233]	Loss 0.1162 (0.1373)	
training:	Epoch: [26][146/233]	Loss 0.1633 (0.1375)	
training:	Epoch: [26][147/233]	Loss 0.1100 (0.1373)	
training:	Epoch: [26][148/233]	Loss 0.1154 (0.1372)	
training:	Epoch: [26][149/233]	Loss 0.2300 (0.1378)	
training:	Epoch: [26][150/233]	Loss 0.1256 (0.1377)	
training:	Epoch: [26][151/233]	Loss 0.1232 (0.1376)	
training:	Epoch: [26][152/233]	Loss 0.1152 (0.1375)	
training:	Epoch: [26][153/233]	Loss 0.1531 (0.1376)	
training:	Epoch: [26][154/233]	Loss 0.1761 (0.1378)	
training:	Epoch: [26][155/233]	Loss 0.1335 (0.1378)	
training:	Epoch: [26][156/233]	Loss 0.1496 (0.1379)	
training:	Epoch: [26][157/233]	Loss 0.1210 (0.1378)	
training:	Epoch: [26][158/233]	Loss 0.0908 (0.1375)	
training:	Epoch: [26][159/233]	Loss 0.1392 (0.1375)	
training:	Epoch: [26][160/233]	Loss 0.2008 (0.1379)	
training:	Epoch: [26][161/233]	Loss 0.1335 (0.1378)	
training:	Epoch: [26][162/233]	Loss 0.1337 (0.1378)	
training:	Epoch: [26][163/233]	Loss 0.1413 (0.1378)	
training:	Epoch: [26][164/233]	Loss 0.1182 (0.1377)	
training:	Epoch: [26][165/233]	Loss 0.1538 (0.1378)	
training:	Epoch: [26][166/233]	Loss 0.1610 (0.1379)	
training:	Epoch: [26][167/233]	Loss 0.0897 (0.1377)	
training:	Epoch: [26][168/233]	Loss 0.1488 (0.1377)	
training:	Epoch: [26][169/233]	Loss 0.1057 (0.1375)	
training:	Epoch: [26][170/233]	Loss 0.1387 (0.1375)	
training:	Epoch: [26][171/233]	Loss 0.1113 (0.1374)	
training:	Epoch: [26][172/233]	Loss 0.1204 (0.1373)	
training:	Epoch: [26][173/233]	Loss 0.1172 (0.1372)	
training:	Epoch: [26][174/233]	Loss 0.0696 (0.1368)	
training:	Epoch: [26][175/233]	Loss 0.1186 (0.1367)	
training:	Epoch: [26][176/233]	Loss 0.1650 (0.1368)	
training:	Epoch: [26][177/233]	Loss 0.1110 (0.1367)	
training:	Epoch: [26][178/233]	Loss 0.0883 (0.1364)	
training:	Epoch: [26][179/233]	Loss 0.1226 (0.1363)	
training:	Epoch: [26][180/233]	Loss 0.1640 (0.1365)	
training:	Epoch: [26][181/233]	Loss 0.1440 (0.1365)	
training:	Epoch: [26][182/233]	Loss 0.1318 (0.1365)	
training:	Epoch: [26][183/233]	Loss 0.1526 (0.1366)	
training:	Epoch: [26][184/233]	Loss 0.0924 (0.1364)	
training:	Epoch: [26][185/233]	Loss 0.1216 (0.1363)	
training:	Epoch: [26][186/233]	Loss 0.1532 (0.1364)	
training:	Epoch: [26][187/233]	Loss 0.1338 (0.1364)	
training:	Epoch: [26][188/233]	Loss 0.1304 (0.1363)	
training:	Epoch: [26][189/233]	Loss 0.1452 (0.1364)	
training:	Epoch: [26][190/233]	Loss 0.1202 (0.1363)	
training:	Epoch: [26][191/233]	Loss 0.1647 (0.1364)	
training:	Epoch: [26][192/233]	Loss 0.1693 (0.1366)	
training:	Epoch: [26][193/233]	Loss 0.1475 (0.1367)	
training:	Epoch: [26][194/233]	Loss 0.1128 (0.1365)	
training:	Epoch: [26][195/233]	Loss 0.1899 (0.1368)	
training:	Epoch: [26][196/233]	Loss 0.0964 (0.1366)	
training:	Epoch: [26][197/233]	Loss 0.0884 (0.1364)	
training:	Epoch: [26][198/233]	Loss 0.1150 (0.1363)	
training:	Epoch: [26][199/233]	Loss 0.1124 (0.1361)	
training:	Epoch: [26][200/233]	Loss 0.1117 (0.1360)	
training:	Epoch: [26][201/233]	Loss 0.1986 (0.1363)	
training:	Epoch: [26][202/233]	Loss 0.1540 (0.1364)	
training:	Epoch: [26][203/233]	Loss 0.1339 (0.1364)	
training:	Epoch: [26][204/233]	Loss 0.1456 (0.1364)	
training:	Epoch: [26][205/233]	Loss 0.1298 (0.1364)	
training:	Epoch: [26][206/233]	Loss 0.1486 (0.1365)	
training:	Epoch: [26][207/233]	Loss 0.1702 (0.1366)	
training:	Epoch: [26][208/233]	Loss 0.1149 (0.1365)	
training:	Epoch: [26][209/233]	Loss 0.1478 (0.1366)	
training:	Epoch: [26][210/233]	Loss 0.1311 (0.1366)	
training:	Epoch: [26][211/233]	Loss 0.1605 (0.1367)	
training:	Epoch: [26][212/233]	Loss 0.2614 (0.1373)	
training:	Epoch: [26][213/233]	Loss 0.1244 (0.1372)	
training:	Epoch: [26][214/233]	Loss 0.1509 (0.1373)	
training:	Epoch: [26][215/233]	Loss 0.1374 (0.1373)	
training:	Epoch: [26][216/233]	Loss 0.1612 (0.1374)	
training:	Epoch: [26][217/233]	Loss 0.1336 (0.1374)	
training:	Epoch: [26][218/233]	Loss 0.1473 (0.1374)	
training:	Epoch: [26][219/233]	Loss 0.0802 (0.1371)	
training:	Epoch: [26][220/233]	Loss 0.1439 (0.1372)	
training:	Epoch: [26][221/233]	Loss 0.1236 (0.1371)	
training:	Epoch: [26][222/233]	Loss 0.1167 (0.1370)	
training:	Epoch: [26][223/233]	Loss 0.2315 (0.1374)	
training:	Epoch: [26][224/233]	Loss 0.1365 (0.1374)	
training:	Epoch: [26][225/233]	Loss 0.1491 (0.1375)	
training:	Epoch: [26][226/233]	Loss 0.1713 (0.1376)	
training:	Epoch: [26][227/233]	Loss 0.1632 (0.1378)	
training:	Epoch: [26][228/233]	Loss 0.1403 (0.1378)	
training:	Epoch: [26][229/233]	Loss 0.1178 (0.1377)	
training:	Epoch: [26][230/233]	Loss 0.1849 (0.1379)	
training:	Epoch: [26][231/233]	Loss 0.1467 (0.1379)	
training:	Epoch: [26][232/233]	Loss 0.1122 (0.1378)	
training:	Epoch: [26][233/233]	Loss 0.1556 (0.1379)	
Training:	 Loss: 0.1376

Training:	 ACC: 0.9985 0.9985 0.9982 0.9989
Validation:	 ACC: 0.8313 0.8331 0.8700 0.7926
Validation:	 Best_BACC: 0.8323 0.8331 0.8485 0.8161
Validation:	 Loss: 0.3740
Pretraining:	Epoch 27/200
----------
training:	Epoch: [27][1/233]	Loss 0.1220 (0.1220)	
training:	Epoch: [27][2/233]	Loss 0.1240 (0.1230)	
training:	Epoch: [27][3/233]	Loss 0.1142 (0.1201)	
training:	Epoch: [27][4/233]	Loss 0.1478 (0.1270)	
training:	Epoch: [27][5/233]	Loss 0.1092 (0.1234)	
training:	Epoch: [27][6/233]	Loss 0.1571 (0.1290)	
training:	Epoch: [27][7/233]	Loss 0.0920 (0.1238)	
training:	Epoch: [27][8/233]	Loss 0.1086 (0.1219)	
training:	Epoch: [27][9/233]	Loss 0.1135 (0.1209)	
training:	Epoch: [27][10/233]	Loss 0.1219 (0.1210)	
training:	Epoch: [27][11/233]	Loss 0.1672 (0.1252)	
training:	Epoch: [27][12/233]	Loss 0.1468 (0.1270)	
training:	Epoch: [27][13/233]	Loss 0.1590 (0.1295)	
training:	Epoch: [27][14/233]	Loss 0.1311 (0.1296)	
training:	Epoch: [27][15/233]	Loss 0.1497 (0.1309)	
training:	Epoch: [27][16/233]	Loss 0.1179 (0.1301)	
training:	Epoch: [27][17/233]	Loss 0.2069 (0.1346)	
training:	Epoch: [27][18/233]	Loss 0.1430 (0.1351)	
training:	Epoch: [27][19/233]	Loss 0.1126 (0.1339)	
training:	Epoch: [27][20/233]	Loss 0.1382 (0.1341)	
training:	Epoch: [27][21/233]	Loss 0.1178 (0.1334)	
training:	Epoch: [27][22/233]	Loss 0.1450 (0.1339)	
training:	Epoch: [27][23/233]	Loss 0.2031 (0.1369)	
training:	Epoch: [27][24/233]	Loss 0.0985 (0.1353)	
training:	Epoch: [27][25/233]	Loss 0.1322 (0.1352)	
training:	Epoch: [27][26/233]	Loss 0.1469 (0.1356)	
training:	Epoch: [27][27/233]	Loss 0.1181 (0.1350)	
training:	Epoch: [27][28/233]	Loss 0.1133 (0.1342)	
training:	Epoch: [27][29/233]	Loss 0.1133 (0.1335)	
training:	Epoch: [27][30/233]	Loss 0.1181 (0.1330)	
training:	Epoch: [27][31/233]	Loss 0.1127 (0.1323)	
training:	Epoch: [27][32/233]	Loss 0.1200 (0.1319)	
training:	Epoch: [27][33/233]	Loss 0.1045 (0.1311)	
training:	Epoch: [27][34/233]	Loss 0.1312 (0.1311)	
training:	Epoch: [27][35/233]	Loss 0.1064 (0.1304)	
training:	Epoch: [27][36/233]	Loss 0.1288 (0.1303)	
training:	Epoch: [27][37/233]	Loss 0.1346 (0.1305)	
training:	Epoch: [27][38/233]	Loss 0.1182 (0.1301)	
training:	Epoch: [27][39/233]	Loss 0.1139 (0.1297)	
training:	Epoch: [27][40/233]	Loss 0.1440 (0.1301)	
training:	Epoch: [27][41/233]	Loss 0.0900 (0.1291)	
training:	Epoch: [27][42/233]	Loss 0.1387 (0.1293)	
training:	Epoch: [27][43/233]	Loss 0.1217 (0.1292)	
training:	Epoch: [27][44/233]	Loss 0.1385 (0.1294)	
training:	Epoch: [27][45/233]	Loss 0.1446 (0.1297)	
training:	Epoch: [27][46/233]	Loss 0.1356 (0.1298)	
training:	Epoch: [27][47/233]	Loss 0.1393 (0.1300)	
training:	Epoch: [27][48/233]	Loss 0.1768 (0.1310)	
training:	Epoch: [27][49/233]	Loss 0.0972 (0.1303)	
training:	Epoch: [27][50/233]	Loss 0.1227 (0.1302)	
training:	Epoch: [27][51/233]	Loss 0.1548 (0.1306)	
training:	Epoch: [27][52/233]	Loss 0.1353 (0.1307)	
training:	Epoch: [27][53/233]	Loss 0.0829 (0.1298)	
training:	Epoch: [27][54/233]	Loss 0.1153 (0.1296)	
training:	Epoch: [27][55/233]	Loss 0.1224 (0.1294)	
training:	Epoch: [27][56/233]	Loss 0.1426 (0.1297)	
training:	Epoch: [27][57/233]	Loss 0.1678 (0.1303)	
training:	Epoch: [27][58/233]	Loss 0.1135 (0.1301)	
training:	Epoch: [27][59/233]	Loss 0.0957 (0.1295)	
training:	Epoch: [27][60/233]	Loss 0.1437 (0.1297)	
training:	Epoch: [27][61/233]	Loss 0.1451 (0.1300)	
training:	Epoch: [27][62/233]	Loss 0.1769 (0.1307)	
training:	Epoch: [27][63/233]	Loss 0.1403 (0.1309)	
training:	Epoch: [27][64/233]	Loss 0.1202 (0.1307)	
training:	Epoch: [27][65/233]	Loss 0.1325 (0.1307)	
training:	Epoch: [27][66/233]	Loss 0.1121 (0.1304)	
training:	Epoch: [27][67/233]	Loss 0.1584 (0.1309)	
training:	Epoch: [27][68/233]	Loss 0.1057 (0.1305)	
training:	Epoch: [27][69/233]	Loss 0.1357 (0.1306)	
training:	Epoch: [27][70/233]	Loss 0.0979 (0.1301)	
training:	Epoch: [27][71/233]	Loss 0.1430 (0.1303)	
training:	Epoch: [27][72/233]	Loss 0.1554 (0.1306)	
training:	Epoch: [27][73/233]	Loss 0.1337 (0.1307)	
training:	Epoch: [27][74/233]	Loss 0.1237 (0.1306)	
training:	Epoch: [27][75/233]	Loss 0.1174 (0.1304)	
training:	Epoch: [27][76/233]	Loss 0.1128 (0.1302)	
training:	Epoch: [27][77/233]	Loss 0.1305 (0.1302)	
training:	Epoch: [27][78/233]	Loss 0.1318 (0.1302)	
training:	Epoch: [27][79/233]	Loss 0.1316 (0.1302)	
training:	Epoch: [27][80/233]	Loss 0.1263 (0.1302)	
training:	Epoch: [27][81/233]	Loss 0.1108 (0.1299)	
training:	Epoch: [27][82/233]	Loss 0.1397 (0.1300)	
training:	Epoch: [27][83/233]	Loss 0.1172 (0.1299)	
training:	Epoch: [27][84/233]	Loss 0.1108 (0.1297)	
training:	Epoch: [27][85/233]	Loss 0.1765 (0.1302)	
training:	Epoch: [27][86/233]	Loss 0.1210 (0.1301)	
training:	Epoch: [27][87/233]	Loss 0.1117 (0.1299)	
training:	Epoch: [27][88/233]	Loss 0.1004 (0.1296)	
training:	Epoch: [27][89/233]	Loss 0.0954 (0.1292)	
training:	Epoch: [27][90/233]	Loss 0.1240 (0.1291)	
training:	Epoch: [27][91/233]	Loss 0.1334 (0.1292)	
training:	Epoch: [27][92/233]	Loss 0.1227 (0.1291)	
training:	Epoch: [27][93/233]	Loss 0.1147 (0.1289)	
training:	Epoch: [27][94/233]	Loss 0.1844 (0.1295)	
training:	Epoch: [27][95/233]	Loss 0.1283 (0.1295)	
training:	Epoch: [27][96/233]	Loss 0.1337 (0.1296)	
training:	Epoch: [27][97/233]	Loss 0.2114 (0.1304)	
training:	Epoch: [27][98/233]	Loss 0.1420 (0.1305)	
training:	Epoch: [27][99/233]	Loss 0.1500 (0.1307)	
training:	Epoch: [27][100/233]	Loss 0.1914 (0.1313)	
training:	Epoch: [27][101/233]	Loss 0.1810 (0.1318)	
training:	Epoch: [27][102/233]	Loss 0.1027 (0.1315)	
training:	Epoch: [27][103/233]	Loss 0.1091 (0.1313)	
training:	Epoch: [27][104/233]	Loss 0.1543 (0.1315)	
training:	Epoch: [27][105/233]	Loss 0.0895 (0.1311)	
training:	Epoch: [27][106/233]	Loss 0.1088 (0.1309)	
training:	Epoch: [27][107/233]	Loss 0.1498 (0.1311)	
training:	Epoch: [27][108/233]	Loss 0.1278 (0.1311)	
training:	Epoch: [27][109/233]	Loss 0.1089 (0.1309)	
training:	Epoch: [27][110/233]	Loss 0.1180 (0.1308)	
training:	Epoch: [27][111/233]	Loss 0.1111 (0.1306)	
training:	Epoch: [27][112/233]	Loss 0.1105 (0.1304)	
training:	Epoch: [27][113/233]	Loss 0.1251 (0.1303)	
training:	Epoch: [27][114/233]	Loss 0.1597 (0.1306)	
training:	Epoch: [27][115/233]	Loss 0.1421 (0.1307)	
training:	Epoch: [27][116/233]	Loss 0.1141 (0.1306)	
training:	Epoch: [27][117/233]	Loss 0.1398 (0.1306)	
training:	Epoch: [27][118/233]	Loss 0.1010 (0.1304)	
training:	Epoch: [27][119/233]	Loss 0.1897 (0.1309)	
training:	Epoch: [27][120/233]	Loss 0.1360 (0.1309)	
training:	Epoch: [27][121/233]	Loss 0.1257 (0.1309)	
training:	Epoch: [27][122/233]	Loss 0.1111 (0.1307)	
training:	Epoch: [27][123/233]	Loss 0.1145 (0.1306)	
training:	Epoch: [27][124/233]	Loss 0.1169 (0.1305)	
training:	Epoch: [27][125/233]	Loss 0.1188 (0.1304)	
training:	Epoch: [27][126/233]	Loss 0.1466 (0.1305)	
training:	Epoch: [27][127/233]	Loss 0.1158 (0.1304)	
training:	Epoch: [27][128/233]	Loss 0.1444 (0.1305)	
training:	Epoch: [27][129/233]	Loss 0.1288 (0.1305)	
training:	Epoch: [27][130/233]	Loss 0.1096 (0.1303)	
training:	Epoch: [27][131/233]	Loss 0.1096 (0.1302)	
training:	Epoch: [27][132/233]	Loss 0.1447 (0.1303)	
training:	Epoch: [27][133/233]	Loss 0.1117 (0.1302)	
training:	Epoch: [27][134/233]	Loss 0.2209 (0.1308)	
training:	Epoch: [27][135/233]	Loss 0.1552 (0.1310)	
training:	Epoch: [27][136/233]	Loss 0.1294 (0.1310)	
training:	Epoch: [27][137/233]	Loss 0.1276 (0.1310)	
training:	Epoch: [27][138/233]	Loss 0.0978 (0.1307)	
training:	Epoch: [27][139/233]	Loss 0.1267 (0.1307)	
